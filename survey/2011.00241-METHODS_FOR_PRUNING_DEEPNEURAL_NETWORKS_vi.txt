# PHƯƠNG PHÁP PRUNING MẠNG NƠRON SÂU
BẢN THẢO

Sunil Vadera
Đại học Salford,
Greater Manchester, M5 4WT, Vương quốc Anh
S.Vadera@salford.ac.uk

Salem Ameen
Đại học Salford,
Greater Manchester, M5 4WT, Vương quốc Anh
S.Ameen@edu.salford.ac.uk

2 tháng 8, 2021

TÓM TẮT

Bài báo này trình bày một khảo sát về các phương pháp pruning mạng nơron sâu. Bài bắt đầu bằng việc phân loại hơn 150 nghiên cứu dựa trên phương pháp tiếp cận cơ bản được sử dụng và sau đó tập trung vào ba danh mục: các phương pháp sử dụng pruning dựa trên độ lớn, các phương pháp sử dụng clustering để xác định dư thừa, và các phương pháp sử dụng phân tích độ nhạy để đánh giá tác động của pruning. Một số nghiên cứu có ảnh hưởng quan trọng trong các danh mục này được trình bày để làm nổi bật các phương pháp tiếp cận cơ bản và kết quả đạt được. Hầu hết các nghiên cứu đều trình bày kết quả được phân tán trong tài liệu khi các kiến trúc mới, thuật toán và bộ dữ liệu đã phát triển theo thời gian, làm cho việc so sánh giữa các nghiên cứu khác nhau trở nên khó khăn. Do đó, bài báo cung cấp một tài nguyên cho cộng đồng có thể được sử dụng để nhanh chóng so sánh kết quả từ nhiều phương pháp khác nhau trên nhiều bộ dữ liệu đa dạng, và một loạt các kiến trúc, bao gồm AlexNet, ResNet, DenseNet và VGG. Tài nguyên này được minh họa bằng cách so sánh các kết quả được công bố để pruning AlexNet và ResNet50 trên ImageNet và ResNet56 và VGG16 trên dữ liệu CIFAR10 để tiết lộ phương pháp pruning nào hoạt động tốt về mặt duy trì độ chính xác trong khi đạt được tỷ lệ nén tốt. Bài báo kết thúc bằng việc xác định một số hướng nghiên cứu đầy hứa hẹn cho tương lai.

Từ khóa: Deep learning, Mạng nơron, Pruning mạng sâu

1 GIỚI THIỆU

Deep learning và việc sử dụng nó trong các ứng dụng cao cấp như xe tự lái (Kuutti et al., 2021), dự đoán ung thư vú (McKinney et al., 2020), nhận dạng giọng nói (Hinton et al., 2012) và xử lý ngôn ngữ tự nhiên (Otter et al., 2021) đã đưa sự quan tâm đến Trí tuệ Nhân tạo lên một tầm cao mới, với hầu hết các quốc gia đều đặt nó làm trung tâm trong các chiến lược công nghiệp và thương mại để đổi mới.

Mặc dù có các loại kiến trúc khác nhau (Pouyanfar et al., 2019), các mạng sâu thường bao gồm các lớp nơron được kết nối với các nơron trong các lớp trước thông qua các liên kết có trọng số. Một đặc điểm khác, được coi là trung tâm cho khả năng dự đoán của chúng (Sejnowski, 2020), là chúng có một số lượng lớn các tham số cần được học, với các mạng như ResNet50 (He et al., 2016) có hơn 25 triệu tham số và VGG16 (Simonyan và Zisserman, 2015) có hơn 138 triệu trọng số. Do đó, một câu hỏi rõ ràng là liệu có thể phát triển các mạng nhỏ hơn, hiệu quả hơn mà không làm giảm độ chính xác hay không? Một hướng nghiên cứu nhằm giải quyết câu hỏi này là đầu tiên huấn luyện một mạng lớn và sau đó pruning và fine-tune mạng đó. Mặc dù các phương pháp pruning mạng nơron nông đã được đề xuất trong những năm 1980 và 90 (Mozer và Smolensky, 1988; Kruschke, 1988; Reed, 1993), những tiến bộ gần đây trong deep learning và tiềm năng của nó cho các ứng dụng trong hệ thống nhúng đã dẫn đến việc gia tăng số lượng và đa dạng các thuật toán để pruning mạng nơron sâu.

Do đó, bài báo này trình bày một khảo sát về nghiên cứu gần đây về pruning mạng nơron có thể được sử dụng để hiểu các loại thuật toán được phát triển, đánh giá cao các ý tưởng chính làm nền tảng cho các thuật toán và làm quen với các phương pháp tiếp cận và vấn đề chính trong lĩnh vực này. Bài báo nhằm đạt được mục tiêu này bằng cách trình bày con đường tiến bộ từ các thuật toán trước đó đến nghiên cứu gần đây, phân loại các thuật toán dựa trên phương pháp tiếp cận được sử dụng, làm nổi bật những điểm tương đồng và khác biệt giữa các thuật toán và kết thúc với một số hướng nghiên cứu tương lai.

Các nghiên cứu về phương pháp pruning đều thực hiện đánh giá thực nghiệm so sánh hiệu suất của các thuật toán trên các kiến trúc và bộ dữ liệu benchmark khác nhau. Các đánh giá này đã phát triển khi các kiến trúc deep learning mới được phát triển, khi các bộ dữ liệu mới trở nên có sẵn và khi các thuật toán pruning mới được đề xuất. Bài báo này cũng cung cấp và minh họa việc sử dụng một tài nguyên tập hợp các kết quả được báo cáo ở một nơi, cho phép các nhà nghiên cứu nhanh chóng so sánh các kết quả được báo cáo trên các kiến trúc và bộ dữ liệu khác nhau.

Khảo sát này xác định hơn 150 nghiên cứu về pruning mạng nơron, có thể được phân loại thành tám nhóm sau dựa trên phương pháp tiếp cận cơ bản được sử dụng:

1. Phương pháp pruning dựa trên độ lớn (Chauvin, 1988; Weigend, 1990; Weigend et al., 1991; Zhou et al., 2018b), dựa trên quan điểm rằng tính quan trọng của trọng số và nơron có thể được xác định bởi các thước đo cục bộ như độ lớn của chúng hoặc được xấp xỉ bởi tác động của chúng lên lớp tiếp theo.

2. Phương pháp similarity và clustering (Chen et al., 1993; Han et al., 2016a; Li et al., 2019c; RoyChowdhury et al., 2017; Sussmann, 1992; Zhou et al., 2018a) nhằm xác định các trọng số trùng lặp hoặc tương tự mà dư thừa và có thể được pruning mà không ảnh hưởng đến độ chính xác.

3. Phương pháp phân tích độ nhạy (Mozer và Smolensky, 1988; LeCun et al., 1990; Hassibi et al., 1993b,a; Cohen et al., 2016; Lee et al., 2018b; Lin et al., 2018b), đánh giá tác động của việc loại bỏ hoặc nhiễu loạn trọng số lên loss function và sau đó loại bỏ một tỷ lệ các trọng số có ít ảnh hưởng nhất đến độ chính xác.

4. Phương pháp knowledge distillation (Bucilu˘a et al., 2006; Hinton et al., 2015; Gregor Urban et al., 2017; Zhang et al., 2019b) sử dụng mô hình gốc, được gọi là Teacher, để học một mô hình mới nhỏ gọn hơn gọi là Student.

5. Phương pháp low rank (Sainath et al., 2013; Jaderberg et al., 2014; Lin et al., 2018b) phân tích một ma trận trọng số thành tích của hai ma trận nhỏ hơn có thể được sử dụng để thực hiện một chức năng tương đương hiệu quả hơn so với ma trận trọng số lớn đơn lẻ.

6. Phương pháp quantization (Jung et al., 2019; Zhou et al., 2017; Zhao et al., 2019b; Courbariaux et al., 2016; Chen et al., 2015; Jacob et al., 2018), dựa trên việc sử dụng quantization, hashing, biểu diễn độ chính xác thấp và nhị phân của các trọng số như một cách để giảm tính toán.

7. Phương pháp thiết kế kiến trúc (Baker et al., 2017; Dai et al., 2019; Li et al., 2019d; Liu et al., 2019a; Lin et al., 2020c; Zhong et al., 2018; Zoph và Le., 2017) sử dụng tìm kiếm thông minh và phương pháp reinforcement learning để tạo ra các kiến trúc mạng nơron.

8. Phương pháp hybrid (Chung và Shin, 2016; Goetschalckx et al., 2018; Gadosey et al., 2019) sử dụng sự kết hợp các phương pháp nhằm tận dụng các hiệu ứng nén tích lũy của các loại phương pháp khác nhau.

Phụ lục A cung cấp một bảng phân loại các nghiên cứu hiện có thành 8 danh mục, giúp các nhà nghiên cứu làm việc với một loại phương pháp cụ thể có thể tìm các nghiên cứu liên quan. Với sự đa dạng của các nghiên cứu và sự có sẵn của các khảo sát đã bao phủ một số danh mục trên, bài báo này tập trung vào các thuật toán gần đây trong ba danh mục đầu tiên để pruning.

Reed (1993) cung cấp một khảo sát xuất sắc về các phương pháp pruning trước kỷ nguyên deep learning. Độc giả quan tâm đến việc sử dụng quantization, low rank và phương pháp knowledge distillation được tham khảo khảo sát của Lebedev và Lempitsky (2018) và độc giả quan tâm đến phương pháp thiết kế kiến trúc được tham khảo khảo sát toàn diện của Elsken et al. (2019). Pruning mạng chỉ là một bước trong việc phát triển các mô hình hiệu quả và một khảo sát gần đây của Menghani (2021) tóm tắt toàn bộ phạm vi các phương pháp, từ việc sử dụng quantization và learning, đến cơ sở hạ tầng phần mềm và phần cứng có sẵn để triển khai hiệu quả các mô hình. Một hướng nghiên cứu quan trọng khác, xứng đáng có một khảo sát riêng, và không nằm trong phạm vi của bài báo này, là việc sử dụng các phương pháp Bayesian biến phân để regularization (Arbib, 2003; Goodfellow et al., 2016; Huang và Wang, 2018; Lin et al., 2020d; Srivastava et al., 2014; Wen et al., 2016; Zhao et al., 2019a).

Hình 1 cho thấy một lựa chọn các phương pháp được bao phủ chi tiết hơn trong khảo sát này và bao gồm một phân loại phụ của các phương pháp magnitude và phân tích độ nhạy. Khảo sát tìm thấy tương đối ít phương pháp sử dụng similarity và clustering, và việc phân loại phụ thêm không hữu ích. Các phương pháp dựa trên magnitude có thể được phân loại phụ thành: (i) các phương pháp phụ thuộc dữ liệu sử dụng một mẫu các ví dụ để đánh giá mức độ mà việc loại bỏ trọng số ảnh hưởng đến đầu ra từ lớp tiếp theo; (ii) các phương pháp độc lập với dữ liệu, sử dụng các thước đo như độ lớn của một trọng số; và (iii) việc sử dụng các phương pháp tối ưu hóa để giảm số lượng trọng số trong một lớp trong khi xấp xỉ chức năng của lớp đó. Các phương pháp sử dụng phân tích độ nhạy có thể được phân loại phụ thành những phương pháp: (i) áp dụng xấp xỉ chuỗi Taylor của loss function và (ii) sử dụng sampling để ước tính thay đổi trong loss function khi các trọng số bị loại bỏ.

Phần còn lại của bài báo được tổ chức như sau. Phần 2 trình bày kiến thức nền tảng. Các phần 3 đến 5 mô tả các phương pháp đại diện trong ba danh mục: pruning dựa trên magnitude, clustering và similarity, và phân tích độ nhạy. Phần 3 cũng bao gồm việc bao phủ Lottery Hypothesis, một vấn đề về sự tồn tại của các mạng nhỏ hơn và fine-tuning, cắt ngang các phương pháp khác nhau. Phần 6 trình bày so sánh các kết quả được công bố để pruning AlexNet, ResNet và VGG để minh họa tài nguyên được cung cấp để so sánh các phương pháp. Phần 7 kết thúc bằng việc làm nổi bật một số hiểu biết quan trọng và đề xuất hướng nghiên cứu tương lai.

2 KIẾN THỨC NỀN TẢNG

Phần này giới thiệu kiến thức nền tảng được giả định trong khảo sát. Hình 2 cho thấy cấu trúc của một trong những mạng nơron tích chập (CNN) sớm nhất, LeNet-5 (LeCun et al., 1989), nhận dạng chữ số viết tay bằng cách áp dụng các phép tích chập và pooling để xác định đặc trưng. Các đặc trưng này sau đó cung cấp đầu vào cho các lớp kết nối đầy đủ để phân loại hình ảnh. Phép toán pooling lấy các feature map làm đầu vào và giảm kích thước của chúng bằng cách áp dụng một phép toán, chẳng hạn như giá trị tối đa trong một vùng lân cận trong khi phép toán tích chập áp dụng các bộ lọc (hoặc kernel) cho các kênh đầu vào (hoặc feature map) để tạo ra các feature map đầu ra. Các bộ lọc là các ma trận k x k trượt qua các feature map đầu vào và tích chập với các phần tử tương ứng của các feature map đầu vào để tạo ra các feature map đầu ra. Các phần tử của một bộ lọc tương ứng với các trọng số (hoặc tham số) được sử dụng để biến đổi các vùng trong feature map từ một lớp sang lớp tiếp theo và cần được học thông qua huấn luyện. Do đó, các trọng số (hoặc tham số), hoặc cá nhân hoặc tập thể như các bộ lọc, là các ứng cử viên chính để pruning.

Mô hình LeNet-5, với 60K tham số trong 5 lớp, đạt được kết quả ấn tượng trên một bộ dữ liệu được gọi là MNIST (LeCun et al., 1998). Trong một bước đột phá vào năm 2012, AlexNet xây dựng dựa trên các khái niệm trong LeNet-5 và phát triển một mạng sâu hơn với hơn 60M tham số trong 8 lớp để thắng một cuộc thi được gọi là ImageNet với một biên độ đáng kể (Krizhevsky et al., 2012). Thành công này được theo sau bởi việc phát triển các kiến trúc như VGG, ResNet, và ResNeXT sử dụng số lượng lớp và tham số ngày càng tăng để đạt được những cải thiện hơn nữa trong cuộc thi ImageNet (Pouyanfar et al., 2019). Số lượng tham số khổng lồ trong các mô hình này đòi hỏi tài nguyên tính toán lớn hơn và cản trở việc sử dụng chúng trong các hệ thống nhúng, điều này đã thúc đẩy nghiên cứu về pruning được khảo sát trong bài báo này.

Các phương pháp pruning được phát triển được đánh giá trên một loạt các kiến trúc (ví dụ: ResNet, VGG, DenseNet) và bộ dữ liệu (ví dụ: ImageNet CIFAR, SVHN). Khan et al. (2020) trình bày một hướng dẫn về các kiến trúc deep learning và Phụ lục B tóm tắt các bộ dữ liệu. Khi đánh giá các phương pháp pruning, các bài báo được khảo sát sử dụng các thước đo sau để báo cáo kết quả của họ:

• Độ chính xác Top-1 và Top-5, báo cáo tỷ lệ lần phân loại chính xác xuất hiện đầu tiên hoặc trong top 5 danh sách kết quả được xếp hạng. Trong các phần dưới đây, trừ khi chúng tôi chỉ rõ một thước đo, độ chính xác Top-1 nên được giả định.

• Tỷ lệ nén, là tỷ số tham số trước và sau khi một mô hình được pruning.

• Hiệu quả tính toán về mặt FLOPS (Floating Point Operations) cần thiết để thực hiện một phân loại.

Ký hiệu được sử dụng trong bài báo được định nghĩa tại nơi nó được sử dụng và cũng được tóm tắt trong Phụ lục C. Với kiến thức nền tảng này, các Phần 3 đến 5 mô tả một số nghiên cứu có ảnh hưởng quan trọng làm nổi bật các đặc điểm của các danh mục phương pháp được khảo sát trong bài báo này.

3 PRUNING DỰA TRÊN MAGNITUDE

Phần này trình bày các phương pháp pruning loại bỏ trọng số, nút và bộ lọc dựa trên một thước đo độ lớn hoặc tác động mà các bộ lọc có lên lớp tiếp theo. Tiểu mục 3.1 tóm tắt một phương pháp có ảnh hưởng sớm để pruning trọng số và Tiểu mục 3.2 trình bày một chủ đề nóng gần đây, được gọi là Lottery Hypothesis, làm phục hồi nghiên cứu về sự tồn tại của các mạng nhỏ hơn và đặt ra các vấn đề về fine-tuning một mạng đã được pruning. Tiểu mục 3.3 mô tả các ý tưởng chính đằng sau các phương pháp pruning bộ lọc và feature map.

3.1 Network Pruning của Trọng số

Một trong những nghiên cứu đầu tiên sử dụng magnitude-based pruning cho mạng sâu là của Han et al. (2015) áp dụng một quy trình trong đó các trọng số dưới một ngưỡng nhất định được pruning. Sau khi được pruning, mạng được fine-tune và quy trình được lặp lại cho đến khi độ chính xác của nó bắt đầu giảm.

Han et al. (2015) thực hiện một số thí nghiệm để so sánh ưu điểm của phương pháp pruning lặp dựa trên magnitude của họ. Đầu tiên, họ áp dụng phương pháp của họ trên một mạng kết nối đầy đủ được gọi là LeNet-300-100 và sau đó trên Lenet-5 (Hình 2), cả hai đều được huấn luyện trên dữ liệu MNIST. Kết quả của họ cho thấy có thể giảm số lượng trọng số theo một hệ số 12 mà không làm giảm độ chính xác. Thứ hai, họ áp dụng iterative pruning cho AlexNet và VGG16 được huấn luyện trên dữ liệu ImageNet, và cho thấy có thể giảm số lượng trọng số theo một hệ số 9 và 12 tương ứng. Thứ ba, họ so sánh ưu điểm của việc sử dụng regularization để giảm magnitude của trọng số để hỗ trợ pruning tiếp theo. Họ khám phá regularization với cả norm L1 và L2 và kết luận rằng L1 tốt hơn ngay sau pruning (không có fine-tuning), nhưng L2 tốt hơn nếu các trọng số của mô hình được pruning được fine-tune. Các thí nghiệm của họ cũng gợi ý rằng các lớp trước đó (tức là, gần với đầu vào hơn) nhạy cảm nhất với pruning và rằng iterative pruning tốt hơn so với pruning tỷ lệ trọng số cần thiết trong một chu kỳ (tức là, one-shot pruning).

Nghiên cứu của Han et al. (2015) đáng chú ý ở chỗ (i) nó chứng minh rằng có thể giảm kích thước của mạng sâu đáng kể mà không làm giảm độ chính xác, (ii) nó làm nổi bật lợi ích của iterative pruning và (iii) nó thúc đẩy nghiên cứu sâu hơn về các câu hỏi như liệu huấn luyện lại từ đầu hay fine-tuning tốt hơn sau pruning.

Guo et al. (2016) lưu ý rằng magnitude pruning có thể dẫn đến việc loại bỏ sớm các trọng số có thể trở nên quan trọng khi các trọng số khác bị loại bỏ. Để giải quyết điều này, họ đề xuất một phương pháp được gọi là Dynamic Network Surgery (Dyn Surg) duy trì một mask chỉ ra trọng số nào nên được loại bỏ và giữ lại trong mỗi chu kỳ huấn luyện, do đó cho phép phục hồi các trọng số trước đó được đánh dấu để pruning nếu chúng hóa ra là quan trọng. Guo et al. (2016) so sánh phương pháp của họ với magnitude pruning, với kết quả cho thấy nó giảm số lượng trọng số theo một hệ số hơn 17 cho AlexNet trên ImageNet.

3.2 Lottery Ticket Hypothesis

Một trong những quan sát thú vị nhất trong (Han et al., 2015) là việc khởi tạo lại các trọng số không dẫn đến các mô hình chính xác và, dựa trên các thử nghiệm của họ, việc fine-tune các trọng số của mô hình được pruning là tốt hơn. Tiếp theo từ quan sát này, Frankle và Carbin (2019) đề xuất Lottery Ticket Hypothesis phát biểu rằng: một mạng được huấn luyện chứa một mạng con, có thể được huấn luyện để chính xác ít nhất bằng mạng gốc bằng cách sử dụng không quá số epoch được sử dụng để huấn luyện mạng gốc. Mạng con này được gọi là winning lottery ticket, vì nó may mắn được khởi tạo với các trọng số phù hợp.

Để kiểm tra giả thuyết này, họ đề xuất hai phương pháp pruning. Đầu tiên, trong một phương pháp one-shot, họ sử dụng magnitude pruning để pruning p% các trọng số, đặt lại các trọng số còn lại về giá trị ban đầu của chúng và huấn luyện lại. Thứ hai, họ sử dụng một phương pháp iterative pruning với n chu kỳ, với mỗi chu kỳ pruning p^(1/n) các trọng số.

Họ thực hiện thí nghiệm trên mạng kết nối đầy đủ LeNet-300-100 cho dữ liệu MNIST, và các biến thể của VGG và ResNet cho dữ liệu CIFAR10. Các thí nghiệm của họ trên mạng LeNet-300-100 pruning một phần trăm các trọng số từ mỗi lớp ngoại trừ lớp cuối cùng, trong đó phần trăm được pruning được giảm một nửa. Kết quả của họ với iterative pruning cho thấy: (i) một mạng con chỉ là 3.6% kích thước ban đầu của nó hoạt động tốt như nhau, (ii) khởi tạo ngẫu nhiên của các mạng được pruning dẫn đến việc học chậm hơn so với việc sử dụng khởi tạo trọng số ban đầu, (iii) các mạng con (được gọi là winning tickets) được tìm thấy, học nhanh hơn mạng gốc, (iv) có sự cải thiện liên tục trong tốc độ học khi kích thước mạng giảm, nhưng chỉ đến một điểm, sau đó việc học chậm lại và bắt đầu thoái lui về hiệu suất của mạng gốc, (v) iterative pruning có xu hướng dẫn đến các mạng nhỏ hơn chính xác hơn so với one-shot pruning.

Các thí nghiệm của họ trên các mạng lớn hơn, VGG và ResNet, cho thấy việc xác định winning lotteries phụ thuộc vào learning rate, với tốc độ thấp hơn thành công xác định winning lottery subnet, và rằng pruning trọng số trên toàn bộ mạng, trái ngược với từng lớp một, tạo ra kết quả tốt hơn.

Những kết quả này cung cấp bằng chứng thực nghiệm tốt cho Lottery Hypothesis và việc trao giải thưởng bài báo hay nhất trong Hội nghị Quốc tế về Biểu diễn Học 2019 là dấu hiệu của tầm quan trọng của bài báo và sự chú ý mà nó thu hút.

Trong bài báo của họ, "Rethinking the value of network pruning", Liu et al. (2019b) thách thức tuyên bố rằng việc sử dụng các trọng số ban đầu của một mô hình được pruning tốt hơn so với khởi tạo ngẫu nhiên. Để kiểm tra điều này, họ thực hiện thí nghiệm trên VGG, ResNet, và DenseNet sử dụng dữ liệu CIFAR10, CIFAR100, và ImageNet. Họ định nghĩa ba loại chế độ pruning: structured pruning, trong đó tỷ lệ các kênh được pruning mỗi lớp được định nghĩa trước; automatic pruning, trong đó tỷ lệ các kênh được pruning tổng thể được định nghĩa trước nhưng tỷ lệ mỗi lớp được xác định bởi thuật toán; và unstructured weight pruning, trong đó chỉ tỷ lệ trọng số được pruning được định nghĩa trước. Kết quả của họ gợi ý rằng đối với structured và automatic pruning, khởi tạo ngẫu nhiên hiệu quả bằng (nếu không muốn nói là hơn). Tuy nhiên, đối với các mạng unstructured, khởi tạo ngẫu nhiên có thể đạt được kết quả tương tự trên các bộ dữ liệu nhỏ nhưng đối với dữ liệu quy mô lớn như ImageNet, fine-tuning tạo ra kết quả tốt hơn.

Thoạt nhìn, những phát hiện của họ mâu thuẫn với Lottery Hypothesis. Tuy nhiên, trong một nghiên cứu tiếp theo, Frankle et al. (2019) thừa nhận rằng việc đặt các trọng số của mạng được pruning về giá trị ban đầu của chúng không hoạt động tốt trên các mạng lớn hơn và gợi ý rằng các phương pháp huấn luyện lại từ khởi tạo ngẫu nhiên cũng không hoạt động tốt, ngoại trừ các mức độ pruning vừa phải (lên đến 30%). Do đó họ đề xuất đặt các trọng số về những giá trị thu được trong một vòng lặp huấn luyện sau đó, mà sau đó họ chứng minh là có lợi trong việc xác định khởi tạo trọng số tốt cho các vấn đề quy mô lớn hơn như ImageNet.

Các nghiên cứu trên tập trung vào đánh giá thực nghiệm của các mạng được huấn luyện và sử dụng trên cùng bộ dữ liệu, và chủ yếu trên các nhiệm vụ phân loại xử lý hình ảnh. Morcos et al. (2019) khám phá một số câu hỏi thú vị khác:

• Các lottery được tìm thấy cho một nhiệm vụ phân loại hình ảnh có thể chuyển giao cho các nhiệm vụ khác không?
• Lottery có quan sát được trong các nhiệm vụ khác (như xử lý ngôn ngữ tự nhiên), và các kiến trúc không?
• Chúng có thể chuyển giao qua các optimizer khác nhau không?

Để khám phá những câu hỏi này, họ thực hiện thí nghiệm với VGG19 và ResNet50 sử dụng sáu bộ dữ liệu (Fashion-MNIST, SVHN, CIFAR10, CIFAR100, ImageNet, Places365), trong đó các lottery (tức là, mạng con với khởi tạo) được xác định cho một nhiệm vụ được sử dụng cho một nhiệm vụ khác. Các thí nghiệm của họ sử dụng iterative magnitude-based pruning, chọn 20% trọng số trên tất cả các lớp, và với late setting của trọng số (như được đề xuất trong (Frankle và Carbin, 2019)). Kết quả thú vị: nói chung, winning initializations mang qua các nhiệm vụ xử lý hình ảnh tương tự và winning tickets từ các nhiệm vụ quy mô lớn hơn có thể chuyển giao hơn so với tickets từ các nhiệm vụ quy mô nhỏ hơn. Trong một số trường hợp, ví dụ, việc sử dụng VGG19 trên dữ liệu Fashion-MNIST, winning tickets thu được từ việc sử dụng VGG19 trên các bộ dữ liệu lớn hơn (CIFAR100, ImageNet) hoạt động tốt hơn so với những tickets thu được trực tiếp từ dữ liệu Fashion-MNIST.

Hubens et al. (2020) thực hiện các thử nghiệm thực nghiệm xác nhận kết quả tương tự về kích thước của các mạng được pruning. Họ cho thấy rằng khi một mạng được huấn luyện trên một bộ dữ liệu lớn hơn, như ImageNet, và được chuyển giao và fine-tune cho một nhiệm vụ khác, pruning có thể dẫn đến một mạng nhỏ hơn so với nếu nó được huấn luyện từ đầu trên nhiệm vụ mới.

Morcos et al. (2019) thực hiện thí nghiệm trong đó lottery tickets được xác định bằng một optimizer, ADAM (adaptive moment estimation), và sau đó sử dụng một optimizer khác, SGD (Stochastic Gradient Descent) với momentum, và ngược lại trên dữ liệu CIFAR10. Kết quả của họ gợi ý rằng, nói chung, winning tickets độc lập với optimizer.

Để kiểm tra xem lottery hypothesis có giữ trong các loại vấn đề khác không, Yu et al. (2019) thực hiện thí nghiệm trên xử lý ngôn ngữ tự nhiên (NLP) và các nhiệm vụ điều khiển trong games. Đối với NLP, họ sử dụng LSTM cho dữ liệu Wikitext-2 (Merity et al., 2017) và mô hình Transformer để dịch tin tức từ tiếng Anh sang tiếng Đức (Vaswani et al., 2017). Các thí nghiệm được thực hiện với 20 vòng iterative pruning và với one-shot pruning. Tỷ lệ pruning 20% được sử dụng và sau pruning, trọng số được đặt lại về những giá trị học được trong vòng huấn luyện sau đó. Đối với các nhiệm vụ điều khiển, họ sử dụng Reinforcement Learning (RL) và thực hiện thí nghiệm trên các mạng kết nối đầy đủ được sử dụng cho 3 môi trường OpenAI Gym (Brockman et al., 2016) và 9 game Atari sử dụng mạng tích chập (Bellemare et al., 2015). Từ kết quả của họ trên NLP và các nhiệm vụ điều khiển RL, họ kết luận rằng cả iterative pruning và late setting của trọng số đều vượt trội so với khởi tạo ngẫu nhiên của các mạng được pruning, với iterative pruning là thiết yếu khi một số lượng đáng kể trọng số (tức là, hơn hai phần ba) được pruning. Đối với các game Atari, kết quả khác nhau: trong một trường hợp, nó dẫn đến cải thiện so với mạng gốc (game Berzerk) trong khi ở trường hợp khác, một cải thiện ban đầu được theo sau bởi một sự giảm đáng kể về độ chính xác khi lượng pruning tăng (game Space Invaders). Trong các trường hợp khác, pruning dẫn đến giảm hiệu suất (ví dụ, game Assault). Vậy tóm lại, Yu et al. (2019) cung cấp một số bằng chứng rằng lottery hypothesis giữ cho các nhiệm vụ NLP và cho một số nhiệm vụ điều khiển sử dụng RL.

3.3 Pruning Feature Maps và Filters

Mặc dù các loại phương pháp mô tả trong Tiểu mục 3.1 dẫn đến ít trọng số hơn, chúng yêu cầu các thư viện hoặc phần cứng chuyên dụng để xử lý các ma trận trọng số thưa thớt kết quả (Li et al., 2017; Denil et al., 2013; Ayinde et al., 2019). Ngược lại, pruning ở các mức độ chi tiết cao hơn, như pruning bộ lọc và kênh được hưởng lợi từ các tối ưu hóa đã có sẵn trong nhiều toolkit hiện tại. Điều này đã dẫn đến một số phương pháp để pruning feature maps và bộ lọc được tóm tắt trong phần này.

Để đánh giá trực giác và ký hiệu đằng sau các phương pháp này, đáng để ghi nhớ cách các bộ lọc được áp dụng cho các kênh đầu vào để tạo ra các feature maps đầu ra. Hình 3 minh họa quy trình, cho thấy cách một hình ảnh với 3 kênh được lấy làm đầu vào và tích chập với các bộ lọc để tạo ra 4 feature maps đầu ra. Với sự trực quan hóa được cung cấp bởi Hình 3, làm thế nào có thể pruning tốt nhất các bộ lọc và kênh? Khảo sát tiết lộ ba hướng nghiên cứu chính:

1. Phương pháp pruning kênh phụ thuộc dữ liệu, dựa trên quan điểm rằng khi các đầu vào khác nhau được trình bày, các kênh đầu ra (tức là, feature maps) nên thay đổi vì chúng có nghĩa là phát hiện các đặc trưng phân biệt. Các tiểu mục 3.3.1 và 3.3.2 mô tả các phương pháp áp dụng quan điểm này.

2. Phương pháp pruning độc lập với dữ liệu, sử dụng thuộc tính của các bộ lọc và kênh đầu ra, chẳng hạn như tỷ lệ số không có mặt, để quyết định bộ lọc và kênh nào nên được pruning. Các tiểu mục 3.3.3 đến 3.3.6 mô tả các phương pháp theo hướng này.

3. Phương pháp pruning xấp xỉ kênh dựa trên tối ưu hóa, sử dụng các phương pháp tối ưu hóa để tái tạo các bộ lọc để xấp xỉ các feature maps đầu ra. Các phương pháp pruning tiêu biểu cho phương pháp này được mô tả trong các tiểu mục 3.3.6 và 3.3.7.

3.3.1 Pruning dựa trên variance của các kênh và bộ lọc

Polyak và Wolf (2015) đề xuất hai phương pháp để pruning kênh: Inbound pruning, nhằm giảm số lượng kênh đến một bộ lọc và Reduce and Reuse pruning, nhằm giảm số lượng kênh đầu ra.

Ý tưởng đằng sau Inbound pruning là đánh giá mức độ mà đóng góp của một kênh đầu vào để tạo ra một feature map đầu ra thay đổi với các ví dụ khác nhau. Đánh giá này được thực hiện bằng cách áp dụng mạng cho một mẫu hình ảnh và sau đó sử dụng variance trong một feature map như một thước đo đóng góp của nó.

Chính thức hơn, cho Wj,i, bộ lọc thứ j cho kênh đầu vào thứ i, và X^p_i, đầu vào từ kênh thứ i cho ví dụ thứ p, đóng góp cho feature map đầu ra thứ j, Y^p_j,i được định nghĩa bởi:

Y^p_j,i = ||Wj,i ⊛ X^p_i||_F (1)

Với định nghĩa này, thước đo được sử dụng để đánh giá sự biến thiên trong đóng góp của nó, s²j,i từ N mẫu là:

s²j,i = var{Y^p_j,i | p = 1...N} (2)

Inbound pruning sử dụng thước đo này để xếp hạng các bộ lọc Wj,i và loại bỏ bất kỳ bộ lọc nào rơi xuống dưới một ngưỡng được chỉ định.

Phương pháp Reduce and Reuse pruning tập trung vào đánh giá các biến thiên trong feature maps đầu ra khi các mẫu khác nhau được trình bày. Nghĩa là, phương pháp đầu tiên tính toán các biến thiên trong feature maps đầu ra s²j sử dụng:

s²j = var{||∑(i=1 to m) Y^p_j,i||_F | p = 1...N} (3)

Trong đó m là số lượng kênh đầu vào và N là số lượng mẫu. Reduce and Reuse sau đó sử dụng thước đo này để giữ lại một tỷ lệ các feature maps đầu ra và các bộ lọc tương ứng dẫn đến biến thiên lớn nhất.

Việc loại bỏ một feature map đầu ra có vấn đề vì nó được mong đợi như một kênh đầu vào trong lớp tiếp theo. Để khắc phục điều này, họ xấp xỉ một kênh bị loại bỏ bằng cách sử dụng các kênh khác. Nghĩa là, nếu Yi, Y'i là đầu ra của một lớp trước và sau khi pruning một lớp tương ứng, mục tiêu là tìm một ma trận A sao cho:

min_A ∑_i ||Yi - AY'i||²₂ (4)

Ma trận A sau đó được bao gồm như một lớp tích chập bổ sung của các bộ lọc 1x1 theo hướng được đề xuất bởi Lin et al. (2014).

Polyak và Wolf (2015) đánh giá phương pháp trên trên mạng Scratch, sử dụng các bộ dữ liệu CASIA-WebFace và Labeled Faces in the Wild (LFW). Họ sử dụng pruning từng lớp một, trong đó mỗi lớp được pruning, và mạng được fine-tune trước khi chuyển sang lớp tiếp theo. Họ thực nghiệm với hai phương pháp pruning của họ riêng lẻ và kết hợp, và so sánh kết quả với việc sử dụng random pruning, một phương pháp xấp xỉ low rank (Zhang et al., 2016) và Fitnets, một phương pháp sử dụng phương pháp Knowledge Distillation để học các mạng nhỏ hơn (Romero et al., 2014). Trong các thí nghiệm với phương pháp Inbound pruning, họ pruning các kênh trong đó s²j,i dưới một ngưỡng nhất định, được chọn sao cho độ chính xác tổng thể được duy trì trên 84%. Đối với các thí nghiệm với phương pháp Reduce and Reuse, họ thử các mức pruning khác nhau: 50%, 75%, và 90% cho các lớp trước đó tiếp theo là 50% cho các lớp sau đó. Việc áp dụng tỷ lệ pruning thấp hơn cho các lớp sau đó theo sau một quan sát rằng pruning mạnh các lớp sau đó dẫn đến giảm đáng kể về độ chính xác.

Kết quả từ các thí nghiệm của họ cho thấy: (i) phương pháp dựa trên variance hiệu quả hơn so với việc sử dụng random pruning, (ii) việc sử dụng fine-tuning giúp khôi phục độ chính xác, đặc biệt trong các lớp sau đó, (iii) các phương pháp của họ dẫn đến nén lớn hơn so với việc sử dụng phương pháp low rank và việc sử dụng Fitnets khi áp dụng cho mạng Scratch.

3.3.2 Entropy-based channel pruning

Thay vì variance, Luo và Wu (2017) đề xuất một metric dựa trên entropy để đánh giá tầm quan trọng của mỗi bộ lọc. Trong phương pháp filter pruning của họ, nếu một feature map chứa ít thông tin hơn, bộ lọc tương ứng của nó được coi là ít quan trọng hơn, và có thể được pruning. Để tính toán giá trị entropy của một feature map cụ thể, họ đầu tiên lấy mẫu dữ liệu và thu được một tập hợp feature maps cho mỗi bộ lọc. Mỗi feature map được giảm xuống một thước đo điểm bằng cách sử dụng phương pháp global average pooling, và tập hợp các thước đo liên quan đến mỗi bộ lọc được rời rạc hóa thành q nhóm. Entropy của một bộ lọc, Hj sau đó được sử dụng để đánh giá khả năng phân biệt của một bộ lọc (Luo và Wu, 2017):

Hj = -∑(i=1 to q) Pi log(Pi) (5)

Trong đó Pi là xác suất của một ví dụ thuộc nhóm i.

Họ khám phá cả one-shot pruning tiếp theo bởi fine-tuning và layer wise pruning trong đó họ fine-tune với chỉ một hoặc hai epoch học ngay sau khi pruning một lớp. Chiến lược layer wise của họ là một sự thỏa hiệp thú vị giữa việc fine-tune đầy đủ sau khi pruning mỗi lớp, có thể tốn kém về mặt tính toán, và chỉ fine-tune ở cuối, có thể không tính đến các tác động liên kết của việc pruning các lớp trước đó.

Họ đánh giá ưu điểm của phương pháp dựa trên entropy bằng cách áp dụng nó cho VGG16 và ResNet-50 trên dữ liệu ImageNet. Đối với VGG16, họ tập trung vào 10 lớp đầu tiên và, cũng thay thế các lớp kết nối đầy đủ bằng việc sử dụng average pooling để có được thêm các giảm. Họ so sánh kết quả của họ trên VGG16 với những kết quả thu được bởi phương pháp magnitude-based pruning và phương pháp APoZ (c/f Tiểu mục 3.3.3). Kết quả của họ gợi ý rằng: (i) phương pháp dựa trên entropy đạt được hơn 16 lần nén, mặc dù điều này với chi phí của việc giảm 1.56% về độ chính xác, (ii) việc sử dụng magnitude pruning dẫn đến 13 lần nén, và (iii) APoZ dẫn đến 2.7 lần nén. Tuy nhiên, cần lưu ý rằng tỷ lệ nén cao hơn đạt được bởi việc sử dụng phương pháp dựa trên entropy bao gồm việc giảm do thay thế các lớp kết nối đầy đủ bằng average pooling, mà không có nó việc sử dụng phương pháp dựa trên entropy dẫn đến tỷ lệ nén thấp hơn APoZ (Bảng 3 trong Luo và Wu (2017)).

3.3.3 APoZ: Network trimming dựa trên số không trong một kênh

Trái ngược với việc sử dụng các mẫu dữ liệu để tính toán variance của một feature map hoặc entropy của nó, Hu et al. (2016), gợi ý một phương pháp trực tiếp dựa trên quan điểm rằng số lượng số không trong một feature map đầu ra là dấu hiệu của sự dư thừa của nó. Dựa trên quan điểm này, họ đề xuất một phương pháp sử dụng số lượng trung bình các activation bằng không (APoZ) trong một feature map (sau ReLU) như một thước đo độ yếu của một bộ lọc tạo ra feature map.

Các thí nghiệm của họ với LeNet5 trên MNIST và VGG16 trên ImageNet và nhằm đầu tiên tìm các lớp phù hợp nhất để pruning và sau đó để iteratively pruning các lớp này theo một cách bespoke duy trì hoặc cải thiện độ chính xác. Sau pruning, họ thực nghiệm với cả việc huấn luyện lại từ đầu và fine-tuning các trọng số và ưu tiên cái sau đó vì kết quả tốt hơn.

Đối với LeNet-5, họ quan sát rằng hầu hết các tham số (hơn 90%) nằm trong lớp tích chập thứ 2 và lớp kết nối đầy đủ đầu tiên và do đó họ tập trung vào pruning hai lớp này trong bốn vòng lặp pruning và fine-tuning, dẫn đến kích thước của lớp tích chập giảm từ 50 xuống 24 bộ lọc và số lượng nơron trong lớp kết nối đầy đủ giảm từ 500 xuống 252. Tổng thể, điều này đại diện cho tỷ lệ nén 3.85.

Đối với VGG16, họ cũng tập trung vào một lớp tích chập có 512 bộ lọc và một lớp kết nối đầy đủ với 4096 nút. Sau 6 vòng lặp, họ giảm những thứ này xuống 390 bộ lọc và 1513 nút, đạt được tỷ lệ nén 2.59.

3.3.4 Pruning small filters và filter sketching

Li et al. (2017) mở rộng ý tưởng của magnitude pruning của trọng số cho các bộ lọc bằng cách đề xuất việc loại bỏ các bộ lọc có tổng giá trị tuyệt đối nhỏ nhất trong số các bộ lọc trong một lớp. Nghĩa là, nếu các bộ lọc để tạo ra feature map thứ j là Wj,i ∈ R^(k×k) và m là số lượng feature maps đầu vào, thì magnitude của bộ lọc thứ j được định nghĩa bởi:

sj = ∑(i=1 to m) ||Wj,i||₁ (6)

Khi các sj được tính toán, một tỷ lệ các bộ lọc nhỏ nhất cùng với các feature maps liên quan của chúng và các bộ lọc trong lớp tiếp theo được loại bỏ. Sau khi một lớp được pruning, mạng được fine-tune, và pruning được tiếp tục từng lớp một.

Để kiểm tra phương pháp này, họ thực hiện thí nghiệm trên VGG16 và ResNet56 & 110 trên CIFAR10 và ResNet34 trên ImageNet. Bằng cách phân tích độ nhạy của các lớp thông qua thực nghiệm, họ xác định tỷ lệ pruning phù hợp cho mỗi lớp sẽ không làm giảm độ chính xác đáng kể. Tổng thể, đối với VGG16, họ có thể pruning các tham số 64%. Một tỷ lệ đáng kể của việc pruning này trong các lớp 8 đến 13 bao gồm các bộ lọc nhỏ hơn (2x2 và 4x4), mà họ nhận thấy có thể được pruning 50% mà không giảm độ chính xác. Mức độ pruning cho các mạng khác khiêm tốn hơn, với tỷ lệ pruning tốt nhất cho ResNet-56 và ResNet110 trên CIFAR10 lần lượt là 3.7% và 32.4%, và cho ResNet-34 trên ImageNet là 10.8%.

Họ cũng so sánh phương pháp của họ với phương pháp dựa trên variance (Tiểu mục 3.3.1) và kết luận rằng việc sử dụng thước đo trên cho các bộ lọc hoạt động ít nhất cũng tốt nhưng mà không cần tính toán thêm các feature maps thông qua các mẫu dữ liệu.

Một phương pháp gần đây được gọi là filter sketch cũng nhằm giảm số lượng bộ lọc mà không cần lấy mẫu các ví dụ (Lin et al., 2020a). Ý tưởng chính trong filter sketching là minimize sự khác biệt giữa các covariance của tập hợp bộ lọc gốc và tập hợp giảm. Mặc dù điều này có thể được thực hiện bằng các phương pháp tối ưu hóa, Lin et al. (2020a) sử dụng một thuật toán greedy được gọi là Frequent Direction (Liberty, 2013) hiệu quả hơn.

Lin et al. (2020a) đánh giá phương pháp filter sketch trên GoogleNet, ResNet56 và ResNet110 sử dụng dữ liệu CIFAR10, và trên ResNet50 với dữ liệu ImageNet. Kết quả cho thấy nó hoạt động tốt tương đối với phương pháp để pruning small filters và một phương pháp sử dụng tối ưu hóa để pruning kênh (c/f Tiểu mục 3.3.7) về mặt giảm số lượng tham số mà không mất đáng kể về độ chính xác.

3.3.5 Pruning filters dựa trên geometric median

He et al. (2019) chỉ ra rằng pruning dựa trên magnitude của các bộ lọc giả định rằng có một số bộ lọc nhỏ và rằng sự phân bố của magnitude đủ rộng để phân biệt đầy đủ những bộ lọc đóng góp từ những bộ lọc không đóng góp. Vậy, ví dụ, nếu hầu hết các trọng số nhỏ, người ta có thể kết thúc việc loại bỏ một số lượng đáng kể bộ lọc và nếu hầu hết các bộ lọc có giá trị lớn, không có bộ lọc nào sẽ được loại bỏ, mặc dù có thể có các bộ lọc tương đối nhỏ. Do đó, họ đề xuất một phương pháp dựa trên quan điểm rằng geometric median của các bộ lọc chia sẻ hầu hết thông tin chung trong các bộ lọc khác và do đó một bộ lọc gần nó có thể được bao phủ bởi các bộ lọc khác nếu bị xóa. Tính toán geometric median có thể tốn thời gian, vì vậy họ xấp xỉ tính toán của nó bằng cách giả định rằng một trong các bộ lọc sẽ là geometric mean. Chiến lược pruning của họ là pruning và fine-tune lặp đi lặp lại bằng cách sử dụng một hệ số pruning cố định cho tất cả các lớp.

Họ thực hiện đánh giá so với một số phương pháp bao gồm pruning small filters (Li et al., 2017), ThiNet (Luo et al., 2017), Soft filter pruning (He et al., 2018a), và NISP (Yu et al., 2018). Các phương pháp này được đánh giá trên ResNets được huấn luyện trên dữ liệu CIFAR10 và ImageNet, với tỷ lệ pruning 30 và 40 phần trăm. Nói chung, sự giảm về độ chính xác tương tự qua các phương pháp khác nhau, mặc dù có một giảm đáng kể trong FLOPS khi sử dụng phương pháp geometric median trên ResNet-50 (53.5%) so với các phương pháp khác (ví dụ, ThiNet 36.7%, Soft filter pruning 41%, NISP 44%).

3.3.6 ThiNet và AOFP

Luo et al. (2017) đặt nhiệm vụ pruning như một vấn đề tối ưu hóa và đề xuất một hệ thống ThiNet trong đó mục tiêu là tìm một tập con của các kênh đầu vào có thể xấp xỉ tốt nhất các feature maps đầu ra. Các kênh không trong tập con và các bộ lọc tương ứng của chúng sau đó có thể được loại bỏ. Giải quyết vấn đề tối ưu hóa là thách thức về mặt tính toán, vì vậy ThiNet sử dụng một thuật toán greedy tìm một kênh đóng góp ít nhất, thêm nó vào danh sách sẽ được loại bỏ, và lặp lại quy trình với các kênh còn lại cho đến khi số lượng kênh được chọn bằng số lượng sẽ được pruning. Khi một tập con các bộ lọc để giữ lại được xác định, trọng số của chúng được thu được bằng cách sử dụng least squares để tìm các bộ lọc W minimize (Luo et al., 2017):

∑(i=1 to m) ||Yi - W^T Xi||² (7)

Trong đó Yi là m điểm được lấy mẫu trong các kênh đầu ra và Xi các kênh đầu vào tương ứng của chúng.

Họ đánh giá phương pháp của họ trong hai tập hợp thí nghiệm. Trong tập đầu tiên, họ điều chỉnh VGG16, thay thế các lớp kết nối đầy đủ bằng các lớp global average pooling (GAP), áp dụng nó cho dữ liệu UCSD-Birds và sau đó pruning nó bằng ThiNet, APoZ và phương pháp small filters. Kết quả của họ cho thấy có ít suy giảm về độ chính xác với ThiNet hơn ApoZ, mà lần lượt, tốt hơn phương pháp small filters.

Trong tập hợp thí nghiệm thứ hai của họ, họ sử dụng VGG16 và ResNet50 được huấn luyện trên dữ liệu ImageNet. Đối với VGG16, quy trình của họ bao gồm pruning một lớp và sau đó fine-tuning nhỏ với một epoch huấn luyện với một epoch bổ sung ở cuối mỗi nhóm các lớp tích chập và thêm 12 epoch fine-tuning sau lớp cuối cùng. Với việc sử dụng GAP, ThiNet, giảm số lượng tham số khoảng 94% với chi phí của việc giảm 1% trong độ chính xác Top-1. Đối với ResNet, ThiNet được áp dụng trên hai lớp tích chập đầu tiên của mỗi residual block, giữ nguyên kích thước đầu ra của các block. Sau khi pruning mỗi lớp, một epoch fine-tuning được thực hiện, và 9 epoch được sử dụng để fine-tuning ở cuối. Kết quả cho thấy ThiNet có thể giảm một nửa số lượng tham số với mất 1.87% trong độ chính xác Top-1.

Ding et al. (2019b) đề xuất một phương pháp tương tự như ThiNet, được gọi là Approximated Oracle Filter Pruning (AOFP), nhằm xác định tập con các bộ lọc, mà nếu bị loại bỏ, sẽ có ít ảnh hưởng nhất đến các feature maps trong lớp tiếp theo. Tuy nhiên, trong khi quy trình tìm kiếm được áp dụng trong ThiNet sử dụng một phương pháp greedy bottom up, AOFP áp dụng một tìm kiếm nhị phân top-down trong đó một nửa các bộ lọc trong một lớp được chọn ngẫu nhiên và đặt để được pruning. Tác động của việc loại bỏ các bộ lọc này đến feature map được tạo ra trong lớp tiếp theo được đo và ghi lại đối với mỗi bộ lọc được đặt là pruned. Quy trình này được lặp lại cho các lựa chọn ngẫu nhiên khác nhau, và tác động trung bình mỗi bộ lọc được sử dụng như một dấu hiệu của tác động của việc loại bỏ một bộ lọc. Top 50% các bộ lọc sẽ dẫn đến tác động tồi tệ nhất nếu bị loại bỏ được giữ lại và quy trình được lặp lại trừ khi điều này sẽ dẫn đến một giảm không thể chấp nhận được về độ chính xác. So với ThiNet, và các phương pháp khác, AOFP không yêu cầu tỷ lệ pruning được cố định trước khi pruning một lớp.

AOFP được đánh giá bằng cách pruning AlexNet, VGG và ResNet được huấn luyện trên dữ liệu CIFAR10 và ImageNet. Họ so sánh AOFP với một số phương pháp bao gồm: ThinNet, Network Slimming (Liu et al., 2017), Pruning using Agents (Huang et al., 2018), Online Filter Weakening (Zhou et al., 2018b), NISP (Yu et al., 2018), Optimizing Channel Pruning (He et al., 2017), Structured Probabilistic Pruning (Wang et al., 2017), Autopruner (Luo và Wu, 2018), và ISTA (Ye et al., 2018), với kết quả của họ cho thấy AOFP có khả năng giảm lớn hơn trong FLOPS mà không làm giảm độ chính xác.

3.3.7 Optimizing channel selection với LASSO regression

He et al. (2017) cũng đặt việc lựa chọn kênh như một vấn đề tối ưu hóa. Cho một kênh Y thu được bằng cách áp dụng một bộ lọc Wi cho m kênh đầu vào Xi:

Y = ∑(i=1 to m) Xi W^T_i (8)

Họ định nghĩa nhiệm vụ như một để tối ưu hóa:

argmin_(β,W) (1/2)||Y - ∑(i=1 to c) βi Xi W^T_i||²_F
subject to ||β||₀ ≤ p (9)

Trong đó p chỉ ra số lượng kênh được giữ lại và βi ∈ {0,1} chỉ ra việc giữ lại hoặc loại bỏ một kênh.

Trái ngược với ThiNet, áp dụng một heuristic greedy để giải quyết vấn đề tối ưu hóa này, He et al. (2017) nới lỏng vấn đề từ L₀ đến L₁ regularization và sử dụng LASSO regression để giải quyết:

argmin_(β,W) (1/2)||Y - ∑(i=1 to c) βi Xi W^T_i||²_F + λ||β||₁
subject to ||β||₀ ≤ p (10)

Theo sau việc lựa chọn các kênh họ sử dụng least squares để có được các trọng số sửa đổi theo cách tương tự như phương pháp được áp dụng trong ThiNet.

Họ thực hiện đánh giá thực nghiệm trên VGG16, ResNet50 và một phiên bản của mạng Xception, được huấn luyện trên dữ liệu CIFAR10 và ImageNet. Họ cũng khám phá mức độ mà các mô hình được pruning có thể được sử dụng cho transfer learning bằng cách sử dụng chúng cho nhiệm vụ phát hiện đối tượng PASCAL VOC 2007.

Trong tập hợp thí nghiệm đầu tiên của họ, họ đánh giá phương pháp của họ trên các lớp đơn của VGG16 được huấn luyện trên CIFAR10 mà không có bất kỳ fine-tuning nào, và cho thấy thuật toán của họ duy trì độ chính xác Top-5 tốt hơn so với phương pháp pruning small filters. Họ cũng bao gồm kết quả từ một phương pháp naïve chọn k feature maps đầu tiên và cho thấy rằng, đối với một số lớp (ví dụ conv3_3 trong VGG16), điều này đôi khi hoạt động tốt hơn phương pháp pruning small filters, làm nổi bật một điểm yếu tiềm năng của magnitude-based pruning.

Trong một tập hợp thí nghiệm thứ hai, với VGG16 trên CIFAR10, họ áp dụng phương pháp của họ trên toàn bộ mạng, sử dụng tỷ lệ pruning bespoke cho các lớp và fine-tuning để đạt được cải thiện 2, 4 và 5 lần trong thời gian chạy, nhưng dẫn đến giảm độ chính xác Top-5 lần lượt là 0%, 1%, và 1.7%. So sánh, phương pháp pruning small filters dẫn đến giảm lớn hơn 0.8%, 8.6% và 14.6%.

Các thí nghiệm của họ trên ResNet50 áp dụng tỷ lệ pruning bespoke mỗi lớp, giữ lại 70% các lớp rất nhạy cảm với pruning, và 30% các lớp ít nhạy cảm hơn. Kết quả độ chính xác Top-5 trên ImageNet cho thấy cải thiện hai lần trong thời gian chạy với chi phí của việc giảm 1.4% về độ chính xác so với độ chính xác baseline 92.2%, trong khi kết quả trên mạng Xception cho thấy giảm 1% về độ chính xác từ baseline 92.8%.

Thí nghiệm về việc sử dụng phiên bản pruned của mô hình VGG16 trên nhiệm vụ benchmark phát hiện đối tượng PASCAL VOC 2007 dẫn đến tăng 2 lần về tốc độ với giảm 0.4% về average precision.

4 PRUNING DỰA TRÊN SIMILARITY VÀ CLUSTERING

Cho rằng các mạng nơron có thể bị over-parametrised, có thể có các trọng số hoặc bộ lọc trùng lặp thực hiện các chức năng tương tự và có thể được loại bỏ mà không ảnh hưởng đến độ chính xác (Sussmann, 1992; RoyChowdhury et al., 2017; Han et al., 2016b; Son et al., 2018; Li et al., 2019e).

RoyChowdhury et al. (2017) khám phá giả thuyết này bằng cách sử dụng tích vô hướng của hai bộ lọc (hoặc ma trận trọng số) như một thước đo tương tự. Thuật toán pruning của họ bao gồm việc nhóm các bộ lọc tương tự và sau đó thay thế mỗi nhóm bộ lọc bằng bộ lọc trung bình của chúng. Họ thực hiện thí nghiệm với cả multilayer perceptron (MLP) và CNN cho dữ liệu CIFAR10. MLP có ba lớp: hai lớp đầu tiên là các lớp kết nối đầy đủ và lớp thứ ba là một lớp softmax với 10 nút đại diện cho lớp CIFAR10. CNN có hai lớp tích chập, mỗi lớp tiếp theo bởi một ReLU, và một lớp max pooling 2x2. Các lớp tích chập được tiếp theo bởi hai lớp kết nối đầy đủ để thực hiện phân loại. Trong cả hai trường hợp, lớp đầu tiên được thay đổi với 100, 500 và 1000 đơn vị (nút hoặc bộ lọc) để khám phá các tác động của việc tăng over parametrisation. Phát hiện chính của họ là có xu hướng lớn hơn nhiều cho các trọng số/bộ lọc tương tự xảy ra trong MLP hơn trong CNN. Kết quả là, có cơ hội lớn hơn để sử dụng tương tự như một cơ sở để pruning MLP hơn để pruning CNN. Tuy nhiên, kết quả của họ gợi ý rằng một thuật toán pruning dựa trên similarity tốt hơn trong việc giữ lại độ chính xác so với việc sử dụng phương pháp small filters.

Ayinde et al. (2019) cũng phát triển một phương pháp sử dụng clustering để xác định các bộ lọc tương tự. Họ cũng áp dụng tích vô hướng như một thước đo tương tự, nhưng sử dụng một phương pháp agglomerative hierarchical clustering để nhóm các bộ lọc tương tự và thay thế các bộ lọc bằng cách chọn ngẫu nhiên một bộ lọc từ mỗi cluster. Họ thực hiện các thí nghiệm khác nhau với VGG16 trên CIFAR10 và ResNet34 trên ImageNet. Đối với thử nghiệm trên VGG16 với dữ liệu CIFAR10, họ cho thấy rằng, khi một giá trị tối ưu cho ngưỡng tương tự được xác định, phương pháp của họ đạt được cả tỷ lệ pruning tốt hơn và độ chính xác hơn so với các phương pháp khác, bao gồm pruning of small filters, Network Slimming, một phương pháp sử dụng regularization để xác định các kênh yếu (Liu et al., 2017), và try-and-learn, một phương pháp sử dụng phân tích độ nhạy (c/f Section 5).

5 CÁC PHƯƠNG PHÁP PHÂN TÍCH ĐỘ NHẠY

Mục tiêu chính của pruning là loại bỏ các trọng số, bộ lọc và kênh có ít ảnh hưởng nhất đến độ chính xác của một mô hình. Các phương pháp dựa trên magnitude và similarity được mô tả ở trên giải quyết mục tiêu này một cách ngầm định bằng cách sử dụng các thuộc tính của trọng số, bộ lọc và kênh có thể ảnh hưởng đến độ chính xác. Ngược lại, phần này trình bày các phương pháp sử dụng phân tích độ nhạy để mô hình hóa tác động của việc nhiễu loạn và loại bỏ trọng số, bộ lọc và kênh đối với hàm loss.

Tiểu mục 5.1 mô tả các phương pháp đánh giá tầm quan trọng của các kênh và Tiểu mục 5.2 đến 5.4 trình bày sự phát triển của một dòng nghiên cứu xấp xỉ tác động của việc nhiễu loạn các trọng số đối với hàm loss bằng cách sử dụng Chuỗi Taylor, từ nghiên cứu sớm nhất phát triển các phương pháp cho MLP đến nghiên cứu gần đây hơn về các phương pháp pruning CNN.

5.1 Pruning bằng cách đánh giá tầm quan trọng của các nút và kênh

Skeletonization, một phương pháp được đề xuất bởi Mozer và Smolensky (1988), là một trong những phương pháp sớm nhất để pruning mạng nơron. Để tính toán tác động của việc loại bỏ các nút, Skeletonization giới thiệu khái niệm attentional strength để biểu thị tầm quan trọng của các nút khi tính toán các activation. Cho attentional strengths của các nút, ai, đầu ra yj từ nút j, được định nghĩa bởi:

yj = f(∑i wji ai yi) (11)

Trong đó f được giả định là hàm sigmoid. Tầm quan trọng của một nút ri sau đó được định nghĩa theo sự khác biệt trong loss khi ai được đặt bằng không và khi nó được đặt bằng một và có thể được xấp xỉ bởi đạo hàm của loss đối với attentional strength ai:

ri = L|ai=0 - L|ai=1 ≈ ∂L/∂ai |ai=1 (12)

Thông qua thực nghiệm, Mozer và Smolensky (1988) phát hiện ra rằng linear loss hoạt động tốt hơn quadratic loss vì sự khác biệt giữa các đầu ra và targets nhỏ sau huấn luyện. Ngoài ra, họ nhận thấy rằng ∂L(t)/∂ai không ổn định theo thời gian, vì vậy họ sử dụng một thước đo trung bình có trọng số để tính toán tầm quan trọng r̂i:

r̂i(t+1) = 0.8r̂i(t) + 0.2∂L(t)/∂ai (13)

Mozer và Smolensky (1988) trình bày một số thí nghiệm nhỏ nhưng rất thú vị. Những thí nghiệm này bao gồm việc tạo ra các ví dụ trong đó đầu ra có tương quan với bốn đầu vào, A, B, C, và D, với tương quan đầy đủ trên A và giảm đến không tương quan với D. Họ cung cấp điều này như đầu vào cho một mạng với một nút ẩn và sau huấn luyện họ quan sát rằng các trọng số từ các đầu vào đến nút ẩn theo các tương quan, mặc dù thước đo liên quan chỉ cho thấy nút đầu vào A là quan trọng, cung cấp một số sự an tâm rằng thước đo khác với các trọng số. Trong một ví dụ khác, họ phát triển một mạng để mô hình hóa một mạng multiplexor 4-bit, có 4 bit làm đầu vào và hai bit để điều khiển bit nào trong 4 bit được đầu ra. Họ thử hai cấu hình mạng: trong cấu hình đầu tiên, họ sử dụng 4 nút ẩn và trong cấu hình thứ hai họ sử dụng 8 nút ẩn và sử dụng skeletonization để giảm kích thước của nó xuống 4 nút ẩn. Khi giới hạn huấn luyện đến 1000 epoch, họ thấy rằng bắt đầu với 4 nút ẩn ban đầu, dẫn đến thất bại hội tụ trong 17% các trường hợp, trong khi bắt đầu với 8 nút lớp ẩn tiếp theo bởi skeletonization hội tụ trong tất cả các trường hợp và, cũng giữ lại độ chính xác. Điều này dường như là một trong những minh chứng đầu tiên rằng, để bắt đầu, có thể cần thiết để overparameterize một mạng để tìm winning lotteries.

Ý tưởng đánh giá tầm quan trọng của các nút này đã được mở rộng đến các kênh bởi hai phương pháp, cụ thể là Network Slimming (Liu et al., 2017) và Sparse Structure Selection (SSS) (Huang và Wang, 2018), học một thước đo tầm quan trọng như một phần của quy trình huấn luyện. Cả hai đều sử dụng một tham số γ cho mỗi kênh (tương tự như attentional strength) scale đầu ra của một kênh. Cho một hàm loss L, loss mới L' được định nghĩa với một thuật ngữ regularization bổ sung trên các yếu tố scale γ:

L' = L + λ∑u g(γ) (14)

Trong đó hàm g được chọn là norm L1 để giảm γ về phía không (như trong Lasso regression).

Hai phương pháp khác nhau trong cách họ triển khai quy trình huấn luyện nhằm minimize L' với Network Slimming tận dụng việc sử dụng các lớp batch normalization đôi khi có mặt sau các lớp tích chập trong khi SSS triển khai một quy trình tổng quát hơn không giả định sự hiện diện của các lớp batch, và cho phép sử dụng các yếu tố scale cho các khối (như residual và inception blocks) có thể cho phép giảm độ sâu của một mạng.

Huang và Wang (2018) thực nghiệm với SSS trên dữ liệu CIFAR-10, CIFAR-100, và ImageNet trên VGG16, và ResNet. Đối với CIFAR10, SSS có thể giảm số lượng tham số trong VGG16 30% mà không mất độ chính xác. Đối với ResNet-164, nó có thể đạt được tăng tốc 2.5 lần với chi phí của mất 2% về độ chính xác cho CIFAR-10 và CIFAR-100.

Đối với VGG16 trên ImageNet, SSS có thể giảm FLOPS khoảng 75%, mặc dù giảm tham số là tối thiểu, điều này phù hợp với các phương pháp khác cho số lượng lớn tham số trong các lớp kết nối đầy đủ trong VGG16. Trên ResNet50, SSS đạt được giảm 15% FLOPS với chi phí của giảm 0.1% về độ chính xác Top-1.

5.2 Pruning Weights với OBD và OBS

Một số nghiên cứu sử dụng chuỗi Taylor để xấp xỉ tác động của nhiễu loạn trọng số đối với hàm loss (LeCun et al., 1990; Hassibi et al., 1993b; Wang et al., 2019). Cho thay đổi trong trọng số ΔW, một xấp xỉ Chuỗi Taylor của thay đổi trong loss ΔL có thể được phát biểu như (Bishop, 2006):

ΔL = (∂L^T/∂W)ΔW + (1/2)ΔW^T HΔW + O(||dW||³) (15)

Trong đó H là một ma trận Hessian có các phần tử là đạo hàm bậc hai của loss đối với các trọng số:

Hij = ∂²L/(∂wi∂wj) (16)

Hầu hết các phương pháp áp dụng xấp xỉ này giả định rằng thuật ngữ bậc ba là không đáng kể. Trong Optimal Brain Damage (OBD), LeCun et al. (1990), cũng giả định rằng thuật ngữ bậc nhất có thể được bỏ qua cho rằng mạng sẽ được huấn luyện để đạt được một cực tiểu cục bộ, dẫn đến một xấp xỉ quadratic đơn giản:

ΔL = (1/2)ΔW^T HΔW (17)

Cho số lượng lớn trọng số, tính toán Hessian tốn kém về mặt tính toán, vì vậy họ cũng giả định rằng thay đổi trong loss có thể được xấp xỉ bởi các phần tử đường chéo của Hessian, dẫn đến thước đo saliency sau sk của một trọng số wk:

sk = (Hkk w²k)/2 (18)

Trong đó các đạo hàm bậc hai, Hkk được tính toán theo cách tương tự như cách gradient được tính toán trong backpropagation.

Hassibi et al. (1993b) lập luận rằng việc bỏ qua các phần tử không đường chéo của một Hessian là một giả định mạnh, và đề xuất một phương pháp pruning thay thế, được gọi là Optimal Brain Surgeon (OBS), nhằm tính đến tất cả các phần tử của một Hessian (Hassibi et al., 1993b,a).

Sử dụng một vector đơn vị, em, để biểu thị việc lựa chọn trọng số thứ m là trọng số sẽ được pruning, OBS tái đặt pruning như một nhiệm vụ tối ưu hóa dựa trên ràng buộc:

min_dw (1/2)dw^T H dw
subject to e^T_m · dw + dwm = 0 (19)

Công thức hóa điều này với một nhân tử Lagrangian, λ, nhiệm vụ là minimize:

(1/2)dw^T · H · dw + λ(e^T_m · dw + dwm) (20)

Bằng cách lấy đạo hàm và sử dụng ràng buộc trên, họ cho thấy saliency, sk của trọng số wk có thể được tính toán bằng:

sk = (1/2) w²k/[H^(-1)]k,k (21)

Họ cho thấy rằng trên bài toán XOR, được mô hình hóa bằng một mạng MLP với 2 đầu vào, 2 nút lớp ẩn và một đầu ra, OBS tốt hơn trong việc phát hiện các trọng số chính xác để xóa so với OBD hoặc magnitude pruning. Họ cũng cho thấy rằng OBS có thể giảm đáng kể số lượng trọng số cần thiết cho các mạng nơron được huấn luyện trên các bài toán Monk (Thrun, 1991) và cho NetTalk (Sejnowski và Rosenberg, 1987), một trong những ứng dụng cổ điển của mạng nơron, nó có thể giảm số lượng trọng số cần thiết từ 18000 xuống 1560.

5.3 Pruning Feature Maps với First-Order Taylor Approximations

Các phương pháp được mô tả trong Tiểu mục 5.2 tập trung vào tác động của việc loại bỏ trọng số trong một mạng kết nối đầy đủ. Molchanov et al. (2016) giới thiệu một phương pháp sử dụng chuỗi Taylor để xấp xỉ những gì xảy ra nếu một feature map bị loại bỏ. Trái ngược với OBD và OBS, giả định rằng thuật ngữ bậc nhất có thể được bỏ qua, họ áp dụng một xấp xỉ bậc nhất, bỏ qua các thuật ngữ bậc cao hơn, chủ yếu trên cơ sở độ phức tạp tính toán. Sử dụng xấp xỉ bậc nhất có vẻ kỳ lạ cho lập luận thuyết phục về việc bỏ qua các thuật ngữ này; tuy nhiên họ lập luận rằng mặc dù gradient bậc nhất có xu hướng về không, giá trị kỳ vọng của thay đổi trong loss tỷ lệ với variance, không phải là không và là một thước đo ổn định khi một giải pháp cục bộ được đạt đến. Cho một feature map với N phần tử Yi,j, xấp xỉ bậc nhất bằng cách sử dụng chuỗi Taylor dẫn đến thước đo sau của thay đổi tuyệt đối trong loss (Molchanov et al., 2016):

ΔL(Y) = |1/N ∑i,j (∂L/∂Yi,j)Yi,j| (22)

Quy mô của thước đo này sẽ thay đổi trong các lớp khác nhau, và do đó họ áp dụng normalization L2 trong mỗi lớp. Quy trình pruning họ áp dụng bao gồm việc chọn một feature map bằng thước đo, pruning nó, và sau đó fine-tuning mạng trước khi lặp lại quy trình cho đến khi một điều kiện dừng, tính đến nhu cầu giảm số lượng FLOPS trong khi duy trì độ chính xác, được đáp ứng. Các thí nghiệm của họ tiết lộ một số phát hiện thú vị:

1. Từ các thí nghiệm trên VGG16 và AlexNet trên dữ liệu UCSD-Birds và Oxford-Flowers, họ cho thấy rằng các feature maps được chọn bởi tiêu chí của họ tương quan đáng kể gần gũi hơn với những feature maps được chọn bởi một phương pháp oracle so với OBD và APoZ. Trên dữ liệu ImageNet, họ thấy rằng OBD tương quan tốt nhất khi AlexNet được sử dụng.

2. Trong các thí nghiệm về transfer learning, trong đó họ fine-tune VGG16 trên dữ liệu UCSD-Birds, họ trình bày kết quả cho thấy phương pháp của họ hoạt động tốt hơn APoZ và OBD khi số lượng tham số được pruning tăng. Trong một thí nghiệm trong đó AlexNet được fine-tune cho dữ liệu Oxford Flowers, họ cho thấy cả phương pháp của họ và OBD hoạt động tốt hơn APoZ.

3. Trong một ví dụ nổi bật về lợi ích tiềm năng của pruning, họ minh chứng phương pháp của họ trên một mạng để nhận dạng cử chỉ tay yêu cầu hơn 37 GFLOPs cho một suy luận đơn lẻ nhưng chỉ yêu cầu 3 GFLOPs sau pruning, tất cả với việc giảm 2.6% về độ chính xác.

Trong một xuất bản tiếp theo, Molchanov et al. (2019) thừa nhận một số hạn chế của phương pháp trên, cụ thể là việc giả định rằng tất cả các lớp có cùng tầm quan trọng không hoạt động cho skip connections (được sử dụng trong kiến trúc ResNet) và việc đánh giá tác động của thay đổi trong feature maps dẫn đến tăng yêu cầu bộ nhớ. Do đó họ đề xuất một công thức thay thế, cũng sử dụng xấp xỉ chuỗi Taylor, nhưng dựa trên ước tính squared loss do việc loại bỏ tham số thứ m:

(ΔLm)² = |gmwm + (1/2)wmHmW|² (23)

Trong đó gm là gradient bậc nhất và Hm là hàng thứ m của ma trận Hessian. Thước đo tầm quan trọng của một bộ lọc sau đó được thu được bằng cách tổng hợp các đóng góp do mỗi tham số trong một bộ lọc.

Thuật toán pruning được sử dụng tiến hành như sau. Trong mỗi epoch, họ sử dụng một số lượng cố định mini-batches để ước tính tầm quan trọng của mỗi bộ lọc và sau đó, dựa trên tầm quan trọng của chúng, một số lượng xác định trước các bộ lọc bị loại bỏ. Mạng sau đó được fine-tune, và quy trình được lặp lại cho đến khi một mục tiêu pruning, như số lượng bộ lọc mong muốn hoặc một giới hạn cho một sự giảm chấp nhận được về độ chính xác được đạt đến.

Molchanov et al. (2019) thực hiện các thí nghiệm ban đầu trên các phiên bản của LeNet và ResNet trên dữ liệu CIFAR10, sử dụng cả xấp xỉ bậc hai và bậc nhất (trong phương trình 23) và cho rằng kết quả từ cả hai tương quan tốt với một phương pháp oracle, họ sử dụng thước đo bậc nhất hiệu quả hơn đáng kể để tính toán.

Trong các thí nghiệm với các phiên bản của ResNet, VGG, và DenseNet trên dữ liệu ImageNet, họ xem xét tác động của việc sử dụng thước đo tầm quan trọng tại các điểm trước và sau các lớp batch normalization, và kết luận rằng tùy chọn sau dẫn đến tương quan lớn hơn với một phương pháp oracle. Kết quả từ phương pháp của họ cho thấy nó hoạt động đặc biệt tốt trên pruning ResNet-50 và ResNet-34, vượt trội so với kết quả từ ThiNet và NISP. Kết quả được báo cáo cho các mạng khác cũng ấn tượng, với phương pháp của họ có thể pruning 76% tham số trong VGG với mất 0.19% về độ chính xác và có thể giảm số lượng tham số trong DenseNet 43% với chi phí của giảm 0.29% về độ chính xác.

5.4 Pruning Feature Maps với Second-Order Taylor Approximations

Các phương pháp bậc nhất được mô tả ở trên giả định tương tác tối thiểu qua các kênh và bộ lọc. Phần này tóm tắt các phương pháp pruning gần đây nhằm tính đến tác động của các phụ thuộc tiềm năng giữa các kênh và bộ lọc.

Trong một phương pháp được gọi là EigenDamage, cũng sử dụng xấp xỉ chuỗi Taylor, Wang et al. (2019) xem xét lại các giả định được đưa ra bởi OBD và OBS khi xấp xỉ Hessian. Để thúc đẩy phương pháp của họ, họ bắt đầu bằng việc minh họa rằng mặc dù OBS tốt hơn OBD khi pruning một trọng số tại một thời điểm, nó không nhất thiết vượt trội khi pruning nhiều trọng số cùng một lúc. Điều này chủ yếu vì OBS không mô hình hóa chính xác tác động của việc loại bỏ nhiều trọng số, đặc biệt khi chúng có tương quan. Để tránh vấn đề này, họ sử dụng một Fisher Information Matrix để xấp xỉ Hessian và sau đó họ sử dụng một phương pháp, được đề xuất bởi Grosse và Martens (2016), để biểu diễn một Fisher Matrix bằng một Kronecker Factored Eigenbasis (KFE). Việc tái tham số hóa này cho phép pruning được thực hiện trong một không gian mới trong đó Fisher Matrix gần như đường chéo. Pruning có thể được thực hiện bằng cách đầu tiên ánh xạ các trọng số đến một không gian KFE trong đó chúng gần như độc lập, và sau đó ánh xạ ngược kết quả đến không gian ban đầu.

EigenDamage được đánh giá trên VGG, và ResNet trên dữ liệu CIFAR10, CIFAR100 và Tiny-ImageNet. Các thí nghiệm được thực hiện với one-shot pruning trong đó fine-tuning được thực hiện ở cuối và với iterative pruning trong đó fine-tuning được thực hiện sau mỗi chu kỳ. Trong cả hai trường hợp, kết quả cho thấy EigenDamage vượt trội so với các phiên bản được điều chỉnh của OBD, OBS và Network Slimming (Liu et al., 2017).

Peng et al. (2019) cũng sử dụng xấp xỉ chuỗi Taylor để phát triển một phương pháp Collaborative Channel Pruning (CCP) dựa trên một thước đo tác động của một sự kết hợp các kênh. Cho một mask b, trong đó bi = 1, chỉ ra việc giữ lại một kênh và bi = 0, chỉ ra một kênh sẽ được pruning, họ đặt nhiệm vụ như một để tìm bi minimize loss L:

L(b;W) = L(W) + ∑(i=1 to co)(bi - 1)g^T_i wi + (1/2)∑(i=1,j=1 to co)(bi - 1)(bj - 1)w^T_i Hi,j wj (24)

Trong đó gi là đạo hàm bậc nhất của loss đối với các trọng số trong kênh đầu ra thứ i, Hi,j là Hessians, và co biểu thị số lượng kênh đầu ra.

Bằng cách đặt ui = g^T_i wi và si,j = (1/2)w^T_i Hi,j wj phương trình trên có thể được viết như vấn đề tối ưu hóa quadratic 0-1 sau (Peng et al., 2019):

min_bi ∑(i=1 to co) ui(bi - 1) + ∑(i=1,j=1 to co) si,j(bi - 1)(bj - 1)
subject to: ||b||₀ = p and bi ∈ {0,1} (25)

Trong đó p biểu thị số lượng kênh sẽ được giữ lại trong một lớp. Họ lưu ý rằng các gradient gi và do đó ui có thể được tính toán trong thời gian tuyến tính. Tuy nhiên, cho độ phức tạp của việc tính toán các ma trận Hessian, họ rút ra các xấp xỉ bậc nhất cho các hàm loss, mà họ áp dụng khi tính toán si,j. Để giải quyết vấn đề tối ưu hóa quadratic, họ nới lỏng ràng buộc đến bi ∈ [0,1] và sử dụng một phương pháp quadratic programming để tìm bi được sử dụng để chọn top p kênh để giữ lại. Họ áp dụng quy trình tối ưu hóa trên mỗi lớp để có được các mask bi, sử dụng những thứ này để pruning và sau đó thực hiện fine-tuning ở cuối.

Một đánh giá thực nghiệm của CCP được thực hiện bằng cách pruning các mô hình ResNet được huấn luyện trên dữ liệu CIFAR10 và ImageNet, và kết quả được so sánh với một số phương pháp bao gồm: pruning small filters, ThinNet, optimizing channel pruning, Soft Filter pruning (He et al., 2018a), NISP (Yu et al., 2018) và AutoML (He et al., 2018b). Đối với CIFAR10, các thí nghiệm được thực hiện với tỷ lệ pruning 35% và 40%, và trong mỗi trường hợp, CCP có sự giảm nhỏ hơn về độ chính xác (0.04% và 0.08% tương ứng) so với các phương pháp khác, ngoại trừ phương pháp pruning small filters dẫn đến một cải thiện nhỏ về độ chính xác (0.02%). Tuy nhiên, phương pháp pruning small filters có giảm thấp hơn nhiều trong FLOPS (27.6%) so với CCP (52.6%). Kết quả cho ImageNet cho thấy, đối với các giảm tương tự trong FLOPS, CCP có ít sự giảm về độ chính xác hơn so với các phương pháp khác.

Đáng lưu ý, giống như EigenDamage, CCP có thể có được kết quả tốt mà không cần một quy trình lặp sử dụng fine-tuning sau khi pruning mỗi lớp.

6 MỘT TÀI NGUYÊN ĐỂ SO SÁNH CÁC KẾT QUẢ ĐƯỢC CÔNG BỐ

Như các phần trên mô tả, các nghiên cứu trước đó về pruning báo cáo kết quả trên các bộ dữ liệu, kiến trúc và phương pháp khác nhau đã phát triển theo thời gian, làm cho việc so sánh kết quả qua các nghiên cứu khác nhau trở nên khó khăn. Khảo sát cung cấp một tài nguyên dưới dạng bảng pivot có thể được sử dụng bởi cộng đồng để khám phá hiệu suất được báo cáo của hơn 50 phương pháp trên các kiến trúc và dữ liệu khác nhau. Phụ lục D cho thấy có bao nhiều lần mỗi sự kết hợp của dữ liệu và kiến trúc đã được sử dụng, chỉ ra sự đa dạng rộng rãi của các so sánh có thể.

Để minh họa việc sử dụng tài nguyên, chúng tôi sử dụng nó để so sánh các kết quả được báo cáo trên hai sự kết hợp của kiến trúc và dữ liệu mà có một số lượng đáng kể so sánh qua các phương pháp pruning khác nhau, cụ thể là: (i) AlexNet và ResNet50 trên ImageNet và (ii) ResNet56 và VGG16 trên CIFAR10. Hình 4 cho thấy kết quả được báo cáo về mặt sự giảm trong độ chính xác Top-1, và phần trăm giảm trong FLOPS hoặc tham số trong đó các nhãn được sử dụng cho các phương pháp pruning từ các nguồn chính, với các hậu tố phản ánh các biến thể trong phương pháp pruning được sử dụng. Các quan sát chính là:

1. Đối với AlexNet trên ImageNet, AOFP-B2 đạt được giảm 41% FLOPS với tăng 0.46% về độ chính xác và Dyn Surg (Guo et al., 2016), SSR-L (Lin et al., 2020d) và NeST (Dai et al., 2019) đạt được hơn 93% giảm tham số mà không mất độ chính xác. Các phương pháp khác làm giảm độ chính xác không nhất thiết dẫn đến giảm lớn hơn về tham số.

2. So với AlexNet, khó prune ResNet50 trên ImageNet hơn, mặc dù AOFP-C1 đạt được giảm 33% FLOPS mà không ảnh hưởng đến độ chính xác. Khi độ chính xác bị ảnh hưởng, có các phương pháp cho thấy giảm đáng kể về tham số. Những phương pháp này bao gồm KSE và ThinNet với giảm tham số 78% và 95%, với giảm 0.64%, và 0.84% về độ chính xác, tương ứng.

3. Khi pruning ResNet56 trên dữ liệu CIFAR10, các phương pháp KSE và SFP-NFT cho thấy giảm FLOPS 60% và 28% mà không làm giảm độ chính xác. Đối với VGG16 trên CIFAR10, AOFP, PF EC và NetSlimming dẫn đến giảm 75%, 63%, và 51% trong flops tương ứng mà không giảm độ chính xác. Đối với cả hai mạng, có vẻ khó để đạt được thêm giảm (ngoài KSE và AOFP) ngay cả khi làm giảm độ chính xác.

4. Các biểu đồ cho thấy một số phương pháp có thể giảm FLOPS và tham số mà không làm giảm độ chính xác và hỗ trợ khả năng tổng quát hóa (ví dụ, AOFP, SFP, NetSlimming), mặc dù làm giảm độ chính xác một chút đôi khi có thể dẫn đến giảm đáng kể hơn trong FLOPS và tham số.

5. Khi nhìn vào kết quả trong các phương pháp, có thể xác nhận kỳ vọng của chúng ta rằng làm giảm độ chính xác có thể dẫn đến giảm lớn hơn về tham số và FLOPS (ví dụ, xem kết quả cho Filter Sketch và FPGM trong Hình 4(b)). Tuy nhiên, sự đánh đổi này không rõ ràng khi xem xét kết quả qua các phương pháp pruning khác nhau.

6. Mặc dù AOFP hoạt động tốt trong việc giữ lại độ chính xác cho ba trong bốn trường hợp, nói chung, hiệu suất của các phương pháp thay đổi tùy thuộc vào kiến trúc và bộ dữ liệu.

7 KẾT LUẬN VÀ NGHIÊN CỨU TƯƠNG LAI

Bài báo này đã trình bày một khảo sát về các phương pháp pruning mạng nơron, tập trung vào các phương pháp dựa trên magnitude pruning, sử dụng similarity và clustering, và các phương pháp dựa trên phân tích độ nhạy.

Các phương pháp magnitude-based pruning đã phát triển từ việc loại bỏ các trọng số nhỏ trong MLP đến các phương pháp pruning bộ lọc và kênh dẫn đến giảm đáng kể kích thước của mạng sâu. Phạm vi các phương pháp được phát triển bao gồm: (i) những phương pháp phụ thuộc dữ liệu và sử dụng các ví dụ để đánh giá mức độ liên quan của các kênh đầu ra, (ii) các phương pháp độc lập với dữ liệu, đánh giá đóng góp của các bộ lọc và kênh trực tiếp, và (iii) các phương pháp sử dụng thuật toán tối ưu hóa để tìm các bộ lọc xấp xỉ các kênh.

Các phương pháp dựa trên phân tích độ nhạy trong suốt nhất ở chỗ chúng dựa trên xấp xỉ loss do thay đổi đối với một mạng. Sự phát triển của các phương pháp dựa trên xấp xỉ chuỗi Taylor đại diện cho dòng nghiên cứu chính trong danh mục phương pháp này. Các nghiên cứu khác nhau đã áp dụng các giả định khác nhau để làm cho tính toán của xấp xỉ Taylor khả thi. Trong một trong những nghiên cứu đầu tiên, phương pháp OBD giả định một ma trận Hessian đường chéo, bỏ qua cả gradient bậc nhất và gradient bậc hai không đường chéo. Điều này được theo sau bởi OBS, một phương pháp nhằm tính đến các phần tử không đường chéo của Hessian nhưng đã được chỉ ra là gặp khó khăn khi pruning nhiều trọng số có tương quan. Phương pháp EigenDamage nhằm tính đến tương quan tốt hơn bằng cách xấp xỉ Hessian với một Fisher Information Matrix và sử dụng một reparameterization đến một không gian mới trong đó các trọng số gần như độc lập. Trong một phương pháp thay thế, phương pháp Collaborative Channel Pruning (CCP) đặt nhiệm vụ pruning như một vấn đề quadratic programming. Molchanov et al. (2016) phát triển một phương pháp dựa trên xấp xỉ bậc nhất, lập luận rằng variance trong loss, khi huấn luyện tiếp cận một giải pháp cục bộ, là một chỉ báo ổn định và cung cấp một thước đo tốt về tầm quan trọng của các bộ lọc. Trái ngược với hầu hết các phương pháp khác áp dụng pruning từng lớp một với fine tuning sau mỗi lớp, cả EigenDamage và CCP cho thấy có thể có được kết quả tốt với one-shot pruning tiếp theo bởi fine-tuning. Ba phương pháp gần đây này đều cho thấy kết quả tốt trên các mạng và bộ dữ liệu quy mô lớn, mặc dù các so sánh thực nghiệm trực tiếp giữa chúng vẫn chưa được công bố. Khảo sát cũng tìm thấy hai phương án thay thế cho việc sử dụng xấp xỉ chuỗi Taylor: một phương pháp nhằm học bộ lọc nào để pruning (Huang et al., 2018) và một phương pháp dựa trên việc sử dụng multi-armed bandits (Ameen và Vadera, 2020), cả hai đều có tiềm năng khám phá các con đường nghiên cứu mới về các phương pháp pruning.

Khảo sát tiết lộ một số kết quả tích cực về Lottery Hypothesis: Lotteries dường như hoạt động tốt trong transfer learning, và lotteries tồn tại cho các nhiệm vụ như NLP và cho các kiến trúc như LSTM. Lotteries thậm chí có vẻ độc lập với loại optimizer được sử dụng trong huấn luyện. Phần lớn nghiên cứu hiện tại về lotteries dựa trên mạng sâu, nhưng thú vị là một trong những bài báo sớm nhất trong lĩnh vực này chứng minh nhu cầu overparameterize một mạng feedforward nhỏ để mô hình hóa một multiplexor 4-bit. Do đó, có thể chứng minh được hữu ích để khám phá các thuộc tính của lotteries trên các vấn đề nhỏ hơn cũng như các mạng lớn hơn ngày nay. Sự tồn tại của lotteries tốt có vẻ phụ thuộc vào quy trình fine-tuning được áp dụng và một quan sát thú vị, thách thức một số nghiên cứu thực nghiệm được báo cáo, là ngay cả random pruning có thể đạt được kết quả tốt sau fine-tuning (Mittal et al., 2019), vì vậy nghiên cứu sâu hơn về cách các trọng số còn lại bù đắp cho những trọng số bị loại bỏ, có thể dẫn đến những hiểu biết mới. Mặc dù các nghiên cứu về lotteries cung cấp hiểu biết có giá trị, nghiên cứu sâu hơn về phần cứng và thư viện chuyên dụng cần thiết để các phương pháp pruning trọng số riêng lẻ trở nên thực tế (Elsen et al., 2020).

Khảo sát tìm thấy ít nghiên cứu nhất về các phương pháp sử dụng similarity và clustering để phát triển các phương pháp pruning. Một phương pháp sử dụng thước đo cosine similarity kết luận rằng nó phù hợp hơn cho MLP hơn CNN, trong khi một phương pháp sử dụng agglomerative clustering của các bộ lọc dẫn đến giảm lên đến 3 lần trên ResNet khi nó được áp dụng cho ImageNet. Những kết quả này gợi ý có giá trị trong việc phát triển một hiểu biết lý thuyết hơn về tương đương chức năng của các lớp mạng sâu khác nhau, tương tự như các nghiên cứu về tương đương của MLP (Kurková và Kainen, 1994).

Cho các phương pháp pruning khác nhau, một số có thể bổ sung, và có một số bằng chứng rằng việc kết hợp chúng có thể dẫn đến nén mạng sâu hơn. Ví dụ, He et al. (2017) trình bày kết quả cho thấy việc kết hợp phương pháp của họ dựa trên việc sử dụng Lasso regression với factorization dẫn đến thêm lợi ích, và Han et al. (2016b) sử dụng một pipeline của magnitude pruning, clustering và Huffman coding để tăng mức độ nén có thể đạt được.

Một trong những thách thức trong việc hiểu các đánh giá thực nghiệm được báo cáo trong các bài báo được khảo sát là, khi các kiến trúc deep learning mới đã phát triển và khi các phương pháp mới đã được công bố, các so sánh được thực hiện đã phát triển. Do đó khảo sát đã tổng hợp các kết quả được công bố của hơn 50 phương pháp cho nhiều bộ dữ liệu và kiến trúc đa dạng có sẵn như một tài nguyên cho các nhà nghiên cứu khác. Phần 6 trình bày một so sánh dựa trên dữ liệu này, làm nổi bật các phương pháp hoạt động tốt cho các kiến trúc khác nhau trên dữ liệu ImageNet và CIFAR10. So sánh các kết quả được công bố gợi ý rằng có thể đạt được giảm đáng kể cho AlexNet, ResNet và VGG, mặc dù không có phương pháp đơn lẻ nào là tốt nhất, và khó prune ResNet hơn so với các kiến trúc khác. Người ta có thể đưa ra giả thuyết rằng điều này là do việc sử dụng skip connections làm cho nó tối ưu hơn, mặc dù điều này cần được khám phá. Tương tự, cho rằng các phương pháp khác nhau có vẻ tốt nhất cho các kiến trúc khác nhau, đáng để nghiên cứu và phát triển các phương pháp cho các kiến trúc cụ thể. Dữ liệu cũng tiết lộ rằng có các đánh giá hạn chế trên các mạng khác như InceptionNet, DenseNet, SegNet, FCN32 và các bộ dữ liệu như CIFAR100, Flowers102, CUB200-2011 (xem Phụ lục D). Một đánh giá độc lập toàn diện của các phương pháp bao gồm xem xét các vấn đề được nêu ra bởi Lottery hypothesis qua một phạm vi rộng hơn về dữ liệu và kiến trúc sẽ là một tiến bộ hữu ích trong lĩnh vực này.

Kết luận, khảo sát này đã trình bày các hướng nghiên cứu chính trong pruning mạng nơron bằng cách tóm tắt cách lĩnh vực này đã tiến bộ từ các thuật toán sớm tập trung vào các mạng kết nối đầy đủ nhỏ đến các mạng nơron sâu lớn hơn nhiều ngày nay. Khảo sát đã nhằm làm nổi bật các động cơ và hiểu biết được xác định trong các bài báo, và cung cấp một tài nguyên để so sánh các kết quả được báo cáo, kiến trúc và bộ dữ liệu được sử dụng trong một số nghiên cứu mà chúng tôi hy vọng sẽ hữu ích cho các nhà nghiên cứu trong lĩnh vực này.
