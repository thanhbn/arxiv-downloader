# 2307.10188.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/survey/2307.10188.pdf
# File size: 776838 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Several categories of Large Language Models (LLMs): A ShortSurveySaurabh Pahune1,†,‡, Manoj Chandrasekharan21Cardinal Health, Dublin OH 43017, USA; Email: saurabh.pahune@cardinalhealth.com, Tel.:+1-901-691-75512Email: manoj.c@memphis.eduAbstract:Large Language Models (LLMs) have become effective tools for natural language process-ing and have been used in many different ﬁelds. This essay offers a succinct summary of variousLLM subcategories. The survey emphasizes recent developments and efforts made for various LLMkinds, including task-based ﬁnancial LLMs, multilingual language LLMs, biomedical and clinicalLLMs, vision language LLMs, and code language models. The survey gives a general summary of themethods, attributes, datasets, transformer models, and comparison metrics applied in each categoryof LLMs. Furthermore, it highlights unresolved problems in the ﬁeld of developing chatbots andvirtual assistants, such as boosting natural language processing, enhancing chatbot intelligence, andresolving moral and legal dilemmas. The purpose of this study is to provide readers, developers,academics, and users interested in LLM-based chatbots and virtual intelligent assistant technologieswith useful information and future directions. This survey sheds light on the possibilities of LLMs andlays the groundwork for additional study and advancement in the area by looking at the background,beneﬁts, and drawbacks of LLMs generally as well as the implications of various LLM models.Thusthis paper offers signiﬁcant information and future directions. Our goal is to look at LLM’s history,the advantages and disadvantages of LLMs in general, the types of various LLM models (eg: ﬁnance,clinical, multilingual, code, vision), and what all of this implies for the futureKeywords:Natural language processing;large language models (LLM);ﬁnancial LLMs; multilin-gual language LLMs; biomedical and clinical LLMs; vision language LLMs;code language models;transformer model;datasets;virtual intelligent assistant1. IntroductionThe origins of the ﬁrst AI language models can be found in the early history of AI.One of the oldest instances of an AI language model is the ELIZA language model, whichmade its debut in 1966 at MIT[1,2].An LLM is a development of the language model ideain AI that signiﬁcantly increases the amount of data utilized for inference and training. Asa result, the AI model’s capabilities are greatly increased. An LLM normally includes atleast one billion parameters, while there isn’t a deﬁned size for the data set that must beused for training.A trained deep-learning model called a big language model can read andproduce text in a way that is similar to what a human would. Everything is accomplishedbehind the scenes using a sizable transformer model. In 2017[3] “Attention is All YouNeed,” to establish a transformer model (The ‘T’ in all the GPT models). It is based on theattention mechanism, dispensing with recurrence and convolutions entirely. Transformerlanguage models use a deep neural network architecture called a Transformer and they aretrained to predict either masked words (i.e. ﬁll-in-the-blank) or upcoming words in text[4].Uszkoreit et al. describe the Transformer, a cutting-edge neural network design based on aself-attention mechanism that aims to be especially effective at interpreting language[5].Transformer language models have revolutionized the ﬁeld of natural language processing(NLP) since their introduction in 2018[6]. Transformer language models have receivedwidespread public attention, yet their generated text is often surprising even to NLPresearchers[4,7]. As per recent research, some of the top LLMs announced and releasedin the last few years (e.g. GPT-3/4[8], LLaMA[9], PaLM[10], MiniGPT-4[11], FinGPT[12],OPT[13], BERT[14], Bloomberggpt[15], BLOOM 176B[16], GPT NEO-X[17], RoBERTa [18],Dolly2.0[19] ;)[10,13,20–22]. For applications ranging from web search and chatbots to1

--- PAGE 2 ---
2 of 26medical and ﬁnancial document analysis, many language models are employed in thebusiness[4,15,23].
Figure 1.Distribution of language modelsAs per Figure1. numerous language models have emerged. Language models with alot of parameters and great processing power are collectively referred to as "Large LanguageModels" (LLM)[24]. Whereas A sort of language model known as a statistical languagemodel (SLM) uses statistical methods to give probability values to word sequences in alanguage. It is predicated on the notion that by examining the frequencies and patternsfound in a sizable corpus of text, it is possible to predict the likelihood of a speciﬁc wordappearing in a speciﬁc situation[25].In a Neural Language Model (NLM), the probabilitydistribution of word sequences in a language is modeled using neural network topologies.NLMs are made to catch intricate word relationships and produce text that is appropriate forthe surrounding context[26,27]. The term "Transformer Language Models" (TLMs) is usedto describe language models that especially make use of the Transformer architecture[3].The term "pre-trained language models" (PLMs) refers to language models that have beentrained in an unsupervised fashion on sizable corpora before being adjusted for particulardownstream tasks. By extracting patterns and structures from massive volumes of textdata, these models learn representations of generic language. In recent years, in the ﬁeldof healthcare, there are various biomedical and clinical transformer models are availablefor clinical concept extraction and medical relation[28]. BioBERT[29], ClinicalBERT[30],BioMegatron[31],GatorTron-base[32], GatorTron-medium[32], GatorTron-large [32]. In 2021,One of the largest models in the world for reading comprehension and natural language
Figure 2.A word cloud showing the frequency of terms used in the articles we reviewed.

--- PAGE 3 ---
3 of 26inference, Megatron-Turing Natural Language Generation 530B was created by Nvidiaand Microsoft to facilitate tasks like summarizing and content creation[33]. HuggingFaceunveiled BLOOM last year, an open big language model that can produce text in overa dozen programming languages in addition to 46 different natural languages and 13programming languages[34]. vision language models [35] a family of Visual LanguageModels (VLM) models that can be rapidly adapted to novel tasks using only a handful ofannotated examples is an open challenge for multimodal machine learning research. Table1.cited the work as having been mentioned by the corresponding author on this languagemodel ﬁeld.
Figure 3.Mapping of visualize research articlesIn Figure3., by using connecting lines to visually represent the citations for this work,it demonstrates the connection. Useful research resources and project state are included inthis time-based citation network to arrange this review of the literature. The cited researchaddresses several topics, including model features, datasets, transformer models, andbenchmarks for assessing LLM performance.The rest of the article is organized as follows. Section2.1presents prior related workthat has various versions of LLM models (Finance base, Clinical base, Vision base, Code-base, Multilingual based)and tables respectively. In Section3, deﬁne the conﬁguration ofvarious models with billions of parameters and created taxonomy tables to discuss thedetailed methodology of LLM models such as (Benchmark and dataset, Dataset content,Implementation details etc.), A mapping of the articles used in this paper was also devel-oped Figure3. In Section4(Open Issues and Research Directions ), the open issues andpotential future directions of LLM mdoels. The conclusions are described in Section5.Table 1.Studies on different language modelsTypes of Language ModelsStudyStatistical language models (SLM)Jelinek et al. [36], Rosenfeld et al. [37]Pre-trained language models (PLM)Matthew et al.[38]Large language models (LLM)Kaplan et al.[39]Neural language models (NLM)Bengio et al.[40]Transformer language models (TLM)Vaswani et al.[3]

--- PAGE 4 ---
4 of 262. RELATED WORK2.1. Multilingual language-image modelAn artiﬁcial intelligence model that can comprehend and produce both textual andvisual content across several languages is called a multilingual language-image model. Inorder to assess and provide useful information from both modalities, these models are par-ticularly trained to process and understand the complicated relationships between wordsand visuals. Chen et al. [41] study PaLI (Pathways Language and Image model), a modelthat extends this approach to the joint modeling of language and vision. Scheible et al.[42]discovered GottBERT is a pure German Language Model. AlexaTM 20B Soltan at al.[43]demonstrate that multilingual large-scale sequence-to-sequence (seq2seq) models,).pre-trained on a mixture of denoising and Causal Language Modeling (CLM). Dossou et al.[44]present AfroLM, a multilingual language model pretrained from scratch on 23 Africanlanguages (the largest effort to date) using our novel self-active learning framework. Scaoet al.[16] present BLOOM, a 176B-parameter open-access Multilingual language modeldesigned.Table 2.Various multilingual language LLMs.ModelsStudyYearBLOOMScao et al.[16]2022AfroLMDossou et al.[44]2022AlexaTM 20BSoltan at al.[43]2022GottBERTScheible et al.[42]2020PaLIChen at al.[41]20222.2. Clinical and biomedical transformer modelA clinical and biomedical transformer model is a type of artiﬁcial intelligence that wascreated with the express purpose of processing and analyzing clinical and biomedical textdata. These transformer models make use of the architecture of the transformer, whichhas excelled in jobs requiring natural language processing. Clinical notes, electronic healthrecords, research articles, and other pertinent sources of clinical and biomedical data areincluded in the large-scale datasets used to train the clinical and biomedical transformermodels. These models gain knowledge of the speciﬁc words, phrases, and ideas used inthe medical ﬁeld. Clinical and biomedical transformer models’ main goals are to deriveinsightful information, carry out text categorization, entity recognition, relation extraction,question answering, and other activities particular to the clinical and biomedical area.Clinical decision support, information retrieval, patient risk assessment, and automateddocumentation are just a few of the activities that they may help healthcare workers with.Yang et al.[32] develop from scratch a large clinical language model – GatorTron – using>90 billion words of text (including >82 billion words of de-identiﬁed clinical text) and sys-tematically evaluate it on 5 clinical NLP tasks including clinical concept extraction, medicalrelation extraction, semantic textual similarity, natural language inference (NLI), and medi-cal question answering (MQA). Lee et al.[29] introduce BioBERT (Bidirectional EncoderRepresentations from Transformers for Biomedical Text Mining), which is a domain-speciﬁclanguage representation model pre-trained on large-scale biomedical corpora. Li et al[45]Present Hi-BEHRT, a hierarchical Transformer-based model that can signiﬁcantly expandthe receptive ﬁeld of Transformers and extract associations from much longer sequences.Using a multimodal large-scale linked longitudinal electronic health records. Wang etal.[46] propose an innovative causal inference model–InferBERT, by integrating the ALite Bidirectional Encoder Representations from Transformers (ALBERT). Large languagemodels in health care As per Anmol et al.[47] has already been proposed that LLMs, suchas ChatGPT, could have applications in the ﬁeld of health care due to the large volumesof free-text information available for training models.An LLM trained on more than 90billion words of text from electronic health records (EHR)[28] Author Yang et al. develop a

--- PAGE 5 ---
5 of 26scratch a large clinical language model GatorTron using more than 90 billion words of text.Existing biomedical and clinical transformer models for clinical concept extraction and med-ical relation such as BioBERT[29], ClinicalBERT[30], BioMegatron[31],GatorTron-base[32],GatorTron-medium[32], GatorTron-large [32].Santosh et al.[48] propose PathologyBERT - a pre-trained masked language modelwhich was trained on 347,173 histopathology specimen reports and publicly released inthe Huggingface1repository. Comprehensive experiments demonstrate that pre-training oftransformer model on pathology corpora yields performance improvements on NaturalLanguage Understanding (NLU) and Breast Cancer Diagnose Classiﬁcation when com-pared to nonspeciﬁc language models. Jaiswal et al.[49] intorduce RadBERT-CL whichis"Factually-Aware Contrastive Learning For Radiology Report Classiﬁcation." Also showthat the representations learned by RadBERT-CL can capture critical medical information inthe latent space. Gu et al.[14] accelerate research in biomedical and released state-of-the-artpretrained and task-speciﬁc models for the community, and created a leaderboard featuringBLURB benchmark (Biomedical Language Understanding Reasoning Benchmark)). Theauthor challenges, the major advantage of domain-speciﬁc pretraining from scratch stemsfrom having an in-domain vocabulary. Peng et al.[50] introduce BLUE, a collection ofresources for evaluating and analyzing biomedical natural language representation models.ﬁnd that the BERT models pre-trained on PubMed abstracts and clinical notes see betterperformance than do most state-of-the-art models. Beltagy et al.[51] SCIBERT leveragesunsupervised pretraining on a large multi-domain corpus of scientiﬁc publications toimprove performance on downstream scientiﬁc NLP tasks. Alsentzer et al.[30] releasedClinical BERT models for clinical text: one for generic clinical text and another for dischargesummaries speciﬁcally. Also, demonstrate on several clinical NLP tasks that improve-ments this system offers over traditional BERT and BioBERT. Shin et al.[31] come up withBioMegatron consider as large biomedical domain lanuage model. Which show consistentimprovements on benchmarks with larger BioMegatron model trained on a larger domaincorpus, contributing to our understanding of domain language model applications.Table 3.Various biomedical and clinical LLMs.ModelsStudyYearGatorTron-baseYang et al.[32]2022BioBERTLee et al.[29]2020EntityBERTLin et al.[52]2021Hi-BEHRTLi et al[45]2022InferBERTWang et al.[46]2021PathologyBERTSantosh et al.[48]2022PubMedBERTGu et al.[14]2021SciBERTBeltagy et al.[51]2019RadBERTYan et al.[53]2022ClinicalBERTAlsentzer et al.[30]2019BlueBERTPeng et al.[50]2019BioMegatronShin et al.[31]20192.3. Large language model for ﬁnanceThese models can analyze and grasp complicated ﬁnancial text data efﬁciently bymaking use of deep learning techniques like transformer architectures. They can help withjobs including compiling ﬁnancial reports, summarizing ﬁnancial documents, researchinginvestments, managing portfolios, and analyzing ﬁnancial news. Financial professionals’ability to make more educated, data-driven decisions may be improved by the use of largelanguage models in the ﬁeld. They can offer insights for investing plans, assist in identi-fying market trends, evaluate risk factors, and spot abnormalities. Wu et al.[15] presentBloombergGPT (A large language model for ﬁnance), a 50 billion parameter languagemodel that is trained on a wide range of ﬁnancial data. Author validates BloombergGPT on

--- PAGE 6 ---
6 of 26standard LLM benchmarks, open ﬁnancial benchmarks, and a suite of internal benchmarksthat most accurately reﬂect our intended usage. Scao et al.[16] present BLOOM, a 176B-parameter open-access language model designed and built. BLOOM is a decoder-onlyTransformer language model that was trained on the ROOTS corpus, a dataset comprisinghundreds of sources in 46 natural and 13 programming languages (59 in total). Black etal.[17] introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language modeltrained on the Pile[54],evaluate its performance on a range of language-understanding,mathematics, and knowledge-based tasks. Araci et al.[55]introduce FinBERT languagemodel based on BERT, to tackle NLP tasks in the ﬁnancial domain. Zhang et al.[13] presentOpen Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformersranging from 125M to 175B parameters. Yang et al.[12] present an open-source largelanguage model FinGPT, for the ﬁnance sector. FinGPT responds innovatively by lever-aging pre-existing LLMs and ﬁne-tuning them to speciﬁc ﬁnancial applications. Xie atal.[56] discovers the PIXIU LLM model for Instruction Data and Evaluation Benchmark forFinance.Table 4.Various ﬁnance-based LLMs.ModelsStudyYearBloombergGPTWu et al.[15]2023GPT-NeoXBlack et al.[17]2022OPTZhang et al.[13]2022BLOOM-176BScao et al.[16]2022FinBERTAraci et al.[55]2019FinGPTYang et al.[12]2023PIXIUXie at al.[56]20232.4. Classiﬁcations of vision language modelsArtiﬁcial intelligence models called "vision language models" are created to compre-hend and produce data from the combination of visual and linguistic inputs. These modelsseek to close the gap between the comprehension of images or other visual content andthat of natural language. These models are capable of carrying out a number of tasks,such as image captioning, visual question answering (VQA), image generation from textdescriptions, and image-text matching. For instance, a vision language model can producea caption for an image that accurately describes the image’s content. Similar to this, themodel can offer pertinent responses or justiﬁcations when asked a text query regarding apicture. Alayrac et al.[57] comes with Flamingo, a family of Visual Language Models (VLM)models that can be rapidly adapted to novel tasks using only a handful of annotated exam-ples is an open challenge for multimodal machine learning research. Monajatipoor et al.[58]Vision-and-language (VL) models take image and text as input and learn to capture theassociations between them.Prior studies show that pre-trained VL models can signiﬁcantlyimprove the model performance for downstream tasks such as Visual Question Answering(VQA). Ko et al.[59] present F-VLM, a simple open-vocabulary object detection method builtupon Frozen Vision and Language Models.Where F-VLM simpliﬁes the current multi-stagetraining pipeline by eliminating the need for knowledge distillation or detection tailoredpretraining. Zhu et al.[11] projected MiniGPT-4 which is a enhancing vision-languageunderstanding with advanced large language models. Hong et al.[60] propose a recurrentBERT model that is time-aware for use in of vision-and-language navigation(VLN). In thispaper author propose a recurrent BERT model that is time-aware for use in VLN. Thrush etal.[61] present a novel task and dataset for evaluating the ability of vision and languagemodels to conduct visio-linguistic compositional reasoning, which we call Winoground.Wang et al.[62] propose a smaller and faster VL model, MiniVLM, which can be ﬁnetunedwith good performance on various downstream tasks like its larger counterpart. MiniVLM

--- PAGE 7 ---
7 of 26consists of two modules, a vision feature extractor and a transformer-based vision-languagefusion module.Table 5.Various vision language LLMs.ModelsStudyYearFlamingoAlayrac et al.[57]2022BERTHopMonajatipoor et al.[58]2022F-VLMKuo et al.[59]2022MiniVLMWang et al.[62]2020VLN-BERTHong et al.[60]2021WinogroundThrush et al.[61]2022MiniGPT-4Zhu et al.[11]20232.5. Classiﬁcations of code large language model (Code LLMs)Large-scale language models have the potential to promote programming by pro-moting code reuse, knowledge sharing, and developer collaboration. They can aid ineliminating errors, automating repetitive coding processes, and accelerating the develop-ment process. A code big language model is designed to help programmers with a varietyof coding-related tasks. These models are capable of tasks like code completion, codegeneration, code summarization, and code translation and can comprehend the syntax,semantics, and programming patterns of code. Luo et al.[63] In this paper introduced Wiz-ardCoder, which empowers Code LLMs with complex instruction ﬁne-tuning, by adaptingthe Evol-Instruct method to the domain of code. Nijkamp et al.[64] release a family oflarge language models up to 16.1B parameters, called CODEGEN, on natural languageand programming language data. Jain et al.[65] present an approach to augment theselarge language models with post-processing steps based on program analysis and synthesistechniques, that understand the syntax and semantics of programs. Wang et al. [66] presentCodeT5,a uniﬁed pre-trained encoder-decoder Transformer model that better leverages thecode semantics conveyed from the developer-assigned identiﬁers.Table 6.Various code language models (Code LLMs)ModelsStudyYearWizardCoderLuo et al.[63]2023CodeGenNijkamp et al.[64]2022JigsawJain et al.[65]2022CodeT5Wang et al. [66]2021

--- PAGE 8 ---
8 of 263. Taxonomy tables of various LLM modelsTable 7.Classiﬁcations of multilingual language modelLLM ModelBenchmark andDatasetDataset contentImplementationdetailsApplication Versions of ModelPALI [41]WebLI[67],COCO[68],Multilingualcaptioning onCrossmodal-3600[69], VQA[70]New image-texttraining setcontaining 10Bimages and texts inover 100 languages-PathwaysLanguage andImage modelPaLI-3B, PaLI-15B,PaLI-17B
GottBERT[42]The German dataportion of theOSCAR measures145GB of textcontainingapproximately 21.5billion words inapproximately 459million documents(one document perline)A vocabulary of52k subwordtokens based on 40GB of randomlysampleddocuments of theGerman OSCARportion.256 core TPU podusing the RoBERTaBASE architectureA pure GermanLanguage ModelGottBERT, GermanBERT, XLMRoBERTa, dbmzBERT[42]
BLOOM [16]BLOOM is adecoder-onlyTransformerlanguage modelthat was trained onthe ROOTScorpus[71], 2022),A compositecollection of 498Hugging Facedatasets[72],HELMbenchmark[73]Datasetcomprisinghundreds ofsources in 46natural and 13programminglanguages (59)156 TFLOPs in ourfastestconﬁguration withNVIDIA V100GPUsA 176B-ParameterOpen-AccessMultilingualLanguage ModelBLOOM-1B7,BLOOM-560M,BLOOM-1.1B,BLOOM-1.7B,BLOOM-1.7B,BLOOM-7.1B

--- PAGE 9 ---
9 of 26Table7– continued from previous page
AlexaTM20B[43]Wikipedia andmC4 [70]Data in 12languages,namely, Arabic,English, French,German, Hindi,Italian,Japanese,Marathi,Portuguese,Spanish, Tamil,and Telugu,Pack sequencesof tokens toproducesequences ofapproximately1024 subwordunits.TrainedAlexaTM 20Bfor 120 days on128 A100 GPUsA Large-ScaleMultilingualSeq2seq Model--
AfroLM[44]LanguagesCorpora Details[64,74,75],YOSMdataset[76]Several workson sentimentanalysis havebeen done onhigh resourcelanguageswhile lowresourceslanguages likeYoruba andother Africanlanguages, datacomprised 1500movie reviewsthat weresourced fromIMDB, RottenTomatoes,Letterboxd,Cinemapointer,andNollyrated[76]Google Cloudwith a single48GB NVIDIAA100 GPUA Self-ActiveLearning-basedMultilingualPre-trainedLanguageModel for 23AfricanLanguagesAfroLM-Large,AfroLM-Large(w/AL) toAfroLM-Large(w/o AL),AfriBERTa-Large[44]-

--- PAGE 10 ---
10 of 26Table 8.Classiﬁcations of various clinical/biomedical transformer modelsLLM ModelBenchmark andDatasetDataset contentImplementationdetailsApplication Versions of Model
GatorTron-base[32]5 clinical NLPtasks named entityrecognition [NER],medical relationextraction (MRE),semantic textualsimilarity (STS),natural languageinference(NLI),and medicalquestionanswering (MQA),MedNLI[77], 2019n2c2[78], emrQAMedication [79]A total number of290,482,002 clinicalnotes from2,476,628 patientswere extractedfrom the UFHealth IntegratedData Repository(IDR), theenterprise datawarehouse of theUF Healthsystem[32]992 A100 80GGPUs from 124NVIDIA DGXA large clinicallanguage modelGatorTron-base,GatorTron-base(1/4data),GatorTron-medium,GatorTron-large
BioBERT [29]Pre-trained onbiomedical domaincorpora (PubMedabstracts, EnglishWikipedia,BooksCorpus andPMC full-textarticles)NCBI Disease with6881 (Number ofannotations),2010i2b2/VA diseasewith 19665(Number ofannotations) ,BC5CDR diseasewith 12694(Number ofannotations)Eight NVIDIAV100 GPUs.A pre-trainedbiomedicallanguagerepresentationmodel forbiomedical textminingBioBERT v1.0,BioBERT v1.1
Hi-BEHRT [45]Modelperformance onincident riskprediction for fourdiseases: heartfailure (HF),diabetes, chronickidney disease(CKD), and stroke.Clinical PracticeResearch Datalink(CPRD)[80],myocardialfailure[81],diabetes mellitus[82]2 GPUs [45]HierarchicalTransformer-basedmodel for accurateprediction ofclinical eventsusing multimodallongitudinalelectronic healthrecordsBEHRT, Hi-BEHRT
InferBERT [46]Two FAERSdatasets,Analgesics-induced acute liverfailure andTramadol-relatedmortalitiesdatasetsFAERS datasetEffects: (Acuteliver ﬁbrosis andcirrhosis, ORAcute liver failureand associateddisorders, ORCholestasis andjaundice) ANDDrugs byAND-groups:[Analgesics (AnyRole)]” to extract45,773 [83]one NVIDIA TeslaV100 GPU.ATransformer-BasedCausal InferenceFramework forEnhancing Phar-macovigilance-

--- PAGE 11 ---
11 of 26Table8– continued from previous page
PathologyBERT[48]Twonon-overlappingcorpora ofhistopathologyreports fromEmory UniversityHospital (EUH)Emory UniversityInstitutionalReviewBoard(IRB), a totalof 340,492unstructuredHistopathologyspecimens from67,136 patientswere extractedfrom the clinicaldata warehouse ofEmory UniversityHospital (EUH)between the years1981 and 2021.GeForce QuadroRTX 6000 24 GBGPUsA NewTransformerLanguage Modelfor PathologyDomain(Note:PathologyBERT onCorpus II topredict 6 breastcancer diagnoseseverity (invasivebreast cancer, highrisk lesion,borderline lesion,non-breast cancer,benign, andnegative)[48].-
RadBERT [53]MIMIC-CXRdataset [84],pseudo-labeledusing automaticlabeler [85]MIMIC-CXRdataset whichconsists of 377, 110chest-Xray imagesof 227, 827 patientsalong with theircorrespondingde-identiﬁedradiology reports.-Factually-AwareContrastiveLearning ForRadiology ReportClassiﬁcationRadBERT-CL
PubMedBERT [14]BLURB: AComprehensiveBenchmark forBiomedical NLP,BC5-chem[86],BC5-disease[86],NCBI-disease[87],BC2GM[88],PubMedQA[89]BLUE mixesPubMed-basedbiomedicalapplications (sixdatasets such asBC5, ChemProt,and HoC) withMIMIC-basedclinicalapplications (fourdatasets such asi2b2 and MedNLI)One DGX-2machine with 16V100 GPUsIt was pretrainedover PubMedabstracts andcomplete PubMedCentral articlesand is an uncasedBERT Base modelBERT, RoBERTa,BioBERT, SciBERT,Clinical- BERT ,and BlueBERT
SciBERT[51]Corpus,EBM-NLP[90],SciERC[91],ACL-ARC[92]Train SCIBERT ona random sampleof 1.14M papersfrom Semanticscholar[93] corpusconsists of 18%papers from thecomputer sciencedomain and 82%from the broadbiomedical domainSingle TPU v3 with8 coresA PretrainedLanguage Modelfor Scientiﬁc Text-
ClinicalBERT[30]i2b2 2006[94], i2b22010[95], i2b22012[96], i2b22014[97]Clinical NLP Task,MedNLI naturallanguage inferencetask[98], Useclinical text fromthe approximately2 million notes inthe MIMIC-III v1.4database[99]A single GeForceGTX TITAN X 12GB GPUEvaluatesrepresentations ofclinical notes usingbidirectionaltransformers(ClinicalBERT)Bio+Clinical BERT,Clinical BERT [30]

--- PAGE 12 ---
12 of 26Table8– continued from previous pageBioMegatron[31]Used downstreambiomedicalbenchmarkdatasets forNamed EntityRecognition(NER),RelationExtraction(RE),and QuestionAnswering(QA)BC5CDR[86] NERdataset annotateddisease,ChemProt[100]dataset containssentences fromPubMed abstracts,BioASQ-7b factoidtask[101] is abiomedical QAdatasetBatch size of 64 perGPU with dataparallelism on 16GPUsBioMegatronconsider as largebiomedical domainlanguage modelBioMegatron-345m,BioMegatron-800m,BioMegatron-1.2b
BlueBERT[50]Uses both PubMedtext andde-identiﬁedclinical notes fromMIMIC-III[99],PubMedabstract[102]Models weretrained with 5Msteps on thePubMed corpusand 0.2M stepsand BLUE containsﬁve tasks with tencorpora that covera broad range ofdata quantities anddifﬁculties.(Sentencesimilarity, Namedentity recognition,Relation extraction,Documentclassiﬁcation,Inference task )-BiomedicalLanguageUnderstandingEvaluation (BLUE)benchmark tofacilitate researchin thedevelopment ofpre-traininglanguagerepresentations inthe biomedicinedomainBlueBERT-Base +Uncased+PubMed,BlueBERT-Base +Uncased+PubMed+ MIMIC-III

--- PAGE 13 ---
13 of 26Table 9.Classiﬁcations of large language model for ﬁnanceLLM ModelBenchmark andDatasetDataset contentImplementationdetailsApplication Versions of ModelBloombergGPT[15]C4[103],FinPile[54], publicﬁnancial NLPbenchmarks [104]Colossal CleanCrawled Corpus(C4) gives us avocabulary size of125,000, Dump ofEnglish Wikipediafrom July 1, 2022.8 NVIDIA 40GBA100 GPUsA large languagemodel for ﬁnanceBLOOM-style,BLOOM176B[16]
GPT-NeoX [17] Pile [54]It has 22 datasources, coarselybroken down into5 categories(Academic Writing,Web-scrapes andInternet Resources,Prose, Dialogue,Miscellaneous)8 NVIDIAA100-SXM4-40GBGPUs andconﬁgured withtwo AMD EPYC7532 CPUsAn Open-SourceAutoregressiveLanguage ModelGPT-NeoX-20BOPT-175B[13]BookCorpus[105],MinhashLSH[106],RoBERTaCCNews[18]Eight Transformerlanguage modelsranging from 125million to 175billion parametersOn 992 80GB A100GPUsOpen Pre-trainedTransformerLanguage ModelsOPT from 125M to175B (Ex:OPT-125M toOPT-175B)BLOOM-176B[16]ROOTS corpus[71],A compositecollection of 498Hugging Facedatasets[72],SuperGLUE[107],STS datasets fromMTEB[108], HELMbenchmark[73]ROOTS corpus(datasetcomprisinghundreds ofsources in 46natural and 13programminglanguages (59 intotal))8 NVIDIA A10080GB GPUsA 176B-ParameterOpen-AccessMultilingualLanguage ModelBLOOM-560M,BLOOM-1B7, BLOOM-1.7B,BLOOM-3B,BLOOM-7.1B,BLOOMZ[109]
FinBERT[55]Financialcorpus(TRC2-ﬁnancial)[110],FinancialPhraseBank[111],FiQASentiment[112]FiQA Sentiment isa dataset that wascreated forﬁnancial opinionmining andquestionansweringchallenge, use thedata for Task 1,which includes1,174 ﬁnancialnews headlinesand tweets withtheircorrespondingsentiment scoreAmazon p2.xlargeEC2 instance withone NVIDIA K80GPU, 4 vCPUsFinancialsentiment analysiswith pre-trainedlanguage modelsFinBERT-task,FinBERT-domain
FinGPT[12]Academic datasets,Novel ﬁnancialdatasetDifferent ﬁnancialdata sources, suchas Financial News,Company Fillings,Social MediaDiscussions, andCompanyAnnouncements-An open-sourcelarge languagemodel (Financialsentiment analysis,Financial Frauddetection, Creditscoring, Portfoliooptimization,Financialeducation)FinLLM

--- PAGE 14 ---
14 of 26Table9– continued from previous pageLLM ModelBenchmark andDatasetDataset contentImplementationdetailsApplication Versions of ModelPIXIU [56]FinancialEvaluationBenchmark,ﬁnancialinstruction tuningdataset FIT onvarious ﬁnancialNLP, FiQA-SA,Financial PhraseBank (FPB),Benchmark FLAREFiQA-SA consist11,730 instructions(newsheadlines,tweets),FPB has 48,450news data types[56]8 A100 40GBGPUs.A Large LanguageModel, InstructionData andEvaluationBenchmark forFinanceFinMA

--- PAGE 15 ---
15 of 26Table 10.Classiﬁcations of vision language modelsLLM ModelBenchmark andDatasetDataset contentImplementationdetailsApplication Versions of ModelFlamingo[57]MultiModalMassiveWeb(M3W) dataset,Pairs ofimage/video andtext, ALIGN [113],VQAv2, VATEX,VizWizALIGN datasercomposed of 1.8billion imagespaired with alt-text,LTIP (Long Textand Image Pairs)which consists of312 million imageand text pairs.-A Visual LanguageModel forFew-Shot LearningFlamingo-3B,Flamingo-9B,Flamingo
BERTHop [58]ChestX-ray14,MIMIC-CXRMIMIC-CXR labelsare generatedusing ChexPert[85]and NegBio[114]auto labelers.OpenI comprises3,996 reports and8,121 associatedimages from 3,996unique patientscollected byIndiana Universityfrom multipleinstitutes.-An EffectiveVision-and-Language Modelfor Chest X-rayDisease DiagnosisPixelHop++,BlueBERT
F-VLM [59]LVISBenchmark[115]LVIS dataset whichcontains a largeand diverse set of1203 objectcategories suitableforopen-vocabularydetection.-A simpleopen-vocabularyobject detectionmethod built uponFrozen Vision andLanguage Models.F-VLM-R50,F-VLM-R50x4,F-VLM-R50x16,F-VLM-R50x64MiniGPT-4 [11]ConceptualCaption[116],SBU[117] andLAION[118]Conceptual 12M(CC12M), a datasetwith 12 millionimage-text pairsspeciﬁcally meantto be used forvision-and-languagepre-training4 A100 (80GB)GPUsMiniGPT-4 aims toalign visualinformation from apretrained visionencoder with anadvanced largelanguage model(LLM)–
MiniVLM [62]ImageCaptioning[119],VQA[120], NaturalLanguage VisualReasoning for Real(NLVR2)[121],COCO imagecaptioning taskVQA is arepresentation of[CLS] is used topredict the answerover a shared set of3129 answers witha linear layer2 Intel(R) Xeon(R)CPU E5-2620 v4@2.10GHzA Smaller andFasterVision-LanguageModel-
VLN-BERT[60]R2R[122],REVERIE[123]Matterport3D, alarge-scale RGB-Ddataset containing10,800 panoramicviews from 194,400RGB-D images of90 building-scalescenes[124]Single NVIDIA2080Ti GPUA RecurrentVision-and-Language BERTfor NavigationVisualBERT, VLBERT

--- PAGE 16 ---
16 of 26Table10– continued from previous pageWinoground[61]Winogrounddataset washand-curated byfour expertannotators withextensiveexperience invision andlanguage researchas well ascomputationallinguisticsDataset has 1600image-text pairs intotal, with 800correct and 800incorrect pairings-Probing Vision andLanguage Modelsfor Visio-LinguisticCompositionality-

--- PAGE 17 ---
17 of 26Table 11.Classiﬁcations of code language models (Code LLMs)LLM ModelBenchmark andDatasetDataset contentImplementationdetailsApplication Versions of ModelWizardCoder [63]Four codegenerationbenchmarks:HumanEval[125],HumanEval+[126],MBPP[127], andDS-1000[128]Initialized it withthe 20K instruction-following datasetcalled CodeAlpaca[67].Evol-Instructtechnique on thisdataset consistingof 20,000 samplesto produce evolveddata.-Empowering CodeLarge LanguageModels withEvol-InstructWizardLM [129]
CodeGen [64]THEPILE,BIGQUERY, andBIGPYTHON.THEPILE is an825.18 GiB Englishtext corpuscollected [54],BIGQUERY is asubset of Google’spublicly availableBigQuery dataset,datasetBIGPYTHONcontains a largeamount of data inthe programminglanguage, Python.Google’s TPU-v4hardwareAn open LLM forcode withmulti-turnprogram synthesisCodeGen -MULTI ,CodeGen-NL,CodeGen-MONO
Jigsaw [65]PandasEvaldataset, HackathondatasetThis datasetconsists of 68Python Pandastasks. Each taskcan be solvedusing a single lineof code bycomposing at most2-3 Pandasfunctions,Hackathon datasetconsists of 21Pandas tasks; eachtask-Large LanguageModels meetProgram Synthesis-
CodeT5[66] SearchNet[130]Consists of 99natural languagequeries with about4k expert relevanceannotations oflikely results fromCodeSearchNetCorpus. Thecorpus containsabout 6 millionfunctions fromopen-source codespanning sixprogramminglanguages (Go,Java, JavaScript,PHP, Python, andRuby)16 NVIDIA A100GPUs with 40Gmemory.Uniﬁed Pre-trainedEncoder-DecoderModels for CodeUnderstandingand GenerationCodeBERT

--- PAGE 18 ---
18 of 26Table 12.Detailed of several existing LLMs conﬁguration with Millions/ Billions of parameters.ModelOptimizer and LayersModel sizeReferenceGPT-2Adam, 12 layers1.5 billion[131]GPT-3Adam, 96 layers175 billion[132,133]Microsoft DialoGPT-147 million[134,135]BloombergGPTGELU, 70 layers50 billion[15]Vicuna-13 billion[136]Dolly2.0-12 billion[19]BLOOMAdam,70 layers176 billion[34]LLaMAAdamW,-65 billion[9]Jurassic-1-178 billion[137]GLMAdamW,-130 billion[138]PaLMAdafactor,-540 billion[139]OPT 175BAdamW, 96175 billion[13]ChinchillaAdam,80 layers70 billion[67]BERT-baseAdam,12 layers100 million[14]BERT-largeAdam,24 layers300 million[14]ALBERTAdam ,12 layers12 million[140]RoBERTa baseAdam,12 layers125 million[18]RoBERTa largeAdam,24 layers355 million[18]Megatron-Turing NLG-530 billion[141]BioBERT-13.5 billion[29]ClinicalBERTAdam1.28 billion[30,142]BioMegatronAdam,241.2 billion[31]GatorTron-baseAdam,24 layers345 million[32,143]GatorTron-mediumAdam,48 layers3.9 billion[32,143]GatorTron-largeAdam, 56 layers8.9 billion[32,143]GopherAdam,-280 billion[144]GPT-NeoXAdamW20 billion[17]Bloom 176Adam,24 layers176 billion[16]PubMedBERT-110 million[89]AlexaTM 20B-,46layers19.75 billion[43]AfroLM-Large-,10layers264 million[44]Hi-BEHRTAdam, layers264 million[45]PathologyBERTAdam, 12 Layers347 million[48]BioMegatronAdam, 24 Layers345 million[31]BioMegatron mediumAdam, 36 Layers800 million[31]BioMegatron largeAdam, 24 Layers1.2 billion[31]BloombergGPTAdam, 70 Layers50.6 billion[15]BLOOM-styleAdam, 70 Layers50 billion[145]GPT-NeoX-20BAdam, 44 Layers20 billion[17]CODEGEN-16.1 billion[64]In Table12. based on what we’ve seen, the billions to millions range. Dataset optimiza-tion is a crucial step in LLM models, particularly those with a large number of parameters,with the goal of improving the model’s functionality and speed. To make sure the trainingdata is representative, diverse, and in line with the anticipated results, dataset optimizationentails carefully choosing and preparing the training data. Researchers and programmerscan enhance the model’s capacity to comprehend and produce words, leading to moreprecise and cogent responses, by optimizing the dataset. Basically, dataset optimizationhelps LLM models reach their full potential by supplying high-quality training data that isin line with the particular tasks or objectives at hand.

--- PAGE 19 ---
19 of 264. Open Issues and Research DirectionsDue to the size of large language models, their deployment requires a high level oftechnical expertise, including a ﬁrm understanding of deep learning, transformer models,distributed software, and hardware as well as ethical and legal issues arising from theliability and harm potential of such systems.Many professionals in the IT sector are working to support research and create tech-nologies that can open up access to broad language models, allowing customers andcompanies of all sizes to take advantage of them.It is not clear how large clinical language models with billions of parameters can helpmedical AI systems utilize unstructured electronic health records (EHRs) within the currentlegal and ethical framework while ensuring privacy of patient information and accuracy ofthe information provided[28].Scaling and maintaining large language models can be difﬁcult and expensive. Build-ing a foundational large language model often requires months of training time and millionsof dollars [33].And because LLMs require a signiﬁcant amount of training data, developers andenterprises can ﬁnd it a challenge to access large-enough datasets to train such systemswhile ensuring data is collected ethically and with permission of the parties involved.Maintaining them by putting in place systems to ensure accurate and useful outputs atscale is also a signiﬁcant challenge.5. ConclusionIn this study, the most recent advances in large language models (LLMs) were show-cased and the key concepts, ﬁndings, and strategies for understanding and exploitingLLMs were presented. A wide range of issues are covered in this study, including modelfeatures, datasets, transformer models, and LLM performance benchmarks. Recent studieshave focused on various LLM types, such as multilingual LLMs, biomedical and clinicalLLMs, vision language LLMs, and code language models. This survey attempts to coverthe most recent research on LLMs and provides academics and engineers with a helpfulresource.AcronymsBERTBidirectional Encoder Representation From Transformers.BioBERTBidirectional Encoder Representations from Transformers for Biomedical TextMining.BLUEBiomedical Language Understanding Evaluation.C4Colossal Clean Crawled Corpus.LLaMALarge Language Model Meta AI.LLMLarge Language Model.MTEBMassive text embedding benchmark.RoBERTaRobustly Optimized BERT approach.6. References1.Singh, S.; Thakur, H.K. Survey of various AI chatbots based on technology used. In Proceedingsof the 2020 8th International Conference on Reliability, Infocom Technologies and Optimization(Trends and Future Directions)(ICRITO). IEEE, 2020, pp. 1074–1079.2.Weizenbaum, J. ELIZA—a computer program for the study of natural language communicationbetween man and machine.Communications of the ACM1966,9, 36–45.

--- PAGE 20 ---
20 of 263.Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A.N.; Kaiser,Ł.; Polosukhin,I. Attention is all you need.Advances in neural information processing systems2017,30.4.Chang, T.A.; Bergen, B.K. Language model behavior: A comprehensive survey.arXiv preprintarXiv:2303.115042023.5.Uszkoreit, J. Transformer: A novel neural network architecture for language understanding.Google AI Blog2017,31.6.Radford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.; et al. Improving language understand-ing by generative pre-training2018.7.Devlin, J.; Chang, M.W.; Lee, K.; Toutanova, K. Bert: Pre-training of deep bidirectional trans-formers for language understanding.arXiv preprint arXiv:1810.048052018.8.GPT-4 is OpenAI’s most advanced system, producing safer and more useful responses.https://openai.com/product/gpt-4. [Online; Accessed 06-17-2023].9.Introducing LLaMA: A foundational, 65-billion-parameter large language model.https://ai.facebook.com/blog/large-language-model-llama-meta-ai/. [Online; Accessed 06-17-2023].10.Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra, G.; Roberts, A.; Barham, P.; Chung,H.W.; Sutton, C.; Gehrmann, S.; et al. Palm: Scaling language modeling with pathways.arXivpreprint arXiv:2204.023112022.11.Zhu, D.; Chen, J.; Shen, X.; Li, X.; Elhoseiny, M. Minigpt-4: Enhancing vision-languageunderstanding with advanced large language models.arXiv preprint arXiv:2304.105922023.12.Yang, H.; Liu, X.Y.; Wang, C.D. FinGPT: Open-Source Financial Large Language Models.arXivpreprint arXiv:2306.060312023.13.Zhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.; Chen, S.; Dewan, C.; Diab, M.; Li, X.; Lin,X.V.; et al. Opt: Open pre-trained transformer language models.arXiv preprint arXiv:2205.010682022.14.Gu, Y.; Tinn, R.; Cheng, H.; Lucas, M.; Usuyama, N.; Liu, X.; Naumann, T.; Gao, J.; Poon, H.Domain-speciﬁc language model pretraining for biomedical natural language processing.ACMTransactions on Computing for Healthcare (HEALTH)2021,3, 1–23.15.Wu, S.; Irsoy, O.; Lu, S.; Dabravolski, V.; Dredze, M.; Gehrmann, S.; Kambadur, P.; Rosenberg, D.;Mann, G. Bloomberggpt: A large language model for ﬁnance.arXiv preprint arXiv:2303.175642023.16.Scao, T.L.; Fan, A.; Akiki, C.; Pavlick, E.; Ili´ c, S.; Hesslow, D.; Castagné, R.; Luccioni, A.S.; Yvon,F.; Gallé, M.; et al. Bloom: A 176b-parameter open-access multilingual language model.arXivpreprint arXiv:2211.051002022.17.Black, S.; Biderman, S.; Hallahan, E.; Anthony, Q.; Gao, L.; Golding, L.; He, H.; Leahy, C.;McDonell, K.; Phang, J.; et al. Gpt-neox-20b: An open-source autoregressive language model.arXiv preprint arXiv:2204.067452022.18.Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer,L.; Stoyanov, V. Roberta: A robustly optimized bert pretraining approach.arXiv preprintarXiv:1907.116922019.19.EDWARDS, B. A really big deal”—Dolly is a free, open source, ChatGPT-style AI model.https://arstechnica.com/information-technology/2023/04/a-really-big-deal-dolly-is-a-free-open-source-chatgpt-style-ai-model/, 2023. [Online; Accessed 06-17-2023].20.Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.D.; Dhariwal, P.; Neelakantan, A.; Shyam,P.; Sastry, G.; Askell, A.; et al. Language models are few-shot learners.Advances in neuralinformation processing systems2020,33, 1877–1901.21.Peng, B.; Li, C.; He, P.; Galley, M.; Gao, J. Instruction tuning with gpt-4.arXiv preprintarXiv:2304.032772023.22.Lopez-Lira, A.; Tang, Y. Can chatgpt forecast stock price movements? Return predictability andlarge language models.arXiv preprint arXiv:2304.076192023.23.Lee, A. What are large language models used for.NVIDIA Blog2023.24.Zhao, W.X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.; Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.;et al. A survey of large language models.arXiv preprint arXiv:2303.182232023.25.Gao, J.; Lin, C.Y. Introduction to the special issue on statistical language modeling, 2004.26.Melis, G.; Dyer, C.; Blunsom, P. On the state of the art of evaluation in neural language models.arXiv preprint arXiv:1707.055892017.27.Bengio, Y.; Ducharme, R.; Vincent, P. A neural probabilistic language model.Advances in neuralinformation processing systems2000,13.

--- PAGE 21 ---
21 of 2628.Yang, X.; Chen, A.; PourNejatian, N.; Shin, H.C.; Smith, K.E.; Parisien, C.; Compas, C.; Martin,C.; Costa, A.B.; Flores, M.G.; et al. A large language model for electronic health records.npjDigital Medicine2022,5, 194.29.Lee, J.; Yoon, W.; Kim, S.; Kim, D.; Kim, S.; So, C.H.; Kang, J. BioBERT: a pre-trained biomedicallanguage representation model for biomedical text mining.Bioinformatics2020,36, 1234–1240.30.Alsentzer, E.; Murphy, J.R.; Boag, W.; Weng, W.H.; Jin, D.; Naumann, T.; McDermott, M. Publiclyavailable clinical BERT embeddings.arXiv preprint arXiv:1904.033232019.31.Shin, H.C.; Zhang, Y.; Bakhturina, E.; Puri, R.; Patwary, M.; Shoeybi, M.; Mani, R. BioMegatron:Larger biomedical domain language model.arXiv preprint arXiv:2010.060602020.32.Yang, X.; Chen, A.; PourNejatian, N.; Shin, H.C.; Smith, K.E.; Parisien, C.; Compas, C.; Martin,C.; Flores, M.G.; Zhang, Y.; et al. Gatortron: A large clinical language model to unlock patientinformation from unstructured electronic health records.arXiv preprint arXiv:2203.035402022.33.LEE, A. What Are Large Language Models Used For?https://blogs.nvidia.com/blog/2023/01/26/what-are-large-language-models-used-for/, 2023. [Online; Accessed 06-17-2023].34.BigScience Blog.https://bigscience.huggingface.co/blog/bloom, 2023. [Online; Accessed06-17-2023].35.Lee, S.; Kim, W.J.; Ye, J.C. LLM Itself Can Read and Generate CXR Images.arXiv preprintarXiv:2305.114902023.36.Jelinek, F.Statistical methods for speech recognition; MIT press, 1998.37.Rosenfeld, R. Two decades of statistical language modeling: Where do we go from here?Proceedings of the IEEE2000,88, 1270–1278.38.Peters, M.E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; Zettlemoyer, L. “Deepcontextualized word representations.In Proceedings of the 2018 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT2018, New Orleans, Louisiana,USA.39.Kaplan, J.; McCandlish, S.; Henighan, T.; Brown, T.B.; Chess, B.; Child, R.; Gray, S.; Radford, A.;Wu, J.; Amodei, D. Scaling laws for neural language models.arXiv preprint arXiv:2001.083612020.40.Bengio, Y.; Senécal, J.S. Adaptive importance sampling to accelerate training of a neuralprobabilistic language model. Technical report, IDIAP, 2003.41.Chen, X.; Wang, X.; Changpinyo, S.; Piergiovanni, A.; Padlewski, P.; Salz, D.; Goodman, S.;Grycner, A.; Mustafa, B.; Beyer, L.; et al. Pali: A jointly-scaled multilingual language-imagemodel.arXiv preprint arXiv:2209.067942022.42.Scheible, R.; Thomczyk, F.; Tippmann, P.; Jaravine, V.; Boeker, M. GottBERT: a pure Germanlanguage model.arXiv preprint arXiv:2012.021102020.43.Soltan, S.; Ananthakrishnan, S.; FitzGerald, J.; Gupta, R.; Hamza, W.; Khan, H.; Peris, C.; Rawls,S.; Rosenbaum, A.; Rumshisky, A.; et al. Alexatm 20b: Few-shot learning using a large-scalemultilingual seq2seq model.arXiv preprint arXiv:2208.014482022.44.Dossou, B.F.; Tonja, A.L.; Yousuf, O.; Osei, S.; Oppong, A.; Shode, I.; Awoyomi, O.O.; Emezue,C.C. AfroLM: A Self-Active Learning-based Multilingual Pretrained Language Model for 23African Languages.arXiv preprint arXiv:2211.032632022.45.Li, Y.; Mamouei, M.; Salimi-Khorshidi, G.; Rao, S.; Hassaine, A.; Canoy, D.; Lukasiewicz, T.;Rahimi, K. Hi-BEHRT: Hierarchical Transformer-based model for accurate prediction of clinicalevents using multimodal longitudinal electronic health records.IEEE Journal of Biomedical andHealth Informatics2022.46.Wang, X.; Xu, X.; Tong, W.; Roberts, R.; Liu, Z. InferBERT: a transformer-based causal inferenceframework for enhancing pharmacovigilance.Frontiers in Artiﬁcial Intelligence2021,4, 659622.47.Arora, A.; Arora, A. The promise of large language models in health care.The Lancet2023,401, 641.48.Santos, T.; Tariq, A.; Das, S.; Vayalpati, K.; Smith, G.H.; Trivedi, H.; Banerjee, I. PathologyBERT–Pre-trained Vs. A New Transformer Language Model for Pathology Domain.arXiv preprintarXiv:2205.068852022.49.Jaiswal, A.; Tang, L.; Ghosh, M.; Rousseau, J.F.; Peng, Y.; Ding, Y. RadBERT-CL: factually-awarecontrastive learning for radiology report classiﬁcation. In Proceedings of the Machine Learningfor Health. PMLR, 2021, pp. 196–208.50.Peng, Y.; Yan, S.; Lu, Z. Transfer learning in biomedical natural language processing: anevaluation of BERT and ELMo on ten benchmarking datasets.arXiv preprint arXiv:1906.054742019.

--- PAGE 22 ---
22 of 2651.Beltagy, I.; Lo, K.; Cohan, A. SciBERT: A pretrained language model for scientiﬁc text.arXivpreprint arXiv:1903.106762019.52.Lin, C.; Miller, T.; Dligach, D.; Bethard, S.; Savova, G. EntityBERT: Entity-centric masking strat-egy for model pretraining for the clinical domain. Association for Computational Linguistics(ACL), 2021.53.Yan, A.; McAuley, J.; Lu, X.; Du, J.; Chang, E.Y.; Gentili, A.; Hsu, C.N. RadBERT: Adapt-ing transformer-based language models to radiology.Radiology: Artiﬁcial Intelligence2022,4, e210258.54.Gao, L.; Biderman, S.; Black, S.; Golding, L.; Hoppe, T.; Foster, C.; Phang, J.; He, H.; Thite, A.;Nabeshima, N.; et al. The pile: An 800gb dataset of diverse text for language modeling.arXivpreprint arXiv:2101.000272020.55.Araci, D. Finbert: Financial sentiment analysis with pre-trained language models.arXiv preprintarXiv:1908.100632019.56.Xie, Q.; Han, W.; Zhang, X.; Lai, Y.; Peng, M.; Lopez-Lira, A.; Huang, J. PIXIU: A Large LanguageModel, Instruction Data and Evaluation Benchmark for Finance.arXiv preprint arXiv:2306.054432023.57.Alayrac, J.B.; Donahue, J.; Luc, P.; Miech, A.; Barr, I.; Hasson, Y.; Lenc, K.; Mensch, A.; Millican,K.; Reynolds, M.; et al. Flamingo: a visual language model for few-shot learning.Advances inNeural Information Processing Systems2022,35, 23716–23736.58.Monajatipoor, M.; Rouhsedaghat, M.; Li, L.H.; Jay Kuo, C.C.; Chien, A.; Chang, K.W. Berthop:An effective vision-and-language model for chest x-ray disease diagnosis. In Proceedingsof the Medical Image Computing and Computer Assisted Intervention–MICCAI 2022: 25thInternational Conference, Singapore, September 18–22, 2022, Proceedings, Part V. Springer, 2022,pp. 725–734.59.Kuo, W.; Cui, Y.; Gu, X.; Piergiovanni, A.; Angelova, A. F-VLM: Open-Vocabulary ObjectDetection upon Frozen Vision and Language Models.arXiv preprint arXiv:2209.156392022.60.Hong, Y.; Wu, Q.; Qi, Y.; Rodriguez-Opazo, C.; Gould, S. Vln bert: A recurrent vision-and-language bert for navigation. In Proceedings of the Proceedings of the IEEE/CVF conferenceon Computer Vision and Pattern Recognition, 2021, pp. 1643–1653.61.Thrush, T.; Jiang, R.; Bartolo, M.; Singh, A.; Williams, A.; Kiela, D.; Ross, C. Winoground:Probing vision and language models for visio-linguistic compositionality. In Proceedings of theProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022,pp. 5238–5248.62.Wang, J.; Hu, X.; Zhang, P.; Li, X.; Wang, L.; Zhang, L.; Gao, J.; Liu, Z. Minivlm: A smaller andfaster vision-language model.arXiv preprint arXiv:2012.069462020.63.Luo, Z.; Xu, C.; Zhao, P.; Sun, Q.; Geng, X.; Hu, W.; Tao, C.; Ma, J.; Lin, Q.; Jiang, D. WizardCoder:Empowering Code Large Language Models with Evol-Instruct.arXiv preprint arXiv:2306.085682023.64.Nijkamp, E.; Pang, B.; Hayashi, H.; Tu, L.; Wang, H.; Zhou, Y.; Savarese, S.; Xiong, C. Codegen:An open large language model for code with multi-turn program synthesis.arXiv preprintarXiv:2203.134742022.65.Jain, N.; Vaidyanath, S.; Iyer, A.; Natarajan, N.; Parthasarathy, S.; Rajamani, S.; Sharma, R.Jigsaw: Large language models meet program synthesis. In Proceedings of the Proceedings ofthe 44th International Conference on Software Engineering, 2022, pp. 1219–1231.66.Wang, Y.; Wang, W.; Joty, S.; Hoi, S.C. Codet5: Identiﬁer-aware uniﬁed pre-trained encoder-decoder models for code understanding and generation.arXiv preprint arXiv:2109.008592021.67.Hoffmann, J.; Borgeaud, S.; Mensch, A.; Buchatskaya, E.; Cai, T.; Rutherford, E.; Casas, D.d.L.;Hendricks, L.A.; Welbl, J.; Clark, A.; et al. Training compute-optimal large language models.arXiv preprint arXiv:2203.155562022.68.Chen, X.; Fang, H.; Lin, T.Y.; Vedantam, R.; Gupta, S.; Dollár, P.; Zitnick, C.L. Microsoft cococaptions: Data collection and evaluation server.arXiv preprint arXiv:1504.003252015.69.Thapliyal, A.V.; Pont-Tuset, J.; Chen, X.; Soricut, R. Crossmodal-3600: A massively multilingualmultimodal evaluation dataset.arXiv preprint arXiv:2205.125222022.70.Xue, L.; Constant, N.; Roberts, A.; Kale, M.; Al-Rfou, R.; Siddhant, A.; Barua, A.; Raffel, C. mT5:A massively multilingual pre-trained text-to-text transformer.arXiv preprint arXiv:2010.119342020.71.Laurençon, H.; Saulnier, L.; Wang, T.; Akiki, C.; Villanova del Moral, A.; Le Scao, T.; Von Werra,L.; Mou, C.; González Ponferrada, E.; Nguyen, H.; et al. The bigscience roots corpus: A 1.6

--- PAGE 23 ---
23 of 26tb composite multilingual dataset.Advances in Neural Information Processing Systems2022,35, 31809–31826.72.Lhoest, Q.; Villanova del Moral, A.; Jernite, Y.; Thakur, A.; von Platen, P.; Patil, S.; Chaumond, J.;Drame, M.; Plu, J.; Tunstall, L.; et al. Datasets: A Community Library for Natural LanguageProcessing. In Proceedings of the Proceedings of the 2021 Conference on Empirical Methodsin Natural Language Processing: System Demonstrations; Association for ComputationalLinguistics: Online and Punta Cana, Dominican Republic, 2021; pp. 175–184.https://doi.org/10.18653/v1/2021.emnlp-demo.21.73.Liang, P.; Bommasani, R.; Lee, T.; Tsipras, D.; Soylu, D.; Yasunaga, M.; Zhang, Y.; Narayanan, D.;Wu, Y.; Kumar, A.; et al. Holistic evaluation of language models.arXiv preprint arXiv:2211.091102022.74.Adelani, D.I.; Alabi, J.O.; Fan, A.; Kreutzer, J.; Shen, X.; Reid, M.; Ruiter, D.; Klakow, D.;Nabende, P.; Chang, E.; et al. A few thousand translations go a long way! leveraging pre-trainedmodels for african news translation.arXiv preprint arXiv:2205.020222022.75.Ogueji, K.; Zhu, Y.; Lin, J. Small Data? No Problem! Exploring the Viability of Pretrained Multi-lingual Language Models for Low-resourced Languages. In Proceedings of the Proceedingsof the 1st Workshop on Multilingual Representation Learning; Association for ComputationalLinguistics: Punta Cana, Dominican Republic, 2021; pp. 116–126.https://doi.org/10.18653/v1/2021.mrl-1.11.76.Shode, I.; Adelani, D.I.; Feldman, A. yosm: A new yoruba sentiment corpus for movie reviews.arXiv preprint arXiv:2204.097112022.77.Herlihy, C.; Rudinger, R. MedNLI is not immune: Natural language inference artifacts in theclinical domain.arXiv preprint arXiv:2106.014912021.78.Wang, Y.; Fu, S.; Shen, F.; Henry, S.; Uzuner, O.; Liu, H.; et al. The 2019 n2c2/ohnlp track onclinical semantic textual similarity: overview.JMIR medical informatics2020,8, e23375.79.Pampari, A.; Raghavan, P.; Liang, J.; Peng, J. emrqa: A large corpus for question answering onelectronic medical records.arXiv preprint arXiv:1809.007322018.80.Herrett, E.; Gallagher, A.M.; Bhaskaran, K.; Forbes, H.; Mathur, R.; Van Staa, T.; Smeeth, L. Dataresource proﬁle: clinical practice research datalink (CPRD).International journal of epidemiology2015,44, 827–836.81.Conrad, N.; Judge, A.; Tran, J.; Mohseni, H.; Hedgecott, D.; Crespillo, A.P.; Allison, M.; Hem-ingway, H.; Cleland, J.G.; McMurray, J.J.; et al. Temporal trends and patterns in heart failureincidence: a population-based study of 4 million individuals.The Lancet2018,391, 572–580.82.Kuan, V.; Denaxas, S.; Gonzalez-Izquierdo, A.; Direk, K.; Bhatti, O.; Husain, S.; Sutaria, S.;Hingorani, M.; Nitsch, D.; Parisinos, C.A.; et al. A chronological map of 308 physical and mentalhealth conditions from 4 million individuals in the English National Health Service.The LancetDigital Health2019,1, e63–e77.83.XingqiaoWang. DeepCausalPV-master.https://github.com/XingqiaoWang/DeepCausalPV-master, 2021. [Online; Accessed 06-17-2023].84.Johnson, A.E.; Pollard, T.J.; Greenbaum, N.R.; Lungren, M.P.; Deng, C.y.; Peng, Y.; Lu, Z.; Mark,R.G.; Berkowitz, S.J.; Horng, S. MIMIC-CXR-JPG, a large publicly available database of labeledchest radiographs.arXiv preprint arXiv:1901.070422019.85.Irvin, J.; Rajpurkar, P.; Ko, M.; Yu, Y.; Ciurea-Ilcus, S.; Chute, C.; Marklund, H.; Haghgoo, B.;Ball, R.; Shpanskaya, K.; et al. Chexpert: A large chest radiograph dataset with uncertaintylabels and expert comparison. In Proceedings of the Proceedings of the AAAI conference onartiﬁcial intelligence, 2019, Vol. 33, pp. 590–597.86.Li, J.; Sun, Y.; Johnson, R.J.; Sciaky, D.; Wei, C.H.; Leaman, R.; Davis, A.P.; Mattingly, C.J.;Wiegers, T.C.; Lu, Z. BioCreative V CDR task corpus: a resource for chemical disease relationextraction.Database2016,2016.87.Do˘ gan, R.I.; Leaman, R.; Lu, Z. NCBI disease corpus: a resource for disease name recognitionand concept normalization.Journal of biomedical informatics2014,47, 1–10.88.Smith, L.; Tanabe, L.K.; Kuo, C.J.; Chung, I.; Hsu, C.N.; Lin, Y.S.; Klinger, R.; Friedrich, C.M.;Ganchev, K.; Torii, M.; et al. Overview of BioCreative II gene mention recognition.Genomebiology2008,9, 1–19.89.Jin, Q.; Dhingra, B.; Liu, Z.; Cohen, W.W.; Lu, X. Pubmedqa: A dataset for biomedical researchquestion answering.arXiv preprint arXiv:1909.061462019.90.Nye, B.; Li, J.J.; Patel, R.; Yang, Y.; Marshall, I.J.; Nenkova, A.; Wallace, B.C. A corpus withmulti-level annotations of patients, interventions and outcomes to support language processing

--- PAGE 24 ---
24 of 26for medical literature. In Proceedings of the Proceedings of the conference. Association forComputational Linguistics. Meeting. NIH Public Access, 2018, Vol. 2018, p. 197.91.Luan, Y.; He, L.; Ostendorf, M.; Hajishirzi, H. Multi-task identiﬁcation of entities, relations, andcoreference for scientiﬁc knowledge graph construction.arXiv preprint arXiv:1808.096022018.92.Jurgens, D.; Kumar, S.; Hoover, R.; McFarland, D.; Jurafsky, D. Measuring the evolution of ascientiﬁc ﬁeld through citation frames.Transactions of the Association for Computational Linguistics2018,6, 391–406.93.Ammar, W.; Groeneveld, D.; Bhagavatula, C.; Beltagy, I.; Crawford, M.; Downey, D.; Dunkel-berger, J.; Elgohary, A.; Feldman, S.; Ha, V.; et al. Construction of the literature graph in semanticscholar.arXiv preprint arXiv:1805.022622018.94.Uzuner, Ö.; Luo, Y.; Szolovits, P. Evaluating the state-of-the-art in automatic de-identiﬁcation.Journal of the American Medical Informatics Association2007,14, 550–563.95.Uzuner, Ö.; South, B.R.; Shen, S.; DuVall, S.L. 2010 i2b2/VA challenge on concepts, assertions,and relations in clinical text.Journal of the American Medical Informatics Association2011,18, 552–556.96.Sun, W.; Rumshisky, A.; Uzuner, O. Evaluating temporal relations in clinical text: 2012 i2b2challenge.Journal of the American Medical Informatics Association2013,20, 806–813.97.Stubbs, A.; Uzuner, Ö. Annotating longitudinal clinical narratives for de-identiﬁcation: The2014 i2b2/UTHealth corpus.Journal of biomedical informatics2015,58, S20–S29.98.Romanov, A.; Shivade, C. Lessons from natural language inference in the clinical domain.arXivpreprint arXiv:1808.067522018.99.Johnson, A.E.; Pollard, T.J.; Shen, L.; Lehman, L.w.H.; Feng, M.; Ghassemi, M.; Moody, B.;Szolovits, P.; Anthony Celi, L.; Mark, R.G. MIMIC-III, a freely accessible critical care database.Scientiﬁc data2016,3, 1–9.100.Krallinger, M.; Rabal, O.; Leitner, F.; Vazquez, M.; Salgado, D.; Lu, Z.; Leaman, R.; Lu, Y.; Ji,D.; Lowe, D.M.; et al. The CHEMDNER corpus of chemicals and drugs and its annotationprinciples.Journal of cheminformatics2015,7, 1–17.101.Tsatsaronis, G.; Balikas, G.; Malakasiotis, P.; Partalas, I.; Zschunke, M.; Alvers, M.R.; Weis-senborn, D.; Krithara, A.; Petridis, S.; Polychronopoulos, D.; et al. An overview of the BIOASQlarge-scale biomedical semantic indexing and question answering competition.BMC bioinfor-matics2015,16, 1–28.102.Peters, M.E.; Ammar, W.; Bhagavatula, C.; Power, R. Semi-supervised sequence tagging withbidirectional language models.arXiv preprint arXiv:1705.001082017.103.Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y.; Li, W.; Liu, P.J.Exploring the Limits of Transfer Learning with a Uniﬁed Text-to-Text Transformer.Journal ofMachine Learning Research2020,21, 1–67.104.Shah, R.; Chawla, K.; Eidnani, D.; Shah, A.; Du, W.; Chava, S.; Raman, N.; Smiley, C.; Chen,J.; Yang, D. When FLUE Meets FLANG: Benchmarks and Large Pretrained Language Modelfor Financial Domain. In Proceedings of the Proceedings of the 2022 Conference on EmpiricalMethods in Natural Language Processing; Association for Computational Linguistics: AbuDhabi, United Arab Emirates, 2022; pp. 2322–2335.105.Zhu, Y.; Kiros, R.; Zemel, R.; Salakhutdinov, R.; Urtasun, R.; Torralba, A.; Fidler, S. Aligningbooks and movies: Towards story-like visual explanations by watching movies and readingbooks. In Proceedings of the Proceedings of the IEEE international conference on computervision, 2015, pp. 19–27.106.Rajaraman, A.; Ullman, J.D.Mining of massive datasets; Cambridge University Press, 2011.107.Wang, A.; Pruksachatkun, Y.; Nangia, N.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; Bowman,S. Superglue: A stickier benchmark for general-purpose language understanding systems.Advances in neural information processing systems2019,32.108.Muennighoff, N.; Tazi, N.; Magne, L.; Reimers, N. MTEB: Massive text embedding benchmark.arXiv preprint arXiv:2210.073162022.109.Muennighoff, N.; Wang, T.; Sutawika, L.; Roberts, A.; Biderman, S.; Scao, T.L.; Bari, M.S.; Shen,S.; Yong, Z.X.; Schoelkopf, H.; et al. Crosslingual generalization through multitask ﬁnetuning.arXiv preprint arXiv:2211.017862022.110.Reuters Corpora (RCV1, RCV2, TRC2).https://trec.nist.gov/data/reuters/reuters.html, 2004.[Online; Accessed 06-17-2023].111.Malo, P.; Sinha, A.; Takala, P.; Korhonen, P.; Wallenius, J. FinancialPhraseBank-v1. 0, 2013.

--- PAGE 25 ---
25 of 26112.Maia, M.; Handschuh, S.; Freitas, A.; Davis, B.; McDermott, R.; Zarrouk, M.; Balahur, A.Www’18 open challenge: ﬁnancial opinion mining and question answering. In Proceedings ofthe Companion proceedings of the the web conference 2018, 2018, pp. 1941–1942.113.Jia, C.; Yang, Y.; Xia, Y.; Chen, Y.T.; Parekh, Z.; Pham, H.; Le, Q.; Sung, Y.H.; Li, Z.; Duerig, T.Scaling up visual and vision-language representation learning with noisy text supervision. InProceedings of the International conference on machine learning. PMLR, 2021, pp. 4904–4916.114.Peng, Y.; Wang, X.; Lu, L.; Bagheri, M.; Summers, R.; Lu, Z. NegBio: a high-performance tool fornegation and uncertainty detection in radiology reports.AMIA Summits on Translational ScienceProceedings2018,2018, 188.115.Gupta, A.; Dollar, P.; Girshick, R. Lvis: A dataset for large vocabulary instance segmentation. InProceedings of the Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, 2019, pp. 5356–5364.116.Changpinyo, S.; Sharma, P.; Ding, N.; Soricut, R. Conceptual 12m: Pushing web-scale image-textpre-training to recognize long-tail visual concepts. In Proceedings of the Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 3558–3568.117.Ordonez, V.; Kulkarni, G.; Berg, T. Im2text: Describing images using 1 million captionedphotographs.Advances in neural information processing systems2011,24.118.Schuhmann, C.; Vencu, R.; Beaumont, R.; Kaczmarczyk, R.; Mullis, C.; Katta, A.; Coombes, T.;Jitsev, J.; Komatsuzaki, A. Laion-400m: Open dataset of clip-ﬁltered 400 million image-text pairs.arXiv preprint arXiv:2111.021142021.119.Zhou, L.; Palangi, H.; Zhang, L.; Hu, H.; Corso, J.; Gao, J. Uniﬁed vision-language pre-trainingfor image captioning and vqa. In Proceedings of the Proceedings of the AAAI conference onartiﬁcial intelligence, 2020, Vol. 34, pp. 13041–13049.120.Goyal, Y.; Khot, T.; Summers-Stay, D.; Batra, D.; Parikh, D. Making the v in vqa matter: Elevatingthe role of image understanding in visual question answering. In Proceedings of the Proceedingsof the IEEE conference on computer vision and pattern recognition, 2017, pp. 6904–6913.121.Suhr, A.; Zhou, S.; Zhang, A.; Zhang, I.; Bai, H.; Artzi, Y. A corpus for reasoning about naturallanguage grounded in photographs.arXiv preprint arXiv:1811.004912018.122.Anderson, P.; Wu, Q.; Teney, D.; Bruce, J.; Johnson, M.; Sünderhauf, N.; Reid, I.; Gould, S.; VanDen Hengel, A. Vision-and-language navigation: Interpreting visually-grounded navigationinstructions in real environments. In Proceedings of the Proceedings of the IEEE conference oncomputer vision and pattern recognition, 2018, pp. 3674–3683.123.Qi, Y.; Wu, Q.; Anderson, P.; Wang, X.; Wang, W.Y.; Shen, C.; Hengel, A.v.d. Reverie: Remoteembodied visual referring expression in real indoor environments. In Proceedings of theProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020,pp. 9982–9991.124.Chang, A.; Dai, A.; Funkhouser, T.; Halber, M.; Niessner, M.; Savva, M.; Song, S.; Zeng, A.;Zhang, Y. Matterport3D: Learning from RGB-D Data in Indoor Environments.InternationalConference on 3D Vision (3DV)2017.125.Chen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Pinto, H.P.d.O.; Kaplan, J.; Edwards, H.; Burda, Y.;Joseph, N.; Brockman, G.; et al. Evaluating large language models trained on code.arXivpreprint arXiv:2107.033742021.126.Liu, J.; Xia, C.S.; Wang, Y.; Zhang, L. Is your code generated by chatgpt really correct? rigorousevaluation of large language models for code generation.arXiv preprint arXiv:2305.012102023.127.Austin, J.; Odena, A.; Nye, M.; Bosma, M.; Michalewski, H.; Dohan, D.; Jiang, E.; Cai, C.; Terry,M.; Le, Q.; et al. Program synthesis with large language models.arXiv preprint arXiv:2108.077322021.128.Lai, Y.; Li, C.; Wang, Y.; Zhang, T.; Zhong, R.; Zettlemoyer, L.; Yih, S.W.t.; Fried, D.; Wang, S.; Yu,T. DS-1000: A natural and reliable benchmark for data science code generation.arXiv preprintarXiv:2211.115012022.129.Xu, C.; Sun, Q.; Zheng, K.; Geng, X.; Zhao, P.; Feng, J.; Tao, C.; Jiang, D. Wizardlm: Empoweringlarge language models to follow complex instructions.arXiv preprint arXiv:2304.122442023.130.Husain, H.; Wu, H.H.; Gazit, T.; Allamanis, M.; Brockschmidt, M. Codesearchnet challenge:Evaluating the state of semantic code search.arXiv preprint arXiv:1909.094362019.131.Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I.; et al. Language models areunsupervised multitask learners.OpenAI blog2019,1, 9.132.Koubaa, A. GPT-4 vs. GPT-3.5: A concise showdown2023.133.GPT-3.https://en.wikipedia.org/wiki/GPT-3#GPT-3.5, 2023. [Online; Accessed 06-17-2023].

--- PAGE 26 ---
26 of 26134.Zhang, Y.; Sun, S.; Galley, M.; Chen, Y.C.; Brockett, C.; Gao, X.; Gao, J.; Liu, J.; Dolan, B. Dialogpt:Large-scale generative pre-training for conversational response generation.arXiv preprintarXiv:1911.005362019.135.Microsoft Research-DialoGPT.https://www.microsoft.com/en-us/research/project/large-scale-pretraining-for-response-generation/, 2019. [Online; Accessed 06-17-2023].136.Padmanabha, N.H. A step-by-step guide to running Vicuna-13B Large Language Model onyour GPU / CPU machine.https://www.linkedin.com/pulse/step-by-step-guide-running-vicuna-13b-large-language-nischal/, 2023. [Online; Accessed 06-17-2023].137.Lieber, O.; Sharir, O.; Lenz, B.; Shoham, Y. Jurassic-1: Technical details and evaluation.WhitePaper. AI21 Labs2021,1.138.Zeng, A.; Liu, X.; Du, Z.; Wang, Z.; Lai, H.; Ding, M.; Yang, Z.; Xu, Y.; Zheng, W.; Xia, X.; et al.Glm-130b: An open bilingual pre-trained model.arXiv preprint arXiv:2210.024142022.139.WIKIPEDIA.https://en.wikipedia.org/wiki/PaLM, 2022. [Online; Accessed 06-17-2023].140.Khadhraoui, M.; Bellaaj, H.; Ammar, M.B.; Hamam, H.; Jmaiel, M. Survey of BERT-base modelsfor scientiﬁc text classiﬁcation: COVID-19 case study.Applied Sciences2022,12, 2891.141.Smith, S.; Patwary, M.; Norick, B.; LeGresley, P.; Rajbhandari, S.; Casper, J.; Liu, Z.; Prabhumoye,S.; Zerveas, G.; Korthikanti, V.; et al. Using deepspeed and megatron to train megatron-turingnlg 530b, a large-scale generative language model.arXiv preprint arXiv:2201.119902022.142.ClinicalBERT.https://huggingface.co/medicalai/ClinicalBERT. [Online; Accessed 06-17-2023].143.Hugging Face.https://huggingface.co/UFNLP/gatortron-base. [Online; Accessed 06-17-2023].144.Alford, A. Google Trains 280 Billion Parameter AI Language Model Gopher.https://www.infoq.com/news/2022/01/deepmind-gopher/, 2022. [Online; Accessed 06-17-2023].145.Hoffmann, J.; Borgeaud, S.; Mensch, A.; Buchatskaya, E.; Cai, T.; Rutherford, E.; de Las Casas,D.; Hendricks, L.A.; Welbl, J.; Clark, A.; et al. An empirical analysis of compute-optimal largelanguage model training.Advances in Neural Information Processing Systems2022,35, 30016–30030.
