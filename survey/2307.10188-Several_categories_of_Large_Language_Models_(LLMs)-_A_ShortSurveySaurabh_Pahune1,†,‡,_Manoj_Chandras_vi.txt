# 2307.10188.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/survey/2307.10188.pdf
# Kích thước tệp: 776838 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Một số danh mục của Mô hình Ngôn ngữ Lớn (LLMs): Một Khảo sát Ngắn

Saurabh Pahune1,†,‡, Manoj Chandrasekharan2

1Cardinal Health, Dublin OH 43017, USA; Email: saurabh.pahune@cardinalhealth.com, Tel.:+1-901-691-7551
2Email: manoj.c@memphis.edu

Tóm tắt:
Các Mô hình Ngôn ngữ Lớn (LLMs) đã trở thành các công cụ hiệu quả cho xử lý ngôn ngữ tự nhiên và đã được sử dụng trong nhiều lĩnh vực khác nhau. Bài luận này cung cấp một tóm tắt ngắn gọn về các phân loại phụ khác nhau của LLM. Khảo sát nhấn mạnh các phát triển gần đây và những nỗ lực được thực hiện cho các loại LLM khác nhau, bao gồm LLMs tài chính dựa trên nhiệm vụ, LLMs ngôn ngữ đa ngôn ngữ, LLMs y sinh và lâm sàng, LLMs ngôn ngữ thị giác, và các mô hình ngôn ngữ mã. Khảo sát cung cấp một tóm tắt tổng quát về các phương pháp, thuộc tính, bộ dữ liệu, mô hình transformer, và các chỉ số so sánh được áp dụng trong từng danh mục LLMs. Hơn nữa, nó làm nổi bật các vấn đề chưa được giải quyết trong lĩnh vực phát triển chatbot và trợ lý ảo, chẳng hạn như tăng cường xử lý ngôn ngữ tự nhiên, nâng cao trí tuệ chatbot, và giải quyết các tình huống khó xử về đạo đức và pháp lý. Mục đích của nghiên cứu này là cung cấp cho độc giả, nhà phát triển, học giả, và người dùng quan tâm đến công nghệ chatbot và trợ lý thông minh ảo dựa trên LLM những thông tin hữu ích và hướng phát triển tương lai. Khảo sát này làm sáng tỏ các khả năng của LLMs và đặt nền móng cho nghiên cứu và tiến bộ bổ sung trong lĩnh vực này bằng cách xem xét bối cảnh, lợi ích, và nhược điểm của LLMs nói chung cũng như những hàm ý của các mô hình LLM khác nhau. Do đó, bài báo này cung cấp thông tin quan trọng và hướng phát triển tương lai. Mục tiêu của chúng tôi là xem xét lịch sử của LLM, các ưu điểm và nhược điểm của LLMs nói chung, các loại mô hình LLM khác nhau (ví dụ: tài chính, lâm sàng, đa ngôn ngữ, mã, thị giác), và tất cả những điều này có ý nghĩa gì cho tương lai.

Từ khóa: Xử lý ngôn ngữ tự nhiên; mô hình ngôn ngữ lớn (LLM); LLMs tài chính; LLMs ngôn ngữ đa ngôn ngữ; LLMs y sinh và lâm sàng; LLMs ngôn ngữ thị giác; mô hình ngôn ngữ mã; mô hình transformer; bộ dữ liệu; trợ lý thông minh ảo

1. Giới thiệu

Nguồn gốc của các mô hình ngôn ngữ AI đầu tiên có thể được tìm thấy trong lịch sử ban đầu của AI. Một trong những ví dụ lâu đời nhất của mô hình ngôn ngữ AI là mô hình ngôn ngữ ELIZA, ra mắt vào năm 1966 tại MIT[1,2]. LLM là sự phát triển của ý tưởng mô hình ngôn ngữ trong AI, tăng cường đáng kể lượng dữ liệu được sử dụng cho suy luận và huấn luyện. Kết quả là, khả năng của mô hình AI được tăng cường đáng kể. Một LLM thường bao gồm ít nhất một tỷ tham số, mặc dù không có kích thước xác định cho bộ dữ liệu phải được sử dụng để huấn luyện. Một mô hình học sâu được huấn luyện gọi là mô hình ngôn ngữ lớn có thể đọc và tạo ra văn bản theo cách tương tự như con người. Mọi thứ được thực hiện phía sau hậu trường bằng cách sử dụng một mô hình transformer có kích thước lớn. Vào năm 2017[3] "Attention is All You Need," để thiết lập một mô hình transformer (chữ 'T' trong tất cả các mô hình GPT). Nó dựa trên cơ chế attention, loại bỏ hoàn toàn tính lặp lại và tích chập. Các mô hình ngôn ngữ Transformer sử dụng một kiến trúc mạng nơ-ron sâu gọi là Transformer và chúng được huấn luyện để dự đoán các từ bị che (tức là điền vào chỗ trống) hoặc các từ sắp tới trong văn bản[4]. Uszkoreit et al. mô tả Transformer, một thiết kế mạng nơ-ron tiên tiến dựa trên cơ chế self-attention nhằm đặc biệt hiệu quả trong việc hiểu ngôn ngữ[5]. Các mô hình ngôn ngữ Transformer đã cách mạng hóa lĩnh vực xử lý ngôn ngữ tự nhiên (NLP) kể từ khi được giới thiệu vào năm 2018[6]. Các mô hình ngôn ngữ Transformer đã nhận được sự chú ý rộng rãi từ công chúng, tuy nhiên văn bản được tạo ra thường gây ngạc nhiên ngay cả đối với các nhà nghiên cứu NLP[4,7]. Theo nghiên cứu gần đây, một số LLMs hàng đầu được công bố và phát hành trong những năm gần đây (ví dụ: GPT-3/4[8], LLaMA[9], PaLM[10], MiniGPT-4[11], FinGPT[12], OPT[13], BERT[14], Bloomberggpt[15], BLOOM 176B[16], GPT NEO-X[17], RoBERTa [18], Dolly2.0[19] ;)[10,13,20–22]. Đối với các ứng dụng từ tìm kiếm web và chatbot đến

--- TRANG 2 ---
2 của 26 phân tích tài liệu y tế và tài chính, nhiều mô hình ngôn ngữ được sử dụng trong doanh nghiệp[4,15,23].

Hình 1. Phân bố các mô hình ngôn ngữ

Như trong Hình 1, nhiều mô hình ngôn ngữ đã xuất hiện. Các mô hình ngôn ngữ có nhiều tham số và sức mạnh xử lý lớn được gọi chung là "Mô hình Ngôn ngữ Lớn" (LLM)[24]. Trong khi đó, một loại mô hình ngôn ngữ được biết đến là mô hình ngôn ngữ thống kê (SLM) sử dụng các phương pháp thống kê để đưa ra giá trị xác suất cho các chuỗi từ trong một ngôn ngữ. Nó dựa trên khái niệm rằng bằng cách kiểm tra tần suất và các mẫu được tìm thấy trong một kho văn bản lớn, có thể dự đoán khả năng một từ cụ thể xuất hiện trong một tình huống cụ thể[25]. Trong Mô hình Ngôn ngữ Nơ-ron (NLM), phân bố xác suất của các chuỗi từ trong một ngôn ngữ được mô hình hóa bằng cách sử dụng các topo mạng nơ-ron. NLMs được tạo ra để nắm bắt các mối quan hệ từ phức tạp và tạo ra văn bản phù hợp với ngữ cảnh xung quanh[26,27]. Thuật ngữ "Mô hình Ngôn ngữ Transformer" (TLMs) được sử dụng để mô tả các mô hình ngôn ngữ đặc biệt sử dụng kiến trúc Transformer[3]. Thuật ngữ "mô hình ngôn ngữ được tiền huấn luyện" (PLMs) đề cập đến các mô hình ngôn ngữ đã được huấn luyện theo cách không giám sát trên các kho dữ liệu lớn trước khi được điều chỉnh cho các nhiệm vụ downstream cụ thể. Bằng cách trích xuất các mẫu và cấu trúc từ khối lượng lớn dữ liệu văn bản, những mô hình này học được các biểu diễn của ngôn ngữ tổng quát. Trong những năm gần đây, trong lĩnh vực chăm sóc sức khỏe, có nhiều mô hình transformer y sinh và lâm sàng khác nhau có sẵn để trích xuất khái niệm lâm sàng và mối quan hệ y tế[28]. BioBERT[29], ClinicalBERT[30], BioMegatron[31], GatorTron-base[32], GatorTron-medium[32], GatorTron-large [32]. Vào năm 2021, một trong những mô hình lớn nhất thế giới để hiểu đọc và suy luận ngôn ngữ tự nhiên, Megatron-Turing Natural Language Generation 530B được tạo ra bởi Nvidia

Hình 2. Đám mây từ hiển thị tần suất các thuật ngữ được sử dụng trong các bài báo chúng tôi đã xem xét.

--- TRANG 3 ---
3 của 26 và Microsoft để hỗ trợ các nhiệm vụ như tóm tắt và tạo nội dung[33]. HuggingFace đã công bố BLOOM năm ngoái, một mô hình ngôn ngữ lớn mở có thể tạo ra văn bản trong hơn một chục ngôn ngữ lập trình ngoài 46 ngôn ngữ tự nhiên khác nhau và 13 ngôn ngữ lập trình[34]. Các mô hình ngôn ngữ thị giác [35] một họ các mô hình Ngôn ngữ Thị giác (VLM) có thể được thích ứng nhanh chóng với các nhiệm vụ mới chỉ sử dụng một số ít ví dụ được chú thích là một thách thức mở cho nghiên cứu học máy đa phương thức. Bảng 1 trích dẫn công trình được đề cập bởi tác giả tương ứng trong lĩnh vực mô hình ngôn ngữ này.

Hình 3. Sơ đồ trực quan hóa các bài báo nghiên cứu

Trong Hình 3, bằng cách sử dụng các đường kết nối để biểu diễn trực quan các trích dẫn cho công trình này, nó thể hiện mối liên kết. Các nguồn tài nguyên nghiên cứu hữu ích và trạng thái dự án được bao gồm trong mạng trích dẫn dựa trên thời gian này để sắp xếp bài đánh giá tài liệu này. Nghiên cứu được trích dẫn đề cập đến nhiều chủ đề, bao gồm các tính năng mô hình, bộ dữ liệu, mô hình transformer, và các tiêu chuẩn để đánh giá hiệu suất LLM.

Phần còn lại của bài báo được tổ chức như sau. Phần 2.1 trình bày các công trình liên quan trước đây có nhiều phiên bản khác nhau của mô hình LLM (Dựa trên tài chính, Dựa trên lâm sàng, Dựa trên thị giác, Dựa trên mã, Dựa trên đa ngôn ngữ) và các bảng tương ứng. Trong Phần 3, định nghĩa cấu hình của các mô hình khác nhau với hàng tỷ tham số và tạo ra các bảng phân loại để thảo luận về phương pháp chi tiết của các mô hình LLM như (Tiêu chuẩn và bộ dữ liệu, Nội dung bộ dữ liệu, Chi tiết triển khai, v.v.), Một sơ đồ các bài báo được sử dụng trong bài báo này cũng được phát triển Hình 3. Trong Phần 4 (Vấn đề Mở và Hướng Nghiên cứu), các vấn đề mở và hướng tương lai tiềm năng của các mô hình LLM. Kết luận được mô tả trong Phần 5.

Bảng 1. Nghiên cứu về các mô hình ngôn ngữ khác nhau

| Loại Mô hình Ngôn ngữ | Nghiên cứu |
|---|---|
| Mô hình ngôn ngữ thống kê (SLM) | Jelinek et al. [36], Rosenfeld et al. [37] |
| Mô hình ngôn ngữ được tiền huấn luyện (PLM) | Matthew et al.[38] |
| Mô hình ngôn ngữ lớn (LLM) | Kaplan et al.[39] |
| Mô hình ngôn ngữ nơ-ron (NLM) | Bengio et al.[40] |
| Mô hình ngôn ngữ Transformer (TLM) | Vaswani et al.[3] |

--- TRANG 4 ---
4 của 26

2. CÔNG TRÌNH LIÊN QUAN

2.1. Mô hình ngôn ngữ-hình ảnh đa ngôn ngữ

Một mô hình trí tuệ nhân tạo có thể hiểu và tạo ra cả nội dung văn bản và hình ảnh qua nhiều ngôn ngữ được gọi là mô hình ngôn ngữ-hình ảnh đa ngôn ngữ. Để đánh giá và cung cấp thông tin hữu ích từ cả hai phương thức, những mô hình này được huấn luyện đặc biệt để xử lý và hiểu các mối quan hệ phức tạp giữa từ ngữ và hình ảnh. Chen et al. [41] nghiên cứu PaLI (Pathways Language and Image model), một mô hình mở rộng cách tiếp cận này cho việc mô hình hóa chung ngôn ngữ và thị giác. Scheible et al. [42] phát hiện GottBERT là một Mô hình Ngôn ngữ Đức thuần túy. AlexaTM 20B Soltan et al. [43] chứng minh rằng các mô hình chuỗi-đến-chuỗi (seq2seq) đa ngôn ngữ quy mô lớn được tiền huấn luyện trên hỗn hợp denoising và Causal Language Modeling (CLM). Dossou et al. [44] trình bày AfroLM, một mô hình ngôn ngữ đa ngôn ngữ được tiền huấn luyện từ đầu trên 23 ngôn ngữ châu Phi (nỗ lực lớn nhất đến nay) sử dụng khung học tự động tích cực mới của chúng tôi. Scao et al. [16] trình bày BLOOM, một mô hình ngôn ngữ đa ngôn ngữ mở với 176B tham số được thiết kế.

Bảng 2. Các LLMs ngôn ngữ đa ngôn ngữ khác nhau.

| Mô hình | Nghiên cứu | Năm |
|---|---|---|
| BLOOM | Scao et al.[16] | 2022 |
| AfroLM | Dossou et al.[44] | 2022 |
| AlexaTM 20B | Soltan et al.[43] | 2022 |
| GottBERT | Scheible et al.[42] | 2020 |
| PaLI | Chen et al.[41] | 2022 |

2.2. Mô hình transformer lâm sàng và y sinh

Một mô hình transformer lâm sàng và y sinh là một loại trí tuệ nhân tạo được tạo ra với mục đích rõ ràng là xử lý và phân tích dữ liệu văn bản lâm sàng và y sinh. Những mô hình transformer này sử dụng kiến trúc transformer, đã xuất sắc trong các công việc yêu cầu xử lý ngôn ngữ tự nhiên. Hồ sơ lâm sàng, hồ sơ sức khỏe điện tử, bài báo nghiên cứu, và các nguồn dữ liệu lâm sàng và y sinh có liên quan khác được bao gồm trong các bộ dữ liệu quy mô lớn được sử dụng để huấn luyện các mô hình transformer lâm sàng và y sinh. Những mô hình này có được kiến thức về các từ, cụm từ và khái niệm cụ thể được sử dụng trong lĩnh vực y tế. Mục tiêu chính của các mô hình transformer lâm sàng và y sinh là rút ra thông tin sâu sắc, thực hiện phân loại văn bản, nhận dạng thực thể, trích xuất mối quan hệ, trả lời câu hỏi, và các hoạt động khác cụ thể cho lĩnh vực lâm sàng và y sinh. Hỗ trợ quyết định lâm sàng, truy xuất thông tin, đánh giá rủi ro bệnh nhân, và tài liệu tự động chỉ là một số hoạt động mà họ có thể giúp đỡ các nhân viên chăm sóc sức khỏe. Yang et al.[32] phát triển từ đầu một mô hình ngôn ngữ lâm sàng lớn – GatorTron – sử dụng >90 tỷ từ văn bản (bao gồm >82 tỷ từ văn bản lâm sàng đã được khử định danh) và đánh giá hệ thống trên 5 nhiệm vụ NLP lâm sàng bao gồm trích xuất khái niệm lâm sàng, trích xuất mối quan hệ y tế, tương tự ngữ nghĩa văn bản, suy luận ngôn ngữ tự nhiên (NLI), và trả lời câu hỏi y tế (MQA). Lee et al.[29] giới thiệu BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), đây là một mô hình biểu diễn ngôn ngữ đặc thù miền được tiền huấn luyện trên các kho dữ liệu y sinh quy mô lớn. Li et al[45] trình bày Hi-BEHRT, một mô hình dựa trên Transformer phân cấp có thể mở rộng đáng kể trường tiếp nhận của Transformers và trích xuất các liên kết từ các chuỗi dài hơn nhiều. Sử dụng hồ sơ sức khỏe điện tử liên kết dọc đa phương thức quy mô lớn. Wang et al.[46] đề xuất một mô hình suy luận nhân quả sáng tạo–InferBERT, bằng cách tích hợp A Lite Bidirectional Encoder Representations from Transformers (ALBERT). Các mô hình ngôn ngữ lớn trong chăm sóc sức khỏe Theo Anmol et al.[47] đã được đề xuất rằng LLMs, như ChatGPT, có thể có ứng dụng trong lĩnh vực chăm sóc sức khỏe do khối lượng lớn thông tin văn bản tự do có sẵn để huấn luyện mô hình. Một LLM được huấn luyện trên hơn 90 tỷ từ văn bản từ hồ sơ sức khỏe điện tử (EHR)[28] Tác giả Yang et al. phát triển

--- TRANG 5 ---
5 của 26 từ đầu một mô hình ngôn ngữ lâm sàng lớn GatorTron sử dụng hơn 90 tỷ từ văn bản. Các mô hình transformer y sinh và lâm sàng hiện có để trích xuất khái niệm lâm sàng và mối quan hệ y tế như BioBERT[29], ClinicalBERT[30], BioMegatron[31], GatorTron-base[32], GatorTron-medium[32], GatorTron-large [32].

Santosh et al.[48] đề xuất PathologyBERT - một mô hình ngôn ngữ masked được tiền huấn luyện đã được huấn luyện trên 347,173 báo cáo mẫu mô bệnh học và được phát hành công khai trong kho Huggingface1. Các thí nghiệm toàn diện chứng minh rằng việc tiền huấn luyện mô hình transformer trên kho dữ liệu bệnh học mang lại cải thiện hiệu suất trong Hiểu Ngôn ngữ Tự nhiên (NLU) và Phân loại Chẩn đoán Ung thư Vú khi so sánh với các mô hình ngôn ngữ không đặc thù. Jaiswal et al.[49] giới thiệu RadBERT-CL là "Factually-Aware Contrastive Learning For Radiology Report Classification." Cũng cho thấy rằng các biểu diễn được học bởi RadBERT-CL có thể nắm bắt thông tin y tế quan trọng trong không gian tiềm ẩn. Gu et al.[14] đẩy nhanh nghiên cứu trong y sinh và phát hành các mô hình tiền huấn luyện và đặc thù nhiệm vụ tiên tiến cho cộng đồng, và tạo ra một bảng xếp hạng có tiêu chuẩn BLURB (Biomedical Language Understanding Reasoning Benchmark). Tác giả thách thức, lợi thế chính của việc tiền huấn luyện đặc thù miền từ đầu xuất phát từ việc có một từ vựng trong miền. Peng et al.[50] giới thiệu BLUE, một bộ sưu tập các nguồn tài nguyên để đánh giá và phân tích các mô hình biểu diễn ngôn ngữ tự nhiên y sinh. Phát hiện rằng các mô hình BERT được tiền huấn luyện trên tóm tắt PubMed và ghi chú lâm sàng thấy hiệu suất tốt hơn so với hầu hết các mô hình tiên tiến. Beltagy et al.[51] SCIBERT tận dụng tiền huấn luyện không giám sát trên một kho dữ liệu đa miền lớn các ấn phẩm khoa học để cải thiện hiệu suất trên các nhiệm vụ NLP khoa học downstream. Alsentzer et al.[30] phát hành các mô hình Clinical BERT cho văn bản lâm sàng: một cho văn bản lâm sàng chung và một khác cho tóm tắt xuất viện cụ thể. Cũng chứng minh trên một số nhiệm vụ NLP lâm sàng rằng hệ thống này cung cấp những cải thiện so với BERT và BioBERT truyền thống. Shin et al.[31] đưa ra BioMegatron được coi là mô hình ngôn ngữ miền y sinh lớn. Cho thấy những cải thiện nhất quán trên các tiêu chuẩn với mô hình BioMegatron lớn hơn được huấn luyện trên kho dữ liệu miền lớn hơn, góp phần vào hiểu biết của chúng ta về các ứng dụng mô hình ngôn ngữ miền.

Bảng 3. Các LLMs y sinh và lâm sàng khác nhau.

| Mô hình | Nghiên cứu | Năm |
|---|---|---|
| GatorTron-base | Yang et al.[32] | 2022 |
| BioBERT | Lee et al.[29] | 2020 |
| EntityBERT | Lin et al.[52] | 2021 |
| Hi-BEHRT | Li et al[45] | 2022 |
| InferBERT | Wang et al.[46] | 2021 |
| PathologyBERT | Santosh et al.[48] | 2022 |
| PubMedBERT | Gu et al.[14] | 2021 |
| SciBERT | Beltagy et al.[51] | 2019 |
| RadBERT | Yan et al.[53] | 2022 |
| ClinicalBERT | Alsentzer et al.[30] | 2019 |
| BlueBERT | Peng et al.[50] | 2019 |
| BioMegatron | Shin et al.[31] | 2019 |

2.3. Mô hình ngôn ngữ lớn cho tài chính

Những mô hình này có thể phân tích và nắm bắt dữ liệu văn bản tài chính phức tạp một cách hiệu quả bằng cách sử dụng các kỹ thuật học sâu như kiến trúc transformer. Chúng có thể giúp đỡ các công việc bao gồm biên soạn báo cáo tài chính, tóm tắt tài liệu tài chính, nghiên cứu đầu tư, quản lý danh mục đầu tư, và phân tích tin tức tài chính. Khả năng của các chuyên gia tài chính trong việc đưa ra quyết định có cơ sở dữ liệu tốt hơn có thể được cải thiện bằng việc sử dụng các mô hình ngôn ngữ lớn trong lĩnh vực này. Chúng có thể cung cấp thông tin sâu sắc cho các chiến lược đầu tư, hỗ trợ xác định xu hướng thị trường, đánh giá các yếu tố rủi ro, và phát hiện bất thường. Wu et al.[15] trình bày BloombergGPT (Một mô hình ngôn ngữ lớn cho tài chính), một mô hình ngôn ngữ 50 tỷ tham số được huấn luyện trên một loạt dữ liệu tài chính. Tác giả xác thực BloombergGPT trên

--- TRANG 6 ---
6 của 26 các tiêu chuẩn LLM chuẩn, tiêu chuẩn tài chính mở, và một bộ tiêu chuẩn nội bộ phản ánh chính xác nhất việc sử dụng dự định của chúng tôi. Scao et al.[16] trình bày BLOOM, một mô hình ngôn ngữ truy cập mở 176B tham số được thiết kế và xây dựng. BLOOM là một mô hình ngôn ngữ Transformer chỉ giải mã được huấn luyện trên kho dữ liệu ROOTS, một bộ dữ liệu bao gồm hàng trăm nguồn trong 46 ngôn ngữ tự nhiên và 13 ngôn ngữ lập trình (tổng cộng 59). Black et al.[17] giới thiệu GPT-NeoX-20B, một mô hình ngôn ngữ tự động hồi quy 20 tỷ tham số được huấn luyện trên Pile[54], đánh giá hiệu suất của nó trên một loạt các nhiệm vụ hiểu ngôn ngữ, toán học, và dựa trên kiến thức. Araci et al.[55] giới thiệu mô hình ngôn ngữ FinBERT dựa trên BERT, để giải quyết các nhiệm vụ NLP trong miền tài chính. Zhang et al.[13] trình bày Open Pre-trained Transformers (OPT), một bộ transformers tiền huấn luyện chỉ giải mã từ 125M đến 175B tham số. Yang et al.[12] trình bày một mô hình ngôn ngữ lớn mã nguồn mở FinGPT, cho lĩnh vực tài chính. FinGPT phản ứng sáng tạo bằng cách tận dụng các LLMs có sẵn và tinh chỉnh chúng cho các ứng dụng tài chính cụ thể. Xie et al.[56] khám phá mô hình LLM PIXIU cho Dữ liệu Hướng dẫn và Tiêu chuẩn Đánh giá cho Tài chính.

Bảng 4. Các LLMs dựa trên tài chính khác nhau.

| Mô hình | Nghiên cứu | Năm |
|---|---|---|
| BloombergGPT | Wu et al.[15] | 2023 |
| GPT-NeoX | Black et al.[17] | 2022 |
| OPT | Zhang et al.[13] | 2022 |
| BLOOM-176B | Scao et al.[16] | 2022 |
| FinBERT | Araci et al.[55] | 2019 |
| FinGPT | Yang et al.[12] | 2023 |
| PIXIU | Xie et al.[56] | 2023 |

2.4. Phân loại các mô hình ngôn ngữ thị giác

Các mô hình trí tuệ nhân tạo được gọi là "mô hình ngôn ngữ thị giác" được tạo ra để hiểu và tạo ra dữ liệu từ sự kết hợp của đầu vào thị giác và ngôn ngữ. Những mô hình này tìm cách thu hẹp khoảng cách giữa việc hiểu hình ảnh hoặc nội dung thị giác khác và việc hiểu ngôn ngữ tự nhiên. Những mô hình này có khả năng thực hiện một số nhiệm vụ, như tạo chú thích hình ảnh, trả lời câu hỏi thị giác (VQA), tạo hình ảnh từ mô tả văn bản, và khớp hình ảnh-văn bản. Ví dụ, một mô hình ngôn ngữ thị giác có thể tạo ra một chú thích cho hình ảnh mô tả chính xác nội dung của hình ảnh. Tương tự như vậy, mô hình có thể cung cấp câu trả lời hoặc lý giải có liên quan khi được hỏi một câu hỏi văn bản về một hình ảnh. Alayrac et al.[57] đưa ra Flamingo, một họ các mô hình Ngôn ngữ Thị giác (VLM) có thể được thích ứng nhanh chóng với các nhiệm vụ mới chỉ sử dụng một số ít ví dụ được chú thích là một thách thức mở cho nghiên cứu học máy đa phương thức. Monajatipoor et al.[58] Các mô hình thị giác-và-ngôn ngữ (VL) nhận hình ảnh và văn bản làm đầu vào và học cách nắm bắt các liên kết giữa chúng. Các nghiên cứu trước cho thấy rằng các mô hình VL được tiền huấn luyện có thể cải thiện đáng kể hiệu suất mô hình cho các nhiệm vụ downstream như Trả lời Câu hỏi Thị giác (VQA). Ko et al.[59] trình bày F-VLM, một phương pháp phát hiện đối tượng từ vựng mở đơn giản được xây dựng dựa trên Các Mô hình Ngôn ngữ và Thị giác Đông lạnh. Trong đó F-VLM đơn giản hóa đường ống huấn luyện đa giai đoạn hiện tại bằng cách loại bỏ nhu cầu chưng cất kiến thức hoặc tiền huấn luyện tùy chỉnh phát hiện. Zhu et al.[11] dự án MiniGPT-4 là việc nâng cao hiểu biết ngôn ngữ-thị giác với các mô hình ngôn ngữ lớn tiên tiến. Hong et al.[60] đề xuất một mô hình BERT lặp lại có ý thức thời gian để sử dụng trong điều hướng thị giác-và-ngôn ngữ (VLN). Trong bài báo này tác giả đề xuất một mô hình BERT lặp lại có ý thức thời gian để sử dụng trong VLN. Thrush et al.[61] trình bày một nhiệm vụ và bộ dữ liệu mới để đánh giá khả năng của các mô hình thị giác và ngôn ngữ trong việc thực hiện lý luận tổng hợp thị giác-ngôn ngữ, mà chúng tôi gọi là Winoground. Wang et al.[62] đề xuất một mô hình VL nhỏ hơn và nhanh hơn, MiniVLM, có thể được tinh chỉnh với hiệu suất tốt trên các nhiệm vụ downstream khác nhau như đối tác lớn hơn của nó. MiniVLM

--- TRANG 7 ---
7 của 26 bao gồm hai mô-đun, một bộ trích xuất đặc trưng thị giác và một mô-đun hợp nhất thị giác-ngôn ngữ dựa trên transformer.

Bảng 5. Các LLMs ngôn ngữ thị giác khác nhau.

| Mô hình | Nghiên cứu | Năm |
|---|---|---|
| Flamingo | Alayrac et al.[57] | 2022 |
| BERTHop | Monajatipoor et al.[58] | 2022 |
| F-VLM | Kuo et al.[59] | 2022 |
| MiniVLM | Wang et al.[62] | 2020 |
| VLN-BERT | Hong et al.[60] | 2021 |
| Winoground | Thrush et al.[61] | 2022 |
| MiniGPT-4 | Zhu et al.[11] | 2023 |

2.5. Phân loại mô hình ngôn ngữ lớn mã (Code LLMs)

Các mô hình ngôn ngữ quy mô lớn có tiềm năng thúc đẩy lập trình bằng cách khuyến khích tái sử dụng mã, chia sẻ kiến thức, và hợp tác giữa các nhà phát triển. Chúng có thể hỗ trợ loại bỏ lỗi, tự động hóa các quy trình mã hóa lặp đi lặp lại, và tăng tốc quá trình phát triển. Một mô hình ngôn ngữ lớn mã được thiết kế để giúp đỡ các lập trình viên với nhiều nhiệm vụ liên quan đến mã hóa. Những mô hình này có khả năng thực hiện các nhiệm vụ như hoàn thành mã, tạo mã, tóm tắt mã, và dịch mã và có thể hiểu cú pháp, ngữ nghĩa, và các mẫu lập trình của mã. Luo et al.[63] Trong bài báo này giới thiệu WizardCoder, trao quyền cho Code LLMs với tinh chỉnh hướng dẫn phức tạp, bằng cách thích ứng phương pháp Evol-Instruct với miền mã. Nijkamp et al.[64] phát hành một họ các mô hình ngôn ngữ lớn lên đến 16.1B tham số, gọi là CODEGEN, trên dữ liệu ngôn ngữ tự nhiên và ngôn ngữ lập trình. Jain et al.[65] trình bày một cách tiếp cận để tăng cường những mô hình ngôn ngữ lớn này với các bước hậu xử lý dựa trên phân tích chương trình và kỹ thuật tổng hợp, hiểu cú pháp và ngữ nghĩa của chương trình. Wang et al. [66] trình bày CodeT5, một mô hình Transformer encoder-decoder tiền huấn luyện thống nhất tận dụng tốt hơn ngữ nghĩa mã được truyền đạt từ các định danh do nhà phát triển gán.

Bảng 6. Các mô hình ngôn ngữ mã khác nhau (Code LLMs)

| Mô hình | Nghiên cứu | Năm |
|---|---|---|
| WizardCoder | Luo et al.[63] | 2023 |
| CodeGen | Nijkamp et al.[64] | 2022 |
| Jigsaw | Jain et al.[65] | 2022 |
| CodeT5 | Wang et al. [66] | 2021 |

--- TRANG 8 ---
8 của 26

3. Bảng phân loại của các mô hình LLM khác nhau

Bảng 7. Phân loại mô hình ngôn ngữ đa ngôn ngữ

| Mô hình LLM | Tiêu chuẩn và Bộ dữ liệu | Nội dung bộ dữ liệu | Chi tiết triển khai | Ứng dụng | Phiên bản của Mô hình |
|---|---|---|---|---|---|
| PALI [41] | WebLI[67], COCO[68], Chú thích đa ngôn ngữ trên Crossmodal-3600[69], VQA[70] | Bộ huấn luyện hình ảnh-văn bản mới chứa 10B hình ảnh và văn bản trong hơn 100 ngôn ngữ | - | Mô hình Ngôn ngữ và Hình ảnh Pathways | PaLI-3B, PaLI-15B, PaLI-17B |
| GottBERT[42] | Phần dữ liệu Đức của OSCAR đo được 145GB văn bản chứa khoảng 21.5 tỷ từ trong khoảng 459 triệu tài liệu (một tài liệu mỗi dòng) | Từ vựng 52k token con từ dựa trên 40 GB tài liệu được lấy mẫu ngẫu nhiên của phần OSCAR Đức. | 256 core TPU pod sử dụng kiến trúc RoBERTa BASE | Một Mô hình Ngôn ngữ Đức thuần túy | GottBERT, German BERT, XLM RoBERTa, dbmz BERT[42] |
| BLOOM [16] | BLOOM là một mô hình ngôn ngữ Transformer chỉ giải mã được huấn luyện trên kho dữ liệu ROOTS[71], 2022), Một bộ sưu tập tổng hợp của 498 bộ dữ liệu Hugging Face[72], tiêu chuẩn HELM[73] | Bộ dữ liệu bao gồm hàng trăm nguồn trong 46 ngôn ngữ tự nhiên và 13 ngôn ngữ lập trình (59) | 156 TFLOPs trong cấu hình nhanh nhất của chúng tôi với NVIDIA V100 GPUs | Một Mô hình Ngôn ngữ Đa ngôn ngữ Truy cập Mở 176B-Tham số | BLOOM-1B7, BLOOM-560M, BLOOM-1.1B, BLOOM-1.7B, BLOOM-1.7B, BLOOM-7.1B |

--- TRANG 9 ---
9 của 26

Bảng 7 – tiếp tục từ trang trước

| AlexaTM20B[43] | Wikipedia và mC4 [70] | Dữ liệu trong 12 ngôn ngữ, cụ thể là Ả Rập, Tiếng Anh, Pháp, Đức, Hindi, Ý, Nhật, Marathi, Bồ Đào Nha, Tây Ban Nha, Tamil, và Telugu, Đóng gói các chuỗi token để tạo ra các chuỗi khoảng 1024 đơn vị từ con. | Huấn luyện AlexaTM 20B trong 120 ngày trên 128 A100 GPUs | Một Mô hình Seq2seq Đa ngôn ngữ Quy mô Lớn | -- |
|---|---|---|---|---|---|
| AfroLM[44] | Chi tiết Kho ngữ liệu Ngôn ngữ[64,74,75], bộ dữ liệu YOSM[76] | Một số công trình về phân tích cảm xúc đã được thực hiện trên các ngôn ngữ tài nguyên cao trong khi các ngôn ngữ tài nguyên thấp như tiếng Yoruba và các ngôn ngữ châu Phi khác, dữ liệu bao gồm 1500 đánh giá phim được lấy từ IMDB, Rotten Tomatoes, Letterboxd, Cinemapointer, và Nollyrated[76] | Google Cloud với một NVIDIA A100 GPU 48GB duy nhất | Một Mô hình Ngôn ngữ Tiền huấn luyện Đa ngôn ngữ dựa trên Học Tự động Tích cực cho 23 Ngôn ngữ châu Phi | AfroLM-Large, AfroLM-Large(w/AL) đến AfroLM-Large(w/o AL), AfriBERTa-Large[44] |

--- TRANG 10 ---
10 của 26

Bảng 8. Phân loại các mô hình transformer lâm sàng/y sinh khác nhau

| Mô hình LLM | Tiêu chuẩn và Bộ dữ liệu | Nội dung bộ dữ liệu | Chi tiết triển khai | Ứng dụng | Phiên bản của Mô hình |
|---|---|---|---|---|---|
| GatorTron-base[32] | 5 nhiệm vụ NLP lâm sàng nhận dạng thực thể có tên [NER], trích xuất mối quan hệ y tế (MRE), tương tự ngữ nghĩa văn bản (STS), suy luận ngôn ngữ tự nhiên(NLI), và trả lời câu hỏi y tế (MQA), MedNLI[77], 2019 n2c2[78], emrQA Medication [79] | Tổng số 290,482,002 ghi chú lâm sàng từ 2,476,628 bệnh nhân được trích xuất từ Kho Dữ liệu Tích hợp UF Health (IDR), kho dữ liệu doanh nghiệp của hệ thống UF Health[32] | 992 A100 80G GPUs từ 124 NVIDIA DGX | Một mô hình ngôn ngữ lâm sàng lớn | GatorTron-base, GatorTron-base(1/4data), GatorTron-medium, GatorTron-large |
| BioBERT [29] | Được tiền huấn luyện trên kho dữ liệu miền y sinh (tóm tắt PubMed, Wikipedia tiếng Anh, BooksCorpus và bài báo toàn văn PMC) | NCBI Disease với 6881 (Số chú thích), 2010 i2b2/VA disease với 19665(Số chú thích), BC5CDR disease với 12694(Số chú thích) | Tám NVIDIA V100 GPUs. | Một mô hình biểu diễn ngôn ngữ y sinh được tiền huấn luyện cho khai thác văn bản y sinh | BioBERT v1.0, BioBERT v1.1 |
| Hi-BEHRT [45] | Hiệu suất mô hình trong dự đoán rủi ro xảy ra cho bốn bệnh: suy tim (HF), tiểu đường, bệnh thận mãn tính (CKD), và đột quỵ. | Clinical Practice Research Datalink (CPRD)[80], suy tim cơ tim[81], tiểu đường mellitus[82] | 2 GPUs [45] | Mô hình dựa trên Transformer Phân cấp để dự đoán chính xác các sự kiện lâm sàng sử dụng hồ sơ sức khỏe điện tử dọc đa phương thức | BEHRT, Hi-BEHRT |
| InferBERT [46] | Hai bộ dữ liệu FAERS, Suy gan cấp do thuốc giảm đau và bộ dữ liệu tử vong liên quan đến Tramadol | Bộ dữ liệu FAERS Hiệu ứng: (Xơ gan cấp và xơ cứng, OR Suy gan cấp và các rối loạn liên quan, OR Ứ mật và vàng da) AND Thuốc theo nhóm AND: [Thuốc giảm đau (Bất kỳ Vai trò nào)]" để trích xuất 45,773 [83] | một NVIDIA Tesla V100 GPU. | Một Khung Suy luận Nhân quả dựa trên Transformer để Tăng cường Dược lực học | - |

--- TRANG 11 ---
11 của 26

Bảng 8 – tiếp tục từ trang trước

| PathologyBERT[48] | Hai kho dữ liệu không chồng chéo của báo cáo mô bệnh học từ Bệnh viện Đại học Emory (EUH) | Hội đồng Đánh giá Thể chế Đại học Emory (IRB), tổng cộng 340,492 mẫu mô bệnh học không có cấu trúc từ 67,136 bệnh nhân được trích xuất từ kho dữ liệu lâm sàng của Bệnh viện Đại học Emory (EUH) từ năm 1981 đến 2021. | GeForce Quadro RTX 6000 24 GB GPUs | Một Mô hình Ngôn ngữ Transformer Mới cho Miền Bệnh học (Lưu ý: PathologyBERT trên Corpus II để dự đoán 6 mức độ nghiêm trọng chẩn đoán ung thư vú (ung thư vú xâm lấn, tổn thương nguy cơ cao, tổn thương biên giới, không phải ung thư vú, lành tính, và âm tính)[48]. | - |
| RadBERT [53] | Bộ dữ liệu MIMIC-CXR [84], được gắn nhãn giả sử dụng bộ gắn nhãn tự động [85] | Bộ dữ liệu MIMIC-CXR bao gồm 377, 110 hình ảnh X-quang ngực của 227, 827 bệnh nhân cùng với các báo cáo X-quang tương ứng đã được khử định danh. | - | Học Tương phản Nhận thức Sự thật cho Phân loại Báo cáo X-quang | RadBERT-CL |
| PubMedBERT [14] | BLURB: Một Tiêu chuẩn Toàn diện cho NLP Y sinh, BC5-chem[86], BC5-disease[86], NCBI-disease[87], BC2GM[88], PubMedQA[89] | BLUE kết hợp các ứng dụng y sinh dựa trên PubMed (sáu bộ dữ liệu như BC5, ChemProt, và HoC) với các ứng dụng lâm sàng dựa trên MIMIC (bốn bộ dữ liệu như i2b2 và MedNLI) | Một máy DGX-2 với 16 V100 GPUs | Nó được tiền huấn luyện trên tóm tắt PubMed và bài báo PubMed Central đầy đủ và là một mô hình BERT Base không phân biệt chữ hoa thường | BERT, RoBERTa, BioBERT, SciBERT, Clinical- BERT, và BlueBERT |
| SciBERT[51] | Corpus, EBM-NLP[90], SciERC[91], ACL-ARC[92] | Huấn luyện SCIBERT trên một mẫu ngẫu nhiên của 1.14M bài báo từ kho dữ liệu Semantic scholar[93] bao gồm 18% bài báo từ miền khoa học máy tính và 82% từ miền y sinh rộng | Một TPU v3 duy nhất với 8 cores | Một Mô hình Ngôn ngữ Tiền huấn luyện cho Văn bản Khoa học | - |
| ClinicalBERT[30] | i2b2 2006[94], i2b2 2010[95], i2b2 2012[96], i2b2 2014[97] | Nhiệm vụ NLP Lâm sàng, nhiệm vụ suy luận ngôn ngữ tự nhiên MedNLI[98], Sử dụng văn bản lâm sàng từ khoảng 2 triệu ghi chú trong cơ sở dữ liệu MIMIC-III v1.4[99] | Một GeForce GTX TITAN X 12 GB GPU duy nhất | Đánh giá biểu diễn của ghi chú lâm sàng sử dụng transformers hai chiều (ClinicalBERT) | Bio+Clinical BERT, Clinical BERT [30] |

--- TRANG 12 ---
12 của 26

Bảng 8 – tiếp tục từ trang trước

| BioMegatron[31] | Sử dụng các bộ dữ liệu tiêu chuẩn y sinh downstream cho Nhận dạng Thực thể Có tên (NER), Trích xuất Mối quan hệ (RE), và Trả lời Câu hỏi (QA) | Bộ dữ liệu BC5CDR[86] NER được chú thích bệnh tật, bộ dữ liệu ChemProt[100] chứa các câu từ tóm tắt PubMed, nhiệm vụ factoid BioASQ-7b[101] là một bộ dữ liệu QA y sinh | Kích thước batch 64 mỗi GPU với song song dữ liệu trên 16 GPUs | BioMegatron được coi là mô hình ngôn ngữ miền y sinh lớn | BioMegatron-345m, BioMegatron-800m, BioMegatron-1.2b |
| BlueBERT[50] | Sử dụng cả văn bản PubMed và ghi chú lâm sàng đã khử định danh từ MIMIC-III[99], tóm tắt PubMed[102] | Các mô hình được huấn luyện với 5M bước trên kho dữ liệu PubMed và 0.2M bước và BLUE chứa năm nhiệm vụ với mười kho dữ liệu bao gồm một loạt rộng các số lượng dữ liệu và độ khó khác nhau.(Tương tự câu, Nhận dạng thực thể có tên, Trích xuất mối quan hệ, Phân loại tài liệu, Nhiệm vụ suy luận) | - | Tiêu chuẩn Đánh giá Hiểu Ngôn ngữ Y sinh (BLUE) để hỗ trợ nghiên cứu trong phát triển biểu diễn ngôn ngữ tiền huấn luyện trong miền y sinh | BlueBERT-Base +Uncased+PubMed, BlueBERT-Base +Uncased+PubMed+ MIMIC-III |

--- TRANG 13 ---
13 của 26

Bảng 9. Phân loại mô hình ngôn ngữ lớn cho tài chính

| Mô hình LLM | Tiêu chuẩn và Bộ dữ liệu | Nội dung bộ dữ liệu | Chi tiết triển khai | Ứng dụng | Phiên bản của Mô hình |
|---|---|---|---|---|---|
| BloombergGPT[15] | C4[103], FinPile[54], tiêu chuẩn NLP tài chính công cộng [104] | Colossal Clean Crawled Corpus (C4) cung cấp cho chúng tôi kích thước từ vựng 125,000, Dump của Wikipedia tiếng Anh từ ngày 1 tháng 7, 2022. | 8 NVIDIA 40GB A100 GPUs | Một mô hình ngôn ngữ lớn cho tài chính | BLOOM-style, BLOOM176B[16] |
| GPT-NeoX [17] | Pile [54] | Nó có 22 nguồn dữ liệu, được chia thô thành 5 danh mục (Viết Học thuật, Web-scrapes và Tài nguyên Internet, Văn xuôi, Đối thoại, Linh tinh) | 8 NVIDIA A100-SXM4-40GB GPUs và được cấu hình với hai CPU AMD EPYC 7532 | Một Mô hình Ngôn ngữ Tự động Hồi quy Mã nguồn Mở | GPT-NeoX-20B |
| OPT-175B[13] | BookCorpus[105], MinhashLSH[106], RoBERTa CCNews[18] | Tám mô hình ngôn ngữ Transformer từ 125 triệu đến 175 tỷ tham số | Trên 992 80GB A100 GPUs | Mô hình Ngôn ngữ Transformer Tiền huấn luyện Mở | OPT từ 125M đến 175B (Ví dụ: OPT-125M đến OPT-175B) |
| BLOOM-176B[16] | Kho dữ liệu ROOTS[71], Một bộ sưu tập tổng hợp của 498 bộ dữ liệu Hugging Face[72], SuperGLUE[107], bộ dữ liệu STS từ MTEB[108], tiêu chuẩn HELM[73] | Kho dữ liệu ROOTS (bộ dữ liệu bao gồm hàng trăm nguồn trong 46 ngôn ngữ tự nhiên và 13 ngôn ngữ lập trình (tổng cộng 59)) | 8 NVIDIA A100 80GB GPUs | Một Mô hình Ngôn ngữ Đa ngôn ngữ Truy cập Mở 176B-Tham số | BLOOM-560M, BLOOM-1B7, BLOOM-1.7B, BLOOM-3B, BLOOM-7.1B, BLOOMZ[109] |
| FinBERT[55] | Kho dữ liệu tài chính (TRC2-financial)[110], FinancialPhraseBank[111], FiQA Sentiment[112] | FiQA Sentiment là một bộ dữ liệu được tạo ra cho thách thức khai thác ý kiến tài chính và trả lời câu hỏi, sử dụng dữ liệu cho Nhiệm vụ 1, bao gồm 1,174 tiêu đề tin tức tài chính và tweet với điểm cảm xúc tương ứng | Amazon p2.xlarge EC2 instance với một NVIDIA K80 GPU, 4 vCPUs | Phân tích cảm xúc tài chính với mô hình ngôn ngữ được tiền huấn luyện | FinBERT-task, FinBERT-domain |
| FinGPT[12] | Bộ dữ liệu học thuật, Bộ dữ liệu tài chính mới | Các nguồn dữ liệu tài chính khác nhau, như Tin tức Tài chính, Hồ sơ Công ty, Thảo luận Truyền thông Xã hội, và Thông báo Công ty | - | Một mô hình ngôn ngữ lớn mã nguồn mở (Phân tích cảm xúc tài chính, Phát hiện gian lận tài chính, Chấm điểm tín dụng, Tối ưu hóa danh mục đầu tư, Giáo dục tài chính) | FinLLM |

--- TRANG 14 ---
14 của 26

Bảng 9 – tiếp tục từ trang trước

| Mô hình LLM | Tiêu chuẩn và Bộ dữ liệu | Nội dung bộ dữ liệu | Chi tiết triển khai | Ứng dụng | Phiên bản của Mô hình |
|---|---|---|---|---|---|
| PIXIU [56] | Tiêu chuẩn Đánh giá Tài chính, bộ dữ liệu điều chỉnh hướng dẫn tài chính FIT trên NLP tài chính khác nhau, FiQA-SA, Financial Phrase Bank (FPB), Tiêu chuẩn FLARE | FiQA-SA bao gồm 11,730 hướng dẫn (tiêu đề tin tức, tweet), FPB có 48,450 loại dữ liệu tin tức[56] | 8 A100 40GB GPUs. | Một Mô hình Ngôn ngữ Lớn, Dữ liệu Hướng dẫn và Tiêu chuẩn Đánh giá cho Tài chính | FinMA |

--- TRANG 15 ---
15 của 26

Bảng 10. Phân loại các mô hình ngôn ngữ thị giác

| Mô hình LLM | Tiêu chuẩn và Bộ dữ liệu | Nội dung bộ dữ liệu | Chi tiết triển khai | Ứng dụng | Phiên bản của Mô hình |
|---|---|---|---|---|---|
| Flamingo[57] | Bộ dữ liệu MultiModal Massive Web (M3W), Cặp hình ảnh/video và văn bản, ALIGN [113], VQAv2, VATEX, VizWiz | ALIGN datacom bao gồm 1.8 tỷ hình ảnh được ghép cặp với alt-text, LTIP (Long Text and Image Pairs) bao gồm 312 triệu cặp hình ảnh và văn bản. | - | Một Mô hình Ngôn ngữ Thị giác cho Học Few-Shot | Flamingo-3B, Flamingo-9B, Flamingo |
| BERTHop [58] | ChestX-ray14, MIMIC-CXR | Nhãn MIMIC-CXR được tạo ra bằng ChexPert[85] và bộ gắn nhãn tự động NegBio[114]. OpenI bao gồm 3,996 báo cáo và 8,121 hình ảnh liên quan từ 3,996 bệnh nhân duy nhất được thu thập bởi Đại học Indiana từ nhiều viện. | - | Một Mô hình Thị giác-và-Ngôn ngữ Hiệu quả cho Chẩn đoán Bệnh X-quang Ngực | PixelHop++, BlueBERT |
| F-VLM [59] | Tiêu chuẩn LVIS[115] | Bộ dữ liệu LVIS chứa một tập hợp lớn và đa dạng 1203 danh mục đối tượng phù hợp cho phát hiện từ vựng mở. | - | Một phương pháp phát hiện đối tượng từ vựng mở đơn giản được xây dựng dựa trên Mô hình Ngôn ngữ và Thị giác Đông lạnh. | F-VLM-R50, F-VLM-R50x4, F-VLM-R50x16, F-VLM-R50x64 |
| MiniGPT-4 [11] | ConceptualCaption[116], SBU[117] và LAION[118] | Conceptual 12M (CC12M), một bộ dữ liệu với 12 triệu cặp hình ảnh-văn bản được thiết kế đặc biệt để sử dụng cho tiền huấn luyện thị giác-và-ngôn ngữ | 4 A100 (80GB) GPUs | MiniGPT-4 nhằm căn chỉnh thông tin thị giác từ một bộ mã hóa thị giác được tiền huấn luyện với một mô hình ngôn ngữ lớn tiên tiến (LLM) | – |
| MiniVLM [62] | ImageCaptioning[119], VQA[120], Natural Language Visual Reasoning for Real (NLVR2)[121], nhiệm vụ chú thích hình ảnh COCO | VQA là một biểu diễn của [CLS] được sử dụng để dự đoán câu trả lời trên một tập hợp chia sẻ 3129 câu trả lời với một lớp tuyến tính | 2 Intel(R) Xeon(R) CPU E5-2620 v4 @2.10GHz | Một Mô hình Thị giác-Ngôn ngữ Nhỏ hơn và Nhanh hơn | - |
| VLN-BERT[60] | R2R[122], REVERIE[123] | Matterport3D, một bộ dữ liệu RGB-D quy mô lớn chứa 10,800 khung nhìn toàn cảnh từ 194,400 hình ảnh RGB-D của 90 cảnh quy mô tòa nhà[124] | NVIDIA 2080Ti GPU duy nhất | Một BERT Thị giác-và-Ngôn ngữ Lặp lại cho Điều hướng | VisualBERT, VL BERT |

--- TRANG 16 ---
16 của 26

Bảng 10 – tiếp tục từ trang trước

| Winoground[61] | Bộ dữ liệu Winoground được tuyển chọn thủ công bởi bốn chuyên gia chú thích với kinh nghiệm sâu rộng trong nghiên cứu thị giác và ngôn ngữ cũng như ngôn ngữ học tính toán | Bộ dữ liệu có tổng cộng 1600 cặp hình ảnh-văn bản, với 800 cặp đúng và 800 cặp sai | - | Thăm dò Mô hình Thị giác và Ngôn ngữ cho Tính Tổng hợp Thị giác-Ngôn ngữ | - |
|---|---|---|---|---|---|

--- TRANG 17 ---
17 của 26

Bảng 11. Phân loại mô hình ngôn ngữ mã (Code LLMs)

| Mô hình LLM | Tiêu chuẩn và Bộ dữ liệu | Nội dung bộ dữ liệu | Chi tiết triển khai | Ứng dụng | Phiên bản của Mô hình |
|---|---|---|---|---|---|
| WizardCoder [63] | Bốn tiêu chuẩn tạo mã: HumanEval[125], HumanEval+[126], MBPP[127], và DS-1000[128] | Khởi tạo nó với bộ dữ liệu theo hướng dẫn 20K gọi là CodeAlpaca[67]. Kỹ thuật Evol-Instruct trên bộ dữ liệu này bao gồm 20,000 mẫu để tạo ra dữ liệu tiến hóa. | - | Trao quyền cho Mô hình Ngôn ngữ Lớn Mã với Evol-Instruct | WizardLM [129] |
| CodeGen [64] | THEPILE, BIGQUERY, và BIGPYTHON. | THEPILE là một kho dữ liệu văn bản tiếng Anh 825.18 GiB được thu thập [54], BIGQUERY là một tập con của bộ dữ liệu BigQuery công khai của Google, bộ dữ liệu BIGPYTHON chứa một lượng lớn dữ liệu trong ngôn ngữ lập trình, Python. | Phần cứng TPU-v4 của Google | Một LLM mở cho mã với tổng hợp chương trình đa lượt | CodeGen -MULTI, CodeGen-NL, CodeGen-MONO |
| Jigsaw [65] | Bộ dữ liệu PandasEval, bộ dữ liệu Hackathon | Bộ dữ liệu này bao gồm 68 nhiệm vụ Python Pandas. Mỗi nhiệm vụ có thể được giải quyết bằng một dòng mã duy nhất bằng cách kết hợp tối đa 2-3 hàm Pandas, bộ dữ liệu Hackathon bao gồm 21 nhiệm vụ Pandas; mỗi nhiệm vụ- | - | Mô hình Ngôn ngữ Lớn gặp Tổng hợp Chương trình | - |
| CodeT5[66] | SearchNet[130] | Bao gồm 99 truy vấn ngôn ngữ tự nhiên với khoảng 4k chú thích liên quan của chuyên gia về kết quả có khả năng từ Kho dữ liệu CodeSearchNet. Kho dữ liệu chứa khoảng 6 triệu hàm từ mã nguồn mở trải rộng sáu ngôn ngữ lập trình (Go, Java, JavaScript, PHP, Python, và Ruby) | 16 NVIDIA A100 GPUs với bộ nhớ 40G. | Mô hình Encoder-Decoder Tiền huấn luyện Thống nhất cho Hiểu và Tạo Mã | CodeBERT |

--- TRANG 18 ---
18 của 26

Bảng 12. Chi tiết của một số cấu hình LLMs hiện có với Triệu/ Tỷ tham số.

| Mô hình | Tối ưu hóa và Lớp | Kích thước mô hình | Tài liệu tham khảo |
|---|---|---|---|
| GPT-2 | Adam, 12 lớp | 1.5 tỷ | [131] |
| GPT-3 | Adam, 96 lớp | 175 tỷ | [132,133] |
| Microsoft DialoGPT | - | 147 triệu | [134,135] |
| BloombergGPT | GELU, 70 lớp | 50 tỷ | [15] |
| Vicuna | - | 13 tỷ | [136] |
| Dolly2.0 | - | 12 tỷ | [19] |
| BLOOM | Adam, 70 lớp | 176 tỷ | [34] |
| LLaMA | AdamW, - | 65 tỷ | [9] |
| Jurassic-1 | - | 178 tỷ | [137] |
| GLM | AdamW, - | 130 tỷ | [138] |
| PaLM | Adafactor, - | 540 tỷ | [139] |
| OPT 175B | AdamW, 96 | 175 tỷ | [13] |
| Chinchilla | Adam, 80 lớp | 70 tỷ | [67] |
| BERT-base | Adam, 12 lớp | 100 triệu | [14] |
| BERT-large | Adam, 24 lớp | 300 triệu | [14] |
| ALBERT | Adam, 12 lớp | 12 triệu | [140] |
| RoBERTa base | Adam, 12 lớp | 125 triệu | [18] |
| RoBERTa large | Adam, 24 lớp | 355 triệu | [18] |
| Megatron-Turing NLG | - | 530 tỷ | [141] |
| BioBERT | - | 13.5 tỷ | [29] |
| ClinicalBERT | Adam | 1.28 tỷ | [30,142] |
| BioMegatron | Adam, 24 | 1.2 tỷ | [31] |
| GatorTron-base | Adam, 24 lớp | 345 triệu | [32,143] |
| GatorTron-medium | Adam, 48 lớp | 3.9 tỷ | [32,143] |
| GatorTron-large | Adam, 56 lớp | 8.9 tỷ | [32,143] |
| Gopher | Adam, - | 280 tỷ | [144] |
| GPT-NeoX | AdamW | 20 tỷ | [17] |
| Bloom 176 | Adam, 24 lớp | 176 tỷ | [16] |
| PubMedBERT | - | 110 triệu | [89] |
| AlexaTM 20B | -, 46 lớp | 19.75 tỷ | [43] |
| AfroLM-Large | -, 10 lớp | 264 triệu | [44] |
| Hi-BEHRT | Adam, lớp | 264 triệu | [45] |
| PathologyBERT | Adam, 12 Lớp | 347 triệu | [48] |
| BioMegatron | Adam, 24 Lớp | 345 triệu | [31] |
| BioMegatron medium | Adam, 36 Lớp | 800 triệu | [31] |
| BioMegatron large | Adam, 24 Lớp | 1.2 tỷ | [31] |
| BloombergGPT | Adam, 70 Lớp | 50.6 tỷ | [15] |
| BLOOM-style | Adam, 70 Lớp | 50 tỷ | [145] |
| GPT-NeoX-20B | Adam, 44 Lớp | 20 tỷ | [17] |
| CODEGEN | - | 16.1 tỷ | [64] |

Trong Bảng 12, dựa trên những gì chúng ta đã thấy, khoảng từ tỷ đến triệu. Tối ưu hóa bộ dữ liệu là một bước quan trọng trong các mô hình LLM, đặc biệt là những mô hình có số lượng tham số lớn, với mục tiêu cải thiện chức năng và tốc độ của mô hình. Để đảm bảo dữ liệu huấn luyện có tính đại diện, đa dạng, và phù hợp với kết quả mong đợi, tối ưu hóa bộ dữ liệu bao gồm việc lựa chọn và chuẩn bị dữ liệu huấn luyện một cách cẩn thận. Các nhà nghiên cứu và lập trình viên có thể nâng cao khả năng của mô hình trong việc hiểu và tạo ra từ ngữ, dẫn đến các phản hồi chính xác và mạch lạc hơn, bằng cách tối ưu hóa bộ dữ liệu. Về cơ bản, tối ưu hóa bộ dữ liệu giúp các mô hình LLM đạt được tiềm năng đầy đủ bằng cách cung cấp dữ liệu huấn luyện chất lượng cao phù hợp với các nhiệm vụ hoặc mục tiêu cụ thể.

--- TRANG 19 ---
19 của 26

4. Vấn đề Mở và Hướng Nghiên cứu

Do kích thước của các mô hình ngôn ngữ lớn, việc triển khai chúng đòi hỏi một trình độ chuyên môn kỹ thuật cao, bao gồm hiểu biết vững chắc về học sâu, mô hình transformer, phần mềm phân tán, và phần cứng cũng như các vấn đề đạo đức và pháp lý phát sinh từ trách nhiệm pháp lý và tiềm năng gây hại của các hệ thống như vậy.

Nhiều chuyên gia trong lĩnh vực CNTT đang làm việc để hỗ trợ nghiên cứu và tạo ra các công nghệ có thể mở ra quyền truy cập vào các mô hình ngôn ngữ rộng, cho phép khách hàng và doanh nghiệp thuộc mọi quy mô tận dụng chúng.

Không rõ ràng các mô hình ngôn ngữ lâm sàng lớn với hàng tỷ tham số có thể giúp các hệ thống AI y tế sử dụng hồ sơ sức khỏe điện tử (EHRs) không có cấu trúc trong khung pháp lý và đạo đức hiện tại như thế nào trong khi đảm bảo quyền riêng tư của thông tin bệnh nhân và độ chính xác của thông tin được cung cấp[28].

Việc mở rộng và duy trì các mô hình ngôn ngữ lớn có thể khó khăn và tốn kém. Xây dựng một mô hình ngôn ngữ lớn nền tảng thường đòi hỏi nhiều tháng thời gian huấn luyện và hàng triệu đô la [33].

Và vì LLMs đòi hỏi một lượng đáng kể dữ liệu huấn luyện, các nhà phát triển và doanh nghiệp có thể thấy đó là một thách thức để truy cập các bộ dữ liệu đủ lớn để huấn luyện các hệ thống như vậy trong khi đảm bảo dữ liệu được thu thập một cách đạo đức và có sự cho phép của các bên liên quan.

Duy trì chúng bằng cách triển khai các hệ thống để đảm bảo đầu ra chính xác và hữu ích ở quy mô cũng là một thách thức đáng kể.

5. Kết luận

Trong nghiên cứu này, những tiến bộ mới nhất trong các mô hình ngôn ngữ lớn (LLMs) đã được giới thiệu và các khái niệm chính, phát hiện, và chiến lược để hiểu và khai thác LLMs đã được trình bày. Một loạt rộng các vấn đề được bao gồm trong nghiên cứu này, bao gồm các tính năng mô hình, bộ dữ liệu, mô hình transformer, và các tiêu chuẩn hiệu suất LLM. Các nghiên cứu gần đây đã tập trung vào các loại LLM khác nhau, như LLMs đa ngôn ngữ, LLMs y sinh và lâm sàng, LLMs ngôn ngữ thị giác, và mô hình ngôn ngữ mã. Khảo sát này cố gắng bao gồm nghiên cứu mới nhất về LLMs và cung cấp cho các học giả và kỹ sư một nguồn tài nguyên hữu ích.

Từ viết tắt

BERT - Bidirectional Encoder Representation From Transformers.
BioBERT - Bidirectional Encoder Representations from Transformers for Biomedical Text Mining.
BLUE - Biomedical Language Understanding Evaluation.
C4 - Colossal Clean Crawled Corpus.
LLaMA - Large Language Model Meta AI.
LLM - Large Language Model.
MTEB - Massive text embedding benchmark.
RoBERTa - Robustly Optimized BERT approach.

6. Tài liệu tham khảo

1. Singh, S.; Thakur, H.K. Survey of various AI chatbots based on technology used. In Proceedings of the 2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions)(ICRITO). IEEE, 2020, pp. 1074–1079.

2. Weizenbaum, J. ELIZA—a computer program for the study of natural language communication between man and machine. Communications of the ACM 1966, 9, 36–45.

--- TRANG 20 ---
20 của 26

3. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A.N.; Kaiser, Ł.; Polosukhin, I. Attention is all you need. Advances in neural information processing systems 2017, 30.

4. Chang, T.A.; Bergen, B.K. Language model behavior: A comprehensive survey. arXiv preprint arXiv:2303.11504 2023.

5. Uszkoreit, J. Transformer: A novel neural network architecture for language understanding. Google AI Blog 2017, 31.

6. Radford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.; et al. Improving language understanding by generative pre-training 2018.

7. Devlin, J.; Chang, M.W.; Lee, K.; Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 2018.

8. GPT-4 is OpenAI's most advanced system, producing safer and more useful responses. https://openai.com/product/gpt-4. [Online; Accessed 06-17-2023].

9. Introducing LLaMA: A foundational, 65-billion-parameter large language model. https://ai.facebook.com/blog/large-language-model-llama-meta-ai/. [Online; Accessed 06-17-2023].

10. Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra, G.; Roberts, A.; Barham, P.; Chung, H.W.; Sutton, C.; Gehrmann, S.; et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 2022.

11. Zhu, D.; Chen, J.; Shen, X.; Li, X.; Elhoseiny, M. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 2023.

12. Yang, H.; Liu, X.Y.; Wang, C.D. FinGPT: Open-Source Financial Large Language Models. arXiv preprint arXiv:2306.06031 2023.

13. Zhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.; Chen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X.V.; et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 2022.

14. Gu, Y.; Tinn, R.; Cheng, H.; Lucas, M.; Usuyama, N.; Liu, X.; Naumann, T.; Gao, J.; Poon, H. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH) 2021, 3, 1–23.

15. Wu, S.; Irsoy, O.; Lu, S.; Dabravolski, V.; Dredze, M.; Gehrmann, S.; Kambadur, P.; Rosenberg, D.; Mann, G. Bloomberggpt: A large language model for finance. arXiv preprint arXiv:2303.17564 2023.

16. Scao, T.L.; Fan, A.; Akiki, C.; Pavlick, E.; Ilić, S.; Hesslow, D.; Castagné, R.; Luccioni, A.S.; Yvon, F.; Gallé, M.; et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 2022.

17. Black, S.; Biderman, S.; Hallahan, E.; Anthony, Q.; Gao, L.; Golding, L.; He, H.; Leahy, C.; McDonell, K.; Phang, J.; et al. Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745 2022.

18. Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer, L.; Stoyanov, V. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 2019.

19. EDWARDS, B. A really big deal"—Dolly is a free, open source, ChatGPT-style AI model. https://arstechnica.com/information-technology/2023/04/a-really-big-deal-dolly-is-a-free-open-source-chatgpt-style-ai-model/, 2023. [Online; Accessed 06-17-2023].

20. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. Language models are few-shot learners. Advances in neural information processing systems 2020, 33, 1877–1901.

21. Peng, B.; Li, C.; He, P.; Galley, M.; Gao, J. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277 2023.

22. Lopez-Lira, A.; Tang, Y. Can chatgpt forecast stock price movements? Return predictability and large language models. arXiv preprint arXiv:2304.07619 2023.

23. Lee, A. What are large language models used for. NVIDIA Blog 2023.

24. Zhao, W.X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.; Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.; et al. A survey of large language models. arXiv preprint arXiv:2303.18223 2023.

25. Gao, J.; Lin, C.Y. Introduction to the special issue on statistical language modeling, 2004.

26. Melis, G.; Dyer, C.; Blunsom, P. On the state of the art of evaluation in neural language models. arXiv preprint arXiv:1707.05589 2017.

27. Bengio, Y.; Ducharme, R.; Vincent, P. A neural probabilistic language model. Advances in neural information processing systems 2000, 13.

--- TRANG 21 ---
21 của 26

28. Yang, X.; Chen, A.; PourNejatian, N.; Shin, H.C.; Smith, K.E.; Parisien, C.; Compas, C.; Martin, C.; Costa, A.B.; Flores, M.G.; et al. A large language model for electronic health records. npj Digital Medicine 2022, 5, 194.

29. Lee, J.; Yoon, W.; Kim, S.; Kim, D.; Kim, S.; So, C.H.; Kang, J. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics 2020, 36, 1234–1240.

30. Alsentzer, E.; Murphy, J.R.; Boag, W.; Weng, W.H.; Jin, D.; Naumann, T.; McDermott, M. Publicly available clinical BERT embeddings. arXiv preprint arXiv:1904.03323 2019.

31. Shin, H.C.; Zhang, Y.; Bakhturina, E.; Puri, R.; Patwary, M.; Shoeybi, M.; Mani, R. BioMegatron: Larger biomedical domain language model. arXiv preprint arXiv:2010.06060 2020.

32. Yang, X.; Chen, A.; PourNejatian, N.; Shin, H.C.; Smith, K.E.; Parisien, C.; Compas, C.; Martin, C.; Flores, M.G.; Zhang, Y.; et al. Gatortron: A large clinical language model to unlock patient information from unstructured electronic health records. arXiv preprint arXiv:2203.03540 2022.

33. LEE, A. What Are Large Language Models Used For? https://blogs.nvidia.com/blog/2023/01/26/what-are-large-language-models-used-for/, 2023. [Online; Accessed 06-17-2023].

34. BigScience Blog. https://bigscience.huggingface.co/blog/bloom, 2023. [Online; Accessed 06-17-2023].

35. Lee, S.; Kim, W.J.; Ye, J.C. LLM Itself Can Read and Generate CXR Images. arXiv preprint arXiv:2305.11490 2023.

36. Jelinek, F. Statistical methods for speech recognition; MIT press, 1998.

37. Rosenfeld, R. Two decades of statistical language modeling: Where do we go from here? Proceedings of the IEEE 2000, 88, 1270–1278.

38. Peters, M.E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; Zettlemoyer, L. "Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA.

39. Kaplan, J.; McCandlish, S.; Henighan, T.; Brown, T.B.; Chess, B.; Child, R.; Gray, S.; Radford, A.; Wu, J.; Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 2020.

40. Bengio, Y.; Senécal, J.S. Adaptive importance sampling to accelerate training of a neural probabilistic language model. Technical report, IDIAP, 2003.

41. Chen, X.; Wang, X.; Changpinyo, S.; Piergiovanni, A.; Padlewski, P.; Salz, D.; Goodman, S.; Grycner, A.; Mustafa, B.; Beyer, L.; et al. Pali: A jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794 2022.

42. Scheible, R.; Thomczyk, F.; Tippmann, P.; Jaravine, V.; Boeker, M. GottBERT: a pure German language model. arXiv preprint arXiv:2012.02110 2020.

43. Soltan, S.; Ananthakrishnan, S.; FitzGerald, J.; Gupta, R.; Hamza, W.; Khan, H.; Peris, C.; Rawls, S.; Rosenbaum, A.; Rumshisky, A.; et al. Alexatm 20b: Few-shot learning using a large-scale multilingual seq2seq model. arXiv preprint arXiv:2208.01448 2022.

44. Dossou, B.F.; Tonja, A.L.; Yousuf, O.; Osei, S.; Oppong, A.; Shode, I.; Awoyomi, O.O.; Emezue, C.C. AfroLM: A Self-Active Learning-based Multilingual Pretrained Language Model for 23 African Languages. arXiv preprint arXiv:2211.03263 2022.

45. Li, Y.; Mamouei, M.; Salimi-Khorshidi, G.; Rao, S.; Hassaine, A.; Canoy, D.; Lukasiewicz, T.; Rahimi, K. Hi-BEHRT: Hierarchical Transformer-based model for accurate prediction of clinical events using multimodal longitudinal electronic health records. IEEE Journal of Biomedical and Health Informatics 2022.

46. Wang, X.; Xu, X.; Tong, W.; Roberts, R.; Liu, Z. InferBERT: a transformer-based causal inference framework for enhancing pharmacovigilance. Frontiers in Artificial Intelligence 2021, 4, 659622.

47. Arora, A.; Arora, A. The promise of large language models in health care. The Lancet 2023, 401, 641.

48. Santos, T.; Tariq, A.; Das, S.; Vayalpati, K.; Smith, G.H.; Trivedi, H.; Banerjee, I. PathologyBERT–Pre-trained Vs. A New Transformer Language Model for Pathology Domain. arXiv preprint arXiv:2205.06885 2022.

49. Jaiswal, A.; Tang, L.; Ghosh, M.; Rousseau, J.F.; Peng, Y.; Ding, Y. RadBERT-CL: factually-aware contrastive learning for radiology report classification. In Proceedings of the Machine Learning for Health. PMLR, 2021, pp. 196–208.

50. Peng, Y.; Yan, S.; Lu, Z. Transfer learning in biomedical natural language processing: an evaluation of BERT and ELMo on ten benchmarking datasets. arXiv preprint arXiv:1906.05474 2019.

--- TRANG 22 ---
22 của 26

51. Beltagy, I.; Lo, K.; Cohan, A. SciBERT: A pretrained language model for scientific text. arXiv preprint arXiv:1903.10676 2019.

52. Lin, C.; Miller, T.; Dligach, D.; Bethard, S.; Savova, G. EntityBERT: Entity-centric masking strategy for model pretraining for the clinical domain. Association for Computational Linguistics (ACL), 2021.

53. Yan, A.; McAuley, J.; Lu, X.; Du, J.; Chang, E.Y.; Gentili, A.; Hsu, C.N. RadBERT: Adapting transformer-based language models to radiology. Radiology: Artificial Intelligence 2022, 4, e210258.

54. Gao, L.; Biderman, S.; Black, S.; Golding, L.; Hoppe, T.; Foster, C.; Phang, J.; He, H.; Thite, A.; Nabeshima, N.; et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 2020.

55. Araci, D. Finbert: Financial sentiment analysis with pre-trained language models. arXiv preprint arXiv:1908.10063 2019.

56. Xie, Q.; Han, W.; Zhang, X.; Lai, Y.; Peng, M.; Lopez-Lira, A.; Huang, J. PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance. arXiv preprint arXiv:2306.05443 2023.

57. Alayrac, J.B.; Donahue, J.; Luc, P.; Miech, A.; Barr, I.; Hasson, Y.; Lenc, K.; Mensch, A.; Millican, K.; Reynolds, M.; et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems 2022, 35, 23716–23736.

58. Monajatipoor, M.; Rouhsedaghat, M.; Li, L.H.; Jay Kuo, C.C.; Chien, A.; Chang, K.W. Berthop: An effective vision-and-language model for chest x-ray disease diagnosis. In Proceedings of the Medical Image Computing and Computer Assisted Intervention–MICCAI 2022: 25th International Conference, Singapore, September 18–22, 2022, Proceedings, Part V. Springer, 2022, pp. 725–734.

59. Kuo, W.; Cui, Y.; Gu, X.; Piergiovanni, A.; Angelova, A. F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models. arXiv preprint arXiv:2209.15639 2022.

60. Hong, Y.; Wu, Q.; Qi, Y.; Rodriguez-Opazo, C.; Gould, S. Vln bert: A recurrent vision-and-language bert for navigation. In Proceedings of the Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, 2021, pp. 1643–1653.

61. Thrush, T.; Jiang, R.; Bartolo, M.; Singh, A.; Williams, A.; Kiela, D.; Ross, C. Winoground: Probing vision and language models for visio-linguistic compositionality. In Proceedings of the Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 5238–5248.

62. Wang, J.; Hu, X.; Zhang, P.; Li, X.; Wang, L.; Zhang, L.; Gao, J.; Liu, Z. Minivlm: A smaller and faster vision-language model. arXiv preprint arXiv:2012.06946 2020.

63. Luo, Z.; Xu, C.; Zhao, P.; Sun, Q.; Geng, X.; Hu, W.; Tao, C.; Ma, J.; Lin, Q.; Jiang, D. WizardCoder: Empowering Code Large Language Models with Evol-Instruct. arXiv preprint arXiv:2306.08568 2023.

64. Nijkamp, E.; Pang, B.; Hayashi, H.; Tu, L.; Wang, H.; Zhou, Y.; Savarese, S.; Xiong, C. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474 2022.

65. Jain, N.; Vaidyanath, S.; Iyer, A.; Natarajan, N.; Parthasarathy, S.; Rajamani, S.; Sharma, R. Jigsaw: Large language models meet program synthesis. In Proceedings of the Proceedings of the 44th International Conference on Software Engineering, 2022, pp. 1219–1231.

66. Wang, Y.; Wang, W.; Joty, S.; Hoi, S.C. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859 2021.

67. Hoffmann, J.; Borgeaud, S.; Mensch, A.; Buchatskaya, E.; Cai, T.; Rutherford, E.; Casas, D.d.L.; Hendricks, L.A.; Welbl, J.; Clark, A.; et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 2022.

68. Chen, X.; Fang, H.; Lin, T.Y.; Vedantam, R.; Gupta, S.; Dollár, P.; Zitnick, C.L. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325 2015.

69. Thapliyal, A.V.; Pont-Tuset, J.; Chen, X.; Soricut, R. Crossmodal-3600: A massively multilingual multimodal evaluation dataset. arXiv preprint arXiv:2205.12522 2022.

70. Xue, L.; Constant, N.; Roberts, A.; Kale, M.; Al-Rfou, R.; Siddhant, A.; Barua, A.; Raffel, C. mT5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934 2020.

71. Laurençon, H.; Saulnier, L.; Wang, T.; Akiki, C.; Villanova del Moral, A.; Le Scao, T.; Von Werra, L.; Mou, C.; González Ponferrada, E.; Nguyen, H.; et al. The bigscience roots corpus: A 1.6

--- TRANG 23 ---
23 của 26

tb composite multilingual dataset. Advances in Neural Information Processing Systems 2022, 35, 31809–31826.

72. Lhoest, Q.; Villanova del Moral, A.; Jernite, Y.; Thakur, A.; von Platen, P.; Patil, S.; Chaumond, J.; Drame, M.; Plu, J.; Tunstall, L.; et al. Datasets: A Community Library for Natural Language Processing. In Proceedings of the Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations; Association for Computational Linguistics: Online and Punta Cana, Dominican Republic, 2021; pp. 175–184. https://doi.org/10.18653/v1/2021.emnlp-demo.21.

73. Liang, P.; Bommasani, R.; Lee, T.; Tsipras, D.; Soylu, D.; Yasunaga, M.; Zhang, Y.; Narayanan, D.; Wu, Y.; Kumar, A.; et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110 2022.

74. Adelani, D.I.; Alabi, J.O.; Fan, A.; Kreutzer, J.; Shen, X.; Reid, M.; Ruiter, D.; Klakow, D.; Nabende, P.; Chang, E.; et al. A few thousand translations go a long way! leveraging pre-trained models for african news translation. arXiv preprint arXiv:2205.02022 2022.

75. Ogueji, K.; Zhu, Y.; Lin, J. Small Data? No Problem! Exploring the Viability of Pretrained Multilingual Language Models for Low-resourced Languages. In Proceedings of the Proceedings of the 1st Workshop on Multilingual Representation Learning; Association for Computational Linguistics: Punta Cana, Dominican Republic, 2021; pp. 116–126. https://doi.org/10.18653/v1/2021.mrl-1.11.

76. Shode, I.; Adelani, D.I.; Feldman, A. yosm: A new yoruba sentiment corpus for movie reviews. arXiv preprint arXiv:2204.09711 2022.

77. Herlihy, C.; Rudinger, R. MedNLI is not immune: Natural language inference artifacts in the clinical domain. arXiv preprint arXiv:2106.01491 2021.

78. Wang, Y.; Fu, S.; Shen, F.; Henry, S.; Uzuner, O.; Liu, H.; et al. The 2019 n2c2/ohnlp track on clinical semantic textual similarity: overview. JMIR medical informatics 2020, 8, e23375.

79. Pampari, A.; Raghavan, P.; Liang, J.; Peng, J. emrqa: A large corpus for question answering on electronic medical records. arXiv preprint arXiv:1809.00732 2018.

80. Herrett, E.; Gallagher, A.M.; Bhaskaran, K.; Forbes, H.; Mathur, R.; Van Staa, T.; Smeeth, L. Data resource profile: clinical practice research datalink (CPRD). International journal of epidemiology 2015, 44, 827–836.

81. Conrad, N.; Judge, A.; Tran, J.; Mohseni, H.; Hedgecott, D.; Crespillo, A.P.; Allison, M.; Hemingway, H.; Cleland, J.G.; McMurray, J.J.; et al. Temporal trends and patterns in heart failure incidence: a population-based study of 4 million individuals. The Lancet 2018, 391, 572–580.

82. Kuan, V.; Denaxas, S.; Gonzalez-Izquierdo, A.; Direk, K.; Bhatti, O.; Husain, S.; Sutaria, S.; Hingorani, M.; Nitsch, D.; Parisinos, C.A.; et al. A chronological map of 308 physical and mental health conditions from 4 million individuals in the English National Health Service. The Lancet Digital Health 2019, 1, e63–e77.

83. XingqiaoWang. DeepCausalPV-master. https://github.com/XingqiaoWang/DeepCausalPV-master, 2021. [Online; Accessed 06-17-2023].

84. Johnson, A.E.; Pollard, T.J.; Greenbaum, N.R.; Lungren, M.P.; Deng, C.y.; Peng, Y.; Lu, Z.; Mark, R.G.; Berkowitz, S.J.; Horng, S. MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs. arXiv preprint arXiv:1901.07042 2019.

85. Irvin, J.; Rajpurkar, P.; Ko, M.; Yu, Y.; Ciurea-Ilcus, S.; Chute, C.; Marklund, H.; Haghgoo, B.; Ball, R.; Shpanskaya, K.; et al. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the Proceedings of the AAAI conference on artificial intelligence, 2019, Vol. 33, pp. 590–597.

86. Li, J.; Sun, Y.; Johnson, R.J.; Sciaky, D.; Wei, C.H.; Leaman, R.; Davis, A.P.; Mattingly, C.J.; Wiegers, T.C.; Lu, Z. BioCreative V CDR task corpus: a resource for chemical disease relation extraction. Database 2016, 2016.

87. Doğan, R.I.; Leaman, R.; Lu, Z. NCBI disease corpus: a resource for disease name recognition and concept normalization. Journal of biomedical informatics 2014, 47, 1–10.

88. Smith, L.; Tanabe, L.K.; Kuo, C.J.; Chung, I.; Hsu, C.N.; Lin, Y.S.; Klinger, R.; Friedrich, C.M.; Ganchev, K.; Torii, M.; et al. Overview of BioCreative II gene mention recognition. Genome biology 2008, 9, 1–19.

89. Jin, Q.; Dhingra, B.; Liu, Z.; Cohen, W.W.; Lu, X. Pubmedqa: A dataset for biomedical research question answering. arXiv preprint arXiv:1909.06146 2019.

90. Nye, B.; Li, J.J.; Patel, R.; Yang, Y.; Marshall, I.J.; Nenkova, A.; Wallace, B.C. A corpus with multi-level annotations of patients, interventions and outcomes to support language processing

--- TRANG 24 ---
24 của 26

for medical literature. In Proceedings of the Proceedings of the conference. Association for Computational Linguistics. Meeting. NIH Public Access, 2018, Vol. 2018, p. 197.

91. Luan, Y.; He, L.; Ostendorf, M.; Hajishirzi, H. Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. arXiv preprint arXiv:1808.09602 2018.

92. Jurgens, D.; Kumar, S.; Hoover, R.; McFarland, D.; Jurafsky, D. Measuring the evolution of a scientific field through citation frames. Transactions of the Association for Computational Linguistics 2018, 6, 391–406.

93. Ammar, W.; Groeneveld, D.; Bhagavatula, C.; Beltagy, I.; Crawford, M.; Downey, D.; Dunkelberger, J.; Elgohary, A.; Feldman, S.; Ha, V.; et al. Construction of the literature graph in semantic scholar. arXiv preprint arXiv:1805.02262 2018.

94. Uzuner, Ö.; Luo, Y.; Szolovits, P. Evaluating the state-of-the-art in automatic de-identification. Journal of the American Medical Informatics Association 2007, 14, 550–563.

95. Uzuner, Ö.; South, B.R.; Shen, S.; DuVall, S.L. 2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text. Journal of the American Medical Informatics Association 2011, 18, 552–556.

96. Sun, W.; Rumshisky, A.; Uzuner, O. Evaluating temporal relations in clinical text: 2012 i2b2 challenge. Journal of the American Medical Informatics Association 2013, 20, 806–813.

97. Stubbs, A.; Uzuner, Ö. Annotating longitudinal clinical narratives for de-identification: The 2014 i2b2/UTHealth corpus. Journal of biomedical informatics 2015, 58, S20–S29.

98. Romanov, A.; Shivade, C. Lessons from natural language inference in the clinical domain. arXiv preprint arXiv:1808.06752 2018.

99. Johnson, A.E.; Pollard, T.J.; Shen, L.; Lehman, L.w.H.; Feng, M.; Ghassemi, M.; Moody, B.; Szolovits, P.; Anthony Celi, L.; Mark, R.G. MIMIC-III, a freely accessible critical care database. Scientific data 2016, 3, 1–9.

100. Krallinger, M.; Rabal, O.; Leitner, F.; Vazquez, M.; Salgado, D.; Lu, Z.; Leaman, R.; Lu, Y.; Ji, D.; Lowe, D.M.; et al. The CHEMDNER corpus of chemicals and drugs and its annotation principles. Journal of cheminformatics 2015, 7, 1–17.

101. Tsatsaronis, G.; Balikas, G.; Malakasiotis, P.; Partalas, I.; Zschunke, M.; Alvers, M.R.; Weissenborn, D.; Krithara, A.; Petridis, S.; Polychronopoulos, D.; et al. An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition. BMC bioinformatics 2015, 16, 1–28.

102. Peters, M.E.; Ammar, W.; Bhagavatula, C.; Power, R. Semi-supervised sequence tagging with bidirectional language models. arXiv preprint arXiv:1705.00108 2017.

103. Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y.; Li, W.; Liu, P.J. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research 2020, 21, 1–67.

104. Shah, R.; Chawla, K.; Eidnani, D.; Shah, A.; Du, W.; Chava, S.; Raman, N.; Smiley, C.; Chen, J.; Yang, D. When FLUE Meets FLANG: Benchmarks and Large Pretrained Language Model for Financial Domain. In Proceedings of the Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing; Association for Computational Linguistics: Abu Dhabi, United Arab Emirates, 2022; pp. 2322–2335.

105. Zhu, Y.; Kiros, R.; Zemel, R.; Salakhutdinov, R.; Urtasun, R.; Torralba, A.; Fidler, S. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the Proceedings of the IEEE international conference on computer vision, 2015, pp. 19–27.

106. Rajaraman, A.; Ullman, J.D. Mining of massive datasets; Cambridge University Press, 2011.

107. Wang, A.; Pruksachatkun, Y.; Nangia, N.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; Bowman, S. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems 2019, 32.

108. Muennighoff, N.; Tazi, N.; Magne, L.; Reimers, N. MTEB: Massive text embedding benchmark. arXiv preprint arXiv:2210.07316 2022.

109. Muennighoff, N.; Wang, T.; Sutawika, L.; Roberts, A.; Biderman, S.; Scao, T.L.; Bari, M.S.; Shen, S.; Yong, Z.X.; Schoelkopf, H.; et al. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786 2022.

110. Reuters Corpora (RCV1, RCV2, TRC2). https://trec.nist.gov/data/reuters/reuters.html, 2004. [Online; Accessed 06-17-2023].

111. Malo, P.; Sinha, A.; Takala, P.; Korhonen, P.; Wallenius, J. FinancialPhraseBank-v1. 0, 2013.

--- TRANG 25 ---
25 của 26

112. Maia, M.; Handschuh, S.; Freitas, A.; Davis, B.; McDermott, R.; Zarrouk, M.; Balahur, A. Www'18 open challenge: financial opinion mining and question answering. In Proceedings of the Companion proceedings of the the web conference 2018, 2018, pp. 1941–1942.

113. Jia, C.; Yang, Y.; Xia, Y.; Chen, Y.T.; Parekh, Z.; Pham, H.; Le, Q.; Sung, Y.H.; Li, Z.; Duerig, T. Scaling up visual and vision-language representation learning with noisy text supervision. In Proceedings of the International conference on machine learning. PMLR, 2021, pp. 4904–4916.

114. Peng, Y.; Wang, X.; Lu, L.; Bagheri, M.; Summers, R.; Lu, Z. NegBio: a high-performance tool for negation and uncertainty detection in radiology reports. AMIA Summits on Translational Science Proceedings 2018, 2018, 188.

115. Gupta, A.; Dollar, P.; Girshick, R. Lvis: A dataset for large vocabulary instance segmentation. In Proceedings of the Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 5356–5364.

116. Changpinyo, S.; Sharma, P.; Ding, N.; Soricut, R. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 3558–3568.

117. Ordonez, V.; Kulkarni, G.; Berg, T. Im2text: Describing images using 1 million captioned photographs. Advances in neural information processing systems 2011, 24.

118. Schuhmann, C.; Vencu, R.; Beaumont, R.; Kaczmarczyk, R.; Mullis, C.; Katta, A.; Coombes, T.; Jitsev, J.; Komatsuzaki, A. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114 2021.

119. Zhou, L.; Palangi, H.; Zhang, L.; Hu, H.; Corso, J.; Gao, J. Unified vision-language pre-training for image captioning and vqa. In Proceedings of the Proceedings of the AAAI conference on artificial intelligence, 2020, Vol. 34, pp. 13041–13049.

120. Goyal, Y.; Khot, T.; Summers-Stay, D.; Batra, D.; Parikh, D. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 6904–6913.

121. Suhr, A.; Zhou, S.; Zhang, A.; Zhang, I.; Bai, H.; Artzi, Y. A corpus for reasoning about natural language grounded in photographs. arXiv preprint arXiv:1811.00491 2018.

122. Anderson, P.; Wu, Q.; Teney, D.; Bruce, J.; Johnson, M.; Sünderhauf, N.; Reid, I.; Gould, S.; Van Den Hengel, A. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 3674–3683.

123. Qi, Y.; Wu, Q.; Anderson, P.; Wang, X.; Wang, W.Y.; Shen, C.; Hengel, A.v.d. Reverie: Remote embodied visual referring expression in real indoor environments. In Proceedings of the Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 9982–9991.

124. Chang, A.; Dai, A.; Funkhouser, T.; Halber, M.; Niessner, M.; Savva, M.; Song, S.; Zeng, A.; Zhang, Y. Matterport3D: Learning from RGB-D Data in Indoor Environments. International Conference on 3D Vision (3DV) 2017.

125. Chen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Pinto, H.P.d.O.; Kaplan, J.; Edwards, H.; Burda, Y.; Joseph, N.; Brockman, G.; et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 2021.

126. Liu, J.; Xia, C.S.; Wang, Y.; Zhang, L. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. arXiv preprint arXiv:2305.01210 2023.

127. Austin, J.; Odena, A.; Nye, M.; Bosma, M.; Michalewski, H.; Dohan, D.; Jiang, E.; Cai, C.; Terry, M.; Le, Q.; et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732 2021.

128. Lai, Y.; Li, C.; Wang, Y.; Zhang, T.; Zhong, R.; Zettlemoyer, L.; Yih, S.W.t.; Fried, D.; Wang, S.; Yu, T. DS-1000: A natural and reliable benchmark for data science code generation. arXiv preprint arXiv:2211.11501 2022.

129. Xu, C.; Sun, Q.; Zheng, K.; Geng, X.; Zhao, P.; Feng, J.; Tao, C.; Jiang, D. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244 2023.

130. Husain, H.; Wu, H.H.; Gazit, T.; Allamanis, M.; Brockschmidt, M. Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 2019.

131. Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I.; et al. Language models are unsupervised multitask learners. OpenAI blog 2019, 1, 9.

132. Koubaa, A. GPT-4 vs. GPT-3.5: A concise showdown 2023.

133. GPT-3. https://en.wikipedia.org/wiki/GPT-3#GPT-3.5, 2023. [Online; Accessed 06-17-2023].

--- TRANG 26 ---
26 của 26

134. Zhang, Y.; Sun, S.; Galley, M.; Chen, Y.C.; Brockett, C.; Gao, X.; Gao, J.; Liu, J.; Dolan, B. Dialogpt: Large-scale generative pre-training for conversational response generation. arXiv preprint arXiv:1911.00536 2019.

135. Microsoft Research-DialoGPT. https://www.microsoft.com/en-us/research/project/large-scale-pretraining-for-response-generation/, 2019. [Online; Accessed 06-17-2023].

136. Padmanabha, N.H. A step-by-step guide to running Vicuna-13B Large Language Model on your GPU / CPU machine. https://www.linkedin.com/pulse/step-by-step-guide-running-vicuna-13b-large-language-nischal/, 2023. [Online; Accessed 06-17-2023].

137. Lieber, O.; Sharir, O.; Lenz, B.; Shoham, Y. Jurassic-1: Technical details and evaluation. White Paper. AI21 Labs 2021, 1.

138. Zeng, A.; Liu, X.; Du, Z.; Wang, Z.; Lai, H.; Ding, M.; Yang, Z.; Xu, Y.; Zheng, W.; Xia, X.; et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 2022.

139. WIKIPEDIA. https://en.wikipedia.org/wiki/PaLM, 2022. [Online; Accessed 06-17-2023].

140. Khadhraoui, M.; Bellaaj, H.; Ammar, M.B.; Hamam, H.; Jmaiel, M. Survey of BERT-base models for scientific text classification: COVID-19 case study. Applied Sciences 2022, 12, 2891.

141. Smith, S.; Patwary, M.; Norick, B.; LeGresley, P.; Rajbhandari, S.; Casper, J.; Liu, Z.; Prabhumoye, S.; Zerveas, G.; Korthikanti, V.; et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990 2022.

142. ClinicalBERT. https://huggingface.co/medicalai/ClinicalBERT. [Online; Accessed 06-17-2023].

143. Hugging Face. https://huggingface.co/UFNLP/gatortron-base. [Online; Accessed 06-17-2023].

144. Alford, A. Google Trains 280 Billion Parameter AI Language Model Gopher. https://www.infoq.com/news/2022/01/deepmind-gopher/, 2022. [Online; Accessed 06-17-2023].

145. Hoffmann, J.; Borgeaud, S.; Mensch, A.; Buchatskaya, E.; Cai, T.; Rutherford, E.; de Las Casas, D.; Hendricks, L.A.; Welbl, J.; Clark, A.; et al. An empirical analysis of compute-optimal large language model training. Advances in Neural Information Processing Systems 2022, 35, 30016–30030.
