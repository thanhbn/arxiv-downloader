# 2308.06767.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/survey/2308.06767.pdf
# File size: 1945191 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 1
A Survey on Deep Neural Network Pruning:
Taxonomy, Comparison, Analysis, and
Recommendations
Hongrong Cheng*, Miao Zhang* †,Member, IEEE , Javen Qinfeng Shi, Member, IEEE
Abstract —Modern deep neural networks, particularly recent large language models, come with massive model sizes that require
significant computational and storage resources. To enable the deployment of modern models on resource-constrained environments
and to accelerate inference time, researchers have increasingly explored pruning techniques as a popular research direction in neural
network compression. More than three thousand pruning papers have been published from 2020 to 2024. However, there is a dearth of
up-to-date comprehensive review papers on pruning. To address this issue, in this survey, we provide a comprehensive review of
existing research works on deep neural network pruning in a taxonomy of 1) universal/specific speedup, 2) when to prune, 3) how to
prune, and 4) fusion of pruning and other compression techniques. We then provide a thorough comparative analysis of eight pairs of
contrast settings for pruning (e.g., unstructured/structured, one-shot/iterative, data-free/data-driven, initialized/pre-trained weights, etc.)
and explore several emerging topics, including pruning for large language models, vision transformers, diffusion models, and large
multimodal models, post-training pruning, and different levels of supervision for pruning to shed light on the commonalities and
differences of existing methods and lay the foundation for further method development. Finally, we provide some valuable
recommendations on selecting pruning methods and prospect several promising research directions for neural network pruning. To
facilitate future research on deep neural network pruning, we summarize broad pruning applications (e.g., adversarial robustness,
natural language understanding, etc.) and build a curated collection of datasets, networks, and evaluations on different applications.
We maintain a repository on https://github.com/hrcheng1066/awesome-pruning that serves as a comprehensive resource for neural
network pruning papers and corresponding open-source codes. We will keep updating this repository to include the latest
advancements in the field.
Index Terms —deep neural network pruning, model compression, model acceleration, large language models, vision transformers,
large multimodal models, diffusion models, edge devices.
✦
1 I NTRODUCTION
Over the past several years, Deep Neural Networks
(DNNs ) have achieved conspicuous progress in various
domains and applications, such as Computer Vision ( CV)
[1, 2], Natural Language Processing ( NLP ) [3, 4], Audio
Signal Processing ( ASP ) [5, 6], and cross-modal applica-
tions [7, 8]. Although DNNs achieve remarkable success
in various areas, their performance relies heavily on model
parameters and computational cost. For example, the widely
used ResNet-50 [9] takes over 95 MB of memory, containing
over 23 million parameters [10]. BERT BASE [3] is around 440
MB with 110 million parameters, GPT-3 includes up to 175
billion parameters [11], and GPT-4 has even more. The trend
of enlarging neural network size is anticipated to persist.
However, the more parameters of DNNs, the more time
and memory space they typically require for processing the
inputs [12]. The high training and inference costs associated
with these models present a significant challenge to their de-
ployment on devices constrained by limited computational
resources (such as CPU, GPU, and memory), energy, and
bandwidth [13, 14, 15]. For example, real-life applications
•* Equal contribution. †Corresponding author.
H. Cheng and J. Q. Shi are with the University of Adelaide. M. Zhang is
with Harbin Institute of Technology (Shenzhen).
E-mail: {hongrong.cheng, javen.shi }@adelaide.edu.au,
{zhangmiao }@hit.edu.cn.
Manuscript received May 01, 2023; revised June 11, 2024.such as autonomous driving, field rescue, and bushfire
prevention necessitate high accuracy and efficient resource
usage, including fast real-time response and compact mem-
ory footprint. Deep neural networks’ computational com-
plexity and memory footprint can make them impractical
for deployment on edge devices [16]. With the popularity
of Large Language Models ( LLMs ) in recent years, there is
growing interest in compressing neural networks for com-
puters with flexible hardware requirements [17]. In addition,
deep neural networks that contain redundant features can
undermine their robustness, elevating the risk of adversarial
attacks [18]. For instance, high-dimensional feature spaces
created by these networks can provide more entry points
for adversarial attacks, undermining the network’s ability
to generalize beyond its original training data.
To relieve this issue, researchers have proposed var-
ious neural network compression techniques to design
lightweight models, including neural network pruning
([19, 20]), low-rank factorizations of the weight matrices
([21, 22]), quantization ([23, 24]), knowledge distillation
([25, 26]), neural architecture search ([27, 28]), and other
techniques ([29, 30]). Among them, there is continuing inter-
est in neural network pruning, which has been proven as a
desirable and effective way to save memory space and com-
putation time at inference while maintaining a comparable
or even better performance compared to the original DNNs.
As shown in Fig. 1, the number of papers on pruning hasarXiv:2308.06767v2  [cs.LG]  9 Aug 2024

--- PAGE 2 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 2
19891990199119921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023
Year02505007501000125015001750Number of PapersNeural Network Compression (total: 9295)
Neural Network Pruning (total: 5025)
Fig. 1: The number of papers on neural network pruning
and compression from 1989-2023.1
been markedly increasing since 2015. It presents more than
half of the papers on neural network compression.
Research on pruning can be traced back to literature as
early as 1988 [31]. However, it was only until the emergence
of [13] that the research community realized the potential
of pruning in removing significant redundancy in deep
neural networks, and pruning began to gain widespread
attention. Several pieces of literature review prior work
on deep neural network pruning, as shown in Table 1.
Although these works overview several aspects of prun-
ing and provide helpful guidance for researchers, many of
them (e.g., [32, 33, 34, 35]) focus on multiple compression
techniques, such as pruning, quantization, and knowledge
distillation, with only brief examination of each technique.
For example, Mishra et al. [32] summarize compression
techniques, including pruning, quantization, low-rank fac-
torization, and knowledge distillation, where pruning is pri-
marily introduced from channel/filter pruning, and many
essential pruning techniques (such as lottery ticket hypoth-
esis) are not included. Some reviews concentrate on one
specific aspect. For example, Wang et al. [36] provide only
an overview of pruning at initialization and do not include
studies on pruning during training, pruning after training,
etc. He and Xiao [37] focuses only on structured pruning and
does not discuss the other two common types of pruning:
unstructured and semi-structured pruning. Ghimire et al.
[33]) focus on reviewing Convolutional Neural Network
(CNN ) pruning and lack descriptions of pruning for other
deep neural networks, such as Recurrent Neural Networks
(RNNs ) [38], Transformer based models [17], and diffusion
models [39]. Some recent review works (such as [35, 40])
survey compression techniques for large models, encom-
passing a broad array of topics from pruning, quantization,
knowledge distillation, etc. Among these, pruning is dis-
cussed concisely.
This survey aims to provide a comprehensive overview
of deep neural network pruning for diverse readers. We
review representative pruning methods, propose a new
taxonomy, conduct a comprehensive analysis of how dif-
ferent pruning techniques perform in practice, and give
practitioners who wish to utilize pruning recommendations
on choosing a suitable pruning method for various require-
ments. Our contributions are as follows:
(1) Comprehensive review. To our best knowledge, this
survey is the most comprehensive overview of modern
deep neural network pruning techniques. It distills ideas
from over 300 related academic papers, covering pruning
1. The data is from https://www.webofknowledge.com.TABLE 1: Representative surveys and descriptions.
Survey Main work description Year
[41] survey of pruning methods 1993
[42] overview 81 papers and propose ShrinkBench, 2020
an open-source framework for evaluation
[43] pruning+weight sharing+low-rank matrix+ 2020
KD+quantization
[44] pruning criteria+procedure 2020
[45] pruning+quantization+KD+low-rank factor. 2020
[32] pruning+KD+NAS+Tensor-Decompose 2020
+quantization+hardware acceleration
[46] work on sparsity in deep learning up to 2020 2021
[36] overview pruning at initialization 2022
[33] pruning+KD+NAS+quantization 2022
+Tensor-Decompose+hardware accel.
[37] structured pruning 2023
[40] pruning+KD+quantization+others for LLM 2023
[47] pruning+KD+quantization+others for LLM 2024
[34] pruning+KD+quantization+others for LLM 2024
[35] model compression+others for LLMs 2024
methods for small to medium models to cutting-edge large
models. In addition, we establish a new taxonomy, as shown
in Fig. 2, and provide detailed descriptions of the represen-
tative methods for each class of pruning methods.
(2) Comparative experiments and analysis. We conduct
a comparative analysis of eight pairs of contrast settings for
pruning and emerging advances, such as pruning for LLMs
and different supervision levels for pruning. Unlike existing
surveys, we conduct experiments and provide discussions.
(3) Collection of abundant resources. We summarize
miscellaneous pruning applications and provide benchmark
datasets, networks, and evaluations for different applica-
tions. Our collected resources in Appendix D can guide re-
searchers and practitioners in understanding, utilizing, and
developing different network pruning methods for various
requirements. Ongoing updates of representative pruning
efforts are available at https://github.com/hrcheng1066/a
wesome-pruning.
(4) Recommendations and future directions. This sur-
vey provides valuable recommendations for choosing ap-
propriate pruning methods for different application require-
ments and highlights promising future research directions.
The remainder of this survey is organized as follows. In
Section 2, we establish a clear taxonomy of pruning. Section
3 - 6 offer an overview of speedup, when to prune, and
how to prune, followed by a comprehensive comparative
analysis of different kinds of pruning methods in Section
7. Section 8 discusses integrating pruning with other com-
pression methods. Practical recommendations for choosing
pruning methods and future directions are provided in
Section 9. We conclude this paper in Section 10.
2 T AXONOMY
There are three critical questions when pruning a deep neu-
ral network. (1) Whether to achieve universal or specific
acceleration through neural network pruning? Universal
acceleration operates independently of specialized hard-
ware/software, while specific acceleration relies on it. (2)
When to prune the neural network? Specifically, is the
neural network pruned before, during, or after training
the network for static pruning or dynamic (i.e., run-time)
pruning? (3) Whether to prune based on specific criteria

--- PAGE 3 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 3
Deep Neural Network Pruning 
Sec. 3
Universal/Specific 
Speedup
Unstructured
Structured
Semi -
structuredSec. 3.1
Sec. 3.2
Sec. 3.3Pruning Criteria
Sensitivity 
and/or Saliency
Loss ChangelpNormMagnitudeSec. 5.1
Sec. 5.2
Sec. 5.3
Sec. 5.4Sec. 5
Learn to Prune
Sparsity Regularization 
based Pruning
Meta Learning based 
Pruning
Graph Neural
Network based Pruning 
Reinforcement Learning 
based PruningSec. 6.1
Sec. 6.2
Sec. 6.3
Sec. 6.4Sec. 6
Pruning&
Quantization
Pruning&Tensor
Decomposition
Pruning&NAS
Pruning&KDFusion of Pruning 
and Other TechniquesSec. 8
When to Prune
Pruning before 
Training
Pruning during 
Training
Pruning after 
TrainingSec. 4.1
Sec. 4.2
Sec. 4.3Sec. 4
Run-time 
PruningSec. 4.4One-shot/
Iterative
Initialized /
Pre-trainedData -Free/
Data -DrivenTraining from 
Scratch/ Fine-tuning
Original Task /
Transfer Learning
Static /Dynamic
Layer -wise 
Weight DensityA Comprehensive Comparative 
AnalysisSec. 7
Sec.7.2
Sec.7.3
Sec.7.4Sec.7.8Sec.7.6
Sec.7.7Unstructured /
StructuredSec.7.1
Sec.7.9
Levels of 
SupervisionSec.7.10 Sec.7.5
Global /LocalSuggestions and 
Future Directions
Sec.9.1
Sec.9.2Recommendations
Future DirectionsSec. 9
Pruning& Multi -
compression Tech.
Fig. 2: An overview of the hierarchical structure of the survey, with sections involving large models highlighted by a pink triangle.
or learn to prune? The answers to the three questions corre-
spond to the three primary aspects of deep neural network
pruning, as depicted in the orange sections of Fig. 2.
The first question is whether speedup depends on
specific hardware/software. It is usually divided into three
types: unstructured ([48, 49, 50]), semi-structured (also
called pattern-based) ([17, 51, 52]) and structured ([20, 39,
53]). Only structured pruning can achieve universal neu-
ral network acceleration without requiring special hard-
ware or software. Conversely, both unstructured and semi-
structured pruning need the support of special hardware
or software. Given that the primary objective of pruning is
acceleration, the first question addresses the most funda-
mental and user-concerned aspect of this process.
The second question particularly concerns the arrange-
ment between pruning weights and training weights of
the neural network for static pruning. Based on whether
pruning is performed before, during, or after training the
network, static pruning can be divided into three categories:
pruning before training ( PBT ) ([50, 54, 55, 56]), pruning dur-
ing training ( PDT ) ([57, 58, 59]), and pruning after training
(PAT ) ([20, 48, 60]). In dynamic pruning, subnetworks are
generated at run-time for each input data.
The third question considers whether to prune neural
networks using specific criteria or by learning. Criteria
rely on a scoring formula to measure the importance of
each weight (or filter, neuron, and so on). Commonly used
pruning criteria include magnitude, norm, loss change, etc.
In addition, it is possible to prune neural networks by learn-
ing, such as through sparsity regularization training [59]
or dynamic sparse training [61]. Whether through criteria
or learning, pruning aims to determine the weights of a
network that should be pruned.
The above three aspects determine the main characteris-
tics of a pruning algorithm. Different combinations of these
three aspects form various pruning methods. We provide a
new taxonomy of deep neural network pruning in Section 3
- 6, and Section 8, as shown in Fig. 2.
3 S PECIFIC OR UNIVERSAL SPEEDUP
This section categorizes pruning into unstructured, semi-
structured, and structured. The first two types correspondto specific speedup, while the third corresponds to universal
speedup.
3.1 Unstructured Pruning
Unstructured pruning, also called non-structured pruning
or weight-wise pruning, is the finest-grained case.
Definition 1 (Unstructured Pruning). Given neural net-
work weights W={w0, w1, ..., w K}, a dataset D=
{(xi,yi)}N
i=1composed of input ( xi), output ( yi) pairs, and
a desired total number of non-zero weights k(less than
K), unstructured pruning can be written as the following
constrained optimization problem [54]:
min
WL(W;D) =min
W1
NNX
i=1ℓ(W; (xi,yi)),
s.t.∥W∥0≤k.(1)
In practice, for small or medium models, unstructured
pruning usually does not directly set the weights to 0
but sets their corresponding masks (or indicators) Mto 0
[56, 62]. In this case, unstructured pruning is regarded as
applying a binary mask to each weight. Consequently, Eq.(1)
is correspondingly changed as:
min
w,mL(W⊙M;D) =min
W,M1
NNX
i=1ℓ(W⊙M; (xi.yi)),
s.t.∥M∥0≤k.(2)
Generally, the network is retrained (i.e., fine-tuning or
training-from-scratch) with fixed masks M, and the masked-
out weights are not involved in retraining. In the case of
large models, such as LLMs, assigning a mask to each
weight poses a challenge due to the immense number of
weights. It is common to directly set the weights that need
to be pruned to zero, as seen in [17, 49]. Fig. 3 is an example
of weight-wise pruning by removing the weights directly (as
shown in Fig. 3 (a)) or masking the weights with their corre-
sponding masks (as shown in Fig. 3 (b)), respectively. Since it
can remove weights anywhere, the irregular replacement of
non-zero weights leads to actual acceleration requiring the
support of special software and/or hardware [13, 58, 63, 64].
Therefore, we classify unstructured pruning as a specific
speedup technique.

--- PAGE 4 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 4
Unpruned Network Pruned Network
(a) From the view of neurons and connections
Conv
BN-ReLU0.22 -0.31 0.05
-0.01 1.23 0.92
1.07 -0.01 1.08Filter1
-0.08 0.01 -0.06
1.23 0.05 0.17
0.74 -1.29 0.83-0.41 0.56 1.13
-0.77 -0.92 0.03
0.03 0.28 0.04Filter2 Filter3
⊙
Filter1 Masks0.22 -0.31 0.05
-0.01 1.23 0.92
1.07 -0.01 1.081 1 0
0 1 1
1 0 10.22 -0.31 0
0 1.23 0.92
1.07 0 1.08
Pruning result (b) From the view of weights and masks
Fig. 3: The visualization of unstructured pruning. The light orange circles denote neurons.
FiltersFeatures×
FeaturesLayer i Layer i+1
Features
FiltersChannels
(a) Structured pruning for CNNs
Layer -wise Row/column -wiseModule
Layer L
…
Layer 0…
X0Module i1…Module ik
XiXi+1XL+1Layer
0 0 0
-0.01 0 0.92
1.07 0 1.080
-0.22
0.42Concat
Scaled Dot -Product 
Attention
Linear Linear LinearLinearHead
Head -wiseQ K V (b) Structured pruning for Transformers
Pattern -wiseConv. Module
1.23 0 0
-0.01 0 0.92
0 0 1.080.42
0
0
2:4 (c) Examples of semi-structured pruning
Fig. 4: The visualization of typical structured and semi-structured pruning: each Conv. consists of cubes representing
weights. Orange cube sets/blocks indicate the pruned structures. Best view in color and zoom in.
3.2 Structured Pruning
Definition 2 (Structured Pruning). Given a specific prune
ratio and a neural network with S={s1, s2, ..., s L}, where
sican be the set of channels, filters, neurons, or transformer
attention heads in layer i. Structured pruning aims to search
forS′={s′
1, s′
2, ..., s′
L}to minimize performance degen-
eration and maximize speed improvement under the given
prune ratio, where s′
i⊆si,i∈ {1, .., L}.
Structured pruning removes entire filters ([15]), channels
([19, 62]), transformer attention heads ([20]), or even lay-
ers ([65]), as shown in Fig. 4 (a) and Fig. 4 (b), and can
rebuild a narrow model with a regular structure2. It does
not require the support of special hardware and software
(such as sparse convolution libraries) and can directly speed
up networks and reduce the size of the neural networks
[16, 62, 63, 67].
3.3 Semi-structured Pruning
To improve the flexibility of structured pruning and achieve
a lower accuracy drop when the pruning rate is high,
some works ([51, 52]) introduce semi-structured pruning,
also called pattern-based pruning in [52], to achieve high
accuracy and structural regularity simultaneously. Some
examples are shown in Fig. 4 (c). For example, Meng et al.
[51] treat a filter as several stripes and propose to prune
stripes in each filter. SparseGPT [17] introduces a 2:4 or 4:8
sparsity pattern to reduce a LLM’s parameters by half. In
this configuration, at least two zeros are mandatory in every
set of four consecutive values. The 2:4 pattern can utilize
NVIDIA Ampere GPU’s sparse tensor cores to accelerate
2. For Transformers [66], deleting rows (columns) of weight matrices
is similar to pruning filters (channels) for CNNs.matrix multiplication [60]. In contrast, structured pruning
is classified as coarse-grained structured pruning ([52, 68]),
while semi-structured pruning is classified as fine-grained.
4 W HEN TO PRUNE
This section distinguishes three pipelines for static pruning,
as illustrated in Fig. 5, and run-time pruning. The exam-
ples of the three pipelines of static pruning are shown in
Fig. 6. Some notations and statistics on three static pruning
pipeline types are shown in Appendix A and D, respectively.
4.1 Pruning Before Training
Pruning Before Training ( PBT ) (as shown in Table 2), also
called foresight pruning [56] or pruning at initialization
[50, 54], represents a class of pruning methods that use
randomly initialized weights for pruning the network. The
principal motivation of PBT methods is to eliminate the cost
of pre-training. Without loss of generality, we define a neu-
ral network as a function f(x;W⊙M). The mask Mis used
for pruning initialized weights W0sampled from a spe-
cific initialization distribution. After pruning, the network
f(x;W0⊙M′)is trained to converge to f(x;Wt⊙M′)after
tepochs, where M′indicates the sparsity after pruning.
PBT usually follows two steps: directly prunes untrained
dense network based on a specific criterion and then trains
the sparse network to convergence for high performance, as
illustrated in Fig. 5 (a). The second step is related to static
sparse training [69] that aims to train a sparse network with
a fixed sparse pattern. By avoiding the time-consuming pre-
training process, PBT methods achieve the gains at training
and inference time.
3. See Appendix A for the notations in Table 2 ˜ Table 4.

--- PAGE 5 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 5
TABLE 2: Representative methods of pruning before training. “U/S” denotes unstructured or structured pruning.3
Method Criterion U/S Data-Driven (Y/N) One-shot (Y/N)
SNIP (2019) [54] |∇WL(W)⊙W| U Y Y
GraSP (2020) [56] −(H∇WL(W))⊙W U Y Y
Smart-Ratio (2020) [55] keep-ratios U N Y
SynFlow (2020) [50] ∇WRSF(W)⊙W U N N
PFS (2020) [70] l1-norm based sparsity regularization S Y N
RST (2022) [71] l2-norm based sparsity regularization U Y N
Randomly 
initialized
networkPrune or
Prune&Train
a few stepsPruned 
networkPruned 
networkPre-trainPretrained 
network(a) Pruning before training pipeline 
(b) Pruning during training pipeline Randomly 
initialized
networkPrunePruned 
networkTrain 
from 
ScratchPruned 
network① ②
Randomly 
initialized
networkJointly 
Prune & 
TrainPruned 
network
Train 
from 
Scratch/
Fine-tunePruned 
network①
②
① ②
(c) Pruning after training pipeline Train 
from 
Scratch/
Fine-tune③
Fig. 5: The typical pipelines of static pruning. Dashed boxes represent models and solid boxes denote actions. Dashed
arrows indicate optional steps.
Currently, PBT primarily targets CNNs. Lee et al. [54]
pioneer the research of PBT and propose a Single-shot
Network Pruning ( SNIP ) method to remove the weights
whose absence leads to the slightest loss change. Lee et al.
[72] explain the feasibility of SNIP through signal propa-
gation, empirically find pruning damages the dynamical
isometry [73] of neural networks, and propose a data-free
orthogonal initialization, an approximation of exact isom-
etry, to prune random networks. Different from the signal
propagation perspective in [72], which focuses on initial-
ization scheme, Wang et al. [56] propose Gradient Signal
Preservation ( GraSP ) to exploit gradient norm after pruning
to remove weights that have the least effect on the gradient
flow. Tanaka et al. [50] go a step further and point out that an
effective subnetwork can be identified without training and
looking at the data. They propose a data-free pruning algo-
rithm called Iterative Synaptic Flow Pruning ( SynFlow ) that
uses the concept of synaptic flow to consider the interaction
of different layers and avoid layer collapse through gradual
pruning. Gebhart et al. [74] exploit the Path Kernel (i.e., the
covariance of path values in a network) based on Neural
Tangent Kernel [75] to unify several initialization pruning
methods, such as SNIP [54], CraSP [56], SynFlow [50], under
a single framework. Some works ([55, 76]) suggest that the
delicate pruning criteria may not be crucial for performance,
but the layerwise sparsity. For example, Su et al. [55] pro-
pose randomly pruning each layer by using a series of
layerwise keep-ratios (smart-ratios) without using any data.
Inspired by Weight Agnostic Neural Networks ( WANNs )
[77], Ramanujan et al. [78] empirically find that with an
untrained network grows wider and deeper, it will contain
a subnetwork that performs as well as a trained network
with the same number of parameters. Then they propose the
edge-popup method to identify such randomly initialized
subnetworks. Unlike this method, which maintains ran-domly weighted values, Bai et al. [71] propose Dual Lottery
Ticket Hypothesis ( DLTH ) where both the subnetwork and
weights are randomly selected at initialization and propose
Random Sparse Network Transformation ( RST ) which fixes
the sparse architecture but gradually train the remaining
weights.
Some recent works ([69, 70, 79]) explore why an effective
subnetwork can be identified at initialization. For example,
Liu et al. [69] empirically demonstrate that the network size
and appropriate layer-wise prune ratios are two vital factors
in training a randomly pruned network at initialization from
scratch to match the performance of the dense models. In
contrast, Kumar et al. [80] find that at the same prune ratio,
subnetworks derived by pruning during or after training
exhibit higher effective parameter count and greater expres-
siveness compared to those pruned at initialization.
4.2 Pruning During Training
Pruning During Training ( PDT ) (as shown in Table 3) gen-
erally takes a randomly initialized dense network f(x;W0)
as the input model and jointly trains and prunes the neural
network by updating weights Wand masks of weights (or
filters, channels, etc.) Mduring training. These dynamic
schemes change the masks and produce the subnetwork
f(x;Wt⊙Mt)after titerations/epochs. After pruning,
many PDT methods ([59, 81, 82, 83, 84, 85, 86]) directly
obtain the subnetworks without the need for training-from-
scratch or fine-tuning. The typical pipeline of PDT is illus-
trated in Fig. 5 (b). PDT methods have been less explored
due to the more complicated dynamic process than PBT
and PAT methods. We summarize the main prior solutions
into four paradigms: (1) sparsity regularization based, (2)
dynamic spare training based, (3) score-based, and (4) dif-
ferentiable pruning based. Methods related to (2) conduct

--- PAGE 6 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 6
TABLE 3: Representative methods of pruning during training. “Retrain” refers to training from scratch or fine-tuning,
“U/S” denotes unstructured or structured pruning.
Method Object Function/Criterion Retrain (Y/N) U/S
Network Slimming (2017) [57] minW,γℓ(y, f(x;W)) +λ∥γ∥1 Y S
SSS (2018) [59] minW,γℓ(y, f(x;W,γ)) +R(W) +λ∥γ∥1 N S
SET (2018) [91] ∥W∥for drop and random for grow N U
DSA (2020) [85] minALv(W(A),A)) N S
GraNet (2021) [92] ∥W∥for drop and gradient for grow N U&S
FreeTickets (2022) [93] ∥W∥for drop and gradient for grow N U
Num of remained weightsoriginal
Iterations0Train from Scratch
𝑡1
(a) SNIP [54] as an example of PBT
Num of remained weightsoriginal
Iterations0Fine-tunePrune 
& 
Train
𝑡1 𝑡2 (b) FCF [89] as an example of PDT
Num of remained weightsoriginal
Iterations0Fine-tunePrune 
& TrainPre-train
𝑡1 𝑡2 𝑡3 (c) GFP [62] as an example of PAT
Fig. 6: The illustration of typical sparsity changes for PBT, PDT, and PAT (best view in color and zoom in). PBT is usually
the cheapest, while PAT is the most expensive.
sparse-to-sparse training, while the other three take dense-
to-sparse training.
4.2.1 Sparsity Regularization based Methods
Sparsity regularization technique [58] is commonly used in
PDT methods. This category of methods starts with dense
networks, imposes sparse constraints on loss functions, and
usually zeros out some weights or their masks during
training. The main effort is to design the effective target
loss function Lwith an advanced penalty scheme and
efficient optimization algorithms. For example, Wen et al.
[58] propose Structured Sparsity Learning ( SSL) to learn a
sparse structure by group LASSO [87] regularization during
training. However, SSL requires computing the gradients of
the regularization term w.r.t. all the weights, which is non-
trivial. Gordon et al. [88] propose MorphNet that reuses
the parameters of Batch Normalization ( BN) and conducts
sparsity regularization on these parameters. However, some
networks (e.g., certain VGGNets [1]) have no BN layers. In-
stead of reusing BN parameters, some works associate scal-
ing factors with channels, layers, etc. For example, Huang
and Wang [59] propose Sparse Structure Selection ( SSS) that
associates scaling factors for CNN micro-structures (e.g.,
channels, residual blocks) and exploit sparsity regulariza-
tion to force the output of the micro-structures to zero, rather
than pushing the weights in the same group to zero as in
[58]. In addition, SSS does not require extra fine-tuning in
[58]. Li et al. [89] propose Factorized Convolutional Filter
(FCF) which introduces a binary scalar to each filter and
proposes a back-propagation algorithm with Alternating
Direction Method of Multipliers ( ADMM ) [90] to train the
weights and the scalars during training jointly.
4.2.2 Dynamic Sparse Training based Methods
A class of the PDT methods ([81, 91, 92, 94, 95, 96, 97]) take
randomly initialized sparse network rather than dense net-
work as the input model. Subsequently, a common method
involves pruning a fraction of unimportant weights and
then regrowing the same number of new weights to adjustthe sparse architecture. By repeating the prune-and-grow
cycle during training, these method keeps searching for
better sparse architecture, a process classified as dynamic
sparse training in [81].
For example, Mocanu et al. [91] propose Sparse Evolu-
tionary Training ( SET ) that removes the smallest positive
and most negative weights and grows new weights in ran-
dom locations. Instead of pruning a fixed fraction of weights
at each redistribution step, such as in SET [91], Mostafa
and Wang [95] propose Dynamic Sparse Reparameterization
(DSR ) which uses an adaptive threshold for pruning. In ad-
dition, DSR [95] reallocates weights across layers and does
not restrict to the inner-layer weight redistribution in SET
[91]. Liu et al. [93] propose an ensemble method FreeTickets
that ensemble sparse subnetworks created by sparse-to-
sparse methods. Rather than using a random regeneration
scheme, Liu et al. [92] propose Gradual Pruning with zero-
cost Neuroregeneration ( GraNet ) to remove connections of
networks based on their weight magnitudes and regrow
connections of networks based on their gradient. They argue
that even the weights with zero gradient values indicate the
connection importance. Sokar et al. [98] pioneer to explore
dynamic sparse training in Reinforcement Learning ( RL).
Evci et al. [99] analyze the reasonability of dynamic
sparse training and find that sparse networks have poor
gradient flow at initialization, but dynamic sparse training
significantly improves gradient flow.
4.2.3 Score-based Methods
Some PDT methods exploit scoring criteria to prune net-
works during training. He et al. [83] propose Soft Filter
Pruning ( SFP) filter pruning method which can train net-
works from scratch and prune them simultaneously by
using the l2norm of each filter as its importance measure.
Instead of associating masks with filters, they directly set
the pruned filter weights to zero, which can be updated
from zero through the forward-backward process. There-
fore, pruned filters in one epoch can be recovered in the
next. However, SFP requires manually preset a prune ratio

--- PAGE 7 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 7
TABLE 4: Representative methods of pruning after training. “U/S” denotes unstructured or structured pruning.
Method Object Function/Criterion U/S
Auto-Balance (2018) [105] score(Fi,j)=∥Fi,j∥1 S
LTH (2019) [48] score( wi)=∥wi∥1 U
Taylor-FO-BN (2019) [106] score( wi)=(∇wiℓ⊙wi)2S
SSR (2019) [107] min
W1
NPN
i=1ℓ(yi, f(xi, W)) +λR(W) S
GReg (2021) [108] score(Fi,j)=∥Fi,j∥1 U&S
GFP (2021) [62] score( ci)=PN
a=1(∇miℓa)2or score( ci)=PN
a=1(P
g∈Group∇mg
iℓa)2S
SparseGPT (2023) [17] score( wij)=[|W|2/diag((xTx+λI)−1)]ij U
Wanda (2023) [49] score( wij)=|wij| · ∥xj∥2 U
LLM-Pruner (2023) [20] score( wi)=|∇wiL ⊙wi−1
2PN
j=1(∇wiL ⊙wi)2| S
ShortGPT (2024) [65] 1−xT
ixi+1
∥xi∥2∥xi+1∥2S
LLM-Streamline (2024) [109] maxlayer icos(xi,xi+n) S
for each layer. He et al. [100] propose Filter Pruning via
Geometric Median ( FPGM ) to prune redundant filters that
are nearest to the geometric median [101] of the filters within
the same layer. However, FPGM also requires a pre-defined
prune ratio for each layer. Liu et al. [57] propose a method
called Network Slimming that introduces a scaling factor
for each channel and jointly trains the weights and the
scaling factors by adding the regular loss ℓwith sparsity
regularization on the factors, and the magnitudes of these
scaling factors are used as the filter scores. In practice, they
reuse the γparameters in BN layers as the scaling factors.
4.2.4 Differentiable Pruning based methods
Advances in differentiable network compression ([27, 28])
popularize differentiable techniques for pruning, such as
[85, 86, 102, 103, 104]. For example, Ning et al. [85] propose
Differentiable Sparsity Allocation ( DSA ) to set layer-wise
prune ratios. They soften the hard pruning with a differ-
entiable method using masks drawn from a distribution
governed by prune ratios, allowing for gradient calculation
of the target loss w.r.t. these ratios. These gradients then
measure layer sensitivity. Guo et al. [102] model channel
pruning as a differentiable Markov process with architecture
parameters, representing each channel as a state and using
state transitions to determine the probability of retaining a
channel based on the retention of its predecessor. Unlike [85]
and [102], Cho et al. [86] generate soft pruning masks for
weights without extra trainable parameters to accomplish
differentiable pruning for CNNs and Transformers.
4.3 Pruning After Training
Pruning After Training ( PAT ) (as shown in Table 4) is the
most popular type of pruning pipeline because it is com-
monly believed that pre-training the dense network is nec-
essary to obtain an efficient subnetwork [110]. Especially for
large models, such as LLMs and diffusion models, pruning
is specifically applied to pre-trained models. This class of
pruning methods typically follows a Pretrain-Prune-Retrain
([62], [20]) or Pretrain-Prune ([17, 19]) process, as shown in
Fig. 5 (c). (1) Pre-train a randomly initialized dense network
f(x;W0)to converge to f(x;Wt). (2) Prune the weights (or
filters, neurons, etc.) that have the least influence on perfor-
mance and fine-tune the pruned network f(x;W′
t⊙M′)for
several iterations, where W′
tandM′are the weights and
masks after pruning, respectively. This step can be done at
least once (i.e., one-shot pruning) or multiple times (i.e.,
iterative pruning). (3) Train the remaining weights fromscratch f(x;W0⊙M′′)or fine-tune f(x;W′′
t⊙M′′)to
recover the performance [111], where W′′
tandM′′are the
final results of weights and masks after the pruning process,
respectively. Sparsity may gradually increase during the
pruning process until it achieves the target.
4.3.1 LTH and its Variants
Lottery Ticket Hypothesis ( LTH ) [48, 112] is one of the most
influential hypotheses in the neural network pruning do-
main. Given a pre-trained network, LTH iteratively removes
a percentage of the weights based on their magnitudes.
After pruning, the remaining weights are retrained from
scratch with the original initialization, rather than random
reinitialization, to match the original networks’ accuracy.
It challenges the commonly held belief that pre-trained
weights must be used for retraining and conjectures the
existence of an independently trainable sparse subnetwork
within a dense network. Inspired by LTH, there are various
follow-up works to identify wider tickets across multiple
kinds of neural networks (such as CNNs [113], Genera-
tive Adversarial Networks ( GANs ) [114], Variational Au-
toEncoders ( V AEs ) [115], Graph Neural Networks ( GNNs )
[116], Transformer based models [117]) and understand LTH
better, which can be classified into five main classes: (1)
proposing a stronger lottery ticket hypothesis, (2) exploring
the transferability of LTH, (3) generalizing LTH to other
contexts, (4) theoretical justification, and (5) revisiting and
questioning LTH.
(1) Some recent works ([113, 118, 119]) prove stronger
hypothesis than LTH [48]. For example, Diffenderfer and
Kailkhura [113] propose a stronger Multi-Prize LTH, which
claims that winning tickets can be robust to extreme forms
of quantization (i.e., binary weights and/or activations).
Based on this, they introduce the Multi-Prize Tickets ( MPTs )
algorithm to find MPTs on binary neural networks for the
first time.
(2) Some literature ([120, 121, 122, 123]) studies the
transferability of a winning ticket found in a source dataset
to another dataset, which provides insights into the trans-
ferability of LTH. For example, S. Morcos et al. [120] find
OneTicket that can generalize across a variety of datasets
and optimizers within the natural image domain. Mehta
[121] propose the ticket transfer hypothesis and transfer
winning tickets for different image classification datasets.
(3) In addition to image classification, LTH has been ex-
tended to other contexts, such as node classification and link
prediction ([123]), vision-and-language ([122]), and NLP

--- PAGE 8 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 8
([117]). For example, Chen et al. [123] pioneer to generalize
LTH to GNNs and propose Graph Lottery Ticket ( GLT ).
Prasanna et al. [117] explore the existence of winning tick-
ets for fine-tuned BERT [3] and identify subnetworks that
match the model’s performance.
(4) On one hand, some literature ([124, 125, 126]) ana-
lyzes the reasons why LTH [48] is able to win. For example,
Zhang et al. [124] exploit dynamical systems theory and
inertial manifold to theoretically verify the validity of LTH.
Evci et al. [99] observe that the success of LTH lies in
effectively re-learning the original pruning solution they
are derived. Zhang et al. [125] pioneer to provide a formal
justification for the improved generalization of winning
tickets observed in experimental results from LTH.
(5) On the other hand, some recent works ([110, 127, 128])
revisit and challenge the existence of LTH. For example, Ma
et al. [127] provide a more rigorous definition of LTH for
precisely identifying winning tickets and find that whether
and when the winning tickets can be identified highly
replies on the training settings, such as learning rate, train-
ing epochs, and architecture characteristics like network
capacities and residual connections. It is more likely to
find winning tickets by using a small learning rate or an
insufficient number of training epochs.
It is worth pointing out that in some works [18, 36],
LTH [48] is classified as a PBT method. However, LTH
selects masks based on a pre-trained network, which does
not conform to the definition of PBT that attempts pruning
the initialized network before training. Therefore, it is more
reasonable to classify LTH as a PAT method.
4.3.2 Other score-based Methods
The most straightforward and intuitive way to select prun-
ing candidates is to evaluate them based on their norms.
For example, Han et al. [13] propose to measure weight
importance by its absolute value. In addition to norm-
based criteria, evaluating loss change with and without the
weights is also popular. For example, Nonnenmacher et al.
[67] propose Second-order Structured Pruning ( SOSP ) to
selectively zero out filter masks to minimize the effects of
the loss change from removing some filters. Ma et al. [20]
propose LLM-Pruner to pioneer the removal of unimportant
coupled channels and multi-attention heads in LLMs using
the first-order Taylor expansion to estimate loss changes and
fine-tune the pruned models using LoRA [129]. Fang et al.
[39] present Diff-Pruning which scores weights in diffusion
models using the first-order Taylor expansion over pruned
timesteps. Shi et al. [130] introduce Unified and Progressive
Pruning ( UPop ), which prunes large multimodal models
by leveraging accumulated trainable mask gradients during
each iteration of the search phase. In addition to using loss
change, some works ([65, 131, 132, 133]) design new metrics.
For example, Men et al. [65] introduce a metric called Block
Influence ( BI), which assesses the significance of a layer by
measuring the extent to which it alters the hidden states and
removes redundant layers of LLMs. Kim et al. [133] assess
the significance of a layer in LLMs by measuring its impact
on perplexity ( PPL) upon its removal.4.3.3 Sparsity Regularization based Methods
Some works ([107, 134, 135, 136, 137]) exploit sparsity reg-
ularization technique. For example, He et al. [134] propose
an alternative two-step algorithm that introduces a scalar
mask to each channel of a pre-trained CNN model, selects
redundant channels based on LASSO regression [138], and
reconstructs the outputs of unpruned channels using lin-
ear least squares. Energy-Constrained Compression ( ECC )
[139] builds an energy consumption model via a bilin-
ear regression function. Network Pruning via Performance
Maximization ( NPPM ) [140] trains a performance predic-
tion network and uses it as a proxy of accuracy to guide
searching for subnetworks based on regularization penalty.
Fang et al. [38] develop a general method called DepGraph
to analyze dependencies in various network structures
(e.g., CNNs, RNNs, GNNs, Transformers) and propose a
structured pruning based on sparsity regularization. Using
thel0regularization approach, Xia et al. [137] efficiently
learn pruning masks that match the target architecture and
maximize performance by jointly optimizing weights and
pruning masks with a min-max objective for LLMs. Some
methods ([38, 53, 105, 108]) select important weights (or
filters, neurons, etc.) by combining norm-based criteria and
sparsity regularization.
4.3.4 Pruning in Early Training
Instead of fully training a network from f(x;W0)to
f(x;WT), this class of methods explore the network archi-
tecture by training a network only for a few iterations or
epochs, i.e., f(x;Wt), where t << T . For example, You et al.
[141] propose Early-Bird ( EB) tickets which indicate that
winning tickets can be identified at the early training stage
via inexpensive training schemes (e.g., early stopping and
low-precision training) at large learning rates and achieve
similar performance to the dense network. Inspired by
[141], Chen et al. [142] propose EarlyBERT that identifies
structured winning tickets in the early stage of BERT [143]
training. Frankle et al. [144] find that, in large-scale settings
(such as ResNet-50 and Inception-v3 on ImageNet), the
subnetworks that exhibit stability to SGD noise are able to
reach full accuracy early in training.
4.3.5 Post-Training Pruning
In contrast to the general PAT methods that follow the
Pretrain-Prune-Retrain procedure, recently proposed post-
training pruning methods simplify the three-step process
Pretrain-Prune. It involves pruning a pre-trained model
f(x;Wt)without retraining, typically achieving negligible
accuracy loss by using compensation mechanisms to miti-
gate performance degradation. This class of pruning meth-
ods is particularly attractive for billion-parameter models
because retraining such pruned models is still very expen-
sive. For example, Kwon et al. [145] propose a structured
post-training pruning framework for Transformers that fea-
tures Fisher-based mask search, rearrangement, and tuning,
achieving pruning on a single GPU in three minutes without
retraining. SparseGPT [17], a groundbreaking unstructured
post-training pruning method for LLMs, tackles the pruning
problem as an approximate sparsity reconstruction problem
and prunes LLMs at least 50% sparsity with minor accuracy

--- PAGE 9 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 9
loss without retraining. To address SparseGPT’s reconstruc-
tion cost, Wanda [49] uses weight magnitudes and input
norms to facilitate unstructured post-training pruning on
LLMs without updating weights. Unlike SparseGPT and
Wanda, SliceGPT [19] implements structured post-training
pruning using orthogonal matrix transformations and prin-
cipal component analysis ( PCA ) to remove columns and
rows of the weight matrices in LLMs. FLAP [146] introduces
a fluctuation metric and uses a bias compensation mecha-
nism for LLMs’ performance recovery.
4.4 Run-time Pruning
The prior works on pruning usually focus on static pruning
methods where the pruned model is reused for different
inputs. In contrast, some methods prune neural networks
according to individual inputs dynamically, known as run-
time pruning [147]. This line of work is based on the premise
that for a given task, the difficulty of producing accurate
output can vary, implying that necessary model capacities
for different inputs are different [64]. For example, Rao
et al. [147] propose a Runtime Network Routing ( RNR )
framework to conduct dynamic routing based on the input
image and current feature maps and select an optimal path
subset for compression. Tang et al. [64] point out that the
importance of channels highly depends on the input data
and propose to generate different subnetworks for each
instance. At inference, only channels with saliencies larger
than the threshold need to be computed, and the redundant
features are skipped. Hua et al. [148] exploit input-specific
characteristics and propose CGNets to predict unimportant
regions by the partial sum of the output activation by
performing convolution on a subset of input channels. Gao
et al. [149] propose Feature Boosting and Suppression ( FBS)
to predict the saliency of channels and skip those with
less contribution to the classification results at run-time.
Elkerdawy et al. [150] pose dynamic model pruning as a
self-supervised binary classification problem. Meng et al.
[151] propose Contrastive Dual Gating ( CDG ), another self-
supervised dynamic pruning method that uses contrastive
learning [152]. Tuli and Jha [153] introduce DynaTran, which
prunes activations at runtime based on the magnitude of the
input matrix to enhance transformer inference throughput.
5 P RUNING CRITERIA
In this section, we summarize some commonly used prun-
ing criteria for evaluating the importance of weights (or
filters, neurons, etc.) from different perspectives, including
magnitude ([12, 154, 155, 156]), norm ([63, 83]), saliency
and/or sensitivity ([82, 157]), and loss change ([15, 62, 67,
157, 158]). There is no rigid boundary between these criteria,
but a different emphasis.
5.1 Magnitude-based Pruning
[31] is one of the earliest works that propose magnitude-
based pruning to reduce hidden units. Han et al. [13] popu-
larize magnitude-based pruning for deep neural networks,
which prunes the lowest-magnitude weights. It is based on
the assumption that weights with smaller absolute valuestend to have the least influence on the network’s output
[63]. The formulation is defined as
mi=(
1 :if∥wi∥1≥a
0 :if∥wi∥1< a, (3)
where ais a threshold. Some criteria combine weight mag-
nitude with activation magnitude. For example, the score
for a weight wijin Wanda [49] is defined by:
sij=|wij| · ∥xj∥2. (4)
In addition to weight and activation, magnitude-based
pruning can be applied to other values ([132, 159]). For ex-
ample, Dery et al. [132] propose to prune LLMs by selecting
modules with the highest magnitude of module relevance
until the pruning constraint is met.
Magnitude-based criteria can be applied to either un-
structured ([48, 49, 154]) or structured pruning ([63, 94, 132]).
For example, Li et al. [63] score the filters by calculating
the sum of the absolute magnitude of their weights. In
addition, magnitude-based criteria can be combined with
global/local and one-shot/iterative schedules. For example,
the works in [160] and [161] propose magnitude-based
iterative global pruning methods. Lubana and Dick [155]
argue that magnitude-based pruning results in faster model
convergence than magnitude-agnostic methods.
5.2 lpNorm
Some methods ([49, 63, 83]) use the lpnorm to evaluate
the importance of weights (or filters, neurons, etc.). For
example, He et al. [83] exploit the lpnorm to evaluate the
importance of the filter Fi,j, as shown in Eq.(5).
∥Fi,j∥p=
c(i)
inX
n=1k(i)X
k1=1k(i)X
k2=1|Fi,j(n, k 1, k2)|p
1
p
, (5)
where k(i)is the kernel size of layer iin a network and c(i)
in
is the number of channels at layer i. Weights with smaller lp
norms are more likely to be pruned than those with higher lp
norms. In addition, the importance value is often optimized
with norm-based sparsity regularization ([105]), which is
discussed in 6.1.
5.3 Sensitivity and/or Saliency
Some works ([82, 162, 163]) utilize sensitivity and/or
saliency to evaluate the importance of weights (or filters,
neurons, etc.). For example, LeCun et al. [162] define weight
saliency as the loss change induced by pruning that weight.
Lee et al. [54] propose a saliency criterion called the connec-
tion sensitivity criterion as the normalized magnitude of the
derivatives gj:
sj(W;D) =|gj(W;D)|
PM
k=1|gk(W;D)|, (6)
where sjis the sensitivity of wj,Wrepresents the network’s
weights, gjis the derivative of the loss L(W⊙M)w.r.t.
the mask mj. The higher the sensitivity, the more important
the weight is. Zhao et al. [82] reformulate the BN layer by

--- PAGE 10 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 10
extending the scale factor γon the shift term β, which is
treated as channel saliency. They reformulate BN as follows:
xout=γ·BN(x) +˜β, (7)
where ˜β=γ·β. Rather than relying on the value of γ,
unimportant channels are pruned based on γ’s distributions.
5.4 Loss Change
It is a widely used criterion to measure the importance
of a weight (or filter, neuron, etc.) by evaluating the loss
change of the network with and without it. The loss change
is usually approximated in a Taylor expansion-based way,
such as [15, 20, 39, 62, 67, 164].
The first-order Taylor expansion is the most commonly
used method for measuring the loss change. The loss change
with a small perturbation at Wis defined as follows:
∆L=L(W+ ∆W)− L(W) =∇WL∆W. (8)
For example, You et al. [15] introduce scaling factors λ
to the BN and exploits the first-order Taylor expansion to
estimate the loss change ∆Lcaused by setting some scaling
factors to zero as follows:
∆L(λ) =|λ∇λL −R1(λ)| ≈ |λ∇λL|=∂L
∂λλ,(9)
where R1(λ)is the Lagrange remainder. The importance
score of the i-th filter is defined as
Score (Fi) =X
(x,y)∈D∂L(y, f(x;W)
∂λiλi, (10)
where λiis the scalar factor of the i-th filter.
The second-order Taylor expansion of the loss func-
tion is early used in [162, 165] for removing unimportant
weights and gradually exploited in many subsequent meth-
ods ([62, 67, 164, 166, 167]), which includes the first-order
(gradient) term, the second-order (Hessian) term, and the
higher-order terms are neglected. Without loss of generality,
the approximation of the loss change leads to
L(W+ ∆W)− L(W) =∇WL∆W+1
2∆WTH∆W,(11)
where H=∇2
WL(W).
For example, Liu et al. [62] apply the second-order Taylor
expansion to approximate the loss change when removing a
channel (setting its mask to 0):
si= ∆L=L(M−ei)− L(M)≈ − eT
i∇ML+1
2eT
i(∇2
ML)ei
=−eT
ig+1
2eT
iHei=−gi+1
2Hii,
(12)
where eiis the one-hot vector with the i-th entry equals one,
andgis the gradient of the loss function Lw.r.t. M.
6 L EARN TO PRUNE
In this section, we present some methods for learning
to prune networks, including sparsity regularization ([57,
59, 134, 168, 169]) and pruning methods based on meta-
learning ([84, 168]), graph neural networks ([170]), and
reinforcement-learning ([147, 171]).6.1 Sparsity Regularization based Pruning
Sparsity regularization based pruning [132, 134] learns the
weights and their masks by solving the following problem:
min
W,ML(W, M ), (13)
where L=ℓ(W, M ) +λR(·). One common way is to
introduce a scaling factor vector γfor weights (or channels,
filters, etc.). The network weights and the scaling factors γ
are trained jointly with sparsity regularization imposed on
the latter. The magnitude of the scaling factors is treated
as the important scores. Specifically, Lin Eq. 13 can be
exemplified as follows:
L=1
NNX
i=1ℓ(yi, f(xi;W,γ)) +λX
γi∈γR(γi). (14)
For example, He et al. [134] cast channel selection as the
minimization of reconstruction error in feature maps and
formulate the channel pruning problem as follows:
min
β,W1
2N∥y−cX
i=1βixciwT
ci∥2
F+λ∥β∥1,
subject to ∥β∥0≤c′,∀i∥wci∥F= 1.(15)
where ∥·∥Fis the Frobenius norm, xciis anN×khkwmatrix
from i-th channel of input x,wciis an n×khkwweight
matrix from i-th channel of W,khandkware the kernel
height and width, respectively. N,c,c′, and nrepresent
the number of samples, channels, retained channels, and
output channels. To solve this problem, He et al. [134] use
LASSO regression [138] and a greedy strategy to select the
unimportant channels.
6.2 Meta-Learning based Pruning
Some works ([84, 168]) adopt meta-learning to prune mod-
els. For example, Liu et al. [84] train a meta network,
PruningNet, to predict weights for different pruned net-
works. The PruningNet takes a network encoding vector
(v1, v2, ..., v L)as input and outputs the weights Wof the
pruned network:
W=PruningNet (v1, v2, ..., v L), (16)
where viis the number of the channels for the i-th layer. The
weights and the corresponding accuracy of each pruned net-
work are obtained by inputting the network encoding into
the fully trained PruningNet. Considering the huge search
space of network encoding vectors, the pruned network is
found by evolutionary search under the constraints.
6.3 Graph Neural Network based Pruning
Any network can be viewed as a graph. Zhang et al. [170]
propose a method called GraphPruning for model compres-
sion. Specifically, GraphPruning designs a graph aggregator
Gwith weights θG, combined with the Fully Connected (FC)
layers, to generate the weights W= (w(1), w(2), ..., w(L))of
the “Pruned Network” as follows:
(n1, n2, ..., n L) =G(b1, b2, ..., b L|θG),
w(i)=FCi(ni|θi),(17)

--- PAGE 11 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 11
where bi∈R1×7denotes the embedding features of the
i-th node, niis the i-th column of the output with the
graph aggregator, θiare the weights of the ith FC layer, and
w(i)are the weights of the i-th pruned layer of the pruned
network. Then, the “Pruned Network” is fully trained. The
graph aggregator is responsible for extracting high-level
features for each node, while each FC layer is used to
generate reasonable weights for the “Pruned Network”.
Afterward, the best configuration of the “Pruned Network”
under computational constraints is searched by RL methods,
during which the weights of the graph aggregator and FCs
are not updated.
6.4 Reinforcement Learning based Pruning
Rather than using RL to search for the best configurations
of the pruned networks as in [170], some AutoML pruning
methods ([147, 171]) adopt RL to compress models auto-
matically. For example, He et al. [171] propose AutoML for
Model Compression ( AMC ), which is based on Q-learning,
a type of RL that focuses on how an agent should take
actions to maximize the cumulative reward. Specifically, He
et al. [171] design the Deep Deterministic Policy Gradient
(DDPG ) agent to receive an embedding state siof layer li
from the environment and output a sparsity ratio as action
ai. Then, layer liis compressed with aiusing a specific
compression method (such as a channel pruning method).
After that, the agent moves to layer li+1and repeats the
same process until the final layer lL. The update process is
as follows:
Loss =1
NNX
i=1(yi−Q(si, ai|WQ))2,
yi=ri−b+γQ(si+1, µ(si+1)|WQ),(18)
where bis the baseline reward, γis a discount factor used
to avoid over-prioritizing short-term rewards, WQare the
weights of the network Qfollowing Block-QNN [172], and
riis the reward of the whole trajectory for the i-th sample.
He et al. [171] observe that Error is inversely-
proportional to log(FLOPs )orlog(#Param ). Based on this
observation, the reward function is defined as:
RFLOPs =−Error·log(FLOPs ),
RParam =−Error·log(#Param ).(19)
This reward function provides an incentive for reducing
FLOPs or the number of network parameters.
7 A C OMPREHENSIVE COMPARATIVE ANALYSIS
In this section, we compare some pruning methods on
commonly used models, including eight pairs of contrast
settings for pruning, different layer-wise densities, and var-
ious supervision levels for pruning. To avoid the influence
of specific functions on pruning results, we mainly use the
same functions under contrast settings (experimental details
in Appendix B). Appendix C provides a more extensive
comparison across different methods.TABLE 5: Top-1 accuracy (%) of unstructured and struc-
tured pruning on VGG-16. “Ratio” refers to the per-
centage of parameters removed from the original count.
Bold /Underline marks the best/second best performance,
respectively, among the compared entities. Unless otherwise
specified, “Ratio” and Bold /Underline in other tables have
the same meaning.
Dataset CIFAR-10 CIFAR-100
Method \Ratio (%) 10.00 40.00 10.00 50.00
SNIP-unstructured [54] 93.84 93.73 73.09 72.67
SNIP-structured 93.74 93.71 72.82 70.68
GraSP-unstructured [56] 93.58 93.18 72.46 71.42
GraSP-structured 93.67 93.04 72.43 71.30
TABLE 6: Perplexity of unstructured, semi-structured, and
structured pruning on LLMs with WikiText2 [174], where
lower is better. “O” denotes OPT [175].
MethodRatio Model
(%) O-125M O-1.3B O-2.7B O-6.7B
Unpruned 0 27.65 14.62 12.47 10.86
SparseGPT-unstruct [17]50.0033.17 26.77 12.88 11.92
SparseGPT-2:4 [17] 45.51 29.44 14.92 13.01
K-OBD-2:4 [164]50.0068.74 27.22 20.23 15.55
K-OBD-struct [164] 75.95 37.68 26.88 25.54
LLM-Surgeon-unstruct [164]
50.0030.30 15.47 12.68 10.97
LLM-Surgeon-2:4 [164] 44.64 25.10 14.64 12.10
LLM-Surgeon-struct [164] 49.78 22.95 17.15 14.90
7.1 Unstructured vs. Structured Pruning
Unstructured pruning methods ([13, 17]) remove weights
anywhere and can achieve high prune ratios with little
impact on accuracy. In contrast, structured pruning ([20, 62,
67]) conducts pruning on entire filters (or channels, neu-
rons, layers, etc.), resulting in really compressed network
and accelerated inference. However, the accuracy is often
lower than that of unstructured pruning under the same
prune ratio, weight-level scoring, pipeline, and learning
schemes. The possible reason is that unstructured prun-
ing only focuses on the importance of individual weights,
while structured pruning forces structural coupling, which
demands simultaneous pruning across multiple layers and
expects all removed weights to be consistently unimportant.
However, achieving consistency in identifying unimportant
weights under the structural coupling constraints is chal-
lenging. Amersfoort et al. [173] argue that SNIP-structured
and GraSP-structured methods incur more noise than their
vanilla unstructured counterparts.
We compare unstructured and structured pruning meth-
ods on VGG-16 [1] and report the best results in Table 5 from
three random runs. Additionally, we compare unstructured,
semi-structured, and structured pruning methods on OPTs
[175] with data sourced from [164] (Table 6). As shown in
Table 5 and Table 6, at the same prune ratio, unstructured
pruning generally outperforms semi-structured (if any),
which performs better than structured pruning.
7.2 One-shot vs. Iterative Pruning
One-shot pruning methods score once and then prune
the network to a target prune ratio. Conversely, the itera-
tive pruning methods alternately process the score-prune-
update cycle until achieving the target prune ratio. As a
result, the pruning cost in one-shot methods is usually neg-
ligible, greatly saving pruning efforts. However, these meth-
ods are not beneficial to those significant weights whose

--- PAGE 12 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 12
10203040506070809095
Prune Ratio of Parameters (%)92.893.093.293.493.693.894.0 T op-1 Accuracy (%) SynFlow (one-shot)
SynFlow (iterative)
(a) VGG-16 on CIFAR-10
10203040506070809095
Prune Ratio of Parameters (%)91.091.592.092.593.093.594.0 T op-1 Accuracy (%) Magnitude (one-shot)
Magnitude (iterative) (b) ResNet-32 on CIFAR-10
80 90 95
Prune Ratio of Parameters (%)20004000600080001000012000 Perplexity
SparseGPT (one-shot)
SparseGPT (iterative)
(c) LLaMA-7B on PTB
10 20 30 40 50
Prune Ratio of Parameters (%)16182022Perplexity
10 2014.815.0Zoom
30 401718Zoom
101520253035404550
Number of Iterations (d) OPT-1.3B on WikiText2
Fig. 7: One-shot vs. iterative pruning. Shaded regions indi-
cate standard deviation based on three independent runs.
Best view in color and zoom in.
importance is not immediately apparent at the beginning
[120]. Therefore, one-shot pruning generally requires more
carefully designed scoring criteria to match the performance
of the original network. In addition, the results in [50] show
that one-shot pruning may more easily suffer from layer
collapse, resulting in a sharp accuracy drop. In contrast,
iterative methods require more pruning cost but generally
yield better accuracy [15, 63, 121, 128, 176].
Lin et al. [96] analyze the difference between one-shot
and iterative pruning methods from the perspective of
stochastic gradient. Their results show that the iterative
pruning method computes a stochastic gradient at the
pruned model and takes a step that best suits the com-
pressed model. In contrast, one-shot pruning computes a
stochastic gradient at the original weights and moves to-
wards the best dense model. The work in [164] theoretically
supports iterative pruning by arguing that the surrogate loss
landscape, based on a Taylor expansion, only holds locally,
making it unreliable for larger weight changes.
We prune VGG-16 [1] and ResNet-32 [9] on CIFAR-10
[177] using SynFlow [50] and Magnitude-based pruning,
and LLaMA-7B [178] on PTB [179] using SparseGPT [17],
applying one-shot and iterative pruning, respectively. Addi-
tionally, we compare the pruning results of OPT-1.3B [175]
on WikiText2 [174] under different iteration settings, using
experimental results from [164]. As illustrated in Fig. 7 (a) -
(c), iterative pruning generally performs better than that of
the corresponding one-shot pruning and Fig. 7 (d) indicates
that more iterations tend to yield better performance.
7.3 Data-free vs. Data-driven Pruning
Pruning methods can be categorized into two types based
on whether data is used during the pruning phase: data-free
and data-driven. Data is generally believed to be essential
for finding good subnetworks. Most existing pruning works
([20, 54, 62, 67, 130]) belong to data-driven methods, and
only a few methods ([50, 55, 180]) are data-free. We apply
PBT methods, including three data-free (Random, Magni-
tude, and SynFlow [50]) and two data-driven (SNIP [54]TABLE 7: Top-1 accuracy (%) of data-free (first three) and
data-driven (last two) pruning methods. Original Top-1 ac-
curacy on CIFAR-10, CIFAR-100, and ImageNet are 93.76%,
73.18%, and 76.20%, respectively. An asterisk (*) denotes a
result from our reproduction; others are from [76].
Model VGG-16* ResNet-32* ResNet-50
Dataset CIFAR-10 CIFAR-100 ImageNet
Method \Ratio (%) 50.00 90.00 50.00 90.00 73.80 89.30
Random 93.12 90.68 72.10 67.29 71.20 65.20
Magnitude 93.32 92.78 71.86 68.77 72.50 66.50
SynFlow [50] 93.74 93.26 72.15 68.44 72.60 68.00
SNIP [54] 93.70 93.40 70.87 68.00 72.70 66.60
GraSP [56] 92.97 92.51 72.13 68.56 72.10 67.20
TABLE 8: Zero-shot accuracy (%) of data-free (first three)
and data-driven (last three) pruning methods on LLaMA-
7B [178] with common sense datasets. “Avg.” is calculated
among four datasets. An asterisk (*) denotes a result ob-
tained from our reproduction; others are from [182].
Ratio (%) Method BoolQ PIQA HellaSwag WinoGrande Avg. ↑
0 Unpruned 73.18 78.35 72.99 67.01 72.88
20.00 Random* 55.57 73.39 64.49 60.38 63.46
w/ tune L1-norm* 58.47 75.35 65.40 60.93 65.04
L2-norm* 65.02 75.14 65.07 62.12 66.84
Wanda [49] 65.75 74.70 64.52 59.35 66.08
LLM-Pruner [20] 64.62 77.20 68.80 63.14 68.44
LoRAPruner [182] 65.62 79.31 70.00 62.76 69.42
50.00 Random* 61.04 62.30 40.37 53.51 54.31
w/ tune L1-norm* 40.73 66.32 42.66 51.85 50.39
L2-norm* 38.50 67.08 43.47 52.80 50.46
Wanda [49] 50.90 57.38 38.12 55.98 50.60
LLM-Pruner [20] 60.28 69.31 47.06 53.43 57.52
LoRAPruner [182] 61.88 71.53 47.86 55.01 59.07
and GraSP [56]) methods, to prune VGG-16 and ResNet-
32/50 on CIFAR-10/100 and ImageNet [181], respectively.
Table 7 shows that SynFlow and SNIP are similarly effective,
with SynFlow significantly outperforming GraSP , indicating
that the effectiveness of PBT methods is not strictly depen-
dent on data usage. In addition, we utilize PAT methods,
selecting Random and L1/L2-norm as data-free methods,
and data-driven approaches such as Wanda [49], LLM-
Pruner [20], and LoRAPruner [182] to prune LLaMA-7B. In
contrast, Table 8 consistently demonstrates that data-driven
PAT methods typically outperform data-free PAT methods.
7.4 Pruning on Initialized vs. Pre-trained Weights
Frankle et al. [76] find the subnetworks in CNNs obtained
by pruning on randomly initialized weights (such as SNIP
[54], GraSP [56], SynFlow [50]) are robust to ablation treat-
ments (i.e., randomly shuffling the mask positions within
each layer or reinitializing weights while keeping masks un-
changed). To find the reason behind such immunity, Singh
and Liu [183] use the Wasserstein distance to measure dis-
tributional similarity and find that the remaining weights’
distribution changes minimally with these ablations, which
helps maintain similar performances. In contrast, Su et al.
[55] find the subnetworks in CNNs achieved by pruning on
pre-trained weights, such as LTH [48], are sensitive to these
ablations. Qiu and Suda [184] claim that training weights
can be decoupled into two dimensions: the locations of
weights and their exact values, with the locations of weights
holding most of the information encoded by the training.
Wolfe et al. [185] theoretically analyze the impact of pre-
training on the performance of a pruned subnetwork in
CNNs obtained by greedy forward selection and find that
the number of pre-training iterations increases logarithmi-

--- PAGE 13 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 13
cally with the dataset size. Unlike CNNs, Transformers
typically need appropriate self-supervised pre-training to
perform well [186]. Therefore, pruning of Transformers gen-
erally targets pre-trained models. We conduct experiments
to explore whether pre-trained weights facilitate achieving
better subnetworks. Fig. 8 (a) indicates that for the PBT
method, GraSP [56], pruning with pre-trained weights does
not necessarily improve Top-1 accuracy. However, Fig. 8
(b) shows that for the PAT method, WDPruning [159], pre-
training may be crucial for obtaining subnetworks with
better performance.
7.5 Global vs. Local Pruning
The difference between global ([62, 164, 171, 187, 188]) and
local ([16, 20, 53, 105, 108]) pruning lies in whether struc-
tures are removed from a subset or all available structures of
a network. A major limitation of local pruning is that setting
a pre-defined prune ratio for each layer can be complex and
lead to sub-optimal sparsity. To simplify, local pruning often
uses a consistent prune ratio across layers. In contrast, global
pruning automatically generates a varying prune ratio for
each layer. However, global pruning poses great challenges,
particularly for LLMs, due to significant variations in layer
magnitudes. For instance, some outlier features may have
magnitudes up to 20 times larger than others [189], leading
to incomparability issues. Ma et al. [20] notes a marginal
advantage of local pruning compared to global pruning
in LLMs. Although previous pruning methods for LLMs
([17, 20, 49, 182]) are primarily local, some global methods
([146, 164, 190]) start to emerge. For example, Bai et al. [190]
argue that local pruning excessively restricts the alignment
of input and output in all the intermediate layers, leading
to a sub-optimal solution, and propose a global pruning
method called SparseLLM to address the drawbacks.
7.6 Training from Scratch vs. Fine-tuning
After pruning, many pruning methods require training the
subnetwork for several epochs to regain performance. Le
and Hua [191] argue that retraining is essential to recover
loss accuracy in pruning. Generally, retraining can be di-
vided into two types: training from scratch or fine-tuning.
There has been debate over whether fine-tuning is more
effective than training from scratch in recovering accuracy.
On the one hand, Liu et al. [110] find that for ResNet, VGG,
and other standard structures on ImageNet, training the
subnetworks with new random initialization can achieve
better performance than fine-tuning them. On the other
hand, Li et al. [63] observe that training a subnetwork from
scratch performs worse than fine-tuning it. Liu et al. [128]
investigate pruning ResNet20 on CIFAR-10 with ADMM-
based [192] one-shot pruning method and find that pruning
& fine-tuning outperforms LTH (pruning & training from
scratch) over various prune ratios. Additionally, the results
in [140, 193] show that fine-tuning is necessary for better
performance on sparse mobile networks than training from
scratch. The results in [39] reveal that training from scratch
requires more steps to achieve convergence, suggesting that
starting pruned models from scratch may not be the most
cost-effective strategy, given its training cost is comparable
10 20 30 40 50 60 70 80 90
Prune Ratio of Parameters (%)73.574.074.575.075.576.076.5 T op-1 Accuracy (%) GraSP (initialize)
GraSP (pre-train)(a) ResNet-152 on CIFAR-100
10 20 30 40 50 60 70 80 90
Prune Ratio of Parameters (%)92939495969798 T op-1 Accuracy (%) WDPruning (initialize)
WDPruning (pre-train) (b) DeiT-Tiny on CIFAR-10
Fig. 8: Pruning using randomly initialized vs. pre-trained
weights. Shaded regions indicate standard deviation based
on three independent runs (best view in color).
10 20 30 40 50 60 70 80 90
Prune Ratio of Parameters (%)7374757677 T op-1 Accuracy (%) GraSP (train from scratch)
GraSP  (fine-tune)
(a) ResNet-152 on CIFAR-100
20 30 40 50
Prune Ratio of Parameters (%)52.555.057.560.062.565.067.570.0 T op-1 Accuracy (%) WDPruning (fine-tune)
WDPruning (train from scratch) (b) DeiT-Tiny on ImageNet
Fig. 9: Training from scratch vs. fine-tuning. Shaded regions
indicate standard deviation based on three independent
runs (best view in color).
to that of pre-trained models. In recent years, some compro-
mise methods (such as weight rewinding [144]) have been
proposed. The results in [111, 144] show that weight rewind-
ing can achieve higher accuracy than fine-tuning. We prune
ResNet-152 and DeiT-Tiny [194] on CIFAR-100 and Ima-
geNet with pre-trained weights, respectively. Then we fine-
tune the pruned networks or train them from scratch. Fig. 9
indicates that fine-tuning generally outperforms training
from scratch. Notably, on ImageNet, fine-tuning achieves
significantly higher accuracy than training from scratch.
7.7 Original Task vs. Transfer Pruning
In recent literature, pruning is combined with transfer learn-
ing [195] that can dramatically improve accuracy and speed
up convergence. For ease of distinction, in this survey, orig-
inal task pruning denotes the pruning pipeline that directly
performs on the target task. In contrast, transfer pruning
is performed on the source task and then transfers the
subnetwork to the target task. Specifically, transfer pruning
is divided into two types: dataset transfer and architecture
transfer. The former prunes networks on the source dataset
and transfers the subnetwork to the target dataset, while the
latter prunes on one architecture and transfers the subnet-
work to another.
Some works ([121, 188, 196, 197]) study the transfer-
ability of sparsity masks on datasets. S. Morcos et al. [120]
observe that for image classification, winning tickets gener-
ated on larger datasets (such as with larger training set size
and/or more classes) consistently transfer better than those
generated with smaller datasets. For example, winning
tickets generated on ImageNet and Places365 demonstrate
better performance across other smaller target datasets such
as CIFAR-10 and CIFAR-100. Iofinova et al. [198] present
a pioneering study of the transfer performance of subnet-
works and find that pruning methods with similar Top-1

--- PAGE 14 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 14
accuracy on ImageNet [181] can have surprisingly different
Top-1 accuracy when used for transfer learning. For pruning
on LLMs, performing pruning directly on the target task
typically yields better performance than zero-shot pruning
[164]. For architecture transfer pruning, Elastic Ticket Trans-
formations ( ETTs ) [199] transforms winning tickets in one
kind of network (such as ResNet [9]) to another deeper or
shallower one from the same model family.
7.8 Static vs. Dynamic Pruning
Static pruning [14] uses static pruning criteria and removes
components. In contrast, dynamic pruning ([64, 148, 149,
200]) exploits input-specific pruning criteria, preserves the
entire network structures, and accelerates the networks by
dynamically skipping unimportant components. However,
dynamic pruning generally does not perform run-time fine-
tuning or retraining. The difference between static and
dynamic pruning is mainly reflected in the pruning criteria
and the pruned model. The advantages and disadvantages
of static and dynamic pruning are shown in Table 9.
7.9 Layer-wise Weight Density Analysis
Some works ([62, 127, 170, 201]) study the distributions of
layer-wise weight density in a subnetwork, showing that
different layers can have very different weight densities. The
differences arise from the joint action of networks’ structural
characteristics and pruning methods. Zhang et al. [202] em-
pirically divide the layers into either “ambient” or “critical”.
Ambient layers are not sensitive to weight changes, while
critical layers are. Thus, ambient layers should be heavily
pruned, resulting in lower weight densities.
Pruning methods can also result in different weight
densities. Gong et al. [203] categorize the sparsity alloca-
tion formed by pruning methods into uniform and non-
uniform. Uniform sparsity methods ([17, 20, 49]) allocate
the same prune ratio for all the layers, whereas non-uniform
methods ([146, 164, 190, 204]) assign varying sparsity rates
to different layers. For example, in CNNs, some pruning
methods tend to assign more weights to earlier layers than
to later ones. Ma et al. [127] investigate the layer-wise keep-
ratios of subnetworks obtained by GraSP [56], SNIP [54],
and LTH [48] on VGG and ResNet, observing a common
trend of declining layer-wise keep-ratios, except for some
special layers (such as the downsampling layers in ResNet).
In contrast, Liu et al. [62] find their pruned networks
keep higher percentages of channels in the deeper layers
than those in the lower layers for image classification. For
pruning Transformers, the results in [201] show that global
magnitude pruning tends to prune Transformer layers uni-
formly, while global first-order methods heavily prune the
deeper layers. Yang et al. [60] discover a unique less-more-
less distribution among stacked ViT blocks. LLM surgeon
[164] prunes relatively more in the first layers and less in the
middle layers. Some works ([65, 133, 205, 206]) reveal that
some layers are not essential and can be entirely removed
or merged.
7.10 Pruning with Different Levels of Supervision
In descending order of supervision level during neural
network pruning, pruning can be divided into supervised,TABLE 9: Advantages and disadvantages of static and dy-
namic pruning.
Type Advantages Disadvantages
Static Pruning model size reduced fixed subnetwork
Dynamic Pruning flexible subnetwork model size unreduced
semi-supervised, self-supervised, and unsupervised prun-
ing [207]. Self-supervised learning can be divided into two
classes: generative and contrastive learning [208]. Similar
to supervised learning, supervised pruning works on fully
labeled datasets, and most current pruning methods fall
into this category. However, supervised pruning suffers
from similar bottlenecks as supervised learning, such as
the expensive manual labeling. As a promising alternative,
semi-supervised, self-supervised, and unsupervised prun-
ing have drawn massive attention.4
For example, Caron et al. [209] observe different results
in self-supervision pruning compared to supervision prun-
ing, where winning tickets initialization only introduces
a slight performance improvement compared to random
re-initialization. Pan et al. [210] claim that unsupervised
pruning usually fails to preserve the accuracy of the original
model. Notably, label supervision for network pruning and
training can be independent. For example, Chen et al. [197]
use supervised pruning method IMP (Iterative Magnitude
Pruning) to explore the subnetworks of self-supervised pre-
trained models (simCLR [211] and MoCo [212]) on Ima-
geNet. Similarly, Jeff Lai et al. [213] exploit the supervised
pruning method IMP to prune self-supervised speech recog-
nition models.
8 F USION OF PRUNING AND OTHER COMPRES -
SION TECHNIQUES
In this section, we review the fusion of neural network
pruning with other network compression techniques, such
as quantization [20], tensor decomposition [158], knowledge
distillation [214], and network architecture search [68]. On
the one hand, fusion provides more choices for network
compression. On the other hand, combined compression
techniques can complement each other to further improve
the performance and prune ratio.
Pruning & Quantization: Quantization [215] is a com-
pression technique that reduces the number of bits used to
represent the network weights and/or activations, signifi-
cantly reducing the model size and memory footprint with
only a minor performance drop. To obtain more compact
models and achieve model acceleration, Han et al. [13] pio-
neer pruning the redundant network connections and quan-
tizing the weights. CLIP-Q [215] jointly performs pruning
and quantization during the fine-tuning stage. MPTs [113]
integrates pruning and quantizing randomly weighted full-
precision neural networks to obtain binary weights and/or
activations. EB [141] applies 8-bit low-precision training to
the stage of searching EB tickets.
Pruning & Tensor Decomposition: Tensor decompo-
sition [22] decomposes convolutions into a sequence of
4. The differences between supervised, semi-supervised, unsuper-
vised, self-supervised learning refer to [208].

--- PAGE 15 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 15
tensors with fewer parameters. In contrast to pruning, it ex-
plores the original weights’ low-rank structure, while keep-
ing the dimension of the convolutional output unchanged.
CC [158] combines channel pruning and tensor decomposi-
tion to compress CNN models by simultaneously learning
model sparsity and low rankness. Hinge [216] introduces
group sparsity to fuse filter pruning and decomposition un-
der the same formulation. Li et al. [217] propose LoSparse to
prune Transformers by combining low-rank approximations
and pruning.
Pruning & NAS: Neural Architecture Search ( NAS ) pro-
vides a mechanism to automatically discover the best archi-
tecture for the problem of interest, offering a new approach
for pruning to find suitable network depth and width. For
example, for CNNs, NPAS [68] performs a compiler-aware
joint network pruning and NAS, determining the filter type
(different kernel sizes), the pruning scheme, and the rate for
each layer. TAS [218] exploits NAS to search for the depth
and width of a network to obtain pruned networks and
uses knowledge distillation to train these pruned networks.
Klein et al. [219] explore structured pruning of fine-tuned
Transformers via NAS.
Pruning & Knowledge Distillation: Knowledge Distil-
lation ( KD) [214] guides the student to effectively inherit
knowledge from the teacher and mimic the teacher’s output.
Some works ([220, 221]) exploit pruning before KD to boost
KD’s quality. For example, Liu et al. [220] prune unimpor-
tant channels to the contents of interest and focus the distil-
lation on the interest regions. Park and No [221] prune the
teacher network first to make it more transferable and then
distill it to the student. Some works ([148, 222, 223, 224])
use KD to train the pruned networks. The results in [222]
show that the pruned network recovered by KD performs
better than it regained by fine-tuning. Zou et al. [225] pro-
pose a data-free deraining model compression method that
distills the pruned model to fit the pre-trained model. [226]
introduce a multi-stage compression strategy, AntGMM, to
compress large multimodal models by utilizing structured
pruning and knowledge distillation.
Pruning & Multi-compression Techniques: Some works
([227, 228, 229]) explore the fusion of pruning with more
than one compression technique. For example, GS [229]
combines pruning, quantization, and KD for GANs com-
pression. Joint-DetNAS [228] integrates pruning, NAS, and
KD for image translation. LadaBERT [227] merges pruning,
matrix factorization, and KD to compress BERTs [143] for
natural language understanding.
9 S UGGESTIONS AND FUTURE DIRECTIONS
In this section, we discuss how to choose different pruning
methods and provide promising directions for future work.
9.1 Recommendations on pruning method selection
After years of research and exploration, there are many off-
the-shelf pruning methods. However, no golden standard
exists to determine which one is the best. Different suitable
pruning methods exist to compact deep neural networks for
specific application requirements and hardware/software
resources. Here are some general recommendations for
choosing an appropriate pruning method.(1) If you do not have special hardware (e.g., FPGAs or
ASICs) or software (such as sparsity convolutional libraries)
but need actual neural network acceleration and compres-
sion, structured pruning is more suitable than unstructured
pruning because most software frameworks and hardware
cannot accelerate sparse matrices’ computation.
(2) If you have sufficient computational resources during
the pruning stage, consider using iterative PAT methods
that can typically minimize the impact on performance
under the same prune ratio. On the other hand, if you have
limited computational resources during both the pruning
and inference stages, consider using one-shot PBT or one-
shot post-training pruning methods, particularly for LLMs.
(3) If you have enough labeled examples on the target
task, consider using supervised pruning methods. However,
if only a few examples on the target task are labeled, semi-
supervised or transfer pruning methods may be considered.
If the examples on the target task are not labeled, consider
self-supervised, unsupervised, or transfer pruning methods.
(4) If you have a sufficient memory footprint during
pruning for NLP tasks, consider heavily compressing large
models rather than lightly compressing smaller models to
meet the same budgets. Some results in [230] show that for
NLP tasks finding pruned models derived from larger dense
networks outperform small dense networks of comparable
size to pruned models.
(5) If you have enough memory to store the dense neural
network during the inference stage and wish to provide
run-time flexible computational cost allocation for different
inputs, dynamic pruning methods can be considered where
inputs with smaller shapes can allocate less computational
cost to perform the task and if the input shape is bigger
more computational cost can be allocated.
(6) If you need to trim down neural networks in multiple
dimensions, you can comprehensively consider layerwise
pruning (decreasing the model’s depth), channel pruning
(reducing the model’s width), and image resolution pruning
(scaling down the model’s input resolution) or token prun-
ing (selectively removing tokens from text data). In addition,
pruning can be integrated with quantization to further
reduce the memory footprint and the neural networks’ size.
(7) If you want to achieve a better tradeoff between
speed and accuracy, the following settings may help: use a
pre-trained model; set an appropriate learning rate (if any)
for both the pruning and retraining stages; fine-tune the
pruned models for several epochs; integrate pruning with
knowledge distillation, NAS, or other compression methods
to achieve complementarity; and adversarial training may
have some help [122].
(8) If you need to train a subnetwork to recover perfor-
mance, Ma et al. [127] show that subnetworks with residual
connections achieve higher accuracy using a relatively small
learning rate. In contrast, subnetworks without residual
connections benefit from a larger learning rate.
(9) To benefit from large-scale pre-training, adding pa-
rameters is more critical than minimizing FLOPs [231].
By incorporating techniques such as dynamic convolution
[232], models with low FLOPs can increase their capacity
without a substantial rise in computational cost, enhancing
their performance during extensive pre-training.

--- PAGE 16 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 16
9.2 Future Directions
We discuss four promising directions for the further devel-
opment of neural network pruning, namely, (1) theories, (2)
techniques, (3) applications, and (4) evaluation.
Theories: Despite the existing works, several funda-
mental questions about pruning still need to be answered.
For example, prior works demonstrate that network layers
contain irreplaceable information as long as redundant ones.
Does a theoretical upper bound of the prune ratio exist
for a given network that still maintains the performance
of its dense equivalent? In other words, how heavily can
a network be pruned theoretically without accuracy loss?
It is a tricky question due to the intricate relationships
between network layers. Besides, is pruning explainable?
A common belief is that deep neural networks are hard
to interpret. As such, making pruning explainable is an
uphill task. However, the interpretability of pruning is vital
for understanding the factors behind pruning (e.g., model
structure and weights) and exploring more effective pruning
methods.
Techniques: To obtain better algorithm designs whose
architectures are learned in an economical, efficient, and
effective manner, it is a trend to extend Automated Ma-
chine Learning ( AutoML ) methods and NAS to pruning.
Furthermore, pruning is also beginning to combine with
various learning contexts, such as lifelong learning [223],
continual learning [233], contrast learning [234], and fed-
erated learning [235], etc. In addition, the rising energy
consumption of networks requires more attention to energy-
aware pruning. However, preliminary efforts mainly focus
on reducing computation and memory costs, which may not
necessarily reduce the most energy consumption. Moreover,
incorporating pruning into hardware to help deploy pruned
networks is also an emerging trend. For example, Sui et al.
[236] propose a hardware-friendly pruning method and
deploy the pruned models on an FPGA platform.
Applications: Pruning has begun to draw attention to
more complex applications such as visual question answer-
ing, natural language understanding, speech recognition,
and content generation than image classification. Foun-
dation models such as GPT-4 [237] might be a possible
way to Artificial General Intelligence ( AGI ). However, its
enormous size hinders its application in many downstream
tasks. Fig. 2 highlights content related to large model prun-
ing. In the future, more pruning methods will enable colos-
sal foundation models to benefit from pruning research,
making them more compact and efficient [17].
Evaluation: With the emergence of many pruning meth-
ods, standardized benchmarks, and metrics are required to
provide a fair evaluation. Different pruning techniques, net-
work architectures, tasks, and experimental settings lead to
incomparable results and make it hard to compare pruning
methods fairly [238]. ShrinkBench [42] takes the first step
and provides a benchmark of pruning methods for image
classification. As pruning is applied to applications beyond
image classification, standardized benchmarks and metrics
for other applications are needed.10 C ONCLUSION
As an essential compression technique, deep neural net-
work pruning has attracted increasing research attention
with the recent emergence of various pruning methods
and applications. This survey conducts a comprehensive
review on the following four scopes: 1) universal/specific
speedup, with a systematic review of unstructured, struc-
tured, and semi-structured pruning; 2) when to prune,
including pruning before/during/after training for static
pruning and run-time pruning; 3) how to prune, including
pruning by criteria and by learning; 4) fusion of pruning
with other compression techniques, such as KD and NAS. A
comprehensive comparative analysis, including eight pairs
of contrast settings for pruning, layer-wise weight density,
and different supervision levels, can help researchers to
efficiently and effectively grasp the characteristics of dif-
ferent pruning methods. In addition, recommendations on
pruning method selection and future research directions are
highlighted and discussed. To facilitate future research, real-
world miscellaneous applications and commonly used re-
sources of datasets, networks, and evaluation metrics in dif-
ferent applications are summarized in Appendix D. To help
researchers and practitioners keep up with the development
of pruning technologies, we continue updating the represen-
tative research efforts and open-source codes for pruning at
https://github.com/hrcheng1066/awesome-pruning.
APPENDIX A
TERMS AND NOTATIONS
This section presents the commonly used terms in pruning
literature. It is worth mentioning that some terms (e.g.,
compression ratio) have different definitions in prior works.
In addition, for better readability, we list the notations used
in the main text in Table 10.
•Prune Ratio : Prune ratio [110] denotes the percent-
age of weights (or filters, neurons, etc.) that are
removed from the dense network and is the com-
plement of Keep Ratio [85]. In general, it can be
determined in two ways: pre-defined or learning-
decided.
•Compression Ratio : Compression ratio in [50, 111] is
defined as the ratio of the original number of weights
to the preserved number of weights, but in [239]
it is defined as the ratio of the preserved number
of weights to the original number of weights. For
example, if 10% of the weights are preserved, the
compression ratio in [111] is 10, but it is 10% in [239].
•Sparsity Ratio : Sparsity ratio denotes the portion of
zero weights (or channels, filters, neurons, etc.) in
networks after pruning [171, 199]. It is equivalent to
compression ratio in [239].
•Speedup Ratio : Speedup ratio is defined as the ratio
of the pruned number of FLOPs in [14], or MACs
in [38] to the original number of FLOPs or MACs,
respectively. In [240], the speedup ratio is calculated
by dividing the pruned number of filters in one layer
by the original number of filters in that layer.
•One-shot Pruning : One-shot pruning, also called
single-shot pruning in [54], scores only once and then
prunes the network to the target prune ratio [48, 76].

--- PAGE 17 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 17
•Iterative Pruning : Iterative pruning [12], also called
greedy pruning or oracle pruning in [241], repeatedly
performs the score-prune-retrain circle for multiple
rounds, and each round is one iteration.
•Local Pruning : Local pruning prunes a network by
subdividing all weights (or filter, channels, etc.,) into
subsets (e.g., layers) and then removing a percentage
of each subset [67].
•Global Pruning : In contrast to local pruning, global
pruning removes structures from all available struc-
tures of a network until a target prune ratio is
reached [67].
•Dynamic Pruning : Dynamic pruning depends on
specific inputs [64], wherein different subnetworks
are generated for each input sample.
•Static Pruning : In contrast to dynamic pruning, the
pruned model is shared by different samples for
static pruning [64]. In other words, the model capac-
ities are fixed for different inputs.
•Lottery Ticket Hypothesis : Lottery Ticket Hypoth-
esis ( LTH ) [48] suggests that a randomly-initialized
dense network f(x;W0)contains a sparse subnet-
work f(x;W0⊙M)which can be trainable with the
original weights to achieve competitive performance
compared to the original networks.
•Winning Tickets : For a randomly initialized net-
work f(x;W0), a winning ticket f(x;W0⊙M)is
its subnetwork that once be trained for Tepochs
(i.e.,f(x;Wt⊙M) will match the performance of the
trained network f(x;Wt)under a non-trivial prune
ratio [48].
•Layer Collapse : Layer collapse is a phenomenon
mentioned in [50], which occurs when all weights
in a layer in a network are removed, rendering the
network untrainable. Hayou et al. [242] provide a
formal definition (i.e., ill-conditioned NN) to this
problem.
•Weight Rewinding : Weight rewinding [144] rewinds
the weights of the subnetwork to the values in an
earlier epoch in training Wt, where t << T .
•Learning Rate Rewinding : Learning rate rewinding,
proposed in [111], trains the remaining weights from
the final values using the learning rate schedule for
a specified number of epochs.
•FLOPs : Float Point Operations (FLOPs) is a com-
monly used metric ([14, 57, 62, 82, 83, 100, 188]) to
evaluate acceleration of the pruned models theoret-
ically. Some works [14, 83] introduce the estimation
methods to compute FLOPs of neural networks.
•MACs : In addition to FLOPs, Multiply-Accumulate
Operations (MACs) is another popular proxy for
evaluating the computational consumption of a net-
work. Nonnenmacher et al. [67] introduce a MACs
estimation method.
•Fine-tuning : In the context of pruning, fine-tuning
continues to train the preserved weights using the
final weight values after pruning [42, 111].
•Training from Scratch : Training from scratch is a par-
ticular case of weight rewinding, where the weights
of the subnetwork are rewinded to their original
values W0.TABLE 10: Notations and descriptions.
Notation Description
x input data or activation/feature
xi thei-th input data or activation/feature
y output data for x
yi the output data for xi
D dataset
f a network function
N the number of samples in a dataset
W the model weights
wi thei-th weight of a model
wij a weight that connects input ito output j
Wt the weights after training tepochs
Fi,j thej-th filter of the i-th layer
ci thei-th channel of a network
ℓ standard loss function, e.g., cross-entropy loss
L target loss function
Lv the validation loss function
L total number of layers in a network
⊙ element-wise multiplication
λ a balance factor
γ a scaling factor vector
M the masks of weights (or filters, channels, etc.)
mi thei-th mask of M
R(·) regularization term
∆L loss change
c(i)
out the number of filters at layer i
A the set of every layer’s keep ratio
APPENDIX B
EXPERIMENTAL SETTINGS
We use CIFAR-10/100 [177] or ImageNet ILSVRC-2012 [181]
to evaluate Top-1 accuracy of the pruning methods on VGG-
16 [1], ResNet-32/152 [9], or DeiT-Tiny [194] for image
classification. CIFAR-10 and CIFAR-100 datasets contain
50K training and 10K test images for 10 and 100 classes,
respectively. ImageNet includes over 1.28 million training
and 50K validation images for 1000 classes. For VGG-16 and
ResNet-32 on CIFAR-10/100, we conduct the experiments
on one NVIDIA A100 GPU (40 GB) using SGD with 0.9
momentum [243], a weight decay of 10−4, and train the
pruned networks for 160 epochs with a batch size of 128.
The initial learning rate is 0.1, reduced at epochs 60 and
120, as in [50]. For DeiT-Tiny on ImageNet, we use four
NVIDIA A100 GPUs (40 GB) with AdamW (0.9 momen-
tum) and a cosine learning rate decay strategy, training the
pruned networks for 100 epochs with a batch size 256 of per
GPU and an initial learning rate of 5×10−4, as in [159].
The code for SNIP [54] and GraSP [56] can be found at
https://github.com/JingtongSu/sanity-checking-pruning.
The code for SynFlow [50] and WDPruning [159] is available
at https://github.com/ganguli-lab/Synaptic-Flow and
https://github.com/andyrull/width-and-Depth-pruning-f
or-Vision-Transformer, respectively.
In addition, we conduct experiments for LLaMA-7B
[178] on one NVIDIA A100 GPU (40 GB) to assess the
zero-shot ability of pruned LLMs on WikiText2 [174] and
PTB [179] for language generation using perplexity (PPL)5.
Besides, we follow LLaMA to implement zero-shot task clas-
sification and multiple-choice on four common sense rea-
soning datasets: BoolQ [244], PIQA [245], HellaSwag [246],
and WinoGrande [247]. The code for LLM-Pruner [20],
SparseGPT [17], and Wanda [49] is available at https:
//github.com/horseee/LLM-Pruner, https://github.c
5. https://huggingface.co/spaces/evaluate-metric/perplexity

--- PAGE 18 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 18
om/IST-DASLab/sparsegpt, and https://github.com/loc
uslab/wanda, respectively.
For the experiments in Section 7.2 of the main text,
we choose a blocksize of 128 for SparseGPT’s iterative
pruning. In Section 7.4, we use pre-trained DeiT-Tiny on
ImageNet as the initialized weights and then retrain DeiT-
Tiny on CIFAR-10 to obtain the pre-trained weights. Both
a randomly initialized ResNet-152 and a ResNet-152 pre-
trained on CIFAR-100 are pruned using GraSP [56]. In
Section 7.5, ResNet-152 pre-trained on CIFAR-100 is pruned
using GraSP , while DeiT-Tiny pre-trained on ImageNet is
pruned using WDPruning [159]. The pruned networks are
subsequently fine-tuned or trained from scratch for the same
number of epochs.
APPENDIX C
MORE COMPARISON RESULTS
In this Section, we provide additional comparison results
of pruning methods under contrast settings, such as un-
structured vs. structured, to expand on the related content
discussed in Section 7 in the main text. For instance, Table 13
- Table 176offer a comprehensive comparison of different
pruning methods applied to CNNs and Transformer-based
models of varying sizes (small, medium, or large).
C.1 More Results for Unstructured vs. Structured Pruning
Table 11 compares the results of applying the same scor-
ing method (LLM-surgeon [164]) to both unstructured and
structured pruning across various models and five prune
ratios. The results, sourced from [164], indicate that unstruc-
tured pruning consistently outperforms structured pruning
at the same prune ratio. Additionally, Table 13 - Table 17
show that unstructured pruning typically achieves higher
prune ratios, such as over 70% or 80%, while structured
pruning generally remains below 50%. In terms of per-
formance, unstructured pruning generally results in better
outcomes at similar prune ratios. For example, under similar
settings, the unstructured pruning method, Jackpot [248]
in Table 13, results in a Top-1 accuracy loss of 0.44 on
ImageNet at an 80% prune ratio, whereas the structured
method, SCOP [249], incurs a 0.89 loss at a 51.80% prune
ratio. The advantages and disadvantages of unstructured
and structured pruning are illustrated in Table 12.
C.2 More Results for One-shot vs. Iterative Pruning
In Table 13, pruning 90% on ResNet-50 with ImageNet by
using SNIP [54] results in a Top-1 accuracy loss of 14.10, and
pruning 95% leads to a loss of 31.30. In contrast, iterative-
SNIP [250] shows a lower Top-1 accuracy loss of 11.90 at
90% pruning and 30.90 at 95% pruning. Similarly, iterative
pruning using LoRAPruner [182] in Table 17 demonstrates
superior performance compared to one-shot pruning with
LLM-Pruner [20], under a similar setting with equivalent
prune ratios on LLaMA-7B. However, the influence of spe-
cific functions on performance may overshadow the ad-
vantages of iterative pruning over one-shot pruning. For
6. The results in Table 13 - Table 17 focus on reflecting the pruning
outcomes under a pair of contrast settings, rather than comparing the
specific pruning methods themselves.TABLE 11: Perplexity of unstructured and structured prun-
ing using LLM-surgeon [164] on LLMs with WikiText2 [174]
(lower is better). Bold /Underline mark the best/second best
performance, respectively, among the compared entities.
Method ModelPrune Ratio (%)
10.00 20.00 30.00 40.00 50.00
unstructOPT-125M27.69 27.83 28.35 28.98 30.30
struct 28.01 28.73 31.82 38.47 49.78
unstruct
OPT-1.3B14.62 14.66 14.81 14.91 15.47
struct 14.70 15.12 16.24 18.45 22.95
unstruct
OPT-2.7B12.01 12.14 12.25 12.28 12.68
struct 12.02 12.27 12.92 14.23 17.15
unstruct
OPT-6.7B10.86 10.87 10.82 10.83 10.97
struct 10.77 11.02 11.64 12.58 14.90
unstructLLaMA-2-7B5.13 5.20 5.36 5.66 6.08
struct 5.25 6.18 7.83 10.39 15.38
TABLE 12: Advantages and disadvantages of unstructured
and structured pruning.
Unstructured Structured
High sparsity with" hardminor accuracy drop
Speedup w/o specific hardwarehard "(e.g., FPGAs or ASICs)
Speedup w/o specific softwarehard "(e.g., sparsity CNNs libraries)
Really compressed withhard "significant acceleration
Structure coupling % "
instance, one-shot pruning SAViT [251] in Table 15 shows a
Top-1 accuracy drop of 1.48 when 30.77% of FLOPs on DeiT-
Tiny are pruned, outperforming iterative pruning SPViT
[252] under similar settings, which exhibits a Top-1 accuracy
drop of 1.50 with only 23.08% of FLOPs pruned.
C.3 More Results for Data-free vs. Data-driven Pruning
Consistent with the main text, the effectiveness of pruning
in PBT methods is not strictly dependent on data usage. For
example, when pruning ResNet-50 on ImageNet, data-free
pruning NTK-SAP [253], as shown in Table 13, outperforms
data-driven pruning methods SNIP [54] and Grasp [56],
with a Top-1 accuracy loss of 15.41 at a 95.60% prune ratio,
compared to losses of 35.51 and 16.47 for SNIP and Grasp,
respectively. However, for PAT methods, data usage appears
crucial for maintaining performance in pruned models. For
instance, under similar settings, data-free pruning DFPC
[254] shows a Top-1 accuracy drop of 1.20 when 49.49% of
FLOPs are pruned, whereas data-driven methods GFP [62]
and PGMPF [255] show smaller drops of 0.37 at a higher
50.11% of FLOPs removed and 0.90 at a higher 53.50%
of FLOPs pruned, respectively. Most pruning methods for
Transformer-based models are data-driven, typically requir-
ing target or calibration data during the pruning process.
For instance, the pruning methods for LLMs in Table 17 are
all data-driven methods.
C.4 More Results for Pruning on Initialized vs. Pre-trained Weights
We prune a randomly initialized VGG-16 and a VGG-16 pre-
trained on CIFAR-10 using SynFlow to obtain the pruning
results on initialized and pre-trained weights, respectively.
A similar approach is applied to ResNet-32 as well. The
results in Fig. 10 show that for PBT methods, SynFlow [50]

--- PAGE 19 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 19
10 20 30 40 50 60 70 80 90
Prune Ratio of Parameters (%)93.093.293.493.693.894.0 T op-1 Accuracy (%) SynFlow (initial)
SynFlow (pretrain)
(a) VGG-16 on CIFAR-10
10 20 30 40 50 60 70 80 90
Prune Ratio of Parameters (%)92.593.093.594.094.5 T op-1 Accuracy (%) SNIP (initial)
SNIP (pretrain) (b) ResNet-32 on CIFAR-10
Fig. 10: PBT methods on initialized vs. pre-trained weights.
Shaded regions indicate standard deviation based on three
independent runs (best view in color).
and SNIP [54], pruning on the pre-trained weights does
not guarantee improved Top-1 accuracy. Cai et al. [255]
compare the results of pruning ResNet-34 on ImageNet
from scratch and with pre-trained weights, and the results
indicate that pre-trained weights are generally crucial for
non-PBT methods to find effective subnetworks. Table 13 -
Table 17 show that most pruning methods, especially those
for Transformer-based models, are based on pre-trained
rather than randomly initialized weights.
C.5 More Results for Global vs. Local Pruning
Tables 13 - Table 17 show that more pruning methods use
global than local pruning. At an equivalent pruning ratio,
global pruning methods, such as Bonsai [132] in Table 17,
can outperform local pruning methods like LLM-pruner
[20] and LoRAPruner [182]. For instance, with a 50% pa-
rameter pruning ratio for LLaMA-7B, Bonsai achieves a
result of 10.92 on WikiText2 and 67.22 and 61.64 on BoolQ
and WinoGrande, respectively, which are better by a large
margin than the results of LLM-pruner (16.41, 60.28, 53.43)
and LoRAPruner (11.60, 61.88, 55.01). However, the results
can still vary significantly with similar settings for different
global pruning methods. For example, as shown in Table 4,
when CP [134] has a FLOPs pruning ratio of 50.11%, the Top-
1 accuracy loss of the pruned ResNet-50 on ImageNet is 1.07,
whereas GFP [62] only has a loss of 0.37. Even with global
pruning, achieving a globally optimal result is challenging.
C.6 More Results for Training from Scratch vs. Fine-tuning
Fine-tuning a pruned network or training it from scratch
are two methods for restoring the performance of pruned
models. Among the pruning methods listed in Table 13 -
Table 17, the majority opt for the former, such as ThiNet [16]
and GFP [62] in Table 13, and SCOP [249] and PLATON
[256] in Table 15. A few methods, like DMCP [102] in
Table 13, choose the latter. CP [134] in Table 13 conducts
comparative experiments between the two methods. Results
indicate that, under the same pruning ratio (50%), fine-
tuning incurs a loss of 1.40 in Top-1 accuracy for ResNet-50
on ImageNet, while training from scratch results in a loss of
4.00.
APPENDIX D
PRUNING FOR SPECIFIC APPLICATIONS
This section summarizes the characteristics of mainstream
applications involved in pruning. The commonly used net-works, datasets, and evaluation metrics of pruning are
shown in Table 18. The statistics of pruning literature across
different applications are shown in Fig. 12.
D.1 Image Classification
Image classification refers to classifying images according to
their visual content and is the most basic task for pruning.
Most pruning works in CV provide experimental results on
ImageNet (ILSVRC-2012) [181]. The representative pruning
results on ImageNet are shown in Fig. 11, with results
sourced from the corresponding papers. The networks used
in papers with the same name may differ in the network
structures, leading to different accuracy even if other set-
tings are the same. For example, some works (e.g., [55, 67])
expand the layers’ width of ResNet-32 by a factor (such
as 2 or 4). Top-1 accuracy of the expanded ResNet-32 is
higher than that of the vanilla ResNet-32. Some works (such
as [57, 59]) use VGG-16 with one FC layer, while others
([82, 135]) adopt two or three FC layers.
D.2 Object Detection
In comparison with image classification, object detection
requires predicting the class and the exact location of each
object in an image. Correspondingly, neural networks for
object detection have more complex architectures, including
backbones and other detection components. In addition, ob-
ject detection generally requires larger input sizes, making
pruning for object detection more challenging than image
classification. Only a few works (e.g., [228, 279]) study
pruning for object detection. For instance, Bonnaerens et al.
[279] propose an anchor pruning method. Girish et al. [262]
investigate LTH for object detection. Liu et al. [62] prune
one-stage and two-stage object detection models to vali-
date performance. The commonly used accuracy metrics, as
shown in Table 18, include mAP (mean Average Precision)
and COCO mAP that is mAP evaluated at Intersection-
Over-Union ( IoU) thresholds evenly distributed between 0.5
and 0.95.
D.3 Image Style Translation
Image style translation, which means transferring the style
from one image to another, is an important application
for deploying GANs on mobile devices. Compared with
networks for image classification, GANs have very different
network structures and outputs, consisting of a generator
and a discriminator that output high-dimension images. Shu
et al. [280] identify two major differences between com-
pressing models for image classification and GANs. First,
the discriminator network does not need to be compact
because it will be discarded after training the generative
network. Second, it is difficult to quantitatively evaluate the
output images by generated GANs. Besides, the training
difficulty poses extra challenges for pruning GANs [280].
Some pruning methods (e.g., [229, 280]) are proposed to
reduce GAN’s parameters and computational complexities.
For example, Chen et al. [114] prune CycleGAN to verify
matching subnetworks in GANs.
As shown in Table 18, FCN-scores [281], and Fr ´echet
Inception Distance ( FID) between the feature distributions

--- PAGE 20 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 20
30 40 50 60 70 80
Remained FLOPs (%)70727476788082T op-1 accuracy (%)[64]
Meta[67]GFP[20]
IE[65]GBN[66]Meta[67]GFP[20]
Hrank[69]GBN[66]
ThiNet[57]Meta[67]GFP[20]
IE[65]
SSSSAViT[50]NViT[81]
CP-ViT[83]S2ViTE[77]
CP-ViT[83]
UVC[78]X-Pruner[80]SViTE[77] SViTE[77]ResNet-50
DeiT-Base
DeiT-Tiny
Fig. 11: Top-1 accuracy of representative pruning methods
on ImageNet (best view in color and zoom in).
of real and generated samples [115] are often utilized to
evaluate pruning results. FCN-scores include pixel accuracy,
class accuracy, and class IoU. The larger FID indicates better
transfer results.
D.4 Adversarial Robustness
In safety-critical but computationally resource-constrained
applications, neural network pruning faces the challenge
of whether sparse models can preserve robustness. While
the research community has extensively explored robust
training and network pruning independently, only a few
recent works ([18, 156]) have studied them jointly. For
example, Sehwag et al. [156] empirically verify that adver-
sarial robustness can be achieved with weight magnitude-
based pruning. Sehwag et al. [18] increase the awareness
of pruning techniques to the robust training objective and
formulates the pruning objective as an empirical risk mini-
mization problem.
The benign accuracy in Table 18 refers to the percentage
of correctly classified original (i.e., non-modified) inputs.
Empirical Robust Accuracy ( ERA ) refers to the percentage
of robust test samples under gradient-based attacks. Veri-
fiable Robust Accuracy ( VRA ) corresponds to the fraction
of test samples that are verified to be robust by network
verification methods as described in [282].
D.5 Other CV Tasks
In addition to the above tasks, some works explore pruning
methods for other CV tasks, such as semantic segmen-
tation ([262]), image deraining ([225]), human-pose esti-
mation ([240]), head-pose estimation ([283]), image super-
resolution ([168]), object tracking ([284]), text-to-image gen-
eration ([39]), and backdoor attack ([285]) etc. For example,
Wang et al. [240] prune CMU-pose model [286] for human
pose estimation. Li et al. [168] prune DnCNN [287] and U-
Net [288] for image denoising.
D.6 Natural Language Processing
See et al. [154] explore one of the earliest pruning methods
of deep neural networks for NLP , where a magnitude-based
method is proposed to prune LSTMs [289] for machine
translation. Yu et al. [290] explore the lottery hypothesis
in NLP . Voita et al. [169] prune multi-head self-attention of
Transformer [66]. Chen et al. [196] pioneer the study of the
lottery ticket hypothesis in pre-trained BERT [3] models and
AS
NLP(a) Pipelines & applications
Struct (b) Pipelines & unstruct/struct
Fig. 12: Statistics of pruning literature in our survey.
find matching subnetworks. Currently, transformer-based
large language models (such as LLaMA [178], OPT [175])
have become the dominant modeling paradigm in natural
language processing.
As shown in Table 187, the GLUE benchmark8pro-
vides nine tasks and their corresponding metrics, including
accuracy, Pearson correlation, Matthew’s correlation, etc.
BiLingual Evaluation Understudy ( BLEU ) score [292] is
often used for evaluating the accuracy of machine trans-
lation. Perplexity is used for language modeling or machine
translation to measure the change in performance due to
pruning. The lower the Perplexity, the better the pruned
model.
D.7 Vision-and-Language Tasks
Vision-and-Language (VL) is one of the most common areas
of multimodal research. VL tasks include Visual Question
Answering (VQA) [293], image-text retrieval, etc. Gan et al.
[122] pioneer the investigation of lottery ticket in VL tasks
and find “relaxed” winning tickets that match 99% of the
full accuracy can be found with a 50%-70% prune ratio of
parameters in UNITER [294]. Table 18 lists widely employed
models, datasets, and evaluation metrics for pruning on
VQA and image-text retrieval. The evaluation metrics TR@1
and IR@1 for image retrieval tasks denote Top-1 text recall
and image recall, respectively.
D.8 Audio and Speech Processing
Speech recognition is one of the most common tasks in au-
dio and speech processing. Lightweight speech recognition
has become an indispensable feature on mobile devices.
Nrang et al. [295] prune Deep Speech 2 [296], a recur-
rent neural network architecture, to validate their gradual
pruning scheme. Ding et al. [297] extend the lottery ticket
hypothesis to speech recognition models and investigate the
existence of winning tickets. PARP [213] prunes pre-trained
wav2vec 2.0 [298] and XLSR-53 [299] for self-supervised
speech recognition.
As shown in Table 18, the Word Error Rate ( WER ) is the
standard metric for measuring the accuracy of speech recog-
nition, which is defined as WER = (S+I+D)/(S+I+C),
7. WMT’14/16 refers to https://www.statmt.org/wmt14/translation-
task.html
8. The detailed information of GLUE tasks can refer to Table 1 in
[291].

--- PAGE 21 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 21
where S,I,D, and Cdenote the number of substitutions,
insertions, deletions, and correct words, respectively. The
lower the value, the better the accuracy of the speech recog-
nition model. The Character Error Rate ( CER ) has the exact
definition of WER, except that CER counts characters while
WER counts words.
REFERENCES
[1] K. Simonyan and A. Zisserman, “Very deep convolutional net-
works for large-scale image recognition,” in ICLR , 2015.
[2] A. Dosovitskiy, L. Beyer, and A. Kolesnikov, et al., “An image is
worth 16x16 words: Transformers for image recognition at scale,”
inICLR , 2021.
[3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-
training of deep bidirectional transformers for language under-
standing,” in NAACL , 2019.
[4] A. Chowdhery, S. Narang, and J. Devlin, et al., “PaLM:
Scaling language modeling with pathways,” arXiv preprint
arXiv:2204.02311 , 2022.
[5] T. Bohnstingl, A. Garg, S. Wo ´zniak, G. Saon, E. Eleftheriou,
and A. Pantazi, “Towards efficient end-to-end speech recognition
with biologically-inspired neural networks,” in NeurIPS , 2021.
[6] S. Latif, M. Shoukat, and F. Shamshad, et al., “Sparks of
large audio models: A survey and outlook,” arXiv preprint
arXiv:2308.12792 , 2023.
[7] J. Lin, H. Yin, and W. Ping, et al., “Vila: On pre-training for visual
language models,” in CVPR , 2024.
[8] Y. Liu, K. Zhang, and Y. Li, et al., “Sora: A review on background,
technology, limitations, and opportunities of large vision mod-
els,” arXiv preprint arXiv:2402.17177 , 2024.
[9] K. He, X. Zhang, S. Ren, , and J. Sun, “Deep residual learning for
image recognition,” in CVPR , 2016, pp. 770–778.
[10] H. You, C. Li, and P . Xu, et al., “Drawing early-bird tickets:
Towards more efficient training of deep networks,” in ICLR , 2020.
[11] J. Wu, W. Gan, Z. Chen, S. Wan, and H. Lin, “AI-generated
content AIGC: A survey,” arXiv preprint arXiv:2303.04226 , 2023.
[12] S. Han, J. Pool, J. Tran, and W. Dally, “Learning both weights and
connections for efficient neural network,” in NIPS , vol. 1, 2015.
[13] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compress-
ing deep neural networks with pruning, trained quantization and
huffman coding,” in ICLR , 2016.
[14] X. Dong, J. Huang, Y. Yang, and S. Yan, “More is Less: A more
complicated network with less inference complexity,” in CVPR ,
2017.
[15] Z. You, K. Yan, J. Ye, M. Ma, and P . Wang, “Gate Decorator: Global
filter pruning method for accelerating deep convolutional neural
networks,” in NeurIPS , 2019.
[16] J.-H. Luo, J. Wu, and W. Lin, “ThiNet: A filter level pruning
method for deep neural network compression,” in ICCV , 2017.
[17] E. Frantar and D. Alistarh, “SparseGPT: Massive language
models can be accurately pruned in one-shot,” arXiv preprint
arXiv:2301.00774 , 2023.
[18] V . Sehwag, S. Wang, P . Mittal, and S. Jana, “HYDRA: Pruning
adversarially robust neural networks,” in NeurIPS , 2020.
[19] S. Ashkboos, M. L. Croci, and M. Gennari, et al., “SliceGPT: Com-
press large language models by deleting rows and columns,” in
ICLR , 2024.
[20] X. Ma, G. Fang, and X. Wang, “LLM-Pruner: On the structural
pruning of large language models,” in NeurIPS , vol. 36, 2023, pp.
21 702–21 720.
[21] E. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus,
“Exploiting linear structure within convolutional networks for
efficient evaluation,” in NIPS , vol. 1, 2014, pp. 1269–1277.
[22] S. Lin, R. Ji, C. Chen, D. Tao, and J. Luo, “Holistic CNN com-
pression via low-rank decomposition with knowledge transfer,”
TP AMI , 2018.
[23] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer,
“QLoRA: Efficient finetuning of quantized LLMs,” arXiv preprint
arXiv:2305.14314 , 2023.
[24] W. Shao, M. Chen, and Z. Zhang, et al., “OmniQuant: Omnidi-
rectionally calibrated quantization for large language models,” in
ICLR , 2024.
[25] Y. Gu, L. Dong, F. Wei, and M. Huang, “MiniLLM: Knowledge
distillation of large language models,” in ICLR , 2024.[26] X. Xu, M. Li, and C. Tao, et al., “A survey on knowledge distil-
lation of large language models,” arXiv preprint arXiv:2402.13116 ,
2024.
[27] H. Liu, K. Simonyan, and Y. Yang, “DARTS: Differentiable archi-
tecture search,” in ICLR , 2019.
[28] M. Zhang, S. Su, S. Pan, X. Chang, E. Abbasnejad, and R. Haf-
fari, “iDARTS: Differentiable architecture search with stochastic
implicit gradients,” in ICML , 2021.
[29] X. Ding, X. Zhang, N. Ma, J. Han, G. Ding, and J. Sun, “RepVGG:
making VGG-style ConvNets great again,” in CVPR , 2021.
[30] X. Chu, L. Qiao, and X. Zhang, et al., “MobileVLM V2: Faster
and stronger baseline for vision language model,” arXiv preprint
arXiv:2402.03766 , 2024.
[31] S. Hanson and L. Pratt, “Comparing biases for minimal network
construction with back-propagation,” in NIPS , vol. 1, 1988.
[32] R. Mishra, H. Prabhat Gupta, and T. Dutta, “A survey on deep
neural network compression: Challenges, overview, and solu-
tions,” arXiv preprint arXiv:2010.03954 , 2020.
[33] D. Ghimire, D. Kil, and S. heum Kim, “A survey on efficient
convolutional neural networks and hardware acceleration,” Elec-
tronics , vol. 11, no. 945, 2022.
[34] S. Park, J. Choi, S. Lee, and U. Kang, “A comprehensive survey
of compression algorithms for language models,” arXiv preprint
arXiv:2401.15347 , 2024.
[35] M. Xu, W. Yin, and D. Cai, et al., “A survey of resource-
efficient LLM and multimodal foundation models,” arXiv preprint
arXiv:2401.08092 , 2024.
[36] H. Wang, C. Qin, Y. Bai, Y. Zhang, and Y. Fu, “Recent advances
on neural network pruning at initialization,” in IJCAI , 2022.
[37] Y. He and L. Xiao, “Structured pruning for deep convolutional
neural networks: A survey,” arXiv preprint arXiv:2303.00566 , 2023.
[38] G. Fang, X. Ma, M. Song, M. B. Mi, and X. Wang, “DepGraph:
Towards any structural pruning,” in CVPR , 2023.
[39] G. Fang, X. Ma, and X. Wang, “Structural pruning for diffusion
models,” in NeurIPS , 2023.
[40] X. Zhu, J. Li, Y. Liu, C. Ma, and W. Wang, “A survey on
model compression for large language models,” arXiv preprint
arXiv:2308.07633 , 2023.
[41] R. Reed, “Pruning algorithms-a survey,” IEEE Transactions on
Neural Networks , vol. 4, no. 5, pp. 740–747, 1993.
[42] D. Blalock, J. J. G. Ortiz, J. Frankle, and J. Guttag, “What is the
state of neural network pruning?” in MLSys , 2020.
[43] J. T. O’ Neill, “An survey of neural network compression,” arXiv
preprint arXiv:2006.03669 , 2020.
[44] J. Liu, S. Tripathi, U. Kurup, and M. Shah, “Pruning algorithms to
accelerate convolutional neural networks for edge applications:
A survey,” arXiv preprint arXiv:2005.04275 , 2020.
[45] T. Choudhary, V . Mishra, A. Goswami, and J. Sarangapani, “A
comprehensive survey on model compression and acceleration,”
Artificial Intelligence Review , pp. 5113–5155, 2020.
[46] T. Hoefler, D. Alistarh, T. Ben-Nun, N. Dryden, and A. Peste,
“Sparsity in deep learning: pruning and growth for efficient
inference and training in neural networks,” JMLR , vol. 22, no.
241, pp. 1–124, 2021.
[47] W. Wang, W. Chen, and Y. Luo, et al., “Model compression and
efficient inference for large language models: A survey,” arXiv
preprint arXiv:2402.09748 , 2024.
[48] J. Frankle and M. Carbin, “The lottery ticket hypothesis: finding
sparse, trainable neural networks,” in ICLR , 2019.
[49] M. Sun, Z. Liu, A. Bair, and J. Z. Kolter, “A simple and effective
pruning approach for large language models,” in ICLR poster ,
2024.
[50] H. Tanaka, D. Kunin, D. L. Yamins, and S. Ganguli, “Pruning
neural networks without any data by iteratively conserving
synaptic flow,” in NeurIPS , vol. 33, 2020, pp. 6377–6389.
[51] F. Meng, H. Cheng, K. Li, H. Luo, X. Guo, G. Lu, and X. Sun,
“Pruning filter in filter,” in NeurIPSW , 2020.
[52] X. Ma, W. Niu, and T. Zhang, et al., “An image enhancing pattern-
based sparsity for real-time inference on mobile devices,” in
ECCV , 2020.
[53] H. Wang and Y. Fu, “Trainability preserving neural structured
pruning,” in ICLR , 2023.
[54] N. Lee, T. Ajanthan, and P . H. Torr, “SNIP: Single-shot network
pruning based on connection sensitivity,” in ICLR , 2019.
[55] J. Su, Y. Chen, T. Cai, T. Wu, R. Gao, L. Wang, and J. D. Lee,
“Sanity-checking pruning methods: random tickets can win the
jackpot,” in NeurIPS , vol. 33, 2020, pp. 20 390–20 401.

--- PAGE 22 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 22
[56] C. Wang, G. Zhang, and R. Grosse, “Picking winning tickets
before training by preserving gradient flow,” in ICLR , 2020.
[57] Z. Liu, J. Li, Z. Shen, G. Huang, Shoumeng, and Z. Changshui,
“Learning efficient convolutional networks through network
slimming,” in ICCV , 2017, pp. 2736–2744.
[58] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li, “Learning structured
sparsity in deep neural networks,” in NIPS , 2016.
[59] Z. Huang and N. Wang, “Data-driven sparse structure selection
for deep neural networks,” in ECCV , 2018, pp. 304–320.
[60] H. Yang, H. Yin, and M. Shen, et al., “Global vision transformer
pruning with hessian-aware saliency,” in CVPR , 2023.
[61] J. Liu, Z. Xu, and R. Shi, et al., “Dynamic sparse training: Find
efficient sparse network from scratch with trainable masked
layers,” in ICLR , 2020.
[62] L. Liu, S. Zhang, Z. Kuang, A. Zhou, J.-H. Xue, X. Wang, Y. Chen,
W. Yang, Q. Liao, and W. Zhang, “Group fisher pruning for
practical network compression,” in ICML , 2021.
[63] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P . Graf,
“Pruning filters for efficient convnets,” in ICLR , 2017.
[64] Y. Tang, Y. Wang, Y. Deng, C. Xu, D. Tao, and C. Xu, “Manifold
regularized dynamic network pruning,” in CVPR , 2021.
[65] X. Men, M. Xu, and Q. Zhang, et al., “ShortGPT: Layers in large
language models are more redundant than you expect,” arXiv
preprint arXiv:2403.03853 , 2024.
[66] A. Vaswani, N. Shazeer, and N. Parmar, et al., “Attention is all
you need,” in NeurIPS , 2017.
[67] M. Nonnenmacher, T. Pfeil, I. Steinwart, and D. Reeb, “SOSP: Ef-
ficiently capturing global correlations by second-order structured
pruning,” in ICLR , 2022.
[68] Z. Li, G. Yuan, and W. Niu, et al., “NPAS: A compiler-aware
framework of unified network pruning and architecture search
for beyond real-time mobile acceleration,” in CVPR , 2021, pp.
14 255–14 266.
[69] S. Liu, T. Chen, X. Chen, L. Shen, D. C. Mocanu, Z. Wang,
and M. Pechenizkiy, “The unreasonable effectiveness of random
pruning: Return of the most naive baseline for sparse training,”
inICLR , 2022.
[70] Y. Wang, X. Zhang, L. Xie, J. Zhou, H. Su, B. Zhang, and X. Hu,
“Pruning from scratch,” in AAAI , 2020, pp. 12 273–12 280.
[71] Y. Bai, H. Wang, Z. Tao, K. Li, and Y. Fu, “Dual lottery ticket
hypothesis,” in ICLR , 2022.
[72] N. Lee, T. Ajanthan, S. Gould, and P . H. Torr, “A signal propaga-
tion perspective for pruning neural networks at initialization,” in
ICLR , 2020.
[73] A. M. Saxe, J. L. McClelland, and S. Ganguli, “Exact solutions
to the nonlinear dynamics of learning in deep linear neural
networks,” in ICLR , 2014.
[74] T. Gebhart, U. Saxena, and P . Schrater, “A unified paths perspec-
tive for pruning at initialization,” arXiv preprint arXiv:/2101.10552 ,
2021.
[75] A. Jacot, F. Gabriel, and C. Hongler, “Neural tangent kernel:
Convergence and generalization in neural networks,” in NIPS ,
2018.
[76] J. Frankle, G. K. Dziugaite, D. M. Roy, and M. Carbin, “Pruning
neural networks at initialization: why are we missing the mark?”
inICLR , 2021.
[77] A. Gaier and D. Ha, “Weight agnostic neural networks,” in
NeurIPS , 2019.
[78] V . Ramanujan, M. Wortsman, A. Kembhavi, A. Farhadi, and
M. Rastegari, “What’s hidden in a randomly weighted neural
network?” in CVPR , 2020.
[79] D. Hoang, S. Liu, and R. Marculescu, et al., “Revisiting pruning
at initialization through the lens of Ramanujan graph,” in ICLR ,
2023.
[80] T. Kumar, K. Luo, and M. Sellke, “No free prune: Information-
theoretic barriers to pruning at initialization,” arXiv preprint
arXiv:2402.01089 , 2024.
[81] U. Evci, T. Gale, J. Menick, P . S. Castro, and E. Elsen, “Rigging
the lottery: Making all tickets winners,” in ICML , 2020.
[82] C. Zhao, B. Ni, J. Zhang, Q. Zhao, W. Zhang, and Q. Tian,
“Variational convolutional neural network pruning,” in CVPR ,
2019, pp. 2780–2789.
[83] Y. He, G. Kang, X. Dong, Y. Fu, and Y. Yang, “Soft filter pruning
for accelerating deep convolutional neural networks,” in IJCAI ,
2018, pp. 2234–2240.
[84] Z. Liu, H. Mu, X. Zhang, Z. Guo, X. Yang, T. K.-T. Cheng,
and J. Sun, “MetaPruning: Meta learning for automatic neuralnetwork channel pruning,” in ICCV , 2019, pp. 3295–3304.
[85] X. Ning, T. Zhao, W. Li, P . Lei, Y. Wang, and H. Yang, “DSA: More
efficient budgeted pruning via differentiable sparsity allocation,”
inECCV , 2020, pp. 592–607.
[86] M. Cho, S. Adya, and D. Naik, “PDP: Parameter-free differen-
tiable pruning is all you need,” in NeurIPS , 2023.
[87] M. Yuan and Y. Lin, “Model selection and estimation in re-
gression with grouped variables,” Journal of the Royal Statistical
Society: Series B (Statistical Methodology) , vol. 68, pp. 49–67, 2006.
[88] A. Gordon, E. Eban, O. Nachum, and B. Chen, “MorphNet:fast
& simple resource-constrained structure learning of deep net-
works,” in CVPR , 2018.
[89] T. Li, B. Wu, Y. Yang, Y. Fan, Y. Zhang, and W. Liu, “Compress-
ing convolutional neural networks via factorized convolutional
filters,” in CVPR , 2019, pp. 3977–3986.
[90] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Dis-
tributed optimization and statistical learning via the alternating
direction method of multipliers,” Foundations and Trends in Ma-
chine Learning , vol. 3, no. 1, pp. 1–122, 2010.
[91] D. C. Mocanu, E. Mocanu, P . Stone, P . H. Nguyen, M. Gibescu,
and A. Liotta, “Scalable training of artificial neural networks
with adaptive sparse connectivity inspired by network science,”
Nature Communications , vol. 9, no. 1, 2018.
[92] S. Liu, T. Chen, and X. Chen, et al., “Sparse training via boosting
pruning plasticity with neuroregeneration,” in NeurIPS , 2021.
[93] S. Liu, T. Chen, and Z. Atashgahi, et al., “Deep ensembling with
no overhead for either training or testing: The all-round blessings
of dynamic sparsity,” in ICLR , 2022.
[94] X. Dai, H. Yin, and N. K. Jha, “NeST: A neural network synthesis
tool based on a grow-and-prune paradigm,” IEEE Transactions on
Computers , vol. 68, pp. 1487–1497, 2019.
[95] H. Mostafa and X. Wang, “Parameter efficient training of deep
convolutional neural networks by dynamic sparse reparameteri-
zation,” in ICML , 2019.
[96] T. Lin, S. U. Stich, and L. Barba, et al., “Dynamic model pruning
with feedback,” in ICLR , 2020.
[97] T. Dettmers and L. Zettlemoyer, “Sparse networks from scratch:
Faster training without losing performance,” arXiv preprint
arXiv:1907.04840 , 2019.
[98] G. Sokar, E. Mocanu, and D. C. Mocanu, et al., “Dynamic sparse
training for deep reinforcement learning,” in IJCAI , 2022.
[99] U. Evci, Y. A. Ioannou, C. Keskin, and Y. Dauphin, “Gradient
flow in sparse neural networks and how lottery tickets win,” in
AAAI , 2022.
[100] Y. He, P . Liu, Z. Wang, Z. Hu, and Y. Yang, “Filter pruning
via geometric median for deep convolutional neural networks
acceleration,” in CVPR , 2019, pp. 4340–4349.
[101] P . Fletcher, S. Venkatasubramanian, and S. Joshi, “Robust statis-
tics on riemannian manifolds via the geometric median,” in
CVPR , 2008.
[102] S. Guo, Y. Wang, Q. Li, and J. Yan, “DMCP: Differentiable Markov
channel pruning for neural networks,” in CVPR , 2020.
[103] M. Ma, J. Wang, and Z. Yu, “Differentiable network pruning via
polarization of probabilistic channelwise soft masks,” Computa-
tional Intelligence and Neuroscience , vol. 2022, 2022.
[104] L. Yu and W. Xiang, “X-pruner: explainable pruning for vision
transformers,” in CVPR , 2023.
[105] X. Ding, G. Ding, J. Han, and S. Tang, “Auto-balanced filter
pruning for efficient convolutional neural networks,” in AAAI ,
vol. 32, no. 1, 2018.
[106] P . Molchanov, A. Mallya, S. Tyree, I. Frosio, and J. Kautz, “Impor-
tance estimation for neural network pruning,” in CVPR , 2019.
[107] S. Lin, R. Ji, and Y. Li, et al., “Towards compact ConvNets via
structure-sparsity regularized filter pruning,” TNNLS , 2019.
[108] H. Wang, C. Qin, Y. Zhang, and Y. Fu, “Neural pruning via
growing regularization,” in ICLR , 2021.
[109] X. Chen, Y. Hu, and J. Zhang, “Compressing large language
models by streamlining the unimportant layer,” arXiv preprint
arXiv:2403.19135 , 2024.
[110] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, “Rethinking
the value of network pruning,” in ICLR , 2019.
[111] A. Renda, J. Frankle, and M. Carbin, “Comparing rewinding and
fine-tuning in neural network pruning,” in ICLR , 2020.
[112] B. Liu, Z. Zhang, and P . He, et al., “A survey of lottery ticket
hypothesis,” arXiv preprint arXiv:2403.04861 , 2024.
[113] J. Diffenderfer and B. Kailkhura, “Multi-prize lottery ticket hy-
pothesis: finding a accurate binary neural networks by pruning a

--- PAGE 23 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 23
randomly weighted network,” in ICLR , 2021.
[114] X. Chen, Z. Zhang, Y. Sui, and T. Chen, “GANs can play lottery
tickets too,” in ICLR , 2021.
[115] N. M. Kalibhat, Y. Balaji, and S. Feizi, “Winning lottery tickets in
deep generative models,” in AAAI , 2021.
[116] B. Hui, D. Yan, X. Ma, and W.-S. Ku, “Rethinking graph lottery
tickets: Graph sparsity matters,” in ICLR , 2023.
[117] S. Prasanna, A. Rogers, and A. Rumshisky, “When BERT plays
the lottery, all tickets are winning,” in EMNLP , 2020.
[118] E. Malach, G. Yehudai, S. Shalev-Shwartz, and O. Shamir, “Prov-
ing the lottery ticket hypothesis: Pruning is all you need,” in
ICML , 2020.
[119] L. Orseau, M. Hutter, and O. Rivasolata, “Logarithmic pruning is
all you need,” in NeurIPS , 2020.
[120] A. S. Morcos, H. Yu, M. Paganini, and Y. Tian, “One ticket to win
them all: Generalizing lottery ticket initializations across datasets
and optimizers,” in NeurIPS , 2019.
[121] R. Mehta, “Sparse transfer learning via winning lottery tickets,”
arXiv preprint arXiv:1905.07785 , 2019.
[122] Z. Gan, Y.-C. Chen, L. Li, T. Chen, Y. Cheng, S. Wang, J. Liu,
L. Wang, and Z. Liu, “Playing lottery tickets with vision and
language,” in AAAI , 2022.
[123] T. Chen, Y. Sui, and X. Chen, et al., “A unified lottery ticket
hypothesis for graph neural networks,” in ICML , 2021.
[124] Z. Zhang, J. Jin, Z. Zhang, Y. Zhou, X. Zhao, J. Ren, J. Liu, L. Wu,
R. Jin, and D. Dou, “Validating the lottery ticket hypothesis with
inertial manifold theory,” in NeurIPS , 2021.
[125] S. Zhang, M. Wang, S. Liu, P .-Y. Chen, and J. Xiong, “Why lottery
ticket wins? a theoretical perspective of sample complexity on
pruned neural networks,” in NeurIPS , 2021.
[126] M. Paul, F. Chen, and B. W. Larsen, et al., “Unmasking the lottery
ticket hypothesis-what’s encoded in a winning ticket’s mask,” in
ICLR , 2023.
[127] X. Ma, G. Yuan, and X. Shen, et al., “Sanity checks for lottery
tickets: Does your winning ticket really win the jackpot?” in
NeurIPS , 2021.
[128] N. Liu, G. Yuan, Z. Che, X. Shen, X. Ma, Q. Jin, J. Ren, J. Tang,
S. Liu, and Y. Wang, “Lottery ticket preserves weight correlation:
Is it desirable or not?” in ICML , 2021.
[129] E. J. Hu, Y. Shen, and P . Wallis, et al., “LoRA: Low-rank adapta-
tion of large language models,” in ICLR poster , 2022.
[130] D. Shi, C. Tao, Y. Jin, Z. Yang, C. Yuan, and J. Wang, “UPop: Uni-
fied and progressive pruning for compressing vision-language
transformers,” in ICML , 2023.
[131] A. Nova, H. Dai, and D. Schuurmans, “Gradient-free structured
pruning with unlabeled data,” in ICML , 2023.
[132] L. Dery, S. Kolawole, and J.-F. Kagy, et al., “Everybody prune
now: Structured pruning of LLMs with only forward passes,”
arXiv preprint arXiv:2402.05406 , 2024.
[133] B.-K. Kim, G. Kim, and T.-H. Kim, et al., “Shortened LLaMA: A
simple depth pruning for large language models,” arXiv preprint
arXiv:2402.02834 , 2024.
[134] Y. He, X. Zhang, and J. Sun, “Channel pruning for accelerating
very deep neural networks,” in ICCV , 2017.
[135] S. Lin, R. Ji, C. Yan, B. Zhang, L. Cao, Q. Ye, F. Huang, and
D. Doermann, “Towards optimal structured CNN pruning via
generative adversarial learning,” in CVPR , 2019.
[136] M. Xia, Z. Zhong, and D. Chen, “Structured pruning learns
compact and accurate models,” in ACL , 2022.
[137] M. Xia, T. Gao, Z. Zeng, and D. Chen, “Sheared LLaMA: Accel-
erating language model pre-training via structured pruning,” in
ICLR , 2024.
[138] R. Tibshirani, “Regression shrinkage and selection via the lasso,”
Journal of the Royal Statistical Society , vol. 58, no. 1, pp. 267–288,
1996.
[139] H. Yang, Y. Zhu, and J. Liu, “ECC: Platform-independent energy-
constrained deep neural network compression via a bilinear
regression model,” in CVPR , 2019.
[140] S. Gao, F. Huang, W. Cai, and H. Huang, “Network pruning via
performance maximization,” in CVPR , 2021, pp. 9270–9280.
[141] H. You, C. Li, and P . Xu, et al., “Drawing early-bird tickets:
towards more efficient training of deep networks,” in ICLR , 2020.
[142] X. Chen, Y. Cheng, S. Wang, Z. Gan, Z. Wang, and J. Liu, “Early-
BERT: Efficient BERT training via early-bird lottery tickets,” in
ACL-IJCNLP , 2021, pp. 2195–2207.
[143] P . Michel, O. Levy, and G. Neubig, “Are sixteen heads really
better than one?” in NeurIPS , 2019, pp. 14 014–14 024.[144] J. Frankle, G. K. Dziugaite, and D. M. Roy, et. al, “Linear mode
connectivity and the lottery ticket hypothesis,” in ICML , 2020.
[145] W. Kwon, S. kim, M. W. Mahoney, J. Hassoun, K. Keutzer,
and A. Gholami, “A fast post-training pruning framework for
transformers,” in NeurIPS , 2022.
[146] Y. An, X. Zhao, and T. Yu, et al., “Fluctuation-based adaptive
structured pruning for large language models,” in AAAI , 2024.
[147] Y. Rao, J. Lu, J. Lin, and J. Zhou, “Runtime network routing for
efficient image classification,” TP AMI , vol. 41, no. 10, 2019.
[148] W. Hua, Y. Zhou, C. D. Sa, Z. Zhang, and G. E. Suh, “Channel
gating neural networks,” in NeurIPS , 2019, pp. 1886–1896.
[149] X. Gao, Y. Zhao, and L. Dudziak, et al., “Dynamic channel
pruning: Feature boosting and suppression,” in ICLR , 2019.
[150] S. Elkerdawy, M. Elhoushi, H. Zhang, and N. Ray, “Fire together
wire together: A dynamic pruning approach with self-supervised
mask prediction,” in CVPR , 2022, pp. 12 454–12 463.
[151] J. Meng, L. Yang, J. Shin, D. Fan, and J. sun Seo, “Contrastive
dual gating: Learning sparse features with contrastive learning,”
inCVPR , 2022.
[152] R. Hadsell, S. Chopra, and Y. LeCun, “Dimensionality reduction
by learning an invariant mapping,” in CVPR , 2006.
[153] S. Tuli and N. K. Jha, “AccelTran: A sparsity-aware accelerator
for dynamic inference with transformers,” IEEE Transactions on
ComputerAided Design of Integrated Circuits and Systems , vol. 42,
pp. 4038–4051, 2023.
[154] A. See, M.-T. Luong, and C. D. Manning, “Compression of
neural machine translation models via pruning,” arXiv preprint
arXiv:/1606.09274 , 2016.
[155] E. S. Lubana and R. P . Dick, “A gradient flow framework for
analyzing network pruning,” in ICLR , 2021.
[156] V . Sehwag, S. Wang, and P . Mittal, et al., “Towards compact and
robust deep neural networks,” arXiv preprint arXiv:/1906.06110 ,
2019.
[157] P . Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz, “Pruning
convolutional neural networks for resource efficient inference,”
inICLR , 2017.
[158] Y. Li, S. Lin, and J. Liu, et al., “Towards compact CNNs via
collaborative compression,” in CVPR , 2021, pp. 6438–6447.
[159] F. Yu, K. Huang, M. Wang, Y. Cheng, W. Chu, and L. Cui, “Width
& depth pruning for vision transformers,” in AAAI , 2022.
[160] J. Rosenfeld, J. Frankle, M. Carbin, and N. Shavit, “On the
predictability of pruning across scales,” in ICML , 2021.
[161] J. Lee, S. Park, S. Mo, S. Ahn, and J. Shin, “Layer-adaptive sparsity
for the magnitude-based pruning,” in ICLR , 2021.
[162] Y. LeCun, J. Denker, and S. Solla, “Optimal brain damage,” in
NIPS , 1989, pp. 598–605.
[163] M. Santacroce, Z. Wen, and Y. Shen, et al., “What matters in
the structured pruning of generative language models?” arXiv
preprint arXiv:2302.03773 , 2023.
[164] T. F. A. van der Ouderaa, M. Nagel, and M. van Baalen, et al.,
“The LLM surgeon,” arXiv preprint arXiv:2312.17244 , 2024.
[165] B. Hassibi and D. G. Stork, “Second order derivatives for network
pruning: Optimal brain surgeon,” in NIPS , 1992.
[166] C. Wang, R. Grosse, and S. Fidler, et al., “EigenDamage: Struc-
tured pruning in the Kronecker-Factored eigenbasis,” in ICML ,
2019.
[167] E. Kurtic, D. Campos, and T. Nguyen, et al., “The optimal BERT
surgeon: Scalable and accurate second-order pruning for large
language models,” in EMNLP , 2022.
[168] Y. Li, S. Gu, K. Zhang, L. Van Gool, and R. Timofte, “DHP:
Differentiable meta pruning via hypernetworks,” in ECCV , 2020.
[169] E. Voita, D. Talbot, F. Moiseev, R. Sennrich, and I. Titov, “Ana-
lyzing multi-head self-attention: Specialized heads do the heavy
lifting, the rest can be pruned,” in ACL , 2019, pp. 5797–5808.
[170] M. Zhang, X. Yu, J. Rong, and L. Ou, “Graph pruning for model
compression,” Applied Intelligence , vol. 52, pp. 11 244–11 256, 2022.
[171] Y. He, J. Lin, Z. Liu, H. Wang, L.-J. Li, and S. Han, “AMC: AutoML
for model compression and acceleration on mobile devices,” in
ECCV , 2018, pp. 784–800.
[172] Z. Zhong, J. Yan, W. Wu, J. Shao, and C.-L. Liu, “Practical block-
wise neural network architecture generation,” in ECCV , 2018.
[173] J. V . Amersfoort, M. Alizadeh, and S. Farquhar, et al., “Sin-
gle shot structured pruning before training,” arXiv preprint
arXiv:2007.00389 , 2020.
[174] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer sentinel
mixture models,” arXiv preprint arXiv:1609.07843 , 2016.
[175] S. Zhang, S. Roller, and N. Goyal, et al., “OPT: Open pre-trained

--- PAGE 24 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 24
transformer language models,” arXiv preprint arXiv:2205.01068 ,
2022.
[176] F. J. Huang and Y. LeCun, “Learning methods for generic object
recognition with invariance to pose and lighting,” in CVPR , 2004.
[177] A. Krizhevsky, “Learning multiple layers of features from tiny
images,” Technical Report TR-2009 , 2009.
[178] H. Touvron, T. Lavril, and G. Izacard, et al., “LLaMA: Open
and efficient foundation language models,” arXiv preprint
arXiv:2302.13971 , 2023.
[179] M. Marcus, B. Santorini, and M. A. Marcinkiewicz, “Building a
large annotated corpus of English: The penn treebank,” Computa-
tional Linguistics , vol. 19, pp. 313–330, 1993.
[180] Y. Li, S. Lin, B. Zhang, J. Liu, D. Doermann, Y. Wu, F. Huang, and
R. Ji, “Exploiting kernel sparsity and entropy for interpretable
CNN compression,” in CVPR , 2019.
[181] O. Russakovsky, J. Deng, and H. Su, et al., “ImageNet large scale
visual recognition challenge,” IJCV , vol. 115, pp. 211–252, 2015.
[182] M. Zhang, H. Chen, and C. Shen, et al., “LoRAPrune: Pruning
meets low-rank parameter-efficient fine-tuning,” arXiv preprint
arXiv:2305.18403 , 2023.
[183] S. Singh and R. Liu, “Why is pruning at initialization immune
to reinitializing and shuffling?” arXiv preprint arXiv:2107.01808 ,
2021.
[184] Y. Qiu and R. Suda, “Train-by-Reconnect: decoupling locations of
weights from their values,” in NeurIPS , 2020.
[185] C. R. Wolfe, Q. Wang, J. L. Kim, and A. Kyrillidis, “How much
pre-training is enough to discover a good subnetwork?” in
PMLR , vol. 144, 2022, pp. 1–34.
[186] I. Amos, J. Berant, and A. Gupta, “Never train from scratch:
Fair comparison of long-sequence models requires data-driven
priors,” in ICLR , 2024.
[187] T.-W. Chin, R. Ding, C. Zhang, and D. Marculescu, “Towards ef-
ficient model compression via learned global ranking,” in CVPR ,
2020, pp. 1518–1528.
[188] R. Tiwari, U. Bamba, A. Chavan, and D. K. Gupta, “ChipNet:
Budget-aware pruning with heaviside continuous approxima-
tions,” in ICLR , 2021.
[189] O. Kovaleva, S. Kulshreshtha, and A. Rogers, et al., “BERT
Busters: Outlier dimensions that disrupt transformers,” in ACL ,
2021.
[190] G. Bai, Y. Li, C. Ling, K. Kim, and L. Zhao, “SparseLLM: Towards
global pruning for pre-trained language models,” arXiv preprint
arXiv:2402.17946 , 2024.
[191] D. H. Le and B.-S. Hua, “Network pruning that matters: a case
study on pretraing variants,” in ICLR , 2021.
[192] T. Zhang, S. Ye, K. Zhang, J. Tang, W. Wen, M. Fardad, and
Y. Wang, “A systematic DNN weight pruning framework using
alternating direction method of multipliers,” in ECCV , 2018.
[193] M. Ye, C. Gong, L. Nie, D. Zhou, A. Klivans, and Q. Liu,
“Good subnetworks provably exist: Pruning via greedy forward
selection,” in ICML , 2020.
[194] M. D. Hugo Touvron, Matthieu Cord, “Training data-efficient
image transformers & distillation through attention,” in ICMR ,
2021.
[195] S. J. Pan and Q. Yang, “A suvery on tranfer learning,” TKDE ,
vol. 22, pp. 1345–1359, 2009.
[196] T. Chen, J. Frankle, and S. Chang, et al., “The lottery ticket
hypothesis for pre-trained BERT networks,” in NeurIPS , 2020.
[197] T. Chen, J. Frankle, and S. Chan, et al., “The lottery tickets
hypothesis for supervised and self-supervised pre-training in
computer vision models,” in CVPR , 2021.
[198] E. Iofinova, A. Peste, M. Kurtz, and D. Alistarh, “How well do
sparse imagenet models transfer?” in CVPR , 2022.
[199] X. Chen, Y. Cheng, S. Wang, Z. Gan, J. Liu, and Z. Wang, “The
elastic lottery ticket hypothesis,” in NeurIPS , 2021.
[200] F. Li, G. Li1, X. He, and J. Cheng, “Dynamic dual gating neural
networks,” in ICCV , 2021.
[201] V . Sanh, T. Wolf, and A. M. Rush, “Movement pruning: Adaptive
sparsity by fine-tuning,” in NeurIPS , 2020.
[202] C. Zhang, S. Bengio, and Y. Singer, “Are all layers created equal?”
JMLR , vol. 23, 2022.
[203] R. Gong, Y. Yong, and Z. Wang, et al., “Fast and controllable post-
training sparsity-learning optimal sparsity allocation with global
constraint in minutes,” in AAAI , 2024, pp. 12 190–12 198.
[204] L. Yin, Y. Wu, and Z. Zhang, et al., “Outlier weighed layerwise
sparsity (OWL): A missing secret sauce for pruning LLMs to high
sparsity,” arXiv preprint arXiv:2310.05175 , 2024.[205] S. Yuan, E. Nie, B. Ma, and M. F ¨arber, “Why lift so heavy?
slimming large language models by cutting off the layers,” arXiv
preprint arXiv:2402.11700 , 2024.
[206] Y. Yang, Z. Cao, and H. Zhao, “LaCo: Large language model
pruning via layer collapse,” arXiv preprint arXiv:2402.11187 , 2024.
[207] S. Kalyan, S. Joshi, S. Sheik, B. U. Pedroni, and G. Cauwcnbcrghs,
“Unsupervised synaptic pruning strategies for restricted boltz-
mann machines,” in BioCAS , 2018.
[208] X. Liu, F. Zhang, and Z. Hou, et al., “Self-supervised learning:
Generative or contrastive,” TKDE , 2021.
[209] M. Caron, A. Morcos, P . Bojanowski, J. Mairal, and A. Joulin,
“Pruning convolutional neural networks with self-supervision,”
arXiv preprint arXiv:2001.03554 , 2020.
[210] S. Pan, Y. Qin, T. Li, X. Li, and L. Hou, “Momentum contrastive
pruning,” in CVPRW , 2022, pp. 2647–2656.
[211] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple
framework for contrastive learning of visual representations,” in
ICML , 2020, pp. 1597–1607.
[212] X. Chen, H. Fan, R. Girshick, and K. He, “Improved base-
lines with momentum contrastive learning,” arXiv preprint
arXiv:2003.04297 , 2020.
[213] C.-I. Jeff Lai, Y. Zhang, A. H. Liu, and S. Chang, et al., “PARP:
Prune, adjust and re-prune for self-supervised speech recogni-
tion,” in NeurIPS , 2021.
[214] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in
a neural network,” arXiv preprint arXiv:/1503.02531 , 2015.
[215] F. Tung and G. Mori, “CLIP-Q: Deep network compression learn-
ing by in-parallel pruning-quantization,” in CVPR , 2018.
[216] Y. Li, S. Gu, C. Mayer, L. V . Gool, and R. Timofte, “Group sparsity:
The hinge between filter pruning and decomposition for network
compression,” in CVPR , 2020, pp. 8018–8027.
[217] Y. Li, Y. Yu, and Q. Zhang, et al., “LoSparse: Structured com-
pression of large language models based on low-rank and sparse
approximation,” in ICML , vol. PMLR 202, 2023, pp. 20 336–20 350.
[218] X. Dong and Y. Yang, “Network pruning via transformable
architecture search,” in NeurIPS , vol. 32, 2019.
[219] A. Klein, J. Golebiowski, X. Ma, V . Perrone, and C. Archambeau,
“Structural pruning of large language models via neural archi-
tecture search,” in AutoML , 2023.
[220] Y. Liu, Z. Shu, Y. Li, Z. Lin, F. Perazzi, and S. Kung, “Content-
aware GAN compression,” in CVPR , 2021, pp. 12 156–12 166.
[221] J. Park and A. No, “Prune your model before distill it,” in ECCV ,
2022.
[222] L. Chen, Y. Chen, J. Xi, and X. Le, “Knowledge from the original
network: restore a better pruned network with knowledge distil-
lation,” Complex & Intelligent Systems , vol. 8, pp. 709–718, 2022.
[223] T. Chen, Z. Zhang, and S. Liu, et al., “Long live the lottery: the
existence of winning tickets in lifelong learning,” in ICLR , 2021.
[224] Y. Zhang, Y. Yao, P . Ram, P . Zhao, T. Chen, M. Hong, Y. Wang, and
S. Liu, “Advancing model pruning via bi-level optimization,” in
NeurIPS , 2022.
[225] W. Zou, Y. Wang, X. Fu, and Y. Cao, “Dreaming to prune image
deraining networks,” in CVPR , 2022, pp. 6023–6032.
[226] M. Wang, Y. Zhao, and J. Liu, et al., “Large multimodal model
compression via efficient pruning and distillation at AntGroup,”
arXiv preprint arXiv:2312.05795 , 2023.
[227] Y. Mao, Y. Wang, C. Wu, C. Zhang, and Y. Wang, et al., “Lad-
aBERT: Lightweight adaptation of BERT through hybrid model
compression,” in COLING , 2020, pp. 3225–3234.
[228] L. Yao, R. Pi, H. Xu, W. Zhang, Z. Li, and T. Zhang, “Joint-
DetNAS: Upgrade your detector with NAS, pruning and dy-
namic distillation,” in CVPR , 2021, pp. 1899–1908.
[229] H. Wang, S. Gui, H. Yang, J. Liu, and Z. Wang, “GAN slimming:
All-in-one GAN compression by a unified optimization frame-
work,” in ECCV , 2020.
[230] Z. Li, E. Wallace, and S. Shen, et al., “Train large, then compress:
Rethinking model size for efficient training and inference of
transformers,” in ICML , 2020.
[231] K. Han, Y. Wang, J. Guo, and E. Wu, “ParameterNet: Parameters
are all you need,” in CVPR , 2024.
[232] Y. Chen, X. Dai, and M. Liu, et al., “Dynamic convolution:
Attention over convolution kernels,” in CVPR , 2020, pp. 11 030–
11 039.
[233] Q. Yan, D. Gong, Y. Liu, A. van den Hengel, and J. Q. Shi,
“Learning bayesian sparse networks with full experience replay
for continual learning,” in CVPR , 2022.
[234] F. Corti, R. Entezari, S. Hooker, D. Bacciu, and O. Saukh, “Study-

--- PAGE 25 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 25
ing the impact of magnitude pruning on contrastive learning
methods,” in ICML , 2022.
[235] Y. Jiang, S. Wang, V . Valls, B. J. Ko, W.-H. Lee, K. K. Leung, and
L. Tassiulas, “Model pruning enables efficient federated learning
on edge devices,” TNNLS , 2022.
[236] X. Sui, Q. Lv, L. Zhi, B. Zhu, Y. Yang, Y. Zhang, and Z. Tan,
“A hardware-friendly high-precision cnn pruning method and
its fpga implementation,” Sensors , vol. 23, 2023.
[237] OpenAI, “Gpt-4 technical report,” OpenAI, Tech. Rep., 2023.
[238] H. Wang, C. Qin, Y. Bai, and Y. Fu, “Why is the state of
neural network pruning so confusing? on the fairness, compar-
ison setup, and trainability in network pruning,” arXiv preprint
arXiv:2301.05219 , 2023.
[239] X. Qian and D. Klabjan, “A probabilistic approach to neural
network pruning,” in ICML , 2021.
[240] D. Wang, L. Zhou, X. Zhang, X. Bai, and J. Zhou, “Exploring
linear relationship in feature map subspace for ConvNets com-
pression,” arXiv preprint arXiv:1803.05729 , 2018.
[241] Y. He, Y. Ding, P . Liu, L. Zhu, H. Zhang, and Y. Yang, “Learning
filter pruning criteria for deep convolutional neural networks
acceleration,” in CVPR , 2020, pp. 2009–2018.
[242] S. Hayou, J.-F. Ton, A. Doucet, and Y. W. Teh, “Robust pruning at
initialization,” in ICLR , 2021.
[243] I. Sutskever, J. Martens, G. Dahl, and G. Hinton, “On the im-
portance of initialization and momentum in deep learning,” in
ICML , 2013, pp. 1130–1147.
[244] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins,
and K. Toutanova, “BoolQ: Exploring the surprising difficulty
of natural yes/no questions,” in NAACL , 2019.
[245] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, “PIQA:
Reasoning about physical commonsense in natural language,”
inAAAI , 2020.
[246] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, “Hel-
laswag: Can a machine really finish your sentence?” in ACL , 2019.
[247] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi, “Wino-
Grande:an adversarial winograd schema challenge at scale,”
Communications of the ACM , vol. 64, pp. 99–106, 2021.
[248] Y. Zhang, M. Lin, and Y. Zhong, et al., “Lottery jackpots exist in
pre-trained models,” TP AMI , 2023.
[249] Y. Tang, Y. Wang, and Y. Xu, et al., “SCOP: Scientific control for
reliable neural network pruning,” in NeurIPS , 2020.
[250] Pau de Jorge, A. Sanyal, H. S. Behl, P . H. Torr, G. gory Rogez,
and P . K. Dokania, “Progressive skeletonization: trimming more
fat from a network at initialization,” in ICLR , 2021.
[251] C. Zheng, zheyang li, and K. Zhang, et al., “SAViT: Structure-
aware vision transformer pruning via collaborative optimiza-
tion,” in NeurIPS , 2022.
[252] H. He, J. Cai, and J. Liu, et al., “Pruning self-attentions into
convolutional layers in single path,” TP AMI , 2024.
[253] Y. Wang, D. Li, and R. Sun, “NTK-SAP: Improving neural net-
work pruning by aligning training dynamics,” in ICLR poster ,
2023.
[254] T. Narshana, C. Murti, and C. Bhattacharyya, “DFPC: Data flow
driven pruning of coupled channels without data,” in ICLR poster ,
2023.
[255] L. Cai, Z. An, C. Yang, Y. Yan, and Y. Xu, “Prior gradient mask
guided pruning-aware fine-tuning,” in AAAI , 2022.
[256] Q. Zhang, S. Zuo, and C. Liang, et al., “PLATON: Pruning large
transformer models with upper confidence bound of weight
importance,” in ICML , 2022.
[257] X. Ding, G. Ding, X. Zhou, Y. Guo, J. Han, and J. Liu, “Global
sparse momentum SGD for pruning very deep neural networks,”
inNeurIPS , 2019, pp. 6379–6391.
[258] M. Alizadeh, S. A. Tailor, and L. Zintgraf, et al., “Prospect
pruning: Finding trainable weights at initialization using meta-
gradients,” in ICLR , 2022.
[259] M. Lin, R. Ji, Y. Wang, Y. Zhang, B. Zhang, Y. Tian, and L. Shao,
“HRank: Filter pruning using high-rank feature map,” in CVPR ,
2020, pp. 1529–1538.
[260] J. Luo and J. Wu, “AutoPruner: An end-to-end trainable filter
pruning method for efficient deep model inference,” Pattern
Recognition , vol. 107, 2020.
[261] X. Wu, S. Gao, and Z. Zhang, et al., “Auto-train-once: Controller
network guided automatic network pruning from scratch,” in
CVPR , 2024.
[262] S. Girish, Shishira R Maiya, K. Gupta, H. Chen, L. Davis, and
A. Shrivastava, “The lottery ticket hypothesis for object recogni-tion,” in CVPR , 2021, pp. 762–771.
[263] Z. Hou and S.-Y. Kung, “Multi-dimensional model compression
of vision transformer,” in ICME , 2022.
[264] T. Chen, Y. Cheng, and Z. Gan, et al., “Chasing sparsity in vision
transformers: An end-to-end exploration,” in NeurIPS , 2021.
[265] S. Yu, T. Chen, and J. Shen, et al., “Unified visual transformer
compression,” in ICLR , 2022.
[266] M. Yin, B. Uzkent, and Y. Shen, et al., “GOHSP: A unified frame-
work of graph and optimization-based heterogeneous structured
pruning for vision transformer,” in AAAI , 2023.
[267] M. Zhu, Y. Tang, and K. Han, “Vision transformer pruning,” in
KDD , 2021.
[268] Z. Song, Y. Xu, and Z. He, et al., “CP-ViT: Cascade vision
transformer pruning via progressive sparsity prediction,” arXiv
preprint arXiv:2203.04570 , 2022.
[269] Y. He and J. T. Zhou, “Data-independent module-aware pruning
for hierarchical vision transformers,” in ICLR poster , 2024.
[270] O. Zafrir, A. Larey, and G. Boudoukh, et al., “Prune once for all:
Sparse pre-trained language models,” in NeurIPS workshop , 2021.
[271] R. Xu, F. Luo, C. Wang, B. Chang, J. Huang, S. Huang, and
F. Huang, “From dense to sparse: Contrastive pruning for better
pre-trained language model compression,” in AAAI , 2022.
[272] A. Jaiswal, S. Liu, and T. Chen, et al., “Instant soup: Cheap
pruning ensembles in a single pass can draw lottery tickets from
large models,” in ICML , 2023.
[273] B. Li, Z. Kong, T. Zhang, J. Li, Z. Li, H. Liu, and C. Ding,
“Efficient transformer-based large scale language representations
using hardware-friendly block structured pruning,” in EMNLP ,
2020.
[274] C. Liang, S. Zuo, and M. Chen, et al., “Super tickets in pre-
trained language models: From model compression to improving
generalization,” in ACL , 2021.
[275] J. Song, K. Oh, and T. Kim, et al., “SLEB: Streamlining llms
through redundancy verification and elimination of transformer
blocks,” arXiv preprint arXiv:2402.09025 , 2024.
[276] P . Xu, W. Shao, and M. Chen, et al., “BESA: Pruning large
language models with blockwise parameter-efficient sparsity al-
location,” arXiv preprint arXiv:2402.16880 , 2024.
[277] S. Guo, J. Xu, and L. L. Zhang, et al., “Compresso: Structured
pruning with collaborative prompting learns compact large lan-
guage models,” arXiv preprint arXiv:2310.05015 , 2023.
[278] T. Chen, T. Ding, and B. Yadav et al., “LoRAShear: Efficient large
language model structured pruning and knowledge recovery,”
arXiv preprint arXiv:2310.18356 , 2023.
[279] M. Bonnaerens, M. Freiberger, and J. Dambre, “Anchor pruning
for object detection,” Computer Vision and Image Understanding ,
vol. 221, 2022.
[280] H. Shu, Y. Wang, X. Jia, K. Han, H. Chen, C. Xu, Q. Tian,
and C. Xu, “Co-evolutionary compression for unpaired image
translation,” in ICCV , 2019.
[281] P . Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image
translation with conditional adversarial networks,” in CVPR ,
2017, pp. 1125–1134.
[282] E. Wong, F. R. Schmidt, J. Hendrik Metzen, and J. Zico Kolter,
“Scaling provable adversarial defenses,” in NIPS , 2018.
[283] N. Aghli and E. Ribeiro, “Combining weight pruning and knowl-
edge distillation for CNN compression,” in CVPRW , 2021.
[284] W. Zhang, N. Wang, K. Chen, Y. Liu, and T. Zhao, “A pruning
method for deep convolutional network based on heat map
generation metrics,” Sensors , vol. 2022, 2022.
[285] Y. Li, M. Zhu, C. Luo, H. Weng, Y. Jang, T. Wei, and S.-T.
Xia, “BAAT: Towards sample-specific backdoor attack with clean
labels,” in NeurIPS , 2022.
[286] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh, “Realtime multi-person
2D pose estimation using part affinity fields,” in CVPR , 2017.
[287] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang, “Beyond
a Gaussian denoiser: Residual learning of deep CNN for image
denoising,” TIP, vol. 26, no. 7, pp. 3142–3155, 2017.
[288] O. Ronneberger, P . Fischer, and T. Brox, “U-Net: Convolutional
networks for biomedical image segmentation,” in MICCAI , 2015.
[289] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”
Neural Computation , vol. 9, no. 8, pp. 1735–1780, 1997.
[290] H. Yu, S. Edunov, Y. Tian, and A. S. Morcos, “Playing the lottery
with rewards and multiple languages: Lottery tickets in RL and
NLP,” in ICLR , 2020.
[291] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman,
“GLUE: A multi-task benchmark and analysis platform for natu-

--- PAGE 26 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 26
ral language understanding,” in ICLR , 2019.
[292] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “BLEU: a method
for automatic evaluation of machine translation,” in ACL , 2002.
[293] Y. Goyal, T. Khot, and D. Summers-Stay, et al., “Making the V in
VQA matter: Elevating the role of image understanding in visual
question answering,” in CVPR , 2017.
[294] Y.-C. Chen, L. Li, L. Yu, A. E. Kholy, F. Ahmed, Z. Gan, Y. Cheng,
and J. Liu, “UNITER: Universal image-text representation learn-
ing,” in ECCV , 2020.
[295] S. Nrang, E. Elsen, G. Diamos, and S. Sengupta, “Exploring
sparsity in recurrent neural networks,” in ICLR , 2017.
[296] D. Amodei, R. Anubhai, J. Bai, and E. Battenberg, et al., “Deep
Speech 2: End-to-end speech recognition in English and Man-
darin,” in ICML , 2016, pp. 719–730.
[297] S. Ding, T. Chen, and Z. Wang, “Audio lottery: Speech recogni-
tion made ultra-lightweight, transferable, and noise-robust,” in
ICLR , 2022.
[298] A. Baevski, H. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0:
A framework for self-supervised learning of speech representa-
tions,” in NeurIPS , 2020.
[299] A. Conneau, A. Baevski, R. Collobert, A. Mohamed, and M. Auli,
“Unsupervised cross-lingual representation learning for speech
recognition,” in InterSpeech , 2021, pp. 2426–2430.
[300] Z. Liu, Y. Lin, and Y. Cao, et al., “Swin Transformer: Hierarchical
vision transformer using shifted windows,” in ICCV , 2021.
[301] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P . Perona, D. Ramanan,
P . Dollar, and L. Zitnick, “Microsoft COCO: Common objects in
context,” in ECCV , 2014.
[302] T.-Y. Lin, P . Goyal, R. Girshick, K. He, and P . Dollar, “Focal loss
for dense object detection,” in ICCV , 2017.
[303] M. Everingham, C. Luc Van Gool, J. Winn, and A. Zisserman,
“The PASCAL visual object classes (VOC) challenge 2007,” Foun-
dations and Trends in Machine Learning , 2008.
[304] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards
real-time object detection with region proposal networks,” in
NIPS , 2015.
[305] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu,
and Alexander C. Berg, “SSD: Single shot multibox detector,” in
ECCV , 2016, pp. 21–37.
[306] J. Ho, A. Jain, and P . Abbeel, “Denoising diffusion probabilis-
tic models,” Advances in Neural Information Processing Systems ,
vol. 33, pp. 6840––6851, 2020.
[307] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and
S. Hochreiter, “GANs trained by a two time-scale update rule
converge to a local nash equilibrium,” in NIPS , vol. 30, 2017.
[308] Z. Liu, P . Luo, X. Wang, and X. Tang, “Deep learning face
attributes in the wild,” in ICCV , 2015.
[309] R. Rombach, A. Blattmann, D. Lorenz, P . Esser, and B. Ommer,
“High-resolution image synthesis with latent diffusion models,”
inCVPR , 2022.
[310] Z. Wang, A. Bovik, H. Sheikh, and E. Simoncelli, “Image quality
assessment: from error visibility to structural similarity,” IEEE
Transactions on Image Processing , vol. 13, p. 600–612, 2004.
[311] F. Yu, A. Seff, and Y. Zhang, et al., “LSUN: Construction of a
large-scale image dataset using deep learning with humans in
the loop,” arXiv preprint arXiv:1506.03365 , 2016.
[312] J. Zhu, T. Park, P . Isola, and A. A. Efros, “Unpaired image-to-
image translation using cycle-consistent adversarial networks,”
inICCV , 2017.
[313] M. Cordts, M. Omran, S. Ramos, and T. Rehfeld, et al., “The
Cityscapes dataset for semantic urban scene understanding,” in
CVPR , 2016, pp. 3213–3223.
[314] A. Abdelhamed, S. Lin, and M. S. Brown, “A high-quality denois-
ing dataset for smartphone cameras,” in CVPR , 2018, pp. 1692–
1700.
[315] Z. Wang, A. Bovik, H. Sheikh, and E. Simoncelli, “Image quality
assessment: from error visibility to structural similarity,” IEEE
Transactions on Image Processing , vol. 13, pp. 600–612, 2004.
[316] X. Shi, X. Ning, and L. Guo, et al., “Memory-oriented structural
pruning for efficient image restoration,” in AAAI , vol. 37, 2023,
pp. 2245–2253.
[317] P . Lison, J. Tiedemann, and M. Kouylekov, “OpenSubtitles2018:
Statistical rescoring of sentence alignments in large, noisy parallel
corpora,” in LREC , 2018.
[318] M. Zhu and S. Gupta, “To prune, or not to prune: exploring the
efficacy of pruning for model compression,” in ICLRW , 2018.
[319] R. Socher, A. Perelygin, and J. Wu, et al., “Recursive deep modelsfor semantic compositionality over a sentiment treebank,” in
EMNLP , 2013.
[320] P . Rajpurkar, J. Zhang, K. Lopyrev, and P . Liang, “SQuAD:
100,000+ questions for machine comprehension of text,” in
EMNLP , 2016.
[321] I. Shankar, D. Nikhil, and C. Kornel, “First quora dataset release:
Question pairs,” 2017. [Online]. Available: https://quoradata.qu
ora.com/First-Quora-Dataset-Release-Question-Pairs
[322] A. Williams, N. Nangia, and S. Bowman, “A broad-coverage
challenge corpus for sentence understanding through inference,”
inNAACL , 2018, pp. 1112–1122.
[323] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer sentinel
mixture models,” in ICLR , 2017.
[324] A. Rousseau, P . Del ´eglise, and Y. Esteve, “TED-LIUM: an auto-
matic speech recognition dedicated corpus,” in LREC , 2012.
[325] R. Ardila, M. Branson, K. Davis, M. Kohler, J. Meyer, M. Henretty,
R. Morais, L. Saunders, F. Tyers, and G. Weber, “Common Voice:
A massively multilingual speech corpus,” in LREC , 2020.
[326] H. Jiang, L. L. Zhang, and Y. Li, et al., “Accurate and structured
pruning for efficient automatic speech recognition,” in INTER-
SPEECH , 2023.
[327] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-
rispeech: An ASR corpus based on public domain audio books,”
inICASSP , 2015, pp. 5206–5210.
[328] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han,
S. Wang, Z. Zhang, and Y. Wu, “Conformer: Convolution-
augmented transformer for speech recognition,” in Interspeech ,
2020.
[329] A. Radford, J. W. Kim, and C. Hallacy, et al., “Learning transfer-
able visual models from natural language supervision,” in ICML ,
2021.
[330] J. Li, D. Li, C. Xiong, and S. Hoi, “BLIP: Bootstrapping language-
image pre-training for unified vision-language understanding
and generation,” arXiv preprint arXiv:2201.12086 , 2022.
[331] Y.-L. Sung, J. Yoon, and M. Bansal, “ECoFLaP: Efficient coarse-
to-fine layer-wise pruning for vision-language models,” in ICLR ,
2024.
[332] P . Young, A. Lai, and M. Hodosh, et al., “From image descrip-
tions to visual denotations: New similarity metrics for semantic
inference over event descriptions,” in ACL , 2014.

--- PAGE 27 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 27
TABLE 13: A summary of representative pruning methods for CNNs in image classification. “U/S” denotes unstructured
and structured pruning, respectively. “Ratio” and “FLOPs” refer to reduced percentages of parameters and FLOPs,
respectively. “Original” and “Pruned” represent Top-1 accuracy of the original and pruned networks, respectively. Entries
marked with an asterisk (*) denote Top-5 accuracy. “ †” denotes MACs. If a second citation is present, it indicates the
reported results. “-” means the corresponding result is not reported.
Method U/SOne-shot Local Data-driven Pre-trained Post-trainingModelRatio FLOPs ImageNet
(Y/N) (Y/N) (Y/N) (Y/N) (Y/N) (%) (%) Original (%) Pruned (%) ∆↓
SNIP (2019) [54] U Y N Y N - ResNet-50 80.00 - 76.15 69.67 6.48
SNIP (2019) [54] [253] U Y N Y N - ResNet-50 89.26 - 76.20 60.98 15.22
SNIP (2019) [54] [253] U Y N Y N - ResNet-50 95.60 - 76.20 40.69 35.51
SNIP (2019) [54] [250] U Y N Y N - ResNet-50 90.00 - 75.60 61.50 14.10
SNIP (2019) [54] [250] U Y N Y N - ResNet-50 95.00 - 75.60 44.30 31.30
GSM (2019) [257] U N N Y Y N ResNet-50 75.00 - 75.72 75.33 0.39
GSM (2019) [257] U N N Y Y N ResNet-50 80.00 - 75.72 74.30 1.42
Iterative-SNIP (2021) [250] U N N Y N - ResNet-50 90.00 - 75.60 63.70 11.90
Iterative-SNIP (2021) [250] U N N Y N - ResNet-50 95.00 - 75.60 54.70 30.90
GraSP (2020) [56] U Y N Y N - ResNet-50 80.00 - 76.15 72.06 4.09
GraSP (2020) [56] [253] U Y N Y N - ResNet-50 89.26 - 76.20 67.74 8.46
GraSP (2020) [56] [253] U Y N Y N - ResNet-50 95.60 - 76.20 59.73 16.47
SynFlow (2020) [50] [253] U N N N N - ResNet-50 89.26 - 76.20 66.81 9.39
SynFlow (2020) [50] [253] U N N N N - ResNet-50 95.60 - 76.20 58.88 17.32
RigL (2020) [81] U Y Y Y N - ResNet-50 80.00 - 76.15 74.60 1.55
FORCE (2021) [250] U N N Y N - ResNet-50 90.00 - 75.60 64.90 10.70
FORCE (2021) [250] U N N Y N - ResNet-50 95.00 - 75.60 59.00 16.60
ProsPr (2022) [258] U Y N Y N - ResNet-50 90.00 - 75.60 66.86 8.74
ProsPr (2022) [258] U Y N Y N - ResNet-50 95.00 - 75.60 59.62 15.98
NTK-SAP (2023) [253] U N N N N - ResNet-50 89.26 - 76.20 68.28 7.92
NTK-SAP (2023) [253] U N N N N - ResNet-50 95.60 - 76.20 60.79 15.41
Jackpot (2023) [248] U Y Y Y Y N ResNet-50 80.00 - 76.15 75.71 0.44
Jackpot (2023) [248] U Y Y Y Y N ResNet-50 90.00 - 76.15 73.04 3.11
FCPTS (2024) [203] U Y Y Y Y N ResNet-50 50.00 - 77.89 77.43 0.46
ThiNet (2017) [16] S N N Y Y Y ResNet-50 33.72 36.79 75.30 74.03 1.27
ThiNet (2017) [16] S N N Y Y Y ResNet-50 51.56 55.83 75.30 72.03 3.27
ThiNet (2017) [16] S N N Y Y Y ResNet-50 66.12 71.50 75.30 68.17 7.13
CP (2017) [134] [62] S N N Y Y Y ResNet-50 - 50.11 76.13 75.06 1.07
CP (2017)-train-from-scratch [134] S N N Y Y N ResNet-50 - 50.00 92.20∗88.20∗4.00
CP (2017)-finetuned [134] S N N Y Y Y ResNet-50 - 50.00 92.20∗90.80∗1.40
SFP (2018) [83] S Y Y Y N - ResNet-50 - 41.80 76.15 74.61 1.54
SSS (2018) [59] S N N Y N - ResNet-50 0.78 15.06 76.12 75.44 0.68
SSS (2018) [59] S N N Y N - ResNet-50 27.06 31.08 76.12 74.18 1.94
IE (2019) [106] S N N Y Y Y ResNet-50 - 20.03 76.18 76.43 -0.25
IE (2019) [106] S N N Y Y Y ResNet-50 - 44.97 76.18 74.50 1.68
IE (2019) [106] S N N Y Y Y ResNet-50 - 67.23 76.18 71.69 4.49
GBN (2019) [15] S N N Y Y Y ResNet-50 - 40.57 75.85 76.19 -0.34
Meta (2019) [84] S Y N Y Y N ResNet-50 - 26.83 76.60 76.20 0.40
FPGM-prune-from-scratch (2019) [100] S Y Y Y N - ResNet-50 - 42.20 76.15 75.03 1.12
FPGM-finetuned (2019) [100] S Y Y Y Y Y ResNet-50 - 42.20 76.15 75.59 0.56
FPGM-prune-from-scratch (2019) [100] S Y Y Y N - ResNet-50 - 53.50 76.15 74.13 2.02
FPGM-finetuned (2019) [100] S Y Y Y Y Y ResNet-50 - 53.50 76.15 74.83 1.32
PFS (2020) [70] S N N Y N - ResNet-50 16.74 26.83 77.20 76.70 0.50
PFS (2020) [70] S N N Y N - ResNet-50 71.16 51.22 77.20 75.60 1.60
HRank (2020) [259] S Y Y Y Y Y ResNet-50 36.70 43.70 76.15 74.98 1.17
SCOP-A (2020) [249] S Y Y Y Y Y ResNet-50 42.80 45.30 76.15 75.95 0.20
SCOP-B (2020) [249] S Y Y Y Y Y ResNet-50 51.80 54.60 76.15 75.26 0.89
AutoPruner (2020) [260] S N N Y Y Y ResNet-50 - 51.10 76.15 74.76 1.39
AutoPruner (2020) [260] S N N Y Y Y ResNet-50 - 66.01 76.15 73.05 3.10
LFPC (2020) [241] S Y Y Y Y Y ResNet-50 - 25.17 76.79 76.95 -0.16
DSA (2020) [85] S Y Y Y N - ResNet-50 - 40.00 76.02 75.10 0.92
DSA (2020) [85] S Y Y Y N - ResNet-50 - 50.00 76.02 74.69 1.33
DMCP-train from scratch (2020) [102] S N Y Y N - ResNet-50 - 31.70 76.60 77.00 -0.40
DMCP-train-from-scratch (2020) [102] S N Y Y N - ResNet-50 - 46.34 76.60 76.20 0.40
GFP (2021) [62] S N N Y Y Y ResNet-50 - 25.17 76.79 76.95 -0.16
GFP (2021) [62] S N N Y Y Y ResNet-50 - 50.11 76.79 76.42 0.37
GFP (2021) [62] S N N Y Y Y ResNet-50 - 75.06 76.79 73.94 2.85
Greg-2 (2021) [108] S N Y Y Y Y ResNet-50 - 32.93 76.13 75.36 0.77
PGMPF (2022) [255] S N Y Y Y Y ResNet-50 - 53.50 76.01 75.11 0.90
DFPC (2023) [254] S N N N Y Y ResNet-50 45.65 49.49 76.10 75.90 1.20
DFPC (2023) [254] S N N N Y Y ResNet-50 62.26 71.10 76.10 73.80 2.30
PDP (2023) [86] S N N Y N - ResNet-50 - 54.90†76.10 75.90 0.20
PPSM (2023) [103] S N N Y Y Y ResNet-50 - 53.07 76.13 75.78 0.35
PPSM (2023) [103] S N N Y Y Y ResNet-50 - 65.91 76.13 75.43 0.70
DepGraph (2023) [38] S Y N Y Y Y ResNet-50 - 51.82†76.15 75.83 0.32
ATO (2024) [261] S N N Y N - ResNet-50 - 55.20 76.13 76.59 -0.46
ATO (2024) [261] S N N Y N - ResNet-50 - 61.70 76.13 76.07 0.06
ATO (2024) [261] S N N Y N - ResNet-50 - 71.00 76.13 74.77 1.36
SSS (2018) [59] S N N N Y N ResNet-101 - 55.52 76.40 75.44 0.96
SFP (2018) [83] S Y Y Y N - ResNet-101 - 42.20 77.37 77.03 0.34
IE (2019) [106] S N N Y Y Y ResNet-101 30.20 39.74 77.37 77.35 0.02
FPGM-finetuned (2019) [100] S Y Y Y Y Y ResNet-101 - 42.20 77.37 77.32 0.05
SCOP-A (2020) [249] S Y Y Y Y Y ResNet-101 46.80 48.60 77.37 77.75 -0.48
SCOP-B (2020) [249] S Y Y Y Y Y ResNet-101 57.80 60.20 77.37 77.36 0.01
GFP (2021) [62] S N N Y Y Y ResNet-101 39.63 50.02 78.29 78.30 -0.01

--- PAGE 28 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 28
TABLE 14: A summary of representative pruning methods in object detection. “O-mAP” and “P-mAP” represent mAP of
the original and pruned networks, respectively. “Ratio” and “FLOPs” have the same meaning as in Table 13.
Method U/SOne-shot Local Pre-trained Post-trainingModelRatio FLOPs PASCAL VOC COCO
(Y/N) (Y/N) (Y/N) (Y/N) (%) (%) O-mAP P-mAP ∆↓ O-mAP P-mAP ∆↓
LTH (2021) [262] U N N Y N Faster-RCNN (ResNet-18) 78.94 - 69.74 68.47 1.27 - - -
CP (2017) [134] S N N Y Y Faster R-CNN (VGG-16) - 50.00 68.70 68.30 0.40 - - -
GFP (2021) [62] S N N Y Y Faster R-CNN - 50.00 - - - 37.40 37.80 -0.40
GFP (2021) [62] S N N Y Y Faster R-CNN - 75.00 - - - 37.40 36.60 0.80
SAViT (2022) [251] S Y N Y Y Faster R-CNN (Swin-T) 57.52 69.42 - - - 45.50 45.20 0.30
FCPTS (2024) [203] U Y Y Y N MobileNetV1 SSD 90.00 - 67.70 65.10 2.60 - - -
FCPTS (2024) [203] U Y Y Y N MobileNetV2 SSD-Lite 90.00 - 68.60 59.10 9.50 - - -
GFP (2021) [62] S N N Y Y RetinaNet 31.03 50.02 - - - 36.50 36.80 -0.30
MDC (2022) [263] S Y Y Y Y DeiT-Base - 60.00 81.80 81.50 0.30 - - -
TABLE 15: A summary of representative pruning methods for ViTs in image classification.
Method U/SOne-shot Local Data-driven Pre-trained Post-trainingModelRatio FLOPs ImageNet
(Y/N) (Y/N) (Y/N) (Y/N) (Y/N) (%) (%) Original (%) Pruned (%) ∆↓
SViTE-T (2021) [264] U N Y Y N - DeiT-Tiny 29.72 25.56 72.20 71.78 0.42
SViTE-T (2021) [264] U N Y Y N - DeiT-Tiny 39.51 34.16 72.20 71.75 0.45
S2ViTE-T [264] (2021) S N Y Y N - Deit-Tiny 26.40 23.69 72.20 70.12 2.08
UVC-T (2022) [265] S N N Y Y Y DeiT-Tiny - 50.77 72.20 71.30 0.90
UVC-T (2022)[265] S N N Y Y Y DeiT-Tiny - 60.88 72.20 70.60 1.60
SAViT-T (2022) [251] S Y N Y Y Y DeiT-Tiny 26.32 30.77 72.20 70.72 1.48
GOHSP-T (2023) [266] S Y N Y Y Y DeiT-Tiny 29.82 30.00 72.20 70.24 1.96
X-Pruner-T (2023) [104] S N N Y Y N DeiT-Tiny - 50.80 72.20 71.10 1.10
SPViT-T (2024) [252] S N N Y Y Y Deit-Tiny 15.79 23.08 72.20 70.70 1.50
SViTE-S (2021)[264] U N Y Y N - DeiT-Small 39.82 36.73 79.90 80.26 -0.36
SCOP-S (2020) [249] S Y Y Y Y Y DeiT-Small - 43.48 79.80 77.50 2.30
S2ViTE-S (2021) [264] S N Y Y N - DeiT-Small 33.94 31.63 79.90 79.22 0.68
UVC-S (2022) [265] S N N Y Y Y DeiT-Small - 49.59 79.80 78.82 0.98
SAViT-S (2022) [251] S Y N Y Y Y DeiT-Small 33.48 32.61 79.85 80.11 -0.26
NViT-S (2023) [60] S N N Y Y N DeiT-Small 4.55 8.70 81.20 82.19 -0.99
NViT-S-ASP (2023) [60] S N N Y Y Y DeiT-Small 52.27 8.70 81.20 82.19 -0.99
GOHSP-S (2023) [266] S Y N Y Y Y DeiT-Small 34.84 35.00 79.90 79.98 -0.08
GOHSP-S (2023) [266] S Y N Y Y Y DeiT-Small 49.77 39.00 79.90 79.86 0.04
X-Pruner-S (2023) [104] S N N Y Y N DeiT-Small - 47.90 79.80 78.93 0.87
SPViT-S (2024) [252] S N N Y Y Y DeiT-Small 28.26 28.05 79.90 78.30 1.60
SViTE-B (2021) [264] U N Y Y N - Deit-Base 39.95 38.30 81.80 81.56 0.24
S2ViTE-B (2021) [264] S N Y Y N - Deit-Base 34.41 33.13 81.80 82.22 -0.42
SCOP-B (2020) [249] S Y Y Y Y Y DeiT-Base - 41.70 81.80 79.70 2.10
VTP-B (2021)[267] S Y N Y Y Y DeiT-Base 22.11 21.59 81.80 81.30 0.50
VTP-B (2021)[267] S Y N Y Y Y DeiT-Base 44.44 43.18 81.80 80.70 1.10
UVC-B (2022)[265] S N N Y Y Y DeiT-Base - 54.50 81.80 80.57 1.23
SAViT-B (2022)[251] S Y N Y Y Y DeiT-Base 70.67 69.89 81.84 81.66 0.18
CP-ViT-B (2022) [268]-w/o finetune S N Y Y Y N DeiT-Base 30 22.16 81.82 80.91 0.91
CP-ViT-B (2022) [268]-w/o finetune S N Y Y Y N DeiT-Base 40 30.67 81.82 80.31 1.51
CP-ViT-B (2022) [268]-w/ finetune S N Y Y Y Y DeiT-Base 30 22.62 81.82 81.66 0.16
CP-ViT-B (2022) [268]-w/ finetune S N Y Y Y Y DeiT-Base 40 32.41 81.82 81.52 0.30
CP-ViT-B (2022) [268]-w/ finetune S N Y Y Y Y DeiT-Base 50 41.62 81.82 81.13 0.69
WDPruning-B (2022)[159] S N Y Y Y Y DeiT-Base - 43.40 81.80 80.76 1.04
MDC-B (2022) [263] S Y Y Y Y Y DeiT-Base - 54.29 81.80 82.30 -0.50
MDC-B (2022) [263] S Y Y Y Y Y DeiT-Base - 60.00 81.80 81.50 0.30
X-Pruner-B (2023) [104] S N N Y Y N DeiT-Base - 51.50 81.80 81.02 0.78
NViT-B (2023)[60] S N N Y Y N DeiT-Base 60.47 61.36 83.36 83.29 0.07
NViT-B-ASP (2023)[60] S N N Y Y Y DeiT-Base 80.23 61.36 83.36 83.29 0.07
SPViT-B (2024) [252] S N N Y Y Y Deit-Base 52.00 51.85 81.80 81.50 0.30
PLATON-B (2022) [256] U N N Y Y Y ViT-Base 60 - 83.50 82.60 0.90
FCPTS-B (2024) [203] U Y Y Y Y N ViT-Base 50 - 75.68 74.90 0.78
FCPTS-B (2024) [203] U Y Y Y Y N ViT-Base 60 - 75.68 72.09 3.60
CP-ViT-B (2022) [268]-w/o finetune S N Y Y Y N ViT-Base 30 23.02 77.91 76.77 1.14
CP-ViT-B (2022) [268]-w/o finetune S N Y Y Y N ViT-Base 40 32.34 77.91 75.09 2.82
CP-ViT-B (2022) [268]-w/ finetune S N Y Y Y Y ViT-Base 30 24.91 77.91 77.75 0.16
CP-ViT-B (2022) [268]-w/ finetune S N Y Y Y Y ViT-Base 40 33.62 77.91 77.36 0.55
CP-ViT-B (2022) [268]-w/ finetune S N Y Y Y Y ViT-Base 50 46.34 77.91 76.75 1.16
DepGraph-B (2023) [38] S Y N Y Y Y ViT-Base - 40.91†81.07 79.17 1.90
DIMAP-T (2023) [269] U Y N N Y Y Swin-Tiny - 32.40 81.16 81.11 0.05
DIMAP-T (2023) [269] U Y N N Y Y Swin-Tiny - 50.80 81.16 80.35 0.81
X-Pruner-T (2023) [104] S N N Y Y N Swin-Tiny - 28.90 81.20 80.70 0.50
DIMAP-S (2023) [269] U Y N N Y Y Swin-Small - 33.20 83.19 82.99 0.20
DIMAP-S (2023) [269] U Y N N Y Y Swin-Small - 52.30 83.19 82.63 0.56
WDPruning-S (2022)[159] S N Y Y Y Y Swin-Small - 27.58 83.00 81.80 1.20
X-Pruner-S (2023) [104] S N N Y Y N Swin-Samll - 27.60 83.20 82.00 1.20
DIMAP-B (2023) [269] U Y N N Y Y Swin-Base - 33.50 83.48 83.43 0.05
DIMAP-B (2023) [269] U Y N N Y Y Swin-Base - 52.70 83.48 83.28 0.20
TABLE 16: A summary of representative pruning methods for BERTs.
Method U/SOne-shot Local Pre-trained Post-trainingModelRatio FLOPs Dataset
(Y/N) (Y/N) (Y/N) (Y/N) (%) (%) QQP MNLI QNLI SST-2 SQuAD1.1
LadaBERT (2020) [227] U Y N Y Y BERT base 60.00 - 71.20/70.70 84.60/83.50 90.50/89.60 93.50/92.80 -
Prune OFA (2021) [270] U N N Y Y BERT base 85.00 - 91.20/90.69 84.06/81.67 91.16/89.95 92.13/91.34 80.80/78.59
PLATON-unstruct (2022) [256] U N N Y Y BERT base 50.00 - - - - - 80.40/78.50
PLATON-unstruct (2022) [256] U N N Y Y BERT base 60.00 - - - - - 80.40/78.00
PLATON-unstruct (2022) [256] U N N Y Y BERT base 80.00 - 91.50/90.70 84.60/83.10 91.30/90.10 92.70/91.30 -
CAP-m (2022) [271] U N Y/N Y Y BERT base 90.00 - 90.90/90.70 84.50/81.00 - 92.90/- 80.70/76.50
oBERT (2022) [167] U N N Y Y BERT base 90.00 - 91.06/90.99 84.54/83.40 - 91.25/89.97 -
ISP (2023) [272] U N N Y Y BERT base 70.00 - - 82.40/82.71 89.10/90.06 - -
BSP (2020)[273] S Y Y Y Y BERT base 30.00 - 91.20/90.70 84.60/82.90 90.50/88.20 93.50/89.30 -
SuperTicket (2021) [274] S Y N Y Y BERT base 13.20 - 91.30/88.30 84.50/84.50 -/91.30 - -
PLATON-struct (2022) [256] S N N Y Y BERT base 50.00 - - - - - 80.4/77.00
PLATON-struct (2022) [256] S N N Y Y BERT base 60.00 - - - - - 80.4/75.60
Mask-Tuning (2022) [145] S N Y Y N BERT base - 40.00 91.00/90.38 84.53/82.26 91.41/90.00 93.57/92.47 -
CAP-f (2022) [271] S N Y/N Y Y BERT base 90.00 - 90.90/90.20 84.50/81.00 - 92.90/89.70 80.70/70.20
KCM (2023) [131] S Y Y Y N BERT base - 30.00 91.00/90.39 84.53/81.18 91.41/90.58 93.57/92.26 -
KCM (2023) [131] S Y Y Y N BERT base - 40.00 91.00/89.15 84.53/77.24 91.41/87.79 93.57/91.11 -

--- PAGE 29 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 29
TABLE 17: A summary of representative pruning methods for LLMs. “ †” indicates the sequence length for inference is
2048, otherwise it is 128. If a second citation is present, it references the reported results. “-” means the corresponding
result is not reported or N/A.
Method U/SOne-shot Local Post-training Pre-trained Ratio Prediction ↓ Common Sense ↑MMLU ↑(Y/N) (Y/N) (Y/N) Model (%) WikiText2 PTB BoolQ PIQA Helleswag WinoGrande
Dense [20] - - - - LLaMA-7B 0.00 12.62/ 5.68†22.14 73.18 78.35 72.99 67.01 34.43 [49]
Dense [49] - - - - LLaMA-2-7B 0.00 12.62 22.14 77.74 - 57.17 68.90 52.08
Dense [190] - - - - LLaMA-2-13B 0.00 4.88†50.94†77.89 - 59.94 72.77 -
Dense [275] - - - - OPT-6.7B 0.00 10.86†- - 76.39 67.16 65.19 -
Dense [190] - - - - OPT-30B 0.00 9.56†14.04†70.46 - 54.27 69.02 -
Dense [133] - - - - Vicuna-7B 0.00 17.10 63.20 78.10 77.30 73.90 69.50 -
SparseGPT (2023) [17] [49] U Y Y N LLaMA-7B 50.00 7.22†- 75.02 - 52.37 69.85 34.43
SparseGPT (2023) [17] [204] U Y Y N LLaMA-7B 70.00 26.30†- 64.53 - 42.11 58.64 -
Wanda (2024) [49] U Y Y N LLaMA-7B 50.00 7.26†- 71.22 - 51.85 66.06 33.49
Wanda (2024) [49] [204] U Y Y N LLaMA-7B 70.00 85.77†- 55.11 - 31.83 51.38 -
BESA (2024) [276] U Y Y N LLaMA-7B 50.00 6.86†66.96†72.17 76.66 54.31 67.64 -
OWL w. SparseGPT (2024) [204] U Y Y N LLaMA-7B 70.00 19.49†- 67.13 - 48.56 62.03 -
OWL w. Wanda (2024) [204] U Y Y N LLaMA-7B 70.00 24.55†- 62.48 - 44.79 58.72 -
LLM-Pruner (2023) [20] S Y Y Y LLaMA-7B 20.00 17.58 30.11 64.62 77.20 68.80 63.14 -
LLM-Pruner (2023) [20] S Y Y Y LLaMA-7B 50.00 38.12/ 16.41†66.35 60.28 69.31 47.06 53.43 -
LoRAPruner (2023) [182] S N Y Y LLaMA-7B 20.00 16.80 28.75 65.62 79.31 70.00 62.76 -
LoRAPruner (2023) [182] S N Y Y LLaMA-7B 50.00 30.12/ 11.60†50.30 61.88 71.53 47.86 55.01 -
Compresso (2023) [277] S N N N LLaMA-7B 28.57 - - 73.55 73.07 49.16 64.80 27.68
Compresso (2023) [277] S N N N LLaMA-7B 35.71 - - 68.69 72.85 47.18 63.38 25.92
LoRAShear (2023) [278] S N N Y LLaMA-7B 20.00 - - 72.78 76.36 69.49 67.63 -
LoRAShear (2023) [278] S N N Y LLaMA-7B 50.00 - - 63.40 72.15 49.83 56.40 -
Bonsai (2024) [132] S N N Y LLaMA-7B 50.00 10.92†- 67.22 - 43.09 61.64 28.92
Shortened (2024) [133] S Y N Y LLaMA-7B 20.00 17.70 30.70 72.70 75.70 70.04 63.60 -
FLAP (2024) [146] S N N N LLaMA-7B 20.00 14.62 - 69.63 76.82 71.20 68.35 -
SliceGPT (2024) [19] [275] S Y N N LLaMA-7B 25.00 - - - 66.87 54.16 63.38 -
SliceGPT (2024) [19] [275] S Y N N LLaMA-7B 30.00 - - - 63.55 49.62 61.33 -
SLEB (2024) [275] S N N N LLaMA-7B 10.00 - - - 76.44 70.23 63.14 -
SLEB (2024) [275] S N N N LLaMA-7B 20.00 12.94†- - 73.07 62.47 58.96 -
SparseGPT (2023) [17] [49] U Y Y N LLaMA-2-7B 50.00 6.52†- 75.02 - 52.37 69.85 38.68
Wanda (2024) [49] U Y Y N LLaMA-2-7B 50.00 6.44†- 75.99 - 52.49 68.19 39.27
BESA (2024) [276] U Y Y N LLaMA-2-7B 50.00 6.60†44.09†74.83 76.66 54.60 68.59 -
LLM-Pruner (2023) [20] [65] S Y Y Y LLaMA-2-7B 27.00 - - 55.20 71.22 56.46 - 23.33
SliceGPT-Alpaca (2024) [19] S Y N N LLaMA-2-7B 20.00 - - - 76.50 65.20 65.51 -
SliceGPT-Alpaca (2024) [19] S Y N N LLaMA-2-7B 30.00 - - - 72.25 55.86 59.83 -
SliceGPT-Wiki (2024) [19] S Y N N LLaMA-2-7B 20.00 6.64†- - 69.42 59.04 65.11 -
SliceGPT-Wiki (2024) [19] S Y N N LLaMA-2-7B 30.00 8.12†- - 63.55 49.62 61.33 -
LLM Surgeon (2024) [164] S N N N LLaMA-2-7B 50.00 43.68 - 39.60 64.36 40.29 52.57 -
K-OBD (2024) [164] S N N N LLaMA-2-7B 50.00 136.33 - 61.56 60.66 36.84 53.04 -
Sheared (2024) [137] S N N Y LLaMA-2-7B 61.00 - - 73.70 75.80 70.80 64.20 26.40
Sheared (2024) [137] S N N Y LLaMA-2-7B 81.00 - - 64.00 73.40 60.70 57.90 25.70
LaCo (2024) [206] S Y N N LLaMA-2-7B 27.10 - - 64.07 69.80 55.69 - 26.45
ShortGPT (2024) [65] S Y N N LLaMA-2-7B 27.10 - - 74.71 66.43 53.02 - 43.96
LLM-Streamline (2024) [109] S Y N Y LLaMA-2-7B 26.20 - - 65.00 70.10 59.20 - 47.00
SparseGPT (2023) [17] [190] U Y Y N LLaMA-2-13B 70.00 12.98†267 .63†70.03 - 42.20 66.54 -
Wanda (2024) [49] [190] U Y Y N LLaMA-2-13B 50.00 23.42†502 .53†- - - - -
SparseLLM (2024) [190] U Y N Y LLaMA-2-13B 70.00 12.95†277 .76†69.87 - 42.50 68.64 -
LLM-Pruner (2023) [20] [65] S Y Y Y LLaMA-2-13B 24.40 - - 56.42 76.66 67.76 - 25.21
SliceGPT-Alpaca (2024) [19] S Y N N LLaMA-2-13B 20.00 - - 77.97 69.64 68.90 -
SliceGPT-Alpaca (2024) [19] S Y N N LLaMA-2-13B 30.00 - - 74.10 60.91 65.82 -
SliceGPT-Wiki (2024) [19] S Y N N LLaMA-2-13B 20.00 5.81†- - 71.87 63.04 69.38 -
SliceGPT-Wiki (2024) [19] S Y N N LLaMA-2-13B 30.00 6.99†- - 66.10 52.69 65.11 -
LaCo (2024) [206] S Y N N LLaMA-2-13B 24.60 - - 63.98 74.27 64.39 - 45.93
ShortGPT (2024) [65] S Y N N LLaMA-2-13B 24.60 - - 62.48 73.45 66.64 - 54.69
SparseGPT (2023) [17] U Y Y N OPT-6.7B 50.00 11.55†17.44†- - - - -
SliceGPT-Alpaca (2024) [19] S Y N N OPT-6.7B 20.00 - - - 74.54 62.84 62.67 -
SliceGPT-Alpaca (2024) [19] S Y N N OPT-6.7B 30.00 - - - 73.34 58.93 61.80 -
SliceGPT-Wiki (2024) [19] S Y N N OPT-6.7B 20.00 11.48†- - 72.74 61.04 61.09 -
SliceGPT-Wiki (2024) [19] S Y N N OPT-6.7B 30.00 12.51†- - 68.61 54.56 60.69 -
SLEB (2024) [275] S N N N OPT-6.7B 10.00 11.22†- - 76.61 66.36 64.72 -
SLEB (2024) [275] S N N N OPT-6.7B 20.00 12.94†- - 74.92 62.13 61.33 -
LLM-Streamline (2024) [109] S Y N Y OPT-6.7B 26.00 - - 63.00 73.10 52.40 - 24.60
SparseGPT (2023) [17] [190] U Y Y N OPT-30B 70.00 9.58†14.41†68.78 - 53.83 67.64 -
Wanda (2024) [49] [190] U Y Y N OPT-30B 70.00 7766 .61†5547 .45†- - - - -
SparseLLM (2024) [190] U Y N Y OPT-30B 70.00 9.56†14.40†69.11 - 53.97 68.43 -
SliceGPT-Alpaca (2024) [19] S Y N N OPT-30B 20.00 - - - 78.35 70.64 66.61 -
SliceGPT-Alpaca (2024) [19] S Y N N OPT-30B 30.00 - - - 76.93 68.66 64.96 -
SliceGPT-Wiki (2024) [19] S Y N N OPT-30B 20.00 9.87†- - 76.50 70.61 66.61 -
SliceGPT-Wiki (2024) [19] S Y N N OPT-30B 30.00 10.27†- - 74.97 68.15 65.04 -
SLEB (2024) [275] S N N N OPT-30B 10.00 9.57†- - 77.64 72.32 68.75 -
SLEB (2024) [275] S N N N OPT-30B 20.00 10.73†- - 76.93 70.62 67.40 -
LLM-Pruner (2023) [20] S Y Y Y Vicuna-7B 20.00 19.69 78.25 63.33 76.17 65.13 60.22 -
LLM-Pruner (2023) [20] [133] S Y Y Y Vicuna-7B 20.00 19.60 76.40 65.40 76.20 68.90 64.40 -
LLM-Pruner (2023) [20] [133] S Y Y Y Vicuna-7B 35.00 27.60 102.00 52.00 72.40 61.60 59.90 -
Shortened (2024) [133] S Y N Y Vicuna-7B 20.00 18.80 67.90 71.70 74.40 67.60 63.60 -
Shortened (2024) [133] S Y N Y Vicuna-7B 35.00 26.60 89.40 65.20 70.40 56.50 56.60 -
FLAP (2024) [146] [133] S N N N Vicuna-7B 20.00 22.00 74.90 73.10 74.80 67.90 65.80 -
FLAP (2024) [146] [133] S N N N Vicuna-7B 35.00 26.60 89.40 65.20 70.40 56.50 56.50 -

--- PAGE 30 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 30
TABLE 18: Summary of commonly used pruning evaluation metrics for various applications.
Task Type Dataset Model Performance Efficiency Example work
Image
CVCIFAR-10/100 [177], ResNet-32/50/56 [9], Top-1/Top-5 accuracy FLOPs, [48], [259],
Classification ImageNet ILSVRC-2012 [181] VGG-16/19 [1], MACs, [67], [62],
ViT [2]/DeiT [194] /Swin [300] ∥W∥0 [99], [50]
Object COCO [301], RetinaNet [302], mAP , FLOPs, [62], [279],
Detection PASCAL VOC 2007/2010 [303], Faster R-CNN [304], COCO mAP ∥W∥0 [262]
SSD [305]
Generative tasks CIFAR-10 [177] DDPMs [306] FID [307] MACs [39]
CelebA-HQ (64 ×64) [308] LDMs [309] SSIM [310]
LSUN Church (256 ×256)) [311]
Image Style horse2zebra [312], CycleGAN [312] FCN-scores [281], FLOPs, [280], [229]
Translation summer2winter [312], FID [307] ∥W∥0
Cityscapes [313]
Adversarial CIFAR-10 [177], VGG-16 [1], Top-1/Top-5 accuracy, ∥W∥0 [18, 156]
Robustness ImageNet ILSVRC-2012 [181] ResNet-18/50 [9], Benign accuracy,
Wide-ResNet-28-5 ERA/VRA
Image Denoising U-Net [288] SIDD [314] PSNR and SSIM [315] ∥W∥0 [316]
Human-pose COCO [301] CMU-pose [286] mAP [240]
Estimation
Machine
NLPWMT’14/16, Transformer-based [66], BLEU [292], FLOPs, [290], [154],
Translation OpenSubtitles2018 [317] LSTMs [289] ∥W∥0 [318], [169]
Natural Language SST-2 [319], QNLI [320] BERTs [3], Accuracy, ∥W∥0 [196], [201],
Understanding SQuAD [320], QQP [321], LLaMA [178] Pearson/Matthew cor., [20, 182]
MNLI [322] OPT [175] F1 [17, 19]
Language Wikitext-2 [323], LSTMs [289], Perplexity ∥W∥0 [290], [318]
Modeling Penn Tree Bank [179] LLaMA [178] [20],[137]
Speech Recognition ASPTED-LIUM [324], wav2vec 2.0 [298], WER, ∥W∥0 [295], [297],
Common Voice [325], LSTMs [289], CER [213, 326]
LibriSpeech [327] Conformer [328]
Image-Text Retrieval
VLCOCO [301] CLIP [329], BLIP [330] TR@1, IR@1 FLOPs [130, 331]
Flickr30K [332] ∥W∥0
Visual Question Answering VQAv2 [293] CLIP [329], BLIP [330] Accuracy FLOPs [130, 331]
