# 2011.00241.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/survey/2011.00241.pdf
# File size: 762993 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
METHODS FOR PRUNING DEEPNEURAL NETWORKS
A P REPRINT
Sunil Vadera
University of Salford,
Greater Manchester, M5 4WT, UK
S.Vadera@salford.ac.ukSalem Ameen
University of Salford,
Greater Manchester, M5 4WT, UK
S.Ameen@edu.salford.ac.uk
August 2, 2021
ABSTRACT
This paper presents a survey of methods for pruning deep neural networks. It begins by categoris-
ing over 150 studies based on the underlying approach used and then focuses on three categories:
methods that use magnitude based pruning, methods that utilise clustering to identify redundancy,
and methods that use sensitivity analysis to assess the effect of pruning. Some of the key inﬂuenc-
ing studies within these categories are presented to highlight the underlying approaches and results
achieved. Most studies present results which are distributed in the literature as new architectures,
algorithms and data sets have developed with time, making comparison across different studied
difﬁcult. The paper therefore provides a resource for the community that can be used to quickly
compare the results from many different methods on a variety of data sets, and a range of architec-
tures, including AlexNet, ResNet, DenseNet and VGG. The resource is illustrated by comparing the
results published for pruning AlexNet and ResNet50 on ImageNet and ResNet56 and VGG16 on
the CIFAR10 data to reveal which pruning methods work well in terms of retaining accuracy whilst
achieving good compression rates. The paper concludes by identifying some promising directions
for future research.
Keywords Deep learning, Neural networks, Pruning deep networks
1 INTRODUCTION
Deep learning and its use in high proﬁle applications such as autonomous vehicles (Kuutti et al., 2021), predicting
breast cancer (McKinney et al., 2020), speech recognition (Hinton et al., 2012) and natural language processing (Otter
et al., 2021) have propelled interest in Artiﬁcial Intelligence to new heights, with most countries making it central to
their industrial and commercial strategies for innovation.
Although there are different types of architectures (Pouyanfar et al., 2019), deep networks typically consist of layers
of neurons that are connected to neurons in preceding layers via weighted links. Another characteristic, which is
considered central to their predictive power (Sejnowski, 2020), is that they have a large number of parameters that
need to be learned, with networks such as ResNet50 (He et al., 2016) having more than 25 million parameters and
VGG16 (Simonyan and Zisserman, 2015) having more than 138 million weights. An obvious question, therefore, is to
ask whether it is possible to develop smaller, more efﬁcient networks without compromising accuracy? One direction
of work aimed at addressing this question has been to ﬁrst train a large network and then to prune and ﬁne-tune a
network. Although methods for pruning shallow neural networks were proposed in the 1980s and 90s (Mozer and
Smolensky, 1988; Kruschke, 1988; Reed, 1993), recent advances in deep learning and its potential for applications
in embedded systems has led to an increasing number and variety of algorithms for pruning deep neural networks.
Hence, this paper presents a survey of recent work on pruning neural networks that can be used to understand the
types of algorithms developed, appreciate the key ideas underpinning the algorithms and gain familiarity with the
major approaches and issues in the ﬁeld. The paper aims to achieve this goal by presenting the progressive path
from the earlier algorithms to the recent work, categorising algorithms based on the approach used, highlighting the
similarities and differences between the algorithms and concluding with some directions for future research.arXiv:2011.00241v2  [cs.LG]  30 Jul 2021

--- PAGE 2 ---
Methods for Pruning Deep Neural Networks A P REPRINT
The studies on pruning methods all carry out empirical evaluations that compare the performance of algorithms on
different architectures and benchmark data sets. These evaluations have evolved as new deep learning architectures
have developed, as new data sets have become available and as new pruning algorithms have been proposed. This
paper also provides and illustrates the use of a resource that brings together the reported results in one place, allowing
researchers to quickly compare the reported results on different architectures and data sets.
The survey identiﬁed over 150 studies on pruning neural networks, which can be categorised into the following eight
groups based on the underlying approach used:
1.Magnitude based pruning methods (Chauvin, 1988; Weigend, 1990; Weigend et al., 1991; Zhou et al.,
2018b), which are based on the view that the saliency of weights and neurons can be determined by local
measures such as their magnitude or approximated by their effect on the next layer.
2.Similarity and clustering methods (Chen et al., 1993; Han et al., 2016a; Li et al., 2019c; RoyChowdhury
et al., 2017; Sussmann, 1992; Zhou et al., 2018a) which aim to identify duplicate or similar weights which
are redundant and can be pruned without impacting accuracy.
3.Sensitivity analysis methods (Mozer and Smolensky, 1988; LeCun et al., 1990; Hassibi et al., 1993b,a;
Cohen et al., 2016; Lee et al., 2018b; Lin et al., 2018b), that assess the effect of removing or perturbing
weights on the loss and then remove a proportion of the weights that have least impact on accuracy.
4.Knowledge distillation methods (Bucilu ˇa et al., 2006; Hinton et al., 2015; Gregor Urban et al., 2017; Zhang
et al., 2019b) which utilise the original model, termed the Teacher, to learn a more compact new model called
the Student.
5.Low rank methods (Sainath et al., 2013; Jaderberg et al., 2014; Lin et al., 2018b) that factor a weight
matrix into a product of two smaller matrices which can then be used to perform an equivalent function more
efﬁciently than the single larger weight matrix.
6.Quantization methods (Jung et al., 2019; Zhou et al., 2017; Zhao et al., 2019b; Courbariaux et al., 2016;
Chen et al., 2015; Jacob et al., 2018), which are based on using quantization, hashing, low precision and
binary representations of the weights as a way of reducing the computations.
7.Architectural design methods (Baker et al., 2017; Dai et al., 2019; Li et al., 2019d; Liu et al., 2019a; Lin
et al., 2020c; Zhong et al., 2018; Zoph and Le., 2017) that utilise intelligent search and reinforcement learning
methods to generate neural network architectures.
8.Hybrid methods (Chung and Shin, 2016; Goetschalckx et al., 2018; Gadosey et al., 2019) which utilise a
combination of methods aimed at taking advantage of the cumulative compressing effects of the different
types of methods.
Appendix A provides a table which classiﬁes the existing studies into the 8 categories, enabling researchers working
on a particular type of method to locate related studies. Given the range of studies, and availability of surveys already
covering some of the above categories, this paper focuses on recent algorithms in the ﬁrst three categories for pruning.
Reed (1993) provides an excellent survey of pruning methods prior to the deep learning era. Readers interested in the
use of quantization, low rank and knowledge distillation methods are referred to the survey by Lebedev and Lempitsky
(2018) and readers interested in architectural design methods are referred to the comprehensive survey by Elsken et al.
(2019). Pruning networks is just one step in developing efﬁcient models and a recent survey by Menghani (2021)
summarises the full range of methods, from use of quantization and learning, to the available software and hardware
infrastructure for efﬁcient deployment of models. Another important direction of work, worthy of a survey in its own
right, and not in the scope of this paper, is the use of variational Bayesian methods for regularization (Arbib, 2003;
Goodfellow et al., 2016; Huang and Wang, 2018; Lin et al., 2020d; Srivastava et al., 2014; Wen et al., 2016; Zhao
et al., 2019a).
Fig. 1 shows a selection of the methods covered in greater detail in this survey and includes a sub-categorization
of magnitude and sensitivity analysis methods. The survey found relatively few methods that utilise similarity and
clustering, and further sub-categorization is not useful. Magnitude based methods can be sub-categorised into: (i)
data dependent methods that utilise a sample of examples to assess the extent to which removing weights impacts the
outputs from the next layer; (ii) data independent methods, that utilise measures such as the magnitude of a weight;
and (iii) the use of optimisation methods to reduce the number of weights in a layer whilst approximating the function
of the layer. Methods that utilise sensitivity analysis can be sub-categorized into those that: (i) adopt a Taylor series
approximation of the loss and (ii) use sampling to estimate the change in loss when weights are removed.
The rest of this paper is organised as follows. Section 2 presents the background. Sections 3 to 5 describe representative
methods in the three categories: magnitude based pruning, clustering and similarity, and sensitivity analysis. Section 3
also includes coverage of the Lottery Hypothesis, an issue about the existence of smaller networks and ﬁne-tuning,
2

--- PAGE 3 ---
Methods for Pruning Deep Neural Networks A P REPRINT
Figure 1: A selection of pruning methods grouped in terms of the approach adopted
Figure 2: The LeNet-5 network and how it processes an input image via convolutions (Conv.) and pooling operations
to produce features maps (FMs) and uses fully connected (FC) layers to perform classiﬁcation
that cuts across the different methods. Section 6 presents a comparison of the published results for pruning AlexNet,
ResNet and VGG to illustrate the resource provided for comparing the methods. Section 7 concludes by highlighting
some key insights and suggesting directions for future research.
2 BACKGROUND
This section introduces the background knowledge assumed in the survey.1Fig. 2 shows the structure of one of the
earliest convolutional neural networks (CNNs), LeNet-5 (LeCun et al., 1989), which recognises handwritten digits
by applying convolutions and pooling operations to identify features. These features then provide the input to fully
connected layers that classify the images. The pooling operation takes feature maps as input and reduces their size by
applying an operation, such as the maximum value within a neighbourhood while the convolution operation applies
ﬁlters (or kernels) to the input channels (or feature maps) to produce the output feature maps. The ﬁlters are k x k
1Readers unfamiliar with deep neural networks are referred to tutorial accounts such as (Goodfellow et al., 2016) for further
details
3

--- PAGE 4 ---
Methods for Pruning Deep Neural Networks A P REPRINT
matrices that slide over the input feature maps and convolve with the corresponding elements of the input feature maps
to produce the output feature maps. The elements of a ﬁlter correspond to the weights (or parameters) that are used
to transform regions in feature maps in one layer to the next and need to be learned through training. The weights (or
parameters), either individually or collectively as ﬁlters, are therefore the primary candidates for pruning.
The LeNet-5 model, with 60K parameters in 5 layers, achieved impressive results on a data set known as MNIST
(LeCun et al., 1998).2In a breakthrough in 2012, AlexNet built upon the concepts in LeNet-5 and developed a
deeper network with over 60M parameters in 8 layers to win a competition known as ImageNet by a signiﬁcant mar-
gin (Krizhevsky et al., 2012). This success was followed by the development of architectures like VGG, ResNet, and
ResNeXT that used an increasing number of layers and parameters to gain further improvements in the ImageNet
competition (Pouyanfar et al., 2019). The huge number of parameters in these models does necessitate greater com-
putational resources and inhibits their use in embedded systems, which has motivated the research on pruning that is
surveyed in this paper.
The pruning methods developed are evaluated on a range of architectures (e.g., ResNet, VGG, DenseNet) and data sets
(e.g., ImageNet CIFAR, SVHN). Khan et al. (2020) presents a tutorial on deep learning architectures and Appendix B
summarises the data sets. When evaluating pruning methods, the surveyed papers use the following measures to report
their results:
• The Top-1 and Top-5 accuracy, which report the proportion of times the correct classiﬁcation appears ﬁrst or
in the top 5 list of ranked results. In the sections below, unless we explicitly qualify a measure, the Top-1
accuracy should be assumed.
• The compression rate, which is the ratio of parameters before and after a model is pruned.
• The computational efﬁciency in terms of the FLOPS (Floating Point Operations) required to perform a clas-
siﬁcation.
The notation used in the paper is deﬁned where it is used and also summarised in Appendix C. With this background
in place, Sections 3 to 5 describe some key inﬂuential studies that bring out the features of the categories of methods
surveyed in this paper.
3 MAGNITUDE BASED PRUNING
This section presents pruning methods that remove weights, nodes, and ﬁlters based on a measure of magnitude or the
effect ﬁlters have on the next layer. Subsection 3.1 summarizes an early inﬂuential method for pruning weights and
Subsection 3.2 presents a recent hot topic, termed the Lottery Hypothesis, that reinvigorates research on the existence
of smaller networks and raises issues about ﬁne-tuning a pruned network. Subsection 3.3 describes the key ideas
behind methods that prune ﬁlters and feature maps.
3.1 Network Pruning of Weights
One of the ﬁrst studies to utilise magnitude based pruning for deep networks is due to Han et al. (2015) who adopt a
process in which weights below a given threshold are pruned. Once pruned, the network is ﬁne-tuned and the process
repeated until its accuracy begins to deteriorate.
Han et al. (2015) carry out several experiments to compare the merits of their magnitude based iterative pruning
method. First, they apply their method on a fully connected network known as LeNet-300-100 and then on Lenet-5
(Fig. 2), both of which are trained on the MNIST data. Their results show that it is possible to reduce the number
of weights by a factor of 12 without compromising accuracy. Second, they apply iterative pruning to AlexNet and
VGG16 trained on the ImageNet data, and show that it is possible to reduce the number of weights by a factor of 9
and 12 respectively. Thirdly, they compare the merits of using regularisation to drive down the magnitude of weights
to aid subsequent pruning. They explore regularisation with both the L1andL2norms and conclude that L1is better
immediately after pruning (without ﬁne-tuning), but L2is better if the weights of the pruned model are ﬁne-tuned.
Their experiments also suggest that the earlier layers (i.e., closer to the inputs) are the most sensitive to pruning and
that iterative pruning is better than pruning the required proportion of weights in one cycle (i.e., one-shot pruning).
The study by Han et al. (2015) is notable in that (i) it demonstrated that it was possible to reduce the size of deep
networks signiﬁcantly without compromising accuracy, (ii) it highlighted the beneﬁts of iterative pruning and (iii)
it prompted further research on questions such as whether retraining from scratch or ﬁne-tuning is better following
pruning.
2Modiﬁed National Institute of Standards and Technology
4

--- PAGE 5 ---
Methods for Pruning Deep Neural Networks A P REPRINT
Guo et al. (2016) note that magnitude pruning can lead to premature removal of weights that can become important
given removal of other weights. To address this, they propose a method known as Dynamic Network Surgery (Dyn
Surg) which maintains a mask that indicates which weights should be removed and retained in each training cycle,
thereby allowing reinstatement of weights previously marked to be pruned if they turn out to be important. Guo et al.
(2016) compare their method with magnitude pruning, with the results showing that it is reduces the number of weights
by a factor of over 17 for AlexNet on ImageNet.
3.2 The Lottery Ticket Hypothesis
One of the most interesting observations in (Han et al., 2015) is that re-initialization of the weights does not lead to
accurate models and, based on their trials, it was better to ﬁne-tune the weights of the pruned model. Following on
from this observation, Frankle and Carbin (2019) propose the Lottery Ticket Hypothesis which states that: a trained
network contains a subnetwork, which can be trained to be at least as accurate as the original network using no more
than the number of epochs used for training the original network. This subnetwork is termed a winning lottery ticket,
given that it was lucky to be initialised with suitable weights.
To test this hypothesis, they propose two pruning methods. First, in a one-shot method, they use magnitude pruning to
prune p% of the weights, reset the remaining weights to their initial values and retrain. Second, they utilise an iterative
pruning method with ncycles, with each cycle pruning p1=nof the weights.
They perform experiments on the fully connected LeNet-300-100 network for the MNIST data, and variants of VGG
and ResNet for the CIFAR10 data. Their experiments on the LeNet-300-100 network prune a percent of the weights
from each layer except the ﬁnal layer, in which the percent pruned is reduced by half. Their results with iterative
pruning show that: (i) a subnetwork that is only 3.6% of its original size performs just as well, (ii) random initialization
of the pruned networks results in slower learning in comparison to use of the original weight initializations, (iii) that the
subnetworks (termed winning tickets) found, learn faster than the original network, (iv) there is continual improvement
in the rate of learning as the size of the network reduces, but only up to a point, after which learning slows down and
begins to regress to the performance of the original network, (v) iterative pruning tends to result in more accurate
smaller networks than one-shot pruning.
Their experiments on the larger networks, VGG and ResNet, show that identiﬁcation of winning lotteries depends on
the learning rate, with a lower rate successfully identifying winning lottery subnets, and that pruning weights over all
the network, as opposed to layer by layer produces better results.
These results provide good empirical evidence for the Lottery Hypothesis and the award of a best paper prize in
the 2019 International Conference on Learning Representations is indicative of the signiﬁcance of the paper and the
attention it has attracted.
In their paper, “Rethinking the value of network pruning”, Liu et al. (2019b) challenge the claim that it is better to
utilise the initial weights of a pruned model when compared with random initialization. To test this, they carry out
experiments on VGG, ResNet, and DenseNet using the CIFAR10, CIFAR100, and ImageNet data. They deﬁne three
types of pruning regime: structured pruning, where the proportion of channels that are pruned per layer is predeﬁned;
automatic pruning, where the proportion of channels pruned overall is predeﬁned but the per layer rate is determined
by the algorithm; and unstructured weight pruning, where only the proportion of weights pruned is predeﬁned. Their
results suggest that for structured and automatic pruning, random initialization is equally (if not more) effective.
However, for unstructured networks, random initialization can achieve similar results on small data sets but for large
scale data such as ImageNet, ﬁne-tuning produces better results.
At ﬁrst sight, their ﬁndings contradict the Lottery Hypothesis. However, in a follow up study, Frankle et al. (2019)
acknowledge that setting the weights of pruned networks to their initial values does not work well on larger networks
and suggest that methods for retraining from random initializations do not work well either, except for moderate levels
of pruning (up to 30%). They therefore propose setting the weights to those obtained in a later iteration of training,
which they then demonstrate to be beneﬁcial in identifying good initialization of weights for larger scale problems
such as ImageNet.
The above studies focus on empirical evaluations of networks trained and used on the same data sets, and primarily on
image processing classiﬁcation tasks. Morcos et al. (2019) explore a number of other interesting questions:
• Are the lotteries found for one image classiﬁcation task transferable to other tasks?
• Are lotteries observable in other tasks (such as natural language processing), and architectures?
• Are they transferable across different optimizers?
5

--- PAGE 6 ---
Methods for Pruning Deep Neural Networks A P REPRINT
Figure 3: Fig. 3. Illustration of how the feature maps are computed, where Wj;iare the k x k ﬁlters used on the input
channels Xito obtain output feature maps Yj
To explore these questions, they carry out experiments with VGG19 and ResNet50 using six data sets (Fashion-
MNIST, SVHN, CIFAR10, CIFAR100, ImageNet, Places365), in which the lotteries (i.e., subnetworks with initial-
izations) identiﬁed for one task are used for another task. Their experiments use iterative magnitude based pruning,
selecting 20% of the weights over all the layers, and with late setting of weights (as proposed in (Frankle and Carbin,
2019)). The results are interesting: in general, winning initializations carry across similar image processing tasks
and winning tickets from larger scale tasks were more transferable than the tickets from the smaller scale tasks. In
some cases, for example, the use of VGG19 on the Fashion-MNIST data, the winning tickets obtained from the use
of VGG19 on the larger data sets (CIFAR100, ImageNet) performed better than those obtained directly from the
Fashion-MNIST data.
Hubens et al. (2020) carry out empirical trials that conﬁrm similar results on the size of the pruned networks. They
show that when a network is trained on a larger data set, such as ImageNet, and transferred and ﬁne-tuned for a
different task, pruning can result in a smaller network than if it was trained from scratch on the new task.
Morcos et al. (2019) carry out experiments in which lottery tickets are identiﬁed using one optimizer, ADAM (adaptive
moment estimation), and then utilise a different optimizer, SGD (Stochastic Gradient Descent) with momentum, and
vice versa on the CIFAR10 data. Their results suggest that, in general, winning tickets are optimizer independent.
To test if the lottery hypothesis holds in other types of problems, Yu et al. (2019) carry out experiments on natural
language processing (NLP) and control tasks in games. For NLP, they utilise LSTMs for the Wikitext-2 data (Merity
et al., 2017) and Transformer models for translating news in English to German (Vaswani et al., 2017). The experi-
ments were carried out with 20 rounds of iterative pruning and with one-shot pruning. A pruning rate of 20% was used
and following pruning, weights were reset to those learned during a later round of training. For control tasks, they
utilise Reinforcement Learning (RL) and carry out experiments on fully connected networks used for 3 OpenAI Gym
environments (Brockman et al., 2016) and 9 Atari games that utilise convolutional networks (Bellemare et al., 2015).
From their results on NLP and the RL control tasks, they conclude that both iterative pruning and late setting of
weights are superior in comparison to random initialization of pruned networks, with iterative pruning being essential
when a signiﬁcant number of weights (i.e., more than two-thirds) are pruned. For the Atari games, the results varied:
in one case, it led to improvements over the original network (Berzerk game) while in another, an initial improvement
was followed by a signiﬁcant drop in accuracy as the amount of pruning increased (Space Invaders game). In other
cases, pruning resulted in a reduction in performance (e.g., Assault game). Thus in summary, Yu et al. (2019) provide
some evidence that the lottery hypothesis holds for NLP tasks and for some control tasks that utilise RL.
3.3 Pruning Feature Maps and Filters
Although the kind of methods described in Subsection 3.1 result in fewer weights, they require specialist libraries or
hardware for processing the resulting sparse weight matrices (Li et al., 2017; Denil et al., 2013; Ayinde et al., 2019).
In contrast, pruning at higher levels of granularity, such as pruning ﬁlters and channels beneﬁts from the optimizations
already available in many current toolkits. This has led to a number of methods for pruning feature maps and ﬁlters
which are summarized in this section.
To appreciate the intuition and notation behind these methods, it is worth bearing in mind how ﬁlters are applied to
the input channels to produce the output feature maps. Figure 3 illustrates the process, showing how an image with 3
channels is taken as input and convolved with the ﬁlters to produce the 4 output feature maps. Given the visualisation
offered by Figure 3, how can one best prune the ﬁlters and channels? The survey revealed three main directions of
research:
6

--- PAGE 7 ---
Methods for Pruning Deep Neural Networks A P REPRINT
1.Data dependent channel pruning methods , which are based on the view that when different inputs are
presented, the output channels (i.e., feature maps) should vary given they are meant to detect discriminative
features. Subsections 3.3.1 and 3.3.2 describe the methods that adopt this view.
2.Data independent pruning methods , that use properties of the ﬁlters and output channels, such as the
proportion of zeros present, to decide which ﬁlters and channels should be pruned. Subsections 3.3.3 to 3.3.6
describe methods that take this direction.
3.Optimization based channel approximation pruning methods , that use optimization methods to recreate
the ﬁlters to approximate the output feature maps. Pruning methods that typify this approach are described in
Subsections 3.3.6 and 3.3.7.
3.3.1 Pruning based on variance of channels and ﬁlters
Polyak and Wolf (2015) propose two methods for pruning channels: Inbound pruning, which aims to reduce the
number of channels incoming to a ﬁlter and Reduce and Reuse pruning, which aims to reduce the number of output
channels.
The idea behind Inbound pruning is to assess the extent to which an input channel’s contribution to producing an
output feature map varies with different examples. This assessment is done by applying the network to a sample of the
images and then using the variance in a feature map as a measure of its contribution.
More formally, given Wj;i, the jthﬁlter for the ithinput channel, and Xp
i, the input from the ithchannel for the pth
example, the contribution to the jthoutput feature map, Yp
j;iis deﬁned by:
Yp
j;i=Wj;iXp
i
F(1)
Given this deﬁnition, the measure used to assess the variation in its contribution, s2
j;ifrom the Nsamples is:
s2
j;i=varn
Yp
j;ijp=1:::No
(2)
Inbound pruning uses this measure to rank the ﬁlters Wj;iand removes any that fall below a speciﬁed threshold.
The Reduce and Reuse pruning method focuses on assessing the variations in the output feature maps when different
samples are presented. That is, the method ﬁrst computes the variations in the output feature maps s2
jusing:
s2
j=var m
å
i=1Yp
j;i
Fjp=1:::N!
(3)
Where mis the number of input channels and Nis the number of samples. Reduce and Reuse then uses this measure
to retain a proportion of the output feature maps and corresponding ﬁlters that results in the greatest variation.
Removal of an output feature map is problematic given it is expected as an input channel in the next layer. To overcome
this, they approximate a removed channel using the other channels. That is, if Yi,Y0
iare the outputs of a layer before
and after pruning a layer respectively, the aim is to ﬁnd a matrix A such that:
min
Aå
iYi AY0
i2
2(4)
The matrix Ais then included as an additional convolutional layer of 1x1 ﬁlters along the lines proposed by Lin et al.
(2014).
Polyak and Wolf (2015) evaluate the above approach on the Scratch network, using the CASIA-WebFace and the
Labeled Faces in the Wild (LFW) data sets. They utilise layer by layer pruning, where each layer is pruned, and the
network ﬁne-tuned before moving on to the next layer. They experiment with their two pruning methods individually
and in combination, and compare the results with the use of random pruning, a low rank approximation method (Zhang
et al., 2016) and Fitnets, a method that uses the Knowledge Distillation approach to learn smaller networks (Romero
et al., 2014). In the experiments with the Inbound pruning method, they prune channels where s2
j;iis below a given
threshold, selected such that the overall accuracy is maintained above 84%. For the experiments with the Reduce and
Reuse method, they try different levels of pruning: 50%, 75%, and 90% for the earlier layers followed by 50% for the
later layers. The adoption of a lower pruning rate for the later layers follows an observation that heavy pruning of the
later layers results in a marked reduction in accuracy.
7

--- PAGE 8 ---
Methods for Pruning Deep Neural Networks A P REPRINT
The results from their experiments show that: (i) the variance based method is more effective than use of random
pruning, (ii) the use of ﬁne-tuning does help in recovering accuracy, especially in the later layers, (iii) their methods
result in greater compression than use of a low rank method and the use of Fitnets when applied to the Scratch network.
3.3.2 Entropy-based channel pruning
Instead of the variance, Luo and Wu (2017) propose an entropy-based metric to evaluate the importance of each ﬁlter.
In their ﬁlter pruning method, if a feature map contains less information, its corresponding ﬁlter is considered less
important, and could be pruned. To compute the entropy value of a particular feature map, they ﬁrst sample the data
and obtain a set of feature maps for each ﬁlter. Each feature map is reduced to a point measure using a global average
pooling method, and the set of measures associated with each ﬁlter are discretized into q groups. The entropy of a
ﬁlter, Hjis then used to assess the discriminative power of a ﬁlter (Luo and Wu, 2017):
Hj=q
å
i=1Pilog(Pi) (5)
Where Piis the probability of an example being in group i.
They explore both one-shot pruning followed by ﬁne-tuning and layer wise pruning in which they ﬁne-tune with just
one or two epochs of learning immediately after pruning a layer. Their layer wise strategy is an interesting compromise
between fully ﬁne-tuning after pruning each layer, which can be computationally expensive, and only ﬁne-tuning at
the end, which can fail to take account of the knock-on effects of pruning previous layers.
They evaluate the merits of the entropy-based method by applying it to VGG16 and ResNet-50 on the ImageNet data.
For VGG16, they focus on the ﬁrst 10 layers and, also replace the fully connected layers by use of average pooling to
obtain further reductions. They compare their results on VGG16 with those obtained by the magnitude based pruning
method and APoZ method (c/f Subsection 3.3.3). Their results suggest that: (i) the entropy-based method achieves
more than a 16 fold compression, though this is at the expense of a 1.56% reduction in accuracy, (ii) use of magnitude
pruning results in a 13 fold compression, and (iii) APoZ results in a 2.7 fold compression. However, it should be
noted that the higher compression rate achieved by the use of entropy includes the reduction due to the replacement
of the fully connected layers by average pooling, without which the use of the entropy-based method leads to a lower
compression rate than APoZ (Table 3 in Luo and Wu (2017)).
3.3.3 APoZ: Network trimming based on zeros in a channel
In contrast to the use of samples of data to compute the variance of a feature map or its entropy, Hu et al. (2016),
suggest a direct method that is based on the view that the number of zeros in an output feature map is indicative of its
redundancy. Based on this view, they propose a method that uses the average number of zero activations (APoZ) in a
feature map (after the ReLU) as a measure of the weakness of a ﬁlter that generates the feature map.
Their experiments are with LeNet5 on MNIST and VGG16 on ImageNet and aimed at ﬁrst ﬁnding the most appropriate
layers to prune and then to iteratively prune these layers in a bespoke way that maintains or improves accuracy.
Following pruning, they experiment with both retraining from scratch and ﬁne-tuning the weights and prefer the latter
given better results.
For LeNet-5, they observe that most of the parameters (over 90%) are in the 2nd convolution layer and the ﬁrst fully
connected layer and hence they focus on pruning these two layers in four iterations of pruning and ﬁne-tuning, resulting
in the size of the convolutional layer reducing from 50 to 24 ﬁlters and the number of neurons in the fully connected
layer reducing from 500 to 252. Overall, this represents a compression rate of 3.85.
For VGG16, they also focus on one convolutional layer that has 512 ﬁlters and a fully connected layer with 4096
nodes. After 6 iterations, they reduce these to 390 ﬁlters and 1513 nodes, achieving a compression rate of 2.59.
3.3.4 Pruning small ﬁlters and ﬁlter sketching
Li et al. (2017) extend the idea of magnitude pruning of weights to ﬁlters by proposing the removal of ﬁlters that
have the smallest absolute sum among the ﬁlters in a layer. That is, if the ﬁlters for producing the jthfeature map are
Wj;i2Rkxkandmis the number of input feature maps, then the magnitude of the jthﬁlter is deﬁned by:
sj=m
å
i=1Wj;i
1(6)
Once the sjare computed, a proportion of the smallest ﬁlters together with their associated feature maps and ﬁlters in
the next layer are removed. After a layer is pruned, the network is ﬁne-tuned, and pruning is continued layer by layer.
8

--- PAGE 9 ---
Methods for Pruning Deep Neural Networks A P REPRINT
To test this approach, they carry out experiments on VGG16 and ResNet56 & 110 on CIFAR10 and ResNet34 on
ImageNet. By analyzing the sensitivity of the layers through experimentation, they determine appropriate pruning
ratios for each layer that would not compromise accuracy signiﬁcantly. Overall, for VGG16, they are able to prune
the parameters by 64%. A signiﬁcant proportion of this pruning is in layers 8 to 13 which consist of the smaller ﬁlters
(2x2 and 4x4), which they notice can be pruned by 50% without reducing accuracy. The level of pruning for the
other networks is more modest, with the best pruning rate for ResNet-56 and ResNet110 on CIFAR10 being 3.7% and
32.4% respectively, and for ResNet-34 on ImageNet being 10.8%.
They also compare their approach with the variance-based method (Subsection 3.3.1) and conclude that use of the
above measure over ﬁlters performs at least as well but without the additional need to compute the feature maps via
samples of the data.
A recent method known as ﬁlter sketch also aims to reduce the number of ﬁlters without the need to sample exam-
ples (Lin et al., 2020a). The key idea in ﬁlter sketching is to minimize the difference between the co-variances of the
original set of ﬁlters and the reduced set. Although this can be done using optimization methods, Lin et al. (2020a)
utilise a greedy algorithm known as Frequent Direction (Liberty, 2013) which is more efﬁcient.
Lin et al. (2020a) evaluate the ﬁlter sketch method on GoogleNet, ResNet56 and ResNet110 using the CIFAR10 data,
and on ResNet50 with the ImageNet data. The results show that it performs well relative to the method for pruning
small ﬁlters and a method that uses optimization to prune channels (c/f Subsection 3.3.7) in terms of reducing the
number of parameters without a signiﬁcant loss in accuracy.
3.3.5 Pruning ﬁlters based on geometric median
He et al. (2019) point out that pruning based on the magnitude of ﬁlters assumes that there are some small ﬁlters and
that the spread of magnitude is wide enough to adequately distinguish those ﬁlters that contribute from those do not
contribute. So, for example, if most of the weights are small, one could end up removing a signiﬁcant number of
ﬁlters and if most of the ﬁlters have large values, no ﬁlters would be removed, even though there may be ﬁlters that are
relatively small. Hence, they propose a method based on the view that the geometric median of the ﬁlters shares most
of the information common in the other ﬁlters and hence a ﬁlter that is close to it can be covered by the other ﬁlters if
deleted. Computing the geometric median can be time-consuming, so they approximate its computation by assuming
that one of the ﬁlters will be the geometric mean. Their pruning strategy is to prune and ﬁne-tune repeatedly using a
ﬁxed pruning factor for all layers.
They carry out an evaluation with respect to several methods including pruning small ﬁlters (Li et al., 2017),
ThiNet (Luo et al., 2017), Soft ﬁlter pruning (He et al., 2018a), and NISP (Yu et al., 2018). These methods are
evaluated on ResNets trained on the CIFAR10 and ImageNet data, with pruning rates of 30 and 40 percent. In general,
the drop in accuracy is similar across the different methods, though there is a signiﬁcant reduction in FLOPS when
using the geometric median method on ResNet-50 (53.5%) compared to the other methods (e.g., ThiNet 36.7%, Soft
ﬁlter pruning 41%, NISP 44%).
3.3.6 ThiNet and AOFP
Luo et al. (2017) formulate the pruning task as an optimization problem and propose a system ThiNet in which the
objective is to ﬁnd a subset of input channels that can best approximate the output feature maps. The channels not in
the subset and their corresponding ﬁlters can then be removed. Solving the optimization problem is computationally
challenging, so ThiNet uses a greedy algorithm that ﬁnds a channel that contributes the least, adds it to the list to be
removed, and repeats the process with the remaining channels until the number of channels selected equals the number
to be pruned. Once a subset of ﬁlters to be retained is identiﬁed, their weights are obtained by using least squares to
ﬁnd the ﬁlters Wthat minimize (Luo et al., 2017):
m
å
i=1 
Yi WTXi2(7)
Where Yiare the msampled points in the output channels and Xitheir corresponding input channels.
They evaluate their approach in two sets of experiments. In the ﬁrst, they adapt VGG16, replacing the fully connected
layers by global average pooling (GAP) layers, apply it to the UCSD-Birds data and then prune it using ThiNet, APoZ
and the small ﬁlters method. Their results show there is less degradation in accuracy with ThiNet than ApoZ, which
in turn, is better than the small ﬁlters method.
In their second set of experiments, they utilise VGG16 and ResNet50 trained on the ImageNet data. For VGG16, their
procedure involves pruning a layer and then minor ﬁne-tuning with one epoch of training with an additional epoch at
9

--- PAGE 10 ---
Methods for Pruning Deep Neural Networks A P REPRINT
the end of each group of convolutional layers and a further 12 epochs of ﬁne-tuning after the ﬁnal layer. With the use of
GAP, ThiNet, reduces the number of parameters by about 94% at the expense of a 1% reduction in Top-1 accuracy. For
ResNet, ThiNet is applied on the ﬁrst two convolutional layers of each residual block, keeping the output dimensions
of the blocks the same. After pruning each layer, one epoch of ﬁne-tuning is performed, and 9 epochs are used for
ﬁne-tuning at the end. The results show that ThiNet is able to halve the number of parameters with a 1.87% loss in
Top-1 accuracy.
Ding et al. (2019b) propose a similar method to ThiNet, called Approximated Oracle Filter Pruning (AOFP), which
aims to identify the subset of ﬁlters, which if removed, will have the least effect on the feature maps in the next layer.
However, whereas, the search procedure adopted in ThiNet uses a greedy bottom up approach, AOFP adopts a top-
down binary search in which half of the ﬁlters in a layer are randomly selected and set to be pruned. The effect of
removing these ﬁlters on the feature map produced in the next layer is measured and recorded against each ﬁlter that
is set as pruned. This process is repeated for different random selections, and the average effect per ﬁlter used as an
indication of the effect of removing a ﬁlter. The top 50% of the ﬁlters that would result in the worst effect if removed
are retained and the process repeated unless this would result in an unacceptable reduction in accuracy. In comparison
to ThiNet, and other methods, AOFP does not require the rate of pruning to be ﬁxed in advance of pruning a layer.
AOFP is evaluated by pruning AlexNet, VGG and ResNet trained on the CIFAR10 and ImageNet data. They compare
AOFP with several methods including: ThinNet, Network Slimming (Liu et al., 2017), Pruning using Agents (Huang
et al., 2018), Online Filter Weakening (Zhou et al., 2018b), NISP (Yu et al., 2018), Optimizing Channel Pruning (He
et al., 2017), Structured Probabilistic Pruning (Wang et al., 2017), Autopruner (Luo and Wu, 2018), and ISTA (Ye
et al., 2018), with their results showing that AOFP is capable of greater reductions in FLOPS without compromising
accuracy.
3.3.7 Optimizing channel selection with LASSO regression
He et al. (2017) also formulate channel selection as an optimization problem. Given a channel Yobtained by applying
a ﬁlter Witominput channels Xi:
Y=m
å
i=1XiWT
i (8)
They deﬁne the task as one to optimize:
argmin
b;W1
2Y c
å
i=1biXiWT
i2
F
subject tokbk0p(9)
Where pindicates the number of channels retained and bi2f0;1gindicates the retention or removal of a channel.
In contrast to ThiNet, which adopts a greedy heuristic to solve this optimization problem, He et al. (2017) relax the
problem from L0toL1regularization and utilise LASSO regression to solve :
argmin
b;W1
2Y c
å
i=1biXiWT
i2
F+lkbk1
subject tokbk0p(10)
Following the selection of the channels they utilise least squares to obtain the revised weights in a manner similar to
the approach adopted in ThiNet.
They carry out empirical evaluations on VGG16, ResNet50 and a version of the Xception network, trained on the
CIFAR10 and ImageNet data. They also explore the extent to which the pruned models can be used for transfer
learning by using them for the PASCAL VOC 2007 object detection task.
In their ﬁrst set of experiments, they evaluate their method on single layers of VGG16 trained on CIFAR10 without any
ﬁne-tuning, and show that their algorithm maintains Top-5 accuracy better than the method of pruning small ﬁlters.
They also include results from a na ¨ıve method that selects the ﬁrst k feature maps and show that, for some layers (e.g.
conv3 3 in VGG16), this sometimes performs better than the method of pruning small ﬁlters, highlighting a potential
weakness of magnitude-based pruning.
In a second set of experiments, with VGG16 on CIFAR10, they apply their method on the full network, using bespoke
pruning ratios for the layers and ﬁne-tuning to achieve 2, 4 and 5 fold improvements in run-time, but resulting in drops
10

--- PAGE 11 ---
Methods for Pruning Deep Neural Networks A P REPRINT
of Top-5 accuracy of 0%,1%, and 1.7% respectively. In comparison, the method for pruning small ﬁlters results in
larger drops of 0.8%, 8.6% and 14.6%.
Their experiments on ResNet50 adopt bespoke pruning rates per layer, retaining 70% of layers that are very sensitive to
pruning, and 30% of the less sensitive layers. The Top-5 accuracy results on ImageNet show a two-fold improvement
in run-time at the expense of a 1.4% drop in accuracy compared to a baseline accuracy of 92.2%, while the results on
the Xception network show a drop of 1% in accuracy from a baseline of 92.8%.
The experiment on using a pruned version of a VGG16 model on the PASCAL VOC 2007 object detection benchmark
task results in a 2-fold increase in speed with a 0.4% drop in average precision.
4 PRUNING BASED ON SIMILARITY AND CLUSTERING
Given that neural networks can be over-parametrised, it is plausible that there could be duplicate weights or ﬁlters that
perform similar functions and can be removed without impacting accuracy (Sussmann, 1992; RoyChowdhury et al.,
2017; Han et al., 2016b; Son et al., 2018; Li et al., 2019e).
RoyChowdhury et al. (2017) explore this hypothesis by using the inner product of two ﬁlters (or weight matrices)
as a measure of similarity. Their pruning algorithm involves grouping ﬁlters that are similar and then replacing each
group of ﬁlters by their mean ﬁlter. They carry out experiments with both a multilayer perceptron (MLP) and a CNN
for the CIFAR10 data. The MLP has three layers: the ﬁrst two are fully connected layers and the third is a softmax
layer with 10 nodes representing the class for CIFAR10. The CNN has two convolution layers, each followed by a
ReLU, and a 2x2 max pooling layer. The convolutional layers are followed by two fully connected layers to perform
the classiﬁcation. In both cases, the ﬁrst layer is varied with 100, 500 and 1000 units (nodes or ﬁlters) to explore the
effects of increasing over parametrisation. Their main ﬁnding is that there is a much greater propensity for similar
weights/ﬁlters to occur in MLPs than in CNNs. As a consequence, there is a greater opportunity for using similarity
as a basis for pruning MLPs than for pruning CNNs. Nevertheless, their results suggest that a similarity based pruning
algorithm is better at retaining accuracy than using the small ﬁlters method.
Ayinde et al. (2019) also develop a method that uses clustering to identify similar ﬁlters. They too adopt the inner
product as a measure of similarity, but use an agglomerative hierarchical clustering method to group similar ﬁlters and
replace the ﬁlters by randomly selecting one ﬁlter from each cluster. They carry out various experiments with VGG16
on CIFAR10 and ResNet34 on ImageNet. For the trial on VGG16 with the CIFAR10 data, they show that, once
an optimal value for the threshold for similarity is determined, their method achieves both a better pruning rate and
accuracy than other methods, including pruning of small ﬁlters, Network Slimming, a method that uses regularization
to identify weak channels (Liu et al., 2017), and try-and-learn, a method that uses sensitivity analysis (c/f Section 5).
5 SENSITIVITY ANALYSIS METHODS
The primary goal of pruning is to remove weights, ﬁlters and channels that have least effect on the accuracy of a
model. The magnitude and similarity based methods described above address this goal implicitly by using properties
of weights, ﬁlters and channels that can affect accuracy. In contrast, this section presents methods that use sensitivity
analysis to model the effect of perturbing and removing weights, ﬁlters and channels on the loss function.
Subsection 5.1 describes methods that assess the importance of channels and Subsections 5.2 to 5.4 present the de-
velopment of a line of research that approximates the effect of perturbing the weights on the loss function using the
Taylor Series, from the earliest work which developed methods for MLPs to the more recent research on methods for
pruning CNNs.
5.1 Pruning by assessing the importance of nodes and channels
Skeletonization, a method proposed by Mozer and Smolensky (1988), was one of the earliest approaches to pruning
neural networks. To calculate the effect of removing nodes, Skeletonization introduced the notion of attentional
strength to denote the importance of nodes when computing activations. Given the attentional strengths of the nodes,
ai, the output yjfrom node j, is deﬁned by:
yj=f 
å
iwjiaiyi!
(11)
Where fis assumed to be the sigmoid function. The importance of a node riis then deﬁned in terms of the difference
in loss when aiis set to zero and when it is set to one and can be approximated by the derivative of the loss with
11

--- PAGE 12 ---
Methods for Pruning Deep Neural Networks A P REPRINT
respect to the attentional strength ai:
ri=Lai=0 Lai=1 ¶L
¶ai
ai=1(12)
Through experimentation, Mozer and Smolensky (1988) found that the linear loss worked better a than the quadratic
loss because the difference between the outputs and targets was small following training. In addition, they noticed that
the¶L(t)/¶aiwere not stable with time, so they used a weighted average measure to compure the importance bri:
bri(t+1) =0:8bri(t)+0:2¶L(t)
¶ai(13)
Mozer and Smolensky (1988) present a number of small but very interesting experiments. These include generating
examples where the output is correlated to four inputs, A,B,C, and D, with full correlation on A and reducing to no
correlation with D. They provide this as input to a network with one hidden node and following training they observe
that the weights from the inputs to the hidden node follow the correlations, although the relevance measure only shows
input node A as important, providing some reassurance that the measure is different from the weights. In another
example, they develop a network to model a 4-bit multiplexor network, which has 4 bits as input and two bits to
control which of the 4 bits is output. They try two network conﬁgurations: in the ﬁrst, they utilise 4 hidden nodes
and in the second they utilise 8 hidden nodes and use skeletonization to reduce its size to 4 hidden nodes. When
limiting training to 1000 epochs, they ﬁnd that starting with 4 hidden nodes initially, results in failure to converge in
17% of the cases, while beginning with 8 hidden layer nodes followed by skeletonization converges in all the cases
and, also retains accuracy. This appears to be one of the ﬁrst demonstrations that, to begin, it may be necessary to
overparameterize a network in order to ﬁnd winning lotteries.
This idea of assessing the importance of nodes has been extended to channels by two methods, namely Network
Slimming (Liu et al., 2017) and Sparse Structure Selection (SSS) (Huang and Wang, 2018), that learn a measure of
importance as part of the training process. Both utilise a parameter gfor each channel (analogous to the attentional
strength) which scales the output of a channel. Given a loss function L, the new loss L0is deﬁned with an additional
regularization term over the scaling factors g:
L0=L+lå
ug(g) (14)
Where the function gis selected as the L1norm to reduce gtowards zero (as in Lasso regression).
The two methods differ in the way they implement the training process aimed at minimizing L0with Network Slim-
ming taking advantage of the use of batch normalization layers that are sometimes present following convolutional
layers while SSS implements a more general process that does not assume the presence of batch layers, and allows
use of scaling factors for blocks (such as residual and inception blocks) that can enable reduction of the depth of a
network.
Huang and Wang (2018) experiment with SSS on the CIFAR-10, CIFAR-100, and ImageNet data on VGG16, and
ResNet. For CIFAR10, SSS is able to reduce the number of parameters in VGG16 by 30% without loss of accuracy.
For ResNet-164, it is able to achieve a 2.5 times speedup at the cost of a 2% loss in accuracy for CIFAR-10 and
CIFAR-100.
For VGG16 on ImageNet, SSS is able to reduce the FLOPs by about 75%, though parameter reduction is minimal,
which is consistent with other methods given the large number of parameters in the fully connected layers in VGG16.
On ResNet50, SSS achieves a 15% reduction in FLOPs at a cost of a 0.1% reduction in Top-1 accuracy.
5.2 Pruning Weights with OBD and OBS
Several studies utilise the Taylor series to approximate the effect of weight perturbations on the loss function (LeCun
et al., 1990; Hassibi et al., 1993b; Wang et al., 2019). Given the change in weights DW, a Taylor Series approximation
of the change in loss DLcan be stated as (Bishop, 2006):
DL=¶LT
¶WDW+1
2DWTHDW+O 
kdWk3
(15)
Where His a Hessian matrix whose elements are the second order derivatives of the loss with respect to the weights:
Hi j=¶2L
¶wi¶wj(16)
12

--- PAGE 13 ---
Methods for Pruning Deep Neural Networks A P REPRINT
Most methods that adopt this approximation assume that the third order term is negligible. In Optimal Brain Damage
(OBD), LeCun et al. (1990), also assume that the ﬁrst order term can be ignored given that the network will have been
trained to achieve a local minima, resulting in a simpliﬁed quadratic approximation:
DL=1
2DWTHDW (17)
Given the large number of weights, computing the Hessian is computationally expensive, so they also assume that the
change in loss can be approximated by the diagonal elements of the Hessian, resulting in the following measure of the
saliency skof a weight wk:
sk=Hkkw2
k
2(18)
Where the second order derivatives, Hkkare computed in a manner similar to the way the gradient is computed in
backpropagation.
Hassibi et al. (1993b) argue that ignoring the non-diagonal elements of a Hessian is a strong assumption, and propose
an alternative pruning method, called Optimal Brain Surgeon (OBS), that aims to take account of all the elements of a
Hessian (Hassibi et al., 1993b,a).
Using a unit vector, em, to denote the selection of the mthweight as the one to be pruned, OBS reformulates pruning
as a constraint-based optimization task:
min
dw1
2dwTHdw
subject to eT
m:dw+dwm=0(19)
Formulating this with a Lagrangian multiplier, l, the task is to minimize:
1
2dwT:H:dw+l 
eT
m:dw+dwm
(20)
By taking derivatives and utilizing the above constraint, they show the saliency, skof weight wkcan be computed
using:
sk=1
2w2
k
[H 1]k;k(21)
They show that on the XOR problem, modelled using a MLP network with 2 inputs, 2 hidden layer nodes and one
output, OBS is better at detecting the correct weights to delete than OBD or magnitude pruning. They also show
that OBS is able to signiﬁcantly reduce the number of weights required for neural networks trained on the Monk
problems (Thrun, 1991) and for NetTalk (Sejnowski and Rosenberg, 1987), one of the classical applications of neural
networks, it is able to reduce the number of weights required from 18000 to 1560.
5.3 Pruning Feature Maps with First-Order Taylor Approximations
The methods described in Subsection 5.2 focus on the effect of removing weights in a fully connected network.
Molchanov et al. (2016) introduce a method that uses the Taylor series to approximate what happens if a feature map
is removed. In contrast to OBD and OBS, which assume that the ﬁrst order term can be ignored, they adopt a ﬁrst
order approximation, ignoring the higher order terms, primarily on grounds of computational complexity. Using a
ﬁrst order approximation seems odd given the convincing argument for ignoring these terms; however they argue that
although the ﬁrst order gradient tends to zero, the expected value of the change in loss is proportional to the variance,
which is not zero and is a measure of the stability as a local solution is reached. Given a feature map with Nelements
Yi;j, the ﬁrst order approximation using the Taylor series leads to the following measure of the absolute change in
loss (Molchanov et al., 2016):
DL(Y) =1
Nå
i;j¶L
¶Yi;jYi;j(22)
The scale of this measure will vary in different layers, and they therefore apply L2normalization within each layer.
The pruning process they adopt involves selecting a feature map using the measure, pruning it, and then ﬁne-tuning the
network before repeating the process until a stopping condition, that takes account of the need to reduce the number
of FLOPs while maintaining accuracy, is met. Their experiments reveal several interesting ﬁndings:
13

--- PAGE 14 ---
Methods for Pruning Deep Neural Networks A P REPRINT
1. From experiments on VGG16 and AlexNet on the UCSD-Birds and Oxford-Flowers data, they show that
the features maps selected by their criteria correlate signiﬁcantly more closely to those selected by an oracle
method than OBD and APoZ. On the ImageNet data, they ﬁnd that OBD correlates best when AlexNet is
used.
2. In experiments on transfer learning, where they ﬁne-tune VGG16 on the UCSD-Birds data, they present
results showing that their method performs better than APoZ and OBD as the number of parameters pruned
increases. In an experiment in which AlexNet is ﬁne-tuned for the Oxford Flowers data, they show that both
their method and OBD perform better than APoZ.
3. In a striking example of the potential beneﬁts of pruning, they demonstrate their method on a network for
recognizing hand gestures that requires over 37 GFLOPs for a single inference but only requires 3 GFLOPs
after pruning, all be it with a 2.6% reduction in accuracy.
In a follow up publication, Molchanov et al. (2019) acknowledge some limitations of the above approach, namely that
assuming that all layers have the same importance does not work for skip connections (used in the ResNet architecture)
and that assessing the impact of changes in feature maps leads to increases in memory requirements. They therefore
propose an alternative formulation, also using a Taylor series approximation, but based on estimating the squared loss
due to the removal of the mthparameter:
(DLm)2=
gmwm+1
2wmHmW2
(23)
Where gmis the ﬁrst order gradient and Hmis the mthrow of the Hessian matrix. The measure of importance of a ﬁlter
is then obtained by summing the contributions due to each parameter in a ﬁlter.
The pruning algorithm employed proceeds as follows. In each epoch, they utilise a ﬁxed number of mini-batches to
estimate the importance of each ﬁlter and then, based on their importance, a predeﬁned number of ﬁlters is removed.
The network is then ﬁne-tuned, and the process repeated until a pruning goal, such as the desired number of ﬁlters or
a limit for an acceptable drop in accuracy is reached.
Molchanov et al. (2019) carry out initial experiments on versions of LeNet and ResNet on the CIFAR10 data, using
both the second and ﬁrst order approximations (in equation 23) and given that the results from both correlate well with
an oracle method, they utilise the ﬁrst-order measure which is signiﬁcantly more efﬁcient to compute.
In experiments with versions of ResNet, VGG, and DenseNet on the ImageNet data, they consider the effect of using
the measure of importance at points before and after the batch normalization layers, and conclude that the latter option
results in greater correlation with an oracle method. The results from their method show that it works especially well
on pruning ResNet-50 and ResNet-34, outperforming the results from ThiNet and NISP. The reported results for other
networks are also impressive, with their method able to prune 76% of the parameters in VGG with a 0.19% loss in
accuracy and able to reduce the number of parameters in DenseNet by 43% at the expense of a 0.29% reduction in
accuracy.
5.4 Pruning Feature Maps with Second-Order Taylor Approximations
The ﬁrst order methods described above assume minimal interaction across channels and ﬁlters. This section summa-
rizes recent pruning methods that aim to take account of the effect of the potential dependencies amongst the channels
and ﬁlters.
In a method called EigenDamage, that also utilises the Taylor series approximation, Wang et al. (2019) revisit the
assumptions made by OBD and OBS when approximating the Hessian. To motivate their method, they begin by
illustrating that although OBS is better than OBD when pruning one weight at a time, it is not necessarily superior when
pruning multiple weights at a time. This is primarily because OBS does not correctly model the effect of removing
multiple weights, especially when they are correlated. To avoid this problem, they utilise a Fisher Information Matrix
to approximate the Hessian and then they utilise a method, proposed by Grosse and Martens (2016), to represent a
Fisher Matrix by a Kronecker Factored Eigenbasis (KFE). This reparameterization allows pruning to be done in a new
space in which the Fisher Matrix is approximately diagonal. Pruning can thus be done by ﬁrst mapping the weights to
a KFE space in which they are approximately independent, and then mapping back the results to the original space.
EigenDamage is evaluated on VGG, and ResNet on the CIFAR10, CIFAR100 and the Tiny-ImageNet data. Exper-
iments are carried out with one-shot pruning where ﬁne-tuning is performed at the end and with iterative pruning
in which ﬁne-tuning is performed after each cycle. In both cases, the results show that EigenDamage outperforms
adapted versions of OBD, OBS and Network Slimming (Liu et al., 2017).
14

--- PAGE 15 ---
Methods for Pruning Deep Neural Networks A P REPRINT
Peng et al. (2019) also utilise a Taylor series approximation to develop a Collaborative Channel Pruning (CCP) method
that is based on a measure of the impact of a combination of channels. Given a mask b, where bi=1, indicates the
retention of a channel and bi=0, indicates a channel to be pruned, they formulate the task as one to ﬁnd the bithat
minimize the loss L:
L(b;W) =L(W)+co
å
i=1(bi 1)gT
iwi+1
2co
å
i=1;j=1(bi 1)(bj 1)wT
iHi;jwj (24)
Where giare the ﬁrst order derivatives of the loss with respect to the weights in the ithoutput channel, Hi;jare Hessians,
andcodenotes the number of output channels.
By setting ui=gT
iwiandsi;j=1
2wT
iHi;jwjthe above equation can be written as the following 0-1 quadratic optimiza-
tion problem (Peng et al., 2019):
min
bico
å
i=1ui(bi 1)+co
å
i=1;j=1si;j(bi 1)(bj 1)
subject to:kbk0=pandbi2f0;1g(25)
Where pdenotes the number of channels to be retained in a layer. They note that the gradients giand hence uican
be computed in linear time. However, given the complexity of computing the Hessian matrices, they derive ﬁrst order
approximations for the loss functions, which they adopt when computing si;j. To solve the quadratic optimization
problem, they relax the constraint to bi2[0;1]and use a quadratic programming method to ﬁnd the biwhich are used
to select the top pchannels to retain. They apply the optimization process on each layer to obtain the masks bi, use
these to prune and then perform ﬁne-tuning at the end.
An empirical evaluation of CCP is carried out by pruning the ResNet models trained on the CIFAR10 and ImageNet
data, and the results compared to several methods including: pruning small ﬁlters, ThinNet, optimizing channel prun-
ing, Soft Filter pruning (He et al., 2018a), NISP (Yu et al., 2018) and AutoML (He et al., 2018b). For CIFAR10, the
experiments are carried out with a pruning rate of 35% and 40%, and in each case, CCP has a smaller drop in accuracy
(0.04% and 0.08% respectively) than the other methods, with the exception of the method for pruning small ﬁlters
which results in a small improvement in accuracy (0.02%). However, the pruning small ﬁlters method has a much
lower reduction in the FLOPS (27.6%) in comparison to CCP (52.6%). The results for ImageNet show, that for similar
reductions in FLOPS, CCP has less of a drop in accuracy than the other methods.
It’s worth noting, that like EigenDamage, CCP is able to obtain good results without the need for an iterative process
that uses ﬁne-tuning after pruning each layer.
6 A Resource for Comparing Published Results
As the above sections describe, previous studies of pruning report results on varying data sets, architectures and
methods that have evolved with time, making comparison of results across the different studies difﬁcult. The survey
provides a resource in the form of a pivot table that can be used by the community to explore the reported performance
of over 50 methods on different architectures and data. Appendix D shows how many times each combination of data
and architecture has been used, indicating the wide variety of comparisons possible.
To illustrate the use of the resource, we use it to compare the reported results on two combinations of architecture and
data for which there are a signiﬁcant number of comparisons across different pruning methods, namely: (i) AlexNet
and ResNet50 on ImageNet and (ii) ResNet56 and VGG16 on CIFAR10. Figure 4 show the results reported in terms
of the drop in Top-1 accuracy, and percent reduction in FLOPs or parameters where the labels used for the pruning
methods are from the primary sources, with sufﬁxes reﬂecting the variations in pruning methodology used. The main
observations are that:
1. For AlexNet on ImageNet, AOFP-B2 achieves a 41% reduction in FLOPs with a 0.46% increase in accuracy
and Dyn Surg (Guo et al., 2016), SSR-L (Lin et al., 2020d) and NeST (Dai et al., 2019) achieve over 93%
reduction in parameters without loss in accuracy. Other methods that compromise accuracy do not necessarily
result in a greater reduction in parameters.
2. In comparison to AlexNet, it is harder to prune ResNet50 on ImageNet, although AOFP-C1 achieves 33%
reduction in FLOPs without affecting accuracy. As accuracy is compromised, there are methods that show
signiﬁcant reductions in parameters. These include KSE and ThinNet with reductions in parameters by 78%
and 95%, with a decline of 0.64%, and 0.84% in accuracy, respectively.
15

--- PAGE 16 ---
Methods for Pruning Deep Neural Networks A P REPRINT
Figure 4: Results of pruning AlexNet and, ResNet50 on ImageNet (left column), and ResNet56 and VGG16 on the
CIFAR10 data (right column). Charts show the percent reduction in parameters where available (blue bars, left axis)
and FLOPs (orange bars, left axis), and reduction in baseline Top-1 accuracy (grey line, right axis). The labels used
for the methods are from the primary sources, with sufﬁxes reﬂecting the variations in pruning rates
3. When pruning ResNet56 on the CIFAR10 data, the methods KSE and SFP-NFT show reductions in FLOPs of
60% and 28% without compromising accuracy. For VGG16 on CIFAR10, AOFP, PF EC and NetSlimming
result in a 75%, 63%, and 51% reduction in ﬂops respectively without reductions in accuracy. For both net-
works, it appears to be difﬁcult to gain further reductions (beyond KSE and AOFP) even when compromising
accuracy.
4. The charts show that several methods are able to reduce FLOPs and parameters without compromising ac-
curacy and aid generalizability (e.g., AOFP, SFP, NetSlimming), though compromising accuracy a little can
sometimes lead to more signiﬁcant reductions in FLOPs and parameters.
5. When looking at results within methods, it is possible to conﬁrm our expectation that compromising accuracy
can result in greater reductions in parameters and FLOPS (e.g, see results for Filter Sketch and FPGM in
Fig.4(b)). However, this tradeoff is not evident when considering results across different pruning methods.
6. Although AOFP does perform well in retaining accuracy for three of the four cases, in general, the perfor-
mance of the methods varies depending on the architecture and the data set.
7 CONCLUSION AND FUTURE WORK
This paper has presented a survey of methods for pruning neural networks, focusing on methods based on magnitude
pruning, use of similarity and clustering, and methods based on sensitivity analysis.
Magnitude based pruning methods have developed from removal of small weights in MLPs to methods for pruning
ﬁlters and channels which lead to substantial reductions in the size of deep networks. The range of methods developed
include: (i) those that are data dependent and use examples to assess the relevance of output channels, (ii) methods that
16

--- PAGE 17 ---
Methods for Pruning Deep Neural Networks A P REPRINT
are independent of data, which assess the contributions of ﬁlters and channels directly, and (iii) methods that utilise
optimization algorithms to ﬁnd ﬁlters that approximate channels.
Methods based on sensitivity analysis are the most transparent in that they are based on approximating the loss due
to changes to a network. The development of methods based on a Taylor series approximation represents the primary
line of research in this category of methods. Different studies have adopted different assumptions in order to make the
computation of the Taylor approximation feasible. In one of the ﬁrst studies, the OBD method assumed a diagonal
Hessian matrix, ignoring both ﬁrst order gradients and second order non-diagonal gradients. This was followed by
OBS, a method that aimed to take account of non-diagonal elements of the Hessian but has been shown to struggle
when pruning multiple weights that are correlated. The EigenDamage method aims to take better account of correla-
tions by approximating the Hessian with a Fisher Information Matrix and using a reparameterization to a new space
in which the weights are approximately independent. In an alternative approach, the Collaborative Channel Pruning
(CCP) method formulates the pruning task as a quadratic programming problem. Molchanov et al. (2016) develop
a method based on a ﬁrst-order approximation, arguing that the variance in the loss, as training approaches a local
solution, is an indicator of stability and provides a good measure of the importance of ﬁlters. In contrast to most of the
other methods that adopt layer by layer pruning with ﬁne tuning after each layer, both EigenDamage and CCP show
that it is possible to obtain good results with one-shot pruning followed by ﬁne-tuning. These three recent methods all
show good results on large scale networks and data sets, though direct empirical comparisons between them have yet
to be published. The survey also found two alternatives to use of Taylor series approximations: a method that aims to
learn which ﬁlters to prune (Huang et al., 2018) and a method based on the use of multi-armed bandits (Ameen and
Vadera, 2020), both of which have the potential to explore new avenues of research on pruning methods.
The survey reveals a number of positive results about the Lottery Hypothesis: Lotteries appear to perform well in
transfer learning, and lotteries exist for tasks such as NLP and for architectures such as LSTMs. Lotteries even seem
to be independent of the type of optimizer used during training. Much of the current research on lotteries is based on
deep networks, but it is interesting that one of the earliest papers in the ﬁeld demonstrates the need to overparameterize
a small feedforward network for modelling a 4-bit multiplexor. Thus, it might prove fruitful to explore the properties
of lotteries on smaller problems as well as the larger networks of today. The existence of good lotteries does appear
to depend on the ﬁne-tuning process adopted and an interesting observation, that challenges some of the empirical
studies reported, is that even random pruning can achieve good results following ﬁne-tuning (Mittal et al., 2019), so
further studies of how the remaining weights compensate for those that are removed, could results in new insights.
Although studies on lotteries provide valuable insight, further research on specialist hardware and libraries is needed
for methods that prune individual weights to become practical (Elsen et al., 2020).
The survey found least research on methods that use similarity and clustering to develop pruning methods. A method
that utilised a cosine similarity measure concluded that it was more suitable for MLPs than CNNs, while a method that
utilises agglomerative clustering of ﬁlters results in up to a 3-fold reduction on ResNet when it is applied to ImageNet.
These results suggest there is merit in developing a more theoretical understanding of the functional equivalence of
different classes of deep networks, analogous to the studies on equivalence of MLPs (Kurkov ´a and Kainen, 1994).
Given the different approaches to pruning, some may be complimentary, and there is some evidence that combining
them might result in further compression of networks. For example, He et al. (2017) present results showing that
combining their method based on the use of Lasso regression with factorization results in additional gains, and Han
et al. (2016b) use a pipeline of magnitude pruning, clustering and Huffman coding to increase the level of compression
that can be achieved.
One of the challenges in making sense of the empirical evaluations reported in the papers surveyed is that, as new
deep learning architectures have developed and as new methods have been published, the comparisons carried out
have evolved. The survey has therefore collated the published results of over 50 methods for a variety of data sets
and architecture that is available as a resource for other researchers. Section 6 presents a comparison based on this
data, highlighting the methods that work well for different architectures on the ImageNet and CIFAR10 data. The
comparison of published results suggests that signiﬁcant reductions can be obtained for AlexNet, ResNet and VGG,
though there is no single method that is best, and that it is harder to prune ResNet than the other architectures. One
can hypothesize that this is because its use of skip connections makes it more optimal, though this is something that
needs exploring. Likewise, given that different methods seem best for different architectures, it is worth studying and
developing methods for speciﬁc architectures. The data also reveals that there are limited evaluations on other networks
such as the InceptionNet, DenseNet, SegNet, FCN32 and datasets such as CIFAR100, Flowers102, CUB200-2011 (see
Appendix D). A comprehensive independent evaluation of the methods that includes consideration of the issues raised
by the Lottery hypothesis across a wider range of data and architectures would be a useful advance in the ﬁeld.
In conclusion, this survey has presented the key research directions in pruning neural networks by summarizing how
the ﬁeld has progressed from the early algorithms that focused on small fully connected networks to the much larger
17

--- PAGE 18 ---
Methods for Pruning Deep Neural Networks A P REPRINT
deep neural networks of today. The survey has aimed to highlight the motivations and insights identiﬁed in the papers,
and provides a resource for comparison of the reported results, architectures and data sets used in several studies which
we hope will be useful to researchers in the ﬁeld.3
3Resource is available by emailing the ﬁrst author.
18

--- PAGE 19 ---
Methods for Pruning Deep Neural Networks A P REPRINT
Appendix A: Categorisation of Studies on Pruning
Magnitude based pruning methods
1988-91 Kruschke (1988); Hanson and Pratt (1989); Weigend (1990); Weigend et al. (1991)
2011 Graves (2011)
2015 Han et al. (2015); Polyak and Wolf (2015)
2016 Guo et al. (2016); Hu et al. (2016); Wen et al. (2016)
2017 Aghasi et al. (2017); He et al. (2017); Li et al. (2017); Liu et al. (2017); Luo and Wu (2017); Luo et al. (2017)
Wang et al. (2017); Zhu and Gupta (2017)
2018 Chen et al. (2018); He et al. (2018a); Huang and Wang (2018); Lee et al. (2018a)
Li et al. (2018); Liu and Liu (2018); Luo and Wu (2018); Qin et al. (2018)
Yazdani et al. (2018); Ye et al. (2018); Yu et al. (2018); Zhang et al. (2018); Zhou et al. (2018b)
2019 Ding et al. (2019c,b); Dettmers and Zettlemoyer (2019); Frankle et al. (2019); Frankle and Carbin (2019)
Gui et al. (2019); He et al. (2019); Helwegen et al. (2019); Hou et al. (2019); Lee et al. (2019); Li et al. (2019b)
Liu et al. (2019b,c); Mittal et al. (2019); Morcos et al. (2019); Song et al. (2019); Xu et al. (2019)
Yu et al. (2019); Zhao et al. (2019a); Zhou et al. (2019); Zhu et al. (2019)
2020 Hubens et al. (2020); Kim et al. (2020); Li et al. (2020); Lin et al. (2020a,d); Niu et al. (2020)
Similarity and clustering methods
2017 RoyChowdhury et al. (2017)
2018 Dubey et al. (2018); Son et al. (2018)
2019 Ayinde et al. (2019); Ding et al. (2019a); Li et al. (2019c,e)
2021 Lin et al. (2021)
Sensitivity analysis methods
1988 Mozer and Smolensky (1988)
1990 LeCun et al. (1990)
1992-1993 Hassibi and Stork (1992); Hassibi et al. (1993b,a)
2006-2009 Xu and Ho (2006); Endisch et al. (2007, 2009)
2016 Cohen et al. (2016); Grosse and Martens (2016); Molchanov et al. (2016)
2017 Ameen (2017); Anwar et al. (2017); Dong et al. (2017)
Guo and Potkonjak (2017); Neklyudov et al. (2017)
2018 Lin et al. (2018b); Bao et al. (2018); Carreira-Perpin ´an and Idelbayev (2018); Jiang et al. (2018)
Huang et al. (2018); Huynh et al. (2018); Zhuang et al. (2018)
2019 Chen et al. (2019a); Deng et al. (2019); Jin et al. (2019); Lee et al. (2018b)
Li et al. (2019a); Molchanov et al. (2019); Peng et al. (2019); Qin et al. (2019); Wang et al. (2019); Xiao et al. (2019)
2020 Ameen and Vadera (2020)
Knowledge distillation methods
2006 Bucilu ˇa et al. (2006)
2014-2017 Romero et al. (2014); Hinton et al. (2015); Gregor Urban et al. (2017)
2019 Bao et al. (2019); Lemaire et al. (2019); Zhang et al. (2019b); Dong and Yang (2019)
2020 Kundu and Sundaresan (2020)
2021 Kaliamoorthi et al. (2021)
Methods based on rank and reconstruction
2013-2016 Sainath et al. (2013); Jaderberg et al. (2014); Lebedev et al. (2015); Zhang et al. (2016)
2018 Lin et al. (2018a)
2020 Lin et al. (2020b)
Quantization methods
2011 Vanhoucke et al. (2011)
2014-17 Denton et al. (2014); Gong et al. (2014); Chen et al. (2015); Hubara et al. (2016); Rastegari et al. (2016); Zhou et al. (2017)
2018 Jacob et al. (2018); Krishnamoorthi (2018); Liu et al. (2018b); Tung and Mori (2018)
2019 Banner et al. (2019); Chen et al. (2019b); Jung et al. (2019); Zhao et al. (2019b)
2020 Wang et al. (2020)
2021 Stock et al. (2021)
Architectural design methods
2017 Baker et al. (2017); Howard et al. (2017); Lin et al. (2017); Zoph and Le. (2017)
2018 Gordon et al. (2018); He et al. (2018b); Hsu et al. (2018); Liu et al. (2018a); Pham et al. (2018); Zhong et al. (2018)
2019 Dai et al. (2019); Real et al. (2019); Liu et al. (2019a); Tan et al. (2019); Zhang et al. (2019a)
2020 Elsen et al. (2020); Evci et al. (2020); Lin et al. (2020c)
Hybrid Methods
2016 Chung and Shin (2016); Han et al. (2016b)
2018-2019 Goetschalckx et al. (2018); Gadosey et al. (2019)
Survey Papers
1993 Reed (1993)
2018-2021 Cheng et al. (2018a,b); Lebedev and Lempitsky (2018); Elsken et al. (2019); Menghani (2021)
19

--- PAGE 20 ---
Methods for Pruning Deep Neural Networks A P REPRINT
Appendix B: Summary of Data Sets used in Comparing Pruning Methods
MNIST (LeCun et al., 1998): The MNIST (Modiﬁed National Institute of Standards and Technology) data set con-
sists of handwritten 28x28 images of digits. It has 60,000 examples of training data and 10,000 examples for
the test set.
PASCAL VOC (Everingham et al., 2015): The PASCAL VOC data sets have formed the basis of an annual compe-
titions from 2005 to 2012. The VOC 2007 data annotates objects in 20 classes and consists of 9,963 images
and 24,640 annotated objects. The VOC 2012 data, which consists of 11530 images, are annotated with
27450 regions of interest and 6929 segmentations.
CamVid (Brostow et al., 2009): CamVid (Cambridge-driving Labelled Video Database) is a data set with videos
captured from an automobile. In total over 10mins of video is provided together with over 700 images from
the videos that have been labelled. Each pixel of an image is labelled to indicate if it is part of an objects in
one of 32 semantic classes.
Oxford-Flowers (Nilsback and Zisserman, 2008): The Oxford-Flowers data consists of 102 classes of common
ﬂowers in the UK. It provides 2040 training images and 6129 images for testing.
LFW Huang et al. (2008): The LFW (Labelled Faces in the Wild) is one of the largest and widely used data sets to
evaluate face recognition algorithms. It includes 250x250 pixel images of over 5.7K individuals, with over
13K images in total.
CIFAR-10 &100 (Krizhevsky et al., 2009): The CIFAR-10 (Canadian Institute for Advanced Research) data set is
a collection of 32x32 colour images in 10 different classes. The data set splits into two sets: 50,000 images
for training and 10,000 for testing. CIFAR-100 is similar to CIFAR-10 but has 100 classes, where each class
has 500 training images and 100 test images.
ImageNet (Deng et al., 2009): ImageNet contains millions of images organized using the WordNet hierarchy. It has
over 14M images classiﬁed in over 21K groups and has provided the data sets for the ImageNet Large Scale
Visual Recognition Challenges (ILSVRC) held since 2010. It is one of the most widely used data sets in
benchmarking deep learning models and methods for pruning. A smaller subset known as TinyImageNet is
sometimes used and is also available (https://tiny-imagenet.herokuapp.com). It consists of 200 classes with
500 training, 50 validation and 50 testing images per class.
SVHN (Netzer et al., 2011): The SVHN (Street View House Number) data set is a collection of 600K, 32x32 images
of house numbers in Google Street View images. The data set provides 73,257 images for training and 26,032
for testing.
UCSD-Birds (Wah et al., 2011): The UCSD-Birds data set provides 11788 images of birds, labelled as one of 200
different types of species. The data is split into training and testing sets of 5994 and 5794 respectively.
Places365 (Zhou et al., 2014): Places365 is a data set with 8 million 200x200 pixel images of scenes labeled with
one of 434 categories, such as bridge, kitchen, boxing ring, etc. It provides 50 images per class for validation
and the test set consist of 500 images per class.
CASIA-WebFace (Yi, Dong et al., 2014): Zhen et al. 2014 [170] CASIA-WebFace is a data set that was created for
evaluating face recognition systems. It provides over 494K images of over 10K individuals.
WMT’14 En2De:(Bojar et al., 2014): WMT’14 En2De is one of the benchmark language data sets provided for a
task set for the Workshop on Statistical Machine Translation held in 2014. This data set consists of 4.5M
English-German pairs of sentences.
FashionMNIST (Xiao et al., 2017): FashionMNIST is an alternative to the MNIST data set with 28x28 images of
fashion products classiﬁed in 10 categories. Like MNIST, there are 60,000 images for training and 10,000
images for testing,
Appendix C: Summary of Notation
• In general, we use Xto denote input channels, Wto denote weights of ﬁlters and Yto denote output channels.
•Yij is used to denote the output feature map obtained by applying a ﬁlter Wj;ion input channels Xi
•wi,wj,wj;iare used to denote individual weights.
•bis used to denote a binary mask where bi=1 indicates that a feature map or ﬁlter should be retained and
b=0 indicates that it should be removed.
•Lis used to denotes a loss function
20

--- PAGE 21 ---
Methods for Pruning Deep Neural Networks A P REPRINT
•L0;L1;L2denote norms, with L0counting non-zero values, L1, being the sum of absolute. values, L2being
the square root of the sum of squares (Euclidean distance).
•kWknwill be used to indicate the use of a norm in an equation with the an equation, with the subscript n
indicating the speciﬁc norm.
•kWkF, known as the Frobenius norm is sometimes used to denote the application of the Euclidean distance
to the elements of a matrix
Appendix D: Number of Reported Results for a given Architecture and Data Set
CALTECH256
CamVid
CIFAR10
CIFAR100
CUB200-2011
Flowers102
ImageNet
MNIST
Pascal VOC
SVHN
Total
Architecture
AlexNet 22 22
DenseNet-100 3 3
DenseNet-40 3 1 2 3
FCN32 1 1
LeNet-300 9 9
LeNet5 23 23
Res101 6 6
Res110 13 13
Res152 3 3
Res164 1 1 1
Res18 1 4 1 1 3 8
Res20 6 6
Res32 17 10 17
Res34 3 3
Res50 26 26
Res56 20 1 21
SegNet 1 1
VGG16 1 13 1 3 1 10 24
VGG19 12 12 1 13
Total 1 1 48 13 3 1 59 28 1 2 115
21

--- PAGE 22 ---
Methods for Pruning Deep Neural Networks A P REPRINT
References
Aghasi A, Abdi A, Nguyen N, Romberg J (2017) Net-Trim: Convex Pruning of Deep Neural Networks with Perfor-
mance Guarantee. In: Proceedings of the 31st International Conference on Neural Information Processing Systems,
Curran Associates Inc., Red Hook, NY , USA, pp 3180–3189
Ameen S (2017) Optimizing Deep Learning Networks using Multi-Armed Bandits, PhD Thesis. Phd thesis, University
of Salford, Greater Manchester, UK
Ameen S, Vadera S (2020) Pruning Neural Networks Using Multi-Armed Bandits. The Computer Journal 63(7):1099–
1108, DOI 10.1093/comjnl/bxz078, URL https://academic.oup.com/comjnl/advance-article/doi/10.
1093/comjnl/bxz078/5574718
Anwar S, Hwang K, Sung W (2017) Structured Pruning of Deep Convolutional Neural Networks. J Emerg Technol
Comput Syst 13(3):1–18
Arbib MA (2003) The handbook of brain theory and neural networks. MIT press
Ayinde BO, Inanc T, Zurada JM (2019) Redundant feature pruning for accelerated inference in deep neural networks.
Neural Networks 118:148–158
Baker B, Gupta O, Naik N, Raskar R (2017) Designing Neural Network Architectures using Reinforcement Learning.
In: International Conference on Learning Representations, URL https://arxiv.org/abs/1611.02167 ,1611.
02167
Banner R, Nahshan Y , Soudry D (2019) Post training 4-bit quantization of convolutional networks for rapid-
deployment. In: Advances in Neural Information Processing Systems, pp 7948–7956
Bao R, Yuan X, Chen Z, Ma R (2018) Cross-Entropy Pruning for Compressing Convolutional Neural Networks. Neural
Comput 30(11):3128–3149
Bao Z, Liu J, Zhang W (2019) Using Distillation to Improve Network Performance after Pruning and Quantization. In:
Proceedings of the 2019 2nd International Conference on Machine Learning and Machine Intelligence, Association
for Computing Machinery, New York, NY , USA, MLMI 2019, pp 3–6
Bellemare MG, Naddaf Y , Veness J, Bowling M (2015) The arcade learning environment: An evaluation platform for
general agents. Journal of Artiﬁcial Intelligence Research 47:253–279
Bishop CM (2006) Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag
New York, Inc.
Bojar O, Buck C, Federmann C, Haddow B, Koehn P, Leveling J, Monz C, Pecina P, Post M, Saint-Amand H, Soricut
R, Specia L, Tamchyna A (2014) Findings of the 2014 Workshop on Statistical Machine Translation. In: Proceedings
of the Ninth Workshop on Statistical Machine Translation, pp 12–58
Brockman G, Cheung V , Pettersson L, Schneider J, Schulman J, Tang J, Zaremba W (2016) OpenAI Gym. CoRR
abs/1606.0, URL http://arxiv.org/abs/1606.01540 ,1606.01540
Brostow GJ, Fauqueur J, Cipolla R (2009) Semantic object classes in video: A high-deﬁnition ground truth database.
Pattern Recognition Letters 30(2):88–97
Bucilu ˇa C, Caruana R, Niculescu-Mizil A (2006) Model compression. In: Proceedings of the 12th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, ACM, pp 535–541
Carreira-Perpin ´an MA, Idelbayev Y (2018) “Learning-Compression” Algorithms for Neural Net Pruning. In: Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 8532–8541
Chauvin Y (1988) A back-propagation algorithm with optimal use of hidden units. In: Advances in Neural Information
Processing Systems, pp 519–526
Chen AM, Lu Hm, Hecht-Nielsen R (1993) On the Geometry of Feedforward Neural Network Error Surfaces. Neural
computation 5(6):910–927
Chen C, Tung F, Vedula N, Mori G (2018) Constraint-aware deep neural network compression. In: Proceedings of the
European Conference on Computer Vision (ECCV), pp 400–415
Chen S, Lin L, Zhang Z, Gen M (2019a) Evolutionary NetArchitecture Search for Deep Neural Networks Pruning.
In: Proceedings of the 2019 2nd International Conference on Algorithms, Computing and Artiﬁcial Intelligence,
Association for Computing Machinery, New York, NY , USA, ACAI 2019, pp 189–196
Chen S, Wang W, Pan SJ (2019b) MetaQuant: Learning to Quantize by Learning to Penetrate Non-differentiable
Quantization. In: Advances in Neural Information Processing Systems, pp 3918–3928
22

--- PAGE 23 ---
Methods for Pruning Deep Neural Networks A P REPRINT
Chen W, Wilson JT, Tyree S, Weinberger KQ, Chen Y (2015) Compressing Neural Networks with the Hashing Trick.
International Conference on Machine Learning pp 2285–2294
Cheng J, Wang Ps, Li G, Hu Qh, Lu Hq (2018a) Recent advances in efﬁcient computation of deep convolutional neural
networks. Frontiers of Information Technology & Electronic Engineering 19(1):64–77
Cheng Y , Wang D, Zhou P, Zhang T (2018b) Model Compression and Acceleration for Deep Neural Networks: The
Principles, Progress, and Challenges. IEEE Signal Processing Magazine 35(1):126–136
Chung J, Shin T (2016) Simplifying Deep Neural Networks for Neuromorphic Architectures. In: Proceedings of the
53rd Annual Design Automation Conference, Association for Computing Machinery, New York, NY , USA, DAC
-16, pp 1–6, DOI 10.1145/2897937.2898092, URL https://doi.org/10.1145/2897937.2898092
Cohen JP, Lo HZ, Ding W (2016) RandomOut: Using a convolutional gradient norm to rescue convolutional ﬁlters.
arXiv preprint arXiv:160205931 URL https://arxiv.org/abs/1602.05931 ,1602.05931
Courbariaux M, Hubara I, Soudry D, El-Yaniv R, Bengio Y (2016) BinaryNet: Training Deep Neural Networks with
Weights and Activations Constrained to+ 1 or-1. arXiv preprint arXiv:160202830
Dai X, Yin H, Jha NK (2019) NeST: A neural network synthesis tool based on a grow-and-prune paradigm. IEEE
Transactions on Computers 68(10):1487–1497
Deng J, Dong W, Socher R, Li LJ, Li K, Li F (2009) Imagenet: A large-scale hierarchical image database. In: IEEE
Conference on Computer Vision and Pattern Recognition,, IEEE, pp 248–255
Deng W, Zhang X, Liang F, Lin G (2019) An Adaptive Empirical Bayesian Method for Sparse Deep Learning. In:
Advances in Neural Information Processing Systems, pp 5564–5574
Denil M, Shakibi B, Dinh L, de Freitas N (2013) Predicting parameters in deep learning. In: Advances in Neural
Information Processing Systems, pp 2148–2156
Denton EL, Zaremba W, Bruna J, LeCun Y , Fergus R (2014) Exploiting linear structure within convolutional networks
for efﬁcient evaluation. In: Advances in Neural Information Processing Systems, pp 1269–1277
Dettmers T, Zettlemoyer L (2019) Sparse networks from scratch: Faster training without losing performance. arXive
preprint arXiv:190704840 1907.04840
Ding X, Ding G, Guo Y , Han J (2019a) Centripetal SGD for pruning very deep convolutional networks with compli-
cated structure. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 4943–
4953
Ding X, Ding G, Guo Y , Han J, Yan C (2019b) Approximated Oracle Filter Pruning for Destructive CNN Width
Optimization. arXiv preprint arXiv:190504748
Ding X, DIng G, Zhou X, Guo Y , Han J, Liu J (2019c) Global Sparse Momentum SGD for Pruning Very Deep Neural
Networks. In: Advances in Neural Information Processing Systems, pp 6379–6391
Dong X, Yang Y (2019) Network Pruning via Transformable Architecture Search. arXiv preprint arXiv:190509717
Dong X, Chen S, Pan SJ (2017) Learning to Prune Deep Neural Networks via Layer-Wise Optimal Brain Surgeon.
In: Proceedings of the 31st International Conference on Neural Information Processing Systems, Curran Associates
Inc., Red Hook, NY , USA, NIPS-17, pp 4860–4874
Dubey A, Chatterjee M, Ahuja N (2018) Coreset-based neural network compression. In: Proceedings of the European
Conference on Computer Vision (ECCV), pp 454–470
Elsen E, Dukhan M, Gale T, Simonyan K (2020) Fast sparse convnets. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pp 14629–14638
Elsken T, Metzen JH, Hutter F (2019) Neural Architecture Search: A Survey. Journal of Machine Learning Research
20:1–21
Endisch C, Hackl C, Schr ¨oder D (2007) Optimal brain surgeon for general dynamic neural networks. In: Proceedings
of the Aritﬁcial Intelligence 13th Portuguese Conference on Progress in Artiﬁcial Intelligence, Springer-Verlag,
Berlin, Heidelberg, EPIA’07, p 15–28
Endisch C, Stolze P, Endisch P, Hackl C, Kennel R (2009) Levenberg-marquardt-based OBS algorithm using adap-
tive pruning interval for system identiﬁcation with dynamic neural networks. In: Proceedings of the 2009 IEEE
International Conference on Systems, Man and Cybernetics, IEEE Press, SMC’09, pp 3402–3408
Evci U, Gale T, Menick J, Castro PS, Elsen E (2020) Rigging the lottery: Making all tickets winners. In: Proceedings
of the 37th International Conference on Machine Learning, 1911.11134
23

--- PAGE 24 ---
Methods for Pruning Deep Neural Networks A P REPRINT
Everingham M, Eslami SMA, Gool LV , Williams CKI, Winn J, Zisserman A (2015) The PASCAL Visual Object
Classes Challenge: A Retrospective. International Journal of Computer Vision 111(1):98–136
Frankle J, Carbin M (2019) The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. In: Interna-
tional Conference on Learning Representations, New Orleans, Louisiana, United States, URL https://iclr.cc/
Conferences/2019
Frankle J, Dziugaite GK, Roy DM, Carbin M (2019) Stabilizing the Lottery Ticket Hypothesis. arXiv preprint
arXiv:190301611 URL http://arxiv.org/abs/1903.01611 ,1903.01611
Gadosey PK, Li Y , Yamak PT (2019) On Pruned, Quantized and Compact CNN Architectures for Vision Applica-
tions: An Empirical Study. In: Proceedings of the International Conference on Artiﬁcial Intelligence, Information
Processing and Cloud Computing, Association for Computing Machinery, New York, NY , USA, AIIPCC -19, pp
1–8, DOI 10.1145/3371425.3371481, URL https://doi.org/10.1145/3371425.3371481
Goetschalckx K, Moons B, Wambacq P, Verhelst M (2018) Efﬁciently Combining SVD, Pruning, Clustering and
Retraining for Enhanced Neural Network Compression. In: Proceedings of the 2nd International Workshop on
Embedded and Mobile Deep Learning, Association for Computing Machinery, New York, NY , USA, EMDL-18, pp
1–6, DOI 10.1145/3212725.3212733, URL https://doi.org/10.1145/3212725.3212733
Gong Y , Liu L, Yang M, Bourdev L (2014) Compressing Deep Convolutional Networks using Vector Quantization.
arXiv preprint arXiv:14126115
Goodfellow I, Bengio Y , Courville A (2016) Deep learning. MIT press
Gordon A, Eban E, Nachum O, Chen B, Wu H, Yang TJ, Choi E (2018) MorphNet: Fast & Simple Resource-
Constrained Structure Learning of Deep Networks. In: IEEE Cionference on Computer Vision and Pattern Recog-
nition, Salt Lake CIty,UT, pp 1586–1595
Graves A (2011) Practical variational inference for neural networks. In: Advances in Neural Information Processing
Systems, pp 2348–2356
Gregor Urban, Geras KJ, Kahou SE, Ozlem Aslan S, Wang RC, Mohamed A, Philipose M, Richardson M (2017) Do
deep convolutional nets really need to be deep and convolutional? In: Proceedings of the International Conference
on Learning Representations, pp 1–13
Grosse R, Martens J (2016) A Kronecker-factored approximate Fisher matrix for convolution layers. In: International
Conference on Machine Learning, pp 573–582
Gui S, Wang HN, Yang H, Yu C, Wang Z, Liu J (2019) Model Compression with Adversarial Robustness: A Uniﬁed
Optimization Framework. In: Advances in Neural Information Processing Systems, pp 1283–1294
Guo J, Potkonjak M (2017) Pruning Filters and Classes: Towards On-Device Customization of Convolutional Neural
Networks. In: Proceedings of the 1st International Workshop on Deep Learning for Mobile Systems and Applica-
tions, Association for Computing Machinery, New York, NY , USA, EMDL-17, pp 13–17
Guo Y , Yao A, Chen Y (2016) Dynamic network surgery for efﬁcient DNNs. In: Proc. of the 30th Internatonal
Conference on Neural Information Processing Systems, pp 1387–1395
Han S, Pool J, Tran J, Dally WJ (2015) Learning both weights and connections for efﬁcient neural networks. arXiv
preprint arXiv:150602626
Han S, Liu X, Mao H, Pu J, Pedram A, Horowitz MA, Dally WJ (2016a) EIE: efﬁcient inference engine on compressed
deep neural network. arXiv preprint arXiv:160201528
Han S, Mao H, Dally WJ (2016b) A deep neural network compression pipeline: Pruning, quantization, huffman
encoding. ICLR URL https://arxiv.org/abs/1510.00149
Hanson SJ, Pratt LY (1989) Comparing biases for minimal network construction with back-propagation. In: Advances
in Neural Information Processing Systems, pp 177–185
Hassibi B, Stork DG (1992) Second order derivatives for network pruning: Optimal Brain Surgeon. In: Advances in
Neural Information Processing Systems, Morgan Kaufmann, pp 164–171
Hassibi B, Stork DG, Wolff G, Watanabe T (1993a) Optimal Brain Surgeon: Extensions and performance comparison.
Neural Information Processing Systems pp 263–279
Hassibi B, Stork DG, Wolff GJ (1993b) Optimal Brain Surgeon and general network pruning. In: IEEE International
Conference on Neural Networks,, IEEE, pp 293–299
He K, Zhang X, Ren S, Sun J (2016) Deep Residual Learning for Image Recognition. Proceedings of the IEEE
Conferences on Computer Vision and Pattern Recognition pp 770–778
24

--- PAGE 25 ---
Methods for Pruning Deep Neural Networks A P REPRINT
He Y , Zhang X, Sun J (2017) Channel pruning for accelerating very deep neural networks. In: IEEE International
Conference on Computer Vision, pp 1398–1406
He Y , Kang G, Dong X, Fu Y , Yang Y (2018a) Soft ﬁlter pruning for accelerating deep convolutional neural networks.
In: Proceedings of the 27th International Joint Conference on Artiﬁcial Intelligence, AAAI Press, Stockholm,
Sweden, pp 2234–2240
He Y , Lin J, Liu Z, Wang H, Li LJ, Han S (2018b) AMC: Automl for model compression and acceleration on mobile
devices. In: Proceedings of the European Conference on Computer Vision (ECCV), pp 784–800
He Y , Liu P, Wang Z, Hu Z, Yang Y (2019) Filter pruning via geometric median for deep convolutional neural networks
acceleration. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 4340–4349
Helwegen K, Widdicombe J, Geiger L, Liu Z, Cheng KT, Nusselder R (2019) Latent Weights Do Not Exist: Rethinking
Binarized Neural Network Optimization. arXiv preprint arXiv:190602107
Hinton G, Li Deng DY , Dahl G, Mohamed Ar, Jaitly N, Senior A, Vanhoucke V , Nguyen P, Sainath T, Kingsbury B
(2012) Deep Neural Networks for Acoustic Modeling in Speech Recognition. IEEE pp 2–17, DOI 10.1109/MSP.
2012.2205597
Hinton G, Vinyals O, Dean J (2015) Distilling the knowledge in a neural network. arXiv preprint arXiv:150302531
Hou L, Zhu J, Kwok J, Gao F, Qin T, Liu Ty (2019) Normalization Helps Training of Quantized LSTM. In: Advances
in Neural Information Processing Systems, pp 7344–7354
Howard AG, Zhu M, Chen B, Kalenichenko D, Wang W, Weyand T, Andreetto M, Adam H (2017) MobileNets:
Efﬁcient Convolutional Neural Networks for Mobile Vision Applications. CoRR abs/1704.0, URL http://arxiv.
org/abs/1704.04861 ,1704.04861
Hsu CH, Chang SH, Liang JH, Chou HP, Liu CH, Chang SC, Pan JY , Chen YT, Wei W, Juan DC (2018) MONAS:
Multi-objective neural architecture search using reinforcement learning. arXiv preprint arXiv:180610332 1806.
10332
Hu H, Peng R, Tai YW, Tang CK (2016) Network Trimming: A data-driven neuron pruning approach towards efﬁcient
deep architectures. arXiv preprint arXiv:160703250
Huang GB, Mattar M, Berg T, Learned-Miller E (2008) Labeled faces in the wild: A database forstudying face
recognition in unconstrained environments. In: Workshop on Faces in’Real-Life’Images: Detection, Alignment,
and Recognition
Huang Q, Zhou K, You S, Neumann U (2018) Learning to Prune Filters in Convolutional Neural Networks. In: IEEE
Winter Conference on Applications of Computer Vision, pp 709–718
Huang Z, Wang N (2018) Data-driven sparse structure selection for deep neural networks. In: Proceedings of the
European Conference on Computer Vision (ECCV), pp 304–320
Hubara I, Courbariaux M, Soudry D, El-Yaniv R, Bengio Y (2016) Binarized neural networks. In: Proceedings of the
30th International Conference on Neural Information Processing Systems, pp 4114–4122
Hubens N, Mancas M, Decombas M, Preda M, Zaharia T, Gosselin B, Dutoit T (2020) An Experimental Study of the
Impact of Pre-Training on the Pruning of a Convolutional Neural Network. In: Proceedings of the 3rd International
Conference on Applications of Intelligent Systems, Association for Computing Machinery, New York, NY , USA,
APPIS 2020, DOI 10.1145/3378184.3378224, URL https://doi.org/10.1145/3378184.3378224
Huynh LN, Lee Y , Balan RK (2018) D-Pruner: Filter-Based Pruning Method for Deep Convolutional Neural Network.
In: Proceedings of the 2nd International Workshop on Embedded and Mobile Deep Learning, Association for
Computing Machinery, New York, NY , USA, EMDL-18, pp 7–12
Jacob B, Kligys S, Chen B, Zhu M, Tang M, Howard A, Adam H, Kalenichenko D (2018) Quantization and train-
ing of neural networks for efﬁcient integer-arithmetic-only inference. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pp 2704–2713
Jaderberg M, Vedaldi A, Zisserman A (2014) Speeding up convolutional neural networks with low rank expansions.
arXiv preprint arXiv:14053866
Jiang C, Li G, Qian C, Tang K (2018) Efﬁcient DNN Neuron Pruning by Minimizing Layer-wise Nonlinear Recon-
struction Error. In: Proceedings of the Twenty-Seventh International Joint Conference on Artiﬁcial Intelligence, pp
2298–2304
Jin S, Di S, Liang X, Tian J, Tao D, Cappello F (2019) DeepSZ: A Novel Framework to Compress Deep Neural
Networks by Using Error-Bounded Lossy Compression. In: Proceedings of the 28th International Symposium on
High-Performance Parallel and Distributed Computing, Association for Computing Machinery, New York, NY ,
USA, HPDC -19, pp 159–170
25

--- PAGE 26 ---
Methods for Pruning Deep Neural Networks A P REPRINT
Jung S, Son C, Lee S, Son J, Han JJ, Kwak Y , Hwang SJ, Choi C (2019) Learning to quantize deep networks by
optimizing quantization intervals with task loss. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp 4350–4359
Kaliamoorthi P, Siddhant A, Li E, Johnson M (2021) Distilling large language models into tiny and effective students
using pQRNN. arXiv preprint arXiv:210108890 2101.08890
Khan A, Anabia S, Zahoora U, Qureshi AS (2020) A survey of the recent architectures of deep convolutional neural
networks. Artiﬁcial Intelligence Review 53:5455–5516
Kim T, Ahn D, Kim JJ (2020) V-LSTM: An Efﬁcient LSTM Accelerator Using Fixed Nonzero-Ratio Viterbi-Based
Pruning. In: ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, Association for Com-
puting Machinery, New York, NY , USA, pp 326–326
Krishnamoorthi R (2018) Quantizing deep convolutional networks for efﬁcient inference: A whitepaper. arXiv preprint
arXiv:180608342 1806.08342
Krizhevsky A, Nair V , Hinton G (2009) The CIFAR-10 and CIFAR-100 datasets. online:
http://wwwcstorontoedu/kriz/cifarhtml
Krizhevsky A, Sutskever I, Hinton GE (2012) Imagenet classiﬁcation with deep convolutional neural networks. In:
Advances in Neural Information Processing Systems, pp 1097–1105
Kruschke JK (1988) Creating local and distributed bottlenecks in hidden layers of back-propagation networks. Pro-
ceedings 1988 Connectionist Models Summer School pp 120–126
Kundu S, Sundaresan S (2020) Attentionlite: Towards efﬁcient self-attention models for vision. arXiv preprint
arXiv:210105216 2101.05216
Kurkov ´a V , Kainen C Paul (1994) Functionally equivalent feedforward neural networks. Neural Computation 6:543–
558
Kuutti S, Bowden R, Jin Y , Barber P, Fallah S (2021) A survey of deep learning applications to autonomous vehicle
control. IEEE Transactions on Intelligent Transportation Systems 22(2):712–733, DOI 10.1109/TITS.2019.2962338
Lebedev V , Lempitsky V (2018) Speeding-up convolutional neural networks: A survey. Bulletin of the Polish Academy
of Sciences: Technical Sciences 66(6):799–810
Lebedev V , Ganin Y , Rakhuba M, Oseledets I, Lempitsky V (2015) Speeding-up Convolutional Neural Networks Us-
ing Fine-tuned CP-Decomposition. arXiv preprint arXiv: 14126553 URL http://arxiv.org/abs/1412.6553 ,
1412.6553v3
LeCun Y , Boser B, Denker JS, Henderson D, Howard RE, Hubbard W, Jackel LD (1989) Backpropagation applied to
handwritten zip code recognition. Neural computation 1(4):541–551
LeCun Y , Denker JS, Solla SA, Howard RE, Jackel LD (1990) Optimal brain damage. In: Touretzky D (ed) Proceed-
ings of Neural Information Processing Systems, Morgan Kaufmann Publishers, vol 89, pp 598–506
LeCun Y , Bottou L, Bengio Y , Haffner P (1998) Gradient-based learning applied to document recognition. Proceedings
of the IEEE 86(11):2278–2324
Lee D, Kang S, Choi K (2018a) ComPEND: Computation Pruning through Early Negative Detection for ReLU in
a Deep Neural Network Accelerator. In: Proceedings of the 2018 International Conference on Supercomputing,
Association for Computing Machinery, New York, NY , USA, ICS -18, pp 139–148
Lee K, Kim H, Lee H, Shin D (2019) Flexible Group-Level Pruning of Deep Neural Networks for Fast Inference
on Mobile GPUs: Work-in-Progress. In: Proceedings of the International Conference on Compliers, Architectures
and Synthesis for Embedded Systems Companion, Association for Computing Machinery, New York, NY , USA,
CASES -19, pp 1–2, DOI 10.1145/3349569.3351537, URL https://doi.org/10.1145/3349569.3351537
Lee N, Ajanthan T, Torr PHS (2018b) Snip: Single-shot network pruning based on connection sensitivity. arXiv
preprint arXiv:181002340
Lemaire C, Achkar A, Jodoin PM (2019) Structured Pruning of Neural Networks with Budget-Aware Regularization.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 9108–9116
Li G, Qian C, Jiang C, Lu X, Tang K (2018) Optimization Based Layer-Wise Magnitude-Based Pruning for DNN
Compression. In: Proceedings of the 27th International Joint Conference on Artiﬁcial Intelligence, AAAI Press,
IJCAI-18, pp 2383–2389
Li H, Kadav A, Durdanovic I, Samet H, Graf HP (2017) Pruning ﬁlters for efﬁcient convnets. In: 5th International
Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Pro-
ceedings, OpenReview.net, URL https://openreview.net/forum?id=rJqFGTslg
26

--- PAGE 27 ---
Methods for Pruning Deep Neural Networks A P REPRINT
Li H, Liu N, Ma X, Lin S, Ye S, Zhang T, Lin X, Xu W, Wang Y (2019a) ADMM-Based Weight Pruning for Real-Time
Deep Learning Acceleration on Mobile Devices. In: Proceedings of the 2019 on Great Lakes Symposium on VLSI,
Association for Computing Machinery, New York, NY , USA, GLSVLSI -19, pp 501–506, DOI 10.1145/3299874.
3319492, URL https://doi.org/10.1145/3299874.3319492
Li J, Qi Q, Wang J, Ge C, Li Y , Yue Z, Sun H (2019b) OICSR: Out-In-Channel Sparsity Regularization for Compact
Deep Neural Networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp
7046–7055
Li L, Zhu J, Sun M (2019c) Deep Learning Based Method for Pruning Deep Neural Networks. In: IEEE International
Conference on Multimedia & Expo Workshops (ICMEW), pp 312–317
Li Q, Li C, Chen H (2020) Incremental Filter Pruning via Random Walk for Accelerating Deep Convolutional Neural
Networks. In: Proceedings of the 13th International Conference on Web Search and Data Mining, Association for
Computing Machinery, New York, NY , USA, pp 358–366
Li X, Zhou Y , Pan Z, Feng J (2019d) Partial order pruning: for best speed/accuracy trade-off in neural architecture
search. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 9145–9153
Li Y , Lin S, Zhang B, Liu J, Doermann D, Wu Y (2019e) Exploiting kernel sparsity and entropy for interpretable cnn
compression. In: Computer Vision and Pattern Recognition, pp 2800–2809
Liberty E (2013) Simple and deterministic matrix sketching. In: Proceedings of ACM SIGKDD ¿ IEEE Transactions
on Neural Networks and Learning Systems: Paper submitted 19 International Conference on Knowledge Discovery
and Data Mining, pp 581–588
Lin J, Rao Y , Lu J, Zhou J (2017) Runtime Neural Pruning. In: Proceedings of the 31st International Conference on
Neural Information Processing Systems, Curran Associates Inc., Red Hook, NY , USA, pp 2178–2188
Lin M, Chen Q, Yan S (2014) Network in a Network. In: 2nd International Conference on Learning Representations
(ICLR), p arXiv 1312.4400, URL http://arxiv.org/abs/1312.4400
Lin M, Ji R, Li S, Ye Q, Tian Y , Liu J, Tian Q (2020a) Filter Sketch for Network Pruning. arXiv preprint arXiv
200108514 URL http://arxiv.org/abs/2001.08514 ,2001.08514
Lin M, Ji R, Wang Y , Zhang Y , Zhang B, Tian Y , Shao L (2020b) Hrank: Filter pruning using high-rank fea-
ture map. In: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE Com-
puter Society, Los Alamitos, CA, USA, pp 1526–1535, DOI 10.1109/CVPR42600.2020.00160, URL https:
//doi.ieeecomputersociety.org/10.1109/CVPR42600.2020.00160
Lin M, Ji R, Zhang Y , Zhang B, Wu Y , Tian Y (2020c) Channel Pruning via Automatic Structure Search. In:
Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence, pp 673–679, URL
http://arxiv.org/abs/2001.08565 ,2001.08565
Lin M, Ji R, Chen B, Chao F, Liu J, Zeng W, Tian Y , Tian Q (2021) Training compact cnns for image classiﬁcation
using dynamic-coded ﬁlter fusion. arXiv preprint arXiv 210706916 2107.06916
Lin S, Ji R, Chen C, Tao D, Luo J (2018a) Holistic CNN Compression via Low-rank Decomposition with Knowledge
Transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence 41(12):2889–2905
Lin S, Ji R, Li Y , Wu Y , Huang F, Zhang B (2018b) Accelerating Convolutional Networks via Global & Dynamic
Filter Pruning. In: Proceedings of the Twenty-Seventh International Joint Conference on Artiﬁcial Intelligence
(IJCAI-18), pp 2425–2432
Lin S, Ji R, Li Y , Deng C, Li X (2020d) Toward Compact ConvNets via Structure-Sparsity Regularized Filter Pruning.
IEEE Transactions on Neural Networks and Learning Systems 31(2):574–588
Liu C, Liu Q (2018) Improvement of Pruning Method for Convolution Neural Network Compression. In: Proceedings
of the 2018 2nd International Conference on Deep Learning Technologies, Association for Computing Machinery,
New York, NY , USA, ICDLT -18, pp 57–60, DOI 10.1145/3234804.3234824, URL https://doi.org/10.1145/
3234804.3234824
Liu C, Zoph B, Neumann M, Shlens J, Hua W, Li LJ, Fei-Fei L, Yuille A, Huang J, Murphy K (2018a) Progressive
neural architecture search. In: Proceedings of the European conference on computer vision (ECCV), pp 19–34
Liu Z, Li J, Shen Z, Huang G, Yan S, Zhang C (2017) Learning efﬁcient convolutional networks through network
slimming. In: Proceedings of IEEE Conference on Computer Vision, pp 2755–2763
Liu Z, Xu J, Peng X, Xiong R (2018b) Frequency-domain dynamic pruning for convolutional neural networks. In:
Advances in Neural Information Processing Systems, pp 1043–1053
27

--- PAGE 28 ---
Methods for Pruning Deep Neural Networks A P REPRINT
Liu Z, Mu H, Zhang X, Guo Z, Yang X, Cheng K, Sun J (2019a) Metapruning: Meta learning for automatic neural
network channel pruning. In: IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea
(South), October 27 - November 2, 2019, IEEE, pp 3295–3304, DOI 10.1109/ICCV .2019.00339, URL https:
//doi.org/10.1109/ICCV.2019.00339
Liu Z, Sun M, Zhou T, Huang G, Darrell T (2019b) Rethinking the value of network pruning. In: 7th International
Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, OpenReview.net,
URL https://openreview.net/forum?id=rJlnB3C5Ym
Liu Z, Tang H, Lin Y , Han S (2019c) Point-V oxel CNN for Efﬁcient 3D Deep Learning. In: Advances in Neural
Information Processing Systems, pp 963–973
Luo JH, Wu J (2017) An Entropy-based Pruning Method for CNN Compression. arXiv preprint arXiv:170605791
Luo JH, Wu J (2018) Autopruner: An end-to-end trainable ﬁlter pruning method for efﬁcient deep model inference.
arXiv preprint arXiv:180508941
Luo JH, Wu J, Lin W (2017) ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression. In:
IEEE International Conference on Computer Vision (ICCV), pp 5058–5066
McKinney S, Sieniek M, Godbole V , N A, H A, T B, M C, GC C, A D, M E, F GV , F G, M HB, D H, S J (2020)
International evaluation of an AI system for breast cancer screening. Nature 577:89–94
Menghani G (2021) Efﬁcient deep learning: A survey on making deep learning models smaller, faster, and better.
arXiv preprint arXiv: 210608962 URL https://arxiv.org/abs/2106.08962 ,2106.08962
Merity S, Xiong C, Bradbury J, Socher R (2017) Pointer sentinel mixture models. In: International Conference in
Learning Representations, p arXiv:1609.07843, DOI arXiv:1609.07843, URL https://arxiv.org/abs/1609.
07843
Mittal D, Bhardwaj S, Khapra MM, Ravindran B (2019) Studying the Plasticity in Deep Convolutional Neural Net-
works Using Random Pruning. Machine Vision Applications 30(2):203–216
Molchanov P, Tyree S, Karras T, Aila T, Kautz J (2016) Pruning Convolutional Neural Networks for Resource Efﬁcient
Transfer Learning. arXiv preprint arXiv:161106440
Molchanov P, Mallya A, Tyree S, Frosio I, Kautz J (2019) Importance Estimation for Neural Network Pruning. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 11264–11272
Morcos AS, Yu H, Paganini M, Tian Y (2019) One ticket to win them all: generalizing lottery ticket initializations
across datasets and optimizers. In: Proceeding of Neural Information Processing Systems (NeuralPS), URL https:
//arxiv.org/abs/1906.02773
Mozer MC, Smolensky P (1988) Skeletonization: A Technique for Trimming the Fat from a Network via Relevance
Assessment. Advances in Neural Information Processing Systems (NIPS) pp 107–115
Neklyudov K, Molchanov D, Ashukha A, Vetrov D (2017) Structured Bayesian Pruning via Log-Normal Multiplica-
tive Noise. In: Proceedings of the 31st International Conference on Neural Information Processing Systems, Curran
Associates Inc., Red Hook, NY , USA, NIPS-17, pp 6778–6787
Netzer Y , Wang T, Coates A, Bissacco A, Wu B, Ng AY (2011) Reading digits in natural images with unsupervised
feature learning. In: NIPS workshop on deep learning and unsupervised feature learning, URL http://ufldl.
stanford.edu/housenumbers/nips2011{_}housenumbers.pdf
Nilsback ME, Zisserman A (2008) Automated ﬂower classiﬁcation over a large number of classes. In: International
Conference on Computer Vision, Graphics & Image Processing, IEEE, pp 722–729
Niu W, Ma X, Lin S, Wang S, Qian X, Lin X, Wang Y , Ren B (2020) PatDNN: Achieving Real-Time DNN Execution
on Mobile Devices with Pattern-Based Weight Pruning. In: Proceedings of the Twenty-Fifth International Confer-
ence on Architectural Support for Programming Languages and Operating Systems, Association for Computing
Machinery, New York, NY , USA, ASPLOS -20, pp 907–922
Otter DW, Medina JR, Kalita JK (2021) A survey of the usages of deep learning for natural language processing. IEEE
Transactions on Neural Networks and Learning Systems 32(2):604–624, DOI 10.1109/TNNLS.2020.2979670
Peng H, Wu J, Chen S, Huang J (2019) Collaborative Channel Pruning for Deep Networks. In: International Confer-
ence on Machine Learning, pp 5113–5122
Pham H, Guan M, Zoph B, Le Q, Dean J (2018) Efﬁcient neural architecture search via parameters sharing. In: Dy
J, Krause A (eds) Proceedings of the 35th International Conference on Machine Learning, PMLR, pp 4095–4104,
URL http://proceedings.mlr.press/v80/pham18a.html
Polyak A, Wolf L (2015) Channel-level acceleration of deep face representations. IEEE Access 3:2163–2175
28

--- PAGE 29 ---
Methods for Pruning Deep Neural Networks A P REPRINT
Pouyanfar S, Sadiq S, Yan Y , Tian H, Tao Y , Reyes MP, Shyu ML, Chen SC, Iyengar SS (2019) A Survey on Deep
Learning: Algorithms, Techniques, and Applications. ACM Computing Surveys 51(5):Article 92, 36 pages
Qin Z, Yu F, Liu C, Chen X (2018) Demystifying neural network ﬁlter pruning. arXiv preprint arXiv: 181102639
Qin Z, Yu F, Liu C, Chen X (2019) CAPTOR :a class adaptive ﬁlter pruning framework for convolutional neural
networks in mobile applications. In: Proceedings of the 24th Asia and South Paciﬁc Design Automation, ACM
Press, New York, USA, pp 444–449
Rastegari M, Ordonez V , Redmon J, Farhadi A (2016) XNOR-Net: ImageNet Classiﬁcation Using Binary Convolu-
tional Neural Networks. arXiv preprint arXiv:160305279
Real E, Aggarwal A, Huang Y , Le QV (2019) Regularized evolution for image classiﬁer architecture search. In:
Proceedings of AAAI conference on Artriﬁcial Intgelligence, vol 33, pp 4780–4798
Reed R (1993) Pruning algorithms-a survey. IEEE Transactions on Neural Networks 4(5):740–747
Romero A, Ballas N, Kahou SE, Chassang A, Gatta C, Bengio Y (2014) Fitnets: Hints for thin deep nets. arXiv
preprint arXiv:14126550
RoyChowdhury A, Sharma P, Learned-Miller E, Roy A (2017) Reducing duplicate ﬁlters in deep neural networks. In:
NIPS workshop on deep learning: Bridging theory and practice, pp 1–7
Sainath TN, Kingsbury B, Sindhwani V , Arisoy E, Ramabhadran B (2013) Low-rank matrix factorization for deep
neural network training with high-dimensional output targets. In: IEEE International Conference on Acoustics,
Speech and Signal Processing, IEEE, pp 6655–6659
Sejnowski TJ (2020) The unreasonable effectiveness of deep learning in artiﬁcial intelligence. Proceedings of the
National Academy of Sciences 117(48):30033–30038, DOI 10.1073/pnas.1907373117, URL https://www.pnas.
org/content/117/48/30033 ,https://www.pnas.org/content/117/48/30033.full.pdf
Sejnowski TJ, Rosenberg CR (1987) Parallel networks that learn to pronounce English text. Complex Systems 1:145–
168
Simonyan K, Zisserman A (2015) Very deep convolutional networks for large-scale image recognition. International
Conference on Learning Representations p arXiv 1409.1556.
Son S, Nah S, Mu Lee K (2018) Clustering Convolutional Kernels to Compress Deep Neural Networks. In: Proceed-
ings of the European Conference on Computer Vision (ECCV), pp 225–240
Song J, Chen Y , Wang X, Shen C, Song M (2019) Deep Model Transferability from Attribution Maps. In: Advances
in Neural Information Processing Systems, pp 6179–6189
Srinivas S, Babu RV (2015) Data-free parameter pruning for Deep Neural Networks. arXiv preprint arXiv:150706149
Srivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R (2014) Dropout: A simple way to prevent neural
networks from overﬁtting. The Journal of Machine Learning Research 15(1):1929–1958
Stock P, Fan A, Graham B, Grave E, Gribonval R, Jegou H, Joulin A (2021) Training with quantization noise for
extreme model compression. In: Proceedings of the International Conference on Learning Representations, 2004.
07320
Sun Y , Wang X, X T (2016) Sparsifying Neural Network Connections for Face Recognition. In: IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV , USA, pp 4856–4864,
Sussmann HJ (1992) Uniqueness of weights for feedforward nets with a given input-output map. Neural Networks
5(4):589–593
Tan M, Chen B, Pang R, Vasudevan V , Sandler M, Howard A, Le QV (2019) Mnasnet: Platform-aware neural archi-
tecture search for mobile. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
pp 2815–2823, DOI 10.1109/CVPR.2019.00293
Thrun S (1991) The MONK”s Problems-A Performance Comparison of Different Learning Algorithms. Carnegie
Mellon University (CMU-CS-91-197)
Tung F, Mori G (2018) Clip-q: Deep network compression learning by in-parallel pruning-quantization. In: Proceed-
ings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 7873–7882
Vanhoucke V , Senior A, Mao MZ (2011) Improving the speed of neural networks on cpus. In: Deep Learning and
Unsupervised Feature Learning Workshop, Neural Information Processing Systems
Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser u, Polosukhin I (2017) Attention is all
you need. In: Proceedings of the 31st International Conference on Neural Information Processing Systems, Curran
Associates Inc., Red Hook, NY , USA, NIPS’17, p 6000–6010
29

--- PAGE 30 ---
Methods for Pruning Deep Neural Networks A P REPRINT
Wah C, Branson S, Welinder P, Perona P, Belongie S (2011) The Caltech-UCSD Birds-200-2011 Dataset CNS-TR-
201, URL http://www.vision.caltech.edu/visipedia/CUB-200-2011.html
Wang C, Grosse R, Fidler S, Zhang G (2019) EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis.
In: Proceedings of the 36th International Conference on Machine Learning, pp 6566–6575
Wang H, Zhang Q, Wang Y , Hu H (2017) Structured probabilistic pruning for convolutional neural network accelera-
tion. arXiv preprint arXiv:170906994
Wang P, Chen Q, He X, Cheng J (2020) Towards accurate post-training network quantization via bit-split and stitching.
In: Proceedings of Machine Learning Research, vol 119, pp 9847–9856
Weigend AS, Rumelhart DE, Huberman BA (1991) Generalization by weight-elimination applied to currency ex-
change rate prediction. In: Proceedings of International Joint Conference on Neural Networks, IEEE, pp 837–841
Weigend DE (1990) Back-propagation, weight-elimination and time series prediction. In: Proceedings 1990 Connec-
tionist Models Summer School, pp 105–116
Wen W, Wu C, Wang Y , Chen Y , Li H (2016) Learning Structured Sparsity in Deep Neural Networks. In: Proceedings
of the 30th International Conference on Neural Information, pp 2082–2090, URL http://arxiv.org/abs/1608.
03665
Xiao H, Rasul K, V ollgraf R (2017) Fashion-mnist: a novel image dataset for benchmarking machine learning algo-
rithms. arXiv preprint arXiv: 170807747 1708.07747
Xiao X, Wang Z, Rajasekaran S (2019) AutoPrune: Automatic Network Pruning by Regularizing Auxiliary Parame-
ters. In: Advances in Neural Information Processing Systems, pp 13681–13691
Xu J, Ho DWC (2006) A node pruning algorithm based on optimal brain surgeon for feedforward neural networks.
In: Proceedings of the Third International Conference on Advances in Neural Networks - V olume Part I, Springer-
Verlag, Berlin, Heidelberg, ISNN’06, p 524–529
Xu K, Wang X, Jia Q, An J, Wang D (2018) Globally Soft Filter Pruning For Efﬁcient Convolutional Neural Networks.
URL https://openreview.net/pdf?id=H1fevoAcKX
Xu Y , Wang Y , Zeng J, Han K, Chunjing XU, Tao D, Xu C (2019) Positive-Unlabeled Compression on the Cloud. In:
Advances in Neural Information Processing Systems, pp 2561–2570
Yazdani R, Riera M, Arnau JM, Gonz ´alez A (2018) The Dark Side of DNN Pruning. In: Proceedings of the 45th
Annual International Symposium on Computer Architecture, IEEE Press, ISCA -18, pp 790–801, DOI 10.1109/
ISCA.2018.00071, URL https://doi.org/10.1109/ISCA.2018.00071
Ye J, Lu X, Lin Z, Wang JZ (2018) Rethinking the smaller-norm-less-informative assumption in channel pruning of
convolution layers. arXiv preprint arXiv:180200124
Yi, Dong, Lei Z, Liao S, Li SZ (2014) Learning Face Representation from Scratch. arXiv preprint arXiv:14117923
abs/1411.7, URL http://arxiv.org/abs/1411.7923
Yu H, Edunov S, Tian Y , Morcos AS (2019) Playing the lottery with rewards and multiple languages: lottery tickets in
RL and NLP. arXiv preprint arXiv:190602768 URL http://arxiv.org/abs/1906.02768 ,1906.02768
Yu R, Li A, Chen CF, Lai JH, Morariu VI, Han X, Gao M, Lin CY , Davis LS (2018) Nisp: Pruning networks using
neuron importance score propagation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp 9194–9203
Zhang J, Chen X, Song M, Li T (2019a) Eager Pruning: Algorithm and Architecture Support for Fast Training of Deep
Neural Networks. In: Proceedings of the 46th International Symposium on Computer Architecture, Association
for Computing Machinery, New York, NY , USA, ISCA -19, pp 292–303, DOI 10.1145/3307650.3322263, URL
https://doi.org/10.1145/3307650.3322263
Zhang L, Tan Z, Song J, Chen J, Bao C, Ma K (2019b) SCAN: A Scalable Neural Networks Framework Towards
Compact and Efﬁcient Models. arXiv preprint arXiv:190603951
Zhang T, Ye S, Zhang K, Tang J, Wen W, Fardad M, Wang Y (2018) A systematic DNN weight pruning framework
using alternating direction method of multipliers. In: Proceedings of the European Conference on Computer Vision
(ECCV), pp 184–199
Zhang X, Zou J, He K, Sun J (2016) Accelerating very deep convolutional networks for classiﬁcation and detection,.
IEEE Transactions on Pattern Analysis and Machine Intelligence 38(10):1943–1955
Zhao C, Ni B, Zhang J, Zhao Q, Zhang W, Tian Q (2019a) Variational Convolutional Neural Network Pruning. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 2780–2789
30

--- PAGE 31 ---
Methods for Pruning Deep Neural Networks A P REPRINT
Zhao Y , Gao X, Bates D, Mullins R, Xu CZ (2019b) Focused Quantization for Sparse CNNs. In: Advances in Neural
Information Processing Systems, pp 5585–5594
Zhong Z, Yan J, Wu W, Shao J, Liu HL (2018) Practical block-wise neural network architecture generation. In: Pro-
ceedings IEEE Conference on Computer Vision and Pattern Recognition, pp 2423–2432
Zhou A, Yao A, Guo Y , Xu L, Chen Y (2017) Incremental network quantization: Towards lossless CNNs with low-
precision weights. arXiv preprint arXiv:170203044
Zhou B, Lapedriza A, Xiao J, Torralba A, Oliva A (2014) Learning Deep Features for Scene Recognition using Places
Database. In: Proceedings of 27th International Conference on Neural Information Processing Systems, pp 487–495
Zhou H, Lan J, Liu R, Yosinski J (2019) Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask. arXiv
preprint arXiv 190501067 URL http://arxiv.org/abs/1905.01067 ,1905.01067
Zhou Z, Zhou W, Hong R, Li H (2018a) Online Filter Clustering and Pruning for Efﬁcient Convnets. In: IEEE
International Conference on Image Processing, pp 11–15
Zhou Z, Zhou W, Hong R, Li H (2018b) Online ﬁlter weakening and pruning for efﬁcient convnets. In: 2018 IEEE
International Conference on Multimedia and Expo (ICME), IEEE, pp 1–6
Zhu M, Gupta S (2017) To prune, or not to prune: exploring the efﬁcacy of pruning for model compression. arXiv
preprint arXiv:171001878
Zhu Y , Li C, Luo B, Tang J, Wang X (2019) Dense Feature Aggregation and Pruning for RGBT Tracking. In: Pro-
ceedings of the 27th ACM International Conference on Multimedia, Association for Computing Machinery, New
York, NY , USA, MM -19, pp 465–472
Zhuang Z, Tan M, Zhuang B, Liu J, Guo Y , Wu Q, Huang J, Zhu J (2018) Discrimination-aware channel pruning for
deep neural networks. In: Advances in Neural Information Processing Systems, pp 875–886
Zoph B, Le QV (2017) Neural architecture search with reinforcement learning. In: International Conference on Learn-
ing Representations, URL https://arxiv.org/abs/1611.01578
31
