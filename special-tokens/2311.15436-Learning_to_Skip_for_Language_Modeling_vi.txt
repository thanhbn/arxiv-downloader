# 2311.15436.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/special-tokens/2311.15436.pdf
# Kích thước tệp: 703269 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Học Cách Bỏ Qua cho Mô Hình Ngôn Ngữ
Dewen Zeng1Nan Du1Tao Wang1Yuanzhong Xu2Tao Lei2Zhifeng Chen2Claire Cui2
Tóm tắt
Các mô hình ngôn ngữ quy mô lớn có quá nhiều tham số thể hiện hiệu suất tổng quát hóa ấn tượng trong việc học few-shot theo ngữ cảnh. Tuy nhiên, hầu hết các mô hình ngôn ngữ phân bổ cùng một lượng tham số hoặc tính toán cho mỗi token, bất chấp độ phức tạp hoặc tầm quan trọng của dữ liệu đầu vào. Chúng tôi lập luận rằng trong việc tiền huấn luyện mô hình ngôn ngữ, một lượng tính toán thay đổi nên được gán cho các token khác nhau, và điều này có thể đạt được một cách hiệu quả thông qua một cơ chế định tuyến đơn giản. Khác với các kỹ thuật dừng sớm thông thường nơi các token chỉ có thể thoát sớm ở các lớp đầu, chúng tôi đề xuất một phương pháp tổng quát hơn nhằm bỏ qua động việc thực thi một lớp (hoặc module) cho bất kỳ token đầu vào nào với một bộ định tuyến nhị phân. Trong đánh giá toàn diện của chúng tôi trên 24 nhiệm vụ NLP, chúng tôi chứng minh rằng phương pháp được đề xuất có thể cải thiện đáng kể hiệu suất 1-shot so với các baseline cạnh tranh khác chỉ với chi phí bổ sung nhẹ cho suy luận.

1. Giới thiệu
Các mô hình ngôn ngữ quy mô lớn dựa trên Transformer (Vaswani et al., 2017) được huấn luyện với corpus tổng quát đã cho thấy sự cải thiện to lớn về khả năng tổng quát hóa đặc biệt với việc học few-shot theo ngữ cảnh trong những năm gần đây (Shoeybi et al., 2019; Brown et al., 2020; Rae et al., 2021; Chowdhery et al., 2022; Hoffmann et al., 2022). Bất chấp khả năng sinh văn bản ấn tượng, việc huấn luyện và phục vụ các mô hình khổng lồ này không đơn giản ngay cả với tiến bộ gần đây của phần cứng và phần mềm (Jouppi et al., 2017; Lepikhin et al., 2021; Patterson et al., 2021). Một trong những thách thức chính là việc xử lý mỗi đầu vào yêu cầu kích hoạt tất cả các tham số của mô hình, điều này thường dẫn đến hàng nghìn tỷ phép toán dấu phẩy động (FLOP) cho mỗi dự đoán. Điều này tạo ra gánh nặng lớn cho cả việc huấn luyện và suy luận mô hình vì chúng ta không có quyền kiểm soát lượng tính toán có thể được gán cho mỗi ví dụ đầu vào.

Ngược lại, người ta thường tin rằng nhận thức con người (Stanovich & West, 2000; Levy, 2008) sử dụng các nỗ lực nhận thức khác nhau để vận hành và học tùy thuộc vào 'độ khó' của đầu vào. Cụ thể, một người có thể chỉ cần nỗ lực nhỏ (chi phí tính toán thấp hơn) để xử lý các ví dụ 'dễ', như các từ dừng thường được sử dụng, dấu câu, các mảng trong nền của một hình ảnh, v.v., nhưng cho phép nỗ lực bổ sung (chi phí tính toán nhiều hơn) cho các ví dụ 'khó', ví dụ, một khái niệm trừu tượng hiếm cho lý luận, khi chúng thực sự cần thiết. Do đó, việc phân bổ sức mạnh tính toán tương tự của một mô hình lớn một cách đồng đều cho việc xử lý tất cả các mẫu có xu hướng lãng phí và kém hiệu quả. Vấn đề như vậy thậm chí có thể trở nên trầm trọng hơn khi huấn luyện các mô hình lớn sử dụng corpus dữ liệu thế giới thực ở chỗ sự dư thừa của các ví dụ tầm thường sẽ rõ ràng hơn khi ngày càng nhiều dữ liệu được sử dụng.

Tính toán có điều kiện (Bengio et al., 2013; 2015) là mô hình nơi chỉ một tập hợp con nhỏ các tham số mô hình được kích hoạt dựa trên biểu diễn đầu vào, do đó giảm lượng tính toán cần thiết cho mỗi ví dụ. Tuy nhiên, do tính rời rạc của các quyết định dựa trên mỗi đầu vào, việc huấn luyện mạng neural với các thành phần được kích hoạt có điều kiện end-to-end có thể vi phân và hiệu quả vẫn đang thách thức.

Trong bài báo này, chúng tôi phát triển một framework đơn giản, được gọi là SkipLayer, cho phép một đầu vào bỏ qua bất kỳ lớp nào có thể được bọc bên trong nó dựa trên biểu diễn ngữ cảnh. Cụ thể hơn, các mô hình dựa trên SkipLayer có thể được huấn luyện end-to-end có thể vi phân trong khi cùng lúc các quyết định rời rạc trong quá trình truyền tiến vẫn có thể được tôn trọng, điều này cho phép chúng ta kiểm soát chính xác sự đánh đổi hiệu suất-tính toán thông qua ràng buộc bên ngoài. Hơn nữa, vì các quyết định rời rạc có thể được bảo toàn trong quá trình truyền tiến, chúng tôi cũng phát triển một triển khai hiệu quả để tính toán bổ sung có thể được tiết kiệm thêm trong cả tiền huấn luyện và suy luận cho ngân sách mục tiêu đã cho. Sau đó chúng tôi áp dụng SkipLayer cho kiến trúc Transformer (Vaswani et al., 2017) để chứng minh tiềm năng hiệu quả của phương pháp cho việc tiền huấn luyện và giải mã mô hình ngôn ngữ chỉ decoder. Cuối cùng, chúng tôi xác thực rộng rãi phương pháp của chúng tôi trên một bộ các benchmark NLP được thiết lập tốt từ các nhiệm vụ QA miền mở, đọc hiểu đến suy luận thường thức, đến các nhiệm vụ suy luận ngôn ngữ tự nhiên. Các mô hình dựa trên SkipLayer đã cho thấy hiệu suất 1-shot mạnh với sự đánh đổi tính toán có thể kiểm soát giữa chất lượng mô hình và hiệu quả giải mã so với nhiều baseline cạnh tranh.

2. Phương pháp
Trong phần này, chúng tôi trình bày chi tiết framework SkipLayer được đề xuất, triển khai hiệu quả của nó, và việc áp dụng cho các mô hình ngôn ngữ dựa trên Transformer.

2.1. SkipLayer
Cho Xo[layer] = F[layer](X|W) biểu thị một lớp được tham số hóa (hoặc module) của mạng neural với đầu vào X và đầu ra Xo cho một tập trọng số tùy chọn được biểu diễn bởi W. Ví dụ, một lớp FeedForward thông thường (FFN) có thể được biểu thị là XoFFN = FFFN(X|{Wi, Wo}) trong đó Wi ∈ Rm×h và Wo ∈ Rh×m lần lượt là trọng số đầu vào và đầu ra.

Một SkipLayer FSL được thiết kế để bọc một lớp hiện có sao cho

XoSL = FSL(F[layer](X)|WG)                (1)
= F[layer](X) ⊙ G(X|WG) + X ⊙ (1 − G(X|WG)),

trong đó G(X|WG) ∈ {0,1} là một hàm router với trọng số có thể học WG. Hình 1(a) cho thấy framework tổng thể.

Cho một batch X ∈ RB×T×d của B chuỗi, mỗi chuỗi có độ dài T và chiều embedding d, cho mỗi token đầu vào X[b, t] ∈ Rd, b ≤ B, t ≤ T, chúng ta có

XoSL[b, t] = {
F[layer](X[b, t]), nếu G(X[b, t]) = 1
X[b, t], ngược lại                      (2)

Do đó, như được hiển thị trong Hình 1(a), bất kỳ lớp hiện có nào được áp dụng cho đầu vào theo cách pointwise (ví dụ, FFN) có thể dễ dàng được nhúng bên trong một SkipLayer. Dựa trên ngữ cảnh, nếu router quyết định bỏ qua, đầu vào sẽ được kết nối trực tiếp với đầu ra, ngược lại nó sẽ đi qua logic lớp được nhúng.

Hàm Router. Trung tâm của SkipLayer là hàm router G(X|WG) được học để chỉ gán một tập hợp con đầu vào cho lớp được nhúng để có hiệu suất mô hình tốt nhất dưới ngân sách đã cho. Cho một batch token đầu vào X ∈ RB×T×d, router xuất ra một ma trận mặt nạ nhị phân

M = G(X|WG) ∈ {0,1}B×T, WG ∈ Rd×2.     (3)

Có một số lựa chọn để thiết kế G(X|WG). Một lựa chọn là hàm Sigmoid G(X|WG) = σ(XWG) mà độc lập chuẩn hóa mỗi giá trị để nằm trong phạm vi liên tục (0,1) như xấp xỉ mềm cho mặt nạ nhị phân. Mặc dù xấp xỉ này dễ vi phân, nó cần một ngưỡng bổ sung để tạo ra quy tắc quyết định nhị phân của Phương trình 2.

Lựa chọn thiết kế thứ hai là định tuyến Top-K (K=1) được sử dụng rộng rãi trong các công trình của (Du et al., 2022; Lepikhin et al., 2021), cho G(X|WG) = Top-1(XWG) = argmax XWG. Để giải quyết tính không khả vi của toán tử argmax, như được thảo luận trong (Lepikhin et al., 2021), cho mỗi token đầu vào X[b, t], trước tiên chúng ta chuẩn hóa điểm số tích vô hướng bằng g = Softmax(X[b, t]WG) ∈ R2, và cho

XoSL[b, t] = {
g[1] · F[layer](X[b, t]), nếu argmax g = 1
g[0] · X[b, t], ngược lại                (4)

sao cho gradient có thể được lan truyền ngược qua các hệ số g. Thật không may, trong các thí nghiệm của chúng tôi, chúng tôi thấy rằng công thức Top-1 không thể kiểm soát chính xác độ thưa của mô hình (điều này quan trọng đối với hiệu quả) vì g vẫn là xấp xỉ mềm của quy tắc quyết định nhị phân.

Do đó chúng tôi công thức hóa router với thủ thuật Straight-Through Gumbel-Softmax được hiển thị trong Hình 1(b). Trong quá trình truyền tiến, các giá trị nhị phân được lấy mẫu được trả về cho gating G(X[b, t]) như trong Phương trình 2. Trong quá trình truyền ngược, các xác suất mềm được sử dụng như g trong Phương trình 4 để gradient được lan truyền ngược để cập nhật trọng số router. Vì trong quá trình truyền tiến chúng ta có thể tính toán trực tiếp phần trăm token không bị bỏ qua dựa trên mặt nạ nhị phân của Phương trình 3, chúng ta có thể kiểm soát tốt hơn mật độ của mô hình.

--- TRANG 2 ---
Học Cách Bỏ Qua cho Mô Hình Ngôn Ngữ

Logic LớpRouterĐầu vàoLogic LớpRouter
(a)

Logic Lớp
Router0.7 0.3 Đầu ra
0.20.81.0 (tiến)
0.8 (lùi)bỏ qua
0.0 (tiến)
0.7 (lùi)

ST-Gumbel-Softmax 
Đầu vào (b)

Hình 1. (a) Tổng quan framework SkipLayer của chúng tôi. Router có thể chọn kích hoạt hoặc bỏ qua logic lớp được nhúng dựa trên ngữ cảnh đầu vào. (b) Straight-Through Gumbel-Softmax được sử dụng cho router. Trong quá trình truyền tiến, các biến nhị phân được lấy mẫu. Trong quá trình truyền ngược, gradient có thể được lan truyền ngược để cập nhật router.

Dung lượng Router. Mặt nạ nhị phân của đầu ra router trong Phương trình 3 là việc gán một tập hợp con token trong một batch cho lớp được nhúng bên trong SkipLayer. Để đơn giản, giả sử mỗi chuỗi trong batch kích thước B có cùng độ dài chuỗi T. Khi đó, tỷ lệ r = Σi,jM[i,j]/(B×T) là phần trăm (hoặc xác suất) mà một token được gán cho lớp, còn được gọi là dung lượng. Xem P như ngân sách toàn cục của số lượng token đầu vào có thể được gán cho một lớp. Theo (Du et al., 2022; Lepikhin et al., 2021), chúng tôi giới thiệu một hạng mục loss phụ trợ ℓaux = ΣLi(ri - P)2 trong đó ri là dung lượng của lớp i ≤ L, để mỗi lớp sẽ tuân thủ ràng buộc ngân sách. Hàm loss tổng thể của mô hình sẽ là L = ℓnll + λ · ℓaux trong đó ℓnll là negative log-likelihood của việc dự đoán token tiếp theo trung bình. Bằng cách tối ưu hóa L, một mặt, dung lượng lớp sẽ được đẩy gần hơn với xác suất mục tiêu P. Mặt khác, hạng mục ℓaux sẽ liên tục cải thiện độ chính xác dự đoán của mô hình. Vì hạng mục ℓaux sẽ ép chỉ P phần trăm token trong một batch đi với lớp, để giảm hạng mục đầu tiên ℓnll, các ví dụ 'khó' dẫn đến giảm biên lớn trung bình sẽ được ưu tiên trong khi các ví dụ 'dễ' đã đạt được perplexity thấp sẽ bị bỏ qua để tiết kiệm FLOP.

Dung lượng router cho phép tính linh hoạt của việc kiểm soát sự đánh đổi hiệu suất-tính toán. Cụ thể, chúng ta có thể tăng số lượng lớp tổng cộng trong khi cùng lúc giảm xác suất mục tiêu P để giữ số lượng lớp được kích hoạt trung bình gần như không đổi. Điều này có hiệu quả tách biệt việc tăng dung lượng mô hình khỏi chi phí tính toán cho mỗi dự đoán, và khiến việc đánh đổi dung lượng mô hình tăng để dự đoán tốt hơn trở nên khả thi. Trong quá trình phục vụ, sau đó chúng ta có thể tải mô hình có thể tận dụng tốt nhất bộ nhớ của accelerator, dẫn đến chất lượng dự đoán cao nhất, và chỉ tăng nhẹ chi phí tính toán trong khi vẫn đáp ứng yêu cầu độ trễ đồng thời.

2.2. Triển khai Hiệu quả

Ưu điểm chính của các mô hình dựa trên SkipLayer là số lượng đầu vào được tính toán bởi mỗi lớp khác nhau trên toàn bộ stack lớp và liên tục thay đổi trong quá trình huấn luyện. Cùng lúc, đặc tính động này cũng thách thức cho việc triển khai trên TPU nơi các tính toán của tensor với hình dạng tĩnh thường được ưa thích.

Triển khai cơ bản là trước tiên áp dụng logic lớp đã cho trong Hình 1(a) cho toàn bộ batch và sau đó nhân đầu ra batch với mặt nạ được cho bởi Phương trình 3 để các token bị bỏ qua sẽ không được sử dụng trong lớp. Tuy nhiên, cơ chế mặt nạ này tốn kém về mặt tính toán vì chúng ta không nên dành cùng một lượng tính toán cho các đầu vào bị bỏ qua như những đầu vào không bị bỏ qua đặc biệt khi tỷ lệ bỏ qua cao.

Do đó chúng tôi phát triển một triển khai SkipLayer hiệu quả dựa trên gather và scatter động. Ý tưởng được minh họa trong Hình 2 nơi chúng tôi tập trung vào tính toán thưa của lớp FFN vì nó là thành phần được sử dụng rộng rãi và thường tính toán chuyên sâu. Thuật toán tổng thể bao gồm ba bước chính.

1. Tất cả đầu vào được đánh dấu là bỏ qua hoặc không bỏ qua dựa trên kết quả của router trong Phương trình 3.

2. Tất cả đầu vào không bị bỏ qua (trong hình chữ nhật màu xanh) được gather và chia đều thành các nhóm. Mặc dù mỗi nhóm sẽ được đưa vào FFN để tính toán tuần tự, tất cả các phần tử trong cùng một nhóm sẽ được gather, tính toán và scatter song song.

3. Kết quả tính toán từ các đầu vào không bị bỏ qua sẽ được scatter trở lại đầu ra cuối cùng của lớp FFN này, trong khi các đầu vào bị bỏ qua sẽ được viết trực tiếp vào đầu ra cuối cùng mà không cần tính toán nào.

Kích thước nhóm (số lượng đầu vào trong một nhóm), được ký hiệu là Gsize, là một siêu tham số kiểm soát có bao nhiêu token sẽ được xử lý bởi FFN song song. Vì số lượng đầu vào không bị bỏ qua trong một batch là động và không biết trước, Gsize ảnh hưởng đến hiệu quả huấn luyện. Khi Gsize quá lớn, ví dụ, chỉ có một nhóm duy nhất, nhóm này có thể bao gồm quá nhiều đầu vào bị bỏ qua, dẫn đến hiệu suất dưới mức tối ưu. Khi Gsize quá nhỏ, nó sẽ tạo ra quá nhiều nhóm có kích thước nhỏ, và tính toán sẽ gần như tuần tự. Do đó, sẽ có ít song song, và overhead thậm chí có thể lớn hơn triển khai mặt nạ cơ bản. Trong thực tế, chúng tôi thường đặt Gsize ∝ P · BT trong đó P là xác suất mục tiêu, B là kích thước batch, và T là độ dài chuỗi.

--- TRANG 3 ---
Học Cách Bỏ Qua cho Mô Hình Ngôn Ngữ

LN
Token Đầu ra
kích hoạt Attn
key, valueFFN
queryLN
routerbỏ quaSelf attention 

Hình 3. Tổng quan SkipLayer của chúng tôi cho các mô hình dựa trên Transformer, ví dụ của một lớp đơn. LN là lớp chuẩn hóa, query, key và value đề cập đến tính toán của các phép chiếu query, key và value trong lớp self-attention. Attn là tính toán attention. Các kết nối residual trong lớp self attention và lớp FFN được bỏ qua để đơn giản.

Thuật toán 1 Quá trình truyền tiến của SkipLayer
Dữ liệu: Một batch token X ∈ RB×T×d, xác suất mục tiêu P.
1 Lấy mặt nạ M bằng Phương trình 3.
2 Lấy phép chiếu key và value K ← Fkey(X), V ← Fval(X).
3 for b ≤ B, t ≤ T do
4   if M[b, t] = 1 then
5     Lấy phép chiếu query q ← Fquery(X[b, t])
6     x′ ← FAttn(FLN(X[b, t])|K, q, V) · M[b, t] + X[b, t]
7     XSL[b, t] ← FFFN(FLN(x′)) + x′
8   else
9     XSL[b, t] ← X[b, t]) · (1 − M[b, t])
10  end
11 end
12 ℓaux ← (Σb,tM[b, t]/(B · T) − P)2
13 return XSL, ℓaux

2.3. SkipLayer cho Các mô hình dựa trên Transformer

Trong phần này, chúng tôi tập trung đặc biệt vào việc áp dụng SkipLayer cho các mô hình ngôn ngữ chỉ decoder dựa trên Transformer trong thiết lập học theo ngữ cảnh. Một lớp Transformer chủ yếu bao gồm self-attention, chuẩn hóa lớp, và FFN như các lớp phụ, và có thể được biểu diễn là

Xl′ = FAttn(FLN(Xl)) + Xl,
Xl+1 = FFFN(FLN(Xl′)) + Xl′.                (5)

SkipLayer có thể được áp dụng cho một lớp Transformer đơn được hiển thị trong Hình 3. Chúng tôi đề xuất bọc toàn bộ lớp Transformer vào một SkipLayer để bảo toàn tính nguyên tử của cấu trúc self-attention (FAttn) → FFN (FFFN). Tuy nhiên, lớp FAttn và lớp FFFN có các triển khai bỏ qua hơi khác nhau. Lớp FFFN thường là thành phần tính toán chuyên sâu nhất của mô hình Transformer, nhưng có thể được áp dụng cho một batch token theo cách pointwise. Do đó, mỗi token đầu vào của một batch có thể kích hoạt lớp FFFN độc lập với xác suất P. Vì FFFN tiêu thụ phần lớn tính toán trong một lớp Transformer, chúng ta do đó có thể có tiết kiệm lớn trong FLOP khi xác suất kích hoạt P nhỏ.

Lớp self-attention FAttn tiêu thụ ít tính toán hơn nhiều so với lớp FFFN. Tuy nhiên, FAttn không thể áp dụng cho một batch token theo cách pointwise vì các token cần attend với nhau để tính toán đầu ra attention của chính chúng. Nếu hầu hết các token trong một batch bị bỏ qua khi P nhỏ, các token không bị bỏ qua còn lại sẽ mất hầu hết ngữ cảnh của các chuỗi tương ứng mà chúng thuộc về. Chúng tôi cũng quan sát thực nghiệm chất lượng dự đoán thấp hơn khi cơ chế bỏ qua đơn giản này được áp dụng. Do đó, chúng tôi đề xuất cơ chế bỏ qua một phần sau đây. Như được hiển thị trong Hình 3, khi các token đầu vào bị bỏ qua, các phép chiếu key và value của chúng vẫn được bảo toàn (Dòng 2 trong Thuật toán 1) vì chúng là một phần của ngữ cảnh và cần thiết cho các token không bị bỏ qua còn lại để tiếp tục attend. Tuy nhiên, vì các token bị bỏ qua không yêu cầu tính toán attention bởi chính chúng, chúng ta vẫn có thể bỏ qua các phép chiếu query của chúng. Đáng chú ý rằng Dòng 3-11 trong Thuật toán 1 có thể được tính toán song song sử dụng triển khai hiệu quả của chúng tôi trong Phần 2.2 để tăng tốc độ huấn luyện.

Thuật toán 2 cho thấy logic giải mã greedy của một lớp Transformer dựa trên SkipLayer. Router đưa ra quyết định bỏ qua bằng cách chọn kết quả có khả năng nhất. Các phép chiếu key và value sẽ được tính toán và lưu trong cache giải mã (Dòng 2). Chỉ khi router kích hoạt lớp hiện tại, phép chiếu query sẽ được tính toán cho token hiện tại, và sau đó cache giải mã K và V chứa các phép chiếu key và value của các bước giải mã trước đó sẽ được sử dụng để tính toán self attention. Ngược lại, sẽ không có tính toán nào thêm từ lớp này.

Thuật toán 2 SkipLayer cho mỗi bước giải mã
Dữ liệu: Trạng thái hiện tại x ∈ Rd, cache key và value K, V
1 m = G(x|WG) = argmax x⊤WG
2 K ← Fkey(x), V ← Fval(x)
3 if m = 1 then
4   Lấy phép chiếu query q ← Fquery(x)
5   x′ ← FAttn(FLN(x)|K, q, V) + x
6   xSL ← FFFN(FLN(x′)) + x′
7 else
8   xSL ← x
9 end
10 return: xSL

3. Thiết lập Thí nghiệm

Chúng tôi tập trung vào huấn luyện các mô hình ngôn ngữ chỉ decoder. Phần này trình bày chi tiết thiết lập huấn luyện, siêu tham số, baseline, benchmark, và giao thức đánh giá của chúng tôi.

--- TRANG 4 ---
Học Cách Bỏ Qua cho Mô Hình Ngôn Ngữ

Bảng 1. Kiến trúc và kích thước của các mô hình được huấn luyện trong thí nghiệm của chúng tôi. Tất cả các mô hình được huấn luyện chia sẻ cùng siêu tham số học.

Model       nparams   P      L    Eff-L
Standard (6L)   408M     0      6
6 SkipLayer (12, 50%)  766M    50%    12
SkipLayer (24, 25%)   1.47B   25%    24
SkipLayer (48, 12.5%) 2.92B   12.5%  48

Standard (12L)  766M     0      12
12 SkipLayer (24, 50%) 1.47B   50%    24
SkipLayer (48, 25%)   2.92B   25%    48
SkipLayer (96, 12.5%) 5.79B   12.5%  96

Standard (24L)  1.47B    0      24
24 SkipLayer (48, 50%) 2.92B   50%    48
SkipLayer (96, 25%)   5.79B   25%    96

Tập dữ liệu. Tập dữ liệu tiền huấn luyện có 1.6 nghìn tỷ token đại diện cho một phạm vi rộng các trường hợp sử dụng ngôn ngữ tự nhiên. Một bộ phân loại nội bộ được huấn luyện để phân loại giữa một bộ sưu tập văn bản được tuyển chọn và các trang web khác để chúng tôi có thể ước tính chất lượng nội dung của một trang web. Một tập hợp con được lọc chất lượng cao của các trang web được kết hợp với sách, trang Wikipedia, cuộc trò chuyện, diễn đàn, và tin tức để tạo ra tập dữ liệu cuối cùng giống như (Du et al., 2022) để huấn luyện.

Huấn luyện Mô hình. Chúng tôi đã huấn luyện một số biến thể của các mô hình dựa trên SkipLayer và baseline được hiển thị trong Bảng 1. Chiều mô hình của tất cả các mô hình là 1,536 và chiều ẩn của FFN có 8× chiều mô hình. Chiều ẩn của mỗi đầu attention là 64. nparams là tổng số tham số mô hình có thể huấn luyện được, L là tổng số lớp Transformer, P là xác suất kích hoạt một lớp, và Eff-L là số lượng lớp được kích hoạt hiệu quả trung bình. Độ dài chuỗi được đặt thành 1,024 token trong quá trình huấn luyện, và kích thước batch bao gồm 256 chuỗi. Chúng tôi đặt tốc độ học là 0.1 cho 10K bước huấn luyện đầu tiên và sau đó giảm theo lịch trình căn bậc hai nghịch đảo. Chúng tôi sử dụng optimizer Adafactor với suy giảm moment đầu β1 = 0 và suy giảm moment thứ hai β2 = 0.99. Tỷ lệ dropout được đặt thành 0 vì các token được xử lý là token từ corpus huấn luyện cực kỳ lớn. Chúng tôi sử dụng tokenizer subword SentencePiece (Kudo & Richardson, 2018) với từ vựng kích thước 32K. Trong quá trình huấn luyện, chúng tôi sử dụng float32 cho trọng số mô hình và bfloat16 cho activation. Ngoài loss negative log-likelihood tổng quát, chúng tôi thêm loss phụ trợ được thảo luận trong Phần 2.1 để kiểm soát tỷ lệ bỏ qua của mô hình SkipLayer, trọng số loss phụ trợ λ được đặt thành 0.1. Cuối cùng, Gsize được đặt thành 1,024 cho tất cả các mô hình dựa trên SkipLayer.

Đánh giá Mô hình. Để đánh giá trực tiếp hiệu quả của các mô hình dựa trên SkipLayer, chúng tôi chủ yếu tuân theo giao thức học 1-shot được đề xuất bởi (Radford et al., 2018), được sử dụng rộng rãi để đánh giá chất lượng tổng quát hóa của các mô hình ngôn ngữ được tiền huấn luyện. Chúng tôi đánh giá mỗi ví dụ trong tập phát triển của một benchmark. Cho mỗi benchmark, chỉ một ví dụ sẽ được rút ngẫu nhiên từ tập huấn luyện của nhiệm vụ đó làm demonstration và ngữ cảnh duy nhất, sau đó sẽ được nối với ví dụ đánh giá với hai dòng mới ở giữa, và sau đó đưa vào mô hình. Chúng tôi sử dụng chính xác cùng định dạng prompting như (Radford et al., 2018) cho mỗi benchmark downstream.

Benchmark. Chúng tôi sử dụng 24 tập dữ liệu bao gồm bốn nhiệm vụ sinh ngôn ngữ tự nhiên (NLG) và 20 nhiệm vụ hiểu ngôn ngữ tự nhiên (NLU) để đánh giá. Cho các nhiệm vụ NLG, chúng tôi so sánh chuỗi token được giải mã bởi các mô hình với ground-truth và báo cáo độ chính xác Exact Match (EM). Các nhiệm vụ này là TriviaQA, NQS, WebQS, và SQuADv2. Giải mã greedy được sử dụng cho mỗi nhiệm vụ. Tất cả các nhiệm vụ NLU được công thức hóa thành dạng chọn một câu trả lời đúng từ nhiều tùy chọn ứng viên. Dự đoán dựa trên log-likelihood tối đa của mỗi tùy chọn cho ngữ cảnh logP(option|context) được chuẩn hóa bởi độ dài token của mỗi tùy chọn. Các nhiệm vụ NLU này bao gồm ANLI (R1, R2, R3), ARC (Easy, Challenge), BoolQ, CB, COPA, Hellaswag, Openbookqa, PIQA, Race (middle, high), ReCord, RTE, Storycloze, WIC, Winograd, Winograde và WSC273. Cuối cùng, chúng tôi sử dụng trung bình điểm số trên tất cả tập dữ liệu để báo cáo hiệu suất 1-shot tổng thể của các mô hình trên cả nhiệm vụ NLG và NLU.

Baseline. Chúng tôi xem xét các baseline sau để nghiên cứu hiệu quả của các mô hình dựa trên SkipLayer.

• Standard base model (STD). Các mô hình Transformer dense tiêu chuẩn không có bất kỳ hoạt động bỏ qua nào.

• WideFFN. Vì FFN thường tiêu thụ tính toán chính và có tác động lớn đến hiệu suất dự đoán (Kocsis et al., 2022), WideFFN do đó được thiết kế để tăng gấp đôi chiều ẩn của thành phần FFN. Sau đó chúng tôi chỉ áp dụng SkipLayer cho thành phần FFN mà không thay đổi tổng số lớp. Kết quả là, khi xác suất bỏ qua P = 50%, FLOP tính toán cho mỗi dự đoán không thay đổi do kích thước FFN 2x.

• HighwayNet (Srivastava et al., 2015) là một trong những công trình đầu tiên đề xuất học một hàm gating có thể giúp huấn luyện các mạng rất sâu một cách hiệu quả. Chúng tôi cũng áp dụng ý tưởng này cho thành phần FFN của lớp Transformer như một baseline.

• Random Gating (Rand). Phương pháp gating ngẫu nhiên là baseline nơi hàm gating đã học của mô hình SkipLayer được thay thế bằng một hàm ngẫu nhiên thuần túy không học. Baseline này được thiết kế để đánh giá tầm quan trọng của hàm gating đã học đối với hiệu suất dự đoán của mô hình.

4. Kết quả Đầy đủ

Chúng tôi đã tiến hành đánh giá toàn diện về hiệu quả của SkipLayer và báo cáo kết quả định lượng so với các baseline khác nhau trong phần này.

SkipLayer hoạt động như thế nào trong học 1-shot?

SkipLayer cho phép mỗi đầu vào chọn lọc kích hoạt một lớp cụ thể tùy thuộc vào ngữ cảnh. Bằng cách giữ số lượng lớp được kích hoạt trung bình không đổi trong khi tăng tổng số lớp của mô hình ngôn ngữ, chúng tôi mong đợi rằng dung lượng mô hình tăng có thể cải thiện chất lượng dự đoán của học few-shot. Do đó, trong Hình 4(a-b), trước tiên chúng tôi báo cáo hiệu suất 1-shot trung bình của các mô hình SkipLayer khác nhau. Trong Hình 4(a), trục y là hiệu suất 1-shot trung bình của tất cả các nhiệm vụ NLG, và trục x (thang log) là FLOP tính toán cho mỗi token trong quá trình truyền tiến. Đường đen đầu tiên cho thấy hiệu suất của các mô hình baseline tiêu chuẩn của 6, 12, 24 và 48 lớp tương ứng. Cho mỗi mô hình baseline của một số lớp cụ thể, ví dụ 6L, chúng tôi báo cáo hiệu suất của các mô hình SkipLayer tương ứng có FLOP tính toán tương đương. Ví dụ, ba chấm vàng đại diện cho các mô hình SkipLayer của 12 lớp (12L) với mật độ 50%, 24 lớp (24L) với mật độ 25%, và 48 lớp (48L) với mật độ 12.5% tương ứng. So với mô hình baseline 6L, tất cả ba mô hình SkipLayer này (màu vàng) có cùng số lượng lớp được kích hoạt trung bình là sáu (Số lượng lớp hiệu quả) được ký hiệu là Eff06. Tương tự, đường cam (Eff12) và đường đỏ (Eff24) biểu thị các mô hình SkipLayer với 12 và 24 số lượng lớp được kích hoạt trung bình tương ứng.

Trong tất cả các trường hợp, trước tiên nó cho thấy rằng khi các mô hình SkipLayer trở nên sâu hơn và thưa hơn (giữ cùng số lượng lớp được kích hoạt), nó tiếp tục cải thiện hiệu suất học few-shot với chi phí khiêm tốn của FLOP tăng cho mỗi dự đoán token. Ví dụ, SkipLayer (24L, 25%) có cải thiện hiệu suất 51.5% với chi phí tăng 18.2% FLOP tính toán so với baseline 6L. Tương tự, SkipLayer (48L, 25%) và SkipLayer (96L, 25%) có 28% và 22% với chi phí 19.2% và 20% so với baseline 12L và 24L tương ứng. Hơn nữa, trong Hình 4(a) chúng ta cũng có thể quan sát rằng mặc dù SkipLayer (48L, 12.5%) có ít FLOP tính toán hơn so với baseline 12L, nó đã đạt được hiệu suất 1-shot khá gần. Tương tự, SkipLayer (96L, 12.5%) thậm chí có hiệu suất 1-shot tốt hơn so với baseline 24L bằng cách sử dụng ít FLOP tính toán hơn. Điều này xác minh rằng chúng ta có thể đánh đổi dung lượng mô hình để có chất lượng dự đoán tốt hơn. Tương tự, Hình 4(b) cho thấy các mô hình tương tự rằng dung lượng mô hình tăng có thể dẫn đến chất lượng dự đoán tốt hơn trên các nhiệm vụ NLU trong khi với chi phí tăng FLOP khiêm tốn.

SkipLayer có giải mã và huấn luyện hiệu quả không? Như được hiển thị trong Hình 4(a-b), các mô hình SkipLayer sâu hơn và thưa hơn có cải thiện hiệu suất nhất quán trong học few-shot. Chúng tôi cũng quan tâm đến việc nghiên cứu xem chúng có thể giải mã và huấn luyện nhanh không. Hình 4(c) so sánh thời gian giải mã cho mỗi token của các mô hình khác nhau sử dụng một chip TPU v3 đơn. Nó cho thấy rằng SkipLayer (12, 50%) có thời gian giải mã cho mỗi bước gần như tương tự như baseline 6L. SkipLayer (24, 50%) cũng có tốc độ tương tự như baseline 12L. SkipLayer (48, 50%) có cải thiện hiệu suất 1-shot 8% với chi phí tăng 6% thời gian giải mã cho mỗi bước so với baseline 24L. Khi các mô hình trở nên sâu hơn, thời gian giải mã cho mỗi bước cũng tăng. Tuy nhiên, chúng ta có thể tìm thấy một số sự đánh đổi tốt giữa chất lượng và tốc độ. Ví dụ, SkipLayer (96, 25%) đã đạt được hiệu quả giải mã 20% với mất chất lượng nhỏ chỉ 0.5% so với baseline 48L.

--- TRANG 5 ---
Học Cách Bỏ Qua cho Mô Hình Ngôn Ngữ

6L12L(50%)48L(12.5%)
12L24L(50%)48L(25%)96L(12.5%)
24L48L
48L(50%)96L(25%)
101520
1 2 3 4 5 6
GFlops cho mỗi dự đoán tokenĐiểm Trung bình (NLG)
 Base
 Eff06
 Eff12
 Eff24
(a) Trung bình 1-shot (NLG)

6L12L(50%)24L(25%)48L(12.5%)12L24L(50%)48L(25%)96L(12.5%)
24L48L
48L(50%)96L(25%)
485052
1 2 3 4 5 6
GFlops cho mỗi dự đoán tokenĐiểm Trung bình (NLU)
 Base
 Eff06
 Eff12
 Eff24 (b) Trung bình 1-shot (NLU)

STD (6)
SL (12, 50%)
SL (24, 25%)
SL (48, 12.5%)STD (12)
SL (24, 50%)
SL (48, 25%)
SL (96, 12.5%)STD (24)
SL (48, 50%)
SL (96, 25%)
STD (48)0.02.55.07.510.0
Phương phápThời gian / token (ms) (c) Thời gian giải mã cho mỗi token.

6L
12L(50%)
24L(25%)
48L(12.5%)12L
24L(50%)
48L(25%)24L
48L(50%)0.250.500.751.001.25
1.0 1.5 2.0 2.5 3.0
GFlops cho mỗi dự đoán tokenBước / s (d) Tốc độ huấn luyện.

Hình 4. Hiệu suất 1-shot trung bình của các mô hình SkipLayer khác nhau cho FLOP hiệu quả tương đương cho mỗi dự đoán token trên các nhiệm vụ NLG (a) và NLU (b). (c) So sánh thời gian giải mã cho mỗi token giữa các mô hình dựa trên SkipLayer (SL) và các mô hình baseline tương ứng (STD). (d) So sánh tốc độ huấn luyện giữa các mô hình có mật độ khác nhau.

Hình 4(d) tiếp tục cho thấy tốc độ huấn luyện trên một TPU v4 đơn của các mô hình SkipLayer khác nhau. Nói chung, tốc độ huấn luyện giảm khi nhiều FLOP được sử dụng cho mỗi dự đoán. So với các baseline dense đầy đủ tương ứng, SkipLayer (24, 50%) có lợi ích tốc độ 18%, và SkipLayer (48, 12.5%) có lợi ích 3x.

SkipLayer so sánh với các baseline như thế nào? Hình 5(a-b) đầu tiên so sánh hiệu suất 1-shot trung bình của các phương pháp khác nhau của 6 lớp hiệu quả và 12 lớp trên tất cả các nhiệm vụ NLG. Trong Hình 5(a) khi tất cả các mô hình đều nhỏ, HighwayNet có hiệu suất tương tự như mô hình baseline tiêu chuẩn 6L. Cả hai đều vượt trội hơn phương pháp Random gating nhưng kém hơn một chút so với phương pháp WideFFN. Khi tất cả các mô hình trở nên sâu hơn trong Hình 5(b), baseline tiêu chuẩn 12L scale tốt hơn WideFFN, HighwayNet và phương pháp Random gating. Tuy nhiên, trong cả hai trường hợp, các mô hình dựa trên SkipLayer đã đạt được hiệu suất tốt nhất so với tất cả các baseline khác. HighwayNet hoạt động tệ nhất trong số tất cả các mô hình chúng tôi huấn luyện, một lý do có thể là Highway network được thiết kế chỉ cho kiến trúc FFN, nó hoạt động như một nhánh residual có trọng số được thêm vào lớp được bọc, giúp ích cho tính ổn định huấn luyện của các mạng FFN rất sâu. Tuy nhiên, nhánh residual khá bình thường trong các mô hình Transformer ngày nay, cả self-attention và FFN đều có residual. Trong triển khai của chúng tôi, chúng tôi thay thế residual ban đầu bằng residual highway network dẫn đến suy giảm hiệu suất. Hình 5(c-d) tiếp tục so sánh hiệu suất 1-shot trung bình của các phương pháp khác nhau trên tất cả các nhiệm vụ NLU. Với xu hướng tương tự được quan sát trước đó, WideFFN có hiệu suất scaling gần với các mô hình baseline tiêu chuẩn của 12 lớp, cả hai đều vượt trội đáng kể so với HighwayNet và phương pháp Random gating, và các mô hình dựa trên SkipLayer dẫn đến hiệu suất tốt nhất tổng thể khi các mô hình scale up.

Gating đã học có quan trọng không? Hàm gating bên trong SkipLayer cho phép mỗi ví dụ đầu vào kích hoạt một tập hợp con lớp của mô hình dựa trên ngữ cảnh. Chúng tôi đã quan sát rằng tính linh hoạt chuyển đổi tham số mô hình này cải thiện độ chính xác dự đoán của mô hình khi nó trở nên sâu hơn và thưa hơn. Chúng tôi tự hỏi hàm định tuyến đã học có thể đóng góp bao nhiêu vào hiệu suất dự đoán của các mô hình dựa trên SkipLayer. Chúng tôi tiếp cận câu hỏi này bằng cách so sánh các mô hình dựa trên SkipLayer với các baseline Random gating tương ứng nơi gating không được học bằng cách thay đổi mật độ của mô hình cạnh nhau trong Hình 6. Vì mỗi token kích hoạt một lớp ngẫu nhiên và độc lập, các baseline Random gating có FLOP tính toán tương tự cho mỗi dự đoán token như các mô hình dựa trên SkipLayer cho cùng mật độ mô hình. Tuy nhiên, như được hiển thị trong Hình 6, cho cùng mật độ mô hình, gating đã học của các mô hình dựa trên SkipLayer hoạt động tốt hơn đáng kể so với các phương pháp tương ứng sử dụng gating ngẫu nhiên. Mặc dù các baseline Random gating cũng có tính linh hoạt chuyển đổi tham số mô hình cho mỗi dự đoán token, Hình 6 xác minh rằng các hàm gating đã học hiệu quả hơn nhiều để cải thiện độ chính xác dự đoán.

Hành vi bỏ qua hoạt động như thế nào trong quá trình giải mã? Để nghiên cứu hành vi bỏ qua của mô hình SkipLayer và hiểu loại token nào bỏ qua nhiều lớp hơn những token khác trong quá trình giải mã greedy, chúng tôi thu thập thống kê bỏ qua của kết quả giải mã của 500 câu hỏi được lấy mẫu trong TriviaQA sử dụng mô hình SkipLayer (12L, 50%) của chúng tôi. Trong Hình 7, chúng tôi vẽ biểu đồ bong bóng của 300 token được sử dụng thường xuyên trong kết quả giải mã theo số lượng lớp bị bỏ qua trung bình của chúng (Một token có thể có các mô hình bỏ qua khác nhau dưới các ngữ cảnh khác nhau). Các chấm lớn hơn trong hình đại diện cho nhiều lớp bị bỏ qua hơn. Chúng ta có thể quan sát rằng các token bỏ qua nhiều nhất chủ yếu là các từ chức năng như "and", "to", "ed" hoặc "ing". Các token này có thể dễ dàng được suy ra từ ngữ cảnh trước đó và do đó không cần nhiều tính toán để giải mã. Trong khi các token bỏ qua ít hơn thường là các từ độc lập như "No" hoặc "Paris". Điều này thực sự cho thấy rằng mô hình SkipLayer của chúng tôi có thể thành công xác định các token như vậy và gán tính toán phù hợp tương ứng.

--- TRANG 6 ---
Học Cách Bỏ Qua cho Mô Hình Ngôn Ngữ

HighwayNetStandard
WideFFNRandom (12, 50%)
SkipLayer (12, 50%)
SkipLayer (24, 25%)
051015
Phương phápĐiểm Trung bình (NLG)
(a) Trung bình 1-shot (NLG, 6L)

HighwayNet
StandardWideFFNRandom (24, 50%)
SkipLayer (24, 50%)
SkipLayer (48, 25%)
051015
Phương phápĐiểm Trung bình (NLG) (b) Trung bình 1-shot (NLG, 12L)

HighwayNet
StandardWideFFNRandom (12, 50%)
SkipLayer(12, 50%)
SkipLayer(24, 25%)42454851
Phương phápĐiểm Trung bình (NLU) (c) Trung bình 1-shot (NLU, 6L)

HighwayNet
Standard
WideFFNRandom (24, 50%)
SkipLayer (24, 50%)
SkipLayer (48, 25%)42454851
Phương phápĐiểm Trung bình (NLU) (d) Trung bình 1-shot (NLU, 12L)

Hình 5. Hiệu suất NLG và NLU 1-shot trung bình của các phương pháp khác nhau với 6 (a-b) và 12 (c-d) số lượng lớp được kích hoạt hiệu quả tương ứng.

101520
12% 25% 50% 75%
Mật độĐiểm Trung bình (NLG)
SkipLayer(24)
L24Rand
SkipLayer(48)
L48Rand
(a) Trung bình 1-shot (NLG)

45.047.550.0
12% 25% 50% 75%
Mật độĐiểm Trung bình (NLU)
SkipLayer(24)
L24Rand
SkipLayer(48)
L48Rand (b) Trung bình 1-shot (NLU)

Hình 6. Gating đã học có hiệu suất tốt hơn đáng kể so với các phương pháp tương ứng sử dụng gating ngẫu nhiên.

--- TRANG 7 ---
Học Cách Bỏ Qua cho Mô Hình Ngôn Ngữ

of
States
tm
eryiea
an
ingdusson
Cuped
andph
g
littlearaon
now
Senateu
llflowoneran
veto
SanBest
al
ie
KingdomNo
or
ParisarSeanon

Hình 7. Biểu đồ bong bóng của hành vi bỏ qua của các token trong quá trình giải mã greedy trên tập dữ liệu TriviaQA sử dụng mô hình SkipLayer (12L, 50%) của chúng tôi. Mỗi chấm đại diện cho một token, kích thước chấm lớn hơn có nghĩa là nhiều lớp bị bỏ qua hơn. Văn bản đen/đỏ cho thấy một số token bỏ qua nhiều/ít lớp nhất.

5. Công trình Liên quan

Tính toán Có điều kiện (Bengio et al., 2013; 2015) là một mô hình nơi chỉ một tập hợp con tham số mô hình có thể được kích hoạt cho mỗi ví dụ đầu vào. Early-exit là một loại triển khai cho tính toán có điều kiện nơi các bộ phân loại bên ngoài được trang bị ngưỡng dựa trên độ tin cậy được sử dụng để thoát sớm mà không đi qua toàn bộ stack lớp (Wang et al., 2017; Xin et al., 2020; Schwartz et al., 2020; Liu et al., 2020; Dabre et al., 2020; Elbayad et al., 2020; Schuster et al., 2022). Không giống như các phương pháp này nơi tính toán được kích hoạt cho các lớp dưới, các mô hình dựa trên SkipLayer về mặt kỹ thuật cho phép mỗi đầu vào khám phá 2^L đường dẫn tính toán khác nhau cho một mô hình với L lớp xếp chồng.

Một kỹ thuật thay thế là cho phép mô hình 'học' cách kích hoạt các lớp phụ khác nhau của nó. Do tính rời rạc của các quyết định kích hoạt, các triển khai xấp xỉ mềm và dựa trên RL đã được khám phá trong cộng đồng thị giác (Srivastava et al., 2015; Wang et al., 2018) và NLP (Bapna et al., 2020). Phương pháp của chúng tôi gần với phương pháp học thứ hai nhưng khác ở chỗ SkipLayer không yêu cầu xấp xỉ mềm trong quá trình truyền tiến, mang lại tiết kiệm tính toán không chỉ trong suy luận mà còn trong quá trình huấn luyện. Sự khác biệt này rất quan trọng vì tiền huấn luyện các mô hình ngôn ngữ thường tốn thời gian và tốn kém.

Ngoài ra, các công trình đồng thời như CODA (Lei et al., 2023) và CoLT5 (Ainslie et al., 2023) đã áp dụng phương pháp lựa chọn token tương tự để kích hoạt các lớp Transformer. Tuy nhiên, các công trình này chỉ áp dụng kích hoạt có điều kiện trong các lớp encoder của mô hình encoder-decoder như T5.

Mixture of Experts gần đây đã được đề xuất để cải thiện hiệu quả mô hình (Shazeer et al., 2017; Gross et al., 2017; Lepikhin et al., 2021; Fedus et al., 2021; Roller et al., 2021; Du et al., 2022; Artetxe et al., 2021; Lewis et al., 2021; Zhou et al., 2022; Rajbhandari et al., 2022) bằng cách kích hoạt thưa một tập hợp con expert trong một lớp MoE.

Phương pháp của chúng tôi trực giao với các mô hình MoE ở chỗ một lớp MoE có thể dễ dàng được bọc bởi SkipLayer để có hiệu quả bổ sung. Hơn nữa, SkipLayer có thể áp dụng tính toán có điều kiện cho cả thành phần self-attention và Feed-Forward (FFN) của một lớp Transformer, trong khi các mô hình MoE chủ yếu tập trung vào kích hoạt có điều kiện thành phần FFN trong một lớp MoE.

Structural Dropout (Tompson et al., 2015; Ghiasi et al., 2018; Dai et al., 2019; Fan et al., 2019; Zeng et al., 2021) ngẫu nhiên loại bỏ một nhóm trọng số, ví dụ, một lớp (Fan et al., 2019), trong quá trình huấn luyện để đạt được khả năng tổng quát hóa và robustness tốt hơn cho pruning trong suy luận. Tuy nhiên, lượng tính toán trong suy luận vẫn đồng đều cho mỗi ví dụ. Ngược lại, các mô hình dựa trên SkipLayer học các mô hình bỏ qua từ dữ liệu cho thấy hiệu suất tốt hơn baseline bỏ qua ngẫu nhiên, và có thể gán lượng tính toán không đồng đều cho mỗi ví dụ trong suy luận.

6. Kết luận

Chúng tôi đề xuất một phương pháp tổng quát mới được gọi là SkipLayer để bỏ qua động việc thực thi các lớp tùy ý dựa trên ngữ cảnh đầu vào sử dụng một thuật toán định tuyến đơn giản. Phương pháp này cho phép tính toán không đồng nhất cho các token ở độ phức tạp hoặc tầm quan trọng khác nhau để nhiều tài nguyên tính toán hơn có thể được sử dụng để cải thiện chất lượng dự đoán của các token khó hơn. Mô hình của chúng tôi chứng minh cải thiện hiệu suất 1-shot đáng kể trên 24 nhiệm vụ NLP so với các baseline cạnh tranh khác chỉ với chi phí bổ sung nhỏ cho suy luận.

--- TRANG 8 ---
Học Cách Bỏ Qua cho Mô Hình Ngôn Ngữ

Tài liệu tham khảo

Ainslie, J., Lei, T., de Jong, M., Ontañón, S., Brahma, S.,
Zemlyanskiy, Y., Uthus, D., Guo, M., Lee-Thorp, J., Tay,
Y., et al. Colt5: Faster long-range transformers with
conditional computation. In Proceedings of the 2023
Conference on Empirical Methods in Natural Language
Processing (EMNLP), 2023.

Artetxe, M., Bhosale, S., Goyal, N., Mihaylov, T., Ott, M.,
Shleifer, S., Lin, X. V., Du, J., Iyer, S., Pasunuru, R.,
Anantharaman, G., Li, X., Chen, S., Akin, H., Baines, M.,
Martin, L., Zhou, X., Koura, P. S., O'Horo, B., Wang, J.,
Zettlemoyer, L., Diab, M., Kozareva, Z., and Stoyanov,
V. Efficient large scale language modeling with mixtures
of experts. 2021. URL https://arxiv.org/abs/
2112.10684.

Bapna, A., Arivazhagan, N., and Firat, O. Controlling
computation versus quality for neural sequence models,
2020.

Bengio, E., Bacon, P., Pineau, J., and Precup, D. Conditional
computation in neural networks for faster models. CoRR,
abs/1511.06297, 2015. URL http://arxiv.org/
abs/1511.06297.

Bengio, Y., Léonard, N., and Courville, A. C. Estimating
or propagating gradients through stochastic neurons for
conditional computation. CoRR, abs/1308.3432, 2013.
URL http://arxiv.org/abs/1308.3432.

Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan,
J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry,
G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger,
G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.,
Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E.,
Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C.,
McCandlish, S., Radford, A., Sutskever, I., and Amodei,
D. Language models are few-shot learners. In Larochelle,
H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin,
H. (eds.), Advances in Neural Information Processing
Systems, volume 33, pp. 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.
neurips.cc/paper/2020/file/
1457c0d6bfcb4967418bfb8ac142f64a-Paper.
pdf.

Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
G., Roberts, A., Barham, P., Chung, H. W., Sutton,
C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,
S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer,
N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B.,
Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari,
G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S.,
Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D.,
Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E.,
Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X.,
Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J.,
Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and
Fiedel, N. Palm: Scaling language modeling with pathways. 2022. doi: 10.48550/ARXIV.2204.02311. URL
https://arxiv.org/abs/2204.02311.

Dabre, R., Rubino, R., and Fujita, A. Balancing cost and
benefit with tied-multi transformers. In Birch, A., Finch,
A. M., Hayashi, H., Heafield, K., Junczys-Dowmunt, M.,
Konstas, I., Li, X., Neubig, G., and Oda, Y. (eds.), Proceedings of the Fourth Workshop on Neural Generation
and Translation, NGT@ACL 2020, Online, July 5-10,
2020, pp. 24–34. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.ngt-1.3. URL https:
//doi.org/10.18653/v1/2020.ngt-1.3.

Dai, Z., Chen, M., Gu, X., Zhu, S., and Tan, P. Batch dropblock network for person re-identification and beyond. In
Proceedings of the IEEE/CVF international conference
on computer vision, pp. 3691–3701, 2019.

Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu,
Y., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., Zoph, B.,
Fedus, L., Bosma, M. P., Zhou, Z., Wang, T., Wang, E.,
Webster, K., Pellat, M., Robinson, K., Meier-Hellstern,
K., Duke, T., Dixon, L., Zhang, K., Le, Q., Wu, Y., Chen,
Z., and Cui, C. GLaM: Efficient scaling of language models with mixture-of-experts. In Proceedings of the 39th
International Conference on Machine Learning, volume
162 of Proceedings of Machine Learning Research, pp.
5547–5569. PMLR, 17–23 Jul 2022. URL https://
proceedings.mlr.press/v162/du22c.html.

Elbayad, M., Gu, J., Grave, E., and Auli, M. Depth-adaptive
transformer. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020. OpenReview.net, 2020. URL https:
//openreview.net/forum?id=SJg7KhVKPH.

Fan, A., Grave, E., and Joulin, A. Reducing transformer
depth on demand with structured dropout. arXiv preprint
arXiv:1909.11556, 2019.

Fedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with simple and
efficient sparsity. CoRR, abs/2101.03961, 2021. URL
https://arxiv.org/abs/2101.03961.

Ghiasi, G., Lin, T.-Y., and Le, Q. V. Dropblock: A regularization method for convolutional networks. Advances in
neural information processing systems, 31, 2018.

--- TRANG 9 ---
Học Cách Bỏ Qua cho Mô Hình Ngôn Ngữ

Gross, S., Ranzato, M., and Szlam, A. Hard mixtures of
experts for large scale weakly supervised vision. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 6865–6873, 2017.

Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,
Cai, T., Rutherford, E., de las Casas, D., Hendricks, L. A.,
Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican,
K., van den Driessche, G., Damoc, B., Guy, A., Osindero,
S., Simonyan, K., Elsen, E., Vinyals, O., Rae, J. W., and
Sifre, L. An empirical analysis of compute-optimal large
language model training. In Oh, A. H., Agarwal, A.,
Belgrave, D., and Cho, K. (eds.), Advances in Neural
Information Processing Systems, 2022. URL https:
//openreview.net/forum?id=iBBcRUlOAPR.

Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal,
G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers,
A., et al. In-datacenter performance analysis of a tensor
processing unit. In Proceedings of the 44th Annual International Symposium on Computer Architecture, pp. 1–12,
2017.

Kocsis, P., Súkeník, P., Braso, G., Nießner, M., Leal-Taixé,
L., and Elezi, I. The unreasonable effectiveness of fullyconnected layers for low-data regimes. In Proc. NeurIPS,
2022.

Kudo, T. and Richardson, J. Sentencepiece: A simple and
language independent subword tokenizer and detokenizer
for neural text processing. In EMNLP, 2018.

Lei, T., Bai, J., Brahma, S., Ainslie, J., Lee, K., Zhou,
Y., Du, N., Zhao, V. Y., Wu, Y., Li, B., Zhang, Y., and
Chang, M.-W. Conditional adapters: Parameter-efficient
transfer learning with fast inference. In Advances in
Neural Information Processing Systems, 2023.

Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang,
Y., Krikun, M., Shazeer, N., and Chen, Z. GShard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning
Representations, 2021. URL https://openreview.
net/forum?id=qrwe7XHTmYb.

Levy, R. Expectation-based syntactic comprehension. Cognition, 106(3):1126–1177, 2008. doi: 10.1016/j.cognition.
2007.05.006.

Lewis, M., Bhosale, S., Dettmers, T., Goyal, N., and Zettlemoyer, L. Base layers: Simplifying training of large,
sparse models. In International Conference on Machine
Learning, 2021.

Liu, W., Zhou, P., Wang, Z., Zhao, Z., Deng, H., and Ju,
Q. FastBERT: a self-distilling BERT with adaptive inference time. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp.
6035–6044, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.
537. URL https://aclanthology.org/2020.
acl-main.537.

Patterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.M., Rothchild, D., So, D., Texier, M., and Dean, J. Carbon emissions and large neural network training. arXiv
preprint arXiv:2104.10350, 2021.

Radford, A., Wu, J., Child, R., Luan, D., Amodei,
D., and Sutskever, I. Language models are
unsupervised multitask learners. 2018. URL
https://d4mucfpksywv.cloudfront.
net/better-language-models/
language-models.pdf.

Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J.,
Song, H. F., Aslanides, J., Henderson, S., Ring, R., Young,
S., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A.,
Powell, R., van den Driessche, G., Hendricks, L. A.,
Rauh, M., Huang, P., Glaese, A., Welbl, J., Dathathri, S.,
Huang, S., Uesato, J., Mellor, J., Higgins, I., Creswell,
A., McAleese, N., Wu, A., Elsen, E., Jayakumar, S. M.,
Buchatskaya, E., Budden, D., Sutherland, E., Simonyan,
K., Paganini, M., Sifre, L., Martens, L., Li, X. L., Kuncoro, A., Nematzadeh, A., Gribovskaya, E., Donato, D.,
Lazaridou, A., Mensch, A., Lespiau, J., Tsimpoukelli,
M., Grigorev, N., Fritz, D., Sottiaux, T., Pajarskas, M.,
Pohlen, T., Gong, Z., Toyama, D., de Masson d'Autume,
C., Li, Y., Terzi, T., Mikulik, V., Babuschkin, I., Clark,
A., de Las Casas, D., Guy, A., Jones, C., Bradbury, J.,
Johnson, M., Hechtman, B. A., Weidinger, L., Gabriel,
I., Isaac, W. S., Lockhart, E., Osindero, S., Rimell, L.,
Dyer, C., Vinyals, O., Ayoub, K., Stanway, J., Bennett,
L., Hassabis, D., Kavukcuoglu, K., and Irving, G. Scaling language models: Methods, analysis & insights from
training gopher. CoRR, abs/2112.11446, 2021.

Rajbhandari, S., Li, C., Yao, Z., Zhang, M., Aminabadi,
R. Y., Awan, A. A., Rasley, J., and He, Y. DeepSpeedMoE: Advancing mixture-of-experts inference and training to power next-generation AI scale. In Chaudhuri, K.,
Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato,
S. (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings
of Machine Learning Research, pp. 18332–18346. PMLR,
17–23 Jul 2022. URL https://proceedings.mlr.
press/v162/rajbhandari22a.html.

Roller, S., Sukhbaatar, S., szlam, a., and Weston, J.
Hash layers for large sparse models. In Ranzato, M.,
Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan,
J. W. (eds.), Advances in Neural Information Processing
Systems, volume 34, pp. 17555–17566. Curran Associates, Inc., 2021. URL https://proceedings.

--- TRANG 10 ---
Học Cách Bỏ Qua cho Mô Hình Ngôn Ngữ

neurips.cc/paper/2021/file/
92bf5e6240737e0326ea59846a83e076-Paper.
pdf.

Schuster, T., Fisch, A., Gupta, J. P., Dehghani, M., Bahri,
D., Tran, V. Q., Tay, Y., and Metzler, D. Confident
adaptive language modeling. CoRR, abs/2207.07061,
2022. doi: 10.48550/arXiv.2207.07061. URL https:
//doi.org/10.48550/arXiv.2207.07061.

Schwartz, R., Stanovsky, G., Swayamdipta, S., Dodge, J.,
and Smith, N. A. The right tool for the job: Matching model and instance complexities. In Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics, pp. 6640–6651, Online,
July 2020. Association for Computational Linguistics.
doi: 10.18653/v1/2020.acl-main.593. URL https:
//aclanthology.org/2020.acl-main.593.

Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le,
Q. V., Hinton, G. E., and Dean, J. Outrageously large
neural networks: The sparsely-gated mixture-of-experts
layer. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings. OpenReview.net,
2017. URL https://openreview.net/forum?
id=B1ckMDqlg.

Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J.,
and Catanzaro, B. Megatron-lm: Training multi-billion
parameter language models using gpu model parallelism.
arXiv preprint arXiv:1909.08053, 2019.

Srivastava, R. K., Greff, K., and Schmidhuber, J. Highway
networks. arXiv preprint arXiv:1505.00387, 2015.

Stanovich, K. E. and West, R. F. Individual differences in
reasoning: Implications for the rationality debate? Behavioral and Brain Sciences, 23(5):645–665, 2000. doi:
10.1017/S0140525X00003435.

Tompson, J., Goroshin, R., Jain, A., LeCun, Y., and Bregler, C. Efficient object localization using convolutional
networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 648–656,
2015.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,
Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin,
I. Attention is all you need. In Guyon, I., Luxburg,
U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural
Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.
neurips.cc/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa-Paper.
pdf.

Wang, X., Luo, Y., Crankshaw, D., Tumanov, A., and Gonzalez, J. E. IDK cascades: Fast deep learning by learning
not to overthink. CoRR, abs/1706.00885, 2017. URL
http://arxiv.org/abs/1706.00885.

Wang, X., Yu, F., Dou, Z., Darrell, T., and Gonzalez, J. E.
Skipnet: Learning dynamic routing in convolutional networks. In Ferrari, V., Hebert, M., Sminchisescu, C., and
Weiss, Y. (eds.), Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14,
2018, Proceedings, Part XIII, volume 11217 of Lecture
Notes in Computer Science, pp. 420–436. Springer, 2018.
doi: 10.1007/978-3-030-01261-8\25. URL https://
doi.org/10.1007/978-3-030-01261-8_25.

Xin, J., Tang, R., Lee, J., Yu, Y., and Lin, J. Deebert:
Dynamic early exiting for accelerating bert inference.
arXiv preprint arXiv:2004.12993, 2020.

Zeng, Y., Dai, T., Chen, B., Xia, S.-T., and Lu, J.
Correlation-based structural dropout for convolutional
neural networks. Pattern Recognition, 120:108117, 2021.

Zhou, Y., Lei, T., Liu, H., Du, N., Huang, Y., Zhao, V.,
Dai, A., Chen, Z., Le, Q., and Laudon, J. MixtureOf-experts with expert choice routing. arXiv preprint
arXiv:2202.09368, 2022.
