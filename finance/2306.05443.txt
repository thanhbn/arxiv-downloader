# 2306.05443.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/finance/2306.05443.pdf
# File size: 355252 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
PIXIU: A Large Language Model, Instruction Data
and Evaluation Benchmark for Finance
Qianqian Xie
School of Computer Science
Wuhan University
Wuhan, Hubei, China
xieq@whu.edu.cnWeiguang Han
School of Computer Science
Wuhan University
Wuhan, Hubei, China
han.wei.guang@whu.edu.cn
Xiao Zhang
Sun Yat-Sen University
Shenzhen, Guangdong, China
zhangx767@mail2.sysu.edu.cnYanzhao Lai
School of Economics and Management
Southwest Jiaotong University
Chengdu, Sichuan, China
laiyanzhao@swjtu.edu.cn
Min Peng
School of Computer Science
Wuhan University
Wuhan, Hubei, China
pengm@whu.edu.cnAlejandro Lopez-Lira
University of Florida
alejandro.lopez-lira@warrington.ufl.edu
Jimin Huang
ChanceFocus AMC.
Shanghai, China
jimin@chancefocus.com
Abstract
Although large language models (LLMs) has shown great performance on natural
language processing (NLP) in the financial domain, there are no publicly available
financial tailtored LLMs, instruction tuning datasets, and evaluation benchmarks,
which is critical for continually pushing forward the open-source development of
financial artificial intelligence (AI). This paper introduces PIXIU, a comprehensive
framework including the first financial LLM based on fine-tuning LLaMA with
instruction data, the first instruction data with 136K data samples to support the fine-
tuning, and an evaluation benchmark with 5 tasks and 9 datasets. We first construct
the large-scale multi-task instruction data considering a variety of financial tasks,
financial document types, and financial data modalities. We then propose a financial
LLM called FinMA by fine-tuning LLaMA with the constructed dataset to be able
to follow instructions for various financial tasks. To support the evaluation of
financial LLMs, we propose a standardized benchmark that covers a set of critical
financial tasks, including five financial NLP tasks and one financial prediction task.
With this benchmark, we conduct a detailed analysis of FinMA and several existing
LLMs, uncovering their strengths and weaknesses in handling critical financial
tasks. The model, datasets, benchmark, and experimental results are open-sourced1
to facilitate future research in financial AI.
1https://github.com/chancefocus/PIXIU
Preprint. Under review.arXiv:2306.05443v1  [cs.CL]  8 Jun 2023

--- PAGE 2 ---
1 Introduction
Financial technology (FinTech) has been continually advanced by the development of natural language
processing (NLP) and machine learning (ML) techniques, unlocking diversity capabilities from
predicting stock price movements to advanced financial analytics (Araci, 2019; Han et al ., 2023a;
Xie et al ., 2023; Lopez-Lira and Tang, 2023; Li et al ., 2023). Specifically, the most recent large
language models (LLMs) (Brown et al ., 2020)2have exhibited remarkable abilities in natural language
understanding (NLU) and performing various tasks by following natural language instructions without
training data. Despite these successes, the highly technical nature of financial texts requires domain-
specific LLMs to understand complex financial language and concepts effectively. Such efforts
include existing financial pre-trained language models (PLMs) such as finBERT (Araci, 2019),
FinBERT (Yang et al ., 2020) and FLANG (Shah et al ., 2022). However, those models are considered
small since their parameter size is below one billion, limiting their generalization ability. Recently, a
proprietary financial LLM called BloombergGPT (Wu et al ., 2023) with 50 billion parameters has
been proposed by pre-training a Bloom-style LLM (Scao et al., 2022) on large-scale financial data.
Despite these efforts, there remain several issues, as shown in Table 1. Firstly, BloombergGPT and
its training data are not openly released. Currently, there are no open-sourced financial LLMs, which
can hinder development in the research community. Secondly, previous financial PLMs and the
latest BloombergGPT are not fine-tuned for following natural language instructions (also known as
instruction tuning), which is critical for improving the zero-shot ability on dealing with downstream
financial tasks (Wei et al ., 2021; Ouyang et al ., 2022). Thirdly, there are also no financial instruction
data for supporting the instruction tuning of LLMs and evaluation benchmarks for comprehensively
assessing and comparing the abilities of LLMs for financial tasks. We are thus motivated to consider
the following research questions: 1) how can we develop efficient and openly available LLMs tailored
for finance? 2) how can we build large-scale and high-quality financial instruction data? 3) how can
we build the holistic financial evaluation benchmark for assessing financial LLMs?
Table 1: The comparison of pre-trained language models and large language models for finance.
"Instruct" means whether the model can follow instructions. "NLP" and "Fin" mean if the model is
evaluated with financial NLP tasks and financial prediction tasks.
Model Backbone SizeOpen SourceInstruct LanguageEvaluationRelease DateModel Data NLP Fin
finBERT (Araci, 2019) BERT 110M ! ! % English ! % 08/27/19
FinBERT (Yang et al., 2020) BERT 110M ! % % English ! % 06/15/20
Mengzi-fin (Zhang et al., 2021) RoBERTa 103M ! % % Chinese ! % 10/13/21
FLANG (Shah et al., 2022) ELECTRA 110M ! ! % English ! % 10/31/22
BBT-FinT5 (Lu et al., 2023) T5 220M ! ! % Chinese ! % 02/18/23
BloombergGPT (Wu et al., 2023) BLOOM 50B % % % English ! % 03/30/23
FinMA LLaMA 7/13B ! ! ! English ! ! 06/01/23
To deal with these research questions, we propose PIXIU ( 貔貅 )3, a comprehensive framework that
includes the first financial LLM, FinMA, based on fine-tuning LLaMA (Touvron et al ., 2023) with
multi-task and multi-modal instruction data. Fig 1 presents an overview of multi-task and multi-modal
instruction tuning of FinMA for diverse financial tasks. PIXIU also contains the first instruction data
with 136K data samples to support the fine-tuning and a holistic evaluation benchmark with four
financial NLP tasks and one financial prediction task. It has the following distinguishing features:
•Open resources . We have openly released the financial LLM, instruction tuning data, and
datasets included in the evaluation benchmark, and implementation, to encourage open
research and transparency in the research field.
•Multi-task . PIXIU includes multi-task instruction tuning data covering a diverse set of
financial tasks, including four financial NLP tasks and one financial prediction task. The
multi-task instruction tuning has been proven to be critical for improving the model’s
generalization ability (Sanh et al., 2022; Longpre et al., 2023) to new tasks.
2https://openai.com/blog/chatgpt
3PIXIU (貔貅 )https://en.wikipedia.org/wiki/Pixiu is a mythical creature in Chinese folklore. It
has the head of a dragon and the body of a lion and is believed to be an auspicious creature attracting money and
good fortune.
2

--- PAGE 3 ---
•Multi-modality . Our instruction tuning data consists of multi-modality financial data
such as tables in financial reports and historical stock prices as time-series data for the
stock-movement prediction tasks beyond texts. Moreover, they encompass diverse types of
financial texts, including reports, news articles, tweets, and regulatory filings.
•Diversity . Compared with the evaluation tasks used in BloombergGPT and existing FLUE
benchmark (Shah et al ., 2022), which mainly cover financial NLP tasks, our evaluation
benchmark includes stock movement prediction tasks. It requires the model to fully exploit
both natural texts and time-series data to extract essential information for accurate prediction.
Compared with financial NLP tasks, the financial prediction task is more aligned with real-
world scenarios and more challenging.
To build the multi-task and multi-modal instruction data, we collect open-released training data
from diverse tasks, including financial sentiment analysis, news headline classification, named entity
recognition, question answering, and stock movement prediction, and propose the diverse task-
specific instructions written by domain experts for each task. We create a large-scale instruction
tuning data FIT by assembling the task-specific instructions with data samples from each task. We
thus propose the domain-specific LLM FinMA by conducting the multi-task instruction tuning on
LLaMA with the building dataset. To evaluate our model and other LLMs holistically, we build the
Financial Language Understanding And PRediction Evaluation Benchmark (FLARE) covering 4
financial NLP tasks with 6 datasets, and 1 financial prediction task with 3 datasets.
Based on FLARE, we evaluate the performance of our model, BloombergGPT, and advanced LLMs
in the general domain, such as ChatGPT4and GPT-4 (OpenAI, 2023). Experimental results show that:
1) FinMA significantly outperforms LLMs, including BloombergGPT, ChatGPT, and GPT-4 on most
tasks in FLARE, including financial sentiment analysis, news headline classification, NER, and stock
movement prediction. This demonstrates the importance of tailoring the LLMs specifically for the
financial domain. 2) Despite promising results on most tasks, FinMA underperforms BloombergGPT,
ChatGPT, and GPT-4 on the question answering, which assesses the quantitative reasoning ability of
LLMs. Our analysis finds that this is caused by the limitation of LLaMA on quantitative reasoning and
mathematics. 3) Compared with NLP tasks, all LLMs, including FinMA, ChatGPT and GPT-4, still
present limited performance on stock movement prediction, indicating room for further improvement.
4) FinMA fine-tuned with both NLP and financial prediction tasks, presents the best performance
on one of the stock prediction datasets, indicating the potential of task-specific instruction tuning of
LLMs on financial prediction tasks.
Our contributions can be summarized as follows: 1) We introduce FIT, the first multi-task and
multi-modal instruction tuning data in the financial domain, covering 5 tasks and 9 datasets with
136,609 (136K) data samples. 2) We introduce FLARE, the first evaluation benchmark with both
financial natural language understanding and prediction tasks. 3) We introduce FinMA, the first
openly released and instruction-following financial large language model, which achieves SOTA on 3
financial NLP tasks and 1 financial prediction task. 4) We compare FinMA and existing LLMs on
FLARE. The results demonstrate the superiority of FinMA, the key limitations of LLMs for finance,
and future directions to advance LLMs for finance.
2 Related Work
Financial Language Models Many PLMs for the financial domain have been proposed by continual
pre-training PLMs with large-scale financial texts. Araci (2019) proposed the first financial PLM
called finBERT that pre-trained BERT (Kenton and Toutanova, 2019) with open released financial
corpus such as TRC2-financial5and Financial Phrase Bank (Malo et al ., 2014). finBERT outperforms
neural network methods such as LSTM in financial sentiment classification tasks. Yang et al .(2020)
further proposed FinBERT by pre-training BERT with a 4.9 billion tokens financial communication
corpus, which outperforms BERT on three financial sentiment classification datasets. Shah et al .
(2022) proposed FLANG, a financial PLM with BERT and ELECTRA (Clark et al ., 2020) as the
backbone. Besides English, financial PLMs in other languages, such as Chinese, were also proposed,
such as Mengzi-fin (Zhang et al ., 2021) and BBT-FinT5 (Lu et al ., 2023). Latest, Wu et al .(2023)
proposed BloombergGPT, the first financial large language model with 50 billion parameters, that is
4https://openai.com/blog/chatgpt
5https://trec.nist.gov/data/reuters/reuters.html
3

--- PAGE 4 ---
pre-trained with mixed datasets from the general and financial domain. However, neither the model
nor pre-trained domain datasets are not released. The model is also not instruction-following like
other LLMs such as ChatGPT and GPT-4.
Financial Evaluation Benchmark Shah et al .(2022) proposed the first heterogeneous evaluation
benchmark FLUE with 5 financial NLP tasks, including financial sentiment analysis (Malo et al .,
2014), news headline classification (Sinha and Khandait, 2021), named entity recognition (Alvarado
et al., 2015), structure boundary detection6and question answering (Maia et al ., 2018). Lu et al .
(2023) proposed the first Chinese financial evaluation benchmark BBT-CFLEB7with financial
news classification, summarization, relation extraction, question answering, and negative news
determination task, as well as sentiment classification task of financial social media texts. However,
these benchmarks only consider financial NLP tasks and don’t include financial prediction tasks, such
as stock movement prediction (Soun et al ., 2022) or pair trading (Han et al ., 2023b) that are critical
for evaluating the model’s performance applied to real-world scenarios.
Open Sourced Large Language Models Recent studies have made efforts on democratic AI, where
the representative work is LLaMA (Touvron et al ., 2023) from Meta AI, an open-source LLM
with parameters ranging from 7B and 13B to 65B. LLaMA-13B has comparable and even better
performance than GPT-3 (Brown et al ., 2020) with 175B parameters on common sense reasoning tasks.
Following efforts have been proposed to improve LLaMA for instruction following like ChatGPT, by
instruction tuning. Such as Taori et al .(2023) proposed Alpaca by fine-tuning LLaMA-7B with 52K
instruction-following samples generated with the self-instruct method (Wang et al ., 2022). Chiang
et al.(2023) proposed Vicuna-13B by fine-tuning LLaMA-13B with 70K conversation data from
ShareGPT8. It can generate better answers to user’s questions compared with Alpaca. However,
there are no open-sourced LLMs and instruction-tuning data focused on the financial domain.
3 FIT: Financial Instruction Tuning Dataset
In this section, we introduce our financial instruction tuning dataset FIT, including the background
of raw data, tasks in FIT, and the construction process based on raw data. Different from existing
financial datasets, FIT is the first instruction-tuning dataset for finance LLMs and includes stock
movement prediction except for financial NLP tasks, which is fundamental for real-world financial
applications.
3.1 Raw Data
Derived from real-world finance scenarios, we build our financial instruction tuning dataset FIT
based on the open-sourced data of various financial NLP and prediction tasks. Compared with the
self-instruct method (Wang et al ., 2022) commonly used by existing LLMs such as Alpaca, we choose
to build instruction tuning datasets from open-sourced datasets due to the following reasons: 1) the
open-sourced datasets are usually annotated by domain experts, showing high quality, 2) it has very
low cost and has no limitation on commercial use unlike datasets constructed from ChatGPT or
GPT-4, 3) these open-sourced datasets cover a variety of text types such as news, reports and tweets,
as well as multi-modalities including time series data, tables, and texts. The details9of the raw data
and instruction data are shown in Table 2.
Financial Sentiment Analysis. Financial sentiment analysis task has long been a critical task in the
financial domain (Araci, 2019; Yang et al ., 2020), aiming to analyze the sentiment information of the
input financial texts. Following existing benchmark FLUE (Shah et al ., 2022), we use two datasets:
the Financial Phrase Bank (FPB) dataset (Malo et al., 2014) and FiQA-SA (Maia et al., 2018). FPB
includes English sentences from financial news and their sentiment label of positive, negative, or
neutral annotated by domain experts. FiQA-SA is another widely-adopted dataset, which aims to
predict the sentiment of English financial news and microblog posts on a scale of [-1,1], where 1
means the most positive.
6https://sites.google.com/nlg.csie.ntu.edu.tw/finweb2021/shared-task-finsbd-3
7https://bbt.ssymmetry.com/evaluation.html
8https://sharegpt.com
9For further details of the data split and pre-processing, please refer to Appendix
4

--- PAGE 5 ---
Table 2: The details of the raw data and instruction data.
Data Task Raw Instruction Data Types Modalities License
FPB sentiment analysis 4,845 48,450 news text CC BY-SA 3.0
FiQA-SA sentiment analysis 1,173 11,730 news headlines,tweets text Public
Headline news headline classification 11,412 11,412 news headlines text CC BY-SA 3.0
NER named entity recognition 1,366 13,660 financial agreements text CC BY-SA 3.0
FinQA question answering 8,281 8,281 earnings reports text,table MIT License
ConvFinQA question answering 3,892 3,892 earnings reports text,table MIT License
BigData22 stock movement prediction 7,164 7,164 tweets,historical prices text,time series Public
ACL18 stock movement prediction 27,053 27,053 tweets,historical prices text,time series MIT License
CIKM18 stock movement prediction 4,967 4,967 tweets,historical prices text,time series Public
News Headline Classification. The news headline classification task aims to analyze other informa-
tion, such as price movement in financial texts. We use the Gold news headline dataset (Sinha and
Khandait, 2021) consisting of news headlines from 2000 to 2019 about "gold" and their corresponding
9 tags: “price or not”, “price up”, “price down”, “price stable”, “past price”, “future price”, “past
general”, “future general”, “asset comparison”. The task is to conduct the binary classification for
each tag of each data sample.
Named Entity Recognition. Named Entity Recognition (NER) task is to detect critical financial
entities such as persons, organizations, and locations, which can be used to build financial knowledge
graphs. We use the FIN dataset (Alvarado et al ., 2015) including sentences from public financial
agreements through U.S. Security and Exchange Commission (SEC) filings and manually annotated
entity types from LOCATION (LOC), ORGANISATION (ORG) and PERSON (PER).
Question Answering. Question answering is the task of automatically answer a financial question
based on the provided information. We use two datasets: FinQA (Chen et al ., 2021) and Con-
vFinQA (Chen et al ., 2022). FinQA consists of question-answering pairs annotated by experts and
their corresponding earnings reports (including unstructured documents and tables) from S&P 500
companies. ConvFinQA is an expansion on FinQA that has conversations with the multi-turn question
and answering over earnings reports.
Stock Movement Prediction. As one of the fundamental financial tasks, stock movement prediction
has great potential value in real applications such as investment strategies. Following previous
work (Soun et al ., 2022), we frame the task as a binary classification problem, which is to predict the
binary stock price movement given historical stock prices and tweets. If price movement is higher
than 0.55%, it will be assigned to positive samples (1), or negative samples (-1) if it is lower than
-0.5%. We adopt three commonly-used datasets: BigData22 (Soun et al ., 2022), ACL18 (Xu and
Cohen, 2018), and CIKM18 (Wu et al., 2018).
3.2 Instruction construction
Base on the raw datasets, we further construct our financial instruction datasets, whose statistics are
presented in Table 2. We ask domain experts to write 10 diverse instructions for all datasets except
the ConvFinQA, where we only use one instruction. Since ConvFinQA is a multi-turn conversational
question-answering dataset, which has diverse questions as instructions in nature. For BigData22,
ACL18, CIKM18, we use the same instruction set, since they have the same data types of input data
and task formulation. We show the instruction examples in Table 3. Based on these prompts, we
convert raw datasets from these tasks into instruction-tuning samples, by gathering human-designed
instructions, and input texts along with responses of each dataset. For FPB, FiQA-SA, Headline, NER,
BigData22, ACL18, and CIKM18 datasets, we build instruction tuning samples with the following
template:
Instruction: [task prompt] Text: [input text] Response: [output]
[task prompt] is the prompt designed for each data, [input text] is the input financial data from each
data, e.g. the historical prices and tweets for stock movement prediction datasets, [output] is the
corresponding output for input text, e.g. sentiment label of input text from ["Positive", "Negative",
"Neutral"] in FiQA-SA dataset. For FPB, FiQA-SA, and NER, due to the limited data size, we employ
all 10 instructions for each sample, while we randomly sample one instruction for each sample in
Headline, BigData22, ACL18, and CIKM18 datasets.
5

--- PAGE 6 ---
Table 3: The example prompt for each dataset. FiQA-SA has two types of text, including news
headlines and tweets. We will fill the detailed text type into {category} for each data sample. For
stock movement prediction data such as BigData22, we will fill {tid} and {point} with the detailed
stock name and time from each data sample.
Data Prompt
FPB“Analyze the sentiment of this statement extracted from a financial news article.
Provide your answer as either negative, positive or neutral.
For instance, ’The company’s stocks plummeted following the scandal.’ would be classified as negative."
FiQA-SA“What is the sentiment of the following financial {category}:
Positive, Negative, or Neutral?"
Headline“Consider whether the headline mentions the price of gold.
Is there a Price or Not in the gold commodity market indicated in the news headline?
Please answer Yes or No."
NER“In the sentences extracted from financial agreements in U.S. SEC filings,
identify the named entities that represent a person (’PER’), an organization (’ORG’),
or a location (’LOC’). The required answer format is: ’entity name, entity type’.
For instance, in ’Elon Musk, CEO of SpaceX, announced the launch from Cape Canaveral.’,
the entities would be: ’Elon Musk, PER; SpaceX, ORG; Cape Canaveral, LOC’"
FinQA “Given the financial data and expert analysis, please answer this question:"
ConvFinQA“In the context of this series of interconnected finance-related queries and the additional information
provided by the pretext, table data, and post text from a company’s financial filings,
please provide a response to the final question. This may require extracting information
from the context and performing mathematical calculations. Please take into account the information provided in
the preceding questions and their answers when formulating your response:"
BigData22“Analyze the information and social media posts to determine if the closing price of {tid}
will ascend or descend at {point}. Please respond with either Rise or Fall."
In the sentences extracted from financial agreements in U.S. SEC filings, identify the named entities that
represent a person ("PER"), an organization ("ORG"), or a location ("LOC"). The required answer format
is: "entity name, entity type". For instance, in "Elon Musk, CEO of SpaceX, announced the launch from
Cape Canaveral.", the entities would be:
Elon Musk, PER
SpaceX, ORG
Cape Canaveral, LOC
Text: This LOAN AND SECURITY  AGREEMENT  dated January 27 , 1999 , between SILICON V ALLEY
BANK (" Bank "), a California - chartered bank with its principal place of business at 3003 Tasman Drive ,
Santa Clara , California 95054 with a loan production of fice located at 40 William St ., Ste .
Answer:
Contemplate the data and tweets to guess whether the closing price of $ba will surge or decline at 2020-
11-02. Please declare either Rise or Fall.
Context: date,open,high,low ,close,adj-close,inc-5,inc-10,inc-15,inc-20,inc-25,inc-30
2020-10-19,1.2,2.2,-0.6,-0.1,-0.1,-1.4,-1.2,-0.8,-2.4,-2.4,-2.6
...
Answer:Analyze the sentiment of this statement extracted from a financial news article. Provide your answer as
either negative, positive, or neutral. For instance, "The company's stocks plummeted following the
scandal." would be classified as negative.
Shares of Standard Chartered ( ST AN ) rose 1.2 % in the FTSE 100, while Royal Bank of Scotland ( RBS
) shares rose 2 % and Barclays shares ( BARC ) ( BCS ) were up 1.7 %.
...SILICON V ALLEY  BANK, ORG
Bank, ORG
California, LOC
bank, ORG
3003 Tasman Drive, LOC
Santa Clara, LOC
California, LOC
40 William St, LOC
Positive
Rise...
FinMANER
FPB
BigData22NER
FPB
BigData22
Figure 1: An overview of multi-task and multi-modal instruction tuning of FinMA for diverse
financial tasks.
For FinQA and ConvFinQA, we use the following template:
Instruction: [task prompt] Context: [input context] Question: [input question] Response: [an-
swer]
[input context] is the input contextual information for each data sample. For example, the input
context can be filled with the text and table from the filling files for FinQA. ConvFinQA has
multi-turn conversations with questions and answering. We thus use the following template: We
transform each turn of the conversation for each data sample into one instruction via the template,
which will append previous questions and answer in the [input context].
4 FinMA: Financial Large Language Model
We further build FinMA by fine-tuning LLaMA (Touvron et al ., 2023) with FIT. We train three
models: FinMA-7B and FinMA-30B by fine-tuning LLaMA 7B and 30B checkpoint with instruction
6

--- PAGE 7 ---
tuning data covering NLP tasks, and FinMA-7B-full by fine-tuning LLaMA 7B with full instruction
tuning data . We fine-tune LLaMA-7B with 15 epochs and LLaAM-7B-full with 3 epochs based on
AdamW optimizer (Loshchilov and Hutter, 2017). The batch size is set to 32, the initial learning
rate is 8e-6, and the weight decay is 1e-5. We also set warmup steps to 5% of all training steps. The
maximum length of input texts is 2048. The FinMA-7B is fine-tuned on 8 A100 40GB GPUs. As
for the FinMA-30B model, we fine-tune LLaMA-30B with 20 epochs, which is also based on the
AdamW optimizer. The batch size is set to 24, the initial learning rate is 8e-6, the weight decay
is 1e-5, and warmup steps to 5% of all training steps. The maximum length of input texts is 2048.
Different from FinMA-7B, it can only be distributed fine-tuned on 128 A100 40GB GPUs.
5 FLARE: Financial Evaluation Benchmark
Based on FIT, we design our financial natural language understanding and prediction evaluation
benchmark (FLARE). We randomly select validation sets from FIT to select the best model checkpoint,
and test sets for evaluation. Compared with the existing benchmark FLUE (Sanh et al ., 2022), FLARE
covers financial prediction tasks in addition to NLP tasks10. We believe it is vital to include financial
prediction tasks such as stock movement prediction, to comprehensively evaluate the performance of
LLMs on the practical applications of the financial domain. We show the data statistics of validation,
and test set for each dataset in Table 4. Following previous methods (Li et al ., 2023; Shah et al ., 2022),
Table 4: The details of our evaluation datasets. To compare the performance with BloombergGPT
whose test data is not openly-released, we keep the same numbers and data distributions of our test
datasets with that of BloombergGPT.
Data Task Valid Test Evaluation
FPB (Malo et al., 2014) sentiment analysis 7,740 9,700 F1, Accuracy
FiQA-SA (Maia et al., 2018) sentiment analysis 1,880 2,350 F1, Accuracy
Headline (Sinha and Khandait, 2021) news headline classification 1,0259 2,0547 Avg F1
NER (Alvarado et al., 2015) named entity recognition 1,029 980 Entity F1
FinQA (Chen et al., 2021) question answering 882 1,147 EM Accuracy
ConvFinQA (Chen et al., 2022) question answering 1,489 2,161 EM Accuracy
BigData22 (Soun et al., 2022) stock movement prediction 797 1,471 Accurady, MCC
ACL18 (Xu and Cohen, 2018) stock movement prediction 2,554 3,719 Accurady, MCC
CIKM18 (Wu et al., 2018) stock movement prediction 430 1,142 Accurady, MCC
we evaluate the performance of the sentiment classification task on FPB and FiQA-SA datasets, with
the accuracy (ACC) and weighted F1 Score (F1). The performance of the news headline classification
task is evaluated with the weighted averages of F1 score over all nine categories (Avg F1). For the
performance of NER task, we evaluate with the entity-level F1 score (Entity F1). The performance
on the question-answering task is evaluated with the exact match accuracy (EM Acc). As for the
financial prediction task, following previous methods (Xu and Cohen, 2018; Xie et al ., 2023), we
evaluate the performance with the accuracy (ACC) and the Matthews correlation coefficient (MCC).
6 Experiments on FLARE
The proposed FIT and FLARE allow to train, select the model, and evaluate the performance of
LLMs on financial understanding and predictions. In this section, we investigate how powerful the
FIT-fine-tuned FinMA and other LLMs are on FLARE. We compare FinMA with following LLMs:
1) BloombergGPT (Wu et al ., 2023). The only large language model with 50B parameters pre-trained
with the financial texts. 2) GPT-4 (OpenAI, 2023). A powerful instruction following large language
model with around 1T parameters proposed by OpenAI. 3) ChatGPT. A instruction following large
language model with 175B parameters from OpenAI. 4) BLOOM (Scao et al ., 2022). An open-access
multilingual large language model with 176B parameters. 5) GPT-NeoX (Black et al ., 2022). An
open-sourced large language model with 20B parameters. 6) OPT-66B (Zhang et al ., 2022). An
open-sourced language model with parameters from 125M to 175B. We use the OPT with 66B
parameters. 7) Vicuna-13B (Zhang et al ., 2022). An instruction following large language model by
fine-tuning LLaMA-13B.
10Following BloombergGPT, we don’t include the structure boundary detection task included in FLUE because
they are hard to be converted into the instruction following task.
7

--- PAGE 8 ---
Following previous methods (Wu et al ., 2023; Li et al ., 2023), we report the 20-shot performance of
BloombergGPT and the 5-shot performance of other baseline methods on the FIN dataset. We report
the 5-shot performance of BloombergGPT on FPB and FiQA-SA. We report the 5-shot performance
of all baselines on the News dataset. For the remaining results, we report the zero-shot performance.
The results of some baselines are based on human evaluations, since LLMs without fine-tuning will
fail to generate answers pre-defined in the given instruction. All results of FinMA are conducted on
zero-shot and can be automatically evaluated.
Table 5: The zero-shot and few-shot performance of different LLMs on the FLARE benchmark.
Some results are referenced from (Wu et al ., 2023; Li et al ., 2023; Xie et al ., 2023). BloombergGPT
doesn’t release their test datasets. Test datasets were built to have the same data distribution with
that of BloombergGPT and the performance of FinMA was directly compared with BloombergGPT
following the previous method (Li et al., 2023).
Dataset MetricsGPT
NeoXOPT
66BBLOOMChat
GPTGPT
4Bloomberg
GPTFinMA
7BFinMA
30BFinMA
7B-full
FPBAcc - - - 0.78 0.76 - 0.86 0.87 0.87
F1 0.45 0.49 0.50 0.78 0.78 0.51 0.86 0.88 0.87
FiQA-SA F1 0.51 0.52 0.53 - - 0.75 0.84 0.87 0.79
Headline AvgF1 0.73 0.79 0.77 0.77 0.86 0.82 0.98 0.97 0.97
NER EntityF1 0.61 0.57 0.56 0.77 0.83 0.61 0.75 0.62 0.69
FinQA EmAcc - - - 0.58 0.63 - 0.06 0.11 0.04
ConvFinQA EmAcc 0.28 0.30 0.36 0.60 0.76 0.43 0.25 0.40 0.20
BigData22Acc - - - 0.53 0.54 - 0.48 0.47 0.49
MCC - - - -0.025 0.03 - 0.04 0.04 0.01
ACL18Acc - - - 0.50 0.52 - 0.50 0.49 0.56
MCC - - - 0.005 0.02 - 0.00 0.00 0.10
CIKM18Acc - - - 0.55 0.57 - 0.56 0.43 0.53
MCC - - - 0.01 0.02 - -0.02 -0.05 -0.03
6.1 Results
Overall Performance. For financial NLP tasks, as shown in Table 5, our fine-tuned model FinMA
significantly outperform other LLMs on FPB, FiQA-SA and Headline datasets, showing the impor-
tance of domain specific instruction tuning on improving the performance of LLMs in the specific
domain. For example, FinMA-30B outperforms GPT-4 by 10% F1 score, and BloombergGPT by 37%
F1 score on the FPB dataset. On the NER dataset, FinMA-7B also outperforms BloombergGPT and
other LLMs, and achieve competitive results compared with ChatGPT and GPT-4. Yet for FinQA and
ConvFinQA which requires complex numeric reasoning, there is a large gap between the performance
of GPT and FinMA. As reported in existing studies (Touvron et al ., 2023; Lewkowycz et al ., 2022),
LLaMA includes no mathematical datasets for pre-training, resulted in poor performance on the
mathematical benchmark datasets such as GSM8K (Cobbe et al ., 2021). This finding indicate the
importance of numeric reasoning for financial question answering, which could be the potential
direction for advancing LLMs in the finance area. For financial prediction tasks, all LLMs including
FinMA, ChatGPT and GPT-4 struggle in stock movement prediction. After fine-tuned with both
NLP and financial prediction tasks, FinMA-7B-full can achieve a significantly better performance on
ACL18 dataset compared with ChatGPT and GPT-4. However, it still presents almost zero MCC on
the other two datasets like ChatGPT and GPT-4. This indicates the complexity and challenging of the
financial prediction tasks in FLARE. Compared with existing financial benchmarks that focusing on
NLP tasks, FLAPE provides exciting opportunities for the improvement of LLMs on the fundamental
of financial academic studies and applications.
Further Analysis. We further analyze the influences of model size, and instruction tuning data on
the performance of LLMs on different tasks. FinMA-30B has no significantly better performance
than FinMA-7B on most NLP tasks and the stock movement prediction task. Apparently, the quality
of the instructions rather than the model size is critical for the performance of these tasks. For the
complex question-answering tasks such as ConvFinQA, as shown in Table 6, the larger LLaMA
model generally has better performance. Particularly, Vicuna-7B based on LLaMA-7B has the worst
performance, which are also consistent with previous findings (Cobbe et al ., 2021) that LLaMA with
larger parameters presents better performance on mathematical benchmark datasets. FinMA-7B and
FinMA-30B are not fine-tuned with the financial prediction dataset, they show limited performance
on stock movement prediction similar to ChatGPT and GPT-4. Although they can be generalized to
8

--- PAGE 9 ---
the unseen financial prediction task, there is a large room to be improved along with ChatGPT and
GPT-4. In contrast, FinMA-7B-full fine-tuned with both NLP and prediction datasets, has shown
significantly better performance on the ACL18 dataset, and comparable performance on NLP tasks
with FinMA-7B and GPT-4. This indicates the potential of LLMs to be further adapted and applied
directly on financial prediction tasks via pre-training and fine-tuning on domain datasets.
Table 6: The performance of LLMs over ConvFinQA.
Metrics GPT-4 BloombergGPT FinMA-7B FinMA-30B FinMA-7B-full Vicuna-7B
EmAcc 0.76 0.43 0.25 0.40 0.20 0.10
7 Limitations
Despite the positive contributions of this study, we recognize the following limitations:
1.Model and Training Constraints : We only present FinMA models up to 30B. Due to
computational constraints, FinMA-30B has not been fine-tuned on the full dataset.
2.Complex Task Performance : FinMA, due to the limitation of the backbone model LLaMA,
struggles with tasks requiring quantitative reasoning, such as financial question answering,
and the difficult financial prediction task.
3.Resource Constraints and Generalizability : The development of FinMA, FIT, and FLARE
is influenced by available resources and handcrafted instructions, potentially affecting model
diversity and generalizability. The maximum input size of FinMA is also limited by the
maximum input texts that can be handled by the backbone model LLaMA.
4.Potential Negative Impacts : While our study primarily focuses on the positive aspects and
advancements of financial language understanding models, it is important to acknowledge
the potential negative impacts associated with their use, such as the spread of financial
misinformation or unethical market influence.
8 Conclusion
In this work, we presented PIXIU, encompassing the first open-sourced financial large language
model FinMA, the instruction tuning dataset FIT, and the evaluation benchmark FLARE. Through
extensive evaluation, we demonstrated the effectiveness of FinMA across various financial tasks,
showing the potential of domain-specific instruction tuning of large language models in the financial
domain. However, challenges such as improving performance on complex tasks and addressing
resource constraints remain. Our open-source contribution aims to facilitate further research and
innovation in financial language understanding, prediction, and LLMs, toward more useful and safe
LLMs in the field of finance.
Acknowledgement
This paper would not have been possible without the invaluable contribution of the BELLE code (Ji
et al., 2023a,b).
References
Julio Cesar Salinas Alvarado, Karin Verspoor, and Timothy Baldwin. 2015. Domain adaption of
named entity recognition to support credit risk assessment. In Proceedings of the Australasian
Language Technology Association Workshop 2015 . 84–90.
Dogu Araci. 2019. Finbert: Financial sentiment analysis with pre-trained language models. arXiv
preprint arXiv:1908.10063 (2019).
9

--- PAGE 10 ---
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He,
Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu
Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-
20B: An Open-Source Autoregressive Language Model. In Proceedings of the ACL Workshop on
Challenges & Perspectives in Creating Large Language Models .https://arxiv.org/abs/
2204.06745
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al .2020. Language models
are few-shot learners. Advances in neural information processing systems 33 (2020), 1877–1901.
Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema
Moussa, Matt Beane, Ting-Hao Huang, Bryan R Routledge, et al .2021. FinQA: A Dataset of
Numerical Reasoning over Financial Data. In Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing . 3697–3711.
Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang.
2022. Convfinqa: Exploring the chain of numerical reasoning in conversational finance question
answering. arXiv preprint arXiv:2210.03849 (2022).
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023.
Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. https:
//lmsys.org/blog/2023-03-30-vicuna/
Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 2020. Electra: Pre-
training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555
(2020).
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al .2021. Training verifiers to
solve math word problems. arXiv preprint arXiv:2110.14168 (2021).
Weiguang Han, Boyi Zhang, Qianqian Xie, Min Peng, Yanzhao Lai, and Jimin Huang. 2023a. Select
and Trade: Towards Unified Pair Trading with Hierarchical Reinforcement Learning. arXiv
preprint arXiv:2301.10724 (2023).
Weiguang Han, Boyi Zhang, Qianqian Xie, Min Peng, Yanzhao Lai, and Jimin Huang. 2023b. Select
and Trade: Towards Unified Pair Trading with Hierarchical Reinforcement Learning. ArXiv
abs/2301.10724 (2023).
Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Baochang Ma, and Xiangang Li. 2023a.
BELLE: Be Everyone’s Large Language model Engine. https://github.com/LianjiaTech/
BELLE .
Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, Baochang Ma, and Xiangang
Li. 2023b. Exploring the Impact of Instruction Data Scaling on Large Language Models: An
Empirical Study on Real-World Use Cases. arXiv preprint arXiv:2303.14742 (2023).
Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT: Pre-training of
Deep Bidirectional Transformers for Language Understanding. In Proceedings of NAACL-HLT .
4171–4186.
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay
Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al .2022. Solving
quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858 (2022).
Xianzhi Li, Xiaodan Zhu, Zhiqiang Ma, Xiaomo Liu, and Sameena Shah. 2023. Are ChatGPT and
GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical
Tasks. arXiv preprint arXiv:2305.05862 (2023).
Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V
Le, Barret Zoph, Jason Wei, et al .2023. The flan collection: Designing data and methods for
effective instruction tuning. arXiv preprint arXiv:2301.13688 (2023).
10

--- PAGE 11 ---
Alejandro Lopez-Lira and Yuehua Tang. 2023. Can ChatGPT Forecast Stock Price Movements?
Return Predictability and Large Language Models. arXiv preprint arXiv:2304.07619 (2023).
Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101 (2017).
Dakuan Lu, Jiaqing Liang, Yipei Xu, Qianyu He, Yipeng Geng, Mengkun Han, Yingsi Xin, Hengkui
Wu, and Yanghua Xiao. 2023. BBT-Fin: Comprehensive Construction of Chinese Financial
Domain Pre-trained Language Model, Corpus and Benchmark. arXiv preprint arXiv:2302.09432
(2023).
Macedo Maia, Siegfried Handschuh, André Freitas, Brian Davis, Ross McDermott, Manel Zarrouk,
and Alexandra Balahur. 2018. Www’18 open challenge: financial opinion mining and question
answering. In Companion proceedings of the the web conference 2018 . 1941–1942.
Pekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wallenius, and Pyry Takala. 2014. Good debt
or bad debt: Detecting semantic orientations in economic texts. Journal of the Association for
Information Science and Technology 65, 4 (2014), 782–796.
OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al .2022. Training language models to
follow instructions with human feedback. Advances in Neural Information Processing Systems 35
(2022), 27730–27744.
Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al .2022. Multitask Prompted Training
Enables Zero-Shot Task Generalization. In ICLR 2022-Tenth International Conference on Learning
Representations .
Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al .2022. Bloom: A
176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100
(2022).
Raj Sanjay Shah, Kunal Chawla, Dheeraj Eidnani, Agam Shah, Wendi Du, Sudheer Chava, Natraj
Raman, Charese Smiley, Jiaao Chen, and Diyi Yang. 2022. WHEN FLUE MEETS FLANG:
Benchmarks and Large Pre-trained Language Model for Financial Domain. arXiv preprint
arXiv:2211.00083 (2022).
Ankur Sinha and Tanmay Khandait. 2021. Impact of news on the commodity market: Dataset
and results. In Advances in Information and Communication: Proceedings of the 2021 Future of
Information and Communication Conference (FICC), Volume 2 . Springer, 589–601.
Yejun Soun, Jaemin Yoo, Minyong Cho, Jihyeong Jeon, and U Kang. 2022. Accurate Stock Movement
Prediction with Self-supervised Learning from Sparse Noisy Tweets. In 2022 IEEE International
Conference on Big Data (Big Data) . IEEE, 1691–1700.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An Instruction-following LLaMA
model. https://github.com/tatsu-lab/stanford_alpaca .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée
Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al .2023. Llama: Open
and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023).
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi,
and Hannaneh Hajishirzi. 2022. Self-Instruct: Aligning Language Model with Self Generated
Instructions. arXiv preprint arXiv:2212.10560 (2022).
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv
preprint arXiv:2109.01652 (2021).
11

--- PAGE 12 ---
Huizhe Wu, Wei Zhang, Weiwei Shen, and Jun Wang. 2018. Hybrid deep sequential modeling for
social text-driven stock prediction. In Proceedings of the 27th ACM international conference on
information and knowledge management . 1627–1630.
Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prab-
hanjan Kambadur, David Rosenberg, and Gideon Mann. 2023. Bloomberggpt: A large language
model for finance. arXiv preprint arXiv:2303.17564 (2023).
Qianqian Xie, Weiguang Han, Yanzhao Lai, Min Peng, and Jimin Huang. 2023. The Wall Street
Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction
Challenges. arXiv preprint arXiv:2304.05351 (2023).
Yumo Xu and Shay B Cohen. 2018. Stock movement prediction from tweets and historical prices. In
Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers) . 1970–1979.
Yi Yang, Mark Christopher Siy Uy, and Allen Huang. 2020. Finbert: A pretrained language model
for financial communications. arXiv preprint arXiv:2006.08097 (2020).
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher
Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al .2022. Opt: Open pre-trained transformer
language models. arXiv preprint arXiv:2205.01068 (2022).
Zhuosheng Zhang, Hanqing Zhang, Keming Chen, Yuhang Guo, Jingyun Hua, Yulong Wang, and
Ming Zhou. 2021. Mengzi: Towards lightweight yet ingenious pre-trained models for chinese.
arXiv preprint arXiv:2110.06696 (2021).
12
