# 2309.13064.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/finance/2309.13064.pdf
# File size: 230648 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
InvestLM: A Large Language Model for Investment using Financial
Domain Instruction Tuning
Yi Yang∗, Yixuan Tang , Kar Yan Tam
Hong Kong University of Science and Technology
{imyiyang,yixuantang,kytam}@ust.hk
Abstract
We present a new financial domain large lan-
guage model, InvestLM, tuned on LLaMA-65B
(Touvron et al., 2023), using a carefully curated
instruction dataset related to financial invest-
ment. Inspired by less-is-more-for-alignment
(Zhou et al., 2023), we manually curate a
small yet diverse instruction dataset, covering
a wide range of financial related topics, from
Chartered Financial Analyst (CFA) exam ques-
tions to SEC filings to Stackexchange quan-
titative finance discussions. InvestLM shows
strong capabilities in understanding financial
text and provides helpful responses to invest-
ment related questions. Financial experts, in-
cluding hedge fund managers and research
analysts, rate InvestLM’s response as com-
parable to those of state-of-the-art commer-
cial models (GPT-3.5, GPT-4 and Claude-2).
Zero-shot evaluation on a set of financial NLP
benchmarks demonstrates strong generalizabil-
ity. From a research perspective, this work sug-
gests that a high-quality domain specific LLM
can be tuned using a small set of carefully cu-
rated instructions on a well-trained foundation
model, which is consistent with the Superfi-
cial Alignment Hypothesis (Zhou et al., 2023).
From a practical perspective, this work devel-
ops a state-of-the-art financial domain LLM
with superior capability in understanding fi-
nancial texts and providing helpful investment
advice, potentially enhancing the work effi-
ciency of financial professionals. We release
the model parameters to the research commu-
nity1.
1 Introduction
Large language models (LLMs) have significantly
changed the paradigm of natural language process-
ing (Brown et al., 2020; Touvron et al., 2023) and
∗Correspondence: imyiyang@ust.hk
1InvestLM adopts the same licensing terms as LLaMA
(Touvron et al., 2023). Link: https://github.com/
AbaciNLP/InvestLMhold great potential for artificial general intelli-
gence (Bubeck et al., 2023). Several financial do-
main LLMs have also been developed with the
hope of processing massive financial texts and en-
hancing investment and financial decision-making
for investors and financial professionals.
However, three challenges may hinder the broad
development and adoption of financial domain
LLMs. First, BloombergGPT (Wu et al., 2023),
a foundation model with 50 billion parameters
trained on Bloomberg’s proprietary data, is not
publicly available. Thus, the community cannot
study its capabilities in financial tasks. Second,
while other commercialized LLMs such as Chat-
GPT and Claude-2 are accessible via API, their
model parameters are not publicly available either,
making it expensive to investigate their financial
task capability. Third, the research community has
released several LLMs fine-tuned on financial NLP
tasks, such as FinMA (Xie et al., 2023) and Fin-
GPT (Yang et al., 2023). However, these models
are not only smaller in size2but also exhibit poor
performance when generalizing to financial NLP
tasks beyond their instructed tasks.
To address the challenges, in this work, we de-
velop a new financial domain LLM: InvestLM. In-
vestLM is trained on the LLaMA 65B, using a
carefully curated instruction dataset related to fi-
nance and investment. We release the parameters
of InvestLM and adopt the same licensing terms
as LLaMA. We evaluate the InvestLM’s utility in
providing helpful investment advice by collaborat-
ing with financial experts, including hedge fund
managers and research analysts. Experts rate In-
vestLM’s responses as comparable to those of state-
of-the-art commercial models (GPT-3.5, GPT-4 and
Claude-2). An example of InvestLM and three com-
mercial models’ responses are presented in Table
1. According to expert feedback, GPTs often give
2At the time of writing, the largest FinMA model is 30B
and FinGPT is 7B.arXiv:2309.13064v1  [q-fin.GN]  15 Sep 2023

--- PAGE 2 ---
The railroad company saw earnings roll a bit off track when it reported July 20. The company slightly missed analysts’ revenue
expectations and reported earnings per share (EPS) that matched Wall Street’s estimates. That EPS performance might be fine
for many companies, but it rang warning bells for CSX investors because it was the first time in five years that CSX failed to
surpass EPS estimates. The company saw declining volume in a number of key products it transports, including agricultural and
food, chemicals, and forest. V olume growth in coal provided some locomotion. Intermodal, which means using two transport
modes to move freight (which often means converting freight from highway to intermodal rail), continued to struggle in the
latest quarter. Railroads have grappled with worker shortages and supply chain issues. The latest CSX earnings raises questions
about overall U.S. economic health.
Given the recent performance of the railroad company, is it advisable to buy, hold, or sell stocks in the sector?
InvestLM Based on the information provided, the stock performance appears to be mixed. While revenue missed expec-
tations, EPS met estimates, and coal volume provided some growth. On the other hand, declining volume in
key product categories along with ongoing labor shortages and supply chain issues raise concerns. Overall, the
outlook seems uncertain. As a result, a "hold" recommendation may be more appropriate than an outright "buy"
or "sell" at this time. Investors should continue to monitor the economic indicators and company fundamentals
before making an investment decision.
GPT-3.5 I’m an AI language model, and I can provide you with some information to consider, but please note that
investing decisions should be based on thorough research and analysis. The recent performance of the railroad
company, CSX, seems to have raised concerns about its earnings and volume decline in key product categories.
The struggles with intermodal transportation and overall supply chain issues may also impact the company’s
performance.
When making investment decisions, it’s important to consider factors such as the company’s long-term prospects,
industry trends, and the overall market conditions. Here are a few points to keep in mind:
1. Research the sector: Look into the railroad industry as a whole and analyze its growth potential, overall
trends, and challenges. Understand how other companies within the sector are performing. 2. Evaluate the
company’s fundamentals: Assess CSX’s financial statements, including revenue trends, profit margins, and debt
levels. Consider the company’s competitive position, management team, and their strategies for addressing the
challenges they face. 3. Consider market conditions: Examine the wider economic environment and its impact on
the railroad industry. Factors such as macroeconomic trends, government regulations, and consumer demand can
influence railroad stocks. 4. Diversification: If you already have holdings in the sector, consider the diversification
of your portfolio. Adding more railroad stocks may increase exposure to similar challenges. Diversifying across
other sectors can help mitigate risk.
Ultimately, it is advisable to consult with a financial advisor or conduct further research to make an informed
decision about buying, holding, or selling stocks in the railroad sector. They can provide personalized advice
based on your financial goals and risk tolerance.
GPT-4 As an AI language model, I can provide you with a logical analysis based on data provided, but can’t provide
personalized financial advice. Here are some considerations:
1. **Buy**: If you believe the shortcomings in the CSX earnings report are temporary and that the company can
overcome the challenges it faces, you may choose to buy. A potential advantage is that often when companies
or sectors are performing relatively poorly, their stocks could potentially be undervalued, offering a buying
opportunity for long-term investors. Furthermore, any recovery in the volume of key products could provide a
significant boost to the company’s earnings in the future.
2. **Hold**: If you already own shares of CSX and believe that the company will weather the difficulties over
time, holding could be a viable strategy. If you’re optimistic about the long-term prospects of the rail industry or
the U.S. economy as a whole, there could be value in waiting for improvements.
3. **Sell**: If you believe the challenges that CSX faces are indicative of persistent issues within the rail industry
or the broader economy, it might be a signal to reduce your exposure to this sector.
Overall, the decision should be based on your risk appetite, your faith in the company’s management to turn
things around, and your understanding of the overall industry and economic climate. It may be wise to consult
with a financial advisor or conduct further stock analysis before making a decision.
Claude-2 Based on the information provided, here is my assessment of whether it would be advisable to buy, hold, or sell
stocks in the railroad sector:
- The miss on revenue expectations and meeting EPS expectations rather than beating them could indicate weaker
demand for transportation services. Declining volume in key product categories also points to softer economic
activity. This raises questions about the overall economic outlook.
- Continued struggles in intermodal transportation suggests ongoing supply chain issues are weighing on the
sector. Worker shortages are also a challenge railroads are facing.
- The recent CSX earnings report was the first time in 5 years they did not beat EPS expectations, suggesting the
company is facing more headwinds now compared to the past several years.
- Taken together, the mixed results from CSX and the issues around demand, volumes, supply chains and labor
point to near-term weakness in the railroad sector. While transportation volumes tend to recover as economies
strengthen, the current environment raises uncertainties.
Given this analysis, I would say it is not advisable to buy additional stocks in the railroad sector at this time.
The recent performance and underlying issues suggest risk that future earnings and growth may disappoint. For
current investors, holding positions makes sense if taking a long-term view, but consider trimming or selling
portions of positions to reduce risk exposure until the outlook becomes clearer. In summary, a ""hold"" viewpoint
seems most prudent, with a caveat to potentially reduce risk through selective selling.
Table 1: InvestLM, GPT-3.5, GPT-4, and Claude-2 response to the input question.

--- PAGE 3 ---
vague responses to investment-related questions,
possibly because of its safety requirements. In
scenarios where commercial models offer decisive
answers, InvestLM typically arrives at a logical in-
vestment conclusion similar to these commercial
models, but it is often more concise. Case studies
also reveal that InvestLM effectively mitigates the
hallucination problems often found in LLaMA’s
responses. In addition, InvestLM shows strong gen-
eralizability on several financial NLP tasks - those
that are not used in the instruction tuning.
In addition to contributing a new financial do-
main LLM, we are also interested in understanding
the behaviors of domain instruction tuning. We find
that a diverse set of domain instructions is very ef-
fective in "transforming" a high-quality foundation
model (such as LLaMA-65B) into a high-quality
domain-specific model - suggesting the consistency
with the Superficial Alignment Hypothesis (Zhou
et al., 2023). Second, we discover that generic in-
structions, like those used in Alpaca (Taori et al.,
2023), can detrimentally impact the performance
of instruction-tuned models on domain tasks. This
underscores the importance of curating domain-
specific instructions. Together, these findings pro-
vide insights into how to fine-tune a foundation
model for a specific domain.
The rest of the paper is organized as follows.
First, we describe our manually curated financial
domain instruction dataset. This is followed by an
overview of the instruction tuning procedures for
developing InvestLM. Subsequently, we present ex-
pert evaluation results and report the performance
of InvestLM, comparing it with other LLMs on a
set of financial NLP benchmarks. Lastly, we con-
duct studies to better understand the behaviors of
domain instruction tuning.
2 Instruction Dataset
Our goal is to construct a financial LLM capa-
ble of understanding financial text and providing
helpful responses to investment-related questions.
The Superficial Alignment Hypothesis (Zhou et al.,
2023) posits that A model’s knowledge and capabil-
ities are learned almost entirely during pretraining,
while alignment teaches it which subdistribution
of formats should be used when interacting with
users . Thus, we hypothesize that a well-trained
large language model, such as LLaMA-65B, has
already acquired the ability to understand financial
text, given that its training corpus might encompassfinancial content. So, to enable LLaMA to interact
with investment-related questions, we should fine
tune it with specifically crafted instructions.
We manually curate an instruction dataset from
the following resources:
Stackexchange QFin. We select a set of ques-
tions along with their corresponding high-vote an-
swers from the quantitative finance board.
CFA questions. We choose a set of questions
with detailed answers (including explanations)
from the Chartered Financial Analyst (CFA) ex-
ams.
Academic Journals. We choose articles from
top financial economics journals (such as Journal
of Finance ) and manually create questions related
to asset pricing and risk management, and solicit
answers from the articles.
Textbooks. We choose several classic finance
textbooks (such as Investments, 10th Edition ), and
select the exercises from these textbooks as instruc-
tion/inputs and find their corresponding standard
answers as outputs.
SEC filings. We select earnings conference call
transcripts and SEC filings, curate several questions
based on the content of the filings, and then extract
the corresponding answers from the text.
Financial NLP tasks. We reformat several finan-
cial NLP tasks, such as financial sentiment analysis,
and numerical reasoning as instructions. It’s worth
noting that, even for instructions derived from fi-
nancial NLP tasks, we do not directly use the corre-
sponding label as the output. Instead, we manually
provide a response to augment the label.
Investment questions. We brainstorm financial
and investment-related questions, then use Chat-
GPT and Claude-2 to generate answers. Subse-
quently, we manually verify and paraphrase the
best answer to use as the output.
For each instruction resource, we gather sev-
eral hundred examples, with each example being
a tuple comprising instruction, input, and output.
The detailed breakdown of our financial domain in-
struction dataset is shown in Table 2. Compared to
other general-purpose instruction datasets, such as
that used in Alpaca (Taori et al., 2023), our finance-
specific instruction dataset has much longer instruc-
tion/input and output lengths. Most importantly,

--- PAGE 4 ---
Size Input len. Resp. len.
All 1,335 152.9 145.5
Stackexchange 205 19.4 296.2
CFA 329 125.6 157.4
Academic Journals 200 169.3 74.8
Textbooks 200 128.9 136.6
SEC Filings 80 316.2 88.2
Financial NLP tasks 200 325.9 74.5
Investments 119 72.7 144.3
Table 2: Detailed breakdown of our financial domain
instruction dataset. Input len. and Resp. len. denotes
the average number of tokens of instruction/input and
output response respectively.
our dataset is manually curated to encompass top-
ics related to financial investment.
3 Instruction Tuning on LLaMA
We train InvestLM on LLama-65B using our fi-
nancial domain instruction dataset (Touvron et al.,
2023). Specifically, we use the Low-rank adapta-
tion (LoRa) method (Hu et al., 2021) to tune the
model parameters in order to enhance the train-
ing efficiency. We set the rank to 16. We specifi-
cally target LoRa modules as "q_proj," "k_proj,"
"v_proj," "down_proj," "gate_proj," and "up_proj".
Moreover, most financial texts such as SEC fil-
ings, corporate disclosures, and analyst reports are
long text, usually consisting of thousands of tokens.
Thus it is important that the financial domain LLM
can handle long text. To this end, we utilize Linear
Rope Scaling (Chen et al., 2023) on a scale of 4
to increase context size to 8,192 , which improves
InvestLM’s ability to handle long financial texts.
The detailed training prompt format is shown in
Appendix A.
Following standard fine-tuning hyperparameters,
we train the LLaMA-65B model for 15 epochs. The
learning rate is set to 3e-4, and the batch size to 16
examples. In the subsequent analysis, we also in-
struction tune a LLaMA-7B model. For this model,
we conduct training for 12 epochs with a learning
rate of 3e-3 and a batch size of 32 samples. For
simplicity, we denote InvestLM-65B as InvestLM,
and LLaMA-65B as LLaMA.
4 Expert Evaluation
The primary goal of this work is to build a finan-
cial LLM capable of understanding financial text
and providing helpful responses to investment re-
lated questions. To test whether InvestLM can of-
fer helpful responses, we collaborate and conduct
interviews with a group of six financial experts,including hedge fund managers and financial ana-
lysts.
Baselines . We compare InvestLM with three state-
of-the-art commercial models, GPT-3.5 ,GPT-4
andClaude-2 . OpenAI’s GPT-3.5 and GPT-4 are
large language models tuned with reinforcement
learning from human feedback (RLHF) (Ouyang
et al., 2022). Anthropic’s Claude-2 is a large lan-
guage model that can take up to 100K tokens in the
user’s prompt.3Responses from all baselines are
sampled throughout August 2023.
We manually write 30 test questions that are
related to financial markets and investment. For
each question, we generate a single response from
InvestLM and the three commercial models. We
then ask the financial experts to compare InvestLM
responses to each of the baselines and label which
response is better or whether neither response is
significantly better than the other.
In addition to the expert evaluation, we also con-
duct a GPT-4 evaluation, following the same pro-
tocol used in (Zhou et al., 2023). Specifically, we
send GPT-4 with exactly the same instructions and
data annotations, and ask GPT-4 which response is
better or whether neither response is significantly
better than the other. The expert evaluation inter-
face and GPT-4 evaluation prompt are presented in
Appendix B.
The expert evaluation and GPT-4 evaluation
results are presented in Figure 1 and Figure 2.
These results indicate that financial experts rate
InvestLM’s responses as either comparable to or
better than those of the GPT-3.5 and GPT-4 mod-
els. This expert assessment aligns with GPT-4’s
own evaluation, which also prefers InvestLM’s
responses. While financial experts tend to favor
Claude-2’s responses over InvestLM most of the
time, GPT-4 shows a preference for InvestLM’s re-
sponses. Overall, it is encouraging to observe that
our domain-specific instruction tuning effectively
generates helpful answers to investment-related
questions, especially considering that its founda-
tional model, LLaMA, frequently produces halluci-
nations, as shown in Section 4.1.
Inter-Annotator Agreement . We compute the
Inter-Annotator Agreement of evaluation by the
following rules. We compute the tie-discounted
accuracy for each example between the majority-
vote label and each annotator’s label. If they match,
3https://www.anthropic.com/index/
claude-2

--- PAGE 5 ---
Figure 1: Expert evaluation.
 Figure 2: GPT-4 evaluation.
it scores 1; if one labels it as ‘Tie’, it scores 0.5;
for all other scenarios, the score is 0. Among all
the annotations, we find the expert-expert score is
72.5%. Despite the subjectivity involved in classi-
fying responses, a reasonable level of agreement is
maintained among the annotators.
4.1 InvestLM vs. LLaMA response
In our expert evaluation, we do not compare In-
vestLM with LLaMA because LLaMA is a founda-
tion model that is not instruction tuned, and thus
we believe such a comparison would be unfair. As
a case study, we explore the difference between
InvestLM and LLaMA generated response.
Take the following financial news and instruc-
tions as an example. Input :U.S. investors now
bake in about a 60% chance that the Fed will raise
interest rates at least another quarter point by the
end of the year, pricing in relatively high proba-
bilities of a move at the November meeting. The
likelihood of a rate pause in September, however,
remains near 80%. The Fed last paused in June
before raising rates in July. .Instruction :Based
on the news, should I buy some stocks today?
LLaMA’s response :"No! It is not time to buy
yet. We are still waiting for the market to go down
more and then we can start buying again. The
S&P50000 closed yesterday was -194.372 points
or -0.37%, Dow Jones Industrial Average dropped
131058points-0.7%, Nasdaq Composite lost 10.8%
and Russell 20000000 fell 1.2.1%."
InvestLM’s response :Based on the news, the
answer is not clear. While investors are anticipat-
ing more rate hikes, they are also expecting a high
probability of a pause in September. The uncer-
tainty around the Fed’s next moves makes it difficult
to predict how stocks will react in the near-term.
There are a few factors that could influence stock
prices in the coming months, including inflation
levels, geopolitical events, and earnings reports.
Without knowing more about your individual situ-ation and risk tolerance, it is impossible to say if
investing in stocks today is a good idea. However,
it is important to do your research and understand
the risks involved before making any investment
decisions.
It is evident that LLaMA’s response is riddled
with hallucinations, introducing numbers that are
not even mentioned in the news. In contrast, In-
vestLM’s response is grounded in the information
presented in the news and reflects logical reasoning
with consideration of risks. This underscores the
value of domain instruction tuning.
5 How does InvestLM generalize to
financial NLP tasks?
We further evaluate InvestLM’s performance on fi-
nancial NLP benchmarks, following the evaluation
protocol used in (Shah et al., 2022). For the fi-
nancial sentiment analysis task, we consider Finan-
cial PhraseBank (FPB) dataset (Malo et al., 2014),
FOMC dataset (Shah et al., 2023), FiQA dataset
(FiQA), and FinSent (Huang et al., 2023; Yang
et al., 2020). For the financial document classifica-
tion task, we consider ESG, FLS, and QA datasets.
For the financial numerical reasoning task, we con-
sider the FinQA dataset (Chen et al., 2021). For the
financial text summarization task, we consider the
ECTSum dataset (Mukherjee et al., 2022). Among
the datasets, FinSent ,ESG ,FLS andQAare pro-
prietary datasets that we developed for financial
NLP model evaluation4. Other datasets, including
4FinSent is a sentiment classification dataset containing
10,000 manually annotated sentences from analyst reports of
S&P 500 firms. ESG is a text classification dataset that evalu-
ates an organization’s strategy on environmental, social, and
corporate governance. It contains 2,000 manually annotated
sentences from firms’ ESG reports and annual reports. FLS is
a text classification dataset that infers a firm’s beliefs and opin-
ions about it future events or results in the forward-looking
statements. It contains 3,500 manually annotated sentences
from the Management Discussion and Analysis section of an-
nual reports of Russell 3000 firms. QAis a text classification
dataset containing question-answering pairs extracted from

--- PAGE 6 ---
Dataset Metric LLaMA-65B InvestLM GPT-3.5 BloombergGPT FinMA GPT-4
FinSent Micro-F1 0.71 0.79 0.75 - 0.80 0.81
FPB Micro-F1 0.38 0.71 0.75 0.51 0.88 0.90
FOMC Micro-F1 0.53 0.61 0.60 - 0.52 0.73
FiQA Micro-F1 0.75 0.90 0.77 0.75 0.87 0.92
ESG Micro-F1 0.67 0.80 0.64 - 0.51 0.63
FLS Micro-F1 0.60 0.51 0.57 - 0.27 0.57
QA Micro-F1 0.73 0.81 0.71 - 0.68 0.78
FinQA Acc 0.23 0.29 0.47 - 0.15 0.54
ECTSumRouge-1 0.14 0.26 0.21 - 0.08 0.30
Rouge-2 0.12 0.12 0.13 - 0.01 0.15
Rouge-L 0.13 0.17 0.15 - 0.06 0.20
CHRF++ 23.65 31.53 29.79 - 6.34 36.31
Table 3: Different LLMs performance on financial NLP benchmarks. Bold indicates the best performance metric,
and underline indicates the second-best performance metric.
FPB ,FOMC ,FiQA ,FinQA andECTSum are
publicly available.
We consider the following LLMs, including two
instruction tuned models GPT-3.5 ,GPT-4 from
OpenAI, two financial LLMs, BloombergGPT (a
50B foundation model) and FinMA (an instruction-
tuned model on LLaMA-7B), and one foundation
model LLaMA-65B , upon which InvestLM is built.
We do not consider the FinGPT model, as it was
only built for sentiment analysis at the time of writ-
ing.
5.1 Generalizability on financial NLP tasks
The results are presented in Table 3. First, compar-
ing InvestLM against LLaMA-65, we can see that
domain instruction tuning is very effective. In 8 out
of 9 tasks, InvestLM outperforms LLaMA-65. We
want to emphasize that InvestLM is not explicitly
tuned on these financial NLP tasks - only a small
portion of instructions are formatted using finan-
cial NLP tasks. This is very encouraging given
that our domain instruction dataset only contains
around 1,300 examples. Second, GPT-4 achieves
the best performance in 6 out of the 9 tasks, while
InvestLM achieves the best performance in 2 out
of the 9 tasks, suggesting that GPT-4 is the state-
of-the-art commercial LLM.
5.2 The benefit of domain instruction tuning
To assess the advantages of domain instruction tun-
ing across foundation models of varying sizes, we
train an InvestLM-7B model on the LLaMA-7B
foundation model using our domain instruction
dataset. The results are presented in Table 4. No-
tably, the relative improvement brought about by
earnings conference call transcripts. The goal of the dataset is
to identify whether the answer is valid to the question.Dataset Metric LLaMA-
7BInvestLM-
7B∆-7B LLaMA-
65BInvestLM-
65B∆-65B
FinSent Micro-F1 0.53 0.69 31.1% 0.71 0.79 11.8%
FPB Micro-F1 0.12 0.74 523.7% 0.38 0.71 89.3%
FOMC Micro-F1 0.25 0.40 57.6% 0.53 0.61 15.6%
FiQA Micro-F1 0.31 0.76 145.0% 0.75 0.90 20.8%
ESG Micro-F1 0.19 0.61 218.4% 0.67 0.80 20.1%
FLS Micro-F1 0.34 0.53 54.2% 0.60 0.51 -14.5%
QA Micro-F1 0.72 0.84 16.5% 0.40 0.81 10.8%
FinQA Acc 0.07 0.07 -4.8% 0.03 0.29 25.0%
EctSUMRouge-1 0.06 0.24 282.2% 0.14 0.26 85.0%
Rouge-2 0.06 0.10 62.9% 0.05 0.12 4.4%
Rouge-L 0.06 0.15 147.8% 0.09 0.17 37.4%
CHRF++ 12.90 29.18 126.2% 19.48 31.53 33.3%
Avg. 138.4% 28.2%
Table 4: Performance of LLaMA and InvestLM of dif-
ferent model size. ∆column indicates relative improve-
ment between InvestLM and corresponding LLaMA.
domain instruction tuning is considerably more pro-
nounced for the smaller 7B model compared to the
65B model. Specifically, for the LLaMA-7B model,
domain instruction tuning improves performance
by an average of 138.4% across tasks. In contrast,
for the LLaMA-65B model, there’s a performance
increment of 28.2%. The results indicate that in
scenarios where computational constraints prevent
deploying a 65B model, domain instruction tuning
proves vital in optimizing the performance of the
smaller model.
5.3 Do generic instructions help?
InvestLM is specifically tuned using domain spe-
cific instructions related to financial investment. In
this section, we aim to explore whether the inclu-
sion of general-purpose instructions, such as the
instruction-following data employed in the Alpaca
model (Taori et al., 2023), can further enhance the
model’s performance in domain NLP tasks. Given
that the general-purpose instruction dataset encom-
passes instructions related to numerical reasoning
and sentiment, there is potential that integrating
general-purpose instructions could also improve

--- PAGE 7 ---
Dataset Metric LLaMA-7B InvestLM-7B InvestLM-7B+Alpaca-
Instructions
FinSent Micro-F1 0.53 0.69 0.64
FPB Micro-F1 0.12 0.74 0.42
FOMC Micro-F1 0.25 0.40 0.32
FIQA ABSA Micro-F1 0.31 0.76 0.40
ESG Micro-F1 0.19 0.61 0.48
FLS Micro-F1 0.34 0.53 0.17
QA Micro-F1 0.72 0.84 0.40
FinQA Acc 0.07 0.07 0.03
EctSUMRouge-1 0.06 0.24 0.14
Rouge-2 0.06 0.10 0.05
Rouge-L 0.06 0.15 0.09
Bert Score 0.73 0.78 0.75
CHRF++ 12.90 29.18 19.48
Table 5: Performance of InvestLM-7B trained using
different instruction dataset.
the model’s capability in financial NLP tasks.
To answer this question, we incorporate the
instruction-following data used in the fine-tuning of
the Alpaca model,5, comprising 52K instructions,
into our domain instruction dataset. Using this aug-
mented dataset, we train an InvestLM-7B+Alpaca-
Instructions model. We then evaluate the utility of
generic instructions on the financial NLP bench-
marks, with the results detailed in Table Table 5.
The results lead to an interesting finding that
the inclusion of generic instructions appears to
negatively impact the model’s generalizability on
domain-specific NLP tasks. When comparing
InvestLM-7B+Alpaca-Instructions (trained on the
combined instruction dataset) to InvestLM-7B
(trained solely on the domain instruction dataset),
it’s evident that InvestLM-7B consistently outper-
forms InvestLM-7B+Alpaca-Instructions across all
tasks.
This underscores the value of our carefully
curated domain instructions. This finding sug-
gests that rather than generating a large volume
of general-purpose instructions, creating a set of
high-quality, domain-specific instructions can be
more effective in tapping into a model’s capabilities
for domain tasks.
6 Related Work
Financial Domain LLMs. Both industry and
academia are closely examining the use of LLMs
to analyze vast volumes of financial text, aiming to
enhance investment decision-making and risk man-
agement capabilities. Before the release of Chat-
GPT, several financial domain language models,
such as FinBERT (Huang et al., 2023), were devel-
oped based on BERT’s architecture. Inspired by the
5https://github.com/tatsu-lab/
stanford_alpacaremarkable performance of ChatGPT, efforts have
been made to build financial domain large language
models. Among these, BloombergGPT stands out
as the first foundation model with 50B parame-
ters, trained using Bloomberg’s internal corpora
(Wu et al., 2023). However, BloombergGPT is not
publicly accessible. FinMA tunes LLaMA using
publicly available NLP benchmarks formatted as in-
structions (Xie et al., 2023). InvestLM differs from
BloombergGPT in that we use instruction tuning,
whereas, to our knowledge, BloombergGPT serves
as a foundation model. InvestLM also diverges
from FinMA, while we instruction-tune LLaMA
using a carefully curated dataset that spans a wide
range of financial and investment topics, FinMA
relies on publicly available NLP benchmarks for-
matted as instructions, resulting in a more limited
instruction coverage. Similarly, FinGPT is a LLM
fine-tuned on the News and Tweets sentiment anal-
ysis dataset (Yang et al., 2023), which likely limits
its generalizability to other financial NLP tasks. To
our knowledge, InvestLM is the first open-source
financial domain LLM that can provide insightful
responses to investment-related questions, as cor-
roborated by financial professionals.
Instruction Tuning. Prior work has shown that in-
struction tuning — finetuning large language mod-
els on a collection of NLP tasks formatted with
instructions — can enhance the language model’s
capability to perform an unseen task from an in-
struction (Wei et al., 2021; Sanh et al., 2021).
Several general-purpose instruction datasets have
since been developed, including Natural Instruc-
tions (Mishra et al., 2021), the Flan collection
(Longpre et al., 2023), the OIG dataset (LAION.ai),
among others. Recent studies have proposed
automated methods for constructing instruction
datasets (Wang et al., 2022). Using various in-
struction datasets for training on LLaMA, a se-
ries of LLaMA-based LLMs have been developed,
including Alpaca (Taori et al., 2023) and Vicuna
(Chiang et al., 2023). However, most of these in-
struction datasets are tailored for training general-
purpose LLMs. Motivated by the less-is-more-for-
alignment (LIMA) (Zhou et al., 2023), which posits
that limited instruction tuning data is sufficient to
guide models towards generating high-quality out-
put, we carefully assemble a financial domain in-
struction dataset, specifically designed to provide
helpful responses for investors and financial profes-
sionals.

--- PAGE 8 ---
7 Conclusion
In this work, we present a new financial domain
LLM, InvestLM, instruction tuned on the LLaMA-
65B foundation model, using a set of manually
crafted instruction datasets covering diverse fi-
nancial and investment related topics. InvestLM
shows strong capabilities in understanding finan-
cial text and offers helpful insights in response
to investment-related inquiries. Expert rates In-
vestLM’s responses as comparable to those of state-
of-the-art commercial LLMs. Moreover, our work
sheds light on using a small yet high-quality in-
struction dataset to fine-tune a large foundational
model, suggesting a promising approach for craft-
ing domain-specific LLMs.
Ethics Statement
It is essential to recognize that the responses from
InvestLM should not be construed as definitive in-
vestment advice. All investment strategies and de-
cisions should be made after careful consideration
of one’s financial situation, risk tolerance, and in-
vestment goals. We strongly advocate for potential
investors to consult with professional financial ad-
visors and consider multiple information sources
before making any investment decisions. Relying
solely on InvestLM’s outputs without thorough due
diligence can lead to unintended financial conse-
quences. Thorough analyses should be conducted
to understand the strengths and weaknesses of In-
vestLM.
References
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Sébastien Bubeck, Varun Chandrasekaran, Ronen El-
dan, Johannes Gehrke, Eric Horvitz, Ece Kamar,
Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-
berg, et al. 2023. Sparks of artificial general intelli-
gence: Early experiments with gpt-4. arXiv preprint
arXiv:2303.12712 .
Shouyuan Chen, Sherman Wong, Liangjian Chen, and
Yuandong Tian. 2023. Extending context window of
large language models via positional interpolation.
Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena
Shah, Iana Borova, Dylan Langdon, Reema Moussa,
Matt Beane, Ting-Hao Huang, Bryan Routledge, et al.2021. Finqa: A dataset of numerical reasoning over
financial data. arXiv preprint arXiv:2109.00122 .
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.
2023. Vicuna: An open-source chatbot impressing
gpt-4 with 90%* chatgpt quality. See https://vicuna.
lmsys. org (accessed 14 April 2023) .
FiQA. Financial question answering. https://
sites.google.com/view/fiqa .
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. 2021. Lora: Low-rank adaptation of
large language models.
Allen H Huang, Hui Wang, and Yi Yang. 2023. Finbert:
A large language model for extracting information
from financial text. Contemporary Accounting Re-
search , 40(2):806–841.
LAION.ai. Oig: the open instruction generalist dataset.
https://laion.ai/blog/oig-dataset/ .
Shayne Longpre, Le Hou, Tu Vu, Albert Webson,
Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V
Le, Barret Zoph, Jason Wei, et al. 2023. The flan
collection: Designing data and methods for effective
instruction tuning. arXiv preprint arXiv:2301.13688 .
Pekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wal-
lenius, and Pyry Takala. 2014. Good debt or bad
debt: Detecting semantic orientations in economic
texts. Journal of the Association for Information
Science and Technology , 65(4):782–796.
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and
Hannaneh Hajishirzi. 2021. Cross-task generaliza-
tion via natural language crowdsourcing instructions.
arXiv preprint arXiv:2104.08773 .
Rajdeep Mukherjee, Abhinav Bohra, Akash Banerjee,
Soumya Sharma, Manjunath Hegde, Afreen Shaikh,
Shivani Shrivastava, Koustuv Dasgupta, Niloy Gan-
guly, Saptarshi Ghosh, et al. 2022. Ectsum: A new
benchmark dataset for bullet point summarization
of long earnings call transcripts. arXiv preprint
arXiv:2210.12467 .
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in Neural
Information Processing Systems , 35:27730–27744.
Victor Sanh, Albert Webson, Colin Raffel, Stephen H
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chaffin, Arnaud Stiegler, Teven Le Scao, Arun
Raja, et al. 2021. Multitask prompted training en-
ables zero-shot task generalization. arXiv preprint
arXiv:2110.08207 .

--- PAGE 9 ---
Agam Shah, Suvan Paturi, and Sudheer Chava. 2023.
Trillion dollar words: A new financial dataset, task &
market analysis. arXiv preprint arXiv:2305.07972 .
Raj Sanjay Shah, Kunal Chawla, Dheeraj Eidnani,
Agam Shah, Wendi Du, Sudheer Chava, Natraj Ra-
man, Charese Smiley, Jiaao Chen, and Diyi Yang.
2022. When flue meets flang: Benchmarks and
large pre-trained language model for financial do-
main. arXiv preprint arXiv:2211.00083 .
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. 2023. Stan-
ford alpaca: An instruction-following llama
model. https://github.com/tatsu-lab/
stanford_alpaca .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models.
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-
isa Liu, Noah A Smith, Daniel Khashabi, and Han-
naneh Hajishirzi. 2022. Self-instruct: Aligning lan-
guage model with self generated instructions. arXiv
preprint arXiv:2212.10560 .
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M Dai, and Quoc V Le. 2021. Finetuned lan-
guage models are zero-shot learners. arXiv preprint
arXiv:2109.01652 .
Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski,
Mark Dredze, Sebastian Gehrmann, Prabhanjan Kam-
badur, David Rosenberg, and Gideon Mann. 2023.
Bloomberggpt: A large language model for finance.
arXiv preprint arXiv:2303.17564 .
Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao
Lai, Min Peng, Alejandro Lopez-Lira, and Jimin
Huang. 2023. Pixiu: A large language model, in-
struction data and evaluation benchmark for finance.
arXiv preprint arXiv:2306.05443 .
Hongyang Yang, Xiao-Yang Liu, and Christina Dan
Wang. 2023. Fingpt: Open-source financial large
language models. arXiv preprint arXiv:2306.06031 .
Yi Yang, Mark Christopher Siy Uy, and Allen Huang.
2020. Finbert: A pretrained language model
for financial communications. arXiv preprint
arXiv:2006.08097 .
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao
Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,
Lili Yu, et al. 2023. Lima: Less is more for alignment.
arXiv preprint arXiv:2305.11206 .A Appendix A: Training Format
The training format draws inspiration from the Stan-
ford Alpaca model (Taori et al., 2023). Given the
instructions and input, predict the output.
"prompt with input": "Below is an instruction
that describes a task, paired with further context.
Write a response that appropriately completes the
request. Instruction:{instruction} Input:{input} Re-
sponse:"
"prompt without input": "Below is an instruction
that describes a task, paired with further context.
Write a response that appropriately completes the
request. Instruction:{instruction} Response:"
B Appendix B: Expert Evaluation
Interface and GPT-4 Evaluation
Prompt
Figure 4 presents the interface we used to collect ex-
pert preference judgments. Experts were given sce-
nario simulations and moderate guidance to iden-
tify the appropriate response. Figure 3 shows the
prompt of using GPT-4 to evaluate. Due to GPT-4
preferring not to make financial recommendations,
the prompt was adjusted to ignore the risk of pro-
viding investment advice.

--- PAGE 10 ---
Please read the following news article and the question, and compare two responses generated by the AI assistant. You need to
choose which response is better.
A better response does not need to explain a lot of additional information, and directly expresses the point of view which can
be valuable for readers seeking quick, straightforward advice.
Do not consider the risk associated with providing specific buy or sell suggestions.
News article : {Input}
Question: {Question}
Response A : {Response A} Response B : {Response B}
Please choose an option from the following:
Response A is significantly better.
Response B is significantly better.
Neither is significantly better.
Figure 3: GPT-4 Evaluation Prompt
Please read the following news article and the question, and compare two responses generated by the AI assistant. You need to
choose which response is better.
Note that a better response is more accurate, concise and logical, and can be helpful for your investment decision making.
News article : {Input}
Question: {Question}
Response A : {Response A} Response B : {Response B}
Decide which answer offers better financial investment advice.
Response A is significantly better.
Response B is significantly better.
Neither is significantly better.
Figure 4: Expert Evaluation Interface
