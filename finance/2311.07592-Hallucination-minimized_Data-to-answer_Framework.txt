# 2311.07592.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/finance/2311.07592.pdf
# File size: 1188119 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Hallucination-minimized Data-to-answer Framework
for Financial Decision-makers
Sohini Roychowdhury∗, Andres Alvarez∗, Brian Moore∗, Marko Krema∗, Maria Paz Gelpi†,
Federico Mart ´ın Rodr ´ıguez†,´Angel Rodr ´ıguez‡, Jos ´e Ram ´on Cabrejas‡, Pablo Mart ´ınez Serrano‡
Punit Agrawal,§Arijit Mukherjee§
∗Corporate Data and Analytics Office (CDAO), Accenture LLP, USA
†CDAO, Accenture, Argentina
‡CDAO, Accenture, Spain
§CDAO, Accenture, India,
Email: sohini.roychowdhury@accenture.com
Abstract —Large Language Models (LLMs) have been applied
to build several automation and personalized question-answering
prototypes so far. However, scaling such prototypes to robust
products with minimized hallucinations orfake responses still
remains an open challenge, especially in niche data-table heavy
domains such as financial decision making. In this work, we
present a novel Langchain-based framework that transforms
data tables into hierarchical textual “data chunks” to enable
a wide variety of actionable question answering. First, the user-
queries are classified by intention followed by automated retrieval
of the most relevant data chunks to generate customized LLM
prompts per query. Next, the custom prompts and their responses
undergo multi-metric scoring to assess for hallucinations and
response confidence. The proposed system is optimized with user-
query intention classification, advanced prompting, data scaling
capabilities and it achieves over 90% confidence scores for a
variety of user-queries responses ranging from {What, Where,
Why, How, predict, trend, anomalies, exceptions }that are crucial
for financial decision making applications. The proposed data to
answers framework can be extended to other analytical domains
such as sales and payroll to ensure optimal hallucination control
guardrails.
Index Terms —user-query intention, classification, hallucina-
tions, large language models, benchmarking
I. I NTRODUCTION
The Generative AI (or Gen-AI) domain powered by Large
Language Models (LLMs) is one of the major digital industry
disruptors of the past decade following the era of digital social
networking and web-based content streaming [1]. Although the
name “Generative AI” comes from the family of LLMs called
“Generative Pre-trained Transformer” (GPT) models that were
developed for natural language generation (NLG) tasks [2], the
current state of Gen-AI has evolved for image generation [3],
video generation [4], and multi-modal data question-answering
systems [5]. While LLMs have significantly reduced the entry
barrier to AI due to the ease of use and simple language-based
instructions [6], the path from prototyping solutions powered
by LLMs to building stable products has several roadblocks
with regards to reliability, repeatability and data availability
[7]. The major challenge identified around the trustworthiness
of LLM responses is called hallucinations , wherein, the LLM
presents false facts and data as the real answer [8]. Thisphenomena has been found particularly difficult to control
and specifically detrimental in analytical domains that involve
decision making tasks based on the LLM responses [9]. In
this work, we present a novel framework, infrastructure and
modules that have been developed to enhance the reliability of
LLM responses to assist the decision-makers of the financial
domain.
Hallucinations predominantly occur due to the biases in the
LLM training data. For instance, the early versions of the GPT-
3 model that was trained on 48 TB of text data [10], did not
comprehend numerical or mathematical analytics and made
frequent errors in simple tasks such as finding the maximum
number . Since then, the current set of LLMs available over
major cloud providers have been fine-tuned and modified to
better understand numerical manipulators. Some examples of
hallucinations from the GPT-3 (davinci-003 legacy version)
LLM available over Azure cloud are shown in Fig. 1. Here,
we observe that even though the customizable context provided
to the LLM has various named entities (such as USA, Europe
etc.), LLMs can falsely switch named entities, prioritize facts
with significantly low or high percentage values, or present
an answer based on its prior training data as shown in
the three question answering instances. This work aims to
minimize such hallucinations at instruction level by building a
modular framework that controls and scores for hallucinations
at multiple levels, thereby allowing the users, or financial
decision makers, to build trust around LLM responses.
Since most LLMs are trained on text-data, building contex-
tual querying systems for large tabular data poses three major
challenges. First, the data from tables need to be translated to
language (as data chunks ) to then be queried and understood
by the LLMs. Second, mathematical manipulations need to
be performed in back-end analytical modules to generate data
and text around trends and predictions that might be required
for financial decision making purposes. It is noteworthy that
LLMs have an inbuilt security layer that prohibits making pre-
dictions by default [11], which necessitates for any predictive
trends or anomalies to be separately provided to the LLM
prompts for domain specific question-answering. Third, the
sensibility and reliability of responses involving mathematicalarXiv:2311.07592v1  [cs.CL]  9 Nov 2023

--- PAGE 2 ---
Fig. 1. Examples of hallucinations by the LLMs. In this example, text chunks regarding USA and European economic growth is collected as context (left)
and passed to the LLMs to answer specific questions as a custom prompt. Three user questions are tested (right) that show hallucinations by the GPT-3
(davinci-003 legacy version) LLM. All hallucinations are shown in red. For the first question, there were only 2 insights in the context, but the LLM made
up three insights falsely. For the second question, the LLM focused on the smallest percentage change and hallucinated the first part of the response. For the
third question, the LLM chooses to answer from past training data it has seen, which is a hallucination.
manipulations need to be assessed for accuracy since most
LLMs are not predominantly trained on mathematical and nu-
merical analytics. In this work, we present a novel Langchain-
based [12] framework with custom modules that control for
these challenges towards a hallucination-minimized end-to-end
solution.
This paper makes three major contributions in the form of
the following hallucination-control modules.
1) Table to text generation module to pass textual data to
LLMs. Large data tables are converted to sentences and
stored as data chunks that are hierarchically categorized
to support aggregated querying. Data chunks represent
primary, secondary and trend components. Primary data
chunks store the data table values as template-text while
the secondary data chunks contain sentences with feature
level information, i.e. which feature is the maximum
etc. The trend data chunks have sentences containing
predictions, anomalies and correlation information for
each metric in the data tables.
2) Data chunk ranking and custom prompt generation.
These components filter the most relevant data con-
tent per user-query based on the retrieval augmenta-
tion generation (RAG) [13] mechanism and generate
a customized prompt per user-query to be sent to the
LLM. Filtering for data chunks followed by embedded
similarity search provides a scaled framework where
several hundred thousand data chunks can be mined and
the most relevant data can be retrieved per user-query.
3) Live Quality Scoring module. This novel component
analyzes each custom prompt the returned LLM re-
sponse and evaluates the response for question context,numeric hallucinations, uniqueness from prompt, and
response sensibility in terms of 6 binary quality scores.
These quality scoring metrics further categorize each
LLM response into {Low/Medium/High }confidence.
The proposed framework achieves consistent response
confidence scores of about 90% and higher with iterative
prompting advancements and versioned deployments.
The overall system diagram is shown in Fig. 2. The proposed
framework comprises of two separate processes, namely the
offline and online processes. The offline process involves
converting the table to text generation modules, while the live
process comprises of the custom prompt generation followed
by LLM response scoring per user-query.
II. P RIOR WORK ON LANGUAGE MODELS
Several language-models have been developed and analyzed
over the past decades to enable natural language understanding
(NLU) and natural language generation (NLG) tasks [14].
All existing works can be broadly classified as encoder-only
models that enable machine level understanding of human
language, encoder-decoder models that enable NLU and NLG
combined to process human language and generate desired
outcomes such as summaries or paraphrased verbiage etc. and
decoder-only models such as the LLMs. Some of the language
models landmarks in each category are shown in Table I.
The current generation of decoder-only LLMs with billions
to trillions of parameters that are capable of maintaining
conversations while presenting contextual facts such as the
chatGPT [27] had some distinct developmental landmarks
[28]. The GPT-1 model introduced in June 2018 [23] was
the first generation of the Generative Pre-trained Transformer

--- PAGE 3 ---
Fig. 2. The proposed system architecture for the Data-to-answers framework. The offline process is triggered by data refresh rates while the online process
is triggered by each user-query.
TABLE I
LANDMARK LANGUAGE MODELS AND THEIR CAPABILITIES .
Model Type Example capabilities Model names (parameters, year)
Encoder-only Missing word detection BERT [15] (110-340M, 2018), RoBERTA [16] (125M-355M, 2019)
Typing error correction ALBERT [17] (11M-223M, 2020), ERNIE [18] (114M, 2020)
Encoder-decoder Summary generation, paraphrasing BART [19] (140M, 2019), T5 [20] (80M-11B, 2019)
Limited answering GLM [21] (110M-10B, 2021), Flan-UL2 [22] (20B, 2023)
Decoder-only Language generation GPT-1 [23] (110M, 2018), XLNet[24] (110M-340M, 2019)
Conversational answering GLam [25](1.2T, 2021), LaMDA [26] (137B, 2022), Bard (137B, 2023)
models and it consisted of 117 million parameters, thereby
setting up the foundational architecture for the present-day
ChatGPT[27]. The GPT-1 model demonstrated language un-
derstanding and text generation capabilities by using books
as training data to predict the next word in a sentence. The
GPT-2 model [29] that was developed in February 2019,
represented a significant upgrade with 1.5 billion parameters
and significant improvements in text generation capabilities by
producing coherent and multi-paragraph text. However, con-
cerns regarding unstable text generation and potential misuse
prevented GPT-2 from being publicly released in early 2019.
The GPT-2 model was eventually released in November 2019
after OpenAI conducted a staged rollout to study and mitigate
the potential risks. The next versioned deployment of GPT-
3 [30] was a huge improvement with 176 billion paremeters,
that was launched in November 2022 by OpenAI with safety
guardrails against answering questions with malefic intentions
[30]. The GPT-3 model had far superior text-generation capa-
bilities when compared to all other existing models at the time
which led to its widespread usage in a variety of automation
and question-answering applications such as drafting emails,
writing articles to creating poetry and even correcting human-
written programming code [28]. GPT-3 also demonstrated
an ability to answer factual questions and translate between
languages. Additionally, the OpenAI platform provided usersthe opportunity to interact with GPT-3 model through a chat-
like environment (chatGPT) for free to enhance the question-
answering capabilities of the model. Enhancements to the chat-
GPT platform with GPT3.5 and GPT-4 [27] further enabled
automation prototypes and solutions to be developed with the
LLMs. Some of the well-known foundational LLMs that are
currently being applied for building automation and question-
answering products are presented in Table II.
Apart from the LLMs in Table II, some other notable LLMs
include Galactica that was launched by Meta in 2022 and
trained on 48 million academic journals and articles [38]. Its
primary flaw was hallucinations that were difficult to detect
since the language in the research articles it trained from were
authoritative in nature. While most LLMs are extremely large
to offer inferencing on a single laptop or system, there are
some lighter versions of LLMs that run on single systems
such as Orca by Microsoft [39] that contains 13B parameters
and is built on top of Llama. It is noteworthy that the lighter
LLM versions (with few million parameters) are characterized
with limited memory and reasoning capabilities. In this work,
we build an LLM-agnostic framework wherein any of the
models from Table II can be applied to generate reliable and
trustworthy responses on large tabular data sets.
III. M ATERIALS AND METHODS
Analytical domains such as finance, involve large volumes
of tabular data that need to be accessed and returned to the
user while following specific business rules. Thus, the source
of our Finance-chatbot is data tables that can be pre-loaded
through BigQuery. As first step in the offline process, several
language data chunks are generated per-tabular record entry
while maintaining the pre-defined data hierarchies such as ge-

--- PAGE 4 ---
TABLE II
LIST OF MAJOR LLM S SUITABLE FOR DATA TO ANSWERS USE -CASES AND THEIR CAPABILITIES
Model Name Year Core Capabilities
GPT3.5 [30] Jan 2023 - GPT 3.5 is verbose and suitable for conversational chatbots with follow up questions.
- It has variants with {1.3, 6 and 175 }billion parameters but limited language translation and code interpretation capabilities.
- GPT-3.5 is the faster and lighter version of GPT-3 with fewer parameters (turbo) and faster response times.
GPT4 [27] March 2023 - GPT-4 is one of the largest LLMs capable of code generation/correction, image, video and audio processing and translations.
- GPT-4 is fine tuned for multiple language translation and problem solving tasks and is the most expensive LLM per token level.
- GPT-4 is estimated to have 1.7 trillion parameters. It is fast, conversational, has least contextual limitations.
Llama [31] Feb 2023 - This open-source LLM was designed by Meta AI with 7 billion to 65 billion parameters.
- Its major advantage is efficient language translation, summarizing and processing shorter contexts.
- LLaMA is the foundation for a variety of open-source AI models, including Dolly, Alpaca and Gorilla.
Palm (Text/chat Bison) [32] May 2023 -This LLM by Google contains 540 billion parameters and is stable and reliable for mathematical analytics.
-It has text and chat versions that enable conversational modalities with significantly high volumes of text processing capabilities.
Llama 2 [33] July 2023 - Llama 2 by Meta is an enhancement over LLama with enhanced safety settings against generating harmful content.
- It has 7 billion to 70 billion parameters with the ability to generate non-toxic text without additional prompting.
Anthropic (Calude) [34] March 2023 - This LLM by Anthropic is trained as a conversational assistant with high ethical standards.
- Claude has 175 billion parameters and supports light browser-compatible versions along with AWS-supported virtual LLM.
Claude 2 [35] July 2023 - This LLM can generate any type of written text, summarize existing texts, and perform question-answering.
- This 130 billion parameter model has large input capacity, that can summarize hundreds of pages of documents in minutes.
Cohere [36] June 2023 - This LLM by Cohere that offers access through api calls for tasks such as summarizing, classification, and finding similarities.
- It is developed majorly for text data processing and contains 52 billion parameters.
BloombergGPT [37] March 2023 - This LLM contains 50 billion parameters and is trained on both domain-specific and general-purpose datasets.
- It outperforms existing LLMs on finance-specific tasks without downgraded performance on general LLM benchmarks.
ographical segregation at state and country levels. These data
chunks represent three major types of information. The first
primary-data chunks represent simple readouts of the tabular
data. The second feature-level data chunks represent feature
level manipulations such as minimum and maximum values
per metric. The third trend-data chunks represent anomalies,
exceptions and predictions returned by analytical modules that
can be run offline on the data. These data chunks eliminate
the burden of mathematical manipulations and the possibilities
of hallucinations from LLMs. It is noteworthy that each data
chunk contains 2-10 sentences of text based on the business
rules. This optimal size of data chunks ensures adequate
context generation for a variety of use queries. All the data
chunks are subjected to vectorized embedding using Ada002
[40] or BERT embeddings [15]. The novel framework in this
work aims to minimize the solution scaling challenges that
appear when the number of data chunks grow over a million
records in size.
The next key component for the Finance-chatbot offline pro-
cess in Fig. 2 is prompt templates that contain instructions cor-
responding to each user question category (or intent). The cate-
gories of questions include: “What”, “Why”, “How”, “Which”,
“Compare”, “Summarize”, “What-if”, “trend/anomaly/outlier”
etc. In the live process of the chatbot, for every user-query, an
intent classifier classifies the embedded version of the user-
query to one of the existing intention categories (one vs. all
classification). Based on this classified intention category , the
respective prompt template is populated with relevant data
chunks followed by scoring the response returned by the
LLM for factual and contextual accuracy. The notations and
descriptions of modules in the offline and live processes to
generate a customized prompt per user-query are described in
the following subsections.
A. Notation
Thei-th user-query ( Qi) is a combination of question
intention category ( Ii) and named entities ( QNi) such thatthe named entities can be categorized as financial metric
(QMk), geo-location ( QG′
k) and time-period ( QT′′
k) as shown
in (1-2). The complete set of text data chunks ( C) contain
primary, feature-level and trend data chunks such that the most
relevant data chunks filtered and matched to each user-query
are represented by ciandcopt,i, respectively. The customized
prompt ( Pi) that is generated per user-query contains prompt
template corresponding to intention category ( Ψi), related
custom definitions ( di), and optimal data chunks ( copt,i) in (3).
Finally, each LLM response ( Ri) is evaluated for the financial
metric ( RMk), geo-location ( RG′
k) and time-period ( RT′′
k) in
its content to return qualitative scores S={si,1....si,6}in (4)
for the user to gauge the level of confidence in each response.
Qi=Ii∪QNi,(1)
QNi=∪k,k′,k′′{QMk, QG′
k, QT′′
k},(2)
Pi=∪{Ψi, di, copt,i},(3)
S=Score (Ri,{QMk, QG′
k, QT′′
k},{RMk, RG′
k, RT′′
k}).(4)
B. Data Chunk Ranking Module
The objective of this module is to prevent the LLM from
being overwhelmed by massive amounts of data, and thereby
running into token limitation errors. This module has three
steps to correct for typing errors, then to filter the data chunks
by keywords based on query Qifollowed by selecting the
data chunks that are most similar to the query. These steps
are motivated by the RAG methodology for LLMs in [13] and
described below.
1) In the first step, a Levenstein-distance based spelling
checker is applied to the user-query to counteract manual
typing errors. Next, a sophisticated keyword dictionary
is used to filter data chunks that contain the financial
metric, geo-location and time period in question (as
ci). This keyword dictionary contains details regarding
financial metrics, geo-locations and time period inter-
relationships and hierarchies.

--- PAGE 5 ---
2) In the second step, multiple searches are initiated to
identify data chunks with the named entities ( QNi) while
ensuring relational and hierarchical data integrity.
3) In the third step, the embedded version of the question
(Ada(Qi)) is combined with the filtered data chunks
to identify the filtered data chunks with highest cosine
similarity [28]. Upto 20 top matching data chunks are
returned to customize the prompt per user-query. This
chunk limit of 20 is empirically set based on the token
limitation of 4096 for the Azure GPT-3 model and can
be increased as the LLM token limitations evolve. It
is noteworthy that the business logic/rules in the first
step are fine-tuned to ensure atleast one data chunk is
matched to each query. In the absence of enough data
chunks, the LLMs tend to hallucinate and respond from
their pre-trained data as shown in instance 3 of Fig. 1.
C. Custom Prompt Generation
This module converts a simple user-query to a customized
query with all relevant definitions and data to answer the ques-
tion with minimum hallucinations. For our Finance-chatbot,
the individual prompt templates corresponding to each ques-
tion intention are composed of the following components: an
introduction, a preamble, matching context, instructions and a
few example questions and their answers. The details of each
prompt component is explained as follows.
1) Prompt Introduction: This section sets the persona for
the LLM responses and sets up basic instructions for the
LLM to understand key-definitions and the steps the LLM
must follow to answer the question. This section explains
the information flow in the prompt to the LLM and needs
significant modifications while switching between LLM types.
2) Prompt Preamble: This section guides the LLM through
domain-specific terminologies that are unique to the use-case.
While there may be several custom definitions ( D), only the
definitions relevant to the named entities in the user-query
are used each time ( di). For niche domains such as Finance,
there are often a wide variety of acronyms such as ‘PPP’ that
may stand for ‘Profit Per Period’. This definition is critical
to extracting the right context from the data chunks. Thus,
this section introduces all necessary definitions and data inter-
relationships and hierarchies that are necessary to extract the
necessary context from data chunks.
As an example, we generate a customized prompt for a user-
query: ‘Where in Europe is the highest GDP growth in FY23?’
The process begins by gathering the most related data chunks
based on Section III-B. Next, the intention and named entities
are identified as ‘GDP’ and ‘Europe’ from the question. The
preamble includes relational details that mentions countries
like ‘Germany,’ ‘France,’ and ‘UK’ belong to ‘Europe’ and
hence data chunks from these countries are relevant . Thus,
the country names/keywords are intelligently mapped to the
original named entities in the user-query through the preamble
corpus. While the preamble component structurally resides
next to the introduction section in the prompt, it is the last
component to be executed by the custom prompt generationpipeline. This is due to its unique requirement of having both
the user-query and contextual data in place prior to making the
inter-connections dictated by the business logic specific to the
use-case. Thus, the preamble section maximizes the prompting
capabilities in domains that are heavy on custom definitions
and jargon.
3) Prompt Context: This component combines all the
matching data chunks and creates an overall context for the
LLM to access while building a response. It is noteworthy that
providing more data chunks to the LLM is less detrimental
than providing less data chunks, since the LLMs typically
have the capability to discern between sentences with specific
named entities. For instance if a user-query is: “Which coun-
tries in Asia have below average GDP growth in FY23?”, then
several data chunks with Asian country names and their GDP
relative to the average Asian GDP are returned and the LLM
selects the sentences that have lesser than average Asian GDP
and paraphrases the response. In this scenario, passing few
data chunks can lead to hallucinations due to LLM training
biases.
4) Prompt Instructions: This component conveys step-wise
directions on how the LLM should formulate its response,
while tailoring to the unique question intention category. These
steps are based on the open-source Langchian framework in
[12]. For instance, fetching the answer to a simple user-query
such as: “What is the growth in USA in Q3 FY23?” would
require fewer steps than the user-query: “Why has the growth
in USA slowed down last quarter?” or the query “How can
the growth in USA be improved in Q1 FY24?”. For the
simple “What/where” queries, the goal is to find the most
relevant sentences and to paraphrase them, whereas for the
more complex “Why/How?” questions, instructions dictated by
the business logic and data relationships need to be laid out for
the LLM to extract the most relevant context and to summarize
and paraphrase them. It is noteworthy that each intent-specific
prompt templates contain sequential, algorithmic instructions
along with instructions to handle limiting conditions (when too
many or too few data chunks are returned) to return coherent
and contextually relevant responses.
5) Prompt Example and Question: This final component
represents the precise format of the desired response. For
instance, GPT models (GPT3/3.5) tend to generate responses
in a paragraph format. Depending on the user interface and
the volume of information comprehension, a paragraph format
may not always be the desired response. Thus, for Finance-
chatbots it is a good practise to specify a bullet-point for-
mat for responses. Additionally, the sample question and its
response can be used as a guidance mechanism to include
relevant sources and links in the response as well. One major
caution while generating this segment is that a sample example
need not be a precise example and its directed answer, but it
can be a sequence of strategic questions and answers that aid
interpretation of abstract user-queries while answering a board
spectrum of questions in a prescribed format.
The overall process of customized prompt generation ( Pi)
per user-query Qican be summarized in Algorithm 1.

--- PAGE 6 ---
Algorithm 1: Custom Prompt Generation
Output: Customized Prompt Pi
Input: Query Qi=Ii∪QNi
Initialization;
Ii←classify-intent( Qi),Ii∈[0,1,2, ...l]
Ψi←Prompt-template( Ii)
EQi←Ada-embedding( Qi)
size(copt,i)=0
while size(copt,i)<20do
ci←Filter-data-chunks( C, Q Ni)
copt,i←copt,i∪arg max( EQi, Ada-embedding( ci))
end
di←Filter( D, Q Ni)
Pi=∪{Ψi, di, copt,i}
D. Scoring Engine
All previous modules described above are aimed at min-
imizing the LLM hallucinations prior to receiving a LLM
response. However, based on the abstractness of the user-query
and limitations in data chunks, hallucinations may still occur
and they must be detected and reported to the users along
with the LLM responses. To measure the quality of each LLM
response, we propose a light-weight novel scoring module
using ‘nltk-libraries’ wherein the content of each response is
scored using 6 binary metrics described below.
1) Question-answer continuity metric ( si,1): This binary
metric analyzes the named entities in both the question
(QNi) and response ( RNi) texts. Based on this metric,
we can infer if the response addresses the user-query
or if the LLM has hallucinated and gone off track.
For example, if the user-query is about “quarterly costs
figures”, and the LLM responds with details regarding
“annual figures”, then this metric is 0 as shown in (5).
2) Numeric hallucination metric ( si,2): When the model
returns a response, it is essential to validate that the
numbers in the response exist in the context section of
the customized prompt. Otherwise, it is highly likely
that the facts and figures are fabricated hallucinations.
For example, if the LLM response includes: “The metric
ZZ has increased by X.X%”, then the value X.X% must
be included in the prompt context. This binary metric is
set to 1 if all the numbers in the response are a subset
of the numbers in the prompt context as shown in (6).
3) Uniqueness from prompt metric ( si,3): This binary met-
ric identifies if a consecutive sequence of δwords is the
same within the response and the prompt. This metric
evaluates the paraphrasing capability of the LLM to
answer abstract and complex user-queries. This metric is
set to 1 if there are no sequence of 10-12 words in a row
in the response matching with the customized prompt as
shown in (7).
4) Numeric sensibility metric ( si,4): This binary metric
checks for grammatical flaws in sentences involving
numerical analysis. For instance, if the α-th responsesentence ( Ri,α) contains the word “increase/increasing”,
this sentence should not be accompanied by a negative
number or percentage. Similar number and textual trends
are analyzed for each response sentence. This metric is
set to 1 only if all response sentences are grammatically
sensible as in (8).
5) Context warning metric ( si,5): This warning metric
detects if the context is easily accessible or not. In
situations where multiple data chunks of different sizes
and varying financial metrics are combined to generate a
composite context, this metric is set to 1. This metric is
useful in situations where users don’t approve of a LLM
response, wherein this metric can indicate erroneous data
chunks being pulled into the customized prompt.
6) Contextual continuity metric ( si,6): This metric analyzes
each response sentence ( Ri,α) and verifies if the context
(or named entities) match those in the data chunks of
the prompt ( copt,i(Ni,α)). For instance, if the LLM
response contains, “profits have increased by Y .Y%
in market 1”, but the prompt data chunks contain the
sentence “In quarter TT, the profit has increased by
Y .Y% in market 2”, then this metric is set to 0 as shown
in (9).
si,1=1ifQNi==RNi
0otherwise
,(5)
si,2=1ifnumbers (Ri)⊂numbers (copt,i)
0otherwise
,
(6)
si,3=
1seqδ(Pi)∩seqδ(Ri) == ϕ
0otherwise
,
(7)
si,4=

1 “positive ”∈Ri,α, numbers (Ri,α)>0
1 “negative ”∈Ri,α, numbers (Ri,α)<0
0otherwise

,
(8)
si,6=1RNi,α==copt,i(Ni,α)
0otherwise
. (9)
Finally, based on the response scores, a confidence is asso-
ciated to each LLM response and returned to the user as
{High, Medium, Low }as shown in (10). It is noteworthy
that the response confidence scores assist the user by telling
them that the LLM either understood or completely failed
to comprehend the user-query. A low confidence response
is typically associated with open ended or abstract user-
queries, inadequate data chunks, incorrect question intention
classification and hallucinations. The confidence score tells
the user to assert caution while making key decisions using

--- PAGE 7 ---
medium to low confidence responses. The confidence score
further helps ascertain which user-queries need to be further
refined for reliability.
Confidence (Ri) =

HighP
jsi,j>= 5
Medium 4>=P
jsi,j>= 3
Low otherwise


(10)
E. System Architecture for Finance-chatbot
The major differences between an LLM prototype and a
product include the following: scalability in infrastructure with
growing data chunks, robustness to variations in user-queries,
data freshness, security concerns and constant enhancements
based on human feedback. Thus, the following considerations
are required to design a Finance-chatbot at an Enterprise-level.
•Data tables must be refreshed based on Enterprise-level
guidelines to ensure reliable LLM responses.
•Security and access constraints at user level needs to be
implemented at data chunk storage level.
•Fewer LLM calls per user-query may prevent halluci-
nations while ensuring fast response times. However,
multiple LLM calls per user-query to classify user-query
intentions, to extract named entities or match the best
data chunks may enhance response performances for
previously unseen and abstract user-queries.
•Custom prompt generation with best matching data
chunks is one way of implementing RAG [13] to extract
customized responses by grounding. However, additional
user-history based information may further enhance the
question answering experience at a personalized level.
•Building chatbots for finance-based decision makers in-
volves reliable and repeatable response generation from
the LLM along with the response scores that guide the
users/decision makers.
•Feedback collection is an integral part of the product
pipeline to ensure that “low” confidence responses are
constantly recorded and improved based on business
logic.
To ensure a scalable and usable product, the system archi-
tecture in Fig. 2 can further be expanded to work as a
conversational agent as shown in Fig. 3. This can be achieved
by including the conversation thread in the input of the
live process service that initiates when each user-query is
entered. Passing the conversational continuation information
to the custom prompt enables answering follow-up questions
using the previously selected data chunks. This creates a
conversational thread that is constantly updated with each
LLM prompt and the returned responses. This design allows
for an external calling application to orchestrate a conversation
with follow up queries.
Another design consideration for a scalable personalized
chatbot is the data Extract, Transformation and Load (ETL)
functions [41] to generate data chunks following each data
refresh. This offline process can be further modified to allow
Fig. 3. System diagram of the scalable Data-to-answers setup with conver-
sational threads.
for almost-real time data chunk generation, by incorporating
an asynchronous publish/subscribe system where source data
changes are published, and the data chunk creation system
subscribes to these changes to generate new updated data
chunks [42]. Some existing alternatives to the data chunk
ranking mechanism are ElasticSearch, Apache Solr or Lucene
[43].
One major consideration for scaling a Finance-chatbot as
a service is horizontal scaling to support concurrent users
by running parallel service instances. Here, we assume that
the user-query intention classification and custom prompt
generation services also scale in parallel. As data grows, the
main bottleneck is the time taken for keyword matching while
ranking the best matching data chunks. From a platform per-
spective, if the keyword matching process is designed around
an SQL database, then indexing must be improved to maintain
fast response times ( <3seconds per question). However, for
extremely large amounts of data (like few hundred thousand to
a million data chunks) database (horizontal) sharding needs to
be considered [44] wherein, access to data chunks is rerouted
based on a metric or dimension. ETL processing times can be
a secondary bottleneck in such situations with growing data
chunks. By updating from a batch ETL to an almost real-time
data chunk generation process as described above the overall
query processing times can further be stabilized.
Finally, we can compare our proposed Finance-chatbot
architecture in Fig 3 with the more common RAG archi-
tecture in [13] to answer questions based on domain-related
unstructured data as well. For instance, if we had to extend the
Finance-chatbot to learn from fiscal meeting call summaries
and minutes of meetings, then the following architectural
scaling are necessary. First, the major difference in such a
scenario is that instead of matching questions to data chunks
using keywords followed by filtering and ranking the data
chunks for a customizable prompt generation, we rely on
semantic similarity (of the Ada002 embeddings [40]) between
questions and text fragments to filter and rank. The same

--- PAGE 8 ---
considerations for scaling as discussed above hold. However,
sharding a vector database may require a separation between
topics for such unstructured datasets, rather than using the
structured tabular dimensions. Thus, the infrastructure scaling
considerations discussed here can be applied to structured and
unstructured datasets.
IV. E XPERIMENTS AND RESULTS
In this work, we perform 4 major experiments to ensure
reliability, repeatability and trustworthy responses from our
Data-to-answers Finance-chatbot. First, we assess the best
practices for custom prompt generation required to minimize
hallucinations while being LLM agnostic. Second, we assess
the GPT-3/GPT3.5 LLM response quality scores across a batch
of over 350,000 curated user-queries. Third, we assess the im-
portance of user-query intention classification using a separate
LLM prompt as opposed to a standard pre-trained classifier.
Finally, we assess the response performance variations across
a variety of LLMs for benchmarking purposes.
A. Prompt Engineering for minimized Hallucinations
The best practices for advanced prompt engineering to con-
trol for corner conditions and to ensure reliable and repeatable
responses are discussed below:
1) Hallucination guardrails: Using specific text in the
prompt introduction and preamble sections that instructs
the LLM to refrain from generating responses when
uncertain, or when there is a lack of contextual data.
2) Modular custom prompts: Creating modular prompts en-
ables adaptability to new user-query intentions, prompt
template re-usability and scalability to abstract user-
queries.
3) Customization as needed: Tailoring prompts to suit the
particular requirements of a task or query is imper-
ative for achieving coherent and reliable responses.
Customization allows for accommodating business logic
and data inter-relationships as needed for specific query
intentions such as to explain “Why?” situations.
4) Detailed instructions: Any custom prompt must be as
detailed with easy to follow steps and guidelines. Step by
step instructions enable debugging and troubleshooting
to minimize hallucinations. However, instructions should
not be too long sentences in which case LLM token
limitations may get triggered.
5) Advanced prompting: Prompt instructions should enable
handling for limiting conditions, for instance when few
data chunks or a short context is available the response
must only answer from the context or return “I cannot
answer the question.” Also, in scenarios when the user-
queries are abstract with multiple interpretations, clear
guidelines must be provided to respond with cautionary
text. For instance if the user-query asks about the
“GDP in north eastern states in USA” while the context
contains aggregated USA data only, the LLM must
be prompted to respond with text such as “Apologies,
but I can only answer with regards to the aggregatedGDP data form USA.” Disambiguation enables higher
reliability on the response even if it is not exactly what
the user wanted.
6) Use of delimiters: Incorporating delimiters within the
prompt, such as ’—text—’ or′<text>′, aids the LLM
to identify the location of specific information such as
definitions and instructions, thereby improving response
accuracy.
7) Avoid contradictory instructions: Ensuring that prompt
instructions are consistent while avoiding contradictions
is necessary in limiting conditions to avoid hallucina-
tions.
8) Prompt evolution: The process of custom prompting is
iterative with a focus on performance over perfection.
As LLMs get fine-tuned based on the usage, they may
support larger context in the near future. However, there
will always be a need to fine-tune and evolve the
LLMs to enable complex decision making tasks such
as suggesting prescriptive trends and scenario planning
capabilities. Evolving the prompts based on human
feedback is a necessary component to enable enhanced
user experiences.
B. LLM Response Quality Scoring
In Fig. 4, we observe the average LLM response scoring
metrics {s1, ...s6}on a list of {i= 1 : 356 ,000}curated
set of user-queries that range across all intention categories.
Since all the response scoring metrics are binary in nature, a
Fig. 4. The average LLM response scores across a range of user-queries.
higher frequency of 1 values reflect good response statistics per
user-query and 0 values reflect possible hallucinations. Here,
we observe that the average scores for metrics {s2, s3, s4}
are the highest. This implies that the Azure GPT-3 (davinci-
003 legacy) LLM hallucinates fake numbers less than 10%
of the time, as detected by si,2in (6). Also metrics {s1, s6}
focus on named entity searches and here we observe over 20%
rate of ambiguities in the response contexts. Also, we observe
that the warning metric s5has the highest false rate of over

--- PAGE 9 ---
30%, indicating that the data chunks selected in the context
contain a variety of finance metrics that are different from the
finance metric in the user-query. While a high false rate for
metric s5does not necessarily imply hallucinations, this metric
serves as a diagnostic tool to improve question responses
based on human feedback since it is indicative of issues with
the data chunk filtering and selection modules. These average
response scores for an offline set of user-queries enable quality
assurance for versioned deployments of the Finance-chatbot.
Finally, we assess the overall confidence scores generated
for the same batch of 356,000 questions by assessing the
combined impact of the quality scoring metrics. The response
scores per user-query is categorized as {Low/Medium/High }
based on (10) and variations in the average response confi-
dences across weekly deployments over a period of 8 weeks is
shown in Fig. 5. The response confidence scores are indicative
if the LLM understood the user-query and if it had the required
data to furnish a reliable answer. While a “low” confidence
may not necessarily imply a hallucination has occured, it
offers the user an additional reliability criteria to do their
due diligence and arrive at the answer from other sources as
well. The response confidence scores also reflect an evolu-
tion in the LLMs and advanced prompting capabilities over
time. Additionally, the confidence scores aid prioritization for
prompt enhancements to enable iterative evolution in overall
LLM response quality, reliability and trustworthiness. In Fig.
Fig. 5. Variations in average response confidences per weekly deployment
assessed on a curated set of 356,000 user-queries. The week 6 deployment is
indicative of switching LLMs from GPT-3 (davinci-003 legacy) to GPT 3.5.
5, we observe a consistent improvement in overall confidence
scores by iterative advanced prompting. The major change in
response confidence ratings at week 6 time-frame indicates
a change in LLM from GPT3- GPT 3.5. This observation
indicates that while most Langchain frameworks [12] are LLM
agnostic, the prompts need to be re-engineered to a certain
extent for every LLM change. Thus, the overall response
scoring and confidence measurement modules are significantly
important to plan and monitor such back-end LLM changes
that may otherwise impact user-experience significantly.C. End-to-end Data to Answers with Multiple Prompts
The first and major component in our Finance-chatbot
that controls for hallucinations is the intention classification
process for each user-query. While this process can be easily
automated by a supervised learning models such as decision
trees, one vs. all linear classifiers, SVM etc, classical classifiers
are sensitive to keywords used in the query and noisy data
entry caused by typing errors or abstract user-queries can
cause a pre-trained classifier to mis-classify, leading to the
incorrect prompt template being used for customized prompt
generation thereby leading to a low/medium quality response
that the user may dislike. However, LLMs have been used
successfully for simple sentiment classification tasks and they
are robust to typos as well [45]. In this experiment we evaluate
the one vs. all classification performance of a linear kernel
SVM model that is pre-trained on 900,000 user-queries that
are categorized into 9 classes. The SVM classifier has the best
test scores on a 70/30 split for the 900,000 user-queries when
compared to decision trees, XGBoost and linear classifiers.
We compare the SVM classification performance (in terms
of average precision (Pr) and recall (Re)) with that of an
LLM prompt that passes instructions to read the user-query
and to classify it into one of the 9 intention categories. For
test data, we curate a set of 50 open ended user-queries that are
abstract in nature. The intent classification performances for
each category of user-queries using the SVM classifier vs. a
custom LLM prompt are shown in Table III. Here, we observe
that the macro-level Pr/Re for LLM-based intent classification
is 0.94/0.78, which is significantly higher than the macro
level performance statics for the SVM classifier Pre/Re of
0.69/0.71. This observation aligns with the prior work in [46],
wherein GPT-3 is shown to have better text-based classification
performances when compared to standard classifiers. Thus,
using multiple LLM prompts to first categorize intention
of user-queries followed by building a custom prompt with
relevant context, definitions, instructions and examples can
significantly reduce hallucinations. However, it is noteworthy
that multiple LLM prompts per user-query may increase the
response times.
D. Benchmarking for LLM Performances
Designing LLM-based products requires four major consid-
erations that are different from the prototyping stage. These
considerations are response reliability, response time, infras-
tructure security and usage costs. Although our system design
for the Finance-chatbot has been LLM agnostic so far, we need
to constantly weigh the potential risks and advantages offered
by a variety of LLMs to maintain the reliability and accuracy
of responses. In this experiment, we present a comparative
assessment of the pros vs. the cons of LLM changes and our
LLM selection criteria.
For this experiment, we apply a carefully curated set of
300 user-queries corresponding to the 9 intention categories.
We select straightforward and open ended questions for our
assessment. The assessment regarding the LLM cost incurred

--- PAGE 10 ---
TABLE III
CLASSIFICATION PERFORMANCES OF USER -QUERY CLASSIFICATION USING SVM VS.A CUSTOM LLM PROMPT .
Intent Category Question Types Example user-queries SVM Pr/Re Prompt Pr/Re
0 Basic Information, Definitions What is the growth in USA in FY 23? 0.714/0.625 1/0.625
1 Ranking (Highest/Lowest) Which regions in USA have the highest revenue? 0.446/0.7 0.45/1
2 Direction (Increasing/Decreasing) Is the revenue in Canada increasing? 1/0.33 1/0.83
3 General insights and summaries Summarize the key economic insights in USA? 0.8/1 1/0.5
4 Problem-Solving How can the agricultural revenue be improved in USA? 0.75/1 1/1
5 Diagnostics What are the top drivers for revenue deficits in the Midwest in FY23? 1/1 1/1
6 Performance How is the revenue trend in the south for industrial products in FY23? 0.5/1 1/0.67
7 Outliers What are the outliers/exceptions for financial stocks in NYSE in FY23? 1/0.75 1/1
8 Impact How does the revenue deficits for northeast impact the Midwest trends? 0/0 1/0.4
TABLE IV
ANALYSIS OF RESPONSE VARIATIONS BASED ON LLM CHANGES .
LLM name Prompt token Cost ($) Token Limit ¯s6 ¯s2 ¯s3 ¯s4 Pros Cons
GPT3 (legacy) 0.005 4096 0.83 0.93 0.86 0.93 Creative Responses Higher cost, hallucinations
GPT3.5 0.0008 4096 0.89 0.93 0.69 0.93 Lower cost, Conversation memory Higher hallucinations that GPT3
Text-Bison 0.0005 8192 0.92 0.91 0.90 0.95 Lower cost, lower hallucinations No additional context for follow ups.
Chat-Bison 0.0005 4096 0.85 0.72 0.88 0.95 Lower cost, Conversation memory Hallucinations for big contexts only
TextBision 32K 0.0005 32000 0.92 0.91 0.91 0.95 Lower cost, high token limit, lower hallucinations Higher overall cost per query
per token and the average response scores for the 300 user-
queries along with the overall pro and con of each LLM is
shown in Table IV. Here, we observe that the Bison 32K
model has the highest response scores ( >90%) while the
Chat-Bison and GPT 3.5 have the least response scores. This
observation is intuitive since Chat-Bison and GPT3.5 have
been trained specifically to maintain conversations rather than
return summaries for large contextual data. Thus the optimal
LLM selection plan would be between GPT-3 annd Text-Bison
for such Data-to-answers use-cases.
V. C ONCLUSIONS AND DISCUSSION
LLM-powered chatbot solutions for niche domains such as
Finance are faced with major challenges such as reliability
and scalability. In this work, we present a novel LLM agnos-
tic framework that aims to generate responses to a variety
of user-queries from pure data tables while controlling for
hallucinations in the responses. Our journey from prototyping
to product scaling has provided several leanings to maxi-
mize custom prompt generation capabilities while ensuring
minimized hallucinations for financial decision makers. Our
novel framework controls for hallucinations or fake LLM
responses at various levels and it also scores each response to
provide an additional level of confidence to the users regarding
the accuracy of the response. The proposed system setup is
significantly dissimilar from generic question answering chat-
bots since small error rates and hallucinations can have detri-
mental impacts for financial decision makers. The proposed
response scoring module ensures that the users comprehend a
level of acceptance for the LLM-generated responses, thereby
detecting the 10% instances when the response scores are
“low” that may be indicative of potential hallucinations. The
proposed system has been evaluated for versioned deployments
and for variations in LLM providers as well.
The four major conclusions from this work are as fol-
lows. First, we observe that advanced prompt engineering and
modular infrastructure design plays a crucial role in ensuringreliable and accurate responses. Second, the proposed novel
response scoring module enables response quality monitoring
between data-refreshed deployments. Third, we investigate
the use of multiple LLM prompts per user-query to ensure
higher reliability and lower hallucinations in responses. We
observe that using LLMs have robust user-query intention
classification performances when compared to standard SVM
classifiers for relatively abstract user-queries. Also LLM-based
intention classification is more robust to noisy data and typing
errors, thereby making multi-prompt question-answering sys-
tems more robust than single prompt frameworks. Fourth, we
present our LLM benchmarks and criteria for selection of the
LLMs for domain-specific use cases based on cost-per-query,
reliability and capability to scale.
The novel Data-to-answers framework presented in this
work demonstrates the system and infrastructure level consid-
erations required to take a Finance-chatbot from prototyping
to scaled stages. The proposed framework has the following
advantages over standard Langchain frameworks.
1) Versioned deployments to ensure evolution in advanced
prompting.
2) Ease of use to build reliability and trustworthiness for
the users to enable financial decision making.
3) Infrastructure scalability to support increasing data-
driven capabilities by incorporating data predictions and
trend-related information into data chunks.
4) LLM agnostic framework that can be monitored for
response performances to ensure evolution in response
quality with human feedback.
Future works will be directed towards further enhancing
the hallucination minimization frameworks for unstructured
financial data and documents combined.
ACKNOWLEDGMENT
The authors would like Oscar Rodrigiuz, Kiranjit Kaur, Juan
J. Ricas Corona, Errol Paclibar and Anandam Dhandugari for

--- PAGE 11 ---
their constant experimentation support. Special thanks to Priya
Raman for constant guidance and support.
REFERENCES
[1] L. Olinga, “Chatgpt is the new disruptor-in-chief but there’s a
catch,” 2023. [Online]. Available: https://www.thestreet.com/technology/
chatgpt-is-the-new-disruptor-in-chief-but-theres-a-catch
[2] R. Tang, Y . Lu, and J. Lin, “Natural language generation for effective
knowledge distillation,” in Proceedings of the 2nd Workshop on Deep
Learning Approaches for Low-Resource NLP (DeepLo 2019) , 2019, pp.
202–208.
[3] X. Dai, J. Hou, C.-Y . Ma, S. Tsai, J. Wang, R. Wang, P. Zhang,
S. Vandenhende, X. Wang, A. Dubey et al. , “Emu: Enhancing image
generation models using photogenic needles in a haystack,” arXiv
preprint arXiv:2309.15807 , 2023.
[4] H. Lin, A. Zala, J. Cho, and M. Bansal, “Videodirectorgpt: Consistent
multi-scene video generation via llm-guided planning,” arXiv preprint
arXiv:2309.15091 , 2023.
[5] S. Wu, H. Fei, L. Qu, W. Ji, and T.-S. Chua, “Next-gpt: Any-to-any
multimodal llm,” arXiv preprint arXiv:2309.05519 , 2023.
[6] W. Xie and B. Plancher, “Can large language models reduce the barriers
to entry for high school robotics?” forbes.com , 2023.
[7] P. L. et. al., “Holistic evaluation of language models,” 2023.
[8] J.-Y . Yao, K.-P. Ning, Z.-H. Liu, M.-N. Ning, and L. Yuan, “Llm lies:
Hallucinations are not bugs, but features as adversarial examples,” arXiv
preprint arXiv:2310.01469 , 2023.
[9] F. Leiser, S. Eckhardt, M. Knaeble, A. Maedche, G. Schwabe, and
A. Sunyaev, “From chatgpt to factgpt: A participatory design study to
mitigate the effects of large language model hallucinations on users,” in
Proceedings of Mensch und Computer 2023 , 2023, pp. 81–90.
[10] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al. , “Language mod-
els are few-shot learners,” Advances in neural information processing
systems , vol. 33, pp. 1877–1901, 2020.
[11] A. Chan, “Gpt-3 and instructgpt: Technological dystopianism, utopi-
anism, and “contextual” perspectives in ai ethics and industry,” AI and
Ethics , vol. 3, no. 1, pp. 53–64, 2023.
[12] K. Pandya and M. Holia, “Automating customer service using langchain:
Building custom open-source gpt chatbot for organizations,” 2023.
[13] J. Chen, H. Lin, X. Han, and L. Sun, “Benchmarking large language
models in retrieval-augmented generation,” 2023.
[14] S. Pan, L. Luo, Y . Wang, C. Chen, J. Wang, and X. Wu, “Unifying large
language models and knowledge graphs: A roadmap,” 2023.
[15] M. Golestani, S. Z. Razavi, Z. Borhanifard, F. Tahmasebian, and H. Faili,
“Using bert encoding and sentence-level language model for sentence
ordering,” in International Conference on Text, Speech, and Dialogue .
Springer, 2021, pp. 318–330.
[16] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
L. Zettlemoyer, and V . Stoyanov, “Roberta: A robustly optimized bert
pretraining approach,” arXiv preprint arXiv:1907.11692 , 2019.
[17] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,
“Albert: A lite bert for self-supervised learning of language representa-
tions,” arXiv preprint arXiv:1909.11942 , 2019.
[18] Y . Sun, S. Wang, Y . Li, S. Feng, H. Tian, H. Wu, and H. Wang, “Ernie
2.0: A continual pre-training framework for language understanding,” in
Proceedings of the AAAI conference on artificial intelligence , vol. 34,
no. 05, 2020, pp. 8968–8975.
[19] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,
V . Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to-sequence
pre-training for natural language generation, translation, and comprehen-
sion,” arXiv preprint arXiv:1910.13461 , 2019.
[20] J. Ni, G. H. ´Abrego, N. Constant, J. Ma, K. B. Hall, D. Cer, and Y . Yang,
“Sentence-t5: Scalable sentence encoders from pre-trained text-to-text
models,” arXiv preprint arXiv:2108.08877 , 2021.
[21] Z. Du, Y . Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang, “Glm:
General language model pretraining with autoregressive blank infilling,”
arXiv preprint arXiv:2103.10360 , 2021.
[22] Z. Qin, R. Jagerman, K. Hui, H. Zhuang, J. Wu, J. Shen, T. Liu,
J. Liu, D. Metzler, X. Wang et al. , “Large language models are
effective text rankers with pairwise ranking prompting,” arXiv preprint
arXiv:2306.17563 , 2023.
[23] M. Zong and B. Krishnamachari, “A survey on gpt-3,” arXiv preprint
arXiv:2212.00857 , 2022.[24] F. Shi, S. Kai, J. Zheng, and Y . Zhong, “Xlnet-based prediction model
for cvss metric values,” Applied Sciences , vol. 12, no. 18, p. 8983, 2022.
[25] N. Du, Y . Huang, A. M. Dai, S. Tong, D. Lepikhin, Y . Xu, M. Krikun,
Y . Zhou, A. W. Yu, O. Firat et al. , “Glam: Efficient scaling of
language models with mixture-of-experts,” in International Conference
on Machine Learning . PMLR, 2022, pp. 5547–5569.
[26] M. King, “Administration of the text-based portions of a general iq test
to five different large language models,” TechRxiv , 2023.
[27] OpenAI, “Gpt-4 technical report,” 2023.
[28] H. Naveed, A. U. Khan, S. Qiu, M. Saqib, S. Anwar, M. Usman,
N. Akhtar, N. Barnes, and A. Mian, “A comprehensive overview of
large language models,” 2023.
[29] P. Budzianowski and I. Vuli ´c, “Hello, it’s gpt-2 – how can i help you?
towards the use of pretrained language models for task-oriented dialogue
systems,” 2019.
[30] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-
V oss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler,
J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,
B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,
and D. Amodei, “Language models are few-shot learners,” 2020.
[31] H. T. et al., “Llama: Open and efficient foundation language models,”
2023.
[32] R. A. et. al., “Palm 2 technical report,” 2023.
[33] H. T. et. al., “Llama 2: Open foundation and fine-tuned chat models,”
2023.
[34] Y . Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones,
A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon et al. , “Constitutional
ai: Harmlessness from ai feedback,” arXiv preprint arXiv:2212.08073 ,
2022.
[35] Anthropic, “Model card and evaluations for claude models,” 2023.
[Online]. Available: https://www-files.anthropic.com/production/images/
Model-Card-Claude-2.pdf
[36] Cohere, “Cohere command models,” 2023. [Online]. Available:
https://docs.cohere.com/docs/the-command-model
[37] S. Wu, O. Irsoy, S. Lu, V . Dabravolski, M. Dredze, S. Gehrmann,
P. Kambadur, D. Rosenberg, and G. Mann, “Bloomberggpt: A large
language model for finance,” arXiv preprint arXiv:2303.17564 , 2023.
[38] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Saravia,
A. Poulton, V . Kerkez, and R. Stojnic, “Galactica: A large language
model for science,” 2022.
[39] S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and
A. Awadallah, “Orca: Progressive learning from complex explanation
traces of gpt-4,” 2023.
[40] A. K. Gao, “Vec2vec: A compact neural network approach for
transforming text embeddings with high fidelity,” arXiv preprint
arXiv:2306.12689 , 2023.
[41] E. Y . Gueddoudj and A. Chikh, “Towards a scalable and efficient etl,”
International Journal of Computing and Digital Systems , vol. 14, no. 1,
pp. 10 223–10 231, 2023.
[42] R. Abrahiem, “A new generation of middleware solutions for a near-
real-time data warehousing architecture,” in 2007 IEEE International
Conference on Electro/Information Technology , 2007, pp. 192–197.
[43] N. Boucher, L. Pajola, I. Shumailov, R. Anderson, and M. Conti,
“Boosting big brother: Attacking search engines with encodings,” arXiv
preprint arXiv:2304.14031 , 2023.
[44] F. Hashim, K. Shuaib, and N. Zaki, “Sharding for scalable blockchain
networks,” SN Computer Science , vol. 4, no. 1, p. 2, 2022.
[45] D. Araci, “Finbert: Financial sentiment analysis with pre-trained lan-
guage models,” 2019.
[46] X. Sun, X. Li, J. Li, F. Wu, S. Guo, T. Zhang, and G. Wang, “Text clas-
sification via large language models,” arXiv preprint arXiv:2305.08377 ,
2023.
