# Cải thiện Học đa nhiệm vụ
thông qua Tìm kiếm Vùng Phẳng dựa trên Nhiệm vụ

Hoang Phan∗1Lam Tran∗2Quyen Tran2Ngoc N. Tran3Tuan Truong2
Qi Lei1Nhat Ho4Dinh Phung5Trung Le5

Đại học New York1, VinAI Research2, Đại học Vanderbilt3,
Đại học Texas, Austin4, Đại học Monash, Úc5

26 tháng 5, 2025

## Tóm tắt

Học đa nhiệm vụ (MTL) là một mô hình học tập được sử dụng rộng rãi và mạnh mẽ để huấn luyện các mạng nơ-ron sâu cho phép học hơn một mục tiêu bằng một backbone duy nhất. So với việc huấn luyện các nhiệm vụ riêng biệt, MTL giảm đáng kể chi phí tính toán, cải thiện hiệu quả dữ liệu, và có khả năng nâng cao hiệu suất mô hình bằng cách tận dụng kiến thức qua các nhiệm vụ. Do đó, nó đã được áp dụng trong nhiều ứng dụng khác nhau, từ thị giác máy tính đến xử lý ngôn ngữ tự nhiên và nhận dạng giọng nói. Trong số đó, có một hướng nghiên cứu mới nổi trong MTL tập trung vào việc thao tác gradient nhiệm vụ để tìm ra hướng gradient descent cuối cùng có lợi cho tất cả các nhiệm vụ. Mặc dù đạt được kết quả ấn tượng trên nhiều benchmark, việc áp dụng trực tiếp các phương pháp này mà không sử dụng các kỹ thuật chính quy hóa phù hợp có thể dẫn đến các giải pháp không tối ưu cho các vấn đề thực tế. Đặc biệt, việc huấn luyện chuẩn nhằm tối thiểu hóa loss thực nghiệm trên dữ liệu huấn luyện có thể dễ dàng gặp phải hiện tượng overfitting đối với các nhiệm vụ có ít tài nguyên hoặc bị hỏng bởi các nhiệm vụ có nhãn nhiễu, điều này có thể gây ra sự chuyển giao tiêu cực giữa các nhiệm vụ và giảm hiệu suất tổng thể. Để giảm thiểu những vấn đề này, chúng tôi đề xuất tận dụng một phương pháp huấn luyện được giới thiệu gần đây, có tên là Tối thiểu hóa nhận biết độ sắc nét, có thể nâng cao khả năng tổng quát hóa mô hình trong học đơn nhiệm vụ. Theo đó, chúng tôi trình bày một phương pháp huấn luyện MTL mới, khuyến khích mô hình tìm các điểm cực tiểu phẳng dựa trên nhiệm vụ để cải thiện một cách nhất quán khả năng tổng quát hóa của nó trên tất cả các nhiệm vụ. Cuối cùng, chúng tôi tiến hành các thí nghiệm toàn diện trên nhiều ứng dụng khác nhau để chứng minh ưu điểm của phương pháp đề xuất của chúng tôi so với các phương pháp MTL dựa trên gradient hiện có, như được gợi ý bởi lý thuyết mà chúng tôi phát triển.

## 1 Giới thiệu

Trong vài năm qua, deep learning đã nổi lên như một công cụ mạnh mẽ cho xấp xỉ hàm số bằng cách thể hiện hiệu suất vượt trội và thậm chí vượt qua khả năng của con người trong nhiều ứng dụng. Bất chấp hiệu suất hấp dẫn, việc huấn luyện các mạng nơ-ron độc lập có quy mô lớn để xử lý các nhiệm vụ riêng lẻ không chỉ đòi hỏi tài nguyên tính toán và lưu trữ đắt tiền mà còn cần thời gian chạy dài. Do đó, học đa nhiệm vụ là một phương pháp được ưa chuộng hơn trong nhiều tình huống [1,2,3] vì chúng có thể: (i) tránh tính toán tính năng dư thừa cho mỗi nhiệm vụ thông qua kiến trúc chia sẻ vốn có của chúng; và (ii) giảm số lượng tham số có thể huấn luyện tổng cộng bằng cách chia sẻ tham số cứng [4,5] hoặc chia sẻ tham số mềm [6,7]. Tuy nhiên, các phương pháp tiên tiến hiện có theo hướng học đa nhiệm vụ dựa trên gradient [8,9,10,11,12,13] có xu hướng bỏ qua các tính chất hình học của cảnh quan loss mà chỉ tập trung vào việc tối thiểu hóa lỗi thực nghiệm trong quá trình tối ưu hóa, điều này có thể dễ dàng dẫn đến vấn đề overfitting [14, 15].

Trong khi đó, vấn đề overfitting của các mạng nơ-ron hiện đại thường được quy cho các hàm loss có chiều cao và không lồi, dẫn đến các cảnh quan loss phức tạp chứa nhiều điểm tối ưu cục bộ. Do đó, hiểu biết về bề mặt loss đóng vai trò quan trọng trong việc huấn luyện các mô hình mạnh mẽ, và phát triển các bộ tối thiểu hóa phẳng vẫn là một trong những phương pháp hiệu quả nhất [16,14,17,18]. Để cụ thể hơn, các nghiên cứu gần đây [19,20] cho thấy rằng cảnh quan loss thu được từ việc tối thiểu hóa trực tiếp rủi ro thực nghiệm có thể bao gồm nhiều điểm tối thiểu sắc nét, do đó mang lại khả năng tổng quát hóa kém khi được tiếp xúc với dữ liệu chưa thấy. Hơn nữa, vấn đề này rõ ràng trở nên trầm trọng hơn khi tối ưu hóa nhiều mục tiêu cùng lúc, như trong bối cảnh học đa nhiệm vụ. Chắc chắn, các điểm tối thiểu sắc nét của mỗi mục tiêu thành phần có thể xuất hiện tại các vị trí khác nhau, điều này có thể dẫn đến lỗi tổng quát hóa lớn trên nhiệm vụ liên quan. Để đạt được điều này, việc tìm một vùng phẳng và có giá trị loss thấp chung cho tất cả các nhiệm vụ là mong muốn để cải thiện các phương pháp học đa nhiệm vụ hiện tại.

**Đóng góp.** Để giải quyết thêm desideratum trên, chúng tôi đề xuất một phương pháp huấn luyện MTL mới, kết hợp tối ưu hóa nhận biết độ sắc nét (SAM) [21] được giới thiệu gần đây vào các chiến lược thao tác gradient hiện có trong học đa nhiệm vụ để tăng cường hiệu suất của chúng. Được hướng dẫn bởi lỗi tổng quát hóa trong Định lý 1, phương pháp đề xuất không chỉ định hướng mô hình về giá trị loss thực nghiệm thấp chung trên các nhiệm vụ mà còn khuyến khích mô hình đạt được các vùng phẳng dựa trên nhiệm vụ. Quan trọng là, phương pháp của chúng tôi không phụ thuộc vào mô hình và tương thích với các phương pháp MTL dựa trên gradient hiện tại (xem Hình 1 để có cái nhìn tổng quan về phương pháp của chúng tôi). Bằng cách sử dụng framework đề xuất của chúng tôi, xung đột gradient giữa các nhiệm vụ được giảm thiểu đáng kể, đây là mục tiêu của các nghiên cứu MTL dựa trên gradient gần đây trong việc giảm thiểu sự chuyển giao tiêu cực giữa các nhiệm vụ. Cuối cùng, chúng tôi tiến hành các thí nghiệm toàn diện trên nhiều ứng dụng khác nhau để chứng minh ưu điểm của phương pháp chúng tôi trong việc cải thiện không chỉ hiệu suất nhiệm vụ mà còn độ mạnh mẽ và hiệu chuẩn của mô hình. Cuối cùng nhưng không kém phần quan trọng, theo hiểu biết tốt nhất của chúng tôi, nghiên cứu này là công trình đầu tiên cải thiện học đa nhiệm vụ bằng cách điều tra các tính chất hình học của cảnh quan loss mô hình.

## 2 Công trình liên quan

### 2.1 Học đa nhiệm vụ

Trong học đa nhiệm vụ (MTL), chúng ta thường nhằm mục đích huấn luyện chung một mô hình để giải quyết nhiều nhiệm vụ khác nhau nhưng có tương quan. Nó đã được chứng minh trong các công trình trước đây [22,2,23,24] rằng nó không chỉ có thể nâng cao hiệu suất tổng thể mà còn giảm dung lượng bộ nhớ và tăng tốc quá trình suy luận. Các nghiên cứu trước đây về MTL thường sử dụng cơ chế chia sẻ tham số cứng cùng với các module cụ thể cho nhiệm vụ có trọng lượng nhẹ để xử lý nhiều nhiệm vụ.

**Học đa nhiệm vụ Pareto.** Xuất phát từ thuật toán Multiple-gradient descent (MGDA), một dòng phương pháp MTL dựa trên gradient phổ biến nhằm tìm các giải pháp dừng Pareto, từ đó chúng ta không thể cải thiện thêm hiệu suất mô hình trên bất kỳ nhiệm vụ cụ thể nào mà không làm giảm nhiệm vụ khác [8]. Hơn nữa, các nghiên cứu gần đây đề xuất khám phá toàn bộ mặt trận Pareto bằng cách học các giải pháp đa dạng [25,26,27,28], hoặc lập hồ sơ toàn bộ mặt trận Pareto với hyper-network [29,30]. Mặc dù các phương pháp này có nền tảng lý thuyết và được đảm bảo hội tụ đến các điểm dừng Pareto, kết quả thực nghiệm thường bị hạn chế và thiếu so sánh trong các thiết lập thực tế.

**Cân bằng loss và gradient.** Một nhánh khác của công trình sơ bộ trong MTL tận dụng ý tưởng tái trọng số động các hàm loss dựa trên độ lớn gradient [31], sự không chắc chắn homoscedastic của nhiệm vụ [32], hoặc ưu tiên độ khó [33] để cân bằng gradient giữa các nhiệm vụ. Gần đây hơn, PCGrad [9] phát triển một quy trình thao tác gradient để tránh xung đột giữa các nhiệm vụ bằng cách chiếu gradient nhiệm vụ ngẫu nhiên lên mặt phẳng pháp tuyến của nhiệm vụ khác. Tương tự, [10] đề xuất một phương pháp hội tụ có thể chứng minh để tối thiểu hóa loss trung bình, và [11] tính toán các hệ số tỷ lệ loss sao cho gradient kết hợp có các hình chiếu cùng độ dài lên gradient nhiệm vụ riêng lẻ.

### 2.2 Điểm tối thiểu phẳng

Bộ tối thiểu hóa phẳng đã được phát hiện có thể cải thiện khả năng tổng quát hóa của mạng nơ-ron vì nó cho phép các mô hình tìm các điểm tối thiểu cục bộ rộng hơn, qua đó chúng sẽ mạnh mẽ hơn trước những thay đổi giữa loss huấn luyện và kiểm tra [34,35,36]. Mối quan hệ giữa khả năng tổng quát hóa và độ rộng của các điểm tối thiểu được nghiên cứu về mặt lý thuyết và thực nghiệm trong nhiều nghiên cứu [37,38,39,40], và sau đó, nhiều phương pháp khác nhau tìm kiếm các điểm tối thiểu phẳng đã được đề xuất [41,42,43,44]. Ví dụ, [43,45,46] phân tích tác động của các yếu tố huấn luyện khác nhau, như batch-size, learning rate, covariance của gradient, dropout, lên tính phẳng của các điểm tối thiểu được tìm thấy. Ngoài ra, một số lược đồ theo đuổi các điểm tối thiểu cục bộ rộng bằng cách thêm các số hạng chính quy hóa vào hàm loss [41,47,48,42], ví dụ, hình phạt entropy thấp của đầu ra softmax [41], loss chưng cất [47, 48].

Gần đây, SAM [21], tìm kiếm các vùng phẳng bằng cách tối thiểu hóa rõ ràng loss trường hợp xấu nhất xung quanh mô hình hiện tại, đã nhận được sự chú ý đáng kể do tính hiệu quả và khả năng mở rộng của nó so với các phương pháp trước đây. Đặc biệt, nó đã được khai thác trong nhiều nhiệm vụ và lĩnh vực khác nhau [49,50,51,52,53,54,55]. Một ví dụ đáng chú ý là sự cải thiện mà SAM mang lại cho tối ưu hóa hai cấp meta-learning trong [50]. Một ứng dụng khác của SAM là trong federated learning (FL) [51] trong đó các tác giả đạt được tốc độ hội tụ chặt chẽ hơn so với các công trình FL hiện có, và đề xuất một ràng buộc tổng quát hóa cho mô hình toàn cầu. Ngoài ra, SAM thể hiện khả năng tổng quát hóa của nó trong các mô hình thị giác [54], mô hình ngôn ngữ [53] và tổng quát hóa miền [49]. Tuy nhiên, các nghiên cứu hiện có chỉ tập trung vào các vấn đề đơn nhiệm vụ. Trong công trình này, chúng tôi tận dụng nguyên tắc của SAM để phát triển lý thuyết và đưa ra các phương pháp thực tế, cho phép tìm kiếm các điểm tối thiểu phẳng trong các mô hình học đa nhiệm vụ dựa trên gradient.

## 3 Kiến thức cơ bản

Các phương pháp huấn luyện thông thường tập trung vào việc tối thiểu hóa loss thực nghiệm có thể dễ dàng dẫn đến vấn đề overfitting (tức là, lỗi xác nhận không còn giảm nữa, nhưng loss huấn luyện tiếp tục giảm), do đó hạn chế hiệu suất tổng quát hóa của mô hình. Trong nỗ lực giảm thiểu những hiện tượng như vậy, [21] đề xuất tối thiểu hóa loss trường hợp xấu nhất trong một vùng lân cận của tham số mô hình hiện tại được cho bởi:

$$\min_{\theta} \max_{||\epsilon||_2 \leq \rho} L(\theta + \epsilon), \qquad (1)$$

trong đó $|| \cdot ||_2$ biểu thị chuẩn l2 và $\rho$ đại diện cho bán kính của vùng lân cận. Chúng tôi giả định $L$ có thể vi phân đến bậc một đối với $\theta$. Bài toán tối ưu hóa (1) được gọi là tối thiểu hóa nhận biết độ sắc nét (SAM).

Để giải quyết bài toán (1), [21] đề xuất đầu tiên tìm giải pháp cho bài toán tối đa hóa bên trong bằng cách xấp xỉ $L(\theta + \epsilon)$ thông qua khai triển Taylor bậc một đối với $\epsilon$ xung quanh 0, như sau:

$$\epsilon^* = \arg \max_{||\epsilon||_2 \leq \rho} L(\theta + \epsilon) \approx \arg \max_{||\epsilon||_2 \leq \rho} \epsilon^{\top} \nabla_{\theta} L(\theta) \approx \rho \frac{\nabla_{\theta} L(\theta)}{||\nabla_{\theta} L(\theta)||_2}.$$

Nói cách khác, nhiễu loạn trường hợp xấu nhất được xấp xỉ như gradient có tỷ lệ của loss đối với tham số hiện tại $\theta$. Sau đó, gradient đối với mô hình bị nhiễu loạn này được tính toán để cập nhật $\theta$:

$$g_{SAM} := \nabla_{\theta} \max_{||\epsilon||_2 \leq \rho} L(\theta + \epsilon) \approx \nabla_{\theta} L(\theta + \epsilon)|_{\theta + \epsilon^*} \qquad (2)$$

## 4 Framework đề xuất của chúng tôi

Phần này mô tả framework đề xuất của chúng tôi để cải thiện các phương pháp hiện có về MTL dựa trên gradient. Chúng tôi đầu tiên nhắc lại mục tiêu của học đa nhiệm vụ, sau đó thiết lập các ràng buộc trên cho loss tổng quát của mỗi nhiệm vụ. Tiếp theo, chúng tôi dựa vào các ràng buộc trên này để đưa ra framework đề xuất để cải thiện khả năng tổng quát hóa của mô hình bằng cách hướng dẫn nó đến một vùng phẳng hơn của mỗi nhiệm vụ.

### 4.1 Thiết lập học đa nhiệm vụ

Trong học đa nhiệm vụ, chúng ta được cho một phân phối dữ liệu-nhãn $D$ từ đó chúng ta có thể lấy mẫu một tập huấn luyện $S = \{(x_i, y_i^1, ..., y_i^m)\}_{i=1}^n$, trong đó $x_i$ là một ví dụ dữ liệu và $y_i^1, ..., y_i^m$ là các nhãn của các nhiệm vụ $1, 2, ..., m$ tương ứng.

Mô hình cho mỗi nhiệm vụ $\theta_i = [\theta_{sh}, \theta_i^{ns}]$ bao gồm phần chia sẻ $\theta_{sh}$ và phần không chia sẻ riêng lẻ $\theta_i^{ns}$. Chúng tôi ký hiệu loss tổng quát cho nhiệm vụ $i$ là $L_i^D(\theta_i)$, trong khi loss thực nghiệm của nó trên tập huấn luyện $S$ là $L_i^S(\theta_i)$. Các công trình hiện có trong MTL, điển hình là MGDA [8], PCGrad [9], CAGrad [10], và IMTL [11], nhằm tìm một mô hình đồng thời tối thiểu hóa các loss thực nghiệm cho tất cả các nhiệm vụ:

$$\min_{\theta_{sh}, \theta_{1:m}^{ns}} \{L_1^S(\theta^1), ..., L_m^S(\theta^m)\}, \qquad (3)$$

bằng cách tính toán gradient $g_i$ cho nhiệm vụ thứ $i$ ($i \in [m]$). Tham số mô hình hiện tại sau đó được cập nhật bởi gradient thống nhất $g = \text{gradient\_aggregate}(g_1, g_2, ..., g_m)$, với phép toán chung $\text{gradient\_aggregate}$ là để kết hợp nhiều gradient nhiệm vụ, như được đề xuất trong các nghiên cứu MTL dựa trên gradient.

Ngoài ra, các công trình trước đây chỉ tập trung vào việc tối thiểu hóa các loss thực nghiệm và không quan tâm đến các loss tổng quát để chống lại overfitting. Được lấy cảm hứng từ SAM [21], việc phát triển các phương pháp MTL nhận biết độ sắc nét trong đó các mô hình nhiệm vụ đồng thời tìm kiếm các vùng loss thấp và phẳng là mong muốn. Tuy nhiên, điều này là thách thức vì chúng ta có nhiều hàm mục tiêu trong (3) và mỗi mô hình nhiệm vụ bao gồm một phần chia sẻ và một phần không chia sẻ riêng lẻ. Để giải quyết thử thách trên, trong Định lý 1, chúng tôi phát triển các ràng buộc trên cho các loss tổng quát của nhiệm vụ trong bối cảnh MTL, điều này biểu thị các khái niệm về độ sắc nét cho phần chia sẻ và các phần không chia sẻ và sau đó dựa vào các khái niệm mới này để đưa ra một framework MTL mới thông qua việc tìm kiếm các vùng phẳng dựa trên nhiệm vụ.

### 4.2 Phát triển lý thuyết

Chúng tôi đầu tiên phát biểu định lý chính của chúng tôi để ràng buộc hiệu suất tổng quát hóa của các nhiệm vụ riêng lẻ bằng lỗi thực nghiệm trên tập huấn luyện:

**Định lý 1.** (Phát biểu không chính thức) Đối với bất kỳ bán kính nhiễu loạn $\rho_{sh}, \rho_{ns} > 0$, dưới một số giả định nhẹ, với xác suất $1 - \delta$ (trên lựa chọn tập huấn luyện $S \sim D$) chúng ta có được

$$\{L_i^D(\theta^i)\}_{i=1}^m \leq \left\{\max_{||\epsilon_{sh}||_2 \leq \rho_{sh}} \max_{||\epsilon_i^{ns}||_2 \leq \rho_{ns}} L_i^S(\theta_{sh} + \epsilon_{sh}, \theta_i^{ns} + \epsilon_i^{ns}) + f_i(||\theta^i||_2^2)\right\}_{i=1}^m, \qquad (4)$$

trong đó $f_i: \mathbb{R}^+ \rightarrow \mathbb{R}^+, i \in [m]$ là các hàm tăng ngặt.

Định lý 1 thiết lập mối liên hệ giữa lỗi tổng quát hóa của mỗi nhiệm vụ với lỗi huấn luyện thực nghiệm của nó thông qua nhiễu loạn trường hợp xấu nhất trên không gian tham số. Định lý được phát biểu chính thức và chứng minh được cung cấp trong phụ lục. Ở đây chúng tôi lưu ý rằng nhiễu loạn chia sẻ trường hợp xấu nhất $\epsilon_{sh}$ được học chung cho tất cả các nhiệm vụ, trong khi nhiễu loạn không chia sẻ trường hợp xấu nhất $\epsilon_i^{ns}$ được điều chỉnh cho mỗi nhiệm vụ $i$. Định lý 1 trực tiếp gợi ý cho chúng ta một phương pháp tiếp cận ban đầu và trực tiếp.

Ngoài ra, [21] viện dẫn ràng buộc tổng quát hóa PAC-Bayesian [56], do đó chỉ áp dụng cho loss 0-1 trong thiết lập phân loại nhị phân. Về mặt đóng góp lý thuyết, chúng tôi sử dụng một ràng buộc tổng quát hóa PAC-Bayesian tổng quát hơn [57] để giải quyết các loss tổng quát hơn trong MTL. Hơn nữa, việc phát triển lý thuyết của chúng tôi đòi hỏi chúng tôi phải xử lý nhiều mục tiêu, mỗi mục tiêu bao gồm các phần không chia sẻ và chia sẻ, điều này chắc chắn không đơn giản.

### 4.3 Phương pháp tiếp cận ban đầu và trực tiếp

Một phương pháp tiếp cận đơn giản được hướng dẫn bởi Định lý 1 là tìm các nhiễu loạn không chia sẻ $\epsilon_i^{ns}, i \in [m]$ một cách độc lập cho các phần không chia sẻ và một nhiễu loạn chia sẻ chung cho phần chia sẻ. Được thúc đẩy bởi hướng dẫn lý thuyết này, chúng tôi đề xuất các cập nhật sau.

**Cập nhật các phần không chia sẻ.** Dựa trên các ràng buộc trên trong Định lý 1, vì các nhiễu loạn không chia sẻ $\epsilon_i^{ns}, i \in [m]$ độc lập với mỗi nhiệm vụ, đối với nhiệm vụ $i$, chúng tôi cập nhật phần không chia sẻ $\theta_i^{ns}$ của nó:

$$\epsilon_i^{ns} = \rho_{ns} \frac{\nabla_{\theta_i^{ns}} L_i^S(\theta_{sh}, \theta_i^{ns})}{||\nabla_{\theta_i^{ns}} L_i^S(\theta_{sh}, \theta_i^{ns})||_2},$$

$$g_i^{SAM,ns} = \nabla_{\theta_i^{ns}} L_i^S(\theta_{sh}, \theta_i^{ns} + \epsilon_i^{ns}),$$

$$\theta_i^{ns} = \theta_i^{ns} - \eta g_i^{SAM,ns}, \text{ trong đó } \eta > 0 \text{ là learning rate}. \qquad (5)$$

**Cập nhật phần chia sẻ.** Việc cập nhật phần chia sẻ $\theta_{sh}$ khó khăn hơn vì nhiễu loạn trường hợp xấu nhất $\epsilon_{sh}$ của nó được chia sẻ giữa các nhiệm vụ. Để tìm ra cách cập nhật $\theta_{sh}$ đối với tất cả các nhiệm vụ, chúng tôi đầu tiên thảo luận về trường hợp khi chúng tôi cập nhật điều này đối với nhiệm vụ $i$ mà không quan tâm đến các nhiệm vụ khác. Cụ thể, gradient SAM chia sẻ của nhiệm vụ này được tính toán như:

$$\epsilon_i^{sh} = \rho_{sh} \frac{\nabla_{\theta_{sh}} L_i^S(\theta_{sh}, \theta_i^{ns})}{||\nabla_{\theta_{sh}} L_i^S(\theta_{sh}, \theta_i^{ns})||_2},$$

$$g_i^{SAM,sh} = \nabla_{\theta_{sh}} L_i^S(\theta_{sh} + \epsilon_i^{sh}, \theta_i^{ns}),$$

sau đó chúng tôi có một chiến lược cập nhật đơn giản:

$$g_{sh}^{SAM} = \text{gradient\_aggregate}(g_1^{SAM,sh}, ..., g_m^{SAM,sh}),$$

$$\theta_{sh} = \theta_{sh} - \eta g_{sh}^{SAM}.$$

Theo phân tích của chúng tôi trong Phần 4.4, mỗi $g_i^{SAM,sh} = g_i^{loss,sh} + g_i^{flat,sh}$ bao gồm hai thành phần: (i) $g_i^{loss,sh}$ để điều hướng đến vùng loss thấp của nhiệm vụ và (ii) $g_i^{flat,sh}$ để điều hướng đến vùng phẳng dựa trên nhiệm vụ. Tuy nhiên, việc tổng hợp gradient trực tiếp của $g_i^{SAM,sh}, i \in [m]$ có thể bị ảnh hưởng tiêu cực bởi sự triệt tiêu hoặc xung đột gradient vì nó nhằm kết hợp nhiều yếu tố riêng lẻ với các mục tiêu khác nhau. Trong bài báo này, chúng tôi vượt qua phương pháp tiếp cận ban đầu này bằng cách tìm ra một công thức cập nhật để phân tách gradient SAM thành hai thành phần, mỗi thành phần phục vụ mục đích riêng của nó, và sau đó kết hợp các gradient nhiệm vụ tương ứng của chúng đồng thời. Chúng tôi cũng so sánh phương pháp của chúng tôi với phương pháp tiếp cận ngây thơ trong Phần 5.3.

### 4.4 Phương pháp đề xuất của chúng tôi

Các phần không chia sẻ được cập nhật bình thường như trong Phương trình (5). Việc điều tra cách cập nhật phần chia sẻ hiệu quả hơn là quan trọng hơn. Để hiểu rõ hơn về các gradient của SAM, chúng tôi phân tích các đặc điểm của chúng bằng cách tìm ra chúng như sau:

$$g_i^{SAM,sh} = \nabla_{\theta_{sh}} L_i^S(\theta_{sh} + \epsilon_i^{sh}, \theta_i^{ns}) \stackrel{(1)}{\approx} \nabla_{\theta_{sh}} L_i^S(\theta_{sh}, \theta_i^{ns}) + \langle \epsilon_i^{sh}, \nabla_{\theta_{sh}} L_i^S(\theta_{sh}, \theta_i^{ns}) \rangle$$

$$= \nabla_{\theta_{sh}} L_i^S(\theta_{sh}, \theta_i^{ns}) + \rho_{sh} \frac{\langle \nabla_{\theta_{sh}} L_i^S(\theta_{sh}, \theta_i^{ns}), \nabla_{\theta_{sh}} L_i^S(\theta_{sh}, \theta_i^{ns}) \rangle}{||\nabla_{\theta_{sh}} L_i^S(\theta_{sh}, \theta_i^{ns})||_2}$$

$$= \nabla_{\theta_{sh}} L_i^S(\theta_{sh}, \theta_i^{ns}) + \rho_{sh} ||\nabla_{\theta_{sh}} L_i^S(\theta_{sh}, \theta_i^{ns})||_2 \qquad (6)$$

trong đó trong $(1) \approx$, chúng tôi áp dụng khai triển Taylor bậc một và $\langle \cdot, \cdot \rangle$ biểu thị tích vô hướng. Rõ ràng là việc theo hướng âm của $g_i^{SAM,sh}$ sẽ tối thiểu hóa loss $L_i^S(\theta_{sh}, \theta_i^{ns})$ và chuẩn gradient $||\nabla_{\theta_{sh}} L_i^S(\theta_{sh}, \theta_i^{ns})||_2$ của nhiệm vụ $i$, do đó dẫn mô hình đến vùng có giá trị thấp cho loss của nhiệm vụ này và vùng phẳng hơn của nó với độ lớn chuẩn gradient thấp hơn.

Hơn nữa, được lấy cảm hứng từ việc tìm ra trong Phương trình (6), chúng tôi phân tách gradient $g_i^{SAM,sh} = g_i^{loss,sh} + g_i^{flat,sh}$ trong đó chúng tôi định nghĩa $g_i^{loss,sh} := \nabla_{\theta_{sh}} L_i^S(\theta_{sh}, \theta_i^{ns})$ và $g_i^{flat,sh} := g_i^{SAM,sh} - g_i^{loss,sh}$. Như đã đề cập trước đây, mục đích của gradient âm $-g_i^{loss,sh}$ là định hướng mô hình để tối thiểu hóa loss của nhiệm vụ $i$, trong khi $-g_i^{flat,sh}$ điều hướng mô hình đến vùng phẳng hơn của nhiệm vụ $i$.

Do đó, các gradient SAM $g_i^{SAM,sh}, i \in [m]$ bao gồm hai thành phần với các mục đích khác nhau. Để giảm thiểu xung đột và can thiệp có thể xảy ra của hai thành phần khi tổng hợp, chúng tôi đề xuất tổng hợp các thành phần loss thấp riêng biệt và sau đó các thành phần phẳng riêng biệt. Cụ thể, để tìm một hướng chung dẫn đến các loss có giá trị thấp chung cho tất cả các nhiệm vụ và vùng phẳng hơn chung cho chúng, chúng tôi đầu tiên kết hợp các gradient $g_i^{loss,sh}, i \in [m]$ và các gradient $g_i^{flat,sh}, i \in [m]$, sau đó thêm hai gradient được tổng hợp, và cuối cùng cập nhật phần chia sẻ như:

$$g_{sh}^{loss} = \text{gradient\_aggregate}(g_1^{loss,sh}, ..., g_m^{loss,sh}),$$

$$g_{sh}^{flat} = \text{gradient\_aggregate}(g_1^{flat,sh}, ..., g_m^{flat,sh}),$$

$$g_{sh}^{SAM} = g_{sh}^{loss} + g_{sh}^{flat}; \quad \theta_{sh} = \theta_{sh} - \eta g_{sh}^{SAM},$$

Cuối cùng, các bước chính của framework đề xuất của chúng tôi được tóm tắt trong Thuật toán 1 và sơ đồ tổng thể của phương pháp đề xuất của chúng tôi được thể hiện trong Hình 1.

**Thuật toán 1** Tối thiểu hóa độ sắc nét cho học đa nhiệm vụ

**Đầu vào:** Tham số mô hình $\theta = [\theta_{sh}, \theta_{1:m}^{ns}]$, bán kính nhiễu loạn $\rho = [\rho_{sh}, \rho_{ns}]$, kích thước bước $\eta$ và danh sách $m$ hàm loss có thể vi phân $\{L_i\}_{i=1}^m$.

**Đầu ra:** Tham số cập nhật $\theta^*$

1: **for** nhiệm vụ $i \in [m]$ **do**
2: Tính toán gradient $g_i^{loss,sh}, g_i^{ns} \leftarrow \nabla_{\theta} L_i(\theta)$
3: Hướng nhiễu loạn trường hợp xấu nhất
   $\epsilon_i^{sh} = \rho_{sh} \cdot g_i^{loss,sh} / ||g_i^{loss,sh}||$ và $\epsilon_i^{ns} = \rho_{ns} \cdot g_i^{ns} / ||g_i^{ns}||$
4: Gradient SAM xấp xỉ
   $g_i^{SAM,sh} = \nabla_{\theta_{sh}} L_i(\theta_{sh} + \epsilon_i^{sh}, \theta_i^{ns})$ và $g_i^{SAM,ns} = \nabla_{\theta_i^{ns}} L_i(\theta_{sh}, \theta_i^{ns} + \epsilon_i^{ns})$
5: Tính toán gradient phẳng
   $g_i^{flat,sh} = g_i^{SAM,sh} - g_i^{loss,sh}$
6: **end for**
7: Tính toán gradient cập nhật kết hợp:
   $g_{sh}^{loss} = \text{gradient\_aggregate}(g_1^{loss,sh}, g_2^{loss,sh}, ..., g_m^{loss,sh})$
   $g_{sh}^{flat} = \text{gradient\_aggregate}(g_1^{flat,sh}, g_2^{flat,sh}, ..., g_m^{flat,sh})$
8: Tính toán cập nhật gradient chia sẻ $g_{sh}^{SAM} = g_{sh}^{loss} + g_{sh}^{flat}$
9: Cập nhật tham số mô hình
   $\theta^* = [\theta_{sh}, \theta_{1:m}^{ns}] - \eta [g_{sh}^{SAM}, g_{1:m}^{SAM,ns}]$

## 5 Thí nghiệm

Trong phần này, chúng tôi so sánh phương pháp đề xuất của chúng tôi với các phương pháp tiên tiến khác của học đa nhiệm vụ trong các tình huống khác nhau, từ phân loại hình ảnh đến các vấn đề hiểu cảnh. Tham khảo phụ lục để biết các thiết lập chi tiết được sử dụng cho mỗi bộ dữ liệu và các thí nghiệm bổ sung.

**Bộ dữ liệu và Baseline.** Phương pháp đề xuất của chúng tôi được đánh giá trên bốn benchmark MTL bao gồm Multi-MNIST [25], CelebA [58] cho phân loại thị giác, và NYUv2 [59], CityScapes [60] cho hiểu cảnh. Chúng tôi thể hiện cách framework của chúng tôi có thể tăng cường hiệu suất của các phương pháp MTL dựa trên gradient bằng cách so sánh MGDA vanilla [8], PCGrad [9], CAGrad [10] và IMTL [11] với các phiên bản dựa trên phẳng F-MGDA, F-PCGrad, F-CAGrad và F-IMTL của chúng. Chúng tôi cũng thêm baseline học đơn nhiệm vụ (STL) cho mỗi bộ dữ liệu.

### 5.1 Phân loại hình ảnh

**Multi-MNIST.** Theo giao thức của [8], chúng tôi thiết lập ba thí nghiệm Multi-MNIST với backbone ResNet18 [61], cụ thể là: MultiFashion, MultiMNIST và MultiFashion+MNIST. Trong mỗi bộ dữ liệu, hai hình ảnh được lấy mẫu đều từ MNIST [62] hoặc Fashion-MNIST [63], sau đó một hình được đặt ở góc trên bên trái và hình kia ở góc dưới bên phải. Do đó chúng tôi có được một học hai nhiệm vụ đòi hỏi dự đoán các danh mục của các chữ số hoặc món đồ thời trang ở góc trên bên trái (nhiệm vụ 1) và ở góc dưới bên phải (nhiệm vụ 2) tương ứng.

Như được tóm tắt trong Bảng 1, chúng ta có thể thấy rằng tìm kiếm các vùng phẳng hơn cho tất cả các nhiệm vụ có thể cải thiện hiệu suất của tất cả các baseline trên cả ba bộ dữ liệu. Đặc biệt, các phương pháp dựa trên phẳng đạt được điểm số cao nhất cho mỗi nhiệm vụ và cho trung bình, vượt trội so với STL 1,2% trên MultiFashion và MultiMNIST. Chúng tôi suy đoán rằng sự khác biệt giữa các quỹ đạo cập nhật gradient để phân loại các chữ số từ MNIST và các món đồ thời trang từ FashionMNIST đã dẫn đến hiệu suất vô ích của các baseline, so với STL trên MultiFashion+MNIST. Ngay cả khi tồn tại sự khác biệt giữa các nhiệm vụ, độ chính xác trung bình tốt nhất mà chúng tôi có được khi áp dụng phương pháp của chúng tôi cho CAGrad chỉ thấp hơn STL một chút (<0,4%) trong khi chỉ sử dụng một mô hình duy nhất.

Thú vị là, phương pháp huấn luyện MTL được đề xuất của chúng tôi cũng giúp cải thiện hiệu suất hiệu chuẩn mô hình bằng cách giảm thiểu hiện tượng quá tự tin của mạng nơ-ron sâu. Như có thể thấy từ Hình 2, phương pháp của chúng tôi tạo ra các dự đoán có entropy cao đại diện cho sự không chắc chắn của nó, phương pháp dựa trên ERM xuất ra các dự đoán tự tin cao trên cả dữ liệu trong miền và ngoài miền. Thêm chi tiết về cải thiện hiệu chuẩn mô hình có thể được tìm thấy trong phụ lục.

**CelebA.** CelebA [64] là một bộ dữ liệu khuôn mặt, bao gồm 200K ảnh khuôn mặt người nổi tiếng với 40 thuộc tính. Tương tự như [8], mỗi thuộc tính tạo thành một bài toán phân loại nhị phân, do đó một bài toán phân loại đa nhãn 40 lớp được xây dựng.

Bảng 2 thể hiện các lỗi trung bình trên 40 nhiệm vụ của các phương pháp với tuyến tính hóa vô hướng (LS) và Trọng số không chắc chắn (UW) [32] được bao gồm để phục vụ như các baseline so sánh. Kết quả tốt nhất trong mỗi cặp và trong tất cả được làm nổi bật bằng cách sử dụng chữ đậm và *, tương ứng. Khi số lượng nhiệm vụ lớn, tìm kiếm vùng phẳng vẫn nhất quán thể hiện ưu điểm của nó và lỗi độ chính xác trung bình thấp nhất được đạt bởi F-CAGrad. Thú vị là, khi bộ tối ưu hóa nhận thức về các điểm tối thiểu phẳng, khoảng cách giữa PCGrad, IMTL và CAGrad, (8,23, 8,24 vs 8,22), nhỏ hơn so với những khoảng cách sử dụng huấn luyện ERM thông thường, (8,69, 8,88 và 8,52). Điều này có thể do việc tổng hợp tốt hơn các gradient của nhiệm vụ, có nghĩa là xung đột giữa các gradient này có khả năng giảm khi các tham số chia sẻ tiếp cận vùng phẳng chung của tất cả các nhiệm vụ.

### 5.2 Hiểu cảnh

Hai bộ dữ liệu được sử dụng trong tiểu mục này là NYUv2 [59] và CityScapes [60]. NYUv2 là một bộ dữ liệu cảnh trong nhà chứa 3 nhiệm vụ: phân đoạn ngữ nghĩa 13 lớp, ước lượng độ sâu, và dự đoán pháp tuyến bề mặt. Trong CityScapes, có 19 lớp hình ảnh từ góc nhìn đường phố, được làm thô thành 7 danh mục để tạo hai nhiệm vụ: phân đoạn ngữ nghĩa và ước lượng độ sâu. Đối với hai thí nghiệm này, chúng tôi thêm một số phương pháp MTL gần đây, cụ thể là, bất biến thang đo (SI), trọng số loss ngẫu nhiên (RLW), Trung bình trọng số động (DWA) [2], GradDrop [65], và Nash-MTL [13] có kết quả được lấy từ [13]. Chi tiết của mỗi baseline có thể được tìm thấy trong phụ lục. Cũng theo giao thức tiêu chuẩn được sử dụng trong [2,10,13], Mạng chú ý đa nhiệm vụ [2] được sử dụng trên đầu kiến trúc SegNet [66], kết quả được trình bày của chúng tôi là trung bình trên 10 epoch cuối cùng để phù hợp với công việc trước đây.

**Số liệu đánh giá.** Trong thí nghiệm này, chúng tôi phải xử lý các loại nhiệm vụ khác nhau thay vì chỉ một loại như trong trường hợp phân loại hình ảnh. Vì mỗi trong số chúng có bộ số liệu riêng. Do đó chúng tôi đánh dấu hiệu suất tổng thể của các phương pháp so sánh bằng cách báo cáo cải thiện nhiệm vụ tương đối [67] của chúng trong suốt phần này. Để $M_i$ và $S_i$ là các số liệu thu được bởi mô hình chính và mô hình học đơn nhiệm vụ (STL), tương ứng, cải thiện nhiệm vụ tương đối trên nhiệm vụ thứ i được cho bởi toán học: $\Delta_i := 100 \cdot (-1)^{l_i}(M_i - S_i)/S_i$, trong đó $l_i = 1$ nếu giá trị thấp hơn cho tiêu chí thứ i là tốt hơn và 0 ngược lại. Chúng tôi mô tả kết quả của chúng tôi bằng cải thiện nhiệm vụ tương đối trung bình $\Delta_m\% = \frac{1}{m}\sum_{i=1}^m \Delta_i$.

**CityScapes.** Trong Bảng 3, tác động tích cực của việc tìm kiếm các vùng phẳng được quan sát thấy một cách nhất quán trong tất cả các số liệu và baseline. Đặc biệt, các cải thiện tương đối của MGDA và IMTL được tăng cường đáng kể, đạt được điểm số $\Delta_m\%$ cao nhất và cao thứ hai, tương ứng. Điểm số phân đoạn của PCGrad, CAGrad và IMTL thậm chí còn vượt qua STL. Thú vị là, MGDA thiên về mục tiêu ước lượng độ sâu, dẫn đến hiệu suất ưu thế trên nhiệm vụ đó, các mẫu tương tự xuất hiện trong [11] và thí nghiệm NYUv2 dưới đây.

**NYUv2.** Bảng 4 thể hiện kết quả của mỗi nhiệm vụ và các cải thiện tương đối so với STL của các phương pháp khác nhau. Nói chung, các phiên bản dựa trên phẳng thu được kết quả tương đương hoặc cao hơn trên hầu hết các số liệu, trừ MGDA tại nhiệm vụ phân đoạn, trong đó F-MGDA giảm đáng kể điểm số mIoU. Tuy nhiên, nó thực sự giúp ích đáng kể cho các nhiệm vụ khác, điều này góp phần vào cải thiện tương đối tổng thể của MGDA, từ 1,38% tệ hơn STL đến 0,33% tốt hơn. Đáng chú ý, F-CAGrad và F-IMTL vượt trội so với các đối thủ cạnh tranh của chúng bằng lợi thế lớn trên tất cả các nhiệm vụ, dẫn đến hai cải thiện tương đối hàng đầu, 3,78% và 4,77%.

### 5.3 Nghiên cứu loại bỏ

Cho đến nay, kỹ thuật đề xuất của chúng tôi đã thể hiện hiệu suất tiên tiến dưới các thiết lập khác nhau, bây giờ chúng tôi điều tra chi tiết hơn về cách nó ảnh hưởng đến huấn luyện thông thường bằng cách kiểm tra các bề mặt loss và độ mạnh mẽ của mô hình. Các mẫu tương tự được quan sát thấy trong các thí nghiệm khác và được đưa ra trong phụ lục.

**Xung đột nhiệm vụ.** Để xác nhận thực nghiệm rằng các gradient của nhiệm vụ ít xung đột hơn khi mô hình được hướng đến các vùng phẳng, chúng tôi đo lường xung đột gradient và trình bày kết quả trong Hình 3. Trong khi tỷ lệ phần trăm xung đột gradient của ERM tăng lên hơn 50%, của chúng tôi giảm và tiến gần đến 0%. Việc giảm xung đột gradient này cũng là mục tiêu của các phương pháp MTL dựa trên gradient gần đây trong việc giảm thiểu chuyển giao tiêu cực giữa các nhiệm vụ [9, 68, 3].

**Độ mạnh mẽ của mô hình chống lại nhiễu.** Để xác minh rằng SAM có thể định hướng mô hình đến vùng phẳng và loss thấp chung của tất cả các nhiệm vụ, chúng tôi đo lường hiệu suất mô hình trong một quả cầu Euclidean có bán kính r. Để cụ thể hơn, chúng tôi làm nhiễu loạn các tham số của hai mô hình hội tụ bằng $\epsilon$, nằm trong một quả cầu có bán kính r và vẽ độ chính xác của các mô hình bị nhiễu loạn của mỗi nhiệm vụ khi chúng tôi tăng r từ 0 đến 1000. Tại mỗi giá trị của r, 10 mô hình khác nhau xung quanh quả cầu có bán kính r của mô hình hội tụ được lấy mẫu.

Trong Hình 4, độ chính xác của mô hình được huấn luyện bằng phương pháp của chúng tôi duy trì ở mức cao khi nhiễu tiếp tục tăng cho đến r = 800. Điều này cũng đưa ra bằng chứng rằng mô hình của chúng tôi tìm thấy một vùng thay đổi chậm về loss. Ngược lại, mô hình được huấn luyện một cách ngây thơ mất khả năng dự đoán ngay khi nhiễu xuất hiện và trở thành một bộ phân loại giả mạo đạt độ chính xác 10% trong phân loại 10 lớp.

**So sánh chiến lược tổng hợp.** Bảng 5 cung cấp so sánh giữa việc tổng hợp trực tiếp trên $\{g_i^{SAM,sh}\}_{i=1}^m$ và tổng hợp riêng lẻ trên $\{g_i^{flat,sh}\}_{i=1}^m$ và $\{g_i^{loss,sh}\}_{i=1}^m$ (phương pháp của chúng tôi). So với phương pháp tiếp cận ngây thơ, trong đó các gradient SAM theo nhiệm vụ được tổng hợp trực tiếp, phương pháp phân tách của chúng tôi liên tục cải thiện hiệu suất bằng lợi thế lớn trên tất cả các nhiệm vụ. Kết quả này củng cố lý luận đằng sau việc tổng hợp riêng biệt các hướng loss thấp và hướng phẳng.

**Trực quan hóa các cảnh quan loss.** Theo [69], chúng tôi vẽ các bề mặt loss tại sự hội tụ sau khi huấn luyện Resnet18 từ đầu trên bộ dữ liệu MultiMNIST. Các bề mặt loss kiểm tra của các checkpoint có điểm số độ chính xác xác nhận cao nhất được thể hiện trong Hình 5.

Chúng ta có thể thấy rõ rằng giải pháp được tìm thấy bởi phương pháp đề xuất của chúng tôi không chỉ giảm thiểu độ sắc nét loss kiểm tra cho cả hai nhiệm vụ mà còn có thể cố ý giảm giá trị loss kiểm tra, so với ERM truyền thống. Đây là một hành vi chung khi sử dụng các bộ tối thiểu hóa phẳng vì khoảng cách giữa hiệu suất huấn luyện và kiểm tra đã được thu hẹp [44, 14].

## 6 Kết luận

Trong công trình này, chúng tôi đã trình bày một framework tổng quát có thể được kết hợp vào các phương pháp học đa nhiệm vụ hiện tại theo cơ chế cân bằng gradient. Ý tưởng cốt lõi của phương pháp đề xuất của chúng tôi là việc sử dụng các bộ tối thiểu hóa phẳng trong bối cảnh MTL và chứng minh rằng chúng có thể giúp nâng cao các công trình trước đây cả về mặt lý thuyết và thực nghiệm. Cụ thể, phương pháp của chúng tôi vượt ra ngoài việc tối ưu hóa các mục tiêu theo nhiệm vụ để tạo ra các mô hình có cả lỗi thấp và khả năng tổng quát hóa cao. Về mặt thực nghiệm, hiệu quả của phương pháp chúng tôi được chứng minh trên một loạt các benchmark MTL được sử dụng phổ biến, trong đó phương pháp của chúng tôi liên tục vượt trội so với các phương pháp so sánh.
