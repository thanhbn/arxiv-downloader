# 2307.03374.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multi-task/2307.03374.pdf
# File size: 15426482 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
STG-MTL: Scalable Task Grouping
For Multi-Task Learning Using Data Maps
Ammar Sherif∗1, Abubakar Abid2, Mustafa Elattar1, and Mohamed ElHelw1
1Nile University
2Hugging Face
Abstract
Multi-Task Learning (MTL) is a powerful technique that has gained popularity
due to its performance improvement over traditional Single-Task Learning (STL).
However, MTL is often challenging because there is an exponential number of
possible task groupings, which can make it difficult to choose the best one be-
cause some groupings might produce performance degradation due to negative
interference between tasks. That is why existing solutions are severely suffering
from scalability issues, limiting any practical application. In our paper, we propose
a new data-driven method that addresses these challenges and provides a scal-
able and modular solution for classification task grouping based on a re-proposed
data-driven features, Data Maps, which capture the training dynamics for each
classification task during the MTL training. Through a theoretical comparison with
other techniques, we manage to show that our approach has the superior scalability.
Our experiments show a better performance and verify the method’s effectiveness,
even on an unprecedented number of tasks (up to 100 tasks on CIFAR100). Being
the first to work on such number of tasks, our comparisons on the resulting group-
ing shows similar grouping to the mentioned in the dataset, CIFAR100. Finally,
we provide a modular implementation2for easier integration and testing, with
examples from multiple datasets and tasks.
1 Introduction
Multi-Task Learning (MTL) has emerged as a powerful technique in deep learning [ 39,7] that allows
for joint training of multiple related tasks, leading to improved model performance compared to
traditional Single-Task Learning (STL). By leveraging shared representations and knowledge across
tasks, MTL enhances generalization and mitigates overfitting. Furthermore, MTL promotes faster
learning of related tasks and alleviates the computational requirements of deep learning, making
it particularly valuable in scenarios with limited task-specific data. That is why MTL has gained
significant attention in various domains, including computer vision [ 8,25,30], natural language
processing [ 41,26,16,4], speech recognition [ 13,40], and healthcare [ 26,2,15], and has shown
promising results in improving accuracy, robustness, and efficiency. However, effectively harnessing
the potential of MTL poses several challenges, including the identification of optimal task groupings
[29, 9, 30] and the management of negative interference between tasks [28, 35, 24].
The task grouping problem in MTL is particularly challenging due to the exponential number of
possible task combinations [ 1,9,30,29]. What makes it worse for the exhaustive search is that
each trial involves a complete training and evaluation procedure, leading to computational and
optimization burden. Moreover, inappropriate task groupings may result in performance degradation
∗Corresponding Author <ammarsherif90 [at] gmail [dot] com>
2https://github.com/ammarSherif/STG-MTLarXiv:2307.03374v2  [cs.LG]  26 May 2024

--- PAGE 2 ---
Shared LayersHead 1 Head 2 Head 
InputTask 2 O/P Task  O/P
Data MapsMulti-Head MTL  Architecture
Task 1 O/PTasks Loss
Weights
Specialized MTL  Multi-Head TrainingTasks Loss
Weights
Tasks
ClustersModel 1 Model 
(1) Train Multi-Head MTL  on all the tasks(2) Extract the
data maps(3) Soft-cluster to generate
task membership weights[Optional]
    (4) Use membership as loss weights
to train  specialized modelsFigure 1: Overview of our method to cluster the tasks using Data Maps. (1)we use a single Multi-
head Multi-Task Learning architecture to jointly train all the tasks. Each head is task-specific layers.
(2)we extract the data maps of all the tasks across the epochs in E.(3)we use the data maps to
cluster the tasks using kmeans and generate the memberships according to Equation 2. (4)to evaluate
our clustering results, we train mmodels where each model represents a cluster focusing on particular
tasks using the memberships as loss weights.
due to negative transfer between tasks [ 28,35,24]. Existing solutions, which address these challenges,
often suffer from scalability and modularity issues, making their practical application in real-world
scenarios nearly infeasible.
In this paper, we propose a novel data-driven method for task grouping in MTL for classification
tasks, which overcomes the scalability and modularity limitations. Our method utilizes Data Maps
[33], data-driven features that capture the training dynamics of each classification task during an
MTL training. By analyzing these data maps, we can identify task groupings, both hard and
soft ones, that promote positive transfer and mitigate negative interference as much as possible.
We demonstrate the effectiveness of our method through extensive experimentation, including
experiments on an unprecedented number of tasks, scaling up to 100 tasks to emphasize the practicality
of our approach, where the generated groupings aligned with the dataset categorization. We also
illustrate the performance improvement of the method in comparison to using both MTL and STL for
training. Furthermore, we emphasize the practicality of our approach by providing a modular code
implementation, making it easier for researchers and developers to integrate and test our method on
their own datasets and tasks.
The contributions of this paper are summarized below:
•We propose a novel data-driven method for classification task grouping in MTL, addressing
the challenges of scalability and modularity, by re-proposing the usage of data maps as task
features.
• We utilize soft-clustering weights to enable model specialization via loss weighting.
•We conduct extensive experiments, demonstrating the effectiveness of our method, even on
a large number of tasks (scaling up to 100 classification tasks).
•We provide a modular code implementation of our method, facilitating its adoption and
usage by both the research and the industry communities.
2 Related Work
MTL has been extensively studied to leverage the benefits of information sharing among related tasks,
which can serve as an inductive bias to improve modeling performance [ 5,39]. Another perspective
on MTL is that it enables more efficient utilization of the model capacity by focusing on learning
relevant features and reducing the impact of irrelevant signals, which contributes to overfitting,
2

--- PAGE 3 ---
leading to better generalization. However, when tasks lack shared information, they compete for the
limited model capacity, resulting in performance degradation [ 28,35,24]. To address this challenge,
task grouping has emerged as a promising solution to identify subsets of tasks that can be trained
together. This helps with avoiding negative interference and promoting improved performance.
Traditionally, the decision of task grouping has been approached through costly cross-validation
techniques or human expert knowledge [ 39]. However, these methods have limitations when applied
to different problem domains and do not scale well. Therefore, some simply used correlation, either
between model task predictions [ 36], data labels, or task weights [ 37], to recognize related tasks. For
instance, in medical imaging, these methods have been applied to improve predictions for Cardiac
Indices [ 36,37]. However, these approaches are ineffective in scenarios lacking explicit correlations
between task labels, in the dataset itself, such as exclusive membership binary classification tasks.
Furthermore, both works aim to know task relationships to improve the model predictions of Cardiac
Indices in multi-view or multi-modal settings, using a single model. This is quite different from our
objective of finding the relations to split the tasks into different models to avoid negative interference.
Other attempts have been made to approach the problem differently enabling the models to automate
the search over which parameters to share among particular tasks [ 38,25]. Methods such as Neural
Architecture Search [ 20,12,6,38,32,34], Soft-Parameter Sharing [ 27,23,25], and asymmetric
information transfer [ 18,19,11] have been developed. However, these models often exhibit poor
generalization and struggle to perform well on diverse tasks and domains. Besides, they often require
a large model capacity and do not thus scale well enough with a large number of tasks.
Therefore, gradient-based approaches [ 9,31,14] have also been explored to determine task grouping
in advance. The Task Affinity Grouping (TAG) approach [ 9], which leverages gradients to determine
task similarity, is an example of such an approach. Nevertheless, it has complex training paradigm
and requires Θ(n2)more forward and backward passes to compute the inter task affinities, putting
an issue with scalability even if we enhance the solution’s modularity. Another method, called
Higher-Order Approximation (HOA) [ 30], reduces the exponential number of MTL training, from
the exhaustive search, by considering only the quadratic pairs of task combinations. However, even
with such relaxation, the scalability of HOA remains limited, particularly when dealing with a large
number of tasks.
Recent studies have embraced the utilization of accumulated gradients to depict individual tasks
as vectors [ 14], offering improved scalability by necessitating fine-tuning of only Θ(n)models.
However, this method encounters significant limitations concerning modularity. The requirement
to fine-tune a pre-trained model for each task results in a dependency where all tasks must share
the same pre-trained model, which could pose challenges when dealing with a large number of
tasks. It has also a constraint that all tasks must share the same architecture because task vectors
are expressed in the model’s weight space. Furthermore, the work focuses on using task vectors to
enhance fine-tuning and transfer learning, rather than grouping the tasks into different MTL models
to mitigate negative interference.
The task grouping problem has as well been addressed differently through a Meta-Learning approach
[29], aiming to create a meta-learner that can estimate task grouping gains. Nevertheless, the
computational demands of this approach pose practical challenges for real-world applications; it
requires training MTL networks for every chosen task combination in the training set for multiple
iterations. It furthermore outputs all the possible gains of every task combination, whose numbers
grow exponentially, and runs a search algorithm over these exponentially growing gains to find the
optimal grouping. As a result, the scalability of this solution is severely limited, making it less
feasible for a larger number of tasks.
3 Task Clustering using Data Maps
Now, we elaborate in the components in our method in the next sections. We start with stating the
notations we will use along with our MTL architecture we are using in our experiments in Section 3.1.
Then, we move on to illustrate the data maps, which is crucial component of our method in Section
3.2. In Section 3.3, we talk regarding the approaches we use to cluster the tasks. We also introduce
our evaluation mechanism of our task grouping in Section 3.4. Finally, we conclude this part with a
simple theoretical comparison of our method and the literature from the perspective of scalability and
modularity in Section 3.5. Figure 1 provides an overview of our method.
3

--- PAGE 4 ---
3.1 Preliminaries
Notations In our paper, we use the following notations. The set of all tasks is denoted as T=
{T1, . . . , T n}, where nrepresents the number of tasks and |T|=n. The total number of training
data points is denoted as N. We calculate the data maps at specific epochs, and the set of epochs is
represented as E={E1, . . . , E e}, where Eicorresponds to the ithepoch and |E|=e. The task
clusters are denoted by C={C1, . . . , C m}, and each cluster Cihas an associated centroid ciand
|C|=m. The association weight, soft-cluster weight, of each task ito a cluster jis represented
bywi,j, with the constraint thatPm
j=1wi,j= 1;Wj, WL jare the weight vector and weighted loss
of all the tasks in cluster jrespectively, while Ljsimply represents the resulting loss of model j
denoting cluster j. Notice that the values of wi,jrange from 0to1, where 1signifies full membership
and0indicates no membership.
MTL Architecture Following the previous approaches [ 9,30,29], we utilize a commonly employed
hard-sharing multi-head architecture (Figure 1) for all our MTL experiments, where a single feature
extractor is used to obtain shared representations, and separate task-specific heads are employed
to output the result. Additionally, for all the experiments, we maintain the same data splits, via
prior seeding, and keep the optimization algorithm and other hyperparameters fixed; this is to make
sure any variability in the performance is only attributed to the task grouping and the corresponding
weights if any.
3.2 Data Maps as Task Features
Figure 2: An example of a generated data map
for the “Living being” task after 21 epochs of co-
training on 15 tasks of G2 (Section 4.1)Data Maps [ 33] has been originally developed
as a model-based tool for characterizing and
diagnosing NLP datasets. We re-propose us-
ing them as task features during training in an
MTL settings given their ability to capture the
model behavior of each individual training data
point with respect to each task, which is a valu-
able characterization to the task itself. For that
reason, we employ them, in our work, as task
features due to their simplicity, scalability, and
ability to extract them on the fly without prior
knowledge of the model architecture, thus en-
hancing the modularity of our approach.
The concept behind Data Maps revolves briefly
around extracting two values for each data point:
themodel confidence (µ) of the true class, which
is the average probability of the true class over
the epochs, and the variability (σ) of this con-
fidence, which is the standard deviation of the
true class probabilities over the same epochs. For a particular task, the data map shape is (N,2).
Figure 2 shows a visualization of the resulting Data Map for an example task extracted from CIFAR10
dataset [17].
Because their information is very task-dependent, we thought they can serve as task descriptors. To
further enhance the expressiveness of the extracted features, we also extract data maps at various
epochs, allowing us to gain insights into their evolution over time; the resulting shape in such case is
(n, e, N, 2). Therefore, by analyzing their characteristics, over the different epochs during training,
we can capture crucial information about the relatedness of each task.
In the extraction of data maps, we could have used two approaches. The first approach involves
building a single MTL model that incorporates all tasks and extracting the data maps directly from
this unified model. Alternatively, we can utilize the second approach, where individual models are
constructed for each task, resulting in multiple STLs, and merging the data maps obtained from each
model. Our results are primarily based on the first approach, as it offers the advantage of using a
single training model, simplifying computational complexity, enhancing scalability, and streamlining
experimentation, while having a comparable results to STL, as shown in Section 4.4.
4

--- PAGE 5 ---
3.3 Task Clustering
With the extracted data maps in hand, our next step is to group the tasks into clusters based on their
similarity. We propose using soft clustering to obtain class membership weights, wi,j. To obtain them,
we represent each task as a vector by flattening the concatenated data maps, extracted at different
epochs. We then employ the k-means algorithm [ 22] to cluster these task vectors, aiming to identify
distinct clusters of tasks, into hard clusters. To introduce a more nuanced representation of task
similarities, we incorporate the fuzzification step [ 3] into our approach, yet we modify it for better
computational efficiency, equation 2. This enables soft clustering, where xirepresents the ithtask
vector of the corresponding data maps and F > 1represents the fuzzification index. This fuzzification
process assigns soft memberships to tasks, allowing for more flexible and comprehensive clustering
results. We predominantly rely on the soft clustering approach due to its effectiveness and reliability
in addition to allowing model specialization via loss weighting, as explained in Section 3.4.
wi,j=1
Pn
k=1
∥xi−cj∥
∥xi−ck∥2
F−1(1)
=∥xi−cj∥−2
F−1
Pn
k=1(∥xi−ck∥)−2
F−1(2)
3.4 Model Specialization through Loss Weighting
Input
Model 1 Model 2 Model 
Model 1 Output Model 2 Output Model  Output
Final Output
Figure 3: The procedure to use our specialized trained models to infer the results
In order to assess the effectiveness of our task grouping results, we use loss weighting as a method
of model specialization. We construct MTL models that are tailored to specialize in specific sets
of tasks based on the membership weights obtained from soft clustering results. For each cluster,
we build an individual MTL model that focuses on the tasks assigned to that cluster according to
their corresponding weights (Equation 3). To evaluate the performance of our solution, we apply
the weighted average of the models’ outputs according to the membership weights as in Equation 4,
where Ojis the output of the jthmodel and ◦represents Hadamard, element-wise, product. Figure
3 provides an overview of these operations while inferring the output. By comparing the resulting
values with both STL and traditional MTL schemes in Section 4.4, we can gain insights into the
benefits and improvements brought by our task grouping approach.
WLj=WT
jLj (3)
Final Output =mX
j=1Wj◦Oj (4)
5

--- PAGE 6 ---
Table 1: Comparison of asymptotic growth of the required number of trained MTL models to get task
grouping of various methods and ours
Method Number of Models (↓)
Exhaustive Search Θ (2n)
HOA Θ  n
2
= Θ 
n2
TAG∗Θ 
n2
MTG-Net Θ (n·K)
STG-MTL Θ (1)
∗TAG originally requires one model, yet we
do n
2
updates to compute the inter-task affinities.
For that reason a single training model of TAG is
asymptotically equivalent to training Θ(n2)mod-
els from other approaches.
3.5 Theoretical Scalability Comparison
In the theoretical scalability comparison, we evaluate our method against existing literature, focusing
on the required number of trained models to get the clustering results. Table 1 presents the comparison,
where lower numbers indicate better scalability. Our approach stands out with excellent scalability,
as it only necessitates training a single MTL model to extract data maps and perform clustering, or
evenO(n)if we consider extracting data maps from nSTL models. This offers the most promising
scalability potential for a larger number of tasks. That is why we can scale our experiments to a
very large number of tasks as in Section 4. We also highlight the note of Table 1 that although TAG
requires a single model to extract the groupings, we compute the gradients Θ(n2), n
2
in particular,
times than usual to compute the inter-task affinities pairs. Therefore, one model training of TAG
utilizes the same asymptotic time of Θ(n2)normally trained models.
Furthermore, our method’s data map computation is performed on the fly, making it both model and
task agnostic. This feature enhances the modularity of our approach, enabling effortless adaptation to
different model architectures and tasks without manual intervention. Refer to Appendix B for further
discussion on the modularity aspect of our method.
4 Experiments
In this section, we present a comprehensive overview of our experiments, focusing on assessing the
effectiveness of our method and presenting the corresponding results. As outlined in Section 3.5, the
methods in literature has severe scalability issues. This makes a direct comparison of performance
across all methods unfeasible, considering their poor scalability, the substantial number of tasks
involved in our evaluation, and our limited computational power. To illustrate, the clustering results
of just 15tasks for HOA necessitate the training of 15
2
= 105 models. Therefore, we evaluate our
method against both the MTL and STL results to showcase the added information gained from our
clustering results.
Section 4.1 outlines the specifics of the datasets utilized in our experimentation, as well as the tasks
employed. In Section 4.2, we delve into the details of the model architecture and the hyperparameters
used during experimentation. The outcomes of the soft clustering of tasks are presented in Section
4.3, where we highlight the effectiveness of our approach in grouping related tasks. Finally, in
Section 4.4, we evaluate the quality of the obtained clustering results comparing them to STL and
MTL results. Appendix B illustrates our modular pipeline structure for all the experiments for easier
reproducibility.
4.1 Datasets and Tasks
Our task generation is based on the CIFAR10 and CIFAR100 datasets [ 17] along with CelebA dataset
[21]. We mainly use them due to their public availability and the available high number of tasks we
could generate from them. We show the results of four groups of tasks in our experiments. For all of
these datasets, we define binary tasks to detect whether the input image belongs to a particular label
6

--- PAGE 7 ---
or not. In Group 1 ( G1), for example, we include binary classification tasks that determine whether
an image belongs to CIFAR10 labels. G1 consists of 10 tasks: {airplane, automobile, bird, cat, deer,
dog, frog, horse, ship, truck }. That is the “airplane” task is to detect whether the image is an airplane
or not.
Group 2 ( G2) expands on G1 by introducing additional tasks on CIFAR10, designed for testing
qualitatively the method behavior. These tasks include {Living being, Odd-numbered, Downside,
Not living being, random }. The “Living being” task aims to detect whether an image contains a
living being, which includes images with the labels {bird, cat, deer, dog, frog, horse }. Similarly, the
“Not living being” task focuses on identifying non-living beings; these are {airplane, automobile,
ship, truck }classes in CIFAR10. Notably, “Living being” and “Not living being” are intentionally
designed to be similar tasks to check if the method can detect such similarity. The “Odd-numbered”
task identifies whether the label of a CIFAR10 image is odd or not, encompassing {automobile, cat,
dog, horse, truck }classes. Additionally, we flip half of CIFAR10 images and create a task to train the
model to recognize vertically flipped images, the “Downside” task. Lastly, the “random” task assigns
random binary labels to the entire dataset with a predefined seed for consistency and reproducibility;
the “random” task is added to represent extremely difficult task, for example. It is worth mentioning
that while the original tasks in G1 are imbalanced, the extra tasks in G2 are balanced.
Group 3 ( G3), similar to G1, consists of 100binary classification tasks using the CIFAR100 labels.
We also utilize the 20super labels of CIFAR100 as our ground truth for task clustering evaluation. It
is worth mentioning that CIFAR100 super labels are not intended for task grouping, so they are not
grouped based on visual similarities like our method’s objective. Instead, they are mostly clustered
semantically, even though there are some exceptions like mushrooms and the classes of vehicles 1
and 2. Still, we think they serve as an informative indicator of the effectiveness of our approach,
especially in the visually coherent superclasses. Similarly, we define group 4 ( G4) to include the 40
binary classification tasks included in CelebA.
4.2 Model Architectures and Hyper-Parameters
For all our experiments, we adopt the RESNET18 architecture [ 10] as our base model. Our method is
model-agnostic, so we have also experimented with simpler models, results & archtecture discussed
in Appendix A.1, yet we use RESNET18 considering its moderate model capacity. Furthermore, we
utilize it without any pre-training, ensuring that the model starts from scratch for each task grouping
scenario. The last fully connected layer of RESNET18 serves as the task heads, with the number of
output neurons corresponding to the number of tasks. Each neuron in the task heads represents a
specific classification task. Throughout our experiments, the rest of the network, excluding the task
heads, is shared among all tasks. Also, we train the model for 50epochs in all our experiments: to
extract data maps and to evaluate the models. Additionally, in our clustering process, we primarily
set the fuzzification index ( F) to2. The fuzzification index controls the level of fuzziness in the soft
clustering algorithm, so increasing it produce softer decisions.
In terms of the loss function, we utilize Binary Cross Entropy as the binary classification loss for our
tasks. However, to address the issue of task imbalance, we incorporate a penalty on positive instances
for each task. By applying this penalty, we ensure that the model pays more attention to the minority
label during training, thereby mitigating the impact of the imbalance and promoting better overall
performance. Finally, it worth mentioning that we do not perform any kind of tuning to any model.
We use the same basic settings in all of our experiments.
4.3 Task Clustering Results
Results of our task clustering experiments are presented for all groups. We initially experimented
on G2, generating their data maps as described in Section 3.2 and Clustering them as in Section 3.3,
as depicted in Figure 4. Notably, our method successfully clustered the “random” task separately,
indicating its dissimilarity to the other tasks. Furthermore, throughout all our experiments, the tasks
“Living being” and “Not living being” consistently exhibited the same membership distribution ,
which is reasonable considering their equivalence.
Moreover, when focusing solely on the first 10 tasks from G2 without any additional tasks, our
clustering algorithms demonstrated some semantic clustering capabilities, as shown in Figure 4b.
The algorithm successfully grouped images of living beings, including {bird, cat, deer, dog, frog,
7

--- PAGE 8 ---
(a) G2: 2clusters
(b) G2: 3clusters
(c) G1: 2clusters
 (d) G1: 3clusters
Figure 4: Task grouping of G1 & G2 with F= 2
horse}, while another group consisted of images of non-living beings such as {airplane, automobile,
ship, truck }. Nevertheless, this might be due to the impact of the “Living being” and “Not living
being” tasks; we therefore conducted a similar experiment on G1, generating their data maps and
clustering the tasks, without any extra tasks.
As illustrated in Figure 4c, even without additional tasks, our method performed the same reasonable
clustering for G1, grouping living beings together and non-living things together. Additionally,
Figure 4d demonstrates the clustering using three clusters, revealing that the living being cluster
was divided into two groups: cluster 1and cluster 2. Cluster 1predominantly contained quadruped
animals {cat, dog, horse }, while cluster 2included {bird, frog, deer }that represented the other living
creatures except for the deer. These results showcase the effectiveness of our clustering algorithm in
capturing semantic, visual similarities among tasks based on the visual data leading to meaningful
task groupings.
In addition to our experiments on G1 and G2, we conducted a comprehensive evaluation of an
unprecedented number of tasks, specifically 100tasks from CIFAR100, in G3. As part of this
evaluation, we compared our task clustering results against the predefined superclasses provided by
CIFAR100. It is important to note that the superclasses in CIFAR100 primarily rely on semantic
relationships as illustrated in Section 4.1, so we show the results of people and flowers, as examples.
In Figure 5, we showcase an example of the clustering results for a group of super tasks. It is note-
worthy that our method successfully clusters certain groups of tasks in alignment with the predefined
CIFAR100 superclasses, as illustrated in Figure 5a. However, it is important to acknowledge that
there are cases where the clustering may not be perfect, as depicted in Figure 5b; we think this
is primarily because our method focus one visual similarities, which is exploited during training
rather than semantics. Nevertheless, even in such instances, our clustering algorithm manages to
allocate significant weights of all tasks into distinctive clusters, such as clusters 0and8in Figure 5b.
8

--- PAGE 9 ---
(a) Membership clustering of People superclass
(b) Membership clustering of Flowers superclass
Figure 5: Task grouping of G3 (100 Tasks of CIFAR100) into 20clusters with F= 2
(a) Training Results (G1)
(b) Test Results (G1)
(c) Training Results (G2)
(d) Test Results (G2)
(e) Training Results (G3)
(f) Test Results (G3)
(g) Training Results (G4)
(h) Test Results (G4)
Figure 6: Average F1 Scores of all the Groups on both the training and test sets
Notably, in cluster 8, the participation percentages of the tasks {orchid, poppy, rose, tulip }are the
2ndhighest across all clusters, indicating a close relationship with the missclassified task sunflower,
yet our method suggests that the other four tasks are more visually related. We further discuss all
the clustering results of the 100 Tasks in Appendix A for RESNET18; we also show the clustering
results of simpler CNN model in Appendix A.1, which shows the possible potential of transferring
the results generated from simpler models to other models.
4.4 Evaluation Analysis
To further validate the effectiveness of our method, we conducted a comprehensive evaluation as
described in Section 3.4 on all task groups. Figure 6 presents the average F1 score for both the training
and test sets of all the three sets of tasks; we use F1 as our evaluation metric becuase many tasks are
imbalanced, so we use F1 score to adhere to such issue. Our method is denoted by STG-MTL xxC
(F=2) where xxrepresents the number of clusters. The MTL curve represents the results obtained
from training an MTL model on all tasks without any grouping, while the STL curve represents
the results obtained by training separate STL models for each task and merging their outputs. We
9

--- PAGE 10 ---
(a) MTL vs STG Results
(G2)
(b) Comparing Data Map
Sources (G2)
(c) Different Fusing 2
clusters (G2)
(d) Different Fusing 3
clusters (G2)
(e) MTL vs STG vs STL
Results (G3)
(f) Comparing Data Map
Sources (G3)
(g) MTL vs STG vs STL
Results (G1)
Figure 7: Average F1 score curves on test set generated using the custom CNN
compare the performance of our method against the MTL and STL approaches in both G1 & G2 and
against the MTL approach only in G3 & G4 because the STL performance is poorer than the MTL,
as it overfits.
We also present additional experiments, figure 7 using a simpler model with much less number of
parameters, of a custom CNN architecture depicted in Table 2, in Appendix A.1. These experiments
demonstrate the robustness and adaptability of our method across different model architectures. It is
worth mentioning that even with hard clustering , our method outperforms standard MTL, showcasing
its effectiveness in leveraging task grouping for enhanced performance as shown in Figures 7e and
7g. Moreover, the results obtained using our approach with data maps extracted from the MTL model
are comparable to those from the STL models as in Figures 7b and 7f.
Overall, our method consistently outperforms both the MTL and STL approaches, indicating that the
task grouping provides valuable information for improving task performance. Notably, although our
method tends to overfit, in RESNET18 experiments, and achieves excellent training performance,
it also achieves the best performance on the test set. This suggests that if the models were further
fine-tuned, even greater gains could be achieved, yet we refrain from tuning any of the models in this
study to guarantee fairness in comparison.
5 Conclusion and Future Work
In conclusion, we have presented STG-MTL, which is a novel scalable approach for task grouping in
multi-task learning (MTL) settings. Our method utilizes data maps [ 33] to identify task similarities
and group them accordingly. We showed its superior scalability theoretically in comparison to TAG
[9], HOA [ 30], and MTG-Net [ 29]. We have also demonstrated the effectiveness of our method
through our experiments on CIFAR10 & CIFAR100 [ 17] and CelebA [ 21] datasets , where we
pushed the boundaries by experimenting with 100 tasks , which has never been done before in the
literature proving its scalability. We have also compared our clustering results against the predefined
superclasses in CIFAR100, further validating the effectiveness of our approach. Nevertheless due to
our limited computational power and the poor scalability of the current methods, we were not able
to evaluate the performance of our results against the other methods. Instead, we showed that our
method outperformed both traditional MTL and single-task learning (STL) approaches, showcasing
the quality of task grouping and its ability to improve multi-task learning performance.
For future work, we aim to explore Data Maps’ generalization to other task types, such as regression,
because they are currently limited to classification tasks only. Additionally, we hope our research
could open a new research direction in the MTL community to explore the development of new
features that can capture the training dynamics efficiently, other than data maps. By advancing this
10

--- PAGE 11 ---
research direction, we can unlock new possibilities for enhancing performance and driving further
advancements in the field of MTL.
Acknowledgments We gratefully thank the Fatima Fellowship3for supporting this research especially
during its early stages.
References
[1]V . Aribandi, Y . Tay, T. Schuster, J. Rao, H. S. Zheng, S. V . Mehta, H. Zhuang, V . Q. Tran,
D. Bahri, J. Ni, et al. Ext5: Towards extreme multi-task scaling for transfer learning. In
International Conference on Learning Representations , 2021.
[2]G. Bao, H. Chen, T. Liu, G. Gong, Y . Yin, L. Wang, and X. Wang. Covid-mtl: Multitask
learning with shift3d and random-weighted loss for covid-19 diagnosis and severity assessment.
Pattern Recognition , 124:108499, 2022.
[3]J. C. Bezdek, R. Ehrlich, and W. Full. Fcm: The fuzzy c-means clustering algorithm. Computers
& geosciences , 10(2-3):191–203, 1984.
[4]S. Bickel, J. Bogojeska, T. Lengauer, and T. Scheffer. Multi-task learning for hiv therapy
screening. In Proceedings of the 25th international conference on Machine learning , pages
56–63, 2008.
[5] R. Caruana. Multitask learning. Machine learning , 28:41–75, 1997.
[6]Z. Chen, Y . Shen, M. Ding, Z. Chen, H. Zhao, E. G. Learned-Miller, and C. Gan. Mod-squad:
Designing mixtures of experts as modular multi-task learners. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pages 11828–11837, 2023.
[7]M. Crawshaw. Multi-task learning with deep neural networks: A survey. arXiv preprint
arXiv:2009.09796 , 2020.
[8]J. Fan, T. Zhao, Z. Kuang, Y . Zheng, J. Zhang, J. Yu, and J. Peng. Hd-mtl: Hierarchical deep
multi-task learning for large-scale visual recognition. IEEE transactions on image processing ,
26(4):1923–1938, 2017.
[9]C. Fifty, E. Amid, Z. Zhao, T. Yu, R. Anil, and C. Finn. Efficiently identifying task groupings
for multi-task learning. Advances in Neural Information Processing Systems , 34:27503–27516,
2021.
[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pages 770–
778, 2016.
[11] H. Huang, D. Ye, L. Shen, and W. Liu. Curriculum-based asymmetric multi-task reinforcement
learning. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.
[12] S. Huang, X. Li, Z.-Q. Cheng, Z. Zhang, and A. Hauptmann. Gnas: A greedy neural architecture
search method for multi-attribute learning. In Proceedings of the 26th ACM international
conference on Multimedia , pages 2049–2057, 2018.
[13] Z. Huang, M. Rao, A. Raju, Z. Zhang, B. Bui, and C. Lee. Mtl-slt: multi-task learning for
spoken language tasks. In Proceedings of the 4th Workshop on NLP for Conversational AI ,
pages 120–130, 2022.
[14] G. Ilharco, M. T. Ribeiro, M. Wortsman, L. Schmidt, H. Hajishirzi, and A. Farhadi. Editing mod-
els with task arithmetic. In The Eleventh International Conference on Learning Representations ,
2023. URL https://openreview.net/forum?id=6t0Kwf8-jrj .
[15] M. Islam, V . Vibashan, C. M. Lim, and H. Ren. St-mtl: Spatio-temporal multitask learning
model to predict scanpath while tracking instruments in robotic surgery. Medical Image Analysis ,
67:101837, 2021.
3https://www.fatimafellowship.com/
11

--- PAGE 12 ---
[16] N. Jin, J. Wu, X. Ma, K. Yan, and Y . Mo. Multi-task learning model based on multi-scale cnn
and lstm for sentiment classification. IEEE Access , 8:77060–77072, 2020.
[17] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.
[18] G. Lee, E. Yang, and S. Hwang. Asymmetric multi-task learning based on task relatedness and
loss. In International conference on machine learning , pages 230–238. PMLR, 2016.
[19] H. B. Lee, E. Yang, and S. J. Hwang. Deep asymmetric multi-task feature learning. In
International Conference on Machine Learning , pages 2956–2964. PMLR, 2018.
[20] C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei, A. Yuille, J. Huang, and
K. Murphy. Progressive neural architecture search. In Proceedings of the European conference
on computer vision (ECCV) , pages 19–34, 2018.
[21] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In Proceedings
of International Conference on Computer Vision (ICCV) , December 2015.
[22] S. Lloyd. Least squares quantization in pcm. IEEE transactions on information theory , 28(2):
129–137, 1982.
[23] M. Long, Z. Cao, J. Wang, and P. S. Yu. Learning multiple tasks with multilinear relationship
networks. Advances in neural information processing systems , 30, 2017.
[24] K.-K. Maninis, I. Radosavovic, and I. Kokkinos. Attentive single-tasking of multiple tasks. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages
1851–1860, 2019.
[25] I. Misra, A. Shrivastava, A. Gupta, and M. Hebert. Cross-stitch networks for multi-task learning.
InProceedings of the IEEE conference on computer vision and pattern recognition , pages
3994–4003, 2016.
[26] Y . Peng, Q. Chen, and Z. Lu. An empirical study of multi-task learning on BERT for biomedical
text mining. In D. Demner-Fushman, K. B. Cohen, S. Ananiadou, and J. Tsujii, editors,
Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing , pages
205–214, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/
2020.bionlp-1.22. URL https://aclanthology.org/2020.bionlp-1.22 .
[27] S. Ruder, J. Bingel, I. Augenstein, and A. Søgaard. Latent multi-task architecture learning. In
Proceedings of the AAAI Conference on Artificial Intelligence , volume 33, pages 4822–4829,
2019.
[28] O. Sener and V . Koltun. Multi-task learning as multi-objective optimization. Advances in neural
information processing systems , 31, 2018.
[29] X. Song, S. Zheng, W. Cao, J. Yu, and J. Bian. Efficient and effective multi-task grouping via
meta learning on task combinations. In Advances in Neural Information Processing Systems ,
2022.
[30] T. Standley, A. Zamir, D. Chen, L. Guibas, J. Malik, and S. Savarese. Which tasks should be
learned together in multi-task learning? In International Conference on Machine Learning ,
pages 9120–9132. PMLR, 2020.
[31] G. Strezoski, N. van Noord, and M. Worring. Learning task relatedness in multi-task learning
for images in context. In Proceedings of the 2019 on international conference on multimedia
retrieval , pages 78–86, 2019.
[32] X. Sun, R. Panda, R. Feris, and K. Saenko. Adashare: Learning what to share for efficient deep
multi-task learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors,
Advances in Neural Information Processing Systems , volume 33, pages 8728–8740. Curran
Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/
2020/file/634841a6831464b64c072c8510c7f35c-Paper.pdf .
12

--- PAGE 13 ---
[33] S. Swayamdipta, R. Schwartz, N. Lourie, Y . Wang, H. Hajishirzi, N. A. Smith, and Y . Choi.
Dataset cartography: Mapping and diagnosing datasets with training dynamics. In B. Webber,
T. Cohn, Y . He, and Y . Liu, editors, Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP) , pages 9275–9293, Online, Nov. 2020. Association
for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.746. URL https://
aclanthology.org/2020.emnlp-main.746 .
[34] S. Vandenhende, S. Georgoulis, B. De Brabandere, and L. Van Gool. Branched multi-task
networks: Deciding what layers to share. 2019. URL https://openreview.net/forum?
id=HJxhUpVKDr .
[35] S. Wu, H. R. Zhang, and C. Ré. Understanding and improving information transfer in multi-task
learning. In International Conference on Learning Representations , 2019.
[36] C. Yu, Z. Gao, W. Zhang, G. Yang, S. Zhao, H. Zhang, Y . Zhang, and S. Li. Multitask learning
for estimating multitype cardiac indices in mri and ct based on adversarial reverse mapping.
IEEE Transactions on Neural Networks and Learning Systems , 32(2):493–506, 2021. doi:
10.1109/TNNLS.2020.2984955.
[37] C. Yu, H. Liu, and H. Zhang. Distilling sub-space structure across views for cardiac indices
estimation. Medical Image Analysis , 85:102764, 2023. ISSN 1361-8415. doi: https://doi.org/10.
1016/j.media.2023.102764. URL https://www.sciencedirect.com/science/article/
pii/S1361841523000257 .
[38] L. Zhang, X. Liu, and H. Guan. A tree-structured multi-task model recommender. In First
Conference on Automated Machine Learning (Main Track) , 2022. URL https://openreview.
net/forum?id=BEl4CgaHLlc .
[39] Y . Zhang and Q. Yang. A survey on multi-task learning. IEEE Transactions on Knowledge and
Data Engineering , 34(12):5586–5609, 2022. doi: 10.1109/TKDE.2021.3070203.
[40] Z. Zhang, B. Wu, and B. Schuller. Attention-augmented end-to-end multi-task learning for
emotion prediction from speech. In ICASSP 2019-2019 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP) , pages 6705–6709. IEEE, 2019.
[41] Z. Zhang, W. Yu, M. Yu, Z. Guo, and M. Jiang. A survey of multi-task learning in natural
language processing: Regarding task relatedness and training methods. In A. Vlachos and
I. Augenstein, editors, Proceedings of the 17th Conference of the European Chapter of the
Association for Computational Linguistics , pages 943–956, Dubrovnik, Croatia, May 2023.
Association for Computational Linguistics. doi: 10.18653/v1/2023.eacl-main.66. URL https:
//aclanthology.org/2023.eacl-main.66 .
13

--- PAGE 14 ---
(a) aquatic mammals
 (b) fish
 (c) flowers
(d) food containers
 (e) fruit and vegetables
 (f) household electrical devices
(g) household furniture
 (h) insects
 (i) large carnivores
(j) large man-made outdoor things
 (k) large natural outdoor scenes
 (l) large omnivores and herbivores
(m) medium-sized mammals
 (n) non-insect invertebrates
 (o) people
(p) reptiles
 (q) small mammals
 (r) trees
(s) vehicles 1
 (t) vehicles 2
Figure 8: Task grouping of G3 (100 Tasks of CIFAR100) into 20clusters with F= 2using Data
Maps from RESNET18
A Task Clustering Results of 100 Tasks
In this appendix, we present detailed insights into the task clustering results of 100 tasks, building
upon the experimental setup outlined in Section 4.3. Figure 8 showcases the clustering results using
20clusters, while Figure 9 illustrates the results with 10clusters. Each image in the figures represents
the clustering outcomes for one superclass from CIFAR100.
Notably, the clustering with 20clusters demonstrates successful grouping in many categories, such
as{People, Trees, Food Containers, Flowers, Household Electrical Devices }. Figures 8s and 8t
highlight the close association between Vehicles 1 and 2, as they are almost merged into the same
cluster (Cluster 15). When reducing the number of clusters to 10, we observe enhanced coherence
in the assigned tasks. For instance, the tasks related to “Fruit and Vegetables” are nearly clustered
together after reducing the number of clusters.
Moreover, our method effectively captures the logical associations between categories. Figures 9f and
9g showcase the plausible merge between “Household Electrical Devices” and “Household Furniture.”
Additionally, we observe similar distribution patterns among related categories, such as {Large
Carnivores, Large Omnivores and Herbivores, Medium-Sized Mammals }in Figures 9i, 9l, and 9m
respectively.
Overall, these results reveal the qualitative success of our approach in clustering a very large number
of tasks, highlighting the effectiveness of our method even in such challenging scenarios.
14

--- PAGE 15 ---
(a) aquatic mammals
 (b) fish
 (c) flowers
(d) food containers
 (e) fruit and vegetables
 (f) household electrical devices
(g) household furniture
 (h) insects
 (i) large carnivores
(j) large man-made outdoor things
 (k) large natural outdoor scenes
 (l) large omnivores and herbivores
(m) medium-sized mammals
 (n) non-insect invertebrates
 (o) people
(p) reptiles
 (q) small mammals
 (r) trees
(s) vehicles 1
 (t) vehicles 2
Figure 9: Task grouping of G3 (100 Tasks of CIFAR100) into 10clusters with F= 2using Data
Maps from RESNET18
A.1 Robust Clustering using Less Complex Architectures
15

--- PAGE 16 ---
(a) aquatic mammals
 (b) fish
 (c) flowers
(d) food containers
 (e) fruit and vegetables
 (f) household electrical devices
(g) household furniture
 (h) insects
 (i) large carnivores
(j) large man-made outdoor things
 (k) large natural outdoor scenes
 (l) large omnivores and herbivores
(m) medium-sized mammals
 (n) non-insect invertebrates
 (o) people
(p) reptiles
 (q) small mammals
 (r) trees
(s) vehicles 1
 (t) vehicles 2
Figure 10: Task grouping of G3 (100 Tasks of CIFAR100) into 20clusters with F= 2using Data
Maps from the Custom CNN
Table 2: The custom CNN archtecture used in
our experiments. nis the number of tasks.
Layer Kernel Size Output
Convolution 5×5 32 channels
2D BN + ReLU
2D MaxPool 2×2
Convolution 3×3 64 channels
2D BN + ReLU
Convolution 3×3 64 channels
2D BN + ReLU
2D MaxPool 2×2
Flatten
FC 128
FC nTo assess the robustness and generalization poten-
tial of our approach, we conducted a similar exper-
iment using a simpler CNN architecture. The ar-
chitecture’s details are provided in Table 2, where
n= 100 . Notably, the CNN architecture used for
extracting the data maps in our clustering process
comprises only 472,612learnable parameters. In
contrast, the ResNet18 architecture, from which
we initially derived our clustering results, consists
of11,227,712learnable parameters.
Figure 10 and 11 showcase the task clustering re-
sults obtained using the simplified CNN architec-
ture. Surprisingly, the resulting clustering patterns
from our custom CNN are remarkably comparable
to those achieved with ResNet18. This observa-
tion underscores the method’s expressiveness, as
it effectively captures task relationships without
necessitating a high number of learnable parameters.
This intriguing finding opens the possibility of extrapolating clustering outcomes from less expressive
models to more complex ones. However, further investigations are required to ascertain the extent
and implications of this approach.
16

--- PAGE 17 ---
(a) aquatic mammals
 (b) fish
 (c) flowers
(d) food containers
 (e) fruit and vegetables
 (f) household electrical devices
(g) household furniture
 (h) insects
 (i) large carnivores
(j) large man-made outdoor things
 (k) large natural outdoor scenes
 (l) large omnivores and herbivores
(m) medium-sized mammals
 (n) non-insect invertebrates
 (o) people
(p) reptiles
 (q) small mammals
 (r) trees
(s) vehicles 1
 (t) vehicles 2
Figure 11: Task grouping of G3 (100 Tasks of CIFAR100) into 10clusters with F= 2using Data
Maps from the Custom CNN
17

--- PAGE 18 ---
T rainer
+ train
+ evaluate
+ evaluate_ensemble
+ get_data_maptorch.nn.Module
BaseModel
# _intialize_metrics_generator
+ get_task_output
+ get_probabilities_true_target
+ initialize_losses
+ forwardBaseMetricsDataMapRecorder
+ update
+ reset
+ get_data_map
BaseDataModule
# _initialize
+ get_train_dataloader
+ get_val_dataloader
+ get_test_dataloaderBaseDataset
- __len__
- __getitem__torch.utils.data.Dataset
C100DataModuleC100ModelFigure 12: UML class diagram of our pipeline
B Modularity in the Code Structure
In designing our method, we placed significant emphasis on scalability and modularity. Ensuring
scalability without requiring substantial modifications whenever model architecture or tasks change,
code modularity, is crucial. To achieve this, we designed a modular pipeline that facilitates easy
adaptation to various scenarios with minimal adjustments. The class diagram for our pipeline is
depicted in Figure 12.
Our code architecture is composed of several classes, each with distinct roles. The main class
responsible for training models on datasets is the Trainer . Notably, the Trainer employs an
object from the DataMapRecorder class, which dynamically updates and records data maps after
each iteration. To enhance reusability, we introduce an abstract class called BaseModel , which
serves as the blueprint for all models. Similarly, the Trainer expects a BaseDataModule as
input, an abstract class encompassing dataset information and instructions for building data loaders.
DatamapT askClustering
+ cluster
+ plot_membership
+ get_task_membershipClusteringT ypesklearn.cluster .KMeans
FuzzyKMeans
Figure 13: UML class diagram of task clustering
classesWhen dealing with new sets of tasks, two
steps are necessary. First, we inherit from
BaseModel , implementing its abstract meth-
ods, italicized in the figure. Second, we
provide concrete implementations for the
BaseDataModule ’s abstract methods. Our code
contains examples of these two steps for differ-
ent datasets such as CIFAR10, CIFAR100, and
CelebA.
To train models and extract data maps, only a
few lines of code are required, exemplified in Code 1. The resulting data map from training an MTL
model is then ready for clustering using our DatamapTaskClustering class, depicted in Figure 13,
as shown in Code 2. This design simplifies the integration process for new tasks, models, or datasets,
effectively enhancing the overall modularity of our pipeline and method.
Code 1: Start training and record the data maps
# c r e a t e an i n s t a n c e o f t h e dat a module
data_module = C100DataModule ( . . . )
# Now i n i t i a l i z e a model
model = C100Model ( . . . )
# F i n a l l y , i n i t i a l i z e a t r a i n e r o b j e c t
t r a i n e r = T r a i n e r ( model , data_module , a r g s = args , . . . )
# s t a r t t r a i n i n g
t r a i n e r . t r a i n ( )
Code 2: Generate task memberships from data maps
18

--- PAGE 19 ---
# To f u r t h e r s e e t h e t a s k membership , we do t h e below s t e p s
# g e t t h e datamap from t h e t r a i n e r
dm_mtl = t r a i n e r . get_data_map ( )
# i n i t i a l i z e a c l u s t e r e s t i m a t o r
c l u s t e r _ e s t i m a t o r = D a t a m a p T a s k C l u s t e r i n g ( dm_mtl , n _ c l u s t e r s , . . . )
# g e t t h e r e s u l t s
m t l _ t a s k _ w e i g h t s = c l u s t e r _ e s t i m a t o r . c l u s t e r ( . . . )
19
