# Cân bằng khoảng cách có thể cải thiện cho học tập đa nhiệm

Yanqi Dai1,3 Nanyi Fei2,3 Zhiwu Lu1,3
1Trường Trí tuệ nhân tạo Gaoling, Đại học Nhân dân Trung Quốc, Bắc Kinh, Trung Quốc
2Trường Thông tin, Đại học Nhân dân Trung Quốc, Bắc Kinh, Trung Quốc
3Phòng thí nghiệm trọng điểm Bắc Kinh về Quản lý và Phân tích Dữ liệu lớn, Bắc Kinh, Trung Quốc

## Tóm tắt

Trong học tập đa nhiệm (MTL), cân bằng gradient gần đây đã thu hút nhiều sự quan tâm nghiên cứu hơn cân bằng mất mát vì nó thường dẫn đến hiệu suất tốt hơn. Tuy nhiên, cân bằng mất mát hiệu quả hơn nhiều so với cân bằng gradient, và do đó vẫn đáng được khám phá thêm trong MTL. Lưu ý rằng các nghiên cứu trước đây thường bỏ qua việc tồn tại các khoảng cách có thể cải thiện khác nhau giữa nhiều nhiệm vụ, trong đó khoảng cách có thể cải thiện cho mỗi nhiệm vụ được định nghĩa là khoảng cách giữa tiến trình huấn luyện hiện tại và tiến trình huấn luyện cuối cùng mong muốn. Do đó, sau khi cân bằng mất mát, sự mất cân bằng hiệu suất vẫn xuất hiện trong nhiều trường hợp. Trong bài báo này, theo khuôn khổ cân bằng mất mát, chúng tôi đề xuất hai thuật toán cân bằng khoảng cách có thể cải thiện (IGB) mới cho MTL: một thuật toán sử dụng heuristic đơn giản, và thuật toán khác (lần đầu tiên) triển khai học tăng cường sâu cho MTL. Đặc biệt, thay vì cân bằng trực tiếp các mất mát trong MTL, cả hai thuật toán đều chọn gán trọng số nhiệm vụ động để cân bằng khoảng cách có thể cải thiện. Hơn nữa, chúng tôi kết hợp IGB và cân bằng gradient để thể hiện tính bổ sung giữa hai loại thuật toán này. Các thí nghiệm rộng rãi trên hai bộ dữ liệu benchmark chứng minh rằng các thuật toán IGB của chúng tôi đạt được kết quả tốt nhất trong MTL thông qua cân bằng mất mát và đạt được những cải thiện thêm khi kết hợp với cân bằng gradient. Mã nguồn có sẵn tại https://github.com/YanqiDai/IGB4MTL.

## 1 GIỚI THIỆU

Học tập đa nhiệm (MTL) là việc huấn luyện chung một mô hình duy nhất có thể thực hiện nhiều nhiệm vụ [Caruana, 1998, Ruder, 2017, Zhang và Yang, 2021, Vandenhende và cộng sự, 2021]. So với học tập đơn nhiệm (STL), MTL có hai lợi thế đáng chú ý: 1) mô hình thường có kích thước nhỏ hơn và hiệu quả học tập cao hơn bằng cách chia sẻ tham số giữa các nhiệm vụ [Misra và cộng sự, 2016, Yang và cộng sự, 2020, Vandenhende và cộng sự, 2021], và 2) hiệu suất trên một số nhiệm vụ có thể được cải thiện thêm do sự tương quan giữa các nhiệm vụ khác nhau [Swersky và cộng sự, 2013]. Do đó, MTL đã được sử dụng rộng rãi trong các tình huống ứng dụng thực tế như hệ thống gợi ý [Zhao và cộng sự, 2019] và lái xe tự động [Chowdhuri và cộng sự, 2019].

Lưu ý rằng thách thức lớn nhất trong MTL là hiện tượng bập bênh [Tang và cộng sự, 2020]: huấn luyện chung thường dẫn đến hiệu suất tốt hơn trên một số nhiệm vụ nhưng hiệu suất tệ hơn trên các nhiệm vụ khác. Để vượt qua thách thức này, nhiều phương pháp tối ưu hóa đã được đề xuất cho MTL: một là cân bằng mất mát trực tiếp gán các trọng số khác nhau cho các mất mát của nhiều nhiệm vụ theo nhiều tiêu chí khác nhau [Kendall và cộng sự, 2018, Liu và cộng sự, 2019, Lin và cộng sự, 2022]; phương pháp khác là cân bằng gradient trước tiên tính toán gradient nhiệm vụ và sau đó tổng hợp chúng theo các cách khác nhau [Sener và Koltun, 2018, Liu và cộng sự, 2021a, Navon và cộng sự, 2022]. Vì cân bằng gradient thường hoạt động tốt hơn cân bằng mất mát trong MTL, nó đã thu hút nhiều sự quan tâm nghiên cứu gần đây. Tuy nhiên, chi phí huấn luyện của các thuật toán cân bằng gradient cao hơn đáng kể so với hầu hết các thuật toán cân bằng mất mát, đặc biệt khi có nhiều nhiệm vụ hơn trong MTL [Kurin và cộng sự, 2022].

Mặc dù hầu hết các nghiên cứu trước đây tập trung vào cải thiện hiệu suất của MTL bằng cách đề xuất các phương pháp tiên tiến hơn, chúng tôi nhấn mạnh rằng cải thiện hiệu quả học tập cũng có ý nghĩa nghiên cứu chính trong MTL. Do đó, cả cân bằng mất mát và cân bằng gradient đều đáng nghiên cứu thêm, và sự đánh đổi giữa hiệu quả và hiệu suất có thể được thực hiện theo yêu cầu thực tế. Trong bài báo này, chúng tôi tập trung vào việc cải thiện thêm hiệu suất của các phương pháp cân bằng mất mát hiện có. Cụ thể, trong khuôn khổ cân bằng mất mát, chúng tôi đề xuất hai thuật toán cân bằng khoảng cách có thể cải thiện mới, trong đó khoảng cách có thể cải thiện cho mỗi nhiệm vụ được định nghĩa là khoảng cách giữa tiến trình huấn luyện hiện tại và tiến trình huấn luyện cuối cùng mong muốn. Để định nghĩa khoảng cách có thể cải thiện, trước tiên chúng tôi phải biểu diễn tiến trình huấn luyện hiện tại dưới dạng giảm mất mát được chuẩn hóa bởi mất mát trung bình trong thời gian huấn luyện hiện tại (từ đầu huấn luyện). Như thể hiện trong Hình 1, các mất mát nhiệm vụ được chuẩn hóa có xu hướng hội tụ về các giá trị khác không do dữ liệu huấn luyện hạn chế và khả năng của mô hình. Quan trọng là, các nhiệm vụ khác nhau trong MTL có xu hướng có các mẫu hội tụ khác nhau (do đó có các khoảng cách có thể cải thiện khác nhau tại thời điểm huấn luyện hiện tại). Tuy nhiên, các thuật toán hiện có hiếm khi chú ý rằng các nhiệm vụ được huấn luyện cùng nhau thường có các khoảng cách có thể cải thiện khác nhau, dẫn đến việc một số nhiệm vụ được huấn luyện đầy đủ (hoặc thậm chí làm cho MTL vượt trội STL trên những nhiệm vụ này), trong khi những nhiệm vụ khác vẫn chưa được phù hợp.

Do đó, ý tưởng chính của chúng tôi là gán trọng số nhiệm vụ động để cân bằng khoảng cách có thể cải thiện (IGB), thay vì cân bằng mất mát đã được sử dụng rộng rãi trong MTL.

Chúng tôi đề xuất thuật toán đầu tiên IGBv1 để cân bằng khoảng cách có thể cải thiện thông qua một heuristic đơn giản bằng cách định nghĩa đồng nhất mất mát lý tưởng là 0 cho mỗi nhiệm vụ. Hơn nữa, cân bằng mất mát có thể được giải thích như giảm mất mát tích lũy tối đa trong tương lai bằng cách gán trọng số thông qua các quyết định tuần tự, điều này phù hợp với tình huống ứng dụng của học tăng cường [Kaelbling và cộng sự, 1996]. Do đó, chúng tôi đề xuất thuật toán khác IGBv2 để cùng nhau tối thiểu hóa tất cả các khoảng cách có thể cải thiện thông qua học tăng cường sâu (DRL) [Arulkumaran và cộng sự, 2017]. Ngoài ra, chúng tôi kết hợp IGB và cân bằng gradient để thể hiện tính bổ sung giữa hai loại thuật toán này.

Tóm lại, những đóng góp chính của chúng tôi có ba khía cạnh:

(1) Chúng tôi đề xuất hai thuật toán cân bằng mất mát mới để cân bằng động các khoảng cách có thể cải thiện trong MTL. Theo hiểu biết tốt nhất của chúng tôi, chúng tôi là những người đầu tiên áp dụng DRL cho MTL.

(2) Các thí nghiệm rộng rãi trên hai bộ dữ liệu benchmark chứng minh rằng các thuật toán IGB của chúng tôi hoạt động tốt nhất trong cân bằng mất mát và mang lại những cải thiện thêm khi kết hợp với cân bằng gradient.

(3) Chúng tôi suy nghĩ lại về ý nghĩa của cân bằng mất mát về hiệu quả học tập cũng như tính bổ sung của nó với cân bằng gradient.

## 2 CÔNG TRÌNH LIÊN QUAN

**Học tập đa nhiệm.** Nghiên cứu học tập đa nhiệm (MTL) được chia rộng rãi thành hai loại: một là học tương quan giữa các nhiệm vụ thông qua cấu trúc mô hình [Misra và cộng sự, 2016, Ma và cộng sự, 2018, Liu và cộng sự, 2019], và loại khác là cân bằng quá trình huấn luyện chung của tất cả các nhiệm vụ thông qua các thuật toán tối ưu hóa [Kendall và cộng sự, 2018, Lin và cộng sự, 2022, Sener và Koltun, 2018, Liu và cộng sự, 2021b, Navon và cộng sự, 2022]. Nghiên cứu của chúng tôi chủ yếu quan tâm đến loại sau, có thể được phân loại thành hai loại: cân bằng mất mát và cân bằng gradient.

Cân bằng mất mát trực tiếp cập nhật mô hình bằng cách cộng hoặc lấy trung bình các mất mát có trọng số sau khi gán trọng số nhiệm vụ theo nhiều tiêu chí khác nhau. Thời gian huấn luyện của hầu hết các thuật toán cân bằng mất mát gần như giống với STL, vì dữ liệu đầu vào, chẳng hạn như mất mát của batch hiện tại, có chiều thấp. Phương pháp phổ biến nhất cho cân bằng mất mát là Trọng số Bằng nhau (EW), trực tiếp tối thiểu hóa tổng mất mát của các nhiệm vụ. Ngoài ra, nhiều tiêu chí khác nhau được xem xét trong các nghiên cứu trước. Ví dụ, Kendall và cộng sự [2018] đo lường độ không chắc chắn của nhiệm vụ thông qua các tham số có thể học; Guo và cộng sự [2018] ước tính độ khó nhiệm vụ dựa trên hiệu suất chính; Liu và cộng sự [2019] xem xét tỷ lệ thay đổi mất mát; Lin và cộng sự [2022] gán trọng số nhiệm vụ ngẫu nhiên; Ye và cộng sự [2021] tập trung vào hiệu suất xác nhận metamodel.

Mặt khác, cân bằng gradient trước tiên tính toán gradient nhiệm vụ riêng biệt và sau đó cập nhật mô hình bằng cách tổng hợp gradient nhiệm vụ theo các cách khác nhau. Hiệu suất của cân bằng gradient thường tốt hơn cân bằng mất mát, vì nó có thể xử lý xung đột gradient [Yu và cộng sự, 2020] trực tiếp ở mức gradient. Ví dụ, Chen và cộng sự [2018] chuẩn hóa gradient nhiệm vụ để học mỗi nhiệm vụ với tốc độ tương tự; Sener và Koltun [2018] coi tổng hợp gradient như một bài toán tối ưu hóa đa mục tiêu; Liu và cộng sự [2021a] thêm điều kiện mất mát trung bình tối thiểu vào Sener và Koltun [2018]; Yu và cộng sự [2020] chiếu gradient nhiệm vụ lên các mặt phẳng pháp tuyến của gradient xung đột; Chen và cộng sự [2020] ngẫu nhiên loại bỏ một số gradient nhiệm vụ; Liu và cộng sự [2021b] nhằm mục đích làm cho gradient tổng hợp đóng góp bằng nhau cho mỗi nhiệm vụ; Navon và cộng sự [2022] coi tổng hợp gradient như một trò chơi thương lượng Nash. Tuy nhiên, do nhiều lần lan truyền ngược và tổng hợp gradient chiều cao, cân bằng gradient thường yêu cầu thời gian huấn luyện lớn hơn STL, điều này hạn chế nghiêm trọng hiệu quả học tập của nó trong thực tế. Ngoài ra, một số công trình cũng khám phá sự kết hợp của cân bằng mất mát và cân bằng gradient, sử dụng cân bằng mất mát để cập nhật các tham số riêng biệt cho nhiệm vụ và kết hợp cân bằng mất mát với cân bằng gradient để cập nhật các tham số chia sẻ nhiệm vụ [Liu và cộng sự, 2021b, Lin và cộng sự, 2022, Liu và cộng sự, 2022].

**Học tăng cường.** Học tăng cường (RL) là một phương pháp ra quyết định học máy tương tác cho dữ liệu luồng để tối đa hóa lợi nhuận mong đợi [Kaelbling và cộng sự, 1996]. Nó thường được mô hình hóa bằng cách sử dụng một quá trình quyết định Markov, giả định rằng trạng thái tương lai độc lập với trạng thái quá khứ khi biết trạng thái hiện tại [Watkins, 1989]. Nói cách khác, cho si là trạng thái tại thời điểm i, trạng thái st là Markovian khi và chỉ khi

Pr(st+1|st) = Pr(st+1|s1, s2,···, st). (1)

Soft Actor-Critic (SAC) [Haarnoja và cộng sự, 2018] là một thuật toán actor-critic off-policy dựa trên khuôn khổ RL entropy tối đa, được chọn làm thành phần trọng số mất mát trong thuật toán IGBv2 của chúng tôi. Thuật toán Actor-Critic [Peters và Schaal, 2008] kết hợp RL dựa trên chính sách với RL dựa trên giá trị. Actor tạo ra các hành động và tương tác với môi trường, trong khi critic đánh giá hiệu suất của actor và chỉ đạo các hành động trong giai đoạn tiếp theo. Thuật toán này hiệu quả hơn các phương pháp gradient chính sách vì nó có thể được cập nhật ở mỗi bước. Bộ đệm phát lại [Mnih và cộng sự, 2013] trong SAC là một cơ chế off-policy cổ điển trong đó agent đã học và agent tương tác với môi trường là khác nhau. Nó lưu trữ dữ liệu lịch sử để huấn luyện, bao gồm bốn yếu tố: trạng thái st, hành động at, phần thưởng rt và trạng thái tiếp theo st+1. Cơ chế off-policy có lợi cho việc cải thiện hiệu quả mẫu và giảm bất ổn huấn luyện do dữ liệu chuỗi thời gian gây ra. Học tăng cường entropy tối đa [Ziebart và cộng sự, 2008] thay đổi mục tiêu tối ưu hóa để tối đa hóa cả lợi nhuận mong đợi và entropy mong đợi của chính sách. Nó tăng tính ngẫu nhiên của chính sách, chỉ ra rằng phân bố xác suất của hành động rộng hơn nhiều. Lưu ý rằng nhiều tính ngẫu nhiên hơn được chứng minh là có lợi để cải thiện hiệu suất trong MTL [Lin và cộng sự, 2022, Chen và cộng sự, 2020].

Trong bối cảnh RL, MTL có thể tạo điều kiện chuyển giao kiến thức giữa các nhiệm vụ, điều này đã được chứng minh là cải thiện hiệu suất của các agent RL trong nhiều lĩnh vực khác nhau. Ví dụ, Chen và cộng sự [2021] đã huấn luyện một agent MTL cho mạng quang học tự trị, hiệu quả thúc đẩy các quá trình huấn luyện và cải thiện thông lượng dịch vụ tổng thể. Công trình của chúng tôi khác với cách tiếp cận này ở chỗ chúng tôi, lần đầu tiên, áp dụng học tăng cường để giải quyết các bài toán tối ưu hóa MTL tổng quát.

## 3 PHƯƠNG PHÁP LUẬN

Trong phần này, trước tiên chúng tôi cung cấp một mô hình cân bằng mất mát bất biến tỷ lệ. Dựa trên mô hình này, sau đó chúng tôi mô tả các thuật toán IGB của chúng tôi. Cuối cùng, chúng tôi đưa ra mô hình kết hợp để kết hợp cân bằng mất mát và cân bằng gradient.

### 3.1 CÂN BẰNG MẤT MÁT BẤT BIẾN TỶ LỆ

Mục tiêu của các thuật toán cân bằng mất mát hiện có là tối thiểu hóa tổng có trọng số hoặc trung bình của mất mát nhiệm vụ [Lin và cộng sự, 2021]. Tuy nhiên, nếu mất mát nhiệm vụ ở các tỷ lệ khác nhau, việc cập nhật mô hình có thể bị chi phối bởi một nhiệm vụ cụ thể. Để giải quyết vấn đề khác biệt tỷ lệ của mất mát nhiệm vụ, Navon và cộng sự [2022] giới thiệu một mục tiêu bất biến tỷ lệ Σⁿᵢ₌₁log(Lᵢ), trong đó n là số lượng nhiệm vụ và Lᵢ là mất mát nhiệm vụ thứ i. Lấy cảm hứng từ điều này, chúng tôi đề xuất một mô hình cân bằng mất mát bất biến tỷ lệ được gọi là SI. Tổng mất mát của SI được định nghĩa là:

Ltotal = Σⁿᵢ₌₁λl,i log(Li), (2)

trong đó λl,i là trọng số nhiệm vụ thứ i được gán bởi một thuật toán cân bằng mất mát cụ thể. Trong bài báo này, cả hai thuật toán IGB của chúng tôi đều được thiết kế dựa trên mô hình SI.

### 3.2 CÂN BẰNG KHOẢNG CÁCH CÓ THỂ CẢI THIỆN

Chúng tôi đề xuất hai thuật toán cân bằng mất mát: IGBv1 cân bằng trực tiếp khoảng cách có thể cải thiện giữa các nhiệm vụ được ước tính bởi một heuristic đơn giản, trong khi IGBv2 cùng nhau tối thiểu hóa tất cả khoảng cách có thể cải thiện thông qua một mô hình DRL.

#### 3.2.1 IGBv1

Trong tình huống lý tưởng, mô hình MTL có thể phù hợp hoàn toàn với mỗi dữ liệu huấn luyện, thỏa mãn Li = 0, ∀i. Do đó, chúng tôi giả định rằng mất mát huấn luyện cuối cùng mong muốn của tất cả các nhiệm vụ là 0, để các khoảng cách có thể cải thiện có thể được biểu diễn trực tiếp bởi mất mát được chuẩn hóa của batch hiện tại, trong đó việc chuẩn hóa là để xử lý các tỷ lệ mất mát nhiệm vụ khác nhau.

Để chuẩn hóa mất mát huấn luyện, chúng tôi tính toán mất mát trung bình trên mỗi batch trong epoch thứ hai của tất cả các nhiệm vụ làm Lbase = [Lbase,1, Lbase,2, ..., Lbase,n], vì mất mát của epoch đầu tiên có thể quá lớn để biểu diễn chính xác các tỷ lệ mất mát nếu mô hình MTL được khởi tạo ngẫu nhiên. Trong hai epoch đầu tiên của huấn luyện, trọng số nhiệm vụ được gán bằng hằng số 1. Sau đó trong quá trình huấn luyện tiếp theo, trọng số nhiệm vụ được tính toán như:

λv1 = n × softmax(L/Lbase), (3)

trong đó L = [L1,···, Ln] là mất mát batch hiện tại của tất cả các nhiệm vụ, và L/Lbase biểu thị phép chia từng phần tử của hai vector. Bằng cách này, chúng tôi gán động nhiều trọng số hơn cho các nhiệm vụ có nhiều khoảng cách có thể cải thiện hơn. Quá trình huấn luyện của IGBv1 được tóm tắt trong Thuật toán 1, trong đó chúng tôi ban đầu đặt cả epoch hiện tại ce và batch hiện tại cb là 1.

#### 3.2.2 IGBv2

Trong thực tế, rất khó để mô hình MTL thỏa mãn Li = 0, ∀i ở cuối huấn luyện. Lý do là các phương pháp học sâu hiện có không thể phù hợp hoàn toàn với phân bố dữ liệu huấn luyện, và mất mát tối thiểu trên dữ liệu huấn luyện thường chỉ ra overfitting. Do đó, việc ước tính khoảng cách có thể cải thiện trong IGBv1 là không chính xác, và chúng tôi trình bày IGBv2 có thể cân bằng thích ứng khoảng cách có thể cải thiện thông qua DRL.

Lưu ý rằng gán trọng số nhiệm vụ cho cân bằng mất mát có thể được giải thích như các quyết định tuần tự để tối đa hóa giảm mất mát tích lũy của các nhiệm vụ trong tương lai, trong khi giảm mất mát có thể được coi là phần thưởng phản hồi cho trọng số nhiệm vụ, điều này phù hợp với tình huống ứng dụng của RL. Lấy cảm hứng từ điều này, chúng tôi đề xuất triển khai một mô hình DRL để gán trọng số nhiệm vụ, với việc giảm mất mát của mô hình MTL làm phần thưởng cho DRL.

Chúng tôi chọn Soft Actor-Critic (SAC) [Haarnoja và cộng sự, 2018] làm mô hình DRL để hướng dẫn việc huấn luyện mô hình MTL, đây là một thuật toán actor-critic off-policy dựa trên khuôn khổ RL entropy tối đa. Như được minh họa trong Hình 2, trọng số nhiệm vụ được gán bởi mô hình SAC với mất mát của mô hình MTL làm đầu vào, và mô hình MTL và mô hình SAC được huấn luyện luân phiên. Lý do chọn SAC như sau: 1) Cấu trúc actor-critic [Peters và Schaal, 2008] cho phép mô hình SAC được cập nhật ở mỗi bước, cho phép IGBv2 gán trọng số tối ưu hiện tại kịp thời; 2) Bộ đệm phát lại [Mnih và cộng sự, 2013] trong SAC là một cơ chế off-policy thường được sử dụng, có thể cải thiện hiệu quả mẫu bằng cách huấn luyện mô hình SAC trên dữ liệu lịch sử ngẫu nhiên. Vì mô hình MTL chỉ có thể được huấn luyện một lần, dữ liệu huấn luyện có sẵn cho mô hình SAC của chúng tôi ít hơn nhiều so với các ứng dụng DRL điển hình, làm cho hiệu quả mẫu trở nên quan trọng; 3) Cơ chế entropy tối đa [Ziebart và cộng sự, 2008] có thể tăng tính ngẫu nhiên của hành động, từ đó tăng khả năng khám phá tối ưu toàn cục của mô hình MTL.

Không giống như triển khai DRL để chơi trò chơi [Mnih và cộng sự, 2013] yêu cầu hàng triệu tập, chỉ một lần huấn luyện hoàn chỉnh được phép để có được mô hình tối ưu trong MTL. Do đó, trong công trình này, nhiều chi tiết được thiết kế lại để phù hợp với sự khác biệt giữa hai lĩnh vực khác nhau này. Đầu tiên, chúng tôi xem xét các yếu tố cơ bản của DRL bao gồm môi trường, trạng thái, hành động và phần thưởng:

• **Môi trường**: Chúng tôi coi toàn bộ mô hình MTL và dữ liệu huấn luyện cho MTL là môi trường.

• **Trạng thái**: Trạng thái được yêu cầu để mô tả đúng môi trường hiện tại và không có chiều quá cao để đạt hiệu quả học tập. Do đó, chúng tôi chọn mất mát của batch hiện tại làm trạng thái st, được xác định bởi cả tình huống tham số hiện tại của mô hình MTL và dữ liệu huấn luyện đầu vào của batch hiện tại. Bằng cách này, quá trình huấn luyện quá khứ của mô hình MTL có thể được biểu diễn đầy đủ bởi trạng thái hiện tại, có thể được hình thành như một quá trình quyết định Markov tiêu chuẩn.

• **Hành động**: Chúng tôi coi trọng số nhiệm vụ của batch hiện tại làm hành động at, và giới hạn nó thành các số dương có tổng bằng n để so sánh công bằng với các thuật toán khác.

• **Phần thưởng**: Lấy cảm hứng từ việc tối đa hóa cải thiện tối thiểu của tất cả các nhiệm vụ trong Sener và Koltun [2018] và Liu và cộng sự [2021a], chúng tôi coi giảm mất mát tối thiểu của tất cả các nhiệm vụ trong batch hiện tại làm phần thưởng, trong đó mất mát cũng được chuẩn hóa bởi mất mát trung bình của epoch thứ hai Lbase. Ngoài ra, vì việc giảm mất mát của mô hình MTL khó khăn hơn khi tốc độ học thấp hơn so với lúc bắt đầu huấn luyện, chúng tôi thêm một yếu tố nhân α vào phần thưởng, được tính toán như tỷ lệ của tốc độ học ban đầu so với tốc độ học hiện tại. Cuối cùng, phần thưởng được định nghĩa là:

rt = α × min((Lt - Lt+1)/Lbase), (4)

trong đó Lt là vector của mất mát batch hiện tại và Lt+1 là vector của mất mát batch tiếp theo.

Mô hình SAC gán trọng số nhiệm vụ thích ứng để tối đa hóa phần thưởng tích lũy, để giảm mất mát tích lũy của mỗi nhiệm vụ có thể được tối đa hóa đến khoảng cách có thể cải thiện. Nói cách khác, IGBv2 cùng nhau tối thiểu hóa khoảng cách có thể cải thiện của tất cả các nhiệm vụ bằng cách dần dần tối thiểu hóa mất mát huấn luyện đến mất mát huấn luyện cuối cùng mong muốn.

Hơn nữa, chúng tôi trình bày quá trình huấn luyện của IGBv2 trong Thuật toán 2, trong đó chúng tôi ban đầu đặt cả epoch hiện tại ce và batch hiện tại cb là 1. Ở đầu huấn luyện, vì mô hình SAC chưa được huấn luyện đủ để sử dụng, RLW [Lin và cộng sự, 2022] được triển khai để gán trọng số nhiệm vụ ngẫu nhiên cho MTL, điều này cũng có lợi cho việc huấn luyện mô hình SAC thông qua khám phá ngẫu nhiên. Sau khi có được Lbase, chúng tôi có thể có được trạng thái st, hành động at, và phần thưởng rt-1 trong mỗi batch huấn luyện của mô hình MTL. Những dữ liệu này liên tục được thêm vào bộ đệm phát lại để huấn luyện mô hình SAC. Khi mô hình SAC được huấn luyện đủ tốt, nó được triển khai để gán trọng số nhiệm vụ thay vì RLW.

Ngoài ra, chúng tôi thiết kế lại cẩn thận kích thước bộ đệm phát lại, nhỏ hơn đáng kể so với các ứng dụng SAC điển hình. Khi kích thước bộ đệm quá lớn, vì hiệu suất của mô hình MTL được cải thiện dần dần và mất mát huấn luyện được giảm dần dần, việc huấn luyện mô hình SAC với dữ liệu lịch sử quá sớm không có lợi cho việc huấn luyện hiện tại của mô hình MTL. Ngược lại, khi kích thước bộ đệm quá nhỏ, sự bất ổn huấn luyện do dữ liệu chuỗi thời gian và hiệu quả mẫu thấp có thể làm cho việc huấn luyện mô hình SAC không thỏa đáng, từ đó cũng dẫn đến hiệu suất kém của mô hình MTL.

### 3.3 KẾT HỢP CÂN BẰNG MẤT MÁT VÀ CÂN BẰNG GRADIENT

Các thuật toán IGB của chúng tôi cung cấp động tầm quan trọng khác nhau cho mỗi nhiệm vụ để cân bằng khoảng cách có thể cải thiện, trong khi các thuật toán cân bằng gradient xử lý trực tiếp hơn với xung đột gradient ở mức gradient. Hai loại thuật toán này bổ sung cho nhau và có thể được kết hợp với nhau để cải thiện thêm.

Chúng tôi đầu tiên gán trọng số nhiệm vụ với thuật toán cân bằng mất mát và sau đó nhập mất mát có trọng số vào thuật toán cân bằng gradient để có được gradient cập nhật cuối cùng. Bằng cách này, hiệu suất có thể được cải thiện thêm trong khi giữ thời gian huấn luyện gần như tương tự với chỉ cân bằng gradient, vì hầu hết các thuật toán cân bằng mất mát hầu như không thêm thời gian huấn luyện bổ sung.

Thông thường, cập nhật mô hình với các thuật toán cân bằng gradient hiện có có thể được chia thành hai cách: một cách gán trọng số cho gradient của cả tham số chia sẻ nhiệm vụ và tham số riêng biệt nhiệm vụ, trong khi cách khác chỉ tổng hợp gradient của tham số chia sẻ nhiệm vụ. Như được minh họa trong Thuật toán 3, khi kết hợp cân bằng mất mát và cân bằng gradient trong trường hợp sau, phổ biến hơn, tham số chia sẻ nhiệm vụ được cập nhật bởi cả cân bằng mất mát và cân bằng gradient, trong khi tham số riêng biệt nhiệm vụ được cập nhật độc lập bởi cân bằng mất mát.

## 4 THÍ NGHIỆM

### 4.1 THIẾT LẬP THÍ NGHIỆM

**Bộ dữ liệu.** Chúng tôi huấn luyện và đánh giá mô hình của chúng tôi trên bộ dữ liệu NYUv2 [Silberman và cộng sự, 2012] cho hiểu biết cảnh đa nhiệm và trên bộ dữ liệu QM9 [Ramakrishnan và cộng sự, 2014] cho dự đoán hồi quy đa nhiệm.

NYUv2 là bộ dữ liệu hình ảnh cảnh trong nhà với nhãn 13 lớp mức độ pixel dày đặc. Nó bao gồm 795 mẫu huấn luyện và 654 mẫu kiểm tra. Các nghiên cứu trước [Liu và cộng sự, 2021a, Navon và cộng sự, 2022] thường coi hiệu suất trung bình của 10 epoch cuối cùng trên tập kiểm tra là kết quả cuối cùng, điều này không hợp lý trong nghiên cứu học máy. Do đó, chúng tôi chia ngẫu nhiên 654 mẫu kiểm tra ban đầu thành 197 mẫu xác nhận (tập xác nhận) và 457 mẫu kiểm tra (tập kiểm tra).

QM9 là bộ dữ liệu phân tử hóa học được sử dụng rộng rãi cho mạng nơ-ron đồ thị (GNN) [Wu và cộng sự, 2020], với khoảng 130K mẫu phân tử (được biểu diễn như đồ thị được chú thích với tính năng nút và cạnh). Theo Navon và cộng sự [2022], chúng tôi sử dụng bộ dữ liệu này để dự đoán hồi quy trên 11 tính chất của phân tử hóa học, và chúng tôi sử dụng 110K mẫu để huấn luyện, 10K mẫu để xác nhận và 10K mẫu để kiểm tra.

**Phương pháp so sánh.** Chúng tôi so sánh phương pháp của chúng tôi với các thuật toán cổ điển sau đây đã được mô tả trong Phần 2: (1) Trọng số Bằng nhau (EW) tối thiểu hóa Σⁿᵢ₌₁Li; (2) Trọng số Mất mát Ngẫu nhiên (RLW) [Lin và cộng sự, 2022]; (3) Trung bình Trọng số Động (DWA) [Liu và cộng sự, 2019]; (4) Trọng số Không chắc chắn (UW) [Kendall và cộng sự, 2018]; (5) Thuật toán Gradient Descent Đa bội (MGDA) [Sener và Koltun, 2018]; (6) Chiếu Gradient Xung đột (PCGrad) [Yu và cộng sự, 2020]; (7) Gradient Tránh Xung đột (CAGrad) [Liu và cộng sự, 2021a]; (8) Học tập Đa nhiệm Công bằng (IMTL-G) [Liu và cộng sự, 2021b]; (9) Nash-MTL (Nash) [Navon và cộng sự, 2022].

**Chỉ số đánh giá.** Trong các thí nghiệm, chúng tôi đầu tiên báo cáo các chỉ số đánh giá phổ biến cho mỗi nhiệm vụ trên mỗi bộ dữ liệu. Hơn nữa, vì ý nghĩa của tối ưu hóa MTL là cải thiện cả hiệu suất mô hình và hiệu quả học tập, chúng tôi báo cáo hai chỉ số tổng thể để đánh giá toàn diện các phương pháp tối ưu hóa MTL:

(1) ∆m: giảm hiệu suất trung bình cho mỗi nhiệm vụ so với STL, được định nghĩa là:

∆m = (1/K) Σᵏₖ₌₁ (-1)^δk (Mm,k - Mb,k)/Mb,k, (5)

trong đó K là tổng số chỉ số đánh giá phổ biến trên tất cả các nhiệm vụ, Mm,k là giá trị trên chỉ số thứ k của phương pháp được đánh giá và Mb,k là giá trị của baseline STL, δk = 1 nếu giá trị cao hơn tốt hơn cho chỉ số thứ k và 0 nếu ngược lại [Liu và cộng sự, 2021a, Navon và cộng sự, 2022].

(2) T: thời gian huấn luyện tương đối so với EW, được tính toán như tỷ lệ của thời gian huấn luyện của phương pháp được đánh giá so với baseline EW.

**Chi tiết thực hiện.** Trong các thí nghiệm trên bộ dữ liệu NYUv2, chúng tôi huấn luyện mô hình SegNet từ Badrinarayanan và cộng sự [2017] trong 500 epoch với optimizer Adam [Kingma và Ba, 2014], trong khi tốc độ học ban đầu được đặt thành 1e-4 và giảm một nửa mỗi 100 epoch. Sau đó chúng tôi kiểm tra mô hình có chỉ số xác nhận ∆m tốt nhất để có được hiệu suất thuật toán cuối cùng.

Trong các thí nghiệm trên bộ dữ liệu QM9, theo Navon và cộng sự [2022], chúng tôi chuẩn hóa mỗi mục tiêu nhiệm vụ để có trung bình bằng không và độ lệch chuẩn đơn vị, và sử dụng mã được thực hiện bởi Fey và Lenssen [2019], chứa một GNN phổ biến từ Gilmer và cộng sự [2017] và một toán tử pooling từ Vinyals và cộng sự [2015]. Chúng tôi huấn luyện mô hình trong 300 epoch với optimizer Adam [Kingma và Ba, 2014], trong khi tốc độ học được tìm kiếm trong {1e-3, 5e-4, 1e-4} và giảm bởi bộ lập lịch ReduceOnPlateau theo chỉ số xác nhận ∆m.

Đối với thuật toán IGBv2, mô hình SAC được cập nhật một lần mỗi 50 batch trong quá trình huấn luyện mô hình MTL, trong đó tốc độ học của mô hình SAC được điều chỉnh theo tình huống: 1e-4 khi kết hợp IGBv2 với MGDA [Sener và Koltun, 2018], PCGrad [Yu và cộng sự, 2020] hoặc Nash [Navon và cộng sự, 2022], và 3e-4 khi kết hợp IGBv2 với các thuật toán cân bằng gradient khác hoặc chỉ sử dụng IGBv2. Kích thước bộ đệm phát lại được đặt thành 1e4. Hệ số chiết khấu γ được đặt thành 0.99. Epoch bắt đầu để cập nhật mô hình SAC update_e được đặt thành 4, và epoch bắt đầu để sử dụng mô hình SAC use_e được đặt thành 6.

### 4.2 KẾT QUẢ HIỂU BIẾT CẢNH ĐA NHIỆM TRÊN NYUV2

Hiểu biết cảnh trên bộ dữ liệu NYUv2 là tình huống đánh giá phổ biến nhất trong nghiên cứu MTL, chứa ba nhiệm vụ: phân đoạn ngữ nghĩa, ước tính độ sâu và dự đoán pháp tuyến bề mặt.

Kết quả so sánh được thể hiện trong Bảng 1. Để xác minh tính bổ sung giữa IGB và cân bằng gradient, chúng tôi kết hợp các thuật toán IGB của chúng tôi với tất cả các thuật toán cân bằng gradient được so sánh. Chúng tôi phân loại tất cả các phương pháp thành hai nhóm theo hiệu quả và so sánh hiệu suất trong mỗi nhóm: một là các thuật toán cân bằng mất mát; nhóm khác là các thuật toán cân bằng gradient và cũng là các thuật toán kết hợp bởi IGB và cân bằng gradient.

Đầu tiên, chúng tôi tập trung vào hiệu suất tổng thể và hiệu quả học tập của MTL. Về hiệu suất tổng thể (∆m): Các thuật toán IGB của chúng tôi là tốt nhất trong cân bằng mất mát và cũng cạnh tranh với một số thuật toán cân bằng gradient. Kết hợp IGB và cân bằng gradient tốt hơn so với chỉ sử dụng cân bằng gradient. Đặc biệt, IGBv1 + Nash và IGBv2 + Nash đạt được hiệu suất tốt hơn Nash [Navon và cộng sự, 2022], mang lại hiệu suất SOTA của tất cả các thuật toán. Về hiệu quả học tập (T): Thời gian huấn luyện của cân bằng gradient khoảng ba lần so với cân bằng mất mát. Dù sử dụng một mình hay kết hợp với cân bằng gradient, IGBv1 hầu như không dẫn đến thời gian huấn luyện bổ sung, và IGBv2 dẫn đến thời gian huấn luyện bổ sung nhẹ nhưng có thể đạt được hiệu suất tốt hơn IGBv1 trong hầu hết các tình huống.

Đáng chú ý, với các thuật toán IGB của chúng tôi, người dùng hiện có nhiều tự do hơn để thực hiện sự đánh đổi giữa hiệu quả và hiệu suất trong các ứng dụng MTL thực tế: các thuật toán IGB của chúng tôi có thể được chọn nếu hiệu quả quan trọng hơn, và các thuật toán IGB + cân bằng gradient có thể được chọn nếu hiệu suất quan trọng hơn. Cụ thể, IGBv1 hiệu quả hơn và đơn giản hơn để thực hiện, trong khi IGBv2 đạt được hiệu suất tốt hơn trong hầu hết các tình huống.

Tiếp theo, chúng tôi phân tích tính ưu việt của các thuật toán IGB của chúng tôi thông qua hiệu suất nhiệm vụ cụ thể. So với STL, hiệu suất của hầu hết các thuật toán hiện có gần bằng hoặc thậm chí tốt hơn trên phân đoạn ngữ nghĩa và ước tính độ sâu, nhưng kém hơn đáng kể trên dự đoán pháp tuyến bề mặt. Ở một mức độ nào đó, điều này có nghĩa là trong tình huống NYUv2 MTL, phân đoạn ngữ nghĩa và ước tính độ sâu đơn giản hơn, trong khi dự đoán pháp tuyến bề mặt khó khăn hơn. Sự mất cân bằng hiệu suất như vậy là không mong muốn trong hầu hết các tình huống MTL.

Như mong đợi, các thuật toán IGB của chúng tôi có thể giảm thiểu hiệu quả vấn đề mất cân bằng này. Có nghĩa là, so với các thuật toán cân bằng mất mát khác, hiệu suất của IGBv1 và IGBv2 của chúng tôi cạnh tranh trên phân đoạn ngữ nghĩa và ước tính độ sâu, và tốt hơn đáng kể trên dự đoán pháp tuyến bề mặt, gần với STL.

Hơn nữa, hiệu suất của MGDA [Sener và Koltun, 2018] là tốt nhất trên dự đoán pháp tuyến bề mặt trong số tất cả các thuật toán, nhưng kém hơn đáng kể so với các thuật toán khác trên phân đoạn ngữ nghĩa và ước tính độ sâu, dẫn đến hiệu suất tổng thể kém của nó. Bằng cách kết hợp MGDA và các thuật toán IGB của chúng tôi, chúng tôi giảm thiểu đáng kể vấn đề mất cân bằng hiệu suất và cải thiện đáng kể hiệu suất tổng thể của nó. Như thể hiện trong Hình 3, khi sử dụng IGBv1, mất mát huấn luyện của phân đoạn ngữ nghĩa và ước tính độ sâu giảm nhanh hơn và thấp hơn đáng kể. Trong khi đó, đối với dự đoán pháp tuyến bề mặt, mất mát huấn luyện của IGBv1 + MGDA tương đương với MGDA trong quá trình huấn luyện. Lý do chính là các thuật toán cân bằng khoảng cách có thể cải thiện của chúng tôi có thể tự động tập trung vào các nhiệm vụ chưa được huấn luyện đúng cách.

### 4.3 KẾT QUẢ DỰ ĐOÁN HỒI QUY ĐA NHIỆM TRÊN QM9

Dự đoán 11 tính chất của phân tử hóa học trên bộ dữ liệu QM9 là một bài toán MTL thách thức hơn vì số lượng nhiệm vụ lớn hơn, mức độ liên quan nhiệm vụ thấp hơn và độ khó nhiệm vụ khác nhau hơn. Các nghiên cứu trước đây đã phát hiện rằng hiệu suất của STL trên dự đoán 11 tính chất tốt hơn đáng kể so với MTL [Maron và cộng sự, 2019, Gasteiger và cộng sự, 2020], nhưng được thúc đẩy bởi nhu cầu về hiệu quả học tập cao hơn và kích thước mô hình nhỏ hơn, các phương pháp MTL vẫn đáng nghiên cứu.

Kết quả so sánh trên bộ dữ liệu QM9 được thể hiện trong Bảng 2. Chúng ta có thể thấy rằng các thuật toán IGB của chúng tôi đánh bại tất cả các thuật toán được so sánh ngoại trừ Nash [Navon và cộng sự, 2022] về hiệu suất, và hoạt động tốt hơn nhiều so với tất cả các thuật toán cân bằng gradient về hiệu quả học tập. Ví dụ, thời gian huấn luyện của Nash khoảng bảy lần so với các thuật toán IGB của chúng tôi với hiệu suất gần bằng, và thời gian huấn luyện của MGDA [Sener và Koltun, 2018] là hai mươi hai lần so với các thuật toán IGB của chúng tôi do tổng hợp gradient chiều cao bằng tối ưu hóa đa mục tiêu. Khi số lượng nhiệm vụ tăng, khoảng cách hiệu quả học tập giữa cân bằng mất mát và cân bằng gradient trở nên lớn hơn và lớn hơn. Trong một tình huống như QM9, cân bằng mất mát (đặc biệt là IGB) do đó có giá trị hơn. Thật không may, các quyết định trọng số hoặc tổng hợp được thực hiện bởi các thuật toán cân bằng mất mát và cân bằng gradient hiện có đều không thỏa đáng trên bài toán này (tệ hơn nhiều so với STL). Kết hợp cân bằng mất mát và cân bằng gradient tương đương với kết hợp các quyết định được thực hiện bởi hai loại thuật toán, điều này chưa mang lại hiệu suất tốt hơn.

### 4.4 NGHIÊN CỨU LOẠI BỎ

Như thể hiện trong Bảng 3, chúng tôi phân tích đóng góp của cân bằng khoảng cách có thể cải thiện trong các thuật toán của chúng tôi và tác động của các thiết lập khác nhau trong thuật toán IGBv2 trên bộ dữ liệu NYUv2. Các phương pháp được so sánh bao gồm: (1) EW tối thiểu hóa Σⁿᵢ₌₁Li; (2) SI tối thiểu hóa Σⁿᵢ₌₁log(Li) [Navon và cộng sự, 2022]; (3) IGBv1 (đầy đủ); (4) IGBv2 (phần thưởng / min, buffer 1e4) trong đó phần thưởng được tính toán bằng giảm mất mát trung bình thay vì tối thiểu; (5) IGBv2 (phần thưởng / α, buffer 1e4) trong đó phần thưởng được tính toán mà không có hệ số nhân α; (6) IGBv2 (phần thưởng đầy đủ, buffer 5e3/1e4/5e4/1e6) trong đó kích thước bộ đệm phát lại được đặt thành 5e3, 1e4, 5e4, hoặc 1e6, và IGBv2 (phần thưởng đầy đủ, buffer 1e4) là IGBv2 đầy đủ của chúng tôi.

Chúng tôi có thể quan sát từ Bảng 3 rằng: (1) SI mang lại cải thiện hiệu suất đáng kể so với EW, cho thấy rằng mô hình cân bằng mất mát bất biến tỷ lệ vượt trội so với mô hình cân bằng mất mát truyền thống. (2) IGBv1 và IGBv2 đầy đủ của chúng tôi cải thiện thêm hiệu suất so với SI, điều này xác nhận rõ ràng hiệu quả của cân bằng khoảng cách có thể cải thiện. (3) IGBv2 (phần thưởng / min, buffer 1e4), IGBv2 (phần thưởng / α, buffer 1e4), và IGBv2 (phần thưởng đầy đủ, buffer 5e3/5e4/1e6) đều tạo ra sự suy giảm hiệu suất so với IGBv2 đầy đủ, cho thấy rằng việc thiết kế lại phần thưởng và kích thước bộ đệm phát lại của chúng tôi là cần thiết khi triển khai DRL cho tối ưu hóa MTL.

Hơn nữa, như thể hiện trong Bảng 4, chúng tôi kết hợp các phương pháp cân bằng mất mát hiện có với mô hình SI (phương pháp UW + SI không hoạt động vì UW có thể không tương thích với SI), và hiệu suất của chúng trên NYUv2 thấp hơn nhiều so với các phương pháp IGB của chúng tôi. Điều này cung cấp bằng chứng thêm về hiệu quả của các phương pháp IGB của chúng tôi, tức là trọng số nhiệm vụ của IGB và mô hình SI bổ sung cho nhau (kết hợp chúng dẫn đến hiệu suất tốt hơn).

## 5 KẾT LUẬN

Trong bài báo này, chúng tôi đề xuất hai thuật toán cân bằng mất mát mới, IGBv1 và IGBv2. Chúng tôi gán trọng số nhiệm vụ động để cân bằng khoảng cách có thể cải thiện: IGBv1 sử dụng heuristic đơn giản, và IGBv2 (lần đầu tiên) áp dụng DRL cho MTL. Chúng tôi phân tích tính bổ sung giữa IGB và cân bằng gradient. Các thí nghiệm rộng rãi cho thấy rằng các thuật toán IGB của chúng tôi vượt trội so với tất cả các thuật toán cân bằng mất mát hiện có và mang lại những cải thiện hiệu suất thêm khi kết hợp với cân bằng gradient. Quan trọng hơn, chúng tôi nhấn mạnh lại ý nghĩa của cân bằng mất mát trong MTL về hiệu quả học tập và mang lại cho người dùng nhiều tự do hơn để thực hiện sự đánh đổi giữa hiệu quả và hiệu suất trong các ứng dụng thực tế.

**Lời cảm ơn**

Công trình này được hỗ trợ một phần bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (61976220), và Quỹ Nghiên cứu Cơ bản cho các Trường Đại học Trung ương và Quỹ Nghiên cứu của Đại học Nhân dân Trung Quốc (23XNH026). Zhiwu Lu là tác giả liên lạc.
