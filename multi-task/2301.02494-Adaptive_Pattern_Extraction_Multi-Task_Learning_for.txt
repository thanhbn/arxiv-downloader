# 2301.02494.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multi-task/2301.02494.pdf
# File size: 1205654 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Adaptive Pattern Extraction Multi-Task Learning for
Multi-Step Conversion Estimations
Xuewen Taoâˆ—
xuewen.txw@mybank.cn
MYbank, Ant Group
Beijing, ChinaMingming Haâˆ—
hamingming_0705@foxmail.com
School of Automation and Electrical
Engineering, University of Science
and Technology Beijing; MYbank, Ant
Group
Beijing, ChinaQiongxu Ma
qiongxu.mqx@mybank.cn
MYbank, Ant Group
Shanghai, China
Hongwei Cheng
chw286885@mybank.cn
MYbank, Ant Group
Shanghai, ChinaWenfang Lin
moxi.lwf@mybank.cn
MYbank, Ant Group
Hangzhou, Zhejiang, ChinaXiaobo Guoâ€ 
xb_guo@bjtu.edu.cn
Institute of Information Science,
Beijing Jiaotong University, Mybank,
Ant Group,
Beijing, China
Abstract
Multi-task learning (MTL) has been successfully used in
many real-world applications, which aims to simultaneously
solve multiple tasks with a single model. The general idea of
multi-task learning is designing kinds of global parameter
sharing mechanism and task-specific feature extractor to
improve the performance of all tasks. However, challenge
still remains in balancing the trade-off of various tasks since
model performance is sensitive to the relationships between
them. Less correlated or even conflict tasks will deteriorate
the performance by introducing unhelpful or negative in-
formation. Therefore, it is important to efficiently exploit
and learn fine-grained feature representation corresponding
to each task. In this paper, we propose a Adaptive Pattern
Extraction Multi-task (APEM) framework, which is adaptive
and flexible for large-scale industrial application. APEM is
able to fully utilize the feature information by learning the
interactions between the input feature fields and extracted
corresponding tasks-pecific information. We first introduce
a DeepAuto Group Transformer module to automatically
and efficiently enhance the feature expressivity with a modi-
fied set attention mechanism and a Squeeze-and-Excitation
operation. Second, explicit Pattern Selector is introduced to
further enable selectively feature representation learning by
adaptive task-indicator vectors. Empirical evaluations show
that APEM outperforms the state-of-the-art MTL methods
on public and real-world financial services datasets. More
importantly, we explore the online performance of APEM in
a real industrial-level recommendation scenario.
âˆ—These authors contributed equally to this research.
â€ Xiaobo Guo is the corresponding author.CCS Concepts: â€¢Information systems â†’Information
systems applications ;Computational advertising ;â€¢Multi-
task Learning ;
Keywords: Recommender System, Sequential Dependency,
Multi-Task Learning, Representation Learning
1 Introduction
Multi-task learning (MTL) has enjoyed rather remarkable
successes for various real-world scenarios, such as the online
recommendation [ 7,22], display advertising [ 5], customer
acquisition management, financial service [ 30] and so forth.
MTL techniques simultaneously learn multiple tasks by im-
plicitly passing the message among related tasks [ 3,24], com-
pared with single-task learning, which can improve the over-
all performance of these tasks [ 4,20]. In online advertising,
recommendation, and customer acquisition etc, post-view
click-through rate (CTR), post-click conversion rate (CVR)
and post-view click-through & conversion rate (CTCVR)
estimations are a series of classical tasks [ 15,29] with se-
quential dependence of the customer acquisition process.
In this case, sequential pattern of user behaviours means
the later action only occur after the former action. More
generally, this sequential pattern can be extended to multi-
step conversion. As shown in Fig. 1, a multi-step conversion
example in fiance service strictly follow this sequential de-
pendence pattern. A customer will convert through stages
ofImpressionâ†’Clickâ†’Authorizeâ†’Conversion . Conver-
sion behaviors such like applying loans, make deposit or
purchasing investment products are only permitted after an
authorization. Aimed at various kinds of concrete industrial
applications, extensive studies [ 6,14,15,25,26,28] focus
on the CTR and CVR estimations. However, there are rare
works to provide a formalized definition of the sequential
dependence MTL (SDMTL) problem, which is of particulararXiv:2301.02494v3  [cs.LG]  23 Jan 2023

--- PAGE 2 ---
Conferenceâ€™17, July 2017, Washington, DC, USA Xuewen Tao, Mingming Ha, Qiongxu Ma, Hongwei Cheng, Wenfang Lin, and Xiaobo Guo
significance in multi-step conversion estimations applica-
ble to more diverse scenarios. In addition, the connection
and difference between the general MTL and SDMTL is also
unclear.
ConversionStages
ImpressionClickAuthorizationLoan
Finance
Figure 1. An illustration of a multi-step conversion in Fi-
nance Service.
Series works from ESMM [ 15] toESCM2[29] pay more
attention on the unbiased CVR estimation problem in a view
of causality to correct sample selection bias. The dependent
relation between the tasks like CTR and CVR is implicitly im-
plied via the distribution of sample space. Recently, [ 30] cap-
tures the task dependency through the information transfer
between different conversion steps and combines a calibrator
to further constrain the dependent relationship. However,
the dependency between each steps is still not deeply defined
and discussed in most MTL works from a theoretical format.
Besides, as mentioned above, MTL methods improve pre-
diction result through an information passing mechanism
between tasks, which suggests improper feature sharing will
result in even poorer or imbalanced performance in different
tasks known as a negative transfer phenomenon. Therefore,
general approach in MTL mainly focuses on designing kinds
of information extraction modules (experts) to learn com-
mon and task-specific representations. Such as Cross-Stitch
Network [ 16] and Sluice Network [ 19] employ a linear combi-
nation to leverage representations of different tasks but also
require much more training parameters. SOTA method of
Multi-gate Mixture-of-Experts (MMoE) approach [ 14] adopts
an ensemble of experts submodules and gating network to
model task relationships while consuming less computation.
Progressive Layered Extraction (PLE) [ 22], separates task-
common and task-specific parameters explicitly which could
further avoid parameter conflicts caused by complex task
correlation. These approaches assign individual parameters
to each task to better exploit task information and improve
model generalization. Nonetheless, feature expressivity with
respect to each task is still limited since task-irrelevant in-
formation passing from the shared structure and more fine-
grained representation learning is necessary.
In this paper, we first provide a formal definition of MTL
on sequential dependence problem, and propose an optimiz-
ing object paradigm for recover the dependent relationship
based on theoretical proof. And we also present a novel MTLframework called Adaptive Pattern Extraction Multi-task
(APEM) framework to selectively and dynamically enhance
the representation learning for respective tasks along with
the dependency-based object. APEM consists of two main
modules: Adaptive Sample-wise Representation Generator
(ASRG) and explicit Pattern Selector (PS). ASRG employs a
dynamic selection mechanism to learn the hierarchical fea-
ture interaction from a sample-wise view to further separates
the task-irrelevant information. The implement of explicit
PS enables fine-grained feature learning by introducing task-
specific indicator vectors. In a summary, main contributions
of this paper are presented as follows:
â€¢The SDMTL problem is first formally formulated, and
its connections and differences with the general MTL
problem are illustrated. Moreover, the distribution
dependence relationship between the adjacent task
spaces is revealed from a theoretical perspective.
â€¢We present a multi-task learning framework named
APEM for selectively fine-grained feature represen-
tation learning from a sample-wise view. ASRG and
PS modules within APEM adatively reconstruct the
implicit shared representations and extract explicit
task-specific information in an more efficient way.
â€¢Extensive experiments on public and real-world indus-
trial dataset are conducted to evaluate the effectiveness
of APEM. Experiment results demonstrate that our pro-
posed approach outperforms the state-of-the-art MTL
methods. Furthermore, we explore the boundary of
APEM in real-world industrial applications to prove
its efficiency for large-scale online recommendations.
2 Preliminaries
In this section, the SDMTL problem and the connection be-
tween SDMTL and general MTL are elaborated. Then, from
the expected lossâ€™s point of view, the distribution relation-
ship between the adjacent task domains is revealed.
2.1 Problem Formulation
Consider a SDMTL problem over an input space Xand a set
of task{Tğ‘–}ğ‘
ğ‘–=1, whereğ‘is the number of tasks and the cor-
responding task spaces are denoted as {T1,...,Tğ‘}. A large
dataset of data points {ğ‘¥ğ‘—,ğ‘œ1
ğ‘—,...,ğ‘œğ‘
ğ‘—}ğ‘€
ğ‘—=1are given, where ğ‘€
is the number of data points and ğ‘œğ‘—
ğ‘–âˆˆ{0,1}corresponding
to a binary classification problem or ğ‘œğ‘—
ğ‘–âˆˆRfor a regression
problem is the label of the ğ‘–-th task for the ğ‘—-th data point.
Differing from the general MTL problem, for the SDMTL
problem, there exists the sequential dependence relationship
between tasks in the sense that the current task Tğ‘–depends
on the previous task Tğ‘–âˆ’1, i.e.,Tğ‘–âˆ’1â†’Tğ‘–. Letğ‘‹andğ‘‡ğ‘–be
the random variables over the input space Xand task out-
put space Tğ‘–, respectively. In this paper, for convenience of
analysis, each task is set as a binary classification task. As

--- PAGE 3 ---
Adaptive Pattern Extraction Multi-Task Learning for Multi-Step Conversion Estimations Conferenceâ€™17, July 2017, Washington, DC, USA
mentioned in literature [ 17] and shown in Fig. 1, for sequen-
tial dependence, one of core properties is that if the event
ğ‘‡ğ‘–âˆ’1is not triggered, then the event ğ‘‡ğ‘–must not occur, i.e.,
ğ‘ƒ(ğ‘‡ğ‘–=1,ğ‘‡ğ‘–âˆ’1=0|ğ‘‹)=0, whereğ‘ƒ(Â·|Â·)denotes the condi-
tional probability. Therefore, according to this property, the
random variables ğ‘‡ğ‘–satisfies
ğ‘ƒ(ğ‘‡ğ‘–=1|ğ‘‹)=âˆ‘ï¸
ğ‘¡ğ‘–âˆ’1,...,ğ‘¡ 1âˆˆ{0,1}ğ‘ƒ(ğ‘‡ğ‘–=1,...,ğ‘‡ 1=ğ‘¡1|ğ‘‹)
=ğ‘ƒ(ğ‘‡ğ‘–=1,ğ‘‡ğ‘–âˆ’1=1,...,ğ‘‡ 1=1|ğ‘‹)
=ğ‘ƒ(ğ‘‡ğ‘–=1,ğ‘‡ğ‘–âˆ’1=1|ğ‘‹),
ğ‘ƒ(ğ‘‡ğ‘–=0|ğ‘‹)=âˆ‘ï¸
ğ‘¡ğ‘–âˆ’1,...,ğ‘¡ 1âˆˆ{0,1}ğ‘ƒ(ğ‘‡ğ‘–=0,...,ğ‘‡ 1=ğ‘¡1|ğ‘‹)
=ğ‘ƒ(ğ‘‡ğ‘–=0,ğ‘‡ğ‘–âˆ’1=0|ğ‘‹)+ğ‘ƒ(ğ‘‡ğ‘–=0,ğ‘‡ğ‘–âˆ’1=1|ğ‘‹),
(1)
which implies that the positive samples of ğ‘‡ğ‘–are derived
from the positive samples of the task ğ‘‡ğ‘–while the negative
samples consist of the negative samples of tasks ğ‘‡ğ‘–andğ‘‡ğ‘–âˆ’1
due to the sequential dependence.
In addition, the sequential dependence relationship is also
embodied in the constraints with respect to the conversion
probabilities of the adjacent tasks. In [ 30], the sequential
dependence relationship is formalized as
ğ‘ƒ(ğ‘‡1=1|ğ‘‹)â‰¥ğ‘ƒ(ğ‘‡2=1,ğ‘‡1=1|ğ‘‹)
Â·Â·Â·
â‰¥ğ‘ƒ(ğ‘‡ğ‘–âˆ’1=1,...,ğ‘‡ 1=1|ğ‘‹)
â‰¥ğ‘ƒ(ğ‘‡ğ‘–=1,ğ‘‡ğ‘–âˆ’1=1,...,ğ‘‡ 1=1|ğ‘‹).(2)
Then, a behavioral expectation calibrator is introduced into
AITM [ 30] to guarantee the sequential dependence relation-
ship (2). When the outputs of the model violate this condi-
tion, the designed loss will output a positive penalty term.
However, the condition (2) cannot completely reflect the
dependence relationship between tasks. Reconsidering the
dependence relationship between ğ‘ƒ(ğ‘‡ğ‘–âˆ’1=1,...,ğ‘‡ 1=1|ğ‘‹)
andğ‘ƒ(ğ‘‡ğ‘–=1,...,ğ‘‡ 1=1|ğ‘‹), it leads to
ğ‘ƒ(ğ‘‡ğ‘–âˆ’1=1|ğ‘‹)âˆ’ğ‘ƒ(ğ‘‡ğ‘–=1|ğ‘‹)
=ğ‘ƒ(ğ‘‡ğ‘–âˆ’1=1|ğ‘‹)âˆ’ğ‘ƒ(ğ‘‡ğ‘–=1,ğ‘‡ğ‘–âˆ’1=1|ğ‘‹)
=ğ‘ƒ(ğ‘‡ğ‘–âˆ’1=1|ğ‘‹)
1âˆ’ğ‘ƒ(ğ‘‡ğ‘–=1|ğ‘‡ğ‘–âˆ’1=1,ğ‘‹)
=ğ‘ƒ(ğ‘‡ğ‘–âˆ’1=1|ğ‘‹)ğ‘ƒ(ğ‘‡ğ‘–=0|ğ‘‡ğ‘–âˆ’1=1,ğ‘‹)
=ğ‘ƒ(ğ‘‡ğ‘–=0,ğ‘‡ğ‘–âˆ’1=1|ğ‘‹). (3)
Therefore, the dependence relationship between the adjacent
tasks needs to satisfy the equality constraints (3), which also
contains the dependence information ğ‘ƒ(ğ‘‡ğ‘–=1,ğ‘‡ğ‘–âˆ’1=0|ğ‘‹)=
0.
Define a parametric hypothesis class per task as ğ‘“ğ‘–(ğ‘¥;ğœƒğ‘ ,ğœƒğ‘–):
Xâ†’Tğ‘–, whereğœƒğ‘ andğœƒğ‘–are shared parameters and task-
specific parameters of the task ğ‘–. Also, the task-specific loss
function is defined as ğ¿ğ‘–(Â·,Â·):Tğ‘–Ã—Tğ‘–â†’R+. Similar to the
general MTL problem, the objective of SDMTL is to minimizethe following expected loss:
min
ğœƒğ‘ ,ğœƒ1,...,ğœƒğ‘ğ‘âˆ‘ï¸
ğ‘–=1ğ¸ğ‘‹,ğ‘‡ 1,...,ğ‘‡ğ‘âˆ¼O[ğ‘¤ğ‘–ğ¿ğ‘–(ğ‘“ğ‘–(ğ‘‹;ğœƒğ‘ ,ğœƒğ‘–),ğ‘‡ğ‘–)]
s.t.ğ‘“ğ‘–(ğ‘‹;ğœƒğ‘ ,ğœƒğ‘–)âˆ’ğ‘“ğ‘–âˆ’1(ğ‘‹;ğœƒğ‘ ,ğœƒğ‘–)=ğ‘ƒ(ğ‘‡ğ‘–=0,ğ‘‡ğ‘–âˆ’1=1|ğ‘‹),
ğ‘–=2,...,ğ‘, (4)
whereOis the distribution with domain XÃ—T1Ã—Â·Â·Â·Ã—
Tğ‘, andğ‘¤ğ‘–is the static or dynamically computed weight
per task. Therefore, the SDMTL problem can be considered
as a general MTL with the constraints ğ‘“ğ‘–âˆ’1(ğ‘¥ğ‘—;ğœƒğ‘ ,ğœƒğ‘–âˆ’1)âˆ’
ğ‘“ğ‘–(ğ‘¥ğ‘—;ğœƒğ‘ ,ğœƒğ‘–)=ğ‘ƒ(ğ‘‡ğ‘–=0,ğ‘‡ğ‘–âˆ’1=1|ğ‘‹), which implies that the
difference of ğ‘“ğ‘–âˆ’1(ğ‘¥ğ‘—;ğœƒğ‘ ,ğœƒğ‘–âˆ’1)andğ‘“ğ‘–(ğ‘¥ğ‘—;ğœƒğ‘ ,ğœƒğ‘–)is the proba-
bility of the event ğ‘‡ğ‘–not occurring when the event ğ‘‡ğ‘–âˆ’1is
triggered. Considering the binary classification tasks and the
labelsğ‘œğ‘–
ğ‘—andğ‘œğ‘–âˆ’1
ğ‘—of the adjacent tasks, we can obtain the
label corresponding to the probability ğ‘ƒ(ğ‘‡ğ‘–=0,ğ‘‡ğ‘–âˆ’1=1|ğ‘‹)
as
Table 1. Labels corresponding to tasks Tğ‘–âˆ’1,Tğ‘–, and the de-
pendence relationship.
ğ‘œğ‘–
ğ‘—ğ‘œğ‘–âˆ’1
ğ‘—ğ‘œğ‘–
ğ‘—âˆ’ğ‘œğ‘–âˆ’1
ğ‘—ğ‘ƒ(ğ‘‡ğ‘–=0,ğ‘‡ğ‘–âˆ’1=1|ğ‘‹)
0 0 0 0
1 0 1 1
1 1 0 0
Therefore, according to Table 1, the label of the sequential
dependence between the adjacent tasks is equivalent to the
difference between ğ‘œğ‘–
ğ‘—andğ‘œğ‘–âˆ’1
ğ‘—, i.e.,ğ‘œğ‘–
ğ‘—âˆ’ğ‘œğ‘–âˆ’1
ğ‘—.
Since there exists the dependence relationship between the
previous and current tasks, i.e., T1â†’T 2â†’Tğ‘, the sample
space of the current task depends on that of the previous task.
In general, the sample space of the previous task contains
the sample space of the current one as shown in Fig. 2, which
leads to the data distribution discrepancy between these two
sample spaces. Consider the general CTR, CVR and CTCVR
estimation tasks, i.e., impression â†’clickâ†’Â·Â·Â·â†’ conversion.
We use the random variables ğ‘Œâˆˆ{0,1}andğ‘âˆˆ{0,1}to
denote the click event and the conversion event, respectively.
Then, CTR, CVR and CTCVR with feature input ğ‘‹are defined
asğ‘ƒ(ğ‘Œ|ğ‘‹),ğ‘ƒ(ğ‘|ğ‘Œ=1,ğ‘‹)andğ‘ƒ(ğ‘,ğ‘Œ=1|ğ‘‹), which satisfy
ğ‘ƒ(ğ‘=1,ğ‘Œ=1|ğ‘‹)=ğ‘ƒ(ğ‘Œ=1|ğ‘‹)ğ‘ƒ(ğ‘=1|ğ‘Œ=1,ğ‘‹).(5)
In this case, the training space of the traditional CVR esti-
mation task is generally determined by the samples with
ğ‘Œ=1in the CTR estimation task. However, for a new user,
there are no impression and click records. The conversion
rate estimation task is actually to estimate ğ‘ƒ(ğ‘=1|ğ‘‹). Con-
sideringğ‘ƒ(ğ‘=1,ğ‘Œ=0|ğ‘‹)=0[17] and according to (1), we
can obtain
ğ‘ƒ(ğ‘=1|ğ‘‹)=ğ‘ƒ(ğ‘=1,ğ‘Œ=1|ğ‘‹),
ğ‘ƒ(ğ‘=0|ğ‘‹)=ğ‘ƒ(ğ‘=0,ğ‘Œ=0|ğ‘‹)+ğ‘ƒ(ğ‘=0,ğ‘Œ=1|ğ‘‹).(6)

--- PAGE 4 ---
Conferenceâ€™17, July 2017, Washington, DC, USA Xuewen Tao, Mingming Ha, Qiongxu Ma, Hongwei Cheng, Wenfang Lin, and Xiaobo Guo
î‰€ 1î‰€ 2î‰€ 3î‰€ 4îˆ»
Figure 2. Distribution discrepancy of different task spaces
in SDMTL. The curved surface represents the distribution
Owith domain XÃ—T1Ã—Â·Â·Â·Ã— Tğ‘. The colored circles from
the outside to the inside denote the domains of tasks T1,T2,
T3, andT4, respectively.
According to (6), if the negative data points of the event ğ‘
only derived from the space with ğ‘Œ=1is used to predict
ğ‘ƒ(ğ‘=1|ğ‘‹), then the data distribution discrepancy between
training space and inference space leads to inaccurate pre-
dictions.
2.2 Distribution Dependence Relationship Between
Inference Space and Local Space
In this subsection, the relationship of expected losses be-
tween domains of adjacent tasks Tğ‘–âˆ’1andTğ‘–is established.
In tasksTğ‘–âˆ’1andTğ‘–, the sample space with data points {ğ‘¥ğ‘—âˆˆ
X,ğ‘œğ‘–âˆ’1
ğ‘—âˆˆ{0,1},ğ‘œğ‘–
ğ‘—âˆˆ{0,1}}is called inference space, i.e.,
entire space forTğ‘–âˆ’1andTğ‘–, and the sample space with data
points{ğ‘¥ğ‘—âˆˆX,ğ‘œğ‘–âˆ’1
ğ‘—âˆˆ{1},ğ‘œğ‘–
ğ‘—âˆˆ{0,1}}is called local space,
also called training space in some traditional CVR estimation
methods [ 15]. The distributions of inference and local spaces
are denoted asDandC, respectively.
Therefore, the objective of these two tasks Tğ‘–âˆ’1andTğ‘–with
sequential dependence in inference space is to minimize the
following expected loss:
ğ¸ğ‘‹,ğ‘‡ğ‘–âˆ’1,ğ‘‡ğ‘–âˆ¼D[ğ¿(ğ‘“ğ‘–âˆ’1(ğ‘‹),ğ‘‡ğ‘–âˆ’1)+ğ¿(ğ‘“ğ‘–(ğ‘‹),ğ‘‡ğ‘–)]
=ğ¸ğ‘‹,ğ‘‡ğ‘–âˆ’1âˆ¼D[ğ¿(ğ‘“ğ‘–âˆ’1(ğ‘‹),ğ‘‡ğ‘–âˆ’1)]+ğ¸ğ‘‹,ğ‘‡ğ‘–âˆ¼D[ğ¿(ğ‘“ğ‘–(ğ‘‹),ğ‘‡ğ‘–)]
=âˆ«
Dğ¿(ğ‘“ğ‘–âˆ’1(ğ‘¥),ğ‘¡ğ‘–âˆ’1)ğ‘ƒD(ğ‘¥,ğ‘¡ğ‘–âˆ’1)dğ‘¥dğ‘¡ğ‘–âˆ’1
+âˆ«
Dğ¿(ğ‘“ğ‘–(ğ‘¥),ğ‘¡ğ‘–)ğ‘ƒD(ğ‘¥,ğ‘¡ğ‘–)dğ‘¥dğ‘¡ğ‘–, (7)
whereğ‘ƒD(Â·,Â·)is the joint distribution in inference space. On
the other hand, if the model is trained in the local space C,
then the taskTğ‘–determines the sample distribution of the
taskTğ‘–âˆ’1. With this operation, the expected loss becomes thefollowing form:
ğ¸ğ‘‹,ğ‘‡ğ‘–âˆ’1âˆ¼D[ğ¿(ğ‘“ğ‘–âˆ’1(ğ‘‹),ğ‘‡ğ‘–âˆ’1)]+ğ¸ğ‘‹,ğ‘‡ğ‘–âˆ¼C[ğ¿(ğ‘“ğ‘–(ğ‘‹),ğ‘‡ğ‘–)]
=âˆ«
Dğ¿(ğ‘“ğ‘–âˆ’1(ğ‘¥),ğ‘¡ğ‘–âˆ’1)ğ‘ƒD(ğ‘¥,ğ‘¡ğ‘–âˆ’1)dğ‘¥dğ‘¡ğ‘–âˆ’1
+âˆ«
Cğ¿(ğ‘“ğ‘–(ğ‘¥),ğ‘¡ğ‘–)ğ‘ƒC(ğ‘¥,ğ‘¡ğ‘–)dğ‘¥dğ‘¡ğ‘–, (8)
whereğ‘ƒC(Â·,Â·)is the joint distribution in local space. Next,
the relationship between expected losses in (7) and (8) is
revealed.
Theorem 2.1. If the expected losses in the inference and local
spaces are defined as in (7) and (8), then, for any loss function
ğ¿(Â·,Â·), they satisfy
ğ¸ğ‘‹,ğ‘‡ğ‘–âˆ’1,ğ‘‡ğ‘–âˆ¼D[ğ¿(ğ‘“ğ‘–âˆ’1(ğ‘‹),ğ‘‡ğ‘–âˆ’1)+ğ¿(ğ‘“ğ‘–(ğ‘‹),ğ‘‡ğ‘–)]
=ğ¸ğ‘‹,ğ‘‡ğ‘–âˆ’1âˆ¼D[ğ¿(ğ‘“ğ‘–âˆ’1(ğ‘‹),ğ‘‡ğ‘–âˆ’1)]
+ğ¸ğ‘‹,ğ‘‡ğ‘–âˆ¼Ch
ğ‘ƒD(ğ‘‡ğ‘–âˆ’1=1)ğ‘ƒD(ğ‘‡ğ‘–|ğ‘‹)
ğ‘ƒD(ğ‘‡ğ‘–,ğ‘‡ğ‘–âˆ’1=1|ğ‘‹)ğ¿(ğ‘“ğ‘–(ğ‘‹),ğ‘‡ğ‘–)i
.
(9)
Obviously, the distribution shift also exists in CTR, CVR
and CTCVR estimations when they are trained in different
spaces.
3 The Adaptive Pattern Extraction
Multi-task Framework
The whole architecture of proposed APEM for sequential de-
pendence multi-task learning is illustrated in Figure 3. APEM
consists of two representation learning modules ASRG and
PS to dynamically extract implicit and explicit feature infor-
mation from a sample-wise view, and a sequential depen-
dence task learning loss to reconstruct an unbiased task re-
lationship on a global training space. Adaptive Sample-wise
Representation Generator (ASRG) is responsible for hier-
archical shared-representation learning, adopting inducing
points to interact with different feature field corresponding
to each input. Task Specific Adapter (PS) module cooperates
with ASRG but works as a task-ware information extractor
through designed task indicator and is with an independent
message passing structure to better solve the task conflict.
Besides those two, a sequence dependency learning loss be-
tween tasks is proposed and theoretical proved, which is
able to describe the conditional dependent probability for
sequential based multi-task learning from the whole training
space and consequently improve the prediction result by
precisely capturing the task relationship. We will elaborate
ASRG and PS in section 3.1 and 3.2, and lastly discuss the
relationship between sequence dependence tasks in section
3.3.

--- PAGE 5 ---
Adaptive Pattern Extraction Multi-Task Learning for Multi-Step Conversion Estimations Conferenceâ€™17, July 2017, Washington, DC, USA
TaskTowerğ‘–-1TaskTowerğ‘–
CONCATSequential dependency Learning
â€¦EmbeddingFeasFiled1Filed2Filednâ€¦AdaptiveSample-wise Representation Generator(ASRG)High-Order
InducingPointsTask Indicatorğ‘–Task Indicatorğ‘–-1Nâ„’!"#$%&â„’'"($%&)~ğ‘ƒ(y)|ğ‘¥))â„’'"($%&))*~ğ‘ƒ(y)"*|ğ‘¥)"*)ğ‘¦!y!"#â„’!"#$%&~ğ‘ƒâˆ†y)ğ‘¥)"*,ğ‘¥),âˆ†y)= y)"*âˆ’y)
TaskSpecific Adapter(TSA)TaskSpecific Adapter(TSA)
Figure 3. An illustration of the overall architecture of APEM.
3.1 Adaptive Sample-wise Representation
Generator
Fine-grained feature information extraction corresponding
to different tasks is crucial in multi-task learning and signifi-
cantly affects model performance. But feature generalization
also needs to be included to balance the trade-off between
tasks in terms of shared information. Based on these consid-
erations, we propose a novel representation learning mod-
ule, named Adaptive Sample-wise Representation Generator
(ASRG). Besides learning generalized shared-information,
we design a dynamic selector to learn the feature interac-
tion from a sample-wise view to further separates the task-
irrelevant info. The structure of ASRG is shown in Figure 4,
which mainly consists of an dynamic activation layer and a
feature interaction learning layer.
Dynamic Activation Layer . In recommendation scenario,
input field usually contains kinds of user and item features.
Given an input xfromğ¹different feature fields, we denote x
as the concatenation of all feature fields:
x=[ğ‘¥1,ğ‘¥2,...,ğ‘¥ğ¹], (10)
whereğ‘¥ğ‘–represents the value of the ğ‘–-th feature. As a com-
monly data preprocssing for online recommendation sce-
nario with better generalization, we discretize numerical
featuresğ‘¥ğ‘–through a Log-round operation to get an unique
AdaptiveSample-wise Representation Generator(ASRG)Multi-Head AttentionAdd & Norm
FeatureEmbeddingiInducing Points+Element-wiseQVK
0.80.10.60.30.70.50.20.90.10.60.70.31.00.00.00.01.00.00.01.00.00.01.00.0Dynamic Activation LayerFeatureEmbeddingo
ğ’™ğ’‡(ğ’™)ğœ¸=ğŸ		ğŸ!ğŸ’ğ’‡(ğ’™)ğ’™ğœ¸=ğŸ
ğ’‡ğ’™=ğŸ,ğ’™â‰¤ğœ¸ğŸâˆ’ğŸğœ¸ğŸğ’™ğŸ‘+ğŸ‘ğŸğœ¸ğ’™+ğŸğŸ,âˆ’ğœ¸ğŸ<ğ’™<ğœ¸ğŸğŸ,ğ’™â‰¥ğœ¸ğŸDynamicSelectorDynamicSelector
Transformation LayerFigure 4. The detail structure of Adaptive Sample-wise Rep-
resentation Generator
value, and randomly initialize it with a vector of ğ‘‘ğ‘“dimen-
sion. Thus, we obtain the input embedding for each feature
field asğ»=[â„1,â„2,...,â„ğ¹]T, whereğ»âˆˆRğ¹Ã—ğ‘‘ğ‘“.
A transformation Layer is first applied to project the input
embeddings into a ğ¾dimension vector. The transformation
layer can be any type of deep neural network structure and
here we chose a standard MLP layer just for simplicity. The
outputğ‘§ğ¾is defined as a dynamic selector:
ğ‘§ğ¾=MLP(ğ») (11)
whereğ‘§ğ¾âˆˆRğ¾. Then, we implement a dynamic activation
functionğ‘“ğ·inspired by [ 9] to get a sparser representation
ofğ‘§ğ¾, whose formulation is as follows:
ğ‘§ğ¾=ğ‘“ğ·(ğ‘§ğ¾) (12)
whereğ‘“ğ·is formulated as:
ğ‘“ğ·(ğ‘§)=ï£±ï£´ï£´ ï£²
ï£´ï£´ï£³0, ğ‘§ â‰¤âˆ’ğ›¾
2
âˆ’2
ğ›¾3ğ‘§3+3
2ğ›¾ğ‘§+1
2,âˆ’ğ›¾
2<ğ‘§<ğ›¾
2
1, ğ‘§ â‰¥ğ›¾
2(13)
whereğ›¾=ğ‘€ğ‘ğ‘¥{10âˆ’2ğ‘’-4Â·ğ‘ ğ‘¡ğ‘’ğ‘, 1ğ‘’-3}and maximum ğ‘ ğ‘¡ğ‘’ğ‘
during the training process is around 1ğ‘’6. Dynamic selector
ğ‘§ğ¾works as a information filter which selectively interacts
with input from the sample-wise view due to the Transfor-
mation Layer. As visualized in Figure 5, the output shape of
ğ‘“ğ·becomes steeper with the increase of training step. By

--- PAGE 6 ---
Conferenceâ€™17, July 2017, Washington, DC, USA Xuewen Tao, Mingming Ha, Qiongxu Ma, Hongwei Cheng, Wenfang Lin, and Xiaobo Guo
utilizingğ‘“ğ·,ğ‘§ğ¾creates ağ¾dimension sparse vector only con-
tains values of 0 and 1 corresponding to each input sample.
10.0
 7.5
 5.0
 2.5
 0.0 2.5 5.0 7.5 10.0
z0.000.250.500.751.00fD(z)Step=1
Step=25000
Step=100000
Figure 5. Output of the dynamic activation function ğ‘“ğ·with
the increase of training step.
Feature Interaction Learning Layer . Attention mecha-
nism for learning hierarchical feature interaction is gen-
erally adopted but requires quadratic time complexity in
standard self-attention structure. Here we design a learnable
matrix called inducing points ğ¼, enlightened from Set Trans-
former [ 12] to reduce the computational complexity from
quadratic to linear. We define the inducing points ğ¼âˆˆRğ¾Ã—ğ‘‘ğ‘“,
whereğ¾is the same as in dynamic selector ğ‘§ğ¾. After a
element-wise operation, we get a modified query Ë†ğ‘„as:
Ë†ğ‘„=ğ¼âŠ™ğ‘§ğ¾ (14)
Then, we calculate the output ğ‘‚ğ‘—from the attention opera-
tion according to the following formulation:
ğ‘‚ğ‘—=Attention(Ë†ğ‘„ğ‘—,ğ¾ğ‘—,ğ‘‰ğ‘—;ğœ†) (15)
where Ë†ğ‘„ğ‘—=ğ¼ğ‘—âŠ™ğ‘§ğ¾,ğ¾ğ‘—=ğ»ğ‘Šğ¾
ğ‘—,ğ‘‰ğ‘—=ğ»ğ‘Šğ‘‰
ğ‘—with trainable
parameterğœ†=n
ğ¼ğ‘—,ğ‘Šğ¾
ğ‘—,ğ‘Šğ‘‰
ğ‘—oğ‘š
ğ‘—=1andğ‘šrepresents the num-
ber of multi-head. Then we get output ğ‘‚from the multi-head
attention with parameter ğ‘Šğ‘‚as:
ğ‘‚=concat(ğ‘‚1,...,ğ‘‚â„)ğ‘Šğ‘‚(16)
Finally, the adaptive representation ğ‘Œğ´ğ‘†ğ‘…ğº learned from ASRG
can be formulated in a way of residual network:
ğ‘Œğ´ğ‘†ğ‘…ğº =LayerNorm(ğ‘‚,ğ») (17)
The time complexity of feature interaction learning layer
reduces from ğ‘‚(ğ¹2)toğ‘‚(ğ¾Ã—ğ¹)by introducing ğ¼. As sug-
gested in [ 18],ğ¾, the reduced dimension of ğ¼could be viewed
asğ¾independent memory cells interacting with each fea-
ture field, which is further automatically selected by ğ‘§ğ‘˜to
distinguish feature information explicitly from a sample-
wise view. Compared with traditional shared-representation
learning structure in most MTL methods, ASRG learns more
distinctive info in terms of a dynamic activation layer and
feature interaction learning layer, which is mainly attributed
to the former one combining a transformation layer and
dynamic activation function to generate an adaptive mask
corresponding to each input sample.3.2 Explicit Pattern Selector
Besides effective shared-representation generated by ASRG,
specific feature learning according to each task will strongly
affect the model performance since it directly enhances the
task-relevant information. In most MTL works, task-targeted
feature extractors, such as the task-specific experts proposed
by PLE are deliberately designed to learn the representa-
tion for each task. However, mutual interference between
different tasks still exists since the shared and task-specific
components are not completely separated in these cases.
In order to learn the task-aware information among differ-
ent tasks with a more independent and thoroughly separated
structure, we introduce a module named explicit Task Spe-
cific Adapters (PSs) as detail plotted in Figure 6. PS utilizes
parameterized task indicator vector to interact with previous
sample-wise common shared info from ASRG, which is able
to extract task-specific representation by directly optimizing
respective task object. The approach is similarly adopted in
PAL [21] and K-adapter [27].
Task Specific Adapter(TSA)
AttentionTask IndicatorQKVFeatureEmbeddingoAdd & Norm
Task AwareEmbeddingFeatureEmbeddingi
Figure 6. The detail structure of Pattern Selector.
As illustrated in Figure 6, we take the output ğ‘Œğ´ğ‘†ğ‘…ğºâˆˆ
Rğ¾Ã—ğ‘‘ğ‘“from the Adaptive Sample-wise Representation Gen-
erator to interact with a learnable task indicator vector ğ›¼ğ‘–
corresponding to each task ğ‘–. Theğ¹ğ‘–is the output calculated
through an attention operation between ğ‘Œğ´ğ‘†ğ‘…ğº andğ›¼ğ‘–as:
ğ¹ğ‘–=Attention(ğ›¼ğ‘–,ğ‘Œğ´ğ‘†ğ‘…ğº,ğ‘Œğ´ğ‘†ğ‘…ğº), (18)
whereğ›¼ğ‘–âˆˆR1Ã—ğ‘‘ğ‘“is the task indicator vector and ğ¹ğ‘–âˆˆR1Ã—ğ‘‘ğ‘“
denotes the middle output of task-aware representation for
each taskğ‘–correspondingly. Here, Attention is the same
attention calculation operation as in formula (15). Conse-
quently, for each task ğ‘–, we refer the task-specific informa-
tion generated from the ğ‘˜-th PS layer as ğ‘‡ğ‘˜
ğ‘–and we calculate
it also through a residual network and layer normalization
for training efficiency as in (17):
ğ‘‡ğ‘˜
ğ‘–=LayerNorm(ğ‘‡ğ‘˜âˆ’1
ğ‘–+ğ¹ğ‘˜
ğ‘–), (19)
whereğ‘‡ğ‘˜âˆ’1
ğ‘–âˆˆR1Ã—ğ‘‘ğ‘“means the output from the previous PS
layer for task ğ‘–, andğ¹ğ‘˜
ğ‘–âˆˆR1Ã—ğ‘‘ğ‘“is the task-aware embedding
learned by the interaction between the task indicator for the

--- PAGE 7 ---
Adaptive Pattern Extraction Multi-Task Learning for Multi-Step Conversion Estimations Conferenceâ€™17, July 2017, Washington, DC, USA
ğ‘˜-th layer with task ğ‘–and shared-common embedding. Note,
ğ‘‡0
ğ‘–is ignored at the first iteration.
It can be observed in Figure 6, task aware embedding ğ‘‡
obtained from Task Specific Adapters is trained indepen-
dently and whose message doesnâ€™t pass into ASRG module
among different layers. The proposed structure keeps the
implicit (from ASRG) and explicit (from PS) representation
learning modules more separated, which not only isolates
the negative interference between tasks more thoroughly but
also provides a extendable multi-task learning framework
especially necessary in industrial implementation.
3.3 Loss Function Design Towards Sequential
Dependence Multi-Task Learning
For the multi-task learning without sequential dependence,
the loss function is generally designed as the following form:
L(ğœƒğ‘ ,ğœƒ1,...,ğœƒğ‘)=ğ‘âˆ‘ï¸
ğ‘–=1ğ‘€âˆ‘ï¸
ğ‘—=1ğ‘¤ğ‘–
ğ‘€ğ¿
ğ‘“ğ‘–(ğ‘¥ğ‘—;ğœƒğ‘ ,ğœƒğ‘–),ğ‘œğ‘–
ğ‘—
.(20)
From the loss function (20), it can be observed that this loss
function cannot learn the sequential dependence relation-
ship. As mentioned in subsection 2.1, in this paper, the con-
strained optimization problem can be transformed into the
unconstrained case by using a penalty function. Therefore,
the corresponding loss function for SDMTL is designed as
L(ğœƒğ‘ ,ğœƒ1,...,ğœƒğ‘)
=Lğ‘€âˆ’ğ‘‡ğ‘ğ‘ ğ‘˜+Lğ·âˆ’ğ‘‡ğ‘ğ‘ ğ‘˜
=ğ‘âˆ‘ï¸
ğ‘–=1ğ‘¤ğ‘–
ğ‘€ğ‘€âˆ‘ï¸
ğ‘—=1ğ¿
ğ‘“ğ‘–(ğ‘¥ğ‘—;ğœƒğ‘ ,ğœƒğ‘–),ğ‘œğ‘–
ğ‘—
+ğ‘âˆ‘ï¸
ğ‘–=2ğœğ‘–âˆ’1
ğ‘€ğ‘€âˆ‘ï¸
ğ‘—=1ğ¿
ğ‘“ğ‘–âˆ’1(ğ‘¥ğ‘—;ğœƒğ‘ ,ğœƒğ‘–âˆ’1)âˆ’ğ‘“ğ‘–(ğ‘¥ğ‘—;ğœƒğ‘ ,ğœƒğ‘–),ğ‘œğ‘–âˆ’1
ğ‘—âˆ’ğ‘œğ‘–
ğ‘—
,
(21)
whereğœğ‘–is the penalty coefficients, Lğ‘€âˆ’ğ‘‡ğ‘ğ‘ ğ‘˜ andLğ·âˆ’ğ‘‡ğ‘ğ‘ ğ‘˜
are the loss functions of the main tasks, i.e., Tğ‘–, and the loss
functions of the sequential dependence relationship, respec-
tively. With this operation, each task and their corresponding
dependence relationship can be trained separately. The loss
functions of the dependence relationship Lğ·âˆ’ğ‘‡ğ‘ğ‘ ğ‘˜ can be
regarded as a regularization term. Note that the selection
of negative samples determines the training space. There-
fore, the positive samples of the task Tğ‘–are derived from the
current task while the negative samples of Tğ‘–are derived
from different tasks Tğ‘–andTğ‘–âˆ’1. Similar to the subsection
3.2, expected losses of the dependence relationship derived
from the entire space Dand local spaceCare discussed as
follows.
Theorem 3.1. If the dependence relationship is learned in the
entire spaceDand the local spaceC, respectively, and the corre-
sponding expected losses are denoted as ğ¸ğ‘‹,ğ‘‡ğ‘–âˆ’1,ğ‘‡ğ‘–âˆ¼D[ğ¿(ğ‘“ğ‘–âˆ’1(ğ‘‹)âˆ’ğ‘“ğ‘–(ğ‘‹),ğ‘‡ğ‘–âˆ’1âˆ’ğ‘‡ğ‘–)]andğ¸ğ‘‹,ğ‘‡ğ‘–âˆ’1,ğ‘‡ğ‘–âˆ¼C[ğ¿(ğ‘“ğ‘–âˆ’1(ğ‘‹)âˆ’ğ‘“ğ‘–(ğ‘‹),ğ‘‡ğ‘–âˆ’1âˆ’ğ‘‡ğ‘–)],
then these two expected losses satisfy
ğ¸ğ‘‹,ğ‘‡ğ‘–âˆ’1,ğ‘‡ğ‘–âˆ¼D[ğ¿(ğ‘“ğ‘–âˆ’1(ğ‘‹)âˆ’ğ‘“ğ‘–(ğ‘‹),ğ‘‡ğ‘–âˆ’1âˆ’ğ‘‡ğ‘–)]
=ğ¸ğ‘‹,ğ‘‡ğ‘–âˆ’1,ğ‘‡ğ‘–âˆ¼C"
ğ‘ƒD(ğ‘‡ğ‘–âˆ’1=1)ğ‘ƒD(ğ‘‡ğ‘–âˆ’1âˆ’ğ‘‡ğ‘–|ğ‘‹)
ğ‘ƒD(ğ‘‡ğ‘–âˆ’1âˆ’ğ‘‡ğ‘–,ğ‘‡ğ‘–âˆ’1=1|ğ‘‹)
Ã—ğ¿(ğ‘“ğ‘–âˆ’1(ğ‘‹)âˆ’ğ‘“ğ‘–(ğ‘‹),ğ‘‡ğ‘–âˆ’1âˆ’ğ‘‡ğ‘–)#
(22)
4 Experiments
In this section, we describe the experiments to evaluate the
performance of the proposed APEM framework, which are
conducted on both public benchmark dataset and real-world
industrial dataset in financial service. We also analyze the
contribution of each modules consisting of APEM to further
understand the working mechanism and demonstrate the
effectiveness of proposed method for sequential dependence
multi-task learning.
4.1 Experimental Setup
4.1.1 Datasets. Experiments are conducted on two dataset:
the public benchmark Ali-CCP and an industrial dataset from
the financial scenario.
â€¢Ali-CCP dataset:1The public dataset Ali-CPP is used
as the benchmark for model comparison with Alibaba
Click and Conversion Prediction tasks. We use all the
single-valued categorical features as generally adopted
and randomly take 10 %of the train dataset as the vali-
dation dataset for all models.
â€¢Industrial Dataset: The industrial dataset is collected
from our online recommendation platform in finan-
cial service, which describes usersâ€™ clicking and con-
version behavior responding to financial advertising.
The dataset is divided for training, validation and test
chronologically and we downsample the negative sam-
ples in the training set to keep the ratio of each set is
8:1:1.
4.1.2 Baseline Methods. To validate the effectiveness of
APEM, we conduct our experiments on the following repre-
sentative methods for comparison, which are SOTA multi-
task learning approaches or recent sequence dependency
learning method:
â€¢Single-Task is a three-layer MLP network with hid-
den layer size of [256,128,64] for single-task optimiza-
tion.
â€¢Shared-Bottom constructs a shared bottom layer to
learn the common representation across all tasks and
introduces separated task tower for the object opti-
mization respectively.
1https://tianchi.aliyun.com/dataset/dataDetail?dataId=408

--- PAGE 8 ---
Conferenceâ€™17, July 2017, Washington, DC, USA Xuewen Tao, Mingming Ha, Qiongxu Ma, Hongwei Cheng, Wenfang Lin, and Xiaobo Guo
Table 2. Detailed hyper-parameters settings for each dataset.
Dataset Hyper-parameters Settings
Ali-CCP ğµ=1024,ğ‘‘ğ‘“=18,ğ‘€=2,ğ¾=64,ğ¿=4,ğœ†=10âˆ’3
Industrial dataset ğµ=1024,ğ‘‘ğ‘“=18,ğ‘€=2,ğ¾=64,ğ¿=4,ğœ†=10âˆ’3
â€¢MMOE is inspired by the classic MoE method which
adopts a group of shared bottom subnetworks as ex-
perts and introduces gating network assigning differ-
ent tasks with distinctive weights.
â€¢PLE generalizes CGC method and employs a progres-
sive routing mechanism to extract and separate deeper
semantic knowledge.
â€¢AITM is a shared-bottom structure with adaptive in-
formation transfer for modeling sequential dependency
among multi-step conversions.
â€¢APEM is our proposed approach which adopts adap-
tive sample-wise representation generator and the ex-
plicit Pattern Selector, with dependency learning loss
for the sequence dependence multi-task learning.
4.1.3 Implementation of Lğ·âˆ’ğ‘‡ğ‘ğ‘ ğ‘˜.As discussed in sec-
tion 3.3,Lğ·âˆ’ğ‘‡ğ‘ğ‘ ğ‘˜ can be regarded as a regularization, which
constraints the relationship between sequential dependence
tasks during training process. In this paper, we constructs
a MSE (Mean-Squared Loss) as an implementation for ğ¿in
formula (21):
ğ¿ğ‘€ğ‘†ğ¸=1
ğ‘€ğ‘€âˆ‘ï¸
ğ‘—=1ğ‘¤Â·(ğ‘¦ğ‘—âˆ’Ë†ğ‘¦ğ‘—)2(23)
whereğ‘¦ğ‘—is label from ğ‘œğ‘–âˆ’1
ğ‘—âˆ’ğ‘œğ‘–
ğ‘—and Ë†ğ‘¦ğ‘—is the output from
ğ‘“ğ‘–âˆ’1(ğ‘¥ğ‘—;ğœƒğ‘ ,ğœƒğ‘–âˆ’1)âˆ’ğ‘“ğ‘–(ğ‘¥ğ‘—;ğœƒğ‘ ,ğœƒğ‘–)for inputğ‘—. Each sample is
equally treated with ğ‘¤=1.
4.1.4 Training Setup. In the experiments, each experi-
ment is repeated 5 times, the average performance and the
p value are both reported. We select the optimal hyper-
parameters for each model in terms of grid search [ 13] for
fair comparison. The batch size ğµon each datasets is set as
1024 respectively during the training process. Adam opti-
mizer [ 11] is applied with a learning rate ğœ†of 0.001. The
dimensionğ‘‘ğ‘“of input embedding layer is 18. The number
of the stacked layers ğ¿, the number of the attention heads
ğ‘€and the number of the inducing points ğ¾is illustrated
in Table 2. The activation function of MLP in single-task
modeling is ReLU.
4.2 Performance Comparison
The experimental results for all comparison methods with
the evaluation metric AUC for each task are presented in
Table 4. The best performance on different datasets are high-
lighted in boldface and underline for the best SOTA mehods.As can be observed, APEM outperforms most baseline mod-
els for each task on both datasets respectively.
The average performance on Ali-CCP dataset is poor both
on CTR and CVR targets on all compared methods, which
probably implies the input features are not qualified enough
to express the targets or the irrelevant information affects
significantly. For the latter case, task-specific feature extrac-
tion will play a key role to the prediction results in terms of
filtering negative interference. As observed, APEM achieves
0.6203 and 0.6456 of AUC for CTR and CVR tasks respectively,
with gains of 1.16 %and 7.31 %compared to the Singel-Task
method. The improvements of CTR is significant with com-
parison to other methods but slightly poorer than PLE in
the object of CVR. The result seems to be attributed to the
trade-off between tasks considered by APEM and APEM. The
difference improvements between CTR and CVR is smaller
in APEM and suggests a more balanced optimization among
tasks. The performance on the Industrial dataset of APEM
obtains an considerable gain of 1.29 %and 1.43 %for both
targets and significantly outperforms other methods. Com-
pared with PLE, which achieves a second best result, the
proposed model still gets an increase of the gain by 43 %and
55%and further demonstrates its effectiveness.
4.3 Ablation Study
We conduct ablation study on different submodules in APEM
in order to provide a detail analysis of its function and effi-
ciency. The variant models of APEM consists of following
structures and the notation is just for simplicity:
â€¢APEM without ASRG : removing the dynamic acti-
vation layer in ASRG and replacing with a standard
self attention operation.
â€¢APEM without PS : removing task indicator in PS
layers for all corresponding tasks.
â€¢APEM withoutLğ·âˆ’ğ‘‡ğ‘ğ‘ ğ‘˜: removing the sequence de-
pendence learning loss Lğ·âˆ’ğ‘‡ğ‘ğ‘ ğ‘˜ as denoted as in (21).
â€¢APEM : complete structure of APEM.
The results of ablation study are presented in Table ??with
an evaluation metric AUC on both datasets for CTR and
CVR tasks. As observed, the complete structure of APEM
outperforms all other APEM-variants and we can draw the
following conclusions for each submodule:
(1). Adaptive Sample-wise Representation Generator con-
tributes to learn fine-grained and generalized shared repre-
sentation for both tasks. In which, dynamic selector enables
to select essential information for each sample which en-
hance the knowledge learning. Without the dynamic selec-
tion layer, model performance drops most for both CTR and
CVR target as -0.5 %and -0.57 %in Industrial dataset. Fully
interaction learning via a standard multi-head self-attention
canâ€™t provide enough shared info. We believe that ASRG re-
construct the necessary information in an adaptive manner
which not only learns the feature field interaction but filter

--- PAGE 9 ---
Adaptive Pattern Extraction Multi-Task Learning for Multi-Step Conversion Estimations Conferenceâ€™17, July 2017, Washington, DC, USA
Table 3. The performance (AUC) comparison with baselines.The Gain means the mean AUC improvement compared with
Single-Task method. ** indicates that the improvement of the proposed APEM is statistically significant compared with the
best baseline at a p-value < 0.01 over paired samples t-test.
Models Ali-CPP Industrial Dataset
CTR CVR ğºğ‘ğ‘–ğ‘›ğ¶ğ‘‡ğ‘… ğºğ‘ğ‘–ğ‘›ğ¶ğ‘‰ğ‘… CTR CVR ğºğ‘ğ‘–ğ‘›ğ¶ğ‘‡ğ‘… ğºğ‘ğ‘–ğ‘›ğ¶ğ‘‰ğ‘…
Single-Task 0.6089 0.6011 â€“ â€“ 0.7081 0.7616 â€“ â€“
Shared-Bottom 0.6098 0.6225 0.15 % 3.56% 0.7050 0.7614 -0.37 % 0.11%
MMOE 0.6177 0.6223 1.45 % 3.53% 0.7134 0.7673 0.82 % 0.90%
PLE 0.6195 0.6355 1.74 % 5.72% 0.7140 0.7675 0.90 % 0.92%
AITM 0.6133 0.6391 0.72% 6.32% 0.7110 0.7667 0.47 % 0.81%
ESMM 0.6193 0.6333 1.71 % 5.36% 0.7154 0.7680 1.10 % 0.99%
ESCM2 0.6153 0.6258 1.05 % 4.11% 0.7146 0.7701 0.99% 1.26%
APEM 0.6198 0.6436** 1.79 % 7.07% 0.7167** 0.7714** 1.29 % 1.43%
Table 4. The performance (AUC) comparison of ablation study.
Models Ali-CPP Industrial Dataset
CTR CVR ğºğ‘ğ‘–ğ‘›ğ¶ğ‘‡ğ‘… ğºğ‘ğ‘–ğ‘›ğ¶ğ‘‰ğ‘… CTR CVR ğºğ‘ğ‘–ğ‘›ğ¶ğ‘‡ğ‘… ğºğ‘ğ‘–ğ‘›ğ¶ğ‘‰ğ‘…
APEM w/o ASRG 0.6178 0.6379 -0.32 % -0.89% 0.7131 0.7670 -0.50 % -0.57 %
APEM w/o PS 0.6158 0.6382 -0.65 % -0.84% 0.7141 0.7695 -0.36 % -0.25%
APEM w/oLğ·âˆ’ğ‘‡ğ‘ğ‘ ğ‘˜ 0.6199 0.6319 0.02 % -1.82 % 0.7160 0.7695 -0.10 % -0.25%
APEM 0.6198 0.6436 â€“ â€“ 0.7167 0.7714 â€“ â€“
the noise by utilizing a group-level attention from a sample-
wise view. The contribution in Ali-CPP is still obvious since
whose features seems less expressive as discussed in section
4.2 and is greatly benefited by ASRG.
(2). Explicit Pattern Selectors works as a task-sensitive fea-
ture extractor, which is quite crucial in the multi-task learn-
ing to precisely extract task-relevant information for each
task. It can be evidently observed that without the task at-
tention mechanism (proposed task indicator), model perfor-
mance drops dramatically and is slightly better than without
ASRG in Industrial dataset but worse in Ali-CPP. It is sug-
gested that a vanilla task-specific tower structure doesnâ€™t
generate enough information during task optimizing process.
(3). The proposed sequence dependence learning loss Lğ·âˆ’ğ‘‡ğ‘ğ‘ ğ‘˜
based on theoretical proof contributes to the model perfor-
mance in terms of the additional information passing among
related tasks. Although it seems less significant compared to
other submodules in Industrial dataset but contributes most
in the CVR task in Ali-CPP. CVR task probably depends on
CTR task heavily and Lğ·âˆ’ğ‘‡ğ‘ğ‘ ğ‘˜ modifies the biased object
of original definition, which further optimizes the param-
eters by recovering their complete probability-dependent
relationship.
4.4 Analysis of Dynamic Selector
Dynamic selector ğ‘§ğ¾defined in formula (12) functions as a
sparse mask generated based on the input sample, which
cooperates with Inducing points ğ¼interacting with feature
fields selectively. We conduct several case studies of ğ‘§ğ¾to
provide an intuitive analysis as visualized in Figure 7.
0.50 0.55 0.60 0.65 0.70
(a)0.0%10.0%20.0%30.0%40.0%
6
 4
 2
 0 2 4
(b)4
2
024Low
Mid
HighFigure 7. An illustration of dynamic selector ğ‘§ğ¾.
(a).Distribution of the selection rate for different samples.
(b). Plot of sample embeddings with high, middle and
low selection rate is colored in green, yellow and blue
respectively.
As noted in section 3.1 , ğ‘§ğ¾is ağ¾dimension vector only
with values of 0 and 1, where 1 means interacting with im-
plicit field group in terms of ğ¼correspondingly and vice versa.
We first plot the distribution of the non-zero rate (selection
rate) ofğ‘§ğ¾on the test samples of industrial dataset in Figure 7

--- PAGE 10 ---
Conferenceâ€™17, July 2017, Washington, DC, USA Xuewen Tao, Mingming Ha, Qiongxu Ma, Hongwei Cheng, Wenfang Lin, and Xiaobo Guo
Table 5. The offline performances (AUC) comparison for
two real-world financial scenes.
Models Scene 1 Scene 2
CTR CVR CTR CVR
MMOE 0.8102 0.8034 0.8110 0.8719
APEM 0.8102 0.8072 0.8123 0.8773
Gain - 0.47% 0.16% 0.62%
(a). It can be observed for most samples, the selection rate is
between 55 %and 60 %and indicates that more than half the
interaction groups are required for information extraction.
For specific cases, some needs just less interaction groups
and some needs more. We could regard this as a multi-view
representations for each sample, such as different numbers
of perspectives qualified enough to describe a customerâ€™s
interest specially on online recommendation scenario. We
further randomly plots sample embeddings with high (top
1%), mid (around 58 %) and low (bottom 1 %) selection rates in
Figure 7 (b). As illustrated, samples with different interaction
degrees show significant difference in embedding space and
probably implying distinctive intentions. .
4.5 Efficiency Evaluation
In this section, we evaluate time and storage efficiency of our
proposed method. We record the time cost during the train-
ing (per epoch) and inference process for APEM and other
baseline models in Figure 8 (a), and their respective memory
cost in Figure 8 (b). As illustrated in (a), APEM requires 1177
seconds for training an epoch on the Ali-CCP dataset with 38
millions samples, which is less efficient than other methods
(295s for the best results from Shared-Bottom) but similar to
the PLE (1195s). Since we generally train the model in an of-
fline manner especially for large-scale data, relatively higher
training efficiency is within tolerance. On the inference time,
the essential factor considered in the online industrial appli-
cation, APEM spends 47 seconds for forward propagation on
the test data with 4.2 millions samples. Its deviation from top
performance models like AITM and Shared-Bottom is 12 sec-
onds and it is acceptable considering most industrial online
inference situationâ€™ QPS threshold. Besides, APEM has the
least parameters with 89 Mb (178 Mb for the largest model
of Single-Task) as plotted in Figure 8 (b), which makes it eas-
ily deployed and portable. In a conclusion, APEM achieves
an significant improvement with an appropriate computa-
tional time and advantageous storage capacity compared
with other approved MTL methods.
4.6 Online A/B Performance
We implement an online A/B test between APEM and SOTA
multi-task learning method of MMOE for one week. Two
models are deployed on two real-world financial advertising
scenarios with the objective of maximizing the CVR for the
PLE TAFE MMOE Single-Task AITM Shared-Bottom
(a)0250500750100012501500train cost time(s)train time(s)
020406080
inference cost time(s)inference time(s)
Single-Task PLE MMOE AITM Shared-Bottom TAFE
(b)050100150Size(Mb)Figure 8. Efficiency comparison among models. (a). Training
and Inference time (b). Memory Cost
financial products as investment and credit loan. The offline
comparison is presented in Table 5, APEM gets an improve-
ment of 0.16 %for CTR in Scene 2, 0.47 %and 0.62 %for CVR
on each scene correspondingly. In Figure 9, we can observe
the online performances of APEM compared with MMOE.
As shown in the plot, APEM achieves significant and con-
sistent improvements for both scenarios during the whole
period, average increase of 9.22 %in scenario (a) and 3.76 %in
scenario (b) on the CVR task. The experiment result proves
the efficiency and the stability of proposed mehtod, which is
qualified enough for large-scale industrial application.
5 Related Work
Multi-task Learning. Multi-task Learning (MTL) is pro-
posed to learn the shared information among tasks to im-
prove the model generalization and performance [ 2]. How-
ever, multi-task learning scenario usually suffers from the
performance deterioration as negative transfer because of the
complex relationship between different tasks [ 1,23]. There-
fore, much feature learning works in structure designing
are proposed for necessary information extraction accord-
ing to specific task and balancing the performances among
all tasks. Cross-Stitch Network [ 16] use a linear combina-
tion of shared representations to learn the task-specific em-
beddings for each task. Based on the idea of Cross-Stitch,
Sluice Network [ 19] is a generalized meta-architecture with
more task-specific parameters by dividing each layer into
task-specific and shared subspaces and achieves better per-
formance specially for less correlated tasks. However, these
approaches could not capture the sample dependence and

--- PAGE 11 ---
Adaptive Pattern Extraction Multi-Task Learning for Multi-Step Conversion Estimations Conferenceâ€™17, July 2017, Washington, DC, USA
Monday Tuesday Wednesday Thursday Friday Saturday Sunday
(a)0.0020.0040.0060.0080.0100.0120.0140.016CVR on Scene 1TAFE
MMoE
Monday Tuesday Wednesday Thursday Friday Saturday Sunday
(b)0.0040.0060.0080.0100.0120.0140.0160.018CVR on Scene 2TAFE
MMoE
Figure 9. Online A/B results on two real-world scenarios (a)
and (b).
require more training data and less efficient for large-scale
application. Inspired by the MoE [ 10] structure, multi-gate
Mixture-of-Experts (MMoE) [ 14] employs an ensemble of
experts submodules and gating network to combine the rep-
resentation of the bottom experts to learn the task relation-
ship while consuming less computation. Similarly, Multiple
Relational Attention Network (MRAN) [ 32] models multiple
relationships by three attention-based learning mechanism.
Compared with MMoE, Progressive Layered Extraction (PLE)
method [ 22] propose a novel MTL framework which sepa-
rates task-common and task-specific parameters more explic-
itly and adopts a progressive separation routing mechanism
to better alleviate parameter conflicts caused by complex
task correlation.
Sequential Dependence Multi-task Learning. The most
classical applications of the sequential dependence MTL
(SDMTL) are the multi-step conversion process of customer
acquisition in e-commerce, display advertising or finance
systems. In general, the multi-step conversion process in-
volves impressionâ†’clickâ†’Â·Â·Â·â†’ conversion, which cor-
responds to several estimation tasks like post-view click-
through rate (CTR), post-click conversion rate (CVR) and
post-view click-through & conversion rate (CTCVR) estima-
tions and so fourth. Differing from the general MTL, there
exist dependence relationships between the adjacent tasks
in the SDMTL problem. For the CVR estimation problem,
Entire Space Multi-task Model (ESMM) is proposed in [ 15] to
overcome Sample Selection Bias (SSB) and Data Sparsity (DS)
issues by introducing two auxiliary tasks of predicting CTRand CTCVR. With this operation, the performance of the
CVR estimation will depend heavily on the performance aux-
iliary tasks. As the number of steps increases in multi-step
conversion path, the accumulation of performance errors
becomes intolerable. Aimed at the DS problem of the CVR
estimation, in [ 29], a novel user sequential behavior graph is
established to achieve post-click behavior decomposition by
inserting disjoint purchase-related deterministic action and
other action into between click and conversion. Considering
micro behaviors (userâ€™s interactions with items) and macro
behaviors (userâ€™s interactions with specific components on
the item detail page) of users, Wen et al. [ 28] propose a Hierar-
chically Modeling both Micro and Macro behaviors for CVR
prediction to address SSB and DS issues by using the abun-
dant supervisory labels from micro and macro behaviors. To
models the sequential dependence among multi-step con-
versions, Adaptive Information Transfer Multi-task (AITM)
framework with adaptive information transfer module is de-
veloped in [ 30] to directly predict the end-to-end conversion
probabilities of each step. Besides, causal approaches have
also been applied to achieve the debiasing post-click con-
version rate estimation lately [ 6,8,26,31]. However, for the
sequential dependence multi-task learning problem, there is
rare literature to develop a formalization description.
6 Conclusions
In this paper, we propose a sequence dependence multi-task
learning framework named as Adaptive Pattern Extraction
Multi-task (APEM) framework, which could selectively re-
construct implicit shared representations from a sample-
wise view and extract explicit task-specific information in
an more efficient way compared with common task-aware
tower structure. We accomplish this by involving an Adap-
tive Sample-wise Representation Generator and a Pattern
Selector. For the multi-task learning with dependency gen-
erally encountered in E-commence online recommendation,
we provide a detail theoretical proof about the dependent
relationship from rigorous mathematical perspective. Based
on our analysis, we design a dependence task learning loss to
complete optimizing object in an unbiased format. The per-
formance gains of APEM compared to several SOTA multi-
task approaches on both public and real-world industrial
datasets demonstrates its effectiveness and generalization
characteristics. Besides, we carefully conduct ablation study,
case study, efficiency evaluation and online A/B test to fur-
ther analyze the contributions from different modules and
its applicability for large-scale industrial scenarios.
7 Appendices
7.1 Proof for Theorem 2.1
Proof. Considering the definitions of the inference and local
spaces, and their corresponding expected losses given in (7)

--- PAGE 12 ---
Conferenceâ€™17, July 2017, Washington, DC, USA Xuewen Tao, Mingming Ha, Qiongxu Ma, Hongwei Cheng, Wenfang Lin, and Xiaobo Guo
and (8), we can obtain
ğ‘ƒC(ğ‘‹,ğ‘‡ğ‘–)=ğ‘ƒD(ğ‘‹,ğ‘‡ğ‘–|ğ‘‡ğ‘–âˆ’1=1) (24)
in the sense that the joint distribution of ğ‘‹andğ‘‡ğ‘–inCis
equivalent to, under ğ‘‡ğ‘–âˆ’1=1, the joint distribution of ğ‘‹and
ğ‘‡ğ‘–inD.
According to (24) and the definition of ğ¸ğ‘‹,ğ‘‡ğ‘–âˆ¼D[ğ¿(ğ‘“ğ‘–(ğ‘‹),ğ‘‡ğ‘–)],
the second term in the right-hand side of (9) satisfies
ğ¸ğ‘‹,ğ‘‡ğ‘–âˆ¼Ch
ğ‘ƒD(ğ‘‡ğ‘–âˆ’1=1)ğ‘ƒD(ğ‘‡ğ‘–|ğ‘‹)
ğ‘ƒD(ğ‘‡ğ‘–,ğ‘‡ğ‘–âˆ’1=1|ğ‘‹)ğ¿(ğ‘“ğ‘–(ğ‘‹),ğ‘‡ğ‘–)i
=ğ¸ğ‘‹,ğ‘‡ğ‘–âˆ¼Chğ‘ƒD(ğ‘‡ğ‘–âˆ’1=1)
ğ‘ƒD(ğ‘‡ğ‘–âˆ’1=1|ğ‘‹,ğ‘‡ğ‘–)ğ¿(ğ‘“ğ‘–(ğ‘‹),ğ‘‡ğ‘–)i
=âˆ«
Cğ‘ƒD(ğ‘‡ğ‘–âˆ’1=1)
ğ‘ƒD(ğ‘‡ğ‘–âˆ’1=1|ğ‘¥,ğ‘¡ğ‘–)ğ¿(ğ‘“ğ‘–(ğ‘¥),ğ‘¡ğ‘–)ğ‘ƒC(ğ‘¥,ğ‘¡ğ‘–)dğ‘¥dğ‘¡ğ‘–
=âˆ«
Dğ¿(ğ‘“ğ‘–(ğ‘¥),ğ‘¡ğ‘–)ğ‘ƒD(ğ‘‡ğ‘–âˆ’1=1)
ğ‘ƒD(ğ‘‡ğ‘–âˆ’1=1|ğ‘¥,ğ‘¡ğ‘–)ğ‘ƒD(ğ‘¥,ğ‘¡ğ‘–|ğ‘‡ğ‘–âˆ’1=1)dğ‘¥dğ‘¡ğ‘–
=âˆ«
Dğ¿(ğ‘“ğ‘–(ğ‘¥),ğ‘¡ğ‘–)ğ‘ƒD(ğ‘‡ğ‘–âˆ’1=1)
ğ‘ƒD(ğ‘‡ğ‘–âˆ’1=1|ğ‘¥,ğ‘¡ğ‘–)ğ‘ƒD(ğ‘¥,ğ‘¡ğ‘–,ğ‘‡ğ‘–âˆ’1=1)
ğ‘ƒD(ğ‘‡ğ‘–âˆ’1=1)dğ‘¥dğ‘¡ğ‘–
=âˆ«
Dğ¿(ğ‘“ğ‘–(ğ‘¥),ğ‘¡ğ‘–)ğ‘ƒD(ğ‘¥,ğ‘¡ğ‘–)dğ‘¥dğ‘¡ğ‘–
=ğ¸ğ‘‹,ğ‘‡ğ‘–âˆ¼D[ğ¿(ğ‘“ğ‘–(ğ‘‹),ğ‘‡ğ‘–)]. (25)
Therefore, equations (7), (8) and (25) imply that the relation-
ship (9) holds. â–¡
7.2 Proof for Theorem 3.1
Proof. According to Bayesâ€™ theorem, we can obtain the fol-
lowing equality:
ğ‘ƒD(ğ‘‡ğ‘–âˆ’1=1)ğ‘ƒD(ğ‘‡ğ‘–âˆ’1âˆ’ğ‘‡ğ‘–|ğ‘‹)
ğ‘ƒD(ğ‘‡ğ‘–âˆ’1âˆ’ğ‘‡ğ‘–,ğ‘‡ğ‘–âˆ’1=1|ğ‘‹)=ğ‘ƒD(ğ‘‡ğ‘–âˆ’1=1)
ğ‘ƒD(ğ‘‡ğ‘–âˆ’1=1|ğ‘‹,ğ‘‡ğ‘–âˆ’1âˆ’ğ‘‡ğ‘–).
(26)
Considering the right-hand side of (22) and (26), it leads to
ğ¸ğ‘‹,ğ‘‡ğ‘–âˆ’1,ğ‘‡ğ‘–âˆ¼C"
ğ‘ƒD(ğ‘‡ğ‘–âˆ’1=1)ğ¿(ğ‘“ğ‘–âˆ’1(ğ‘‹)âˆ’ğ‘“ğ‘–(ğ‘‹),ğ‘‡ğ‘–âˆ’1âˆ’ğ‘‡ğ‘–)
ğ‘ƒD(ğ‘‡ğ‘–âˆ’1=1|ğ‘‹,ğ‘‡ğ‘–âˆ’1âˆ’ğ‘‡ğ‘–)#
=âˆ«
C"
ğ‘ƒD(ğ‘‡ğ‘–âˆ’1=1)
ğ‘ƒD(ğ‘‡ğ‘–âˆ’1=1|ğ‘¥,ğ‘¡ğ‘–âˆ’1âˆ’ğ‘¡ğ‘–)ğ¿(ğ‘“ğ‘–âˆ’1(ğ‘¥)âˆ’ğ‘“ğ‘–(ğ‘¥),ğ‘¡ğ‘–âˆ’1âˆ’ğ‘¡ğ‘–)
ğ‘ƒC(ğ‘¥,ğ‘¡ğ‘–âˆ’1âˆ’ğ‘¡ğ‘–)#
dğ‘¥dğ‘¡ğ‘–dğ‘¡ğ‘–
=âˆ«
D"
ğ‘ƒD(ğ‘‡ğ‘–âˆ’1=1)
ğ‘ƒD(ğ‘‡ğ‘–âˆ’1=1|ğ‘¥,ğ‘¡ğ‘–âˆ’1âˆ’ğ‘¡ğ‘–)ğ‘ƒD(ğ‘¥,ğ‘¡ğ‘–âˆ’1âˆ’ğ‘¡ğ‘–|ğ‘‡ğ‘–âˆ’1=1)
Ã—ğ¿(ğ‘“ğ‘–âˆ’1(ğ‘¥)âˆ’ğ‘“ğ‘–(ğ‘¥),ğ‘¡ğ‘–âˆ’1âˆ’ğ‘¡ğ‘–)#
dğ‘¥dğ‘¡ğ‘–âˆ’1dğ‘¡ğ‘–
=âˆ«
Dğ¿(ğ‘“ğ‘–âˆ’1(ğ‘¥)âˆ’ğ‘“ğ‘–(ğ‘¥),ğ‘¡ğ‘–âˆ’1âˆ’ğ‘¡ğ‘–)ğ‘ƒD(ğ‘¥,ğ‘¡ğ‘–âˆ’1âˆ’ğ‘¡ğ‘–)dğ‘¥dğ‘¡ğ‘–âˆ’1dğ‘¡ğ‘–
=ğ¸ğ‘‹,ğ‘‡ğ‘–âˆ’1,ğ‘‡ğ‘–âˆ¼D[ğ¿(ğ‘“ğ‘–âˆ’1(ğ‘‹)âˆ’ğ‘“ğ‘–(ğ‘‹),ğ‘‡ğ‘–âˆ’1âˆ’ğ‘)], (27)
which implies that (22) holds. The proof is completed. â–¡References
[1]Jonathan Baxter. 1997. A Bayesian/information theoretic model of
learning to learn via multiple task sampling. Machine learning 28, 1
(1997), 7â€“39.
[2]Rich Caruana. 1997. Multitask learning. Machine learning 28, 1 (1997),
41â€“75.
[3]Ling Chen, Donghui Chen, Fan Yang, and Jianling Sun. 2021. Neural
episodic control. In A deep multi-task representation learning method
for time series classification and retrieval . Information Sciences, 17â€“32.
[4]Michael Crawshaw. 2020. Multi-task learning with deep neural net-
works: A survey. arXiv preprint arXiv:2009.09796 (2020).
[5]Hongliang Fei, Jingyuan Zhang, Xingxuan Zhou, Junhao Zhao,
Xinyang Qi, and Ping Li. 2021. GemNN: gating-enhanced multi-task
neural networks with feature interaction learning for CTR predic-
tion. In Proceedings of the 44th International ACM SIGIR Conference on
Research and Development in Information Retrieval . 2166â€“2171.
[6]Tiankai Gu, Kun Kuang, Hong Zhu, Jingjie Li, Zhenhua Dong, Wenjie
Hu, Zhenguo Li, Xiuqiang He, and Yue Liu. 2021. Estimating true
post-click conversion via group-stratified counterfactual inference.
[7]Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang
He. 2017. DeepFM: a factorization-machine based neural network for
CTR prediction. arXiv preprint arXiv:1703.04247 (2017).
[8]Siyuan Guo, Lixin Zou, Yiding Liu, Wenwen Ye, Suqi Cheng,
Shuaiqiang Wang, Hechang Chen, Dawei Yin, and Yi Chang. 2021.
Enhanced doubly robust learning for debiasing post-click conversion
rate estimation. In Proceedings of the 44th International ACM SIGIR
Conference on Research and Development in Information Retrieval . 275â€“
284.
[9]H. Hazimeh, Z. Zhao, A. Chowdhery, M. Sathiamoorthy, and E. H. Chi.
2021. DSelect-k: Differentiable Selection in the Mixture of Experts
with Applications to Multi-Task Learning. (2021).
[10] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E
Hinton. 1991. Adaptive mixtures of local experts. Neural computation
3, 1 (1991), 79â€“87.
[11] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980 (2014).
[12] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi,
and Yee Whye Teh. 2019. Set transformer: A framework for attention-
based permutation-invariant neural networks. In International Confer-
ence on Machine Learning . PMLR, 3744â€“3753.
[13] PM Lerman. 1980. Fitting segmented regression models by grid search.
Journal of the Royal Statistical Society: Series C (Applied Statistics) 29, 1
(1980), 77â€“84.
[14] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H
Chi. 2018. Modeling task relationships in multi-task learning with
multi-gate mixture-of-experts. In Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining . 1930â€“
1939.
[15] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang
Zhu, and Kun Gai. 2018. Entire space multi-task model: An effective
approach for estimating post-click conversion rate. In The 41st Interna-
tional ACM SIGIR Conference on Research & Development in Information
Retrieval . 1137â€“1140.
[16] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert.
2016. Cross-stitch networks for multi-task learning. In Proceedings of
the IEEE conference on computer vision and pattern recognition . 3994â€“
4003.
[17] Conor Oâ€™Brien, Kin Sum Liu, James Neufeld, Rafael Barreto, and
Jonathan J Hunt. 2021. An Analysis Of Entire Space Multi-Task Models
For Post-Click Conversion Prediction. In Fifteenth ACM Conference on
Recommender Systems . 613â€“619.
[18] Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adria Puig-
domenech Badia, Oriol Vinyals, Demis Hassabis, Daan Wierstra, and

--- PAGE 13 ---
Adaptive Pattern Extraction Multi-Task Learning for Multi-Step Conversion Estimations Conferenceâ€™17, July 2017, Washington, DC, USA
Charles Blundell. 2017. Neural episodic control. In International Con-
ference on Machine Learning . PMLR, 2827â€“2836.
[19] Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, and Anders
SÃ¸gaard. 2019. Latent multi-task architecture learning. In Proceedings
of the AAAI Conference on Artificial Intelligence , Vol. 33. 4822â€“4829.
[20] Jiayi Shen, Xiantong Zhen, Marcel Worring, and Ling Shao. 2021.
Variational multi-task learning with gumbel-softmax priors. Advances
in Neural Information Processing Systems 34 (2021), 21031â€“21042.
[21] Asa Cooper Stickland and Iain Murray. 2019. Bert and pals: Projected
attention layers for efficient adaptation in multi-task learning. In In-
ternational Conference on Machine Learning . PMLR, 5986â€“5995.
[22] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Pro-
gressive layered extraction (PLE): A novel multi-task learning (MTL)
model for personalized recommendations. In Fourteenth ACM Confer-
ence on Recommender Systems . 269â€“278.
[23] Partoo Vafaeikia, Khashayar Namdar, and Farzad Khalvati. 2020. A
Brief Review of Deep Multi-task Learning and Auxiliary Task Learning.
arXiv preprint arXiv:2007.01126 (2020).
[24] Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, Dengxin
Dai, and Luc Van Gool. 2020. Revisiting multi-task learning in the
deep learning era. arXiv preprint arXiv:2004.13379 2 (2020).
[25] Fangye Wang, Yingxu Wang, Dongsheng Li, Hansu Gu, Tun Lu,
Peng Zhang, and Ning Gu. 2022. Enhancing CTR Prediction with
Context-Aware Feature Representation Learning. arXiv preprint
arXiv:2204.08758 (2022).
[26] Hao Wang, Tai-Wei Chang, Tianqiao Liu, Jianmin Huang, Zhichao
Chen, Chao Yu, Ruopeng Li, and Wei Chu. 2022. ESCM2: Entire Space
Counterfactual Multi-Task Model for Post-Click Conversion Rate Esti-
mation. arXiv preprint arXiv:2204.05125 (2022).[27] Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang,
Guihong Cao, Daxin Jiang, Ming Zhou, et al .2020. K-adapter: Infusing
knowledge into pre-trained models with adapters. arXiv preprint
arXiv:2002.01808 (2020).
[28] Hong Wen, Jing Zhang, Fuyu Lv, Wentian Bao, Tianyi Wang, and Zu-
long Chen. 2021. Hierarchically modeling micro and macro behaviors
via multi-task learning for conversion rate prediction. In Proceedings
of the 44th International ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval . 2187â€“2191.
[29] Hong Wen, Jing Zhang, Yuan Wang, Fuyu Lv, Wentian Bao, Quan
Lin, and Keping Yang. 2020. Entire space multi-task modeling via
post-click behavior decomposition for conversion rate prediction. In
Proceedings of the 43rd International ACM SIGIR conference on research
and development in Information Retrieval . 2377â€“2386.
[30] Dongbo Xi, Zhen Chen, Peng Yan, Yinger Zhang, Yongchun Zhu,
Fuzhen Zhuang, and Yu Chen. 2021. Modeling the sequential depen-
dence among audience multi-step conversions with multi-task learning
in targeted display advertising. In Proceedings of the 27th ACM SIGKDD
Conference on Knowledge Discovery & Data Mining . 3745â€“3755.
[31] Wenhao Zhang, Wentian Bao, Xiao-Yang Liu, Keping Yang, Quan Lin,
Hong Wen, and Ramin Ramezani. 2020. Large-scale causal approaches
to debiasing post-click conversion rate estimation with multi-task
learning. In Proceedings of The Web Conference 2020 . 2775â€“2781.
[32] Jiejie Zhao, Bowen Du, Leilei Sun, Fuzhen Zhuang, Weifeng Lv, and
Hui Xiong. 2019. Multiple relational attention network for multi-
task learning. In Proceedings of the 25th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining . 1123â€“1131.
