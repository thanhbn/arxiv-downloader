# 2208.05379.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multi-task/2208.05379.pdf
# File size: 930333 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Multi-task Active Learning for Pre-trained Transformer-based Models
Guy Rotman and Roi Reichart
Faculty of Industrial Engineering and Management, Technion, IIT
grotman@campus.technion.ac.il
roiri@technion.ac.il
Abstract
Multi-task learning, in which several tasks
are jointly learned by a single model, allows
NLP models to share information from mul-
tiple annotations and may facilitate better
predictions when the tasks are inter-related.
This technique, however, requires annotat-
ing the same text with multiple annotation
schemes which may be costly and laborious.
Active learning (AL) has been demonstrated
to optimize annotation processes by itera-
tively selecting unlabeled examples whose
annotation is most valuable for the NLP
model. Yet, multi-task active learning (MT-
AL) has not been applied to state-of-the-art
pre-trained Transformer-based NLP models.
This paper aims to close this gap. We ex-
plore various multi-task selection criteria in
three realistic multi-task scenarios, reﬂect-
ing different relations between the partici-
pating tasks, and demonstrate the effective-
ness of multi-task compared to single-task
selection. Our results suggest that MT-AL
can be effectively used in order to minimize
annotation efforts for multi-task NLP mod-
els.1 2
1 Introduction
Deep neural networks (DNNs) have recently
achieved state-of-the-art results for many nat-
ural language processing (NLP) tasks and ap-
plications. Of particular importance are con-
textualized embedding models (McCann et al.,
2017; Peters et al., 2018), most of which im-
plement transformer-based architectures with the
self-attention mechanism (Vaswani et al., 2017;
Devlin et al., 2019; Raffel et al., 2020).
1Our code base is available at: https://github.
com/rotmanguy/MTAL .
2Accepted for publication in Transactions of the Associ-
ation for Computational Linguistics (TACL), 2022. Pre-MIT
Press publication versionNevertheless, DNNs often require large labeled
training sets in order to achieve good performance.
While annotating such training sets is costly and
laborious, the active learning (AL) paradigm aims
to minimize these costs by iteratively selecting
valuable training examples for annotation. Re-
cently, AL has been shown effective for DNNs
across various NLP tasks (Duong et al., 2018;
Peris and Casacuberta, 2018; Ein-Dor et al., 2020).
An appealing capability of DNNs is perform-
ing multi-task learning (MTL): Learning multiple
tasks by a single model (Ruder, 2017). This stems
from their architectural ﬂexibility – constructing
increasingly deeper and wider architectures from
basic building blocks – and in their gradient-based
optimization which allows them to jointly up-
date parameters from multiple task-based objec-
tives. Indeed, MTL has become ubiquitous in NLP
(Luan et al., 2018; Liu et al., 2019a).
MTL models for NLP can often beneﬁt from us-
ing corpora annotated for multiple tasks, particu-
larly when these tasks are closely-related and can
inform each other. Prominent examples of multi-
task corpora include OntoNotes (Hovy et al.,
2006), the Universal Dependencies Bank (Nivre
et al., 2020) and STREUSLE (Schneider et al.,
2018). Given the importance of multi-task corpora
for many MTL setups, effective AL frameworks
that support MTL are becoming crucial.
Unfortunately, most AL methods do not support
annotations for more than one task. Multi-task AL
(MT-AL) was proposed by Reichart et al. (2008)
before the neural era, and adapted by Ikhwantri
et al. (2018) to a neural architecture. Recently,
Zhu et al. (2020) proposed an MT-AL model for
slot ﬁlling and intent detection, focusing mostly
on LSTMs (Hochreiter and Schmidhuber, 1997).
In this paper, we are the ﬁrst to systematically
explore MT-AL for large pre-trained Transformer
models. Naturally, our focus is on closely-related
NLP tasks, for which multi-task annotation of thearXiv:2208.05379v2  [cs.CL]  28 Oct 2022

--- PAGE 2 ---
same corpus is likely to be of beneﬁt. Particu-
larly, we consider three challenging real-life multi-
task scenarios, reﬂecting different relations be-
tween the participating NLP tasks: 1. Comple-
menting tasks, where each task may provide valu-
able information to the other task: Dependency
parsing (DP) and named entity recognition (NER);
2. Hierarchically-related tasks, where one of the
tasks depends on the output of the other: Relation
extraction (RE) and NER; and 3. Tasks with differ-
ent annotation granularity: Slot ﬁlling (SF, token
level) and intent detection (ID, sentence level). We
propose various novel MT-AL methods and tailor
them to the speciﬁc properties of the scenarios, in
order to properly address the underlying relations
between the participating tasks. Our experimen-
tal results highlight a large number of patterns that
can guide NLP researchers when annotating cor-
pora with multiple annotation schemes using the
AL paradigm.
2 Previous Work
This paper addresses a previously unexplored
problem: multi-task AL (MT-AL) for NLP with
pre-trained Transformer-based models. We hence
start by covering AL in NLP and then proceed
with multi-task learning (MTL) in NLP.
2.1 Active Learning in NLP
AL has been successfully applied to a variety of
NLP tasks, including semantic parsing (Duong
et al., 2018), syntactic parsing (Reichart and Rap-
poport, 2009; Li et al., 2016), co-reference res-
olution (Li et al., 2020), named entity recogni-
tion (Shen et al., 2017) and machine translation
(Haffari et al., 2009), to name a few. Recent
works demonstrated that models like BERT can
beneﬁt from AL in low-resource settings (Ein-Dor
et al., 2020; Grießhaber et al., 2020), and Bai et al.
(2020) suggested basing the AL selection crite-
rion on linguistic knowledge captured by BERT.
Other works performed cost-sensitive AL, where
instances may have different costs (Tomanek and
Hahn, 2010; Xie et al., 2018). However, most pre-
vious works did not apply AL for MTL, which is
our main focus.
2.2 Multi-task Learning (MTL) in NLP
MTL has become increasingly popular in NLP,
particularly when the solved tasks are closely-
related (Chen et al., 2018; Saﬁ Samghabadi et al.,2020; Zhao et al., 2020). In some cases, the
MTL model is trained in a hierarchical fashion,
where information is propagated from lower-level
(sometimes auxiliary) tasks to higher-level tasks
(Søgaard and Goldberg, 2016; Rotman and Re-
ichart, 2019; Sanh et al., 2019; Wiatrak and Iso-
Sipila, 2020). In other cases, different labeled cor-
pora can be merged to serve as multi-task bench-
marks (McCann et al., 2017; Wang et al., 2018).
This way, a single MTL model can be trained on
multiple tasks, which are typically only distantly-
related. This research considers the setup of
closely-related tasks where annotating a single
corpus w.r.t. multiple tasks is a useful strategy.
3 Task Deﬁnition - Multi-task Active
Learning (MT-AL)
In the MT-AL setup, the AL algorithm is provided
with a textual corpus, where an initial (typically
small) set of n0examples is labeled for ttasks.
The AL algorithm implements an iterative pro-
cess, where at the i-th iteration the goal of the AL
algorithm is to select niadditional unlabeled ex-
amples that will be annotated on all ttasks, such
that the performance of the base NLP model will
be improved as much as possible with respect to
all of them. While such a greedy strategy of gain-
ing the most in the i-th iteration may not yield the
best performance in subsequent iterations, most
AL algorithms are greedy, and we hence follow
this strategy here as well.
We focus on the standard setup of conﬁdence-
based AL, where unlabeled examples with the
lowest model conﬁdence are selected for annota-
tion. Algorithm 1 presents a general sketch of such
AL algorithms, in the context of MTL. This frame-
Algorithm 1 Multi-task Conﬁdence-based Active Learn-
ing (Conﬁdence-based MT-AL)
Input: Labeled data L(annotated on ttasks), Unlabeled data
U
Algorithm:
Fori= 1; : : : ;T:
1. Train a multi-task learning (MTL) model honL.
2. For each u2Ucalculate its aggregated conﬁdence
score Ch(u)on all ttasks according to h.
3. Choose the niunlabeled examples from Uwith the
lowest conﬁdence score Ch(u)and send them for an-
notation according to all ttasks.
4. Add the newly labeled examples to Land remove them
fromU.

--- PAGE 3 ---
work, ﬁrst introduced by Reichart et al. (2008), is
a simple generalization of the single-task AL (ST-
AL) framework, which supports the annotation of
data with respect to multiple tasks.
As discussed in §1, we explore several vari-
ations of the MT-AL setup: Independent tasks
that inform each other (§5), hierarchically-related
tasks, where one task depends on the output of
the other (§6), and tasks with different annotation
granularity, word and sentence level (§7). Before
we can introduce the MT-AL algorithms for each
of these setups, we ﬁrst need to lay their shared
foundations: The single-task and multi-task model
conﬁdence scores.
4 Conﬁdence Estimation in Single-task
and Multi-task Active Learning
We now introduce the conﬁdence scores that we
consider for single-task (ST-AL) and multi-task
(MT-AL) active learning. These conﬁdence scores
are essentially the core of conﬁdence-based AL al-
gorithms (see Steps 2-3 of Algorithm 1). In Table
1 we provide a summary of the various ST-AL and
MT-AL selection methods we explore.
4.1 Single-task Conﬁdence Scores
We consider three conﬁdence scores that have
been widely used in ST-AL:
Random ( ST-R )This baseline method simply as-
signs random scores to the unlabeled examples.
Entropy-based Conﬁdence ( ST-EC )The single-
task entropy-based conﬁdence score is deﬁned as:
ST-EC (x) = 1 E(x): (1)
For sentence classiﬁcation tasks such as ID,
E(x) is simply the entropy over the class predic-
tions of a sample xdivided by the log number of
labels. In our token classiﬁcation tasks (DP, NER,
RE and SF), E(x) is the normalized sentence-level
entropy (Kim et al., 2006), which allows us to es-
timate the uncertainty of the model for a given se-
quence of tokens x= (x1:::xm):
E(x) = 1
mlogsmX
i=1sX
j=1p(yjjxi) logp(yjjxi);
(2)
wheremis the number of tokens, yjis thej’th
possible label, and sis the number of labels. We
perform entropy normalization by averaging the
token-level entropies, in order to mitigate the ef-
fect of the sentence length, and by dividing thescore by the log number of labels. The resulting
conﬁdence score ranges from 0 to 1, where lower
values indicate lower certainty.
Dropout Agreement ( ST-DA )Ensemble meth-
ods have proven effective for AL (see, e.g., (Seung
et al., 1992; Settles and Craven, 2008)) . In this pa-
per, we derive a conﬁdence score inspired by Re-
ichart and Rappoport (2007). We start by creating
k= 10 different models by performing dropout
inference forktimes (Gal and Ghahramani, 2016).
We then compute the single-task dropout agree-
ment score for a sentence xby calculating the av-
erage token-level agreement across model pairs:
ST-DA (x) =1
mk(k 1)X
j6=j0mX
i=11f^yj
i=^yj0
ig;
(3)
where ^yj
iis the predicted label of model jfor the
i’th token. The resulting scores range from 0 to 1,
where lower values indicate lower certainty.3
4.2 Multi-task Conﬁdence Scores
When deriving conﬁdence scores for MT-AL,
multiple design choices should be made. First,
the conﬁdence score of a multi-task model can be
based on both tasks or only on one of them. We
denote with MT-EC andMT-DA the conﬁdence
scores that are equivalent to ST-EC andST-DA :
The only (important) difference is that they are
calculated for a multi-task model. For clarity, we
will augment this notation with the name of the
task according to which the conﬁdence is calcu-
lated. For example, ST-EC-NER andMT-EC-NER
are the EC scores calculated using the named en-
tity recognition (NER) classiﬁer of a single-task
and a multi-task model, respectively.
We can hence evaluate MT-AL algorithms on
cross-task selection, i.e., when the evaluated task
is different from the task used for computing the
conﬁdence scores (and hence for sample selec-
tion). For example, evaluating the performance
of a multi-task model, trained jointly on NER and
DP, on the DP task when the conﬁdence scores
used by the MT-AL algorithm are only based on
the NER classiﬁer ( MT-EC-NER ).
We also consider a family of conﬁdence scores
for MT-AL that are computed with respect to all
participating tasks (joint-selection scores). For
this aim, we consider three simple aggregation
3For sentence classiﬁcation tasks, ST-DA is computed
similarly, without averaging over the tokens.

--- PAGE 4 ---
schemes using the average, maximum, or min-
imum operators over the single-task conﬁdence
scores. For example, the multi-task average con-
ﬁdence ( MT-AVG ) averages for a sample xthe
entropy-based conﬁdence scores over all ttasks:
MT-AVG (x) =1
ttX
i=1MT-EC-i (x); (4)
The multi-task average dropout agreement
score ( MT-AVGDA ) is similarly deﬁned, but the
averaging is over the MT-DA scores. Finally, the
multi-task maximum (minimum) MT-MAX (MT-
MIN ) is computed in a similar manner to MT-AVG
but with the max (min) operator taken over the
task-speciﬁc conﬁdence entropies.
Beyond Direct Manipulations of Conﬁdence
Scores Since our focus in this paper is on multi-
task selection, we would like to consider addi-
tional selection methods which go beyond the sim-
ple methods in previous work. The common prin-
ciple of these methods is that they are less sensi-
tive to the actual values of the conﬁdence scores
and instead consider the relative importance of the
example to the participating tasks.
First, we consider MT-PAR , which is based on
the Pareto-efﬁcient frontier (Lotov and Miettinen,
2008). We start by representing each unlabeled
sample as a t-dimensional space vector c, where
ci=ST-EC-i is the ST conﬁdence score for task
i. Next, we select all samples for which the cor-
responding vector belongs to the Pareto-efﬁcient
frontier. A point belongs to the frontier if for ev-
ery other vector c0the following holds: 1. 8i2
[t];cic0
iand 2.9i2[t];ci<c0
i. If the number
of samples in the frontier is smaller than the total
number of samples to select ( n), we re-iterate the
procedure by removing the vectors of the selected
samples and calculating the next Pareto points. If
there are still ppoints to be selected but the num-
ber of the ﬁnal Pareto points ( f) exceedsp, we
select everybf
pcpoint, ordered by the ﬁrst axis.
Next, inspired by the ﬁeld of information re-
trieval we propose MT-RRF . This method allows
us to consider the rank of each example with re-
spect to the participating tasks, rather than the ac-
tual conﬁdence values. We ﬁrst calculate ri, the
ranked list of the i-th task, by ranking the exam-
ples according to their ST-EC-i scores, from low-
est to highest. We next fuse the resulting tranked
lists into a single ranked list R, using the recipro-
cal rank fusion (RRF) technique (Cormack et al.,
MT-MIN MT-MAX MT-AVG MT-PAR MT-RRFMT-MAX
MT-AVG
MT-PAR
MT-RRF
MT-IND0.16
0.84 0.3
0.57 0.47 0.71
0.72 0.25 0.8 0.73
0.72 0.2 0.77 0.68 0.94
0.00.20.40.60.81.0
Figure 1 : The percentage of shared selected sam-
ples between pairs of MT-AL selection methods
(see experimental details in the text).
2009). The RRF score of an example xis com-
puted as:
RRF -Score (x) =tX
i=11
k+ri(x); (5)
wherekis a constant, set to 60, as in the original
paper. The ﬁnal ranking is computed over the RRF
scores of the examples – from highest to lowest.
Higher-ranked examples are chosen ﬁrst for anno-
tation as they have lower conﬁdence scores. Fi-
nally, MT-IND independently selects the bn
tcmost
uncertain samples according to each task by rank-
ing the ST-EC scores and re-iterating if overlaps
occur.
We ﬁnally compare the selected samples of six
of the MT-AL methods, after training a multi-task
model for a single AL iteration on the DP and
NER tasks (Figure 1). It turns out that while some
of the methods tend to choose very similar exam-
ple subsets for annotation (e.g., MT-IND andMT-
RRF share 94% of the selected samples, and MT-
AVG andMT-MIN share 84% of them), others sub-
stantially differ in their selection (e.g., MT-MAX
shares only 16% of its selected samples with MT-
MIN and 20% with MT-IND ). This observation en-
courages us to continue investigating the impact of
the various selection methods on MT-AL.
5 MT-AL for Complementing Tasks
We start by investigating MT-AL for two closely-
related, complementing, syntactic tasks: Depen-
dency Parsing (DP) and Named Entity Recogni-
tion (NER), which are often solved together by a

--- PAGE 5 ---
Selection Method DescriptionParticipating Tasks
for TrainingParticipating Tasks
for Selection
ST-R Single-task random selection One None
ST-EC Single-task entropy-based conﬁdence One One
ST-DA Single-task dropout agreement One One
MT-R Multi-task random selection All None
MT-EC Multi-task entropy-based conﬁdence All One
MT-DA Multi-task dropout agreement All One
MT-AVG Multi-task average entropy-based conﬁdence All All
MT-AVGDA Multi-task average dropout agreement All All
MT-MAX Multi-task maximum entropy-based conﬁdence All All
MT-MIN Multi-task minimum entropy-based conﬁdence All All
MT-PAR Multi-task Pareto entropy-based conﬁdence All All
MT-RRF Multi-task Reciprocal Rank Fusion entropy-based conﬁdence All All
MT-IND Multi-task independent selection entropy-based conﬁdence All All
Table 1 : Summary of the ST-AL and MT-AL selection methods explored in this paper.
joint multi-task model (Finkel and Manning, 2009;
Nguyen and Nguyen, 2021).
5.1 Research Questions
We focus on three research questions. At ﬁrst,
we would like to establish whether MT-AL meth-
ods are superior to ST-AL methods for multi-task
learning. Our ﬁrst two questions are hence: Q1.1 :
Is multi-task learning effective in this setup? and
Q1.2 : Is AL effective? If so, which AL strategy is
better: ST-AL or MT-AL?
Next, notice that in MT-AL the conﬁdence score
of an example can be based on one or more of
the participating tasks. That is, even if the base
model for which training examples are selected
is an MTL model, the conﬁdence scores used by
the MT-AL algorithm can be based on one task or
more (§4.2). Our third question is thus: Q1.3 : Is it
better to calculate conﬁdence scores based on one
of the participating tasks, or should we consider a
joint conﬁdence score, based on both tasks?4
5.2 Data
We consider the English version of the OntoNotes
5.0 corpus (Hovy et al., 2006), consisting of
seven textual domains: broadcast conversation
(BC), broadcast news (BN), magazine (MZ), news
(NW), bible (PT), telephone conversation (TC)
and web (WB). Sentences are annotated with
constituency-parse trees, named entities, part-of-
speech tags, as well as other labels. We convert
constituency-parse trees to dependency trees using
4This question naturally generalizes when more than two
tasks are involved.the ElitCloud conversion tool.5We do not report
results in the PT domain, as it is not annotated for
NER. Table 2 summarizes the number of sentences
per split for the OntoNotes domains, as well as for
the additional datasets used in our next setups.
5.3 Models
We consider two model types: Single-task and
multi-task models. Our single-task model (ST)
consists of the 12-layer pre-trained BERT-base en-
coder (Devlin et al., 2019), followed by a task de-
coder. At ﬁrst, we implemented a simple multi-
task model (SMT), consisting of a shared 12-layer
pre-trained BERT-base encoder followed by an in-
dependent decoder for each task. However, early
results suggested that it is inferior to single-task
modeling. We therefore implemented a more com-
plex multi-task model (CMT), illustrated in Figure
2. This model consists of (shared) cross-task and
task-speciﬁc modules, similar in nature to the ar-
chitecture proposed by Lin et al. (2018). In partic-
ular, it uses the 8 bottom BERT layers as shared
cross-task layers and employs t+ 1replications of
the 4 top BERT layers, one replication for each
task, as well as a shared cross-task replication.
The input text, as encoded by the shared 8 layers,
eS
1:8, is passed through the shared and non-shared
4-layer modules, eS
8:12andeUi
8:12, respectively. The
task classiﬁers are then fed with the output of the
cross-task layers combined with the output of their
task-speciﬁc layers, following the gating mecha-
5https://github.com/elitcloud/
elit-java .

--- PAGE 6 ---
DP-NER NER-RE SF-ID
BC BN MZ NW TC WB NYT24 NYT29 ScieRC WebNLG WLP ATIS SNIPS
Train 11,877 10,681 6,771 34,967 12,889 15,639 56,193 63,305 1,540 4,973 6,690 4,478 13,084
Dev 2,115 1,293 640 5,894 1,632 2,264 5,000 7,033 217 500 2,320 500 700
Test 2,209 1,355 778 2,325 1,364 1,683 5,000 4,006 451 689 2,343 893 700
Table 2 : Data statistics. We report the number of sentences in the original splits for each pair of tasks.
nism of Rotman and Reichart (2019):
ai(x) =(Wi
g[eS
8:12(x);eUi
8:12(x)] +bi
g);
gi(x) =ai(x)eS
8:12(x) + (1 ai(x))eUi
8:12(x);
where ;is the concatenation operator, is the
element-wise product, is the Sigmoid function,
andWi
gandbi
gare the gating mechanism parame-
ters. The combined vector gi(x)is then fed to the
i-th task-speciﬁc decoder.6
All implementations are based on Hugging-
Face’s Transformers package (Wolf et al., 2020).7
For all models, the DP decoder is based on the Bi-
afﬁne parser (Dozat and Manning, 2017) and the
NER decoder is a simple linear classiﬁer.
5.4 Training and Hyper-parameter Tuning
We consider the following hyper-parameters for
the AL experiments. At ﬁrst, we randomly sample
2% of the original training set to serve as the ini-
tial labeled examples in all experiments and treat
the rest of the training examples as unlabeled. We
also ﬁx our development set to be twice the size
of our initial training set, by randomly sampling
examples from the original development set. We
then run each AL method for 5 iterations, where
at each iteration, the algorithm selects an unla-
beled set of the size of its initial training set (that
is, 2% of the original training set) for annotation.
We then reveal the labels of the selected examples
and add them to the training set of the next iter-
ation. At the beginning of the ﬁnal iteration, our
labeled training set consists of 10% of the original
training data.
In each iteration, we train the models with 20K
gradient steps with an early stopping criterion ac-
cording to the development set. We report LAS
scores for DP and F1 scores for NER. For DP, we
measure our AL conﬁdence scores on the unla-
beled edges. When performing multi-task learn-
ing, we set the stopping criterion as the geometric
6We considered several other parameter-sharing schemes
but witnessed lower performance.
7https://github.com/huggingface/
transformers .
.
.
.
NLP
is
Shared 
Transformer 
Layer
Shared 
Transformer 
Layer
X 
8
Shared 
Transformer 
Layer
Shared 
Transformer 
Layer
X 
4
.
.
.
NER 
Transformer 
Layer
NER 
Transformer 
Layer
X 
4
.
.
.
DP 
Transformer 
Layer
DP 
Transformer 
Layer
X 
4
DP 
Gate
DP 
Decoder
NER 
Decoder
great
NER 
Gate
.
.
Figure 2 : Our complex multi-task model architec-
ture for DP and NER.
Full Training Active Learning
DP NER DP NER
Avg Best Avg Best Avg Best Avg Best
ST (CE) 87.17 1/6 70.35 0/6 86.43 3/6 74.65 6/6
SMT (CE) 86.94 2/6 67.51 0/6 85.86 2/6 70.31 0/6
CMT (CE) 87.04 3/6 72.79 6/6 85.91 1/6 72.11 0/6
ST (LS) 87.64 0/6 71.31 1/6 88.96 0/6 75.61 2/6
SMT (LS) 86.87 0/6 69.07 0/6 87.53 0/6 73.26 1/6
CMT (LS) 87.98 6/6 72.86 5/6 89.03 6/6 74.44 3/6
Table 3 : A comparison of a single-task model (ST),
a simple multi-task model (SMT), and our com-
plex multi-task model (CMT) in full training and
active learning. Models were trained with the
cross-entropy (CE) or label smoothing (LS) losses,
on all OntoNotes domains.
mean of the task scores (F1 for NER and LAS for
DP). We optimize all parameters using the ADAM
optimizer (Kingma and Ba, 2015) with a weight
decay of 0.01, a learning rate of 5e-5, and a batch
size of 32. For label smoothing (see below), we
use= 0:2. Following Dror et al. (2018), we
use the t-test for measuring statistical signiﬁcance
(p-value = 0:05).
5.5 Results
Model Architecture (Q1.1) We would ﬁrst like
to investigate the performance of the single-task
and multi-task models in the full training (FT)
and, more importantly, in the active learning (AL)

--- PAGE 7 ---
setups. We hence compare three architectures:
The single-task model (ST), the simple multi-task
model (SMT), and our complex multi-task model
(CMT). We train each model for DP and for NER
on the six OntoNotes domains using the cross-
entropy (CE) objective function, or with the label
smoothing objective (LS (Szegedy et al., 2016))
that has been demonstrated to decrease calibration
errors of Transformer models (Desai and Durrett,
2020; Kong et al., 2020). ST-AL is performed with
ST-EC and MT-AL with MT-AVG .
Table 3 reports the average scores (Avg col-
umn) over all domains and the number of do-
mains where each model achieved the best results
(Best). The results raise three important observa-
tions. First, SMT is worse on average than ST in
all setups, suggesting that vanilla MT is not al-
ways better than ST training . Second, our CMT
model achieves the best scores in most cases . The
only case where it is inferior to ST, but not to SMT,
is on AL with CE training. However, when train-
ing with LS, it achieves results comparable to or
higher than those of ST on AL.
Third, when comparing CE to LS training, LS
clearly improves the average scores of all models
(besides one case). Interestingly, the improvement
is more signiﬁcant in the AL setup than in the FT
setup. We report that when expanding these exper-
iments to all AL selection methods, LS was found
very effective for both tasks, outperforming CE in
most comparisons, with an average improvement
of 1.8% LAS for DP and 0.9 F1 for NER.
Multi-task vs. Single-task Performance (Q1.2)
We next ask whether MT-AL outperforms strong
ST-AL baselines. Figure 3 presents for every task
and domain the performance of the per-domain
best ST-AL and MT-AL methods after the ﬁnal AL
iteration. Following our observations in Q1.1, we
train all models with the LS objective and base the
multi-task models on the effective CMT model.
Although there is no single method, MT-AL or
ST-AL, which performs best across all domains
and tasks, MT-AL seems to perform consistently
better. The ﬁgure suggests that MT-AL is effec-
tive for both tasks, outperforming the best ST-AL
methods in 4 of 6 DP domains (results are not sta-
tistically signiﬁcant, the average p-value is 0.19)
and in 5 of 6 NER domains (results for 3 do-
mains are statistically signiﬁcant). While the av-
erage gap between MT-AL and ST-AL is small for
DP (0.28% LAS), in NER it is as high as 2.4 F1
BC BN MZ NW TC WB Average878889909192LASST-AL
MT-AL(a) Dependency Parsing
BC BN MZ NW TC WB Average5560657075808590F1ST-AL
MT-AL
(b) Named Entity Recognition
Figure 3 : Performance of the best ST-AL vs. the
best MT-AL method per domain (Q1.2).
points in favor of MT-AL. In fact, for half of the
NER domains, this gap is greater than 4.2 F1.
When comparing individual selection methods,
MT-AVG , and multi-task DP-based entropy, MT-
EC-DP , are the best selection methods for DP,
with average scores of 89.03% and 88.99%, re-
spectively. Single-task DP-based entropy, ST-EC-
DPis third, with an average score of 88.96%,
while the second best ST-AL method, ST-DA-
DP, is ranked only ninth among all methods, out-
performed by seven different MT-AL methods.
For NER, multi-task NER-based entropy, MT-EC-
NER , is the best model with an average F1 score
of 77.13, followed by MT-MAX with an average F1
score of 75.90. The single-task NER-based meth-
ods, ST-EC-NER andST-DA-NER are ranked only
ﬁfth and sixth both with an average score of 75.60.
These results provide an additional indication of
the superiority of MT-AL.
In terms of the MT-AL selection methods that
do not perform a simple aggregation, MT-PAR
andMT-RRF perform similarly, averaging 88.31%
LAS for DP and 75.88 F1 for NER, while MT-IND
achieves poor results for DP and moderate results
for NER (an overall comparison of the MT-AL
methods is provided in x8)).
We next compare the per-iteration performance

--- PAGE 8 ---
1000 1500 2000 2500 3000 350088899091LAS
NW - DP
MT-EC-DP
ST-EC-DP
MT-R
1000 1500 2000 2500 3000 3500
Number of Sentences8082848688F1
NW - NER
MT-EC-NER
ST-EC-NER
MT-RFigure 4 : Performance as a function of the number
of training examples (Q1.2).
Within-task Cross-task Average
DP NER DP NER DP + NER
MT-AL winning % 47.22 63.88 90.74 89.81 78.78
Table 4 : A comparison of MT-AL vs. ST-AL on
within-task, cross-task and average performance.
Values indicate the percentage of comparisons in
which MT-AL methods were superior.
of ST-AL and MT-AL. To this end, Figure 4
presents the performance for the most prominent
MT-AL and ST-AL methods: MT-EC-DP andST-
EC-DP for DP and MT-EC-NER andST-EC-NER
for NER, together with the multi-task random se-
lection method MT-R . We plot for each method its
task score on the NW domain (the one with the
largest dataset) as a function of the training set
size, corresponding to 2% to 10% of the original
training examples. Clearly, the MT-AL methods
are superior across all AL iterations, indicating
the stability of MT-AL as well as its effectiveness
in low-resource setups . Similar patterns are also
observed in the other domains.
As a ﬁnal evaluation for Q1.2, we directly com-
pare pairs of MT-AL and ST-AL methods, per-
forming three comparison types on each of the do-
mains: Within-task : Comparing the performance
ofST-EC-i andST-DA-i to their MT-AL counter-
parts ( MT-EC-i and MT-DA-i ) and to the joint-
selection methods on task i, either DP or NER
(108 comparisons); Cross-task : Comparing theDP NER Average
ST-EC-DP 88.96 71.34 80.15
ST-EC-NER 86.66 75.75 81.21
MT-EC-DP 88.99 73.75 81.37
MT-EC-NER 86.90 77.13 82.01
MT-AVG 89.02 74.44 81.74
MT-MAX 88.64 75.90 82.27
Table 5 : A comparison of AL methods that base
their selection on a single-task vs. joint-task con-
ﬁdence scores. Results are averaged across the
OntoNotes domains (Q1.3).
performance of ST-EC-i andST-DA-i to their MT-
AL counterparts and to the joint-selection meth-
ods on the opposite task (e.g., if the models select
according to DP, we evaluate the NER task; 108
comparisons). This comparison allows us to eval-
uate the effect of single- and multi- task model-
ing on cross-task performance. Since single-task
models cannot be directly applied to the opposite
task, we record the examples selected by the ST-
AL method and train a model for the opposite task
on these examples; and Average : Comparing all
ST-AL methods to all MT-AL methods according
to their average performance on both tasks (264
comparisons).
Table 4 reports the percentage of comparisons
where the MT-AL methods are superior. On aver-
age, the two method types are on par when com-
paring Within-task performance. More interest-
ingly, for Cross-task performance MT-AL meth-
ods are clearly superior with around 90% win-
nings (87% of the cases statistically signiﬁcant).
Finally, the Average also supports the superior-
ity of MT-AL methods which perform better in
79% of the cases (all results are statistically signif-
icant). These results demonstrate the superiority
of MT-AL, particularly (and perhaps unsurpris-
ingly) when both tasks are considered.
Single-task vs. Joint-task Selection (Q1.3)
Next, we turn to our third question which com-
pares single-task vs. joint-task conﬁdence scores.
That is, we ask whether MT-AL methods that base
their selection criterion on more than one task are
better than ST-AL and MT-AL methods that com-
pute conﬁdence scores using a single task only.
To answer this question, we compare the two
best ST-AL and MT-AL methods that are based on
single-task selection to the two best joint-task se-

--- PAGE 9 ---
0.80 0.85 0.90 0.95 1.00
Confidence0.00.10.20.30.40.50.60.70.80.91.0AccuracyR squared = 0.10
(a) DP
0.900 0.925 0.950 0.975 1.000
Confidence0.00.10.20.30.40.50.60.70.80.91.0AccuracyR squared = 0.13
 (b) NER
Figure 5 : Sentence-level accuracy as a function of
entropy-based conﬁdence, for DP (left) and for
NER (right), when training with the CE objective.
The heat maps represent the point frequency.
lection MT-AL methods. As previously, all meth-
ods employ the LS objective. Table 5 reports the
average scores (across domains) of each of these
methods for DP, NER, and the average task score,
based on the ﬁnal AL iteration.
While the method that performs best on aver-
age on both tasks is MT-MAX , a joint-selection
method, the second best method is MT-EC-NER ,
a single-task selection method, and the gap is
only 0.26 points. Not surprisingly, performance is
higher when the evaluated task also serves as the
task that the conﬁdence score is based on, either
solely or jointly with another task.
Although the joint-selection methods are ef-
fective for both tasks, we cannot decisively con-
clude that they are better than MT-AL methods
that perform single-task selection. However, we
do witness another conﬁrmation for our answer to
Q1.2, as all presented MT-AL methods perform
better on average on both tasks (the Average col-
umn) than the ST-AL methods.
Overconﬁdence Analysis Originally, we
trained our models with the standard CE loss.
However, our early experiments suggested
that such CE-based training yields overconﬁ-
dent models, which is likely to severely harm
conﬁdence-based AL methods. While previous
work demonstrated the positive impact of label
smoothing (LS) on model calibration, to the best
of our knowledge, the resulting impact on multi-
task learning has not been explored, speciﬁcally
not in the context of AL. We next analyze this
impact, which is noticeable in our above results,
in more detail.
Figure 5 presents sentence-level conﬁdence
scores as a function of sentence-level accuracyDP
BC BN MZ NW TC WB Average
ST-EC-DP (CE) 0.0720 0.0672 0.0757 0.0506 0.0584 0.3064 0.1051
ST-EC-DP (TS) 0.0753 0.0651 0.0673 0.0421 0.0545 0.3012 0.1009
ST-EC-DP (LS) 0.0269 0.0186 0.0122 0.0107 0.0289 0.1725 0.0450
ST-DA-DP (CE) 0.0410 0.0392 0.0414 0.0337 0.0381 0.1098 0.0505
NER
BC BN MZ NW TC WB Average
ST-EC-NER (CE) 0.0111 0.0167 0.0131 0.0146 0.0090 0.0132 0.0130
ST-EC-NER (TS) 0.0100 0.0172 0.0136 0.0108 0.0087 0.0142 0.0124
ST-EC-NER (LS) 0.0002 0.0002 0.0001 0.0002 0.0009 0.0010 0.0004
ST-DA-NER (CE) 0.0090 0.0121 0.0117 0.0122 0.0082 0.0110 0.0107
DP + NER
BC BN MZ NW TC WB Average
MT-AVG (CE) 0.0436 0.0499 0.0473 0.0345 0.0393 0.1681 0.0637
MT-AVG (TS) 0.0447 0.0468 0.0460 0.0318 0.0396 0.1645 0.0622
MT-AVG (LS) 0.0040 0.0018 0.0013 0.0012 0.0057 0.0546 0.0114
MT-AVGDA (CE) 0.0291 0.0287 0.0302 0.0236 0.0257 0.0718 0.0349
Table 6 : Overconﬁdence Error Results.
when separately training a single-task BERT-base
model on DP (left ﬁgure) and on NER (right ﬁg-
ure) with the CE objective. The conﬁdence scores
were computed according to the ST-EC scores.
The ﬁgure conﬁrms that the model tends to be
overconﬁdent in its predictions. Furthermore, the
lowR2values (0.1 and 0.13 for DP and NER, re-
spectively) indicate poor model calibration, since
conﬁdence scores are not correlated with actual
accuracy. Similar patterns were observed when
training our multi-task models with the CE objec-
tive.
Following this analysis, we turn to investigate
the impact of LS on model predictions in MT-AL.
Inspired by Thulasidasan et al. (2019), who de-
ﬁned the overconﬁdence error (OE) for classiﬁca-
tion tasks, we ﬁrst slightly generalize OEto sup-
port sentence-level scores for token classiﬁcation
tasks. Given Nsentences, for each sentence xwe
start by calculating its accuracy score acc(x)over
its tokens. The conﬁdence score conf (x)is set
to the conﬁdence score of the corresponding AL
method. We then deﬁne OEas:
OE=1
NPN
x=1conf (x)max 
conf (x) acc(x);0
.
In essence,OEpenalizes predictions according to
the gap between their conﬁdence score and their
accuracy, but only when the former is higher.
In Table 6 we compare the OEscores of ST-EC ,
trained with the LS objective to 3 alternatives: ST-
ECtrained with the CE objective, ST-EC with the
post-processing method temperature scaling (TS),
andST-DA trained with the CE objective. Both
TS and dropout inference have been shown to im-
prove conﬁdence estimates (Guo et al., 2017; Ova-
dia et al., 2019), and hence serve as alternatives to
LS in this comparison. OE scores are reported

--- PAGE 10 ---
on the unlabeled set (given the true labels in hind-
sight) at the ﬁnal AL iteration for both tasks. Addi-
tionally,OE scores for MT-AVG andMT-AVGDA
are also reported and averaged on both tasks.
The results are conclusive, LS is the least
overconﬁdent method, achieving the lowest OE
scores on all 18 setups, but one . While LS
achieves a proportionate reduction error (PRE) of
between 57.2% and 96.7% compared to the stan-
dard CE method, DA achieves at most a PRE of
51.9% and TS seems to have almost no effect.
These results conﬁrm that LS is highly effective
in reducing overconﬁdence scores for BERT-based
models, and we are able to show for the ﬁrst time
that such a reduction also holds for multi-task
models.
6 MT-AL for Hierarchically-related
Tasks
Until now, we have considered tasks (DP and
NER) that are mutually informative but can be
trained independently of each other. However,
other multi-task learning scenarios involve a task
that is dependent on the output of another task. A
prominent example is the relation extraction (RE)
task that depends on the output of the NER task,
since the goal of RE is to classify and identify re-
lations between named entities. Importantly, if the
NER part of the model does not perform well, this
harms the RE performance as well. Sample selec-
tion in such a setup should hence reﬂect the hier-
archical relation between the tasks.
6.1 Selection Methods
Since the quality of the classiﬁer for the indepen-
dent task (NER) now affects also the quality of the
classiﬁer for the dependent task (RE), the conﬁ-
dence of each of the tasks may get different rela-
tive importance values. Although this in principle
can also be true for independent tasks (§5), explic-
itly accounting for this property seems more cru-
cial in the current setup.
We hence modify four of our joint-selection
methods (x4) to reﬂect the inherent a-symmetry
between the tasks, by presenting a scaling param-
eter01:8
8We do not include the MT-MAX andMT-MIN meth-
ods in our evaluation. Since the two tasks exhibit conﬁdence
scores in a similar range, scaling their conﬁdence scores ac-
cording to these methods resulted in selecting samples based
almost solely on the task with the higher (in the case of MT-
MAX ) or lower (in the case of MT-MIN ) scaling parameter.a)MT-AVG is now calculated as follows:
MT-AVG (x) =MT-EC-RE (x)+
(1 )MT-EC-NER (x):
b)MT-RRF is calculated similarly by multiplying
the RRF term of RE by and that of NER by 1 .
c)MT-IND is calculated by independently choos-
ing100%of the selected samples according to
the RE scores and 100(1 )%according to the
NER scores.
d)MT-PAR is computed by restricting the ﬁrst
Pareto condition for the position of the RE conﬁ-
dence score: cREqc0
RE, whereqis the value
of the-quantile of the RE conﬁdence scores.9
We apply such a condition if  <0:5. Otherwise,
if > 0:5the condition is applied to the NER
component, and when it is equal to 0:5, the orig-
inal Pareto method is used. Since we restrict the
condition to only one of the tasks, fewer samples
will meet this condition (since 0q1), and
the Pareto frontier will include more samples that
have met the condition for the second task.
6.2 Research Questions
In our experiments, we would like to explore two
research questions: Q2.1 : Which MT-AL selec-
tion methods are most suitable for this setup ? and
Q2.2 : What is the best balance between the partic-
ipating tasks ?
Since RE fully relies on the output of NER, we
limit our experiments only to joint multi-task mod-
els and do not include single-task models.
6.3 Experimental Setup
We experiment with the span-based joint NER and
RE BERT model of Li et al. (2021).10Exper-
iments were conducted on ﬁve diverse datasets:
NYT24 and NYT29 (Nayak and Ng, 2020), Sci-
eRC (Luan et al., 2018), WebNLG (Gardent et al.,
2017), and WLP (Kulkarni et al., 2018). The AL
setup is identical to that of §5.4. Other hyper-
parameters that were not mentioned before are
identical to those of the original implementation.
6.4 Results
Best Selection Method (Q2.1) We start by iden-
tifying the best selection method for this setup. Ta-
ble 7 summarizes the per-task average score for
9The second Pareto condition is similarly modiﬁed, but
now with the <sign.
10https://github.com/JiachengLi1995/
JointIE .

--- PAGE 11 ---
NER NER Best RE RE Best
MT-R 81.96 0/5 52.47 05
MT-AVG (= 1:0)82.64 1/5 59.51 2/5
MT-RRF (= 0:8)82.69 1/5 59.65 1/5
MT-IND (= 1:0)82.86 2/5 60.15 0/5
MT-PAR (= 0:7)82.68 1/5 59.57 2/5
Table 7 : Hierarchical MT-AL results. We report
best average F1 results over all ﬁve datasets for
the bestconﬁguration per method.
the bestvalue of each method across the ﬁve
datasets.
We observe three interesting patterns. First,
MT-AL is very effective in this setup for the de-
pendent task (RE) , while for the independent task
(NER), random selection does not fall too far be-
hind. Second, all MT-AL methods achieve better
performance for higher values by giving more
weight to the RE conﬁdence scores during the se-
lection process. This is an indication that indeed
the selection method should reﬂect the asymmetric
nature of the tasks. Third, overall, MT-IND is the
best performing method , averaging ﬁrst in NER
and in RE, while MT-AVG ,MT-RRF andMT-PAR
achieve similar results in both tasks.
Scaling Conﬁguration (Q2.2) Figure 6 presents
the average F1 scores of the four joint-selection
methods, as well as the random selection method
MT-R , as a function of (the relative weight
of the RE conﬁdence). First, we notice that
joint selection outperforms random selection in
the WebNLG domain only for RE (the dependent
task) but not for NER (except when approaches
1). Second, and more importantly, = 1, that is,
selecting examples only according to the conﬁ-
dence score of the RE (dependent) task, is most
beneﬁcial for both tasks (Q2.1). We hypothesize
that this stems from the fact that the RE conﬁdence
score conveys information about both tasks and
that this combined information provides a stronger
signal with respect to the NER (independent) task,
compared to the NER conﬁdence score. Interest-
ingly, the positive impact of higher values of is
more prominent for NER, even though this means
that the role of the NER conﬁdence score is down-
played in the sample selection process.
For the individual selection methods, we report
thatMT-AVG andMT-IND achieve higher results
asincreases, while MT-PAR andMT-RRF peak
at= 0:7and= 0:8, respectively, and then
drop by 0.2 F1 points for NER and 0.9 F1 points
00.10.20.30.40.50.60.70.80.9 1
5859606162NER
NER AL
RE AL
NER Random
RE Random
4445464748
RE(a) ScieRC
00.10.20.30.40.50.60.70.80.9 1
8687888990NER
NER AL
RE AL
NER Random
RE Random26283032343638
RE
(b) WebNLG
Figure 6 : Average F1 scores over four joint-
selection methods as a function of (the relative
weight of the RE conﬁdence).
for RE on average.
7 MT-AL for Tasks with Different
Annotation Granularity
NLP tasks are deﬁned on different textual units,
with the most common examples of sentence-level
and token-level tasks. Our last investigation con-
siders the scenario of two closely-related tasks that
are of different granularity: Slot ﬁlling (SF, token-
level) and intent detection (ID, sentence-level).
Due to the different annotation nature of the
two tasks, we have to deﬁne the cost of exam-
ple annotation with respect to each. Naturally,
there is no correct way to quantify these costs,
but we aim to propose a realistic model. We de-
note the cost of annotating a sample for SF with
CostSF=m+tpnt, wheremis the number
of tokens in the sentence, tpis a ﬁxed token anno-
tation cost (we set tp= 1) andntis the number
of entities. The cost of annotating a sample for ID
is next denoted with CostID=m+ts, where

--- PAGE 12 ---
tsis a ﬁxed sentence cost (we set ts= 3). Our
solution (see below) allows some examples to be
annotated only with respect to one of the tasks.
For examples that are annotated with respect to
both tasks we consider an additive joint cost where
the token-level term mis considered only once:
JCost =CostSF+CostID m. In our ex-
periments, we allow a ﬁxed annotation budget of
B= 500 per AL iteration.
7.1 Methods
We consider three types of decision methods:
Greedy Active Learning (GRD_AL) : AL meth-
ods that at each iteration greedily choose the least
conﬁdent samples until the budget limitation is
reached; Binary Linear Programming Active
Learning (BLP_AL) : AL methods that at each
iteration opt to minimize the sum of conﬁdence
scores of the chosen samples given the budget con-
straints. The optimization problem (see below)
is solved using a BLP solver;11andBinary Lin-
ear Programming (BLP) : an algorithm that after
training on the initial training set chooses all the
samples at once, by solving the same constrained
optimization problem as in BLP_AL.
For each of these categories, we experiment
with four families of AL methods: a) Unre-
stricted Disjoint Sets (UDJS): This selection
method is based on the non-aggregated multi-task
conﬁdence scores, where each sample can be cho-
sen to be annotated on either task or both. The
UDJS optimization problem aims to maximize
the uncertainty scores ( 1 Conft(x)) of the se-
lected samples given the budget and selection con-
straints:
maxX
x2UX
t2T(1 Conft(x))Xt(x)
s:t:X
x2UX
t2TCostt(x)(Xt(x) Y(x))
+JCost (x)Y(x)B;
1
jTjX
t2TXt(x)Y(x)8x2U;
Xt(x);Y(x)2f0;1g 8x2U;t2T;
where Uis the unlabeled set, Tis the set of
tasks,Conftis the MT-EC-t conﬁdence score,
Xt(x)is a binary indicator indicating the annota-
11https://www.python-mip.com/ .tion of sample xon taskt, andY(x)is a binary in-
dicator indicating the annotation of xon all tasks.
Notice that this formulation may yield anno-
tated examples for only one of the tasks, although
this is unlikely, particularly under an iterative pro-
tocol when the conﬁdence scores of the models are
updated after each iteration.
b) Equal Budget Disjoint Sets (EQB-DJS) :
This strategy is similar to the above UDJS except
that the budget is equally divided between the two
tasks and the optimization problem is solved for
each of them separately. If a sample is chosen to
be annotated for both tasks, we update its cost ac-
cording to the joint cost and re-solve the optimiza-
tion problems until the entire budget is used.
c-f) Joint-task Selection : A sample could only
be chosen to be annotated on both tasks, where
conﬁdence scores are calculated using a multi-task
aggregation. The BLP optimization problem is
formulated as follows:
maxX
x2U(1 Conf (x))Y(x)
s:t:X
x2UJCost (x)Y(x)B;
Y(x)2f0;1g 8x2U;
whereConf is calculated by c)MT-AVG ,d)MT-
MAX ,e)MT-MIN orf)MT-RRF .12
g-j) Single-task Conﬁdence Selection
(STCS) : A sample could only be chosen to be
annotated on both tasks, where the selection
process aims to maximize the uncertainty scores
of only one of the tasks: g) STCS-SF orj)
STCS-ID . Similarly to Joint-task Selection, the
budget constraints are applied to the joint costs.
7.2 Research Questions
We focus on three research questions: Q3.1 : Does
BLP optimization improve upon greedy selection?
Q3.2 : Do optimization selection and active learn-
ing have a complementary effect? and Q3.3 : Is it
better to annotate all samples on both tasks or to
construct a disjoint annotated training set?
7.3 Experimental Setup
We conduct experiments on two prominent
datasets: ATIS (Price, 1990) and SNIPS (Coucke
12Since MT-PAR andMT-IND do not score the examples,
they can only be applied with the greedy decision method. A
discussion on their results is provided in x8.

--- PAGE 13 ---
ATIS SNIPS
BERT Roberta BERT Roberta
SF ID SF ID SF ID SF ID
MT-MINBLP _AL 84.53 92.27 87.94 92.05 77.90 97.00 71.35 95.29
MT-RRFGRD _AL 82.11 91.38 87.57 89.81 71.78 95.71 69.06 92.14
MT-AVGBLP 85.78 89.92 86.38 90.03 73.46 96.57 69.73 95.57
Table 8 : Comparison of decision categories (Q3.1
and Q3.2).
ATIS SNIPS
BERT Roberta BERT Roberta
SF ID SF ID SF ID SF ID
MT-MINBLP _AL 84.53 92.27 87.94 92.05 77.90 97.00 71.35 95.29
UDJSBLP _AL 87.46 90.48 85.05 91.71 73.56 96.71 70.72 95.14
EQB-DJS BLP _AL 86.59 88.02 85.25 89.14 69.92 91.43 63.58 95.00
Table 9 : Comparison of joint-task selection meth-
ods (Q3.3).
et al., 2018), and consider two Transformer-based
encoders: BERT-base (Devlin et al., 2019) and
Roberta-base (Liu et al., 2019b). Our code is
largely based on the implementation of Zhu and
Yu (2017).13We run the AL process for 5 itera-
tions with an initial training set of 50 random sam-
ples and a ﬁxed-size development set of 100 ran-
dom samples. We train all models for 30 epochs
per iteration, with an early stopping criterion and
with label smoothing ( = 0:1). Other hyper-
parameters were set to their default values.
7.4 Results
Optimization-based Selection and AL (Q3.1
and Q3.2) To answer the ﬁrst two questions, we
show in Table 8 the ﬁnal sample F1 performance
for both tasks, when selection is done with the best
selection method of each decision category: MT-
RRFGRD _AL,MT-MINBLP _ALandMT-AVGBLP.
The results conﬁrm that the BLP optimization
(with AL) is indeed superior to greedy AL se-
lection , surpassing it in all setups (two tasks, two
datasets, and two pre-trained language encoders, 5
of the comparisons are statistically signiﬁcant).
The answer to Q3.2 is also mostly positive.
BLP optimization and iterative AL have a comple-
mentary effect, as MT-MINBLP _ALin most cases
achieves higher performance than MT-AVGBLP
(50% of the results are statistically signiﬁcant).
In fact, the answer for the two questions holds
true for all selection methods , as all perform best
using our novel BLP_AL decision method. Sec-
ond is BLP , indicating that the BLP formulation is
highly effective for MT setups under different bud-
13https://github.com/sz128/slot_
filling_and_intent_detection_of_SLU .get constraints. Finally, last is the standard greedy
procedure ( GRD_AL ) which is commonly used in
the AL literature.
Joint-task Selection (Q3.3) To answer Q3.3, we
perform a similar comparison in Table 9, but now
for the most prominent methods for each joint-task
selection method. The results indicate that MT-
MINBLP _ALthat enforces joint annotation is bet-
ter than allowing for non-restricted disjoint an-
notation (UDJSBLP _AL)and than equally split-
ting the budget between the two tasks (EQB-
DJSBLP _AL), where 9 of the 16 comparisons are
statistically signiﬁcant. We hypothesize that the
superiority of MT-MINBLP _ALstems from two
main reasons: 1. Integrating the joint aggrega-
tion conﬁdence score into the optimization func-
tion provides a better signal than the single-task
conﬁdence scores; 2. Having a joint dataset where
all samples are annotated on both tasks rather than
a disjointly annotated (and larger) dataset allows
the model to achieve better performance since the
two tasks are closely related.
Finally, we also report that ST-AL experiments
have led to poor performance. The ST-AL meth-
ods trail by 8.8 (SF) and 4.7 (ID) F1 points from
the best MT-AL method MT-MINBLP _ALon av-
erage. Interestingly, we also observe that selection
according to ID ( STCS-IDBLP _AL) has led to bet-
ter average results on both tasks than selection ac-
cording to SF ( STCS-SFBLP _AL), suggesting that
similarly tox6, selection according to the higher-
level task often yields better results.
8 Overall Comparison
As a ﬁnal evaluation, we turn to compare the per-
formance of our proposed joint-selection MT-AL
methods in all setups. In our ﬁrst setup (§5) we
implemented all MT-AL selection methods with
greedy selection, considering uniform task impor-
tance. Thereafter (§6 and §7), we showed how
these selection methods can be modiﬁed in the
next two setups by either integrating non-uniform
task weights or replacing the greedy selection with
a BLP loss function overconﬁdence scores. How-
ever, not all of our MT-AL selection methods can
be modiﬁed in such ways.14We hence report
our ﬁnal comparison given the conditions applied
in the ﬁrst setup: Assuming uniform task weights
and selecting samples in a greedy manner. Ta-
14See Footnotes 8 and 12 for further details.

--- PAGE 14 ---
DP + NER NER + RE SF + ID
DP NER NER RE SF ID Average
MT-R 86.40 70.78 81.96 52.47 76.94 91.95 76.75
MT-AVG 89.03 74.44 80.23 55.97 76.42 91.09 77.86
MT-MAX 88.64 75.90 81.50 57.74 76.64 89.17 78.27
MT-MIN 88.73 74.77 81.32 54.76 73.75 91.74 77.51
MT-PAR 88.31 75.86 81.47 56.92 77.72 90.28 78.43
MT-RRF 88.31 75.89 81.78 54.29 77.63 92.26 78.36
MT-IND 87.54 74.83 82.13 54.85 76.67 91.44 77.91
Table 10 : Average results of the joint multi-task selection methods across all three setups.
ble 10 summarizes the average performance of
the MT-AL methods in our three proposed setups:
Complementing tasks (DP + NER), hierarchically-
related tasks (NER + RE) and tasks of different
annotation granularity (SF + ID).
Each selection method has its cons and pros,
which we outline in our ﬁnal discussion:
MT-R , not surprisingly, is the worst performing
method on average, as it makes no use of the
model’s predictions. Nevertheless, the method
performs quite well on the third setup (SF + ID)
when compared to the other methods that were
trained with the greedy decision method, the least
successful decision method of this setup. Next,
MT-AVG performs well when the tasks are of
equal importance (DP + NER), but achieves only
moderate performance on the other setups.
Surprisingly, MT-MAX is highly effective de-
spite its simplicity. It is mostly beneﬁcial for the
ﬁrst two setups (DP + NER and NER + RE), where
the tasks are of the same annotation granularity.
It is the third best method overall, and it does
not lag substantially behind the best method, MT-
PAR. Interestingly, MT-MIN , which offers a com-
plementary perspective to MT-MAX , is on average
the worst MT-AL method, excluding MT-R , and is
mainly beneﬁcial for the ﬁrst setup (DP + NER).
The next MT-AL method, MT-PAR , seems to
capture well the joint conﬁdence space of the task
pairs. It is the best method on average, achiev-
ing high average scores in all setups. However,
when incorporating it with other training tech-
niques, such as applying non-uniform weights (for
the second setup), it is outperformed by the other
MT-AL methods. MT-RRF does not lag far be-
hind MT-PAR , achieving similar results on most
tasks, excluding the RE and ID tasks, which are
the higher-level tasks of their setups. Finally, MT-
IND does not excel in three of the four tasks ofthe ﬁrst two setups, while achieving the best aver-
age results on NER, when jointly trained with RE.
Furthermore, the method demonstrates strong per-
formance on the third setup, when the tasks are of
different annotation granularity, justifying the in-
dependent annotation selection in this case.
9 Conclusions
We considered the problem of multi-task active
learning for pre-trained Transformer-based mod-
els. We posed multiple research questions con-
cerning the impact of multi-task modeling, multi-
task selection criteria, overconﬁdence reduction,
the relationships between the participating tasks,
as well as budget constraints, and presented a sys-
tematic algorithmic and experimental investiga-
tion in order to answer these questions. Our re-
sults demonstrate the importance of MT-AL mod-
eling in three challenging real-life scenarios, cor-
responding to diverse relations between the partic-
ipating tasks. In future work, we plan to research
setups with more than two tasks and consider lan-
guage generation and multilingual modeling.
10 Acknowledgments
We would like to thank the action editor and
the reviewers, as well as the members of the
IE@Technion NLP group for their valuable feed-
back and advice. This research was partially
funded by an ISF personal grant No. 1625/18. RR
also acknowledges the support of the Schmidt Ca-
reer Advancement Chair in AI.
References
Guirong Bai, Shizhu He, Kang Liu, Jun Zhao, and
Zaiqing Nie. 2020. Pre-trained language model
based active learning for sentence matching.

--- PAGE 15 ---
InProceedings of the 28th International Con-
ference on Computational Linguistics , pages
1495–1504.
Ying Chen, Wenjun Hou, Xiyao Cheng, and
Shoushan Li. 2018. Joint learning for emotion
classiﬁcation and emotion cause detection. In
Proceedings of the 2018 Conference on Empir-
ical Methods in Natural Language Processing ,
pages 646–651, Brussels, Belgium. Association
for Computational Linguistics.
Gordon V Cormack, Charles LA Clarke, and
Stefan Buettcher. 2009. Reciprocal rank fu-
sion outperforms condorcet and individual rank
learning methods. In Proceedings of the 32nd
international ACM SIGIR conference on Re-
search and development in information re-
trieval , pages 758–759.
Alice Coucke, Alaa Saade, Adrien Ball, Théodore
Bluche, Alexandre Caulier, David Leroy,
Clément Doumouro, Thibault Gisselbrecht,
Francesco Caltagirone, Thibaut Lavril, Maël
Primet, and Joseph Dureau. 2018. Snips voice
platform: an embedded spoken language under-
standing system for private-by-design voice in-
terfaces. CoRR , abs/1805.10190.
Shrey Desai and Greg Durrett. 2020. Calibration
of pre-trained transformers. In Proceedings of
the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP) , pages
295–302.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training
of deep bidirectional transformers for language
understanding. In Proceedings of the 2019
Conference of the North American Chapter
of the Association for Computational Linguis-
tics: Human Language Technologies, Volume 1
(Long and Short Papers) , pages 4171–4186.
Timothy Dozat and Christopher D. Manning.
2017. Deep biafﬁne attention for neural de-
pendency parsing. In 5th International Confer-
ence on Learning Representations, ICLR 2017,
Toulon, France, April 24-26, 2017, Conference
Track Proceedings .
Rotem Dror, Gili Baumer, Segev Shlomov, and
Roi Reichart. 2018. The hitchhiker’s guideto testing statistical signiﬁcance in natural lan-
guage processing. In Proceedings of the 56th
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) ,
pages 1383–1392.
Long Duong, Hadi Afshar, Dominique Estival,
Glen Pink, Philip Cohen, and Mark Johnson.
2018. Active learning for deep semantic pars-
ing. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Lin-
guistics (Volume 2: Short Papers) , pages 43–48,
Melbourne, Australia. Association for Compu-
tational Linguistics.
Liat Ein-Dor, Alon Halfon, Ariel Gera, Eyal
Shnarch, Lena Dankin, Leshem Choshen, Ma-
rina Danilevsky, Ranit Aharonov, Yoav Katz,
and Noam Slonim. 2020. Active Learning for
BERT: An Empirical Study. In Proceedings of
the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP) , pages
7949–7962, Online. Association for Computa-
tional Linguistics.
Jenny Rose Finkel and Christopher D Manning.
2009. Joint parsing and named entity recogni-
tion. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics , pages 326–334.
Yarin Gal and Zoubin Ghahramani. 2016.
Dropout as a bayesian approximation: Repre-
senting model uncertainty in deep learning. In
international conference on machine learning ,
pages 1050–1059. PMLR.
Claire Gardent, Anastasia Shimorina, Shashi
Narayan, and Laura Perez-Beltrachini. 2017.
The WebNLG challenge: Generating text from
RDF data. In Proceedings of the 10th Interna-
tional Conference on Natural Language Gener-
ation , pages 124–133, Santiago de Compostela,
Spain. Association for Computational Linguis-
tics.
Daniel Grießhaber, Johannes Maucher, and
Ngoc Thang Vu. 2020. Fine-tuning BERT
for low-resource natural language understand-
ing via active learning. In Proceedings
of the 28th International Conference on
Computational Linguistics , pages 1158–1171,

--- PAGE 16 ---
Barcelona, Spain (Online). International Com-
mittee on Computational Linguistics.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q
Weinberger. 2017. On calibration of mod-
ern neural networks. In International Confer-
ence on Machine Learning , pages 1321–1330.
PMLR.
Gholamreza Haffari, Maxim Roy, and Anoop
Sarkar. 2009. Active learning for statistical
phrase-based machine translation. In Proceed-
ings of Human Language Technologies: The
2009 Annual Conference of the North American
Chapter of the Association for Computational
Linguistics , pages 415–423.
Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation ,
9(8):1735–1780.
Eduard Hovy, Mitch Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
Ontonotes: the 90% solution. In Proceedings of
the human language technology conference of
the NAACL, Companion Volume: Short Papers ,
pages 57–60.
Fariz Ikhwantri, Samuel Louvan, Kemal Kur-
niawan, Bagas Abisena, Valdi Rachman, Al-
fan Farizki Wicaksono, and Rahmad Mahendra.
2018. Multi-task active learning for neural se-
mantic role labeling on low resource conver-
sational corpus. In Proceedings of the Work-
shop on Deep Learning Approaches for Low-
Resource NLP , pages 43–50.
Seokhwan Kim, Yu Song, Kyungduk Kim, Jeong-
Won Cha, and Gary Geunbae Lee. 2006. Mmr-
based active machine learning for bio named
entity recognition. In Proceedings of the Hu-
man Language Technology Conference of the
NAACL, Companion Volume: Short Papers ,
pages 69–72.
Diederik P Kingma and Jimmy Ba. 2015. Adam:
A method for stochastic optimization. In ICLR
(Poster) .
Lingkai Kong, Haoming Jiang, Yuchen Zhuang,
Jie Lyu, Tuo Zhao, and Chao Zhang. 2020.
Calibrated language model ﬁne-tuning for in-
and out-of-distribution data. In Proceedings of
the 2020 Conference on Empirical Methods inNatural Language Processing (EMNLP) , pages
1326–1340, Online. Association for Computa-
tional Linguistics.
Chaitanya Kulkarni, Wei Xu, Alan Ritter, and
Raghu Machiraju. 2018. An annotated cor-
pus for machine reading of instructions in wet
lab protocols. In Proceedings of the 2018
Conference of the North American Chapter
of the Association for Computational Linguis-
tics: Human Language Technologies, Volume
2 (Short Papers) , pages 97–106, New Orleans,
Louisiana. Association for Computational Lin-
guistics.
Belinda Z. Li, Gabriel Stanovsky, and Luke Zettle-
moyer. 2020. Active learning for coreference
resolution using discrete annotation. In Pro-
ceedings of the 58th Annual Meeting of the As-
sociation for Computational Linguistics , pages
8320–8331, Online. Association for Computa-
tional Linguistics.
Jiacheng Li, Haibo Ding, Jingbo Shang, Julian
McAuley, and Zhe Feng. 2021. Weakly su-
pervised named entity tagging with learnable
logical rules. In Proceedings of the 59th An-
nual Meeting of the Association for Compu-
tational Linguistics and the 11th International
Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers) , pages 4568–
4581, Online. Association for Computational
Linguistics.
Zhenghua Li, Min Zhang, Yue Zhang, Zhanyi Liu,
Wenliang Chen, Hua Wu, and Haifeng Wang.
2016. Active learning for dependency pars-
ing with partial annotation. In Proceedings of
the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers) , pages 344–354.
Ying Lin, Shengqi Yang, Veselin Stoyanov, and
Heng Ji. 2018. A multi-lingual multi-task ar-
chitecture for low-resource sequence labeling.
InProceedings of the 56th Annual Meeting of
the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 799–809.
Xiaodong Liu, Pengcheng He, Weizhu Chen, and
Jianfeng Gao. 2019a. Multi-task deep neural
networks for natural language understanding.
InProceedings of the 57th Annual Meeting of
the Association for Computational Linguistics ,

--- PAGE 17 ---
pages 4487–4496, Florence, Italy. Association
for Computational Linguistics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei
Du, Mandar Joshi, Danqi Chen, Omer Levy,
Mike Lewis, Luke Zettlemoyer, and Veselin
Stoyanov. 2019b. Roberta: A robustly opti-
mized bert pretraining approach. arXiv preprint
arXiv:1907.11692 .
Alexander V . Lotov and Kaisa Miettinen. 2008.
Visualizing the pareto frontier. In Multiobjec-
tive optimization , pages 213–243. Springer.
Yi Luan, Luheng He, Mari Ostendorf, and Han-
naneh Hajishirzi. 2018. Multi-task identiﬁca-
tion of entities, relations, and coreference for
scientiﬁc knowledge graph construction. In
Proceedings of the 2018 Conference on Empir-
ical Methods in Natural Language Processing ,
pages 3219–3232, Brussels, Belgium. Associa-
tion for Computational Linguistics.
Bryan McCann, James Bradbury, Caiming Xiong,
and Richard Socher. 2017. Learned in transla-
tion: contextualized word vectors. In Proceed-
ings of the 31st International Conference on
Neural Information Processing Systems , pages
6297–6308.
Tapas Nayak and Hwee Tou Ng. 2020. Effec-
tive modeling of encoder-decoder architecture
for joint entity and relation extraction. In Pro-
ceedings of the AAAI conference on artiﬁcial in-
telligence , volume 34, pages 8528–8535.
Linh The Nguyen and Dat Quoc Nguyen. 2021.
PhoNLP: A joint multi-task learning model for
Vietnamese part-of-speech tagging, named en-
tity recognition and dependency parsing. In
Proceedings of the 2021 Conference of the
North American Chapter of the Association for
Computational Linguistics: Human Language
Technologies: Demonstrations , pages 1–7, On-
line. Association for Computational Linguis-
tics.
Joakim Nivre, Marie-Catherine de Marneffe, Filip
Ginter, Jan Haji ˇc, Christopher D. Manning,
Sampo Pyysalo, Sebastian Schuster, Francis Ty-
ers, and Daniel Zeman. 2020. Universal Depen-
dencies v2: An evergrowing multilingual tree-
bank collection. In Proceedings of the 12thLanguage Resources and Evaluation Confer-
ence, pages 4034–4043, Marseille, France. Eu-
ropean Language Resources Association.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary
Nado, D Sculley, Sebastian Nowozin, Joshua
Dillon, Balaji Lakshminarayanan, and Jasper
Snoek. 2019. Can you trust your model’s uncer-
tainty? evaluating predictive uncertainty under
dataset shift. Advances in Neural Information
Processing Systems , 32:13991–14002.
Álvaro Peris and Francisco Casacuberta. 2018.
Active learning for interactive neural machine
translation of data streams. In Proceedings of
the 22nd Conference on Computational Natu-
ral Language Learning , pages 151–160, Brus-
sels, Belgium. Association for Computational
Linguistics.
Matthew Peters, Mark Neumann, Mohit Iyyer,
Matt Gardner, Christopher Clark, Kenton Lee,
and Luke Zettlemoyer. 2018. Deep contextu-
alized word representations. In Proceedings
of the 2018 Conference of the North American
Chapter of the Association for Computational
Linguistics: Human Language Technologies,
Volume 1 (Long Papers) , pages 2227–2237.
Patti Price. 1990. Evaluation of spoken language
systems: The atis domain. In Speech and Nat-
ural Language: Proceedings of a Workshop
Held at Hidden Valley, Pennsylvania, June 24-
27, 1990 .
Colin Raffel, Noam Shazeer, Adam Roberts,
Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J Liu.
2020. Exploring the limits of transfer learning
with a uniﬁed text-to-text transformer. Journal
of Machine Learning Research , 21:1–67.
Roi Reichart and Ari Rappoport. 2007. An ensem-
ble method for selection of high quality parses.
InProceedings of the 45th Annual Meeting of
the Association of Computational Linguistics ,
pages 408–415.
Roi Reichart and Ari Rappoport. 2009. Sam-
ple selection for statistical parsers: Cogni-
tively driven algorithms and evaluation mea-
sures. In Proceedings of the Thirteenth Con-
ference on Computational Natural Language
Learning (CoNLL-2009) , pages 3–11.

--- PAGE 18 ---
Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari
Rappoport. 2008. Multi-task active learning for
linguistic annotations. In Proceedings of ACL-
08: HLT , pages 861–869.
Guy Rotman and Roi Reichart. 2019. Deep con-
textualized self-training for low resource depen-
dency parsing. Transactions of the Association
for Computational Linguistics , 7:695–713.
Sebastian Ruder. 2017. An overview of multi-
task learning in deep neural networks. arXiv
preprint arXiv:1706.05098 .
Niloofar Saﬁ Samghabadi, Parth Patwa, Srinivas
PYKL, Prerana Mukherjee, Amitava Das, and
Thamar Solorio. 2020. Aggression and misog-
yny detection using BERT: A multi-task ap-
proach. In Proceedings of the Second Work-
shop on Trolling, Aggression and Cyberbully-
ing, pages 126–131, Marseille, France. Euro-
pean Language Resources Association (ELRA).
Victor Sanh, Thomas Wolf, and Sebastian Ruder.
2019. A hierarchical multi-task approach for
learning embeddings from semantic tasks. In
Proceedings of the AAAI Conference on Artiﬁ-
cial Intelligence , volume 33, pages 6949–6956.
Nathan Schneider, Jena D. Hwang, Vivek Sriku-
mar, Jakob Prange, Austin Blodgett, Sarah R.
Moeller, Aviram Stern, Adi Bitan, and Omri
Abend. 2018. Comprehensive supersense dis-
ambiguation of English prepositions and pos-
sessives. In Proceedings of the 56th Annual
Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages
185–196, Melbourne, Australia. Association for
Computational Linguistics.
Burr Settles and Mark Craven. 2008. An analy-
sis of active learning strategies for sequence la-
beling tasks. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Lan-
guage Processing , pages 1070–1079.
H Sebastian Seung, Manfred Opper, and Haim
Sompolinsky. 1992. Query by committee. In
Proceedings of the ﬁfth annual workshop on
Computational learning theory , pages 287–294.
Yanyao Shen, Hyokun Yun, Zachary Lipton,
Yakov Kronrod, and Animashree Anandkumar.
2017. Deep active learning for named en-
tity recognition. In Proceedings of the 2ndWorkshop on Representation Learning for NLP ,
pages 252–256, Vancouver, Canada. Associa-
tion for Computational Linguistics.
Anders Søgaard and Yoav Goldberg. 2016. Deep
multi-task learning with low level tasks super-
vised at lower layers. In Proceedings of the 54th
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers) ,
pages 231–235.
Christian Szegedy, Vincent Vanhoucke, Sergey
Ioffe, Jon Shlens, and Zbigniew Wojna. 2016.
Rethinking the inception architecture for com-
puter vision. In Proceedings of the IEEE con-
ference on computer vision and pattern recog-
nition , pages 2818–2826.
Sunil Thulasidasan, Gopinath Chennupati, Jeff A
Bilmes, Tanmoy Bhattacharya, and Sarah
Michalak. 2019. On mixup training: Improved
calibration and predictive uncertainty for deep
neural networks. In Advances in Neural Infor-
mation Processing Systems , volume 32. Curran
Associates, Inc.
Katrin Tomanek and Udo Hahn. 2010. A compari-
son of models for cost-sensitive active learning.
InColing 2010: Posters , pages 1247–1255.
Ashish Vaswani, Noam Shazeer, Niki Parmar,
Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. 2017. At-
tention is all you need. In Advances in neural
information processing systems , pages 5998–
6008.
Alex Wang, Amanpreet Singh, Julian Michael,
Felix Hill, Omer Levy, and Samuel Bowman.
2018. GLUE: A multi-task benchmark and
analysis platform for natural language under-
standing. In Proceedings of the 2018 EMNLP
Workshop BlackboxNLP: Analyzing and Inter-
preting Neural Networks for NLP , pages 353–
355, Brussels, Belgium. Association for Com-
putational Linguistics.
Maciej Wiatrak and Juha Iso-Sipila. 2020. Simple
hierarchical multi-task neural end-to-end entity
linking for biomedical text. In Proceedings of
the 11th International Workshop on Health Text
Mining and Information Analysis , pages 12–17,
Online. Association for Computational Linguis-
tics.

--- PAGE 19 ---
Thomas Wolf, Lysandre Debut, Victor Sanh,
Julien Chaumond, Clement Delangue, Anthony
Moi, Pierric Cistac, Tim Rault, Rémi Louf,
Morgan Funtowicz, Joe Davison, Sam Shleifer,
Patrick von Platen, Clara Ma, Yacine Jernite,
Julien Plu, Canwen Xu, Teven Le Scao, Syl-
vain Gugger, Mariama Drame, Quentin Lhoest,
and Alexander M. Rush. 2020. Transformers:
State-of-the-art natural language processing. In
Proceedings of the 2020 Conference on Empir-
ical Methods in Natural Language Processing:
System Demonstrations , pages 38–45. Associa-
tion for Computational Linguistics.
Kaige Xie, Cheng Chang, Liliang Ren, Lu Chen,
and Kai Yu. 2018. Cost-sensitive active learn-
ing for dialogue state tracking. In Proceed-
ings of the 19th Annual SIGdial Meeting on
Discourse and Dialogue , pages 209–213, Mel-
bourne, Australia. Association for Computa-
tional Linguistics.
He Zhao, Longtao Huang, Rong Zhang, Quan Lu,
and Hui Xue. 2020. SpanMlt: A span-based
multi-task learning framework for pair-wise as-
pect and opinion terms extraction. In Proceed-
ings of the 58th Annual Meeting of the As-
sociation for Computational Linguistics , pages
3239–3248, Online. Association for Computa-
tional Linguistics.
Hua Zhu, Wu Ye, Sihan Luo, and Xidong Zhang.
2020. A multitask active learning framework
for natural language understanding. In Pro-
ceedings of the 28th International Conference
on Computational Linguistics , pages 4900–
4914, Barcelona, Spain (Online). International
Committee on Computational Linguistics.
Su Zhu and Kai Yu. 2017. Encoder-decoder with
focus-mechanism for sequence labelling based
spoken language understanding. In 2017 IEEE
International Conference on Acoustics, Speech
and Signal Processing (ICASSP) , pages 5675–
5679. IEEE.
