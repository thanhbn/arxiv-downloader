# 2208.05379.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multi-task/2208.05379.pdf
# File size: 930333 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Multi-task Active Learning for Pre-trained Transformer-based Models
Guy Rotman and Roi Reichart
Faculty of Industrial Engineering and Management, Technion, IIT
grotman@campus.technion.ac.il
roiri@technion.ac.il
Abstract
Multi-task learning, in which several tasks
are jointly learned by a single model, allows
NLP models to share information from mul-
tiple annotations and may facilitate better
predictions when the tasks are inter-related.
This technique, however, requires annotat-
ing the same text with multiple annotation
schemes which may be costly and laborious.
Active learning (AL) has been demonstrated
to optimize annotation processes by itera-
tively selecting unlabeled examples whose
annotation is most valuable for the NLP
model. Yet, multi-task active learning (MT-
AL) has not been applied to state-of-the-art
pre-trained Transformer-based NLP models.
This paper aims to close this gap. We ex-
plore various multi-task selection criteria in
three realistic multi-task scenarios, reÔ¨Çect-
ing different relations between the partici-
pating tasks, and demonstrate the effective-
ness of multi-task compared to single-task
selection. Our results suggest that MT-AL
can be effectively used in order to minimize
annotation efforts for multi-task NLP mod-
els.1 2
1 Introduction
Deep neural networks (DNNs) have recently
achieved state-of-the-art results for many nat-
ural language processing (NLP) tasks and ap-
plications. Of particular importance are con-
textualized embedding models (McCann et al.,
2017; Peters et al., 2018), most of which im-
plement transformer-based architectures with the
self-attention mechanism (Vaswani et al., 2017;
Devlin et al., 2019; Raffel et al., 2020).
1Our code base is available at: https://github.
com/rotmanguy/MTAL .
2Accepted for publication in Transactions of the Associ-
ation for Computational Linguistics (TACL), 2022. Pre-MIT
Press publication versionNevertheless, DNNs often require large labeled
training sets in order to achieve good performance.
While annotating such training sets is costly and
laborious, the active learning (AL) paradigm aims
to minimize these costs by iteratively selecting
valuable training examples for annotation. Re-
cently, AL has been shown effective for DNNs
across various NLP tasks (Duong et al., 2018;
Peris and Casacuberta, 2018; Ein-Dor et al., 2020).
An appealing capability of DNNs is perform-
ing multi-task learning (MTL): Learning multiple
tasks by a single model (Ruder, 2017). This stems
from their architectural Ô¨Çexibility ‚Äì constructing
increasingly deeper and wider architectures from
basic building blocks ‚Äì and in their gradient-based
optimization which allows them to jointly up-
date parameters from multiple task-based objec-
tives. Indeed, MTL has become ubiquitous in NLP
(Luan et al., 2018; Liu et al., 2019a).
MTL models for NLP can often beneÔ¨Åt from us-
ing corpora annotated for multiple tasks, particu-
larly when these tasks are closely-related and can
inform each other. Prominent examples of multi-
task corpora include OntoNotes (Hovy et al.,
2006), the Universal Dependencies Bank (Nivre
et al., 2020) and STREUSLE (Schneider et al.,
2018). Given the importance of multi-task corpora
for many MTL setups, effective AL frameworks
that support MTL are becoming crucial.
Unfortunately, most AL methods do not support
annotations for more than one task. Multi-task AL
(MT-AL) was proposed by Reichart et al. (2008)
before the neural era, and adapted by Ikhwantri
et al. (2018) to a neural architecture. Recently,
Zhu et al. (2020) proposed an MT-AL model for
slot Ô¨Ålling and intent detection, focusing mostly
on LSTMs (Hochreiter and Schmidhuber, 1997).
In this paper, we are the Ô¨Årst to systematically
explore MT-AL for large pre-trained Transformer
models. Naturally, our focus is on closely-related
NLP tasks, for which multi-task annotation of thearXiv:2208.05379v2  [cs.CL]  28 Oct 2022

--- PAGE 2 ---
same corpus is likely to be of beneÔ¨Åt. Particu-
larly, we consider three challenging real-life multi-
task scenarios, reÔ¨Çecting different relations be-
tween the participating NLP tasks: 1. Comple-
menting tasks, where each task may provide valu-
able information to the other task: Dependency
parsing (DP) and named entity recognition (NER);
2. Hierarchically-related tasks, where one of the
tasks depends on the output of the other: Relation
extraction (RE) and NER; and 3. Tasks with differ-
ent annotation granularity: Slot Ô¨Ålling (SF, token
level) and intent detection (ID, sentence level). We
propose various novel MT-AL methods and tailor
them to the speciÔ¨Åc properties of the scenarios, in
order to properly address the underlying relations
between the participating tasks. Our experimen-
tal results highlight a large number of patterns that
can guide NLP researchers when annotating cor-
pora with multiple annotation schemes using the
AL paradigm.
2 Previous Work
This paper addresses a previously unexplored
problem: multi-task AL (MT-AL) for NLP with
pre-trained Transformer-based models. We hence
start by covering AL in NLP and then proceed
with multi-task learning (MTL) in NLP.
2.1 Active Learning in NLP
AL has been successfully applied to a variety of
NLP tasks, including semantic parsing (Duong
et al., 2018), syntactic parsing (Reichart and Rap-
poport, 2009; Li et al., 2016), co-reference res-
olution (Li et al., 2020), named entity recogni-
tion (Shen et al., 2017) and machine translation
(Haffari et al., 2009), to name a few. Recent
works demonstrated that models like BERT can
beneÔ¨Åt from AL in low-resource settings (Ein-Dor
et al., 2020; Grie√ühaber et al., 2020), and Bai et al.
(2020) suggested basing the AL selection crite-
rion on linguistic knowledge captured by BERT.
Other works performed cost-sensitive AL, where
instances may have different costs (Tomanek and
Hahn, 2010; Xie et al., 2018). However, most pre-
vious works did not apply AL for MTL, which is
our main focus.
2.2 Multi-task Learning (MTL) in NLP
MTL has become increasingly popular in NLP,
particularly when the solved tasks are closely-
related (Chen et al., 2018; SaÔ¨Å Samghabadi et al.,2020; Zhao et al., 2020). In some cases, the
MTL model is trained in a hierarchical fashion,
where information is propagated from lower-level
(sometimes auxiliary) tasks to higher-level tasks
(S√∏gaard and Goldberg, 2016; Rotman and Re-
ichart, 2019; Sanh et al., 2019; Wiatrak and Iso-
Sipila, 2020). In other cases, different labeled cor-
pora can be merged to serve as multi-task bench-
marks (McCann et al., 2017; Wang et al., 2018).
This way, a single MTL model can be trained on
multiple tasks, which are typically only distantly-
related. This research considers the setup of
closely-related tasks where annotating a single
corpus w.r.t. multiple tasks is a useful strategy.
3 Task DeÔ¨Ånition - Multi-task Active
Learning (MT-AL)
In the MT-AL setup, the AL algorithm is provided
with a textual corpus, where an initial (typically
small) set of n0examples is labeled for ttasks.
The AL algorithm implements an iterative pro-
cess, where at the i-th iteration the goal of the AL
algorithm is to select niadditional unlabeled ex-
amples that will be annotated on all ttasks, such
that the performance of the base NLP model will
be improved as much as possible with respect to
all of them. While such a greedy strategy of gain-
ing the most in the i-th iteration may not yield the
best performance in subsequent iterations, most
AL algorithms are greedy, and we hence follow
this strategy here as well.
We focus on the standard setup of conÔ¨Ådence-
based AL, where unlabeled examples with the
lowest model conÔ¨Ådence are selected for annota-
tion. Algorithm 1 presents a general sketch of such
AL algorithms, in the context of MTL. This frame-
Algorithm 1 Multi-task ConÔ¨Ådence-based Active Learn-
ing (ConÔ¨Ådence-based MT-AL)
Input: Labeled data L(annotated on ttasks), Unlabeled data
U
Algorithm:
Fori= 1; : : : ;T:
1. Train a multi-task learning (MTL) model honL.
2. For each u2Ucalculate its aggregated conÔ¨Ådence
score Ch(u)on all ttasks according to h.
3. Choose the niunlabeled examples from Uwith the
lowest conÔ¨Ådence score Ch(u)and send them for an-
notation according to all ttasks.
4. Add the newly labeled examples to Land remove them
fromU.

--- PAGE 3 ---
work, Ô¨Årst introduced by Reichart et al. (2008), is
a simple generalization of the single-task AL (ST-
AL) framework, which supports the annotation of
data with respect to multiple tasks.
As discussed in ¬ß1, we explore several vari-
ations of the MT-AL setup: Independent tasks
that inform each other (¬ß5), hierarchically-related
tasks, where one task depends on the output of
the other (¬ß6), and tasks with different annotation
granularity, word and sentence level (¬ß7). Before
we can introduce the MT-AL algorithms for each
of these setups, we Ô¨Årst need to lay their shared
foundations: The single-task and multi-task model
conÔ¨Ådence scores.
4 ConÔ¨Ådence Estimation in Single-task
and Multi-task Active Learning
We now introduce the conÔ¨Ådence scores that we
consider for single-task (ST-AL) and multi-task
(MT-AL) active learning. These conÔ¨Ådence scores
are essentially the core of conÔ¨Ådence-based AL al-
gorithms (see Steps 2-3 of Algorithm 1). In Table
1 we provide a summary of the various ST-AL and
MT-AL selection methods we explore.
4.1 Single-task ConÔ¨Ådence Scores
We consider three conÔ¨Ådence scores that have
been widely used in ST-AL:
Random ( ST-R )This baseline method simply as-
signs random scores to the unlabeled examples.
Entropy-based ConÔ¨Ådence ( ST-EC )The single-
task entropy-based conÔ¨Ådence score is deÔ¨Åned as:
ST-EC (x) = 1 E(x): (1)
For sentence classiÔ¨Åcation tasks such as ID,
E(x) is simply the entropy over the class predic-
tions of a sample xdivided by the log number of
labels. In our token classiÔ¨Åcation tasks (DP, NER,
RE and SF), E(x) is the normalized sentence-level
entropy (Kim et al., 2006), which allows us to es-
timate the uncertainty of the model for a given se-
quence of tokens x= (x1:::xm):
E(x) = 1
mlogsmX
i=1sX
j=1p(yjjxi) logp(yjjxi);
(2)
wheremis the number of tokens, yjis thej‚Äôth
possible label, and sis the number of labels. We
perform entropy normalization by averaging the
token-level entropies, in order to mitigate the ef-
fect of the sentence length, and by dividing thescore by the log number of labels. The resulting
conÔ¨Ådence score ranges from 0 to 1, where lower
values indicate lower certainty.
Dropout Agreement ( ST-DA )Ensemble meth-
ods have proven effective for AL (see, e.g., (Seung
et al., 1992; Settles and Craven, 2008)) . In this pa-
per, we derive a conÔ¨Ådence score inspired by Re-
ichart and Rappoport (2007). We start by creating
k= 10 different models by performing dropout
inference forktimes (Gal and Ghahramani, 2016).
We then compute the single-task dropout agree-
ment score for a sentence xby calculating the av-
erage token-level agreement across model pairs:
ST-DA (x) =1
mk(k 1)X
j6=j0mX
i=11f^yj
i=^yj0
ig;
(3)
where ^yj
iis the predicted label of model jfor the
i‚Äôth token. The resulting scores range from 0 to 1,
where lower values indicate lower certainty.3
4.2 Multi-task ConÔ¨Ådence Scores
When deriving conÔ¨Ådence scores for MT-AL,
multiple design choices should be made. First,
the conÔ¨Ådence score of a multi-task model can be
based on both tasks or only on one of them. We
denote with MT-EC andMT-DA the conÔ¨Ådence
scores that are equivalent to ST-EC andST-DA :
The only (important) difference is that they are
calculated for a multi-task model. For clarity, we
will augment this notation with the name of the
task according to which the conÔ¨Ådence is calcu-
lated. For example, ST-EC-NER andMT-EC-NER
are the EC scores calculated using the named en-
tity recognition (NER) classiÔ¨Åer of a single-task
and a multi-task model, respectively.
We can hence evaluate MT-AL algorithms on
cross-task selection, i.e., when the evaluated task
is different from the task used for computing the
conÔ¨Ådence scores (and hence for sample selec-
tion). For example, evaluating the performance
of a multi-task model, trained jointly on NER and
DP, on the DP task when the conÔ¨Ådence scores
used by the MT-AL algorithm are only based on
the NER classiÔ¨Åer ( MT-EC-NER ).
We also consider a family of conÔ¨Ådence scores
for MT-AL that are computed with respect to all
participating tasks (joint-selection scores). For
this aim, we consider three simple aggregation
3For sentence classiÔ¨Åcation tasks, ST-DA is computed
similarly, without averaging over the tokens.

--- PAGE 4 ---
schemes using the average, maximum, or min-
imum operators over the single-task conÔ¨Ådence
scores. For example, the multi-task average con-
Ô¨Ådence ( MT-AVG ) averages for a sample xthe
entropy-based conÔ¨Ådence scores over all ttasks:
MT-AVG (x) =1
ttX
i=1MT-EC-i (x); (4)
The multi-task average dropout agreement
score ( MT-AVGDA ) is similarly deÔ¨Åned, but the
averaging is over the MT-DA scores. Finally, the
multi-task maximum (minimum) MT-MAX (MT-
MIN ) is computed in a similar manner to MT-AVG
but with the max (min) operator taken over the
task-speciÔ¨Åc conÔ¨Ådence entropies.
Beyond Direct Manipulations of ConÔ¨Ådence
Scores Since our focus in this paper is on multi-
task selection, we would like to consider addi-
tional selection methods which go beyond the sim-
ple methods in previous work. The common prin-
ciple of these methods is that they are less sensi-
tive to the actual values of the conÔ¨Ådence scores
and instead consider the relative importance of the
example to the participating tasks.
First, we consider MT-PAR , which is based on
the Pareto-efÔ¨Åcient frontier (Lotov and Miettinen,
2008). We start by representing each unlabeled
sample as a t-dimensional space vector c, where
ci=ST-EC-i is the ST conÔ¨Ådence score for task
i. Next, we select all samples for which the cor-
responding vector belongs to the Pareto-efÔ¨Åcient
frontier. A point belongs to the frontier if for ev-
ery other vector c0the following holds: 1. 8i2
[t];cic0
iand 2.9i2[t];ci<c0
i. If the number
of samples in the frontier is smaller than the total
number of samples to select ( n), we re-iterate the
procedure by removing the vectors of the selected
samples and calculating the next Pareto points. If
there are still ppoints to be selected but the num-
ber of the Ô¨Ånal Pareto points ( f) exceedsp, we
select everybf
pcpoint, ordered by the Ô¨Årst axis.
Next, inspired by the Ô¨Åeld of information re-
trieval we propose MT-RRF . This method allows
us to consider the rank of each example with re-
spect to the participating tasks, rather than the ac-
tual conÔ¨Ådence values. We Ô¨Årst calculate ri, the
ranked list of the i-th task, by ranking the exam-
ples according to their ST-EC-i scores, from low-
est to highest. We next fuse the resulting tranked
lists into a single ranked list R, using the recipro-
cal rank fusion (RRF) technique (Cormack et al.,
MT-MIN MT-MAX MT-AVG MT-PAR MT-RRFMT-MAX
MT-AVG
MT-PAR
MT-RRF
MT-IND0.16
0.84 0.3
0.57 0.47 0.71
0.72 0.25 0.8 0.73
0.72 0.2 0.77 0.68 0.94
0.00.20.40.60.81.0
Figure 1 : The percentage of shared selected sam-
ples between pairs of MT-AL selection methods
(see experimental details in the text).
2009). The RRF score of an example xis com-
puted as:
RRF -Score (x) =tX
i=11
k+ri(x); (5)
wherekis a constant, set to 60, as in the original
paper. The Ô¨Ånal ranking is computed over the RRF
scores of the examples ‚Äì from highest to lowest.
Higher-ranked examples are chosen Ô¨Årst for anno-
tation as they have lower conÔ¨Ådence scores. Fi-
nally, MT-IND independently selects the bn
tcmost
uncertain samples according to each task by rank-
ing the ST-EC scores and re-iterating if overlaps
occur.
We Ô¨Ånally compare the selected samples of six
of the MT-AL methods, after training a multi-task
model for a single AL iteration on the DP and
NER tasks (Figure 1). It turns out that while some
of the methods tend to choose very similar exam-
ple subsets for annotation (e.g., MT-IND andMT-
RRF share 94% of the selected samples, and MT-
AVG andMT-MIN share 84% of them), others sub-
stantially differ in their selection (e.g., MT-MAX
shares only 16% of its selected samples with MT-
MIN and 20% with MT-IND ). This observation en-
courages us to continue investigating the impact of
the various selection methods on MT-AL.
5 MT-AL for Complementing Tasks
We start by investigating MT-AL for two closely-
related, complementing, syntactic tasks: Depen-
dency Parsing (DP) and Named Entity Recogni-
tion (NER), which are often solved together by a

--- PAGE 5 ---
Selection Method DescriptionParticipating Tasks
for TrainingParticipating Tasks
for Selection
ST-R Single-task random selection One None
ST-EC Single-task entropy-based conÔ¨Ådence One One
ST-DA Single-task dropout agreement One One
MT-R Multi-task random selection All None
MT-EC Multi-task entropy-based conÔ¨Ådence All One
MT-DA Multi-task dropout agreement All One
MT-AVG Multi-task average entropy-based conÔ¨Ådence All All
MT-AVGDA Multi-task average dropout agreement All All
MT-MAX Multi-task maximum entropy-based conÔ¨Ådence All All
MT-MIN Multi-task minimum entropy-based conÔ¨Ådence All All
MT-PAR Multi-task Pareto entropy-based conÔ¨Ådence All All
MT-RRF Multi-task Reciprocal Rank Fusion entropy-based conÔ¨Ådence All All
MT-IND Multi-task independent selection entropy-based conÔ¨Ådence All All
Table 1 : Summary of the ST-AL and MT-AL selection methods explored in this paper.
joint multi-task model (Finkel and Manning, 2009;
Nguyen and Nguyen, 2021).
5.1 Research Questions
We focus on three research questions. At Ô¨Årst,
we would like to establish whether MT-AL meth-
ods are superior to ST-AL methods for multi-task
learning. Our Ô¨Årst two questions are hence: Q1.1 :
Is multi-task learning effective in this setup? and
Q1.2 : Is AL effective? If so, which AL strategy is
better: ST-AL or MT-AL?
Next, notice that in MT-AL the conÔ¨Ådence score
of an example can be based on one or more of
the participating tasks. That is, even if the base
model for which training examples are selected
is an MTL model, the conÔ¨Ådence scores used by
the MT-AL algorithm can be based on one task or
more (¬ß4.2). Our third question is thus: Q1.3 : Is it
better to calculate conÔ¨Ådence scores based on one
of the participating tasks, or should we consider a
joint conÔ¨Ådence score, based on both tasks?4
5.2 Data
We consider the English version of the OntoNotes
5.0 corpus (Hovy et al., 2006), consisting of
seven textual domains: broadcast conversation
(BC), broadcast news (BN), magazine (MZ), news
(NW), bible (PT), telephone conversation (TC)
and web (WB). Sentences are annotated with
constituency-parse trees, named entities, part-of-
speech tags, as well as other labels. We convert
constituency-parse trees to dependency trees using
4This question naturally generalizes when more than two
tasks are involved.the ElitCloud conversion tool.5We do not report
results in the PT domain, as it is not annotated for
NER. Table 2 summarizes the number of sentences
per split for the OntoNotes domains, as well as for
the additional datasets used in our next setups.
5.3 Models
We consider two model types: Single-task and
multi-task models. Our single-task model (ST)
consists of the 12-layer pre-trained BERT-base en-
coder (Devlin et al., 2019), followed by a task de-
coder. At Ô¨Årst, we implemented a simple multi-
task model (SMT), consisting of a shared 12-layer
pre-trained BERT-base encoder followed by an in-
dependent decoder for each task. However, early
results suggested that it is inferior to single-task
modeling. We therefore implemented a more com-
plex multi-task model (CMT), illustrated in Figure
2. This model consists of (shared) cross-task and
task-speciÔ¨Åc modules, similar in nature to the ar-
chitecture proposed by Lin et al. (2018). In partic-
ular, it uses the 8 bottom BERT layers as shared
cross-task layers and employs t+ 1replications of
the 4 top BERT layers, one replication for each
task, as well as a shared cross-task replication.
The input text, as encoded by the shared 8 layers,
eS
1:8, is passed through the shared and non-shared
4-layer modules, eS
8:12andeUi
8:12, respectively. The
task classiÔ¨Åers are then fed with the output of the
cross-task layers combined with the output of their
task-speciÔ¨Åc layers, following the gating mecha-
5https://github.com/elitcloud/
elit-java .

--- PAGE 6 ---
DP-NER NER-RE SF-ID
BC BN MZ NW TC WB NYT24 NYT29 ScieRC WebNLG WLP ATIS SNIPS
Train 11,877 10,681 6,771 34,967 12,889 15,639 56,193 63,305 1,540 4,973 6,690 4,478 13,084
Dev 2,115 1,293 640 5,894 1,632 2,264 5,000 7,033 217 500 2,320 500 700
Test 2,209 1,355 778 2,325 1,364 1,683 5,000 4,006 451 689 2,343 893 700
Table 2 : Data statistics. We report the number of sentences in the original splits for each pair of tasks.
nism of Rotman and Reichart (2019):
ai(x) =(Wi
g[eS
8:12(x);eUi
8:12(x)] +bi
g);
gi(x) =ai(x)eS
8:12(x) + (1 ai(x))eUi
8:12(x);
where ;is the concatenation operator, is the
element-wise product, is the Sigmoid function,
andWi
gandbi
gare the gating mechanism parame-
ters. The combined vector gi(x)is then fed to the
i-th task-speciÔ¨Åc decoder.6
All implementations are based on Hugging-
Face‚Äôs Transformers package (Wolf et al., 2020).7
For all models, the DP decoder is based on the Bi-
afÔ¨Åne parser (Dozat and Manning, 2017) and the
NER decoder is a simple linear classiÔ¨Åer.
5.4 Training and Hyper-parameter Tuning
We consider the following hyper-parameters for
the AL experiments. At Ô¨Årst, we randomly sample
2% of the original training set to serve as the ini-
tial labeled examples in all experiments and treat
the rest of the training examples as unlabeled. We
also Ô¨Åx our development set to be twice the size
of our initial training set, by randomly sampling
examples from the original development set. We
then run each AL method for 5 iterations, where
at each iteration, the algorithm selects an unla-
beled set of the size of its initial training set (that
is, 2% of the original training set) for annotation.
We then reveal the labels of the selected examples
and add them to the training set of the next iter-
ation. At the beginning of the Ô¨Ånal iteration, our
labeled training set consists of 10% of the original
training data.
In each iteration, we train the models with 20K
gradient steps with an early stopping criterion ac-
cording to the development set. We report LAS
scores for DP and F1 scores for NER. For DP, we
measure our AL conÔ¨Ådence scores on the unla-
beled edges. When performing multi-task learn-
ing, we set the stopping criterion as the geometric
6We considered several other parameter-sharing schemes
but witnessed lower performance.
7https://github.com/huggingface/
transformers .
.
.
.
NLP
is
Shared 
Transformer 
Layer
Shared 
Transformer 
Layer
X 
8
Shared 
Transformer 
Layer
Shared 
Transformer 
Layer
X 
4
.
.
.
NER 
Transformer 
Layer
NER 
Transformer 
Layer
X 
4
.
.
.
DP 
Transformer 
Layer
DP 
Transformer 
Layer
X 
4
DP 
Gate
DP 
Decoder
NER 
Decoder
great
NER 
Gate
.
.
Figure 2 : Our complex multi-task model architec-
ture for DP and NER.
Full Training Active Learning
DP NER DP NER
Avg Best Avg Best Avg Best Avg Best
ST (CE) 87.17 1/6 70.35 0/6 86.43 3/6 74.65 6/6
SMT (CE) 86.94 2/6 67.51 0/6 85.86 2/6 70.31 0/6
CMT (CE) 87.04 3/6 72.79 6/6 85.91 1/6 72.11 0/6
ST (LS) 87.64 0/6 71.31 1/6 88.96 0/6 75.61 2/6
SMT (LS) 86.87 0/6 69.07 0/6 87.53 0/6 73.26 1/6
CMT (LS) 87.98 6/6 72.86 5/6 89.03 6/6 74.44 3/6
Table 3 : A comparison of a single-task model (ST),
a simple multi-task model (SMT), and our com-
plex multi-task model (CMT) in full training and
active learning. Models were trained with the
cross-entropy (CE) or label smoothing (LS) losses,
on all OntoNotes domains.
mean of the task scores (F1 for NER and LAS for
DP). We optimize all parameters using the ADAM
optimizer (Kingma and Ba, 2015) with a weight
decay of 0.01, a learning rate of 5e-5, and a batch
size of 32. For label smoothing (see below), we
use= 0:2. Following Dror et al. (2018), we
use the t-test for measuring statistical signiÔ¨Åcance
(p-value = 0:05).
5.5 Results
Model Architecture (Q1.1) We would Ô¨Årst like
to investigate the performance of the single-task
and multi-task models in the full training (FT)
and, more importantly, in the active learning (AL)

--- PAGE 7 ---
setups. We hence compare three architectures:
The single-task model (ST), the simple multi-task
model (SMT), and our complex multi-task model
(CMT). We train each model for DP and for NER
on the six OntoNotes domains using the cross-
entropy (CE) objective function, or with the label
smoothing objective (LS (Szegedy et al., 2016))
that has been demonstrated to decrease calibration
errors of Transformer models (Desai and Durrett,
2020; Kong et al., 2020). ST-AL is performed with
ST-EC and MT-AL with MT-AVG .
Table 3 reports the average scores (Avg col-
umn) over all domains and the number of do-
mains where each model achieved the best results
(Best). The results raise three important observa-
tions. First, SMT is worse on average than ST in
all setups, suggesting that vanilla MT is not al-
ways better than ST training . Second, our CMT
model achieves the best scores in most cases . The
only case where it is inferior to ST, but not to SMT,
is on AL with CE training. However, when train-
ing with LS, it achieves results comparable to or
higher than those of ST on AL.
Third, when comparing CE to LS training, LS
clearly improves the average scores of all models
(besides one case). Interestingly, the improvement
is more signiÔ¨Åcant in the AL setup than in the FT
setup. We report that when expanding these exper-
iments to all AL selection methods, LS was found
very effective for both tasks, outperforming CE in
most comparisons, with an average improvement
of 1.8% LAS for DP and 0.9 F1 for NER.
Multi-task vs. Single-task Performance (Q1.2)
We next ask whether MT-AL outperforms strong
ST-AL baselines. Figure 3 presents for every task
and domain the performance of the per-domain
best ST-AL and MT-AL methods after the Ô¨Ånal AL
iteration. Following our observations in Q1.1, we
train all models with the LS objective and base the
multi-task models on the effective CMT model.
Although there is no single method, MT-AL or
ST-AL, which performs best across all domains
and tasks, MT-AL seems to perform consistently
better. The Ô¨Ågure suggests that MT-AL is effec-
tive for both tasks, outperforming the best ST-AL
methods in 4 of 6 DP domains (results are not sta-
tistically signiÔ¨Åcant, the average p-value is 0.19)
and in 5 of 6 NER domains (results for 3 do-
mains are statistically signiÔ¨Åcant). While the av-
erage gap between MT-AL and ST-AL is small for
DP (0.28% LAS), in NER it is as high as 2.4 F1
BC BN MZ NW TC WB Average878889909192LASST-AL
MT-AL(a) Dependency Parsing
BC BN MZ NW TC WB Average5560657075808590F1ST-AL
MT-AL
(b) Named Entity Recognition
Figure 3 : Performance of the best ST-AL vs. the
best MT-AL method per domain (Q1.2).
points in favor of MT-AL. In fact, for half of the
NER domains, this gap is greater than 4.2 F1.
When comparing individual selection methods,
MT-AVG , and multi-task DP-based entropy, MT-
EC-DP , are the best selection methods for DP,
with average scores of 89.03% and 88.99%, re-
spectively. Single-task DP-based entropy, ST-EC-
DPis third, with an average score of 88.96%,
while the second best ST-AL method, ST-DA-
DP, is ranked only ninth among all methods, out-
performed by seven different MT-AL methods.
For NER, multi-task NER-based entropy, MT-EC-
NER , is the best model with an average F1 score
of 77.13, followed by MT-MAX with an average F1
score of 75.90. The single-task NER-based meth-
ods, ST-EC-NER andST-DA-NER are ranked only
Ô¨Åfth and sixth both with an average score of 75.60.
These results provide an additional indication of
the superiority of MT-AL.
In terms of the MT-AL selection methods that
do not perform a simple aggregation, MT-PAR
andMT-RRF perform similarly, averaging 88.31%
LAS for DP and 75.88 F1 for NER, while MT-IND
achieves poor results for DP and moderate results
for NER (an overall comparison of the MT-AL
methods is provided in x8)).
We next compare the per-iteration performance

--- PAGE 8 ---
1000 1500 2000 2500 3000 350088899091LAS
NW - DP
MT-EC-DP
ST-EC-DP
MT-R
1000 1500 2000 2500 3000 3500
Number of Sentences8082848688F1
NW - NER
MT-EC-NER
ST-EC-NER
MT-RFigure 4 : Performance as a function of the number
of training examples (Q1.2).
Within-task Cross-task Average
DP NER DP NER DP + NER
MT-AL winning % 47.22 63.88 90.74 89.81 78.78
Table 4 : A comparison of MT-AL vs. ST-AL on
within-task, cross-task and average performance.
Values indicate the percentage of comparisons in
which MT-AL methods were superior.
of ST-AL and MT-AL. To this end, Figure 4
presents the performance for the most prominent
MT-AL and ST-AL methods: MT-EC-DP andST-
EC-DP for DP and MT-EC-NER andST-EC-NER
for NER, together with the multi-task random se-
lection method MT-R . We plot for each method its
task score on the NW domain (the one with the
largest dataset) as a function of the training set
size, corresponding to 2% to 10% of the original
training examples. Clearly, the MT-AL methods
are superior across all AL iterations, indicating
the stability of MT-AL as well as its effectiveness
in low-resource setups . Similar patterns are also
observed in the other domains.
As a Ô¨Ånal evaluation for Q1.2, we directly com-
pare pairs of MT-AL and ST-AL methods, per-
forming three comparison types on each of the do-
mains: Within-task : Comparing the performance
ofST-EC-i andST-DA-i to their MT-AL counter-
parts ( MT-EC-i and MT-DA-i ) and to the joint-
selection methods on task i, either DP or NER
(108 comparisons); Cross-task : Comparing theDP NER Average
ST-EC-DP 88.96 71.34 80.15
ST-EC-NER 86.66 75.75 81.21
MT-EC-DP 88.99 73.75 81.37
MT-EC-NER 86.90 77.13 82.01
MT-AVG 89.02 74.44 81.74
MT-MAX 88.64 75.90 82.27
Table 5 : A comparison of AL methods that base
their selection on a single-task vs. joint-task con-
Ô¨Ådence scores. Results are averaged across the
OntoNotes domains (Q1.3).
performance of ST-EC-i andST-DA-i to their MT-
AL counterparts and to the joint-selection meth-
ods on the opposite task (e.g., if the models select
according to DP, we evaluate the NER task; 108
comparisons). This comparison allows us to eval-
uate the effect of single- and multi- task model-
ing on cross-task performance. Since single-task
models cannot be directly applied to the opposite
task, we record the examples selected by the ST-
AL method and train a model for the opposite task
on these examples; and Average : Comparing all
ST-AL methods to all MT-AL methods according
to their average performance on both tasks (264
comparisons).
Table 4 reports the percentage of comparisons
where the MT-AL methods are superior. On aver-
age, the two method types are on par when com-
paring Within-task performance. More interest-
ingly, for Cross-task performance MT-AL meth-
ods are clearly superior with around 90% win-
nings (87% of the cases statistically signiÔ¨Åcant).
Finally, the Average also supports the superior-
ity of MT-AL methods which perform better in
79% of the cases (all results are statistically signif-
icant). These results demonstrate the superiority
of MT-AL, particularly (and perhaps unsurpris-
ingly) when both tasks are considered.
Single-task vs. Joint-task Selection (Q1.3)
Next, we turn to our third question which com-
pares single-task vs. joint-task conÔ¨Ådence scores.
That is, we ask whether MT-AL methods that base
their selection criterion on more than one task are
better than ST-AL and MT-AL methods that com-
pute conÔ¨Ådence scores using a single task only.
To answer this question, we compare the two
best ST-AL and MT-AL methods that are based on
single-task selection to the two best joint-task se-

--- PAGE 9 ---
0.80 0.85 0.90 0.95 1.00
Confidence0.00.10.20.30.40.50.60.70.80.91.0AccuracyR squared = 0.10
(a) DP
0.900 0.925 0.950 0.975 1.000
Confidence0.00.10.20.30.40.50.60.70.80.91.0AccuracyR squared = 0.13
 (b) NER
Figure 5 : Sentence-level accuracy as a function of
entropy-based conÔ¨Ådence, for DP (left) and for
NER (right), when training with the CE objective.
The heat maps represent the point frequency.
lection MT-AL methods. As previously, all meth-
ods employ the LS objective. Table 5 reports the
average scores (across domains) of each of these
methods for DP, NER, and the average task score,
based on the Ô¨Ånal AL iteration.
While the method that performs best on aver-
age on both tasks is MT-MAX , a joint-selection
method, the second best method is MT-EC-NER ,
a single-task selection method, and the gap is
only 0.26 points. Not surprisingly, performance is
higher when the evaluated task also serves as the
task that the conÔ¨Ådence score is based on, either
solely or jointly with another task.
Although the joint-selection methods are ef-
fective for both tasks, we cannot decisively con-
clude that they are better than MT-AL methods
that perform single-task selection. However, we
do witness another conÔ¨Årmation for our answer to
Q1.2, as all presented MT-AL methods perform
better on average on both tasks (the Average col-
umn) than the ST-AL methods.
OverconÔ¨Ådence Analysis Originally, we
trained our models with the standard CE loss.
However, our early experiments suggested
that such CE-based training yields overconÔ¨Å-
dent models, which is likely to severely harm
conÔ¨Ådence-based AL methods. While previous
work demonstrated the positive impact of label
smoothing (LS) on model calibration, to the best
of our knowledge, the resulting impact on multi-
task learning has not been explored, speciÔ¨Åcally
not in the context of AL. We next analyze this
impact, which is noticeable in our above results,
in more detail.
Figure 5 presents sentence-level conÔ¨Ådence
scores as a function of sentence-level accuracyDP
BC BN MZ NW TC WB Average
ST-EC-DP (CE) 0.0720 0.0672 0.0757 0.0506 0.0584 0.3064 0.1051
ST-EC-DP (TS) 0.0753 0.0651 0.0673 0.0421 0.0545 0.3012 0.1009
ST-EC-DP (LS) 0.0269 0.0186 0.0122 0.0107 0.0289 0.1725 0.0450
ST-DA-DP (CE) 0.0410 0.0392 0.0414 0.0337 0.0381 0.1098 0.0505
NER
BC BN MZ NW TC WB Average
ST-EC-NER (CE) 0.0111 0.0167 0.0131 0.0146 0.0090 0.0132 0.0130
ST-EC-NER (TS) 0.0100 0.0172 0.0136 0.0108 0.0087 0.0142 0.0124
ST-EC-NER (LS) 0.0002 0.0002 0.0001 0.0002 0.0009 0.0010 0.0004
ST-DA-NER (CE) 0.0090 0.0121 0.0117 0.0122 0.0082 0.0110 0.0107
DP + NER
BC BN MZ NW TC WB Average
MT-AVG (CE) 0.0436 0.0499 0.0473 0.0345 0.0393 0.1681 0.0637
MT-AVG (TS) 0.0447 0.0468 0.0460 0.0318 0.0396 0.1645 0.0622
MT-AVG (LS) 0.0040 0.0018 0.0013 0.0012 0.0057 0.0546 0.0114
MT-AVGDA (CE) 0.0291 0.0287 0.0302 0.0236 0.0257 0.0718 0.0349
Table 6 : OverconÔ¨Ådence Error Results.
when separately training a single-task BERT-base
model on DP (left Ô¨Ågure) and on NER (right Ô¨Åg-
ure) with the CE objective. The conÔ¨Ådence scores
were computed according to the ST-EC scores.
The Ô¨Ågure conÔ¨Årms that the model tends to be
overconÔ¨Ådent in its predictions. Furthermore, the
lowR2values (0.1 and 0.13 for DP and NER, re-
spectively) indicate poor model calibration, since
conÔ¨Ådence scores are not correlated with actual
accuracy. Similar patterns were observed when
training our multi-task models with the CE objec-
tive.
Following this analysis, we turn to investigate
the impact of LS on model predictions in MT-AL.
Inspired by Thulasidasan et al. (2019), who de-
Ô¨Åned the overconÔ¨Ådence error (OE) for classiÔ¨Åca-
tion tasks, we Ô¨Årst slightly generalize OEto sup-
port sentence-level scores for token classiÔ¨Åcation
tasks. Given Nsentences, for each sentence xwe
start by calculating its accuracy score acc(x)over
its tokens. The conÔ¨Ådence score conf (x)is set
to the conÔ¨Ådence score of the corresponding AL
method. We then deÔ¨Åne OEas:
OE=1
NPN
x=1conf (x)max 
conf (x) acc(x);0
.
In essence,OEpenalizes predictions according to
the gap between their conÔ¨Ådence score and their
accuracy, but only when the former is higher.
In Table 6 we compare the OEscores of ST-EC ,
trained with the LS objective to 3 alternatives: ST-
ECtrained with the CE objective, ST-EC with the
post-processing method temperature scaling (TS),
andST-DA trained with the CE objective. Both
TS and dropout inference have been shown to im-
prove conÔ¨Ådence estimates (Guo et al., 2017; Ova-
dia et al., 2019), and hence serve as alternatives to
LS in this comparison. OE scores are reported

--- PAGE 10 ---
on the unlabeled set (given the true labels in hind-
sight) at the Ô¨Ånal AL iteration for both tasks. Addi-
tionally,OE scores for MT-AVG andMT-AVGDA
are also reported and averaged on both tasks.
The results are conclusive, LS is the least
overconÔ¨Ådent method, achieving the lowest OE
scores on all 18 setups, but one . While LS
achieves a proportionate reduction error (PRE) of
between 57.2% and 96.7% compared to the stan-
dard CE method, DA achieves at most a PRE of
51.9% and TS seems to have almost no effect.
These results conÔ¨Årm that LS is highly effective
in reducing overconÔ¨Ådence scores for BERT-based
models, and we are able to show for the Ô¨Årst time
that such a reduction also holds for multi-task
models.
6 MT-AL for Hierarchically-related
Tasks
Until now, we have considered tasks (DP and
NER) that are mutually informative but can be
trained independently of each other. However,
other multi-task learning scenarios involve a task
that is dependent on the output of another task. A
prominent example is the relation extraction (RE)
task that depends on the output of the NER task,
since the goal of RE is to classify and identify re-
lations between named entities. Importantly, if the
NER part of the model does not perform well, this
harms the RE performance as well. Sample selec-
tion in such a setup should hence reÔ¨Çect the hier-
archical relation between the tasks.
6.1 Selection Methods
Since the quality of the classiÔ¨Åer for the indepen-
dent task (NER) now affects also the quality of the
classiÔ¨Åer for the dependent task (RE), the conÔ¨Å-
dence of each of the tasks may get different rela-
tive importance values. Although this in principle
can also be true for independent tasks (¬ß5), explic-
itly accounting for this property seems more cru-
cial in the current setup.
We hence modify four of our joint-selection
methods (x4) to reÔ¨Çect the inherent a-symmetry
between the tasks, by presenting a scaling param-
eter01:8
8We do not include the MT-MAX andMT-MIN meth-
ods in our evaluation. Since the two tasks exhibit conÔ¨Ådence
scores in a similar range, scaling their conÔ¨Ådence scores ac-
cording to these methods resulted in selecting samples based
almost solely on the task with the higher (in the case of MT-
MAX ) or lower (in the case of MT-MIN ) scaling parameter.a)MT-AVG is now calculated as follows:
MT-AVG (x) =MT-EC-RE (x)+
(1 )MT-EC-NER (x):
b)MT-RRF is calculated similarly by multiplying
the RRF term of RE by and that of NER by 1 .
c)MT-IND is calculated by independently choos-
ing100%of the selected samples according to
the RE scores and 100(1 )%according to the
NER scores.
d)MT-PAR is computed by restricting the Ô¨Årst
Pareto condition for the position of the RE conÔ¨Å-
dence score: cREqc0
RE, whereqis the value
of the-quantile of the RE conÔ¨Ådence scores.9
We apply such a condition if  <0:5. Otherwise,
if > 0:5the condition is applied to the NER
component, and when it is equal to 0:5, the orig-
inal Pareto method is used. Since we restrict the
condition to only one of the tasks, fewer samples
will meet this condition (since 0q1), and
the Pareto frontier will include more samples that
have met the condition for the second task.
6.2 Research Questions
In our experiments, we would like to explore two
research questions: Q2.1 : Which MT-AL selec-
tion methods are most suitable for this setup ? and
Q2.2 : What is the best balance between the partic-
ipating tasks ?
Since RE fully relies on the output of NER, we
limit our experiments only to joint multi-task mod-
els and do not include single-task models.
6.3 Experimental Setup
We experiment with the span-based joint NER and
RE BERT model of Li et al. (2021).10Exper-
iments were conducted on Ô¨Åve diverse datasets:
NYT24 and NYT29 (Nayak and Ng, 2020), Sci-
eRC (Luan et al., 2018), WebNLG (Gardent et al.,
2017), and WLP (Kulkarni et al., 2018). The AL
setup is identical to that of ¬ß5.4. Other hyper-
parameters that were not mentioned before are
identical to those of the original implementation.
6.4 Results
Best Selection Method (Q2.1) We start by iden-
tifying the best selection method for this setup. Ta-
ble 7 summarizes the per-task average score for
9The second Pareto condition is similarly modiÔ¨Åed, but
now with the <sign.
10https://github.com/JiachengLi1995/
JointIE .

--- PAGE 11 ---
NER NER Best RE RE Best
MT-R 81.96 0/5 52.47 05
MT-AVG (= 1:0)82.64 1/5 59.51 2/5
MT-RRF (= 0:8)82.69 1/5 59.65 1/5
MT-IND (= 1:0)82.86 2/5 60.15 0/5
MT-PAR (= 0:7)82.68 1/5 59.57 2/5
Table 7 : Hierarchical MT-AL results. We report
best average F1 results over all Ô¨Åve datasets for
the bestconÔ¨Åguration per method.
the bestvalue of each method across the Ô¨Åve
datasets.
We observe three interesting patterns. First,
MT-AL is very effective in this setup for the de-
pendent task (RE) , while for the independent task
(NER), random selection does not fall too far be-
hind. Second, all MT-AL methods achieve better
performance for higher values by giving more
weight to the RE conÔ¨Ådence scores during the se-
lection process. This is an indication that indeed
the selection method should reÔ¨Çect the asymmetric
nature of the tasks. Third, overall, MT-IND is the
best performing method , averaging Ô¨Årst in NER
and in RE, while MT-AVG ,MT-RRF andMT-PAR
achieve similar results in both tasks.
Scaling ConÔ¨Åguration (Q2.2) Figure 6 presents
the average F1 scores of the four joint-selection
methods, as well as the random selection method
MT-R , as a function of (the relative weight
of the RE conÔ¨Ådence). First, we notice that
joint selection outperforms random selection in
the WebNLG domain only for RE (the dependent
task) but not for NER (except when approaches
1). Second, and more importantly, = 1, that is,
selecting examples only according to the conÔ¨Å-
dence score of the RE (dependent) task, is most
beneÔ¨Åcial for both tasks (Q2.1). We hypothesize
that this stems from the fact that the RE conÔ¨Ådence
score conveys information about both tasks and
that this combined information provides a stronger
signal with respect to the NER (independent) task,
compared to the NER conÔ¨Ådence score. Interest-
ingly, the positive impact of higher values of is
more prominent for NER, even though this means
that the role of the NER conÔ¨Ådence score is down-
played in the sample selection process.
For the individual selection methods, we report
thatMT-AVG andMT-IND achieve higher results
asincreases, while MT-PAR andMT-RRF peak
at= 0:7and= 0:8, respectively, and then
drop by 0.2 F1 points for NER and 0.9 F1 points
00.10.20.30.40.50.60.70.80.9 1
5859606162NER
NER AL
RE AL
NER Random
RE Random
4445464748
RE(a) ScieRC
00.10.20.30.40.50.60.70.80.9 1
8687888990NER
NER AL
RE AL
NER Random
RE Random26283032343638
RE
(b) WebNLG
Figure 6 : Average F1 scores over four joint-
selection methods as a function of (the relative
weight of the RE conÔ¨Ådence).
for RE on average.
7 MT-AL for Tasks with Different
Annotation Granularity
NLP tasks are deÔ¨Åned on different textual units,
with the most common examples of sentence-level
and token-level tasks. Our last investigation con-
siders the scenario of two closely-related tasks that
are of different granularity: Slot Ô¨Ålling (SF, token-
level) and intent detection (ID, sentence-level).
Due to the different annotation nature of the
two tasks, we have to deÔ¨Åne the cost of exam-
ple annotation with respect to each. Naturally,
there is no correct way to quantify these costs,
but we aim to propose a realistic model. We de-
note the cost of annotating a sample for SF with
CostSF=m+tpnt, wheremis the number
of tokens in the sentence, tpis a Ô¨Åxed token anno-
tation cost (we set tp= 1) andntis the number
of entities. The cost of annotating a sample for ID
is next denoted with CostID=m+ts, where

--- PAGE 12 ---
tsis a Ô¨Åxed sentence cost (we set ts= 3). Our
solution (see below) allows some examples to be
annotated only with respect to one of the tasks.
For examples that are annotated with respect to
both tasks we consider an additive joint cost where
the token-level term mis considered only once:
JCost =CostSF+CostID m. In our ex-
periments, we allow a Ô¨Åxed annotation budget of
B= 500 per AL iteration.
7.1 Methods
We consider three types of decision methods:
Greedy Active Learning (GRD_AL) : AL meth-
ods that at each iteration greedily choose the least
conÔ¨Ådent samples until the budget limitation is
reached; Binary Linear Programming Active
Learning (BLP_AL) : AL methods that at each
iteration opt to minimize the sum of conÔ¨Ådence
scores of the chosen samples given the budget con-
straints. The optimization problem (see below)
is solved using a BLP solver;11andBinary Lin-
ear Programming (BLP) : an algorithm that after
training on the initial training set chooses all the
samples at once, by solving the same constrained
optimization problem as in BLP_AL.
For each of these categories, we experiment
with four families of AL methods: a) Unre-
stricted Disjoint Sets (UDJS): This selection
method is based on the non-aggregated multi-task
conÔ¨Ådence scores, where each sample can be cho-
sen to be annotated on either task or both. The
UDJS optimization problem aims to maximize
the uncertainty scores ( 1 Conft(x)) of the se-
lected samples given the budget and selection con-
straints:
maxX
x2UX
t2T(1 Conft(x))Xt(x)
s:t:X
x2UX
t2TCostt(x)(Xt(x) Y(x))
+JCost (x)Y(x)B;
1
jTjX
t2TXt(x)Y(x)8x2U;
Xt(x);Y(x)2f0;1g 8x2U;t2T;
where Uis the unlabeled set, Tis the set of
tasks,Conftis the MT-EC-t conÔ¨Ådence score,
Xt(x)is a binary indicator indicating the annota-
11https://www.python-mip.com/ .tion of sample xon taskt, andY(x)is a binary in-
dicator indicating the annotation of xon all tasks.
Notice that this formulation may yield anno-
tated examples for only one of the tasks, although
this is unlikely, particularly under an iterative pro-
tocol when the conÔ¨Ådence scores of the models are
updated after each iteration.
b) Equal Budget Disjoint Sets (EQB-DJS) :
This strategy is similar to the above UDJS except
that the budget is equally divided between the two
tasks and the optimization problem is solved for
each of them separately. If a sample is chosen to
be annotated for both tasks, we update its cost ac-
cording to the joint cost and re-solve the optimiza-
tion problems until the entire budget is used.
c-f) Joint-task Selection : A sample could only
be chosen to be annotated on both tasks, where
conÔ¨Ådence scores are calculated using a multi-task
aggregation. The BLP optimization problem is
formulated as follows:
maxX
x2U(1 Conf (x))Y(x)
s:t:X
x2UJCost (x)Y(x)B;
Y(x)2f0;1g 8x2U;
whereConf is calculated by c)MT-AVG ,d)MT-
MAX ,e)MT-MIN orf)MT-RRF .12
g-j) Single-task ConÔ¨Ådence Selection
(STCS) : A sample could only be chosen to be
annotated on both tasks, where the selection
process aims to maximize the uncertainty scores
of only one of the tasks: g) STCS-SF orj)
STCS-ID . Similarly to Joint-task Selection, the
budget constraints are applied to the joint costs.
7.2 Research Questions
We focus on three research questions: Q3.1 : Does
BLP optimization improve upon greedy selection?
Q3.2 : Do optimization selection and active learn-
ing have a complementary effect? and Q3.3 : Is it
better to annotate all samples on both tasks or to
construct a disjoint annotated training set?
7.3 Experimental Setup
We conduct experiments on two prominent
datasets: ATIS (Price, 1990) and SNIPS (Coucke
12Since MT-PAR andMT-IND do not score the examples,
they can only be applied with the greedy decision method. A
discussion on their results is provided in x8.

--- PAGE 13 ---
ATIS SNIPS
BERT Roberta BERT Roberta
SF ID SF ID SF ID SF ID
MT-MINBLP _AL 84.53 92.27 87.94 92.05 77.90 97.00 71.35 95.29
MT-RRFGRD _AL 82.11 91.38 87.57 89.81 71.78 95.71 69.06 92.14
MT-AVGBLP 85.78 89.92 86.38 90.03 73.46 96.57 69.73 95.57
Table 8 : Comparison of decision categories (Q3.1
and Q3.2).
ATIS SNIPS
BERT Roberta BERT Roberta
SF ID SF ID SF ID SF ID
MT-MINBLP _AL 84.53 92.27 87.94 92.05 77.90 97.00 71.35 95.29
UDJSBLP _AL 87.46 90.48 85.05 91.71 73.56 96.71 70.72 95.14
EQB-DJS BLP _AL 86.59 88.02 85.25 89.14 69.92 91.43 63.58 95.00
Table 9 : Comparison of joint-task selection meth-
ods (Q3.3).
et al., 2018), and consider two Transformer-based
encoders: BERT-base (Devlin et al., 2019) and
Roberta-base (Liu et al., 2019b). Our code is
largely based on the implementation of Zhu and
Yu (2017).13We run the AL process for 5 itera-
tions with an initial training set of 50 random sam-
ples and a Ô¨Åxed-size development set of 100 ran-
dom samples. We train all models for 30 epochs
per iteration, with an early stopping criterion and
with label smoothing ( = 0:1). Other hyper-
parameters were set to their default values.
7.4 Results
Optimization-based Selection and AL (Q3.1
and Q3.2) To answer the Ô¨Årst two questions, we
show in Table 8 the Ô¨Ånal sample F1 performance
for both tasks, when selection is done with the best
selection method of each decision category: MT-
RRFGRD _AL,MT-MINBLP _ALandMT-AVGBLP.
The results conÔ¨Årm that the BLP optimization
(with AL) is indeed superior to greedy AL se-
lection , surpassing it in all setups (two tasks, two
datasets, and two pre-trained language encoders, 5
of the comparisons are statistically signiÔ¨Åcant).
The answer to Q3.2 is also mostly positive.
BLP optimization and iterative AL have a comple-
mentary effect, as MT-MINBLP _ALin most cases
achieves higher performance than MT-AVGBLP
(50% of the results are statistically signiÔ¨Åcant).
In fact, the answer for the two questions holds
true for all selection methods , as all perform best
using our novel BLP_AL decision method. Sec-
ond is BLP , indicating that the BLP formulation is
highly effective for MT setups under different bud-
13https://github.com/sz128/slot_
filling_and_intent_detection_of_SLU .get constraints. Finally, last is the standard greedy
procedure ( GRD_AL ) which is commonly used in
the AL literature.
Joint-task Selection (Q3.3) To answer Q3.3, we
perform a similar comparison in Table 9, but now
for the most prominent methods for each joint-task
selection method. The results indicate that MT-
MINBLP _ALthat enforces joint annotation is bet-
ter than allowing for non-restricted disjoint an-
notation (UDJSBLP _AL)and than equally split-
ting the budget between the two tasks (EQB-
DJSBLP _AL), where 9 of the 16 comparisons are
statistically signiÔ¨Åcant. We hypothesize that the
superiority of MT-MINBLP _ALstems from two
main reasons: 1. Integrating the joint aggrega-
tion conÔ¨Ådence score into the optimization func-
tion provides a better signal than the single-task
conÔ¨Ådence scores; 2. Having a joint dataset where
all samples are annotated on both tasks rather than
a disjointly annotated (and larger) dataset allows
the model to achieve better performance since the
two tasks are closely related.
Finally, we also report that ST-AL experiments
have led to poor performance. The ST-AL meth-
ods trail by 8.8 (SF) and 4.7 (ID) F1 points from
the best MT-AL method MT-MINBLP _ALon av-
erage. Interestingly, we also observe that selection
according to ID ( STCS-IDBLP _AL) has led to bet-
ter average results on both tasks than selection ac-
cording to SF ( STCS-SFBLP _AL), suggesting that
similarly tox6, selection according to the higher-
level task often yields better results.
8 Overall Comparison
As a Ô¨Ånal evaluation, we turn to compare the per-
formance of our proposed joint-selection MT-AL
methods in all setups. In our Ô¨Årst setup (¬ß5) we
implemented all MT-AL selection methods with
greedy selection, considering uniform task impor-
tance. Thereafter (¬ß6 and ¬ß7), we showed how
these selection methods can be modiÔ¨Åed in the
next two setups by either integrating non-uniform
task weights or replacing the greedy selection with
a BLP loss function overconÔ¨Ådence scores. How-
ever, not all of our MT-AL selection methods can
be modiÔ¨Åed in such ways.14We hence report
our Ô¨Ånal comparison given the conditions applied
in the Ô¨Årst setup: Assuming uniform task weights
and selecting samples in a greedy manner. Ta-
14See Footnotes 8 and 12 for further details.

--- PAGE 14 ---
DP + NER NER + RE SF + ID
DP NER NER RE SF ID Average
MT-R 86.40 70.78 81.96 52.47 76.94 91.95 76.75
MT-AVG 89.03 74.44 80.23 55.97 76.42 91.09 77.86
MT-MAX 88.64 75.90 81.50 57.74 76.64 89.17 78.27
MT-MIN 88.73 74.77 81.32 54.76 73.75 91.74 77.51
MT-PAR 88.31 75.86 81.47 56.92 77.72 90.28 78.43
MT-RRF 88.31 75.89 81.78 54.29 77.63 92.26 78.36
MT-IND 87.54 74.83 82.13 54.85 76.67 91.44 77.91
Table 10 : Average results of the joint multi-task selection methods across all three setups.
ble 10 summarizes the average performance of
the MT-AL methods in our three proposed setups:
Complementing tasks (DP + NER), hierarchically-
related tasks (NER + RE) and tasks of different
annotation granularity (SF + ID).
Each selection method has its cons and pros,
which we outline in our Ô¨Ånal discussion:
MT-R , not surprisingly, is the worst performing
method on average, as it makes no use of the
model‚Äôs predictions. Nevertheless, the method
performs quite well on the third setup (SF + ID)
when compared to the other methods that were
trained with the greedy decision method, the least
successful decision method of this setup. Next,
MT-AVG performs well when the tasks are of
equal importance (DP + NER), but achieves only
moderate performance on the other setups.
Surprisingly, MT-MAX is highly effective de-
spite its simplicity. It is mostly beneÔ¨Åcial for the
Ô¨Årst two setups (DP + NER and NER + RE), where
the tasks are of the same annotation granularity.
It is the third best method overall, and it does
not lag substantially behind the best method, MT-
PAR. Interestingly, MT-MIN , which offers a com-
plementary perspective to MT-MAX , is on average
the worst MT-AL method, excluding MT-R , and is
mainly beneÔ¨Åcial for the Ô¨Årst setup (DP + NER).
The next MT-AL method, MT-PAR , seems to
capture well the joint conÔ¨Ådence space of the task
pairs. It is the best method on average, achiev-
ing high average scores in all setups. However,
when incorporating it with other training tech-
niques, such as applying non-uniform weights (for
the second setup), it is outperformed by the other
MT-AL methods. MT-RRF does not lag far be-
hind MT-PAR , achieving similar results on most
tasks, excluding the RE and ID tasks, which are
the higher-level tasks of their setups. Finally, MT-
IND does not excel in three of the four tasks ofthe Ô¨Årst two setups, while achieving the best aver-
age results on NER, when jointly trained with RE.
Furthermore, the method demonstrates strong per-
formance on the third setup, when the tasks are of
different annotation granularity, justifying the in-
dependent annotation selection in this case.
9 Conclusions
We considered the problem of multi-task active
learning for pre-trained Transformer-based mod-
els. We posed multiple research questions con-
cerning the impact of multi-task modeling, multi-
task selection criteria, overconÔ¨Ådence reduction,
the relationships between the participating tasks,
as well as budget constraints, and presented a sys-
tematic algorithmic and experimental investiga-
tion in order to answer these questions. Our re-
sults demonstrate the importance of MT-AL mod-
eling in three challenging real-life scenarios, cor-
responding to diverse relations between the partic-
ipating tasks. In future work, we plan to research
setups with more than two tasks and consider lan-
guage generation and multilingual modeling.
10 Acknowledgments
We would like to thank the action editor and
the reviewers, as well as the members of the
IE@Technion NLP group for their valuable feed-
back and advice. This research was partially
funded by an ISF personal grant No. 1625/18. RR
also acknowledges the support of the Schmidt Ca-
reer Advancement Chair in AI.
References
Guirong Bai, Shizhu He, Kang Liu, Jun Zhao, and
Zaiqing Nie. 2020. Pre-trained language model
based active learning for sentence matching.

--- PAGE 15 ---
InProceedings of the 28th International Con-
ference on Computational Linguistics , pages
1495‚Äì1504.
Ying Chen, Wenjun Hou, Xiyao Cheng, and
Shoushan Li. 2018. Joint learning for emotion
classiÔ¨Åcation and emotion cause detection. In
Proceedings of the 2018 Conference on Empir-
ical Methods in Natural Language Processing ,
pages 646‚Äì651, Brussels, Belgium. Association
for Computational Linguistics.
Gordon V Cormack, Charles LA Clarke, and
Stefan Buettcher. 2009. Reciprocal rank fu-
sion outperforms condorcet and individual rank
learning methods. In Proceedings of the 32nd
international ACM SIGIR conference on Re-
search and development in information re-
trieval , pages 758‚Äì759.
Alice Coucke, Alaa Saade, Adrien Ball, Th√©odore
Bluche, Alexandre Caulier, David Leroy,
Cl√©ment Doumouro, Thibault Gisselbrecht,
Francesco Caltagirone, Thibaut Lavril, Ma√´l
Primet, and Joseph Dureau. 2018. Snips voice
platform: an embedded spoken language under-
standing system for private-by-design voice in-
terfaces. CoRR , abs/1805.10190.
Shrey Desai and Greg Durrett. 2020. Calibration
of pre-trained transformers. In Proceedings of
the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP) , pages
295‚Äì302.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training
of deep bidirectional transformers for language
understanding. In Proceedings of the 2019
Conference of the North American Chapter
of the Association for Computational Linguis-
tics: Human Language Technologies, Volume 1
(Long and Short Papers) , pages 4171‚Äì4186.
Timothy Dozat and Christopher D. Manning.
2017. Deep biafÔ¨Åne attention for neural de-
pendency parsing. In 5th International Confer-
ence on Learning Representations, ICLR 2017,
Toulon, France, April 24-26, 2017, Conference
Track Proceedings .
Rotem Dror, Gili Baumer, Segev Shlomov, and
Roi Reichart. 2018. The hitchhiker‚Äôs guideto testing statistical signiÔ¨Åcance in natural lan-
guage processing. In Proceedings of the 56th
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) ,
pages 1383‚Äì1392.
Long Duong, Hadi Afshar, Dominique Estival,
Glen Pink, Philip Cohen, and Mark Johnson.
2018. Active learning for deep semantic pars-
ing. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Lin-
guistics (Volume 2: Short Papers) , pages 43‚Äì48,
Melbourne, Australia. Association for Compu-
tational Linguistics.
Liat Ein-Dor, Alon Halfon, Ariel Gera, Eyal
Shnarch, Lena Dankin, Leshem Choshen, Ma-
rina Danilevsky, Ranit Aharonov, Yoav Katz,
and Noam Slonim. 2020. Active Learning for
BERT: An Empirical Study. In Proceedings of
the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP) , pages
7949‚Äì7962, Online. Association for Computa-
tional Linguistics.
Jenny Rose Finkel and Christopher D Manning.
2009. Joint parsing and named entity recogni-
tion. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics , pages 326‚Äì334.
Yarin Gal and Zoubin Ghahramani. 2016.
Dropout as a bayesian approximation: Repre-
senting model uncertainty in deep learning. In
international conference on machine learning ,
pages 1050‚Äì1059. PMLR.
Claire Gardent, Anastasia Shimorina, Shashi
Narayan, and Laura Perez-Beltrachini. 2017.
The WebNLG challenge: Generating text from
RDF data. In Proceedings of the 10th Interna-
tional Conference on Natural Language Gener-
ation , pages 124‚Äì133, Santiago de Compostela,
Spain. Association for Computational Linguis-
tics.
Daniel Grie√ühaber, Johannes Maucher, and
Ngoc Thang Vu. 2020. Fine-tuning BERT
for low-resource natural language understand-
ing via active learning. In Proceedings
of the 28th International Conference on
Computational Linguistics , pages 1158‚Äì1171,

--- PAGE 16 ---
Barcelona, Spain (Online). International Com-
mittee on Computational Linguistics.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q
Weinberger. 2017. On calibration of mod-
ern neural networks. In International Confer-
ence on Machine Learning , pages 1321‚Äì1330.
PMLR.
Gholamreza Haffari, Maxim Roy, and Anoop
Sarkar. 2009. Active learning for statistical
phrase-based machine translation. In Proceed-
ings of Human Language Technologies: The
2009 Annual Conference of the North American
Chapter of the Association for Computational
Linguistics , pages 415‚Äì423.
Sepp Hochreiter and J√ºrgen Schmidhuber. 1997.
Long short-term memory. Neural computation ,
9(8):1735‚Äì1780.
Eduard Hovy, Mitch Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
Ontonotes: the 90% solution. In Proceedings of
the human language technology conference of
the NAACL, Companion Volume: Short Papers ,
pages 57‚Äì60.
Fariz Ikhwantri, Samuel Louvan, Kemal Kur-
niawan, Bagas Abisena, Valdi Rachman, Al-
fan Farizki Wicaksono, and Rahmad Mahendra.
2018. Multi-task active learning for neural se-
mantic role labeling on low resource conver-
sational corpus. In Proceedings of the Work-
shop on Deep Learning Approaches for Low-
Resource NLP , pages 43‚Äì50.
Seokhwan Kim, Yu Song, Kyungduk Kim, Jeong-
Won Cha, and Gary Geunbae Lee. 2006. Mmr-
based active machine learning for bio named
entity recognition. In Proceedings of the Hu-
man Language Technology Conference of the
NAACL, Companion Volume: Short Papers ,
pages 69‚Äì72.
Diederik P Kingma and Jimmy Ba. 2015. Adam:
A method for stochastic optimization. In ICLR
(Poster) .
Lingkai Kong, Haoming Jiang, Yuchen Zhuang,
Jie Lyu, Tuo Zhao, and Chao Zhang. 2020.
Calibrated language model Ô¨Åne-tuning for in-
and out-of-distribution data. In Proceedings of
the 2020 Conference on Empirical Methods inNatural Language Processing (EMNLP) , pages
1326‚Äì1340, Online. Association for Computa-
tional Linguistics.
Chaitanya Kulkarni, Wei Xu, Alan Ritter, and
Raghu Machiraju. 2018. An annotated cor-
pus for machine reading of instructions in wet
lab protocols. In Proceedings of the 2018
Conference of the North American Chapter
of the Association for Computational Linguis-
tics: Human Language Technologies, Volume
2 (Short Papers) , pages 97‚Äì106, New Orleans,
Louisiana. Association for Computational Lin-
guistics.
Belinda Z. Li, Gabriel Stanovsky, and Luke Zettle-
moyer. 2020. Active learning for coreference
resolution using discrete annotation. In Pro-
ceedings of the 58th Annual Meeting of the As-
sociation for Computational Linguistics , pages
8320‚Äì8331, Online. Association for Computa-
tional Linguistics.
Jiacheng Li, Haibo Ding, Jingbo Shang, Julian
McAuley, and Zhe Feng. 2021. Weakly su-
pervised named entity tagging with learnable
logical rules. In Proceedings of the 59th An-
nual Meeting of the Association for Compu-
tational Linguistics and the 11th International
Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers) , pages 4568‚Äì
4581, Online. Association for Computational
Linguistics.
Zhenghua Li, Min Zhang, Yue Zhang, Zhanyi Liu,
Wenliang Chen, Hua Wu, and Haifeng Wang.
2016. Active learning for dependency pars-
ing with partial annotation. In Proceedings of
the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers) , pages 344‚Äì354.
Ying Lin, Shengqi Yang, Veselin Stoyanov, and
Heng Ji. 2018. A multi-lingual multi-task ar-
chitecture for low-resource sequence labeling.
InProceedings of the 56th Annual Meeting of
the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 799‚Äì809.
Xiaodong Liu, Pengcheng He, Weizhu Chen, and
Jianfeng Gao. 2019a. Multi-task deep neural
networks for natural language understanding.
InProceedings of the 57th Annual Meeting of
the Association for Computational Linguistics ,

--- PAGE 17 ---
pages 4487‚Äì4496, Florence, Italy. Association
for Computational Linguistics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei
Du, Mandar Joshi, Danqi Chen, Omer Levy,
Mike Lewis, Luke Zettlemoyer, and Veselin
Stoyanov. 2019b. Roberta: A robustly opti-
mized bert pretraining approach. arXiv preprint
arXiv:1907.11692 .
Alexander V . Lotov and Kaisa Miettinen. 2008.
Visualizing the pareto frontier. In Multiobjec-
tive optimization , pages 213‚Äì243. Springer.
Yi Luan, Luheng He, Mari Ostendorf, and Han-
naneh Hajishirzi. 2018. Multi-task identiÔ¨Åca-
tion of entities, relations, and coreference for
scientiÔ¨Åc knowledge graph construction. In
Proceedings of the 2018 Conference on Empir-
ical Methods in Natural Language Processing ,
pages 3219‚Äì3232, Brussels, Belgium. Associa-
tion for Computational Linguistics.
Bryan McCann, James Bradbury, Caiming Xiong,
and Richard Socher. 2017. Learned in transla-
tion: contextualized word vectors. In Proceed-
ings of the 31st International Conference on
Neural Information Processing Systems , pages
6297‚Äì6308.
Tapas Nayak and Hwee Tou Ng. 2020. Effec-
tive modeling of encoder-decoder architecture
for joint entity and relation extraction. In Pro-
ceedings of the AAAI conference on artiÔ¨Åcial in-
telligence , volume 34, pages 8528‚Äì8535.
Linh The Nguyen and Dat Quoc Nguyen. 2021.
PhoNLP: A joint multi-task learning model for
Vietnamese part-of-speech tagging, named en-
tity recognition and dependency parsing. In
Proceedings of the 2021 Conference of the
North American Chapter of the Association for
Computational Linguistics: Human Language
Technologies: Demonstrations , pages 1‚Äì7, On-
line. Association for Computational Linguis-
tics.
Joakim Nivre, Marie-Catherine de Marneffe, Filip
Ginter, Jan Haji Àác, Christopher D. Manning,
Sampo Pyysalo, Sebastian Schuster, Francis Ty-
ers, and Daniel Zeman. 2020. Universal Depen-
dencies v2: An evergrowing multilingual tree-
bank collection. In Proceedings of the 12thLanguage Resources and Evaluation Confer-
ence, pages 4034‚Äì4043, Marseille, France. Eu-
ropean Language Resources Association.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary
Nado, D Sculley, Sebastian Nowozin, Joshua
Dillon, Balaji Lakshminarayanan, and Jasper
Snoek. 2019. Can you trust your model‚Äôs uncer-
tainty? evaluating predictive uncertainty under
dataset shift. Advances in Neural Information
Processing Systems , 32:13991‚Äì14002.
√Ålvaro Peris and Francisco Casacuberta. 2018.
Active learning for interactive neural machine
translation of data streams. In Proceedings of
the 22nd Conference on Computational Natu-
ral Language Learning , pages 151‚Äì160, Brus-
sels, Belgium. Association for Computational
Linguistics.
Matthew Peters, Mark Neumann, Mohit Iyyer,
Matt Gardner, Christopher Clark, Kenton Lee,
and Luke Zettlemoyer. 2018. Deep contextu-
alized word representations. In Proceedings
of the 2018 Conference of the North American
Chapter of the Association for Computational
Linguistics: Human Language Technologies,
Volume 1 (Long Papers) , pages 2227‚Äì2237.
Patti Price. 1990. Evaluation of spoken language
systems: The atis domain. In Speech and Nat-
ural Language: Proceedings of a Workshop
Held at Hidden Valley, Pennsylvania, June 24-
27, 1990 .
Colin Raffel, Noam Shazeer, Adam Roberts,
Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J Liu.
2020. Exploring the limits of transfer learning
with a uniÔ¨Åed text-to-text transformer. Journal
of Machine Learning Research , 21:1‚Äì67.
Roi Reichart and Ari Rappoport. 2007. An ensem-
ble method for selection of high quality parses.
InProceedings of the 45th Annual Meeting of
the Association of Computational Linguistics ,
pages 408‚Äì415.
Roi Reichart and Ari Rappoport. 2009. Sam-
ple selection for statistical parsers: Cogni-
tively driven algorithms and evaluation mea-
sures. In Proceedings of the Thirteenth Con-
ference on Computational Natural Language
Learning (CoNLL-2009) , pages 3‚Äì11.

--- PAGE 18 ---
Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari
Rappoport. 2008. Multi-task active learning for
linguistic annotations. In Proceedings of ACL-
08: HLT , pages 861‚Äì869.
Guy Rotman and Roi Reichart. 2019. Deep con-
textualized self-training for low resource depen-
dency parsing. Transactions of the Association
for Computational Linguistics , 7:695‚Äì713.
Sebastian Ruder. 2017. An overview of multi-
task learning in deep neural networks. arXiv
preprint arXiv:1706.05098 .
Niloofar SaÔ¨Å Samghabadi, Parth Patwa, Srinivas
PYKL, Prerana Mukherjee, Amitava Das, and
Thamar Solorio. 2020. Aggression and misog-
yny detection using BERT: A multi-task ap-
proach. In Proceedings of the Second Work-
shop on Trolling, Aggression and Cyberbully-
ing, pages 126‚Äì131, Marseille, France. Euro-
pean Language Resources Association (ELRA).
Victor Sanh, Thomas Wolf, and Sebastian Ruder.
2019. A hierarchical multi-task approach for
learning embeddings from semantic tasks. In
Proceedings of the AAAI Conference on ArtiÔ¨Å-
cial Intelligence , volume 33, pages 6949‚Äì6956.
Nathan Schneider, Jena D. Hwang, Vivek Sriku-
mar, Jakob Prange, Austin Blodgett, Sarah R.
Moeller, Aviram Stern, Adi Bitan, and Omri
Abend. 2018. Comprehensive supersense dis-
ambiguation of English prepositions and pos-
sessives. In Proceedings of the 56th Annual
Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages
185‚Äì196, Melbourne, Australia. Association for
Computational Linguistics.
Burr Settles and Mark Craven. 2008. An analy-
sis of active learning strategies for sequence la-
beling tasks. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Lan-
guage Processing , pages 1070‚Äì1079.
H Sebastian Seung, Manfred Opper, and Haim
Sompolinsky. 1992. Query by committee. In
Proceedings of the Ô¨Åfth annual workshop on
Computational learning theory , pages 287‚Äì294.
Yanyao Shen, Hyokun Yun, Zachary Lipton,
Yakov Kronrod, and Animashree Anandkumar.
2017. Deep active learning for named en-
tity recognition. In Proceedings of the 2ndWorkshop on Representation Learning for NLP ,
pages 252‚Äì256, Vancouver, Canada. Associa-
tion for Computational Linguistics.
Anders S√∏gaard and Yoav Goldberg. 2016. Deep
multi-task learning with low level tasks super-
vised at lower layers. In Proceedings of the 54th
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers) ,
pages 231‚Äì235.
Christian Szegedy, Vincent Vanhoucke, Sergey
Ioffe, Jon Shlens, and Zbigniew Wojna. 2016.
Rethinking the inception architecture for com-
puter vision. In Proceedings of the IEEE con-
ference on computer vision and pattern recog-
nition , pages 2818‚Äì2826.
Sunil Thulasidasan, Gopinath Chennupati, Jeff A
Bilmes, Tanmoy Bhattacharya, and Sarah
Michalak. 2019. On mixup training: Improved
calibration and predictive uncertainty for deep
neural networks. In Advances in Neural Infor-
mation Processing Systems , volume 32. Curran
Associates, Inc.
Katrin Tomanek and Udo Hahn. 2010. A compari-
son of models for cost-sensitive active learning.
InColing 2010: Posters , pages 1247‚Äì1255.
Ashish Vaswani, Noam Shazeer, Niki Parmar,
Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
≈Åukasz Kaiser, and Illia Polosukhin. 2017. At-
tention is all you need. In Advances in neural
information processing systems , pages 5998‚Äì
6008.
Alex Wang, Amanpreet Singh, Julian Michael,
Felix Hill, Omer Levy, and Samuel Bowman.
2018. GLUE: A multi-task benchmark and
analysis platform for natural language under-
standing. In Proceedings of the 2018 EMNLP
Workshop BlackboxNLP: Analyzing and Inter-
preting Neural Networks for NLP , pages 353‚Äì
355, Brussels, Belgium. Association for Com-
putational Linguistics.
Maciej Wiatrak and Juha Iso-Sipila. 2020. Simple
hierarchical multi-task neural end-to-end entity
linking for biomedical text. In Proceedings of
the 11th International Workshop on Health Text
Mining and Information Analysis , pages 12‚Äì17,
Online. Association for Computational Linguis-
tics.

--- PAGE 19 ---
Thomas Wolf, Lysandre Debut, Victor Sanh,
Julien Chaumond, Clement Delangue, Anthony
Moi, Pierric Cistac, Tim Rault, R√©mi Louf,
Morgan Funtowicz, Joe Davison, Sam Shleifer,
Patrick von Platen, Clara Ma, Yacine Jernite,
Julien Plu, Canwen Xu, Teven Le Scao, Syl-
vain Gugger, Mariama Drame, Quentin Lhoest,
and Alexander M. Rush. 2020. Transformers:
State-of-the-art natural language processing. In
Proceedings of the 2020 Conference on Empir-
ical Methods in Natural Language Processing:
System Demonstrations , pages 38‚Äì45. Associa-
tion for Computational Linguistics.
Kaige Xie, Cheng Chang, Liliang Ren, Lu Chen,
and Kai Yu. 2018. Cost-sensitive active learn-
ing for dialogue state tracking. In Proceed-
ings of the 19th Annual SIGdial Meeting on
Discourse and Dialogue , pages 209‚Äì213, Mel-
bourne, Australia. Association for Computa-
tional Linguistics.
He Zhao, Longtao Huang, Rong Zhang, Quan Lu,
and Hui Xue. 2020. SpanMlt: A span-based
multi-task learning framework for pair-wise as-
pect and opinion terms extraction. In Proceed-
ings of the 58th Annual Meeting of the As-
sociation for Computational Linguistics , pages
3239‚Äì3248, Online. Association for Computa-
tional Linguistics.
Hua Zhu, Wu Ye, Sihan Luo, and Xidong Zhang.
2020. A multitask active learning framework
for natural language understanding. In Pro-
ceedings of the 28th International Conference
on Computational Linguistics , pages 4900‚Äì
4914, Barcelona, Spain (Online). International
Committee on Computational Linguistics.
Su Zhu and Kai Yu. 2017. Encoder-decoder with
focus-mechanism for sequence labelling based
spoken language understanding. In 2017 IEEE
International Conference on Acoustics, Speech
and Signal Processing (ICASSP) , pages 5675‚Äì
5679. IEEE.
