# Học tích cực đa nhiệm cho các mô hình dựa trên Transformer được đào tạo trước

Guy Rotman và Roi Reichart
Khoa Kỹ thuật Công nghiệp và Quản lý, Technion, IIT
grotman@campus.technion.ac.il
roiri@technion.ac.il

## Tóm tắt

Học đa nhiệm, trong đó một mô hình duy nhất học đồng thời nhiều nhiệm vụ, cho phép các mô hình NLP chia sẻ thông tin từ nhiều chú thích và có thể tạo ra dự đoán tốt hơn khi các nhiệm vụ có liên quan với nhau. Tuy nhiên, kỹ thuật này đòi hỏi chú thích cùng một văn bản với nhiều lược đồ chú thích khác nhau, điều này có thể tốn kém và tốn công sức. Học tích cực (AL) đã được chứng minh là tối ưu hóa các quy trình chú thích bằng cách lựa chọn lặp đi lặp lại các ví dụ chưa được gắn nhãn mà việc chú thích chúng có giá trị nhất cho mô hình NLP. Tuy nhiên, học tích cực đa nhiệm (MT-AL) chưa được áp dụng cho các mô hình NLP dựa trên Transformer được đào tạo trước hiện đại. Bài báo này nhằm mục đích thu hẹp khoảng cách này. Chúng tôi khám phá các tiêu chí lựa chọn đa nhiệm khác nhau trong ba kịch bản đa nhiệm thực tế, phản ánh các mối quan hệ khác nhau giữa các nhiệm vụ tham gia, và chứng minh hiệu quả của lựa chọn đa nhiệm so với lựa chọn đơn nhiệm vụ. Kết quả của chúng tôi gợi ý rằng MT-AL có thể được sử dụng hiệu quả để giảm thiểu nỗ lực chú thích cho các mô hình NLP đa nhiệm vụ.

## 1 Giới thiệu

Các mạng nơ-ron sâu (DNN) gần đây đã đạt được kết quả tối ưu cho nhiều nhiệm vụ và ứng dụng xử lý ngôn ngữ tự nhiên (NLP). Đặc biệt quan trọng là các mô hình nhúng theo ngữ cảnh (McCann et al., 2017; Peters et al., 2018), hầu hết trong số đó thực hiện các kiến trúc dựa trên transformer với cơ chế tự chú ý (Vaswani et al., 2017; Devlin et al., 2019; Raffel et al., 2020).

Tuy nhiên, DNN thường yêu cầu các tập dữ liệu huấn luyện được gắn nhãn lớn để đạt được hiệu suất tốt. Trong khi việc chú thích các tập dữ liệu huấn luyện như vậy tốn kém và tốn công sức, mô hình học tích cực (AL) nhằm mục đích giảm thiểu các chi phí này bằng cách lựa chọn lặp đi lặp lại các ví dụ huấn luyện có giá trị để chú thích. Gần đây, AL đã được chứng minh là hiệu quả cho DNN trên các nhiệm vụ NLP khác nhau (Duong et al., 2018; Peris và Casacuberta, 2018; Ein-Dor et al., 2020).

Một khả năng hấp dẫn của DNN là thực hiện học đa nhiệm (MTL): Học nhiều nhiệm vụ bằng một mô hình duy nhất (Ruder, 2017). Điều này xuất phát từ tính linh hoạt kiến trúc của chúng - xây dựng các kiến trúc ngày càng sâu và rộng hơn từ các khối xây dựng cơ bản - và trong việc tối ưu hóa dựa trên gradient cho phép chúng cập nhật đồng thời các tham số từ nhiều mục tiêu dựa trên nhiệm vụ. Thật vậy, MTL đã trở nên phổ biến trong NLP (Luan et al., 2018; Liu et al., 2019a).

Các mô hình MTL cho NLP thường có thể hưởng lợi từ việc sử dụng các corpus được chú thích cho nhiều nhiệm vụ, đặc biệt là khi các nhiệm vụ này có liên quan chặt chẽ và có thể thông tin cho nhau. Các ví dụ nổi bật về corpus đa nhiệm bao gồm OntoNotes (Hovy et al., 2006), Universal Dependencies Bank (Nivre et al., 2020) và STREUSLE (Schneider et al., 2018). Với tầm quan trọng của corpus đa nhiệm cho nhiều thiết lập MTL, các khung AL hiệu quả hỗ trợ MTL đang trở nên quan trọng.

Thật không may, hầu hết các phương pháp AL không hỗ trợ chú thích cho nhiều hơn một nhiệm vụ. Học tích cực đa nhiệm (MT-AL) được đề xuất bởi Reichart et al. (2008) trước kỷ nguyên nơ-ron, và được điều chỉnh bởi Ikhwantri et al. (2018) cho một kiến trúc nơ-ron. Gần đây, Zhu et al. (2020) đề xuất một mô hình MT-AL cho việc điền khe và phát hiện ý định, tập trung chủ yếu vào LSTM (Hochreiter và Schmidhuber, 1997).

Trong bài báo này, chúng tôi là những người đầu tiên khám phá một cách có hệ thống MT-AL cho các mô hình Transformer được đào tạo trước lớn. Tự nhiên, trọng tâm của chúng tôi là các nhiệm vụ NLP có liên quan chặt chẽ, mà việc chú thích đa nhiệm của cùng một corpus có khả năng có lợi. Đặc biệt, chúng tôi xem xét ba kịch bản đa nhiệm thực tế đầy thử thách, phản ánh các mối quan hệ khác nhau giữa các nhiệm vụ NLP tham gia: 1. Các nhiệm vụ bổ sung, trong đó mỗi nhiệm vụ có thể cung cấp thông tin có giá trị cho nhiệm vụ khác: Phân tích cú pháp phụ thuộc (DP) và nhận dạng thực thể có tên (NER); 2. Các nhiệm vụ liên quan theo thứ bậc, trong đó một trong các nhiệm vụ phụ thuộc vào đầu ra của nhiệm vụ khác: Trích xuất mối quan hệ (RE) và NER; và 3. Các nhiệm vụ với độ chi tiết chú thích khác nhau: Điền khe (SF, mức token) và phát hiện ý định (ID, mức câu). Chúng tôi đề xuất các phương pháp MT-AL mới khác nhau và điều chỉnh chúng theo các đặc tính cụ thể của các kịch bản, nhằm giải quyết đúng cách các mối quan hệ cơ bản giữa các nhiệm vụ tham gia. Kết quả thực nghiệm của chúng tôi làm nổi bật một số lượng lớn các mô hình có thể hướng dẫn các nhà nghiên cứu NLP khi chú thích corpus với nhiều lược đồ chú thích bằng cách sử dụng mô hình AL.

## 2 Công trình trước đây

Bài báo này giải quyết một vấn đề chưa được khám phá trước đây: học tích cực đa nhiệm (MT-AL) cho NLP với các mô hình dựa trên Transformer được đào tạo trước. Do đó, chúng tôi bắt đầu bằng cách trình bày AL trong NLP và sau đó tiếp tục với học đa nhiệm (MTL) trong NLP.

### 2.1 Học tích cực trong NLP

AL đã được áp dụng thành công cho nhiều nhiệm vụ NLP khác nhau, bao gồm phân tích cú pháp ngữ nghĩa (Duong et al., 2018), phân tích cú pháp (Reichart và Rappoport, 2009; Li et al., 2016), giải quyết đồng tham chiếu (Li et al., 2020), nhận dạng thực thể có tên (Shen et al., 2017) và dịch máy (Haffari et al., 2009), để kể một vài. Các công trình gần đây chứng minh rằng các mô hình như BERT có thể hưởng lợi từ AL trong các thiết lập tài nguyên thấp (Ein-Dor et al., 2020; Grießhaber et al., 2020), và Bai et al. (2020) đề xuất dựa trên tiêu chí lựa chọn AL trên kiến thức ngôn ngữ được nắm bắt bởi BERT. Các công trình khác thực hiện AL nhạy cảm với chi phí, nơi các thể hiện có thể có chi phí khác nhau (Tomanek và Hahn, 2010; Xie et al., 2018). Tuy nhiên, hầu hết các công trình trước đây không áp dụng AL cho MTL, đây là trọng tâm chính của chúng tôi.

### 2.2 Học đa nhiệm (MTL) trong NLP

MTL đã trở nên ngày càng phổ biến trong NLP, đặc biệt là khi các nhiệm vụ được giải quyết có liên quan chặt chẽ (Chen et al., 2018; Safi Samghabadi et al., 2020; Zhao et al., 2020). Trong một số trường hợp, mô hình MTL được huấn luyện theo cách thứ bậc, trong đó thông tin được truyền từ các nhiệm vụ cấp thấp hơn (đôi khi là phụ trợ) đến các nhiệm vụ cấp cao hơn (Søgaard và Goldberg, 2016; Rotman và Reichart, 2019; Sanh et al., 2019; Wiatrak và Iso-Sipila, 2020). Trong các trường hợp khác, các corpus được gắn nhãn khác nhau có thể được hợp nhất để phục vụ như các điểm chuẩn đa nhiệm (McCann et al., 2017; Wang et al., 2018). Theo cách này, một mô hình MTL duy nhất có thể được huấn luyện trên nhiều nhiệm vụ, thường chỉ có liên quan xa. Nghiên cứu này xem xét thiết lập các nhiệm vụ có liên quan chặt chẽ trong đó việc chú thích một corpus duy nhất đối với nhiều nhiệm vụ là một chiến lược hữu ích.

## 3 Định nghĩa nhiệm vụ - Học tích cực đa nhiệm (MT-AL)

Trong thiết lập MT-AL, thuật toán AL được cung cấp một corpus văn bản, nơi một tập hợp ban đầu (thường là nhỏ) gồm n₀ ví dụ được gắn nhãn cho t nhiệm vụ. Thuật toán AL thực hiện một quá trình lặp, nơi ở lần lặp thứ i, mục tiêu của thuật toán AL là chọn nᵢ ví dụ chưa được gắn nhãn bổ sung sẽ được chú thích trên tất cả t nhiệm vụ, sao cho hiệu suất của mô hình NLP cơ sở sẽ được cải thiện càng nhiều càng tốt đối với tất cả chúng. Trong khi chiến lược tham lam như vậy để đạt được nhiều nhất trong lần lặp thứ i có thể không mang lại hiệu suất tốt nhất trong các lần lặp tiếp theo, hầu hết các thuật toán AL đều tham lam, và do đó chúng tôi cũng theo chiến lược này ở đây.

Chúng tôi tập trung vào thiết lập tiêu chuẩn của AL dựa trên độ tin cậy, nơi các ví dụ chưa được gắn nhãn với độ tin cậy mô hình thấp nhất được chọn để chú thích. Thuật toán 1 trình bày một phác thảo chung của các thuật toán AL như vậy, trong bối cảnh MTL. Khung này, lần đầu tiên được giới thiệu bởi Reichart et al. (2008), là một tổng quát hóa đơn giản của khung AL đơn nhiệm vụ (ST-AL), hỗ trợ việc chú thích dữ liệu đối với nhiều nhiệm vụ.

**Thuật toán 1** Học tích cực đa nhiệm dựa trên độ tin cậy (MT-AL dựa trên độ tin cậy)
**Đầu vào:** Dữ liệu được gắn nhãn L (được chú thích trên t nhiệm vụ), Dữ liệu chưa được gắn nhãn U
**Thuật toán:**
Với i = 1; ...; T:
1. Huấn luyện một mô hình học đa nhiệm (MTL) h trên L.
2. Với mỗi u ∈ U, tính điểm tin cậy tổng hợp Ch(u) của nó trên tất cả t nhiệm vụ theo h.
3. Chọn nᵢ ví dụ chưa được gắn nhãn từ U với điểm tin cậy Ch(u) thấp nhất và gửi chúng để chú thích theo tất cả t nhiệm vụ.
4. Thêm các ví dụ mới được gắn nhãn vào L và xóa chúng khỏi U.

Như đã thảo luận trong §1, chúng tôi khám phá một số biến thể của thiết lập MT-AL: Các nhiệm vụ độc lập thông tin cho nhau (§5), các nhiệm vụ liên quan theo thứ bậc, nơi một nhiệm vụ phụ thuộc vào đầu ra của nhiệm vụ khác (§6), và các nhiệm vụ với độ chi tiết chú thích khác nhau, mức từ và mức câu (§7). Trước khi chúng tôi có thể giới thiệu các thuật toán MT-AL cho mỗi thiết lập này, trước tiên chúng tôi cần đặt nền tảng chung của chúng: Điểm tin cậy mô hình đơn nhiệm vụ và đa nhiệm.

## 4 Ước tính độ tin cậy trong học tích cực đơn nhiệm vụ và đa nhiệm

Chúng tôi bây giờ giới thiệu các điểm tin cậy mà chúng tôi xem xét cho học tích cực đơn nhiệm vụ (ST-AL) và đa nhiệm (MT-AL). Các điểm tin cậy này về cơ bản là cốt lõi của các thuật toán AL dựa trên độ tin cậy (xem Bước 2-3 của Thuật toán 1). Trong Bảng 1, chúng tôi cung cấp một tóm tắt về các phương pháp lựa chọn ST-AL và MT-AL khác nhau mà chúng tôi khám phá.

### 4.1 Điểm tin cậy đơn nhiệm vụ

Chúng tôi xem xét ba điểm tin cậy đã được sử dụng rộng rãi trong ST-AL:

**Ngẫu nhiên (ST-R)** Phương pháp cơ sở này đơn giản là gán điểm ngẫu nhiên cho các ví dụ chưa được gắn nhãn.

**Độ tin cậy dựa trên Entropy (ST-EC)** Điểm tin cậy dựa trên entropy đơn nhiệm vụ được định nghĩa là:
ST-EC(x) = 1 - E(x) (1)

Đối với các nhiệm vụ phân loại câu như ID, E(x) đơn giản là entropy trên các dự đoán lớp của một mẫu x chia cho log số lượng nhãn. Trong các nhiệm vụ phân loại token của chúng tôi (DP, NER, RE và SF), E(x) là entropy chuẩn hóa mức câu (Kim et al., 2006), cho phép chúng tôi ước tính độ không chắc chắn của mô hình cho một chuỗi token nhất định x = (x₁...xₘ):

E(x) = (1/m·log s) ∑ᵢ₌₁ᵐ ∑ⱼ₌₁ˢ p(yⱼ|xᵢ) log p(yⱼ|xᵢ) (2)

trong đó m là số lượng token, yⱼ là nhãn thứ j có thể, và s là số lượng nhãn. Chúng tôi thực hiện chuẩn hóa entropy bằng cách lấy trung bình các entropy mức token, để giảm thiểu ảnh hưởng của độ dài câu, và bằng cách chia điểm cho log số lượng nhãn. Điểm tin cậy kết quả nằm trong khoảng từ 0 đến 1, nơi giá trị thấp hơn cho thấy độ chắc chắn thấp hơn.

**Thỏa thuận Dropout (ST-DA)** Các phương pháp ensemble đã được chứng minh hiệu quả cho AL (xem, ví dụ, (Seung et al., 1992; Settles và Craven, 2008)). Trong bài báo này, chúng tôi rút ra một điểm tin cậy được lấy cảm hứng từ Reichart và Rappoport (2007). Chúng tôi bắt đầu bằng cách tạo k = 10 mô hình khác nhau bằng cách thực hiện suy luận dropout k lần (Gal và Ghahramani, 2016). Sau đó, chúng tôi tính điểm thỏa thuận dropout đơn nhiệm vụ cho một câu x bằng cách tính thỏa thuận mức token trung bình trên các cặp mô hình:

ST-DA(x) = (1/mk(k-1)) ∑ⱼ≠ⱼ' ∑ᵢ₌₁ᵐ 1{ŷᵢʲ = ŷᵢʲ'} (3)

trong đó ŷᵢʲ là nhãn dự đoán của mô hình j cho token thứ i. Điểm kết quả nằm trong khoảng từ 0 đến 1, nơi giá trị thấp hơn cho thấy độ chắc chắn thấp hơn.

### 4.2 Điểm tin cậy đa nhiệm

Khi rút ra điểm tin cậy cho MT-AL, nhiều lựa chọn thiết kế nên được thực hiện. Đầu tiên, điểm tin cậy của một mô hình đa nhiệm có thể dựa trên cả hai nhiệm vụ hoặc chỉ trên một trong số chúng. Chúng tôi ký hiệu bằng MT-EC và MT-DA các điểm tin cậy tương đương với ST-EC và ST-DA: Sự khác biệt duy nhất (quan trọng) là chúng được tính cho một mô hình đa nhiệm. Để rõ ràng, chúng tôi sẽ bổ sung ký hiệu này với tên của nhiệm vụ theo đó độ tin cậy được tính. Ví dụ, ST-EC-NER và MT-EC-NER là điểm EC được tính bằng cách sử dụng bộ phân loại nhận dạng thực thể có tên (NER) của một mô hình đơn nhiệm vụ và đa nhiệm, tương ứng.

Do đó, chúng tôi có thể đánh giá các thuật toán MT-AL trên lựa chọn chéo nhiệm vụ, tức là khi nhiệm vụ được đánh giá khác với nhiệm vụ được sử dụng để tính điểm tin cậy (và do đó để lựa chọn mẫu). Ví dụ, đánh giá hiệu suất của một mô hình đa nhiệm, được huấn luyện cùng trên NER và DP, trên nhiệm vụ DP khi điểm tin cậy được sử dụng bởi thuật toán MT-AL chỉ dựa trên bộ phân loại NER (MT-EC-NER).

Chúng tôi cũng xem xét một họ điểm tin cậy cho MT-AL được tính đối với tất cả các nhiệm vụ tham gia (điểm lựa chọn chung). Với mục đích này, chúng tôi xem xét ba lược đồ tổng hợp đơn giản sử dụng các toán tử trung bình, tối đa hoặc tối thiểu trên các điểm tin cậy đơn nhiệm vụ. Ví dụ, độ tin cậy trung bình đa nhiệm (MT-AVG) lấy trung bình cho một mẫu x các điểm tin cậy dựa trên entropy trên tất cả t nhiệm vụ:

MT-AVG(x) = (1/t) ∑ᵢ₌₁ᵗ MT-EC-i(x) (4)

Điểm thỏa thuận dropout trung bình đa nhiệm (MT-AVGDA) được định nghĩa tương tự, nhưng việc lấy trung bình là trên các điểm MT-DA. Cuối cùng, tối đa (tối thiểu) đa nhiệm MT-MAX (MT-MIN) được tính theo cách tương tự như MT-AVG nhưng với toán tử max (min) được thực hiện trên các entropy tin cậy cụ thể cho nhiệm vụ.

**Vượt ra ngoài thao tác trực tiếp của điểm tin cậy** Vì trọng tâm của chúng tôi trong bài báo này là lựa chọn đa nhiệm, chúng tôi muốn xem xét các phương pháp lựa chọn bổ sung vượt ra ngoài các phương pháp đơn giản trong công trình trước đây. Nguyên tắc chung của các phương pháp này là chúng ít nhạy cảm với các giá trị thực tế của điểm tin cậy và thay vào đó xem xét tầm quan trọng tương đối của ví dụ đối với các nhiệm vụ tham gia.

Đầu tiên, chúng tôi xem xét MT-PAR, dựa trên biên giới hiệu quả Pareto (Lotov và Miettinen, 2008). Chúng tôi bắt đầu bằng cách biểu diễn mỗi mẫu chưa được gắn nhãn như một vector không gian t-chiều c, trong đó cᵢ = ST-EC-i là điểm tin cậy ST cho nhiệm vụ i. Tiếp theo, chúng tôi chọn tất cả các mẫu mà vector tương ứng thuộc về biên giới hiệu quả Pareto. Một điểm thuộc về biên giới nếu đối với mọi vector c' khác, các điều kiện sau đây đều đúng: 1. ∀i ∈ [t]; cᵢ ≤ c'ᵢ và 2. ∃i ∈ [t]; cᵢ < c'ᵢ. Nếu số lượng mẫu trong biên giới nhỏ hơn tổng số mẫu cần chọn (n), chúng tôi lặp lại quy trình bằng cách loại bỏ các vector của các mẫu đã chọn và tính các điểm Pareto tiếp theo. Nếu vẫn còn p điểm cần được chọn nhưng số lượng các điểm Pareto cuối cùng (f) vượt quá p, chúng tôi chọn mỗi ⌊f/p⌋ điểm, được sắp xếp theo trục đầu tiên.

Tiếp theo, được lấy cảm hứng từ lĩnh vực truy xuất thông tin, chúng tôi đề xuất MT-RRF. Phương pháp này cho phép chúng tôi xem xét thứ hạng của mỗi ví dụ đối với các nhiệm vụ tham gia, thay vì các giá trị tin cậy thực tế. Chúng tôi đầu tiên tính rᵢ, danh sách xếp hạng của nhiệm vụ thứ i, bằng cách xếp hạng các ví dụ theo điểm ST-EC-i của chúng, từ thấp nhất đến cao nhất. Chúng tôi tiếp theo hợp nhất các danh sách xếp hạng t kết quả thành một danh sách xếp hạng duy nhất R, sử dụng kỹ thuật hợp nhất thứ hạng đối ứng (RRF) (Cormack et al., 2009). Điểm RRF của một ví dụ x được tính như sau:

RRF-Score(x) = ∑ᵢ₌₁ᵗ 1/(k + rᵢ(x)) (5)

trong đó k là một hằng số, được đặt thành 60, như trong bài báo gốc. Thứ hạng cuối cùng được tính trên điểm RRF của các ví dụ - từ cao nhất đến thấp nhất. Các ví dụ được xếp hạng cao hơn được chọn trước để chú thích vì chúng có điểm tin cậy thấp hơn. Cuối cùng, MT-IND độc lập chọn ⌊n/t⌋ mẫu không chắc chắn nhất theo mỗi nhiệm vụ bằng cách xếp hạng điểm ST-EC và lặp lại nếu xảy ra chồng chéo.

Chúng tôi cuối cùng so sánh các mẫu đã chọn của sáu phương pháp MT-AL, sau khi huấn luyện một mô hình đa nhiệm cho một lần lặp AL duy nhất trên các nhiệm vụ DP và NER (Hình 1). Hóa ra trong khi một số phương pháp có xu hướng chọn các tập con ví dụ rất tương tự để chú thích (ví dụ, MT-IND và MT-RRF chia sẻ 94% các mẫu đã chọn, và MT-AVG và MT-MIN chia sẻ 84% của chúng), những phương pháp khác khác biệt đáng kể trong việc lựa chọn của chúng (ví dụ, MT-MAX chỉ chia sẻ 16% các mẫu đã chọn với MT-MIN và 20% với MT-IND). Quan sát này khuyến khích chúng tôi tiếp tục điều tra tác động của các phương pháp lựa chọn khác nhau đối với MT-AL.

## 5 MT-AL cho các nhiệm vụ bổ sung

Chúng tôi bắt đầu bằng cách điều tra MT-AL cho hai nhiệm vụ cú pháp có liên quan chặt chẽ, bổ sung: Phân tích cú pháp phụ thuộc (DP) và Nhận dạng thực thể có tên (NER), thường được giải quyết cùng nhau bởi một mô hình đa nhiệm chung (Finkel và Manning, 2009; Nguyen và Nguyen, 2021).

### 5.1 Câu hỏi nghiên cứu

Chúng tôi tập trung vào ba câu hỏi nghiên cứu. Đầu tiên, chúng tôi muốn xác định liệu các phương pháp MT-AL có vượt trội hơn các phương pháp ST-AL cho học đa nhiệm hay không. Hai câu hỏi đầu tiên của chúng tôi do đó là: Q1.1: Học đa nhiệm có hiệu quả trong thiết lập này không? và Q1.2: AL có hiệu quả không? Nếu có, chiến lược AL nào tốt hơn: ST-AL hay MT-AL?

Tiếp theo, lưu ý rằng trong MT-AL, điểm tin cậy của một ví dụ có thể dựa trên một hoặc nhiều nhiệm vụ tham gia. Đó là, ngay cả khi mô hình cơ sở mà các ví dụ huấn luyện được chọn là một mô hình MTL, điểm tin cậy được sử dụng bởi thuật toán MT-AL có thể dựa trên một nhiệm vụ hoặc nhiều hơn (§4.2). Câu hỏi thứ ba của chúng tôi do đó là: Q1.3: Tốt hơn là tính điểm tin cậy dựa trên một trong các nhiệm vụ tham gia, hay chúng ta nên xem xét một điểm tin cậy chung, dựa trên cả hai nhiệm vụ?

### 5.2 Dữ liệu

Chúng tôi xem xét phiên bản tiếng Anh của corpus OntoNotes 5.0 (Hovy et al., 2006), bao gồm bảy miền văn bản: cuộc trò chuyện phát thanh (BC), tin tức phát thanh (BN), tạp chí (MZ), tin tức (NW), kinh thánh (PT), cuộc trò chuyện điện thoại (TC) và web (WB). Các câu được chú thích với cây phân tích cú pháp, thực thể có tên, thẻ từ loại, cũng như các nhãn khác. Chúng tôi chuyển đổi cây phân tích cú pháp thành cây phụ thuộc bằng công cụ chuyển đổi ElitCloud. Chúng tôi không báo cáo kết quả trong miền PT, vì nó không được chú thích cho NER. Bảng 2 tóm tắt số lượng câu cho mỗi phần của các miền OntoNotes, cũng như các bộ dữ liệu bổ sung được sử dụng trong các thiết lập tiếp theo của chúng tôi.

### 5.3 Mô hình

Chúng tôi xem xét hai loại mô hình: Mô hình đơn nhiệm vụ và đa nhiệm. Mô hình đơn nhiệm vụ (ST) của chúng tôi bao gồm bộ mã hóa BERT-base được đào tạo trước 12 lớp (Devlin et al., 2019), tiếp theo bởi một bộ giải mã nhiệm vụ. Đầu tiên, chúng tôi thực hiện một mô hình đa nhiệm đơn giản (SMT), bao gồm một bộ mã hóa BERT-base được đào tạo trước 12 lớp chung tiếp theo bởi một bộ giải mã độc lập cho mỗi nhiệm vụ. Tuy nhiên, kết quả sớm cho thấy rằng nó kém hơn so với mô hình đơn nhiệm vụ. Do đó, chúng tôi thực hiện một mô hình đa nhiệm phức tạp hơn (CMT), được minh họa trong Hình 2. Mô hình này bao gồm các mô-đun chéo nhiệm vụ (được chia sẻ) và cụ thể cho nhiệm vụ, tương tự về bản chất với kiến trúc được đề xuất bởi Lin et al. (2018). Cụ thể, nó sử dụng 8 lớp BERT dưới cùng làm các lớp chéo nhiệm vụ chung và sử dụng t + 1 bản sao của 4 lớp BERT trên cùng, một bản sao cho mỗi nhiệm vụ, cũng như một bản sao chéo nhiệm vụ chung. Văn bản đầu vào, như được mã hóa bởi 8 lớp chung, e^S_{1:8}, được truyền qua các mô-đun 4 lớp chung và không chung, e^S_{8:12} và e^{Ui}_{8:12}, tương ứng. Các bộ phân loại nhiệm vụ sau đó được cung cấp với đầu ra của các lớp chéo nhiệm vụ kết hợp với đầu ra của các lớp cụ thể cho nhiệm vụ của chúng, theo cơ chế cổng của Rotman và Reichart (2019):

aᵢ(x) = σ(W^i_g[e^S_{8:12}(x); e^{Ui}_{8:12}(x)] + b^i_g);
gᵢ(x) = aᵢ(x) ⊙ e^S_{8:12}(x) + (1 - aᵢ(x)) ⊙ e^{Ui}_{8:12}(x);

trong đó [;] là toán tử nối, ⊙ là tích theo từng phần tử, σ là hàm Sigmoid, và W^i_g và b^i_g là các tham số cơ chế cổng. Vector kết hợp gᵢ(x) sau đó được đưa vào bộ giải mã cụ thể cho nhiệm vụ thứ i.

Tất cả các triển khai đều dựa trên gói Transformers của HuggingFace (Wolf et al., 2020). Đối với tất cả các mô hình, bộ giải mã DP dựa trên trình phân tích Biaffine (Dozat và Manning, 2017) và bộ giải mã NER là một bộ phân loại tuyến tính đơn giản.

### 5.4 Huấn luyện và điều chỉnh siêu tham số

Chúng tôi xem xét các siêu tham số sau đây cho các thí nghiệm AL. Đầu tiên, chúng tôi lấy mẫu ngẫu nhiên 2% của tập huấn luyện gốc để phục vụ như các ví dụ được gắn nhãn ban đầu trong tất cả các thí nghiệm và coi phần còn lại của các ví dụ huấn luyện là chưa được gắn nhãn. Chúng tôi cũng cố định tập phát triển của chúng tôi có kích thước gấp đôi tập huấn luyện ban đầu, bằng cách lấy mẫu ngẫu nhiên các ví dụ từ tập phát triển gốc. Sau đó, chúng tôi chạy mỗi phương pháp AL trong 5 lần lặp, nơi ở mỗi lần lặp, thuật toán chọn một tập chưa được gắn nhãn có kích thước bằng tập huấn luyện ban đầu (tức là 2% của tập huấn luyện gốc) để chú thích. Sau đó, chúng tôi tiết lộ các nhãn của các ví dụ đã chọn và thêm chúng vào tập huấn luyện của lần lặp tiếp theo. Vào đầu lần lặp cuối cùng, tập huấn luyện được gắn nhãn của chúng tôi bao gồm 10% dữ liệu huấn luyện gốc.

Trong mỗi lần lặp, chúng tôi huấn luyện các mô hình với 20K bước gradient với tiêu chí dừng sớm theo tập phát triển. Chúng tôi báo cáo điểm LAS cho DP và điểm F1 cho NER. Đối với DP, chúng tôi đo các điểm tin cậy AL của chúng tôi trên các cạnh chưa được gắn nhãn. Khi thực hiện học đa nhiệm, chúng tôi đặt tiêu chí dừng là trung bình hình học của các điểm nhiệm vụ (F1 cho NER và LAS cho DP). Chúng tôi tối ưu hóa tất cả các tham số bằng cách sử dụng trình tối ưu hóa ADAM (Kingma và Ba, 2015) với độ suy giảm trọng số 0,01, tốc độ học 5e-5, và kích thước batch là 32. Đối với làm mịn nhãn (xem bên dưới), chúng tôi sử dụng α = 0,2. Theo Dror et al. (2018), chúng tôi sử dụng t-test để đo ý nghĩa thống kê (p-value = 0,05).

### 5.5 Kết quả

**Kiến trúc mô hình (Q1.1)** Chúng tôi đầu tiên muốn điều tra hiệu suất của các mô hình đơn nhiệm vụ và đa nhiệm trong thiết lập huấn luyện đầy đủ (FT) và, quan trọng hơn, trong thiết lập học tích cực (AL). Do đó, chúng tôi so sánh ba kiến trúc: Mô hình đơn nhiệm vụ (ST), mô hình đa nhiệm đơn giản (SMT), và mô hình đa nhiệm phức tạp (CMT) của chúng tôi. Chúng tôi huấn luyện mỗi mô hình cho DP và cho NER trên sáu miền OntoNotes bằng cách sử dụng hàm mục tiêu cross-entropy (CE), hoặc với mục tiêu làm mịn nhãn (LS (Szegedy et al., 2016)) đã được chứng minh làm giảm lỗi hiệu chỉnh của các mô hình Transformer (Desai và Durrett, 2020; Kong et al., 2020). ST-AL được thực hiện với ST-EC và MT-AL với MT-AVG.

Bảng 3 báo cáo điểm trung bình (cột Avg) trên tất cả các miền và số lượng miền mà mỗi mô hình đạt được kết quả tốt nhất (Best). Kết quả đưa ra ba quan sát quan trọng. Đầu tiên, SMT tệ hơn trung bình so với ST trong tất cả các thiết lập, cho thấy rằng MT vanilla không phải lúc nào cũng tốt hơn huấn luyện ST. Thứ hai, mô hình CMT của chúng tôi đạt được điểm tốt nhất trong hầu hết các trường hợp. Trường hợp duy nhất nó kém hơn ST, nhưng không kém hơn SMT, là trên AL với huấn luyện CE. Tuy nhiên, khi huấn luyện với LS, nó đạt được kết quả tương đương hoặc cao hơn so với ST trên AL.

Thứ ba, khi so sánh huấn luyện CE với LS, LS rõ ràng cải thiện điểm trung bình của tất cả các mô hình (ngoại trừ một trường hợp). Thú vị là, sự cải thiện có ý nghĩa hơn trong thiết lập AL so với thiết lập FT. Chúng tôi báo cáo rằng khi mở rộng các thí nghiệm này cho tất cả các phương pháp lựa chọn AL, LS được tìm thấy rất hiệu quả cho cả hai nhiệm vụ, vượt trội hơn CE trong hầu hết các so sánh, với mức cải thiện trung bình 1,8% LAS cho DP và 0,9 F1 cho NER.

**Hiệu suất đa nhiệm so với đơn nhiệm vụ (Q1.2)** Chúng tôi tiếp theo hỏi liệu MT-AL có vượt trội hơn các cơ sở ST-AL mạnh hay không. Hình 3 trình bày cho mỗi nhiệm vụ và miền hiệu suất của các phương pháp ST-AL và MT-AL tốt nhất trên mỗi miền sau lần lặp AL cuối cùng. Theo quan sát của chúng tôi trong Q1.1, chúng tôi huấn luyện tất cả các mô hình với mục tiêu LS và dựa các mô hình đa nhiệm trên mô hình CMT hiệu quả.

Mặc dù không có phương pháp đơn lẻ nào, MT-AL hay ST-AL, hoạt động tốt nhất trên tất cả các miền và nhiệm vụ, MT-AL dường như hoạt động tốt hơn một cách nhất quán. Hình cho thấy rằng MT-AL hiệu quả cho cả hai nhiệm vụ, vượt trội hơn các phương pháp ST-AL tốt nhất trong 4 trong 6 miền DP (kết quả không có ý nghĩa thống kê, p-value trung bình là 0,19) và trong 5 trong 6 miền NER (kết quả cho 3 miền có ý nghĩa thống kê). Trong khi khoảng cách trung bình giữa MT-AL và ST-AL là nhỏ đối với DP (0,28% LAS), trong NER nó cao tới 2,4 điểm F1 có lợi cho MT-AL. Thật vậy, đối với một nửa số miền NER, khoảng cách này lớn hơn 4,2 F1.

Khi so sánh các phương pháp lựa chọn cá nhân, MT-AVG và entropy dựa trên DP đa nhiệm, MT-EC-DP, là các phương pháp lựa chọn tốt nhất cho DP, với điểm trung bình 89,03% và 88,99%, tương ứng. Entropy dựa trên DP đơn nhiệm vụ, ST-EC-DP đứng thứ ba, với điểm trung bình 88,96%, trong khi phương pháp ST-AL tốt thứ hai, ST-DA-DP, chỉ được xếp hạng thứ chín trong số tất cả các phương pháp, bị vượt trội bởi bảy phương pháp MT-AL khác nhau.

Đối với NER, entropy dựa trên NER đa nhiệm, MT-EC-NER, là mô hình tốt nhất với điểm F1 trung bình 77,13, tiếp theo là MT-MAX với điểm F1 trung bình 75,90. Các phương pháp dựa trên NER đơn nhiệm vụ, ST-EC-NER và ST-DA-NER chỉ được xếp hạng thứ năm và thứ sáu, cả hai với điểm trung bình 75,60. Những kết quả này cung cấp một dấu hiệu bổ sung về sự vượt trội của MT-AL.

Về các phương pháp lựa chọn MT-AL không thực hiện tổng hợp đơn giản, MT-PAR và MT-RRF hoạt động tương tự, trung bình 88,31% LAS cho DP và 75,88 F1 cho NER, trong khi MT-IND đạt được kết quả kém cho DP và kết quả trung bình cho NER (so sánh tổng thể các phương pháp MT-AL được cung cấp trong §8).

Chúng tôi tiếp theo so sánh hiệu suất theo từng lần lặp của ST-AL và MT-AL. Với mục đích này, Hình 4 trình bày hiệu suất cho các phương pháp MT-AL và ST-AL nổi bật nhất: MT-EC-DP và ST-EC-DP cho DP và MT-EC-NER và ST-EC-NER cho NER, cùng với phương pháp lựa chọn ngẫu nhiên đa nhiệm MT-R. Chúng tôi vẽ đồ thị cho mỗi phương pháp điểm nhiệm vụ của nó trên miền NW (miền có bộ dữ liệu lớn nhất) như một hàm của kích thước tập huấn luyện, tương ứng với 2% đến 10% của các ví dụ huấn luyện gốc. Rõ ràng, các phương pháp MT-AL vượt trội trong tất cả các lần lặp AL, cho thấy tính ổn định của MT-AL cũng như hiệu quả của nó trong các thiết lập tài nguyên thấp. Các mô hình tương tự cũng được quan sát thấy trong các miền khác.

Như một đánh giá cuối cùng cho Q1.2, chúng tôi trực tiếp so sánh các cặp phương pháp MT-AL và ST-AL, thực hiện ba loại so sánh trên mỗi miền: Trong nhiệm vụ: So sánh hiệu suất của ST-EC-i và ST-DA-i với các đối tác MT-AL của chúng (MT-EC-i và MT-DA-i) và với các phương pháp lựa chọn chung trên nhiệm vụ i, hoặc DP hoặc NER (108 so sánh); Chéo nhiệm vụ: So sánh hiệu suất của ST-EC-i và ST-DA-i với các đối tác MT-AL của chúng và với các phương pháp lựa chọn chung trên nhiệm vụ đối diện (ví dụ, nếu các mô hình chọn theo DP, chúng tôi đánh giá nhiệm vụ NER; 108 so sánh). So sánh này cho phép chúng tôi đánh giá ảnh hưởng của mô hình đơn và đa nhiệm lên hiệu suất chéo nhiệm vụ. Vì các mô hình đơn nhiệm vụ không thể được áp dụng trực tiếp cho nhiệm vụ đối diện, chúng tôi ghi lại các ví dụ được chọn bởi phương pháp ST-AL và huấn luyện một mô hình cho nhiệm vụ đối diện trên các ví dụ này; và Trung bình: So sánh tất cả các phương pháp ST-AL với tất cả các phương pháp MT-AL theo hiệu suất trung bình của chúng trên cả hai nhiệm vụ (264 so sánh).

Bảng 4 báo cáo tỷ lệ phần trăm so sánh trong đó các phương pháp MT-AL vượt trội. Trung bình, hai loại phương pháp ngang nhau khi so sánh hiệu suất Trong nhiệm vụ. Thú vị hơn, đối với hiệu suất Chéo nhiệm vụ, các phương pháp MT-AL rõ ràng vượt trội với khoảng 90% chiến thắng (87% trường hợp có ý nghĩa thống kê). Cuối cùng, Trung bình cũng hỗ trợ sự vượt trội của các phương pháp MT-AL hoạt động tốt hơn trong 79% trường hợp (tất cả kết quả đều có ý nghĩa thống kê). Những kết quả này chứng minh sự vượt trội của MT-AL, đặc biệt (và có lẽ không ngạc nhiên) khi cả hai nhiệm vụ được xem xét.

**Lựa chọn đơn nhiệm vụ so với nhiệm vụ chung (Q1.3)** Tiếp theo, chúng tôi chuyển sang câu hỏi thứ ba của chúng tôi, so sánh điểm tin cậy đơn nhiệm vụ với nhiệm vụ chung. Đó là, chúng tôi hỏi liệu các phương pháp MT-AL dựa tiêu chí lựa chọn của chúng trên nhiều hơn một nhiệm vụ có tốt hơn các phương pháp ST-AL và MT-AL chỉ tính điểm tin cậy bằng cách sử dụng một nhiệm vụ duy nhất hay không.

Để trả lời câu hỏi này, chúng tôi so sánh hai phương pháp ST-AL và MT-AL tốt nhất dựa trên lựa chọn đơn nhiệm vụ với hai phương pháp lựa chọn nhiệm vụ chung MT-AL tốt nhất. Như trước đây, tất cả các phương pháp sử dụng mục tiêu LS. Bảng 5 báo cáo điểm trung bình (trên các miền) của mỗi phương pháp này cho DP, NER, và điểm nhiệm vụ trung bình, dựa trên lần lặp AL cuối cùng.

Trong khi phương pháp hoạt động tốt nhất trung bình trên cả hai nhiệm vụ là MT-MAX, một phương pháp lựa chọn chung, phương pháp tốt thứ hai là MT-EC-NER, một phương pháp lựa chọn đơn nhiệm vụ, và khoảng cách chỉ là 0,26 điểm. Không ngạc nhiên, hiệu suất cao hơn khi nhiệm vụ được đánh giá cũng phục vụ như nhiệm vụ mà điểm tin cậy dựa trên, hoặc chỉ riêng hoặc cùng với một nhiệm vụ khác.

Mặc dù các phương pháp lựa chọn chung hiệu quả cho cả hai nhiệm vụ, chúng tôi không thể kết luận một cách quyết định rằng chúng tốt hơn các phương pháp MT-AL thực hiện lựa chọn đơn nhiệm vụ. Tuy nhiên, chúng tôi thấy một xác nhận khác cho câu trả lời của chúng tôi cho Q1.2, vì tất cả các phương pháp MT-AL được trình bày hoạt động tốt hơn trung bình trên cả hai nhiệm vụ (cột Trung bình) so với các phương pháp ST-AL.

**Phân tích quá tin tưởng** Ban đầu, chúng tôi huấn luyện các mô hình của chúng tôi với mất mát CE tiêu chuẩn. Tuy nhiên, các thí nghiệm sớm của chúng tôi gợi ý rằng huấn luyện dựa trên CE như vậy tạo ra các mô hình quá tin tưởng, điều này có khả năng gây hại nghiêm trọng cho các phương pháp AL dựa trên độ tin cậy. Trong khi công trình trước đây chứng minh tác động tích cực của làm mịn nhãn (LS) đối với hiệu chỉnh mô hình, theo hiểu biết tốt nhất của chúng tôi, tác động kết quả đối với học đa nhiệm chưa được khám phá, đặc biệt không phải trong bối cảnh AL. Chúng tôi tiếp theo phân tích tác động này, điều này có thể nhận thấy trong các kết quả trên của chúng tôi, chi tiết hơn.

Hình 5 trình bày điểm tin cậy mức câu như một hàm của độ chính xác mức câu khi huấn luyện riêng biệt một mô hình BERT-base đơn nhiệm vụ trên DP (hình trái) và trên NER (hình phải) với mục tiêu CE. Điểm tin cậy được tính theo điểm ST-EC. Hình xác nhận rằng mô hình có xu hướng quá tin tưởng trong các dự đoán của nó. Hơn nữa, các giá trị R² thấp (0,1 và 0,13 cho DP và NER, tương ứng) cho thấy hiệu chỉnh mô hình kém, vì điểm tin cậy không tương quan với độ chính xác thực tế. Các mô hình tương tự được quan sát thấy khi huấn luyện các mô hình đa nhiệm của chúng tôi với mục tiêu CE.

Theo phân tích này, chúng tôi chuyển sang điều tra tác động của LS đối với dự đoán mô hình trong MT-AL. Được lấy cảm hứng từ Thulasidasan et al. (2019), người đã định nghĩa lỗi quá tin tưởng (OE) cho các nhiệm vụ phân loại, chúng tôi đầu tiên tổng quát hóa OE một cách nhẹ nhàng để hỗ trợ điểm mức câu cho các nhiệm vụ phân loại token. Với N câu, cho mỗi câu x, chúng tôi bắt đầu bằng cách tính điểm độ chính xác acc(x) của nó trên các token của nó. Điểm tin cậy conf(x) được đặt thành điểm tin cậy của phương pháp AL tương ứng. Sau đó, chúng tôi định nghĩa OE là:

OE = (1/N) ∑ᵢ₌₁ᴺ max{conf(x) - acc(x), 0}

Về bản chất, OE phạt các dự đoán theo khoảng cách giữa điểm tin cậy và độ chính xác của chúng, nhưng chỉ khi điểm tin cậy cao hơn.

Trong Bảng 6, chúng tôi so sánh điểm OE của ST-EC được huấn luyện với mục tiêu LS với 3 phương án thay thế: ST-EC được huấn luyện với mục tiêu CE, ST-EC với phương pháp hậu xử lý điều chỉnh nhiệt độ (TS), và ST-DA được huấn luyện với mục tiêu CE. Cả TS và suy luận dropout đều được chứng minh cải thiện ước tính tin cậy (Guo et al., 2017; Ovadia et al., 2019), và do đó phục vụ như các phương án thay thế cho LS trong so sánh này. Điểm OE được báo cáo trên tập chưa được gắn nhãn (với các nhãn thực được cung cấp sau) ở lần lặp AL cuối cùng cho cả hai nhiệm vụ. Ngoài ra, điểm OE cho MT-AVG và MT-AVGDA cũng được báo cáo và lấy trung bình trên cả hai nhiệm vụ.

Kết quả là quyết định, LS là phương pháp ít quá tin tưởng nhất, đạt được điểm OE thấp nhất trên tất cả 18 thiết lập, trừ một. Trong khi LS đạt được độ giảm lỗi tỷ lệ (PRE) từ 57,2% đến 96,7% so với phương pháp CE tiêu chuẩn, DA đạt được nhiều nhất là PRE 51,9% và TS dường như không có tác động gì. Những kết quả này xác nhận rằng LS rất hiệu quả trong việc giảm điểm quá tin tưởng cho các mô hình dựa trên BERT, và chúng tôi có thể chỉ ra lần đầu tiên rằng sự giảm như vậy cũng đúng cho các mô hình đa nhiệm.

## 6 MT-AL cho các nhiệm vụ liên quan theo thứ bậc

Cho đến nay, chúng tôi đã xem xét các nhiệm vụ (DP và NER) có thông tin lẫn nhau nhưng có thể được huấn luyện độc lập với nhau. Tuy nhiên, các kịch bản học đa nhiệm khác liên quan đến một nhiệm vụ phụ thuộc vào đầu ra của một nhiệm vụ khác. Một ví dụ nổi bật là nhiệm vụ trích xuất mối quan hệ (RE) phụ thuộc vào đầu ra của nhiệm vụ NER, vì mục tiêu của RE là phân loại và xác định mối quan hệ giữa các thực thể có tên. Quan trọng là, nếu phần NER của mô hình không hoạt động tốt, điều này cũng gây hại cho hiệu suất RE. Do đó, lựa chọn mẫu trong thiết lập như vậy nên phản ánh mối quan hệ thứ bậc giữa các nhiệm vụ.

### 6.1 Phương pháp lựa chọn

Vì chất lượng của bộ phân loại cho nhiệm vụ độc lập (NER) bây giờ cũng ảnh hưởng đến chất lượng của bộ phân loại cho nhiệm vụ phụ thuộc (RE), độ tin cậy của mỗi nhiệm vụ có thể nhận được các giá trị tầm quan trọng tương đối khác nhau. Mặc dù về nguyên tắc điều này cũng có thể đúng cho các nhiệm vụ độc lập (§5), việc tính toán rõ ràng cho đặc tính này dường như quan trọng hơn trong thiết lập hiện tại.

Do đó, chúng tôi sửa đổi bốn phương pháp lựa chọn chung của chúng tôi (§4) để phản ánh sự bất đối xứng cố có giữa các nhiệm vụ, bằng cách trình bày một tham số tỷ lệ α ∈ [0,1]:

a) MT-AVG bây giờ được tính như sau:
MT-AVG(x) = α · MT-EC-RE(x) + (1-α) · MT-EC-NER(x)

b) MT-RRF được tính tương tự bằng cách nhân số hạng RRF của RE với α và của NER với 1-α.

c) MT-IND được tính bằng cách độc lập chọn ⌊100α%⌋ các mẫu đã chọn theo điểm RE và ⌊100(1-α)%⌋ theo điểm NER.

d) MT-PAR được tính bằng cách hạn chế điều kiện Pareto đầu tiên cho vị trí của điểm tin cậy RE: c_RE ≤ q · c'_RE, trong đó q là giá trị của α-quantile của điểm tin cậy RE. Chúng tôi áp dụng điều kiện như vậy nếu α < 0,5. Nếu không, nếu α > 0,5, điều kiện được áp dụng cho thành phần NER, và khi nó bằng 0,5, phương pháp Pareto gốc được sử dụng. Vì chúng tôi hạn chế điều kiện chỉ cho một trong các nhiệm vụ, ít mẫu hơn sẽ đáp ứng điều kiện này (vì 0 ≤ q ≤ 1), và biên giới Pareto sẽ bao gồm nhiều mẫu hơn đã đáp ứng điều kiện cho nhiệm vụ thứ hai.

### 6.2 Câu hỏi nghiên cứu

Trong các thí nghiệm của chúng tôi, chúng tôi muốn khám phá hai câu hỏi nghiên cứu: Q2.1: Phương pháp lựa chọn MT-AL nào phù hợp nhất cho thiết lập này? và Q2.2: Sự cân bằng tốt nhất giữa các nhiệm vụ tham gia là gì?

Vì RE hoàn toàn dựa vào đầu ra của NER, chúng tôi giới hạn các thí nghiệm của chúng tôi chỉ cho các mô hình đa nhiệm chung và không bao gồm các mô hình đơn nhiệm vụ.

### 6.3 Thiết lập thí nghiệm

Chúng tôi thí nghiệm với mô hình BERT NER và RE chung dựa trên span của Li et al. (2021). Các thí nghiệm được thực hiện trên năm bộ dữ liệu đa dạng: NYT24 và NYT29 (Nayak và Ng, 2020), ScieRC (Luan et al., 2018), WebNLG (Gardent et al., 2017), và WLP (Kulkarni et al., 2018). Thiết lập AL giống hệt với §5.4. Các siêu tham số khác không được đề cập trước đây giống hệt với những gì trong việc triển khai gốc.

### 6.4 Kết quả

**Phương pháp lựa chọn tốt nhất (Q2.1)** Chúng tôi bắt đầu bằng cách xác định phương pháp lựa chọn tốt nhất cho thiết lập này. Bảng 7 tóm tắt điểm trung bình cho mỗi nhiệm vụ cho giá trị α tốt nhất của mỗi phương pháp trên năm bộ dữ liệu.

Chúng tôi quan sát thấy ba mô hình thú vị. Đầu tiên, MT-AL rất hiệu quả trong thiết lập này cho nhiệm vụ phụ thuộc (RE), trong khi đối với nhiệm vụ độc lập (NER), lựa chọn ngẫu nhiên không tụt hậu quá xa. Thứ hai, tất cả các phương pháp MT-AL đạt được hiệu suất tốt hơn cho các giá trị α cao hơn bằng cách cho trọng số nhiều hơn cho điểm tin cậy RE trong quá trình lựa chọn. Đây là một dấu hiệu cho thấy thật sự phương pháp lựa chọn nên phản ánh bản chất bất đối xứng của các nhiệm vụ. Thứ ba, nhìn chung, MT-IND là phương pháp hoạt động tốt nhất, trung bình đứng đầu trong NER và trong RE, trong khi MT-AVG, MT-RRF và MT-PAR đạt được kết quả tương tự trong cả hai nhiệm vụ.

**Cấu hình tỷ lệ (Q2.2)** Hình 6 trình bày điểm F1 trung bình của bốn phương pháp lựa chọn chung, cũng như phương pháp lựa chọn ngẫu nhiên MT-R, như một hàm của α (trọng số tương đối của độ tin cậy RE). Đầu tiên, chúng tôi nhận thấy rằng lựa chọn chung vượt trội hơn lựa chọn ngẫu nhiên trong miền WebNLG chỉ cho RE (nhiệm vụ phụ thuộc) nhưng không cho NER (trừ khi α tiến tới 1). Thứ hai, và quan trọng hơn, α = 1, tức là chọn các ví dụ chỉ theo điểm tin cậy của RE (nhiệm vụ phụ thuộc), có lợi nhất cho cả hai nhiệm vụ (Q2.1). Chúng tôi giả thuyết rằng điều này xuất phát từ thực tế rằng điểm tin cậy RE truyền tải thông tin về cả hai nhiệm vụ và thông tin kết hợp này cung cấp một tín hiệu mạnh hơn đối với nhiệm vụ NER (độc lập), so với điểm tin cậy NER. Thú vị là, tác động tích cực của các giá trị α cao hơn nổi bật hơn cho NER, mặc dù điều này có nghĩa là vai trò của điểm tin cậy NER bị hạ thấp trong quá trình lựa chọn mẫu.

Đối với các phương pháp lựa chọn cá nhân, chúng tôi báo cáo rằng MT-AVG và MT-IND đạt được kết quả cao hơn khi α tăng, trong khi MT-PAR và MT-RRF đạt đỉnh ở α = 0,7 và α = 0,8, tương ứng, và sau đó giảm 0,2 điểm F1 cho NER và 0,9 điểm F1 cho RE trung bình.

## 7 MT-AL cho các nhiệm vụ với độ chi tiết chú thích khác nhau

Các nhiệm vụ NLP được định nghĩa trên các đơn vị văn bản khác nhau, với các ví dụ phổ biến nhất là nhiệm vụ mức câu và mức token. Điều tra cuối cùng của chúng tôi xem xét kịch bản của hai nhiệm vụ có liên quan chặt chẽ có độ chi tiết khác nhau: Điền khe (SF, mức token) và phát hiện ý định (ID, mức câu).

Do bản chất chú thích khác nhau của hai nhiệm vụ, chúng tôi phải định nghĩa chi phí chú thích ví dụ đối với mỗi nhiệm vụ. Tự nhiên, không có cách đúng để định lượng các chi phí này, nhưng chúng tôi nhằm mục đích đề xuất một mô hình thực tế. Chúng tôi ký hiệu chi phí chú thích một mẫu cho SF bằng Cost_SF = m + t_p · n_t, trong đó m là số lượng token trong câu, t_p là chi phí chú thích token cố định (chúng tôi đặt t_p = 1) và n_t là số lượng thực thể. Chi phí chú thích một mẫu cho ID tiếp theo được ký hiệu bằng Cost_ID = m + t_s, trong đó t_s là chi phí câu cố định (chúng tôi đặt t_s = 3). Giải pháp của chúng tôi (xem bên dưới) cho phép một số ví dụ được chú thích chỉ đối với một trong các nhiệm vụ. Đối với các ví dụ được chú thích đối với cả hai nhiệm vụ, chúng tôi xem xét một chi phí chung cộng dồn trong đó số hạng mức token m chỉ được xem xét một lần: JCost = Cost_SF + Cost_ID - m. Trong các thí nghiệm của chúng tôi, chúng tôi cho phép ngân sách chú thích cố định B = 500 cho mỗi lần lặp AL.

### 7.1 Phương pháp

Chúng tôi xem xét ba loại phương pháp quyết định:

**Học tích cực tham lam (GRD_AL)**: Các phương pháp AL ở mỗi lần lặp tham lam chọn các mẫu ít tin cậy nhất cho đến khi đạt được giới hạn ngân sách;

**Học tích cực lập trình tuyến tính nhị phân (BLP_AL)**: Các phương pháp AL ở mỗi lần lặp chọn tối thiểu hóa tổng điểm tin cậy của các mẫu đã chọn với các ràng buộc ngân sách. Bài toán tối ưu hóa (xem bên dưới) được giải quyết bằng cách sử dụng một bộ giải BLP;

**Lập trình tuyến tính nhị phân (BLP)**: một thuật toán sau khi huấn luyện trên tập huấn luyện ban đầu chọn tất cả các mẫu cùng một lúc, bằng cách giải quyết cùng một bài toán tối ưu hóa có ràng buộc như trong BLP_AL.

Đối với mỗi danh mục này, chúng tôi thí nghiệm với bốn họ phương pháp AL:

a) **Tập rời rạc không hạn chế (UDJS)**: Phương pháp lựa chọn này dựa trên điểm tin cậy đa nhiệm không tổng hợp, nơi mỗi mẫu có thể được chọn để chú thích trên một trong hai nhiệm vụ hoặc cả hai. Bài toán tối ưu hóa UDJS nhằm mục đích tối đa hóa điểm không chắc chắn (1 - Conf_t(x)) của các mẫu đã chọn với các ràng buộc ngân sách và lựa chọn:

max ∑_{x∈U} ∑_{t∈T} (1 - Conf_t(x)) X_t(x)

s.t. ∑_{x∈U} ∑_{t∈T} Cost_t(x)(X_t(x) - Y(x)) + JCost(x)Y(x) ≤ B;

(1/|T|) ∑_{t∈T} X_t(x) ≤ Y(x) ∀x ∈ U;

X_t(x), Y(x) ∈ {0,1} ∀x ∈ U, t ∈ T;

trong đó U là tập chưa được gắn nhãn, T là tập nhiệm vụ, Conf_t là điểm tin cậy MT-EC-t, X_t(x) là một chỉ báo nhị phân cho biết việc chú thích mẫu x trên nhiệm vụ t, và Y(x) là một chỉ báo nhị phân cho biết việc chú thích x trên tất cả các nhiệm vụ.

Lưu ý rằng công thức này có thể tạo ra các ví dụ được chú thích chỉ cho một trong các nhiệm vụ, mặc dù điều này không có khả năng, đặc biệt là dưới một giao thức lặp khi điểm tin cậy của các mô hình được cập nhật sau mỗi lần lặp.

b) **Tập rời rạc ngân sách bằng nhau (EQB-DJS)**: Chiến lược này tương tự như UDJS ở trên ngoại trừ việc ngân sách được chia đều giữa hai nhiệm vụ và bài toán tối ưu hóa được giải quyết cho mỗi nhiệm vụ riêng biệt. Nếu một mẫu được chọn để chú thích cho cả hai nhiệm vụ, chúng tôi cập nhật chi phí của nó theo chi phí chung và giải lại các bài toán tối ưu hóa cho đến khi toàn bộ ngân sách được sử dụng.

c-f) **Lựa chọn nhiệm vụ chung**: Một mẫu chỉ có thể được chọn để chú thích trên cả hai nhiệm vụ, nơi điểm tin cậy được tính bằng cách sử dụng tổng hợp đa nhiệm. Bài toán tối ưu hóa BLP được công thức hóa như sau:

max ∑_{x∈U} (1 - Conf(x))Y(x)

s.t. ∑_{x∈U} JCost(x)Y(x) ≤ B;

Y(x) ∈ {0,1} ∀x ∈ U;

trong đó Conf được tính bằng c) MT-AVG, d) MT-MAX, e) MT-MIN hoặc f) MT-RRF.

g-j) **Lựa chọn tin cậy đơn nhiệm vụ (STCS)**: Một mẫu chỉ có thể được chọn để chú thích trên cả hai nhiệm vụ, nơi quá trình lựa chọn nhằm mục đích tối đa hóa điểm không chắc chắn chỉ của một trong các nhiệm vụ: g) STCS-SF hoặc j) STCS-ID. Tương tự như Lựa chọn nhiệm vụ chung, các ràng buộc ngân sách được áp dụng cho chi phí chung.

### 7.2 Câu hỏi nghiên cứu

Chúng tôi tập trung vào ba câu hỏi nghiên cứu: Q3.1: Tối ưu hóa BLP có cải thiện so với lựa chọn tham lam không? Q3.2: Lựa chọn tối ưu hóa và học tích cực có tác động bổ sung không? và Q3.3: Tốt hơn là chú thích tất cả các mẫu trên cả hai nhiệm vụ hay xây dựng một tập huấn luyện được chú thích rời rạc?

### 7.3 Thiết lập thí nghiệm

Chúng tôi thực hiện thí nghiệm trên hai bộ dữ liệu nổi bật: ATIS (Price, 1990) và SNIPS (Coucke et al., 2018), và xem xét hai bộ mã hóa dựa trên Transformer: BERT-base (Devlin et al., 2019) và Roberta-base (Liu et al., 2019b). Mã của chúng tôi chủ yếu dựa trên việc triển khai của Zhu và Yu (2017). Chúng tôi chạy quá trình AL trong 5 lần lặp với tập huấn luyện ban đầu gồm 50 mẫu ngẫu nhiên và tập phát triển có kích thước cố định gồm 100 mẫu ngẫu nhiên. Chúng tôi huấn luyện tất cả các mô hình trong 30 epoch cho mỗi lần lặp, với tiêu chí dừng sớm và với làm mịn nhãn (α = 0,1). Các siêu tham số khác được đặt theo giá trị mặc định.

### 7.4 Kết quả

**Lựa chọn dựa trên tối ưu hóa và AL (Q3.1 và Q3.2)** Để trả lời hai câu hỏi đầu tiên, chúng tôi hiển thị trong Bảng 8 hiệu suất F1 mẫu cuối cùng cho cả hai nhiệm vụ, khi lựa chọn được thực hiện với phương pháp lựa chọn tốt nhất của mỗi danh mục quyết định: MT-RRF^{GRD_AL}, MT-MIN^{BLP_AL} và MT-AVG^{BLP}.

Kết quả xác nhận rằng tối ưu hóa BLP (với AL) thật sự vượt trội hơn lựa chọn AL tham lam, vượt trội trong tất cả các thiết lập (hai nhiệm vụ, hai bộ dữ liệu, và hai bộ mã hóa ngôn ngữ được đào tạo trước, 5 trong số các so sánh có ý nghĩa thống kê).

Câu trả lời cho Q3.2 cũng chủ yếu tích cực. Tối ưu hóa BLP và AL lặp có tác động bổ sung, vì MT-MIN^{BLP_AL} trong hầu hết các trường hợp đạt được hiệu suất cao hơn MT-AVG^{BLP} (50% kết quả có ý nghĩa thống kê).

Thật vậy, câu trả lời cho hai câu hỏi đúng cho tất cả các phương pháp lựa chọn, vì tất cả đều hoạt động tốt nhất khi sử dụng phương pháp quyết định BLP_AL mới của chúng tôi. Thứ hai là BLP, cho thấy rằng công thức BLP rất hiệu quả cho các thiết lập MT dưới các ràng buộc ngân sách khác nhau. Cuối cùng, cuối cùng là thủ tục tham lam tiêu chuẩn (GRD_AL) thường được sử dụng trong tài liệu AL.

**Lựa chọn nhiệm vụ chung (Q3.3)** Để trả lời Q3.3, chúng tôi thực hiện một so sánh tương tự trong Bảng 9, nhưng bây giờ cho các phương pháp nổi bật nhất cho mỗi phương pháp lựa chọn nhiệm vụ chung. Kết quả cho thấy rằng MT-MIN^{BLP_AL} buộc chú thích chung tốt hơn so với việc cho phép chú thích rời rạc không hạn chế (UDJS^{BLP_AL}) và hơn so với việc chia đều ngân sách giữa hai nhiệm vụ (EQB-DJS^{BLP_AL}), nơi 9 trong 16 so sánh có ý nghĩa thống kê. Chúng tôi giả thuyết rằng sự vượt trội của MT-MIN^{BLP_AL} xuất phát từ hai lý do chính: 1. Tích hợp điểm tin cậy tổng hợp chung vào hàm tối ưu hóa cung cấp tín hiệu tốt hơn so với điểm tin cậy đơn nhiệm vụ; 2. Có một bộ dữ liệu chung nơi tất cả các mẫu được chú thích trên cả hai nhiệm vụ thay vì một bộ dữ liệu được chú thích rời rạc (và lớn hơn) cho phép mô hình đạt được hiệu suất tốt hơn vì hai nhiệm vụ có liên quan chặt chẽ.

Cuối cùng, chúng tôi cũng báo cáo rằng các thí nghiệm ST-AL đã dẫn đến hiệu suất kém. Các phương pháp ST-AL tụt hậu 8,8 (SF) và 4,7 (ID) điểm F1 so với phương pháp MT-AL tốt nhất MT-MIN^{BLP_AL} trung bình. Thú vị là, chúng tôi cũng quan sát thấy rằng lựa chọn theo ID (STCS-ID^{BLP_AL}) đã dẫn đến kết quả trung bình tốt hơn trên cả hai nhiệm vụ so với lựa chọn theo SF (STCS-SF^{BLP_AL}), cho thấy rằng tương tự như §6, lựa chọn theo nhiệm vụ cấp cao hơn thường mang lại kết quả tốt hơn.

## 8 So sánh tổng thể

Như một đánh giá cuối cùng, chúng tôi chuyển sang so sánh hiệu suất của các phương pháp lựa chọn MT-AL chung được đề xuất của chúng tôi trong tất cả các thiết lập. Trong thiết lập đầu tiên (§5), chúng tôi thực hiện tất cả các phương pháp lựa chọn MT-AL với lựa chọn tham lam, xem xét tầm quan trọng nhiệm vụ đồng nhất. Sau đó (§6 và §7), chúng tôi chỉ ra cách các phương pháp lựa chọn này có thể được sửa đổi trong hai thiết lập tiếp theo bằng cách tích hợp trọng số nhiệm vụ không đồng nhất hoặc thay thế lựa chọn tham lam bằng hàm mất mát BLP trên điểm tin cậy. Tuy nhiên, không phải tất cả các phương pháp lựa chọn MT-AL của chúng tôi đều có thể được sửa đổi theo những cách như vậy. Do đó, chúng tôi báo cáo so sánh cuối cùng của chúng tôi với các điều kiện được áp dụng trong thiết lập đầu tiên: Giả định trọng số nhiệm vụ đồng nhất và chọn mẫu theo cách tham lam. Bảng 10 tóm tắt hiệu suất trung bình của các phương pháp MT-AL trong ba thiết lập được đề xuất của chúng tôi: Các nhiệm vụ bổ sung (DP + NER), các nhiệm vụ liên quan theo thứ bậc (NER + RE) và các nhiệm vụ với độ chi tiết chú thích khác nhau (SF + ID).

Mỗi phương pháp lựa chọn có ưu và nhược điểm riêng, mà chúng tôi phác thảo trong thảo luận cuối cùng:

MT-R, không ngạc nhiên, là phương pháp hoạt động tệ nhất trung bình, vì nó không sử dụng các dự đoán của mô hình. Tuy nhiên, phương pháp hoạt động khá tốt trong thiết lập thứ ba (SF + ID) khi so sánh với các phương pháp khác được huấn luyện với phương pháp quyết định tham lam, phương pháp quyết định kém thành công nhất của thiết lập này. Tiếp theo, MT-AVG hoạt động tốt khi các nhiệm vụ có tầm quan trọng bằng nhau (DP + NER), nhưng chỉ đạt được hiệu suất trung bình trong các thiết lập khác.

Đáng ngạc nhiên, MT-MAX rất hiệu quả mặc dù sự đơn giản của nó. Nó chủ yếu có lợi cho hai thiết lập đầu tiên (DP + NER và NER + RE), nơi các nhiệm vụ có cùng độ chi tiết chú thích. Nó là phương pháp tốt thứ ba tổng thể, và nó không tụt hậu đáng kể so với phương pháp tốt nhất, MT-PAR. Thú vị là, MT-MIN, cung cấp một góc nhìn bổ sung cho MT-MAX, trung bình là phương pháp MT-AL tệ nhất, loại trừ MT-R, và chủ yếu có lợi cho thiết lập đầu tiên (DP + NER).

Phương pháp MT-AL tiếp theo, MT-PAR, dường như nắm bắt tốt không gian tin cậy chung của các cặp nhiệm vụ. Nó là phương pháp tốt nhất trung bình, đạt được điểm trung bình cao trong tất cả các thiết lập. Tuy nhiên, khi kết hợp nó với các kỹ thuật huấn luyện khác, chẳng hạn như áp dụng trọng số không đồng nhất (cho thiết lập thứ hai), nó bị vượt trội bởi các phương pháp MT-AL khác. MT-RRF không tụt hậu xa so với MT-PAR, đạt được kết quả tương tự trong hầu hết các nhiệm vụ, loại trừ các nhiệm vụ RE và ID, là các nhiệm vụ cấp cao hơn của các thiết lập của chúng. Cuối cùng, MT-IND không xuất sắc trong ba trong bốn nhiệm vụ của hai thiết lập đầu tiên, trong khi đạt được kết quả trung bình tốt nhất trên NER, khi được huấn luyện cùng với RE. Hơn nữa, phương pháp thể hiện hiệu suất mạnh mẽ trong thiết lập thứ ba, khi các nhiệm vụ có độ chi tiết chú thích khác nhau, biện minh cho lựa chọn chú thích độc lập trong trường hợp này.

## 9 Kết luận

Chúng tôi đã xem xét vấn đề học tích cực đa nhiệm cho các mô hình dựa trên Transformer được đào tạo trước. Chúng tôi đặt ra nhiều câu hỏi nghiên cứu liên quan đến tác động của mô hình đa nhiệm, tiêu chí lựa chọn đa nhiệm, giảm quá tin tưởng, mối quan hệ giữa các nhiệm vụ tham gia, cũng như các ràng buộc ngân sách, và trình bày một cuộc điều tra thuật toán và thí nghiệm có hệ thống để trả lời các câu hỏi này. Kết quả của chúng tôi chứng minh tầm quan trọng của mô hình MT-AL trong ba kịch bản thực tế đầy thử thách, tương ứng với các mối quan hệ đa dạng giữa các nhiệm vụ tham gia. Trong công trình tương lai, chúng tôi dự định nghiên cứu các thiết lập với nhiều hơn hai nhiệm vụ và xem xét tạo ngôn ngữ và mô hình đa ngôn ngữ.

## 10 Lời cảm ơn

Chúng tôi muốn cảm ơn biên tập viên hành động và các nhà phản biện, cũng như các thành viên của nhóm NLP IE@Technion vì phản hồi và lời khuyên quý giá của họ. Nghiên cứu này được tài trợ một phần bởi một khoản tài trợ cá nhân ISF số 1625/18. RR cũng thừa nhận sự hỗ trợ của Ghế Thúc đẩy Nghề nghiệp Schmidt trong AI.
