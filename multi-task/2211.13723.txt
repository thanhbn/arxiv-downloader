# 2211.13723.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multi-task/2211.13723.pdf
# File size: 2342616 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Improving Multi-task Learning
via Seeking Task-based Flat Regions
Hoang Phan∗1Lam Tran∗2Quyen Tran2Ngoc N. Tran3Tuan Truong2
Qi Lei1Nhat Ho4Dinh Phung5Trung Le5
New York University1, VinAI Research2, Vanderbilt University3,
University of Texas, Austin4, Monash University, Australia5
May 26, 2025
Abstract
Multi-Task Learning (MTL) is a widely used and powerful learning paradigm for training deep
neural networks that allows learning more than one objective by a single backbone. Compared to
training tasks separately, MTL significantly reduces computational costs, improves data efficiency,
and potentially enhances model performance by leveraging knowledge across tasks. Hence, it
has been adopted in a variety of applications, ranging from computer vision to natural language
processing and speech recognition. Among them, there is an emerging line of work in MTL that
focuses on manipulating the task gradient to derive an ultimate gradient descent direction to
benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying
these approaches without using appropriate regularization techniques might lead to suboptimal
solutions to real-world problems. In particular, standard training that minimizes the empirical
loss on the training data can easily suffer from overfitting to low-resource tasks or be spoiled by
noisy-labeled ones, which can cause negative transfer between tasks and overall performance drop.
To alleviate such problems, we propose to leverage a recently introduced training method, named
Sharpness-aware Minimization, which can enhance model generalization ability on single-task
learning. Accordingly, we present a novel MTL training methodology, encouraging the model to
find task-based flat minima for coherently improving its generalization capability on all tasks.
Finally, we conduct comprehensive experiments on a variety of applications to demonstrate the
merit of our proposed approach to existing gradient-based MTL methods, as suggested by our
developed theory.
1 Introduction
Over the last few years, deep learning has emerged as a powerful tool for functional approximation
by exhibiting superior performance and even exceeding human ability on a wide range of applications.
In spite of the appealing performance, training massive independent neural networks to handle
individual tasks requires not only expensive computational and storage resources but also long
runtime. Therefore, multi-task learning is a more preferable approach in many situations [ 1,2,3]
as they can: (i) avoid redundant features calculation for each task through their inherently shared
architecture; and (ii) reduce the number of total trainable parameters by hard parameter sharing [ 4,5]
or soft parameter sharing [ 6,7]. However, existing state-of-the-art methods following the veins of
gradient-based multi-task learning [ 8,9,10,11,12,13] tend to neglect geometrical properties of the
loss landscape yet solely focus on minimizing the empirical error in the optimization process, which
can be easily prone to the overfitting problem [14, 15].
∗Equal contributions.
1arXiv:2211.13723v4  [cs.LG]  23 May 2025

--- PAGE 2 ---
Meanwhile, the overfitting problem of modern neural networks is often attributed to high-
dimensional and non-convex loss functions, which result in complex loss landscapes containing
multiple local optima. Hence, understanding the loss surface plays a crucial role in training robust
models, and developing flat minimizers remains one of the most effective approaches [ 16,14,17,18].
To be more specific, recent studies [ 19,20] show that the obtained loss landscape from directly
minimizing the empirical risk can consist of many sharp minimums, thus yielding poor generalization
capacity when being exposed to unseen data. Moreover, this issue is apparently exacerbated in
optimizing multiple objectives simultaneously, as in the context of multi-task learning. Certainly,
sharp minima of each constituent objective might appear at different locations, which potentially
results in large generalization errors on the associated task. To this end, finding a common flat
and low-loss valued region for all tasks is desirable for improving the current methods of multi-task
learning.
Contribution. To further address the above desideratum, we propose a novel MTL training
method, incorporatingtherecentlyintroducedoptimization sharpness-awareminimization(SAM)[ 21]
intoexistinggradientmanipulationstrategiesinmulti-tasklearningtofurtherboosttheirperformance.
Guiding by the generalization error in Theorem 1, the proposed approach not only orients the model
to the joint low empirical loss value across tasks but also encourages the model to reach the task-based
flat regions. Importantly, our approach is model-agnostic and compatible with current gradient-based
MTL approaches (see Figure 1 for the overview of our approach). By using our proposed framework,
the gradient conflict across tasks is mitigated significantly, which is the goal of recent gradient-based
MTL studies in alleviating negative transfer between tasks. Finally, we conduct comprehensive
experiments on a variety of applications to demonstrate the merit of our approach for improving not
only task performance but also model robustness and calibration. Last but not least, to the best of
our knowledge, ours is the first work to improve multi-task learning by investigating the geometrical
properties of the model loss landscape.
2 Related work
2.1 Multi-task learning
In multi-task learning (MTL), we often aim to jointly train one model to tackle multiple different
but correlated tasks. It has been proven in prior work [ 22,2,23,24] that it is not only able to
enhance the overall performance but also reduce the memory footprint and fasten the inference
process. Previous studies on MTL often employ a hard parameter-sharing mechanism along with
light-weight task-specific modules to handle multiple tasks.
Pareto multi-task learning. Originated from Multiple-gradient descent algorithm (MGDA),
a popular line of gradient-based MTL methods aim to find Pareto stationary solutions, from
which we can not further improve model performance on any particular task without diminishing
another [ 8]. Moreover, recent studies suggest exploring the whole Pareto front by learning diverse
solutions [ 25,26,27,28], or profiling the entire Pareto front with hyper-network [ 29,30]. While
these methods are theoretically grounded and guaranteed to converge to Pareto-stationary points,
the experimental results are often limited and lack comparisons under practical settings.
Loss and gradient balancing. Another branch of preliminary work in MTL capitalizes
on the idea of dynamically reweighting loss functions based on gradient magnitudes [ 31], task
homoscedastic uncertainty [ 32], or difficulty prioritization [ 33] to balance the gradients across tasks.
More recently, PCGrad [ 9] developed a gradient manipulation procedure to avoid conflicts among
2

--- PAGE 3 ---
InputTask 1 Task 2
Shared
encoder
Final gradientTask flat gradient
Combined flat gradient
Task loss gradient
 Combined loss gradient
Low loss regions
Figure 1: We demonstrate our framework in a 2-task problem. For the shared part, task-based flat
gradients (red dashed arrows) steer the model to escape sharp areas, while task-based loss gradients
(orange dashed arrows) lead the model into their corresponding low-loss regions. In our method, we
aggregate them to find the combined flat gradient gflat
shand combined loss gradient gloss
sh, respectively.
Finally, we add those two output gradients to target the joint low-loss and flat regions across the
tasks. Conversely, updating task-specific non-shared parts is straightforward and much easier since
there is the involvement of one objective only.
tasks by projecting random task gradients on the normal plane of the other. Similarly, [ 10] proposes a
provably convergent method to minimize the average loss, and [ 11] calculates loss scaling coefficients
such that the combined gradient has equal-length projections onto individual task gradients.
2.2 Flat minima
Flat minimizer has been found to improve generalization ability of neural networks because it enables
models to find wider local minima, by which they will be more robust against shifts between train
and test losses [ 34,35,36]. This relationship between generalization ability and the width of minima
is theoretically and empirically studied in many studies [ 37,38,39,40], and subsequently, a variety
of methods seeking flat minima have been proposed [ 41,42,43,44]. For example, [ 43,45,46] analyze
the impacts of different training factors, such as batch-size, learning rate, covariance of gradient,
dropout, on the flatness of found minima. Additionally, several schemes pursue wide local minima by
adding regularization terms to the loss function [ 41,47,48,42], e.g., softmax output’s low entropy
penalty, [41], distillation losses [47, 48].
Recently, SAM [ 21], which seeks flat regions by explicitly minimizing the worst-case loss around
the current model, has received significant attention due to its effectiveness and scalability compared
to previous methods. Particularly, it has been exploited in a variety of tasks and domains [ 49,50,51,
52,53,54,55]. A notable example is the improvement that SAM brings to meta-learning bi-level
optimization in [ 50]. Another application of SAM is in federated learning (FL) [ 51] in which the
authors achieved tighter convergence rates than existing FL works, and proposed a generalization
bound for the global model. In addition, SAM shows its generalization ability in vision models [ 54],
language models [ 53] and domain generalization [ 49]. However, existing studies have only focused
3

--- PAGE 4 ---
on single task problems. In this work, we leverage SAM’s principle to develop theory and devise
practical methods, allowing seeking flat minima in gradient-based multi-task learning models.
3 Preliminaries
Conventional training methods that focus on minimizing the empirical loss can be easily prone
to overfitting problems (i.e., the validation error no longer decreases, but the training loss keeps
declining), thus, restricting model generalization performance. In an attempt to alleviate such
phenomenons, [ 21] proposed to minimize the worst-case loss in a neighborhood of the current model
parameter given by:
min
θmax
||ϵ||2≤ρL(θ+ϵ), (1)
where || · || 2denotes the l2norm and ρrepresents the radius of the neighborhood. We assume Lis
differentiable up to the first order with respect to θ. The optimization problem (1)is referred to as
sharpness aware minimization (SAM).
To solve problem (1), [21] proposed to first find the solution for the inner maximization by
approximating L(θ+ϵ)via a first-order Taylor expansion w.r.t ϵaround 0, which is as follows:
ϵ∗= arg max
||ϵ||2≤ρL(θ+ϵ)≈arg max
||ϵ||2≤ρϵ⊤∇θL(θ)≈ρ∇θL(θ)
||∇θL(θ)||2.
Putting into words, the worst-case perturbation is approximated as the scaled gradient of the loss
w.r.t the current parameter θ. Then, the gradient w.r.t this perturbed model is computed to update
θ:
gSAM:=∇θmax
||ϵ||2≤ρL(θ+ϵ)≈ ∇θL(θ+ϵ)|θ+ϵ∗ (2)
4 Our Proposed Framework
This section describes our proposed framework to improve existing methods on gradient-based MTL.
We first recall the goal of multi-task learning, then establish the upper bounds for the general loss
of each task. Subsequently, we rely on these upper bounds to devise the proposed framework for
improving the model generalization ability by guiding it to a flatter region of each task.
4.1 Multi-task learning setting
In multi-task learning, we are given a data-label distribution Dfrom which we can sample a training
setS={(xi, y1
i, ..., ym
i)n
i=1}, where xiis a data example and y1
i, ..., ym
iare the labels of the tasks
1,2, ..., mrespectively.
The model for each task θi= [θsh,θi
ns]consists of the shared part θshand the individual
non-shared part θi
ns. We denote the general loss for task iasLi
D(θi), while its empirical loss over
the training set SasLi
S(θi). Existing works in MTL, typically MGDA [ 8], PCGrad [ 9], CAGrad [ 10],
and IMTL [ 11], aim to find a model that simultaneously minimizes the empirical losses for all tasks:
min
θsh,θ1:m
ns
L1
S 
θ1
, ...,Lm
S(θm)
, (3)
4

--- PAGE 5 ---
by calculating gradient gifor i-th task ( i∈[m]). The current model parameter is then updated by the
unifiedgradient g=gradient_aggregate (g1,g2, . . . ,gm),withthegenericoperationgradient_aggregate
is to combine multiple task gradients, as proposed in gradient-based MTL studies.
Additionally, prior works only focus on minimizing the empirical losses and do not concern the
general losses which combat overfitting. Inspired by SAM [ 21], it is desirable to develop sharpness-
aware MTL approaches wherein the task models simultaneously seek low loss and flat regions.
However, this is challenging since we have multiple objective functions in (3) and each task model
consists of a shared and an individual non-shared parts. To address the above challenge, in Theorem
1, we develop upper bounds for the task general losses in the context of MTL which signifies the
concepts of sharpness for the shared part and non-shared parts and then rely on these new concepts
to devise a novel MTL framework via seeking the task-based flat regions.
4.2 Theoretical development
We first state our main theorem that bounds the generalization performance of individual tasks by
the empirical error on the training set:
Theorem 1. (Informally stated) For any perturbation radius ρsh, ρns>0, under some mild
assumptions, with probability 1−δ(over the choice of training set S ∼ D) we obtain

Li
D 
θim
i=1≤ max
∥ϵsh∥2≤ρsh
max
∥ϵins∥2≤ρnsLi
S 
θsh+ϵsh,θi
ns+ϵi
ns
+fi 
∥θi∥2
2m
i=1,(4)
where fi:R+→R+, i∈[m]are strictly increasing functions.
Theorem 1 establishes the connection between the generalization error of each task with its
empirical training error via worst-case perturbation on the parameter space. The formally stated
theoremandproofareprovidedintheappendix. Herewenotethattheworst-casesharedperturbation
ϵshis commonly learned for all tasks, while the worst-case non-shared perturbation ϵi
nsis tailored
for each task i. Theorem 1 directly hints us an initial and direct approach.
Additionally, [ 21] invokes the [ 56] PAC-Bayesian generalization bound , hence is only applicable
to the 0-1 loss in the binary classification setting. In terms of the theory contribution, we employ
a more general PAC-Bayesian generalization bound [ 57] to tackle more general losses in MTL.
Moreover, our theory development requires us to handle multiple objectives, each of which consists
of the non-shared and shared parts, which is certainly non-trivial.
4.3 Initial and direct approach
A straight-forward approach guided by Theorem 1 is to find the non-shared perturbations ϵi
ns, i∈[m]
independently for the non-shared parts and a common shared perturbation for the shared part.
Driven by this theoretical guidance, we propose the following updates.
Update the non-shared parts. Based on the upper bounds in Theorem 1, because the non-
shared perturbations ϵi
ns, i∈[m]are independent to each task, for task i, we update its non-shared
5

--- PAGE 6 ---
partθi
ns:
ϵi
ns=ρns∇θi
nsLi
S 
θsh,θi
ns
∥∇θi
nsLi
S 
θsh,θi
ns
∥2,
gi,SAM
ns =∇θi
nsLi
S 
θsh,θi
ns+ϵi
ns
,
θi
ns=θi
ns−ηgi,SAM
ns ,where η >0is the learning rate. (5)
Update the shared part. Updating the shared part θshis more challenging because its worst-cased
perturbation ϵshis shared among the tasks. To derive how to update θshw.r.t. all tasks, we first
discuss the case when we update this w.r.t. task iwithout caring about other tasks. Specifically,
this task’s SAM shared gradient is computed as:
ϵi
sh=ρsh∇θshLi
S 
θsh,θi
ns
∥∇θshLi
S 
θsh,θi
ns
∥2,
gi,SAM
sh=∇θshLi
S 
θsh+ϵi
sh,θi
ns
,
then we have a straight-forward updating strategy:
gSAM
sh=gradient_aggregate (g1,SAM
sh, . . . ,gm,SAM
sh),
θsh=θsh−ηgSAM
sh.
AccordingtoouranalysisinSection4.4, each gi,SAM
sh=gi,loss
sh+gi,flat
shisconstitutedbytwocomponents:
(i)gi,loss
shto navigate to the task low-loss region and (ii) gi,flat
shto navigate to the task-based flat
region. However, a direct gradient aggregation of gi,SAM
sh, i∈[m]can be negatively affected by the
gradient cancelation or conflict because it aims to combine many individual elements with different
objectives. In this paper, we go beyond this initial approach by deriving an updating formula to
decompose SAM gradient into two components, each serving its own purpose, and then combining
their corresponding task gradients simultaneously. We also compare our method against the naive
approach in Section 5.3.
4.4 Our proposed approach
The non-shared parts are updated normally as in Equation (5). It is more crucial to investigate how
to update the shared part more efficiently. To better understand the SAM’s gradients, we analyze
their characteristics by deriving them as follows:
gi,SAM
sh=∇θshLi
S 
θsh+ϵi
sh,θi
ns(1)≈ ∇ θsh
Li
S 
θsh,θi
ns
+
ϵi
sh,∇θshLi
S 
θsh,θi
ns
=∇θsh
Li
S 
θsh,θi
ns
+ρsh
∇θshLi
S 
θsh,θi
ns
∥∇θshLi
S 
θsh,θi
ns
∥2,∇θshLi
S 
θsh,θi
ns
=∇θsh
Li
S 
θsh,θi
ns
+ρsh∥∇θshLi
S 
θsh,θi
ns
∥2
(6)
where in(1)≈, we apply the first-order Taylor expansion and ⟨·,·⟩represents the dot product.
It is obvious that following the negative direction of gi,SAM
shwill minimize the loss Li
S 
θsh,θi
ns
and the gradient norm ∥∇θshLi
S 
θsh,θi
ns
∥2of task i, hence leading the model to the low-valued
region for the loss of this task and its flatter region with a lower gradient norm magnitude.
6

--- PAGE 7 ---
Moreover, inspired from the derivation in Equation (6), we decompose the gradient gi,SAM
sh=
gi,loss
sh+gi,flat
shwhere we define gi,loss
sh:=∇θshLi
S 
θsh,θi
ns
andgi,flat
sh:=gi,SAM
sh−gi,loss
sh. As
aforementioned, the purpose of the negative gradient −gi,loss
shis to orient the model to minimize the
loss of the task i, while −gi,flat
shnavigates the model to the task i’s flatter region.
Therefore, the SAM gradients gi,SAM
sh, i∈[m]constitute two components with different purposes.
To mitigate the possible confliction and interference of the two components when aggregating, we
propose to aggregate the low-loss components solely and then the flat components solely. Specifically,
to find a common direction that leads the joint low-valued losses for all tasks and the joint flatter
region for them, we first combine the gradients gi,loss
sh, i∈[m]and the gradients gi,flat
sh, i∈[m], then
add the two aggregated gradients, and finally update the shared part as:
gloss
sh=gradient_aggregate (g1,loss
sh, . . . ,gm,loss
sh),
gflat
sh=gradient_aggregate (g1,flat
sh, . . . ,gm,flat
sh),
gSAM
sh=gloss
sh+gflat
sh;θsh=θsh−ηgSAM
sh,
Finally, the key steps of our proposed framework are summarized in Algorithm 1 and the overall
schema of our proposed method is demonstrated in Figure 1.
Algorithm 1 Sharpness minimization for multi-task learning
Input:Model parameter θ= [θsh,θ1:m
ns], perturbation radius ρ= [ρsh, ρns], step size ηand a list of
mdifferentiable loss functions
Li	m
i=1.
Output: Updated parameter θ∗
1:fortaski∈[m]do
2:Compute gradient gi,loss
sh,gi
ns← ∇ θLi(θ)
3:Worst-case perturbation direction
ϵi
sh=ρsh·gi,loss
sh/gi,loss
shandϵi
ns=ρns·gi
ns/gi
ns
4:Approximate SAM’s gradient
gi,SAM
sh=∇θshLi(θsh+ϵi
sh,θi
ns)andgi,SAM
ns =∇θi
nsLi(θsh,θi
ns+ϵi
ns)
5:Compute flat gradient
gi,flat
sh=gi,SAM
sh−gi,loss
sh
6:end for
7:Calculate combined update gradients:
gloss
sh=gradient_aggregate (g1,loss
sh,g2,loss
sh, . . . ,gm,loss
sh)
gflat
sh=gradient_aggregate (g1,flat
sh,g2,flat
sh, . . . ,gm,flat
sh)
8:Calculate shared gradient update gSAM
sh=gloss
sh+gflat
sh
9:Update model parameter
θ∗= [θsh,θ1:m
ns]−η[gSAM
sh,g1:m,SAM
ns ]
7

--- PAGE 8 ---
5 Experiments
In this section, we compare our proposed method against other state-of-the-art methods of multi-task
learning in different scenarios, ranging from image classification to scene understanding problems.
Refer to the appendix for the detailed settings used for each dataset and additional experiments.
Datasets and Baselines. Our proposed method is evaluated on four MTL benchmarks including
Multi-MNIST [ 25], CelebA [ 58] for visual classification, and NYUv2 [ 59], CityScapes [ 60] for scene
understanding. We show how our framework can boost the performance of gradient-based MTL
methods by comparing vanillaMGDA [8], PCGrad [ 9], CAGrad [ 10] and IMTL [ 11] to their flat-based
versions F-MGDA, F-PCGrad, F-CAGrad and F-IMTL. We also add single task learning (STL)
baseline for each dataset.
5.1 Image classification
Multi-MNIST. Following the protocol of [ 8], we set up three Multi-MNIST experiments with the
ResNet18 [ 61] backbone, namely: MultiFashion, MultiMNIST and MultiFashion+MNIST. In each
dataset, two images are sampled uniformly from the MNIST [ 62] or Fashion-MNIST [ 63], then one
is placed on the top left and the other is on the bottom right. We thus obtain a two-task learning
that requires predicting the categories of the digits or fashion items on the top left (task 1) and on
the bottom right (task 2) respectively.
Table 1: Evaluation of different methods on three Multi-MNIST datasets. Rows with flat-based
minimizers are shaded. Bold numbers denote higher accuracy between flat-based methods and their
baselines.∗denotes the highest accuracy (except for STL, since it unfairly exploits multiple neural
networks). We also use arrows to indicate that the higher is the better (↑)or vice-versa (↓).
MultiFashion MultiMNIST MultiFashion+MNIST
Method
Task 1 ↑ Task 2 ↑Average ↑ Task 1 ↑ Task 2 ↑ Average ↑ Task 1 ↑ Task 2 Average ↑
STL 87.10±0.09 86 .20±0.06 86 .65±0.02 95.33±0.08 94 .16±0.04 94 .74±0.06 98.40±0.02 89 .42±0.03 93 .91±0.02
MGDA 86.76±0.09 85 .78±0.36 86 .27±0.22 95.62±0.02 94 .49±0.10 95 .05±0.06 97.24±0.04 88 .19±0.13 92 .72±0.07
F-MGDA 88.12±0.1187.35±0.1187.73±0.09 96.37±0.06 94.99±0.06 95.68±0.00 97.30±0.09 89.26±0.14 93.28±0.03
PCGrad 86.93±0.17 86 .20±0.14 86 .57±0.12 95.71±0.03 94 .41±0.02 95 .06±0.02 97.12±0.16 88 .45±0.08 92 .78±0.11
F-PCGrad 88.17±0.1487.35±0.2787.76±0.07 96.49±0.05 95.34±0.10 95.92±0.07 97.65±0.0689.35±0.07∗93.50±0.01
CAGrad 86.99±0.17 86 .04±0.15 86 .51±0.16 95.62±0.05 94 .39±0.04 95 .01±0.04 97.19±0.06 88 .18±0.1492.68±0.04
F-CAGrad 88.19±0.19∗87.45±0.1387.82±0.10∗96.54±0.0295.36±0.04∗95.95±0.01∗97.82±0.05∗89.26±0.2293.54±0.13∗
IMTL 87.35±0.22 86 .45±0.09 86 .90±0.15 95.93±0.09 94 .63±0.13 95 .28±0.02 97.47±0.06 88 .46±0.11 92 .97±0.03
F-IMTL 88.1±0.10 87.5±0.04∗87.80±0.0696.55±0.07∗95.16±0.05 95.85±0.05 97.59±0.12 88.99±0.08 93.29±0.02
As summarized in Table 1, we can see that seeking flatter regions for all tasks can improve the
performance of all the baselines across all three datasets. Especially, flat-based methods achieve the
highest score for each task and for the average, outperforming STL by 1.2%on MultiFashion and
MultiMNIST. We conjecture that the discrepancy between gradient update trajectories to classify
digits from MNIST and fashion items from FashionMNIST has resulted in the fruitless performance
of baselines, compared to STL on MultiFashion+MNIST. Even if there exists dissimilarity between
tasks, our best obtained average accuracy when applying our method to CAGrad is just slightly
lower than STL ( <0.4%) while employing a single model only.
Interestingly,ourproposedMTLtrainingmethodalsohelpsimprovemodelcalibrationperformance
by mitigating the over-confident phenomenon of deep neural networks. As can be seen from Figure
8

--- PAGE 9 ---
0.0 0.5 1.0 1.5 2.00.02.55.07.510.012.515.017.5 Our
ERM
0.0 0.5 1.0 1.5 2.00.00.51.01.52.02.53.03.5 Our
ERMFigure 2: Entropy distributions of ResNet18 on in-domain set (left) and out-of-domain set (right).
2, our method produces high-entropy predictions that represent its uncertainty, ERM-based method
outputs high confident predictions on both in and out-of-domain data. More details about model
calibration improvement can be found in the appendix.
Table 2: Mean of error per
category of MTL algorithms
in multi-label classification
on CelebA dataset.
Method Average error ↓
STL 8.77
LS 9.99
UW 9.66
MGDA 9.96
F-MGDA 9.22
PCGrad 8.69
F-PCGrad 8.23
CAGrad 8.52
F-CAGrad 8.22∗
IMTL 8.88
F-IMTL 8.24CelebA. CelebA [ 64] is a face dataset, which consists of 200K
celebrity facial photos with 40 attributes. Similar to [ 8], each attribute
forms a binary classification problem, thus a 40-class multi-label
classification problem is constructed.
Table 2 shows the average errors over 40 tasks of the methods
with linear scalarization (LS) and Uncertainty weighting (UW) [ 32]
being included to serve as comparative baselines. The best results
in each pair and among all are highlighted using boldfont and∗,
respectively. When the number of tasks is large, flat region seeking
still consistently shows its advantages and the lowest average accuracy
error is achieved by F-CAGrad. Interestingly, when the optimizer is
aware of flat minima, the gaps between PCGrad, IMTL and CAGrad,
(8.23,8.24vs8.22), are smaller than those using conventional ERM
training, ( 8.69,8.88and8.52). This might be due to the better
aggregation of tasks’ gradients, which means that the conflict between
these gradients is likely to be reduced when the shared parameters
approach the common flat region of all tasks.
5.2 Scene Understanding
Two datasets used in this sub-section are NYUv2 [ 59] and CityScapes [ 60]. NYUv2 is an indoor
scene dataset that contains 3 tasks: 13-class semantic segmentation, depth estimation, and surface
normal prediction. In CityScapes, there are 19 classes of street-view images, which are coarsened
into 7 categories to create two tasks: semantic segmentation and depth estimation. For these two
experiments, we additionally include several recent MTL methods, namely, scale-invariant (SI),
random loss weighting (RLW), Dynamic Weight Average (DWA) [ 2], GradDrop [ 65], and Nash-MTL
[13] whose results are taken from [13]. Details of each baseline can be found in the appendix. Also
following the standard protocol used in [ 2,10,13], Multi-Task Attention Network [ 2] is employed on
top of the SegNet architecture [ 66], our presented results are averaged over the last 10epochs to
align with previous work.
Evaluation metric. In this experiment, we have to deal with different task types rather than
one only as in the case of image classification. Since each of them has its own set of metrics. We thus
9

--- PAGE 10 ---
mark the overall performance of comparative methods by reporting their relative task improvement
[67] throughout this section. Let MiandSibe the metrics obtained by the main and the
Table 3: Test performance for two-task CityScapes: semantic segmentation and depth estimation.∗
denotes the best score for each task’s metrics.
Segmentation Depth
Method mIoU ↑Pix Acc ↑Abs Err ↓Rel Err ↓∆m%↓
STL 74.01 93 .16 0.0125 27 .77
LS 75.18 93 .49 0.0155 46 .77 22.60
SI 70.95 91 .73 0.0161 33 .83 14.11
RLW 74.57 93 .41 0.0158 47 .79 24.38
DWA 75.24 93 .52 0.0160 44 .3721.45
UW 72.02 92 .85 0.0140 30 .13∗5.89
GradDrop 75.27 93 .53 0.0157 47 .54 23.73
Nash-MTL 75.41 93 .66 0.0129 35 .02 6.82
MGDA 68.84 91 .54 0.0309 33 .50 44.14
F-MGDA 73.77 93.12 0.0129 27.44∗0.67∗
PCGrad 75.13 93 .48 0.0154 42 .07 18.29
F-PCGrad 75.77 93.67 0.0144 39.60 13.65
CAGrad 75.16 93 .48 0.0141 37 .60 11.64
F-CAGrad 76.02 93.72 0.0134 34.64 7.25
IMTL 75.33 93 .49 0.0135 38 .41 11.10
F-IMTL 76.63∗93.76∗0.0124∗31.17 1.87
single-task learning (STL) model, respectively, the relative task improvement on i-th task is
mathematically given by: ∆i:= 100 ·(−1)li(Mi−Si)/Si, where li= 1if a lower value for the i-th
criterion is better and 0otherwise. We depict our results by the average relative task improvement
∆m%=1
mPm
i=1∆i.
CityScapes. In Table 3, the positive effect of seeking flat regions is consistently observed in all
metrics and baselines. In particular, the relative improvements of MGDA and IMTL are significantly
boosted, achieving the highest and second-best ∆m%scores, respectively. The segmentation scores
of PCGrad, CAGrad and IMTL even surpass STL. Intriguingly, MGDA biases to the depth estimation
objective, leading to the predominant performance on that task, similar patterns appear in [ 11] and
the below NYUv2 experiment.
NYUv2. Table 4 shows each task’s results and the relative improvements over STL of different
methods. Generally, the flat-based versions obtain comparable or higher results on most of the
metrics, except for MGDA at the segmentation task, in which F-MGDA notably decreases the mIoU
score. However, it does significantly help other tasks, which contributes to the overall MGDA’s
relative improvement, from 1.38%being worse than STL to 0.33%being higher. Remarkably,
F-CAGrad and F-IMTL outperform their competitors by large margins across all tasks, resulting in
the top two relative improvements, 3.78%and4.77%.
5.3 Ablation study
So far, our proposed technique has shown state-of-the-art performances under different settings, we
now investigate in more detailed how it affects conventional training by inspecting loss surfaces and
model robustness. Similar patterns are observed in other experiments and given in the appendix.
Task conflict. To empirically confirm that tasks’ gradients are less conflicted when the model
is driven to the flat regions, we measure the gradient conflict and present the result in Figure 3.
10

--- PAGE 11 ---
Table 4: Test performance for three-task NYUv2 of Segnet [ 66]: semantic segmentation, depth
estimation, and surface normal. Using the proposed procedure in conjunction with gradient-based
multi-task learning methods consistently advances their overall performance.
Segmentation Depth Surface Normal
mIoU ↑Pix Acc ↑Abs Err ↓Rel Err ↓Angle Distance ↓ Within t◦↑ ∆m%↓
Mean Median 11.25 22.5 30
STL 38.30 63 .76 0.6754 0 .2780 25.01 19 .21 30 .14 57 .20 69 .150.00
LS 39.29 65 .33 0.5493 0 .2263 28.15 23 .96 22 .09 47 .50 61 .08 5.59
SI 38.45 64 .27 0.5354 0 .2201 27.60 23 .37 22 .53 48 .57 62 .32 4.39
RLW 37.17 63 .77 0.5759 0 .2410 28.27 24 .18 22 .26 47 .05 60 .62 7.78
DWA 39.11 65 .31 0.5510 0 .2285 27.61 23 .18 24 .17 50 .18 62 .39 3.57
UW 36.87 63 .17 0.5446 0 .2260 27.04 22 .61 23 .54 49 .05 63 .65 4.05
GradDrop 39.39 65 .12 0.5455 0 .2279 27.48 22 .96 23 .38 49 .44 62 .87 3.58
Nash-MTL 40.13 65 .93 0.5261∗0.2171 25.26 20 .08 28 .4 55 .47 68 .15−4.04
MGDA 30.47 59 .90 0.6070 0.2555 24.88 19 .45 29 .18 56 .88 69 .36 1.38
F-MGDA 26.42 58.78 0.6078 0.2353 24.34∗18.45∗31.64∗58.86∗70.50∗−0.33
PCGrad 38.06 64 .64 0.5550 0 .2325 27.4122.80 23 .86 49 .83 63 .14 3.97
F-PCGrad 40.05 65.42 0.5429 0.2243 27.38 23.00 23.47 49.35 62.74 3.14
CAGrad 39.79 65 .49 0.5486 0 .2250 26.31 21 .58 25 .61 52 .36 65 .58 0.20
F-CAGrad 40.93∗66.68∗0.5285 0.2162 25.4320.39 27.9954.8267.56−3.78
IMTL 39.35 65 .60 0.5426 0 .2256 26.02 21 .19 26 .2 53 .13 66 .24−0.76
F-IMTL 40.42 65.61 0.5389 0.2121∗25.0319.75 28.9056.1968.72−4.77∗
While the percentage of gradient conflict of ERM increases to more than 50%, ours decreases and
approaches 0%. This reduction in gradient conflict is also the goal of recent gradient-based MTL
methods in mitigating negative transfer between tasks [9, 68, 3].
0 25 50 75 100 125 150 175 200
Epochs0102030405060Gradient conflict (%)
Ours ERM
Figure 3: Proportion of conflict between per-task gradients ( g1,loss·g2,loss<0) on
MultiFashion+MNIST dataset.
Model robustness against noise. To verify that SAM can orient the model to the common
flat and low-loss region of all tasks, we measure the model performance within a r-radius Euclidean
ball. To be more specific, we perturb parameters of two converged models by ϵ, which lies in a
r-radius ball and plot the accuracy of the perturbed models of each task as we increase rfrom 0to
11

--- PAGE 12 ---
1000. At each value of r,10different models around the r−radius ball of the converged model are
sampled.
0 200 400 600 800
r20406080100Accuracy
Ours ERM
Figure 4: Accuracy within r−radius ball. Solid/dashed lines denote performance on train/test sets,
respectively.
In Figure 4, the accuracy of the model trained using our method remains at a high level when
noise keeps increasing until r= 800. This also gives evidence that our model found a region that
changes slowly in loss. By contrast, the naively trained model loses its predictive capabilities as
soon as the noise appears and becomes a dummy classifier that attains 10%accuracy in a 10-way
classification.
Aggregation strategies comparison. Table 5 provides a comparison between the direct
aggregation on {gi,SAM
sh}m
i=1andindividual aggregation on {gi,flat
sh}m
i=1and{gi,loss
sh}m
i=1(our method).
Table 5: Two aggregation strategies on CityScapes.
Segmentation Depth
Method mIoU ↑Pix Acc ↑Abs Err ↓Rel Err ↓∆m%↓
ERM 68.84 91 .54 0.0309 33 .50 44.14
Ours (direct) 68.93 91 .41 0.0130 31 .37 6.43
Ours (individual )73.77 93.12 0.0129 27.44∗0.67
Compared to the naive approach, in which per-task SAM gradients are directly aggregated, our
decomposition approach consistently improves performance by a large margin across all tasks. This
result reinforces the rationale behind separately aggregating low-loss directions and flat directions.
Visualization of the loss landscapes. Following [ 69], we plot the loss surfaces at convergence
after training Resnet18 from scratch on the MultiMNIST dataset. Test loss surfaces of checkpoints
that have the highest validation accuracy scores are shown in Figure 5.
We can clearly see that the solution found by our proposed method not only mitigates the test
loss sharpness for both tasks but also can intentionally reduce the test loss value itself, in comparison
to traditional ERM. This is a common behavior when using flat minimizers as the gap between train
and test performance has been narrowed [44, 14].
12

--- PAGE 13 ---
Figure 5: Visualization of test loss surfaces with standard ERM training and when applying our
method. The coordinate plane axes are two random sampled orthogonal Gaussian perturbations.
6 Conclusion
In this work, we have presented a general framework that can be incorporated into current multi-task
learning methods following the gradient balancing mechanism. The core ideas of our proposed
method are the employment of flat minimizers in the context of MTL and proving that they can help
enhance previous works both theoretically and empirically. Concretely, our method goes beyond
optimizing per-task objectives solely to yield models that have both low errors and high generalization
capabilities. On the experimental side, the efficacy of our method is demonstrated on a wide range
of commonly used MTL benchmarks, in which ours consistently outperforms comparative methods.
References
[1] Z. Zhang, P. Luo, C. C. Loy, and X. Tang, “Facial landmark detection by deep multi-task learning,” in
European conference on computer vision , pp. 94–108, Springer, 2014.
[2]S. Liu, E. Johns, and A. J. Davison, “End-to-end multi-task learning with attention,” in Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition , pp. 1871–1880, 2019.
[3]Z. Wang, Y. Tsvetkov, O. Firat, and Y. Cao, “Gradient vaccine: Investigating and improving multi-task
optimization in massively multilingual models,” in International Conference on Learning Representations ,
2020.
[4]I. Kokkinos, “Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level
vision using diverse datasets and limited memory,” in Proceedings of the IEEE conference on computer
vision and pattern recognition , pp. 6129–6138, 2017.
[5]F. Heuer, S. Mantowsky, S. Bukhari, and G. Schneider, “Multitask-centernet (mcn): Efficient and diverse
multitask learning using an anchor free approach,” in Proceedings of the IEEE/CVF International
Conference on Computer Vision , pp. 997–1005, 2021.
[6]Y. Gao, J. Ma, M. Zhao, W. Liu, and A. L. Yuille, “Nddr-cnn: Layerwise feature fusing in multi-task
cnns by neural discriminative dimensionality reduction,” in Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pp. 3205–3214, 2019.
[7]S. Ruder, J. Bingel, I. Augenstein, and A. Søgaard, “Latent multi-task architecture learning,” in
Proceedings of the AAAI Conference on Artificial Intelligence , vol. 33, pp. 4822–4829, 2019.
[8]O. Sener and V. Koltun, “Multi-task learning as multi-objective optimization,” Advances in neural
information processing systems , vol. 31, 2018.
13

--- PAGE 14 ---
[9]T. Yu, S. Kumar, A. Gupta, S. Levine, K. Hausman, and C. Finn, “Gradient surgery for multi-task
learning,” Advances in Neural Information Processing Systems , vol. 33, pp. 5824–5836, 2020.
[10]B. Liu, X. Liu, X. Jin, P. Stone, and Q. Liu, “Conflict-averse gradient descent for multi-task learning,”
Advances in Neural Information Processing Systems , vol. 34, pp. 18878–18890, 2021.
[11]L. Liu, Y. Li, Z. Kuang, J.-H. Xue, Y. Chen, W. Yang, Q. Liao, and W. Zhang, “Towards impartial
multi-task learning,” in International Conference on Learning Representations , 2020.
[12]A. Javaloy and I. Valera, “Rotograd: Gradient homogenization in multitask learning,” in International
Conference on Learning Representations , 2021.
[13]A. Navon, A. Shamsian, I. Achituve, H. Maron, K. Kawaguchi, G. Chechik, and E. Fetaya, “Multi-task
learning as a bargaining game,” arXiv preprint arXiv:2202.01017 , 2022.
[14]J. Kaddour, L. Liu, R. Silva, and M. J. Kusner, “A fair comparison of two popular flat minima optimizers:
Stochastic weight averaging vs. sharpness-aware minimization,” arXiv preprint arXiv:2202.00661 , vol. 1,
2022.
[15]Y. Zhao, H. Zhang, and X. Hu, “Penalizing gradient norm for efficiently improving generalization in
deep learning,” arXiv preprint arXiv:2202.03599 , 2022.
[16]N. S. Keskar, J. Nocedal, P. T. P. Tang, D. Mudigere, and M. Smelyanskiy, “On large-batch training
for deep learning: Generalization gap and sharp minima,” in 5th International Conference on Learning
Representations, ICLR 2017 , 2017.
[17]Z. Li, Z. Wang, and J. Li, “Analyzing sharpness along gd trajectory: Progressive sharpening and edge of
stability,” arXiv preprint arXiv:2207.12678 , 2022.
[18]K. Lyu, Z. Li, and S. Arora, “Understanding the generalization benefit of normalization layers: Sharpness
reduction,” arXiv preprint arXiv:2206.07085 , 2022.
[19]H. He, G. Huang, and Y. Yuan, “Asymmetric valleys: Beyond sharp and flat local minima,” Advances in
neural information processing systems , vol. 32, 2019.
[20]Y. Zheng, R. Zhang, and Y. Mao, “Regularizing neural networks via adversarial model perturbation,” in
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 8156–8165,
2021.
[21]P. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur, “Sharpness-aware minimization for efficiently
improving generalization,” in International Conference on Learning Representations , 2021.
[22] R. Caruana, “Multitask learning,” Machine learning , vol. 28, no. 1, pp. 41–75, 1997.
[23]X.Liu, P.He, W.Chen, andJ.Gao, “Multi-taskdeepneuralnetworksfornaturallanguageunderstanding,”
inProceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp. 4487–
4496, Association for Computational Linguistics, 2019.
[24]S. Ruder, “An overview of multi-task learning in deep neural networks,” arXiv preprint arXiv:1706.05098 ,
2017.
[25]X. Lin, H.-L. Zhen, Z. Li, Q.-F. Zhang, and S. Kwong, “Pareto multi-task learning,” Advances in neural
information processing systems , vol. 32, 2019.
[26]X. Liu, X. Tong, and Q. Liu, “Profiling pareto front with multi-objective stein variational gradient
descent,” Advances in Neural Information Processing Systems , vol. 34, pp. 14721–14733, 2021.
[27]D. Mahapatra and V. Rajan, “Multi-task learning with user preferences: Gradient descent with controlled
ascent in pareto optimization,” in International Conference on Machine Learning , pp. 6597–6607, PMLR,
2020.
14

--- PAGE 15 ---
[28]D. Mahapatra and V. Rajan, “Exact pareto optimal search for multi-task learning: Touring the pareto
front,”arXiv preprint arXiv:2108.00597 , 2021.
[29]X. Lin, Z. Yang, Q. Zhang, and S. Kwong, “Controllable pareto multi-task learning,” arXiv preprint
arXiv:2010.06313 , 2020.
[30]A. Navon, A. Shamsian, G. Chechik, and E. Fetaya, “Learning the pareto front with hypernetworks,” in
International Conference on Learning Representations , 2021.
[31]Z. Chen, V. Badrinarayanan, C.-Y. Lee, and A. Rabinovich, “Gradnorm: Gradient normalization for
adaptive loss balancing in deep multitask networks,” in International conference on machine learning ,
pp. 794–803, PMLR, 2018.
[32]A. Kendall, Y. Gal, and R. Cipolla, “Multi-task learning using uncertainty to weigh losses for scene
geometry and semantics,” in Proceedings of the IEEE conference on computer vision and pattern
recognition , pp. 7482–7491, 2018.
[33]M. Guo, A. Haque, D.-A. Huang, S. Yeung, and L. Fei-Fei, “Dynamic task prioritization for multitask
learning,” in Proceedings of the European conference on computer vision (ECCV) , pp. 270–287, 2018.
[34]Y. Jiang, B. Neyshabur, H. Mobahi, D. Krishnan, and S. Bengio, “Fantastic generalization measures and
where to find them,” in ICLR, OpenReview.net, 2020.
[35]H. Petzka, M. Kamp, L. Adilova, C. Sminchisescu, and M. Boley, “Relative flatness and generalization,”
inNeurIPS , pp. 18420–18432, 2021.
[36]G. K. Dziugaite and D. M. Roy, “Computing nonvacuous generalization bounds for deep (stochastic)
neural networks with many more parameters than training data,” in UAI, AUAI Press, 2017.
[37]S. Hochreiter and J. Schmidhuber, “Simplifying neural nets by discovering flat minima,” in NIPS,
pp. 529–536, MIT Press, 1994.
[38]B. Neyshabur, S. Bhojanapalli, D. McAllester, and N. Srebro, “Exploring generalization in deep learning,”
Advances in neural information processing systems , vol. 30, 2017.
[39]L. Dinh, R. Pascanu, S. Bengio, and Y. Bengio, “Sharp minima can generalize for deep nets,” in
International Conference on Machine Learning , pp. 1019–1028, PMLR, 2017.
[40]S. Fort and S. Ganguli, “Emergent properties of the local geometry of neural loss landscapes,” arXiv
preprint arXiv:1910.05929 , 2019.
[41]G. Pereyra, G. Tucker, J. Chorowski, L. Kaiser, and G. E. Hinton, “Regularizing neural networks by
penalizing confident output distributions,” in ICLR (Workshop) , OpenReview.net, 2017.
[42]P. Chaudhari, A. Choromańska, S. Soatto, Y. LeCun, C. Baldassi, C. Borgs, J. T. Chayes, L. Sagun, and
R. Zecchina, “Entropy-sgd: biasing gradient descent into wide valleys,” Journal of Statistical Mechanics:
Theory and Experiment , vol. 2019, 2017.
[43]N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang, “On large-batch training for
deep learning: Generalization gap and sharp minima,” in ICLR, OpenReview.net, 2017.
[44]P. Izmailov, D. Podoprikhin, T. Garipov, D. P. Vetrov, and A. G. Wilson, “Averaging weights leads to
wider optima and better generalization,” in UAI, pp. 876–885, AUAI Press, 2018.
[45]S. Jastrzebski, Z. Kenton, D. Arpit, N. Ballas, A. Fischer, Y. Bengio, and A. J. Storkey, “Three factors
influencing minima in sgd,” ArXiv, vol. abs/1711.04623, 2017.
[46]C.Wei, S.Kakade, andT.Ma, “Theimplicitandexplicitregularizationeffectsofdropout,” in International
conference on machine learning , pp. 10181–10192, PMLR, 2020.
[47]Y. Zhang, T. Xiang, T. M. Hospedales, and H. Lu, “Deep mutual learning,” 2018 IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pp. 4320–4328, 2018.
15

--- PAGE 16 ---
[48]L. Zhang, J. Song, A. Gao, J. Chen, C. Bao, and K. Ma, “Be your own teacher: Improve the performance
of convolutional neural networks via self distillation,” in Proceedings of the IEEE/CVF International
Conference on Computer Vision , pp. 3713–3722, 2019.
[49]J. Cha, S. Chun, K. Lee, H.-C. Cho, S. Park, Y. Lee, and S. Park, “Swad: Domain generalization by
seeking flat minima,” Advances in Neural Information Processing Systems , vol. 34, pp. 22405–22418,
2021.
[50]M. Abbas, Q. Xiao, L. Chen, P.-Y. Chen, and T. Chen, “Sharp-maml: Sharpness-aware model-agnostic
meta learning,” arXiv preprint arXiv:2206.03996 , 2022.
[51]Z. Qu, X. Li, R. Duan, Y. Liu, B. Tang, and Z. Lu, “Generalized federated learning via sharpness aware
minimization,” arXiv preprint arXiv:2206.02618 , 2022.
[52]D. Caldarola, B. Caputo, and M. Ciccone, “Improving generalization in federated learning by seeking
flat minima,” in European Conference on Computer Vision , pp. 654–672, Springer, 2022.
[53]D.Bahri, H.Mobahi, andY.Tay, “Sharpness-awareminimizationimproveslanguagemodelgeneralization,”
inProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers) , (Dublin, Ireland), pp. 7360–7371, Association for Computational Linguistics, May 2022.
[54]X. Chen, C.-J. Hsieh, and B. Gong, “When vision transformers outperform resnets without pre-training
or strong data augmentations,” arXiv preprint arXiv:2106.01548 , 2021.
[55]V.-A. Nguyen, T.-L. Vuong, H. Phan, T.-T. Do, D. Phung, and T. Le, “Flat seeking bayesian neural
networks,” Advances in Neural Information Processing Systems , 2023.
[56]D. A. McAllester, “Pac-bayesian model averaging,” in Proceedings of the twelfth annual conference on
Computational learning theory , pp. 164–170, 1999.
[57]P. Alquier, J. Ridgway, and N. Chopin, “On the properties of variational approximations of gibbs
posteriors,” Journal of Machine Learning Research , vol. 17, no. 236, pp. 1–41, 2016.
[58]Z. Liu, P. Luo, X. Wang, and X. Tang, “Deep learning face attributes in the wild,” in Proceedings of the
IEEE international conference on computer vision , pp. 3730–3738, 2015.
[59]N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, “Indoor segmentation and support inference from rgbd
images,” in European conference on computer vision , pp. 746–760, Springer, 2012.
[60]M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and
B. Schiele, “The cityscapes dataset for semantic urban scene understanding,” in Proceedings of the IEEE
conference on computer vision and pattern recognition , pp. 3213–3223, 2016.
[61] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of
the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016.
[62]Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document
recognition,” Proceedings of the IEEE , vol. 86, no. 11, pp. 2278–2324, 1998.
[63]H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-mnist: a novel image dataset for benchmarking machine
learning algorithms,” arXiv preprint arXiv:1708.07747 , 2017.
[64]Z. Liu, P. Luo, X. Wang, and X. Tang, “Large-scale celebfaces attributes (celeba) dataset,” Retrieved
August, vol. 15, no. 2018, p. 11, 2018.
[65]Z. Chen, J. Ngiam, Y. Huang, T. Luong, H. Kretzschmar, Y. Chai, and D. Anguelov, “Just pick a
sign: Optimizing deep multitask models with gradient sign dropout,” Advances in Neural Information
Processing Systems , vol. 33, pp. 2039–2050, 2020.
[66]V. Badrinarayanan, A. Kendall, and R. Cipolla, “Segnet: A deep convolutional encoder-decoder
architecture for image segmentation,” IEEE transactions on pattern analysis and machine intelligence ,
vol. 39, no. 12, pp. 2481–2495, 2017.
16

--- PAGE 17 ---
[67]K.-K.Maninis, I.Radosavovic, andI.Kokkinos, “Attentivesingle-taskingofmultipletasks,” in Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 1851–1860, 2019.
[68]S. Zhu, H. Zhao, P. Wang, H. Deng, J. Xu, and B. Zheng, “Gradient deconfliction via orthogonal
projections onto subspaces for multi-task learning,” 2022.
[69]H. Li, Z. Xu, G. Taylor, C. Studer, and T. Goldstein, “Visualizing the loss landscape of neural nets,”
Advances in neural information processing systems , vol. 31, 2018.
[70]B. Laurent and P. Massart, “Adaptive estimation of a quadratic functional by model selection,” Annals
of Statistics , pp. 1302–1338, 2000.
[71] J.-A. Désidéri, “Multiple-gradient descent algorithm (mgda) for multiobjective optimization,” Comptes
Rendus Mathematique , vol. 350, no. 5-6, pp. 313–318, 2012.
[72]B. Lin, F. Ye, Y. Zhang, and I. W. Tsang, “Reasonable effectiveness of random weighting: A litmus test
for multi-task learning,” arXiv preprint arXiv:2111.10603 , 2021.
[73]D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980 ,
2014.
[74]J. Kwon, J. Kim, H. Park, and I. K. Choi, “Asam: Adaptive sharpness-aware minimization for scale-
invariant learning of deep neural networks,” 18–24 Jul 2021.
[75]C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, “On calibration of modern neural networks,” in
International conference on machine learning , pp. 1321–1330, PMLR, 2017.
[76]G. W. Brier et al., “Verification of forecasts expressed in terms of probability,” Monthly weather review ,
vol. 78, no. 1, pp. 1–3, 1950.
[77]M. P. Naeini, G. Cooper, and M. Hauskrecht, “Obtaining well calibrated probabilities using bayesian
binning,” in Twenty-Ninth AAAI Conference on Artificial Intelligence , 2015.
[78]Y. Ovadia, E. Fertig, J. Ren, Z. Nado, D. Sculley, S. Nowozin, J. Dillon, B. Lakshminarayanan, and
J. Snoek, “Can you trust your model’s uncertainty? evaluating predictive uncertainty under dataset
shift,”Advances in neural information processing systems , vol. 32, 2019.
[79]B. Lakshminarayanan, A. Pritzel, and C. Blundell, “Simple and scalable predictive uncertainty estimation
using deep ensembles,” Advances in neural information processing systems , vol. 30, 2017.
[80]A. Malinin and M. Gales, “Predictive uncertainty estimation via prior networks,” Advances in neural
information processing systems , vol. 31, 2018.
17

--- PAGE 18 ---
Supplement to “Improving Multi-task Learning
via Seeking Task-based Flat Regions"
Due to space constraints, some details were omitted from the main paper. We therefore include
additional theoretical developments (section A) and experimental results (section C) in this appendix.
A Our Theory Development
This section contains the proofs and derivations of our theory development to support the main
submission.
We first start with the following theorem, which is inspired by the general PAC-Bayes in [57].
Theorem 2. With the assumption that adding Gaussian perturbation will raise the test error:
LD(θ)≤Eϵ∼N(0,σ2I)[LD(θ+ϵ)]. Let Tbe the number of parameter θ, and Nbe the cardinality of
S, then the following inequality is true with the probability 1−δ:
LD(θ)≤Eϵ∼N(0,σ2I)[LS(θ+ϵ)] +1√
N"
1
2+T
2log
1 +||θ||2
Tσ2
+ log1
δ+ 6 log( N+T) +L2
8#
where Lis the upper-bound of the loss function.
Proof.We use the PAC-Bayes theory for P=N(0, σ2
PIT)andQ=N(θ, σ2IT)are the prior and
posterior distributions, respectively.
By using the bound in [57], with probability at least 1−δand for all β >0, we have:
Eθ∼Q[LD(θ)]≤Eθ∼Q[LS(θ)] +1
βh
KL(Q∥P) + log1
δ+ Ψ(β, N)i
,
where we have defined:
Ψ(β, N) = log EPESh
expn
β 
LD(θ)− LS(θ)oi
Note that the loss function is bounded by L, according to Hoeffding’s lemma, we have:
Ψ(β, N)≤β2L2
8N.
By Cauchy inequality:
1√
N"
T
2log
1 +||θ||2
Tσ2
+L2
8#
≥L
2√
Nr
Tlog
1 +||θ||2
Tσ2
≥L,
which means that the theorem is proved since the loss function is upper bounded by L, following
assumptions.
Now, we only need to prove the theorem under the case: ||θ||2≤Tσ2h
exp4N
T−1i
.
We need to specify Pin advance since it is a prior distribution. However, we do not know in
advance the value of θthat affects the KL divergence term. Hence, we build a family of distribution
Pas follows:
P=n
Pj=N(0, σ2
PjIT) :σ2
Pj=cexp 1−j
T
, c=σ2 
1 + exp4N
T
, j= 1,2, . . .o
.
18

--- PAGE 19 ---
Setδj=6δ
π2j2, the below inequality holds with probability at least 1−δj:
Eθ∼Q[LD(θ)]≤Eθ∼Q[LS(θ)] +1
βh
KL(Q∥Pj) + log1
δj+β2L2
8Ni
.
Or it can be written as:
Eϵ∼N(0,σ2I)[LD(θ+ϵ)]≤Eϵ∼N(0,σ2I)[LS(θ+ϵ)] +1
βh
KL(Q∥Pj) + log1
δj+β2L2
8Ni
.
Thus, with probability 1−δthe above inequalities hold for all Pj. We choose:
j∗=$
1 +Tlog 
σ2 
1 + exp {4N/T}
σ2+∥θ∥2/T!%
.
Since∥θ∥2
T≤σ2
exp4N
T−1
, we get σ2+∥θ∥2
T≤σ2exp4N
T, thus j∗is well-defined. We also have:
Tlogc
σ2+∥θ∥2/T≤j∗≤1 +Tlogc
σ2+∥θ∥2/T
⇒ logc
σ2+∥θ∥2/T≤j∗
T≤1
T+ logc
σ2+∥θ∥2/T
⇒ −1
T+ logσ2+∥θ∥2/T
c≤−j∗
T≤logσ2+∥θ∥2/T
c
⇒ e−1/Tσ2+∥θ∥2/T
c≤e−j∗/T≤σ2+∥θ∥2/T
c
⇒ σ2+∥θ∥2
T≤ce1−j∗
T≤e1
T
σ2+∥θ∥2
T
⇒ σ2+∥θ∥2
T≤σ2
Pj∗≤e1
T
σ2+∥θ∥2
T
.
Hence, we have:
KL(Q∥Pj∗) =1
2hTσ2+∥θ∥2
σ2
Pj∗−T+Tlogσ2
Pj∗
σ2i
≤1
2hTσ2+∥θ∥2
σ2+∥θ∥2/T−T+Tloge1/T 
σ2+∥θ∥2/T
σ2i
≤1
2h
1 +Tlog 
1 +∥θ∥2
Tσ2i
.
19

--- PAGE 20 ---
For the term log1
δj∗, use the inequality log(1 + et)≤1 +tfort >0:
log1
δj∗= log(j∗)2π2
6δ= log1
δ+ logπ2
6
+ 2 log( j∗)
≤log1
δ+ logπ2
6+ 2 log
1 +Tlogσ2 
1 + exp(4 N/T)
σ2+∥θ∥2/T
≤log1
δ+ logπ2
6+ 2 log
1 +Tlog 
1 + exp(4 N/T)
≤log1
δ+ logπ2
6+ 2 log
1 +T 
1 +4N
T
≤log1
δ+ logπ2
6+ log(1 + T+ 4N).
Choosing β=√
N, with probability at least 1−δwe get:
1
βh
KL(Q∥Pj∗) + log1
δj∗+β2L2
8Ni
≤1√
Nh1
2+T
2log
1 +∥θ∥2
Tσ2
+ log1
δ+ 6 log( N+T)i
+L2
8√
N.
Thus the theorem is proved.
Back to our context of multi-task learning in which we have mtasks with each task model:
θi= [θsh,θi
ns], we can prove the following theorem.
Theorem 3. With the assumption that adding Gaussian perturbation will rise the test error:
LD(θi)≤Eϵ∼N(0,σ2I)
LD(θi+ϵ)
. Let Tibe the number of parameter θiandNbe the cardinality
ofS. We have the following inequality holds with probability 1−δ(over the choice of training set
S ∼ D):
Li
D 
θim
i=1≤
Eϵ∼N(0,σ2I)
LS(θi+ϵ)
+fi 
∥θi∥2
2m
i=1, (7)
where
fi 
∥θi∥2
2
=1√
N"
1
2+Ti
2log
1 +||θ||2
Tiσ2
+ log1
δ+ 6 log( N+Ti) +L2
8#
.
Proof.The result for the base case m= 1can be achieved by using Theorem 2 where ξ=δand
f1is defined accordingly. We proceed by induction, suppose that Theorem 3 is true for all i∈[n]
with probability 1−δ/2, which also means:

Li
D 
θin
i=1≤
Eϵ∼N(0,σI)
LS(θi+ϵ)
+fi 
∥θi∥2
2n
i=1.
Using Theorem 2 for θn+1andξ=δ/2, with probability 1−δ/2, we have:
Ln+1
D 
θn+1
≤Eϵ∼N(0,σI)
LS(θn+1+ϵ)
+fn+1 
∥θn+1∥2
2
.
Using the inclusion–exclusion principle, with probability at least 1−δ, we reach the conclusion
form=n+ 1.
We next prove the result in the main paper. Let us begin by formally restating the main theorem
as follows:
20

--- PAGE 21 ---
Theorem 4. For any perturbation radius ρsh, ρns>0, with probability 1−δ(over the choice of
training set S ∼ D) we obtain:

Li
D 
θim
i=1≤ max
∥ϵsh∥2≤ρsh"
max
∥ϵins∥2≤ρnsLi
S 
θsh+ϵsh,θi
ns+ϵi
ns
+fi 
∥θi∥2
2#m
i=1,(8)
where fi 
∥θi∥2
2
is defined the same as in Theorem 3.
Proof.Theorem 3 gives us
h
Li
D
θiim
i=1≤h
Eϵ∼N(0,σ2I)h
Li
S
θi+ϵi
+fi
∥θi∥2im
i=1
=Z
Eϵinsh
Li
S
θsh+ϵsh,θi
ns+ϵi
nsi
p(ϵsh)dϵsh+fi
∥θi∥2m
i=1
=Eϵshh
Eϵinsh
Li
S
θsh+ϵsh,θi
ns+ϵi
nsi
+fi
∥θi∥2im
i=1,
where p(ϵsh)is the density function of Gaussian distribution; ϵshandϵi
nsare drawn from their
corresponding Gaussian distributions.
We have ϵi
ns∼N(0, σ2Ins)with the dimension Ti,ns, therefore ∥ϵi
ns∥follows the Chi-square
distribution. As proven in [70], we have for all i:
P
∥ϵi
ns∥2
2≥Ti,nsσ2+ 2σ2p
Ti,nst+ 2tσ2
≤e−t,∀t >0
P
∥ϵi
ns∥2
2< Ti,nsσ2+ 2σ2p
Ti,nst+ 2tσ2
>1−e−t
for all t >0.
Select t=ln(√
N), we derive the following bound for the noise magnitude in terms of the
perturbation radius ρnsfor all i:
P
∥ϵi
ns∥2
2≤σ2(2 ln(√
N) +Ti,ns+ 2q
Ti,nsln(√
N))
>1−1√
N. (9)
Moreover, we have ϵsh∼N(0, σ2Ish)with the dimension Tsh, therefore ∥ϵsh∥follows the Chi-
square distribution. As proven in [70], we have:
P
∥ϵsh∥2
2≥Tshσ2+ 2σ2p
Tsht+ 2tσ2
≤e−t,∀t >0
P
∥ϵsh∥2
2< Tshσ2+ 2σ2p
Tsht+ 2tσ2
>1−e−t
for all t >0.
Select t=ln(√
N), we derive the following bound for the noise magnitude in terms of the
perturbation radius ρsh:
P
∥ϵsh∥2
2≤σ2(2 ln(√
N) +Tsh+ 2q
Tshln(√
N))
>1−1√
N. (10)
21

--- PAGE 22 ---
By choosing σless thanρsh q
2 lnN1/2+Tsh+2√
TshlnN1/2andminiρnsq
2 lnN1/2+Ti,ns+2√
Ti,nslnN1/2, and
referring to (9,10), we achieve both:
P 
∥ϵi
ns∥< ρns
>1−1
N1/2,∀i,
P(∥ϵsh∥< ρsh)>1−1
N1/2.
Finally, we finish the proof as:

Li
D 
θim
i=1≤Eϵsh
Eϵins
Li
S 
θsh+ϵsh,θi
ns+ϵi
ns
+fi 
∥θi∥2m
i=1
≤max||ϵsh||<ρsh
max||ϵins||<ρnsLi
S 
θsh+ϵsh,θi
ns+ϵi
ns
+2√
N−1
N+fi 
∥θi∥2m
i=1
To reach the final conclusion, we redefine:
fi 
∥θi∥2
=2√
N−1
N+fi 
∥θi∥2
.
Here we note that we reach the final inequality due to the following derivations:
Eϵsh
Eϵins
Li
S 
θsh+ϵsh,θi
ns+ϵi
nsm
i=1
≤Z
Bsh"Z
BinsLi
S 
θsh+ϵsh,θi
ns+ϵi
ns
dϵi
ns+1√
N#m
i=1dϵsh
+Z
Bc
sh"Z
BinsLi
S 
θsh+ϵsh,θi
ns+ϵi
ns
dϵi
ns+1√
N#m
i=1dϵsh
≤Z
Bsh"Z
BinsLi
S 
θsh+ϵsh,θi
ns+ϵi
ns
dϵi
ns#m
i=1dϵsh+
1−1√
N1√
N+1√
N
≤max||ϵsh||<ρsh
max||ϵins||<ρns
Li
S 
θsh+ϵsh,θi
ns+ϵi
nsm
i=1+2√
N−1
N,
where Bsh={ϵsh:||ϵsh|| ≤ρsh},Bc
shis the compliment set, and Bi
ns=
ϵi
ns:||ϵi
ns|| ≤ρns	
.
B Gradient aggregation strategies overview
This section details how the gradient_aggregate operation is defined according to recent gradient-
based multi-task learning methods that we employed as baselines in the main paper, including MGDA
[8], PCGrad [ 9], CAGrad [ 10] and IMTL [ 11]. Assume that we are given mvectors g1,g2, . . . ,gm
represent task gradients. Typically, we aim to find a combined gradient vector as:
g=gradient_aggregate (g1,g2, . . . ,gm)
.
22

--- PAGE 23 ---
B.1 Multiple-gradient descent algorithm - MGDA
[8] apply MGDA [ 71] to find the minimum-norm gradient vector that lies in the convex hull composed
by task gradients g1,g2, . . . ,gm:
g=argmin ||mX
i=1wigi||2, s.t.mX
i=1wi= 1and , wi≥0∀i.
This approach can guarantee that the obtained solutions lie on the Pareto front of task objective
functions.
B.2 Projecting conflicting gradients - PCGrad
PCgrad resolves the disagreement between tasks by projecting gradients that conflict with each
other, i.e. ⟨gi,gj⟩<0, to the orthogonal direction of each other. Specifically, giis replaced by its
projection on the normal plane of gj:
gi
PC=gi−gi·gj
||gj||2gj.
Then compute the aggregated gradient based on these deconflict vectors g=Pm
igi
PC.
B.3 Conflict Averse Gradient Descent - CAGrad
CAGrad [ 10] seeks a worst-case direction in a local ball around the average gradient of all tasks, g0,
that minimizes conflict with all of the gradients. The updated vector is obtained by optimizing the
following problem:
max
g∈Rmin
i∈[m]⟨gi, g⟩s.t.||g−g0|| ≤c|||g0|,
where g0=1
mPm
igiis the averaged gradient and cis a hyper-parameter.
B.4 Impartial multi-task learning - IMTL
IMTL [11] proposes to balance per-task gradients by finding the combined vector g, whose projections
onto{gi}m
i=1are equal. Following this, they obtain the closed-form solution for the simplex vector
wfor reweighting task gradients:
w=g1U⊤
DU⊤−1
where ui=gi/gi,U=
u1−u2,···,u1−um
,andD=
g1−g2,···,g1−gm
Theaggregated
vector is then calculated as g=Pm
iwigi.
C Implementation Details
In this part, we provide implementation details regarding the empirical evaluation in the main paper
along with additional comparison experiments.
23

--- PAGE 24 ---
C.1 Baselines
In this subsection, we briefly introduce some of the comparative methods that appeared in the main
text:
•Linear scalarization (LS) minimizes the unweighted sum of task objectivesPm
iLi(θ).
•Scale-invariant (SI) aims toward obtaining similar convergent solutions even if losses are scaled
with different coefficients via minimizingPm
ilogLi(θ).
•Random loss weighting (RLW) [ 72] is a simple yet effective method for balancing task losses or
gradients by random weights.
•Dynamic Weight Average (DWA) [ 2] simply adjusts the weighting coefficients by taking the
rate of change of loss for each task into account.
•GradDrop [ 65] presents a probabilistic masking process that algorithmically eliminates all
gradient values having the opposite sign w.r.t a predefined direction.
C.2 Image classification
Network Architectures. For two datasets in this problem, Multi-MNIST and CelebA, we replicate
experiments from [ 8,25] by respectively using the Resnet18 (11M parameters) and Resnet50 (23M
parameters) [ 61] with the last output layer removed as the shared encoders and constructing linear
classifiers as the task-specific heads, i.e. 2 heads for Multi-MNIST and 40 for CelebA, respectively.
Training Details. We train the all the models under our proposed framework and baselines
using:
•Multi-MNIST: Adam optimizer [ 73] with a learning rate of 0.001for200epochs using a batch
size of 256. Images from the three datasets are resized to 36×36.
•CelebA: Batch-size of 256and images are resized to 64×64×3. Adam [ 73] is used again with
a learning rate of 0.0005, which is decayed by 0.85for every 10 epochs, our model is trained
for50epochs in total.
Regarding the hyperparameter for SAM [21], we use their adaptive version [74] where both ρsh
andρnsare set equally and extensively tuned from 0.005to5.
C.3 Scene understanding
Two datasets used in this problem are NYUv2 and CityScapes. Similar to [ 13], all images in the
NYUv2 dataset are resized to 288×384while all images in the CityScapes dataset are resized to
128×256to speed up the training process. We follow the exact protocol in [ 13] for implementation.
Specifically, SegNet [ 66] is adopted as the architecture for the backbone and Multi-Task Attention
Network MTAN [ 2] is applied on top of it. We train each method for 200 epochs using Adam
optimizer [ 73] with an initial learning rate of 1e−4and reduced it to 5e−5after 100 epochs. We
use a batch size of 2 for NYUv2 and 8 for CityScapes. The last 10 epochs are averaged to get the
final results, and all experiments are run with three random seeds.
24

--- PAGE 25 ---
D Additional Results
To further show the improvement of our proposed training framework over the conventional one, this
section provides additional comparison results in terms of qualitative results, predictive performance,
convergent behavior, loss landscape, model sharpness, and gradient norm. Please note that in
these experiments, we choose IMTL and F-IMTL as two examples for standard andflat-aware
gradient-based MTL training respectively. We also complete the ablation study in the main paper
by providing results on all three datasets in the Multi-MNIST dataset.
D.1 Image segmentation qualitative result
In this section, we provide qualitative results of our method of the CityScapes experiment. We
compare our proposed method against its main baseline by highlighting typical cases where our
method excels in generalization performance. Figure 6 shows some visual examples of segmentation
outputs on the test set. Note that in the CityScapes dataset, the “void" class is identified as unclear
and pixels labeled as void do not contribute to either objective or score [60].
(a) A training sample (after augmentation)
(b) Corresponding original image (before augmentation)
(c) Predictions on an unseen image
(d) Predictions on an unseen image
Figure 6: Semantic segmentation prediction comparison on CityScapes . From left to right are
input images, ground truth, and segmentation outputs from SegNet [ 66] using ERM training and
sharpness-aware training. Regions that are represented in gray color are ignored during training.
(Best viewed in color).
25

--- PAGE 26 ---
While there is only a small gap between the segmentation performance of IMTL and F-IMTL,
we found that a small area, which is the car hood and located at the bottom of images, is often
incorrectly classified. For example, in Figure 6, the third and fourth rows compare the prediction
of SegNet [ 66] with ERM training and with our proposed method. It can be seen that both of
them could not detect this area correctly, this is because this unclear “void" class did not appear
during training. Even worse, the currently employed data augmentation technique in the codebase
of Nash-MTL and other recent multi-task learning methods [ 13,10] consists of RandomCrop, which
often unintentionally excludes edge regions. For example, Figure 6a shows an example fed to the
neural network for training, which excludes the car hood and its logo, compared to the original
image (Figure 6b). Therefore, we can consider this "void" class as a novel class in this experiment,
since its appearance is ignored in both training and evaluation. Even though, in Figures 6c and 6d
our training method is still able to distinguish between this unknown area and other nearby known
classes, which empirically shows the robustness and generalization ability of our method over ERM.
D.2 Predictive performance
In this part, we provide experimental justification for an intriguing insight into the connection
between model sharpness and model calibration. Empirically, we found that when a model converges
to flatter minima, it tends to be more calibrated. We start by giving the formal definition of a
well-calibrated classification model and three metrics to measure the calibration of a model, then we
analyze our empirical results.
Consider a C-class classification problem with a test set of Nsamples given in the form (xi, yi)N
i=1
where yiis the true label for the sample xi. Model outputs the predicted probability for a given
sample xito fall into Cclasses, is given by
ˆp(xi) = [ˆp(y= 1|xi), . . . , ˆp(y=C|xi)].
ˆp(y=c|xi)is also the confidence of the model when assigning the sample xito class c. The predicted
label ˆyiis the class with the highest predicted value, ˆp(xi) :=max cˆp(y=c|xi). We refer to ˆp(xi)as
the confidence score of a sample xi.
Model calibration is a desideratum of modern deep neural networks, which indicates that the
predicted probability of a model should match its true probability. This means that the classification
network should be not only accurate but also confident about its prediction, i.e. being aware of
when it is likely to be incorrect. Formally stated, the perfect calibration [75] is:
P(ˆy=y|ˆp=q) =q,∀q∈[0,1]. (11)
Metric. The exact computation of Equation 11 is infeasible, thus we need to define some metrics
to evaluate how well-calibrated a model is.
•Brier score ↓(BS) [76] assesses the accuracy of a model’s predicted probability by taking into
account the absolute difference between its confidence for a sample to fall into a class and the
true label of that sample. Formally,
BS=1
NNX
i=1CX
c=1(ˆp(y=c|xi)−1[yi=c])2.
26

--- PAGE 27 ---
•Expected calibration error ↓(ECE) compares the predicted probability (or confidence) of a
model to its accuracy [ 77,75]. To compute this error, we first bin the confidence interval [0,1]
intoMequal bins, then categorize data samples into these bins according to their confidence
scores. We finally compute the absolute value of the difference between the average confidence
and the average accuracy within each bin, and report the average value over all bins as the
ECE. Specifically, let Bmdenote the set of indices of samples having their confidence scores
belonging to the mthbin. The average accuracy and the average confidence within this bin are:
acc(Bm) =1
|Bm|X
i∈Bm1[ ˆyi=yi],
conf(Bm) =1
|Bm|X
i∈Bmˆp(xi).
Then the ECE of the model is defined as:
ECE =MX
m=1|Bm|
N|acc(Bm)−conf(Bm)|.
In short, the lower ECE neural networks obtain, the more calibrated they are.
•Predictive entropy (PE) is a widely-used measure of uncertainty [ 78,79,80] via the predictive
probability of the model output. When encountering an unseen sample, a well-calibrated model
is expected to yield a high PE, representing its uncertainty in predicting out-of-domain (OOD)
data.
PE=1
CCX
c=1−ˆp(y=c|xi) log ˆp(y=c|xi).
Figures 7 and 8 plot the distribution of the model’s predicted entropy in the case of in-domain
and out-domain testing, respectively. We can see when considering the flatness of minima, the model
shows higher predictive entropy on both in-domain and out-of-domain, compared to ERM. This also
means that our model outputs high uncertainty prediction when it is exposed to a sample from a
different domain.
27

--- PAGE 28 ---
0.0 0.5 1.0 1.5 2.00.02.55.07.510.012.515.017.5Ours
ERM
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75051015202530Ours
ERM
0.0 0.5 1.0 1.5 2.00246810121416Ours
ERM
0.00 0.25 0.50 0.75 1.00 1.25 1.500510152025Ours
ERMFigure 7: Histograms of predictive entropy of ResNet18 [ 61] on in domain dataset, train and test on
MultiMNIST (left) and MultiFashion (right). We use the orange lines to denote ERM training while
blue lines indicate our proposed method.
0.0 0.5 1.0 1.5 2.00123456
Ours
ERM
0.0 0.5 1.0 1.5 2.00.00.51.01.52.02.53.03.5Ours
ERM
0.0 0.5 1.0 1.5 2.00.00.51.01.52.02.53.03.54.0Ours
ERM
0.0 0.5 1.0 1.5 2.0 2.501234Ours
ERM
Figure 8: Out of domain: model is trained on MultiMNIST, then tested on MultiFashion (left)
and vice versa (right). Models trained with ERM give over-confident predictions as their predictive
entropy concentrates around 0.
28

--- PAGE 29 ---
Here, we calculate the results for both tasks 1 and 2 as a whole and plot their ECE in Figure 9.
When we look at the in-domain prediction in more detail, our model still outperforms ERM in terms
of expected calibration error. We hypothesize that considering flat minima optimizer not only lowers
errors across tasks but also improves the predictive performance of the model.
0.00.20.40.60.81.0Expected Accuracy
ECE=4.47ERM
Gap
Accuracy
ECE=2.49Ours
Gap
Accuracy
0.00.20.40.60.81.0Expected Accuracy
ECE=11.72Gap
Accuracy
ECE=3.28Gap
Accuracy
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0Expected Accuracy
ECE=6.54Gap
Accuracy
0.0 0.2 0.4 0.6 0.8 1.0
ConfidenceECE=4.91Gap
Accuracy
Figure 9: The predictive performance (measured by the expected calibration error) of neural networks
has been enhanced by using our proposed training method (right column).
We also report the Brier score and ECE for each task in Table 6 and Table 7. As can be observed
29

--- PAGE 30 ---
Table 6: Brier score on Multi-Fashion, Multi-Fashion+MNIST and MultiMNIST datasets. We use
theboldfont to highlight the best results.
.Dataset Task Multi-Fashion Multi-Fashion+MNIST MultiMNIST
ERMTop left 0.237 0.055 0.082
Bottom right 0.254 0.217 0.106
Average 0.246 0.136 0.094
OursTop left 0.172 0.037 0.059
Bottom right 0.186 0.189 0.075
Average 0.179 0.113 0.067
from these tables, our method shows consistent improvement in the model calibration when both
scores decrease over all scenarios.
Table 7: Expected calibration error on Multi-Fashion, Multi-Fashion+MNIST and MultiMNIST
datasets. Here we set the number of bins equal to 10.
Dataset Task Multi-Fashion Multi-Fashion+MNIST MultiMNIST
ERMTop left 0.113 0.027 0.039
Bottom right 0.121 0.104 0.050
Average 0.117 0.066 0.045
OursTop left 0.034 0.015 0.022
Bottom right 0.032 0.083 0.028
Average 0.033 0.049 0.025
D.3 Effect of choosing perturbation radius ρ.
The experimental results analyzing the sensitivity of model w.r.t ρare given in Figure 10. We evenly
picked ρfrom 0to3.0to run F-CAGrad on three Multi-MNIST datasets.
0.0 0.5 1.0 1.5 2.0 2.5 3.0
8890929496Accuracy
MultiFashion
MultiMnist
MultiFashion+MNIST
Figure 10: Average accuracy when varying ρfrom 0to3.0(with error bar from three independent
runs).
We find that the average accuracy of each task is rather stable from ρ= 0.5, which means the
30

--- PAGE 31 ---
effect of different values of ρin a reasonably small range is similar. It can also easy to notice that
the improvement tends to saturate when ρ≥1.5.
D.4 Gradient conflict.
In the main paper, we measure the percentage of gradient conflict on the MultiFashion+MNIST
dataset. Here, we provide the full results on three different datasets. As can be seen from Figure 11,
there is about half of the mini-batches lead to the conflict between task 1 and task 2 when using
traditional training. Conversely, our proposed method significantly reduces such confliction (less
than 5%) via updating the parameter toward flat regions.
0 25 50 75 100 125 150 175 200
Epochs01020304050Gradient conflict (%)
0 25 50 75 100 125 150 175 200
Epochs0102030405060Ours ERM
0 25 50 75 100 125 150 175 200
Epochs01020304050
Figure 11: Task gradient conflict proportion of models trained with our proposed method and
ERM across MultiFashion, MultiFashion+MNIST and MultiMNIST datasets (columns).
D.5 Loss landscape
Thirdly, we provide additional visual comparisons of the loss landscapes trained with standard
training and with our framework across two tasks of three datasets of Multi-MNIST. As parts of the
obtained visualizations have been presented in the main paper, we provide the rest of them in this
subsection. The results in Figures 12 and 13 consistently show that our method obtains significantly
flatter minima on both two tasks, encouraging the model to generalize well.
Figure 12: Loss landscapes of task 1 and task 2 on MultiFashion
31

--- PAGE 32 ---
Figure 13: Loss landscapes of task 1 and task 2 on MultiFashion+MNIST
D.6 Training curves
Secondly, we compare the test accuracy of trained models under the two settings in Fig. 15. It can
be seen that from the early epochs ( 20-th epoch), the flat-based method outperforms the ERM-based
method on all tasks and datasets. . Although the ERM training model is overfitted after such a
long training, our model retains a high generalizability, as discussed throughout previous sections.
0 25 50 75 100 125 150 175 200
Epochs0.840.860.880.900.920.940.960.981.00Accuracy
0 25 50 75 100 125 150 175 2000.8250.8500.8750.9000.9250.9500.9751.000Ours ERM
0 25 50 75 100 125 150 175 2000.750.800.850.900.951.00
0 25 50 75 100 125 150 175 200
Epochs0.8250.8500.8750.9000.9250.9500.9751.000Accuracy
0 25 50 75 100 125 150 175 200
Epochs0.800.850.900.951.00
0 25 50 75 100 125 150 175 200
Epochs0.750.800.850.900.951.00
Figure 14: Train accuracy of models trained with our proposed method and ERM across 2 tasks
(rows) of MultiFashion, MultiFashion+MNIST and MultiMNIST datasets (columns).
Furthermore, we also plot the training accuracy curves across experiments in Figure 14 to
show that training accuracy scores of both ERM and our proposed method are similar and reach
≈100%from 50-th epoch, which illustrates that the improvement is associated with generalization
enhancement, not better training.
32

--- PAGE 33 ---
0 25 50 75 100 125 150 175 200
Epochs90919293949596Accuracy
0 25 50 75 100 125 150 175 2009092949698Ours ERM
0 25 50 75 100 125 150 175 2008182838485868788
0 25 50 75 100 125 150 175 2008889909192939495Accuracy
0 25 50 75 100 125 150 175 200
Epochs8283848586878889
0 25 50 75 100 125 150 175 200
Epochs80828486Figure 15: Test accuracy of models trained with our proposed method and ERM across 2 tasks
(rows) of MultiFashion, MultiFashion+MNIST and MultiMNIST datasets (columns).
D.7 Model sharpness
Fourthly, Figure 16 displays the evolution of ρ-sharpness of models along training epochs under
conventional loss function (ERM) and worst-case loss function (ours) on training sets of three
datasets from Multi-MNIST, with multiple values of ρ. We can clearly see that under our framework,
for both tasks, the model can guarantee uniformly low loss value in the ρ-ball neighborhood of
parameter across training process. In contrast, ERM suffers from sharp minima from certain epochs
when the model witnesses a large gap between the loss of worst-case perturbed model and current
model. This is the evidence for the benefit that our framework brings to gradient-based methods,
which is all tasks can concurrently find flat minima thus achieving better generalization.
D.8 Gradient norm
Finally, we demonstrate the gradient norm of the loss function w.r.t the worst-case perturbed
parameter of each task. On the implementation side, we calculate the magnitude of the flat gradient
gi,flatfor each task at different values of ρin Figure 17. As analyzed by equation (6) from the main
paper, following the negative direction of gi,SAM
shwill lower the L2norm of the gradient, which
orients the model towards flat regions. This is empirically verified in Figure 17. In contrast, as the
number of epochs increases, gradnorm of the model trained with ERM tends to increase or fluctuate
around a value higher than that of model trained with SAM.
33

--- PAGE 34 ---
0 25 50 75 100 125 150 175 2000.0000.0020.0040.0060.0080.0100.012Sharpness
0 25 50 75 100 125 150 175 2000.00000.00250.00500.00750.01000.01250.01500.0175
Ours ERM
0 25 50 75 100 125 150 175 2000.0000.0020.0040.0060.0080.010
0 25 50 75 100 125 150 175 200
Epochs0.0000.0020.0040.0060.0080.0100.012Sharpness
0 25 50 75 100 125 150 175 200
Epochs0.0000.0050.0100.0150.0200.025
0 25 50 75 100 125 150 175 200
Epochs0.0000.0020.0040.0060.0080.010
(a)ρ= 0.005
0 25 50 75 100 125 150 175 2000.0000.0250.0500.0750.1000.1250.150Sharpness
0 25 50 75 100 125 150 175 20001234
 Ours ERM
0 25 50 75 100 125 150 175 2000.00.10.20.30.40.50.60.7
0 25 50 75 100 125 150 175 200
Epochs0.0000.0250.0500.0750.1000.1250.150Sharpness
0 25 50 75 100 125 150 175 200
Epochs0.00.20.40.60.81.01.2
0 25 50 75 100 125 150 175 200
Epochs0.000.050.100.150.20
(b)ρ= 0.05
0 25 50 75 100 125 150 175 20002468Sharpness
0 25 50 75 100 125 150 175 200020406080
Ours ERM
0 25 50 75 100 125 150 175 200051015202530
0 25 50 75 100 125 150 175 200
Epochs01234567Sharpness
0 25 50 75 100 125 150 175 200
Epochs0102030405060
0 25 50 75 100 125 150 175 200
Epochs05101520
(c)ρ= 0.5
Figure 16: Sharpness of models trained with our proposed method and ERM with different values
ofρ. For each ρ, the top and bottom row respectively represents the first and second task, and each
column respectively represents each dataset in Multi-MNIST: from left to right are MultiFashion,
MultiFashion+MNIST, MultiMNIST.
34

--- PAGE 35 ---
0 25 50 75 100 125 150 175 2000.000.050.100.150.20Gradient norm
0 25 50 75 100 125 150 175 2000.000.050.100.150.200.25
 Ours ERM
0 25 50 75 100 125 150 175 2000.000.020.040.060.08
0 25 50 75 100 125 150 175 200
Epochs0.000.050.100.150.200.25Gradient norm
0 25 50 75 100 125 150 175 200
Epochs0.00.10.20.30.4
0 25 50 75 100 125 150 175 200
Epochs0.000.020.040.060.080.100.12
(a)ρ= 0.005
0 25 50 75 100 125 150 175 2000.00.20.40.60.81.0Gradient norm
0 25 50 75 100 125 150 175 2000.000.250.500.751.001.251.501.75
 Ours ERM
0 25 50 75 100 125 150 175 2000.00.20.40.60.8
0 25 50 75 100 125 150 175 200
Epochs0.00.20.40.60.81.0Gradient norm
0 25 50 75 100 125 150 175 200
Epochs0.00.51.01.52.02.5
0 25 50 75 100 125 150 175 200
Epochs0.00.10.20.30.40.50.60.7
(b)ρ= 0.05
0 25 50 75 100 125 150 175 20001234567Gradient norm
0 25 50 75 100 125 150 175 20002468101214
Ours ERM
0 25 50 75 100 125 150 175 20001234567
0 25 50 75 100 125 150 175 200
Epochs01234567Gradient norm
0 25 50 75 100 125 150 175 200
Epochs05101520
0 25 50 75 100 125 150 175 200
Epochs01234567
(c)ρ= 0.5
Figure 17: Gradient magnitude at the worst-case perturbations of models trained with our
proposed method and ERM with different values of ρ. For each ρ, the top and bottom row
respectively represents the first and second task, and each column respectively represents each
dataset in Multi-MNIST: from left to right are MultiFashion, MultiFashion+MNIST, MultiMNIST.
35
