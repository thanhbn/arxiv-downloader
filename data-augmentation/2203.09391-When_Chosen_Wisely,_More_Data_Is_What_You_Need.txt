# 2203.09391.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/data-augmentation/2203.09391.pdf
# File size: 1157241 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
When Chosen Wisely, More Data Is What You Need:
A Universal Sample-Efﬁcient Strategy For Data Augmentation
Ehsan Kamallooy
Univeristy of Alberta
kamalloo@ualberta.caMehdi Rezagholizadeh
Huawei Noah’s Ark Lab
mehdi.rezagholizadeh@huawei.com
Ali Ghodsi
University of Waterloo
ali.ghodsi@uwaterloo.ca
Abstract
Data Augmentation (DA) is known to improve
the generalizability of deep neural networks.
Most existing DA techniques naively add a
certain number of augmented samples without
considering the quality and the added compu-
tational cost of these samples. To tackle this
problem, a common strategy, adopted by sev-
eral state-of-the-art DA methods, is to adap-
tively generate or re-weight augmented sam-
ples with respect to the task objective dur-
ing training. However, these adaptive DA
methods: (1) are computationally expensive
and not sample-efﬁcient, and (2) are designed
merely for a speciﬁc setting. In this work, we
present a universal DA technique, called Glit-
ter, to overcome both issues. Glitter can be
plugged into any DA method, making train-
ing sample-efﬁcient without sacriﬁcing perfor-
mance. From a pre-generated pool of aug-
mented samples, Glitter adaptively selects a
subset of worst-case samples with maximal
loss, analogous to adversarial DA. Without
altering the training strategy, the task objec-
tive can be optimized on the selected sub-
set. Our thorough experiments on the GLUE
benchmark, SQuAD, and HellaSwag in three
widely used training setups including consis-
tency training, self-distillation and knowledge
distillation reveal that Glitter is substantially
faster to train and achieves a competitive per-
formance, compared to strong baselines.1
1 Introduction
The undeniable importance of data in deep learn-
ing (Sambasivan et al., 2021; Rogers, 2021) and the
costly process of data annotation has propelled re-
searchers into leveraging Data Augmentation (DA)
in a broad range of applications from computer
vision (Cubuk et al., 2019; Wang et al., 2020) to
Equal Contribution.
yWork done while interning at Huawei Noah’s Ark Lab.
1Our code is available at https://github.com/
huawei-noah/KD-NLP/tree/main/Glitter .natural language processing (NLP) including ma-
chine translation (Sennrich et al., 2016; Shen et al.,
2020), language understanding (Shen et al., 2020;
Qu et al., 2021; Du et al., 2021; Kamalloo et al.,
2021), and question answering (Alberti et al., 2019;
Longpre et al., 2019; Shakeri et al., 2020). DA
is shown to be effective in improving generaliza-
tion of deep neural networks (DeVries and Taylor,
2017; Xie et al., 2020) and in increasing the num-
ber of training samples especially in low resource
data regimes (Sennrich et al., 2016; Zhang et al.,
2018). Nonetheless, in NLP, the discrete nature of
text poses additional complexity to DA as gener-
ating semantically viable text from another text is
challenging (Feng et al., 2021).
DA methods can be broadly categorized into
task-aware and task-agnostic methods. Task-
agnostic DA methods essentially generate aug-
mented text regardless of the task at hand and often
do not warrant additional training or ﬁne-tuning.
They can be based on some hand-crafted heuristics
(Zhang et al., 2015; Wei and Zou, 2019), back-
translation (Sennrich et al., 2016; Edunov et al.,
2018), or token replacement from a pre-trained lan-
guage model (Kobayashi, 2018; Wu et al., 2019; Ng
et al., 2020). Even though deploying task-agnostic
methods is straightforward, these methods do not
take into account any task-speciﬁc information, and
thus, their performance is usually limited. On the
other hand, task-aware DA methods are capable
of generating augmented samples, conditioned on
the downstream task objective (Hu et al., 2019;
Xie et al., 2020; Rashid et al., 2021). These meth-
ods adapt augmented examples speciﬁcally for a
task in that they construct augmented examples,
sometimes partly, during training. Despite their ad-
vantages, they often incur additional training costs,
resulting in a prohibitively slow and a computation-
ally expensive training.
In general, the central problems surrounding DA
techniques in NLP can be summarized as follows:arXiv:2203.09391v1  [cs.LG]  17 Mar 2022

--- PAGE 2 ---
First, DA methods are mostly not sample-efﬁcient
in that they add arbitrary number of augmented
samples to the training data and naively incorpo-
rate all of them into training without investigat-
ing how many of augmented samples are actually
needed. Second, although more effective, task-
aware methods are notoriously time-consuming to
train. This is especially problematic in large-scale
datasets such as SQuAD (Rajpurkar et al., 2016)
and MNLI (Williams et al., 2018). Third, most
DA methods are not universal as they work solely
with a particular setup—e.g., training a single-
network (Xie et al., 2020), or training in teacher-
student settings (Rashid et al., 2021). Overall, the
importance of both sample efﬁciency and training
efﬁciency for DA has been often overlooked.
Motivated by the above problems, in this work,
we introduce a universal DA method, Glitter
2,
which can be plugged into any DA method to make
them sample-efﬁcient, and task-aware without sac-
riﬁcing performance. Speciﬁcally, given a pool
of augmented samples that are generated ofﬂine,
our proposed method follows a minimax approach
(Farnia and Tse, 2016) to select a small subset with
maximal expected loss ( maximization step ) during
training. Without any further adjustments to the
training algorithm, the task objective can be opti-
mized for this selected subset ( minimization step ).
Our key contributions in this paper can be sum-
marized as follows:
1.Glitter is a universal method which can be
effortlessly applied to any DA method to en-
force sample efﬁciency while maintaining (or
even boosting) their performance.
2.We devise strategies to adapt Glitter for a
variety of widely used training setups includ-
ing single-network, consistency training, self-
distillation and knowledge distillation.
3.Through our empirical evaluations, we show
that Glitter achieves superior performance
over state-of-the-art DA methods on GLUE,
SQuAD, and HellaSwag, while signiﬁcantly
speeding up the training.
2 Related Work
2.1 Task-agnostic DA in NLP
Contextual augmentation techniques (Kobayashi,
2018; Wu et al., 2019) use pre-trained language
2Inspired by “ All that is gold does not glitter ” —J.R.R.
Tolkien, The Fellowship of the Ring.models for DA. Kobayashi (2018) propose bidi-
rectional LSTM language models for word substi-
tution conditioned on the label of their input text.
SSMBA (Ng et al., 2020) and TinyBERT (Jiao
et al., 2020) perturb the input by masking some of
the tokens, and then, sample tokens from a BERT
model to replace the masked tokens and generate
augmented samples. Back-Translation (Sennrich
et al., 2016) augments data using two consecutive
translation models: the ﬁrst model to translate the
input into an arbitrary target language; then, a sec-
ond model to translate the result back into its orig-
inal language. Mixed-up (Guo et al., 2019) gen-
erates augmented samples based on interpolating
word embedding and sentence embedding vectors.
Shen et al. (2020) introduce a set of cut-off tech-
niques that zero out contiguous spans of the em-
bedding matrix at token level, feature level and
span level. EDA (Wei and Zou, 2019) consists of
simple word-level operations including synonym
replacement, random deleting, random insertion
and random swapping.
2.2 Task-aware DA in NLP
One approach to leverage task-speciﬁc informa-
tion is to assign different weights to augmented
samples based on their individual impacts on the
model (Yi et al., 2021). Although effective, the
re-weighting mechanism largely ignores sample
efﬁciency. Wu et al. (2019) introduce a mask-and-
reconstruct approach, namely c-BERT, that ﬁne-
tune a pre-trained BERT model to predict label-
compatible tokens. CoDA (Qu et al., 2021) com-
bines various label-preserving transformations with
adversarial training jointly with a contrastive regu-
larization objective. Unsupervised DA (UDA; Xie
et al. 2020) uses off-the-shelf DA methods and
adds an auxiliary consistency loss to the training
objective. However, UDA is not sample-efﬁcient
and it is designed only for a single-network setup;
how to deploy it in other training scenarios such as
knowledge distillation is not clear. Hu et al. (2019)
propose a reinforcement learning-based technique
where the reward function is deﬁned based on
whether generated augmented samples are label-
preserving or not.
2.3 DA for KD
KD (Bucilu ˇa et al., 2006; Hinton et al., 2015), ini-
tially proposed as a model compression technique,
aims at transferring the knowledge of an already
trained model, called teacher , to a smaller or a

--- PAGE 3 ---
same-size student model. Several studies found
that DA can signiﬁcantly boost KD’s performance
in NLP. TinyBERT (Jiao et al., 2020) uses a task-
agnostic DA technique for its task-speciﬁc ﬁne-
tuning. Kamalloo et al. (2021) and Rashid et al.
(2021) showed that DA can also be tailored for
KD. In particular, MATE-KD (Rashid et al., 2021)
tunes a separate masked language model in order to
generate augmented samples with maximum diver-
gence. Kamalloo et al. (2021) and Du et al. (2021)
employkNN retrieval to fetch augmented samples
from a massive sentence bank.
Glitter differs from previous work in that it si-
multaneously focuses on sample efﬁciency, and
universality such that it can be freely used in any
training setting.
3 Methodology
In this section, we introduce our task-aware DA
method, Glitter
 , that aims at using an efﬁcient
number of augmented samples without sacriﬁcing
performance. Our proposed strategy is agnostic
to DA methods; it can be seamlessly plugged into
any DA method with any training setting to enforce
sample efﬁciency.
Existing learning-based DA methods train a sep-
arate DA model and adapt its output for a particular
objective function that is entirely task-dependent:
 min
`DA(M(
(x;);))
x0= 
(x;)(1)
where`DA()is a loss function, geared towards the
objective of the task, 
(;)is the DA model with
trainable parameters , andM(;)refers to the
original model, parameterized by .
In contrast to learning-based DA, we propose to
generate many augmented candidates using any ar-
bitrary DA method prior training, and adaptively se-
lect most suitable candidates during training. This
procedure does not introduce additional trainable
parameters into training, and more importantly, is
capable of automatically ignoring unnecessary aug-
mented examples. Let (xi;yi)N
i=12f(X;Y)grep-
resent training data such that a pair xi2X and
yi2Y are an input example and its corresponding
label. Suppose a pool of Kaugmented examples,
X0(i) =fx0
k(i)gK
k=1, are sampled from some DA
model for each training example (xi;yi)2(X;Y).
Note that Glitter imposes no restrictions on how to
augment training data; augmented samples can be
generated via a single or even multiple DA models.Sample Selection. Given a pool of augmented
samples, our approach is to adaptively select the
best candidates according to particular deﬁned cri-
teria. Inspired by the minimax approach (Farnia
and Tse, 2016; V olpi et al., 2018), our selection
mechanism is based on ﬁnding top- k1(out ofK)
worst-case augmented samples from the X0set.
Minimizing the main model loss function on these
worst-case augmented samples will help improv-
ing generalization of the model (V olpi et al., 2018).
In order to rank augmented samples, we evaluate
X0(i)based on a distance function with respect
to the corresponding original training sample, xi,
within the model’s latent space:
X0(i) topk1
`eval 
M(xi;);M(X0(i);)
X0(i) =fx0
j(i)gk1
j=1X0(i)
(2)
where topk1()denotes returns top- k1indices based
on the scores returned by `eval,X0(i)is the set of
k1selected augmented samples for xi;`eval()is
the evaluation loss which is determined via the task
objective.
Updating the Model Parameters. After obtain-
ing the top-k1augmented samples, we group them
with the original training samples, fxig[X0(i),
and subsequently, update the model parameters
only based on this selected set of augmented sam-
ples on the original loss:
L() =NX
i=1`task
M(xi;);M(X0(i););yi
t t 1 r(L())jt 1
(3)
whereNis the number of training samples, is
the learning rate, and `task()is the ﬁnal task loss—
e.g., cross entropy (ce) for classiﬁcation—that is
computed over both original data and selected aug-
mented data. In the remainder of this section, we
discuss how Glitter can be applied to popular train-
ing settings including general DA for single net-
works, and DA for teacher-student (KD) setups.
Note that Glitter is not restricted to these settings
and may be adapted for other settings such as DAIR
(Huang et al., 2022).
3.1 General DA for Single Networks
We consider three potential setups for the single
network scenario: (1) General single network, (2)

--- PAGE 4 ---
Figure 1: Illustration of Glitter
 (from left to right): ﬁrst, generating augmented samples from different DA
techniques; second, forming a pool of samples X0(i); third, evaluating the augmented samples using the `eval()
loss; fourth, ﬁltering the top- k1samples based on their corresponding `eval(); ﬁfth, updating the parameters of the
model by minimizing the task loss `task(:).
Self-distillation, and (3) Consistency training.
General Single Network. In this setup, aug-
mented samples are exploited in a semi-supervised
manner where we can evaluate them based on the
divergence of their predicted output M(x0
k(i);) =
p(yjx0
k(i);)from the ground-truth label or the pre-
diction of the original corresponding training sam-
pleM(xi;) =p(yjxi;)using the cross entropy
loss,`ce:
`eval=`ce 
yi;M(x0
k(i);)
or
`eval=`ce 
M(xi;);M(x0
k(i);)
:(4)
The cross entropy criterion is not the only option
here. Other choices for `evalinclude (but not limited
to) focal loss (Lin et al., 2017), and tilted loss (Li
et al., 2021).
For the ﬁnal task loss, `taskwe can deploy a stan-
dard cross entropy loss over both training samples
and their corresponding selected augmented sam-
ples:
`task=`ce 
yi;M(xi;)
+
1
k1X
x2X0(i)`ce 
yi;M(x;)
:(5)
Consistency Training (CT; Xie et al. 2020). In
this conﬁguration, we can employ the same `eval
introduced in Eq. (4). As a result, our method nat-
urally selects top- k1most inconsistent augmented
samples for each training sample. Then, the net-
work is optimized to make predictions for input
augmented samples that are consistent with pre-
dictions of their corresponding original trainingsamples:
`CT
task=`ce 
yi;M(xi;t)
+
1
k1X
x2X0(i)`ce 
M(xi;t 1);M(x;t)
:(6)
As stated by Xie et al. (2020), the second term
in Eq. (6)leverages the previous prediction of the
network for each training example.
Self-Distillation (Self-KD). In Self-KD, we ﬁrst
train a model, and then, use it ( M(;)) as a teacher
to train an identical model but initialized from
scratch using KD (Furlanello et al., 2018). How to
adjust`evaland`taskis detailed in §3.2.
3.2 DA for Teacher-Student (KD)
In this setup, we have a teacher model, T(; )with
parameters that is already trained on the training
data, along with a student model, M(;), which we
aim to train. The selection criterion for augmented
samples is to maximize divergence between the
teacher and the student:
`KD
eval=`KL
T 
x0
k(i); 
;M 
x0
k(i);
(7)
where`KLrefers to the KL divergence. After se-
lecting the maximum divergence augmented sam-
ples, then we calculate the KD loss as following:
`KD
task=`ce 
yi;M(xi;)
+ (1 )
1
k1+ 1X
x2fxig[X0(i)`KL 
T(x; );M(x;)
(8)
whereis a hyperparameter.

--- PAGE 5 ---
4 Experiments
4.1 Setup
To incorporate unlabelled augmented data into
training, we adopt CT (Xie et al., 2020) and KD
(Hinton et al., 2015). To this end, we conduct ex-
periments under two settings:
Standalone where we train a single model on
the augmented data. In this setting, we seek to an-
swer two questions: (1) How much is DA capable
of improving the model generalization? (2) Does
sample efﬁciency of Glitter hurt performance? For
this purpose, we ﬁne-tune RoBERTa base(Liu et al.,
2019) using CT and Self-KD on augmented data.
Distilled where we distill DistilRoBERTa (Sanh
et al., 2019) (student) from RoBERTa Large (Liu
et al., 2019) (teacher) using the augmented data.
Note that the teacher is already trained on the
original data and DA comes into play only dur-
ing distilling the student model. Our goal here is
to investigate whether DA is an effective means in
knowledge transfer to curb the capacity gap (Cho
and Hariharan, 2019) between a large model and a
small one.
In both settings, we take the best performing
model on the development set and evaluate it on
the test set (depicted by Test). Additionally, for
the standalone model setting, we also report results
on the development set when models are trained
only for 5 epochs (depicted by Dev), similar to
CoDA (Qu et al., 2021), to make a comparison
with baselines. Our Devresults are an average of
10 runs with different seeds. The implementation
details and hyperparameters are provided in §A.
4.1.1 DA Methods
We leverage three widely used textual augmenta-
tion methods:
1.EDA (Wei and Zou, 2019)3: We randomly
replace 5% of the tokens with their synonyms
and randomly delete up to 10%.
2.Back-Translation (BT; Sennrich et al.
2016) : We use fairseq (Ott et al., 2019) to
translate sentences into German and then back
into English. We do nucleus sampling (Holtz-
man et al., 2020) with p= 0:9for both trans-
lations. We ﬁnd that p= 0:6works better on
sentiment classiﬁcation.
3https://github.com/makcedward/nlpaug3.Mask-and-Reconstruct (MR; Ng et al.
2020) : We randomly mask 15% of the tokens
and construct a new sentence by sampling
from a pre-trained BERT Large for masked to-
kens. We adopt top- ksampling with k= 20
to select new tokens. For MNLI, we obtain
better results with top-10 sampling.
For each augmentation method, we generate 12
augmented examples per training instance for all
datasets, except for large datasets—i.e., MNLI,
QQP, and SQuAD—where the number of aug-
mented examples are 8 per train example.
4.1.2 Baselines
Because the two environments—i.e., standalone
and distilled—are different in nature, we compare
Glitter with different baselines for each environ-
ment. For both, Vanilla-DA that takes all aug-
mented data into account without reservation is
the ﬁrst baseline.
The baselines for the standalone setting are:
CoDA (Qu et al., 2021), MMEL (Yi et al., 2021),
and HiddenCut (Chen et al., 2021). And for dis-
tilled, we consider MATE-KD (Rashid et al., 2021).
4.2 GLUE
The GLUE benchmark (Wang et al., 2019) is a
well-known suite of nine4tasks that aim at evalu-
ating natural language understanding models. We
present test results in the distilled mode in Table 1.
Glitter consistently outperforms Vanilla-DA, while
it is faster to train. Speciﬁcally, Glitter achieves
parity with Vanilla-DA for EDA in terms of the
overall average score, while scoring +0.2% and
+0.4% higher for BT and MR, respectively. We ob-
serve that only in few cases Vanilla-DA negligibly
outperforms Glitter—e.g., on MRPC, and STS-B
for BT. Nonetheless, Glitter 8x/1x trains 50% faster
than Vanilla-DA 8xon average, and 30% faster for
8x/2x . Also, Glitter surpasses MATE-KD by +0.2%
in the overall score. Unlike Glitter, MATE-KD in-
troduces additional parameters to the model during
training and it trains drastically slower because it
generates augmented examples on-the-ﬂy. More-
over, Table 1 illustrates that MR yields the best
test results across the three DA methods except for
SST where BT leads to better results. Based on this
observation, we report results on MR augmented
4We excluded WNLI since our DA methods are not de-
signed for this task.

--- PAGE 6 ---
MethodCoLA SST MRPC STS-B QQP MNLI-m/mm QNLI RTEAvg.Mcc Acc Acc/F 1 P/S Acc/F 1 Acc Acc Acc
RoB Large(teacher) 63.8 96.8 90.6 92.4 81.5 90.3/89.8 94.8 88.3 87.3
BERT Large|60.5 94.9 87.4 87.1 80.7 86.7/85.9 92.7 70.1 82.5
DistilRoB 55.2 93.9 85.9 86.0 80.3 84.0/83.1 90.6 73.6 81.1
KD 54.9 94.0 86.8 87.3 80.5 85.1/83.7 91.9 73.5 81.7
Task-Aware DA
MATE-KD|56.0 94.9 90.2 88.0 81.2 85.5/84.8 92.1 75.0 82.8
EDA (Wei and Zou, 2019)
Vanilla-DA (8x) 55.5 94.8 87.6 86.1 80.7 85.3/84.7 92.0 72.8 81.8
Glitter
 54.5 95.1 87.5 86.5 80.4 85.4/84.8 92.1 73.2 81.8
8x/2x 8x/1x 8x/2x 8x/2x 8x/2x 8x/2x 8x/2x 8x/1x
Back-Translation
Vanilla-DA (8x) 53.4 95.1 88.5 87.5 80.9 85.9/ 85.9 92.2 73.5 82.1
Glitter
 54.9 95.1 88.4 87.3 80.9 86.2 /85.3 92.2 73.7 82.3
8x/2x 8x/1x 8x/1x 8x/2x 8x/2x 8x/2x 8x/2x 8x/2x
Mask-and-reconstruct
Vanilla-DA (8x) 58.8 94.5 88.7 87.0 80.9 85.8/84.9 91.8 74.0 82.6
Glitter
 59.2 95.1 89.2 87.6 81.0 86.6/84.8 92.4 74.1 83.0
8x/1x 8x/1x 8x/2x 8x/1x 8x/2x 8x/2x 8x/2x 8x/2x
Table 1: Test results of the distilled experiment on GLUE. (|) denotes results are taken verbatim from: BERT Large
(Devlin et al., 2019), and MATE-KD (Rashid et al., 2021). Bold and underlined numbers indicate the best and the
second best results across the DA methods.
MethodCoLA SST MRPC STS-B QQP MNLI-m QNLI RTEAvg.Mcc Acc Acc/F 1 P/S Acc/F 1 Acc Acc Acc
RoBERTa 61.9 95.4 88.6 89.3 80.4 87.6 93.0 81.6 84.7
Self-KD 61.7 95.7 89.0 89.0 80.8 88.3 93.0 81.7 84.9
+ Vanilla-DA 61.5 96.1 88.9 89.7 81.0 88.0 92.9 81.1 84.9
8x 8x 8x 8x 8x 8x 8x 12x
+ Glitter
 62.5 96.0 89.8 89.5 81.1 88.1 93.5 82.3 85.4
8x/1x 8x/2x 8x/2x 8x/2x 8x/2x 8x/2x 8x/2x 12x/1x
CT + Vanilla-DA 59.4 95.6 89.0 85.8 80.3 82.5 92.0 80.2 83.1
8x 8x 8x 10x 8x 8x 8x 10x
CT + Glitter
 62.7 95.8 89.2 87.9 80.9 84.1 92.9 81.8 84.4
8x/1x 8x/1x 8x/1x 10x/1x 8x/2x 8x/2x 8x/2x 10x/1x
Table 2: Test result of the standalone experiments on GLUE using RoBERTa base.
data for all GLUE datasets except for SST in the
remainder of our experiments.
For the standalone mode, Tables 2 and 3 present
the results on test and dev, respectively. Similar to
distilled, Glitter outperforms Vanilla-DA by +0.5%
for both self-KD and CT. Self-KD yields better re-
sults than CT on all GLUE tasks except CoLA. CT
falls short on most GLUE tasks, compared to no
DA results—i.e., top-2 rows in Table 2. This is why,
we only evaluated Glitter with self-KD on the dev
data. Glitter achieves superior performance gains,
compared to all three baselines on all datasets ex-
cept QNLI. The key advantage of Glitter is that the
training procedure remains intact.
4.2.1 Out-of-Domain Generalization
We also evaluate Glitter on OOD datasets. To this
end, we test our models, already trained on GLUE
tasks, on OOD datasets whose data distribution
differs from the original data. In particular, hereare our selected OOD datasets:
•SST: IMDb (Maas et al., 2011), IMDb-
Cont. (Gardner et al., 2020), and IMDb-
CAD (Kaushik et al., 2020), as done in
Chen et al. (2021). Although both SST and
IMDb datasets are collected on movie reviews,
IMDb reviews tend to be substantially longer
than SST sentences.
•STS-B: SICK (Marelli et al., 2014), a seman-
tic relatedness dataset, created from image
and video captions. SICK and STS-B are col-
lected on roughly identical domains, but from
different sources.
•QQP: PAWS QQP(Zhang et al., 2019), anal-
ogous to Chen et al. (2021), and MQP (Mc-
Creery et al., 2020), a medical question simi-
larity dataset.

--- PAGE 7 ---
MethodSST MRPC MNLI-m QNLI RTE IMDb-Con. A-NLI HANS
Acc F 1 Acc Acc Acc Acc Acc Acc
RoB94.8 90.2 87.6 92.8 78.7 - - -
CoDA95.3 91.7 88.1 93.6 82.0 - - -
HiddenCut95.8 92.0 88.2 93.7 83.4 87.8 32.8 71.2
MMELy94.60.8 91.90.4 88.10.1 93.20.1 85.31.0 90.50.7 31.40.6 74.50.6
RoBy94.30.1 91.60.5 87.70.1 92.80.2 84.50.8 90.00.4 30.80.9 73.60.7
Self-KD 94.3 0.2 91.50.3 87.90.1 92.90.2 84.00.6 90.30.5 30.90.4 73.50.7
+ Vanilla-DA 95.4 0.5 92.00.3 88.20.1 93.40.1 84.40.7 90.20.4 31.30.5 73.90.4
+ Glitter
 95.70.2 92.20.5 88.20.1 93.40.1 85.60.7 90.60.2 31.80.4 74.60.3
Table 3: Dev results of the standalone experiment on GLUE using RoBERTa base. () denotes results are taken
verbatim from: RoB and CoDA (Qu et al., 2021), and HiddenCut (Chen et al., 2021). (y) indicates the results are
obtained from our implementation of MMEL (Yi et al., 2021).
•MNLI: SciTail (Khot et al., 2018), collected
from school-level science questions, and sim-
ilar to Chen et al. (2021), A-NLI (Nie et al.,
2020), and HANS (McCoy et al., 2019).
• RTE: HANS (McCoy et al., 2019).
Table 10 in §B.1 showcases the OOD results for
the distilled mode. Glitter outperforms Vanilla-DA
in most cases, and is on par with it for nearly the
rest. The only exceptions are IMDb-Cont., MQP,
and PAWS QQPwhere Vanilla-DA outperforms Glit-
ter by almost 1% on average. Also, all models
do not generalize well to PAWS QQPand A-NLI
because their performance is below a majority-
class performance. Moreover, a ﬁne-tuned Distil-
RoBERTa achieves the best OOD performance on
HANS, highlighting that DA is not actually helpful
for OOD accuracy on HANS.
Table 3 (the right side) reports the OOD results
for standalone models. The complete results are
presented in §B.2—i.e., Table 11 on test and Ta-
ble 12 on dev. Glitter overwhelmingly outperforms
all the baselines with a few exceptions. In the dev
results, the ﬁne-tuned model with no DA achieves
the best OOD generalization on IMDb, and SciTail,
while HiddenCut scores the highest on A-NLI with
a 1% margin. Similarly, in the test results, Glitter
trails Self-KD with no DA on IMDb, IMDb-CAD,
and SciTail.
4.3 HellaSwag
HellaSwag (Zellers et al., 2019) is a dataset for situ-
ated commonsense reasoning that involves picking
the best ending given a context. We augment con-
texts in HellaSwag using only BT to ensure that
the choices remain meaningful for the augmented
contexts. Because our standalone results have been
consistent with the distilled results, we report our
results only in the distilled mode. According to ourMethodSQuAD HellaSwag
EM/F 1 Acc
RoB Large 88.9/94.6 85.2
DistilRoB 80.9/87.9 42.9
KD 81.1/88.2 42.5
+ Vanilla-DA (8x) 81.8/89.1 41.8
+ Glitter
 (8x/2x) 83.6/90.3 44.1
Table 4: Dev results of the distilled experiment on two
downstream tasks.
results demonstrated in Table 4, Glitter comfortably
surpasses Vanilla-DA by a +2.3% margin.
4.4 SQuAD
SQuAD (Rajpurkar et al., 2016) is a crowd-sourced
reading comprehension benchmark that consists of
more than 100K questions, derived from Wikipedia
passages. The task objective is to extract an an-
swer span from a given question/passage pair. We
augment questions in SQuAD v1.1 using only BT
to ensure that the answer can still be found in the
given passage for the augmented questions. Anal-
ogous to HellaSwag, we report our results only in
the distilled mode. As shown in Table 4, Glitter
outperformas Vanilla-DA by +1.8% in exact-match
accuracy on the development set.
We also evaluate our trained models under dis-
tribution shift by testing them on QA datasets
from four different domains: Wikipedia, New
York Times, Reddit, and Amazon product reviews
(Miller et al., 2020). The OOD results are pre-
sented in Table 5. Glitter is consistently superior to
Vanilla-DA in all four domains.
5 Ablation Study and Discussion
In this section, we aim to answer the following
questions:

--- PAGE 8 ---
MethodWiki NYT Reddit Amzn
EM EM EM EM
RoB Large 84.4 85.9 76.6 74.4
DistilRoB 76.6 78.1 66.2 62.9
KD 76.5 78.7 65.7 63.0
+ Vanilla-DA 77.3 79.0 65.9 63.3
+ Glitter
 79.3 80.7 68.1 64.7
Table 5: OOD results for models trained on SQuAD
and tested on QA datasets from four different domains
(Miller et al., 2020).
•How does training time of Glitter compare
against Vanilla-DA?
•Instead of adaptively selecting augmented
data during training, can we pre-process them
to dispense with unnecessary examples prior
to training?
•How many augmented examples are required
for Glitter to work?
•Is our selection strategy based on sorting of
`evalin Glitter important?
For this purpose, we conduct a detailed analy-
sis on 4 GLUE tasks—i.e., SST, MRPC, QNLI,
and RTE. We trained models based on Vanilla-DA
and Glitter using Self-KD and tested them on the
development set (the dev setting).
Runtime Analysis. Throughout our experiments
in §4, we compare Glitter with Vanilla-DA when
number of augmentations are similar for both
methods—i.e., 8x. A natural question is: how
would both DA methods behave with fewer aug-
mented data? To this end, we vary augmentation
size from 1xto8xand train different Vanilla-DA
models on each augmented dataset. We measure
average the training time per epoch for all models.
Figure 2 illustrates the dev accuracy as the train-
ing time increases. The training speed of Glitter
8x/2x is slightly faster than Vanilla-DA 6xon SST,
MRPC, and QNLI and for Glitter 8x/1x , is faster
than Vanilla-DA 4xon RTE. Glitter is superior of
the two on all datasets.
Effect of Pre-processing Augmented Data. We
conjecture that Glitter does not need any data en-
gineering on augmented examples to obtain prefer-
able performance gains. However, Vanilla-DA
may require some pre-processing by weeding out
potentially noisy data to become more effective.
To investigate this, we exploit two pre-processingMethodSST MRPC QNLI RTE
Acc F 1 Acc Acc
Vanilla-DA 95.1 92.2 93.3 84.8
= 0:7 95.1 92.5 93.4 84.8
= 0:9 95.0 92.2 93.3 83.8
LP 94.8 92.4 93.3 84.8
Glitter
 95.8 92.8 93.4 85.9
= 0:7 95.0 91.5 93.5 85.2
= 0:9 95.0 92.5 93.3 84.1
LP 95.1 92.2 93.5 85.9
Table 6: Dev results of self-KD exhibiting the effective-
ness of different pre-processing techniques to ﬁlter aug-
mented examples on 4 GLUE tasks. and LP depict
a minimum conﬁdence threshold, and label preserving,
respectively.
techniques: (1) Conﬁdence-based ﬁltering: Aug-
mented examples for which the model’s conﬁdence
is below a minimum threshold are discarded,
(2) Label-preserving augmentation (LP): Aug-
mented examples for which the model predicts a
different label than the original example are dis-
carded. The results, reported in Table 6, show
no meaningful performance gains by these pre-
processing techniques. For Vanilla-DA, minimum
conﬁdence threshold of 0:7performs slightly better
as it brings minor improvements on MRPC (+0.3%)
and QNLI (+0.1%), but is still lower than Glit-
ter. On the other hand, applying these techniques
slightly deteriorates the performance of Glitter in
almost all cases. The only improvements are +0.1%
on QNLI for LP and =0.7.
Effect of Augmentation Size in Glitter. We ex-
plore how augmentation size affects the perfor-
mance of Glitter. Throughout our experiments, we
ﬁx the augmentation size to 8x, but now, we reduce
augmentation size Kto6xand4x, while retaining
selection size k1as before—i.e., 1 for RTE, and 2
for the rest. Our results, shown in Table 7, reveal
that whenKbecomes close to k1, Glitter’s per-
formance declines. Nonetheless, for a sufﬁciently
large augmentation, Glitter starts to shine. For SST,
and MRPC, the magic number is 8x, whereas for
QNLI, and RTE, Glitter performs best on 6x. An-
other parameter in Glitter is the selection size k1.
We ﬁnd that for all tasks, the best value can be cho-
sen fromf1;2g(2 by default). Using this method,
tuningk1is straightforward and does not impose
additional complexity to our method.
Effect of Selection Strategy in Glitter. In this
section, our objective is to assess whether our pro-
posed selection algorithm is crucial in Glitter. To

--- PAGE 9 ---
400 600 800 1000 1200
Avg. train time per epoch (Seconds)94.094.595.095.596.0Acc
1x
2x4x 6x8x
No Aug.Glitter(a) SST
25 50 75 100 125 150
Avg. train time per epoch (Seconds)91.091.592.092.593.0F1
1x2x4x
6x8x
No Aug.Glitter (b) MRPC
1000 2000 3000 4000
Avg. train time per epoch (Seconds)92939495Acc
1x2x4x6x8x
No Aug.Glitter (c) QNLI
25 50 75 100 125
Avg. train time per epoch (Seconds)84.084.585.085.586.0Acc
1x2x
4x6x8x No Aug.Glitter (d) RTE
Figure 2: Runtime Analysis of DA when training RoBERTa baseusing self-KD. The red point signiﬁes Glitter.
MethodSST MRPC QNLI RTE
Acc F 1 Acc Acc
Glitter
 (8x) 95.8 92.8 93.4 85.9
Glitter
 (6x) 94.7 92.7 93.7 86.3
Glitter
 (4x) 95.0 92.1 93.3 85.7
Glitter-Rnd (8x/2x) 94.3 91.4 93.2 85.2
Glitter-Rnd (8x/1x) 94.3 91.8 93.2 84.5
Table 7: Dev results of self-KD for studying the effect
of augmentation size and the selection algorithm for 4
GLUE tasks.
this end, we sample random augmented examples
at each iteration, namely Glitter-Rnd , instead of
selecting worst-case examples. As illustrated in Ta-
ble 7 (the bottom two rows), the performance drops
on all datasets—i.e., 0.2% on QNLI, and more than
1% on the rest, conﬁrming the effectiveness of our
selection algorithm.
6 Conclusion
In this work, we proposed a universal DA tech-
nique, namely Glitter , that can be freely applied
to any DA technique to enforce sample efﬁciency
without introducing additional parameters or chang-
ing the training procedure. We extensively evalu-
ated Glitter on a broad range of NLU tasks and in
various widely used settings including consistency
training, self-distillation and knowledge distillation
and demonstrated substantial efﬁciency gains with-
out compromising effectiveness. Extending Glitter
to auto-regressive models for machine translation
and abstractive summarization is an interesting di-
rection for future work.
References
Chris Alberti, Daniel Andor, Emily Pitler, Jacob De-
vlin, and Michael Collins. 2019. Synthetic QA cor-
pora generation with roundtrip consistency. In Pro-
ceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 6168–
6173, Florence, Italy. Association for Computa-
tional Linguistics.Cristian Bucilu ˇa, Rich Caruana, and Alexandru
Niculescu-Mizil. 2006. Model compression. In Pro-
ceedings of the 12th ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, pages 535–541.
Jiaao Chen, Dinghan Shen, Weizhu Chen, and Diyi
Yang. 2021. HiddenCut: Simple data augmentation
for natural language understanding with better gener-
alizability. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Papers) ,
pages 4380–4390, Online. Association for Computa-
tional Linguistics.
Jang Hyun Cho and Bharath Hariharan. 2019. On the
efﬁcacy of knowledge distillation. In Proceedings
of the IEEE/CVF International Conference on Com-
puter Vision (ICCV) , pages 4794–4802.
Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay
Vasudevan, and Quoc V Le. 2019. AutoAugment:
Learning augmentation policies from data. In Pro-
ceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) ,
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Terrance DeVries and Graham W Taylor. 2017.
Improved regularization of convolutional neu-
ral networks with cutout. arXiv preprint
arXiv:1708.04552 .
Jingfei Du, Edouard Grave, Beliz Gunel, Vishrav
Chaudhary, Onur Celebi, Michael Auli, Veselin
Stoyanov, and Alexis Conneau. 2021. Self-training
improves pre-training for natural language under-
standing. In Proceedings of the 2021 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies , pages 5408–5418, Online. Association for
Computational Linguistics.
Sergey Edunov, Myle Ott, Michael Auli, and David
Grangier. 2018. Understanding back-translation at

--- PAGE 10 ---
scale. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing ,
pages 489–500, Brussels, Belgium. Association for
Computational Linguistics.
Farzan Farnia and David Tse. 2016. A minimax ap-
proach to supervised learning. Advances in Neural
Information Processing Systems , 29:4240–4248.
Steven Feng, Varun Gangal, Jason Wei, Sarath Chan-
dar, Soroush V osoughi, Teruko Mitamura, and Ed-
uard Hovy. 2021. A survey of data augmentation
approaches for NLP. In Findings of the Association
for Computational Linguistics: ACL-IJCNLP 2021 ,
pages 968–988, Online. Association for Computa-
tional Linguistics.
Tommaso Furlanello, Zachary Lipton, Michael Tschan-
nen, Laurent Itti, and Anima Anandkumar. 2018.
Born again neural networks. In Proceedings of the
35th International Conference on Machine Learning ,
pages 1607–1616. PMLR.
Matt Gardner, Yoav Artzi, Victoria Basmov, Jonathan
Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi,
Dheeru Dua, Yanai Elazar, Ananth Gottumukkala,
Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco,
Daniel Khashabi, Kevin Lin, Jiangming Liu, Nel-
son F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer
Singh, Noah A. Smith, Sanjay Subramanian, Reut
Tsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou.
2020. Evaluating models’ local decision boundaries
via contrast sets. In Findings of the Association
for Computational Linguistics: EMNLP 2020 , pages
1307–1323, Online. Association for Computational
Linguistics.
Hongyu Guo, Yongyi Mao, and Richong Zhang. 2019.
Augmenting data with mixup for sentence clas-
siﬁcation: An empirical study. arXiv preprint
arXiv:1905.08941 .
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531 .
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
Yejin Choi. 2020. The curious case of neural text de-
generation. In International Conference on Learn-
ing Representations .
Zhiting Hu, Bowen Tan, Ruslan Salakhutdinov, Tom
Mitchell, and Eric P Xing. 2019. Learning data ma-
nipulation for augmentation and weighting. arXiv
preprint arXiv:1910.12795 .
Tianjian Huang, Shaunak Halbe, Chinnadhurai Sankar,
Pooyan Amini, Satwik Kottur, Alborz Geramifard,
Meisam Razaviyayn, and Ahmad Beirami. 2022.
DAIR: Data augmented invariant regularization. In
International Conference on Learning Representa-
tions .
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,
Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.2020. TinyBERT: Distilling BERT for natural lan-
guage understanding. In Findings of the Association
for Computational Linguistics: EMNLP 2020 , pages
4163–4174, Online. Association for Computational
Linguistics.
Ehsan Kamalloo, Mehdi Rezagholizadeh, Peyman
Passban, and Ali Ghodsi. 2021. Not far away, not so
close: Sample efﬁcient nearest neighbour data aug-
mentation via MiniMax. In Findings of the Associ-
ation for Computational Linguistics: ACL-IJCNLP
2021 , pages 3522–3533, Online. Association for
Computational Linguistics.
Divyansh Kaushik, Eduard Hovy, and Zachary Lipton.
2020. Learning the difference that makes a differ-
ence with counterfactually-augmented data. In Inter-
national Conference on Learning Representations .
Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018.
SciTail: A textual entailment dataset from science
question answering. In Thirty-Second AAAI Confer-
ence on Artiﬁcial Intelligence .
Sosuke Kobayashi. 2018. Contextual augmentation:
Data augmentation by words with paradigmatic re-
lations. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers) , pages 452–457,
New Orleans, Louisiana. Association for Computa-
tional Linguistics.
Tian Li, Ahmad Beirami, Maziar Sanjabi, and Virginia
Smith. 2021. Tilted empirical risk minimization. In
International Conference on Learning Representa-
tions .
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming
He, and Piotr Dollár. 2017. Focal loss for dense ob-
ject detection. In Proceedings of the IEEE interna-
tional conference on computer vision , pages 2980–
2988.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
RoBERTa: A robustly optimized BERT pretraining
approach. arXiv:1907.11692.
Shayne Longpre, Yi Lu, Zhucheng Tu, and Chris
DuBois. 2019. An exploration of data augmentation
and sampling techniques for domain-agnostic ques-
tion answering. arXiv preprint arXiv:1912.02145 .
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y . Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies , pages 142–150, Port-
land, Oregon, USA. Association for Computational
Linguistics.

--- PAGE 11 ---
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zampar-
elli. 2014. A SICK cure for the evaluation of compo-
sitional distributional semantic models. In Proceed-
ings of the Ninth International Conference on Lan-
guage Resources and Evaluation (LREC’14) , pages
216–223, Reykjavik, Iceland. European Language
Resources Association (ELRA).
Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019.
Right for the wrong reasons: Diagnosing syntactic
heuristics in natural language inference. In Proceed-
ings of the 57th Annual Meeting of the Association
for Computational Linguistics , pages 3428–3448,
Florence, Italy. Association for Computational Lin-
guistics.
Clara H. McCreery, Namit Katariya, Anitha Kannan,
Manish Chablani, and Xavier Amatriain. 2020. Ef-
fective transfer learning for identifying similar ques-
tions: Matching user questions to COVID-19 FAQs.
InProceedings of the 26th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery & Data
Mining , pages 3458—-3465. Association for Com-
puting Machinery.
John Miller, Karl Krauth, Benjamin Recht, and Ludwig
Schmidt. 2020. The effect of natural distribution
shift on question answering models. In Proceedings
of the 37th International Conference on Machine
Learning , volume 119 of Proceedings of Machine
Learning Research , pages 6905–6916. PMLR.
Marius Mosbach, Maksym Andriushchenko, and Diet-
rich Klakow. 2021. On the stability of ﬁne-tuning
BERT: Misconceptions, explanations, and strong
baselines. In International Conference on Learning
Representations .
Nathan Ng, Kyunghyun Cho, and Marzyeh Ghassemi.
2020. SSMBA: Self-supervised manifold based data
augmentation for improving out-of-domain robust-
ness. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP) , pages 1268–1283, Online. Associa-
tion for Computational Linguistics.
Yixin Nie, Adina Williams, Emily Dinan, Mohit
Bansal, Jason Weston, and Douwe Kiela. 2020. Ad-
versarial NLI: A new benchmark for natural lan-
guage understanding. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics , pages 4885–4901, Online. Association
for Computational Linguistics.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela
Fan, Sam Gross, Nathan Ng, David Grangier, and
Michael Auli. 2019. fairseq: A fast, extensible
toolkit for sequence modeling. In Proceedings of
the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics
(Demonstrations) , pages 48–53, Minneapolis, Min-
nesota. Association for Computational Linguistics.
Yanru Qu, Dinghan Shen, Yelong Shen, Sandra Sajeev,
Weizhu Chen, and Jiawei Han. 2021. CoDA:Contrast-enhanced and diversity-promoting data
augmentation for natural language understanding.
InInternational Conference on Learning Represen-
tations .
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing , pages 2383–2392, Austin,
Texas. Association for Computational Linguistics.
Ahmad Rashid, Vasileios Lioutas, and Mehdi Reza-
gholizadeh. 2021. MATE-KD: Masked adversarial
TExt, a companion to knowledge distillation. In Pro-
ceedings of the 59th Annual Meeting of the Associa-
tion for Computational Linguistics and the 11th In-
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers) , pages 1062–
1071, Online. Association for Computational Lin-
guistics.
Anna Rogers. 2021. Changing the world by changing
the data. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Papers) ,
pages 2182–2194, Online. Association for Computa-
tional Linguistics.
Nithya Sambasivan, Shivani Kapania, Hannah Highﬁll,
Diana Akrong, Praveen Paritosh, and Lora M Aroyo.
2021. “everyone wants to do the model work, not
the data work”: Data cascades in high-stakes ai. In
Proceedings of the 2021 CHI Conference on Human
Factors in Computing Systems , pages 1–15. Associ-
ation for Computing Machinery.
Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019. DistilBERT, a distilled ver-
sion of BERT: smaller, faster, cheaper and lighter.
arXiv:1910.01108.
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Improving neural machine translation mod-
els with monolingual data. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) , pages
86–96, Berlin, Germany. Association for Computa-
tional Linguistics.
Siamak Shakeri, Cicero Nogueira dos Santos, Henghui
Zhu, Patrick Ng, Feng Nan, Zhiguo Wang, Ramesh
Nallapati, and Bing Xiang. 2020. End-to-end syn-
thetic data generation for domain adaptation of ques-
tion answering systems. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP) , pages 5445–5460, On-
line. Association for Computational Linguistics.
Dinghan Shen, Mingzhi Zheng, Yelong Shen, Yanru
Qu, and Weizhu Chen. 2020. A simple but tough-
to-beat data augmentation approach for natural lan-
guage understanding and generation. arXiv preprint
arXiv:2009.13818 .

--- PAGE 12 ---
Riccardo V olpi, Hongseok Namkoong, Ozan Sener,
John Duchi, Vittorio Murino, and Silvio Savarese.
2018. Generalizing to unseen domains via adversar-
ial data augmentation. Advances in neural informa-
tion processing systems , 31.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. 2019.
GLUE: A multi-task benchmark and analysis plat-
form for natural language understanding. In Inter-
national Conference on Learning Representations .
Dongdong Wang, Yandong Li, Liqiang Wang, and Bo-
qing Gong. 2020. Neural networks are more produc-
tive teachers than human raters: Active mixup for
data-efﬁcient knowledge distillation from a black-
box model. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition ,
pages 1498–1507.
Jason Wei and Kai Zou. 2019. EDA: Easy data aug-
mentation techniques for boosting performance on
text classiﬁcation tasks. In Proceedings of the
2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 6382–6388, Hong Kong,
China. Association for Computational Linguistics.
Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume
1 (Long Papers) , pages 1112–1122, New Orleans,
Louisiana. Association for Computational Linguis-
tics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander Rush. 2020. Trans-
formers: State-of-the-art natural language process-
ing. In Proceedings of the 2020 Conference on Em-
pirical Methods in Natural Language Processing:
System Demonstrations , pages 38–45, Online. Asso-
ciation for Computational Linguistics.
Xing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han,
and Songlin Hu. 2019. Conditional BERT contex-
tual augmentation. In International Conference on
Computational Science , pages 84–95. Springer Inter-
national Publishing.
Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong,
and Quoc Le. 2020. Unsupervised data augmenta-
tion for consistency training. Advances in Neural
Information Processing Systems , 33:6256–6268.
Mingyang Yi, Lu Hou, Lifeng Shang, Xin Jiang, Qun
Liu, and Zhi-Ming Ma. 2021. Reweighting aug-mented samples by minimizing the maximal ex-
pected loss. In International Conference on Learn-
ing Representations .
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. HellaSwag: Can
a machine really ﬁnish your sentence? In Pro-
ceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 4791–
4800, Florence, Italy. Association for Computational
Linguistics.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin,
and David Lopez-Paz. 2018. mixup: Beyond empir-
ical risk minimization. In International Conference
on Learning Representations .
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
siﬁcation. In Advances in Neural Information Pro-
cessing Systems , volume 28, pages 649–657. Curran
Associates, Inc.
Yuan Zhang, Jason Baldridge, and Luheng He. 2019.
PAWS: Paraphrase adversaries from word scram-
bling. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
1298–1308, Minneapolis, Minnesota. Association
for Computational Linguistics.

--- PAGE 13 ---
A Implementation Details
A.1 Fine-tuning details
We adopted the publicly available pre-trained
RoBERTa (Liu et al., 2019) and DistilRoBERTa
(Sanh et al., 2019)—using the Huggingface Trans-
formers library (Wolf et al., 2020) and the Pytorch
Lightning library5.
For the testsettings, the model is evaluated on
the development data once per epoch for small
datasets and twice per epoch for large ones—i.e.,
SST-2, MNLI, QNLI, SQuAD, and HellaSwag.
The best performing model is chosen for testing.
Our learning rate schedule follows a linear de-
cay scheduler with a warm-up, speciﬁed as a ra-
tio of the total number of training steps. Maxi-
mum number of epochs is set to 20 for all tasks
except SQuAD, following (Mosbach et al., 2021).
For large datasets, we early stop with a patience
of 10. The learning rate, and the batch size are
tuned for each task separately. The details of hy-
perparameters are summarized in Table 9. We ran
RoBERTa baseexperiments with the similar hyper-
parameters, but with these exceptions: On QNLI,
learning rate, batch size, and weight decay are set
to3e-5, 64, and 0.1; warmup ratio is set to 0.06 on
QQP.
Fordevexperiments, we follow CoDA (Qu et al.,
2021) on the GLUE tasks. Speciﬁcally, we train
the model for 5 epochs with a batch size of 32,
learning rate 1e-5, warmup ratio 0.06, weight decay
0.1, and linear learning rate decay. For SQuAD,
and HellaSwag, the hyperparameters are detailed
in Table 8.
All experiments were conducted on two Nvidia
Tesla V100 GPUs.
Hyperparam. SQuAD HellaSwag
Learning rate 1:5e-5 1:5e-5
Batch size 16 32
Max length 512 512
Max epochs 3 20
Warmup ratio 0.06 0.06
Grad. acc. steps 4 1
Weight Decay 0.01 0.01
temp.(for KD) 5.0 10.0
Table 8: Hyperparameters of DistilRoBERTa on two
downstream tasks.
5https://github.com/PyTorchLightning/
pytorch-lightningA.2 Knowledge distillation details
We implemented knowledge distillation by caching
the teacher’s logits prior to training. We performed
grid search to ﬁnd the best softmax temperature 
fromf5.0, 10.0, 12.0, 20.0, 30.0 g. The value of 
used in our experiments are reported in Tables 8
and 9 for DistilRoBERTa and RoBERTa base; with
the exception = 20:0on MRPC for RoBERTa base.
Loss weight , in Eq. (8), is set to 0.5 for all tasks
except CoLA in which = 0:75.
B OOD results
B.1 Distilled Mode
OOD results for models trained in the distilled
mode are presented in Table 10.
B.2 Standalone Mode
Table 11 presents OOD results for models trained
using testsettings, and Table 12 (complementary
to Table 3 in §4.2.1) presents OOD results for dev
experiments.

--- PAGE 14 ---
Hyperparam. CoLA SST MRPC STS-B QQP MNLI-m/mm QNLI RTE
Learning rate 1e-5 1 e-5 1 e-5 1 e-5 1 e-5 3 e-5/1e-5 5 e-51e-5
Batch size 32 64 16 32 64 64 12832
Max length 128 256 128 128 256 256 256 256
Warmup ratio 0.1 0.06 0.06 0.06 0.10.08/0.06 0.08 0.06
Gradient acc. steps 1 4 1 1 4 4 4 1
Weight Decay 0.1 0.1 0.1 0.1 0.1 0.0/0.1 0.00.1
Softmax temp. (for KD) 30.0 20.0 12.012.0 20.0 12.0 12.0 12.0
Table 9: Hyperparameters of DistilRoBERTa on the GLUE benchmark. We used the same conﬁguration for
RoBERTa basealbeit with a few exceptions marked by ().
Trained On ! SST SST SST STS QQP QQP MNLI MNLI RTE
MethodIMDb IMDb-Con. IMDb-CAD SICK MQP PA WS QQP SciTail A-NLI HANS
Acc Acc Acc P/S Acc/F 1 Acc Acc Acc Acc
RoB Large 93.7 92.0 94.0 84.3 71.6 43.6 82.0 45.9 81.8
DistilRoB 90.2 87.6 92.5 79.6 67.3 36.3 74.8 27.8 71.3
KD 90.6 87.4 93.2 79.9 65.6 33.1 77.3 28.9 70.6
EDA (Wei and Zou, 2019)
Vanilla-DA 91.8 87.2 92.9 80.0 59.9 38.0 75.8 27.3 66.6
Glitter
 91.2 87.1 94.0 80.0 64.0 36.6 75.6 28.8 65.6
Back-Translation
Vanilla-DA 92.2 87.9 92.1 80.3 69.6 35.0 76.5 27.9 68.0
Glitter
 92.4 87.9 92.8 81.2 68.7 35.2 77.6 30.4 70.5
Masked-and-reconstruct
Vanilla-DA 91.8 88.8 92.9 80.4 68.5 33.7 77.4 28.5 69.3
Glitter
 92.0 88.0 92.5 80.7 68.8 35.3 78.2 29.9 70.9
Table 10: OOD results of models whose in-domain test results are reported in Table 1 for the distilled mode. Bold
numbers indicate the best result across DistilRoB models.
Trained On ! SST SST SST STS QQP QQP MNLI MNLI RTE
MethodIMDb IMDb-Con. IMDb-CAD SICK MQP PA WS QQP SciTail A-NLI HANS
Acc Acc Acc P/S Acc/F 1 Acc Acc Acc Acc
RoB Base 92.2 89.1 94.3 80.6 70.7 38.6 78.5 31.4 78.5
Self-KD 92.6 89.1 95.0 80.2 70.9 37.6 79.4 32.1 79.5
+ Vanilla-DA 91.8 88.8 94.8 81.5 71.4 38.8 78.4 31.5 79.3
+ Glitter
 92.0 89.6 94.8 81.7 72.1 39.4 79.1 32.7 80.1
CT + Vanilla-DA 90.6 88.1 92.1 76.6 70.6 38.3 76.6 30.3 78.4
CT + Glitter
 92.2 88.6 93.7 79.4 70.7 38.8 77.0 31.6 80.2
Table 11: OOD results of models whose in-domain test results are reported in Table 2 for the standalone experiment.
Bold numbers indicate the best result.

--- PAGE 15 ---
Trained On ! SST SST SST MNLI MNLI MNLI RTE
MethodIMDb IMDb-Con. IMDb-CAD SciTail A-NLI HANS HANS
Acc Acc Acc Acc Acc Acc
RoB Base 91.90.3 90.00.4 94.10.4 80.10.4 31.00.6 73.70.7 78.30.4
HiddenCut- 87.8 90.4 - 32.8 71.2-
MMELy91.60.1 90.50.7 94.50.4 79.70.3 31.40.6 74.50.6 78.30.3
Self-KD 91.90.3 90.30.5 94.40.4 79.90.3 30.90.4 73.50.7 78.20.4
+ Vanilla-DA 91.60.4 90.20.4 94.30.3 79.30.4 31.30.5 73.90.4 77.80.3
+ Glitter
 91.70.2 90.60.2 94.80.2 79.40.1 31.80.4 74.60.3 78.40.2
Table 12: OOD results of models with devsettings in the standalone mode, same models whose results are reported
in Table 3. () denotes results are taken verbatim from: HiddenCut (Chen et al., 2021). (y) indicates the results are
obtained from our implementation of MMEL (Yi et al., 2021). Bold numbers indicate the best result.
