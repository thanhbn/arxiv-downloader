# 2305.14890.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/data-augmentation/2305.14890.pdf
# Kích thước file: 3328047 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
HARD: Augmentation Cứng cho Chưng Cất Mạnh Mẽ
Arne F. Nix1-2,*, Max F. Burg2-3, Fabian H. Sinz1-2, **
1Viện Tin sinh học và Tin học Y tế, Đại học Tübingen
2Viện Khoa học Máy tính và Viện Khoa học Dữ liệu Campus, Đại học Göttingen
3Viện Vật lý Lý thuyết, Đại học Tübingen
*arne.nix@uni-goettingen.de ,**sinz@cs.uni-goettingen.de

Tóm tắt
Chưng cất kiến thức (KD) là một phương pháp đơn giản và thành công để chuyển giao kiến thức từ mô hình giáo viên sang mô hình học sinh chỉ dựa trên hoạt động chức năng. Tuy nhiên, KD hiện tại có một vài nhược điểm: gần đây đã được chỉ ra rằng phương pháp này không phù hợp để chuyển giao các bias quy nạp đơn giản như tính bất biến dịch chuyển, gặp khó khăn trong việc chuyển giao khả năng tổng quát hóa ngoài miền, và thời gian tối ưu hóa dài gấp nhiều lần so với việc huấn luyện mô hình không-KD thông thường. Để cải thiện các khía cạnh này của KD, chúng tôi đề xuất Hard Augmentations for Robust Distillation (HARD), một khung augmentation dữ liệu có thể áp dụng tổng quát, tạo ra các điểm dữ liệu tổng hợp mà giáo viên và học sinh có sự bất đồng. Chúng tôi chỉ ra trong một ví dụ đơn giản rằng khung augmentation của chúng tôi giải quyết được vấn đề chuyển giao các tính bất biến đơn giản với KD. Sau đó chúng tôi áp dụng khung của mình trong các nhiệm vụ thực tế với nhiều mô hình augmentation khác nhau, từ các phép biến đổi không gian đơn giản đến các thao tác hình ảnh không bị ràng buộc với variational autoencoder đã được pre-train. Chúng tôi thấy rằng các augmentation học được của chúng tôi cải thiện đáng kể hiệu suất KD trên đánh giá trong miền và ngoài miền. Hơn nữa, phương pháp của chúng tôi thậm chí vượt trội hơn cả các augmentation dữ liệu tiên tiến nhất và vì các đầu vào huấn luyện được augment có thể được trực quan hóa, chúng cung cấp cái nhìn sâu sắc định tính về các thuộc tính được chuyển giao từ giáo viên sang học sinh. Do đó HARD đại diện cho một kỹ thuật augmentation dữ liệu có thể áp dụng tổng quát, được tối ưu hóa động để cải thiện khả năng tổng quát hóa và tốc độ hội tụ của các mô hình được huấn luyện với KD.1

1 Giới thiệu
Các phương pháp chưng cất kiến thức (KD) [27,37,60] là những công cụ mạnh mẽ và linh hoạt để chuyển giao kiến thức của một mô hình giáo viên đã cho sang đối tượng chuyển giao, tức mô hình học sinh, mà không cần sao chép các trọng số. Thay vào đó, các phương pháp này khớp hoạt động chức năng của học sinh (ví dụ đầu ra softmax) với hoạt động của giáo viên cho các đầu vào được trình bày. Do đó, những phương pháp này độc lập với các chi tiết kiến trúc và cho phép chưng cất kiến thức được áp dụng trong các kịch bản như nén mô hình [7,27], học liên tục [4,42,52], hoặc thậm chí khoa học thần kinh [35], nơi mà học chuyển giao truyền thống sẽ không thể sử dụng. Các phương pháp KD cũng dường như là chìa khóa để huấn luyện các mô hình mới đánh đổi các bias quy nạp để có thêm tính linh hoạt và nhiều tham số hơn [17,53,55] trên dữ liệu nhỏ hơn [9,40,54]. Tuy nhiên, Nix et al. [40] gần đây đã chỉ ra rằng các phương pháp KD hiện tại thất bại trong việc chuyển giao thậm chí các tính bất biến đơn giản giữa giáo viên và học sinh. Ngoài ra, các nghiên cứu trước đây đã chỉ ra rằng KD dẫn đến khoảng cách lớn hơn giữa học sinh và giáo viên trên hiệu suất đánh giá ngoài miền so với hiệu suất trong miền [6,41], thậm chí trong các trường hợp mà học sinh khớp gần như hoàn hảo với giáo viên [6] (xem Bảng 5). Hiện tượng này đặc biệt rõ ràng đối với các giáo viên đặc biệt mạnh mẽ [41]. Do đó chúng tôi dự kiến rằng việc chuyển giao các thuộc tính mạnh mẽ là một vấn đề khó khăn đối với KD nói chung.

1Code có sẵn tại https://github.com/sinzlab/HARD
Preprint. Đang được đánh giá.arXiv:2305.14890v2 [cs.CV] 25 May 2023

--- TRANG 2 ---
Hình 1: Khung HARD bất khả tri nhiệm vụ của chúng tôi chuyển đổi giữa việc huấn luyện học sinh để khớp với giáo viên và huấn luyện augmentor để tạo ra các mẫu mới mà học sinh kém hiệu suất trong khi duy trì hiệu suất cao của giáo viên. Chúng tôi tối ưu hóa augmentor và học sinh trong các giai đoạn xen kẽ thông qua loss học sinh-giáo viên L˜ s↔˜t và loss giáo viên-giáo viên L˜t↔t. Chúng tôi chuyển đổi giữa hai giai đoạn bằng cách so sánh loss mặc định L˜ s trên dữ liệu được augment với các ngưỡng được định trước.

Chúng tôi đưa ra giả thuyết rằng các phương pháp KD về nguyên tắc có khả năng chuyển giao hầu hết kiến thức từ giáo viên sang học sinh nếu dữ liệu huấn luyện được chọn một cách thích hợp. Chúng tôi xác nhận giả thuyết này trên một ví dụ đơn giản nhỏ (Mục 3), cho thấy tầm quan trọng của dữ liệu đầu vào đối với KD. Được thúc đẩy bởi minh chứng này, chúng tôi đề xuất phương pháp Hard Augmentations for Robust Distillation (HARD), một khung tổng quát (Mục 4) để tạo ra các đầu vào huấn luyện được augment nhằm cải thiện việc chuyển giao kiến thức bằng cách tối đa hóa khoảng cách giữa giáo viên và học sinh trong khi giữ nguyên đầu ra của giáo viên. Kết quả là, khung của chúng tôi di chuyển đầu vào theo các hướng mà giáo viên bất biến nhưng thách thức nhất đối với học sinh. Các thí nghiệm của chúng tôi (Mục 5) cho thấy rằng khung bất khả tri nhiệm vụ của chúng tôi cải thiện hiệu quả chuyển giao và do đó giải quyết vấn đề KD không thể chuyển giao tính bất biến dịch chuyển [40]. Ngoài ra, như một phần của khung, chúng tôi đề xuất một số augmentation được tham số hóa (Mục 4.1) có thể được tích hợp với hầu hết các phương pháp KD hiện có và áp dụng cho nhiều nhiệm vụ thị giác máy tính khác nhau. Cuối cùng, chúng tôi chứng minh qua nhiều mô hình khác nhau trên các nhiệm vụ CIFAR10 và ImageNet rằng khung của chúng tôi học được các augmentation có thể diễn giải cải thiện KD đến cùng mức độ và trong nhiều trường hợp thậm chí vượt qua các phương pháp augmentation dữ liệu đã được thiết lập, ngay cả khi đánh giá trong môi trường ngoài miền.

2 Công trình liên quan
Có một truyền thống lâu đời trong việc sử dụng augmentation dữ liệu để mở rộng nhân tạo dữ liệu huấn luyện cho các mô hình học sâu và đặc biệt trong thị giác máy tính, dù là thông qua thêm nhiễu Gaussian, cắt ngẫu nhiên, dịch chuyển, lật, hay xoay [18,33]. Trong những năm gần đây, augmentation dữ liệu trở nên phức tạp hơn [12,24,28,39,59,61], sử dụng nhiều heuristics khác nhau với mục tiêu cải thiện khả năng tổng quát hóa và trong một số trường hợp cả hiệu suất ngoài miền [24]. Một phương pháp augmentation đặc biệt phổ biến là Mixup [61], nội suy ngẫu nhiên hai mẫu đầu vào và nhãn tương ứng của chúng. Tương tự, Cutmix [59] kết hợp hai hình ảnh đầu vào bằng cách dán một vùng cắt ngẫu nhiên của một hình ảnh lên trên hình ảnh khác. Ngoài ra, nhiều nghiên cứu sử dụng các augmentation được tham số hóa được tối ưu để cải thiện một mục tiêu cho trước [11,25,48,67], và một số thậm chí tối ưu hóa các augmentation để cải thiện trên một mục tiêu đối thủ [2, 3, 20, 56, 63–65], tuy nhiên, không áp dụng chúng cho việc chuyển giao kiến thức.

Trong KD, áp dụng augmentation dữ liệu là một công cụ rất hiệu quả để cải thiện sự khớp giữa học sinh và giáo viên [6,58] và tối ưu hóa trên mức meta có thể hữu ích để hỗ trợ việc dạy học [43]. Tương tự với công trình của chúng tôi, Haidar et al. [21], Rashid et al. [45], Zhang et al. [62] đã sử dụng các mục tiêu đối thủ để tối ưu hóa augmentation dữ liệu cho KD, tuy nhiên, họ chỉ tập trung vào các nhiệm vụ xử lý ngôn ngữ tự nhiên và không tối ưu hóa các augmentation hướng tới tính bất biến.

Được truyền cảm hứng bởi khối lượng lớn công trình này, chúng tôi xây dựng một khung bất khả tri nhiệm vụ chỉ chứa một khối xây dựng cụ thể cho miền dữ liệu – việc khởi tạo mô hình augmentor tạo ra các mẫu dữ liệu được augment – mà chúng tôi cung cấp nhiều lựa chọn mô hình hợp lý dựa trên các module biến đổi không gian [29], Mixup [61], và variational autoencoder [10, 31, 34].

--- TRANG 3 ---
x−202yMSE: 5.08ABaseline
MSE: 3.24BMix-Up
MSE: 1.96CGaussian Noise
MSE: 0.10DHARDTeacher Student Train Data
−10 0 100.00.20.4DensityE
−10 0 10F
−10 0 10G
−10 0 10H

Hình 2: Khớp học sinh, một MLP ReLU ba lớp, với hàm giáo viên, cos(x), trong 10.000 lần lặp. Chúng tôi hiển thị kết quả cho 10 seed ngẫu nhiên (A-D) và phân phối của các đầu vào huấn luyện (được augment) dưới dạng histogram được chuẩn hóa (E-H). Chúng tôi so sánh baseline (không augmentation) với Mixup, nhiễu Gaussian và phân phối nhiễu được tối ưu hóa HARD. Chúng tôi báo cáo lỗi bình phương trung bình (MSE) trên 100 đầu vào test được lấy mẫu từ U[−10,10].

3 Dữ liệu Đầu vào Quan trọng cho Chuyển giao Chức năng
Chúng tôi đưa ra giả thuyết rằng việc lựa chọn dữ liệu đầu vào là rất quan trọng cho việc chưng cất kiến thức thành công và chúng tôi minh họa tác động của dữ liệu huấn luyện bằng một ví dụ đơn giản. Để chứng minh điều này, hãy xem xét một nhiệm vụ KD đơn giản trong đó chúng tôi khởi tạo mô hình giáo viên bằng hàm thực ft(x) = cos(x) và học sinh fs(x) bằng một Multilayer Perceptron (MLP) ba lớp với kích hoạt ReLU [1]. Chúng tôi sử dụng dữ liệu đầu vào x được chọn sao cho nó không nắm bắt được tính chu kỳ cos(x) của giáo viên (các điểm màu cam trong Hình 2A). KD đơn giản không nội suy giữa các điểm huấn luyện cho trước cũng không ngoại suy vượt ra ngoài chúng (Hình 2E). Do đó mạng thần kinh học sinh không học được tính chu kỳ của giáo viên và thất bại trong việc nội suy và ngoại suy vượt ra ngoài dữ liệu huấn luyện (Hình 2A).

Việc augment dữ liệu huấn luyện với các đầu vào hữu ích hơn x̃ và nhãn giáo viên ft(x̃) = cos(x̃) có thể giảm thiểu vấn đề này. Một phương pháp được áp dụng thành công cho KD [6] là mở rộng dữ liệu đầu vào thông qua Mixup [61]. Khi áp dụng điều này vào ví dụ minh họa của chúng tôi, chúng tôi tạo ra các đầu vào huấn luyện mới x̃ thông qua nội suy tuyến tính giữa các cặp điểm đầu vào x̃ = (1−α)x1+αx2 (Hình 2F), và ghi lại các phản hồi giáo viên tương ứng ft(x̃) = cos(x̃). Do đó, học sinh học cách nội suy giữa các điểm huấn luyện, nhưng mixup không tăng cường ngoại suy (Hình 2B).

Để tạo ra các điểm dữ liệu sẽ nội suy và ngoại suy vượt ra ngoài các điểm huấn luyện đã có, chúng tôi có thể đơn giản augment bằng cách thêm nhiễu Gaussian ϵ vào các điểm dữ liệu có sẵn, x̃ = x + ϵ, do đó nội suy và ngoại suy vượt ra ngoài dữ liệu huấn luyện (Hình 2G). Chiến lược này giúp học sinh của chúng tôi khớp với giáo viên cũng ngoài chế độ huấn luyện ban đầu (Hình 2C). Tuy nhiên, học sinh chỉ cải thiện trong một biên cố định được xác định bởi mean và variance của phân phối nhiễu.

Chúng tôi có thể cải thiện nội suy và ngoại suy một cách rõ ràng bằng cách tăng variance của phân phối nhiễu hoặc dịch chuyển mean của nó, tuy nhiên, khi chúng tôi chuyển sang không gian đầu vào hình ảnh chiều cao (x∈R→x⃗∈RN), việc chọn heuristic các mẫu mới hữu ích trở nên không rõ ràng và đồng thời các chiến lược khám phá ngẫu nhiên trở nên không khả thi về mặt tính toán. Thay vào đó, chúng tôi đề xuất tối ưu hóa một augmentation được tham số hóa để tạo ra hiệu quả các mẫu huấn luyện mới, khó mà học sinh thiếu hiệu suất, vì ở đây học sinh có thể cải thiện nhiều nhất. Trong ví dụ đơn giản của chúng tôi, chúng tôi minh họa điều này bằng cách tối ưu hóa các tham số của Gaussian (mean và variance) theo khung augmentation HARD của chúng tôi, mà chúng tôi sẽ trình bày trong phần tiếp theo. Điều này cung cấp cho chúng tôi một phân phối nhiễu mà chúng tôi sử dụng để vẽ các ví dụ huấn luyện mới hữu ích x̃ chuyển giao khả năng nội và ngoại suy cho mạng học sinh (Hình 2D,H). Nhìn chung, ví dụ đơn giản này cho thấy rằng việc học các augmentation khó để chọn các điểm dữ liệu mới hữu ích là rất quan trọng để cải thiện hiệu quả ngoại suy vượt ra ngoài phân phối huấn luyện.

--- TRANG 4 ---
4 Học Hard Augmentations cho Chưng cất Mạnh mẽ (HARD)
Khung HARD bất khả tri nhiệm vụ của chúng tôi học cách augment hình ảnh huấn luyện để giúp chuyển giao kiến thức từ mô hình giáo viên sang mô hình học sinh một cách hiệu quả nhất. Phương pháp của chúng tôi yêu cầu ba thành phần chính: một mô hình giáo viên với các tham số bị đóng băng, một mô hình học sinh cần học kiến thức từ giáo viên, và một mô hình augmentation được tham số hóa học cách augment hình ảnh sao cho hầu hết kiến thức của giáo viên được chuyển giao cho học sinh.

Trong các phương pháp KD cổ điển [27], mục tiêu là tối thiểu hóa khoảng cách D[fs(x), ft(x)] giữa activation của học sinh fs(x) và activation của giáo viên ft(x) trên các đầu vào cho trước x∈Rn. Thông thường, đây sẽ là divergence Kullback-Leibler giữa các phân phối softmax của giáo viên và học sinh. Thật không may, chỉ xem xét dữ liệu huấn luyện có thể bỏ lỡ các thuộc tính của giáo viên (ví dụ tính bất biến dịch chuyển) có thể quan trọng cho khả năng tổng quát hóa (xem Mục 3 cho một ví dụ minh họa). Để giải quyết vấn đề này, chúng tôi học một mô hình augmentation được tham số hóa ga để tạo ra các điểm dữ liệu đầu vào mới x̃ = ga(x) chuyển giao các thuộc tính bất biến như vậy từ giáo viên sang học sinh. Do đó, chúng tôi định nghĩa loss giáo viên-học sinh xem xét trường hợp tổng quát hơn của việc khớp học sinh và giáo viên trên các đầu vào được augment x̃∈Rn:

L˜ s↔˜t=D[fs(x̃), ft(x̃)]. (1)

Để chuyển giao cụ thể các thuộc tính bất biến của giáo viên cho học sinh, chúng tôi đề xuất một loss giáo viên-giáo viên đẩy augmentor hướng tới việc tạo ra các điểm dữ liệu mà giáo viên bất biến,

L˜t↔t=D[ft(x̃), ft(x)], (2)

vì đây thường là các augmentation hữu ích cho khả năng tổng quát hóa. Sử dụng cả hai loss này, chúng tôi tối ưu hóa các tham số θa của augmentor để tạo ra các mẫu được augment mà giáo viên cho kết quả activation tương tự nhưng học sinh khác biệt với chúng (Hình 1 trên) và đồng thời chúng tôi tối ưu hóa các tham số θs của học sinh để hoạt động tốt trên các augmentation đó (Hình 1 dưới):

maxθa λsL˜ s↔˜t−λtL˜t↔t và minθs L˜ s↔˜t. (3)

Ở đây, λs và λt cân bằng các thành phần loss và được coi như các siêu tham số. Chúng tôi huấn luyện cả hai thành phần riêng biệt chuyển đổi từ huấn luyện augmentor sang huấn luyện học sinh khi hiệu suất của học sinh trên dữ liệu được augment trở nên tệ hơn một ngưỡng được định trước (L˜ s > ℓmax) và chúng tôi chuyển ngược lại từ huấn luyện học sinh sang augmentor khi hiệu suất của học sinh trên dữ liệu được augment vượt qua một ngưỡng được định trước (L˜ s < ℓmin; Hình 1). Để ngăn chặn việc quên thảm khốc, chúng tôi lưu các augmentor tại mỗi lần chuyển đổi và sử dụng một augmentor được chọn ngẫu nhiên từ tập hợp các augmentor đã lưu trước đó trong mỗi lần lặp khi huấn luyện học sinh.

4.1 Các mô hình augmentor
Để tạo ra các điểm dữ liệu đầu vào mới, việc chọn một augmentor phù hợp với ứng dụng mong muốn và đủ mạnh để tạo ra các augmentation hữu ích là quan trọng. Thông thường, chúng tôi không biết trước các augmentation hữu ích là gì và do đó nên cố gắng cho phép càng nhiều tính linh hoạt càng tốt. Ngoài ra, một số biến thể qua các augmentation có thể có lợi cho việc chuyển giao. Do đó, tất cả các augmentor trong nghiên cứu của chúng tôi đưa tính ngẫu nhiên vào mô hình bằng cách thêm nhiễu Gaussian vào việc tính toán augmentation thông qua kỹ thuật tham số hóa lại [31]. Mặc dù khung của chúng tôi có thể áp dụng tổng quát qua các miền, việc chọn một mô hình augmentation hiệu quả có thể cần được giải quyết cho từng nhiệm vụ riêng lẻ. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng các mô hình augmentor sau:

HARD-Affine Trong mô hình đơn giản nhất, chúng tôi giới hạn các augmentation thành các phép biến đổi affine của lưới tọa độ của các vị trí pixel, tức là dịch chuyển, xoay, co giãn, và biến dạng của hình ảnh. Các mô hình thực hiện các phép biến đổi như vậy được biết đến như spatial transformer [29]. Chúng tôi tận dụng mô hình này cho augmentor của chúng tôi bằng cách học một phân phối trên các entry của ma trận biến đổi affine ϑ∈R2×3 định nghĩa phép biến đổi của lưới lấy mẫu, tức là một phép biến đổi ánh xạ các vị trí pixel từ hình ảnh gốc sang hình ảnh được augment (Hình 3A).

HARD-Mix Ngoài ra chúng tôi xem xét một mô hình augmentor phức tạp hơn một chút, đó là một biến thể thích ứng của các augmentation Mixup [61] và Cutmix [59] thường được sử dụng. Tuy nhiên, thay vì lấy mẫu ngẫu nhiên tỷ lệ và vị trí cutout được sử dụng để kết hợp hình ảnh, chúng tôi học cách kết hợp hình ảnh phụ thuộc vào các hình ảnh đầu vào. Chúng tôi đạt được điều này bằng cách thực hiện một phép chiếu theo patch của hình ảnh đầu vào, sau đó so sánh mỗi patch với cùng một vector query được lấy mẫu từ một phân phối học được (Hình 3B). Chúng tôi chuẩn hóa độ tương tự cho mỗi patch qua mỗi nhóm hình ảnh và sử dụng các trọng số kết quả để kết hợp các patch hình ảnh gốc, tạo ra một hình ảnh kết hợp. Cơ chế này cho phép augmentor của chúng tôi quyết định đặc trưng nào của hình ảnh nào được hiển thị cho học sinh, cho phép nó khám phá không gian nội suy giữa các hình ảnh một cách có hệ thống, thay vì ngẫu nhiên. Vì việc giáo viên bất biến với một phép nội suy được tạo ra bởi HARD-Mix sẽ không có ý nghĩa, chúng tôi không xem xét loss giáo viên-giáo viên L˜t↔t trong trường hợp này và thay vào đó tối ưu hóa học sinh và augmentor cùng nhau.

HARD-VAE Để nâng cao hơn nữa các ràng buộc, chúng tôi muốn sử dụng một augmentor mạnh hơn có thể tạo ra nhiều hình ảnh đa dạng trên toàn bộ không gian hình ảnh. Vì augmentor phải tạo ra các mẫu mới một cách nhanh chóng trong quá trình huấn luyện học sinh, quá trình tạo ra cần phải rất nhanh, giới hạn lựa chọn các mô hình tạo sinh hữu ích. Vì lý do này, chúng tôi tập trung vào các biến thể của kiến trúc variational autoencoder [31], cho phép tái tạo hình ảnh tốt có thể đạt được một cách hợp lý nhanh chóng trong một lần forward pass (Hình 3D). Đối với CIFAR, chúng tôi chọn mô hình VAE rất sâu [10], mà chúng tôi fine-tune bằng cách chỉ tối ưu hóa các tham số của mạng posterior từ lớp 10 trở đi trong decoder. Đối với các thí nghiệm trên ImageNet, chúng tôi sử dụng Residual-Quantized VAE (RQ-VAE) [34] được pre-train trên ImageNet, mà chúng tôi fine-tune toàn bộ và thêm một vector nhiễu vào trạng thái latent. Do đó, khi quá trình huấn luyện tiến triển, mô hình thay đổi từ việc tạo ra các tái tạo thuần túy của một hình ảnh cho trước thành các tạo sinh có điều kiện đầu vào phục vụ như các augmentation của chúng tôi.

--- TRANG 5 ---
[Hình 3: Minh họa các mô hình augmentor được sử dụng trong thí nghiệm của chúng tôi.]

5 Thí nghiệm

5.1 Chuyển giao tính bất biến

Đối với thí nghiệm ban đầu, chúng tôi tái tạo thiết lập từ Nix et al. [40] để kiểm tra liệu chúng tôi có thể chuyển giao bias quy nạp từ một giáo viên bất biến dịch chuyển, CNN và ResNet18 [22], sang một học sinh không có bias quy nạp này được tích hợp vào kiến trúc của mình: một Multi-Layer Perceptron (MLP) và một Vision Transformer (ViT) [17]. Khi huấn luyện học sinh và giáo viên riêng lẻ trên dữ liệu huấn luyện MNIST [15] chuẩn, chúng tôi quan sát thấy một sự giảm nhỏ về hiệu suất tổng quát hóa (-0.6% và -1.2%) giữa giáo viên và học sinh trên tập test MNIST và một khoảng cách lớn (-56.1% và -52.4%) khi chúng tôi đánh giá trên một phiên bản của tập test trong đó các chữ số được dịch chuyển ngẫu nhiên [38]. Như một baseline khác, chúng tôi áp dụng KD thuần túy để chuyển giao tính bất biến dịch chuyển từ giáo viên sang học sinh. Phù hợp với những phát hiện của Nix et al. [40], chúng tôi chỉ quan sát thấy một cải thiện nhỏ trên các tập test trung tâm (+0.2% và +0.3%) và dịch chuyển (+5.1% và +4.3%), có thể là kết quả từ dữ liệu huấn luyện trung tâm mà chúng tôi sử dụng cho việc chuyển giao.

Sau đó chúng tôi kiểm tra liệu việc kết hợp KD với các augmentation của chúng tôi được tạo ra bởi HARD-Affine có vượt trội hơn các baseline này không. Mô hình học sinh kết quả cải thiện đáng kể trên các đầu vào dịch chuyển (+28.6% và +39.4%) so với KD thuần túy và các hình ảnh được tạo ra cho thấy rõ ràng rằng augmentor học cách dịch chuyển các chữ số trong hình ảnh. So với Nix et al. [40] phương pháp của chúng tôi vượt trội hơn kết quả của họ trên nhiệm vụ ViT nhưng, mặc dù cải thiện khả năng tổng quát hóa ngoài miền 28.6% so với baseline, vẫn kém hơn hiệu suất Orbit trên nhiệm vụ MLP. Điều này chứng minh rằng phương pháp của chúng tôi mặc dù tác động lên ít phần của mạng hơn so với Orbit và mặc dù là một phương pháp tổng quát hơn, có thể cải thiện hoặc đạt được hiệu suất tốt hơn khi chuyển giao tính bất biến, và có thể được tổng quát hóa cho các bộ dữ liệu lớn hơn, như chúng tôi chỉ ra dưới đây.

Chúng tôi xác minh rằng cải thiện hiệu suất của học sinh là cụ thể do khung tạo dữ liệu của chúng tôi trong hai thí nghiệm kiểm soát. Thí nghiệm đầu tiên (Random Affine) augment các đầu vào huấn luyện của mô hình học sinh độc lập với một phép biến đổi affine ngẫu nhiên tương tự như mô hình augmentor của chúng tôi, nhưng sử dụng các tham số biến đổi được lấy mẫu đều từ một phạm vi được định trước hợp lý (tức là đảm bảo chữ số luôn hiển thị hoàn toàn). Học sinh này hoạt động tốt trên tập test dịch chuyển, tuy nhiên, hiệu suất giảm đáng kể trên tập test trung tâm. Ngược lại, mô hình HARD-Affine của chúng tôi không bị ràng buộc và học được các augmentation hữu ích hơn, dẫn đến hiệu suất tốt hơn trên các tập test trung tâm.

Trong kiểm soát thứ hai (Shifts) chúng tôi hỏi rằng augmentation dữ liệu có thể cải thiện hiệu suất bao nhiều trong trường hợp tốt nhất (không có KD). Để làm điều này, chúng tôi augment các đầu vào bằng cùng các dịch chuyển ngẫu nhiên đã được áp dụng để thu được dữ liệu test dịch chuyển, dẫn đến cải thiện lớn trên tập test dịch chuyển. Tuy nhiên, các augmentation học được của chúng tôi đạt được điểm số trong phạm vi tương tự trên đánh giá dịch chuyển và vượt trội hơn kết quả của nó trên tập test trung tâm.

5.2 Chuyển giao trên hình ảnh tự nhiên

Sau khi chứng minh rằng phương pháp của chúng tôi thành công trong việc nắm bắt sự khác biệt giữa giáo viên và học sinh và bắc cầu khoảng cách trong bias quy nạp, bây giờ chúng tôi muốn kiểm tra liệu hiệu ứng này có duy trì trong các kịch bản thực tế hơn không.

--- TRANG 6 ---
[Bảng 1: Độ chính xác test MNIST (cột "Centered") và MNIST-C (cột "Shifted")]

Thí nghiệm CIFAR Chúng tôi bắt đầu bằng cách áp dụng khung của chúng tôi vào CIFAR10 [32] trên ba kịch bản KD khác nhau (xem Bảng 2). Cụ thể, chúng tôi kiểm tra các kịch bản mà học sinh thiếu bias quy nạp (ResNet18 → ViT), mà giáo viên có nhiều capacity và truy cập dữ liệu hơn học sinh (ResNet101* → ResNet18), và các kịch bản kết hợp cả hai thuộc tính (ResNet101* → ViT). Đối với tất cả thí nghiệm, chúng tôi giữ thiết lập thí nghiệm càng gần với các thí nghiệm MNIST trước đó càng tốt (xem Phụ lục A cho chi tiết).

Chúng tôi bắt đầu bằng cách thiết lập baseline bằng cách chỉ huấn luyện mô hình giáo viên và chỉ mô hình học sinh trên dữ liệu và đánh giá KD mặc định. Chúng tôi quan sát thấy rằng trên tập dữ liệu nhỏ này một ResNet18 nhỏ hoạt động tốt hơn (78.5% độ chính xác) so với một ViT lớn hơn (68.5%), có thể là do bias quy nạp vượt trội của ResNet trên nhiệm vụ này và tập dữ liệu nhỏ. Tiếp theo, chúng tôi thấy rằng việc thêm augmentation dữ liệu mặc định (xoay ngẫu nhiên, cắt, lật ngang) vào baseline học sinh làm tăng đáng kể hiệu suất lên 92.6% và 78.3% tương ứng cho ResNet18 và ViT. Việc thêm các augmentation mặc định này vào KD thông thường cũng dẫn đến một sự tăng hiệu suất lớn (xem Bảng 2).

Điều đặc biệt đáng chú ý là việc thêm augmentation dữ liệu mặc định vào KD đã dẫn đến một sự tăng hiệu suất đáng kể, các augmentation dữ liệu được học bởi HARD-Affine vượt trội hơn baseline này cho ViT. Về mặt định tính, các hình ảnh được augment thể hiện nhiều phép biến đổi không gian đa dạng, cho thấy rằng sự khác biệt trong các ví dụ này dẫn đến sự tăng hiệu suất quan sát được (Bảng 2, phải).

Sau đó chúng tôi điều tra hiệu suất của chiến lược augmentation HARD-VAE và thấy cải thiện hiệu suất so với baseline KD + augmentation chuẩn cho việc chuyển giao cho học sinh ViT (+1.0% và +1.9%). Tuy nhiên, kiểm tra các hình ảnh được augment chỉ ra rằng augmentor của chúng tôi thiếu các dịch chuyển vị trí đối tượng mong đợi, mà thay vào đó học các thay đổi phong cách trong hình ảnh (Bảng 2, phải). Điều này thúc đẩy chúng tôi kết hợp augmentation HARD-Affine và HARD-VAE dẫn đến hiệu suất tốt nhất (lên đến +7.8%) cho tất cả các cặp giáo viên-học sinh (HARD-VAE-Affine trong Bảng 2) và các hình ảnh kết quả thể hiện tính biến đổi cả về phong cách và sự liên kết không gian (Bảng 2, phải).

Thí nghiệm ImageNet Sau khi thiết lập hiệu suất của phương pháp chúng tôi cho CIFAR10, chúng tôi mở rộng kết quả của mình cho phân loại trên ImageNet [14]. Ở đây chúng tôi hướng đến việc chưng cất một giáo viên ResNet50 [22], được huấn luyện với augmentation dữ liệu Deep-augment và AugMix [25], thành một ResNet18 nhỏ hơn và ViT-S (biến thể vision transformer nhỏ) [17] mà chúng tôi muốn đặc biệt mạnh mẽ với các nhiễu hình ảnh tự nhiên. Việc chưng cất thành ResNet18 cho phép chúng tôi điều tra khả năng nén mô hình, bởi vì ResNet18 là một mạng nhỏ hơn so với ResNet50, nhưng với kiến trúc tương tự. Việc chưng cất thành kiến trúc ViT-S với kích thước patch 14 kiểm tra thêm liệu KD có chuyển giao bias quy nạp tính bất biến dịch chuyển của ResNet50 trên một bộ dữ liệu lớn hơn không.

Chúng tôi đánh giá trên các tập test phổ biến cho cả hiệu suất tổng quát hóa trong miền (ID) [5,46] và ngoài miền (OOD) [19, 23,25,26,57] (Bảng 3 và 4, tương ứng). Để điều tra đúng cách khả năng ngoại suy của huấn luyện KD, chúng tôi đã huấn luyện một baseline KD mạnh bằng cách áp dụng nhiều augmentation dữ liệu khác nhau: chúng tôi chuyển đổi ngẫu nhiên giữa Cutmix [59] và Mixup [61], mỗi cái vẽ trọng số nội suy của chúng từ phân phối β với α = 1, cũng như augmentation AugMix [24]. Đối với huấn luyện học sinh độc lập, chúng tôi áp dụng thêm các augmentation dữ liệu nhẹ hơn khác nhau (Cutmix với α = 1, Mixup với α = 0.1, và Trivialaugment [39]). Vì chúng tôi hỏi cách KD có thể được cải thiện trong môi trường tài nguyên hạn chế, chúng tôi chạy thí nghiệm của mình ngắn hơn một bậc so với đề xuất cho tiên tiến nhất trong KD [6] (200 epoch cho tất cả ResNet18 và 150 epoch cho tất cả thí nghiệm ViT-S). Đối với mô hình học sinh và KD, chúng tôi thực hiện tìm kiếm lưới nhỏ qua các siêu tham số learning-rate và weight-decay. Sau đó chúng tôi huấn luyện các mô hình với khung HARD của chúng tôi dựa trên các siêu tham số của thiết lập KD hoạt động tốt nhất. Các thiết lập cụ thể cho augmentor được chọn thông qua tìm kiếm lưới nhỏ trong thiết lập ResNet18 (để biết chi tiết xem Phụ lục A).

Đầu tiên chúng tôi đánh giá hiệu suất ID của các phương pháp (Bảng 3) bắt đầu với baseline giáo viên và học sinh độc lập, cho thấy khoảng cách hiệu suất lớn hơn giữa học sinh ResNet18 và giáo viên ResNet50 so với học sinh ViT-S (5.1% và 2.6% trên tập validation ImageNet, tương ứng). KD thuần túy giảm đáng kể khoảng cách này cho ViT-S (+2.1% cải thiện hiệu suất so với độc lập). Đối với học sinh ResNet18, KD chỉ đạt được cải thiện nhỏ (0.7% V2) hoặc không cải thiện (0.0% Val), mặc dù khoảng cách ban đầu giữa giáo viên và học sinh lớn hơn.

Việc áp dụng augmentation HARD-Affine, HARD-Mix và HARD-VAE trên nhiệm vụ này cải thiện so với baseline KD thuần túy qua hầu hết các mô hình augmentation và tập test với mức tăng hiệu suất học sinh lên đến 0.9% cho ResNet18 (HARD-Affine) và 0.6% cho ViT-S (HARD-VAE). Đối với ViT-S, phương pháp HARD-VAE hoạt động tốt nhất của chúng tôi thậm chí khớp với hiệu suất của giáo viên trên 2 trong 3 tập test.

Đối với thiết lập OOD (Bảng 4), chúng tôi quan sát thấy rằng khoảng cách ban đầu giữa học sinh và giáo viên lớn hơn so với dữ liệu ID qua tất cả các tập dữ liệu (lên đến 35.1% khác biệt), trừ Im-A trong thiết lập ViT-S. Các augmentation dữ liệu mạnh mẽ mà chúng tôi áp dụng cho baseline KD thuần túy ủng hộ hiệu suất OOD, do đó có thể dự đoán rằng KD thuần túy dẫn đến cải thiện hiệu suất tốt so với baseline độc lập (lên đến 21.3% cải thiện trên Im-C). Tất cả ba phương pháp HARD chuyển giao một số khả năng tổng quát hóa của giáo viên dẫn đến cải thiện trên một số học sinh và tập dữ liệu, tuy nhiên, HARD-Affine thất bại trong việc đạt hiệu suất KD trong cả hai thiết lập và HARD-VAE hoạt động kém hơn cho học sinh ResNet18 trong các kịch bản OOD này. Tuy nhiên, HARD-Mix và HARD-VAE (cho ViT-S) vượt trội hơn KD thuần túy trên nhiều tập test và gần bằng với tất cả các tập khác, trên toàn bộ. Cho rằng chúng tôi đã chọn một baseline rất mạnh bằng cách áp dụng các augmentation dữ liệu mạnh mẽ tiên tiến, chúng tôi thấy những kết quả này đặc biệt khuyến khích.

--- TRANG 7 ---
[Bảng 2: Độ chính xác test trên tập test CIFAR10]

--- TRANG 8 ---
[Bảng 3: Đánh giá trong miền cho ImageNet]
[Bảng 4: Đánh giá ngoài miền cho ImageNet]

--- TRANG 9 ---
6 Thảo luận

Khả năng diễn giải HARD cho phép chúng tôi có cái nhìn sâu sắc về cơ chế chưng cất vì các hình ảnh được augment minh họa kiến thức được chuyển giao (Hình 4). Như mong đợi, HARD-Affine học cách thu nhỏ hình ảnh để dịch chuyển và xoay hình ảnh sao cho đối tượng trong hình ảnh được hiển thị ở các vị trí khác nhau (hàng 2-4 trong Hình 4) và co giãn sao cho hình ảnh bị cắt (hàng 1). Vì HARD-Mix là một phần mở rộng có thể học động của mixup, nó hoặc là gộp hai đối tượng vào cùng một hình ảnh (hàng 1 và 4), đặc biệt nếu chúng không ở cùng vị trí, hoặc sử dụng một hình ảnh để thay đổi phong cách (hàng 2) hoặc nền (hàng 3) của hình ảnh khác. Cuối cùng, HARD-VAE chủ yếu tác động đến phong cách của hình ảnh và thêm vào đó các biến dạng nhỏ cho các vùng hình ảnh cụ thể, điều này có thể nhận thấy qua độ sáng hình ảnh thay đổi và làm mờ một số đặc trưng tần số cao.

Giới hạn và tác động rộng hơn Chưng cất kiến thức tiên tiến thường xử lý các mô hình khổng lồ (hàng tỷ tham số) và thời gian huấn luyện cực dài (>9,000 epoch) [6,13]. So sánh với điều này, nghiên cứu của chúng tôi nhẹ về mặt tính toán khi yêu cầu khoảng 400 A100 GPU ngày qua tất cả thí nghiệm. Chúng tôi tin rằng việc khám phá các mô hình augmentor linh hoạt hơn với không gian latent có ý nghĩa ngữ nghĩa như các mô hình diffusion [44,47,49,50] có thể cải thiện các phương pháp đề xuất của chúng tôi hơn nữa. Tuy nhiên, việc tạo ra một hình ảnh duy nhất với các mô hình diffusion hiện có yêu cầu nhiều giây. Điều này là quá dài, vì vậy chúng tôi để lại việc khám phá khả năng sử dụng của chúng trong kỹ thuật augmentation dữ liệu động đề xuất của chúng tôi cho công việc tương lai. Nói chung, KD cho phép chúng tôi chưng cất các mô hình nhỏ hơn hoạt động tương tự như các mô hình foundation lớn. Việc cải thiện quá trình chưng cất để hiệu quả hơn làm giảm rào cản áp dụng KD qua các phòng thí nghiệm với ngân sách tính toán khác nhau và giảm tác động môi trường. Đồng thời, việc chuyển giao khả năng tổng quát hóa một cách hiệu quả và nhất quán dẫn đến các mô hình chưng cất nhỏ hơn hấp dẫn để sử dụng, do đó chúng tôi mong đợi các mô hình nhỏ hơn như vậy được sử dụng phong phú do đó giảm carbon footprint chung cho việc sử dụng mô hình. Tóm lại, nghiên cứu của chúng tôi đề xuất các hướng để cải thiện KD một cách hiệu quả về mặt hiệu suất, hiệu quả, và do đó tác động môi trường.

7 Kết luận

Trong công trình này, chúng tôi đã giới thiệu một khung tổng quát, bất khả tri nhiệm vụ, và modular để mở rộng chưng cất kiến thức bằng các augmentation dữ liệu có thể học được. Các mô hình augmentation được tối ưu để tạo ra các đầu vào mà giáo viên và học sinh bất đồng, giữ dự đoán của giáo viên không thay đổi cùng lúc. Chúng tôi chỉ ra rằng những augmentation này có thể giải quyết vấn đề của KD và chuyển giao các thuộc tính tính bất biến, thậm chí trong các trường hợp mà bias quy nạp của giáo viên khác biệt với học sinh. Chúng tôi chứng minh thêm rằng các augmentation học được của chúng tôi đạt được hiệu suất cạnh tranh với các kỹ thuật augmentation dữ liệu cổ điển đã được thiết lập thậm chí khi học sinh và giáo viên chia sẻ bias quy nạp tương tự. Nhìn chung, khung của chúng tôi cung cấp một công cụ mạnh mẽ tăng cường hiệu suất chuyển giao và cung cấp cái nhìn sâu sắc độc đáo về kiến thức được chuyển giao thông qua các augmentation có thể diễn giải của nó.

--- TRANG 10 ---
Lời cảm ơn

Hơn nữa, chúng tôi cảm ơn Felix Schlüter vì những cái nhìn sâu sắc hữu ích về các vấn đề đánh giá cũng như Mohammad Bashiri, Pawel Pierzchlewicz và Suhas Shrinivasan vì các nhận xét và thảo luận hữu ích. Các tác giả cảm ơn Trường Sau đại học Quốc tế Max Planck về Hệ thống Thông minh (IMPRS-IS) đã hỗ trợ Arne Nix và Max F. Burg.

Công trình này được hỗ trợ bởi Quỹ Nghiên cứu Cyber Valley (CyVy-RF-2019-01), bởi Bộ Giáo dục và Nghiên cứu Liên bang Đức (BMBF) thông qua Trung tâm AI Tübingen (FKZ: 01IS18039A), bởi Deutsche Forschungsgemeinschaft (DFG) trong SFB 1233, Robust Vision: Inference Principles and Neural Mechanisms (TP12), số dự án: 276693517, và được tài trợ bởi Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Project-ID 432680300 – SFB 1456. FHS được hỗ trợ bởi Carl-Zeiss-Stiftung và ghi nhận sự hỗ trợ của DFG Cluster of Excellence "Machine Learning – New Perspectives for Science", EXC 2064/1, số dự án 390727645.

Tài liệu tham khảo

[1] Abien Fred Agarap. Deep learning using rectified linear units (relu). arXiv preprint arXiv:1803.08375, 2018.

[2] Anthreas Antoniou, Amos Storkey, and Harrison Edwards. Data Augmentation Generative Adversarial Networks. feb 2022.

[3] Sima Behpour, Kris M. Kitani, and Brian D. Ziebart. ADA: Adversarial data augmentation for object detection. Proceedings - 2019 IEEE Winter Conference on Applications of Computer Vision, WACV 2019, pages 1243–1252, mar 2019. doi: 10.1109/WACV.2019.00137.

[4] Ari S Benjamin, David Rolnick, and Konrad P Kording. Measuring and regularizing networks in function space. In 7th International Conference on Learning Representations, ICLR 2019, 2019.

[5] Lucas Beyer, Olivier J Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. Are we done with imagenet? arXiv preprint arXiv:2006.07159, 2020.

[6] Lucas Beyer, Xiaohua Zhai, Amélie Royer, Larisa Markeeva, Rohan Anil, and Alexander Kolesnikov. Knowledge distillation: A good teacher is patient and consistent. jun 2021. URL https://arxiv.org/abs/2106.05237v1http://arxiv.org/abs/2106.05237.

[7] Cristian Bucilă, Rich Caruana, and Alexandra Niculescu-Mizil. Model compression. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, volume 2006, pages 535–541, 2006. ISBN 1595933395. doi: 10.1145/1150402.1150464.

[8] Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, and Quoc V. Le. Symbolic discovery of optimization algorithms, 2023.

[9] Xianing Chen, Qiong Cao, Yujie Zhong, Jing Zhang, Shenghua Gao, and Dacheng Tao. DearKD: Data-Efficient Early Knowledge Distillation for Vision Transformers. apr 2022. doi: 10.48550/arxiv.2204.12997. URL https://arxiv.org/abs/2204.12997v2.

[10] Rewon Child. Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images. nov 2020. doi: 10.48550/arxiv.2011.10650. URL https://arxiv.org/abs/2011.10650v2http://arxiv.org/abs/2011.10650.

[11] Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V. Le. AutoAugment: Learning Augmentation Policies from Data. Cvpr 2019, (Section 3):113–123, may 2018. doi: 10.48550/arxiv.1805.09501. URL https://arxiv.org/abs/1805.09501v3.

[12] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 702–703, 2020.

--- TRANG 11 ---
[13] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin F. Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Patrick Collier, Alexey Gritsenko, Vighnesh Birodkar, Cristina Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetić, Dustin Tran, Thomas Kipf, Mario Lučić, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, and Neil Houlsby. Scaling vision transformers to 22 billion parameters, 2023.

[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009.

[15] Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141–142, 2012.

[16] Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise quantization. 9th International Conference on Learning Representations, ICLR, 2022.

[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. 2020. URL https://github.com/http://arxiv.org/abs/2010.11929.

[18] Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A rotation and a translation suffice: Fooling cnns with simple transformations. 2017.

[19] Robert Geirhos, Claudio Michaelis, Felix A. Wichmann, Patricia Rubisch, Matthias Bethge, and Wieland Brendel. IMAGENET-TRAINED CNNS ARE BIASED TOWARDS TEXTURE; INCREASING SHAPE BIAS IMPROVES ACCURACY AND ROBUSTNESS, nov 2018. ISSN 23318422. URL http://arxiv.org/abs/1811.12231.

[20] Kehong Gong, Jianfeng Zhang, and Jiashi Feng. PoseAug: A Differentiable Pose Augmentation Framework for 3D Human Pose Estimation. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 8571–8580, may 2021. ISSN 10636919. doi: 10.1109/CVPR46437.2021.00847. URL https://arxiv.org/abs/2105.02465v1.

[21] Md Akmal Haidar, Mehdi Rezagholizadeh, Abbas Ghaddar, Khalil Bibi, Philippe Langlais, and Pascal Poupart. CILDA: Contrastive Data Augmentation using Intermediate Layer Knowledge Distillation. apr 2022. doi: 10.48550/arxiv.2204.07674. URL https://arxiv.org/abs/2204.07674v1http://arxiv.org/abs/2204.07674.

[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2016-Decem:770–778, dec 2015. ISSN 10636919. doi: 10.1109/CVPR.2016.90. URL https://arxiv.org/abs/1512.03385v1.

[23] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations, mar 2019. ISSN 23318422. URL http://arxiv.org/abs/1903.12261.

[24] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method to improve robustness and uncertainty. arXiv preprint arXiv:1912.02781, 2019.

[25] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization. In Proceedings of the IEEE International Conference on Computer Vision, pages 8320–8329, 2021. ISBN 9781665428125. doi: 10.1109/ICCV48922.2021.00823. URL https://github.com/hendrycks/imagenet-r.

[26] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. CVPR, 2021.

[27] Geoffrey Hinton and Jeff Dean. Distilling the Knowledge in a Neural Network. Technical report, 2015.

[28] Philip TG Jackson, Amir Atapour Abarghouei, Stephen Bonner, Toby P Breckon, and Boguslaw Obara. Style augmentation: data augmentation via style randomization. In CVPR workshops, volume 6, pages 10–11, 2019.

[29] Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. Spatial transformer networks. In Advances in Neural Information Processing Systems, volume 2015-Janua, pages 2017–2025. Neural information processing systems foundation, jun 2015. URL https://arxiv.org/abs/1506.02025v3.

[30] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. 3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings, dec 2014. URL https://arxiv.org/abs/1412.6980v9.

[31] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In 2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings. International Conference on Learning Representations, ICLR, dec 2014. doi: 10.48550/arxiv.1312.6114. URL https://arxiv.org/abs/1312.6114v10.

[32] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. CIFAR-10 (Canadian Institute for Advanced Research). URL http://www.cs.toronto.edu/~kriz/cifar.html.

[33] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In F. Pereira, C.J. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 25. Curran Associates, Inc., 2012. URL https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf.

[34] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11523–11532, 2022.

[35] Zhu Li, Adrian Perez-Suay, Gustau Camps-Valls, and Dino Sejdinovic. Kernel Dependence Regularizers and Gaussian Processes with Applications to Algorithmic Fairness. Technical report, 2019.

[36] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts, 2017.

[37] Patrick McClure and Nikolaus Kriegeskorte. Representational distance learning for deep neural networks. Frontiers in Computational Neuroscience, 10(DEC):131, dec 2016. ISSN 16625188. doi: 10.3389/fncom.2016.00131. URL http://journal.frontiersin.org/article/10.3389/fncom.2016.00131/full.

[38] Norman Mu and Justin Gilmer. MNIST-C: A Robustness Benchmark for Computer Vision. jun 2019. doi: 10.5281/zenodo.3237938. URL https://arxiv.org/abs/1906.02337v1http://arxiv.org/abs/1906.02337.

[39] Samuel G Müller and Frank Hutter. Trivialaugment: Tuning-free yet state-of-the-art data augmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 774–782, 2021.

[40] Arne Nix, Suhas Shrinivasan, Edgar Y Walker, and Fabian Sinz. Can Functional Transfer Methods Capture Simple Inductive Biases? In Gustau Camps-Valls, Francisco J R Ruiz, and Isabel Valera, editors, Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pages 10703–10717. PMLR, 2022. URL https://proceedings.mlr.press/v151/nix22a.html.

--- TRANG 12 ---
[41] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, và Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023.

[42] Pingbo Pan, Siddharth Swaroop, Alexander Immer, Runa Eschenhagen, Richard E Turner, và Mohammad Emtiyaz Khan. Continual deep learning by functional regularisation of memorable past. In Advances in Neural Information Processing Systems, volume 2020-Decem, 2020.

[43] Hieu Pham, Zihang Dai, Qizhe Xie, và Quoc V. Le. Meta Pseudo Labels. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 11553–11563, mar 2021. ISSN 10636919. doi: 10.1109/CVPR46437.2021.01139. URL https://arxiv.org/abs/2003.10580v4.

[44] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, và Mark Chen. Hierarchical Text-Conditional Image Generation with CLIP Latents, April 2022. URL http://arxiv.org/abs/2204.06125. arXiv:2204.06125 [cs].

[45] Ahmad Rashid, Vasileios Lioutas, và Mehdi Rezagholizadeh. MATE-KD: Masked adversarial text, a companion to knowledge distillation. In ACL-IJCNLP 2021 - 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Proceedings of the Conference, pages 1062–1071, 2021. ISBN 9781954085527. doi: 10.18653/v1/2021.acl-long.86.

[46] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, và Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In International conference on machine learning, pages 5389–5400. PMLR, 2019.

[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, và Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684–10695, 2022.

[48] Evgenia Rusak, Lukas Schott, Roland S. Zimmermann, Julian Bitterwolf, Oliver Bringmann, Matthias Bethge, và Wieland Brendel. A simple way to make neural networks robust against diverse image corruptions. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 12348 LNCS:53–69, jan 2020. ISSN 16113349. doi: 10.48550/arxiv.2001.06057. URL https://arxiv.org/abs/2001.06057v5.

[49] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, và Mohammad Norouzi. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding, May 2022. URL http://arxiv.org/abs/2205.11487. arXiv:2205.11487 [cs].

[50] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, và Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256–2265. PMLR, 2015.

[51] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, và Zbigniew Wojna. Rethinking the inception architecture for computer vision. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2818–2826, 2016. doi: 10.1109/CVPR.2016.308.

[52] Michalis K Titsias, Jonathan Schwarz, Alexander G. de G. Matthews, Razvan Pascanu, và Yee Whye Teh. Functional Regularisation for Continual Learning with Gaussian Processes. 2019. URL http://arxiv.org/abs/1901.11356.

[53] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, và Alexey Dosovitskiy. MLP-Mixer: An all-MLP Architecture for Vision. Advances in Neural Information Processing Systems, 29:24261–24272, may 2021. ISSN 10495258. doi: 10.48550/arxiv.2105.01601. URL https://arxiv.org/abs/2105.01601v4.

--- TRANG 13 ---
[54] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, và Hervé Jégou. Training data-efficient image transformers & distillation through attention. pages 10347–10357, jul 2020. ISSN 2640-3498. URL https://proceedings.mlr.press/v139/touvron21a.htmlhttp://arxiv.org/abs/2012.12877.

[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, và Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, volume 2017-Decem, pages 5999–6009, 2017.

[56] Riccardo Volpi, John Duchi, Hongseok Namkoong, Vittorio Murino, Ozan Sener, và Silvio Savarese. Generalizing to Unseen Domains via Adversarial Data Augmentation. Advances in Neural Information Processing Systems, 2018-Decem:5334–5344, may 2018. ISSN 10495258. doi: 10.48550/arxiv.1805.12018. URL https://arxiv.org/abs/1805.12018v2.

[57] Haohan Wang, Songwei Ge, Zachary Lipton, và Eric P Xing. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems, pages 10506–10518, 2019.

[58] Huan Wang, Suhas Lohit, Michael Jones, và Yun Fu. Knowledge Distillation Thrives on Data Augmentation. dec 2020. doi: 10.48550/arxiv.2012.02909. URL https://arxiv.org/abs/2012.02909v1http://arxiv.org/abs/2012.02909.

[59] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, và Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6023–6032, 2019.

[60] Sergey Zagoruyko và Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. In 5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings, 2017. URL https://github.com/szagoruyko/attention-transfer.

[61] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, và David Lopez-Paz. mixup: Beyond Empirical Risk Minimization. 6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings, oct 2017. doi: 10.48550/arxiv.1710.09412. URL https://arxiv.org/abs/1710.09412v2.

[62] Minjia Zhang, Niranjan Uma Naresh, và Yuxiong He. Adversarial Data Augmentation for Task-Specific Knowledge Distillation of Pre-trained Transformers. Proceedings of the AAAI Conference on Artificial Intelligence, 36(10):11685–11693, 2022. ISSN 2159-5399. doi: 10.1609/aaai.v36i10.21423. URL www.aaai.org.

[63] Xiaofeng Zhang, Zhangyang Wang, Dong Liu, và Qing Ling. DADA: Deep Adversarial Data Augmentation for Extremely Low Data Regime Classification. ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings, 2019-May:2807–2811, may 2019. ISSN 15206149. doi: 10.1109/ICASSP.2019.8683197. URL https://arxiv.org/abs/1809.00981v1.

[64] Xinyu Zhang, Qiang Wang, Jian Zhang, và Zhao Zhong. Adversarial AutoAugment. dec 2019. doi: 10.48550/arxiv.1912.11188. URL https://arxiv.org/abs/1912.11188v1http://arxiv.org/abs/1912.11188.

[65] Long Zhao, Ting Liu, Xi Peng, và Dimitris Metaxas. Maximum-entropy adversarial data augmentation for improved generalization and robustness. In Advances in Neural Information Processing Systems, volume 2020-Decem, 2020. URL https://github.com/garyzhao/ME-ADA.

[66] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, và Yi Yang. Random erasing data augmentation, 2017.

[67] Dominik Zietlow, Michael Lohaus, Guha Balakrishnan, Matthäus Kleindessner, Francesco Locatello, Bernhard Schölkopf, và Chris Russell. Leveling Down in Computer Vision: Pareto Inefficiencies in Fair Deep Classifiers. 2022. URL http://arxiv.org/abs/2203.04913.

--- TRANG 14 ---
A Chi tiết Thiết lập

Các thí nghiệm của chúng tôi trên MNIST được thiết kế để tái tạo Nix et al. [40] và do đó tuân theo thiết lập của họ một cách chính xác, sử dụng cùng thiết lập huấn luyện và kiến trúc mô hình.

A.1 Thí nghiệm CIFAR10

Huấn luyện Chúng tôi huấn luyện trên toàn bộ bộ dữ liệu CIFAR10 (trừ 10% được giữ lại làm tập validation) trong 300 epoch với batch-size 256. Là một optimizer, chúng tôi sử dụng Adam [30] với learning rate 0.0003 và regularization L2 là 2·10^-9. Quá trình huấn luyện của chúng tôi bắt đầu với việc khởi động tuyến tính learning rate trong 20 epoch. Độ chính xác validation được giám sát sau mỗi epoch và nếu nó không cải thiện trong 20 epoch liên tiếp, chúng tôi giảm learning rate với hệ số 0.8 và khôi phục mô hình hoạt động tốt nhất trước đó. Quá trình huấn luyện được dừng sớm nếu chúng tôi giảm năm lần.

Mô hình Các mô hình khác nhau mà chúng tôi sử dụng nói chung tuân theo kiến trúc và thiết lập tiêu chuẩn được biết đến từ tài liệu. Đối với ViT, chúng tôi sử dụng một biến thể nhỏ hơn của nó trên nhiệm vụ CIFAR. Nó bao gồm sáu lớp và tám attention head trong toàn bộ mạng. Tỷ lệ dropout được đặt thành 0.1 và hidden dimension được chọn là 512 ở tất cả các vị trí.

KD và HARD Sau các thí nghiệm ban đầu trên MNIST, chúng tôi quyết định sử dụng softmax temperature 5.0 cho tất cả thí nghiệm liên quan đến KD. Chúng tôi hơn nữa chỉ dựa vào loss KL-Divergence để tối ưu hóa mô hình của chúng tôi. Đối với các thí nghiệm với khung augmentation của chúng tôi, chúng tôi có cùng thiết lập như trước đó cho việc huấn luyện học sinh (KD) và các thiết lập riêng biệt cho việc huấn luyện augmentor. Ở đó chúng tôi có các thiết lập khác nhau tùy thuộc vào việc chúng tôi sử dụng augmentor VAE (hay augmentor Affine). Ở đó chúng tôi giảm batch-size xuống 160 (128) và learning-rate 0.0001 (0.05). Chúng tôi khởi tạo cả hai augmentor để thực hiện một phép biến đổi identity, tức là VAE được lấy từ Child [10] đã được pre-train. Các ngưỡng cho việc chuyển đổi được đặt là ℓmin = 10%(5%) và ℓmax = 60%(40%). Các chế độ huấn luyện được chuyển đổi nếu ngưỡng bị vượt qua trong 5 lần lặp liên tiếp. Cả λs và λt đều được đặt thành 1 cho các thí nghiệm. Đối với thí nghiệm ResNet101* → ResNet18, chúng tôi thấy một thiết lập khác nhau hiệu quả hơn một chút với ℓmin = 5% và ℓmax = 40% và một lần chuyển đổi chỉ xảy ra nếu ngưỡng bị vượt qua trong 10 lần lặp liên tiếp.

A.2 Thí nghiệm ImageNet

Huấn luyện Baseline Nói chung, tất cả các thí nghiệm ImageNet của chúng tôi tuân theo một thiết lập tương tự. Chúng tôi huấn luyện với batch-size 512 mẫu sử dụng optimizer Lion [8] với việc khởi động learning-rate tuyến tính đến một learning-rate ban đầu được định nghĩa. Sau đó, chúng tôi làm nguội learning-rate theo lịch trình cosine [36] với giá trị cuối cùng là 0. Quá trình huấn luyện chạy trong 200 epoch cho tất cả thí nghiệm ResNet18 và 150 epoch cho các thí nghiệm ViT-S. Trong suốt quá trình huấn luyện, độ chính xác validation được giám sát trên một tập heldout bao gồm các mẫu được chọn ngẫu nhiên từ tập huấn luyện, chiếm 1% tổng số mẫu. Hiệu suất validation được sử dụng để chọn epoch hoạt động tốt nhất trong suốt quá trình huấn luyện để đánh giá cuối cùng và các siêu tham số tốt nhất trong quá trình tìm kiếm lưới. Chúng tôi huấn luyện ở độ phân giải 224 pixel với việc thay đổi kích thước và cắt ngẫu nhiên, cũng như lật ngang ngẫu nhiên được áp dụng trong tất cả các lần huấn luyện. Tất cả các lần chạy huấn luyện được thực hiện với precision hỗn hợp tự động và tối ưu hóa 8bit [16].

Huấn luyện Học sinh Sau một tìm kiếm lưới, chúng tôi thấy rằng đối với huấn luyện học sinh độc lập, một tối ưu hóa với learning-rate 0.0001 với weight decay 0.1 cho học sinh ResNet18 và learning-rate 0.00005 với weight decay 0.001 cho học sinh ViT-S hoạt động tốt nhất. Đối với cả hai học sinh, chúng tôi áp dụng augmentation nhẹ trong quá trình huấn luyện với Mixup (α = 0.2) [61] và CutMix (α = 1.0) [59]. Đối với baseline ViT-S, chúng tôi thêm áp dụng Trivial-Augment [39] và xóa ngẫu nhiên các pixel từ hình ảnh đầu vào [66] với xác suất 0.1. Chúng tôi tối ưu hóa loss cross-entropy tiêu chuẩn với label-smoothing [51] bổ sung được trộn với hệ số 0.1.

KD và HARD Như được mô tả trong bài báo chính, cấu hình cho các thí nghiệm KD (bao gồm HARD) chủ yếu khác biệt trong việc lựa chọn augmentation, cũng như learning-rate và weight-decay. Các thí nghiệm KD thuần túy sử dụng Mixup (α = 1.0) và CutMix (α = 1.0) cũng như augmentation AugMix [24]. Softmax temperature được chọn là 1.0 trong các thí nghiệm trước đó và được giữ cho tất cả thí nghiệm. Learning-rate cho tất cả thí nghiệm KD và HARD được chọn thông qua tìm kiếm lưới là 0.0001 trong tất cả trường hợp và weight-decay là 0.001 trong hầu hết trường hợp, trừ các thí nghiệm HARD với học sinh ResNet18 nơi weight-decay 0.05 được sử dụng.

--- TRANG 15 ---
B Kết quả Chưng cất Kiến thức từ Tài liệu

Chúng tôi đã đánh giá (lại) các mô hình học sinh và giáo viên từ hai thí nghiệm KD có hiệu suất cao [6, 41] trong tài liệu trên cả tập test trong miền và ngoài miền.

[Bảng 5: Hiệu suất trong miền và ngoài miền cho hai thí nghiệm KD từ tài liệu]

--- TRANG 16 ---
[Phần này trống]
