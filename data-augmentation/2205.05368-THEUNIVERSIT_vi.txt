# 2205.05368.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/data-augmentation/2205.05368.pdf
# Kích thước tệp: 5595642 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
ĐẠI HỌC EDINBURGH

Các Mô hình Ngôn ngữ Tiền huấn luyện như
Những Người Tái chú thích

Chang Shu
Thạc sĩ Khoa học bằng Nghiên cứu
Khoa Triết học, Tâm lý học và Khoa học Ngôn ngữ
Đại học Edinburgh
2022

--- TRANG 2 ---
Tóm tắt
Tiếng ồn chú thích lan tràn trong các tập dữ liệu, nhưng việc xem xét lại thủ công một kho tài liệu có sai sót tốn thời gian và dễ xảy ra lỗi. Do đó, với kiến thức tiền nghiệm trong các Mô hình Ngôn ngữ Tiền huấn luyện và sự thống nhất mong đợi trên tất cả các chú thích, chúng tôi cố gắng giảm tiếng ồn chú thích trong kho tài liệu thông qua hai nhiệm vụ tự động: (1) Phát hiện Không nhất quán Chú thích cho biết độ tin cậy của các chú thích, và (2) Sửa chữa Lỗi Chú thích khắc phục các chú thích bất thường.

Chúng tôi điều tra cách thu được các biểu diễn chú thích nhạy cảm về mặt ngữ nghĩa từ các Mô hình Ngôn ngữ Tiền huấn luyện, mong đợi nhúng các ví dụ có chú thích giống hệt nhau vào các vị trí liền kề lẫn nhau ngay cả khi không có tinh chỉnh. Chúng tôi đề xuất một điểm tin cậy mới để tiết lộ khả năng không nhất quán chú thích dựa trên tính nhất quán lân cận. Sau đó, chúng tôi tinh chỉnh bộ phân loại dựa trên Mô hình Ngôn ngữ Tiền huấn luyện với xác thực chéo để sửa chữa chú thích. Bộ sửa chữa chú thích được mở rộng thêm với hai phương pháp: (1) gán nhãn mềm bằng Ước lượng Mật độ Kernel và (2) một mất mát tương phản đồng nghiệp xa mới.

Chúng tôi nghiên cứu tái chú thích trong trích xuất quan hệ và tạo ra một tập dữ liệu được xem xét lại thủ công mới, Re-DocRED, để đánh giá tái chú thích cấp tài liệu. Các điểm tin cậy được đề xuất cho thấy sự đồng thuận hứa hẹn với các bản xem xét lại của con người, đạt được Binary F1 là 93.4 và 72.5 trong việc phát hiện không nhất quán trên TACRED và DocRED tương ứng. Hơn nữa, các bộ phân loại nhận thức lân cận dựa trên học tập tương phản đồng nghiệp xa và nhãn không chắc chắn đạt được Macro F1 lên tới 66.2 và 57.8 trong việc sửa chữa chú thích trên TACRED và DocRED tương ứng. Những cải tiến này không chỉ mang tính lý thuyết: Thay vào đó, các tập huấn luyện được khử nhiễu tự động cho thấy cải thiện hiệu suất lên tới 3.6% cho các mô hình trích xuất quan hệ hiện đại nhất, và khung được đề xuất được mong đợi sẽ nhanh hơn hàng trăm lần so với các người tái chú thích con người theo kinh nghiệm.

--- TRANG 3 ---
Lời cảm ơn
Luận văn này được dành tặng cho cha tôi, Shengguo Shu. Tôi chúc cha một sinh nhật vui vẻ, và cảm ơn cha vì luôn ở bên.

Tôi vô cùng biết ơn các giáo viên hướng dẫn của mình, Giáo sư Bonnie Webber, Tiến sĩ Beatrice Alex và Andreas Grivas, vì đã đưa tôi vào dự án thú vị này và hỗ trợ liên tục. Tôi tin rằng họ là một số giáo viên hướng dẫn và nhà nghiên cứu NLP tốt nhất trên hành tinh, và thật vinh dự khi được làm việc với họ. Ngoài ra, tôi muốn cảm ơn cố vấn cá nhân của mình, Tiến sĩ Catherine Lai, vì sự giúp đỡ và lời khuyên trong suốt quá trình học thạc sĩ. Tôi cũng đánh giá cao Luxi He vì đã đọc và chỉnh sửa và Anda Zhou vì đã thảo luận trong suốt luận văn này.

Tôi cũng muốn bày tỏ lòng biết ơn với các giáo viên hướng dẫn trước đây của mình, Tiến sĩ Rui Zhang, Tiến sĩ Tao Yu, Tiến sĩ Jian Qiu, và Giáo sư Zhiyuan Liu, vì những hướng dẫn và lòng tốt trong quá khứ trong việc cung cấp cơ hội thực tập tại Đại học Penn State, Đại học Yale, Alibaba Cloud và Đại học Tsinghua. Tôi cũng đánh giá cao tất cả các cố vấn và đồng nghiệp trong các kỳ thực tập này, Peng Shi, Jie Zhou, Taiyan Li, Yusen Zhang và Xiangyu Dong.

Cuối cùng, tôi muốn nói lời cảm ơn sâu sắc nhất tới những người thân yêu của mình. Đó là một năm khốn khổ với tôi về mặt thể chất và tinh thần, và tôi chắc chắn sẽ không thể vượt qua nếu không có sự hỗ trợ và tình yêu vô tận của các bạn.

Đạo có thể được nói ra không phải là Đạo vĩnh cửu. Mặc dù những phát hiện trong luận văn này là tạm thời, tôi biết ơn vì niềm vui trong sáng mà cuộc khám phá này mang lại.

--- TRANG 4 ---
Tuyên bố
Tôi tuyên bố rằng luận văn này được tôi tự soạn thảo, công việc chứa đựng trong đây là của riêng tôi ngoại trừ các phần được nêu rõ khác trong văn bản, và công việc này chưa được nộp để lấy bất kỳ bằng cấp hay chứng chỉ nghề nghiệp nào khác ngoài những gì đã được nêu rõ.

(Chang Shu)

--- TRANG 5 ---
Mục lục
1 Giới thiệu 1
1.1 Động lực . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 Điều tra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.3 Đóng góp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.4 Cấu trúc Luận văn . . . . . . . . . . . . . . . . . . . . . . . . . . 5

2 Nghiên cứu Liên quan 7
2.1 Các Mô hình Ngôn ngữ Tiền huấn luyện . . . . . . . . . . . . . . . 7
2.1.1 Kiến thức Tiền nghiệm trong Mô hình Ngôn ngữ Tiền huấn luyện . . . . . . . . 8
2.1.2 Gợi ý cho Khả năng Chuyển giao Kiến thức . . . . . . . . . . . . 10
2.2 Tiếng ồn Chú thích . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.2.1 Phân tích Tiếng ồn Nhãn . . . . . . . . . . . . . . . . . . . 12
2.2.2 Học chịu Nhiễu . . . . . . . . . . . . . . . . . . . . . . . . 15
2.3 Cải thiện Chất lượng Chú thích . . . . . . . . . . . . . . . . . . . 16
2.3.1 Cải thiện Quy trình Chú thích . . . . . . . . . . . . . . . . . 16
2.3.2 Phát hiện Không nhất quán Chú thích . . . . . . . . . . . . . . 17
2.3.3 Sửa chữa Lỗi Chú thích . . . . . . . . . . . . . . . . . . . . 18

3 Nhiệm vụ và Dữ liệu 19
3.1 Tái chú thích trong Trích xuất Quan hệ . . . . . . . . . . . . . . . . 19
3.1.1 Định nghĩa Phát hiện Không nhất quán Chú thích . . . . . . . . . 20
3.1.2 Định nghĩa Sửa chữa Lỗi Chú thích . . . . . . . . . . . . . . 21
3.1.3 Cơ sở của Tái chú thích Tự động . . . . . . . . . . . . . . . . 21
3.2 Tập dữ liệu trong Trích xuất Quan hệ . . . . . . . . . . . . . . . . . 22
3.2.1 Tập dữ liệu Mục tiêu . . . . . . . . . . . . . . . . . . . . . 23
3.2.2 Tập dữ liệu Đã xem xét lại Hiện có . . . . . . . . . . . . . . . 25
3.2.3 Tập dữ liệu Đã xem xét lại Mới: Re-DocRED . . . . . . . . . . . 26

4 Phát hiện Không nhất quán Chú thích 28
4.1 Tổng quan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
4.2 Phương pháp luận . . . . . . . . . . . . . . . . . . . . . . . . . . 29
4.2.1 Các Mô hình Ngôn ngữ Tiền huấn luyện . . . . . . . . . . . . . 29
4.2.2 Phương pháp Biểu diễn Quan hệ . . . . . . . . . . . . . . . . 31
4.2.3 Tính nhất quán Lân cận . . . . . . . . . . . . . . . . . . . . 35
4.3 Đánh giá . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
4.4 Thí nghiệm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
4.4.1 Chi tiết Thực hiện . . . . . . . . . . . . . . . . . . . . . . . 38
4.4.2 Phương pháp Biểu diễn Quan hệ . . . . . . . . . . . . . . . . 41
4.4.3 Phân loại theo Thỏa thuận Lân cận . . . . . . . . . . . . . . . 42
4.5 Thảo luận . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
4.5.1 Sự đánh đổi của Biểu diễn Quan hệ . . . . . . . . . . . . . . . 44

5 Sửa chữa Lỗi Chú thích 47
5.1 Tổng quan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
5.2 Phương pháp luận . . . . . . . . . . . . . . . . . . . . . . . . . . 48
5.2.1 Xác thực Chéo . . . . . . . . . . . . . . . . . . . . . . . . 48
5.2.2 Gán nhãn Không chắc chắn . . . . . . . . . . . . . . . . . . . 50
5.2.3 Sửa chữa Nhận thức Lân cận . . . . . . . . . . . . . . . . . . 54
5.3 Đánh giá . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
5.4 Thí nghiệm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
5.4.1 Chi tiết Thực hiện . . . . . . . . . . . . . . . . . . . . . . . 62
5.4.2 KNN Không-shot so với Bộ sửa chữa Nơ-ron Tinh chỉnh . . . . . . 64
5.4.3 Gán nhãn Không chắc chắn . . . . . . . . . . . . . . . . . . . 66
5.4.4 Bộ phân loại Nhận thức Lân cận . . . . . . . . . . . . . . . . 66
5.4.5 Học tập Tương phản với Không chắc chắn . . . . . . . . . . . . . 67
5.4.6 Học trên Tập Huấn luyện Đã khử nhiễu . . . . . . . . . . . . . . 70
5.5 Thảo luận . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
5.5.1 Chất lượng Xem xét lại của Bộ Tái chú thích Tự động . . . . . . . . 72
5.5.2 Hiệu quả của Bộ Tái chú thích Tự động . . . . . . . . . . . . . . 75

6 Kết luận và Hướng Tương lai 77
6.1 Kết luận . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
6.2 Hướng Tương lai . . . . . . . . . . . . . . . . . . . . . . . . . . . 78

--- TRANG 6 ---
Thư mục tài liệu tham khảo 81

--- TRANG 7 ---
Chương 1
Giới thiệu

Tiếng ồn chú thích lan tràn trong các tập dữ liệu và trở nên ngày càng có vấn đề khi các phương pháp dựa trên dữ liệu được tích hợp ngày càng nhiều vào Xử lý Ngôn ngữ Tự nhiên (NLP). Luận văn này là nghiên cứu đầu tiên tận dụng kiến thức tiền nghiệm trong các Mô hình Ngôn ngữ Tiền huấn luyện để phát hiện tiếng ồn chú thích và sự không nhất quán cũng như sửa chữa các lỗi chú thích. Chúng tôi giới thiệu động lực chính đằng sau dự án này và phác thảo các điều tra mà chúng tôi đã tiến hành để tiếp cận vấn đề này. Chúng tôi cũng tóm tắt những đóng góp chính và nội dung chính của mỗi chương trong luận văn.

1.1 Động lực

Những thập kỷ gần đây đã chứng kiến những thay đổi sâu sắc trong nghiên cứu NLP từ các phương pháp tượng trưng sang các kỹ thuật thống kê, và sau đó là các phương pháp nơ-ron (Khurana et al., 2017). Nghiên cứu NLP sớm chủ yếu dựa vào một tập hợp hữu hạn các quy tắc viết tay phản ánh kiến thức ngôn ngữ học phổ biến. Ngược lại, mô hình NLP mới nhất liên quan đến việc để các mô hình nơ-ron học kiến thức ngôn ngữ học tiềm ẩn từ lượng lớn dữ liệu. Khi các phương pháp dựa trên dữ liệu thống trị nghiên cứu NLP, tầm quan trọng của chất lượng chú thích ngày càng rõ ràng. Tuy nhiên, hầu hết các tiến bộ gần đây trong NLP vẫn tập trung vào phát triển các mô hình với khả năng học biểu diễn nâng cao, đánh giá thấp sự suy giảm hiệu suất mô hình do tiếng ồn chú thích trong dữ liệu huấn luyện (Larson et al., 2020; Khayrallah and Koehn, 2018) và hướng sai lầm của đánh giá mô hình do dữ liệu kiểm tra có sai sót (Northcutt et al., 2021a). Lý do chính đằng sau hiện tượng này là việc chú thích một tập dữ liệu tốn thời gian, chi phí cao và tốn nhiều lao động, cũng như việc xem xét lại các tập dữ liệu có nhiễu và/hoặc không nhất quán. Do đó, một bộ tái chú thích tự động có thể giảm một phần chi phí lao động hoặc thậm chí thay thế hoàn toàn lao động con người sẽ có lợi cho lĩnh vực NLP.

Sự gia tăng của các Mô hình Ngôn ngữ Tiền huấn luyện (PLMs) là một trong những cuộc cách mạng quan trọng nhất đã xuất hiện trong kỷ nguyên NLP nơ-ron (Qiu et al., 2020; Min et al., 2021). Thay vì học tập cụ thể theo lĩnh vực, PLMs trước tiên được tiền huấn luyện không giám sát trên kho tài liệu quy mô lớn, và sau đó được tinh chỉnh trên các nhiệm vụ hạ nguồn. Các nghiên cứu gần đây cho thấy giai đoạn tiền huấn luyện trao cho PLMs kiến thức thường thức phong phú (Peters et al., 2019; Davison et al., 2019; Jiang et al., 2020a,b) và kiến thức ngôn ngữ học (Clark et al., 2019,?; Liu et al., 2019a; Chi et al., 2020; Ettinger, 2020). Kiến thức tiền nghiệm trong PLMs đã được chứng minh là linh hoạt trong thực tế. Ví dụ, PLMs có thể được sử dụng trực tiếp để đánh giá việc sinh văn bản (Zhang et al., 2020; Sellam et al., 2020) và thăm dò kiến thức thực tế (Peters et al., 2019). Xét đến tính chất hấp dẫn của PLMs, chúng tôi tò mò liệu chúng có thể đóng góp vào tái chú thích tự động hay không.

Động lực chính và sự mới lạ của dự án này liên quan đến việc áp dụng các Mô hình Ngôn ngữ Tiền huấn luyện làm bộ tái chú thích để cải thiện chất lượng chú thích với chi phí giảm và kết quả cạnh tranh. Như được lập luận bởi Dickinson và Meurers (2003), các ví dụ có nhãn khác nhau nhưng xuất hiện trong các ngữ cảnh rất tương tự có khả năng là những không nhất quán hoặc lỗi chú thích. Trùng hợp thay, PLMs nổi tiếng với khả năng xuất sắc trong việc thu được nhúng ngữ cảnh hóa. Do đó, ý tưởng chính của việc phát hiện hoặc sửa chữa các chú thích khác biệt với PLMs là so sánh nhãn của ví dụ mục tiêu với các ví dụ khác được nhúng trong khu vực lân cận của nó.

1.2 Điều tra

Chúng tôi là những người đầu tiên nghiên cứu toàn diện tiềm năng của PLMs trong tái chú thích dữ liệu sử dụng cả tập dữ liệu trích xuất quan hệ cấp câu và cấp tài liệu, TACRED (Zhang et al., 2017a) và DocRED (Yao et al., 2019b). Trích xuất quan hệ là nhiệm vụ xác định quan hệ giữa hai thực thể trong ngữ cảnh – một được gọi là chủ thể, một khác là khách thể. Chúng tôi đang đánh giá các bộ tái chú thích cho trích xuất quan hệ cấp câu sử dụng hai tập dữ liệu được tạo ra từ TACRED — TACRev (Alt et al., 2020) và Re-TACRED (Stoica et al., 2021) với các bản xem xét lại của con người. Để đánh giá các bộ tái chú thích trong trích xuất quan hệ cấp tài liệu, chúng tôi tự tái chú thích một tập con của DocRED, để tạo ra một tập dữ liệu xem xét lại con người mới được chú thích với các quan hệ cấp tài liệu.

Nhiệm vụ tái chú thích bao gồm hai bước: Phát hiện Không nhất quán Chú thích và Sửa chữa Lỗi Chú thích (Hình 1.1). Phát hiện Không nhất quán Chú thích (AID) đánh giá tính nhất quán của mỗi chú thích đã cho so với các chú thích khác trong ngữ cảnh tương tự. Sửa chữa Lỗi Chú thích (AEC) đề xuất chú thích phù hợp cho ví dụ được xác định là bất thường.

Hình 1.1: Tổng quan về Bộ phát hiện Không nhất quán Chú thích (AID) và Bộ sửa chữa Lỗi Chú thích (AEC) được đề xuất của chúng tôi trong ngữ cảnh trích xuất quan hệ. Bộ phát hiện Không nhất quán Chú thích cho biết liệu chú thích quan sát có nhất quán với các chú thích khác hay không. Bộ sửa chữa Lỗi Chú thích khắc phục các chú thích được xác định là bị gán nhãn sai.

Phát hiện Không nhất quán Chú thích dựa vào tính chất thuận lợi của các phương pháp biểu diễn và độ nhạy và đặc hiệu mong muốn của bộ phát hiện nhiễu. Thay vì tinh chỉnh PLMs, chúng tôi trước tiên nghiên cứu các kỹ thuật gợi ý (Liu et al., 2021a) và sửa đổi đầu vào (Zhou và Chen, 2021) khác nhau để thu được biểu diễn quan hệ thông tin và có thể phân biệt của mỗi trường hợp từ PLMs. Sau khi tối ưu hóa bộ nhúng quan hệ, chúng tôi tận dụng thuật toán K-Nearest Neighbour (Mucherino et al., 2009) để xác định các chú thích có thể không nhất quán dựa trên hình học cục bộ của các ví dụ được chú thích trong không gian nhúng. Hơn nữa, chúng tôi đề xuất một điểm tin cậy mới dựa trên khoảng cách xem xét kết hợp các nhãn trong vùng lân cận và phân bố toàn cục của lớp được gán của ví dụ truy vấn. Các thí nghiệm tiết lộ rằng các gợi ý không đủ thông tin và quá thiên vị dẫn đến hiệu suất giảm trong việc phát hiện không nhất quán. Theo kinh nghiệm, các điểm tin cậy được đề xuất của chúng tôi kết hợp với các gợi ý quan hệ cho thấy sự đồng thuận hứa hẹn với các bản xem xét lại của con người trong Re-TACRED và TACRev, đạt được điểm F1 nhị phân là 93.4 và 72.5 trong việc phát hiện không nhất quán trên TACRED và DocRED tương ứng.

Mặt khác, Bộ sửa chữa Lỗi Chú thích dựa trên PLM được tinh chỉnh bằng xác thực chéo Stone (1977); Tibshirani (1996); Allen (1974) để đưa ra các quyết định xem xét lại chính xác. Bộ sửa chữa tự động vani gồm các PLMs được xếp chồng bởi một bộ phân loại quan hệ nơ-ron. Chúng tôi cải thiện quá trình học tập của các mô hình AEC với nhãn không chắc chắn và học tập nhận thức lân cận. Lấy cảm hứng từ các nhãn mềm (Thiel, 2008; Nguyen et al., 2014; Liu et al., 2017; Zhao et al., 2014; Algan và Ulusoy, 2021), chúng tôi xây dựng các nhãn với sự không chắc chắn của các mẫu dựa trên hàng xóm của chúng hoặc mật độ xác suất ước tính liên quan đến mỗi lớp nhãn để thay thế các nhãn cứng quá tự tin. Cụ thể, một phương pháp thay thế một phần nhãn cứng bằng nhãn đa số trong số các hàng xóm của chúng, và một cách khác tạo ra các vector nhãn mềm từ mật độ kernel ước tính tương ứng với mỗi lớp. Chúng tôi cũng khám phá hai phương pháp để tăng cường bộ phân loại quan hệ với kiến thức hàng xóm: (1) bộ mã hóa Transformer nhận thức thứ hạng (Vaswani et al., 2017) để thu được nhúng quan hệ chú ý đến hàng xóm của chúng, và (2) học tập tương phản đồng nghiệp xa (Khosla et al., 2020) để bao gồm thông tin hàng xóm vào hàm mất mát. Ngoài việc lấy mẫu các ví dụ tích cực và tiêu cực trong batch, khung học tập tương phản đồng nghiệp xa chọn các ví dụ tích cực từ hàng xóm bằng khoảng cách đồng nghiệp được đề xuất của chúng tôi được tính toán bằng cách kết hợp tần suất xuất hiện cùng nhau và khoảng cách Euclidean bình phương của chúng. Các kết quả thực nghiệm của chúng tôi cho thấy rằng bộ sửa chữa chú thích nhận thức hàng xóm được huấn luyện với học tập tương phản đồng nghiệp xa đạt được macro F1 lên tới 66.2 trên TACRED và 57.8 trên DocRED. Hơn nữa, việc huấn luyện các mô hình trích xuất quan hệ hiện đại nhất trên các tập huấn luyện được khử nhiễu tự động bởi bộ sửa chữa chú thích được tối ưu hóa của chúng tôi dẫn đến cải thiện tối đa của micro F1 là 3.5% trên TACRED, 3.4% trên TACRev, và 1.1% trên DocRED.

Cuối cùng, chúng tôi phát hiện ra rằng Bộ sửa chữa Lỗi Chú thích được đề xuất của chúng tôi có thể tự động xem xét lại các chú thích nhanh hơn hàng trăm lần so với các người xem xét lại con người với độ tin cậy chấp nhận được. Tương tự, một bộ phát hiện không nhất quán chú thích có thể phát hiện các chú thích đáng ngờ thậm chí nhanh hơn hàng chục nghìn lần so với con người. Do đó, chúng tôi tin rằng việc áp dụng các bộ tái chú thích tự động trước các bản xem xét lại thủ công hoặc thậm chí hoàn toàn dựa vào kết quả xem xét lại của chúng sẽ cải thiện đáng kể chất lượng dữ liệu và NLP dựa trên dữ liệu.

1.3 Đóng góp

Các đóng góp chính của luận văn là:

• Tập dữ liệu RE-DocRED: Để nghiên cứu chất lượng chú thích và đánh giá bộ tái chú thích tự động được đề xuất của chúng tôi trong các nhiệm vụ cấp tài liệu, chúng tôi đã xây dựng một tập dữ liệu mới, Re-DocRED, bằng cách xem xét lại 411 ví dụ trong tập dữ liệu trích xuất quan hệ cấp tài liệu, DocRED. Đây là tập dữ liệu RE được tái chú thích đầu tiên ở cấp tài liệu và sẽ có lợi cho nghiên cứu tương lai về tiếng ồn chú thích.

• PLMs cho Tái chú thích: Chúng tôi là những người đầu tiên tận dụng kiến thức tiền nghiệm trong các Mô hình Ngôn ngữ Tiền huấn luyện để phát hiện không nhất quán chú thích và sửa chữa lỗi chú thích. Các phát hiện của chúng tôi chứng minh rằng chuyên môn thực tế và ngôn ngữ học trong các Mô hình Ngôn ngữ Tiền huấn luyện có thể áp dụng được cho tái chú thích tự động ngay cả trong kịch bản không-shot.

• Điều tra Gợi ý: Chúng tôi nghiên cứu toàn diện tác động của các dạng gợi ý và sửa đổi đầu vào khác nhau đến các nhiệm vụ tái chú thích. Theo kinh nghiệm, chúng tôi phát hiện ra rằng các gợi ý có ngữ cảnh không đủ hoặc ý nghĩa mạnh mẽ sẽ đánh lạc hướng các bộ tái chú thích tự động.

• Điểm Tin cậy: Chúng tôi đề xuất một điểm tin cậy mới được tính toán chung bởi khoảng cách và độ tin cậy của các hàng xóm. Độ tin cậy của các hàng xóm được xấp xỉ bởi mật độ kernel ước tính của các lớp được gán của chúng. Thí nghiệm chỉ ra rằng điểm tin cậy rất hiệu quả trong việc phát hiện các chú thích không nhất quán tiềm năng.

• Nhãn Không chắc chắn dựa trên Hàng xóm: Chúng tôi xây dựng các nhãn với sự không chắc chắn dựa trên phân bố của các hàng xóm để tránh bộ sửa chữa chú thích phụ thuộc quá nhiều vào các nhãn cứng được quan sát. Cả việc thay thế nhãn dựa trên K-Nearest Neighbours và nhãn mềm dựa trên Ước lượng Mật độ Kernel đều cho thấy cải thiện thuyết phục đối với hiệu suất bộ sửa chữa.

• Học tập Tương phản Đồng nghiệp Xa: Chúng tôi phát triển một khung học tập tương phản mới với các ví dụ tích cực tăng cường được chọn bởi khoảng cách đồng nghiệp được định nghĩa mới của chúng tôi. Dựa trên tần suất xuất hiện cùng nhau và khoảng cách của chúng, chúng tôi công thức hóa khoảng cách đồng nghiệp để chọn các ví dụ tích cực đáng tin cậy và có giá trị nhất từ các hàng xóm của các ví dụ truy vấn. Kết quả chứng minh sức mạnh của việc kết hợp mất mát tương phản đồng nghiệp xa và mất mát cross-entropy, đặc biệt khi được sử dụng kết hợp với nhãn mềm dựa trên Ước lượng Mật độ Kernel.

1.4 Cấu trúc Luận văn

Tóm tắt các chương tiếp theo của luận văn được liệt kê như sau:

• Chương 2 giới thiệu nghiên cứu trước đây về (1) kiến thức tiền nghiệm trong các Mô hình Ngôn ngữ Tiền huấn luyện và gợi ý cho khả năng chuyển giao của nó, (2) phân tích tiếng ồn chú thích và các phương pháp học chịu nhiễu, và (3) các nỗ lực cải thiện chất lượng chú thích thủ công và tự động.

• Chương 3 định nghĩa hai nhiệm vụ được điều tra, Phát hiện Không nhất quán Chú thích và Sửa chữa Lỗi Chú thích trong ngữ cảnh trích xuất quan hệ, và mô tả các tập dữ liệu cho thí nghiệm.

• Chương 4 trình bày nghiên cứu thực nghiệm của chúng tôi về các phương pháp biểu diễn quan hệ và các bộ phân loại nhị phân dựa trên tính nhất quán hàng xóm cho Phát hiện Không nhất quán Chú thích.

• Chương 5 mô tả điều tra của chúng tôi về Sửa chữa Lỗi Chú thích dựa trên xác thực chéo, và hai cải tiến, nhãn không chắc chắn và nhận thức hàng xóm.

• Chương 6 kết luận các phát hiện của chúng tôi trong suốt nghiên cứu và trình bày một số hướng hấp dẫn để khám phá trong tương lai.

--- TRANG 8 ---
Chương 2
Nghiên cứu Liên quan

Ba chủ đề quan trọng nhất đối với luận văn này là: (1) Các Mô hình Ngôn ngữ Tiền huấn luyện; (2) xác định tiếng ồn và không nhất quán có thể phát sinh trong chú thích, tập trung vào chú thích các quan hệ thay vì chỉ chú thích các chuỗi đơn giản; và (3) cải thiện chú thích các quan hệ thông qua việc tự động hóa các nỗ lực nhận biết và sửa chữa các token có nhiễu hoặc không nhất quán. Chúng tôi sẽ đề cập đến nghiên cứu trước đây về từng chủ đề này trong phần riêng của nó, để làm rõ và biện minh cho công việc mà chúng tôi đã thực hiện ở đây.

2.1 Các Mô hình Ngôn ngữ Tiền huấn luyện

Các nhiệm vụ Xử lý Ngôn ngữ Tự nhiên khác nhau tập trung vào một lĩnh vực độc lập, nhưng một số nhiệm vụ có liên quan nội tại. Ví dụ, trong khi Phân tích Cú pháp Phụ thuộc (Dozat và Manning, 2017; Kübler et al., 2009; Nivre, 2005; Li et al., 2018) tạo ra các cây phụ thuộc cú pháp, và trong khi Gán nhãn Vai trò Ngữ nghĩa (SRL) (Palmer et al., 2010; Màrquez et al., 2008) dự đoán cấu trúc vị từ-đối số tiềm ẩn, cả hai nhiệm vụ đều yêu cầu các mô hình có khả năng cơ bản để nắm bắt cấu trúc ngữ pháp của ngữ cảnh đã cho. Do đó, mô hình trong NLP gần đây đã chuyển từ thiết kế mô hình cụ thể theo nhiệm vụ sang một pipeline tiền huấn luyện và tinh chỉnh.

Thông thường, một mô hình sequence-to-sequence với nhiều tham số được huấn luyện trên một kho tài liệu khổng lồ để thực hiện các nhiệm vụ mô hình hóa ngôn ngữ hoặc các nhiệm vụ tái tạo văn bản theo cách học không giám sát trong giai đoạn tiền huấn luyện và sau đó các tham số mô hình được tinh chỉnh tinh tế với dữ liệu cụ thể theo nhiệm vụ và các mục tiêu học tập như trích xuất quan hệ và phân tích cảm xúc. Dựa trên đặc điểm của nó, những mô hình đó được biết đến rộng rãi như các Mô hình Ngôn ngữ Tiền huấn luyện (PLMs). Theo kịch bản sử dụng dự định, PLMs có thể được chia thành PLMs mục đích chung và PLMs mục đích đặc biệt. PLMs mục đích chung thường được tiền huấn luyện trên kho tài liệu chung, chẳng hạn như Wikipedia, và với các nhiệm vụ tiền huấn luyện cơ bản và kiến trúc mô hình (Devlin et al., 2019; Joshi et al., 2020; Brown et al., 2020; Raffel et al., 2020; Lewis et al., 2020; Yang et al., 2019; Liu et al., 2019b). Những PLMs đó được sử dụng rộng rãi hơn và thực hiện đồng đều trên các nhiệm vụ NLP đa dạng. Tuy nhiên, đối với những nhiệm vụ yêu cầu kiến thức chuyên môn, chúng có thể không có khả năng thực hiện hiệu quả vì thiếu tiền huấn luyện cụ thể theo lĩnh vực. Do đó, PLMs mục đích đặc biệt thường được tiền huấn luyện trên kho tài liệu chuyên nghiệp (Lee et al., 2020; Feng et al., 2020; Li et al., 2020b), hoặc tinh chỉnh kiến trúc mô hình chung của mô hình mục đích chung (Peters et al., 2019; Zhang et al., 2019) hoặc thêm các nhiệm vụ tiền huấn luyện phụ trợ (Soares et al., 2019).

Mục tiêu của chúng tôi là hỗ trợ giảm thiểu tiếng ồn chú thích và cải thiện chất lượng của các tập dữ liệu. Luận văn này là nghiên cứu đầu tiên điều tra toàn diện các vai trò có thể mà PLMs có thể đóng trong tái chú thích tự động. Để xem xét lại bất kỳ chú thích hiện có nào được trình bày trong các tập dữ liệu, người ta cần có hai loại kiến thức tiền nghiệm: (1) Kiến thức thực tế và ngôn ngữ học để xem xét lại các chú thích không tuân thủ thông thức; (2) Kiến thức về phân bố chú thích tổng thể để phát hiện các chú thích xuất hiện không nhất quán với hầu hết các chú thích khác. Các nghiên cứu gần đây tiết lộ rằng hầu hết PLMs thực sự sở hữu kiến thức thực tế và ngôn ngữ học chung dồi dào ngay cả khi không có bất kỳ tinh chỉnh hoặc tiêm kiến thức nào, chẳng hạn như Peters et al. (2019) và Hewitt và Manning (2019b). Do đó, chúng tôi tập trung vào phát triển khung để tái chú thích tự động dựa trên PLMs mục đích chung. Phần tiếp theo (Phần 2.1.1) thảo luận về những tiến bộ gần đây trong việc thăm dò kiến thức tiền nghiệm của PLMs mục đích chung. Phần 2.1.2 sau đó thảo luận về học tập dựa trên gợi ý phổ biến để rút ra kiến thức quan tâm từ PLMs và áp dụng nó vào các nhiệm vụ hạ nguồn. Cả hai hướng đều thúc đẩy các phương pháp mà chúng tôi áp dụng để thu được các biểu diễn phong phú thông tin và có tính phân biệt cao của các ví dụ trong tập dữ liệu để tái kiểm tra tự động.

2.1.1 Kiến thức Tiền nghiệm trong Mô hình Ngôn ngữ Tiền huấn luyện

Giống như những con mọt sách có thể trở thành bách khoa toàn thư thông qua việc đọc nhiều, PLMs cũng có khả năng thu được kiến thức ngôn ngữ học và thông thức mở rộng thông qua học không giám sát trên kho tài liệu chung quy mô lớn. Nhiều điều tra thực nghiệm và phân tích đã được tiến hành gần đây để biện minh cho trực giác này về việc thăm dò và định lượng kiến thức tiền nghiệm cơ bản trong PLMs. Những phát hiện đó chắc chắn tạo ra sự tin tưởng đủ để tin tưởng và tận dụng kiến thức tiền nghiệm trong PLMs để xem xét lại các nhãn hiện có trong các tập dữ liệu có sai sót.

Thăm dò thực tế khám phá kiến thức ẩn từ PLMs lần đầu tiên được đề xuất bởi Peters et al. (2019). Để khám phá các giải pháp cho nhiệm vụ này, họ giới thiệu khung LAMA (Phân tích Mô hình Ngôn ngữ), dự định thăm dò kiến thức thông thức trong PLMs. Theo khung được đề xuất, các mục trong cơ sở kiến thức được chuyển đổi thành các câu cloze với các mẫu trong đó các quan hệ hoặc đề cập thực thể được thay thế bằng mask. Sau đó, kiến thức tiền nghiệm trong PLMs được đánh giá bằng hiệu suất của chúng trong việc hoàn thành các token bị thiếu. Ví dụ, cho câu cloze "Newton was born in [MASK]", nếu PLMs có thể xếp hạng "UK" hoặc "England" cao hơn để điền vào chỗ trống, chúng sẽ được coi là có nhiều kiến thức thực tế hơn. Kết quả thực nghiệm của họ thuyết phục rằng PLMs không có tinh chỉnh vẫn chứa kiến thức quan hệ đáng tin cậy và có thể xử lý các câu hỏi miền mở dựa trên kiến thức thực tế của chúng. Davison et al. (2019) cũng phát triển một khung tương tự để khai thác kiến thức thông thức từ PLMs. Họ trước tiên rút ra các câu được mask từ các triple quan hệ và sau đó tận dụng PLMs để xếp hạng tính hợp lệ của một triple với thông tin tương hỗ point-wise ước tính giữa hai đề cập thực thể mà không cần tinh chỉnh. X-FACTOR bởi Jiang et al. (2020a) cố gắng tổng quát hóa thăm dò thực tế kiểu cloze cho tình huống đa ngôn ngữ bằng cách soạn các biến thể của mẫu cloze trong 23 ngôn ngữ khác nhau về mặt loại hình và đề xuất một cải tiến cho việc thăm dó kiến thức đa ngôn ngữ từ PLMs dựa trên chuyển đổi mã. Các thí nghiệm của họ chứng minh khả năng tiếp cận đa ngôn ngữ của kiến thức thực tế trong PLM. Công việc của Jiang et al. (2020b) tiếp tục tối ưu hóa quy trình truy vấn dựa trên cloze để ước tính chính xác hơn kiến thức thực tế trong PLMs bằng cách thay thế các gợi ý được tạo thủ công bằng một pipeline tự động tạo ra các mẫu dựa trên paraphrase và khai thác ý định. Họ gợi ý rằng chất lượng hoặc khả năng tương thích của các gợi ý có thể tác động đến hiệu suất của các bộ thăm dò kiến thức của PLMs.

Ngoài kiến thức thông thức, PLMs có thể hiểu một lượng đáng kể các hiện tượng ngôn ngữ học chỉ bằng cách tiền huấn luyện trên văn bản thuần túy quy mô lớn mà không có chú thích ngôn ngữ học rõ ràng. Hewitt và Manning (2019b) chứng minh kiến thức cú pháp trong PLMs bằng cách cho thấy rằng các cây cú pháp được nhúng một cách nhất quán trong không gian biểu diễn của PLMs theo các biến đổi tuyến tính nhất định. Clark et al. (2019) cũng xác nhận sự tồn tại của kiến thức cú pháp trong PLMs thông qua phân tích thực nghiệm của phân bố attention, hiển thị mô hình chỉ đạo đối tượng của giới từ và động từ, determiners của danh từ hoặc các đề cập đồng quy chiếu. Tenney et al. (2019) chỉ ra rằng PLMs có quy trình tiềm ẩn xử lý thông tin ngôn ngữ học tương tự như một pipeline NLP thông thường của gán nhãn part-of-speech, phân tích cú pháp phụ thuộc, nhận dạng thực thể có tên, và sau đó đồng quy chiếu. Thông qua mười sáu nhiệm vụ thăm dò khác nhau, Liu et al. (2019a) điều tra các tác động chi tiết của các nhiệm vụ tiền huấn luyện lên kiến thức ngôn ngữ học được học bởi PLMs và chứng minh rằng PLMs có kiến thức về phụ thuộc ngữ nghĩa và giải quyết đồng quy chiếu. Dựa trên phân tích không gian nhúng từ bởi PLMs đa ngôn ngữ, Chi et al. (2020) khám phá các quan hệ ngữ pháp phổ quát qua các ngôn ngữ được nắm bắt bởi PLMs. Ettinger (2020) đề xuất một bộ chẩn đoán mới được rút ra từ các thí nghiệm ngôn ngữ con người để kiểm tra kiến thức ngôn ngữ học trong PLMs. Họ gợi ý rằng PLMs có thể xác định mạnh mẽ các hoàn thành tốt từ những cái xấu liên quan đến danh mục hoặc vai trò chung và truy xuất hypernyms danh từ nhưng hơi do dự so với các đánh giá viên con người.

2.1.2 Gợi ý cho Khả năng Chuyển giao Kiến thức

Học tập dựa trên gợi ý, một phương pháp đơn giản để áp dụng kiến thức tiền nghiệm trong PLMs vào các nhiệm vụ hạ nguồn, gần đây đã thu hút nhiều sự chú ý hơn vì nó hỗ trợ học zero-shot. Thay vì thích nghi tốn thời gian của tất cả các tham số trong PLMs theo mục tiêu học tập hạ nguồn, các phương pháp dựa trên gợi ý tái công thức hóa các nhiệm vụ hạ nguồn thành các nhiệm vụ hoàn thành ngôn ngữ với các gợi ý được thiết kế tốt. Liu et al. (2021a) soạn một cuộc khảo sát toàn diện và có hệ thống về các phát triển gần đây của các kỹ thuật dựa trên gợi ý, và Saunshi et al. (2021) công thức hóa một khung toán học vững chắc để giải thích tại sao các phương pháp dựa trên gợi ý sẽ hoạt động cho các nhiệm vụ hạ nguồn. Dựa trên cuộc khảo sát, chúng tôi giới thiệu học tập dựa trên gợi ý tạo thành cơ sở cho việc khám phá của chúng tôi về việc lấy PLMs làm bộ tái chú thích tự động để thúc đẩy chất lượng của các tập dữ liệu từ hai khía cạnh: (1) các loại gợi ý và (2) các ứng dụng của chúng liên quan đến nhiệm vụ tái chú thích các tập dữ liệu trích xuất quan hệ của chúng tôi.

Theo dạng của chúng, các gợi ý có thể được chia thành hai kiểu: kiểu cloze và kiểu prefix. Các gợi ý kiểu cloze (Cui et al., 2021; Petroni et al., 2019b) yêu cầu PLMs điền vào khoảng trống được mask có ý định trong một câu hoàn thành một phần, và dự đoán được thực hiện bằng các quyết định điền hoặc xếp hạng được thực hiện bởi PLMs. Nó thường được sử dụng để xử lý các nhiệm vụ phân loại có các hạn chế hoặc mục tiêu rõ ràng, chẳng hạn như trích xuất quan hệ hoặc phân tích cảm xúc. Ngược lại, các gợi ý kiểu prefix (Li và Liang, 2021; Lester et al., 2021) khuyến khích PLMs tạo ra một tiếp tục của câu đã cho. Nó phù hợp hơn cho những nhiệm vụ liên quan đến việc sinh văn bản, chẳng hạn như tóm tắt văn bản hoặc trả lời câu hỏi. Các phương pháp được sử dụng để soạn gợi ý có thể được phân loại thành các gợi ý dựa trên mẫu thủ công và gợi ý được tạo tự động. Loại gợi ý đầu tiên được tạo bởi các mẫu được tạo thủ công dựa trên hiểu biết của con người về ngữ cảnh của nhiệm vụ. Ví dụ, vì con người biết rằng các đề cập quan hệ thường xuất hiện giữa các đề cập chủ thể và đối tượng trong văn bản, việc chèn token mask giữa các đề cập chủ thể và đối tượng sẽ là cách hợp lý nhất để soạn các gợi ý dạng cloze truy xuất quan hệ có thể được giữ trong ngữ cảnh. Loại gợi ý thứ hai được tạo tự động, tìm kiếm hoặc tinh chỉnh với một phần nhỏ dữ liệu hạ nguồn. Xét đến tính linh hoạt của biểu đạt trong ngôn ngữ tự nhiên, các gợi ý được tạo có thể rút ra kiến thức liên quan đến nhiệm vụ từ PLMs một cách chính xác hơn để tăng cường hiệu suất hạ nguồn. Tuy nhiên, đi kèm với ưu điểm này, một mẫu tự động có thể làm cho các mô hình tương đối dễ bị overfitting so với một mẫu được tạo thủ công vì các ý nghĩa mạnh mẽ trong gợi ý có thể dễ dàng đánh lạc hướng PLMs. Nhược điểm này đặc biệt đáng chú ý khi chúng ta áp dụng các phương pháp dựa trên gợi ý để kiểm tra lại các chú thích có vấn đề.

Khi chúng tôi điều tra bộ tái chú thích tự động trong ngữ cảnh chú thích quan hệ, các ứng dụng gợi ý trong trích xuất quan hệ có thể rất có liên quan. Trích xuất quan hệ là nhiệm vụ dự đoán quan hệ cơ bản giữa các thực thể chủ quan và khách quan đã cho trong ngữ cảnh. Chen et al. (2021) xác định hai thách thức chính của việc áp dụng phương pháp dựa trên gợi ý vào nhiệm vụ trích xuất quan hệ: (1) khó khăn trong kỹ thuật gợi ý do không gian nhãn của các quan hệ được mở rộng, và (2) tầm quan trọng khó nắm bắt của các token xuất hiện trong ngữ cảnh. Để vượt qua hai trở ngại này, họ phát triển khung KnowPrompt xây dựng gợi ý có thể học được cho trích xuất quan hệ với các từ mẫu ảo và từ trả lời. Họ tiếp tục tiêm kiến thức về thực thể và quan hệ thông qua việc đánh dấu các khoảng thực thể bằng cách bao bọc các đề cập thực thể với các dấu hiệu đặc biệt chẳng hạn như [E]. Tương ứng, Han et al. (2021) tận dụng kỹ thuật soạn gợi ý để tạo thành gợi ý với thông tin loại thực thể dồi dào. Kỹ thuật soạn gợi ý là tổng hợp gợi ý cuối cùng bằng cách tổng hợp một số gợi ý phụ dựa trên các quy tắc logic. Ví dụ, để trích xuất quan hệ giữa "Google" và "Alphabet" với ngữ cảnh "Google became a subsidiary of Alphabet", chúng ta có thể soạn gợi ý hoàn chỉnh là "The [MASK] Google [MASK] the [MASK] Alphabet" từ các gợi ý phụ "The [MASK] Google", "The [MASK] Alphabet", " Google [MASK] Alphabet". Hai gợi ý phụ đầu tiên cho phép PLMs liên kết kiến thức bổ sung về thực thể "Google" và "Alphabet" một cách độc lập trước khi đoán quan hệ của chúng.

2.2 Tiếng ồn Chú thích

Tiếng ồn chú thích dưới dạng nhãn không nhất quán hoặc không chính xác dường như phổ biến trong hầu hết các tập dữ liệu máy học. Kusendová (2005) cho rằng tiếng ồn có thể phát sinh từ các hướng dẫn không rõ ràng hoặc không đủ, ngữ cảnh không rõ ràng, thành kiến từ phía người chú thích, hoặc sự khác biệt so với những gì người viết hướng dẫn mong đợi. Ngày nay, việc huấn luyện các mô hình mạng nơ-ron thường yêu cầu lượng lớn dữ liệu được chú thích, vì vậy vấn đề này đã trở nên nổi bật ngày càng trong máy học. Những người chú thích của các tập dữ liệu này thường thiếu chuyên môn trong lĩnh vực liên quan vì phương pháp được sử dụng phổ biến nhất để soạn dữ liệu có giám sát quy mô lớn liên quan đến việc crowdsource chú thích với các nền tảng như Amazon Mechanical Turk (Crowston, 2012), và việc chú thích được thực hiện bởi lao động giá rẻ thay vì các chuyên gia được trả lương cao. Cụ thể, các nền tảng phân phối một phần nhỏ toàn bộ nhiệm vụ chú thích với các hướng dẫn ngắn gọn cho những người lao động đám đông không chuyên và sau đó hợp nhất các chú thích một phần với nhau để tạo thành tập dữ liệu khổng lồ. Vì những người lao động đám đông có thể có tiêu chuẩn chú thích riêng của họ, chất lượng của các tập dữ liệu crowdsourced đáng lo ngại do những không nhất quán và lỗi tiềm ẩn và gây nguy hiểm cho tính mạnh mẽ của các hệ thống máy học. Xét đến những sự kiện này, chúng tôi tin rằng một pipeline tự động để khử nhiễu các chú thích trong tập dữ liệu là một cách đáng giá và có ý nghĩa để giải quyết tắc nghẽn trong máy học. Hơn nữa, vì PLMs được tiền huấn luyện trên dữ liệu không nhãn theo cách học không giám sát mà không có bất kỳ sự can thiệp của con người nào, việc áp dụng PLMs để cải thiện chất lượng dữ liệu là một phương pháp hấp dẫn để giảm yếu tố con người không thể kiểm soát trong máy học.

Trong Phần 2.2.1, chúng tôi giới thiệu tiếng ồn chú thích trong ngữ cảnh của các nhiệm vụ phân loại từ bốn quan điểm: định nghĩa, phân loại, nguồn gốc và tác động hạ nguồn của nó, xác định ngữ cảnh của điều tra tái chú thích tự động của chúng tôi. Trong Phần 2.2.2, chúng tôi mô tả các phương pháp hợp lý để học trên các tập dữ liệu không hoàn hảo mà không thay đổi tiếng ồn chú thích. So với học chịu nhiễu, chúng tôi tin rằng việc xem xét lại các chú thích lỗi với PLMs mang lại nhiều tính diễn giải và hiểu biết hơn để xử lý tiếng ồn chú thích.

2.2.1 Phân tích Tiếng ồn Nhãn

Dựa trên nghiên cứu trước đây về tiếng ồn nhãn (Sharou et al., 2021; Larson et al., 2020; Frénay và Verleysen, 2014; Beck et al., 2020), chúng tôi mô tả ngắn gọn tiếng ồn nhãn liên quan đến các nhiệm vụ phân loại từ bốn khía cạnh sau:

Định nghĩa Tiếng ồn Chú thích Đối với phân loại đa lớp có giám sát, mỗi mẫu tương ứng với một lớp đúng, nhưng việc nhận dạng lớp này sẽ được truyền vào một quy trình nhiễu để trở thành các nhãn quan sát được trình bày cho các mô hình phân loại. Các chú thích quan sát có thể khác với nhãn đúng của các mẫu. Trong quy trình này, các chú thích bị tổn hại được gọi là tiếng ồn nhãn (Angluin và Laird, 1987), trái ngược với tiếng ồn đặc trưng, là nhiễu loạn các giá trị đặc trưng. Tuy nhiên, vì các nhãn có nhiễu thường có xác suất xuất hiện tương đối thấp hơn trong vùng lân cận của chúng so với các nhãn bình thường, các ví dụ được gán nhãn sai có thể được định nghĩa là các bất thường hoặc ngoại lệ của phân bố lớp được gán của chúng trong một số trường hợp. Do đó, phát hiện bất thường (Schölkopf et al., 1999; Hayton et al., 2000; Schölkopf et al., 2001; Hoffmann, 2007; Chandola et al., 2009) và phát hiện ngoại lệ (Barnett, 1978; Sebert, 1997; Zhou et al., 2021b; Hodge và Austin, 2004; Niu et al., 2011; Hawkins, 1980; Winkens et al., 2020) có liên quan lớn đến nhiệm vụ của chúng tôi. Ví dụ, Winkens et al. (2020) thúc đẩy chúng tôi kết hợp mất mát cross-entropy với một mất mát tương phản để tăng cường hiệu suất của bộ tái chú thích tự động, và chúng tôi tiếp tục cân nhắc chiến lược lấy mẫu của học tập tương phản thông qua một tiêu chí mới dựa trên khoảng cách. Khung được phát triển bởi Zhou et al. (2021b) cũng dựa trên học tập tương phản và PLMs, phát hiện out-of-distribution trong trích xuất quan hệ bằng khoảng cách Mahalanobis (McLachlan, 1999) của các biểu diễn ẩn của các ví dụ trong lớp thứ hai từ cuối. Tuy nhiên, nhiệm vụ của chúng tôi, tái chú thích các chú thích có vấn đề, khác biệt đáng kể so với phát hiện out-of-distribution, dự định giải quyết các vấn đề do các phân bố khác nhau của dữ liệu huấn luyện và dữ liệu kiểm tra thế giới thực.

Phân loại Tiếng ồn Chú thích Mặc dù phân loại tiếng ồn chú thích có tiềm năng lớn, chúng tôi chỉ định nghĩa hai loại tiếng ồn chú thích ở đây để giảm phạm vi điều tra: không nhất quán chú thích và lỗi chú thích. Hầu hết thời gian, biểu đạt của không nhất quán và lỗi có thể thay thế cho nhau, nhưng chúng tôi sẽ đưa ra các định nghĩa hẹp cho chúng trong ngữ cảnh của dự án chúng tôi. Không nhất quán chú thích là chú thích cụ thể khác biệt với các chú thích được chia sẻ bởi các ví dụ có ngữ cảnh tương tự (Larson et al., 2020; Hollenstein et al., 2016; Qian et al., 2021; Li et al., 2020a), nhưng chú thích không nhất quán không nhất thiết phải không chính xác. Ví dụ, quan hệ giữa "James" và "Bob" trong ngữ cảnh "James has a son called Bob", có thể được chú thích là father hoặc family member of một cách hợp lý. Tuy nhiên, nếu trong hầu hết các trường hợp tương tự, chúng ta chọn chú thích ví dụ với quan hệ chính xác nhất có thể, thì nhãn family member of ở đây có thể được coi là không nhất quán. Ngược lại, lỗi chú thích là một khái niệm rộng, đề cập đến các chú thích trái ngược với thông thức hoặc mâu thuẫn với ngữ cảnh đã cho Reiss et al. (2020); Suzuki et al. (2017); Matousek và Tihelka (2017); Haverinen et al. (2011); Bryant (2019). Do đó, trong ví dụ trước, cả hai nhãn (father hoặc family member of) đều không phải là lỗi chú thích.

Nguồn gốc Tiếng ồn Chú thích Các nguồn gốc điển hình của tiếng ồn chú thích là: (1) Những người chú thích không có đủ thông tin hoặc kiến thức để hoàn thành thành công nhiệm vụ chú thích Hickey (1996); Brodley và Friedl (1999); Pechenizkiy et al. (2006), điều này không phổ biến khi, ví dụ, dữ liệu văn bản y tế hoặc pháp lý được chú thích. (2) Sự không nhất quán và lỗi được đưa vào trong kịch bản chú thích crowdsourcing, khi một số lượng lớn các chuyên gia không tham gia vào quy trình chú thích (Larson et al., 2020; Snow et al., 2008; Raykar et al., 2010; Yuen et al., 2011). (3) Mục tiêu của nhiệm vụ chú thích là mơ hồ hoặc chủ quan, chẳng hạn như chú thích để phân loại hình ảnh hoặc phân tích y tế (Grivas et al., 2020; Malossini et al., 2006; Smyth, 1996; Fornaciari et al., 2021). (4) Có sự phân tâm trong quá trình chú thích hoặc các vấn đề trong thiết kế giao diện chú thích, chẳng hạn như thiếu cơ chế phản hồi (Sculley và Cormack, 2008). Một cách trực giác, tái chú thích tự động được mong đợi sẽ giảm thiểu nguồn thứ hai và thứ tư của tiếng ồn chú thích một cách hiệu quả. Nguồn đầu tiên có thể được giải quyết bằng PLMs được tiền huấn luyện trên dữ liệu cụ thể theo lĩnh vực (Lee et al., 2020; Feng et al., 2020; Li et al., 2020b), nhưng chúng tôi để lại điều này cho khám phá tương lai.

Tác động lên Hiệu suất Mô hình Tác động tiêu cực của tiếng ồn chú thích có thể đáng kể. Các thí nghiệm được tiến hành bởi Larson et al. (2020) cho thấy rằng bất kỳ loại không nhất quán nào trong dữ liệu crowdsourced sẽ làm giảm hiệu suất mô hình trong slot-filling, mặc dù các loại không nhất quán khác nhau có thể có mức độ tác động khác nhau. Khayrallah và Koehn (2018) nghiên cứu thực nghiệm tác động của tiếng ồn chú thích đa dạng trong kho tài liệu song song cho dịch máy và tiết lộ rằng các mô hình dịch máy nơ-ron dễ bị lỗi hơn đối với tiếng ồn chú thích so với các phương pháp dịch máy thống kê. Chen et al. (2019) gợi ý rằng độ chính xác trên tập Test có thể được sử dụng như hàm bậc hai để đánh giá tỷ lệ nhiễu trong tập dữ liệu nếu tiếng ồn chú thích có thể được phân loại thành tiếng ồn đối xứng (van Rooyen et al., 2015). Alt et al. (2020) và Northcutt et al. (2021a) chứng minh rằng tiếng ồn chú thích trong tập Test làm sai lệch đáng kể quá trình đánh giá và lựa chọn mô hình. Ngoài ra, Northcutt et al. (2021a) lập luận rằng các mô hình ít mạnh hơn với ít tham số hơn hoặc kiến trúc mô hình đơn giản hơn có nhiều khả năng kháng và chính quy hóa để học mạnh mẽ trên dữ liệu với phân bố bất đối xứng của tiếng ồn hơn các mô hình lớn với khả năng học biểu diễn tiên tiến hơn. Hess et al. (2020) trình bày một giải thích toán học cho sự suy giảm của các bộ phân loại softmax do tiếng ồn chú thích bằng cách tái công thức hóa một bộ phân loại softmax thành phân cụm K-means và suy ra mối quan hệ giữa méo mó dự đoán và tiếng ồn chú thích dựa trên Tính liên tục Lipschitz Sohrab (2003).

2.2.2 Học chịu Nhiễu

Thay vì cải thiện chất lượng chú thích như các bộ tái chú thích tự động, học chịu nhiễu dự định giảm thiểu tác động tiêu cực của tiếng ồn chú thích trong giai đoạn huấn luyện, thường bằng cách học cách đối xử với các nhãn với các mức độ tin cậy khác nhau. Các kỹ thuật được sử dụng phổ biến nhất để giảm thiểu nhiễu loạn của dữ liệu có nhiễu là gán nhãn mềm (Thiel, 2008) và học chương trình Soviany et al. (2021); Portelas et al. (2020); Wang et al. (2020); Bengio et al. (2009). Mặc dù học chịu nhiễu theo kinh nghiệm dẫn đến tính mạnh mẽ được cải thiện để xử lý tiếng ồn chú thích, công việc của chúng tôi cải thiện thành phần cơ bản của nó về phát hiện chú thích đáng ngờ với một bộ phát hiện dựa trên PLM nâng cao và tính diễn giải của quy trình học hộp đen của nó với việc sửa chữa chú thích.

Ý tưởng cốt lõi của việc áp dụng nhãn mềm để giảm bớt tiếng ồn trong dữ liệu huấn luyện là tránh phụ thuộc nặng nề và mù quáng vào các nhãn quan sát. Ví dụ, Thiel (2008) cho thấy lợi ích của nhãn mềm về cải thiện khả năng chống chịu nhiễu của bộ phân loại so với các nhãn cứng. Liu et al. (2017) xây dựng nhãn mềm của các ví dụ trong tập dữ liệu trích xuất quan hệ có giám sát xa có nhiễu bằng cách xem xét kết hợp cả độ tin cậy của nhãn quan sát và tương quan cặp thực thể trong ngữ cảnh, dẫn đến cải thiện xuất sắc của hiệu suất mô hình. Tương tự, Algan và Ulusoy (2021) rút ra nhãn mềm từ các đặc trưng của các ví dụ trong dữ liệu huấn luyện và dần dần cập nhật nhãn mềm với mục tiêu meta ở đầu mỗi lần lặp huấn luyện, trong đó mục tiêu meta được thu được bằng cách làm sạch một phần nhỏ dữ liệu huấn luyện.

Trực giác chính đằng sau học chương trình cho học kháng nhiễu là để mô hình học trên dữ liệu đáng tin cậy với tỷ lệ học lớn trước và sau đó điều chỉnh tinh tế các tham số dựa trên dữ liệu có thể có nhiễu. MentorNet được đề xuất bởi Jiang et al. (2018) là một ví dụ sinh động để minh họa ý tưởng này. Quy trình học kháng nhiễu liên quan đến hai mạng nơ-ron được ghép đôi, MentorNet và StudentNet. MentorNet học chương trình có thể giúp StudentNet tập trung vào các ví dụ huấn luyện với xác suất tương đối cao được gán nhãn đúng. Chương trình được dạy bởi MentorNet ban đầu được học từ một tập dữ liệu nhỏ với nhãn đã kiểm tra và sau đó được cân nhắc lặp đi lặp lại dựa trên phản hồi học tập của StudentNet. Northcutt et al. (2021b) tiếp tục lấy MentorNet làm đường cơ sở để khám phá học tin cậy trong lĩnh vực chung. Tương tự, Zhou et al. (2021a) tận dụng học chương trình dựa trên các tham số dữ liệu để tạo ra các mô hình phát hiện từ khóa kháng nhiễu. Để tăng cường kết quả học tập, Higuchi et al. (2021) sử dụng ensemble thời gian của mô hình và tăng cường dữ liệu để tạo ra nhãn giả cho việc soạn dữ liệu có nhiễu như chương trình khó hơn cho các mô hình học cách xử lý tiếng ồn chú thích.

2.3 Cải thiện Chất lượng Chú thích

Ngoài học chịu nhiễu, chúng ta cũng có thể trực tiếp cải thiện hoặc kiểm tra chất lượng chú thích. Theo mức độ tự động hóa, chúng tôi phân loại các phương pháp cải thiện chất lượng chú thích thành ba lớp: (1) Cải thiện Quy trình Chú thích, tức là tối ưu hóa thủ công yếu tố và các bước trong quy trình chú thích, (2) Phát hiện Không nhất quán Chú thích, tức là phát hiện các nhãn đáng ngờ hoặc chỉ ra độ tin cậy của nhãn dựa trên để sửa chữa thủ công hoặc tự động hạ nguồn, và (3) Sửa chữa Lỗi Chú thích, tức là tự động sửa chữa các ví dụ được gán nhãn sai.

Trong Chương 4, chúng tôi giới thiệu một phương pháp mới dựa trên PLM cho Phát hiện Không nhất quán Chú thích, và trong Chương 5, chúng tôi tiếp tục phát triển các phương pháp tương phản cho Sửa chữa Lỗi Chú thích dựa trên xác thực chéo. Các mô hình được đề xuất của chúng tôi có hai tính chất quan trọng cho phép chúng nổi bật so với nghiên cứu trước đây trong Phát hiện Không nhất quán Chú thích và chỉnh sửa nhãn: (1) Chúng tôi là những người đầu tiên tận dụng phân bố của nhúng ngữ cảnh hóa từ PLMs để chỉ ra sự không nhất quán chú thích và sửa chữa lỗi nhãn. (2) Các phương pháp được đề xuất của chúng tôi không dựa vào bất kỳ kiến thức ngôn ngữ học rõ ràng, quy tắc được định nghĩa trước, hoặc dữ liệu đã làm sạch nào, cho phép chúng tổng quát hóa sang các lĩnh vực khác nhau.

2.3.1 Cải thiện Quy trình Chú thích

Cải thiện quy trình thu thập các chú thích là cách đơn giản nhất để giảm bớt tiếng ồn chú thích, nhưng nó thường yêu cầu nhiều lao động con người hơn và tăng chi phí chú thích. Để giảm tiếng ồn do các chuyên gia không gây ra, chúng ta có thể theo nghiên cứu trước đây trong việc chọn các ứng cử viên của người chú thích vượt qua bài kiểm tra đủ điều kiện, huấn luyện có hệ thống các người chú thích, hoặc chú thích theo chu kỳ (Roit et al., 2020; Li và Liu, 2015; Alex et al., 2010). Để giảm bớt tiếng ồn do sự không nhất quán nội bộ giữa nhiều người chú thích, chúng ta có thể chồng chéo một phần nhỏ công việc chú thích của họ để đo lường sự đồng thuận của họ hoặc thu thập lặp đi lặp lại các chú thích của mỗi ví dụ từ nhiều người chú thích và sau đó tổng hợp các quyết định của họ một cách toàn diện (Hovy et al., 2013; Passonneau và Carpenter, 2014; Parde và Nielsen, 2017; Nowak và Rüger, 2010; Jamison và Gurevych, 2015). Đối với tiếng ồn được giới thiệu bởi các mục tiêu chú thích phức tạp, chúng ta có thể tái công thức hóa các mục tiêu chú thích hoặc trang bị nền tảng chú thích với một trợ lý chú thích dựa trên học tập tích cực (Dobbie et al., 2021; Nghiem et al., 2021; Weeber et al., 2021). Tiếng ồn do sự phân tâm trong giai đoạn chú thích cũng có thể được cải thiện bằng cách giám sát liệu những người chú thích có thể gán nhãn chính xác cho các ví dụ thăm dò với chú thích vàng đã biết hay không (Oppenheimer et al., 2009).

2.3.2 Phát hiện Không nhất quán Chú thích

Đã có nhiều điều tra mở rộng trong Phát hiện Không nhất quán Chú thích để hiểu rõ hơn về phân bố tiếng ồn chú thích. Nghiên cứu sớm trong lĩnh vực này tập trung vào phát hiện các thẻ part-of-speech không nhất quán (Abney et al., 1999; Eskin, 2000; Matsumoto và Yamashita, 2000; Nakagawa và Matsumoto, 2002; Matsumoto và Yamashita, 2000; Ma et al., 2001). Kể từ đó, các phương pháp thống kê và dựa trên quy tắc đã hiệu quả trong việc sửa chữa kho tài liệu cho các nhiệm vụ dự đoán cú pháp khác, chẳng hạn như gán nhãn vai trò ngữ nghĩa (Dickinson và Lee, 2008), và phân tích cú pháp phụ thuộc (Dickinson, 2010; Dickinson và Smith, 2011). Tuy nhiên, nhiệm vụ trích xuất quan hệ mà chúng tôi điều tra như nhiệm vụ ngữ nghĩa tương đối phức tạp để phát hiện sự không nhất quán trong ngữ cảnh của nó vì tính linh hoạt của biểu đạt ngữ nghĩa. Ví dụ, phát hiện không nhất quán trong tập dữ liệu biểu đạt đa từ hoặc ý nghĩa từ cần các mô hình mạnh mẽ hơn. Dligach và Palmer (2011) sàng lọc các chú thích đáng để kiểm tra lại dựa trên dữ liệu ý nghĩa từ. Họ coi các ví dụ được dự đoán lặp đi lặp lại là đáng ngờ bởi cả bộ gán thẻ máy dựa trên máy vectơ hỗ trợ và bộ phát hiện mơ hồ dựa trên bộ phân loại xác suất có thể huấn luyện như các chú thích không nhất quán. Hollenstein et al. (2016) đề xuất một thuật toán dựa trên xếp hạng tần suất tuyệt đối và entropy của phân bố nhãn để tự động phát hiện không nhất quán trong các tập dữ liệu biểu đạt đa từ và gán thẻ supersense. Qian et al. (2021) phát triển một bộ phát hiện không nhất quán tự động cho tập dữ liệu đối thoại hướng nhiệm vụ, MultiWOZ, dựa trên các script sử dụng biểu thức chính quy. Công việc của chúng tôi, theo hiểu biết tốt nhất của chúng tôi, là nỗ lực đầu tiên phát hiện không nhất quán chú thích trong lĩnh vực trích xuất quan hệ.

2.3.3 Sửa chữa Lỗi Chú thích

Sửa chữa Lỗi Chú thích là một bước tiếp theo từ Phát hiện Không nhất quán Chú thích, yêu cầu các mô hình không chỉ xác định các nhãn đáng ngờ mà còn đề xuất một sửa đổi hợp lý cùng lúc (Zhang et al., 2015; Nicholson et al., 2015; Bhadra và Hein, 2015). Là một trong những tiến bộ gần đây trong sửa chữa nhãn, Wu et al. (2021) phát triển một khung meta-learning trước tiên ước lượng nhãn mềm của mỗi ví dụ trong tập dữ liệu dưới sự hướng dẫn của tập dữ liệu meta nhỏ với nhãn đã làm sạch, và sau đó rút ra meta learner để sửa chữa chú thích từ meta-process của ước lượng nhãn mềm. Tương tự, Zheng et al. (2021) cũng thực hiện quy trình sửa chữa chú thích trên một tập dữ liệu nhỏ với nhãn đã kiểm tra như meta-process và phát triển khung dựa trên meta-learning bao gồm hai mô hình mạng, trong đó meta-model để sửa chữa các chú thích có nhiễu, và main model để khai thác nhãn đã chỉnh sửa. Hai mô hình mạng trong khung này được huấn luyện cộng tác như một vấn đề tối ưu hóa hai cấp. Ngược lại, Zou et al. (2021) đề xuất một khung ensemble không giám sát để sửa chữa chú thích mà không yêu cầu dữ liệu vàng đã kiểm tra. Họ trước tiên tổng hợp các chú thích cho các ví dụ tổng thể trong tập dữ liệu bằng thuật toán expectation-maximization, sau đó sàng lọc trường hợp khó để tạo thành tập dữ liệu nhắm mục tiêu với phương pháp lọc hai bước, và cuối cùng áp dụng bộ phân loại Adaboost được huấn luyện trên tập dữ liệu còn lại có rủi ro thấp để dự đoán các sửa chữa nhãn trên tập dữ liệu mục tiêu. Tóm lại, ngoài việc là người đầu tiên khám phá Sửa chữa Lỗi Chú thích trong trích xuất quan hệ, chúng tôi cũng trình bày một hướng nghiên cứu hấp dẫn về việc thay thế quy trình meta-learning của sửa chữa nhãn với kiến thức tiền nghiệm mạnh mẽ trong PLMs. Trong tương lai, chúng tôi cũng sẽ khám phá liệu việc sàng lọc trước các chú thích đáng ngờ nhất có thể hỗ trợ việc chỉnh sửa chú thích tự động được đề xuất của chúng tôi hay không.

--- TRANG 9 ---
Chương 3
Nhiệm vụ và Dữ liệu

Phát hiện Không nhất quán Chú thích và Sửa chữa Lỗi Chú thích là hai nhiệm vụ quan trọng nhất của tái chú thích tự động. Để làm rõ phạm vi của luận văn này, chúng tôi đưa ra cả định nghĩa khái niệm và toán học của hai nhiệm vụ này. Chúng tôi cũng thảo luận về cách các Mô hình Ngôn ngữ Tiền huấn luyện có thể đặt nền tảng cho việc tái chú thích thành công. Với các mục tiêu rõ ràng, chúng tôi mô tả chi tiết hai loại tập dữ liệu mà chúng tôi sử dụng để tiến hành thí nghiệm: tập dữ liệu mục tiêu để học tập và tập dữ liệu đã xem xét lại để đánh giá.

3.1 Tái chú thích trong Trích xuất Quan hệ

Trích xuất Quan hệ (RE) là nhiệm vụ dự đoán các quan hệ ngữ nghĩa giữa các thực thể chủ quan và khách quan đã cho, cụ thể là thực thể đầu và thực thể cuối, trong ngữ cảnh Wang et al. (2021); Aydar et al. (2020); Cui et al. (2017). Một ví dụ RE điển hình có thể là phân biệt quan hệ giữa thực thể đầu "SpaceX" và thực thể cuối "Elon Musk", với câu "SpaceX was founded in 2002 by Elon Musk". Thông thường, các tập dữ liệu RE chứa một ngữ cảnh văn bản và các khoảng của thực thể đầu và cuối trong ngữ cảnh và có các loại quan hệ có thể được xác định trước. Một số tập dữ liệu cũng cung cấp thông tin phụ trợ của các thực thể, chẳng hạn như các loại Nhận dạng Thực thể Có tên (NER). Do đó, chú thích trong RE là quá trình chọn quan hệ giữa thực thể đầu và cuối trong ngữ cảnh từ một tập hợp các loại quan hệ. Trích xuất Quan hệ là một thành phần không thể thiếu để soạn các đồ thị kiến thức Li et al. (2019) hữu ích cho các ứng dụng NLP hạ nguồn khác nhau như trả lời câu hỏi (Dubey, 2021; Saffari et al., 2021; Sen et al., 2021) và hệ thống đối thoại (Liu et al., 2021b; Chaudhuri et al., 2021; Gao et al., 2021a).

Nhiệm vụ tái chú thích giả định rằng bất kỳ chú thích nào được quan sát trong tập dữ liệu không nhất thiết phải là nhãn đúng của ví dụ trong thực tế vì tiếng ồn chú thích (Phần 2.2.1). Do đó, tái chú thích trong RE là để tái kiểm tra các quan hệ được chú thích, đề xuất độ tin cậy của chúng, và khuyến nghị các quan hệ tốt hơn nếu có thể.

Để giải thích chính thức nhiệm vụ, chúng tôi cũng trình bày mô tả toán học của tái chú thích RE. Gọi R biểu thị tập hợp các loại quan hệ, A biểu thị tất cả các chú thích, sub và obj biểu thị các thực thể chủ quan và khách quan (thực thể đầu và cuối) tương ứng, và c biểu thị ngữ cảnh của mỗi ví dụ. R chứa n loại quan hệ hữu hạn t, cụ thể là R = {ti}^n. Cho ngữ cảnh ci, có một quan hệ hợp lệ đúng ri(subi, obji) ∈ R giữa các thực thể subi và obji. Tuy nhiên, do tiếng ồn chú thích có thể, chúng ta chỉ có thể thấy r'i(subi, obji) ∈ R là quan hệ được quan sát trong tập dữ liệu. Mỗi ví dụ trong tập dữ liệu với chú thích của nó có thể được ký hiệu là ai(r'i(subi, obji), ci) ∈ A. Vì vậy, tái chú thích trong RE là để chứng minh liệu r = r' hay tiết lộ quan hệ đúng r, dựa trên r' được quan sát, các chú thích tổng thể A và kiến thức thế giới thực tiềm ẩn. Các nhiệm vụ tương ứng là: Phát hiện Không nhất quán Chú thích và Sửa chữa Lỗi Chú thích.

3.1.1 Định nghĩa Phát hiện Không nhất quán Chú thích

Phát hiện Không nhất quán Chú thích (AID) là nhiệm vụ xác minh liệu chú thích của mỗi ví dụ có nhất quán với các chú thích khác hay không. AID trên các tập dữ liệu Trích xuất Quan hệ nhằm phát hiện liệu quan hệ được quan sát của mỗi ví dụ có nhất quán với các ví dụ khác có thực thể và ngữ cảnh tương tự hay không.

AID cũng có thể có lợi cho việc huấn luyện mô hình hạ nguồn trong nhiều khía cạnh khác nhau. Nó giúp một người tái chú thích con người bắt đầu bằng cách nhìn vào các chú thích dễ bị lỗi nhất để tiết kiệm thời gian. Các mô hình có thể được hưởng lợi từ việc phân biệt dữ liệu đáng tin cậy và dữ liệu không đáng tin cậy trong quá trình huấn luyện theo cách học chương trình (Soviany et al., 2021; Wang et al., 2020), hoặc bằng cách điều chỉnh trọng số của dữ liệu huấn luyện (Wang et al., 2019).

Trong dự án này, chúng tôi định nghĩa AID là nhiệm vụ phân loại nhị phân với mục tiêu y ∈ [0, 1] trong đó 1 có nghĩa là chú thích nhất quán và 0 có nghĩa là chú thích không nhất quán. Ví dụ, có một ví dụ RE với ngữ cảnh c "Alan Turing died in 1954" và quan hệ được quan sát r'(Alan Turing, 1954) là date of birth. Các mô hình AID được huấn luyện để dự đoán y = 0 để chỉ ra nó là không nhất quán, nếu chúng ta có đủ lý do để tin rằng r' ≠ r dựa trên ngữ cảnh c, quan hệ được quan sát r'(Alan Turing, 1954) và phân bố toàn cục của tất cả các chú thích.

3.1.2 Định nghĩa Sửa chữa Lỗi Chú thích

Nhiệm vụ Sửa chữa Lỗi Chú thích (AEC) được coi là phức tạp hơn nhiệm vụ AID. Nó không chỉ chứng minh các chú thích mà còn cố gắng dự đoán các nhãn đúng cho các chú thích được xác định là không chính xác cùng lúc. Đối với các tập dữ liệu Trích xuất Quan hệ, các mô hình AEC nhằm xác nhận các quan hệ được chú thích giữa các thực thể trong ngữ cảnh và sửa chữa các quan hệ không hợp lệ.

Không nghi ngờ gì, các mô hình mới nhất với hàng triệu tham số có thể dễ dàng nắm bắt các biểu diễn từ dữ liệu ở mức độ chưa từng có. Tuy nhiên, một lượng nghiên cứu ngày càng tăng tiết lộ rằng chất lượng dữ liệu là một yếu tố không thể bỏ qua làm giảm đáng kể hiệu suất mô hình hoặc đánh lạc hướng đánh giá (Larson et al., 2020; Khayrallah và Koehn, 2018; Northcutt et al., 2021a). Do đó, AEC được mong đợi trở thành một giải pháp thay thế hấp dẫn cho việc tăng cường chất lượng dữ liệu thủ công. Thông thường, những người chú thích con người mất hàng trăm giờ để xem xét lại các tập dữ liệu quy mô lớn, trong khi các hệ thống AEC sẽ rút ngắn đáng kể thời gian xem xét lại và giảm chi phí.

Trong luận văn này, chúng tôi định nghĩa AEC là nhiệm vụ phân loại đa lớp với các lớp mục tiêu giống với tập hợp các quan hệ được định nghĩa trước R. Ví dụ, nếu một quan hệ được quan sát r'(Alan Turing, 1954) trong ngữ cảnh c của "Alan Turing died in 1954" là date of birth, các mô hình AEC được mong đợi dự đoán quan hệ đúng r = date of death, trong đó r ∈ R.

3.1.3 Cơ sở của Tái chú thích Tự động

Hình 3.1: Nhúng từ từ các Mô hình Ngôn ngữ Tiền huấn luyện có thể nắm bắt các biến thể ngữ nghĩa tinh tế trong ngữ cảnh. Do đó, các Mô hình Ngôn ngữ Tiền huấn luyện mở ra những khả năng mới trong tái chú thích tự động. Hình từ Reif et al. (2019).

Ý tưởng cơ bản của tái chú thích tự động là đối chiếu chú thích truy vấn với các chú thích khác trong ngữ cảnh tương tự thông qua các mô hình. Như được hiển thị trong Hình 3.1, các Mô hình Ngôn ngữ Tiền huấn luyện (PLMs) thể hiện độ nhạy đặc biệt với các biến thể tinh tế trong ngữ cảnh. Do đó, chúng tôi tin rằng có thể thu được các biểu diễn quan hệ có thể phân biệt đủ để tái chú thích tự động dựa trên các nhúng bởi PLMs.

Về mặt toán học, Saunshi et al. (2021) khám phá các yếu tố chính để áp dụng thành công PLMs vào nhiệm vụ quan tâm bằng cách tái công thức hóa các nhiệm vụ phân loại dựa trên PLM thành các nhiệm vụ hoàn thành câu. Họ đưa ra một tiêu chí, Định nghĩa 3.2 trong bài báo của họ, để đánh giá liệu một nhiệm vụ phân loại hạ nguồn T có thể được coi là một nhiệm vụ tự nhiên, cụ thể là một nhiệm vụ hoàn thành câu tương tự, liên quan đến nhúng dựa trên PLM F ∈ R^{d×V}, trong đó V là kích thước từ vựng. Nếu chúng ta giả định p ∈ DC để chỉ ra phân bố xác suất trên ngữ cảnh C, và p_{j|c} để biểu thị phân bố điều kiện đúng trên các từ trong từ vựng W được đưa ra ngữ cảnh c, tiêu chí có thể được công thức hóa là bất đẳng thức:

min_{v∈row-span(F);||v||_∞≤B} l_T({p_{j|c}}, v) ≤ t     (3.1)

, trong đó l_T là surrogate 1-Lipschitz (Mairal, 2013) đối với mất mát phân loại của nhiệm vụ T, {p_{j|c}} biểu thị một mô hình ngôn ngữ, và t và B là hai ràng buộc. Thông qua các chứng minh chi tiết, họ đưa ra một giải thích trực giác về t và B: (1) t chỉ ra tính mơ hồ của nhiệm vụ hạ nguồn được đo bằng lỗi Bayes (Franklin, 2005), và (2) B tỷ lệ nghịch với khối lượng xác suất của tập hợp các từ chỉ thị. Vì vậy, nhiệm vụ hạ nguồn ít mơ hồ hơn chủ yếu liên quan đến các từ thường xuyên sẽ có t và B nhỏ hơn. Về mặt định tính, Saunshi et al. (2021) gợi ý hai yếu tố tích cực giúp PLM giải quyết các nhiệm vụ hạ nguồn: (1) nhúng dựa trên PLM có thể nắm bắt ý nghĩa ngữ nghĩa cần thiết trong ngữ cảnh và (2) các nhiệm vụ quan tâm có thể giải quyết được bằng cách phân biệt các từ có ý nghĩa khác nhau rõ ràng.

Dựa trên sự khai sáng, chúng tôi phát triển các khung cho các nhiệm vụ AID và AEC được mô tả trong Chương 4 và Chương 5 tương ứng.

3.2 Tập dữ liệu trong Trích xuất Quan hệ

Để nghiên cứu toàn diện các bộ tái chú thích tự động, chúng tôi cần hai loại tập dữ liệu: (1) tập dữ liệu mục tiêu với các chú thích chưa được kiểm tra và (2) các tập dữ liệu đã xem xét lại tương ứng với các chú thích đã được làm sạch.

Tập dữ liệu mục tiêu là dữ liệu thế giới thực với các chú thích quan sát không nhất thiết phải chính xác hoặc nhất quán. Do đó, việc sử dụng đầu tiên của các tập dữ liệu mục tiêu là yêu cầu các bộ tái chú thích học trên chúng, và sau đó đánh giá liệu các bộ tái chú thích có thể phát hiện không nhất quán hoặc sửa chữa lỗi tự động hay không. Trong trường hợp này, chúng tôi kết hợp tất cả các phân chia dữ liệu với chú thích chưa được kiểm tra, cụ thể là các tập Train, Dev và Test gốc cùng nhau như tập Train mới cho bộ tái chú thích được đề xuất của chúng tôi. Việc sử dụng thứ hai của các tập dữ liệu mục tiêu là cho đánh giá hạ nguồn, cụ thể là để chứng minh liệu các mô hình RE hiện đại nhất (SOTA) có thể được hưởng lợi từ dữ liệu được khử nhiễu bởi bộ sửa chữa chú thích được đề xuất của chúng tôi hay không. Trong trường hợp này, chúng tôi tuân theo các phân chia dữ liệu gốc để tiến hành các thí nghiệm RE hạ nguồn. Thông tin chi tiết hơn về đánh giá hạ nguồn sẽ được giới thiệu trong Phần 5.3.

Mặt khác, chúng tôi coi các phiên bản đã xem xét lại của các tập dữ liệu mục tiêu là tiêu chuẩn vàng của việc xem xét lại. Nếu chúng tôi coi các chú thích đã kiểm tra lại trong các tập dữ liệu đã xem xét lại là ground truth, chúng tôi có thể đánh giá các bộ tái chú thích được đề xuất của chúng tôi bằng cách so sánh các dự đoán của chúng với các bản xem xét lại thủ công trong các tập dữ liệu đã xem xét lại với các chỉ số phân loại. Do đó, chúng tôi rút ra các tập Dev và Test với các chú thích đã kiểm tra cho bộ tái chú thích được đề xuất của chúng tôi từ các tập dữ liệu đã xem xét lại.

Vì chúng tôi quyết định điều tra tái chú thích trong RE, chúng tôi chọn tập dữ liệu RE cấp câu phổ biến TACRED (Zhang et al., 2017a) và tập dữ liệu RE cấp tài liệu DocRED (Yao et al., 2019b), làm hai tập dữ liệu mục tiêu. Tập dữ liệu cấp câu TACRED đã có hai phiên bản đã xem xét lại gọi là TACRev (Alt et al., 2020) và Re-TACRED (Stoica et al., 2021). Tuy nhiên, theo hiểu biết tốt nhất của chúng tôi, không có bản xem xét lại thủ công hiện có nào của tập dữ liệu cấp tài liệu DocRED. Do đó, để nghiên cứu những thách thức mới do độ dài ngữ cảnh và độ phức tạp liên câu đặt ra, chúng tôi tự tái chú thích một tập con của DocRED, gọi là Re-DocRED.

3.2.1 Tập dữ liệu Mục tiêu

TACRED

TACRED (Zhang et al., 2017a), The TAC Relation Extraction Dataset, là một trong những tập dữ liệu lớn nhất và được sử dụng rộng rãi nhất cho nhiệm vụ RE cấp câu, chứa các ví dụ từ văn bản web đến các bài báo từ các thử thách TAC Knowledge Base Population (TACKBP). Các ví dụ trong TACRED bao gồm 41 loại quan hệ tích cực giống hệt với các thử thách TACKBP (ví dụ: per:title) và một loại tiêu cực (no_relation) nếu không có quan hệ được định nghĩa nào được giữ giữa các thực thể đầu và cuối đã cho. Những ví dụ này được tạo ra bằng cách kết hợp các chú thích con người có sẵn từ các thử thách TACKBP và crowdsourcing. Tuy nhiên, như được chỉ ra bởi Alt et al. (2020) và Stoica et al. (2021), TACRED là một tập dữ liệu điển hình với các lỗi chú thích và không nhất quán lan tràn. Vì TACRED ban đầu có 68,124 mẫu Train, 22,631 mẫu Dev và 15,509 mẫu Test, có tổng cộng 103,738 ví dụ để huấn luyện các bộ tái chú thích cấp câu.

DocRED

DocRED (Yao et al., 2019b), Document-Level Relation Extraction Dataset, là một tập dữ liệu RE cấp tài liệu được rút ra từ Wikipedia và Wikidata. DocRED yêu cầu đọc nhiều câu trong một tài liệu để trích xuất thực thể và suy ra các quan hệ lẫn nhau của chúng thông qua lý luận thông thức và tổng hợp thông tin ngữ cảnh của tài liệu. So với tập dữ liệu TACRED, các ví dụ trong tập dữ liệu DocRED thường thể hiện các quan hệ liên câu phức tạp hơn. Chúng có thể phức tạp hơn các phương pháp trích xuất quan hệ (RE) hiện có tập trung vào trích xuất các quan hệ trong câu cho các cặp thực thể đơn lẻ. Hơn nữa, DocRED có 96 loại quan hệ hợp lệ cũng đáng kể hơn so với TACRED.

DocRED chứa 132,375 thực thể và 56,354 sự kiện quan hệ trên 5,053 tài liệu Wikipedia được chú thích bằng con người. Chúng tôi là những người đầu tiên kiểm tra chất lượng chú thích và phát hiện sự thiếu hụt chú thích trên DocRED. Như được hiển thị trong Bảng 3.1, chúng tôi loại bỏ một số ví dụ có nhiều quan hệ để giảm tính mơ hồ trong khi huấn luyện, và có 50,503 ví dụ trong tập Train cho các bộ tái chú thích cấp tài liệu.

[Bảng 3.1: Thống kê của các tập dữ liệu mục tiêu TACRED và DocRED và các tập dữ liệu đã xem xét lại TACRev, Re-TACRED, và Re-DocRED. Ví dụ là tiêu cực nếu không có quan hệ được định nghĩa nào được giữ giữa thực thể đầu và cuối.]

3.2.2 Tập dữ liệu Đã xem xét lại Hiện có

TACRev

TACRev (Alt et al., 2020) là điều tra đầu tiên về tiếng ồn chú thích trong tập dữ liệu TACRED. Các nhà ngôn ngữ học tái kiểm tra 5,000 ví dụ thách thức nhất trong TACRED, và 2,526 trong số chúng đã được xem xét lại từ các nhãn gốc, trong đó khoảng 57% nhãn tiêu cực được sửa đổi thành nhãn tích cực. Họ chứng minh rằng trần hiệu suất của các mô hình SOTA trước đây trên tập dữ liệu TACRED phần lớn là do tiếng ồn chú thích, cho thấy rằng 4 mô hình SOTA có thể cải thiện 8% điểm F1 tuyệt đối bằng cách đánh giá trên tập dữ liệu TACRev đã tinh chỉnh.

Tập dữ liệu TACRev chỉ phát hành 2,526 ví dụ đã xem xét lại mà không có các chú thích được chứng minh là chính xác. Chúng tôi trước tiên xáo trộn 2,526 ví dụ đã xem xét lại và sau đó chia đều chúng thành tập Dev và tập Test (Bảng 3.1). Tập Dev chứa 40 quan hệ bao gồm no_relation, trong khi tập Test chứa 39 quan hệ.

Re-TACRED

So với TACRev, Re-TACRED (Stoica et al., 2021) là phiên bản crowdsourced của việc xem xét lại TACRED thay vì được gán lại nhãn bởi một số nhà ngôn ngữ học. Toàn bộ tập dữ liệu TACRED đã được kiểm tra lại bằng một chiến lược chú thích crowdsourcing cải tiến và hiệu quả về chi phí với cơ chế kiểm soát chất lượng. Trong quá trình tinh chỉnh định nghĩa quan hệ nhằm giải quyết định nghĩa quan hệ mơ hồ trong TACKBP, các tác giả tiếp tục giới thiệu các quan hệ mới (ví dụ: org:member như quan hệ nghịch đảo của org:members và org:subsidiaries) và đổi tên một số quan hệ ban đầu (ví dụ: per:alternate_name thành per:identity). Vì các bộ tái chú thích tự động không thể dự đoán các quan hệ mới chỉ bằng cách học trên tập dữ liệu TACRED vani mà không có những quan hệ mới này, chúng tôi xóa tất cả các ví dụ với các quan hệ mới được giới thiệu trong Re-TACRED để phù hợp với nhiệm vụ của chúng tôi. Được huấn luyện và đánh giá trên tập dữ liệu Re-TACRED, có thể quan sát thấy cải thiện trung bình 14.3% điểm F1-score, cho thấy rằng Re-TACRED về cơ bản đã tăng cường chất lượng chú thích và có thể đánh giá các mô hình trích xuất quan hệ một cách trung thực hơn.

Tương tự, chúng tôi khai thác Re-TACRED để đánh giá mức độ mà bộ tái chú thích tự động được đề xuất của chúng tôi có thể gán lại nhãn cho tập dữ liệu với tính nhất quán cao hơn chỉ bằng cách học trên dữ liệu có nhiễu. Chúng tôi tái cấu trúc tập dữ liệu Re-TACRED để đánh giá upstream theo các chiến lược tương tự trên TACRev. Tập dữ liệu bao gồm 10,729 ví dụ đã được gán lại nhãn và vẫn giữ các quan hệ đã tồn tại trong tập quan hệ TACRED và TACKBP gốc. Để làm cho kết quả trên Re-TACRED có thể so sánh với TACRev, chúng tôi chỉ giữ lại các ví dụ đã xem xét lại, xáo trộn và sau đó chia đều thành tập Dev với 5264 ví dụ và tập Test với 5365 ví dụ bao gồm 36 quan hệ khác nhau.

3.2.3 Tập dữ liệu Đã xem xét lại Mới: Re-DocRED

Chúng tôi tạo ra tập dữ liệu đầu tiên chứa các tái chú thích thủ công trong RE cấp tài liệu, gọi là Re-DocRED. Đây là một tập con đã xem xét lại của DocRED, chứa việc xem xét lại các ví dụ với các không nhất quán và lỗi chú thích thách thức.

Vì các nhãn của tập Test của DocRED không công khai, Re-DocRED được rút ra từ các tập Train và Dev của DocRED, chỉ có 50503 sự kiện quan hệ. Re-DocRED bao gồm 411 tái chú thích tổng cộng và được chia đều thành tập Dev và Test với phạm vi bao phủ 55 loại quan hệ (Bảng 3.1). Tập dữ liệu Re-DocRED được lấy mẫu và chú thích trong hai giai đoạn:

Lựa chọn Dữ liệu Chúng tôi đầu tiên tuân theo chiến lược lựa chọn dữ liệu tương tự được đề xuất trong TACRev (Alt et al., 2020) để tận dụng tối đa lao động con người hạn chế của chúng tôi. Các ví dụ có thể thách thức cho chú thích được sàng lọc theo sự bất đồng của các dự đoán bởi nhiều mô hình RE hiện đại nhất. Cụ thể, chúng tôi tinh chỉnh CorefBERT (Ye et al., 2020) với BERT-base, ATLOP (Xu et al., 2021) với RoBERTa-large, và SSAN (Xu et al., 2021) với RoBERTa-base và RoBERTa-large trên tập Train và đối chiếu các dự đoán của chúng với các chú thích con người trên tập Dev của DocRED để tìm ra các nhãn dễ bị lỗi từ các ví dụ Dev. Không giống như tiêu chí được áp dụng trong TACRev, là chọn các ví dụ bị phân loại sai bởi ít nhất một nửa số mô hình, chúng tôi quan sát thực nghiệm rằng nếu các chú thích con người khác với tất cả bốn dự đoán mô hình này, các chú thích có khả năng không chính xác hoặc không nhất quán trên DocRED. Quy trình lựa chọn dữ liệu tự động này thu hẹp phạm vi cho việc xem xét lại con người xuống 560 ví dụ.

Chú thích Thủ công Các ví dụ được chọn trước tiên được xác nhận bởi một sinh viên đại học mới tốt nghiệp ngành Ngôn ngữ học và sau đó được kiểm tra bởi tác giả của luận văn này, một sinh viên thạc sĩ nghiên cứu ngành Ngôn ngữ học với bằng cử nhân Khoa học Máy tính. Việc xác nhận và kiểm tra được tiến hành độc lập thông qua nền tảng chú thích trực tuyến dựa trên INCEpTION (Klie et al., 2018). Đây là một nền tảng chú thích ngữ nghĩa mã nguồn mở được xây dựng bởi UKP Lab tại TU Darmstadt. Các quy trình xác nhận và kiểm tra thủ công mất khoảng 35 và 10 giờ, tương ứng. Do đó, 411 ví dụ được xác nhận lại chất lượng cao từ tập Dev DocRED được chọn để tạo thành tập dữ liệu Re-DocRED, bao gồm 57.5% ví dụ với nhãn đã xem xét lại và 42.5% ví dụ với nhãn gốc.
