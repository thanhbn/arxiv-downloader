# 2204.11406.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/data-augmentation/2204.11406.pdf
# File size: 601399 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Robust Self-Augmentation for Named Entity Recognition with Meta
Reweighting
Linzhi Wu1, Pengjun Xie, Jie Zhou2, Meishan Zhang3y,
Chunping Ma, Guangwei Xu, Min Zhang3
1School of New Media and Communication, Tianjin University
2School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University
3Institute of Computing and Intelligence, Harbin Institute of Technology (Shenzhen)
{tjuwlz2020, machunpingjj}@163.com, sanny02@sjtu.edu.cn
{xpjandy, mason.zms, ahxgwOnePiece}@gmail.com, zhangmin2021@hit.edu.cn
Abstract
Self-augmentation has received increasing re-
search interest recently to improve named en-
tity recognition (NER) performance in low-
resource scenarios. Token substitution and
mixup are two feasible heterogeneous self-
augmentation techniques for NER that can
achieve effective performance with certain spe-
cialized efforts. Noticeably, self-augmentation
may introduce potentially noisy augmented
data. Prior research has mainly resorted to
heuristic rule-based constraints to reduce the
noise for speciﬁc self-augmentation methods
individually. In this paper, we revisit these two
typical self-augmentation methods for NER,
and propose a uniﬁed meta-reweighting strat-
egy for them to achieve a natural integra-
tion. Our method is easily extensible, impos-
ing little effort on a speciﬁc self-augmentation
method. Experiments on different Chinese and
English NER benchmarks show that our token
substitution and mixup method, as well as their
integration, can achieve effective performance
improvement. Based on the meta-reweighting
mechanism, we can enhance the advantages
of the self-augmentation techniques without
much extra effort.
1 Introduction
Named entity recognition (NER), which aims to
extract predeﬁned named entities from a piece of
unstructured text, is a fundamental task in the nat-
ural language processing (NLP) community, and
has been studied extensively for several decades
(Hammerton, 2003; Huang et al., 2015; Chiu and
Nichols, 2016; Ma and Hovy, 2016). Recently,
supervised sequence labeling neural models have
been exploited most popularly for NER, leading to
state-of-the-art (SOTA) performance (Zhang and
Yang, 2018; Li et al., 2020a; Ma et al., 2020).
Although great progress has been made, devel-
oping an effective NER model usually requires a
Equal contributions.
yCorresponding author.
Feature Representation
CRF
EncoderDecoderOutput
Token SubstitutionMixupSelf-Augmentation Meta Reweighting
InputFigure 1: The main idea of our work, where the two
heterogeneous self-augmentation methods (i.e., token
substitution and mixup) are integrated by a uniﬁed meta
reweighting framework.
large-scale and high-quality labeled training corpus,
which is often difﬁcult to be obtained in real-world
scenarios due to the expensive and time-consuming
annotations by human experts. Moreover, it would
be extremely serious because the target language,
target domain, and the desired entity type could all
be inﬁnitely varied. As a result, the low-resource
setting with only a small amount of annotated cor-
pus available is far more common in practice, even
though it may result in signiﬁcant performance
degradation due to the overﬁtting problem.
Self-augmentation is a prospective solution to
this problem, which has received widespread at-
tention (Zhang et al., 2018; Wei and Zou, 2019;
Dai and Adel, 2020; Chen et al., 2020; Karimi
et al., 2021). The major motivation is to gener-
ate a pseudo training example set deduced from
the original gold-labeled training data automati-
cally. For NER, a token-level task, the feasible
self-augmentation techniques include token sub-
stitution (Dai and Adel, 2020; Zeng et al., 2020)
and mixup (Zhang et al., 2020a; Chen et al., 2020),
which are deformed at the ground-level inputs and
the high-level hidden representations, respectively.
Nonetheless, there are still some limitations cur-
rently for the above token substitution and mixup
methods. For one thing, both of them require some
specialized efforts to improve their effectiveness
due to the potential noise introduced by the self-
augmentation, which may restrict the valid seman-arXiv:2204.11406v4  [cs.CL]  14 May 2022

--- PAGE 2 ---
tic representation of the augmented data. For in-
stance, token substitution is typically limited to the
named entities in the training corpus (Wu et al.,
2019), and the mixup tends to be imposed on the
example pairs with small semantic distance gaps
(Chen et al., 2020). For another thing, though the
two techniques seem to be orthogonal and probably
complementary to each other, it remains a poten-
tial challenge to effectively and naturally integrate
them.
In this work, we revisit the token substitution
and mixup methods for NER, and investigate the
two heterogeneous techniques under a uniﬁed meta
reweighting framework (as illustrated in Figure 1).
First, we try to relax the previous constraints to a
broader scope for these methods, allowing for more
diverse and larger-scale pseudo training examples.
However, this would inevitably produce some low-
quality augmented examples (i.e., noisy pseudo
data) in terms of linguistic correctness, which may
negatively affect the model performance. To this
end, we present a meta reweighting strategy for
controlling the quality of the augmented examples
and leading to noise-robust training. Also, we can
naturally integrate the two methods by using the
example reweighting mechanism, without any spe-
cialization in a speciﬁc self-augmentation method.
Finally, we carry out experiments on several
Chinese and English NER benchmark datasets to
evaluate our proposed methods. We mainly focus
on the low-resource settings, which can be simu-
lated by using only part of the standard training
set when the scale is large. Experimental results
show that both our token substitution and mixup
method coupled with the meta-reweighting can ef-
fectively improve the performance of our baseline
model, and the combination can bring consistent
improvement. Positive gains become more signif-
icant as the scale of the training data decreases,
indicating that our self-augmentation methods can
handle the low-resource NER well. In addition, our
methods can still work even with a large amount
of training data. The code is available at https:
//github.com/LindgeW/MetaAug4NER .
2 Our Approach
In this section, we ﬁrstly describe our baseline
model. Then, we present our self-augmentation
methods to enhance the baseline model in the low-
resource settings. Finally, we elaborate on our meta
reweighting strategy, which aims to alleviate thenegative impact of the noisy augmented examples
caused by the self-augmentation while also ele-
gantly combining these augmentation methods.
2.1 Baseline Model
NER task is typically formulated as a sequence
labeling problem, which transforms entities/non-
entities into token-level boundary label sequence
by using the BIO or BIOES schema (Huang et al.,
2015; Lample et al., 2016). In this work, we adopt
BERT-BiLSTM-CRF as our basic model architec-
ture which consists of four components: (1) input
representation, (2) BiLSTM encoding, (3) CRF de-
coding, and (4) training objective.
Input Representation Given an input sequence
X= (x1;;xn)of lengthn, we ﬁrst convert it
into sequential hidden vectors using the pre-trained
BERT (Devlin et al., 2019):
e1;;en=BERT (X); (1)
where each token is mapped to a contextualized
representation correspondingly.
Encoding We use a bidirectional LSTM layer
to further extract the contextual representations,
where the process can be formalized as:
h1;;hn=BiLSTM (e1;;en);(2)
where hiis the hidden state output of the i-th token
in the sequence ( i2[1;n]).
Decoding First, a linear transformation layer is
used to calculate the initial label scores. Then,
a label transition matrix Tis used to model the
label dependency. Let Y= (y1;;yn)be a label
sequence, the score s(YjX)can be computed by:
oi=Whi+b;
s(YjX) =nX
i=1(Tyi 1;yi+oi[yi]);(3)
where W,bandTare the model parameters. Fi-
nally, we employ the Viterbi algorithm (Viterbi,
1967) to ﬁnd the best label sequence Y.
Training Objective We exploit the sentence-
level cross-entropy objective for training. Given
a gold-labeled training example (X;Y ), we have
the conditional probability p(YjX)based on the

--- PAGE 3 ---
Input BER T BiLSTM Meta-Reweighting Decoder Answer
Huawei won reviews
I like Apple  phone  
 I like Huawei phone  
I love Apple phone  
CRFMixup
EMS
NWS TSFigure 2: An overview of the self-augmentation framework for NER.
scoring function deﬁned in Equation 3, and then ap-
ply a cross-entropy function to calculate the single
example loss:
p(YjX) =exp 
s(YjX)
P
eYexp 
s(eYjX);
L(X;Y ) = logp(YjX):(4)
whereeYdenotes the candidate label sequences.
2.2 Self-Augmentation
Self-augmentation methods can reduce the demand
for abundant manually-annotated examples, which
can be implemented at the input level and represen-
tation level. Token substitution and mixup are two
popular methods for NER that correspond to these
two distinct levels. Here, we try to extend these
two self-augmentation methods.
Token Substitution Token substitution aims to
generate pseudo examples based on the original
gold-labeled training data by replacing the tokens
of input sentence with their synonym alternatives
(Wu et al., 2019; Dai and Adel, 2020). For NER,
Wu et al. (2019) adopted this method to obtain
performance gains on Chinese datasets where the
substituted objects are limited to named entities.
Dai and Adel (2020) empirically demonstrated the
superiority of synonym replacement among various
augmentation schemes where the synonyms are
retrieved from the off-the-shelf WordNet thesaurus.
Our token substitution is performed by building
a synonym dictionary, which covers the named en-
tity synonyms as well as numerous normal word
synonyms. Following Wu et al. (2019), we treat
all entities of the same type from the training setas synonyms, which are added to the entity dictio-
nary. We name it as entity mention substitution
(EMS). Meanwhile, we extend the substitution to
non-entity tokens (i.e., the corresponding label is
‘O’), which is named as normal word substitution
(NWS). Since unlabeled data in a speciﬁc domain
is easily accessible, we adopt the word2vec-based
algorithm (Mikolov et al., 2013; Pennington et al.,
2014) to mine tokens with similar semantics on
Wikidata via distributed word representation (Ya-
mada et al., 2020), and build a normal word syn-
onym dictionary from the k-nearest token set based
on cosine similarity distance. Note that this scheme
does not require access to thesaurus for a speciﬁc
domain in order to obtain synonyms.
Figure 2 presents an example of token substi-
tution, where EMS and NWS are both involved.
Speciﬁcally, for a given gold-labeled training ex-
ample (X;Y ), we replace the entity token of X
with a sampled entity from the entity dictionary
which has the same entity type, and meanwhile
replace the non-entity token of Xwith a sampled
synonym. Then, we can obtain a pseudo exam-
ple(X;Y). Especially, we balance the EMS and
NWS strategies based on a ratio by adjusting the
percentage of EMS operations, aiming for a good
trade-off between entity diversity and context di-
versity. And, we refer to this method as TS in the
rest of this paper for short.
Mixup for CRF Unlike token substitution per-
formed at the ground input, the mixup technique
(Zhang et al., 2018) generates virtual examples at
the feature representation level in the NLP ﬁeld
(Guo et al., 2019). The main idea is to perform
linear interpolations on both the input and ground-
truth output between randomly sampled example

--- PAGE 4 ---
pairs from the given training set. Chen et al. (2020)
presented the ﬁrst work based on the token classiﬁ-
cation framework for the NER task, and the mixup
strategy is constrained to the examples pairs where
the input sentences are semantically similar by us-
ing speciﬁc heuristic rules. Different from their
method, we extend the mixup technique to the CRF
decoding.
Formally, give an example pair (X1;Y1)and
(X2;Y2)randomly sampled from the gold-labeled
training set, we ﬁrstly obtain their vector rep-
resentations through Equation 1, resulting in
e1;1e1;n1, ande2;1e2;n2, respectively. Then
we apply the linear interpolation to obtain a new
virtual training example (X;Y). Here we assume
a regularization of pair-wise linear interpolation
over the input representations and the output scores,
where the following attributes should be satisﬁed:
X:(
BERT (X) =e1en
ei=e1;i+ (1 )e2;i;i2[1;n]
Y:s(YjX) =s(Y1jX) + (1 )s(Y2jX);(5)
wheren= max(n1;n2)1andis sampled from
aBeta(;)distribution (2[0;1]and > 0).
According to this formulation, the loss function can
be reformulated as:
L(X;Y) = logexp 
s(YjX)
P
eYexp 
s(eYjX)
=L(X;Y 1) + (1 )L(X;Y 2):(6)
which aligns with the training objective of Equation
4. In this way, our mixup method can ﬁt well with
the structural decoding.
2.3 Meta Reweighting
Although the self-augmentation techniques can ef-
ﬁciently generate numerous pseudo training exam-
ples, how to control the quality of augmented ex-
amples is a potential challenge that cannot be over-
looked. In particular, unlike sentence-level classiﬁ-
cation tasks, entity recognition is highly sensitive
to the semantics of the context. While positive
augmented examples can help our model advance,
some low-quality augmented examples that are in-
evitably introduced during self-augmentation may
hurt the ﬁnal model performance.
In this paper, we leverage a meta reweighting
mechanism to dynamically and adaptively assign
1Special zero-vector pads are used to align two sequences
with different lengths.Algorithm 1 The training procedure of the meta
reweighting strategy
Input: Initial model parameters (0), clean train-
ing datasetD, augmented training dataset ^D,
batch sizem;n , training steps T
Output: Updated model parameters (T)
1:fort= 1toTdo
2: Initialize the trainable parameter .
3: {xc,yc} SampleMiniBatch( D,m).
4: {xa,ya} SampleMiniBatch( ^D,n).
5:La Pn
i=1iL(f(xa;i; (t));ya;i).
6:r(t) Grad(La;(t)).
7: ^(t) (t) r(t).
8:Lc 1
mPm
i=1L(f(xc;i;^(t));yc;i).
9:r Grad(Lc;).
10: ^w Sigmoid ( r).
11:w ^wP
j^wj+.
12: ^La Pn
i=1wiL(f(xa;i; (t));ya;i).
13:r(t) Grad(^La;(t)).
14: (t+1) OptimizerStep ((t);r(t)).
15:end for
the example-wise weights to each mini-batch of
training data, motivated by Ren et al. (2018). The
key idea is that a small and clean meta-data set
is applied to guide the training of model parame-
ters, and the loss produced by the mini-batch of
meta-data is exploited to reweight the augmented
examples in each batch online. Intuitively, if the
data distribution and gradient-descent direction of
the augmented example are similar to those of the
sample in the meta-data set, our model could better
ﬁt this positive augmented sample and increase its
weight, and vice versa. In other words, the clean
and valid augmented examples are more likely to
be fully trained.
More speciﬁcally, suppose that we have a
set ofNaugmented training examples ^D=
f(Xi;Yi)gN
i=1, our ﬁnal optimizing objective can
be formulated as a weighted loss as follows:
(w) = arg min
NX
i=1wiL(f(Xi; );Yi);(7)
wherewi0is the learnable weight for the loss
ofi-th training example. f(; ) represents the
forward process of our model (with parameter ).
The optimal parameter wis further determined by
minimizing the following loss computed on the
meta example set D=f(Xm
i;Ym
i)gM
i=1(M

--- PAGE 5 ---
N):
w=arg min
w1
MMX
i=1L(f(Xm
i; (w));Ym
i);
(8)
Accordingly, we need to calculate the optimal
andwin Equation 7 and 8 based on two nested
loops of optimization iteratively. For simplicity and
efﬁciency, we take a single gradient-descent step
for each training iteration to update them via an
online-approximation manner. At every training
stept, we sample a mini-batch augmented exam-
plesf(Xi;Yi)gn
i=1initialized with the learnable
weights. After a single optimization step, we
have:
^(t+1)() = (t) rnX
i=1iL(f(Xi; );Yi);
(9)
whereis the inner-loop step size. Based on
the updated parameters, we then calculate the
loss of the sampled mini-batch meta examples
f(Xmeta
j;Ymeta
j)gm
j=1:
Lmeta(^)=1
mmX
j=1L(f(Xmeta
j;^(t+1));Ymeta
j);
(10)
To generalize the parameters ^well to the meta-
data set, we take the gradients of w.r.t the meta
loss to produce example weights and normalize it
along mini-batch:
^wi=( riLmeta(^)
i=0);
wi=^wiP
j^wj+:(11)
where()is the sigmoid function and is a small
value to avoid division by zero. Finally, we opti-
mize the model parameters over augmented exam-
ples with the calculated weights.
Algorithm 1 illustrates the detailed training pro-
cedure of the meta reweighting strategy. It is note-
worthy that the augmented training examples con-
tain the original clean training examples, which
serve as the unbiased meta-data. Since the algo-
rithm execution just requires a clear deﬁnition of
the training objective for the input examples, it
is also well adaptable for the virtual augmented
examples generated by our mixup method.3 Experiments
3.1 Settings
Datasets To validate our methods, we conduct
experiments on Chinese benchmarks: OntoNotes
4.0 (Weischedel et al., 2011) and Weibo NER
(Peng and Dredze, 2015), as well as English bench-
marks: CoNLL 2003 (Sang and Meulder, 2003)
and OntoNotes 5.02(Pradhan et al., 2013). The Chi-
nese datasets are split into training, development
and test sections following Zhang and Yang (2018)
while we take the same data split as Benikova et al.
(2014) and Pradhan et al. (2012) on the English
datasets. We follow Lample et al. (2016) to use
the BIOES tagging scheme for all datasets. The
detailed statistics can be found in Table 4.
Dataset Type Train Dev Test
OntoNotes 4#sent 15.7k 4.3k 4.3k
#char 491.9k 200.5k 208.1k
#entity 12.6k 6.6k 7.3k
Weibo#sent 1.4k 0.27k 0.27k
#char 73.8k 14.5k 14.8k
#entity 1.9k 0.4k 0.4k
CoNLL03#sent 15.0k 3.5k 3.7k
#token 203.6k 51.4k 46.4k
#entity 23.5k 5.9k 5.6k
OntoNotes 5#sent 59.9k 8.5k 8.3k
#token 1088.5k 147.7k 152.7k
#entity 81.8k 11.1k 11.3k
Table 4: Statistics of datasets. #sent and #entity stand
for the number of sentences and entity words, respec-
tively.
Implementation Details We use one-layer BiL-
STM and the hidden size is set to 768. The
dropout ratio is set to 0.5 for the input and output
of BiLSTM. Regarding BERT, we adopt BERT-
base model3(BERT-base-cased for the English
NER) and ﬁne-tune the inside parameters together
with all other module parameters. We use the
AdamW(Loshchilov and Hutter, 2019) optimizer
to update the trainable parameters with 1=0.9 and
2=0.99. For the BERT parameters, the learning
rate is set to 2e 5. For other module parameters ex-
cluding BERT, a learning rate of 1e 3and weight
decay of 1e 4are used. Gradient clipping is used
to avoid gradient explosion by a maximum value
2https://catalog.ldc.upenn.edu/
LDC2013T19
3https://github.com/huggingface/
transformers

--- PAGE 6 ---
ModelsON 4 ON 5 CoNLL03
5% 10% 30% 5% 10% 30% 5% 10% 30%
Baseline 75.07 76.14 80.88 81.22 83.51 86.27 85.12 87.11 89.24
+ TS w/oMR 74.58 75.94 79.83 82.12 83.82 86.23 85.93 87.66 89.14
+ TS w/MR 76.08 76.85 81.23 82.58 83.92 86.50 86.25 88.00 89.55
+ Mixup w/oMR 75.21 76.03 80.00 82.63 83.77 86.04 86.18 87.75 89.48
+ Mixup w/MR 76.15 76.75 80.97 82.83 84.12 86.60 86.33 88.03 89.75
+ Both w/oMR 76.33 76.91 81.40 82.85 84.33 86.88 86.51 88.10 89.96
+ Both (Final) 76.82 77.13 81.66 82.98 84.52 87.09 86.76 88.25 90.12
Dai and Adel (2020) 75.05 76.75 81.24 82.47 83.90 86.55 86.22 87.86 89.91
Chen et al. (2020) – – – – – – 84.85 87.85 89.87
Chen et al. (2020) ( Semi ) – – – – – – 86.33 88.78 90.25
Table 1: Results on OntoNotes and CoNLL03 using 5%, 10%, and 30% of the training data. Semi : additional
10,000 unlabeled training examples are used.
Models ON 4 Weibo
Baseline 81.73 69.10
+ TS w/oMR 81.38 68.69
+ TS w/MR 81.85 69.61
+ Mixup w/oMR 81.68 69.96
+ Mixup w/MR 82.15 70.53
+ Both w/MR 82.33 71.15
+ Both (Final) 82.48 71.42
Meng et al. (2019)y81.63 67.60
Hu and Wei (2020) 80.20 64.00
Mengge et al. (2020) 80.60 69.23
Li et al. (2020a) 81.82 68.55
Nie et al. (2020a)y81.18 69.78
Nie et al. (2020b) – 69.80
Li et al. (2020b)y82.11 –
Ma et al. (2020) 82.81 70.50
Xuan et al. (2020)y82.04 71.25
Liu et al. (2021) 82.08 70.75
Table 2: Performance comparisons using the full train-
ing data on OntoNotes 4 (ON 4) and Weibo. Previous
SOTA results are also offered for comparisons. yde-
notes external knowledge is used.
of 5.0. All the models are trained on NIVIDIA
Tesla V100 (32G) GPUs. The higher library4
is utilized for the implementation of second-order
optimization involved in Algorithm 1.
For the NWS, we use the word vectors trained on
Wikipedia data5based on the GloVe model (Pen-
nington et al., 2014) and build the synonym set
for any given non-entity word based on the top-5
4https://github.com/facebookresearch/
higher
5https://dumps.wikimedia.org/Models CoNLL03 ON 5
Baseline 91.23 88.22
+ TS w/oMR 90.98 87.55
+ TS w/MR 91.64 88.84
+ Mixup w/oMR 91.04 87.46
+ Mixup w/MR 91.42 88.98
+ Both w/MR 91.88 89.24
+ Both (Final) 92.15 89.43
Chen et al. (2020) 91.83 –
Clark et al. (2018)z92.60 88.80
Fisher and Vlachos (2019) – 89.20
Li et al. (2020b)y93.04 91.11
Yu et al. (2020)y93.50 91.30
Xu et al. (2021)y– 90.85
Table 3: Performance comparisons using the full train-
ing data on CoNLL03 and OntoNotes 5 (ON 5). Pre-
vious SOTA results are also offered for comparisons. z
means the multi-task learning with more unlabeled data.
ydenotes external knowledge is used.
cosine similarity, where stop-words are excluded.
As mentioned in Section 2.2, we deﬁned two core
hyper-parameters for our self-augmentation meth-
ods, one for TS (i.e., ) and the other for mixup
(i.e.,). Speciﬁcally, we set = 20% and
by sampled from the Beta(;)distribution with
= 7, where the details will be shown in the anal-
ysis section. Meanwhile, we conduct the augmen-
tation up to 5 times corresponding to the original
training data.
Evaluation We conduct each experiment by 5
times and report the average F1 score. The best-
performing model on the development set is then
used to evaluate on the test set.

--- PAGE 7 ---
3.2 Main Results
The main results are presented in Table 1, 2 and
3, verifying the effectiveness of our method under
the low-resource setting and the standard full-scale
setting, respectively. Since Weibo is a small-scale
dataset, we do not consider its partial training set
for the low-resource setting.
Low-Resource Setting We randomly sample
5%, 10%, and 30% of the original training data
from OntoNotes and CoNLL03 for simulation stud-
ies. Table 1 shows the results where F1 scores
of the baseline, +TS, +Mixup, and +Both are re-
ported. We can observe that: (1) the baseline perfor-
mance will drop signiﬁcantly as the size of training
data get reduced gradually, which demonstrates
the performance of the supervised NER model re-
lies heavily on the scale of the labeled training
data. (2) although the number of training examples
has increased, vanilla self-augmentation (without
meta reweighting) might degrade the model perfor-
mance due to potentially unreliable pseudo-labeled
examples. The meta reweighting strategy helps to
adaptively weight the augmented examples during
training, which combats the negative impact and
leads to a stable and positive performance boost.
In addition, as the scale of the training data de-
creases, the effectiveness of the augmentation meth-
ods can be more signiﬁcant, indicating that our self-
augmentation methods are highly beneﬁcial for the
low-resource settings, and the two-stage combina-
tion of the two heterogeneous methods can yield
better performance consistently.
Full-Scale Setting Table 2 and 3 show the results
using full-scale training data. The results demon-
strate that our baseline model is already strong.
The model after vanilla augmentation could per-
form slightly worse since each training example
is treated equally even if it is noisy. This also
implies our meta reweighting makes great sense.
Furthermore, our ﬁnal model (+Both) can further
achieve performance gains by integrating these self-
augmentation methods with the meta reweighting
mechanism. The overall trend is similar to the low-
resource setting, but the gains are relatively smaller
when the training data is sufﬁcient. That may be at-
tributed that the size of training data is large enough
to narrow the performance gap between the base-
line and augmented models. It also suggests that
our method does not hurt the model performance
even when using enough training data.Comparison with Previous Work We also com-
pare our method with previous representative
SOTA work, where all referred systems exploit
the pre-trained BERT model. As shown, compared
to Dai and Adel (2020) and Chen et al. (2020), our
method performs better when using limited train-
ing data. For Chen et al. (2020), the pure mixup
performs slightly better due to the well-designed
example sampling strategy, but our overall frame-
work outperforms theirs. Moreover, our method
can match the performance of the semi-supervised
setting that uses additional 10K unlabeled training
data. Besides, our ﬁnal model, without utilizing
much external knowledge, can achieve very com-
petitive results on the full training set in comparison
to most previous systems.
3.3 Analysis
In this subsection, we further conduct detailed ex-
perimental analyses on the CoNLL03 dataset for a
better understanding of our method. Our main con-
cern is on the low-resource setting, therefore the
models based on 5%, 10% and 30% of the original
training data are our main focus.
Augmentation Times The size of augmented ex-
amples is an essential factor in ﬁnal model perfor-
mance. Typically, we examine the 5% CoNLL03
training data. As illustrated in Figure 3, the larger
pseudo examples can obtain better performance in
a certain range. However, as the times of augmen-
tation increases, the uptrend of performance slows
down. The improvement tends to be stable when
the pseudo samples are increased to about 5 times
the original training data. Excessively increasing
the augmentation times does not necessarily bring
consistent performance improvement. And we se-
lect an appropriate value for training data of differ-
ent sizes from a range [1, 8].
0 1 2 3 4 5 6 7 88585.58686.587
Augmentation TimesF1 (%)
Figure 3: Inﬂuence of augmentation times for 5% train-
ing data of CoNLL03. Times=0 means original training
data without any augmentation.

--- PAGE 8 ---
0204060801008585.58686.5
γ(%)F1 (%)
(a) 5%0204060801008787.588
γ(%)
(b) 10%baseline TS
0204060801008989.2589.5
γ(%)
(c) 30%Figure 4: Performance against different EMS rate .
Inﬂuence of for Token Substitution Regard-
ing our TS strategy, we take both NWS and EMS
into account simultaneously. The two parts are
blended by a percentage parameter , namely
for EMS and 1 for NWS. Here we examine
the inﬂuence of in the sole self-augmentation
model by TS. Figure 4 shows the results, where
= 0and= 100% denote the model with only
NWS and EMS, respectively. As shown, our model
can achieve the overall better performance when
= 20% , indicating that both of them are helpful
for the TS strategy, and NWS can be slightly bet-
ter. One possible reason is that the entity words
in original training examples are relatively sparse
(i.e., the ‘O’ label is dominant), allowing the NWS
to produce more diverse pseudo examples.
Mixup Parameter We further inspect the model
with the mixup strategy alone so as to understand
the important factors of the mixup model. First, we
analyze the inﬂuence of the mixing parameter .
As depicted in Figure 5, we can see that indeed af-
fects the effectiveness of the mixup method greatly.
Considering the feature of Beta distribution, the
sampledwill be more concentrated around 0.5 as
thevalue becomes large, resulting in a relatively
balanced weight between the mixed example pairs.
The model performance remains stable when is
around 7. Second, we study where to conduct the
mixup operation since there are two main options
in our framework, i.e., the hidden representations
of either the BERT or BiLSTM for linear inter-
polation. Table 5 reports the comparison results,
demonstrating the former is a better choice.
Case Study To further understand the effec-
tiveness of the meta-reweighting mechanism, we
present several high-quality and low-quality exam-
ples in Table 6. As shown, the difference between
the positive and negative examples for TS could be
reﬂected in the syntactic and semantic validity of
the augmented examples. Similarly, for the mixup,
135791184.585.586.5
αF1 (%)
(a) 5%135791186.587.588.5
α
(b) 10%baseline Mixup
13579118989.590
α
(c) 30%Figure 5: Performance against different mixing param-
eter by Beta(;)distribution.
Method 5% 10% 30%
Baseline 85.12 87.11 89.24
Mixing on BiLSTM 86.15 87.66 89.41
Mixing on BERT 86.33 88.03 89.75
Table 5: Performance comparison of the mixup strategy
on BERT or BiLSTM layer.
it seems that the valid example pairs are more likely
to generate positive augmented examples.
4 Related Work
In recent years, research on NER has concen-
trated on either enriching input text representations
(Zhang and Yang, 2018; Nie et al., 2020b; Ma et al.,
2020) or reﬁning model architectures with various
external knowledge (Zhang and Yang, 2018; Ye
and Ling, 2018; Li et al., 2020a; Xuan et al., 2020;
Li et al., 2020b; Yu et al., 2020; Shen et al., 2021).
Particularly, NER model, with the aid of large pre-
trained language models (Peters et al., 2018; Devlin
et al., 2019; Liu et al., 2019), has achieved impres-
sive performance gains. However, these models
mostly depend on rich manual annotations, making
it hard to cope with the low-resource challenges
in real-world applications. Instead of pursuing a
sophisticated model architecture, in this work, we
exploit the BiLSTM-CRF model coupled with the
pre-trained BERT as our basic model structure.
Self-augmentation methods have been widely
investigated in various NLP tasks (Zhang et al.,
2018; Wei and Zou, 2019; Dai and Adel, 2020;
Zeng et al., 2020; Ding et al., 2020). The main-
stream methods can be broadly categorized into
three types: (1) token substitution (Kobayashi,
2018; Wei and Zou, 2019; Dai and Adel, 2020;
Zeng et al., 2020), which performs local substitu-
tion for a given sentence, (2) paraphrasing (Kumar
et al., 2019; Xie et al., 2020; Zhang et al., 2020b),
which involves sentence-level rewriting without sig-

--- PAGE 9 ---
Augmentation Examples
Original [Diana] PER met [Will
Carling] PER at an ex-
clusive gymnasium in
[London] LOC.
Positive TS [Freddy Pinas] PERinvited
[John Marzano] PER at an
available gym in [UK] LOC.
Negative TS [Tim Henman] PER visit
[Simpson] PERat an avail-
able room in [NICE] LOC.
Positive MixupFrench 1997 budget due
around September 10 -
Juppe.
Jewish 1999 deﬁcit
due about October 20
M.Atherton.
Negative MixupOlympic champion Agassi
meets Karim Alami of Mo-
rocco in the ﬁrst round.
Olympic champion
Nathalie Lancien of France
also missed the winning
attack.
Table 6: Case study on positive and negative augmenta-
tion with respect to the TS and mixup.
niﬁcantly changing the semantics, and (3) mixup
(Zhang et al., 2018; Chen et al., 2020; Sun et al.,
2020), which carries out the feature-level augmen-
tation. As a data-agnostic augmentation technique,
mixup can help improve the generalization and ro-
bustness of our neural model acting as an useful
regularizer (Verma et al., 2019). For NER, token
substitution and mixup are very suitable and have
been exploited successfully with specialized efforts
(Dai and Adel, 2020; Chen et al., 2020; Zeng et al.,
2020), while the paraphrasing strategy may result
in structure incompleteness and token-label incon-
sistency, thus there has not been widely concerned
yet. In this work, we mainly investigate the token
substitution and mixup techniques for NER, as well
as their integration. Despite the success of various
self-augmentation methods, quality control may be
an issue easily overlooked by most methods.
Many previous studies have explored the ex-
ample weighting mechanism in domain adaption
(Jiang and Zhai, 2007; Wang et al., 2017; Osumi
et al., 2019). Xia et al. (2018) and Wang et al.
(2019) looked into the example weighting methodsfor cross-domain tasks. Ren et al. (2018) adapted
the MAML algorithm (Finn et al., 2017) and pro-
posed a meta-learning algorithm to automatically
weight training examples of the noisy label using
a small unbiased validation set. Inspired by their
work, we extend the meta example reweighting
mechanism to the NER task, which is exploited
to adaptively reweight mini-batch augmented ex-
amples during training. The main purpose is to
mitigate the potential noise effects brought by the
self-augmentation techniques, advancing a noise-
robust model, especially in low-resource scenarios.
5 Conclusion
In this paper, we re-examine two heterogeneous
self-augmentation methods (i.e., TS and mixup) for
NER, extending them into more unrestricted aug-
mentations without heuristic constraints. We fur-
ther exploit a meta reweighting strategy to alleviate
the potential negative impact of noisy augmented
examples introduced by the aforementioned relax-
ation. Experiments conducted on several bench-
marks show that our self-augmentation methods
along with the meta reweighting mechanism are
very effective in low-resource settings, and still
work when enough training data is used. The com-
bination of the two methods can lead to consis-
tent performance improvement across all datasets.
Since our framework is general and does not rely
on a speciﬁc model backbone, we will further in-
vestigate other feasible model structures.
Acknowledgements
We thank the valuable comments of all anonymous
reviewers. This work is supported by grants from
the National Natural Science Foundation of China
(No. 62176180).
References
Darina Benikova, Chris Biemann, Max Kisselew, and
Sebastian Padó. 2014. Germeval 2014 named en-
tity recognition shared task: Companion paper. the
KONVENS GermEval Shared Task on Named Entity
Recognition .
Jiaao Chen, Zhenghui Wang, Ran Tian, Zichao Yang,
and Diyi Yang. 2020. Local additivity based data
augmentation for semi-supervised NER. In Proceed-
ings of EMNLP , November 16-20, 2020 , pages 1241–
1251.
Jason P. C. Chiu and Eric Nichols. 2016. Named en-

--- PAGE 10 ---
tity recognition with bidirectional lstm-cnns. TACL ,
pages 357–370.
Kevin Clark, Minh-Thang Luong, Christopher D. Man-
ning, and Quoc V . Le. 2018. Semi-supervised se-
quence modeling with cross-view training. In Pro-
ceedings of EMNLP , October 31 - November 4, 2018 ,
pages 1914–1925.
Xiang Dai and Heike Adel. 2020. An analysis of sim-
ple data augmentation for named entity recognition.
InProceedings of COLING, December 8-13, 2020 ,
pages 3861–3867.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of NAACL-HLT, June 2-7,
2019 , pages 4171–4186.
Bosheng Ding, Linlin Liu, Lidong Bing, Canasai Kru-
engkrai, Thien Hai Nguyen, Shaﬁq Joty, Luo Si, and
Chunyan Miao. 2020. DAGA: Data augmentation
with a generation approach for low-resource tagging
tasks. In Proceedings of EMNLP , pages 6045–6057.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.
Model-agnostic meta-learning for fast adaptation of
deep networks. In Proceedings of ICML, 6-11 Au-
gust 2017 , volume 70, pages 1126–1135.
Joseph Fisher and Andreas Vlachos. 2019. Merge and
label: A novel neural network architecture for nested
NER. In Proceedings of ACL, July 28- August 2,
2019 , pages 5840–5850.
Hongyu Guo, Yongyi Mao, and Richong Zhang. 2019.
Augmenting data with mixup for sentence classiﬁca-
tion: An empirical study. CoRR , abs/1905.08941.
James Hammerton. 2003. Named entity recognition
with long short-term memory. In Proceedings of
HLT-NAACL, May 31 - June 1, 2003 , pages 172–
175.
Dou Hu and Lingwei Wei. 2020. SLK-NER: exploiting
second-order lexicon knowledge for chinese NER.
InSEKE, July 9-19, 2020 , pages 413–417.
Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi-
rectional LSTM-CRF models for sequence tagging.
CoRR , abs/1508.01991.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in NLP. In Pro-
ceedings of ACL , pages 264–271.
Akbar Karimi, Leonardo Rossi, and Andrea Prati. 2021.
AEDA: an easier data augmentation technique for
text classiﬁcation. In Findings of EMNLP , 16-20
November, 2021 , pages 2748–2754.
Sosuke Kobayashi. 2018. Contextual augmentation:
Data augmentation by words with paradigmatic re-
lations. In Proceedings of NAACL-HLT, June 1-6,
2018 , pages 452–457.Ashutosh Kumar, Satwik Bhattamishra, Manik Bhan-
dari, and Partha P. Talukdar. 2019. Submodular
optimization-based diverse paraphrasing and its ef-
fectiveness in data augmentation. In Proceedings of
NAACL-HLT, June 2-7, 2019 , pages 3609–3619.
Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
InProceedings of NAACL-HLT, June 12-17, 2016 ,
pages 260–270.
Xiaonan Li, Hang Yan, Xipeng Qiu, and Xuanjing
Huang. 2020a. FLAT: chinese NER using ﬂat-lattice
transformer. In Proceedings of ACL, July 5-10,
2020 , pages 6836–6842.
Xiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong
Han, Fei Wu, and Jiwei Li. 2020b. A uniﬁed MRC
framework for named entity recognition. In Pro-
ceedings of ACL, July 5-10, 2020 , pages 5849–5859.
Wei Liu, Xiyan Fu, Yue Zhang, and Wenming Xiao.
2021. Lexicon enhanced chinese sequence la-
beling using BERT adapter. In Proceedings of
ACL/IJCNLP , August 1-6, 2021 , pages 5847–5858.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized BERT pretraining ap-
proach. CoRR , abs/1907.11692.
Ilya Loshchilov and Frank Hutter. 2019. Decoupled
weight decay regularization. In Proceedings of
ICLR 2019, May 6-9, 2019 .
Ruotian Ma, Minlong Peng, Qi Zhang, Zhongyu Wei,
and Xuanjing Huang. 2020. Simplify the usage of
lexicon in chinese NER. In Proceedings of ACL,
July 5-10, 2020 , pages 5951–5960.
Xuezhe Ma and Eduard H. Hovy. 2016. End-to-end
sequence labeling via bi-directional lstm-cnns-crf.
InProceedings of ACL, August 7-12, 2016 , pages
1064–1074.
Yuxian Meng, Wei Wu, Fei Wang, Xiaoya Li, Ping
Nie, Fan Yin, Muyu Li, Qinghong Han, Xiaofei Sun,
and Jiwei Li. 2019. Glyce: Glyph-vectors for chi-
nese character representations. In Proceedings of
NeurIPS, December 8-14, 2019 , pages 2742–2753.
Xue Mengge, Bowen Yu, Tingwen Liu, Yue Zhang,
Erli Meng, and Bin Wang. 2020. Porous lattice
transformer encoder for Chinese NER. In Proceed-
ings of COLING , pages 3831–3841.
Tomás Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed repre-
sentations of words and phrases and their composi-
tionality. In Proceedings of NeurIPS , pages 3111–
3119.

--- PAGE 11 ---
Yuyang Nie, Yuanhe Tian, Yan Song, Xiang Ao, and
Xiang Wan. 2020a. Improving named entity recog-
nition with attentive ensemble of syntactic informa-
tion. In Findings of EMNLP 2020, 16-20 November
2020 , pages 4231–4245.
Yuyang Nie, Yuanhe Tian, Xiang Wan, Yan Song, and
Bo Dai. 2020b. Named entity recognition for social
media texts with semantic augmentation. In Pro-
ceedings of EMNLP , November 16-20, 2020 , pages
1383–1391.
Kosuke Osumi, Takayoshi Yamashita, and Hironobu
Fujiyoshi. 2019. Domain adaptation using a gradi-
ent reversal layer with instance weighting. In 16th
International Conference on Machine Vision Appli-
cations, May 27-31, 2019 , pages 1–5.
Nanyun Peng and Mark Dredze. 2015. Named en-
tity recognition for chinese social media with jointly
trained embeddings. In Proceedings of EMNLP ,
September 17-21, 2015 , pages 548–554.
Jeffrey Pennington, Richard Socher, and Christopher D.
Manning. 2014. Glove: Global vectors for word rep-
resentation. In Proceedings of EMNLP , October 25-
29, 2014 , pages 1532–1543.
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of NAACL-HLT, June
1-6, 2018 , pages 2227–2237.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Hwee Tou Ng, Anders Björkelund, Olga Uryupina,
Yuchen Zhang, and Zhi Zhong. 2013. Towards ro-
bust linguistic analysis using ontonotes. In Proceed-
ings of CoNLL, August 8-9, 2013 , pages 143–152.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. Conll-
2012 shared task: Modeling multilingual unre-
stricted coreference in ontonotes. In Proceedings of
EMNLP-CoNLL, July 13, 2012 , pages 1–40.
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel
Urtasun. 2018. Learning to reweight examples for
robust deep learning. In Proceedings of ICML, July
10-15, 2018 , volume 80, pages 4331–4340.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of CoNLL, May 31 - June 1, 2003 ,
pages 142–147.
Yongliang Shen, Xinyin Ma, Zeqi Tan, Shuai Zhang,
Wen Wang, and Weiming Lu. 2021. Locate and la-
bel: A two-stage identiﬁer for nested named entity
recognition. In Proceedings of ACL/IJCNLP , August
1-6, 2021 , pages 2782–2794.
Lichao Sun, Congying Xia, Wenpeng Yin, Tingting
Liang, Philip S. Yu, and Lifang He. 2020. Mixup-
transformer: Dynamic data augmentation for NLPtasks. In Proceedings of COLING, December 8-13,
2020 , pages 3436–3440.
Vikas Verma, Alex Lamb, Christopher Beckham, Amir
Najaﬁ, Ioannis Mitliagkas, David Lopez-Paz, and
Yoshua Bengio. 2019. Manifold mixup: Better rep-
resentations by interpolating hidden states. In Pro-
ceedings of ICML, 9-15 June 2019 , pages 6438–
6447.
Andrew J. Viterbi. 1967. Error bounds for convolu-
tional codes and an asymptotically optimum decod-
ing algorithm. IEEE Trans. Inf. Theory , 13(2):260–
269.
Rui Wang, Masao Utiyama, Lemao Liu, Kehai Chen,
and Eiichiro Sumita. 2017. Instance weighting
for neural machine translation domain adaptation.
InProceedings of EMNLP , September 9-11, 2017 ,
pages 1482–1488.
Zhi Wang, Wei Bi, Yan Wang, and Xiaojiang Liu. 2019.
Better ﬁne-tuning via instance weighting for text
classiﬁcation. In Proceedings of AAAI, January 27 -
February 1, 2019 , pages 7241–7248.
Jason W. Wei and Kai Zou. 2019. EDA: easy data aug-
mentation techniques for boosting performance on
text classiﬁcation tasks. In Proceedings of EMNLP-
IJCNLP , November 3-7, 2019 , pages 6381–6387.
Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,
Martha Palmer, Nianwen Xue, Mitchell Marcus,
Ann Taylor, Craig Greenberg, Eduard Hovy, Robert
Belvin, et al. 2011. Ontonotes release 4.0.
LDC2011T03, Penn.: Linguistic Data Consortium .
Fangzhao Wu, Junxin Liu, Chuhan Wu, Yongfeng
Huang, and Xing Xie. 2019. Neural chinese named
entity recognition via CNN-LSTM-CRF and joint
training with word segmentation. In Proceedings of
WWW, May 13-17, 2019 , pages 3342–3348.
Rui Xia, Zhenchun Pan, and Feng Xu. 2018. Instance
weighting for domain adaptation via trading off sam-
ple selection bias and variance. In Proceedings of
IJCAI , page 4489–4495.
Qizhe Xie, Zihang Dai, Eduard H. Hovy, Thang Lu-
ong, and Quoc Le. 2020. Unsupervised data aug-
mentation for consistency training. In Proceedings
of NeurIPS, December 6-12, 2020 .
Lu Xu, Zhanming Jie, Wei Lu, and Lidong Bing. 2021.
Better feature integration for named entity recogni-
tion. In Proceedings of NAACL-HLT, June 6-11,
2021 , pages 3457–3469.
Zhenyu Xuan, Rui Bao, and Shengyi Jiang. 2020.
FGN: fusion glyph network for chinese named en-
tity recognition. In CCKS, November 12-15, 2020 ,
volume 1356, pages 28–40.
Ikuya Yamada, Akari Asai, Jin Sakuma, Hiroyuki
Shindo, Hideaki Takeda, Yoshiyasu Takefuji, and
Yuji Matsumoto. 2020. Wikipedia2vec: An efﬁcient

--- PAGE 12 ---
toolkit for learning and visualizing the embeddings
of words and entities from wikipedia. In Proceed-
ings of EMNLP: System Demonstrations, November
16-20, 2020 , pages 23–30.
Zhi-Xiu Ye and Zhen-Hua Ling. 2018. Hybrid semi-
markov CRF for neural sequence labeling. In Pro-
ceedings of ACL, July 15-20, 2018 , pages 235–240.
Juntao Yu, Bernd Bohnet, and Massimo Poesio. 2020.
Named entity recognition as dependency parsing. In
Proceedings of ACL, July 5-10, 2020 , pages 6470–
6476.
Xiangji Zeng, Yunliang Li, Yuchen Zhai, and Yin
Zhang. 2020. Counterfactual generator: A weakly-
supervised method for named entity recognition.
InProceedings of EMNLP , November 16-20, 2020 ,
pages 7270–7280.
Hongyi Zhang, Moustapha Cissé, Yann N. Dauphin,
and David Lopez-Paz. 2018. mixup: Beyond em-
pirical risk minimization. In Proceedings of ICLR,
April 30 - May 3, 2018 .
Rongzhi Zhang, Yue Yu, and Chao Zhang. 2020a. Se-
qmix: Augmenting active sequence labeling via se-
quence mixup. In Proceedings of EMNLP , Novem-
ber 16-20, 2020 , pages 8566–8579.
Yi Zhang, Tao Ge, and Xu Sun. 2020b. Parallel data
augmentation for formality style transfer. In Pro-
ceedings of ACL , pages 3221–3228.
Yue Zhang and Jie Yang. 2018. Chinese NER using
lattice LSTM. In Proceedings of ACL, July 15-20,
2018 , pages 1554–1564.
