# 2211.03946.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/data-augmentation/2211.03946.pdf
# Kích thước tệp: 3162522 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Hiểu về Vai trò của Mixup trong Knowledge Distillation:
Một Nghiên cứu Thực nghiệm
Hongjun Choi, Eun Som Jeon, Ankita Shukla, Pavan Turaga
Geometric Media Lab
Trường Nghệ thuật, Truyền thông và Kỹ thuật, Đại học Bang Arizona
Trường Kỹ thuật Điện, Máy tính và Năng lượng, Đại học Bang Arizona
hchoi71@asu.edu, ejeon6@asu.edu, ashukl20@asu.edu, pturaga@asu.edu

Tóm tắt
Mixup là một kỹ thuật tăng cường dữ liệu phổ biến dựa trên việc tạo ra các mẫu mới bằng cách nội suy tuyến tính giữa hai mẫu dữ liệu đã cho, để cải thiện cả khả năng tổng quát hóa và tính mạnh mẽ của mô hình được huấn luyện. Mặt khác, Knowledge distillation (KD) được sử dụng rộng rãi cho việc nén mô hình và học chuyển tiếp, bao gồm việc sử dụng kiến thức ngầm của một mạng lớn hơn để hướng dẫn việc học của một mạng nhỏ hơn. Thoạt nhìn, hai kỹ thuật này có vẻ rất khác nhau, tuy nhiên, chúng tôi phát hiện ra rằng "độ mịn" là liên kết kết nối giữa hai kỹ thuật này và cũng là thuộc tính quan trọng trong việc hiểu sự tương tác của KD với mixup. Mặc dù nhiều biến thể mixup và phương pháp distillation đã được đề xuất, vẫn còn nhiều điều cần hiểu về vai trò của mixup trong knowledge distillation. Trong bài báo này, chúng tôi trình bày một nghiên cứu thực nghiệm chi tiết về các khía cạnh quan trọng khác nhau của tính tương thích giữa mixup và knowledge distillation. Chúng tôi cũng xem xét kỹ lưỡng hành vi của các mạng được huấn luyện với mixup dưới ánh sáng của knowledge distillation thông qua phân tích mở rộng, trực quan hóa và các thí nghiệm toàn diện về phân loại hình ảnh. Cuối cùng, dựa trên các phát hiện của chúng tôi, chúng tôi đề xuất các chiến lược cải tiến để hướng dẫn mạng học sinh nhằm nâng cao hiệu quả của nó. Ngoài ra, các phát hiện của nghiên cứu này cung cấp những gợi ý sâu sắc cho các nhà nghiên cứu và người thực hành thường sử dụng các kỹ thuật từ KD. Mã nguồn của chúng tôi có sẵn tại https://github.com/hchoi71/MIX-KD.

1. Giới thiệu
Các mạng nơ-ron sâu đã đạt được hiệu suất ấn tượng trong một loạt các nhiệm vụ bao gồm dịch thuật ngôn ngữ [30, 26], phân loại hình ảnh [34, 6], và nhận dạng giọng nói [2, 19]. Để cải thiện thêm hiệu quả và hiệu suất của mô hình, một số lượng lớn các kỹ thuật huấn luyện đã được đề xuất như mixup-augmentation [38, 35] và knowledge distillation [8]. Cụ thể, mixup [38] là một kỹ thuật tăng cường dữ liệu thường được sử dụng dựa trên việc sử dụng các tổ hợp lồi của các mẫu và nhãn của chúng. Kỹ thuật này được giới thiệu để cải thiện khả năng tổng quát hóa cũng như tăng tính mạnh mẽ chống lại các ví dụ đối kháng. Gần đây, có một sự quan tâm ngày càng tăng trong việc giảm kích thước mô hình trong khi vẫn duy trì hiệu suất tương đương, điều này thu hẹp khoảng cách giữa các mạng lớn và các mạng nhỏ. KD là một trong những phương pháp hứa hẹn cho nhu cầu này [8]. Mục tiêu của KD là khai thác khả năng học biểu diễn kiến thức ngắn gọn (logit hoặc đặc trưng) từ một mô hình lớn hơn và sau đó nhúng kiến thức đó vào một mô hình nhỏ hơn. Ví dụ, trong phân loại hình ảnh, các mạng nơ-ron sâu tạo ra xác suất lớp bằng cách sử dụng hàm softmax chuyển đổi logit fi thành xác suất pi bằng cách so sánh fi với các logit khác như sau: pi=exp(fi/T)∑jexp(fj/T).

Trong knowledge distillation truyền thống [8], nhiệt độ T được sử dụng để tạo ra phân phối mềm hơn của các xác suất giả giữa các lớp đầu ra, trong đó nhiệt độ cao hơn làm tăng entropy của đầu ra, do đó cung cấp nhiều thông tin hơn để mô hình học sinh học hỏi.

Động lực: Trên bề mặt, mixup và KD rất khác nhau, tuy nhiên chúng tôi phát hiện ra rằng "độ mịn" hóa ra là liên kết kết nối giữa hai kỹ thuật này và là một thuộc tính rất quan trọng để hiểu sự tương tác của KD với mixup. Một cách trực quan, KD bao gồm việc học sinh bắt chước phân phối xác suất mịn của giáo viên, trong khi mixup nhân tạo đưa vào độ mịn (các nhãn không hoàn toàn one-hot) trong một mô hình thông qua các tổ hợp tuyến tính của các nhãn.

Mặc dù mixup-augmentation và KD là những kỹ thuật phổ biến trong việc huấn luyện mạng trong các ứng dụng khác nhau, sự tương tác giữa hai kỹ thuật này chưa được khám phá kỹ lưỡng. Trong bài báo này, chúng tôi điều tra chi tiết tác động của mixup-augmentation đối với knowledge distillation.

Để phát triển cái nhìn sâu sắc về một số hành vi thú vị của mạng, chúng tôi cung cấp các hình ảnh trực quan khác nhau ở mức độ đặc trưng và logit. Ví dụ, để phân tích cách mixup thực thi biểu diễn đặc trưng ở giữa mỗi lớp, chúng tôi chọn 5 lớp tương tự về mặt ngữ nghĩa từ bộ dữ liệu CIFAR100 (baby, boy, girl, man, và woman). Sau đó, chúng tôi chiếu các đặc trưng được trích xuất từ lớp gần cuối thành 2-D sử dụng tsne [29] như được thấy trong Hình 1. Hàng đầu tiên của hình biểu diễn biểu diễn đặc trưng cho hai mạng được huấn luyện từ đầu với mixup (Mix-S:ResNet110 hoặc Mix-S:ResNet56) và không có mixup (S:ResNet110 hoặc S:ResNet56). Nói chung, chúng tôi quan sát thấy rằng mô hình có khả năng cao hơn (S:ResNet110) khuyến khích các mạng sâu học các phép chiếu chặt chẽ trong mỗi lớp trong khi mô hình có khả năng thấp hơn (S:ResNet56) học các phép chiếu khuếch tán hơn. Điều này cũng được xác minh bởi các chỉ số phân cụm, như V-score [21], trong đó giá trị cao hơn ngụ ý việc phân cụm tốt hơn. Thú vị là, các mô hình được huấn luyện với mixup phân tán các đặc trưng trong các lớp tương tự, so với mô hình được huấn luyện không có mixup, mặc dù mô hình được huấn luyện với mixup cho thấy độ chính xác kiểm tra tốt hơn do việc tăng khả năng tổng quát hóa trên dữ liệu chưa thấy. Trong khi đó, chúng tôi phát hiện những quan sát tương tự về biểu diễn đặc trưng từ các mô hình được chưng cất khi sử dụng nhiệt độ cao hơn (tức là chưng cất logit mềm hơn) như được thấy trong hàng thứ hai của Hình 1. Từ bảng trong Hình 1, nhiệt độ T cao là lựa chọn mặc định để cải thiện hiệu suất. Bằng cách này, giáo viên chuyển giao nhiều thông tin hơn cho học sinh mặc dù nhiệt độ cao hơn bề ngoài thúc đẩy sự phân tán đặc trưng trong các lớp tương tự. Tuy nhiên, việc chuyển giao chất lượng giám sát cao cho mạng học sinh cũng rất quan trọng vì nó có thể hướng dẫn học sinh học các biểu diễn phân biệt từ mạng giáo viên vượt trội. Do đó, chúng tôi tìm cách tăng tín hiệu giám sát từ giáo viên đến học sinh mà không ảnh hưởng đến hiệu suất. Đây là nơi mixup trình bày con đường tiến về phía trước.

Tóm lại, trọng tâm của cái nhìn sâu sắc của chúng tôi là nếu một giáo viên được huấn luyện với dữ liệu được làm mịn từ mixup, thì việc 'làm mịn' thêm ở nhiệt độ cao trong quá trình chưng cất có thể được tránh. Điều này đảm bảo tín hiệu giám sát mạnh hơn trong khi vẫn hưởng lợi từ việc tăng cường bộ dữ liệu thông qua mixup.

Chúng tôi tóm tắt các đóng góp của mình như sau:
1. Chúng tôi cung cấp những cái nhìn sâu sắc mới về việc thiết kế các chiến lược cải tiến để học một mô hình học sinh, bằng cách hiểu sâu hơn về hành vi của các đặc trưng/logit và mạng được huấn luyện với mixup trong KD.
2. Để giảm tính quan trọng của việc chọn 'nhiệt độ' phù hợp, chúng tôi phát triển một sơ đồ tái tỉ lệ đơn giản đưa các đặc tính thống kê khác nhau trong logit giữa giáo viên và học sinh về một phạm vi tương tự trong khi vẫn bảo tồn thông tin tương đối giữa các lớp, do đó nhiệt độ T không còn được sử dụng.
3. Chúng tôi xác định rằng các cặp mixup được nội suy mạnh áp đặt thêm độ mịn lên logit, do đó chúng tôi có thể chỉ tạo ra một vài cặp mixup trong một batch, gọi là partial mixup (PMU), và vẫn đạt được hiệu suất tương đương và thậm chí tốt hơn với biến thể này trong KD.

2. Kiến thức cơ bản
Đầu tiên chúng tôi giới thiệu kiến thức cơ bản về mixup và KD thông qua mô tả toán học đơn giản. Cho dữ liệu huấn luyện D={(x1, y1), ...,(xn, yn)}, mục tiêu của nhiệm vụ phân loại là học bộ phân loại f:X →Rk bằng cách ánh xạ đầu vào x∈ X ⊆ Rd đến nhãn y∈ Y ={1,2, ..., K}. Cho L(f(x), y) là hàm mất mát đo lường mức độ kém của bộ phân loại f(x) trong việc dự đoán nhãn y.

Mixup Augmentation [38] Trong mixup augmentation, hai mẫu được trộn lẫn với nhau bằng nội suy tuyến tính như sau: ˜xij(λ) =λxi+ (1−λ)xj, và ˜yij(λ) =λyi+ (1−λ)yj, trong đó λ∈[0,1] tuân theo phân phối Pλ với λ∼Beta(α,α). Khi đó, hàm mất mát mixup có thể được mô tả như sau:
Lmix(f) =1/n² ∑i=1ⁿ ∑j=1ⁿ Eλ∼Pλ[L(f(˜xij(λ)),˜yij(λ))], (1)

trong đó L biểu diễn hàm mất mát cross-entropy trong nghiên cứu này. Cụ thể, siêu tham số λ trong Phương trình 1 được sử dụng để chỉ định mức độ trộn lẫn. Nói cách khác, tham số điều khiển α trong phân phối beta điều khiển cường độ nội suy giữa các cặp đặc trưng-đích, tức là α cao tạo ra các mẫu được nội suy mạnh.

Một số lượng đáng kể các biến thể của mixup đã được đề xuất [31, 35, 11]. Chiến lược chung của các phương pháp dựa trên trộn lẫn này về bản chất tương tự ở chỗ chúng mở rộng phân phối huấn luyện bằng cách pha trộn các hình ảnh và trộn lẫn nhãn của chúng theo tỷ lệ. Do đó, trong nghiên cứu này, chúng tôi chỉ chú ý đến mixup thông thường để điều tra sự tương tác giữa mixup và knowledge distillation [38].

Knowledge Distillation [8] Trong knowledge distillation, cho một mô hình giáo viên fT được huấn luyện trước trên bộ dữ liệu, mô hình học sinh fS được huấn luyện trên cùng một tập dữ liệu bằng cách sử dụng kiến thức ngắn gọn được tạo ra bởi fT. Cụ thể, một khi mạng giáo viên được huấn luyện, tham số của nó được đóng băng trong quá trình huấn luyện trong KD, và sau đó, mạng học sinh được huấn luyện bằng cách giảm thiểu sự tương tự giữa đầu ra của nó và các nhãn mềm được tạo ra bởi mạng giáo viên. Để làm điều này, chúng tôi giảm thiểu sự khác biệt giữa logit của học sinh và giáo viên như sau:
Lkd(fT, fS) = 1/n ∑i=1ⁿ KL(S(fT(xi)/T),S(fS(xi)/T)) (2)

trong đó S chỉ hàm softmax, KL đo lường sự khác biệt Kullback-Leiber, và logit đầu ra của mô hình được làm mịn bởi nhiệt độ T.

Data Augmentation trong Knowledge Distillation Gần đây, một số công trình đã sử dụng data augmentation và đạt được kết quả đầy hứa hẹn trong việc các mẫu được tăng cường cho phép các mạng học kiến thức thoải mái từ các góc nhìn khác nhau trong các khung distillation [33, 32, 12]. Trong các phương pháp có sẵn rộng rãi, chúng thường không cung cấp cái nhìn sâu sắc về hoạt động bên trong của các mô hình. Trong khi đó, các công trình gần đây [15, 23] đã nghiên cứu các quan điểm tương thích/không tương thích của KD với label smoothing [25] để cung cấp cái nhìn sâu sắc như vậy về các mô hình sâu thông qua phân tích thực nghiệm. Thay vào đó, chúng tôi quan tâm đến cơ chế cơ bản của augmentation dưới ánh sáng của quá trình KD. Nghiên cứu của chúng tôi bao gồm các khía cạnh kép nhằm trình bày cả quan điểm tương thích và không tương thích thông qua phân tích thực nghiệm toàn diện. Hơn nữa, dựa trên các quan sát, chúng tôi đề xuất một chiến lược học tập tốt hơn để nâng cao hiệu suất của mạng.

3. Những phát hiện chính từ sự tương tác giữa Mixup và KD
Trong phần này, chúng tôi thảo luận về các phát hiện chính của mình. Chúng tôi bắt đầu bằng cách tham khảo Hình 2. Ở đây, chúng tôi mô tả bốn kịch bản có thể trong đó mixup-augmentation có thể tham gia vào KD như sau; giáo viên tiêu chuẩn và học sinh tiêu chuẩn (T&S), giáo viên được huấn luyện mixup và học sinh tiêu chuẩn (Mix-T&S), giáo viên được huấn luyện mixup và học sinh được huấn luyện mixup (Mix-T&Mix-S), và giáo viên tiêu chuẩn và học sinh được huấn luyện mixup (T&Mix-S) dưới cùng nhiệt độ T= 4. Chúng tôi cố định mô hình học sinh là ResNet56 (RN56) và đánh giá nó bằng cách thay đổi các mô hình giáo viên từ ResNet20 (RN20) đến ResNet110 (RN110).

Như được thấy trong Hình 2, chúng tôi có thể đưa ra hai quan sát:
Thứ nhất, hiệu suất học sinh với sự giúp đỡ của giáo viên được huấn luyện mixup (Mix-T&S, Mix-T&Mix-S) luôn cho thấy hiệu quả ít hơn so với sự giúp đỡ của giáo viên tiêu chuẩn (T&S, T&Mix-S) dưới cùng cài đặt nhiệt độ ngay cả khi giáo viên được huấn luyện mixup bản thân cho thấy độ chính xác kiểm tra tốt hơn so với giáo viên được huấn luyện không có mixup. Thứ hai, nói chung, các mô hình giáo viên có khả năng cao hơn chưng cất học sinh tốt hơn, nhưng với giáo viên có khả năng thấp hơn (T:RN44 & S:RN56), hiệu suất học sinh vẫn cho thấy cải thiện độ chính xác, so với mô hình học sinh vanilla (đường gạch ngang xanh, S (No KD)). Dựa trên những quan sát này, bài báo này nhằm mục đích điều tra các câu hỏi sau: 1) Tại sao mô hình giáo viên được huấn luyện mixup làm suy giảm hiệu quả của học sinh trong KD? Chúng tôi trả lời câu hỏi này trong phần 3. Sau đó, 2) chúng ta có thể làm gì để cải thiện hiệu quả của knowledge distillation khi áp dụng mixup-augmentation? Chúng tôi giải quyết câu hỏi này trong phần 4.

--- TRANG 4 ---
Quan sát 1) Mixup vs Non-Mixup. Để điều tra tác động của mạng được huấn luyện với mixup-augmentation, chúng tôi đã chọn một vài lớp và chia chúng thành hai nhóm: (1) các lớp khác nhau về mặt ngữ nghĩa (Beaver, Apple, Aquarium Fish, Rocket, và Turtle) và (2) các lớp tương tự về mặt ngữ nghĩa (Baby, Boy, Girl, Man, và Woman), tất cả đều từ CIFAR100. Ví dụ, trong Hình 3, hình bên trái minh họa biểu diễn đặc trưng của lớp gần cuối trên tập huấn luyện. Nếu chúng ta nhìn vào số 1 trong vòng tròn đỏ, các phép chiếu của mô hình được huấn luyện mixup từ các lớp tương tự được phân tán nhiều hơn trong khi những phép chiếu của các lớp khác nhau vẫn được bảo tồn tốt trong cấu trúc của chúng. Mất mát thông tin này cũng có thể được đo lường bởi chỉ số phân cụm, V-score trên biểu đồ cột bên phải của hình, dẫn đến sự giảm mạnh trong V-score đối với các lớp tương tự.

Quan sát 2) Chưng cất từ các mô hình giáo viên có độ chính xác thấp. Nếu mạng học sinh được thiết lập, việc duy trì chất lượng tốt của sự giám sát (tức là hiệu suất của mạng giáo viên) là rất quan trọng trong việc huấn luyện học sinh tốt hơn. Như được theo dõi trong số 2 trong vòng tròn đỏ, các phép chiếu của học sinh với giáo viên có độ chính xác thấp hơn được phân tán đáng kể, sự giảm đáng kể trong V-score, cuối cùng làm suy giảm hiệu suất của học sinh. Điều này ngụ ý rằng học sinh tốt hơn được chưng cất với sự giúp đỡ của các đặc trưng có thể phân biệt được cho bởi giáo viên có khả năng cao.

Quan sát 3) Chưng cất từ mạng giáo viên được huấn luyện mixup. Quan sát 1) cho thấy rằng mô hình được huấn luyện mixup phân tán các đặc trưng trong các lớp tương tự. Bây giờ chúng tôi xem xét trường hợp trong đó giáo viên được huấn luyện mixup truyền đạt kiến thức cho học sinh. Trong KD, khi các mô hình học sinh và giáo viên được huấn luyện trên cùng một tập huấn luyện, chúng tôi lập luận rằng học sinh được huấn luyện với sự giám sát bởi giáo viên được huấn luyện mixup không thể tận dụng việc học kiến thức vượt trội do sự phân tán đặc trưng. Như được thể hiện bởi số 3 trong hình này, biểu diễn đặc trưng của các lớp tương tự trong mạng học sinh cuối cùng trở nên phân tán hơn, và V-score cũng giảm trên cả tập huấn luyện và kiểm tra. Chúng tôi cung cấp các hình ảnh trực quan của tập huấn luyện và kiểm tra trong tài liệu bổ sung.

Quan sát 4) và 5) Biểu diễn Logit. Khác với biểu diễn đặc trưng được thể hiện trong các quan sát 1)-3), chúng tôi tiếp tục trực quan hóa phân phối xác suất của mạng học sinh. Đầu tiên, chúng tôi tính trung bình xác suất của tất cả các lớp trên tập huấn luyện và kiểm tra của CIFAR100 và minh họa phân phối trung bình của các ví dụ thuộc về cùng một danh mục để hiển thị dự đoán của mô hình cho danh mục đó. Để so sánh với đo lường định lượng, chúng tôi cũng cung cấp các giá trị độ chính xác trung bình và entropy được tính toán trên tất cả các ví dụ trong Hình 4, trong đó entropy được sử dụng phổ biến để đo lường độ mịn của phân phối. Giá trị entropy càng cao thì phân phối càng mịn.

Ở đây, chúng tôi quan sát hai hiện tượng thú vị; Quan sát 4) Một mô hình được huấn luyện mixup tạo ra logit đầu ra mềm hơn, được minh họa bởi các thanh đỏ ngắn trong cả tập huấn luyện và kiểm tra. Từ quan sát này, chúng tôi suy đoán rằng mixup-augmentation tham gia vào việc huấn luyện học sinh trong KD đóng góp thêm độ mịn cho logit. Hơn nữa, học sinh học từ giáo viên tiêu chuẩn vượt trội hơn giáo viên được huấn luyện mixup về mặt độ chính xác (74.98% vs 73.92% và 71.78% vs 70.60%) dưới cùng cài đặt nhiệt độ T= 4. Kết luận, khi chưng cất từ giáo viên được huấn luyện mixup, việc sử dụng nhiệt độ cao ảnh hưởng bất lợi đến độ chính xác của học sinh. Quan sát 5) Khi kiến thức kém được tạo ra bởi giáo viên có độ chính xác thấp (T:RN20) chuyển giao cho học sinh, mức độ tin cậy trong dự đoán giảm đáng kể như được thấy trong các thanh xám trên cả hai tập, dẫn đến sự giảm đáng kể trong độ chính xác kiểm tra trong cả hai trường hợp (74.98% →71.78%, 73.92% →70.60%). Điều này ngụ ý rằng việc chuyển giao chất lượng tốt của sự giám sát cho học sinh là rất quan trọng để knowledge distillation thành công.

Quan sát 6) Mix-T&S ở nhiệt độ thấp. Từ quan sát 5), trong khi T tăng được tin là có lợi để tạo ra biểu diễn tốt hơn cho KD, chúng tôi nhận xét rằng khi có mặt giáo viên được huấn luyện mixup, T tăng có thể là tác động bất lợi đến hiệu suất của KD do sự phân tán đặc trưng/độ mịn quá mức trong logit. Tại thời điểm này, người ta có thể nêu ra câu hỏi sau. Điều gì sẽ xảy ra nếu chúng ta hạ nhiệt độ để giảm độ mịn trong logit?

Để điều tra mức độ nhiệt độ ảnh hưởng đến hiệu suất kiểm tra trong KD, chúng tôi trực quan hóa độ chính xác kiểm tra dưới dạng bản đồ nhiệt trong Hình 5. Trong hình này, chúng tôi khám phá tất cả các tổ hợp của giáo viên và học sinh được huấn luyện mixup. Bảng bên trái của hình này hiển thị độ chính xác kiểm tra dưới cài đặt T= 4 và bảng bên phải dành cho T= 1. Chúng tôi lưu ý rằng khi áp dụng nhiệt độ thấp hơn, làm cho logit ít mịn hơn, chất lượng kiến thức được chuyển giao cho học sinh càng tốt. Kết quả này chứng minh rằng kiến thức từ giáo viên được huấn luyện mixup ở nhiệt độ cao làm cho mô hình học sinh kém hiệu quả hơn.

4. Các chiến lược học tập hiệu quả cho Mix-KD
Dựa trên các phát hiện của chúng tôi, chúng tôi quan sát thấy rằng độ mịn trong logit là một thuộc tính quan trọng trong distillation, như được thấy trong Hình 5. Vì rất khó để đo lường độ mịn phù hợp từ quan điểm của học sinh, nhiều phương pháp distillation đã phụ thuộc nhiều vào tìm kiếm brute-force ngây thơ để tìm nhiệt độ phù hợp. Để giảm tính quan trọng của việc chọn nhiệt độ phù hợp và giảm bớt độ mịn quá mức bởi mixup mạnh, chúng tôi giới thiệu một chiến lược học tập hiệu quả để cải thiện hiệu suất của KD.

Partial mixup. Tham số điều khiển trong phân phối beta đóng vai trò quan trọng trong việc điều khiển cường độ giải thích, điều này cũng ảnh hưởng đến mức độ đầu ra được làm mềm. Khi α→1, nó cung cấp logit đầu ra mềm hơn. Tuy nhiên, có sự đánh đổi giữa việc tránh độ mịn quá mức và cải thiện tính mạnh mẽ chống lại các cuộc tấn công đối kháng trong knowledge distillation. Để giảm bớt vấn đề này, chúng tôi đề xuất chỉ tạo ra một lượng nhỏ các cặp mixup được sử dụng trong huấn luyện, gọi là partial mixup (PMU). Ví dụ, PMU=10% có nghĩa là chỉ 10% cặp mixup được sử dụng trong một batch và phần còn lại được giữ nguyên. Để hiểu thêm về hành vi của partial mixup ảnh hưởng đến xác suất đầu ra như thế nào, chúng tôi cung cấp các ví dụ mẫu sử dụng 2 lớp trong tài liệu bổ sung.

Rescaled logits. Ở đây, chúng tôi đề xuất sử dụng độ lệch chuẩn như nhiệt độ sao cho logit được tạo ra bởi lớp đầu ra của mạng được tái tỉ lệ bằng cách chia nó cho độ lệch chuẩn của logit đó, do đó nhiệt độ T không còn là siêu tham số. Cách này có thể đưa hai đặc tính thống kê khác nhau giữa logit của giáo viên và học sinh về các phạm vi tương tự trong khi nó không làm tổn hại cấu trúc tương đối giữa các lớp. Chúng tôi nhấn mạnh tầm quan trọng của việc tái tỉ lệ logit vì phần trộn lẫn ngẫu nhiên λ từ phân phối beta điều khiển mức độ mịn, tạo ra đầu ra mịn bất thường ở mỗi lần lặp. Do đó, chúng tôi thay thế logit đầu ra bằng logit được tái tỉ lệ, và hàm mất mát trở thành như sau:

Lkdr(˜fT,˜fS) = 1/n ∑i=1ⁿ KL(S(fS(xi)/σ(fS(xi))),S(fT(xi)/σ(fT(xi)))), (3)

trong đó ˜f=f/σ(f), S chỉ hàm softmax, KL là sự khác biệt Kullback-Leibler, và σ(·) là độ lệch chuẩn của logit đầu vào. Sau đó, mục tiêu huấn luyện cuối cùng cho học sinh trong KD như sau:

minE(x,y)∼D Eλ∼Pλ[γkdLmix(˜fS) +αkdLkdr(˜fT,˜fS)], (4)

trong đó γkd và αkd là các tham số cân bằng. Lưu ý, siêu tham số của lượng partial trong PMU không được ký hiệu trong phương trình này.

Lựa chọn α và PMU. Hơn nữa, chúng tôi điều tra cách mỗi siêu tham số (lượng partial trong PMU và tham số điều khiển α) ảnh hưởng đến hiệu suất học sinh trong KD dưới tổ hợp sau, T:RN110 và Mix-S:RN20 trên bộ dữ liệu CIFAR100. Chúng tôi đánh giá hiệu suất học sinh bằng cách thay đổi mức độ lượng partial từ 10% đến 80% với các tham số điều khiển khác nhau trong khoảng [0.2,0.4,0.7,1.0]. Hình 6 chỉ ra rằng PMU với α cao nói chung dẫn đến học sinh tốt hơn trong cài đặt này. Hiệu suất thấp nhất được quan sát trong FMU (sử dụng đầy đủ cặp mixup) với α= 1.0. Chúng tôi lưu ý rằng PMU không nhất thiết nhằm mục đích vượt trội hơn FMU, nhưng nó đóng vai trò điều tiết để điều chỉnh độ mịn một cách tinh tế, do đó giúp chúng tôi hiểu tác động của độ mịn trong KD. Chúng tôi trình bày phân tích hiệu suất trong phần 5.1 và cũng nghiên cứu thêm cách partial mixup phản ứng với các cuộc tấn công đối kháng trong phần 5.3.

5. Thí nghiệm
Trong phần này, chúng tôi trình bày các kết quả thí nghiệm để xác nhận các phỏng đoán và phát hiện của chúng tôi. Trong phần trước, chúng tôi lưu ý rằng (1) giáo viên được huấn luyện mixup tạo ra học sinh kém hơn so với giáo viên được huấn luyện không có mixup dưới cùng nhiệt độ (2) nhưng việc hạ nhiệt độ có thể khôi phục độ chính xác kiểm tra. Trong KD, mixup augmentation chỉ hữu ích cho giáo viên nếu giáo viên được huấn luyện mixup cung cấp cho học sinh những lợi ích bổ sung như kiến thức tốt hơn và sức mạnh bổ sung của tính mạnh mẽ. Thông qua phân tích của chúng tôi, chúng tôi thấy rằng hiệu ứng mong đợi của việc sử dụng giáo viên được huấn luyện mixup không thỏa mãn. Do đó, chúng tôi sử dụng giáo viên được huấn luyện không có mixup cho thí nghiệm của mình, tức là T&Mix-S.

5.1. Phân loại hình ảnh trên CIFAR100 & ImageNet
Thí nghiệm trên CIFAR-100: Bảng 1 so sánh độ chính xác top-1 của các phương pháp distillation khác nhau và đánh giá các lựa chọn mạng khác nhau cho giáo viên-học sinh. Hai hàng đầu tiên của Bảng 1 đại diện cho nhiều tổ hợp giáo viên-học sinh khác nhau bằng cách sử dụng các mạng như sau: Wide residual networks (Wd-w) [37] trong đó d và w đại diện cho độ sâu và độ rộng trong mạng tương ứng, MobileNetV2 (MN2) [22], ShuffleNetV1 (SN1) [39]/ShuffleNetV2 (SN2)[14], VGG (VG) [24], và ResNet (RN) [5].

Tất cả các mô hình được huấn luyện trong 240 epochs với tốc độ học 0.05 được giảm 0.1 mỗi 30 epochs sau 150 epochs. Các tham số cân bằng γkd và αkd là 0.1 và 0.9 cho tất cả các cài đặt tương ứng. Trong Bảng 1, chúng tôi báo cáo kết quả của bốn cài đặt khác nhau như sau, các mô hình được chưng cất không có mixup (No Mixup), các mô hình được chưng cất với PMU (10% và 50% khi α= 1), và các mô hình được chưng cất với mixup đầy đủ (FMU, α= 1). Như được thấy trong Bảng 1, các học sinh được huấn luyện với PMU luôn vượt trội hơn những học sinh được huấn luyện không có mixup. Đáng ngạc nhiên, trong một số trường hợp (ví dụ, T:W40-2 & S:W16-2 và T:W40-2 & S:SN1), học sinh của chúng tôi được huấn luyện với PMU hoạt động tốt hơn cả giáo viên.

Thí nghiệm trên ImageNet: Bảng 2 hiển thị độ chính xác top-1 trên ImageNet [3]. Trong thí nghiệm này, chúng tôi chọn ResNet34 và ResNet18 [5] làm mạng giáo viên và mạng học sinh tương ứng. Chúng tôi huấn luyện mô hình trong 100 epochs và với tốc độ học ban đầu là 0.1 được giảm 0.1 tại 30, 60, và 80 epochs. Kích thước batch được đặt là 256. Để so sánh với các phương pháp distillation khác, các siêu tham số của các phương pháp khác tuân theo các bài báo tương ứng của chúng. Các tham số cân bằng γkd và αkd là 0.1 và 0.9 tương ứng, và chúng tôi báo cáo partial mixup 10% và 100% (FMU). Chúng tôi quan sát thấy rằng phương pháp được đề xuất với mixup đầy đủ tăng độ chính xác top-1 và top-5 lên 2.07% và 1.56% so với baseline và mixup đầy đủ cho thấy hiệu suất tốt hơn 10% partial mixup. Chúng tôi sẽ thảo luận về các trường hợp hiệu suất thấp hơn trong phần tiếp theo.

Phân tích hiệu suất: Dựa trên Bảng 1, trong một số tổ hợp giáo viên-học sinh, chúng tôi quan sát thấy rằng việc thêm nhiều cặp mixup hơn giúp học sinh đạt được độ chính xác cao hơn so với lượng nhỏ các cặp. Hơn nữa, đối với các kiểu kiến trúc khác nhau, việc sử dụng mixup đầy đủ vượt trội hơn những kiểu khác trong một số trường hợp. Giả định từ quan sát này là các mạng từ các kiến trúc khác nhau cố gắng tìm kiếm các con đường giải pháp của chúng, có nghĩa là giáo viên và học sinh có các phân phối không giống nhau trong logit, và do đó việc áp đặt thêm độ mịn được tạo ra bởi augmentation mạnh có thể cung cấp cho học sinh thông tin bổ sung về cách giáo viên kiểu khác nhau đại diện cho kiến thức. Ngoài ra, trong trường hợp ImageNet có số lượng lớn các lớp, kiến thức được tạo ra học được bởi hai hình ảnh trộn lẫn và nhãn của chúng có thể tạo ra kiến thức tương đối ít thông tin hơn so với CIFAR-100, vì vậy độ mịn mạnh bởi mixup đầy đủ có lợi cho việc chưng cất mô hình học sinh tốt hơn trong trường hợp này. Trong khi chúng tôi trình bày một cách heuristic rằng hiệu suất được kiểm soát bởi lượng độ mịn, lượng độ mịn chính xác nên được áp đặt trên các bộ dữ liệu hoặc mạng vẫn là một câu hỏi mở có thể tạo thành cơ sở cho công việc tương lai.

5.2. Tính mạnh mẽ đối với các ví dụ đối kháng
Một hậu quả không mong muốn của các mô hình được huấn luyện là tính dễ vỡ của chúng đối với các ví dụ đối kháng [4]. Các ví dụ đối kháng được tạo ra bằng cách thêm các nhiễu nhỏ (không thể nhận thấy bằng mắt) vào các mẫu hợp pháp để làm suy giảm hiệu suất của mô hình. Thật không may, nhiều phương pháp distillation đã phát triển để cải thiện hiệu suất của KD, bỏ qua các cuộc tấn công. Do đó, trong phần này, chúng tôi đánh giá mô hình của chúng tôi chống lại các cuộc tấn công white-box trong đó chúng tôi đã sử dụng các mô hình được huấn luyện để tạo ra các ví dụ đối kháng sử dụng hai cuộc tấn công phổ biến, phương pháp Fast Gradient Sign Method (FGSM) và Iterative FGSM (I-FGSM) [4]. Đối với I-FGSM, chúng tôi sử dụng 10 lần lặp với kích thước bước bằng nhau. Kết quả cho cả hai cuộc tấn công được tóm tắt trong Bảng 3. Đối với các phương pháp distillation, cài đặt của giáo viên và học sinh là T:RN110 & S:RN20, trong đó chúng được huấn luyện trên CIFAR100. Chúng tôi áp dụng mixup augmentation cho tất cả các phương pháp mà chúng tôi khám phá (xem phương pháp+ mixup trong bảng). Chúng tôi huấn luyện mô hình của chúng tôi sử dụng partial mixup chỉ 10% với α= 1. Ngay cả khi chỉ sử dụng 10% cặp mixup, nó cho thấy khả năng chống chịu ấn tượng đối với cả hai cuộc tấn công. Chúng tôi nhận thấy rằng các phương pháp distillation với mixup, sử dụng các bản đồ đặc trưng như ICKD và CRD hoạt động kém hơn những phương pháp không có mixup. Chúng tôi sẽ hiển thị nhiều kết quả so sánh hơn của các lượng cặp mixup khác nhau trong phần tiếp theo.

5.3. Nghiên cứu Ablation
Trong phần này, chúng tôi tiến hành nghiên cứu ablation cho các siêu tham số với tổ hợp T:WN40-2 & S:WN16-2 networks. Như được thể hiện trong Bảng 4, các mô hình được chưng cất với PMU và giá trị α cao nói chung mang lại hiệu suất tốt hơn. Hơn nữa, để điều tra mức độ partial mixup và α phản ứng chống lại cả hai cuộc tấn công như thế nào, chúng tôi cũng hiển thị độ chính xác kiểm tra trong Hình 7. Như được thể hiện trong cột thứ nhất và thứ hai của biểu đồ, khi augmentation mạnh được tham gia vào huấn luyện như mixup đầy đủ (FMU với giá trị α cao) hoặc số lượng cao các cặp mixup (PMU 80%), nó cải thiện tính mạnh mẽ. Thú vị là, chỉ 10% partial mixup (thanh xám trong cột giữa) bảo vệ tốt cả hai cuộc tấn công. Đối với cột cuối cùng của hình, chúng tôi đã chọn hai mô hình được chưng cất từ FMU (α= 0.2) và PMU (50%, α= 0.7) trong đó chúng có hiệu suất kiểm tra tương tự trong Bảng 4. Chúng tôi quan sát thấy rằng PMU (50%, α= 0.7) cho thấy tính mạnh mẽ hơi cao hơn so với FMU (α= 0.2).

6. Kết luận
Trong công trình này, chúng tôi nghiên cứu vai trò của mixup trong knowledge distillation. Chúng tôi quan sát thấy rằng mạng giáo viên được huấn luyện mixup tạo ra sự giám sát kém do độ mịn quá mức được áp đặt lên các đặc trưng và logit, đặc biệt là ở nhiệt độ cao trong quá trình distillation. Do đó, các học sinh trải qua sự giảm hiệu suất trong KD. Chúng tôi hỗ trợ các phát hiện của mình thông qua một loạt các phân tích thực nghiệm và thí nghiệm quy mô lớn về nhiệm vụ phân loại hình ảnh. Các phát hiện của chúng tôi cung cấp cái nhìn sâu sắc về hoạt động bên trong của mô hình được chưng cất được huấn luyện với mixup augmentation. Những cái nhìn sâu sắc này cho phép chúng tôi phát triển một chiến lược học tập cải tiến sử dụng logit được tái tỉ lệ và partial mixup.

Như chúng tôi đã đề cập trước đó, các augmentation dựa trên trộn lẫn khác nhau đã chứng minh hiệu quả của chúng cho các nhiệm vụ cụ thể. Tuy nhiên, những augmentation này có thể có xu hướng tạo ra các mẫu huấn luyện không hợp lý khi nó pha trộn các hình ảnh ngẫu nhiên [11], điều này có thể làm biến dạng cấu trúc tương đối hợp lý giữa các danh mục. Kết quả là, điều này có thể tạo ra hiệu ứng mịn không thuận lợi trên logit trong quá trình distillation. Do đó, việc phát triển một phương pháp augmentation tự động chọn các mẫu hợp lý hơn đảm bảo độ mịn phù hợp nhất sẽ thúc đẩy thêm tiến bộ trong lĩnh vực distillation. Chúng tôi sẽ phát triển thêm kỹ thuật này trong công việc tương lai của mình.

7. Lời cảm ơn
Tài liệu này dựa trên công việc được hỗ trợ bởi Cơ quan Dự án Nghiên cứu Tiên tiến Quốc phòng (DARPA) theo Thỏa thuận số HR00112290073. Được phê duyệt để phát hành công khai; phân phối không giới hạn.

--- TRANG 9 ---
Tài liệu tham khảo
[1] Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D Lawrence, và Zhenwen Dai. Variational information distillation for knowledge transfer. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 9163–9171, 2019.

[2] Chung-Cheng Chiu, Tara N Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J Weiss, Kanishka Rao, Ekaterina Gonina, và các tác giả khác. State-of-the-art speech recognition with sequence-to-sequence models. Trong Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, trang 4774–4778, 2018.

[3] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, và Li Fei-Fei. Imagenet: A large-scale hierarchical image database. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, trang 248–255, 2009.

[4] Ian J Goodfellow, Jonathon Shlens, và Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.

[5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image recognition. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, trang 770–778, 2016.

[6] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, và Mu Li. Bag of tricks for image classification with convolutional neural networks. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 558–567, 2019.

[7] Byeongho Heo, Minsik Lee, Sangdoo Yun, và Jin Young Choi. Knowledge transfer via distillation of activation boundaries formed by hidden neurons. Trong Proceedings of the AAAI Conference on Artificial Intelligence, tập 33 (01), trang 3779–3787, 2019.

[8] Geoffrey Hinton, Oriol Vinyals, và Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.

[9] Zehao Huang và Naiyan Wang. Like what you like: Knowledge distill via neuron selectivity transfer. arXiv preprint arXiv:1707.01219, 2017.

[10] Jangho Kim, SeongUk Park, và Nojun Kwak. Paraphrasing complex network: Network compression via factor transfer. arXiv preprint arXiv:1802.04977, 2018.

[11] Jang-Hyun Kim, Wonho Choo, Hosan Jeong, và Hyun Oh Song. Co-mixup: Saliency guided joint mixup with supermodular diversity. arXiv preprint arXiv:2102.03065, 2021.

[12] Xingjian Li, Haoyi Xiong, Chengzhong Xu, và Dejing Dou. Smile: Self-distilled mixup for efficient transfer learning. arXiv preprint arXiv:2103.13941, 2021.

[13] Li Liu, Qingle Huang, Sihao Lin, Hongwei Xie, Bing Wang, Xiaojun Chang, và Xiaodan Liang. Exploring inter-channel correlation for diversity-preserved knowledge distillation. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, trang 8271–8280, 2021.

[14] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, và Jian Sun. Shufflenet v2: Practical guidelines for efficient cnn architecture design. Trong Proceedings of the European Conference on Computer Vision, trang 116–131, 2018.

[15] Rafael Müller, Simon Kornblith, và Geoffrey Hinton. When does label smoothing help? arXiv preprint arXiv:1906.02629, 2019.

[16] Wonpyo Park, Dongju Kim, Yan Lu, và Minsu Cho. Relational knowledge distillation. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 3967–3976, 2019.

[17] Nikolaos Passalis và Anastasios Tefas. Learning deep representations with probabilistic knowledge transfer. Trong Proceedings of the European Conference on Computer Vision, trang 268–284, 2018.

[18] Baoyun Peng, Xiao Jin, Jiaheng Liu, Dongsheng Li, Yichao Wu, Yu Liu, Shunfeng Zhou, và Zhaoning Zhang. Correlation congruence for knowledge distillation. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, trang 5007–5016, 2019.

[19] Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz Kaiser, và Geoffrey Hinton. Regularizing neural networks by penalizing confident output distributions. arXiv preprint arXiv:1701.06548, 2017.

[20] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, và Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.

[21] Andrew Rosenberg và Julia Hirschberg. V-measure: A conditional entropy-based external cluster evaluation measure. Trong Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, trang 410–420, 2007.

[22] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, và Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 4510–4520, 2018.

[23] Zhiqiang Shen, Zechun Liu, Dejia Xu, Zitian Chen, Kwang-Ting Cheng, và Marios Savvides. Is label smoothing truly incompatible with knowledge distillation: An empirical study. arXiv preprint arXiv:2104.00676, 2021.

[24] Karen Simonyan và Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.

[25] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, và Zbigniew Wojna. Rethinking the inception architecture for computer vision. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 2818–2826, 2016.

[26] Xu Tan, Yi Ren, Di He, Tao Qin, Zhou Zhao, và Tie-Yan Liu. Multilingual neural machine translation with knowledge distillation. arXiv preprint arXiv:1902.10461, 2019.

[27] Yonglong Tian, Dilip Krishnan, và Phillip Isola. Contrastive representation distillation. arXiv preprint arXiv:1910.10699, 2019.

[28] Frederick Tung và Greg Mori. Similarity-preserving knowledge distillation. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, trang 1365–1374, 2019.

[29] Laurens Van der Maaten và Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.

[30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. Trong Advances in Neural Information Processing Systems, trang 5998–6008, 2017.

[31] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-Paz, và Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. Trong Proceedings of the International Conference on Machine Learning, trang 6438–6447, 2019.

[32] Dongdong Wang, Yandong Li, Liqiang Wang, và Boqing Gong. Neural networks are more productive teachers than human raters: Active mixup for data-efficient knowledge distillation from a blackbox model. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 1498–1507, 2020.

[33] Huan Wang, Suhas Lohit, Michael Jones, và Yun Fu. Knowledge distillation thrives on data augmentation. arXiv preprint arXiv:2012.02909, 2020.

[34] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, và Quoc V Le. Self-training with noisy student improves imagenet classification. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 10687–10698, 2020.

[35] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, và Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, trang 6023–6032, 2019.

[36] Sergey Zagoruyko và Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. arXiv preprint arXiv:1612.03928, 2016.

[37] Sergey Zagoruyko và Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.

[38] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, và David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.

[39] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, và Jian Sun. Shufflenet: An extremely efficient convolutional neural network for mobile devices. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, trang 6848–6856, 2018.
