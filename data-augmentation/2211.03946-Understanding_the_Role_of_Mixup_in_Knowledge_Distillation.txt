# 2211.03946.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/data-augmentation/2211.03946.pdf
# File size: 3162522 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Understanding the Role of Mixup in Knowledge Distillation:
An Empirical Study
Hongjun Choi, Eun Som Jeon, Ankita Shukla, Pavan Turaga
Geometric Media Lab
School of Arts, Media and Engineering, Arizona State University
School of Electrical, Computer and Energy Engineering, Arizona State University
hchoi71@asu.edu, ejeon6@asu.edu, ashukl20@asu.edu, pturaga@asu.edu
Abstract
Mixup is a popular data augmentation technique based
on creating new samples by linear interpolation between
two given data samples, to improve both the generaliza-
tion and robustness of the trained model. Knowledge dis-
tillation (KD), on the other hand, is widely used for model
compression and transfer learning, which involves using a
larger network’s implicit knowledge to guide the learning
of a smaller network. At first glance, these two techniques
seem very different, however, we found that “smoothness”
is the connecting link between the two and is also a crucial
attribute in understanding KD’s interplay with mixup. Al-
though many mixup variants and distillation methods have
been proposed, much remains to be understood regarding
the role of a mixup in knowledge distillation. In this pa-
per, we present a detailed empirical study on various impor-
tant dimensions of compatibility between mixup and knowl-
edge distillation. We also scrutinize the behavior of the
networks trained with a mixup in the light of knowledge
distillation through extensive analysis, visualizations, and
comprehensive experiments on image classification. Fi-
nally, based on our findings, we suggest improved strate-
gies to guide the student network to enhance its effective-
ness. Additionally, the findings of this study provide in-
sightful suggestions to researchers and practitioners that
commonly use techniques from KD. Our code is available
athttps://github.com/hchoi71/MIX-KD .
1. Introduction
Deep neural networks have achieved impressive perfor-
mance on a wide range of tasks including language trans-
lation [30, 26], image classification [34, 6], and speech
recognition [2, 19]. To further improve the model’s effi-
ciency and performance, a large number of training tech-niques have been proposed such as mixup-augmentation
[38, 35] and knowledge distillation [8]. In specific, mixup
[38] is a commonly used data augmentation technique based
on using convex combinations of samples, and their labels.
This technique was introduced to improve generalization,
as well as increase the robustness against adversarial ex-
amples. Recently, there has been an increasing interest in
reducing the model size while preserving comparable per-
formance, which narrows the gap between large networks
and small networks. KD is one of the promising methods
for this demand [8]. The goal of KD is to exploit the ability
to learn concise knowledge representation (logit or feature)
from a larger model and then embed such knowledge into a
smaller model. For example, in image classification, deep
neural networks produce class probability by using softmax
function that converts logit fiinto a probability piby com-
paring fiwith other logits as follows: pi=exp(fi/T)P
jexp(fj/T).
In the conventional knowledge distillation [8], the tempera-
tureTis utilized to generate a softer distribution of pseudo-
probabilities among the output classes, where a higher tem-
perature increases the entropy of the output, thus, providing
more information to learn for the student model.
Motivation: On the surface, mixup and KD are very
different, however, we found that “ smoothness ” turns out to
be the connecting link between the two and a very impor-
tant attribute to understanding KD’s interplay with mixup.
Intuitively, the KD involves a student mimicking smoothed
probability distribution of the teacher, whereas mixup ar-
tificially introduces smoothness (the labels are not strictly
one-hot) in a model via linear combinations of the labels.
Although mixup-augmentation and KD are common tech-
niques in training networks in various applications, the in-
terplay between the two has not been well explored. In
this paper, we investigate in detail the impact of mixup-
augmentation on knowledge distillation.
To develop insight into several interesting behaviors ofarXiv:2211.03946v2  [cs.CV]  9 Nov 2022

--- PAGE 2 ---
Baby
Boy
Girl
Man
WomanS:RN56
V-Score:0.8256
KD,  T=1
T:RN110 & S:RN56  
V-Score:0.7884Mix-S:RN56
V-Score:0.5908S:RN110
V-Score:0.9592
KD,  T=8
T:RN110 & S:RN56  
V-Score:0.6978KD,  T=20
T:RN110 & S:RN56  
V-Score:0.6579
Mix-S:RN110
V-Score:0.7244
Figure 1. Feature representation of CIFAR100 extracted from the penultimate layer. Here, we only illustrate the train set as the student and
teacher networks are trained on the same train set. In the first row, we observe that the higher capacity model (ResNet110 shown as RN110)
promotes a tighter clustering in each class (higher V-score) than ResNet56 (RN56). Meanwhile, mixup-trained models (Mix-S:RN110 and
Mix-S:RN56) disperse features of the same instances. The second row of the figure shows that increasing temperature Tin KD has a
similar effect on the projections, resulting in dispersed feature representations. Here, ⋆indicates the mean point of each cluster. The table
shows the corresponding test accuracy. Best viewed in color.
the network, we provide various visualizations in feature
and logit levels. For example, to analyze how mixup en-
forces a feature representation in-between each class, we
pick 5 semantically similar classes from the CIFAR100
dataset (baby, boy, girl, man, and woman). Then, we project
the features extracted from the penultimate layer into 2-
D using tsne [29] as seen in Figure 1. The first row of
the figure represents the feature representation for two net-
works trained from scratch with mixup (Mix-S:ResNet110
or Mix-S:ResNet56) and without mixup (S:ResNet110 or
S:ResNet56). Generally, we observe that the higher capac-
ity model (S:ResNet110) encourages the deep networks to
learn tight projections in each class while the lower capacity
model (S:ResNet56) learns more diffuse projections. This is
also verified by clustering metrics, such as the V-score [21],
where a higher value implies better clustering. Interest-
ingly, the mixup-trained models disperse features in similar
classes, compared to the one trained without mixup, even
though a mixup-trained model shows better test accuracy
due to an increase in generalization on unseen data. Mean-
while, we found similar observations on the feature repre-
sentation from distilled models when a higher temperature
is used (i.e., distilling more softer logit) as seen in the sec-
ond row of Figure 1. From the table in Figure 1, high tem-
perature Tis a default choice to improve the performance.
In this way, the teacher transfers more information to the
student even though higher temperatures superficially pro-
mote feature dispersion in similar classes. However, trans-
ferring a high quality of supervision to a student network
is also crucial as it can guide the student to learn discrim-
inative representations from the superior teacher network.
Thus, we seek a way to increase supervisory signals fromteacher to student, without compromising on performance.
This is where mixup presents a way forward.
In short, the crux of our insight is that if a teacher
is trained with smoothed data from mixup, then further
‘smoothing’ at high temperatures during distillation can be
avoided. This ensures stronger supervisory signals while
enjoying the benefits of dataset augmentation via mixup.
We summarize our contributions as follows:
1. We provide new insights into devising improved strate-
gies for learning a student model, by a deeper un-
derstanding of the behavior of features/logits and net-
works trained with mixup during KD.
2. To reduce the criticality of choosing a proper ‘tem-
perature’, we develop a simple rescaling scheme that
brings different statistical characteristics in logits be-
tween teacher and student to a similar range while
preserving the relative information between classes,
thereby the temperature Tis no longer used.
3. We identified that strongly interpolated mixup pairs
impose extra smoothness on the logits, thus we can
generate only a few mixup pairs in a batch, called par-
tial mixup (PMU), and yet achieve comparable and
even better performance with this variant in KD.
2. Background
We first introduce the background of mixup and KD
through a simple mathematical description. Given the train-
ing data D={(x1, y1), ...,(xn, yn)}, the goal of the clas-
sification task is to learn classifier f:X →Rkby mapping

--- PAGE 3 ---
input x∈ X ⊆ Rdto label y∈ Y ={1,2, ..., K}. Let
L(f(x), y)be the loss function that measures how poorly
the classifier f(x)predicts the label y.
Mixup Augmentation [38] In mixup augmentation, two
samples are mixed together by linear interpolation as fol-
lows: ˜xij(λ) =λxi+ (1−λ)xj, and ˜yij(λ) =λyi+ (1−
λ)yj, where λ∈[0,1]follows the distribution Pλwhere
λ∼Beta(α,α). Then, the mixup loss function can be de-
scribed as
Lmix(f) =1
n2nX
i=1nX
j=1Eλ∼Pλ[L(f(˜xij(λ)),˜yij(λ))],(1)
where Lrepresents the cross-entropy loss function in this
study. Specifically, a hyper-parameter λin Equation 1 is
used to specify the extent of mixing. In other words, the
control parameter αin a beta distribution commands the
strength of interpolation between feature-target pairs, i.e.,
the high αgenerating strongly interpolated samples.
A fair amount of variants of mixup have been proposed
[31, 35, 11]. The general strategy of these mixing-based
methods is intrinsically similar in that they extend the train-
ing distribution by blending the images and mixing their la-
bels proportionally. Thus, in this study, we only pay atten-
tion to the conventional mixup to investigate the interplay
between mixup and knowledge distillation [38].
Knowledge Distillation [8] In knowledge distillation,
given a pre-trained teacher model fTon the dataset in ad-
vance, the student model fSis trained over the same set of
data by utilizing concise knowledge generated by fT. In
specific, once the teacher network is trained, its parameter
is frozen during the training in KD, and then, the student
network is trained by minimizing the similarity between its
output and the soft labels generated by the teacher network.
To this end, we minimize the divergence between logits of
the student and the teacher as follows:
Lkd(fT, fS) =1
nnX
i=1KL(S(fT(xi)
T),S(fS(xi)
T))(2)
where Sindicates the softmax function, KL measures the
Kullback-Leiber divergence, and output logits of the model
smoothen by temperature, T.
Data Augmentation in Knowledge Distillation Recently,
several works have utilized data augmentation and achieved
promising results in that augmented samples enable the net-
works to learn relaxed knowledge from different views in
distillation frameworks [33, 32, 12]. Across the broadly
available methods, they often do not provide insight into
the inner working of the models. Meanwhile, recent works
[15, 23] have studied KD’s compatible/incompatible views
with label smoothing [25] to provide such insight into deep
models via empirical analysis. Instead, we are interested
in the underlying mechanism of augmentation in the light
Figure 2. The student (fixed RN56) test accuracy on CIFAR100
under different teacher networks with T= 4 . Here, T and
S denote the standard teacher and the standard student. Mix-T
and Mix-S are mixup-trained teachers and mixup-trained students,
respectively. Mixup-trained teachers guide lower-performance
students than the one with standard teacher models, and lower-
capacity teacher models generally distill inferior students.
of the KD process. Our study spans double aspects that at-
tempt to present both compatible and incompatible views
through comprehensive empirical analysis. Further, based
on the observations, we suggest a better learning strategy to
enhance the network’s performance.
3. Key findings from Mixup and KD interplay
In this section, we discuss our main findings. We first
begin by referring to Figure 2. Here, we describe four possi-
ble scenarios where mixup-augmentation could be involved
in KD as follows; standard teacher and standard student
(T&S), mixup-trained teacher and standard student (Mix-
T&S), mixup-trained teacher and mixup-trained student
(Mix-T&Mix-S), and standard teacher and mixup-trained
student (T&Mix-S) under the same temperature being T=
4. We fix the student model as ResNet56 (RN56) and evalu-
ate it by varying the teacher models from ResNet20 (RN20)
to ResNet110 (RN110).
As seen in Figure 2, we can make two observations:
First, the student performance with the help of a mixup-
trained teacher (Mix-T&S, Mix-T&Mix-S) always shows
less effectiveness than with the help of a standard teacher
(T&S, T&Mix-S) under the same settings of temperature
even if the mixup-trained teacher itself shows better test
accuracy than the teacher trained without mixup. Sec-
ond, generally, higher capacity teacher models distill bet-
ter students, but with a lower capacity teacher (T:RN44 &
S:RN56), the student performance still shows improvement
in accuracy, compared to the vanilla student model (green
dashed line, S (No KD)). Based on these observations, this
paper aims to investigate the following questions: 1) Why
does a mixup-trained teacher model impair the student’s
effectiveness in KD? We answer this question in section
3. Then, 2) what can we do to improve the effectiveness
of knowledge distillation when mixup-augmentation is ap-
plied? We address this question in section 4.

--- PAGE 4 ---
T: ResNet110 T: ResNet20 No KDTrain Set
S: ResNet56
Mix-S: ResNet56No KD Mix-T: ResNet110 Mix-T: ResNet20Different Class
 
Simillar Class
Beaver
Apple
Aquarium 
Fish
Rocket
TurtleBaby
Boy
Girl
Man
Woman
Feature Representation V-Score
 Simillar Class
 Different ClassT & S
Mix-T & S
Mix-T & Mix-S
T & Mix-SS (No KD)
Mix-S (No KD)
Figure 3. Feature representations of the penultimate layer with various combinations (T:standard teacher, S:standard student, Mix-T:mixup-
trained teacher, Mix-S:mixup-trained teacher) and the corresponding V-scores on the CIFAR100 dataset. We selected two groups, seman-
tically similar classes (Baby, Boy, Girl, Man, and Woman), and semantically different classes (Beaver, Apple, Aquarium Fish, Rocket,
and Turtle). Here, the features are extracted from the student networks. Observation 1) Mixup augmentation encourages the features of
samples from similar classes to be dispersed, while it still well-preserves the feature separability in different classes. Observation 2) The
use of a lower-accuracy teacher network enables the student to learn by less-discriminative features, degrading the performance of KD.
Observation 3) In presence of a mixup-trained teacher, the dispersion of feature representation undermines the benefit of supervision by
the teacher. ⋆represents the center point of each cluster. Best viewed in color.
Observation 1) Mixup vs Non-Mixup. To investigate the
effect of the network trained with mixup-augmentation, we
selected a few classes and divided them into two groups:
(1) semantically different classes (Beaver, Apple, Aquar-
ium Fish, Rocket, and Turtle) and (2) semantically similar
classes (Baby, Boy, Girl, Man, and Woman), all from CI-
FAR100. For example, in Figure 3, the left figure illustrates
the feature representation of the penultimate layer on train
sets. If we look at the number 1 in the red circle, the pro-
jections of a mixup-trained model from similar classes are
more dispersed while the ones of different classes are still
well-preserved in their structure. This information loss can
also be measured by clustering metric, V-score on the right
histogram of the figure, resulting in a drastic drop in V-score
for similar classes.
Observation 2) Distilling from low-accuracy teacher
models. If the student network is settled, retaining the fine
quality of supervision (i.e., the performance of a teacher
network) is crucial in training better students. As followed
in the number 2 in the red circle, the projections of the stu-
dent with the lower-accuracy teacher are notably dispersed,
significant drop in V-score, eventually impairing the stu-
dent’s performance. This implies that the better student is
distilled with the help of discernible features given by the
high-capacity teacher.
Observation 3) Distilling from a mixup-trained teacher
network. Observation 1) showed that the mixup-trained
model scatters the features in similar classes. We now
look at the case where a mixup-trained teacher conveys the
knowledge to the student. In KD, as the student and teacher
models are trained on the same train set, we argue that a
student trained with supervision by a mixup-trained teachercannot take advantage of learning superior knowledge due
to feature scattering. As shown by number 3 in this figure,
feature representations of similar classes in the student net-
work eventually become more spread out, and V-scores also
drop on both train and test sets. We provide the visualiza-
tions of train and test sets in the supplementary material.
Observation 4) and 5) Logit representation. Unlike the
feature representations shown in observations 1)-3), we fur-
ther visualize the probability distribution of the student net-
work. First, we average probabilities of all classes on the
train and test of CIFAR100 and illustrate the mean distri-
bution of the examples that belong to the same category to
show the model’s prediction of that category. To compare
with quantitative measurement, we also provide the average
accuracy and entropy values computed across all examples
in Figure 4, where the entropy is popularly used to mea-
sure the smoothness of the distribution. The higher entropy
value is the smoother distribution.
Here, we observe two intriguing phenomena; Obser-
vation 4) A mixup-trained model produces softer output
logits, illustrated by the short red bars in both train and
test set. From this observation, we surmise that a mixup-
augmentation involved in training the student in KD con-
tributes extra smoothness to the logits. Further, the student
learned from a standard teacher outperforms the mixup-
trained teacher in terms of accuracy (74.98% vs 73.92% and
71.78% vs 70.60%) under the same settings of the temper-
atureT= 4. Conclusively, when distilling from a mixup-
trained teacher, the use of high temperature adversely im-
pacts the accuracy of the student. Observation 5) When the
inferior knowledge produced by the low-accuracy teacher
(T:RN20) transfers to the student, the confidence level on

--- PAGE 5 ---
0.30.40.50.60.70.80.91.0Probability
0.30.40.50.60.70.80.91.0Probability0.30.40.50.60.70.80.91.0Probability
0.30.40.50.60.70.80.91.0Probability
S:RN56 & T:RN110
S:RN56 S:RN56 & T:RN20
Mix-S:RN56 & MixT:RN110
Mix-S:RN56 Mix-S:RN56 & Mix-T:RN20
Baby Boy Girl Man Woman Baby Boy Girl Man WomanTrain Set Test Set
Figure 4. Probability distributions of the following configurations: with/without a mixup-trained model (S:RN56/Mix-S:RN56), the stan-
dard student model trained with the help of different capacity teachers (T:RN110 & S:RN56, T:RN20 & S:RN56), and the student model
trained with mixup in the presence of different mixup-trained teachers (Mix-T:RN110 & Mix-S:RN56, Mix-T:RN20 & Mix-S:RN56). We
show the mean distributions for five similar classes in CIFAR100 on the left and provide the average accuracy and entropy values for all
examples. Observation 4) When the model is trained with the mixup, the confidence in predictions decreases on both sets (also verified by
higher entropy value). Observation 5) Also, in the case of distilling from lower-capacity teachers, the confidence in predictions is much
lower than the distilled model from high-capacity teachers. Best viewed in color.
the prediction significantly falls as seen in the gray bars on
both sets, leading to a remarkable decrease in test accuracy
on both cases (74.98% →71.78%, 73.92% →70.60%). It
implies that transferring decent quality of supervision to the
student is crucial to successful knowledge distillation.
Observation 6) Mix-T&S at low temperature. From ob-
servation 5), while increased Tis believed to be helpful to
produce better representation for KD, we remark that in the
presence of a mixup-trained teacher, an increased Tcan be
an adverse effect on the performance of KD because of the
feature dispersion/exorbitant smoothness in the logit. At
this point, one might give rise to the following question.
What if we lower the temperature to lessen smoothness in
the logit?
Teacher NetworksStudent Networks
Test Accuracy (%)Mix-T & S,  T=4 Mix-T & S,  T=1
Teacher Networks Student Networks
Test Accuracy (%)
Figure 5. Visualization with the heatmap representation. When
distilling from a mixup-trained teacher (Mix-T) at a higher tem-
perature, it transfers inferior knowledge to the student. With lower
temperature T= 1, it produces better students, covering the rela-
tive high-accuracy region in the heatmap.
To investigate how much temperature impacts the test
performance in KD, we visualize the test accuracy as a
heatmap in Figure 5. In this figure, we explore all combina-
tions of a mixup-trained teacher and student. The left panel
of this figure shows test accuracy under the setting T= 4
and the right panel is for T= 1. We note that when a lowertemperature, that makes the logit less smooth, is applied,
the better the quality of knowledge that is transferred to the
student. This result demonstrates that knowledge from a
mixup-trained teacher at high temperatures renders the stu-
dent model less effective.
4. Effective learning strategies for Mix-KD
Based on our findings, we observe that smoothness in
the logit is a critical attribute in distillation, as seen in Fig-
ure 5. Since it is difficult to measure suitable smoothness
from the perspective of the student, many distillation meth-
ods have relied heavily on a naive brute-force search to find
the proper temperature. To reduce the criticality of choosing
the proper temperature and alleviating excessive smooth-
ness by a strong mixup, we introduce an effective learning
strategy to improve the performance of KD.
Partial mixup. The control parameter in the beta distri-
bution plays a key role in controlling the strength of in-
terpretation, which also affects the degree of softened out-
put. As α→1, it provides softer output logits. However,
there is a trade-off between avoiding excessive smoothness
and improvement in robustness against adversarial attacks
in knowledge distillation. To alleviate this issue, we suggest
generating only small amounts of mixup pairs used in train-
ing, called partial mixup (PMU). For example, PMU=10%
refers to only 10% mixup pairs used in a batch and the rests
keep untouched. To further understand the behavior of how
partial mixup affects output probability, we provide toy ex-
amples using 2 classes in the supplementary material.
Rescaled logits. Here, we suggest using standard deviation
as temperature such that the logit produced by the output
layer of the network is rescaled by dividing it by the stan-
dard deviation of that logit, thereby the temperature Tis

--- PAGE 6 ---
no longer hyper-parameter. This way can bring two dif-
ferent statistical characteristics between the teacher and the
student logits to similar ranges while it does not hurt the
relative structure in-between classes. We underline the im-
portance of rescaling logit since the random mixing portion
λfrom a beta distribution governs the smoothness degree,
yielding irregular smooth output at each iteration. There-
fore, we replace the output logit with rescaled one, and the
loss function becomes as follows:
Lkdr(˜fT,˜fS) =1
nnX
i=1KL(S(fS(xi)
σ(fS(xi))),S(fT(xi)
σ(fT(xi)))),(3)
where ˜f=f/σ(f),Sindicates the softmax function, KL
is the Kullback-Leibler divergence, and σ(·)is standard de-
viation of input logit. Then, the final training objective for
the student in KD is as follows:
minE(x,y)∼DEλ∼Pλ[γkdLmix(˜fS) +αkdLkdr(˜fT,˜fS)],(4)
where γkdandαkdare balancing parameters. Note, the
hyper-parameter of the partial amount in PMU is not de-
noted in this equation.
0.2
0.4
0.7
1
FMU
PMU=10%
PMU=30%
PMU=50%
PMU=80%
70.9-71.1
71.1-71.3
71.3-71.5
71.5-71.7
71.7-71.9
71.9-72α
Figure 6. Test accuracy on CIFAR100 under different configura-
tions of αand the amounts of mixup pairs. The reported accuracy
is averaged over 3 runs.
Choice of αand PMU. Further, we investigate how each
hyper-parameter (partial amount in PMU and the control
parameter α) affects the student performance in KD un-
der the following combination, T:RN110 and Mix-S:RN20
on the CIFAR100 dataset. We evaluate the student perfor-
mance by varying the degree of partial amount from 10%
to 80% with different control parameters in the range of
[0.2,0.4,0.7,1.0]. Figure 6 indicates that PMU with high α
generally leads to better students in this setting. The lowest
performance is observed in FMU (fully mixup pairs used)
withα= 1.0. We note that PMU does not necessarily aim
to outperform FMU, but it plays a regulatory role to ad-
just smoothness finely, thus helping us to understand the
effect of smoothness in KD. We present performance anal-
ysis in section 5.1 and also further study how partial mixup
responds to adversarial attacks in section 5.3.5. Experiments
In this section, we present the experimental results to val-
idate our conjectures and findings. In the previous section,
we noted that (1) a mixup-trained teacher generates an infe-
rior student compared to the teacher trained without mixup
under the same temperature (2) but lowering temperature
can recover the test accuracy. In KD, mixup augmenta-
tion is only useful for the teacher if a mixup-trained teacher
provides the student with further benefits such as better
knowledge and additional power of robustness. Through
our analysis, we see that the expected effect of the use of
mixup-trained teachers is not satisfactory. Therefore, we
use the teacher trained without mixup for our experiment,
i.e., T&Mix-S.
5.1. Image classification on CIFAR100 & ImageNet
Experiments on CIFAR-100: Table 1 compares top-1 ac-
curacy of various distillation methods and evaluates various
network choices for teacher-student. The first two rows of
the Table 1 represent many different teacher-student combi-
nations by utilizing the networks as follows: Wide residual
networks (Wd-w) [37] where d and w represent depth and
width in the networks respectively, MobileNetV2 (MN2)
[22], ShuffleNetV1 (SN1) [39]/ShuffleNetV2 (SN2)[14],
VGG (VG) [24], and ResNet (RN) [5].
All the models are trained for 240 epochs with a learn-
ing rate of 0.05 decayed by 0.1 every 30 epochs after 150
epochs. The balancing parameters γkdandαkdare 0.1 and
0.9 for all settings, respectively. In Table 1, we report our
results of four different settings as follows, distilled mod-
els without mixup (No Mixup), distilled models with PMU
(10% and 50% when α= 1), and distilled models with full
mixup (FMU, α= 1). As seen in Table 1, the students
trained with PMU consistently outperform the ones trained
without mixup. Surprisingly, in some cases (e.g., T:W40-
2 & S:W16-2 and T:W40-2 & S:SN1), our student trained
with PMU performs better than the teacher.
Experiments on ImageNet: Table 2 shows the top-1 ac-
curacy on ImageNet [3]. In this experiment, we choose
ResNet34 and ResNet18 [5] as the teacher network and
student network, respectively. We train the model for 100
epochs and with initial learning rate is 0.1 decayed by 0.1
at 30, 60, and 80 epochs. The batch size is set as 256.
For comparison with other distillation methods, the hyper-
parameters of other methods follow their respective papers.
The balancing parameters γkdandαkdare 0.1 and 0.9 re-
spectively, and we report the partial mixup of 10% and
100% (FMU). We observe that the proposed method with
full mixup boosts the top-1 and top-5 accuracy by 2.07%
and 1.56% over the baseline and fully mixup shows better
performance than 10% partial mixup. We will discuss lower
performance cases in the following section.
Performance analysis: Based on Table 1, in some of

--- PAGE 7 ---
Table 1. CIFAR-100 test accuracy (%) of student networks trained with several distillation methods. The results of the other distillation
methods except ∗are quoted from [27]. The best and second-best results are highlighted in Bold and Red. ∗is performed with our
implementation based on the author-provided code.
Teacher W40-2 W40-2 RN56 RN110 RN110 RN32 ×4 VG13 VG13 RN50 RN50 RN32 ×4 RN32 ×4 W40-2
Student W16-2 W40-1 RN20 RN20 RN32 RN8 ×4 VG8 MN2 MN2 VG8 SN1 SN2 SN1
Teacher 75.61 75.61 72.34 74.31 74.31 79.42 74.64 74.64 79.34 79.34 79.42 79.42 75.61
Student 73.26 71.98 69.06 69.06 71.14 72.50 70.36 64.6 64.6 70.36 70.50 71.82 70.50
KD[8] 74.92 73.54 70.66 70.67 73.08 73.33 72.98 67.37 67.35 73.81 74.07 74.45 74.83
FitNet[20] 73.58 72.24 69.21 68.99 71.06 73.50 71.02 64.14 63.16 70.69 73.59 73.54 73.73
AT[36] 74.08 72.77 70.55 70.22 72.31 73.44 71.43 59.40 58.58 71.84 71.73 72.73 73.32
SP[28] 73.83 72.43 69.67 70.04 72.69 72.94 72.68 66.30 68.08 73.34 73.48 74.56 74.52
CC[18] 73.56 72.21 69.63 69.48 71.48 72.97 70.71 64.86 65.43 70.25 71.14 71.29 71.38
VID[1] 74.11 73.30 70.38 70.16 72.61 73.09 71.23 65.56 67.57 70.30 73.38 73.40 73.61
RKD[16] 73.35 72.22 69.61 69.25 71.82 71.90 71.48 64.52 64.43 71.50 72.28 73.21 72.21
PKT[17] 74.54 73.45 70.34 70.25 72.61 73.64 72.88 67.13 66.52 73.01 74.10 74.69 73.89
AB[7] 72.50 72.38 69.47 69.53 70.98 73.17 70.94 66.06 67.20 70.65 73.55 74.31 73.34
FT[10] 73.25 71.59 69.84 70.22 72.37 72.86 70.58 61.78 60.99 70.29 71.75 72.50 72.03
NST[9] 73.68 72.24 69.60 69.53 71.96 73.30 71.53 58.16 64.96 71.28 74.12 74.68 74.89
CRD[27] 75.48 74.14 71.16 71.46 73.48 75.51 73.94 69.73 69.11 74.30 75.11 75.65 76.05
ICKD∗[13] 75.64 74.18 71.56 71.29 73.49 74.78 73.36 68.61 68.65 73.43 74.96 75.34 76.18
Ours (No mixup) 75.38 73.70 71.85 71.61 73.60 75.46 72.92 67.37 67.72 73.10 73.38 75.06 75.09
Ours (PMU=10%) 76.06 74.42 72.09 71.94 74.07 76.87 73.60 68.52 69.55 74.29 75.89 77.06 76.78
Ours (PMU=50%) 75.87 74.69 71.80 71.78 73.97 77.13 74.00 69.14 69.69 74.61 76.83 77.60 77.18
Ours (FMU) 75.69 73.34 70.98 70.99 73.48 77.25 73.84 68.81 69.80 74.50 77.17 77.92 77.00
Table 2. Top-1 and Top-5 accuracy (%) on ImageNet validation dataset compared with various knowledge distillation methods.
Teacher Student KD[8] AT[36] RKD[16] SP[28] CC[18] CRD[27] Ours(PMU=10%) Ours(FMU)
Top-1 73.31 69.75 70.66 70.70 70.59 70.79 69.96 71.17 71.38 71.82
Top-5 91.42 89.07 89.88 90.00 89.68 89.80 89.17 90.13 90.40 90.63
the teacher-student combinations, we observe that adding
more mixup pairs helps the student achieve higher accu-
racy than small amounts of pairs. Furthermore, for differ-
ent architectural styles, the use of full mixup outperforms
others in some cases. The presumption from this obser-
vation is that the networks from different architectures try
to seek their solution paths, which means that the teacher
and the student have unalike distributions in the logits, and
thus imposing extra smoothness induced by strong augmen-
tation might provide the student with additional informa-
tion about how the different style teacher represents knowl-
edge. Also, in the case of ImageNet having a large number
of classes, the generated knowledge learned by two-mixing
images and their labels might produce relatively less in-
formative knowledge compared to CIFAR-100, so a strong
smoothness by full mixup is favorable to distilling a better
student model in this case. While we heuristically present
that the performance is controlled by the amount of smooth-
ness, how much exact smoothness should be imposed across
datasets or networks remains an open question that can form
the basis for further future work.
5.2. Robustness to adversarial examples
One undesirable consequence of models trained is their
fragility to adversarial examples [4]. Adversarial examples
are made by adding tiny (visually imperceptible) perturba-Table 3. Classification accuracy against the white-box attack with
various perturbations of ϵfor every pixel. All methods are trained
with full mixup ( α= 0.2) and our distillation is trained with par-
tial mixup only 10% ( α= 1.0) on CIFAR100.
FGSM
eps Vanilla(+ mixup ) KD(+ mixup ) ICKD(+ mixup ) CRD(+ mixup ) Ours
0.0 69.14(69.42) 70.36(70.65) 71.00(70.36) 71.03(69.25) 72.01
0.001 61.14(63.18) 64.25(65.02) 64.74(64.59) 64.09(63.98) 68.92
0.003 48.80(51.92) 52.98(54.98) 53.35(54.43) 52.91(53.25) 62.21
0.005 39.00(42.59) 44.13(46.28) 44.00(46.40) 43.99(44.20) 55.89
0.01 23.25(27.61) 29.89(32.29) 29.50(31.96) 29.75(29.09) 44.10
I-FGSM
0.0 69.14(69.42) 70.36(70.65) 71.00(70.36) 71.03(69.25) 72.01
0.001 61.57(63.03) 64.08(64.85) 64.58(64.45) 63.91(63.92) 68.91
0.003 46.79(50.55) 51.43(53.56) 51.72(53.11) 51.39(51.80) 61.53
0.005 34.05(39.19) 39.96(42.99) 39.94(42.97) 40.02(40.78) 53.93
0.01 13.28(18.58) 19.71(23.12) 19.34(23.13) 19.59(19.70) 37.40
tions to legitimate samples to deteriorate the model per-
formance. Unfortunately, many distillation methods have
evolved to improve the performance of KD, disregarding at-
tacks. Thus, in this section, we evaluate our model against
white-box attacks where we used trained models themselves
to generate adversarial examples using two common at-
tacks, the Fast Gradient Sign Method (FGSM) and the It-
erative FGSM (I-FGSM) methods [4]. For I-FGSM, we
use 10 iterations with an equal step size. The results for
both attacks are summarized in Table 3. For distillation
methods, the setting of teacher and student is T:RN110 &
S:RN20, where they were trained on CIFAR100. We ap-
plied mixup augmentation to all methods we explored (see.

--- PAGE 8 ---
40
45
50
55
60
65
70
75
80
0
0.001
0.003
0.005
0.01
Accuracy (%)
eps
FGSM
Ours w/o MU
Ours FMU (0.7)
Ours PMU (10%)
Ours PMU (50%)
Ours PMU (80%)
35
45
55
65
75
85
0
0.001
0.003
0.005
0.01
Accuracy (%)
eps
I
-
FGSM
Ours w/o MU
Ours FMU (0.1)
Ours FMU (0.4)
Ours FMU (1.0)
40
45
50
55
60
65
70
75
80
0
0.001
0.003
0.005
0.01
Accuracy (%)
eps
FGSM
Ours w/o MU
Ours FMU (0.1)
Ours FMU (0.4)
Ours FMU (1.0)
35
45
55
65
75
0
0.001
0.003
0.005
0.01
Accuracy (%)
eps
I
-
FGSM
Ours w/o MU
Ours FMU (0.7)
Ours PMU (10%)
Ours PMU (50%)
Ours PMU (80%)
40
45
50
55
60
65
70
75
80
0
0.001
0.003
0.005
0.01
Accuracy (%)
eps
FGSM
Ours w/o MU
Ours FMU (0.2)
Ours PMU (0.7, 50%)
35
45
55
65
75
0
0.001
0.003
0.005
0.01
Accuracy (%)
eps
I
-
FGSM
Ours w/o MU
Ours FMU (0.2)
Ours PMU (0.7, 50%)Figure 7. Histogram of test accuracy under the attacks on CIFAR100. The first column indicates comparative results of different αwhen
full mixup is used. The second column shows the comparison between full mixup and partial mixup with α= 0.7. And the last figure
shows the comparison between FMU (0.2) and PMU (0.7, 50%).
method+ mixup in the table). We trained our model using a
partial mixup of only 10% with α= 1. Even with only 10%
mixup pairs used, it shows impressively resistant to both at-
tacks. We notice that distillation methods with the mixup,
which utilize feature maps such as ICKD and CRD under-
perform those without mixup. We will show more compar-
ative results of different amounts of mixup pairs in the next
section.
5.3. Ablation Study
In this section, we conduct an ablation study for hyper-
parameters with a combination of T:WN40-2 & S:WN16-2
networks. As shown in Table 4, the distilled models with
PMU and high αvalue generally yields better performance.
Further, to investigate how much the degree of partial mixup
andαreact against both attacks, we also show test accu-
racy in Figure 7. As shown in the first and second columns
of the histogram, when strong augmentation is involved in
training such as full mixup (FMU with a high value of α)
or the high number of mixup pairs (PMU 80%), it improves
robustness. Interestingly, only 10% partial mixup (gray bar
in the middle column) defends both attacks well. For the
last column of the figure, we selected two distilled models
from FMU ( α= 0.2) and PMU (50%, α= 0.7) where they
have similar test performance in Table 4. We observe that
PMU (50%, α= 0.7) shows slightly higher robustness than
FMU ( α= 0.2).
6. Conclusions
In this work, we study the role of a mixup in knowl-
edge distillation. We observed that a mixup-trained teacher
network produces inferior supervision due to exorbitant
smoothness imposed on the features and logits, especially
at high temperatures during distillation. Thus, the students
experience a reduction in their performance in KD. We sup-Table 4. Test accuracy under different settings of αand partial
mixup amount on CIFAR100. The accuracy is averaged over 3
runs. M. denotes mixup.
w/ M.α 0.1 0.2 0.4 0.7 1.0
PMU=10% 75.30 75.45 75.89 75.94 75.92
PMU=30% 75.51 75.40 75.71 76.21 75.95
PMU=50% 75.50 75.50 75.83 76.02 75.97
PMU=80% 75.36 75.66 76.05 76.19 75.78
FMU 75.69 76.01 75.96 75.92 75.75w/o M.KD 74.60%
Vanilla 73.26%
port our findings through a series of empirical analyses and
large-scale experiments on the image classification task.
Our findings provide insight into the inner workings of the
distilled model trained with mixup augmentation. These in-
sights allow us to develop an improved learning strategy us-
ing rescaled logits and partial mixups.
As we mentioned earlier, various mix-based augmen-
tations have shown their effectiveness for particular tasks.
However, these augmentations may tend to create unrea-
sonable training samples as it blends random images [11],
which could distort reasonable relative structure among cat-
egories. As a result, this potentially yields an unfavorable-
smooth effect on logits during distillation. Therefore, de-
veloping an augmentation method that automatically selects
more reasonable samples ensuring the best-fitting smooth-
ness would further boost the progress in the distillation
field. We will further develop this technique in our future
work.
7. Acknowledgements
This material is based upon work supported by the De-
fense Advanced Research Projects Agency (DARPA) under
Agreement No. HR00112290073. Approved for public re-
lease; distribution is unlimited.

--- PAGE 9 ---
References
[1] Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D
Lawrence, and Zhenwen Dai. Variational information dis-
tillation for knowledge transfer. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 9163–9171, 2019.
[2] Chung-Cheng Chiu, Tara N Sainath, Yonghui Wu, Ro-
hit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli
Kannan, Ron J Weiss, Kanishka Rao, Ekaterina Gonina,
et al. State-of-the-art speech recognition with sequence-
to-sequence models. In Proceedings of the IEEE Interna-
tional Conference on Acoustics, Speech and Signal Process-
ing, pages 4774–4778, 2018.
[3] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition , pages 248–255, 2009.
[4] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.
Explaining and harnessing adversarial examples. arXiv
preprint arXiv:1412.6572 , 2014.
[5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 770–778, 2016.
[6] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Jun-
yuan Xie, and Mu Li. Bag of tricks for image classifica-
tion with convolutional neural networks. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 558–567, 2019.
[7] Byeongho Heo, Minsik Lee, Sangdoo Yun, and Jin Young
Choi. Knowledge transfer via distillation of activation
boundaries formed by hidden neurons. In Proceedings of the
AAAI Conference on Artificial Intelligence , volume 33 (01),
pages 3779–3787, 2019.
[8] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-
ing the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2015.
[9] Zehao Huang and Naiyan Wang. Like what you like: Knowl-
edge distill via neuron selectivity transfer. arXiv preprint
arXiv:1707.01219 , 2017.
[10] Jangho Kim, SeongUk Park, and Nojun Kwak. Paraphrasing
complex network: Network compression via factor transfer.
arXiv preprint arXiv:1802.04977 , 2018.
[11] Jang-Hyun Kim, Wonho Choo, Hosan Jeong, and Hyun Oh
Song. Co-mixup: Saliency guided joint mixup with super-
modular diversity. arXiv preprint arXiv:2102.03065 , 2021.
[12] Xingjian Li, Haoyi Xiong, Chengzhong Xu, and Dejing Dou.
Smile: Self-distilled mixup for efficient transfer learning.
arXiv preprint arXiv:2103.13941 , 2021.
[13] Li Liu, Qingle Huang, Sihao Lin, Hongwei Xie, Bing Wang,
Xiaojun Chang, and Xiaodan Liang. Exploring inter-channel
correlation for diversity-preserved knowledge distillation. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 8271–8280, 2021.
[14] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.
Shufflenet v2: Practical guidelines for efficient cnn architec-ture design. In Proceedings of the European Conference on
Computer Vision , pages 116–131, 2018.
[15] Rafael M ¨uller, Simon Kornblith, and Geoffrey Hinton.
When does label smoothing help? arXiv preprint
arXiv:1906.02629 , 2019.
[16] Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho.
Relational knowledge distillation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 3967–3976, 2019.
[17] Nikolaos Passalis and Anastasios Tefas. Learning deep rep-
resentations with probabilistic knowledge transfer. In Pro-
ceedings of the European Conference on Computer Vision ,
pages 268–284, 2018.
[18] Baoyun Peng, Xiao Jin, Jiaheng Liu, Dongsheng Li, Yichao
Wu, Yu Liu, Shunfeng Zhou, and Zhaoning Zhang. Correla-
tion congruence for knowledge distillation. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 5007–5016, 2019.
[19] Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz
Kaiser, and Geoffrey Hinton. Regularizing neural networks
by penalizing confident output distributions. arXiv preprint
arXiv:1701.06548 , 2017.
[20] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou,
Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets:
Hints for thin deep nets. arXiv preprint arXiv:1412.6550 ,
2014.
[21] Andrew Rosenberg and Julia Hirschberg. V-measure: A
conditional entropy-based external cluster evaluation mea-
sure. In Proceedings of the Joint Conference on Empiri-
cal Methods in Natural Language Processing and Compu-
tational Natural Language Learning , pages 410–420, 2007.
[22] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-
moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted
residuals and linear bottlenecks. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 4510–4520, 2018.
[23] Zhiqiang Shen, Zechun Liu, Dejia Xu, Zitian Chen, Kwang-
Ting Cheng, and Marios Savvides. Is label smoothing
truly incompatible with knowledge distillation: An empiri-
cal study. arXiv preprint arXiv:2104.00676 , 2021.
[24] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556 , 2014.
[25] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the inception archi-
tecture for computer vision. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition , pages
2818–2826, 2016.
[26] Xu Tan, Yi Ren, Di He, Tao Qin, Zhou Zhao, and Tie-Yan
Liu. Multilingual neural machine translation with knowledge
distillation. arXiv preprint arXiv:1902.10461 , 2019.
[27] Yonglong Tian, Dilip Krishnan, and Phillip Isola.
Contrastive representation distillation. arXiv preprint
arXiv:1910.10699 , 2019.
[28] Frederick Tung and Greg Mori. Similarity-preserving knowl-
edge distillation. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 1365–1374,
2019.

--- PAGE 10 ---
[29] Laurens Van der Maaten and Geoffrey Hinton. Visualiz-
ing data using t-sne. Journal of machine learning research ,
9(11), 2008.
[30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In Advances in Neural
Information Processing Systems , pages 5998–6008, 2017.
[31] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Na-
jafi, Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Ben-
gio. Manifold mixup: Better representations by interpolating
hidden states. In Proceedings of the International Confer-
ence on Machine Learning , pages 6438–6447, 2019.
[32] Dongdong Wang, Yandong Li, Liqiang Wang, and Boqing
Gong. Neural networks are more productive teachers than
human raters: Active mixup for data-efficient knowledge
distillation from a blackbox model. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 1498–1507, 2020.
[33] Huan Wang, Suhas Lohit, Michael Jones, and Yun Fu.
Knowledge distillation thrives on data augmentation. arXiv
preprint arXiv:2012.02909 , 2020.
[34] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V
Le. Self-training with noisy student improves imagenet clas-
sification. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 10687–
10698, 2020.
[35] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk
Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regular-
ization strategy to train strong classifiers with localizable fea-
tures. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 6023–6032, 2019.
[36] Sergey Zagoruyko and Nikos Komodakis. Paying more at-
tention to attention: Improving the performance of convolu-
tional neural networks via attention transfer. arXiv preprint
arXiv:1612.03928 , 2016.
[37] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-
works. arXiv preprint arXiv:1605.07146 , 2016.
[38] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and
David Lopez-Paz. mixup: Beyond empirical risk minimiza-
tion. arXiv preprint arXiv:1710.09412 , 2017.
[39] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
Shufflenet: An extremely efficient convolutional neural net-
work for mobile devices. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition , pages
6848–6856, 2018.
