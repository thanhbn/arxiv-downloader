# 2210.11768.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/data-augmentation/2210.11768.pdf
# File size: 593809 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Published as a conference paper at ICLR 2023
AUGMENTATION WITH PROJECTION :
TOWARDS AN EFFECTIVE AND EFFICIENT
DATA AUGMENTATION PARADIGM FOR DISTILLATION
Ziqi Wang1, Yuexin Wu2y, Frederick Liu2, Daogao Liu3, Le Hou2, Hongkun Yu2, Jing Li2,
Heng Ji1
1University of Illinois Urbana-Champaign2Google3University of Washington
fziqiw9, hengjig@illinois.edufcrickwu, frederickliu, lehou, hongkuny, jingli g@google.com
dgliu@uw.edu
ABSTRACT
Knowledge distillation is one of the primary methods of transferring knowledge
from large to small models. However, it requires massive task-speciÔ¨Åc data, which
may not be plausible in many real-world applications. Data augmentation methods
such as representation interpolation, token replacement, or augmentation with
models are applied to tackle this problem. However, these data augmentation
methods either potentially cause shifts in decision boundaries (representation inter-
polation), are not expressive enough (token replacement), or introduce too much
computational overhead (augmentation with models). To this end, we propose Aug-
Pro ( Augmentation with Projection), an effective and efÔ¨Åcient data augmentation
method for distillation. Our method builds on top of representation interpola-
tion augmentation methods to maintain the diversity of expressions and converts
the augmented data to tokens to avoid shifting decision boundaries. It uses simple
operations that come with little computational overhead. The results on multiple
GLUE tasks show that our methods can improve distillation performance by a
large margin at a low time cost. Codes are available at https://github.com/
google-research/google-research/tree/master/augpro .
1 I NTRODUCTION
Large-scale language models (Devlin et al., 2018; Raffel et al., 2020; Brown et al., 2020; Zhang et al.,
2022c) have achieved great success on various natural language processing (NLP) tasks, such as
information extraction (Lu et al., 2021) and question answering (Kassner & Sch ¬®utze, 2020). However,
large-scale models have high computational overhead, which limits their deployment in edge devices
and fast response scenarios (Sun et al., 2020b). One widely used solution is to perform knowledge
distillation (Hinton et al., 2015) from large-scale models to small-scale models. This method, however,
usually requires a large amount of data to guarantee the transfer quality, which may not be easily
obtained in real-world applications. To this end, data augmentation methods are applied (Liang et al.,
2020; Wang & Yang, 2020; Zhang et al., 2022b) to improve the distillation performance.
There are three major types of data augmentation methods: (1) Representation interpolation. For
example, Liang et al. (2020), Chen et al. (2020a) and Sun et al. (2020a) apply linear interpolation
(Zhang et al., 2017) to word embeddings, hidden states between transformer layers, and encoder
outputs, respectively, to augment the original dataset with virtual data points. Data points are virtual
because they are not real language inputs. Instead, they are representations (e.g., embeddings). (2)
Token replacement. Kobayashi (2018) replaces tokens with their synonyms. Easy Data augmentation
(Wei & Zou, 2019) combines synonym replacement, random insertion, random swap, and random
deletion. (3) Augmentation with models. Yoo et al. (2021) and Zhou et al. (2021) use GPT-3 (Brown
et al., 2020) and T5 (Raffel et al., 2020) respectively as the language model to generate new text data
of similar types. (1) supports many operations such as linear interpolation (Zhang et al., 2017) and
Work was done when the Ô¨Årst author was interning at Google.
yCorresponding author
1arXiv:2210.11768v2  [cs.CL]  11 Mar 2023

--- PAGE 2 ---
Published as a conference paper at ICLR 2023
small perturbation (Madry et al., 2017). It makes the methods very expressive in generating a diverse
range of data. However, the newly generated representations (e.g., embeddings) may sit outside of
the real data distribution. For instance, word embeddings are converted from a vocabulary in the text
domain. Performing augmentation at this level may result in representations that do not have their
counterparts in the vocabulary. As a result, the augmented data may mislead the model to generate a
shifted decision boundary that can largely affect the qualities (Section 3). (2) can generate in-domain
data easily. By using synonym replacement (Wang & Yang, 2015), new data can be obtained at a low
cost. Despite this good property, this stream of methods lacks the ability to generate diversiÔ¨Åed data.
Subsequently, they contribute little to sampling low-resource data areas and limit the performance
gains in practice. (3) generates both diversiÔ¨Åed and in-domain data using large language models
such as GPT-3 (Brown et al., 2020) and T5 (Raffel et al., 2020). Due to their large computational
overheads, on the other hand, the Ô¨Ånal distillation quality will be highly limited to the amount of
generated data, which is usually not affordable to the scale of even tens of thousands of sentences in
practice. Figure 1 summarizes the advantages of each augmentation method.
Representation Interpolation
AugmentationWith ModelsTokenReplacementHigh Quality
High QualityCheapCheapNo Shifted Decision BoundariesExpressive
Figure 1: An illustration of each augmentation
method‚Äôs advantages.Considering all the approaches above, we pro-
pose AugPro, an effective and efÔ¨Åcient data aug-
mentation method for the distillation scenario,
which absorbs the advantages above without be-
ing limited by their drawbacks. SpeciÔ¨Åcally,
AugPro: (1) (effectiveness) is as expressive as
representation interpolation; (2) (effectiveness)
does not mislead decision boundaries; (3) (efÔ¨Å-
ciency) has low computational overhead. In dis-
tillation settings, we can always use the teacher
to label the hallucinated data in the knowledge
distillation scenario. This suggests that we can
encourage AugPro to produce as diverse data as
possible that are not limit to instances with only
the same or Ô¨Çipped labels.
Concretely, our method builds on top of repre-
sentation interpolation augmentation methods (property (1)), which does not constrain the generated
data to be within small regions of their ‚Äúparents‚Äù. The key of AugPro is to convert the augmented
data to the format of tokens through projections (property (2)) with low-cost operations (property (3)).
We conduct our experiments on GLUE (Wang et al., 2018) datasets. Results show that our method
could boost the distillation performance signiÔ¨Åcantly with low computational overhead.
To sum up, our contributions are:
‚Ä¢We propose an effective and efÔ¨Åcient data augmentation method for knowledge distillation.
‚Ä¢We empirically evaluate the effectiveness and efÔ¨Åciency of AugPro and theoretically examine
that AugPro satisÔ¨Åes three properties under certain circumstances.
2 R ELATED WORK
Knowledge Distillation Knowledge distillation was Ô¨Årst proposed by (Hinton et al., 2015). It aims
to distill knowledge from one model to another by minimizing the distance between the outputs of
two models on the same input. With the rise of transformers (Vaswani et al., 2017) and BERT (Devlin
et al., 2018), more and more attention has been paid to the distillation of pre-training language models.
Tang et al. (2019) distill Ô¨Åne-tuned BERT to a single-layer BiLSTM network and makes the BiLSTM
network as good as ELMo (Peters et al., 2018). Sun et al. (2019) not only distill from outputs but also
distill from teacher models‚Äô hidden layers. These methods distill language models in the Ô¨Åne-tuning
stage, whereas Sanh et al. (2019) and Sun et al. (2020b) focus on distilling language models in the
pre-training stage directly to make student models task-agnostic. TinyBERT (Jiao et al., 2019) distill
BERT from both the pre-training and Ô¨Åne-tuning stages. We focus on a widely used setting. We
distill knowledge in the Ô¨Åne-tuning stage by minimizing the distance between two models‚Äô outputs.
Data Augmentation Representation interpolation methods are popular in the computer vision re-
search community. MixUp (Zhang et al., 2017) uses linear interpolation to get augmented images and
2

--- PAGE 3 ---
Published as a conference paper at ICLR 2023
labels. Given an image x1,x2and their labels y1,y2, MixUp uses a linear interpolation to generate a
new data point x0and its label y0:
x0= MixUp(x1;x2) =x1+ (1 )x2; y0= MixUp(y1;y2) =y1+ (1 )y2 (1)
FGSM (Goodfellow et al., 2014) and PGA (Madry et al., 2017) use gradients to generate adversarial
examples. Given an image x, FGSM will generate new data x0:
x0=x+Sign(rxL) (2)
whereLis the loss of a speciÔ¨Åc task and is a small value. x0andxhave the same label. CutMix
(Yun et al., 2019) cuts images and then concatenates them together to get a new one. Though these
methods were originally designed for images, they can be adapted to NLP tasks. Liang et al. (2020)
use MixUp on word embeddings to augment data for knowledge distillation. Chen et al. (2020b) use
MixUp on hidden states between transformer layers. Jindal et al. (2020) also use MixUp on hidden
states but consider the effect of mean and variance. Zhang et al. (2022b) apply PGA to student
models‚Äô embeddings and leave teacher models‚Äô embeddings unchanged, Ô¨Ånding that PGA can beneÔ¨Åt
knowledge distillation. Token replacement methods mainly focus on language inputs. Synonym
replacement methods (Kobayashi, 2018) replace tokens with their synonyms. Easy data augmentation
(EDA) (Wei & Zou, 2019) incorporates synonym replacement, random insertion, random swap, and
random deletion. TreeMix (Zhang et al., 2022a) uses a constituency parser to decide which token
should be replaced. Augmentation with models is another approach to generating new data. FlipDA
(Zhou et al., 2021) uses T5 to generate data that has Ô¨Çipped labels. GPT3Mix (Yoo et al., 2021)
designs prompts and uses GPT3 to generate new data. Back translation (Yu et al., 2018) uses neural
networks to translate inputs to another language and then translate them back. Our method (AugPro)
uses representation interpolation as the backbone and utilizes projection to convert representations to
tokens with low-cost operations.
3 M OTIVATING EXAMPLES
Though representation interpolation methods have the good property of generating diverse data,
we Ô¨Ånd that these vision techniques cannot be directly applied to NLP tasks. This is because
representation interpolation augments data in a continuous manner, under which case the new data
may never exist in the discrete input space, causing the decision boundary shifts.
Take an example of a simple two-dimensional problem with linear separability (Figure 2). Let X=
fx1;x2;x3;x4gbe the universe of all data to learn, and Y=fy1;y2;y3;y4gbe the corresponding
labels. Suppose we know all of X;Yand run the linear support vector machine (SVM) with a hard
margin, i.e., min;bkk2such thatyi(>xi+b)1for alli, we get the solution ;bSince it is
hard to get all data in the real-world setting, we suppose that we only have fx1;x3gas the training
dataset. If we simply use MixUp, we get xMixUp , as the augmented data, whose label yMixUp =
sign(()>xMixUp +b) =y3=y4. Now running the linear SVM with fx2;x4;xMixUpgwith
labelsfy2;y4;yMixUpg, we getMixUp andbMixUp . Nevertheless, sign(>
MixUpx3+bMixUp )6=y3.
As a comparison, if we project xMixUp to its nearest neighbor and get xMixUp P=x2whose
labelyMixUp P=y2, running SVM with fx1;x2;xMixUp Pgandfy1;y2;yMixUp Pgcan get
MixUp PandbMixUp P, which can classify all data correctly. Appendix B shows the concret
number of each parameter.
To this end, augmented data should be the real data in the input space, i.e., the format of tokens, to
leverage this problem. This observation leads to our method AugPro which uses projection to convert
augmented representations to symbolic tokens. Compared with the virtual data point generated by
representation interpolation, projection can explore more real data and leads to a lower error (Section
4.2 and Appendix H).
4 M ETHODOLOGY
In this section, we Ô¨Årst formulate the deÔ¨Ånition of knowledge distillation in NLP (Hinton et al., 2015)
and then introduce our method AugPro.
3

--- PAGE 4 ---
Published as a conference paper at ICLR 2023
x1
x2 x3x4
‚àí+
(a) Ground-Truth
x1
x2 x3x4
xMixUp‚àí+ (b) MixUp SVM
x1
x2 x3x4
‚àí+
Projection (c) MixUp with Projection SVM
Figure 2: Decision boundary shifts in 2D space for discrete datasets. (a) is the gound-truth, where
x1;x3are the observable training data while x2;x4with transparent colors mean the unseen data. In
(b), one gets augmented data xMixUp with labelyMixUp = 1, and do SVM with fx1;x3;xMixUpg
with their labels. In (c), one projects xMixUp to its nearest neighbor x2, and do SVM with fx1;x2;x3g.
We see that the correction of projection in (c) brings smaller decision boundary shifts than (b).
4.1 KNOWLEDGE DISTILLATION
Knowledge distillation is a method to distill knowledge from large-scale models to small-scale models.
Formally speaking, considering an NLP classiÔ¨Åcation task, we have a corpus D=f(xi;yi)gN
i=1that
containsNinput-output pairs, where xiis an input sentence with tokens xi= [wi1;;wini];wk2
V;V is the vocabulary, niis the number of tokens in xi.yiis the output label for xi. We use plain
texts rather than bold texts for xbecause language inputs are a sequence of tokens, which is different
from images. Then we distill knowledge from a large-scale model f(;T)with parameter T(i.e., a
teacher model) to a small-scale model g(;S)with parameter S(i.e., a student model). In practice,
Thas much more parameters than S. The distillation process can be divided into two stages:
‚Ä¢Teacher training. Optimize Ton the datasetD. In classiÔ¨Åcation problems, we use cross-entropy
loss to do empirical risk minimization on T:
0
T= arg min
T1
NNX
i=1CrossEntropy (f(xi;T);yi) (3)
‚Ä¢Student training. Optimize Son the datasetDwith both ground-truth labels and outputs from
teachers. In classiÔ¨Åcation problems,
0
S= arg min
SLKD= arg min
S1
NNX
i=1CrossEntropy (g(xi;S);yi) +d(g(xi;S);f(xi;0
T))
(4)
whered(;)is a distance function. In practice, d(;)could be cross-entropy or mean square error.
Empirical results from former studies (Hinton et al., 2015; Sun et al., 2020b; Sanh et al., 2019; Sun
et al., 2019) show that knowledge distillation will train a better 0
Sbecause the student model not
only learns from ground-truth labels but also learns the generality from the teacher model.
Note for the student training, we can combine knowledge distillation and data augmentation together:
0
S= arg min
SLKD+LAug
whereLAugdenotes the knowledge distillation loss on augmented data which leads to different
variants of methods. As one important way to help the student learn more effectively, how to generate
new data with augmentation loss is the key and major discussion topic in the remaining sections.
4.2 A UGPRO: AUGMENTATION WITH PROJECTION
In this section, we will introduce four variants of LAug: two backbones (MixUp and FGSM) and two
AugPro variants building on top of them. Figure 4 shows the concept of our proposed method.
4

--- PAGE 5 ---
Published as a conference paper at ICLR 2023
Algorithm 1: AugPro Algorithm
Input: DatasetD=f(X;Y )g, representation interpolation function h(), projection function p(), the
teacher model fwith Ô¨Åne-tuned parameters 0
Tand the student model gwith parameter Sthat
needs to be Ô¨Åne-tuned, vocabulary V, learning rate , training steps K, batch sizeB, sentence
lengthL, embedding dimension H.
Output: Fine-tuned0
S
‚Ä¢k= 0
‚Ä¢whilek<K
‚ÄìSample a data batch B=f(x;y)g2VBLfromD=f(X;Y )g.
‚ÄìGet augmented representations Brep=h(B)2RBLH(e.g., Equation (1, 2))
‚ÄìProject representations to tokens B0=p(Brep)2VBL(e.g., Equation 5)
‚ÄìUse Equation (4) to compute LKD.
‚ÄìCompute lossLAug(e.g., Equation (6, 7)) based on B0,fandg.
‚ÄìS=S r(LKD+LAug);k=k+ 1
‚Ä¢returnS
AugPro builds on top of representation interpolation augmentation methods. The pipeline (Algorithm
1) can be divided into three steps: (1) We Ô¨Årst get augmented representations (i.e. h()). (2) Then
we use projection to convert representations to tokens (i.e. p())). (3) At last, we compute LAugto
update student models. The key to step (2) is projection. Concretely, language models map tokens
to representations, and projection aims to Ô¨Ånd an inverse mapping to map representations back to
tokens. This way, AugPro will avoid shifting decision boundaries (Section 3). However, the inverse
mapping is hard to Ô¨Ånd in practice. First, popular language model architectures such as transformers
(Vaswani et al., 2017) are usually complex and are hard to get the inverse mapping. Second, the
input space of language is discrete, making the mapping irreversible. Thus, we could only use the
approximation technique to get the approximated inverse mapping. To this end, we focus on the
inverse mapping on the embedding level. First, the embedding mapping‚Äôs structure is much simpler
than the transformer layer and is straightforward for us to Ô¨Ånd the inverse mapping. Second, we can
use the nearest-neighbors as the approximation method, which is a cheap approximation.
Based on the analysis above, we use the nearest-neighbors to Ô¨Ånd our projection, i.e., the function
p()in the Algorithm 1. AugPro does not rely on speciÔ¨Åc representation interpolation methods. In
this paper, we apply AugPro to MixUp and FGSM, i.e. the h()in Algorithm 1 is MixUp or FGSM
as in Equation (1,2). We will illustrate two variants to perform projection (step (2)) and compute
LAugloss (step (3)) in the following texts.
We slightly abuse the notion of fandgto illustrate AugPro better. We divide fandginto two parts:
the Ô¨Årst part is an embedding function that maps tokens to embedding vectors ( feandge,edenotes
embeddings ), the rest is the second part ( flandgl,ldenotes layers ). Under this deÔ¨Ånition, f=flfe
andg=glge.
AugPro-Mix We get AugPro-Mix by applying the AugPro paradigm to MixUp. First, we apply
MixUp on word embeddings and labels. This gives us embeddings from the teacher ef
MixUp , embed-
dings from the student eg
MixUp , and the label yMixUp .LAugbecomes
LMixUp =1
MMX
j=1[CrossEntropy (gl(eg
MixUp;j;S);yMixUp;j)+d(gl(eg
MixUp;j;S);fl(ef
MixUp;j;0
T))]
when we use MixUp data to construct losses, where Mdenotes the number of augmented data.
For AugPro, we use the nearest-neighbors to get AugPro-Mix tokens xAugPro -Mix:
xAugPro -Mix= [wAugPro -Mix;1;;wAugPro -Mix;n]
wherewAugPro -Mix;i= max
w2VSim(ef
MixUp (i); fe(w))(5)
5

--- PAGE 6 ---
Published as a conference paper at ICLR 2023
Simmeans the similarity function, which can be the cosine similarity. e(i)means theith embedding
vector in e. Concrete examples can be found in Appendix D. Then the loss function LAugbecomes:
LAugPro -Mix=1
MMX
j=1d(g(xAugPro -Mix;j;S);f(xAugPro -Mix;j;0
T)) (6)
We do not use yMixUp because the projection operation (the nearest-neighbors in AugPro-Mix) does
not necessarily preserve the label.
AugPro-FGSM Though adversarial examples (AE) are originally aimed to improve the robustness
of models and may harm the performance on clean inputs (Raghunathan et al., 2019), Zhang et al.
(2022b) shows that AE can beneÔ¨Åt knowledge distillation. We get AugPro-FGSM by applying
AugPro to FGSM. We Ô¨Årst apply FGSM to the student model and get augmented data eg
FGSM . The
augmented data can be used to construct LAugdirectly:
LFGSM =1
MMX
j=1d(gl(eg
FGSM;j;S);fl(ef
j;0
T))
Following Equation 5, we could get xAugPro -FGSM by changing the footnotes accordingly.
We usually set in Equation (2) large in AugPro-FGSM since we do not want xAugPro -FGSM to be
the same as the original input, whereas in Equation (2) is a small value in FGSM. We use the cosine
similarity to implement the Sim function. The loss function LAugbecomes:
LAugPro -FGSM =1
MMX
j=1d(g(xAugPro -FGSM;j;S);f(xAugPro -FGSM;j;0
T)) (7)
Label Diversity We take two sentences from SST-2 dataset (Socher et al., 2013) as an example to
further explain that projection does not necessarily preserve labels but generates diverse labels. The
Ô¨Årst sentence is watch on video at home with the sentiment Neutral . The second sentence
isas good with the sentiment Positive . Then we can get the AugPro-Mix sentence watch
good video at home . Obviously the label of AugPro-Mix sentence should be Positive
rather than the linear interpolation of Positive andNeutral . This is the desired property in
distillation as we will use the teacher to label these newly generated data points.
Computational Overhead . If we assume the complexity of computing cosine similarity between
two vectors isO(d)wheredis the dimension of vectors, then the complexity of the projection (the
nearest-neighbors in our implementation) is O(NVd ), whereNis the sentence length and Vis the
vocabulary size. Nis usually within hundreds. Vis usually around 30;000in popular pre-train
language models using sub-word tokens such as BERT (Devlin et al., 2018) and T5 (Raffel et al.,
2020). As a result, O(NVd )brings little costs. On the other hand, the projection operation could
be parallelized since the NV similarity calculations do not affect each other. In modern parallel
computing architectures, such as GPUs and TPUs, projection can be calculated in a much faster
manner. Compared to the major large-scale language models‚Äô complexities, this computation will
take a small portion of resources. The detailed running time comparison can be found in Section 5.2.
Three Properties of AugPro. Since AugPro supports operations used in representation interpola-
tion methods, AugPro is expressive (property (1)). AugPro also converts representations to tokens
to avoid shifting decision boundaries, leading to a smaller error rate (property (2)). It can be shown
that AugPro-Mix has a1
4Nlower error rate than MixUp, and AugPro-FGSM has a1
2Nlower error
rate than FGSM with certain assumptions (Appendix H). Moreover, AugPro has a low computational
overhead to guarantee efÔ¨Åciency (property (3)), as described in the previous paragraph.
5 E XPERIMENTS
Our experiments aim to answer two questions: (1) How effective is AugPro when applied to the
knowledge distillation scenario? (2) Is AugPro efÔ¨Åcient?
Datasets and Settings Following previous knowledge distillation works (Liang et al., 2020; Zhang
et al., 2022b), we use GLUE (Wang et al., 2018) datasets as the benchmark. We use EncT5 (Liu
6

--- PAGE 7 ---
Published as a conference paper at ICLR 2023
et al., 2021) as our teacher and student models for the following reaons: (1) T5 has a much better
performance than BERT and is close to SOTA in many tasks. EncT5 is a simpliÔ¨Åed version of T5
that uses the whole encoders of T5 but only one decoder layer. EncT5 performs similarly to T5 on
classiÔ¨Åcation tasks such as GLUE tasks with fewer parameters. For example, EncT5 (small) only
contains 37M parameters but can perform similarly to T5 (small), which contains 77M parameters.
Using EncT5 will make the results more convincing and show that our method is still useful even with
powerful models. (2) Previous methods (Liang et al., 2020; Zhang et al., 2022b) distill knowledge
from a 12-layer BERT to a 6-layer BERT or a 3-layer BERT. However, the gap between the teacher
and student models is marginal. Therefore, the improvement space is limited, and the existence of
variance will weaken the credibility of the results. To this end, we distill knowledge from EncT5
(Large, 24-layer, 354M, teacher) to EncT5 (small, 8-layer, 37M, student), as the two models have a
signiÔ¨Åcant performance gap.
Baselines and Training We train several baselines for comparison: (1) Fine-Tuning (FT) : We
directly Ô¨Åne-tune EncT5 on the dataset. (2) Knowledge Distillation (KD) : We Ô¨Årst Ô¨Åne-tune a
teacher model (EncT5 Large), then distill knowledge from the teacher model to the student model
(EncT5 Small). (3) Knowledge Distillation + Back Translation (KD+BT) : Back translation (Yu
et al., 2018) Ô¨Årst translates input to another language and then translates it back. We choose back
translation as a representative method for the data augmentation type ‚Äúaugmentation with models‚Äù.
(4)Knowledge Distillation + K-Nearest-Neighbors (KD+KNN) KNN (Wang & Yang, 2015) Ô¨Årst
selects tokens from inputs, then replaces them with the K nearest neighbors in the embedding space.
KNN can be regarded as one token replacement method. (5) KD+MixUp (6)KD+FGSM (7)KD+
TMix (Chen et al., 2020b) MixUp on the hidden state between transformer layers. The last three
methods are of the ‚Äúrepresentation interpolation‚Äù type. We train student models with 0.6M steps and
512 batch size. Due to the high computation cost, we only augment data to twice as large as the
original dataset size for back translation. For all other methods, we augment data to twice as large as
the original batch size for each batch, i.e., we augment 0:6Msteps512batch size = 307:2Mdata
in total. More training details are in Appendix C.
5.1 E FFECTIVENESS OF AUGPRO
Table 1 shows the results of knowledge distillation. Due to the high cost, we only report back
translation results on the RTE dataset. We Ô¨Årst use the training data to train a teacher model and then
distill knowledge from the teacher model to the student model on the training data. We can conclude
that: (1) All data augmentation methods will beneÔ¨Åt the distillation. (2) AugPro can signiÔ¨Åcantly
improve the distillation performance compared with corresponding baselines. SpeciÔ¨Åcally, AugPro is
extremely useful for low-resource datasets such as CoLA and RTE. AugPro-Mix achieves scores
5.97% and 9.02% higher than MixUp on CoLA and RTE, respectively. AugPro-FGSM achieves
scores 10.52% and 8.31% higher than FGSM on CoLA and RTE, respectively. For large datasets
such as MNLI, AugPro-Mix and AugPro-FGSM can also improve the performance. (3) Moreover,
combining AugPro-FGSM and AugPro-Mix achieves the best performance in all listed methods.
Compared with vanilla knowledge distillation, combining AugPro-Mix and AugPro-FGSM improves
the performance from 2% to 14%.
Table 2 uses a different setting from Table 1. We only keep 10% training data labeled and assume
others are unlabeled. Then we use labeled training data to train a teacher model and unlabeled
training data to do knowledge distillation‚Äîthis is a more realistic setting since it is often easier to
get unlabeled data than to get labeled data. The conclusions above still hold. SpeciÔ¨Åcally, AugPro
can improve the accuracy from 1% to 2% on average on three datasets. Compared with the vanilla
distillation, AugPro can improve around 2% accuracy at most on three datasets.
5.2 E FFICIENCY OF AUGPRO
The efÔ¨Åciency of AugPro lies in two aspects. First, its complexity is low. Second, it can be computed
in parallel. To fully demonstrate these two advantages, we report the real-time cost of AugPro and
baselines in Table 3. KD+data augmentation is rough twice the time of vanilla KD since these
methods use twice the data as vanilla KD. We can also observe that augmentation with models
(KD+BT) takes much more time than other kinds of baselines, which shows that this method is not
efÔ¨Åcient enough. At last, AugPro brings little computational overhead as the time cost is the same as
7

--- PAGE 8 ---
Published as a conference paper at ICLR 2023
SST-2 CoLA MNLI-MM/M QNLI QQP MRPC STS-B RTE
Acc Matthew Acc Acc Acc/F1 Acc/F1 PC/SC Acc
67.3k 8.5k 392.7k 104.7k 363.8k 3.7k 5.7k 2.5k
EncT5 24-FT (354M) 97:20 63:60 91:40=91:10 95:40 92:73=90:00 91:42=93:30 88:19=88:00 86:30
EncT5 8-FT (37M) 92:89 45:84 84:69=84:26 89:84 91:45=88:41 87:01=90:91 86:39=85:94 59:21
EncT5 8-KD 92:09 45:56 85:93=85:61 89:46 91:36=88:24 84:56=88:85 87:29=87:18 61:37
+BT - - - - - - - 61:73
+KNN 94:27 54:60 87:13=87:01 91:54 92:14=89:40 86:03=90:32 87:14=87:27 66:79
+TMix 93:35 44:42 86:79=86:79 91:10 91:76=88:84 87:25=90:97 87:52=87:35 63:18
+MixUp 93:23 51:63 86:73=86:69 91:31 91:82=88:97 88:48=91:68 87:47=87:33 62:82
+AugPro-Mix 94:38 57:60 87:40=87:27 92:06 92:06=89:23 89:46=92:34 88:10=87:87 71:84
+FGSM 92:20 46:37 85:88=85:53 89:58 91:21=88:06 84:56=89:23 87:56=87:26 62:09
+AugPro-FGSM 94:61 56:89 87:02=86:85 91:67 92:10=89:26 88:24=91:67 87:64=87:51 70:40
+FGSM+MixUp 93:12 50:79 86:85=86:65 91:09 91:75=88:85 87:25=90:88 87:15=87:00 62:82
+AugPro-FGSM+AugPro-Mix 95:18 59 :01 87 :97=87:87 92 :92 92 :30=89:54 89 :46=92:42 88 :34=88:04 74 :73
Table 1: Knowledge distillation on the GLUE dataset. We Ô¨Årst use the training data to train a teacher
model and then distill knowledge from the teacher model to the student model on the training data.
EncT5 Ldenotes EncT5 with Ltransformer layers. L = 24 andL = 8 denote the teacher model with
354M parameters and the student model with 37M parameters, respectively.
KD +MixUp +AugPro-Mix +FGSM +AugPro-FGSM +FGSM+MixUp +AugPro-Mix+AugPro-FGSM
MNLI-MM/M 84:81=84:39 85:53=85:33 86:36=85:87 85:07=84:58 85:87=85:82 85 :70=85:65 86:76=86:81
SST-2 92:09 93 :35 94 :04 92 :09 94:27 93:46 94 :04
QNLI 89:68 90 :28 90 :98 89 :99 91 :03 90 :50 91:58
Table 2: Knowledge distillation on the GLUE dataset with a different setting from Table 1. We regard
10% of the data as labeled and the rest as unlabeled. The teacher model is Ô¨Årst trained on labeled
training data and then used for knowledge distillation on unlabeled training data.
the baselines. Results also show that KNN is much slower than other methods, which is explained in
Appendix G.
5.3 A BLATION STUDY
In ablation studies, we follow settings used in Table 1 unless otherwise stated.
Perturbation scale for in AugPro-FGSM The key hyperparameter in AugPro-FGSM is in
Equation (2). Small will makexAugPro-FGSM the same as the original input. Large tends to
makexAugPro-FGSM hard to understand, meaningless, and out of the domain. Therefore, a proper 
is essential. Our experiments Ô¨Ånd that = 35 is the best Ô¨Åt for T5 embeddings. Table 4 shows
KD+AugPro-FGSM performance with different .
Signs of gradients in AugPro-FGSM are not important The effectiveness of AugPro-
FGSM comes from gradients‚Äô signs and the projection in AugPro. To prove that AugPro-
FGSM mainly beneÔ¨Åts from AugPro, we implement two AugPro-FGSM variants: AugPro-
FGSMD (Descent Projection) that uses the opposite signs to AugPro-FGSM , and AugPro-
FGSMR (Random Projection) that uses random signs. Table 6 shows the results of AugPro-
FGSM and its two variants. We can observe AugPro-FGSM has a similar score to its variants in
all settings. Thus AugPro-FGSM mainly beneÔ¨Åts from AugPro. We also conduct experiments that
follow the setting of Table 2, and results can be found in Appendix E.
AugPro generates diverse labels We show that AugPro generates diverse labels at the end of Section
4. Here we empirically show that assuming AugPro preserving labels may harm performance. If
AugPro preserves labels, AugPro-Mix and AugPro-FGSM data should have the same labels as
MixUp and original data, respectively. We use these augmented data together with labels to Ô¨Åne-tune
student models directly. Results in Table 5 suggest that such augmented data and labels may harm
performance. Therefore, AugPro generates diverse labels and does not necessarily preserve labels.
AugPro consistently beneÔ¨Åts KD with different data sizes Figure 3 shows the performance of
AugPro with different data sizes. It can be observed that AugPro is better than all baselines in all data
sizes. Moreover, AugPro is extremely useful when the data size is small. For example, AugPro can
1Converted time. We run BT on TPU v2 and compute the equivalent time cost.
8

--- PAGE 9 ---
Published as a conference paper at ICLR 2023
KD +BT1+KNN +TMix +MixUp +AugPro-Mix +FGSM +AugPro-FGSM
Time (min) 1:68 13:15 4:57 3:48 3:48 3 :48 3 :24 3 :24
Table 3: Time (minutes) costs every 1000 steps on average of various methods on 8 TPU v3 slices.
Costs contain data augmentation, the forward pass, and the backpropagation. The table is divided
into four parts. Each part contains a speciÔ¨Åc data augmentation type (KD, augmentation with models,
token replacement, representation interpolation and AugPro).
 MNLI-MM/M SST-2 QNLI
30 85:60=85:28 93:69 90:44
35 85:87=85:82 94 :27 91 :03
40 85:79=85:58 93:23 90:81
100 85:18=84:74 91:97 90:44
Table 4: KD+AugPro-FGSM performance with
different.
Data Size 20% 50% 100%
Finetune 80:99=80:71 83 :43=82:55 84 :69=84:26
AugPro-Mix 80:62=80:44 83:24=82:60 84:53=83:92
AugPro-FGSM 80:74=80:32 83:16=82:44 84:37=83:61
Table 5: AugPro-Mix and AugPro-FGSM are
used to Ô¨Åne-tune student models. MixUp la-
bels are used for AugPro-Mix data. AugPro-
FGSM uses the original label.KD MNLI-MM/M SST-2
+AugPro-FGSM 87:02=86:85 94:61
+AugPro-FGSMD 87:08=86:89 94:27
+AugPro-FGSMR 86:57=86:52 94:27
+MixUp+AugPro-FGSM 87:35=87:41 94:50
+MixUp+AugPro-FGSMD 87:45=87:44 94:04
+MixUp+AugPro-FGSMR 87:48=87:49 94:15
+AugPro-Mix+AugPro-FGSM 87:97=87:87 95 :18
+AugPro-Mix+AugPro-FGSMD 87:81=87:67 94:61
+AugPro-Mix+AugPro-FGSMR 87:77=87:62 94:95
Table 6: KD+AugPro-FGSM and its vari-
ants performance with different signs. AugPro-
FGSMD denotes FGSM with Decent Pro-
jection. AugPro-FGSMD uses the opposite
sign to AugPro-FGSM . AugPro-FGSMR de-
notes FGSM with Random Projection. AugPro-
FGSMR uses the random sign.
0 20 40 60 80 100
Data Size / %909192939495Accuracy / %
AugPro-FGSM
FGSM
KDAugPro-Mix
MixUpAugPro-Mix + AugPro-FGSM
MixUp + FGSM
(a) SST-2
0 20 40 60 80 100
Data Size / %808182838485868788Accuracy / %
AugPro-FGSM
FGSM
KDAugPro-Mix
MixUpAugPro-Mix + AugPro-FGSM
MixUp + FGSM (b) MNLI-M
Figure 3: AugPro performance with different data sizes. Figure (a) and Figure (b) are for SST-2
dataset and MNLI-M dataset. Blue lines (or triangle markers) are AugPro methods. Yellow lines (or
diamond markers) are baseline methods. The green line (or X marker) is KD. AugPro has the same
line type as the corresponding baseline. For example, AugPro-FGSM and FGSM are all dashed lines.
improve the accuracy of 4% (SST-2) and 6% (MNLI-M) when the data size is 10%. We also report
results on the MNLI-MM dataset in Appendix F.
6 C ONCLUSIONS AND FUTURE WORK
We propose AugPro, an effective and efÔ¨Åcient data augmentation paradigm for knowledge distillation.
We use projections to tackle the problem of shifting decision boundaries caused by traditional repre-
sentation interpolation methods in knowledge distillation. Moreover, AugPro has low computation
costs and is fast in modern computing architectures. Results on GLUE tasks prove the effectiveness
and efÔ¨Åciency of AugPro. In the future, we will further explore the impact of AugPro on labels to
make it helpful in other scenarios.
9

--- PAGE 10 ---
Published as a conference paper at ICLR 2023
REFERENCES
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Ben-
jamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and
Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell,
M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems , volume 33,
pp. 1877‚Äì1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.
cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .
Jiaao Chen, Zichao Yang, and Diyi Yang. MixText: Linguistically-informed interpolation of hid-
den space for semi-supervised text classiÔ¨Åcation. In Proceedings of the 58th Annual Meeting
of the Association for Computational Linguistics , pp. 2147‚Äì2157, Online, July 2020a. Asso-
ciation for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.194. URL https:
//aclanthology.org/2020.acl-main.194 .
Jiaao Chen, Zichao Yang, and Diyi Yang. Mixtext: Linguistically-informed interpolation of hidden
space for semi-supervised text classiÔ¨Åcation. arXiv preprint arXiv:2004.12239 , 2020b.
Felix Chern, Blake Hechtman, Andy Davis, Ruiqi Guo, David Majnemer, and Sanjiv Kumar. Tpu-knn:
K nearest neighbor search at peak Ô¨Çop/s. arXiv preprint arXiv:2206.14286 , 2022.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
David A Freedman. On tail probabilities for martingales. the Annals of Probability , pp. 100‚Äì118,
1975.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572 , 2014.
Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531 , 2(7), 2015.
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351 ,
2019.
Amit Jindal, Arijit Ghosh Chowdhury, Aniket Didolkar, Di Jin, Ramit Sawhney, and Rajiv Ratn
Shah. Augmenting NLP models using latent feature interpolations. In Proceedings of the 28th
International Conference on Computational Linguistics , pp. 6931‚Äì6936, Barcelona, Spain (Online),
December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.
coling-main.611. URL https://aclanthology.org/2020.coling-main.611 .
Nora Kassner and Hinrich Sch ¬®utze. Bert-knn: Adding a knn search component to pretrained language
models for better qa. In Findings of the Association for Computational Linguistics: EMNLP 2020 ,
pp. 3424‚Äì3430, 2020.
Sosuke Kobayashi. Contextual augmentation: Data augmentation by words with paradigmatic
relations. In Proceedings of the 2018 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers) , pp.
452‚Äì457, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi:
10.18653/v1/N18-2072. URL https://aclanthology.org/N18-2072 .
Kevin J Liang, Weituo Hao, Dinghan Shen, Yufan Zhou, Weizhu Chen, Changyou Chen, and
Lawrence Carin. Mixkd: Towards efÔ¨Åcient distillation of large-scale language models. In Interna-
tional Conference on Learning Representations , 2020.
Frederick Liu, Siamak Shakeri, Hongkun Yu, and Jing Li. Enct5: Fine-tuning t5 encoder for
non-autoregressive tasks. arXiv preprint arXiv:2110.08426 , 2021.
10

--- PAGE 11 ---
Published as a conference paper at ICLR 2023
Yaojie Lu, Hongyu Lin, Jin Xu, Xianpei Han, Jialong Tang, Annan Li, Le Sun, Meng Liao, and
Shaoyi Chen. Text2event: Controllable sequence-to-structure generation for end-to-end event
extraction. arXiv preprint arXiv:2106.09232 , 2021.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 ,
2017.
Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee,
and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018
Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Papers) , pp. 2227‚Äì2237, 2018.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a uniÔ¨Åed text-to-text
transformer. J. Mach. Learn. Res. , 21(140):1‚Äì67, 2020.
Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John C Duchi, and Percy Liang. Adversarial
training can hurt generalization. arXiv preprint arXiv:1906.06032 , 2019.
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of
bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 , 2019.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.
InProceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , pp.
1631‚Äì1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics.
URLhttps://www.aclweb.org/anthology/D13-1170 .
Lichao Sun, Congying Xia, Wenpeng Yin, Tingting Liang, Philip S Yu, and Lifang He. Mixup-
transformer: dynamic data augmentation for nlp tasks. arXiv preprint arXiv:2010.02394 , 2020a.
Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for bert model
compression. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language
Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-
IJCNLP) , pp. 4323‚Äì4332, 2019.
Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. Mobilebert: a
compact task-agnostic bert for resource-limited devices. arXiv preprint arXiv:2004.02984 , 2020b.
Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-
speciÔ¨Åc knowledge from bert into simple neural networks. arXiv preprint arXiv:1903.12136 ,
2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing
systems , 30, 2017.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE:
A multi-task benchmark and analysis platform for natural language understanding. In Proceedings
of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for
NLP, pp. 353‚Äì355, Brussels, Belgium, November 2018. Association for Computational Linguistics.
doi: 10.18653/v1/W18-5446. URL https://aclanthology.org/W18-5446 .
William Yang Wang and Diyi Yang. That‚Äôs so annoying!!!: A lexical and frame-semantic embedding
based data augmentation approach to automatic categorization of annoying behaviors using#
petpeeve tweets. In Proceedings of the 2015 conference on empirical methods in natural language
processing , pp. 2557‚Äì2563, 2015.
Zihan Wang and Bo Yang. Attention-based bidirectional long short-term memory networks for
relation classiÔ¨Åcation using knowledge distillation from bert. In 2020 IEEE Intl Conf on De-
pendable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Com-
puting, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Tech-
nology Congress (DASC/PiCom/CBDCom/CyberSciTech) , pp. 562‚Äì568, 2020. doi: 10.1109/
DASC-PICom-CBDCom-CyberSciTech49142.2020.00100.
11

--- PAGE 12 ---
Published as a conference paper at ICLR 2023
Jason Wei and Kai Zou. Eda: Easy data augmentation techniques for boosting performance on text
classiÔ¨Åcation tasks. arXiv preprint arXiv:1901.11196 , 2019.
Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee, and Woomyeong Park. Gpt3mix:
Leveraging large-scale language models for text augmentation. arXiv preprint arXiv:2104.08826 ,
2021.
Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi,
and Quoc V Le. Qanet: Combining local convolution with global self-attention for reading
comprehension. arXiv preprint arXiv:1804.09541 , 2018.
Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.
Cutmix: Regularization strategy to train strong classiÔ¨Åers with localizable features. In Proceedings
of the IEEE/CVF international conference on computer vision , pp. 6023‚Äì6032, 2019.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. arXiv preprint arXiv:1710.09412 , 2017.
Le Zhang, Zichao Yang, and Diyi Yang. TreeMix: Compositional constituency-based data aug-
mentation for natural language understanding. In Proceedings of the 2022 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies , pp. 5243‚Äì5258, Seattle, United States, July 2022a. Association for Computational
Linguistics. doi: 10.18653/v1/2022.naacl-main.385. URL https://aclanthology.org/
2022.naacl-main.385 .
Minjia Zhang, Niranjan Uma Naresh, and Yuxiong He. Adversarial data augmentation for task-
speciÔ¨Åc knowledge distillation of pre-trained transformers. In Proceedings of the AAAI Conference
on ArtiÔ¨Åcial Intelligence , volume 36, pp. 11685‚Äì11693, 2022b.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher
Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language
models. arXiv preprint arXiv:2205.01068 , 2022c.
Jing Zhou, Yanan Zheng, Jie Tang, Jian Li, and Zhilin Yang. Flipda: Effective and robust data
augmentation for few-shot learning. arXiv preprint arXiv:2108.06332 , 2021.
12

--- PAGE 13 ---
Published as a conference paper at ICLR 2023
A T HE CONCEPT FIGURE OF AUGPRO
Here we show a concept Ô¨Ågure (Figure 4) to let readers better understand the difference between
AugPro (e.g., AugPro-Mix) and previous works (e.g., MixUp (Zhang et al., 2017)).
Input1Input2Teacher EmbeddingMixUpTeacher ModelMixUp InputStudent EmbeddingStudent ModelMixUp InputMixUpKnowledge Distillation
Input1Input2Teacher EmbeddingMixUpTeacher ModelMixUp InputStudent ModelKnowledge Distillation
AugPro-Mix InputProjection
Figure 4: Left: MixUp with knowledge distillation. Right : AugPro-Mix with knowledge distillation.
B C ONCRETE NUMBERS OF THE EXAMPLE IN SECTION 3
X=fx1= (2:5;2);x2= (2; 2);x3= ( 2:5; 2);x4= ( 2;2)g
Y=fy1= +1;y2= +1;y3= 1;y4= 1g
‚Ä¢= (4=9; 1=18);b= 0:
‚Ä¢xMixUp =13
25x3+12
25x1;MixUp = (250=533;200=533);bMixUp = 12=13
‚Ä¢MixUp P= (4=9;0);bMixUp P= 1=9
C I MPLEMENTATION DETAILS
C.1 H YPERPARAMETERS
We use JAX and T5X to implement EncT5 and AugPro, and use T5 1.1 checkpoints to initialize
models. The batch size is 512, and the maximum sentence length is 128. Training steps are 0.6M for
all experiments. We use 8 TPU v3 slices to do all experiments.
To Ô¨Åne-tune the teacher model, we use a dropout rate of 0.1 and a learning rate of 1e-3 for all GLUE
tasks.
For knowledge distillation, we set the dropout rate to be 0.1 for both the teacher and student models.
We Ô¨Ånd that adding dropout to teachers will make the distillation better. We run all experiments with
1e-3 and 1e-5 learning rates and report the best results. As a result, learning rate is set to 1e-5 for all
experiments on the STSB dataset, EncT5 8-FTandEncT5 8-KD experiments on CoLA, MRPC and
RTE datasets. All other experiments use the learning rate 1e-3.
The(Equation (1)) for AugPro-Mix is 0.5. Previous works (Zhang et al., 2017; Liang et al., 2020)
use a beta distribution to sample for MixUp. We try Beta (0:4;0:4)and= 0:5for MixUp
and Ô¨Ånd they have similar performance in most tasks. In some tasks such as CoLA, = 0:5is better.
Therefore, we use = 0:5for MixUp.
Following previous works (Zhou et al., 2021), we set k= 15 for the KNN baseline and randomly
select 0:1portion of tokens to replace. We use outputs of the 4th layer of the student model and the
12th layer of the teacher model, i.e., the middle layer of both models, to conduct TMix experiments.
13

--- PAGE 14 ---
Published as a conference paper at ICLR 2023
C.2 I MPLEMENTATION DETAIL OF EQUATION 5
Algorithm 1 shows the AugPro pipeline. Here, we show a detailed implementation of Equation 5 in
Algorithm 2. We believe these two algorithms can help readers reproduce our methods and results.
Algorithm 2: Projection Algorithm
Input: Augmented representations Brep2RBLH, whereBis the batch size, Lis the sentence length,
andHis the embedding dimension. V ocabulary VcontainsZtokens. Token embeddings
E2RZH.
Output: Projected augmented data B02VBL
‚Ä¢ Compute the similarity between each token pair Sims =BrepET2RBLZ
‚Ä¢ Find the nearest neighbor B0
index = arg min
axis=2Sims2NBL
‚Ä¢ Find tokens according to indices to get B0
‚Ä¢returnB0
D E XAMPLES OF AUGPRO-MIX AND AUGPRO-FGSM
We use three MNLI inputs to show the augmented data by AugPro-Mix and AugPro-FGSM.
Following are three MNLI inputs:
(i)hypothesis : The judgments need to consider the broader
public interest.
premise: These judgments need to be made in a consistent
manner with consideration of the broader public interest in
the program or activity under review.
(ii)hypothesis : I agree.
premise: yeah i i agree with that
(iii)hypothesis : She‚Äôs never been to a hospital.
premise: and uh as if that wasn‚Äôt bad enough the the ones
that were half alive that they rushed to the hospital and
she got to work on and she got to see them die and uh and
they just get all the world‚Äôs worst situations very few
rewarding situations uh and
If we use AugPro-Mix on (i) and (ii), (ii) and (iii), we will get:
(i + ii) hypothesis : I judgment.
premise: yeah judgmenti needi agree made in
(ii + iii) hypothesis : She agree. never been to a hospital.
premise: yeah uh as agreeif that wasn‚Äôt bad enough the the
ones that were half alive that they rushed to the hospital
and she got to work on and she got to see them die and uh
and they just get all the world‚Äôs worst situations very few
rewarding situations uh an
If we use AugPro-FGSM on (i), (ii) and (iii), we will get:
(i)hypothesis : River judgment Mit handy to consider the
broader public interest.
premise: These judgment Mit handy to be made in a
consistent manner with consideration- the broader public
interest in the federal cl Àòadire activity under reviewed.
(ii)hypothesis : Kyle agree.
premise: yeah chambres chambres agree with that
14

--- PAGE 15 ---
Published as a conference paper at ICLR 2023
SST-2 CoLA MNLI-MM/M QNLI QQP MRPC STS-B RTE
Acc Matthew Acc Acc Acc/F1 Acc/F1 PC/SC Acc
67.3k 8.5k 392.7k 104.7k 363.8k 3.7k 5.7k 2.5k
EncT5 24-FT (354M) 97:20 63:60 91:40=91:10 95:40 92:73=90:00 91:42=93:30 88:19=88:00 86:30
EncT5 8-FT (37M) 92:89 45:84 84:69=84:26 89:84 91:45=88:41 87:01=90:91 86:39=85:94 59:21
EncT5 8-KD 92:09 45:56 85:93=85:61 89:46 91:36=88:24 84:56=88:85 87:29=87:18 61:37
+Random 92:78 45:89 86:37=85:55 89:35 91:42=88:43 84:07=88:93 87:72=87:57 61:37
+MixUp 93:23 51:63 86:73=86:69 91:31 91:82=88:97 88:48=91:68 87:47=87:33 62:82
+AugPro-Mix 94:38 57:60 87:40=87:27 92:06 92:06=89:23 89:46=92:34 88:10=87:87 71:84
Table 7: Results of the random generation baseline.
(iii)hypothesis : She‚Äô Mit never been to a hospital.
premise: and uh as if that wasn‚Äô financing bad enough
the the ones that are half alive that they rushed to the
hospital and she got to work on and she got to see them die
and uh and they just get all the world‚Äô Mit worst situations
very few rewarding situations uh and
The above-augmented sentences are originally token lists, but not real sentences. Luckily, T5 uses Sen-
tencePiece to construct its vocabulary, which supports the precise de-tokenization for any token list. To
convert token lists to sentences, we use a simple command "".join(tokens).replace(" ","
").
Linguistic Analysis. Our motivation focuses on the perspective of machine learning, i.e., avoiding
shifting decision boundaries by converting representations to tokens. Here, we would like to add
a brief analysis from a linguistic perspective. We can observe that the above-augmented sentences
may have grammatical errors, ‚Äùmeaningless‚Äù tokens, and may be less meaningful than original
sentences. However, ‚Äùmeaningless‚Äù to humans does not suggest meaningless to models, as AugPro
indeed boosts the distillation performance. Besides, augmented sentences are not totally semantically
meaningless to humans. It is hard to see why augmented data is so helpful from the linguistic
perspective, suggesting that we should focus on analyzing these data from the machine learning
perspective, which is exactly our motivation. To further support our motivation, we conduct a simple
baseline ‚Äùrandom generation‚Äù that randomly chooses tokens from the vocabulary and concatenates
them to form an augmented sentence. Random generation can generate meaningless sentences easily.
Results are shown in Table 7. We can conclude that random generation is a poor augmentation
method, suggesting that AugPro is helpful not because of the meaningful or meaningless semantics,
but because of avoiding shifting decision boundaries.
E S IGNS OF GRADIENTS IN AUGPRO-FGSM IS NOT IMPORTANT (MORE
RESULTS )
Table 8 shows the effect of signs of gradients in AugPro-FGSM with the setting of Table 2. The
conclusion is same as the conslusion concluded from Table 6.
F A UGPRO CONSISTENTLY BENEFITS KD WITH DIFFERENT DATA SIZES
(MORE RESULTS )
The main texts show that AugPro consistently beneÔ¨Åts KD with different data sizes on SST-2 and
MNLI-M datasets. Figure 5 shows this conclusion still holds on the MNLI-MM dataset. The overall
trend is similar to the MNLI-M dataset.
G KNN IS SLOWER THAN OTHER METHODS
There are two reasons that KNN is slower than other methods. First, KNN has a higher computational
complexityO(NVdlogk )than AugPro (O(NVd )). Second, KNN is hard to be implemented by
XLA. A popular and fast implementation of KNN is to use np.argpartition . However, XLA
15

--- PAGE 16 ---
Published as a conference paper at ICLR 2023
MNLI-MM/M SST-2 QNLI
KD+AugPro-FGSM 85:87=85:82 94 :27 91:03
KD+AugPro-FGSMD 85:87=85:52 93:9291:05
KD+AugPro-FGSMR 85:6=85:39 93:35 90:85
KD+MixUp+AugPro-FGSM 86:20=86:28 93:81 90:99
KD+MixUp+AugPro-FGSMD 86:44=86:19 93:92 90:98
KD+MixUp+AugPro-FGSMR 86:38=86:42 94 :04 91 :34
KD+AugPro-Mix+AugPro-FGSM 86:76=86:81 94 :04 91 :58
KD+AugPro-Mix+AugPro-FGSMD 86:59=86:57 93:81 91:56
KD+AugPro-Mix+AugPro-FGSMR 86:62=86:48 93:69 91:4
Table 8: KD+AugPro-FGSM and its variants performance with different signs. AugPro-FGSMD de-
notes FGSM with Decent Projection. AugPro-FGSMD uses the opposite sign to AugPro-FGSM .
AugPro-FGSMR denotes FGSM with Random Projection. AugPro-FGSMR uses the random sign.
This table follows the setting in Table 2.
0 20 40 60 80 100
Data Size / %808182838485868788Accuracy / %
AugPro-FGSM
FGSM
KDAugPro-Mix
MixUpAugPro-Mix + AugPro-FGSM
MixUp + FGSM
Figure 5: AugPro performance with different data sizes on MNLI-MM dataset. Blue lines (or triangle
markers) are AugPro methods. Yellow lines (or diamond markers) are baseline methods. The green
line (or X marker) is KD. AugPro has the same line type as the corresponding baseline. For example,
AugPro-FGSM and FGSM are all dashed lines.
16

--- PAGE 17 ---
Published as a conference paper at ICLR 2023
does not support partition operations2, making KNN hard to be implemented with JAX on TPUs.
To this end, Chern et al. (2022) propose another method to run KNN on TPUs at peak FLOP/s. We
use their method to implement KNN, but such implementation may not be the optimal solution for
KNN.
H A UGPRO CAN ACHIEVE LESS ERROR RATES
SupposeX=f 1;1g2 lognis the universe of all data with labels Yto learn. DeÔ¨Åne a distribution P
such that (x;y)P ifxis uniformly independently drawn from Xandyis uiformly independently
drawn fromf 1;+1g.XDis the training data compositing of nsamples uniformly and independently
drawn fromX.
Supposenis even. Let x(j)be thej-th coordinate of vector x. We constructXAugPro -Mixas
follows: Index the elements in XDarbitrarily, and xidenotes the i-th element. For each positive
integeri2[n=2], we construct an augmented data zisuch that, for each coordinate j2[2 logn],
zi(j) =x2i 1(j)ifx2i 1(j) =x2i(j), otherwisezi(j)is uniformly chosen from f1; 1g.
Let‚Äôs consider optimal algorithms AandAAugPro -Mix.Acan only observeXD(orXMixUp ,XFGSM )
and their labelsYD(orYMixUp ,YFGSM ). For proof, we assume Acan only observeXDand their
labelsYD. The other two can be proved similarly. AAugPro -Mixcan observeXAugPro -Mixand their
corresponding labels YAugPro -Mix.
DeÔ¨Åne the generalization error error(A) := Pr (x;y)P;(XD;YD)[A(x;XD;YD)6=y], where
A(x;XD;YD)2 f+1; 1gis the prediction of xoutputted byA, and error(AAugPro -Mix) =
Pr(x;y)P;(XAugPro -Mix;YAugPro -Mix)[A(x;XAugPro -Mix;YAugPro -Mix)6=y]. We have the following
claim:
Claim 1. One has
lim
n!1error(A) error(AAugPro -Mix))
1=(4n)= 1:
As for constructing XAugPro -FGSM , for each xi2 XD, we add Gaussian
N(0;4)to each coordinate of xi, and then project back to Xto getwi. Simi-
larly, assumeAAugPro -FGSM can observe the dataset XAugPro -FGSM and its labels
YAugPro -FGSM , and deÔ¨Åne the generalization error to be error(AAugPro -FGSM ) =
Pr(x;y)P;(XAugPro -FGSM;YAugPro -FGSM )[A(x;XAugPro -FGSM;YAugPro -FGSM )6=y].
Claim 2. One has
lim
n!1error(A) error(AAugPro -FGSM )
1=(2n)= 1:
H.1 P ROOF OF CLAIM 1
We provide the following preliminary background of martingale to prove Claim 1 for completeness.
DeÔ¨Ånition 1 (Martingale) .A sequence of random variables Y1;Y2;is a martingale with respect
to another sequence X1;X2;if for alln,E[jYnj]<1andE[Yn+1jX1;;Xn] =Yn.
DeÔ¨Ånition 2 (Martingale Difference) .fDkg1
k=1is a martingale difference sequence w.r.t. fXkg1
k=1
if for alln:
‚Ä¢Dnis a measurable function of X1;;Xn
‚Ä¢E[jDnj]<1
‚Ä¢E[Dn+1jX1;;Xn] = 0 .
IfYkis a martingale, then Dk=Yk Yk 1is a martingale difference. For martingale difference, we
have the following theorem:
2https://github.com/google/jax/issues/10541
17

--- PAGE 18 ---
Published as a conference paper at ICLR 2023
Proposition 1 (Freedman‚Äôs Inequality, Freedman (1975)) .Consider a real-valued martingale dif-
ference sequencefXtgwhich is uniformly bounded, i.e. jXtjMalmost surely for all t. DeÔ¨Åne
the predictable quadratic variation process of the martingale Wt:=Pt
j=1E[X2
jjFj 1]for allt,
wherefFtgis the Ô¨Åltration. Then for all `0and2>0,
Pr"
9k0 :jkX
t=1Xtj`&Wk2#
2 exp `2=2
2+M`=3
:
With these tools, we are ready to prove the claim.
Proof of Claim 1. In our setting, it is evident that error(A) = PrxP[x =2 XD]=2 = 1=2 
E[jXDj]=2n2, where the expectation is taken over the randomness of XD, andjjdenotes the
cardinality after removing duplicate elements.
Similarly, we have error(AAugPro -Mix) = 1=2 E[jXAugPro -Mixj]=2n2. It sufÔ¨Åces to prove
lim
n!1E[jXAugPro -Mixj jXDj]
n=2= 1:
First, we show for some constant c1>0,
Pr
jXDjn c1p
nlogn
1 1=poly(n): (8)
It is equivalent to drawing the elements in XDone by one. LetXi
Ddenote the elements after drawing
xi. LetYi=jXi
Dj, andDibe the indicator that xi6=xjfor all 0<j <i , whereD1= 1. Then we
knowjXDj=Yn,Di=Yi Yi 1, and for anyXi 1
D,
Pr[Di= 1jXi 1
D] = 1 jXi 1
Dj
jXj1 1
n:
LetFibe the Ô¨Åltration, and hence E[DijFi 1]1 1
n. Let ~Di:=Di E[DijFi 1]be
a martingale difference sequence. Hence j~Dij1almost surely, and E[~D2
ijFi 1]1. Let
Wi=Pi
j=1E[~D2
jjFj 1]. By Freedman‚Äôs Inequality, one has
Pr[Yn<n c1p
nlogn]
= Pr[nX
i=1Di<n c1p
nlogn]
Pr[jnX
i=1~Dijc1p
nlogn]
= Pr[jnX
i=1~Dijc1p
nlogn^Wnn]
2 exp( c2
1nlogn
2n+ 2c1pnlogn=3):
Choosingc1large enough proves Equation (8).
Similarly we show for some constant c2>0,
Pr
jXAugPro -Mixj3n=2 3c2p
nlogn
1 1=poly(n): (9)
It is equivalent to constructing XAugPro -Mixby iterations. For i-th iteration, we draw x2i 1andx2i
i.i.d. uniformly and construct zias described before. Let Xi
AugPro -Mixdenote the data we get after
constructing x2i 1;x2iandzi. LetY0
i=jXi
AugPro -MixjandD0
ibe the indicator that Y0
i Y0
i 1= 3
(i.e. all of the three data are distinct and Ô¨Årst constructed). We know Y0
n=23Pn=2
i=1D0
i. For any
18

--- PAGE 19 ---
Published as a conference paper at ICLR 2023
Ô¨Åxed vector x2f  1;1g2 logn, we know Pr[x2i 1=x] = Pr[x2i=x] = Pr[zi=x] = 1=n2.
Hence for anyXi 1
AugPro -Mixand by union bound, one has
E[D0
i= 1jXi 1
AugPro -Mix]1 3(jXi 1
AugPro -Mixj+ 3)
jXj1 9
2n:
LetF0
ibe the Ô¨Åltration and hence E[D0
ijF0
i 1]1 9
2n. Let ~D0
i:=D0
i E[D0
ijFi 1]be a
martingale difference sequence. Similarly j~D0
ij1almost surely and E[~D02
ijFi 1]1. Let
W0
i=Pi
j=1E[~D02
jjF0
j 1]. By Freedman‚Äôs Inequality, one has
Pr[Y0
n=2<3n=2 3c2p
nlogn]
Pr[n=2X
i=1D0
i<n= 2 c2p
nlogn]
Pr[jn=2X
i=1~D0
ijc2p
nlogn O(1)]
= Pr[jn=2X
i=1~D0
ijc2p
nlogn O(1)^W0
n=2n=2]
2 exp( c02
2nlogn
n+ 2c0
2pnlogn=3);
wherec0
2=c2 O(1). Choosingc2large enough completes the proof of Equation (9).
Combining equations (8) and (9) proves the statement.
H.2 P ROOF OF CLAIM 2
Proof. Similarly, we have error(AAugPro -FGSM ) = 1=2 E[jXAugPro -FGSMj]=2n2and it sufÔ¨Åces
to prove
lim
n!1E[jXAugPro -FGSMj E[jXDj]]
n= 1:
We already have Equation (8). It sufÔ¨Åces to prove that for some constant c3>0,
Pr
jXAugPro -FGSMj2n c3p
nlogn
1 1=poly(n): (10)
For eachxi2X AugPro -FGSM and each coordinate j2[2 logn], we know Pr[xi(j) =wi(j)] =
(1=2)<0:7, where is cumulative distribution function (CDF) of one-dimensional standard
Gaussian distribution, i.e. (t) = PrxN(0;1)[xt].
It is equivalent to constructing XAugPro -FGSM iteration by iteration, where in i-th iteration, we draw
xiand construct wias described before. Let Xi
AugPro -FGSM be the data we get after constructing xi
andwi, letYi=jXi
AugPro -FGSMjand letDibe the indicator that Yi Yi 1= 2. For any Ô¨Åxed vector
x2f  1;1g2 logn, we know Pr[xi=x] = 1=n2= Pr[wi=x]andPr[wi=xi]<0:72 logn<
1=n1:02. LetFibe the Ô¨Åltration, and for any Xi 1
AugPro -FGSM , by union bound, one has
E[Di= 1jX 1
AugPro -FGSM ]1 2jXi 1
AugPro -FGSMj
jXj 1=n1:021 3=n:
19

--- PAGE 20 ---
Published as a conference paper at ICLR 2023
Let~Di:=Di E[Dij Fi 1]be a martingale difference sequence. We know j~Dij  1 and
E[~DijFi 1]1almost surely. Let Wi=Pi
j=1E[~D2
ijFj 1]. By Freedman‚Äôs Inequality, one has
Pr[Yn<2n 2c3p
nlogn]
Pr[nX
i=1Di<n c3p
nlogn]
Pr[jnX
i=1~Dij<c0
3p
nlogn^Wnn]
2 exp( c02
3nlogn
2n+ 2c0
3pnlogn=3);
wherec0
3=c3 O(1). Choosingc3large enough completes the proof.
Combing Equation (8) and (10) completes the proof.
20
