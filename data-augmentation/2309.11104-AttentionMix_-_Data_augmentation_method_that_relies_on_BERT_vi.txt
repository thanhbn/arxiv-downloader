# AttentionMix: Phương pháp tăng cường dữ liệu dựa trên cơ chế attention của BERT

Dominik Lewy, Jacek Mańdziuk
Khoa Toán học và Khoa học Thông tin,
Đại học Công nghệ Warsaw,
dominik.lewy@gmail.com, jacek.mandziuk@pw.edu.pl

## Tóm tắt

Phương pháp Mixup đã chứng minh là một kỹ thuật tăng cường dữ liệu mạnh mẽ trong Thị giác Máy tính, với nhiều phương pháp kế thừa thực hiện việc trộn hình ảnh một cách có hướng dẫn. Một trong những hướng nghiên cứu thú vị là chuyển ý tưởng Mixup cơ bản sang các lĩnh vực khác, ví dụ như Xử lý Ngôn ngữ Tự nhiên (NLP). Mặc dù đã có một số phương pháp áp dụng Mixup cho dữ liệu văn bản, vẫn còn chỗ cho các phương pháp mới và cải tiến. Trong nghiên cứu này, chúng tôi giới thiệu AttentionMix, một phương pháp trộn mới dựa trên thông tin attention. Trong khi bài báo tập trung vào cơ chế attention của BERT, phương pháp được đề xuất có thể áp dụng cho bất kỳ mô hình nào dựa trên attention. AttentionMix được đánh giá trên 3 bộ dữ liệu phân loại cảm xúc tiêu chuẩn và trong cả ba trường hợp đều vượt trội hơn hai phương pháp chuẩn sử dụng cơ chế Mixup, cũng như phương pháp BERT gốc. Kết quả xác nhận rằng thông tin dựa trên attention có thể được sử dụng hiệu quả cho việc tăng cường dữ liệu trong lĩnh vực NLP.

## 1 Giới thiệu

Trong những năm gần đây, việc giới thiệu kiến trúc transformer [18] đã thống trị nhiều tác vụ học thuật (ví dụ [22]) và ứng dụng thương mại trong lĩnh vực Xử lý Ngôn ngữ Tự nhiên (NLP), với kiến trúc Bidirectional Encoder Representations from Transformers (BERT) [3] có vị trí đặc biệt trong thể loại này.

Song song với đó, một nhóm thú vị các phương pháp tăng cường dữ liệu dựa trên việc trộn hình ảnh đã được giới thiệu và nghiên cứu sâu trong lĩnh vực Thị giác Máy tính (CV). Phương pháp điển hình trong nhóm này là Mixup [25]. Phương pháp này trộn hình ảnh và các nhãn được mã hóa one-hot tương ứng một cách tuyến tính trong quá trình huấn luyện để tạo ra các quan sát tổng hợp. Phương pháp này có nhiều phương pháp kế thừa đã điều chỉnh nó cho các cài đặt cụ thể [6, 26], giải quyết các lỗ hổng cụ thể [19] hoặc làm cho quá trình trộn hiệu quả hơn bằng cách hướng dẫn nhất định [6,17,21].

Khái niệm trộn các mẫu huấn luyện cũng có thể áp dụng cho lĩnh vực NLP trong phân loại câu [5, 7]. Tuy nhiên, trong lĩnh vực này nó chưa được nghiên cứu kỹ lưỡng, chỉ có một số phương pháp cố gắng áp dụng phương pháp Mixup gốc.

Trong nghiên cứu này, chúng tôi đề xuất một mở rộng của phương pháp Mixup cho phân loại câu sử dụng thông tin về attention [18] từ mô hình BERT để cải thiện quy trình trộn câu. Chúng tôi chỉ ra rằng phương pháp được đề xuất vượt trội hơn phương pháp BERT cơ bản và hai phương pháp áp dụng Mixup gốc. Ngoài ra, chúng tôi thực hiện nghiên cứu loại bỏ để giải thích tại sao một số lớp attention nhất định tạo ra thông tin có ý nghĩa hơn, từ quan điểm của quá trình huấn luyện, dẫn đến độ chính xác cao hơn cho một tác vụ nhất định.

### 1.1 Động lực

Các kỹ thuật tăng cường dữ liệu khác nhau đáng kể giữa các phương thức, ví dụ CV và NLP. Trong CV, có rất nhiều cơ chế tăng cường dữ liệu tương đối đơn giản như xoay, cắt, lật, thay đổi tỷ lệ, v.v. Phần lớn chúng đều cụ thể theo phương thức và không áp dụng được cho văn bản. Trong NLP, số lượng cơ chế tăng cường tương đối đơn giản bị hạn chế chỉ còn một vài, chẳng hạn như thay thế từ đồng nghĩa hoặc chèn/xóa/hoán đổi ngẫu nhiên [23].

Động lực chính của nghiên cứu này là mở rộng danh mục các phương pháp tăng cường tương đối đơn giản trong lĩnh vực NLP bằng cách cho phép sử dụng các phương pháp tăng cường trộn đã chứng minh rất hiệu quả trong CV. Để đạt được điều này, chúng tôi nâng cao phương pháp Mixup bằng cách thêm cơ chế dựa trên attention cụ thể cho văn bản.

### 1.2 Đóng góp

Đóng góp chính của nghiên cứu này là đề xuất một phương pháp tăng cường mới cho phân loại câu (AttentionMix) dựa trên phương pháp Mixup, sử dụng trọng số attention để hướng dẫn quá trình trộn. Mặc dù bài báo tập trung vào việc sử dụng attention trong BERT [3], cách tiếp cận tương tự có thể áp dụng cho bất kỳ kiến trúc nào khác triển khai cơ chế attention [18]. AttentionMix được kiểm tra thực nghiệm trên 3 bộ dữ liệu phân loại câu. Ngoài ra, thông tin được lưu trữ trong các lớp attention hoạt động tốt nhất được phân tích để làm sáng tỏ lý do về hiệu suất mạnh mẽ của chúng.

Trong khi trong tài liệu đã có một số phương pháp tăng cường có hướng dẫn được chuyển từ lĩnh vực CV sang lĩnh vực NLP (được tóm tắt trong Mục 2.1), bài báo này là đầu tiên đề xuất phương pháp tăng cường văn bản có hướng dẫn bắt nguồn từ cơ chế cụ thể của NLP, tức là attention.

## 2 Nghiên cứu liên quan

### 2.1 Tăng cường dữ liệu giống Mixup

Phương pháp điển hình bắt đầu nghiên cứu về trộn hình ảnh như một hình thức tăng cường dữ liệu là Mixup [25], thực hiện nội suy tuyến tính hai hình ảnh và các nhãn tương ứng của chúng. Có các phương pháp khác thực hiện trộn theo cách có hướng dẫn, tức là bằng cách xác định các phần liên quan nhất của hình ảnh, ví dụ [21] sử dụng thông tin từ bộ phân loại CNN, [17] áp dụng phương pháp thống kê, hoặc [6] trong quá trình trộn, sử dụng gradient của mạng nơ-ron. Một bài khảo sát về các kỹ thuật tăng cường dữ liệu dựa trên trộn cho phân loại hình ảnh được trình bày trong bài báo khảo sát gần đây [9].

Ý tưởng trên, bắt nguồn từ lĩnh vực CV, cũng chứng minh hữu ích trong phân loại văn bản. [5] sử dụng ý tưởng Mixup để thực hiện tăng cường embedding từ và embedding câu trong quá trình huấn luyện mạng CNN và LSTM. Các nghiên cứu tiếp theo [7,16] mở rộng nghiên cứu này bằng cách xem xét kiến trúc BERT và thử nghiệm với Manifold Mixup [19] – một biến thể của Mixup, áp dụng trộn cho các lớp ẩn của mạng. Một ví dụ khác là [24] sử dụng thông tin saliency dựa trên gradient để trộn câu gốc ở cấp độ từ. DropMix [8], cũng sử dụng thông tin saliency dựa trên gradient, nhưng thêm vào đó kết hợp trộn với cơ chế dropout để có được mẫu trộn.

Tất cả các phương pháp trên đều xác minh liệu các cơ chế đã thành công trong CV có thể áp dụng hiệu quả cho NLP hay không, mặc dù với một số điều chỉnh cụ thể theo lĩnh vực (ví dụ thay đổi câu thành embedding có thể được trộn [5, 7] hoặc tổng hợp thông tin saliency dựa trên gradient ở cấp độ từ [24]).

Ngược lại với các phương pháp trên dựa vào cải tiến có nguồn gốc từ lĩnh vực CV, phương pháp của chúng tôi là đầu tiên sử dụng hướng dẫn xuất phát từ cơ chế cụ thể của văn bản, tức là attention.

### 2.2 Phương pháp sử dụng attention

Một ví dụ thú vị về loại phương pháp này là [10] sử dụng attention từ BERT như một phương tiện để xác định các từ quan trọng trong câu hỏi liên quan đến thai kỳ cho mục đích chú thích. Phương pháp này không phân biệt các lớp/đầu attention, và đối với một từ nhất định chỉ đơn giản là tổng hợp attention từ tất cả các đầu từ tất cả các lớp, và coi các từ có tổng attention cao hơn là quan trọng hơn. Một phương pháp rất tương tự được đề xuất trong [20] để xác định từ khóa chỉ ra lời nói thù địch. Một phát hiện thú vị từ bài báo này là đối với các bộ dữ liệu khác nhau (Boomer-hate vs. Asian-hate), cơ chế attention hoạt động khác nhau (trong trường hợp trước, nó liên kết các từ thù địch với nhóm mục tiêu, và trong trường hợp sau, nó không làm như vậy), điều này gợi ý rằng khả năng áp dụng của attention có thể khác nhau giữa các bộ dữ liệu.

Cũng có khá nhiều bài báo phân tích các thuộc tính của trọng số attention. Ví dụ, [2] cố gắng hiểu attention trong bối cảnh vị trí tương đối hoặc mật độ của chúng, cũng như từ quan điểm liệu các lớp/đầu attention cụ thể có học được bất kỳ cấu trúc ngôn ngữ nào không (ví dụ đề cập đồng tham chiếu chú ý đến tiền từ của chúng). [4], mặt khác, tiếp cận việc phân tích attention bằng cách phân nhóm không giám sát các đầu attention. Họ tìm thấy 4 nhóm mẫu attention và xác định nhóm nào có tác động lớn nhất đến độ chính xác. Theo nghiên cứu của họ, việc liên kết một đầu cụ thể với một nhóm có thể thay đổi trong quá trình huấn luyện và cũng có thể khác nhau tùy thuộc vào bộ dữ liệu được phân tích.

## 3 Kiến thức nền tảng

### 3.1 Attention trong BERT

Kiến trúc BERT [3] bao gồm nhiều lớp attention, mỗi lớp chứa nhiều đầu attention. Một đầu attention nhận đầu vào là một chuỗi embedding t = [t1, ..., tn] tương ứng với n token của câu đầu vào. Những embedding này (ti) được chuyển đổi thành các vectơ truy vấn, khóa và giá trị (qi, ki, vi) bằng cách sử dụng ma trận Q, K và V được học trong quá trình huấn luyện cho mỗi đầu attention riêng biệt. Mỗi đầu tính toán trọng số attention α giữa tất cả các cặp token theo Phương trình 1.

αij = exp(qiᵀkj) / Σₗ₌₁ⁿ exp(qiᵀkl)     (1)

Điều trên đại diện cho tích vô hướng được chuẩn hóa bởi softmax giữa các vectơ truy vấn và khóa.

### 3.2 Tăng cường Mixup

Mixup [25] là một cơ chế tăng cường độc lập dữ liệu đơn giản nhưng hiệu quả được sử dụng trong CV, xây dựng các mẫu huấn luyện tổng hợp như một phép nội suy tuyến tính của các hình ảnh đầu vào. Các mẫu tổng hợp được xây dựng bằng Phương trình 2 và 3.

x̃ = λxi + (1-λ)xj     (2)
ỹ = λyi + (1-λ)yj     (3)

trong đó xi, xj là hai mẫu ngẫu nhiên từ dữ liệu huấn luyện, yi, yj là các nhãn được mã hóa one-hot của chúng, và λ là tỷ lệ trộn.

## 4 Phương pháp đề xuất

Trộn trong BERT có thể được áp dụng ở nhiều cấp độ khác nhau của mạng:

• Ở cấp độ embedding từ – Hình 1 (trái) – cùng một từ bất kể ngữ cảnh sẽ có cùng vectơ embedding.

• Ở cấp độ encoding từ – Hình 1 (giữa) – cùng một từ sẽ có embedding khác nhau tùy thuộc vào ngữ cảnh.

• Ở cấp độ embedding câu – Hình 1 (phải) – embedding diễn ra sau khi các vectơ của từng từ được tổng hợp lên cấp độ câu.

Phương pháp AttentionMix mà chúng tôi đề xuất tuân theo khái niệm triển khai đầu tiên.

### 4.1 AttentionMix

AttentionMix nhằm sử dụng thông tin đến từ các đầu attention (Phương trình 1) để hướng dẫn quá trình trộn. Vì thông tin attention chỉ có liên quan và áp dụng được ở cấp độ embedding từ và encoding từ, cấp độ embedding câu sẽ không được khám phá trong nghiên cứu này (tất cả embedding token đã được tổng hợp lên cấp độ embedding câu, tại giai đoạn này việc sử dụng attention được gán cho các token riêng lẻ là không thể).

Hơn nữa, chúng tôi tập trung vào việc tăng cường ở cấp độ embedding từ vì giả thuyết làm việc là việc sử dụng attention gần hơn với đầu vào và trước giai đoạn encoding (tức là học ngữ cảnh của mỗi token và điều chỉnh embedding của nó dựa trên điều đó) sẽ dẫn đến độ chính xác cao hơn của mô hình.

Chúng tôi khám phá các phương pháp khác nhau để sử dụng thông tin attention. Hãy xem xét L lớp attention với H đầu mỗi lớp. Sau đó, đối với mỗi đầu h ∈ H trong lớp l ∈ L và mỗi câu S, ma trận trọng số attention AWʰˡ(S) có dạng:

AWʰˡ(S) = [αij]n×n     (4)

trong đó n là số token trong một câu. αij đại diện cho tác động của token αij lên biểu diễn lớp tiếp theo của token hiện tại.

Dựa trên AWʰˡ(S), chúng tôi tính toán mức độ liên quan của mỗi token trong câu từ quan điểm của một đầu duy nhất (Phương trình 5) và trung bình từ tất cả các đầu trong một lớp duy nhất (Phương trình 6).

Bᵸᵉᵃᵈ ʰˡ = Σᵢ αij / n     (5)

Bˡᵃʸᵉʳ ˡ = ΣᴴH=1 Bᵸᵉᵃᵈ ʰˡ / H     (6)

Mức độ liên quan trên của mỗi token trong một câu được tính toán cho mỗi quan sát (câu) trong bộ dữ liệu huấn luyện. Đối với hai cặp (câu, nhãn): (x1, y1) và (x2, y2), các phương trình để tạo câu trộn như sau:

λvector = B1 / (B1 + B2)     (7)
x̃ = λvector ⊙ x1 + (1 - λvector) ⊙ x2     (8)
λlabel = Σλvector / |λvector|     (9)
ỹ = λlabel · y1 + (1 - λlabel) · y2     (10)

trong đó B1 và B2 là các vectơ liên quan, được tính toán bằng Phương trình 5 hoặc Phương trình 6, cho các quan sát (x1, y1) và (x2, y2) tương ứng. λvector là vectơ tỷ lệ trộn được sử dụng để trộn embedding token, λlabel là tỷ lệ trộn được sử dụng để trộn các nhãn được mã hóa one-hot, và |λvector| là số giá trị liên quan token. λvector đại diện cho tầm quan trọng của mỗi token riêng lẻ trong một câu và λlabel là một giá trị duy nhất (trung bình của tất cả các phần tử λvector) xác định mức độ tương đối mà mỗi trong hai vectơ nhãn được mã hóa one-hot đóng góp vào việc tính toán ỹ (Phương trình 10).

Các phương trình 5-10 mô tả chính thức thuật toán AttentionMix, theo hiểu biết của chúng tôi, trình bày nỗ lực đầu tiên để sử dụng attention đến từ BERT để hướng dẫn quá trình tăng cường Mixup.

## 5 Thiết lập thực nghiệm

### 5.1 Bộ dữ liệu

Chúng tôi đánh giá AttentionMix trên 3 bộ dữ liệu chuẩn phân loại câu, được tóm tắt trong Bảng 1.

• SST – là bộ dữ liệu Stanford Sentiment Treebank [15] với thang đo cảm xúc 5 cấp độ chi tiết. Lưu ý rằng trong tài liệu, một phiên bản nhị phân đơn giản hóa của bộ dữ liệu này cũng được xem xét. Trong các thí nghiệm, chúng tôi chọn cài đặt gốc không nhị phân với thang đo cảm xúc 5 điểm.

• MR – là bộ dữ liệu Movie Review với cảm xúc nhị phân [14].

• IMDB - Internet Movie Database là bộ dữ liệu phân loại cảm xúc nhị phân lớn hơn nhiều cho đánh giá phim [11].

### 5.2 Kiến trúc và thiết lập thực nghiệm

Tất cả các thí nghiệm được thực hiện sử dụng mạng BERT [3] với 12 lớp attention với 12 đầu mỗi lớp. Kích thước ẩn của khối transformer bằng 768. Phiên bản này thường được gọi là bert-base.

Đối với mỗi phương pháp và mỗi bộ dữ liệu, thí nghiệm được thực hiện 3 lần. Mỗi lần chạy bao gồm việc tinh chỉnh mô hình BERT được khởi tạo với trọng số bert-base-uncased (mô hình được huấn luyện trước trên tiếng Anh không phân biệt chữ thường và chữ hoa) cho một bộ dữ liệu và phương pháp nhất định, trong 100 epoch với tốc độ học 1e-6 và dropout 0.1. Tất cả các thí nghiệm được thực hiện trên GPU Nvidia A100 40GB. Trong phần còn lại của bài báo, từ huấn luyện sẽ đề cập đến việc tinh chỉnh mô hình BERT được mô tả ở trên cho một tác vụ phân loại cụ thể.

Trong mỗi thí nghiệm sử dụng tăng cường dựa trên Mixup (tức là tất cả các thí nghiệm trừ huấn luyện BERT tiêu chuẩn), việc tăng cường được áp dụng trong quá trình huấn luyện cho tất cả các mẫu huấn luyện. Về mặt kỹ thuật, AttentionMix được triển khai tương tự như Mixup, tức là mỗi batch dữ liệu được trộn với một phiên bản được xáo trộn của cùng một batch, vì vậy kích thước của dữ liệu huấn luyện không thay đổi.

Mỗi bộ dữ liệu được chia thành 3 phần: train, val và test theo Bảng 1. Kết quả trên phần test được trình bày, cho mô hình đạt độ chính xác cao nhất trên phần val. Kết quả trung bình của 3 lần chạy được báo cáo.

### 5.3 Phương pháp chuẩn và siêu tham số của chúng

Chúng tôi so sánh AttentionMix với ba baseline: (1) huấn luyện BERT tiêu chuẩn không có Mixup, được gọi là phương pháp vanilla, (2) sự thích ứng của wordMixup [5], và (3) một trường hợp đặc biệt của TMix [1], mà chúng tôi gọi là MixupEncoding. Sự khác biệt chính so với AttentionMix là cả hai phương pháp tăng cường tham chiếu giống Mixup đều không sử dụng hướng dẫn đến từ cơ chế attention. Ngoài ra, [5] sử dụng kiến trúc LSTM hoặc CNN thay vì BERT, và các embedding được sử dụng ở cấp độ từ, không phải cấp độ token. MixupEncoding so với TMix [1] thực hiện trộn sau toàn bộ bộ mã hóa BERT, không phải ở một lớp ẩn được chọn ngẫu nhiên.

Ngoài các siêu tham số được đề cập trong Mục 5.2, hai phương pháp chuẩn dựa trên Mixup sử dụng một siêu tham số bổ sung, tức là tỷ lệ trộn, được lấy mẫu theo cách tương tự như trong Mixup [25].

## 6 Kết quả và phân tích thực nghiệm

### 6.1 Bộ dữ liệu Stanford Sentiment Treebank

Kết quả cho bộ dữ liệu SST được trình bày trong Bảng 2. Phương pháp BERT vanilla đạt 51.17% và kém hơn cả hai chuẩn khác sử dụng Mixup trong quá trình huấn luyện (wordMixup và MixupEncoding).

Trong tất cả các thí nghiệm AttentionMix được trình bày trong Bảng 2, độ chính xác trung bình cao hơn phương pháp vanilla đã được đạt được, và trong tất cả trừ một trong số chúng, kết quả AttentionMix vượt trội hơn cả 3 phương pháp chuẩn.

Kết quả chi tiết hơn được mô tả trong Hình 2. Hình con bên trái trình bày độ chính xác trung bình khi tất cả các đầu trong một lớp nhất định được sử dụng, và các hình con giữa và phải là những phân tích sâu vào độ chính xác trung bình khi các đầu đơn lẻ trong các lớp hoạt động tốt nhất (0 và 10 tương ứng) được xem xét.

Kết quả được trình bày trong Hình 2 cho thấy việc sử dụng các lớp attention khác ngoài top-3 trong quá trình tăng cường rõ ràng làm giảm độ chính xác trên tập test. Hơn nữa, khi nhìn vào các hình con giữa và phải, kết quả cao nhất trong số các đầu riêng lẻ được đạt bởi một đầu từ lớp 10, việc học trong lớp 0 đồng đều hơn và có nhiều đầu "mạnh" hơn trong lớp này. Trong lớp 0, việc sử dụng bất kỳ đầu riêng lẻ nào đều dẫn đến độ chính xác cao hơn huấn luyện BERT tiêu chuẩn và đối với 6 trong số 12 đầu, nó vượt trội hơn tất cả các phương pháp cạnh tranh.

Ngược lại, một cái nhìn gần hơn vào lớp 10 (Hình 2 (phải)) cho thấy chỉ có 2 đầu (0 và 3) với kết quả cao hơn tất cả các chuẩn và chỉ 3 (0, 3 và 11) với độ chính xác cao hơn huấn luyện BERT tiêu chuẩn.

### 6.2 Bộ dữ liệu Movie Review

Bảng 3 trình bày kết quả cho bộ dữ liệu MR. Chuẩn vanilla đạt 85.14%. Rõ ràng, có sự khác biệt đáng kể về độ chính xác giữa MR và SST, do thực tế là MR là bộ dữ liệu nhị phân — do đó có khả năng ít phức tạp hơn nhiều. Thú vị là cả hai phương pháp sử dụng Mixup trong quá trình huấn luyện (wordMixup và MixupEncoding) đều đạt độ chính xác thấp hơn, điều này phù hợp với kết quả được báo cáo trong [7].

Trong 5 trong số 6 thí nghiệm, AttentionMix đạt độ chính xác trung bình cao hơn huấn luyện vanilla. Các thí nghiệm sử dụng attention từ các lớp cụ thể đạt kết quả cao nhất. Tương tự như SST, kết quả chi tiết có sẵn trong Hình 3. Hình con bên trái trình bày độ chính xác trung bình từ việc sử dụng tất cả các đầu trong một lớp nhất định và các hình con giữa và phải là kết quả liên quan đến đầu, tức là độ chính xác trung bình khi các đầu đơn lẻ trong các lớp hoạt động tốt nhất (lớp 1 và lớp 6 tương ứng) được sử dụng.

Trong hình con bên trái, chỉ có 2 lớp attention đạt độ chính xác test cao hơn phương pháp chuẩn tốt nhất, trong trường hợp này là huấn luyện BERT tiêu chuẩn. Khi nhìn vào các hình con giữa và phải, có thể quan sát thấy một xu hướng tương tự như bộ dữ liệu SST, tức là nhìn chung có nhiều đầu vượt trội hơn phương pháp chuẩn tốt nhất có thể được tìm thấy trong các lớp sớm hơn các lớp muộn hơn. Trong hình con giữa, đề cập đến lớp 1, có 7 đầu với độ chính xác test vượt trội hơn tất cả các phương pháp chuẩn, trong khi đối với lớp 6 (hình con phải) chỉ có 4 đầu như vậy.

### 6.3 Bộ dữ liệu Internet Movie Database

Kết quả cho bộ dữ liệu IMDB được hiển thị trong Bảng 4. Phương pháp BERT vanilla đạt 93.54% và yếu hơn cả hai chuẩn liên quan đến Mixup. Cả bộ dữ liệu IMDB và MR đều đề cập đến phân loại cảm xúc nhị phân, mặc dù IMDB có gấp 3 lần mẫu huấn luyện hơn, đây là lý do có thể nhất của độ chính xác cao hơn trên bộ dữ liệu này trong tất cả các thí nghiệm, so với MR.

AttentionMix đạt độ chính xác trung bình cao hơn huấn luyện vanilla trong 5 trong số 6 thí nghiệm. Ngoài ra, tất cả các thí nghiệm AttentionMix sử dụng các đầu riêng lẻ (3 hàng cuối của Bảng 4) đều tạo ra kết quả vượt trội hơn cả 3 phương pháp chuẩn. Kết quả liên quan đến đầu được minh họa trong Hình 4. Hình con bên trái trình bày độ chính xác trung bình khi tất cả các đầu trong một lớp nhất định được sử dụng, và các hình con giữa và phải hiển thị độ chính xác trung bình của các đầu đơn lẻ trong các lớp hoạt động tốt nhất (0 và 1 tương ứng).

Có thể thấy trong Hình 4 rằng việc sử dụng các lớp attention khác ngoài top-2 trong quá trình tăng cường làm giảm độ chính xác trên tập test xuống dưới độ chính xác của các phương pháp chuẩn. Kết quả cao nhất trong số các đầu riêng lẻ được đạt bởi đầu từ lớp 0 (hình con giữa). Tương tự như SST, việc học trong lớp này đồng đều hơn, với số lượng đầu hiệu quả cao. Việc sử dụng bất kỳ đầu riêng lẻ nào từ lớp 0 đều dẫn đến độ chính xác cao hơn BERT và một nửa số đầu vượt trội hơn cả 3 phương pháp cạnh tranh.

Ngược lại, trong Hình 4 (phải), đề cập đến lớp 1, chỉ có một đầu (số 11) vượt trội hơn tất cả các phương pháp chuẩn và một nửa số đầu vượt trội hơn chuẩn BERT.

Tóm lại, có hai kết luận chính có thể rút ra từ các thí nghiệm được trình bày trên ba bộ dữ liệu phân loại câu. Thứ nhất, nếu cần đưa ra quyết định về cài đặt AttentionMix mà không có khả năng kiểm tra các cấu hình siêu tham số khác nhau (ví dụ do tài nguyên tính toán hạn chế), khuyến nghị sử dụng vectơ liên quan được trích xuất từ một đầu duy nhất vì các vectơ như vậy mang lại hiệu suất cao hơn trong các thí nghiệm của chúng tôi. Thứ hai, đầu duy nhất này nên được chọn trong số các đầu thuộc về các lớp ban đầu. Nó xuất phát từ các thí nghiệm, rằng trong những lớp sớm hơn này có nhiều đầu vượt trội hơn kết quả chuẩn, mặc dù với lời cảnh báo rằng với chiến lược chọn đầu như vậy, đầu hoạt động tốt nhất tổng thể có thể bị bỏ qua (ví dụ đầu 3 từ lớp 10 trong các thí nghiệm SST).

### 6.4 Độ phức tạp tính toán

Huấn luyện AttentionMix yêu cầu 2 thao tác bổ sung so với wordMixup: forward pass một phần (câu được truyền qua bộ mã hóa để nhận attention) và tính toán λvector (không tốn nhiều tính toán). Thời gian huấn luyện tăng thêm khoảng 26%. Thời gian suy luận hoàn toàn giống nhau cho cả hai phương pháp.

### 6.5 Nghiên cứu loại bỏ

Chúng tôi điều tra sâu hơn tại sao thông tin nhất định đến từ ma trận trọng số attention dẫn đến tăng độ chính xác cao hơn. Vì ba bộ dữ liệu bao gồm các loại phân tích cảm xúc khác nhau, chúng tôi đặt giả thuyết rằng một số phần của lời nói có thể có tác động cao hơn đến phân loại cảm xúc so với những phần khác. Cụ thể, giả định của chúng tôi là tính từ, trạng từ và động từ có thể chỉ ra nhiều hơn cho dự đoán lớp cảm xúc, vì cảm xúc thường được phản ánh bởi các câu như:

• love, like, hate – động từ
• fantastic, disappointing – tính từ
• quite, very, extremely – trạng từ

Đối với bộ dữ liệu SST, giả thuyết này chỉ được xác nhận cho một số vectơ liên quan được suy ra từ thông tin attention. Hình 5 hiển thị attention trung bình được trao cho một phần nhất định của lời nói bởi đầu attention 8 trong lớp 0 và đầu attention 3 trong lớp 10, việc sử dụng chúng dẫn đến hai mô hình hoạt động tốt nhất. Đối với đầu 8 trong lớp 0, thực sự attention cao được trao cho tính từ, trạng từ và động từ, nhưng đối với đầu attention 3 trong lớp 10, attention rất cao được trao cho dấu câu. Hiện tượng attention cao liên quan đến dấu câu này đã được quan sát trước đây trong [2].

Đối với bộ dữ liệu MR, attention trung bình tương đối được gán cho mỗi phần của lời nói cho hai đầu hoạt động tốt nhất được trình bày trong Hình 6. Đáng ngạc nhiên, đối với cả hai đầu, attention chỉ ra hướng có vẻ không trực quan, ít nhất là đối với con người. Attention cao nhất được gán cho PUNCT và CCONJ, đối với bộ dữ liệu này chủ yếu bao gồm các ký hiệu trích dẫn, các loại dấu ngoặc khác nhau, dấu phẩy, dấu chấm, dấu hỏi, dấu cảm thán và 3 từ thông thường: and, but và or.

Đối với bộ dữ liệu IMDB, hai đầu hoạt động tốt nhất thuộc về lớp 0 (đầu 4 và 6). Attention trung bình tương đối được gán cho mỗi phần của lời nói cho hai đầu này được mô tả trong Hình 7. Tương tự như kết quả SST cho đầu 8 lớp 0, giả thuyết ban đầu đứng vững cho cả hai đầu. Trong số các phần khác của lời nói, đầu 4 chú ý đến tính từ và trạng từ, và đầu 6 đến tính từ và động từ.

Tóm lại, trên cả 3 bộ dữ liệu, trong 3 trong số 6 đầu hoạt động tốt nhất, attention được trao cho tính từ, trạng từ và động từ. Trong 3 trường hợp còn lại, lựa chọn attention ít trực quan hơn.

Về bản chất, dựa trên kết quả được trình bày trong Bảng 2, 3 và 4, chúng tôi kết luận rằng việc sử dụng thông tin từ cơ chế attention hữu ích trong việc tạo ra các mẫu tăng cường. Đồng thời, một giải thích chi tiết về lý do cho việc tập trung attention cụ thể trong một số trường hợp cần nghiên cứu thêm.

Sự tồn tại của các đầu attention không trực quan đã được quan sát trong tài liệu [4,13]. Bài báo đầu tiên, bên cạnh các chủ đề khác, nghiên cứu cái gọi là đầu attention dọc chủ yếu chú ý đến dấu chấm, dấu phẩy và token đặc biệt của BERT. Nghiên cứu khác gọi những điều trên là đầu phân cách, và định lượng tỷ lệ phổ biến cao của chúng ở mức 73.43%, nói rằng chúng tồn tại tự nhiên cùng với các chức năng khác của đầu.

Các thí nghiệm loại bỏ được trình bày trong bài báo tập trung vào câu x1, dựa trên đó vectơ tỷ lệ trộn (λvector trong Phương trình 7) được tính toán. Một con đường khả thi khác, mặc dù khó triển khai hơn, là phân tích sự tập trung attention dựa trên câu x2. Chúng tôi dự định điều tra con đường này trong nghiên cứu tương lai.

## 7 Nhận xét kết luận

Lấy cảm hứng từ thành công của tăng cường Mixup trong lĩnh vực CV, chúng tôi giới thiệu một phương pháp tăng cường liên quan đến Mixup trong bối cảnh phân loại văn bản. Không giống như các phương pháp dựa trên Mixup trước đây dành cho lĩnh vực NLP, chúng tôi đề xuất sử dụng phương pháp trộn có hướng dẫn, và hướng tới mục tiêu này sử dụng thông tin attention của BERT như nguồn hướng dẫn cho quá trình tăng cường. Chúng tôi chỉ ra thực nghiệm rằng phương pháp được đề xuất, AttentionMix, vượt trội hơn phương pháp BERT vanilla và hai phương pháp chuẩn dựa trên Mixup được sử dụng để so sánh. Kết quả hỗ trợ hiệu quả của việc sử dụng trộn có hướng dẫn dựa trên attention trong lĩnh vực NLP.

Trong nghiên cứu tương lai, chúng tôi dự định tìm kiếm một phương pháp tự động để chọn thông tin attention phù hợp nhất cho một bộ dữ liệu nhất định. Mục tiêu này liên quan đến khó khăn nổi tiếng trong việc giải thích thông tin đến từ các đầu attention [2, 12]. Một hiện tượng còn xa mới được hiểu đầy đủ.

Một hướng nghiên cứu khác là đánh giá hiệu quả AttentionMix khi được áp dụng ở cấp độ encoding từ – trái ngược với việc triển khai cấp độ embedding từ hiện tại.

## Tài liệu tham khảo

[1] Chen, J., Yang, Z., Yang, D.: Mixtext: Linguistically-informed interpolation of hidden space for semi-supervised text classification. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020. pp. 2147–2157 (2020). https://doi.org/10.18653/v1/2020.acl-main.194

[2] Clark, K., Khandelwal, U., Levy, O., Manning, C.D.: What does BERT look at? an analysis of bert's attention. In: Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, BlackboxNLP@ACL 2019, Florence, Italy. pp. 276–286 (2019). https://doi.org/10.18653/v1/W19-4828

[3] Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre-training of deep bidirectional transformers for language understanding. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA. pp. 4171–4186 (2019). https://doi.org/10.18653/v1/n19-1423

[4] Guan, Y., Leng, J., Li, C., Chen, Q., Guo, M.: How far does BERT look at: Distance-based clustering and analysis of bert's attention. In: Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online). pp. 3853–3860 (2020). https://doi.org/10.18653/v1/2020.coling-main.342

[5] Guo, H., Mao, Y., Zhang, R.: Augmenting data with mixup for sentence classification: An empirical study. CoRR abs/1905.08941 (2019), http://arxiv.org/abs/1905.08941

[6] Huang, S., Wang, X., Tao, D.: Snapmix: Semantically proportional mixing for augmenting fine-grained data. CoRR abs/2012.04846 (2020)

[7] Jindal, A., Gnaneshwar, D., Sawhney, R., Shah, R.R.: Leveraging BERT with mixup for sentence classification (student abstract). In: The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, New York, NY, USA. pp. 13829–13830 (2020)

[8] Kong, F., Zhang, R., Guo, X., Mensah, S., Mao, Y.: Dropmix: A textual data augmentation combining dropout with mixup. In: Goldberg, Y., Kozareva, Z., Zhang, Y. (eds.) Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022. pp. 890–899. Association for Computational Linguistics (2022). https://doi.org/10.18653/v1/2022.emnlp-main.57

[9] Lewy, D., Mańdziuk, J.: An overview of mixing augmentation methods and augmentation strategies. Artificial Intelligence Review 56, 2111–2169 (2023). https://doi.org/10.1007/s10462-022-10227-z

[10] Luo, X., Ding, H., Tang, M., Gandhi, P., Zhang, Z., He, Z.: Attention mechanism with BERT for content annotation and categorization of pregnancy-related questions on a community q&a site. In: IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2020, Virtual Event, South Korea. pp. 1077–1081 (2020). https://doi.org/10.1109/BIBM49941.2020.9313379

[11] Maas, A.L., Daly, R.E., Pham, P.T., Huang, D., Ng, A.Y., Potts, C.: Learning word vectors for sentiment analysis. In: The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, Portland, Oregon, USA. pp. 142–150 (2011)

[12] Michel, P., Levy, O., Neubig, G.: Are sixteen heads really better than one? In: Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, Vancouver, BC, Canada. pp. 14014–14024 (2019)

[13] Pande, M., Budhraja, A., Nema, P., Kumar, P., Khapra, M.M.: The heads hypothesis: A unifying statistical approach towards understanding multi-headed attention in BERT. In: Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Virtual Event. pp. 13613–13621. AAAI Press (2021)

[14] Pang, B., Lee, L.: Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In: ACL 2005, 43rd Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, University of Michigan, USA. pp. 115–124 (2005). https://doi.org/10.3115/1219840.1219855

[15] Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.D., Ng, A.Y., Potts, C.: Recursive deep models for semantic compositionality over a sentiment treebank. In: Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, Seattle, Washington, USA. pp. 1631–1642 (2013)

[16] Sun, L., Xia, C., Yin, W., Liang, T., Yu, P.S., He, L.: Mixup-transformer: Dynamic data augmentation for NLP tasks. In: Scott, D., Bel, N., Zong, C. (eds.) Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020. pp. 3436–3440. International Committee on Computational Linguistics (2020). https://doi.org/10.18653/v1/2020.coling-main.305

[17] Uddin, A.F.M.S., Monira, M.S., Shin, W., Chung, T., Bae, S.: Saliencymix: A saliency guided data augmentation strategy for better regularization. CoRR abs/2006.01791 (2020)

[18] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. CoRR abs/1706.03762 (2017)

[19] Verma, V., Lamb, A., Beckham, C., Najafi, A., Mitliagkas, I., Lopez-Paz, D., Bengio, Y.: Manifold Mixup: Better Representations by Interpolating Hidden States. In: Proceedings of the 36th International Conference on Machine Learning, ICML 2019, Long Beach, California, USA. vol. 97, pp. 6438–6447 (2019)

[20] Vishwamitra, N., Hu, R.R., Luo, F., Cheng, L., Costello, M., Yang, Y.: On analyzing covid-19-related hate speech using BERT attention. In: 19th IEEE International Conference on Machine Learning and Applications, ICMLA 2020, Miami, FL, USA. pp. 669–676 (2020). https://doi.org/10.1109/ICMLA51294.2020.00111

[21] Walawalkar, D., Shen, Z., Liu, Z., Savvides, M.: Attentive Cutmix: An Enhanced Data Augmentation Approach for Deep Learning Based Image Classification. In: 2020 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2020. pp. 3642–3646 (2020). https://doi.org/10.1109/ICASSP40776.2020.9053994

[22] Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.R.: GLUE: A multi-task benchmark and analysis platform for natural language understanding. In: 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA (2019)

[23] Wei, J.W., Zou, K.: EDA: easy data augmentation techniques for boosting performance on text classification tasks. In: Inui, K., Jiang, J., Ng, V., Wan, X. (eds.) Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019. pp. 6381–6387. Association for Computational Linguistics (2019). https://doi.org/10.18653/v1/D19-1670

[24] Yoon, S., Kim, G., Park, K.: Ssmix: Saliency-based span mixup for text classification. In: Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event. pp. 3225–3234 (2021). https://doi.org/10.18653/v1/2021.findings-acl.285

[25] Zhang, H., Cissé, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond Empirical Risk Minimization. In: 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada (2018)

[26] Zhou, K., Yang, Y., Qiao, Y., Xiang, T.: Domain generalization with mixstyle. CoRR abs/2104.02008 (2021)
