# 2110.01852.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/data-augmentation/2110.01852.pdf
# File size: 2995728 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Data Augmentation Approaches in Natural Language
Processing: A Survey
Bohan Li, Yutai Hou, Wanxiang Che
Harbin Institute of Technology, Harbin, China
Abstract
Asaneﬀectivestrategy, dataaugmentation(DA)alleviatesdatascarcityscenar-
ios where deep learning techniques may fail. It is widely applied in computer
vision then introduced to natural language processing and achieves improve-
ments in many tasks. One of the main focuses of the DA methods is to improve
the diversity of training data, thereby helping the model to better generalize to
unseen testing data. In this survey, we frame DA methods into three categories
based on the diversity of augmented data, including paraphrasing, noising, and
sampling. Our paper sets out to analyze DA methods in detail according to the
above categories. Further, we also introduce their applications in NLP tasks as
well as the challenges. Some useful resources are provided in Appendix A.
Keywords: Data Augmentation, Natural Language Processing
2010 MSC: 00-01, 99-00
A person in white clothes and jeans is standing there.
Original InputA person in white sweaterand jeans is standing there.ParaphrasingA personpeople in white sweater and jeansis standing there.NoisingThere stands a girl wearing white sweater and jeans.Sampling最终版
Corresponding author
Email addresses: bhli@ir.hit.edu.cn (Bohan Li), ythou@ir.hit.edu.cn (Yutai Hou),
car@ir.hit.edu.cn (Wanxiang Che)
Preprint submitted to Journal of L ATEX Templates June 28, 2022arXiv:2110.01852v3  [cs.CL]  27 Jun 2022

--- PAGE 2 ---
Contents
1 Introduction 4
2 Data Augmentation Methods in NLP 6
2.1
Paraphrasing-based Methods . . . . . . . . . . . . . . . . . . . 7
2.1.1 Thesauruses . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.1.2 Semantic Embeddings . . . . . . . . . . . . . . . . . . . . 8
2.1.3 Language Models . . . . . . . . . . . . . . . . . . . . . . . 9
2.1.4 Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2.1.5 Machine Translation . . . . . . . . . . . . . . . . . . . . . 11
2.1.6 Model Generation . . . . . . . . . . . . . . . . . . . . . . 12
2.2
Noising-based Methods . . . . . . . . . . . . . . . . . . . . . . 13
2.2.1 Swapping . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.2.2 Deletion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.2.3 Insertion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.2.4 Substitution . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.3
Sampling-based Methods . . . . . . . . . . . . . . . . . . . . . 16
2.3.1 Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.3.2 Non-pretrained Models . . . . . . . . . . . . . . . . . . . . 17
2.3.3 Pretrained Models . . . . . . . . . . . . . . . . . . . . . . 18
2.3.4 Self-training . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2.3.5 Mixup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.4 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
3 Strategies and Tricks 22
3.1 Method Stacking . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
3.2 Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
3.2.1 The Use of Augmented Data . . . . . . . . . . . . . . . . 23
3.2.2 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . 23
3.2.3 Training Strategies . . . . . . . . . . . . . . . . . . . . . . 23
3.2.4 Training Objects . . . . . . . . . . . . . . . . . . . . . . . 24
3.3 Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
4 Applications on NLP Tasks 25
5 Related Topics 29
5.1 Pretrained Language Models . . . . . . . . . . . . . . . . . . . . 29
5.2 Contrastive Learning . . . . . . . . . . . . . . . . . . . . . . . . . 29
5.3 Other Data Manipulation Methods . . . . . . . . . . . . . . . . . 29
5.4 Generative Adversarial Networks . . . . . . . . . . . . . . . . . . 29
5.5 Adversarial Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . 30
6 Challenges and Opportunities 30
2

--- PAGE 3 ---
7 Conclusion 31
Appendix A Related Resources 55
3

--- PAGE 4 ---
1. Introduction
Data augmentation refers to methods used to increase the amount of data
by adding slightly modiﬁed copies of already existing data or newly created syn-
thetic data from existing data. Such methods alleviate data scarcity scenarios
where deep learning techniques may fail, so DA has received active interest and
demand recently. Data augmentation is widely applied in the ﬁeld of computer
vision[1], suchasﬂippingandrotation, thenintroducedtonaturallanguagepro-
cessing (NLP). Diﬀerent to images, natural language is discrete, which makes
the adoption of DA methods more diﬃcult and underexplored in NLP.
Large numbers of DA methods have been proposed recently, and a survey of
existing methods is beneﬁcial so that researchers could keep up with the speed
of innovation. Liu et al. [2] and Feng et al. [3] both present surveys that give a
bird’s eye view of DA for NLP. They directly divide the categories according to
the methods. These categories thus tend to be too limited or general, e.g., back-
translation andmodel-based techniques . Bayer et al. [4] post a survey on DA
for text classiﬁcation only. In this survey, we will provide an inclusive overview
of DA methods in NLP. One of our main goals is to show the nature of DA,
i.e.,why data augmentation works . To facilitate this, we category DA methods
according to the diversity of augmented data, since improving training data
diversity is one of the main thrusts of DA eﬀectiveness. We frame DA methods
into three categories, including paraphrasing, noising, and sampling.1
Speciﬁcally, paraphrasing -based methods generate the paraphrases of the
original data as the augmented data. This category brings limited changes
compared with the original data. Noising-based methods add more continuous
or discrete noises to the original data and involve more changes. Sampling -
based methods master the distribution of the original data to sample new data
as augmented data. With the help of artiﬁcial heuristics and trained models,
such methods can sample brand new data rather than changing existing data
and therefore generate even more diverse data.
Our paper sets out to analyze DA methods in detail according to the above
categories. In addition, we also introduce their applications in NLP tasks as
well as the challenges. The rest of the paper is structured as follows:
•Section 2 presents a comprehensive review of the three categories and
analyzes every single method in those categories. We also introduce the
characteristics of the methods, e.g., the granularity and the level.
•Section 3 refers to a summary of common strategies and tricks to improve
the quality of augmented data, including method stacking, optimization,
and ﬁltering strategies.
1This survey has been accepted by AI OPEN: https://www.sciencedirect.com/journal/
ai-open. We also provide further resources at: https://github.com/BohanLi0110/
NLP-DA-Papers
4

--- PAGE 5 ---
•Section 4 analyzes the application of the above methods in NLP tasks.
We also show the development of DA methods through a timeline.
•Section 5 introduces some related topics of data augmentation, including
pre-trained language models, contrastive learning, similar data manipu-
lation methods, generative adversarial networks, and adversarial attacks.
We aim to connect data augmentation with other topics and show their
diﬀerence at the same time.
•Section 6 lists some challenges we observe in NLP data augmentation,
including theoretical narrative and generalized methods. These points
also reveal the future development direction of data augmentation.
•Section 7 concludes the paper.
DA
for
NLPParaphrasingThesauruses Zhang et al. [5], Wei et al. [6], Coulombe et al. [7]
Semantic
EmbeddingsWang et al. [8]
Language
ModelsJiao et al. [9]
Rules Coulombe et al. [7], Regina et al. [10], Louvan et al. [11]
Machine
TranslationBack-translation Xie et al. [12], Zhang et al. [13]
Unidirectional
TranslationNishikawa et al. [14], Bornea et al. [15]
Model
GenerationHou et al. [16], Li et al. [17], Liu et al. [18]
NoisingSwapping Wei et al. [6], Luque et al. [19], Yan et al. [20]
Deletion Wei et al. [6], Peng et al. [21], Yu et al. [22]
Insertion Wei et al. [6], Peng et al. [21], Yan et al. [20]
Substitution Coulombe et al. [7], Xie et al. [23], Louvan et al. [11]
SamplingRules Min et al. [24], Liu et al. [25]
Non-
pretrainedKang et al. [26], Zhang et al. [13], Raille et al. [27]
PretrainedTavor et al. [28], Kumar et al. [29], Ng et al. [30]
Thakur et al. [31], Quteineh et al. [32]
Self
TrainingDu et al. [33], Montella et al. [34]
Mixup Guo et al. [35], Cheng et al. [36]
Figure 1: Taxonomy of NLP DA methods.
5

--- PAGE 6 ---
A person in white clothes and jeans is standing there.
Original InputA person in white sweaterand jeans is standing there.ParaphrasingA personpeople in white sweater and jeansis standing there.NoisingThere stands a girl wearing white sweater and jeans.Sampling最终版Figure2: Dataaugmentationtechniquesincludethreecategories. Theexamplesoftheoriginal
data and augmented data are on the left and right, respectively. As we can see, the diversity
ofparaphrasing ,noising, andsampling increases in turn compared to the original input.
2. Data Augmentation Methods in NLP
Data Augmentation aims at generating additional, synthetic training data in
insuﬃcient data scenes. Data augmentation ranges from simple techniques like
rule-based methods to learnable generation-based methods, and all the above
methods essentially guarantee the validity of the augmented data [27]. That
is to say, DA methods need to make sure that the augmented data is valid for
the task, i.e., be considered to be part of the same distribution of the original
data [27]. For example, similar semantics in machine translation and the same
label in text classiﬁcation as the original data.
On the basis of validity, augmented data is also expected to be diverse to
improve model generalization on downstream tasks. This involves the diversity
of augmented data. In this survey, we novelly divide DA methods into three
categories according to the diversity of their augmented data: paraphrasing,
noising, and sampling.
•The paraphrasing-based methods generate augmented data that has lim-
ited semantic diﬀerence from the original data, based on proper and re-
strained changes to sentences. The augmented data convey very similar
information as the original form.
•The noising-based methods add discrete or continuous noise under the
premise of guaranteeing validity. The point of such methods is to improve
the robustness of the model.
•The sampling-based methods master the data distributions and sample
novel data within them. Such methods output more diverse data and
satisfy more needs of downstream tasks based on artiﬁcial heuristics and
trained models.
6

--- PAGE 7 ---
2. Semantic Embeddings
4. Rules
1. Thesaurus
3. MLMs
5. Machine Translation
6. Model Generation
Sentence
-
level
Phrase
-
level
Word
-
level最终版Figure 3: Data augmentation techniques by paraphrasing include three levels: word-level,
phrase-level, and sentence-level.
Figure 4: Paraphrasing by using thesauruses.
As shown in the examples and diagrams in Figure 2, the paraphrasing, nois-
ing, and sampling-based methods provide more diversity in turn. In this section,
we will introduce and analyze them in detail.2
2.1.
 Paraphrasing-based Methods
As common phenomena in natural language, paraphrases are alternative
ways to convey the same information as the original form [37, 38]. Naturally,
the generation of paraphrases is a suitable scheme for data augmentation. Para-
phrasing consists of several levels, including lexical paraphrasing, phrase para-
phrase, and sentence paraphrase (Figure 3). Therefore, the paraphrasing-based
DA techniques introduced below can also be included into these three levels.
2.1.1. Thesauruses
Some works replace words in the original text with their synonyms and
hypernyms,3soastoobtainanewwayofexpressionwhilekeepingthesemantics
of the original text as unchanged as possible. As shown in Figure 4, thesauruses
like WordNet [40] contain such lexical triplets of words and are often used as
external resources.
Zhang et al. [5] are the ﬁrst to apply thesaurus in data augmentation. They
use a thesaurus derived from WordNet,4which sorts the synonyms of words
according to their similarity. For each sentence, they retrieve all replaceable
words and randomly choose rof them to be replaced. The probability of number
2The speciﬁc classiﬁcation is shown in Figure 12
3Replacing a word with an antonym or a hyponym (more speciﬁc word) is usually not a
semantically invariant transformation. [39]
4The thesaurus is obtained from the Mytheas component used in LibreOﬃce project.
7

--- PAGE 8 ---
Being late is terrible.                                                     Being late is bad.
awful……Being late is terrible.                                                     Being late is bad.is
awfulbadfierceterriblebadfierceawful最终版（右边这个小的）
awful……Figure 5: Paraphrasing by using semantic embeddings.
risdeterminedbyageometricdistributionwithparameter pinwhichP[r]pr.
Given a word, the index sof its chosen synonym is also determined by a another
geometric distribution in which P[s]ps. The method ensures that synonyms
that are more similar to the original word are selected with greater probability.
Some methods [41, 42, 43] apply a similar method.
A widely used text augmentation method called EDA ( EasyDataAug-
mentation Techniques) [6] also replaces the original words with their synonyms
using WordNet: they randomly choose nwords, which are not stop words, from
the original sentence.5Each of these words is replaced with a random synonym.
Zhang et al. [44] apply a similar method in extreme multi-label classiﬁcation.
In addition to synonyms, Coulombe et al. [7] propose to use hypernyms to
replace the original words. They also recommend the parts of speech of the
augmented word in order of increasing diﬃculty: adverbs, adjectives, nouns,
and verbs. Zuo et al. [45] use WordNet and VerbNet [46] to retrieve synonyms,
hypernyms, and words of the same category.
/lightbulbThesauruses
Advantage(s):
1. Easy to use.
Limitation(s):
1. The scope and part of speech of augmented words are limited.
2. This method cannot resolve the ambiguity problem.
3. Sentence semantics may be aﬀected if there are too many substitutions.
2.1.2. Semantic Embeddings
This method overcomes the limitations of replacement range and parts of
speech in the thesaurus-based method. It uses pre-trained word embeddings,
such as Glove, Word2Vec, FastText, etc., and replaces the original word in the
sentence with its closest neighbor in embedding space, as shown in Figure 5.
In the Twitter message classiﬁcation task, Wang et al. [8] pioneer to use
both word embeddings and frame embeddings instead of discrete words.6As
for word embeddings, each original word in the tweet is replaced with one of
its k-nearest-neighbor words using cosine similarity. For example, “Being late is
terrible” becomes “Being behind are bad”. As for frame semantic embeddings,
5nis proportional to the length of the sentence.
6The frame embeddings refer to the continuous embeddings of semantic frames [47].
8

--- PAGE 9 ---
Figure 6: Paraphrasing by using language models.
the authors semantically parse 3.8 million tweets and build a continuous bag-of-
frame model to represent each semantic frame using Word2Vec [48]. The same
data augmentation approach as words is then applied to semantic frames.
Compared to Wang et al. [8], Liu et al. [49] only use word embeddings to
retrieve synonyms. In the meanwhile, they edit the retrieving result with a
thesaurus for balance. RamirezEchavarria et al. [50] create the dictionary of
embeddings for selection.
/lightbulbSemantic Embeddings
Advantage(s):
1. Easy to use.
2. Higher replacement hit rate and more comprehensive replacement
range.
Limitation(s):
1. This method cannot resolve the ambiguity problem.7
2. Sentence semantics may be aﬀected if there are too many substitutions.
2.1.3. Language Models
Pretrained language models have become mainstream models in recent years
due to their excellent performance. Masked language models (MLMs) such as
BERT and RoBERTa can predict masked words in text based on context, which
can be used for text data augmentation (as shown in Figure 6). Moreover,
this approach alleviates the ambiguity problem since MLMs consider the whole
context.
Wu et al. [51] ﬁne-tune on pre-trained BERT to perform conditional MLM
task. They alter the segmentation embeddings to label embeddings, which are
learnedcorrespondingtotheannotatedlabelsonlabeleddatasets. Theyusethis
ﬁne-tuned conditional BERT to augment sentences. Speciﬁcally, a few words in
a labeled sentence are randomly mask then ﬁlled by the conditional BERT.
Jiao et al. [9] use both word embeddings and masked language models to
obtain augmented data. They apply the tokenizer of BERT to tokenize words
into multiple word pieces. Each word piece is replaced with probability 0.4. If
a word piece is not a complete word (“est” for example), it is replaced by its
K-nearest-neighbor words in the Glove embedding space. If the word piece is
a complete word, the authors replace it with [MASK] and employ BERT to
predictKWords to ﬁll in the blank. Regina et al. [10], Tapia-Téllez et al. [52],
Lowell et al. [53], and Palomino et al. [54] apply methods that are similar to Jiao
et al. [9]. They mask multiple words in a sentence and generate new sentences
by ﬁlling these masks to generate more varied sentences. In addition, RNNs are
9

--- PAGE 10 ---
She is not overly optimistic.                                She isn’t overly optimistic. RulesShe is not overly optimistic.                                She isn ’t overly optimistic.Rules
最终版（右边这个小的）Figure 7: Paraphrasing by using rules.
also used for replacing the original word based on the context ([55, 56]).
/lightbulbLanguage Models
Advantage(s):
1. This approach alleviates the ambiguity problem.
2. This method considers context semantics.
Limitation(s):
1. Still limited to the word level.
2. Sentence semantics may be aﬀected if there are too many substitutions.
2.1.4. Rules
This method requires some heuristics about natural language to ensure the
maintaining of sentence semantics, as shown in Figure 7.
On the one hand, some works rely on existing dictionaries or ﬁxed heuristics
to generate word-level and phrase-level paraphrases. Coulombe et al. [7] in-
troduce the use of regular expressions to transform the form without changing
sentence semantics, such as the abbreviations and prototypes of verbs, modal
verbs, and negation. For example, replace “is not” with “isn’t”. Similarly, Regina
et al. [10] use word-pair dictionaries to perform replacements between the ex-
panded form and the abbreviated form.
On the other hand, some works generate sentence-level paraphrases for orig-
inal sentences with some rules, e.g. dependency trees. Coulombe et al. [7] use a
syntactic parser to build a dependency tree for the original sentence. Then the
dependency tree is used for syntax transformation. For example, replace “Sally
embraced Peter excitedly.” with “Peter was embraced excitedly by Sally.”. De-
houck et al. [57] apply a similar method. Louvan et al. [11] crop particular
fragments on the dependency tree to create a smaller sentence. They also ro-
tate the target fragment around the root of the dependency parse structure,
without harming the original semantics.
/lightbulbRules
Advantage(s):
1. Easy to use.
2. This method preserves the original sentence semantics.
Limitation(s):
1. This method requires artiﬁcial heuristics.
2. Low coverage and limited variation.
10

--- PAGE 11 ---
It‘s so kind of you.                  你真好。 You are so nice.
最终版Figure 8: Paraphrasing by machine translation.
2.1.5. Machine Translation
Translation is a natural means of paraphrasing. With the development of
machine translation models and the availability of online APIs, machine transla-
tion is popular as an augmentation method in many tasks, as shown in Figure 8.
Back-translation .This method means that the original text is translated into
other languages, and then translated back to obtain the augmented text in the
original language. Diﬀerent from word-level methods, back-translation does not
directly replace individual words but rewrites the whole sentence in a generated
way.
Xie et al. [12], Yu et al. [58], and Fabbri et al. [59] use English-French
translation models (in both directions) to perform back-translation on each
sentence and obtain their paraphrases. Lowell et al. [53] also introduce this
methodasoneoftheunsuperviseddataaugmentationmethods. Zhangetal.[13]
leverage back-translation to obtain the formal expression of the original data in
the style transfer task.
In addition to some trained machine translation models, some cloud transla-
tion API services like Google and DeepL are common tools for back-translation
and are applied by some works like [7, 19, 60, 42, 61, 62, 10, 63, 64].8
Some works add additional features based on vanilla back-translation. Nu-
gent et al. [65] propose a range of softmax temperature settings to ensure diver-
sity while preserving semantic meaning. Qu et al. [66] combine back-translation
with adversarial training, to synthesize diverse and informative augmented ex-
amples by organically integrating multiple transformations. Zhang et al. [13]
employ a discriminator to ﬁlter the sentences in the back-translation results.
This method greatly improves the quality of the augmented data as a thresh-
old.
Unidirectional Translation .Diﬀerent from back-translation, the unidirec-
tional translation method directly translates the original text into other lan-
guages once, without translating it back to the original language. This method
usually occurs in a multilingual scene.
Inthetaskofunsupervisedcross-lingualwordembeddings(CLWEs),Nishikawa
et al. [14] build pseudo-parallel corpus with an unsupervised machine transla-
tion model. The authors ﬁrst train unsupervised machine translation (UMT)
models using the source/target training corpora and then translate the corpora
8The links of the above Cloud Translation API services are: https://cloud.google.com/
translate/docs/apis (Google) and https://www.deepl.com/translator (DeepL).
11

--- PAGE 12 ---
Figure 9: Paraphrasing by model generation.
using the UMT models. The machine-translated corpus is used together with
the original corpus to learn monolingual word embeddings for each language
independently. Finally, the learned monolingual word embeddings are mapped
to a shared CLWE space. This method both facilitates the structural similarity
of two monolingual embedding spaces and improves the quality of CLWEs in
the unsupervised mapping method.
Bornea et al. [15], Barrire et al. [67], and Aleksandr et al. [63] translate the
original English corpus into several other languages and obtain multiplied data.
Correspondingly, they use multilingual models.
/lightbulbMachine Translation
Advantage(s):
1. Easy to use.
2. Wide range of applications.
3. This approach guarantees correct syntax and unchanged semantics.
Limitation(s):
1. Poor controllability and limited diversity because of the ﬁxed machine
translation models.
2.1.6. Model Generation
Some methods employ Seq2Seq models to generate paraphrases directly.
Such models output more diverse sentences given proper training objects, as
shown in Figure 9.
Hou et al. [16] propose a Seq2Seq data augmentation model for the lan-
guage understanding module of task-based dialogue systems. They feed the
delexicalized input utterance and the speciﬁed diverse rank k(e.g. 1, 2, 3)
into the Seq2Seq model as the input to generate a new utterance. Similarly,
Hou et al. [68] encodes the concatenated multiple input utterances by an L-
layer transformer. The proposed model uses duplication-aware attention and
diverse-oriented regularization to generate more diverse sentences.
In the task of aspect term extraction, Li et al. [17] adopt Transformer as the
basic structure. The masked original sentences as well as their label sequences
are used to train a model Mthat reconstructs the masked fragment as the
augmented data.9Kober et al. [69] use GAN to generate samples that are
9Half of the words in original sentences whose sequence labels are not ‘O’ are masked.
12

--- PAGE 13 ---
very similar to the original data. Liu et al. [18] employ a pre-trained model to
provide prior information to the proposed Transformer-based model. Then the
proposed model could generate both context-relevant answerable questions and
unanswerable questions.
/lightbulbModel Generation
Advantage(s):
1. Wide range of applications.
2. Strong application.
Limitation(s):
1. Require for training data.
2. High training diﬃculty.
2.2.
 Noising-based Methods
The focus of paraphrasing is to make the semantics of the augmented data as
similar to the original data as possible. In contrast, the noising-based methods
add faint noise that does not seriously aﬀect the semantics, so as to make it
appropriately deviate from the original data. Humans greatly reduce the im-
pact of weak noise on semantic understanding through their grasp of linguistic
phenomena and prior knowledge, but this noise can pose challenges for mod-
els. Thus, this method not only expands the amount of training data but also
improves model robustness.
2.2.1. Swapping
The semantics of natural language are sensitive to text order, while slight
order change is still readable for humans [70]. Therefore, the random swapping
between words even sentences within a reasonable range can be used as a data
augmentation method.
Wei et al. [6] randomly choose two words in the sentence and swap their
positions. This process is repeated ntimes, in which nis proportional to the
sentence length l. Longpre et al. [61], Rastogi et al. [62], and Zhang et al. [44]
also apply the same method. Dai et al. [43] split the token sequence into seg-
ments according to labels, then randomly choose some segments to shuﬄe the
order of the tokens inside, with the label order unchanged.
In addition to word-level swapping, some works also propose sentence-level
even instance-level swapping. In the task of tweet sentiment analysis, Luque
et al. [19] divide tweets into two halves. They randomly sample and combine
ﬁrst halves with second halves that have the same label. Although the data
generated in this way may be ungrammatical and semantically unsound, it still
carries relatively complete semantics and emotional polarity compared to indi-
vidual words. Yan et al. [20] perform sentence-level random swapping on legal
documents classiﬁcation. Since sentences independently contain relatively com-
plete semantics comparing to words, the sentence order in the legal document
has little eﬀect on the meaning of the original text. Consequently, the authors
shuﬄe the sentences to obtain the augmented text.
13

--- PAGE 14 ---
Figure 10: The example of ﬁve noising-based methods.
2.2.2. Deletion
This method means randomly deleting words in a sentence or deleting sen-
tences in a document.
As for word-level deletion, Wei et al. [6] randomly remove each word in
the sentence with probability p. Longpre et al. [61], Rastogi et al. [62], and
Zhang et al. [44] also apply the same method. In the task of spoken language
understanding, Peng et al. [21] augment input dialogue acts by deleting slot
values to obtain more combinations.
As for sentence-level deletion, Yan et al. [20] randomly delete each sentence
in a legal document according to a certain probability. They do this because
there exist many irrelevant statements and deleting them will not aﬀect the
understandingofthelegalcase. Yuetal.[22]employtheattentionmechanismto
determine the objective of both word-level and sentence-level random deletion.
2.2.3. Insertion
This method means randomly inserting words into a sentence or inserting
sentences into a document.
As for word-level insertion, Wei et al. [6] select a random synonym of a
random word in a sentence that is not a stop word, then insert that synonym
into a random position in the sentence. This process is repeated ntimes. In the
task of spoken language understanding, Peng et al. [21] augment input dialogue
acts by inserting slot values to obtain more combinations.
14

--- PAGE 15 ---
In legal documents classiﬁcation, since documents with the same label may
have similar sentences, Yan et al. [20] employ sentence-level random insertion.
They randomly select sentences from other legal documents with the same label
to get augmented data.
/info-circleRandom insertion introduces new noisy information that may change the
original label. Tips to avoid this problem:
1. Word level: use label-independent external resources.
2. Sentence level: use other samples with the same labels as the original
data.
2.2.4. Substitution
Thismethodmeansrandomlyreplacingwordsorsentenceswithotherstrings.
Diﬀerent from the above paraphrasing methods, this method usually avoids us-
ing strings that are semantically similar to the original data.
Someworksimplementsubstitutionthroughexistingouterresources. Coulombe
et al. [7] and Regina et al. [10] introduce a list of the most common misspellings
in English to generate augmented texts containing common misspellings.10For
example, “across” is easily misspelled as “accross”. Xie et al. [23] borrow from
the idea of “word-dropout” and improve generalization by reducing the infor-
mation in the sentence. This work uses “_” as a placeholder to replace random
words, indicating that the information at that position is empty.
Some works use task-related resources or generate random strings for sub-
stitution. Xie et al. [12] and Xie et al. [23] replace the original words with other
words in the vocabulary, and they use the TF-IDF value and the unigram fre-
quency to choose words from the vocabulary, respectively. Lowell et al. [53] and
Daval et al. [42] also explore this method as one of unsupervised data augmenta-
tion methods. Wang et al. [71] propose a method that randomly replaces words
in the input and target sentences with other words in the vocabulary. In NER,
Dai et al. [43] replace the original token with a random token in the training
set with the same label. Qin et al. [72] propose a multi-lingual code-switching
method that replaces original words in the source language with words of other
languages. In the task of task-oriented dialogue, random substitution is a useful
way to generate augmented data. Peng et al. [21] augment input dialogue acts
by replacing slot values to obtain more combinations in spoken language under-
standing. In slot ﬁlling, Louvan et al. [11] do slot substitution according to the
slot label. Song et al. [73] augment the training data for dialogue state tracking
by copying user utterances and replace the corresponding real slot values with
generated random strings.
10A list of common spelling errors in English can be obtained from the online resources of
Oxford Dictionaries: https://en.oxforddictionaries.com/spelling/common-misspellings
15

--- PAGE 16 ---
/info-circleRandom substitution introduces new noisy information that may change
the original label. Tips to avoid this problem:
1. Word level: use label-independent external resources.
2. Sentence level: use other samples with the same labels as the original
data.
/lightbulbNoising
Advantage(s):
1. Noising-based methods improve model robustness.
Disadvantage(s):
1. Poor interpretability.
2. Limited diversity for every single method.
2.3.
 Sampling-based Methods
Sampling-based methods grasp the data distribution and sample new data
within it. Similar to paraphrasing-based models, they also involve rules and
trainedmodelstogenerateaugmenteddata. Thediﬀerenceisthatthesampling-
basedmethodsaretask-speciﬁcandrequiretaskinformationlikelabelsanddata
format.11Such methods not only ensure validity but also increase diversity.
They satisfy more needs of downstream tasks based on artiﬁcial heuristics and
trained models, and can be designed according to speciﬁc task requirements.
Thus, they are usually more ﬂexible and diﬃcult than the former two categories.
2.3.1. Rules
This method uses some rules to directly generate new augmented data.
Heuristics about natural language and the corresponding labels are sometimes
required to ensure the validity of the augmented data. The model structure
is as shown in Figure 11(a). Diﬀerent from the above rule-based paraphrasing
method, this method constructs valid but not guaranteed to be similar to the
original data (even diﬀerent labels).
Min et al. [24] swap the subject and object of the original sentence, and
convert predicate verbs into passive form. For example, inverse “This small
collection contains 16 El Grecos.” into “16 El Grecos contain this small col-
lection.”. The labels of new samples are determined by rules. Liu et al. [25]
apply data augmentation methods in the task of solving math word problems
(MWPs). They ﬁlter out some irrelevant numbers. Then some rules are used
to construct new data based on the idea of double-checking, e.g., constructing
augmented data describing distance =timespeedby reusing the original
data describing time =distance=speed . The output equations of this method
are computationally right. Given the training set of Audio-Video Scene-Aware
11Recall that paraphrasing-based methods are task-independent and only require the origi-
nal sentence as input.
16

--- PAGE 17 ---
(e) Mixup. (d) Self -training. (c) Pretrained Models.(b) Non -pretrained Models. (a) Rules.Figure 11: Sampling-based models.
Dialogue that provides 10 question-answer pairs for each video, Mou et al. [74]
shuﬄe the ﬁrst npairs as dialogue history and take the n+ 1-th question as
what needs to be answered. In natural language inference, Kang et al. [26]
apply external resources like PPDB and artiﬁcial heuristics to construct new
sentences. Then they combine the new sentences with original sentences as
augmented pairs according to rules, for example, if A entails B and B entails
C, then A entails C . Kober et al. [69] deﬁne some rules to construct positive
and negative pairs using adjective-noun (AN) and noun-noun (NN) compounds.
For example, given <car;car> , they construct <fastcar;car> as a positive
sample and <fastcar;redcar > as a negative sample. Shakeel et al. [75] con-
struct both paraphrase annotations and non-paraphrase annotations through
three properties including reﬂexivity, symmetry, and transitive extension. Yin
et al. [76] use two kinds of rules including symmetric consistency and transitive
consistency, as well as logic-guided DA methods to generate DA samples.
/lightbulbRules
Advantage(s):
1. Easy to use.
Limitation(s):
1. Require for artiﬁcial heuristics.
2. Low coverage and limited variation.
2.3.2. Non-pretrained Models
Some methods use non-pretrained models to generate augmented data. Such
methods usually entail the idea of back translation (BT) [77],12which is to
train a target-to-source Seq2Seq model and use the model to generate source
12Note that the idea of back translation here is DIFFERENT from the above paraphrasing
method called “back-translation” in Section 2.1.5.
17

--- PAGE 18 ---
sentences from target sentences, i.e., constructing pseudo-parallel sentences [13].
Such Seq2Seq model learns the internal mapping between the distributions of
the target and the source, as shown in Figure 11(b). This is diﬀerent from the
model generation based paraphrasing method because the augmented data of
the paraphrasing method shares similar semantics with the original data.
Sennrich et al. [78] train an English-to-Chinese NMT model using existing
parallel corpus, and use the target English monolingual corpus to generate Chi-
nese corpus through the above English-to-Chinese model. Kang et al. [26] train
a Seq2Seq model for each label ( entailment ,contradiction , andneutral) and
then generate new data using the Seq2Seq model given a sentence and a speciﬁc
label. Chen et al. [79] adopt the Tranformer architecture and map the “rewrite
utterance!request utterance” to the machine translation process. Moreover,
they enforce the optimization process of the Seq2Seq generation with a policy
gradient technique for controllable rewarding. Zhang et al. [13] use Transformer
as the encoder and transfer the knowledge from Grammatical Error Correction
to Formality Style Transfer. Raille et al. [27] create the Edit-transformer, a
Transformer-based model works cross-domain. Yoo et al. [80] propose a novel
VAE model to output the semantic slot sequence and the intent label given an
utterance.
/lightbulbNon-pretrained Models
Advantage(s):
1. Strong diversity.
2. Strong application.
Limitation(s):
1. Require training data.
2. High training diﬃculty.
2.3.3. Pretrained Models
In recent years, large-scale language models (LM) have achieved great suc-
cess by acquiring rich linguistic knowledge through pretraining. Thus, they are
naturally used as augmentation tools, as shown in Figure 11(c).
Tavor et al. [28] propose a data augmentation method named LAMBDA.
They generate labeled augmented sentences with GPT-2, which is ﬁne-tuned
on the training set in advance. Then the augmented sentences are ﬁltered by a
classiﬁer to ensure the data quality. Kumar et al. [29] applies a similar method
without the classiﬁer for ﬁltering.
Some works adopt masked language models to obtain augmented data. Ng
et al. [30] use the masked language model to construct a corruption model and a
reconstruction model. Given the input data points, they initially generate data
far away from the original data manifold with the corruption model. Then the
reconstruction model is used to pull the data point back to the original data
manifold as the ﬁnal augmented data.
Some works adopt auto-regressive models to obtain augmented data. Peng
et al. [21] use the pre-trained SC-GPT and SC-GPT-NLU to generate utter-
ances and dialogue acts respectively. The results are ﬁltered to ensure the data
18

--- PAGE 19 ---
quality. Abonizio et al. [81] ﬁne-tune DistilBERT [82] on original sentences to
generate synthetic sentences. Especially, GPT-2 is a popular model used for
generating augmented data. Quteineh et al. [32] use label-conditioned GPT-2
to generate augmented data. Tarján et al. [83] generate augmented data with
GPT-2 and retokenize them into statistically derived subwords to avoid the vo-
cabulary explosion in a morphologically rich language. Zhang et al. [44] use
GPT-2 to generate substantially diversiﬁed augmented data in extreme multi-
label classiﬁcation.
/lightbulbPretrained Models
Advantage(s):
1. Strong diversity.
2. Strong application.
Limitation(s):
1. Require training data.
2.3.4. Self-training
In some scenarios, unlabeled raw data is easy to obtain. Thus, converting
such data into valid data would greatly increase the amount of data, as shown
in Figure 11(d).
Thakur et al. [31] ﬁrst ﬁne-tune BERT on the original data, then use the
ﬁne-tuned BERT to label unlabeled sentence pairs. Such augmented data, as
well as the gold data, are used to train SBERT together. Miao et al. [84] further
introduce data distillation into the self-training process. They output the label
of unlabeled data by the iteratively updated teacher model. Yang et al. [85]
apply a similar self-training method in question answering; a cross-attention-
based teacher model is used to determine the label of each QA pair. Du et
al. [33] introduce SentAugment, a data augmentation method that computes
task-speciﬁc query embeddings from labeled data to retrieve sentences from a
bank of billions of unlabeled sentences crawled from the web.
Some methods directly transfer exsiting models from other tasks to gen-
erate pseudo-parallel corpus. Montella et al. [34] make use of Wikipedia to
leverage a massive sentences. Then they use Stanford OpenIE package to
extract the triplets given Wikipedia sentences. For example, given “ Barack
Obama was born in Hawaii. ”, the returned triples by Stanford OpenIE are
< BarackObama ;was;born > and < BarackObama ;wasbornin ;Hawaii >
Such mappings are ﬂipped as the augmented data of RDF-to-text tasks. Alek-
sandr et al. [63] apply a similar method. Since BERT does well on object-
property (OP) relationship prediction and object-aﬀordance (OA) relationship
prediction, Zhao et al. [86] directly use a ﬁne-tuned BERT to predict the label
of OP and OA samples.
19

--- PAGE 20 ---
/lightbulbSelf-training
Advantage(s):
1. Easier than generative models.
2. Suitable for data-sparse scenarios.
Disadvantage(s):
1. Require for unlabeled data.
2.3.5. Mixup
This method uses virtual embeddings instead of generated natural language
form text as augmented samples. The existing data is used as the basis to
sample in the virtual vector space, and the sampled data may have diﬀerent
labels than the original data.
The idea of Mixup ﬁrst appears in the image ﬁeld by Zhang et al. [87]. In-
spired by this work, Guo et al. [35] propose two variants of Mixup for sentence
classiﬁcation. The ﬁrst one called wordMixup conducts sample interpolation in
the word embedding space, and the second one called senMixup interpolates the
hidden states of sentence encoders. The interpolated new sample through word-
Mixup as well as senMixup, and their common interpolated label are obtained
as follows:
eBij
t=Bi
t+ (1 )Bj
t; (1)
eBij
fkg=f(Bi)fkg+ (1 )f(Bj)fkg; (2)
~yij=yi+ (1 )yj; (3)
in whichBi
t;Bj
t2RNddenote the t-th word in two original sentences, and
f(Bi);f(Bj)denote the hidden layer sentence representation. Moreover, yi;yj
are the corresponding original labels.
Mixup is widely applied in many works recently. Given the original samples,
Cheng et al. [36] ﬁrstly construct their adversarial samples following [88], and
then apply two Mixup strategies named PadvandPaut: The former interpolates
between adversarial samples, and the latter interpolates between the two cor-
responding original samples. Similarly, Sun et al. [89], Bari et al. [90] , and Si
et al. [91] both apply such Mixup method for text classiﬁcation. Sun et al. [89]
proposeMixup-TransformerwhichcombinesMixupwithtransformer-basedpre-
trained architecture. They test its performance on text classiﬁcation datasets.
Chen et al. [92] introduce Mixup into NER, proposing both Intra-LADA and
InterLADA.
/lightbulbMixup
Advantage(s):
1. Generating augmented data between diﬀerent labels.
Disadvantage(s):
1. Poor interpretability.
20

--- PAGE 21 ---
Table 1: Characteristics of diﬀerent DA methods. Learnable denotes whether the methods
involve model training; onlineandoﬄinedenote whether the DA process is during or af-
ter model training. Ext.Know denotes to whether the methods require external knowledge
resources to generate augmented data. Pretrain denotes whether the methods require a pre-
trained model. Task-related denotes whether the methods consider the label information,
task format, and task requirements to generate augmented data. Leveldenotes the depth and
extent to which elements of the instance/data are modiﬁed by the DA; t,e, andldenote text,
embedding, and label, respectively. Granularity indicates the extent to which the method
could augment; w,p, andsdenote word, phrase, and sentence, respectively.
Learnable Ext.Know Pretrain Task-related Level GranularityParaphrasingThesauruses - 3 - - t w
Semantic Embeddings - 3 - - t w; p
Language Models - - 3 - t w
Rules - 3 - - t w; p; s
Machine Translation - - - - t s
Model Generation oﬄine - - 3 t sNoisingSwapping - - - - t w; p; s
Deletion - - - - t w; p; s
Insertion - 3 - - t w; p; s
Substitution - 3 - - t w; p; sSamplingRules - 3 - 3 t; l w; p; s
Non-pretrained oﬄine - - 3 t; l s
Pretrained oﬄine - 3 3 t; l s
Self-training oﬄine - - 3 t; l s
Mixup online - - 3 e; l s
2.4. Analysis
As shown in Table 1, we compare the above DA methods by various aspects.
•It is easy to ﬁnd that nearly all paraphrasing-based and noising-based
methods are not learnable, except for Seq2Seq andMixup. However, most
sampling-based methods are learnable except for the rule-based ones.
Learnable methods are usually more complex than non-learnable ones,
thus sampling-based methods generate more diverse and ﬂuent data than
the former two.
•Among all learnable methods, Mixupis the only onlineone. That is to
say, the DA process is during model training. Thus, Mixupis the only one
that outputs cross-label and discrete embedding from augmented data.
•Comparing Learnable andResource , we could see that most non-learnable
methods require external knowledge resources which go beyond the orig-
inal dataset and task deﬁnition. Commonly used resources include se-
mantic thesauruses like WordNet and PPDB, handmade resources like
misspelling dictionary in [7], and artiﬁcial heuristics like the ones in [24]
and [26].
•Through Learnable ,Ext.Know andPretrain, it can be seen that in ad-
dition to artiﬁcial heuristics, DA requires other external interventions to
21

--- PAGE 22 ---
generate valid new data. This includes model training objectives, exter-
nal knowledge resources, and knowledge implicit in pretrained language
models.
•Comparing Learnable andTask-related , wecouldseethatallparaphrasing-
based and noising-based methods except model generation are not task-
related. They generate augmented data given only original data without
labels or task deﬁnition. However, all sampling-based methods are task-
related because heuristics and model training are adopted to satisfy the
needs of speciﬁc tasks.
•Comparing LevelandTask-related , we could see that they are relevant.
The paraphrasing-based methods are at the text level. The same is true
for noising-based methods, except for Mixup, which augments both em-
beddings and labels. All sampling-based methods are at the text and label
level since the labels are also considered and constructed during augmen-
tation.
•Comparing Learnable andGranularity , we could see that almost all non-
learnable methods could be used for word-level and phrase-level DA, but
all learnable methods could only be applied for sentence-level DA. Al-
though learnable methods generate high-quality augmented sentences, un-
fortunately, they do not work for document augmentation because of their
weaker processing ability for documents. Thus, document augmentation
still relies on simple non-learnable methods, which is also a current situa-
tion we have observed in our research.
3. Strategies and Tricks
The three types of DA methods including paraphrasing, noising, and sam-
pling, as well as their characteristics, have been introduced above. In practical
applications, the eﬀect of the DA method is inﬂuenced by many factors. In this
chapter, we present these factors to inspire our readers to use some strategies
and tricks for selecting and constructing suitable DA methods.
3.1. Method Stacking
The methods in Section 2 are not mandatory to be applied alone. They
could be combined for better performance. Common combinations include:
The Same Type of Methods .Some works combine diﬀerent paraphrasing-
based methods and obtain diﬀerent paraphrases, to increase the richness of aug-
mented data. For example, Liu et al. [49] use both thesauruses and semantic
embeddings, andJiaoetal.[9]usebothsemanticembeddingsandMLMs. Asfor
noising-based methods, the former unlearnable ways are usually used together
like [21]. It is because these methods are simple, eﬀective, and complemen-
tary. Some methods also adopt diﬀerent sources of noising or paraphrasing like
[10] and [23]. The combination of diﬀerent resources could also improve the
robustness of the model.
22

--- PAGE 23 ---
Unsupervised Methods .In some scenarios, the simple and task-independent
unsupervised DA methods could meet the demand. Naturally, they are grouped
together and widely used. Wei et al. [6] introduce a DA toolkit called EDA that
consists of synonym replacement, random insertion, random swap, and random
deletion. EDA is very popular and used for many tasks ([61, 62]). UDA by Xie
et al [12] includes back-translation and unsupervised noising-based methods; it
is also used in many tasks like [42].
Multi-granularity .Some works apply the same method at diﬀerent levels to
enrich the augmented data with changes of diﬀerent granularities and improve
the robustness of the model. For example, Wang et al. [8] train both word
embeddings and frame embeddings by Word2Vec; Guo et al. [35] apply Mixup
at the word and sentence level, and Yu et al. [22] use a series of noising-based
methods at both the word and the sentence level.
3.2. Optimization
The optimization process of DA methods directly inﬂuences the quality of
augmented data. We introduce it through four angles: the use of augmented
data, hyperparameters, training strategies, and training objects.
3.2.1. The Use of Augmented Data
The way of using augmented data directly inﬂuences the ﬁnal eﬀect. From
the perspective of data quality, the augmented data could be used to pre-train
a model if it is not of high quality; otherwise, it could be used to train a
model directly. From the perspective of data amount, if the amount of the
augmented data is much higher than the original data, they are usually not
directly used together for model training. Instead, some common practices
include (1) oversampling the original data before training the model (2) pre-
training the model with the augmented data and ﬁne-tuning it on the original
data.
3.2.2. Hyperparameters
All the above methods involve hyperparameters that largely aﬀect the aug-
mentation eﬀect. We list some common hyperparameters in Figure 12:
3.2.3. Training Strategies
Some works apply training strategies based on the basic data augmentation
methods. For example, Qu et al. [66] combine back-translation with adversarial
training. Similarly, Quteineh et al. [32] transform the basic pre-trained model
into an optimization problem13to maximize the usefulness of the generated
output. Hu et al. [93] and Liu et al. [94] use pre-trained language models to
generate augmented data, and transfer such progress into reinforcement learn-
ing. Some works ([62, 95]) take the idea of Generative Adversarial Networks to
generate challenging augmented data.
13Monte Carlo Tree Search.
23

--- PAGE 24 ---
MethodsParaphrasing1. Thesauruses
2. Semantic
Embeddings
3. Language
Models
4. Rules(1) Number of replacements
(2) Probability of replacement
5. Machine
Translation(1) Number of (intermediate) languages
(2) Types of (intermediate) languages
6. Model
Generation(1) Parameters in the neural network
Noising1. Swapping
2. Deletion
3. Insertion
4. Substitution(1) Number of operations
(2) Probability of operations
Sampling1. Rules (1) Number of replacements
2. Non
-Pretrained
3. Pretrained
4. Self-training
5. Mixup(1) Parameters in the neural network
Figure 12: Hyperparameters that aﬀect the augmentation eﬀect in each DA method.
3.2.4. Training Objects
Training objects are essential for model training, especially for the learnable
DA methods. Nugent et al. [65] propose a range of softmax temperature settings
to ensure diversity while preserving semantic meaning. Hou et al. [68] use
duplication-awareattentionanddiverse-orientedregularizationtogeneratemore
diverse sentences. Cheng et al. [36] employ curriculum learning to encourage
the model to focus on the diﬃcult training examples.
3.3. Filtering
Sometimes the progress of data augmentation inevitably introduces some
noiseevenerrors,thusﬁlteringmechanismsareintroducedtoavoidthisproblem.
Some works ﬁlter input data in the initial stage to avoid inappropriate input
aﬀecting the augmentation eﬀect. A typical example is sentence length, i.e.,
ﬁlter sentences that are too short ([17]). Liu et al. [25] ﬁlter out irrelevant
numbers without augmenting them in solving Math Word Problems, to ensure
the generated data is computationally right.
Inaddition, someworksﬁlterthesyntheticaugmenteddataatthe end-stage.
This is usually achieved through a model. For example, Zhang et al. [13] employ
a discriminator to ﬁlter the back-translation results. Tavor et al. [28] and Peng
et al. [21] both apply a classiﬁer to ﬁlter the augmented sentences generated by
pre-trained models to ensure the data quality.
24

--- PAGE 25 ---
Table 2: The application of DA methods in NLP tasks. Note that if a paper involves multiple
methods, we count it multiple times.
Text Text Structure
Classiﬁcation Generation PredictionParaphrasingThesauruses [5], [6], [49], [7],
[42], [61], [44], [45],
[97], [98]- [42], [43]
Embeddings [8], [49], [98] - -
Language Models [10], [52], [55], [51],
[99][56] -
Rules [10], [7], [11] - [100], [101]
Machine Translation [42], [61], [10], [12],
[60], [62], [64], [7],
[19], [67], [102],
[97], [103][13], [59] [42], [58], [15],
[104]
Model Generation [18], [69], [105],
[106], [107][18], [108], [109],
[110][18], [16], [68],
[17], [111], [76]NoisingSwapping [6], [61], [44], [62],
[20], [19], [112],
[98]- [43]
Deletion [6], [61], [44], [62],
[20], [22], [113][21] -
Insertion [6], [61], [44], [62],
[98][21] -
Substitution [42], [10], [12], [7],
[102][23], [71], [21] [42], [11], [43],
[114]SamplingRules [24], [26], [69], [75],
[102], [115], [116],
[117][74], [118], [119],
[120][121]
Non-Pretrained [26], [27], [80],
[122], [123][13], [79], [124],
[125], [78][80], [126]
Pretrained [44], [30], [29], [32],
[94], [28], [81],
[127], [128][21], [30], [83] [21], [129]
Self-training [33], [85], [63], [84] [34], [130] [85]
Mixup [35], [89], [91],
[131][36] [92]
4. Applications on NLP Tasks
Although a variety of data augmentation methods have emerged in the ﬁeld
of NLP in recent years, it is diﬃcult to directly compare their performance. This
is because diﬀerent tasks, evaluation metrics, datasets, model architectures,
and experimental settings make direct comparisons meaningless. Therefore,
basedontheworkintroducedabove, weanalyzethedataaugmentationmethods
from the perspective of diﬀerent NLP tasks including text classiﬁcation, text
generation, and structured prediction [96].
•Text classiﬁcation is the simplest and most basic natural language pro-
cessing problem. That is, for a piece of text input, output the category to
25

--- PAGE 26 ---
which the text belongs, where the category is a pre-deﬁned closed set.14
•Textgeneration, asthenameimplies, istogeneratethecorrespondingtext
given the input data. The most classic example is machine translation.
•The structured prediction problem is usually unique to NLP. Diﬀerent
from the text classiﬁcation, there are strong correlation and format re-
quirements between the output categories in the structured prediction
problem.
In this section, we try to analyze the features as well as the development status
of DA in these tasks. Some statistical results are shown in Table 2 and Table 3.
DA methods are applied more widely in text classiﬁcation than other NLP
tasks in general and in each category. Moreover, each individual DA method
could be applied to text classiﬁcation. Such application advantage is because of
thesimpleformoftextclassiﬁcation: giventheinputtext,itdirectlyinvestigates
the model’s understanding of semantics by label prediction. Therefore, it is
relatively simple for data augmentation to only consider retaining the semantics
of words that are important for classiﬁcation.
As for text generation, it prefers sampling-based methods to bring more se-
manticdiversity. Andstructuredpredictionprefersparaphrasing-basedmethods
because it is sensitive to data format. Thus, it has higher requirements for data
validity.
By comparing each DA method, we can see that simple and eﬀective un-
supervised methods, including machine translation, thesaurus-based paraphras-
ing, and random substitution, are quite popular. In addition, learnable methods
like paraphrasing-based model generation and sampling-based pretrained mod-
els, also gain a lot of attention because of their diversity and eﬀectiveness.
We also show the development process of the DA method on three types of
tasks through a timeline (Table 3). On the whole, the number of applications
of DA in these tasks has increased these years. Text classiﬁcation is the ﬁrst
task to use DA, and the number of corresponding papers is also larger than
the other two tasks. In terms of text generation and structured prediction,
DA is receiving increasing attention. Paraphrasing-based methods have always
been a popular method. In recent years, sampling-based methods show clear
momentum in text classiﬁcation and text generation, because they bring more
gainstopowerfulpretrainedlanguagemodelsthanparaphrasing-basedmethods.
However, people still tend to use paraphrasing and noising-based methods in
structured prediction.
14Text matching tasks such as Natural Language Inference can also be transformed into
text classiﬁcation.
26

--- PAGE 27 ---
Table 3 Timeline of DA methods applied in three kinds of NLP tasks. The time for each
paper is based on its ﬁrst arXiv version (if exists) or estimated submission time. P denotes
paraphrasing-based methods; N denotes noising-based methods; S denotes sampling-based
methods.
Text Classiﬁcation Text Generation Structured Prediction
2015.09 •Zhang et al. [5] P • •
•Wang et al. [8] P • •
2015.11 • •Sennrich et al. [78] S •
2016.01 •Xu et al. [115] S • •
...• • •
2017.03 • •Xie et al. [23] N •
2017.05 • •Fadaee et al. [56] P •
...• • •
2018.04 • • •Yu et al. [58] P
2018.05 •Kang et al. [26] S • •
2018.06 •Kobayashi et al. [55] P • •
2018.07 • • •Hou et al. [16] P
2018.08 •Aroyehun et al. [64] P •Wang et al. [71] N •
•Risch et al.[103] P • •
2018.09 •Yoo et al. [80] S • •Yoo et al. [80] S
2018.10 •Du et al. [112] N • •Sahin et al. [100] P
2018.12 •Coulombe et al. [7] P, N • •
•Wu et al. [51] P • •
2019.01 •Wei et al. [6] P, N • •
2019.04 •Xie et al. [12] P, N • •
2019.05 •Guo et al. [35] S •Gao et al. [132] N •
2019.06 • •Xia et al. [133] S •
• •Bergmanis et al. [120] S •
• •Kumar et al. [109] P •
2019.07 •Yu et al. [22] N •Li et al. [110] P •Zmigrod et al. [121] S
2019.08 • • •Yin et al. [76] P
2019.09 •Luque et al. [19] P, N • •
•Yan et al. [20] N • •
27

--- PAGE 28 ---
Text Classiﬁcation Text Generation Structured Prediction
2019.11 •Anaby et al. [28] S • •Longpre et al. [104] P
•Malandrakis et al. [134] P • •
•Niu et al. [123] S • •
•Zhao et al. [107] P • •
2019.12 •Shakeel et al. [75] S • •
2020.01 • • •Yoo et al. [111] P
2020.03 •Kumar et al. [29] S • •
•Raille et al. [27] S • •
2020.04 •Lun et al. [102] P, N, S •Peng et al. [21] N, S •Li et al. [17] P
• • •Peng et al. [21] S
2020.05 •Kober et al. [69] P, S •Zhang et al. [13] P, S •
•Cao et al. [135] S • •
2020.06 •Liu et al. [49] P •Cheng et al. [36] S •
•Qin et al. [72] N • •Qin et al. [72] N
2020.07 •Min et al. [24] S •Chen et al. [125] S •Andreas et al. [101] P
•Rastogi et al. [62] P, N •Tarjan et al. [83] S •
•Regina et al. [10] P, N •Mou et al. [74] S •
•Asai et al. [119] S • •
2020.09 •Ng et al. [30] S •Ng et al. [30] S •Yang et al. [85] S
•Zhang et al. [44] P,N, S •Zhang et al. [118] S •
2020.10 •Barrire et al. [67] P •Fabbri et al. [59] P •Liu et al. [18] P
•Louvan et al. [11] P • •Louvan et al. [11] N
•Tapia-Téllez et al. [52] P • •Chen et al. [92] S
•Sun et al. [89] S • •Dai et al. [43] P, N
•Abonizio et al. [81] S • •Riabi et al. [129] S
•Zuo et al. [45] P • •
2020.11 •Longpre et al. [61] P, N • •
•Quteineh et al. [32] S • •
2020.12 •Miao et al. [84] S •Wan et al. [108] P •Bornea et al. [15] P
•Daval et al. [42] P ,N •Yao et al. [124] •Hou et al. [68] P
•Liu et al. [94] S •Montella et al. [34] S S •Daval et al. [42] P ,N
•Aleksandr et al. [63] S •Chen et al. [79] S •
•Si et al. [91] S • •
•Xu et al. [105] P • •
•Liu et al. [97] P • •
•Guo et al. [106] P • •
•Si et al. [131] S • •
2021.01 •Shi et al. [114] N • •Shi et al. [114] N
•Staliunaite et al. [127] S • •
•Dong et al. [128] S • •
2021.06 •Chen et al. [116] S •Xu et al. [130] S •
•Chen et al. [113] N • •
•Jiang et al. [117] S • •
•Kovatchev et al. [98] P, N • •
2021.08 •Bari et al. [99] P • •Liu et al. [126] S
28

--- PAGE 29 ---
5. Related Topics
How does data augmentation relate to other learning methods? In this
section, we connect data augmentation with other similar topics.
5.1. Pretrained Language Models
The training of most pre-trained language models (PLMs) is based on self-
supervisedlearning. Self-supervisedlearningmainlyusesauxiliarytaskstomine
its supervised information from large-scale unsupervised data, and trains the
network through this constructed supervised information, so that it can learn
valuable representations for downstream tasks. From this perspective, PLMs
also introduce more training data into downstream tasks, in an implicit way.
On the other hand, the general large-scale unsupervised data of PLMs may be
out-of-domain for speciﬁc tasks. Diﬀerently, the task-related data augmentation
methods essentially focus on speciﬁc tasks.
5.2. Contrastive Learning
Contrastive learning is to learn an embedding space in which similar samples
are close to each other while dissimilar ones are far apart. It focuses on learning
the common features between similar samples and distinguishing the diﬀerences
between dissimilar ones. The ﬁrst step of contrastive learning is applying data
augmentation to construct similar samples with the same label, and the second
step is to randomly choose instances as the negative samples. Thus, contrastive
learning is one of the applications of data augmentation.
5.3. Other Data Manipulation Methods
In addition to DA, there are some other data manipulation methods to im-
prove model generalization [136, 93]. Oversampling is usually used in data im-
balance scenarios. It simply samples original data from the minority group as
new samples, instead of generating augmented data. Data cleaning is addition-
ally applied to the original data to improve data quality and reduce data noise.
It usually includes lowercasing, stemming, lemmatization, etc. Data weight-
ingassigns diﬀerent weights to diﬀerent samples according to their importance
during training, without generating new data. Data synthesis provides entire
labeled artiﬁcial examples instead of augmented data generated by models or
rules.
5.4. Generative Adversarial Networks
Generative Adversarial Networks (GANs) are ﬁrst introduced by Goodfellow
et al. [137]. As a type of semi-supervised method, GANs include the generative
model, which is mainly used to challenge the discriminator of GANs, while the
generative models in some DA methods are directly used to augment training
data. Moreover, the generative model of GANS is applied as a DA method
in some scenes like [62, 138, 95, 69, 122, 135], and have demonstrated to be
eﬀective for data augmentation purposes.
29

--- PAGE 30 ---
5.5. Adversarial Attacks
Adversarial attacks are techniques to generate adversarial examples attack-
ing a machine learning model, i.e., causing the model to make a mistake. Some
works use DA methods like code-switch substitution to generate adversarial
examples as consistency regularization [139].
6. Challenges and Opportunities
Data augmentation has seen a great process over the last few years, and it
has provided a great contribution to large-scale model training as well as the
development of downstream tasks. Despite the process, there are still challenges
to be addressed. In this section, we discuss some of these challenges and future
directions that could help advance the ﬁeld.
Theoretical Narrative .At this stage, there appears to be a lack of system-
atic probing work and theoretical analysis of DA methods in NLP. The few
related works are of DA in the image domain, considering data augmentation
as encoding a priori knowledge about data or task invariance [140], variance
reduction [141] or regularization methods [142]. In NLP, Most previous works
propose new methods or prove the eﬀectiveness of the DA method on down-
stream tasks, but do not explore the reasons and laws behind it, e.g., from
the perspective of mathematics. The discrete nature of natural language makes
theoretical narrative essential since narrative helps us understand the nature of
DA, without being limited to determining eﬀectiveness through experiments.
More Exploration on Pretrained Language Models .In recent years, pre-
trained language models have been widely applied in NLP, which contain rich
knowledge through self-supervision on a huge scale of corpora. There are
works using pre-trained language models for DA, but most of them are lim-
ited to [MASK] completion [51], direct generation after ﬁne-tuning [44], or self-
training [33]. Is DA still helpful in the era of pre-trained language models? Or,
how to further use the information in pre-trained models to generate more di-
verse and high-quality data with less cost? There are some initial explorations
in these directions [143, 144], while we still look forward to more works in the
future.
Few-shot Scenarios .In few-shot scenarios, models are required to achieve
performance which rivals that of traditional machine learning models, yet the
amount of training data is extremely limited. DA methods provide a direct
solution to the problem. However, most current works in few-shot scenarios
are paraphrasing-based methods [59]. Such methods ensure the validity of the
augmented data, but also lead to insuﬃcient semantic diversity. Mainstream
pretrained language models obtain rich semantic knowledge by language mod-
eling. Such knowledge even covers to some extent the semantic information
introduced by traditional paraphrasing-based DA methods. In other words, the
improvement space that traditional DA methods bring to pretrained language
30

--- PAGE 31 ---
models has been greatly compressed. Therefore, it is an interesting question
how to provide models with fast generalization and problem solving capability
by generating high quality augmented data in few-shot scenarios.
Retrieval Augmentation .Retrieval-augmented language models integrate
retrieval into pre-training and downstream usage [145, 146].15Retrieval aug-
mentation makes models much more parameter-eﬃcient, as they need to store
less knowledge in their parameters and can instead retrieve it. It also enables
eﬃcient domain adaptation by simply updating the data used for retrieval [147].
Recently, the size of the retrieval corpora has achieved explosive growth [148]
and models have been equipped with the ability to query the web for answering
questions [149, 150]. In the future, there may be diﬀerent forms of retrieval to
leverage diﬀerent kinds of information such as common sense knowledge, fac-
tual relations, linguistic information, etc. Retrieval augmentation could also be
combined with more structured forms of knowledge retrieval, such as methods
from knowledge base population and open information extraction.
More Generalized Methods for NLP .Natural language is most diﬀerent
from image or sound in that its representation is discrete. At the same time,
NLP includes speciﬁc tasks such as structured prediction that are not available
inothermodalities. Therefore,unlikegeneralmethodssuchas clippingforimage
augmentation or speed perturbation for audio augmentation, there is currently
no DA method that can be eﬀective for all NLP tasks. This means that there is
still a gap for DA methods between diﬀerent NLP tasks. With the development
of pre-trained models, this seems to have some possibilities. Especially the
proposal of T5 [151] and GPT3 [152], as well as the emergence of prompting
learning further verify that the formalization of tasks in natural language can be
independent of the traditional categories, and a more generalized model could
be obtained by unifying task deﬁnitions.
Working with Long Texts and Low Resources Languages .The existing
methods have made signiﬁcant progress in short texts and common languages.
However, limited by model capabilities, DA methods on long texts still struggle
with the simplest methods of paraphrasing and noising [49, 20, 22] (as shown in
Table 1). At the same time, limited by data resources, augmentation methods
of low resource languages are scarce [29], although they have more demand
for data augmentation. Obviously, exploration in these two directions is still
limited, and they could be promising directions.
7. Conclusion
In this paper, we presented a comprehensive and structured survey of data
augmentation for natural language processing. In order to inspect the nature
15Seehttps://ruder.io/ml-highlights-2021/ for further information.
31

--- PAGE 32 ---
of DA, we framed DA methods into three categories according to the diver-
sityof augmented data, including paraphrasing, noising, and sampling. Such
categories help to understand and develop DA methods. We also introduced
the characteristics of DA methods and their applications in NLP tasks, then
analyzed them through a timeline. In addition, we introduced some tricks and
strategies so that researchers and practitioners can refer to obtain better model
performance. Finally, we distinguish DA with some related topics and outlined
current challenges as well as opportunities for future research.
References
[1] C. Shorten, T. M. Khoshgoftaar, A survey on image data augmen-
tation for deep learning, J. Big Data 6 (2019) 60. doi:10.1186/
s40537-019-0197-0 .
URL https://doi.org/10.1186/s40537-019-0197-0
[2] P. Liu, X. Wang, C. Xiang, W. Meng, A survey of text data aug-
mentation, in: 2020 International Conference on Computer Communi-
cation and Network Security (CCNS), IEEE, 2020, pp. 191–195. doi:
10.1109/CCNS50731.2020.00049 .
[3] S. Y. Feng, V. Gangal, J. Wei, S. Chandar, S. Vosoughi, T. Mitamura,
E. Hovy, A survey of data augmentation approaches for NLP, in: Findings
of the Association for Computational Linguistics: ACL-IJCNLP 2021,
Association for Computational Linguistics, Online, 2021, pp. 968–988.
doi:10.18653/v1/2021.findings-acl.84 .
URL https://aclanthology.org/2021.findings-acl.84
[4] M. Bayer, M. Kaufhold, C. Reuter, A survey on data augmentation for
text classiﬁcation, CoRR abs/2107.03158 (2021). arXiv:2107.03158 .
URL https://arxiv.org/abs/2107.03158
[5] X. Zhang, J. J. Zhao, Y. LeCun, Character-level convolutional networks
for text classiﬁcation, in: C. Cortes, N. D. Lawrence, D. D. Lee,
M. Sugiyama, R. Garnett (Eds.), Advances in Neural Information Pro-
cessing Systems 28: Annual Conference on Neural Information Processing
Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, 2015,
pp. 649–657.
URL https://proceedings.neurips.cc/paper/2015/hash/
250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html
[6] J. W. Wei, K. Zou, EDA: easy data augmentation techniques for boost-
ing performance on text classiﬁcation tasks, in: K. Inui, J. Jiang, V. Ng,
X. Wan (Eds.), Proceedings of the 2019 Conference on Empirical Meth-
ods in Natural Language Processing and the 9th International Joint Con-
ference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong
32

--- PAGE 33 ---
Kong, China, November 3-7, 2019, Association for Computational Lin-
guistics, 2019, pp. 6381–6387. doi:10.18653/v1/D19-1670 .
URL https://doi.org/10.18653/v1/D19-1670
[7] C. Coulombe, Text data augmentation made simple by leveraging NLP
cloud apis, ArXiv abs/1812.04718 (2018). arXiv:1812.04718 .
URL http://arxiv.org/abs/1812.04718
[8] W.Y.Wang, D.Yang, That’ssoannoying!!!: Alexicalandframe-semantic
embedding based data augmentation approach to automatic categoriza-
tion of annoying behaviors using #petpeeve tweets, in: L. Màrquez,
C. Callison-Burch, J. Su, D. Pighin, Y. Marton (Eds.), Proceedings of
the 2015 Conference on Empirical Methods in Natural Language Process-
ing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, 2015, pp.
2557–2563. doi:10.18653/v1/d15-1306 .
URL https://doi.org/10.18653/v1/d15-1306
[9] X.Jiao, Y.Yin, L.Shang, X.Jiang, X.Chen, L.Li, F.Wang, Q.Liu, Tiny-
bert: Distilling BERT for natural language understanding, in: T. Cohn,
Y. He, Y. Liu (Eds.), Findings of the Association for Computational Lin-
guistics: EMNLP2020, OnlineEvent, 16-20November2020, Vol.EMNLP
2020ofFindingsofACL,AssociationforComputationalLinguistics, 2020,
pp. 4163–4174. doi:10.18653/v1/2020.findings-emnlp.372 .
URL https://doi.org/10.18653/v1/2020.findings-emnlp.372
[10] M.Regina, M.Meyer, S.Goutal, Textdataaugmentation: Towardsbetter
detection of spear-phishing emails, arXiv abs/2007.02033 (2020). arXiv:
2007.02033 .
URL https://arxiv.org/abs/2007.02033
[11] S. Louvan, B. Magnini, Simple is better! lightweight data augmentation
for low resource slot ﬁlling and intent classiﬁcation, in: M. L. Nguyen,
M. C. Luong, S. Song (Eds.), Proceedings of the 34th Paciﬁc Asia Confer-
ence on Language, Information and Computation, PACLIC 2020, Hanoi,
Vietnam, October24-26, 2020, AssociationforComputationalLinguistics,
2020, pp. 167–177.
URL https://aclanthology.org/2020.paclic-1.20/
[12] Q. Xie, Z. Dai, E. H. Hovy, T. Luong, Q. Le, Unsupervised data
augmentation for consistency training, in: H. Larochelle, M. Ranzato,
R. Hadsell, M. Balcan, H. Lin (Eds.), Advances in Neural Information
Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual,
2020.
URL https://proceedings.neurips.cc/paper/2020/hash/
44feb0096faa8326192570788b38c1d1-Abstract.html
[13] Y. Zhang, T. Ge, X. Sun, Parallel data augmentation for formality style
transfer, in: D. Jurafsky, J. Chai, N. Schluter, J. R. Tetreault (Eds.),
33

--- PAGE 34 ---
Proceedings of the 58th Annual Meeting of the Association for Com-
putational Linguistics, ACL 2020, Online, July 5-10, 2020, Association
for Computational Linguistics, 2020, pp. 3221–3228. doi:10.18653/v1/
2020.acl-main.294 .
URL https://doi.org/10.18653/v1/2020.acl-main.294
[14] S. Nishikawa, R. Ri, Y. Tsuruoka, Data augmentation for learning bilin-
gual word embeddings with unsupervised machine translation, CoRR
abs/2006.00262 (2020). arXiv:2006.00262 .
URL https://arxiv.org/abs/2006.00262
[15] M. A. Bornea, L. Pan, S. Rosenthal, R. Florian, A. Sil, Multilingual trans-
fer learning for QA using translation as data augmentation, in: Thirty-
Fifth AAAI Conference on Artiﬁcial Intelligence, AAAI 2021, Thirty-
Third Conference on Innovative Applications of Artiﬁcial Intelligence,
IAAI 2021, The Eleventh Symposium on Educational Advances in Ar-
tiﬁcial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, AAAI
Press, 2021, pp. 12583–12591.
URL https://ojs.aaai.org/index.php/AAAI/article/view/17491
[16] Y. Hou, Y. Liu, W. Che, T. Liu, Sequence-to-sequence data augmentation
for dialogue language understanding, in: E. M. Bender, L. Derczynski,
P. Isabelle (Eds.), Proceedings of the 27th International Conference on
Computational Linguistics, COLING 2018, Santa Fe, New Mexico, USA,
August 20-26, 2018, Association for Computational Linguistics, 2018, pp.
1234–1245.
URL https://aclanthology.org/C18-1105/
[17] K. Li, C. Chen, X. Quan, Q. Ling, Y. Song, Conditional augmentation
foraspecttermextractionviamaskedsequence-to-sequencegeneration, in:
D.Jurafsky,J.Chai,N.Schluter,J.R.Tetreault(Eds.),Proceedingsofthe
58th Annual Meeting of the Association for Computational Linguistics,
ACL 2020, Online, July 5-10, 2020, Association for Computational Lin-
guistics, 2020, pp. 7056–7066. doi:10.18653/v1/2020.acl-main.631 .
URL https://doi.org/10.18653/v1/2020.acl-main.631
[18] D. Liu, Y. Gong, J. Fu, Y. Yan, J. Chen, J. Lv, N. Duan, M. Zhou,
Tell me how to ask again: Question data augmentation with control-
lable rewriting in continuous space, in: B. Webber, T. Cohn, Y. He,
Y. Liu (Eds.), Proceedings of the 2020 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2020, Online, November
16-20, 2020, Association for Computational Linguistics, 2020, pp. 5798–
5810. doi:10.18653/v1/2020.emnlp-main.467 .
URL https://doi.org/10.18653/v1/2020.emnlp-main.467
[19] F. M. Luque, Atalaya at TASS 2019: Data augmentation and robust em-
beddings for sentiment analysis, in: M. Á. G. Cumbreras, J. Gonzalo,
E. M. Cámara, R. Martínez-Unanue, P. Rosso, J. Carrillo-de-Albornoz,
34

--- PAGE 35 ---
S. Montalvo, L. Chiruzzo, S. Collovini, Y. Gutiérrez, S. M. J. Zafra,
M. Krallinger, M. Montes-y-Gómez, R. Ortega-Bueno, A. Rosá (Eds.),
Proceedings of the Iberian Languages Evaluation Forum co-located with
35th Conference of the Spanish Society for Natural Language Processing,
IberLEF@SEPLN 2019, Bilbao, Spain, September 24th, 2019, Vol. 2421
of CEUR Workshop Proceedings, CEUR-WS.org, 2019, pp. 561–570.
URL http://ceur-ws.org/Vol-2421/TASS_paper_1.pdf
[20] G. Yan, Y. Li, S. Zhang, Z. Chen, Data augmentation for deep learn-
ing of judgment documents, in: Z. Cui, J. Pan, S. Zhang, L. Xiao,
J. Yang (Eds.), Intelligence Science and Big Data Engineering. Big Data
and Machine Learning - 9th International Conference, IScIDE 2019,
Nanjing, China, October 17-20, 2019, Proceedings, Part II, Vol. 11936
of Lecture Notes in Computer Science, Springer, 2019, pp. 232–242.
doi:10.1007/978-3-030-36204-1\_19 .
URL https://doi.org/10.1007/978-3-030-36204-1_19
[21] B. Peng, C. Zhu, M. Zeng, J. Gao, Data augmentation for spoken lan-
guage understanding via pretrained models, arXiv abs/2004.13952 (2020).
arXiv:2004.13952 .
URL https://arxiv.org/abs/2004.13952
[22] S. Yu, J. Yang, D. Liu, R. Li, Y. Zhang, S. Zhao, Hierarchical data aug-
mentation and the application in text classiﬁcation, IEEE Access 7 (2019)
185476–185485. doi:10.1109/ACCESS.2019.2960263 .
URL https://doi.org/10.1109/ACCESS.2019.2960263
[23] Z.Xie, S.I.Wang, J.Li, D.Lévy, A.Nie, D.Jurafsky, A.Y.Ng, Datanois-
ing as smoothing in neural network language models, in: 5th International
Conference on Learning Representations, ICLR 2017, Toulon, France,
April 24-26, 2017, Conference Track Proceedings, OpenReview.net, 2017.
URL https://openreview.net/forum?id=H1VyHY9gg
[24] J. Min, R. T. McCoy, D. Das, E. Pitler, T. Linzen, Syntactic data aug-
mentation increases robustness to inference heuristics, in: D. Jurafsky,
J. Chai, N. Schluter, J. R. Tetreault (Eds.), Proceedings of the 58th An-
nual Meeting of the Association for Computational Linguistics, ACL 2020,
Online, July 5-10, 2020, Association for Computational Linguistics, 2020,
pp. 2339–2352. doi:10.18653/v1/2020.acl-main.212 .
URL https://doi.org/10.18653/v1/2020.acl-main.212
[25] Q.Liu, W.Guan, S.Li, F.Cheng, D.Kawahara, S.Kurohashi, Reverseop-
eration based data augmentation for solving math word problems, CoRR
abs/2010.01556 (2020). arXiv:2010.01556 .
URL https://arxiv.org/abs/2010.01556
[26] D. Kang, T. Khot, A. Sabharwal, E. H. Hovy, Adventure: Adversar-
ial training for textual entailment with knowledge-guided examples, in:
35

--- PAGE 36 ---
I. Gurevych, Y. Miyao (Eds.), Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguistics, ACL 2018, Mel-
bourne, Australia, July 15-20, 2018, Volume 1: Long Papers, Association
for Computational Linguistics, 2018, pp. 2418–2428. doi:10.18653/v1/
P18-1225 .
URL https://aclanthology.org/P18-1225/
[27] G. Raille, S. Djambazovska, C. Musat, Fast cross-domain data augmen-
tation through neural sentence editing, arXiv abs/2003.10254 (2020).
arXiv:2003.10254 .
URL https://arxiv.org/abs/2003.10254
[28] A. Anaby-Tavor, B. Carmeli, E. Goldbraich, A. Kantor, G. Kour, S. Shlo-
mov, N. Tepper, N. Zwerdling, Do not have enough data? deep learning
to the rescue!, in: The Thirty-Fourth AAAI Conference on Artiﬁcial In-
telligence, AAAI 2020, The Thirty-Second Innovative Applications of Ar-
tiﬁcial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium
on Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New York,
NY, USA, February 7-12, 2020, AAAI Press, 2020, pp. 7383–7390.
URL https://aaai.org/ojs/index.php/AAAI/article/view/6233
[29] V. Kumar, A. Choudhary, E. Cho, Data augmentation using pre-trained
transformer models, arXiv abs/2003.02245 (2020). arXiv:2003.02245 .
URL https://arxiv.org/abs/2003.02245
[30] N. Ng, K. Cho, M. Ghassemi, SSMBA: self-supervised manifold based
data augmentation for improving out-of-domain robustness, in: B. Web-
ber, T. Cohn, Y. He, Y. Liu (Eds.), Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing, EMNLP 2020, On-
line, November 16-20, 2020, Association for Computational Linguistics,
2020, pp. 1268–1283. doi:10.18653/v1/2020.emnlp-main.97 .
URL https://doi.org/10.18653/v1/2020.emnlp-main.97
[31] N.Thakur, N.Reimers, J.Daxenberger, I.Gurevych, AugmentedSBERT:
data augmentation method for improving bi-encoders for pairwise sen-
tence scoring tasks, in: K. Toutanova, A. Rumshisky, L. Zettlemoyer,
D. Hakkani-Tür, I. Beltagy, S. Bethard, R. Cotterell, T. Chakraborty,
Y. Zhou (Eds.), Proceedings of the 2021 Conference of the North Ameri-
can Chapter of the Association for Computational Linguistics: Human
Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021,
Association for Computational Linguistics, 2021, pp. 296–310. doi:
10.18653/v1/2021.naacl-main.28 .
URL https://doi.org/10.18653/v1/2021.naacl-main.28
[32] H. Quteineh, S. Samothrakis, R. Sutcliﬀe, Textual data augmentation for
eﬃcient active learning on tiny datasets, in: B. Webber, T. Cohn, Y. He,
Y. Liu (Eds.), Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2020, Online, November 16-20,
36

--- PAGE 37 ---
2020, Association for Computational Linguistics, 2020, pp. 7400–7410.
doi:10.18653/v1/2020.emnlp-main.600 .
URL https://doi.org/10.18653/v1/2020.emnlp-main.600
[33] J. Du, E. Grave, B. Gunel, V. Chaudhary, O. Celebi, M. Auli, V. Stoy-
anov, A. Conneau, Self-training improves pre-training for natural lan-
guage understanding, in: K. Toutanova, A. Rumshisky, L. Zettlemoyer,
D. Hakkani-Tür, I. Beltagy, S. Bethard, R. Cotterell, T. Chakraborty,
Y. Zhou (Eds.), Proceedings of the 2021 Conference of the North Ameri-
can Chapter of the Association for Computational Linguistics: Human
Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021,
Association for Computational Linguistics, 2021, pp. 5408–5418. doi:
10.18653/v1/2021.naacl-main.426 .
URL https://doi.org/10.18653/v1/2021.naacl-main.426
[34] S. Montella, B. Fabre, T. Urvoy, J. Heinecke, L. M. Rojas-Barahona, De-
noising pre-training and data augmentation strategies for enhanced RDF
verbalization with transformers, arXiv abs/2012.00571 (2020). arXiv:
2012.00571 .
URL https://arxiv.org/abs/2012.00571
[35] H. Guo, Y. Mao, R. Zhang, Augmenting data with mixup for sentence
classiﬁcation: An empirical study, arXiv abs/1905.08941 (2019). arXiv:
1905.08941 .
URL http://arxiv.org/abs/1905.08941
[36] Y. Cheng, L. Jiang, W. Macherey, J. Eisenstein, Advaug: Robust ad-
versarial augmentation for neural machine translation, in: D. Jurafsky,
J. Chai, N. Schluter, J. R. Tetreault (Eds.), Proceedings of the 58th An-
nual Meeting of the Association for Computational Linguistics, ACL 2020,
Online, July 5-10, 2020, Association for Computational Linguistics, 2020,
pp. 5961–5970. doi:10.18653/v1/2020.acl-main.529 .
URL https://doi.org/10.18653/v1/2020.acl-main.529
[37] R. Barzilay, K. R. McKeown, Extracting paraphrases from a parallel
corpus, in: Proceedings of the 39th Annual Meeting of the Association
for Computational Linguistics, Association for Computational Linguis-
tics, Toulouse, France, 2001, pp. 50–57. doi:10.3115/1073012.1073020 .
URL https://aclanthology.org/P01-1008
[38] N. Madnani, B. J. Dorr, Generating phrasal and sentential paraphrases:
A survey of data-driven methods, Computational Linguistics 36 (3) (2010)
341–387. doi:10.1162/coli_a_00002 .
URL https://www.aclweb.org/anthology/J10-3003
[39] C. Coulombe, Text data augmentation made simple by leveraging NLP
cloud apis, arXiv abs/1812.04718 (2018). arXiv:1812.04718 .
URL http://arxiv.org/abs/1812.04718
37

--- PAGE 38 ---
[40] G. A. Miller, Wordnet: A lexical database for english, Commun. ACM
38 (11) (1995) 39–41. doi:10.1145/219717.219748 .
URL http://doi.acm.org/10.1145/219717.219748
[41] J. Mueller, A. Thyagarajan, Siamese recurrent architectures for learning
sentence similarity, in: D. Schuurmans, M. P. Wellman (Eds.), Proceed-
ings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence, February
12-17, 2016, Phoenix, Arizona, USA, AAAI Press, 2016, pp. 2786–2792.
URL http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/
view/12195
[42] G. Daval-Frerot, Y. Weis, WMD at SemEval-2020 tasks 7 and 11: As-
sessing humor and propaganda using unsupervised data augmentation,
in: Proceedings of the Fourteenth Workshop on Semantic Evaluation, In-
ternational Committee for Computational Linguistics, Barcelona (online),
2020, pp. 1865–1874.
URL https://www.aclweb.org/anthology/2020.semeval-1.246
[43] X.Dai, H.Adel, Ananalysisofsimpledataaugmentationfornamedentity
recognition, in: D. Scott, N. Bel, C. Zong (Eds.), Proceedings of the 28th
International Conference on Computational Linguistics, COLING 2020,
Barcelona, Spain(Online), December8-13, 2020, InternationalCommittee
on Computational Linguistics, 2020, pp. 3861–3867. doi:10.18653/v1/
2020.coling-main.343 .
URL https://doi.org/10.18653/v1/2020.coling-main.343
[44] D. Zhang, T. Li, H. Zhang, B. Yin, On data augmentation for ex-
treme multi-label classiﬁcation, arXiv abs/2009.10778 (2020). arXiv:
2009.10778 .
URL https://arxiv.org/abs/2009.10778
[45] X. Zuo, Y. Chen, K. Liu, J. Zhao, Knowdis: Knowledge enhanced data
augmentation for event causality detection via distant supervision, in:
D. Scott, N. Bel, C. Zong (Eds.), Proceedings of the 28th International
Conference on Computational Linguistics, COLING 2020, Barcelona,
Spain (Online), December 8-13, 2020, International Committee on Com-
putational Linguistics, 2020, pp. 1544–1550. doi:10.18653/v1/2020.
coling-main.135 .
URL https://doi.org/10.18653/v1/2020.coling-main.135
[46] K. K. Schuler, VerbNet: A broad-coverage, comprehensive verb lexicon,
University of Pennsylvania, 2005.
[47] C. F. Baker, C. J. Fillmore, J. B. Lowe, The Berkeley FrameNet project,
in: 36th Annual Meeting of the Association for Computational Linguistics
and 17th International Conference on Computational Linguistics, Volume
1, Association for Computational Linguistics, Montreal, Quebec, Canada,
1998, pp. 86–90. doi:10.3115/980845.980860 .
URL https://aclanthology.org/P98-1013
38

--- PAGE 39 ---
[48] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, J. Dean, Distributed
representations of words and phrases and their compositionality, in:
C. J. C. Burges, L. Bottou, Z. Ghahramani, K. Q. Weinberger (Eds.),
Advances in Neural Information Processing Systems 26: 27th Annual
Conference on Neural Information Processing Systems 2013. Proceedings
of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United
States, 2013, pp. 3111–3119.
URL https://proceedings.neurips.cc/paper/2013/hash/
9aa42b31882ec039965f3c4923ce901b-Abstract.html
[49] S. Liu, K. Lee, I. Lee, Document-level multi-topic sentiment classiﬁcation
of email data with bilstm and data augmentation, Knowl. Based Syst. 197
(2020) 105918. doi:10.1016/j.knosys.2020.105918 .
URL https://doi.org/10.1016/j.knosys.2020.105918
[50] D. Ramirez-Echavarria, A. Bikakis, L. Dickens, R. Miller, A. Vlachidis,
On the eﬀects of knowledge-augmented data in word embeddings, CoRR
abs/2010.01745 (2020). arXiv:2010.01745 .
URL https://arxiv.org/abs/2010.01745
[51] X. Wu, S. Lv, L. Zang, J. Han, S. Hu, Conditional bert contextual aug-
mentation, in: ICCS, 2019.
[52] J. M. Tapia-Téllez, H. J. Escalante, Data augmentation with transformers
for text classiﬁcation, in: L. Martínez-Villaseñor, O. Herrera-Alcántara,
H.E.Ponce, F.Castro-Espinoza(Eds.), AdvancesinComputationalIntel-
ligence - 19th Mexican International Conference on Artiﬁcial Intelligence,
MICAI 2020, Mexico City, Mexico, October 12-17, 2020, Proceedings,
Part II, Vol. 12469 of Lecture Notes in Computer Science, Springer, 2020,
pp. 247–259. doi:10.1007/978-3-030-60887-3\_22 .
URL https://doi.org/10.1007/978-3-030-60887-3_22
[53] D. Lowell, B. E. Howard, Z. C. Lipton, B. C. Wallace, Unsupervised data
augmentation with naive augmentation and without unlabeled data, in:
M. Moens, X. Huang, L. Specia, S. W. Yih (Eds.), Proceedings of the
2021 Conference on Empirical Methods in Natural Language Processing,
EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11
November, 2021, Association for Computational Linguistics, 2021, pp.
4992–5001.
URL https://aclanthology.org/2021.emnlp-main.408
[54] D. Palomino, J. O. Luna, Palomino-ochoa at TASS 2020: Transformer-
based data augmentation for overcoming few-shot learning, in: M. Á. G.
Cumbreras, J. Gonzalo, E. M. Cámara, R. Martínez-Unanue, P. Rosso,
S.M.J.Zafra, J.A.O.Zambrano, A.Miranda, J.P.Zamorano, Y.Gutiér-
rez, A. Rosá, M. Montes-y-Gómez, M. G. Vega (Eds.), Proceedings of the
IberianLanguagesEvaluationForum(IberLEF2020)co-locatedwith36th
39

--- PAGE 40 ---
Conference of the Spanish Society for Natural Language Processing (SE-
PLN 2020), Málaga, Spain, September 23th, 2020, Vol. 2664 of CEUR
Workshop Proceedings, CEUR-WS.org, 2020, pp. 171–178.
URL http://ceur-ws.org/Vol-2664/tass_paper1.pdf
[55] S. Kobayashi, Contextual augmentation: Data augmentation by words
with paradigmatic relations, in: M. A. Walker, H. Ji, A. Stent (Eds.),
Proceedings of the 2018 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technolo-
gies, NAACL-HLT, New Orleans, Louisiana, USA, June 1-6, 2018, Vol-
ume 2 (Short Papers), Association for Computational Linguistics, 2018,
pp. 452–457. doi:10.18653/v1/n18-2072 .
URL https://doi.org/10.18653/v1/n18-2072
[56] M. Fadaee, A. Bisazza, C. Monz, Data augmentation for low-resource
neural machine translation, in: R. Barzilay, M. Kan (Eds.), Proceedings
of the 55th Annual Meeting of the Association for Computational Lin-
guistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 2:
Short Papers, Association for Computational Linguistics, 2017, pp. 567–
573. doi:10.18653/v1/P17-2090 .
URL https://doi.org/10.18653/v1/P17-2090
[57] M. Dehouck, C. Gómez-Rodríguez, Data augmentation via subtree swap-
ping for dependency parsing of low-resource languages, in: D. Scott,
N. Bel, C. Zong (Eds.), Proceedings of the 28th International Conference
on Computational Linguistics, COLING 2020, Barcelona, Spain (Online),
December8-13,2020,InternationalCommitteeonComputationalLinguis-
tics, 2020, pp. 3818–3830. doi:10.18653/v1/2020.coling-main.339 .
URL https://doi.org/10.18653/v1/2020.coling-main.339
[58] A. W. Yu, D. Dohan, M. Luong, R. Zhao, K. Chen, M. Norouzi, Q. V. Le,
Qanet: Combining local convolution with global self-attention for reading
comprehension, in: 6th International Conference on Learning Represen-
tations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018,
Conference Track Proceedings, OpenReview.net, 2018.
URL https://openreview.net/forum?id=B14TlG-RW
[59] A. R. Fabbri, S. Han, H. Li, H. Li, M. Ghazvininejad, S. R. Joty,
D. R. Radev, Y. Mehdad, Improving zero and few-shot abstractive sum-
marization with intermediate ﬁne-tuning and data augmentation, in:
K. Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani-Tür, I. Beltagy,
S. Bethard, R. Cotterell, T. Chakraborty, Y. Zhou (Eds.), Proceedings of
the 2021 Conference of the North American Chapter of the Association for
ComputationalLinguistics: HumanLanguageTechnologies, NAACL-HLT
2021, Online, June 6-11, 2021, Association for Computational Linguistics,
2021, pp. 704–717. doi:10.18653/v1/2021.naacl-main.57 .
URL https://doi.org/10.18653/v1/2021.naacl-main.57
40

--- PAGE 41 ---
[60] M. Ibrahim, M. Torki, N. El-Makky, AlexU-BackTranslation-TL at
SemEval-2020 task 12: Improving oﬀensive language detection using data
augmentation and transfer learning, in: Proceedings of the Fourteenth
Workshop on Semantic Evaluation, International Committee for Com-
putational Linguistics, Barcelona (online), 2020, pp. 1881–1890. doi:
10.18653/v1/2020.semeval-1.248 .
URL https://aclanthology.org/2020.semeval-1.248
[61] S. Longpre, Y. Wang, C. DuBois, How eﬀective is task-agnostic data aug-
mentation for pretrained transformers?, in: Findings of the Association
for Computational Linguistics: EMNLP 2020, Association for Computa-
tional Linguistics, Online, 2020, pp. 4401–4411. doi:10.18653/v1/2020.
findings-emnlp.394 .
URL https://www.aclweb.org/anthology/2020.findings-emnlp.394
[62] C. Rastogi, N. Moﬁd, F. Hsiao, Can we achieve more with less? exploring
data augmentation for toxic comment classiﬁcation, arXiv abs/2007.00875
(2020). arXiv:2007.00875 .
URL https://arxiv.org/abs/2007.00875
[63] A. Perevalov, A. Both, Augmentation-based answer type classiﬁcation of
the SMART dataset, in: N. Mihindukulasooriya, M. Dubey, A. Gliozzo,
J. Lehmann, A. N. Ngomo, R. Usbeck (Eds.), Proceedings of the SeMan-
tic AnsweR Type prediction task (SMART) at ISWC 2020 Semantic Web
Challenge co-located with the 19th International Semantic Web Confer-
ence (ISWC 2020), Virtual Conference, November 5th, 2020, Vol. 2774 of
CEUR Workshop Proceedings, CEUR-WS.org, 2020, pp. 1–9.
URL http://ceur-ws.org/Vol-2774/paper-01.pdf
[64] S. T. Aroyehun, A. F. Gelbukh, Aggression detection in social media:
Using deep neural networks, data augmentation, and pseudo labeling,
in: R. Kumar, A. K. Ojha, M. Zampieri, S. Malmasi (Eds.), Proceed-
ings of the First Workshop on Trolling, Aggression and Cyberbullying,
TRAC@COLING 2018, Santa Fe, New Mexico, USA, August 25, 2018,
Association for Computational Linguistics, 2018, pp. 90–97.
URL https://aclanthology.org/W18-4411/
[65] T. Nugent, N. Stelea, J. L. Leidner, Detecting environmental, social
and governance (ESG) topics using domain-speciﬁc language models and
data augmentation, in: T. Andreasen, G. D. Tré, J. Kacprzyk, H. L.
Larsen, G. Bordogna, S. Zadrozny (Eds.), Proceedings of the 14th In-
ternational Conference on Flexible Query Answering Systems (FQAS
2021), Bratislava, Slovakia, September 19-24, 2021, Vol. 12871 of Lec-
ture Notes in Computer Science, Springer, 2021, pp. 157–169. doi:
10.1007/978-3-030-86967-0\_12 .
[66] Y. Qu, D. Shen, Y. Shen, S. Sajeev, W. Chen, J. Han, Coda: Contrast-
enhancedanddiversity-promotingdataaugmentationfornaturallanguage
41

--- PAGE 42 ---
understanding, in: 9th International Conference on Learning Represen-
tations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, OpenRe-
view.net, 2021.
URL https://openreview.net/forum?id=Ozk9MrX1hvA
[67] V. Barrière, A. Balahur, Improving sentiment analysis over non-english
tweetsusingmultilingualtransformersandautomatic translationfordata-
augmentation, in: D. Scott, N. Bel, C. Zong (Eds.), Proceedings of
the 28th International Conference on Computational Linguistics, COL-
ING 2020, Barcelona, Spain (Online), December 8-13, 2020, Interna-
tional Committee on Computational Linguistics, 2020, pp. 266–271. doi:
10.18653/v1/2020.coling-main.23 .
URL https://doi.org/10.18653/v1/2020.coling-main.23
[68] Y. Hou, S. Chen, W. Che, C. Chen, T. Liu, C2c-genda: Cluster-to-cluster
generation for data augmentation of slot ﬁlling, in: Thirty-Fifth AAAI
Conference on Artiﬁcial Intelligence, AAAI 2021, Thirty-Third Confer-
ence on Innovative Applications of Artiﬁcial Intelligence, IAAI 2021, The
Eleventh Symposium on Educational Advances in Artiﬁcial Intelligence,
EAAI 2021, Virtual Event, February 2-9, 2021, AAAI Press, 2021, pp.
13027–13035.
URL https://ojs.aaai.org/index.php/AAAI/article/view/17540
[69] T. Kober, J. Weeds, L. Bertolini, D. J. Weir, Data augmentation for
hypernymy detection, in: P. Merlo, J. Tiedemann, R. Tsarfaty (Eds.),
Proceedings of the 16th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics: Main Volume, EACL 2021, Online,
April 19 - 23, 2021, Association for Computational Linguistics, 2021, pp.
1034–1048.
URL https://aclanthology.org/2021.eacl-main.89/
[70] J. Wang, H.-C. Chen, R. Radach, A. Inhoﬀ, Reading Chinese script: A
cognitive analysis, Psychology Press, 1999.
[71] X. Wang, H. Pham, Z. Dai, G. Neubig, Switchout: an eﬃcient data aug-
mentation algorithm for neural machine translation, in: E. Riloﬀ, D. Chi-
ang, J. Hockenmaier, J. Tsujii (Eds.), Proceedings of the 2018 Conference
onEmpiricalMethodsinNaturalLanguageProcessing, Brussels, Belgium,
October 31 - November 4, 2018, Association for Computational Linguis-
tics, 2018, pp. 856–861. doi:10.18653/v1/d18-1100 .
URL https://doi.org/10.18653/v1/d18-1100
[72] L. Qin, M. Ni, Y. Zhang, W. Che, Cosda-ml: Multi-lingual code-switching
data augmentation for zero-shot cross-lingual NLP, in: C. Bessiere (Ed.),
Proceedings of the Twenty-Ninth International Joint Conference on Ar-
tiﬁcial Intelligence, IJCAI 2020, ijcai.org, 2020, pp. 3853–3860. doi:
10.24963/ijcai.2020/533 .
URL https://doi.org/10.24963/ijcai.2020/533
42

--- PAGE 43 ---
[73] X. Song, L. Zang, S. Hu, Data augmentation for copy-mechanism in
dialogue state tracking, in: M. Paszynski, D. Kranzlmüller, V. V.
Krzhizhanovskaya, J. J. Dongarra, P. M. A. Sloot (Eds.), Computa-
tional Science - ICCS 2021 - 21st International Conference, Krakow,
Poland, June 16-18, 2021, Proceedings, Part I, Vol. 12742 of Lecture
Notes in Computer Science, Springer, 2021, pp. 736–749. doi:10.1007/
978-3-030-77961-0\_59 .
URL https://doi.org/10.1007/978-3-030-77961-0_59
[74] X. Mou, B. Sigouin, I. Steenstra, H. Su, Multimodal dialogue state
tracking by QA approach with data augmentation, arXiv abs/2007.09903
(2020). arXiv:2007.09903 .
URL https://arxiv.org/abs/2007.09903
[75] M. H. Shakeel, A. Karim, I. Khan, A multi-cascaded model with data aug-
mentation for enhanced paraphrase detection in short texts, Inf. Process.
Manag. 57 (3) (2020) 102204. doi:10.1016/j.ipm.2020.102204 .
URL https://doi.org/10.1016/j.ipm.2020.102204
[76] Y. Yin, L. Shang, X. Jiang, X. Chen, Q. Liu, Dialog state tracking
with reinforced data augmentation, in: The Thirty-Fourth AAAI Con-
ference on Artiﬁcial Intelligence, AAAI 2020, The Thirty-Second Inno-
vative Applications of Artiﬁcial Intelligence Conference, IAAI 2020, The
Tenth AAAI Symposium on Educational Advances in Artiﬁcial Intelli-
gence, EAAI 2020, New York, NY, USA, February 7-12, 2020, AAAI
Press, 2020, pp. 9474–9481.
URL https://aaai.org/ojs/index.php/AAAI/article/view/6491
[77] R. Sennrich, B. Haddow, A. Birch, Improving neural machine translation
models with monolingual data, in: Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics (Volume 1: Long Pa-
pers), Association for Computational Linguistics, Berlin, Germany, 2016,
pp. 86–96. doi:10.18653/v1/P16-1009 .
URL https://aclanthology.org/P16-1009
[78] R. Sennrich, B. Haddow, A. Birch, Improving neural machine translation
models with monolingual data, in: Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics, ACL 2016, August
7-12, 2016, Berlin, Germany, Volume 1: Long Papers, The Association for
Computer Linguistics, 2016. doi:10.18653/v1/p16-1009 .
URL https://doi.org/10.18653/v1/p16-1009
[79] Y. Chen, S. Lu, F. Yang, X. Huang, X. Fan, C. Guo, Pattern-aware
data augmentation for query rewriting in voice assistant systems, arXiv
abs/2012.11468 (2020). arXiv:2012.11468 .
URL https://arxiv.org/abs/2012.11468
[80] K. M. Yoo, Y. Shin, S. Lee, Data augmentation for spoken language un-
derstanding via joint variational generation, in: The Thirty-Third AAAI
43

--- PAGE 44 ---
Conference on Artiﬁcial Intelligence, AAAI 2019, The Thirty-First Inno-
vative Applications of Artiﬁcial Intelligence Conference, IAAI 2019, The
Ninth AAAI Symposium on Educational Advances in Artiﬁcial Intelli-
gence, EAAI2019, Honolulu, Hawaii, USA,January27-February1, 2019,
AAAIPress, 2019, pp.7402–7409. doi:10.1609/aaai.v33i01.33017402 .
URL https://doi.org/10.1609/aaai.v33i01.33017402
[81] H. Q. Abonizio, S. B. Junior, Pre-trained data augmentation for text clas-
siﬁcation, in: R. Cerri, R. C. Prati (Eds.), Intelligent Systems - 9th Brazil-
ian Conference, BRACIS 2020, Rio Grande, Brazil, October 20-23, 2020,
Proceedings, Part I, Vol. 12319 of Lecture Notes in Computer Science,
Springer, 2020, pp. 551–565. doi:10.1007/978-3-030-61377-8\_38 .
URL https://doi.org/10.1007/978-3-030-61377-8_38
[82] V. Sanh, L. Debut, J. Chaumond, T. Wolf, Distilbert, a distilled version of
BERT: smaller, faster, cheaper and lighter, CoRR abs/1910.01108 (2019).
arXiv:1910.01108 .
URL http://arxiv.org/abs/1910.01108
[83] B. Tarján, G. Szaszák, T. Fegyó, P. Mihajlik, Deep transformer based
data augmentation with subword units for morphologically rich online
ASR, arXiv abs/2007.06949 (2020). arXiv:2007.06949 .
URL https://arxiv.org/abs/2007.06949
[84] L. Miao, M. Last, M. Litvak, Twitter data augmentation for monitoring
publicopiniononCOVID-19interventionmeasures,in: K.Verspoor,K.B.
Cohen, M. Conway, B. de Bruijn, M. Dredze, R. Mihalcea, B. C. Wallace
(Eds.), Proceedingsofthe1stWorkshoponNLPforCOVID-19@EMNLP
2020, Online, December 2020, Association for Computational Linguistics,
2020. doi:10.18653/v1/2020.nlpcovid19-2.19 .
URL https://doi.org/10.18653/v1/2020.nlpcovid19-2.19
[85] Y. Yang, N. Jin, K. Lin, M. Guo, D. Cer, Neural retrieval for ques-
tion answering with cross-attention supervised data augmentation, in:
C. Zong, F. Xia, W. Li, R. Navigli (Eds.), Proceedings of the 59th An-
nual Meeting of the Association for Computational Linguistics and the
11th International Joint Conference on Natural Language Processing,
ACL/IJCNLP 2021, (Volume 2: Short Papers), Virtual Event, August
1-6, 2021, Association for Computational Linguistics, 2021, pp. 263–268.
doi:10.18653/v1/2021.acl-short.35 .
URL https://doi.org/10.18653/v1/2021.acl-short.35
[86] Z. Zhao, E. E. Papalexakis, X. Ma, Learning physical common sense
as knowledge graph completion via BERT data augmentation and con-
strained tucker factorization, in: B. Webber, T. Cohn, Y. He, Y. Liu
(Eds.), Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2020, Online, November 16-20,
2020, Association for Computational Linguistics, 2020, pp. 3293–3298.
44

--- PAGE 45 ---
doi:10.18653/v1/2020.emnlp-main.266 .
URL https://doi.org/10.18653/v1/2020.emnlp-main.266
[87] H. Zhang, M. Cissé, Y. N. Dauphin, D. Lopez-Paz, mixup: Beyond em-
pirical risk minimization, in: 6th International Conference on Learning
Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3,
2018, Conference Track Proceedings, OpenReview.net, 2018.
URL https://openreview.net/forum?id=r1Ddp1-Rb
[88] Y. Cheng, L. Jiang, W. Macherey, Robust neural machine translation
with doubly adversarial inputs, in: Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguistics, Association for
Computational Linguistics, Florence, Italy, 2019, pp. 4324–4333. doi:
10.18653/v1/P19-1425 .
URL https://aclanthology.org/P19-1425
[89] L. Sun, C. Xia, W. Yin, T. Liang, P. S. Yu, L. He, Mixup-transformer:
Dynamic data augmentation for NLP tasks, in: D. Scott, N. Bel, C. Zong
(Eds.), Proceedings of the 28th International Conference on Computa-
tional Linguistics, COLING 2020, Barcelona, Spain (Online), December
8-13, 2020, International Committee on Computational Linguistics, 2020,
pp. 3436–3440. doi:10.18653/v1/2020.coling-main.305 .
URL https://doi.org/10.18653/v1/2020.coling-main.305
[90] M. S. Bari, M. T. Mohiuddin, S. R. Joty, Multimix: A robust data aug-
mentation strategy for cross-lingual NLP, CoRR abs/2004.13240 (2020).
arXiv:2004.13240 .
URL https://arxiv.org/abs/2004.13240
[91] C. Si, Z. Zhang, F. Qi, Z. Liu, Y. Wang, Q. Liu, M. Sun, Better robust-
ness by more coverage: Adversarial training with mixup augmentation for
robust ﬁne-tuning, arXiv abs/2012.15699 (2020). arXiv:2012.15699 .
URL https://arxiv.org/abs/2012.15699
[92] J. Chen, Z. Wang, R. Tian, Z. Yang, D. Yang, Local additivity based data
augmentation for semi-supervised NER, in: B. Webber, T. Cohn, Y. He,
Y. Liu (Eds.), Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2020, Online, November 16-20,
2020, Association for Computational Linguistics, 2020, pp. 1241–1251.
doi:10.18653/v1/2020.emnlp-main.95 .
URL https://doi.org/10.18653/v1/2020.emnlp-main.95
[93] Z. Hu, B. Tan, R. Salakhutdinov, T. M. Mitchell, E. P. Xing, Learning
data manipulation for augmentation and weighting, in: H. M. Wallach,
H. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. B. Fox, R. Garnett
(Eds.), Advances in Neural Information Processing Systems 32: Annual
Conference on Neural Information Processing Systems 2019, NeurIPS
2019, December 8-14, 2019, Vancouver, BC, Canada, 2019, pp. 15738–
15749.
45

--- PAGE 46 ---
URL https://proceedings.neurips.cc/paper/2019/hash/
671f0311e2754fcdd37f70a8550379bc-Abstract.html
[94] R. Liu, G. Xu, C. Jia, W. Ma, L. Wang, S. Vosoughi, Data boost: Text
dataaugmentationthroughreinforcementlearningguidedconditionalgen-
eration, in: B. Webber, T. Cohn, Y. He, Y. Liu (Eds.), Proceedings of
the 2020 Conference on Empirical Methods in Natural Language Process-
ing, EMNLP 2020, Online, November 16-20, 2020, Association for Com-
putational Linguistics, 2020, pp. 9031–9041. doi:10.18653/v1/2020.
emnlp-main.726 .
URL https://doi.org/10.18653/v1/2020.emnlp-main.726
[95] S. Shehnepoor, R. Togneri, W. Liu, M. Bennamoun, Gangster: A fraud
review detector based on regulated GAN with data augmentation, CoRR
abs/2006.06561 (2020). arXiv:2006.06561 .
URL https://arxiv.org/abs/2006.06561
[96] Che,Wanxiang and Guo,Jiang and Cui,Yiming, Natural language pro-
cessing: methods based on pre-trained models, Electronic Industry Press,
2021.
[97] C. Liu, D. Yu, BLCU-NLP at SemEval-2020 task 5: Data augmentation
for eﬃcient counterfactual detecting, in: Proceedings of the Fourteenth
Workshop on Semantic Evaluation, International Committee for Compu-
tational Linguistics, Barcelona (online), 2020, pp. 633–639.
URL https://aclanthology.org/2020.semeval-1.81
[98] V. Kovatchev, P. Smith, M. G. Lee, R. T. Devine, Can vectors read
minds better than experts? comparing data augmentation strategies for
the automated scoring of children’s mindreading ability, in: C. Zong,
F. Xia, W. Li, R. Navigli (Eds.), Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics and the 11th In-
ternational Joint Conference on Natural Language Processing, ACL/I-
JCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6,
2021, Association for Computational Linguistics, 2021, pp. 1196–1206.
doi:10.18653/v1/2021.acl-long.96 .
URL https://doi.org/10.18653/v1/2021.acl-long.96
[99] M. S. Bari, T. Mohiuddin, S. Joty, UXLA: A robust unsupervised data
augmentation framework for zero-resource cross-lingual NLP, in: Pro-
ceedings of the 59th Annual Meeting of the Association for Computa-
tional Linguistics and the 11th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers), Association for Computa-
tional Linguistics, Online, 2021, pp. 1978–1992. doi:10.18653/v1/2021.
acl-long.154 .
URL https://aclanthology.org/2021.acl-long.154
[100] G. G. Sahin, M. Steedman, Data augmentation via dependency tree mor-
phing for low-resource languages, arXiv abs/1903.09460 (2019). arXiv:
46

--- PAGE 47 ---
1903.09460 .
URL http://arxiv.org/abs/1903.09460
[101] J. Andreas, Good-enough compositional data augmentation, in: Proceed-
ings of the 58th Annual Meeting of the Association for Computational
Linguistics, Association for Computational Linguistics, Online, 2020, pp.
7556–7566. doi:10.18653/v1/2020.acl-main.676 .
URL https://aclanthology.org/2020.acl-main.676
[102] J. Lun, J. Zhu, Y. Tang, M. Yang, Multiple data augmentation strategies
for improving performance on automatic short answer scoring, in: The
Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020,
The Thirty-Second Innovative Applications of Artiﬁcial Intelligence Con-
ference, IAAI 2020, The Tenth AAAI Symposium on Educational Ad-
vances in Artiﬁcial Intelligence, EAAI 2020, New York, NY, USA, Febru-
ary 7-12, 2020, AAAI Press, 2020, pp. 13389–13396.
URL https://aaai.org/ojs/index.php/AAAI/article/view/7062
[103] J. Risch, R. Krestel, Aggression identiﬁcation using deep learning and
data augmentation, in: Proceedings of the First Workshop on Trolling,
Aggression and Cyberbullying (TRAC-2018), Association for Computa-
tional Linguistics, Santa Fe, New Mexico, USA, 2018, pp. 150–158.
URL https://aclanthology.org/W18-4418
[104] S. Longpre, Y. Lu, Z. Tu, C. DuBois, An exploration of data augmenta-
tion and sampling techniques for domain-agnostic question answering, in:
Proceedings of the 2nd Workshop on Machine Reading for Question An-
swering, Association for Computational Linguistics, Hong Kong, China,
2019, pp. 220–227. doi:10.18653/v1/D19-5829 .
URL https://aclanthology.org/D19-5829
[105] B. Xu, S. Qiu, J. Zhang, Y. Wang, X. Shen, G. de Melo, Data aug-
mentation for multiclass utterance classiﬁcation - A systematic study, in:
D. Scott, N. Bel, C. Zong (Eds.), Proceedings of the 28th International
Conference on Computational Linguistics, COLING 2020, Barcelona,
Spain (Online), December 8-13, 2020, International Committee on Com-
putational Linguistics, 2020, pp. 5494–5506. doi:10.18653/v1/2020.
coling-main.479 .
URL https://doi.org/10.18653/v1/2020.coling-main.479
[106] Z. Guo, Z. Liu, Z. Ling, S. Wang, L. Jin, Y. Li, Text classiﬁcation by
contrastive learning and cross-lingual data augmentation for Alzheimer’s
disease detection, in: Proceedings of the 28th International Conference on
Computational Linguistics, International Committee on Computational
Linguistics, Barcelona, Spain (Online), 2020, pp. 6161–6171. doi:10.
18653/v1/2020.coling-main.542 .
URL https://aclanthology.org/2020.coling-main.542
47

--- PAGE 48 ---
[107] Z. Zhao, S. Zhu, K. Yu, Data augmentation with atomic templates for
spoken language understanding, in: Proceedings of the 2019 Conference
on Empirical Methods in Natural Language Processing and the 9th In-
ternational Joint Conference on Natural Language Processing (EMNLP-
IJCNLP), Association for Computational Linguistics, Hong Kong, China,
2019, pp. 3637–3643. doi:10.18653/v1/D19-1375 .
URL https://aclanthology.org/D19-1375
[108] Z. Wan, X. Wan, W. Wang, Improving grammatical error correction with
data augmentation by editing latent representation, in: D. Scott, N. Bel,
C. Zong (Eds.), Proceedings of the 28th International Conference on Com-
putational Linguistics, COLING 2020, Barcelona, Spain (Online), Decem-
ber 8-13, 2020, International Committee on Computational Linguistics,
2020, pp. 2202–2212. doi:10.18653/v1/2020.coling-main.200 .
URL https://doi.org/10.18653/v1/2020.coling-main.200
[109] A. Kumar, S. Bhattamishra, M. Bhandari, P. Talukdar, Submodular
optimization-based diverse paraphrasing and its eﬀectiveness in data aug-
mentation, in: Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long and Short Papers), Association for
Computational Linguistics, Minneapolis, Minnesota, 2019, pp. 3609–3619.
doi:10.18653/v1/N19-1363 .
URL https://aclanthology.org/N19-1363
[110] J. Li, L. Qiu, B. Tang, D. Chen, D. Zhao, R. Yan, Insuﬃcient data can
also rock! learning to converse using smaller data with augmentation, in:
AAAI, 2019.
[111] K. M. Yoo, H. Lee, F. Dernoncourt, T. Bui, W. Chang, S. Lee, Varia-
tional hierarchical dialog autoencoder for dialog state tracking data aug-
mentation, in: B. Webber, T. Cohn, Y. He, Y. Liu (Eds.), Proceedings of
the 2020 Conference on Empirical Methods in Natural Language Process-
ing, EMNLP 2020, Online, November 16-20, 2020, Association for Com-
putational Linguistics, 2020, pp. 3406–3425. doi:10.18653/v1/2020.
emnlp-main.274 .
URL https://doi.org/10.18653/v1/2020.emnlp-main.274
[112] W. Du, A. Black, Data augmentation for neural online chats response se-
lection, in: Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd
International Workshop on Search-Oriented Conversational AI, Associa-
tion for Computational Linguistics, Brussels, Belgium, 2018, pp. 52–58.
doi:10.18653/v1/W18-5708 .
URL https://aclanthology.org/W18-5708
[113] J. Chen, D. Shen, W. Chen, D. Yang, HiddenCut: Simple data aug-
mentation for natural language understanding with better generalizabil-
ity, in: Proceedings of the 59th Annual Meeting of the Association
48

--- PAGE 49 ---
for Computational Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1: Long Papers), As-
sociation for Computational Linguistics, Online, 2021, pp. 4380–4390.
doi:10.18653/v1/2021.acl-long.338 .
URL https://aclanthology.org/2021.acl-long.338
[114] H. Shi, K. Livescu, K. Gimpel, Substructure substitution: Structured
data augmentation for NLP, in: C. Zong, F. Xia, W. Li, R. Navigli (Eds.),
Findings of the Association for Computational Linguistics: ACL/IJCNLP
2021, OnlineEvent, August1-6, 2021, Vol.ACL/IJCNLP2021ofFindings
of ACL, Association for Computational Linguistics, 2021, pp. 3494–3508.
doi:10.18653/v1/2021.findings-acl.307 .
URL https://doi.org/10.18653/v1/2021.findings-acl.307
[115] Y. Xu, R. Jia, L. Mou, G. Li, Y. Chen, Y. Lu, Z. Jin, Improved relation
classiﬁcation by deep recurrent neural networks with data augmentation,
in: N. Calzolari, Y. Matsumoto, R. Prasad (Eds.), COLING 2016, 26th
International Conference on Computational Linguistics, Proceedings of
the Conference: Technical Papers, December 11-16, 2016, Osaka, Japan,
ACL, 2016, pp. 1461–1470.
URL https://aclanthology.org/C16-1138/
[116] Y. Chen, C. Kedzie, S. Nair, P. Galuscakova, R. Zhang, D. Oard, K. McK-
eown, Cross-language sentence selection via data augmentation and ratio-
nale training, in: Proceedings of the 59th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 11th International Joint
Conference on Natural Language Processing (Volume 1: Long Papers),
Association for Computational Linguistics, Online, 2021, pp. 3881–3895.
doi:10.18653/v1/2021.acl-long.300 .
URL https://aclanthology.org/2021.acl-long.300
[117] Z. Jiang, J. Han, B. Sisman, X. L. Dong, CoRI: Collective relation in-
tegration with data augmentation for open information extraction, in:
Proceedings of the 59th Annual Meeting of the Association for Computa-
tional Linguistics and the 11th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers), Association for Computa-
tional Linguistics, Online, 2021, pp. 4706–4716. doi:10.18653/v1/2021.
acl-long.363 .
URL https://aclanthology.org/2021.acl-long.363
[118] R. Zhang, Y. Zheng, J. Shao, X. Mao, Y. Xi, M. Huang, Dialogue dis-
tillation: Open-domain dialogue augmentation using unpaired data, in:
B. Webber, T. Cohn, Y. He, Y. Liu (Eds.), Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language Processing, EMNLP
2020, Online, November 16-20, 2020, Association for Computational Lin-
guistics, 2020, pp. 3449–3460. doi:10.18653/v1/2020.emnlp-main.277 .
URL https://doi.org/10.18653/v1/2020.emnlp-main.277
49

--- PAGE 50 ---
[119] A. Asai, H. Hajishirzi, Logic-guided data augmentation and regularization
for consistent question answering, in: D. Jurafsky, J. Chai, N. Schluter,
J. R. Tetreault (Eds.), Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics, ACL 2020, Online, July 5-10,
2020, Association for Computational Linguistics, 2020, pp. 5642–5650.
doi:10.18653/v1/2020.acl-main.499 .
URL https://doi.org/10.18653/v1/2020.acl-main.499
[120] T.Bergmanis, S.Goldwater, Dataaugmentationforcontext-sensitiveneu-
ral lemmatization using inﬂection tables and raw text, in: Proceedings of
the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 1
(LongandShortPapers), AssociationforComputationalLinguistics, Min-
neapolis, Minnesota, 2019, pp. 4119–4128. doi:10.18653/v1/N19-1418 .
URL https://aclanthology.org/N19-1418
[121] R. Zmigrod, S. J. Mielke, H. M. Wallach, R. Cotterell, Counterfactual
data augmentation for mitigating gender stereotypes in languages with
rich morphology, in: A. Korhonen, D. R. Traum, L. Màrquez (Eds.),
Proceedings of the 57th Conference of the Association for Computational
Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1:
Long Papers, Association for Computational Linguistics, 2019, pp. 1651–
1661. doi:10.18653/v1/p19-1161 .
URL https://doi.org/10.18653/v1/p19-1161
[122] Y. Zhou, F. Dong, Y. Liu, Z. Li, J. Du, L. Zhang, Forecasting emerging
technologies using data augmentation and deep learning, Scientometrics
123 (1) (2020) 1–29. doi:10.1007/s11192-020-03351-6 .
URL https://doi.org/10.1007/s11192-020-03351-6
[123] T. Niu, M. Bansal, Automatically learning data augmentation policies for
dialoguetasks, in: Proceedingsofthe2019ConferenceonEmpiricalMeth-
ods in Natural Language Processing and the 9th International Joint Con-
ference on Natural Language Processing (EMNLP-IJCNLP), Association
for Computational Linguistics, Hong Kong, China, 2019, pp. 1317–1323.
doi:10.18653/v1/D19-1132 .
URL https://aclanthology.org/D19-1132
[124] L. Yao, B. Yang, H. Zhang, B. Chen, W. Luo, Domain transfer based data
augmentation for neural query translation, in: D. Scott, N. Bel, C. Zong
(Eds.), Proceedings of the 28th International Conference on Computa-
tional Linguistics, COLING 2020, Barcelona, Spain (Online), December
8-13, 2020, International Committee on Computational Linguistics, 2020,
pp. 4521–4533. doi:10.18653/v1/2020.coling-main.399 .
URL https://doi.org/10.18653/v1/2020.coling-main.399
[125] G. Chen, Y. Chen, Y. Wang, V. O. K. Li, Lexical-constraint-aware neu-
ral machine translation via data augmentation, in: C. Bessiere (Ed.),
50

--- PAGE 51 ---
Proceedings of the Twenty-Ninth International Joint Conference on Ar-
tiﬁcial Intelligence, IJCAI 2020, ijcai.org, 2020, pp. 3587–3593. doi:
10.24963/ijcai.2020/496 .
URL https://doi.org/10.24963/ijcai.2020/496
[126] L. Liu, B. Ding, L. Bing, S. Joty, L. Si, C. Miao, MulDA: A multilin-
gual data augmentation framework for low-resource cross-lingual NER,
in: Proceedings of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long Papers), Associa-
tion for Computational Linguistics, Online, 2021, pp. 5834–5846. doi:
10.18653/v1/2021.acl-long.453 .
URL https://aclanthology.org/2021.acl-long.453
[127] I. Staliunaite, P. J. Gorinski, I. Iacobacci, Improving commonsense causal
reasoning by adversarial training and data augmentation, in: Thirty-
Fifth AAAI Conference on Artiﬁcial Intelligence, AAAI 2021, Thirty-
Third Conference on Innovative Applications of Artiﬁcial Intelligence,
IAAI 2021, The Eleventh Symposium on Educational Advances in Ar-
tiﬁcial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, AAAI
Press, 2021, pp. 13834–13842.
URL https://ojs.aaai.org/index.php/AAAI/article/view/17630
[128] X. Dong, Y. Zhu, Z. Fu, D. Xu, G. de Melo, Data augmentation with ad-
versarialtrainingforcross-lingualNLI,in: Proceedingsofthe59thAnnual
Meeting of the Association for Computational Linguistics and the 11th In-
ternational Joint Conference on Natural Language Processing (Volume 1:
Long Papers), Association for Computational Linguistics, Online, 2021,
pp. 5158–5167. doi:10.18653/v1/2021.acl-long.401 .
URL https://aclanthology.org/2021.acl-long.401
[129] A. Riabi, T. Scialom, R. Keraron, B. Sagot, D. Seddah, J. Staiano, Syn-
thetic data augmentation for zero-shot cross-lingual question answering,
in: M. Moens, X. Huang, L. Specia, S. W. Yih (Eds.), Proceedings of
the 2021 Conference on Empirical Methods in Natural Language Process-
ing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic,
7-11 November, 2021, Association for Computational Linguistics, 2021,
pp. 7016–7030.
URL https://aclanthology.org/2021.emnlp-main.562
[130] X. Xu, G. Wang, Y.-B. Kim, S. Lee, AugNLG: Few-shot natural language
generation using self-trained data augmentation, in: Proceedings of the
59thAnnualMeetingoftheAssociationforComputationalLinguisticsand
the 11th International Joint Conference on Natural Language Processing
(Volume 1: Long Papers), Association for Computational Linguistics, On-
line, 2021, pp. 1183–1195. doi:10.18653/v1/2021.acl-long.95 .
URL https://aclanthology.org/2021.acl-long.95
51

--- PAGE 52 ---
[131] C. Si, Z. Zhang, F. Qi, Z. Liu, Y. Wang, Q. Liu, M. Sun, Better robustness
by more coverage: Adversarial and mixup data augmentation for robust
ﬁnetuning, in: Findings of the Association for Computational Linguistics:
ACL-IJCNLP 2021, Association for Computational Linguistics, Online,
2021, pp. 1569–1576. doi:10.18653/v1/2021.findings-acl.137 .
URL https://aclanthology.org/2021.findings-acl.137
[132] F.Gao, J.Zhu, L.Wu, Y.Xia, T.Qin, X.Cheng, W.Zhou, T.-Y.Liu, Soft
contextualdataaugmentationforneuralmachinetranslation, in: Proceed-
ingsofthe57thAnnualMeetingoftheAssociationforComputationalLin-
guistics, Association for Computational Linguistics, Florence, Italy, 2019,
pp. 5539–5544. doi:10.18653/v1/P19-1555 .
URL https://aclanthology.org/P19-1555
[133] M. Xia, X. Kong, A. Anastasopoulos, G. Neubig, Generalized data aug-
mentation for low-resource translation, in: Proceedings of the 57th An-
nual Meeting of the Association for Computational Linguistics, Associa-
tion for Computational Linguistics, Florence, Italy, 2019, pp. 5786–5796.
doi:10.18653/v1/P19-1579 .
URL https://aclanthology.org/P19-1579
[134] N. Malandrakis, M. Shen, A. K. Goyal, S. Gao, A. Sethi, A. Metallinou,
Controlled text generation for data augmentation in intelligent artiﬁcial
agents, in: A. Birch, A. M. Finch, H. Hayashi, I. Konstas, T. Luong,
G. Neubig, Y. Oda, K. Sudoh (Eds.), Proceedings of the 3rd Workshop on
Neural Generation and Translation@EMNLP-IJCNLP 2019, Hong Kong,
November 4, 2019, Association for Computational Linguistics, 2019, pp.
90–98. doi:10.18653/v1/D19-5609 .
URL https://doi.org/10.18653/v1/D19-5609
[135] R. Cao, R. K. Lee, Hategan: Adversarial generative-based data augmen-
tation for hate speech detection, in: D. Scott, N. Bel, C. Zong (Eds.),
Proceedings of the 28th International Conference on Computational Lin-
guistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020,
International Committee on Computational Linguistics, 2020, pp. 6327–
6338. doi:10.18653/v1/2020.coling-main.557 .
URL https://doi.org/10.18653/v1/2020.coling-main.557
[136] J. Kukacka, V. Golkov, D. Cremers, Regularization for deep learning: A
taxonomy, CoRR abs/1710.10686 (2017). arXiv:1710.10686 .
URL http://arxiv.org/abs/1710.10686
[137] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. C. Courville, Y. Bengio, Generative adversarial networks,
CoRR abs/1406.2661 (2014). arXiv:1406.2661 .
URL http://arxiv.org/abs/1406.2661
52

--- PAGE 53 ---
[138] J. Morris, E. Liﬂand, J. Y. Yoo, J. Grigsby, D. Jin, Y. Qi, TextAttack:
A framework for adversarial attacks, data augmentation, and adversar-
ial training in NLP, in: Proceedings of the 2020 Conference on Empir-
ical Methods in Natural Language Processing: System Demonstrations,
Association for Computational Linguistics, Online, 2020, pp. 119–126.
doi:10.18653/v1/2020.emnlp-demos.16 .
URL https://aclanthology.org/2020.emnlp-demos.16
[139] B. Zheng, L. Dong, S. Huang, W. Wang, Z. Chi, S. Singhal, W. Che,
T. Liu, X. Song, F. Wei, Consistency regularization for cross-lingual ﬁne-
tuning, in: C. Zong, F. Xia, W. Li, R. Navigli (Eds.), Proceedings of the
59thAnnualMeetingoftheAssociationforComputationalLinguisticsand
the 11th International Joint Conference on Natural Language Processing,
ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-
6, 2021, Association for Computational Linguistics, 2021, pp. 3403–3417.
doi:10.18653/v1/2021.acl-long.264 .
URL https://doi.org/10.18653/v1/2021.acl-long.264
[140] T. Dao, A. Gu, A. Ratner, V. Smith, C. De Sa, C. Ré, A kernel theory
of modern data augmentation, in: International Conference on Machine
Learning, PMLR, 2019, pp. 1528–1537.
[141] S. Chen, E. Dobriban, J. Lee, A group-theoretic framework for data aug-
mentation, Advances in neural information processing systems 33 (2020)
21321–21333.
[142] S. Wu, H. Zhang, G. Valiant, C. Ré, On the generalization eﬀects of linear
transformations in data augmentation, in: International Conference on
Machine Learning, PMLR, 2020, pp. 10410–10420.
[143] J. Zhou, Y. Zheng, J. Tang, L. Jian, Z. Yang, FlipDA: Eﬀective and ro-
bust data augmentation for few-shot learning, in: Proceedings of the 60th
Annual Meeting of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), Association for Computational Linguistics, Dublin,
Ireland, 2022, pp. 8646–8665. doi:10.18653/v1/2022.acl-long.592 .
URL https://aclanthology.org/2022.acl-long.592
[144] A. Liu, S. Swayamdipta, N. A. Smith, Y. Choi, Wanli: Worker and ai col-
laboration for natural language inference dataset creation, arXiv preprint
arXiv:2201.05955 (2022).
[145] D. Singh, S. Reddy, W. Hamilton, C. Dyer, D. Yogatama, End-to-end
training of multi-document reader and retriever for open-domain question
answering, Advances in Neural Information Processing Systems 34 (2021)
25968–25981.
[146] D. Yogatama, C. de Masson d’Autume, L. Kong, Adaptive semiparamet-
ric language models, Transactions of the Association for Computational
Linguistics 9 (2021) 362–373.
53

--- PAGE 54 ---
[147] U. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, M. Lewis, General-
ization through memorization: Nearest neighbor language models, arXiv
preprint arXiv:1911.00172 (2019).
[148] S. Borgeaud, A. Mensch, J. Hoﬀmann, T. Cai, E. Rutherford, K. Millican,
G. v. d. Driessche, J.-B. Lespiau, B. Damoc, A. Clark, et al., Improv-
ing language models by retrieving from trillions of tokens, arXiv preprint
arXiv:2112.04426 (2021).
[149] M. Komeili, K. Shuster, J. Weston, Internet-augmented dialogue genera-
tion, arXiv preprint arXiv:2107.07566 (2021).
[150] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,
C. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al., Webgpt:
Browser-assisted question-answering with human feedback, arXiv preprint
arXiv:2112.09332 (2021).
[151] C.Raﬀel, N.Shazeer, A.Roberts, K.Lee, S.Narang, M.Matena, Y.Zhou,
W. Li, P. J. Liu, Exploring the limits of transfer learning with a uniﬁed
text-to-text transformer, J. Mach. Learn. Res. 21 (2020) 140:1–140:67.
URL http://jmlr.org/papers/v21/20-074.html
[152] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-
Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler,
J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,
B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,
D. Amodei, Language models are few-shot learners, in: H. Larochelle,
M. Ranzato, R. Hadsell, M. Balcan, H. Lin (Eds.), Advances in Neural
Information Processing Systems 33: Annual Conference on Neural
Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020.
URL https://proceedings.neurips.cc/paper/2020/hash/
1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html
54

--- PAGE 55 ---
Appendix A. Related Resources
Therearesome popular resources thatprovideshelpfulinformation
or APIs of DA.
- A Visual Survey of Data Augmentation in NLP (Blog)
- EDA: Easy Data Augmentation Techniques for Boosting Performance on
Text Classiﬁcation Tasks (Repo)
- Unsupervised Data Augmentation (Repo)
- Unsupervised Data Augmentation (Pytorch) (Repo)
- nlpaug: Data Augmentation in NLP (Repo)
- TextAttack: Generating Adversarial Examples for NLP Models (Repo)
- AugLy: ADataAugmentationsLibraryforAudio, Image, Text, andVideo
(Repo)
- NL-Augmenter: A Collaborative Repository of Natural Language Trans-
formations (Repo)
In addition to English, there are resources in other languages such
as:
- Turkish: nlpaug: Data Augmentation in NLP (Repo)
- Chinese: NLP Data Augmentation with EDA, BERT, BART, and back-
translation (Repo)
- Chinese: ChineseNLPDataAugmentation4Paddle: NLP Data Augmenta-
tionwithEDAandBERTContextualAugmentation, CustomizedforPad-
dleNLP (Repo)
- Chinese: Tencent AI Lab Embedding Corpus for Chinese Words and
Phrases (Link)
55
