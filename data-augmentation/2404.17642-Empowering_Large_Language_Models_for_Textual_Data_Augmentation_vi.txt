# Tăng cường khả năng của các Mô hình Ngôn ngữ Lớn cho Tăng cường Dữ liệu Văn bản

## Tóm tắt
Với khả năng hiểu và thực hiện các hướng dẫn bằng ngôn ngữ tự nhiên, các mô hình ngôn ngữ lớn (LLM) có thể có tiềm năng hoạt động như một công cụ mạnh mẽ cho việc tăng cường dữ liệu văn bản. Tuy nhiên, chất lượng dữ liệu được tăng cường phụ thuộc rất nhiều vào các hướng dẫn tăng cường được cung cấp, và hiệu quả có thể dao động qua các tác vụ hạ nguồn khác nhau. Trong khi việc thiết kế thủ công và lựa chọn hướng dẫn có thể mang lại một số cải thiện, cách tiếp cận này đối mặt với các vấn đề về khả năng mở rộng và tính nhất quán trong thực tế do sự đa dạng của các tác vụ hạ nguồn. Trong công trình này, chúng tôi giải quyết những hạn chế này bằng cách đề xuất một giải pháp mới, có thể tự động tạo ra một nhóm lớn các hướng dẫn tăng cường và lựa chọn các hướng dẫn phù hợp nhất được thông báo theo tác vụ, do đó tăng cường khả năng của LLM để tạo ra dữ liệu tăng cường chất lượng cao cho các tác vụ hạ nguồn khác nhau. Thông qua thực nghiệm, phương pháp được đề xuất tạo ra dữ liệu tăng cường với chất lượng tốt hơn một cách nhất quán so với các phương pháp tăng cường dữ liệu không dựa trên LLM và dựa trên LLM, dẫn đến hiệu suất tốt nhất trên 26 tác vụ học few-shot được lấy từ một loạt các lĩnh vực ứng dụng rộng rãi.

## 1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) gần đây đã chứng minh tiềm năng của chúng trong việc thực hiện tăng cường dữ liệu trên dữ liệu văn bản. Đóng vai trò như một hàm biến đổi bảo toàn ngữ nghĩa, LLM biến đổi văn bản gốc dựa trên các hướng dẫn để tạo ra các tăng cường dữ liệu đa dạng và thông tin. Với dữ liệu được tăng cường, người dùng có thể tiếp tục huấn luyện một mô hình có thể trải rộng và giá cả phải chăng (ví dụ: OPT) để thực hiện các tác vụ cụ thể.

Khác với các phương pháp truyền thống dựa trên heuristic như hoán đổi từ và các phương pháp dựa trên mô hình như dịch ngược, LLM mang lại tiềm năng lớn để tạo ra các tăng cường trôi chảy, đa dạng và nhất quán về mặt ngữ nghĩa hơn cho dữ liệu văn bản, nhờ vào khả năng hiểu biết và tổng quát hóa tuyệt vời của chúng.

Mặc dù có thành công ban đầu của LLM trong tăng cường dữ liệu văn bản, các phương pháp hiện tại đơn giản nhắc LLM với các hướng dẫn tăng cường do con người tạo ra (tức là các phương pháp Manual-LLMDA) có những nghẽn cổ chai chính sau: (1) Đầu tiên, hiệu quả của chúng phụ thuộc rất nhiều vào chất lượng của các hướng dẫn tăng cường, được thiết kế thủ công bởi các chuyên gia lĩnh vực. Quá trình thủ công này không chỉ đòi hỏi kiến thức chuyên sâu về lĩnh vực mà còn dễ có sự không nhất quán, có thể ảnh hưởng đến chất lượng dữ liệu tăng cường. Các biến thể tinh tế trong cách thức công thức hóa các hướng dẫn này có thể ảnh hưởng đáng kể đến kết quả, như đã được chứng minh trong các nghiên cứu gần đây; (2) Thứ hai, thông thường các hướng dẫn tăng cường văn bản được viết theo dạng không phụ thuộc vào tác vụ cho mục đích chung, tuy nhiên, việc thiếu thông tin ngữ cảnh về các tác vụ hạ nguồn có thể dẫn đến sự chênh lệch hiệu suất đáng kể trên các tác vụ hạ nguồn khác nhau, như được hiển thị trong Hình 1. Mà không xem xét các tính chất cụ thể của tác vụ đích, LLM có thể tạo ra dữ liệu tăng cường chất lượng thấp.

Để giải quyết những thách thức nêu trên, trong bài báo này, chúng tôi giới thiệu một framework mới - Self-LLMDA tự động hóa việc tạo ra và lựa chọn hướng dẫn tăng cường, tạo điều kiện cho LLM tạo ra dữ liệu tăng cường cụ thể theo tác vụ. Giai đoạn đầu tiên của Self-LLMDA nhằm mở rộng phạm vi của các chiến lược tăng cường hạt giống thông qua việc tạo ra các hướng dẫn đa dạng và hiệu quả dựa trên LLM. Tiếp theo, Self-LLMDA sử dụng một mô hình chấm điểm để xác định và lựa chọn các hướng dẫn phù hợp nhất có khả năng tăng cường hiệu suất của các mô hình đích. Cách tiếp cận tăng cường dữ liệu văn bản mới này đảm bảo sự cân bằng giữa độ rộng sinh tạo của các hướng dẫn tăng cường và độ chính xác nhắm mục tiêu của hướng dẫn cụ thể theo tác vụ cho các tác vụ hạ nguồn.

Trong nghiên cứu của chúng tôi, chúng tôi tiến hành các thí nghiệm rộng rãi trên một bộ sưu tập lớn các tác vụ học few-shot được sử dụng trong các nghiên cứu trước đây. Bộ sưu tập này bao gồm 26 loại tác vụ khác nhau qua việc phát hiện lời nói thù hận, trả lời câu hỏi, suy luận ngôn ngữ tự nhiên, và các tập dữ liệu phát hiện cụm từ. Nghiên cứu của chúng tôi nổi bật với phạm vi bao quát rộng rãi về các tác vụ, thiết lập một chuẩn mực mới trong việc áp dụng LLM cho tăng cường dữ liệu văn bản khi so sánh với các công trình trước đây. Kết quả thực nghiệm chứng minh rằng phương pháp Self-LLMDA được đề xuất vượt trội đáng kể so với các phương pháp cơ sở khác nhau trong việc tạo ra dữ liệu văn bản tăng cường chất lượng cao.

Để tóm tắt, những đóng góp chính của chúng tôi như sau:
• Chúng tôi giới thiệu một framework Self-LLMDA, tự động hóa việc tạo ra và lựa chọn các hướng dẫn tăng cường cụ thể theo tác vụ cho LLM, cung cấp tăng cường dữ liệu hiệu quả cho dữ liệu văn bản.
• Thông qua một tập hợp toàn diện các thí nghiệm, chúng tôi xác nhận hiệu quả của Self-LLMDA, chứng minh hiệu suất vượt trội trong việc nâng cao chất lượng dữ liệu và độ chính xác của mô hình so với các phương pháp tăng cường dữ liệu văn bản hiện tại.
• Các phân tích chuyên sâu của chúng tôi tiết lộ rằng Self-LLMDA có thể tổng quát hóa tốt qua các mô hình đích khác nhau và các hướng dẫn tăng cường chưa thấy trước đây, chứng minh tính linh hoạt và tiềm năng ứng dụng rộng rãi.

## 2 Nghiên cứu Liên quan

### 2.1 Tăng cường Dữ liệu Văn bản Không dựa trên LLM
Các phương pháp tăng cường dữ liệu văn bản truyền thống bao gồm nhiều kỹ thuật khác nhau nhằm tăng cường sự đa dạng của các tập dữ liệu văn bản mà không dựa vào các mô hình ngôn ngữ lớn (tức là các phương pháp Non-LLMDA). Các phương pháp này bao gồm từ các phương pháp heuristic đơn giản đến các phương pháp dựa trên mô hình sinh tạo.

Đối với các cách tiếp cận dựa trên heuristic, như thay thế từ đồng nghĩa và xáo trộn từ, nổi bật với hiệu quả tính toán và sự đơn giản, khiến chúng lý tưởng cho tăng cường dữ liệu quy mô lớn với yêu cầu tính toán tối thiểu. Một ví dụ đáng chú ý khác là kỹ thuật Easy Data Augmentation (EDA) được giới thiệu bởi Wei và Zou (2019), sử dụng các nhiễu loạn cấp token - chèn ngẫu nhiên, xóa và hoán đổi - để cải thiện hiệu suất qua một loạt các tác vụ phân loại văn bản.

Đối với các cách tiếp cận dựa trên mô hình, các nhà nghiên cứu đã sử dụng các mô hình seq2seq và mô hình ngôn ngữ cho tăng cường dữ liệu. Dịch ngược sử dụng các mô hình dịch để bảo toàn tính toàn vẹn ngữ nghĩa trong khi tạo ra các câu diễn giải lại. Các mô hình ngôn ngữ có điều kiện như BERT và RoBERTa cũng có thể được sử dụng cho tăng cường dữ liệu. Bằng cách che các từ trong câu và sau đó tạo ra các từ thay thế, các mô hình này giới thiệu các biến thể ngôn ngữ. Hơn nữa, các phương pháp khác tận dụng khả năng của các mô hình ngôn ngữ sinh tạo như GPT-2 và BART cho tăng cường dữ liệu. Các cách tiếp cận này thực hiện tạo có điều kiện dựa trên nhãn lớp. Ngoài ra, một số nghiên cứu đã khám phá tăng cường trong không gian đặc trưng. Các kỹ thuật Mixup nội suy trong embedding từ hoặc câu, trong khi những kỹ thuật khác giới thiệu nhiễu nhân và cộng ngẫu nhiên vào các vector đặc trưng. Mặc dù hữu ích, các phương pháp Non-LLMDA truyền thống này thường đi kèm với những hạn chế về khả năng đọc và tính nhất quán theo ngữ cảnh.

### 2.2 Tăng cường Dữ liệu Văn bản dựa trên LLM
Những tiến bộ gần đây trong LLM đã chứng minh sự vượt trội của chúng trong việc tạo ra dữ liệu tăng cường chất lượng cao và phù hợp với ngữ cảnh. LLM ngày càng được sử dụng như các hàm biến đổi bảo toàn nhãn, trong đó một ví dụ ban đầu được biến đổi hoặc nhiễu loạn theo các hướng dẫn được tạo thủ công.

Đồng thời, một số nghiên cứu đã khám phá việc tạo ra các ví dụ tổng hợp tương tự về mặt khái niệm nhưng khác biệt về mặt ngữ nghĩa. Tuy nhiên, các phương pháp này phần lớn dựa vào thiết kế hướng dẫn thủ công. Ngược lại, công trình của chúng tôi tự động tạo ra các hướng dẫn tăng cường bảo toàn nhãn bằng cách nhắc LLM, do đó giảm sự phụ thuộc vào các hướng dẫn được tạo thủ công. Hơn nữa, chúng tôi giới thiệu một mô hình lựa chọn hướng dẫn chọn các hướng dẫn phù hợp cho các tác vụ hạ nguồn tùy ý.

## 3 Bài toán Chuẩn bị

**Định nghĩa Bài toán.** Tăng cường dữ liệu văn bản liên quan đến việc áp dụng một hàm biến đổi bảo toàn nhãn T(·) vào một tập dữ liệu D={(xi,yi)}^k_{i=1}, trong đó mỗi ví dụ bao gồm một văn bản đầu vào xi (một chuỗi các token) và một nhãn tương ứng yi (cũng là một chuỗi các token). Tập dữ liệu tăng cường D′ được tạo ra như sau, đảm bảo rằng nhãn đầu ra y′i không thay đổi:

x′i = T(xi), y′i = yi. (1)

Một mô hình đích F sau đó được huấn luyện trên sự kết hợp của các tập dữ liệu gốc và tăng cường, D ∪ D′, với mục tiêu huấn luyện được định nghĩa là:

L(x̂i,ŷi)∈D∪D′(Fθ(x̂i),ŷi). (2)

Do đó, việc thiết kế một hàm biến đổi T(·) hiệu quả tạo ra dữ liệu tăng cường D′ chất lượng cao là quan trọng để cải thiện hiệu suất hạ nguồn của mô hình Fθ.

**Manual-LLMDA.** Đối với các phương pháp Manual-LLMDA, hàm biến đổi T(·) được thực hiện thông qua sự kết hợp của một LLM và một hướng dẫn được tạo thủ công Iman (ví dụ: diễn giải lại). LLM được nhắc để tạo ra các biến đổi bảo toàn ngữ nghĩa của văn bản đầu vào xi cho tập dữ liệu tăng cường D′:

x′i = LLM(Iman, xi), y′i = yi (3)

## 4 Phương pháp Đề xuất - Self-LLMDA

Để giảm những nỗ lực của con người trong việc thiết kế các hướng dẫn tăng cường và lựa chọn hướng dẫn cụ thể theo tác vụ cho một tác vụ cụ thể, chúng tôi đề xuất Self-LLMDA được mô tả trong Hình 2. Quá trình bắt đầu với việc LLM tạo ra một tập hợp đa dạng các hướng dẫn tiềm năng I={Ij}^n_{j=0} từ một tập hợp các hướng dẫn hạt giống I_seed={I_man}:

I = LLM(I_seed). (4)

Một mô hình lựa chọn S sau đó chấm điểm các hướng dẫn được tạo này đối với tập dữ liệu D để xác định hướng dẫn phù hợp nhất I*:

I* = S(I, D). (5)

Dựa trên hướng dẫn được chọn I*, LLM thực hiện tăng cường dữ liệu trên D, tạo ra một tập dữ liệu tăng cường D′ được nâng cao để huấn luyện mô hình đích hiệu quả hơn.

### 4.1 Tự Tạo Hướng dẫn Tăng cường

Lấy cảm hứng từ phương pháp học self-instruct, giai đoạn này tạo ra các hướng dẫn tăng cường từ một tập hạt giống gồm 13 hướng dẫn được tạo thủ công bởi con người. Các hướng dẫn hạt giống này đóng vai trò như các mẫu, hướng dẫn LLM tạo ra các hướng dẫn mới và đa dạng duy trì tính toàn vẹn ngữ nghĩa của văn bản đầu vào. Để tạo ra một tập hợp rộng rãi và đa dạng các hướng dẫn tăng cường mà không có sự thiên vị được giới thiệu bởi một vài ví dụ tác vụ, chúng tôi loại trừ dữ liệu cụ thể theo tác vụ khỏi việc tạo hướng dẫn. Điều này sẽ tận dụng khả năng học zero-shot của LLM để tạo ra một loạt rộng các hướng dẫn tăng cường tiềm năng. Chúng tôi sử dụng lời nhắc sau để khuyến khích LLM khám phá các kỹ thuật tăng cường khác nhau:

"Nghĩ ra một loạt các phương pháp tăng cường dữ liệu văn bản và bạn cần tạo ra phương pháp tăng cường dữ liệu đa dạng hơn có thể giữ nguyên ý nghĩa ngữ nghĩa của câu đầu vào. {I_seed}"

Thông qua các chu kỳ tạo ra và tinh chỉnh lặp đi lặp lại, chúng tôi lọc ra các hướng dẫn quá giống với những hướng dẫn hiện có dựa trên ROUGE-L. Các hướng dẫn độc đáo được tạo ra từ mỗi lần lặp sau đó được kết hợp trở lại vào nhóm hướng dẫn hạt giống, làm phong phú thêm các hướng dẫn hạt giống cho các vòng tạo tiếp theo. Quá trình này được lặp lại cho đến khi chúng tôi đạt được một bộ sưu tập gồm 100 hướng dẫn tăng cường. Để đảm bảo sự đa dạng và loại bỏ sự dư thừa, chúng tôi tiếp tục tinh chỉnh tập hợp này bằng cách loại bỏ các bản sao dựa trên tên phương pháp của chúng. Quá trình lọc này dẫn đến một tập hợp cuối cùng gồm 51 hướng dẫn tăng cường độc đáo.

### 4.2 Lựa chọn Hướng dẫn Được Thông báo theo Tác vụ

Nhận thức rằng các hướng dẫn tăng cường có thể không áp dụng được tổng quát qua các tác vụ khác nhau, chúng tôi triển khai một cơ chế lựa chọn, được điều chỉnh theo các yêu cầu cụ thể của mỗi tác vụ và mô hình đích tương ứng. Quá trình này liên quan đến một mô hình chấm điểm S để đánh giá tính phù hợp của mỗi hướng dẫn cho tác vụ hiện tại. Mô hình chấm điểm S, như được hiển thị trong Hình 3, đưa ra một điểm xếp hạng qj cho biết hiệu quả của hướng dẫn dựa trên cặp hướng dẫn và tập dữ liệu tác vụ. Dựa trên khả năng tuân theo hướng dẫn đáng chú ý của FLAN-T5, chúng tôi chọn FLAN-T5-Large làm xương sống của mô hình chấm điểm. Đầu vào cho mô hình chấm điểm S là:

"Cho tập dữ liệu cho tác vụ T và hướng dẫn dữ liệu, xác định xem đây có phải là hướng dẫn phù hợp để giải quyết tác vụ cho mô hình F không. Tập dữ liệu tác vụ: {xi}^m_{i=0} Hướng dẫn: Ij. Hướng dẫn này có phù hợp không?"

trong đó T là tên tác vụ (ví dụ: GLUE-RTE), F là tên mô hình đích (ví dụ: OPT-125m). Vì hầu hết các tác vụ không có mô tả tác vụ và việc thiết kế thủ công mô tả tác vụ tốn thời gian, chúng tôi sử dụng các ví dụ few-shot {xi}^m_{i=0} từ tập dữ liệu làm mô tả tác vụ. Ở đây, chúng tôi tính toán qj bằng cách đánh giá giá trị logit của token "yes" ở vị trí cuối cùng của đầu vào từ FLAN-T5-Large, như được hiển thị trong Hình 3. Tiếp theo, chúng tôi sẽ giới thiệu quy trình tối ưu hóa và suy luận của mô hình chấm điểm S tương ứng.

**Tối ưu hóa Mô hình.** Mô hình lựa chọn hướng dẫn được huấn luyện để ưu tiên các hướng dẫn tăng cường được tạo ra dựa trên tác động của chúng đối với hiệu suất tác vụ hạ nguồn. Mục tiêu của nó là gán điểm cao nhất cho các hướng dẫn dẫn đến tăng cường dữ liệu hiệu quả nhất. Để tăng cường khả năng mở rộng và hiệu quả tính toán, mô hình của chúng tôi tối ưu hóa quá trình lựa chọn cho một tác vụ cụ thể D bằng cách lấy mẫu một tập con các hướng dẫn tăng cường {Ij}^n_{j=0} (trong đó n > 1) từ nhóm ứng cử viên. Mô hình sau đó tính toán điểm {qj}^n_{j=0}, đại diện cho hiệu quả tương đối của mỗi hướng dẫn. Mục tiêu tối ưu hóa được công thức hóa như một hàm mất mát cross-entropy, được thiết kế để phân biệt chính xác giữa hiệu quả của các hướng dẫn này {Ij}^n_{j=0}. Hàm mất mát được cho bởi:

L_S = -∑^n_{j=0} is_max(rj) log σ(qj) (6)

Ở đây, is_max đóng vai trò như một hàm chỉ báo nhị phân xác định hướng dẫn tạo ra hiệu quả tối đa, và σ là softmax chuẩn hóa qj (xác suất tạo ra token "yes" được hiểu là điểm xếp hạng liên quan đến hướng dẫn thứ j) trên các hướng dẫn tăng cường được lấy mẫu, rj là hiệu suất tác vụ hạ nguồn của một mô hình đích được huấn luyện trên dữ liệu tăng cường D′j ∪ D.

**Suy luận Mô hình.** Khi gặp phải một tác vụ mới, mô hình lựa chọn S đánh giá tất cả các hướng dẫn tiềm năng để xác định hướng dẫn phù hợp nhất I*, được biểu thị bằng điểm cao nhất:

I* = I_argmax({qj}^{|I|}_{j=0}) (7)

Hướng dẫn tối ưu này, I*, sau đó được sử dụng để nhắc LLM tạo ra dữ liệu tăng cường. Cơ chế lựa chọn này đảm bảo việc sử dụng hướng dẫn hiệu quả nhất để tăng cường tính hữu ích của dữ liệu qua các tác vụ NLP đa dạng.

## 5 Thí nghiệm

### 5.1 Thiết lập Thí nghiệm

**Tập dữ liệu Đánh giá.** Trong nghiên cứu này, chúng tôi chọn 26 tác vụ học few-shot trải rộng một loạt các thách thức NLP, được lấy từ CrossFit, UnifiedQA, và MetaICL. Các tập dữ liệu này được chọn vì tính đa dạng, bao gồm cả các tác vụ phân loại (Class)—như suy luận ngôn ngữ tự nhiên, phát hiện diễn giải, và nhận dạng lời nói thù hận—và các tác vụ không phân loại (Non-Class), đặc biệt là trả lời câu hỏi, để đảm bảo một phổ đánh giá rộng rãi. Việc lựa chọn tác vụ lớn hơn đáng kể và đa dạng hơn so với các công trình liên quan khác.

Để điều tra khả năng tổng quát hóa của Self-LLMDA, chúng tôi chia 26 tác vụ thành các tác vụ huấn luyện và kiểm tra theo dạng "train → test". Chúng tôi huấn luyện các phương pháp lựa chọn hướng dẫn tăng cường trên các tác vụ huấn luyện và đánh giá chúng trên các tác vụ kiểm tra. Việc chia tác vụ liên quan đến bốn cài đặt: Class → Class, Class → Non-Class, Non-Class → Class, và Random → Random, trong đó "Random" đại diện cho một hỗn hợp các tác vụ được chọn ngẫu nhiên. Thiết kế này cho phép chúng tôi điều tra hiệu suất của các mô hình lựa chọn khi được áp dụng qua các loại tác vụ tương tự và khác biệt, cung cấp cái nhìn sâu sắc về khả năng tổng quát hóa và hiệu quả của chúng.

**Số liệu Đánh giá.** Để xử lý đồng thời tất cả các loại tác vụ, chúng tôi thống nhất tất cả các tác vụ hạ nguồn, bao gồm các tác vụ phân loại và không phân loại, sử dụng cách tiếp cận text-to-text. Đối với mỗi tác vụ, chúng tôi đưa văn bản đầu vào vào mô hình đích Fθ và huấn luyện nó để tạo ra văn bản đích tương ứng. Chúng tôi chọn OPT từ ba kích thước khác nhau (ví dụ: 125m, 350m và 1.3b) làm mô hình đích Fθ. Trong quá trình huấn luyện, Fθ nhận ví dụ huấn luyện xi làm đầu vào, và được tối ưu hóa để tạo ra yi sử dụng hàm mục tiêu negative likelihood:

L_Fθ(yi) = -∑^{|yi|}_{t=1} log P_Fθ(y^t_i|xi, y^{<t}_i) (8)

Trong thời gian suy luận, cho đầu vào kiểm tra x_test cũng như một tập hợp các ứng cử viên C, có thể là một tập hợp các nhãn (trong các tác vụ phân loại) hoặc các lựa chọn câu trả lời (trong các tác vụ không phân loại), Fθ tính toán xác suất có điều kiện của mỗi nhãn c ∈ C, trong đó c là một chuỗi các token. Nhãn có xác suất có điều kiện tối đa được trả về như một dự đoán:

argmax_{c∈C} ∑^{|c|}_{t=1} log P_Fθ(c^t|x_test, c^{<t}) (9)

Cụ thể, chúng tôi sử dụng macro-F1 cho các tác vụ phân loại, và accuracy cho các tác vụ không phân loại trong thí nghiệm của chúng tôi. Hiệu suất tổng thể sau đó được lượng hóa bằng cách tính toán macro-average của các điểm này qua tất cả các tác vụ, bao gồm cả số liệu accuracy và macro-F1. Để đảm bảo độ bền vững và giảm thiên vị lấy mẫu, mỗi thí nghiệm dưới mỗi cài đặt chia tách được lặp lại với năm random seed khác nhau. Đối với mỗi tác vụ few-shot, chúng tôi áp dụng một cách tiếp cận thống nhất bằng cách chọn ngẫu nhiên k = 16 ví dụ huấn luyện. Theo Min và cộng sự (2022), chúng tôi không tạo ra sự cân bằng nhãn hoàn hảo giữa k ví dụ huấn luyện.

**Phương pháp Cơ sở.** Trong nghiên cứu này, chúng tôi so sánh pipeline tăng cường mới của chúng tôi, Self-LLMDA, với hai danh mục khác nhau của các phương pháp tăng cường dữ liệu làm baseline: Non-LLMDA và Manual-LLMDA. Đối với cả Manual-LLMDA và Self-LLMDA, chúng tôi sử dụng GPT-3.5 Turbo làm LLM xương sống. Để biết mô tả chi tiết về các phương pháp baseline này, vui lòng xem Phụ lục E. Cụ thể:

• **Phương pháp Non-LLMDA.** Danh mục này bao gồm 13 kỹ thuật tăng cường truyền thống: **Cấp Ký tự:** Các thao tác như hoán đổi ngẫu nhiên, mô phỏng lỗi OCR, xóa, chèn, và thay thế. **Cấp Từ:** Các biến đổi, bao gồm hoán đổi từ, xóa, lỗi chính tả, và chèn dựa trên embedding. **Cấp Ngữ cảnh:** Sử dụng các mô hình ngôn ngữ cho việc chèn từ (ví dụ: sử dụng GPT2) và thay thế (ví dụ: với BERT), và dịch ngược.

• **Phương pháp Manual-LLMDA.** Tập hợp này bao gồm 13 hướng dẫn tăng cường được thiết kế thủ công cho LLM, bao gồm: **Cấp Ký tự:** Các nhiễu loạn tương tự như trong Non-LLMDA. **Cấp Từ:** Hoán đổi, thay thế, và tăng cường part-of-speech (POS). **Cấp Câu:** Sắp xếp lại và các chiến lược trộn dữ liệu. **Cấp Ngữ cảnh:** Che dự đoán, thay thế theo ngữ cảnh, và dịch ngược.

Chúng tôi cũng báo cáo hiệu suất trung bình và tốt nhất của Non-LLMDA và Manual-LLMDA để so sánh tốt hơn. Một nghiên cứu ablation mở rộng về mô hình lựa chọn được thông báo theo tác vụ của chúng tôi, được trình bày trong § 5.3.

### 5.2 Kết quả Chính

Việc phân tích các kết quả thí nghiệm được trình bày trong Bảng 1 tiết lộ một số phát hiện: **Đầu tiên**, có sự không nhất quán về hiệu suất giữa các hướng dẫn khác nhau từ Manual-LLMDA. Tác động của các hướng dẫn tăng cường khác nhau qua các tác vụ hạ nguồn và mô hình khác nhau. Điều này làm nổi bật khó khăn trong việc tạo ra hướng dẫn tăng cường dữ liệu hiệu quả tổng quát. **Thứ hai**, Manual-LLMDA không phải lúc nào cũng tốt hơn Non-LLMDA. Trong các so sánh có kiểm soát tập trung vào các chủ đề tăng cường cụ thể, những ưu điểm của Manual-LLMDA so với Non-LLMDA không rõ ràng. Ví dụ, trong các ngữ cảnh của "Back Translation" và "Word Swap", Non-LLMDA vượt trội hơn Manual-LLMDA trong 5 trên 12 và 7 trên 12 trường hợp, tương ứng. **Cuối cùng**, kết quả thí nghiệm cho thấy sự vượt trội của Self-LLMDA. Mô hình đề xuất của chúng tôi liên tục vượt trội hơn các phương pháp baseline này, làm nổi bật hiệu quả của việc tích hợp tạo hướng dẫn tự động với lựa chọn hướng dẫn cụ thể theo tác vụ có mục tiêu. Cách tiếp cận này không chỉ tối ưu hóa hiệu suất mà còn giảm những nỗ lực thủ công thường được yêu cầu để thiết kế các chiến lược tăng cường hiệu quả, thể hiện tiềm năng của mô hình chúng tôi trong việc nâng cao các thực hành tăng cường dữ liệu.

### 5.3 Nghiên cứu Ablation

Chúng tôi thêm một nghiên cứu ablation để hiểu tác động của hai thành phần chính trong framework của chúng tôi: tự tạo hướng dẫn tăng cường và lựa chọn hướng dẫn được thông báo theo tác vụ. **Đầu tiên**, chúng tôi huấn luyện một mô hình lựa chọn hướng dẫn được thông báo theo tác vụ S trên các hướng dẫn được tạo thủ công từ Manual-LLMDA và đặt tên là Manual-LLMDA+ để hiểu sự đóng góp của các hướng dẫn tăng cường tự tạo ra bởi LLM. **Thứ hai**, chúng tôi kiểm tra hiệu quả của mô hình lựa chọn của chúng tôi bằng cách so sánh ba chiến lược lựa chọn thay thế: (1) Random-select, chọn ngẫu nhiên hướng dẫn từ nhóm các phương pháp tăng cường cho mỗi tác vụ; (2) Empirical-select, chọn prompt đạt hiệu suất trung bình cao nhất qua các tác vụ huấn luyện, dưới giả định rằng các prompt thành công trên tác vụ huấn luyện sẽ tổng quát hóa tốt cho tác vụ kiểm tra; và (3) LLM-Select, nhắc LLM chọn hướng dẫn phù hợp nhất từ các ứng cử viên dựa trên các quá trình ra quyết định nội bộ. Kết quả trong Bảng 2 cho thấy Self-LLMDA liên tục vượt trội hơn các phương pháp thay thế này, cho thấy lợi ích của tự tạo hướng dẫn và lựa chọn được thông báo theo tác vụ trong việc nâng cao hiệu suất mô hình.

### 5.4 Phân tích Siêu tham số

Ở đây, chúng tôi kiểm tra chặt chẽ tác động của hai siêu tham số quan trọng đối với việc huấn luyện mô hình hướng dẫn được thông báo theo tác vụ của chúng tôi: n và m. Siêu tham số n chỉ định số lượng hướng dẫn tăng cường được lấy mẫu để tối ưu hóa Phương trình 6. Cần lưu ý rằng, chúng tôi chỉ thay đổi n tại thời gian huấn luyện, trong khi tại thời gian suy luận, chúng tôi sẽ tính toán điểm cho tất cả các hướng dẫn được tạo ra và chọn cái có điểm lớn nhất. Mặt khác, m xác định số lượng ví dụ từ tập dữ liệu tác vụ được sử dụng để đại diện cho tác vụ, ảnh hưởng đến hiệu suất của mô hình trong cả giai đoạn tối ưu hóa và suy luận. Phân tích của chúng tôi, được mô tả trong Hình 5, làm nổi bật một số phát hiện chính: (1) **Số lượng Hướng dẫn Tối ưu:** Chúng tôi nhận thấy rằng việc đặt n = 2 dẫn đến hiệu suất tốt nhất, vượt trội hơn các cấu hình khác. Điều này gợi ý rằng một so sánh theo cặp, như được công thức hóa trong Phương trình 6, hiệu quả nhất cho quá trình học của mô hình chúng tôi. (2) **Ví dụ Đại diện:** Thú vị là, một số lượng nhỏ hơn các ví dụ (m) dường như nắm bắt được bản chất của các tác vụ tốt hơn. Quan sát này cho thấy rằng một tập hợp lớn hơn các ví dụ có thể gây ra nhiễu, có thể làm giảm khả năng của mô hình trong việc đại diện chính xác các tác vụ để lựa chọn hướng dẫn.

### 5.5 Phân tích Chuyên sâu về Mô hình Lựa chọn Hướng dẫn Được Thông báo theo Tác vụ

Trong phần này, chúng tôi cung cấp một phân tích chi tiết về hiệu suất và khả năng tổng quát hóa của mô hình lựa chọn hướng dẫn S, tập trung vào khả năng tổng quát hóa của nó đối với các hướng dẫn tăng cường chưa biết, các mô hình đích chưa biết, và các nghiên cứu trường hợp cụ thể của các hướng dẫn tăng cường mà nó lựa chọn.

**Tổng quát hóa đối với Hướng dẫn Tăng cường Chưa biết.** Trong phân tích này, chúng tôi đi sâu vào khả năng thích ứng của mô hình lựa chọn với các hướng dẫn tăng cường chưa biết bằng cách mô phỏng một môi trường động trong đó các hướng dẫn mới được tạo ra bất đồng bộ bởi LLM. Kịch bản này phản ánh các ứng dụng thực tế trong đó tập hợp hướng dẫn tăng cường có thể mở rộng mà không cần phải huấn luyện lại mô hình lựa chọn. Để kiểm tra điều này, chúng tôi giới hạn giai đoạn huấn luyện của mô hình lựa chọn với một tập con giới hạn của các hướng dẫn tăng cường tự tạo ra (30% tổng số được tạo ra bởi LLM), sử dụng toàn bộ các hướng dẫn được tạo ra để đánh giá tại thời gian suy luận.

Như kết quả được hiển thị trong Hình 6, chúng tôi có thể quan sát thấy một sự cải thiện hiệu suất của mô hình lựa chọn của chúng tôi so với hiệu suất tốt nhất của Non-LLMDA và Manual-LLMDA. Điều này cho thấy độ bền vững của mô hình lựa chọn của chúng tôi trong việc thích ứng với các hướng dẫn tăng cường gia tăng, hiệu quả lựa chọn các hướng dẫn phù hợp ngay cả khi đối mặt với các hướng dẫn chưa biết trước đây. Những quan sát này làm nổi bật hiệu quả của mô hình lựa chọn của chúng tôi trong một kịch bản tăng cường động.

**Tổng quát hóa đối với Mô hình Đích Chưa biết.** Nghiên cứu của chúng tôi mở rộng để đánh giá khả năng thích ứng của mô hình lựa chọn được thông báo theo tác vụ qua các mô hình đích đa dạng. Bằng cách áp dụng mô hình lựa chọn, ban đầu được huấn luyện trên hiệu suất tác vụ của một mô hình đích cụ thể, cho các mô hình khác nhau. Kết quả của những thí nghiệm này được trình bày trong Bảng 3. Các phát hiện của chúng tôi cho thấy rằng các hướng dẫn tăng cường được chọn bởi mô hình của chúng tôi vẫn hiệu quả ngay cả khi được áp dụng cho các mô hình đích khác nhau. Đáng chú ý, trong hầu hết các kịch bản, mô hình Self-LLMDA của chúng tôi, khi được chuyển đổi sang các mô hình đích thay thế, vượt trội hơn kết quả tốt nhất thu được bằng cách sử dụng Non-LLMDA và Manual-LLMDA. Điều này cho thấy rằng mô hình cơ bản xác định hiệu quả hướng dẫn thông qua mô hình lựa chọn hướng dẫn của chúng tôi có thể chuyển đổi được.

**Phân tích về Các Hướng dẫn Được Chọn.** Chúng tôi tiến hành một phân tích chi tiết về các hướng dẫn tăng cường được chọn bởi mô hình lựa chọn của chúng tôi, và các phát hiện được trực quan hóa trong Hình 7. Những cái nhìn sâu sắc chính từ phân tích này như sau: (1) **Sự Đa dạng của Các Hướng dẫn Được Chọn:** Sự phân bố của các hướng dẫn được chọn thể hiện một sự đa dạng rộng rãi trong các loại tăng cường được chọn bởi mô hình, với 3, 2, và 6 hướng dẫn tăng cường dữ liệu độc đáo được xác định cho các mô hình 125m, 350m, và 1.3b dưới Class → Non-Class, tương ứng. Điều này chứng minh khả năng của mô hình trong việc thích ứng và lựa chọn từ một phổ rộng các chiến lược tăng cường để đáp ứng các yêu cầu cụ thể của các tác vụ khác nhau. (2) **Biến thiên qua Các Mô hình:** Các mô hình lựa chọn thể hiện sự khác biệt đáng chú ý khi mô hình được áp dụng cho các mô hình đích khác nhau. Sự biến thiên này cho thấy sự khác biệt về sở thích qua các mô hình đích khác nhau. (3) **Sở thích cho Các Hướng dẫn Dựa trên Diễn giải:** Một phần đáng kể của các hướng dẫn được chọn thuộc về danh mục tăng cường dựa trên diễn giải, như "Text Paraphrase", "Paraphrase", "Contextual Paraphrase", và "Sentence Paraphrase". Sở thích này không chỉ làm nổi bật hiệu quả và tính ứng dụng chung của các tăng cường dựa trên diễn giải mà còn minh họa khả năng sắc thái của mô hình lựa chọn được thông báo theo tác vụ trong việc phân biệt và đề xuất biến thể diễn giải phù hợp nhất cho một tác vụ cụ thể.

## 6 Kết luận

Trong công trình này, chúng tôi giới thiệu Self-LLMDA, một framework mới tận dụng khả năng của LLM cho tăng cường dữ liệu văn bản. Cách tiếp cận của chúng tôi giải quyết những thách thức liên quan đến các phương pháp tăng cường dữ liệu truyền thống và những hạn chế của việc tạo hướng dẫn thủ công trong tăng cường dựa trên LLM. Self-LLMDA tự động hóa việc tạo ra và lựa chọn các hướng dẫn tăng cường, do đó nâng cao đáng kể chất lượng và tính ứng dụng của dữ liệu tăng cường qua các tác vụ hạ nguồn đa dạng. Được kiểm tra trên 26 tác vụ học few-shot đa dạng, Self-LLMDA liên tục vượt trội hơn cả các phương pháp Non-LLMDA và Manual-LLMDA, thể hiện hiệu quả và tính ứng dụng của nó.

## 7 Hạn chế

Nghiên cứu này thừa nhận một số ràng buộc xác định phạm vi của công trình hiện tại và phác thảo hướng cho nghiên cứu tương lai:

• **Đánh giá trên Phạm vi Hạn chế của LLM:** Các thí nghiệm của chúng tôi được tiến hành chủ yếu với GPT3.5 Turbo do chi phí cao liên quan đến việc sử dụng các mô hình OpenAI. Trong khi kết quả đầy hứa hẹn trong Bảng 1 gợi ý rằng phương pháp Self-LLMDA đề xuất của chúng tôi có thể có hiệu suất thậm chí tốt hơn trên các mô hình tiên tiến hơn như GPT 4 Turbo, việc kiểm tra toàn diện là không khả thi. Tương tự, nhu cầu tính toán của việc đánh giá các LLM mã nguồn mở như LLAMA-70b-chat, kết hợp với số lượng lớn các tác vụ trong nghiên cứu của chúng tôi, vượt quá tài nguyên của chúng tôi. Mặc dù có những hạn chế này, chúng tôi lạc quan rằng Self-LLMDA sẽ thể hiện hiệu suất nâng cao qua một phổ rộng hơn của LLM.

• **Khám phá Meta-Prompting:** Trong framework Self-LLMDA, chúng tôi sử dụng một meta-prompt để hướng dẫn LLM tạo ra các hướng dẫn tăng cường đa dạng và liên quan. Tuy nhiên, việc khám phá các kỹ thuật meta-prompting của chúng tôi có hạn. Chúng tôi thừa nhận rằng kỹ thuật prompt engineering tinh vi hơn có thể tiếp tục tinh chỉnh chất lượng và hiệu quả của các hướng dẫn được tạo ra. Việc điều tra các chiến lược meta-prompting tiên tiến hơn vẫn là một lĩnh vực để khám phá trong tương lai.

• **Phân tích các Phương pháp Tăng cường Ensemble:** Nghiên cứu của chúng tôi không điều tra các lợi ích tiềm năng của việc kết hợp nhiều tập hợp dữ liệu tăng cường (ví dụ: D ∪ D′₁ ∪ D′₂). Các cách tiếp cận ensemble như vậy giới thiệu các phức tạp bổ sung, như xác định số lượng tối ưu của các hướng dẫn tăng cường cần bao gồm. Trong khi chúng tôi giả định rằng tăng cường ensemble có thể cải thiện hiệu suất mô hình, khía cạnh này nằm ngoài phạm vi của nghiên cứu hiện tại và được đánh dấu cho điều tra tiếp theo.
