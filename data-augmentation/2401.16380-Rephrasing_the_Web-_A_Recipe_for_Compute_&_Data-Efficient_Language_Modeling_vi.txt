# 2401.16380.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/data-augmentation/2401.16380.pdf
# Kích thước tệp: 1057019 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Diễn Đạt Lại Web: Công Thức Cho Mô Hình Hóa Ngôn Ngữ Hiệu Quả Về Tính Toán & Dữ Liệu
Diễn Đạt Lại Web:
Công Thức Cho Mô Hình Hóa Ngôn Ngữ Hiệu Quả Về Tính Toán và Dữ Liệu
Pratyush Maini∗†
Đại học Carnegie Mellon
pratyushmaini@cmu.eduSkyler Seto∗, He Bai, David Grangier, Yizhe Zhang, Navdeep Jaitly
Apple
{sseto,hbai22,grangier,yizhe zhang,njaitly }@apple.com
Tóm tắt
Các mô hình ngôn ngữ lớn được huấn luyện trên những lượng dữ liệu khổng lồ thu thập từ web, thường không có cấu trúc, nhiều nhiễu và được diễn đạt kém. Các định luật quy mô hiện tại cho thấy rằng việc học từ dữ liệu như vậy đòi hỏi một lượng lớn cả tính toán và dữ liệu, tăng theo kích thước của mô hình được huấn luyện. Điều này không khả thi vì chi phí tính toán lớn và thời gian liên quan đến việc tiền huấn luyện, cũng như sự khan hiếm sắp tới của dữ liệu chất lượng cao trên web. Trong công trình này, chúng tôi đề xuất WebRephrase Augmented Pre-training (WRAP) sử dụng một mô hình được điều chỉnh theo hướng dẫn sẵn có để paraphrase các tài liệu trên web theo các phong cách cụ thể như "như Wikipedia" hoặc ở "định dạng câu hỏi-đáp án" để đồng thời tiền huấn luyện LLM trên các bản diễn đạt lại thực và tổng hợp. Đầu tiên, chúng tôi chỉ ra rằng việc sử dụng WRAP trên bộ dữ liệu C4, vốn tự nhiên có nhiều nhiễu, tăng tốc tiền huấn luyện khoảng 3×. Với cùng ngân sách tính toán tiền huấn luyện, nó cải thiện perplexity hơn 10% trung bình trên các tập con khác nhau của Pile, và cải thiện độ chính xác câu hỏi đáp án zero-shot trên 13 nhiệm vụ hơn 2%. Thứ hai, chúng tôi điều tra tác động của phong cách diễn đạt lại đến hiệu suất của mô hình, đưa ra những hiểu biết về cách thành phần dữ liệu huấn luyện có thể tác động đến hiệu suất của LLM trong các thiết lập OOD. Những cải thiện của chúng tôi được quy cho thực tế là dữ liệu tổng hợp được diễn đạt lại có tiện ích cao hơn so với chỉ dữ liệu thực vì nó (i) kết hợp đa dạng phong cách phản ánh chặt chẽ phong cách đánh giá downstream, và (ii) có 'chất lượng' cao hơn so với dữ liệu thu thập từ web.

1 Giới thiệu
Việc tiền huấn luyện mô hình ngôn ngữ lớn (LLM) đã được dân chủ hóa và mở nguồn phần lớn, cho phép các phòng thí nghiệm học thuật khác nhau và các ngành công nghiệp tiền huấn luyện các LLM tùy chỉnh. Tuy nhiên, một yếu tố phân biệt chính giữa các mô hình này là thành phần và kích thước của dữ liệu được sử dụng để huấn luyện chúng. Các chiến lược tuyển chọn dữ liệu được yêu cầu để lọc ra những dữ liệu thu thập từ web không có cấu trúc và/hoặc được diễn đạt kém (Eisenstein, 2013). Trong khi một số chiến lược này đã được công khai (Brown et al., 2020; Wenzek et al., 2020; Penedo et al., 2023), hầu hết các kỹ thuật tuyển chọn dữ liệu tiên tiến không được cộng đồng nghiên cứu biết đến, và chỉ còn lại bằng chứng giai thoại. Nghiên cứu về tuyển chọn dữ liệu đòi hỏi nhiều vòng huấn luyện lại, làm cho việc tài liệu hóa các kỹ thuật dẫn đến cải thiện thực tế trở thành một nỗ lực tốn kém. Mặt khác, các định luật quy mô cho mô hình ngôn ngữ (như định luật quy mô Chinchilla (Hoffmann et al., 2022)) cho thấy rằng với kích thước mô hình tăng, chúng ta cũng nên tăng cả tính toán huấn luyện và kích thước dữ liệu một cách tuyến tính. Điều này không khả thi vì (a) dữ liệu chất lượng cao có hạn (Villalobos et al., 2022), và việc lặp lại ngay cả một số epoch nhỏ (4 hoặc nhiều hơn) dẫn đến lợi ích giảm dần hoặc overfitting (Muennighoff et al., 2023; Touvron et al., 2023; Xue et al., 2023); và (b) tiền huấn luyện trong thời gian dài như vậy là cực kỳ tốn kém.

Trong khi đó, việc sử dụng dữ liệu tổng hợp đã trở nên nổi bật trong mô hình điều chỉnh LLM được tiền huấn luyện qua instruction fine-tuning, RLHF (Ouyang et al., 2022), và instruction backtranslation (Li et al., 2023b). Gần đây, trong bối cảnh tiền huấn luyện, dữ liệu tổng hợp

∗Đóng góp ngang nhau
†Công việc thực hiện trong thời gian thực tập tại Apple

--- TRANG 2 ---
Diễn Đạt Lại Web: Công Thức Cho Mô Hình Hóa Ngôn Ngữ Hiệu Quả Về Tính Toán & Dữ Liệu
(a)
0 100 200 300
Token được thấy (tỷ)404142434445464748Độ chính xác Zeroshot trung bình
~3x Nhanh hơn
WRAP-85B (của chúng tôi)
C4-85B
C4-170B
RW-300B(b)
85 35 18170 150 85 75
Kích thước Pool Dữ liệu (Tỷ Token)051015202530Perplexity trung bình trên PILECông thức:  WRAP  C4
Kích thước mô hình:  350M  1.3B Kích thước mô hình:  350M  1.3B (c)

Hình 1: (a) Công thức WRAP: Chúng tôi prompt một mô hình được điều chỉnh theo hướng dẫn sẵn có để diễn đạt lại các bài báo trên web, và tiền huấn luyện một LLM trên hỗn hợp dữ liệu thực và tổng hợp. (b) Hiệu suất zero-shot của các mô hình GPT 1.3B được huấn luyện trên các kết hợp C4 và biến thể tổng hợp. Mỗi bước tương ứng với một batch 1M mẫu. (c) Perplexity trung bình có trọng số trên 21 miền phụ của Pile cho các kích thước mô hình và lượng dữ liệu tiền huấn luyện khác nhau.

được sử dụng để tạo ra các bộ dữ liệu như Tiny Stories (Eldan & Li, 2023) và dữ liệu tổng hợp chất lượng sách giáo khoa (Gunasekar et al., 2023; Li et al., 2023c). Những bộ dữ liệu này được sử dụng để huấn luyện các mô hình ngôn ngữ nhỏ hơn (như họ mô hình Phi) có hiệu suất tương đương với các mô hình ngôn ngữ lớn hơn trên một số nhiệm vụ nhất định. Tuy nhiên, quy trình tạo dữ liệu của chúng vẫn phần lớn không minh bạch, và cực kỳ tốn kém, đòi hỏi prompt một mô hình GPT-3.5 để tạo ra hàng tỷ token. Ngoài ra, việc tạo dữ liệu như vậy có thể tạo ra "thiên lệch kiến thức" lớn bằng cách cụ thể tạo ra dữ liệu liên quan đến các nhiệm vụ mà chúng ta muốn thể hiện tốt. Trong khi dữ liệu tổng hợp đã cho thấy triển vọng, không rõ liệu điều này có phải do bản chất chất lượng cao hơn của dữ liệu tổng hợp, hay do lựa chọn chủ đề chiến lược (Maini, 2023).

Trong công trình này, chúng tôi đề xuất WebRephrase Augmented Pre-training (WRAP) - nhằm kết nối ba thách thức quan trọng xuất phát từ sự mơ hồ xung quanh tuyển chọn dữ liệu - (i) bạn nên tiền huấn luyện trên dữ liệu gì? (ii) làm thế nào bạn có thể tiền huấn luyện với dữ liệu hạn chế? (iii) làm thế nào bạn có thể tiền huấn luyện một cách hiệu quả về mặt tính toán? Cụ thể, chúng tôi chỉ ra rằng việc diễn đạt lại các tài liệu trên web bằng cách sử dụng một LLM kích thước trung bình sẵn có cho phép các mô hình học hiệu quả hơn nhiều so với việc học từ văn bản thô trên web, và giải thích cho những cải thiện hiệu suất trên các bộ dữ liệu out of distribution không thể được bù đắp bằng dữ liệu web bổ sung. Phương pháp đề xuất của chúng tôi bao gồm việc sử dụng một LLM được tiền huấn luyện sẵn có để diễn đạt lại các tài liệu từ một corpus web thành các phong cách khác nhau. Tổng quan về cách tiếp cận của chúng tôi được thể hiện trong Hình 1a.

Trong công trình của chúng tôi, chúng tôi giải quyết hai thách thức quan trọng gặp phải trong quá trình tuyển chọn dữ liệu tổng hợp trong các công trình của Gunasekar et al. (2023) - chi phí tạo ra và thiên lệch dữ liệu - bằng cách diễn đạt lại các bài báo trên web. (i) WRAP cho phép sử dụng một LLM mã nguồn mở và nhỏ hơn nhiều (1.8B/7B v/s GPT3.5) để diễn đạt lại các tài liệu không có cấu trúc và được diễn đạt kém theo các phong cách khác nhau, vì nó không dựa vào LLM như một ngân hàng kiến thức. (ii) Nhờ bản chất duy trì thông tin của việc diễn đạt lại, chúng tôi có thể tận dụng sự đa dạng tự nhiên của web, thay vì dựa vào LLM cho thông tin có thể dễ bị lỗi thực tế và/hoặc thiên lệch dữ liệu. Công trình của chúng tôi cho thấy rằng chỉ "phong cách" alone có thể dẫn đến những cải thiện lớn trong hiệu suất downstream.

Sử dụng WRAP trên C4, chúng tôi đánh giá hiệu suất mô hình trên 13 nhiệm vụ zero-shot khác nhau, và 21 miền mô hình hóa ngôn ngữ khác nhau của Pile, và thấy rằng tiền huấn luyện LLM với dữ liệu tổng hợp cho phép chúng tôi huấn luyện các mô hình tương đương với dữ liệu ít hơn 5x, hoặc tính toán ít hơn 3x. Thực tế, các mô hình được huấn luyện trên dữ liệu tổng hợp của chúng tôi cũng vượt trội so với các mô hình TinyLLama gần đây được huấn luyện cho 3 nghìn tỷ token (10x dữ liệu và tính toán) trên một số nhiệm vụ Q/A zero-shot. Chúng tôi cũng quan sát thấy giảm perplexity khoảng 50% trên Pile, và lưu ý rằng mô hình 350M tham số của chúng tôi được huấn luyện trên kết hợp dữ liệu thực và tổng hợp được diễn đạt lại chỉ trên 15% của toàn bộ corpus C4, vượt trội so với việc tiền huấn luyện một mô hình 1.3B tham số trên toàn bộ C4. Cuối cùng, chúng tôi tiến hành phân tích về khả năng rò rỉ dữ liệu, các thuộc tính của phong cách dữ liệu tổng hợp, và cách kết hợp dữ liệu tổng hợp để cải thiện tiền huấn luyện LLM dựa trên WRAP.

--- TRANG 3 ---
Diễn Đạt Lại Web: Công Thức Cho Mô Hình Hóa Ngôn Ngữ Hiệu Quả Về Tính Toán & Dữ Liệu

2 Công trình liên quan
Định luật quy mô neural cho mô hình ngôn ngữ Các định luật quy mô neural liên hệ số lượng tham số mô hình tối ưu và lượng dữ liệu huấn luyện cho một lượng tính toán cố định. Hoffmann et al. (2022) đã trình bày các định luật quy mô Chinchilla cho mô hình ngôn ngữ chứng minh rằng có một mối quan hệ tuyến tính giữa kích thước của mô hình và lượng dữ liệu huấn luyện cần thiết. Các phát hiện của họ chỉ ra rằng các mô hình trước đây như Gopher (Rae et al., 2021) bị huấn luyện thiếu nghiêm trọng. Gần đây, các mô hình như Llama (Touvron et al., 2023) được huấn luyện với nhiều dữ liệu hơn. Những định luật quy mô này được rút ra cho mô hình huấn luyện một epoch. Gần đây, Muennighoff et al. (2023) cho thấy rằng tiện ích biên của dữ liệu lặp lại giảm nhanh chóng khi huấn luyện quá 4 epoch, và xây dựng định luật quy mô dưới dữ liệu lặp lại. Đồng thời, Xue et al. (2023) cho thấy rằng việc lặp lại ngay cả một phần nhỏ dữ liệu tiền huấn luyện có thể dẫn đến overfitting và giảm hiệu suất mô hình.

Lựa chọn tập dữ liệu Việc lựa chọn dữ liệu chất lượng cao để tiền huấn luyện LLM vẫn là một lĩnh vực nghiên cứu tích cực, có tác động cao, nhưng chưa được nghiên cứu đầy đủ. Ví dụ, mô hình GPT-2 được tiền huấn luyện trên tất cả các liên kết outbound từ Reddit, một nền tảng mạng xã hội, nhận được ít nhất 3 karma (Brown et al., 2020). Điều này được sử dụng như một chỉ số heuristic rằng tài liệu có thể thú vị, giáo dục, hoặc chỉ đơn giản là hài hước. Các công trình tiếp theo đã sử dụng các heuristic khác như ưu tiên các tài liệu giống wikipedia (Gururangan et al., 2022). Rae et al. (2021) sử dụng nhiều bộ lọc heuristic để loại bỏ tài liệu, như sự vắng mặt của một số stopword nhất định, độ dài tài liệu, phần trăm ký tự chữ cái, độ dài từ trung bình, tỷ lệ ký hiệu trên từ, phần trăm dòng bắt đầu bằng dấu đầu dòng, hoặc kết thúc bằng dấu ba chấm, v.v. Công trình của họ làm nổi bật sự phức tạp của việc lọc dữ liệu văn bản. Một mô hình thay thế để xây dựng các bộ dữ liệu tốt hơn để huấn luyện là chưng cất các bộ dữ liệu chất lượng cao. Xie et al. (2023) đề xuất một phương pháp, DoReMi, để lựa chọn hỗn hợp dữ liệu tốt nhất cho việc tiền huấn luyện mô hình ngôn ngữ bằng cách cân bằng lại dữ liệu từ các miền khác nhau. Đồng thời, Abbas et al. (2023) cho thấy rằng việc loại bỏ trùng lặp dữ liệu tiền huấn luyện có thể cải thiện hiệu quả tiền huấn luyện. Gần đây, một số phương pháp được đề xuất để tự động lọc dữ liệu chất lượng thấp để fine-tuning LLM nhanh hơn (Chen et al., 2023; Solaiman & Dennison, 2021; Zhou et al., 2023). Đồng thời, trong lĩnh vực các mô hình image-language như CLIP (Radford et al., 2021), benchmark Datacomp (Gadre et al., 2023) và các entry gần đây (Maini et al., 2023; Yu et al., 2023) đã phát triển các cách tiếp cận để lọc ra các tập con chất lượng thấp từ các bộ dữ liệu tiền huấn luyện như LAION (Schuhmann et al., 2022), hoặc từ các lần thu thập common crawl.

Tăng cường dữ liệu và dữ liệu tổng hợp Eldan & Li (2023) cho thấy rằng một bộ dữ liệu được tạo tổng hợp dưới dạng những câu chuyện mà trẻ nhỏ có thể hiểu cho phép huấn luyện một mô hình ngôn ngữ nhỏ có thể tạo ra các câu mạch lạc. Gunasekar et al. (2023) cho thấy rằng chỉ dữ liệu chất lượng sách giáo khoa (tổng hợp) giúp các mô hình đạt được hiệu suất tiên tiến trên các nhiệm vụ lý luận và mã hóa. Các cách tiếp cận tương tự được sử dụng trong công trình đồng thời để tăng cường khả năng lý luận mã hóa và toán học trong khi fine-tuning Liu et al. (2023a); Wei et al. (2023). Shumailov et al. (2023) chỉ ra rằng huấn luyện trên dữ liệu tổng hợp thực sự có thể có hại cho hiệu suất mô hình, đặc biệt khi chúng ta thực hiện nhiều vòng tiền huấn luyện LLM và sau đó huấn luyện LLM tiếp theo trên dữ liệu được tạo bởi LLM trước đó. Mặt khác, một số công trình khác đã chỉ ra rằng chiến lược như vậy thực sự có thể hữu ích. Li et al. (2023b) và K¨oksal et al. (2023) thảo luận về cách một mô hình có thể tạo ra dữ liệu hướng dẫn và sau đó fine-tune trên dữ liệu được tạo bởi chính nó để cải thiện hiệu suất. Jung et al. (2023) thảo luận về cách các chu kỳ lặp lại dữ liệu tổng hợp như vậy có thể giúp huấn luyện một mô hình paraphrase và tóm tắt rất nhỏ thậm chí vượt trội so với GPT-3.

Các tài liệu về thị giác và đa phương thức cũng đã chứng kiến sự gia tăng của các công trình xem xét việc sử dụng dữ liệu tổng hợp để huấn luyện. Các công trình của Bansal & Grover (2023); Trabucco et al. (2023); Azizi et al. (2023) đã chỉ ra rằng việc sử dụng dữ liệu tổng hợp kết hợp với dữ liệu thực đạt được hiệu suất mô hình tiên tiến cả trong phân phối và ngoài phân phối. Cubuk et al. (2020) sử dụng các mô hình generative để tạo ra các tăng cường hình ảnh cho khái quát hóa miền tốt hơn. Cũng có nhiều nghiên cứu về việc tăng tính đa dạng của các tăng cường và giá trị của chúng để cải thiện khái quát hóa (Choi et al., 2019; Fort et al., 2021; Hoffer et al., 2020). Tuy nhiên, Alemohammad et al. (2023) chỉ ra rằng các mô hình được tạo ra được huấn luyện cho hơn năm chu kỳ dữ liệu được tạo bởi chính chúng có thể trải qua sự sụp đổ mode nghiêm trọng.

--- TRANG 4 ---
Diễn Đạt Lại Web: Công Thức Cho Mô Hình Hóa Ngôn Ngữ Hiệu Quả Về Tính Toán & Dữ Liệu

3 WRAP: Web Rephrase Augmented Pretraining
Việc tạo dữ liệu tổng hợp bằng cách sử dụng một mô hình ngôn ngữ sẵn có có thể vừa tốn kém về mặt tính toán vừa thách thức về mặt vận hành. Các cách tiếp cận trước đây để tạo dữ liệu chất lượng sách giáo khoa tổng hợp bằng LLM (Gunasekar et al., 2023) đòi hỏi (1) một mô hình ngôn ngữ chứa đủ kiến thức thế giới để tạo ra các bài báo đáng để huấn luyện, do đó tăng chi phí tạo ra; (2) một sự lựa chọn cẩn thận các prompt cho phép tạo ra các bài báo chất lượng cao và đa dạng để lấp đầy bất kỳ khoảng trống kiến thức nào trong corpus tổng hợp. Thách thức này đã được làm nổi bật trong công trình tiếp theo của Li et al. (2023c), và có khả năng vô tình tạo ra thiên lệch trong các mô hình ngôn ngữ (Maini, 2023), trái ngược với những mô hình được huấn luyện trên sự đa dạng tự nhiên của web. Như một biện pháp khắc phục cho thách thức của (i) chi phí tạo ra, và (ii) đa dạng dữ liệu, chúng tôi đề xuất WRAP tận dụng sự đa dạng tự nhiên của các bài báo trên web, cho phép chúng tôi sử dụng các LLM nhỏ hơn đáng kể (so với GPT-3.5) để tạo ra các paraphrase chất lượng cao của các bài báo nhiễu và không có cấu trúc trên web.

3.1 Diễn đạt lại Web
Đã được quan sát trong công trình trước đây rằng việc tăng trọng số dữ liệu chất lượng cao, chẳng hạn như văn bản từ Wikipedia, có thể hữu ích để cải thiện mô hình hóa ngôn ngữ. Những thuật ngữ này nói chung đã được định nghĩa rất lỏng lẻo và chỉ có bằng chứng giai thoại về điều tương tự (Brown et al., 2020; Wenzek et al., 2020). Đồng thời, dữ liệu web thiếu văn bản ở định dạng câu hỏi-đáp hoặc đối thoại, đây là một trường hợp sử dụng nổi bật của các mô hình ngôn ngữ. Dựa trên hai hiểu biết này, chúng tôi thiết kế các phong cách diễn đạt lại cho công trình của mình.

Phong cách diễn đạt lại Thay vì bằng chứng giai thoại ở trên, chúng tôi cố gắng diễn đạt lại các tài liệu trên web theo bốn phong cách khác nhau - (i) Dễ (văn bản mà ngay cả một đứa trẻ mới biết đi cũng hiểu được); (ii) Trung bình (bằng tiếng Anh chất lượng cao như được tìm thấy trên Wikipedia); (iii) Khó (bằng ngôn ngữ ngắn gọn và khó hiểu); (iv) Q/A (ở định dạng câu hỏi-đáp đối thoại). Để vận hành việc diễn đạt lại trong những biến thể phong cách này, chúng tôi prompt một cách thích hợp một mô hình được điều chỉnh theo hướng dẫn. Các ví dụ được diễn đạt lại của bốn phong cách này và các mẫu prompt được sử dụng trong công trình của chúng tôi được cung cấp trong Phụ lục G.

Tạo dữ liệu tổng hợp Bây giờ, chúng tôi chi tiết cách chúng tôi sử dụng một mô hình ngôn ngữ được điều chỉnh theo hướng dẫn để diễn đạt lại văn bản từ các bộ dữ liệu được thu thập từ web như C4 (Raffel et al., 2020) (mà chúng tôi sử dụng cho tất cả các thí nghiệm). Cụ thể, chúng tôi sử dụng một mô hình Mistral-7B instruction-tuned đông lạnh (Jiang et al., 2023) (xem Ablation trong Phần 6 cho các mô hình khác). Để tạo dữ liệu tổng hợp theo phong cách "trung bình", mô hình Mistral được prompt bằng hướng dẫn sau: "Đối với đoạn văn sau đây, hãy cung cấp cho tôi một paraphrase của nó bằng ngôn ngữ tiếng Anh chất lượng cao như trong các câu trên Wikipedia". Prompt được tạo ra bằng cách sử dụng phản hồi con người lặp lại bằng cách so sánh đầu ra của các LLM kích thước 'trung bình' với đầu ra của GPT-4. Chúng tôi sử dụng đầu ra mô hình để tạo ra một corpus song song của dữ liệu tổng hợp "chất lượng cao" tương ứng với dữ liệu web nhiễu gốc. Mỗi ví dụ có tối đa 300 token, được quyết định dựa trên quan sát thực nghiệm của chúng tôi rằng việc yêu cầu một LLM diễn đạt lại hơn 300 token thường dẫn đến mất thông tin. Thảo luận về chất lượng dữ liệu có thể được tìm thấy trong Phần C.

Kết hợp dữ liệu thực và tổng hợp Phương pháp diễn đạt lại dữ liệu web của chúng tôi tự nhiên kết hợp sự đa dạng thông tin được tìm thấy trên internet. Tuy nhiên, nó không kết hợp nhiễu trong dữ liệu thực. Trong khi dữ liệu tổng hợp có thể giúp LLM tiền huấn luyện nhanh hơn, chúng tôi cũng muốn chúng có thể hiểu văn bản web nhiễu có thể chứa đầy lỗi chính tả và lỗi ngôn ngữ để LLM không thất bại trong các tình huống đối mặt với người dùng. Để kết hợp sự đa dạng phong cách này trong mô hình hóa ngôn ngữ, chúng tôi lấy mẫu dữ liệu thực và tổng hợp theo tỷ lệ 1:1.

3.2 Chi tiết triển khai
Kiến trúc Chúng tôi huấn luyện các mô hình transformer chỉ decoder (Vaswani et al., 2017) ở ba quy mô khác nhau: nhỏ, trung bình và XL. Mô hình quy mô nhỏ (128M tham số) bao gồm 12 layer, 12 attention head, và kích thước chiều ẩn là 768. Mô hình quy mô trung bình (350M tham số) bao gồm 24 layer, 16 attention head, và kích thước chiều ẩn là

--- TRANG 5 ---
Diễn Đạt Lại Web: Công Thức Cho Mô Hình Hóa Ngôn Ngữ Hiệu Quả Về Tính Toán & Dữ Liệu

1024. Mô hình quy mô XL (1.3B tham số) bao gồm 24 layer, 16 attention head, và kích thước chiều ẩn là 2048. Chúng tôi không sử dụng dropout trong bất kỳ mô hình nào và độ dài sequence tối đa là 1024. Các mô hình được huấn luyện bằng repository Megatron-LM của NVIDIA.

Tiền huấn luyện Chúng tôi huấn luyện tất cả các mô hình XL của chúng tôi tổng cộng 300k bước với kích thước batch một triệu token, trừ khi được chỉ định khác. Chúng tôi sử dụng tỷ lệ học tối đa là 3e−4 cho các mô hình 128M và 350M tham số, và 2e−4 cho mô hình 1.3B tham số. Tỷ lệ học tối thiểu là 1e−5. Chúng tôi sử dụng weight decay 0.01, cùng với gradient clipping norm 1.0. Chúng tôi sử dụng scheduler tỷ lệ học cosine với warmup cho 1% tổng số bước; và optimizer Adam với β1=0.9 và β2=0.999.

4 Đánh giá Perplexity
Chúng tôi đánh giá perplexity của mô hình được tiền huấn luyện trên tập validation của nhiều bộ dữ liệu out-of-distribution. Tất cả các mô hình đều được huấn luyện trên bộ dữ liệu C4 (Raffel et al., 2020), hoặc một bản diễn đạt lại theo phong cách cụ thể của nó. Tất cả các đánh giá được thực hiện trên 21 miền phụ của Pile (Gao et al., 2020). Những tập con này được tạo ra từ 10,000 tài liệu đầu tiên từ mỗi miền của bộ dữ liệu Pile. Sau đó chúng tôi đánh giá perplexity của mô hình trên những tập con này. Chi tiết đánh giá bổ sung được cung cấp trong Phụ lục D. Điều quan trọng cần lưu ý là chúng tôi đánh giá perplexity trên Pile thay vì C4. Huấn luyện trên nhiều phân phối văn bản (tổng hợp và thực tế) có chi phí là giảm ít hơn 1 perplexity trên tập validation C4. Để hiểu sự lựa chọn đánh giá của chúng tôi, và tại sao chúng tôi quan sát thấy sự gia tăng perplexity này, chúng tôi lưu ý rằng huấn luyện trên corpus C4 tương ứng với việc tối thiểu hóa mục tiêu

θc4=min θEx∼Dc4[L(θ;x)], (1)

cố gắng mô hình hóa chính xác văn bản web C4. Ngược lại, huấn luyện trên nhiều phong cách tương ứng với việc tối thiểu hóa rủi ro trên một phân phối khác,

θWRAP =min θEx∼Dc4∪Dsyn[L(θ;x)]. (2)

Giải cho phương trình 2 không tối thiểu hóa rủi ro chỉ trên C4, và do đó việc so sánh θc4 và θWRAP trên C4 là không công bằng. Để so sánh có ý nghĩa các mô hình được huấn luyện trên C4 và trên các bản diễn đạt lại tổng hợp của nó, chúng tôi đánh giá khả năng khái quát hóa của chúng trên 21 miền khác nhau của Pile (Gao et al., 2020). Kết quả cho mỗi miền được trình bày trong Hình 2.

Độ phức tạp dữ liệu Trong Hình 1c, chúng tôi chỉ ra rằng các mô hình được huấn luyện với ít token hơn (150B) và thậm chí các mô hình 350M nhỏ hơn vượt trội so với việc huấn luyện trên C4 đầy đủ cho 300B token cho thấy việc học nhanh hơn với các bản diễn đạt lại tổng hợp. Trên một số miền như ArXiv và HackerNews, chúng tôi quan sát thấy rằng huấn luyện với dữ liệu tổng hợp cho phép giảm perplexity gần 3x so với perplexity của các mô hình được huấn luyện chỉ trên dữ liệu thực. Điều này cho thấy rằng trong nhiều trường hợp có thể không thể bù đắp lợi thế hiệu suất của việc tiền huấn luyện trên dữ liệu tổng hợp bằng cách chỉ huấn luyện trên nhiều dữ liệu thực hơn. Tổng thể, trung bình trên nhiều tập con của Pile, các mô hình của chúng tôi cải thiện perplexity 50% so với các mô hình được huấn luyện chỉ trên dữ liệu thực.

Tốc độ học Chúng tôi quan sát thấy rằng ngay cả tại checkpoint đầu tiên (10B token) của huấn luyện WRAP, perplexity trung bình của LLM trên Pile thấp hơn so với điều đạt được bằng cách tiền huấn luyện trên C4 cho 15 checkpoint. Điều này cho thấy tăng tốc tiền huấn luyện 15x. Chúng tôi hoãn thảo luận về tốc độ học đến các nhiệm vụ 'zero-shot' để thực hiện so sánh có ý nghĩa hơn.

5 Nhiệm vụ Zero-shot
Bây giờ chúng tôi đánh giá các mô hình ngôn ngữ được tiền huấn luyện của chúng tôi trên các benchmark câu hỏi đáp án (QA) zero-shot khác nhau bằng cách sử dụng LLM Evaluation Harness1 (Gao et al., 2023).

5.1 Bộ dữ liệu
Chúng tôi đánh giá các mô hình của chúng tôi trên tổng cộng 13 benchmark zero-shot khác nhau để đánh giá khả năng của chúng trên các nhiệm vụ ngôn ngữ tự nhiên khác nhau như lý luận thông thường, hiểu biết ngôn ngữ và kiến thức và lý luận toán học.

Hiểu biết chung Danh mục Hiểu biết chung bao gồm các bộ dữ liệu kiểm tra kỹ năng nhận thức rộng hơn và khả năng hiểu ngôn ngữ. ARC Easy (ARC-E) (Clark et al., 2018) là phần ít thách thức hơn của ARC-C, có các câu hỏi đòi hỏi kỹ năng lý luận cơ bản. BoolQ (Clark et al., 2019) bao gồm các câu hỏi boolean tập trung vào khả năng hiểu đọc và hiểu biết ngôn ngữ chung. Winogrande (Wino.) (ai2, 2019) thách thức các mô hình với lý luận thông thường trong ngôn ngữ, đặc biệt là trong việc phân tích đại từ. PIQA (Bisk et al., 2020) đánh giá hiểu biết về các quy trình vật lý, một phần thiết yếu của lý luận thông thường thực tế. HellaSwag (Zellers et al., 2019) kiểm tra khả năng hoàn thành các tình huống một cách mạch lạc, đòi hỏi cả hiểu biết ngôn ngữ và lý luận thông thường. TruthfulQA (Lin et al., 2021) tập trung vào việc tạo ra các câu trả lời chính xác và đúng sự thật, do đó kiểm tra tính chính xác thực tế của mô hình. OpenBookQA (OBQA) (Mihaylov et al., 2018) đánh giá hiểu biết về một loạt các thực tế và khái niệm rộng. Cuối cùng, LogiQA-2 (Liu et al., 2023b) đánh giá khả năng của mô hình để hiểu và áp dụng các nguyên tắc logic.

Kiến thức chuyên môn Trong danh mục Kiến thức chuyên môn, chúng tôi bao gồm các bộ dữ liệu đòi hỏi chuyên môn trong các miền cụ thể. ARC Challenge (ARC-C) (Clark et al., 2018) chứa các câu hỏi thi khoa học thách thức từ lớp 3 đến 9, đòi hỏi kiến thức tiên tiến. SciQ (Johannes Welbl, 2017) cung cấp các câu hỏi thi khoa học để kiểm tra hiểu biết và lý luận của các mô hình trong lĩnh vực khoa học. PubMedQA (Jin et al., 2019) tập trung vào tài liệu y sinh, đánh giá khả năng hiểu trong thông tin y tế và sức khỏe. MathQA (Amini et al., 2019) kiểm tra giải quyết vấn đề toán học, đòi hỏi cả khả năng hiểu số học và lý luận. Cuối cùng, MMLU (Hendrycks et al., 2021) bao trùm nhiều miền, từ chuyên nghiệp đến học thuật, kiểm tra mô hình trên các chủ đề chuyên môn.

5.2 Kết quả
Chúng tôi so sánh hiệu suất của một mô hình được huấn luyện trên hỗn hợp dữ liệu thực và tổng hợp với các mô hình được huấn luyện trên các phần khác nhau của dữ liệu thực. Trong tất cả các thí nghiệm, chúng tôi sử dụng bộ dữ liệu C4 (Raffel et al., 2020) để diễn đạt lại và tạo ra các phần dữ liệu tổng hợp. Chúng tôi sử dụng viết tắt 'Real Tok.' để biểu thị số lượng token dữ liệu web có sẵn

1Chúng tôi sử dụng git commit - 89618bf8 để đảm bảo tính nhất quán trong tất cả các thí nghiệm với kích thước batch là 32.

--- TRANG 6 ---
Diễn Đạt Lại Web: Công Thức Cho Mô Hình Hóa Ngôn Ngữ Hiệu Quả Về Tính Toán & Dữ Liệu

Bộ dữ liệu (Real Tok.) ARC-E BoolQ Wino. PIQA HellaSwag TruthfulQA OBQA LogiQA Trung bình
Nửa C4 (85B) 61.2 59.1 57.3 74.9 46.5 34.1 22.4 23.5 47.4
C4 đầy đủ (170B) 61.6 54.2 59.0 74.9 46.8 33.5 25.0 23.4 47.3
RW (160B) 61.6 60.7 57.5 74.3 45.2 36.8 21.8 23.2 47.6
RW (320B) 60.7 61.1 57.1 74.4 45.6 36.0 22.6 22.5 47.5
Pythia-Pile (300B) 60.5 63.3 57.5 70.8 40.4 38.9 22.2 22.2 47.0
TinyLlama (1T) 60.3 57.8 59.1 73.3 45.0 37.6 21.8 24.5 47.4
Tổng hợp (85B) 63.9 60.0 58.8 76.1 45.2 44.0 23.0 24.1 49.4
Tổng hợp+C4 (85B) 64.1 62.2 58.9 75.4 46.2 40.6 24.1 23.9 49.4

Bảng 1: Đánh giá các LLM ∼1.3B tham số trên 'Nhiệm vụ Hiểu biết chung' trên các bộ dữ liệu tập trung vào lý luận chung, hiểu biết ngôn ngữ và lý luận thông thường. Kết quả cho WRAP được tính trung bình trên 3 lần chạy

Bộ dữ liệu (Real Tok.) ARC-C SciQ PubMedQA MathQA MMLU Trung bình
Nửa C4 (85B) 26.3 84.5 57.2 23.4 24.2 43.1
C4 đầy đủ (170B) 26.8 85.0 57.4 24.3 23.9 43.5
RW (160B) 27.2 87.2 56.2 24.1 25.9 44.1
RW (320B) 27.8 88.0 57.4 23.0 25.4 44.3
Pythia-Pile (300B) 26.1 86.6 60.6 25.2 24.3 44.6
TinyLlama (1T) 27.8 88.9 61.4 24.1 25.8 45.6
Tổng hợp (85B) 29.7 87.0 60.2 23.4 24.6 45.0
Tổng hợp+C4 (85B) 29.9 87.6 61.5 23.9 24.8 45.5

Bảng 2: Đánh giá các LLM ∼1.3B tham số trên 'Nhiệm vụ Kiến thức chuyên môn' đòi hỏi kiến thức miền cụ thể như khoa học, y học, toán học và logic. Kết quả cho WRAP được tính trung bình trên 3 lần chạy.

cho tiền huấn luyện. Trong các thí nghiệm 'Tổng hợp + Thực', chúng tôi bổ sung cùng số lượng bản diễn đạt lại tổng hợp. Chúng tôi chọn 'Real Tokens' làm thước đo so sánh vì chúng tôi có thể diễn đạt lại cùng một tài liệu nhiều lần, nghĩa là tổng kích thước corpus không có ý nghĩa, và 'kiến thức' corpus là đơn vị tiền tệ thực sự quan tâm.

Phương pháp cơ sở Chúng tôi tiền huấn luyện LLM của (i) Nửa C4, và (ii) C4 đầy đủ tương ứng với khoảng 85 tỷ và 170 tỷ token thực (Raffel et al., 2020). Chúng tôi cũng tiền huấn luyện các mô hình riêng của chúng tôi trên (iii) 160 tỷ và (iv) 320 tỷ token của Bộ dữ liệu RefinedWeb (Penedo et al., 2023). Ngoài ra, chúng tôi cũng so sánh với mô hình (iv) Pythia-1.4B đã được huấn luyện trên Pile (Gao et al., 2020). Bộ dữ liệu này không còn có sẵn công khai, do đó chúng tôi sử dụng một mô hình được tiền huấn luyện. Cuối cùng, chúng tôi cũng so sánh với mô hình (v) TinyLlama gần đây (Zhang et al., 2024) được huấn luyện cho 3 epoch trên 1 nghìn tỷ token dữ liệu từ SlimPajama (Shen et al., 2023) và StarCoder (Li et al., 2023a).

Cải thiện chung Trên tất cả các nhiệm vụ trong Bảng 1, chúng tôi quan sát thấy rằng các mô hình được huấn luyện trên dữ liệu tổng hợp kết hợp với bộ dữ liệu C4 (Tổng hợp+C4) thể hiện hiệu suất trung bình tổng thể cao hơn là 49.4% so với những mô hình được huấn luyện chỉ trên bộ dữ liệu C4 thực với phần chia 85B token, đạt điểm trung bình 47.4%. Điều này cho thấy rằng việc bao gồm dữ liệu tổng hợp có thể tăng cường khả năng hiểu biết chung của các mô hình NLP. Hơn nữa, ngay cả mô hình TinyLLama được huấn luyện với 10x tính toán và dữ liệu, cũng thể hiện tương đương với các mô hình khác được huấn luyện trên dữ liệu thực. Điều này cho thấy rằng các cải thiện từ việc lọc hoặc thêm nhiều dữ liệu thực rất thấp. Trái ngược với điều này, WRAP cho thấy rằng tiền huấn luyện ngay cả trên lượng nhỏ dữ liệu tổng hợp có thể đóng góp vào các cải thiện hiệu suất lớn.

Nhiệm vụ Kiến thức chuyên môn Thông điệp chính từ kết quả trong Bảng 2 là dữ liệu tổng hợp không thể truyền đạt 'kiến thức mới'. Nó chỉ có thể giúp tiền huấn luyện nhanh hơn, điều này cũng là tiền đề của công trình chúng tôi. Cụ thể, chúng tôi lưu ý một số phát hiện chính:

1. Tiền huấn luyện trên các bộ dữ liệu lớn hơn giúp cải thiện hiệu suất, có thể bằng cách phơi bày LLM với nhiều "kiến thức" hơn. Ví dụ, mô hình Pythia (300B) đạt được điểm trung bình 44.6%, vượt trội so với điểm 43.5% của bộ dữ liệu C4 (85B) nhỏ hơn.

--- TRANG 7 ---
Diễn Đạt Lại Web: Công Thức Cho Mô Hình Hóa Ngôn Ngữ Hiệu Quả Về Tính Toán & Dữ Liệu

2. Mặc dù có lợi thế của một bộ dữ liệu lớn hơn, các cải thiện bão hòa. Ví dụ, mô hình RefinedWeb (320B) chỉ vượt trội so với mô hình RefinedWeb (160B) 0.2%. Tương tự, mô hình TinyLlama (1T token) thể hiện tương đương với mô hình WRAP, mà chỉ có 85B token dữ liệu web thô.

Cải thiện cụ thể Chúng tôi thấy cải thiện tối đa trong bộ dữ liệu TruthfulQA, với mô hình Tổng hợp (85B) đạt 44.0%, cao hơn đáng kể so với hiệu suất của bất kỳ mô hình nào khác trên bộ dữ liệu này. Điều này có thể là do các LLM được điều chỉnh theo hướng dẫn đã sửa các quan niệm sai lầm tiềm ẩn trong khi diễn đạt lại văn bản. Thú vị, chúng tôi nhận thấy rằng việc thêm dữ liệu thực vào mô hình tổng hợp (Tổng hợp+C4) làm giảm hiệu suất trên TruthfulQA 4%, xuống 40.5%, cho thấy sự pha loãng tiềm ẩn các lợi ích thu được từ dữ liệu tổng hợp khi kết hợp với dữ liệu thực. Các bộ dữ liệu khác như HellaSwag và BoolQ, mà các mô hình được huấn luyện trên C4 thể hiện tốt, tiếp tục cho thấy lợi ích của việc kết hợp C4 và các bản diễn đạt lại tổng hợp.

6 Phân tích và Ablation
Chúng tôi tiếp tục đặt các Câu hỏi Nghiên cứu (RQ) sau để điều tra với độ chi tiết tốt hơn cách tăng cường hiệu suất tối ưu.

6.1 Phân tích Kết hợp Dữ liệu
RQ1: Tầm quan trọng của việc có dữ liệu C4 thực như thế nào? Các phát hiện của chúng tôi trong Bảng 1-2 cho thấy rằng dữ liệu tổng hợp sử dụng prompt QA đủ để có hiệu suất mạnh trên các nhiệm vụ QA

--- TRANG 8 ---
Diễn Đạt Lại Web: Công Thức Cho Mô Hình Hóa Ngôn Ngữ Hiệu Quả Về Tính Toán & Dữ Liệu

Bộ dữ liệu (Real Tok.) ARC-C SciQ PubMedQA MathQA MMLU Trung bình
Med+C4-35B 27.2 82.2 46.2 23.1 25.2 40.8
QA+C4-35B 29.0 85.1 62.2 22.5 26.1 45.0
Med-35B 27.0 80.0 59.4 22.5 24.7 42.7
QA-35B 27.1 85.5 59.2 22.2 25.0 43.8

Bảng 4: Tầm quan trọng của Dữ liệu Thực: Đánh giá các LLM ∼1.3B tham số trên Nhiệm vụ Kiến thức chuyên môn. Kết quả cho thấy rằng việc thêm dữ liệu thực giúp cải thiện hiệu suất mô hình khi tiền huấn luyện trên paraphrase 'Q/A-style'.

nhiệm vụ. Tuy nhiên, khi được đánh giá trên perplexity Pile, chúng tôi quan sát thấy sự xuống cấp đáng kể về perplexity trên nhiều miền phụ trong Hình 3. Điều này có thể là do dữ liệu tổng hợp rất sạch chứa ít ký tự đặc biệt và có cấu trúc cao. Ngược lại, một số miền phụ của Pile như OWT và Hackernews có những token đặc biệt như vậy. Trên các miền như Philpapers và Gutenberg, chúng tôi quan sát thấy rằng việc loại bỏ văn bản C4 thực khỏi dữ liệu tiền huấn luyện, và huấn luyện chỉ trên các tài liệu tổng hợp làm giảm hiệu suất đáng kể (tăng perplexity). Điều này một lần nữa được quy cho thực tế là dữ liệu tổng hợp không chứa một số 'tag' và 'phong cách' phổ biến trong các lần thu thập dữ liệu thực, và nhấn mạnh cách WRAP là một chiến lược tốt hơn so với tiền huấn luyện chỉ trên dữ liệu tổng hợp. Về hiệu suất trên các nhiệm vụ zero-shot, chúng tôi một lần nữa lưu ý rằng sự có mặt của dữ liệu thực giúp cải thiện hiệu suất zero-shot trong Bảng 3,4. Vì các nhiệm vụ zero-shot chứa các cặp Q/A được viết tốt, hiệu ứng này không rõ ràng như đối với perplexity trên dữ liệu thực.

RQ2: Việc kết hợp nhiều bộ dữ liệu tổng hợp có cải thiện hiệu suất không? Chúng tôi đo lường tác động của việc kết hợp nhiều phong cách tổng hợp với C4 để huấn luyện. Chúng tôi xem xét hai biến thể: kết hợp theo tỷ lệ 1:1 có nghĩa là có hai bản sao của C4 để khớp với hai phong cách tổng hợp (trung bình và QA), và tỷ lệ 1:2 kết hợp chỉ một instance của bộ dữ liệu C4. Đối với các nhiệm vụ QA zero-shot, phát hiện của chúng tôi trong Bảng 5-6 cho thấy hiệu suất thấp hơn so với việc chỉ kết hợp dữ liệu QA và C4. Các đánh giá trên Pile được thể hiện trong Hình 4. Chúng tôi nhận thấy rằng cả paraphrase 'Q/A' và 'Wikipedia' đều giúp cải thiện hiệu suất trên một số miền nhất định. Ví dụ, 'Stackexchange', có nhiều câu hỏi-đáp án hưởng lợi từ sự có mặt của dữ liệu tổng hợp theo phong cách Q/A. Tổng thể, chúng tôi lưu ý rằng có một cải thiện nhỏ về perplexity trung bình trên Pile bằng cách kết hợp nhiều phong cách.

6.2 Ablation Phương pháp
RQ3: Tầm quan trọng của việc có một re-phraser chất lượng cao như thế nào? Để trả lời điều này, chúng tôi sử dụng dữ liệu từ bốn mô hình re-phrasing khác nhau (T5-base (Raffel et al., 2020), Qwen-1.8B-chat (Bai et al., 2023a), Mistral-7B-chat (Jiang et al., 2023), và Vicuna-13B-chat-v1.3 (Chiang et al., 2023)) và huấn luyện một mô hình 345M cho 30B token. Chúng tôi tạo dữ liệu từ tất cả các mô hình bằng cách sử dụng cùng một prompt. Trong trường hợp mô hình T5-base, chúng tôi fine-tune mô hình cho 1 epoch trên các cặp re-phrase từ mô hình Vicuna-13b-chat. Chúng tôi thấy rằng tiền huấn luyện trên dữ liệu được tạo bởi các mô hình re-phrase nhỏ hơn như Qwen-1.8B và Mistral-7B đạt được perplexity thấp hơn so với Vicuna 13B (Hình 5). Đồng thời, mô hình T5-base được fine-tune của chúng tôi thể hiện kém hơn đáng kể so với phần còn lại. Ngay cả như vậy, tất cả các mô hình rephrase đều giảm perplexity so với chỉ dữ liệu C4 thực. Vẫn còn là một câu hỏi mở để kiểm tra giới hạn của việc chúng ta có thể huấn luyện một mô hình paraphrase nhỏ đến mức nào để có thể tạo ra dữ liệu tổng hợp chất lượng cao để mở rộng hơn nữa khả năng áp dụng của WRAP.

--- TRANG 9 ---
Diễn Đạt Lại Web: Công Thức Cho Mô Hình Hóa Ngôn Ngữ Hiệu Quả Về Tính Toán & Dữ Liệu

Bộ dữ liệu (Real Tok.) ARC-E BoolQ Wino. PIQA HellaSwag TruthfulQA OBQA LogiQA Trung bình
Med+C4-35B 59.8 57.0 55.7 74.6 44.5 36.5 23.8 21.5 46.7
QA+C4-35B 62.2 63.3 55.7 74.8 44.6 41.4 22.4 23.2 48.4
Combined-1:1-35B 60.6 60.2 57.7 73.8 43.7 40.2 22.0 22.1 47.5
Combined-1:2-35B 61.4 62.0 57.0 74.8 44.6 39.5 23.0 21.3 48.0

Bảng 6: Kết hợp nhiều phong cách: Đánh giá các LLM ∼1.3B tham số được huấn luyện cho 150B token trên Nhiệm vụ Hiểu biết chung. Kết quả cho thấy rằng việc kết hợp các phong cách rephrasing không mang lại lợi ích hiệu suất trên các nhiệm vụ zero-shot so với chỉ phong cách Q/A.

RQ4: Dữ liệu tổng hợp có cải thiện so với augmentation không? Các cải thiện được quan sát bằng cách tiền huấn luyện trên dữ liệu tổng hợp có giống như tiền huấn luyện với augmentation không? Để kiểm tra điều này, chúng tôi xem xét hai baseline augmentation văn bản phổ biến - thay thế từ đồng nghĩa và xóa ngẫu nhiên bằng thư viện NL-Augmenter (Dhole et al., 2021). Chúng tôi tiền huấn luyện một mô hình 350M tham số cho 15B token để tiến hành nhóm thí nghiệm này. Tổng kích thước pool chỉ khoảng 1.5B token, có nghĩa là mô hình sẽ phải lặp lại dữ liệu khoảng 10 lần trong giai đoạn tiền huấn luyện, trừ khi được augment. Như thấy trong phân tích perplexity trong Hình 6, các mô hình được huấn luyện trên dữ liệu được augment thể hiện kém hơn đáng kể so với những mô hình được huấn luyện trên kết hợp dữ liệu thực và tổng hợp. Điều này cho thấy rằng dữ liệu tổng hợp tăng cường quá trình học, và không chỉ đơn giản là một dạng augmentation khác.

RQ5: Phong cách của dữ liệu tổng hợp tác động đến hiệu suất trên các miền chuyên môn như thế nào? Chúng tôi so sánh hiệu suất của các mô hình khác nhau được huấn luyện trên các phong cách dữ liệu tổng hợp khác nhau. Cụ thể, chúng tôi tạo ra bốn phong cách dữ liệu tổng hợp (dễ, trung bình, khó, và Q/A) và đánh giá hiệu suất của việc huấn luyện trên kết hợp mỗi phong cách trên các tập con Pile. Các prompt để tạo ra những phong cách dữ liệu tổng hợp này được nêu trong Phụ lục G. Kết quả tương ứng với các thế hệ từ mô hình Vicuna-v1.3, và cho một mô hình 128M được huấn luyện cho 3B token được tóm tắt trong Hình 7. Chúng tôi thấy rằng huấn luyện với kết hợp dữ liệu C4 thực và dữ liệu tổng hợp khớp với phong cách của miền tại đánh giá cải thiện hiệu suất. Tuy nhiên, chúng tôi thấy rằng không có phong cách dữ liệu tổng hợp đơn lẻ nào thể hiện tốt nhất trên tất cả các miền, dẫn đến hiệu suất tương tự trên việc huấn luyện với kết hợp dữ liệu C4 thực và mỗi biến thể phong cách tổng hợp. Trong khi việc biết phong cách tổng hợp tốt nhất để tiền huấn luyện một LLM là không thực tế, một oracle lựa chọn phong cách tổng hợp tốt nhất trên tất cả các miền sẽ cải thiện perplexity 16% - cho thấy tầm quan trọng của việc huấn luyện với các phong cách dữ liệu đa dạng để khái quát hóa LLM, ngay cả khi kiến thức cơ bản vẫn giữ nguyên.

--- TRANG 10 ---
Diễn Đạt Lại Web: Công Thức Cho Mô Hình Hóa Ngôn Ngữ Hiệu Quả Về Tính Toán & Dữ Liệu

RQ6: Có rò rỉ dữ liệu từ mô hình rephrase đến mô hình được huấn luyện không? Chúng tôi điều tra xem dữ liệu tổng hợp của chúng tôi có duy trì ý nghĩa semantic tương tự trong khi khác biệt về mặt phong cách so với dữ liệu C4 gốc và khớp với phong cách của các miền PILE khác nhau không. Chúng tôi bắt đầu bằng cách so sánh các cặp ví dụ của dữ liệu tổng hợp và thực để xác nhận rằng cải thiện hiệu suất không được quy cho rò rỉ kiến thức từ các mô hình rephrase. Chúng tôi lấy một tập con của 1000 mẫu đầu tiên từ mỗi bộ dữ liệu.

Chúng tôi thể hiện độ tương tự cosine của các embedding câu từ một mô hình BERT được tiền huấn luyện với objective SimCSE (Gao et al., 2021) cho các prompt medium và qa trong Hình 8(a) và (b). Khi tính toán độ tương tự, chúng tôi loại bỏ các outlier. Các hình có phân phối sử dụng Gaussian Kernel Density Estimator (KDE) để xây dựng phân phối cho thống kê từ

--- TRANG 11 ---
Diễn Đạt Lại Web: Công Thức Cho Mô Hình Hóa Ngôn Ngữ Hiệu Quả Về Tính Toán & Dữ Liệu

1000 giá trị. Độ tương tự cosine của các cặp thực-tổng hợp cao hơn so với một số baseline bao gồm hai mẫu thực ngẫu nhiên từ C4, một baseline tiếp tục tính toán cosine giữa nửa đầu của một mẫu và mẫu đầy đủ, và độ tương tự cosine giữa nửa đầu và nửa sau của cùng một mẫu. Độ tương tự cao cho thấy rằng các re-phrase duy trì ý nghĩa tương tự với các bản đối tác thực của chúng mà không thêm thông tin.

7 Hạn chế và Cơ hội

7.1 Phân tích Chi phí
Bạn nên tạo dữ liệu tổng hợp, hay chỉ huấn luyện lâu hơn trên dữ liệu thực?

Các ứng dụng của WRAP nằm trong cả hai mô hình - (i) các thiết lập dữ liệu tài nguyên thấp như một mô hình ngôn ngữ cho tiếng Phần Lan (Luukkonen et al., 2023), và (ii) các thiết lập giàu dữ liệu như huấn luyện trên common crawl. Trong trường hợp trước, không có tùy chọn thay thế nào để thu thập nhiều dữ liệu hơn một cách ngây thơ, và do đó, dữ liệu tổng hợp là một giải pháp tự nhiên nên vượt trội so với việc huấn luyện chỉ trên dữ liệu trong miền. Tuy nhiên, có một sự quan tâm đáng kể trong việc huấn luyện các mô hình ngôn ngữ bằng tiếng Anh, hoặc rộng hơn, dữ liệu web chung. Việc sử dụng dữ liệu tổng hợp có phải là một tùy chọn khả thi ngay cả trong mô hình này không?

Trước khi chúng tôi đi sâu vào tính khả thi của việc tiền huấn luyện trên dữ liệu tổng hợp, chúng tôi nên thừa nhận kết quả của Bảng 1. Mô hình TinyLlama được huấn luyện cho 3 nghìn tỷ token cũng kém hiệu suất so với một mô hình được huấn luyện cùng lúc trên dữ liệu thực và tổng hợp. Thực tế, nó thể hiện khá tương đương với các mô hình được huấn luyện cho 300B token chỉ trên dữ liệu thực. Điều này cho thấy rằng trần cải thiện bằng cách huấn luyện lâu hơn có thể không cao như vậy (đối với mô hình kích thước 350M/1.3B tham số; các mô hình lớn hơn có thể hưởng lợi từ việc huấn luyện lâu hơn).

Để phân tích sự đánh đổi chi phí này, chúng tôi so sánh chi phí tạo dữ liệu tổng hợp, so với chi phí huấn luyện một mô hình ngôn ngữ trên dữ liệu bổ sung. Đối với các thí nghiệm tạo dữ liệu tổng hợp, chúng tôi sử dụng thư viện vLLM (Kwon et al., 2023) để tạo nhanh. Cụ thể, chúng tôi có thể tạo ra 3M token mỗi giờ trên một A100 đơn khi sử dụng Mistral-7B. Tạo 85B token (như trong công trình của chúng tôi) chiếm khoảng 25K giờ GPU.

Để so sánh, trên 64 A100, chúng tôi đạt được throughput 0.5M token mỗi giây. Giả sử huấn luyện cho 300B token, có nghĩa là 256 ngày GPU, chiếm khoảng 6k giờ GPU để huấn luyện một mô hình đơn. Ngược lại, huấn luyện một mô hình 13B sẽ mất khoảng 30K giờ GPU. Ở quy mô huấn luyện một mô hình 13B, việc giảm chi phí huấn luyện 3-10x có thể kết hợp chi phí phụ trội của việc huấn luyện với dữ liệu tổng hợp trong một lần chạy duy nhất.

Trong khi chi phí tạo dữ liệu chất lượng cao vẫn tương đối cao, hai nguồn cải thiện quan trọng tác động đến phân tích chi phí này. Đầu tiên, nếu chúng tôi sử dụng mô hình Qwen-1.8B Bai et al. (2023b) để rephrasing, chúng tôi có thể có được throughput token cao hơn 3x. Như thấy trong kết quả sơ bộ của chúng tôi trong Hình 5, mô hình được tiền huấn luyện trên các rephrase được tạo bởi mô hình Qwen thể hiện tương đương với mô hình Mistral. Điều này giảm chi phí tạo ra 3x. Công trình gần đây trong speculative decoding (Liu et al., 2023c) và inference được tối ưu hóa (Xia et al., 2024) cho thấy rằng chúng ta có thể tận dụng cải thiện 3-5x khác trong chi phí tạo ra. Do đó, thực sự, ngay cả ở quy mô huấn luyện mô hình 1.3B tham số, chúng ta đã có thể cải thiện chi phí tiền huấn luyện chỉ sử dụng dữ liệu thực.

Hai lợi thế bổ sung quan trọng của việc tạo dữ liệu tổng hợp không thể được tính đến trong thảo luận ở trên:

1. Chi phí tạo dữ liệu tổng hợp là một khoản đầu tư một lần, và chúng ta có thể huấn luyện nhiều mô hình với các quy mô khác nhau một khi dữ liệu được tạo ra.

2. Tạo dữ liệu có thể song song hóa 100%, trong khi huấn luyện đòi hỏi sự sẵn có của một cluster lớn với kết nối inter-node nhanh. Điều này tốn kém hơn nhiều. Mặt khác, việc tạo ra có thể được coi như một quy trình phụ có thể lấp đầy các GPU trống trong bất kỳ cluster tính toán quy mô lớn nào, và chạy trên các máy GPU đơn.

7.2 Đa dạng của Các Thế hệ Tổng hợp
Một hạn chế khác là việc thực thi đa dạng trong dữ liệu được tạo ra. Sự đa dạng này đến từ cả "phong cách" và "kiến thức" chứa trong dữ liệu được tạo ra. Các công trình gần đây (Li et al., 2023b;c) sử dụng một lựa chọn các chủ đề, hoặc tình huống để seed mô hình tạo ra các văn bản mới. Tuy nhiên, một nghiên cứu gần đây của Padmakumar et al. (2023) cho thấy rằng việc sử dụng các mô hình ngôn ngữ để viết có hỗ trợ AI có xu hướng giảm đa dạng nội dung, đặc biệt khi sử dụng các mô hình được điều chỉnh theo hướng dẫn. Trong khi chúng tôi sử dụng mô hình rephrasing cụ thể để giảm thiểu các vấn đề liên quan đến đa dạng tạo nội dung mới, việc đánh giá sự có mặt (hoặc thiếu) và tác động của đa dạng nội dung trong các mô hình paraphrase vẫn còn cho công trình tương lai.

8 Kết luận
Các mô hình ngôn ngữ mạnh được tiền huấn luyện trên kết hợp dữ liệu thực và tổng hợp. Việc sử dụng dữ liệu tổng hợp cho phép nướng trực tiếp các thuộc tính mong muốn như công bằng, thiên lệch, và phong cách (như tuân theo hướng dẫn) vào dữ liệu, loại bỏ nhu cầu điều chỉnh thuật toán huấn luyện cụ thể. Điều này cung cấp một cách tiếp cận thay thế để điều chỉnh các mô hình ngôn ngữ theo giá trị con người. Sự gia tăng gần đây về mối quan tâm xung quanh dữ liệu tổng hợp, đặc biệt cho instruction-tuning các mô hình ngôn ngữ, đáng chú ý, với các nhà nghiên cứu đồng thời cũng tận dụng nó để tiền huấn luyện. Khi chúng ta chuyển sang mô hình này, việc hiểu các thuộc tính của dữ liệu được cung cấp cho các mô hình của chúng ta là tối quan trọng. Bài báo này nhằm mục đích là một hướng dẫn toàn diện về việc sử dụng dữ liệu phong cách tổng hợp khác nhau trong tiền huấn luyện LLM. Chúng tôi đi sâu vào tầm quan trọng của nó từ hai góc nhìn: (1) Trong các tình huống với dữ liệu chất lượng cao khan hiếm, các rephrase tổng hợp cung cấp nhiều giá trị hơn so với việc chỉ lặp lại dữ liệu hiện có; (2) Dữ liệu tổng hợp có thể là một lợi ích cho việc khái quát hóa trên các miền văn bản khác nhau, và để tạo ra văn bản theo các phong cách bị thiểu đại diện trong bộ dữ liệu tiền huấn luyện. Khi các practitioner tạo dữ liệu tổng hợp để huấn luyện mô hình, họ sẽ đối mặt với các lựa chọn thiết kế quan trọng và tốn kém - (i) Chất lượng của bộ tạo dữ liệu tổng hợp quan trọng như thế nào?; (ii) Làm thế nào để cân bằng dữ liệu thực và tổng hợp? (iii) Khi nào việc huấn luyện trên dữ liệu tổng hợp đạt đến điểm lợi ích giảm dần về mặt epoch? Công trình này thực hiện bước đầu tiên hướng tới việc trả lời những câu hỏi này.

Ngược lại, điều cần thiết là lưu ý các hạn chế cố có, và cơ hội với dữ liệu tổng hợp. Chúng tôi làm nổi bật hai hạn chế: (1) chi phí tạo ra vẫn lớn và đòi hỏi các LM mạnh, và (2) việc thực thi đa dạng trong dữ liệu được tạo ra là thách thức. Trong công trình này, chúng tôi tận dụng sự đa dạng tự nhiên của web để tạo ra "re-phrase" tổng hợp. Điều này giới hạn mô hình khỏi việc học "kiến thức" mới và chỉ tăng cường quá trình học thông qua việc cung cấp đầu vào chất lượng cao. Trong khi công trình trước đây đòi hỏi hiểu biết phức tạp hơn về các điểm mù của mô hình, có thể tạo thiên lệch kiến thức chứa trong phân phối dữ liệu tiền huấn luyện. Tuy nhiên, chúng tôi chứng minh tiềm năng của dữ liệu tổng hợp để cải thiện hiệu quả huấn luyện LLM cả về tính toán và kích thước dữ liệu.

Tài liệu tham khảo
Winogrande: An adversarial winograd schema challenge at scale. 2019.
Amro Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S. Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. ArXiv , abs/2303.09540, 2023. URL https://api.semanticscholar.org/CorpusID:257557221 .
Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, and Richard G Baraniuk. Self-consuming generative models go mad. arXiv preprint arXiv:2307.01850 , 2023.
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pp. 2357–2367, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1245. URL https://aclanthology.org/N19-1245 .
Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and David J Fleet. Synthetic data from diffusion models improves imagenet classification. arXiv preprint arXiv:2304.08466 , 2023.
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609 , 2023a.
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609 , 2023b.
Hritik Bansal and Aditya Grover. Leaving reality to imagination: Robust classification via generated datasets. arXiv preprint arXiv:2302.02503 , 2023.

--- TRANG 14 ---
Diễn Đạt Lại Web: Công Thức Cho Mô Hình Hóa Ngôn Ngữ Hiệu Quả Về Tính Toán & Dữ Liệu

Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence , 2020.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems , volume 33, pp. 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper files/paper/2020/ file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .
Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. Alpagasus: Training a better alpaca with fewer data. arXiv preprint arXiv:2307.08701 , 2023.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P . Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/ .
Dami Choi, Alexandre Passos, Christopher J Shallue, and George E Dahl. Faster neural network training with data echoing. arXiv preprint arXiv:1907.05550 , 2019.
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pp. 2924–2936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL https://aclanthology.org/N19-1300 .
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1 , 2018.
Together Computer. Redpajama: an open dataset for training large language models. 2023. URL https://github.com/togethercomputer/RedPajama-Data .
Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops , pp. 702–703, 2020.
Kaustubh D. Dhole, Varun Gangal, Sebastian Gehrmann, Aadesh Gupta, Zhenhao Li, Saad Mahamood, Abinaya Mahendiran, Simon Mille, Ashish Srivastava, Samson Tan, Tongshuang Wu, Jascha Sohl-Dickstein, Jinho D. Choi, Eduard Hovy, Ondrej Dusek, Sebastian Ruder, Sajant Anand, Nagender Aneja, Rabin Banjade, Lisa Barthe, Hanna Behnke, Ian Berlot-Attwell, Connor Boyle, Caroline Brun, Marco Antonio Sobrevilla Cabezudo, Samuel Cahyawijaya, Emile Chapuis, Wanxiang Che, Mukund Choudhary, Christian Clauss, Pierre Colombo, Filip Cornell, Gautier Dagan, Mayukh Das, Tanay Dixit, Thomas Dopierre, Paul-Alexis Dray, Suchitra Dubey, Tatiana Ekeinhor, Marco Di Giovanni, Rishabh Gupta, Rishabh Gupta, Louanes Hamla, Sang Han, Fabrice Harel-Canada, Antoine Honore, Ishan Jindal, Przemyslaw K. Joniak, Denis Kleyko, Venelin Kovatchev, Kalpesh Krishna, Ashutosh Kumar, Stefan Langer, Seungjae Ryan Lee, Corey James Levinson, Hualou Liang, Kaizhao Liang, Zhexiong Liu, Andrey Lukyanenko, Vukosi Marivate, Gerard de Melo, Simon Meoni, Maxime Meyer, Afnan Mir, Nafise Sadat Moosavi, Niklas Muennighoff, Timothy Sum Hon Mun, Kenton Murray, Marcin Namysl, Maria Obedkova, Priti Oli, Nivranshu Pasricha, Jan Pfister, Richard Plant, Vinay Prabhu, Vasile Pais, Libo Qin, Shahab Raji, Pawan Kumar Rajpoot, Vikas Raunak, Roy Rinberg, Nicolas Roberts, Juan Diego Rodriguez, Claude Roux, Vasconcellos P . H. S., Ananya B. Sai, Robin M. Schmidt, Thomas Scialom, Tshephisho Sefara, Saqib N. Shamsi, Xudong Shen, Haoyue Shi, Yiwen Shi, Anna Shvets, Nick Siegel, Damien Sileo, Jamie Simon, Chandan Singh, Roman Sitelew, Priyank Soni, Taylor Sorensen, William Soto, Aman Srivastava, KV Aditya Srivatsa, Tony Sun, Mukund Varma T, A Tabassum, Fiona Anting Tan, Ryan Teehan, Mo Tiwari, Marie Tolkiehn, Athena Wang, Zijian Wang, Gloria Wang, Zijie J. Wang, Fuxuan Wei, Bryan Wilie, Genta Indra Winata, Xinyi Wu, Witold Wydma ´nski, Tianbao Xie, Usama Yaseen, M. Yee, Jing Zhang, and Yue Zhang. Nl-augmenter: A framework for task-sensitive natural language augmentation, 2021.
Jacob Eisenstein. What to do about bad language on the internet. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pp. 359–369, Atlanta, Georgia, June 2013. Association for Computational Linguistics. URL https://aclanthology.org/N13-1037 .
Ronen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speak coherent english? arXiv preprint arXiv:2305.07759 , 2023.
Stanislav Fort, Andrew Brock, Razvan Pascanu, Soham De, and Samuel L Smith. Drawing multiple augmentation samples per image during training efficiently decreases test error. arXiv preprint arXiv:2105.13343 , 2021.
Richard Futrell, Kyle Mahowald, and Edward Gibson. Large-scale evidence of dependency length minimization in 37 languages. Proceedings of the National Academy of Sciences , 112 (33):10336–10341, 2015.
Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. arXiv preprint arXiv:2304.14108 , 2023.
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/10256836 .
Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings. In 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021 , pp. 6894–6910. Association for Computational Linguistics (ACL), 2021.
Edward Gibson et al. The dependency locality theory: A distance-based theory of linguistic complexity. Image, language, brain , 2000:95–126, 2000.
Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C ´esar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644 , 2023.
Suchin Gururangan, Dallas Card, Sarah Dreier, Emily Gade, Leroy Wang, Zeyu Wang, Luke Zettlemoyer, and Noah A. Smith. Whose language counts as high quality? measuring language ideologies in text data selection. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pp. 2562–2580, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. emnlp-main.165. URL https://aclanthology.org/2022.emnlp-main.165 .
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR) , 2021.

--- TRANG 15 ---
Diễn Đạt Lại Web: Công Thức Cho Mô Hình Hóa Ngôn Ngữ Hiệu Quả Về Tính Toán & Dữ Liệu

Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel Soudry. Augment your batch: Improving generalization through instance repetition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 8129–8138, 2020.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825 , 2023.
Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. Pubmedqa: A dataset for biomedical research question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pp. 2567–2577, 2019.
Matt Gardner Johannes Welbl, Nelson F. Liu. Crowdsourcing multiple choice science questions. 2017.
Jaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian Fisher, Taylor Sorensen, and Yejin Choi. Impossible distillation: from low-quality model to high-quality dataset & model for summarization and paraphrasing. arXiv preprint arXiv:2305.16635 , 2023.
Abdullatif K ¨oksal, Timo Schick, Anna Korhonen, and Hinrich Sch ¨utze. Longform: Optimizing instruction tuning for long text generation with corpus extraction. arXiv preprint arXiv:2304.08460 , 2023.
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles , 2023.
Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo ˜ao Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu ˜noz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the source be with you! 2023a.
Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. Self-alignment with instruction backtranslation. arXiv preprint arXiv:2308.06259 , 2023b.
Yuanzhi Li, S ´ebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463 , 2023c.
Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods, 2021.
Bingbin Liu, Sebastien Bubeck, Ronen Eldan, Janardhan Kulkarni, Yuanzhi Li, Anh Nguyen, Rachel Ward, and Yi Zhang. Tinygsm: achieving¿ 80% on gsm8k with small language models. arXiv preprint arXiv:2312.09241 , 2023a.
Hanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, Ming Zhou, and Yue Zhang. Logiqa 2.0 — an improved dataset for logical reasoning in natural language understanding. IEEE/ACM Transactions on Audio, Speech, and Language Processing , pp. 1–16, 2023b. doi: 10.1109/TASLP .2023.3293046.
Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, and Hao Zhang. Online speculative decoding. arXiv preprint arXiv:2310.07177 , 2023c.
Risto Luukkonen, Ville Komulainen, Jouni Luoma, Anni Eskelinen, Jenna Kanerva, Hanna-Mari Kupari, Filip Ginter, Veronika Laippala, Niklas Muennighoff, Aleksandra Piktus, et al. Fingpt: Large generative models for a small language. arXiv preprint arXiv:2311.05640 , 2023.
Pratyush Maini. Phi-1.5 model: A case of comparing apples to oranges? 2023. URL https://pratyushmaini.github.io/phi-1 5/.
Pratyush Maini, Sachin Goyal, Zachary C Lipton, J Zico Kolter, and Aditi Raghunathan. T-mars: Improving visual representations by circumventing text feature learning. arXiv preprint arXiv:2307.03132 , 2023.
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP , 2018.
Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. arXiv preprint arXiv:2305.16264 , 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35:27730–27744, 2022.
Masanori Oya. Three types of average dependency distances of sentences in a multilingual parallel corpus. In Proceedings of the 35th Pacific Asia Conference on Language, Information and Computation , pp. 652–661, 2021.
Vishakh Padmakumar, Behnam Hedayatnia, Di Jin, Patrick Lange, Seokhwan Kim, Nanyun Peng, Yang Liu, and Dilek Hakkani-Tur. Investigating the representation of open domain dialogue context for transformer models. In Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue , pp. 538–547, Prague, Czechia, September 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023. sigdial-1.50 .
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116 , 2023.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning , pp. 8748–8763. PMLR, 2021.
Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446 , 2021.

--- TRANG 16 ---
Diễn Đạt Lại Web: Công Thức Cho Mô Hình Hóa Ngôn Ngữ Hiệu Quả Về Tính Toán & Dữ Liệu

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research , 21(1):5485–5551, 2020.
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems , 35:25278–25294, 2022.
Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Joel Hestness, Natalia Vassilieva, Daria Soboleva, and Eric Xing. Slimpajama-dc: Understanding data combinations for llm training. arXiv preprint arXiv:2309.10818 , 2023.
Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. Model dementia: Generated data makes models forget. arXiv preprint arXiv:2305.17493 , 2023.
Irene Solaiman and Christy Dennison. Process for adapting language models to society (palms) with values-targeted datasets. Advances in Neural Information Processing Systems , 34:5861–5873, 2021.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.
Brandon Trabucco, Kyle Doherty, Max Gurinas, and Ruslan Salakhutdinov. Effective data augmentation with diffusion models. arXiv preprint arXiv:2302.07944 , 2023.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 30, 2017.
Pablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. Will we run out of data? an analysis of the limits of scaling datasets in machine learning. arXiv preprint arXiv:2211.04325 , 2022.
Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Source code is all you need. arXiv preprint arXiv:2312.02120 , 2023.
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm ´an, Armand Joulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of the Twelfth Language Resources and Evaluation Conference , pp. 4003–4012, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https://aclanthology.org/2020. lrec-1.494 .
Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding, 2024.
Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. arXiv preprint arXiv:2305.10429 , 2023.
Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You. To repeat or not to repeat: Insights from scaling llm under token-crisis. arXiv preprint arXiv:2305.13230 , 2023.
Haichao Yu, Yu Tian, Sateesh Kumar, Linjie Yang, and Heng Wang. The devil is in the details: A deep dive into the rabbit hole of data filtering. arXiv preprint arXiv:2309.15954 , 2023.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , 2019.
Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model, 2024.
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206 , 2023.

[Tài liệu tiếp tục với nhiều trang nữa chứa các phần phụ lục chi tiết về dữ liệu, phương pháp, và kết quả thí nghiệm bổ sung]
