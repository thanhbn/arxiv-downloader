# 2309.11104.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/data-augmentation/2309.11104.pdf
# File size: 882981 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
AttentionMix : Data augmentation method that relies on BERT
attention mechanism
Dominik Lewy, Jacek Ma´ ndziuk
Faculty of Mathematics and Information Science,
Warsaw University of Technology,
dominik.lewy@gmail.com, jacek.mandziuk@pw.edu.pl
Abstract
TheMixup method has proven to be a powerful data augmentation technique in Computer Vision,
with many successors that perform image mixing in a guided manner. One of the interesting research
directions is transferring the underlying Mixup idea to other domains, e.g. Natural Language Process-
ing (NLP). Even though there already exist several methods that apply Mixup to textual data, there
is still room for new, improved approaches. In this work, we introduce AttentionMix , a novel mixing
method that relies on attention-based information. While the paper focuses on the BERT attention
mechanism, the proposed approach can be applied to generally any attention-based model. Attention-
Mix is evaluated on 3 standard sentiment classification datasets and in all three cases outperforms
two benchmark approaches that utilize Mixup mechanism, as well as the vanilla BERT method. The
results confirm that the attention-based information can be effectively used for data augmentation in
the NLP domain.
1 Introduction
In recent years the introduction of transformer ar-
chitectures [18] dominated many academic tasks
(e.g. [22]) and commercial applications in the Natu-
ral Language Processing (NLP) area, with Bidirec-
tional Encoder Representations from Transformers
(BERT) [3] architecture having a special position
in this genre.
In parallel, an interesting group of data augmen-
tation methods that rely on mixing images was in-
troduced and studied in-depth in the Computer Vi-
sion (CV) domain. The canonical method in this
group is Mixup [25]. The method mixes images and
their corresponding one-hot-encoded labels linearly
during the training process to create synthetic ob-
servations. This method has many successors that
adjusted it to specific settings [6, 26], addressed
particular vulnerabilities [19] or made the mixing
process more effective by means of a certain guid-
ance [6,17,21].
The concept of mixing training samples is also
applicable to the NLP domain for sentence classi-
fication [5, 7]. However, in this area it is not yetwell studied, with only several methods that try to
apply the vanilla Mixup approach.
In this work, we propose an extension of the
Mixup method for sentence classification that uses
the information on attention [18] coming from
BERT model to improve the sentence mixing pro-
cedure. We show that the proposed approach out-
performs the base BERT method and two methods
that apply vanilla Mixup . Additionally, we perform
an ablation study to explain why certain attention
layers produce more meaningful information, from
the point of view of the training process, that re-
sults in higher accuracy for a given task.
1.1 Motivation
Data augmentation techniques differ significantly
between modalities, e.g. CV and NLP. In CV,
there is a multitude of relatively simple data aug-
mentation mechanisms like rotation, cropping, flip-
ping, scaling, etc. The vast majority of them
are modality-specific and are not applicable to
text. In NLP, the number of relatively simple aug-
mentation mechanisms is limited to just a few,
1arXiv:2309.11104v1  [cs.CL]  20 Sep 2023

--- PAGE 2 ---
such as synonym replacement or random inser-
tion/deletion/swap [23].
The main motivation of this work is to ex-
tend the repertoire of relatively simple augmenta-
tion methods in the NLP domain by enabling the
use of mixing augmentation methods that proven
very effective in CV. To this end, we enhance the
Mixup method by adding text-specific attention-
based mechanism.
1.2 Contribution
The main contribution of this work is a proposal of
a new augmentation method for sentence classifica-
tion ( AttentionMix ) based on the Mixup method,
which utilizes attention weights to guide the mixing
process. Even though the paper focuses on atten-
tion utilization in BERT [3], the same approach
can be applied to any other architecture that im-
plements attention mechanism [18]. AttentionMix
is empirically tested on 3 sentence classification
datasets. Additionally, the information stored in
the best performing attention layers is analyzed to
shed light on the reasons of their strong perfor-
mance.
While in the literature there have already been
proposed several guided augmentation approaches
transferred from the CV domain to the NLP do-
main (summarized in Section 2.1), this paper is the
first to propose a guided text augmentation method
rooted in the NLP-specific mechanism, i.e. atten-
tion.
2 Related work
2.1 Mixup like data augmentation
A canonical method that started the research on
image mixing as a form of data augmentation is
Mixup [25], that linearly interpolates two images
and its corresponding labels. There are other meth-
ods that perform mixing in a guided manner, i.e.
by means of identifying the most relevant parts of
the image, e.g. [21] which uses information com-
ing from a CNN classifier, [17] that applies statis-
tical approach, or [6] which, in the mixing process,
utilizes the neural network gradients. A review of
mixing based data augmentation techniques for im-
age classification is presented in the recent surveypaper [9].
The above idea, that originated in the CV do-
main, proven also useful in text classification. [5]
utilizes the Mixup idea to perform augmentation of
word embeddings and sentence embedding in the
training process of CNN and LSTM networks. Sub-
sequent works [7,16] extend this research by consid-
ering BERT architecture and experimenting with
Manifold Mixup [19] – a variation of Mixup , which
applies mixing to hidden layers of the network. An-
other example is [24] which utilizes gradient based
saliency information to mix the original sentence
on a word level. DropMix [8], also utilizes gradient
based saliency information, but additionally com-
bines mixing with dropout mechanism to obtain a
mixed sample.
All the above methods verify whether the mecha-
nisms that have already been successful in CV can
be effectively applied to NLP, albeit with certain
domain-specific adjustments (e.g. changing a sen-
tence into an embedding that can be mixed [5, 7]
or summing gradient based saliency information at
the word level [24]).
Contrary to the above approaches that rely
on improvements rooted in the CV domain, our
method is the first to use the guidance stemming
from the text-specific mechanism, i.e. attention.
2.2 Methods utilizing attention
An interesting example of this type of method
is [10] which utilizes attention from BERT as a
means to determine important words in pregnancy-
related questions for annotation purposes. The
approach does not differentiate attention lay-
ers/heads, and for a given word simply aggregates
attention from all heads from all layers, and deems
words with higher attention sum as more impor-
tant. A very similar approach is proposed in [20]
to identify keywords indicative of hate speech. An
interesting finding from this paper is that for dif-
ferent datasets (Boomer-hate vs. Asian-hate) the
attention mechanism behaves differently (in the for-
mer, it associates hate words with a target group,
and in the latter, it does not), which suggest that
the applicability of attention may differ between
datasets.
There are also quite many papers that analyse
the properties of the attention weights. For in-
stance, [2] tries to understand attentions in the con-
2

--- PAGE 3 ---
text of their relative position or density, as well as
from the perspective of whether particular atten-
tion layers/heads learned any language structures
(e.g. co-referent mention that attends to their an-
tecedents). [4], on the other hand, approaches the
analysis of attention by means of an unsupervised
clustering of attention heads. They find 4 groups
of attention patterns and identify which of them
have the greatest impact on accuracy. According
to their research, association of a particular head
to a group can change in the course of training and
can also differ depending on the analysed dataset.
3 Background
3.1 BERT attention
BERT architecture [3] consists of multiple attention
layers, each of them containing multiple attention
heads. An attention head takes as input a sequence
of embeddings t= [t1, ..., t n] corresponding to n
tokens of the input sentence. Those embeddings
(ti) are transformed into query, key, and value vec-
tors ( qi,ki,vi) using Q,KandVmatrices learned
during the training process for each attention head
separately. Each head computes attention weight
αbetween all pairs of tokens according to Eq. 1.
αij=exp(qT
ikj)Pn
l=1exp(qT
ikl)(1)
The above stands for a softmax-normalized dot
product between the query and key vectors.
3.2 Mixup augmentation
Mixup [25] is a simple, yet effective, data-agnostic
augmentation mechanism utilized in CV, that con-
structs synthetic training saples as a linear interpo-
lation of the input images. Synthetic samples are
constructed using Eqs. 2 and 3.
˜x=λxi+ (1−λ)xj (2)
˜y=λyi+ (1−λ)yj (3)
where xi, xjare two random samples from the
training data, yi, yjare their one-hot encoded la-
bels, and λis the mixing ratio.4 Proposed approach
Mixing in BERT can be applied on various levels
of the network:
•At the word embedding level – Figure 1 (left)
– the same word regardless of the context will
have the same embedding vector.
•At the word encoding level – Figure 1 (middle)
– the same word will have different embedding
depending on the context.
•At the sentence embedding level – Figure 1
(right) – embedding takes place after vectors of
individual words are aggregated to a sentence
level.
TheAttentionMix method that we propose follows
the first implementation concept.
4.1 AttentionMix
AttentionMix aims to utilize the information com-
ing from attention heads (Eq. 1) to guide the mix-
ing process. Since attention information is relevant
and applicable only on the word embedding and
word encoding levels, the sentence embedding level
will not be explored in this work (all token em-
beddings are already aggregated to sentence level
embedding, at which stage utilization of attention
assigned to individual tokens is impossible).
Furthermore, we focus on the augmentation at
the word embedding level since the working hy-
pothesis is that utilizing attention closer to the in-
put and prior to the encoding stage (i.e. learning
the context of each token and adjusting its embed-
ding based on that) will lead to higher model’s ac-
curacy.
We explore various methods for utilizing atten-
tion information. Let’s consider Lattention layers
with Hheads each. Then, for each head h∈H
in layer l∈Land each sentence Sthe attention
weight matrix AWhl(S) has the form:
AWhl(S) = [αij]n×n (4)
where nis the number of tokens in a sentence. αij
represents the impact of token αijon the next-layer
representation of the current token.
Based on AWhl(S), we calculate the relevance of
each token in the sentence from the perspective of
3

--- PAGE 4 ---
Figure 1: AttentionMix at the word-embedding level proposed in this paper (left subfigure) and two
Mixup schemes: at the word-encoding level (middle) and at the sentence-encoding level (right).
a single head (Eq. 5) and the mean from all heads
in a single layer (Eq. 6).
Bhead hl=P
iαij
n(5)
Blayer l=PH
h=1Bhead hl
H(6)
The above relevance of each token in a sentence
is calculated for each observation (sentence) in the
training dataset. For two pairs of (sentence, label):
(x1, y1) and ( x2, y2) the equations for creating a
mixed sentence are as follows:
λvector =B1
B1+B2(7)
˜x=λvector⊙x1+ (1−λvector )⊙x2 (8)
λlabel=Pλvector
|λvector|(9)
˜y=λlabel·y1+ (1−λlabel)·y2 (10)
where B1andB2are the relevance vectors, cal-
culated using either Eq. 5 or Eq. 6, for observa-
tions ( x1, y1) and ( x2, y2), respectively. λvector is
the mixing ratio vector used for token embedding
mixing, λlabelis the mixing ratio used to mix one-
hot-encoded labels, and |λvector|is the number of
token relevance values. λvector represents the im-
portance of each individual token in a sentence andλlabelis a single value (the mean of all λvector ele-
ments) that is defines the relative degree to which
each of the two one-hot-encoded vectors of labels
contributes to the calculation of ˜ y(Eq. 10).
Eqs. 5-10 formally describe the AttentionMix al-
gorithm that, to our knowledge, presents the first
attempt to utilize attention coming from BERT to
guide the Mixup augmentation process.
5 Experimental setup
5.1 Datasets
We evaluate AttentionMix on 3 sentence classifica-
tion benchmark datasets, summarized in Table 1.
•SST – is the Stanford Sentiment Treebank
dataset [15] with fine-grained 5-level sentiment
scale. Note that in the literature, a simplified
binary version of this data set is also consid-
ered. In the experiments, we chose the orig-
inal non-binary setting with a 5-point senti-
ment scale.
•MR – is a Movie Review dataset with binary
sentiment [14].
•IMDB - Internet Movie Database is a much
larger binary sentiment classification dataset
for movie reviews [11].
4

--- PAGE 5 ---
Algorithm 1 AttentionMix
1:forepoch = 1,2, . . . , max epoch do
2: forbatch = 1,2, . . . , max batch do
3: Extract token embeddings and attention weight matrices (4) from BERT model
4: Calculate relevance of each token according to either Eq. 5 or Eq. 6
5: Create a shuffled copy of the above artefacts (embedding, relevance vector)
6: forobservation = 1,2, . . . , max observation do
7: Calculate the mixing ratio vector between observation and its respective counterpart in a
shuffled copy according to Eq. 7
8: Calculate the contribution of each sentence from a pair according to Eq. 9
9: Create mixed sample based on Eq. 8 and Eq. 10
10: end for
11: Train model on batch containing mixed samples
12: end for
13:end for
Table 1: Train, validation, and test sizes of the
benchmark datasets.
Dataset Train Validation Test
SST 8544 1101 2210
MR 8530 1066 1066
IMDB 25000 10% of Train 25000
5.2 Architecture and Experimental
setup
All experiments were conducted using BERT net-
work [3] with 12 attention layers with 12 heads
each. The hidden size of the transformer block was
equal to 768. This version is commonly referred to
asbert-base .
For each method and each dataset, the experi-
ment was conducted 3 times. Each run consisted in
fine-tuning the BERT model initialized with bert-
base-uncased weights (the model pre-trained on En-
glish language with no distinction between small
and capital letters) for a given dataset and method,
for 100 epochs with a learning rate of 1 e−6and
dropout of 0 .1. All experiments were conducted on
Nvidia A100 GPU 40GB. In the reminder of the
paper, the word training will refer to the above-
described fine-tuning of the BERT model to a par-
ticular classification task.
In each experiment that utilizes Mixup -based
augmentation (i.e. all experiments except stan-
dard BERT training), the augmentation is appliedduring the training process to all training samples.
Technically, AttentionMix is implemented similarly
toMixup , i.e. each batch of data is mixed with a
shuffled version of the same batch, so the size of the
training data does not change.
Each dataset was split into 3 parts: train, val
and test according to Table 1. The results on the
test part are presented, for the model that achieved
the highest accuracy on the val part. The average
outcome of 3 runs is reported.
5.3 Benchmark methods and their
hyperparameters
We compare AttentionMix with three baselines: (1)
standard BERT training without Mixup , referred
to as vanilla approach, (2) adaptation of word-
Mixup [5], and (3) a special case of TMix [1], which
we refer to as MixupEncoding . The main difference
compared to AttentionMix is that both reference
Mixup -like augmentation methods do not use the
guidance coming from the attention mechanism.
Additionally, [5] uses LSTM or CNN architecture
instead of BERT, and the embeddings are utilized
at the word level, not the token level. MixupEn-
coding compared to TMix [1] performs mixing af-
ter the BERT entire encoder, not at a randomly
chosen hidden layer.
Apart from hyperparameters mentioned in Sec-
tion 5.2, the two Mixup -based benchmark methods
use one additional hyperparameter, i.e. a mixing
ratio, sampled in the same way as in Mixup [25].
5

--- PAGE 6 ---
6 Experimental results and
analysis
6.1 Stanford Sentiment Treebank
dataset
The results for the SST dataset are presented in Ta-
ble 2. The vanilla BERT method reached 51 .17%
and was inferior to both other benchmarks that uti-
lizeMixup in the training process ( wordMixup and
MixupEncoding ).
In all AttentionMix experiments presented in Ta-
ble 2, higher mean accuracy than the vanilla ap-
proach was achieved, and in all but one of them
theAttentionMix results exceeded all 3 benchmark
approaches.
More detailed results are depicted in Figure 2.
The left subfigure presents the average accuracy
when all heads in a given layer are utilized, and
the middle and right subfigures are deep dives into
the average accuracy when single heads in the top
performing layers (0 and 10, respectively) are con-
sidered.
The results presented in Figure 2 show that the
use of other than top-3 attention layers in the aug-
mentation process clearly deteriorates the accuracy
on the test set. Furthermore, when looking at
the middle and right subfigures, the highest result
among individual heads is achieved by a head from
layer 10, the learning in layer 0 is more uniform and
there are more “strong” heads in this layer. In layer
0 the use of any individual head results in higher
accuracy than the standard BERT training and for
6 out of 12 heads it excels all competitive methods.
On the contrary, a closer look at layer 10 (Fig-
ure 2 (right)) shows that there are only 2 heads (0
and 3) with the results higher than all benchmarks
and just 3 (0, 3 and 11) with the accuracy higher
than the standard BERT training.
6.2 Movie Review dataset
Table 3 presents the results for the MR dataset.
The vanilla benchmark reached 85 .14%. Clearly,
there is a substantial difference in accuracy between
MR and SST, due to the fact that MR is a binary
— hence potentially much less complex – dataset.
Interestingly, both methods that utilize Mixup in
the training process ( wordMixup andMixupEncod-Table 2: SST dataset. Comparison of the average
results of 3 benchmark methods and AttentionMix .
ForAttentionMix , two attention levels are consid-
ered: the layer level – (all heads from a layer) and
thehead level – (a single head within all layers).
In each case, the results of the 3 best performing
configurations are presented. The details are de-
picted in Figure 2. All experiments were repeated
3 times.
Approach Attention Accuracy
layer head mean std
standard training — — 51.17 0.97
wordMixup — — 51.60 0.18
MixupEncoding — — 51.30 1.13
AttentionMix 10 all 52.05 0.86
AttentionMix 0 all 51.78 0.23
AttentionMix 2 all 51.45 0.21
AttentionMix 10 3 52.76 0.58
AttentionMix 0 8 52.62 0.21
AttentionMix 0 0 52.37 0.26
ing) achieved lower accuracy, which is in line with
the results reported in [7].
In 5 out of 6 experiments, AttentionMix achieved
higher mean accuracy than the vanilla training.
The experiments utilizing attention from particu-
lar layers achieved the highest results. Similarly
to SST, detailed results are available in Figure 3.
The left subfigure presents the average accuracy re-
sulting from utilizing all heads in a given layer and
middle and right subfigures are head related out-
comes, i.e. the average accuracy when single heads
in the top performing layers (layer 1 and layer 6,
respectively) are used.
In the left subfigure, only 2 attention layers
achieved the test accuracy higher than the best
benchmark approach, which in this case is stan-
dard BERT training. When looking at middle and
right subfigures a trend similar to SST dataset can
be observed, i.e. generally more heads that outper-
form the best benchmark approach can be found in
earlier layers than in the later ones. In the mid-
dle subfigure, which refers to layer 1, there are 7
heads with test accuracy exceeding all benchmark
methods, while for layer 6 (right subfigure) there
are only 4 such heads.
6

--- PAGE 7 ---
Figure 2: SST dataset. The average test accuracy for experiments utilizing mean attention from: all
heads in a layer (Eq. 6) – left subfigure , single heads (Eq. 5) in layer 0 – middle subfigure , single heads
(Eq. 5) in layer 10 – right subfigure . The red line indicates the vanilla BERT accuracy, and the green
one, the best accuracy achieved by the Mixup benchmark methods. All experiments were repeated 3
times.
Figure 3: MR dataset. The average test accuracy for experiments utilizing mean attention from: all
heads in a layer (Eq. 6) – left subfigure , single heads (Eq. 5) in layer 1 – middle subfigure , single heads
(Eq. 5) in layer 6 – right subfigure . The red line indicates the vanilla BERT accuracy, and the green one,
the best accuracy achieved by the Mixup benchmark methods. All experiments were repeated 3 times.
Figure 4: IMDB dataset. The average test accuracy for experiments utilizing mean attention from: all
heads in a layer (Eq. 6) – left subfigure , single heads (Eq. 5) in layer 0 – middle subfigure , single heads
(Eq. 5) in layer 1 – right subfigure . The red line indicates the vanilla BERT accuracy, and the green one,
the best accuracy achieved by the Mixup benchmark methods. All experiments were repeated 3 times.
6.3 Internet Movie Database dataset
The results for the IMDB dataset are shown in Ta-
ble 4. The vanilla BERT approach achieved 93 .54%and was weaker than both Mixup -related bench-
marks. IMDB and MR datasets, both refer to
binary sentiment classification, though IMDB has
3 times more training samples, which is the most
7

--- PAGE 8 ---
Table 3: MR dataset. Comparison of the average
results of 3 benchmark methods and AttentionMix .
ForAttentionMix , two attention levels are consid-
ered: the layer level – (all heads from a layer) and
thehead level – (a single head within all layers).
In each case, the results of the 3 best performing
configurations are presented. The details are de-
picted in Figure 3. All experiments were repeated
3 times.
Approach Attention Accuracy
layer head mean std
standard training — — 85.14 0.61
wordMixup — — 84.90 0.86
MixupEncoding — — 85.08 0.41
AttentionMix 1 all 85.27 0.47
AttentionMix 6 all 85.18 0.43
AttentionMix 2 all 85.05 0.24
AttentionMix 1 8 85.68 0.53
AttentionMix 1 3 85.65 0.28
AttentionMix 6 11 85.58 0.11
probable reason of higher accuracy on this dataset
in all experiments, compared to MR.
AttentionMix achieved higher mean accuracy
than the vanilla training in 5 out of 6 experiments.
Additionally, all AttentionMix experiments utiliz-
ing individual heads (the last 3 rows of Table 4)
produced the results exceeding all 3 benchmark ap-
proaches. Head-related results are illustrated in
Figure 4. The left subfigure presents the average
accuracy when all heads in a given layer are used,
and middle and right subfigures show the average
accuracy of single heads in the best performing lay-
ers (0 and 1, respectively).
It can be seen in Figure 4 that the use of other
than top-2 attention layers in the augmentation
process deteriorates the accuracy on the test set be-
low the accuracy of the benchmark methods. The
highest result among individual heads is achieved
by the head from layer 0 (middle subfigure). Simi-
larly to SST, learning in this layer is more uniform,
with high number of effective heads. The use of
any individual head from layer 0 results in higher
accuracy than BERT and half of the heads excel all
3 competitive approaches.
On the contrary, in Figure 4 (right), that refers
to layer 1, only one head (number 11) is superior
to all benchmark approaches and half of the heads
exceed the BERT benchmark.In summary, there are two main conclusions that
can be drawn from the presented experiments on
the three sentence classification datasets. Firstly,
if a decision about AttentionMix settings needs to
be made without the possibility of checking dif-
ferent hyperparameter configurations (e.g. due to
limited computational resources), it is advised to
use the vector of relevance that is extracted from
a single head since such vectors yielded higher per-
formance in our experiments. Secondly, this sin-
gle head should rather be selected among heads
belonging to the initial layers. It stems from the
experiments, that in these earlier layers there are
more heads that outperform benchmark results, al-
beit with a word of warning that with such a head
selection strategy the overall best performing head
can potentially be omitted (e.g. head 3 from layer
10 in SST experiments).
Table 4: IMDB dataset. Comparison of the mean
results of 3 benchmark methods and AttentionMix .
ForAttentionMix , two attention levels are consid-
ered: the layer level – (all heads from a layer) and
thehead level – (a single head amongst all layers).
In each case, the results of the 3 best performing
configurations are presented. The details are de-
picted in Figure 4. All experiments were repeated
3 times.
Approach Attention Accuracy
layer head mean std
standard training — — 93.54 0.21
wordMixup — — 93.88 0.14
MixupEncoding — — 93.72 0.16
AttentionMix 1 all 93.80 0.06
AttentionMix 0 all 93.73 0.29
AttentionMix 2 all 93.52 0.09
AttentionMix 0 4 94.12 0.03
AttentionMix 0 6 94.08 0.18
AttentionMix 0 11 94.06 0.11
6.4 Computational complexity
AttentionMix training requires 2 additional oper-
ations compared to wordMixup : partial forward
pass (the sentence is passed through encoder to
get attentions) and calculation of λvector (non-
computationally intensive mean). The training
time overhead is around 26%. The inference time
is exactly the same for both approaches.
8

--- PAGE 9 ---
6.5 Ablation study
We further investigated why certain information
coming from attention weight matrices results in
higher accuracy boost. Since the three datasets
consist of various types of sentiment analysis, we
hypothesized that certain parts of speech may have
higher impact on sentiment classification than oth-
ers. Specifically, our assumption was that adjec-
tives, adverbs, and verbs could possibly be more
indicative for the sentiment class prediction, since
the sentiment is usually reflected by the statements
like:
•love, like, hate – verbs
•fantastic, disappointing – adjectives
•quite, very, extremely – adverbs
For the SST dataset, this hypothesis was con-
firmed only for some relevance vectors derived from
attention information. Figure 5 shows the mean at-
tention given to a certain part of speech by atten-
tion head 8 in layer 0 and attention head 3 in layer
10, whose usage resulted in the two best performing
models. For head 8 in layer 0 indeed high atten-
tion is given to adjectives, adverbs, and verbs, but
for attention head 3 in layer 10 very high attention
is given to punctuation. This phenomenon of high
punctuation-related attention has been previously
observed in [2].
For the MR dataset, the relative mean attention
assigned to each part of speech for the two top-
performing heads is presented in Figure 6. Surpris-
ingly, for both heads the attention points to seem-
ingly unintuitive direction, at least for humans.
The highest attention is attributed to PUNCT and
CCONJ which for this dataset cover mostly cita-
tion symbols, various types of brackets, a comma,
a dot, a question mark, an exclamation mark, and
3 common words: and,butandor.
For the IMDB dataset, the two best performing
heads belong to layer 0 (heads 4 and 6). The rela-
tive mean attention assigned to each part of speech
for these two heads is depicted in Figure 7. Sim-
ilarly to the SST results for layer 0 head 8, the
original hypothesis stands for both heads. Among
other parts of speech, head 4 attends to adjectives
and adverbs, and head 6 to adjectives and verbs.
In summary, across all 3 datasets, in 3 out of
6 top performing heads the attention is given to
Figure 5: SST dataset. The mean attention value
assigned to each part of speech for a given atten-
tion head, relative to the mean attention value of
the head after training on SST. The abbreviations
stand for: ADJ – adjective, ADP – adposition,
ADV – adverb, AUX – auxiliary, CONJ - conjunc-
tion, CCONJ - coordinating conjunction, DET –
determiner, INTJ – interjection, NOUN – noun,
NUM - numeral, PART – particle, PRON - pro-
noun, PROPN - proper noun, PUNCT - punctua-
tion, SCONJ - subordinating conjunction, SYM -
symbola and VERB - verb.
adjectives, adverbs, and verbs. In the remaining 3
cases, the attention choices are less intuitive.
In essence, based on the results presented in Ta-
bles 2, 3 and 4 we conclude that using information
from the attention mechanism is helpful in creating
augmented samples. At the same time, a detailed
9

--- PAGE 10 ---
Figure 6: MR dataset. The mean attention value
assigned to each part of speech for a given atten-
tion head, relative to the mean attention value of
the head after training on MR. For the list of ab-
breviations, please refer to the caption of Figure 5.
explanation of the reasons for particular attention
focus in certain cases requires further studies.
The existence of unintuitive attention heads has
been already observed in the literature [4,13]. The
first paper, besides other topics, studies the so-
called vertical attention heads that attend mostly
to dots, comas and BERT special tokens. The
other work refers to the above as delimiter heads,
and quantifies their high prevalence at the level of
73.43%, stating that they naturally coexist with
other functions of the head.
The ablation experiments presented in the pa-
per concentrated on sentence x1, based on which
Figure 7: IMDB dataset. The mean attention value
assigned to each part of speech for a given atten-
tion head, relative to the mean attention value of
the head after training on IMDB. For the list of ab-
breviations, please refer to the caption of Figure 5.
the mixing ratio vector ( λvector in Eq. 7) is cal-
culated. Another possible path, though harder to
implement, is analysis of the attention focus based
on sentence x2. We plan to investigate this path in
our future work.
7 Concluding remarks
Inspired by the success of Mixup augmentation in
the CV domain, we introduce a Mixup -related aug-
mentation method in the context of text classifi-
cation. Unlike previous Mixup -based approaches
devoted to the NLP domain, we propose to use
10

--- PAGE 11 ---
a guided mixing approach, and towards this end
utilize BERT attention information as the source
of guidance for the augmentation process. We
show empirically that the proposed method, Atten-
tionMix , outperforms the vanilla BERT approach
and two Mixup -based benchmark methods used for
comparison. The results support the effectiveness
of the use of guided attention-based mixing in the
NLP domain.
In future work, we plan to search for an auto-
mated method of selecting the most relevant atten-
tion information for a given dataset. This goal is
associated with the well-known difficulty in inter-
preting the information coming from the attention
heads [2, 12]. A phenomenon that is far from fully
understood.
Another research direction is evaluation of the
AttentionMix efficacy when applied on the word
encoding level – as opposed to the current word
embedding level implementation.
References
[1] Chen, J., Yang, Z., Yang, D.: Mixtext:
Linguistically-informed interpolation of
hidden space for semi-supervised text clas-
sification. In: Proceedings of the 58th
Annual Meeting of the Association for
Computational Linguistics, ACL 2020,
Online, July 5-10, 2020. pp. 2147–2157
(2020). https://doi.org/10.18653/v1/2020.acl-
main.194
[2] Clark, K., Khandelwal, U., Levy, O., Man-
ning, C.D.: What does BERT look at? an
analysis of bert’s attention. In: Proceed-
ings of the 2019 ACL Workshop Black-
boxNLP: Analyzing and Interpreting Neural
Networks for NLP, BlackboxNLP@ACL
2019, Florence, Italy. pp. 276–286 (2019).
https://doi.org/10.18653/v1/W19-4828,
https://doi.org/10.18653/v1/W19-4828
[3] Devlin, J., Chang, M., Lee, K., Toutanova,
K.: BERT: pre-training of deep bidirectional
transformers for language understanding.
In: Proceedings of the 2019 Conference of
the North American Chapter of the Associa-
tion for Computational Linguistics: Human
Language Technologies, NAACL-HLT 2019,Minneapolis, MN, USA. pp. 4171–4186 (2019).
https://doi.org/10.18653/v1/n19-1423,
https://doi.org/10.18653/v1/n19-1423
[4] Guan, Y., Leng, J., Li, C., Chen, Q.,
Guo, M.: How far does BERT look at:
Distance-based clustering and analysis of
bert’s attention. In: Proceedings of the
28th International Conference on Computa-
tional Linguistics, COLING 2020, Barcelona,
Spain (Online). pp. 3853–3860 (2020).
https://doi.org/10.18653/v1/2020.coling-
main.342, https://doi.org/10.18653/v1/
2020.coling-main.342
[5] Guo, H., Mao, Y., Zhang, R.: Augmenting
data with mixup for sentence classification:
An empirical study. CoRR abs/1905.08941
(2019), http://arxiv.org/abs/1905.08941
[6] Huang, S., Wang, X., Tao, D.: Snapmix: Se-
mantically proportional mixing for augment-
ing fine-grained data. CoRR abs/2012.04846
(2020)
[7] Jindal, A., Gnaneshwar, D., Sawhney,
R., Shah, R.R.: Leveraging BERT with
mixup for sentence classification (student
abstract). In: The Thirty-Fourth AAAI
Conference on Artificial Intelligence, AAAI
2020, New York, NY, USA. pp. 13829–13830
(2020), https://ojs.aaai.org/index.php/
AAAI/article/view/7186
[8] Kong, F., Zhang, R., Guo, X., Mensah, S.,
Mao, Y.: Dropmix: A textual data augmen-
tation combining dropout with mixup. In:
Goldberg, Y., Kozareva, Z., Zhang, Y. (eds.)
Proceedings of the 2022 Conference on Empir-
ical Methods in Natural Language Processing,
EMNLP 2022, Abu Dhabi, United Arab Emi-
rates, December 7-11, 2022. pp. 890–899. Asso-
ciation for Computational Linguistics (2022).
https://doi.org/10.18653/v1/2022.emnlp-
main.57, https://doi.org/10.18653/v1/
2022.emnlp-main.57
[9] Lewy, D., Ma´ ndziuk, J.: An overview
of mixing augmentation methods and
augmentation strategies. Artificial Intel-
ligence Review 56, 2111–2169 (2023).
https://doi.org/10.1007/s10462-022-10227-z
11

--- PAGE 12 ---
[10] Luo, X., Ding, H., Tang, M., Gandhi, P.,
Zhang, Z., He, Z.: Attention mechanism
with BERT for content annotation and
categorization of pregnancy-related ques-
tions on a community q&a site. In: IEEE
International Conference on Bioinformat-
ics and Biomedicine, BIBM 2020, Virtual
Event, South Korea. pp. 1077–1081 (2020).
https://doi.org/10.1109/BIBM49941.2020.9313379,
https://doi.org/10.1109/BIBM49941.
2020.9313379
[11] Maas, A.L., Daly, R.E., Pham, P.T., Huang,
D., Ng, A.Y., Potts, C.: Learning word vectors
for sentiment analysis. In: The 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies,
Proceedings of the Conference, Portland, Ore-
gon, USA. pp. 142–150 (2011), https://
aclanthology.org/P11-1015/
[12] Michel, P., Levy, O., Neubig, G.: Are sixteen
heads really better than one? In: Advances
in Neural Information Processing Systems
32: Annual Conference on Neural Infor-
mation Processing Systems 2019, NeurIPS
2019, Vancouver, BC, Canada. pp. 14014–
14024 (2019), https://proceedings.
neurips.cc/paper/2019/hash/
2c601ad9d2ff9bc8b282670cdd54f69f-Abstract.
html
[13] Pande, M., Budhraja, A., Nema, P., Ku-
mar, P., Khapra, M.M.: The heads hy-
pothesis: A unifying statistical approach to-
wards understanding multi-headed attention
in BERT. In: Thirty-Fifth AAAI Confer-
ence on Artificial Intelligence, AAAI 2021,
Virtual Event. pp. 13613–13621. AAAI Press
(2021), https://ojs.aaai.org/index.php/
AAAI/article/view/17605
[14] Pang, B., Lee, L.: Seeing stars: Exploiting
class relationships for sentiment catego-
rization with respect to rating scales. In:
ACL 2005, 43rd Annual Meeting of the
Association for Computational Linguistics,
Proceedings of the Conference, University
of Michigan, USA. pp. 115–124 (2005).
https://doi.org/10.3115/1219840.1219855,
https://aclanthology.org/P05-1015/[15] Socher, R., Perelygin, A., Wu, J., Chuang,
J., Manning, C.D., Ng, A.Y., Potts, C.: Re-
cursive deep models for semantic composi-
tionality over a sentiment treebank. In: Pro-
ceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing,
EMNLP 2013, Seattle, Washington, USA. pp.
1631–1642 (2013), https://aclanthology.
org/D13-1170/
[16] Sun, L., Xia, C., Yin, W., Liang, T., Yu,
P.S., He, L.: Mixup-transformer: Dynamic
data augmentation for NLP tasks. In: Scott,
D., Bel, N., Zong, C. (eds.) Proceedings
of the 28th International Conference on
Computational Linguistics, COLING 2020,
Barcelona, Spain (Online), December 8-13,
2020. pp. 3436–3440. International Commit-
tee on Computational Linguistics (2020).
https://doi.org/10.18653/v1/2020.coling-
main.305, https://doi.org/10.18653/v1/
2020.coling-main.305
[17] Uddin, A.F.M.S., Monira, M.S., Shin, W.,
Chung, T., Bae, S.: Saliencymix: A saliency
guided data augmentation strategy for bet-
ter regularization. CoRR abs/2006.01791
(2020)
[18] Vaswani, A., Shazeer, N., Parmar, N., Uszko-
reit, J., Jones, L., Gomez, A.N., Kaiser,
L., Polosukhin, I.: Attention is all you
need. CoRR abs/1706.03762 (2017), http:
//arxiv.org/abs/1706.03762
[19] Verma, V., Lamb, A., Beckham, C., Najafi,
A., Mitliagkas, I., Lopez-Paz, D., Bengio, Y.:
Manifold Mixup: Better Representations by
Interpolating Hidden States. In: Proceedings
of the 36th International Conference on Ma-
chine Learning, ICML 2019, Long Beach, Cal-
ifornia, USA. vol. 97, pp. 6438–6447 (2019)
[20] Vishwamitra, N., Hu, R.R., Luo, F., Cheng,
L., Costello, M., Yang, Y.: On analyz-
ing covid-19-related hate speech using
BERT attention. In: 19th IEEE Inter-
national Conference on Machine Learn-
ing and Applications, ICMLA 2020,
Miami, FL, USA. pp. 669–676 (2020).
https://doi.org/10.1109/ICMLA51294.2020.00111,
12

--- PAGE 13 ---
https://doi.org/10.1109/ICMLA51294.
2020.00111
[21] Walawalkar, D., Shen, Z., Liu, Z., Sav-
vides, M.: Attentive Cutmix: An Enhanced
Data Augmentation Approach for Deep
Learning Based Image Classification. In:
2020 IEEE International Conference on
Acoustics, Speech and Signal Process-
ing, ICASSP 2020. pp. 3642–3646 (2020).
https://doi.org/10.1109/ICASSP40776.2020.9053994
[22] Wang, A., Singh, A., Michael, J., Hill,
F., Levy, O., Bowman, S.R.: GLUE:
A multi-task benchmark and analysis plat-
form for natural language understanding. In:
7th International Conference on Learning
Representations, ICLR 2019, New Orleans,
LA, USA (2019), https://openreview.net/
forum?id=rJ4km2R5t7
[23] Wei, J.W., Zou, K.: EDA: easy data
augmentation techniques for boosting per-
formance on text classification tasks. In:
Inui, K., Jiang, J., Ng, V., Wan, X. (eds.)
Proceedings of the 2019 Conference on
Empirical Methods in Natural Language
Processing and the 9th International Joint
Conference on Natural Language Processing,
EMNLP-IJCNLP 2019, Hong Kong, China,
November 3-7, 2019. pp. 6381–6387. Associ-
ation for Computational Linguistics (2019).
https://doi.org/10.18653/v1/D19-1670,
https://doi.org/10.18653/v1/D19-1670
[24] Yoon, S., Kim, G., Park, K.: Ssmix:
Saliency-based span mixup for text classifi-
cation. In: Findings of the Association for
Computational Linguistics: ACL/IJCNLP
2021, Online Event. pp. 3225–3234 (2021).
https://doi.org/10.18653/v1/2021.findings-
acl.285, https://doi.org/10.18653/v1/
2021.findings-acl.285
[25] Zhang, H., Ciss´ e, M., Dauphin, Y.N., Lopez-
Paz, D.: mixup: Beyond Empirical Risk Mini-
mization. In: 6th International Conference on
Learning Representations, ICLR 2018, Van-
couver, BC, Canada (2018)
[26] Zhou, K., Yang, Y., Qiao, Y., Xiang, T.:
Domain generalization with mixstyle. CoRR
abs/2104.02008 (2021)
13
