# Viết lại Truy vấn Thích ứng: Căn chỉnh các Bộ viết lại thông qua Xác suất Biên của Câu trả lời Hội thoại

Tianhua Zhang♡∗, Kun Li♡∗, Hongyin Luo♢,
Xixin Wu♡, James Glass♢, Helen Meng♡
♡Đại học Trung văn Hồng Kông, Đặc khu Hành chính Hồng Kông, Trung Quốc
♢Viện Công nghệ Massachusetts, Cambridge MA, Hoa Kỳ
thzhang@link.cuhk.edu.hk, kunli@se.cuhk.edu.hk

## Tóm tắt

Viết lại truy vấn là một kỹ thuật quan trọng cho việc truy xuất đoạn văn trong hỏi đáp hội thoại miền mở (CQA). Nó khử ngữ cảnh hóa các truy vấn hội thoại thành các câu hỏi độc lập phù hợp cho các bộ truy xuất có sẵn. Các phương pháp hiện tại cố gắng tích hợp sở thích của bộ truy xuất trong quá trình huấn luyện các mô hình viết lại. Tuy nhiên, những cách tiếp cận này thường dựa vào các chú thích mở rộng như các bản viết lại trong miền và/hoặc nhãn đoạn văn liên quan, hạn chế khả năng tổng quát hóa và thích ứng của mô hình. Trong bài báo này, chúng tôi giới thiệu AdaQR (Viết lại Truy vấn Thích ứng), một khung để huấn luyện các mô hình viết lại truy vấn với các chú thích viết lại hạn chế từ các tập dữ liệu gốc và hoàn toàn không có nhãn đoạn văn. Cách tiếp cận của chúng tôi bắt đầu bằng việc tinh chỉnh các mô hình ngôn ngữ lớn nhỏ gọn chỉ sử dụng ~10% chú thích viết lại từ phần huấn luyện tập dữ liệu gốc. Các mô hình sau đó được sử dụng để tạo ra các ứng viên viết lại cho mỗi thể hiện truy vấn. Một cách tiếp cận mới được đề xuất để đánh giá sở thích của bộ truy xuất đối với các ứng viên này bằng xác suất của câu trả lời có điều kiện trên truy vấn hội thoại bằng cách biên hóa các đoạn văn Top-K. Điều này phục vụ như phần thưởng để tối ưu hóa bộ viết lại thêm bằng cách sử dụng Tối ưu hóa Sở thích Trực tiếp (DPO), một quá trình không cần chú thích viết lại và truy xuất. Kết quả thí nghiệm trên bốn tập dữ liệu CQA miền mở cho thấy AdaQR không chỉ tăng cường khả năng trong miền của bộ viết lại với yêu cầu chú thích hạn chế, mà còn thích ứng hiệu quả với các tập dữ liệu ngoài miền.

## 1 Giới thiệu

Truy xuất đoạn văn trong hỏi đáp hội thoại miền mở (CQA) đã thu được sự quan tâm đáng kể trong những năm gần đây (Anantha et al., 2021). Không giống như truy xuất tiêu chuẩn với các truy vấn một lượt (Kwiatkowski et al., 2019), nó đặt ra những thách thức độc đáo trong việc giải quyết các phụ thuộc hội thoại như bỏ sót, mơ hồ và giải quyết đồng tham chiếu (Qu et al., 2020; Adlakha et al., 2022). Nhiều phương pháp hiện tại (Yu et al., 2021; Lin et al., 2021b; Li et al., 2022) giải quyết những thách thức này bằng cách huấn luyện các bộ truy xuất chuyên biệt. Tuy nhiên, việc huấn luyện lại các bộ truy xuất cho tìm kiếm hội thoại có thể tốn kém và có thể không tận dụng đầy đủ các lợi ích của các bộ truy xuất một lượt có sẵn (Wu et al., 2022).

Một cách tiếp cận phổ biến để vượt qua thách thức này bao gồm viết lại truy vấn (QR) (Elgohary et al., 2019; Vakulenko et al., 2020; Yu et al., 2020). Trong phương pháp này, các truy vấn hội thoại được khử ngữ cảnh hóa thành các truy vấn độc lập, tự chứa, sau đó được xử lý bởi các bộ truy xuất có sẵn để tìm thông tin liên quan. Các nghiên cứu sớm (Elgohary et al., 2019; Anantha et al., 2021) tập trung vào việc tinh chỉnh các mô hình ngôn ngữ để cải tổ các bản viết lại của con người. Tuy nhiên, Ye et al. (2023) lưu ý rằng các chú thích của con người có thể chỉ giải quyết sự mơ hồ trong khi bỏ qua ngữ cảnh thông tin trong các cuộc hội thoại. Họ đề xuất sử dụng các mô hình ngôn ngữ lớn (LLM) để tạo ra bản viết lại. Nghiên cứu gần đây (Wu et al., 2022; Mo et al., 2024; Yoon et al., 2024) nhấn mạnh tầm quan trọng của việc tích hợp tín hiệu truy xuất trong quá trình huấn luyện bộ viết lại để tăng cường hiệu suất truy xuất xuôi dòng. Yoon et al. (2024) căn chỉnh các mô hình được tinh chỉnh với phản hồi của bộ truy xuất về xếp hạng của các đoạn văn vàng bằng cách sử dụng Tối ưu hóa Sở thích Trực tiếp (DPO).

Tuy nhiên, những cách tiếp cận này thường đòi hỏi lượng lớn nhãn viết lại và/hoặc đoạn văn để giám sát, nhưng tài nguyên lại khan hiếm và đắt đỏ để thu thập (Yu et al., 2021). Hơn nữa, chúng chủ yếu tối ưu hóa các hệ thống QR cho hiệu suất trong miền, tức là các nhãn huấn luyện đến từ các tập dữ liệu được xác thực, trong khi khả năng thích ứng và hiệu suất ngoài miền chưa được khám phá. Do đó, bài báo này tập trung vào: (1) huấn luyện hiệu quả và hiệu suất các mô hình QR với yêu cầu chú thích hạn chế và (2) kiểm tra khả năng thích ứng của chúng với căn chỉnh sở thích dưới giám sát yếu.

Theo paradigm để căn chỉnh LLM lấy tinh chỉnh có giám sát (SFT) và tối ưu hóa sở thích tuần tự (Ouyang et al., 2022), tối ưu hóa hướng tới sở thích của bộ truy xuất có thể được áp dụng cho các mô hình đã trải qua tinh chỉnh có giám sát cho viết lại truy vấn. Căn chỉnh với sở thích của bộ truy xuất có thể tinh chỉnh thêm bộ viết lại để cải tổ các truy vấn với khả năng thu hồi tốt hơn của các đoạn văn liên quan (Yoon et al., 2024). Quan trọng là, chúng tôi cũng thấy nó có khả năng thích ứng các bộ viết lại với các kịch bản CQA ngoài miền (§4). Tuy nhiên, một vấn đề cốt lõi là cách mô hình hóa sở thích của bộ truy xuất. Trong khi Yoon et al. (2024) sử dụng xếp hạng của đoạn văn vàng như sở thích của bộ truy xuất, chúng tôi nhằm khám phá mức độ có thể giảm việc sử dụng dữ liệu được gán nhãn. Chúng tôi lập luận rằng các câu trả lời tương ứng trong cuộc hội thoại có thể được sử dụng để hình thành sở thích của bộ truy xuất. Hơn nữa, câu trả lời hội thoại dễ tiếp cận hơn so với các đoạn văn vàng, vì chúng tự nhiên xảy ra khi dữ liệu CQA được tổng hợp.

Chúng tôi đề xuất một cách tiếp cận tối ưu hóa sở thích mới, AdaQR, cho các bộ viết lại truy vấn, nhằm tối ưu hóa các bộ viết lại để phục vụ cho các bộ truy xuất, bằng cách sử dụng câu trả lời hội thoại để mô hình hóa sở thích của bộ truy xuất. Cụ thể, chúng tôi đầu tiên để bộ viết lại SFT tạo ra một số bản viết lại, sau đó được sử dụng làm truy vấn để truy xuất một tập các đoạn văn bằng bộ truy xuất đích. Tiếp theo, chúng tôi tính toán xác suất có điều kiện của câu trả lời cho mỗi đoạn văn được truy xuất và cuộc hội thoại, và thu được xác suất biên của câu trả lời bằng cách biên hóa tập đoạn văn. Xác suất biên của câu trả lời phục vụ như phần thưởng định lượng sở thích của bộ truy xuất đối với các bản viết lại. Cuối cùng, chúng tôi ghép đôi những bản viết lại này dựa trên phần thưởng của chúng để tối ưu hóa bộ viết lại SFT với DPO.

Chúng tôi kiểm tra hiệu suất trong miền của AdaQR nơi dữ liệu huấn luyện cho SFT và tối ưu hóa sở thích đều đến từ các tập dữ liệu được xác thực. Kết quả thực nghiệm cho thấy AdaQR cải thiện đáng kể chất lượng của các bản viết lại được tạo ra bởi bộ viết lại, so với phiên bản chỉ SFT, dẫn đến hiệu suất tương đương hoặc thậm chí tốt hơn so với các phương pháp QR SOTA hiện tại. Quan trọng hơn, đánh giá ngoài miền, nơi tối ưu hóa sở thích được áp dụng cho bộ viết lại SFT ngoài miền, cũng quan sát thấy mức tăng hiệu suất tương tự, chứng minh khả năng của AdaQR trong việc thích ứng bộ viết lại với các miền đích.

Các đóng góp chính của công trình này là: (1) Chúng tôi đề xuất AdaQR, một cách tiếp cận tối ưu hóa sở thích để tăng cường bộ viết lại truy vấn. (2) AdaQR mô hình hóa sở thích của bộ truy xuất bằng cách tận dụng câu trả lời trong cuộc hội thoại, cho phép huấn luyện các bộ viết lại truy vấn cho các nhiệm vụ hỏi đáp hội thoại khác nhau ngay cả không có nhãn đoạn văn. (3) Thí nghiệm cho thấy AdaQR không chỉ có thể khuếch đại khả năng trong miền của bộ viết lại, mà còn thích ứng chúng với các nhiệm vụ hỏi đáp hội thoại ngoài miền.

## 2 Phương pháp

### 2.1 Công thức Nhiệm vụ

Chúng tôi tập trung vào nhiệm vụ viết lại truy vấn cho truy xuất đoạn văn hội thoại sử dụng các bộ truy xuất có sẵn. Cho lịch sử hội thoại H<t = {qi, ai}t−1i=1, trong đó t biểu thị số lượt hiện tại, một mô hình viết lại truy vấn Mθ được huấn luyện để biến đổi câu hỏi hiện tại qt thành một truy vấn độc lập, tự chứa ˆrt. Chúng tôi bỏ qua chỉ số dưới t trong mô tả tiếp theo để đơn giản. Bộ truy xuất R, vẫn không thay đổi, lấy ˆr làm đầu vào để tìm kiếm các đoạn văn liên quan từ kho văn liệu P. Trong quá trình huấn luyện hoàn chỉnh của QR, chúng tôi chỉ cần số lượng hạn chế nhãn viết lại {r} từ hai tập dữ liệu gốc, không cần chú thích đoạn văn.

### 2.2 Tổng quan

Chúng tôi đề xuất AdaQR để xây dựng các bộ viết lại truy vấn áp dụng cho các kịch bản hỏi đáp hội thoại khác nhau, thông qua tối ưu hóa sở thích không bao gồm nhãn viết lại hoặc đoạn văn trong miền. AdaQR sử dụng xác suất của câu trả lời như phần thưởng định lượng sở thích của bộ truy xuất đối với các bản viết lại, để tối ưu hóa thêm một bộ viết lại đã được điều chỉnh trên dữ liệu ngoài miền và thích ứng nó với tập dữ liệu đích.

Như được hiển thị trong Hình 1, AdaQR, khởi động nóng với bộ viết lại truy vấn SFT được tinh chỉnh với số lượng hạn chế dữ liệu được gán nhãn (trong miền hoặc ngoài miền) trước đó (§2.3), hoạt động trong quy trình: (1) Lấy mẫu các bản viết lại từ bộ viết lại SFT, sau đó được sử dụng làm truy vấn tìm kiếm để truy xuất đoạn văn; (2) Tính xác suất biên của câu trả lời như phần thưởng dựa trên cuộc hội thoại và các đoạn văn được truy xuất (§2.4); (3) Xây dựng các cặp sở thích sử dụng phần thưởng và điều chỉnh bộ viết lại với Tối ưu hóa Sở thích Trực tiếp (Rafailov et al., 2024) (§2.5).

### 2.3 Tinh chỉnh Có giám sát

**Nhãn Viết lại Hạn chế** Để trang bị cho các mô hình khả năng viết lại truy vấn cơ bản, chúng tôi cần một số lượng hạn chế nhãn viết lại để tinh chỉnh có giám sát. Chúng tôi riêng biệt tuyển chọn nhãn (~10% tập huấn luyện) từ hai tập dữ liệu gốc, QReCC (Anantha et al., 2021) và TopiOCQA (Adlakha et al., 2022) để phân tích hiệu suất thích ứng toàn diện của AdaQR. Chúng tôi tạo ra QReCC-SFT với 3,850 nhãn viết lại được tạo bởi ChatGPT (gpt-3.5-turbo) dưới thiết lập học ít mẫu từ công trình trước đây (Ye et al., 2023). Chúng tôi tạo ra TopiOCQA-SFT với 4,278 thể hiện huấn luyện, với nhãn viết lại được tạo bởi gpt-4. Cùng hướng dẫn và ví dụ học trong ngữ cảnh được sử dụng trong QReCC từ Ye et al. (2023) được tuân theo. Lời nhắc hoàn chỉnh được chi tiết trong Phụ lục Bảng 4. Lưu ý rằng không cần nhãn đoạn văn ở giai đoạn này. Hai mô hình tinh chỉnh kết quả MSFT, sau đó được thích ứng và kiểm tra trên các tập dữ liệu bổ sung.

**Mục tiêu Huấn luyện** Chúng tôi sử dụng dữ liệu được gán nhãn tuyển chọn để tinh chỉnh một mô hình ngôn ngữ, trang bị cho nó khả năng viết lại truy vấn cơ bản. Cho lịch sử hội thoại H và truy vấn hiện tại q, LM Mθ được huấn luyện để dự đoán bản viết lại r bằng cách tối thiểu hóa log-likelihood âm như

LSFT = −log pMθ(r|H, q) (1)

Cần lưu ý rằng các mô hình được huấn luyện theo cách trên có thể có khả năng viết lại không đầy đủ, đặc biệt cho truy vấn hội thoại ngoài miền (sẽ được hiển thị trong §4). Tiếp theo, chúng tôi áp dụng AdaQR cho những bộ viết lại SFT này để tăng cường khả năng của chúng cho cả kịch bản trong miền và ngoài miền.

### 2.4 Thu thập Phần thưởng

**Lấy mẫu Viết lại** Để thu được dữ liệu huấn luyện cho tối ưu hóa sở thích, chúng tôi sử dụng các mô hình được tinh chỉnh, MSFT, để lấy mẫu ba ứng viên viết lại {ˆri}3i=1 cho mỗi truy vấn hội thoại q với nhiệt độ T = 1. Chiến lược này tạo ra các ứng viên viết lại hợp lý cho tối ưu hóa sở thích, bỏ qua nhu cầu gán nhãn của con người đắt đỏ hoặc lời nhắc mô hình ngôn ngữ lớn được áp dụng bởi công trình trước đây (Yoon et al., 2024).

**Tính toán Phần thưởng** Chúng tôi đề xuất một phương pháp tính toán phần thưởng mới chỉ dựa vào các lượt hội thoại, loại bỏ nhu cầu nhãn đoạn văn. Được thúc đẩy bởi Lewis et al. (2021), chúng tôi sử dụng giám sát yếu với xác suất biên của câu trả lời như phản hồi sở thích, coi các đoạn văn được truy xuất như một biến tiềm ẩn. Cụ thể, cho mỗi ứng viên viết lại ˆri, chúng tôi truy xuất K đoạn văn hàng đầu Pi = {pik}Kk=1 với bộ truy xuất R. Một mô hình ngôn ngữ lớn được huấn luyện trước A sau đó tính toán log xác suất Sk của việc tạo ra câu trả lời đích a có điều kiện trên mỗi đoạn văn được truy xuất pik và câu hỏi gốc q được nối sau lịch sử hội thoại H:

Sk = log pA(a|H, q, pik) (2)

Chúng tôi chọn một mô hình được huấn luyện trước do phỏng đoán rằng LLM có khả năng vốn có được thiết lập trong quá trình huấn luyện trước, trong khi tinh chỉnh hoặc căn chỉnh sau này có thể ảnh hưởng đến phân bố logit, dẫn đến thuế căn chỉnh hoặc lệch căn chỉnh khả năng (Huang et al., 2023; Lin et al., 2024; Gekhman et al., 2024).

Để kiểm soát ảnh hưởng của bản viết lại trong tính toán xác suất, chúng tôi sử dụng cuộc hội thoại gốc làm câu hỏi đầu vào thay vì các truy vấn được viết lại. Điều này đảm bảo đoạn văn cơ sở pik chỉ chịu trách nhiệm về đóng góp vào điểm số Sk. Bằng cách biên hóa K đoạn văn hàng đầu, chúng tôi tính toán xác suất biên của câu trả lời ei như sở thích của bộ truy xuất đối với ứng viên viết lại ˆri:

ei = ΣKk=1 PR(pik|ˆri)Sk (3)

trong đó PR(pik|ˆri) biểu thị phân bố trên các đoạn văn thu được bằng cách áp dụng hàm softmax cho điểm số truy xuất của chúng. Trực giác, một bản viết lại hiệu quả hơn được sử dụng như một truy vấn tìm kiếm cải thiện cơ hội thu hồi các đoạn văn có khả năng liên quan. Một đoạn văn với mức độ liên quan cao hơn càng tăng cường khả năng tạo ra câu trả lời. Do đó, bản viết lại này dẫn đến phần thưởng tốt hơn với xác suất biên cao hơn.

### 2.5 Tối ưu hóa Sở thích

Mục tiêu của chúng tôi là căn chỉnh mô hình viết lại SFT Mθ để tạo ra các bản viết lại được bộ truy xuất đích ưa thích, được định lượng bởi các xác suất câu trả lời biên e như trong Phương trình 3. Vì vậy, chúng tôi áp dụng Tối ưu hóa Sở thích Trực tiếp (Rafailov et al., 2024) với e làm phần thưởng để điều chỉnh Mθ.

**Xây dựng Cặp Sở thích** Cho mỗi ví dụ hội thoại, chúng tôi xây dựng dữ liệu sở thích cặp đôi {(H, q, ˆrw, ˆrl)} bằng cách chọn các cặp bản viết lại (ˆrw, ˆrl) từ {ˆri}3i=1 sao cho ew − el > δ, trong đó δ > 0 là một siêu tham số. Do đặc tính của ei, ràng buộc này đảm bảo rằng bản viết lại được ưa thích ˆrw sẽ dẫn đến các đoạn văn hữu ích có khả năng cao hơn so với bản không được ưa thích ˆrl. Không giống như các phương pháp thông thường, dữ liệu sở thích của chúng tôi được phát triển mà không có chú thích của con người, bằng cách sử dụng đo lường tự động về sở thích của bộ truy xuất trong §2.4.

**Mục tiêu Huấn luyện** Sử dụng dữ liệu sở thích cặp đôi, chúng tôi điều chỉnh mô hình Mθ với DPO. Mục tiêu huấn luyện là tối thiểu hóa

LDPO = −log σ(β log Mθ(ˆrw|q, H)/MSFT(ˆrw|q, H) − β log Mθ(ˆrl|q, H)/MSFT(ˆrl|q, H)) (4)

trong đó MSFT là mô hình tham chiếu từ đó Mθ được khởi tạo, σ là hàm sigmoid, và β là một siêu tham số. Với mục tiêu này, mô hình được tối ưu hóa để tối đa hóa sự tương phản giữa các bản viết lại được ưa thích và không được ưa thích. Do đó nó được khuyến khích tạo ra các bản viết lại với xác suất biên của câu trả lời cao hơn, có nhiều khả năng dẫn đến các đoạn văn hữu ích. Xem thuật toán hoàn chỉnh của AdaQR trong Phụ lục Thuật toán 1.

## 3 Thí nghiệm

**Tập dữ liệu** Chúng tôi đánh giá AdaQR cho nhiệm vụ truy xuất hội thoại trên bốn điểm chuẩn hỏi đáp hội thoại (CQA): QReCC (Anantha et al., 2021), TopiOCQA (Adlakha et al., 2022), Doc2Dial (Feng et al., 2020) và MultiDoc2Dial (Feng et al., 2021). Các câu trả lời trong QReCC thể hiện mức độ trùng lặp từ-mức tương đối lớn với các đoạn văn hỗ trợ trong khi TopiOCQA sử dụng các phản hồi dạng tự do như câu trả lời (Xem Phân tích 5.1). TopiOCQA và MultiDoc2Dial liên quan đến chuyển đổi chủ đề với các lượt trong cuộc hội thoại dựa trên nhiều tài liệu.

**Hệ thống Truy xuất** Chúng tôi điều tra hiệu suất của AdaQR sử dụng cả bộ truy xuất thưa thớt và dày đặc. BM25 phục vụ như bộ truy xuất thưa thớt cho tất cả các tập dữ liệu. Cho bộ truy xuất dày đặc, chúng tôi sử dụng ANCE (Xiong et al., 2020) được huấn luyện trên các nhiệm vụ truy xuất đoạn văn MS-MARCO (Bajaj et al., 2018) cho tất cả các tập dữ liệu trừ Doc2Dial, để phù hợp với các nghiên cứu trước đây (Jang et al., 2024; Yoon et al., 2024). Chúng tôi sử dụng E5-unsupervised (Wang et al., 2024) cho Doc2Dial theo Liu et al. (2024). Đáng chú ý, chúng tôi không huấn luyện thêm các bộ truy xuất cho nhiệm vụ cụ thể của chúng tôi. Chi tiết hơn được liệt kê trong Phụ lục B.1.

**Metrics Đánh giá** Chúng tôi đánh giá hiệu suất truy xuất bằng một số metrics: Mean Reciprocal Rank (MRR) tính toán thứ hạng trung bình của các đoạn văn vàng. Normalized Discounted Cumulative@3 (NDCG) đánh giá kết quả truy xuất top-3 bằng cách xem xét cả mức độ liên quan và thứ hạng. Recall@k phản ánh liệu bộ truy xuất có thành công xác định các đoạn văn vàng trong kết quả top-k không.

**Thực hiện** Chúng tôi tinh chỉnh hai mô hình SFT với hai tập dữ liệu gốc: QReCC-SFT và TopiOCQA-SFT. Mỗi mô hình SFT trải qua huấn luyện thêm với DPO, sử dụng phản hồi bộ truy xuất được thu thập trong §2.4 trên tất cả bốn tập dữ liệu tương ứng. Mô hình cốt lõi của chúng tôi cho tất cả các cấu hình là Mistral-7B (Jiang et al., 2023). Để xác định khả năng tổng quát hóa qua các LM khác nhau, chúng tôi tiếp tục đánh giá hiệu suất của Gemma-7B (Team et al., 2024) và Llama2-7B (Touvron et al., 2023) cho tất cả các điểm chuẩn với truy xuất thưa thớt trong Phụ lục Bảng 9. Chúng tôi sử dụng Mistral-7B-v0.1 được huấn luyện trước làm mô hình CQA cho tính toán phần thưởng. Xem chi tiết huấn luyện trong Phụ lục B.2.

**Baseline** (1) T5QR (Lin et al., 2020) tinh chỉnh T5-base (Raffel et al., 2020) để bắt chước các bản viết lại của con người. (2) CONQRR (Wu et al., 2022) tối ưu hóa các bộ viết lại truy vấn sử dụng học tăng cường, với xếp hạng của các đoạn văn có độ trùng lặp token tối đa với câu trả lời như giám sát yếu. (3) ConvGQR (Mo et al., 2024) huấn luyện các mô hình viết lại và mở rộng truy vấn với Mean Squared Error giữa các embedding của truy vấn và đoạn văn liên quan như một loss phụ. (4) IterCQR (Jang et al., 2024) lặp lại huấn luyện bộ viết lại truy vấn với độ tương tự cosine giữa các đoạn văn vàng và các truy vấn được cải tổ bởi ChatGPT như tín hiệu IR. (5) LLM IQR (Ye et al., 2023) giới thiệu "viết lại-sau đó-chỉnh sửa" để nhắc ChatGPT trước tiên tạo ra các bản viết lại và sau đó chỉnh sửa chúng theo các tiêu chí được định nghĩa trước. (6) RETPO (Yoon et al., 2024) nhắc gpt-4 tạo ra nhiều bản viết lại và thu thập xếp hạng đoạn văn vàng như phản hồi truy xuất trên tất cả dữ liệu huấn luyện của QReCC và TopioCQA. RETPO tinh chỉnh Llama2-7b (Touvron et al., 2023) để sao chép bản viết lại với sở thích truy xuất tốt nhất, được gọi là (7) Llama2 Distill, và sau đó căn chỉnh nó với sở thích truy xuất sử dụng DPO. (8) GPT4 Prompting (Yoon et al., 2024) tạo ra các bản viết lại cho các câu hỏi kiểm tra với gpt-4. (9) Chúng tôi thực hiện Gold-Label trong thiết lập của chúng tôi, với xếp hạng của các đoạn văn vàng như sở thích truy xuất. Điều này phục vụ như một cận trên cho hiệu suất của AdaQR được giám sát yếu của chúng tôi.

## 4 Kết quả Chính

Bảng 1 và 2 hiển thị kết quả chính của AdaQR trên 4 điểm chuẩn và 2 bộ truy xuất, cùng với so sánh với các phương pháp baseline.

**Tối ưu hóa sở thích mang lại cải thiện so với SFT.** AdaQR (Ours) cho thấy cải thiện nhất quán và rõ ràng so với phiên bản chỉ SFT của nó trên tất cả các kết hợp của tập dữ liệu và bộ truy xuất, chỉ ra hiệu quả của tối ưu hóa sở thích trong việc tăng cường thêm khả năng của bộ viết lại.

**AdaQR cải thiện hiệu suất trong miền và ngoài miền.** Đối với các kịch bản trong miền, trên QReCC, cách tiếp cận của chúng tôi, QReCC-SFT+Ours, vượt trội so với tất cả các baseline; trong khi trên TopiOCQA, TopiOCQA-SFT+Ours vượt qua các baseline khác ngoài RETPO và biến thể ablated của nó Llama2 Distill dưới bộ truy xuất BM25, về hiệu suất trung bình. Những baseline này đều đòi hỏi nhãn đoạn văn, đặc biệt là RETPO và Llama2 Distill bao gồm việc sử dụng rộng rãi cả nhãn đoạn văn và nhãn viết lại từ sự kết hợp của hai tập dữ liệu trên. Đối với các kịch bản ngoài miền trên bốn tập dữ liệu, Ours bắt đầu với SFT hạt giống không đồng nhất và sau đó trải qua tối ưu hóa sở thích trên các tập dữ liệu đích (ví dụ, QReCC SFT+Ours trên TopiOCQA, TopiOCQA SFT+Ours trên QReCC), vẫn vượt qua hầu hết các baseline. Trong hầu hết trường hợp, SFT hạt giống không đồng nhất tụt hậu so với các baseline, nhưng tiến gần hoặc vượt qua chúng sau tối ưu hóa sở thích (+Ours). Cùng nhau, cách tiếp cận của chúng tôi không chỉ có thể khuếch đại khả năng trong miền của bộ viết lại mà còn thành công thích ứng chúng với các nhiệm vụ ngoài miền, ngay cả khi không có nhãn đoạn văn.

**Câu trả lời hội thoại hiệu quả như nhãn đoạn văn cho tối ưu hóa sở thích.** Cả cách tiếp cận của chúng tôi và biến thể của nó với Gold-Label, đều được thực hiện trong cùng một paradigm nhưng với các loại phần thưởng khác nhau (xác suất biên của câu trả lời vs. xếp hạng của các đoạn văn vàng). Hai cách tiếp cận có hiệu suất tương đương, và đôi khi Ours thậm chí vượt trội Gold-Label, chứng minh vai trò hiệu quả của phần thưởng dựa trên xác suất biên để mô hình hóa sở thích của bộ truy xuất, trong khi phần thưởng như vậy dễ tiếp cận hơn. Điều này khiến AdaQR trở thành một phương pháp khá hiệu quả về chi phí để thích ứng các bộ viết lại cho các nhiệm vụ CQA khác nhau.

**AdaQR có khả năng áp dụng cho cả bộ truy xuất thưa thớt và dày đặc.** Hiệu ứng tích cực của cách tiếp cận của chúng tôi có thể được thấy cho cả bộ truy xuất thưa thớt (BM25) và dày đặc (E5, ANCE), điều này xác minh khả năng áp dụng chung của cách tiếp cận của chúng tôi đối với các bộ truy xuất khác nhau. Mặt khác, lưu ý rằng bộ truy xuất ANCE có hiệu suất tốt hơn BM25 cho TopiOCQA, như được minh họa bởi các metrics truy xuất cao hơn của SFT dưới ANCE so với dưới BM25. Chúng tôi cũng quan sát thấy rằng đối với TopiOCQA, sự cải thiện của Ours so với SFT dưới ANCE lớn hơn so với dưới BM25. Do đó, hợp lý để suy đoán rằng lợi ích mang lại bởi cách tiếp cận của chúng tôi sẽ trở nên rõ ràng hơn với các bộ truy xuất tốt hơn.

Các mẫu tương tự cũng thể hiện trong các trường hợp sử dụng Gemma và Llama2 làm mô hình cơ sở (xem Bảng 9). Điều này chỉ ra hiệu quả chung của AdaQR với các LM mã nguồn mở khác nhau.

## 5 Phân tích

### 5.1 So sánh các Phương pháp Giám sát Yếu

Chúng tôi so sánh cách tiếp cận tính toán phần thưởng của chúng tôi (Ours) với một phương pháp giám sát yếu dựa trên mức từ (Pseudo-Label) trong cùng thiết lập, như được hiển thị trong Bảng 3. Cả hai phương pháp đều loại bỏ nhu cầu nhãn trong miền. Phương pháp của chúng tôi tạo ra phản hồi truy xuất thông qua việc đánh giá xác suất của câu trả lời đích bằng cách biên hóa các đoạn văn top-K. Ngược lại, Pseudo-Label sử dụng xếp hạng của các đoạn văn giả liên quan có điểm F1 tối đa với câu trả lời như sở thích của bộ truy xuất, theo Wu et al. (2022).

**AdaQR vượt trội Pseudo-Label trên TopiOCQA, Doc2Dial và MultiDoc2Dial** trên tất cả các thiết lập về hiệu suất trung bình. Hiệu suất tương đối tốt của Pseudo-Label trên QReCC được quy cho các đặc tính của tập dữ liệu, nơi các câu trả lời thể hiện mức độ trùng lặp cao với các câu trong các đoạn văn hỗ trợ. Do đó, giám sát yếu dựa trên mức từ đơn giản này có thể dễ dàng xác định đoạn văn liên quan, với 82% đoạn văn vàng được phát hiện. Tuy nhiên, Pseudo-Label đặc biệt nhạy cảm với ảnh hưởng của yêu cầu trùng lặp mức từ. Chúng tôi trực quan hóa hiệu suất so với SFT trên bốn tập dữ liệu với điểm F1 giảm dần trong Hình 2. Hiệu suất truy xuất của Pseudo-Label giảm đáng kể khi điểm F1 giảm. Đáng chú ý, khi được đánh giá trên TopiOCQA, có các phản hồi dạng tự do như câu trả lời, cách tiếp cận này thậm chí ảnh hưởng tiêu cực đến kết quả, dẫn đến hiệu suất kém hơn phiên bản chỉ SFT, tức là Pseudo-Label/SFT < 1. Ngược lại, AdaQR đo lường sở thích của bộ truy xuất từ mức ngữ nghĩa, thể hiện độ mạnh mẽ và ổn định lớn hơn, cung cấp cải thiện hiệu suất nhất quán trên tất cả các thiết lập.

Hơn nữa, như được phân tích trong Phụ lục D và Hình 5, AdaQR cho thấy sự tăng cường đáng kể trong việc giải quyết các truy vấn thách thức với chuyển đổi chủ đề so với SFT, đạt được tỷ lệ hiệu suất tương đương với đối tác Gold-Label. Tuy nhiên, sự phụ thuộc nặng nề của Pseudo-Label vào sự trùng lặp mức từ cản trở hiệu quả của nó.

### 5.2 Hiệu ứng của Giá trị K trong Tính toán Phần thưởng

Hình 3 mô tả hiệu suất truy xuất trên số lượng đoạn văn được truy xuất hàng đầu khác nhau (K = 1,3,5,7,9) cho tính toán phần thưởng trong §2.4. Các metrics hiệu suất chi tiết được trình bày trong Bảng 10 của Phụ lục. Tối ưu hóa sở thích trực tiếp trên tất cả các giá trị ứng viên của K cải thiện đáng kể hiệu suất truy xuất so với phiên bản SFT, phản ánh hiệu quả của phương pháp tính toán phần thưởng được đề xuất của chúng tôi. Việc chỉ dựa vào đoạn văn được truy xuất top-1 có thể mang lại kết quả dưới tối ưu. Ví dụ, chỉ 36,4% thể hiện huấn luyện xếp hạng đoạn văn vàng đầu tiên với BM25 trên QReCC. Đối với Doc2Dial, việc tăng giá trị K có xu hướng tăng cường hiệu suất tổng thể. AdaQR thể hiện độ mạnh mẽ đối với thông tin không liên quan tiềm tàng khi có nhiều đoạn văn được tham gia bằng cách mở rộng K, và hiệu quả phản ánh sở thích của bộ truy xuất mà không cần đoạn văn trong miền hoặc nhãn viết lại. Chúng tôi chọn K = 5 trên tất cả các thiết lập để đạt được sự cân bằng giữa hiệu quả và hiệu suất, tránh thiên vị thủ công hướng tới cấu hình tốt nhất.

### 5.3 Hiệu ứng của Khối lượng Dữ liệu trong Tối ưu hóa Sở thích

Chúng tôi lấy mẫu ngẫu nhiên các cặp viết lại từ những cặp được sử dụng trong thí nghiệm chính ở các tỷ lệ khác nhau, 20%−80% với khoảng cách 20%, và sau đó sử dụng DPO để điều chỉnh QReCC-SFT trên những bộ cặp sở thích này riêng lẻ. Hiệu suất trung bình trên QReCC (trong miền) và Doc2dial (ngoài miền) được vẽ trong Hình 4(a). Hiệu suất thường cải thiện với khối lượng dữ liệu lớn hơn. Đáng chú ý, ngay cả với chỉ 20% cặp, cách tiếp cận của chúng tôi vẫn đạt được cải thiện thỏa mãn so với phiên bản SFT của nó.

AdaQR cho phép chúng tôi tích hợp thêm các thể hiện trong tập huấn luyện QReCC thiếu nhãn đoạn văn vàng cho tối ưu hóa sở thích, trong khi những ví dụ này không thể được sử dụng cho huấn luyện của cách tiếp cận Gold-Label. Để xác minh lợi ích của những dữ liệu không được gán nhãn này được kích hoạt bởi giám sát yếu của chúng tôi, chúng tôi thu thập cặp từ mỗi ví dụ với khoảng cách phần thưởng lớn nhất cho cả Gold-Label và Ours, và giảm dần kích thước dữ liệu được sử dụng cho Ours. Trong Hình 4(b), Ours với 40% cặp đạt được mức hiệu suất tương tự như Gold-Label, tại thời điểm đó Ours sử dụng ít hơn 22% số cặp huấn luyện so với Gold-Label. Quan trọng, khi được huấn luyện trên ≥50% cặp, Ours luôn vượt qua Gold-Label và đạt đỉnh với dữ liệu đầy đủ, làm nổi bật lợi thế của AdaQR trong việc khai thác dữ liệu không được gán nhãn.

## 6 Các Công trình Liên quan

**Truy xuất Hội thoại** là một nhiệm vụ tiền đề cho hỏi đáp hội thoại miền mở. Nhiều cách tiếp cận hiện tại (Yu et al., 2021; Li et al., 2022; Lin et al., 2021b) tinh chỉnh các bộ truy xuất dày đặc chuyên biệt. Tuy nhiên, để tận dụng các lợi ích của các bộ truy xuất một lượt có sẵn, viết lại truy vấn hội thoại đã được áp dụng để chuyển đổi mỗi câu hỏi hội thoại thành một truy vấn độc lập (Elgohary et al., 2019; Vakulenko et al., 2020; Yu et al., 2020). Các cách tiếp cận trước đây thường huấn luyện các mô hình viết lại truy vấn với nhãn viết lại của con người (Elgohary et al., 2019; Anantha et al., 2021) hoặc dựa trên LLM (Ye et al., 2023; Jang et al., 2024). Tuy nhiên, việc thu thập nhãn trong miền để huấn luyện trên mỗi tập dữ liệu cụ thể tỏ ra tốn kém và việc tinh chỉnh tiêu chuẩn một mình thất bại trong việc tích hợp phản hồi truy xuất, có thể dẫn đến hiệu suất dưới tối ưu (Wu et al., 2022; Yoon et al., 2024). Mặc dù các nghiên cứu gần đây (Wu et al., 2022; Mo et al., 2024; Yoon et al., 2024) đề xuất tích hợp tín hiệu từ bộ truy xuất với căn chỉnh sở thích, chúng đòi hỏi lượng lớn nhãn trong miền. Để giảm bớt nút thắt dữ liệu này, chúng tôi đề xuất huấn luyện các mô hình viết lại truy vấn hiệu quả và đánh giá hiệu suất thích ứng chỉ với nhãn viết lại hạn chế và hoàn toàn không có chú thích đoạn văn.

**Tối ưu hóa Sở thích** là một lĩnh vực nghiên cứu quan trọng tập trung vào việc đảm bảo rằng các mô hình ngôn ngữ lớn tuân thủ các giá trị và ý định của con người (Bai et al., 2022; Ouyang et al., 2022; Rafailov et al., 2024). Ouyang et al. (2022) tinh chỉnh LLM với phản hồi của con người để căn chỉnh chúng với ý định người dùng, bao gồm việc thu thập các minh chứng và xếp hạng đầu ra mô hình được chú thích bởi con người, theo sau là học có giám sát và học tăng cường. Kim et al. (2023) sử dụng phản hồi tổng hợp thay vì chú thích của con người, bằng mô hình hóa phần thưởng với phản hồi tổng hợp để mô phỏng các minh chứng chất lượng cao. Ngoài việc cải thiện khả năng chung của LLM, một số công trình cũng tập trung vào các khía cạnh cụ thể. Ví dụ, Tian et al. (2023) sử dụng các phép đo tính chân thực như tín hiệu sở thích proxy để khuyến khích tính thực tế trong mô hình. Yoon et al. (2024) sử dụng xếp hạng của đoạn văn vàng như sở thích của bộ truy xuất để tối ưu hóa mô hình cho viết lại truy vấn tìm kiếm. Cách tiếp cận của chúng tôi tương tự như Yoon et al. (2024) trong nhiệm vụ đích, viết lại truy vấn. Tuy nhiên, các tín hiệu sở thích được sử dụng để căn chỉnh mô hình của chúng tôi được tổng hợp mà không có bất kỳ dữ liệu được gán nhãn liên quan đến truy xuất nào.

## 7 Kết luận

Chúng tôi giới thiệu AdaQR để tăng cường các bộ viết lại truy vấn với nhãn trong miền tối thiểu đến không, thông qua tối ưu hóa sở thích hướng tới sở thích của bộ truy xuất. Một đặc điểm mới của AdaQR là trong việc đo lường sở thích của bộ truy xuất với các xác suất biên của câu trả lời dựa trên cuộc hội thoại và các đoạn văn được truy xuất, cho phép cải thiện hoặc thích ứng các bộ viết lại truy vấn mà không cần dữ liệu được gán nhãn. Thí nghiệm cho thấy AdaQR mang lại cải thiện đáng kể cho hiệu suất trong miền của bộ viết lại truy vấn và thích ứng chúng tốt với các nhiệm vụ hỏi đáp hội thoại ngoài miền. AdaQR thể hiện hứa hẹn trong việc thiết lập các bộ viết lại truy vấn hiệu quả cho các nhiệm vụ hỏi đáp hội thoại tùy ý với nỗ lực tối thiểu.

## Hạn chế

Mặc dù AdaQR thể hiện khả năng tổng quát hóa và thích ứng đáng chú ý với yêu cầu nhãn viết lại hạn chế, vẫn có một số hạn chế. Đầu tiên, đánh giá của chúng tôi trong nghiên cứu này được thực hiện trên bốn tập dữ liệu. Chúng tôi sử dụng QReCC và TopiOCQA như hai tập dữ liệu gốc riêng biệt, và thích ứng các bộ viết lại SFT với ba tập dữ liệu khác như đánh giá ngoài miền. Điều này có thể không bao gồm tất cả các kịch bản có thể. Thứ hai, do các ràng buộc tính toán, chúng tôi không tham gia vào phân tích chi tiết về cách chất lượng của nhãn viết lại và số lượng chú thích trong giai đoạn tinh chỉnh có giám sát có thể ảnh hưởng đến hiệu suất tổng thể. Mục tiêu của chúng tôi là sử dụng một lượng nhỏ nhãn để đạt được hiệu suất truy xuất tốt. Khám phá thêm về tác động của chất lượng và số lượng nhãn vẫn là một hướng có thể để cải thiện hiệu suất viết lại và giảm nhu cầu về chú thích. Cuối cùng, chúng tôi đề xuất AdaQR để tạo ra sở thích của bộ truy xuất sử dụng xác suất có điều kiện của câu trả lời. Trong khi chúng tôi phân tích ngắn gọn tác động của các loại tổ chức đoạn văn khác nhau (nối và biên hóa) trong Phụ lục E, trọng tâm chính của chúng tôi trong bài báo này nằm ở việc phân tích kỹ lưỡng của phương pháp biên hóa do tính mạnh mẽ và hiệu quả của nó. Tuy nhiên, chúng tôi thừa nhận lợi ích tiềm tàng của việc đi sâu hơn vào các phương pháp dựa trên nối, có thể cung cấp những hiểu biết có giá trị cho cộng đồng nghiên cứu trong việc giải quyết nhiệm vụ viết lại truy vấn.

## Tuyên bố Đạo đức

Viết lại truy vấn là công cụ quan trọng trong việc làm rõ ý định tìm kiếm của người dùng trong các cuộc hội thoại tìm kiếm thông tin, cải thiện việc truy xuất các đoạn văn liên quan. Công trình của chúng tôi có thể cải thiện đáng kể hiệu suất của các bộ viết lại truy vấn. Tuy nhiên, điều quan trọng là phải nhận ra rằng cách tiếp cận của chúng tôi không thể luôn đảm bảo các bản viết lại hoàn hảo và có thể truy xuất thông tin không liên quan hoặc thậm chí không thực tế. Điều này một phần do những thiếu sót vốn có của các mô hình ngôn ngữ lớn, phục vụ như các mô hình nền tảng trong cách tiếp cận của chúng tôi. Những mô hình này có xu hướng tạo ra ảo giác. Các lý do tiềm tàng khác bao gồm các bộ truy xuất không hoàn hảo và phạm vi tìm kiếm hạn chế. Kết quả truy xuất chất lượng thấp có thể gây nhầm lẫn hoặc thậm chí đánh lạc hướng người dùng. Do đó, để đảm bảo độ tin cậy của kết quả truy xuất trong các kịch bản thực tế, điều quan trọng là phải triển khai các cơ chế lọc hiệu quả, như reranker, để xác định và loại trừ các đoạn văn chứa thông tin không thực tế.
