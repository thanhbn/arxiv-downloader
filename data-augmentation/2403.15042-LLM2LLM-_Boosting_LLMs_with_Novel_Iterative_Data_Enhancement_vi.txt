# 2403.15042.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/data-augmentation/2403.15042.pdf
# Kích thước tệp: 655192 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
LLM2LLM: Tăng cường LLM với Phương pháp Cải tiến Dữ liệu Lặp mới
Nicholas Lee∗1Thanakul Wattanawong∗1Sehoon Kim1
Karttikeya Mangalam1Sheng Shen1Gopala Anumanchipalli1
Michael W. Mahoney1,2,3Kurt Keutzer1Amir Gholami1,2
1UC Berkeley2ICSI3LBNL
{nicholas.lee, j.wat, sehoonkim, mangalam, s.sheng, gopala, mahoneymw, keutzer, amirgh}@berkeley.edu
Tóm tắt
Các mô hình ngôn ngữ lớn đã được đào tạo trước (LLM) hiện đang là nghệ thuật tiên tiến nhất để giải quyết phần lớn các tác vụ xử lý ngôn ngữ tự nhiên. Trong khi nhiều ứng dụng thực tế vẫn cần tinh chỉnh để đạt được mức độ hiệu suất thỏa mãn, nhiều trong số chúng ở trong chế độ dữ liệu thấp, khiến việc tinh chỉnh trở nên thách thức. Để giải quyết vấn đề này, chúng tôi đề xuất LLM2LLM, một chiến lược tăng cường dữ liệu có mục tiêu và lặp sử dụng LLM giáo viên để cải thiện một bộ dữ liệu hạt giống nhỏ bằng cách tăng cường dữ liệu bổ sung có thể được sử dụng để tinh chỉnh trên một tác vụ cụ thể. LLM2LLM (1) tinh chỉnh LLM học sinh cơ bản trên dữ liệu hạt giống ban đầu, (2) đánh giá và trích xuất các điểm dữ liệu mà mô hình trả lời sai, và (3) sử dụng LLM giáo viên để tạo ra dữ liệu tổng hợp dựa trên các điểm dữ liệu không chính xác này, sau đó được thêm trở lại vào dữ liệu đào tạo. Phương pháp này khuếch đại tín hiệu từ các điểm dữ liệu được dự đoán không chính xác bởi LLM trong quá trình đào tạo và tái tích hợp chúng vào bộ dữ liệu để tập trung vào các ví dụ thách thức hơn cho LLM. Kết quả của chúng tôi cho thấy LLM2LLM cải thiện đáng kể hiệu suất của LLM trong chế độ dữ liệu thấp, vượt trội hơn cả tinh chỉnh truyền thống và các đường cơ sở tăng cường dữ liệu khác. LLM2LLM giảm sự phụ thuộc vào việc curation dữ liệu tốn nhiều lao động và mở đường cho các giải pháp LLM có thể mở rộng và hiệu suất cao hơn, cho phép chúng ta giải quyết các miền và tác vụ bị hạn chế về dữ liệu. Chúng tôi đạt được cải thiện lên tới 24,2% trên bộ dữ liệu GSM8K, 32,6% trên CaseHOLD, 32,0% trên SNIPS, 52,6% trên TREC và 39,8% trên SST-2 so với tinh chỉnh thông thường trong chế độ dữ liệu thấp sử dụng mô hình học sinh Llama-2-7B. Mã nguồn của chúng tôi có sẵn tại https://github.com/SqueezeAILab/LLM2LLM.

1 Giới thiệu
Các mô hình ngôn ngữ lớn đã được đào tạo trước (LLM) đã đạt được hiệu suất ấn tượng trên nhiều điểm chuẩn và bộ dữ liệu khác nhau mà trước đây đã yêu cầu các kiến trúc mạng nơ-ron chuyên biệt. Đối với nhiều điểm chuẩn tổng quát này (Hendrycks et al., 2020; Zhong et al., 2023), LLM được gợi ý với các hướng dẫn tùy chỉnh hoặc các ví dụ trong ngữ cảnh. Tuy nhiên, trong nhiều ứng dụng thực tế, các chiến lược gợi ý này không phải là giải pháp phù hợp cho tất cả. Ví dụ, LLM có giới hạn về lượng ngữ cảnh đầu vào mà chúng có thể xử lý, do đó hạn chế số lượng ví dụ trong ngữ cảnh hoặc hướng dẫn mà chúng ta có thể nhập vào để làm cho LLM tuân theo một hành vi nhất định. Đối với các tác vụ đơn giản gần gũi với dữ liệu mà LLM đã được đào tạo trước, việc gợi ý mở rộng có thể không cần thiết. Tuy nhiên, việc áp dụng LLM vào các lĩnh vực chuyên biệt (ví dụ, một lĩnh vực y tế cụ thể (Nori et al., 2023) hoặc dữ liệu riêng tư với các giao thức ngách) có thể thách thức hơn, thường yêu cầu các gợi ý dài một cách cấm đoán để đạt được hiệu suất đầy đủ. Ngay cả khi độ dài gợi ý không vượt quá giới hạn, việc xử lý các gợi ý dài làm tăng độ trễ và chi phí của mỗi suy luận. Ngoài ra, LLM cũng có xu hướng quên hoặc bỏ qua thông tin trong ngữ cảnh dài (Liu et al., 2023b), dẫn đến các giảm độ chính xác tiềm tàng ngay cả khi mô hình có thể xử lý các gợi ý đầu vào dài. Trong khi Tạo sinh Tăng cường Truy xuất (RAG) (Lewis et al., 2020) đã được phát triển để giải quyết một số thách thức này, đôi khi nó có thể truy xuất các đoạn văn hoặc tài liệu không liên quan, có thể làm suy giảm hiệu suất tạo sinh. Hơn nữa, RAG không nhất thiết giải quyết vấn đề độ trễ và chi phí vì việc xử lý một gợi ý đầu vào dài vẫn có thể được yêu cầu.

Một phương pháp hứa hẹn để giải quyết vấn đề này là tinh chỉnh. Với sự xuất hiện của Tinh chỉnh Hiệu quả Tham số (PEFT) (Hu et al., 2021; Mangrulkar et al., 2022), các tài nguyên tính toán cần thiết để tinh chỉnh một LLM cụ thể cho tác vụ đã giảm đáng kể. Tuy nhiên, đây là một vấn đề mới: tinh chỉnh thành công đòi hỏi đủ dữ liệu đào tạo. Điều này có thể thách thức đối với một số ứng dụng, nơi chúng ta chỉ có quyền truy cập vào một lượng nhỏ dữ liệu cụ thể cho tác vụ. Thường thì, việc thu thập, làm sạch và gán nhãn dữ liệu bổ sung có thể tốn kém và mất thời gian. Vậy câu hỏi chính là: chúng ta nên tăng dữ liệu đào tạo của người dùng như thế nào để đủ cho việc tinh chỉnh?

Tăng cường dữ liệu là một phương pháp đã biết có thể giúp mở rộng bộ dữ liệu đào tạo một cách hiệu quả. Đối với các tác vụ xử lý ngôn ngữ tự nhiên (NLP), người ta có thể sử dụng các phương pháp như thay thế từ đồng nghĩa, thay thế ký tự (ví dụ, bằng cách cố ý đưa ra lỗi chính tả), hoán đổi ngẫu nhiên và dịch ngược, chỉ kể một vài (Wei và Zou, 2019; Belinkov và Bisk, 2017; Coulombe, 2018; Zhang et al., 2018). Tuy nhiên, các phương pháp này không thành công trong việc mở rộng dữ liệu đào tạo một cách hiệu quả cho việc tinh chỉnh LLM trong trường hợp các tác vụ mới và chuyên biệt, như chúng tôi sẽ trình bày sau trong Phần 4.3.

Để giải quyết vấn đề này, một số bài báo gần đây đã khám phá việc sử dụng LLM để mở rộng bộ dữ liệu tinh chỉnh (Dai et al., 2023; Kumar et al., 2020; Zhou et al., 2023; Chen et al., 2023; Cao et al., 2023; Wei et al., 2023; Zhu et al., 2023). Phương pháp này đã được chứng minh là hiệu quả hơn các phương pháp tăng cường dữ liệu truyền thống. Tuy nhiên, các phương pháp này thường áp dụng tăng cường dữ liệu dựa trên LLM trên tất cả bộ dữ liệu đào tạo có sẵn, mà không xem xét độ chính xác dự đoán của LLM trên các điểm dữ liệu đào tạo cá nhân. Chúng tôi đã quan sát thấy rằng đối với nhiều tác vụ lý luận như số học và hiểu đọc, LLM giải đúng các ví dụ đơn giản hơn trong bộ dữ liệu tinh chỉnh, nhưng có thể gặp khó khăn với các ví dụ khó hơn. Sẽ không tối ưu nếu tiếp tục tăng cường các điểm dữ liệu mà LLM đã đạt được độ chính xác cao.

Để giải quyết những thách thức này, chúng tôi giới thiệu LLM2LLM, một khung tăng cường dữ liệu có mục tiêu và lặp mới sử dụng LLM giáo viên để mở rộng bộ dữ liệu đào tạo, với một phương pháp có mục tiêu và lặp. Cụ thể hơn, chúng tôi đóng góp như sau:

• Chúng tôi đề xuất LLM2LLM, một kỹ thuật tăng cường dữ liệu dựa trên LLM có mục tiêu và lặp mà tăng cường các bộ dữ liệu cụ thể cho tác vụ nhỏ một cách hiệu quả và hiệu suất. LLM2LLM đạt được điều này bằng cách (1) tinh chỉnh LLM học sinh trên bộ dữ liệu ban đầu, (2) đánh giá trên dữ liệu đào tạo và trích xuất các điểm dữ liệu mà mô hình trả lời sai sau khi đào tạo, và (3) sử dụng tăng cường dữ liệu theo phong cách Self-Instruct (Wang et al., 2023) để tăng cường các điểm dữ liệu này, sau đó được thêm trở lại vào dữ liệu đào tạo (Phần 3.1).

• Chúng tôi đánh giá LLM2LLM trên các tập con được lấy mẫu ngẫu nhiên từ GSM8K (Cobbe et al., 2021), CaseHOLD (Zheng et al., 2021), SNIPS (Coucke et al., 2018), TREC (Li và Roth, 2002) và SST-2 (Socher et al., 2013) để đánh giá hiệu quả của phương pháp trong chế độ dữ liệu thấp (Phần 4.2). Ở đây, chúng tôi đạt được cải thiện lên tới 24,2% trên GSM8K, 32,6% trên CaseHOLD, 32,0% trên SNIPS, 52,6% trên TREC và 39,8% trên SST-2 (Bảng 1).

• Chúng tôi thực hiện một loạt các nghiên cứu khử so sánh LLM2LLM với một số đường cơ sở hiện có cũng như với các biến thể của LLM2LLM để đánh giá hiệu quả của các quyết định thiết kế của chúng tôi (Phần 4.5). Chúng tôi quan sát thấy rằng cả tính chất lặp và có mục tiêu của LLM2LLM đều quan trọng để cải thiện hiệu suất mô hình.

--- TRANG 2 ---
2 Bối cảnh và Công trình Liên quan

2.1 LLM Tuân theo Hướng dẫn
Các công trình sớm nhất (Wei et al., 2021; Longpre et al., 2023; Chung et al., 2022; Aribandi et al., 2021; Sanh et al., 2021; Muennighoff et al., 2023; Wang et al., 2022b; Mishra et al., 2022; Wang et al., 2022a; Xu et al., 2022) trong tinh chỉnh hướng dẫn liên quan đến việc thu thập và xử lý các bộ dữ liệu NLP hiện có khác nhau để cải thiện hiệu suất của LLM trên một phạm vi rộng các tác vụ. Self-Instruct (Wang et al., 2023) loại bỏ sự phụ thuộc vào các bộ dữ liệu hiện có bằng cách giới thiệu một khung để bootstrapping các bộ dữ liệu hướng dẫn với các đầu ra của chính mô hình. Các công trình tiếp theo (Ouyang et al., 2022; Taori et al., 2023; Geng et al., 2023; Chiang et al., 2023; Xu et al., 2023; Mukherjee et al., 2023; Mitra et al., 2023; Kang et al., 2023; Nori et al., 2023) tận dụng các mô hình mạnh hơn (Achiam et al., 2023; Touvron et al., 2023a,b) để tinh chỉnh các mô hình tuân theo hướng dẫn có mục đích chung mạnh hơn.

2.2 LLM Tự cải thiện
Nhiều công trình ban đầu (Zelikman et al., 2023; Haluptzok et al., 2023; Zelikman et al., 2022; Madaan et al., 2023; Gulcehre et al., 2023; Singh et al., 2023) khám phá việc sử dụng tự cải thiện để tinh chỉnh LLM. Các công trình này thường lọc các đầu ra của mô hình trước khi tinh chỉnh nó trên các đầu ra của chính nó. LLM2LLM khác với các phương pháp này, vì chúng tôi không trực tiếp tinh chỉnh trên các đầu ra của mô hình của chúng tôi, và chúng tôi sử dụng một mô hình giáo viên để cung cấp phản hồi dưới dạng dữ liệu tổng hợp.

Đồng thời với công trình của chúng tôi, một số bài báo gần đây đã được công bố sử dụng phương pháp lặp để cải thiện LLM (Chen et al., 2024; Anil et al., 2023; Burns et al., 2023; Li et al., 2023b; Yuan et al., 2024). Các công trình này kết hợp ý tưởng từ Học tăng cường (RL) và Tự chơi (Samuel, 2000; Tesauro et al., 1995) để lặp lại xây dựng LLM mạnh hơn bằng cách tinh chỉnh trên các đầu ra của chính mô hình. LLM2LLM được phân biệt bằng cách nó tập trung vào chế độ dữ liệu thấp cho tinh chỉnh cụ thể tác vụ của LLM trong khi những công trình khác đang cố gắng tạo ra LLM có mục đích chung mạnh hơn. Ngoài ra, kỹ thuật của chúng tôi sử dụng độc quyền các điểm dữ liệu mà mô hình trả lời sai trong quá trình đào tạo, và nó sử dụng mô hình giáo viên để tăng cường các điểm dữ liệu này. Thay vì cung cấp phản hồi dưới dạng phê bình hoặc lý luận, phản hồi của mô hình giáo viên chỉ dưới dạng các điểm dữ liệu tổng hợp, điều này đơn giản hóa quy trình đào tạo.

2.3 Tăng cường Dữ liệu
Tăng cường dữ liệu đã được nghiên cứu lâu trong NLP. Công trình ban đầu tăng cường ở mức ký tự (Belinkov và Bisk, 2017; Coulombe, 2018) và từ (Wei và Zou, 2019). Đáng chú ý, Tăng cường Dữ liệu Dễ dàng (EDA) (Wei và Zou, 2019) là một phương pháp ban đầu phổ biến sử dụng các tăng cường mức từ: thay thế từ đồng nghĩa, chèn ngẫu nhiên, hoán đổi và xóa để tăng cường dữ liệu cho phân loại văn bản. Chúng tôi giới thiệu độc giả đến (Feng et al., 2021) để có tóm tắt đầy đủ hơn về tăng cường dữ liệu trong NLP.

Một phương pháp mới phổ biến là sử dụng chính LLM để tổng hợp dữ liệu đào tạo mới (Deng et al., 2023a; Prasad et al., 2023; Fu et al., 2023b; Dai et al., 2023; Ubani et al., 2023; Fang et al., 2023; Liu et al., 2023a; Yu et al., 2023; Kumar et al., 2020; Yoo et al., 2021; Wang et al., 2021; Ding et al., 2023; Li et al., 2023a; Liang et al., 2023). Một ví dụ đáng chú ý là AugGPT (Dai et al., 2023), đã sử dụng ChatGPT để diễn đạt lại văn bản để tăng cường các tác vụ phân loại văn bản.

Nhiều kỹ thuật này tạo ra lượng dữ liệu tổng hợp rất lớn. Công trình gần đây (Chen et al., 2023; Cao et al., 2023; Wei et al., 2023; Zhou et al., 2023) phát hiện ra rằng người ta có thể sao chép kết quả tinh chỉnh trên các bộ dữ liệu lớn này với các tập con nhỏ hơn đáng kể.

3 Phương pháp
Chúng tôi giả định rằng chúng ta được cung cấp một mô hình LLM M (ví dụ, GPT-3.5 hoặc Llama-2-7B) đã được đào tạo trước trên một bộ dữ liệu nguồn nào đó (ví dụ, Common Crawl). Mục tiêu là thích ứng M (từ đây gọi là mô hình học sinh) với một miền mục tiêu mới bằng cách sử dụng một bộ dữ liệu hạt giống nhỏ D, trong đó D có thể có các đặc điểm chưa thấy, so với bộ dữ liệu đã được đào tạo trước (ví dụ, một bộ dữ liệu y tế với thuật ngữ cụ thể, hoặc một cơ sở dữ liệu riêng với các đặc điểm cụ thể). Trong trường hợp này, hiệu suất zero-shot hoặc tinh chỉnh của mô hình có thể không thỏa mãn. Trong khi các chiến lược để giải quyết thách thức này đã được khám phá, ví dụ, thông qua các phương pháp học few-shot cải tiến như đã thảo luận trong Phần 2, ở đây chúng tôi tập trung nghiêm ngặt vào việc làm giàu bộ dữ liệu mục tiêu D được cung cấp bằng một LLM. Phương pháp này trực giao với các kỹ thuật đã nêu trên, cung cấp một giải pháp bổ sung có thể được áp dụng cùng với chúng.

Để làm giàu D, AugGPT (Dai et al., 2023) đã giới thiệu một phương pháp hứa hẹn tạo ra dữ liệu tăng cường bổ sung bằng cách áp dụng một LLM được gợi ý cho tất cả các điểm dữ liệu có sẵn trong bộ dữ liệu đào tạo mục tiêu. Tuy nhiên, phương pháp này thiếu sót bằng cách tăng cường dữ liệu một cách không phân biệt mà không xem xét hiệu suất khác nhau của mô hình học sinh trên các điểm dữ liệu khác nhau. Ví dụ, mô hình có thể dễ dàng giải quyết phần lớn bộ dữ liệu, nhưng nó có thể gặp khó khăn với một tập con nhỏ các ví dụ thách thức hơn. Trong trường hợp này, thay vì mở rộng bộ dữ liệu một cách không phân biệt bằng cách sao chép các trường hợp đơn giản hơn, một chiến lược tăng cường tốt hơn sẽ là tạo ra nhiều điểm dữ liệu hơn phù hợp về mặt khái niệm với các ví dụ thách thức này. Điều này là bởi vì phương pháp trước có thể dẫn đến thời gian đào tạo dài hơn mà không có cải thiện hiệu suất đáng chú ý.

Ở đây, chúng tôi đề xuất một công thức tổng quát hơn của một quy trình tăng cường dữ liệu dựa trên LLM giải quyết hạn chế đã nêu trên. Để làm như vậy, chúng tôi xem xét quy trình lặp sau:
Dn+1=f(Mgiáo viên,Mhọc sinh, Dn,···, D0).(1)

Trong Phương trình (1), Mgiáo viên là mô hình giáo viên, Mhọc sinh là mô hình học sinh (có thể được tinh chỉnh trong nhiều lần lặp), n đề cập đến bước thứ n của tăng cường dữ liệu, Dn+1 là bộ dữ liệu đào tạo mới ở lần lặp tiếp theo, và f là thuật toán tạo dữ liệu. Tại mỗi bước, mô hình giáo viên có quyền truy cập vào cách mô hình học sinh hoạt động ở bước thứ n (ví dụ, nhãn đúng/sai, hoặc có thể là phân phối dự đoán cho các mô hình hộp trắng), và dựa trên đó nó có thể chỉnh sửa các điểm dữ liệu đào tạo cho lần lặp tiếp theo.

Lưu ý rằng LLM2LLM khác với chưng cất kiến thức (Hinton et al., 2015). Chưng cất kiến thức thường áp dụng cho các trường hợp mà mô hình giáo viên có độ chính xác cao trên dữ liệu mục tiêu. Ngược lại, trong trường hợp này, có thể mô hình giáo viên cũng hoạt động dưới mức tối ưu trên dữ liệu mục tiêu (ví dụ, trong trường hợp cơ sở dữ liệu riêng, nơi giáo viên thiếu kiến thức cụ thể cho lĩnh vực). Tuy nhiên, nếu mô hình giáo viên có đủ khả năng lý luận để tạo ra các ví dụ tương tự về mặt khái niệm nhưng khác nhau về mặt ngữ nghĩa khi nó được cung cấp cả gợi ý và câu trả lời, thì khung của chúng tôi có thể cải thiện hiệu suất.

Trong LLM2LLM, chúng tôi xem xét một cụ thể hóa cụ thể của Phương trình (1), như đã thảo luận tiếp theo.

3.1 LLM2LLM
Thuật toán đầu cuối của LLM2LLM được trình bày trong Thuật toán 1. Lấy cảm hứng từ Self-Instruct (Wang et al., 2023), chúng tôi sử dụng mô hình giáo viên Mgiáo viên để tạo ra dữ liệu tổng hợp từ các điểm dữ liệu mà mô hình trả lời sai trong quá trình đào tạo để nhắm mục tiêu các khuyết điểm này trong mô hình học sinh. Cụ thể hơn, chúng tôi đầu tiên đào tạo mô hình học sinh cơ bản Mhọc sinh trên dữ liệu mục tiêu được cung cấp D0, và chúng tôi đánh giá hiệu suất của nó (dòng 4-5 của Thuật toán 1). Sau đó chúng tôi lọc kết quả và giữ lại các ví dụ đào tạo không chính xác mà mô hình học sinh gặp khó khăn để trả lời đúng (Ei trong dòng 6). Sau đó mô hình giáo viên được gợi ý để tạo ra các điểm dữ liệu đào tạo bổ sung phù hợp về mặt khái niệm nhưng khác biệt về mặt ngữ nghĩa (dòng 7, xem Phần B.4 để biết chi tiết về gợi ý). Mô hình giáo viên không nhất thiết phải lớn hơn, mặc dù điều đó có thể cải thiện hiệu suất. Yêu cầu chính cho mô hình giáo viên là có khả năng lý luận để có thể tuân theo hướng dẫn tăng cường dữ liệu, và khả năng tạo ra các điểm dữ liệu tương tự như các ví dụ không chính xác.

Quy trình này được minh họa sơ đồ trong Hình 1.

Một quyết định thiết kế tinh tế nhưng quan trọng trong LLM2LLM là chúng tôi chỉ sử dụng các ví dụ từ dữ liệu hạt giống khi gợi ý mô hình giáo viên để tạo ra các điểm dữ liệu bổ sung. Điều này tương tự như Alpaca (Taori et al., 2023), nhưng khác với Evol-Instruct (Xu et al., 2023). Có hai lý do chính cho điều này. Thứ nhất, phương pháp của chúng tôi ngăn ngừa suy giảm dữ liệu từ nhiều lần lặp tăng cường. Các thí nghiệm ban đầu cho thấy rằng trong khi mô hình giáo viên có thể tạo ra các tăng cường chất lượng cao, một số ví dụ chứa lỗi logic. Do đó, việc tăng cường thêm được áp dụng cho các ví dụ này có thể làm lan truyền lỗi, làm suy giảm chất lượng của bộ dữ liệu theo thời gian. Điều này được nêu bật trong các nghiên cứu khử của chúng tôi trong Bảng 4, nơi sử dụng cả dữ liệu hạt giống và dữ liệu tổng hợp để tăng cường dữ liệu dẫn đến sự giảm độ chính xác.

Thứ hai, phương pháp này giới hạn lượng dữ liệu mới được tạo ra tổng thể. Giả sử bộ dữ liệu hạt giống ban đầu có kích thước n, và tại mỗi lần lặp, mô hình học sinh có tỷ lệ pi của bộ dữ liệu đào tạo Di sai, trong đó 0< pi<1. Nếu chúng ta bao gồm dữ liệu tăng cường vào dữ liệu hạt giống để tạo dữ liệu, thì kích thước của bộ dữ liệu Dj tại bước j sẽ là

|Dj|=n∏i=0^j(1 +pi)≥n(1 +pmin)j.

Điều này có một giới hạn dưới tăng theo cấp số nhân với mỗi bước. Việc giới hạn các câu trả lời sai Wi đầu vào trong quá trình tạo bộ dữ liệu chỉ với dữ liệu từ dữ liệu hạt giống ban đầu cho phép chúng tôi giới hạn tổng số điểm dữ liệu đào tạo thành

|Dj|=n+∑i=0^j npi≤n(1 +jpmax),

có giới hạn trên tăng tuyến tính với số bước. Các đánh giá thực nghiệm được trình bày trong Phần 4.5.2 (Bảng 4) xác nhận điều này.

--- TRANG 3 ---
[Hình 1 được mô tả trong văn bản]

4 Kết quả

4.1 Thiết lập Thí nghiệm
Để đánh giá hiệu suất của LLM2LLM, chúng tôi áp dụng khung của chúng tôi để tinh chỉnh Llama-2-7B trên nhiều bộ dữ liệu mục tiêu khác nhau, bao gồm GSM8K (Cobbe et al., 2021), CaseHOLD (Nori et al., 2023), SNIPS (Coucke et al., 2018), TREC (Li và Roth, 2002) và SST-2 (Socher et al., 2013). Chúng tôi lấy mẫu con các bộ dữ liệu này với các tỷ lệ lấy mẫu khác nhau từ 0,02% đến 50% để đánh giá hiệu suất trong các chế độ dữ liệu thấp khác nhau. Mô hình giáo viên của chúng tôi cho các kết quả này là GPT-3.5 (phiên bản 1106) trừ khi được chỉ định khác. Chúng tôi đã xem xét một số mô hình giáo viên khác, bao gồm GPT-3.5, GPT-4-Turbo, Llama-2-70B (Touvron et al., 2023b), và Airoboros-L2-70B (Durbin, 2023) trong Phần 4.4. Chúng tôi bao gồm thiết lập thí nghiệm chi tiết hơn trong Phần A. Ngoài ra, chúng tôi đã thực hiện các thí nghiệm bổ sung (Phần B.6) để đảm bảo rằng dữ liệu tăng cường từ các mô hình giáo viên khác biệt với bộ dữ liệu thử nghiệm được sử dụng để đánh giá độ chính xác mô hình cuối cùng. Điều này giải quyết vấn đề rò rỉ dữ liệu thử nghiệm tiềm năng có thể xảy ra nếu mô hình giáo viên đã được đào tạo trên dữ liệu tương tự.

4.2 Kết quả Chính
Ở đây, chúng tôi thảo luận về hiệu suất của LLM2LLM với lượng dữ liệu đào tạo khác nhau bằng cách trình bày kết quả tinh chỉnh Llama-2-7B trên GSM8K sử dụng GPT-3.5 làm mô hình giáo viên. Sau đó chúng tôi thảo luận về cách các xu hướng này mở rộng sang các bộ dữ liệu khác nhau (Bảng 1).

Độ chính xác mô hình cuối cùng sau khi áp dụng 10 lần lặp của LLM2LLM được đưa ra trong Bảng 1. Đối với chế độ dữ liệu thấp với 74 ví dụ có sẵn (tức là 1% của bộ dữ liệu đào tạo GSM8K), tinh chỉnh vanilla chỉ đạt được 0,99% độ chính xác thử nghiệm. Tuy nhiên, LLM2LLM tăng độ chính xác lên 19,56% bằng cách lặp lại tạo ra 391 ví dụ bổ sung dựa trên các điểm dữ liệu mà mô hình mắc lỗi. Với dữ liệu có sẵn nhiều hơn một chút là 149 ví dụ hạt giống (tức là 2% của bộ dữ liệu đào tạo) chúng ta có thể đạt được 25,70% độ chính xác. Như được hiển thị trong độ chính xác cơ bản với 20% dữ liệu trong Bảng 2, chúng ta sẽ cần hơn 10× nhiều điểm dữ liệu đào tạo hơn để đạt được độ chính xác này nếu chúng ta chỉ dựa vào tinh chỉnh vanilla. Chúng tôi cũng nhấn mạnh rằng LLM2LLM có thể dẫn đến những lợi ích đáng chú ý với các chế độ đầy đủ dữ liệu (ví dụ, 100% dữ liệu), mặc dù với cải thiện nhỏ hơn so với cơ bản so với các chế độ dữ liệu thấp hơn.

Chúng tôi quan sát một xu hướng tương tự cho CaseHOLD, SNIPS, TREC và SST-2, nơi LLM2LLM giúp cải thiện hiệu suất trong chế độ dữ liệu thấp. Thú vị là, LLM2LLM thường tạo ra tỷ lệ nhiều dữ liệu tăng cường hơn cho GSM8K so với các bộ dữ liệu khác. Điều này là do độ chính xác cơ bản thấp hơn cho GSM8K tổng thể, cho thấy rằng đây là một bộ dữ liệu khó hơn so với những bộ khác. Tuy nhiên, trong tất cả các trường hợp, chúng tôi thấy rằng LLM2LLM giúp phục hồi một mô hình hiệu suất cao.

Trong Hình 2, chúng tôi cũng minh họa cách độ chính xác cơ bản cải thiện trên GSM8K và CaseHOLD với mỗi lần lặp áp dụng LLM2LLM. Chúng ta có thể quan sát sự gia tăng nhanh chóng trong độ chính xác thử nghiệm trong vài lần lặp đầu tiên của LLM2LLM, đặc biệt là trong các chế độ dữ liệu thấp hơn.

4.3 So sánh với Các Phương pháp Tăng cường Khác
Trong Bảng 2, chúng tôi so sánh phương pháp của chúng tôi với các kỹ thuật tăng cường khác, bao gồm EDA (Wei và Zou, 2019) và AugGPT (Dai et al., 2023). Chúng tôi cũng so sánh với việc thêm dữ liệu từ tập đào tạo chưa thấy. Chi tiết về tất cả các phương pháp tăng cường mà chúng tôi sử dụng trong so sánh được cung cấp trong Phần A.3.

Trên GSM8K, LLM2LLM vượt trội hơn tinh chỉnh ngây thơ hơn 20%, EDA hơn 8%, và AugGPT hơn 5%. Tương tự, trên CaseHOLD, LLM2LLM vượt trội hơn cơ bản tinh chỉnh khoảng 35%, EDA 2,3%, và AugGPT 1,1%. Những cải thiện này, đặc biệt là so với AugGPT, có thể được quy cho khả năng của LLM2LLM tạo ra các ví dụ có mục tiêu hơn dựa trên nơi mô hình gặp khó khăn, trái ngược với AugGPT tăng cường dữ liệu một cách không phân biệt. Điều này cho phép sử dụng ngân sách dữ liệu tăng cường hiệu quả và có mục tiêu hơn.

4.4 Lựa chọn Mô hình Giáo viên
Cho đến nay, chúng tôi đã minh họa hiệu suất của LLM2LLM với GPT-3.5 làm mô hình giáo viên, nhưng các LLM khác cũng có thể đóng vai trò này. Một mô hình giáo viên mạnh hơn được kỳ vọng sẽ mang lại tăng cường chất lượng cao hơn và, do đó, độ chính xác cao hơn. Bảng A.1 cho thấy độ chính xác của LLM2LLM với GPT-4-Turbo, Llama-2-70B, và Airoboros-L2-70B làm mô hình giáo viên trên GSM8K. Với 74 ví dụ dữ liệu hạt giống, LLM2LLM chỉ đạt được 11,8% độ chính xác với Llama-2-70B, có thể được đối chiếu với 15,0% với Airoboros và 19,8% với GPT-4-Turbo. Điều này phù hợp với kỳ vọng của chúng tôi, vì lý luận toán học của GPT-4-Turbo được biết là tốt hơn các mô hình khác, thường ngang bằng với GPT-4 (Fu et al., 2023a; Deng et al., 2023b). Phân tích định tính của dữ liệu tăng cường sử dụng các mô hình khác nhau (Hình B.16) hỗ trợ thêm cho điều này, cho thấy các mô hình Llama và Airoboros tạo ra dữ liệu ít đa dạng hơn so với GPT-3.5 hoặc GPT-4-Turbo.

4.5 Nghiên cứu Khử
Ở đây, chúng tôi cung cấp các nghiên cứu khử để chứng minh các quyết định thiết kế mà chúng tôi đã thực hiện trong LLM2LLM.

4.5.1 Tăng cường Lặp so với Tăng cường Một lần
Chúng tôi đầu tiên đánh giá hiệu quả của tăng cường lặp so với thêm tất cả dữ liệu tăng cường cùng một lúc. Để đánh giá điều này, chúng tôi so sánh độ chính xác cuối cùng đạt được bằng cách tăng cường dữ liệu trong 10 lần lặp với việc thêm lượng dữ liệu tương đương trong một lần lặp duy nhất, cho cả bộ dữ liệu GSM8K và CaseHOLD. Như được hiển thị trong Bảng 3, sử dụng một bước tăng cường duy nhất với lượng dữ liệu tăng cường lớn hơn hoạt động kém hơn đáng kể so với thay thế thực hiện 10 bước lặp của LLM2LLM với số lượng tăng cường nhỏ hơn cho mỗi lần lặp. Cụ thể, trên GSM8K, tăng cường một điểm dữ liệu cho mỗi ví dụ trong 10 bước mang lại độ chính xác cao hơn 7,4% so với tăng cường năm điểm dữ liệu cho mỗi ví dụ trong một bước duy nhất. Tương tự, trên CaseHOLD, tăng cường lặp của một điểm dữ liệu cho mỗi ví dụ trong 10 bước dẫn đến cải thiện 4,6% so với tăng cường một lần với bốn điểm dữ liệu cho mỗi ví dụ. Điều này chứng minh phương pháp tăng cường lặp của LLM2LLM tạo ra một điểm dữ liệu cho mỗi ví dụ được trả lời không chính xác.

4.5.2 Tăng cường Dữ liệu với Dữ liệu Hạt giống so với Dữ liệu Tăng cường
Trong mỗi lần lặp, LLM2LLM đánh giá hiệu suất của mô hình học sinh chỉ trên bộ dữ liệu hạt giống ban đầu và tạo ra dữ liệu tăng cường từ các ví dụ hạt giống không chính xác. Tuy nhiên, một thay thế có thể là thực hiện đánh giá và tăng cường dữ liệu sử dụng cả dữ liệu hạt giống và dữ liệu tăng cường trước đó. Cách sau thường dẫn đến hiệu suất dưới tối ưu cũng như số lượng điểm dữ liệu tăng cường tổng thể quá mức, như chúng tôi chứng minh trong Bảng 4. Trên GSM8K, tạo ra dữ liệu tăng cường từ dữ liệu tăng cường lần lặp trước đó mang lại 18,3% độ chính xác, trong khi sử dụng dữ liệu hạt giống để tăng cường thêm cải thiện độ chính xác lên 23,75%. Chúng tôi quan sát xu hướng tương tự cho CaseHOLD. Như đã thảo luận trong Phần 3.1, một lý do tiềm năng cho sự giảm hiệu suất, khi sử dụng dữ liệu tăng cường để tăng cường thêm, liên quan đến sự lệch khỏi phân phối dữ liệu ban đầu.

4.5.3 Tinh chỉnh Từ đầu so với Tinh chỉnh Liên tục
Một quyết định chính khác cho LLM2LLM là có tiếp tục tinh chỉnh từ checkpoint của lần lặp cuối cùng (tức là tinh chỉnh liên tục) hay khởi động lại tinh chỉnh từ mô hình đã được đào tạo trước tại mỗi lần lặp (tức là tinh chỉnh từ đầu). Xem xét bản chất không lồi của mục tiêu tối ưu hóa và các cảnh quan mất mát phức tạp, quyết định này không nhất thiết rõ ràng. Tuy nhiên, như được hiển thị trong Bảng 5, chúng tôi quan sát thấy rằng tinh chỉnh từ đầu luôn vượt trội hơn tinh chỉnh liên tục một cách nhất quán và đáng kể, với cải thiện độ chính xác lên tới 9%. Hiệu suất kém hơn của tinh chỉnh liên tục có thể được quy cho việc overfitting tiềm năng với dữ liệu hạt giống nhỏ trong nhiều lần lặp tinh chỉnh, đặc biệt là trong các chế độ dữ liệu thấp hơn nơi dữ liệu hạt giống nhỏ. Điều này có thể được giảm thiểu bằng cách khởi động lại tinh chỉnh từ đầu trong mỗi lần lặp với dữ liệu tăng cường đủ được thêm vào dữ liệu hạt giống để tạo thành bộ dữ liệu đào tạo.

--- TRANG 4 ---
[Thuật toán 1 và các bảng kết quả được mô tả trong văn bản]

--- TRANG 5 ---
[Hình 2 và các bảng kết quả tiếp tục]

--- TRANG 6 ---
[Các bảng kết quả và phân tích tiếp tục]

--- TRANG 7 ---
[Các bảng kết quả và phân tích tiếp tục]

--- TRANG 8 ---
[Các bảng kết quả và phân tích tiếp tục]

--- TRANG 9 ---
5 Kết luận
Chúng tôi đã giới thiệu LLM2LLM, một khung tăng cường dữ liệu dựa trên LLM thích ứng và lặp sử dụng LLM để mở rộng các bộ dữ liệu tinh chỉnh nhỏ hơn thay vì tạo ra dữ liệu thủ công. Khung này giảm đáng kể lượng dữ liệu thực cần thiết, và nó cho phép chúng ta mở rộng bộ dữ liệu một cách hiệu quả với dữ liệu tổng hợp có thể phù hợp hoặc thậm chí vượt qua hiệu quả của việc thu thập dữ liệu thủ công. Phương pháp này hiệu quả vì bản chất lặp và có mục tiêu của quy trình, cho phép chúng ta tăng cường tín hiệu từ các điểm dữ liệu mà LLM trả lời sai. Kết quả là, chúng tôi có thể đạt được cải thiện 24,2% trên GSM8K, 32,6% trên CaseHOLD, 32,0% trên SNIPS, 52,6% trên TREC, và 39,8% trên bộ dữ liệu SST-2 trong chế độ dữ liệu thấp sử dụng mô hình học sinh Llama-2-7B. Công việc tương lai có thể tập trung vào việc điều chỉnh các siêu tham số của khung của chúng tôi cũng như tích hợp phương pháp của chúng tôi với các kỹ thuật LLM khác như điều chỉnh gợi ý và học few-shot.

Hạn chế
Kết quả của chúng tôi chủ yếu phản ánh các cải thiện xảy ra trong chế độ dữ liệu đào tạo thấp, từ hàng chục ví dụ đến vài nghìn. Tuy nhiên, các thực hành viên có thể xử lý các bộ dữ liệu lớn hơn thỉnh thoảng, trong đó phương pháp của chúng tôi có thể nằm ngoài phạm vi.

Hơn nữa, có thể có các yếu tố khác giúp giải thích sự chênh lệch về hiệu suất giữa các mô hình giáo viên khác nhau. Ngoài ra, chúng tôi đã phân tích dữ liệu được tạo ra để tìm sự khác biệt về chất lượng, nhưng có thể có những cách khác để thu hẹp khoảng cách giữa các mô hình mã nguồn mở và các mô hình GPT như một mô hình giáo viên. Điều này đòi hỏi điều tra thêm.

Trọng tâm của chúng tôi chủ yếu phản ánh một trường hợp sử dụng cụ thể nơi có dữ liệu đào tạo thấp có sẵn do khó khăn trong việc thu thập dữ liệu như hạn chế về lao động hoặc tài nguyên. Khám phá các hiệu ứng của việc sử dụng dữ liệu tổng hợp để tiếp tục cải thiện hiệu suất khi có dữ liệu dồi dào là một hướng nghiên cứu hứa hẹn.

Tuyên bố Đạo đức
LLM2LLM dựa vào việc sử dụng LLM để tăng cường bộ dữ liệu đào tạo để đào tạo một LLM học sinh khác một cách hiệu quả hơn. Điều này có thể giảm chi phí năng lượng và tiền bạc của thử nghiệm và nghiên cứu học máy, vì nó cho phép những người có bộ dữ liệu nhỏ hơn đạt được hiệu suất tốt hơn trên một tác vụ cụ thể cho lĩnh vực. Tất nhiên, việc sử dụng sai phương pháp này có thể dẫn đến dữ liệu không đạo đức được tạo ra, có thể dẫn đến tác hại xã hội. Điều này không phải là mối quan tâm cụ thể cho công việc này, mà cho nghiên cứu LLM nói chung. Hơn nữa, vẫn còn những câu hỏi mở về các thiên kiến ẩn tiềm tàng và các vấn đề đạo đức xung quanh đầu ra được tạo ra của LLM mà các tác giả và thực hành viên của phương pháp này nhận thức được và tiếp tục xem xét trong suốt toàn bộ quy trình.

Lời cảm ơn
Chúng tôi đánh giá cao phản hồi có giá trị từ Andrew Aikawa. Chúng tôi thừa nhận sự hỗ trợ tốt bụng từ đội Furiosa. Chúng tôi cũng đánh giá cao sự hỗ trợ từ Microsoft thông qua Nghiên cứu Mô hình Nền tảng Tăng tốc của họ, bao gồm sự hỗ trợ tuyệt vời từ Sean Kuno. Hơn nữa, chúng tôi đánh giá cao sự hỗ trợ từ Google Cloud, đội Google TRC, và cụ thể là Jonathan Caton, và Giáo sư David Patterson. Phòng thí nghiệm của Giáo sư Keutzer được tài trợ bởi tập đoàn Intel, Intel One-API, đội Intel VLAB, trung tâm xuất sắc Intel One-API, Apple, Samsung, Panasonic, cũng như tài trợ thông qua BDD và BAIR. Chúng tôi đánh giá cao phản hồi và hỗ trợ tuyệt vời từ Ellick Chan, Saurabh Tangri, Andres Rodriguez, và Kittur Ganesh. Sehoon Kim muốn thừa nhận sự hỗ trợ từ Quỹ Nghiên cứu Tiên tiến Hàn Quốc (KFAS). Amir Gholami được hỗ trợ thông qua tài trợ từ Samsung SAIT. Michael W. Mahoney cũng muốn thừa nhận Giải thưởng Nghiên cứu Giảng viên J. P. Morgan Chase cũng như DOE, NSF, và ONR. Kết luận của chúng tôi không nhất thiết phản ánh vị trí hoặc chính sách của các nhà tài trợ, và không nên suy ra sự chứng thực chính thức nào.

--- TRANG 10 ---
[Tài liệu tham khảo tiếp tục]

--- TRANG 11 ---
[Tài liệu tham khảo tiếp tục]

--- TRANG 12 ---
[Tài liệu tham khảo tiếp tục]

--- TRANG 13 ---
[Tài liệu tham khảo tiếp tục]

--- TRANG 14 ---
[Phụ lục A - Thiết lập Thí nghiệm]

--- TRANG 15 ---
[Phụ lục tiếp tục]

--- TRANG 16 ---
[Phụ lục tiếp tục]

--- TRANG 17 ---
[Phụ lục tiếp tục]

--- TRANG 18 ---
[Phụ lục tiếp tục với các ví dụ định dạng]

--- TRANG 19 ---
[Phụ lục tiếp tục với các ví dụ định dạng]

--- TRANG 20 ---
[Phụ lục tiếp tục với các gợi ý hệ thống]

--- TRANG 21 ---
[Phụ lục tiếp tục với các ví dụ trong ngữ cảnh]

--- TRANG 22 ---
[Phụ lục tiếp tục với các gợi ý hệ thống]

--- TRANG 23 ---
[Phụ lục tiếp tục với các ví dụ trong ngữ cảnh]

--- TRANG 24 ---
[Phụ lục tiếp tục với các gợi ý hệ thống]

--- TRANG 25 ---
[Phụ lục tiếp tục với các ví dụ trong ngữ cảnh]

--- TRANG 26 ---
[Phụ lục tiếp tục với các gợi ý hệ thống]

--- TRANG 27 ---
[Phụ lục tiếp tục với các ví dụ trong ngữ cảnh]

--- TRANG 28 ---
[Phụ lục tiếp tục với các gợi ý hệ thống]

--- TRANG 29 ---
[Phụ lục tiếp tục với các ví dụ trong ngữ cảnh và ví dụ so sánh các mô hình]
