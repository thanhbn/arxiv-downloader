# 2305.14890.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/data-augmentation/2305.14890.pdf
# File size: 3328047 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
HARD: Hard Augmentations for Robust Distillation
Arne F. Nix1-2,*, Max F. Burg2-3, Fabian H. Sinz1-2, **
1Institute for Bioinformatics and Medical Informatics, University of Tübingen
2Institute for Computer Science and Campus Institute Data Science, University of Göttingen
3Institute for Theoretical Physics, University of Tübingen
*arne.nix@uni-goettingen.de ,**sinz@cs.uni-goettingen.de
Abstract
Knowledge distillation (KD) is a simple and successful method to transfer knowl-
edge from a teacher to a student model solely based on functional activity. However,
current KD has a few shortcomings: it has recently been shown that this method is
unsuitable to transfer simple inductive biases like shift equivariance, struggles to
transfer out of domain generalization, and optimization time is magnitudes longer
compared to default non-KD model training. To improve these aspects of KD, we
propose Hard Augmentations for Robust Distillation (HARD), a generally applica-
ble data augmentation framework, that generates synthetic data points for which
the teacher and the student disagree. We show in a simple toy example that our
augmentation framework solves the problem of transferring simple equivariances
with KD. We then apply our framework in real-world tasks for a variety of aug-
mentation models, ranging from simple spatial transformations to unconstrained
image manipulations with a pretrained variational autoencoder. We find that our
learned augmentations significantly improve KD performance on in-domain and
out-of-domain evaluation. Moreover, our method outperforms even state-of-the-art
data augmentations and since the augmented training inputs can be visualized,
they offer a qualitative insight into the properties that are transferred from the
teacher to the student. Thus HARD represents a generally applicable, dynamically
optimized data augmentation technique tailored to improve the generalization and
convergence speed of models trained with KD.1
1 Introduction
Knowledge distillation (KD) methods [ 27,37,60] are powerful and flexible tools to transfer the
knowledge of a given teacher model to the transfer target, the student model, without copying the
weights. Instead, these methods match the student’s functional activity (e.g. the softmax output) to
that of the teacher for the presented inputs. Hence, those methods are independent of architectural
details and allow knowledge distillation to be applied in scenarios like model compression [ 7,27],
continual learning [ 4,42,52], or even neuroscience [ 35], where traditional transfer learning would be
impossible to use. KD methods also appear to be key to training new models that trade off inductive
biases for more flexibility and more parameters [ 17,53,55] on smaller data [ 9,40,54]. However,
Nix et al. [40] recently showed that current KD methods fail to transfer even simple equivariances
between teacher and student. Additionally, previous work showed that KD leads to a larger gap
between student and teacher on out-of-domain evaluation performance compared to within domain
performance [ 6,41], even in cases where the student almost perfectly matches the teacher [ 6] (see
Table 5). This phenomenon is especially pronounced for particularly robust teachers [ 41]. Thus we
expect that transferring robustness properties is a difficult problem for KD in general.
1Code available at https://github.com/sinzlab/HARD
Preprint. Under review.arXiv:2305.14890v2  [cs.CV]  25 May 2023

--- PAGE 2 ---
Figure 1: Our task-agnostic HARD framework switches between training the student to match the
teacher and training the augmentor to generate new samples on which the student underperforms
while maintaining high teacher performance. We optimize the augmentor and student in interchanging
phases through a student-teacher loss L˜ s↔˜tand teacher-teacher loss L˜t↔t. We switch between the
two phases by comparing the default loss L˜ son augmented data to pre-defined thresholds.
We hypothesize that KD methods are in principle capable of transferring most knowledge from a
teacher to a student if the training data is chosen adequately. We confirm this hypothesis on a small toy
example (Section 3), showing the importance of input data for KD. Motivated by this demonstration,
we propose our Hard Augmentations for Robust Distillation (HARD) method, a general framework
(Section 4) to generate augmented training inputs which improve knowledge transfer by maximizing
the distance between teacher and student while leaving the teacher’s output unchanged. Consequently,
our framework moves the input in directions that the teacher is invariant to but which are most
challenging for the student. Our experiments (Section 5) show that our task-agnostic framework
improves transfer effectiveness and thereby solves the problem of of KD not being able to transfer
shift equivariance [ 40]. Additionally, as part of our framework, we propose several parameterized
augmentations (Section 4.1) that can be integrated with most existing KD methods and are applicable
to a variety of different computer vision tasks. Finally, we demonstrate across multiple different
models on the tasks of CIFAR10 and ImageNet that our framework learns interpretable augmentations
that improve KD to the same level and in many cases even beyond established data augmentation
methods, even when evaluated in an out-of-domain setting.
2 Related Work
There is a long tradition in using data augmentations to artificially extend training data for deep
learning models and particularly in computer vision, be it through adding Gaussian noise, random
crops, shifts, flips, or rotations [ 18,33]. In recent years, data augmentations became more complex
[12,24,28,39,59,61], employing a multitude of different heuristics with the aim to improve
generalization and in some cases also out-of-domain performance [ 24]. A particularly popular
augmentation method is Mixup [61], which randomly interpolates two input samples and their labels
respectively. Similarly, Cutmix [59] combines two input images by pasting a random crop of one
image on top of the other. Also, many studies use parameterized augmentations optimized to improve
a given objective [ 11,25,48,67], and some even optimize the augmentations to improve on an
adversarial objective [2, 3, 20, 56, 63–65], however, without applying them for knowledge transfer.
In KD, applying data augmentations is a very effective tool to improve matching between student and
teacher [ 6,58] and optimizing on a meta level can be useful to aide the teaching [ 43]. Similar to our
work, Haidar et al. [21], Rashid et al. [45], Zhang et al. [62] utilized adversarial objectives to optimize
data augmentations for KD, however, they were solely focused on natural language processing tasks
and do not optimize the augmentations towards invariance.
Inspired by this large body of work we formulate a task-agnostic framework containing only one
building block that is specific to the data-domain – the instantiation of the augmentor model generating
the augmented data samples – for which we offer a variety of reasonable model choices based on
spatial transformer modules [29], Mixup [61], and variational autoencoders [10, 31, 34].
2

--- PAGE 3 ---
x−202yMSE: 5.08ABaseline
MSE: 3.24BMix-Up
MSE: 1.96CGaussian Noise
MSE: 0.10DHARDTeacher Student Train Data
−10 0 100.00.20.4DensityE
−10 0 10F
−10 0 10G
−10 0 10H
Figure 2: Fitting the student, a three-layer ReLU MLP, to the teacher function, cos(x), for10,000
iterations. We show results for 10 random seeds ( A-D) and the distribution of (augmented) training
inputs as a normalized histogram ( E-H). We compare baseline (no augmentations) with Mixup,
Gaussian noise and an HARD-optimized noise distribution. We report mean-squared-error (MSE) on
100 test inputs sampled from U[−10,10].
3 Input Data Matters for Functional Transfer
We hypothesize that the choice of input data is crucial to successfully knowledge distillation and
we illustrate the impact of training data by a simple toy example. To demonstrate this, consider a
simple KD task in which we instantiate the teacher-model by the true function ft(x) = cos( x)and
the student fs(x)by a three layer Multilayer Perceptron (MLP) with ReLU activation [ 1]. We use
input data xchosen such that it does not capture the teacher’s cos(x)periodicity (orange points in
Figure 2A). Simple KD does neither interpolate between the given training points nor extrapolates
beyond them (Figure 2E). Hence the student neural network does not learn the teacher’s periodicity
and fails to interpolate and extrapolate beyond the training data (Figure 2A).
Augmenting the training data with more helpful inputs ˜xand teacher labels ft(˜x) = cos(˜ x)could
mitigate this problem. One method successfully applied to KD [ 6] is to extend the input data through
Mixup [ 61]. When applying this to our illustrative example, we create new training inputs ˜xthrough
linear interpolation between pairs of input points ˜x= (1−α)x1+αx2(Figure 2F), and recording
the corresponding teacher responses ft(˜x) = cos(˜ x). Thus, the student learns to interpolate between
training points, but mixup does not enhance extrapolation (Figure 2B).
To generate datapoints that would interpolate and extrapolate beyond already available training points,
we could simply augment by adding Gaussian noise ϵto the available data points, ˜x=x+ϵ, hence
interpolating and extrapolating beyond the training data (Figure 2G). This strategy helps our student
to match the teacher also outside the original training regime (Figure 2C). However, the student only
improves within a fixed margin that is determined by the noise distribution’s mean and variance.
We could obviously improve interpolation and extrapolation by increasing the noise distribution’s
variance or shifting it’s mean, however, as we move to a high dimensional image input space
(x∈R→⃗ x∈RN) it becomes unclear how to heuristically select helpful new samples and at the
same time random exploration strategies become computationally infeasible. Instead, we propose to
optimize a parameterized augmentation to efficiently generate new, hard training samples on which
the student lacks performance, as here the student could improve the most. In our toy example,
we illustrate this by optimizing the Gaussian’s parameters (mean and variance) according to our
augmentation framework HARD , which we will present in the next section. This provides us with
a noise distribution which we use to draw new helpful training examples ˜xthat transfer inter- and
extrapolation to the student network (Figure 2D,H). Overall, this toy example shows that learning
hard augmentations to select new helpful data points is crucial to efficiently improve extrapolation
beyond the training distribution.
3

--- PAGE 4 ---
4 Learning Hard Augmentations for Robust Distillation (HARD)
Our task-agnostic HARD framework learns augmenting training images to most efficiently help
knowledge-transfer from a teacher to a student model. Our method requires three main components:
a teacher model with frozen parameters, a student model that should learn knowledge from the
teacher, and a parameterized augmentation model that learns to augment images such that most of the
teacher’s knowledge is transferred to the student.
In classical KD methods[ 27], the objective is to minimize a distance D[fs(x), ft(x)]between the
student’s activation fs(x)and the teacher’s activation ft(x)on given inputs x∈Rn. Usually,
this would be the Kullback-Leibler divergence between the softmax distributions of teacher and
student. Unfortunately, only considering training data could miss properties of the teacher (eg. shift
invariance) that might be crucial for generalization (see Section 3 for an illustrative example). To
resolve this issue, we learn a parametrized augmentation model gato generate new input data points
˜x=ga(x)transferring such invariance properties from the teacher to the student. Hence, we define a
teacher-student loss considering the more general case of matching student and teacher on augmented
inputs ˜x∈Rn:
L˜ s↔˜t=D[fs(˜x), ft(˜x)]. (1)
To specifically transfer the teacher’s invariance properties to the student, we propose a teacher-teacher
losspushing the augmentor towards generating data points on which the teacher is invariant,
L˜t↔t=D[ft(˜x), ft(x)], (2)
as these are often useful augmentations for generalization. Using both of these losses, we optimize
the augmentor’s parameters θato generate augmented samples on which the teacher results in similar
activations but the student differs from them (Figure 1 top) and simultaneously we optimize the
student’s parameters θsto perform well on those augmentations (Figure 1 bottom):
max
θaλsL˜ s↔˜t−λtL˜t↔t and min
θsL˜ s↔˜t. (3)
Here, λsandλttrade off the loss terms and are treated as hyper-parameters. We train both compo-
nents separately switching from training the augmentor to training the student when the student’s
performance on augmented data gets worse than a pre-defined threshold ( L˜ s> ℓmax) and we switch
back from student to augmentor training when the student’s performance on augmented data sur-
passes a pre-defined threshold ( L˜ s< ℓmin; Figure 1). To prevent catastrophic forgetting, we save
augmentors at every switch and employ an augmentor randomly chosen out of the set of previously
saved augmentors in each iteration when training the student.
4.1 The augmentor models
To generate new input data points it is important to choose an augmentor that suits the desired
application and is powerful enough to generate useful augmentations. Usually, we do not know a
priori what useful augmentations are and thus should try to allow as much flexibility as possible.
Additionally, some variance over augmentations could benefit the transfer. Thus, all augmentors in
our study introduce randomness in the model by adding Gaussian noise into the computation of the
augmentation through the reparametrization trick [ 31]. While our framework is universally applicable
across domains, choosing an effective augmentation model likely needs to be addressed for each task
individually. In our experiments, we use the following augmentor models:
HARD-Affine In the simplest model, we limit the augmentations to affine transformations of the
coordinate grid of pixel locations, i.e. shifts, rotations, scalings, and shears of images. Models
implementing such transformations are known as spatial transformers [29]. We leverage this model
for our augmentor by learning a distribution over the entries of an affine transformation matrix
ϑ∈R2×3that defines the transformation of the sampling grid, i.e. a transformation that maps the
pixel positions from the original image to the augmented image (Figure 3A).
HARD-Mix Additionally we consider a slightly more complex augmentor model, which is an
adaptive variant of the commonly used Mixup [ 61] and Cutmix [ 59] augmentations. However, instead
of randomly sampling the ratio and cutout position that are used to combine images, we learn how
4

--- PAGE 5 ---
Learned
Noise  
DistributionP ositional
Encoding
Learned
Quer y
Distribution
T r ansform
T r ansformSoftmax
Learned
T r ansform
DistributionBA
C
Figure 3: Illustration of the augmentor models used in our experiments. (A)HARD-Mix: Image-
dependent patch-wise interpolation of multiple images. (B)HARD-Affine: Learned distribution of
affine transformations in the pixel coordinates. (C)HARD-V AE: Finetuning (parts of) a pretrained
V AE.
to combine the images dependent on the input images. We achieve this by performing a patch-wise
projection of the input image, followed by comparing each patch with the same query vector sampled
from a learned distribution (Figure 3B). We normalize similarities for each patch over each group
of images and use the resulting weights to combine the original image patches, giving a combined
image. This mechanism allows our augmentor to decide which features of which image are shown
to the student, enabling it to explore the interpolated space between images systematically, instead
of randomly. As it would not make sense for the teacher to be invariant to an interpolation as it is
generated by HARD-Mix, we do not consider the teacher-teacher-loss L˜t↔tin this case and optimize
student and augmentor jointly instead.
HARD-V AE To lift constraints further, we wanted to use a more powerful augmentor that could
generate a large variety of images across the entire image-space. As the augmentor has to generate new
samples on-the-fly during the student training, the generation process needs to be very fast, limiting
the choice of useful generative models. For this reason, we focus on variants of the variational
autoencoder architecture [ 31], allowing for good image reconstructions which can be achieved
reasonably fast in a single forward pass (Figure 3D). For CIFAR, we choose the very deep VAE [10]
model, which we finetune by solely optimizing parameters of the posterior network from layer 10
onward in the decoder. For the experiments on ImageNet, we use a Residual-Quantized V AE (RQ-
V AE) [ 34] pretrained on ImageNet, which we finetune in its entirety and add a noise vector on the
latent state. Hence, as training progresses, the model changes from generating plain reconstructions
of a given image to input conditioned generations that serve as our augmentations.
5 Experiments
5.1 Transferring equivariance
For our initial experiment, we reproduce the setup from Nix et al. [40] to test whether we can transfer
the inductive bias from a shift equivariant teacher, CNN and ResNet18 [ 22], to a student that does
not have this inductive bias built into its architecture: a Multi-Layer Perceptron (MLP) and a Vision
Transformer (ViT) [ 17]. When training the students and teachers by themselves on standard MNIST
[15] training data, we observe a small drop of generalization performance (-0.6% and -1.2%) between
5

--- PAGE 6 ---
Table 1: MNIST (columns “Centered”) and MNIST-C (columns “Shifted”) test accuracies (mean
and standard error of the mean across 4 random seeds) comparing KD without augmentation and
our HARD-Affine method to Orbit transfer [ 40], which also learns and transfers equivariances. The
left two columns show the transfer results from a small CNN teacher to a MLP student. The right
columns show analogous experiments between a ResNet18 teacher and a small ViT student. The
best performing transfer is shown in bold for each column. Examples of our HARD-Affine learned
data augmentations shown on the right. We include the controls Random Affine andMNIST-C Shifts
(marked by italics).
CNN→MLP ResNet18 →ViT
Method Centered Shifted Centered Shifted
Teacher only 99.0 ±0.0 91.3±0.5 99.5±0.0 92.8±0.5
Student only 98.4 ±0.0 35.2±0.7 98.3±0.0 40.4±0.8
+ Random Affine 92.1 ±0.6 81.0±2.0 95.4±0.3 90.4±1.0
+ MNIST-C Shifts 98.1 ±0.1 86.5±0.3 98.5±0.0 93.7±0.2
Orbit [40] 98.8 95.2 98.4 84.0
KD 98.6 ±0.0 40.3±0.6 98.6±0.1 44.7±1.9
+ HARD
 -Affine 98.6 ±0.1 68.9±2.5 99.2±0.0 84.1±2.3
teacher and student on the MNIST test set and a large gap (-56.1% and -52.4%) when we evaluate on
a version of the test set in which digits were randomly shifted [ 38]. As another baseline, we applied
plain KD to transfer shift equivariance from teacher to student. Consistent with the findings of Nix
et al. [40], we only observe a small improvement on the centered (+0.2% and +0.3%) and the shifted
(+5.1% and +4.3%) test sets, which likely result from the centered training data we use for transfer.
We then test if combining KD with our augmentations produced by HARD-Affine would outperform
these baselines. The resulting student model improves significantly on shifted inputs (+28.6% and
+39.4%) compared to plain KD and the generated images clearly show that the augmentor learns to
shift the digits within the image. Compared to Nix et al. [40] our approach outperforms their results
on the ViT task but, while improving the out-of-domain generalization by 28.6% over baseline, stays
behind the Orbit performance on the MLP task. This demonstrates that our method while acting on
fewer parts of the network compared to Orbit and while being a more general method, can improve or
reach better performance when it comes to transferring invariances, and can be generalized to bigger
datasets, as we show below.
We verify that the student’s performance improvement is specifically due to our data generation
framework in two control experiments. The first experiment (Random Affine) augments the training
inputs of a stand-alone student model with a random affine transformation akin to our augmentor
model, but using transformation parameters sampled uniformly from a pre-defined, reasonably
constrained range (i.e. ensuring the digit is always fully visible). This student performs well on the
shifted test set, however, performance significantly degrades on the centered test set. In comparison,
our HARD-Affine model is unconstrained and learns more useful augmentations, leading to better
performance on the centered test sets.
In our second control (Shifts) we asked how much data augmentation could improve the performance
in the best case (without KD). For this, we augment the inputs by the same random shifts that were
applied to obtain the shifted test data, leading to great improvements on the shifted test set. However,
our learned augmentations achieve scores in a similar range on the shifted evaluation and outperform
its results on the centered test set.
5.2 Transfer on natural images
After demonstrating that our method successfully captures the difference between teacher and student
and bridges a gap in inductive bias, we now want to test whether this effect holds up in more realistic
scenarios.
6

--- PAGE 7 ---
Table 2: Test accuracies on the CIFAR10 test set. Standard error of the mean is reported where
available across three different seeds. Best transfer is highlighted in bold. The ResNet101∗models
were pretrained on ImageNet. Examples of augmented test images from ResNet18 →ViT experiments
with samples across different iterations are shown to the right.
ResNet18
↓
ViTResNet101∗
↓
ViTResNet101∗
↓
ResNet18
Teacher only 92.5 ±0.0 95.5 95.5
Student only 68.5 ±0.5 68.5±0.5 78.5
+ Standard Aug. 78.3 ±0.4 78.3±0.4 92.6
+ Random Affine Aug. 58.9 ±0.4 58.9±0.4 79.3
KD 67.9 ±0.1 68.5 84.4
+ Standard Aug. 80.9 ±0.1 79.3 93.3
+ HARD
 -Affine 87.8±0.8 84.4 93.5
+ HARD
 -V AE 81.9 ±0.4 81.2 91.0
+ HARD
 -V AE-Affine 87.6±0.6 87.1 94.0
Original -Afﬁne
 -V AE -V AE-Af f.
CIFAR experiments We begin by applying our framework to CIFAR10 [ 32] on three different
KD scenarios (see Table 2). Specifically, we test scenarios where the student lacks an inductive
bias (ResNet18 →ViT), where the teacher has more capacity and access to data than the student
(ResNet101∗→ResNet18), and to scenarios combining both properties (ResNet101∗→ViT). For all
experiments, we keep the experimental setup as close to our previous MNIST experiments as possible
(see Appendix A for details).
We start by establishing baselines by training only the teacher and only the student models on the data
and evaluating default KD. We observe that on this small data set a small ResNet18 performs better
(78.5% accuracy) than a larger ViT (68.5%), likely because of the ResNet’s superior inductive bias on
this task and small data set. Next, we find that adding default data augmentations (random rotations,
cropping, horizontal flips) to the student baselines significantly boosts performance to 92.6% and
78.3% for the ResNet18 and ViT, respectively. Adding these default augmentations to typical KD
leads to a great performance boost, too (see Table 2).
Given that adding default data augmentation to KD already leads to a substantial performance
boost, it is particularly noteworthy that the data augmentations learned by HARD-Affine outperform
this baseline for the ViT. Qualitatively, the augmented images exhibit a large variety of spatial
transformations, suggesting that a difference in these examples lead to the observed performance
boost (Table 2, right).
We then investigated performance of our HARD-V AE augmentation strategy and found performance
improvement over the KD + standard augmentations baseline for transfer to the ViT (+1.0% and
+1.9%) student. However, inspecting the augmented images indicates that our augmentor lacks
the expected shifts of object positions, but rather learns stylistic changes in the image (Table 2,
right). This motivated us to combine HARD-Affine and HARD-V AE augmentation resulting in best
performance (up to +7.8%) for all teacher-student pairings (HARD-V AE-Affine in Table 2) and the
resulting images demonstrate variability in both style and spatial alignment (Table 2, right).
ImageNet experiments Having established our methods’ performance for CIFAR10, we extend
our results to classification on ImageNet [ 14]. Here we aim to distill a ResNet50 [ 22] teacher, trained
with Deep-augment and AugMix data augmentations [ 25], into a smaller ResNet18 and ViT-S (small
vision transformer variant) [ 17] that we want to be particularly robust to natural image corruptions,.
The distillation into ResNet18 allows us to investigate the capability for model compression, because
ResNet18 is a smaller network compared to ResNet50, but with a similar architecture. Distillation
into a ViT-S architecture with a patch-size of 14 tests additionally if KD transfers the ResNet50’s
inductive bias of shift equivariance on a larger dataset.
We evaluate on common test sets for both in-domain (ID) [ 5,46] and out-of-domain (OOD) [ 19,
23,25,26,57] generalization performance (Tables 3 and 4, respectively). To properly investigate
the extrapolation abilities of KD training, we trained a strong KD baseline by applying several
7

--- PAGE 8 ---
ResNet50 →ResNet18 ResNet50 →ViT-S
Val ReaL V2 Val ReaL V2
Teacher 75.8 83.1 63.7 75.8 83.1 63.7
Student 70.7 78.1 57.4 73.2 79.4 60.3
KD 70.7 78.7 58.1 75.3 82.8 62.9
+ HARD
 -Affine 71.6 79.5 58.6 74.9 82.3 62.4
+ HARD
 -Mix 71.4 79.4 58.6 75.7 83.0 63.3
+ HARD
 -V AE 71.0 78.9 58.7 75.8 83.1 63.5Table 3: In-domain
evaluation for ImageNet:
reporting Top-1 accuracy in
% on ImageNet-Validaton
[14], ImageNet-ReaL [ 5]
and ImageNet-V2 [ 46]
with KD from a robust
ResNet50 [ 25] teacher to
ResNet18 (columns 2-4)
and ViT-S (columns 5-7)
students.
Table 4: In-domain evaluation for ImageNet: Reporting Top-1 accuracy in % on ImageNet-A [ 26],
ImageNet-R [ 25], ImageNet-Sketch [ 57] and ImageNet-Style [ 19] and mean-corruption-error on
ImageNet-C (lower is better) [23].
ResNet50 →ResNet18 ResNet50 →ViT-S
Im-A Im-R Im-C ↓Sketch Style Im-A Im-R Im-C ↓Sketch Style
Teacher 3.8 46.8 53.0 32.6 21.2 3.8 46.8 53.0 32.6 21.2
Student 1.6 30.0 88.1 18.4 4.4 8.0 26.3 78.1 13.8 6.6
KD 1.6 40.2 69.2 26.0 13.4 3.3 45.0 56.8 29.6 18.7
+ HARD
 -Affine 1.5 38.2 73.1 24.9 10.4 3.4 40.8 62.2 26.2 14.5
+ HARD
 -Mix 1.8 39.9 68.8 26.1 13.7 3.5 45.4 56.2 29.9 19.2
+ HARD
 -V AE 1.7 39.5 72.5 25.8 12.1 3.4 45.4 57.4 30.7 18.1
data augmentations: we randomly switch between Cutmix [ 59] and Mixup [ 61], each drawing their
interpolation weight from a β-distribution with α= 1, as well as AugMix [ 24] augmentations. For the
standalone student training, we additionally apply various lighter data augmentations (Cutmix with
α= 1, Mixup with α= 0.1, and Trivialaugment [39]). Since we ask how KD can be improved in a
setting of limited resources, we run our experiments an order of magnitude shorter than proposed for
the state-of-the-art in KD [ 6] (200 epochs for all ResNet18 and 150 epochs for all ViT-S experiments).
For student and KD models, we perform a small grid search over learning-rate and weight-decay
hyperparameters. We then train the models with our HARD framework based on the hyperparameters
of our best performing KD setting. The augmentor-specific settings are selected through a small
grid-search in the ResNet18 setting (for details see Appendix A).
We first evaluate the ID performance of our methods (Table 3) beginning with the standalone teacher
and student baselines, which reveal a larger performance gap between the ResNet18 student and the
ResNet50 teacher compared to the ViT-S student (5.1% and 2.6% on the ImageNet validation set,
respectively). Plain KD significantly reduces this gap for the ViT-S (+2.1% performance improvement
compared to standalone). For the ResNet18 student KD achieves only small (0.7% V2) improvements
or no improvements (0.0% Val), even though the initial gap between teacher and student is larger.
Applying HARD-Affine, HARD-Mix and HARD-V AE augmentation on this task improves over plain
KD across most augmentation models and test sets with student performance gains of up to 0.9%
for ResNet18 (HARD-Affine) and 0.6% for ViT-S (HARD-V AE). For ViT-S, our best-performing
HARD-V AE method even matches the teacher’s performance on 2 out of 3 test sets.
For the OOD setting (Table 4), we observe that the initial gap between student and teacher is larger
than on ID data across all data sets (up to 35.1% difference), except for Im-A in the ViT-S setting. The
aggressive data augmentations we apply for the plain KD baseline favor OOD performance, hence
it is expected that plain KD results in good performance improvement over the standalone baseline
(up to 21.3% imporvement on Im-C). All three HARD approaches transfer some of the teacher’s
generalization abilities leading to improvements on a number of students and data sets, however,
HARD-Affine fails to reach the KD performance in both settings and HARD-V AE underperforms for
the ResNet18 student in these OOD scenarios. However, HARD-Mix and HARD-V AE (for ViT-S)
outperform plain KD on several test sets and are roughly on par on all others, across the board. Given
8

--- PAGE 9 ---
that we chose a very strong baseline by applying aggressive state-of-the-art data augmentations we
find these results especially encouraging.
6 Discussion
Original -Afﬁne
 -V AE -Mix
Figure 4: Example augmentations applied to im-
ages of the ImageNet validation set obtained from
augmentor models in the ViT-S setting at the end
of training.Interpretability HARD enables us to gain in-
sight into the distillation mechanism as the aug-
mented images illustrate the knowledge that is
transferred (Figure 4). As expected, HARD-
Affine learns to downscale images to shift and
rotate the images such that the object in the im-
age is shown in different places (row 2-4 in Fig-
ure 4) and scales such that the images is cropped
(row 1). As HARD-Mix is a dynamically learn-
able extension of mixup, it either merges two
objects into the same picture (row 1 and 4), es-
pecially if they are not in the same position, or
uses one image to change the style (row 2) or
background (row 3) of another. Finally, HARD-
V AE mostly impacts the style of an image and
additionally adds small distortions to specific
image regions, which is noticable by the altered
image brightness and the blurring of some high-
frequency features.
Limitations and broader impact State-of-
the-art knowledge distillation typically deals
with huge models (billions of parameters) and
incredibly long training times (>9,000 epochs)
[6,13]. In comparison, our study is computa-
tionally lightweight in requiring approximately 400 A100GPU days across all our experiments. We
believe exploring even more flexible augmentor models with a semantically meaningful latent space
as for example diffusion models [ 44,47,49,50] could improve our proposed methods even further.
However, generating a single image with out-of-the-box diffusion models requires multiple seconds.
This is prohibitively long, so leave exploring their usability in our proposed dynamic data augmen-
tation technique for future work. In general, KD allows us to distill smaller models that perform
similar to large foundation models. Improving the distillation process to be more efficient lowers
the barrier of applying KD across labs with various compute budget and decreases environmental
impact. At the same time, transferring generalization abilities effectively and consistently results in
smaller distilled models that are appealing to use, thus we would expect such smaller models to be
used abundantly hence lowering the general carbon footprint for model usage. In conclusion, our
study proposes avenues to efficiently improve KD in terms of performance, efficiency, and hence
environmental impact.
7 Conclusion
In this work we introduced a general, task-agnostic, and modular framework to extend knowledge
distillation by learnable data augmentations. The augmentation models are optimized to generate
inputs on which teacher and student disagree, keeping the teacher’s predictions unchanged at the
same time. We show that these augmentations can solve the issue of KD and transfer equivariance
properties, even in cases where the teacher’s inductive biases are distinct from the student’s. We
further demonstrate that our learned augmentations achieve performance competitive to established
classical data augmentation techniques even when student and teacher share similar inductive biases.
Overall our framework offers a powerful tool that enhances transfer performance and offers a unique
insights into the transferred knowledge through its interpretable augmentations.
9

--- PAGE 10 ---
Acknowledgements
Furthermore, we thank Felix Schlüter for his helpful insights into evaluation problems as well
as Mohammad Bashiri, Pawel Pierzchlewicz and Suhas Shrinivasan for helpful comments and
discussions. The authors thank the International Max Planck Research School for Intelligent Systems
(IMPRS-IS) for supporting Arne Nix and Max F. Burg.
This work was supported by the Cyber Valley Research Fund (CyVy-RF-2019-01), by the Ger-
man Federal Ministry of Education and Research (BMBF) through the Tübingen AI Center (FKZ:
01IS18039A), by the Deutsche Forschungsgemeinschaft (DFG) in the SFB 1233, Robust Vision:
Inference Principles and Neural Mechanisms (TP12), project number: 276693517, and funded by the
Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Project-ID 432680300 –
SFB 1456. FHS is supported by the Carl-Zeiss-Stiftung and acknowledges the support of the DFG
Cluster of Excellence “Machine Learning – New Perspectives for Science”, EXC 2064/1, project
number 390727645.
References
[1]Abien Fred Agarap. Deep learning using rectified linear units (relu). arXiv preprint
arXiv:1803.08375 , 2018.
[2]Anthreas Antoniou, Amos Storkey, and Harrison Edwards. Data Augmentation Generative
Adversarial Networks. feb 2022.
[3]Sima Behpour, Kris M. Kitani, and Brian D. Ziebart. ADA: Adversarial data augmentation for
object detection. Proceedings - 2019 IEEE Winter Conference on Applications of Computer
Vision, WACV 2019 , pages 1243–1252, mar 2019. doi: 10.1109/WACV .2019.00137.
[4]Ari S Benjamin, David Rolnick, and Konrad P Kording. Measuring and regularizing networks
in function space. In 7th International Conference on Learning Representations, ICLR 2019 ,
2019.
[5]Lucas Beyer, Olivier J Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord.
Are we done with imagenet? arXiv preprint arXiv:2006.07159 , 2020.
[6]Lucas Beyer, Xiaohua Zhai, Amélie Royer, Larisa Markeeva, Rohan Anil, and Alexander
Kolesnikov. Knowledge distillation: A good teacher is patient and consistent. jun 2021. URL
https://arxiv.org/abs/2106.05237v1http://arxiv.org/abs/2106.05237 .
[7]Cristian Bucil ˇa, Rich Caruana, and Alexandra Niculescu-Mizil. Model compression. In
Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining , volume 2006, pages 535–541, 2006. ISBN 1595933395. doi: 10.1145/1150402.
1150464.
[8]Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham,
Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, and Quoc V . Le. Symbolic discovery
of optimization algorithms, 2023.
[9]Xianing Chen, Qiong Cao, Yujie Zhong, Jing Zhang, Shenghua Gao, and Dacheng Tao. DearKD:
Data-Efficient Early Knowledge Distillation for Vision Transformers. apr 2022. doi: 10.48550/
arxiv.2204.12997. URL https://arxiv.org/abs/2204.12997v2 .
[10] Rewon Child. Very Deep V AEs Generalize Autoregressive Models and Can Outperform Them
on Images. nov 2020. doi: 10.48550/arxiv.2011.10650. URL https://arxiv.org/abs/
2011.10650v2http://arxiv.org/abs/2011.10650 .
[11] Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V . Le. AutoAugment:
Learning Augmentation Policies from Data. Cvpr 2019 , (Section 3):113–123, may 2018. doi:
10.48550/arxiv.1805.09501. URL https://arxiv.org/abs/1805.09501v3 .
[12] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical
automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition workshops , pages 702–703, 2020.
10

--- PAGE 11 ---
[13] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin
Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Je-
natton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias
Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin F.
Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings,
Mark Patrick Collier, Alexey Gritsenko, Vighnesh Birodkar, Cristina Vasconcelos, Yi Tay,
Thomas Mensink, Alexander Kolesnikov, Filip Paveti ´c, Dustin Tran, Thomas Kipf, Mario
Luˇci´c, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, and Neil Houlsby. Scaling vision
transformers to 22 billion parameters, 2023.
[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
recognition , pages 248–255. Ieee, 2009.
[15] Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE
Signal Processing Magazine , 29(6):141–142, 2012.
[16] Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise
quantization. 9th International Conference on Learning Representations, ICLR , 2022.
[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image
Recognition at Scale. 2020. URL https://github.com/http://arxiv.org/abs/2010.
11929 .
[18] Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A
rotation and a translation suffice: Fooling cnns with simple transformations. 2017.
[19] Robert Geirhos, Claudio Michaelis, Felix A. Wichmann, Patricia Rubisch, Matthias Bethge,
and Wieland Brendel. IMAGENET-TRAINED CNNS ARE BIASED TOWARDS TEXTURE;
INCREASING SHAPE BIAS IMPROVES ACCURACY AND ROBUSTNESS, nov 2018.
ISSN 23318422. URL http://arxiv.org/abs/1811.12231 .
[20] Kehong Gong, Jianfeng Zhang, and Jiashi Feng. PoseAug: A Differentiable Pose Augmentation
Framework for 3D Human Pose Estimation. Proceedings of the IEEE Computer Society
Conference on Computer Vision and Pattern Recognition , pages 8571–8580, may 2021. ISSN
10636919. doi: 10.1109/CVPR46437.2021.00847. URL https://arxiv.org/abs/2105.
02465v1 .
[21] Md Akmal Haidar, Mehdi Rezagholizadeh, Abbas Ghaddar, Khalil Bibi, Philippe Langlais, and
Pascal Poupart. CILDA: Contrastive Data Augmentation using Intermediate Layer Knowledge
Distillation. apr 2022. doi: 10.48550/arxiv.2204.07674. URL https://arxiv.org/abs/
2204.07674v1http://arxiv.org/abs/2204.07674 .
[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for
Image Recognition. Proceedings of the IEEE Computer Society Conference on Computer
Vision and Pattern Recognition , 2016-Decem:770–778, dec 2015. ISSN 10636919. doi:
10.1109/CVPR.2016.90. URL https://arxiv.org/abs/1512.03385v1 .
[23] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common
corruptions and perturbations, mar 2019. ISSN 23318422. URL http://arxiv.org/abs/
1903.12261 .
[24] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshmi-
narayanan. Augmix: A simple data processing method to improve robustness and uncertainty.
arXiv preprint arXiv:1912.02781 , 2019.
[25] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo,
Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and
Justin Gilmer. The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution
Generalization. In Proceedings of the IEEE International Conference on Computer Vision ,
pages 8320–8329, 2021. ISBN 9781665428125. doi: 10.1109/ICCV48922.2021.00823. URL
https://github.com/hendrycks/imagenet-r.
11

--- PAGE 12 ---
[26] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural
adversarial examples. CVPR , 2021.
[27] Geoffrey Hinton and Jeff Dean. Distilling the Knowledge in a Neural Network. Technical
report, 2015.
[28] Philip TG Jackson, Amir Atapour Abarghouei, Stephen Bonner, Toby P Breckon, and Boguslaw
Obara. Style augmentation: data augmentation via style randomization. In CVPR workshops ,
volume 6, pages 10–11, 2019.
[29] Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. Spatial trans-
former networks. In Advances in Neural Information Processing Systems , volume 2015-
Janua, pages 2017–2025. Neural information processing systems foundation, jun 2015. URL
https://arxiv.org/abs/1506.02025v3 .
[30] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. 3rd Interna-
tional Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings ,
dec 2014. URL https://arxiv.org/abs/1412.6980v9 .
[31] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In 2nd International
Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings . Interna-
tional Conference on Learning Representations, ICLR, dec 2014. doi: 10.48550/arxiv.1312.6114.
URL https://arxiv.org/abs/1312.6114v10 .
[32] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. CIFAR-10 (Canadian Institute for Advanced
Research). URL http://www.cs.toronto.edu/$\sim$kriz/cifar.html .
[33] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep
convolutional neural networks. In F. Pereira, C.J. Burges, L. Bottou, and K.Q. Weinberger,
editors, Advances in Neural Information Processing Systems , volume 25. Curran Associates,
Inc., 2012. URL https://proceedings.neurips.cc/paper_files/paper/2012/file/
c399862d3b9d6b76c8436e924a68c45b-Paper.pdf .
[34] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive
image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 11523–11532, 2022.
[35] Zhu Li, Adrian Perez-Suay, Gustau Camps-Valls, and Dino Sejdinovic. Kernel Dependence
Regularizers and Gaussian Processes with Applications to Algorithmic Fairness. Technical
report, 2019.
[36] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts, 2017.
[37] Patrick McClure and Nikolaus Kriegeskorte. Representational distance learning for deep neural
networks. Frontiers in Computational Neuroscience , 10(DEC):131, dec 2016. ISSN 16625188.
doi: 10.3389/fncom.2016.00131. URL http://journal.frontiersin.org/article/10.
3389/fncom.2016.00131/full .
[38] Norman Mu and Justin Gilmer. MNIST-C: A Robustness Benchmark for Computer Vision. jun
2019. doi: 10.5281/zenodo.3237938. URL https://arxiv.org/abs/1906.02337v1http:
//arxiv.org/abs/1906.02337 .
[39] Samuel G Müller and Frank Hutter. Trivialaugment: Tuning-free yet state-of-the-art data
augmentation. In Proceedings of the IEEE/CVF international conference on computer vision ,
pages 774–782, 2021.
[40] Arne Nix, Suhas Shrinivasan, Edgar Y Walker, and Fabian Sinz. Can Functional Transfer Meth-
ods Capture Simple Inductive Biases? In Gustau Camps-Valls, Francisco J R Ruiz, and Isabel
Valera, editors, Proceedings of The 25th International Conference on Artificial Intelligence
and Statistics , volume 151 of Proceedings of Machine Learning Research , pages 10703–10717.
PMLR, 2022. URL https://proceedings.mlr.press/v151/nix22a.html .
12

--- PAGE 13 ---
[41] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V o, Marc Szafraniec, Vasil Khalidov,
Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran,
Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra,
Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick
Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without
supervision, 2023.
[42] Pingbo Pan, Siddharth Swaroop, Alexander Immer, Runa Eschenhagen, Richard E Turner, and
Mohammad Emtiyaz Khan. Continual deep learning by functional regularisation of memorable
past. In Advances in Neural Information Processing Systems , volume 2020-Decem, 2020.
[43] Hieu Pham, Zihang Dai, Qizhe Xie, and Quoc V . Le. Meta Pseudo Labels. Proceedings of
the IEEE Computer Society Conference on Computer Vision and Pattern Recognition , pages
11553–11563, mar 2021. ISSN 10636919. doi: 10.1109/CVPR46437.2021.01139. URL
https://arxiv.org/abs/2003.10580v4 .
[44] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical
Text-Conditional Image Generation with CLIP Latents, April 2022. URL http://arxiv.org/
abs/2204.06125 . arXiv:2204.06125 [cs].
[45] Ahmad Rashid, Vasileios Lioutas, and Mehdi Rezagholizadeh. MATE-KD: Masked adversarial
text, a companion to knowledge distillation. In ACL-IJCNLP 2021 - 59th Annual Meeting of
the Association for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing, Proceedings of the Conference , pages 1062–1071, 2021. ISBN
9781954085527. doi: 10.18653/v1/2021.acl-long.86.
[46] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet
classifiers generalize to imagenet? In International conference on machine learning , pages
5389–5400. PMLR, 2019.
[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pages 10684–10695, 2022.
[48] Evgenia Rusak, Lukas Schott, Roland S. Zimmermann, Julian Bitterwolf, Oliver Bringmann,
Matthias Bethge, and Wieland Brendel. A simple way to make neural networks robust against
diverse image corruptions. Lecture Notes in Computer Science (including subseries Lecture
Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) , 12348 LNCS:53–69, jan
2020. ISSN 16113349. doi: 10.48550/arxiv.2001.06057. URL https://arxiv.org/abs/
2001.06057v5 .
[49] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed
Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes,
Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic Text-
to-Image Diffusion Models with Deep Language Understanding, May 2022. URL http:
//arxiv.org/abs/2205.11487 . arXiv:2205.11487 [cs].
[50] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-
vised learning using nonequilibrium thermodynamics. In International Conference on Machine
Learning , pages 2256–2265. PMLR, 2015.
[51] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Re-
thinking the inception architecture for computer vision. In 2016 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 2818–2826, 2016. doi: 10.1109/CVPR.2016.308.
[52] Michalis K Titsias, Jonathan Schwarz, Alexander G. de G. Matthews, Razvan Pascanu, and
Yee Whye Teh. Functional Regularisation for Continual Learning with Gaussian Processes.
2019. URL http://arxiv.org/abs/1901.11356 .
[53] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas
Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic,
and Alexey Dosovitskiy. MLP-Mixer: An all-MLP Architecture for Vision. Advances in
Neural Information Processing Systems , 29:24261–24272, may 2021. ISSN 10495258. doi:
10.48550/arxiv.2105.01601. URL https://arxiv.org/abs/2105.01601v4 .
13

--- PAGE 14 ---
[54] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
Hervé Jégou. Training data-efficient image transformers & distillation through attention. pages
10347–10357, jul 2020. ISSN 2640-3498. URL https://proceedings.mlr.press/v139/
touvron21a.htmlhttp://arxiv.org/abs/2012.12877 .
[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-
tion Processing Systems , volume 2017-Decem, pages 5999–6009, 2017.
[56] Riccardo V olpi, John Duchi, Hongseok Namkoong, Vittorio Murino, Ozan Sener, and Silvio
Savarese. Generalizing to Unseen Domains via Adversarial Data Augmentation. Advances in
Neural Information Processing Systems , 2018-Decem:5334–5344, may 2018. ISSN 10495258.
doi: 10.48550/arxiv.1805.12018. URL https://arxiv.org/abs/1805.12018v2 .
[57] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global represen-
tations by penalizing local predictive power. In Advances in Neural Information Processing
Systems , pages 10506–10518, 2019.
[58] Huan Wang, Suhas Lohit, Michael Jones, and Yun Fu. Knowledge Distillation Thrives on Data
Augmentation. dec 2020. doi: 10.48550/arxiv.2012.02909. URL https://arxiv.org/abs/
2012.02909v1http://arxiv.org/abs/2012.02909 .
[59] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon
Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In
Proceedings of the IEEE/CVF international conference on computer vision , pages 6023–6032,
2019.
[60] Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving
the performance of convolutional neural networks via attention transfer. In 5th International
Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings , 2017.
URL https://github.com/szagoruyko/attention-transfer.
[61] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond
Empirical Risk Minimization. 6th International Conference on Learning Representations,
ICLR 2018 - Conference Track Proceedings , oct 2017. doi: 10.48550/arxiv.1710.09412. URL
https://arxiv.org/abs/1710.09412v2 .
[62] Minjia Zhang, Niranjan Uma Naresh, and Yuxiong He. Adversarial Data Augmentation for
Task-Specific Knowledge Distillation of Pre-trained Transformers. Proceedings of the AAAI
Conference on Artificial Intelligence , 36(10):11685–11693, 2022. ISSN 2159-5399. doi:
10.1609/aaai.v36i10.21423. URL www.aaai.org .
[63] Xiaofeng Zhang, Zhangyang Wang, Dong Liu, and Qing Ling. DADA: Deep Adversarial Data
Augmentation for Extremely Low Data Regime Classification. ICASSP , IEEE International
Conference on Acoustics, Speech and Signal Processing - Proceedings , 2019-May:2807–2811,
may 2019. ISSN 15206149. doi: 10.1109/ICASSP.2019.8683197. URL https://arxiv.
org/abs/1809.00981v1 .
[64] Xinyu Zhang, Qiang Wang, Jian Zhang, and Zhao Zhong. Adversarial AutoAugment. dec 2019.
doi: 10.48550/arxiv.1912.11188. URL https://arxiv.org/abs/1912.11188v1http://
arxiv.org/abs/1912.11188 .
[65] Long Zhao, Ting Liu, Xi Peng, and Dimitris Metaxas. Maximum-entropy adversarial data
augmentation for improved generalization and robustness. In Advances in Neural Information
Processing Systems , volume 2020-Decem, 2020. URL https://github.com/garyzhao/
ME-ADA.
[66] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data
augmentation, 2017.
[67] Dominik Zietlow, Michael Lohaus, Guha Balakrishnan, Matthäus Kleindessner, Francesco
Locatello, Bernhard Schölkopf, and Chris Russell. Leveling Down in Computer Vision: Pareto
Inefficiencies in Fair Deep Classifiers. 2022. URL http://arxiv.org/abs/2203.04913 .
14

--- PAGE 15 ---
A Setup Details
Our experiments on MNIST were meant to reproduce Nix et al. [40] and thus follow their setup
exactly, using the same training setup and model architectures.
A.1 CIFAR10 Experiments
Training We train on the entire CIFAR10 dataset (excluding 10% held-out as validation set) for
300 epochs with a batch-size of 256. As an optimizer, we use Adam [ 30] with a learning rate of
0.0003 and an L2-regularization of 2 ·10−9. Our training begins with a linear warmup of the learning
rate for 20 epochs. The validation accuracy is monitored after every epoch and if it has not improved
for 20 consecutive epochs, we decay the learning rate by a factor of 0.8 and restore the previously
best performing model. The training is stopped prematurely if we decay five times.
Models The different models we use generally follow the standard architecture and settings know
from the literature. For the ViT, we use a smaller variant of it on the CIFAR task. It consists of six
layers and eight attention heads throughout the network. The dropout rate is set to 0.1 and the hidden
dimension is chosen as 512 in all places.
KD and HARD After initial experiments on MNIST, we decided to use a softmax temperature
of 5.0 for all experiments involving KD. We furthermore rely solely on the KL-Divergence loss
to optimize our model. For the experiments with our augmentation framework, we have the same
settings as before for the student (KD) training and separate settings for the augmentor training. There
we have different settings depeding on whether we use the V AE augmentor (or the Affine augmentor).
There we reduce the batch-size to 160 (128) and a learning-rate of 0.0001 (0.05). We initialize both
augmentors to perform an identity transformation, i.e. the V AE is taken pretrained from Child [10].
The thresholds for switching are set as ℓmin= 10%(5%) andℓmax= 60%(40%) . The train modi are
switched if the threshold is surpassed for 5 consecutive iterations. Both λsandλtare set to 1 for the
experiments. For the experiment ResNet101∗→ResNet18, we found a slightly different setting to
be more effective with ℓmin=5% and ℓmax= 40% and a switch only happening if the threshold is
surpassed for 10 consecutive iterations.
A.2 ImageNet Experiments
Baseline Training In general, all our ImageNet experiments follow a similar setup. We train with
a batch-size of 512 samples using the Lion optimizer [ 8] with a linear learning-rate warmup to a
defined initial learning-rate. Afterwards, we anneal the learning-rate following a cosine schedule
[36] with a final value of 0. The training runs for 200 epochs for all ResNet18 experiments and 150
epochs for the ViT-S experiments. Throughout the training, the validation accuracy is monitored on
a heldout set consisting of samples randomly chosen from training set, making up 1% of the total
number of samples. The validation performance is used to pick the best performing epoch throughout
training for final evaluation and the best hyperparameters during grid-search. We train at a resolution
of 224 pixels with random resizing and cropping, as well as random horizontal flips applied in all
trainings. All training runs are performed with automatic mixed precision and 8bit optimization [ 16].
Student Training After a grid-search, we found that for the standalone student training, an opti-
mization with learning-rate 0.0001 with weight decay 0.1 for the ResNet18 student and learning-rate
0.00005 with weight decay 0.001 for the ViT-S student worked best. For both students, we apply
light augmentations during training with Mixup ( α= 0.2) [61] and CutMix ( α= 1.0) [59]. For the
ViT-S baseline, we additionally apply Trivial-Augment [ 39] and randomly erase pixels from the input
image [ 66] with a probability of 0.1. We optimize the standard cross-entropy loss with additional
label-smoothing [51] mixed in with a factor of 0.1.
KD and HARD As described in the main paper, the configuration for the KD experiments (including
HARD) mainly differ in the choice of augmentation, as well as learning-rate and weight-decay. The
plain KD experiments use Mixup ( α= 1.0) and CutMix ( α= 1.0) as well as AugMix [ 24]
augmentation. The softmax temperature was chosen as 1.0 in prior experiments and kept for all
experiments. The learning-rate for all KD and HARD experiments was chosen through a grid-search
15

--- PAGE 16 ---
to be 0.0001 in all cases and weight-decay is 0.001 in most cases, except for HARD experiments
with a ResNet18 student where a weight-decay of 0.05 is used.
B Knowledge Distillation Results from the Literature
We (re-)evaluated student and teacher models from two high-performing KD experiments [6, 41] in
the literature on both in-domain and out-of-domain test sets.
Table 5: In-domain and out-of-domain performance for two KD experiments from the literature.
Showing that the gap in out-of-domain evaluations is larger than in-domain.
In Domain Out of Domain
Model Val ReaL V2 Im-A Im-R Im-C Sketch Style
BiT ResNet152 (Teacher)[6]82.9 87.8 72.0 31.9 49.2 51.0 37.4 16.9
BiT ResNet50 (Distilled) 82.8 87.5 72.5 25.1 45.3 51.8 31.6 15.1
Dino Teacher (Teacher)[41]86.5 89.7 78.4 76.1 79.3 27.3 62.8 34.6
ViT-S/14 (Distilled) 81.2 86.7 71.2 34.4 55.1 53.4 42.2 13.5
16
