# 2306.15766.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/data-augmentation/2306.15766.pdf
# File size: 484668 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Large Language Models as Annotators: Enhancing Generalization of NLP
Models at Minimal Cost
Parikshit Bansal
Microsoft Research India
parikshitb52@gmail.comAmit Sharma
Microsoft Research India
amshar@microsoft.com
Abstract
State-of-the-art supervised NLP models
achieve high accuracy but are also susceptible
to failures on inputs from low-data regimes,
such as domains that are not represented in
training data. As an approximation to collect-
ing ground-truth labels for the specific domain,
we study the use of large language models
(LLMs) for annotating inputs and improving
the generalization of NLP models. Specifically,
given a budget for LLM annotations, we
present an algorithm for sampling the most
informative inputs to annotate and retrain
the NLP model. We find that popular active
learning strategies such as uncertainty-based
sampling do not work well. Instead, we
propose a sampling strategy based on the
difference in prediction scores between the
base model and the finetuned NLP model,
utilizing the fact that most NLP models are
finetuned from a base model. Experiments
with classification (semantic similarity) and
ranking (semantic search) tasks show that our
sampling strategy leads to significant gains
in accuracy for both the training and target
domains.
1 Introduction
A common limitation of supervised NLP models is
that they fail to generalize in low data regimes, cor-
responding to inputs from subgroups or domains
that have limited labelled data in the training set.
These generalisation errors occur due to distribu-
tion shifts between the new inputs and training data,
that render some of the correlations learnt by the
model as invalid (Wang et al., 2022). For instance,
models may learn spurious correlations with sen-
sitive attributes like gender (Sun et al., 2019) or
may over-emphasize lexical patterns (Gururangan
et al., 2018); or in some cases, inputs may exhibit a
new concept that has not been seen in the training
data (Gama et al., 2014).
As a motivating example, consider the task of
determining semantic similarity between a pair ofsentences (Reimers and Gurevych, 2019). This task
forms the basis of information retrieval and recom-
mendation systems such as similar question recom-
mendation on online forums (Wang et al., 2018)
or product recommendation on e-commerce web-
sites (He and McAuley, 2016). In such systems,
it is common to encounter new unseen domains
during deployment. For instance, introduction of
a new item category or users from a new demo-
graphic may cause failures for a deployed model
due to a shift in distribution of inputs in the sys-
tem compared to the training data. Unlabelled data
is readily available for such distribution shift (i.e.,
new questions posted by users or items from a new
category), but labelling the data requires consid-
erable human effort. In other cases, failures may
occur due to hard-to-learn semantic patterns found
in a small minority of the training data (see the
example pair containing lexically similar questions
on oxygen and glucose in Figure 1).
A common solution in all these cases is to col-
lect more labelled data distinct from the distribu-
tion of training data, but labelling (or annotating )
data is an expensive and manual process. To ad-
dress this issue, prior work suggests using large
language models (LLMs, (Ouyang et al., 2022;
Brown et al., 2020)) to annotate data. LLMs like
GPT-3 obtain promising accuracy for annotating
data for a variety of NLP tasks including senti-
ment classification (Ding et al., 2022), keyword
relevance (Choi et al., 2023; Gilardi et al., 2023)
and question answering (Gilardi et al., 2023). How-
ever, LLM-based annotations can be noisy and due
to efficiency reasons, we cannot deploy LLM mod-
els directly.
In this paper, we take the natural next step and
ask whether annotations from LLMs can be used
to enhance generalization of existing NLP models.
Given a corpus of unlabelled data, we find that a
naive application of LLMs (annotating inputs at
random) provides only marginal gains on total ac-arXiv:2306.15766v1  [cs.CL]  27 Jun 2023

--- PAGE 2 ---
curacy and in some cases, can worsen accuracy for
low-data groups. To optimize the sampling, we
formulate the problem of sampling inputs for anno-
tation as an active learning problem (Zhang et al.,
2022). However, we find that the popular sampling
strategy based on model uncertainty (Lewis, 1995)
is also not optimal for LLM-based annotation.
Using experiments on classification (semantic
similarity) and ranking (semantic search) tasks, we
propose an alternative strategy for sampling inputs.
For cost-efficient sampling of new unlabeled in-
puts for LLM annotations, an intuitive solution is
to annotate only those inputs that the NLP model
is expected to be incorrect on, i.e., inputs where
the NLP model’s prediction and the ground truth
label would differ. In the absence of GT labels
for new inputs, we propose a metric, Conditional
Informativeness , that approximates this intuition.
We utilize the fact that state-of-the-art supervised
NLP models are often finetuned from a base model
such as BERT (Vaswani et al., 2017) that provides
an initial embedding for the input. For a given
input and an NLP task, Conditional Informative-
ness measures the deviation between the prediction
score from the base model and the score from the
NLP model finetuned using the available labelled
data for the task. We argue that the inputs which
have the max deviation between the two scores are
the ones likely to be incorrectly predicted by the
finetuned model and hence the most informative
ones for finetuning over the base model.
Our sampling metric provides a practical way to
improve generalization of NLP models for a task
(see Figure 1 for an illustration). Given a budget for
LLM annotation (i.e., number of queries), we select
the inputs having the maximum Conditional Infor-
mativeness for LLM annotation and then retrain the
NLP model using this additional training data. Our
algorithm shows significant improvements in target
domain and total accuracy, on the Quora dataset
for the semantic similarity task, and on Amazon
and Wikipedia datasets for the semantic search task.
Our algorithm also provides higher gains than the
uncertainty-based sampling from active learning
literature. This may be because of the error distri-
bution of LLM annotations: only for inputs with
high deviation, the LLM-based annotations may be
expected to be more accurate than the base model.
To summarize, we make the following contribu-
tions.
1) The Conditional Informativeness metric for sam-pling inputs for LLM-based annotation that outper-
forms commonly used active learning approaches.
2) Experiments on semantic similarity and search
tasks that show LLM annotations can significantly
improve both in-domain and target domain accu-
racy.
2 Related Work
LLMs for data augmentation. A popular frame-
work for improving a NLP model’s generalization
has been to generate new data using LLMs and test
the model’s output using a human-in-the-loop, i.e.
LLMs are used in partnership with human partic-
ipants for data generation and testing/debugging
models (Ribeiro and Lundberg, 2022; Wang et al.,
2021). In recent work, (He et al., 2023b) utilise the
same strategy for training an NLP model: they use
GPT-3 for data generation over under-represented
groups, which are then annotated by users before
including in training set.
However, with more capable LLMs like Chat-
GPT, LLMs are now capable of not just generat-
ing data, but also annotating it (while faithfully
following annotation instructions). Recent work
(Gilardi et al., 2023; He et al., 2023a; Ding et al.,
2022) has looked at the annotation accuracy for
LLMs and found them to be at par with crowd-
worker annotators. Combining generation and an-
notation, parallel to us, (Whitehouse et al., 2023)
explore the utility of both input and labels gener-
ated from LLMs for crosslingual common sense
reasoning tasks. Similarly, for the task of building
a sentence embedding using contrastive learning,
(Cheng et al., 2023) use LLMs to both generate
novel input pairs and then score their similarity.
Motivated by real-world applications from infor-
mation retrieval, we focus our attention on the un-
supervised domain adaptation (Ramponi and Plank,
2020) (UDA) setting where unlabelled inputs are
easily available. UDA methods assume a source
labeled domain and a target unlabeled domain with
the goal of adapting to the target domain (while
also performing well on the source domain). For
instance, (Saad-Falcon et al., 2023) motivate the
passage reranking task where a large number of un-
labelled passages are available. They use LLMs to
generate synthetic queries for a given passage and
then use such augmented data to train a downstream
model. Given the potential of LLM-annotated data
for training downstream classifiers and the asso-
ciated costs of querying them, we study how to

--- PAGE 3 ---
Pair2LLM Output1. Condition Informativeness Computation  4. Model training
Uncertainty
 2. Input Sampling
Conditional
Informativeness
Random
Lexical Similarity  Semantic Similarity 
What is structure 
for glucose?
What is structure 
for oxygen?How can I make 
friends?
How do I make 
friends?
Low Semantic Similarity 
High Lexical SimilarityOutput all pair of sentences
asking the same question.
Pair1 : What is structure for glucose?, 
What is structure for oxygen?
Pair2 : How can I make friends?, 
How do I make friends?
Answer : 
LLM3. LLM Annotation
Input Prompt
High Semantic Similarity 
High Lexical SimilarityAccuracySamplingFigure 1: Enhanced Generalization using LLM Annotations. Illustration of our algorithm using the duplicate
question detection task. We propose a sampling strategy based on deviation of an NLP model’s similarity score
from the base model, called (base model) -conditional informativeness. Inputs are sampled using this strategy (Step
2), annotated using an LLM (Step 3) and then added to the training set of the NLP model. Our sampling strategy
performs significantly better than random or active learning-based strategies.
efficiently utilise these annotations to train a more
generalizable NLP model; specifically, which in-
puts to annotate for maximum benefit?
Semantic similarity with limited labeled data.
(Chen et al., 2023) present a comprehensive survey
of data augmentation techniques for limited label
data settings in NLP. AugSBERT (Thakur et al.,
2020) present an augmentation strategy that uses a
bigger (oracle) cross-encoder model for generating
(pseudo-)labels for unlabeled inputs. These inputs
are then utilised to train a smaller and efficient
NLP model. Such an oracle, however, is limited
by the training data whereas LLMs are known to
have zero-shot capabilities that generalize to new
domains(Hou et al., 2023).
In addition to augmentation, unsupervised do-
main adaptation methods have also been pro-
posed. Apart from the main task learning loss,
(Ramesh Kashyap et al., 2021) propose an addi-
tional loss which minimizes the divergence be-
tween source and target domain representations.
Recent work UDApter (Malik et al., 2023) com-
bines UDA methods with adapters for efficient
domain adaptation. However, domain matching
techniques work only under a restrictive set of
assumptions (Li et al., 2020). Instead, we aim
to approximate the ground-truth labels throughLLMs, thereby converting the unsupervised prob-
lem into a simpler, supervised learning problem.
(Dua et al., 2022) investigates the failure modes
of Open-domain question answering when faced
with distribution shifts. In addition they propose a
few-shot data augmentation method for improving
generalisation of these models. The augmentations
uses LLMs to generate question for a given pas-
sage.
Active Learning. Choosing which inputs to an-
notate has been classically studied as an active
learning problem (Settles, 2009).In active learn-
ing setup, we are given a small set of Llabeled
inputs, along with a large pool of Uunlabeled in-
puts. We are also specified a budget B, which
denotes the number of inputs from the unlabeled
data that can be annotated by an oracle/human. Ac-
tive learning explores how to best sample Binputs
from the unlabeled pool to maximize the general-
ization accuracy of the final model that is trained
on the original L+ (annotated) Bsamples. Ac-
tive Learning uses two primary criterion for sam-
ple selection : Informativeness and Representa-
tiveness (Zhang et al., 2022). The most popular
informativeness technique is uncertainty sampling
(Lewis, 1995; Schröder et al., 2021) and for rep-
resentativeness is diversity/density. As an appli-

--- PAGE 4 ---
cation, recent work (Margatina et al., 2023) uses
active learning in an in-context learning setting for
LLMs and shows that similarity based sampling
(instead of uncertainty and diversity) are most ef-
fective for in-context learning. In this paper, we
focus on LLM-based annotations and evaluate the
uncertainty-based informativeness sampling tech-
nique. Based on our experiments, we also propose
a new informativeness criterion.
3 Conditional informativeness criterion
for sampling LLM annotations
3.1 Background: Building NLP classifiers
using base models
Given a domain of sentences, X, and a task T:
X → { 0,1}we consider learning a classifier func-
tionf:X → { 0,1}which follows the task
i.e.f(x) =T(x)∀x∈ X . The function
aims to learn features which are predictive of the
output label and their mapping to the output la-
bel. A subset of the domain Xis denoted by
X={x0, x1, x2, . . . , x |X|} ⊆ X . The output
label of xiisT(xi)and is denoted by ti. A set of
examples can be represented as
D={(xi, ti) :i∈[|X|]} (1)
Unlabeled examples lack the task label ti.
Semantic Similarity. As an example, consider
the semantic similarity task (Cer et al., 2017). In-
puts for semantic similarity come from X × Y
where XandYare a pair of domain of sentences.
The domains can be the same or different for sym-
metric and asymmetric similarity respectively. For
a given input (xi, yi), the task output is 1 if a pair
are semantically similar, and 0 if they are not. The
classifier for semantic similarity is hence defined
asf:X × Y → { 0,1}. We denote a training set
as :
D={((xi, yi), ti) :i∈[|X|]} (2)
Further details on semantic similarity are in
Supp. E.
Finetuning on Base model. NLP models are usu-
ally finetuned on top of some pretrained text mod-
els (e.g., we use MSMARCO-DistilBERT-v4 for se-
mantic similarity) called Base model. The base
model adheres to an approximation of the task
based on the pretraining dataset and provides ini-
tial embedding for the input. We call these features
defined by the base model as pretrained features .3.2 A domain adaptation case study: Which
inputs to annotate?
To evaluate different input sampling techniques for
LLM annotations, we consider the semantic similar-
ity task of duplicate question detection. We train bi-
encoders (SBERT (Reimers and Gurevych, 2019))
on the Quora Questions Pair dataset (Wang et al.,
2018),using MSMARCO-DistilBERT-v4 as the base
model. To simulate a challenging target domain,
we remove 60% of “extreme” examples from the
training dataset. These are examples where the
base model either obtains the lowest mean squared
error w.r.t. the ground truth labels or obtains the
highest mean squared error. That is, half of the
examples (30%) are the easy examples the base
model is (most) correct on and the remaining half
are the hard examples where the base model is
(most) incorrect on. Further, we remove labels
from the target domain. Hence from the original
data we have 40% “source” labeled examples and
60% “target” unlabeled examples. For accuracy
evaluation on both source and target domains, we
create analogous domains over the test set too.
We consider an active learning setup where se-
lected inputs from target domain can be annotated
by an LLM and augmented in the training set.
After augmentation, the model is trained on the
source domain + augmented dataset. We consider
two popular active-sampling approaches in liter-
ature: Random and Uncertainty-based sampling.
Apart from these, we include two additional sam-
pling techniques based on our knowledge of the
target domain: base-consistent-sample andbase-
inconsistent-sample . These are designed to capture
theeasy andhard examples that constitute the tar-
get domain. Given labeled data L, unlabeled inputs
U and a budget for annotation as B we have :
•random-sampling. We randomly select B
inputs out of the Uunlabeled inputs for anno-
tations.
•uncertainty-sampling. We first finetune
the base model on the given labeled data L
and then select the B(budget) most uncertain
(according to the finetuned model) unlabeled
inputs (out of U).
•base-consistent-sampling. We choose top B
examples having lowest (MSE) error on base
model predictions with GT labels.
•base-inconsistent-sampling . We choose top

--- PAGE 5 ---
Bexamples having highest (MSE) error on
base model predictions with GT labels.
These Binputs are then annotated and included for
final training on L+B.
AUC under different sampling strategies. Us-
ing gpt-3.5-turbo as the annotater LLM, we report
AUC (area-under-(ro)curve) in Table 1. We set a
budget Bof 10% of the dataset for annotation. For
details on prompts used, see Sec 4.1.
Looking at the AUC metric for the complete
target domain, we observe that random-sampling
and uncertain-sampling lead to similar improve-
ments compared to the training set. Compared to
these active learning techniques, base-inconsistent-
sampling leads to almost twice the AUC improve-
ment. That is, annotations with LLM are best un-
der base-inconsistent-sampling. Remarkably, with
only 10% of the examples annotated, AUC with
base-inconsistent-sampling is even higher than the
setting where we augment the fulltarget domain
(100% of examples). In contrast, base-consistent-
sampling hurts generalization. Even though base-
consistent-sampling was designed to sample exam-
ples with low base model error, it obtains worse
AUC than base-inconsistent-sampling on test exam-
ples with low base model error. Results on using
Ground Truth (GT) labels for annotations (instead
of LLM annotations) are in Supp. Table 11.
Implications. The above results indicate that for
LLM annotations, uncertainty-sampling may not
be the best technique. To understand these results,
note that the original model finetuned on training
set (first row in Table 1, with no augmentation from
target domain) has high generalization (AUC) for
low base error inputs while generalizing poorly
for high base error inputs. Annotating with base-
consistent-sampling is thus a waste of budget as
the base and simple finetuned model are already
good on the low base error inputs. Moreover since
LLM-annotations are not perfect, augmenting with
base-consistent-sampling introduces noise into the
model, when the model already has a high accu-
racy.
On the other hand, high base error examples,
which are targeted by base-inconsistent-sampling,
do have substantial room for AUC improvement
when considering the original finetuned model.
This indicates that LLM annotations should focus
only on base-inconsistent-sampling inputs, as such
annotations may be the most informative .3.3 Conditional Informativeness metric
Based on the experiments above, we find that when
annotated with LLMs, high base error, or base-
inconsistent examples are the most informative for
training. But base-inconsistent-sampling, as de-
scribed above, is not practical since it requires
knowledge of the ground truth labels of inputs.
Hence in this section, we develop an approxi-
mate metric for quantifying the degree of base-
inconsistency of unlabeled inputs.
We use a metric which measures deviation of the
finetuned NLP model from the base model, and call
it Conditional Informativeness, since it depends on
the base model in addition to the finetuned model.
For a input xiwe define it as
zi(f, f 0) =Dev(f(xi), f0(xi)) (3)
where f0is the base model, fis the finetuned
model and Dev is a measure of deviation. We use
simple squared error in our work.The intuition is
that during the finetuning process with the goal of
minimizing error, a model is more likely to deviate
from the base model’s score on an input if the base
model has high error on that input. Here we assume
that the finetuned model’s score deviation captures
this notion of base error, which can be generalized
to the unlabelled inputs.
We present qualitative examples from our metric
on Quora dataset in Table 2. These inputs were
selected by our Conditional Informativeness metric
as having high deviation. While for the first pair
of examples the lexical similarity (base semantic)
is of the pair is low, their semantic meaning ( du-
plicate question semantics) is the same, while for
the second pair, while the lexical similarity is high,
their semantic similarity is low. When doing LLM
annotations, inputs like these would be the most
informative for training.
The formulation above defines Conditional Infor-
mativeness based on deviation of individual input
semantic similarity scores. But we can also define
Conditional Informativeness using deviation at a
domain level. For example, for a multi-domain
dataset with domain information for each input, the
metric can be averaged over the entire domain to
find the most suitable domains for LLM annota-
tions.

--- PAGE 6 ---
Data Complete Test High Base Error Low Base Error
Initial Train Set 86.824 ±0.038 59.335 ±0.139 99.068 ±0.048
+ 100% (complete target domain) 87.544 ±0.035 65.785 ±0.121 98.164 ±0.044
+ Random-sampling 10% 87.052 ±0.151 60.551 ±0.701 98.805 ±0.058
+ Uncertain-sampling 10% 87.620 ±0.029 61.594 ±0.433 99.081 ±0.024
+ Base-consistent-sampling 10% 86.763 ±0.149 59.986 ±0.340 98.833 ±0.024
+ Base-inconsistent-sampling 10% 88.108 ±0.062 65.538 ±0.175 98.861 ±0.046
Table 1: AUC for Quora duplicate questions task, before and after including LLM-based annotations using four
different sampling techniques: random, uncertainty, base-consistent and base-inconsistent. AUC is evaluated on the
full test set, the test subset with high base model error and the test subset with low base model error. Sampling just
10% of the data for annotation using base-inconsistent-sampling is better than annotating with the complete (100%)
target dataset.
PairSimilarity
Base Finetuned
What is a good diet plan for a commuter that wants to gain weight ?Low HighWhat food should I eat to gain weight ?
How can you determine the structure for glucose ?High LowHow can you determine the structure for oxygen ?
Table 2: Quora test examples having high Conditional
Informativeness , i.e. finetuned predictions are different
from base model predictions. Base model captures lexi-
cal similarity while finetuned captures target semantics.
4 EAGLE: Enhanced Generalization
using LLM Annotations
Based on the Conditional Informativeness metric,
we now present the EAGLE algorithm for enhanc-
ing generalization of NLP models using LLM an-
notations. As in Section 3, we consider an active
learning setup where we are given some labeled
examples Land a pool of unlabeled inputs Ualong
with a budget Bof annotating unlabeled inputs
(using LLMs). In addition to standard classifica-
tion tasks, our algorithm can also work for other
tasks such as ranking. We first present the gen-
eral algorithm and then present instantiations of it
for a classification task (semantic similarity) and a
ranking task (semantic search).
4.1 The EAGLE Algorithm
Step 1: Computing Conditional Informativeness
As the first step, we finetune our base model on the
labeled data Lto get a finetuned model fi.e.,
f= argminfE(xi,ti)∈L[L(f(xi), ti)] (4)
Using f, we compute the Conditional Informa-
tiveness score zi(f, f 0)where f0is the base model,
for each unlabeled input xi∈Ui.e.
z={zi(f, f 0) :xi∈U} (5)Step 2: Sampling inputs using Conditional In-
formativeness The next step involves sampling
appropriate inputs for LLM annotations. We either
choose to do an input-wise Conditional Informa-
tiveness sampling, or if the data is domain anno-
tated, we can do domain level annotations. For
input-wise sampling we select the top Bsamples
i.e.
Usampled ={xi:zi∈top(z, B)} (6)
For domain level annotations, we can obtain the
domain-level Conditional Informativeness metric
(by averaging the metric over inputs belonging to
the domain). In this case, the budget B is uniformly
distributed over inputs in selected domains.
Step 3: Annotating sampled inputs using LLM
Given a sampled set of unlabelled input Usampled ,
we use LLM annotations for these inputs to get
an annotated set as L′
sampled. We denote LLM
annotations function by T′:X → { 0,1}, and
hence the LLM annotation for input xiast′
ij∈
{0,1}. The augmented dataset made from Uis
hence L′
L′
sampled ={(xi, t′
i) :xi∈U} (7)
Step 4: Finetuning classifier on augmented la-
belled data Finally we finetune the base model
on the augmented dataset L+L′
sampledusing Eq 4.
4.2 Application: Semantic Similarity
We present how our algorithm can be used for
the semantic similarity task described in Section
3. Step 1 follows from the main algorithm. The
Conditional Informativeness computation follows
Eq 3, with the only caveat being that the classifier
function now takes two inputs :
zi(f, f 0) =Dev(f(xi, yi), f0(xi, yi)) (8)

--- PAGE 7 ---
Sampling is done in the same way with the algo-
rithm selecting top Binputs having highest zi.
LLM Annotation Details Consider a set of un-
labeled examples Uconsisting of pairs (xi, yi)to
be annotated by the LLM. We construct a prompt
which consists of set of pairs (xi, yi)of sentences.
For cost-efficiency we consider 10 pairs in each
prompt for our experiments. The prompt asks the
LLM to output all the pairs which are semantically
similiar (with semantics defined appropriately in-
side the prompt). All pairs outputted by LLM as
similar are considered similar while rest are not.
See Table 3 for example annotation outputs on
Quora dataset.
Pair Deviation GT LLM
Why does Cuba tolerate the presence of Guantanamo Bay Naval Base ?Low 0 1What is the deal with Guantanamo Bay ? Why isn’t it closed yet ?
What are the best headhunters in Mexico ?Low 1 0Who are the best headhunters in Mexico ?
What is third pricing model ?High 0 0What is a pricing model ?
How was trading performed in Ancient India ?
High 1 1 Is there proof of ancient Indians trading overseas ? If yes ,
then what did they trade in and with what countries ?
Table 3: LLM (gpt-3.5-turbo) annotations for some low
and high deviation examples. LLM is able to correctly
guess the high deviation ones while is incorrect on the
low deviation ones. LLM annotation accuracy is agnos-
tic of the deviation. See Supp A for prompts used.
4.3 Application: Semantic Search
While semantic similarity is a fundamental task,
real world applications often rely on semantic
search . In such applications, the Xis called set of
allqueries denoted as X={x0, x1, x2, . . . , x |X|},
while Yis the set of labels denoted as Y=
{y0, y1, y2, . . . , y |Y|}. These search for an optimal
semantic match for a sentence x∈Xfrom the set
Y, i.e.
g(x,T) =argmaxyi∈YT(x, yi) (9)
In practice since we don’t have the true semantics
T(e.g., relevance to query), we use some approxi-
mation of semantics for argmax. We denote a set
of examples by:
Dsearch ={((xi, Y), Ti) :i∈[|X|]} (10)
where
Ti={tij:j∈[|Y|]} (11)
Unlabeled samples lack Tiinformation. Following
Eqn. 3, Conditional Informativeness on set Xis
defined as
y=g(xi, f0)
zi(f, f 0) =Dev(f(xi, y), f0(xi, y))(12)where g(.)finds the nearest yj∈Yforxiaccord-
ing to base embedding function f0(Eqn 9).
LLM Annotation Details The unlabeled set U
now consisting of pairs (xi, Y)to be annotated
by LLMs. Querying semantic similarity for each
query, label pair {(xi, yj) :yj∈Y}is very expen-
sive. Hence we first create a filtered set of labels
from a semantic similarity model (in our case fine-
tuned model) f. With slight abuse of notation, we
consider an extension of the function gin Eq 9
asg(x, f, K )where gnow outputs a set of top K
labels for each query. Our filtered set is hence
Y′=g(x, f, K )where fis the finetuned model.
The set Y′hence consists of top Kranked labels
for a query according to the finetuned model f.
The rest of the labels (which weren’t in the top K
ranking of the finetuned model) i.e. Y/Y′have
their semantic similarity set to 0. We query the
LLM for semantic similarity of labels in the fil-
tered set Y′, where |Y′|=K. Hence this helps
us reduce the complexity of searching through the
whole label space by restricting the search space us-
ing the finetuned model f. We take K= 10 for all
experiments. For each pair {(xi, yj) :yj∈Y′}we
can then query LLM similar to semantic similarity
setup above.
L′={((xi, Y), T′
i) : (xi, Y)∈U} (13)
We empirically observe that it is better to provide
one prompt for each query along with its top K
filtered labels. The labels should be ordered by
their semantic similarity score according to the
model fin the prompt. Example prompts used
in our experiments can be found in Supp. A. The
filtering step in semantic search can use any good
similarity model. In our experiments, we utilise
our finetuned model ffor the filtering step.
5 Experiments
We evaluate EAGLE algorithm on two tasks: 1)se-
mantic similarity , a fundamental task; 2)semantic
search , a real-world task motivated by information
retrieval applications. We assume that in addition
to some labeled examples, we are also given a large
pool of unlabeled inputs. For semantic similarity
we consider generalisation in limited labeled data
setting, while for semantic search we evaluate gen-
eralisation to unlabeled target domains. In limited
labeled data setting, both the labeled and unlabeled
inputs follow the same distribution while when

--- PAGE 8 ---
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5
Quantiles 
0.6
0.4
0.2
0.00.20.40.60.81.0Gain in AUC(a) LLM-annotated
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5
Quantiles 
0.40.60.81.01.2Gain in AUC (b) GT-annotated
Figure 2: Gain in AUC on including LLM-annotated and GT-based augmentations on Quora dataset. The orange
line is gain in AUC with uncertainty based sampling (shaded region shows std err.). We divide the unlabeled data
into 20 quantiles, based on the Conditional Informativeness metric. Conditional Informativeness increases from left
to right. For LLMs, uncertainty is not a good method for sampling and Conditional Informativeness based sampling
is better, while for GT-based augmentations uncertainty based sampling is better.
Data Test AUC
Initial Train set 85.780 ±0.002
+ Random LLM 86.001 ±0.139
+ Uncertainty LLM 86.058 ±0.089
+ Conditional Informativeness LLM 86.432 ±0.106
+ Random GT 86.677 ±0.068
+ Uncertainty GT 87.125 ±0.135
+ Conditional Informativeness GT 87.445 ±0.091
Table 4: AUC for different sampling techniques for
Quora semantic similarity task. We sample 10% of unla-
beled data. For LLM-based annotations, the best method
is to sample using our Conditional Informativeness sam-
pling while for GT based annotation, both uncertain and
Conditional Informativeness based sampling are good.
adapting to unlabeled target domain (the unlabeled
input), there is a distribution shift in inputs of the
labeled and unlabeled examples. We show how our
Conditional Informativeness based sampling inputs
helps to improve generalization in both of these set-
tings. We do our experiments on embedding-based
semantic similarity as defined below.
Embedding-based Semantic Similarity/Search
The search argmax operation (Eq 9) over the com-
plete query set |X|is quadratic (i.e. |X|×|Y|). For
efficient computation, we use embeddings based se-
mantic similarity (SBERT (Reimers and Gurevych,
2019)), where both queries and labels are seperately
embedded into a Ndimensional unit norm space.
The dot product between the embedded represen-
tations of sentences gives the semantic similarity.
Hence the goal is to learn a embedding functionh:X ∪Y → RNs.t.h(xi)⊺h(yi)gives the seman-
tic similarity between the functions.
Sampling methods. In addition to Conditional
Informativeness, we consider random-sampling
and an active learning uncertainty-based sampling
algorithm. As an oracle, we also consider ground-
truth labels for the same inputs sampled by each of
these sampling algorithms.
Implementation details. We consider the base
model as MSMARCO-DistilBERT-v4 for both tasks.
For LLM-based annotations we use GPT-3.5-
Turbo. See Supp. A for prompts used in the
experiments. We tried open source models such as
TogetherComputer/RedPajama-INCITE-7B-Base
(along with the Chat and Instruct version) or
MosaicML/mpt-7b-chat but did not obtain good
annotation accuracy. All results are reported for 3
seeds. Other training details are in Supp. C.
5.1 Semantic Similarity
Setup We conduct experiments on the Quora
Question Pairs (Wang et al., 2018) dataset, which
consists of pairs of questions. The task is to label
each pair as a duplicate or not, i.e., whether the
questions have the same intent or not. We subsam-
ple 38400 training pairs from the train set. We
consider a setup where 10% of the Quora dataset
is labeled by ground truth, while rest of the 90%
forms the unlabeled pool of data. We present test
AUC (Area-Under-ROC) numbers as the evaluation
metric.

--- PAGE 9 ---
Wikipedia Amazon
USA Total Books Total
Initial Train set 12.530 ±0.034 19.048 ±0.019 17.226 ±0.008 24.904 ±0.076
+ Target LLM Random 40% 13.188 ±0.073 19.232 ±0.024 17.959 ±0.075 25.065 ±0.049
+ Target LLM Conditional Informativeness (bottom 40%) 13.089 ±0.079 19.209 ±0.034 18.021 ±0.033 25.110 ±0.038
+ Target LLM Conditional Informativeness (middle 40%) 13.166 ±0.021 19.228 ±0.022 18.123 ±0.060 25.216 ±0.012
+ Target LLM Conditional Informativeness (top 40%) 13.372 ±0.058 19.363 ±0.023 18.351 ±0.028 25.271 ±0.030
+ Target GT Random 40% 13.893 ±0.047 19.430 ±0.052 18.375 ±0.068 25.329 ±0.051
+ Target GT Conditional Informativeness (bottom 40%) 13.911 ±0.032 19.395 ±0.013 18.455 ±0.054 25.271 ±0.020
+ Target GT Conditional Informativeness (middle 40%) 13.973 ±0.070 19.327 ±0.023 18.400 ±0.027 25.213 ±0.075
+ Target GT Conditional Informativeness (top 40%) 13.878 ±0.015 19.414 ±0.015 18.613 ±0.043 25.285 ±0.057
Table 5: P@1 for test target domain (USA in Wikipedia and Books in Amazon) and complete test set. For LLM-
based annotation, top 40% samples according to our Conditional Informativeness are optimal for total accuracy
(while also being optimal for target domains accuarcy). For GT based annotations, Random Sampling is best for
total accuracy. Best target domain accuracy method for GT is inconclusive.
Comparison with Random and Uncertainty
Sampling We follow the Algorithm from 4.1 for
semantic similarity. Using the model finetuned on
labeled data, we sample 10% of unlabeled data for
annotation, according to various sampling strate-
gies (namely random, uncertainty and Conditional
Informativeness). For details on how LLM annota-
tions are done see Sec 4.2. We also present results
on annotations with ground truth labels i.e. t′
i=ti
(Sec 4.2). In Table 4, we show that for LLM-
based annotations, Conditional Informativeness-
based sampling achieves significantly better test
AUC than random and uncertainty sampling. In
comparison, for annotating with GT labels, both un-
certainty and Conditional Informativeness- based
sampling yield high AUC.
Evaluating Conditional Informativeness-based
Quantiles To find out why uncertainty-based
sampling did not work for LLM annotations, we
divide the data into 20 quantiles, each having 5%
of unlabeled data based on Conditional Informa-
tiveness metric. Figure 2 shows the gain in AUC
on including these samples (LLM or GT annotated)
with the training data. As a comparison, the or-
ange line in the plot signifies accuracy on sampling
5% from uncertainty metric (shaded portion is std
error). For LLM annotations, we observe that un-
certainty is not a good technique for sampling and
Conditional Informativeness-based sampling is bet-
ter, while for GT-based augmentations uncertainty-
based sampling provides better gains than Condi-
tional Informativeness-based sampling.
5.2 Semantic Search
Next, we evaluate the utility of Conditional In-
formativeness sampling for generalisation to un-
labeled target domains in semantic search tasks.Datasets We consider two recommen-
dation datasets for semantic search : 1)
LF-WikiSeeAlsoTitles-320K (Bhatia et al.,
2016) (i.e., Wikipedia) considers a recommen-
dation/retrieval setting. The train set consists of
Wikipedia page titles (queries X) along with a
large set of page titles (labels Y). For Wikipedia, a
labelyjis semantically similar to a query xiif the
label is likely to occur in the SeeAlso section of the
query article’s wiki-page. As described in Section
4.3 for the semantic search task, the set of labels
remains fixed to Y. The task is to learn embed-
dings which follow the semantics above. For each
article Xwe also parse its category information,
which we use as it domain label. If for an article
xi, it’s categorical information contains "USA" or
"America" it belongs to the domain USA, otherwise
not. 2) LF-AmazonTitles-131K (Bhatia et al.,
2016) (i.e., Amazon) considers recommendations
in e-commerce AlsoBought product setting. Given
a query product ( X) the labels correspond to
possible products a user might buy ( Y). Here too
we consider categorical information for all query
products X. We construct two domains in Amazon.
All products in "Books" category are in the Books
domain, while all products in the "Kitchen and
Dining" category form the Kitchen domain.
Setup For Wikipedia we consider the USAdo-
main as our target unlabeled domain, and the rest
of the dataset as our labeled data. Similarly for
Amazon we construct two versions of the dataset,
one where we Books domain as the target unlabeled
domain and another where we consider Kitchen as
the target unlabeled domain. We use Precision@1
(P@1) metric for evaluation, i.e. the fraction of
queries whose top ranked label is semantically sim-

--- PAGE 10 ---
ilar (or relevant ) to the query, i.e,
Precision @1=Exi∈X[T(xi, g(xi, f))]
For the GT annotation oracle, we annotate the top
Ksampled labels (using the finetuned model f)
with ground truth information i.e. for a query xi,
t′
ij=tij∀yj∈Y′andt′
ij= 0∀yj/∈Y′. See
Sec 4.3 for notation (Eq 11,13). Note that for all
labels which are not ranked in top Kby the fine-
tuned model have their semantic similarity set to 0,
even if they were relevant in GT. For other details
refer to Supp. C.
Results We present test P@1 for the target do-
mains ( USA domain in Wikipedia and Books do-
main in Amazon) and the complete source + target
domain test sets in Table 5. We find that when aug-
menting with LLM based annotations, selecting
inputs which are in the top 40% inputs according
to our Conditional Informativeness are optimal for
total accuracy (while also being optimal for tar-
get domains accuarcy). For GT based annotations,
Random Sampling is best for total accuracy, though
results are not significant.
Using Domain-knowledge for Qualitative mea-
sure of Conditional Informativeness On the
Amazon recommendation task, consider domain
adaptation to Books orKitchen domains. For Book
recommendations using only book titles (e.g., say
The Kite Runner forA Thousand Splendid
Suns ) the Conditional Informativeness would be
high for encoder based models (assuming that en-
coder doesn’t have the necessary domain knowl-
edge for book recommendations, i.e., the two books
share the same author). That is, it would re-
quire more world knowledge than for domains like
Kicthen , (e.g., Kaiser Bakeware Muffin Pan
forNordic Ware Brownie Pan ) which are more
likely to be consistent with the base model’s seman-
tics (in this case lexical similarity).
For the domain Kitchen, we can see in Table 6
that including LLM-based annotations for domain
Kitchen does not provide any gains compared to
the base model. In comparison, for other domains
like Books, LLM annotations lead to better gener-
alisation than both base and training set finetuned
models. Refer to Supp. B for a plot showing how
LLMs are not better than finetuned/base model for
Amazon(Kitchen) domain, whereas for Wiki(USA)
and Amazon(Books) LLMs are significantly better
(Fig 3). For accuracy improvements on Kitchen do-
main, techniques utilising regularisation to baseFinetuning Dataset Wikipeda(USA) Amazon(Books) Amazon(Kitchen)
Conditional Informativeness 9.57 9.48 8.62
Base Model 12.33 16.78 32.91
Training set 12.54 17.37 31.53
Training set+Target LLM 12.95 18.28 32.91
Table 6: Domain averaged Conditional Informativeness
(x100) scores for the target domains (USA in Wikipedia
and Books/Kitchen in Amazon) are shown in the top row.
The next three rows present P@1 numbers for target
domains. LLM annotations help for USA andBooks
target domain; but for the Kitchen target domain, the
Base model has the same accuracy as LLM-augmented
model.
model may be suitable and LLMs may not be
needed.
6 Conclusion
We showed how LLMs can be used for annotations
and how sampling of inputs plays an important role
in improving an NLP model’s generalization. To
this end, we presented a novel sampling algorithm
for input selection that performs better than the
popular technique of uncertainty-based sampling.
As future work, we would like to test whether
the Conditional Informativeness metric applies to
other NLP tasks beyond semantic similarity. For
the semantic search setting, given the generative ca-
pabilities of LLMs, an interesting future direction
is to use LLMs to generate labels for queries while
restricting the generated label set to our target label
set.
References
K. Bhatia, K. Dahiya, H. Jain, P. Kar, A. Mittal,
Y . Prabhu, and M. Varma. 2016. The extreme classi-
fication repository: Multi-label datasets and code.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-
Gazpio, and Lucia Specia. 2017. Semeval-2017
task 1: Semantic textual similarity-multilingual and
cross-lingual focused evaluation. arXiv preprint
arXiv:1708.00055 .
Jiaao Chen, Derek Tam, Colin Raffel, Mohit Bansal,
and Diyi Yang. 2023. An empirical survey of data
augmentation for limited data learning in nlp. Trans-
actions of the Association for Computational Linguis-
tics, 11:191–211.

--- PAGE 11 ---
Qinyuan Cheng, Xiaogui Yang, Tianxiang Sun, Linyang
Li, and Xipeng Qiu. 2023. Improving contrastive
learning of sentence embeddings from ai feedback.
arXiv preprint arXiv:2305.01918 .
Jonathan H Choi, Kristin E Hickman, Amy Monahan,
and Daniel Schwarcz. 2023. Chatgpt goes to law
school. Available at SSRN .
Kunal Dahiya, Nilesh Gupta, Deepak Saini, Akshay
Soni, Yajun Wang, Kushal Dave, Jian Jiao, Prasenjit
Dey, Amit Singh, Deepesh Hada, et al. 2023. Ngame:
Negative mining-aware mini-batching for extreme
classification. In Proceedings of the Sixteenth ACM
International Conference on Web Search and Data
Mining , pages 258–266.
Bosheng Ding, Chengwei Qin, Linlin Liu, Lidong Bing,
Shafiq Joty, and Boyang Li. 2022. Is gpt-3 a good
data annotator? arXiv preprint arXiv:2212.10450 .
Dheeru Dua, Emma Strubell, Sameer Singh, and
Pat Verga. 2022. To adapt or to annotate: Chal-
lenges and interventions for domain adaptation in
open-domain question answering. arXiv preprint
arXiv:2212.10381 .
João Gama, Indrundefined Žliobaitundefined, Al-
bert Bifet, Mykola Pechenizkiy, and Abdelhamid
Bouchachia. 2014. A survey on concept drift adapta-
tion. ACM Comput. Surv. , 46(4).
Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli.
2023. Chatgpt outperforms crowd-workers for text-
annotation tasks. arXiv preprint arXiv:2303.15056 .
Suchin Gururangan, Swabha Swayamdipta, Omer Levy,
Roy Schwartz, Samuel R Bowman, and Noah A
Smith. 2018. Annotation artifacts in natural language
inference data. arXiv preprint arXiv:1803.02324 .
Ruining He and Julian McAuley. 2016. Ups and downs:
Modeling the visual evolution of fashion trends with
one-class collaborative filtering. In proceedings of
the 25th international conference on world wide web ,
pages 507–517.
Xingwei He, Zhenghao Lin, Yeyun Gong, A Jin, Hang
Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan Duan,
Weizhu Chen, et al. 2023a. Annollm: Making large
language models to be better crowdsourced annota-
tors. arXiv preprint arXiv:2303.16854 .
Zexue He, Marco Tulio Ribeiro, and Fereshte
Khani. 2023b. Targeted data generation: Find-
ing and fixing model weaknesses. arXiv preprint
arXiv:2305.17804 .
Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu,
Ruobing Xie, Julian McAuley, and Wayne Xin
Zhao. 2023. Large language models are zero-shot
rankers for recommender systems. arXiv preprint
arXiv:2305.08845 .David D Lewis. 1995. A sequential algorithm for train-
ing text classifiers: Corrigendum and additional data.
InAcm Sigir Forum , volume 29, pages 13–19. ACM
New York, NY , USA.
Bo Li, Yezhen Wang, Tong Che, Shanghang Zhang,
Sicheng Zhao, Pengfei Xu, Wei Zhou, Yoshua Ben-
gio, and Kurt Keutzer. 2020. Rethinking distribu-
tional matching based domain adaptation. arXiv
preprint arXiv:2006.13352 .
Bhavitvya Malik, Abhinav Ramesh Kashyap, Min-Yen
Kan, and Soujanya Poria. 2023. Udapter–efficient
domain adaptation using adapters. arXiv preprint
arXiv:2302.03194 .
Katerina Margatina, Timo Schick, Nikolaos Aletras, and
Jane Dwivedi-Yu. 2023. Active learning principles
for in-context learning with large language models.
arXiv preprint arXiv:2305.14264 .
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in Neural
Information Processing Systems , 35:27730–27744.
Abhinav Ramesh Kashyap, Devamanyu Hazarika, Min-
Yen Kan, and Roger Zimmermann. 2021. Domain
divergences: A survey and empirical analysis. In
Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
pages 1830–1849, Online. Association for Computa-
tional Linguistics.
Alan Ramponi and Barbara Plank. 2020. Neural unsu-
pervised domain adaptation in nlp—a survey. arXiv
preprint arXiv:2006.00632 .
Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:
Sentence embeddings using siamese bert-networks.
arXiv preprint arXiv:1908.10084 .
Marco Tulio Ribeiro and Scott Lundberg. 2022. Adap-
tive testing and debugging of NLP models. In Pro-
ceedings of the 60th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers) , pages 3253–3267, Dublin, Ireland. Associa-
tion for Computational Linguistics.
Jon Saad-Falcon, Omar Khattab, Keshav Santhanam,
Radu Florian, Martin Franz, Salim Roukos, Avirup
Sil, Md Arafat Sultan, and Christopher Potts. 2023.
Udapdr: Unsupervised domain adaptation via llm
prompting and distillation of rerankers. arXiv
preprint arXiv:2303.00807 .
Christopher Schröder, Andreas Niekler, and Martin
Potthast. 2021. Revisiting uncertainty-based query
strategies for active learning with transformers. arXiv
preprint arXiv:2107.05687 .
Burr Settles. 2009. Active learning literature survey.

--- PAGE 12 ---
Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang,
Mai ElSherief, Jieyu Zhao, Diba Mirza, Eliza-
beth Belding, Kai-Wei Chang, and William Yang
Wang. 2019. Mitigating gender bias in natural lan-
guage processing: Literature review. arXiv preprint
arXiv:1906.08976 .
Nandan Thakur, Nils Reimers, Johannes Daxenberger,
and Iryna Gurevych. 2020. Augmented sbert: Data
augmentation method for improving bi-encoders for
pairwise sentence scoring tasks. arXiv preprint
arXiv:2010.08240 .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel Bowman. 2018. GLUE:
A multi-task benchmark and analysis platform for nat-
ural language understanding. In Proceedings of the
2018 EMNLP Workshop BlackboxNLP: Analyzing
and Interpreting Neural Networks for NLP , pages
353–355, Brussels, Belgium. Association for Com-
putational Linguistics.
Jindong Wang, Cuiling Lan, Chang Liu, Yidong
Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun
Zeng, and Philip Yu. 2022. Generalizing to unseen
domains: A survey on domain generalization. IEEE
Transactions on Knowledge and Data Engineering .
Shuohang Wang, Yang Liu, Yichong Xu, Chenguang
Zhu, and Michael Zeng. 2021. Want to reduce
labeling cost? gpt-3 can help. arXiv preprint
arXiv:2108.13487 .
Chenxi Whitehouse, Monojit Choudhury, and Al-
ham Fikri Aji. 2023. Llm-powered data augmen-
tation for enhanced crosslingual performance. arXiv
preprint arXiv:2305.14288 .
Zhisong Zhang, Emma Strubell, and Eduard Hovy. 2022.
A survey of active learning for natural language pro-
cessing. arXiv preprint arXiv:2210.10109 .

--- PAGE 13 ---
A LLM Prompting
A.1 Semantic Similarity
Prompts used : Systems and Question prompt
See Table 7
A.2 Semantic Search : Top K sampling with
Finetuned for Recommendations
Prompts used : Systems and Question prompt
See Table 8, Table 9, Table 9.
B LLM Annotation Accuracy
For different Ground Truth Conditional
Informativeness-based quantiles (i.e. the
metric is computed over base model error with
the ground truth label instead of a deviation with
finetuned model) we show the accuracy of LLM
annotations as compared to the Finetuned and
Base models across different datasets. For Quora
(Fig 3a) we can see that LLMs augmentation
performance is always worse than finetuned
model and is only comparable on the highest
quantiles. Similarly for Wikipedia-USA (Fig 3b)
we can see that LLMs are better than finetuned
model for around 50% of higher quantiles while
for Amazon-Books (Fig 3c) LLMs are always
better than/comparable to finetuned model. For
Amazon-Kitchen (Fig 3d), LLMs are comparable
to finetuned and base model and hence don’t offer
much advantage. This analysis can be used by
practitioner to decide which domains to augment
with LLMs.
C Training Details
C.1 Semantic Similarity
We finetune using quora with learning rate of 1e-4
for 2 epochs for all experiments. We use a batch
size of 32 and have a linearly decay learning rate
scheduler. We use MSE Loss for training. The
initial quora dataset is subsampled by a factor of
10 (i.e. 38400 samples). We further subsample by
10 for active learning setup.
C.2 Semantic Search
Recommendation are trained using a recent state-
of-art algorithm (Dahiya et al., 2023). We also
subsample both the datasets by a factor of 10. We
finetune both models for 100 epochs.D Ground Truth Annotations for
Hard/Easy Target Domain in Quora
See Table 11
E Formalising Semantic Similarity
Given a pair domains of sentences (short texts), X
andY, the task of semantic similarity is concerned
with learning a function f:X ×Y → { 0,1}which
for a given pair of sentence (xi, yj)from the set
X × Y outputs either 1 (or 0) to show that xiis
(or isn’t) semantically same as yj. For symmet-
ric tasks (like duplicate question detection) the set
XandYcan be the same, but we consider the
general case. The semantics are broadly defined
and depend on the target task. For e.g., for task
of duplicate question detection (say on quora) a
good model foutputs whether for a pair of ques-
tion the answer for one of the questions, answers
the second question. For the task of say books rec-
ommendations, semantics might require capturing
the similarity in pairs of sentences in context of
the authors of the books, the books genre, their tar-
get audience etc. We hence mathematically denote
semantics as S:X × Y → { 0,1}.
We consider the subset of these domains as X=
{x0, x1, . . . x |X|} ⊆ X ,Y={y0, y1, . . . y |Y|} ⊆
Y. The similarity between a pair (xi, yj)is hence
S(xi, yj)shortened as sij. For semantic search
a set of examples can be sufficient represented by
subset of pair of indices D0⊆[|X|]×[|Y|]denoted
by
D={(xi, yj, sij) : (i, j)∈D0} (14)
where xi, yjare the pair of sentences and sijis
the semantic similarity. Unlabeled data lacks sij
information.

--- PAGE 14 ---
System Prompt You are an expert in judging the intended answer for short questions on quora. You
use the intended answer to find duplicate questions (i.e. questions having the same
intended answer and answering one of the questions will answer the other question
too. Given a pair of questions from one of these forums you are effectively able to
discern if they are duplicates of each other or not by reasoning about the intended
answer.
Question Prompt Given pairs of questions from (say) Quora, output the pair of questions that are
asking the same question (i.e. have the same intended answer). Reason about the
intended answer to solve this. Here is an example of output "Pair1".
Pair1: %s Pair2: %s Pair3: %s Pair4: %s Pair5: %s Pair6: %s Pair7: %s Pair8: %s
Pair9: %s Pair10: %s
Pairs having duplicates are :
Table 7: Prompts Used for Quora
System Prompt You are an expert on United States centric Wikipedia articles and article titles. You
are able to infer the content and context of an article accurately from the title of
the article alone. Also, given a reference article title, you are able to accurately
discern which articles should be in the ’SeeAlso’ section of the reference article’s
wikipedia page.
Question Prompt You are given the main article title and titles for possible "SeeAlso" articles. You
have to output which <SeeAlsoArticle> is most likely to be in the "SeeAlso"
section of the main article. The topics of the articles are around or relating to
United States somehow. You should infer the content of the articles from their
titles and output SeeAlso articles are closely related to the main article’s topic and
provide additional useful information to the reader. SeeAlso articles should also
help readers explore related areas and should have value to the reader. Output just
the article id (e.g. SeeAlsoArticle1) for the most likely article. Here is an example
of output "SeeAlsoArticle1".
MainArticleTitle: %s SeeAlsoArticle1: %s SeeAlsoArticle2: %s SeeAlsoArticle3:
%s SeeAlsoArticle4: %s SeeAlsoArticle5: %s SeeAlsoArticle6: %s SeeAlsoArti-
cle7: %s SeeAlsoArticle8: %s SeeAlsoArticle9: %s SeeAlsoArticle10: %s
Most likely <SeeAlsoArticle> to be in "SeeAlso" section is :
Table 8: Prompts Used for Wikipedia USA
System Prompt You are an expert on relevance (/similarity) between books sold on e-commerce
websites. Given a reference book, you are able to accurately discern the relevance
of other books to the reference book.
Question Prompt Given a product a customer has recently bought and a list of 10 possible products,
output (product id of) which product is most relevant for the customer. Relevant
products have similar/complementary use cases. Here is an example of output
"Product1".
Question BoughtProduct: %s Product1: %s Product2: %s Product3: %s Product4:
%s Product5: %s Product6: %s Product7: %s Product8: %s Product9: %s Prod-
uct10: %s
Most relevant product is :
Table 9: Prompts Used for Amazon Books

--- PAGE 15 ---
System Prompt You are an expert on ’Kitchen and Dining’ products sold on e-commerce websites
and on judging their utility to a customer. Given a product bought from e-commerce
website, you are able to accurately discern the relevance of other products to the
customer.
Question Prompt Given a product a customer has recently bought and a list of 10 possible products,
output (product id of) which product is most relevant for the customer. Relevant
products have similar/complementary use cases. Here is an example of output
"Product1".
Question BoughtProduct: %s Product1: %s Product2: %s Product3: %s Product4:
%s Product5: %s Product6: %s Product7: %s Product8: %s Product9: %s Prod-
uct10: %s
Most relevant product is :
Table 10: Prompts Used for Amazon Kitchen
0 2 4 6 8
Quantiles 
0.30.40.50.60.70.80.91.0Accuracy
Quora
LLM Acc.
Finetuned Acc.
Base Acc.
(a) LLM vs Finetuned accuracy across quantiles on Quora
Question Pairs
0 2 4 6 8
Quantiles 
0.20.30.40.50.60.70.80.9P@1
Wikipedia (USA)
LLM P@1
Finetuned P@1
Base P@1(b) LLM vs Finetuned accuracy across quantiles on Wikipedia
USA category
0 2 4 6 8
Quantiles 
0.20.30.40.50.60.70.80.9P@1
Amazon (Books)
LLM P@1
Finetuned P@1
Base P@1
(c) LLM vs Finetuned accuracy across quantiles on Amazon
Books category
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
Quantiles 
0.10.20.30.40.50.60.70.80.9P@1
Amazon (Kitchen)
LLM P@1
Finetuned P@1
Base P@1(d) LLM vs Finetuned accuracy across quantiles on Amazon
Kitchen category
Figure 3: LLM vs Finetuned accuracy across quantiles

--- PAGE 16 ---
Data base-inconsistent-sample base-consistent-sample Total
Training set 59.335 ±0.139 99.068 ±0.048 86.824 ±0.038
+ 100% (complete target domain) 78.134 ±0.187 98.229 ±0.084 90.740 ±0.063
+ Random 50% 74.432 ±0.204 98.496 ±0.063 89.893 ±0.123
+ Uncertain 50% 77.083 ±0.248 98.077 ±0.090 90.159 ±0.069
+ base-consistent-sample 50% 51.403 ±0.390 99.499 ±0.018 86.034 ±0.046
+ base-inconsistent-sample 50% 79.801 ±0.142 96.830 ±0.067 89.644 ±0.065
+ Random 16% 69.761 ±0.352 98.732 ±0.014 89.020 ±0.071
+ Uncertain 16% 70.102 ±0.164 98.522 ±0.047 88.735 ±0.060
+ base-consistent-sample 16% 56.082 ±0.350 99.266 ±0.040 86.561 ±0.103
+ base-inconsistent-sample 16% 76.093 ±0.325 97.709 ±0.065 89.482 ±0.098
Table 11: AUC for Quora with different Ground Truth (GT) Annotations.
