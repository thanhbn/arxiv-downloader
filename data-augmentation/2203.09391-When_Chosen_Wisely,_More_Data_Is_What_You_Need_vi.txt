# 2203.09391.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/data-augmentation/2203.09391.pdf
# Kích thước tệp: 1157241 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Khi Được Chọn Một Cách Khôn Ngoan, Dữ Liệu Nhiều Hơn Là Điều Bạn Cần:
Một Chiến Lược Tăng Cường Dữ Liệu Hiệu Quả Mẫu Toàn Cầu
Ehsan Kamallooy
Đại học Alberta
kamalloo@ualberta.ca

Mehdi Rezagholizadeh
Phòng thí nghiệm Noah's Ark của Huawei
mehdi.rezagholizadeh@huawei.com

Ali Ghodsi
Đại học Waterloo
ali.ghodsi@uwaterloo.ca

Tóm tắt
Tăng cường dữ liệu (DA) được biết đến là cải thiện khả năng tổng quát hóa của mạng nơ-ron sâu. Hầu hết các kỹ thuật DA hiện tại một cách ngây thơ thêm một số lượng nhất định các mẫu được tăng cường mà không xem xét chất lượng và chi phí tính toán bổ sung của những mẫu này. Để giải quyết vấn đề này, một chiến lược phổ biến, được áp dụng bởi một số phương pháp DA tiên tiến, là tạo ra hoặc tái cân bằng trọng số các mẫu được tăng cường một cách thích ứng với mục tiêu nhiệm vụ trong quá trình huấn luyện. Tuy nhiên, những phương pháp DA thích ứng này: (1) tốn kém về mặt tính toán và không hiệu quả mẫu, và (2) được thiết kế chỉ cho một cài đặt cụ thể. Trong công trình này, chúng tôi trình bày một kỹ thuật DA toàn cầu, được gọi là Glitter, để vượt qua cả hai vấn đề. Glitter có thể được cắm vào bất kỳ phương pháp DA nào, làm cho việc huấn luyện hiệu quả mẫu mà không hy sinh hiệu suất. Từ một nhóm các mẫu được tăng cường tạo trước, Glitter một cách thích ứng chọn một tập con các mẫu trường hợp xấu nhất với mất mát tối đa, tương tự như DA đối kháng. Mà không thay đổi chiến lược huấn luyện, mục tiêu nhiệm vụ có thể được tối ưu hóa trên tập con được chọn. Các thí nghiệm kỹ lưỡng của chúng tôi trên chuẩn GLUE, SQuAD, và HellaSwag trong ba cài đặt huấn luyện được sử dụng rộng rãi bao gồm huấn luyện nhất quán, tự chưng cất và chưng cất tri thức cho thấy rằng Glitter nhanh hơn đáng kể để huấn luyện và đạt được hiệu suất cạnh tranh, so với các cơ sở mạnh.¹

1 Giới thiệu
Tầm quan trọng không thể phủ nhận của dữ liệu trong học sâu (Sambasivan et al., 2021; Rogers, 2021) và quy trình tốn kém của việc gán nhãn dữ liệu đã thúc đẩy các nhà nghiên cứu tận dụng Tăng cường Dữ liệu (DA) trong một loạt các ứng dụng từ thị giác máy tính (Cubuk et al., 2019; Wang et al., 2020) đến xử lý ngôn ngữ tự nhiên (NLP) bao gồm dịch máy (Sennrich et al., 2016; Shen et al., 2020), hiểu ngôn ngữ (Shen et al., 2020; Qu et al., 2021; Du et al., 2021; Kamalloo et al., 2021), và trả lời câu hỏi (Alberti et al., 2019; Longpre et al., 2019; Shakeri et al., 2020). DA được chỉ ra là hiệu quả trong việc cải thiện khả năng tổng quát hóa của mạng nơ-ron sâu (DeVries and Taylor, 2017; Xie et al., 2020) và trong việc tăng số lượng mẫu huấn luyện đặc biệt trong các chế độ dữ liệu tài nguyên thấp (Sennrich et al., 2016; Zhang et al., 2018). Tuy nhiên, trong NLP, bản chất rời rạc của văn bản tạo ra độ phức tạp bổ sung cho DA vì việc tạo ra văn bản có ý nghĩa ngữ nghĩa từ một văn bản khác là thách thức (Feng et al., 2021).

Các phương pháp DA có thể được phân loại rộng rãi thành các phương pháp nhận biết nhiệm vụ và không nhận biết nhiệm vụ. Các phương pháp DA không nhận biết nhiệm vụ về cơ bản tạo ra văn bản được tăng cường bất kể nhiệm vụ đang xử lý và thường không đảm bảo huấn luyện hoặc tinh chỉnh bổ sung. Chúng có thể dựa trên một số heuristic thủ công (Zhang et al., 2015; Wei and Zou, 2019), dịch ngược (Sennrich et al., 2016; Edunov et al., 2018), hoặc thay thế token từ một mô hình ngôn ngữ được huấn luyện trước (Kobayashi, 2018; Wu et al., 2019; Ng et al., 2020). Mặc dù việc triển khai các phương pháp không nhận biết nhiệm vụ là đơn giản, những phương pháp này không tính đến bất kỳ thông tin cụ thể nhiệm vụ nào, và do đó, hiệu suất của chúng thường bị hạn chế. Mặt khác, các phương pháp DA nhận biết nhiệm vụ có khả năng tạo ra các mẫu được tăng cường, có điều kiện trên mục tiêu nhiệm vụ hạ nguồn (Hu et al., 2019; Xie et al., 2020; Rashid et al., 2021). Những phương pháp này thích nghi các ví dụ được tăng cường cụ thể cho một nhiệm vụ trong đó chúng xây dựng các ví dụ được tăng cường, đôi khi một phần, trong quá trình huấn luyện. Mặc dù có những ưu điểm, chúng thường phát sinh chi phí huấn luyện bổ sung, dẫn đến việc huấn luyện cấm đoán chậm và tốn kém về mặt tính toán.

Nói chung, các vấn đề trung tâm xung quanh các kỹ thuật DA trong NLP có thể được tóm tắt như sau:

--- TRANG 2 ---
Đầu tiên, các phương pháp DA chủ yếu không hiệu quả mẫu trong đó chúng thêm số lượng tùy ý các mẫu được tăng cường vào dữ liệu huấn luyện và một cách ngây thơ kết hợp tất cả chúng vào huấn luyện mà không điều tra có bao nhiêu mẫu được tăng cường thực sự cần thiết. Thứ hai, mặc dù hiệu quả hơn, các phương pháp nhận biết nhiệm vụ nổi tiếng tốn thời gian để huấn luyện. Điều này đặc biệt có vấn đề trong các tập dữ liệu quy mô lớn như SQuAD (Rajpurkar et al., 2016) và MNLI (Williams et al., 2018). Thứ ba, hầu hết các phương pháp DA không phải là toàn cầu vì chúng hoạt động duy nhất với một cài đặt cụ thể—ví dụ, huấn luyện một mạng đơn (Xie et al., 2020), hoặc huấn luyện trong cài đặt giáo viên-học sinh (Rashid et al., 2021). Tổng thể, tầm quan trọng của cả hiệu quả mẫu và hiệu quả huấn luyện cho DA đã thường bị bỏ qua.

Được thúc đẩy bởi các vấn đề trên, trong công trình này, chúng tôi giới thiệu một phương pháp DA toàn cầu, Glitter², có thể được cắm vào bất kỳ phương pháp DA nào để làm cho chúng hiệu quả mẫu, và nhận biết nhiệm vụ mà không hy sinh hiệu suất. Cụ thể, cho một nhóm các mẫu được tăng cường được tạo ra ngoại tuyến, phương pháp được đề xuất của chúng tôi theo một cách tiếp cận minimax (Farnia and Tse, 2016) để chọn một tập con nhỏ với mất mát dự kiến tối đa (bước tối đa hóa) trong quá trình huấn luyện. Mà không có bất kỳ điều chỉnh nào thêm vào thuật toán huấn luyện, mục tiêu nhiệm vụ có thể được tối ưu hóa cho tập con được chọn này (bước tối thiểu hóa).

Những đóng góp chính của chúng tôi trong bài báo này có thể được tóm tắt như sau:

1. Glitter là một phương pháp toàn cầu có thể được áp dụng một cách dễ dàng vào bất kỳ phương pháp DA nào để thực thi hiệu quả mẫu trong khi duy trì (hoặc thậm chí tăng cường) hiệu suất của chúng.

2. Chúng tôi nghĩ ra các chiến lược để thích nghi Glitter cho nhiều cài đặt huấn luyện được sử dụng rộng rãi bao gồm mạng đơn, huấn luyện nhất quán, tự chưng cất và chưng cất tri thức.

3. Thông qua các đánh giá thực nghiệm của chúng tôi, chúng tôi chỉ ra rằng Glitter đạt được hiệu suất vượt trội so với các phương pháp DA tiên tiến trên GLUE, SQuAD, và HellaSwag, trong khi tăng tốc đáng kể việc huấn luyện.

2 Công trình liên quan
2.1 DA không nhận biết nhiệm vụ trong NLP
Các kỹ thuật tăng cường ngữ cảnh (Kobayashi, 2018; Wu et al., 2019) sử dụng các mô hình ngôn ngữ được huấn luyện trước cho DA. Kobayashi (2018) đề xuất các mô hình ngôn ngữ LSTM hai chiều để thay thế từ có điều kiện trên nhãn của văn bản đầu vào của chúng. SSMBA (Ng et al., 2020) và TinyBERT (Jiao et al., 2020) xáo trộn đầu vào bằng cách che một số token, và sau đó, lấy mẫu token từ một mô hình BERT để thay thế các token bị che và tạo ra các mẫu được tăng cường. Dịch Ngược (Sennrich et al., 2016) tăng cường dữ liệu bằng cách sử dụng hai mô hình dịch liên tiếp: mô hình đầu tiên để dịch đầu vào sang một ngôn ngữ đích tùy ý; sau đó, một mô hình thứ hai để dịch kết quả trở lại ngôn ngữ gốc của nó. Mixed-up (Guo et al., 2019) tạo ra các mẫu được tăng cường dựa trên việc nội suy các vector nhúng từ và vector nhúng câu. Shen et al. (2020) giới thiệu một tập hợp các kỹ thuật cắt bỏ mà không đưa vào các khoảng liên tiếp của ma trận nhúng ở cấp độ token, cấp độ tính năng và cấp độ khoảng. EDA (Wei and Zou, 2019) bao gồm các phép toán đơn giản ở cấp độ từ bao gồm thay thế từ đồng nghĩa, xóa ngẫu nhiên, chèn ngẫu nhiên và hoán đổi ngẫu nhiên.

2.2 DA nhận biết nhiệm vụ trong NLP
Một cách tiếp cận để tận dụng thông tin cụ thể nhiệm vụ là gán các trọng số khác nhau cho các mẫu được tăng cường dựa trên tác động cá nhân của chúng lên mô hình (Yi et al., 2021). Mặc dù hiệu quả, cơ chế tái cân bằng trọng số phần lớn bỏ qua hiệu quả mẫu. Wu et al. (2019) giới thiệu một cách tiếp cận che-và-tái-tạo, cụ thể là c-BERT, tinh chỉnh một mô hình BERT được huấn luyện trước để dự đoán các token tương thích nhãn. CoDA (Qu et al., 2021) kết hợp các biến đổi bảo toàn nhãn khác nhau với huấn luyện đối kháng cùng với một mục tiêu chính quy hóa tương phản. Tăng cường dữ liệu không giám sát (UDA; Xie et al. 2020) sử dụng các phương pháp DA có sẵn và thêm một mất mát nhất quán phụ trợ vào mục tiêu huấn luyện. Tuy nhiên, UDA không hiệu quả mẫu và nó được thiết kế chỉ cho cài đặt mạng đơn; cách triển khai nó trong các kịch bản huấn luyện khác như chưng cất tri thức không rõ ràng. Hu et al. (2019) đề xuất một kỹ thuật dựa trên học tăng cường nơi hàm phần thưởng được định nghĩa dựa trên việc các mẫu được tăng cường tạo ra có bảo toàn nhãn hay không.

2.3 DA cho KD
KD (Bucilua et al., 2006; Hinton et al., 2015), ban đầu được đề xuất như một kỹ thuật nén mô hình, nhằm mục đích chuyển giao tri thức của một mô hình đã được huấn luyện, được gọi là giáo viên, sang một mô hình học sinh nhỏ hơn hoặc cùng kích thước. Một số nghiên cứu phát hiện ra rằng DA có thể tăng cường đáng kể hiệu suất của KD trong NLP. TinyBERT (Jiao et al., 2020) sử dụng một kỹ thuật DA không nhận biết nhiệm vụ cho việc tinh chỉnh cụ thể nhiệm vụ của nó. Kamalloo et al. (2021) và Rashid et al. (2021) chỉ ra rằng DA cũng có thể được điều chỉnh cho KD. Cụ thể, MATE-KD (Rashid et al., 2021) điều chỉnh một mô hình ngôn ngữ có mặt nạ riêng biệt để tạo ra các mẫu được tăng cường với độ phân kỳ tối đa. Kamalloo et al. (2021) và Du et al. (2021) sử dụng truy xuất kNN để lấy các mẫu được tăng cường từ một ngân hàng câu khổng lồ.

Glitter khác với công trình trước đó ở chỗ nó đồng thời tập trung vào hiệu quả mẫu, và tính toàn cầu sao cho nó có thể được sử dụng tự do trong bất kỳ cài đặt huấn luyện nào.

3 Phương pháp
Trong phần này, chúng tôi giới thiệu phương pháp DA nhận biết nhiệm vụ của chúng tôi, Glitter, nhằm mục đích sử dụng một số lượng hiệu quả các mẫu được tăng cường mà không hy sinh hiệu suất. Chiến lược được đề xuất của chúng tôi là bất khả tri với các phương pháp DA; nó có thể được cắm một cách liền mạch vào bất kỳ phương pháp DA nào với bất kỳ cài đặt huấn luyện nào để thực thi hiệu quả mẫu.

Các phương pháp DA dựa trên học tập hiện tại huấn luyện một mô hình DA riêng biệt và thích nghi đầu ra của nó cho một hàm mục tiêu cụ thể hoàn toàn phụ thuộc vào nhiệm vụ:

min `DA(M(θ(x;φ);θ))
x'= θ(x;φ) (1)

nơi `DA(·) là một hàm mất mát, hướng tới mục tiêu của nhiệm vụ, θ(·;φ) là mô hình DA với các tham số có thể huấn luyện φ, và M(·;θ) đề cập đến mô hình ban đầu, được tham số hóa bởi θ.

Trái ngược với DA dựa trên học tập, chúng tôi đề xuất tạo ra nhiều ứng viên được tăng cường bằng cách sử dụng bất kỳ phương pháp DA tùy ý nào trước khi huấn luyện, và một cách thích ứng chọn các ứng viên phù hợp nhất trong quá trình huấn luyện. Thủ tục này không đưa vào các tham số có thể huấn luyện bổ sung vào huấn luyện, và quan trọng hơn, có khả năng tự động bỏ qua các ví dụ được tăng cường không cần thiết. Cho {(xi,yi)}ᴺᵢ₌₁ ⊆ {(X,Y)} đại diện cho dữ liệu huấn luyện sao cho một cặp xi ∈ X và yi ∈ Y là một ví dụ đầu vào và nhãn tương ứng của nó. Giả sử một nhóm K ví dụ được tăng cường, X'⁽ⁱ⁾ = {x'ₖ⁽ⁱ⁾}ᴷₖ₌₁, được lấy mẫu từ một mô hình DA nào đó cho mỗi ví dụ huấn luyện (xi,yi) ∈ (X,Y). Lưu ý rằng Glitter không áp đặt hạn chế nào về cách tăng cường dữ liệu huấn luyện; các mẫu được tăng cường có thể được tạo ra qua một hoặc thậm chí nhiều mô hình DA.

Lựa chọn mẫu. Cho một nhóm các mẫu được tăng cường, cách tiếp cận của chúng tôi là một cách thích ứng chọn các ứng viên tốt nhất theo các tiêu chí được định nghĩa cụ thể. Được truyền cảm hứng bởi cách tiếp cận minimax (Farnia and Tse, 2016; Volpi et al., 2018), cơ chế lựa chọn của chúng tôi dựa trên việc tìm các mẫu được tăng cường trường hợp xấu nhất top-k₁ (trong số K) từ tập X'. Tối thiểu hóa hàm mất mát mô hình chính trên những mẫu được tăng cường trường hợp xấu nhất này sẽ giúp cải thiện khả năng tổng quát hóa của mô hình (Volpi et al., 2018). Để xếp hạng các mẫu được tăng cường, chúng tôi đánh giá X'⁽ⁱ⁾ dựa trên một hàm khoảng cách với mẫu huấn luyện ban đầu tương ứng, xi, trong không gian tiềm ẩn của mô hình:

X̃'⁽ⁱ⁾ = topₖ₁{ℓeval(M(xi;θ);M(X'⁽ⁱ⁾;θ))}
X̃'⁽ⁱ⁾ = {x'ⱼ⁽ⁱ⁾}ₖ₁ⱼ₌₁ ⊆ X'⁽ⁱ⁾ (2)

nơi topₖ₁(·) biểu thị trả về các chỉ số top-k₁ dựa trên các điểm số được trả về bởi ℓeval, X̃'⁽ⁱ⁾ là tập hợp k₁ mẫu được tăng cường được chọn cho xi; ℓeval(·) là mất mát đánh giá được xác định qua mục tiêu nhiệm vụ.

Cập nhật các tham số mô hình. Sau khi có được các mẫu được tăng cường top-k₁, chúng tôi nhóm chúng với các mẫu huấn luyện ban đầu, {xi} ∪ X̃'⁽ⁱ⁾, và sau đó, cập nhật các tham số mô hình chỉ dựa trên tập hợp được chọn các mẫu được tăng cường này trên mất mát ban đầu:

L(θ) = ∑ᴺᵢ₌₁ ℓtask(M(xi;θ);M(X̃'⁽ⁱ⁾;θ);yi)
θt ← θt-1 - α∇θ(L(θ))|θt-1 (3)

nơi N là số lượng mẫu huấn luyện, α là tỷ lệ học, và ℓtask(·) là mất mát nhiệm vụ cuối cùng—ví dụ, entropy chéo (ce) cho phân loại—được tính toán trên cả dữ liệu ban đầu và dữ liệu được tăng cường được chọn. Trong phần còn lại của phần này, chúng tôi thảo luận về cách Glitter có thể được áp dụng cho các cài đặt huấn luyện phổ biến bao gồm DA chung cho mạng đơn, và DA cho cài đặt giáo viên-học sinh (KD). Lưu ý rằng Glitter không bị hạn chế trong những cài đặt này và có thể được thích nghi cho các cài đặt khác như DAIR (Huang et al., 2022).

3.1 DA chung cho mạng đơn
Chúng tôi xem xét ba cài đặt tiềm năng cho kịch bản mạng đơn: (1) Mạng đơn chung, (2) Tự chưng cất, và (3) Huấn luyện nhất quán.

Mạng đơn chung. Trong cài đặt này, các mẫu được tăng cường được khai thác theo cách bán giám sát nơi chúng ta có thể đánh giá chúng dựa trên độ phân kỳ của đầu ra dự đoán M(x'ₖ⁽ⁱ⁾;θ) = p(y|x'ₖ⁽ⁱ⁾;θ) từ nhãn sự thật cơ bản hoặc dự đoán của mẫu huấn luyện ban đầu tương ứng M(xi;θ) = p(y|xi;θ) sử dụng mất mát entropy chéo, ℓce:

ℓeval = ℓce(yi;M(x'ₖ⁽ⁱ⁾;θ))
hoặc
ℓeval = ℓce(M(xi;θ);M(x'ₖ⁽ⁱ⁾;θ)): (4)

Tiêu chí entropy chéo không phải là lựa chọn duy nhất ở đây. Các lựa chọn khác cho ℓeval bao gồm (nhưng không giới hạn) mất mát tiêu điểm (Lin et al., 2017), và mất mát nghiêng (Li et al., 2021).

Đối với mất mát nhiệm vụ cuối cùng, ℓtask chúng ta có thể triển khai một mất mát entropy chéo tiêu chuẩn trên cả mẫu huấn luyện và các mẫu được tăng cường được chọn tương ứng của chúng:

ℓtask = ℓce(yi;M(xi;θ))
+ (1/k₁) ∑ₓ∈X̃'⁽ⁱ⁾ ℓce(yi;M(x;θ)): (5)

Huấn luyện nhất quán (CT; Xie et al. 2020). Trong cấu hình này, chúng ta có thể sử dụng cùng ℓeval được giới thiệu trong Eq. (4). Kết quả là, phương pháp của chúng tôi một cách tự nhiên chọn các mẫu được tăng cường không nhất quán nhất top-k₁ cho mỗi mẫu huấn luyện. Sau đó, mạng được tối ưu hóa để tạo ra dự đoán cho các mẫu đầu vào được tăng cường nhất quán với dự đoán của các mẫu huấn luyện ban đầu tương ứng:

ℓᶜᵀtask = ℓce(yi;M(xi;θt))
+ (1/k₁) ∑ₓ∈X̃'⁽ⁱ⁾ ℓce(M(xi;θt-1);M(x;θt)): (6)

Như được nêu bởi Xie et al. (2020), thuật ngữ thứ hai trong Eq. (6) tận dụng dự đoán trước đó của mạng cho mỗi ví dụ huấn luyện.

Tự chưng cất (Self-KD). Trong Self-KD, chúng ta đầu tiên huấn luyện một mô hình, và sau đó, sử dụng nó (M(·;θ)) như một giáo viên để huấn luyện một mô hình giống hệt nhưng được khởi tạo từ đầu bằng cách sử dụng KD (Furlanello et al., 2018). Cách điều chỉnh ℓeval và ℓtask được chi tiết trong §3.2.

3.2 DA cho giáo viên-học sinh (KD)
Trong cài đặt này, chúng ta có một mô hình giáo viên, T(·;θ̄) với các tham số θ̄ đã được huấn luyện trên dữ liệu huấn luyện, cùng với một mô hình học sinh, M(·;θ), mà chúng ta nhằm mục đích huấn luyện. Tiêu chí lựa chọn cho các mẫu được tăng cường là tối đa hóa độ phân kỳ giữa giáo viên và học sinh:

ℓᴷᴰeval = ℓKL(T(x'ₖ⁽ⁱ⁾;θ̄);M(x'ₖ⁽ⁱ⁾;θ)) (7)

nơi ℓKL đề cập đến độ phân kỳ KL. Sau khi chọn các mẫu được tăng cường độ phân kỳ tối đa, sau đó chúng ta tính toán mất mát KD như sau:

ℓᴷᴰtask = ℓce(yi;M(xi;θ))
+ (1-λ) · (1/(k₁+1)) ∑ₓ∈{xi}∪X̃'⁽ⁱ⁾ ℓKL(T(x;θ̄);M(x;θ)) (8)

nơi λ là một siêu tham số.

--- TRANG 4 ---
4 Thí nghiệm
4.1 Thiết lập
Để kết hợp dữ liệu được tăng cường không nhãn vào huấn luyện, chúng tôi áp dụng CT (Xie et al., 2020) và KD (Hinton et al., 2015). Với mục đích này, chúng tôi tiến hành thí nghiệm dưới hai cài đặt:

Độc lập nơi chúng tôi huấn luyện một mô hình đơn trên dữ liệu được tăng cường. Trong cài đặt này, chúng tôi tìm cách trả lời hai câu hỏi: (1) DA có khả năng cải thiện khả năng tổng quát hóa mô hình đến mức nào? (2) Hiệu quả mẫu của Glitter có làm tổn hại hiệu suất không? Với mục đích này, chúng tôi tinh chỉnh RoBERTa base (Liu et al., 2019) sử dụng CT và Self-KD trên dữ liệu được tăng cường.

Chưng cất nơi chúng tôi chưng cất DistilRoBERTa (Sanh et al., 2019) (học sinh) từ RoBERTa Large (Liu et al., 2019) (giáo viên) sử dụng dữ liệu được tăng cường. Lưu ý rằng giáo viên đã được huấn luyện trên dữ liệu ban đầu và DA chỉ tham gia trong quá trình chưng cất mô hình học sinh. Mục tiêu của chúng tôi ở đây là điều tra xem DA có phải là một phương tiện hiệu quả trong chuyển giao tri thức để hạn chế khoảng cách năng lực (Cho and Hariharan, 2019) giữa một mô hình lớn và một mô hình nhỏ không.

Trong cả hai cài đặt, chúng tôi lấy mô hình hoạt động tốt nhất trên tập phát triển và đánh giá nó trên tập kiểm tra (được mô tả bởi Test). Ngoài ra, đối với cài đặt mô hình độc lập, chúng tôi cũng báo cáo kết quả trên tập phát triển khi các mô hình được huấn luyện chỉ trong 5 epoch (được mô tả bởi Dev), tương tự như CoDA (Qu et al., 2021), để so sánh với các đường cơ sở. Kết quả Dev của chúng tôi là trung bình của 10 lần chạy với các hạt giống khác nhau. Các chi tiết triển khai và siêu tham số được cung cấp trong §A.

4.1.1 Các phương pháp DA
Chúng tôi tận dụng ba phương pháp tăng cường văn bản được sử dụng rộng rãi:

1. EDA (Wei and Zou, 2019)³: Chúng tôi ngẫu nhiên thay thế 5% các token bằng từ đồng nghĩa của chúng và ngẫu nhiên xóa đến 10%.

2. Dịch Ngược (BT; Sennrich et al. 2016): Chúng tôi sử dụng fairseq (Ott et al., 2019) để dịch câu sang tiếng Đức và sau đó trở lại tiếng Anh. Chúng tôi thực hiện lấy mẫu nucleus (Holtzman et al., 2020) với p = 0,9 cho cả hai bản dịch. Chúng tôi thấy rằng p = 0,6 hoạt động tốt hơn trên phân loại cảm xúc.

3. Che và Tái tạo (MR; Ng et al. 2020): Chúng tôi ngẫu nhiên che 15% các token và xây dựng một câu mới bằng cách lấy mẫu từ một BERT Large được huấn luyện trước cho các token bị che. Chúng tôi áp dụng lấy mẫu top-k với k = 20 để chọn các token mới. Đối với MNLI, chúng tôi có được kết quả tốt hơn với lấy mẫu top-10.

Đối với mỗi phương pháp tăng cường, chúng tôi tạo ra 12 ví dụ được tăng cường cho mỗi thể hiện huấn luyện cho tất cả các tập dữ liệu, trừ các tập dữ liệu lớn—tức là, MNLI, QQP, và SQuAD—nơi số lượng ví dụ được tăng cường là 8 cho mỗi ví dụ huấn luyện.

4.1.2 Đường cơ sở
Vì hai môi trường—tức là, độc lập và chưng cất—khác nhau về bản chất, chúng tôi so sánh Glitter với các đường cơ sở khác nhau cho mỗi môi trường. Đối với cả hai, Vanilla-DA lấy tất cả dữ liệu được tăng cường vào tài khoản mà không có dự trữ là đường cơ sở đầu tiên.

Các đường cơ sở cho cài đặt độc lập là: CoDA (Qu et al., 2021), MMEL (Yi et al., 2021), và HiddenCut (Chen et al., 2021). Và cho chưng cất, chúng tôi xem xét MATE-KD (Rashid et al., 2021).

4.2 GLUE
Chuẩn GLUE (Wang et al., 2019) là một bộ được biết đến gồm chín⁴ nhiệm vụ nhằm mục đích đánh giá các mô hình hiểu ngôn ngữ tự nhiên. Chúng tôi trình bày kết quả kiểm tra ở chế độ chưng cất trong Bảng 1. Glitter nhất quán vượt trội Vanilla-DA, trong khi nó nhanh hơn để huấn luyện. Cụ thể, Glitter đạt được sự ngang bằng với Vanilla-DA cho EDA về điểm trung bình tổng thể, trong khi ghi điểm cao hơn +0,2% và +0,4% cho BT và MR, tương ứng. Chúng tôi quan sát rằng chỉ trong một vài trường hợp Vanilla-DA một cách không đáng kể vượt trội Glitter—ví dụ, trên MRPC, và STS-B cho BT. Tuy nhiên, Glitter 8x/1x huấn luyện nhanh hơn 50% so với Vanilla-DA 8x trung bình, và nhanh hơn 30% cho 8x/2x. Ngoài ra, Glitter vượt trội MATE-KD +0,2% trong điểm số tổng thể. Không giống như Glitter, MATE-KD đưa vào các tham số bổ sung vào mô hình trong quá trình huấn luyện và nó huấn luyện chậm hơn đáng kể vì nó tạo ra các ví dụ được tăng cường một cách tức thời. Hơn nữa, Bảng 1 minh họa rằng MR mang lại kết quả kiểm tra tốt nhất trên ba phương pháp DA trừ SST nơi BT dẫn đến kết quả tốt hơn. Dựa trên quan sát này, chúng tôi báo cáo kết quả trên dữ liệu được tăng cường MR cho tất cả các tập dữ liệu GLUE trừ SST trong phần còn lại của các thí nghiệm của chúng tôi.

Đối với chế độ độc lập, Bảng 2 và 3 trình bày kết quả trên kiểm tra và phát triển, tương ứng. Tương tự như chưng cất, Glitter vượt trội Vanilla-DA +0,5% cho cả self-KD và CT. Self-KD mang lại kết quả tốt hơn CT trên tất cả các nhiệm vụ GLUE trừ CoLA. CT thất bại trên hầu hết các nhiệm vụ GLUE, so với kết quả không DA—tức là, 2 hàng đầu trong Bảng 2. Đây là lý do tại sao, chúng tôi chỉ đánh giá Glitter với self-KD trên dữ liệu phát triển. Glitter đạt được lợi ích hiệu suất vượt trội, so với tất cả ba đường cơ sở trên tất cả các tập dữ liệu trừ QNLI. Ưu điểm chính của Glitter là thủ tục huấn luyện vẫn nguyên vẹn.

4.2.1 Tổng quát hóa ngoài miền
Chúng tôi cũng đánh giá Glitter trên các tập dữ liệu OOD. Với mục đích này, chúng tôi kiểm tra các mô hình của chúng tôi, đã được huấn luyện trên các nhiệm vụ GLUE, trên các tập dữ liệu OOD có phân phối dữ liệu khác với dữ liệu ban đầu. Cụ thể, đây là các tập dữ liệu OOD được chọn của chúng tôi:

• SST: IMDb (Maas et al., 2011), IMDb-Cont. (Gardner et al., 2020), và IMDb-CAD (Kaushik et al., 2020), như đã thực hiện trong Chen et al. (2021). Mặc dù cả tập dữ liệu SST và IMDb đều được thu thập trên đánh giá phim, các đánh giá IMDb có xu hướng dài hơn đáng kể so với các câu SST.

• STS-B: SICK (Marelli et al., 2014), một tập dữ liệu liên quan ngữ nghĩa, được tạo ra từ chú thích hình ảnh và video. SICK và STS-B được thu thập trên các miền gần như giống hệt nhau, nhưng từ các nguồn khác nhau.

• QQP: PAWS QQP (Zhang et al., 2019), tương tự như Chen et al. (2021), và MQP (McCreery et al., 2020), một tập dữ liệu tương tự câu hỏi y tế.

--- TRANG 5 ---
Hình 1: Minh họa Glitter (từ trái sang phải): đầu tiên, tạo ra các mẫu được tăng cường từ các kỹ thuật DA khác nhau; thứ hai, hình thành một nhóm mẫu X'(i); thứ ba, đánh giá các mẫu được tăng cường bằng cách sử dụng mất mát ℓeval(); thứ tư, lọc các mẫu top-k₁ dựa trên ℓeval() tương ứng của chúng; thứ năm, cập nhật các tham số của mô hình bằng cách tối thiểu hóa mất mát nhiệm vụ ℓtask(·).

--- TRANG 6 ---
[Bảng 1: Kết quả kiểm tra của thí nghiệm chưng cất trên GLUE]

--- TRANG 7 ---
[Bảng 2: Kết quả kiểm tra của thí nghiệm độc lập trên GLUE sử dụng RoBERTa base]

[Bảng 3: Kết quả phát triển của thí nghiệm độc lập trên GLUE sử dụng RoBERTa base]

• MNLI: SciTail (Khot et al., 2018), được thu thập từ các câu hỏi khoa học cấp trường học, và tương tự như Chen et al. (2021), A-NLI (Nie et al., 2020), và HANS (McCoy et al., 2019).

• RTE: HANS (McCoy et al., 2019).

Bảng 10 trong §B.1 trình bày kết quả OOD cho chế độ chưng cất. Glitter vượt trội Vanilla-DA trong hầu hết các trường hợp, và ngang bằng với nó cho gần như phần còn lại. Các ngoại lệ duy nhất là IMDb-Cont., MQP, và PAWS QQP nơi Vanilla-DA vượt trội Glitter khoảng 1% trung bình. Ngoài ra, tất cả các mô hình không tổng quát hóa tốt cho PAWS QQP và A-NLI vì hiệu suất của chúng dưới hiệu suất lớp đa số. Hơn nữa, một DistilRoBERTa được tinh chỉnh đạt được hiệu suất OOD tốt nhất trên HANS, nhấn mạnh rằng DA thực sự không hữu ích cho độ chính xác OOD trên HANS.

Bảng 3 (phía bên phải) báo cáo kết quả OOD cho các mô hình độc lập. Kết quả hoàn chỉnh được trình bày trong §B.2—tức là, Bảng 11 trên kiểm tra và Bảng 12 trên phát triển. Glitter áp đảo vượt trội tất cả các đường cơ sở với một vài ngoại lệ. Trong kết quả phát triển, mô hình được tinh chỉnh không có DA đạt được khả năng tổng quát hóa OOD tốt nhất trên IMDb, và SciTail, trong khi HiddenCut ghi điểm cao nhất trên A-NLI với biên độ 1%. Tương tự, trong kết quả kiểm tra, Glitter theo sau Self-KD không có DA trên IMDb, IMDb-CAD, và SciTail.

4.3 HellaSwag
HellaSwag (Zellers et al., 2019) là một tập dữ liệu cho suy luận thông thường có tình huống liên quan đến việc chọn kết thúc tốt nhất cho một bối cảnh. Chúng tôi tăng cường các bối cảnh trong HellaSwag chỉ sử dụng BT để đảm bảo rằng các lựa chọn vẫn có ý nghĩa cho các bối cảnh được tăng cường. Vì kết quả độc lập của chúng tôi đã nhất quán với kết quả chưng cất, chúng tôi báo cáo kết quả của chúng tôi chỉ ở chế độ chưng cất. Theo kết quả của chúng tôi được thể hiện trong Bảng 4, Glitter thoải mái vượt trội Vanilla-DA với biên độ +2,3%.

4.4 SQuAD
SQuAD (Rajpurkar et al., 2016) là một chuẩn đọc hiểu được đám đông tạo ra bao gồm hơn 100K câu hỏi, được suy ra từ các đoạn văn Wikipedia. Mục tiêu nhiệm vụ là trích xuất một khoảng trả lời từ một cặp câu hỏi/đoạn văn được cho. Chúng tôi tăng cường các câu hỏi trong SQuAD v1.1 chỉ sử dụng BT để đảm bảo rằng câu trả lời vẫn có thể được tìm thấy trong đoạn văn được cho cho các câu hỏi được tăng cường. Tương tự như HellaSwag, chúng tôi báo cáo kết quả của chúng tôi chỉ ở chế độ chưng cất. Như được hiển thị trong Bảng 4, Glitter vượt trội Vanilla-DA +1,8% trong độ chính xác khớp chính xác trên tập phát triển.

Chúng tôi cũng đánh giá các mô hình được huấn luyện của chúng tôi dưới sự chuyển dịch phân phối bằng cách kiểm tra chúng trên các tập dữ liệu QA từ bốn miền khác nhau: Wikipedia, New York Times, Reddit, và đánh giá sản phẩm Amazon (Miller et al., 2020). Kết quả OOD được trình bày trong Bảng 5. Glitter nhất quán vượt trội Vanilla-DA trong tất cả bốn miền.

5 Nghiên cứu loại bỏ và thảo luận
Trong phần này, chúng tôi nhằm mục đích trả lời các câu hỏi sau:

--- TRANG 8 ---
[Bảng 4: Kết quả phát triển của thí nghiệm chưng cất trên hai nhiệm vụ hạ nguồn]

[Bảng 5: Kết quả OOD cho các mô hình được huấn luyện trên SQuAD và được kiểm tra trên các tập dữ liệu QA từ bốn miền khác nhau]

• Thời gian huấn luyện của Glitter so với Vanilla-DA như thế nào?

• Thay vì chọn dữ liệu được tăng cường một cách thích ứng trong quá trình huấn luyện, chúng ta có thể xử lý trước chúng để loại bỏ các ví dụ không cần thiết trước khi huấn luyện không?

• Có bao nhiêu ví dụ được tăng cường cần thiết để Glitter hoạt động?

• Chiến lược lựa chọn của chúng tôi dựa trên sắp xếp ℓeval trong Glitter có quan trọng không?

Với mục đích này, chúng tôi tiến hành một phân tích chi tiết trên 4 nhiệm vụ GLUE—tức là, SST, MRPC, QNLI, và RTE. Chúng tôi huấn luyện các mô hình dựa trên Vanilla-DA và Glitter sử dụng Self-KD và kiểm tra chúng trên tập phát triển (cài đặt phát triển).

Phân tích thời gian chạy. Trong suốt các thí nghiệm của chúng tôi trong §4, chúng tôi so sánh Glitter với Vanilla-DA khi số lượng tăng cường tương tự cho cả hai phương pháp—tức là, 8x. Một câu hỏi tự nhiên là: cả hai phương pháp DA sẽ hoạt động như thế nào với ít dữ liệu được tăng cường hơn? Với mục đích này, chúng tôi thay đổi kích thước tăng cường từ 1x đến 8x và huấn luyện các mô hình Vanilla-DA khác nhau trên mỗi tập dữ liệu được tăng cường. Chúng tôi đo thời gian huấn luyện trung bình cho mỗi epoch cho tất cả các mô hình. Hình 2 minh họa độ chính xác phát triển khi thời gian huấn luyện tăng. Tốc độ huấn luyện của Glitter 8x/2x hơi nhanh hơn Vanilla-DA 6x trên SST, MRPC, và QNLI và cho Glitter 8x/1x, nhanh hơn Vanilla-DA 4x trên RTE. Glitter vượt trội cả hai trên tất cả các tập dữ liệu.

Hiệu quả của việc xử lý trước dữ liệu được tăng cường. Chúng tôi suy đoán rằng Glitter không cần bất kỳ kỹ thuật dữ liệu nào trên các ví dụ được tăng cường để có được lợi ích hiệu suất ưa thích. Tuy nhiên, Vanilla-DA có thể yêu cầu một số xử lý trước bằng cách loại bỏ dữ liệu có thể nhiễu để trở nên hiệu quả hơn. Để điều tra điều này, chúng tôi khai thác hai kỹ thuật xử lý trước:

[Bảng 6: Kết quả phát triển của self-KD thể hiện hiệu quả của các kỹ thuật xử lý trước khác nhau để lọc các ví dụ được tăng cường trên 4 nhiệm vụ GLUE]

kỹ thuật: (1) Lọc dựa trên độ tin cậy: Các ví dụ được tăng cường mà độ tin cậy của mô hình dưới ngưỡng tối thiểu được loại bỏ, (2) Tăng cường bảo toàn nhãn (LP): Các ví dụ được tăng cường mà mô hình dự đoán nhãn khác với ví dụ ban đầu được loại bỏ. Kết quả, được báo cáo trong Bảng 6, cho thấy không có lợi ích hiệu suất có ý nghĩa bằng các kỹ thuật xử lý trước này. Đối với Vanilla-DA, ngưỡng độ tin cậy tối thiểu 0,7 hoạt động hơi tốt hơn vì nó mang lại cải thiện nhỏ trên MRPC (+0,3%) và QNLI (+0,1%), nhưng vẫn thấp hơn Glitter. Mặt khác, việc áp dụng những kỹ thuật này hơi làm xấu hiệu suất của Glitter trong hầu hết các trường hợp. Các cải thiện duy nhất là +0,1% trên QNLI cho LP và λ = 0,7.

Hiệu quả của kích thước tăng cường trong Glitter. Chúng tôi khám phá cách kích thước tăng cường ảnh hưởng đến hiệu suất của Glitter. Trong suốt các thí nghiệm của chúng tôi, chúng tôi cố định kích thước tăng cường thành 8x, nhưng bây giờ, chúng tôi giảm kích thước tăng cường K xuống 6x và 4x, trong khi giữ nguyên kích thước lựa chọn k₁ như trước—tức là, 1 cho RTE, và 2 cho phần còn lại. Kết quả của chúng tôi, được hiển thị trong Bảng 7, cho thấy rằng khi K trở nên gần với k₁, hiệu suất của Glitter giảm. Tuy nhiên, đối với một tăng cường đủ lớn, Glitter bắt đầu tỏa sáng. Đối với SST, và MRPC, con số kỳ diệu là 8x, trong khi đối với QNLI, và RTE, Glitter hoạt động tốt nhất trên 6x. Một tham số khác trong Glitter là kích thước lựa chọn k₁. Chúng tôi thấy rằng cho tất cả các nhiệm vụ, giá trị tốt nhất có thể được chọn từ {1,2} (2 theo mặc định). Sử dụng phương pháp này, việc điều chỉnh k₁ là đơn giản và không áp đặt độ phức tạp bổ sung cho phương pháp của chúng tôi.

Hiệu quả của chiến lược lựa chọn trong Glitter. Trong phần này, mục tiêu của chúng tôi là đánh giá xem thuật toán lựa chọn được đề xuất của chúng tôi có quan trọng trong Glitter không. Với mục đích này, chúng tôi lấy mẫu các ví dụ được tăng cường ngẫu nhiên tại mỗi lần lặp, được gọi là Glitter-Rnd, thay vì chọn các ví dụ trường hợp xấu nhất. Như được minh họa trong Bảng 7 (hai hàng cuối), hiệu suất giảm trên tất cả các tập dữ liệu—tức là, 0,2% trên QNLI, và hơn 1% trên phần còn lại, xác nhận hiệu quả của thuật toán lựa chọn của chúng tôi.

--- TRANG 9 ---
[Hình 2: Phân tích thời gian chạy của DA khi huấn luyện RoBERTa base sử dụng self-KD]

[Bảng 7: Kết quả phát triển của self-KD để nghiên cứu hiệu quả của kích thước tăng cường và thuật toán lựa chọn cho 4 nhiệm vụ GLUE]

6 Kết luận
Trong công trình này, chúng tôi đã đề xuất một kỹ thuật DA toàn cầu, cụ thể là Glitter, có thể được áp dụng tự do vào bất kỳ kỹ thuật DA nào để thực thi hiệu quả mẫu mà không đưa vào các tham số bổ sung hoặc thay đổi thủ tục huấn luyện. Chúng tôi đã đánh giá một cách toàn diện Glitter trên một loạt các nhiệm vụ NLU và trong các cài đặt được sử dụng rộng rãi khác nhau bao gồm huấn luyện nhất quán, tự chưng cất và chưng cất tri thức và đã chứng minh lợi ích hiệu quả đáng kể mà không làm giảm hiệu quả. Mở rộng Glitter cho các mô hình tự hồi quy cho dịch máy và tóm tắt trừu tượng là một hướng thú vị cho công trình tương lai.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo được duy trì nguyên với định dạng gốc]

--- TRANG 10 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 11 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 12 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 13 ---
A Chi tiết triển khai
A.1 Chi tiết tinh chỉnh
Chúng tôi áp dụng RoBERTa (Liu et al., 2019) và DistilRoBERTa (Sanh et al., 2019) được huấn luyện trước có sẵn công khai—sử dụng thư viện Huggingface Transformers (Wolf et al., 2020) và thư viện Pytorch Lightning⁵.

Đối với cài đặt kiểm tra, mô hình được đánh giá trên dữ liệu phát triển một lần cho mỗi epoch cho các tập dữ liệu nhỏ và hai lần cho mỗi epoch cho các tập dữ liệu lớn—tức là, SST-2, MNLI, QNLI, SQuAD, và HellaSwag. Mô hình hoạt động tốt nhất được chọn để kiểm tra.

Lịch trình tỷ lệ học của chúng tôi theo một bộ lập lịch suy giảm tuyến tính với một giai đoạn khởi động, được chỉ định như một tỷ lệ của tổng số bước huấn luyện. Số epoch tối đa được đặt thành 20 cho tất cả các nhiệm vụ trừ SQuAD, theo (Mosbach et al., 2021). Đối với các tập dữ liệu lớn, chúng tôi dừng sớm với độ kiên nhẫn 10. Tỷ lệ học và kích thước lô được điều chỉnh cho mỗi nhiệm vụ riêng biệt. Chi tiết về các siêu tham số được tóm tắt trong Bảng 9. Chúng tôi chạy các thí nghiệm RoBERTa base với các siêu tham số tương tự, nhưng với những ngoại lệ này: Trên QNLI, tỷ lệ học, kích thước lô, và suy giảm trọng số được đặt thành 3e-5, 64, và 0,1; tỷ lệ khởi động được đặt thành 0,06 trên QQP.

Đối với các thí nghiệm phát triển, chúng tôi theo CoDA (Qu et al., 2021) trên các nhiệm vụ GLUE. Cụ thể, chúng tôi huấn luyện mô hình trong 5 epoch với kích thước lô 32, tỷ lệ học 1e-5, tỷ lệ khởi động 0,06, suy giảm trọng số 0,1, và suy giảm tỷ lệ học tuyến tính. Đối với SQuAD, và HellaSwag, các siêu tham số được chi tiết trong Bảng 8.

Tất cả các thí nghiệm được tiến hành trên hai GPU Nvidia Tesla V100.

[Bảng 8: Siêu tham số của DistilRoBERTa trên hai nhiệm vụ hạ nguồn]

A.2 Chi tiết chưng cất tri thức
Chúng tôi triển khai chưng cất tri thức bằng cách cache các logit của giáo viên trước khi huấn luyện. Chúng tôi thực hiện tìm kiếm lưới để tìm nhiệt độ softmax τ tốt nhất từ {5,0, 10,0, 12,0, 20,0, 30,0}. Giá trị τ được sử dụng trong các thí nghiệm của chúng tôi được báo cáo trong Bảng 8 và 9 cho DistilRoBERTa và RoBERTa base; với ngoại lệ τ = 20,0 trên MRPC cho RoBERTa base. Trọng số mất mát λ, trong Eq. (8), được đặt thành 0,5 cho tất cả các nhiệm vụ trừ CoLA trong đó λ = 0,75.

B Kết quả OOD
B.1 Chế độ chưng cất
Kết quả OOD cho các mô hình được huấn luyện trong chế độ chưng cất được trình bày trong Bảng 10.

B.2 Chế độ độc lập
Bảng 11 trình bày kết quả OOD cho các mô hình được huấn luyện bằng cài đặt kiểm tra, và Bảng 12 (bổ sung cho Bảng 3 trong §4.2.1) trình bày kết quả OOD cho các thí nghiệm phát triển.

--- TRANG 14 ---
[Bảng 9: Siêu tham số của DistilRoBERTa trên chuẩn GLUE]

[Bảng 10: Kết quả OOD của các mô hình có kết quả kiểm tra trong miền được báo cáo trong Bảng 1 cho chế độ chưng cất]

--- TRANG 15 ---
[Bảng 11: Kết quả OOD của các mô hình có kết quả kiểm tra trong miền được báo cáo trong Bảng 2 cho thí nghiệm độc lập]

[Bảng 12: Kết quả OOD của các mô hình với cài đặt phát triển ở chế độ độc lập]
