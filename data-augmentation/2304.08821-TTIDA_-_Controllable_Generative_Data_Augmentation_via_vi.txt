# 2304.08821.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/data-augmentation/2304.08821.pdf
# Kích thước tệp: 7395129 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
TTIDA: Tăng cường dữ liệu sinh kiểm soát được thông qua
mô hình văn bản sang văn bản và văn bản sang hình ảnh

Yuwei Yin1, Jean Kaddour2, Xiang Zhang3, Yixin Nie4,
Zhenguang Liu5, Lingpeng Kong1, Qi Liu1
1Khoa Khoa học máy tính, Đại học Hồng Kông; 2Đại học College London
3Đại học Alberta; 4Đại học North Carolina tại Chapel Hill; 5Đại học Chiết Giang
{ywyin, lpk, liuqi}@cs.hku.hk; jean.kaddour.20@ucl.ac.uk;
xzhang23@ualberta.ca; yixin1@cs.unc.edu; zhenguangliu@zju.edu.cn

Tóm tắt
Tăng cường dữ liệu đã được thiết lập như một
phương pháp hiệu quả để bổ sung thông tin hữu
ích cho các tập dữ liệu ít tài nguyên. Các kỹ thuật
tăng cường truyền thống như tiêm nhiễu và biến
đổi hình ảnh đã được sử dụng rộng rãi. Ngoài ra,
tăng cường dữ liệu sinh (GDA) đã được chứng
minh là tạo ra dữ liệu đa dạng và linh hoạt hơn.
Trong khi mạng đối nghịch sinh (GAN) thường
được sử dụng cho GDA, chúng thiếu tính đa dạng
và khả năng điều khiển so với các mô hình khuếch
tán văn bản sang hình ảnh. Trong bài báo này,
chúng tôi đề xuất TTIDA (Tăng cường dữ liệu
văn bản sang văn bản sang hình ảnh) để tận dụng
khả năng của các mô hình sinh văn bản sang văn
bản (T2T) và văn bản sang hình ảnh (T2I) quy
mô lớn được huấn luyện trước cho việc tăng cường
dữ liệu. Bằng cách điều kiện hóa mô hình T2I
trên các mô tả chi tiết được tạo ra bởi mô hình
T2T, chúng tôi có thể tạo ra các hình ảnh có nhãn
giống thực tế một cách linh hoạt và có thể điều
khiển được. Các thí nghiệm về phân loại trong
miền, phân loại chéo miền và các tác vụ tạo chú
thích hình ảnh cho thấy những cải thiện nhất quán
so với các baseline tăng cường dữ liệu khác. Các
nghiên cứu phân tích trong các cài đặt khác nhau,
bao gồm few-shot, long-tail và đối nghịch, càng
củng cố thêm tính hiệu quả của TTIDA trong việc
nâng cao hiệu suất và tăng độ bền vững.1

1 Giới thiệu
Tăng cường dữ liệu phổ biến trong quy trình tiền
xử lý của các tác vụ học máy khác nhau, đặc biệt
là những tác vụ có dữ liệu được gán nhãn không
đầy đủ (Shorten và Khoshgoftaar, 2019). Nó mang
lại nhiều lợi ích, chẳng hạn như giảm thiểu chi phí
liên quan đến thu thập và chú thích dữ liệu, giảm
bớt những lo ngại liên quan đến khan hiếm và mất
cân bằng dữ liệu, và giảm thiểu tác động có hại
của overfitting đối với khả năng tổng quát hóa của
mô hình. Hiệu quả của tăng cường dữ liệu dựa trên
mức độ mà tập dữ liệu được tăng cường gần đúng
phân phối dữ liệu cơ bản. Do đó, mục tiêu tối ưu
là có được một loạt các mẫu phản ánh phân phối
tự nhiên của sự phong phú và đa dạng trong một
danh mục đối tượng đích nhất định.

Hầu hết các phương pháp tăng cường thông thường
được sử dụng trong thị giác máy tính dựa trên các
biến đổi thủ công sử dụng một mảng hạn chế các
tính bất biến cơ bản, chẳng hạn như xoay, cắt và
điều chỉnh màu sắc (Shorten và Khoshgoftaar, 2019;
Mikołajczyk và Grochowski, 2018; Fawzi và cộng sự,
2016). Những biến đổi này được chỉ định trước và
áp dụng đồng đều trên toàn bộ tập dữ liệu, điều này
có thể không tối ưu cho các loại dữ liệu hoặc kịch
bản khác nhau. Hạn chế như vậy thúc đẩy nhu cầu
về các phương pháp tiên tiến và linh hoạt hơn để
tăng cường dữ liệu một cách hiệu quả trong bối cảnh
các tác vụ thị giác máy tính khác nhau. Để có được
nhiều hình ảnh đa dạng hơn, các nghiên cứu trước
đây (Antoniou và cộng sự, 2017; Bowles và cộng sự,
2018) về tăng cường dữ liệu sinh (GDA) nhằm gần
đúng phân phối dữ liệu của tập dữ liệu hình ảnh
quan sát được bằng cách sử dụng mạng đối nghịch
sinh (Goodfellow và cộng sự, 2020) (GANs). Tuy
nhiên, mỗi danh mục cần một mô hình GAN riêng
biệt để được huấn luyện, điều này không linh hoạt
và phát sinh chi phí huấn luyện đáng kể. Bên cạnh
đó, quá trình huấn luyện của GANs nổi tiếng là
không ổn định, đặc biệt khi tập huấn luyện nhỏ. Nó
cũng gây ra vấn đề sụp đổ chế độ, tạo ra các hình
ảnh ít đa dạng hơn.

Trong bài báo này, chúng tôi nhằm giải quyết các
hạn chế của các phương pháp GDA hiện có cho các
tác vụ thị giác bằng cách khám phá các mô hình văn
bản sang hình ảnh (T2I) có thể truy cập công khai,
tạo ra hình ảnh theo văn bản đầu vào. Các mô hình
này dựa trên mô hình khuếch tán (Sohl-Dickstein
và cộng sự, 2015; Rombach và cộng sự, 2022) và
đã được huấn luyện với các cặp văn bản-hình ảnh
quy mô lớn thu được từ Web. Do đó, các mô hình
T2I có thể tạo ra một loạt hình ảnh giống thực tế
được điều kiện hóa trên các mô tả văn bản đa dạng.
Lợi ích của việc sử dụng các mô hình này là đa dạng.
1) chúng cung cấp giao diện ngôn ngữ cho phép
điều khiển linh hoạt và tạo ra các hình ảnh mong
muốn; 2) chúng là domain-agnostic và có khả năng
tạo ra dữ liệu mở rộng với tính đa dạng cao; 3)
chúng phục vụ như một công cụ đa năng để tổng hợp
dữ liệu chất lượng cao cho các tác vụ thị giác trong
các kịch bản khác nhau, bao gồm trong miền, chéo
miền, long-tail, và vì vậy.

Trong lĩnh vực xử lý ngôn ngữ tự nhiên, Edwards
và cộng sự (2021) đã sử dụng mô hình văn bản sang
văn bản (T2T) được huấn luyện trước quy mô lớn
GPT-2 (Radford và cộng sự, 2019) như một phương
pháp tăng cường dữ liệu cho các tác vụ phân loại
văn bản. Dựa trên khái niệm này, chúng tôi đề xuất
TTIDA (Tăng cường dữ liệu văn bản sang văn bản
sang hình ảnh) để tận dụng khả năng sinh của các
mô hình văn bản sang hình ảnh (T2I) và văn bản
sang văn bản (T2T) được huấn luyện trước quy mô
lớn cho mục đích tăng cường dữ liệu. Cụ thể, trước
tiên chúng tôi tinh chỉnh mô hình T2T, chẳng hạn
như GPT-2 và T5 (Raffel và cộng sự, 2020), trên
một tập đa dạng các chú thích hình ảnh. Sau đó, đối
với mỗi danh mục đối tượng, văn bản nhãn được
nhập vào mô hình T2T để có được mô tả chi tiết
hơn về đối tượng mà chúng tôi mong muốn. Sau
đó, mô hình T2I, chẳng hạn như GLIDE (Nichol
và cộng sự, 2022), được sử dụng để tạo ra nhiều
hình ảnh giống thực tế của đối tượng, được điều
kiện hóa trên văn bản nhãn gốc hoặc mô tả chi tiết
hơn. Các hình ảnh tổng hợp được tạo ra như vậy
được sử dụng để tăng cường tập dữ liệu hình ảnh
gốc. Sử dụng các mô tả chi tiết làm prompts cho
mô hình T2I là có lợi vì văn bản nhãn thường chỉ
chứa một hoặc hai từ và do đó, thường đơn giản.
Do đó, việc sử dụng mô hình T2T mang lại khả
năng điều khiển cho các hình ảnh được tạo ra, và
tính đa dạng của văn bản được tạo ra cũng đảm bảo
tính đa dạng của các hình ảnh được tạo ra.

Để đánh giá hiệu quả của phương pháp chúng tôi,
một loạt thí nghiệm được tiến hành trên các tác vụ
khác nhau, bao gồm (1) phân loại hình ảnh với các
kịch bản khác biệt như cài đặt dữ liệu cân bằng,
long-tail và đối nghịch, (2) phân loại hình ảnh chéo
miền, và (3) tạo chú thích hình ảnh. Kết quả thí
nghiệm trên các benchmark CIFAR (Krizhevsky và
cộng sự, 2009), Office (Saenko và cộng sự, 2010;
Venkateswara và cộng sự, 2017), và MS COCO (Lin
và cộng sự, 2014) chứng thực việc cải thiện hiệu
suất nhất quán của phương pháp chúng tôi so với
các baseline tăng cường dữ liệu khác. Đáng chú ý,
chúng tôi quan sát thấy rằng tính ưu việt của phương
pháp chúng tôi nổi bật hơn trong các trường hợp
dữ liệu khan hiếm hoặc miền dữ liệu đa dạng. Hơn
nữa, chúng tôi cho thấy rằng TTIDA có khả năng
nâng cao độ bền vững của mô hình trước các cuộc
tấn công đối nghịch.

Các đóng góp của chúng tôi được tóm tắt như sau:
• Chúng tôi đề xuất TTIDA, một phương pháp mới
kết hợp sức mạnh sinh của các mô hình văn bản
sang văn bản (T2T) và văn bản sang hình ảnh
(T2I) được huấn luyện trước quy mô lớn cho việc
tăng cường dữ liệu một cách có thể điều khiển
và linh hoạt.
• Chúng tôi chứng minh hiệu quả của TTIDA trong
việc nâng cao hiệu suất mô hình trên các benchmark
phân loại trong miền, phân loại chéo miền và tạo
chú thích hình ảnh.
• Các nghiên cứu phân tích trong các cài đặt khác
nhau, bao gồm few-shot, long-tail và đối nghịch,
càng củng cố thêm tính hiệu quả và độ bền vững
của TTIDA.

2 Công trình liên quan
2.1 Biến đổi hình ảnh truyền thống
Việc áp dụng các kỹ thuật tăng cường thông thường
cho thị giác máy tính đã được thừa nhận rộng rãi
và được xác nhận là hiệu quả (Perez và Wang, 2017).
Các kỹ thuật này thường dựa trên các biến đổi thủ
công khai thác một tập hợp hạn chế các tính bất
biến cơ bản (Goodfellow và cộng sự, 2016), bao
gồm nhưng không giới hạn ở cắt, xoay và lật
(Shorten và Khoshgoftaar, 2019). Việc chỉ bao gồm
một số lượng lớn hơn các kỹ thuật tăng cường hình
ảnh không phải lúc nào cũng dẫn đến cải thiện hiệu
suất vì một số phương pháp có thể thể hiện độ nhạy
cảm đối với việc lựa chọn tăng cường hình ảnh
(Grill và cộng sự, 2020).

2.2 Tăng cường dữ liệu sinh
Antoniou và cộng sự (2017) đề xuất sử dụng mạng
đối nghịch sinh (GANs) có điều kiện hình ảnh
(Goodfellow và cộng sự, 2020) để tạo ra hình ảnh
trong lớp được điều kiện hóa trên một miền nguồn.
Tương tự, Bowles và cộng sự (2018) sử dụng GANs
để tăng cường dữ liệu cho các tác vụ phân đoạn não.
Ngoài GANs, một số nhà nghiên cứu sử dụng mô
hình ngôn ngữ được huấn luyện trước GPT-2 (Radford
và cộng sự, 2019) như một phương pháp tăng cường
dữ liệu cho nhiều tác vụ phân loại văn bản Edwards
và cộng sự (2021) và các benchmark lý luận thông
thường (Yang và cộng sự, 2020). Bên cạnh các phương
pháp sinh, một hướng công trình khác cố gắng học
các chiến lược tăng cường tiên tiến sử dụng một
tập con cố định các hàm tăng cường. Cubuk và cộng
sự (2019, 2020) sử dụng học tăng cường để tự động
tìm một chính sách tăng cường cụ thể cho tập dữ
liệu được chứng minh là hiệu quả trên các tác vụ
downstream.

--- TRANG 2 ---
hình ảnh syn
hình ảnh thực nhãn: "bike" Tập dữ liệu hình ảnh
chú thích hình ảnh tinh chỉnh
mô tả bike
Tập dữ liệu được tăng cường
đầu vào hoặc Bước: 1 2 3 4
mô hình T2I mô hình T2T

Hình 1: Tổng quan về TTIDA (Tăng cường dữ liệu văn bản sang văn bản sang hình ảnh). Mũi tên ở các màu khác nhau đại diện cho các bước khác nhau. Bước 1: tinh chỉnh mô hình văn bản sang văn bản (T2T) bằng chú thích văn bản của hình ảnh. Bước 2: nhập văn bản nhãn, tức là "bike", vào mô hình T2T để tạo ra mô tả giống chú thích về xe đạp. Bước 3: nhập văn bản nhãn gốc hoặc mô tả được tạo ra vào mô hình văn bản sang hình ảnh (T2I) để tạo ra hình ảnh tổng hợp (syn) chất lượng cao. Bước 4: kết hợp hình ảnh thực từ tập dữ liệu gốc với hình ảnh được tăng cường để huấn luyện mô hình.

3 Phương pháp
Trong phần này, chúng tôi trình bày chi tiết phương pháp TTIDA cho việc tăng cường dữ liệu sinh và các động lực đằng sau framework của chúng tôi.

3.1 Tổng quan
Hình 1 cho thấy tổng quan về phương pháp tăng cường dữ liệu của chúng tôi, trong đó các mũi tên ở các màu khác nhau biểu thị các bước pipeline khác nhau. Đối với mỗi danh mục đối tượng, tức là bike trong Hình 1, chúng tôi nhập văn bản nhãn "bike" vào mô hình T2I như GLIDE (Nichol và cộng sự, 2022) để tạo ra nhiều hình ảnh giống thực tế của đối tượng này (Bước 3). Sau đó, chúng tôi kết hợp hình ảnh thực từ tập dữ liệu gốc với hình ảnh tổng hợp được tạo ra cùng nhau (Bước 4). Tập dữ liệu được tăng cường được sử dụng trực tiếp cho việc huấn luyện mô hình. Thông thường, văn bản nhãn là một từ hoặc cụm từ ngắn. Để tự động có được prompt tinh tế hơn cho mô hình T2I, trước tiên chúng tôi có thể nhập văn bản nhãn vào mô hình sinh văn bản sang văn bản (T2T) được tinh chỉnh với chú thích hình ảnh (Bước 1) để tạo ra mô tả đối tượng dài hơn (Bước 2), ví dụ, "một chiếc xe đạp màu trắng gần tường". Bước 1 và Bước 2 là tùy chọn vì mô hình T2I vẫn có thể tạo ra hình ảnh chất lượng cao với đầu vào văn bản nhãn. Tuy nhiên, mô hình T2T có thể tạo ra các mô tả đối tượng chính xác hoặc được cá nhân hóa với bối cảnh phong phú hơn, tăng tính đa dạng của hình ảnh tổng hợp ở mức độ lớn.

3.2 Công thức hóa
Chúng tôi ký hiệu tập huấn luyện của tập dữ liệu phân loại hình ảnh là D={D1, ..., Dn} có n danh mục khác nhau. Danh mục thứ i là Di={li, Xi}, trong đó li là văn bản nhãn, và Xi={x1, ..., xm} là m hình ảnh thực trong danh mục này. Đầu tiên, chúng tôi tinh chỉnh mô hình T2T t2t() trên corpus chú thích với loss mô hình hóa ngôn ngữ (dự đoán token tiếp theo) được điều kiện hóa trên prompt tương ứng p, tức là để tối đa hóa:

∑u ∑v log p(tuv|tu<v, pu); (1)

trong đó u là số lượng chú thích trong tập huấn luyện của tập dữ liệu chú thích hình ảnh, và v là chỉ số token của chú thích thứ u. tu<v biểu thị tất cả các token được tạo ra trước đó trước token thứ v. Prompt pu của chú thích thứ u bao gồm các thực thể của chú thích.

Sau đó, chúng tôi nhập nhãn danh mục li vào mô hình T2T để có được câu mô tả:
di = t2t(li). (2)

Sau đó, mô hình T2I t2i() sử dụng li hoặc di làm prompt đầu vào để tạo ra nhiều hình ảnh tổng hợp đa dạng {X̂i = x̂ji}Gj=1 của cùng một đối tượng như được mô tả trong li hoặc di, trong đó G là số lượng hình ảnh tổng hợp:

hoặc x̂ji = t2i(li) (3)
hoặc x̂ji = t2i(di). (4)

Cuối cùng, chúng tôi hợp nhất dữ liệu gốc của danh mục thứ i Di={li, Xi} với hình ảnh được tạo ra X̂i để xây dựng danh mục được tăng cường Daugi:

Daugi := {li, Xi, X̂i} = Di ∪ X̂i. (5)

Lặp lại quá trình này cho mỗi danh mục Di trong tập dữ liệu D, chúng tôi có thể có một tập dữ liệu được tăng cường Daug = {Daug1, ..., Daugn}.

--- TRANG 3 ---
3.3 Mô hình văn bản sang văn bản
Chúng tôi sử dụng Generative Pre-Training (GPT-2) (Radford và cộng sự, 2019) làm mô hình T2T. Cụ thể, chúng tôi áp dụng GPT2LMHeadModel2 cơ bản và đặt độ dài câu tối đa là 20 và kích thước beam là 5 cho tìm kiếm beam. Mô hình GPT được tinh chỉnh với loss mô hình hóa ngôn ngữ, và bộ tối ưu hóa Adam (Kingma và Ba, 2015) trong 5 epoch sử dụng tất cả các chú thích trong tập huấn luyện của MS COCO 2015 Image Captioning Task (Lin và cộng sự, 2014). Khi tinh chỉnh GPT-2, prompt pu trong Phương trình 1 là một template chứa tất cả các token thực thể {eui}ni=1 được trích xuất từ câu chú thích thứ u trước đó. Chúng tôi thêm vào đầu template "Write an image description with keywords including eu1, eu2, ..., and eun:" vào câu chú thích gốc. Sau đó, chúng tôi đưa chú thích được prompted vào mô hình GPT-2 để tinh chỉnh GPT-2 sử dụng loss mô hình hóa ngôn ngữ. Bằng cách này, bằng cách chỉnh sửa các từ thực thể của prompts đầu vào khi tạo chú thích hình ảnh sử dụng mô hình GPT-2 được tinh chỉnh, chúng tôi có thể điều khiển linh hoạt các câu được tạo ra và do đó có thể sửa đổi nội dung của các hình ảnh được tạo ra bởi mô hình T2I.

3.4 Mô hình văn bản sang hình ảnh
Chúng tôi áp dụng Guided Language to Image Diffusion for Generation and Editing (GLIDE) (Nichol và cộng sự, 2022) làm mô hình T2I của chúng tôi. Không giống như các mô hình văn bản sang hình ảnh khác chủ yếu tập trung vào việc tạo ra hình ảnh với các phong cách nghệ thuật khác nhau, chẳng hạn như DALL-E (Ramesh và cộng sự, 2022) và Stable Diffusion (Rombach và cộng sự, 2022), GLIDE nhằm tạo ra hình ảnh giống thực tế từ văn bản prompt đầu vào. Chúng tôi đưa vào mô hình T2I các câu mô tả hình ảnh được tạo ra bởi mô hình T2T để tạo ra hình ảnh tổng hợp như tăng cường dữ liệu một cách có thể điều khiển hơn.

4 Thiết lập thí nghiệm
Trong phần này, chúng tôi trình bày chi tiết về tất cả các thiết lập thí nghiệm, bao gồm giới thiệu tác vụ (4.1), tập dữ liệu (4.2), tăng cường dữ liệu (4.3), phương pháp của chúng tôi và các phương pháp baseline (4.4), mô hình backbone (4.5). Xem Phụ lục C để biết chi tiết huấn luyện cụ thể.

4.1 Tác vụ
Để minh họa tính linh hoạt của phương pháp chúng tôi, chúng tôi giải quyết một tập hợp đa dạng các tác vụ thị giác máy tính. Bây giờ chúng tôi giải thích từng trường hợp cụ thể về cách các cài đặt này có thể chứng minh lợi ích của TTIDA.

Phân loại hình ảnh trong miền Trong tác vụ này, miền của tập huấn luyện giống với miền của tập kiểm tra. Chúng tôi xem xét cài đặt dữ liệu cân bằng: Mỗi danh mục có cùng số lượng hình ảnh. Các cài đặt khác như Few-shot, long-tail và đối nghịch sẽ được thảo luận trong Phần 6.

Phân loại hình ảnh chéo miền Mô hình huấn luyện trên hình ảnh của một miền và kiểm tra trên hình ảnh của miền khác. Ví dụ, tập dữ liệu chéo miền D có K miền D={D1, ..., DK}, và mỗi miền Dk chứa cùng n danh mục {Ck1, ..., Ckn}. Hình ảnh trong danh mục thứ i Cisrc của miền nguồn Dsrc và những hình ảnh trong danh mục thứ i Citgt của miền đích Dtgt biểu thị cùng một đối tượng. Hình ảnh của các miền khác nhau khác biệt rõ ràng về một số khía cạnh, chẳng hạn như góc độ và vị trí, tông màu và đặc điểm nền. Do đó, kết quả thí nghiệm trên tác vụ này sẽ chứng minh lợi ích của việc tạo ra hình ảnh đa dạng và do đó xác minh đặc tính domain-agnostic của phương pháp chúng tôi.

Tạo chú thích hình ảnh Trong tác vụ này, mô hình cần tạo ra một câu chú thích mô tả hình ảnh đầu vào một cách chính xác. Bằng cách kiểm tra hiệu suất của TTIDA trên tác vụ này, chúng tôi cho thấy những lợi thế của việc kết hợp sức mạnh của các mô hình ngôn ngữ tự hồi quy (mô hình T2T) và mô hình T2I để tạo ra các cặp (văn bản, hình ảnh) hợp lý để cải thiện khả năng tạo từ hình ảnh sang văn bản (chú thích).

4.2 Tập dữ liệu
Phân loại hình ảnh trong miền Chúng tôi tiến hành thí nghiệm phân loại hình ảnh trong miền trên hai tập dữ liệu. CIFAR-100 chứa 100 lớp khác biệt có thể phân biệt và kích thước nhỏ hơn của các mẫu huấn luyện - 500 hình ảnh - trong mỗi lớp. Chúng tôi sử dụng độ chính xác phân loại làm thước đo đánh giá.

Phân loại hình ảnh chéo miền Chúng tôi áp dụng hai tập dữ liệu chéo miền để đo lường hiệu quả của TTIDA. 1) Tập dữ liệu Office-31 (Saenko và cộng sự, 2010) chứa 31 danh mục đối tượng trong ba miền: Amazon, DSLR và Webcam. 2) Office-Home (Venkateswara và cộng sự, 2017) là một tập dữ liệu benchmark khác cho thích ứng miền chứa 4 miền bao gồm Art, Clipart, Product và Real-World, trong đó mỗi miền có 65 danh mục.

Tạo chú thích hình ảnh Chúng tôi sử dụng tập dữ liệu chú thích hình ảnh của Microsoft COCO (common objects in context) 2015 Image Captioning Task. MS COCO Captions (Lin và cộng sự, 2014) chứa hơn một triệu rưỡi chú thích mô tả hơn 330,000 hình ảnh. Đối với hình ảnh huấn luyện và xác thực, năm chú thích được tạo ra độc lập bởi con người được cung cấp cho mỗi hình ảnh.

4.3 Tăng cường dữ liệu
Hình ảnh tổng hợp cho CIFAR Hình ảnh trong tập dữ liệu CIFAR có kích thước 32×32; việc chỉ định đơn giản việc tạo ra với độ phân giải thấp như vậy từ GLIDE sẽ làm giảm đáng kể chất lượng hình ảnh. Thay vào đó, chúng tôi tạo ra hình ảnh có kích thước 256×256 với nhiều chi tiết hơn và sau đó thực hiện thay đổi kích thước. Đối với CIFAR-100, 500 hình ảnh được tạo ra bởi mô hình GLIDE. Tổng cộng, chúng tôi cung cấp 50000 mẫu huấn luyện bổ sung cho hai tập dữ liệu này.

Hình ảnh tổng hợp cho Office Không giống như CIFAR, hình ảnh gốc trong tập dữ liệu Office-31 và Office-Home thay đổi từ độ phân giải thấp đến độ phân giải cao. Bên cạnh đó, những hình ảnh này không có cùng hình dạng, vì vậy CDTrans thực hiện một loạt các biến đổi hình ảnh. Hình ảnh tổng hợp của chúng tôi có kích thước 256×256 ban đầu và sau đó thực hiện các biến đổi tương tự như trong CDTrans. Đối với mỗi danh mục trong mỗi miền, số lượng hình ảnh tổng hợp được tạo ra bởi mô hình T2I giống như số lượng của tập dữ liệu Office.

Hình ảnh tổng hợp cho MS COCO Chúng tôi tạo ra hình ảnh tổng hợp sử dụng tập huấn luyện của chú thích COCO như tăng cường. Ngoài ra, chúng tôi trích xuất thực thể từ chú thích COCO sử dụng CoreNLP và Natural Language Toolkit (NLTK), và sau đó đưa vào mô hình T2I các thực thể hoặc chú thích tổng hợp được tạo ra bởi mô hình T2T sử dụng các thực thể này.

4.4 Phương pháp của chúng tôi và các phương pháp Baseline
TTIDA Chúng tôi sử dụng các mô hình T2T và T2I như được mô tả trong Phần 3.3 và 3.4. Cụ thể, đối với tất cả thí nghiệm, chúng tôi sử dụng cài đặt mô hình cơ bản của mô hình GLIDE công khai và tuân theo quy trình tạo ra tiêu chuẩn: chúng tôi sử dụng mô hình GLIDE cơ bản để tạo ra hình ảnh 64×64, và sau đó đưa chúng vào mô hình upsample để có được hình ảnh chất lượng cao có độ phân giải 256×256. Sau đó, chúng tôi thực hiện thay đổi kích thước để phù hợp với kích thước hình ảnh của các tập dữ liệu khác nhau. Ngoại trừ thay đổi kích thước và chuẩn hóa hình ảnh, không có biến đổi hình ảnh nào khác được áp dụng.

Biến đổi hình ảnh truyền thống Việc kết hợp các kỹ thuật tăng cường hình ảnh bổ sung không nhất thiết dẫn đến cải thiện hiệu suất, vì một số phương pháp có thể nhạy cảm với việc lựa chọn tăng cường (Grill và cộng sự, 2020). Do đó, chúng tôi áp dụng quy trình biến đổi hình ảnh được sử dụng trong SimCLR (Chen và cộng sự, 2020a) và MoCo (He và cộng sự, 2020; Chen và cộng sự, 2020b, 2021), đã được xác nhận về hiệu quả của nó.

Tăng cường dữ liệu sinh dựa trên GAN Để so sánh phương pháp TTIDA của chúng tôi với các phương pháp tăng cường dữ liệu sinh trước đây, chúng tôi sử dụng ba mô hình GAN đại diện, cụ thể là DCGAN (Radford và cộng sự, 2016), CycleGAN (Zhu và cộng sự, 2017) và StyleGAN (Karras và cộng sự, 2021b, 2020, 2021a), để tăng cường hình ảnh. Cụ thể hơn, chúng tôi tuân theo các triển khai gốc của DCGAN, CycleGAN và StyleGAN cho việc huấn luyện và tạo ra GAN.

4.5 Mô hình Backbone
Phân loại hình ảnh trong miền Đối với tất cả thí nghiệm trên CIFAR, chúng tôi sử dụng kiến trúc ResNet-101 tiêu chuẩn (He và cộng sự, 2016) làm backbone.

Phân loại hình ảnh chéo miền Chúng tôi áp dụng mô hình tiên tiến CDTrans (Xu và cộng sự, 2022) làm mô hình baseline. Chúng tôi tuân theo triển khai của CDTrans và chỉ thêm một module xử lý dữ liệu để kết hợp hình ảnh tổng hợp của chúng tôi vào tập dữ liệu miền nguồn gốc.

Tạo chú thích hình ảnh Chúng tôi sử dụng mô hình tiên tiến mPLUG (Li và cộng sự, 2022) làm baseline và tuân theo triển khai của mPLUG. Tương tự, chúng tôi thêm một khối xử lý dữ liệu để tích hợp dữ liệu được tăng cường của chúng tôi vào tập dữ liệu chú thích COCO.

5 Kết quả
Trong phần này, chúng tôi báo cáo tất cả kết quả thí nghiệm w.r.t. các cài đặt được mô tả trong Phần 4.

--- TRANG 4 ---
CIFAR-100 + 20% + 50% + 100% + Max
Img Trans DA +0.3% +0.4% +0.4% +0.5%
DCGAN DA +0.4% +0.5% +0.7% +1.0%
CycleGAN DA +0.5% +0.7% +1.0% +1.2%
StyleGAN DA +0.7% +0.9% +1.2% +1.4%
TTIDA (nhãn) +1.1% +1.8% +2.3% +2.7%
TTIDA (mô tả) +1.3% +2.1% +2.6% +3.0%

Bảng 1: Độ chính xác phân loại trên CIFAR-100. Chúng tôi báo cáo cải thiện độ chính xác khi thêm hình ảnh tổng hợp được tạo ra bởi các mô hình khác nhau được mô tả trong Phần 4.4. "+Max" biểu thị điểm số cao nhất sử dụng 200%, 300%, 400% hoặc 500% hình ảnh tổng hợp.

Office-31 A→D A→W D→A
Trước khi tinh chỉnh 97.0% 96.7% 81.1%
w/o dữ liệu syn 97.4% 96.8% 81.3%
w/ dữ liệu syn 98.0% 97.1% 81.6%

Office-31 D→W W→A W→D
Trước khi tinh chỉnh 99.0% 81.9% 100%
w/o dữ liệu syn 99.0% 82.0% 100%
w/ dữ liệu syn 99.1% 82.2% 100%

Bảng 2: Độ chính xác phân loại miền đích theo mọi hướng trên tập dữ liệu Office-31. Office-31 có ba miền, cụ thể là Amazon (A), DSLR (D) và Webcam (W). "Trước khi tinh chỉnh" đại diện cho điểm số kiểm tra của các checkpoint tốt nhất của mô hình tiên tiến CDTrans. "w/ dữ liệu syn" và "w/o dữ liệu syn" đại diện cho kết quả sau khi tinh chỉnh với và không có hình ảnh tổng hợp được tạo ra bởi TTIDA tương ứng.

5.1 Phân loại hình ảnh trong miền
Kết quả thí nghiệm phân loại hình ảnh trên tập dữ liệu CIFAR-100 được hiển thị trong Bảng 1. TTIDA vượt trội hơn tất cả baselines trên mỗi tỷ lệ tổng hợp, chứng minh hiệu quả của phương pháp chúng tôi trong việc tăng cường hiệu suất phân loại hình ảnh.

5.2 Phân loại hình ảnh chéo miền
Chúng tôi tiếp tục huấn luyện các checkpoint CDTrans tốt nhất trong 50 epoch trên mỗi hướng thích ứng miền Office với và không có hình ảnh tổng hợp của chúng tôi. Kết quả phân loại miền đích trên tập dữ liệu Office-31 và Office-Home được báo cáo trong Bảng 2 và Bảng 3 tương ứng. Chúng tôi quan sát thấy rằng tinh chỉnh liên tục nâng cao độ chính xác phân loại cho tất cả các hướng, đặc biệt là khi huấn luyện với dữ liệu được tăng cường của chúng tôi. Kết quả xác minh hiệu quả của TTIDA trong việc cải thiện hiệu suất mô hình trên các tác vụ phân loại hình ảnh chéo miền.

5.3 Tạo chú thích hình ảnh
Chúng tôi huấn luyện mô hình mPLUG cơ bản (Li và cộng sự, 2022) (mplug.en.base) dựa trên mô hình CLIP được huấn luyện trước (Radford và cộng sự, 2021) (ViT-B-16) trong 5 epoch trên tập dữ liệu chú thích hình ảnh COCO với các kích thước huấn luyện khác nhau. Mô hình được đánh giá bằng các thước đo BLEU (Papineni và cộng sự, 2002), ROUGE (Lin, 2004) và CIDEr (Vedantam và cộng sự, 2015). Cụ thể, chúng tôi áp dụng triển khai evaluator (Chen và cộng sự, 2015) để tính toán điểm số BLEU4, ROUGE-L cấp câu và CIDEr-D.

Bảng 4 so sánh điểm số kiểm tra của mô hình mPLUG được tinh chỉnh với và không có dữ liệu tổng hợp được tạo ra bởi TTIDA. Như kết quả cho thấy, TTIDA có thể tăng cường thêm hiệu suất mô hình trên các đánh giá khác nhau dưới các cài đặt kích thước dữ liệu khác nhau.

Art A→C A→P A→R Trung bình A
Trước khi tinh chỉnh 68.8% 85.0% 86.9% 80.23%
w/o dữ liệu syn 68.9% 85.4% 87.1% 80.47%
w/ dữ liệu syn 69.2% 85.7% 87.6% 80.83%

Clipart C→A C→P C→R Trung bình C
Trước khi tinh chỉnh 81.5% 87.1% 87.3% 85.30%
w/o dữ liệu syn 81.8% 87.2% 87.4% 85.47%
w/ dữ liệu syn 82.2% 87.5% 87.4% 85.70%

Product P→A P→C P→R Trung bình P
Trước khi tinh chỉnh 79.6% 63.3% 88.2% 77.03%
w/o dữ liệu syn 79.7% 64.5% 88.3% 77.50%
w/ dữ liệu syn 80.1% 65.9% 88.5% 78.17%

RealWorld R→A R→C R→P Trung bình R
Trước khi tinh chỉnh 82.0% 66.0% 90.6% 79.53%
w/o dữ liệu syn 82.6% 66.1% 90.7% 79.80%
w/ dữ liệu syn 82.8% 66.4% 90.9% 80.03%

Bảng 3: Độ chính xác phân loại miền đích theo mọi hướng trên tập dữ liệu Office-Home. Office-Home có bốn miền, cụ thể là Art (A), Clipart (C), Product (P) và RealWorld (R). "Trước khi tinh chỉnh" đại diện cho điểm số kiểm tra của các checkpoint CDTrans tốt nhất. "w/ dữ liệu syn" và "w/o dữ liệu syn" đại diện cho kết quả sau khi tinh chỉnh trong 50 epoch với và không có hình ảnh tổng hợp được tạo ra bởi TTIDA tương ứng.

#Dữ liệu + Tỷ lệ syn BLEU4 ROUGE-L CIDEr-D
5000 + 100% +3.2% +1.0% +2.4%
10000 + 100% +1.9% +2.1% +3.0%
50000 + 100% +1.5% +1.3% +5.2%
100000 + 100% +0.5% +0.2% +1.0%
200000 + 100% +0.7% +0.3% +2.1%

Bảng 4: Điểm số kiểm tra của mô hình mPLUG được tinh chỉnh trên tập dữ liệu MS COCO 2015 Image Captioning. Chúng tôi báo cáo điểm số BLEU, ROUGE và CIDEr với (+ 100%) và không có (+ 0) hình ảnh tổng hợp được tạo ra bởi TTIDA.

--- TRANG 5 ---
Hình 2: Kết quả trên benchmark CIFAR-100 dưới cài đặt few-shot.

6 Phân tích
Trong phần này, chúng tôi tiến hành các nghiên cứu phân tích để hiểu rõ hơn về cách framework đề xuất của chúng tôi đóng góp vào hiệu suất mô hình.

6.1 Hình ảnh tổng hợp với các tỷ lệ khác nhau
Thông thường, tăng cường dữ liệu hữu ích hơn trong các cài đặt ít tài nguyên hơn là trong các cài đặt nhiều tài nguyên. Để xác minh ý tưởng này, chúng tôi điều chỉnh tỷ lệ quy mô của tập huấn luyện để tạo ra bối cảnh tài nguyên cao và thấp. Việc tăng cường tập huấn luyện gốc với hình ảnh tổng hợp với các tỷ lệ khác nhau được thực hiện với mục đích xác định các điều kiện tối ưu cho việc áp dụng TTIDA. Kết quả thí nghiệm phân loại hình ảnh trên tập dữ liệu CIFAR-100 được hiển thị trong Hình 2. Chúng tôi báo cáo độ chính xác phân loại (%) trên tập dữ liệu CIFAR với hình ảnh huấn luyện tổng hợp với các tỷ lệ khác nhau so với tổng số hình ảnh gốc. Nó tiết lộ rằng việc kết hợp số lượng lớn hơn hình ảnh tổng hợp dẫn đến việc giảm lỗi phân loại có thể nhận thấy trong tất cả các trường hợp. Ngoài ra, quan sát thấy rằng hiệu quả của phương pháp đề xuất đặc biệt nổi bật trong các tình huống mà số lượng hình ảnh huấn luyện CIFAR gốc ít ỏi. Những phát hiện này cho thấy rằng chiến lược tăng cường được đề xuất trong công trình này có triển vọng đặc biệt trong các trường hợp mà tập dữ liệu gốc được đặc trưng bởi tài nguyên hạn chế. Để củng cố thêm cho tuyên bố này, hiệu quả của TTIDA được đánh giá trên tập con CIFAR long-tail tổng hợp.

6.2 Huấn luyện trên tập dữ liệu Long-tail
Chúng tôi kiểm tra phương pháp của mình trên tập con CIFAR long-tail tổng hợp, trong đó mỗi danh mục có số lượng hình ảnh khác nhau. Chúng tôi xây dựng tập con Long-tail CIFAR Dlt={Dlt1, ..., Dltn} từ tập dữ liệu CIFAR cân bằng gốc D={D1, ..., Dn} có n danh mục khác nhau, trong đó danh mục con thứ i Dlti có i/n hình ảnh trong Di.

Chúng tôi báo cáo cải thiện độ chính xác phân loại (%) sau khi sử dụng hình ảnh tổng hợp được tạo ra bởi TTIDA trên tập dữ liệu long-tail CIFAR-100. Như được hiển thị trong Hình 3, kết quả thí nghiệm phù hợp với những phát hiện trong Hình 2. Do đó, chúng tôi kết luận rằng hiệu quả của TTIDA trong việc cải thiện độ chính xác phân loại hình ảnh, đặc biệt là trong các trường hợp ít tài nguyên, được xác minh thêm.

Hình 3: Kết quả trên benchmark Long-tail CIFAR-100 dưới cài đặt few-shot.

6.3 Huấn luyện với hình ảnh đối nghịch
Chúng tôi tiến hành thí nghiệm sau khi thêm hình ảnh đối nghịch vào tập huấn luyện. Đối với mỗi lớp, chúng tôi thu thập thủ công các hình ảnh như vậy từ Internet, ví dụ, hình ảnh với phong cách bất thường (Phụ lục A). Thí nghiệm này sẽ kiểm tra xem một tập hợp đa dạng dữ liệu tổng hợp có giúp làm cho bộ phân loại bền vững hơn hay không.

Tập dữ liệu + 100% Syn + Adv Acc
CIFAR-100 41.8%
CIFAR-100 ✓ ✓ 0.7%

Bảng 5: Độ chính xác phân loại trên CIFAR-100 với hình ảnh huấn luyện đối nghịch. Chúng tôi đặt số lượng hình ảnh huấn luyện gốc là 10k và báo cáo kết quả với và không có dữ liệu tổng hợp (Syn) và đối nghịch (Adv) được thêm vào tập huấn luyện.

Như được hiển thị trong Bảng 5, chúng tôi so sánh sự khác biệt về độ chính xác phân loại trước và sau khi thêm hình ảnh đối nghịch vào tập kiểm tra. Mô hình được huấn luyện với dữ liệu được tăng cường hoạt động tốt hơn 2.80%, chứng minh rằng nó tăng cường độ bền vững của mô hình đối với hình ảnh bất thường.

--- TRANG 6 ---
Tập dữ liệu nhãn mô tả Acc
CIFAR-100 ✓ +1.7%
CIFAR-100 ✓ +1.9%

Tập dữ liệu nhãn mô tả Acc trung bình
Office-31 ✓ +0.25%
Office-31 ✓ +0.35%

Tập dữ liệu nhãn mô tả BLEU4
MS COCO ✓ +1.5%
MS COCO ✓ +2.1%

Bảng 6: So sánh prompts T2I. Chúng tôi so sánh điểm số kiểm tra (CIFAR-100, Office-31, MS COCO) sau khi tăng cường với hình ảnh được tạo ra với văn bản nhãn gốc hoặc mô tả được tạo ra bởi TTIDA.

6.4 Prompts được tạo ra bởi mô hình T2T
Như được hiển thị trong Hình 1, chúng tôi có thể prompt mô hình T2I bằng tên nhãn ngắn hoặc mô tả dài hơn về hình ảnh sẽ được tạo ra. Trong ablation này, chúng tôi so sánh hiệu suất mô hình giữa việc sử dụng văn bản nhãn gốc hoặc mô tả giống chú thích làm prompts cho mô hình T2I.

Bảng 6 cho thấy rằng việc sử dụng mô tả phong phú để tạo ra hình ảnh dẫn đến cải thiện tốt hơn so với việc sử dụng văn bản nhãn, đặc biệt là trên benchmark MS COCO. Chúng tôi giả thuyết rằng việc sử dụng mô tả có lợi hơn cho các tác vụ chéo miền và tác vụ COCO quy mô lớn vì các câu được tạo ra bởi mô hình T2T có bối cảnh phong phú hơn để mô hình T2I có thể tạo ra hình ảnh đa dạng hơn. Ngược lại, dữ liệu được tăng cường với quá nhiều tính đa dạng ít hiệu quả hơn cho tác vụ phân loại trong miền CIFAR.

6.5 Tinh chỉnh mô hình T2I
Để giải thích thêm các quan sát và xác minh giả thuyết trong phần trước, chúng tôi tinh chỉnh mô hình T2I trên tập huấn luyện của CIFAR-100 để hình ảnh tổng hợp trông giống hình ảnh gốc hơn. Cụ thể, chúng tôi tuân theo thực hành mã nguồn mở để tinh chỉnh GLIDE sử dụng tập huấn luyện của tập dữ liệu chú thích hình ảnh MS COCO. Bảng 7 chứng minh rằng tinh chỉnh mô hình T2I mang lại lợi ích thêm như mong đợi.

Tập dữ liệu nhãn mô tả tinh chỉnh Acc
CIFAR-100 ✓ +1.5%
CIFAR-100 ✓ ✓ +2.0%
CIFAR-100 ✓ +1.9%
CIFAR-100 ✓ ✓ +2.3%

Bảng 7: So sánh tinh chỉnh mô hình T2I. Chúng tôi so sánh độ chính xác phân loại giữa việc sử dụng văn bản nhãn gốc hoặc mô tả trên tác vụ phân loại hình ảnh CIFAR-100. Mô hình văn bản sang hình ảnh được tinh chỉnh trên tập huấn luyện CIFAR-100 nếu "tinh chỉnh" được đánh dấu.

7 Công trình tương lai
7.1 Dịch ngược đa phương thức
Trong dịch máy, dịch ngược (Sennrich và cộng sự, 2016) được sử dụng rộng rãi như một phương pháp tăng cường dữ liệu mạnh mẽ. Nó thực hiện quá trình dịch nguồn→đích→nguồn để tăng cường dữ liệu văn bản của ngôn ngữ nguồn. Tương tự, dịch ngược đa phương thức có thể được tiến hành sử dụng mô hình văn bản sang hình ảnh như GLIDE và mô hình hình ảnh sang văn bản như mô hình được sử dụng trong các tác vụ chú thích hình ảnh. Với các bộ sinh chất lượng cao có sẵn, chúng tôi có thể tạo ra dữ liệu văn bản và hình ảnh bổ sung bằng cách thực hiện dịch ngược văn bản→hình ảnh→văn bản và hình ảnh→văn bản→hình ảnh tương ứng.

7.2 Tương quan giả tạo
Tương quan giả tạo là tương quan giữa các đặc trưng hình ảnh và một số lớp đích nhất định mà các đặc trưng không gây ra lớp sau (Kaddour và cộng sự, 2022). Chúng tự nhiên xảy ra trong nhiều tập dữ liệu (Neuhaus và cộng sự, 2022; Vasudevan và cộng sự, 2022; Lynch và cộng sự, 2023), nhưng một mô hình dựa vào chúng trở nên có vấn đề khi đối mặt với hình ảnh mà những tương quan này không tồn tại. Với tính linh hoạt của TTIDA, chúng tôi tin rằng một hướng đầy hứa hẹn là tăng cường tập huấn luyện với hình ảnh chứa các đặc trưng không giả tạo. Điều này có thể là một giải pháp thay thế hấp dẫn để giảm thiểu sự phụ thuộc của mô hình vào các đặc trưng giả tạo so với mô hình hiện tại của việc giảm mẫu các nhóm đa số (Idrissi và cộng sự, 2022; Schwartz và Stanovsky, 2022; Arjovsky và cộng sự, 2022).

8 Kết luận
Trong công trình này, chúng tôi đề xuất TTIDA, một lược đồ tăng cường dữ liệu sử dụng các mô hình văn bản sang hình ảnh và văn bản sang văn bản được huấn luyện trước quy mô lớn. Chúng tôi tập trung vào việc tạo ra hình ảnh giống thực tế dựa trên (i) văn bản nhãn ngắn gọn và (ii) prompts mô tả dài hơn. Kết quả thí nghiệm được tiến hành cho các tác vụ phân loại hình ảnh và chú thích hình ảnh dưới các cài đặt khác nhau chứng minh tính hiệu quả và độ bền vững của phương pháp tăng cường đề xuất. Hơn nữa, chúng tôi ablate một số thành phần của phương pháp và kết luận với các hướng công trình tương lai.

9 Hạn chế
Mặc dù phương pháp của chúng tôi hiệu quả trên các tác vụ trên, phương pháp tăng cường dữ liệu này có thể được áp dụng cho nhiều ứng dụng học máy hơn, đặc biệt là trong các tình huống ít tài nguyên. Nhiều thí nghiệm hơn trên các tác vụ khác nhau với một loạt tìm kiếm siêu tham số rộng có thể củng cố thêm các kết luận của chúng tôi. Bên cạnh đó, việc khám phá các chiến lược lọc để tránh dữ liệu không phù hợp được thêm vào tập huấn luyện là cần thiết, ví dụ, các mẫu ngoại lệ quá khác biệt so với phân phối tập huấn luyện theo một số thước đo khoảng cách.

10 Tuyên bố đạo đức
Như được mô tả trong (Nichol và cộng sự, 2022, Phần 6), mô hình GLIDE gốc tạo ra hình ảnh giả nhưng thực tế với khả năng thông tin sai lệch hoặc thiên vị được giới thiệu bởi dữ liệu mà nó được huấn luyện. Tuy nhiên, chúng tôi tin rằng những lo ngại đạo đức như vậy có thể được giải quyết bằng lọc dữ liệu phù hợp, như được đề xuất bởi Nichol và cộng sự (2022).

Tài liệu tham khảo
[Phần tài liệu tham khảo được duy trì nguyên bằng tiếng Anh do tính chất học thuật]

--- TRANG 7 ---
A Nghiên cứu trường hợp
Hình ảnh gốc Hình 4 cho thấy hình ảnh xe đạp gốc của miền Art và Real-World của tập dữ liệu Office-Home. Nền hình ảnh và phong cách khác biệt rõ ràng ở các miền khác nhau.

(a) Xe đạp trong Office-Home Art.
(b) Xe đạp trong Office-Home Real-World.

Hình 4: Hình ảnh xe đạp gốc trong miền Art và Real-World của tập dữ liệu Office-Home.

Hình ảnh tổng hợp của chúng tôi Hình 5 cho thấy hình ảnh xe đạp tổng hợp được tạo ra bởi TTIDA. Chúng tôi có thể quan sát thấy rằng mô hình GLIDE vanilla có xu hướng tạo ra hình ảnh giống hình ảnh giống thực tế trong thế giới thực.

Hình ảnh đối nghịch Hình 6 cho thấy hình ảnh đối nghịch mà chúng tôi thu thập từ Internet. Những hình ảnh này có nền bất thường hoặc phong cách kỳ lạ, vì vậy chúng phá vỡ tương quan giả tạo giữa nội dung và nền trong tập dữ liệu gốc.

Hình 5: Hình ảnh xe đạp tổng hợp được tạo ra bởi TTIDA.

Hình 6: Hình ảnh đối nghịch được thu thập từ Internet.

B Thống kê dữ liệu
Bảng 8 liệt kê thống kê của các tập dữ liệu trong các tác vụ phân loại hình ảnh trong miền (CIFAR) và chéo miền (Office).

Tập dữ liệu (Miền) # tổng # lớp # mỗi lớp
CIFAR-100 50000 100 500
Office-31 (Amazon) 2817 31 91
Office-31 (DSLR) 498 31 16
Office-31 (Webcam) 795 31 26
Office-Home (Art) 2427 65 37
Office-Home (Clipart) 4365 65 67
Office-Home (Product) 4439 65 68
Office-Home (Real-World) 4357 65 67

Bảng 8: Thống kê của tập dữ liệu phân loại hình ảnh. CIFAR-100 có 50000 hình ảnh để huấn luyện và thêm 10000 hình ảnh để kiểm tra. Tập dữ liệu Office-31 có 3 miền khác nhau và tập dữ liệu Office-Home có 4 miền.

C Chi tiết huấn luyện
Phân loại hình ảnh trong miền Mô hình ResNet-101 tiêu chuẩn được huấn luyện từ đầu trong thời gian 200 epoch trên một GPU NVIDIA A40. Để đánh giá hiệu quả của mô hình, một tập xác thực holdout được sử dụng, được lấy mẫu ngẫu nhiên từ mỗi danh mục của tập huấn luyện 20 phần trăm và được đánh giá sau mỗi epoch. Sau huấn luyện, checkpoint tối ưu được xác định bằng cách chọn mô hình hoạt động tốt nhất trên tập xác thực. Checkpoint được chọn sau đó được sử dụng để đánh giá độ chính xác của mô hình trên tập kiểm tra. Đối với mỗi cài đặt, chúng tôi lặp lại quá trình ba lần với ba seed ngẫu nhiên khác nhau {7, 17, 42} và báo cáo điểm số kiểm tra trung bình.

Đối với tất cả thí nghiệm phân loại trên CIFAR, hàm loss là cross-entropy và kích thước batch là 128. Chúng tôi sử dụng bộ tối ưu hóa stochastic gradient descent (SGD) với tốc độ học ban đầu là 0.1, momentum là 0.9 và weight decay là 0.0005. Chúng tôi thực hiện lịch trình tốc độ học multi-step với gamma là 0.2 và milestones là {60, 120, 160}. Ngoài ra, 10 epoch đầu tiên là epoch warm-up với tốc độ học tăng tuyến tính.

Phân loại hình ảnh chéo miền Chúng tôi tuân theo tất cả cài đặt huấn luyện của CDTrans (Xu và cộng sự, 2022) và tinh chỉnh checkpoint tốt nhất được công bố trên benchmark Office trong 50 epoch.

Tạo chú thích hình ảnh Tương tự, chúng tôi tuân theo tất cả chi tiết huấn luyện của mPLUG (Li và cộng sự, 2022) và tinh chỉnh checkpoint tốt nhất được công bố trên tập huấn luyện MS COCO trong 5 epoch.
