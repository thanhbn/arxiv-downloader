DualMix: Khơi dậy tiềm năng của việc tăng cường dữ liệu cho học tăng trưởng lớp trực tuyến
Yunfeng Fan1, Wenchao Xu1, Haozhao Wang2;*, Jiaqi Zhu3, Junxiao Wang4;5, và Song Guo1
1Đại học Bách khoa Hồng Kông,2Đại học Khoa học và Công nghệ Hoa Trung
3Trường Tự động hóa, Viện Công nghệ Bắc Kinh,4KAUST,5Trung tâm AI SDAIA-KAUST
fyunfeng.fan,wenchao.xu g@polyu.edu.hk ,hzwang@hust.edu.cn ,
E1111838@u.nus.edu ,junxiao.wang@kaust.edu.sa ,song.guo@polyu.edu.hk

Tóm tắt
Học tăng trưởng lớp trực tuyến (OCI) đã khơi dậy các phương pháp tiếp cận mới để mở rộng kiến thức mô hình được đào tạo trước đó từ các luồng dữ liệu đến tuần tự với các lớp mới. Thật không may, học OCI có thể bị tổn hại bởi hiện tượng quên thảm khốc (CF) khi các ranh giới quyết định cho các lớp cũ có thể trở nên không chính xác khi bị nhiễu loạn bởi các lớp mới. Các tài liệu hiện tại đã áp dụng tăng cường dữ liệu (DA) để giảm thiểu việc quên của mô hình, trong khi vai trò của DA trong OCI vẫn chưa được hiểu rõ cho đến nay. Trong bài báo này, chúng tôi chỉ ra về mặt lý thuyết rằng các mẫu được tăng cường với mối tương quan thấp hơn với dữ liệu gốc hiệu quả hơn trong việc ngăn chặn quên lãng. Tuy nhiên, việc tăng cường tích cực cũng có thể làm giảm tính nhất quán giữa dữ liệu và các nhãn tương ứng, điều này thúc đẩy chúng tôi khai thác DA phù hợp để tăng cường hiệu suất OCI và ngăn chặn vấn đề CF. Chúng tôi đề xuất phương pháp Mixup Cải tiến (EnMix) trộn các mẫu được tăng cường và các nhãn của chúng đồng thời, được chỉ ra là tăng cường đa dạng mẫu trong khi duy trì tính nhất quán mạnh mẽ với các nhãn tương ứng. Hơn nữa, để giải quyết vấn đề mất cân bằng lớp, chúng tôi thiết kế phương pháp Mixup Thích ứng (AdpMix) để hiệu chuẩn các ranh giới quyết định bằng cách trộn các mẫu từ cả lớp cũ và lớp mới và điều chỉnh động tỷ lệ trộn nhãn. Phương pháp của chúng tôi được chứng minh là hiệu quả trên một số bộ dữ liệu chuẩn thông qua các thí nghiệm mở rộng, và được chỉ ra là tương thích với các kỹ thuật dựa trên phát lại khác.

1. Giới thiệu
Học sâu (DL) đã đạt được những thành tựu đáng kể bằng cách bắt chước trí tuệ con người để khai thác kiến thức từ các bộ dữ liệu được thu thập cẩn thận. Được truyền cảm hứng thêm từ quá trình học tập của con người, học liên tục (CL), còn được gọi là học tăng trưởng, đã bước tiếp theo để mở rộng kiến thức mô hình từ một số nhiệm vụ đến tuần tự [25, 34, 36]. Tuy nhiên, trong khi học các nhiệm vụ mới, CL có thể bị tổn hại bởi hiện tượng quên thảm khốc (CF) rằng khả năng phân loại của mô hình có thể giảm nghiêm trọng trên các nhiệm vụ cũ [29, 12, 20]. Hiện tượng CF có thể đáng kể đặc biệt dưới các cài đặt thực tế hơn, tức là học tăng trưởng lớp trực tuyến (OCI), nơi các lớp mới liên tục đến với luồng dữ liệu và mỗi mẫu lô chỉ có thể được quan sát một lần [16, 28].

Các phương pháp dựa trên phát lại [6, 27, 33] đã được chỉ ra là hiệu quả chống lại CF cho OCI bằng cách lưu trữ các mẫu nhiệm vụ trong quá khứ trong bộ đệm bộ nhớ và phát lại chúng trong khi đào tạo cho các nhiệm vụ mới. Nghiên cứu gần đây [45, 46, 27, 15] đã nhằm mục đích giảm thiểu CF bằng cách thực hiện tăng cường dữ liệu (DA) [37, 35] cho cả các mẫu được đệm trước đó và mới đến. Các phương pháp này tập trung vào thiết kế tổn thất phức tạp kết hợp với các kỹ thuật học tương phản hoặc tự giám sát (SSL) [42, 30] để thay thế hàm tổn thất cross-entropy (CE) ít hiệu quả hơn bằng các hàm tổn thất tiên tiến như tổn thất InfoNCE [17] hoặc thông tin lẫn nhau [13]. Tuy nhiên, vai trò của DA trong OCI vẫn chưa rõ ràng và cần được khai thác. Khác với hiện trạng tài liệu, trong bài báo này, chúng tôi sẽ xem xét mặt đầu vào của OCI, khám phá điều gì cấu thành một DA tốt trong OCI và cách áp dụng nó một cách hiệu quả?

Một cách trực quan, DA có thể tạo ra các mẫu với tính đa dạng mở rộng, điều này được chỉ ra là có lợi cho học biểu diễn [31, 11, 41] và có thể ngăn chặn CF trong OCI. Tuy nhiên, trái với trực giác, [44] tiết lộ rằng việc áp dụng DA trực tiếp, mà không có nhiều lần lặp [5] trên phát lại kinh nghiệm (ER) [10], dẫn đến hiệu suất tồi tệ hơn so với ER không có tăng cường. Bất thường như vậy khuyến khích chúng tôi so sánh các phương pháp DA khác nhau cho OCI. Như được chỉ ra trong Hình 2, DA không phải lúc nào cũng gây hại cho hiệu suất OCI, nhưng việc chọn phương pháp tăng cường phù hợp và cường độ là quan trọng. Các kết quả chỉ ra rằng tăng cường tích cực có khả năng thực hiện tốt hơn trong OCI, trong khi DA quá mạnh có thể dẫn đến hiệu suất tệ hơn (Crop-1.3 tệ hơn Crop-0.8).

Để giải thích hiện tượng nêu trên, chúng tôi đã tiến hành phân tích lý thuyết về mối quan hệ giữa DA và việc quên lãng trong OCI. Phân tích của chúng tôi cho thấy rằng các mẫu được tăng cường từ bộ đệm bộ nhớ nên có hiệp phương sai thấp trên cross entropy trung bình của mô hình so với các mẫu gốc. Tuy nhiên, việc tăng cường quá mức với hiệp phương sai thấp có thể tạo ra các mẫu mới lệch khỏi nhãn thực tế, dẫn đến việc đưa vào thông tin sai lệch. Để giải quyết vấn đề này, chúng tôi đề xuất Mixup Cải tiến (EnMix), áp dụng mixup [43] trên các mẫu được tăng cường từ bộ nhớ để đảm bảo DA mạnh hơn trong khi vẫn duy trì tính nhất quán cao với nhãn của chúng, như được chỉ ra trong Hình 1. Ngoài ra, chúng tôi quan sát thấy rằng các ranh giới quyết định giữa các lớp cũ và mới bị thiên vị về phía các lớp cũ do mất cân bằng lớp [19, 8] trong OCI. Để giải quyết vấn đề này, chúng tôi đề xuất Mixup Thích ứng (AdpMix) để điều chỉnh các ranh giới quyết định giữa các lớp cũ và mới theo sự mất cân bằng trọng số từ bộ phân loại. Hai phương pháp chúng tôi đề xuất được gọi chung là DualMix.

Tóm lại, bài báo này có những đóng góp sau:
• Chúng tôi cung cấp một giải thích lý thuyết rằng hiệp phương sai thấp giữa các mẫu DA và các mẫu gốc về cross entropy trung bình là có lợi trong OCI. Ngoài ra, chúng tôi giới thiệu EnMix, tăng cường DA tiêu chuẩn bằng cách tạo ra các mẫu mạnh hơn và đáng tin cậy hơn.
• Phương pháp AdpMix được đề xuất để tạo ra sự cân bằng giữa các ranh giới quyết định của các lớp cũ và mới bằng cách điều chỉnh động tỷ lệ trộn của các nhãn.
• Các phương pháp tăng cường được đề xuất của chúng tôi, chỉ điều chỉnh mặt đầu vào, được chỉ ra là vượt trội hơn các phương pháp hiện có trên một số bộ dữ liệu chuẩn và tương thích với các kỹ thuật dựa trên phát lại khác, như được chứng minh qua các kết quả thực nghiệm mở rộng.

2. Công trình liên quan
2.1. Học liên tục trực tuyến dựa trên phát lại
Trong cài đặt học liên tục trực tuyến (OCL) [7, 24], dữ liệu đến theo từng lô nhỏ tuần tự, và các lô trước đó từ nhiệm vụ hiện tại và trước đó không thể được sử dụng lại, đưa ra thách thức của việc học một lần qua hiệu quả. OCL có thể được phân loại thành hai loại dựa trên việc liệu nhiệm vụ mới có chứa các lớp mới hay không: tăng trưởng lớp trực tuyến (OCI) và tăng trưởng miền trực tuyến (ODI) [2, 14]. Công trình này chủ yếu tập trung vào cài đặt OCI thách thức hơn, phụ thuộc rất nhiều vào các phương pháp dựa trên phát lại. Chaudhry et al. [10] đề xuất phát lại kinh nghiệm (ER) để lưu trữ một tập con dữ liệu từ các nhiệm vụ đã thấy trước đó trong bộ nhớ và phát lại chúng trong quá trình đào tạo trên các nhiệm vụ mới. Các biến thể của ER đã được phát triển để tối ưu hóa các chiến lược lựa chọn mẫu và học biểu diễn. A-GEM [9] ràng buộc gradient từ các mẫu bộ nhớ để ngăn chặn tổn thất trung bình từ các nhiệm vụ trước đó tăng lên. MIR [3] so sánh các mức tăng tổn thất của các mẫu bộ nhớ sau khi cập nhật mô hình dựa trên dữ liệu lô hiện tại để thực hiện truy xuất mẫu. GSS [4] lưu trữ các mẫu với tính đa dạng hơn trong các hướng gradient trong bộ nhớ. ASER [33] chấm điểm mỗi mẫu bằng giá trị Sharply theo khả năng bảo tồn các ranh giới quyết định tiềm ẩn của các lớp đã thấy trước đó. Mặc dù các phương pháp này nhằm mục đích ngăn chặn quên lãng bằng cách lưu trữ và xem lại các mẫu bộ nhớ đại diện, chúng không giải quyết vấn đề mất cân bằng lớp nghiêm trọng trong OCI dựa trên phát lại. GDumb [32] được phát triển đặc biệt để giải quyết vấn đề mất cân bằng lớp bằng cách tham lam giữ số lượng mẫu từ mỗi lớp cân bằng trong bộ nhớ và đào tạo mô hình chỉ với dữ liệu bộ nhớ. Tuy nhiên, nó giảm quá mức việc sử dụng dữ liệu nhiệm vụ mới, dẫn đến khai thác đặc trưng không đầy đủ. Trong công trình này, chúng tôi đề xuất hai phương pháp đơn giản nhưng hiệu quả để ngăn chặn quên lãng và giảm thiểu mất cân bằng lớp đồng thời. Các phương pháp của chúng tôi được thiết kế đặc biệt cho cài đặt OCI và chỉ điều chỉnh mặt đầu vào. Chúng tôi chứng minh tính ưu việt của các phương pháp của mình trên nhiều bộ dữ liệu chuẩn.

2.2. Tăng cường dữ liệu và học liên tục
Để giảm thiểu hiện tượng CF, nhiều nghiên cứu gần đây [27, 45, 15] đã tập trung vào học các biểu diễn với các đặc trưng phù hợp. Ví dụ, Mai et al. [27] đề xuất SCR, sử dụng học tương phản để khuyến khích gom cụm các mẫu từ cùng một lớp, và thay thế bộ phân loại tuyến tính bằng bộ phân loại Nearest-Class-Mean để giải quyết vấn đề trọng số mất cân bằng. Gu et al. [13] phân tích học biểu diễn với thông tin lẫn nhau và đề xuất DVC để giữ lại nhiều thông tin hơn từ các nhiệm vụ trước đó, cách tương tự với [15]. Zhu et al. [45] sử dụng hỗn hợp dữ liệu [43] để tạo ra các mẫu với các lớp mới ảo và cũng được truyền cảm hứng bởi SSL để tăng cường các lớp bằng cách xoay các mẫu đào tạo để thúc đẩy học các đặc trưng toàn diện [46]. Mặc dù các phương pháp này sử dụng DA để tạo ra các mẫu đa dạng, mục đích của chúng là thỏa mãn các yêu cầu cho học tương phản hoặc SSL, và chúng chưa kiểm tra vai trò của DA trong CL. Hơn nữa, các phương pháp này đòi hỏi thay đổi trong cấu trúc mô hình (như các đầu phân loại bổ sung cho các lớp mới) trong quá trình đào tạo. Trong bài báo này, chúng tôi điều tra vai trò của DA trong OCI dựa trên phát lại với các giải thích lý thuyết, và đề xuất hai phương pháp DA chỉ điều chỉnh mặt đầu vào.

3. Phương pháp
3.1. Hiểu biết rút ra từ phần trình bày lý thuyết
Trong bài báo này, chúng tôi nghiên cứu một cài đặt học OCI được giám sát như trong [26, 5, 23]. Xem xét một luồng dữ liệu D=fD1;D2;:::;DNg và Di= (Xi;Yi) là dữ liệu cho nhiệm vụ i. Xi và Yi đại diện cho các mẫu và nhãn tương ứng trong nhiệm vụ i. Yi\Yj=; cho i≠j. Trong giai đoạn đào tạo, dữ liệu Di từ nhiệm vụ i chỉ có thể được thấy một lần, có nghĩa là chỉ được phép một epoch để đào tạo. Mô hình có thể được chia thành trích xuất đặc trưng f, với tham số, và một bộ phân loại tuyến tính (đơn-đầu cho tất cả các lớp) với trọng số {wc}c=1^C (C là số lớp cho tất cả các nhiệm vụ, và chúng tôi bỏ qua độ lệch để đơn giản hóa). Chúng tôi ký hiệu hi=f(xi)∈Rd là các đặc trưng được trích xuất của mẫu xi. F(xi) = [wc^T f(xi;j)]^C/Σ∈RC là xác suất đầu ra của mẫu xi. () là phép toán softmax.

Khi mô hình được đào tạo trên nhiệm vụ t, theo [13], rủi ro thực nghiệm mà mô hình nhằm mục đích tối thiểu hóa được định nghĩa là:
Rt(F)def= E(x;y)∈Dt[L(y;F(x))] + E(x;y)∈D^M_{t-1}[L(y;F(x))]

trong đó D^M_{t-1} là bộ nhớ có kích thước cố định sau khi được đào tạo trên nhiệm vụ t-1. Chúng tôi bỏ qua  trong F để đơn giản. := |Dt|/|D^M| và := 1/(1 + 2|Dt|/|D[1;t]|). D[1;t] là tất cả dữ liệu đã thấy {D1;D2;:::;Dt}. Nếu chúng tôi áp dụng một phép biến đổi ngẫu nhiên g∈G trên các mẫu bộ nhớ, dữ liệu bộ nhớ mở rộng là D^Mg={x;g(x)|x∈D^M}. Rủi ro với DA của số hạng thứ hai trong Phương trình 1, chỉ ra kiến thức thu được từ các nhiệm vụ trong quá khứ, là:
R^Mg_t(F) = E(x;y)∈D^Mg_{t-1}[L(y;F(x))]

Mục tiêu của CL là tối thiểu hóa rủi ro thực nghiệm trên tất cả các nhiệm vụ đã thấy cho đến nay, vì vậy rủi ro thực nghiệm từ nhiệm vụ 1 đến t-1 mà không quên lãng nên là:
R^obj_{t-1}(F)def= E(x;y)∈D[1;t-1][L(y;F(x))]

Do đó, chúng tôi định nghĩa khoảng cách quên lãng là:
FGMg= Eg[R^Mg_t(F)−R^obj_{t-1}(F)]^2

Chúng tôi tiếp tục định nghĩa hiệp phương sai trung bình của tổn thất CE của mẫu trong tập dữ liệu D^Mg_{t-1} là:
COMg= E_{xi;xj∈D^Mg_{t-1}}[Cov[q(xi);q(xj)]]

trong đó Cov[;] đại diện cho hiệp phương sai. q được định nghĩa là q(xi) = yi^T log(F(xi)).

Hình 2 chỉ ra rằng nếu DA có thể tạo ra thông tin đa dạng hơn, thì vấn đề quên lãng có thể được giảm thiểu tương ứng. Được truyền cảm hứng từ [40], chúng tôi đưa ra phân tích chặt chẽ và chứng minh nó trong Phụ lục.

Mệnh đề 1. Cho một mô hình được đào tạo trên dữ liệu tuần tự {D1;D2;:::;Dt-1} với bộ đệm bộ nhớ D^M_{t-1}, có một nhiệm vụ mới với Dt. Xem xét các phép biến đổi dữ liệu g1∈G và g2∈G, chúng được áp dụng trên các mẫu bộ nhớ để thu được D^Mg1_{t-1} và D^Mg2_{t-1}. Nếu hiệp phương sai trung bình của tổn thất CE trong D^Mg1_t thấp hơn so với trong D^Mg2_t, tức là COMg1<COMg2, mô hình sẽ bị quên lãng ít hơn trên các nhiệm vụ trước đó, FGMg1<FGMg2.

Mệnh đề này chỉ ra rằng DA nên làm giảm sự tương quan giữa các mẫu được tăng cường và các mẫu gốc. Vì hiệp phương sai được xác định bởi đầu ra mô hình và việc quên lãng ít hơn có thể đạt được nếu mô hình thay đổi ít sau khi học nhiệm vụ mới, chúng tôi tính toán phương sai của xác suất đầu ra dựa trên mô hình cũ như một chỉ số để đánh giá cường độ của DA, được truyền cảm hứng từ [40]:
σ=1/Cold Σ_{i∈[Cold]}(VarMg(μ))^{1/2}; μ=1/|Mg| Σ_{xi∈Mg}F(xi)

Xem Phần 4.5 cho các kết quả thực nghiệm.

3.2. Tăng cường mixup được cải tiến
Như được mô tả trong Phần 3.1, chúng tôi nên tiến hành DA mạnh trong bộ nhớ cho OCI để giảm thiểu quên lãng. Tuy nhiên, khi DA quá mạnh, tức là xác suất đầu ra của view được tăng cường xa so với mẫu gốc, view được tăng cường có khả năng giữ tương quan yếu với nhãn, điều này sẽ phá hủy việc học có giám sát (ví dụ "Crop-1.3" với DA mạnh hơn nhưng hiệu suất kém hơn "Crop-0.8" trong Hình 2). Làm thế nào chúng ta có thể giảm thêm sự tương quan giữa các view được tăng cường và các mẫu gốc mà không phá hủy sự tương ứng với nhãn của chúng?

Để giải quyết vấn đề, chúng tôi đề xuất một phương pháp đơn giản nhưng hiệu quả, mixup được cải tiến (EnMix), dựa trên DA chuyên sâu với sự tương quan nhãn đáng tin cậy. Cho một phép biến đổi dữ liệu g∈G và một mẫu x, view được tăng cường sẽ là ~x=g(x). EnMix xây dựng các mẫu ảo bằng cách trộn các view được tăng cường từ các mẫu gốc khác nhau và cũng nội suy tuyến tính các nhãn của chúng:
~xe=λ~xi+(1−λ)~xj
~ye=λyi+(1−λ)yj

trong đó λ∈[0;1] và λ∼Beta(α;α), α∈(0;1). Do trộn với các mẫu khác (bao gồm các lớp khác) trong bộ nhớ, xác suất đầu ra mô hình của các mẫu trộn nên không tương tự với các mẫu gốc. Điều quan trọng là nhãn cũng được trộn, xây dựng sự tương quan giữa các mẫu được cải tiến với các nhãn khác nhau. Thông qua cách này, chúng tôi không chỉ giảm sự tương quan giữa các view được tăng cường và các mẫu gốc, mà còn bảo tồn tính nhất quán giữa các view và nhãn. Quy trình làm việc về cách trộn được chỉ ra như Hình 3.

3.3. Mixup thích ứng để cân bằng
Trong các phần trên, chúng tôi cố gắng tạo ra các mẫu mới thông qua DA, làm phong phú đa dạng của dữ liệu nhiệm vụ trước đó trong bộ nhớ và giảm thiểu vấn đề CF. Tuy nhiên, theo các công trình trước đó [26], hiện tượng CF xuất hiện không chỉ vì việc quên lãng về kiến thức trước đó, mà còn do tính chất mất cân bằng lớp vốn có trong OCI dựa trên phát lại. Số lượng mẫu từ các lớp mới thường lớn hơn số lượng mẫu từ các lớp cũ trong bộ nhớ, và sự mất cân bằng này tăng cường khi số lượng nhiệm vụ tăng lên.

Sự mất cân bằng lớp cuối cùng dẫn đến ranh giới quyết định bị thiên vị nghiêm trọng về phía các lớp cũ. Để xác minh điều này, chúng tôi ký hiệu tỷ lệ phân loại sai theo [26]: er(n;o) biểu thị tỷ lệ các mẫu kiểm tra lớp mới bị phân loại sai thành lớp cũ so với tổng số mẫu lớp mới bị phân loại sai. Quy tắc ký hiệu tương tự được áp dụng cho er(o;n). Như được chỉ ra trong Hình 4, các mẫu từ lớp cũ có thể bị phân loại không chính xác thành lớp mới với xác suất cao và xác suất các mẫu từ lớp mới bị phân loại sai thành lớp cũ tương đối nhỏ, chỉ ra rằng các ranh giới quyết định giữa lớp cũ và lớp mới bị mất cân bằng nghiêm trọng. Hơn nữa, khi kích thước bộ nhớ giảm, sự mất cân bằng này càng trở nên trầm trọng hơn. Để điều chỉnh các ranh giới quyết định, DA cũng có thể là một phương pháp đơn giản. Thực hiện DA trên bộ nhớ mở rộng đa dạng mẫu, và cũng cân bằng số lượng ở một mức độ nhỏ. Tuy nhiên, tính ngẫu nhiên của DA vẫn hạn chế hiệu quả của nó trên OCI.

Nhằm mục đích điều chỉnh ranh giới quyết định một cách trực tiếp hơn, chúng tôi khuyến khích DA tạo ra các mẫu gần các ranh giới quyết định bằng cách trộn các mẫu từ lớp cũ và lớp mới. Để đẩy ranh giới quyết định xa hơn khỏi lớp cũ, chúng tôi điều chỉnh thích ứng tỷ lệ trộn trên nhãn bằng AdpMix:
xa=λx·xi+(1−λx)·xj
ye=λy·yi+(1−λy)·yj

trong đó xi và xj từ lớp cũ (dữ liệu bộ nhớ) và lớp mới (dữ liệu nhiệm vụ hiện tại). Phương trình 8 tương tự như Phương trình 7. Sự khác biệt là hai mặt: 1) EnMix trộn các mẫu được tăng cường từ bộ nhớ và AdpMix trộn dữ liệu gốc từ bộ nhớ và tập dữ liệu nhiệm vụ sắp tới. 2) EnMix sử dụng cùng tỷ lệ cho dữ liệu thô và nhãn trong khi tỷ lệ λy cho nhãn khác với tỷ lệ λx cho dữ liệu trong AdpMix. Ý tưởng chính là khi ranh giới quyết định quá gần với lớp cũ, tỷ lệ trộn λy cũng nên thiên vị hơn về phía lớp cũ, tức là cho λy một giá trị lớn hơn so với λx.

Lý do chính cho sự mất cân bằng lớp là từ vấn đề số lượng, nhưng trong OCI, chúng ta không thể biết số lượng mẫu nhiệm vụ mới trước, làm cho việc suy ra giá trị λ dựa trên khoảng cách số lượng không phù hợp. Ngoài ra, CL giữ lại kiến thức được lưu trữ trong bộ nhớ bằng cách học từ các nhiệm vụ trước đó. Do đó, không đáng tin cậy để đánh giá sự mất cân bằng chỉ bằng kích thước mẫu. Một công trình gần đây [1] tiết lộ rằng sự mất cân bằng lớp dẫn đến trọng số w của bộ phân loại tuyến tính bị thiên vị. Do đó, chúng tôi sử dụng trọng số w để thiết kế sơ đồ điều chỉnh của λy:
λy={(min(λx+||w_new||/||w_old||,1); ||w_new||/||w_old||>γ; λx>β; λx; khác)}

trong đó old hoặc new biểu thị một chỉ số cho lớp cũ hoặc lớp mới tương ứng. γ, β và α là các siêu tham số. γ được sử dụng để xác nhận sự xuất hiện của mất cân bằng lớp. α và β kiểm soát mức độ của điều chỉnh thích ứng. Khi sự lệch ranh giới quyết định xảy ra, chúng tôi đẩy nó xa khỏi lớp cũ. Độ lớn của điều chỉnh được tính theo giá trị chuẩn L-2 tương đối của trọng số.

Chúng tôi viết lại xác suất đầu ra của lớp c để chỉ ra cách AdpMix có thể điều chỉnh việc cập nhật trọng số w:
pi;c=exp(wc^T hi)/Σ_{j=1}^C exp(wj^T hj)

Để đơn giản, chúng tôi giả sử rằng bộ đệm không được cập nhật trong quá trình đào tạo của nhiệm vụ hiện tại t, tức là dữ liệu nhiệm vụ trước đó và dữ liệu nhiệm vụ hiện tại được chia rõ ràng thành D^M và Dt. Tổn thất CE được tính như:
L=Σ_{xi∈Dt∪D^M} Σ_{c=1}^C I{yi=c} log pi;c

trong đó I{} là hàm chỉ báo. Theo tính toán gradient của tổn thất CE, chúng ta có thể thu được công thức cập nhật của wc như:
wc=wc+η[Σ_{xi∈Dt;yi=c}(1−pi;c)hi−Σ_{xi∈Dt;yi≠c}pi;c hi] + η[Σ_{xi∈D^M;yi=c}(1−pi;c)hi−Σ_{xi∈D^M;yi≠c}pi;c hi]

Vì hàm kích hoạt cho trích xuất đặc trưng [18] thường là ReLU, hi luôn dương. Do số lượng mẫu trong Dt thường lớn hơn nhiều so với mỗi mẫu lớp trong D^M, lợi ích hoặc giảm cho wc chủ yếu bị ảnh hưởng bởi nhiệm vụ hiện tại Dt. Chúng ta có thể thấy rằng trọng số của lớp c từ nhiệm vụ cũ bị suy yếu cực kỳ vì không có mẫu với lớp c trong Dt (số hạng đầu tiên trong dữ liệu nhiệm vụ hiện tại bằng 0), trong khi trọng số của lớp từ nhiệm vụ hiện tại sẽ được tăng lên hiệu quả. Khi chúng ta áp dụng AdpMix như Phương trình 8, Phương trình 12 sẽ được thay đổi thành (số hạng dữ liệu bộ nhớ được bỏ qua vì ảnh hưởng ít hơn):
wc=wc+η[Σ_{xi∈Dt;yi=c}(λy−pi;c)hi−Σ_{xi∈Dt;yi≠c}pi;c hi]

Khi c thuộc về nhiệm vụ hiện tại, tỷ lệ λy làm giảm sự tăng trưởng quá mức của trọng số tương ứng. Khi c là một lớp cũ, hoạt động hỗn hợp làm cho lợi ích không bằng 0. Và chúng ta tiếp tục cân bằng trọng số bằng cách điều chỉnh thích ứng λy. Điều này cũng chỉ ra rằng việc điều chỉnh tỷ lệ trộn theo trọng số của bộ phân loại là hợp lý.

4. Đánh giá
4.1. Bộ dữ liệu
Split CIFAR-100 được xây dựng bằng cách chia bộ dữ liệu CIFAR-100 [21] thành 20 nhiệm vụ mà không có lớp nào chồng lấp giữa mỗi nhiệm vụ. Mỗi nhiệm vụ được gán ngẫu nhiên 5 lớp dữ liệu. Kích thước hình ảnh là 32×32×3. Có tổng cộng 3,000 hình ảnh trong mỗi lớp, được chia thành 2,500 mẫu đào tạo và 500 mẫu kiểm tra.

Split Mini-ImageNet chia bộ dữ liệu Mini-ImageNet [38], chứa 100 lớp, thành 20 nhiệm vụ rời rạc như trong [26]. Mỗi nhiệm vụ bao gồm 5 lớp ngẫu nhiên, và mỗi lớp bao gồm 500 hình ảnh 84×84×3 để đào tạo và 100 hình ảnh để kiểm tra.

Split Tiny-ImageNet được sử dụng để xác minh hiệu quả của thuật toán trong các tình huống phức tạp hơn. Chúng tôi chia Tiny-ImageNet [22] thành 20 nhiệm vụ rời rạc và mỗi nhiệm vụ chứa 10 lớp. Mỗi lớp chứa 500 hình ảnh 64×64×3 để đào tạo và 100 hình ảnh để kiểm tra.

4.2. Đường cơ sở và thước đo
Các đường cơ sở mà chúng tôi so sánh là các phương pháp được đề cập trong Phần 2: A-GEM, và ER, GSS, MIR, ASER, tập trung vào lựa chọn mẫu, và SCR và DVC, hai thuật toán với DA tiêu chuẩn. Chúng tôi áp dụng DualMix của mình trên bốn phương pháp dựa trên ER và so sánh chúng với A-GEM, SCR và DVC. Finetune và iif offline được sử dụng làm giới hạn dưới và trên với cùng cài đặt trong [26].

Chúng tôi sử dụng hai thước đo tiêu chuẩn trong học liên tục để đo lường hiệu suất: Độ chính xác trung bình và Quên lãng trung bình. Đặt ai;j là độ chính xác của mô hình trên tập kiểm tra của nhiệm vụ j sau khi được đào tạo từ nhiệm vụ 1 đến nhiệm vụ i. fi;j đại diện cho mô hình đã quên bao nhiều về nhiệm vụ j sau khi được đào tạo trên nhiệm vụ i. Hai thước đo được định nghĩa là:
Độ chính xác trung bình (Ai) = 1/i Σ_{j=1}^i ai;j
Quên lãng trung bình (Fi) = 1/(i-1) Σ_{j=1}^{i-1} fi;j
trong đó fk;j= max_{l∈{1,...,k-1}}(al;j)−ak;j; ∀j<k

4.3. Chi tiết thực hiện
Chúng tôi sử dụng ResNet-18 được giảm cho tất cả các bộ dữ liệu như trong [26, 13]. Một đầu đơn được sử dụng cho tất cả các lớp. DA bình thường được sử dụng ở đây là sự kết hợp của bốn phương pháp tăng cường như được sử dụng trong [13, 27]: cắt ngẫu nhiên, lật ngang, nhiễu màu và thang độ xám. Chúng tôi sử dụng Stochastic Gradient Descent (SGD) để tối ưu hóa mô hình và đặt tốc độ học là 0.1, kích thước lô là 10. α được đặt là 0.2. γ, β và α được đặt lần lượt là 2.0, 0.5 và 0.05. EnMix và AdpMix đều được kết hợp với DA tiêu chuẩn. Tất cả kết quả thí nghiệm chúng tôi trình bày là trung bình của 10 lần chạy, được thực hiện trên một GPU NVIDIA GeForce RTX 3090.

4.4. Đánh giá hiệu suất so sánh
Hiệu suất độ chính xác. Các kết quả độ chính xác được minh họa trong Bảng 1. Chúng tôi áp dụng chiến lược DualMix của mình trên bốn phương pháp dựa trên ER, ER, GSS, MIR và ASER, tối ưu hóa lựa chọn mẫu cho bộ nhớ trong OCI. Theo kết quả, DualMix có thể cải thiện các phương pháp dựa trên ER đáng kể trong ba bộ dữ liệu, cho thấy rằng phương pháp trộn của chúng tôi hiệu quả trên nhiều phân phối mẫu khác nhau. Như chúng ta có thể thấy, phương pháp của chúng tôi thực hiện tốt hơn trong bộ nhớ tương đối nhỏ (gấp đôi độ chính xác trên CIFAR-100 với ER và MIR), do thiếu nghiêm trọng đa dạng dữ liệu trong cài đặt như vậy. Trong Mini-ImageNet, phương pháp của chúng tôi cũng có thể đạt được 69.2% và 40.3% cải thiện hiệu suất với bộ nhớ 1K và 2K tương ứng. Thú vị là, so với ER, lợi ích trên các phương pháp dựa trên ER khác tương đối nhỏ. Trực giác, sự kết hợp của các phương pháp tốt hơn nên dẫn đến hiệu suất tốt hơn. Tuy nhiên, phương pháp của chúng tôi tăng hiệu suất trên ASER và GSS với mức độ hạn chế. Một mặt, điều này có thể do hiệu suất vốn có tiên tiến của phương pháp này, làm cho việc tiếp tục cải thiện trở nên khó khăn. Mặt khác, chiến lược lựa chọn mẫu tạo ra một phân phối khá khác với dữ liệu gốc, dẫn đến tăng cường thiên vị. Ngược lại, các mẫu thu được bởi ER thông qua lấy mẫu hồ chứa [39] đồng đều hơn. Kết hợp với DA, có tiềm năng lớn hơn để thu được các mẫu đa dạng và đầy đủ hơn. Ngoài việc cải thiện hiệu quả hiệu suất của một số phương pháp hiện tại, chiến lược của chúng tôi cũng chứng minh tính ưu việt so với các phương pháp hiện đại, SCR và DVC. Để so sánh công bằng, kích thước lô bộ nhớ của SCR giống với DualMix và DVC của chúng tôi. Có thể thấy rằng phương pháp của chúng tôi thực hiện tốt hơn đáng kể so với DVC trong hầu hết các trường hợp, mặc dù thực tế là DVC và SCR vẫn yêu cầu các hàm tổn thất mới để tăng cường khám phá đặc trưng về các nhiệm vụ cũ và hiện tại. Những kết quả này cho thấy rằng DA đóng một vai trò quan trọng trong OCI, và tiềm năng của nó chưa được khai thác đầy đủ trong các phương pháp tiếp cận trước đây.

Tỷ lệ quên lãng. Bảng 2 chỉ ra Quên lãng trung bình vào cuối đào tạo. Chúng tôi áp dụng DualMix trên ER để so sánh với các đường cơ sở khác trong CIFAR-100, Mini-ImageNet và Tiny-ImageNet. Chúng tôi không minh họa kết quả Quên lãng trung bình của SCR vì nó thực hiện kém trên các nhiệm vụ do kích thước lô bộ nhớ hạn chế. Phương pháp của chúng tôi có thể đạt được quên lãng thấp nhất trên ba bộ dữ liệu chuẩn với các kích thước bộ nhớ khác nhau. Trong CIFAR-100, phương pháp của chúng tôi có thể đạt được 8.7%−18.0% giảm Quên lãng trung bình so với đường cơ sở mạnh nhất DVC. Con số này trong Tiny-ImageNet thậm chí có thể đạt 38.1% với bộ nhớ 2K, mở ra một khoảng cách lớn với các phương pháp khác.

4.5. Nghiên cứu loại bỏ
Hiệu quả của từng thành phần. Chúng tôi điều tra hiệu quả của từng thành phần của phương pháp chúng tôi. Như được chỉ ra trong Bảng 3, một DA mạnh đã có thể tạo ra cải thiện so với đường cơ sở. Áp dụng EnMix mạnh hơn, cải thiện lớn hơn nữa xảy ra trên Độ chính xác trung bình và Quên lãng trung bình trong tất cả các trường hợp. Ngoài ra, AdpMix cũng cải thiện hiệu suất của mô hình trong các thí nghiệm khác nhau, thậm chí còn đáng kể hơn EnMix. Sau khi kết hợp tất cả các thành phần, phương pháp của chúng tôi đạt được hiệu suất tốt nhất đặc biệt khi bộ nhớ nhỏ. Đây là một hiện tượng trực quan, làm nổi bật tầm quan trọng của tăng cường dữ liệu để làm phong phú đa dạng và giảm thiểu sự mất cân bằng lớp. Các kết quả chỉ ra rằng mỗi thành phần của chúng tôi đều cần thiết.

Kết quả thực nghiệm về EnMix và tương quan. Để xác minh mệnh đề 1 của chúng tôi, chúng tôi sử dụng phương sai trung bình của xác suất đầu ra dựa trên các mô hình cũ, như được mô tả trong Phương trình 6, để chứng minh hiệu quả của EnMix. Trong Hình 5, chúng tôi vẽ các điểm phân tán của σ và Độ chính xác trung bình cuối cùng trên CIFAR-100 với các chiến lược DA khác nhau. Chúng ta có thể thấy rằng tồn tại một sự tương quan âm rõ ràng giữa hiệp phương sai và độ chính xác. "Flip", "Colorjitter" và "GrayScale" có cường độ tương đối yếu, vì vậy các mẫu tăng cường có tương quan cao hơn với các mẫu gốc, dẫn đến cải thiện yếu hơn. Cường độ của "Crop" và sự kết hợp của chúng mạnh hơn và EnMix của chúng tôi thúc đẩy chúng thêm, đạt được hiệu suất tốt hơn.

Ranh giới quyết định chính xác với AdpMix. Như chúng tôi đã thảo luận trong 3.1, tồn tại ranh giới quyết định bị thiên vị nghiêm trọng giữa lớp cũ và lớp mới. Do đó, chúng tôi đề xuất AdpMix để đẩy ranh giới quyết định xa khỏi các mẫu của lớp cũ. Như được chỉ ra trong Hình 6, chúng tôi vẽ tỷ lệ lỗi của er(n;o) và er(o;n) khi chúng tôi áp dụng AdpMix trên ER. So với các kết quả trong Hình 4, er(o;n) giảm đáng kể và khoảng cách giữa er(o;n) và er(n;o) gần như bị xóa sổ. Điều này cho thấy rằng phương pháp của chúng tôi đã sửa chữa ranh giới quyết định trước đó.

So sánh thời gian chạy. Phương pháp của chúng tôi chỉ thực hiện tăng cường trộn để làm phong phú các mẫu, thêm một lượng thời gian đào tạo nhất định. Tuy nhiên, chi phí thời gian tăng của phương pháp chúng tôi khá nhỏ so với một số phương pháp lựa chọn mẫu khác hoặc các phương pháp liên quan đến tương phản. Chúng ta có thể thấy từ Hình 7 rằng ER và A-GEM, sử dụng phương pháp hồ chứa để cập nhật và truy xuất bộ nhớ, có thời gian đào tạo ngắn nhất. Các chiến lược lựa chọn mẫu khác nhau được áp dụng trong các phương pháp tiếp cận khác, tăng đáng kể thời gian chạy đặc biệt là SCR, DVC và GSS. Phương pháp của chúng tôi chỉ tăng cường dữ liệu bộ nhớ, và thời gian chạy tăng ít hơn và trong phạm vi có thể kiểm soát.

5. Thảo luận
Học liên tục có thể rơi vào hiệu suất kém vì quên lãng thảm khốc. Chúng tôi xem xét lại chiến lược tăng cường dữ liệu trong học tăng trưởng lớp trực tuyến, một cài đặt thách thức và thực tế hơn. Chúng tôi chứng minh rằng tăng cường tích cực tạo ra các mẫu với tương quan thấp với các mẫu gốc có lợi cho việc tránh quên lãng. Tuy nhiên, tính nhất quán nhãn xảy ra khi tăng cường tiêu chuẩn quá mạnh. Chúng tôi đề xuất Mixup Cải tiến (EnMix), trộn các mẫu được tăng cường và nhãn của chúng dựa trên DA tiêu chuẩn, dẫn đến tăng đa dạng mẫu và tính nhất quán với nhãn. Hơn nữa, để giải quyết sự mất cân bằng lớp trong OCI dựa trên phát lại, chúng tôi giới thiệu Mixup Thích ứng (AdpMix) để trộn các mẫu từ lớp cũ và lớp mới, có thể hiệu chuẩn lại ranh giới quyết định bị thiên vị. Phương pháp của chúng tôi có thể được kết hợp trực tiếp với các phương pháp hiện tại và thúc đẩy chúng một cách hiệu quả. Thực hành trong bài báo này cho thấy rằng CF trong CL có thể được giảm thiểu rất nhiều chỉ thông qua chiến lược DA phù hợp.

Tài liệu tham khảo
[1] Hongjoon Ahn và Taesup Moon. A simple class decision balancing for incremental learning. arXiv preprint arXiv:2003.13947, 4, 2020. 5
[2] Motasem Alfarra, Zhipeng Cai, Adel Bibi, Bernard Ghanem, và Matthias Müller. Simcs: Simulation for online domain-incremental continual segmentation. arXiv preprint arXiv:2211.16234, 2022. 2
[3] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin, và Lucas Page-Caccia. Online continual learning with maximal interfered retrieval. Trong H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, và R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. 2
[4] Rahaf Aljundi, Min Lin, Baptiste Goujaud, và Yoshua Bengio. Gradient based sample selection for online continual learning. Advances in neural information processing systems, 32, 2019. 2
[5] Rahaf Aljundi, Min Lin, Baptiste Goujaud, và Yoshua Bengio. Online continual learning with no task boundaries. arXiv preprint arXiv:1903.08671, 3, 2019. 2, 3
[6] Pietro Buzzega, Matteo Boschini, Angelo Porrello, và Simone Calderara. Rethinking experience replay: a bag of tricks for continual learning. Trong 2020 25th International Conference on Pattern Recognition (ICPR), pages 2180–2187. IEEE, 2021. 1
[7] Zhipeng Cai, Ozan Sener, và Vladlen Koltun. Online continual learning with natural distribution shifts: An empirical study with visual data. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8281–8290, 2021. 2
[8] Hyuntak Cha, Jaeho Lee, và Jinwoo Shin. Co2l: Contrastive continual learning. Trong Proceedings of the IEEE/CVF International conference on computer vision, pages 9516–9525, 2021. 2
[9] Arslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, và Mohamed Elhoseiny. Efficient lifelong learning with a-gem. arXiv preprint arXiv:1812.00420, 2018. 2
[10] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, và Marc'Aurelio Ranzato. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486, 2019. 2
[11] Sylvia Frühwirth-Schnatter. Data augmentation and dynamic linear models. Journal of time series analysis, 15(2):183–202, 1994. 2
[12] Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, và Yoshua Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211, 2013. 1
[13] Yanan Gu, Xu Yang, Kun Wei, và Cheng Deng. Not just selection, but exploration: Online class-incremental continual learning via dual view consistency. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7442–7451, 2022. 2, 3, 6
[14] Nuwan Gunasekara, Heitor Gomes, Albert Bifet, và Bernhard Pfahringer. Adaptive online domain incremental continual learning. Trong Artificial Neural Networks and Machine Learning–ICANN 2022: 31st International Conference on Artificial Neural Networks, Bristol, UK, September 6–9, 2022, Proceedings, Part I, pages 491–502. Springer, 2022. 2
[15] Yiduo Guo, Bing Liu, và Dongyan Zhao. Online continual learning through mutual information maximization. Trong International Conference on Machine Learning, pages 8109–8126. PMLR, 2022. 1, 3
[16] Jiangpeng He, Runyu Mao, Zeman Shao, và Fengqing Zhu. Incremental learning in online scenario. Trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13926–13935, 2020. 1
[17] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, và Ross Girshick. Momentum contrast for unsupervised visual representation learning. Trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729–9738, 2020. 2
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image recognition. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 5
[19] Chris Dongjoo Kim, Jinseo Jeong, và Gunhee Kim. Imbalanced continual learning with partitioning reservoir sampling. Trong Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIII 16, pages 411–428. Springer, 2020. 2
[20] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. 1
[21] A Krizhevsky. Learning multiple layers of features from tiny images. Master's thesis, University of Toronto, 2009. 6
[22] Ya Le và Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015. 6
[23] Huiwei Lin, Shanshan Feng, Xutao Li, Wentao Li, và Yunming Ye. Anchor assisted experience replay for online class-incremental learning. IEEE Transactions on Circuits and Systems for Video Technology, 2022. 3
[24] Bing Liu. Learning on the job: Online lifelong and continual learning. Trong Proceedings of the AAAI Conference on Artificial Intelligence, pages 13544–13549, 2020. 2
[25] David Lopez-Paz và Marc'Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30, 2017. 1
[26] Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, và Scott Sanner. Online continual learning in image classification: An empirical survey. Neurocomputing, 469:28–51, 2022. 3, 4, 6
[27] Zheda Mai, Ruiwen Li, Hyunwoo Kim, và Scott Sanner. Supervised contrastive replay: Revisiting the nearest class mean classifier in online class-incremental continual learning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3589–3599, 2021. 1, 3, 6
[28] Marc Masana, Xialei Liu, Bartłomiej Twardowski, Mikel Menta, Andrew D Bagdanov, và Joost van de Weijer. Class-incremental learning: survey and performance evaluation on image classification. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. 1
[29] Michael McCloskey và Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. Trong Psychology of learning and motivation, volume 24, pages 109–165. Elsevier, 1989. 1
[30] Mehdi Noroozi, Ananth Vinjimoor, Paolo Favaro, và Hamed Pirsiavash. Boosting self-supervised learning via knowledge transfer. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, pages 9359–9367, 2018. 2
[31] Luis Perez và Jason Wang. The effectiveness of data augmentation in image classification using deep learning. arXiv preprint arXiv:1712.04621, 2017. 2
[32] Ameya Prabhu, Philip HS Torr, và Puneet K Dokania. Gdumb: A simple approach that questions our progress in continual learning. Trong Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16, pages 524–540. Springer, 2020. 3
[33] Dongsub Shim, Zheda Mai, Jihwan Jeong, Scott Sanner, Hyunwoo Kim, và Jongseong Jang. Online class-incremental continual learning with adversarial shapley value. Trong Proceedings of the AAAI Conference on Artificial Intelligence, pages 9630–9638, 2021. 1, 3
[34] Hanul Shin, Jung Kwon Lee, Jaehong Kim, và Jiwon Kim. Continual learning with deep generative replay. Advances in neural information processing systems, 30, 2017. 1
[35] Connor Shorten và Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning. Journal of big data, 6(1):1–48, 2019. 1
[36] Gido M Van de Ven và Andreas S Tolias. Three scenarios for continual learning. arXiv preprint arXiv:1904.07734, 2019. 1
[37] David A Van Dyk và Xiao-Li Meng. The art of data augmentation. Journal of Computational and Graphical Statistics, 10(1):1–50, 2001. 1
[38] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. Advances in neural information processing systems, 29, 2016. 6
[39] Jeffrey S Vitter. Random sampling with a reservoir. ACM Transactions on Mathematical Software (TOMS), 11(1):37–57, 1985. 7
[40] Huan Wang, Suhas Lohit, Michael Jeffrey Jones, và Yun Fu. What makes a "good" data augmentation in knowledge distillation-a statistical perspective. Trong Advances in Neural Information Processing Systems, 2022. 3, 4
[41] Sebastien C Wong, Adam Gatt, Victor Stamatescu, và Mark D McDonnell. Understanding data augmentation for classification: when to warp? Trong 2016 international conference on digital image computing: techniques and applications (DICTA), pages 1–6. IEEE, 2016. 2
[42] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, và Stéphane Deny. Barlow twins: Self-supervised learning via redundancy reduction. Trong International Conference on Machine Learning, pages 12310–12320. PMLR, 2021. 2
[43] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, và David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017. 2, 3
[44] Yaqian Zhang, Bernhard Pfahringer, Eibe Frank, Albert Bifet, Nick Jin Sean Lim, và Yunzhe Jia. A simple but strong baseline for online continual learning: Repeated augmented rehearsal. arXiv preprint arXiv:2209.13917, 2022. 2
[45] Fei Zhu, Zhen Cheng, Xu-Yao Zhang, và Cheng-lin Liu. Class-incremental learning via dual augmentation. Advances in Neural Information Processing Systems, 34:14306–14318, 2021. 1, 3
[46] Fei Zhu, Xu-Yao Zhang, Chuang Wang, Fei Yin, và Cheng-Lin Liu. Prototype augmentation and self-supervision for incremental learning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5871–5880, 2021. 1, 3
