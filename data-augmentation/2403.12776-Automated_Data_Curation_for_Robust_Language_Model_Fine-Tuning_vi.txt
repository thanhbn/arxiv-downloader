# Tự động hóa Tuyển chọn Dữ liệu cho Việc Tinh chỉnh Mô hình Ngôn ngữ Mạnh mẽ

Jiuhai Chen
Đại học Maryland, Cleanlab
jchen169@umd.edu

Jonas Mueller
Cleanlab
jonas@cleanlab.ai

## Tóm tắt

Các Mô hình Ngôn ngữ Lớn đã trở thành phương pháp chuẩn mực cho các tác vụ tạo văn bản chuỗi-đến-chuỗi, nhưng đối với các tác vụ/lĩnh vực chuyên biệt, một LLM được tiền huấn luyện thiếu khả năng cụ thể để tạo ra các phản hồi chính xác hoặc được định dạng tốt. Tinh chỉnh có giám sát chuyên biệt hóa một LLM bằng cách huấn luyện nó trên tập dữ liệu gồm các prompt mẫu với các phản hồi mục tiêu, nhưng dữ liệu thực tế có xu hướng nhiễu. Trong khi tồn tại nhiều thuật toán tinh chỉnh, ở đây chúng tôi xem xét góc nhìn AI tập trung vào dữ liệu về việc tinh chỉnh LLM, nghiên cứu cách tuyển chọn có hệ thống tập dữ liệu huấn luyện để cải thiện LLM được tạo ra thông qua bất kỳ thuật toán tinh chỉnh nào.

Chúng tôi giới thiệu một pipeline tuyển chọn dữ liệu tự động CLEAR (Đánh giá và Chỉnh sửa LLM dựa trên Độ tin cậy) cho các tập dữ liệu điều chỉnh hướng dẫn, có thể được sử dụng với bất kỳ LLM và quy trình tinh chỉnh nào. CLEAR ước tính dữ liệu huấn luyện nào có chất lượng thấp và lọc hoặc chỉnh sửa nó. Việc tự động xác định dữ liệu nào cần lọc hoặc chỉnh sửa được thực hiện thông qua các ước tính độ tin cậy từ LLM, để đảm bảo chỉ những thay đổi tin cậy đối với tập dữ liệu. Khác với các kỹ thuật tuyển chọn dữ liệu hiện có, CLEAR là một khung toàn diện có thể cải thiện tập dữ liệu (và đầu ra mô hình được huấn luyện) mà không cần tính toán tinh chỉnh bổ sung. Chúng tôi không giả định việc tiếp cận với một LLM mạnh hơn so với mô hình đang được tinh chỉnh (ví dụ: dựa vào GPT-4 khi tinh chỉnh GPT-3.5), để xem liệu CLEAR có thể cải thiện một cách có ý nghĩa khả năng của bất kỳ LLM nào. Các thí nghiệm cho thấy rằng CLEAR liên tục cải thiện hiệu suất của các mô hình được tinh chỉnh trên nhiều tập dữ liệu và mô hình (như GPT-3.5 và Llama2).

## 1 Giới thiệu

Các Mô hình Ngôn ngữ Lớn (LLM) được tiền huấn luyện trên các kho văn bản quy mô internet đã cho thấy khả năng đáng chú ý trong việc tạo ra văn bản giống con người hữu ích (Brown et al., 2020; Touvron et al., 2023). Tuy nhiên, hiệu quả của LLM trong các lĩnh vực hoặc tác vụ chuyên biệt phụ thuộc vào quá trình điều chỉnh hướng dẫn (tức là tinh chỉnh có giám sát, hoặc căn chỉnh), trong đó các mô hình được tiền huấn luyện được tiếp tục huấn luyện sử dụng các tập dữ liệu đại diện tốt cho lĩnh vực đó (Wei et al., 2022). Ở đây chúng tôi xem xét các tập dữ liệu huấn luyện chuỗi-đến-chuỗi gồm các cặp (prompt, phản hồi mục tiêu). Sau khi huấn luyện, chúng tôi cung cấp cho LLM các prompt mới từ cùng lĩnh vực và muốn nó tạo ra các phản hồi giống với các mục tiêu mong đợi.

Vì các LLM có tỷ parameter tự động hấp thụ các mẫu/thông tin trên toàn bộ tập dữ liệu, chất lượng của dữ liệu điều chỉnh hướng dẫn là tối quan trọng đối với việc tinh chỉnh hiệu quả (Zhou et al., 2023a; Xu et al., 2023; Kong et al., 2023). Thật không may, hầu hết các tập dữ liệu điều chỉnh hướng dẫn thực tế đều nhiễu, chứa các ví dụ có chất lượng thấp theo nhiều cách khác nhau: phản hồi mục tiêu có thể không chính xác, viết kém, prompt có thể vô nghĩa/không đầy đủ/mơ hồ, hoặc cả hai có thể không liên quan do lỗi xử lý dữ liệu. Dữ liệu huấn luyện khuyết tật như vậy dẫn đến các mô hình được tinh chỉnh có đầu ra không chính xác, không liên quan, thiên lệch, định dạng kém hoặc khuyết tật theo những cách khác. Việc tìm và sửa dữ liệu chất lượng thấp bằng thủ công là thách thức trong các tập dữ liệu lớn.

Trong khi hầu hết nghiên cứu machine learning lặp lại các chiến lược mô hình hóa (kiến trúc, hàm loss, thuật toán huấn luyện, ...) cho một tập dữ liệu cố định để tạo ra kết quả tốt hơn, khoa học AI tập trung vào dữ liệu mới nổi đặt câu hỏi về cách chúng ta có thể lặp lại có hệ thống trên tập dữ liệu trong khi giữ chiến lược mô hình hóa cố định để tạo ra kết quả tốt hơn (Mazumder et al., 2022). Thành công trong các dự án AI thực tế thường yêu cầu cả hai phương pháp. Vì nhiều thuật toán tinh chỉnh hiện có đã được đề xuất (Zhang et al., 2023), chúng tôi theo tinh thần của AI tập trung vào dữ liệu và đề xuất CLEAR, một pipeline tuyển chọn dữ liệu toàn diện và tự động để tăng cường hiệu quả của các tập dữ liệu điều chỉnh hướng dẫn cho bất kỳ LLM và thuật toán tinh chỉnh nào.

Pipeline CLEAR của chúng tôi bao gồm hai giai đoạn: Auto-Filter và Auto-Correct, cùng nhau cung cấp một giải pháp toàn diện để cải thiện chất lượng dữ liệu cho việc tinh chỉnh. Giai đoạn Auto-Filter loại bỏ dữ liệu có chất lượng thấp một cách tin cậy khỏi tập dữ liệu mà không cần bất kỳ tinh chỉnh LLM nào. Nó đã có thể cải thiện đáng kể tập dữ liệu, sao cho chúng ta có thể tạo ra các LLM được tinh chỉnh tốt hơn mà không cần bất kỳ tính toán tinh chỉnh LLM bổ sung nào. Đối với các thiết lập có thể tinh chỉnh LLM nhiều hơn một lần, giai đoạn Auto-Correct sử dụng LLM được tinh chỉnh hiện tại để sửa đổi một số ví dụ có thể được cải thiện một cách tin cậy. Tinh chỉnh LLM lại trên tập dữ liệu được chỉnh sửa này mang lại hiệu suất được cải thiện.

Các thay đổi thuật toán đối với tập dữ liệu nói chung có hại trừ khi được thực hiện với sự cẩn thận cực kỳ. Lọc quá nhiều dữ liệu hạn chế số lượng ví dụ để học, và chỉnh sửa dữ liệu có thể tạo ra nhiều thiên lệch khác nhau hoặc khuếch đại các lỗi trong đầu ra mô hình hiện có. Do đó, tất cả các thay đổi dữ liệu trong CLEAR được áp dụng một cách thận trọng dựa trên các biện pháp đo lường độ tin cậy cẩn thận. Cụ thể, chúng tôi dựa vào BSDetector (Chen và Mueller, 2023), một phương pháp có thể được sử dụng với bất kỳ LLM nào để có được điểm tin cậy đáng tin cậy cho đầu ra của chính nó cũng như ước tính độ tin cậy rằng các đầu ra đã cho (ví dụ: phản hồi mục tiêu) là tốt. CLEAR chỉ lọc dữ liệu được xác định một cách tin cậy là có chất lượng thấp, và chỉ sửa đổi dữ liệu khi phản hồi được LLM chỉnh sửa được xác định một cách tin cậy là tốt hơn so với phản hồi tập dữ liệu hiện tại. Các thí nghiệm của chúng tôi tiết lộ việc xử lý cẩn thận độ tin cậy này là quan trọng để phát triển một giải pháp lọc + chỉnh sửa dữ liệu phổ quát vẫn hiệu quả trên các tập dữ liệu điều chỉnh hướng dẫn đa dạng mà không cần bất kỳ thay đổi thủ công nào.

## 2 Nghiên cứu liên quan

### 2.1 Tuyển chọn Dữ liệu cho ML

Tuyển chọn dữ liệu đã là chìa khóa trong việc triển khai thực tế của học có giám sát cổ điển, với một loạt các phương pháp được phát triển để giải quyết việc gán nhãn sai tập dữ liệu, các giá trị ngoại lai và các vấn đề dữ liệu khác (Mazumder et al., 2022). Các chiến lược thuật toán như ước tính và loại bỏ nhiễu (Northcutt et al., 2021; Zhou et al., 2023b; Wang et al., 2022), học tích cực để ưu tiên hóa dữ liệu (Settles, 2009; Chen et al., 2021), và gán nhãn tập thể (Snow et al., 2008) đã chứng minh cách tạo ra các mô hình tốt hơn bằng cách tạo ra dữ liệu tốt hơn. Các chiến lược này được thiết kế cho các tác vụ machine learning cổ điển như phân loại/hồi quy, nơi các tập dữ liệu ít phức tạp hơn so với trong điều chỉnh hướng dẫn.

### 2.2 Tinh chỉnh Hướng dẫn

Nghiên cứu đáng kể đã được tiến hành về điều chỉnh hướng dẫn để chuyên biệt hóa/cải thiện LLM (Kumar et al., 2016; Raffel et al., 2020; Efrat và Levy, 2020; Li và Liang, 2021; Chen et al., 2022a; Wei et al., 2022; Wang et al., 2023a). FLAN (Wei et al., 2022) là một phương pháp phổ biến sử dụng một mô hình ngôn ngữ được tiền huấn luyện 137 tỷ tham số, được tinh chỉnh sử dụng hướng dẫn trên hơn 60 tập dữ liệu NLP được diễn đạt bằng các mẫu hướng dẫn ngôn ngữ tự nhiên. Wang et al. (2023a) đã cho thấy cách các tập dữ liệu điều chỉnh hướng dẫn khác nhau có thể tạo ra các kỹ năng cụ thể trong mô hình, mặc dù không có tập dữ liệu đơn lẻ nào (hoặc sự kết hợp của chúng) cung cấp hiệu suất tối ưu trên tất cả các đánh giá. Trái ngược với các nỗ lực trước đây nhằm tạo ra một mô hình Nền tảng tổng quát có khả năng khái quát hóa trên một loạt rộng các tác vụ chưa thấy, mục tiêu của chúng tôi trong bài viết này là huấn luyện LLM tốt nhất có thể cho một tác vụ hẹp cụ thể.

### 2.3 Tuyển chọn Dữ liệu cho Tinh chỉnh Hướng dẫn

Chất lượng của dữ liệu huấn luyện trong tạo văn bản có ý nghĩa quan trọng đến mức các tập dữ liệu điều chỉnh hướng dẫn trước đây thường được tuyển chọn bằng tay (Khashabi et al., 2020; Ye et al., 2021; Wei et al., 2022; Wang et al., 2023b; Chen et al., 2022b; Honovich et al., 2023). Wang et al. (2023b) đã giới thiệu các kỹ thuật tự động bằng cách sử dụng GPT-3 (Brown et al., 2020) để tạo ra 52.000 hướng dẫn độc đáo không liên kết trực tiếp với các tác vụ cụ thể. Sự đổi mới này đã mở ra những con đường mới để tạo ra các tập dữ liệu hướng dẫn bằng cách trích xuất kiến thức từ các mô hình giáo viên.

Sau khi Meta mở mã nguồn LLM được tiền huấn luyện LLaMa (Touvron et al., 2023), nhiều nhà nghiên cứu bắt đầu tuyển chọn các tập dữ liệu điều chỉnh hướng dẫn để huấn luyện các biến thể hữu ích của LLM này. Alpaca (Taori et al., 2023) giới thiệu phương pháp tự hướng dẫn để tự động tạo ra các ví dụ hướng dẫn (prompt), từ đó giảm sự phụ thuộc vào đầu vào thủ công. Vicuna (Chiang et al., 2023) tận dụng sự đa dạng rộng lớn của các loại dữ liệu và cấu trúc có thể truy cập thông qua ShareGPT. WizardLM (Xu et al., 2023) tăng cường tập dữ liệu bằng cách tinh chỉnh và đa dạng hóa các hướng dẫn để tăng độ phức tạp/biến thiên một cách tiến hóa. UltraChat (Ding et al., 2023) giới thiệu các phạm vi được định nghĩa tốt khác nhau, tạo ra một cách có hệ thống nhiều hướng dẫn trong mỗi phạm vi để cải thiện hiệu suất cụ thể của tác vụ. LIMA (Zhou et al., 2023a) chọn một nghìn mẫu dữ liệu chất lượng cao một cách chiến lược, cho thấy những cải thiện đáng chú ý trong hiệu suất LLM. Li et al. (2023a) đã đề xuất một chỉ số tuân thủ hướng dẫn để xác định các ví dụ tốt trong tập dữ liệu.

Nhiều nghiên cứu tinh chỉnh LLM hiện tại đã tập trung vào việc chưng cất các mô hình giáo viên như ChatGPT mạnh hơn LLM đang được tinh chỉnh (Taori et al., 2023; Chiang et al., 2023). Nhiều kỹ thuật tuyển chọn dữ liệu dựa trên LLM hiện có cũng sử dụng các LLM mạnh hơn cho quá trình tuyển chọn dữ liệu so với LLM đang được tinh chỉnh. Ngược lại, chúng tôi nhắm đến việc tạo ra các LLM tốt nhất cho các tác vụ cụ thể, trong đó ngay cả các LLM tiên tiến nhất như GPT-4 cũng gặp khó khăn trong việc thực hiện. Do đó, tất cả việc tuyển chọn dữ liệu trong bài viết này được thực hiện bằng cách sử dụng cùng một LLM với LLM đang được tinh chỉnh, để thực sự đánh giá việc tuyển chọn dữ liệu này có thể tăng cường hiệu suất LLM vượt qua biên giới như thế nào.

## 3 Tuyển chọn Dữ liệu Tự động với CLEAR

Một tập dữ liệu điều chỉnh hướng dẫn I={(xi, yi)^n_{i=1}} đi kèm với các hướng dẫn/prompt x và các phản hồi mục tiêu y tương ứng thu được từ một lĩnh vực cụ thể. Mục tiêu là tinh chỉnh LLM để cải thiện khả năng hiểu và thực thi hướng dẫn của nó, sao cho nó có thể tạo ra các phản hồi tương tự như các mục tiêu mong đợi cho các hướng dẫn mới gặp phải trong quá trình triển khai mô hình. Trên thực tế, các tập dữ liệu điều chỉnh hướng dẫn lớn có nhiễu, chứa các vấn đề như: phản hồi viết kém, phản hồi không chính xác, phản hồi không liên quan/không hữu ích, hướng dẫn mơ hồ/không đầy đủ, vấn đề định dạng dữ liệu, v.v. Các tập dữ liệu này thường được lấy từ các nhật ký trò chuyện lộn xộn hoặc được viết bởi các nhóm người làm sai khi vội vàng tạo ra dữ liệu quy mô lớn.

Vì các ánh xạ chuỗi-đến-chuỗi có tính chiều cao cực kỳ, việc học của mô hình có thể dễ dàng bị suy giảm bởi dữ liệu huấn luyện khuyết tật ẩn nấp trong một số vùng của không gian chiều cao này. Để phát triển một phương pháp có thể được sử dụng với bất kỳ mô hình LLM nào và bất kỳ quy trình tinh chỉnh nào, chúng tôi xem xét các thay đổi tập dữ liệu đơn giản thay vì các phương pháp tập trung vào mô hình thay đổi thuật toán huấn luyện để mạnh mẽ hơn. Các thay đổi tập dữ liệu của chúng tôi sẽ có lợi cho các LLM của thập kỷ tới, trong khi các thay đổi huấn luyện có xu hướng cụ thể theo mô hình.

Pipeline tuyển chọn dữ liệu được đề xuất của chúng tôi bao gồm hai bước chính: Auto-Filter và Auto-Correct, nhằm phát hiện các cặp (prompt, phản hồi) có vấn đề trong dữ liệu và chỉnh sửa chúng nếu có thể. Auto-Filter sử dụng bộ đánh giá chất lượng phản hồi dựa trên độ tin cậy (Chen và Mueller, 2023), để ước tính độ tin cậy của chúng tôi rằng mỗi cặp trong tập dữ liệu là tốt. Sau đó, LLM được tinh chỉnh chỉ trên dữ liệu có độ tin cậy cao. Bước lọc dữ liệu đơn giản này đã tăng cường việc tinh chỉnh LLM cho các tập dữ liệu nhiễu, và không yêu cầu chi phí tính toán tinh chỉnh bổ sung.

Lọc dữ liệu loại bỏ thông tin, một số trong đó có thể hữu ích. Chúng tôi đề xuất sử dụng LLM được tinh chỉnh kết quả để chỉnh sửa một số phản hồi xấu được xác định trong tập dữ liệu gốc, mà LLM được tinh chỉnh có thể tạo ra một câu trả lời thay thế có độ tin cậy cao. Điều này được xác định bằng cách so sánh phản hồi được tạo ra bởi LLM được tinh chỉnh với phản hồi gốc trong tập dữ liệu. Thay vì loại bỏ ví dụ như vậy khỏi tập dữ liệu trong giai đoạn lọc trước đó, chúng tôi bảo tồn prompt và thay thế phản hồi mục tiêu bằng phản hồi LLM được tinh chỉnh trong các trường hợp phản hồi sau này được ưa thích một cách tin cậy. Sau khi tự động chỉnh sửa tập dữ liệu theo cách này, LLM có thể được tinh chỉnh lại để tạo ra một phiên bản tốt hơn nữa của mô hình (mà không có bất kỳ thay đổi nào trong thuật toán tinh chỉnh). Chu kỳ tinh chỉnh LLM và tinh chỉnh dữ liệu này có thể được lặp lại trong một chu kỳ đạo đức (xem Hình 1).

### 3.1 Auto-Filter

Để ước tính chất lượng của các phản hồi trong tập dữ liệu gốc, CLEAR khác biệt với phương pháp thông thường là yêu cầu các LLM có khả năng như ChatGPT trực tiếp đánh giá cặp đầu vào-đầu ra theo các tiêu chí khác nhau (ví dụ: tính hữu ích như được hiển thị trong Bảng 5). Thay vào đó, chúng tôi sử dụng các ước tính độ tin cậy từ LLM, cụ thể là ước tính BSDetector được giới thiệu bởi Chen và Mueller (2023). Điều này ước tính độ tin cậy rằng một phản hồi là tốt dựa trên hai yếu tố: tính nhất quán quan sát được và sự chắc chắn của tự phản ánh.

BSDetector sử dụng cùng một LLM để tạo ra nhiều phản hồi ứng viên cho một prompt đã cho (thông qua các kỹ thuật tăng đa dạng như lấy mẫu nhiệt độ và chuỗi-của-tư duy), và sau đó đánh giá sự căn chỉnh ngữ nghĩa giữa các phản hồi ứng viên này và phản hồi mục tiêu trong tập dữ liệu (thông qua suy luận ngôn ngữ tự nhiên). Ngoài tính nhất quán quan sát này, BSDetector bổ sung tích hợp các đánh giá tự đánh giá LLM trực tiếp của phản hồi mục tiêu (trực tiếp nhắc LLM báo cáo độ tin cậy của nó rằng phản hồi là tốt). Các ước tính độ tin cậy kết quả tính đến cả sự không chắc chắn aleatoric và epistemic, mà không yêu cầu bất kỳ thay đổi/huấn luyện nào của LLM (thậm chí không cần truy cập vào các tham số LLM, cho phép phương pháp này được sử dụng với các API LLM tùy ý). Các thí nghiệm tiếp theo tiết lộ rằng phương pháp dựa trên độ tin cậy này để phát hiện dữ liệu chất lượng thấp chính xác hơn so với việc chấm điểm LLM thông thường về chất lượng phản hồi (xem Hình 2).

Cho một tập dữ liệu tinh chỉnh hướng dẫn gồm các cặp đầu vào-đầu ra {(xi, yi)^n_{i=1}}, chúng tôi sử dụng BSDetector với LLM được tiền huấn luyện cơ sở (trước khi được tinh chỉnh) để ước tính điểm tin cậy ci cho mỗi cặp (xi, yi). Sau đó chúng tôi lọc các cặp dữ liệu có điểm tin cậy thấp dưới ngưỡng được định trước γ:

F = {(xi, yi)|ci > γ}.

Sau đó, chúng tôi tinh chỉnh LLM trên dữ liệu huấn luyện còn lại F.

### 3.2 Auto-Correct

Cho đến nay, chúng tôi đã xem xét việc lọc dữ liệu được ước tính là có chất lượng thấp, nhưng điều gì sẽ xảy ra nếu một số dữ liệu này có thể được tự động chỉnh sửa? Một phương pháp trực tiếp sẽ là thay thế các phản hồi chất lượng thấp bằng các phản hồi được tạo ra bởi LLM. Đối với các lĩnh vực chuyên biệt, một LLM đa mục đích được tiền huấn luyện như GPT-4 có thể không thể tạo ra các phản hồi tốt hơn cho chúng ta xem xét. Nhưng LLM mà chúng tôi đã tinh chỉnh sau giai đoạn Auto-Filter được chuyên biệt hóa cho lĩnh vực của chúng tôi và sẽ có thể tạo ra một số phản hồi hợp lý. Nếu việc lọc tự động được thực hiện tốt, thì LLM được tinh chỉnh này sẽ thể hiện ít lỗi hơn khi được huấn luyện trên dữ liệu ít lỗi hơn.

Trong giai đoạn Auto-Correct, chúng tôi tiến hành tạo ra các phản hồi {(xi, y'i)^n_{i=1}} thông qua các truy vấn đến mô hình được tinh chỉnh này cho mỗi prompt xi trong tập dữ liệu của chúng tôi. Điều còn lại là quyết định khi nào phản hồi ứng viên y'i được tạo ra bởi LLM được tinh chỉnh hiện tại của chúng tôi tốt hơn một cách tin cậy so với phản hồi tập dữ liệu gốc yi. Đối với điều này, chúng tôi trực tiếp hỏi LLM Nền tảng cơ sở của chúng tôi (trước khi tinh chỉnh) thông qua prompt LLM-as-judge trong Bảng 1. Vì BSDetector tương thích với bất kỳ LLM nào, chúng tôi có thể có được các ước tính độ tin cậy cho các dự đoán ưu tiên LLM-as-judge này. Đối với các ví dụ mà độ tin cậy (như được ước tính bởi BSDetector) rằng y'i tốt hơn yi nằm trên một ngưỡng η: chúng tôi thay thế phản hồi mục tiêu của chúng bằng phản hồi được tạo ra bởi LLM và giữ lại cặp này trong tập dữ liệu được tuyển chọn của chúng tôi (thay vì lọc nó). Tập dữ liệu được tự động chỉnh sửa này sau đó được sử dụng để tinh chỉnh LLM thêm, để tạo ra một mô hình được cải thiện thêm nữa.

## 4 Thí nghiệm

**Tập dữ liệu.** Chúng tôi đánh giá hiệu quả của quy trình tuyển chọn dữ liệu của chúng tôi trên các phiên bản nhiễu của ba tập dữ liệu tinh chỉnh có giám sát (tạo văn bản) (xem Hình 3,4,5). **SQuAD-N** (Rajpurkar et al., 2016): các prompt là các bài báo và các phản hồi mục tiêu là các câu trả lời cho các câu hỏi được tạo ra bởi các crowdworker dựa trên một tập hợp các bài báo Wikipedia, với mỗi câu trả lời là một đoạn văn bản hoặc khoảng cụ thể từ bài báo liên quan. **Emails-N**: các prompt là email và các phản hồi mục tiêu bao gồm việc phân loại email thành một trong bảy chủ đề được định nghĩa trước bằng cách kiểm tra chủ đề và nội dung của email và cũng thay đổi dựa trên độ dài của email (việc nội dung email ngắn, trung bình hay dài ảnh hưởng đến cách viết phản hồi). **DROP-N** (Dua et al., 2019): các prompt là các bài báo và các phản hồi mục tiêu là các câu trả lời cho các câu hỏi hiểu đọc yêu cầu lý luận rời rạc trên các đoạn văn (trả lời chính xác yêu cầu giải quyết các tham chiếu trong một câu hỏi, có thể đến nhiều nơi trong bài báo, và thực hiện các thao tác cơ bản trên các tham chiếu như cộng, đếm hoặc sắp xếp).

Để nghiên cứu cách phương pháp của chúng tôi xử lý dữ liệu nhiễu, chúng tôi đã làm nhiễu 20% của mỗi tập dữ liệu huấn luyện (không phải tập kiểm tra tương ứng). Đối với tập dữ liệu Emails, sự nhiễu là hoán đổi ngẫu nhiên các phản hồi mục tiêu qua các ví dụ khác nhau. Để làm nhiễu một tập con của các tập dữ liệu SQuAD và DROP, nơi các phản hồi mục tiêu được chứa trong một đoạn văn bối cảnh trong hướng dẫn được cung cấp, chúng tôi chọn một câu ngẫu nhiên từ bối cảnh làm phản hồi mục tiêu.

**Chỉ số đánh giá.** Đối với mỗi tập dữ liệu, việc đánh giá hiệu suất tinh chỉnh LLM của chúng tôi tập trung vào hai chỉ số (được tính toán trên một tập kiểm tra được giữ lại cố định): tần suất phản hồi của mô hình tuân thủ cấu trúc JSON hợp lệ và tần suất phản hồi của mô hình chính xác. Đối với mỗi mô hình được tạo ra thông qua một phương pháp tinh chỉnh, chúng tôi báo cáo tỷ lệ phản hồi mô hình ở định dạng JSON hợp lệ, và độ chính xác của phản hồi mô hình (được tính toán thông qua tỷ lệ khớp chính xác với các phản hồi tham chiếu mục tiêu, vì chúng tôi mong đợi một mô hình được giám sát tốt có thể khớp với các loại phản hồi mục tiêu mà nó được tinh chỉnh).

**Phương pháp cơ sở.** Nghiên cứu của chúng tôi cũng đánh giá các phương pháp không tinh chỉnh sau: **Zero-shot** trên GPT-3.5-turbo/GPT-4.0/Llama-2-7b-chat là truy vấn trực tiếp các mô hình Nền tảng được tiền huấn luyện này. **Few-shot** trên GPT-3.5-turbo/GPT-4.0/Llama-2-7b-chat là truy vấn trực tiếp các mô hình Nền tảng được tiền huấn luyện này sử dụng học trong bối cảnh (với số lượng ví dụ được chỉ ra từ tập dữ liệu được chèn vào mỗi prompt làm bối cảnh few-shot). Đối với các phương pháp tinh chỉnh, chúng tôi sử dụng tinh chỉnh mô hình đầy đủ trên Llama-2-7b-chat và API tinh chỉnh GPT-3.5 Turbo của OpenAI. Tinh chỉnh trên dữ liệu nhiễu đề cập đến việc tinh chỉnh mô hình trên các tập dữ liệu gốc mà không có bất kỳ tuyển chọn dữ liệu nào. **Auto-Filter** đề cập đến việc tinh chỉnh mô hình trên các phiên bản được tuyển chọn của tập dữ liệu, nơi dữ liệu có mức độ tin cậy thấp đã được loại bỏ như được mô tả trong Mục 3.1. Quy trình này đặt giá trị tin cậy trung vị trên tập dữ liệu làm ngưỡng γ, lọc bất kỳ dữ liệu nào dưới ngưỡng này. **Auto-Correct** đề cập đến việc tinh chỉnh mô hình trên các phiên bản được tuyển chọn của tập dữ liệu, nơi dữ liệu nhất định có các phản hồi được chỉnh sửa được tạo ra như được mô tả trong Mục 3.2 (chúng tôi đặt η = 0.8). Quy trình tinh chỉnh vẫn giữ nguyên khi đánh giá các chiến lược tuyển chọn dữ liệu khác nhau – chúng tôi chỉ thay đổi tập dữ liệu huấn luyện, không phải mô hình/thuật toán.

**Chi tiết khác.** Chúng tôi nghiên cứu hiệu quả của các chiến lược tuyển chọn dữ liệu trên hai phương pháp tinh chỉnh khác nhau. Trên mô hình Llama-2-7b-chat, chúng tôi tiến hành tinh chỉnh mô hình đầy đủ, trong đó tất cả các tham số của mạng neural được cập nhật thông qua bộ tối ưu hóa Adam. Chúng tôi đặt kích thước batch là 128, và huấn luyện trong 3 epoch, sử dụng tốc độ học 1×10^-5 với lịch trình tốc độ học cosine đi kèm. Đối với mô hình GPT-3.5 Turbo, chúng tôi sử dụng API tinh chỉnh của OpenAI. Thuật toán huấn luyện/siêu tham số chính xác được sử dụng vẫn không được tiết lộ cho chúng tôi, nhưng API này đã được quan sát là có hiệu quả cao cho việc tinh chỉnh LLM. Khi đánh giá đầu ra từ các mô hình của chúng tôi tại thời điểm kiểm tra, chúng tôi thực hiện tất cả việc tạo văn bản với nhiệt độ 0, và giới hạn số lượng token đầu ra tối đa là 512.

## 5 Kết quả

Bảng 2 trình bày kết quả của các thí nghiệm chính của chúng tôi. Trong số các phương pháp không tinh chỉnh, GPT-4 nổi bật như LLM vượt trội, thể hiện hiệu suất mạnh nhất trên ba tập dữ liệu. Đối với mô hình GPT-4 được tiền huấn luyện, học few-shot vượt trội hơn học zero-shot. Nhưng đối với mô hình Llama-7B-chat được tiền huấn luyện, học few-shot tạo ra kết quả tệ hơn nhiều so với học zero-shot, do độ nhạy cảm cao hơn của mô hình nhỏ hơn đối với việc lựa chọn các minh chứng few-shot (Chen et al., 2023; Wang et al., 2024).

Đối với các mô hình được tinh chỉnh, chúng tôi quan sát thấy rằng huấn luyện trên toàn bộ tập dữ liệu nhiễu mà không tuyển chọn thậm chí có thể làm giảm hiệu suất mô hình. Tinh chỉnh chỉ với một nửa dữ liệu, được tinh chỉnh thông qua lọc tự động, mang lại kết quả tốt hơn so với sử dụng tập dữ liệu hoàn chỉnh, không được tuyển chọn. Hơn nữa, dữ liệu huấn luyện được tuyển chọn thông qua chiến lược Auto-Correct của chúng tôi tăng cường thêm hiệu suất mô hình. Hình 3,4,5 mô tả cho mỗi tập dữ liệu: một phản hồi sai được tự động xác định trong giai đoạn Auto-Filter sau đó được chỉnh sửa trong giai đoạn Auto-Correct.

Các mô hình được tinh chỉnh của chúng tôi có thể vượt trội hơn ngay cả mô hình tiên tiến nhất, GPT-4 với prompting ba lần. Điều này nhấn mạnh cách ngay cả các LLM mạnh nhất cũng có thể thiếu khả năng giải quyết một cách thỏa đáng các thách thức lĩnh vực cụ thể. Khác với một số nghiên cứu tinh chỉnh khác, GPT-4 không tham gia vào bất kỳ phần nào của quá trình tuyển chọn dữ liệu hoặc huấn luyện làm nền tảng cho các LLM được tinh chỉnh của chúng tôi ở đây.

### 5.1 Ước tính Chất lượng Phản hồi trong Auto-Filter

Chúng tôi so sánh việc sử dụng bộ đánh giá chất lượng phản hồi dựa trên độ tin cậy trong quy trình Auto-Filter của chúng tôi với bộ đánh giá dựa trên chấm điểm LLM trực tiếp. Phương pháp sau trực tiếp nhắc LLM (ví dụ GPT-3.5-turbo) chấm điểm một cặp đầu vào-đầu ra đã cho (Li et al., 2023b) sử dụng thang đánh giá Likert từ 1 đến 5. Bảng 5 mô tả prompt được sử dụng để đánh giá chất lượng dựa trên điểm số này. Sau khi chấm điểm chất lượng của mỗi cặp (hướng dẫn, phản hồi) trong tập dữ liệu, chúng tôi loại bỏ 50% có điểm số thấp nhất. Sau đó, chúng tôi tinh chỉnh mô hình trên dữ liệu còn lại.

Bảng 3 trình bày kết quả so sánh phương pháp dựa trên điểm số này với phương pháp dựa trên độ tin cậy của chúng tôi từ Mục 3.1. Chúng tôi cũng xem xét kết quả dựa trên việc tinh chỉnh LLM trên 50% được chọn ngẫu nhiên của dữ liệu. Trên tất cả các tập dữ liệu, bộ đánh giá dựa trên độ tin cậy của chúng tôi hoặc khớp hoặc vượt trội hơn hiệu suất của bộ đánh giá dựa trên điểm số và lựa chọn dữ liệu ngẫu nhiên, đạt được hiệu suất tốt hơn đáng kể trong tập dữ liệu Email-N.

### 5.2 Sử dụng LLM trong Auto-Correct

Ở đây chúng tôi xem xét một biến thể của giai đoạn Auto-Correct của chúng tôi, nơi chúng tôi tạo ra các phản hồi ứng viên thay thế từ mô hình Nền tảng được tiền huấn luyện cơ sở, thay vì từ phiên bản được tinh chỉnh tiếp theo của LLM này. Cụ thể, chúng tôi xem xét GPT-3.5 Turbo để tạo ra các phản hồi ứng viên y' sau đó được đưa vào cùng quy trình Auto-Correct được mô tả trong Mục 3.2. Bảng 4 tiết lộ rằng việc sử dụng phiên bản được tinh chỉnh của LLM này để tạo ra các phản hồi ứng viên hoạt động tốt hơn trên tất cả các tập dữ liệu.

## 6 Kết luận

Bài viết này trình bày một pipeline tổng quát để tuyển chọn các phiên bản tốt hơn của một tập dữ liệu tinh chỉnh hướng dẫn hiện có. Phương pháp CLEAR tập trung vào dữ liệu của chúng tôi có thể được kết hợp với bất kỳ mô hình và thuật toán tinh chỉnh nào. Trong khi các mô hình và thuật toán tinh chỉnh tốt hơn chắc chắn sẽ được phát minh, các phương pháp tập trung vào dữ liệu như của chúng tôi có thể vẫn hữu ích. Khi các LLM trong tương lai tiến bộ, khả năng tuyển chọn dữ liệu của chúng thông qua CLEAR sẽ tiến bộ, tạo điều kiện cho các LLM tốt hơn nữa được huấn luyện trên dữ liệu được tuyển chọn tốt hơn này.

Các thí nghiệm chứng minh rằng quy trình tuyển chọn dữ liệu của chúng tôi tạo ra những cải thiện đáng kể trong hiệu suất của các LLM được tinh chỉnh trên các tập dữ liệu nhiễu, mô hình và thuật toán huấn luyện khác nhau (mà không được điều chỉnh cho từng thiết lập). Trong khi phương pháp của chúng tôi sửa các vấn đề trong tập dữ liệu hiện có, việc tăng cường dữ liệu này với các ví dụ tổng hợp bổ sung là một phương pháp tập trung vào dữ liệu khác có vẻ hứa hẹn để kết hợp với CLEAR.

## Hạn chế

Trong khi pipeline tuyển chọn dữ liệu tự động của chúng tôi trình bày một tiến bộ đáng kể trong việc tăng cường chất lượng của các tập dữ liệu điều chỉnh hướng dẫn cho các mô hình ngôn ngữ lớn (LLM), điều quan trọng là phải thừa nhận các hạn chế của nó. Khung hiện tại của pipeline không tính đến một cách rõ ràng khả năng thiên lệch trong tập dữ liệu gốc hoặc những thiên lệch được giới thiệu trong quá trình tuyển chọn tự động. Vì hiệu suất của mô hình và chất lượng đầu ra của nó phụ thuộc vào dữ liệu mà nó được huấn luyện, bất kỳ thiên lệch nào vốn có đều có thể được duy trì hoặc khuếch đại thông qua các lần lặp tiếp theo của tinh chỉnh và chỉnh sửa.
