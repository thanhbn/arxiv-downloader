# 2303.07811.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/interpretability/2303.07811.pdf
# File size: 2047573 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
ICICLE: Interpretable Class Incremental Continual Learning
Dawid Rymarczyk1,2,3,‚àóJoost van de Weijer4,5Bartosz Zieli ¬¥nski1,3,6
Bart≈Çomiej Twardowski4,5,6
1Faculty of Mathematics and Computer Science, Jagiellonian University
2Doctoral School of Exact and Life Sciences, Jagiellonian University3Ardigen SA
4Autonomous University of Barcelona5Computer Vision Center6IDEAS NCBR
‚àódawid.rymarczyk@doctoral.uj.edu.pl
Abstract
Continual learning enables incremental learning of new
tasks without forgetting those previously learned, result-
ing in positive knowledge transfer that can enhance per-
formance on both new and old tasks. However, contin-
ual learning poses new challenges for interpretability, as
the rationale behind model predictions may change over
time, leading to interpretability concept drift. We address
this problem by proposing Interpretable Class-InCremental
LEarning (ICICLE), an exemplar-free approach that adopts
a prototypical part-based approach. It consists of three
crucial novelties: interpretability regularization that distills
previously learned concepts while preserving user-friendly
positive reasoning; proximity-based prototype initialization
strategy dedicated to the fine-grained setting; and task-
recency bias compensation devoted to prototypical parts.
Our experimental results demonstrate that ICICLE reduces
the interpretability concept drift and outperforms the ex-
isting exemplar-free methods of common class-incremental
learning when applied to concept-based models.
1. Introduction
With the growing use of deep learning models in diverse
domains, including robotics [10], medical imaging [17],
and autonomous driving [43], there is a pressing need to
develop models that can adapt to ever-changing conditions
and learn new tasks from non-stationary data. However,
a significant challenge with neural networks is their ten-
dency to suffer from catastrophic forgetting [26, 30, 44],
where performance on previous tasks deteriorates rapidly
as new ones are acquired. Continual Learning (CL) [19] has
emerged as a promising technique to address this challenge
by enabling models to learn new tasks without forgetting
those learned before.
While existing CL approaches significantly reduce catas-
trophic forgetting, they are often difficult for humans to
understand. It is especially problematic because deep net-
Task 4 Task 3 Task 2 
Fine-tuning LWF ICICLE EWC LWM Input image Task 1 
Figure 1: We process the input image (top left) through the
network and visualize how its specific areas are similar to
one of the prototypes. The interpretability concept drift oc-
curs when such a similarity map differs between tasks. ICI-
CLE performs best, preserving similarity maps better than
the other continual learning methods.
works often predict the right answer for the wrong reason
(the ‚ÄúClever Hans‚Äù phenomenon), leading to excellent per-
formance in training but poor performance in practice [63].
This results in serious societal problems that deeply affect
health, freedom, racial bias, and safety [11]. As a result,
some initial steps were taken in the literature to introduce
explainable posthoc methods into the CL setup [32, 52, 64].
However, explaining black boxes, rather than replacing
them with interpretable (self-explainable) models, can es-
calate the problem by providing misleading or false char-
acterizations [70] or adding unnecessary authority to the
black box [12]. Therefore, there is a clear need for inno-
vative machine-learning models that are inherently inter-
pretable [11]. To the best of our knowledge, no interpretable
CL approach has been proposed so far.
In this work, we introduce Interpretable Class-arXiv:2303.07811v2  [cs.LG]  31 Jul 2023

--- PAGE 2 ---
Incremental Learning (ICICLE), an interpretable approach
to class-incremental learning based on prototypical parts
methodology. Similarly to This looks like that reason-
ing [16], ICICLE learns a set of prototypical parts repre-
senting reference concepts derived from the training data
and makes predictions by comparing the input image parts
to the learned prototypes. However, the knowledge transfer
between tasks in continual learning poses new challenges
for interpretability. Mainly because the rationale behind
model predictions may change over time, leading to inter-
pretability concept drift and making explanations inconsis-
tent (see Figure 1 and Table 1). Therefore, ICICLE contains
multiple mechanisms to prevent this drift and, at the same
time, obtain satisfactory results. First, we propose an inter-
pretability regularization suited for prototypical part-based
models to retain previously gained knowledge while main-
taining model plasticity. It ensures that previously learned
prototypical parts are similarly activated within the current
task data, which makes explanations consistent over time.
Moreover, considering the fine-grained nature of considered
datasets, we introduce proximity-based prototype initializa-
tion for a new task. It searches for representative concepts
within the new task data close to previously learned con-
cepts, allowing the model to recognize high-level features
of the new task and focusing on tuning details. Thirdly,
to overcome task-recency bias in class-incremental learn-
ing scenarios, we propose a simple yet effective method
that balances the logits of all tasks based on the last task
data. Finally, we reduce multi-stage training while preserv-
ing user-friendly positive reasoning.
We evaluate ICICLE on two datasets, namely CUB-200-
2011 [84] and Stanford Cars [46], and conduct exhaustive
ablations to demonstrate the effectiveness of our approach.
We show that this problem is challenging but opens up a
promising new area of research that can further advance our
understanding of CL methods. Our contributions can be
summarized as follows:
‚Ä¢ We are the first to introduce interpretable class-
incremental learning and propose a new method ICI-
CLE, based on prototypical part methodology.
‚Ä¢ We propose interpretability regularization that pre-
vents interpretability concept drift without using ex-
emplars.
‚Ä¢ We define a dedicated prototype initialization strategy
and a method compensating for task-recency bias.
2. Related Works
Continual Learning and Class Incremental Learning
Existing continual learning methods can be broadly cate-
gorized into three types: replay-based, architecture-based,
and regularization-based methods [19, 54]. Replay-based
methods either save a small amount of data from previ-
ously seen tasks [5, 15] or generate synthetic data with a
generative model [86, 93]. The replay data can be usedIOU
METHOD TASK 1 TASK 2 TASK 3 MEAN
FINETUNING 0.115 0.149 0.260 0.151
EWC 0.192 0.481 0.467 0.334
LWF 0.221 0.193 0.077 0.188
LWM 0.332 0.312 0.322 0.325
ICICLE 0.705 0.753 0.742 0.728
Table 1: Quantitative results for interpretability concept
drift presented in Figure 1. We compute IoU between simi-
larities obtained after each task and after incremental tasks.
E.g. in column ‚Äútask 1‚Äù, we calculate IoU between similar-
ity maps of task one prototypes after each learning episode.
during training together with the current data, such as in
iCaRL [68] and LUCIR [37], or to constrain the gradient di-
rection while training, such as in AGEM [14]. Architecture-
based methods activate different subsets of network param-
eters for different tasks by allowing model parameters to
grow linearly with the number of tasks. Previous works
following this strategy include DER [89], Piggyback [50],
PackNet [51]. Regularization-based methods add an addi-
tional regularization term derived from knowledge of previ-
ous tasks to the training loss. This can be done by either reg-
ularizing the weight space, which constrains important pa-
rameters [79, 82], or the functional space, which constrains
predictions or intermediate features [23, 38]. EWC [44],
MAS [3], REWC [49], SI [92], and RWalk [13] constrain
the importance of network parameters to prevent forgetting.
Methods such as LWF [48], LWM [21], and BiC [88] lever-
age knowledge distillation to regularize features or predic-
tions. Additionally, more challenging setups are considered
in the field such as open-set interpretable continual learn-
ing [57]. When it comes to interpretable CL, the generative
replay approaches [58] provide a certain degree of latent
clarity. However, they require a decoder (for visualization)
and may fail to produce realistic prototype images [16].
Class-incremental learning (class-IL) is the most challeng-
ing scenario where the classifier learns new classes sequen-
tialy. The model needs to maintain good performance on all
classes seen so far [83]. Two types of evaluation methods
are defined [54]: task-agnostic (no access to task-ID during
inference, e.g., BiC [88]) and task-aware (task-ID is given
during inference, e.g., HAT [78]).
Explainable Artificial Intelligence In the field of deep
learning explanations, two types of models have been ex-
plored: post hoc and self-explainable models [70]. Post hoc
models explain the reasoning process of black-box meth-
ods, including saliency maps [53, 67, 76, 77, 80], con-
cept activation vectors [18, 29, 40, 45, 90], counterfac-
tual examples [1, 31, 56, 62, 87], and image perturbation
analysis [7, 24, 25, 69]. Self-explainable models, on the
other hand, aim to make the decision process more trans-
parent and have attracted significant attention [4, 9]. Re-
cently, researchers have focused on enhancing the concept

--- PAGE 3 ---
convolutional layers 
 (backbone ùëì, add-on layers ùëìA, and sigmoid) prototype parts layers ùò®t -1(           ,                ) = 
gt-1(           ,                ) = gt-1(           ,                ) = 1.0
1.0
1.04.25 
5.72 
0.22 0.71 Zx
ùëì ùë•
gt(           ,                ) = 
gt(           ,                ) = 
gt(           ,                ) = 0.12 
0.78 1.01.0
1.0
1.00.39 
last layers class labels Wilson 
Warbler Prothonotary 
Warbler 
1.00.61 gt-1(           ,                ) = 1.00 Cardinal 1.47 
TASK t 
0.49 gt(           ,                ) = 
0.90 Sooty 
Albatross ùëìA
TASK t-1 prototypes p
œÉ ht-1 
ht Figure 2: The architecture of our ICICLE with separate prototypical part layers gtfor each task t. In this example, prototypes
of classes Prothonotary Warbler andCardinal belong to task t‚àí1, while prototypes of Wilson Warbler andSooty Albatross
to task t. Layers gtare preceded by shared backbone f, add-on fA, and sigmoid. Moreover, they are followed by the last
layers htwith weight ht
ci= 1if prototype piis assigned to class cand equals 0otherwise.
of prototypical parts introduced in ProtoPNet [16] to repre-
sent the activation patterns of networks. Several extensions
have been proposed, including TesNet [85] and Deformable
ProtoPNet [22], which exploit orthogonality in prototype
construction. ProtoPShare [74], ProtoTree [59], and Pro-
toPool [73] reduce the number of prototypes used in classi-
fication. Other methods consider hierarchical classification
with prototypes [33], prototypical part transformation [47],
and knowledge distillation techniques from prototypes [39].
Prototype-based solutions have been widely adopted in var-
ious applications such as medical imaging [2, 6, 41, 72, 81],
time-series analysis [28], graph classification [71, 94], se-
quence learning [55], and semantic segmentation [75]. In
this work, we adapt the prototype mechanism to class in-
cremental learning.
3. Methods
The aim of our approach is to increase the interpretabil-
ity in the class-incremental scenario. For this purpose, we
adapt prototypical parts [16], which directly participate in
the model computation, making explanations faithful to the
classification decision. To make this work self-contained,
we first recall the prototypical parts methodology, and then
we describe how we adapt them to the class-incremental
scenario. We aim to compensate for interpretability con-
cept drift, which we define at the end of this section. Aswe aim to compensate for interpretability concept drift, we
define it at the end of this section.
3.1. Prototypical parts methodology
Architecture. The original implementation of prototyp-
ical parts [16] introduces an additional prototypical part
layer gproceeded by a backbone convolutional network f
with an add-on fAand followed by the fully connected layer
h. The fAadd-on consists of two 1√ó1convolutional layers
and a sigmoid activation at the end, translating the convolu-
tional output to a prototypical part space. The prototypical
part layer gconsists of Kprototypes pi‚ààRDper class, and
their assignment is handled by the fully connected layer h.
If prototype piis assigned to class c, then hci= 1, other-
wise, it is set to ‚àí0.5.
Inference. Given an input image x, the backbone fgen-
erates its representation f(x)of shape H√óW√óD, where
HandWare the height and width of the representation
obtained at the last convolutional layer, and Dis the num-
ber of channels. This representation is translated by fA
to a prototypical part space, again of size H√óW√óD.
Then, each prototypical part piis compared to each of
H√óWrepresentation vectors to calculate the maximum
similarity (i.e. the maximal activation of this prototype
on the analyzed image) max j‚àà{1..HW}sim(pi, zj), where

--- PAGE 4 ---
prototype pt-1 
f  after 
task t-1
prototype 
similarity map 
after task tprototype 
similarity map 
after task t-1mask of the 
highest similarityminimize 
MSE 
low highsimilarityf  after 
task tFigure 3: Our interpretability regularization aims to min-
imize the changes in the prototype similarities. It takes a
prototype pt‚àí1of previous tasks and an image from task
t, selects the image area with the highest similarity to this
prototype (binary mask S), and punishes the model for any
changes in this area caused by training task t.
sim(pi, zj) = log|zj‚àípi|2+1
|zj‚àípi|2+Œ∑andŒ∑‚â™1. To obtain the fi-
nal predictions, we push those values through the fully con-
nected (and appropriately initialized) layer h.
Training. Training is divided into three optimization
phases: warm-up, joint learning, and convex optimization
of the last layer. The first phase trains add-on fAand the
prototypical part layer g. The second phase learns fA,g,
and the backbone network f. The last phase fine-tunes
the fully-connected layer h. Training is conducted with the
cross-entropy loss supported by two regularizations, cluster
and separation costs [16]. Cluster encourages each training
image to have a latent patch close to at least one prototype
of its class. In contrast, the separation cost encourages ev-
ery latent patch of a training image to stay away from the
prototypes of the remaining classes.
3.2. ICICLE
Significant modifications of architecture and train-
ing are required to employ prototypical parts methodol-
ogy to class-incremental learning (the inference is iden-
tical). Mostly because incremental learning has con-
siderably different conjectures. It assumes Ttasks
(C1, X1),(C2, X2), . . . , (CT, XT), where each task tcon-
tains classes Ctand training set Xt. Moreover, during task
t, only Xttraining data are available, as we consider the
exemplar-free setup, where it is prohibited to save any data
from previous tasks (no replay buffer is allowed).
Architecture. As in the baseline model, ICICLE com-
prises backbone fand add-on fA. However, it does not use
one fixed prototypical part layer gand one fully-connected
layer h. Instead, it introduces a prototypical part layer gt
and a fully-connected layer htfor each successive task.
Layers gtconsist of Mtprototypical parts, where Mt=
K¬∑CtandKis the number of prototypes per class. On
the other hand, layer hthas weight ht
ci= 1 if prototype pi
is assigned to class cand it is set to 0otherwise. We elim-
inated negative weights ( ‚àí0.5) from the last layer because
Latent space 
after task t-1 
prototypes from task t-1 
initial locations for 
task t prototypes 
representations of task t data Random initialization 
Proximity initialization 
clusters of representations Figure 4: We introduce a new proximity-based prototype
initialization. It starts by passing training samples of task t
through the network (green dots) and choosing representa-
tions closest to existing prototypes (violet diamonds). This
results in many points, which we cluster (purple circles) to
obtain the initial locations of task tprototypical parts (yel-
low diamonds). Such initialization (bottom right) is pre-
ferred over random initialization (top right), where new pro-
totypes can be created far from the old ones, even though
they are only slightly different.
multi-stage training is not beneficial for a class-incremental
scenario (see Figure 6).
Training. To prevent catastrophic forgetting, ICICLE
modifies the loss function of the baseline solution. Ad-
ditionally, it introduces three mechanisms: interpretabil-
ity regularization, proximity-based prototype initialization,
and task-recency bias compensation. Regarding the base-
line loss function, the cross-entropy is calculated on the full
output of the model, including logits from classes learned
in previous tasks. However, the cluster and separation costs
are only calculated within the gthead.
Interpretability regularization. Knowledge distillation [35]
is one of the strong regularization methods applied to pre-
vent forgetting [48]. However, the results obtained by its
straightforward application are not satisfactory and lead to
significant interpretation drift (see Figure 1 and Section 5).
Therefore, we introduce an additional regularization cost
LIR(see Figure 3), inspired by [39], that minimizes the
changes in the similarities for the prototypical parts of the
previous tasks. It is defined as:
LIR=HX
i=0WX
j=0|sim(pt‚àí1, zt
i,j)‚àísim(pt, zt
i,j)|¬∑Si,j(1)
where sim(pt‚àí1, zt
i,j)is computed for the model stored be-
fore training task t, and Sis a binary mask of size H√óW,
indicating the representation pixels with the highest similar-
ity (Œ≥quantile of those pixels). Such similarity distillation

--- PAGE 5 ---
gives higher plasticity when learning a new task but, at the
same time, reduces the interpretability drift.
Proximity-based prototype initialization. Random initial-
ization of prototypes, proposed in [16], fails in the incre-
mental learning (see Table 5). Most probably because new
prototypes can be created far from the old ones, which
are only slightly different (e.g. wing prototypes of vari-
ous bird species). Therefore, we introduce initialization
that sets new prototypes close to the old ones (see Fig-
ure 4). We start by passing training samples of task t
through the network and determining which patches are
the most similar to existing prototypes (we choose patches
from the Œ±quantile). More specifically, we compute set
{zt
j: max isim(pt‚àí1
i, zt
j)‚ààŒ±quantile }. This results in
many candidates for new task prototypical parts. To obtain
K¬∑Ctprototypes, we perform KMeans++ clustering, and
the resulting centers of clusters are used to initialize the pro-
totypical parts in gt.
Task-recency bias compensation. When the model learns
taskt, the similarities to the prototypes of previous tasks
drop, mostly due to the changes in the backbone (see Fig-
ure 5). That is why, after training the final task, we com-
pensate for this using T‚àí1constants obtained using the
last tasks data. More precisely, for each of the previous
tasks t < T , we take logits yt=ht‚ó¶gt‚ó¶f(x)obtained
for all x‚ààXTand calculate bias ctso that |{x‚ààXT:
max ( yt+ct)>maxyT}|=u|XT|. Intuitively, we ad-
justctso the model changes u%of its prediction from task
T to task t. We determined experimentally that u= 10% is
optimal.
3.3. Interpretability Concept Drift
As noted in the caption of Figure 1, the interpretability
concept drift occurs when a similarity map differs between
tasks. Therefore, it can be formally defined as:
ICD =EH,W
i,j=1sim(pt‚àí1, zt
i,j)‚àísim(pt, zt
i,j),(2)
where (zi,j)H,W
i,j=1corresponds to input image representa-
tion,pt‚àí1andptcorrespond to prototypical part pbefore
and after task t, andsim is similarity defined in Section 3.1
of the paper. Thus, the greater ICD , the greater the inter-
pretability concept drift.
4. Experimental Setup
We evaluate our approach on the CUB-200-2011 [84]
and Stanford Cars [46] datasets to classify 200 bird species
and 196 car models, respectively. We consider 4, 10, and
20 task learning scenarios for birds and 4, 7, and 14 options
for cars. As the backbone f, we take ResNet-34 [34] with-
out the last layer and pre-trained on ImageNet [20]. We set
the number of prototypes per class to 10. Moreover, we use
prototypical parts of size 1√ó1√ó256and1√ó1√ó128for birds
and cars, respectively. The weights of CE, cluster, separa-
tion, and distillation costs in the loss function equal 1.0,0.8,
Class index After compensation 
 Before compensation 
Class index Avg class activation Figure 5: When the model learns task t, the similarities to
the prototypes of previous tasks drop and are significantly
lower than those of new tasks (upper plot). That is why,
after training the final task, we compensate it with T‚àí1
calculated constants. As a result, the similarities obtained
by prototypes of all tasks are roughly equalized.
‚àí0.08, and 0.01. In distillation, we take Œª= 1/49repre-
sentation pixels with the highest similarity. For proximity-
based initialization, we use Œ±= 0.5. For task-recency bias
compensation, we take ct, which changes the predictions of
the last validation set by less than 10%. As the implemen-
tation framework, we use FACIL [54] based on the PyTorch
library1. Details on the experimental setup are provided in
the Supplementary Materials2.
5. Results
Performance. We evaluated the effectiveness of ICI-
CLE by comparing it with commonly used exemplar-free
baseline methods in class-incremental learning, including
LWF [48], LWM [21], and EWC[44]3. Additionally, Fine-
tuning, and Freezing of the feature extractor (not trained at
all) are provided. We also report multitask learning as an
upper-bound where the various tasks are learned jointly in
a multitask manner. To do so, we analyzed task-aware and
task-agnostic accuracy for each task after the last one (Ta-
ble 3) and the aggregated incremental average accuracies
after learning the last task in scenarios involving 4, 10, and
20 tasks for CUB (Table 2) and 4, 7, and 14 tasks for Stan-
ford Cars (Supplementary Materials). All methods use the
same feature extractor network architectures and ProtoPNet
for prototypical part-based learning. Our method outper-
formed the baseline methods in all cases, indicating its su-
perior performance for prototypical part-based learning in a
continual manner. ICICLE retains knowledge from previ-
ous tasks better, which results in a more balanced accuracy
between tasks and higher accuracy for the first task com-
pared to all other approaches. However, despite the signifi-
cant improvement, our approach still has room for improve-
ment compared to the upper-bound of multi-task training.
With a larger number of tasks, the forgetting of the model is
1https://pytorch.org
2Code available at: https://github.com/gmum/ICICLE
3Extending interpretable models with more complicated exemplar-free
methods is not straightforward, and we, therefore, excluded methods such
as SDC [91] (which requires learning with a metric loss) and PASS [95]
(which requires a combination with self-supervised learning).

--- PAGE 6 ---
AVG.INC.TASK -AWARE ACCURACY AVG.INC.TASK -AGNOSTIC ACCURACY
METHOD 4TASKS 10TASKS 20TASKS 4TASKS 10TASKS 20TASKS
FREEZING 0.560¬±0.027 0.531¬±0.042 0.452¬±0.055 0.309¬±0.024 0.115¬±0.028 0.078¬±0.004
FINETUNING 0.229¬±0.005 0.129¬±0.017 0.147¬±0.021 0.177¬±0.006 0.072¬±0.008 0.044¬±0.006
EWC 0.445¬±0.012 0.288¬±0.034 0.188¬±0.031 0.213¬±0.008 0.095¬±0.007 0.046¬±0.011
LWM 0.452¬±0.023 0.294¬±0.032 0.226¬±0.025 0.180¬±0.028 0.090¬±0.011 0.044¬±0.008
LWF 0.301¬±0.048 0.175¬±0.028 0.129¬±0.023 0.219¬±0.019 0.078¬±0.008 0.072¬±0.008
ICICLE 0.654¬±0.011 0.602¬±0.035 0.497¬±0.099 0.350¬±0.053 0.185¬±0.005 0.099¬±0.003
Multi-task 0.858¬±0.005 0.905¬±0.012 0.935¬±0.019 0.499¬±0.009 0.196¬±0.017 0.148¬±0.009
FeTrIL [65] 0.750¬±0.008 0.607¬±0.018 0.407¬±0.051 0.375¬±0.006 0.199¬±0.003 0.127¬±0.011
PASS [95] 0.775¬±0.006 0.647¬±0.003 0.518¬±0.012 0.395¬±0.001 0.233¬±0.009 0.139¬±0.017
Table 2: Average incremental accuracy comparison for different numbers of tasks on CUB-200-2011, demonstrating the
negative impact of the high number of tasks to be learned on models‚Äô performance. Despite this trend, ICICLE outperforms
the baseline methods across all task numbers. Additionally, we show the gap between interpretable and black-box models by
comparing ICICLE to FeTrIL and PASS.
TASK-AWARE ACCURACY TASK-AGNOSTIC ACCURACY
METHOD TASK 1 TASK 2 TASK 3 TASK 4 TASK 1 TASK 2 TASK 3 TASK 4
FREEZING 0.806¬±0.024 0.462¬±0.037 0.517¬±0.041 0.455¬±0.027 0.570¬±0.031 0.195¬±0.017 0.258¬±0.019 0.213¬±0.020
FINETUNING 0.007¬±0.004 0.016¬±0.008 0.032¬±0.009 0.759¬±0.019 0.0¬±0.0 0.0¬±0.0 0.0¬±0.0 0.759¬±0.019
EWC 0.244¬±0.024 0.378¬±0.072 0.539¬±0.043 0.602¬±0.054 0.001¬±0.001 0.059¬±0.004 0.267¬±0.031 0.527¬±0.051
LWF 0.169¬±0.046 0.119¬±0.008 0.235¬±0.017 0.743¬±0.061 0.158¬±0.035 0.003¬±0.002 0.018¬±0.003 0.537¬±0.142
LWM 0.195¬±0.012 0.412¬±0.014 0.430¬±0.028 0.772¬±0.011 0.027¬±0.024 0.023¬±0.020 0.085¬±0.005 0.772¬±0.037
ICICLE 0.523¬±0.020 0.663¬±0.053 0.709¬±0.038 0.723¬±0.002 0.233¬±0.014 0.365¬±0.021 0.314¬±0.011 0.486¬±0.021
Table 3: Comparison of task accuracies for modified ProtoPNet architecture in a class-incremental learning scenario after
4 tasks train on CUB-200-2011 dataset, averaged over 3 runs with standard error of the mean. Our ICICLE outperforms
baseline methods and achieves the best results for all previous incremental tasks, demonstrating its ability to maintain prior
knowledge while learning new tasks. Freezing due to the weight fixation cannot properly learn new tasks.
higher, resulting in poorer results, which may be attributed
to the number of details that prototypes need to capture to
classify a task correctly. Furthermore, we have noticed that
freezing is a robust baseline for a task-aware scenario be-
cause of the model‚Äôs fixed nature and pretrained backbone.
Interpretability To evaluate if the prototype‚Äôs graphical
representation of the concept has changed and how much
we use the IoU metric [75]. IoU measures what is the over-
lap of the prototype visual representation (like in Figure 1)
from the task in which it was learned, through all the fol-
lowing tasks. Freezing is superior in preserving the proto-
typical information because all weights from previous tasks
are fixed. In terms of methods allowing changes in back-
bone and previously learned prototypes, ICICLE is superior
over all baselines, as shown in Table 1. ICICLE keeps in-
terpretable prototypes consistent with interpretability regu-
larization distilling already learned concepts.
5.1. Ablation study and analysis
Why changes in ProtoPNet architecture and training are
needed? ProtoPNet in the last training stage (last layer
convex optimizations) aims to finetune positive connections
and regularize the negative to be 0. As a result, the con-
verged model returns interpretations in the form of positive
reasoning, desired by the end users [11]. In the CL setting,
the last step of training changes the negative connections
in a different manner (see Figure 6). On the other hand,Regularization Initialization Compensation TAw acc. TAg acc.
0.216 0.182
‚úì 0.559 0.280
‚úì ‚úì 0.654 0.335
‚úì ‚úì ‚úì 0.654 0.350
Table 4: Influence of different novel components on the
average incremental accuracy in four tasks learning sce-
nario. Combination of all components results in the best-
performing model.
Avg weight value 
class index class index negative connections positive connections 
Figure 6: Average weight of positive and negative connec-
tions per class in 4 task learning scenario. Unbalanced and
strong negative connections between tasks result in unde-
sired properties in terms of the model‚Äôs interpretability.
in an exemplar-free continual learning scenario, conduct-
ing the last-layer learning phase is unfeasible at the end of
the training. That is why, we modified the ProtoPNet‚Äôs last
layer and retained only positive connections initialized to 1,
eliminating the need for the convex optimization step.

--- PAGE 7 ---
similarity regularization distance regularization feature regularization 
space in which loss function 
has the same value prototypical part 
image part representation 
ProtoPNet similarity function similarity value 
distance value Figure 7: Visualization of three possible approaches to in-
terpretability similarity and their influence on the model
plasticity. Only similarity-based regularization takes into
account how a given image part corresponds to a prototyp-
ical part. If it is close then the similarity value is high and
small changes in the distance results in a great decrease in
similarity. While latent vectors that are distant from pro-
totypical parts can more freely be changed by the model to
better represents the current task data. Other approaches are
limiting the models‚Äô plasticity treating each latent represen-
tation of the image part as equally important.
What is the influence of each of the introduced compo-
nents? Table 4 presents the influence of different com-
ponents of our approach on the final model‚Äôs average in-
cremental accuracy in the CUB-200-2011 dataset with four
tasks split scenario. Combining all the components resulted
in the best-performing model. Our results show that com-
pensation of task-recency bias helps in task-agnostic eval-
uation and gives additional improvement of 4.5%. How-
ever, most of the accuracy improvements were attributed to
interpretability regularization and proximity initialization.
Notably, task-recency bias compensation significantly im-
proved the performance of task one classes compared to an
approach without it, from 0.028 to 0.255 in a task-agnostic
scenario, as detailed in the Supplementary Materials.
Where should we perform interpretability regulariza-
tion? The ProtoPNet model‚Äôs prototypical layer can be
regularized in three different ways: feature regularization
on add-on layer representations, regularization of distances
between prototypical parts and latent data vectors, and
similarity-based regularization. The strictest approach is
feature regularization, which does not allow the model to
change how it represents data from a new task, resulting
in significantly reduced model plasticity. When distances
are regularized, the model can change its representation to
maintain the same distance from the prototype on the sur-
face of the sphere. On the other hand, similarity-based reg-
ularization allows the model to retain key knowledge from
previous tasks by preserving only the information related toInitialization type Random Distant All Proximity
Task aware acc. 0.559 0.592 0.626 0.654
Task agnostic acc. 0.280 0.290 0.297 0.335
Table 5: Comparison of different initialization strategies for
prototypical parts. Our proximity initialization of new task
prototypes is superior.
specific features that are close to the prototypical parts in the
latent space, allowing for greater flexibility in exchange for
forgetting irrelevant features. Therefore, we stick to inter-
pretability regularization in ICICLE, which is based on sim-
ilarities and maintains the essential knowledge from previ-
ous tasks while retaining high plasticity to learn new ones.
Figure 7 illustrates these three approaches and their com-
parison in terms of average incremental accuracy for Pro-
toPNet only with regularization (without changing initial-
ization): 0.507,0.535, and 0.559in task-aware and 0.261,
0.230, and 0.280in task-agnostic scenarios for feature, dis-
tance, and similarity-based regularization, respectively, on
the CUB-200-2011 dataset with four tasks scenarios.
What is the influence of hyperparameters in inter-
pretability regularization? In Figure 8 and Figure 9 the
influence of Œªand mask percentile threshold in the inter-
pretability regularization on average incremental accuracies
are presented. We use CUB-200-2011 datasets with four
tasks split setting. For this dataset, the results reveal that
the regularization of only the maximum prototypical sim-
ilarity is the most effective (Figure 9). Regarding ŒªIR, a
value that is too small leads to high network plasticity, in-
creased forgetting, and poor results, while a value that is too
large reduces model plasticity and may not represent new
knowledge well.
Which way is the best to initialize new prototypical
parts? In this ablation part, we investigate the optimal
strategy for initializing prototypical parts at the beginning
of a new task in the ProtoPNet model. We evaluate our ini-
tialization method, which initializes the parts in close prox-
imity to existing prototypes, against three other approaches:
random initialization, clustering of all image part represen-
tations, and clustering of only distant latent vectors. Re-
sults are presented in Table 5. The proximity initialization
method outperforms the distant strategy, as the latter tends
to assign prototypical parts to latent vectors that correspond
to the background of the images, resulting in learning irrel-
evant concepts that can easily activate on other task data, as
shown in the Supplementary Materials.
Does ICICLE generalize to other architectures?
Lastly, we show that ICICLE generalizes to other concept-
based architecture. We demonstrate that using a TesNet
model [85], and provide results in Table 6, where ICICLE
obtains the best results. The average incremental accuracy
of ICICLE with TesNet is even better than ProtoPNet for
both task-aware and task-agnostic evaluation.

--- PAGE 8 ---
Freezing Finetuning EWC LWM LWF ICICLE
TAw Acc. 0.637 0.355 0.592 0.648 0.581 0.746
TAg Acc. 0.222 0.183 0.272 0.252 0.205 0.362
Table 6: Results for four task learning scenario on CUB-
200-2011 dataset with TesNet [85] as a concept-based ar-
chitecture. The table shows the versatility of the ICICLE
approach for interpretable models.
Figure 8: Influence of the ŒªIRin the interpretability regu-
larization.
Figure 9: Influence of Œ≥in the interpretability regulariza-
tion. Notice that regularizing only in the place of maximum
similarity is the most beneficial for ICICLE for the four task
learning scenario in CUB-200-2011.
6. Conclusions and future work
This work proposes a novel approach called ICICLE for
interpretable class incremental learning. ICICLE is based
on prototypical parts and incorporates interpretability reg-
ularization, proximity initialization, and compensation for
task-recency bias. The proposed method outperforms clas-
sical class-incremental learning methods applied for pro-
totypical part-based networks in terms of task-aware and
task-agnostic accuracies while maintaining prototype inter-
pretability. We also conducted ablation studies and multiple
analyses to justify our choices and highlight the challenges
associated with combining interpretable concepts with CL.
This work is expected to inspire research on XAI and CL.
Moving forward, we plan to explore methods suitable for
single-class incremental learning with interpretable models.
We also intend to investigate how other interpretable archi-
tectures, such as B-COS [8], can be adapted to the class
incremental learning scenario.Limitations. Our work is limited to prototypical part
methods, that are suited for fine-grained image recognition
and inherits their drawbacks previously discussed in [27,
36, 42, 60, 73].However, recently there are first works gen-
eralizing them to standard datasets (not fine-grained) [61].
Additionally, as we consider only an exemplar-free scenario
and closed-set recognition, we do not analyze how having
a replay buffer would influence the method‚Äôs performance
and how this method would fit in the open-set settings.
Impact. ICICLE highlights that traditional exemplar-free
approaches for continual learning are not well suited for
gray-box models that utilize concepts for predictions. This
finding has implications for the development of continual
learning methods, as they must balance the need for gener-
ality with the need to be adapted to specific architectures.
Furthermore, it has an impact on the field of concept-based
models and explainable AI, demonstrating the need for fur-
ther research on CL methods for XAI. In some cases, prac-
titioners who know that their system will need to learn new
tasks continuously may choose to use black-box models and
explainers rather than interpretable models, sacrificing the
fidelity of explanations for improved model performance.
Acknowledgements
Joost van de Weijer and Bart≈Çomiej Twardowski ac-
knowledge the support from the Spanish Government
funding for projects PID2019-104174GB-I00, TED2021-
132513B-I00, and grant RYC2021-032765-I, the European
Commission under the Horizon 2020 Programme, funded
by MCIN/AEI/10.13039/501100011033 and by European
Union NextGenerationEU/PRTR. The work of Dawid Ry-
marczyk was supported by the National Science Centre
(Poland), grant no. 2022/45/N/ST6/04147. He also re-
ceived an incentive scholarship from the program Ex-
cellence Initiative - Research University funds at the
Jagiellonian University in Krak ¬¥ow. The work of Bar-
tosz Zieli ¬¥nski was funded by the National Science Centre
(Poland) grant no. 2022/47/B/ST6/03397 and the research
project ‚ÄúBio-inspired artificial neural network‚Äù (grant no.
POIR.04.04.00-00-14DE/18-00) within the Team-Net pro-
gram of the Foundation for Polish Science co-financed by
the European Union under the European Regional Devel-
opment Fund. Finally, some experiments were performed
on servers purchased with funds from a Priority Research
Area (Artificial Intelligence Computing Center Core Facil-
ity) grant under the Strategic Programme Excellence Initia-
tive at Jagiellonian University.
References
[1] Ehsan Abbasnejad, Damien Teney, Amin Parvaneh, Javen
Shi, and Anton van den Hengel. Counterfactual vision and
language learning. In Proceedings of the IEEE/CVF Con-

--- PAGE 9 ---
ference on Computer Vision and Pattern Recognition , pages
10044‚Äì10054, 2020. 2
[2] Michael Anis Mihdi Afnan, Yanhe Liu, Vincent Conitzer,
Cynthia Rudin, Abhishek Mishra, Julian Savulescu, and Ma-
soud Afnan. Interpretable, not black-box, artificial intelli-
gence should be used for embryo selection. Human Repro-
duction Open , 2021. 3
[3] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny,
Marcus Rohrbach, and Tinne Tuytelaars. Memory aware
synapses: Learning what (not) to forget. In European Con-
ference on Computer Vision , 2018. 2
[4] David Alvarez Melis and Tommi Jaakkola. Towards robust
interpretability with self-explaining neural networks. In S.
Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett, editors, Advances in Neural Infor-
mation Processing Systems , volume 31. Curran Associates,
Inc., 2018. 2
[5] Jihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo Ha,
and Jonghyun Choi. Rainbow memory: Continual learn-
ing with a memory of diverse samples. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 8218‚Äì8227, 2021. 2
[6] Alina Jade Barnett, Fides Regina Schwartz, Chaofan Tao,
Chaofan Chen, Yinhao Ren, Joseph Y Lo, and Cynthia
Rudin. A case-based interpretable deep learning model for
classification of mass lesions in digital mammography. Na-
ture Machine Intelligence , 3(12):1061‚Äì1070, 2021. 3
[7] Dominika Basaj, Witold Oleszkiewicz, Igor Sieradzki,
Micha≈Ç G ¬¥orszczak, B Rychalska, T Trzcinski, and B Zielin-
ski. Explaining self-supervised image representations with
visual probing. In International Joint Conference on Artifi-
cial Intelligence , 2021. 2
[8] Moritz B ¬®ohle, Mario Fritz, and Bernt Schiele. B-cos net-
works: alignment is all we need for interpretability. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 10329‚Äì10338, 2022. 8
[9] Wieland Brendel and Matthias Bethge. Approximating
CNNs with bag-of-local-features models works surprisingly
well on imagenet. In International Conference on Learning
Representations , 2019. 2
[10] Lukas Brunke, Melissa Greeff, Adam W Hall, Zhaocong
Yuan, Siqi Zhou, Jacopo Panerati, and Angela P Schoel-
lig. Safe learning in robotics: From learning-based control
to safe reinforcement learning. Annual Review of Control,
Robotics, and Autonomous Systems , 5:411‚Äì444, 2022. 1
[11] Rudin C. et al. Interpretable machine learning: Fundamental
principles and 10 grand challenges. Statistics Surveys , 16:1‚Äì
85, 2022. 1, 6
[12] Rudin C. and Radin J. Why are we using black box models
in ai when we don‚Äôt need to? a lesson from an explainable ai
competition. Harvard Data Science Review , 1(2):10‚Äì1162,
2019. 1
[13] Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajan-
than, and Philip HS Torr. Riemannian walk for incremental
learning: understanding forgetting and intransigence. In Eu-
ropean Conference on Computer Vision , 2018. 2[14] Arslan Chaudhry, Marc‚ÄôAurelio Ranzato, Marcus Rohrbach,
and Mohamed Elhoseiny. Efficient lifelong learning with a-
gem. In International Conference on Learning Representa-
tions , 2019. 2
[15] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny,
Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS
Torr, and Marc‚ÄôAurelio Ranzato. On tiny episodic memo-
ries in continual learning. arXiv preprint arXiv:1902.10486 ,
2019. 2
[16] Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia
Rudin, and Jonathan K Su. This looks like that: deep learn-
ing for interpretable image recognition. Advances in Neural
Information Processing Systems , 32, 2019. 2, 3, 4, 5
[17] Xuxin Chen, Ximin Wang, Ke Zhang, Kar-Ming Fung,
Theresa C Thai, Kathleen Moore, Robert S Mannel, Hong
Liu, Bin Zheng, and Yuchen Qiu. Recent advances and clin-
ical applications of deep learning in medical image analysis.
Medical Image Analysis , page 102444, 2022. 1
[18] Zhi Chen, Yijie Bei, and Cynthia Rudin. Concept whitening
for interpretable image recognition. Nature Machine Intelli-
gence , 2(12):772‚Äì782, 2020. 2
[19] Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah
Parisot, Xu Jia, Ales Leonardis, Greg Slabaugh, and Tinne
Tuytelaars. A continual learning survey: Defying forgetting
in classification tasks. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence , 2021. 1, 2
[20] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248‚Äì255. Ieee, 2009. 5
[21] Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng,
Ziyan Wu, and Rama Chellappa. Learning without memoriz-
ing. In Conference on Computer Vision and Pattern Recog-
nition , 2019. 2, 5
[22] Jon Donnelly, Alina Jade Barnett, and Chaofan Chen. De-
formable protopnet: An interpretable image classifier using
deformable prototypes. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 10265‚Äì10275, 2022. 3
[23] Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas
Robert, and Eduardo Valle. Podnet: Pooled outputs dis-
tillation for small-tasks incremental learning. In Computer
Vision‚ÄìECCV 2020: 16th European Conference, Glasgow,
UK, August 23‚Äì28, 2020, Proceedings, Part XX 16 , pages
86‚Äì102. Springer, 2020. 2
[24] Ruth Fong, Mandela Patrick, and Andrea Vedaldi. Un-
derstanding deep networks via extremal perturbations and
smooth masks. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 2950‚Äì2958,
2019. 2
[25] Ruth C Fong and Andrea Vedaldi. Interpretable explana-
tions of black boxes by meaningful perturbation. In Pro-
ceedings of the IEEE international conference on computer
vision , pages 3429‚Äì3437, 2017. 2
[26] Robert M. French. Catastrophic forgetting in connectionist
networks. Trends in cog. scie. , 1999. 1
[27] Srishti Gautam, Marina M-C H ¬®ohne, Stine Hansen, Robert
Jenssen, and Michael Kampffmeyer. This looks more like

--- PAGE 10 ---
that: Enhancing self-explaining models by prototypical rele-
vance propagation. Pattern Recognition , 136:109172, 2023.
8
[28] Alan H Gee, Diego Garcia-Olano, Joydeep Ghosh, and
David Paydarfar. Explaining deep classification of time-
series data with learned prototypes. In CEUR workshop pro-
ceedings , volume 2429, page 15. NIH Public Access, 2019.
3
[29] Amirata Ghorbani, James Wexler, James Y Zou, and Been
Kim. Towards automatic concept-based explanations. In H.
Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ¬¥e-Buc, E.
Fox, and R. Garnett, editors, Advances in Neural Informa-
tion Processing Systems , volume 32. Curran Associates, Inc.,
2019. 2
[30] Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville,
and Yoshua Bengio. An empirical investigation of catas-
trophic forgetting in gradient-based neural networks. In In-
ternational Conference on Learning Representations , 2014.
1
[31] Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh,
and Stefan Lee. Counterfactual visual explanations. In In-
ternational Conference on Machine Learning , pages 2376‚Äì
2384. PMLR, 2019. 2
[32] Filip Guzy, Micha≈Ç Wo ¬¥zniak, and Bartosz Krawczyk. Evalu-
ating and explaining generative adversarial networks for con-
tinual learning under concept drift. In 2021 International
Conference on Data Mining Workshops (ICDMW) , pages
295‚Äì303. IEEE, 2021. 1
[33] Peter Hase, Chaofan Chen, Oscar Li, and Cynthia Rudin. In-
terpretable image recognition with hierarchical prototypes.
InProceedings of the AAAI Conference on Human Compu-
tation and Crowdsourcing , volume 7, pages 32‚Äì40, 2019. 3
[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770‚Äì778, 2016. 5
[35] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling
the knowledge in a neural network. In NIPS Deep Learning
Workshop , 2014. 4
[36] Adrian Hoffmann, Claudio Fanconi, Rahul Rade, and Jonas
Kohler. This looks like that... does it? shortcomings of latent
space prototype interpretability in deep networks, 2021. 8
[37] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and
Dahua Lin. Learning a unified classifier incrementally via re-
balancing. In International Conference on Computer Vision ,
2019. 2
[38] Xinting Hu, Kaihua Tang, Chunyan Miao, Xian-Sheng Hua,
and Hanwang Zhang. Distilling causal effect of data in class-
incremental learning. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
3957‚Äì3966, 2021. 2
[39] Monish Keswani, Sriranjani Ramakrishnan, Nishant Reddy,
and Vineeth N Balasubramanian. Proto2proto: Can you
recognize the car, the way i do? In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10233‚Äì10243, 2022. 3, 4[40] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai,
James Wexler, Fernanda Viegas, et al. Interpretability be-
yond feature attribution: Quantitative testing with concept
activation vectors (tcav). In International conference on ma-
chine learning , pages 2668‚Äì2677. PMLR, 2018. 2
[41] Eunji Kim, Siwon Kim, Minji Seo, and Sungroh Yoon. Xpro-
tonet: Diagnosis in chest radiography with global and local
explanations. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 15719‚Äì
15728, 2021. 3
[42] Sunnie SY Kim, Nicole Meister, Vikram V Ramaswamy,
Ruth Fong, and Olga Russakovsky. Hive: evaluating the
human interpretability of visual explanations. In Computer
Vision‚ÄìECCV 2022: 17th European Conference, Tel Aviv,
Israel, October 23‚Äì27, 2022, Proceedings, Part XII , pages
280‚Äì298. Springer, 2022. 8
[43] B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Man-
nion, Ahmad A Al Sallab, Senthil Yogamani, and Patrick
P¬¥erez. Deep reinforcement learning for autonomous driving:
A survey. IEEE Transactions on Intelligent Transportation
Systems , 23(6):4909‚Äì4926, 2021. 1
[44] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel
Veness, Guillaume Desjardins, Andrei A Rusu, Kieran
Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-
Barwinska, et al. Overcoming catastrophic forgetting in neu-
ral networks. National Academy of Sciences , 2017. 1, 2, 5
[45] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen
Mussmann, Emma Pierson, Been Kim, and Percy Liang.
Concept bottleneck models. In Hal Daum ¬¥e III and Aarti
Singh, editors, Proceedings of the 37th International Con-
ference on Machine Learning , volume 119 of Proceedings
of Machine Learning Research , pages 5338‚Äì5348. PMLR,
13‚Äì18 Jul 2020. 2
[46] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
3d object representations for fine-grained categorization. In
Proceedings of the IEEE international conference on com-
puter vision workshops , pages 554‚Äì561, 2013. 2, 5
[47] Oscar Li, Hao Liu, Chaofan Chen, and Cynthia Rudin. Deep
learning for case-based reasoning through prototypes: A
neural network that explains its predictions. In Proceedings
of the AAAI Conference on Artificial Intelligence , volume 32,
2018. 3
[48] Zhizhong Li and Derek Hoiem. Learning without forgetting.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence , 2017. 2, 4, 5
[49] Xialei Liu, Marc Masana, Luis Herranz, Joost Van de Wei-
jer, Antonio M Lopez, and Andrew D Bagdanov. Rotate your
networks: Better weight consolidation and less catastrophic
forgetting. In International Conference on Pattern Recogni-
tion, 2018. 2
[50] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggy-
back: Adapting a single network to multiple tasks by learn-
ing to mask weights. In European Conference on Computer
Vision , 2018. 2
[51] Arun Mallya and Svetlana Lazebnik. Packnet: Adding mul-
tiple tasks to a single network by iterative pruning. In Con-
ference on Computer Vision and Pattern Recognition , 2018.
2

--- PAGE 11 ---
[52] Emanuele Marconato, Gianpaolo Bontempo, Stefano Teso,
Elisa Ficarra, Simone Calderara, and Andrea Passerini.
Catastrophic forgetting in continual concept bottleneck mod-
els. In Image Analysis and Processing. ICIAP 2022 Work-
shops: ICIAP International Workshops, Lecce, Italy, May
23‚Äì27, 2022, Revised Selected Papers, Part II , pages 539‚Äì
547. Springer, 2022. 1
[53] Diego Marcos, Sylvain Lobry, and Devis Tuia. Semantically
interpretable activation maps: what-where-how explanations
within cnns. In 2019 IEEE/CVF International Conference
on Computer Vision Workshop (ICCVW) , pages 4207‚Äì4215.
IEEE, 2019. 2
[54] Marc Masana, Xialei Liu, Bartlomiej Twardowski, Mikel
Menta, Andrew D Bagdanov, and Joost van de Weijer. Class-
incremental learning: Survey and performance evaluation on
image classification. IEEE Transactions on Pattern Analysis
and Machine Intelligence , pages 1‚Äì20, 2022. 2, 5
[55] Yao Ming, Panpan Xu, Huamin Qu, and Liu Ren. Inter-
pretable and steerable sequence learning via prototypes. In
Proceedings of the 25th ACM SIGKDD International Con-
ference on Knowledge Discovery & Data Mining , pages
903‚Äì913, 2019. 3
[56] Ramaravind K Mothilal, Amit Sharma, and Chenhao Tan.
Explaining machine learning classifiers through diverse
counterfactual explanations. In Proceedings of the 2020
Conference on Fairness, Accountability, and Transparency ,
pages 607‚Äì617, 2020. 2
[57] Martin Mundt, Yongwon Hong, Iuliia Pliushch, and Vis-
vanathan Ramesh. A wholistic view of continual learn-
ing with deep neural networks: Forgotten lessons and the
bridge to active and open world learning. Neural Networks ,
160:306‚Äì336, 2023. 2
[58] Martin Mundt, Iuliia Pliushch, Sagnik Majumder, Yongwon
Hong, and Visvanathan Ramesh. Unified probabilistic deep
continual learning through generative replay and open set
recognition. Journal of Imaging , 8(4):93, 2022. 2
[59] Meike Nauta et al. Neural prototype trees for interpretable
fine-grained image recognition. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 14933‚Äì14943, 2021. 3
[60] Meike Nauta, Annemarie Jutte, Jesper Provoost, and Christin
Seifert. This looks like that, because... explaining proto-
types for interpretable image recognition. In Machine Learn-
ing and Principles and Practice of Knowledge Discovery in
Databases: International Workshops of ECML PKDD 2021,
Virtual Event, September 13-17, 2021, Proceedings, Part I ,
pages 441‚Äì456. Springer, 2022. 8
[61] Meike Nauta, J ¬®org Schl ¬®otterer, Maurice van Keulen, and
Christin Seifert. Pip-net: Patch-based intuitive prototypes
for interpretable image classification. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 2744‚Äì2753, 2023. 8
[62] Yulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, Xian-
Sheng Hua, and Ji-Rong Wen. Counterfactual vqa: A cause-
effect look at language bias. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 12700‚Äì12710, 2021. 2[63] Schramowski P. et al. Making deep neural networks right for
the right scientific reasons by interacting with their explana-
tions. Nature Machine Intelligence , 2(8):476‚Äì486, 2020. 1
[64] Arijit Patra and J Alison Noble. Incremental learning of fetal
heart anatomies using interpretable saliency maps. In Med-
ical Image Understanding and Analysis: 23rd Conference,
MIUA 2019, Liverpool, UK, July 24‚Äì26, 2019, Proceedings
23, pages 129‚Äì141. Springer, 2020. 1
[65] Gr ¬¥egoire Petit, Adrian Popescu, Hugo Schindler, David Pi-
card, and Bertrand Delezoide. Fetril: Feature translation for
exemplar-free class-incremental learning. In Proceedings of
the IEEE/CVF Winter Conference on Applications of Com-
puter Vision , pages 3911‚Äì3920, 2023. 6
[66] Ameya Prabhu, Philip HS Torr, and Puneet K Dokania.
Gdumb: A simple approach that questions our progress in
continual learning. In Computer Vision‚ÄìECCV 2020: 16th
European Conference, Glasgow, UK, August 23‚Äì28, 2020,
Proceedings, Part II 16 , pages 524‚Äì540. Springer, 2020. 13
[67] Sylvestre-Alvise Rebuffi, Ruth Fong, Xu Ji, and Andrea
Vedaldi. There and back again: Revisiting backpropagation
saliency methods. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
8839‚Äì8848, 2020. 2
[68] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg
Sperl, and Christoph H Lampert. icarl: Incremental classi-
fier and representation learning. In Conference on Computer
Vision and Pattern Recognition , 2017. 2
[69] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
‚Äù why should i trust you?‚Äù explaining the predictions of any
classifier. In Proceedings of the 22nd ACM SIGKDD interna-
tional conference on knowledge discovery and data mining ,
pages 1135‚Äì1144, 2016. 2
[70] Cynthia Rudin. Stop explaining black box machine learn-
ing models for high stakes decisions and use interpretable
models instead. Nature Machine Intelligence , 1(5):206‚Äì215,
2019. 1, 2
[71] Dawid Rymarczyk, Daniel Dobrowolski, and Tomasz Danel.
Progrest: Prototypical graph regression soft trees for molec-
ular property prediction. SIAM International Conference on
Data Mining , 2023. 3
[72] Dawid Rymarczyk, Aneta Kaczy ¬¥nska, Jaros≈Çaw Kraus,
Adam Pardyl, and Bartosz Zieli ¬¥nski. Protomil: Multiple in-
stance learning with prototypical parts for fine-grained in-
terpretability. In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases . Springer,
2022. 3
[73] Dawid Rymarczyk, ≈Åukasz Struski, Micha≈Ç G ¬¥orszczak, Ko-
ryna Lewandowska, Jacek Tabor, and Bartosz Zieli ¬¥nski. In-
terpretable image classification with differentiable proto-
types assignment. In Proceedings of the European Confer-
ence on Computer Vision (ECCV) , 2022. 3, 8
[74] Dawid Rymarczyk, ≈Åukasz Struski, Jacek Tabor, and Bartosz
Zieli ¬¥nski. Protopshare: Prototypical parts sharing for simi-
larity discovery in interpretable image classification. In Pro-
ceedings of the 27th ACM SIGKDD International Confer-
ence on Knowledge Discovery & Data Mining , pages 1420‚Äì
1430, 2021. 3

--- PAGE 12 ---
[75] Miko≈Çaj Sacha, Dawid Rymarczyk, ≈Åukasz Struski, Jacek
Tabor, and Bartosz Zieli ¬¥nski. Protoseg: Interpretable seman-
tic segmentation with prototypical parts. In Winter Confer-
ence on Applications of Computer Vision (WACV) , 2023. 3,
6
[76] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,
Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.
Grad-cam: Visual explanations from deep networks via
gradient-based localization. In Proceedings of the IEEE in-
ternational conference on computer vision , pages 618‚Äì626,
2017. 2
[77] Ramprasaath R Selvaraju, Stefan Lee, Yilin Shen, Hongxia
Jin, Shalini Ghosh, Larry Heck, Dhruv Batra, and Devi
Parikh. Taking a hint: Leveraging explanations to make vi-
sion and language models more grounded. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 2591‚Äì2600, 2019. 2
[78] Joan Serra, Didac Suris, Marius Miron, and Alexandros
Karatzoglou. Overcoming catastrophic forgetting with hard
attention to the task. In International Conference on Machine
Learning , 2018. 2
[79] Yujun Shi, Li Yuan, Yunpeng Chen, and Jiashi Feng. Con-
tinual learning via bit-level information preserving. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 16674‚Äì16683, 2021. 2
[80] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.
Deep inside convolutional networks: Visualising image clas-
sification models and saliency maps. In In Workshop at Inter-
national Conference on Learning Representations . Citeseer,
2014. 2
[81] Gurmail Singh and Kin-Choong Yow. These do not look
like those: An interpretable deep learning model for image
recognition. IEEE Access , 9:41482‚Äì41493, 2021. 3
[82] Shixiang Tang, Dapeng Chen, Jinguo Zhu, Shijie Yu, and
Wanli Ouyang. Layerwise optimization by gradient de-
composition for continual learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 9634‚Äì9643, 2021. 2
[83] Gido M van de Ven and Andreas S Tolias. Three scenar-
ios for continual learning. In NeurIPS Continual Learning
Workshop , 2018. 2
[84] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-
ona, and Serge Belongie. The caltech-ucsd birds-200-2011
dataset. 2011. 2, 5
[85] Jiaqi Wang et al. Interpretable image recognition by con-
structing transparent embedding space. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 895‚Äì904, 2021. 3, 7, 8
[86] Liyuan Wang, Kuo Yang, Chongxuan Li, Lanqing Hong,
Zhenguo Li, and Jun Zhu. Ordisco: Effective and efficient
usage of incremental unlabeled data for semi-supervised
continual learning. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
5383‚Äì5392, 2021. 2
[87] Pei Wang and Nuno Vasconcelos. Scout: Self-aware dis-
criminant counterfactual explanations. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 8981‚Äì8990, 2020. 2[88] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye,
Zicheng Liu, Yandong Guo, and Yun Fu. Large scale in-
cremental learning. In Conference on Computer Vision and
Pattern Recognition , 2019. 2
[89] Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynam-
ically expandable representation for class incremental learn-
ing. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 3014‚Äì3023,
2021. 2
[90] Chih-Kuan Yeh, Been Kim, Sercan Arik, Chun-Liang Li,
Tomas Pfister, and Pradeep Ravikumar. On completeness-
aware concept-based explanations in deep neural networks.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and
H. Lin, editors, Advances in Neural Information Process-
ing Systems , volume 33, pages 20554‚Äì20565. Curran Asso-
ciates, Inc., 2020. 2
[91] Lu Yu, Bartlomiej Twardowski, Xialei Liu, Luis Herranz,
Kai Wang, Yongmei Cheng, Shangling Jui, and Joost van de
Weijer. Semantic drift compensation for class-incremental
learning. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 6982‚Äì6991,
2020. 5
[92] Friedemann Zenke, Ben Poole, and Surya Ganguli. Contin-
ual learning through synaptic intelligence. In International
Conference on Machine Learning , 2017. 2
[93] Mengyao Zhai, Lei Chen, and Greg Mori. Hyper-
lifelonggan: Scalable lifelong learning for image condi-
tioned generation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
2246‚Äì2255, 2021. 2
[94] Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and
Cheekong Lee. Protgnn: Towards self-explaining graph neu-
ral networks. 2022. 3
[95] Fei Zhu, Xu-Yao Zhang, Chuang Wang, Fei Yin, and Cheng-
Lin Liu. Prototype augmentation and self-supervision for
incremental learning. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
5871‚Äì5880, 2021. 5, 6

--- PAGE 13 ---
Additional experimental setup details
Here we present additional details on the
experimental setup. We performed a hy-
perparameter search for Œªdist (Œªdist ‚àà
{10.0,5.0,0.0,0.5,0.1,0.05,0.01,0.005,0.001,0.0005,
0.0001}). We use Adam optimizer with a learning rate
of0.001and parameters Œ≤1= 0.9andŒ≤2= 0.999. We
set the batch size to 75and use input images of resolution
224√ó224√ó3. The weights of the network are initialized
with Xavier‚Äôs normal initializer.
We perform a warmup training where the weights of f
are frozen for 10epochs, and then we train the model until it
converges with 12epochs early stopping. We use the learn-
ing schema presented in Table 7. Depending on the num-
ber of tasks, we perform warm-up training with {5,5,4}
epochs and joint training phase for {21,15,11}, longer with
fewer tasks. Similarly, we perform prototype projection ev-
ery{10,7,5}epoch. So with more tasks, we perform fewer
training epochs (Table 7).
Results on Stanford Cars
Table 8 depicts how the ICICLE method works on the
Stanford Cars dataset compared to other baseline methods.
Results are consistent with the ones on CUB-200-2011 and
show that ICICLE outperforms all standard CL learning
methods adapted to ProtoPNet architecture.
Comparison to GDumb
Additionally, we compared our approach with
GDumb [66], a baseline method in CL learning, in
scenarios involving 3, 5, and 10 images per class with 4
tasks learning achieving 20.3, 34.2, 57.6, and 13.0, 26.7,
48.8 for task-aware and task-agnostic respectively. ICICLE
outperformed GDumb with a small number of examples,
and task-aware for GDumb-10 was the only exception
where GDumb achieved a higher accuracy score.
Distant initialization
In Table 5, we showed that proximity-based initializa-
tion is most beneficial. However, here in Figure 10, we
show how initialization of prototypical parts at a distance
from already existing once generates concepts that are too
general or carry information about background.
Task-recency bias compensation
Here, in Table 9 we show what is the influence of task-
recency bias compensation on each task accuracy for task
agnostic scenario. After compensation, the accuracy on task
1 increased the most, but at the same time accuracy on all
other tasks was sacrificed. However, the average accuracy
after the compensation is increased.
Figure 10: Image depicts the 2D TSNE projection of proto-
types. One can observe that there is a cluster of prototypes
from all tasks except the first one (yellow box). This is ob-
servable with distant initialization and prototype visualiza-
tions show that those prototypical parts are representing the
background or vague concepts.
Detailed results after learning each task
In Table 10, and Table 11 we show detailed accuracies of
the ICICLE after learning each task for CUB-200-2011 on a
single run with the same seed in four and ten tasks learning
scenarios.
Analysis of the hyperparameters for baselines
In Table 12, Table 13, and Table 14 we show the influ-
ence of the hyperparameters for each of the baseline meth-
ods, EWC, LWF, and LWM, respectively. Based on that,
the parameters of these methods were chosen for compari-
son with ICICLE.

--- PAGE 14 ---
Phase Model layers Learning rate Scheduler Weight decay Duration
Warm-upadd-on 1√ó1convolution 1¬∑10‚àí3
None None 5,5,4epochsprototypical layer 1¬∑10‚àí3
Jointconvolutions f 1¬∑10‚àí4
by half every
5epochs10‚àí421,15,10epochs
early stoppingadd-on 1√ó1convolution 1¬∑10‚àí3
prototypical layer 1¬∑10‚àí3
Table 7: Learning schema for the ICICLE method.
AVG.INC.TASK -AWARE ACCURACY AVG.INC.TASK -TASK AGNOSTIC ACCURACY
METHOD 4TASKS 7TASKS 14TASKS 4TASKS 7TASKS 14TASKS
FREEZING 0.572¬±0.031 0.518¬±0.041 0.486¬±0.026 0.309¬±0.012 0.155¬±0.031 0.092¬±0.014
FINETUNING 0.216¬±0.009 0.167¬±0.011 0.149¬±0.012 0.182¬±0.006 0.124¬±0.013 0.057¬±0.001
EWC 0.456¬±0.021 0.315¬±0.037 0.287¬±0.041 0.258¬±0.019 0.152¬±0.022 0.011¬±0.009
LWM 0.459¬±0.072 0.416¬±0.048 0.305¬±0.022 0.233¬±0.026 0.171¬±0.016 0.080¬±0.008
LWF 0.375¬±0.021 0.356¬±0.024 0.250¬±0.020 0.230¬±0.011 0.171¬±0.005 0.092¬±0.008
ICICLE 0.654¬±0.014 0.645¬±0.003 0.583¬±0.048 0.335¬±0.005 0.203¬±0.010 0.116¬±0.018
Table 8: Average incremental accuracy comparison for different numbers of tasks on Stanford Cars, demonstrating the
negative impact of the high number of tasks to be learned on models‚Äô performance. Despite this trend, ICICLE outperforms
the baseline methods across all task numbers.
TASK 1 TASK 2 TASK 3 TASK 4 AVG
TASK AWARE 0.514 0.717 0.725 0.698 0.663
BEFORE 0.028 0.301 0.434 0.575 0.335
AFTER 0.233 0.365 0.314 0.486 0.350
Table 9: Task-recency bias compensation influence of a sin-
gle run. Results show that our compensation method bal-
ance more the results per each task in task agnostic scenario.

--- PAGE 15 ---
TASK-AWARE ACCURACY TASK-TASK AGNOSTIC ACCURACY
After TASK 1 TASK 2 TASK 3 TASK 4 AVG TASK 1 TASK 2 TASK 3 TASK 4 AVG
TASK 1 0.806 NA NA NA 0.806 0.806 NA NA NA 0.806
TASK 2 0.740 0.759 NA NA 0.750 0.089 0.747 NA NA 0.418
TASK 3 0.622 0.736 0.759 NA 0.706 0.033 0.633 0.549 NA 0.404
TASK 4 0.514 0.717 0.725 0.698 0.663 0.028 0.484 0.378 0.505 0.349
Table 10: Results of the ICICLE method before task-recency compensation for four task learning scenario after each learning
episode.
TASK-AWARE ACCURACY
After TASK 1 TASK 2 TASK 3 TASK 4 TASK 5 TASK 6 TASK 7 TASK 8 TASK 9 TASK 10 AVG
TASK 1 0.920 NA NA NA NA NA NA NA NA NA 0.920
TASK 2 0.666 0.869 NA NA NA NA NA NA NA NA 0.767
TASK 3 0.462 0.818 0.858 NA NA NA NA NA NA NA 0.713
TASK 4 0.420 0.751 0.774 0.774 NA NA NA NA NA NA 0.680
TASK 5 0.314 0.625 0.680 0.672 0.784 NA NA NA NA NA 0.615
TASK 6 0.268 0.538 0.627 0.617 0.760 0.747 NA NA NA NA 0.593
TASK 7 0.265 0.476 0.584 0.617 0.713 0.706 0.769 NA NA NA 0.590
TASK 8 0.258 0.413 0.551 0.555 0.667 0.634 0.741 0.764 NA NA 0.573
TASK 9 0.253 0.398 0.494 0.492 0.598 0.587 0.701 0.745 0.852 NA 0.569
TASK 10 0.244 0.371 0.462 0.441 0.573 0.560 0.667 0.729 0.816 0.803 0.567
TASK-AGNOSTIC ACCURACY
TASK 1 TASK 2 TASK 3 TASK 4 TASK 5 TASK 6 TASK 7 TASK 8 TASK 9 TASK 10 AVG
TASK 1 0.920 NA NA NA NA NA NA NA NA NA 0.920
TASK 2 0.010 0.869 NA NA NA NA NA NA NA NA 0.439
TASK 3 0.0 0.060 0.854 NA NA NA NA NA NA NA 0.305
TASK 4 0.0 0.0 0.339 0.751 NA NA NA NA NA NA 0.273
TASK 5 0.0 0.0 0.030 0.323 0.746 NA NA NA NA NA 0.220
TASK 6 0.0 0.0 0.004 0.090 0.451 0.684 NA NA NA NA 0.205
TASK 7 0.0 0.0 0.0 0.020 0.193 0.432 0.712 NA NA NA 0.194
TASK 8 0.0 0.0 0.0 0.020 0.073 0.233 0.497 0.643 NA NA 0.181
TASK 9 0.0 0.0 0.0 0.0 0.035 0.138 0.338 0.435 0.676 NA 0.180
TASK 10 0.0 0.0 0.0 0.0 0.016 0.070 0.214 0.285 0.484 0.643 0.171
Table 11: Results of the ICICLE method before task-recency compensation for ten task learning scenario after each learning
episode.
TASK-AWARE ACCURACY TASK-TASK AGNOSTIC ACCURACY
Œ± 0.01 0.1 1.0 5.0 10.0 0.01 0.1 1.0 5.0 10.0
0.185 0.329 0.441 0.197 0.167 0.170 0.185 0.213 0.168 0.144
Table 12: Influence of the alpha parameter in EWC on the accuracy of ProtoPNet architecture in four task learning scenario.
TASK-AWARE ACCURACY TASK-TASK AGNOSTIC ACCURACY
Œ≥ 0.001 0.01 0.1 1.0 10.0 0.001 0.01 0.1 1.0 10.0
0.240 0.240 0.431 0.355 0.231 0.209 0.209 0.212 0.209 0.209
Table 13: Influence of the Œ≥parameter in LWM on the accuracy of ProtoPNet architecture in four task learning scenario.
TASK-AWARE ACCURACY TASK-TASK AGNOSTIC ACCURACY
Œª 0.001 0.01 0.1 1.0 10.0 0.001 0.01 0.1 1.0 10.0
0.232 0.232 0.238 0.359 0.249 0.207 0.210 0.210 0.231 0.221
Table 14: Influence of the Œªparameter in LWF on the accuracy of ProtoPNet architecture in four task learning scenario.
