# Giải thích các Mô hình Ngôn ngữ Pretrained thông qua Concept Bottlenecks

Zhen Tan
Arizona State University
ztan36@asu.edu

Yuan Bo
Zhejiang University
byuan@zju.edu.cn

Lu Cheng
University of Illinois Chicago
lucheng@uic.edu

Jundong Li
University of Virginia
jundong@virginia.edu

Song Wang
University of Virginia
sw3wv@virginia.edu

Huan Liu
Arizona State University
huanliu@asu.edu

## Tóm tắt

Các mô hình ngôn ngữ pretrained (PLMs) đã đạt được những bước tiến đáng kể trong nhiều tác vụ xử lý ngôn ngữ tự nhiên. Tuy nhiên, việc thiếu khả năng giải thích do bản chất "hộp đen" của chúng đặt ra những thách thức cho việc triển khai có trách nhiệm. Mặc dù các nghiên cứu trước đây đã cố gắng cải thiện khả năng giải thích bằng cách sử dụng, ví dụ, trọng số attention trong các lớp self-attention, những trọng số này thường thiếu sự rõ ràng, dễ đọc và trực quan. Trong nghiên cứu này, chúng tôi đề xuất một phương pháp mới để giải thích PLMs bằng cách sử dụng các concept cấp cao, có ý nghĩa và dễ hiểu đối với con người. Ví dụ, chúng tôi học concept "Food" và điều tra cách nó ảnh hưởng đến dự đoán về tình cảm của mô hình đối với một bài đánh giá nhà hàng. Chúng tôi giới thiệu C3M, kết hợp các concept được con người chú thích và được máy tạo ra để trích xuất các neuron ẩn được thiết kế để đóng gói các concept có ý nghĩa về mặt ngữ nghĩa và cụ thể cho tác vụ. Thông qua các đánh giá thực nghiệm trên các tập dữ liệu thế giới thực, chúng tôi chứng minh rằng phương pháp của chúng tôi cung cấp những hiểu biết sâu sắc có giá trị để giải thích hành vi PLM, giúp chẩn đoán các lỗi của mô hình và tăng cường tính robustness của mô hình giữa các nhãn concept nhiễu.

## 1 Giới thiệu

Mặc dù các Mô hình Ngôn ngữ Pretrained (PLMs) như BERT (Devlin et al., 2018) đã đạt được thành công đáng kể trong nhiều tác vụ NLP (Zhu et al., 2020; Liu and Lapata, 2019), chúng thường được coi là các hộp đen, đặt ra những trọng ngại đáng kể cho việc triển khai có trách nhiệm trong các tình huống thế giới thực, đặc biệt trong các lĩnh vực quan trọng như chăm sóc sức khỏe (Koh et al., 2020). Do đó, việc cho phép khả năng giải thích của PLMs là rất quan trọng để đạt được AI có trách nhiệm xã hội (Cheng et al., 2021). Cho đến nay, nhiều nghiên cứu hiện tại (Belinkov and Glass, 2019; Madsen et al., 2022) tận dụng trọng số attention được trích xuất từ các lớp self-attention để cung cấp tầm quan trọng ở cấp độ token hoặc cụm từ. Những giải thích cấp thấp này được phát hiện là không trung thực (Yin and Neubig, 2022) và thiếu khả năng đọc và trực quan (Losch et al., 2019), dẫn đến những giải thích không ổn định hoặc thậm chí không hợp lý.

[Hình 1: Minh họa CBE-PLMs. Thông qua PLMs, các văn bản gốc x được ánh xạ đầu tiên vào một lớp trung gian gồm một tập hợp các concept có thể hiểu được bởi con người c, sau đó được sử dụng để dự đoán nhãn mục tiêu y.]

Để giải quyết những hạn chế này, chúng tôi tìm cách giải thích thông qua các concept có thể hiểu được bởi con người sử dụng các đặc trưng trừu tượng hơn (ví dụ, các khái niệm chung) thay vì các đặc trưng đầu vào thô ở cấp độ token (Zarlenga et al., 2022; Liao and Vaughan, 2023). Nền tảng của công trình này là các Concept Bottleneck Models (CBMs) (Koh et al., 2020) giải thích các mô hình sâu (ví dụ, ResNet (He et al., 2016)) cho các tác vụ phân loại hình ảnh sử dụng các concept cấp cao (ví dụ, hình dạng). Đối với các tác vụ NLP như phân tích tình cảm, các concept có thể là Food, Ambiance và Service như được hiển thị trong Hình 1, trong đó mỗi concept tương ứng với một neuron trong lớp concept bottleneck. Lớp quyết định cuối cùng sau đó là một hàm tuyến tính của những concept này. Việc sử dụng các concept cải thiện đáng kể khả năng đọc và trực quan của các giải thích so với các đặc trưng cấp thấp như "lobster".

Chúng tôi đề xuất nghiên cứu Concept-Bottleneck-Enabled Pretrained Language Models (CBE-PLMs). Có ba thách thức chính: Thứ nhất, CBMs không thể được áp dụng trực tiếp vì PLMs được pre-train và fine-tune trên các corpus riêng biệt trong khi CBMs hoạt động trên cùng các tác vụ phân loại hình ảnh end-to-end trong quá trình training và testing. Do đó, các corpus được sử dụng để pre-train PLMs có thể chứa các tương quan text-concept hữu ích không được thấy trong tác vụ downstream. Cần có một cuộc điều tra về khả năng thích ứng của CBMs với CBE-PLMs. Thứ hai, phần lớn các CBMs hiện tại (Koh et al., 2020; Zarlenga et al., 2022) yêu cầu các concept được con người chú thích. Điều này có thể thách thức đối với ngôn ngữ tự nhiên vì người chú thích có thể cần đọc qua toàn bộ văn bản để hiểu ngữ cảnh và gắn nhãn một concept (Németh et al., 2020). Điều này hạn chế việc sử dụng thực tế và khả năng mở rộng của CBE-PLMs. Thứ ba, nhiều nghiên cứu đã xác định sự đánh đổi giữa khả năng giải thích và độ chính xác tác vụ khi sử dụng CBMs vì các concept được xác định trước có thể bỏ sót thông tin quan trọng cho dự đoán tác vụ mục tiêu (Zarlenga et al., 2022). Do đó, việc cải thiện cả khả năng giải thích và hiệu suất tác vụ để đạt được sự đánh đổi tối ưu giữa interpretability-utility là rất quan trọng.

Để giải quyết thách thức đầu tiên, chúng tôi điều chỉnh các chiến lược training tiêu chuẩn trong CBMs (Koh et al., 2020) để học CBE-PLMs và tiến hành phân tích toàn diện để xác định cách tốt nhất để điều chỉnh CBMs để giải thích PLMs. Đối với thách thức thứ hai về khám phá và gắn nhãn concept, chúng tôi đề xuất tận dụng Large Language Models (LLMs) được huấn luyện trên các corpus được tạo ra bởi con người rộng lớn và các phản hồi, chẳng hạn như ChatGPT (OpenAI, 2023), để xác định các concept mới trong văn bản và tạo pseudo-labels (thông qua prompting) cho các concept chưa được gắn nhãn. Các nghiên cứu gần đây (Bommasani et al., 2022; OpenAI, 2023) cho thấy những LLMs này đóng gói một lượng đáng kể kiến thức thường thức của con người. Bằng cách mở rộng tập hợp nhỏ các concept được con người chỉ định với các concept được máy tạo ra, chúng tôi tăng tính đa dạng của concept và thông tin hữu ích cho dự đoán. Ngoài ra, các pseudo-labels được tạo ra cung cấp cho chúng tôi một tập lớn các instances với nhãn concept nhiễu, bổ sung cho tập nhỏ các instances với nhãn sạch. Để tiếp tục cải thiện sự đánh đổi interpretability-utility (thách thức thứ ba), chúng tôi đề xuất học từ các nhãn concept nhiễu và kết hợp cơ chế MixUp cấp concept (Zhang et al., 2017) cho phép CBE-PLMs học cộng tác từ cả tập concept nhiễu và sạch. Chúng tôi đặt tên framework của mình để huấn luyện CBE-PLMs là ChatGPT-guided Concept augmentation with Concept-level Mixup (C3M). Tóm lại, các đóng góp của chúng tôi bao gồm:

• Chúng tôi cung cấp cuộc điều tra toàn diện đầu tiên về các chiến lược training tiêu chuẩn của CBMs để giải thích PLMs và benchmark CBE-PLMs.

• Chúng tôi đề xuất C3M, tận dụng LLMs và MixUp để giúp PLMs học từ các concept được con người chú thích và được máy tạo ra. C3M giải phóng CBMs khỏi các concept được xác định trước và tăng cường sự đánh đổi interpretability-utility.

• Chúng tôi chứng minh tính hiệu quả và tính robustness của test-time concept intervention cho các CBE-PLMs đã học được cho các tác vụ phân loại văn bản thông thường.

## 2 Nghiên cứu liên quan

### 2.1 Giải thích các Mô hình Ngôn ngữ Pretrained

PLMs như Word2Vec (Mikolov et al., 2013), BERT (Devlin et al., 2018), và loạt GPT gần đây hơn (Radford et al., 2019; Brown et al., 2020; OpenAI, 2023) đã chứng minh hiệu suất ấn tượng trong nhiều tác vụ NLP. Tuy nhiên, bản chất mờ ám của chúng đặt ra thách thức trong việc hiểu cách PLMs hoạt động bên trong (Diao et al., 2022). Để cải thiện khả năng giải thích và tính minh bạch của PLMs, các nhà nghiên cứu đã khám phá các phương pháp khác nhau, như trực quan hóa trọng số attention (Galassi et al., 2020), probing feature representations (Mishra et al., 2017; Lundberg and Lee, 2017; Bills et al., 2023), và sử dụng counterfactuals (Wu et al., 2021; Ross et al., 2021), trong số những cái khác, để cung cấp giải thích ở cấp độ token địa phương, cấp độ instance, hoặc cấp độ neuron. Tuy nhiên, những phương pháp này thường thiếu tính trung thực và trực quan, và có khả năng đọc kém, điều này làm suy yếu tính đáng tin cậy của chúng (Madsen et al., 2022).

Gần đây, các nhà nghiên cứu đã chuyển sang các giải thích concept cấp độ toàn cục mà con người có thể hiểu một cách tự nhiên. Mặc dù mức độ giải thích này đã được khám phá ít hơn trong NLP so với computer vision (Goyal et al., 2019; Kim et al., 2018; Mu and Andreas, 2020), nó đã thu hút sự chú ý. Ví dụ, một nghiên cứu (Vig et al., 2020) điều tra gender classification bias bằng cách kiểm tra mối liên kết của các từ nghề nghiệp như 'nurse' với giới tính. Ngoài ra, CBMs (Koh et al., 2020; Zarlenga et al., 2022) đã nổi lên như các framework mới để đạt được khả năng giải thích cấp concept trong các hệ thống phân loại hình ảnh nhẹ. CBMs thường bao gồm một lớp đứng trước classifier fully connected cuối cùng, trong đó mỗi neuron tương ứng với một concept có thể được con người giải thích. CBMs cũng cho thấy lợi thế trong việc cải thiện độ chính xác thông qua sự can thiệp của con người trong quá trình testing. Tuy nhiên, việc áp dụng CBMs để giải thích PLMs quy mô lớn hơn vẫn chưa được khám phá đủ.

Việc triển khai CBMs đòi hỏi sự tham gia của con người trong việc xác định tập concept và chú thích các nhãn concept. Những yêu cầu như vậy là thách thức đối với ngôn ngữ tự nhiên vì con người có thể cần đọc qua toàn bộ văn bản để hiểu ngữ cảnh và gắn nhãn một concept (Németh et al., 2020).

### 2.2 Học từ Nhãn Nhiễu

Giải quyết dữ liệu được gắn nhãn không chính xác hoặc phân loại sai trong các tình huống thế giới thực là mục tiêu của việc học từ nhãn nhiễu, với các kỹ thuật bao gồm ước tính ma trận chuyển đổi nhiễu (Liu et al., 2022), tối thiểu hóa rủi ro robust (Englesson and Azizpour, 2021), và nhiều hơn nữa. Gần đây, khả năng chống chịu của các phương pháp học bán giám sát như MixMatch (Berthelot et al., 2019) và FixMatch (Sohn et al., 2020) đối với nhiễu nhãn đã được khám phá bằng cách sử dụng pseudo-labels cho dữ liệu chưa được gắn nhãn. Được truyền cảm hứng từ họ, chúng tôi đề xuất sử dụng một LLM (ChatGPT) như một fixed-label guesser, tạo ra các nhãn concept trung gian nhiễu để có thể dự đoán các nhãn tác vụ.

Đáng chú ý, CBMs chuyên về giải thích và khả năng tương tác của các mô hình sâu cho các tác vụ phân loại chung. Trong khi Multi-Aspect Sentiment Analysis (Zhang et al., 2022) (MASA) chia sẻ mục tiêu tương tự khi sử dụng các aspect như concept, nó khác biệt vì các concept không bị giới hạn trong các đặc trưng aspect tinh tế và có thể là các ý tưởng trừu tượng hoặc các khái niệm rộng hơn xuyên suốt toàn bộ ngữ cảnh. Các nhãn aspect trong MASA, chủ yếu được sử dụng cho độ chính xác dự đoán, không phải lúc nào cũng bắt buộc. Để tóm tắt, nghiên cứu này tiên phong trong việc khám phá toàn diện việc sử dụng các concept để giải thích PLMs quy mô lớn, và cung cấp một framework robust để khai thác các tín hiệu nhiễu từ LLMs để đạt được kết quả có thể giải thích được từ các PLMs nhẹ hơn, có thể dễ dàng hiểu được bởi người dùng.

## 3 Kích hoạt Concept Bottlenecks cho Mô hình Ngôn ngữ Pretrained

### 3.1 Thiết lập Bài toán

Chúng tôi tập trung vào việc giải thích các dự đoán của PLMs được fine-tune cho các tác vụ phân loại văn bản. Cho dữ liệu D={(x(i), y(i), c(i))ni=1}, trong đó x∈Rd là đầu vào văn bản gốc, y∈R là nhãn mục tiêu cần dự đoán, và c∈Rk là một vector của k concept từ tập concept C với |C|=k, chúng tôi xem xét một PLM fθ được tham số hóa bởi θ mã hóa một đầu vào văn bản x∈Rd thành biểu diễn tiềm ẩn z∈Re của nó.

Chiến lược fine-tuning vanilla, được định nghĩa cụ thể trong Phụ lục A, có thể được trừu tượng hóa như x→z→y.

**Concept-Bottleneck-Enabled Pretrained Language Models.** Các concept bottlenecks gốc trong CBMs (Koh et al., 2020) đến từ việc thay đổi kích thước một trong các lớp trong CNN encoder để khớp với số lượng concept. Tuy nhiên, vì các PLM encoders thường cung cấp biểu diễn văn bản với số chiều cao hơn nhiều so với số lượng concept, việc giảm trực tiếp các neuron trong lớp sẽ ảnh hưởng đáng kể đến chất lượng của biểu diễn văn bản đã học. Để giải quyết vấn đề này, chúng tôi thay vào đó thêm một lớp tuyến tính với sigmoid activation, được ký hiệu là pψ, chiếu biểu diễn tiềm ẩn đã học z∈Re vào không gian concept c∈Rk. Quá trình này có thể được biểu diễn như x→z→c→y.

Lưu ý rằng, không giống như các nghiên cứu trước đây cho phân loại hình ảnh, mỗi concept ở đây không cần phải là nhị phân (tức là, có mặt hay không). Chúng tôi cho phép các concept đa lớp, ví dụ, concept "Food" trong một đánh giá nhà hàng có thể là positive, negative, hoặc unknown. Chúng tôi gọi PLM và projector (fθ, pψ) cùng nhau là concept encoder và mô hình hoàn chỉnh (fθ, pψ, gϕ) là Concept-Bottleneck-Enabled Pretrained Language Models (CBE-PLMs).

Trong quá trình training, CBE-PLMs tìm cách đạt được hai mục tiêu: (1) căn chỉnh dự đoán concept ĉ=pψ(fθ(x)) với nhãn concept ground-truth c của x và (2) căn chỉnh dự đoán nhãn ŷ=gϕ(pψ(fθ(x))) với nhãn tác vụ ground-truth y. Chúng tôi do đó điều chỉnh ba chiến lược thông thường, independent training, sequential training, và joint training, được đề xuất trong (Koh et al., 2020) để học CBE-PLM. Các công thức chi tiết của chúng được đưa ra trong Phụ lục A.

### 3.2 Benchmarking CBE-PLMs

Chúng tôi đề xuất benchmark hiệu suất của vanilla fine-tuning và ba chiến lược training cho CBE-PLMs sử dụng hai tập dữ liệu phân loại văn bản: CEBaB (Abraham et al., 2022) và IMDB (Maas et al., 2011). Cả hai tập dữ liệu đều chứa các concept được con người gắn nhãn. Chúng tôi xem xét bốn PLMs điển hình theo Abraham et al. (2022). Mô tả về các PLM backbones, tập dữ liệu, và nhãn concept được trình bày chi tiết trong Phần 5.1, Phần 5.2, và Phụ lục G. Chúng tôi xem xét điểm tác vụ mục tiêu và điểm dự đoán concept làm các metric đánh giá cho utility và interpretability, tương ứng.

**CBM cho CBE-PLMs.** Trong thí nghiệm này, chúng tôi nhằm xác định chiến lược training tối ưu cho CBE-PLMs. Kết quả được mô tả trong Hình 2 xác nhận rằng standard-PLMs thường mang lại điểm tác vụ cao nhất, chứng minh rằng việc triển khai concept bottleneck thực sự có thể ảnh hưởng tiêu cực đến hiệu suất tác vụ mục tiêu. Tuy nhiên, không xem xét các nhãn concept, standard-PLMs thiếu khả năng giải thích. Ngược lại, CBE-PLMs được huấn luyện joint thể hiện điểm tác vụ cao hơn và điểm dự đoán concept vượt trội so với các đối tác của chúng. Sự khác biệt này từ CBMs trong lĩnh vực hình ảnh, nơi cả ba chiến lược đều hiển thị hiệu suất tương tự (Koh et al., 2020), là đáng chú ý. Chúng tôi quy cho điều này cho việc pretraining rộng rãi của PLMs trên nhiều corpus được tạo ra bởi con người và số lượng tham số lớn hơn so với các vision encoders được nghiên cứu như ResNets (He et al., 2016). Không giống như independent hoặc sequential training nơi PLM encoder được cố định sau khi training trên các nhãn concept, joint training cho phép PLMs sử dụng khả năng của chúng để học concept và nhãn mục tiêu cùng nhau, làm cho các concept activations đã học từ lớp bottleneck căn chỉnh tốt hơn với các nhãn tác vụ. Với lợi thế này của joint training, chúng tôi áp dụng nó như chiến lược mặc định để huấn luyện CBE-PLMs trong các phần tiếp theo.

[Hình 2: Minh họa sự đánh đổi interpretability-accuracy sử dụng các backbone khác nhau. Phía trên bên phải cho thấy sự đánh đổi thuận lợi hơn, tức là, Pareto front interpretability-utility tốt hơn. Chúng tôi cũng hiển thị khoảng tin cậy cho cả hai chiều.]

Trong khi những phát hiện ban đầu từ việc áp dụng vanilla CBM (Koh et al., 2020) để giải thích PLMs có vẻ khuyến khích, chúng yêu cầu các concept được con người chú thích trong quá trình training. Điều này chứng minh là không thực tế trong các tình huống thế giới thực do số lượng lớn các concept tiềm năng và quá trình chú thích tốn thời gian (Németh et al., 2020). Thường chỉ có một số lượng hạn chế văn bản đi kèm với các concept được gắn nhãn thủ công. Hơn nữa, khi con người liên tục thu thập các concept mới, việc framework training tự động khám phá và kết hợp các concept mới là mong muốn. Do đó, chúng tôi nhằm thiết kế một framework chung để huấn luyện CBE-PLMs.

## 4 C3M: Một Framework Chung để Học CBE-PLMs

Chúng tôi định nghĩa các phần dữ liệu sau đây theo các tình huống thế giới thực. Chúng tôi gọi một tập dữ liệu với các concept được con người chú thích là source concept dataset, ký hiệu là Ds={(x(i), y(i), c(i)s)nsi=1}, trong đó ns biểu thị kích thước và cs∈Rks là một vector của ks concept từ tập concept nguồn được xác định trước Cs. Chúng tôi cũng xem xét một tập dữ liệu khác không có nhãn concept, được gọi là unlabeled concept dataset, ký hiệu là Du={(x(i), y(i))nui=1}. Tập dữ liệu hoàn chỉnh sau đó là sự kết hợp của hai tập dữ liệu này: D={Ds,Du}. ns và ks thường nhỏ, hạn chế hiệu quả của CBE-PLMs. Cụ thể, ns nhỏ dẫn đến các nhãn concept thưa thớt trong D, và vanilla CBM không thể được huấn luyện trên các tập dữ liệu với các concept chưa được gắn nhãn Du. Ngoài ra, ks nhỏ cho thấy rằng chúng ta có thể không có đủ thông tin cho dự đoán mô hình.

Để giải quyết những hạn chế này, chúng tôi đề xuất ChatGPT-guided Concept augmentation with Concept-level Mixup (C3M), một framework mới để huấn luyện CBE-PLMs hiệu quả. Như được minh họa trong Hình 3, ở mức độ cao, chúng tôi mở rộng tập concept Cs và chú thích pseudo concept labels cho unlabeled concept dataset sử dụng ChatGPT. Vì những pseudo labels này là nhiễu, chúng tôi đề xuất một concept-level MixUp mới để huấn luyện CBE-PLMs hiệu quả trên tập dữ liệu được mở rộng với các nhãn concept nhiễu.

### 4.1 ChatGPT-guided Concept Augmentation

Trong phần này, chúng tôi chi tiết cách tận dụng ChatGPT (GPT4) để tự động (1) mở rộng tập concept, và (2) chú thích các nhãn concept bị thiếu.

#### 4.1.1 Concept Set Augmentation

Mục tiêu của concept set augmentation là tự động tạo ra các concept chất lượng cao sử dụng các concept được con người chỉ định Cs làm tham chiếu. Những concept được tạo ra này nên có ý nghĩa về mặt ngữ nghĩa và hữu ích cho dự đoán tác vụ mục tiêu. Được truyền cảm hứng từ LF-CBM (Oikarinen et al., 2023), chúng tôi truy vấn ChatGPT với các prompt thích hợp để tạo ra các concept bổ sung. Các prompt của chúng tôi được thiết kế sử dụng "in-context learning" (Brown et al., 2020; Min et al., 2022; Xie et al., 2022), và bao gồm các ví dụ từ chú thích của con người. Dưới đây là một ví dụ về prompt ChatGPT được thiết kế cho tác vụ phân loại tình cảm sử dụng tập dữ liệu IMDB (Maas et al., 2011):

Ngoài {Acting, Storyline, Emotional Arousal, Cinematography}, những đặc trưng quan trọng bổ sung nào để đánh giá xem một {movie} có tốt hay không?

Dấu ngoặc đơn đại diện cho các trường có thể được tùy chỉnh cho các tác vụ khác nhau. Các concept Acting, Storyline, Emotional Arousal, và Cinematography là từ tập concept nguồn Cs với các nhãn được chú thích thủ công theo quy trình trong Phụ lục B. Khác với LF-CBM chỉ tạo ra concept dựa vào GPT3 (Brown et al., 2020), chúng tôi tiếp tục bao gồm một tập nhỏ các concept được con người chỉ định trong prompt để cải thiện chất lượng của các concept được tạo ra. Thông tin bổ sung này có thể giúp lọc hiệu quả các đầu ra không mong muốn mà không cần các thao tác bổ sung (ví dụ, xóa). Các concept hiếm thấy được loại bỏ sử dụng một ngưỡng được xác định trước và các concept được tạo ra còn lại được gọi là tập concept được mở rộng Ca với kích thước ka. Kết quả được đưa ra trong Bảng 5 trong Phụ lục G.

#### 4.1.2 Noisy Concept Label Annotation

Bước tiếp theo là tự động chú thích các concept chưa được gắn nhãn sử dụng các nhãn nhiễu. Chúng tôi một lần nữa tận dụng sức mạnh của ChatGPT đã được chứng minh là đóng gói một lượng đáng kể kiến thức thường thức của con người (Bommasani et al., 2022; OpenAI, 2023; Singh et al., 2023) và cho thấy hiệu suất mạnh mẽ cho một số tác vụ chú thích văn bản (Gilardi et al., 2023). Như chúng tôi cũng sẽ chỉ ra ở đây, LLMs rất thành thạo một cách đáng ngạc nhiên trong việc xác định các concept ngôn ngữ khi được prompt phù hợp. Sử dụng cùng ví dụ về đánh giá phim, prompt cho bước này được thiết kế như sau:

a. Theo đánh giá "{text 1}", "{concept 1}" của bộ phim là "positive".
b. Theo đánh giá "{text 2}", "{concept 2}" của bộ phim là "negative".
c. Theo đánh giá "{text 3}", "{concept 3}" của bộ phim là "unknown".
d. Theo đánh giá "{texti}", "{concept i}" của bộ phim như thế nào? Vui lòng trả lời với một tùy chọn trong "positive, negative, hoặc unknown".

Theo chiến lược "in-context learning" tương tự được mô tả trong Phần 4.1.1, Prompts a-c là ba ví dụ được con người chú thích được chọn ngẫu nhiên để đại diện cho các nhãn concept positive, negative, và unknown, tương ứng. Prompt d là instance truy vấn. Mục tiêu là có được các nhãn nhiễu cho bất kỳ {text i} và {concept i} nào đã cho. Có ba loại chú thích concept nhiễu:

1. Nhãn nhiễu cho các concept được con người chỉ định trong Ds. Tập dữ liệu kết quả D̃s được sử dụng để xác thực chất lượng của các nhãn được tạo ra bởi ChatGPT chỉ (Xem Bảng 4 trong Phụ lục F).

2. Nhãn nhiễu cho các concept được ChatGPT tạo ra trong Ds. Tập concept được mở rộng được ký hiệu là csa = (cs||ca)∈Rks+ka, trong đó || đề cập đến toán tử nối và ca∈Rka đại diện cho các concept được tạo ra. Ví dụ, chúng tôi xác định các concept quan trọng mới như Soundtrack sử dụng ChatGPT cho các đánh giá phim IMDB.

3. Nhãn nhiễu cho cả concept được con người chỉ định và được ChatGPT tạo ra trong unlabeled concept datasets Du. Tập concept được mở rộng được ký hiệu là c̃sa = (c̃s||c̃a)∈Rks+ka và c̃s∈Rks, c̃a∈Rka đại diện cho các nhãn concept được tạo ra cho các concept được con người chỉ định và được ChatGPT tạo ra, tương ứng.

Tóm lại, chúng tôi chuyển đổi tập dữ liệu gốc với các nhãn concept thưa thớt thành tập dữ liệu được mở rộng với các concept mới và nhãn nhiễu: D={Ds,Du} → D̃={D̃sa,D̃u}. Các ví dụ về hai loại truy vấn này được minh họa trong Phụ lục J.

[Hình 3: Minh họa framework C3M được đề xuất.]

### 4.2 Học từ Nhãn Concept Nhiễu

Trong khi việc huấn luyện trực tiếp CBE-PLMs trên tập dữ liệu được chuyển đổi D̃ là đơn giản, nhược điểm của phương pháp này là việc đối xử bình đẳng với các chú thích của con người và các nhãn nhiễu được ChatGPT tạo ra, có thể dẫn đến các không chính xác trong dự đoán và giải thích. Để cải thiện khả năng giải thích và độ chính xác, chúng tôi giới thiệu một phương pháp Concept-level MixUp (CM) mới. Nó ủng hộ hành vi lồi của PLMs giữa các concept được con người chú thích và được ChatGPT tạo ra, do đó tăng cường tính robustness của nó chống lại các nhãn concept nhiễu.

#### 4.2.1 Concept-level MixUp

Để sử dụng tốt hơn các nhãn concept nhiễu, CM đầu tiên nội suy tuyến tính các văn bản và nhãn concept giữa các concept được con người chú thích (D̃sa) và các concept được ChatGPT tạo ra (D̃u). Cụ thể, chúng tôi nội suy bất kỳ hai bộ ba text-concept-label (x(i), c(i), y(i)), (x(j), c(j), y(j)) cho cả biểu diễn tiềm ẩn (z(i), z(j)), concept (c(i), c(j)), và nhãn tác vụ (y(i), y(j)) sử dụng MixUp(·) được định nghĩa như sau:

λ∼Beta(α, α); λ̂= max(λ,1−λ);
z(i)=fθ(xi); z(j)=fθ(xj);
ẑ(i,j)=λ̂z(i)+ (1−λ̂)z(j);
ĉ(i,j)=λ̂c(i)+ (1−λ̂)c(j);
ŷ(i,j)=λ̂y(i)+ (1−λ̂)y(j), (1)

trong đó α là một siêu tham số cho phân phối Beta. Đáng chú ý, λ̂≥0.5 bảo tồn thứ tự của các concept được con người chú thích và được ChatGPT tạo ra để tính toán các thành phần loss riêng lẻ trong Eq. (4) một cách thích hợp. Sau đó, chúng tôi kết hợp và xáo trộn dữ liệu được con người chú thích và được ChatGPT chú thích trong tập dữ liệu được chuyển đổi D̃={D̃sa,D̃u}:

W=Shuffle(D̃) =Shuffle(D̃sa||D̃u), (2)

trong đó || chỉ ra việc nối hai phần của tập dữ liệu. Tiếp theo, chúng tôi thực hiện MixUp(·) cho instance thứ i như sau:

(ẑ(i)sa,ĉ(i)sa,ŷ(i)sa) =MixUp(D̃(i)sa,W(i)),
(ẑ(i)u,ĉ(i)u,ŷ(i)u) =MixUp(D̃(i)u,W(i)). (3)

Thông qua những bước này, chúng ta có thể tạo ra một "phiên bản trộn" cho mỗi instance trong D̃sa và D̃u, trong khi bảo tồn một phần lớn hơn của instance gốc.

#### 4.2.2 Loss Function

Hàm loss LjointMixUp để huấn luyện CBE-PLMs với tập dữ liệu MixUped được định nghĩa dưới đây:

Lsa=Ljoint(ẑ(i)sa,ĉ(i)sa,ŷ(i)sa);
Lu=Ljoint(ẑ(i)u,ĉ(i)u,ŷ(i)u);
LjointMixUp =Lsa+τLu, (4)

trong đó τ là một siêu tham số và Ljoint là joint training loss được sử dụng trong vanilla CBM được công thức hóa trong Phụ lục A. Theo cách này, chúng tôi lan truyền ngược gradients của các nhãn concept nhiễu trộn và nhãn concept vàng để cập nhật các tham số trong CBE-PLMs.

## 5 Thí nghiệm

### 5.1 Tập dữ liệu

Trong phần này, chúng tôi đưa ra mô tả chi tiết về các tập dữ liệu được thí nghiệm. Mỗi tập dữ liệu có hai thành phần: source concept dataset và unlabeled concept dataset (D={Ds,Du}). Các tập dữ liệu hiện có với các nhãn concept được con người chú thích rất hạn chế. Một source concept dataset là CEBaB (Abraham et al., 2022; Wu et al., 2022), một tập dữ liệu phân loại tình cảm thông thường cho các đánh giá nhà hàng. Du tương ứng của nó là các đánh giá nhà hàng từ Yelp Dataset1. Chúng tôi cũng tuyển chọn một tập dữ liệu khác cho các đánh giá phim. Cụ thể, chúng tôi lấy mẫu ngẫu nhiên hai phần đánh giá từ tập dữ liệu IMDB (Maas et al., 2011) để đại diện cho Ds và Du, tương ứng. Theo một nghiên cứu NLP trước đây (Cai et al., 2021), chúng tôi chú thích thủ công các nhãn concept cho Ds trong các đánh giá phim. Chi tiết chú thích được bao gồm trong Phụ lục B. Để thuận tiện, chúng tôi vẫn gọi hai tập dữ liệu mới này là CEBaB và IMDB. Mỗi concept chứa ba giá trị, tức là, Negative, Positive, và Unknown. Như được mô tả trong Phần 4.1, mỗi tập dữ liệu D sau đó được chuyển đổi thành D̃={D̃sa,D̃u}. Thống kê cơ bản của các tập dữ liệu được chuyển đổi và các concept được con người chú thích của chúng được đưa ra trong Bảng 3 trong Phụ lục E và Bảng 4 trong Phụ lục F, tương ứng. Lưu ý rằng cột cuối cùng trong Bảng 4 cho thấy độ chính xác của các concept được ChatGPT gắn nhãn trong Ds, như được mô tả trong Phần 4.1. Bảng 5 trong Phụ lục G cung cấp thống kê về các concept được mở rộng. Cả dữ liệu được con người chú thích và được ChatGPT tạo ra, cùng với việc triển khai framework đều được phát hành2.

### 5.2 PLM Backbones

Chúng tôi thí nghiệm với cùng các PLM backbones như trong bài báo CEBaB (Abraham et al., 2022): GPT2 (Radford et al., 2019), BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), và BiLSTM (Hochreiter and Schmidhuber, 1997) với CBOW (Mikolov et al., 2013). Để có hiệu suất tốt hơn, chúng tôi thu được các biểu diễn của các văn bản đầu vào bằng cách pooling embedding của tất cả các token. Điểm được báo cáo là trung bình của sáu lần chạy độc lập, mỗi lần mất 5 đến 40 phút. Chi tiết triển khai và giá trị tham số được bao gồm trong Phụ lục C và Bảng 2 trong Phụ lục D.

### 5.3 Độ chính xác Tác vụ vs Khả năng giải thích

Bảng 1 trình bày kết quả cho hai tập dữ liệu gốc (D) và các phiên bản được chuyển đổi của chúng (D̃). Chúng tôi có những quan sát sau:

**CBE-PLMs cung cấp khả năng giải thích và hiệu suất dự đoán tác vụ cạnh tranh.** So với standard PLMs (được huấn luyện chỉ với nhãn tác vụ), CBE-PLMs cung cấp khả năng giải thích cấp concept với chỉ một sự giảm nhỏ trong dự đoán tác vụ. Thú vị là, một PLM nhỏ hơn, tức là LSTM với CBOW embeddings, đạt được độ chính xác tác vụ được cải thiện khi học từ các nhãn concept. Điều này cho thấy rằng sự đánh đổi accuracy-interpretability trong concept learning không phải là cần thiết, trái ngược với quan điểm phổ biến. Các concept có thể giúp hướng dẫn PLMs được huấn luyện trên các corpus nhỏ hơn với ít tham số hơn hướng tới hiệu suất dự đoán tốt hơn.

**Các nhãn concept nhiễu có thể tạo điều kiện cho việc huấn luyện CBE-PLMs trên các tập dữ liệu nhỏ.** Kích thước cực kỳ hạn chế của source concept dataset IMDB (được đặt cố ý là 100) mang lại điểm test thấp một cách không ngạc nhiên. Việc chuyển đổi D thành D̃ sử dụng ChatGPT cho các concept instances được gắn nhãn nhiễu dẫn đến những cải thiện đáng kể trong cả dự đoán concept và tác vụ cho CBE-PLMs-CM.

**Việc học không phê phán từ các nhãn concept nhiễu có thể làm suy giảm hiệu suất.** Kết quả cho CEBaB trong Bảng 1 chứng minh rằng, việc học từ tập dữ liệu được chuyển đổi D̃ trực tiếp dẫn đến hiệu suất kém hơn cho CBE-PLMs. Không giống như IMDB, source concept dataset trong CEBaB chứa đủ training instances, do đó, việc buộc CBE-PLMs học từ các nhãn concept nhiễu sẽ dẫn dắt mô hình một cách không mong muốn, làm trầm trọng thêm cả hiệu suất dự đoán concept và tác vụ.

[Bảng 1: So sánh độ chính xác tác vụ và khả năng giải thích sử dụng tập dữ liệu CEBaB và IMDB. Metrics cho cả nhãn tác vụ và concept được viết là Accuracy/Macro F1. Điểm được báo cáo tính bằng %. Điểm in đậm cho thấy CBE-PLM dưới thiết lập hiện tại vượt trội hơn đối tác standard PLM của nó. CM biểu thị Concept-level MixUp.]

**CBE-PLMs-CM được huấn luyện thông qua framework C3M được đề xuất liên tục mang lại sự đánh đổi interpretability-utility vượt trội.** Bằng cách khuyến khích CBE-PLMs nội suy tuyến tính giữa các ví dụ với các concept được gắn nhãn vàng và những ví dụ với các concept được ChatGPT tạo ra, mô hình có thể trích xuất kiến thức ngữ nghĩa hữu ích trong khi trở nên robust với các nhãn concept nhiễu. Kết quả là đầy hứa hẹn: Chúng tôi đạt được dự đoán cấp concept tốt nhất (thước đo khả năng giải thích) mà không hy sinh hiệu suất dự đoán tác vụ, và trong một số trường hợp, CBE-PLMs được huấn luyện thông qua C3M thậm chí có thể vượt trội hơn các đối tác standard PLM của chúng.

### 5.4 Dự đoán Có thể giải thích

Một lợi thế độc đáo của CBMs là các quy tắc quyết định của nó có thể được giải thích như một tổ hợp tuyến tính của các biến có thể hiểu được (Koh et al., 2020). Kế thừa sức mạnh này, CBE-PLMs được đề xuất của chúng tôi có thể cung cấp các giải thích cấp concept trực quan cho các dự đoán bằng cách đánh giá các activations của mỗi concept. Chúng tôi đo lường đóng góp concept sử dụng tích của activation và trọng số tương ứng trong linear label predictor gϕ (Oikarinen et al., 2023). Các concept với activation âm được chỉ định là "Neg Concept". Chúng tôi làm nổi bật các concept đóng góp nhiều nhất trong các trực quan hóa của chúng tôi. Kết quả trực quan hóa được chứng minh trong Hình 4 cho một ví dụ toy, trong khi các nghiên cứu trường hợp CEBaB và IMDB trong thế giới thực có thể được tìm thấy trong Phụ lục H. Những trực quan hóa này cung cấp những hiểu biết sâu sắc mới thú vị cho các ứng dụng thế giới thực. Ví dụ, các concept âm (ví dụ, Service) đóng góp nhiều hơn vào dự đoán cuối cùng của tình cảm tích cực trong Hình 4, làm cho tình cảm được dự đoán cao thứ hai (Y=4) thay vì cao nhất (Y=5). Hơn nữa, kết quả khả năng giải thích như Hình 6 trong Phụ lục H ngụ ý rằng các concept như "Food" và "Ambiance" nặng hơn trong đánh giá nhà hàng của khách hàng so với "Noise" và "Menu Variety".

[Hình 4: Minh họa dự đoán có thể giải thích cho một ví dụ toy trong phân tích tình cảm đánh giá nhà hàng.]

### 5.5 Test-time Intervention

[Hình 5: Kết quả của Test-time Intervention. "NI" biểu thị "no intervention", "RI (W/O CM)" biểu thị "random intervention on CBE-PLMs without the concept-level MixUp", "RI" biểu thị "random intervention on CBE-PLMs", và "OI" biểu thị "oracle intervention".]

Một sức mạnh khác của CBE-PLMs là chúng cho phép test-time concept intervention (được kế thừa từ CBMs), tạo điều kiện cho các tương tác sâu sắc hơn, thân thiện với người dùng. Để đánh giá sức mạnh này, chúng tôi theo Koh et al. (2020) để can thiệp vào các concept được dự đoán và điều tra tác động của những can thiệp như vậy đến độ chính xác dự đoán test-time. Các dự đoán sai concept phát sinh từ các nhãn không chính xác của ChatGPT hoặc concept activation không chính xác. Nhớ lại rằng đầu vào của task label predictor là các concept activations được dự đoán â=pϕ(fθ(x)) thay vì các concept ternary được dự đoán ĉ. Trong một concept-level intervention I, activation âj của concept thứ j với một target concept cj được đặt vào percentile thứ 5, 95, hoặc 50 của âj trên phân phối training cho cj Negative, Positive, hoặc Unknown tương ứng. Nhiều concept có thể được can thiệp bằng cách thay thế tất cả các concept activations được dự đoán liên quan và cập nhật dự đoán. Các thí nghiệm được tiến hành trên phiên bản được chuyển đổi D̃ của tập dữ liệu CEBaB. Hình 5 hiển thị kết quả cho CBE-PLMs sử dụng BERT và GPT2 làm PLM backbones (với những quan sát tương tự cho LSTM và RoBERTa). Một nghiên cứu trường hợp được minh họa thêm trong Phụ lục I. Kết quả cho thấy rằng độ chính xác tác vụ cải thiện đáng kể khi nhiều concept được sửa chữa bởi oracle. Ngoài ra, trong khi hiệu suất của CBE-PLMs giảm khi nhiều concept được can thiệp một cách không chính xác (ngẫu nhiên), concept-level MixUp được đề xuất giảm thiểu hiệu quả tác động này. Đáng chú ý, sự suy giảm hiệu suất là nhỏ khi chỉ có hai concept được can thiệp sai. Những phát hiện này nhấn mạnh những lợi thế rõ rệt của test-time intervention cho CBE-PLMs được huấn luyện thông qua C3M. Thứ nhất, các chuyên gia lĩnh vực có thể tương tác với mô hình để chỉnh sửa bất kỳ giá trị concept được dự đoán không chính xác nào. Thứ hai, trong thực tế, ngay cả các chuyên gia cũng có thể vô tình triển khai các can thiệp không chính xác. Tuy nhiên, bất chấp tính nhạy cảm này, chiến lược concept-level MixUp được đề xuất của chúng tôi kiềm chế hiệu quả sự suy giảm hiệu suất, đặc biệt khi các không chính xác chỉ ảnh hưởng đến một tập con nhỏ của can thiệp. Điều này chứng thực cho tính robustness của framework được đề xuất.

## 6 Kết luận

Phân tích của chúng tôi bắt đầu với một cuộc kiểm tra kỹ lưỡng về ba chiến lược training, xác định joint training là hiệu quả nhất. Hơn nữa, chúng tôi đề xuất framework C3M, được thiết kế để hợp lý hóa quá trình training của CBE-PLMs trong sự hiện diện của các nhãn concept không đầy đủ. Hơn nữa, chúng tôi trưng bày khả năng giải thích của các mô hình của chúng tôi trong quá trình ra quyết định và làm sáng tỏ cách tính dễ hiểu này có thể được khai thác để tăng cường độ chính xác test thông qua concept intervention.

**Triển vọng:** Nghiên cứu của chúng tôi đặt nền móng cho các nghiên cứu tương lai tập trung vào việc tăng cường tính minh bạch và robustness của PLMs. Chúng tôi thấy trước rằng CBE-PLMs có thể cho thấy khả năng chống chịu cao hơn đối với các thiên lệch dữ liệu so với standard PLMs, đã được biết là hiển thị hiệu suất thiên lệch do các tương quan giả tạo giữa các thuộc tính nhạy cảm (ví dụ, giới tính) và nhãn tác vụ (Wang and Culotta, 2021; Udomcharoenchaikit et al., 2022). Ví dụ, một PLM thiên lệch có thể suy luận sai các mẫu như người dùng nữ viết các đánh giá cực đoan hơn trong khi người dùng nam có xu hướng hướng tới những đánh giá vừa phải. CBE-PLMs, bằng cách tập trung vào các nhãn concept và chỉ dựa vào những concept này cho các phân loại, có thể giảm những thiên lệch như vậy. Nếu các concept không được liên kết với các thuộc tính nhạy cảm và mối quan hệ của chúng với nhãn tác vụ là nhất quán, CBE-PLMs có thể cung cấp sự công bằng được tăng cường.

## Hạn chế

Trong khi phương pháp của chúng tôi trình bày một bước quan trọng hướng tới các mô hình ngôn ngữ pretrained có thể giải thích hơn, một số hạn chế đáng được khám phá thêm. Thứ nhất, phương pháp của chúng tôi phụ thuộc rất nhiều vào độ chính xác của các concept được xác định trước. Bất chấp những kết quả đầy hứa hẹn, sự phụ thuộc này đặt ra vấn đề về thiên lệch tiềm ẩn có mặt trong quá trình lựa chọn concept (cho cả concept được con người chỉ định và được ChatGPT tạo ra). Nếu một concept không được định nghĩa rõ hoặc nếu các concept quan trọng bị thiếu, điều này có thể dẫn đến những giải thích không đầy đủ hoặc méo mó. Thứ hai, phương pháp được đề xuất trong bài báo này chưa được thí nghiệm trên các mô hình ngôn ngữ rất lớn, chẳng hạn như Bloom (Scao et al., 2022). Ý tưởng cốt lõi của framework này là sử dụng các mô hình ngôn ngữ lớn (LLMs) để cung cấp giải thích cho các mô hình ngôn ngữ pretrained (PLMs) tương đối nhẹ hơn. Tuy nhiên, framework được đề xuất có tính chất phổ quát và nên tương thích với bất kỳ PLMs nào. Các cuộc điều tra sử dụng PLMs lớn hơn được dành cho các nỗ lực nghiên cứu tương lai. Thứ ba, quá trình prompting các mô hình ngôn ngữ lớn để tạo ra các nhãn concept vẫn là một nghệ thuật phần nào. Trong khi chúng tôi đã đề xuất một phương pháp có hệ thống để xây dựng các prompt mong muốn, hiệu suất của mô hình vẫn có thể nhạy cảm với chất lượng và cấu trúc của những prompt này. Cuối cùng, trong khi phương pháp được đề xuất của chúng tôi cho thấy kết quả đầy hứa hẹn trong các tác vụ tiếng Anh, nó chưa được kiểm tra rộng rãi trên các ngôn ngữ khác. Điều này hạn chế khả năng áp dụng của nó trong thiết lập đa ngôn ngữ. Nghiên cứu tương lai nên mở rộng phương pháp này sang các ngôn ngữ khác và tiến hành phân tích đa ngôn ngữ. Chúng tôi hy vọng nghiên cứu tương lai sẽ xây dựng dựa trên công trình của chúng tôi để giải quyết những hạn chế này, đưa chúng ta đến gần hơn với các mô hình ngôn ngữ thực sự có thể giải thích, có trách nhiệm và có thể áp dụng phổ quát.

## Tuyên bố Đạo đức

Trong việc tiến hành nghiên cứu này, chúng tôi tuân thủ nghiêm ngặt ACL Ethics Policy. Tất cả dữ liệu được sử dụng trong công trình của chúng tôi đều có sẵn công khai hoặc được ẩn danh, đảm bảo không có thông tin nhận dạng cá nhân nào được liên quan. Công trình được trình bày trong bài báo này đóng góp đáng kể cho lĩnh vực xử lý ngôn ngữ tự nhiên và machine learning. Bằng cách cải thiện khả năng giải thích của các mô hình ngôn ngữ pre-trained, chúng tôi đang đóng góp vào việc tạo ra các hệ thống AI minh bạch và đáng tin cậy hơn. Tiến bộ này được kỳ vọng sẽ có tác động rộng rãi trên nhiều lĩnh vực ngày càng dựa vào AI, bao gồm chăm sóc sức khỏe, giáo dục, kinh doanh và tài chính, tăng cường các quy trình ra quyết định và tương tác người dùng với các hệ thống AI. Tuy nhiên, hiệu quả tăng lên của những mô hình này cũng có thể đặt ra những mối quan tâm xã hội tiềm ẩn nếu không được sử dụng có trách nhiệm. Việc lạm dụng những công nghệ NLP tiên tiến này có thể dẫn đến vi phạm quyền riêng tư, lan truyền thông tin sai lệch, hoặc khuếch đại các thiên lệch hiện có trong dữ liệu. Như với bất kỳ công nghệ mạnh mẽ nào, việc xem xét các tác động đạo đức của nó và quản lý việc triển khai của nó một cách cẩn thận để đảm bảo nó được sử dụng cho lợi ích của xã hội là cần thiết. Công trình của chúng tôi cũng nhấn mạnh nhu cầu nghiên cứu liên tục về các chiến lược giảm thiểu thiên lệch tiềm ẩn trong các hệ thống AI và bảo vệ quyền riêng tư của người dùng. Là các nhà nghiên cứu, chúng tôi cam kết làm việc hướng tới những mục tiêu này và thúc giục những người sử dụng công nghệ này tuân thủ những nguyên tắc tương tự.

## Tài liệu tham khảo

[Phần tài liệu tham khảo giữ nguyên định dạng gốc vì chứa thông tin bibliographic chuẩn]

## Phụ lục

### A Định nghĩa các Chiến lược Training

Cho một đầu vào văn bản x∈Rd, concepts c∈Rk và nhãn y của nó, các chiến lược để fine-tune text encoder fθ, projector pψ và label predictor gϕ được định nghĩa như sau:

i) Vanilla fine-tuning một PLM: Các nhãn concept bị bỏ qua, và sau đó text encoder fθ và label predictor gϕ được fine-tune như sau:
θ, ϕ= argminθ,ϕLCE(gϕ(fθ(x), y),

hoặc như sau (frozen text encoder fθ):
ϕ= argminϕLCE(gϕ(fθ(x), y),

trong đó LCE chỉ ra cross-entropy loss. Trong công trình này chúng tôi chỉ xem xét tùy chọn đầu tiên vì hiệu suất tốt hơn đáng kể.

ii) Independently training PLM với concept và task labels: Text encoder fθ, projector pψ và label predictor gϕ được huấn luyện riêng biệt với ground truth concepts labels và task labels như sau:
θ, ψ= argminθ,ψLCE(pψ(fθ(x)), c),
ϕ= argminϕLCE(gϕ(c), y).

Trong quá trình inference, label predictor sẽ sử dụng đầu ra từ projector thay vì ground-truth concepts.

iii) Sequentially training PLM với concept và task labels: Chúng tôi đầu tiên học concept encoder như chiến lược independent training ở trên, và sau đó sử dụng đầu ra của nó để huấn luyện label predictor:
ϕ= argminϕLCE(gϕ(pψ(fθ(x), y).

iv) Jointly training PLM với concept và task labels: Học concept encoder và label predictor thông qua một tổng có trọng số Ljoint của hai objectives được mô tả ở trên:
θ, ψ, ϕ = argminθ,ψ,ϕLjoint(x, c, y )
= argminθ,ψ,ϕ[LCE(gϕ(pψ(fθ(x), y)
+γLCE(pψ(fθ(x)), c)].

Đáng chú ý rằng CBE-PLMs được huấn luyện jointly nhạy cảm với loss weight γ. Chúng tôi báo cáo kết quả hiệu quả nhất ở đây, giá trị được kiểm tra cho γ được đưa ra trong Bảng 2 trong Phụ lục D.

### B Chi tiết Chú thích Concept Thủ công cho Tập dữ liệu IMDB

Chính sách chú thích của chúng tôi tuân theo một nghiên cứu trước đây (Cai et al., 2021) để chú thích tập dữ liệu NLP. Đối với tập dữ liệu IMDB, chúng tôi chú thích bốn concept (Acting, Storyline, Emotional Arousal, Cinematography) thủ công. Mặc dù các concept được con người hiểu một cách tự nhiên, hai sinh viên Thạc sĩ quen thuộc với phân tích tình cảm được chọn làm người chú thích để chú thích độc lập với công cụ chú thích được giới thiệu bởi Yang et al. (2017). Điểm F1 strict quadruple matching giữa hai người chú thích là 85.74%, cho thấy sự đồng thuận nhất quán giữa hai người chú thích (Kim and Klinger, 2018). Trong trường hợp bất đồng, một chuyên gia thứ ba sẽ được yêu cầu đưa ra quyết định cuối cùng.

### C Chi tiết Triển khai

Trong phần này, chúng tôi cung cấp chi tiết về các thiết lập triển khai của các thí nghiệm. Cụ thể, chúng tôi triển khai framework của mình với PyTorch (Paszke et al., 2017) và HuggingFace (Wolf et al., 2020) và huấn luyện framework của mình trên một GPU Nvidia A100 80GB duy nhất. Chúng tôi tuân theo một nghiên cứu trước (Abraham et al., 2022) để triển khai backbone. Tất cả các mô hình backbone có số lượng token tối đa là 512 và kích thước batch là 8. Chúng tôi sử dụng optimizer Adam để cập nhật backbone, projector, và label predictor theo Phần 3.1. Các giá trị của các siêu tham số khác (Bảng 2 trong Phụ lục D) cho mỗi loại PLM cụ thể được xác định thông qua grid search. Chúng tôi chạy tất cả các thí nghiệm trên GPU Nvidia A100 với 80GB RAM.

### D Tham số và Ký hiệu

Trong phần này, chúng tôi cung cấp các ký hiệu được sử dụng trong bài báo này cùng với mô tả của chúng để hiểu toàn diện. Chúng tôi cũng liệt kê các giá trị thí nghiệm và tối ưu của chúng, như được hiển thị trong Bảng 2.

### E Thống kê Chia dữ liệu

Thống kê và chính sách chia của các tập dữ liệu được thí nghiệm, bao gồm source concept dataset Ds, unlabeled concept dataset Du, và các phiên bản được mở rộng của chúng. Chi tiết cụ thể được trình bày trong Bảng 3.

### F Thống kê Concepts được Con người Chú thích

Thống kê về Concepts được Con người Chú thích trong cả tập dữ liệu CEBaB và IMDB. Chúng tôi cũng bao gồm độ chính xác của dự đoán concept của ChatGPT ở đây. Chi tiết cụ thể được trình bày trong Bảng 4.

### G Thống kê Concepts trong Tập dữ liệu được Chuyển đổi

Thống kê và chính sách chia của các tập dữ liệu được chuyển đổi của các tập dữ liệu được thí nghiệm được trình bày trong Bảng 5.

### H Kết quả khác về Dự đoán Có thể giải thích

Các nghiên cứu trường hợp về dự đoán có thể giải thích cho cả tập dữ liệu CEBaB và IMDB được đưa ra trong Hình 6 và Hình 7 tương ứng.

### I Một nghiên cứu trường hợp về Test-time Intervention

Chúng tôi trình bày một nghiên cứu trường hợp về Test-time Intervention sử dụng một ví dụ từ transformed unlabeled concept data D̃u của tập dữ liệu CEBaB, như được hiển thị trong Hình 8. Hàng đầu tiên hiển thị các nhãn concept mục tiêu được tạo ra bởi ChatGPT. Hàng thứ hai cho thấy các dự đoán từ mô hình CBE-PLM được huấn luyện, dự đoán sai hai concept ("Waiting time" và "Waiting area"). Hàng thứ ba chứng minh test-time intervention sử dụng ChatGPT làm oracle, sửa chữa các nhãn tác vụ được dự đoán. Cuối cùng, hàng thứ tư triển khai test-time intervention với human oracle, chỉnh sửa concept mà ChatGPT ban đầu gắn nhãn sai.

### J Ví dụ về Truy vấn ChatGPT

Trong bài báo này, chúng tôi truy vấn ChatGPT cho 1) mở rộng tập concept, và 2) chú thích các nhãn concept bị thiếu. Lưu ý rằng trong thực tế, chúng tôi truy vấn ChatGPT (GPT4) thông qua OpenAI API. Ở đây chúng tôi chứng minh các ví dụ từ ChatGPT (GPT4) GUI để minh họa tốt hơn. Các minh họa được đưa ra trong Hình 9 và Hình 10.

[Các bảng và hình được đề cập sẽ được giữ nguyên format gốc như trong văn bản tiếng Anh]
