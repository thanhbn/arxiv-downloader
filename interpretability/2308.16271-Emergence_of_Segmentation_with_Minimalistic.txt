# 2308.16271.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/interpretability/2308.16271.pdf
# File size: 12280741 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Emergence of Segmentation with Minimalistic
White-Box Transformers
Yaodong Yu1,⋆Tianzhe Chu1,2,⋆Shengbang Tong1,3Ziyang Wu1Druv Pai1
Sam Buchanan4Yi Ma1,5
1UC Berkeley2ShanghaiTech3NYU4TTIC5HKU
https://ma-lab-berkeley.github.io/CRATE
Figure 1: Self-attention maps from a supervised cratewith 8×8patchestrained using classification. The
cratearchitectureautomaticallylearnstoperformobjectsegmentationwithoutacomplexself-supervised
training recipe or any fine-tuning with segmentation-related annotations. For each image pair, we visualize the
original image on the left and the self-attention map of the image on the right.
Abstract
Transformer-likemodelsforvisiontaskshaverecentlyproveneffectiveforawide
rangeofdownstreamapplicationssuchassegmentationanddetection. Previous
works have shown that segmentation properties emerge in vision transformers
(ViTs)trainedusingself-supervisedmethodssuchasDINO,butnotinthosetrained
onsupervisedclassificationtasks. Inthisstudy,weprobewhethersegmentation
emergesintransformer-basedmodels solelyasaresultofintricateself-supervised
learningmechanisms,orifthesameemergencecanbeachievedundermuchbroader
conditions through proper design of the model architecture. Through extensive
experimentalresults,wedemonstratethatwhenemployingawhite-boxtransformer-
like architecture known as crate, whose design explicitly models and pursues
low-dimensional structures in the data distribution, segmentation properties, at
both the whole and parts levels, already emerge with a minimalistic supervised
trainingrecipe. Layer-wisefiner-grainedanalysisrevealsthattheemergentprop-
ertiesstronglycorroboratethedesignedmathematicalfunctionsofthewhite-box
network. Ourresultssuggestapathtodesignwhite-boxfoundationmodelsthatare
simultaneouslyhighly performantandmathematically fullyinterpretable. Codeis
athttps://github.com/Ma-Lab-Berkeley/CRATE .
1. Introduction
Representation learning in an intelligent system aims to transform high-dimensional, multi-modal
sensory data of the world—images, language, speech—into a compact form that preserves its es-
sential low-dimensional structure, enabling efficient recognition (say, classification), grouping (say,
segmentation), and tracking [26, 31]. Classical representation learning frameworks, hand-designed
fordistinctdatamodalitiesandtasksusingmathematicalmodelsfordata[12,38,39,48,49],have
largelybeenreplacedbydeeplearning-basedapproaches,whichtrainblack-boxdeepnetworkswith
massive amounts of heterogeneous data on simple tasks, then adapt the learned representations on
⋆Equal contribution.arXiv:2308.16271v1  [cs.CV]  30 Aug 2023

--- PAGE 2 ---
Figure2: (Left)Visualizingtheself-attentionmapforaninputimageusingthe cratemodel.Theinput
tokensfor crateconsistof Nnon-overlappingimagepatchesanda [CLS]token. Weusethe cratemodel
to transform these tokens to their representations, and de-rasterize the self-attention map associated to the
[CLS]tokenandthe imagepatchtokensatthe penultimatelayertogenerateaheatmapvisualization. Details
are provided in Section 3.1. ( Right)Overview of one layer of crate architecture. Thecratemodel is a
white-box transformer-like architecture derived via unrolled optimization on the sparse rate reduction objective
(Section2). Eachlayercompressesthedistributionoftheinputtokensagainstalocalsignalmodel,andsparsifies
it in a learnable dictionary. This makes the model mathematically interpretable and highly performant [51].
downstream tasks [3, 4, 35]. This data-driven approach has led to tremendous empirical successes—
in particular, foundation models [3] have demonstrated state-of-the-art results in fundamental vision
tasks such as segmentation [22] and tracking [45]. Among vision foundation models, DINO [6,
35] showcases asurprising emergent properties phenomenon inself-supervised visiontransformers
(ViTs [11])—ViTs contain explicit semantic segmentation information even without trained with
segmentation supervision. Follow-up works have investigated how to leverage such segmenta-
tion information in DINO models and achieved state-of-the-art performance on downstream tasks,
including segmentation, co-segmentation, and detection [2, 46].
As demonstrated in Caron et al. [6], the penultimate-layer features in ViTs trained with DINO corre-
latestronglywithsaliencyinformationinthevisualinput—forexample,foreground-background
distinctions and object boundaries (similar to the visualizations shown in Figure 1)—which allows
these features to be used for image segmentation and other tasks. However, to bring about the
emergence of these segmentation properties, DINO requires a delicate blend of self-supervised
learning, knowledge distillation, and weight averaging during training. It remains unclear if every
component introduced in DINO is essential for the emergence of segmentation masks. In particular,
there is no such segmentation behavior observed in the vanilla supervised ViT models that are
trainedonclassificationtasks[6],althoughDINOemploysthesameViTarchitectureasitsbackbone.
In this paper, we question the prevailing wisdom, stemming from the successes of DINO, that a
complex self-supervised learning pipeline is necessary to obtain emergent properties in transformer-
like vision models. We contend that an equally-promising approach to promote segmentation
propertiesintransformeristo designthetransformerarchitecturewiththestructureoftheinputdatain
mind, representing a marrying of the classical approach to representation learning with the modern,
data-driven deep learning framework. We call such an approach to transformer architecture design
white-box transformer , in contrast to the black-box transformer architectures (e.g., ViTs [11]) that
currently prevail as the backbones of vision foundation models. We experiment with the white-box
transformer architecture crateproposed by Yu et al. [51], an alternative to ViTs in which each layer
is mathematically interpretable, and demonstrate through extensive experiments that:
Thewhite-boxdesignof crateleadstotheemergenceofsegmentationpropertiesinthenetwork’s
self-attention maps, solely through a minimalistic supervised training recipe —the supervised
classification training used in vanilla supervised ViTs [11].
We visualize the self-attention maps of cratetrained with this recipe in Figure 1, which share
similarqualitative(objectsegmentation)behaviorstotheonesshowninDINO[6]. Furthermore,
as to be shown in Figure 7, each attention head in the learned white-box crateseems to capture a
different semantic part of the objects of interest. This represents the first supervised vision model with
emergent segmentation properties , and establishes white-box transformers as a promising direction for
interpretable data-driven representation learning in foundation models.
Outline. Theremainderofthepaperisorganizedasfollows. InSection2,wereviewthedesignof
crate, the white-box transformer model we study in our experiments. In Section 3, we outline our
2

--- PAGE 3 ---
experimental methodologies to study segmentation in transformer-like architectures, and provide a
basic analysis which compares the segmentation in supervised crateto the vanilla supervised ViT
andDINO.InSection4,wepresentextensiveablationsandmoredetailedanalysisofthesegmentation
propertywhichutilizesthewhite-boxstructureof crate,andweobtainstrongevidencethatthe
white-box design of crateis the key to the emergent properties we observe.
Notation. We denote the (patchified) input image by X= [x1, . . . ,xN]∈RD×N, where xi∈RD×1
represents the i-th image patch and Nrepresents the total numberof image patches. xiis referred
to as atoken, and this term can be used interchangeably with image patch. We use f∈ F:RD×N→
Rd×(N+1)to denote the mapping induced by the model; it is a composition of L+ 1layers, such that
f=fL◦ ··· ◦ fℓ◦ ··· ◦ f1◦f0,where fℓ:Rd×(N+1)→Rd×(N+1),1≤ℓ≤Lrepresentsthemapping
ofthe ℓ-thlayer,and f0:X∈RD×N→Z1∈Rd×(N+1)isthepre-processinglayerthattransforms
image patches X= [x1, . . . ,xN]to tokens Z1=z1
[CLS],z1
1, . . . ,z1
N
, where z1
[CLS]denotes the “class
token”, amodel parameter eventually usedforsupervisedclassification inour training setup. Welet
Zℓ=
zℓ
[CLS],zℓ
1, . . . ,zℓ
N
∈Rd×(N+1)(1)
denote the inputtokens of the ℓthlayer fℓfor1≤ℓ≤L, so that zℓ
i∈Rdgives the representationof
theithimagepatch xibeforethe ℓthlayer,and zℓ
[CLS]∈Rdgivestherepresentationoftheclasstoken
before the ℓthlayer. We use Z=ZL+1to denote the output tokens of the last ( Lth) layer.
2. Preliminaries: White-Box Vision Transformers
In this section, we revisit the cratearchitecture ( Coding RAte reduction Transform Er)—a white-box
visiontransformerproposedinYuetal.[51]. cratehasseveraldistinguishingfeaturesrelativeto
the vision transformer (ViT) architecture [11] that underlie the emergent visual representations we
observe in our experiments. We first introduce the network architecture of cratein Section 2.1, and
then present how to learn the parameters of cratevia supervised learning in Section 2.2.
2.1. Design of crate—A White-Box Transformer Model
Representation learning via unrolling optimization. As described in Yu et al. [51], the white-box
transformer crate f:RD×N→Rd×(N+1)isdesignedtotransforminputdata X∈RD×Ndrawn
from a potentially nonlinear and multi-modal distribution to piecewise linearized and compact feature
representations Z∈Rd×(N+1). Itdoesthisbyposinga localsignalmodel forthemarginaldistribution
ofthetokens zi. Namely,itassertsthatthetokensareapproximatelysupportedonaunionofseveral,
sayK, low-dimensional subspaces, say of dimension p≪d, whose orthonormal bases are given by
U[K]= (Uk)K
k=1where each Uk∈Rd×p. With respect to this local signal model, the cratemodel is
designed to optimize the sparse rate reduction objective [51]:
max
f∈FEZ
∆R(Z|U[K])−λ∥Z∥0
= max
f∈FEZ
R(Z)−λ∥Z∥0−Rc(Z;U[K])
, (2)
where Z=f(X),thecodingrate R(Z)is(atightapproximationfor[30])theaveragenumberof
bits required to encode the tokens ziup to precision εusing a Gaussian codebook, and Rc(Z|U[K])
is an upper bound on the average number of bits required to encode the tokens’ projections onto
each subspace in the local signal model, i.e., U∗
kzi, up to precision εusing a Gaussian codebook [51].
Whenthesesubspacesaresufficientlyincoherent, theminimizersoftheobjective(2)asafunctionof
Zcorrespond to axis-aligned and incoherent subspace arrangements [52].
Hence, a network designed to optimize (2) by unrolled optimization [7, 16, 32] incrementally
transforms the distribution of Xtowards the desired canonical forms: each iteration of unrolled
optimization becomes a layer of the representation f, to wit
Zℓ+1=fℓ(Zℓ), (3)
with the overall representation fthus constructed as
f:Xf0
− − →Z1→ ··· → Zℓfℓ
− − →Zℓ+1→ ··· → ZL+1=Z. (4)
Importantly,intheunrolledoptimizationparadigm, eachlayer fℓhasitsown,untied,localsignalmodel
3

--- PAGE 4 ---
Uℓ
[K]: each layer models the distribution of input tokens Zℓ, enabling the linearization of nonlinear
structures in the input distribution Xat a global scale over the course of the application of f.
The above unrolled optimization framework admits a variety of design choices to realize the layers
fℓthat incrementally optimize (2). crateemploys a two-stage alternating minimization approach
with a strong conceptual basis [51], which we summarize here and describe in detail below:
1.First, the distribution of tokens Zℓiscompressed against the local signal model Uℓ
[K]by an
approximate gradient step on Rc(Z|Uℓ
[K])to create an intermediate representation Zℓ+1/2;
2.Second,thisintermediaterepresentationis sparselyencoded usingalearnabledictionary Dℓto
generate the next layer representation Zℓ+1.
Experimentsdemonstratethatevenaftersupervisedtraining, crateachievesitsdesigngoalsfor
representation learning articulated above [51].
Compression operator: Multi-Head Subspace Self-Attention (MSSA). Given local models Uℓ
[K],
toformtheincrementaltransformation fℓoptimizing(2)atlayer ℓ,cratefirstcompressesthetoken
setZℓagainst the subspaces by minimizing the coding rate Rc(· |Uℓ
[K]). As Yu et al. [51] show,
doingthisminimizationlocallybyperformingastepofgradientdescenton Rc(· |Uℓ
[K])leadstothe
so-called multi-head subspace self-attention ( MSSA) operation, defined as
MSSA(Z|U[K]).=p
(N+ 1)ε2[U1, . . . ,UK]
(U∗
1Z) softmax(( U∗
1Z)∗(U∗
1Z))
...
(U∗
KZ) softmax(( U∗
KZ)∗(U∗
KZ))
,(5)
and the subsequent intermediate representation
Zℓ+1/2=Zℓ−κ∇ZRc(Zℓ|U[K])≈
1−κ·p
(N+ 1)ε2
Zℓ+κ·p
(N+ 1)ε2·MSSA(Zℓ|U[K]),(6)
where κ >0is a learning rate hyperparameter. This block bears a striking resemblance to the ViT’s
multi-head self-attention block, with a crucial difference: the usual query, key, and value projection
matrices within a single head are here all identical, and determined by our local model for the
distributionoftheinputtokens. Wewilldemonstrateviacarefulablationstudiesthatthisdistinction
is crucial for the emergence of useful visual representations in crate.
Sparsification operator: Iterative Shrinkage-Thresholding Algorithm (ISTA). The remaining
termtooptimizein (2)isthedifferenceoftheglobalcodingrate R(Z)andthe ℓ0normofthetokens,
whichtogetherencouragethe representationsto beboth sparseandnon-collapsed. Yuet al.[51]
show that local minimization of this objective in a neighborhood of the intermediate representations
Zℓ+1/2is approximately achieved by a LASSO problem with respect to a sparsifying orthogonal
dictionary Dℓ. Taking an iterative step towards solving this LASSO problem gives the iterative
shrinkage-thresholding algorithm (ISTA) block [47, 51]:
Zℓ+1=fℓ(Zℓ) = ReLU( Zℓ+1/2+ηDℓ∗(Zℓ+1/2−DℓZℓ+1/2)−ηλ1).=ISTA(Zℓ+1/2|Dℓ).(7)
Here, η > 0is a step size, and λ > 0is the sparsification regularizer. The ReLU nonlinearity
appearinginthisblockarisesfromanadditionalnonnegativityconstraintontherepresentations
intheLASSOprogram,motivatedbythegoalofbetterseparatingdistinctmodesofvariabilityin
thetokendistribution[17]. TheISTAblockisreminiscentoftheMLPblockintheViT,butwitha
relocated skip connection.
The overall cratearchitecture. Combining the MSSA and the ISTA block, as above, together with
a suitable choice of hyperparameters, we arrive at the definition of a single cratelayer:
Zℓ+1/2.=Zℓ+MSSA(Zℓ|Uℓ
[K]), fℓ(Zℓ) =Zℓ+1.=ISTA(Zℓ+1/2|Dℓ). (8)
These layers are composed to obtain the representation f, as in (4). We visualize the cratearchitec-
ture in Figure 2. Full pseudocode (both mathematical and PyTorch-style) is given in Appendix A.
The forward and backward pass of crate.The above conceptual framework separates the role
offorward “optimization,” where each layer incrementally transforms its input towards a compact
4

--- PAGE 5 ---
CRATECRATECRATEViTViTViTFigure 3: Visualization of PCA components. We compute the PCA of the patch-wise representations of
eachcolumnandvisualizethefirst3componentsfortheforegroundobject. Eachcomponentismatchedtoa
different RGB channel and the background is removed by thresholding the first PCA component of the full
image. Therepresentationsof cratearebetteraligned,andwithlessspuriouscorrelations,totextureand
shape components of the input than those of ViT. See the pipeline in Appendix B.2 for more details.
and structured representation via compression and sparsification of the token representations using
the localsignal models Uℓ
[K]and sparsifyingdictionaries Dℓat eachlayer, and backward “learning,”
wherethelocalsignalmodelsandsparsifyingdictionariesarelearnedfromsupervised(asinour
experiments) or self-supervised training via back propagation to capture structures in the data.
We believe that such mathematically clear designs of crateplay a key role in the emergence of
semantically meaningful properties in the final learned models, as we will soon see.
2.2. Training cratewith Supervised Learning
Asdescribedinprevioussubsection,giventhe localsignalmodels (Uℓ
[K])L
ℓ=1andsparsifyingdictionaries
(Dℓ)L
ℓ=1,eachlayerof crateisdesignedtooptimizethesparseratereductionobjective(2). Toenable
more effective compression and sparsification, the parameters of local signal models need to be
identified. Previouswork[51]proposestolearntheparameters (Uℓ
[K],Dℓ)L
ℓ=1fromdata,specifically
in a supervised manner by solving the following classification problem:
min
W,fX
iℓCE(WzL+1
i,[CLS], yi)whereh
zL+1
i,[CLS],zL+1
i,1, . . . ,zL+1
i,Ni
=f(Xi), (9)
where (Xi, yi)is the ithtraining (image, label) pair, W∈Rd×Cmaps the [CLS]token to a vector of
logits, Cis the number of classes, and ℓCE(·,·)denotes the softmax cross-entropy loss.1
3. Measuring Emerging Properties in crate
Wenowstudytheemergentsegmentationpropertiesinsupervised cratebothqualitativelyand
quantitatively. Asdemonstratedinpreviouswork[6],segmentationwithintheViT[11]emergesonly
when applying DINO, a very specialized self-supervised learning method [6]. In particular, a vanilla
ViT trained on supervised classification does not develop the ability to perform segmentation. In contrast,
as we demonstrate both qualitatively and quantitatively in Section 3 and Section 4, segmentation
properties emerge in crateeven when using standard supervised classification training.
Ourempiricalresultsdemonstratethatself-supervisedlearning,aswellasthespecializeddesign
options in DINO [6] (e.g., momentum encoder, student and teacher networks, self-distillation, etc.)
are not necessary for the emergence of segmentation. We train all models ( crateand ViT) with
the same number of data and iterations, as well as optimizers, to ensure experiments and ablations
provide a fair comparison—precise details are provided in Appendix C.1.
1This is similar to the supervised ViT training used in Dosovitskiy et al. [11].
5

--- PAGE 6 ---
Supervised CRATESupervised ViT
Supervised CRATESupervised ViT
ViTCRATE(a)Visualization of coarse semantic segmentation.Model Train mIoU
crate-S/8 Supervised 23.9
crate-B/8 Supervised 23.6
ViT-S/8 Supervised 14.1
ViT-B/8 Supervised 19.2
ViT-S/8 DINO 27.0
ViT-B/8 DINO 27.3
(b)mIoU evaluation.
Figure 4: Coarse semantic segmentation via self-attention map. (a) We visualize the segmentation masks for
bothcrateand the supervised ViT. We select the attention head with the best segmentation performance for
crateand ViT separately. ( b) We quantitatively evaluate the coarse segmentation mask by evaluating the
mIoU score on the validation set of PASCAL VOC12 [13]. Overall, cratedemonstrates superior segmentation
performance to the supervised ViT both qualitatively (e.g., in (a), where the segmentation maps are much
cleaner and outline the desired object), and quantitatively (e.g., in (b)).
3.1. Qualitative Measurements
Visualizing self-attentionmaps. Toqualitativelymeasure theemergence phenomenon, we adopt
the attention map approach based on the [CLS]token, which has been widely used as a way to
interpretandvisualizetransformer-likearchitectures[1,6]. Indeed,weusethesamemethodologyas
[1, 6], noting that in cratethe query-key-value matrices are all the same; a more formal accounting
is deferred to Appendix B.1. The visualization results of self-attention maps are summarized in
Figure1andFigure7. Weobservethattheself-attentionmapsofthe cratemodelcorrespondto
semantic regions in the input image. Our results suggest that the cratemodel encodes a clear
semanticsegmentationofeachimageinthenetwork’sinternalrepresentations,whichissimilarto
the self-supervised method DINO [6]. In contrast, as shown in Figure 14 in the Appendices, the
vanilla ViT trained on supervised classification does not exhibit similar segmentation properties.
PCAvisualization forpatch-wiserepresentation. Following previouswork [2,35]on visualizing
thelearnedpatch-wisedeepfeaturesofimage,westudytheprincipalcomponentanalysis(PCA)on
the deep token representations of crateand ViT models. Again, we use the same methodology as
thepreviousstudies [2,35],and amorefullaccounting ofthemethodisdeferred toAppendixB.2.
WesummarizethePCAvisualizationresultsofsupervised crateinFigure3. Withoutsegmentation
supervision, crateisabletocapturetheboundaryoftheobjectintheimage. Moreover,theprincipal
componentsdemonstratefeaturealignmentbetweentokenscorrespondingtosimilarpartsofthe
object; for example, the red channel corresponds to the horse’s leg. On the other hand, the PCA
visualization of the supervised ViT model is considerably less structured. We also provide more
PCA visualization results in Figure 9.
3.2. Quantitative Measurements
Besidesqualitativelyassessingsegmentationpropertiesthroughvisualization,wealsoquantitatively
evaluate the emergent segmentation property of crateusing existing segmentation and object
detection techniques [6, 46]. Both methods apply the internal deep representations of transformers,
suchasthepreviouslydiscussedself-attentionmaps,toproducesegmentationmaskswithoutfurther
training on special annotations (e.g., object boxes, masks, etc.).
Coarsesegmentationviaself-attentionmap. AsshowninFigure1, crateexplicitlycapturesthe
object-level semanticswithclearboundaries. Toquantitativelymeasurethe qualityoftheinduced
segmentation, we utilize the raw self-attention maps discussed earlier to generate segmentation
masks. Then,weevaluatethestandardmIoU(meanintersectionoverunion)score[28]bycomparing
the generated segmentation masks against ground truth masks. This approach has been used in
previous work on evaluating the segmentation performance of the self-attention maps [6]. A more
detailedaccountingofthemethodologyisfoundinAppendixB.3. Theresultsaresummarizedin
Figure4. cratelargelyoutperformsViTbothvisuallyandintermsofmIoU,whichsuggeststhat
the internal representations in crateare much more effective for producing segmentation masks.
Object detection and fine-grained segmentation. To further validate and evaluate the rich semantic
information captured by crate, we employ MaskCut [46], a recent effective approach for object
detection and segmentation that does not require human annotations. As usual, we provide a more
detailed methodological description in Appendix B.4. This procedure allows us to extract more
6

--- PAGE 7 ---
ViTCRATE
Supervised CRATESupervised ViT
Supervised CRATESupervised ViT
Figure 5: Visualization of on COCO val2017 [27] with MaskCut. (Top Row) Supervised cratearchitecture
clearly detects the major objects in the image. ( Bottom Row ) Supervised ViT sometimes fails to detect the major
objects in the image (columns 2, 3, 4).
Detection Segmentation
Model Train AP 50AP75AP AP 50AP75AP
crate-S/8 Supervised 2.9 1.0 1.1 1.8 0.7 0.8
crate-B/8 Supervised 2.9 1.0 1.3 2.2 0.7 1.0
ViT-S/8 Supervised 0.1 0.1 0.0 0.0 0.0 0.0
ViT-B/8 Supervised 0.8 0.2 0.4 0.7 0.5 0.4
ViT-S/8 DINO 5.0 2.02.44.0 1.31.7
ViT-B/8 DINO 5.1 2.32.54.1 1.31.8
Table 1: Object detection and fine-grained segmentation via MaskCut on COCO val2017 [27] . We consider
models with different scales and evaluate the average precision measured by COCO’s official evaluation metric.
Thefirstfourmodelsarepre-trainedwithimageclassificationtasksunderlabelsupervision;thebottomtwo
modelsare pre-trainedvia theDINOself-supervisedtechnique[6]. crateconclusivelyperforms betterthan
the ViT at detection and segmentation metrics when both are trained using supervised classification.
fine-grained segmentation from an image based on the token representations learned in crate.
We visualize the fine-grained segmentations produced by MaskCut in Figure 5 and compare the
segmentationanddetectionperformanceinTable1. Basedontheseresults,weobservethatMaskCut
withsupervisedViTfeaturescompletelyfailstoproducesegmentationmasksincertaincases,for
example,thefirstimageinFigure5andtheViT-S/8rowinTable1. ComparedtoViT, crateprovides
better internal representation tokens for both segmentation and detection.
4. White-Box Empowered Analysis of Segmentation in crate
In this section, we delve into the segmentation properties of crateusing analysis powered by our
white-boxperspective. Tostartwith,weanalyzetheinternaltokenrepresentationsfromdifferent
layersof crateandstudythepowerofthenetworksegmentationasafunctionofthelayerdepth.
Wethenperformanablationstudyonvariousarchitecturalconfigurationsof cratetoisolatethe
essentialcomponentsfordevelopingsegmentationproperties. Finally,weinvestigatehowtoidentify
the“semantic”meaningofcertainsubspacesandextractmorefine-grainedinformationfrom crate.
We use the crate-B/8 and ViT-B/8 as the default models for evaluation in this section.
Role of depth in crate.Each layer of crateis designed for the same conceptual purpose: to
optimizethesparseratereductionandtransformthetokendistributiontocompactandstructured
forms (Section 2). Given that the emergence of semantic segmentation in crateis analogous to
theclustering of tokens belonging to similar semantic categories in the representation Z, we therefore
expectthesegmentationperformanceof cratetoimprovewithincreasingdepth. Totestthis,we
utilize theMaskCut pipeline (described inSection 3.2) toquantitatively evaluate the segmentation
performanceoftheinternalrepresentationsacrossdifferentlayers. Meanwhile,weapplythePCA
visualization (described in Section 3.1) for understanding how segmentation emerges with respect
todepth. ComparedtotheresultsinFigure3,aminordifferenceinourvisualizationisthatweshow
the first four principal components in Figure 6 and do not filter out background tokens.
7

--- PAGE 8 ---
7 8 9 10 11 12
Layer index0.00.20.40.60.81.0AP Score on Segmentation
CRATE-B/8
ViT-B/8
Shallow Deep layer 1 layer 4 layer 6 layer 8 layer 11 
Figure6: Effectofdepthforsegmentationinsupervised crate.(Left)Layer-wisesegmentationperformance
ofcrateand ViT via MaskCut pipeline on COCO val2017 (Higher AP score means better segmentation
performance). ( Right) We follow the implementation in Amir et al. [2]: we first apply PCA on patch-wise
features. Then, for the gray figure, we visualize the 1st components, and for the colored figure, we visualize the
2nd, 3rd and 4th components, which correspond to the RGB color channels. See more results in Figure 9.
COCO Detection VOC Seg.
Model Attention Nonlinearity AP 50AP75AP mIoU
crate MSSA ISTA 2.1 0.7 0.8 23.9
crate-MLP MSSA MLP 0.2 0.2 0.2 22.0
crate-MHSA MHSA ISTA 0.1 0.1 0.0 18.4
ViT MHSA MLP 0.1 0.1 0.0 14.1
Table2: Ablationstudyofdifferent cratevariants. Weusethe Small-Patch8 (S-8)modelconfiguration
across all experiments in this table.
The results are summarized in Figure 6. We observe that the segmentation score improves when
usingrepresentationsfromdeeperlayers,whichalignswellwiththeincrementaloptimizationdesign
ofcrate. In contrast, even though the performance of ViT-B/8 slightly improves in later layers, its
segmentation scores are significantly lower than those of crate(c.f. failures in Figure 5, bottom
row). The PCA results are presented in Figure 6 ( Right). We observe that representations extracted
from deeper layers of crateincreasingly focus on the foreground object and are able to capture
texture-level details. Figure 9 in the Appendix has more PCA visualization results.
Ablationstudyofarchitecturein crate.Boththeattentionblock( MSSA)andtheMLPblock( ISTA)
incrateare different from the ones in the ViT. In order to understand the effect of each component
for emerging segmentation properties of crate, we study three different variants of crate:crate,
crate-MHSA,crate-MLP,wherewedenotetheattentionblockandMLPblockinViTas MHSAand
MLPrespectively. We summarize different model architectures in Table 2.
ForallmodelsinTable2,weapplythesamepre-trainingsetupontheImageNet-21kdataset. We
then apply the coarse segmentation evaluation (Section 3.2) and MaskCut evaluation (Section 3.2)
to quantitatively compare the performance of different models. As shown in Table 2, cratesig-
nificantly outperforms other model architectures across all tasks. Interestingly, we find that the
coarse segmentation performance (i.e., VOC Seg) of the ViT can be significantly improved by simply
replacingthe MHSAinViTwiththe MSSAincrate,despitethearchitecturaldifferencesbetween MHSA
andMSSAbeing small. This demonstrates the effectiveness of the white-box design.
Identifyingsemanticpropertiesofattentionheads. AsshowninFigure1,theself-attentionmap
between the [CLS]token and patch tokens contains clear segmentation masks. We are interested in
capturingthesemanticmeaningofcertainattention heads;thisisanimportanttaskforinterpretability,
andisalreadystudiedforlanguagetransformers[34]. Intuitively,eachheadcapturescertainfeatures
ofthedata. Givena cratemodel,wefirstforwardaninputimage(e.g.ahorseimageasinFigure7)
andselectfourattentionheadswhichseemtohavesemanticmeaningbymanualinspection. After
identifyingtheattentionheads,wevisualizetheself-attentionmapoftheseheadsonotherinput
images. We visualize the results in Figure 7. Interestingly, we find that each of the selected attention
headscapturesadifferentpartoftheobject,andevenadifferentsemanticmeaning. Forexample,
theattentionheaddisplayedinthefirstcolumnofFigure7capturesthelegsofdifferentanimals,
8

--- PAGE 9 ---
Head 0 
“Leg” Head 1 
“Body” Head 3 
“Face” Head 4 
“Ear” Head 0 
“Leg” Head 1 
“Body” Head 3 
“Face” Head 4 
“Ear” Figure7: Visualizationof semanticheads. Weforward amini-batchof imagesthrougha supervised crate
andexaminetheattentionmapsfromalltheheadsinthepenultimatelayer. Wevisualizeaselectionofattention
heads to show that certain heads convey specific semantic meaning, i.e. head 0↔"Legs",head 1↔"Body",head
3↔"Face",head 4↔"Ear".
and the attention head displayed in the last column captures the ears and head. This parsing of the
visual input into a part-whole hierarchy has been a fundamental goal of learning-based recognition
architectures since deformable part models [14, 15] and capsule networks [20, 40]—strikingly, it
emerges from the white-box design of cratewithin our simple supervised training setup.2
5. Related Work
Visualattentionandemergenceofsegmentation. Theconceptofattentionhasbecomeincreasingly
significant in intelligence, evolving from early computational models [21, 23, 41] to modern neural
networks [11, 44]. In deep learning, the self-attention mechanism has been widely employed in
processing visual data [11] with state-of-the-art performance on various visual tasks [6, 18, 35].
DINO [6] demonstrated that attention maps generated by self-supervised Vision Transformers
(ViT)[11] can implicitly perform semantic segmentation of images. This emergence of segmen-
tation capability has been corroborated by subsequent self-supervised learning studies [6, 18, 35].
Capitalizing on these findings, recent segmentation methodologies [2, 22, 46] have harnessed these
emergent segmentations to attain state-of-the-art results. Nonetheless, there is a consensus, as high-
lighted in studies like Caron et al. [6], suggesting that such segmentation capability would not
2In this connection, we note that Hinton [19] recently surmised that the query, key, and value projections in
the transformer should be made equal for this reason—the design of crateand Figure 7 confirm this.
9

--- PAGE 10 ---
manifest in a supervised ViT. A key motivation and contribution of our research is to show that
transformer-like architectures, as in crate, can exhibit this ability even with supervised training.
White-boxmodels. Indataanalysis,therehascontinuallybeensignificantinterestindeveloping
interpretableandstructuredrepresentationsofthedataset. Theearliestmanifestationsofsuchinterest
wereinsparsecodingviadictionarylearning[47],whicharewhite-boxmodelsthattransformthe
(approximatelylinear)dataintohuman-interpretablestandardforms(highlysparsevectors). The
advent of deep learning has not changed this desire much, and indeed attempts have been made
tomarrythepowerofdeeplearningwiththeinterpretabilityofwhite-boxmodels. Suchattempts
includescatteringnetworks[5],convolutionalsparsecodingnetworks[36],andthesparsemanifold
transform [9]. Another line of work constructs deep networks from unrolled optimization [7, 43,
50,51]. Suchmodelsarefullyinterpretable,yetonlyrecentlyhavetheydemonstratedcompetitive
performancewithblack-boxalternativessuchasViTatImageNetscale[51]. Thisworkbuildson
onesuchpowerfulwhite-boxmodel, crate[51],anddemonstratesmoreofitscapabilities,while
serving as an example for the fine-grained analysis made possible by white-box models.
6. Discussions and Future Work
In this study, we demonstrated that when employing the white-box model crateas a foundational
architectureinplaceoftheViT,thereisanaturalemergenceofsegmentationmasksevenwhileusing
a straightforward supervised training approach. Our empirical findings underscore the importance
ofprincipledarchitecturedesignfordevelopingbettervisionfoundationmodels. Assimplermodels
are more interpretable and easier to analyze, we are optimistic that the insights derived from white-
box transformers in this work will contribute to a deeper empirical and theoretical understanding of
the segmentation phenomenon. Furthermore, our findings suggest that white-box design principles
holdpromiseinofferingconcreteguidelinesfordevelopingenhancedvisionfoundationmodels. Two
compelling directions for further research would be investigating how to better engineer white-box
models such as crateto match the performance of self-supervised learning methods (such as
DINO), and expanding the range of tasks for which white-box models are practically useful.
Acknowledgements. We thank Xudong Wang and Baifeng Shi for valuable discussions on segmen-
tation properties in vision transformers.
References
[1]Samira Abnar and Willem Zuidema. “Quantifying attention flow in transformers”. arXiv
preprint arXiv:2005.00928 (2020). 6, 16.
[2]ShirAmir,YossiGandelsman,ShaiBagon,andTaliDekel.“DeepViTFeaturesasDenseVisual
Descriptors”. ECCVW What is Motion For? (2022). 2, 6, 8, 9, 16, 17.
[3]RishiBommasani,DrewAHudson,EhsanAdeli,RussAltman,SimranArora,Sydneyvon
Arx,MichaelSBernstein,JeannetteBohg,AntoineBosselut,EmmaBrunskill,etal.“Onthe
opportunities and risks of foundation models”. arXiv preprint arXiv:2108.07258 (2021). 2.
[4]TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. “Language models
arefew-shotlearners”. Advancesinneuralinformationprocessingsystems 33(2020),pp.1877–
1901. 2.
[5]JoanBrunaandStéphaneMallat.“Invariantscatteringconvolutionnetworks”. IEEEtransactions
on pattern analysis and machine intelligence 35.8 (Aug. 2013), pp. 1872–1886. 10.
[6]MathildeCaron,HugoTouvron,IshanMisra,HervéJégou,JulienMairal,PiotrBojanowski,
andArmandJoulin.“Emergingpropertiesinself-supervisedvisiontransformers”. Proceedings
of the IEEE/CVF international conference on computer vision . 2021, pp. 9650–9660. 2, 5–7, 9, 16, 19.
[7]KwanHoRyanChan,YaodongYu,ChongYou,HaozhiQi,JohnWright,andYiMa.“ReduNet:
A White-box Deep Network from the Principle of Maximizing Rate Reduction”. Journal of
Machine Learning Research 23.114 (2022), pp. 1–103. 3, 10.
[8]Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham,
XuanyiDong,ThangLuong,Cho-JuiHsieh,etal.“Symbolicdiscoveryofoptimizationalgo-
rithms”. arXiv preprint arXiv:2302.06675 (2023). 18.
10

--- PAGE 11 ---
[9]Yubei Chen, Dylan Paiton, and Bruno Olshausen. “The sparse manifold transform”. Advances
in neural information processing systems 31 (2018). 10.
[10]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. “Imagenet: A large-scale
hierarchicalimagedatabase”. 2009IEEEconferenceoncomputervisionandpatternrecognition .
Ieee. 2009, pp. 248–255. 18.
[11]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
ThomasUnterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,
et al. “An image is worth 16x16 words: Transformers for image recognition at scale”. arXiv
preprint arXiv:2010.11929 (2020). 2, 3, 5, 9, 18.
[12]MichaelEladandMichalAharon.“Imagedenoisingviasparseandredundantrepresentations
over learned dictionaries”. IEEE transactions on image processing: a publication of the IEEE Signal
Processing Society 15.12 (Dec. 2006), pp. 3736–3745. 1.
[13]M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The
PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results . http://www.pascal-
network.org/challenges/VOC/voc2012/workshop/index.html. 6, 18.
[14]Pedro Felzenszwalb, David McAllester, and Deva Ramanan. “A discriminatively trained,
multiscale, deformable part model”. 2008 IEEE Conference on Computer Vision and Pattern
Recognition . ieeexplore.ieee.org, June 2008, pp. 1–8. 9.
[15]PedroFFelzenszwalbandDanielPHuttenlocher.“PictorialStructuresforObjectRecognition”.
International journal of computer vision 61.1 (Jan. 2005), pp. 55–79. 9.
[16]Karol Gregor and Yann LeCun. “Learning fast approximations of sparse coding”. Proceedings
of the 27th International Conference on International Conference on Machine Learning . Omnipress.
2010, pp. 399–406. 3.
[17]Florentin Guth, John Zarka, and Stéphane Mallat. “Phase Collapse in Neural Networks”.
International Conference on Learning Representations . 2022. 4.
[18]Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. “Masked
autoencoders are scalable vision learners”. Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition . 2022, pp. 16000–16009. 9.
[19]Geoffrey Hinton. “How to represent part-whole hierarchies in a neural network” (Feb. 2021).
arXiv: 2102.12627 [cs.CV] . 9.
[20]Geoffrey E Hinton, Alex Krizhevsky, and Sida D Wang. “Transforming Auto-Encoders”.
ArtificialNeuralNetworksandMachineLearning–ICANN2011 .SpringerBerlinHeidelberg,2011,
pp. 44–51. 9.
[21]Laurent Itti and Christof Koch. “Computational modelling of visual attention”. Nature reviews
neuroscience 2.3 (2001), pp. 194–203. 9.
[22]Alexander Kirillov, Eric Mintun, Nikhila Ravi, HanziMao, Chloe Rolland, Laura Gustafson,
Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. “Segment anything”.
arXiv preprint arXiv:2304.02643 (2023). 2, 9.
[23]ChristofKochandShimonUllman.“Shiftsinselectivevisualattention:towardstheunderlying
neural circuitry.” Human neurobiology 4.4 (1985), pp. 219–227. 9.
[24]Philipp Krähenbühl and Vladlen Koltun. “Efficient inference in fully connected crfs with
gaussian edge potentials”. Advances in neural information processing systems 24 (2011). 17.
[25]AlexKrizhevsky,GeoffreyHinton,etal.“Learningmultiplelayersoffeaturesfromtinyimages”
(2009). 18.
[26]YannLeCun.“Apathtowardsautonomousmachineintelligenceversion0.9.2,2022-06-27”.
Open Review 62 (2022). 1.
[27]Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,Piotr
Dollár,andCLawrenceZitnick.“Microsoftcoco:Commonobjectsincontext”. ComputerVision–
ECCV2014:13thEuropeanConference,Zurich,Switzerland,September6-12,2014,Proceedings,Part
V 13. Springer. 2014, pp. 740–755. 7, 18.
[28]JonathanLong,EvanShelhamer,andTrevorDarrell.“Fullyconvolutionalnetworksforseman-
ticsegmentation”. ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition .
2015, pp. 3431–3440. 6, 17.
[29]IlyaLoshchilovandFrankHutter.“Decoupledweightdecayregularization”. arXivpreprint
arXiv:1711.05101 (2017). 18.
11

--- PAGE 12 ---
[30]YiMa,HarmDerksen,WeiHong,andJohnWright.“Segmentationofmultivariatemixeddata
via lossy data coding and compression”. PAMI(2007). 3.
[31]Yi Ma, Doris Tsao, and Heung-Yeung Shum. “On the principles of parsimony and self-
consistencyfortheemergenceofintelligence”. FrontiersofInformationTechnology&Electronic
Engineering 23.9 (2022), pp. 1298–1323. 1.
[32]Vishal Monga, Yuelong Li, and Yonina C Eldar. “Algorithm Unrolling: Interpretable, Efficient
Deep Learning for Signal and Image Processing”. IEEE Signal Processing Magazine 38.2 (Mar.
2021), pp. 18–44. 3.
[33]Maria-ElenaNilsbackandAndrewZisserman.“Automatedflowerclassificationoveralarge
numberofclasses”. 2008SixthIndianConferenceonComputerVision,Graphics&ImageProcessing .
IEEE. 2008, pp. 722–729. 18.
[34]Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan,BenMann,AmandaAskell,YuntaoBai,AnnaChen,TomConerly,DawnDrain,
DeepGanguli,ZacHatfield-Dodds,DannyHernandez,ScottJohnston,AndyJones,Jackson
Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan,
SamMcCandlish,andChrisOlah.“In-contextLearningandInductionHeads”. TransformerCir-
cuits Thread (2022). https://transformer-circuits.pub/2022/in-context-learning-and-induction-
heads/index.html. 8.
[35]MaximeOquab,TimothéeDarcet,ThéoMoutakanni,HuyVo,MarcSzafraniec,VasilKhali-
dov,PierreFernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal. “Dinov2:
Learning robust visual features without supervision”. arXiv preprint arXiv:2304.07193 (2023).
2, 6, 9, 16.
[36]Vardan Papyan, Yaniv Romano, Jeremias Sulam, and Michael Elad. “Theoretical Foundations
ofDeepLearningviaSparseRepresentations:AMultilayerSparseModelandItsConnectionto
Convolutional Neural Networks”. IEEE Signal Processing Magazine 35.4 (July 2018), pp. 72–89.
10.
[37]Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. “Cats and dogs”.
2012 IEEE conference on computer vision and pattern recognition . IEEE. 2012, pp. 3498–3505. 18.
[38]Ignacio Ramirez, Pablo Sprechmann, and Guillermo Sapiro. “Classification and clustering via
dictionarylearningwithstructuredincoherenceandsharedfeatures”. 2010IEEEComputer
Society Conference on Computer Vision and Pattern Recognition . IEEE. 2010, pp. 3501–3508. 1.
[39]Shankar Rao, Roberto Tron, René Vidal, and Yi Ma. “Motion segmentation in the presence
of outlying, incomplete, or corrupted trajectories”. IEEE transactions on pattern analysis and
machine intelligence 32.10 (Oct. 2010), pp. 1832–1845. 1.
[40]Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. “Dynamic Routing Between Capsules”.
AdvancesinNeuralInformationProcessingSystems .Ed.byIGuyon,UVonLuxburg,SBengio,
H Wallach, R Fergus, S Vishwanathan, and R Garnett. Vol. 30. Curran Associates, Inc., 2017. 9.
[41]Brian J Scholl. “Objects and attention: The state of the art”. Cognition 80.1-2 (2001), pp. 1–46. 9.
[42]Jianbo Shi and Jitendra Malik. “Normalized cuts and image segmentation”. IEEE Transactions
on pattern analysis and machine intelligence 22.8 (2000), pp. 888–905. 17.
[43]BaharehTolooshamsandDembaBa.“StableandInterpretableUnrolledDictionaryLearning”.
arXiv preprint arXiv:2106.00058 (2021). 10.
[44]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. “Attention is all you need”. Advances in neural information
processing systems 30 (2017). 9.
[45]QianqianWang,Yen-YuChang,RuojinCai,ZhengqiLi,BharathHariharan,AleksanderHolyn-
ski, and Noah Snavely. “Tracking Everything Everywhere All at Once”. arXiv:2306.05422
(2023). 2.
[46]Xudong Wang, Rohit Girdhar, Stella X Yu, and Ishan Misra. “Cut and learn for unsuper-
visedobjectdetectionandinstancesegmentation”. ProceedingsoftheIEEE/CVFConferenceon
Computer Vision and Pattern Recognition . 2023, pp. 3124–3134. 2, 6, 9, 17, 18.
[47]JohnWrightandYiMa. High-DimensionalDataAnalysiswithLow-DimensionalModels:Principles,
Computation, and Applications . Cambridge University Press, 2022. 4, 10.
[48]Allen Y Yang, John Wright, Yi Ma, and S Shankar Sastry. “Unsupervised segmentation of
naturalimagesvialossydatacompression”. ComputerVisionandImageUnderstanding 110.2
(2008), pp. 212–225. 1.
12

--- PAGE 13 ---
[49]Jianchao Yang, John Wright, Thomas S Huang, and Yi Ma. “Image super-resolution via sparse
representation”. IEEE transactions on image processing: a publication of the IEEE Signal Processing
Society19.11 (Nov. 2010), pp. 2861–2873. 1.
[50]Yongyi Yang, David P Wipf, et al. “Transformers from an optimization perspective”. Advances
in Neural Information Processing Systems 35 (2022), pp. 36958–36971. 10.
[51]YaodongYu,SamBuchanan,DruvPai,TianzheChu,ZiyangWu,ShengbangTong,BenjaminD.
Haeffele, and Yi Ma. White-Box Transformers via Sparse Rate Reduction . 2023. arXiv: 2306.01129
[cs.LG]. 2–5, 10, 14, 18.
[52]YaodongYu,KwanHoRyanChan,ChongYou,ChaobingSong,andYiMa.“LearningDiverse
and Discriminative Representations via the Principle of Maximal Coding Rate Reduction”.
Advances in Neural Information Processing Systems 33 (2020), pp. 9422–9434. 3.
13

--- PAGE 14 ---
Appendix
A.crateImplementation
Inthissection, weprovidethedetailsonourimplementationof crate, bothatahigherlevelforuse
in mathematical analysis, and at a code-based level for use in reference implementations. While we
used the same implementation as in Yu et al. [51], we provide the details here for completeness.
A.1. Forward-Pass Algorithm
We provide the details on the forward pass of cratein Algorithm 1.
Algorithm 1 CRATE Forward Pass.
Hyperparameter: Numberoflayers L,featuredimension d,subspacedimension p,imagedimension
(C, H, W ), patch dimension (PH, PW), sparsification regularizer λ > 0, quantization error ε,
learning rate η >0.
Parameter: Patch projection matrix W∈Rd×D. ▷ D.=PHPW.
Parameter: Class token z0
[CLS]∈Rd.
Parameter: Positional encoding Epos∈Rd×(N+1). ▷ N.=H
PH·W
PW.
Parameter: Local signal models (Uℓ
[K])L
ℓ=1where each Uℓ
[K]= (Uℓ
1, . . . ,Uℓ
K)and each Uℓ
k∈Rd×p.
Parameter: Sparsifying dictionaries (Dℓ)L
ℓ=1where each Dℓ∈Rd×d.
Parameter: Sundry LayerNorm parameters.
1:function MSSA(Z∈Rd×(N+1)|U[K]∈RK×d×p)
2:returnp
(N+ 1)ε2KX
k=1Uk(U∗
kZ) softmax(( U∗
kZ)∗(U∗
kZ)) ▷Eq. (5)
3:end function
4:function ISTA(Z∈Rd×(N+1)|D×Rd×d)
5:return ReLU( Z+ηD∗(Z−DZ)−ηλ1) ▷Eq. (7)
6:end function
7:function CRATEForwardPass (IMG∈RC×H×W)
8: X.= [x1, . . . ,xN]←Patchify (IMG) ▷X∈RD×Nand each xi∈RD.
9: #f0Operator
10:
z1
1, . . . ,z1
N
←WX ▷z1
i∈Rd.
11: Z1←z1
[CLS],z1
1, . . . ,z1
N
+Epos ▷Z1∈Rd×(N+1).
12: #fℓOperators
13:forℓ∈ {1, . . . , L }do
14: Zℓ
n←LayerNorm (Zℓ) ▷Zℓ
n∈Rd×(N+1)
15: Zℓ+1/2←Zℓ
n+MSSA(Zℓ
n|Uℓ
[K]) ▷Zℓ+1/2∈Rd×(N+1)
16: Zℓ+1/2
n←LayerNorm (Zℓ+1/2) ▷Zℓ+1/2
n∈Rd×(N+1)
17: Zℓ+1←ISTA(Zℓ+1/2
n|Dℓ) ▷Zℓ+1∈Rd×(N+1)
18:end for
19:return Z←ZL+1
20:end function
14

--- PAGE 15 ---
A.2. PyTorch-Like Code for Forward Pass
Similar to the previous subsection, we provide the pseudocode for the MSSAblock and ISTAblock in
Algorithm 2, and then present the pseudocode for the forward pass of cratein Algorithm 3.
Algorithm 2 PyTorch-Like Code for MSSA and ISTA Forward Passes
1class ISTA:
2 # initialization
3 def __init__(self, dim, hidden_dim, dropout = 0., step_size=0.1, lambd=0.1):
4 self.weight = Parameter(Tensor(dim, dim))
5 init.kaiming_uniform_(self.weight)
6 self.step_size = step_size
7 self.lambd = lambd
8 # forward pass
9 def forward(self, x):
10 x1 = linear(x, self.weight, bias=None)
11 grad_1 = linear(x1, self.weight.t(), bias=None)
12 grad_2 = linear(x, self.weight.t(), bias=None)
13 grad_update = self.step_size * (grad_2 - grad_1) - self.step_size * self.lambd
14 output = relu(x + grad_update)
15 return output
16class MSSA:
17 # initialization
18 def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):
19 inner_dim = dim_head * heads
20 project_out = not (heads == 1 and dim_head == dim)
21 self.heads = heads
22 self.scale = dim_head ** -0.5
23 self.attend = Softmax(dim = -1)
24 self.dropout = Dropout(dropout)
25 self.qkv = Linear(dim, inner_dim, bias=False)
26 self.to_out = Sequential(Linear(inner_dim, dim), Dropout(dropout)) if project_out
else nn.Identity()
27 # forward pass
28 def forward(self, x):
29 w = rearrange(self.qkv(x), ’b n (h d) -> b h n d’, h = self.heads)
30 dots = matmul(w, w.transpose(-1, -2)) * self.scale
31 attn = self.attend(dots)
32 attn = self.dropout(attn)
33 out = matmul(attn, w)
34 out = rearrange(out, ’b h n d -> b n (h d)’)
35 return self.to_out(out)
Algorithm 3 PyTorch-Like Code for CRATE Forward Pass
1class CRATE:
2 # initialization
3 def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):
4 # define layers
5 self.layers = []
6 self.depth = depth
7 for _ in range(depth):
8 self.layers.extend([LayerNorm(dim), MSSA(dim, heads, dim_head, dropout)])
9 self.layers.extend([LayerNorm(dim), ISTA(dim, mlp_dim, dropout)])
10 # forward pass
11 def forward(self, x):
12 for ln1, attn, ln2, ff in self.layers:
13 x_ = attn(ln1(x)) + ln1(x)
14 x = ff(ln2(x_))
15 return x
15

--- PAGE 16 ---
B. Detailed Experimental Methodology
In this section we formally describe each of the methods used to evaluate the segmentation property
ofcratein Section 3 and Section 4, especially compared to DINO and supervised ViT. This section
repeatsexperimentalmethodologiescoveredlessformallyinotherworks;westrivetorigorously
define the experimental methodologies in this section.
B.1. Visualizing Attention Maps
We recapitulate the method to visualize attention maps in Abnar and Zuidema [1] and Caron et al.
[6], at first specializing their use to instances of the cratemodel before generalizing to the ViT.
Forthe kthheadatthe ℓthlayerof crate,wecomputethe self-attentionmatrix Aℓ
k∈RNdefinedas
follows:
Aℓ
k=
Aℓ
k,1
...
Aℓ
k,N
∈RN,where Aℓ
k,i=exp(⟨Uℓ∗
kzℓ
i,Uℓ∗
kzℓ
[CLS]⟩)
PN
j=1exp(⟨Uℓ∗
kzℓ
j,Uℓ∗
kzℓ
[CLS]⟩). (10)
We then reshape the attention matrix Aℓ
kinto a√
N×√
Nmatrix and visualize the heatmap as
shown in Figure 1. For example, the ithrow and the jthcolumn element of the heatmap in Figure 1
correspondstothe mthcomponentof Aℓ
kifm= (i−1)·√
N+j. InFigure1,weselectoneattention
head of crateand visualize the attention matrix Aℓ
kfor each image.
FortheViT,theentiremethodologyremainsthesame,exceptthattheattentionmapisdefinedin
the following reasonable way:
Aℓ
k=
Aℓ
k,1
...
Aℓ
k,N
∈RN,where Aℓ
k,i=exp(⟨Kℓ∗
kzℓ
i,Qℓ∗
kzℓ
[CLS]⟩)
PN
j=1exp(⟨Kℓ∗
kzℓ
j,Qℓ∗
kzℓ
[CLS]⟩). (11)
where the “query” and “key” parameters of the standard transformer at head kand layer ℓare
denoted Kℓ
kandQℓ
krespectively.
B.2. PCA Visualizations
As in the previous subsection, we recapitulate the method to visualize the patch representations
using PCA from Amir et al. [2] and Oquab et al. [35]. As before we specialize their use to instances
of the cratemodel before generalizing to the ViT.
Wefirstselect Jimagesthatbelongtothesameclass, {Xj}J
j=1,andextractthetokenrepresentations
for each image at layer ℓ, i.e.,zℓ
j,[CLS],zℓ
j,1, . . . ,zℓ
j,N
forj∈[J]. In particular, zℓ
j,irepresents the ith
token representation atthe ℓthlayer for the jthimage. We then compute the firstPCA components
ofbZℓ={bzℓ
1,1, . . . ,bzℓ
1,N, . . . ,bzℓ
J,1, . . . ,bzℓ
J,N}, and use bzℓ
j,ito denote the aggregated token represen-
tation for the i-th token of Xj, i.e.,bzℓ
j,i= [(U∗
1bzℓ
j,i)⊤, . . . , (U∗
Kbzℓ
j,i)⊤]⊤∈R(p·K)×1. We denote the
first eigenvector of the matrix bZ∗bZbyu0and compute the projection values as {σλ(⟨u0,zℓ
j,i⟩)}i,j,
where σλ(x) =x,|x| ≥λ
0,|x|< λis the hard-thresholding function. We then select a subset of token
representations from bZwith σλ(⟨u0,zℓ
j,i⟩)>0. which correspond to non-zero projection values
after thresholding, and we denote this subset as bZs⊆bZ. This selection step is used to remove
thebackground[35]. WethencomputethefirstthreePCAcomponentsof bZswiththefirstthree
eigenvectors of matrix bZ∗
sbZsdenoted as {u1,u2,u3}. We define the RGB tuple for each token as:
[rj,i, gj,i, bj,i] = [⟨u1,zℓ
j,i⟩,⟨u2,zℓ
j,i⟩,⟨u3,zℓ
j,i⟩], i∈[N], j∈[J],zℓ
j,i∈bZs. (12)
Next,foreachimage Xjwecompute Rj,Gj,Bj,where Rj= [rj,1, . . . , r j,N]⊤∈Rd×1(similarfor
GjandBj). Thenwereshapethethreematricesinto√
N×√
Nandvisualizethe“PCAcomponents”
of image Xjvia the RGB image (Rj,Gj,Bj)∈R3×√
N×√
N.
16

--- PAGE 17 ---
The PCA visualization of ViTs are evaluated similarly, with the exception of utilizing the “ Key”
features bzℓ
j,i= [(K∗
1bzℓ
j,i)⊤, . . . , (K∗
Kbzℓ
j,i)⊤]⊤. Previouswork[2]demonstratedthatthe“ Key”features
leadtolessnoisyspacestructuresthanthe“ Query”features. Intheexperiments(suchasinFigure3),
we set the threshold value λ=1
2.
B.3. Segmentation Maps and mIoU Scores
We now discuss the methods used to compute the segmentation maps and the corresponding
mean-Union-over-Intersection (mIoU) scores.
Indeed, suppose we have already computed the attention maps Aℓ
k∈RNfor a given image as
in Appendix B.1. We then threshold each attention map by setting its top P= 60%of entries to
1and setting the rest to 0. The remaining matrix, say ˜Aℓ
k∈ {0,1}N, forms a segmentation map
corresponding to the kthhead in the ℓthlayer for the image.
Suppose that the tokens can be partitioned into Msemantic classes, and the mthsemantic class has
a boolean ground truth segmentation map Sm∈ {0,1}N. We want to compute the quality of the
attention-created segmentation map above, with respect to the ground truth maps. For this, we
use the mean-intersection-over-union (mIoU) metric [28] as described in the sequel. Experimental
results yield that the heads at a given layer correspond to different semantic features. Thus, for each
semantic class mand layer ℓ, we attempt to find the best-matched head at layer ℓand use this to
compute the intersection-over-union, obtaining
mIoUℓ
m.= max
k∈[K]∥Sm⊙Aℓ
k∥0
∥Sm∥0+∥Aℓ
k∥0− ∥Sm⊙Aℓ
k∥0, (13)
where ⊙denotes element-wise multiplication and ∥·∥0counts the number of nonzero elements
in the input vector (and since the inputs are boolean vectors, this is equivalent to counting the
numberof 1’s). ToreporttheoverallmIoUscoreforlayer ℓ(orwithoutreferent,forthelastlayer
representations), we compute the quantity
mIoUℓ.=1
MMX
m=1mIoUℓ
m, (14)
and average it amongst all images for which we know the ground truth.
B.4. MaskCut
We apply the MaskCut pipeline (Algorithm 4) to generate segmentation masks and detection
bounding box (discussed in Section 3.2). As described by Wang et al. [46], we iteratively apply
Normalized Cuts [42] on the patch-wise affinity matrix Mℓ, where Mℓ
ij=PK
k=1⟨Uℓ∗
kzℓ
i,Uℓ∗
kzℓ
j⟩. At
eachiterativestep,wemaskouttheidentifiedpatch-wiseentrieson Mℓ. Toobtainmorefine-grained
segmentation masks, MaskCut employs Conditional Random Fields (CRF) [24] to post-process the
masks,whichsmoothstheedgesandfiltersoutunreasonablemasks. Correspondingly,thedetection
bounding box is defined by the rectangular region that tightly encloses a segmentation mask.
Algorithm 4 MaskCut
Hyperparameter: n, the number of objects to segment.
1:function MaskCut (M)
2:fori∈ {1, . . . , n }do
3: mask←NCut (M) ▷maskis a boolean array
4: M←M⊙mask ▷Equivalent to applying the mask to M
5: masks [i]←mask
6:end for
7:return masks
8:end function
FollowingtheofficialimplementationbyWangetal.[46],weselecttheparametersas n= 3, τ= 0.15,
where ndenotestheexpectednumberofobjectsand τdenotesthethresholdingvaluefortheaffinity
matrix Mℓ,i.e. entriessmallerthan 0.15willbeset to 0. InTable 1,weremovethe post-processing
CRF step in MaskCut when comparing different model variants.
17

--- PAGE 18 ---
C. Experimental Setup and Additional Results
In this section, we provide the experimental setups for experiments presented in Section 3 and
Section4, as wellas additionalexperimental results. Specifically, weprovidethedetailedexperimen-
tal setup for training and evaluation on Appendix C.1. We then present additional experimental
resultsonthetransferlearningperformanceof cratewhenpre-trainedonImageNet-21k[10]inAp-
pendixC.2. InAppendixC.3,weprovideadditionalvisualizationsontheemergenceofsegmentation
masks in crate.
C.1. Setups
Model setup We utilize the cratemodel as described by Yu et al. [51] at scales -S/8 and -B/8.
In a similar manner, we adopt the ViT model from Dosovitskiy et al. [11] using the same scales
(-S/8 and -B/8), ensuring consistent configurations between them. One can see the details of crate
transformer in Appendix A.
Trainingsetup Allvisualmodelsaretrainedforclassificationtasks(seeSection2.2)onthecomplete
ImageNetdataset[10],commonlyreferredtoasImageNet-21k. Thisdatasetcomprises14,197,122
images distributed across 21,841 classes. For training, each RGB image was resized to dimen-
sions 3×224×224, normalized using means of (0.485,0.456,0.406)and standard deviations of
(0.229,0.224,0.225), and then subjected to center cropping and random flipping. We set the mini-
batch size as 4,096 and apply the Lion optimizer [8] with learning rate 9.6×10−5and weight decay
0.05. All the models, including crates and ViTs are pre-trained with 90 epochs on ImageNet-21K.
Evaluation setup We evaluate the coarse segmentation, as detailed in Section Section 3.2, using
attention maps onthe PASCALVOC 2012 validation set [13] comprising 1,449 RGB images. Addition-
ally,weimplementtheMaskCut[46]pipeline,asdescribedinSection3.2,ontheCOCO val2017[27],
whichconsistsof5,000RGBimages,andassessourmodels’performanceforbothobjectdetection
andinstancesegmentationtasks. Allevaluationproceduresareunsupervised,andwedonotupdatethe
model weights during this process.
C.2. Transfer Learning Evaluation
We evaluate transfer learning performance of crateby fine-tuning models that are pre-trained
onImageNet-21kforthefollowingdownstreamvisionclassificationtasks: ImageNet-1k[10], CI-
FAR10/CIFAR100 [25], Oxford Flowers-102 [33], Oxford-IIIT-Pets [37]. We also finetune on two
pre-trainedViTmodels(-T/8and-B/8)forreference. Specifically,weusetheAdamWoptimizer[29]
and configurethelearningrate to 5×10−5, weight decay as0.01. Due tomemoryconstraints, weset
the batch size to be 128 for all experiments conducted for the base models and set it to be 256 for the
other smaller models. We report our results in Table 3.
Datasets crate-T crate-S crate-B ViT-T ViT-B
# parameters 5.74M 14.12M 38.83M 10.36M 102.61M
ImageNet-1K 62.7 74.2 79.5 71.8 85.8
CIFAR10 94.1 97.2 98.1 97.2 98.9
CIFAR100 76.7 84.1 87.9 84.4 90.1
Oxford Flowers-102 82.2 92.2 96.7 92.1 99.5
Oxford-IIIT-Pets 77.0 86.4 90.7 86.2 91.8
Table3: Top1accuracyof crateonvariousdatasetswithdifferentmodelscaleswhenpre-trainedonImageNet-
21K and fine-tuned on the given dataset.
18

--- PAGE 19 ---
C.3. Additional Visualizations
Supervised CRATE 
DINO 
Figure 8: Additional visualizations of the attention map of crate-S/8 and comparison with DINO [6]. Top
2rows: visualizationsofattentionmapsfrom supervised crate-S/8.Bottom2rows : visualizationsofattention
maps borrowed from DINO’s paper. The figure shows that supervised cratehas at least comparable attention
maps with DINO. Precise methodology is discussed in Appendix B.1.
Supervised CRATE 
Supervised ViT 
Shallow Deep 
Figure 9: Additional layer-wise PCA visualization. Top 2 rows : visualizations of the PCA of the features
fromsupervised crate-B/8.Bottom2rows : visualizationsofthePCAofthefeaturesfrom supervised ViT-B/8.
Thefigureshowsthat supervised crateshowsabetterfeaturespacestructurewithanexplicitly-segmented
foregroundobjectandlessnoisybackgroundinformation. TheinputimageisshowninFigure1’s topleftcorner.
Precise methodology is discussed in Appendix B.2.
19

--- PAGE 20 ---
2 3 5 9 20 50 90
Epocs0.00.20.40.60.8AP Score on Detection
CRATE-S/8
Random Converged init epo. 2 epo. 5 epo. 20 epo. 50 
Figure 10: Effect of training epochs in supervised crate.(Left) Detection performance computed at each epoch
viaMaskCutpipelineonCOCOval2017(HigherAPscoremeansbetterdetectionperformance). ( Right)We
visualizethePCAofthefeaturesatthepenultimatelayer computedateachepoch . Astrainingepochsincrease,
foreground objects can be explicitly segmented and separated into different parts with semantic meanings.
50%, N(0, 10) 
50%, N(0, 25) 
50%, N(0, 50) 
50%, N(0, 75) 
Figure 11: Adding Gaussian noise with different standard deviation. We add Gaussian noise to the input
image on a randomly chosen set of 50% of the pixels, with different standard deviations, and visualize all 6
headsinlayer10of crate-S/8. Thevaluesofeachentryineachcoloroftheimageareintherange (0,255).
Theright 2 columns , which contain edge information, remain unchanged with different scales of Gaussian noise.
Themiddle column shows that texture-level information will be lost as the input becomes noisier.
20

--- PAGE 21 ---
10%, N(0, 75) 
25%, N(0, 75) 
50%, N(0, 75) 
75%, N(0, 75) Figure 12: Adding Gaussian noise to a different percentage of the pixels. We add Gaussian noise with
standard deviation 75to a randomly chosen set of pixels within the input image, taking a different number
of pixels in each experiment. We visualize all 6 heads in layer 10 of crate-S/8. The values of each entry in
eachchanneloftheimageareintherange (0,255). Inadditiontotheobservationin Figure11,wefindthat
crateshifts its focus as the percentage of noisy pixels increases. For example, in the middle column, the head
first focuses on the texture of the door. Then, it starts to refocus on the edges. Interestingly, the tree pops up in
noisier cases’ attention maps.
21

--- PAGE 22 ---
CRATE 
 CRATE-sth CRATE-MLP CRATE-MHSA Figure13: Attention mapof crate’svariants insecond-to-last layer . In additiontothe quantitativeresults
discussedin Section4,weprovidevisualizationresultsforthearchitecturalablationstudy. crate-MLPand
crate-MHSAhave been discussed in Section 4 while crate-sth maintains both MSSAand ISTAblocks, and
instead switches theactivationfunction in the ISTAblock from ReLU tosoft thresholding, in accordance with
analternative formulationof the ISTAblockwhich doesnot imposea non-negativityconstraint inthe LASSO
(see Section 2.1 for more details). Attention maps with clear segmentations emerge in all architectures with the
MSSAblock.
CRATE V iT V iT CRATE 
Figure 14: More attention maps of supervised crateand ViT on images from COCO val2017. We select the
second-to-last layer attention maps for crateand the last layer for ViT.
22
