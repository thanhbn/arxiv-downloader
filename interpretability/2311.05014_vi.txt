# 2311.05014.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/interpretability/2311.05014.pdf
# Kích thước tệp: 1672392 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Diễn giải các Mô hình Ngôn ngữ Được Huấn luyện Trước thông qua Nút thắt Khái niệm
Zhen Tan
Arizona State University
ztan36@asu.edu
Yuan Bo
Zhejiang University
byuan@zju.edu.cnLu Cheng
University of Illinois Chicago
lucheng@uic.edu
Jundong Li
University of Virginia
jundong@virginia.eduSong Wang
University of Virginia
sw3wv@virginia.edu
Huan Liu
Arizona State University
huanliu@asu.edu
Tóm tắt
Các mô hình ngôn ngữ được huấn luyện trước (PLMs) đã đạt được những bước tiến đáng kể trong nhiều tác vụ xử lý ngôn ngữ tự nhiên khác nhau. Tuy nhiên, việc thiếu khả năng diễn giải do tính chất "hộp đen" của chúng tạo ra thách thức cho việc triển khai có trách nhiệm. Mặc dù các nghiên cứu trước đây đã cố gắng cải thiện khả năng diễn giải bằng cách sử dụng, ví dụ, trọng số attention trong các lớp self-attention, những trọng số này thường thiếu sự rõ ràng, dễ đọc và trực quan. Trong nghiên cứu này, chúng tôi đề xuất một phương pháp mới để diễn giải PLMs bằng cách sử dụng các khái niệm cấp cao, có ý nghĩa mà con người có thể hiểu được dễ dàng. Ví dụ, chúng tôi học khái niệm về "Thức ăn" và điều tra xem nó ảnh hưởng như thế nào đến dự đoán cảm xúc của mô hình đối với một đánh giá nhà hàng. Chúng tôi giới thiệu C3M, kết hợp các khái niệm được chú thích bởi con người và được tạo ra bởi máy để trích xuất các neuron ẩn được thiết kế để bao bọc các khái niệm có ý nghĩa về mặt ngữ nghĩa và cụ thể cho tác vụ. Thông qua các đánh giá thực nghiệm trên các tập dữ liệu thực tế, chúng tôi chứng minh rằng phương pháp của chúng tôi cung cấp những hiểu biết có giá trị để diễn giải hành vi PLM, giúp chẩn đoán lỗi mô hình, và tăng cường độ bền vững của mô hình trong môi trường nhãn khái niệm nhiễu.
1 Giới thiệu
Mặc dù các Mô hình Ngôn ngữ Được Huấn luyện Trước (PLMs) như BERT (Devlin et al., 2018) đã đạt được thành công đáng kể trong nhiều tác vụ NLP khác nhau (Zhu et al., 2020; Liu and Lapata, 2019), chúng thường được coi là hộp đen, tạo ra những trở ngại đáng kể cho việc triển khai có trách nhiệm trong các tình huống thực tế, đặc biệt là trong các lĩnh vực quan trọng như chăm sóc sức khỏe (Koh et al., 2020). Do đó, việc cho phép khả năng diễn giải của PLMs là rất quan trọng để đạt được AI có trách nhiệm xã hội (Cheng et al., 2021). Cho đến nay, nhiều công trình hiện có (Belinkov and Glass, 2019; Madsen et al., 2022) tận dụng các trọng số attention được trích xuất từ các lớp self-attention để cung cấp tầm quan trọng ở cấp token hoặc cấp cụm từ. Những giải thích cấp thấp này được phát hiện là không trung thực (Yin and Neubig,
Hình 1: Minh họa về CBE-PLMs. Thông qua PLMs, văn bản gốc x đầu tiên được ánh xạ vào một lớp trung gian bao gồm một tập hợp các khái niệm có thể hiểu được bởi con người c, sau đó được sử dụng để dự đoán nhãn mục tiêu y.
2022) và thiếu khả năng đọc và tính trực quan (Losch et al., 2019), dẫn đến những giải thích không ổn định hoặc thậm chí không hợp lý. Để giải quyết những hạn chế này, chúng tôi tìm cách giải thích thông qua các khái niệm có thể hiểu được bởi con người sử dụng các đặc trưng trừu tượng hơn (ví dụ, các khái niệm chung) thay vì các đặc trưng đầu vào thô ở cấp token (Zarlenga et al., 2022; Liao and Vaughan, 2023). Nền tảng của công trình này là các Mô hình Nút thắt Khái niệm (CBMs) (Koh et al., 2020) diễn giải các mô hình sâu (ví dụ, ResNet (He et al., 2016)) cho các tác vụ phân loại hình ảnh sử dụng các khái niệm cấp cao (ví dụ, hình dạng). Đối với các tác vụ NLP như phân tích cảm xúc, các khái niệm có thể là Thức ăn, Không khí, và Dịch vụ như thể hiện trong Hình 1, trong đó mỗi khái niệm tương ứng với một neuron trong lớp nút thắt khái niệm. Lớp quyết định cuối cùng sau đó là một hàm tuyến tính của những khái niệm này. Việc sử dụng các khái niệm cải thiện đáng kể khả năng đọc và tính trực quan của các giải thích so với các đặc trưng cấp thấp như "tôm hùm".
Chúng tôi đề xuất nghiên cứu các Mô hình Ngôn ngữ Được Huấn luyện Trước Được Kích hoạt Nút thắt Khái niệm (CBE-PLMs). Có ba thách thức chính: Thứ nhất, CBMs không thể được điều chỉnh trực tiếp vì PLMs được huấn luyện trước và tinh chỉnh trên các corpus riêng biệt trong khi CBMs hoạt động trên các tác vụ phân loại hình ảnh end-to-end giống nhau trong quá trình huấn luyện và thử nghiệm. Do đó, các corpus được sử dụng để huấn luyện trước PLMs có thể chứa các tương quan văn bản-khái niệm hữu ích mà không được thấy trong tác vụ downstream. Một cuộc điều tra về khả năng thích ứng của CBMs với CBE-PLMs là cần thiết. Thứ hai, phần lớn các arXiv:2311.05014v1  [cs.CL]  8 Nov 2023

--- TRANG 2 ---
CBMs hiện có (Koh et al., 2020; Zarlenga et al., 2022) yêu cầu các khái niệm được chú thích bởi con người. Điều này có thể gây khó khăn cho ngôn ngữ tự nhiên vì người chú thích có thể cần đọc qua toàn bộ văn bản để hiểu ngữ cảnh và gắn nhãn một khái niệm (Németh et al., 2020). Điều này hạn chế việc sử dụng thực tế và khả năng mở rộng của CBE-PLMs. Thứ ba, nhiều nghiên cứu đã xác định sự đánh đổi giữa khả năng diễn giải và độ chính xác tác vụ khi sử dụng CBMs vì các khái niệm được xác định trước có thể bỏ qua thông tin quan trọng cho việc dự đoán tác vụ mục tiêu (Zarlenga et al., 2022). Do đó, việc cải thiện cả khả năng diễn giải và hiệu suất tác vụ là rất quan trọng để đạt được sự đánh đổi tối ưu giữa khả năng diễn giải và tiện ích.
Để giải quyết thách thức đầu tiên, chúng tôi điều chỉnh các chiến lược huấn luyện tiêu chuẩn trong CBMs (Koh et al., 2020) để học CBE-PLMs và tiến hành các phân tích toàn diện để xác định cách tốt nhất để điều chỉnh CBMs để diễn giải PLMs. Đối với thách thức thứ hai về khám phá và gắn nhãn khái niệm, chúng tôi đề xuất tận dụng các Mô hình Ngôn ngữ Lớn (LLMs) được huấn luyện trên các corpus rộng lớn do con người tạo ra và phản hồi, như ChatGPT (OpenAI, 2023), để xác định các khái niệm mới trong văn bản và tạo ra các nhãn giả (thông qua prompting) cho các khái niệm chưa được gắn nhãn. Các nghiên cứu gần đây (Bommasani et al., 2022; OpenAI, 2023) cho thấy rằng những LLMs này bao gồm một lượng đáng kể kiến thức thường thức của con người. Bằng cách tăng cường tập hợp nhỏ các khái niệm được con người chỉ định với các khái niệm được máy tạo ra, chúng tôi tăng tính đa dạng khái niệm và thông tin hữu ích cho việc dự đoán. Ngoài ra, các nhãn giả được tạo ra cung cấp cho chúng tôi một tập lớn các thể hiện với nhãn khái niệm nhiễu, bổ sung cho tập nhỏ hơn các thể hiện với nhãn sạch. Để cải thiện thêm sự đánh đổi giữa khả năng diễn giải và tiện ích (thách thức thứ ba), chúng tôi đề xuất học từ các nhãn khái niệm nhiễu và kết hợp một cơ chế MixUp cấp khái niệm (Zhang et al., 2017) cho phép CBE-PLMs học hợp tác từ cả tập khái niệm nhiễu và sạch. Chúng tôi đặt tên cho khung của chúng tôi để huấn luyện CBE-PLMs là Tăng cường Khái niệm được Hướng dẫn bởi ChatGPT với MixUp Cấp Khái niệm (C3M). Tóm lại, những đóng góp của chúng tôi bao gồm:
•Chúng tôi cung cấp cuộc điều tra toàn diện đầu tiên về các chiến lược huấn luyện tiêu chuẩn của CBMs để diễn giải PLMs và đánh giá CBE-PLMs.
•Chúng tôi đề xuất C3M, tận dụng LLMs và MixUp để giúp PLMs học từ các khái niệm được chú thích bởi con người và được tạo ra bởi máy. C3M giải phóng CBMs khỏi các khái niệm được định trước và tăng cường sự đánh đổi giữa khả năng diễn giải và tiện ích.•Chúng tôi chứng minh hiệu quả và độ bền vững của can thiệp khái niệm thời gian thử nghiệm cho các CBE-PLMs đã học được cho các tác vụ phân loại văn bản phổ biến.
2 Công trình liên quan
2.1 Diễn giải các Mô hình Ngôn ngữ Được Huấn luyện Trước
PLMs như Word2Vec (Mikolov et al., 2013), BERT (Devlin et al., 2018), và chuỗi GPT gần đây hơn (Radford et al., 2019; Brown et al., 2020; OpenAI, 2023) đã thể hiện hiệu suất ấn tượng trong nhiều tác vụ NLP khác nhau. Tuy nhiên, bản chất mờ mịt của chúng tạo ra thách thức trong việc hiểu cách PLMs hoạt động bên trong (Diao et al., 2022). Để cải thiện khả năng diễn giải và tính minh bạch của PLMs, các nhà nghiên cứu đã khám phá các phương pháp khác nhau, như trực quan hóa trọng số attention (Galassi et al., 2020), thăm dò biểu diễn đặc trưng (Mishra et al., 2017; Lundberg and Lee, 2017; Bills et al., 2023), và sử dụng counterfactuals (Wu et al., 2021; Ross et al., 2021), trong số các phương pháp khác, để cung cấp giải thích ở cấp token địa phương, cấp thể hiện, hoặc cấp neuron. Tuy nhiên, các phương pháp này thường thiếu tính trung thực và tính trực quan, và có khả năng đọc kém, điều này làm suy yếu độ tin cậy của chúng (Madsen et al., 2022).
Gần đây, các nhà nghiên cứu đã chuyển sang các giải thích cấp khái niệm toàn cục mà con người có thể hiểu được một cách tự nhiên. Mặc dù mức độ diễn giải này đã được khám phá ít hơn trong NLP so với thị giác máy tính (Goyal et al., 2019; Kim et al., 2018; Mu and Andreas, 2020), nó đã thu hút sự chú ý. Ví dụ, một nghiên cứu (Vig et al., 2020) điều tra bias phân loại giới tính bằng cách kiểm tra mối liên kết của các từ nghề nghiệp như 'y tá' với giới tính. Ngoài ra, các CBMs (Koh et al., 2020; Zarlenga et al., 2022) đã nổi lên như các khung mới để đạt được khả năng diễn giải cấp khái niệm trong các hệ thống phân loại hình ảnh nhẹ. CBMs thường bao gồm một lớp trước lớp phân loại hoàn toàn kết nối cuối cùng, trong đó mỗi neuron tương ứng với một khái niệm có thể được con người diễn giải. CBMs cũng cho thấy lợi thế trong việc cải thiện độ chính xác thông qua can thiệp của con người trong quá trình thử nghiệm. Tuy nhiên, việc áp dụng CBMs để diễn giải PLMs quy mô lớn hơn vẫn chưa được khám phá đầy đủ.
Việc triển khai CBMs đòi hỏi sự tham gia của con người trong việc xác định tập khái niệm và chú thích các nhãn khái niệm. Những yêu cầu như vậy gây khó khăn cho ngôn ngữ tự nhiên vì con người có thể cần đọc qua toàn bộ văn bản để hiểu ngữ cảnh và gắn nhãn một khái niệm (Németh et al., 2020).

--- TRANG 3 ---
2.2 Học từ Nhãn Nhiễu
Mục tiêu của việc học từ nhãn nhiễu là giải quyết dữ liệu được gắn nhãn không chính xác hoặc phân loại sai trong các tình huống thực tế, với các kỹ thuật bao gồm ước lượng ma trận chuyển đổi nhiễu (Liu et al., 2022), tối thiểu hóa rủi ro bền vững (Englesson and Azizpour, 2021), và nhiều hơn nữa. Gần đây, khả năng chống chịu của các phương pháp học bán giám sát như MixMatch (Berthelot et al., 2019) và FixMatch (Sohn et al., 2020) đối với nhiễu nhãn đã được phát hiện bằng cách sử dụng nhãn giả cho dữ liệu chưa được gắn nhãn. Được truyền cảm hứng từ chúng, chúng tôi đề xuất sử dụng một LLM (ChatGPT) như một bộ đoán nhãn cố định, tạo ra các nhãn khái niệm trung gian nhiễu để có thể dự đoán nhãn tác vụ.
Đáng chú ý, CBMs chuyên về diễn giải và tính tương tác của các mô hình sâu cho các tác vụ phân loại chung. Trong khi Phân tích Cảm xúc Đa khía cạnh (Zhang et al., 2022) (MASA) chia sẻ các mục tiêu tương tự khi sử dụng các khía cạnh như khái niệm, nó khác biệt vì các khái niệm không bị giới hạn trong các đặc trưng khía cạnh chi tiết và có thể là các ý tưởng trừu tượng hoặc các khái niệm rộng hơn trong toàn bộ ngữ cảnh. Các nhãn khía cạnh trong MASA, chủ yếu được sử dụng cho độ chính xác dự đoán, không phải lúc nào cũng bắt buộc. Tóm lại, nghiên cứu này tiên phong trong việc khám phá toàn diện việc sử dụng các khái niệm để diễn giải PLMs quy mô lớn, và cung cấp một khung mạnh mẽ để khai thác các tín hiệu nhiễu từ LLMs để đạt được kết quả có thể diễn giải từ các PLMs trọng lượng nhẹ hơn, có thể dễ dàng hiểu được bởi người dùng.
3 Kích hoạt Nút thắt Khái niệm cho các Mô hình Ngôn ngữ Được Huấn luyện Trước
3.1 Thiết lập Bài toán
Chúng tôi tập trung vào việc diễn giải các dự đoán của PLMs được tinh chỉnh cho các tác vụ phân loại văn bản. Cho dữ liệu D={(x(i), y(i), c(i))n
i=1}, trong đó x∈Rd là đầu vào văn bản gốc, y∈R là nhãn mục tiêu để dự đoán, và c∈Rk là một vector của k khái niệm từ tập khái niệm C với |C|=k, chúng tôi xem xét một PLM fθ được tham số hóa bởi θ mà mã hóa một văn bản đầu vào x∈Rd thành biểu diễn tiềm ẩn z∈Re của nó.
Chiến lược tinh chỉnh vanilla, được định nghĩa cụ thể trong Phụ lục A, có thể được trừu tượng hóa như x→z→y.
Các Mô hình Ngôn ngữ Được Huấn luyện Trước Được Kích hoạt Nút thắt Khái niệm. Các nút thắt khái niệm gốc trong CBMs (Koh et al., 2020) đến từ việc thay đổi kích thước một trong các lớp trong bộ mã hóa CNN để khớp với số lượng khái niệm. Tuy nhiên, vì các bộ mã hóa PLM thường cung cấp biểu diễn văn bản với kích thước cao hơn nhiều so với số lượng khái niệm, việc giảm trực tiếp các neuron trong lớp sẽ ảnh hưởng đáng kể đến chất lượng biểu diễn văn bản đã học. Để giải quyết vấn đề này, thay vào đó chúng tôi thêm một lớp tuyến tính với kích hoạt sigmoid, ký hiệu là pψ, chiếu biểu diễn tiềm ẩn đã học z∈Re vào không gian khái niệm c∈Rk. Quá trình này có thể được biểu diễn như x→z→c→y.
Lưu ý rằng, không giống như các công trình trước đây cho phân loại hình ảnh, mỗi khái niệm ở đây không cần phải là nhị phân (tức là, có mặt hay không). Chúng tôi cho phép các khái niệm đa lớp, ví dụ, khái niệm "Thức ăn" trong một đánh giá nhà hàng có thể là tích cực, tiêu cực, hoặc không xác định.
Chúng tôi gọi PLM và bộ chiếu (fθ, pψ) cùng nhau là bộ mã hóa khái niệm và mô hình hoàn chỉnh (fθ, pψ, gϕ) là Các Mô hình Ngôn ngữ Được Huấn luyện Trước Được Kích hoạt Nút thắt Khái niệm (CBE-PLMs).
Trong quá trình huấn luyện, CBE-PLMs tìm cách đạt được hai mục tiêu: (1) căn chỉnh dự đoán khái niệm ĉ=pψ(fθ(x)) với nhãn khái niệm ground-truth c của x và (2) căn chỉnh dự đoán nhãn ŷ=gϕ(pψ(fθ(x))) với nhãn tác vụ ground-truth y. Chúng tôi tương ứng điều chỉnh ba chiến lược thông thường, huấn luyện độc lập, huấn luyện tuần tự, và huấn luyện chung, được đề xuất trong (Koh et al., 2020) để học CBE-PLM. Các công thức chi tiết của chúng được đưa ra trong Phụ lục A.
3.2 Đánh giá CBE-PLMs
Chúng tôi đề xuất đánh giá hiệu suất của việc tinh chỉnh vanilla và ba chiến lược huấn luyện cho CBE-PLMs sử dụng hai tập dữ liệu phân loại văn bản: CEBaB (Abraham et al., 2022) và IMDB (Maas et al., 2011). Cả hai tập dữ liệu đều chứa các khái niệm được gắn nhãn bởi con người. Chúng tôi xem xét bốn PLMs điển hình theo Abraham et al. (2022). Mô tả về các backbone PLM, tập dữ liệu, và nhãn khái niệm được trình bày chi tiết trong Phần 5.1, Phần 5.2, và Phụ lục G. Chúng tôi xem xét điểm số tác vụ mục tiêu và điểm số dự đoán khái niệm làm các thước đo đánh giá cho tiện ích và khả năng diễn giải, tương ứng.
CBM cho CBE-PLMs. Trong thí nghiệm này, chúng tôi nhằm xác định chiến lược huấn luyện tối ưu cho CBE-PLMs. Kết quả được mô tả trong Hình 2 xác nhận rằng các PLMs tiêu chuẩn thường cho ra điểm số tác vụ cao nhất, chứng minh rằng việc triển khai nút thắt khái niệm thực sự có thể ảnh hưởng tiêu cực đến hiệu suất tác vụ mục tiêu. Tuy nhiên, mà không xem xét các nhãn khái niệm, các PLMs tiêu chuẩn thiếu khả năng diễn giải. Ngược lại, CBE-PLMs được huấn luyện chung thể hiện điểm số tác vụ cao hơn và điểm số dự đoán khái niệm vượt trội so với các đối tác của chúng. Sự khác biệt này từ CBMs trong lĩnh vực hình ảnh

--- TRANG 4 ---
0 20 40 60 80 100
Concept Macro F1 (%)4050607080T ask Macro F1 (%)
(a) CEBaB0 20 40 60 80
Concept Accuracy (%)607080T ask Accuracy (%)
(b) IMDBLSTM-standard
GPT2-standardBERT-standard
RoBERT a-standardLSTM-independent
GPT2-independentBERT-independent
RoBERT a-independentLSTM-sequential
GPT2-sequentialBERT-sequential
RoBERT a-sequentialLSTM-joint
GPT2-jointBERT-joint
RoBERT a-jointHình 2: Minh họa về sự đánh đổi giữa khả năng diễn giải và độ chính xác sử dụng các backbone khác nhau. Phần trên-phải cho thấy sự đánh đổi thuận lợi hơn, tức là, một Pareto front khả năng diễn giải-tiện ích tốt hơn. Chúng tôi cũng cho thấy các khoảng tin cậy cho cả hai chiều.
lĩnh vực, nơi cả ba chiến lược đều hiển thị hiệu suất tương tự (Koh et al., 2020), là đáng chú ý. Chúng tôi quy cho điều này do việc huấn luyện trước rộng rãi của PLMs trên nhiều corpus do con người tạo ra và số lượng tham số lớn hơn so với các bộ mã hóa thị giác được nghiên cứu như ResNets (He et al., 2016). Không giống như huấn luyện độc lập hoặc tuần tự nơi bộ mã hóa PLM bị cố định sau khi huấn luyện trên các nhãn khái niệm, huấn luyện chung cho phép PLMs sử dụng khả năng của chúng để học các khái niệm và nhãn mục tiêu cùng nhau, làm cho các kích hoạt khái niệm đã học từ lớp nút thắt được căn chỉnh tốt hơn với các nhãn tác vụ. Với lợi thế của huấn luyện chung này, chúng tôi áp dụng nó làm chiến lược mặc định để huấn luyện CBE-PLMs trong các phần tiếp theo.
Trong khi những phát hiện ban đầu từ việc áp dụng CBM vanilla (Koh et al., 2020) để diễn giải PLMs có vẻ khuyến khích, chúng yêu cầu các khái niệm được chú thích bởi con người trong quá trình huấn luyện. Điều này chứng minh là không thực tế trong các tình huống thực tế do số lượng lớn các khái niệm tiềm năng và quá trình chú thích tốn thời gian (Németh et al., 2020). Thường thì, chỉ có một số lượng hạn chế văn bản có các khái niệm được gắn nhãn thủ công. Hơn nữa, khi con người liên tục thu được các khái niệm mới, việc khung huấn luyện tự động khám phá và kết hợp các khái niệm mới là mong muốn. Do đó, chúng tôi nhằm thiết kế một khung chung để huấn luyện CBE-PLMs.
4 C3M: Một Khung Chung để Học CBE-PLMs
Chúng tôi định nghĩa các phần dữ liệu sau đây theo các tình huống thực tế. Chúng tôi gọi một tập dữ liệu với các khái niệm được chú thích bởi con người là tập dữ liệu khái niệm nguồn, ký hiệu là Ds={(x(i), y(i), c(i)
s)ns
i=1}, trong đó ns biểu thị kích thước và cs∈Rks là một vector của ks khái niệm từ tập khái niệm nguồn được định trước Cs. Chúng tôi cũng xem xét một tập dữ liệu khác mà không có nhãn khái niệm, được gọi là tập dữ liệu khái niệm không gắn nhãn, ký hiệu là Du={(x(i), y(i))nu
i=1}. Tập dữ liệu hoàn chỉnh sau đó là sự kết hợp của hai tập dữ liệu này: D={Ds,Du}. ns và ks thường nhỏ, hạn chế hiệu quả của CBE-PLMs. Cụ thể, ns nhỏ dẫn đến các nhãn khái niệm thưa thớt trong D, và CBM vanilla không thể được huấn luyện trên các tập dữ liệu có các khái niệm không gắn nhãn Du. Ngoài ra, ks nhỏ cho thấy rằng chúng ta có thể không có đủ thông tin cho việc dự đoán mô hình.
Để giải quyết những hạn chế này, chúng tôi đề xuất Tăng cường Khái niệm được Hướng dẫn bởi ChatGPT với MixUp Cấp Khái niệm (C3M), một khung mới để huấn luyện CBE-PLMs hiệu quả. Như được minh họa trong Hình 3, ở mức cao, chúng tôi tăng cường tập khái niệm Cs và chú thích các nhãn khái niệm giả cho tập dữ liệu khái niệm không gắn nhãn sử dụng ChatGPT. Vì những nhãn giả này có nhiễu, chúng tôi đề xuất một MixUp cấp khái niệm mới để huấn luyện CBE-PLMs hiệu quả trên tập dữ liệu tăng cường với nhãn khái niệm nhiễu.
4.1 Tăng cường Khái niệm được Hướng dẫn bởi ChatGPT
Trong phần này, chúng tôi trình bày chi tiết cách tận dụng ChatGPT (GPT4) để tự động (1) tăng cường tập khái niệm, và (2) chú thích các nhãn khái niệm bị thiếu.
4.1.1 Tăng cường Tập Khái niệm
Mục tiêu của việc tăng cường tập khái niệm là tự động tạo ra các khái niệm chất lượng cao sử dụng các khái niệm được con người chỉ định Cs làm tham chiếu. Những khái niệm được tạo ra này nên có ý nghĩa về mặt ngữ nghĩa và hữu ích cho việc dự đoán tác vụ mục tiêu. Được truyền cảm hứng từ LF-CBM (Oikarinen et al., 2023), chúng tôi truy vấn ChatGPT với các prompt thích hợp để tạo ra các khái niệm bổ sung. Các prompt của chúng tôi được thiết kế sử dụng "in-context learning" (Brown et al., 2020; Min et al., 2022; Xie et al., 2022), và bao gồm các ví dụ từ chú thích của con người. Dưới đây là một ví dụ về prompt ChatGPT được thiết kế cho tác vụ phân loại cảm xúc sử dụng tập dữ liệu IMDB (Maas et al., 2011):

--- TRANG 5 ---
Ngoài {Acting ,Storyline ,Emotional Arousal ,
Cinematography }, còn có những đặc trưng quan trọng
bổ sung nào để đánh giá xem một {movie }có tốt
hay không?
Dấu ngoặc đại diện cho các trường có thể được
tùy chỉnh cho các tác vụ khác nhau. Các khái niệm
Acting ,Storyline ,Emotional Arousal , và
Cinematography là từ tập khái niệm nguồn
Cs với nhãn được chú thích thủ công theo
các thủ tục trong Phụ lục B. Khác với LF-
CBM chỉ tạo ra các khái niệm chỉ dựa vào
GPT3 (Brown et al., 2020), chúng tôi tiếp tục bao gồm
một tập nhỏ các khái niệm được con người chỉ định trong
prompt để cải thiện chất lượng của các khái niệm được tạo ra.
Thông tin bổ sung này có thể giúp lọc hiệu quả
đầu ra không mong muốn mà không cần các thao tác
bổ sung (ví dụ, xóa). Các khái niệm hiếm khi thấy
được loại bỏ sử dụng một ngưỡng được định trước và
các khái niệm được tạo ra còn lại được gọi là
tập khái niệm tăng cường Ca với kích thước ka. Kết quả
được đưa ra trong Bảng 5 ở Phụ lục G.
4.1.2 Chú thích Nhãn Khái niệm Nhiễu
Bước tiếp theo là tự động chú thích các khái niệm
không gắn nhãn sử dụng nhãn nhiễu. Chúng tôi một lần nữa tận dụng
sức mạnh của ChatGPT đã được chứng minh là bao gồm
một lượng đáng kể kiến thức thường thức của con người (Bommasani et al., 2022; OpenAI, 2023; Singh et al., 2023) và cho thấy hiệu suất mạnh
cho một số tác vụ chú thích văn bản (Gilardi
et al., 2023). Như chúng tôi cũng sẽ cho thấy ở đây, LLMs
có trình độ đáng ngạc nhiên trong việc xác định các khái niệm ngôn ngữ
khi được nhắc nhở một cách phù hợp. Sử dụng cùng
ví dụ về đánh giá phim, prompt cho bước này
được thiết kế như sau:
a. Theo đánh giá " {text 1}", 
"{concept 1}" của phim là "positive".
b. Theo đánh giá " {text 2}", 
"{concept 2}" của phim là "negative".
c. Theo đánh giá " {text 3}", 
"{concept 3}" của phim là "unknown".
d. Theo đánh giá " {texti}", 
"{concept i}" của phim như thế nào? Vui lòng trả lời với một
lựa chọn trong "positive, negative, or unknown".
Theo một chiến lược "in-context learning" tương tự
được mô tả trong Phần 4.1.1, Prompts a-c là ba
ví dụ được chú thích bởi con người được chọn ngẫu nhiên để
thể hiện nhãn khái niệm positive, negative, và unknown,
tương ứng. Prompt d là thể hiện truy vấn.
Mục tiêu là thu được nhãn nhiễu cho bất kỳ
{text i} và {concept i} đã cho. Có ba loại
chú thích khái niệm nhiễu:
1.Nhãn nhiễu cho các khái niệm được con người chỉ định trong
Ds. Tập dữ liệu kết quả ˜Ds được sử dụng để xác thực chất lượng của nhãn được tạo ra bởi ChatGPT chỉ
(Xem Bảng 4 trong Phụ lục F).
2.Nhãn nhiễu cho các khái niệm được ChatGPT tạo ra
trong Ds. Tập khái niệm tăng cường được ký hiệu là
csa = (cs||ca)∈Rks+ka, trong đó ||đề cập đến
toán tử nối và ca∈Rka đại diện cho
các khái niệm được tạo ra. Ví dụ, chúng tôi xác định
các khái niệm quan trọng mới như Soundtrack
sử dụng ChatGPT cho các đánh giá phim IMDB.
3.Nhãn nhiễu cho cả các khái niệm được con người chỉ định và
được ChatGPT tạo ra trong tập dữ liệu khái niệm không gắn nhãn Du. Tập khái niệm tăng cường
được ký hiệu là ˜csa= ( ˜cs||˜ca)∈Rks+ka và
˜cs∈Rks,˜ca∈Rka đại diện cho các nhãn khái niệm được tạo ra cho các khái niệm được con người chỉ định và được ChatGPT tạo ra, tương ứng.
Tóm lại, chúng tôi chuyển đổi tập dữ liệu gốc
với nhãn khái niệm thưa thớt thành một
tập dữ liệu tăng cường với các khái niệm mới và nhãn nhiễu: D=
{Ds,Du} → ˜D={˜Dsa,˜Du}. Ví dụ về hai
loại truy vấn này được minh họa trong Phụ lục J.
Hình 3: Minh họa về khung C3M được đề xuất.
4.2 Học từ Nhãn Khái niệm Nhiễu
Trong khi việc huấn luyện CBE-PLMs trực tiếp trên
tập dữ liệu đã chuyển đổi ˜D là đơn giản, nhược điểm của phương pháp này
là việc đối xử bình đẳng với các chú thích của con người
và nhãn nhiễu được ChatGPT tạo ra, có thể
dẫn đến các bất chính xác trong dự đoán và diễn giải. Để cải thiện khả năng diễn giải và
độ chính xác, chúng tôi giới thiệu một phương pháp MixUp
Cấp Khái niệm (CM) mới. Nó ủng hộ một hành vi lồi
của PLMs giữa các khái niệm được chú thích bởi con người và được ChatGPT tạo ra, từ đó tăng cường độ bền vững của nó
chống lại nhãn khái niệm nhiễu.

--- TRANG 6 ---
4.2.1 MixUp Cấp Khái niệm
Để sử dụng tốt hơn các nhãn khái niệm nhiễu, CM
đầu tiên nội suy tuyến tính các văn bản và nhãn khái niệm
giữa các khái niệm được chú thích bởi con người ( ˜Dsa) và
các khái niệm được ChatGPT tạo ra ( ˜Du). Cụ thể,
chúng tôi nội suy bất kỳ hai bộ ba văn bản-khái niệm-nhãn
(x(i), c(i), y(i)),(x(j), c(j), y(j)) cho cả biểu diễn tiềm ẩn ( z(i), z(j)), khái niệm ( c(i), c(j)),
và nhãn tác vụ ( y(i), y(j)) sử dụng MixUp ( ·)
được định nghĩa như sau:
λ∼Beta(α, α);ˆλ= max( λ,1−λ);
z(i)=fθ(xi);z(j)=fθ(xj);
ˆz(i,j)=ˆλz(i)+ (1−ˆλ)z(j);
ˆc(i,j)=ˆλc(i)+ (1−ˆλ)c(j);
ˆy(i,j)=ˆλy(i)+ (1−ˆλ)y(j),(1)
trong đó α là một siêu tham số cho phân phối Beta.
Đáng chú ý, ˆλ≥0.5 bảo tồn thứ tự của
các khái niệm được chú thích bởi con người và được ChatGPT tạo ra
để tính toán các thành phần mất mát riêng lẻ
trong Eq. (4) một cách thích hợp. Sau đó, chúng tôi kết hợp và
xáo trộn dữ liệu được chú thích bởi con người và được ChatGPT chú thích
trong tập dữ liệu đã chuyển đổi ˜D={˜Dsa,˜Du}:
W=Shuffle (˜D) =Shuffle (˜Dsa||˜Du),(2)
trong đó|| biểu thị việc nối hai phần
của tập dữ liệu. Tiếp theo, chúng tôi thực hiện MixUp ( ·) cho
thể hiện thứ i như sau:
(ˆz(i)
sa,ˆc(i)
sa,ˆy(i)
sa) =MixUp (˜D(i)
sa,W(i)),
(ˆz(i)
u,ˆc(i)
u,ˆy(i)
u) =MixUp (˜D(i)
u,W(i)).(3)
Thông qua những bước này, chúng tôi có thể tạo ra một "phiên bản trộn"
cho mỗi thể hiện trong ˜Dsa và ˜Du, trong khi
bảo tồn một phần lớn hơn của thể hiện gốc.
4.2.2 Hàm Mất mát
Hàm mất mát LjointMixUp để huấn luyện CBE-
PLMs với tập dữ liệu đã MixUp được định nghĩa dưới đây:
Lsa=Ljoint(ˆz(i)
sa,ˆc(i)
sa,ˆy(i)
sa);
Lu=Ljoint(ˆz(i)
u,ˆc(i)
u,ˆy(i)
u);
LjointMixUp =Lsa+τLu,(4)
trong đó τ là một siêu tham số và Ljoint là
mất mát huấn luyện chung được sử dụng trong CBM vanilla được công thức hóa trong Phụ lục A. Theo cách này, Chúng tôi lan truyền ngược gradient
của các nhãn khái niệm nhiễu trộn và nhãn khái niệm vàng
để cập nhật các tham số trong CBE-PLMs.5 Thí nghiệm
5.1 Tập dữ liệu
Trong phần này, chúng tôi đưa ra mô tả chi tiết về
các tập dữ liệu được thí nghiệm. Mỗi tập dữ liệu
có hai thành phần: tập dữ liệu khái niệm nguồn và
tập dữ liệu khái niệm không gắn nhãn ( D={Ds,Du}). Các tập dữ liệu hiện có với nhãn khái niệm được chú thích bởi con người
rất hạn chế. Một tập dữ liệu khái niệm nguồn là
CEBaB (Abraham et al., 2022; Wu et al., 2022), một
tập dữ liệu phân loại cảm xúc phổ biến cho đánh giá nhà hàng. Du tương ứng của nó là các đánh giá nhà hàng
từ Yelp Dataset1. Chúng tôi cũng sưu tập
một tập dữ liệu khác cho đánh giá phim. Cụ thể,
chúng tôi lấy mẫu ngẫu nhiên hai phần đánh giá từ
tập dữ liệu IMDB (Maas et al., 2011) để đại diện
cho Ds và Du, tương ứng. Theo một
công trình NLP trước đây (Cai et al., 2021), chúng tôi chú thích thủ công
các nhãn khái niệm cho Ds trong các đánh giá phim. Các chi tiết chú thích thêm được bao gồm trong Phụ lục B. Để thuận tiện, chúng tôi vẫn gọi hai
tập dữ liệu mới này là CEBaB và IMDB. Mỗi khái niệm chứa ba giá trị, tức là, Negative, Positive,
và Unknown. Như được mô tả trong Phần 4.1, mỗi
tập dữ liệu D sau đó được chuyển đổi thành ˜D={˜Dsa,˜Du}.
Thống kê cơ bản của các tập dữ liệu đã chuyển đổi và
các khái niệm được chú thích bởi con người của chúng được đưa ra trong
Bảng 3 trong Phụ lục E và Bảng 4 trong Phụ lục F,
tương ứng. Lưu ý rằng cột cuối cùng trong Bảng 4
cho biết độ chính xác của các khái niệm được ChatGPT gắn nhãn
trong Ds, như được mô tả trong Phần 4.1. Bảng 5
trong Phụ lục G cung cấp thống kê về các khái niệm tăng cường. Cả dữ liệu được chú thích bởi con người và được ChatGPT tạo ra, cùng với việc triển khai khung được phát hành2.
5.2 Backbone PLM
Chúng tôi thí nghiệm với các backbone PLM giống nhau
như trong bài báo CEBaB (Abraham et al., 2022):
GPT2 (Radford et al., 2019), BERT (Devlin et al.,
2018), RoBERTa (Liu et al., 2019), và BiL-
STM (Hochreiter and Schmidhuber, 1997) với
CBOW (Mikolov et al., 2013). Để có hiệu suất tốt hơn, chúng tôi thu được biểu diễn của các văn bản đầu vào
bằng cách gộp embedding của tất cả các token. Điểm số được báo cáo là trung bình của sáu lần chạy độc lập, mỗi lần mất 5 đến 40 phút. Các chi tiết triển khai thêm và giá trị tham số được bao gồm trong
Phụ lục C và Bảng 2 trong Phụ lục D.
1https://www.kaggle.com/datasets/omkarsabnis/
yelp-reviews-dataset
2https://github.com/Zhen-Tan-dmml/CBM_NLP.git

--- TRANG 7 ---
Bảng 1: So sánh độ chính xác tác vụ và khả năng diễn giải sử dụng tập dữ liệu CEBaB và IMDB. Các thước đo cho cả nhãn tác vụ và khái niệm được viết là Accuracy /Macro F1 . Điểm số được báo cáo bằng %. Điểm số in đậm cho biết rằng CBE-PLM dưới thiết lập hiện tại vượt trội hơn đối tác PLM tiêu chuẩn của nó. CM biểu thị MixUp Cấp Khái niệm.
Dataset CEBaB IMDB
ModelD ˜D D ˜D
Task Concept Task Concept Task Concept Task Concept
PLMsLSTM 40.57/60.67 - 43.34/64.47 - 68.25/53.37 - 90.5/90.46 -
GPT2 66.69/77.25 - 67.26/78.81 - 71.67/67.53 - 97.64/97.55 -
BERT 68.75/78.71 - 71.81/82.58 - 80.5/78.4 - 98.89/98.68 -
RoBERTa 71.36/80.17 - 73.12/82.64 - 84.1/82.5 - 99.13/99.12 -
CBE-PLMsLSTM 56.47/67.82 86.46/85.24 54.54/65.84 83.46/84.74 68.5/55.4 72.5/77.5 93.02/91.53 76.92/75.41
GPT2 64.04/77.75 92.14/92.05 63.57/74.71 90.17/90.13 70.05/69.53 80.6/82.5 96.85/96.81 86.14/88.06
BERT 67.27/79.24 93.65/92.75 68.23/78.13 89.64/90.45 77.42/74.57 80.2/83.7 97.62/97.58 92.57/92.05
RoBERTa 70.98/79.89 96.12/95.34 69.85/79.29 91.45/92.23 82.33/80.13 86.7/85.3 98.45/98.12 93.99/94.28
CBE-PLMs-CMLSTM - - 59.67/70.53 88.75/86.67 - - 94.35/92.32 83.83/84.52
GPT2 - - 65.54/77.87 93.58/92.32 - - 97.89/97.88 89.64/88.25
BERT - - 70.58/80.07 94.43/93.26 - - 98.18/98.06 94.87/94.32
RoBERTa - - 72.88/81.91 96.3/98.5 - - 99.69/99.66 96.35/96.36
5.3 Độ Chính xác Tác vụ so với Khả năng Diễn giải
Bảng 1 trình bày kết quả cho hai tập dữ liệu gốc
( D) và các phiên bản đã chuyển đổi của chúng ( ˜D).
Chúng tôi có các quan sát sau:
CBE-PLMs cung cấp khả năng diễn giải và hiệu suất dự đoán tác vụ cạnh tranh. So với
các PLMs tiêu chuẩn (được huấn luyện chỉ với nhãn tác vụ),
CBE-PLMs cung cấp khả năng diễn giải cấp khái niệm
chỉ với sự giảm nhỏ trong dự đoán tác vụ.
Thú vị là, một PLM nhỏ hơn, tức là, LSTM với
embeddings CBOW, đạt được độ chính xác tác vụ cải thiện
khi học từ nhãn khái niệm. Điều này gợi ý rằng sự đánh đổi giữa độ chính xác và khả năng diễn giải
trong việc học khái niệm là không cần thiết, trái với quan điểm phổ biến. Các khái niệm có thể giúp hướng dẫn PLMs
được huấn luyện trên các corpus nhỏ hơn với ít tham số hơn
hướng tới hiệu suất dự đoán tốt hơn.
Nhãn khái niệm nhiễu có thể tạo thuận lợi cho việc huấn luyện
CBE-PLMs trên các tập dữ liệu nhỏ. Kích thước cực kỳ
hạn chế của tập dữ liệu khái niệm nguồn IMDB (được
cố ý đặt thành 100) cho ra điểm số thử nghiệm thấp không đáng ngạc nhiên. Chuyển đổi D thành ˜D sử dụng ChatGPT cho
các thể hiện khái niệm có nhãn nhiễu dẫn đến những cải thiện đáng kể trong cả dự đoán khái niệm và tác vụ
cho CBE-PLMs-CM.
Việc học không phê phán từ nhãn khái niệm nhiễu
có thể làm giảm hiệu suất. Kết quả cho CEBaB trong
Bảng 1 chứng minh rằng, việc học từ tập dữ liệu đã chuyển đổi ˜D trực tiếp dẫn đến hiệu suất kém hơn
cho CBE-PLMs. Không giống như IMDB, tập dữ liệu khái niệm nguồn
trong CEBaB chứa đủ thể hiện huấn luyện, do đó, việc buộc CBE-PLMs
học từ nhãn khái niệm nhiễu sẽ dẫn hướng mô hình
một cách không mong muốn, làm xấu đi cả hiệu suất dự đoán khái niệm
và tác vụ.
CBE-PLMs-CM được huấn luyện thông qua khung
C3M được đề xuất nhất quán mang lại sự đánh đổi
khả năng diễn giải-tiện ích vượt trội. Bằng cách khuyến khích CBE-PLMs nội suy tuyến tính giữa
các ví dụ với khái niệm được gắn nhãn vàng và những ví dụ
với các khái niệm được ChatGPT tạo ra, mô hình có thể
trích xuất kiến thức ngữ nghĩa hữu ích
trong khi trở nên bền vững với nhãn khái niệm nhiễu. Kết quả là đầy hứa hẹn: Chúng tôi đạt được dự đoán cấp khái niệm tốt nhất (thước đo khả năng diễn giải) mà không hy sinh hiệu suất dự đoán tác vụ, và trong một số trường hợp, CBE-PLMs được huấn luyện thông qua C3M thậm chí có thể vượt trội hơn các đối tác PLM tiêu chuẩn của chúng.
5.4 Dự đoán Có thể Giải thích
Một lợi thế độc đáo của CBMs là các quy tắc quyết định của nó
có thể được diễn giải như một tổ hợp tuyến tính của
các biến có thể hiểu được (Koh et al., 2020). Kế thừa
sức mạnh này, CBE-PLMs được đề xuất của chúng tôi
có thể cung cấp các giải thích cấp khái niệm trực quan cho
dự đoán bằng cách đánh giá các kích hoạt của mỗi
khái niệm. Chúng tôi đo đóng góp khái niệm sử dụng
tích của kích hoạt và trọng số tương ứng
trong bộ dự đoán nhãn tuyến tính gϕ (Oikarinen et al.,
2023). Các khái niệm với kích hoạt tiêu cực được chỉ định
là "Neg Concept". Chúng tôi làm nổi bật các khái niệm
đóng góp nhiều nhất trong các trực quan hóa của chúng tôi. Kết quả trực quan hóa được thể hiện trong Hình 4 cho
một ví dụ mẫu, trong khi các nghiên cứu trường hợp thực tế CEBaB và IMDB có thể được tìm thấy trong Phụ lục H. Những
trực quan hóa này cung cấp những hiểu biết mới hấp dẫn vào
các ứng dụng thực tế. Ví dụ, các khái niệm tiêu cực (ví dụ, Service) đóng góp nhiều hơn vào dự đoán cuối cùng
của cảm xúc tích cực trong Hình 4, làm cho cảm xúc dự đoán cao thứ hai ( Y= 4)
thay vì cao nhất ( Y= 5). Hơn nữa, các kết quả khả năng diễn giải

--- TRANG 8 ---
như Hình 6 trong Phụ lục H ngụ ý rằng các khái niệm như "Food" và "Ambiance"
có trọng lượng nặng hơn trong đánh giá nhà hàng của khách hàng
so với "Noise" và "Menu Variety".
Hình 4: Minh họa về dự đoán có thể giải thích cho một
ví dụ mẫu trong phân tích cảm xúc đánh giá nhà hàng.
5.5 Can thiệp Thời gian Thử nghiệm
0 2 4 6 8 10
Number of Concepts Intervened2040608090T est Accuracy (%)
NI RI(W/O CM)
 RI OI
(a) BERT
0 2 4 6 8 10
Number of Concepts Intervened2040608090T est Accuracy (%)
NI RI(W/O CM)
 RI OI (b) GPT2
Hình 5: Kết quả của Can thiệp Thời gian Thử nghiệm. "NI"
biểu thị "no intervention", "RI (W/O CM)" biểu thị "random intervention on CBE-PLMs without the concept-
level MixUp", "RI" biểu thị "random intervention on
CBE-PLMs", và "OI" biểu thị "oracle intervention".
Một sức mạnh khác của CBE-PLMs là chúng cho phép can thiệp khái niệm thời gian thử nghiệm (được kế thừa từ
CBMs), tạo thuận lợi cho những tương tác sâu hơn, thân thiện với người dùng. Để đánh giá sức mạnh này, chúng tôi theo Koh et al.
(2020) để can thiệp vào các khái niệm dự đoán và
điều tra tác động của những can thiệp như vậy đối với độ chính xác dự đoán thời gian thử nghiệm. Dự đoán sai khái niệm
phát sinh từ nhãn không chính xác của ChatGPT hoặc kích hoạt khái niệm không chính xác. Nhớ lại rằng đầu vào của
bộ dự đoán nhãn tác vụ là các kích hoạt khái niệm dự đoán â=pϕ(fθ(x)) thay vì các khái niệm ternary dự đoán ĉ. Trong một can thiệp cấp khái niệm I, kích hoạt âj của khái niệm thứ j với
khái niệm mục tiêu cj được đặt thành phần trăm thứ 5, 95, hoặc 50
của âj qua phân phối huấn luyện cho
Negative, Positive, hoặc Unknown cj tương ứng.
Nhiều khái niệm có thể được can thiệp bằng cách thay thế tất cả các kích hoạt khái niệm dự đoán liên quan
và cập nhật dự đoán. Các thí nghiệm được
tiến hành trên phiên bản đã chuyển đổi ˜D của
tập dữ liệu CEBaB. Hình 5 thể hiện kết quả cho CBE-
PLMs sử dụng BERT và GPT2 làm backbone PLM (với các quan sát tương tự cho LSTM và RoBERTa). Một nghiên cứu trường hợp được minh họa thêm trong
Phụ lục I. Kết quả cho thấy rằng độ chính xác tác vụ
cải thiện đáng kể khi nhiều khái niệm được
sửa chữa bởi oracle. Ngoài ra, trong khi hiệu suất của CBE-PLMs suy giảm khi nhiều khái niệm
được can thiệp một cách không chính xác (ngẫu nhiên), MixUp cấp khái niệm được đề xuất hiệu quả giảm thiểu
tác động này. Đáng chú ý, sự suy giảm hiệu suất là
nhỏ khi chỉ có hai khái niệm bị can thiệp
một cách sai lầm. Những phát hiện này nhấn mạnh những lợi thế rõ rệt của can thiệp thời gian thử nghiệm
cho CBE-PLMs được huấn luyện thông qua C3M. Đầu tiên, các chuyên gia lĩnh vực có thể tương tác với mô hình để sửa chữa bất kỳ
giá trị khái niệm dự đoán không chính xác nào. Thứ hai, trong thực tế, thậm chí các chuyên gia cũng có thể vô tình thực hiện
các can thiệp không chính xác. Tuy nhiên, bất chấp sự
dễ bị tổn thương này, chiến lược MixUp cấp khái niệm được đề xuất của chúng tôi hiệu quả kiềm chế sự suy giảm hiệu suất, đặc biệt khi các bất chính xác chỉ ảnh hưởng đến một tập con nhỏ
của can thiệp. Điều này chứng thực cho độ bền vững
của khung được đề xuất.
6 Kết luận
Phân tích của chúng tôi bắt đầu với một cuộc kiểm tra toàn diện
của ba chiến lược huấn luyện, xác định huấn luyện chung
là hiệu quả nhất. Hơn nữa, chúng tôi đề xuất
khung C3M, được thiết kế để hợp lý hóa quá trình huấn luyện của CBE-PLMs khi có mặt
các nhãn khái niệm không đầy đủ. Hơn nữa, chúng tôi thể hiện khả năng diễn giải của các mô hình của chúng tôi trong
quá trình ra quyết định và làm sáng tỏ cách tính dễ hiểu này có thể được khai thác để tăng cường độ chính xác thử nghiệm thông qua can thiệp khái niệm.
Triển vọng: Nghiên cứu của chúng tôi đặt nền móng
cho các nghiên cứu tương lai tập trung vào việc tăng cường tính minh bạch và độ bền vững của PLMs. Chúng tôi dự báo rằng
CBE-PLMs có thể có khả năng cho thấy khả năng phục hồi
nhiều hơn đối với bias dữ liệu so với PLMs tiêu chuẩn, đã được biết đến là hiển thị hiệu suất bias
do tương quan giả mạo giữa các thuộc tính nhạy cảm (ví dụ, giới tính) và nhãn tác vụ (Wang and Culotta, 2021; Udomcharoenchaikit et al., 2022).
Ví dụ, một PLM bias có thể suy ra sai
các mẫu như người dùng nữ viết nhiều đánh giá cực đoan hơn trong khi người dùng nam có xu hướng hướng tới những đánh giá vừa phải.
CBE-PLMs, bằng cách tập trung vào nhãn khái niệm và
dựa hoàn toàn vào những khái niệm này cho phân loại,
có thể giảm những bias như vậy. Nếu các khái niệm không
liên quan đến các thuộc tính nhạy cảm và mối quan hệ của chúng với nhãn tác vụ là nhất quán, CBE-PLMs
có thể cung cấp tính công bằng được tăng cường.

--- TRANG 9 ---
Hạn chế
Trong khi phương pháp của chúng tôi trình bày một bước đáng kể
hướng tới các mô hình ngôn ngữ được huấn luyện trước có thể diễn giải hơn, một số hạn chế đáng để khám phá thêm.
Đầu tiên, phương pháp của chúng tôi phụ thuộc nhiều vào độ chính xác
của các khái niệm được định trước. Bất chấp những
kết quả đầy hứa hẹn, sự phụ thuộc này làm nảy sinh vấn đề về
bias tiềm năng có mặt trong quá trình lựa chọn khái niệm
(cho cả khái niệm được con người chỉ định và được ChatGPT tạo ra). Nếu một khái niệm không được định nghĩa rõ hoặc nếu
các khái niệm quan trọng bị thiếu, điều này có thể dẫn đến
các diễn giải không đầy đủ hoặc thiên lệch. Thứ hai, phương pháp luận được đề xuất trong bài báo này chưa được
thí nghiệm trên các mô hình ngôn ngữ rất lớn, như
Bloom (Scao et al., 2022). Ý tưởng cốt lõi của
khung này là sử dụng các mô hình ngôn ngữ lớn
(LLMs) để cung cấp giải thích cho các mô hình ngôn ngữ được huấn luyện trước (PLMs) có trọng lượng nhẹ hơn tương đối. Tuy nhiên, khung được đề xuất có tính
phổ quát và nên tương thích với bất kỳ PLMs nào. Các điều tra sử dụng PLMs lớn hơn được
dành cho các nỗ lực nghiên cứu tương lai. Thứ ba,
quá trình nhắc nhở các mô hình ngôn ngữ lớn để tạo ra nhãn khái niệm vẫn còn hơi giống như một nghệ thuật.
Trong khi chúng tôi đã đề xuất một phương pháp có hệ thống để
xây dựng các prompt mong muốn, hiệu suất của
mô hình vẫn có thể nhạy cảm với chất lượng và cấu trúc của những prompt này. Cuối cùng, trong khi phương pháp được đề xuất của chúng tôi cho thấy kết quả đầy hứa hẹn trong các tác vụ tiếng Anh, nó chưa được thử nghiệm rộng rãi trên
các ngôn ngữ khác. Điều này hạn chế khả năng áp dụng của nó trong
một thiết lập đa ngôn ngữ. Công trình tương lai nên mở rộng
phương pháp này sang các ngôn ngữ khác và tiến hành phân tích xuyên ngôn ngữ. Chúng tôi hy vọng nghiên cứu tương lai sẽ xây dựng trên công trình của chúng tôi để giải quyết những hạn chế này, đưa chúng ta đến gần hơn với các mô hình ngôn ngữ thực sự có thể diễn giải, có trách nhiệm và áp dụng phổ quát.
Tuyên bố Đạo đức
Trong việc tiến hành nghiên cứu này, chúng tôi tuân thủ nghiêm ngặt
Chính sách Đạo đức ACL. Tất cả dữ liệu được sử dụng trong công trình của chúng tôi
đều có sẵn công khai hoặc được ẩn danh, đảm bảo không có thông tin nhận dạng cá nhân nào
được liên quan. Công trình được trình bày trong bài báo này đóng góp đáng kể cho lĩnh vực xử lý ngôn ngữ tự nhiên và học máy. Bằng cách cải thiện
khả năng diễn giải của các mô hình ngôn ngữ được huấn luyện trước,
chúng tôi đang đóng góp vào việc tạo ra các hệ thống AI minh bạch và đáng tin cậy hơn. Sự tiến bộ này
dự kiến sẽ có tác động rộng rãi trên
nhiều lĩnh vực ngày càng dựa vào AI, bao gồm chăm sóc sức khỏe, giáo dục, kinh doanh, và tài chính, tăng cường các quy trình ra quyết định và tương tác của người dùng với hệ thống AI. Tuy nhiên, hiệu quả gia tăng của những mô hình này cũng có thể làm nảy sinh
các mối quan ngại xã hội tiềm năng nếu không được sử dụng có trách nhiệm.
Việc sử dụng sai các công nghệ NLP tiên tiến này
có thể dẫn đến vi phạm quyền riêng tư, sự lan truyền của thông tin sai lệch, hoặc sự khuếch đại các bias hiện có trong dữ liệu. Như với bất kỳ công nghệ mạnh mẽ nào,
việc xem xét các tác động đạo đức của nó
và quản lý việc triển khai của nó một cách cẩn thận để đảm bảo nó được
sử dụng cho sự cải thiện của xã hội là điều cần thiết. Công trình của chúng tôi cũng
nhấn mạnh nhu cầu nghiên cứu liên tục về
các chiến lược giảm thiểu bias tiềm năng trong hệ thống AI
và bảo vệ quyền riêng tư của người dùng. Là các nhà nghiên cứu, chúng tôi
cam kết làm việc hướng tới những mục tiêu này và thúc giục
những người sử dụng công nghệ này tuân thủ
các nguyên tắc tương tự.
Tài liệu tham khảo
Eldar D Abraham, Karel D'Oosterlinck, Amir Feder,
Yair Gat, Atticus Geiger, Christopher Potts, Roi Re-
ichart, and Zhengxuan Wu. 2022. Cebab: Estimating
the causal effects of real-world concepts on nlp model
behavior. Advances in Neural Information Process-
ing Systems , 35:17582–17596.
Yonatan Belinkov and James Glass. 2019. Analysis
methods in neural language processing: A survey.
Transactions of the Association for Computational
Linguistics , 7:49–72.
David Berthelot, Nicholas Carlini, Ian Goodfellow,
Nicolas Papernot, Avital Oliver, and Colin A Raf-
fel. 2019. Mixmatch: A holistic approach to semi-
supervised learning. Advances in neural information
processing systems , 32.
Steven Bills, Nick Cammarata, Dan Moss-
ing, Henk Tillman, Leo Gao, Gabriel Goh,
Ilya Sutskever, Jan Leike, Jeff Wu, and
William Saunders. 2023. Language mod-
els can explain neurons in language models.
https://openaipublic.blob.core.windows.
net/neuron-explainer/paper/index.html .
Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ
Altman, Simran Arora, Sydney von Arx, Michael S.
Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas
Card, Rodrigo Castellon, Niladri Chatterji, Annie
Chen, Kathleen Creel, Jared Quincy Davis, Dora
Demszky, Chris Donahue, Moussa Doumbouya,
Esin Durmus, Stefano Ermon, John Etchemendy,
Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor
Gale, Lauren Gillespie, Karan Goel, Noah Goodman,
Shelby Grossman, Neel Guha, Tatsunori Hashimoto,
Peter Henderson, John Hewitt, Daniel E. Ho, Jenny

--- TRANG 10 ---
Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil
Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth
Karamcheti, Geoff Keeling, Fereshte Khani, Omar
Khattab, Pang Wei Koh, Mark Krass, Ranjay Kr-
ishna, Rohith Kuditipudi, Ananya Kumar, Faisal Lad-
hak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle
Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma,
Ali Malik, Christopher D. Manning, Suvir Mirchan-
dani, Eric Mitchell, Zanele Munyikwa, Suraj Nair,
Avanika Narayan, Deepak Narayanan, Ben Newman,
Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan,
Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Pa-
padimitriou, Joon Sung Park, Chris Piech, Eva Porte-
lance, Christopher Potts, Aditi Raghunathan, Rob
Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani,
Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa
Sadigh, Shiori Sagawa, Keshav Santhanam, Andy
Shih, Krishnan Srinivasan, Alex Tamkin, Rohan
Taori, Armin W. Thomas, Florian Tramèr, Rose E.
Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai
Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan
You, Matei Zaharia, Michael Zhang, Tianyi Zhang,
Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn
Zhou, and Percy Liang. 2022. On the opportunities
and risks of foundation models.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Hongjie Cai, Rui Xia, and Jianfei Yu. 2021. Aspect-
category-opinion-sentiment quadruple extraction
with implicit aspects and opinions. In Proceedings
of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers) , pages 340–350.
Lu Cheng, Kush R Varshney, and Huan Liu. 2021. So-
cially responsible ai algorithms: Issues, purposes,
and challenges. Journal of Artificial Intelligence Re-
search , 71:1137–1181.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805 .
Shizhe Diao, Zhichao Huang, Ruijia Xu, Xuechun Li,
Yong Lin, Xiao Zhou, and Tong Zhang. 2022. Black-
box prompt learning for pre-trained language models.
arXiv preprint arXiv:2201.08531 .
Erik Englesson and Hossein Azizpour. 2021. Gener-
alized jensen-shannon divergence loss for learning
with noisy labels. Advances in Neural Information
Processing Systems , 34:30284–30297.
Andrea Galassi, Marco Lippi, and Paolo Torroni. 2020.
Attention in natural language processing. IEEE trans-
actions on neural networks and learning systems ,
32(10):4291–4308.Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli.
2023. Chatgpt outperforms crowd-workers for text-
annotation tasks. arXiv preprint arXiv:2303.15056 .
Yash Goyal, Amir Feder, Uri Shalit, and Been Kim.
2019. Explaining classifiers with causal concept ef-
fect (cace). arXiv preprint arXiv:1907.07165 .
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 770–
778.
Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation , 9(8):1735–
1780.
Been Kim, Martin Wattenberg, Justin Gilmer, Carrie
Cai, James Wexler, Fernanda Viegas, et al. 2018. In-
terpretability beyond feature attribution: Quantitative
testing with concept activation vectors (tcav). In In-
ternational conference on machine learning , pages
2668–2677. PMLR.
Evgeny Kim and Roman Klinger. 2018. Who feels what
and why? annotation of a literature corpus with se-
mantic roles of emotions. In Proceedings of the 27th
International Conference on Computational Linguis-
tics, pages 1345–1359.
Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen
Mussmann, Emma Pierson, Been Kim, and Percy
Liang. 2020. Concept bottleneck models. In Inter-
national Conference on Machine Learning , pages
5338–5348. PMLR.
Q Vera Liao and Jennifer Wortman Vaughan. 2023. Ai
transparency in the age of llms: A human-centered
research roadmap. arXiv preprint arXiv:2306.01941 .
Yang Liu, Hao Cheng, and Kun Zhang. 2022. Identifia-
bility of label noise transition matrix. arXiv preprint
arXiv:2202.02016 .
Yang Liu and Mirella Lapata. 2019. Text summariza-
tion with pretrained encoders. In Proceedings of
the 2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 3730–3740.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Max Losch, Mario Fritz, and Bernt Schiele. 2019.
Interpretability beyond classification output: Se-
mantic bottleneck networks. arXiv preprint
arXiv:1907.10882 .
Scott M Lundberg and Su-In Lee. 2017. A unified ap-
proach to interpreting model predictions. Advances
in neural information processing systems , 30.

--- TRANG 11 ---
Andrew Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of the 49th annual meeting of the associ-
ation for computational linguistics: Human language
technologies , pages 142–150.
Andreas Madsen, Siva Reddy, and Sarath Chandar. 2022.
Post-hoc interpretability for neural nlp: A survey.
ACM Computing Surveys , 55(8):1–42.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,
Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-
moyer. 2022. Rethinking the role of demonstra-
tions: What makes in-context learning work? arXiv
preprint arXiv:2202.12837 .
Saumitra Mishra, Bob L Sturm, and Simon Dixon. 2017.
Local interpretable model-agnostic explanations for
music content analysis. In ISMIR , volume 53, pages
537–543.
Jesse Mu and Jacob Andreas. 2020. Compositional
explanations of neurons. Advances in Neural Infor-
mation Processing Systems , 33:17153–17163.
Renáta Németh, Domonkos Sik, and Fanni Máté.
2020. Machine learning of concepts hard even
for humans: The case of online depression fo-
rums. International Journal of Qualitative Methods ,
19:1609406920949338.
Tuomas Oikarinen, Subhro Das, Lam M Nguyen, and
Tsui-Wei Weng. 2023. Label-free concept bottleneck
models. In The Eleventh International Conference
on Learning Representations .
OpenAI. 2023. Gpt-4 technical report.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin,
Alban Desmaison, Luca Antiga, and Adam Lerer.
2017. Automatic differentiation in pytorch. In
NeurIPS .
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Alexis Ross, Ana Marasovi ´c, and Matthew E Peters.
2021. Explaining nlp models via minimal con-
trastive editing (mice). In Findings of the Association
for Computational Linguistics: ACL-IJCNLP 2021 ,
pages 3840–3852.
Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon,
Matthias Gallé, et al. 2022. Bloom: A 176b-
parameter open-access multilingual language model.
arXiv preprint arXiv:2211.05100 .Manmeet Singh, Vaisakh SB, Neetiraj Malviya, et al.
2023. Mind meets machine: Unravelling gpt-4's cog-
nitive psychology. arXiv preprint arXiv:2303.11436 .
Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao
Zhang, Han Zhang, Colin A Raffel, Ekin Dogus
Cubuk, Alexey Kurakin, and Chun-Liang Li. 2020.
Fixmatch: Simplifying semi-supervised learning
with consistency and confidence. Advances in neural
information processing systems , 33:596–608.
Can Udomcharoenchaikit, Wuttikorn Ponwitayarat,
Patomporn Payoungkhamdee, Kanruethai Masuk,
Weerayut Buaphet, Ekapol Chuangsuwanich, and
Sarana Nutanong. 2022. Mitigating spurious cor-
relation in natural language understanding with coun-
terfactual inference. In Proceedings of the 2022 Con-
ference on Empirical Methods in Natural Language
Processing , pages 11308–11321.
Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov,
Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart
Shieber. 2020. Investigating gender bias in language
models using causal mediation analysis. Advances
in neural information processing systems , 33:12388–
12401.
Zhao Wang and Aron Culotta. 2021. Robustness to
spurious correlations in text classification via auto-
matically generated counterfactuals. In Proceedings
of the AAAI Conference on Artificial Intelligence ,
volume 35, pages 14024–14031.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. 2020. Hug-
gingface's transformers: State-of-the-art natural lan-
guage processing.
T Wu, M Tulio Ribeiro, J Heer, and D Weld. 2021.
Polyjuice: Generating counterfactuals for explaining,
evaluating, and improving models. In Joint Confer-
ence of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th Interna-
tional Joint Conference on Natural Language Pro-
cessing (ACL-IJCNLP 2021) .
Zhengxuan Wu, Karel D'Oosterlinck, Atticus Geiger,
Amir Zur, and Christopher Potts. 2022. Causal proxy
models for concept-based model explanations. arXiv
preprint arXiv:2209.14279 .
Sang Michael Xie, Aditi Raghunathan, Percy Liang,
and Tengyu Ma. 2022. An explanation of in-context
learning as implicit bayesian inference. In Interna-
tional Conference on Learning Representations .
Jie Yang, Yue Zhang, Linwei Li, and Xingxuan Li. 2017.
Yedda: A lightweight collaborative text span annota-
tion tool. arXiv preprint arXiv:1711.03759 .

--- TRANG 12 ---
Kayo Yin and Graham Neubig. 2022. Interpreting lan-
guage models with contrastive explanations. arXiv
preprint arXiv:2202.10419 .
Mateo Espinosa Zarlenga, Pietro Barbiero, Gabriele
Ciravegna, Giuseppe Marra, Francesco Giannini,
Michelangelo Diligenti, Frederic Precioso, Stefano
Melacci, Adrian Weller, Pietro Lio, et al. 2022. Con-
cept embedding models. In NeurIPS 2022-36th Con-
ference on Neural Information Processing Systems .
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and
David Lopez-Paz. 2017. mixup: Beyond empirical
risk minimization. arXiv preprint arXiv:1710.09412 .
Wenxuan Zhang, Xin Li, Yang Deng, Lidong Bing, and
Wai Lam. 2022. A survey on aspect-based senti-
ment analysis: tasks, methods, and challenges. IEEE
Transactions on Knowledge and Data Engineering .
Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,
Wengang Zhou, Houqiang Li, and Tieyan Liu. 2020.
Incorporating bert into neural machine translation. In
International Conference on Learning Representa-
tions .
A Định nghĩa về Các Chiến lược Huấn luyện
Cho một đầu vào văn bản x∈Rd, các khái niệm c∈Rk và
nhãn của nó y, các chiến lược để tinh chỉnh bộ mã hóa văn bản fθ, bộ chiếu pψ và bộ dự đoán nhãn
gϕ được định nghĩa như sau:
i) Tinh chỉnh vanilla một PLM: Các nhãn khái niệm
bị bỏ qua, và sau đó bộ mã hóa văn bản fθ và bộ dự đoán nhãn gϕ được tinh chỉnh như sau:
θ, ϕ= argmin
θ,ϕLCE(gϕ(fθ(x), y),
hoặc như sau (bộ mã hóa văn bản fθ bị đóng băng):
ϕ= argmin
ϕLCE(gϕ(fθ(x), y),
trong đó LCE biểu thị mất mát cross-entropy. Trong
công trình này chúng tôi chỉ xem xét lựa chọn đầu tiên vì
hiệu suất tốt hơn đáng kể của nó.
ii) Huấn luyện độc lập PLM với các nhãn khái niệm
và tác vụ: Bộ mã hóa văn bản fθ, bộ chiếu
pψ và bộ dự đoán nhãn gϕ được huấn luyện riêng biệt
với nhãn khái niệm ground truth và nhãn tác vụ
như sau:
θ, ψ= argmin
θ,ψLCE(pψ(fθ(x)), c),
ϕ= argmin
ϕLCE(gϕ(c), y).
Trong quá trình suy luận, bộ dự đoán nhãn sẽ sử dụng
đầu ra từ bộ chiếu thay vì các khái niệm ground-truth.iii) Huấn luyện tuần tự PLM với các nhãn khái niệm
và tác vụ: Chúng tôi đầu tiên học bộ mã hóa khái niệm
như chiến lược huấn luyện độc lập ở trên, và sau đó
sử dụng đầu ra của nó để huấn luyện bộ dự đoán nhãn:
ϕ= argmin
ϕLCE(gϕ(pψ(fθ(x), y).
iv) Huấn luyện chung PLM với các nhãn khái niệm và tác vụ: Học bộ mã hóa khái niệm và bộ dự đoán nhãn thông qua một tổng có trọng số Ljoint của hai mục tiêu
được mô tả ở trên:
θ, ψ, ϕ = argmin
θ,ψ,ϕLjoint(x, c, y )
= argmin
θ,ψ,ϕ[LCE(gϕ(pψ(fθ(x), y)
+γLCE(pψ(fθ(x)), c)].
Đáng chú ý rằng CBE-PLMs được huấn luyện chung
nhạy cảm với trọng số mất mát γ. Chúng tôi báo cáo
kết quả hiệu quả nhất ở đây, giá trị được thử nghiệm cho γ được
đưa ra trong Bảng 2 ở Phụ lục D.
B Chi tiết về Chú thích Khái niệm Thủ công
cho Tập dữ liệu IMDB
Chính sách chú thích của chúng tôi tuân theo một công trình
trước đây (Cai et al., 2021) để chú thích các tập dữ liệu NLP. Đối với tập dữ liệu IMDB, chúng tôi chú thích bốn
khái niệm (Acting, Stroyline, Emotional Arousal,
Cinematography) thủ công. Mặc dù các khái niệm được hiểu tự nhiên bởi con người, hai
sinh viên Thạc sĩ quen thuộc với phân tích cảm xúc
được chọn làm người chú thích để chú thích độc lập
với công cụ chú thích được giới thiệu bởi Yang
et al. (2017). Điểm F1 khớp tứ tự nghiêm ngặt
giữa hai người chú thích là 85.74%, điều này
cho thấy sự đồng thuận nhất quán giữa hai
người chú thích (Kim and Klinger, 2018). Trong trường hợp
bất đồng, một chuyên gia thứ ba sẽ được yêu cầu đưa ra
quyết định cuối cùng.
C Chi tiết Triển khai
Trong phần này, chúng tôi cung cấp thêm chi tiết về các thiết lập triển khai của các thí nghiệm của chúng tôi. Cụ thể, chúng tôi triển khai khung của chúng tôi với Py-
Torch (Paszke et al., 2017) và HuggingFace (Wolf
et al., 2020) và huấn luyện khung của chúng tôi trên một
GPU Nvidia A100 80GB duy nhất. Chúng tôi tuân theo một công trình
trước đây (Abraham et al., 2022) để triển khai backbone. Tất cả các mô hình backbone có số token tối đa
là 512 và kích thước batch là 8. Chúng tôi sử dụng
bộ tối ưu Adam để cập nhật backbone, bộ chiếu, và bộ dự đoán nhãn theo Phần 3.1.

--- TRANG 13 ---
Giá trị của các siêu tham số khác (Bảng 2 trong
Phụ lục D) cho từng loại PLM cụ thể được xác định thông qua grid search. Chúng tôi chạy tất cả các thí nghiệm trên GPU Nvidia A100 với 80GB RAM.
D Tham số và Ký hiệu
Trong phần này, chúng tôi cung cấp các ký hiệu được sử dụng trong
bài báo này cùng với mô tả của chúng để hiểu toàn diện. Chúng tôi cũng liệt kê các giá trị thí nghiệm
và tối ưu của chúng, như thể hiện trong Bảng 2.
E Thống kê Phân chia Dữ liệu
Thống kê và chính sách phân chia của các tập dữ liệu được thí nghiệm, bao gồm tập dữ liệu khái niệm nguồn Ds,
tập dữ liệu khái niệm không gắn nhãn Du, và các phiên bản tăng cường của chúng. Chi tiết cụ thể được trình bày
trong Bảng 3.
F Thống kê về Các Khái niệm Được Chú thích
bởi Con người
Thống kê về Các Khái niệm Được Chú thích bởi Con người trong
cả tập dữ liệu CEBaB và IMDB. Chúng tôi cũng bao gồm
độ chính xác của dự đoán khái niệm của ChatGPT ở đây.
Chi tiết cụ thể được trình bày trong Bảng 4.
G Thống kê về Các Khái niệm trong Tập dữ liệu
Đã Chuyển đổi
Thống kê và chính sách phân chia của các tập dữ liệu
đã chuyển đổi của các tập dữ liệu được thí nghiệm được trình bày
trong Bảng 5.
H Thêm Kết quả về Dự đoán Có thể Giải thích
Các nghiên cứu trường hợp về dự đoán có thể giải thích cho cả
tập dữ liệu CEBaB và IMDB được đưa ra trong Hình 6 và
Hình 7 tương ứng.
I Một nghiên cứu trường hợp về Can thiệp Thời gian
Thử nghiệm
Chúng tôi trình bày một nghiên cứu trường hợp về Can thiệp Thời gian
Thử nghiệm sử dụng một ví dụ từ dữ liệu khái niệm không gắn nhãn đã chuyển đổi ˜Du của tập dữ liệu CEBaB, như thể hiện
trong Hình 8. Hàng đầu tiên hiển thị các nhãn khái niệm mục tiêu
được tạo ra bởi ChatGPT. Hàng thứ hai
cho thấy các dự đoán từ mô hình CBE-PLM đã được huấn luyện,
mô hình này dự đoán sai hai khái niệm ("Waiting
time" và "Waiting area"). Hàng thứ ba chứng minh can thiệp thời gian thử nghiệm sử dụng ChatGPT làm
oracle, điều này sửa chữa các nhãn tác vụ dự đoán. Cuối cùng, hàng thứ tư triển khai can thiệp thời gian thử nghiệm
với oracle con người, sửa chữa khái niệm
mà ChatGPT ban đầu gắn nhãn sai.
J Ví dụ về Truy vấn ChatGPT
Trong bài báo này, chúng tôi truy vấn ChatGPT cho 1) tăng cường
tập khái niệm, và 2) chú thích các nhãn khái niệm bị thiếu. Lưu ý rằng trong thực tế, chúng tôi truy vấn ChatGPT
(GPT4) thông qua OpenAI API. Ở đây chúng tôi minh họa
các ví dụ từ GUI ChatGPT (GPT4) để minh họa tốt hơn. Các minh họa được đưa ra trong Hình 9
và Hình 10.

--- TRANG 14 ---
Bảng 2: Các tham số chính trong bài báo này với chú thích và giá trị được đánh giá của chúng. Lưu ý rằng các giá trị in đậm cho biết
các giá trị tối ưu.
Notations Specification Definitions or Descriptions Values
max_len - maximum token number of input 128 / 256 / 512
batch_size - batch size 8
plm_epoch - maximum training epochs for PLM and Projector 20
clf_epoch - maximum training epochs for the linear classifier 20
hidden_dim - hidden dimension size 128
emb_dim LSTM embedding dimension for LSTM 300
lrLSTM learning rate when the backbone is LSTM 1e-1 / 1e-2 / 5e-2 / 1e-3 / 1e-4
GPT2 learning rate when the backbone is GPT2 1e-3 / 5e-3/ 1e-4 / 5e-4/ 1e-5
BERT learning rate when the backbone is BERT 1e-4 / 5e-4/ 1e-5 / 3e-5/ 5e-5
RoBERTa learning rate when the backbone is RoBERTa 1e-4 / 5e-4/ 1e-5 / 3e-5/ 5e-5
\gamma - loss weight in the joint loss Ljoint 0.1 / 0.3 / 0.5/ 0.7 / 1.0
\tau - loss weight in the joint-MixUp loss LjointMixUp 0.1 / 0.5 / 1.0/ 1.5 / 2.0
Bảng 3: Thống kê về các tập dữ liệu được thí nghiệm. k biểu thị số lượng khái niệm.
DatasetDs Du˜Dsa˜DuTask
Train/Dev/Test k Train/Dev/Test k Train/Dev/Test k Train/Dev/Test k
CEBaB 1755/1673/1685 4 2000/500/500 0 1755/1673/1685 10 2000/500/500 10 5-way classification
IMDB 100/50/50 4 1000/1000/1000 0 100/50/50 8 1000/1000/1000 8 2-way classification
Bảng 4: Thống kê về các khái niệm được con người chỉ định trong Ds và độ chính xác của dự đoán khái niệm của ChatGPT.
Dataset (Ds) Concept Negative Positive Unknown Total ChatGPT Acc.
CEBaBFood 1693 (33.1%) 2087 (40.8%) 1333 (26.1%) 5113 77.9%
Ambiance 787 (15.4%) 994 (19.4%) 3332 (65.2%) 5113 69.2%
Service 1249 (24.4%) 1397 (27.3%) 2467 (48.2%) 5113 78.7%
Noise 645 (12.6%) 442 (8.6%) 4026 (78.7%) 5113 77.7%
IMDBActing 76 (38%) 66 (33%) 58 (29%) 200 73.0%
Storyline 80 (40%) 77 (38.5%) 43 (21.5%) 200 64.0%
Emotional Arousal 74 (37%) 73 (36.5%) 53 (26.5%) 200 60.5%
Cinematography 118 (59%) 43 (21.5%) 39 (19.4%) 200 66.5%
Hình 6: Minh họa về dự đoán có thể giải thích cho một ví dụ từ tập dữ liệu CEBaB.

--- TRANG 15 ---
Bảng 5: Thống kê về các khái niệm trong tập dữ liệu đã chuyển đổi ( ˜D). Các khái niệm được con người chỉ định được gạch chân. Các khái niệm hiển thị màu xám không được sử dụng trong thí nghiệm vì tỷ lệ nhãn "Unknown" quá lớn.
Dataset Concept Negative Positive Unknown Total
CEBaBFood 2043(25.2%) 4382(54.0%) 1688(20.8%) 8113
Ambiance 868(10.7%) 1659(20.4%) 5586(68.9%) 8113
Service 1543(19.0%) 2481(30.6%) 4089(50.4%) 8113
Noise 668(8.2%) 477(5.9%) 6968(85.9%) 8113
Cleanliness 55(0.7%) 610(7.5%) 7448(91.8%) 8113
Price 714(8.8%) 527(6.5%) 6872(84.7%) 8113
Location 303(3.7%) 2598(32.0%) 5212(64.2%) 8113
Menu Variety 238(2.9%) 2501(30.8%) 5374(66.2%) 8113
Waiting Time 572(7.1%) 608(7.5%) 6933(85.5%) 8113
Waiting Area 267(3.3%) 1136(14.0%) 6710(82.7%) 8113
Parking 53(0.7%) 107(1.3%) 7953(98.0%) 8113
Wi-Fi 9(0.1%) 39(0.5%) 8065(99.4%) 8113
Kids-Friendly 15(0.2%) 536(6.6%) 7562(93.2%) 8113
IMDBSentiment 1624(50.7%) 1576(49.2%) 0(0.0%) 3200
Acting 663(20.7%) 1200(37.5%) 1337(41.8%) 3200
Storyline 1287(40.2%) 1223(38.2%) 690(21.6%) 3200
Emotiona Arousal 1109(34.7%) 1136(35.5%) 955(29.8%) 3200
Cinematography 165(5.2%) 481(15.0%) 2554(79.8%) 3200
Soundtrack 107(3.3%) 316(9.9%) 2777(86.8%) 3200
Directing 537(16.8%) 850(26.6%) 1813(56.7%) 3200
Background Setting 288(9.0%) 581(18.2%) 2331(72.8%) 3200
Editing 304(9.5%) 240(7.5%) 2656(83.0%) 3200
Hình 7: Minh họa về dự đoán có thể giải thích cho một ví dụ từ tập dữ liệu IMDB.

--- TRANG 16 ---
Hình 8: Minh họa về dự đoán có thể giải thích cho một ví dụ từ dữ liệu khái niệm không gắn nhãn đã chuyển đổi ˜Du
của tập dữ liệu CEBaB. Hộp màu nâu với đường nét đứt chỉ ra can thiệp thời gian thử nghiệm trên các khái niệm tương ứng.
Hình 9: Minh họa về việc truy vấn ChatGPT để có thêm khái niệm cho tập dữ liệu IMDB.
Hình 10: Minh họa về việc truy vấn ChatGPT để chú thích nhãn khái niệm bị thiếu cho tập dữ liệu IMDB.
