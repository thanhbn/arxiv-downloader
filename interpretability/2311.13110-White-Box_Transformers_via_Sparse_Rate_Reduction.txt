# 2311.13110.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/interpretability/2311.13110.pdf
# File size: 37854139 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
White-Box Transformers via Sparse Rate Reduction
White-Box Transformers via Sparse Rate Reduction:
Compression Is All There Is?
Yaodong Yu†,⋆yyu@eecs.berkeley.edu
Sam Buchanan‡,⋆sam@ttic.edu
Druv Pai†,⋆druvpai@berkeley.edu
Tianzhe Chu†,♮chutzh@berkeley.edu
Ziyang Wu†zywu@berkeley.edu
Shengbang Tong†tsb@berkeley.edu
Hao Bai♯haob2@illinois.edu
Yuexiang Zhai†simonzhai@berkeley.edu
Benjamin D. Haeffele♭bhaeffele@jhu.edu
Yi Ma†,♢mayi@hku.hk, yima@eecs.berkeley.edu
†University of California, Berkeley
‡Toyota Technological Institute at Chicago
♮ShanghaiTech University
♯University of Illinois, Urbana-Champaign
♭Johns Hopkins University
♢University of Hong Kong
Abstract
In this paper, we contend that a natural objective of representation learning is to compress
and transform the distribution of the data, say sets of tokens, towards a low-dimensional
Gaussian mixture supported on incoherent subspaces. The goodness of such a representa-
tion can be evaluated by a principled measure, called sparse rate reduction , that simultane-
ously maximizes the intrinsic information gain and extrinsic sparsity of the learned represen-
tation. From this perspective, popular deep network architectures, including transformers,
can be viewed as realizing iterative schemes to optimize this measure. Particularly, we de-
rive a transformer block from alternating optimization on parts of this objective: the multi-
head self-attention operator compresses the representation by implementing an approximate
gradient descent step on the coding rate of the features, and the subsequent multi-layer
perceptron sparsifies the features. This leads to a family of white-box transformer-like deep
network architectures, named crate , which are mathematically fully interpretable. We
show, by way of a novel connection between denoising and compression, that the inverse
to the aforementioned compressive encoding can be realized by the same class of crate
architectures. Thus, the so-derived white-box architectures are universal to both encoders
and decoders. Experiments show that these networks, despite their simplicity, indeed learn
to compress and sparsify representations of large-scale real-world image and text datasets,
and achieve strong performance across different settings: ViT, MAE, DINO, BERT, and
GPT2. We believe the proposed computational framework demonstrates great potential in
.⋆Equal contribution.
1arXiv:2311.13110v4  [cs.LG]  6 Sep 2024

--- PAGE 2 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
bridging the gap between theory and practice of deep learning, from a unified perspective
of data compression. Code is available at: https://ma-lab-berkeley.github.io/CRATE .
2

--- PAGE 3 ---
White-Box Transformers via Sparse Rate Reduction
Contents
1 Introduction 5
1.1 The Representation Learning Problem . . . . . . . . . . . . . . . . . . . . . 5
1.2 Review of Existing Approaches . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.3 Goals and Contributions of This Work . . . . . . . . . . . . . . . . . . . . . 11
2 White-Box Encoding via Structured Lossy Compression 14
2.1 Desiderata and Objective of Representation Learning . . . . . . . . . . . . . 14
2.2 Learning Parsimonious Representations via Unrolled Optimization . . . . . 20
2.3 Self-Attention as Gradient Descent on Coding Rate of Tokens . . . . . . . 22
2.4 MLP as Proximal Gradient Descent for Sparse Coding of Tokens . . . . . . 24
2.5 The Overall White-Box Transformer Architecture: CRATE . . . . . . . . . 25
3 White-Box Decoding via Structured Denoising and Diffusion 27
3.1 Denoising-Diffusion against Low-Dimensional Structures . . . . . . . . . . . 28
3.2 Parsimony and Consistency via Structured Denoising-Diffusion . . . . . . . 31
3.3 Structured Denoising-Diffusion via Invertible Transformer Layers . . . . . . 33
4 Experimental Evaluations 35
4.1 Empirical Verification of CRATE on Many Practical Tasks . . . . . . . . . 36
4.1.1 Supervised Image Classification via ViT . . . . . . . . . . . . . . . . 36
4.1.2 Image Completion via Masked Autoencoding . . . . . . . . . . . . . 37
4.1.3 Self-Supervised Learning via DINO Training Method . . . . . . . . . 41
4.1.4 Pre-Training Language Models via BERT and GPT . . . . . . . . . 43
4.2 Analysis and Visualization of Learned CRATE Layers . . . . . . . . . . . . 47
4.3 Emergence of Semantic Properties in Learned CRATE Attention Maps . . . 50
4.3.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
4.3.2 Measuring the Emergence of Segmentation . . . . . . . . . . . . . . 51
4.3.3 Analysis of Segmentation in CRATE . . . . . . . . . . . . . . . . . . 52
5 Conclusions and Open Directions 54
A Technical Details for Section 2 57
A.1 Companion to Section 2.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
A.2 Companion to Section 2.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
A.2.1 Auxiliary Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
B Technical Details for Section 3 64
B.1 An Overview of Diffusion Processes . . . . . . . . . . . . . . . . . . . . . . . 64
B.2 Companion to Section 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
B.2.1 Auxiliary Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
B.3 Companion to Section 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
B.3.1 Key Auxiliary Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . 83
B.3.2 Concentration Inequalities for Our Setting . . . . . . . . . . . . . . . 86
B.3.3 Generic Concentration Inequalities . . . . . . . . . . . . . . . . . . . 90
3

--- PAGE 4 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
B.4 Companion to Section 3.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
C Additional Implementation Details and Experimental Results 95
C.1 Details about CRATE for Image Classification . . . . . . . . . . . . . . . . 95
C.2 Details about CRATE-MAE for Image Completion . . . . . . . . . . . . . . 96
C.3 Details about CRATE-DINO for Self-Supervised Learning . . . . . . . . . . 97
C.4 Details about CRATE-BERT and CRATE-GPT on Natural Language . . . 97
C.5 Ablation Study of CRATE on Image Classification . . . . . . . . . . . . . . 98
C.6 Ablation Study of ISTA Layer in CRATE . . . . . . . . . . . . . . . . . . . 101
C.7 Ablation Study of MSSA Layer and ISTA Layer in CRATE and Comparison
with ViT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
C.8 Additional Experimental Results of Layer-Wise Analysis . . . . . . . . . . . 103
C.9 Additional Experimental Results of Evaluating Compression and Sparsity for
ViT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
C.10 Details and Experimental Results of Attention Map Visualization . . . . . . 108
DPyTorch code for CRATE 110
D.1 PyTorch-Like Pseudocode for MSSA and ISTA Blocks . . . . . . . . . . . . 110
D.2 PyTorch-Like Pseudocode for CRATE Encoder . . . . . . . . . . . . . . . . 110
D.3 PyTorch-Like Pseudocode for CRATE Decoder . . . . . . . . . . . . . . . . 112
D.4 PyTorch-Like Pseudocode for CRATE Image Classifier . . . . . . . . . . . . 112
4

--- PAGE 5 ---
White-Box Transformers via Sparse Rate Reduction
1 Introduction
1.1 The Representation Learning Problem
In recent years, deep learning has seen tremendous empirical success in processing and
modeling massive amounts of high-dimensional and multi-modal data (Krizhevsky et al.,
2009; He et al., 2016; Radford et al., 2021; Chen et al., 2020; He et al., 2022). As argued by
Ma et al. (2022), much of this success is owed to deep networks’ ability in effectively learning
compressible low-dimensional structures in the data distribution and then transforming
the distribution to a parsimonious, i.e. compact and structured , representation. Such a
representation then facilitates many downstream tasks, e.g., in vision, classification (He
et al., 2016; Dosovitskiy et al., 2021), recognition and segmentation (Carion et al., 2020;
He et al., 2020; Kirillov et al., 2023), and generation (Karras et al., 2019; Rombach et al.,
2022; Saharia et al., 2022).
Representation learning via compressive encoding and decoding. To state the
common problem behind all these practices more formally, one may view a given dataset
as samples of a random vector xin a high-dimensional space, say RD. Typically, the
distribution of xhas much lower intrinsic dimension than the ambient space. Generally
speaking, by learning a representation , we typically mean to learn a continuous mapping,
sayf(·), that transforms xto a so-called feature vector zin another (typically lower-
dimensional) space, say Rd. It is hopeful that through such a mapping:
x∈RDf(x)− − − − → z∈Rd, (1)
the low-dimensional intrinsic structures of xare identified and represented by zin a more
compact and structured way so as to facilitate subsequent tasks such as classification or
generation. The feature zcan be viewed as a (learned) compact code for the original data
x, so the mapping fis also called an encoder . The fundamental question of representation
learning, then, and a central problem that we will address in this work, is:
What is a principled and effective measure for the goodness of representations?
Conceptually, the quality of a representation zdepends on how well it identifies the most
relevant and sufficient information of xfor subsequent tasks, and how efficiently it represents
this information. For long it was believed and argued that “sufficiency” or “goodness” of
a learned feature should be defined in terms of a specific task. For example, zjust needs
to be sufficient for predicting a class label yin a classification problem. To understand the
role of deep learning or deep networks in this type of representation learning, Tishby and
Zaslavsky (2015) proposed the information bottleneck framework, which suggests that a
measure of feature goodness is to maximize the mutual information between zandywhile
minimizing the mutual information between zandx.
Nevertheless, in recent years the predominant practice has been to learn first a task-
agnostic representation by pre-training a large deep neural network, in some cases known
as afoundation model (Bommasani et al., 2021). The so-learned representation can subse-
quently be fine-tuned for multiple specific tasks. This has been shown to be more effective
and efficient for many practical tasks across diverse data modalities, including speech (Rad-
ford et al., 2023), language (Brown et al., 2020), and natural images (Oquab et al., 2023).
5

--- PAGE 6 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Notice that representation learning in this context is very different from that for a specific
task, where zonly needs to be good enough for predicting a specific y. In a task-agnostic
setting, the learned representation zneeds to encode almost all essential information about
the distribution of the data x. That is, the learned representation znot only is a more com-
pact and structured representation for the intrinsic structures of x, but can also recover x
to a certain degree of faithfulness. Hence, it is natural to ask, in the task-agnostic context,
what a principled measure of goodness for a learned (feature) representation should be.1
Conceptually, we argue that one effective way, perhaps the only way, to verify whether
a representation zhas encoded sufficient information about xis to see how well we can
recover xfromzthrough an (inverse) mapping, say g, known as a decoder (or a generator):
x∈RDf(x)− − − − → z∈Rdg(z)− − − − → bx∈RD. (2)
As the encoder fis typically compressive and lossy, we should not expect the inverse
mapping to recover xexactly, but an approximate bx=g◦f(x)≈x. We normally seek
optimal encoding and decoding mappings such that the decoded bxis the closest to x,
either sample-wise—say, by minimizing the expected mean squared error—or in a relaxed
distributional sense. We refer to the above process as compressive encoding and decoding
orcompressive autoencoding . This idea is highly compatible with the original goals laid out
for autoencoders by Kramer (1991); Hinton and Zemel (1993), which can be viewed as a
generalization of the classic principal component analysis (Jolliffe, 2002) for the case where
the low-dimensional structure of xis linear.
Through tremendous empirical efforts over the last eleven years, it has become clear that
deep networks are very effective in modeling nonlinear encoding and decoding mappings.
Many applications of deep learning, including those mentioned above, rely on realizing
such an encoding or decoding scheme partially or entirely by learning forgseparately or
together. Although, conceptually, the decoder gshould be the “inverse” to the encoder f,
in practice it has never been clear how the architectures of encoder and decoder should be
related to each other. In many cases, the architectural design of the decoder has little to
do with that of the encoder, often chosen via empirical tests and ablations (for instance,
in masked autoencoders (He et al., 2021) and latent diffusion models (Esser et al., 2020;
Rombach et al., 2022)). We believe a good theoretical framework for representation learning
should clearly reveal relationships between architectures for the encoder and the decoder. We
strive to achieve this level of clarity in this work.
1.2 Review of Existing Approaches
Opening the black-box of modern deep networks through compression. Along
the development of deep learning, many deep network architectures have been proposed and
practiced for forg, from the classic LeNet (LeCun et al., 1998) to AlexNet (Krizhevsky
1. As we know, in recent practice of learning task-agnostic representations, one type of deep architectures,
known as transformers (Vaswani et al., 2017), have emerged as an almost universal choice for the backbone
of deep networks, for either discriminative or generative tasks, from language to vision. We will review
the details of this architecture momentarily. As we will see in this work, clarifying the principled measure
for feature goodness is also the key to fully understand why a transformer-like architecture is suitable for
task-agnostic pretraining, as well as to reveal the precise role and function of each layer in transformer-like
deep networks.
6

--- PAGE 7 ---
White-Box Transformers via Sparse Rate Reduction
Figure 1: Deep network layers fℓwhich optimize the rate reduction. The separate
components of the data distribution are transformed by the network operators to a configu-
ration which maximizes the information gain. Here, fmay be realized by a ReduNet (Chan
et al., 2022), in which each layer implements a gradient descent iteration for optimizing the
rate reduction.
et al., 2012), to ResNet (He et al., 2016) and then to the more recent transformer (Vaswani
et al., 2017). Despite their popularity, these networks have largely been designed empirically
and trained and used as “black-box” function approximators. As a result, desired properties
of the learned feature representation zare not clearly specified or justified, and many
heuristic measures or loss functions have been proposed and practiced for training task-
agnostic representations with these models.
The recent work of Yu et al. (2020); Chan et al. (2022) has attempted to provide a
principled framework that interprets the deep architectures of the ResNet and CNNs from
the perspective of optimizing a measure of “information gain” for the learned representation.
When the structured representation sought is a mixture of low-dimensional Gaussians, the
information gain can be precisely measured by the so-called coding rate reduction , denoted
as ∆R(z), and defined as the difference between the coding rates for the feature set as a
whole andthe coding rate for its structured components . It was shown that one can derive
from this objective a deep network architecture, known as the ReduNet (Yu et al., 2020;
Chan et al., 2022), that shares a striking resemblance to ResNets and CNNs. The layers
of a ReduNet are fully interpretable as realizing an iterative gradient descent method for
optimizing the coding rate reduction objective ∆ R(z), as in Figure 1:
f:xfpre
− − − → z1→ ··· → zℓfℓ
− − →zℓ+1→ ···fL
− − →zL+1=z, (3)
where fpreis some data pre-processing map, and
zℓ+1=fℓ(zℓ)≈zℓ+η∇
∆R(zℓ)
(4)
i.e., each layer ℓis constructed to incrementally optimize the ∆ R(zℓ) by taking an ap-
proximate gradient ascent step with step size η. We will refer to such a mathematically
interpretable network as a “white-box” deep network in the sense that the motivation and
structure of each network layer is well understood (i.e., as approximating an incremental
improvement of some desired objective function). Although rate reduction offers a good
theoretical framework for understanding architectures of existing deep networks such as
ResNets and CNNs, direct implementations of ReduNet have not yet generated competi-
tive practical performance on real-world datasets and tasks at scale. In this work, we will
7

--- PAGE 8 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
see how this outstanding gap between theory and practice2can be bridged through a gen-
eralization and improvement to the rate reduction objective such that its gradient descent
operator resembles the structure of a transformer layer, in such a way that the resulting
trasnformer-like architecture achieves competitive empirical performance.
Transformer models and compression. In recent years, transformers (Vaswani et al.,
2017) have emerged as the most popular, nearly universal, model of choice for the encoder f
and decoder gin learning representations for high-dimensional structured data, such as text
(Vaswani et al., 2017; Devlin et al., 2019; Brown et al., 2020), images (Dosovitskiy et al.,
2021; Dehghani et al., 2023), and other types of signals (Gong et al., 2023; Arnab et al.,
2021). In a nutshell, a transformer first converts each data point (such as a text corpus
or image) into a set or sequence of tokens , and then performs further processing on the
token sets, in a medium-agnostic manner (Vaswani et al., 2017; Dosovitskiy et al., 2021). A
cornerstone of the transformer model is the so-called (self-)attention layer , which exploits
the statistical correlations among the sequence of tokens to refine the token representa-
tion. Yet the transformer network architecture is empirically designed and lacks a rigorous
mathematical interpretation. In fact, the output of the attention layer itself has several
competing interpretations (Vidal, 2022; Li et al., 2023a; Sander et al., 2022; Geshkovski
et al., 2023). As a result, the statistical and geometric relationship between the data xand
the final representation zlearned by a transformer largely remains a mysterious black box.
Nevertheless, in practice, transformers have been highly successful in learning compact
representations that perform well on many downstream tasks. In particular, it serves as the
backbone architecture for the celebrated large language models (LLMs) such as OpenAI’s
GPT-4 (OpenAI, 2023b). Although the precise reason why it works well remains unclear,
it has been hypothesized by OpenAI’s researchers from a heuristic standpoint that the
transformer architecture in LLMs implicitly minimizes the Kolmogorov complexity of the
representations (Simons Institute, 2023), a quantitative notion of compression measured by
the length of the code that can generate the data in consideration. However, we know that
Kolmogorov complexity is largely a theoretical concept and in general not computationally
tractable for high-dimensional distributions. Hence, if transformers in LLMs indeed conduct
compression, they should be based on a measure of complexity that is amenable to tractable
and efficient computation. The design of Helmholtz machines (and Boltzman machines)
based on the minimum description length principle can be viewed as early attempts to
make compression computable (Hinton and Zemel, 1993). In this work, we argue that a
natural choice of this computable measure of compression behind transformers is precisely
a combination of rate reduction and sparsity of the learned representations. As we will see,
revealing such a measure could be the key to understand the transformer architecture.
Denoising-diffusion models and compression. Diffusion models (Sohl-Dickstein et al.,
2015; Ho et al., 2020; Song and Ermon, 2019; Song et al., 2021b,a) have recently become
a popular method for learning high-dimensional data distributions, particularly of natural
images, which are known to be highly structured in a manner that is notoriously difficult
to model mathematically (Ruderman, 1994; Wakin et al., 2005; Donoho and Grimes, 2005).
The core concept of diffusion models is to start with features zsampled from a Gaussian
2. The gap between theory and practice is not just characteristic of the rate reduction framework. The
situation is as dire for all theoretical frameworks ever proposed for understanding deep networks.
8

--- PAGE 9 ---
White-Box Transformers via Sparse Rate Reduction
Figure 2: Distribution flow in denoising-diffusion models. Starting with generic noise z=ez0,
the probability density of intermediate iterates is shaped towards the true distribution of ezLlocally
and iteratively through the operators gℓ, which use the score function ∇logqℓat each layer ℓ.
noise distribution (or some other standard template) and denoise and deform the feature
distribution until it converges to the original data distribution, which often has low intrin-
sic dimension. This process is computationally intractable if modeled at just a single scale
of noise (Koehler et al., 2023; Chen et al., 2023b; Bovier et al., 2005; Qin and Risteski,
2023), so it is typically broken into multiple incremental steps that denoise iteratively, as
in Figure 2:
g:z=ez0→ez1→ ··· → ezℓgℓ
− − →ezℓ+1→ ··· → ezLgpost
− − − − → bx, (5)
where gpostis a data post-processing map, and
ezℓ+1=gℓ(ezℓ) =ezℓ+τ∇logqℓ(ezℓ), (6)
where qℓis the density of ezℓ, i.e., the density of ezLafter corruption with the ℓ-th scale of
Gaussian noise, and ∇logqℓis the so-called score function (Hyv¨ arinen, 2005), or equiva-
lently an estimate for the “optimal denoising function” for qℓ(Efron, 2011a). In practice,
the score function is modeled using a generic black-box deep network.3Diffusion models
have shown effectiveness at learning and sampling from the data distribution (Karras et al.,
2022; Chen et al., 2023a; Rombach et al., 2022). However, despite some recent efforts (Song
et al., 2023), they generally do not establish any clear correspondence between the initial
features and data samples. Hence, diffusion models themselves do not offer a parsimo-
nious or interpretable representation of the data distribution. Yet, conceptually, the above
iterative denoising process (5) is compressing the feature distribution onto a targeted low-
dimensional data distribution. In this work, we will show that if one were to compress and
transform a distribution onto a standard mixture of (low-dimensional) Gaussians, the asso-
ciated optimal denoising function takes an explicit form that is similar to the gradient of the
rate reduction and to a transformer layer. This provides a path to take a transformer-like
encoder fdesigned to compress the data distribution into a parsimonious and structured
representation, and derive its distributional inverse through a process analogous to (5),
yielding a white-box architecture for compressive autoencoding.
3. The score function ∇logqℓbetween two layers is typically learned by fitting relationships between ezℓ
andezℓ+1, the data distribution at successive scales of corruption by Gaussian noise, from a large number
of samples with a black-box deep network designed for denoising.
9

--- PAGE 10 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Low-dimensionality promoting measures: sparsity and rate reduction. In both
of the previous popular methods, transformers and denoising-diffusion models, a representa-
tion was learned implicitly as a byproduct of solving a downstream task (e.g., classification
or generation/sampling) using deep networks. The networks used are typically chosen em-
pirically. Therefore, it is difficult to rigorously ensure or impose any desired properties for
the learned representation, except by trial and error. However, complementary to these
popular empirical practices, a line of research has attempted to explicitly learn a desired
representation of the data distribution as a task in and of itself; this is most commonly
done by trying to explicitly identify and represent low-dimensional structures in the input
data. Classical examples of this paradigm include model-based approaches such as sparse
coding (Olshausen and Field, 1997; Chen et al., 2018) and dictionary learning (Aharon
et al., 2006; Spielman et al., 2012; Gribonval et al., 2015; Zhai et al., 2020b), out of which
grew early attempts at designing and interpreting deep network architectures as learning
a sparse representation (Papyan et al., 2018; Bruna and Mallat, 2013). More recent ap-
proaches build instead from a model-free perspective, where one learns a representation
through a sufficiently-informative pretext task such as compressing similar and separating
dissimilar data via contrastive learning (Tian et al., 2020; Wang et al., 2022; Bardes et al.,
2022; Shwartz-Ziv and LeCun, 2023). Compared to black-box deep learning approaches,
both model-based and model-free representation learning schemes have the advantage of be-
ing more interpretable: they allow users to explicitly design desired properties of the learned
representation z. To a large extent, the rate reduction framework (Yu et al., 2020; Chan
et al., 2022; Pai et al., 2023) strikes a good balance between the above model-based and
model-free methods. Like contrastive learning, it aims to identify the data distribution by
compressing similar/correlated data and separating dissimilar/uncorrelated data (Yu et al.,
2020). Meanwhile, like the model-based methods, it actively maps the data distribution to
a family of desired representations, say a mixture of low-dimensional Gaussians (Ma et al.,
2007; Vidal et al., 2016).
Unrolled optimization: a unified paradigm for network interpretation & design.
As we have discussed above, low-dimensionalty promoting measures, such as sparsity or
coding rate reduction, allow users to construct white-box deep network architectures (Gre-
gor and LeCun, 2010; Chan et al., 2022) in a forward-construction fashion by unrolling an
optimization strategy for the chosen objective of the representations , such that each layer
of the constructed network implements an iteration of the optimization algorithm (Gregor
and LeCun, 2010; Chan et al., 2022; Tolooshams and Ba, 2022). In his recent work, Hinton
(2022) has also begun to hypothesize that the role of a deep network, with its forward
pass, is likely to optimize certain feature goodness layer-wise. In this paradigm, the most
challenging question is:
What fundamental measure of goodness for the representations is a deep network
trying to optimize in its forward pass?
In the unrolled optimization paradigm, if the desired objectives are narrowly defined, say
promoting sparsity alone (Papyan et al., 2018; Bruna and Mallat, 2013), it has so far
proved difficult to arrive at network architectures that can achieve competitive practical
performance on large real-world datasets. Other work has attempted to derive empirically-
designed popular network architectures through unrolled optimization on a reverse-engineered
10

--- PAGE 11 ---
White-Box Transformers via Sparse Rate Reduction
learning objective for the representation, such as Yang et al. (2022); Hoover et al. (2023);
Weerdt et al. (2023). In this case, the performance of the networks may remain intact,
but the reverse-engineered representation learning objective is usually highly complex and
not interpretable, and the properties of the optimal representation—or indeed the actually-
learned representation—remain opaque. Such approaches do not retain the key desired
benefits of unrolled optimization. As we will argue in this work, to measure the goodness of
a learned representation in terms of its intrinsic compactness and extrinsic simplicity, it is
crucial to combine the measure of sparsity (Papyan et al., 2018; Bruna and Mallat, 2013)
and that of coding rate reduction (Yu et al., 2020; Chan et al., 2022) . As we will see, this
combination will largely resolve the aforementioned limitations of extant methods that rely
solely on sparsity or solely on rate reduction.
1.3 Goals and Contributions of This Work
From the above discussion, we can observe that there has been an outstanding wide gap
between the practice and theory of representation learning via deep networks. The fast ad-
vancement in the practice of deep learning has been primarily driven by empirical black-box
models and methods that lack clear mathematical interpretations or rigorous guarantees.
Yet almost all existing theoretical frameworks have only attempted to address limited or
isolated aspects of practice, or only proposed and studied idealistic models that fall far short
of producing practical performance that can compete with their empirical counterparts.
Bridging the gap between theory and practice. Therefore, the primary goal of this
work is to remedy this situation with a more complete and unifying framework that has
shown great promise in bridging this gap between theory and practice. On one hand, this
new framework is able to provide a unified understanding of the many seemingly disparate
approaches and methods based on deep networks, including compressive encoding/decoding
(or autoencoding), rate reduction, and denoising-diffusion. On the other hand, as we will
see, this framework can guide us to derive or design deep network architectures that are not
only mathematically fully interpretable but also obtain competitive performance on many
learning tasks on large-scale real-world image or text datasets.
A theory of white-box deep networks. More specifically, we propose a unified objec-
tive, a principled measure of goodness, for learning compact and structured representations.
For a learned representation, this objective aims to optimize both its intrinsic complexity in
terms of coding rate reduction and its extrinsic simplicity in terms of sparsity. We call this
objective the sparse rate reduction , specified later in (15) and (17). The intuition behind
this objective is illustrated in Figure 3. To optimize this objective, we propose to learn a
sequence of incremental mappings that emulate unrolling certain gradient-descent-like iter-
ative optimization scheme for the objective function. As we will see, this naturally leads
to a transformer-like deep network architecture that is entirely a “white box” in the sense
that its optimization objective, network operators, and learned representation are all fully
interpretable mathematically. We name such a white-box deep architecture “ crate ,” or
“crate -Transformer,” short for a Coding- RATE transformer. We also show mathemat-
ically that these incremental mappings are invertible in a distributional sense, and their
inverses consist of essentially the same class of mathematical operators. Hence a nearly
11

--- PAGE 12 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Figure 3: The optima of the sparse rate reduction. After pre-processing input data Xinto
a sequence of tokens Z1, our crate network attempts to optimize the sparse rate reduction of the
token features Z=ZL+1. The optimal representations, according to the sparse rate reduction
objective, are linearized —having low-dimensional linear subspace structure— sparse —where the
subspaces are axis-aligned—and compressed —adhering closely to that structure, with low or no
noise. In the sequel, we discuss how crate achieves such representations via constructing each layer
to iteratively optimize the sparse rate reduction.
identical crate architecture can be used for realizing encoders, decoders, or together for
auto-encoders.
Practice of white-box deep networks. To show that this framework can truly bridge
the gap between theory and practice, we have conducted extensive experiments on both im-
age and text data to evaluate the practical performance of the crate model on a wide range
of learning tasks and settings that conventional transformers have demonstrated strong per-
formance. Surprisingly, despite its conceptual and structural simplicity, crate has demon-
strated competitive performance with respect to its black-box counterparts on alltasks and
settings, including image classification via supervised learning (Dosovitskiy et al., 2021),
unsupervised masked completion for imagery and language data (He et al., 2022; Devlin
et al., 2019; Liu et al., 2019), self-supervised feature learning for imagery data (Caron et al.,
2021), and language modeling via next-word prediction (Radford et al., 2018). Moreover,
thecrate model demonstrates additional practical benefits: each layer and network op-
erator statistically and geometrically meaningful, the learned model is significantly more
interpretable compared to black-box transformers, and the features show semantic meaning,
i.e., they can be easily used to segment an object from its background and partition it into
shared parts.
Note that with limited resources, in this work we do not strive for state-of-the-art
performance on all of the aforementioned tasks, which would require heavy engineering or
extensive fine-tuning; nor can we implement and test our models at current industrial scales.
Overall, our implementations for these tasks are basic and uniform, without significant task-
specific customization. Nevertheless, we believe these experiments have convincingly verified
that the derived white-box deep network crate model is universally effective and sets a
solid baseline for further engineering development and improvement.
Outline of the paper:
•In Section 2.1, we give a formal formulation for representation learning, both con-
ceptually and quantitatively. We argue that a principled measure of goodness for
a learned feature representation is the so-called sparse rate reduction that simulta-
12

--- PAGE 13 ---
White-Box Transformers via Sparse Rate Reduction
neously characterizes the representation’s intrinsic information gain and its extrinsic
sparsity. In Section 2.2, we contend that the fundamental role of a deep network is
to optimize such an objective by unrolling an iterative optimization scheme such as
gradient descent.
•From Section 2.3 to Section 2.5, we show that a transformer-like deep architecture
can be derived from unrolling an alternating minimization scheme for the sparse rate
reduction objective. In particular, in Section 2.3 we derive a multi-head self-attention
layer as an unrolled gradient descent step to minimize the lossy coding rate of the
token set with respect to a (learned) low-dimensional Gaussian mixture codebook. In
Section 2.4 we show that the multi-layer perceptron which immediately follows the
multi-head self-attention in transformer blocks can be interpreted as (and replaced by)
a layer which constructs a sparse coding of the token representations. This creates a
new white-box, i.e., fully mathematically interpretable, transformer-like architecture
called crate , summarized in Section 2.5, where each layer performs a single step of
an alternating minimization algorithm to optimize the sparse rate reduction objective.
•In Section 3 we reveal a fundamental connection between compression via rate re-
duction and the diffusion-denoising process for learning a representation for the data
distribution. In particular, we show that if one denoises the tokens towards a family
of low-dimensional subspaces, the associated score function assumes an explicit form
similar to a self-attention operator seen in transformers. We also establish that the
gradient descent of rate reduction essentially conducts structured denoising against
the (learned) low-dimensional Gaussian mixture model for the tokens. This con-
nection allows us to construct a white-box decoder based on a structured diffusion
process, as a distributional inverse to the structured denoising process implemented
by the crate encoder. One can show that the decoder essentially shares the same ar-
chitecture as the encoder, and they together form a symmetric white-box autoencoder
that is fully mathematically interpretable.
•In Section 4 we provide extensive experimental results to show that the crate net-
works, despite being simple and often smaller, can already learn the desired com-
pressed and sparse representations on large-scale real-world datasets, all while achiev-
ing performance on par with seasoned transformer networks on a wide variety of
popular tasks and settings, including ViT for image classification, MAE for image
completion, DINO for image segmentation with self-supervised learning, and BERT
and GPT for text completion and prediction. In addition, we demonstrate, both
qualitatively and quantitatively, that the internal representations of crate are more
interpretable than vanilla vision transformers trained on image classification.
At the end of the paper, in Appendices A to C, we provide adequate technical details and
experimental details for the above sections, to ensure that all our claims in the main body
are verifiable and experiments are reproducible. Appendix D gives PyTorch-like pseudocode
for our implementation of crate .
13

--- PAGE 14 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
2 White-Box Encoding via Structured Lossy Compression
In this section, we provide a technical formulation and justification for our new framework
and approach. To wit, we provide a (gentle yet) complete derivation from first principles
of our white-box transformer approach. While being a self-contained introduction to our
framework, and providing a transparently interpretable transformer-like deep network ar-
chitecture, it also foreshadows several connections between previously disparate technical
approaches to representation learning. These we make clear in the next Section 3 en route
to extending our technical framework to autoencoding.
Notation. We consider a general learning setup associated with real-world signals. We
have some random variable X=
x1, . . . ,xN
∈RD×Nwhich is our data source; each
xi∈RDis interpreted as a token4, there are Ntokens xiin each data sample X, and
thexi’s may have arbitrary correlation structures. To obtain a useful representation of the
input, we learn an encoder mapping f:RD×N→Rd×n. The features—that is, the output
of the encoder—are denoted by the random variable Z.=f(X).=
z1, . . . ,zn
∈Rd×n,
whence each zi∈Rdis a feature vector. The number of features nis typically the same
as the number of tokens N, or not much more (e.g., due to pre-processing), in which
case there is a natural correspondence between feature vectors ziand tokens xi. In the
auto-encoding context, we also learn a decoder mapping g:Rd×n→RD×N, such that
X≈cX.=g(Z).=bx1, . . . ,bxN
, whence each bxi∈RDis the auto-encoding of token xi.
As we have alluded to before, a central question we want to answer in this work is the
purpose of such an encoder and decoder in representation learning: namely, how should we
design the encoder and decoder mappings to optimize a representation learning objective?
As we will see, one specific form of the encoder fand the decoder g, that can be naturally
deduced through iterative optimization of the objective, is composed of multiple basic op-
erators, also known as layers in the language of deep neural networks. In such cases, we
write f=fL◦ ··· ◦ f1◦fpreandg=gpost◦gL−1◦ ··· ◦ g0, where fℓ:Rd×n→Rd×n
andgℓ:Rd×n→Rd×nare the ℓthlayer of the encoder and decoder respectively, and
fpre:RD×N→Rd×nandgpost:Rd×n→RD×Nare the pre- and post-processing layers
respectively. The input to the ℓthlayer of the encoder is denoted Zℓ.=
zℓ
1, . . . ,zℓ
n
∈Rd×n,
and the input to the ℓthlayer of the decoder is denoted eZℓ.=ezℓ
1, . . . ,ezℓ
n
∈Rd×n. In
particular, Zℓ+1=fℓ(Zℓ) andeZℓ+1=gℓ(eZℓ). Figure 4 depicts this overall process.
2.1 Desiderata and Objective of Representation Learning
Representation learning via the principle of parsimony and consistency. Fol-
lowing the framework of rate reduction (Chan et al., 2022), we contend that the goal of
representation learning is to find a feature mapping f:X∈RD×N→Z∈Rd×nwhich
transforms input data X∈RD×Nwith a potentially nonlinear and multi-modal distribu-
tion to a parsimonious feature representation Z∈Rd×n(Ma et al., 2022). As in Ma et al.
(2022), a complete desiderata for the learned representations ought to be:
4. For language transformers, tokens roughly correspond to words (Vaswani et al., 2017), while for vision
transformers, tokens correspond to image patches (Dosovitskiy et al., 2021).
14

--- PAGE 15 ---
White-Box Transformers via Sparse Rate Reduction
............
Figure 4: The autoencoding process to be studied in Sections 2 and 3 . Each encoder layer
fℓand decoder layer gL−ℓare (partial) inverses of each other. Moreover, the overall representation
Z=f(X) is parsimonious ( compressed ,linearized , and sparse , as in Section 2.1), and the
autoencoding is to be consistent in the sense that X≈cX.
1.Compressed : being strictly distributed according to some standard low-dimensional
structures matching the intrinsic low-dimensionality of the data, so as to ensure a
compact encoding of the data.
2.Linearized : the low-dimensional structures have (piecewise) linear geometry, so as
to aid interpolation and extrapolation in the representation space.
3.Sparse : the low-dimensional structures corresponding to different parts of the data
distribution are statistically incoherent or geometrically orthogonal , and also axis-
aligned , so as to ensure a more compact encoding and aid downstream processing.
4.Consistent : for autoencoding/generative purposes, we desire that the learned rep-
resentation is invertible , in the sense that we can decode features to recover the cor-
responding input data, either on the level of individual samples or distribution-wise.
For the last item, specifically, we would also like to learn an inverse mapping: g:Z∈
Rd×n→cX∈RD×Nsuch that cXandXare quantitatively close in some sense. Figure 4
illustrates the overall process and the desired four goals of such a representation learning.
In this section (Section 2), we will mainly show how to achieve the first three items on this
list by developing an encoding scheme; we will address the last item in the next section
(Section 3) by showing how the proposed encoding scheme can be naturally reversed.
An objective which promotes parsimonious representations. Previously, Yu et al.
(2020) have proposed to obtain parsimonious representations via maximizing the informa-
tion gain (Ma et al., 2022), a principled measure of the information content of the features.
A concrete instantiation of the information gain is the coding rate reduction (Yu et al.,
2020) of the features, i.e.,
∆R(Z|Π[K]) =R(Z)−Rc(Z|Π[K]). (7)
The first term R(Z) in the above expression is an estimate of the lossy coding rate (i.e.,
rate distortion function ) for the whole set of features, when using a codebook adapted to
Gaussians. More specifically, if we view the token feature vectors ( zi)i∈[n]inZ∈Rd×nas
15

--- PAGE 16 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
i.i.d. samples from a single zero-mean Gaussian, an approximation of their (lossy) coding
rate, subject to quantization precision ϵ >0, is given in (Ma et al., 2007) as:
R(Z).=1
2logdet( I+αZ∗Z) =1
2logdet( I+αZZ∗),where α.=d
nϵ2. (8)
The second term Rcin the rate reduction objective (7) is also an estimate of the lossy
coding rate, but under a different and more precise codebook—one which views the token
feature vectors ( zi)i∈[n]as i.i.d. samples of a mixture of Gaussians, where assignment of
tokens to a particular Gaussian is known and specified by the Boolean membership matrices
Π[K]= (Πk)k∈[K], and the kthGaussian has nkassociated tokens. We obtain an estimate
for the coding rate Rcas
Rc(Z|Π[K]).=1
2KX
k=1logdet( I+γkZΠkZ∗),where γk.=d
nkϵ2. (9)
As shown in Yu et al. (2020), maximizing the rate reduction ∆ R, i.e., the difference between
RandRc, promotes that the token features ziare compactly encoded as a mixture of low-
dimensional Gaussian distributions, where different Gaussian are statistically incoherent .
A generalized measure of rate reduction for tokens. In more realistic and general
scenarios, the features Zcan be a collection of tokens ( zi)N
i=1which have a sophisticated and
task-specific joint distribution, which can encode rich information about the data5which
we should also seek to capture in the final representation.
To realize our above desiderata in this context—namely, seeking a compact representa-
tion of a complex joint distribution of the token features—we only require that the desired
marginal distribution of individual tokens zishould be a mixture of (say K) low-dimensional
Gaussian distributions . Without loss of generality, we may assume that the kthGaussian
has mean 0∈Rd, covariance Σk⪰0∈Rd×d, and support spanned by the orthonormal
basisUk∈Rd×p. We denote U[K]= (Uk)K
k=1to be the set of all bases for the Gaussians.
In the sequel, we often identify the basis Ukwith the subspace itself.
For future reference, we provide a formal definition of this statistical model below.
Note that we may incorporate random noise as a way to model benign deviations from the
previously described idealized model.6
Low-Dimensional Gaussian Mixture Codebook: LetZ=
z1, . . . ,zn
∈Rd×nbe
a matrix-valued random variable. We impose the following statistical model on Z, param-
eterized by orthonormal bases U[K]= (Uk)k∈[K]∈(Rd×p)K: each token zihas marginal
distribution given by
zid=Usiαi,∀i∈[n] (10)
where (si)i∈[n]∈[K]nare random variables corresponding to the subspace indices, and
(αi)i∈[n]∈(Rp)nare zero-mean Gaussian variables. If we optionally specify a noise pa-
rameter σ≥0, we mean that we “diffuse” the tokens with Gaussian noise: by an abuse of
5. For example, co-occurrences between words in language data, or object parts in image data.
6. Our noise model is standard and simple, but can be made more sophisticated at essentially no conceptual
cost—the qualitative results will be the same.
16

--- PAGE 17 ---
White-Box Transformers via Sparse Rate Reduction
notation, each token zihas marginal distribution given by
zid=Usiαi+σwi,∀i∈[n] (11)
where (wi)i∈[n]∈(Rd)nare i.i.d. standard Gaussian variables, independent of siandαi.
From the perspective of statistics, we may view U[K]as multiple “principal subspaces”
(Vidal et al., 2016), which, just as in principal component analysis, are preferred to be
incoherent or nearly orthogonal to each other. From the perspective of signal processing,
we may view U[K]as “local signal models” for the input distribution. From the perspective
of information theory, we may view the bases U[K]as codebooks and the vectors αik.=U∗
kzi
as the “codes” of the token features ziwith respect to these codebooks. Motivated by (10),
we desire these codes to have a Gaussian marginal distribution within each subspace; under
this model, we can compute the coding rate of these codes, similar to (8), as
R(U∗
kZ).=1
2logdet( I+β(U∗
kZ)∗(U∗
kZ)),where β.=p
nϵ2. (12)
We emphasize that here, under (10), the joint distribution of such Zis underspecified, so
the true optimal codebook for Zis unknown and so the lossy coding rate for Zis impossible
to compute. However, since the desired marginal distribution of each token ziis a mixture
of low-dimensional Gaussians supported on subspaces U[K], we may obtain an upper bound
of the coding rate for the token set Z, which we denote as Rc, by projecting the tokens zi
onto these subspaces and summing up the coding rates on each subspace:
Rc(Z|U[K]).=KX
k=1R(U∗
kZ) =1
2KX
k=1logdet( I+β(U∗
kZ)∗(U∗
kZ)). (13)
This form of the coding rate can be viewed as a generalization to the coding rate Rc
in the original rate reduction objective defined in (7). In particular, the original objective
is defined with respect to a set of known membership labels Π[K]specific to the particular
data realization X. In contrast, the objective here is defined with respect to subspaces
U[K]which are in principle defined externally to any specific data realization, though they
support the token feature distribution. Since a single token can have nonzero projection
onto multiple subspaces Uk, yet must belong to exactly one category defined by Πk, the
coding rate defined in (13) may be viewed as a generalization of the coding rate defined in
(9). We may correspondingly generalize the coding rate reduction ∆ R, obtaining:
∆R(Z|U[K]).=R(Z)−Rc(Z|U[K]). (14)
Sparse rate reduction. It is easy to see that the rate reduction is invariant to arbi-
trary joint rotations of the representations and subspaces (Ma et al., 2007). In particular,
optimizing the rate reduction may not naturally lead to axis-aligned (i.e., sparse ) represen-
tations. For instance, consider the three sets of learned representations in Figure 5. The
coding rate reduction increases from (a) to (b), but because it is invariant under rotations,
remains the same from (b) to (c). Therefore, we would like to transform the representations
17

--- PAGE 18 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Figure 5: Comparison of three sets of representations via rate reduction and sparsity.
Each Sirepresents one linear subspace, and the number of blue balls represents the difference between
the coding rates ∆ R(Z|U[K]) =R(Z)−Rc(Z|U[K]).
(and their supporting subspaces) so that the features Zeventually become sparse7with
respect to the standard coordinates of the resulting representation space as in Figure 5(c).
The combined rate reduction and sparsification process is illustrated in Figure 3 or
Figure 4. Computationally, we may combine the above two goals into a unified objective
for optimization:
max
f∈FEZ=f(X)
∆R(Z|U[K])−λ∥Z∥0
, (15)
or equivalently,
max
f∈FEZ=f(X)
R(Z)−Rc(Z|U[K])−λ∥Z∥0
, (16)
where the ℓ0“norm”, defined as the number of nonzero values of the input vector/matrix,
promotes the sparsity of the learned token representations Z=f(X).8
We call this objective “ sparse rate reduction. ” In practice, one typically relaxes the ℓ0
norm to the ℓ1norm for better computability (Wright and Ma, 2022), obtaining:
max
f∈FEZ=f(X)
R(Z)−Rc(Z|U[K])−λ∥Z∥1
. (17)
By a little abuse of language, we often refer to this objective function also as the sparse
rate reduction .
Remark 1 (Connections to likelihood maximization and energy-based models ).
One natural interpretation of the Gaussian rate distortion R(Z) is as a lossy surrogate for
the log-likelihood of Zunder the assumption that the columns ziare drawn i.i.d. from a
zero-mean Gaussian whose covariance is estimated using Z(Cover, 1999). Similar inter-
pretations hold for Rc—as a surrogate for the un-normalized log-likelihood of Zunder the
assumption that the columns of ziare drawn from (10)—and ∆ R—as the difference of these
log-likelihoods. In some sense, the latter interpretations of the desired feature distribution
are “local,” in that they manage the part of the feature distribution aligned with the U[K].
If we also interpret the sparse regularization term −λ∥Z∥1in this way, we obtain the
interpretation that we prefer the features Zto have un-normalized log-density equal to
7. Concretely, having few nonzero entries.
8. To simplify the notation, we will discuss the objective for one sample Xat a time with the understanding
that we always mean to optimize the expectation.
18

--- PAGE 19 ---
White-Box Transformers via Sparse Rate Reduction
−λ∥Z∥1, so as to have density proportional to e−λ∥Z∥1. This is a more “global” interpre-
tation of the feature distribution. In this way, regularization can be seen as “exponentially
tilting” (Keener, 2010) the desired density towards one which is lower-entropy or more
axis-aligned.
One recently popular class of models which performs maximum-likelihood estimation is
energy-based models (LeCun et al., 2006). In particular, the overall objective function (17)
has a natural interpretation as an “energy function.” In particular, if we assume that our
surrogate likelihoods are exact likelihoods (up to constants), then the desired probability
distribution of the feature set Zis known up to constants as
p(Z|U[K]) =Ce−E(Z|U[K]).=Cexp(−λ∥Z∥1)·det(I+αZ∗Z)QK
k=1det 
I+β(U∗
kZ)∗(U∗
kZ),(18)
where we define the energy function
E(Z|U[K]) =−[R(Z)−Rc(Z|U[K])−λ∥Z∥1], (19)
where the term det( I+αZ∗Z)/(QK
k=1det(I+β(U∗
kZ)∗(U∗
kZ))) has a natural intrinsic
geometric interpretation as the ratio of the “volume” of the whole feature set Zand the
product of “volumes” of its projections into the subspaces (Ma et al., 2007).
Minimizing the above energy E(Z|U[K]) is equivalent to maximizing the sparse rate
reduction objective (17). In this sense, rate reduction-based approaches to representation
learning are qualitatively similar to certain classes of energy-based models.
Remark 2 (Intrinsic and extrinsic measures of goodness for the representations ).
Our notion of parsimony, as described above, desires the representations to have both in-
trinsic andextrinsic properties; that is, properties which are invariant to arbitrary rotations
of the data distribution (e.g., compression and linearization), and those which are not (e.g.,
sparsity). There are separate long lines of work optimizing intrinsic measures of goodness
for the representations (Yu et al., 2020; Chan et al., 2022; Dai et al., 2022; Pai et al., 2023)
as well as extrinsic measures (Gregor and LeCun, 2010; Elad et al., 2010; Elad, 2010; Zhai
et al., 2020b,a; Tolooshams and Ba, 2022; Wright and Ma, 2022). Both classes of methods—
that is, optimizing intrinsic and extrinsic measures of goodness of the representations—have
individually been successful in learning compact and structured representations which are
useful for downstream tasks. In this work, we combine and conceptually unify these per-
spectives on representation learning. In particular, our methodology seeks to optimize both
intrinsic and extrinsic measures. Overall, we achieve even greater empirical success than
previous white-box representation learning methods via learning intrinsically and extrinsi-
cally parsimonious representations.
Remark 3 (Black-box representations learned through pretext tasks ).Representa-
tion learning has also been quantitatively studied as the byproduct of black-box neural net-
works trained to solve pretext tasks, e.g., classification, contrastive learning, etc. Such end-
to-end approaches do not explicitly attempt to learn parsimonious representations through
the architecture or the objective. Meanwhile, we explicitly attempt to learn good represen-
tations which maximize the sparse rate reduction. Below, we give a concrete example of a
conceptual separation between these two approaches, and their resulting representations.
19

--- PAGE 20 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Multi-Head Subspace 
Self -A tt ention
(MSSA)compr ession
Sparse Coding
Pr o ximal St ep
(ISTA)sparsification
Figure 6: The ‘main loop’ of the CRATE white-box deep network design. After pre-
processing input data Xinto a sequence of tokens Z1,crate constructs a deep network that
transforms the data to a canonical configuration of low-dimensional subspaces by successive com-
pression against a local model for the distribution, generating Zℓ+1/2, and sparsification against
a global dictionary, generating Zℓ+1. Repeatedly stacking these blocks and training the model pa-
rameters via backpropagation yields a powerful and interpretable representation of the data.
Black-box representation learning has been most studied in the context of the supervised
classification pretext task. Both empirical work and theoretical work has demonstrated that,
under broad conditions, black-box neural networks trained with the cross-entropy loss on
supervised classification have representations which obey neural collapse (Papyan et al.,
2020; Zhu et al., 2021; Fang et al., 2021; Yaras et al., 2022), a phenomenon where represen-
tations of data from a given class are highly clustered around a single point, and the points
from each class are maximally separated. Wang et al. (2023a) (theoretically) and He and
Su (2022b) (empirically) showed that a progressive neural collapse phenomenon, governed
by a law of data separation, occurs from shallow to deep layers. This can be viewed as a
form of “compression” of the features of each class towards a finite set of points, which form
a geometric structure called a simplex equiangular tight frame . This is distinguished from
our approach to lossy compression through the sparse rate reduction in two particular ways.
First, our representation for a data point is a token set, whereas commonly neural collapse
is observed in cases where the representation is for a whole data point, so our representation
is more fine-grained than those studied by neural collapse. Second, our proposed compres-
sion objective—sparse rate reduction—encourages the features to be diverse and expanded
within their supporting subspaces, and in particular not collapsed to individual points . This
is a more fundamental difference which suggests that our approach is at odds with neural
collapse. More generally, our sparse rate reduction-based approach obtains qualitatively
and conceptually different representations than black-box networks.
2.2 Learning Parsimonious Representations via Unrolled Optimization
Although easy to state, each term of the sparse rate reduction objective proposed in the
previous section, viz.
max
f∈FEZ=f(X)
R(Z)−Rc(Z|U[K])−λ∥Z∥1
, (17)
can be computationally very challenging to optimize. Hence it is natural to take an ap-
proximation approach that realizes the global transformation fthrough a concatenation of
multiple, say L, simple incremental and local operations fℓthat push the representation
20

--- PAGE 21 ---
White-Box Transformers via Sparse Rate Reduction
distribution towards the desired parsimonious template distribution:
f:Xfpre
− − − → Z1→ ··· → Zℓfℓ
− − →Zℓ+1→ ··· → ZL+1=Z, (20)
where fpre:RD×N→Rd×nis the pre-processing mapping that transforms the input token
setX∈RD×Nto a first-layer representation Z1∈Rd×n, as in Figure 6.
Each incremental forward mapping Zℓ+1=fℓ(Zℓ), or a “layer”, transforms the token
distribution to optimize the above sparse rate reduction objective (17), conditioned on a
model, say a mixture of subspaces whose bases are Uℓ
[K], of the distribution of its input
tokens Zℓ:
max
fℓ∈FℓEZℓ+1=fℓ(Zℓ)
R(Zℓ+1)−Rc(Zℓ+1|Uℓ
[K])−λ∥Zℓ+1∥1
. (21)
Conceptually, if we follow the idea of the ReduNet (Chan et al., 2022), each fℓshould
conduct a “gradient-ascent” like operation:
Zℓ+1=fℓ(Zℓ)≈Zℓ+η∇
R(Zℓ)−Rc(Zℓ|Uℓ
[K])−λ∥Zℓ∥1
, (22)
≈Zℓ+η∇logp(Zℓ|Uℓ
[K]), (23)
where p(Z|U[K]) was defined in (18). An acute reader might have noticed that the term
∇logp(Z|U[K]) resembles that of a score function and the update (23) resembles that of
adenoising process , i.e., it moves the current iterate Zℓtowards the maximum-likelihood
token set with respect to the signal model Uℓ
[K]. We will thoroughly explore connections of
the above gradient ascent operation to denoising and diffusion processes in Section 3. For
now, we are interested in how to actually optimize the objective (17).
An alternating optimization strategy. As already explored in the work of Chan et al.
(2022), it is difficult to directly compute the gradient and optimize the rate reduction term
∆R,9let alone now with the non-smooth ℓ1term∥Z∥1. Nevertheless, from an optimization
perspective, once we decide on using an incremental approach to optimizing (17), there are
a variety of alternative optimization strategies. In this work we opt for perhaps the simplest
possible choice that exploit the special structure of the objective. Given a model Uℓ
[K]for
Zℓ, we opt for a two-step alternating minimization process with a strong conceptual basis:
Zℓ+1/2is chosen to incrementally minimize Rc(Zℓ+1/2|Uℓ
[K]), (24)
Zℓ+1is chosen to incrementally minimizeh
λ∥Zℓ+1∥0−R(Zℓ+1)i
. (25)
For the first step (24), we compress the tokens Zℓvia an approximate gradient step to
minimize an estimate for the coding rate Rc(Zℓ|Uℓ
[K]). Namely, Rc(Zℓ|Uℓ
[K]) measures
the compression of Zℓagainst (i.e., adherence to) the statistical structure delineated in (10)
with subspace bases Uℓ
[K]. Thus, taking a gradient step on Rcwith learning rate κ > 0
pushes the tokens towards having the desired statistics:
Zℓ+1/2≈Zℓ−κ∇Rc(Zℓ|Uℓ
[K]). (26)
9. This was part of the reason why the validity of ReduNet from Chan et al. (2022) could only be verified
with small datasets – it is difficult to scale the method up to produce competitive performance in practice.
21

--- PAGE 22 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Unfortunately, the gradient of the coding rate ∇Rcis costly to compute, as it involves K
separate matrix inverses, one for each of the Ksubspaces with basis Uℓ
k. However, as we
will formally derive in Section 2.3, this gradient can be naturally approximated using a so-
called MSSA(·) operator, which has a similar functional form to the multi-head self-attention
operator (Vaswani et al., 2017) with Kheads (i.e., one for each subspace, coming from
each matrix inverse), yet has a more explicit interpretation as approximately the (negative)
gradient of a compression measure Rc(Zℓ|Uℓ
[K]). As a result, we obtain a transformed
token set Zℓ+1/2given by
Zℓ+1/2.= (1−βκ)Zℓ+βκMSSA(Zℓ|Uℓ
[K])≈Zℓ−κ∇Rc(Zℓ|Uℓ
[K]), (27)
where β=p/(nϵ2) is defined in (12).
For the second step of (25), we sparsify the compressed tokens, choosing Zℓ+1via
a suitably-relaxed proximal gradient step to minimize the remaining term λ∥Zℓ+1∥1−
R(Zℓ+1). As we will argue in detail in Section 2.4, we can find such a Zℓ+1by solving a
sparse representation problem with respect to a sparsifying codebook, i.e., dictionary Dℓ:
Zℓ+1≈arg min
Z
λ∥Z∥1+1
2∥Zℓ+1/2−DℓZ∥2
F
. (28)
In this work, we choose to implement this step with an iteration of the iterative shrinkage-
thresholding algorithm (ISTA), which has classically been used to solve such sparse rep-
resentation problems (Beck and Teboulle, 2009). We call such an iteration the ISTA(·)
operator, formally defined in Section 2.4. We obtain tokens Zℓ+1given by
Zℓ+1.=ISTA(Zℓ+1/2|Dℓ)≈arg min
Z
λ∥Z∥1+1
2∥Zℓ+1/2−DℓZ∥2
F
. (29)
Both compression and sparsification are applied incrementally and repeatedly, as these
operations form layers of the network
fℓ:ZℓMSSA− − − − → Zℓ+1/2ISTA− − − − → Zℓ+1(30)
as in (20). Figure 7 graphically demonstrates the idealized effect of one layer.
2.3 Self-Attention as Gradient Descent on Coding Rate of Tokens
In this subsection and the next, we provide technical details about each of the two steps
mentioned in the Section 2.2, in particular the precise forms of the MSSA(·) and ISTA(·)
operators.
For the first step, where we compress the set of tokens against the Ksubspaces by
minimizing the upper bound for the coding rate Rc:
Zℓ+1/2is chosen to incrementally minimize Rc(Zℓ+1/2|Uℓ
[K]). (24)
As in Section 2.2, the compression operator takes an approximate gradient descent step on
Rc. The gradient of Rc(Z|U[K]) is given by
∇Rc(Z|U[K]) =βKX
k=1Uk(U∗
kZ)
I+β(U∗
kZ)∗(U∗
kZ)−1
. (31)
22

--- PAGE 23 ---
White-Box Transformers via Sparse Rate Reduction
Multi-Head Subspace 
Self -A tt ention
(MSSA)compr ession
Sparse Coding
Pr o ximal St ep
(ISTA)sparsification
Figure 7: The effect of one encoder layer fℓon the distribution of its input . First, Zℓ
is compressed against the codebook Uℓ
[K]to obtain Zℓ+1/2. Then, Zℓ+1/2is sparsified using the
codebook Dℓto obtain Zℓ+1.
The expression in (31) is highly expensive to compute exactly, since it requires Kma-
trix inverses, making the use of naive gradient descent intractable on large-scale problems.
Therefore, we seek an efficient approximation to this gradient; we choose to use the first-
order Neumann series:
∇Rc(Z|U[K])≈βKX
k=1Uk(U∗
kZ) 
I−β(U∗
kZ)∗(U∗
kZ)
(32)
=β KX
k=1UkU∗
k!
Z−β2KX
k=1Uk(U∗
kZ)(U∗
kZ)∗(U∗
kZ). (33)
The above approximate gradient expression (32) approximates the residual of each pro-
jected token feature U∗
kziregressed by other token features U∗
kzj(Chan et al., 2022). But,
differently from (Chan et al., 2022), not all token features in this auto-regression are from
the same subspace. Hence, to compress each token feature with token features from its own
group, we can compute their similarity through an auto-correlation among the projected
features as ( U∗
kZ)∗(U∗
kZ) and convert it to a distribution of membership with a softmax,
namely softmax(( U∗
kZ)∗(U∗
kZ)). Thus, as we show in more detail in Appendix A.1, if we
only use similar tokens to regress and denoise each other, then a gradient step on the coding
rate with learning rate κcan be naturally approximated as follows:
Zℓ+1/2= (1−βκ)Zℓ+βκMSSA(Zℓ|Uℓ
[K])≈Zℓ−κ∇Rc(Zℓ|Uℓ
[K]), (34)
where MSSA is defined through an SSAoperator as:
SSA(Z|Uk).= (U∗
kZ) softmax(( U∗
kZ)∗(U∗
kZ)), k∈[K], (35)
MSSA(Z|U[K]).=β
U1, . . . ,UK
SSA(Z|U1)
...
SSA(Z|UK)
. (36)
Here the SSAoperator in (35) resembles the attention operator in a typical transformer
(Vaswani et al., 2017), except that here the linear operators of value, key, and query are
all set to be the same as the subspace basis, i.e., Vk=Kk=Qk=U∗
k. We note that re-
cently Hinton (2021) has surmised that it is more sensible to set the “value, key, and query”
23

--- PAGE 24 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
projection matrices in a transformer to be equal. Our derivation confirms this mathemat-
ically. Hence, we name SSA(· |Uk) :Rd×n→Rp×ntheSubspace Self-Attention (SSA)
operator (more details and justification can be found in (102) in Appendix A.1). Then, the
whole MSSA operator in (36), formally defined as MSSA(· |U[K]):Rd×n→Rd×nand called
theMulti-Head Subspace Self-Attention (MSSA) operator, aggregates the attention head
outputs by averaging using model-dependent weights, similar in concept to the popular
multi-head self-attention operator in existing transformer networks. The overall gradient
step (34) resembles the multi-head self-attention implemented with a skip connection in
transformers.
In our implementation, we find that replacing the term β
U1, . . . ,UK
in the MSSA
operator (36) with another trainable parameter W∈Rd×pKlargely speeds up the model
training and optimization. Thus the MSSA block becomes
MSSA(Z|U[K],W).=W
SSA(Z|U1)
...
SSA(Z|UK)
. (37)
2.4 MLP as Proximal Gradient Descent for Sparse Coding of Tokens
In the previous subsection, we focused on how to compress a set of token features Zℓ
against a set of low-dimensional subspaces with orthonormal bases Uℓ
[K], obtaining a more
compressed token set Zℓ+1/2which approximately minimizes Rc(Zℓ+1/2|Uℓ
[K]). That is,
we solved (24) from Section 2.2:
Zℓ+1/2is chosen to incrementally minimize Rc(Zℓ+1/2|Uℓ
[K]). (24)
Now, it remains to choose Zℓ+1, by solving (25) from Section 2.2:
Zℓ+1is chosen to incrementally minimize λ∥Zℓ+1∥0−R(Zℓ+1) (25)
=λ∥Zℓ+1∥0−1
2logdet 
I+α(Zℓ+1)∗(Zℓ+1).(38)
On top of optimizing the remaining terms in the overall sparse rate reduction objective
(15), this step also serves an important conceptual role in itself. Namely, the term ∥Z∥0in
the objective (25) serves to sparsify the compressed tokens, leading to a more compact and
structured (i.e., parsimonious ) representation. In addition, the coding rate R(Z) in (25)
promotes diversity and non-collapse of the representations, a highly desirable property.
Similarly to Section 2.2, the gradient ∇R(Z) involves a matrix inverse (Chan et al.,
2022), and thus naive proximal gradient to solve (25) becomes intractable on large-scale
problems. We therefore take a different, simplifying approach to trading off between rep-
resentational diversity and sparsification: we posit a (complete) incoherent or orthogonal
dictionary Dℓ∈Rd×d, and ask to sparsify the intermediate iterates Zℓ+1/2with respect to
Dℓ. That is, Zℓ+1/2≈DℓZℓ+1where Zℓ+1is more sparse; that is, it is a sparse encoding of
Zℓ+1/2. The dictionary Dℓis used to sparsify all tokens simultaneously. By the incoherence
assumption, we have ( Dℓ)∗(Dℓ)≈I. Thus from (8) we have
R(Zℓ+1/2)≈R(DℓZℓ+1)≈R(Zℓ+1). (39)
24

--- PAGE 25 ---
White-Box Transformers via Sparse Rate Reduction
Thus we aim to solve (25) with the following program:
Zℓ+1≈arg min
Z∥Z∥0subject to Zℓ+1/2=DℓZ. (40)
The above sparse representation program is usually solved by relaxing it to an unconstrained
convex program, known as LASSO (Tibshirani, 1996; Wright and Ma, 2022):
Zℓ+1≈arg min
Z
λ∥Z∥1+1
2∥Zℓ+1/2−DℓZ∥2
F
. (41)
In our implementation, we also add a non-negative constraint to Zℓ+1, and solve the corre-
sponding non-negative LASSO:
Zℓ+1≈arg min
Z≥0
λ∥Z∥1+1
2∥Zℓ+1/2−DℓZ∥2
F
. (42)
We briefly justify the non-negativity constraint here. Given the dictionary Dℓ, the i-th
column of Zℓ+1can be interpreted as a sparse code for approximating the i-th token —
thei-th column of Zℓ+1/2. The non-negative value in Zℓ+1indicates to what extent the
dictionary atom is selected or not. There are both theoretical benefits (Zarka et al., 2020;
Guth et al., 2022) and empirical benefits (Sun et al., 2018) to this modeling decision,
mostly shown on classification problems, and validated in our own experiments in Table 13.
We incrementally optimize (42) by performing an unrolled proximal gradient descent step,
known as an ISTA step (Beck and Teboulle, 2009), to give the update:
Zℓ+1=ISTA(Zℓ+1/2|Dℓ), (43)
where ISTA(Z|D).= ReLU( Z−ηD∗(DZ−Z)−ηλ1). (44)
In Appendix A.2, we will show one can arrive at a similar operator to the above ISTA-like
update for optimizing (25) by properly linearizing and approximating the coding rate R(Z).
2.5 The Overall White-Box Transformer Architecture: CRATE
By combining the above two steps:
1. (Section 2.3) Local compression of tokens within a sample towards a mixture-of-subspace
structure, leading to the multi-head subspace self-attention block – MSSA;
2. (Section 2.4) Global sparsification of token sets across all samples through sparse coding,
leading to the sparsification block – ISTA;
we can get the following rate-reduction-based transformer layer, illustrated in Figure 8,
Zℓ+1/2.=Zℓ+MSSA(Zℓ|Uℓ
[K]),Zℓ+1.=ISTA(Zℓ+1/2|Dℓ). (45)
Composing multiple such layers following the incremental construction of our represen-
tation in (20), we obtain a white-box transformer architecture that transforms the data
tokens towards a compact and sparse union of incoherent subspaces. An overall flow of this
architecture was shown in Figure 6.
25

--- PAGE 26 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
MSSA
ISTA
ISTA
MSSA
·
·
··
·
·
Figure 8: One layer of the CRATE encoder architecture. The full architecture is simply a
concatenation of such layers, with some initial tokenizer, pre-processing head, and final task-specific
head (i.e., a classification head).
Remark 4 (Design choices in CRATE ).We note that in this work, at each stage of our
network construction, we have chosen arguably the simplest possible construction to use.
We can substitute each part of this construction, so long as the new part maintains the
same conceptual role, and obtain another white-box architecture. Nevertheless, our such-
constructed architecture, called crate , connecting to existing transformer models, is not
only fully mathematically interpretable, but also obtains competitive results on real-world
datasets, as we will see in Section 4.
Remark 5 (The roles of the forward pass and backward propagation ).In contrast
to other unrolled optimization approaches such as the ReduNet (Chan et al., 2022), we
explicitly model the distribution of each ZℓandZℓ+1/2at each layer, either by a mixture
of linear subspaces or sparsely generated from a dictionary. In Section 2.2, we introduced
the interpretation that at each layer ℓ, the learned bases for the subspaces Uℓ
[K]and the
learned dictionaries Dℓtogether serve as a codebook oranalysis filter that encodes and
transforms the intermediate representations at each layer ℓ. Since the input distribution
to layer ℓis first modeled by Uℓ
[K]then transformed by Dℓ, the input distribution to each
layer is different, and so we require a separate code book at each layer to obtain the most
parsimonious encoding. Parameters of these codebooks (i.e., the subspace bases and the
dictionaries), heretofore assumed as being perfectly known, are actually learned from data
(say via backward propagation within end-to-end training).
Hence, our methodology features a clear conceptual separation between forward “op-
timization” and backward “learning” for the so-derived white-box deep neural network.
Namely, in its forward pass, we interpret each layer as an operator which, conditioned on a
26

--- PAGE 27 ---
White-Box Transformers via Sparse Rate Reduction
learned model (i.e., a codebook) for the distribution of its input, transforms this distribution
towards a more parsimonious representation. In its backward propagation, the codebook
of this model, for the distribution of the input to each layer, is updated to better fit the
input-output relationship. This conceptual interpretation implies a certain agnosticism of
the model representations towards the particular task and loss; in particular, many types
of tasks and losses will ensure that the models at each layer are fit, which ensures that the
model produces parsimonious representations. To wit, we show in the sequel (Section 4)
that the crate architecture promotes parsimonious representations and maintains layer-
wise white-box operational characteristics on several different tasks, losses, and modalities.
3 White-Box Decoding via Structured Denoising and Diffusion
In Section 2, we have presented a principled metric for measuring the quality of learned
representations—the sparse rate reduction (15)—and showed how to derive, via incremen-
tal optimization of this objective, a white-box transformer architecture ( crate ) for general
representation learning of high-dimensional data. Conceptually, this corresponds to a (com-
pressive) encoder :
f:X→Z,
mapping high-dimensional data to representations preserving the distinct modes of intrinsic
variability of the data.
For numerous reasons, ranging from being able to use the learned representations Zfor
generation and prediction to having flexible avenues to learn the parameters ( Uℓ
[K])ℓ∈[L]of
the white-box encoder ffrom data, it is highly desirable to have a corresponding construc-
tion of a decoder :
g:Z→cX,
mapping the representations to approximations cXof the original data distribution. How-
ever, it is challenging to construct a white-box decoder purely following the unrolled opti-
mization framework that we have presented and exploited in Section 2 to derive the crate
encoder. Previous works, including notably the ReduNet of Chan et al. (2022), obtain white-
box architectures for encoding only; on the other hand, models that have incorporated a
decoder for learning (self-)consistent representations via autoencoding and closed-loop tran-
scription (Dai et al., 2022), including in unsupervised settings, have leveraged black-box
deep network architectures for both the encoder fand the decoder g(Dai et al., 2023), or
limited-capacity architectures for the decoder g(Tolooshams and Ba, 2022). Can compres-
sion alone, measured through the sparse rate reduction (15), be used to derive a white-box
decoder architecture? And in such a white-box decoder architecture, what are the relevant
operators for recovering the data distribution cX≈Xfrom the representation Z, and can
they be related to the operators in the encoder f?
In this section, we will resolve both of these fundamental questions affirmatively. We do
this by establishing a powerful connection between compression , around which we have de-
rived the crate encoder architecture, and diffusion-denoising , the mathematical processes
by which a data distribution is transformed into pure noise by incremental corruptions, and
then recovered incrementally, using information about the data distribution at each corrup-
tion level. Figure 9 illustrates this connection with an intuitive example. This connection
27

--- PAGE 28 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Figure 9: Compression and denoising against the low-dimensional Gaussian mixture
token model (10) are equivalent. Left: the effect of compression against the low-dimensional
Gaussian mixture model for tokens (10), i.e., taking gradient steps on the coding rate Rc(· |U[K])—
or equivalently, using the MSSA(· |U[K]) operator—which is shown in Theorem 6 to be equivalent to
projecting onto the U[K].Right: the effect of denoising against (10), i.e., taking gradient steps on
the score function of the noisy model (11) at small noise levels σ, or equivalently small times t. Up
to scaling factors (not pictured), these two operations are equivalent, and in any case have similar
geometric and statistical interpretations as a projection onto the support of the data distribution.
This connection motivates our structured denoising-diffusion framework, as elaborated in Section 3.2.
allows us to interpret the layers of the crate encoder, which we have shown in Section 2
perform compression against learnable local signal models, say following (10), as performing
denoising against the signal model. Since we are denoising against a highly structured input
distribution, we call this process “ structured denoising ”. Given the model, this structured
denoising process can be reversed in order to incrementally reconstruct the data distribu-
tion across several layers—we call this process “ structured diffusion ”, analogously but not
identically to the denoising-diffusion process which underlies diffusion models. The struc-
tured denoising-diffusion processes naturally supply the construction of the first white-box
decoder architecture for end-to-end representation learning.
3.1 Denoising-Diffusion against Low-Dimensional Structures
In Section 2, we derived each layer fℓof the encoder fviacompression of the token distri-
bution against a local signal model (i.e., the model (10)), and sparsification in the standard
basis. To derive a corresponding white-box decoder g, we will make a connection between
compression and denoising , a problem with a rich mathematical theory and powerful im-
plications for practical representation learning. In this section, we review the fundamental
concepts of this theory in order to motivate our later developments.
One-step denoising via Tweedie’s formula. Consider, for simplicity, a single token zℓ
♮
which has a particular marginal distribution, and define a noisy observation zℓ.=zℓ
♮+σℓw,
where σℓ>0 is a positive noise level, and wis a standard Gaussian vector independent
ofzℓ
♮. We imagine that zℓ
♮represents the marginal distribution of any token at layer ℓof
28

--- PAGE 29 ---
White-Box Transformers via Sparse Rate Reduction
the encoding process, and zℓhas the same interpretation subject to a (small) Gaussian
corruption. To denoise the observation zℓis to recover, up to statistical limits, the signal
(given by (10), which we will write here as zℓ
♮) from the noisy observation zℓ.10In the
mean-square sense, the optimal estimate is E[zℓ
♮|zℓ]. Letting z7→qℓ(z) denote the density
ofzℓ,11Tweedie’s formula (Efron, 2011a) allows us to express this in closed-form:
E[zℓ
♮|zℓ] =zℓ+ (σℓ)2∇logqℓ(zℓ). (46)
Tweedie’s formula expresses the optimal representation in terms of an additive correction
(in general a nonlinear function of zℓ) to the noisy observations by the gradient of the
log-likelihood of the distribution of the noisy observations, also known as the score function
∇logqℓ(Hyv¨ arinen, 2005). One may interpret Tweedie’s formula as denoising via a gradient
ascent step on the score function at noise level σℓ. This connection is well-known in the
areas of estimation theory and inverse problems (Efron, 2011a; Stein, 1981; Raphan and
Simoncelli, 2011; Milanfar, 2013; Kadkhodaie and Simoncelli, 2020; Venkatakrishnan et al.,
2013; Romano et al., 2017), and more recently has found powerful applications in the
training of generative models for natural images (Hyv¨ arinen, 2005; Vincent, 2011; Sohl-
Dickstein et al., 2015; Song et al., 2021b,a).
The practical question, of course, is then whether it is possible to efficiently learn to de-
noise . The additive correction with score function in (46) depends on the current noise level
and the token distribution, and for general high-dimensional distributions (such as those
of natural images, as above), this token distribution is unknown and can be prohibitively
costly to compute. Nevertheless, in practice, the score function is often empirically modeled
and approximated with a neural network (say a transformer), or another nonparametric es-
timator, and estimated with a large number of samples and huge amounts of computation.
Despite the empirical success of such diffusion-denoising methods in learning distributions
of images (Rombach et al., 2022), there has been little theoretical justification for why
transformer-like architectures would be effective to model such score functions.
Denosing against a low-dimensional Gaussian mixture. In the work of Hyv¨ arinen
(2005), the score function is used to learn a data distribution from a restricted parametric
family. As shown by Hyv¨ arinen (2005), for certain broad classes of parametric families,
the score function is efficiently computable, e.g. for a mixture of Gaussians, independent
component analysis models, over-complete dictionary learning, etc. Here (i.e., in this section
and hereafter), we follow the same methodology. Namely, suppose that zℓ
♮has the low-
dimensional Gaussian mixture distribution outlined in (10), so that zℓhas the distribution
outlined in (11) with noise level σℓ. In this case, we can obtain a closed-form expression for
the score function ∇logqℓ, which, when combined with Tweedie’s formula (46) and some
10. In representation learning, we typically think of zℓnot as an “observation”, but as a small perturbation
off of the target model, whose structure matches our desiderata for representation learning. Similarly,
rather than “recovery” of the structure from noisy observations, we are concerned with transforming the
current distribution of the data to be closer to the target model. We will see in the next section how
compression provides the bridge between these two perspectives; accordingly, we describe the denoising
problem using language specific to either perspective according to context.
11. We emphasize that qℓdepends on the noise level σℓ, although we suppress this in the notation for
concision.
29

--- PAGE 30 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
mild technical assumptions, gives the following approximation (shown in Appendix B.2):
E[zℓ
♮|zℓ]≈
U1, . . . ,UK
diag
softmax
1
2(σℓ)2
∥U∗
1zℓ∥2
2...
∥U∗
Kzℓ∥2
2


⊗I

U∗
1zℓ
...
U∗
Kzℓ
,(47)
where ⊗denotes the Kronecker product. In the small-noise limit σℓ→0, the operator
implemented by (47) becomes a projection of the observation zℓonto the support of the
distribution of the signal model zℓ
♮, a significant characterization of the local behavior of
denoising against the signal model (10). Moreover, perhaps surprisingly, this operation is
quite similar to the MSSA block derived in Section 2.3, specialized to the case n= 1. Indeed,
the operation in (47) resembles a self-attention layer in a standard transformer architecture
with Kheads, sequence length n= 1, and the “query-key-value” constructs being replaced
by a single linear projection U∗
kzℓof the token zℓ.
Stochastic denoising process. The above approach only denoises the token zℓonce.
Much of the practical power of denoising via the score function, however, stems from the
ability to iteratively denoise in small increments . Starting with the token zℓ, given access
to score functions of the distribution of zℓ
♮perturbed at at all noise levels up to σℓ, iterative
denoising of zℓproduces new samples from the noiseless distribution of tokens zℓ
♮. By
Tweedie’s formula (46), this means that denoising zℓis equivalent to representing the signal
zℓ
♮in a precise distributional sense. In a simple instantiation, this representation process
takes the following form (Song et al., 2021b). First, consider a diffusion process , indexed
by time t∈[0, T] for T= (σℓ)2>0, which transforms the distribution of zℓ
♮towards the
noisy distribution of zℓ:
dzt= dwt, t∈[0, T],
z0d=zℓ
♮.(48)
Here, ( wt)t∈[0,T]is a Wiener process, and we express this process in (48) as a stochastic
differential equation (SDE); for background on SDEs, see Appendix B.1. This SDE has a
unique (strong, i.e., pathwise well-defined) solution which has distribution ztd=zℓ
♮+wt.
Recalling that ( wt)t∈[0,T]is a Wiener process, wtis unconditionally distributed as N(0, tI),
so that zT=z(σℓ)2d=zℓ. As above, we write qtto denote the density of zt. Then by the
theory of time reversal for diffusion processes (Haussmann and Pardoux, 1986; Millet et al.,
1989a), the random process ( z←
t)t∈[0,T], where z←
t.=zT−t, uniquely solves the following
SDE:
dz←
t=∇logqT−t(z←
t) dt+ dw←
t, t∈[0, T],
z←
0d=zℓ,(49)
where w←
tis another Wiener process.12Because ( zT−t)t∈[0,T]solves (49), it follows that
this process yields a representation (via sampling) for zℓ
♮, as promised. Crucially, it can be
12. In the mathematical literature, both (48) and (49) are classified as (Markov) diffusion processes (Bakry
et al., 2016). By virtue of (46), in this work we will refer to (48) as “diffusion” and (49) as “denoising”.
30

--- PAGE 31 ---
White-Box Transformers via Sparse Rate Reduction
rigorously shown that an iterative denoising-diffusion process such as (49) is both necessary
and sufficient for representing high-dimensional multimodal data distributions efficiently
(Koehler et al., 2023; Ge et al., 2018; Qin and Risteski, 2023).
Deterministic denoising process. In (49), z←
tfollows a noisy gradient flow to maxi-
mize (log-)likelihood of the current iterate against a smoothly time-varying, i.e., “diffused,”
probabilistic model for the data distribution. We observe that each infinitesimal update of
(49) is similar to Tweedie’s formula (46), which takes a single gradient step on a “diffused”
log-likelihood to denoise. Thus, we interpret the process (49) as a stochastic denoising pro-
cess. In practice, and in particular towards our design of a corresponding crate decoder,
it will be useful to have a deterministic analogue of this process. The probability flow ODE
affords such a process: following Song et al. (2021b), the dynamics of the probability density
ofz←
tin (49) is identical to that of the ODE:
dz←
t=1
2∇logqT−t(z←
t) dt, t∈[0, T],
z←
0d=zℓ.(50)
It is significant that the representation for zℓ
♮afforded by the deterministic process (50)
can be characterized simply (again using (46)) as iterative denoising, across multiple noise
scales. This leads to the core insight of diffusion-denoising:
Denoising is equivalent to learning a representation of the data distribution.
With these preliminaries in hand, we will develop a deeper link between compression and
denoising that we can leverage to build a consistent encoder-decoder pair f, gfor general
(n >1) sets of tokens in the next section.
3.2 Parsimony and Consistency via Structured Denoising-Diffusion
In Section 3.1, we have presented the core ideas of the denoising-diffusion theory in the con-
text of the token marignal distribution of our signal model (10). We now significantly extend
the applicability of this theory, by connecting it to the compression framework we have in-
troduced in Section 2. We will use this extension to define our structured diffusion-denoising
paradigm, around which we derive a corresponding white-box decoder architecture for the
encoder f.
To this end, we study in detail a special case of the signal model (10): we assume
that the indices sispecifying the subspace membership of each token zℓ
iare i.i.d. random
variables, taking values in the set {1, . . . , K }with equal probability, independently of the
noises wiand the coefficients αi. We have seen in (47) in the previous subsection that
for such a model with vanishing noise level σℓ>0, denoising against the token marginal
distribution implements a projection onto the support of the noiseless distribution. We
prove that this same property is shared by taking a gradient step on the compression
objective Rc(Zℓ|Uℓ
[K]), as in the construction of the MSSA block in our white-box encoder
in Section 2.3, confirming the qualitative picture in Figure 9:
Theorem 6 (Informal version of Theorem 16 in Appendix B.3) .Suppose Zℓfollows the
noisy Gaussian codebook model (11), with infinitesimal noise level σℓ>0and subspace
31

--- PAGE 32 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
memberships sidistributed as i.i.d. categorical random variables on the set of subspace
indices {1, . . . , K }, independently of all other sources of randomness. Suppose in addition
that the number of tokens n, the representation dimension d, the number of subspaces K,
and the subspace dimensions phave relative sizes matching those of practical transformer
architectures including the crate encoder (specified in detail in Assumption 15). Then the
negative compression gradient −∇ziRc(Zℓ|Uℓ
[K])points from zℓ
ito the nearest Uℓ
k.
Theorem 6 establishes in a representative special case of the Gaussian codebook model
(10) that at low noise levels, compression against the local signal model Uℓ
[K]is equivalent
to denoising against the local signal model . Viewed through the lens of the deterministic
denoising process (50), this establishes a link between the gradient of the compression term
Rcand the score function for the Gaussian codebook model. Most importantly, this allows
us to understand the MSSA operators of the crate encoder, derived in Section 2.3 from
a different perspective, as realizing an incremental transformation of the data distribution
towards the local signal model, via denoising. This important property guarantees that a
corresponding deterministic diffusion process—namely, the time reversal of the denoising
process (50)—implies an inverse operator for the compression operation implemented by
MSSA.
Using this connection to construct a principled autoencoder. The above result
suggests the following approach to constructing white-box autoencoding networks. Given
the representation of the token distribution at layer ℓ, namely Zℓ, we construct a determin-
istic structured denoising process, identical to (50), which compresses the data towards the
local signal model at layer ℓof the representation f, namely Uℓ
[K]. Using the equivalence
asserted in Theorem 6, we can express this structured denoising process in terms of Rc, on
small timescales T >0 (as we work out in detail in Appendix B.4):
dZℓ(t) =−1
2(T−t)∇Rc(Zℓ(t)|Uℓ
[K]) dt. (51)
This process interpolates between the signal model (10), at t= 0, to a noisy version of
the signal model at t=T. By the same token, time reversal of diffusion processes gives a
structured diffusion process, which transforms the signal model to an incrementally more
noisy version:
deZℓ(t) =1
2t∇Rc(eZℓ(t)|Uℓ
[K]) dt. (52)
These two processes are inverses of one another in a distributional sense. To use these
structured denoising and diffusion processes for representation learning, we may ambitiously
treat the first-layer distribution Z1=fpre(X) itself as being a small deviation off the
distribution of the first local signal model U1
[K]. In this way, the incrementally-constructed
representation (20), which we have been studying at a single “layer” ℓthus far, naturally
leads to the following (completely formal) structured denoising process in which layer index
ℓand time tare unified into a single parameter, and where Z(0) = Z1is the preprocessed
data distribution:
dZ(t) =−1
2(T−t)∇Rc(Z(t)|U[K](t)) dt. (53)
32

--- PAGE 33 ---
White-Box Transformers via Sparse Rate Reduction
Similarly, we have the (completely formal) inverse process, a structured diffusion process:
deZ(t) =1
2t∇Rc(eZ(t)|U[K](T−t)) dt. (54)
These two equations provide a conceptual basis for transforming data to and from a struc-
tured, parsimonious representation, via the denoising-diffusion theory. On the one hand,
their similar functional forms—unified through compression, via the compression gradient
∇Rcand Theorem 6—demonstrate that the operators necessary for structured denoising
and structured diffusion take essentially the same form. On the other hand, the connec-
tion we have made in Section 2.3 between the gradient of the compression term of the rate
reduction objective and transformer-like network layers implies that a transformer-like ar-
chitecture is sufficient for both compressive encoding and decoding. We can therefore realize
compressive autoencoding with a fully mathematically-interpretable network architecture,
which we now instantiate.
3.3 Structured Denoising-Diffusion via Invertible Transformer Layers
In Section 2, we described a method to construct a white-box transformer-like encoder
network via unrolled optimization meant to compress the data against learned geometric
and statistical structures, say against a distribution of tokens where each token is marginally
distributed as a Gaussian mixture supported on U[K]. Armed with the structured diffusion-
denoising correspondence outlined in Section 3.2 and its connection to compression, we now
generalize the crate compressive encoder architecture to a full encoder-decoder pair, with
essentially identical operators transforming the data distribution from layer to layer, and
thus similarly interpretable operational characteristics.
A white-box encoder layer which implements structured denoising. Recall that
in Section 3.2, we established a continuous-time deterministic dynamical system which
implements structured denoising , in that it denoises the initial data towards the desired
parsimonious structure:
dZ(t) =−1
2(T−t)∇Rc(Z(t)|U[K](t)) dt. (55)
In order to construct a network architecture, we use a first-order discretization (the dis-
cretization scheme being another design choice) of this process, which we describe in detail
in Appendix B.4. This obtains the iteration
Zℓ+1/2≈Zℓ−κ∇Rc(Zℓ|Uℓ
[K]), (56)
in particular Zℓ+1/2=Zℓ+MSSA(Zℓ|Uℓ
[K]), (57)
where MSSA(·) was defined in (36). In order to perform structured denoising while ensuring
that the representation structures (e.g., supporting subspaces) themselves are parsimonious
(i.e., sparse), similar to Section 2, we insert a sparsification step for the features. Namely,
we instantiate a learnable dictionary Dℓ∈Rd×dand sparsify against it, obtaining
Zℓ+1≈arg min
Z≥0
λ∥Z∥1+1
2∥Zℓ+1/2−DℓZ∥2
F
, (58)
33

--- PAGE 34 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
in particular Zℓ+1=ISTA(Zℓ+1/2|Dℓ), (59)
where ISTA(·) was defined in (43). This yields a two step iteration for the ℓthencoder layer
fℓ, where Zℓ+1=fℓ(Zℓ):
Zℓ+1/2=Zℓ+MSSA(Zℓ|Uℓ
[K]),Zℓ+1=ISTA(Zℓ+1/2|Dℓ). (60)
This is the same layer as in the crate encoder, whose conceptual behavior we have illus-
trated in Figure 7. Thus, we have re-derived the crate encoder layer from a useful alternate
perspective, that of structured denoising. Namely, we have shown an equivalence between
structured denoising and unrolled optimization in this case, which stems from the fact that
the diffusion probability flow (53) is conceptually and mechanically similar to gradient flow
on the compression objective in certain regimes. Thus, we have demonstrated a useful
conceptual connection between discretized diffusion processes and unrolled optimization as
iteratively compressing or denoising the signal against the learned data structures.
A white-box decoder layer which implements structured diffusion. Previously,
we have shown how the structured denoising approach can recover the crate encoder
architecture, derived through unrolled optimization in Section 2, as an encoder. Now, we
go beyond the purview of unrolled optimization. Namely, we use the connections concretely
described in Section 3.2 to construct a decoder which implements structured diffusion .
Recall that the encoder is constructed from a discretization of the structured denoising
ODE given in (53). Its pathwise time reversal, which inverts the transformation of the data
distribution induced by the structured denoising ODE, is given formally by the structured
diffusion ODE:
deZ(t) =1
2t∇Rc(eZ(t)|U[K](T−t)) dt. (61)
For this reason, we use the structured diffusion ODE as the backbone of our decoder.
Indeed, a first-order discretization of this ODE obtains the iteration:
eZℓ+1=eZℓ+1/2−MSSA(eZℓ+1/2|Vℓ
[K])≈eZℓ+1/2+κ∇Rc(eZℓ+1/2|Vℓ
[K]), (62)
where Vℓ
[K]= (Vℓ
1, . . . ,Vℓ
K) and each Vℓ
k∈Rd×pare the bases of the subspaces to “anti-
compress” against. To invert the effect of a sparsifying ISTA(·) step, we instantiate a
learnable synthesis dictionary Eℓ∈Rd×dand multiply by it, obtaining the iteration:
eZℓ+1/2=EℓeZℓ,eZℓ+1=eZℓ+1/2−MSSA(eZℓ+1/2|Vℓ
[K]). (63)
This constructs the ( ℓ+ 1)stlayer gℓof our decoder as
eZℓ+1=gℓ(eZℓ).=EℓeZℓ−MSSA(EℓeZℓ|Vℓ
[K]). (64)
A graphical depiction of the encoder and decoder layers, with layer normalization added to
match the implementation, is found in Figure 10.
Remark 7 (Lack of weight-tying in autoencoding architecture ).Our derivation sug-
gests simple pre-set values for Vℓ
[K]andEℓ, i.e., UL−ℓ
[K]and (Dℓ)∗respectively. However,
we do notenforce these choices, or in any way share the weights between the encoder and
34

--- PAGE 35 ---
White-Box Transformers via Sparse Rate Reduction
+
-
Figure 10: Diagram of each encoder layer ( top) and decoder layer ( bottom ) in CRATE.
Notice that the two layers are highly anti-parallel—each is constructed to do the operations of the
other in reverse order. That is, in the decoder layer gℓ, the ISTA(·) block of fL−ℓis partially
inverted first using a linear layer, then the MSSA(·) block of fL−ℓis reversed; this order unravels the
transformation done in fL−ℓ.
decoder a priori in our implementation. This is because the discretization of the struc-
tured denoising and structured diffusion dynamics in (53) and (54) is good yet imperfect;
similarly, the rotation perspective on the ISTA(·) (i.e., from Section 2.3) is good yet imper-
fect, so the such-derived inversion above is also imperfect. Thus we should not expect a
1-1 correspondence between codebooks in the encoder and decoder. Correspondingly, each
encoder layer and its corresponding decoder layer are only approximate mutual inverses.
Remark 8 (Alternate instantiations of structured denoising-diffusion ).In this sec-
tion, to construct a decoder we established a specific connection between denoising against
the model (10) and compression using a gradient step on the coding rate Rc. Mechanically,
this connection was used to justify an inversion of the ISTA compression operator defined
in Section 2. However, as indicated in Remark 1, one may interpret the whole sparse rate
reduction (17) as a compression objective. Understanding to what degree an analogue of
Theorem 6 holds for the gradient of the sparse rate reduction would be quite interesting;
in particular, it would inform an analogue of this structured denoising-diffusion framework
which could be used to construct potentially more powerful white-box transformer-like ar-
chitectures from the sparse rate reduction. In this work, however, we use arguably the
simplest possible instantiation of our structured denoising-diffusion framework, in that we
make the simplest possible connection between denoising and compression.
4 Experimental Evaluations
In this section, we conduct experiments to study the empirical performance of our pro-
posed white-box transformer crate on real-world datasets and tasks. As the analysis
in Section 2 and Section 3 suggests, the compression/denoising and sparsification steps
can each be achieved through various alternative design choices or optimization strategies.
The current crate adopts arguably the most basic choices. So our goal with the experi-
ments is notto compete, using such a rudimentary design, with other empirically-designed
transformer architectures which are heavily engineered on these tasks. Rather, the goal
is to convince the reader that our transformer-like architecture crate obtains promising
performance on large-scale datasets, while being fully white-box—both mathematically and
empirically interpretable—and theoretically principled. We aim to achieve this goal through
the following three demonstrations:
35

--- PAGE 36 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
•First, we verify convincingly that the white-box transformer architecture, crate is
practically effective and can achieve strong performance on many large-scale real-
world datasets and tasks. These include supervised and self-supervised learning tasks
on both vision and natural language data: ViT, MAE, DINO, BERT, and GPT.
•Second, unlike most empirically designed black-box networks that can usually only be
evaluated through their end-to-end performance, we show that the white-box design
of our network allows us to look inside the deep architecture and verify if individual
layers and operators of the learned network indeed perform their design objective—say
performing incremental optimization for the objective (17).
•Third, we study the white-box transformers through post hoc interpretability. Exten-
sive qualitative and quantitative experimental results demonstrate that the internal
representations of the white-box transformer are much more interpretable, with clear
and easily-extractable semantic meaning, compared to black-box transformers.
In the remainder of this section we highlight a selection of results; additional experimen-
tal details and results can be found in Appendix C. PyTorch-like pseudocode is given in
Appendix D.
4.1 Empirical Verification of CRATE on Many Practical Tasks
4.1.1 Supervised Image Classification via ViT
We first study the performance of crate architecture on supervised image classification,
where we apply crate as the backbone architecture.
Model architecture. We implement the architecture that is described in Section 2.5,
with minor modifications that are described in Appendix C.1. Let Cbe the number of
classes. We use the pre-processing map:
fpre(X) =
z1
[CLS],WpreX
+Epos, (65)
where z1
[CLS]∈Rdis a trainable class token parameter, X=
x1, . . . ,xN
∈RD×Nis the
input image patches, Wpre∈Rd×Dis a trainable matrix, and Epos∈Rd×nis a trainable
positional encoding matrix, where n=N+ 1. We also consider a classification head at the
output of the crate model:
fhead(
z1, . . . ,zn
) =Wheadz1, (66)
where Z=
z1, . . . ,zn
is the output of the encoder crate ,z1in particular is the feature
corresponding to the class token z1
[CLS], and Whead∈RC×dis a trainable classification
head. The output of fheadis a set of unnormalized log-probabilities for the different classes.
We consider different model sizes of crate by varying the token dimension d, number
of heads K, and the number of layers L. We consider four model sizes in this work: crate -
Tiny, crate -Small, crate -Base, and crate -Large. We provide details about model config-
urations of crate in Table 8. More implementation details can be found in Appendix C.1;
PyTorch-style pseudocode can be found in Appendix D.
36

--- PAGE 37 ---
White-Box Transformers via Sparse Rate Reduction
Pre-training methodology. LetH: ∆C×∆C→R, where ∆ Cis the set of probability
distributions on [ C], be the cross-entropy function, defined as
H(p,q) =Ex∼p[−logq(x)] =−CX
c=1pclog(qc). (67)
The pre-training loss is
LCE(f, fhead).=EX,yh
H 
y,softmax
(fhead◦f)(X)	i
. (68)
We pre-train on ImageNet-1K (Deng et al., 2009), using the Lion optimizer (Chen et al.,
2023c). More details about the training and datasets can be found in Appendix C.1.
Fine-tuning methodology. For fine-tuning (possibly on a different dataset with a dif-
ferent number of classes), we re-initialize fheadusing parameters with the appropriate value
ofC, and train on the cross-entropy loss in (68), updating both fandfhead. Again, we
train using the Lion optimizer (Chen et al., 2023c), this time on a variety of commonly
used datasets: CIFAR10/CIFAR100 (Krizhevsky et al., 2009), Oxford Flowers (Nilsback
and Zisserman, 2008), and Oxford-IIIT-Pets (Parkhi et al., 2012). More details about the
training and datasets can be found in Appendix C.1.
Classification accuracy. We study the empirical performance of the proposed networks
by measuring their top-1 accuracy on ImageNet-1K as well as transfer learning performance
on several widely used downstream datasets. We summarize the results in Table 1. Our
transfer learning methodology is to fine-tune using (68) starting from the pre-trained fand
a re-initialized fhead.
As our designed architecture leverages parameter sharing in both the attention block
(MSSA) and the nonlinearity block ( ISTA), our crate -Base model (22.80 million) has a
similar number of parameters to the ViT-Small (22.05 million), and less than 30% of the
parameters of an identically configured ViT-Base (86.54 million). From Table 1, we find
that with a similar number of model parameters, our proposed network achieves similar
ImageNet-1K and transfer learning performance as ViT, while having a simple and princi-
pled design. Moreover, with the same set of training hyperparameters, we observe promis-
ing scaling behavior in crate —we consistently improve the performance by scaling up the
model size. For comparison, directly scaling ViT on ImageNet-1K does not always lead to
consistent performance improvement measured by top-1 accuracy, even while such scaling
behavior is touted as one of the main benefits of the transformer architecture (Dosovitskiy
et al., 2021). Furthermore, we also study the performance of crate when pre-training on
a larger dataset—ImageNet-21K (Deng et al., 2009). We then fine-tune the pre-trained
crate models on downstream tasks including the ImageNet-1K dataset. The results are
summarized in Table 14. In particular, the crate -B achieves 79.5% top-1 accuracy on
ImageNet-1K. To summarize, we achieve promising performance on real-world large-scale
datasets by directly implementing our principled architecture.
4.1.2 Image Completion via Masked Autoencoding
Now, we study how we can use the full autoencoder architecture based on crate described
in Section 3, including both the crate encoder defined in Section 2.5 and the crate decoder
37

--- PAGE 38 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Table 1: Top-1 classification accuracy of crate on various datasets with different model scales
when pre-trained on ImageNet-1K. For ImageNet-1K/ImageNet-1K ReaL, we directly evaluate the
top-1 accuracy. For other datasets, we use models that are pre-trained on ImageNet as initialization
and the evaluate the transfer learning performance via fine-tuning.
Model crate -T crate -S crate -B crate -L ViT-T ViT-S
# parameters 6.09M 13.12M 22.80M 77.64M 5.72M 22.05M
ImageNet-1K 66.7 69.2 70.8 71.3 71.5 72.4
ImageNet-1K ReaL 74.0 76.0 76.5 77.4 78.3 78.4
CIFAR10 95.5 96.0 96.8 97.2 96.6 97.2
CIFAR100 78.9 81.0 82.7 83.6 81.8 83.2
Oxford Flowers-102 84.6 87.1 88.7 88.3 85.1 88.5
Oxford-IIIT-Pets 81.4 84.9 85.3 87.4 88.5 88.6
defined in Section 3.3, to perform masked autoencoding (He et al., 2021)—an unsupervised
masked image completion task. As shorthand, we call the crate -masked autoencoder
crate -MAE.
Model architecture. We implement the encoder and decoder architectures described in
Section 3, with a few changes detailed in Appendix C.2. We use the pre-processing map:
fpre(X) =WpreX+Epos, (69)
where X=
x1, . . . ,xN
∈RD×N,Wpre∈Rd×Dis a trainable matrix, and Epos∈Rd×nis
a trainable positional encoding matrix, where n=N. Similarly we use the post-processing
map:
gpost(eZ) =Wpost(eZ−Epos), (70)
where Wpost∈RD×dis another trainable matrix.
We consider different model sizes of crate by varying the token dimension d, number
of heads K, and number of layers L; such parameters will be kept the same for the encoder
and decoder. This choice is is different from the proposed MAE architecture in He et al.
(2021), which uses an asymmetric encoder-decoder setup where the decoder is significantly
smaller and more lightweight. This asymmetric approach is conceptually messier, whereas
our symmetric approach is totally in line with our white-box derivation. We consider
three model sizes: crate -MAE-Small, crate -MAE-Base, and crate -MAE-Large. Table 9
displays the model configurations and number of parameters, with a comparison to MAE,
showing that crate uses around 30% of the parameters of MAE with the same model
configuration (e.g., layers L, token dimension d, and number of heads K).
Pre-training methodology. Masked autoencoding (He et al., 2021) “masks out” a large
percentage of randomly selected input image tokens in the input X=
x1, . . . ,xN
∈RD×N
and then attempts to reconstruct the whole image, measuring success by the resulting au-
toencoding reconstruction loss and performance on downstream tasks. The regular masked
autoencoding (MAE) loss (He et al., 2021) is defined as
LMAE(f, g).=EX,Ω
∥(g◦f)(Mask Ω(X))−X∥2
2
, (71)
38

--- PAGE 39 ---
White-Box Transformers via Sparse Rate Reduction
Mask ed ViT -MAE CRA TE-MAE Original Mask ed ViT -MAE CRA TE-MAE Original
Figure 11: Autoencoding visualization comparison of crate -Base to MAE-Base (He et al., 2021)
with 75% patches masked. We observe that the reconstructions from crate -Base are on par with
the reconstructions from MAE-Base, despite using fewer than 1 /3 of the parameters.
39

--- PAGE 40 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Table 2: Average reconstruction loss over the training and validation sets of ImageNet-1K as a
function of architecture model size. We see that the performance of crate -Base, while a bit worse
than MAE-Base, obtains promising performance on the challenging masked autoencoding task.
Reconstruction Loss crate -Base MAE-Base
Training 0.266 0.240
Validation 0.303 0.267
where Mask Ω(·) replaces all tokens in Xwhose indices are in Ω with the same masked
token. Namely, for each data point, we sample a subset Ω ⊆[N] of a fixed size, and set
Xmask=
xmask
1, . . . ,xmask
N
, where xmask
i=xmaskifi∈Ω and xmask
i=xiifi /∈Ω, where
xmask∈RDis a trainable mask token.
For pre-training, we employ the AdamW optimizer (Loshchilov and Hutter, 2019). We
mainly use ImageNet-1K (Deng et al., 2009) as the main pre-training dataset. More details
about training and datasets can be found in Appendix C.2.
Fine-tuning methodology. Following He et al. (2021), we fine-tune using supervised
classification to examine the linear separability of the features. To do this, we use either
full-model fine-tuning or linear probing. The classification head is a “global pooling” head:
fhead(Z) =Whead 
1
nnX
i=1zi!
=1
nWheadZ1n, where Z=
z1, . . . ,zn
, (72)
where 1nis the all-ones vector in Rn; note that this is different to the classification setup
in Section 4.1.1, owing to not incorporating a class token. We train using the cross-entropy
loss, where His defined in (67):
LCE(f, fhead).=EX,yh
H(y,softmax {(fhead◦f)(X)})i
. (73)
For full fine-tuning, we update both fandfhead. For linear probing, we update only
fhead. We fine-tune crate -MAE on several commonly used downstream datasets: CI-
FAR10/CIFAR100 (Krizhevsky et al., 2009), Oxford Flowers (Nilsback and Zisserman,
2008), and Oxford-IIIT-Pets (Parkhi et al., 2012). For fine-tuning fhead, we use the AdamW
optimizer on minibatches (Loshchilov and Hutter, 2019). For linear probing, we use a logis-
tic regresion solver to compute fheadin one shot using the full dataset. More details about
training can be found in Appendix C.2.
Autoencoding performance. We analyze the autoencoding reconstruction performance
ofcrate -MAE and compare with the original MAE in He et al. (2021). In Figure 11, we
qualitatively compare the masked autoencoding performance of crate -MAE-Base to MAE-
Base (He et al., 2021). We observe that both models are able to reconstruct the data well,
despite crate using less than a third of the parameters of MAE. In Table 2, we display
the average reconstruction loss of crate -MAE-Base and MAE-Base, showing a similar
quantitative conclusion.
40

--- PAGE 41 ---
White-Box Transformers via Sparse Rate Reduction
Table 3: Top-1 classification accuracy of crate -MAE-Small, crate -MAE-Base, (ViT-)MAE-
Small, and (ViT-)MAE-Base when pre-trained on ImageNet-1K and fine-tuned on classification
(either on the whole network, or using logistic regression a.k.a. full-batch linear probing) for several
smaller datasets. Our results show that the representation learning in crate is useful for down-
stream tasks on a level comparable to that of the regular MAE, on top of having several side benefits
such as parameter efficiency and emergent properties (as discussed in the sequel).
Model crate -MAE-S crate -MAE-B ViT-MAE-S ViT-MAE-B
# parameters 25.4M 44.6M 47.6M 143.8M
Fine-Tuning
CIFAR10 96.2 96.8 97.6 98.5
CIFAR100 79.0 80.3 83.0 87.0
Oxford Flowers-102 71.7 78.5 84.2 92.5
Oxford-IIIT-Pets 73.7 76.7 81.7 90.3
Linear Probing
CIFAR10 79.4 80.9 79.9 87.9
CIFAR100 56.6 60.1 62.3 68.0
Oxford Flowers-102 57.7 61.8 66.8 66.4
Oxford-IIIT-Pets 40.6 46.2 51.8 80.1
Fine-tuning performance. In Table 3, we compare the fine-tuning and linear probing
performance of crate -MAE models pretrained on ImageNet-1K across different model sizes
and datasets. We demonstrate that crate -MAE has comparable finetuning performance
to the usual MAE models while also saving many parameters. This demonstrates that
our representations are highly geometrically and statistically structured (e.g., having linear
structure), and carry semantic content that is useful for downstream tasks.
4.1.3 Self-Supervised Learning via DINO Training Method
Self-supervised learning (SSL) has gained increasing popularity and become a dominant
approach for pre-training deep networks. In this part, we study whether the success of SSL
in transformers can be applied to the proposed crate architecture. Specifically, we train
crate backbones with DINO (Caron et al., 2021), a widely used SSL framework for ViTs
that demonstrate great feature learning capabilities.
Model architecture. To start with, we briefly introduce the self-supervised learning
method DINO (self- distillation with nolabels) (Caron et al., 2021). DINO applies knowl-
edge distillation (Hinton et al., 2015), with a “teacher” encoder network fproj
t◦ftand
“student” encoder network fproj
s◦fs, to learn from two different “views” of an image. Here,
for each “backbone” model f(either fsorft), we can write (as in other experimental set-
tings) f=fL◦···◦ f1◦fpre, where the pre-processing map fpre:RD×N→Rd×nis defined
as
fpre(X) =
z1
[CLS],WpreX
+Epos, (74)
where n=N+1 and z1
[CLS]∈Rd,Wpre∈Rd×DandEpos∈Rd×nare trainable parameters,
and each layer fℓ:Rd×n→Rd×nis a transformer layer, such as the crate layer or ViT
layer. Each projection map fproj:Rd×n→Rd(either fproj
sorfproj
t) is defined, as in Caron
41

--- PAGE 42 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
et al. (2021), as
fproj(
z1, . . . ,zn
) =Wproj
3ReLU( Wproj
2ReLU( Wproj
1z1+bproj
1) +bproj
2) +bproj
3,(75)
where Wproj
i∈Rd×dare trainable weights and bproj
i∈Rdare trainable biases.
In our experiments, for the “backbone” layers fℓ◦ ··· ◦ f1for the teacher and student
models, we adopt the same model configuration as in the supervised learning experiments
introduced in Section 4.1.1. Specially, due to the heavy computational demand for DINO,
we train with two smaller model sizes for this task: crate -Small and crate -Base (note
crate -Base has a similar number of parameters as as ViT-Small). Refer to Caron et al.
(2021) for more implementation details about DINO, including additional small task-specific
alterations.
Pre-training methodology. We parameterize ( fs, fproj
s) by a parameter set θs, and
similarly parameterize ( ft, fproj
t) by a parameter set θt. Note that by construction, θsand
θtparameterize the same architecture and thus are elements of the same Euclidean space.
The training objective is to minimize
L(θs,θt).=EX
H(P(Augg(X)
|{z}
view 1;θt, τt), P(Augl(X)|{z}
view 2;θs, τs))
, (76)
where Augg(·) denotes a global view augmentation and Augl(·) denotes a local view augmen-
tation13, and the probability P(·;·,·) is defined as
P(X;θ, τ) = softmax
τ−1(fhead
θ◦fθ)(X)
, (77)
where fθs=fs, and so on, and τ >0 is the softmax temperature which controls how sharply
the softmax concentrates about its largest values. By passing two different views (local and
global) of the same image to the student network and teacher network respectively, the
DINO objective is designed to encourage “local-to-global” correspondences—the features of
the local view Augl(X) for the student network are encouraged to be similar to the features
of the global view Augg(X) for the teacher network.
The training is done in the following (slightly unintuitive) way using exponential moving
averages:
θk+1
s=θk
s−η∇θsL(θk
s,θk
t), (78)
θk+1
t=λθk
t+ (1−λ)θk+1
s, (79)
where η >0 is a learning rate, λ∈(0,1) is an exponential moving average parameter, and
kis the training iteration. Caron et al. (2021) applies the stop-gradient operator on the
teacher network, so gradients are propagated only through the student network and not
the teacher. In reality, Lis replaced with an empirical version over mini-batches, and the
13. We follow the multi-crop strategy (Caron et al., 2020). The views are distorted crops of the original
image. A global view is a high-resolution (e.g. 224 ×224) crop that covers a large (e.g. >50%) area of
the input image while a local view is of lower resolution (e.g. 96 ×96) and covers a smaller (e.g. <50%)
area of the original image.
42

--- PAGE 43 ---
White-Box Transformers via Sparse Rate Reduction
iteration for θscan use a more complex algorithm than stochastic gradient descent. In
particular, we adopt a similar training recipe as reported in Caron et al. (2021); we train on
ImageNet-1K (Deng et al., 2009) and use the AdamW optimizer (Loshchilov and Hutter,
2019), with a learning rate schedule that is composed of a linear warm-up phase followed by
a cosine decay schedule. Due to the differences in the backbone architecture, we only alter
the choice of base learning rates and weight decay parameters in our experiments. More
details can be found in Appendix C.3.
Fine-tuning methodology. Similar to the evaluation protocol of Caron et al. (2021),
we evaluate the classification performance using the features of the pre-trained networks.
Precisely, we consider the extracted class token feature ztfrom the pre-trained teacher
network ftin the following way. Specifically, we obtain
zt←z1 where
z1, . . . ,zn
=ft(X). (80)
From here, we fine-tune on supervised classification in two different ways.
First, we use the feature vectors ztto form a weighted k-nearest neighbor classifier (Wu
et al., 2018). That is, with Cclasses we build a classification head fhead
k:Rd→[C] which
performs weighted k-nearest neighbors with respect to the training dataset. We do this
for several choices of k, and in our shown results we select the classifier fhead
k◦ftwith the
highest accuracy on the training set.
Second, we use the feature vectors ztto build a linear probing model. That is, with C
classes we build a classification head fhead:Rd→RCdefined as
fhead(z) =Wheadz, (81)
for a trainable parameter Whead∈RC×d, by minimizing the cross-entropy loss (68)
LCE(f, fhead).=EX,yh
H(y,softmax {(fhead◦f)(X)})i
, (82)
over fhead(leaving ffixed). We do this by using a logistic regression solver to compute
fheadin one shot using the full fine-tuning dataset.
The fine-tuning dataset in our experiments is the same as the training dataset, i.e.,
ImageNet-1K (Deng et al., 2009).
Fine-tuning results. We report the Top-1 test accuracy of both our fine-tuning method-
ologies on ImageNet-1K in Table 4. These results show that, with only minimal changes
of hyper-parameter settings, the proposed crate architecture can learn meaningful and
informative features via self-supervised learning.
Later in Section 4.3, we will show how the features learned by the self-supervised DINO
can already produce good image segmentation results, see Figure 19.
4.1.4 Pre-Training Language Models via BERT and GPT
We now study the performance of crate architectures on text data. Specifically, we con-
sider two widely used pre-training tasks for learning language representations—Bidirectional
Encoder Representations from Transformers (BERT) pre-training (Devlin et al., 2019; Liu
et al., 2019) and Generative Pre-Training (GPT) pre-training (Radford et al., 2019).
43

--- PAGE 44 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Table 4: Top-1 classification accuracy on ImageNet-1K of crate -Small, crate -Base, and ViT-
Small when pre-trained using the DINO objective.
Model #parameters k-NN Linear Probing
crate -Small/16 13.12M 53.02% 58.41%
crate -Base/8 22.80M 59.91% 67.42%
ViT-Small/16 22.05M 69.36% 72.02%
ViT-Small/8 22.05M 71.98% 75.63%
CRATE-BERT model architecture. For BERT pre-training, we apply the RoBERTa
tokenizer (Sennrich et al., 2016; Liu et al., 2019) with vocabulary size V= 50,265 as the
default tokenizer. We implement the crate architecture that is used in image classification
Section 4.1.1, except for the pre-processing layer and the task-specific head. Specifically,
the pre-processing layer is described as:
fpre(X) =fembed(X) +Epos, (83)
where fembedis an embedding function which maps input tokens xito embedding vectors in
Rd, andEpos∈Rd×nis the sinusoid positional embedding defined in Vaswani et al. (2017).
The final output layer takes the form of a linear head:
fhead(Z) =WheadZ, (84)
where Whead∈RV×d.
We denote the crate model with BERT pre-training as crate -BERT. We summarize
the configurations of crate -BERT models in Table 10 (upper). For crate -BERT-Base, we
use the same width, number of attention heads, and depth as BERT-Base (Liu et al., 2019).
crate -BERT-Base uses 60.9M parameters, while BERT-Base uses 110M parameters, nearly
twice as many.
CRATE-BERT pre-training methodology. The training loss for crate -BERT is
masked language modeling (masked LM)14, which is defined as
LBERT (f, fhead) =EX,Ω"
1
|Ω|X
i∈ΩH(1(xi),softmax {(fhead◦f)(X)i})#
, (85)
where ( fhead◦f)(X)i∈RVdenotes the ithindex of the model output—that is, the log-
probabilities of each potential token in the vocabulary to take up the ithindex in the
sequence X. Similarly 1(xi)∈ {0,1}Vrepresents the one-hot embedding vector of the
token xiin the vocabulary.
To pre-train, we use the Adam optimizer (Kingma and Ba, 2015). We use the same pre-
training dataset as BERT (Devlin et al., 2019), including BooksCorpus (Zhu et al., 2015)
and English Wikipedia. Refer to Devlin et al. (2019) for more details about the pre-training
tasks.
14. The original BERT pre-training tasks also contain next sentence prediction, but we discard this task due
to observations shown by Liu et al. (2019).
44

--- PAGE 45 ---
White-Box Transformers via Sparse Rate Reduction
CRATE-GPT model architecture. Forcrate -GPT pretraining, following the setup
in the GPT-2 paper (Radford et al., 2019), we modify our crate architecture to align with
the next word prediction task used in GPT. Specifically, we replace the attention in MSSA
with a causally masked self-attention, i.e., replacing
softmax(( U∗
kZ)∗(U∗
kZ))→softmax( CausalMask ((U∗
kZ)∗(U∗
kZ))), (86)
where CausalMask (A)ij=(
Aij, i≤j
−∞, i > j.(87)
in (35). We use the same pre-processing and head architecture as crate -BERT:
fpre(X) =fembed(X) +Epos, fhead(Z) =WheadZ. (88)
We denote the crate model for GPT pre-training as crate -GPT. We summarize the
configurations of GPT models in Table 10 (lower), and set the model architecture parameters
such that crate -GPT has the same layer specifications as GPT2 with parameter size 60M
(c.f. GPT2’s 124M, more than twice as many).
CRATE-GPT pre-training methodology. The training objective of GPT is next word
prediction (causal language modeling, causal LM), which is defined as
LGPT(f, fhead) =EX1
|X| −1|X|−1X
i=1H(1(xi+1),softmax {(fhead◦f)(X)i})
, (89)
where |X|denotes the total number of tokens in the input sample X. For GPT pretraining,
we mainly follow the implementations in Karpathy (2022); in particular we set the block
size as 1,024. We pre-train crate -GPT models on OpenWebText (Gokaslan and Cohen,
2019) using the Adam optimizer (Kingma and Ba, 2015). Refer to Radford et al. (2019) for
more details about the pre-training tasks.
As usual, we defer the pre-training setup details of crate -BERT and crate -GPT to
Appendix C.4.
CRATE-BERT evaluation results. Pre-training results of crate -BERT is summa-
rized in Table 5. We use the officially released BERT checkpoints for BERT evalua-
tion (Google, 2021). We evaluate the crate -BERT model by fine-tuning on downstream
tasks from the GLUE benchmark (Wang et al., 2019). Single-task Finetuning of Table 5
means that we fine-tune the model only on the task we evaluate. In addition, following
previous work (Liu et al., 2019), models pre-trained only on the masked language modeling
task should be further fine-tuned on MNLI before evaluating on MRPC, STS-B and RTE to
be competitive to BERT. That is, first fine-tuning on MNLI improves the performance on
these tasks. Therefore, we also investigate this training recipe and include the correspond-
ing results in the Further Finetuning after MNLI section of Table 5. The AVG score is the
average of all GLUE tasks shown in the table.15We also present the pre-training loss and
GLUE evaluation scores of crate -BERT with respect to progressing pre-training steps in
15. We exclude the problematic CoLA task, as it has been reported multiple times that the performance
depends heavily on the seed chosen for BERT (Huggingface, 2023).
45

--- PAGE 46 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Table 5: Performance on the GLUE benchmark (higher is better). All scores are obtained using the
HuggingFace evaluation script (Huggingface, 2020). F1 scores are reported for QQP and MRPC,
Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks.
SST-2 MRPC STS-B QQP MNLI QNLI RTE WNLI Avg
Acc F1 Sprm F1 M/MM Acc Acc Acc All
Single-task Finetuning
BERT-Medium (41M) 90.5 87.8 87.6 86.2 80.8/81.2 88.6 64.6 30.1 77.5
BERT-Base (110M) 92.3 90.9 88.5 88.2 84.4/84.6 91.7 64.3 53.5 82.0
crate -BERT-Base (61M) 85.9 80.0 73.0 85.8 76.1/75.3 83.4 53.8 56.3 74.4
Further Finetuning after MNLI
BERT-Medium (41M) 90.5 89.6 88.0 86.2 80.8/81.2 88.6 72.6 30.1 78.6
BERT-Base (110M) 92.3 90.3 86.8 88.2 84.4/84.6 91.7 79.4 53.5 83.5
crate -BERT-Base (61M) 85.9 84.7 83.2 85.8 76.1/75.3 83.4 65.0 56.3 77.3
0 50 100 150 200 250 300
Step (x100)2345678LossTrain Loss
1530 60 90 135 180 240 300
Step (x100)6667686970717273Score
GLUE AVG
0 100 200 300 400 500 600
Step (x100)3.03.54.04.55.05.56.0LossTrain Loss
Val Loss
3.23.33.43.53.6
Figure 12: (left) The training loss curve of crate -BERT-Base trained on the Wikipedia and
BookCorpus datasets. ( middle ) GLUE single-task finetuning scores evaluated after each pretraining
checkpoint of crate -BERT-Base. ( right) The loss curve of crate -GPT-Base trained on the Open-
WebText dataset.
Figure 12 (left and middle). The progressing GLUE average scores are calculated under the
single-task finetuning setting.16From Table 5, we find that crate -BERT-Base can achieve
competitive performance compared to BERT models realized by standard transformers.
CRATE-GPT evaluation results. We evaluate crate -GPT models by measuring the
zero-shot cross-entropy loss on different datasets. We evaluate crate -GPT-Base on Open-
WebText (OWT) as well as other datasets as shown in (Radford et al., 2019). We compare
with the official release of GPT2-Base which is pre-trained on WebText (OpenAI, 2019),
while crate -GPT-Base is pre-trained on OWT. Meanwhile, in order to compare crate -
GPT and the GPT2 model under similar number of model parameters, we consider the
GPT2 with a smaller model size, denoted by GPT2-Small, and apply the same training
setup described in Karpathy (2022). Therefore, we first fine-tune GTP2-Base on OWT
before evaluating on the OWT test dataset. The evaluation results on the test datasets are
shown in Table 6. We also present the pre-training loss of crate -GPT with respect to pro-
gressing pre-training steps in Figure 12 (right). From both Table 6 and Figure 12 (right),
16. We use 3 epochs (instead of 8) for the MNLI task for all evaluation except the 24,000 and 30,000 steps
because it suffices to get a good score.
46

--- PAGE 47 ---
White-Box Transformers via Sparse Rate Reduction
Table 6: Zero-shot cross-entropy loss of the crate -GPT2-Base model and GPT2-Small, GPT2-Base
model evaluated on the test split of the datasets (lower is better).
#parameters OWT LAMBADA WikiText PTB Avg
GPT2-Base 124M 2.85 4.12 3.89 4.63 3.87
GPT2-Small 64M 3.04 4.49 4.31 5.15 4.25
crate -GPT2-Base 60M 3.37 4.91 4.61 5.53 4.61
[GPT2] When Mary and John went to the
store, John gave a drink to
Mary (59.1%) them (21.7%) the (5.85%)[crate-GPT2] When Mary and John went
to the store, John gave a drink to
the (29.8%) Mary (28.0%) John (10.1%)
[BERT] He loved sitting between her
brothers on the couch and watching [MASK] .
tv(59.7%) television (12.4%) her (11.0%)[crate-BERT] He loved sitting between her
brothers on the couch and watching [MASK] .
tv(48.9%) couch (38.6%) room (2.31%)
Figure 13: Examples of predictions made by our models and the official models. The token in
Italic is the ground truth token. We compare crate -GPT2-Base to GPT2-Base on the next word
prediction task and crate -BERT-Base to BERT-Medium on the masked language modeling task.
we can observe that the crate architecture can be applied as backbones for generative
language models and achieve competitive performance compared to GPT2.
For more technical details about the evaluation of the pre-trained crate -BERT and
crate -GPT, please refer to Appendix C.4.
Generated texts by CRATE-BERT and CRATE-GPT. We also explored some
qualitative examples generated by crate -BERT and crate -GPT, as shown in Figure 13.
We observe a similar behavior of crate and the standard Transformers, where both answers
make sense and the standard transformer slightly outperforms crate .
4.2 Analysis and Visualization of Learned CRATE Layers
We now study the internal representations of crate at different layers— {Zℓ}L
ℓ=1. We
measure the designed representation learning objective, sparse rate reduction (16), of token
representations from different layers. We mainly consider the crate encoder architectures
on image classification tasks (Section 4.1.1) for the experiments in this subsection.
Do layers of CRATE achieve their design goals? As described in Section 2.3 and
Section 2.4, the MSSA block is designed to optimize the compression term Rc(Z|U[K]) and
theISTA block to sparsify the token representations (corresponding to the sparsification
term∥Z∥0). To understand whether crate indeed optimizes these terms, for each layer ℓ,
we measure (i) the compression term Rc(Zℓ+1/2|Uℓ
[K]) on the MSSA block outputs Zℓ+1/2;
and (ii) sparsity ∥Zℓ+1∥0on the ISTA block outputs Zℓ+1. Specifically, we evaluate these two
terms by using training/validation samples from ImageNet-1K. Both terms are evaluated
at the per-sample level and averaged over B= 1000 samples.
47

--- PAGE 48 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
2 4 6 8 10 12
Layer index - 
60070080090010001100Rc(Z) [SSA block]
Measure coding rate across layers
train
val
2 4 6 8 10 12
Layer index - 
0.200.250.300.350.400.450.50Sparsity [ISTA block]
Measure output sparsity across layers
train
val
Figure 14: Left: The compression term Rc(Zℓ+1/2|Uℓ
[K]) of the MSSA outputs at different layers.
Right : the sparsity of the ISTA output block, ∥Zℓ+1∥0/(d·N), at different layers. (Model: crate -
Small).
In Figure 14 we show the plots of these two key measures at all layers for the learned
crate -small model. We find that as the layer index ℓincreases, both the compression and
the sparsification terms improve in most cases. The increase in the sparsity measure of
the last layer is caused by the extra linear layer for classification.17These results suggest
thatcrate aligns well with the original design goals: once learned, it essentially learns
to gradually compress and sparsity the representations through its layers. In addition,
we also measure the compression and sparsification terms on crate models with different
model sizes as well as intermediate model checkpoints and the results are shown by plots
in Figure 21 of Appendix C.8. The observations are very consistent across all different
model sizes—both the compression and sparsification terms improve in most scenarios.
Models with more layers tend to optimize the objectives more effectively, confirming our
understanding of each layer’s roles.
To see the effect of learning, we present the evaluations on crate -Small trained with
different numbers of epochs in Figure 15. When the model is not trained enough (e.g. un-
trained), the architecture does not optimize the objectives effectively. However, during
training—learning better subspaces Uℓ
[K]and dictionaries Dℓ—the designed blocks start to
optimize the objectives much more effectively.
Visualizing layer-wise token representations. To gain a better understanding of the
token representations of crate , we visualize the output of each ISTA block at layer ℓin
Figure 16. Specifically, we visualize the Zℓ+1via heatmap plots. We observe that the
output Zℓ+1becomes more sparse as the layer increases. Moreover, besides the sparsity, we
also find that Zℓ+1becomes more structured (i.e., low-rank), which indicates that the set
of token representations become closer to linear subspaces, confirming our mental picture
of the geometry of each layer (as in Figure 6 and Figure 4).
Visualizing layer-wise subspaces in multi-head self-attention. We now visualize
theUℓ
[K]matrices used in the MSSA block. In Section 2.3, we assumed that Uℓ
[K]were
17. Note that the learned sparse (tokens) features need to be mixed in the last layer for predicting the class.
The phenomenon of increase in the sparsity measure at the last layer suggests that each class of objects
may be associated with a number of features, and some of these features are likely to be shared across
different classes.
48

--- PAGE 49 ---
White-Box Transformers via Sparse Rate Reduction
2 4 6 8 10 12
Layer index - 
8009001000110012001300140015001600Rc(Z) [SSA block]
Measure coding rate across layers
rand init
epoch 1
epoch 20
epoch 150
2 4 6 8 10 12
Layer index - 
0.150.200.250.300.350.400.450.50Sparsity [ISTA block]
Measure output sparsity across layers
rand init
epoch 1
epoch 20
epoch 150
Figure 15: The compression term Rc(Zℓ+1/2|Uℓ
[K]) (left) and sparsification term ∥Zℓ+1∥0/(d·N)
(right) across models trained with different numbers of epochs. (Model: crate -Base).
Layer=1
0.000.250.500.751.001.251.501.752.00
(a)ℓ= 1.
Layer=2
0.000.250.500.751.001.251.501.752.00 (b)ℓ= 2.
Layer=3
0.000.250.500.751.001.251.501.752.00 (c)ℓ= 3.
Layer=4
0.000.250.500.751.001.251.501.752.00 (d)ℓ= 4.
Layer=5
0.000.250.500.751.001.251.501.752.00
(e)ℓ= 5.
Layer=6
0.000.250.500.751.001.251.501.752.00 (f)ℓ= 6.
Layer=7
0.000.250.500.751.001.251.501.752.00 (g)ℓ= 7.
Layer=8
0.000.250.500.751.001.251.501.752.00 (h)ℓ= 8.
Layer=9
0.000.250.500.751.001.251.501.752.00
(i)ℓ= 9.
Layer=10
0.000.250.500.751.001.251.501.752.00 (j)ℓ= 10.
Layer=11
0.000.250.500.751.001.251.501.752.00 (k)ℓ= 11.
Layer=12
0.000.250.500.751.001.251.501.752.00 (l)ℓ= 12.
Figure 16: Visualizing layer-wise token Zℓrepresentations at each layer ℓ. To enhance the visual
clarity, we randomly extract a 50 ×50 sub-matrix from Zℓfor display purposes. (Model: crate -
Tiny)
incoherent to capture different “views” of the set of tokens. In Fig. 17, we first normalize
the columns in each Uℓ
k, then we visualize the [ Uℓ
1, . . . ,Uℓ
K]∗[Uℓ
1, . . . ,Uℓ
K]∈RpK×pK. The
(i, j)thblock in each sub-figure corresponds to ( Uℓ
i)∗Uℓ
jfori, j∈[K] at a particular layer
ℓ. We find that the learned Uℓ
[K]are approximately incoherent, which aligns well with our
49

--- PAGE 50 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Layer=1
0.20.40.60.81.0
(a)ℓ= 1.
Layer=2
0.20.40.60.81.0 (b)ℓ= 2.
Layer=3
0.20.40.60.81.0 (c)ℓ= 3.
Layer=4
0.20.40.60.81.0 (d)ℓ= 4.
Layer=5
0.20.40.60.81.0
(e)ℓ= 5.
Layer=6
0.20.40.60.81.0 (f)ℓ= 6.
Layer=7
0.20.40.60.81.0 (g)ℓ= 7.
Layer=8
0.20.40.60.81.0 (h)ℓ= 8.
Layer=9
0.20.40.60.81.0
(i)ℓ= 9.
Layer=10
0.20.40.60.81.0 (j)ℓ= 10.
Layer=11
0.20.40.60.81.0 (k)ℓ= 11.
Layer=12
0.20.40.60.81.0 (l)ℓ= 12.
Figure 17: We visualize the [ Uℓ
1, . . . ,Uℓ
K]∗[Uℓ
1, . . . ,Uℓ
K]∈RpK×pKat different layers. The ( i, j)-th
block in each sub-figure corresponds to ( Uℓ
i)∗Uℓ
jfori, j∈[K] at a particular layer ℓ. To enhance
the visual clarity, for each subspace Ui, we randomly pick 4 directions for display purposes. (Model:
crate -Tiny)
assumptions. One interesting observation is that the Uℓ
[K]becomes more incoherent when
the layer index ℓis larger, which suggests that the token representations are more separable.
This mirrors the situation in other popular deep networks (He and Su, 2022a).
4.3 Emergence of Semantic Properties in Learned CRATE Attention Maps
In this subsection, we analyze the attention maps within crate models trained on vision
tasks. Previous work (Caron et al., 2021) use the self-attention in vision transformers to
study semantic segmentation of the input image. Caron et al. (2021) demonstrated that a
specific self-supervised training method, named DINO, can lead to the emergence of seg-
mentation in vision transformers. On the other hand, ViTs trained with supervised learning
do not have such properties. In contrast, as we will present in this subsection, we find that
the white-box design of crate leads to the emergence of segmentation properties in the
network’s self-attention maps, solely through a minimalistic supervised training recipe—the
supervised classification training used in vanilla supervised ViTs . We quantify the segmen-
tation properties of crate both qualitatively and quantitatively and compare the results
with ViTs. Through extensive evaluations, we find that the self-attention maps in white-box
50

--- PAGE 51 ---
White-Box Transformers via Sparse Rate Reduction
Figure 18: Self-attention maps from a supervised CRATE with 8×8patches trained using
classification. The crate architecture automatically learns to perform object segmentation without
a complex self-supervised training recipe or any fine-tuning with segmentation-related annotations.
For each image pair, we visualize the original image on the left and the self-attention map of the
image on the right.
transformers ( crate ) are much more interpretable than vanilla black-box vision transform-
ers.
4.3.1 Experimental Setup
Model architecture We utilize the crate model as described in Section 2.5 at sizes
-S/8 and -B/8 (that is, crate -Small and crate -Base with patch size 8 ×8). We similarly
adopt the ViT model from Dosovitskiy et al. (2021) using the same scales (-S/8 and -B/8),
ensuring consistent configurations between them. Refer to Table 14 for detailed model
performance evaluations on classification tasks.
Datasets. In this subsection, all visual models are trained for classification tasks, using
the methodology described in Section 4.1.1, on the complete ImageNet dataset (Deng et al.,
2009), commonly referred to as ImageNet-21K. This dataset comprises 14,197,122 images
distributed across 21,841 classes. We apply the MaskCut (Wang et al., 2023b) pipeline
on the COCO val2017 (Lin et al., 2014), which consists of 5,000 RGB images, and assess
our models’ performance for both object detection and instance segmentation tasks. All
evaluation procedures are unsupervised, and we do not update the model weights during the
detection and segmentation evaluation processes.
4.3.2 Measuring the Emergence of Segmentation
Visualizing self-attention maps. To qualitatively measure the emergence phenomenon,
we adopt the attention map approach based on the [ CLS] token, which has been widely used
as a way to interpret and visualize transformer-like architectures (Abnar and Zuidema,
2020; Caron et al., 2021). Indeed, we use the same methodology as Abnar and Zuidema
(2020); Caron et al. (2021), noting that in crate the query-key-value matrices are all the
same; a more formal accounting is deferred to Appendix C.10. The visualization results of
self-attention maps are summarized in Figure 18. We observe that the self-attention maps
of the crate model correspond to semantic regions in the input image. Our results suggest
that the crate model encodes a clear semantic segmentation of each image in the network’s
internal representations, which is similar to the self-supervised method DINO (Caron et al.,
51

--- PAGE 52 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
DINO CRATE 
 Superv. ViT Superv. CRATE 
Figure 19: Visualization of segmentation with MaskCut on COCO val2017 (Lin et al.,
2014). Top and Bottom Rows :crate architecture clearly detects main objects in the image when
trained using either supervised classification technique in Section 4.1.1 or the DINO self-supervised
technique in Section 4.1.3 (Caron et al., 2021). Middle Row : Note that compared to crate , ViT
trained via classification often fails to detect main objects in the images (columns 2, 3, 4).
2021). In contrast, as shown in Figure 27 in the Appendices, the vanilla ViT trained on
supervised classification does not exhibit similar segmentation properties.
Object detection and fine-grained segmentation. To further validate and evaluate
the rich semantic information captured by crate , we employ MaskCut (Wang et al., 2023b),
a recent effective approach for object detection and segmentation that does not require
human annotations. As usual, we provide a more detailed methodological description in
Appendix C.10. This procedure allows us to extract more fine-grained segmentation for
an image based on the token representations learned by different methods. In Figure 19,
we visualize the fine-grained segmentation produced by MaskCut on features from ViT
trained by classification, crate trained by classification as in Section 4.1.1, and crate
trained via DINO as in Section 4.1.3, respectively. We compare the segmentation and
detection performance in Table 7. Based on these results, we observe that MaskCut on
features learned with supervised ViT typically fails to produce good segmentation masks,
for example, the first image in Figure 19 and the ViT-S/8 row in Table 7. On the other hand,
we notice that, regardless of the training task (supervised or unsupervised), crate is able
to capture semantically meaningful boundaries of the main objects in an image. Compared
to ViT, crate provides better internal representation tokens for both segmentation and
detection.
4.3.3 Analysis of Segmentation in CRATE
Segmentation emerges through minimalistic design. Our empirical results demon-
strate that self-supervised learning, as well as the specialized design options in DINO (Caron
et al., 2021) (e.g., momentum encoder, student and teacher networks, self-distillation, etc.)
are not necessary for the emergence of segmentation. We contend that an equally-promising
52

--- PAGE 53 ---
White-Box Transformers via Sparse Rate Reduction
Table 7: Object detection and fine-grained segmentation via MaskCut on COCO
val2017 (Lin et al., 2014) . We consider models with different scales and evaluate the aver-
age precision measured by COCO’s official evaluation metric. The first four models are pre-trained
with image classification tasks under label supervision; the bottom three models are pre-trained via
the DINO self-supervised technique (Caron et al., 2021). crate conclusively performs better than
the ViT at detection and segmentation metrics when both are trained using supervised classification.
Detection Segmentation
Model Train AP 50 AP75 AP AP 50 AP75 AP
crate -S/8 Supervised 2.9 1.0 1.1 1.8 0.7 0.8
crate -B/8 Supervised 2.9 1.0 1.3 2.2 0.7 1.0
ViT-S/8 Supervised 0.1 0.1 0.0 0.0 0.0 0.0
ViT-B/8 Supervised 0.8 0.2 0.4 0.7 0.5 0.4
crate -B/8 DINO 3.5 1.1 1.6 2.4 0.5 1.0
ViT-S/8 DINO 5.0 2.0 2.4 4.0 1.3 1.7
ViT-B/8 DINO 5.1 2.3 2.5 4.1 1.3 1.8
Head 0 
“Leg” Head 1 
“Body” Head 3 
“Face” Head 4 
“Ear” Head 0 
“Leg” Head 1 
“Body” Head 3 
“Face” Head 4 
“Ear” 
Figure 20: Visualization of semantic heads. We forward a mini-batch of images through a
supervised crate and examine the attention maps from all the heads in the penultimate layer. We
visualize a selection of attention heads to show that certain heads convey specific semantic meaning,
i.e.head 0 ↔“Legs” ,head 1 ↔“Body” ,head 3 ↔“Face” ,head 4 ↔“Ear” .
approach to promote segmentation properties in transformer is to design the transformer
53

--- PAGE 54 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
architecture with the structure of the input data in mind . This finding of crate represents
thefirst supervised vision model with emergent segmentation properties , and establishes
white-box transformers as a promising direction for interpretable data-driven representa-
tion learning in foundation models.
Identifying semantic properties of attention heads. We are interested in capturing
the semantic meaning of certain attention heads ; this is an important task for interpretabil-
ity and is already studied for language transformers (Olsson et al., 2022). Intuitively, each
head captures certain features of the data. Given a crate model, we first forward an input
image (e.g. a horse image as in Figure 20) and select four attention heads which seem to
have semantic meaning by manual inspection. After identifying the attention heads, we
visualize the self-attention map of these heads on other input images. Interestingly, we find
that each of the selected attention heads captures a different part of the object, and even a
different semantic meaning. For example, the attention head displayed in the first column
of Figure 20 captures the legs of different animals, and the attention head displayed in the
last column captures the ears and head. This parsing of the visual input into a part-whole
hierarchy has been a fundamental goal of learning-based recognition architectures since de-
formable part models (Felzenszwalb and Huttenlocher, 2005; Felzenszwalb et al., 2008) and
capsule networks (Hinton et al., 2011; Sabour et al., 2017)—strikingly, it emerges from the
white-box design of crate within our simple supervised training setup.
5 Conclusions and Open Directions
In this paper, we propose a new theoretical framework that allows us to derive a deep
transformer-like network architecture, named crate , as an incremental optimization scheme
to learn compressed and sparse representation of the input data, measured by a principled
objective for information gain: the sparse rate reduction . The so-derived and learned deep
architectures are not only fully mathematically interpretable but also consistent on a layer-
by-layer level with their design objective. Despite being arguably the simplest among all
possible designs, these networks already demonstrate strong performance – slightly worse
than existing black-box models – on large-scale real-world (image or text) datasets and
tasks (discriminative and generative), and in supervised, unsupervised, and self-supervised
settings. In fact, recent follow-up work (Yang et al., 2024) suggested that one can sig-
nificantly improve the performance of crate -like white-box models with a light training
recipe on vision tasks, and to a large extent close the gap between white-box models such
ascrate and the conventional Transformers on some tasks. Still, a gap persists. It would
be interesting to investigate how to construct white-box models that can achieve same level
of performance as black-box models, but we leave this for future work.
We believe this work truly helps bridge the gap between theory and practice of deep neural
networks, as well as helps unify seemingly separate approaches to learning and representing
data distributions through the perspective of data compression. These approaches include,
but are not limited to: rate reduction, denoising-diffusion, and transformers. As we have
seen, in these approaches, the role of each layer of the associated deep neural networks
can be interpreted mathematically as operators to incrementally compress (denoise) and
transform the data distribution according to its low-dimensional structures (modeled as
Gaussian mixtures). In particular, our work provides a principled justification for slightly
54

--- PAGE 55 ---
White-Box Transformers via Sparse Rate Reduction
earlier empirical syntheses of some of these approaches, for example the recent success of
Transformer-type architectures for denoising in the context of diffusion models (Peebles and
Xie, 2022).
To a large extent, our work suggests that a universal way to effectively and efficiently
learn a data distribution with intrinsically low-dimensional structures in a high-dimensional
space is through incremental compression , as already stated as a slogan by Wright and Ma
(2022):
We compress to learn, and we learn to compress.
This work and the previous work on ReduNet by Chan et al. (2022) together strongly suggest
that various deep neural networks are simply means to an end—compression. Different
optimization schemes for compression are manifested as different network architectures and
operators, e.g. LeNet, LISTA, ResNet, ReduNet, Transformers, which are now elucidated
by CRATE.
From a practical standpoint, it is clear that this new framework can now provide us with
theoretical guidelines to design and justify new, potentially more powerful, deep architec-
tures for representation learning that are based on more advanced optimization techniques
or strategies, instead of the basic gradient-based technique used to construct crate . Also,
in this work, we have only used crate to learn deterministic autoencoding models. We
believe that this framework can be extended to learn structured representations of the data
distribution based on more advanced structured diffusion and denoising. This could po-
tentially lead to more controllable and consistent generative methods, since improving the
consistency and efficiency of existing diffusion-based generative models remains a challeng-
ing open problem (Song et al., 2023).
Furthermore, as suggested by Dai et al. (2022); Ma et al. (2022), for any system to learn
a low-dimensional data distribution autonomously and continuously , one needs to integrate
the encoder and decoder not only as an autoencoder but as a closed-loop transcription :
x∈RDf(x)− − − − → z∈Rdg(z)− − − − → bx∈RDf(x)− − − − → bz∈Rd, (90)
which allows the system to correct itself by minimizing the discrepancy between the internal
representations zof the data and bzof the generated approximation, without any external
help (by human) to compare between the data xandbx. So far, the effectiveness of such a
closed-loop transcription system has only been demonstrated with black-box deep architec-
tures such as the ResNet (Dai et al., 2022; Tong et al., 2023). It would be possible now to
build a closed-loop transcription with purely white-box encoder and decoder designs. This
could ultimately enable us to develop complete learning systems that are capable of learning
autonomously and continuously an internal data representation that is fully mathematically
interpretable, controllable, and eventually self-consistent, or “aligned”, with the true exter-
nal data distribution. The so-learned representation, just like our acquired memory, can
support both discriminative and generative tasks, and serve both recognition and prediction
purposes.
Last but not least, we hope that this work provides an alternative viewpoint that may
help clarify the ultimate capabilities of modern artificial intelligence (AI) systems, which
are typically based on deep networks such as transformers. Just as with all other natural
55

--- PAGE 56 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
phenomena or technical innovations that were once “black boxes” to people, significant
confusion and anxiety is arising in society about the potential or implications of emerging
new AI systems, including the recent large language models (LLMs) such as GPT-4 (Ope-
nAI, 2023b), large image generation models such as Midjourney and DALL-E 3 (Betker
et al., 2023), and many other multi-modal large language models (MLLMs) (Liu et al.,
2023; OpenAI, 2023a). From the perspective of this work, in which we developed a clear
understanding of transformer-type learning systems, these large models are unlikely to do
anything beyond purely mechanical data compression (encoding) and interpolation (decod-
ing). That is, this work suggests that regarding essence of these existing large AI models,
however magical and mysterious they might appear to be,
compression is all there is.
This leaves us with a much more profound and challenging question: whether compression
alone is all one needs for a system, natural or artificial, to develop general intelligence or
even sentience? We believe that compression, employed correctly, is merely the very first
step of a long path towards general intelligence.
Acknowledgments and Disclosure of Funding
Yaodong Yu would like to thank Kwan Ho Ryan Chan for the valuable discussions they had
regarding visualizing tokens in vision transformers. Yaodong Yu and Yi Ma acknowledge
support from the joint Simons Foundation-NSF DMS grant #2031899, the ONR grant
N00014-22-1-2102, and Yi Ma also acknowledges partial support from TBSI, InnoHK, and
the University of Hong Kong. Other members of the team were partially supported by NSF
1704458, the Northrop Grumman Mission Systems Research in Applications for Learning
Machines (REALM) initiative, NIH NIA 1R01AG067396, and ARO MURI W911NF-17-1-
0304.
56

--- PAGE 57 ---
White-Box Transformers via Sparse Rate Reduction
Appendix
Appendix A. Technical Details for Section 2
A.1 Companion to Section 2.3
We now give the derivation of the approximation alluded to in Section 2.3. In the process,
we make some simple approximations and technical assumptions. The validity of these
assumptions may be explored, and the approximations refined, altogether providing a more
complex (and possibly more performant) resulting self-attention like operator. For the
sake of technical clarity and simplicity in this work, we make perhaps the simplest possible
choices . As a result, we do not claim that our network is optimally designed, but rather
that the principles we develop in this work (compression, denoising, sparsification, unrolled
optimization) can provide the backbone for far superior and more interpretable network
architectures in the future on sundry tasks. As it is, with our straightforward, simple, and
interpretable design, we still obtain meaningful conceptual results and very solid empirical
performance.
Recall that β=p/(nϵ2).
Approximation 9. LetZ∈Rd×nhave unit-norm columns, and U[K]= (U1, . . . ,UK)such
that each Uk∈Rd×pis an orthogonal matrix, the (Uk)K
k=1are incoherent, and the columns
ofZapproximately lie onSK
k=1Span( Uk). Let κ >0. Then
Z−κ∇ZRc(Z|U[K])≈(1−κβ)Z+κβMSSA(Z|U[K]), (91)
where as in Section 2.3 we have
SSA(Z|Uk) = (U∗
kZ) softmax(( U∗
kZ)∗(U∗
kZ)), (92)
MSSA(Z|U[K]) =β
U1, . . . ,UK
SSA(Z|U1)
...
SSA(Z|UK)
, (93)
where softmax( ·)is the softmax operator (applied to each column of an input matrix), i.e.,
softmax( v) =1Pn
i=1evi
ev1
...
evn
, (94)
softmax 
v1, . . . ,vK
=
softmax( v1), . . . , softmax( vK)
. (95)
Proof. According to (31), the gradient ∇ZRc(Z|U[K]) is
∇ZRc(Z|U[K]) =βKX
k=1UkU∗
kZ(I+β(U∗
kZ)∗(U∗
kZ))−1. (96)
Notice that according to (Chan et al., 2022), the gradient is precisely the residual of a
ridge regression for each (projected) token U∗
kziusing other projected tokens U∗
kzjas the
regressors, hence being the residual of an auto-regression.
57

--- PAGE 58 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
However, as we have seen in the work of ReduNet (Chan et al., 2022), computing the
inverse ( I+β(U∗
kZ)∗(U∗
kZ))−1can be expensive. Hence for computational efficiency, we
may approximate it with the first order term of its von Neumann expansion:
∇ZRc(Z|U[K]) =βKX
k=1UkU∗
kZ
I+β(U∗
kZ)∗(U∗
kZ)−1
(97)
≈βKX
k=1UkU∗
kZ
I−β(U∗
kZ)∗(U∗
kZ)
(98)
=βKX
k=1Uk
U∗
kZ−βU∗
kZ[(U∗
kZ)∗(U∗
kZ)]
(99)
Notice that the term ( U∗
kZ)∗(U∗
kZ) is the auto-correlation among the projected tokens.
As the tokens Zmay be from different subspaces, we would prefer to use only tokens that
belong to the same subspace to regress and compress themselves. Hence we may convert
the above correlation term into a subspace-membership indicator with a softmax operation,
whence (99) becomes
∇ZRc(Z|U[K])≈βKX
k=1Uk
U∗
kZ−βU∗
kZ[(U∗
kZ)∗(U∗
kZ)]
(100)
≈βKX
k=1UkU∗
kZ−β2KX
k=1Uk
U∗
kZsoftmax(( U∗
kZ)∗(U∗
kZ))
(101)
Then, we can rewrite the above approximation to the gradient of Rcas:
∇ZRc(Z|U[K])≈βKX
k=1UkU∗
kZ−β2KX
k=1Uk(U∗
kZsoftmax(( U∗
kZ)∗(U∗
kZ))) (102)
=βKX
k=1UkU∗
kZ−β2KX
k=1UkSSA(Z|Uk) (103)
=β KX
k=1UkU∗
k!
Z
| {z }
≈Z−β2
U1,···,UK
SSA(Z|U1)
...
SSA(Z|UK)
 (104)
≈βZ−β2
U1,···,UK
SSA(Z|U1)
...
SSA(Z|UK)
 (105)
=βZ−βMSSA(Z|U[K]) (106)
Thus the gradient descent step with learning rate κ >0 gives
Z−κ∇ZRc(Z|U[K])≈(1−κβ)Z+κβMSSA(Z|U[K]). (107)
58

--- PAGE 59 ---
White-Box Transformers via Sparse Rate Reduction
A.2 Companion to Section 2.4
In the main text Section 2.4, our connection between the second step of the alternating
minimization and the LASSO optimization was high-level and heuristic. In some sense, the
choice to pose the minimization step as a LASSO was a simple, reliable, and interpretable
choice which works well in practice, but is nonetheless not backed up by rigorous theoretical
justification.
In the following subsection, we provide a mathematical justification for a reformulation
of the minimization step using a majorization-minimization framework. We further show
that the associated unrolled optimization step bears a strong resemblance to the ISTA step.
This confirms our earlier discussion — we took the simplest possible choice in designing
crate , but by more rigorous derivation we can uncover alternative operators—which
differ from the ISTA operator defined in (43)in Section 2.4— that nonetheless have
the same conceptual function and may perform better in practice.
Assumptions. In this section, we present a rigorous optimization analysis of an incremen-
tal minimization approach to the objective (25). We will show that under two simplifying
assumptions, namely
1. The columns of Zℓ+1/2are normalized, in the sense that diag(( Zℓ+1/2)∗Zℓ+1/2) =1;18
2. We have d≥n,19and the columns of Zℓ+1/2are orthogonal, so that ( Zℓ+1/2)∗Zℓ+1/2=
I.20
the approach leads to an update iteration that is equal to a slightly simplified version
of the ISTA block (43). We see this as a justification for our derivation in Section 2.4,
which obtained the ISTA block by introducing an additional simplifying assumption on the
distribution of the data at layer ℓ.
Analysis. Following (42), we will consider the natural relaxation of the ℓ0“norm” to the
ℓ1norm, and incorporate a nonnegativity constraint. Consider the objective
φ(Z) =λ∥Z∥1+χ{Z≥0}(Z)−1
2log det ( I+αZ∗Z)
| {z }
R(Z), (108)
18. This is a natural assumption in transformer-type architectures such as crate due to the use of LayerNorm
blocks—although these blocks (indeed, as we use them in crate ) include trainable mean and scale offsets
as well as an additional mean subtraction operation (Phuong and Hutter, 2022), they are initialized to
have zero mean and unit norm, hence this assumption corresponds to an analysis of the network at its
initialization.
19. This assumption is without loss of generality, as we will see in the analysis below. The reason is that
Z∗ZandZZ∗have the same nonzero eigenvalues regardless of the shape of Z, which implies that
log det( I+αZ∗Z) = log det( I+αZZ∗). In particular, interpreting the norms appropriately (with a
slight abuse of notation), we have φ(Z) =φ(Z∗), so for the purposes of analysis we can always proceed
as though Zis a tall matrix (as long as we do not use any special properties of αin our derivation).
20. This assumption is strictly stronger than the previous one, and strictly stronger than an assumption of
incoherence on the columns. It corresponds to the representation Zℓ+1/2being non-collapsed, which we
expect to hold at initialization due to the projections U[K]being random.
59

--- PAGE 60 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
where Z∈Rd×nandα=d/(nε2), and χ{Z≥0}denotes the characteristic function for the
set of elementwise-nonnegative matrices Z. As in Appendix A.1, we calculate
∇ZR(Z) =αZ(I+αZ∗Z)−1. (109)
We consider an incremental optimization scheme for the highly nonlinear and nonconvex ob-
jective φ. Following Section 2.3, we optimize locally at a “post-compression” iterate Zℓ+1/2.
We follow the standard proximal majorize-minimize framework (Wright and Ma, 2022) for
incremental/local optimization: this begins with the second-order Taylor expansion for the
smooth part of φin a neighborhood of the current iterate Zℓ+1/2:
R(Z) =R(Zℓ+1/2) +D
∇ZR(Zℓ+1/2),Z−Zℓ+1/2E
+Z1
0(1−t)D
Z−Zℓ+1/2,∇2R(Zt)
Z−Zℓ+1/2E
dt,(110)
where for any Z∈Rd×n,Zt=tZℓ+1/2+(1−t)Z. The proximal majorization-minimization
approach alternates two steps to minimize φ:
1. First, use assumptions on Zℓ+1/2to derive an upper bound on the operator norm of
the Hessian ∇2R(Z) over the effective domain of the optimization problem. We will
write Lfor this (uniform) upper bound. This yields a quadratic upper bound for the
smooth part of the objective φ.
2. Then, alternately minimize the smooth part of the quadratic upper bound as a function
ofZ, and take a proximal step on the nonsmooth part. It can be shown (Wright and
Ma, 2022) that corresponds to the iteration
Z+= prox λ
L(∥ · ∥1+χ{Z≥0})
Z+1
L∇ZR(Z)
(111)
In the alternating minimization setting of this paper for optimizing (15), we only take
one such step, starting at Zℓ+1/2.
We will instantiate this program below, showing quantitative error bounds related to our
assumptions above as necessary. Rather than directly applying the iteration (111), we will
derive it below under our aforementioned assumptions.
Starting at (110), our first task is to upper bound the quadratic residual. This corre-
sponds to estimating
D
Z−Zℓ+1/2,∇2R(Zt)
Z−Zℓ+1/2E
(112)
≤sup
t∈[0,1]∇2R(Zt)
ℓ2→ℓ2Z−Zℓ+1/22
F(113)
with Cauchy-Schwarz. Using Lemma 10, we can estimate the operator norm term in the
previous bound in terms of properties of Zℓ+1/2. We need to bound
αsup
∥∆∥F≤1 
∆−αZt(I+αZ∗
tZt)−1(Z∗
t∆+∆∗Zt)
(I+αZ∗
tZt)−1
F, (114)
60

--- PAGE 61 ---
White-Box Transformers via Sparse Rate Reduction
and Lemma 11 gives that this term is no larger than 9 α/4 for any Zand any t. With this
estimate and (110), we have a quadratic upper bound for −R(Z):
−R(Z)≤ −R(Zℓ+1/2) +D
−∇ZR(Zℓ+1/2),Z−Zℓ+1/2E
+9α
8Z−Zℓ+1/22
F.(115)
Meanwhile, by our assumptions above, we have
−∇ZR(Zℓ+1/2) =−αZℓ+1/2(I+αI)−1=−α
1 +αZℓ+1/2. (116)
We now minimize the preceding quadratic upper bound as a function of Z. Differentiating,
the minimizer Zoptis calculated as
Zopt=
1 +4
9(1 + α)
Zℓ+1/2, (117)
and it is well-known that the proximal operator of the sum of χ{Z≥0}andλ∥ · ∥ 1is simply
the one-sided soft-thresholding operator (Wright and Ma, 2022)
proxχ{Z≥0}+λ∥ · ∥1(Z) = max {Z−λ1,0}, (118)
where the maximum is applied elementwise. As in Section 2.4, we may write this elementwise
maximum simply as ReLU. Thus, one step of proximal majorization-minimization under
our simplifying assumptions takes the form
Zℓ+1= ReLU
1 +4
9(1 + α)
Zℓ+1/2−4λ
9α1
, (119)
Finally, we point out one additional elaboration which introduces the dictionary Dthat
appears in the ISTA block in Section 2.4. Notice that for any orthogonal D, one has
R(DZ) =R(Z) for every Z. This symmetry implies equivariance properties of ∇ZR(Z)
and∇2
ZR(Z): for every Zand every ∆and every orthogonal D,
D∇ZR(Z) =∇ZR(DZ), (120)
⟨D∆,∇2
ZR(Z) (D∆)⟩=⟨∆,∇2
ZR(DZ) (∆)⟩. (121)
Hence the quadratic Taylor expansion (110) can be written equivalently as
R(Z) =R(D∗Zℓ+1/2) +D
∇ZR(D∗Zℓ+1/2),Z−D∗Zℓ+1/2E
+Z1
0(1−t)D
Z−D∗Zℓ+1/2,∇2R(D∗Zt)
Z−D∗Zℓ+1/2E
dt,
(122)
for any orthogonal D. The significance of this is that we have obtained an expression equiva-
lent to (110), but with Zℓ+1/2replaced by D∗Zℓ+1/2; moreover, because our approximation
arguments above are not affected by left-multiplication of Zℓ+1/2by an orthogonal matrix
(this operation does not change the norms of the columns of Zℓ+1/2, or their correlations,
61

--- PAGE 62 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
and hence the matrix’s incoherence), we can apply exactly the same line of reasoning above
to obtain that an equivalent proximal majorization-minimization iteration is given by
Zℓ+1= ReLU
1 +4
9(1 + α)
D∗Zℓ+1/2−4λ
9α1
, (123)
for any orthogonal dictionary D. This gives an update quite similar to the ISTA block
(43) in the case where the dictionary used in Section 2.4 is orthogonal, but without a skip
connection. The operator defined in (123) is not exactly the same as the ISTA operator
defined in (43) but exhibits substantial similarities.
We thus obtain a natural white-box version of this part of the architecture, along with
the natural interpretation that its purpose is to sparsify the compressed tokens Zℓ+1/2with
respect to a (learnable) dictionary , which accords with recent empirical studies (Li et al.,
2023b).
Other architectures? As we mentioned at the start of this section, the preceding deriva-
tion is performed in the most elementary possible setting in order to demonstrate the
majorization-minimization approach for layer design. More precise approximations or as-
sumptions may lead to superior layer designs that better optimize the target objective (15)
(and in particular (25)). We mention two here:
1.Beyond exactly-incoherent features : our derivations above assumed that the in-
coming representations Zℓ+1/2were already maximal for the expansion term Rin
(25). It is desirable to obtain a ‘perturbative’ derivation, which applies in cases where
Zℓ+1/2is not fully orthogonal, but instead near-orthogonal, in particular incoherent
(Wright and Ma, 2022). The derivations above can be adapted to this setting; the per-
turbation bounds become slightly more delicate, and the ultimate layer (123) changes
to involve additional normalization.
2.Beyond orthogonal dictionaries : The symmetries of the expansion term Rin (25)
may be followed to lead to a pair of dictionaries DandD′and an objective that
sparsifies DZD′. This type of transformation is suggestive of popular architectures
that mix over tokens (Tolstikhin et al., 2021; Trockman et al., 2023), however we con-
sider the simpler form DZin this work. In addition, we have focused for simplicity on
orthogonal dictionaries D; as in the previous bullet, one may consider in a similar way
dictionaries Dwhich are complete and near-orthogonal. Adapting the derivation to
overcomplete dictionaries is an interesting future direction that we expect to improve
the scalability of crate ; one avenue to achieve this could be increasing the number
of projections U[K]and their embedding dimensions.
A.2.1 Auxiliary Lemmas
Lemma 10. Consider the function
R(Z) =1
2log det ( I+αZ∗Z), (124)
where α >0is a constant. Then we have
∇ZR(Z) =αZ(I+αZ∗Z)−1, (125)
62

--- PAGE 63 ---
White-Box Transformers via Sparse Rate Reduction
and the Hessian operator ∇2
ZR(Z):Rd×n→Rd×nsatisfies that for any ∆∈Rd×n,
∇2
ZR(Z) (∆) (126)
=α∆(I+αZ∗Z)−1−α2Z(I+αZ∗Z)−1(Z∗∆+∆∗Z) (I+αZ∗Z)−1. (127)
Proof. The gradient calculation follows from (Yu et al., 2020), for example. For the Hessian,
we use the usual approach to calculating derivatives: if ∆is any matrix with the same shape
asZandt >0,
∇2
ZR(Z) (∆) =∂
∂t
t=0[t7→ ∇ ZR(Z+t∆)], (128)
valid since Ris smooth. We have
∇ZR(Z+t∆) (129)
=α(Z+t∆) (I+α(Z+t∆)∗(Z+t∆))−1(130)
=α(Z+t∆) (I+αZ∗Z+αt[Z∗∆+∆∗Z+t∆∗∆])−1(131)
=α(Z+t∆)
I+αt(I+αZ∗Z)−1[Z∗∆+∆∗Z+t∆∗∆]−1
(I+αZ∗Z)−1(132)
=α(Z+t∆) ∞X
k=0(−αt)k
(I+αZ∗Z)−1[Z∗∆+∆∗Z+t∆∗∆]k!
(I+αZ∗Z)−1,(133)
where in the fourth line we require that tis sufficiently close to 0 in order to invoke the
Neumann series. First, notice that the term involving ∆∗∆does not play a role in the final
expression: after we differentiate with respect to tand take a limit t→0, terms arising due
to differentiation of t7→t∆∗∆go to zero, because whenever the summation index k >0
we have a term ( −αt)kthat goes to zero as t→0. We thus obtain with the product rule
∂
∂t
t=0[t7→ ∇ ZR(Z+t∆)] (134)
=α∆(I+αZ∗Z)−1−α2Z(I+αZ∗Z)−1(Z∗∆+∆∗Z) (I+αZ∗Z)−1. (135)
Lemma 11. One has
sup
∥∆∥F≤1 
∆−αZt(I+αZ∗
tZt)−1(Z∗
t∆+∆∗Zt)
(I+αZ∗
tZt)−1
F≤9
4. (136)
Proof. Fix∆satisfying ∥∆∥F≤1. By the triangle inequality,
 
∆−αZt(I+αZ∗
tZt)−1(Z∗
t∆+∆∗Zt)
(I+αZ∗
tZt)−1
F(137)
≤∆(I+αZ∗
tZt)−1
F+αZt(I+αZ∗
tZt)−1(Z∗
t∆+∆∗Zt)(I+αZ∗
tZt)−1
F.(138)
For the first term, we note that
∆(I+αZ∗
tZt)−1
F= 
(I+αZ∗
tZt)−1⊗I
vec(∆)
F, (139)
63

--- PAGE 64 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
and since ( I+αZ∗
tZt)−1⪯I, we obtain from Cauchy-Schwarz21
∆(I+αZ∗
tZt)−1
F≤ ∥∆∥F. (140)
We can use a similar idea to control the second term. We have from the triangle inequality
Zt(I+αZ∗
tZt)−1(Z∗
t∆+∆∗Zt)(I+αZ∗
tZt)−1
F(141)
≤Zt(I+αZ∗
tZt)−1Z∗
t∆(I+αZ∗
tZt)−1
F(142)
+(I+αZ∗
tZt)−1Z∗
t∆(I+αZ∗
tZt)−1Z∗
t
F. (143)
For the first term, we have
Zt(I+αZ∗
tZt)−1Z∗
t∆(I+αZ∗
tZt)−1
F(144)
= 
(I+αZ∗
tZt)−1⊗Zt(I+αZ∗
tZt)−1Z∗
t
vec(∆)
F(145)
≤σmax 
(I+αZ∗
tZt)−1
σmax 
Zt(I+αZ∗
tZt)−1Z∗
t
∥∆∥F (146)
≤1
α∥∆∥F. (147)
The last estimate follows from a computation using the SVD of Zt. Meanwhile, we have
for the second term by a similar argument (using the fact that the singular values of Aand
A∗are identical for any matrix A)
(I+αZ∗
tZt)−1Z∗
t∆(I+αZ∗
tZt)−1Z∗
t
F≤σmax 
(I+αZ∗
tZt)−1Z∗
t2∥∆∥F (148)
≤1
4α∥∆∥F, (149)
where once again the estimate follows from a computation involving the SVD of Zt(together
with the fact that the function σ7→σ/(1+ασ2) is bounded on σ≥0 by 1 /(2√α)). Putting
it together, we have obtained
 
∆−αZt(I+αZ∗
tZt)−1(Z∗
t∆+∆∗Zt)
(I+αZ∗
tZt)−1
F≤9
4∥∆∥F, (150)
which gives the claim after taking suprema.
Appendix B. Technical Details for Section 3
B.1 An Overview of Diffusion Processes
In this section, we give an overview of the basics of time-reversible Itˆ o diffusion processes,
the mathematical foundation for diffusion models. This is to make this paper more self-
contained by providing knowledge about general diffusion processes that we will apply to
our special models. The coverage adapts that of Millet et al. (1989b); Song et al. (2021b);
Karras et al. (2022).
21. Recall that the eigenvalues of a Kronecker product of symmetric matrices are the tensor product of the
eigenvalues (with multiplicity).
64

--- PAGE 65 ---
White-Box Transformers via Sparse Rate Reduction
Consider a generic Itˆ o diffusion process ( z(t))t∈[0,T], where z(t) is an Rm-valued random
variable, given by the SDE
dz(t) =b(z(t), t) dt+ Σ(z(t), t) dw(t),z(0)∼P, ∀t∈[0, T] (151)
where wis a Brownian motion and Pis some probability measure on Rm(in this case
representing the data distribution). Here the drift coefficient b:Rm×R→Rmanddiffusion
coefficient Σ:Rm×R→Rm×mare functions. To make sense of (151) and also verify the
existence of strong (i.e., pathwise well-defined) solutions, we need some regularity on them,
and we choose the following assumption:
A1.band Σ have some spatial smoothness and do not grow too fast, i.e., there is a constant
K≥0 such that for all x,ez∈Rmwe have
sup
t∈[0,T][∥Σ(x, t)−Σ(ez, t)∥F+∥b(x, t)−b(ez, t)∥2]≤K∥x−ez∥2 (152)
sup
t∈[0,T][∥Σ(x, t)∥F+∥b(x, t)∥2]≤K(1 +∥x∥2). (153)
In general, z(t) may not have a density w.r.t. the Lebesgue measure on Rm. For example,
suppose that Pis supported on some low-dimensional linear subspace (or even a Dirac
delta measure), and take Σ to be the orthoprojector onto this subspace. Then z(t) will
be supported on this subspace for all tand thus not have a density w.r.t. the Lebesgue
measure. Thus, when further discussing processes of the type (151), we make the following
assumption
A2.z(t) has a probability density function p(·, t) for all t >0.
This is guaranteed by either of the following conditions (Millet et al., 1989b):
A2.1 band Σ are differentiable in ( x, t) and have H¨ older-continuous derivatives, and P
has a density w.r.t. the Lebesgue measure;
A2.2 The event
{rank(Σ( z(s), s)) =mfor all sin some neighborhood of 0 } (154)
happens P-almost surely.
Define Ψ: Rm×R→Rm×mby
Ψ(x, t).= Σ(x, t)Σ(x, t)∗. (155)
To discuss time-reversibility, we also need the following local integrability condition, which
is another measure of sharp growth of the coefficients (or precisely their derivatives):
A3. The functions ( x, t)7→ ∇ x·(Ψ(x, t)p(x, t)) are integrable on sets of the form D×[t0,1]
fort0>0 and Da bounded measurable subset of Rm:
Z1
t0Z
D∥∇x·(Ψ(x, t)p(x, t))∥2dxdt <∞. (156)
65

--- PAGE 66 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
To write the notation out more explicitly,
∇x·(Ψ(x, t)p(x, t)) =
∇x·(Ψ1(x, t)p(x, t))
...
∇x·(Ψm(x, t)p(x, t))
 (157)
where ∇x·(Ψi(x, t)p(x, t)) =mX
j=1∂
∂xj[Ψij(x, t)p(x, t)] (158)
where Ψiis the ithrow of Ψ transposed to a column, and Ψijis the ( i, j)thentry of Ψ.
Note that Millet et al. (1989b) phrases this in terms of an local integrability condition
on each |∇x·(Ψi(x, t)p(x, t))|, which would naturally give a local integrability condition
on∥∇x·(Ψ(x, t)p(x, t))∥∞. However, all norms on Rmare equivalent, and so this leads
to a local integrability condition for ∥∇x·(Ψ(x, t)p(x, t))∥2as produced. Note that the
assumptions do not guarantee that the involved derivatives exist, in which case they are
taken in the distributional (e.g., weak) sense, whence they should exist (Millet et al., 1989b).
Under assumptions A1—A3, Millet et al. (1989b) guarantees the existence of another
process ( ez(t))t∈[0,T]such that the laws of z(t) andez(T−t) are the same for all t∈[0, T].
This process ( ez(t))t∈[0,T]is called the time reversal of (z(t))t∈[0,T], and is shown to have
law given by
dez(t) =b←(ez(t), t) dt+ Σ←(ez(t), t) dw←(t),ez(0)∼p(·, T),∀t∈[0, T] (159)
where w←(t) is an independent Brownian motion and
b←(x, t) =−b(x, T−t) +∇x·[Ψ(x, T−t)p(x, T−t)]
p(x, T−t)(160)
=−b(x, T−t) +∇x·Ψ(x, T−t) + Ψ( x, T−t)[∇xlogp(x, T−t)], (161)
Σ←(x, t) = Σ( x, T−t). (162)
We would next like to develop an ODE which transports the probability mass Pin
the same way as (151) — namely, find another process ( z(t))t∈[0,T]which has deterministic
dynamics, yet has the same law as ( z(t))t∈[0,T]. Song et al. (2021b) looks at the Fokker-
Planck equations (which can be defined, at least in a weak sense, under assumptions A1–A2)
and manipulates them to get the following dynamics for z(t):
dz(t) =b(z(t), t) dt, z(0)∼P, ∀t∈[0, T], (163)
where b(x, t) =b(x, t)−1
2·∇x·[Ψ(x, t)p(x, t)]
p(x, t)(164)
=b(x, t)−1
2∇x·Ψ(x, t)−1
2Ψ(x, t)[∇xlogp(x, t)]. (165)
Now to get a similar process for ez(t), namely a process ( ez(t))t∈[0,T]which evolves determin-
istically yet has the same law as ( ez(t))t∈[0,T], we may either take the time reversal of (163)
or apply the Fokker-Planck method to (159), in both cases obtaining the same dynamics:
dez(t) =b←(ez(t), t) dt,ez(0)∼p(·, T),∀t∈[0, T], (166)
66

--- PAGE 67 ---
White-Box Transformers via Sparse Rate Reduction
where
b←(x, t) =−b(x, T−t) (167)
=−b(x, T−t) +1
2·∇x·[Ψ(x, T−t)p(x, T−t)]
p(x, T−t)(168)
=−b(x, t) +1
2∇x·Ψ(x, T−t) +1
2Ψ(x, T−t)[∇xlogp(x, T−t)]. (169)
The quantity ∇xlogp(x, t) is of central importance; it is denoted the score at time t, and we
use the notation s(x, t).=∇xlogp(x, t) for it. With this substitution, we have the following
dynamics for our four processes:
dz(t) =b(z(t), t) dt+ Σ(z(t), t) dw(t),z(0)∼P (170)
dez(t) = [−b(ez(t), T−t) +∇x·Ψ(ez(t), T−t) + Ψ(ez(t), T−t)s(ez(t), T−t)] dt(171)
+ Σ(ez(t), T−t) dw←(t),ez(0)∼p(·, T) (172)
dz(t) =
b(z(t), t)−1
2∇x·Ψ(z(t), t)−1
2Ψ(z(t), t)s(z(t), t)
dt,z(0)∼P (173)
dez(t) =
−b(ez(t), T−t) +1
2∇x·Ψ(ez(t), T−t) (174)
+1
2Ψ(ez(t), T−t)s(ez(t), T−t)
dt,ez(0)∼p(·, T). (175)
In practice, one fits an estimator for s(·,·) and estimates p(·, T) and runs a discretization
of either (159) or (166) to sample approximately from P. One common instantiation used in
diffusion models (Karras et al., 2022) is the so-called variance-exploding diffusion process,
which has the coefficient settings
b(x, t) = 0 , Σ(x, t) =√
2I (176)
which implies that
Ψ(x, t) = 2I. (177)
This means that the four specified processes are of the form
dz(t) =√
2 dw(t),z(0)∼P (178)
dez(t) = 2 s(ez(t), T−t) dt+√
2 dw←(t),ez(0)∼p(·, T) (179)
dz(t) =−s(z(t), t) dt,z(0)∼P (180)
dez(t) =s(ez(t), T−t),ez(0)∼p(·, T). (181)
Notice that the determinstic flows are actually gradient flows on the score, which concretely
reveals a connection between sampling and optimization, and thus between diffusion models
(precisely those which use the probability flow ODE to sample) and unrolled optimization
networks.
In this context, we can also establish the connection between diffusion networks and
iterative denoising. In the variance-exploding setting, we have
z(t)∼ N(z(0),2tI), (182)
67

--- PAGE 68 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
which can be easily computed using results from, e.g., S¨ arkk¨ a and Solin (2019). Thus z(t) is
a noisy version of z(0), with noise level increasing monotonically with t, and sampling z(0)
fromz(t) conceptually removes this noise. Concretely, Tweedie’s formula (Efron, 2011b)
says that the optimal denoising function E[z(0)|z(t)] has a simple form in terms of the
score function:
E[z(0)|z(t)] =z(t) + 2t·s(z(t), t). (183)
In other words, the score function spoints from the current iterate z(t) to the value of the
optimal denoising function, so it is a negative multiple of the conditionally-expected noise.
Following the score by (stochastic) gradient flow or its discretization is thus equivalent to
iterative denoising.
B.2 Companion to Section 3.1
Here we produce the approximation result alluded to in Section 3.1. To simplify the proofs,
we use the following notation correspondences: x7→zℓ,z7→zℓ
♮, and σ7→σℓ.
Approximation 12. Suppose that zfollows the low-dimensional Gaussian mixture model
(10) with respect to subspaces U[K], so that x=z+σw, where wis a standard Gaussian
vector independent of z, follows (11). That is, for some random index s∼πwhere πis a
distribution on [K]andsis chosen independently of all other randomness, we have
z=Usα, (184)
where αis a zero-mean Gaussian vector with diagonal covariance Λ. For each k∈[K],
define
Σi.= Cov[ z|s=k] =UkΛU∗
k. (185)
Assume (for the sake of normalization) that
πip
det(Σi+σ2I)=πjp
det(Σj+σ2I), for all 1≤i≤j≤K. (186)
Then we have
E[z|x]≈
U1, . . . ,UK
diag
softmax
1
2σ2
∥U∗
1x∥2
2...
∥U∗
Kx∥2
2


⊗Ip

U∗
1x
...
U∗
Kx
,(187)
where ⊗denotes the Kronecker product, i.e., the block matrix defined by
A⊗B=
A11B··· A1nB
.........
Am1B···AmnB
. (188)
Proof. Define Mk.= (Σk+σ2I)−1/2. From Proposition 14, we have
∇xlogq(x) =−KX
k=1e∗
ksoftmax
−1
2
∥M∗
1x∥2
2...
∥M∗
Kx∥2
2

MkM∗
kx (189)
68

--- PAGE 69 ---
White-Box Transformers via Sparse Rate Reduction
=−KX
k=1e∗
ksoftmax
−1
2σ2
∥σM∗
1x∥2
2...
∥σM∗
Kx∥2
2

MkM∗
kx (190)
=−KX
k=1e∗
ksoftmax
1
2σ2
∥x∥2
2− ∥σM∗
1x∥2
2...
∥x∥2
2− ∥σM∗
Kx∥2
2

MkM∗
kx. (191)
Now define Pk.=Id−σMk, and let U⊥
k∈Rd×(d−p)be an orthogonal complement of Uk.
Then we have
Pk=Id−σMk (192)
=Id−σ 
Σk+σ2Id−1/2(193)
=Id−σ
UkU⊥
kΛ 0
0 0U∗
k
(U⊥
k)∗
+σ2Id−1/2
(194)
=Id−σ
UkU⊥
kΛ+σ2Ip 0
0 σ2Id−pU∗
k
(U⊥
k)∗−1/2
(195)
=Id−
UkU⊥
kσ(Λ+σ2Ip)−1/20
0 σ·(σ2)−1/2Id−pU∗
k
(U⊥
k)∗
(196)
=Id−
UkU⊥
k
(σ−2Λ+Ip)−1/20
0 Id−pU∗
k
(U⊥
k)∗
(197)
=
UkU⊥
k
Ip−(σ−2Λ+Ip)−1/20
0 0U∗
k
(U⊥
k)∗
(198)
≈
UkU⊥
kIp0
0 0U∗
k
(U⊥
k)∗
(199)
=UkU∗
k. (200)
Thus Pkis approximately a projection when σis small. Under this algebraic relation, we
have
∇xlogq(x) (201)
=−KX
k=1e∗
ksoftmax
1
2σ2
∥x∥2
2− ∥σM∗
1x∥2
2...
∥x∥2
2− ∥σM∗
Kx∥2
2

MkM∗
kx (202)
=−1
σ2KX
k=1e∗
ksoftmax
1
2σ2
∥x∥2
2− ∥(Id−P1)∗x∥2
2...
∥x∥2
2− ∥(Id−PK)∗x∥2
2

(Id−Pk)(Id−Pk)∗x (203)
≈ −1
σ2KX
k=1e∗
ksoftmax
1
2σ2
∥P∗
1x∥2
2...
∥P∗
Kx∥2
2

(Id−Pk)(Id−Pk)∗x (204)
69

--- PAGE 70 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
≈ −1
σ2KX
k=1e∗
ksoftmax
1
2σ2
∥P∗
1x∥2
2...
∥P∗
Kx∥2
2

(Id−Pk)∗x (205)
=−x
σ2KX
k=1e∗
ksoftmax
1
2σ2
∥P∗
1x∥2
2...
∥P∗
Kx∥2
2

+1
σ2KX
k=1e∗
ksoftmax
1
2σ2
∥P∗
1x∥2
2...
∥P∗
Kx∥2
2

P∗
kx
(206)
=−1
σ2x+1
σ2KX
k=1e∗
ksoftmax
1
2σ2
∥P∗
1x∥2
2...
∥P∗
Kx∥2
2

P∗
kx (207)
≈ −1
σ2x+1
σ2KX
k=1e∗
ksoftmax
1
2σ2
∥U∗
1x∥2
2...
∥U∗
Kx∥2
2

UkU∗
kx (208)
=−1
σ2x+1
σ2
U1,···,UK
diag
softmax
1
2σ2
∥U∗
1x∥2
2...
∥U∗
Kx∥2
2


⊗Ip

U∗
1x
...
U∗
Kx
.(209)
Plugging this into Tweedie’s formula, we have
E[z|x]≈
U1,···,UK
diag
softmax
1
2σ2
∥U∗
1x∥2
2...
∥U∗
Kx∥2
2


⊗Ip

U∗
1x
...
U∗
Kx
.(210)
Remark 13.Although Approximation 12 is stated as an approximation rather than as a
proposition, we believe it should be possible without too much extra work to convert it
into a statement of asymptotic equivalence as σ→0 (in particular, holding for σbelow the
smallest (nonzero) eigenvalue of Λ). Most approximations taken in the derivation of Ap-
proximation 12 can immediately be turned into asymptotic claims; the only slightly delicate
point is treating the softmax, which can be accomplished using standard “high tempera-
ture” convergence behavior of the softmax function (Gao and Pavel, 2017) (in particular,
asσ→0 in our expressions, the softmax concentrates on the “best head”).
B.2.1 Auxiliary Lemmas
Proposition 14. Suppose that zfollows the low-dimensional Gaussian mixture model (10)
with respect to subspaces U[K], so that x=z+σw, where wis a standard Gaussian
vector independent of z, follows (11). That is, for some random index s∼πwhere πis a
distribution on [K]andsis chosen independently of all other randomness, we have
z=Usα, (211)
where αis a zero-mean Gaussian vector with diagonal covariance Λ. For each k∈[K],
define
Σi.= Cov[ z|s=k] =UkΛU∗
k,Mk.= (Σk+σ2I)−1/2. (212)
70

--- PAGE 71 ---
White-Box Transformers via Sparse Rate Reduction
Assume (for the sake of normalization) that
πidet(Mi) =πjdet(Mj), for all 1≤i≤j≤K. (213)
Then, letting x7→q(x)be the density of x, we have
∇xlogq(x) (214)
=−
M1,···,MK
diag
softmax
−1
2
∥M∗
1x∥2
2...
∥M∗
Kx∥2
2


⊗Id

M∗
1x
...
M∗
Kx
. (215)
Proof. By the law of total probability, we have
∇xlogq(x) =∇xlogKX
k=1q(x|k)πk (216)
=PK
k=1πk∇xq(x|k)PK
k=1q(x|k)πk(217)
where q(x|k) is the conditional density of xgiven the event {s=k}. To compute this
quantity, note that conditional on the value of s, we have
x=zs+σw∼ N(0,Σs+σ2Id). (218)
Thus we have
q(x|k) =1p
(2π)ddet(Σk+σ2Id)exp
−1
2x∗(Σk+σ2Id)−1x
, (219)
This gives
∇xq(x|k) =−q(x|k)·(Σk+σ2Id)−1x. (220)
Putting this all together, we get
∇xlogq(x) (221)
=−PK
k=1q(x|k)πk·(Σk+σ2Id)−1xPK
k=1q(x|k)πk(222)
=−PK
k=1πkdet(Σk+σ2Id)−1/2exp 
−1
2x∗(Σk+σ2Id)−1x
·(Σk+σ2Id)−1x
PK
k=1πkdet(Σk+σ2Id)−1/2exp 
−1
2x∗(Σk+σ2Id)−1x (223)
=−PK
k=1πkdet(Mk) exp 
−1
2x∗MkM∗
kx
·MkM∗
kx
PK
k=1πkdet(Mk) exp 
−1
2x∗MkM∗
kx (224)
=−PK
k=1πkdet(Mk) exp 
−1
2∥M∗
kx∥2
2
·MkM∗
kx
PK
k=1πkdet(Mk) exp 
−1
2x∗MkM∗
kx . (225)
Given our assumption that each πkdet(Mk) is the same, we have
∇xlogq(x) (226)
71

--- PAGE 72 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
=−PK
k=1πkdet(Mk) exp 
−1
2∥M∗
kx∥2
2
·MkM∗
kx
PK
k=1πkdet(Mk) exp 
−1
2∥M∗
kx∥2
2 (227)
=−PK
k=1exp 
−1
2∥M∗
kx∥2
2
·MkM∗
kx
PK
k=1exp 
−1
2∥M∗
kx∥2
2 (228)
=−KX
k=1e∗
ksoftmax
−1
2
∥M∗
1x∥2
2...
∥M∗
Kx∥2
2

MkM∗
kx (229)
=−
M1, . . . ,MK
diag
softmax
−1
2
∥M∗
1x∥2
2...
∥M∗
Kx∥2
2


⊗Id

M∗
1x
...
M∗
Kx
. (230)
B.3 Companion to Section 3.2
In this section, we prove a formal version of the result Theorem 6 stated in Section 3.2. That
is, we examine a basic yet representative instantiation of the signal model (11), and show
that under this model, in a natural regime of parameter scales motivated by the architecture
ofcrate applied to standard image classification benchmarks, the operation implemented
by taking a gradient step on the compression term of the sparse rate reduction objective
(15) corresponds to a projection operation at quantization scales ε2proportional to the size
of the deviation. This leads us in particular to a formal version of the result Theorem 6.
Signal model. We consider an instantiation of the model (11), elaborated here. That is,
we fix a distribution over tokens Z∈Rd×ninduced by the following natural signal model:
each token ziis drawn independently from the normalized isotropic Gaussian measure on
one of K p-dimensional subspaces with orthonormal bases U1, . . . ,UK∈Rd×p,22which
comprise the low-dimensional structure in the observed tokens, then corrupted with i.i.d.
Gaussian noise N(0,σ2
dI); the subspace each token is drawn from is selected uniformly at
random, independently of all other randomness in the problem. This signal model there-
fore corresponds to the setting of uncorrelated tokens, with maximum entropy coordinate
distributions within subspaces. It is natural to first develop our theoretical understanding
of the connection between compression and the score function in the uncorrelated setting,
although in general, the ability of crate to capture correlations in the data through the
MSSA block is essential. In connection with the latter issue, we note that our proofs will
generalize straightforwardly to the setting of “well-dispersed” correlated tokens: see the
discussion in Remark 18.
We make the further following assumptions within this model:
1. Inspired by an ablation in Figure 17, which suggests that the learned crate model on
supervised classification on Imagenet has signal models Ukwhich are near-incoherent,
we will assume that the subspaces Ukhave pairwise orthogonal column spaces. Our
22. More precisely, ziis distributed according to the pushforward of the normalized isotropic Gaussian
measure N(0,1
pI) onRpby the bases Uk.
72

--- PAGE 73 ---
White-Box Transformers via Sparse Rate Reduction
proofs will generalize straightforwardly to the setting where the subspaces are merely
incoherent: see the discussion in Remark 18.
2. We assume that the relative scales of these parameters conform to the crate -Base
settings, trained on Imagenet: from Table 9, these parameters are
(a)d= 768;
(b)n= 196;
(c)K= 12;
(d)p=d/K = 64.
In particular, d≫n≫pandKp=d.
These precise parameter values will not play a role in our analysis. We merely require the
following quantitative relationships between the parameter values, which are more general
than the above precise settings.
Assumption 15. We have ε≤1,U⊤
kUk′=1k=k′Ifor all k̸=k′, and the following
parameter settings and scales:
•d≥n≥p≥K≥2;
•Kp=d;
•C1√nlogn≤1
2n/K, where C1is the same as the universal constant C1in the state-
ment of Proposition 25;
•6C2
2n≤d, where C2is the same as the universal constant C3in the statement of
Proposition 28;
•2C2
4n≤d, where C4is the same as the universal constant C1in Proposition 24;
note: there is no self-reference, as the third inequality is not used to prove Proposi-
tion 25, the fourth is not used to prove Proposition 28, and the fifth is not used to prove
Proposition 24.
The first and second inequalities together imply in particular that p≥n/K. The third
inequality implies that C1√nlogn < n/K . The first, second, and and third inequalities
together imply that p > C 1√nlogn, and that 0 < n/K −C1√nlogn < n/K < n/K +
C1√nlogn < n .
These inequalities are verifiable in practice if one wishes to explicitly compute the ab-
solute constants C1, C2, C3, C4, and indeed they hold for our crate -Base model.
Formally, let µ(K, p, σ2) denote the probability measure on Rd×ncorresponding to the
noisy Gaussian mixture distribution specified above. We let Zo∼µdenote an observation
distributed according to this signal model: formally, there exists a (random) map i7→si,
fori∈[n] and si∈[K], such that
zoi=Usiαi+δi, i= 1, . . . , n, (231)
73

--- PAGE 74 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
where ∆=
δ1. . .δn
∼i.i.d.N(0,σ2
dI), and (independently) αi∼i.i.d.N(0,1
pI). It is
convenient to write this observation model in block form. To this end, let Kk=Pn
i=11si=k
fork∈[K] denote the number of times the k-th subspace is represented amongst the
columns of Zo(a random variable). Then by rotational invariance of the Gaussian distri-
bution, we have
Zod=
U1A1. . .UKAK
Π+∆, (232)
whered= denotes equality in distribution, Π∈Rn×nis a uniformly random permutation
matrix, and each Ak∈Rp×Kk. We also define X♮to be the noise-free version of Zo.
Because of this equality in distribution, we will commit the mild abuse of notation of
identifying the block representation (232) with the observation model (231) that follows the
distribution µ.
Denoising in the uncorrelated tokens model. In the uncorrelated tokens model (232),
the marginal distribution of each column of Zois identical, and equal to an equiproportional
mixture of (normalized) isotropic Gaussians on the subspaces U1, . . .Uk, convolved with
the noise distribution N(0,σ2
dI). This marginal distribution was studied in Appendix B.2,
where it was shown that when the perturbation level σ2→0, the score function for this
marginal distribution approximately implements a projection operation onto the nearest
subspace Uk.
Hence, we can connect compression, as implemented in the MSSA block of the crate
architecture, to denoising in the uncorrelated tokens model by showing that at similar
local scales, and for suitable settings of the model parameters, the compression operation
implements a projection onto the low-dimensional structure of the distribution, as well.
Compression operation. The MSSA block of the crate architecture arises from taking
an (approximate) gradient step on the Rcterm of the sparse rate reduction objective (15).
This term writes
Rc(Z|U[K]) =1
2KX
k=1log det ( I+β(U∗
kZ)∗U∗
kZ), (233)
where
β=p
nε2, (234)
andε >0 is the quantization error. Calculating the gradient, we have
∇ZRc(Z|U[K]) =KX
k=1UkU∗
kZ 
β−1I+ (U∗
kZ)∗U∗
kZ−1. (235)
Minimizing the sparse rate reduction objective corresponds to taking a gradient descent
step on Rc(· |U[K]). Performing this operation at the observation from the uncorrelated
tokens model Zo, the output can be written as
Z+=Zo−η∇Rc(Zo|U[K]), (236)
where η >0 is the step size.
74

--- PAGE 75 ---
White-Box Transformers via Sparse Rate Reduction
Main result on projection. We will see shortly that the behavior of the compression
output (236) depends on the relative scales of the perturbation about the low-dimensional
structure σ2and the target quantization error ε2.
Theorem 16. There are universal constants C1, C2, C3, C4>0such that the following
holds. Suppose Assumption 15 holds, and moreover suppose that σ≤1andC1βσ≤1
2.
Then with probability at least 1−KC 2 
e−C3d+e−C4n/K+n−2
, it holds
Z+−
∆−ηPU[K](β∆Π∗)Π
+1 +β−1−η
1 +β−1X♮(237)
≤C5Kη
σ2β2+σ(1 +p
n/d) +√
Kβσ2(1 +p
n/d) +p
n/d
. (238)
Here, PU[K]implements a projection onto the relevant subspaces for each token in the lim-
iting case as ε→0, and is precisely defined in (313) and(314) .
We give the proof of Theorem 16 below. First, we make three remarks on interpreting
the result, our technical assumptions, and our analysis.
Remark 17.Theorem 16 admits the following interesting interpretation in an asymptotic
setting, where we can identify the leading-order behavior of the gradient and confirm our
hypothesis about the connection between compression and score-following. Choose η=β−1,
so that the guarantee in Theorem 16 incurs some cancellation, and moreover delineate more
precise dependencies on the RHS of the guarantee:
Z+−
∆− PU[K](∆Π∗)Π
+1
1 +β−1X♮(239)
≲nK2ε2
dσ2d2
n2K2ε4+σ(1 +p
n/d) +dσ2
n√
Kε2(1 +p
n/d) +p
n/d
(240)
≲K3/2σ2+σ2d
nε2+nK2
d
σ+rn
d
ε2, (241)
where we used Assumption 15, which implies p=d/K andn/d≤1. We will check in
due course whether we have satisfied the hypotheses of Theorem 16, so that this guarantee
indeed applies. To this end, we optimize this bound as a function of ε >0, since this is a
parameter of the compression model. The optimal εis straightforward to compute using
calculus: it satisfies
ε2=s
σ2d
nK2n
d
σ+rn
d
(242)
=σd
nKq
σ+pn
d, (243)
and the value of the residual arising from Theorem 16 with this choice of εis no larger than
an absolute constant multiple of
K3/2σ2+s
K2σ2d
nnσ
d+n
d3/2
=Kσ
√
Kσ+s
σ+rn
d
. (244)
75

--- PAGE 76 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Moreover, with this choice of ε,βsatisfies
β−1=ε2nK
d=s
σ
1 +pn
dσ2. (245)
In particular, the condition βσ≲1 in Theorem 16 demands
s
σ+rn
d≲1, (246)
which holds for sufficiently small σand sufficiently large d≥n, showing that Theorem 16 can
be nontrivially applied in this setting. If we consider a simplifying limiting regime where
n, d→+∞such that n/d→0 and n/K→+∞, we observe the following asymptotic
behavior of the guarantee of Theorem 16:
Z+−
∆− PU[K](∆Π∗)Π
+1
1 +√σX♮≲Kσ3/2
1 +√
Kσ
. (247)
This demonstrates that a gradient step on Rcperforms denoising: there is a noise-level-
dependent shrinkage effect applied to the signal X♮, which vanishes as σ→0, and meanwhile
the noise term ∆is reduced.
Moreover, as σ→0, we can express the limiting form of PU[K]exactly as an orthogonal
projection, since this drives β−1→0: following (313) and (314), we have here
PU[K]=
P1. . .PK
, (248)
where
Pk→X
k′̸=kUk′projim(Ak′)⊥U∗
k′. (249)
This shows that, in an asymptotic sense, a gradient step on Rcserves to suppress the effect of
the perturbation applied to the observations Zoabout the local signal model X♮.This verifies
our claim previously that in this setting, there is a correspondence between a score-following
algorithm and a compression-based approach: locally, both project the observations onto
the structures of the signal model.
It can be shown moreover that the shrinkage effect on X♮demonstrated here appears
as a consequence of using the Rc“compression” term for the gradient step in crate ; when
the gradient step is taken instead on the full ∆ Rrate reduction objective (which is compu-
tationally prohibitive, of course), there is zero shrinkage, and perfect denoising is performed
for a wider variety of step sizes ηthan the choice made here. We see the introduction of
this shrinkage effect this as the price of constructing an efficient and interpretable network
architecture. In practice, the ISTA block of crate counteracts this shrinkage effect, which
is anyways minor at reasonable parameter scales.
Remark 18.We have made two assumptions which may not hold exactly in practice: namely,
we have assumed that the Uk’s have orthogonal columns, namely U⊤
kUk′=1k=k′I, and
we have assumed that the linear combination coefficients Akthat form the matrix X♮
are i.i.d. samples from Gaussian distributions. Both these assumptions can be made more
76

--- PAGE 77 ---
White-Box Transformers via Sparse Rate Reduction
realistic, at the cost of additional (non-instructive) complexity in the analysis; we briefly go
over how.
Relaxing the orthogonality condition U⊤
kUk′=1k=k′Ito near-orthogonality, namely
∥U⊤
kUk′−1k=k′I∥ ≤ νfor a small ν, as observed in practice (recall Figure 17) would
introduce additional small error terms in the proof, say polynomial in ν. The magnitudes
of these errors could in principle be precisely tracked, whence one could obtain a similar
result to Theorem 16.
Secondly, we have assumed that the Ak’s have independent columns which are sampled
from (the same) Gaussian distribution. However, in the conceptual framework for crate , we
exploit the joint distribution (and in particular the correlations) between the tokens in order
to obtain good performance for our model. Our analysis is not completely agnostic to this
fact; as we will see, the proof of Theorem 16 only leverages the independence of the columns
of each Ak’s in order to obtain high-probability upper bounds on the smallest and largest
singular value of the token matrices. If these bounds were ensured by some other method,
such as appropriate normalization and incoherence, a similar conclusion to Theorem 16 could
hold in the more realistic correlated tokens model. Going beyond well-conditioned token
matrices for each subspace would require additional modeling assumptions, and additional
investigative experimental work to determine a realistic basis for such assumptions.
Remark 19.We have not attempted to optimize constants or rates of concentration in the
proof of Theorem 16, preferring instead to pursue a straightforward analysis that leads to
a qualitative interpretation of the behavior of the rate reduction gradient in our model
problem. Minor improvements to the concentration analysis would enable the parameter
scaling requirements in Assumption 15 to be relaxed slightly, and the probability bound in
Theorem 16 that scales as K/n2can easily be improved to any positive power of 1 /n.
Proof of Theorem 16. We start by noticing that, by orthonormality of the subspaces Uk,
we have by (232)
U∗
kZo=
0. . .Ak. . .0
Π+U∗
k∆, (250)
so that
 
β−1I+ (U∗
kZo)∗U∗
kZo−1=Π∗

β−1I
...
β−1I+A∗
kAk
...
β−1I

| {z }
Dk+Ξk
−1
Π,(251)
because permutation matrices are orthogonal matrices, and where the perturbation Ξkis
defined by
Ξk=Π∆∗UkU∗
k∆Π∗+
0 . . . ∆∗
1UkAk . . . 0
.........
A∗
kU∗
k∆1. . .∆∗
kUkAk+A∗
kU∗
k∆k. . .A∗
kU∗
k∆K
.........
0 . . . ∆∗
KUkAk . . . 0
,(252)
77

--- PAGE 78 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
and where we have defined (implicitly) in addition

∆1. . .∆K
=∆Π∗. (253)
The matrix Dk≻0, so we can write
 
β−1I+ (U∗
kZo)∗U∗
kZo−1=Π∗D−1
k 
I+ΞkD−1
k−1Π, (254)
from which it follows
U∗
kZo 
β−1I+ (U∗
kZo)∗U∗
kZo−1(255)
= 
0. . .Ak(β−1I+A∗
kAk)−1. . .0
+U∗
k∆Π∗D−1
k 
I+ΞkD−1
k−1Π.(256)
The task before us is therefore to control ∥ΞkD−1
k∥<1, in order to apply the Neumann
series to further simplify this expression. We will do this in stages: first, we invoke several
auxiliary lemmas to construct a high-probability event on which the random quantities in
the preceding expression are controlled about their nominal values; next, we show that the
Neumann series can be applied on this event and a main term extracted; finally, we simplify
this main term further in order to establish the claimed expression.
High-probability event construction. In order to achieve the appropriate control on
all random quantities, we would like to construct a high-probability event on which the
random quantities are not too large. By Propositions 22 to 24 and union bound, there exist
universal constants Ci>0 for which
P
∥∆∥ ≤σ(C1+p
n/d)
∀k∈[K]:∥Ak∥ ≤1 +C2p
n/d
∀k∈[K]:∥A⊤
kAk−I∥ ≤C3p
n/d
≥1−C4K(e−C5d+e−C6n/K+n−2).(257)
The event we compute the probability of, which we denote E⋆, is precisely the good event
that we want. Formally,
E⋆.=

∥∆∥ ≤σ(C1+p
n/d)
∀k∈[K]:∥Ak∥ ≤1 +C2p
n/d
∀k∈[K]:∥A⊤
kAk−I∥ ≤C3p
n/d

. (258)
We know that E⋆occurs with high probability, and are able to strongly control the random
quantities to the degree desired.
Main term extraction. By Lemma 21 and our hypotheses on the problem parameters,
we have on E⋆that
∥ΞkD−1
k∥ ≤Cβσ < 1. (259)
We can therefore apply the Neumann series to obtain
U∗
kZo 
β−1I+ (U∗
kZo)∗U∗
kZo−1(260)
= 
0. . .Ak(β−1I+A∗
kAk)−1. . .0
+U∗
k∆Π∗D−1
k
I−ΞkD−1
k+P∞
j=2(−1)j 
ΞkD−1
kj
Π.(261)
78

--- PAGE 79 ---
White-Box Transformers via Sparse Rate Reduction
Again on E⋆, we have
∞X
j=2(−1)j 
ΞkD−1
kj≤∞X
j=2ΞkD−1
kj≤C(βσ)2 1
1−Cβσ≤C′(βσ)2. (262)
Moreover, as in the proof of Lemma 21, we have on the previous event that
U∗
k∆Π∗D−1
k≤Cβσ. (263)
Thus, if we define a “main term”
Mk=
0. . .Ak(β−1I+A∗
kAk)−1. . .0 
I−ΞkD−1
k
+U∗
k∆Π∗D−1
k
Π,(264)
we have on the same event as previously
U∗
kZo 
β−1I+ (U∗
kZo)∗U∗
kZo−1−Mk≤C(βσ)2. (265)
To conclude, we need only study this main term, since Ukhas operator norm 1.
Simplifying the main term. Our approach will be to control the main term Mkaround
a simpler expression, using basic perturbation theory; by the triangle inequality for the
operator norm, this will give control of the desired gradient term. After distributing, Mk
is a sum of three terms; we will start with the simplest term. We first compute
U∗
k∆Π∗D−1
k=U∗
kh
β∆1. . .∆k 
β−1I+A∗
kAk−1. . . β ∆Ki
. (266)
We are going to argue that the residual
U∗
k∆Π∗D−1
k−U∗
k
β∆1. . .0. . . β ∆K (267)
is small. To this end, note that by the fact that Ukhas unit operator norm,
U∗
k∆Π∗D−1
k−U∗
k
β∆1. . .0. . . β ∆K (268)
≤h
0. . .∆k 
β−1I+A∗
kAk−1. . .0i (269)
=∆k 
β−1I+A∗
kAk−1 (270)
≤ ∥∆k∥ 
β−1I+A∗
kAk−1. (271)
By (258) and (317) from Lemma 20, the second term here is controlled on E⋆. For the first
term, we note that by definition and the fact that the unit sphere is invariant to rotations
(and permutations are rotations),
∥∆∥= sup
∥u∥2≤1∥∆u∥2= sup
∥u∥2≤1
∆1. . .∆K
u
2(272)
= sup
∥u∥2≤1KX
i=1∆iui
2, (273)
79

--- PAGE 80 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
where uiare coordinate-subset-induced partitions of the vector uinduced by those of ∆Π∗.
This yields immediately
∥∆∥ ≤ sup
∥u∥2≤1KX
i=1∥∆iui∥2≤
max
k∈[K]∥∆k∥
sup
∥u∥2≤1KX
i=1∥ui∥2≤√
K
max
k∈[K]∥∆k∥
,(274)
by the triangle inequality and inequalities for ℓpnorms. Similarly, choosing a specific uin
the operator norm expression, namely one that is supported entirely on one of the coordinate
partitions ui, shows that
∥∆∥ ≥ ∥∆iui∥2 (275)
for any i, whence
max
k∈[K]∥∆k∥ ≤ ∥∆∥. (276)
It follows that we control the first term above on E⋆. Combining this reasoning, we conclude
from the above
U∗
k∆Π∗D−1
k−U∗
k
β∆1. . .0. . . β ∆K (277)
≤σ(C+p
n/d) 
1
1 +β−1+C′p
n/d
1 +β−1!
(278)
≲σ(1 +Cp
n/d), (279)
where the second line uses Assumption 15 to remove the higher-order residual.
next, we recall that Ξkis a sum of two terms; we will do one term at a time for concision.
We have first

0. . .Ak(β−1I+A∗
kAk)−1. . .0
Π∆∗UkU∗
k∆Π∗(280)
=
0. . .Ak(β−1I+A∗
kAk)−1. . .0
∆∗
1...
∆∗
K
UkU∗
k
∆∗
1...
∆∗
K
∗
(281)
=Ak(β−1I+A∗
kAk)−1∆∗
kUkU∗
k
∆1. . .∆K
. (282)
We then multiply this term by D−1
kon the right to get the term that appears in Mk
(ignoring the multiplication on the right by Π, because it does not change operator norms).
In particular, we will control
Ak(β−1I+A∗
kAk)−1∆∗
kUkU∗
k
∆1. . .∆K
D−1
k, (283)
showing that this term is small. We will accomplish this with the block diagonal structure
ofDk: indeed, this gives that D−1
kis obtained by blockwise inversion, and hence
Ak(β−1I+A∗
kAk)−1∆∗
kUkU∗
k
∆1. . .∆K
D−1
k (284)
=Ak(β−1I+A∗
kAk)−1∆∗
kUkU∗
kh
β∆1. . .∆k 
β−1I+A∗
kAk−1. . . β ∆Ki(285)
80

--- PAGE 81 ---
White-Box Transformers via Sparse Rate Reduction
≤√
KmaxnAk(β−1I+A∗
kAk)−1∆∗
kUkU∗
k∆k(β−1I+A∗
kAk)−1, (286)
max
k′̸=kβAk(β−1I+A∗
kAk)−1∆∗
kUkU∗
k∆k′o
, (287)
where the second line uses (274). We will give a coarse control of this term—the error could
be improved further by exploiting more thoroughly independence of the blocks ∆kto show
that the maximum over k′̸=kin the last line of the preceding expression is smaller. We
have by submultiplicativity of the operator norm and the triangle inequality
Ak(β−1I+A∗
kAk)−1∆∗
kUkU∗
k∆k(β−1I+A∗
kAk)−1 (288)
≤ 
1
1 +β−1+Cp
n/d
1 +β−1!2
∥∆∗
kUkU∗
k∆k∥ (289)
≤
1 +Cp
n/d
∥∆∗
kUkU∗
k∆k∥, (290)
where the first line uses Lemma 20, and the second line uses Assumption 15 to simplify the
residual as above. We have meanwhile from the definition of E⋆
∥∆∗
kUkU∗
k∆k∥ ≤ ∥∆k∥2≲σ2
1 +p
n/d
, (291)
because UkU∗
kis an orthogonal projection, and again using Assumption 15 to simplify the
residual. We can argue analogously to simplify the other term in the maximum appearing
above, and this yields
Ak(β−1I+A∗
kAk)−1∆∗
kUkU∗
k
∆1. . .∆K
D−1
k (292)
≲√
Kβσ2
1 +Cp
n/d
1 +p
n/d
, (293)
where we used the fact that ε≤1 and the rest of Assumption 15 implies that β≥1. This
residual simplifies using Assumption 15 to
Ak(β−1I+A∗
kAk)−1∆∗
kUkU∗
k
∆1. . .∆K
D−1
k≲√
Kβσ2
1 +Cp
n/d
.(294)
next, we examine the last term, which is the other summand arising in the definition of
Ξk. We have

0. . .Ak(β−1I+A∗
kAk)−1. . .0
0 . . . ∆∗
1UkAk . . . 0
.........
A∗
kU∗
k∆1. . .∆∗
kUkAk+A∗
kU∗
k∆k. . .A∗
kU∗
k∆K
.........
0 . . . ∆∗
KUkAk . . . 0
(295)
=Ak(β−1I+A∗
kAk)−1
A∗
kU∗
k∆1. . .(∆∗
kUkAk+A∗
kU∗
k∆k). . .A∗
kU∗
k∆K. (296)
Now multiplying on the right by D−1
kgives the term (again ignoring the right-multiplication
byΠ, which does not affect the operator norm)
Ak(β−1I+A∗
kAk)−1h
βA∗
kU∗
k∆1. . .(∆∗
kUkAk+A∗
kU∗
k∆k) 
β−1I+A∗
kAk−1. . . β A∗
kU∗
k∆Ki
.(297)
81

--- PAGE 82 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
We will argue that this term is close to the term
Ak(β−1I+A∗
kAk)−1
βA∗
kU∗
k∆1. . .0. . . β A∗
kU∗
k∆K
(298)
in operator norm. The argument is similar to the preceding arguments: for this, it suffices
to bound
h
0. . .Ak(β−1I+A∗
kAk)−1(∆∗
kUkAk+A∗
kU∗
k∆k) 
β−1I+A∗
kAk−1. . .0i,(299)
which is the same as controlling the operator norm of the nonzero block. Using submul-
tiplicativity and Lemma 20 along with the simplifications we have done above leveraging
Assumption 15, we obtain
Ak(β−1I+A∗
kAk)−1(∆∗
kUkAk+A∗
kU∗
k∆k) 
β−1I+A∗
kAk−1 (300)
≤
1 +Cp
n/d
∥∆∗
kUkAk+A∗
kU∗
k∆k∥. (301)
Meanwhile, on E⋆we have the operator norm of ∆kandAkcontrolled, using again (276).
Applying then the triangle inequality and submultiplicativity, we obtain
∥∆∗
kUkAk+A∗
kU∗
k∆k∥≲σ
1 +p
n/d
, (302)
again simplifying with Assumption 15. This shows that (297) is close to (298) with devia-
tions of the order ≲σ(1 +p
n/d).
Aggregating the previous results. Combining our perturbation analysis above, we
have established control
Mk−h 
I−Ak(β−1I+A∗
kAk)−1A∗
k
U∗
k
β∆1. . .0. . . β ∆K
(303)
+
0. . .Ak(β−1I+A∗
kAk)−1. . .0i
Π (304)
≲σ(1 +p
n/d) +√
Kβσ2(1 +p
n/d). (305)
It is convenient to include one additional stage of simplification here: namely, we use
Lemma 20 once more to simplify the second term in the nominal value of Mkappearing
here. namely, we have (arguing as we have above, once again)

0. . .Ak(β−1I+A∗
kAk)−1. . .0
−h
0. . .1
1+β−1Ak. . .0i (306)
=1
1 +β−1Ak−Ak 
β−1I+A∗
kAk−1(307)
≲p
n/d, (308)
from which it follows
Mk−h 
I−Ak(β−1I+A∗
kAk)−1A∗
k
U∗
k
β∆1. . .0. . . β ∆K
(309)
+h
0. . .1
1+β−1Ak. . .0ii
Π (310)
82

--- PAGE 83 ---
White-Box Transformers via Sparse Rate Reduction
≲σ(1 +p
n/d) +√
Kβσ2(1 +p
n/d) +p
n/d. (311)
Meanwhile, recall the residual of scale ≲(σβ)2arising when we controlled the gradient term
around Mk:U∗
kZo 
β−1I+ (U∗
kZo)∗U∗
kZo−1−Mk≤C(βσ)2. (312)
Combining these two bounds with the triangle inequality controls the gradient term around
its nominal value. Now, we sum these errors over k(again with the triangle inequality) to
obtain control of the aggregate gradient around its nominal value. We introduce notation
to concisely capture the accumulations of the (approximate) orthogonal projections arising
in the nominal value of the main term: for each k∈[K], define
Pk=X
k′̸=kUk′ 
I−Ak′(β−1I+A∗
k′Ak′)−1A∗
k′
U∗
k′, (313)
and define an overall (approximate) projection operator (which acts on block matrices
partitioned compatibly with the class sizes nk, as in (232)) by
PU[K]=
P1. . .PK
. (314)
Then the above argument implies
∇ZRc(Zo|U[K])− PU[K](β∆Π∗)Π−1
1 +β−1X♮(315)
≲K
σ2β2+σ(1 +p
n/d) +√
Kβσ2(1 +p
n/d) +p
n/d
, (316)
which is enough to conclude.
B.3.1 Key Auxiliary Lemmas
In this section we state and prove two key concentration inequalities that are used in the
proof of the main theorem. They rely on simpler results which will be conveyed in subse-
quent subsections.
Lemma 20. There exist universal constants C, C′>0such that the following holds. Let
d, p, n, K ∈Nbe such that Assumption 15 holds. Let Ak,k∈[K], be defined as above. Let
E⋆be the good event defined in (258) . IfE⋆occurs, then for k∈[K]we have
(β−1I+A⊤
kAk)−1−1
1 +β−1I≤Cp
n/d
(1 +β−1). (317)
and in addition
Ak(β−1I+A⊤
kAk)−1−1
1 +β−1Ak≤C′p
n/d
(1 +β−1). (318)
83

--- PAGE 84 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Proof. Since E⋆holds, for all k∈[K] we have
∥Ak∥ ≤1 +C1p
n/d, ∥A⊤
kAk−I∥ ≤C2p
n/d. (319)
By Assumption 15, we have ∥A⊤
kAk−I∥<1, soA⊤
kAkis well-conditioned. Write
Ξ.=A⊤
kAk−I, (320)
so that
(β−1I+A⊤
kAk)−1= ((1 + β−1)I+Ξ)−1(321)
=1
1 +β−1
I+1
1 +β−1Ξ−1
(322)
=1
1 +β−1∞X
j=0
−1
1 +β−1j
Ξj(323)
=1
1 +β−1I+1
1 +β−1∞X
j=1
−1
1 +β−1j
Ξj(324)
by the Neumann series. This gives us
(β−1I+A⊤
kAk)−1−1
1 +β−1I=1
1 +β−1∞X
j=1
−1
1 +β−1j
Ξj(325)
≤1
1 +β−1∞X
j=11
1 +β−1j
∥Ξ∥j(326)
≤1
1 +β−1∞X
j=1 
C2p
n/d
1 +β−1!j
(327)
=C2p
n/d
(1 +β−1)(1 + β−1−C2p
n/d). (328)
Meanwhile, by Assumption 15, it holds
C2p
n/d≤p
1/6, (329)
so it follows
C2p
n/d
1 +β−1−C2p
n/d≤2C2p
n/d. (330)
By the submultiplicativity of the operator norm, we thus have
Ak(β−1I+A⊤
kAk)−1−1
1 +β−1Ak≤ ∥Ak∥(β−1I+A⊤
kAk)−1−1
1 +β−1I(331)
≤[1 +C1p
n/d]C2p
n/d
(1 +β−1)(1 + β−1−C2p
n/d)(332)
84

--- PAGE 85 ---
White-Box Transformers via Sparse Rate Reduction
≤2[1 +C1p
n/d]C2p
n/d
1 +β−1(333)
= 2C2p
n/d+C1C2n/d
1 +β−1. (334)
By Assumption 15, we have that there exists some absolute constant C3>0 with C3·n/d≤p
n/d, which gives
Ak(β−1I+A⊤
kAk)−1−1
1 +β−1Ak≤2(C2+C1C2C−1
3)p
n/d
1 +β−1, (335)
as desired.
Lemma 21. There exist universal constants C1, C2>0such that the following holds. Let
d, p, n, K ∈Nbe such that Assumption 15 holds. Let Ak,k∈[K], be defined as above. Let
Dkbe defined as in (251) . Let Ξkbe defined as in (252) . Let E⋆be the good event defined
in(258) . IfE⋆occurs, then for k∈[K]we have
∥ΞkDk∥−1≤C1β[σ2+σ(C2+p
n/d)]. (336)
Proof. Since we have
Dk=
β−1I
...
β−1I+A⊤
kAk
...
β−1I
(337)
it holds that
D−1
k=
βI
...
(β−1I+A⊤
kAk)−1
...
βI
. (338)
We will use the straightforward estimate ∥ΞkD−1
k∥ ≤ ∥ Ξk∥∥D−1
k∥and bound the two
matrices’ operator norms individually. By the previous expression,
∥D−1
k∥= max {β,∥(β−1I+A⊤
kAk)−1∥} ≤ β, (339)
because A⊤
kAk⪰0, so we need only control the operator norm of Ξk. To this end, note
the convenient expression
Ξk=Π∆∗UkU∗
k∆Π∗+ 2 sym 
(∆Π∗)∗
0. . .UkAk. . .0
, (340)
where sym( ·) denotes the symmetric part operator. By the triangle inequality, the operator
norm of Ξkis no larger than the sum of the operator norms of each term in the previous
85

--- PAGE 86 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
expression. The operator norm of the first term is no larger than ∥∆∥2, because Πis
a permutation matrix and UU∗
kis an orthogonal projection. Meanwhile, using that the
symmetric part operator is the orthogonal projection onto the space of symmetric matrices,
it follows
2 sym 
(∆Π∗)∗
0. . .UkAk. . .0≤2(∆Π∗)∗
0. . .UkAk. . .0, (341)
and then we find as above that the RHS is no larger than 2 ∥∆∥∥Ak∥. Since the good event
E⋆defined in (258) holds by assumption, we have that there are constants C1, C2>0 such
that
∥∆∥ ≤σ
C1+rn
d
(342)
∥Ak∥ ≤1 +C2rn
d. (343)
By Assumption 15 we have d≥n, so thatp
n/d≤1. Therefore we have
∥∆∥ ≤σ(C1+ 1) = C3σ (344)
forC3.=C1+ 1 another universal constant. Thus on this good event we have
2∥∆∥∥Ak∥ ≤C3σ
1 +C2p
n/d
. (345)
Therefore, we have
∥Ξk∥ ≤ ∥∆∥2+ 2∥∆∥∥Ak∥ (346)
≤C2
3σ2+C3σ(1 +C2p
n/d) (347)
≤C4[σ2+σ(1 +C2p
n/d)] (348)
where C4= max {C3, C2
3}is another universal constant. Thus on E⋆we have
∥ΞkD−1
k∥ ≤C4β[σ2+σ(1 +C2p
n/d)]≤C5β[σ2+σ(1 +p
n/d)] (349)
forC5>0 another obvious universal constant.
B.3.2 Concentration Inequalities for Our Setting
In this section we prove some simple concentration inequalities that are adapted to the
problem setting. These results are used to prove the key lemmata above, and indeed are also
invoked in the main theorem. They follow from even simpler concentration inequalities that
are abstracted away from the problem setting, which we discuss in the following subsections.
Proposition 22. There are universal constants C1, C2, C3>0such that the following
holds. Let d, n∈Nbe such that Assumption 15 holds. Let ∆∈Rd×nbe defined as above.
Then
P
∥∆∥> σ
C1+rn
d
≤C2e−C3d. (350)
86

--- PAGE 87 ---
White-Box Transformers via Sparse Rate Reduction
Proof. We use Proposition 27 with the parameters q=d,n=n, and x=σ/√
d, which
obtains
P[∥∆∥> s]≤C1exp
−d(
s√
d/σ−√n
C2√
d−1)2
,∀s >σ√
d(√n+C2√
d) (351)
notice that we have
s√
d/σ−√n
C2√
d−1 =1
C2s
σ−rn
d
−1,σ√
d(√n+C2√
d) =σrn
d+C2
.(352)
To make the squared term equal to 1, we pick
s=σrn
d+ 2C2
, (353)
which gives
P
∥∆∥> σ
2C2+rn
d
≤C2e−d. (354)
Proposition 23. There are universal constants C1, C2, C3, C4>0such that the following
holds. Let p, n, K ∈Nbe such that Assumption 15 holds. Let Ak,k∈[K], be defined as
above. Then
P
∥Ak∥>1 +C1rn
d
≤C2exp
−C3n
K
+C4
n2(355)
Proof. By Propositions 25 and 27 with parameters n=n,k=K,q=p, and x= 1/√p, if
we define
nmin.=jn
K−C1p
nlognk
, n max.=ln
K+C1p
nlognm
(356)
then we have
P[∥Ak∥> s]≤nmaxX
n=nminP[∥Ak∥> s|Kk=n]P[Kk=n] +C2
n2(357)
≤nmaxX
n=nminC3exp 
−ns√p−√p
C4√
4−12!
P[Kk=n] +C2
n2, (358)
for all sobeying
s≥1√p(√p+C4√nmax) (359)
= 1 + C4rnmax
p. (360)
Thus we have that the concentration holds for all sobeying
s≥1 +C4rnmax
p. (361)
87

--- PAGE 88 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
In order to cancel out the most interior terms, we choose
s= 1 + 2 C4rnmax
p. (362)
This choice obtains
P[∥Ak∥> s]≤nmaxX
n=nminC3exp 
−ns√p−√p
C4√n−12!
P[Kk=n] +C2
n2(363)
=nmaxX
n=nminC3exp
−n

2rnmax
n|{z}
≥1−1

2
| {z }
≥1
P[Kk=n] +C2
n2(364)
≤nmaxX
n=nminC3exp(−n)P[Kk=n] +C2
n2(365)
≤nmaxX
n=nminC3exp(−nmin)P[Kk=n] +C2
n2(366)
≤C3exp(−nmin) +C2
n2(367)
=C3exp
−n
K+C1p
nlogn
+C2
n2(368)
≤C3exp
−n
K+1
2p
nlogn
+C2
n2(369)
≤C3exp
−1
2·n
K
+C2
n2. (370)
To obtain the conclusion of the theorem, note that any ssuch that
s≥1 + 2 C4rnmax
p= 1 + 2 C4s
n/K
p+C1√nlogn
p= 1 + 2 C4s
n
d+C1√nlogn
p(371)
enjoys the same high-probability bound. By Assumption 15, we have
1 + 2 C4s
n
d+C1√nlogn
p≤1 + 2 C4s
n
d+1
2·n/K
p(372)
= 1 + 2 C4r
n
d+1
2·n
d= 1 + 2 C4r
3
2·rn
d(373)
whence the ultimate conclusion is obtained by combining constants.
88

--- PAGE 89 ---
White-Box Transformers via Sparse Rate Reduction
Proposition 24. There are universal constants C1, C2, C3, C4>0such that the following
holds. Let p, n, K ∈Nbe such that Assumption 15 holds. Let Ak,k∈[K], be defined as
above. Then
P
∥A⊤
kAk−I∥> C 1rn
d
≤C2exp
−C3n
K
+C4
n2. (374)
Proof. By Propositions 25 and 28 with parameters n=n,k=K,q=p, and x= 1/√p, if
we define
nmin.=jn
K−C1p
nlognk
, n max.=ln
K+C1p
nlognm
(375)
then we have
P[∥A⊤
kAk−I∥> s] (376)
≤nmaxX
n=nminP[∥A⊤
kAk−I∥> s|Kk=n]P[Kk=n] +C2
n2(377)
≤C2
n2+nmaxX
n=nminP[Kk=n]·

C3exp 
−n
1
C2
4C5√
n/ps−12!
,ifC2
4C5p
n/p≤s≤C2
4
C3exp 
−n
1
C4C5√
n/p√s−12!
,ifs≥C2
4.(378)
In order to cancel the most terms, we choose
s= 2C2
4C5rnmax
p. (379)
In order to assure ourselves that this choice still has s≤C2
4(so that we can use the first
case for all n), we have
s= 2C2
4C5rnmax
p(380)
= 2C2
4C5s
n/K +C1√nlogn
p(381)
= 2C2
4C5s
n/K +1
2n/K
p(382)
= 2r
3
2C2
4C5·s
n/K
p(383)
=√
6C2
4C5·rn
d(384)
≤C2
4when√
6C5rn
d≤1. (385)
Of course, this condition is assured by Assumption 15. Now that we have this, we know s
falls in the first, and so we have
P
∥A⊤
kAk−I∥> C 1rn
d
(386)
89

--- PAGE 90 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
≤C2
n2+nmaxX
n=nminP[Kk=n]·

C3exp 
−n
1
C2
4C5√
n/ps−12!
,ifC2
4C5p
n/p≤s≤C2
4
C3exp 
−n
1
C4C5√
n/p√s−12!
,ifs≥C2
4(387)
≤C2
n2+nmaxX
n=nminC3exp
−n(
2C2
4C5p
nmax/p
C2
4C5p
n/p−1)2
P[Kk=n] (388)
=C2
n2+nmaxX
n=nminC3exp 
−n
2rnmax
n−12!
P[Kk=n] (389)
≤C3exp
−1
2·n
K
+C2
n2(390)
where the last inequality follows from the exact same argument as in Proposition 23.
B.3.3 Generic Concentration Inequalities
In this subsection we prove the base-level concentration inequalities used throughout the
proofs in this paper.
Binomial concentration.
Proposition 25. There exist universal constants C1, C2>0such that the following holds.
Letn, k∈Z. For each i∈[k], let Bi∼Bin(n,1/k), such that the Biare identically
(marginally) distributed but not necessarily independent binomial random variables. Let E
be an event. Then for any i∈[k], we have
P[E]≤⌈n/k+C1√nlogn⌉X
b=⌊n/k−C1√nlogn⌋P[E|Bi=b]P[Bi=b] +C2
n2. (391)
Proof. We have
P[E] =E[E[E|Bi]] =nX
b=0P[E|Bi=b]P[Bi=b]. (392)
Each Biis unconditionally distributed as Bin( n,1/k). By union bound and Hoeffding’s
inequality (Vershynin, 2018, Theorem 2.2.6), we have
P[|Bi−n/k| ≥t]≤2 exp
−2t2
n
. (393)
Inverting this inequality obtains that there exists some (simple) universal constants C1, C2>
0 such that
Ph
|Bi−n/k| ≥C1p
nlogni
≤C2
n3. (394)
Thus, if we define
bmin.=jn
k−C1p
nlognk
, b max.=ln
k+C1p
nlognm
, (395)
90

--- PAGE 91 ---
White-Box Transformers via Sparse Rate Reduction
then we have
P[E] =nX
b=0P[E|Bi=b]P[Bi=b] (396)
=bmin−1X
b=0P[E|Bi=b]|{z}
≤1P[Bi=b]|{z}
≤C2/n3+nX
b=bmax+1P[E|Bi=b1]|{z }
≤1P[Bi=b]|{z}
≤C2/n3(397)
+bmaxX
b=bminP[E|Bi=b]P[Bi=b] (398)
≤bmin−1X
b=0C2
n3+nX
b=bmax+1C2
n3+bmaxX
b=bminP[E|Bi=b]P[Bi=b] (399)
≤nX
b=0C2
n3+bmaxX
b=bminP[E|Bi=b]P[Bi=b] (400)
=C2
n2+C2
n3+bmaxX
b=bminP[E|Bi=b]P[Bi=b] (401)
≤2C2
n2+bmaxX
b=bminP[E|Bi=b]P[Bi=b]. (402)
Remark 26.Notice that a simple adaptation of this argument can turn the additive proba-
bility C3/n2intoC′
3/nzfor any positive integer z∈N(where C′
3depends on z). However,
trying to replace it with C′
3e−C′nis more difficult.
Gaussian concentration.
Proposition 27. There are universal constants C1, C2, C3>0such that the following
holds. Let n, q∈N, and let M∈Rq×nbe such that Mij∼i.i.d.N(0, x2). Then
P[∥M∥> s]≤C1exp 
−ns/x−√q
C2√n−12!
,∀s > x√q+C2√n	
(403)
P[∥M∥> s]≤C1exp 
−qs/x−√n
C3√q−12!
,∀s > x√n+C3√q	
. (404)
Proof. Define M.=1
xM, so that Mij∼i.i.d.N(0,1). By Vershynin (2018, Example 2.5.8,
Lemma 3.4.2), we see that each row Mihas Orlicz norm ∥Mi∥ψ2≤C1for some universal
constant C1>0.
By Vershynin (2018, Theorem 4.6.1) we have for some other universal constant C2>0
that for all t >0,
√q−C2
1C2(√n+t)≤σmin(n,q)(M)≤σ1(M)≤√q+C2
1C2(√n+t) (405)
91

--- PAGE 92 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
with probability at least 1 −2e−t2. Defining C3.=C2
1C2and noting that ∥·∥=σ1(·), we
have with the same probability that
∥M∥ −√q≤C3 √n+t
. (406)
Simplifying, we obtain
∥M∥ −√q≤C3 √n+t
(407)
1
x∥M∥ −√q≤C3 √n+t
(408)
∥M∥ −x√q≤C3x √n+t
(409)
∥M∥ ≤x√q+C3 √n+t	
. (410)
Define s >0 by
s.=x√q+C3 √n+t	
⇐⇒ t=s/x−√q
C3−√n. (411)
note that the range of validity is
t >0⇐⇒ s > x√q+C3√n	
. (412)
Forsin this range, we have
P[∥M∥> s]≤2 exp 
−s/x−√q
C3−√n2!
= 2 exp 
−ns/x−√q
C3√n−12!
.(413)
The other inequality follows from applying this inequality to M⊤.
Proposition 28. There are universal constants C1, C2, C3>0such that the following
holds. Let n, q∈N, and let M∈Rq×nbe such that Mij∼i.i.d.N(0, x2). Then
PhM⊤M−qx2I> si
(414)
≤

C1exp
−nn
1
C2
2C3√nqx2s−1o2
,ifC2
2C3√nqx2≤s≤C2
2qx2
C1exp
−nn
1
C2C3√nx√s−1o2
,ifs≥C2
2qx2.(415)
Proof. Define M.=1
xM, so that Mij∼i.i.d.N(0,1). By Vershynin (2018, Example 2.5.8,
Lemma 3.4.2), we see that each row has Orlicz norm ∥Mi∥ψ2≤C1for some universal
constant C1>0.
By Vershynin (2018, Eq. 4.22) we have for some other universal constant C2>0 that
for all t >0,
1
qM⊤M−I≤C2
1max{δ, δ2}where δ.=C2√n+t√q. (416)
92

--- PAGE 93 ---
White-Box Transformers via Sparse Rate Reduction
with probability at least 1 −2e−t2. Simplifying, we obtain
1
qM⊤M−I≤C2
1max{δ, δ2} (417)
M⊤M−qI≤C2
1qmax{δ, δ2} (418)
(x−1M)⊤(x−1M)−qI≤C2
1qmax{δ, δ2} (419)
x−2M⊤M−qI≤C2
1qmax{δ, δ2} (420)
M⊤M−qx2I≤C2
1qx2·max{δ, δ2}. (421)
Now from simple algebra and the fact that n≥1, we have
max
δ, δ2	
=δ⇐⇒ 0≤t≤C−1
2√q−√n (422)
max
δ, δ2	
=δ2⇐⇒ t≥C−1
2√q−√n. (423)
Now define s≥0 by
s.=C2
1qx2·max{δ, δ2}. (424)
Thus in the first case we have
s.=C2
1C2√qx2(√n+t)⇐⇒ t=1
C2
1C2√qx2s−√n, (425)
and in particular the first case holds when
max{δ, δ2}=δ⇐⇒ 0≤t≤C−1
2√q−√n⇐⇒ C2
1C2√nqx2≤s≤C2
1qx2. (426)
Meanwhile, in the second case, we have
s.=C2
1C2
2(√n+t)2x2⇐⇒ t=1
C1C2x√s−√n (427)
where we obtain only one solution to the quadratic equation by requiring t≥0, and in
particular the second case holds when
max{δ, δ2}=δ⇐⇒ t≥C−1
2√q−√n⇐⇒ s≥C2
1qx2. (428)
Thus we have
P[∥M⊤M−qx2I∥> s] (429)
≤

2 exp
−n
1
C2
1C2√qx2s−√no2
,ifC2
1C2√nqx2≤s≤C2
1qx2
2 exp
−n
1
C1C2x√s−√no2
,ifs≥C2
1qx2(430)
=

2 exp
−nn
1
C2
1C2√nqx2s−1o2
,ifC2
1C2√nqx2≤s≤C2
1qx2
2 exp
−nn
1
C1C2√nx√s−1o2
,ifs≥C2
1qx2.(431)
93

--- PAGE 94 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
B.4 Companion to Section 3.3
In this section, we justify the scaling applied to ∇Rcin Section 3.2, and supply the dis-
cretization scheme used in Section 3.3.
First, suppose that Zℓ
♮satisfies (10), and Zt.=Zℓ
♮+σtW, where Wis a standard
Gaussian matrix, so that Ztsatisfies (11) with noise level σt>0. Let qtbe the density
ofZt. Theoretical analysis from (Lu et al., 2023) and empirical analysis from (Song and
Ermon, 2019) demonstrates that under generic conditions, we have that
∥∇qt(Zt)∥2∝1
σ2
t, (432)
ignoring all terms in the right-hand side except for those involving σt. On the other hand,
from the proof of Theorem 16, we obtain that −∇Rc(Zt) has constant (in σt) magnitude
with high probability. Thus, in order to have them be the same magnitude, we need to divide
−∇Rc(Zt) byσ2
tto have it be a drop-in replacement for the score function, as alluded to
in Section 3.2.
Second, we wish to explicitly state our discretization scheme given in Section 3.3. To
wit, we provide a discretization scheme that turns the structured diffusion ODE (61) into its
gradient descent analogue (62); the other discretization, namely of the structured denoising
ODE, from (55) to (56) occurs similarly. To begin with, define the shorthand notation
f(t,eZ(t)).=∇Rc(eZ(t)|U[K](T−t)), (433)
so that we have
deZ(t) =1
2tf(t,eZ(t)) dt. (61)
FixL, and let 0 < t1< t2<···< tL=T, such that t1is small. (These will be specified
shortly in order to supply the discretization scheme.) A suitable first-order discretization
is given by
eZℓ+1≈eZℓ+tℓ+1−tℓ
2tℓf(tℓ,eZℓ). (434)
Thus it remains to set t1, . . . , t Lsuch that
tℓ+1−tℓ
2tℓ=κ (435)
for some constant κ, we observe that we must set
tℓ+1= (1 + 2 κ)tℓ, (436)
so that the time grows exponentially in the index. The reverse process time decays expo-
nentially in the index, which matches practical discretization schemes for ordinary diffusion
models (Song and Ermon, 2019). Finally, we have T=tL= (1+2 κ)Lt1, so that t1=T
(1+2κ)L.
94

--- PAGE 95 ---
White-Box Transformers via Sparse Rate Reduction
Appendix C. Additional Implementation Details and Experimental
Results
In this section, we provide details about our experiments, and report the results of additional
experiments that were not covered in the main text. crate takes arguably the most basic
design choices possible, and so we do notattempt to directly compete with state-of-the-art
performance from heavily engineered and empirically designed transformers. The results of
our experiments are meant to convey a few core messages:
•Despite not being engineered to compete with the state-of-the-art, crate performs
strongly on large-scale real-world datasets , including classification on ImageNet-1K.
crate also achieves strong transfer learning performance.
•Because our model is designed through unrolled optimization of a well-understood ob-
jective, each layer is interpretable . In particular, we can analyze the performance of
crate , as well as design network modifications, on a layer-wise basis . This is pow-
ered by an arguably unparalleled level of insight into the role of each operator in our
network.
•We make the simplest possible choices during the design of crate , but these can be
changed easily while keeping the same framework . We study a few modifications later
in this section (Appendix C.5) and show that they do not significantly hurt empirical
performance, but emphasize here that there is significant potential for improvement
with different architecture choices (and in particular a different theoretical analysis).
C.1 Details about CRATE for Image Classification
In this subsection, we provide more details for implementing crate on vision tasks.
Training setup. We fine-tune our pre-trained crate on the following target datasets:
CIFAR10/CIFAR100 (Krizhevsky et al., 2009), Oxford Flowers-102 (Nilsback and Zisser-
man, 2008), Oxford-IIIT-Pets (Parkhi et al., 2012). For each fine-tuning task, we employ
the AdamW optimizer (Loshchilov and Hutter, 2019). We configure the learning rate as
5×10−5, weight decay as 0 .01, and batch size as 256. To allow transfer learning, in all
training and evaluations setups we first resize our input data to 224 height and width. For
data augmentations during pre-training and fine-tuning, we also adopt several standard
techniques: random cropping, random horizontal flipping, and random augmentation (with
number of transformations n= 2 and magnitude of transformations m= 14).
Pre-training on ImageNet-1K. We apply the Lion optimizer (Chen et al., 2023c) for
pre-training both crate and ViT models. We configure the learning rate as 2 .4×10−4,
weight decay as 0.5, and batch size as 2,048. We incorporate a warm-up strategy with
a linear increase over 5 epochs, followed by training the models for a total of 150 epochs
with cosine decay. For data augmentation, we only apply the standard techniques, random
cropping and random horizontal flipping, on the ImageNet-1K dataset. We apply label
smoothing with smoothing parameter 0 .1. One training epoch of crate -Base takes around
240 seconds using 16 A100 40GB GPUs.
95

--- PAGE 96 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Table 8: Image classification model configurations for different sizes of crate , parameter counts,
and comparisons to ViT models.
Model Size L d K N crate # Parameters ViT # Parameters
Tiny 12 384 6 196 6.1M 5.72M
Small 12 576 12 196 13.1M 22.05M
Base 12 768 12 196 22.8M 86.54M
Large 24 1024 16 196 77.6M 307M
Table 9: Model configurations for different sizes of crate -MAE, parameter counts, and comparisons
to MAE models. Note that MAE does not provide model configurations smaller than Base (He et al.,
2021). We observe that crate -MAE-Base uses around 30% of the parameters of MAE-Base.†Note
that MAE-Large has more than 12 encoder layers — we use the model configuration from He et al.
(2021) for the MAE models — but it is the next-smallest MAE model after Base, so it is suitable
for a comparison with crate -MAE-Large.
Model Size L d K N crate # Parameters MAE # Parameters
Small 12 576 12 196 25.4M 47.5M
Base 12 768 12 196 44.6M 143.8M
Large 12 1024 16 196 78.5M 406.0M†
Fine-tuning on image classification tasks. For each fine-tuning task, we use the
AdamW optimizer (Loshchilov and Hutter, 2019). We configure the learning rate as 5 ×10−5,
weight decay as 0.01, and batch size as 512. To allow transfer learning, we first resize our
input data to 224. For data augmentations, we also adopt several standard techniques:
random cropping, random horizontal flipping, and random augmentation (with number of
transformations n= 2 and magnitude of transformations m= 14).23
C.2 Details about CRATE-MAE for Image Completion
Pre-training on ImageNet-1K. We apply the AdamW optimizer (Loshchilov and Hut-
ter, 2019) for pre-training both crate -MAE models on ImageNet-1K. We configure the
learning rate as 3 ×10−5, weight decay as 0 .1, and batch size as 4 ,096. We incorporate
a warm-up strategy with a linear increase over 40 epochs, followed by training the models
for a total of 800 epochs with cosine decay. For data augmentation, we apply the standard
augmentations as used in He et al. (2021), random cropping and random horizontal flipping.
Fine-tuning CRATE-MAE models. Recall that in Section 4.1.2, we described two
methods of fine-tuning our model: full fine-tuning, which updates all the weights, and lin-
ear probing via logistic regression, which only updates the classification head. For full fine-
tuning, we apply the same fine-tuning configuration (data augmentation, training epochs,
optimization, etc.) as described in Appendix C.1. For linear probing, we apply the op-
timization solver in scikit-learn , i.e., linear model.LogisticRegression , to learn a
23.https://github.com/huggingface/pytorch-image-models/blob/main/timm/data/auto_augment.py
96

--- PAGE 97 ---
White-Box Transformers via Sparse Rate Reduction
Table 10: (upper ) Configuration of the crate BERT-family models. ( lower ) Configuration of the
crate GPT-family models. drepresents the dimension of the token, Krepresents the number of
attentions head in the attention block, Lrepresents the model depth, and hrepresents the hidden
dimension of the MLP block in standard transformers. For crate models, there is no hidden
dimension.
Model Size Pre-training d K L h crate BERT
Medium Masked LM 512 8 8 2,048 33M 41M
Base Masked LM 768 12 12 3,072 61M 110M
Large Masked LM 1,024 16 24 4,096 129M 335M
Model Size Pre-training d K L h crate GPT-2
Small Causal LM 512 8 12 2,048 - 64M
Base Causal LM 768 12 12 3,072 60M 124M
Large Causal LM 1,280 20 36 5,120 242M 774M
logistic regression model with ℓ2regularization, and use cross-validation to select the ℓ2
regularization parameter for each model-dataset pair. In our experiments, we consider ℓ2
regularization parameters from the set {10−3,10−2,10−1,1,10,102,103}.
C.3 Details about CRATE-DINO for Self-Supervised Learning
DINO pre-training. To train crate with the DINO self-supervised learning objective,
we use the ImageNet-1K dataset and match our training recipe closely to the official im-
plementation with some minor variations. Specifically, we choose the base learning rate to
be 0.001 and the weight decay parameter to be 0 .005 in our experiments. Other hyper-
parameters such as the temperature parameters τtandτsare chosen to exactly match with
the publicly available implementation of DINO24. Due to computational limitations, we
only train our models for 100 epochs, compared to 400 to 800 epochs of training for the
official implementation.
Linear probing models trained via DINO. As introduced in Section 4.1.3, we use
both weighted nearest neighbor ( k−NN) and logistic regression on top of the features ex-
tracted from the teacher networks after pre-training with DINO. For k−NN, we search the
number of neighbors kfrom the candidate set of {10,20,100,200}and choose the model that
achieves the highest training accuracy. For logistic regression, we use the same procedure
as introduced earlier for crate -MAE in Appendix C.2.
C.4 Details about CRATE-BERT and CRATE-GPT on Natural Language
Details about pre-training for CRATE-BERT. We use a batch size of 8,096 and
train for 30,000 steps with the Adam optimizer (Kingma and Ba, 2015). For the Adam
optimizer, we use ( β1, β2) = (0 .9,0.98), and a weight decay of 0 .01. For the learning rate
scheduler, we apply the linear warm-up and linear decay, with the peak learning rate at the
iteration of 1,800 steps with value η= 10−3.
24.https://github.com/facebookresearch/dino
97

--- PAGE 98 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Details about pre-training for CRATE-GPT. We use a batch size of 384 and train
for 600,000 steps with the Adam optimizer (Kingma and Ba, 2015). For the Adam optimizer,
we use ( β1, β2) = (0 .9,0.95), and weight decay of 0 .1. For the learning rate scheduler, we
apply the linear warm-up and cosine decay, with a peak value of η= 6×10−4at the 2 ,000
iteration, and minimum value 6 ×10−5. The training and validation losses over iterations
are shown in Figure 12 (right). The training/validation loss converges around 3 .37 after
training with a batch size of 384 and 600,000 iterations. In comparison, the open GPT-2
implementation is pre-trained on OpenWebText with a batch size of 512 and 600,000 steps,
and converges to a validation loss of 2 .85 (Karpathy, 2022).
Details about fine-tuning CRATE-BERT on GLUE. For all the tasks in GLUE, we
use a learning rate of 2 ×10−5without any hyperparameter sweep. We fine-tune the three
models (BERT-Base, BERT-Medium and crate -BERT-Base) using the same fine-tuning
configuration. We fine-tune for 8 epochs on MNLI, for 5 epochs on WNLI and MRPC
(because these two datasets are tiny), and for 3 epochs on all other tasks. For all tasks, we
use a batch size of 32. The maximum sequence length is set to 128 for WNLI and 512 for
all other tasks. We find that the performance of BERT on WNLI is very sensitive to the
sequence length, so we picked the sequence length to be the same as Huggingface (2020)
(while crate -BERT is very stable to the sequence length). We do not observe a significant
difference of performance using different sequence length for other tasks. Evaluations shown
in Figure 12 are meant to demonstrate the effectiveness of learning, so we finetune 3 epochs
for all tasks (except MNLI in the 24000 and 30000 step), and we use the sequence length
of 512 for all tasks.
Details about CRATE-GPT evaluation. We use the test split as the validation set
for all tasks. The evaluation datasets WikiText2 and WikiText103 has the same test split
so we merge them to WikiText. For both models (GPT2-Base and crate -GPT2-Base) on
all tasks, we use a block size of 1 ,024, evaluation batch size of 4, and evaluate for 1 ,000
iterations to get a more accurate estimation.
C.5 Ablation Study of CRATE on Image Classification
Training hyperparameters for CRATE on image classification. In Table 11, we
present evaluation of crate trained with various parameters. More specifically, we inves-
tigate the effect of number of epochs, weight decay, learning rate, step size ( η) and the
regularization term ( λ) inISTA block. As shown in Table 11, crate demonstrates consis-
tently satisfactory performance across a diverse range of hyperparameters.
Exploring architecture variants of CRATE. In this section, we explore the two fol-
lowing alternative architectures. One architecture involves a modification to the atten-
tion mechanism, while the other involves a modification to the sparsification mechanism.
Again, we re-emphasize that these choices, although principled, are entirely modular and
the choices we make here still lead to very simple architectures. A more sophisticated anal-
ysis may lead to different, more complicated architectures that perform better in practice.
The architectures we experiment with are:
•Compression-inspired attention mechanism: revert the change in (37). That is, the
attention mechanism implements (35) and (36) directly.
98

--- PAGE 99 ---
White-Box Transformers via Sparse Rate Reduction
Table 11: Top 1 accuracy of crate on various datasets with different architecture design variants
when trained on ImageNet.
Model epoch weight decay lr η(ISTA) λ(ISTA) ImageNet
crate -B 150 (default) 0.5 (default) 2 .4×10−40.1 0.1 70.8
crate -B 150 0.5 2 .4×10−40.02 0.1 70.7
crate -B 150 0.5 2 .4×10−40.5 0.1 66.7
crate -B 150 0.5 2 .4×10−40.1 0.02 70.8
crate -B 150 0.5 2 .4×10−40.1 0.5 70.5
crate -B 90 0.5 2 .4×10−40.1 0.1 69.5
crate -B 300 0.5 2 .4×10−40.1 0.1 70.9
crate -B 150 1.0 2.4×10−40.1 0.1 70.3
crate -B 150 0.05 2.4×10−40.1 0.1 70.2
crate -B 150 0.5 4 .8×10−40.1 0.1 70.2
crate -B 150 0.5 1 .2×10−40.1 0.1 70.3
•Majorization-minimization proximal step sparsification: instead of (43), implement
(123).
We obtain the following classification results in Table 12. After conducting additional
simplifications to the network architecture (i.e., imposing additional constraints to the net-
work architecture design), we discover that crate maintains reasonable performance on
ImageNet-1K.
Table 12: Top 1 accuracy of crate on various datasets with different architecture design variants
when trained on ImageNet.
Model MSSA-block ISTA-block ImageNet
crate -B default default 70.8
crate -B Eq. (35) and (36) default 63.3
crate -B default Eq. (123) 68.6
Empirical evaluation of soft-thresholding-based architecture. We summarize the
results of CRATE with soft-thresholding activation in Table 13. We use λ= 10 in Sλη
and set all other hyperparameters the same as in the original CRATE-Base evaluation on
ImageNet-1K. We find that such a soft-thresholding model achieves slightly worse perfor-
mance–a drop of 3.2% top-1 accuracy—compared to the default CRATE-Base (with ReLU
activation).
Pre-training on ImageNet-21K. We study crate when pre-training with supervised
learning on a larger pre-training dataset, ImageNet-21K (Deng et al., 2009). This dataset
comprises 14,197,122 images distributed across 21,841 classes. For training, each RGB im-
age was resized to dimensions 3 ×224×224, normalized using means of (0 .485,0.456,0.406)
99

--- PAGE 100 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Table 13: Top 1 accuracy of crate on ImageNet-1k with different architecture design variants.
The SoftShrink activation Sληis defined as Sλη(x) = sgn( x)·(|x| −λη)+, where sgn is the sign
function and ( ·)+= max( ·,0).
Model MSSA-block ISTA-block ImageNet CIFAR 10* CIFAR 100*
crate -B default ReLU activation (default) 70.8% 96.8% 82.7%
crate -B default SoftShrink activation 67.6% 96.0% 76.8%
Table 14: Top 1 accuracy of crate on various datasets with different model scales when pre-trained
on ImageNet-21K and fine-tuned on the downstream datasets.
crate -T crate -S crate -B ViT-T ViT-B
# parameters 5.74M 14.12M 38.83M 10.36M 102.61M
ImageNet-1K 62.7 74.2 79.5 71.8 85.8
CIFAR10 94.1 97.2 98.1 97.2 98.9
CIFAR100 76.7 84.1 87.9 84.4 90.1
Oxford Flowers-102 82.2 92.2 96.7 92.1 99.5
Oxford-IIIT-Pets 77.0 86.4 90.7 86.2 91.8
and standard deviations of (0 .229,0.224,0.225), and then subjected to center cropping and
random flipping. We set the mini-batch size as 4,096 and apply the Lion optimizer (Chen
et al., 2023c) with learning rate 9 .6×10−5and weight decay 0 .05. All the models, includ-
ingcrate s and ViTs are pre-trained with 90 epochs on ImageNet-21K. For fine-tuning on
downstream tasks, we use the AdamW optimizer Loshchilov and Hutter (2019) and config-
ure the learning rate to 5 ×10−5, weight decay as 0.01. Due to memory constraints, we set
the batch size to be 128 for all experiments conducted for the base models and set it to be
256 for the other smaller models.
As summarized in Table 14, we evaluate transfer learning performance of crate by fine-
tuning models that are pre-trained on ImageNet-21k for the following downstream vision
classification tasks: ImageNet-1k (Deng et al., 2009), CIFAR10/CIFAR100 (Krizhevsky
et al., 2009), Oxford Flowers-102 (Nilsback and Zisserman, 2008), Oxford-IIIT-Pets (Parkhi
et al., 2012). We also evaluate the transfer learning fine-tuning performance of two ViT
models (-T/8 and -B/8) pre-trained on ImageNet-21K for reference. For both crate and
ViT models, we consider the image patch with size 8 ×8.
100

--- PAGE 101 ---
White-Box Transformers via Sparse Rate Reduction
C.6 Ablation Study of ISTA Layer in CRATE
We study the role of the ISTA block in our white-box architecture design. Specifically, we
remove the ISTA block in crate and compare its performance with the default crate in
both vision and language tasks. The results are summarized in Table 15 and 16. We can
find that the constructed white-box transformers without the ISTA block is able to achieve
non-trivial performance on ImageNet. On the other hand, the white-box transformer crate
(with the ISTA sparsity block) achieves much better performance than crate without the
ISTA block, demonstrating the effectiveness of the sparsity block in our architecture design.
Table 15: Top 1 accuracy of crate with different architecture design variants when trained on
ImageNet.
Model MSSA-block ISTA-block ImageNet
crate -B default default 70.8
crate -B default ✗ 61.9
Table 16: Zero-shot cross-entropy loss of the crate -GPT2-Base model with and without the ISTA
block (lower is better).
MSSA-block ISTA-block OWT
crate -GPT2-Base default default 3.37
crate -GPT2-Base default ✗ 3.68
C.7 Ablation Study of MSSA Layer and ISTA Layer in CRATE and
Comparison with ViT
COCO Detection VOC Seg. ImageNet-1K
Model Attention Nonlinearity AP 50 AP75 AP mIoU Accuracy
crate MSSA ISTA 2.1 0.7 0.8 23.9 74.2
crate -MLP MSSA MLP 0.2 0.2 0.2 22.0 79.2
crate -MHSA MHSA ISTA 0.1 0.1 0.0 18.4 77.8
ViT MHSA MLP 0.1 0.1 0.0 14.1 80.9
Table 17: Ablation study of different crate variants. We use the Small-Patch8 (S-8)
model configuration across all experiments in this table.
Both the attention block ( MSSA) and the MLP block ( ISTA) incrate are different from
the ones in the ViT. In order to understand the effect of each component of crate , we
study three different variants of crate :crate ,crate -MHSA,crate -MLP, where we denote
the attention block and MLP block in ViT as MHSA and MLPrespectively. We summarize
different model architectures in Table 17.
101

--- PAGE 102 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
For all models in Table 17, we first apply the same pre-training setup on the ImageNet-
21k dataset. The detection and segmentation results are evaluated on models pre-trained
on ImageNet-21k. Secondly, we further fine-tune the ImageNet-21k pre-trained models
on ImageNet-1k with 50 epochs and summarize the accuracy of different models in Table
Table 17. We then apply the coarse segmentation evaluation and MaskCut evaluation
(described in Section 4.3.2) to quantitatively compare the performance of different models.
As shown in Table 17, crate significantly outperforms other model architectures across all
tasks. Interestingly, we find that the coarse segmentation performance (i.e., VOC Seg) of
the ViT can be significantly improved by simply replacing the MHSA in ViT with the MSSA
incrate , despite the architectural differences between MHSA and MSSA being small. This
demonstrates the effectiveness of the white-box design.
102

--- PAGE 103 ---
White-Box Transformers via Sparse Rate Reduction
C.8 Additional Experimental Results of Layer-Wise Analysis
In this subsection, we provide additional experimental results on crate , including layer-
wise measurements and visualizations.
Layer-wise evaluation of compression and sparsity. Similar to Figure 14, we conduct
the layer-wise evaluation of compression term and sparsity for crate -Tiny, crate -Base,
andcrate -Large in Figure 21. We observe similar behavior as mentioned in Section 4.2:
both the compression term and the sparsity term improves as the layer index increases.
Visualizing layer-wise token representations. In Figure 16, we visualize the token
representations Zℓat different layers ℓ∈ {1, . . . , 12}. We provide more results of the layer-
wise token representation visualization on other samples in Figure 22, Figure 23, Figure 24,
and Figure 25 (Model: crate -Base).
103

--- PAGE 104 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
2 4 6 8 10 12
Layer index - 
400450500550600650700Rc(Z) [SSA block]
Measure coding rate across layers
(a) Compression (Model: crate -Tiny).
2 4 6 8 10 12
Layer index - 
0.250.300.350.400.450.500.550.60Sparsity [ISTA block]
Measure output sparsity across layers (b) Sparsity (Model: crate -Tiny).
2 4 6 8 10 12
Layer index - 
600800100012001400Rc(Z) [SSA block]
Measure coding rate across layers
(c) Compression (Model: crate -Base).
2 4 6 8 10 12
Layer index - 
0.10.20.30.40.50.60.7Sparsity [ISTA block]
Measure output sparsity across layers (d) Sparsity (Model: crate -Base).
0 5 10 15 20 25
Layer index - 
100012001400160018002000Rc(Z) [SSA block]
Measure coding rate across layers
(e) Compression (Model: crate -Large).
0 5 10 15 20 25
Layer index - 
0.10.20.30.40.50.60.7Sparsity [ISTA block]
Measure output sparsity across layers (f) Sparsity (Model: crate -Large).
Figure 21: Left: The compression term Rc(Zℓ+1/2) of the MSSA outputs at different layers. Right :
the sparsity of the ISTA output block, ∥Zℓ+1∥0/(d·N), at different layers.
104

--- PAGE 105 ---
White-Box Transformers via Sparse Rate Reduction
Layer=1
0.000.250.500.751.001.251.501.752.00
Layer=2
0.000.250.500.751.001.251.501.752.00
Layer=3
0.000.250.500.751.001.251.501.752.00
Layer=4
0.000.250.500.751.001.251.501.752.00
Layer=5
0.000.250.500.751.001.251.501.752.00
Layer=6
0.000.250.500.751.001.251.501.752.00
Layer=7
0.000.250.500.751.001.251.501.752.00
Layer=8
0.000.250.500.751.001.251.501.752.00
Layer=9
0.000.250.500.751.001.251.501.752.00
Layer=10
0.000.250.500.751.001.251.501.752.00
Layer=11
0.000.250.500.751.001.251.501.752.00
Layer=12
0.000.250.500.751.001.251.501.752.00
Figure 22: Visualizing layer-wise token Zℓrepresentations at each layer ℓ. To enhance the visual
clarity, we randomly extract a 50 ×50 sub-matrix from Zℓfor display purposes. ( Sample 1 )
Layer=1
0.000.250.500.751.001.251.501.752.00
Layer=2
0.000.250.500.751.001.251.501.752.00
Layer=3
0.000.250.500.751.001.251.501.752.00
Layer=4
0.000.250.500.751.001.251.501.752.00
Layer=5
0.000.250.500.751.001.251.501.752.00
Layer=6
0.000.250.500.751.001.251.501.752.00
Layer=7
0.000.250.500.751.001.251.501.752.00
Layer=8
0.000.250.500.751.001.251.501.752.00
Layer=9
0.000.250.500.751.001.251.501.752.00
Layer=10
0.000.250.500.751.001.251.501.752.00
Layer=11
0.000.250.500.751.001.251.501.752.00
Layer=12
0.000.250.500.751.001.251.501.752.00
Figure 23: Visualizing layer-wise token Zℓrepresentations at each layer ℓ. To enhance the visual
clarity, we randomly extract a 50 ×50 sub-matrix from Zℓfor display purposes. ( Sample 2 )
105

--- PAGE 106 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Layer=1
0.000.250.500.751.001.251.501.752.00
Layer=2
0.000.250.500.751.001.251.501.752.00
Layer=3
0.000.250.500.751.001.251.501.752.00
Layer=4
0.000.250.500.751.001.251.501.752.00
Layer=5
0.000.250.500.751.001.251.501.752.00
Layer=6
0.000.250.500.751.001.251.501.752.00
Layer=7
0.000.250.500.751.001.251.501.752.00
Layer=8
0.000.250.500.751.001.251.501.752.00
Layer=9
0.000.250.500.751.001.251.501.752.00
Layer=10
0.000.250.500.751.001.251.501.752.00
Layer=11
0.000.250.500.751.001.251.501.752.00
Layer=12
0.000.250.500.751.001.251.501.752.00
Figure 24: Visualizing layer-wise token Zℓrepresentations at each layer ℓ. To enhance the visual
clarity, we randomly extract a 50 ×50 sub-matrix from Zℓfor display purposes. ( Sample 3 )
Layer=1
0.000.250.500.751.001.251.501.752.00
Layer=2
0.000.250.500.751.001.251.501.752.00
Layer=3
0.000.250.500.751.001.251.501.752.00
Layer=4
0.000.250.500.751.001.251.501.752.00
Layer=5
0.000.250.500.751.001.251.501.752.00
Layer=6
0.000.250.500.751.001.251.501.752.00
Layer=7
0.000.250.500.751.001.251.501.752.00
Layer=8
0.000.250.500.751.001.251.501.752.00
Layer=9
0.000.250.500.751.001.251.501.752.00
Layer=10
0.000.250.500.751.001.251.501.752.00
Layer=11
0.000.250.500.751.001.251.501.752.00
Layer=12
0.000.250.500.751.001.251.501.752.00
Figure 25: Visualizing layer-wise token Zℓrepresentations at each layer ℓ. To enhance the visual
clarity, we randomly extract a 50 ×50 sub-matrix from Zℓfor display purposes. ( Sample 4 )
106

--- PAGE 107 ---
White-Box Transformers via Sparse Rate Reduction
C.9 Additional Experimental Results of Evaluating Compression and Sparsity
for ViT
We conduct experiments to evaluate the compression ( Rc) and sparsity ( ℓ0) of token repre-
sentations from each layer of a pre-trained ViT-Base (downloaded from https://github.
com/huggingface/pytorch-image-models ). We summarize the results in Figure 26. We
find that without our white-box design, the vanilla ViT does not optimize our proposed
sparse rate reduction objective. This contrasts with the results shown in Figures 14 and
15 of the work, wherein we can observe that the compression term Rcand sparsity value
decrease layerwise for crate , in accordance with our theory.
2 4 6 8 10 12
Layer index - 
350400450500550600650Rc(Z) [SSA block]
Measure coding rate across layers
ViT-S
2 4 6 8 10 12
Layer index - 
0.00.20.40.60.81.0Sparsity [ISTA block]
Measure output sparsity across layers
ViT-S
2 4 6 8 10 12
Layer index - 
0.750.800.850.900.951.00Sparsity [ISTA block]
Measure output sparsity across layers
Threshold 1
Threshold 0.5
Threshold 0.1
2 4 6 8 10 12
Layer index - 
60070080090010001100Rc(Z) [SSA block]
Measure coding rate across layers
ViT-B
2 4 6 8 10 12
Layer index - 
0.00.20.40.60.81.0Sparsity [ISTA block]
Measure output sparsity across layers
ViT-B
2 4 6 8 10 12
Layer index - 
0.840.860.880.900.920.940.960.981.00Sparsity [ISTA block]
Measure output sparsity across layers
Threshold 1
Threshold 0.5
Threshold 0.1
2 4 6 8 10 12
Layer index - 
60070080090010001100Rc(Z) [SSA block]
Measure coding rate across layers
CRATE-S
2 4 6 8 10 12
Layer index - 
0.200.250.300.350.400.450.50Sparsity [ISTA block]
Measure output sparsity across layers
CRATE-S
2 4 6 8 10 12
Layer index - 
0.050.100.150.200.250.300.350.400.45Sparsity [ISTA block]
Measure output sparsity across layers
Threshold 1
Threshold 0.5
Threshold 0.1
Figure 26: Left: The compression term Rc(Zℓ+1/2) of the multi-head self-attention outputs at
different layers. Middle : The sparsity of outputs of the MLPblock, ∥Zℓ+1∥0/(d·N), at different
layers. Right : To get a more fine-grained understanding of the sparsity of MLPblock outputs of ViT,
we use three different thresholds τ∈ {1.0,0.5,0.1}and measureP
i,j1{|Zℓ+1
i,j|< τ}/(d·N), where
Zℓ+1
i,jrepresents the j-th element in the i-th token representation. (First row model: ViT-Small;
second row model: ViT-Base; third row model: our proposed crate -Small).
107

--- PAGE 108 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
C.10 Details and Experimental Results of Attention Map Visualization
We recapitulate the method to visualize attention maps in Abnar and Zuidema (2020);
Caron et al. (2021), at first specializing their use to instances of the crate model before
generalizing to the ViT.
For the kthhead at the ℓthlayer of crate , we compute the self-attention matrix Aℓ
k∈Rn
defined as follows:
Aℓ
k=
Aℓ
k,1...
Aℓ
k,N
∈RN,where Aℓ
k,i=exp(⟨Uℓ∗
kzℓ
i,Uℓ∗
kzℓ
[CLS]⟩)
PN
j=1exp(⟨Uℓ∗
kzℓ
j,Uℓ∗
kzℓ
[CLS]⟩). (437)
We then reshape the attention matrix Aℓ
kinto a√n×√nmatrix and visualize the heatmap
as shown in Figure 20. For example, the ithrow and the jthcolumn element of the heatmap
in Figure 20 corresponds to the mthcomponent of Aℓ
kifm= (i−1)·√n+j. In Figure 20,
we select 4 attention heads of crate and visualize the attention matrix Aℓ
kfor each image.
For the ViT, the entire methodology remains the same, except that the attention map
is defined in the following reasonable way:
Aℓ
k=
Aℓ
k,1...
Aℓ
k,N
∈RN,where Aℓ
k,i=exp(⟨Kℓ∗
kzℓ
i,Qℓ∗
kzℓ
[CLS]⟩)
PN
j=1exp(⟨Kℓ∗
kzℓ
j,Qℓ∗
kzℓ
[CLS]⟩). (438)
where the “query” and “key” parameters of the standard transformer at head kand layer
ℓare denoted Kℓ
kandQℓ
krespectively.
Details about MaskCut. We apply the MaskCut pipeline (Algorithm 1) to generate
segmentation masks and detection bounding box (discussed in Section 4.3.2). As described
by Wang et al. (2023b), we iteratively apply Normalized Cuts (Shi and Malik, 2000) on
the patch-wise affinity matrix Mℓ, where Mℓ
ij=PK
k=1⟨Uℓ∗
kzℓ
i,Uℓ∗
kzℓ
j⟩. At each iterative
step, we mask out the identified patch-wise entries on Mℓ. To obtain more fine-grained seg-
mentation masks, MaskCut employs Conditional Random Fields (CRF) (Kr¨ ahenb¨ uhl and
Koltun, 2011) to post-process the masks, which smooths the edges and filters out unrea-
sonable masks. Correspondingly, the detection bounding box is defined by the rectangular
region that tightly encloses a segmentation mask.
Algorithm 1: MaskCut
Hyperparameter: S, the number of objects to segment.
1Function MaskCut( M):
2fori∈ {1, . . . , S }do
3 mask←NCut (M) ; // mask is a boolean array
4 M←M⊙mask ; // Equivalent to applying the mask to M
5 masks [i]←mask;
6endfor
7return
Additional visualization of attention map. We provide additional experiments on
comparing the attention maps of crate and ViT in Figure 27.
108

--- PAGE 109 ---
White-Box Transformers via Sparse Rate Reduction
CRATE V iT V iT CRATE 
Figure 27: More attention maps of supervised crate and ViT on images from COCO
val2017. We select the second-to-last layer attention maps to visualize for crate and the
last layer for ViT.
109

--- PAGE 110 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Appendix D. PyTorch code for CRATE
We provide PyTorch -style code for implementing our proposed network architecture crate ,
including the encoder architecture in Section 2 and the decoder architecture described in
Section 3.
•In Appendix D.1, we provide the PyTorch -style pseudocode for the MSSA and ISTA
blocks.
•In Appendix D.2, we provide the PyTorch -style pseudocode for an encoder layer fℓof
thecrate transformer.
•In Appendix D.3, we provide the PyTorch style pseudocode for a decoder layer gℓof
thecrate transformer.
•In Appendix D.4, as an example of all the top-level architectures we implement for
experiments in Section 4, we implement the crate image classifier.
D.1 PyTorch-Like Pseudocode for MSSA and ISTA Blocks
Algorithm 2: PyTorch -style pseudocode for MSSA block in transformer.
1class MSSA :
2 # initialization
3 def __init__ (self , dim , heads = 8, dim_head = 64, dropout = 0.):
4 inner_dim = dim_head * heads
5 project_out = not ( heads == 1 and dim_head == dim)
6 self . heads = heads
7 self . scale = dim_head ** -0.5
8 self . attend = Softmax (dim = -1)
9 self . dropout = Dropout ( dropout )
10 self .qkv = Linear (dim , inner_dim , bias = False )
11 self . to_out = Sequential ( Linear ( inner_dim , dim ), Dropout (
dropout )) if project_out else nn. Identity ()
12
13 # forward pass
14 def forward (self , x):
15 w = rearrange ( self .qkv(x), ’b n (h d) -> b h n d’, h = self .
heads )
16 dots = matmul (w, w. transpose (-1, -2)) * self . scale
17 attn = self . attend ( dots )
18 attn = self . dropout ( attn )
19 out = matmul (attn , w)
20 out = rearrange (out , ’b h n d -> b n (h d)’)
21 return self . to_out (out )
D.2 PyTorch-Like Pseudocode for CRATE Encoder
110

--- PAGE 111 ---
White-Box Transformers via Sparse Rate Reduction
Algorithm 3: PyTorch -style pseudocode for ISTA block in transformer.
1class ISTA :
2 # initialization
3 def __init__ (self , dim , dropout = 0., step_size =0.1 , lambd =0.1) :
4 super (). __init__ ()
5 self . weight = nn. Parameter ( torch . Tensor (dim , dim ))
6 with torch . no_grad ():
7 init . kaiming_uniform_ ( self . weight )
8 self . step_size = step_size
9 self . lambd = lambd
10
11 # forward pass
12 def forward (self , x):
13 x1 = F. linear (x, self .weight , bias = None )
14 grad_update = self . step_size * F. linear (x1 - x, self . weight .
t() , bias = None )
15 output = F. relu (x - grad_update - self . step_size * self .
lambd )
16 return output
Algorithm 4: PyTorch -style pseudocode for crate encoder.
1class CRATEEncoder ( Module ):
2 # initialization
3 def __init__ (self , dim , depth , heads , dim_head , dropout = 0.):
4 self . layers = []
5 self . depth = depth
6 for _ in range ( depth ):
7 self . layers . extend ([ LayerNorm (dim ), MSSA (dim , heads ,
dim_head , dropout )])
8 self . layers . extend ([ LayerNorm (dim ), ISTA (dim , dropout )])
9
10 # forward pass
11 def forward (self , x):
12 for ln1 , attn , ln2 , ff in self . layers :
13 x_ = attn (ln1 (x)) + ln1 (x)
14 x = ff(ln2(x_))
15 return x
111

--- PAGE 112 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
D.3 PyTorch-Like Pseudocode for CRATE Decoder
Algorithm 5: PyTorch -style pseudocode for crate decoder.
1class CRATEDecoder ( Module ):
2 # initialization
3 def __init__ (self , dim , depth , heads , dim_head , dropout = 0.):
4 self . layers = []
5 self . depth = depth
6 for _ in range ( depth ):
7 self . layers . extend ([ LayerNorm (dim ), Linear (dim )])
8 self . layers . extend ([ LayerNorm (dim ), MSSA (dim , heads ,
dim_head , dropout )])
9
10 # forward pass
11 def forward (self , x):
12 for ln1 , lin , ln2 , attn in self . layers :
13 x_ = lin (ln1 (x))
14 x = ln2(x_) - attn (ln2(x_))
15 return x
D.4 PyTorch-Like Pseudocode for CRATE Image Classifier
112

--- PAGE 113 ---
White-Box Transformers via Sparse Rate Reduction
Algorithm 6: PyTorch -style pseudocode for crate image classifier.
1class CRATEClassifier ( Module ):
2 # initialization
3 def init (self , image_size , patch_size , num_classes , dim , depth ,
heads , channels = 3, dim_head = 64, dropout = 0., emb_dropout
= 0.):
4 # define patch , image dimensions and number of patches
5 image_height , image_width = pair ( image_size )
6 patch_height , patch_width = pair ( patch_size )
7 num_patches = ( image_height // patch_height ) * ( image_width
// patch_width )
8 patch_dim = channels * patch_height * patch_width
9
10 # define patch embedding , positional embedding , dropout , and
transformer
11 self . to_patch_embedding = Sequential ( Rearrange , LayerNorm (
patch_dim ), Linear ( patch_dim , dim), LayerNorm (dim))
12 self . pos_embedding = Parameter ( random (1, num_patches + 1,
dim ))
13 self . cls_token = Parameter ( random (1, 1, dim ))
14 self . dropout = Dropout ( emb_dropout )
15 self . transformer = CRATEEncoder (dim , depth , heads , dim_head ,
dropout )
16
17 # define pooling , latent layer , and MLP head
18 self . pool = pool
19 self . to_latent = Identity ()
20 self . mlp_head = Sequential ( LayerNorm (dim ), Linear (dim ,
num_classes ))
21
22 # forward pass
23 def forward (self , img):
24 x = self . to_patch_embedding (img)
25 b, n, _ = shape (x)
26 cls_tokens = repeat ( self . cls_token , ’1 1 d -> b 1 d’, b = b)
27 x = concatenate (( cls_tokens , x), dim =1)
28 x += self . pos_embedding [:, :(n + 1)]
29 x = self . dropout (x)
30 x = self . transformer (x)
31 x = mean (x, dim = 1) if self . pool == ’mean ’ else x[:, 0]
32 x = self . to_latent (x)
33 return self . mlp_head (x)
113

--- PAGE 114 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
References
Samira Abnar and Willem H. Zuidema. Quantifying Attention Flow in Transformers. In
Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors, Proceedings
of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020,
Online, July 5-10, 2020 , pages 4190–4197. Association for Computational Linguistics,
2020. doi: 10.18653/V1/2020.ACL-MAIN.385. URL https://doi.org/10.18653/v1/
2020.acl-main.385 .
Michal Aharon, Michael Elad, and Alfred M. Bruckstein. K-SVD: An algorithm for design-
ing overcomplete dictionaries for sparse representation. IEEE Trans. Signal Process. , 54
(11):4311–4322, 2006. doi: 10.1109/TSP.2006.881199. URL https://doi.org/10.1109/
TSP.2006.881199 .
Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, and Cordelia
Schmid. ViViT: A Video Vision Transformer. In 2021 IEEE/CVF International
Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-
17, 2021 , pages 6816–6826. IEEE, 2021. doi: 10.1109/ICCV48922.2021.00676. URL
https://doi.org/10.1109/ICCV48922.2021.00676 .
Dominique Bakry, Ivan Gentil, and Michel Ledoux. Analysis and Geometry of Markov Dif-
fusion Operators . Springer International Publishing, August 2016. ISBN 9783319343235.
URL https://play.google.com/store/books/details?id=tQICvgAACAAJ .
Adrien Bardes, Jean Ponce, and Yann LeCun. VICReg: Variance-Invariance-covariance
Regularization for Self-supervised Learning. In International Conference on Learning
Representations , 2022. URL https://openreview.net/forum?id=xm6YD62D1Ub .
Amir Beck and Marc Teboulle. A Fast Iterative Shrinkage-thresholding Algorithm for Linear
Inverse Problems. SIAM J. Imaging Sci. , 2(1):183–202, 2009. doi: 10.1137/080716542.
URL https://doi.org/10.1137/080716542 .
James Betker, Gabriel Goh, Li Jing, et al. Improving Image Generation with Better Cap-
tions. 2023.
Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ B. Altman, Simran Arora, Sydney
von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik
Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, An-
nie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue,
Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Etha-
yarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D.
Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John
Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain,
Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani,
Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, and
et al. On the Opportunities and Risks of Foundation Models. CoRR , abs/2108.07258,
2021. URL https://arxiv.org/abs/2108.07258 .
114

--- PAGE 115 ---
White-Box Transformers via Sparse Rate Reduction
Anton Bovier, V´ eronique Gayrard, and Markus Klein. Metastability in reversible diffusion
processes II: precise asymptotics for small eigenvalues. Journal of the European Mathe-
matical Society , 7(1):69–99, March 2005. ISSN 1435-9855. doi: 10.4171/JEMS/22. URL
https://ems.press/content/serial-article-files/31531 .
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Pra-
fulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christo-
pher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. Language Models are Few-shot Learners. In Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin,
editors, Advances in Neural Information Processing Systems 33: Annual Confer-
ence on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-
12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html .
Joan Bruna and St´ ephane Mallat. Invariant Scattering Convolution Networks. IEEE Trans.
Pattern Anal. Mach. Intell. , 35(8):1872–1886, 2013. doi: 10.1109/TPAMI.2012.230. URL
https://doi.org/10.1109/TPAMI.2012.230 .
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov,
and Sergey Zagoruyko. End-to-end Object Detection with Transformers. In Andrea
Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision
- ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Pro-
ceedings, Part I , volume 12346 of Lecture Notes in Computer Science , pages 213–229.
Springer, 2020. doi: 10.1007/978-3-030-58452-8 \13. URL https://doi.org/10.1007/
978-3-030-58452-8_13 .
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand
Joulin. Unsupervised Learning of Visual Features by Contrasting Cluster Assignments.
In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and
Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-
12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
70feb62b69f16e0238f741fab228fec2-Abstract.html .
Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´ e J´ egou, Julien Mairal, Piotr Bojanowski,
and Armand Joulin. Emerging Properties in Self-supervised Vision Transformers. In 2021
IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC,
Canada, October 10-17, 2021 , pages 9630–9640. IEEE, 2021. doi: 10.1109/ICCV48922.
2021.00951. URL https://doi.org/10.1109/ICCV48922.2021.00951 .
Kwan Ho Ryan Chan, Yaodong Yu, Chong You, Haozhi Qi, John Wright, and Yi Ma.
ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction.
Journal of Machine Learning Research , 23(114):1–103, 2022. URL http://jmlr.org/
papers/v23/21-0631.html .
115

--- PAGE 116 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved Analysis of Score-based Genera-
tive Modeling: User-friendly Bounds under Minimal Smoothness Assumptions. In An-
dreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and
Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023,
23-29 July 2023, Honolulu, Hawaii, USA , volume 202 of Proceedings of Machine Learn-
ing Research , pages 4735–4763. PMLR, 2023a. URL https://proceedings.mlr.press/
v202/chen23q.html .
Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang. Sampling is
as easy as learning the score: theory for diffusion models with minimal data assumptions.
InThe Eleventh International Conference on Learning Representations , 2023b. URL
https://openreview.net/forum?id=zyLVMgsZ0U_ .
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A Simple
Framework for Contrastive Learning of Visual Representations. In Proceedings of the
37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Vir-
tual Event , volume 119 of Proceedings of Machine Learning Research , pages 1597–1607.
PMLR, 2020. URL http://proceedings.mlr.press/v119/chen20j.html .
Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu
Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, and Quoc V. Le. Symbolic
Discovery of Optimization Algorithms. CoRR , abs/2302.06675, 2023c. doi: 10.48550/
ARXIV.2302.06675. URL https://doi.org/10.48550/arXiv.2302.06675 .
Yubei Chen, Dylan M. Paiton, and Bruno A. Olshausen. The Sparse Manifold
Transform. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grau-
man, Nicol` o Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural In-
formation Processing Systems 31: Annual Conference on Neural Information Pro-
cessing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr´ eal, Canada ,
pages 10534–10545, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/
8e19a39c36b8e5e3afd2a3b2692aea96-Abstract.html .
Thomas M Cover. Elements of information theory . John Wiley & Sons, 1999.
Xili Dai, Shengbang Tong, Mingyang Li, Ziyang Wu, Michael Psenka, Kwan Ho Ryan Chan,
Pengyuan Zhai, Yaodong Yu, Xiaojun Yuan, Heung-Yeung Shum, and Yi Ma. CTRL:
Closed-loop Transcription to an LDR via Minimaxing Rate Reduction. Entropy , 24(4):
456, 2022. doi: 10.3390/E24040456. URL https://doi.org/10.3390/e24040456 .
Xili Dai, Ke Chen, Shengbang Tong, Jingyuan Zhang, Xingjian Gao, Mingyang Li, Druv
Pai, Yuexiang Zhai, Xiaojun Yuan, Heung-Yeung Shum, Lionel M. Ni, and Yi Ma. Closed-
loop Transcription via Convolutional Sparse Coding. CoRR , abs/2302.09347, 2023. doi:
10.48550/ARXIV.2302.09347. URL https://doi.org/10.48550/arXiv.2302.09347 .
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin
Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin,
Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Car-
los Riquelme Ruiz, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjo-
erd van Steenkiste, Gamaleldin Fathy Elsayed, Aravindh Mahendran, Fisher Yu, Avital
116

--- PAGE 117 ---
White-Box Transformers via Sparse Rate Reduction
Oliver, Fantine Huot, Jasmijn Bastings, Mark Collier, Alexey A. Gritsenko, Vighnesh
Birodkar, Cristina Nader Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov,
Filip Pavetic, Dustin Tran, Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers,
Jeremiah J. Harmsen, and Neil Houlsby. Scaling Vision Transformers to 22 Billion
Parameters. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engel-
hardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Ma-
chine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA , volume 202
ofProceedings of Machine Learning Research , pages 7480–7512. PMLR, 2023. URL
https://proceedings.mlr.press/v202/dehghani23a.html .
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A
large-scale hierarchical image database. In 2009 IEEE Computer Society Conference
on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami,
Florida, USA , pages 248–255. IEEE Computer Society, 2009. doi: 10.1109/CVPR.2009.
5206848. URL https://doi.org/10.1109/CVPR.2009.5206848 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training
of Deep Bidirectional Transformers for Language Understanding. In Jill Burstein, Christy
Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1
(Long and Short Papers) , pages 4171–4186. Association for Computational Linguistics,
2019. doi: 10.18653/V1/N19-1423. URL https://doi.org/10.18653/v1/n19-1423 .
David L. Donoho and Carrie Grimes. Image Manifolds which are Isometric to Euclidean
Space. J. Math. Imaging Vis. , 23(1):5–24, 2005. doi: 10.1007/S10851-005-4965-4. URL
https://doi.org/10.1007/s10851-005-4965-4 .
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain
Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Trans-
formers for Image Recognition at Scale. In 9th International Conference on Learning
Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net,
2021. URL https://openreview.net/forum?id=YicbFdNTTy .
Bradley Efron. Tweedie’s Formula and Selection Bias. Journal of the American Statis-
tical Association , 106(496):1602–1614, 2011a. ISSN 0162-1459. doi: 10.1198/jasa.2011.
tm11181. URL http://dx.doi.org/10.1198/jasa.2011.tm11181 .
Bradley Efron. Tweedie’s formula and selection bias. Journal of the American Statistical
Association , 106(496):1602–1614, 2011b.
Michael Elad. Sparse and Redundant Representations - From Theory to Applications in
Signal and Image Processing . Springer, 2010. ISBN 978-1-4419-7010-7. doi: 10.1007/
978-1-4419-7011-4. URL https://doi.org/10.1007/978-1-4419-7011-4 .
Michael Elad, M´ ario A. T. Figueiredo, and Yi Ma. On the Role of Sparse and Redundant
Representations in Image Processing. Proc. IEEE , 98(6):972–982, 2010. doi: 10.1109/
JPROC.2009.2037655. URL https://doi.org/10.1109/JPROC.2009.2037655 .
117

--- PAGE 118 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Patrick Esser, Robin Rombach, and Bj¨ orn Ommer. Taming transformers for high-resolution
image synthesis. arXiv [cs.CV] , December 2020. URL http://arxiv.org/abs/2012.
09841 .
Cong Fang, Hangfeng He, Qi Long, and Weijie J Su. Exploring deep neural networks via
layer-peeled model: Minority collapse in imbalanced training. Proceedings of the National
Academy of Sciences , 118(43):e2103091118, 2021.
Pedro F Felzenszwalb and Daniel P Huttenlocher. Pictorial Structures for Object Recog-
nition. International journal of computer vision , 61(1):55–79, January 2005. ISSN 0920-
5691, 1573-1405. doi: 10.1023/B:VISI.0000042934.15159.49. URL https://doi.org/10.
1023/B:VISI.0000042934.15159.49 .
Pedro F. Felzenszwalb, David A. McAllester, and Deva Ramanan. A discriminatively
trained, multiscale, deformable part model. In 2008 IEEE Computer Society Conference
on Computer Vision and Pattern Recognition (CVPR 2008), 24-26 June 2008, Anchor-
age, Alaska, USA . IEEE Computer Society, 2008. doi: 10.1109/CVPR.2008.4587597.
URL https://doi.org/10.1109/CVPR.2008.4587597 .
Bolin Gao and Lacra Pavel. On the Properties of the Softmax Function with Application
in Game Theory and Reinforcement Learning. CoRR , abs/1704.00805, 2017. URL http:
//arxiv.org/abs/1704.00805 .
Rong Ge, Holden Lee, and Andrej Risteski. Simulated Tempering Langevin Monte Carlo
II: An Improved Proof using Soft Markov Chain Decomposition. CoRR , abs/1812.00793,
2018. URL http://arxiv.org/abs/1812.00793 .
Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, and Philippe Rigollet. A mathematical
perspective on transformers. CoRR , abs/2312.10794, 2023. doi: 10.48550/ARXIV.2312.
10794. URL https://doi.org/10.48550/arXiv.2312.10794 .
Aaron Gokaslan and Vanya Cohen. OpenWebText Corpus, 2019.
Yuan Gong, Andrew Rouditchenko, Alexander H. Liu, David Harwath, Leonid Karlinsky,
Hilde Kuehne, and James R. Glass. Contrastive Audio-visual Masked Autoencoder. In
The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali,
Rwanda, May 1-5, 2023 . OpenReview.net, 2023. URL https://openreview.net/pdf?
id=QPtMRyk5rb .
Google. huggingface BERT releases. https://huggingface.co/google/bert_uncased_
L-8_H-512_A-8/tree/main , 2021.
Karol Gregor and Yann LeCun. Learning Fast Approximations of Sparse Coding. In Jo-
hannes F¨ urnkranz and Thorsten Joachims, editors, Proceedings of the 27th International
Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel , pages 399–
406. Omnipress, 2010. URL https://icml.cc/Conferences/2010/papers/449.pdf .
R´ emi Gribonval, Rodolphe Jenatton, and Francis R. Bach. Sparse and Spurious: Dictionary
Learning With Noise and Outliers. IEEE Trans. Inf. Theory , 61(11):6298–6319, 2015.
doi: 10.1109/TIT.2015.2472522. URL https://doi.org/10.1109/TIT.2015.2472522 .
118

--- PAGE 119 ---
White-Box Transformers via Sparse Rate Reduction
Florentin Guth, John Zarka, and St´ ephane Mallat. Phase Collapse in Neural Networks.
InThe Tenth International Conference on Learning Representations, ICLR 2022, Vir-
tual Event, April 25-29, 2022 . OpenReview.net, 2022. URL https://openreview.net/
forum?id=iPHLcmtietq .
U G Haussmann and E Pardoux. Time Reversal of Diffusions. The Annals of Probability , 14
(4):1188–1205, October 1986. ISSN 0091-1798, 2168-894X. doi: 10.1214/aop/1176992362.
URL https://projecteuclid.org/journals/annals-of-probability/volume-14/
issue-4/Time-Reversal-of-Diffusions/10.1214/aop/1176992362.full .
Hangfeng He and Weijie J. Su. A Law of Data Separation in Deep Learning. CoRR ,
abs/2210.17020, 2022a. doi: 10.48550/ARXIV.2210.17020. URL https://doi.org/10.
48550/arXiv.2210.17020 .
Hangfeng He and Weijie J. Su. A Law of Data Separation in Deep Learning. CoRR ,
abs/2210.17020, 2022b. doi: 10.48550/ARXIV.2210.17020. URL https://doi.org/10.
48550/arXiv.2210.17020 .
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Im-
age Recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition,
CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016 , pages 770–778. IEEE Computer So-
ciety, 2016. doi: 10.1109/CVPR.2016.90. URL https://doi.org/10.1109/CVPR.2016.
90.
Kaiming He, Georgia Gkioxari, Piotr Doll´ ar, and Ross B. Girshick. Mask R-CNN. IEEE
Trans. Pattern Anal. Mach. Intell. , 42(2):386–397, 2020. doi: 10.1109/TPAMI.2018.
2844175. URL https://doi.org/10.1109/TPAMI.2018.2844175 .
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll´ ar, and Ross B. Girshick.
Masked Autoencoders Are Scalable Vision Learners. CoRR , abs/2111.06377, 2021. URL
https://arxiv.org/abs/2111.06377 .
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll´ ar, and Ross B. Girshick.
Masked Autoencoders Are Scalable Vision Learners. In IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-
24, 2022 , pages 15979–15988. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01553. URL
https://doi.org/10.1109/CVPR52688.2022.01553 .
Geoffrey Hinton. How to represent part-whole hierarchies in a neural network, 2021.
Geoffrey Hinton. The Forward-forward Algorithm: Some Preliminary Investigations, 2022.
Geoffrey E Hinton and Richard Zemel. Autoencoders, Minimum Description Length
and Helmholtz Free Energy. In J. Cowan, G. Tesauro, and J. Alspector, ed-
itors, Advances in Neural Information Processing Systems , volume 6. Morgan-
Kaufmann, 1993. URL https://proceedings.neurips.cc/paper_files/paper/1993/
file/9e3cfc48eccf81a0d57663e129aef3cb-Paper.pdf .
119

--- PAGE 120 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Geoffrey E. Hinton, Alex Krizhevsky, and Sida D. Wang. Transforming Auto-encoders. In
Timo Honkela, Wlodzislaw Duch, Mark A. Girolami, and Samuel Kaski, editors, Artificial
Neural Networks and Machine Learning - ICANN 2011 - 21st International Conference
on Artificial Neural Networks, Espoo, Finland, June 14-17, 2011, Proceedings, Part I ,
volume 6791 of Lecture Notes in Computer Science , pages 44–51. Springer, 2011. doi: 10.
1007/978-3-642-21735-7 \6. URL https://doi.org/10.1007/978-3-642-21735-7_6 .
Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the Knowledge in a Neural
Network. CoRR , abs/1503.02531, 2015. URL http://arxiv.org/abs/1503.02531 .
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models.
In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and
Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-
12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html .
Benjamin Hoover, Yuchen Liang, Bao Pham, Rameswar Panda, Hendrik Strobelt,
Duen Horng Chau, Mohammed J. Zaki, and Dmitry Krotov. Energy Transformer. CoRR ,
abs/2302.07253, 2023. doi: 10.48550/ARXIV.2302.07253. URL https://doi.org/10.
48550/arXiv.2302.07253 .
Huggingface. huggingface text classification examples. https://github.com/
huggingface/transformers/tree/main/examples/pytorch/text-classification ,
2020.
Huggingface. huggingface CoLA GitHub Issue. https://github.com/huggingface/
transformers/issues/25043 , 2023.
Aapo Hyv¨ arinen. Estimation of Non-normalized Statistical Models by Score Matching. J.
Mach. Learn. Res. , 6:695–709, 2005. URL http://jmlr.org/papers/v6/hyvarinen05a.
html.
Ian T. Jolliffe. Principal Component Analysis . Springer-Verlag, 2002.
Zahra Kadkhodaie and Eero P. Simoncelli. Solving Linear Inverse Problems Using the Prior
Implicit in a Denoiser. CoRR , abs/2007.13640, 2020. URL https://arxiv.org/abs/
2007.13640 .
Andrej Karpathy. nanoGPT. https://github.com/karpathy/nanoGPT , 2022.
Tero Karras, Samuli Laine, and Timo Aila. A Style-based Generator Architecture for
Generative Adversarial Networks. In IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019 , pages 4401–4410.
Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.00453. URL
http://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_
Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_
paper.html .
120

--- PAGE 121 ---
White-Box Transformers via Sparse Rate Reduction
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating
the Design Space of Diffusion-based Generative Models. In NeurIPS ,
2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/
a98846e9d9cc01cfb87eb694d946ce6b-Abstract-Conference.html .
Robert W Keener. Theoretical statistics: Topics for a core course . Springer, 2010.
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In
Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Rep-
resentations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Pro-
ceedings , 2015. URL http://arxiv.org/abs/1412.6980 .
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chlo´ e Rolland, Laura Gustafson,
Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll´ ar, and
Ross B. Girshick. Segment Anything. CoRR , abs/2304.02643, 2023. doi: 10.48550/
ARXIV.2304.02643. URL https://doi.org/10.48550/arXiv.2304.02643 .
Frederic Koehler, Alexander Heckett, and Andrej Risteski. Statistical Efficiency of Score
Matching: The View from Isoperimetry. In The Eleventh International Conference on
Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net,
2023. URL https://openreview.net/pdf?id=TD7AnQjNzR6 .
Philipp Kr¨ ahenb¨ uhl and Vladlen Koltun. Efficient Inference in Fully Connected CRFs with
Gaussian Edge Potentials. In John Shawe-Taylor, Richard S. Zemel, Peter L. Bartlett,
Fernando C. N. Pereira, and Kilian Q. Weinberger, editors, Advances in Neural Infor-
mation Processing Systems 24: 25th Annual Conference on Neural Information Pro-
cessing Systems 2011. Proceedings of a meeting held 12-14 December 2011, Granada,
Spain , pages 109–117, 2011. URL https://proceedings.neurips.cc/paper/2011/
hash/beda24c1e1b46055dff2c39c98fd6fc1-Abstract.html .
Mark A. Kramer. Nonlinear principal component analysis using autoassociative neural
networks. AIChE Journal , 1991.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny
images. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet Classification with Deep
Convolutional Neural Networks. In Peter L. Bartlett, Fernando C. N. Pereira, Christo-
pher J. C. Burges, L´ eon Bottou, and Kilian Q. Weinberger, editors, Advances in Neu-
ral Information Processing Systems 25: 26th Annual Conference on Neural Information
Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe,
Nevada, United States , pages 1106–1114, 2012. URL https://proceedings.neurips.
cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html .
Yann LeCun, L´ eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning
applied to document recognition. Proc. IEEE , 86(11):2278–2324, 1998. doi: 10.1109/5.
726791. URL https://doi.org/10.1109/5.726791 .
121

--- PAGE 122 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. A tutorial on
energy-based learning. Predicting structured data , 1(0), 2006.
Hongkang Li, Meng Wang, Sijia Liu, and Pin-Yu Chen. A Theoretical Understanding
of Shallow Vision Transformers: Learning, Generalization, and Sample Complexity. In
The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali,
Rwanda, May 1-5, 2023 . OpenReview.net, 2023a. URL https://openreview.net/pdf?
id=jClGv3Qjhb .
Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li, Ankit Singh Rawat, Sashank J.
Reddi, Ke Ye, Felix Chern, Felix X. Yu, Ruiqi Guo, and Sanjiv Kumar. The Lazy
Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers. In The
Eleventh International Conference on Learning Representations, ICLR 2023, Kigali,
Rwanda, May 1-5, 2023 . OpenReview.net, 2023b. URL https://openreview.net/pdf?
id=TJ2nxciYCk- .
Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ra-
manan, Piotr Doll´ ar, and C. Lawrence Zitnick. Microsoft COCO: Common Objects
in Context. In David J. Fleet, Tom´ as Pajdla, Bernt Schiele, and Tinne Tuytelaars,
editors, Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzer-
land, September 6-12, 2014, Proceedings, Part V , volume 8693 of Lecture Notes in Com-
puter Science , pages 740–755. Springer, 2014. doi: 10.1007/978-3-319-10602-1 \48. URL
https://doi.org/10.1007/978-3-319-10602-1_48 .
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual Instruction Tuning.
CoRR , abs/2304.08485, 2023. doi: 10.48550/ARXIV.2304.08485. URL https://doi.
org/10.48550/arXiv.2304.08485 .
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,
Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Robustly Optimized
BERT Pretraining Approach. CoRR , abs/1907.11692, 2019. URL http://arxiv.org/
abs/1907.11692 .
Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In 7th In-
ternational Conference on Learning Representations, ICLR 2019, New Orleans, LA,
USA, May 6-9, 2019 . OpenReview.net, 2019. URL https://openreview.net/forum?
id=Bkg6RiCqY7 .
Yibin Lu, Zhongjian Wang, and Guillaume Bal. Understanding the diffusion models by
conditional expectations. arXiv preprint arXiv:2301.07882 , 2023.
Yi Ma, Harm Derksen, Wei Hong, and John Wright. Segmentation of Multivariate Mixed
Data via Lossy Data Coding and Compression. IEEE Trans. Pattern Anal. Mach. Intell. ,
29(9):1546–1562, 2007. doi: 10.1109/TPAMI.2007.1085. URL https://doi.org/10.
1109/TPAMI.2007.1085 .
Yi Ma, Doris Tsao, and Heung-Yeung Shum. On the principles of Parsimony and Self-
consistency for the emergence of intelligence. Frontiers Inf. Technol. Electron. Eng. , 23
122

--- PAGE 123 ---
White-Box Transformers via Sparse Rate Reduction
(9):1298–1323, 2022. doi: 10.1631/FITEE.2200297. URL https://doi.org/10.1631/
FITEE.2200297 .
Peyman Milanfar. A Tour of Modern Image Filtering: New Insights and Methods, Both
Practical and Theoretical. IEEE Signal Process. Mag. , 30(1):106–128, 2013. doi: 10.
1109/MSP.2011.2179329. URL https://doi.org/10.1109/MSP.2011.2179329 .
A Millet, D Nualart, and M Sanz. Integration by Parts and Time Reversal for Diffusion
Processes. Annals of probability , 17(1):208–238, 1989a. ISSN 0091-1798. URL http:
//www.jstor.org/stable/2244207 .
Annie Millet, David Nualart, and Marta Sanz. Integration by parts and time reversal for
diffusion processes. The Annals of Probability , pages 208–238, 1989b.
Maria-Elena Nilsback and Andrew Zisserman. Automated Flower Classification over a
Large Number of Classes. In Sixth Indian Conference on Computer Vision, Graphics
& Image Processing, ICVGIP 2008, Bhubaneswar, India, 16-19 December 2008 , pages
722–729. IEEE Computer Society, 2008. doi: 10.1109/ICVGIP.2008.47. URL https:
//doi.org/10.1109/ICVGIP.2008.47 .
Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A
strategy employed by V1? Vision research , 37(23):3311–3325, 1997.
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn
Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy
Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack
Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context Learning and In-
duction Heads. CoRR , abs/2209.11895, 2022. doi: 10.48550/ARXIV.2209.11895. URL
https://doi.org/10.48550/arXiv.2209.11895 .
OpenAI. huggingface GPT Model Card. https://huggingface.co/gpt2 , 2019.
OpenAI. GPT-4V(ision) System Card, 2023a. URL https://cdn.openai.com/papers/
GPTV_System_Card.pdf .
OpenAI. GPT-4 Technical Report. CoRR , abs/2303.08774, 2023b. doi: 10.48550/ARXIV.
2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774 .
Maxime Oquab, Timoth´ ee Darcet, Th´ eo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khali-
dov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud
Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li,
Ishan Misra, Michael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv´ e J´ egou,
Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learn-
ing Robust Visual Features without Supervision. CoRR , abs/2304.07193, 2023. doi:
10.48550/ARXIV.2304.07193. URL https://doi.org/10.48550/arXiv.2304.07193 .
Druv Pai, Michael Psenka, Chih-Yuan Chiu, Manxi Wu, Edgar Dobriban, and Yi Ma.
Pursuit of a discriminative representation for multiple subspaces via sequential games.
123

--- PAGE 124 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
J. Frankl. Inst. , 360(6):4135–4171, 2023. doi: 10.1016/J.JFRANKLIN.2023.02.011. URL
https://doi.org/10.1016/j.jfranklin.2023.02.011 .
Vardan Papyan, Yaniv Romano, Jeremias Sulam, and Michael Elad. Theoretical Foun-
dations of Deep Learning via Sparse Representations: A Multilayer Sparse Model and
Its Connection to Convolutional Neural Networks. IEEE Signal Process. Mag. , 35(4):
72–89, 2018. doi: 10.1109/MSP.2018.2820224. URL https://doi.org/10.1109/MSP.
2018.2820224 .
Vardan Papyan, X. Y. Han, and David L. Donoho. Prevalence of Neural Collapse during
the terminal phase of deep learning training. CoRR , abs/2008.08186, 2020. URL https:
//arxiv.org/abs/2008.08186 .
Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs.
In2012 IEEE Conference on Computer Vision and Pattern Recognition, Providence, RI,
USA, June 16-21, 2012 , pages 3498–3505. IEEE Computer Society, 2012. doi: 10.1109/
CVPR.2012.6248092. URL https://doi.org/10.1109/CVPR.2012.6248092 .
William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv
preprint arXiv:2212.09748 , 2022.
Mary Phuong and Marcus Hutter. Formal Algorithms for Transformers. CoRR ,
abs/2207.09238, 2022. doi: 10.48550/ARXIV.2207.09238. URL https://doi.org/10.
48550/arXiv.2207.09238 .
Yilong Qin and Andrej Risteski. Fit Like You Sample: Sample-efficient Generalized Score
Matching from Fast Mixing Markov Chains. CoRR , abs/2306.09332, 2023. doi: 10.48550/
ARXIV.2306.09332. URL https://doi.org/10.48550/arXiv.2306.09332 .
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language
understanding by generative pre-training. 2018.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini
Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger,
and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Super-
vision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International
Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event , volume
139 of Proceedings of Machine Learning Research , pages 8748–8763. PMLR, 2021. URL
http://proceedings.mlr.press/v139/radford21a.html .
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya
Sutskever. Robust Speech Recognition via Large-scale Weak Supervision. In Andreas
Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and
Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023,
23-29 July 2023, Honolulu, Hawaii, USA , volume 202 of Proceedings of Machine Learning
124

--- PAGE 125 ---
White-Box Transformers via Sparse Rate Reduction
Research , pages 28492–28518. PMLR, 2023. URL https://proceedings.mlr.press/
v202/radford23a.html .
Martin Raphan and Eero P. Simoncelli. Least Squares Estimation Without Priors or Su-
pervision. Neural Comput. , 23(2):374–420, 2011. doi: 10.1162/NECO \A\00076. URL
https://doi.org/10.1162/NECO_a_00076 .
Yaniv Romano, Michael Elad, and Peyman Milanfar. The Little Engine That Could: Reg-
ularization by Denoising (RED). SIAM J. Imaging Sci. , 10(4):1804–1844, 2017. doi:
10.1137/16M1102884. URL https://doi.org/10.1137/16M1102884 .
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨ orn Ommer.
High-resolution Image Synthesis with Latent Diffusion Models. In IEEE/CVF Conference
on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June
18-24, 2022 , pages 10674–10685. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01042. URL
https://doi.org/10.1109/CVPR52688.2022.01042 .
Daniel L Ruderman. The statistics of natural images. Network: Computation in Neural
Systems , 5(4):517–548, January 1994. ISSN 0954-898X. doi: 10.1088/0954-898X \5\4\
006. URL https://doi.org/10.1088/0954-898X_5_4_006 .
Sara Sabour, Nicholas Frosst, and Geoffrey E. Hinton. Dynamic Routing Between
Capsules. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wal-
lach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances
in Neural Information Processing Systems 30: Annual Conference on Neural In-
formation Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA ,
pages 3856–3866, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/
2cad8fa47bbef282badbb8de5374b894-Abstract.html .
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L.
Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol
Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi.
Photorealistic Text-to-image Diffusion Models with Deep Language Understanding.
InNeurIPS , 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/
ec795aeadae0b7d230fa35cbaf04c041-Abstract-Conference.html .
Michael E. Sander, Pierre Ablin, Mathieu Blondel, and Gabriel Peyr´ e. Sinkformers:
Transformers with doubly stochastic attention. In Gustau Camps-Valls, Francisco
J. R. Ruiz, and Isabel Valera, editors, International Conference on Artificial Intelli-
gence and Statistics, AISTATS 2022, 28-30 March 2022, Virtual Event , volume 151
ofProceedings of Machine Learning Research , pages 3515–3530. PMLR, 2022. URL
https://proceedings.mlr.press/v151/sander22a.html .
Simo S¨ arkk¨ a and Arno Solin. Applied stochastic differential equations , volume 10. Cam-
bridge University Press, 2019.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural Machine Translation of Rare
Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association
125

--- PAGE 126 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume
1: Long Papers . The Association for Computer Linguistics, 2016. doi: 10.18653/V1/
P16-1162. URL https://doi.org/10.18653/v1/p16-1162 .
Jianbo Shi and Jitendra Malik. Normalized Cuts and Image Segmentation. IEEE Trans.
Pattern Anal. Mach. Intell. , 22(8):888–905, 2000. doi: 10.1109/34.868688. URL https:
//doi.org/10.1109/34.868688 .
Ravid Shwartz-Ziv and Yann LeCun. To Compress or Not to Compress - Self-supervised
Learning and Information Theory: A Review. CoRR , abs/2304.09355, 2023. doi: 10.
48550/ARXIV.2304.09355. URL https://doi.org/10.48550/arXiv.2304.09355 .
Simons Institute. An Observation on Generalization, Aug. 2023. URL https://www.
youtube.com/watch?v=AKMuA_TVz3A .
Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep
Unsupervised Learning using Nonequilibrium Thermodynamics. In Francis R. Bach and
David M. Blei, editors, Proceedings of the 32nd International Conference on Machine
Learning, ICML 2015, Lille, France, 6-11 July 2015 , volume 37 of JMLR Workshop and
Conference Proceedings , pages 2256–2265. JMLR.org, 2015. URL http://proceedings.
mlr.press/v37/sohl-dickstein15.html .
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising Diffusion Implicit Mod-
els. In 9th International Conference on Learning Representations, ICLR 2021, Virtual
Event, Austria, May 3-7, 2021 . OpenReview.net, 2021a. URL https://openreview.
net/forum?id=St1giarCHLP .
Yang Song and Stefano Ermon. Generative Modeling by Estimating Gradients of the
Data Distribution. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Flo-
rence d’Alch´ e-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural
Information Processing Systems 32: Annual Conference on Neural Information Pro-
cessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada ,
pages 11895–11907, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
3001ef257407d5a371a96dcd947c7d93-Abstract.html .
Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Er-
mon, and Ben Poole. Score-based Generative Modeling through Stochastic Differen-
tial Equations. In 9th International Conference on Learning Representations, ICLR
2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net, 2021b. URL https:
//openreview.net/forum?id=PxTIG12RRHS .
Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency Models. In
Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato,
and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML
2023, 23-29 July 2023, Honolulu, Hawaii, USA , volume 202 of Proceedings of Machine
Learning Research , pages 32211–32252. PMLR, 2023. URL https://proceedings.mlr.
press/v202/song23a.html .
126

--- PAGE 127 ---
White-Box Transformers via Sparse Rate Reduction
Daniel A. Spielman, Huan Wang, and John Wright. Exact Recovery of Sparsely-used Dic-
tionaries. In Shie Mannor, Nathan Srebro, and Robert C. Williamson, editors, COLT
2012 - The 25th Annual Conference on Learning Theory, June 25-27, 2012, Edinburgh,
Scotland , volume 23 of JMLR Proceedings , pages 37.1–37.18. JMLR.org, 2012. URL
http://proceedings.mlr.press/v23/spielman12/spielman12.pdf .
Charles M Stein. Estimation of the Mean of a Multivariate Normal Dis-
tribution. The Annals of Statistics , 9(6):1135–1151, November 1981.
ISSN 0090-5364, 2168-8966. doi: 10.1214/aos/1176345632. URL https:
//projecteuclid.org/journals/annals-of-statistics/volume-9/issue-6/
Estimation-of-the-Mean-of-a-Multivariate-Normal-Distribution/10.1214/
aos/1176345632.full .
Xiaoxia Sun, Nasser M. Nasrabadi, and Trac D. Tran. Supervised Deep Sparse Coding
Networks. In 2018 IEEE International Conference on Image Processing, ICIP 2018,
Athens, Greece, October 7-10, 2018 , pages 346–350. IEEE, 2018. doi: 10.1109/ICIP.2018.
8451701. URL https://doi.org/10.1109/ICIP.2018.8451701 .
Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip
Isola. What Makes for Good Views for Contrastive Learning? In Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin,
editors, Advances in Neural Information Processing Systems 33: Annual Confer-
ence on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-
12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
4c2e5eaae9152079b9e95845750bb9ab-Abstract.html .
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal
Statistical Society Series B: Statistical Methodology , 58(1):267–288, 1996.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle.
In2015 IEEE Information Theory Workshop, ITW 2015, Jerusalem, Israel, April 26 -
May 1, 2015 , pages 1–5. IEEE, 2015. doi: 10.1109/ITW.2015.7133169. URL https:
//doi.org/10.1109/ITW.2015.7133169 .
Bahareh Tolooshams and Demba E. Ba. Stable and Interpretable Unrolled Dictionary
Learning. Trans. Mach. Learn. Res. , 2022, 2022. URL https://openreview.net/forum?
id=e3S0Bl2RO8 .
Ilya O. Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai,
Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszko-
reit, Mario Lucic, and Alexey Dosovitskiy. MLP-Mixer: An all-MLP Ar-
chitecture for Vision. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N.
Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neu-
ral Information Processing Systems 34: Annual Conference on Neural Informa-
tion Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual , pages
24261–24272, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/
cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Abstract.html .
127

--- PAGE 128 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Shengbang Tong, Xili Dai, Ziyang Wu, Mingyang Li, Brent Yi, and Yi Ma. Incremental
Learning of Structured Memory via Closed-loop Transcription. In The Eleventh Interna-
tional Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5,
2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=XrgjF5-M3xi .
Asher Trockman, Devin Willmott, and J. Zico Kolter. Understanding the Covariance Struc-
ture of Convolutional Filters. In The Eleventh International Conference on Learning Rep-
resentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net, 2023. URL
https://openreview.net/pdf?id=WGApODQvwRg .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.
Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In Is-
abelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fer-
gus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neu-
ral Information Processing Systems 30: Annual Conference on Neural Informa-
tion Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA ,
pages 5998–6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/
3f5ee243547dee91fbd053c1c4a845aa-Abstract.html .
Singanallur V. Venkatakrishnan, Charles A. Bouman, and Brendt Wohlberg. Plug-and-
play priors for model based reconstruction. In IEEE Global Conference on Signal and
Information Processing, GlobalSIP 2013, Austin, TX, USA, December 3-5, 2013 , pages
945–948. IEEE, 2013. doi: 10.1109/GLOBALSIP.2013.6737048. URL https://doi.org/
10.1109/GlobalSIP.2013.6737048 .
Roman Vershynin. High-dimensional probability: An introduction with applications in data
science , volume 47. Cambridge university press, 2018.
Rene Vidal. Attention: Self-expression Is All You Need, 2022. URL https://openreview.
net/forum?id=MmujBClawFo . Unpublished; available: https://openreview.net/
forum?id=MmujBClawFo .
Ren´ e Vidal, Yi Ma, and S. Shankar Sastry. Generalized Principal Component Analy-
sis, volume 40 of Interdisciplinary applied mathematics . Springer, 2016. ISBN 978-
0-387-87810-2. doi: 10.1007/978-0-387-87811-9. URL https://doi.org/10.1007/
978-0-387-87811-9 .
Pascal Vincent. A Connection Between Score Matching and Denoising Autoencoders. Neural
Comput. , 23(7):1661–1674, 2011. doi: 10.1162/NECO \A\00142. URL https://doi.
org/10.1162/NECO_a_00142 .
Michael B Wakin, David L Donoho, Hyeokho Choi, and Richard G Baraniuk. The multiscale
structure of non-differentiable image manifolds. In Wavelets XI , volume 5914, pages 413–
429. SPIE, 2005.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R.
Bowman. GLUE: A Multi-task Benchmark and Analysis Platform for Natural Lan-
guage Understanding. In 7th International Conference on Learning Representations,
128

--- PAGE 129 ---
White-Box Transformers via Sparse Rate Reduction
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net, 2019. URL
https://openreview.net/forum?id=rJ4km2R5t7 .
Haoqing Wang, Xun Guo, Zhi-Hong Deng, and Yan Lu. Rethinking Minimal Sufficient
Representation in Contrastive Learning. In IEEE/CVF Conference on Computer Vision
and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022 , pages
16020–16029. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01557. URL https://doi.
org/10.1109/CVPR52688.2022.01557 .
Peng Wang, Xiao Li, Can Yaras, Zhihui Zhu, Laura Balzano, Wei Hu, and Qing Qu.
Understanding Deep Representation Learning via Layerwise Feature Compression and
Discrimination. CoRR , abs/2311.02960, 2023a. doi: 10.48550/ARXIV.2311.02960. URL
https://doi.org/10.48550/arXiv.2311.02960 .
Xudong Wang, Rohit Girdhar, Stella X. Yu, and Ishan Misra. Cut and Learn for Un-
supervised Object Detection and Instance Segmentation. In IEEE/CVF Conference on
Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June
17-24, 2023 , pages 3124–3134. IEEE, 2023b. doi: 10.1109/CVPR52729.2023.00305. URL
https://doi.org/10.1109/CVPR52729.2023.00305 .
Brent De Weerdt, Yonina C. Eldar, and Nikos Deligiannis. Designing Transformer Net-
works for Sparse Recovery of Sequential Data Using Deep Unfolding. In IEEE Inter-
national Conference on Acoustics, Speech and Signal Processing ICASSP 2023, Rhodes
Island, Greece, June 4-10, 2023 , pages 1–5. IEEE, 2023. doi: 10.1109/ICASSP49357.
2023.10094712. URL https://doi.org/10.1109/ICASSP49357.2023.10094712 .
John Wright and Yi Ma. High-Dimensional Data Analysis with Low-Dimensional Models:
Principles, Computation, and Applications . Cambridge University Press, 2022.
Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. Unsupervised Feature Learn-
ing via Non-parametric Instance Discrimination. In 2018 IEEE Conference on Com-
puter Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June
18-22, 2018 , pages 3733–3742. Computer Vision Foundation / IEEE Computer Society,
2018. doi: 10.1109/CVPR.2018.00393. URL http://openaccess.thecvf.com/content_
cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html .
Jinrui Yang, Xianhang Li, Druv Pai, Yuyin Zhou, Yi Ma, Yaodong Yu, and Cihang Xie.
Scaling white-box transformers for vision. arXiv preprint arXiv:2405.20299 , 2024.
Yongyi Yang, Zengfeng Huang, and David P. Wipf. Transformers from an Optimization
Perspective. In NeurIPS , 2022. URL http://papers.nips.cc/paper_files/paper/
2022/hash/efd1e27afcb94addd03b9e14c8d9f78f-Abstract-Conference.html .
Can Yaras, Peng Wang, Zhihui Zhu, Laura Balzano, and Qing Qu. Neural Collapse
with Normalized Features: A Geometric Analysis over the Riemannian Manifold.
InNeurIPS , 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/
4b3cc0d1c897ebcf71aca92a4a26ac83-Abstract-Conference.html .
129

--- PAGE 130 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Yaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. Learning
Diverse and Discriminative Representations via the Principle of Maximal Coding Rate
Reduction. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Bal-
can, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems
33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/
2020/hash/6ad4174eba19ecb5fed17411a34ff5e6-Abstract.html .
John Zarka, Louis Thiry, Tom´ as Angles, and St´ ephane Mallat. Deep Network Classification
by Scattering and Homotopy Dictionary Learning. In 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . Open-
Review.net, 2020. URL https://openreview.net/forum?id=SJxWS64FwH .
Yuexiang Zhai, Hermish Mehta, Zhengyuan Zhou, and Yi Ma. Understanding l4-based
Dictionary Learning: Interpretation, Stability, and Robustness. In 8th International
Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020. OpenReview.net, 2020a. URL https://openreview.net/forum?id=SJeY-1BKDS .
Yuexiang Zhai, Zitong Yang, Zhenyu Liao, John Wright, and Yi Ma. Complete dictionary
learning via l 4-norm maximization over the orthogonal group. The Journal of Machine
Learning Research , 21(1):6622–6689, 2020b.
Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio
Torralba, and Sanja Fidler. Aligning Books and Movies: Towards Story-like Visual
Explanations by Watching Movies and Reading Books. In 2015 IEEE International
Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015 ,
pages 19–27. IEEE Computer Society, 2015. doi: 10.1109/ICCV.2015.11. URL https:
//doi.org/10.1109/ICCV.2015.11 .
Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu.
A Geometric Analysis of Neural Collapse with Unconstrained Features. In Marc’Aurelio
Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman
Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Con-
ference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-
14, 2021, virtual , pages 29820–29834, 2021. URL https://proceedings.neurips.cc/
paper/2021/hash/f92586a25bb3145facd64ab20fd554ff-Abstract.html .
130
