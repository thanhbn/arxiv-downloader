# Giải thích các Mô hình Code dựa trên Transformer:
Chúng Học được gì? Khi nào Chúng không Hoạt động?

Tác giả thứ 1: Ahmad Haji Mohammadkhani
Đại học Calgary
Calgary, Canada
ahmad.hajimohammadkh@ucalgary.ca

Tác giả thứ 2: Chakkrit Tantithamthavorn  
Đại học Monash
Melbourne, Australia
Chakkrit@monash.edu

Tác giả thứ 3: Hadi Hemmatif
Đại học York
Toronto, Canada
hemmati@yorku.ca

**Tóm tắt** — Trong những năm gần đây, đã có sự quan tâm rộng rãi đến việc thiết kế các mô hình dựa trên mạng nơ-ron sâu để tự động hóa các tác vụ kỹ thuật phần mềm downstream trên mã nguồn, như tạo tài liệu code, tìm kiếm code và sửa chữa chương trình. Mặc dù mục tiêu chính của các nghiên cứu này là cải thiện hiệu quả của tác vụ downstream, nhiều nghiên cứu chỉ cố gắng sử dụng mô hình mạng nơ-ron tốt nhất tiếp theo, mà không có phân tích sâu đúng đắn về lý do tại sao một giải pháp cụ thể hoạt động hoặc không hoạt động, trên các tác vụ hoặc kịch bản cụ thể. Trong bài báo này, sử dụng một phương pháp eXplainable AI (XAI) mẫu (cơ chế attention), chúng tôi nghiên cứu hai mô hình ngôn ngữ lớn (LLM) gần đây cho code (CodeBERT và GraphCodeBERT) trên một tập các tác vụ downstream kỹ thuật phần mềm: tạo tài liệu code (CDG), cải tiến code (CR), và dịch code (CT). Thông qua các nghiên cứu định lượng và định tính, chúng tôi xác định những gì CodeBERT và GraphCodeBERT học được (đặt attention cao nhất lên, về mặt các loại token mã nguồn), trên các tác vụ này. Chúng tôi cũng chỉ ra một số mẫu phổ biến khi mô hình không hoạt động như mong đợi (hoạt động kém ngay cả trên các vấn đề dễ) và đề xuất các khuyến nghị có thể giảm bớt các thách thức được quan sát.

**Từ khóa chỉ mục** — Explainable AI (XAI), LLM, Mô hình Code, Có thể Giải thích, Attention, Transformer.

## I. GIỚI THIỆU

Các Mô hình Ngôn ngữ Lớn (LLM) cho code (gọi tắt: mô hình code) được đề xuất để phân tích các kho dữ liệu lớn về mã nguồn và ngôn ngữ tự nhiên được thu thập từ các nền tảng mã nguồn mở (ví dụ: GitHub và StackOverflow). Các mô hình code được pre-train như vậy đã được sử dụng để tự động hóa các tác vụ liên quan đến mã nguồn khác nhau, ví dụ: hiểu code, tạo code, phát hiện bản sao code [1], phát hiện lỗi [2], [3], và tóm tắt code [4]. Việc tự động hóa các tác vụ kỹ thuật phần mềm như vậy đã được chứng minh là cải thiện đáng kể năng suất của các nhà phát triển phần mềm và giảm chi phí phát triển phần mềm.

Các nghiên cứu gần đây đã đề xuất các mô hình code dựa trên Transformer, ví dụ: CodeBERT [5], GraphCodeBERT [6], CodeGPT [7], CodeT5 [8]. Tuy nhiên, hầu hết các nghiên cứu này thường tập trung vào việc cải thiện độ chính xác của nó — mà không xem xét khía cạnh khả năng giải thích của nó. Do đó, khi triển khai các mô hình như vậy trong thực tế, các practitioner vẫn không biết tại sao các mô hình như vậy cung cấp một khuyến nghị hoặc gợi ý nhất định.

Hãy xem xét một đoạn mã Python nhất định của thuật toán bubble sort. Một mô hình tóm tắt code có thể có khả năng tóm tắt chính xác rằng đoạn mã đã cho là một thuật toán bubble sort. Tuy nhiên, các nhà phát triển có thể không tin tưởng các mô hình nếu các mô hình tạo ra văn bản tự nhiên chính xác dựa trên thụt lề, khoảng trắng hoặc dấu ngoặc đơn của đoạn mã Python, thay vì thông tin ngữ nghĩa có ý nghĩa (tức là bubble sort). Do đó, các dự đoán chính xác được tạo ra bởi các mô hình không đảm bảo rằng các mô hình được học một cách chính xác.

Vì vậy, việc thiếu khả năng giải thích của các mô hình code lớn và phức tạp có thể dẫn đến việc thiếu áp dụng trong thực tế.

Trong bài báo này, chúng tôi tiến hành một nghiên cứu thực nghiệm để phân tích các mô hình code thông qua lăng kính của Explainable AI. Cụ thể, chúng tôi tập trung vào hai mô hình code nổi tiếng, tức là CodeBERT và GraphCodeBERT với ba tác vụ downstream cụ thể cho hiểu & tạo, tức là các tác vụ Tóm tắt Code (Code →Text), Biến đổi Code (Code →Code), và Dịch Code (Code →Code). Để giải thích các dự đoán của các mô hình này, chúng tôi tận dụng cơ chế attention bên trong kiến trúc Transformer, đây là một phương pháp Explainable AI nội tại. Cơ chế attention cho phép chúng tôi hiểu những token quan trọng nhất trong chuỗi đầu vào đóng góp nhiều nhất cho các token trong chuỗi đầu ra. Cụ thể, chúng tôi nhằm mục đích giải quyết hai câu hỏi nghiên cứu sau:

**Các mô hình code được pre-train học được gì?**

Kết quả: Phân tích điểm attention và phân bố của chúng trên các loại token khác nhau cho thấy các mô hình học tập trung vào các loại token cụ thể cho mỗi tác vụ downstream. Trong CDG, các mô hình học tập trung vào chữ ký phương thức (tức là tên phương thức và đối số đầu vào). Trong khi đó trong CT, cú pháp, đó là các token liên quan đến ngôn ngữ lập trình, thu hút nhiều attention hơn. CR có vị trí trung gian so với hai tác vụ kia và có phân bố attention cân bằng hơn. Ngoài ra, được chỉ ra rằng GraphCodeBERT chú ý nhiều hơn đến các phần cấu trúc của mã nguồn, thay vì CodeBERT, điều này có thể là kết quả của bước bổ sung của GraphCodeBERT để phân tích code và tận dụng luồng dữ liệu của code. Những quan sát này phù hợp với những gì được mong đợi từ một mô hình code dẫn chúng tôi đến kết luận rằng hai mô hình được nghiên cứu thực sự đang học (trong hầu hết các trường hợp) những gì chúng được cho là phải học.

**Khi nào các mô hình code được pre-train không hoạt động?**

Kết quả. Phát hiện của chúng tôi cho thấy có những tình huống nhất định khiến các mô hình hoạt động kém trên các tác vụ khác nhau. Ví dụ, các mô hình không hoạt động tốt với các mẫu có mã nguồn dài hoặc phức tạp và/hoặc câu trả lời mong đợi dài (đầu ra mô hình). Chúng tôi cũng chỉ ra rằng hiệu suất kém của mô hình thường được phản ánh trong phân bố attention của nó. Nói cách khác, bất cứ khi nào mô hình không đạt được đầu ra tốt cho một mô hình, nó cũng đã thất bại trong việc chú ý đủ đến các loại token tương ứng cho tác vụ downstream tương ứng. Chúng tôi cũng đã cung cấp một số khuyến nghị về cách có thể giảm bớt những điểm yếu này trong bài báo.

Những phát hiện này dẫn chúng tôi đến kết luận rằng mặc dù các mô hình được pre-train đã cho thấy kết quả tuyệt vời trên các tác vụ kỹ thuật phần mềm; không có mô hình nào trong số chúng có thể được coi là một vấn đề đã đóng và có những khía cạnh nhất định của các mô hình này cần được tập trung nhiều hơn thông qua các nghiên cứu tiếp theo. Việc giải thích các mô hình này có thể làm sáng tỏ những điểm yếu của chúng và cung cấp hướng đi cho nghiên cứu tương lai.

**Khoa học Mở**. Để thúc đẩy sáng kiến khoa học mở, chúng tôi đã công khai gói tái tạo tại GitHub.¹

## II. BỐI CẢNH & CÔNG TRÌNH LIÊN QUAN

Khả năng giải thích hiện đang trở thành một mối quan tâm quan trọng trong kỹ thuật phần mềm. Nhiều nhà nghiên cứu thường sử dụng các kỹ thuật AI/ML để dự đoán lỗi, phát hiện malware và ước lượng nỗ lực. Mặc dù các kỹ thuật AI/ML này có thể cải thiện đáng kể năng suất của nhà phát triển, chất lượng phần mềm và trải nghiệm người dùng cuối, các practitioner vẫn không hiểu tại sao các mô hình AI/ML như vậy đưa ra những dự đoán đó [9], [10], [11], [12], [13], [2]. Để giải quyết thách thức này, các nhà nghiên cứu đề xuất các phương pháp khác nhau để tạo ra giải thích ở hai cấp độ:

(1) **Giải thích toàn cục** có thể được tạo ra bằng các kỹ thuật học máy có thể giải thích (ví dụ: cây quyết định, quy tắc quyết định và kỹ thuật hồi quy logistic) hoặc các kỹ thuật nội tại cụ thể cho mô hình (ví dụ: ANOVA, tầm quan trọng biến) để toàn bộ quá trình dự đoán và khuyến nghị trở nên minh bạch và có thể hiểu được. Tuy nhiên, các kỹ thuật nội tại cụ thể cho mô hình như vậy nhằm mục đích cung cấp khả năng giải thích toàn cục, mà không cung cấp giải thích cho các dự đoán cá nhân.

(2) **Giải thích cục bộ**, mặt khác, có thể được tạo ra bằng một số kỹ thuật (ví dụ: LIME, SHAP) để giải thích các dự đoán của các mô hình AI/ML hộp đen phức tạp (ví dụ: mạng nơ-ron, random forest). Các kỹ thuật như vậy có thể cung cấp giải thích cho mỗi dự đoán cá nhân (tức là một instance cần được giải thích), cho phép người dùng hiểu rõ hơn tại sao dự đoán được đưa ra.

Trong kỹ thuật phần mềm, AI có thể giải thích gần đây đã được nghiên cứu trong lĩnh vực dự đoán lỗi (tức là một mô hình phân loại để dự đoán liệu một file/class/method có bị lỗi trong tương lai hay không). Cụ thể, nghiên cứu khảo sát của Jiarpakdee et al. [11] phát hiện rằng việc giải thích các dự đoán quan trọng và hữu ích ngang bằng với việc cải thiện độ chính xác của dự đoán lỗi. Tuy nhiên, đánh giá tài liệu của họ phát hiện rằng 91% (81/96) các nghiên cứu dự đoán lỗi chỉ tập trung vào việc cải thiện độ chính xác dự đoán, mà không xem xét việc giải thích các dự đoán, trong khi chỉ 4% trong số 96 nghiên cứu này tập trung vào việc giải thích các dự đoán.

Mặc dù XAI vẫn là một chủ đề rất ít được nghiên cứu trong cộng đồng kỹ thuật phần mềm, rất ít nghiên cứu XAI hiện có đã cho thấy một số cách sử dụng thành công, ví dụ: trong dự đoán lỗi. Trong một ví dụ, Wattanakriengkrai et al. [14] và Pornprasit và Tantithamthavorn [15] đã sử dụng các kỹ thuật model-agnostic (ví dụ: LIME) cho dự đoán lỗi cấp dòng (ví dụ: dự đoán dòng nào sẽ bị lỗi trong tương lai), giúp các nhà phát triển định vị các dòng lỗi một cách hiệu quả về chi phí. Trong một ví dụ khác, Jiarpakdee et al. [12] và Khanan et al. [16] đã sử dụng các kỹ thuật model-agnostic (ví dụ: LIME) để giải thích các mô hình dự đoán lỗi, giúp các nhà phát triển hiểu rõ hơn tại sao một file được dự đoán là có lỗi. Rajapaksha et al. [13] và Pornprasit et al. [15] đã đề xuất các kỹ thuật model-agnostic dựa trên quy tắc cục bộ để tạo ra hướng dẫn có thể hành động nhằm giúp các nhà quản lý vạch ra các kế hoạch cải thiện chất lượng hiệu quả nhất.

### A. Khoảng trống Nghiên cứu

Mặc dù có những nỗ lực nghiên cứu về khả năng giải thích của các tác vụ phân loại trong các lĩnh vực SE (ví dụ: dự đoán lỗi), ít nghiên cứu tập trung vào các mô hình code được pre-train dựa trên transformer. Cụ thể, các practitioner thường nêu lên những lo ngại ví dụ: tại sao mã nguồn này được tạo ra? tại sao token code này được sửa đổi?. Việc thiếu khả năng giải thích của các mô hình code có thể dẫn đến thiếu niềm tin, cản trở việc áp dụng trong thực tế.

Để giải quyết thách thức này, bài báo này nhằm mục đích giải quyết các câu hỏi nghiên cứu sau: (RQ1) Các mô hình code được pre-train học được gì? và (RQ2) Khi nào các mô hình code được pre-train không hoạt động?

## III. THIẾT LẬP THỰC NGHIỆM

### A. Lựa chọn Mô hình Code được Pre-train

Các mô hình code được pre-train, như transformer, là các mô hình học sâu được huấn luyện trên các tập dữ liệu rộng lớn (ví dụ: các dự án GitHub, bài đăng StackOverflow) để hiểu và tạo mã nguồn. Các mô hình này, còn được gọi là mô hình ngôn ngữ của code, sử dụng các kỹ thuật tự giám sát, bao gồm các kiến trúc dựa trên BERT như Masked Language Modeling (MLM) và Next Sentence Prediction (NSP). Việc huấn luyện này trên các kho dữ liệu lớn cho phép chúng nắm bắt các biểu diễn toàn cục của cả mã nguồn và ngôn ngữ tự nhiên cụ thể cho lập trình. Các mô hình này mang lại lợi ích có giá trị cho các tác vụ downstream đa dạng, loại bỏ nhu cầu xây dựng các mô hình mới từ đầu và tăng cường khả năng tái sử dụng. Đáng chú ý, những tiến bộ gần đây đã tạo ra các mô hình code được pre-train dựa trên Transformer khác nhau (ví dụ: CodeBERT, GraphCodeBERT, CodeGPT, CodeT5). Bài báo này tập trung vào hai mô hình transformer cụ thể: CodeBERT và GraphCodeBERT.

**CodeBERT** [5] là một mô hình được pre-train đa ngôn ngữ bimodal cho ngôn ngữ lập trình (PL) và ngôn ngữ tự nhiên (NL). Nó có một encoder Transformer đa lớp, được huấn luyện trên Masked Language Modeling (MLM) và Replaced Token Detection (RTD) với cả NL và PL làm đầu vào. Mô hình có kiến trúc tương tự như BERT [17] và cho thấy kết quả đầy hứa hẹn trên nhiều tác vụ downstream như Dịch Code, Phát hiện Bản sao, Phát hiện Lỗi, v.v. [5]. CodeBERT đã được chọn làm một trong những mô hình code của chúng tôi vì sự phổ biến của nó tại thời điểm tiến hành nghiên cứu này và nhiều công trình liên quan đã đề xuất một kỹ thuật dựa trên nó hoặc sử dụng nó làm baseline so sánh [18].

**GraphCodeBERT** [6] là một mô hình được pre-train tương tự như CodeBERT nhưng cũng xem xét cấu trúc cấp độ ngữ nghĩa của code. Nó sử dụng data-flow trong giai đoạn pre-training và sử dụng MLM, cùng với Edge Prediction và Node Alignment làm các tác vụ pre-training. Với tính năng này được bao gồm, mô hình có thể cải thiện kết quả trên các tác vụ benchmark của nó so với CodeBERT, nhưng như chúng tôi sẽ chỉ ra trong các thí nghiệm của mình, trong các tác vụ như Tạo Tài liệu Code, nó cho thấy một nhược điểm lớn. GraphCodeBERT đã được chọn làm một trong những mô hình code của chúng tôi vì ít nhất về mặt lý thuyết, nó là một bước tiến so với CodeBERT với thông tin được thêm vào mô hình từ chính code. Nói cách khác, nó thuộc về một danh mục khác của các mô hình code, điều này giúp với tính tổng quát của các phát hiện của chúng tôi.

### B. Fine-tuning Mô hình Code được Pre-train trên Ba Tác vụ Downstream

Các mô hình code được pre-train hiện có đã được sử dụng cho các tác vụ downstream kỹ thuật phần mềm khác nhau, có thể được phân loại thành bốn loại: (1) Text →Text (ví dụ: dịch ngôn ngữ của tài liệu code [7], tái tạo truy vấn [19]); (2) Text →Code (ví dụ: tìm kiếm code [20], [21]); (3) Code→Text (ví dụ: tóm tắt code [22], tạo thông điệp commit [23], [24]); và (4) Code →Code (ví dụ: sửa chữa chương trình tự động [25], [26], [27], dịch ngôn ngữ lập trình [28], hoàn thành code [29]). Trong bài báo này, chúng tôi sẽ tập trung vào ba tác vụ downstream sau.

**Tạo Tài liệu Code (CDG)** hoặc Tóm tắt Code (Code →Text) là một tác vụ NLP được thiết kế để tạo ra các comment ngôn ngữ tự nhiên cho một mã nguồn đã cho, có thể giúp các nhà phát triển hiểu rõ hơn các code trong các dự án phần mềm với các comment sai hoặc thiếu và giảm thời gian bổ sung cần phải dành để đọc mã nguồn. Ví dụ, cho một phương thức Python ("def sum(x,y): ..."), mô hình NLP sẽ tạo ra các comment ngôn ngữ tự nhiên như ("Đây là một hàm tổng").

**Cải tiến Code (Code→Code)** là một tác vụ NLP được thiết kế để tạo ra mã nguồn được cải tiến (ví dụ: một phiên bản đã sửa) cho một mã nguồn đã cho (ví dụ: một phiên bản có lỗi). Cải tiến code đã được nghiên cứu rộng rãi trong bối cảnh đánh giá code [30], [31], [32], giúp các nhà phát triển nhận được code đã được cải tiến có khả năng được phê duyệt mà không cần chờ phản hồi của người đánh giá.

**Dịch Code (Code→Code)** là một tác vụ NLP được thiết kế để tạo ra mã nguồn bằng một ngôn ngữ (ví dụ: Java) cho một mã nguồn đã cho bằng ngôn ngữ khác (ví dụ: C#).

### C. Cài đặt siêu tham số

Trong quá trình huấn luyện mô hình, chúng tôi sử dụng các giá trị tham số mặc định như sau: độ dài nguồn tối đa là 256 và độ dài đích tối đa là 128 với tỷ lệ học 5e-4, với kích thước batch là 16 và huấn luyện trong 100 epoch.

Cả hai mô hình đều tuân theo các bước tương tự để tạo ra đầu ra. Sau khi huấn luyện một mô hình trên một tác vụ downstream trên tập dữ liệu huấn luyện tương ứng, mô hình tạo ra đầu ra cho mỗi mục dữ liệu kiểm tra, từng token một. Đó là, trong thời gian suy luận, trong mỗi bước, một số token (tùy thuộc vào kích thước beam, được đặt trong mô hình) được chọn từ các ứng viên dự đoán tiềm năng, và quá trình này được lặp lại (các token mới được thêm vào chuỗi đầu ra ứng viên) cho đến khi mô hình tạo ra token kết thúc câu, điều này cho biết kết thúc dự đoán.

### D. Đánh giá Độ chính xác Mô hình

Để đảm bảo rằng các giải thích được tạo ra từ các mô hình của chúng tôi là đáng tin cậy, các mô hình phải chính xác. Để đánh giá độ chính xác của mô hình, chúng tôi sử dụng điểm BLEU-4 làm mịn được sử dụng trong nghiên cứu gốc của CodeBERT [5] và thường được sử dụng bởi các kỹ thuật tạo tài liệu baseline [33]. Đối với các tác vụ khác, chúng tôi sử dụng điểm BLEU-4. BLEU-4 là chỉ số đánh giá duy nhất chúng tôi sử dụng trong bài báo này và từ bây giờ, trừ khi chúng tôi nói rõ ràng ngược lại khi chúng tôi sử dụng điểm BLEU, chúng tôi đang đề cập đến điểm BLEU-4. Điểm BLEU [34] tính toán sự chồng lấp n-gram của dự đoán và tài liệu hoặc đoạn code vàng. Vì trong CDG, các câu được tạo ra thường ngắn, và n-gram cấp cao hơn không có khả năng có sự chồng lấp, CodeBERT sử dụng một phiên bản làm mịn [35] để bù đắp điều này, bằng cách cung cấp số đếm bổ sung cho các sự chồng lấp n-gram cấp cao hơn.

Đối với tác vụ tạo tài liệu code, CodeBERT đạt điểm BLEU tổng thể là 17.83 (19.06 và 17.65 cho Python và Java, tương ứng), trong khi GraphCodeBERT đạt điểm BLEU tổng thể là 5.3 và 4.21 cho Java và Python, tương ứng. Đối với tác vụ cải tiến code, CodeBERT đạt điểm BLEU là 91.07 với khớp chính xác là 5.16, trong khi GraphCodeBERT đạt điểm BLEU là 91.31 với khớp chính xác là 9.1. Đối với tác vụ dịch code (Java sang C#), CodeBERT đạt điểm BLEU là 79.92 với khớp chính xác là 59.0, trong khi GraphCodeBERT đạt điểm BLEU là 80.58 với khớp chính xác là 59.4.

### E. Giải thích Mô hình thông qua Điểm Attention

Các mô hình Transformer có thể hiểu các phụ thuộc dài giữa các từ trong một câu (hoặc token trong một đoạn code) bằng cách hưởng lợi từ cơ chế attention. Cơ chế attention về cơ bản hoạt động với một key (k), query (q), và value (v), và dk biểu thị chiều của key. Ở dạng đơn giản nhất, nó tính toán sự tương tự giữa query, key và values như:

Attention(q, k, v) = softmax(qk^T/√dk)V

trong đó keys và queries là các phần tử trong chuỗi. Tính toán tất cả các tích chấm của qi.kj sẽ dẫn đến một ma trận trong đó mỗi hàng đại diện cho trọng số attention cho một phần tử cụ thể i đến tất cả các phần tử khác trong chuỗi. Sau đó, một lớp softmax và phép nhân với vector value sẽ được áp dụng cho ma trận để thu được trung bình có trọng số. Điều này có nghĩa là mọi phụ thuộc giữa mọi hai phần tử sẽ được xem xét trong đầu ra cuối cùng.

Attention là một phương pháp XAI hợp lý để giải thích CodeBERT và GraphCodeBERT. Để tính toán attention cho mỗi token, chúng tôi cần các trọng số cho các lớp attention encoder-decoder. Vì chúng tôi có sáu lớp decoder Transformer được chồng lên nhau, chúng tôi có sáu lớp attention. Thay vì bằng cách nào đó tổng hợp các giá trị attention của 6 lớp này thành một chỉ số, chúng tôi quyết định giữ dữ liệu của tất cả các lớp và phân tích vai trò của các lớp khác nhau trong việc giải thích các đầu ra. Các trọng số attention có sẵn bên trong mô hình, nhưng thư viện Transformers được sử dụng bởi CodeBERT và GraphCodeBERT không cung cấp chúng theo mặc định. Do đó, chúng tôi đã thay đổi implementation của mô hình để thu thập chúng, cũng như vậy. Lưu ý rằng điểm attention cho mỗi token là đầu ra của một lớp softmax, do đó nó có giá trị từ 0 đến 1.

## IV. KẾT QUẢ THỰC NGHIỆM

### A. (RQ1) Các mô hình code được pre-train học được gì?

**Phương pháp**. Để trả lời RQ này, chúng tôi phân tích các trọng số attention cho mỗi loại token chứ không phải các token riêng lẻ. Để làm như vậy, trước tiên chúng tôi cần định nghĩa một danh sách các loại token. Đây là một quyết định chủ quan về các loại token nào có ý nghĩa quan tâm cho nghiên cứu của chúng tôi. Chúng tôi chọn một tập gồm bảy loại token bao gồm tất cả các token và nhóm chúng theo mức độ liên quan ngữ nghĩa của chúng, như sau:

**Tên phương thức**: Tên của phương thức đang nghiên cứu có thể là một trong những yếu tố quyết định chính về việc nhận thức phương thức đó làm gì, đây là một bước rất quan trọng đối với mô hình, đặc biệt trong một số tác vụ như CDG. Điều này chỉ bao gồm tên phương thức chính trong mỗi mẫu (nhắc nhở rằng mỗi mẫu đầu vào là mã nguồn cho một phương thức) chứ không phải các phương thức được gọi trong thân phương thức chính.

**Định danh loại**: Danh mục này đại diện cho tất cả các từ khóa được sử dụng để định danh các loại token trong các ngôn ngữ nguồn Python và Java của chúng tôi. Để biết thêm thông tin về các token này, hãy xem các loại node "type identifier" và "* type" trong tree-sitter cho mỗi ngôn ngữ.

**Từ khóa ngôn ngữ**: Các token lệnh điều khiển luồng đều được gộp lại với nhau cho mỗi ngôn ngữ trong danh mục này. Các token này như sau: Đối với Python: {False, None, True, and, as, assert, async, await, break, class, continue, def, del, elif, else, except, finally, for, from, global, if, import, in, is, lambda, nonlocal, not, or, pass, raise, return, try, while, with, yield} và đối với Java: {if, else, switch, case, while, class, enum, interface, annotation, public, protected, private, static, abstract, final, native, synchronized, transient, volatile, strictfp, assert, return, throw, try, catch, finally, default, super, do, for, break, continue, super, void, import, extends, implements, import, instanceof, new, null, package, this, throws}.

**Lời gọi phương thức**: Danh mục này bao gồm tất cả các token là tên của các phương thức được gọi trong thân phương thức đang nghiên cứu. Chúng cũng có thể là công cụ trong việc mô tả phương thức đang làm gì và có thể chứa lỗi cần sửa.

**Biến cục bộ**: Ở đây chúng tôi xem xét tất cả các biến chỉ được sử dụng trong thân phương thức đang nghiên cứu. Đó là, các đối số đầu vào của phương thức bị loại trừ.

**Biến đầu vào**: Danh mục này chỉ chứa các đối số đầu vào của phương thức đang nghiên cứu. Chúng tôi đã tách nó khỏi danh mục Biến cục bộ.

**Khác**: Danh mục này đại diện cho tất cả các token không được bao gồm trong bất kỳ danh mục nào ở trên. Chủ yếu là các token như dấu chấm câu, giá trị hằng số, dấu ngoặc đơn, v.v.

Lưu ý rằng mặc dù các loại token này được chọn một cách chủ quan, kết quả sẽ biện minh cho lựa chọn thiết kế này bằng cách chỉ ra rằng chúng nằm trong số các token quan trọng nhất và không có nhiều token đóng góp được để lại cho danh mục "Khác". Chúng tôi cũng nên nhấn mạnh rằng mức độ trừu tượng về những gì tạo nên một "danh mục token" tùy thuộc vào người dùng XAI. Ví dụ, chúng tôi quyết định tách danh mục Đối số Đầu vào khỏi danh mục Tên Biến, để phân tích tốt hơn hiệu ứng của chúng riêng lẻ, nhưng việc hợp nhất hai danh mục là một lựa chọn thiết kế hợp lệ (chỉ ở mức độ trừu tượng khác).

Đối với mỗi mẫu trong dữ liệu kiểm tra, chúng tôi thực hiện các bước này để tìm phân bố điểm attention trên các danh mục khác nhau:

Đối với mỗi token được tạo (mỗi bước), chúng tôi lấy trọng số attention của nó đối với các token đầu vào và tìm loại tương ứng của chúng. Sau đó, chúng tôi tích lũy điểm attention của tất cả các token trong mỗi danh mục để có được điểm tổng cho danh mục đó trong mẫu đó. Các danh mục của chúng tôi bao gồm tất cả các token nên tổng của tất cả điểm cho mỗi bước bằng một. Chúng tôi thực hiện cùng một quá trình cho tất cả các token đầu ra trong tất cả các đoạn code của tập dữ liệu kiểm tra và thu thập sự tích lũy của điểm attention cho mỗi danh mục.

Đáng chú ý là kích thước của các danh mục token khá mất cân bằng. Ví dụ, chỉ có một tên phương thức cho mỗi mẫu nhưng có nhiều token trong danh mục "khác". Do đó, chúng tôi chuẩn hóa điểm tổng của mỗi danh mục, theo dân số của nó như trong Bảng II. Điều này cho chúng tôi điểm attention cho mỗi token cho mỗi danh mục. Cuối cùng, chúng tôi chuẩn hóa điểm của tất cả các danh mục, từ 0 đến 100 cho mục đích so sánh dễ dàng hơn.

Trong số các danh mục được định nghĩa này, "Tên phương thức", "Biến đầu vào", và "Biến cục bộ" đại diện nhiều hơn cho các khía cạnh đặt tên của mã nguồn. Vì vậy chúng tôi nhóm chúng trong danh mục cấp cao hơn là "Đặt tên", trong khi "Lời gọi phương thức", "Định danh loại", và "Từ khóa ngôn ngữ" liên quan nhiều hơn đến cấu trúc của code. Do đó, chúng tôi coi chúng là danh mục cấp cao hơn là "Cấu trúc". Cũng lưu ý rằng chúng tôi chỉ báo cáo điểm trung bình của tất cả sáu lớp cho mỗi tác vụ và mô hình ở đây, vì việc báo cáo tất cả kết quả cho mỗi lớp sẽ quá dài và cũng vì kết quả của RQ này khá tương tự trên các lớp khác nhau và tất cả chúng đều tuân theo các mẫu tương tự.

**Kết quả**. Trong ba tác vụ downstream đang nghiên cứu, chúng tôi mong đợi các điểm attention chuẩn hóa khác nhau cho mỗi danh mục cấp cao, như sau: (a) CT là một tác vụ phụ thuộc nhiều vào cấu trúc, vì mô hình phải học cấu trúc ngôn ngữ nguồn, và tạo ra cấu trúc tương đương trong ngôn ngữ đích. (b) Trong CDG, cấu trúc ít quan trọng hơn (các khối lồng nhau và cây cú pháp có ít liên quan đến tài liệu đầu ra). Mặt khác, tên rất quan trọng trong tác vụ này, vì chúng về cơ bản mô tả chức năng của mã nguồn. (c) Cuối cùng, chúng tôi mong đợi cải tiến code nằm ở giữa hai đầu này, vì cả tên và cấu trúc đều quan trọng trong việc debug code.

Bảng III cho thấy điểm attention tổng chuẩn hóa của mỗi danh mục cấp cao và xác nhận giả thuyết của chúng tôi. Tạo Tài liệu Code, như một tác vụ phụ thuộc nhiều vào đặt tên, có điểm attention chuẩn hóa cao đáng kể hơn 63% cho danh mục Đặt tên, trong khi nó chú ý ít hơn nhiều đến danh mục loại token Cấu trúc, so với các tác vụ khác. Nó cũng có số cao hơn cho danh mục Khác có thể hiểu được xem xét thực tế rằng các comment NL trong code cũng là một phần của danh mục này. Dịch Code, mặt khác, là tác vụ duy nhất có điểm attention chuẩn hóa hơn 50% cho các token Cấu trúc và ít hơn bất kỳ tác vụ nào cho danh mục Đặt tên. Cải tiến Code trong so sánh này giữ vị trí trung gian giữa hai tác vụ được đề cập trong cả hai danh mục.

Trong Bảng IV, chúng tôi có phân tích chi tiết hơn cho mỗi danh mục của chúng tôi. Chúng tôi thấy rằng cả hai mô hình đều chú ý nhiều hơn đến các token cấu trúc cho CT. Đi vào chi tiết hơn, kết quả cho thấy attention này tập trung nhiều hơn vào Lời gọi phương thức và Định danh loại thay vì Từ khóa ngôn ngữ, có khoảng chỉ 6% điểm chuẩn hóa. Rất thú vị khi nghiên cứu riêng lẻ, danh mục Tên phương thức có điểm cao thứ hai sau Lời gọi phương thức. Mặc dù trong việc dịch code, tên phương thức thường không thay đổi, điều này cho thấy các mô hình sử dụng đáng kể tên của phương thức để hiểu chức năng của nó.

Trong tác vụ CDG, chúng tôi quan sát thấy sự phụ thuộc lớn vào các danh mục Đặt tên, kết quả cho thấy danh mục Tên phương thức đóng vai trò quan trọng nhất. Nó luôn có điểm chuẩn hóa gần 40% hoặc cao hơn. Trong trường hợp không có Định danh loại, trong GraphCodeBERT CDG python, danh mục này có điểm cao nhất từng có trong tất cả các loại token trên tất cả các tác vụ/mô hình. Trực giác, lượng quan trọng này có thể biện minh, cho rằng ngay cả con người cũng phụ thuộc rất nhiều vào tên phương thức để hiểu chức năng của chúng. Ngoài ra, trong ba trong số bốn thí nghiệm khác nhau cho tác vụ này, Biến đầu vào có điểm attention chuẩn hóa cao thứ hai (từ 11% đến 16%). Điều này về cơ bản có nghĩa là các mô hình đã học được rằng trong khi tạo tài liệu cho một phương thức, phần chính và thú vị nhất là chữ ký của phương thức chứ không phải thân.

Cải tiến Code giữ vị trí trung gian giữa hai tác vụ khác có điểm rất gần cho bốn danh mục Tên phương thức, Biến đầu vào, Lời gọi phương thức và Biến. Vì trong tác vụ này mô hình được cho là tìm lỗi và cố gắng sửa chúng, có vẻ như các mô hình đã học được rằng ít lỗi xảy ra hơn trong các danh mục như Định danh loại, Từ khóa ngôn ngữ và Khác. Điều này có ý nghĩa vì chúng ta biết rằng cơ sở dữ liệu cho tác vụ này được thu thập từ các dự án công khai và chúng có lẽ không có lỗi cú pháp. Tên phương thức cũng ít có khả năng có lỗi, nhưng như chúng ta đã thấy trong các tác vụ khác, danh mục này luôn có sức hấp dẫn tối thiểu đối với các mô hình.

Thú vị là theo bảng, Định danh loại là danh mục hoàn toàn liên quan đến cú pháp của code và không bao gồm bất kỳ đặt tên nào, có đóng góp nhiều nhất cho CT với khoảng cách so với các danh mục khác. Nó có điểm 20.67% và 21.66% cho CodeBERT và GraphCodeBERT tương ứng trong khi điểm của nó trong các danh mục khác dưới 14%. Ngoài ra, đáng chú ý là GraphCodeBERT sử dụng dataflow của code để nắm bắt cấu trúc của nó; luôn có điểm cao hơn cho danh mục này so với CodeBERT.

Các mẫu tương tự xuất hiện hợp lệ cho Tên biến và cho CR. Điểm của danh mục này cho CR là 17.85% và 18.93%, trong khi trong các tác vụ khác điểm luôn dưới 12%.

Tương tự, danh mục Tên phương thức có điểm cao hơn hẳn trong CDG. Đối với tác vụ này trong python, danh mục này có điểm 39.44% và 41.04%, và trong Java, nó có 46.17% và 54.21% cho CodeBERT và GraphCodeBERT, tương ứng.

Với tất cả những quan sát này, chúng ta có thể thấy một mẫu quan trọng khi so sánh các tác vụ khác nhau với nhau. Tên phương thức và biến đầu vào (về cơ bản là dòng đầu tiên của các mẫu code) là các danh mục quan trọng nhất cho CDG; Lời gọi phương thức và biến cục bộ đóng vai trò quan trọng nhất trong cải tiến code, cùng với tên phương thức với tầm quan trọng thấp hơn một chút. Mặt khác, dịch code quan tâm đến định danh loại và từ khóa ngôn ngữ, hơn bất kỳ tác vụ nào khác, trong khi vẫn quan tâm đến một số danh mục đặt tên, cũng như vậy. Trong Bảng III, chúng tôi đã tổng hợp các số cho hai danh mục chính của chúng tôi và chúng ta có thể thấy một mẫu mà chúng tôi mong đợi. Các token đặt tên quan trọng cho tất cả các tác vụ, nhưng ít hơn cho dịch code thay vào đó quan tâm nhiều hơn đến các token cấu trúc so với các tác vụ khác.

### B. (RQ2) Khi nào các mô hình code được pre-train không hoạt động?

**Phương pháp**. Để cung cấp giải thích về khi nào CodeBERT và GraphCodeBert hoạt động tốt và khi nào chúng thất bại, trong RQ này, chúng tôi bắt đầu bằng một phân tích định tính của một số dự đoán mẫu. Sau đó chúng tôi đưa ra giả thuyết dựa trên các quan sát của mình và cuối cùng xác minh chúng một cách định lượng trên toàn bộ tập dữ liệu. Để định nghĩa hiệu suất mạnh và yếu của các mô hình, chúng tôi không thể đơn giản dựa vào các giá trị tuyệt đối của chỉ số đánh giá (BLEU). Vì độ lớn của điểm BLEU một phần phụ thuộc vào mức độ khó hoặc dễ của tác vụ tạo tài liệu, cho mỗi mã mẫu. Do đó, chúng tôi cần bằng cách nào đó đo lường mức độ khó của tác vụ tạo tài liệu, cho một mã nguồn.

Để giải quyết câu hỏi nghiên cứu này, chúng tôi tạo ra các chỉ số để đo lường độ phức tạp mẫu và đánh giá hiệu suất mô hình tương ứng.

Trong bối cảnh CR và CT, nơi việc sao chép token chiếm ưu thế, Khoảng cách Levenshtein (LD) giữa đầu vào và đầu ra mong đợi ('đầu ra vàng') phục vụ như một chỉ số độ phức tạp phù hợp. Bằng cách tính toán LD cho tất cả các mẫu tập dữ liệu, chúng tôi xác định một phần ba dễ nhất dựa trên LD thấp hơn.

Đối với tác vụ CDG, mặc dù có sự khác biệt đầu vào-đầu ra trong ngôn ngữ lập trình (PL) và ngôn ngữ tự nhiên (NL), phương pháp của chúng tôi vẫn tương tự. Chúng tôi xác định độ khó bằng cách giao nhau các token được tiền xử lý trong tài liệu đầu ra vàng cho mỗi phương thức với các token trong mã nguồn của phương thức.

Để tìm sự chồng lấp giữa mã nguồn và các token đầu ra, chúng tôi thực hiện các bước tiền xử lý này: Đầu tiên, chúng tôi loại bỏ dấu chấm câu và các token ngắn hơn ba ký tự, trong tài liệu đầu ra. Sau đó, lemmatize các token đó bằng lemmatizer Wordnet [36] tiêu chuẩn, được cung cấp trong gói NLTK. Tiếp theo, chúng tôi tokenize mã nguồn bằng parser cho ngôn ngữ tương ứng. Lưu ý rằng do tokenization của CodeBERT và GraphCodeBERT, có thể chia một từ có ý nghĩa thành nhiều token, chúng tôi không sử dụng tokenization của chúng cho phân tích này. Cuối cùng, chúng tôi tạo một tập các token không phân biệt chữ hoa chữ thường nằm ở giao điểm của các token đầu ra và mã nguồn đã xử lý.

Bây giờ một tác vụ tạo tài liệu dễ/khó là khi sự chồng lấp giữa hai tập cao/thấp. Do đó, giống như các ngưỡng cắt cho CR và CT, chúng tôi coi một phần ba đầu tiên các mẫu với sự chồng lấp cao nhất là dễ, và một phần ba các trường hợp với ít sự chồng lấp nhất là các vấn đề khó, và bỏ qua phần còn lại (mức độ khó trung bình).

Quá trình trên cho chúng tôi biết mẫu nào được coi là khó và mẫu nào được coi là dễ. Bây giờ chúng tôi cần đo lường hiệu suất của các mô hình. Để làm như vậy, chúng tôi sử dụng điểm BLEU vì nó là điểm được chấp nhận và đáng tin cậy nhất được áp dụng trong các tác vụ này trong tài liệu. Đối với độ chính xác, chúng tôi cũng lấy một phần ba các mẫu với điểm BLEU cao nhất là Cao và các mẫu với một phần ba điểm BLEU ít nhất là Thấp.

Với những định nghĩa này, sẽ có bốn danh mục (cho các tuple của <mức độ dễ, độ chính xác mô hình>) như dưới đây:

**Dễ−Cao**: Danh mục này chứa các mục dữ liệu kiểm tra là các vấn đề dễ (có nghĩa là sự tương tự cao giữa đầu vào và dữ liệu vàng) mà các mô hình đã đạt được điểm BLEU Cao trên chúng.

**Khó−Cao**: Danh mục này chứa các mẫu được gắn nhãn là Khó và Cao. Điều này có nghĩa là ngay cả với việc thiếu các token chung, mô hình đã có thể đạt được kết quả thỏa đáng trong những trường hợp này.

**Khó−Thấp**: Danh mục này bao gồm các trường hợp khó một lần nữa, và như mong đợi, kết thúc với độ chính xác Thấp cho dự đoán của mô hình.

**Dễ−Thấp**: Danh mục này là thú vị nhất trong bài báo này, vì nó có thể chỉ ra những điểm yếu tiềm năng của mô hình và rất phù hợp để được phân tích và "giải thích". Các mẫu trong nhóm này, nằm trong số các mẫu với sự chồng lấp cao hơn trong tập dữ liệu tương ứng của chúng có nghĩa là mô hình đang có một công việc khá dễ dàng để dự đoán. Tuy nhiên, điểm BLEU như chỉ số độ chính xác của mô hình đang cho thấy hiệu suất kém so với các mẫu khác.

Để thực hiện quan sát thủ công (nghiên cứu định tính) cho RQ này, sau khi nhóm tập dữ liệu kiểm tra của chúng tôi thành bốn danh mục này, chúng tôi chọn ngẫu nhiên 100 mẫu từ danh mục mục tiêu (Dễ−Thấp), và phân tích thủ công đầu ra và trọng số attention của chúng.

Đối với mỗi mẫu, chúng tôi ghi lại những phát hiện thú vị nhất để xác định các mẫu thường xuyên nhất. Bằng cách này chúng tôi phát triển một số giả thuyết. Cuối cùng, chúng tôi cố gắng xác minh các giả thuyết này bằng cách nghiên cứu định lượng toàn bộ tập dữ liệu kiểm tra, liên quan đến các giả thuyết. Đầu ra của giai đoạn định lượng này ở dạng một số thống kê mô tả để xác nhận hoặc bác bỏ các quan sát được thực hiện dựa trên 100 mẫu.

**Kết quả**. Với dữ liệu được chia theo các nhóm đã định nghĩa, Bảng V cho thấy tỷ lệ dân số danh mục mục tiêu so với toàn bộ tập dữ liệu cho mỗi tác vụ-mô hình. Tiếp theo, chúng tôi sẽ giải thích các quan sát từ phân tích thủ công.

**Quan sát 1: Các mô hình code được pre-train không hoạt động tốt, khi tài liệu đầu ra vàng dài.**

Quan sát đầu tiên của chúng tôi liên quan đến tác vụ CDG là trong các trường hợp với tài liệu vàng dài, điểm BLEU có xu hướng thấp! Chúng tôi vẽ biểu đồ phân bố của điểm BLEU, theo độ dài của tài liệu vàng, trong Hình 1. Theo biểu đồ, hầu hết điểm BLEU cao xảy ra khi độ dài của tài liệu vàng nhỏ hơn 50 ký tự.

Nói chung, ý tưởng của điểm BLEU là về việc đếm số n-gram chung giữa tham chiếu và đầu ra. Các tài liệu được tạo bởi mô hình thường ngắn nên đối với các câu dài hơn, có ít cơ hội hơn để mô hình chọn cùng cách diễn đạt và từ với cùng thứ tự. Một giải thích khả thi khác cho quan sát này là tài liệu dài hơn có nghĩa là phương thức thực hiện một tác vụ phức tạp hơn và do đó khó hơn cho mô hình để tạo ra tài liệu đúng cho phương thức phức tạp.

Một giải pháp tiềm năng cho vấn đề này là buộc mô hình tạo ra các chuỗi dài hơn làm đầu ra sẽ tăng cơ hội có điểm BLEU cao, trong các trường hợp với tài liệu tham chiếu dài. Nhưng rõ ràng, vì đây không thực sự là lỗi của mô hình và trong những trường hợp này, điểm BLEU thấp không nhất thiết chỉ ra một dự đoán xấu (như ví dụ được hiển thị trong Hình 2), cách tốt nhất để xử lý vấn đề này là xem xét các chỉ số đánh giá khác và lý tưởng là những chỉ số chủ quan hơn.

**Quan sát 2: Các mô hình code được pre-train không hoạt động tốt, khi mã nguồn đầu vào phức tạp.**

Một quan sát thú vị khác là về độ dài của mã nguồn. Kết quả cho thấy trong các trường hợp với code dài hơn, điểm BLEU thường thấp hơn. Chúng tôi bắt đầu phân tích ban đầu với dữ liệu CDG và kết quả, được tóm tắt trong Hình 3, cho thấy xu hướng giảm của điểm BLEU, bằng sự tăng của độ dài mã nguồn. Ví dụ, điểm BLEU trung bình cho các trường hợp ngắn hơn và dài hơn 300 token lần lượt là 0.161 và 0.149.

Có hai giải thích cho quan sát này: (1) độ dài cố định của đầu vào trong các mô hình, về cơ bản có nghĩa là nếu độ dài của mã nguồn cao hơn một giá trị cố định (trong các thí nghiệm của chúng tôi, 256 token), thì đầu vào sẽ bị cắt ngắn. Điều này có nghĩa là một số token sẽ không đến được decoder, và do đó chúng ta có một suy giảm tiềm năng trong điểm cuối cùng. (2) độ phức tạp tăng của code. Tương tự như Quan sát 1, mã nguồn dài hơn có nghĩa là logic phức tạp hơn, nhiều đối tượng hơn, và chức năng cần xem xét cho mô hình có lẽ dẫn đến kết quả kém hơn.

Theo những lý do này, hai giải pháp cơ bản có thể được đề xuất: (a) tăng ngưỡng đầu vào và (b) giảm độ dài của đầu vào. Ngưỡng đầu vào có thể dễ dàng được sửa đổi trong quá trình huấn luyện của mô hình và chỉ yêu cầu nhiều tài nguyên hơn. Tùy chọn thứ hai, tuy nhiên, là một giải pháp phức tạp hơn đã được thực hiện một cách ngây thơ bằng việc cắt ngắn. Một lựa chọn tiềm năng khác là refactor các phương thức dài thành nhiều phương thức nhỏ hơn, sau đó chuyển mỗi phương thức cho các mô hình để tạo tài liệu, và cuối cùng hợp nhất tất cả tài liệu đầu ra thành một tài liệu.

Tiếp theo, để mở rộng quan sát này trên tất cả các tác vụ-mô hình, và tìm thêm kết quả thống kê, chúng tôi đã sử dụng một số chỉ số độ phức tạp code phổ biến và tiến hành phân tích trên tất cả 8 mô hình-tác vụ để hiểu rõ hơn nguyên nhân gốc rễ của kết quả kém cho các đoạn code dài. Chúng tôi chọn 'số token', 'độ phức tạp cyclomatic', 'độ sâu khối lồng nhau', 'số biến' làm chỉ số độ phức tạp. Để đo lường độ khó của tác vụ, chúng tôi cũng bao gồm cùng 'khoảng cách Levenshtein' cho CT và CR và 'sự chồng lấp' cho CDG, trong phân tích của chúng tôi.

Đối với mỗi tác vụ-mô hình, chúng tôi có năm chỉ số khác nhau để nghiên cứu, vì vậy chúng tôi có một biểu đồ cho mỗi chỉ số. Trong mỗi biểu đồ, phân bố của các mẫu trong tập dữ liệu tương ứng, liên quan đến chỉ số đó được hiển thị bằng các thanh màu xanh, và cùng phân bố nhưng chỉ cho danh mục mục tiêu (Dễ-Thấp) được hiển thị màu đỏ. Với việc trực quan hóa này, chúng ta có thể xác định bất kỳ sự khác biệt nào về xu hướng trên một chỉ số cụ thể trong danh mục mục tiêu so với toàn bộ tập dữ liệu.

Hình 4 cho thấy kết quả cho cải tiến code (CR) cho CodeBERT cho số token và số biến (tất cả các biểu đồ cho GraphCodeBERT và các chỉ số khác cho CodeBERT có thể được tìm thấy trong repo công khai). Như được minh họa trong các biểu đồ, danh mục Dễ-Thấp có phân bố rất tương tự với toàn bộ tập dữ liệu, ngoại trừ xu hướng tăng nhẹ cho một số chỉ số được báo cáo như số token và số biến. Điều này có nghĩa là mô hình có xu hướng đưa ra quyết định xấu, bất cứ khi nào mã nguồn trở nên phức tạp hơn về số token và biến, ngay cả khi sự chồng lấp của các token cao. Ví dụ, xem xét số token, như một thước đo độ phức tạp code, tỷ lệ các mẫu với hơn 100 token chủ yếu cao hơn trong danh mục Dễ-Thấp so với chia sẻ của cùng các mẫu trong tổng. Điều đó có nghĩa là các mẫu với nhiều token có nhiều khả năng được giả định "Dễ" trong các danh mục của chúng tôi (nhiều sự chồng lấp hơn giữa đầu vào và đầu ra) nhưng thực tế, chúng khó hơn cho mô hình để hiểu (cho độ dài dài của đoạn code).

Hình 5 cho thấy kết quả cho dịch code (CT) cho số token, độ sâu khối lồng nhau, và độ phức tạp cyclomatic cho CodeBERT (tất cả các biểu đồ cho GraphCodeBERT và các chỉ số khác cho CodeBERT có thể được tìm thấy trong repo công khai). Đối với tác vụ này, số biến tuân theo cùng mẫu như tác vụ CR, đó là danh mục Dễ-Thấp khó hơn dựa trên những chỉ số đó. Mặt khác, xem xét số token, độ sâu khối lồng nhau, và thậm chí độ phức tạp cyclomatic, có một kết nối đảo ngược. Nói cách khác, các mẫu có giá trị nhỏ hơn của những chỉ số này, có mật độ cao hơn trong danh mục Dễ-Thấp. Ví dụ, trong cả hai mô hình, các mẫu với token ít hơn 20, là khoảng 50% của dân số danh mục mục tiêu, mặc dù chúng chiếm một phần rất nhỏ của tổng tập dữ liệu. Một giải thích khả thi là khi mã nguồn quá ngắn (số token rất nhỏ và rất ít khối lồng nhau), mô hình không thể dịch nó đúng cách, do thiếu đủ thông tin/ngữ cảnh. Một giải thích khác là thực tế rằng khó hơn để duy trì điểm BLEU cao khi đầu vào rất ngắn.

Hình 6 và Hình 7 minh họa một số kết quả tương tự cho tạo tài liệu code (CDG) cho CodeBERT (tất cả các biểu đồ cho GraphCodeBERT và các chỉ số khác cho CodeBERT có thể được tìm thấy trong repo công khai). Trong tác vụ downstream này, chúng ta có thể thấy các mẫu phụ thuộc nhiều hơn vào ngôn ngữ, thay vì mô hình. Trong tất cả các trường hợp, số biến, số token, và độ sâu khối lồng nhau tuân theo cùng mẫu chung.

Trong những thí nghiệm này, chúng ta thấy rằng cả hai mô hình đều gặp khó khăn với các mẫu có độ phức tạp thấp trong Java. Tuy nhiên, chúng có vấn đề tìm ra các mẫu phức tạp hơn trong Python, cũng như vậy. Ví dụ trong Python, các mẫu với hơn 80 token hoặc 8 biến, luôn có mật độ cao hơn trong danh mục Dễ-Thấp so với tất cả tập dữ liệu. Cuối cùng, thú vị là xem xét độ phức tạp cyclomatic, cả hai mô hình trong cả hai ngôn ngữ đều gặp khó khăn với các mẫu có độ phức tạp cao hơn.

Vậy tổng hợp lại, người ta có thể kết luận rằng các mô hình code hoạt động kém trên các đoạn code với giá trị cực đoan của các chỉ số liên quan đến độ phức tạp theo cả hai hướng (tức là cả code dài với nhiều khối lồng nhau và token và cũng code rất ngắn với chỉ một vài biến và token).

**Quan sát 3: Các mô hình code được pre-train không hoạt động tốt, khi các mô hình không tập trung vào các danh mục quan trọng:**

Cuối cùng, chúng tôi phân tích đóng góp của các danh mục token tương tự như RQ1, nhưng cụ thể cho danh mục mục tiêu (Dễ−Thấp). Trong Bảng VI chúng tôi có điểm chuẩn hóa của hai danh mục chính cho các mẫu mục tiêu. Chúng tôi quan tâm đến việc so sánh kết quả cho danh mục này và kết quả trước đó cho toàn bộ tập dữ liệu kiểm tra. Do đó, chúng tôi tính toán sự khác biệt giữa hai điều này từ Bảng VI và Bảng III, và kết quả được hiển thị trong Bảng VII. Các số âm trong bảng này chỉ ra sự giảm trong danh mục mục tiêu.

Như kết quả cho thấy, có sự giảm đáng kể trong điểm cho danh mục Cấu trúc trong CT. Trong khi trả lời RQ1, chúng tôi cho thấy rằng tác vụ này chủ yếu dựa vào nhóm token này IV-A. Tương tự, chúng tôi có sự giảm nhẹ hơn trong điểm của danh mục Đặt tên trong CDG trong khi danh mục đặt tên cũng chứng minh là danh mục quan trọng hơn cho CDG.

Cả hai manh mối này, dẫn chúng tôi đến kết luận rằng kết quả kém thỏa đáng hơn, bất cứ khi nào mô hình không chú ý đủ đến danh mục token quan trọng tương ứng cho một tác vụ cụ thể. Dựa trên quan sát này, các câu hỏi nghiên cứu tiềm năng để điều tra trong tương lai là: "Liệu mô hình sẽ hoạt động tốt hơn nếu chúng ta giúp nó bằng cách gắn thẻ các loại token? Việc khuếch đại thủ công điểm attention của các danh mục cụ thể theo tác vụ có thể có lợi cho các mô hình code".

## V. HẠN CHẾ

Một hạn chế của nghiên cứu này là các thí nghiệm của chúng tôi chỉ trên code Java và Python. Việc bao gồm các tập dữ liệu khác (cũng như code Python cho các tác vụ CT và CR) yêu cầu rất nhiều tiền xử lý để trở nên nhất quán với thiết kế và yêu cầu của chúng tôi và sẽ vượt quá phạm vi và kích thước của một bài báo hội nghị. Chúng tôi dự định mở rộng những phân tích này sang các ngôn ngữ khác trong tương lai.

Một hạn chế khác là chúng tôi đã sử dụng điểm BLEU làm chỉ số đánh giá cho độ chính xác của mô hình, thường được sử dụng trong các tác vụ downstream tạo tài liệu để giảm tính chủ quan của kết quả. Tuy nhiên, như chúng tôi đã đề cập trong bài báo, nó không phải là một chỉ số toàn diện vì nó không thể tìm thấy các cách diễn đạt lại hoặc các trường hợp mà dự đoán không sai, nhưng không khớp chính xác với nhãn vàng.

Các quan sát chúng tôi đưa ra cũng bị giới hạn bởi các mẫu chính mà chúng tôi đã quan sát trong 100 mẫu, một cách thủ công. Mặc dù sau đó chúng tôi xác nhận chúng một cách định lượng, tất nhiên có thể có một số giải thích khác cũng như vậy mà chúng tôi đã bỏ lỡ quan sát, do các mẫu chúng tôi đã chọn.

Ngoài ra, nghiên cứu chỉ giới hạn ở ba tác vụ downstream (CT, CDG, và CR) và hai mô hình code (CodeBERT và GraphCodeBERT). Cần thêm công việc để tổng quát hóa các phát hiện cho các mô hình dựa trên Transformer khác, trong tương lai.

Đáng chú ý là việc thực hiện tác vụ CDG trong framework GraphCodeBERT, được thực hiện sử dụng các cài đặt và siêu tham số giống hệt như các tác vụ và mô hình khác. Phương pháp đồng nhất này có thể đóng góp vào độ chính xác thấp hơn được quan sát trong hiệu suất của tác vụ CDG trong mô hình.

Cuối cùng, nghiên cứu này được tiến hành trước khi ChatGPT được công bố. Vì vậy một phần mở rộng rất liên quan của công việc này sẽ là bao gồm mô hình GPT-4 cả như một mô hình code cũng như một phương pháp XAI để cung cấp giải thích về các quyết định của chính nó và các mô hình khác.

## VI. KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI

Bài báo này đề xuất một phương pháp để giải thích các mô hình code được pre-train (ví dụ: CodeBert và GraphCodeBert), sử dụng cơ chế attention end-to-end nội bộ của chúng, như phương pháp XAI. Không giống như hầu hết nghiên cứu XAI, nơi giải thích được áp dụng cho các mô hình có độ chính xác cao để đảm bảo kết quả đáng tin cậy, chúng tôi đã sử dụng XAI trên cả kịch bản độ chính xác cao (để tìm hiểu các mô hình học được gì) và thấp (để xem khi nào chúng không hoạt động tốt). Các phát hiện của chúng tôi không chỉ cung cấp các quan sát về những gì các mô hình dựa trên Transformer tiên tiến này học theo các danh mục loại token và tại sao chúng hoạt động kém trong một số kịch bản, mà còn đề xuất các khuyến nghị có thể hành động, như sử dụng các chỉ số đánh giá chủ quan hơn cho tác vụ CDG, cung cấp các loại token làm đầu vào bổ sung cho mô hình, và khuếch đại thủ công điểm attention cho các loại token cụ thể. Trong tương lai, chúng tôi dự định mở rộng công việc này bằng cách kiểm tra các tác vụ downstream, mô hình và phương pháp XAI khác. Ngoài ra, chúng tôi cũng dự định làm việc trên các mô hình code được pre-train bằng cách thực hiện các khuyến nghị được đề xuất từ các quan sát của chúng tôi. Cuối cùng, chúng tôi dự định sử dụng mô hình GPT-4 để cả truy cập kết quả của nó như mô hình code và sử dụng nó như một kỹ thuật XAI để giải thích tại sao một đầu ra cụ thể được cung cấp bởi mô hình.

## TÀI LIỆU THAM KHẢO

[1] G. Shobha, A. Rana, V. Kansal, và S. Tanwar, "Code clone detection—a systematic review," Emerging Technologies in Data Mining and Information Security, pp. 645–655, 2021.

[2] C. Pornprasit, C. Tantithamthavorn, J. Jiarpakdee, M. Fu, và P. Thongtanunam, "Pyexplainer: Explaining the predictions of just-in-time defect models," trong 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2021, pp. 407–418.

[3] J. Humphreys và H. K. Dam, "An explainable deep model for defect prediction," trong 2019 IEEE/ACM 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering (RAISE). IEEE, 2019, pp. 49–55.

[4] A. LeClair, S. Haque, L. Wu, và C. McMillan, "Improved code summarization via a graph neural network," trong Proceedings of the 28th international conference on program comprehension, 2020, pp. 184–195.

[5] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang et al., "Codebert: A pre-trained model for programming and natural languages," trong Findings of the Association for Computational Linguistics: EMNLP 2020, 2020, pp. 1536–1547.

[6] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan, A. Svyatkovskiy, S. Fu et al., "Graphcodebert: Pre-training code representations with data flow," arXiv preprint arXiv:2009.08366, 2020.

[7] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement, D. Drain, D. Jiang, D. Tang et al., "Codexglue: A machine learning benchmark dataset for code understanding and generation," arXiv preprint arXiv:2102.04664, 2021.

[8] Y. Wang, W. Wang, S. Joty, và S. C. Hoi, "Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation," arXiv preprint arXiv:2109.00859, 2021.

[9] C. Tantithamthavorn, J. Jiarpakdee, và J. Grundy, "Actionable analytics: Stop telling me what it is; please tell me what to do," IEEE Software, vol. 38, no. 4, pp. 115–120, 2021.

[10] C. K. Tantithamthavorn và J. Jiarpakdee, "Explainable ai for software engineering," trong 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2021, pp. 1–2.

[11] J. Jiarpakdee, C. K. Tantithamthavorn, và J. Grundy, "Practitioners' perceptions of the goals and visual explanations of defect prediction models," trong 2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR). IEEE, 2021, pp. 432–443.

[12] J. Jiarpakdee, C. K. Tantithamthavorn, H. K. Dam, và J. Grundy, "An empirical study of model-agnostic techniques for defect prediction models," IEEE Transactions on Software Engineering, vol. 48, no. 1, pp. 166–185, 2020.

[13] D. Rajapaksha, C. Tantithamthavorn, J. Jiarpakdee, C. Bergmeir, J. Grundy, và W. Buntine, "Sqaplanner: Generating data-informed software quality improvement plans," IEEE Transactions on Software Engineering, vol. 48, no. 8, pp. 2814–2835, 2021.

[14] S. Wattanakriengkrai, P. Thongtanunam, C. Tantithamthavorn, H. Hata, và K. Matsumoto, "Predicting defective lines using a model-agnostic technique," IEEE Transactions on Software Engineering, vol. 48, no. 5, pp. 1480–1496, 2020.

[15] C. Pornprasit và C. K. Tantithamthavorn, "Jitline: A simpler, better, faster, finer-grained just-in-time defect prediction," trong 2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR). IEEE, 2021, pp. 369–379.

[16] C. Khanan, W. Luewichana, K. Pruktharathikoon, J. Jiarpakdee, C. Tantithamthavorn, M. Choetkiertikul, C. Ragkhitwetsagul, và T. Sunetnanta, "Jitbot: An explainable just-in-time defect prediction bot," trong 2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2020, pp. 1336–1339.

[17] J. Devlin, M.-W. Chang, K. Lee, và K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018.

[18] C. Pan, M. Lu, và B. Xu, "An empirical study on software defect prediction using codebert model," Applied Sciences, vol. 11, no. 11, p. 4793, 2021.

[19] K. Cao, C. Chen, S. Baltes, C. Treude, và X. Chen, "Automated query reformulation for efficient search based on query logs from stack overflow," trong 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE, 2021, pp. 1273–1285.

[20] X. Gu, H. Zhang, và S. Kim, "Deep code search," trong 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE). IEEE, 2018, pp. 933–944.

[21] T. Nguyen, P. C. Rigby, A. T. Nguyen, M. Karanfil, và T. N. Nguyen, "T2api: Synthesizing api code usage templates from english texts with statistical translation," trong Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering, 2016, pp. 1013–1017.

[22] S. Haque, A. LeClair, L. Wu, và C. McMillan, "Improved automatic summarization of subroutines via attention to file context," trong Proceedings of the 17th International Conference on Mining Software Repositories, 2020, pp. 300–310.

[23] S. Jiang, A. Armaly, và C. McMillan, "Automatically generating commit messages from diffs using neural machine translation," trong 2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2017, pp. 135–146.

[24] L. Liu, X. Hu, W. Song, R. Fu, T. Liu, và G. Hu, "Neural multitask learning for simile recognition," trong Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018, pp. 1543–1553.

[25] N. Jiang, T. Lutellier, và L. Tan, "Cure: Code-aware neural machine translation for automatic program repair," trong 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE, 2021, pp. 1161–1173.

[26] Y. Li, S. Wang, và T. N. Nguyen, "Dlfix: Context-based code transformation learning for automated program repair," trong Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, 2020, pp. 602–614.

[27] Z. Chen, S. Kommrusch, M. Tufano, L.-N. Pouchet, D. Poshyvanyk, và M. Monperrus, "Sequencer: Sequence-to-sequence learning for end-to-end program repair," IEEE Transactions on Software Engineering, vol. 47, no. 9, pp. 1943–1959, 2019.

[28] B. Roziere, M.-A. Lachaux, L. Chanussot, và G. Lample, "Unsupervised translation of programming languages," Advances in Neural Information Processing Systems, vol. 33, pp. 20 601–20 611, 2020.

[29] A. Svyatkovskiy, S. K. Deng, S. Fu, và N. Sundaresan, "Intellicode compose: Code generation using transformer," trong Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2020, pp. 1433–1443.

[30] R. Tufano, S. Masiero, A. Mastropaolo, L. Pascarella, D. Poshyvanyk, và G. Bavota, "Using pre-trained models to boost code review automation," trong Proceedings of the 44th International Conference on Software Engineering, 2022, pp. 2291–2302.

[31] P. Thongtanunam, C. Pornprasit, và C. Tantithamthavorn, "Autotransform: Automated code transformation to support modern code review process," trong Proceedings of the 44th International Conference on Software Engineering, 2022, pp. 237–248.

[32] Y. Liu, C. Tantithamthavorn, Y. Liu, P. Thongtanunam, và L. Li, "Autoupdate: Automatically recommend code updates for android apps," arXiv preprint arXiv:2209.07048, 2022.

[33] X. Hu, G. Li, X. Xia, D. Lo, và Z. Jin, "Deep code comment generation," trong 2018 IEEE/ACM 26th International Conference on Program Comprehension (ICPC). IEEE, 2018, pp. 200–20 010.

[34] K. Papineni, S. Roukos, T. Ward, và W.-J. Zhu, "Bleu: a method for automatic evaluation of machine translation," trong Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, pp. 311–318.

[35] C.-Y. Lin và F. J. Och, "Orange: a method for evaluating automatic evaluation metrics for machine translation," trong COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics, 2004, pp. 501–507.

[36] E. Loper và S. Bird, "Nltk: The natural language toolkit," CoRR, vol. cs.CL/0205028, 2002. [Trực tuyến]. Có sẵn: http://dblp.uni-trier.de/db/journals/corr/corr0205.html#cs-CL-0205028
