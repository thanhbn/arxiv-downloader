# 2305.15775.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/interpretability/2305.15775.pdf
# File size: 43544320 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Concept-Centric Transformers:
Enhancing Model Interpretability through Object-Centric Concept Learning
within a Shared Global Workspace
Jinyung Hong1, Keun Hee Park1, and Theodore P. Pavlic1, 2
1School of Computing and Augmented Intelligence
2School of Life Sciences
Arizona State University
Tempe, AZ 85281
{jhong53, kpark53, tpavlic }@asu.edu
Abstract
Many interpretable AI approaches have been proposed
to provide plausible explanations for a model’s decision-
making. However, configuring an explainable model that ef-
fectively communicates among computational modules has
received less attention. A recently proposed shared global
workspace theory showed that networks of distributed mod-
ules can benefit from sharing information with a bottle-
necked memory because the communication constraints en-
courage specialization, compositionality, and synchroniza-
tion among the modules. Inspired by this, we propose
Concept-Centric Transformers, a simple yet effective con-
figuration of the shared global workspace for interpretabil-
ity, consisting of: i) an object-centric-based memory mod-
ule for extracting semantic concepts from input features,
ii) a cross-attention mechanism between the learned con-
cept and input embeddings, and iii) standard classification
and explanation losses to allow human analysts to directly
assess an explanation for the model’s classification reason-
ing. We test our approach against other existing concept-
based methods on classification tasks for various datasets,
including CIFAR100, CUB-200-2011, and ImageNet, and
we show that our model achieves better classification accu-
racy than all baselines across all problems but also gener-
ates more consistent concept-based explanations of classi-
fication output.
1. Introduction
Although state-of-the-art machine-learning models have
achieved remarkable performance across a wide range of
applications, their intrinsic lack of transparency due to their
many degrees of training freedom limits their usage insafety-critical areas—such as medical diagnostics, health-
care, public infrastructure safety, and visual inspection
for civil engineering—where trustworthy domain-specific
knowledge is crucial for decision making. Recently, sev-
eral developed methods provide post hoc explanations that
identify relevant features that a trained model uses to make
predictions [58, 64, 70, 78], but these are commonly crit-
icized for focusing only on low-level features [1, 43, 44,
79]. In contrast, intrinsically interpretable models [67]
have been proposed to make decisions based on human-
understandable “concepts,” the foundation of domain exper-
tise [1,6,12,13,25,32,42,43,45,50,65,91,94]. The gap be-
tween post hoc explainability and intrinsically interpretable
models is also discussed in the NLP community concern-
ing interpreting attention mechanisms [3]. In particular, the
debate over what degree of interpretability can be ascribed
to attention weights over input tokens still needs to be set-
tled to help meet the need for interpretability of attention
mechanisms [1, 9, 40, 71, 86].
Ideally, an intrinsically interpretable model will generate
explanations that are compositions of individually meaning-
ful modules. Modular explanations may improve the hu-
man understanding, and natural neuronal systems that con-
tinue to inspire AI development are often described as hav-
ing modular architectures themselves [4, 5, 8, 66]. Thus,
structuring algorithms to promote learning of modular la-
tent structures may also lead to better overall performance.
Motivated by improving modularity in interpretable mod-
els, we propose the Concept-Centric Transformer (CCT) , a
framework of intrinsically interpretable models inspired by
the Shared Global Workspace (SGW) [30], a new concep-
tual framework meant to generally encourage modularity by
forcing parallel specialized components to compete for bot-
tlenecked access to a shared memory. The configuration of
CCT allows trained models to have simple, modular struc-arXiv:2305.15775v3  [cs.LG]  7 Nov 2023

--- PAGE 2 ---
tures that can extract semantic concepts with or without the
guidance of ground-truth explanations of the concepts.
In what follows, we frame our CCT as a novel exten-
sion of the SGW concept to interpretable model develop-
ment, and we describe how CCT is implemented using three
key components: i) Concept-Slot-Attention (CSA) mod-
ulethat interfaces with image embedding from a backbone
model and produces a set of task-dependent embeddings for
concepts, ii) Cross-Attention (CA) module that generates
classification outputs using cross-attention between input
features and the CSA module’s embeddings, and iii) spe-
cialized loss penalties, including Explanation Loss , when
expert’s knowledge can be leveraged, and Sparsity Loss ,
an entropy-based loss to enforce the sparsity to determine
the importance of features during training. The CCT archi-
tecture is designed to augment existing deep-learning back-
bones to add explainability to them. Consequently, we val-
idate our approach on three image benchmark datasets—
CIFAR100 Super-class [23], CUB-200-2011 [83], and Ima-
geNet [19]—combining it with various deep-learning back-
bones, such as Vision Transformer (ViT) [22], Swin Trans-
former (SwinT) [55], and ConvNeXt [56].
2. Related Work
Significant advances have recently been made in devis-
ing explainable and interpretable models to measure the im-
portance of individual features for predictive output. The
post hoc analysis is one general approach to analyzing a
trained model by matching explanations to classification
outputs [1, 58, 64]. For example, activation maximiza-
tion [62, 81, 92] and saliency visualization [70, 78, 80] are
well-known methods for CNNs. Attention-based inter-
pretable approaches have also been introduced to identify
the most relevant parts of the input that the network focuses
on when making a decision [24, 26, 27, 37, 96–99].
In addition, designing methods that explain predictions
with high-level, human-understandable concepts [6, 12, 13,
25, 32, 42, 43, 45, 50, 65, 89, 91, 94] is one of the recent ad-
vancements in the field of interpretability. These intrinsi-
cally interpretable methods focus on identifying common
activation patterns in the nodes of the last layer of the neu-
ral network corresponding to human-understandable cate-
gories or constraining the network to learn such concepts.
Among them, our work is most similar to Concept Trans-
formers (CTs) [65], a framework that learns high-level con-
cepts defined with a set of related dimensions. Those con-
cepts, which can be part-specific or global, typically can
boost the performance of the learning task while offering
explainability at no additional cost to the network. How-
ever, that approach relies on extracting concepts based on
provided image patches even though each image patch may
be an unreliable predictor of high-level concepts. Our
CCT formulation generalizes this approach beyond imagepatches.
Historically, it has been argued that it is better to build an
intelligent system from many interacting specialized mod-
ules rather than a single “monolithic” entity to deal with a
broad spectrum of conditions and tasks [29, 59, 66]. Thus,
there has been significant effort on synchronization be-
tween computationally specialized modules via a shared
global workspace [16,31,39,60,69]. Furthermore, work on
the integration of modular computational architectures with
working memories takes inspiration from biology, neuro-
science, and cognitive science [35, 36, 63, 87]. The recently
proposed shared global workspace [30] shows how to uti-
lize the attention mechanism to encourage the most helpful
information to be shared among neural modules in modern
AI frameworks. This approach is the inspiration for our
use of an explicit working memory in the CCT to improve
the generalization of Transformer- and object-centric-based
models in the context of explainable models.
3. Preliminary
The Shared Global Workspace in AI Models. In-
spired by the Global Workspace Theory (GWT) from
cognitive science [2, 17, 18, 72–75], the Shared Global
Workspace (SGW) [30] explores how GWT can be mani-
fested in modern AI models to possess communication and
coordination schemes where several sparsely communicat-
ingspecialists (specific computing modules dealing with
the input) interact via a shared workspace (a shared working
memory module). To do so, the transformer and slot-based
methods were extended by adding a shared workspace and
allowing the modules to compete for write access in each
computational stage (Fig. 1). Replacing pairwise commu-
nications among the modules with interaction facilitated by
the shared workspace allows for: i) higher-order interaction
among the modules, ii) dynamic filtering due to the mem-
ory persistence, and iii) computational sophistication of us-
ing shared workspace for synchronizing different special-
ists. Motivated by the SGW, we aim to discover an efficient
configuration for intrinsically interpretable AI frameworks
and propose a simple but efficient way of interacting be-
tween a shared working space and specialists to improve
interpretability and performance.
Variants of Slot-Attention. Due to its simple yet effec-
tive design, Slot-Attention (SA) [57] has gained significant
attention in unsupervised object-centric learning to mimic
the development of symbolic understanding in human cog-
nition. The iterative attention mechanism allows SA to learn
and compete between slots for explaining parts of the input,
showing a soft clustering effect on visual inputs [57]. How-
ever, as revealed by recent studies, the vanilla SA module
as innately limited in that: i) the random initialization for
slots hampers addressing object-binding in input and ii) it

--- PAGE 3 ---
Figure 1. The shared global workspace [30] emerges from three
steps: 1) A collection of computational modules (or specialists )
perform standard processing, and a subset of the specialists be-
comes active at a particular computational stage depending upon
the input; 2) The active specialist writes information in a shared
working memory module (or shared workspace ); 3) The updated
contents of the workspace are broadcast to all specialists. We
explore how these steps can be used to add explainability in AI
frameworks. The generic figure above is inspired by [30, Fig. 1].
heavily depends on hyperparameter tuning so that it cannot
generally be applicable in many domains. Thus, some vari-
ants of SA, including I-SA [10] and BO-QSA [41], have
been proposed recently to address those issues1.
There are several existing examples on leveraging slot-
based methods in explainable models to extract semantic
concepts [49, 84]. However, few studies have emphasized
the perspective of modular architecture to foster commu-
nication between the slot-based model and other modules.
We leverage the three SA variants above as the shared
workspace of the SGW and explore how to encourage the
interactions among them to achieve better interpretability
and performance.
4. Concept-Centric Transformers
For supervised classification tasks, we introduce
Concept-Centric Transformers (CCTs), an instantiation of
the SGW for configuring an intrinsically interpretable
model, and we will describe the connection between the
SGW steps (Fig. 1) and our formulation. Our model con-
sists of: i) Concept-Slot-Attention (CSA) module that acts
as a shared memory module and extracts the latent con-
cept embedding specific to each batch of input, ii) Cross-
Attention (CA) module for broadcasting between input em-
bedding and the extracted concept embedding from the
CSA module so that it produces classification output as well
as faithful and plausible concept-based explanations and en-
courages pairwise interactions among them, and iii) special-
ized losses, including Explanation Loss andSparsity Loss ,
1We omit details of their technical differences as they are out of scope.which is our information broadcast scheme to encourage in-
terpretability. The CCT architecture, summarized in Fig. 2,
is described in the following sections. Further details, in-
cluding limitations (Appendix D), are given in the appendix.
4.1. Between Specialists and the Shared Workspace
Following the general structure of the SGW, we lever-
agespecialists , which are the computational modules of our
backbone, and a shared workspace by utilizing slot-based
methods in our CSA module, including SA [57], I-SA [10],
and BO-QSA [41]. Because of the modularity in our for-
mulation, those three SA variants are interchangeable, and
we demonstrate the performance comparison among them
in our experiments. We use a conventional key–query–value
attention mechanism to implement the competition between
specialists to write into the workspace, similar to the SGW.
The CSA module encodes a set of Linput feature vectors E
into concept representations Sconcept, which we refer to as
concept slots .
Concept Binding Specific to Each Input Batch. With
the number of concepts C, the concept slots Sconcept∈
RC×dfirst perform competitive attention [57] on the input
features E∈RL×D. For this, we apply linear projection
qCSAon the concept slots to obtain the queries and projec-
tions kCSAandvCSAon the inputs to obtain the keys and the
values, all having the same size d2. Then, we perform a dot
product between the queries and keys to get the attention
matrix ACSA∈RC×L. InACSA, each entry ACSA
c,lis the at-
tention weight of concept slot cfor attending over the input
vector l. We normalize ACSAby applying softmax across
concept slots, i.e., along the axis C. This implements a form
of competition among slots for attending to each input l.
We then seek to group and aggregate the attended in-
puts and obtain the attention readout for each concept slot.
Intuitively, this represents how much the attended inputs
contribute to semantically representing each concept. For
this, we normalize the attention matrix ACSA∈RC×L
along the axis Land multiply it with the input values
vCSA(E)∈RL×d. This produces the attention readout
in the form of a matrix U∈RC×dwhere each row
uc∈Rdis the readout corresponding to concept slot c;
ACSA= softmax C(qCSA(Sconcept)·kCSA(E)⊤/√
d),ACSA
c,l=
ACSA
c,l/PL
l=1ACSA
c,l, and U=ACSA·vCSA(E). We use
the readout information obtained from concept binding
and update each concept slot. The aggregated updates U
are finally used to update the concept slots via a learned
recurrent function, for which we use a Gated Recurrent
Unit (GRU) [14] with dhidden units so that Sconcept=
GRU(Sconcept,U). The processes above form one refine-
ment iteration. The concept slots obtained from the last iter-
2For simplicity, embedding size dis shared equally in our method.

--- PAGE 4 ---
(a) Concept-Centric Transformers via a Shared Workspace
 (b) Interpretable Broadcast to Specialists
Figure 2. Overall architecture of Concept-Centric Transformers (CCTs). (a) The CCT (in a solid gray line) is a drop-in replacement for
the classifier head of any backbone architecture, such as a ViT or a CNN, and consists of two modules: (1) Concept-Slot-Attention module
and (2) Cross-Attention module. (b) The process of training uses losses to induce an interpretable broadcast scheme.
ation are considered final. The overall module is described
in Algorithm 1 in pseudo-code in Appendix B.
4.2. Broadcast Updated Memories to Specialists
At this stage of the SGW, each specialist must up-
date its status using information broadcast from the shared
workspace. We also leverage the cross-attention mechanism
(called the CA module) to make specialists queries (step 3
in Figs 1 and 2) and perform dot products between them
and the values from the updated concept slots to update the
state of each specialist. However, because we aim to config-
ure an interpretable model and perform classification tasks,
we modify the iterative process by combining it with our
desired downstream classification task: i) guided by expert
knowledge if ground-truth concept explanations are avail-
able, or ii) using only sparsity loss to enforce minimizing
the entropy of the broadcasting information.
A set of Linput feature vectors E∈RL×Dare re-
used with a linear projection qCAto attain the queries,
and projections kCAandvCAare applied to the extracted
concept slots with position embedding ˆSfrom the CSA
module. The resulting keys and values are used in
a cross-attention mechanism with the queries, and the
cross-attention then outputs an attention weight ACA=
softmax L
qCA(E)·kCA(ˆSconcept)⊤/√
d
∈RL×Cbe-
tween each patch–concept slot pair. The final output of the
CA module is the product obtained by multiplying the at-
tention map ACA, the values vCA(ˆSconcept)∈RC×d, and an
output matrix O∈Rd×ncthat projects onto the (unnormal-
ized) nclogits over the output classes and then averagingover input features3; that is, for i= 1, . . . , n c,
logiti=1
LPL
l=1ACA
l·vCA(ˆSconcept)·O:,i (1)
So, given an input xto the network, the conditional proba-
bility of output class i∈ {1, . . . , n c}is:
Pr(i|x) = softmax iPC
c=1βcγc(x)
(2)
withβccomponents βci:= (vCA(ˆSconcept)·O)c,iwhere
γc(x)are non-negative relevance scores that depend on x
through the averaged attention weights; that is, γc(x) =
(1/L)PL
l=1ACA
l,c. We can interpret the equations above
from the two following perspectives:
1) Faithful Concept-slot-based Explanations by Design.
The CA module output is a multinomial logistic regression
model over positive variables γc(x)that measures the con-
tribution of each concept slot. Faithfulness is the degree
to which explanation reflects the decision and aims to en-
sure that the explanations are indeed explaining model op-
eration [33,48]. As shown in [65], the result of the CA mod-
ule follows the linear relation between the value vectors and
the classification logits and comes from the design choices
of computing outputs from the value matrix vCA(ˆSconcept)
through the linear projection vCA(ˆSconcept)·Oand aggregat-
ing patch contributions by averaging. So, our CA module is
also guaranteed to be faithful by design by satisfying Propo-
sition 1 in [65] and the technical definitions of faithfulness
from [1].
3For simplicity, we describe a single-head attention model here; a
multi-head version [82] is available and is also used in our experiments.

--- PAGE 5 ---
2) Dynamic State Update for Specialists with Informa-
tion Broadcast. In the original definition of the SGW,
ACA·vCA(ˆSconcept)is the formal computation of the update
for specialists (step 3 from [30, Sec 2.1]). Instead of apply-
ing an additional iterative process of updating specialists,
Eqs 1 and 2 are to produce classification outputs using the
weight O. So, the attention mask ACAcan contain not only
information from the updated memory but also classifica-
tion error. Furthermore, by directly manipulating the mask
ACA, we finally define explanation loss and sparsity loss to
enhance the model’s explainability.
4.3. Training Objectives for Interpretability
Plausibility by Construction with Explanation Loss.
Plausibility refers to how convincing the interpretation is to
humans [9,33]. To provide plausible human-understandable
explanations, we leverage the idea of explicitly guiding the
attention heads to focus on concepts in the input based on
domain expertise that are important for correctly classify-
ing the input. Similar to [20], given a desired distribution of
attention Hprovided by domain knowledge, the attention
weights from the CA module of the CCT ACAare used as a
regularization term by adding an explanation cost to the ob-
jective function that is proportional to Lexpl=∥A−H∥2
F,
where ∥·∥is the Frobenius norm. The ground-truth expla-
nation Hcan indicate global (e.g., sub-class or dominant
attribute of a bird) or spatial/image-patch-level information
(e.g., eye color of a bird). Below, we demonstrate the effec-
tiveness of this loss in the experiments of CIFAR100 Super-
class and CUB-200-2011.
Sparsity Loss based on Entropy. The advantage of
richer, more informative labels can increase interpretabil-
ity, but this comes at the expense of additional annota-
tion effort. A methodology that can bypass this trade-off
would be particularly worthwhile. We can attain this ca-
pability through our configuration that involves interactions
between specialists and a shared workspace. We introduce
the sparsity loss based on minimizing the entropy of the at-
tention mask ACA;Lsparse =H(A) =H(a1, . . . , a |A|) =
(1/|A|)P
i−ai·log(ai). This loss can be used in the ex-
periments with/without the ground-truth explanations.
Final Loss. Thus, the final loss to train the model be-
comes L=Lcls+λexplLexpl+λsparseLsparse , where Lcls
denotes the conventional classification loss. Notice that the
constant λexpl≥0controls the relative contribution of the
explanation loss to the total loss so our model can be ap-
plied with ground-truth explanations ( λexpl>0) or without
them ( λexpl= 0). Finally, the constant λsparse handles the
intensity of sparsity loss. Our experiments demonstrate that
our model can perform well without using additional com-
plicated losses, such as contrastive or reconstruct losses.Model F.C. Acc. ( %)S.C. Acc. ( %)
Vanilla ResNet†NA 83.2±0.2
Vanilla ViT-T NA 86.2±0.3
Hierarchical Model†71.2±0.2 84.7±0.1
DL2†[23] 75.3±0.1 84.3±0.1
MultiplexNet†[34] 74.4±0.2 85.4±0.3
PIP-Net [61] NA 83.9±0.2
ProtoPFormer [89] NA 81.7±0.1
ProtoPool [68] NA 82.9±0.4
ProtoPNet [11] NA 82.3±0.1
Deform-ProtoPNet [21] NA 83.7±1.0
BotCL [84] NA 56.9±10
CT [65] 73.3±2.9 92.1±0.2
CCT: ViT-T+SA 80.3±0.4 92.6±0.1
CCT: ViT-T+I-SA 83.3±0.1 92.8±0.1
CCT: ViT-T+BO-QSA 83.4±0.1 93.0±0.1
Table 1. Test accuracy on fine-grained class (F.C.) and super-
class (S.C.) label prediction on CIFAR100. Notice that the clas-
sification of super-classes and fine-grained classes are performed
simultaneously and that this kind of experiment can be done in
deep learning with constraints, but our method and CT are only
among concept-based approaches. †indicates results from [34].
5. Experiments
We evaluate the performance of our CCT on three
distinct datasets: CIFAR100 Super-class [23], CUB-200-
2011 [83], and ImageNet [19]. Specific objectives guide the
selection of these datasets. Firstly, the CIFAR100 Super-
class is a testing ground to demonstrate our model’s excep-
tional capabilities under fully supervised conditions, where
complete global concept explanations are available. Sec-
ondly, CUB-200-2011 acts as an intermediary dataset, al-
lowing us to showcase our model’s prowess in both super-
vised and unsupervised explanation setups. Lastly, we chal-
lenge our CCT with the ImageNet dataset, which represents
a fully unsupervised scenario due to the absence of concept
explanations. Despite this hurdle, we adapt our model to
achieve remarkable performance even without any concept
explanations. This comprehensive setup underscores our
CCT’s adaptability and versatility across various use cases,
visual backbones, and data scenarios. Our experimental re-
sults are robustly validated through three different random
seeds and 95% confidence intervals, with additional details
provided in Appendix C.
5.1. Evaluation on CIFAR100 Super-class
The CIFAR100 Super-class dataset is a variant of the CI-
FAR100 [47] image dataset. It consists of 100 fine-grained
classes (F.C.) of images that are further grouped into 20

--- PAGE 6 ---
super-classes (S.C.). For instance, the five fine-grained
classes baby, boy, girl, man, andwoman belong to the super-
class people . Since the introduction of deep-learning mod-
els with logical constraints [23], this dataset has been used
as one of the benchmark datasets to assess the effectiveness
of embedding the constraints into neural networks (in-depth
surveys can be found in [15, 28]). Our study highlights that
the concepts within our CCT (and closely related CT [65])
can be understood as constraints , with differences outlined
in Appendix C.4. We leverage individual fine-grained im-
age classes as global concept explanations for a multi-class
prediction task with 20 super-classes, employing a Vision
Transformer-Tiny (ViT-T)4as the backbone5.
Following [34], our CCT’s performance is compared
against three baseline groups. The first group includes
vanilla backbone models like Wide ResNet 28-10 [93] and
ViT-T. The second group involves neural-network models
with logical constraints, including Hierarchical Model [34],
DL2 [23], and MultiplexNet [34]. The third group com-
prises concept-based explainable models; except for CT,
these models cannot handle both fine-grained and super-
class tasks concurrently. For our CCT’s CSA module, we
configure three SA variants—SA, I-SA, and BO-QSA—
with the backbone and evaluate fine-grained class accuracy
by comparing ground-truth concept explanations with top-1
predicted concepts based on the resulting attention scores.
Table 1 presents the experimental outcomes, showing
our CCT’s substantial outperformance of all baselines. It
significantly enhances ViT-T’s backbone performance and
achieves a remarkable increase in fine-grained class accu-
racy compared to CT. Notably, CT performs less effec-
tively than logic-constraint-based approaches, highlighting
our module’s superior role in shaping global concepts.
Figure 3 shows two examples that demonstrate where
our CCT’s concept learning excels both quantitatively and
qualitatively over CT. CT’s poor fine-grained class accu-
racy might be the result of making hallucinations to greed-
ily achieve super-class accuracy without forming sound la-
tent concepts. Though both models correctly classify super-
classes on the shown examples, the explanatory decision-
making processes perform entirely differently in the two
models. Although the ground-truth class of the left image
in Fig. 3 is boy, the best-matching class concept with the
highest attention score by CT was baby , which is the incor-
rect fine-grained class but also belongs to the correct super-
class people . In the right example image, we observe a sim-
ilar behavior of CT. In contrast, our CCT’s predicted con-
cepts for both examples correctly match their ground-truth
fine-grained classes with sparser concept attention scores
4For this experiment, Swin Transformer and ConvNeXt were not used
as a backbone for our model because the number of parameters of two
models (both SwinT-T and ConvNeXt-T are 28M) is larger than one of
ViT-T.
5For global concepts, we use the embedding of the CLS token as inputs.
Figure 3. Comparison of class predictions for CT [65] and CCT
(“Ours”) in examples where both make correct CIFAR100 super-
class predictions. The 100 classes are indexed from 1 (top left in
10x10 grid) to 100 (bottom right in 10x10 grid). ( Left) Ground-
truth class label is boy(12), but CT mispredicted as baby (3),
whereas CCT’s prediction is correct. ( Right ) Ground-truth label is
shark (84), but CT incorrectly selects ray(78) whereas CCT again
makes a correct class prediction.
than CT. Further details and results are described in Ap-
pendix C.4.
5.2. Evaluation on CUB-200-2011
The CUB-200-2011 [83] dataset comprises 11,788 bird
images categorized into 200 species. Each image is an-
notated with various discrete concepts, e.g., the shape of
the beak, or the color of the body, aiding species identi-
fication. The dataset involves 312 concepts distributed un-
evenly across images, so we utilize a pre-processing method
from [65] to address this. Additional results and details are
described in Appendix C.5.
With Concept Explanations. We consider a real-world
scenario where many-to-many and non-deterministic rela-
tionships between concepts and outputs exists, along with a
mix of global andspatially localized concepts. We use CSA
and CA modules within CCT to handle both global and
spatial concepts, averaging their logits for interpretability.
We use various backbones, including ViT [22], Swin Trans-
former (SwinT) [55], and ConvNeXt [56] for this dataset.
We use the embeddings of the tokenized image patches,
while as input to the CCT in charge of the concepts, we
use the embedding of the CLS token.
In Table 2, we compare our CCT with other methods
based on Multi-stage (i.e., complex training) and End-to-
end(i.e., training with backpropagation) training. All of
the configurations of our CCT achieve over 87% classifica-
tion accuracy, clearly outperforming other approaches. This
confirms that the overall configuration of CCT enhances
classification performance. Notably, our model surpasses
non-interpretable baselines (B-CNN) and methods requir-
ing complex training.
Figure 4 highlights the distinction between CT and our

--- PAGE 7 ---
Method Test Accuracy ( %)
Multi-
stagePart R-CNN: 76.4 PS-CNN: 76.2 PN-CNN: 85.4
SPDA-CNN: 85.1 PA-CNN: 82.8 MG-CNN: 83.0
2-level attn.: 77.9 FCAN: 82.0 Neural const.: 81.0
ProtoPNet: 84.8 Deform-ProtoPNet : 86.5 PIP-net : 84.3±0.2
End-
to-
endB-CNN: 85.1 CAM: 70.5 DeepLAC: 80.3
ST-CNN: 84.1 MA-CNN: 86.5 RA-CNN: 85.3
CEM: 77.1 ProtoPFormer: 84.9 CT (w/w.o): 86.4±0.2/75.4±0.3
CCT
(ours)ViT-L+SA: 90.0±0.3 ViT-L+I-SA: 90.3±0.3 ViT-L+BO-QSA: 90.3±0.1
SwinT-L+SA: 90.7±0.02 SwinT-L+I-SA : 90.9±0.4 SwinT-L+BO-QSA (w/w.o): 91.2±0.2/90.9±0.4
ConvNeXt-L+SA: 87.8±0.3ConvNeXt-L+I-SA: 89.3±0.6ConvNeXt-L+BO-QSA : 89.4±0.4
Table 2. Performance comparison on CUB-200-2011. For B-CNN [53], Part R-CNN [96], PS-CNN [37], PN-CNN [7], SPDA-CNN [95],
PA-CNN [46], MG-CNN [85], 2-level attn. [88], FCAN [54], Neural const. [76], ProtoPNet [12], CAM [98], DeepLAC [52], ST-CNN [38],
MA-CNN [97], and RA-CNN [24], performance from [65]. For ProtoPFormer [89] and CEM [94], best performance directly from
their works. For CT [65] and CCT, results from our evaluation. The number of parameters of the backbone we use is: 307M (ViT-L),
197M (SwinT-L), and 197M (ConvNeXt-L). (w/w.o) indicates the performance with/without ground-truth concept explanations.
Figure 4. Prediction comparison between CT and our CCT with
ground-truth explanation. ( Left) All CT’s predictions are incor-
rect. The highlighted explanations in purple are key attributes typ-
ically present when correctly classifying Pelagic Cormorant ,
the incorrect label. ( Right ) Our CCT’s predictions are correct.
CCT, illustrating that CT lacks learning global concepts.
We emphasize concepts with the highest attention scores
in all images. The figure shows a case where the CT’s
predictions were completely incorrect by converging to the
single wrong label. Although the ground-truth class of
the images was Brandt Cormorant , all CT’s predic-
tions were Pelagic Cormorant . Furthermore, the at-
tributes CT used to make the incorrect classification in-
cluded a subset of attributes (purple-colored attributes in
Fig. 4) typically associated with correct predictions of the
incorrect Pelagic Cormorant label. Thus, the image-
patch-centric CT hallucinates key aspects associated with
incorrect labels, whereas our CCT shows more robust con-cept explanations of correct classifications.
Without Concept Explanations. Although the domain
expert’s knowledge is the most effective tool for guiding
a model’s explainability, the pre-precessing step to define
the visual concepts for tasks may require time-consuming
labeling and rely on human judgment. Importantly, we
demonstrate that our CCT also works effectively without
concept explanations, a capability not shared by other mod-
els. We can easily set up our loss with λexpl= 0(Final Loss
defined in section 4.3) and evaluate our model as the same
hyperparameter setups of the experiment with explanation
using only classification loss and sparsity loss.
In Table 2, our CCT’s best configuration (Swin-L+BO-
QSA) without explanation achieved 90% test accuracy,
which is very marginal compared to the one with explana-
tion. In contrast, CT can also be trained without explana-
tion, but its performance is starkly degraded.
Figure 5 visualizes the activation of the learned latent
concepts in our model from the dataset. Following [84], we
additionally trained our CCT by setting the number of class
labels ( n) to 50 and the number of latent concepts ( k) to
20. The figure showcases five latent concepts—7, 3, 19, 5,
and 4. In our experimental results, we identified key con-
cepts that CCT focuses on when classifying bird images.
Concept 7 focuses on the beak and upper torso, while Con-
cept 3 considers multiple features like the beak, eyes, and
tail. Concept 19 captures unique head and feather struc-
tures, Concept 5 outlines the bird’s entire body, and Concept
4 isolates birds from complex backgrounds. These demon-
strate the model’s nuanced focus and classification skills,
even in an unsupervised environment where we don’t have
access to its concept explanation.

--- PAGE 8 ---
Figure 5. The activation of latent concepts learned from CUB-200-
2011 and examples showing each latent concept. Further results
can be found in Appendix C.5.
Model Test Acc. ( %)
Vanilla ViT-S 83.3±0.2
ProtoPFormer(Deit-B) [89] 83.4±2.2
ProtoPool [68] (ResNet-101) 76.5±0.8
ProtoPNet [11] (ResNet-101) 77.7±0.3
Deform-ProtoPNet [21] (ResNet-101) 76.1±0.3
CT [65] (ViT-S) 27.0±0.2
BotCL†[84] (ResNet-101) 83.0
CCT: ViT-S+SA 76.3±0.2
CCT: ViT-S+I-SA 83.6±0.2
CCT: ViT-S+BO-QSA 83.7±0.2
Table 3. Test accuracy on ImageNet. We used the first 200
classes following [84]. †indicates the best result from [84].
The number of parameters is: 22M (ViT-S), 45M (ResNet-101),
50M (ConvNeXt-S), and 86M (Deit-B).
5.3. Evaluations on ImageNet
Finally, to validate that our model can learn latent con-
cepts without explanations, we tested CCT on ImageNet
following the approach in [84]. We used the relatively small
ViT (ViT-S) as the backbone6. Additional results and details
are in Appendix C.6.
Table 3 shows ImageNet classification performance. The
combination of ViT-S with BO-QSA in the experiment has
the best performance, achieving 83.7%test accuracy de-
spite using a small-sized backbone, which meets or sur-
passes the performance of other SOTA methods—including
some concept-based approaches that were not applicable or
achieved very poor performance.
In addition, we trained a simple CCT model by setting
the number of classes ( n) to 20 and the number of latent
concepts ( k) to 10 as in [84] to visualize the consistency
of the learned concepts. Figure 6 depicts five concepts we
6We avoided using a backbone larger than ResNet-101 (45M), which
excluded SwinT-S (50M) and ConvNeXt-S (50M).
Figure 6. The activation of latent concepts learned from ImageNet
and examples showing each latent concept. Further results can be
found in Appendix C.6.
selected and five pairs of representative images for each la-
tent concept learned from ImageNet. Our CCT shows that it
excels in isolating and emphasizing specific object features
across various concepts. Concept 10 isolates hen contours
by highlighting the background, while Concept 8 focuses
on birds’ ventral regions. Concept 3 skillfully segregates
fish components, Concept 5 captures shark oral regions,
and Concept 6 outlines goldfish shapes. CCT’s capabili-
ties in contouring and highlighting semantically meaning-
ful areas surpass those of existing models. It also excels in
unsupervised image retrieval, clustering semantically sim-
ilar images together as evident in Fig. 6. This showcases
the strength of our concept-centric approach in generating
semantically coherent results.
6. Conclusions
We proposed Concept-Centric Transformers, an intrinsi-
cally interpretable model via a Shared Global Workspace,
allowing for achieving better interpretability, performance,
and versatility for when expert knowledge is available or
not. A natural future research direction is to extend the
concept extraction module to acquire composable “pieces”
of knowledge [77] and then learn the underlying compo-
sition rules or mechanisms relating the acquired pieces to
each other. Those rules can be captured formally, as with
first-order logic [6]. It is also promising to investigate other
applications, such as model debugging and medical-image
diagnosis.
Acknowledgments: Supported in part by NSF award
2223839 and USACE ERDC award W912HZ-21-2-0040.
References
[1] David Alvarez Melis and Tommi Jaakkola. Towards robust
interpretability with self-explaining neural networks. Ad-
vances in Neural Information Processing Systems , 31, 2018.
1, 2, 4

--- PAGE 9 ---
[2] Bernard J Baars. A cognitive theory of consciousness . Cam-
bridge University Press, 1993. 2
[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
Neural machine translation by jointly learning to align and
translate. arXiv preprint arXiv:1409.0473 , 2014. 1
[4] Carliss Young Baldwin and Kim B Clark. Design rules: The
power of modularity , volume 1. MIT press, 2000. 1
[5] Dana H Ballard. Cortical connections and parallel process-
ing: Structure and function. Behavioral and brain sciences ,
9(1):67–90, 1986. 1
[6] Pietro Barbiero, Gabriele Ciravegna, Francesco Giannini,
Pietro Li ´o, Marco Gori, and Stefano Melacci. Entropy-based
logic explanations of neural networks. In Proceedings of the
AAAI Conference on Artificial Intelligence , volume 36, pages
6046–6054, 2022. 1, 2, 8
[7] Steve Branson, Grant Van Horn, Serge Belongie, and Pietro
Perona. Bird species categorization using pose normalized
deep convolutional nets. arXiv preprint arXiv:1406.2952 ,
2014. 7
[8] Rodney A Brooks. Intelligence without representation. Arti-
ficial intelligence , 47(1-3):139–159, 1991. 1
[9] Diogo V Carvalho, Eduardo M Pereira, and Jaime S Cardoso.
Machine learning interpretability: A survey on methods and
metrics. Electronics , 8(8):832, 2019. 1, 5
[10] Michael Chang, Tom Griffiths, and Sergey Levine. Object
representations as fixed points: Training iterative refinement
algorithms with implicit differentiation. Advances in Neural
Information Processing Systems , 35:32694–32708, 2022. 3,
A
[11] Chaofan Chen, Oscar Li, Alina Barnett, Jonathan Su, and
Cynthia Rudin. This looks like that: deep learning for inter-
pretable image recognition. CoRR , abs/1806.10574, 2018. 5,
8
[12] Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia
Rudin, and Jonathan K Su. This looks like that: deep learn-
ing for interpretable image recognition. Advances in Neural
Information Processing Systems , 32, 2019. 1, 2, 7
[13] Zhi Chen, Yijie Bei, and Cynthia Rudin. Concept whitening
for interpretable image recognition. Nature Machine Intelli-
gence , 2(12):772–782, 2020. 1, 2
[14] Kyunghyun Cho, Bart van Merri ¨enboer, Caglar Gulcehre,
Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. Learning phrase representations using RNN
encoder–decoder for statistical machine translation. In Pro-
ceedings of the 2014 Conference on Empirical Methods in
Natural Language Processing (EMNLP) , pages 1724–1734,
2014. 3
[15] Tirtharaj Dash, Sharad Chitlangia, Aditya Ahuja, and Ash-
win Srinivasan. A review of some techniques for inclusion
of domain-knowledge into deep neural networks. Scientific
Reports , 12(1):1040, 2022. 6
[16] Stanislas Dehaene and Jean-Pierre Changeux. Experimental
and theoretical approaches to conscious processing. Neuron ,
70(2):200–227, 2011. 2
[17] Stanislas Dehaene, Michel Kerszberg, and Jean-Pierre
Changeux. A neuronal model of a global workspace in ef-
fortful cognitive tasks. Proceedings of the national Academy
of Sciences , 95(24):14529–14534, 1998. 2[18] Stanislas Dehaene, Hakwan Lau, and Sid Kouider. What is
consciousness, and could machines have it? Robotics, AI,
and Humanity: Science, Ethics, and Policy , pages 43–56,
2021. 2
[19] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248–255. Ieee, 2009. 2, 5
[20] Ameet Deshpande and Karthik Narasimhan. Guiding atten-
tion for self-supervised learning with transformers. arXiv
preprint arXiv:2010.02399 , 2020. 5
[21] Jon Donnelly, Alina Jade Barnett, and Chaofan Chen. De-
formable protopnet: An interpretable image classifier using
deformable prototypes. CoRR , abs/2111.15000, 2021. 5, 8
[22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 2, 6, B, C
[23] Marc Fischer, Mislav Balunovic, Dana Drachsler-Cohen, Ti-
mon Gehr, Ce Zhang, and Martin Vechev. DL2: training
and querying neural networks with logic. In International
Conference on Machine Learning , pages 1931–1941. PMLR,
2019. 2, 5, 6, B, C
[24] Jianlong Fu, Heliang Zheng, and Tao Mei. Look closer to
see better: Recurrent attention convolutional neural network
for fine-grained image recognition. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 4438–4446, 2017. 2, 7
[25] Amirata Ghorbani, James Wexler, James Y Zou, and Been
Kim. Towards automatic concept-based explanations. Ad-
vances in Neural Information Processing Systems , 32, 2019.
1, 2
[26] Ross Girshick. Fast R-CNN. In Proceedings of the IEEE
International Conference on Computer Vision , pages 1440–
1448, 2015. 2
[27] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition , pages
580–587, 2014. 2
[28] Eleonora Giunchiglia, Mihaela Catalina Stoian, and Thomas
Lukasiewicz. Deep learning with logical constraints. arXiv
preprint arXiv:2205.00523 , 2022. 6
[29] Anirudh Goyal and Yoshua Bengio. Inductive biases for deep
learning of higher-level cognition. Proceedings of the Royal
Society A , 478(2266):20210068, 2022. 2
[30] Anirudh Goyal, Aniket Rajiv Didolkar, Alex Lamb, Kar-
tikeya Badola, Nan Rosemary Ke, Nasim Rahaman,
Jonathan Binas, Charles Blundell, Michael Curtis Mozer,
and Yoshua Bengio. Coordination among neural modules
through a shared global workspace. In International Confer-
ence on Learning Representations , 2021. 1, 2, 3, 5
[31] Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun
Sodhani, Sergey Levine, Yoshua Bengio, and Bernhard
Sch¨olkopf. Recurrent independent mechanisms. arXiv
preprint arXiv:1909.10893 , 2019. 2

--- PAGE 10 ---
[32] Yash Goyal, Amir Feder, Uri Shalit, and Been Kim. Ex-
plaining classifiers with causal concept effect (CaCE). arXiv
preprint arXiv:1907.07165 , 2019. 1, 2
[33] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri,
Franco Turini, Fosca Giannotti, and Dino Pedreschi. A sur-
vey of methods for explaining black box models. ACM Com-
puting Surveys (CSUR) , 51(5):1–42, 2018. 4, 5
[34] Nick Hoernle, Rafael Michael Karampatsis, Vaishak Belle,
and Kobi Gal. MultiplexNet: Towards fully satisfied log-
ical constraints in neural networks. In Proceedings of the
AAAI Conference on Artificial Intelligence , volume 36, pages
5700–5709, 2022. 5, 6, B
[35] Jinyung Hong and Ted Pavlic. Representing prior knowledge
using randomly, weighted feature networks for visual rela-
tionship detection. In Combining Learning and Reasoning:
Programming Languages, Formalisms, and Representations ,
2022. 2
[36] Jinyung Hong and Theodore P. Pavlic. An insect-inspired
randomly, weighted neural network with random fourier fea-
tures for neuro-symbolic relational learning. In Proceed-
ings of the 15th International Workshop on Neural-Symbolic
Learning and Reasoning (Ne/Sy 2022) , October 25–27 2021.
2
[37] Shaoli Huang, Zhe Xu, Dacheng Tao, and Ya Zhang. Part-
stacked CNN for fine-grained visual categorization. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pages 1173–1182, 2016. 2, 7
[38] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.
Spatial transformer networks. Advances in Neural Informa-
tion Processing Systems , 28, 2015. 7
[39] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,
Andrew Zisserman, and Joao Carreira. Perceiver: General
perception with iterative attention. In International confer-
ence on machine learning , pages 4651–4664. PMLR, 2021.
2
[40] Sarthak Jain and Byron C Wallace. Attention is not explana-
tion. arXiv preprint arXiv:1902.10186 , 2019. 1
[41] Baoxiong Jia, Yu Liu, and Siyuan Huang. Improving object-
centric learning with query optimization. In The Eleventh In-
ternational Conference on Learning Representations , 2022.
3, A
[42] Dmitry Kazhdan, Botty Dimanov, Mateja Jamnik, Pietro Li `o,
and Adrian Weller. Now you see me (CME): concept-based
model extraction. arXiv preprint arXiv:2010.13233 , 2020.
1, 2
[43] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai,
James Wexler, Fernanda Viegas, et al. Interpretability be-
yond feature attribution: Quantitative testing with concept
activation vectors (TCA V). In International Conference on
Machine Learning , pages 2668–2677. PMLR, 2018. 1, 2
[44] Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maxi-
milian Alber, Kristof T Sch ¨utt, Sven D ¨ahne, Dumitru Erhan,
and Been Kim. The (un) reliability of saliency methods. Ex-
plainable AI: Interpreting, explaining and visualizing deep
learning , pages 267–280, 2019. 1
[45] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen
Mussmann, Emma Pierson, Been Kim, and Percy Liang.Concept bottleneck models. In International Conference on
Machine Learning , pages 5338–5348. PMLR, 2020. 1, 2
[46] Jonathan Krause, Hailin Jin, Jianchao Yang, and Li Fei-Fei.
Fine-grained recognition without part annotations. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pages 5546–5555, 2015. 7
[47] Alex Krizhevsky. Learning multiple layers of features from
tiny images. Master’s thesis, University of Toronto, Toronto,
Canada, 2009. 5
[48] Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure
Leskovec. Faithful and customizable explanations of black
box models. In Proceedings of the 2019 AAAI/ACM Confer-
ence on AI, Ethics, and Society , pages 131–138, 2019. 4
[49] Liangzhi Li, Bowen Wang, Manisha Verma, Yuta
Nakashima, Ryo Kawasaki, and Hajime Nagahara. Scouter:
Slot attention-based classifier for explainable image recog-
nition. In Proceedings of the IEEE/CVF international
conference on computer vision , pages 1046–1055, 2021. 3
[50] Oscar Li, Hao Liu, Chaofan Chen, and Cynthia Rudin. Deep
learning for case-based reasoning through prototypes: A
neural network that explains its predictions. In Proceedings
of the AAAI Conference on Artificial Intelligence , volume 32,
2018. 1, 2
[51] Zenan Li, Zehua Liu, Yuan Yao, Jingwei Xu, Taolue Chen,
Xiaoxing Ma, L Jian, et al. Learning with logical constraints
but without shortcut satisfaction. In The Eleventh Interna-
tional Conference on Learning Representations , 2023. B
[52] Di Lin, Xiaoyong Shen, Cewu Lu, and Jiaya Jia. Deep
lac: Deep localization, alignment and classification for fine-
grained recognition. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages 1666–
1674, 2015. 7
[53] Tsung-Yu Lin, Aruni RoyChowdhury, and Subhransu Maji.
Bilinear CNN models for fine-grained visual recognition. In
Proceedings of the IEEE International Conference on Com-
puter Vision , pages 1449–1457, 2015. 7
[54] Xiao Liu, Tian Xia, Jiang Wang, Yi Yang, Feng Zhou, and
Yuanqing Lin. Fully convolutional attention networks for
fine-grained recognition. arXiv preprint arXiv:1603.06765 ,
2016. 7
[55] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 10012–10022, 2021. 2, 6, B, C
[56] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-
enhofer, Trevor Darrell, and Saining Xie. A convnet for the
2020s. In Proceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition , pages 11976–11986,
2022. 2, 6, B, C
[57] Francesco Locatello, Dirk Weissenborn, Thomas Un-
terthiner, Aravindh Mahendran, Georg Heigold, Jakob
Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-
centric learning with slot attention. Advances in Neural In-
formation Processing Systems , 33:11525–11538, 2020. 2, 3,
A

--- PAGE 11 ---
[58] Scott M Lundberg and Su-In Lee. A unified approach to
interpreting model predictions. Advances in Neural Infor-
mation Processing Systems , 30, 2017. 1, 2
[59] Marvin Minsky. Society of mind . Simon and Schuster, 1988.
2
[60] Tsendsuren Munkhdalai, Alessandro Sordoni, Tong Wang,
and Adam Trischler. Metalearned neural memory. Advances
in Neural Information Processing Systems , 32, 2019. 2
[61] Meike Nauta, J ¨org Schl ¨otterer, Maurice van Keulen, and
Christin Seifert. Pip-net: Patch-based intuitive prototypes
for interpretable image classification. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 2744–2753, June 2023. 5
[62] Anh Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas
Brox, and Jeff Clune. Synthesizing the preferred inputs for
neurons in neural networks via deep generator networks. Ad-
vances in Neural Information Processing Systems , 29, 2016.
2
[63] Hubert Ramsauer, Bernhard Sch ¨afl, Johannes Lehner,
Philipp Seidl, Michael Widrich, Thomas Adler, Lukas
Gruber, Markus Holzleitner, Milena Pavlovi ´c, Geir Kjetil
Sandve, et al. Hopfield networks is all you need. arXiv
preprint arXiv:2008.02217 , 2020. 2
[64] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
” why should i trust you?” explaining the predictions of any
classifier. In Proceedings of the 22nd ACM SIGKDD interna-
tional conference on knowledge discovery and data mining ,
pages 1135–1144, 2016. 1, 2
[65] Mattia Rigotti, Christoph Miksovic, Ioana Giurgiu, Thomas
Gschwind, and Paolo Scotton. Attention-based interpretabil-
ity with concept transformers. In International Conference
on Learning Representations , 2021. 1, 2, 4, 5, 6, 7, 8, A, B
[66] Philip Robbins. Modularity of mind. In Edward N. Zalta, ed-
itor,The Stanford Encyclopedia of Philosophy . Metaphysics
Research Lab, Stanford University, Winter 2017 edition,
2017. 1, 2
[67] Cynthia Rudin. Stop explaining black box machine learn-
ing models for high stakes decisions and use interpretable
models instead. Nature Machine Intelligence , 1(5):206–215,
2019. 1
[68] Dawid Rymarczyk, Lukasz Struski, Michal G ´orszczak, Ko-
ryna Lewandowska, Jacek Tabor, and Bartosz Zielinski. In-
terpretable image classification with differentiable proto-
types assignment. CoRR , abs/2112.02902, 2021. 5, 8
[69] Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae,
Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol
Vinyals, Razvan Pascanu, and Timothy Lillicrap. Relational
recurrent neural networks. Advances in neural information
processing systems , 31, 2018. 2
[70] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,
Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.
Grad-cam: Visual explanations from deep networks via
gradient-based localization. In Proceedings of the IEEE In-
ternational Conference on Computer Vision , pages 618–626,
2017. 1, 2
[71] Sofia Serrano and Noah A Smith. Is attention interpretable?
arXiv preprint arXiv:1906.03731 , 2019. 1[72] Murray Shanahan. A cognitive architecture that combines
internal simulation with a global workspace. Consciousness
and cognition , 15(2):433–449, 2006. 2
[73] Murray Shanahan. Embodiment and the inner life: Cognition
and Consciousness in the Space of Possible Minds . Oxford
University Press, 2010. 2
[74] Murray Shanahan. The brain’s connective core and its role
in animal cognition. Philosophical Transactions of the Royal
Society B: Biological Sciences , 367(1603):2704–2714, 2012.
2
[75] Murray Shanahan and Bernard Baars. Applying global
workspace theory to the frame problem. Cognition ,
98(2):157–176, 2005. 2
[76] Marcel Simon and Erik Rodner. Neural activation constella-
tions: Unsupervised part model discovery with convolutional
networks. In Proceedings of the IEEE International Confer-
ence on Computer Vision , pages 1143–1151, 2015. 7
[77] Gautam Singh, Yeongbin Kim, and Sungjin Ahn. Neural
systematic binder. In The Eleventh International Conference
on Learning Representations , 2023. 8
[78] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi ´egas,
and Martin Wattenberg. Smoothgrad: removing noise by
adding noise. arXiv preprint arXiv:1706.03825 , 2017. 1, 2
[79] Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai.
One pixel attack for fooling deep neural networks. IEEE
Transactions on Evolutionary Computation , 23(5):828–841,
2019. 1
[80] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic
attribution for deep networks. In International Conference
on Machine Learning , pages 3319–3328. PMLR, 2017. 2
[81] A ¨aron Van Den Oord, Nal Kalchbrenner, and Koray
Kavukcuoglu. Pixel recurrent neural networks. In Interna-
tional Conference on Machine Learning , pages 1747–1756.
PMLR, 2016. 2
[82] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in Neural
Information Processing Systems , 30, 2017. 4
[83] Catherine Wah, Steve Branson, Peter Welinder, Pietro
Perona, and Serge Belongie. Caltech–UCSD Birds-200-
2011 (CUB-200-2011) dataset. Technical Report CNS-TR-
2011-001, California Institute of Technology, 2011. 2, 5, 6
[84] Bowen Wang, Liangzhi Li, Yuta Nakashima, and Hajime Na-
gahara. Learning bottleneck concepts in image classification.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 10962–10971, 2023.
3, 5, 7, 8, B, C, D, F, G, H, I
[85] Dequan Wang, Zhiqiang Shen, Jie Shao, Wei Zhang, Xi-
angyang Xue, and Zheng Zhang. Multiple granularity de-
scriptors for fine-grained categorization. In Proceedings
of the IEEE International Conference on Computer Vision ,
pages 2399–2406, 2015. 7
[86] Sarah Wiegreffe and Yuval Pinter. Attention is not not expla-
nation. arXiv preprint arXiv:1908.04626 , 2019. 1
[87] Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi
Fan, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer.
Memvit: Memory-augmented multiscale vision transformer

--- PAGE 12 ---
for efficient long-term video recognition. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 13587–13597, 2022. 2
[88] Tianjun Xiao, Yichong Xu, Kuiyuan Yang, Jiaxing Zhang,
Yuxin Peng, and Zheng Zhang. The application of two-
level attention models in deep convolutional neural network
for fine-grained image classification. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 842–850, 2015. 7
[89] Mengqi Xue, Qihan Huang, Haofei Zhang, Lechao Cheng,
Jie Song, Minghui Wu, and Mingli Song. ProtoPFormer:
Concentrating on prototypical parts in vision transform-
ers for interpretable image recognition. arXiv preprint
arXiv:2208.10431 , 2022. 2, 5, 7, 8
[90] Zhun Yang, Adam Ishay, and Joohyung Lee. Learning to
solve constraint satisfaction problems with recurrent trans-
former. In Proceedings of the Eleventh International Con-
ference on Learning Representations , 2023. K
[91] Chih-Kuan Yeh, Been Kim, Sercan Arik, Chun-Liang Li,
Tomas Pfister, and Pradeep Ravikumar. On completeness-
aware concept-based explanations in deep neural net-
works. Advances in Neural Information Processing Systems ,
33:20554–20565, 2020. 1, 2
[92] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and
Hod Lipson. Understanding neural networks through deep
visualization. arXiv preprint arXiv:1506.06579 , 2015. 2
[93] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-
works. arXiv preprint arXiv:1605.07146 , 2016. 6
[94] Mateo Espinosa Zarlenga, Pietro Barbiero, Gabriele
Ciravegna, Giuseppe Marra, Francesco Giannini, Michelan-
gelo Diligenti, Frederic Precioso, Stefano Melacci, Adrian
Weller, Pietro Lio, et al. Concept embedding models. In
NeurIPS 2022-36th Conference on Neural Information Pro-
cessing Systems , 2022. 1, 2, 7
[95] Han Zhang, Tao Xu, Mohamed Elhoseiny, Xiaolei Huang,
Shaoting Zhang, Ahmed Elgammal, and Dimitris Metaxas.
Spda-cnn: Unifying semantic part detection and abstraction
for fine-grained recognition. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition ,
pages 1143–1152, 2016. 7
[96] Ning Zhang, Jeff Donahue, Ross Girshick, and Trevor Dar-
rell. Part-based R-CNNs for fine-grained category detection.
InComputer Vision–ECCV 2014: 13th European Confer-
ence, Zurich, Switzerland, September 6-12, 2014, Proceed-
ings, Part I 13 , pages 834–849. Springer, 2014. 2, 7
[97] Heliang Zheng, Jianlong Fu, Tao Mei, and Jiebo Luo. Learn-
ing multi-attention convolutional neural network for fine-
grained image recognition. In Proceedings of the IEEE Inter-
national Conference on Computer Vision , pages 5209–5217,
2017. 2, 7
[98] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,
and Antonio Torralba. Learning deep features for discrimi-
native localization. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages 2921–
2929, 2016. 2, 7
[99] Bolei Zhou, Yiyou Sun, David Bau, and Antonio Torralba.
Interpretable basis decomposition for visual explanation. InProceedings of the European Conference on Computer Vi-
sion (ECCV) , pages 119–134, 2018. 2

--- PAGE 13 ---
Supplementary Material
A. Reproducibility
All source codes, figures, models, etc., are available at
https://github.com/PavlicLab/WACV2024-
Hong-Concept_Centric_Transformers.git .
B. Method Details
Algorithm. Algorithm 1 shows the details of the
Concept-Slot-Attention (CSA) module in our Cocenpt-
Centric Transformer (CCT) in pseudo-code. Algorithm 1
is described based on the SA [57], but we simplify it by re-
moving the last LayerNorm andMLP layers. Because of
the modular characteristics in our framework, we leverage
three slot-based approaches, including SA [57], I-SA [10],
and BO-QSA [41], which are interchangeable. We imple-
mented the above approaches based on their official reposi-
tories/works.
Positional Embedding for Concept Slots. Because the
resulting set of concept slots is orderless [57], it is difficult
for the later module to: (i) recognize which high-level con-
cept each concept slot is representing and (ii) identify which
concept slot each high-level concept belongs to. Thus, we
add a positional encoding pcto each concept slot to avoid
these challenges; in particular, ˆsc=sc+pc. After this, the
concept embedding representation ˆSconcept∈RC×dwith
positional embedding Pis passed as the final concept rep-
resentation to the next module.
Algorithm 1 Concept-Slot-Attention (CSA) module . The
module receives the set of input features E∈RL×D;
the number of concepts C; and the dimension of con-
cepts d. The model parameters include: the linear projec-
tionqCSA, kCSA, vCSAwith output dimension d; aGRU net-
work; a Gaussian distribution’s mean and diagonal covari-
anceµ, σ∈Rd.
Sconcept=Tensor (C, d)▷concept −slots ∈RC×d
Sconcept∼ N(µ, σ)
E=LayerNorm (E)
fort= 0, . . . , T do
Sconcept=LayerNorm (Sconcept)
ACSA= softmax(1√
dqCSA(Sconcept)·
kCSA(E)⊤,axis =′concept −slots′)
ACSA=ACSA/ACSA.sum(axis =′inputs′)
U=ACSA·vCSA(E)
Sconcept=GRU(state =Sconcept,inputs =U)
end for
return SconceptNumber of Iterations in the Concept Slot Attention
module. In the original usage of slot-based approaches,
the refinement could be repeated several times depending
on the tasks. For setting the number of iterations T(Al-
gorithm 1), we set the different number of iterations Tfor
the three slot-based methods we used. For the vanilla SA,
we set Tto 1 because the initialization for slots in the SA
has some limitations revealed by [10], and we found that a
single iteration to update the concept slots benefits in which
the learned concept slots possess much information is per-
formed through the training with backpropagation. In other
words, our proposed interpretable broadcast scheme (4.2 in
the main text) contributes to better performances for the
SA than the internally refined iterations used in the con-
ventional slot-based methods. Furthermore, we set Tto3
for the BO-QSA and the I-SA by simply following the best
values of iterations in their works.
Comparison with Concept Transformers [65]. As
shown in [65, Fig. 1], Concept Transformers (CTs) are the
limited usage in our formulation, which leverages learn-
able vectors for concept learning instead of using “a shared
workspace”. We highlight that the concept embedding rep-
resentation in our CCT is more sophisticated and general-
izable than the one in CTs. Each concept embedding in
CT is represented as a simple learnable vector shared with
all input batches to learn. As such, it may be difficult for
the vector to capture which image features can contribute to
each concept more than others. For instance, as shown in
Fig. 2 in the main text, given three images belonging to the
same class “Winter Wren”, the image features with the same
spatial positions from each image can have different impor-
tance for providing information to learn the same global
concept, such as “what is the main body color for Winter
Wren?” or“what is the body shape for it?” . In contrast,
our proposed module naturally aggregates how much each
image feature contributes to each concept using the atten-
tionACSA(See in Algorithm 1) and provides batch-specific
concept embedding representations that have more seman-
tically meaningful information. In all of the experiments,
we demonstrate that our CCTs always outperform CTs.
C. Further Experimental Results and Details
In this section, we explain further experimental results
and details. All experiments are conducted with three dif-
ferent random seeds and 95% confidence intervals.
C.1. Dataset Statistics
Table A-1 depicts the statistics of all benchmark datasets
in our experiments. Because all datasets have no portion of
validation, we manually pick the portion of the validation
dataset for exploring the best hyperparameters for models.

--- PAGE 14 ---
For CIFAR-100 Super-class dataset, it serves as a no-
table example for nuanced image categorization. Originat-
ing from the original CIFAR-100 dataset, it contains 100
fine-grained image classes that are aggregated into 20 dis-
tinct super-classes for broader categorization. For instance,
the super-class ”vehicles 1” comprises specific, fine-grained
classes like ”bicycle,” ”bus,” ”motorcycle,” ”pickup truck,”
and ”train.” This structured hierarchy makes the dataset a
widely used benchmark in evaluating deep learning mod-
els, particularly those that incorporate logical constraints.
For CUB-200-2011, we explain the pre-processing
steps following [65]. Initially, the dataset has 312 binary
attributes, but we filter to retain only those attributes that
occur in at least 45 %of all samples in a given class and
occur in at least 8 classes. Thus, we get a total of 108
attributes, and based on this, we group them into two
kinds of concepts: spatial and global concepts. We finally
get 13 global and 95 spatial concepts by looking at each
attribute. For example, hasshape::perching-like
and hasprimary color::black are global
concepts, and haseyecolor::black and
hasforehead color::yellow are spatial con-
cepts.
For the ImageNet dataset, we adopted the methodology
presented in [84] to validate our CCT’s ability to learn la-
tent concepts without the need for explicit concept expla-
nations. [84] uses attention maps to identify a predefined
set of concepts within the dataset, focusing on the first 200
classes of ImageNet for their evaluation. Following this ap-
proach, we also utilized the first 200 classes in ImageNet
for training and conducted evaluations to determine test ac-
curacy on the ImageNet dataset. This experiment serves to
benchmark our CCT model’s capabilities in learning latent
concepts without explicit explanations, further showcasing
its effectiveness in an unsupervised setting.
C.2. Hardware Specification of The Server
The hardware specification of the server that we used to
experiment is as follows:
• CPU: Intel® CoreTMi7-6950X CPU @ 3.00GHz (up
to 3.50 GHz)
• RAM: 128 GB (DDR4 2400MHz)
• GPU: NVIDIA GeForce Titan Xp GP102 (Pascal ar-
chitecture, 3840 CUDA Cores @ 1.6 GHz, 384-bit bus
width, 12 GB GDDR G5X memory)
C.3. Model Architectures
We leverage three kinds of backbones, including Vision
Transformers (ViT) [22], Swin Transformers (Swin) [55],
and ConvNeXt [56]. We employ timm Python library sup-
ported by Hugging Face™.Variants of ViT. In our experiments, we use three variants
of Vision Transformer (ViT), ViT-Large ,ViT-Small ,
and ViT-Tiny (vitlarge patch16 224,
vitsmall patch16 224, andvittiny patch16
224, respectively in timm ). These variants are defined by
their number of encoder blocks, the number of attention
heads on each block, and the dimension of the hidden layer.
TheViT-Large has 24 encoder blocks with 16 heads,
and the dimension of the hidden layer is 1024. In addition,
TheVit-Small has 12 encoder blocks with 6 heads,
and the dimension of the hidden layer is 384. Finally, the
ViT-Tiny has 12 encoder blocks with 3 heads, and the
dimension of the hidden layer is 192, which is much more
lightweight. A comparison between the variants of the ViT
is shown in Table A-2.
Architecture of Swin-Large. In our experi-
ment, we use the Swin Transformer (Swin)-
Large ( swin large patch4 window7 224
.msin22k intimm ). See Table A-3 for the overall
architecture of Swin-Large.
Architecture of ConvNeXt-Large. In our experiment,
we leverage ConvNeXt-Large ( convnext large
.fbin22k ftin1k intimm ). See Table A-4 for the
overall architecture of ConvNeXt-Large.
C.4. CIFAR100 Super-class Experiments
The Difference Between CCT and Other Deep-Learning
Approaches with Logical Constraints. The logical con-
straint by [23] to address this task was introduced, i.e., once
the model classifies an image into a class, the predicted
probabilities of that super-class must first arrive at the whole
mass. Referring to this, several deep-learning approaches
leverage it [34, 51]. For instance, the five classes, baby,
boy, girl, man, andwoman , belong to the super-class people .
Thus, Pr people(x) = 1 ifxis classified as baby . So, logical
constraints can be formulated as ∧s∈superclass (Prs(x) =
0∨Prs(x) = 1) , where the probability of the super-class
is the sum of its corresponding classes’ probabilities, e.g.,
Prpeople(x) =Prbaby(x) +Prboy(x) +Prgirl(x) +Prman(x) +
Prwoman (x).
However, because our CCT (and CT) follows Proposi-
tion 1 in [65], the probability of choosing the preferred su-
perclass is sc= arg max s(βc)sof class c(Refer to Eq.(2)
in the main text). This means the super-class is determined
by the largest attention score of the class among all classes,
which is different from the definition of the logical con-
straint above.
Additional Results. Figure A-1 showcases several exam-
ples of correct predictions by CT and our CCT, highlighting

--- PAGE 15 ---
Table A-1. Benchmark Dataset Statistics. †indicates the rescaled size of inputs for the ViT backbone, which is different from the original
sizes of the datasets. For the ImageNet experiment setup, we follow [84].
Dataset CIFAR100 Super-class CUB-200-2011 ImageNet
Input size 3×28×28 3×224×224†3×224×224†
# Classes 20 (super-class) 200 (bird species) 200 (objects)
# Concepts 100 (class) 13 (global), 95 (spatial) 50 (latent spatial)
# Training samples 55,000 5,994 255,224
# Validation samples 5,000 1,000 10,000
# Test samples 10,000 4,794 20,000
Model Layers Hidden Dim. Heads # Params.
ViT-Tiny 12 192 3 5M
ViT-Small 12 384 6 22M
ViT-Large 24 1024 16 304 M
Table A-2. Comparison between variants of the ViT [22]
Model Layers (per stage) Hidden Dim. Attention Heads # Params.
Swin-Large (2, 2, 18, 2) 192 6 197M
Table A-3. Overall architecture of the Swin Transformer [55]
Model Conv. Layers (per block) Hidden Dim. Attention Heads # Params.
ConvNeXt-Large 2 512 16 50M
Table A-4. Overall architecture of the ConvNeXt [56]
that compared to CT, our CCT achieves sparser explanation
attention scores, which indicates a more confident and ro-
bust degree of belief for decision-making.
Hyperparameter Settings. For the experimental results,
including Table 1 and Fig. 3 in the main text, referring
to [23], we first find the best experimental setups of both
ViT-Tiny and CT because they have not been evaluated on
CIFAR100 Super-class before. Once the hyperparameter
setup for the ViT-Tiny backbone is found to achieve simi-
lar performance to the Wide ResNet (the backbone for the
second baseline group), we apply the same hyperparame-
ter setting to both CT and our CCT. Table A-5a presents all
shared hyperparameters for ViT-Tiny, CT, and all configura-
tions of CCT. Please refer to Table A-6 for hyperparameter
settings used in our baseline experiments.C.5. CUB-200-2011 Experiments
Additional Results with Comparison with CT. With
concept explanations, we introduce additional experimental
results to compare predictions between CT and our CCT.
Figure A-2 depicts a case where the CT’s predictions
were utterly different from each other despite all im-
ages having the same class. The predicted classes from
the CT in Fig. A-2 were Prothonotary Warbler ,
Pine Warbler , and Magnolia Warbler , respec-
tively. The only difference between the first two pre-
dictions that the CT made is whether the spatial con-
cepthasunder tail color::black exists, indicat-
ing the CT’s performance sensitivity. In addition, the third
prediction from the CT contains new spatial concepts un-
related to the first two examples, which shows analytical
uncertainty as to why the CT made such a decision. In
contrast, the predictions made by our CCT were all correct,
with consistent global explanations. This indicates that al-
though incorporating spatial concepts provide additional in-

--- PAGE 16 ---
Figure A-1. Examples of correct predictions by CT and CCT on CIFAR100 super-class. The 100 classes are indexed from 1 (top left in
10x10 grid) to 100 (bottom right in 10x10 grid).
formation, determining robust global concepts is more im-
portant for the classification task.
Figure A-3 shows some examples where our CCT out-
performs CT to classify Olive sided Flycatcher .
All CT’s predictions were Western Wood Pewee ,
and this is caused by their spatial explana-
tions, including haseyecolor::black and
haslegcolor::black . We discovered that these two
spatial attributes are critical for Western Wood Pewee ,
and thus mislead the model’s predictions. In contrast,
the explanations achieved by CCT are more robust and
consistent.
Finally, Figs. A-4 and A-5 demonstrate the failure cases
where both CCT and CT predict incorrectly. In both mod-
els, noise caused by spatial explanations common in some
classes tends to impair the performance of prediction re-
sults. However, compared to CT, our proposed CCT cap-
tures richer global explanations, which thus helps to achieve
better classification performance in general.Hyperparameter Settings. We follow the official imple-
mentation of CT and produce the experimental results, in-
cluding Table 2, Fig. 4, in the main text, Fig. A-2, and
Fig. A-3 in Appendix. The only difference in hyperparam-
eter settings between our CCT and CT is the learning rate
for the AdamW optimizer, 5e-5 for CT, 1e-5 for the con-
figurations with the ViT-Large backbone of our CCT, 2e-5
for our CCT with SwinT-Large, and 5e-5 for our CCT with
ConvNeXt-Large. Table A-5c shows the shared hyperpa-
rameters for both approaches. For those baselines not listed
in Table A-5c, we refer directly to the data presented in their
respective publications.
Conceptual Visualization For an in-depth look into the
learnt concepts within CUB-200-2011 dataset, we adapted
our CCT model to 50 distinct classes and 20 latent concepts
following [84]. A comprehensive visualization of these re-
fined latent concepts can be viewed in Figure A-6, A-7 and
A-8. Please note that we present these visualization results
without providing concept explanations.
In the CUB-200-2011 dataset, our model’s first concept

--- PAGE 17 ---
Figure A-2. Prediction comparison between CT and our CCT. ( Top) CT’s predictions are incorrect. ( Bottom ) All predictions are correct
by our CCT.
Figure A-3. Prediction comparison between CT and CCT. ( Top Row ) CT’s predictions are incorrect. The purple highlighted explanations
are key attributes in Western Wood Pewee . (Bottom Row ) All predictions are correct by our CCT.
Figure A-4. The example of failure predictions of both CT ( Top Row ) and CCT ( Bottom Row ).
highlights key features of birds such as their head and beak
area. Concept 2 focuses on the contours of seagulls by em-
phasizing the background. Concept 3 considers multiple
features like the beak, eyes, and tail, while Concept 4 iso-
lates birds from complex backgrounds. Similarly, Concept
5 outlines the bird’s entire body. Concept 6 specializes in
highlighting the body area of yellow-bodied birds, and Con-cept 7 zeroes in on the beak and upper torso. Concept 8 not
only highlights contours but also focuses on the bird’s eye
area. Concepts 9 and 10 share similarities with Concept
8, emphasizing the eye region. Concept 11 stands out by
focusing on the head area of red parrots, which is also a
feature that catches human attention. Concepts 12 and 13
are dedicated to the bird’s feet and lower body areas, re-

--- PAGE 18 ---
Figure A-5. The example of failure predictions of both CT ( Top Row ) and CCT ( Bottom Row ).
spectively. Concept 14 traces the entire body of a seagull,
whereas Concept 15 captures the contours of birds in flying
poses. Concept 16, on the other hand, focuses on the con-
tours of birds in sitting poses. Concept 17 centers on the
eye and mid-body regions, while Concept 18 partially cap-
tures key features of long-necked birds. Concept 19 cap-
tures unique head and feather structures. Finally, Concept
20 perfectly outlines the bird’s contour.
These results demonstrate the model’s ability to focus on
a wide range of features, from beaks and eyes to tails and
feet, reinforcing its classification skills. This is particularly
remarkable in an environment where explicit concept ex-
planations are unavailable, highlighting another dimension
where our model excels over others.
Image Retrieval and Clustering. In addition to its ro-
bust classification capabilities, our CCT model excels in the
realm of image retrieval. Remarkably, the model achieves
this without requiring explicit explanations for the 20 la-
tent concepts it identifies within the CUB-200-2011 dataset.
This intuitive clustering of images based on inherent fea-
tures—ranging from the contours of seagulls to the unique
features of a red parrot’s head—demonstrates another di-
mension in which our model surpasses others in the field.
Essentially, it clusters semantically related images based on
these latent concepts, offering coherent results even in an
environment where we don’t have access to concept expla-
nation. For instance, images featuring parrots are clustered
together, driven by Concept 11’s focus on the head area of
red parrots. Similarly, images of seagulls are grouped to-
gether, guided by Concept 2’s emphasis on seagull contours.
Ablation Study. In Figure A-9, we can infer that the
CCT’s performance is sensitive to the explanation lambda
hyperparameter λexpl (4.3 in the main text). There seems
to be an optimal range for explanation lambda, somewhere
between 0 and 10, within which the model performs best.Values of λexplhigher than 10 lead to progressively worse
performance, with severe degradation observed at the high-
est values of lambda tested.
In Figure A-10, we can infer that a lower learning rate
of 1.00e-4 provides the best model performance, especially
when the explanation lambda λexpl is set to 0.0. As the
learning rate increases, the model’s accuracy deteriorates
significantly. The extremely low accuracy at higher learn-
ing rates (1.00e-01 and 1.00e+00) suggests that the model
is unable to learn effectively, likely overshooting the opti-
mal values during training due to too large updates to the
model’s parameters.
C.6. ImageNet Experiments
Experiment Design. To further confirm CCT’s capabil-
ity to derive latent concepts autonomously, we conducted
an experiment on the ImageNet dataset, adhering to the
methodology described in [84]. We chose ViT-S as
our backbone architecture, avoiding models with more
than 45M parameters, such as ResNet-101, Swin-S, and
ConvNeXt-S.
Conceptual Visualization. For an in-depth look into the
learnt concepts, we tailored our CCT model to 20 classes
and 10 latent concepts, as in [84]. Figure A-11 provides a
detailed visualization of these latent concepts. Please note
that we present these visualizaton results without providing
concept explanations.
The first concept our model identifies focuses on jelly
shellfish, specifically isolating the contours of these crea-
tures against their natural backgrounds. The second concept
turns its attention towards chickens, particularly emphasiz-
ing the distinct red comb atop their heads. Following this,
the third concept excels in separating the various compo-
nents of fish species, effectively delineating between differ-
ent parts such as fins, scales, and tails. Moving to marine
life, the fourth concept aims to concentrate on the facial at-
tributes of dolphins and sharks, with a particular focus on

--- PAGE 19 ---
Figure A-6. From left to right, the image displays Concepts 1 through 20 as identified in the CUB-200-2011 dataset following [84]. Each
pair of images consists of the masked version (with attention activation mask) on the left and the original image on the right. For better
visual representation, see A-7 and A-8.
Figure A-7. First 10 concepts (1-10) in CUB-200-2011
their mouths. The fifth concept takes this a step further by
specifically highlighting the regions around the oral cavities
of sharks, setting them apart from other parts of the crea-
ture. In a similar aquatic vein, the sixth concept outlines
the unique shapes and contours of goldfish, capturing their
form effectively. The seventh concept diverges by focusing
solely on the feet of birds, whether they are perched or in
flight. This is complemented by the eighth concept, which
takes a broader approach to birds by concentrating on their
ventral regions, capturing details such as feathers and un-derbellies. The ninth concept specializes in ostriches, par-
ticularly focusing on the distinct features that make up their
head region. Finally, the tenth concept zeroes in on hens.
It particularly focuses on isolating their contours against a
variety of backgrounds, thereby allowing for a clearer un-
derstanding of the hen’s form and structure.
These visualizations demonstrate CCT’s unparalleled
ability to focus on semantically meaningful aspects of the
images.

--- PAGE 20 ---
Figure A-8. Second 10 concepts (11-20) in CUB-200-2011
Figure A-9. Performance comparison of CCTs on CUB-200-2011
with different value of explanation lambda λexpl.
Figure A-10. Performance comparison of CCTs on CUB-200-
2011 with different value of learning rates under different expla-
nation lambda values.
Image Retrieval and Clustering. In addition to identify-
ing intricate latent concepts, our CCT model demonstrates
exceptional capabilities in image retrieval tasks. As illus-
trated in Fig. A-11, the model is adept at clustering im-
ages that share semantic similarities, thereby reinforcing its
utility in generating semantically coherent results. Notably,
our CCT achieves this level of clustering without any need
for explicit concept explanations. This sets it apart from
other models in the field, as it can intuitively group imagesbased on the inherent features recognized through the latent
concepts, ranging from the contours of jelly shellfish to the
unique features of an ostrich’s head. This ability to clus-
ter semantically related images without detailed conceptual
guidance emphasizes another aspect where our model ex-
cels over others in the domain.
Hyperparameter Settings. We follow the official imple-
mentation of CT and produce the experimental results, in-
cluding Table 3 in the main text.
The only difference in hyperparameter settings between
our CCT, CT, and the vanilla ViT-S is the learning rate for
the AdamW optimizer. For the vanilla ViT-S, the learning
rate is 0.0001. For CT, the learning rate is 5e-5, following
that in their CUB-200-2011 experiments. For our CCT, the
learning rate is 0.0001. Table A-5d shows the shared hyper-
parameters for the vanilla ViT-S, CT, and all configurations
of our CCT. Please refer to the Table A-6 for hyperparame-
ter settings used in our baseline experiments.
Limitations of Prototype-based Methods. The observa-
tion from testing prototype-based models such as ProtoP-
Former, ProtoPool, ProtoPNet, and Deform-ProtoPNet on
ImageNet yielded some interesting results. While we ini-
tially reported outcomes for only the first 200 labels of
ImageNet in Table 3, in line with the methodology from
[84], it was necessary for us to reduce the number of proto-
types per instance—a critical hyperparameter for prototype
based model—due to computational constraints, whereas
our transformer-based model, CCT, managed the task with-
out issue. We hypothesize that this limitation arises from
the inherent design of prototype-based models, which uti-
lize ’prototypes’ to offer interpretability to the decision-
making processes of complex machine learning models in
areas such as image classification, object detection, or seg-
mentation. It can be a trivial issue when the size of dataset is
small, however, as the complexity of the dataset increases,

--- PAGE 21 ---
Figure A-11. All 10 concepts (1-10) in ImageNet dataset following [84]
Figure A-12. Performance comparison on ImageNet with different
number of labels.
such as with ImageNet, which contains millions of images
across thousands of categories, the number of prototypes
required to effectively represent all classes grows. This
growth demands significantly more memory and computa-
tional resources, particularly GPU RAM, to store and pro-
cess these prototypes during both training and inference.
Additional Experimental Results. Fig. A-12 shows the
experimental results on ImageNet with different number of
labels to train vanilla ViT-S and CCT. We tested both mod-
els with the same hyperparameter setting in the main text,
and selected the best configuration of CCT, using BO-QSA.
Even though the number of latent concepts in CCT is 50 fol-
lowing [84], CCT mostly outperformed the pretrained back-
bone. When the number of labels is enormous (≥800) , the
performance of CCT is similar to or worse than that of the
backbone. This is typical because the number of 50 latent
concepts we set is insufficient to process all labels.
Figure A-13. Performance comparison of CCTs on ImageNet with
different number of latent concepts. The dashed blue line indicates
the test accuracy of vanilla ViT-S.
Therefore, we performed an additional experiment on
ImageNet using a total of 1000 classes and different num-
bers of latent concepts for CCT. Fig. A-13 demonstrates the
performance comparison of CCTs depending on the number
of latent concepts. When the number of concepts is set to
150, CCT outperformed the vanilla ViT-S (the dashed blue
line), indicating the contribution of setting the number of
latent concepts to the performance of CCT.
D. Limitations
In this section, we explain the limitations of our proposed
approach.
First, because of the CCT’s architectural characteristics
in Section 4 in the main text, the proposed approach en-
forces additive contributions from the user-customized con-
cepts to the classification probabilities. In other words, we
ignore the latent higher-order relations among the concepts.

--- PAGE 22 ---
Name Value
Batch size 64
Epochs 20
Warmup Iters. 10
Learning rate 5e-5
Explanation loss λ 1.0
Weight decay 1e-3
Attention sparsity 0.5
(a) ViT-Tiny, CT and all configurations of CCT on
CIFAR100 Super-class
Name Value
Epochs 60
Learning Rate 1e-4
Number of classes 20
Number of concepts 10
Quantity Bias 0.1
Distinctiveness Bias 0.05
Consistence Bias 0.01
Weak Supervision Bias 0.1
(b) For botCL on CIFAR100 Super-class
Name Value
Batch size 16
Epochs 50
Warmup Iters. 10
Explanation loss λ 1.0
Weight decay 1e-3
Attention sparsity 0.5
(c) For both CT and all configurations of CCT on
CUB-200-2011
Name Value
Batch size 256
Epochs 10
Warmup Iters. 10
Explanation loss λ 0.
Weight decay 1e-3
Attention sparsity 0.
(d) For both CT and all configurations of CCT on
ImageNet
Table A-5. Shared hyperparameters on different datasets.
For instance, in our CUB-200-2011 experiments, we as-
sumed that global and spatial concepts could be represented
and learned parallelly in our framework and that there is
no correlation between the global concept and the spatial
concept, which is independent. However, this is limited
because more complex relationships, such as hierarchical
properties, can exist among them. It might be addressed
by introducing refined architectural properties in our CCT.
For example, the Bi-directional Recurrent Unit can be in-Name CIFAR100 ImageNet
Batch size 64 16
Epochs 10 10
Freeze Epochs 10 10
Pre-train Epochs 0 0
Weight decay 0.0 0.0
LR (prototypes weights) 5e-2 5e-2
LR (backbone) 5e-4 5e-4
(a) For PIP-Net on CIFAR100 and ImageNet, LR
stands for Learning Rate
Name CIFAR100 ImageNet
Batch size 64 64
Epochs 50 10
Warmup Learning Rate 1e-4 1e-4
Feature Learning Rate 4e-4 4e-4
Prototype Learning Rate 3e-3 3e-3
Number of Prototype 2000 2000
(b) For ProtoPFormer on CIFAR100 and Ima-
geNet
Name CIFAR100 ImageNet
Batch size 80 80
Epochs 30 20
Learning Rate 1e-3 1e-3
Gumbel Time 30 30
Number of Classes 20 200
Number of Prototype 202 202
Prototype Depth 256 256
(c) For ProtoPool on CIFAR100 and ImageNet
Name CIFAR100 ImageNet
Batch size 80 80
Epochs 100 100
Learning Rates default default
Number of Classes 20 200
Number of Prototype 100 200
(d) For Deformable-ProtoPNet on CIFAR100 and
ImageNet
Name CIFAR100 ImageNet
Batch size 80 80
Epochs 100 100
Learning Rates default default
Number of Classes 20 200
Number of Prototype 1000 2000
(e) For ProtoPNet on CIFAR100 and ImageNet
Table A-6. Shared hyperparameters on different datasets.
troduced to allow global and spatial concepts to learn each
other’s conceptual influences.
Secondly, although, in the experiments of CUB-200-
2011 and ImageNet without using explanations, we demon-

--- PAGE 23 ---
strate that our CCT can visualize the learned semantic con-
cepts, we acknowledge that our CCT might be integrated
with more sophisticated losses to achieve better visualiza-
tion of the learned concepts. For example, [90] leverages
losses, including reconstruction, contrastive losses, and var-
ious regularizers, to enforce the individual consistency and
mutual distinctness of concepts. In our future research, we
might use those losses and regularizers to allow our model
to achieve better visualization capabilities.
