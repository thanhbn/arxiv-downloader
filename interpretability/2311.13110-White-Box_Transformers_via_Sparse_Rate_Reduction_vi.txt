# 2311.13110.pdf
# Chuyển đổi từ PDF sang TXT  
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/interpretability/2311.13110.pdf
# Kích thước file: 37854139 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Transformer Hộp Trắng thông qua Giảm Tỷ lệ Thưa
Transformer Hộp Trắng thông qua Giảm Tỷ lệ Thưa:
Nén là Tất cả?
Yaodong Yu†,⋆yyu@eecs.berkeley.edu
Sam Buchanan‡,⋆sam@ttic.edu
Druv Pai†,⋆druvpai@berkeley.edu
Tianzhe Chu†,♮chutzh@berkeley.edu
Ziyang Wu†zywu@berkeley.edu
Shengbang Tong†tsb@berkeley.edu
Hao Bai♯haob2@illinois.edu
Yuexiang Zhai†simonzhai@berkeley.edu
Benjamin D. Haeffele♭bhaeffele@jhu.edu
Yi Ma†,♢mayi@hku.hk, yima@eecs.berkeley.edu
†Đại học California, Berkeley
‡Viện Công nghệ Toyota tại Chicago
♮Đại học ShanghaiTech
♯Đại học Illinois, Urbana-Champaign
♭Đại học Johns Hopkins
♢Đại học Hong Kong

Tóm tắt
Trong bài báo này, chúng tôi cho rằng một mục tiêu tự nhiên của học biểu diễn là nén và biến đổi phân phối dữ liệu, chẳng hạn các tập hợp token, hướng tới một hỗn hợp Gaussian chiều thấp được hỗ trợ trên các không gian con không liên kết. Tính tốt của một biểu diễn như vậy có thể được đánh giá bằng một thước đo có nguyên tắc, được gọi là giảm tỷ lệ thưa, đồng thời tối đa hóa lợi ích thông tin nội tại và tính thưa bên ngoài của biểu diễn học được. Từ góc độ này, các kiến trúc mạng sâu phổ biến, bao gồm transformer, có thể được xem như việc thực hiện các sơ đồ lặp để tối ưu hóa thước đo này. Đặc biệt, chúng tôi dẫn xuất một khối transformer từ tối ưu hóa xen kẽ trên các phần của mục tiêu này: toán tử tự chú ý đa đầu nén biểu diễn bằng cách thực hiện một bước gradient descent gần đúng trên tỷ lệ mã hóa của các đặc trưng, và perceptron đa lớp tiếp theo làm thưa các đặc trưng. Điều này dẫn đến một họ các kiến trúc mạng sâu giống transformer hộp trắng, được đặt tên là crate, hoàn toàn có thể diễn giải về mặt toán học. Chúng tôi chỉ ra, thông qua một kết nối mới giữa khử nhiễu và nén, rằng nghịch đảo của mã hóa nén nói trên có thể được thực hiện bởi cùng lớp kiến trúc crate. Do đó, các kiến trúc hộp trắng được dẫn xuất như vậy có tính phổ quát cho cả bộ mã hóa và bộ giải mã. Các thí nghiệm cho thấy rằng những mạng này, mặc dù đơn giản, thực sự học cách nén và làm thưa biểu diễn của các tập dữ liệu hình ảnh và văn bản thực tế quy mô lớn, và đạt được hiệu suất mạnh mẽ trên các cài đặt khác nhau: ViT, MAE, DINO, BERT, và GPT2. Chúng tôi tin rằng khung tính toán được đề xuất thể hiện tiềm năng lớn trong việc

.⋆Đóng góp bằng nhau.
1arXiv:2311.13110v4 [cs.LG] 6 Sep 2024

--- TRANG 2 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
thu hẹp khoảng cách giữa lý thuyết và thực tiễn của học sâu, từ góc độ thống nhất về nén dữ liệu. Mã nguồn có sẵn tại: https://ma-lab-berkeley.github.io/CRATE .
2

--- TRANG 3 ---
Transformer Hộp Trắng thông qua Giảm Tỷ lệ Thưa

Mục lục
1 Giới thiệu 5
1.1 Bài toán Học Biểu diễn . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.2 Tổng quan các Phương pháp Hiện có . . . . . . . . . . . . . . . . . . . . 6
1.3 Mục tiêu và Đóng góp của Nghiên cứu này . . . . . . . . . . . . . . . . . 11
2 Mã hóa Hộp Trắng qua Nén Mất mát Có cấu trúc 14
2.1 Mong muốn và Mục tiêu của Học Biểu diễn . . . . . . . . . . . . . . . . 14
2.2 Học Biểu diễn Tiết kiệm qua Tối ưu hóa Được triển khai . . . . . . . . . 20
2.3 Tự Chú ý như Gradient Descent trên Tỷ lệ Mã hóa của Token . . . . . . 22
2.4 MLP như Gradient Descent Gần kề cho Mã hóa Thưa của Token . . . . . 24
2.5 Kiến trúc Transformer Hộp Trắng Tổng thể: CRATE . . . . . . . . . . . 25
3 Giải mã Hộp Trắng qua Khử nhiễu và Khuếch tán Có cấu trúc 27
3.1 Khử nhiễu-Khuếch tán chống lại Cấu trúc Chiều thấp . . . . . . . . . . . 28
3.2 Tiết kiệm và Nhất quán qua Khử nhiễu-Khuếch tán Có cấu trúc . . . . . 31
3.3 Khử nhiễu-Khuếch tán Có cấu trúc qua Lớp Transformer Khả nghịch . . . 33
4 Đánh giá Thí nghiệm 35
4.1 Xác minh Thực nghiệm CRATE trên Nhiều Tác vụ Thực tiễn . . . . . . . 36
4.1.1 Phân loại Hình ảnh Có giám sát qua ViT . . . . . . . . . . . . . . 36
4.1.2 Hoàn thiện Hình ảnh qua Tự mã hóa Có mặt nạ . . . . . . . . . . 37
4.1.3 Học Tự giám sát qua Phương pháp Huấn luyện DINO . . . . . . . 41
4.1.4 Tiền huấn luyện Mô hình Ngôn ngữ qua BERT và GPT . . . . . . 43
4.2 Phân tích và Trực quan hóa Lớp CRATE Đã học . . . . . . . . . . . . . 47
4.3 Sự Xuất hiện Tính chất Ngữ nghĩa trong Bản đồ Chú ý CRATE Đã học . . 50
4.3.1 Thiết lập Thí nghiệm . . . . . . . . . . . . . . . . . . . . . . . . . 51
4.3.2 Đo lường Sự Xuất hiện của Phân đoạn . . . . . . . . . . . . . . . 51
4.3.3 Phân tích Phân đoạn trong CRATE . . . . . . . . . . . . . . . . . 52
5 Kết luận và Hướng Mở 54
A Chi tiết Kỹ thuật cho Phần 2 57
A.1 Phần bổ sung cho Phần 2.3 . . . . . . . . . . . . . . . . . . . . . . . . . 57
A.2 Phần bổ sung cho Phần 2.4 . . . . . . . . . . . . . . . . . . . . . . . . . 59
A.2.1 Bổ đề Phụ trợ . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
B Chi tiết Kỹ thuật cho Phần 3 64
B.1 Tổng quan về Quá trình Khuếch tán . . . . . . . . . . . . . . . . . . . . . 64
B.2 Phần bổ sung cho Phần 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . 68
B.2.1 Bổ đề Phụ trợ . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
B.3 Phần bổ sung cho Phần 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . 72
B.3.1 Bổ đề Phụ trợ Chính . . . . . . . . . . . . . . . . . . . . . . . . 83
B.3.2 Bất đẳng thức Tập trung cho Cài đặt của chúng ta . . . . . . . . . 86
B.3.3 Bất đẳng thức Tập trung Tổng quát . . . . . . . . . . . . . . . . . 90
3

--- TRANG 4 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
B.4 Phần bổ sung cho Phần 3.3 . . . . . . . . . . . . . . . . . . . . . . . . . 94
C Chi tiết Triển khai Bổ sung và Kết quả Thí nghiệm 95
C.1 Chi tiết về CRATE cho Phân loại Hình ảnh . . . . . . . . . . . . . . . . 95
C.2 Chi tiết về CRATE-MAE cho Hoàn thiện Hình ảnh . . . . . . . . . . . . 96
C.3 Chi tiết về CRATE-DINO cho Học Tự giám sát . . . . . . . . . . . . . . 97
C.4 Chi tiết về CRATE-BERT và CRATE-GPT trên Ngôn ngữ Tự nhiên . . . 97
C.5 Nghiên cứu Loại bỏ của CRATE trên Phân loại Hình ảnh . . . . . . . . . 98
C.6 Nghiên cứu Loại bỏ Lớp ISTA trong CRATE . . . . . . . . . . . . . . . 101
C.7 Nghiên cứu Loại bỏ Lớp MSSA và Lớp ISTA trong CRATE và So sánh
với ViT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
C.8 Kết quả Thí nghiệm Bổ sung của Phân tích Theo lớp . . . . . . . . . . . 103
C.9 Kết quả Thí nghiệm Bổ sung của Đánh giá Nén và Tính thưa cho
ViT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
C.10 Chi tiết và Kết quả Thí nghiệm của Trực quan hóa Bản đồ Chú ý . . . . . 108
D Mã PyTorch cho CRATE 110
D.1 Mã giả giống PyTorch cho Khối MSSA và ISTA . . . . . . . . . . . . . . 110
D.2 Mã giả giống PyTorch cho Bộ mã hóa CRATE . . . . . . . . . . . . . . . 110
D.3 Mã giả giống PyTorch cho Bộ giải mã CRATE . . . . . . . . . . . . . . . 112
D.4 Mã giả giống PyTorch cho Bộ phân loại Hình ảnh CRATE . . . . . . . . 112
4

--- TRANG 5 ---
Transformer Hộp Trắng thông qua Giảm Tỷ lệ Thưa
1 Giới thiệu
1.1 Bài toán Học Biểu diễn
Trong những năm gần đây, học sâu đã chứng kiến thành công thực nghiệm to lớn trong việc xử lý và mô hình hóa lượng lớn dữ liệu đa chiều và đa phương thức (Krizhevsky et al., 2009; He et al., 2016; Radford et al., 2021; Chen et al., 2020; He et al., 2022). Như được lập luận bởi Ma et al. (2022), phần lớn thành công này được quy cho khả năng của mạng sâu trong việc học hiệu quả các cấu trúc chiều thấp có thể nén trong phân phối dữ liệu và sau đó biến đổi phân phối thành một biểu diễn tiết kiệm, tức là nhỏ gọn và có cấu trúc. Một biểu diễn như vậy sau đó tạo điều kiện cho nhiều tác vụ xuôi dòng, ví dụ, trong thị giác, phân loại (He et al., 2016; Dosovitskiy et al., 2021), nhận dạng và phân đoạn (Carion et al., 2020; He et al., 2020; Kirillov et al., 2023), và sinh tạo (Karras et al., 2019; Rombach et al., 2022; Saharia et al., 2022).

Học biểu diễn qua mã hóa và giải mã nén. Để phát biểu bài toán chung đằng sau tất cả những thực tiễn này một cách chính thức hơn, người ta có thể xem một tập dữ liệu đã cho như các mẫu của một vector ngẫu nhiên x trong không gian đa chiều, chẳng hạn RD. Thông thường, phân phối của x có chiều nội tại thấp hơn nhiều so với không gian bao quanh. Nói chung, bằng việc học một biểu diễn, chúng ta thường có nghĩa là học một ánh xạ liên tục, chẳng hạn f(·), biến đổi x thành một vector đặc trưng được gọi là z trong không gian khác (thường là chiều thấp hơn), chẳng hạn Rd. Có hy vọng rằng thông qua ánh xạ như vậy:

x ∈ RD → f(x) → z ∈ Rd, (1)

các cấu trúc nội tại chiều thấp của x được xác định và biểu diễn bởi z theo cách nhỏ gọn và có cấu trúc hơn để tạo điều kiện cho các tác vụ tiếp theo như phân loại hoặc sinh tạo. Đặc trưng z có thể được xem như một mã nhỏ gọn (đã học) cho dữ liệu gốc x, vì vậy ánh xạ f cũng được gọi là bộ mã hóa. Câu hỏi cơ bản của học biểu diễn, và một vấn đề trung tâm mà chúng ta sẽ giải quyết trong nghiên cứu này, là:

Một thước đo có nguyên tắc và hiệu quả cho tính tốt của biểu diễn là gì?

Về mặt khái niệm, chất lượng của một biểu diễn z phụ thuộc vào việc nó xác định tốt như thế nào thông tin liên quan và đầy đủ nhất của x cho các tác vụ tiếp theo, và nó biểu diễn thông tin này hiệu quả như thế nào. Trong thời gian dài, người ta tin và lập luận rằng tính "đầy đủ" hoặc "tốt" của một đặc trưng đã học nên được định nghĩa theo một tác vụ cụ thể. Ví dụ, z chỉ cần đủ để dự đoán nhãn lớp y trong bài toán phân loại. Để hiểu vai trò của học sâu hoặc mạng sâu trong kiểu học biểu diễn này, Tishby và Zaslavsky (2015) đã đề xuất khung thông tin cổ chai, gợi ý rằng một thước đo tính tốt của đặc trưng là tối đa hóa thông tin tương hỗ giữa z và y trong khi tối thiểu hóa thông tin tương hỗ giữa z và x.

Tuy nhiên, trong những năm gần đây, thực tiễn chủ đạo là học trước một biểu diễn không phụ thuộc tác vụ bằng cách tiền huấn luyện một mạng thần kinh sâu lớn, trong một số trường hợp được biết đến như một mô hình nền tảng (Bommasani et al., 2021). Biểu diễn được học như vậy sau đó có thể được tinh chỉnh cho nhiều tác vụ cụ thể. Điều này đã được chứng minh là hiệu quả và hiệu suất hơn cho nhiều tác vụ thực tiễn trên các phương thức dữ liệu đa dạng, bao gồm giọng nói (Radford et al., 2023), ngôn ngữ (Brown et al., 2020), và hình ảnh tự nhiên (Oquab et al., 2023).
5

--- TRANG 6 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma
Chú ý rằng học biểu diễn trong bối cảnh này rất khác so với cho một tác vụ cụ thể, nơi z chỉ cần đủ tốt để dự đoán một y cụ thể. Trong cài đặt không phụ thuộc tác vụ, biểu diễn đã học z cần mã hóa gần như tất cả thông tin thiết yếu về phân phối dữ liệu x. Nghĩa là, biểu diễn đã học z không chỉ là một biểu diễn nhỏ gọn và có cấu trúc hơn cho các cấu trúc nội tại của x, mà còn có thể phục hồi x đến một mức độ trung thực nhất định. Do đó, việc hỏi trong bối cảnh không phụ thuộc tác vụ, một thước đo có nguyên tắc về tính tốt cho một biểu diễn (đặc trưng) đã học nên là gì là điều tự nhiên.¹

Về mặt khái niệm, chúng tôi lập luận rằng một cách hiệu quả, có lẽ là cách duy nhất, để xác minh xem một biểu diễn z đã mã hóa đủ thông tin về x hay không là xem chúng ta có thể phục hồi x từ z tốt như thế nào thông qua một ánh xạ (nghịch đảo), chẳng hạn g, được biết đến như bộ giải mã (hoặc bộ sinh):

x ∈ RD → f(x) → z ∈ Rd → g(z) → x̂ ∈ RD. (2)

Vì bộ mã hóa f thường là nén và mất mát, chúng ta không nên mong đợi ánh xạ nghịch đảo phục hồi x một cách chính xác, mà là một x̂ = g ∘ f(x) ≈ x gần đúng. Chúng ta thường tìm kiếm các ánh xạ mã hóa và giải mã tối ưu sao cho x̂ được giải mã gần nhất với x, hoặc theo từng mẫu—chẳng hạn, bằng cách tối thiểu hóa kỳ vọng sai số bình phương trung bình—hoặc theo nghĩa phân phối nới lỏng. Chúng tôi gọi quá trình trên là mã hóa và giải mã nén hoặc tự mã hóa nén. Ý tưởng này rất tương thích với các mục tiêu ban đầu được đặt ra cho tự mã hóa bởi Kramer (1991); Hinton và Zemel (1993), có thể được xem như một tổng quát hóa của phân tích thành phần chính cổ điển (Jolliffe, 2002) cho trường hợp cấu trúc chiều thấp của x là tuyến tính.

Thông qua nỗ lực thực nghiệm to lớn trong mười một năm qua, rõ ràng là mạng sâu rất hiệu quả trong việc mô hình hóa các ánh xạ mã hóa và giải mã phi tuyến. Nhiều ứng dụng của học sâu, bao gồm những ứng dụng được đề cập ở trên, dựa vào việc thực hiện một sơ đồ mã hóa hoặc giải mã như vậy một phần hoặc toàn bộ bằng cách học f hoặc g riêng biệt hoặc cùng nhau. Mặc dù, về mặt khái niệm, bộ giải mã g nên là "nghịch đảo" của bộ mã hóa f, trong thực tiễn không bao giờ rõ ràng làm thế nào các kiến trúc của bộ mã hóa và bộ giải mã nên liên quan với nhau. Trong nhiều trường hợp, thiết kế kiến trúc của bộ giải mã có ít liên quan đến bộ mã hóa, thường được chọn qua thử nghiệm thực nghiệm và loại bỏ (ví dụ, trong tự mã hóa có mặt nạ (He et al., 2021) và mô hình khuếch tán tiềm ẩn (Esser et al., 2020; Rombach et al., 2022)). Chúng tôi tin rằng một khung lý thuyết tốt cho học biểu diễn nên làm rõ các mối quan hệ giữa kiến trúc cho bộ mã hóa và bộ giải mã. Chúng ta phấn đấu đạt được mức độ rõ ràng này trong nghiên cứu này.

1.2 Tổng quan các Phương pháp Hiện có

Mở hộp đen của mạng sâu hiện đại thông qua nén. Cùng với sự phát triển của học sâu, nhiều kiến trúc mạng sâu đã được đề xuất và thực tiễn cho f hoặc g, từ LeNet cổ điển (LeCun et al., 1998) đến AlexNet (Krizhevsky

1. Như chúng ta biết, trong thực tiễn gần đây của học biểu diễn không phụ thuộc tác vụ, một loại kiến trúc sâu, được biết đến như transformer (Vaswani et al., 2017), đã nổi lên như một lựa chọn gần như phổ quát cho xương sống của mạng sâu, cho cả tác vụ phân biệt hoặc sinh tạo, từ ngôn ngữ đến thị giác. Chúng ta sẽ xem xét chi tiết kiến trúc này ngay sau đây. Như chúng ta sẽ thấy trong nghiên cứu này, làm rõ thước đo có nguyên tắc cho tính tốt của đặc trưng cũng là chìa khóa để hiểu đầy đủ tại sao kiến trúc giống transformer phù hợp cho tiền huấn luyện không phụ thuộc tác vụ, cũng như để tiết lộ vai trò và chức năng chính xác của mỗi lớp trong mạng sâu giống transformer.
6

--- TRANG 7 ---
Transformer Hộp Trắng thông qua Giảm Tỷ lệ Thưa

Hình 1: Các lớp mạng sâu fℓ tối ưu hóa việc giảm tỷ lệ. Các thành phần riêng biệt của phân phối dữ liệu được biến đổi bởi các toán tử mạng thành một cấu hình tối đa hóa lợi ích thông tin. Ở đây, f có thể được thực hiện bởi một ReduNet (Chan et al., 2022), trong đó mỗi lớp thực hiện một lần lặp gradient descent để tối ưu hóa việc giảm tỷ lệ.

et al., 2012), đến ResNet (He et al., 2016) và sau đó đến transformer gần đây hơn (Vaswani et al., 2017). Mặc dù phổ biến, những mạng này phần lớn được thiết kế thực nghiệm và huấn luyện và sử dụng như các bộ xấp xỉ hàm "hộp đen". Kết quả là, các tính chất mong muốn của biểu diễn đặc trưng đã học z không được chỉ định hoặc biện minh rõ ràng, và nhiều thước đo hoặc hàm mất mát heuristic đã được đề xuất và thực tiễn để huấn luyện biểu diễn không phụ thuộc tác vụ với những mô hình này.

Nghiên cứu gần đây của Yu et al. (2020); Chan et al. (2022) đã cố gắng cung cấp một khung có nguyên tắc giải thích các kiến trúc sâu của ResNet và CNN từ góc độ tối ưu hóa một thước đo "lợi ích thông tin" cho biểu diễn đã học. Khi biểu diễn có cấu trúc được tìm kiếm là một hỗn hợp các Gaussian chiều thấp, lợi ích thông tin có thể được đo chính xác bằng cái gọi là giảm tỷ lệ mã hóa, ký hiệu là ∆R(z), và được định nghĩa là sự khác biệt giữa tỷ lệ mã hóa cho tập đặc trưng như một tổng thể và tỷ lệ mã hóa cho các thành phần có cấu trúc của nó. Đã được chỉ ra rằng người ta có thể dẫn xuất từ mục tiêu này một kiến trúc mạng sâu, được biết đến như ReduNet (Yu et al., 2020; Chan et al., 2022), có sự giống nhau đáng kinh ngạc với ResNet và CNN. Các lớp của ReduNet hoàn toàn có thể diễn giải như việc thực hiện một phương pháp gradient descent lặp để tối ưu hóa mục tiêu giảm tỷ lệ mã hóa ∆R(z), như trong Hình 1:

f: x → fpre → z1 → ··· → zℓ → fℓ → zℓ+1 → ··· → fL → zL+1 = z, (3)

trong đó fpre là một ánh xạ tiền xử lý dữ liệu, và

zℓ+1 = fℓ(zℓ) ≈ zℓ + η∇∆R(zℓ) (4)

tức là, mỗi lớp ℓ được xây dựng để tối ưu hóa tăng dần ∆R(zℓ) bằng cách thực hiện một bước gradient ascent gần đúng với kích thước bước η. Chúng ta sẽ gọi một mạng có thể diễn giải toán học như vậy là mạng sâu "hộp trắng" theo nghĩa là động lực và cấu trúc của mỗi lớp mạng được hiểu rõ (tức là, như việc xấp xỉ một cải thiện tăng dần của một hàm mục tiêu mong muốn nào đó). Mặc dù giảm tỷ lệ cung cấp một khung lý thuyết tốt để hiểu kiến trúc của các mạng sâu hiện có như ResNet và CNN, việc triển khai trực tiếp ReduNet vẫn chưa tạo ra hiệu suất thực tiễn cạnh tranh trên các tập dữ liệu thực tế quy mô lớn và các tác vụ. Trong nghiên cứu này, chúng ta sẽ thấy làm thế nào khoảng cách nổi bật này giữa lý thuyết và thực tiễn² có thể được thu hẹp thông qua một tổng quát hóa và cải tiến cho mục tiêu giảm tỷ lệ sao cho toán tử gradient descent của nó giống với cấu trúc của một lớp transformer, theo cách mà kiến trúc giống transformer kết quả đạt được hiệu suất thực nghiệm cạnh tranh.

Mô hình Transformer và nén. Trong những năm gần đây, transformer (Vaswani et al., 2017) đã nổi lên như lựa chọn mô hình phổ biến nhất, gần như phổ quát, cho bộ mã hóa f và bộ giải mã g trong học biểu diễn cho dữ liệu có cấu trúc đa chiều, như văn bản (Vaswani et al., 2017; Devlin et al., 2019; Brown et al., 2020), hình ảnh (Dosovitskiy et al., 2021; Dehghani et al., 2023), và các loại tín hiệu khác (Gong et al., 2023; Arnab et al., 2021). Tóm lại, transformer đầu tiên chuyển đổi mỗi điểm dữ liệu (như một corpus văn bản hoặc hình ảnh) thành một tập hợp hoặc chuỗi các token, và sau đó thực hiện xử lý thêm trên các tập token, theo cách không phụ thuộc môi trường (Vaswani et al., 2017; Dosovitskiy et al., 2021). Nền tảng của mô hình transformer là cái gọi là lớp chú ý (tự), khai thác các tương quan thống kê giữa chuỗi token để tinh chỉnh biểu diễn token. Tuy nhiên, kiến trúc mạng transformer được thiết kế thực nghiệm và thiếu diễn giải toán học nghiêm ngặt. Trên thực tế, đầu ra của lớp chú ý bản thân nó có một số diễn giải cạnh tranh (Vidal, 2022; Li et al., 2023a; Sander et al., 2022; Geshkovski et al., 2023). Kết quả là, mối quan hệ thống kê và hình học giữa dữ liệu x và biểu diễn cuối cùng z được học bởi transformer phần lớn vẫn là một hộp đen bí ẩn.

Tuy nhiên, trong thực tiễn, transformer đã rất thành công trong việc học các biểu diễn nhỏ gọn hoạt động tốt trên nhiều tác vụ xuôi dòng. Đặc biệt, nó phục vụ như kiến trúc xương sống cho các mô hình ngôn ngữ lớn (LLM) được ca ngợi như GPT-4 của OpenAI (OpenAI, 2023b). Mặc dù lý do chính xác tại sao nó hoạt động tốt vẫn không rõ ràng, các nhà nghiên cứu OpenAI đã đưa ra giả thuyết từ quan điểm heuristic rằng kiến trúc transformer trong LLM ngầm tối thiểu hóa độ phức tạp Kolmogorov của các biểu diễn (Simons Institute, 2023), một khái niệm định lượng về nén được đo bằng độ dài của mã có thể sinh ra dữ liệu đang xem xét. Tuy nhiên, chúng ta biết rằng độ phức tạp Kolmogorov phần lớn là một khái niệm lý thuyết và nói chung không thể tính toán được cho các phân phối đa chiều. Do đó, nếu transformer trong LLM thực sự tiến hành nén, chúng nên dựa trên một thước đo độ phức tạp có thể tính toán được một cách dễ dàng và hiệu quả. Thiết kế máy Helmholtz (và máy Boltzman) dựa trên nguyên tắc độ dài mô tả tối thiểu có thể được xem như những nỗ lực sớm để làm cho nén có thể tính toán được (Hinton và Zemel, 1993). Trong nghiên cứu này, chúng tôi lập luận rằng một lựa chọn tự nhiên của thước đo nén có thể tính toán này đằng sau transformer chính xác là một kết hợp của giảm tỷ lệ và tính thưa của các biểu diễn đã học. Như chúng ta sẽ thấy, việc tiết lộ thước đo như vậy có thể là chìa khóa để hiểu kiến trúc transformer.

Mô hình Khử nhiễu-khuếch tán và nén. Mô hình khuếch tán (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song và Ermon, 2019; Song et al., 2021b,a) gần đây đã trở thành một phương pháp phổ biến để học phân phối dữ liệu đa chiều, đặc biệt là của hình ảnh tự nhiên, được biết là có cấu trúc cao theo cách khó mô hình hóa toán học nổi tiếng (Ruderman, 1994; Wakin et al., 2005; Donoho và Grimes, 2005). Khái niệm cốt lõi của mô hình khuếch tán là bắt đầu với các đặc trưng z được lấy mẫu từ một phân phối nhiễu Gaussian

2. Khoảng cách giữa lý thuyết và thực tiễn không chỉ đặc trưng cho khung giảm tỷ lệ. Tình hình cũng khắc nghiệt đối với tất cả các khung lý thuyết từng được đề xuất để hiểu mạng sâu.
8

--- TRANG 9 ---
Transformer Hộp Trắng thông qua Giảm Tỷ lệ Thưa

Hình 2: Dòng phân phối trong mô hình khử nhiễu-khuếch tán. Bắt đầu với nhiễu chung z = z̃0, mật độ xác suất của các lần lặp trung gian được định hình hướng tới phân phối thực của z̃L cục bộ và lặp đi lặp lại thông qua các toán tử gℓ, sử dụng hàm điểm ∇log qℓ tại mỗi lớp ℓ.

(hoặc một số mẫu tiêu chuẩn khác) và khử nhiễu và biến dạng phân phối đặc trưng cho đến khi nó hội tụ về phân phối dữ liệu gốc, thường có chiều nội tại thấp. Quá trình này không thể tính toán được nếu mô hình hóa chỉ ở một tỷ lệ nhiễu duy nhất (Koehler et al., 2023; Chen et al., 2023b; Bovier et al., 2005; Qin và Risteski, 2023), vì vậy nó thường được chia thành nhiều bước tăng dần khử nhiễu lặp đi lặp lại, như trong Hình 2:

g: z = z̃0 → z̃1 → ··· → z̃ℓ → gℓ → z̃ℓ+1 → ··· → z̃L → gpost → x̂, (5)

trong đó gpost là một ánh xạ hậu xử lý dữ liệu, và

z̃ℓ+1 = gℓ(z̃ℓ) = z̃ℓ + τ∇log qℓ(z̃ℓ), (6)

trong đó qℓ là mật độ của z̃ℓ, tức là, mật độ của z̃L sau khi bị hỏng với tỷ lệ thứ ℓ của nhiễu Gaussian, và ∇log qℓ là cái gọi là hàm điểm (Hyvärinen, 2005), hoặc tương đương một ước lượng cho "hàm khử nhiễu tối ưu" cho qℓ (Efron, 2011a). Trong thực tiễn, hàm điểm được mô hình hóa sử dụng một mạng sâu hộp đen chung.³ Mô hình khuếch tán đã chỉ ra hiệu quả trong việc học và lấy mẫu từ phân phối dữ liệu (Karras et al., 2022; Chen et al., 2023a; Rombach et al., 2022). Tuy nhiên, mặc dù có một số nỗ lực gần đây (Song et al., 2023), chúng thường không thiết lập bất kỳ tương ứng rõ ràng nào giữa các đặc trưng ban đầu và mẫu dữ liệu. Do đó, bản thân mô hình khuếch tán không cung cấp một biểu diễn tiết kiệm hoặc có thể diễn giải của phân phối dữ liệu. Tuy nhiên, về mặt khái niệm, quá trình khử nhiễu lặp ở trên (5) đang nén phân phối đặc trưng lên một phân phối dữ liệu chiều thấp được nhắm tới. Trong nghiên cứu này, chúng tôi sẽ chỉ ra rằng nếu người ta nén và biến đổi một phân phối lên một hỗn hợp tiêu chuẩn của các Gaussian (chiều thấp), hàm khử nhiễu tối ưu liên quan có một dạng rõ ràng tương tự như gradient của giảm tỷ lệ và với một lớp transformer. Điều này cung cấp một con đường để lấy một bộ mã hóa f giống transformer được thiết kế để nén phân phối dữ liệu thành một biểu diễn tiết kiệm và có cấu trúc, và dẫn xuất nghịch đảo phân phối của nó thông qua một quá trình tương tự như (5), tạo ra một kiến trúc hộp trắng cho tự mã hóa nén.

Các thước đo thúc đẩy chiều thấp: tính thưa và giảm tỷ lệ. Trong cả hai phương pháp phổ biến trước đó, transformer và mô hình khử nhiễu-khuếch tán, một biểu diễn được học ngầm như một sản phẩm phụ của việc giải quyết một tác vụ xuôi dòng (ví dụ, phân loại hoặc sinh/lấy mẫu) sử dụng mạng sâu. Các mạng được sử dụng thường được chọn thực nghiệm. Do đó, khó để đảm bảo hoặc áp đặt nghiêm ngặt bất kỳ tính chất mong muốn nào cho biểu diễn đã học, ngoại trừ bằng thử và sai. Tuy nhiên, bổ sung cho những thực tiễn thực nghiệm phổ biến này, một dòng nghiên cứu đã cố gắng học rõ ràng một biểu diễn mong muốn của phân phối dữ liệu như một tác vụ tự nó; điều này thường được thực hiện bằng cách cố gắng xác định và biểu diễn rõ ràng các cấu trúc chiều thấp trong dữ liệu đầu vào. Các ví dụ cổ điển của mô hình này bao gồm các phương pháp dựa trên mô hình như mã hóa thưa (Olshausen và Field, 1997; Chen et al., 2018) và học từ điển (Aharon et al., 2006; Spielman et al., 2012; Gribonval et al., 2015; Zhai et al., 2020b), từ đó phát triển những nỗ lực sớm trong thiết kế và diễn giải kiến trúc mạng sâu như học một biểu diễn thưa (Papyan et al., 2018; Bruna và Mallat, 2013). Các phương pháp gần đây hơn thay vào đó xây dựng từ góc độ không mô hình, nơi người ta học một biểu diễn thông qua một tác vụ pretext đủ thông tin như nén tương tự và tách riêng không tương tự dữ liệu qua học đối nghịch (Tian et al., 2020; Wang et al., 2022; Bardes et al., 2022; Shwartz-Ziv và LeCun, 2023). So với các phương pháp học sâu hộp đen, cả sơ đồ học biểu diễn dựa trên mô hình và không mô hình đều có lợi thế là có thể diễn giải hơn: chúng cho phép người dùng thiết kế rõ ràng các tính chất mong muốn của biểu diễn đã học z. Ở một mức độ lớn, khung giảm tỷ lệ (Yu et al., 2020; Chan et al., 2022; Pai et al., 2023) đạt được sự cân bằng tốt giữa các phương pháp dựa trên mô hình và không mô hình ở trên. Giống như học đối nghịch, nó nhằm xác định phân phối dữ liệu bằng cách nén dữ liệu tương tự/tương quan và tách riêng dữ liệu không tương tự/không tương quan (Yu et al., 2020). Đồng thời, giống như các phương pháp dựa trên mô hình, nó tích cực ánh xạ phân phối dữ liệu tới một họ biểu diễn mong muốn, chẳng hạn một hỗn hợp các Gaussian chiều thấp (Ma et al., 2007; Vidal et al., 2016).

Tối ưu hóa được triển khai: một mô hình thống nhất cho diễn giải và thiết kế mạng. Như chúng ta đã thảo luận ở trên, các thước đo thúc đẩy chiều thấp, như tính thưa hoặc giảm tỷ lệ mã hóa, cho phép người dùng xây dựng kiến trúc mạng sâu hộp trắng (Gregor và LeCun, 2010; Chan et al., 2022) theo cách xây dựng tiến theo bằng cách triển khai một chiến lược tối ưu hóa cho mục tiêu được chọn của các biểu diễn, sao cho mỗi lớp của mạng được xây dựng thực hiện một lần lặp của thuật toán tối ưu hóa (Gregor và LeCun, 2010; Chan et al., 2022; Tolooshams và Ba, 2022). Trong nghiên cứu gần đây của mình, Hinton (2022) cũng đã bắt đầu đưa ra giả thuyết rằng vai trò của một mạng sâu, với đường truyền tiến của nó, có khả năng tối ưu hóa tính tốt đặc trưng nhất định theo từng lớp. Trong mô hình này, câu hỏi thách thức nhất là:

Thước đo cơ bản nào về tính tốt cho các biểu diễn mà một mạng sâu đang cố gắng tối ưu hóa trong đường truyền tiến của nó?

Trong mô hình tối ưu hóa được triển khai, nếu các mục tiêu mong muốn được định nghĩa hẹp, chẳng hạn chỉ thúc đẩy tính thưa (Papyan et al., 2018; Bruna và Mallat, 2013), cho đến nay đã chứng minh khó khăn để đến được các kiến trúc mạng có thể đạt được hiệu suất thực tiễn cạnh tranh trên các tập dữ liệu thực tế lớn. Nghiên cứu khác đã cố gắng dẫn xuất các kiến trúc mạng phổ biến được thiết kế thực nghiệm thông qua tối ưu hóa được triển khai trên một mục tiêu học biểu diễn được kỹ thuật ngược, như Yang et al. (2022); Hoover et al. (2023); Weerdt et al. (2023). Trong trường hợp này, hiệu suất của các mạng có thể được duy trì, nhưng mục tiêu học biểu diễn được kỹ thuật ngược thường rất phức tạp và không thể diễn giải, và các tính chất của biểu diễn tối ưu—hoặc thực sự biểu diễn thực sự được học—vẫn mờ mịt. Những phương pháp như vậy không giữ lại những lợi ích chính mong muốn của tối ưu hóa được triển khai. Như chúng tôi sẽ lập luận trong nghiên cứu này, để đo lường tính tốt của một biểu diễn đã học theo tính nhỏ gọn nội tại và sự đơn giản bên ngoài của nó, việc kết hợp thước đo tính thưa (Papyan et al., 2018; Bruna và Mallat, 2013) và giảm tỷ lệ mã hóa (Yu et al., 2020; Chan et al., 2022) là rất quan trọng. Như chúng ta sẽ thấy, sự kết hợp này sẽ giải quyết phần lớn các hạn chế nêu trên của các phương pháp hiện có chỉ dựa trên tính thưa hoặc chỉ dựa trên giảm tỷ lệ.

1.3 Mục tiêu và Đóng góp của Nghiên cứu này

Từ cuộc thảo luận trên, chúng ta có thể quan sát rằng đã có một khoảng cách rộng lớn nổi bật giữa thực tiễn và lý thuyết của học biểu diễn qua mạng sâu. Sự tiến bộ nhanh chóng trong thực tiễn học sâu chủ yếu được thúc đẩy bởi các mô hình và phương pháp hộp đen thực nghiệm thiếu diễn giải toán học rõ ràng hoặc đảm bảo nghiêm ngặt. Tuy nhiên, gần như tất cả các khung lý thuyết hiện có chỉ cố gắng giải quyết các khía cạnh hạn chế hoặc cô lập của thực tiễn, hoặc chỉ đề xuất và nghiên cứu các mô hình lý tưởng không đáp ứng được việc tạo ra hiệu suất thực tiễn có thể cạnh tranh với các đối tác thực nghiệm của chúng.

Thu hẹp khoảng cách giữa lý thuyết và thực tiễn. Do đó, mục tiêu chính của nghiên cứu này là khắc phục tình hình này với một khung hoàn chỉnh và thống nhất hơn đã cho thấy triển vọng lớn trong việc thu hẹp khoảng cách này giữa lý thuyết và thực tiễn. Một mặt, khung mới này có thể cung cấp sự hiểu biết thống nhất về nhiều phương pháp và phương pháp tưởng chừng khác biệt dựa trên mạng sâu, bao gồm mã hóa/giải mã nén (hoặc tự mã hóa), giảm tỷ lệ, và khử nhiễu-khuếch tán. Mặt khác, như chúng ta sẽ thấy, khung này có thể hướng dẫn chúng ta dẫn xuất hoặc thiết kế các kiến trúc mạng sâu không chỉ hoàn toàn có thể diễn giải toán học mà còn đạt được hiệu suất cạnh tranh trên nhiều tác vụ học trên các tập dữ liệu hình ảnh hoặc văn bản thực tế quy mô lớn.

Một lý thuyết về mạng sâu hộp trắng. Cụ thể hơn, chúng tôi đề xuất một mục tiêu thống nhất, một thước đo có nguyên tắc về tính tốt, để học các biểu diễn nhỏ gọn và có cấu trúc. Đối với một biểu diễn đã học, mục tiêu này nhằm tối ưu hóa cả độ phức tạp nội tại của nó theo giảm tỷ lệ mã hóa và sự đơn giản bên ngoài của nó theo tính thưa. Chúng tôi gọi mục tiêu này là giảm tỷ lệ thưa, được chỉ định sau trong (15) và (17). Trực giác đằng sau mục tiêu này được minh họa trong Hình 3. Để tối ưu hóa mục tiêu này, chúng tôi đề xuất học một chuỗi các ánh xạ tăng dần mô phỏng việc triển khai một sơ đồ tối ưu hóa lặp giống gradient-descent nhất định cho hàm mục tiêu. Như chúng ta sẽ thấy, điều này tự nhiên dẫn đến một kiến trúc mạng sâu giống transformer hoàn toàn là một "hộp trắng" theo nghĩa là mục tiêu tối ưu hóa, toán tử mạng, và biểu diễn đã học của nó đều hoàn toàn có thể diễn giải toán học. Chúng tôi đặt tên cho kiến trúc sâu hộp trắng như vậy là "crate", hoặc "crate-Transformer", viết tắt của Coding-RATE transformer. Chúng tôi cũng chỉ ra toán học rằng những ánh xạ tăng dần này có thể nghịch đảo theo nghĩa phân phối, và nghịch đảo của chúng bao gồm cùng một lớp toán tử toán học về cơ bản. Do đó một
11

--- TRANG 12 ---
Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, Ma

Hình 3: Các optima của giảm tỷ lệ thưa. Sau khi tiền xử lý dữ liệu đầu vào X thành một chuỗi token Z1, mạng crate của chúng ta cố gắng tối ưu hóa giảm tỷ lệ thưa của các đặc trưng token Z = ZL+1. Các biểu diễn tối ưu, theo mục tiêu giảm tỷ lệ thưa, được tuyến tính hóa—có cấu trúc không gian con tuyến tính chiều thấp—thưa—nơi các không gian con được căn chỉnh trục—và nén—tuân thủ chặt chẽ cấu trúc đó, với ít hoặc không có nhiễu. Trong phần tiếp theo, chúng ta thảo luận cách crate đạt được những biểu diễn như vậy qua việc xây dựng mỗi lớp để tối ưu hóa lặp giảm tỷ lệ thưa.

kiến trúc crate gần như giống hệt có thể được sử dụng để thực hiện bộ mã hóa, bộ giải mã, hoặc cùng nhau cho tự mã hóa.

Thực tiễn của mạng sâu hộp trắng. Để chỉ ra rằng khung này thực sự có thể thu hẹp khoảng cách giữa lý thuyết và thực tiễn, chúng tôi đã tiến hành các thí nghiệm rộng rãi trên cả dữ liệu hình ảnh và văn bản để đánh giá hiệu suất thực tiễn của mô hình crate trên một loạt các tác vụ và cài đặt học mà transformer thông thường đã thể hiện hiệu suất mạnh mẽ. Đáng ngạc nhiên, mặc dù tính đơn giản về khái niệm và cấu trúc, crate đã thể hiện hiệu suất cạnh tranh so với các đối tác hộp đen của nó trên tất cả các tác vụ và cài đặt, bao gồm phân loại hình ảnh qua học có giám sát (Dosovitskiy et al., 2021), hoàn thiện có mặt nạ không giám sát cho dữ liệu hình ảnh và ngôn ngữ (He et al., 2022; Devlin et al., 2019; Liu et al., 2019), học đặc trưng tự giám sát cho dữ liệu hình ảnh (Caron et al., 2021), và mô hình hóa ngôn ngữ qua dự đoán từ tiếp theo (Radford et al., 2018). Hơn nữa, mô hình crate thể hiện các lợi ích thực tiễn bổ sung: mỗi lớp và toán tử mạng có ý nghĩa thống kê và hình học, mô hình đã học có thể diễn giải đáng kể hơn so với transformer hộp đen, và các đặc trưng hiển thị ý nghĩa ngữ nghĩa, tức là chúng có thể dễ dàng được sử dụng để phân đoạn một đối tượng khỏi nền và phân chia nó thành các phần được chia sẻ.

Lưu ý rằng với tài nguyên hạn chế, trong nghiên cứu này chúng tôi không phấn đấu đạt hiệu suất tối tân trên tất cả các tác vụ nêu trên, điều này sẽ đòi hỏi kỹ thuật nặng hoặc tinh chỉnh rộng rãi; chúng tôi cũng không thể triển khai và thử nghiệm mô hình của mình ở quy mô công nghiệp hiện tại. Tổng thể, việc triển khai của chúng tôi cho những tác vụ này là cơ bản và đồng nhất, không có tùy chỉnh đáng kể theo tác vụ cụ thể. Tuy nhiên, chúng tôi tin rằng những thí nghiệm này đã xác minh thuyết phục rằng mô hình mạng sâu hộp trắng crate được dẫn xuất có hiệu quả phổ quát và đặt ra một đường cơ sở vững chắc cho phát triển và cải tiến kỹ thuật thêm.

Phác thảo của bài báo:
• Trong Phần 2.1, chúng tôi đưa ra một công thức chính thức cho học biểu diễn, cả về mặt khái niệm và định lượng. Chúng tôi lập luận rằng một thước đo có nguyên tắc về tính tốt cho một biểu diễn đặc trưng đã học là cái gọi là giảm tỷ lệ thưa đồng thời đặc trưng hóa lợi ích thông tin nội tại của biểu diễn và tính thưa bên ngoài của nó. Trong Phần 2.2, chúng tôi cho rằng vai trò cơ bản của một mạng sâu là tối ưu hóa mục tiêu như vậy bằng cách triển khai một sơ đồ tối ưu hóa lặp như gradient descent.

• Từ Phần 2.3 đến Phần 2.5, chúng tôi chỉ ra rằng một kiến trúc sâu giống transformer có thể được dẫn xuất từ việc triển khai một sơ đồ tối thiểu hóa xen kẽ cho mục tiêu giảm tỷ lệ thưa. Đặc biệt, trong Phần 2.3 chúng tôi dẫn xuất một lớp tự chú ý đa đầu như một bước gradient descent được triển khai để tối thiểu hóa tỷ lệ mã hóa mất mát của tập token đối với một codebook hỗn hợp Gaussian chiều thấp (đã học). Trong Phần 2.4 chúng tôi chỉ ra rằng perceptron đa lớp ngay sau tự chú ý đa đầu trong các khối transformer có thể được diễn giải như (và thay thế bởi) một lớp xây dựng một mã hóa thưa của các biểu diễn token. Điều này tạo ra một kiến trúc giống transformer hộp trắng mới, tức là hoàn toàn có thể diễn giải toán học, được gọi là crate, được tóm tắt trong Phần 2.5, nơi mỗi lớp thực hiện một bước duy nhất của thuật toán tối thiểu hóa xen kẽ để tối ưu hóa mục tiêu giảm tỷ lệ thưa.

• Trong Phần 3 chúng tôi tiết lộ một kết nối cơ bản giữa nén qua giảm tỷ lệ và quá trình khuếch tán-khử nhiễu để học một biểu diễn cho phân phối dữ liệu. Đặc biệt, chúng tôi chỉ ra rằng nếu người ta khử nhiễu các token hướng tới một họ không gian con chiều thấp, hàm điểm liên quan có một dạng rõ ràng tương tự như toán tử tự chú ý được thấy trong transformer. Chúng tôi cũng thiết lập rằng gradient descent của giảm tỷ lệ về cơ bản tiến hành khử nhiễu có cấu trúc chống lại mô hình hỗn hợp Gaussian chiều thấp (đã học) cho các token. Kết nối này cho phép chúng tôi xây dựng một bộ giải mã hộp trắng dựa trên một quá trình khuếch tán có cấu trúc, như một nghịch đảo phân phối đối với quá trình khử nhiễu có cấu trúc được thực hiện bởi bộ mã hóa crate. Người ta có thể chỉ ra rằng bộ giải mã về cơ bản chia sẻ cùng kiến trúc với bộ mã hóa, và chúng cùng nhau tạo thành một tự mã hóa hộp trắng đối xứng hoàn toàn có thể diễn giải toán học.

• Trong Phần 4 chúng tôi cung cấp kết quả thí nghiệm rộng rãi để chỉ ra rằng các mạng crate, mặc dù đơn giản và thường nhỏ hơn, đã có thể học các biểu diễn nén và thưa mong muốn trên các tập dữ liệu thực tế quy mô lớn, tất cả trong khi đạt được hiệu suất ngang ngửa với các mạng transformer dày dạn kinh nghiệm trên nhiều tác vụ và cài đặt phổ biến, bao gồm ViT cho phân loại hình ảnh, MAE cho hoàn thiện hình ảnh, DINO cho phân đoạn hình ảnh với học tự giám sát, và BERT và GPT cho hoàn thiện và dự đoán văn bản. Ngoài ra, chúng tôi thể hiện, cả định tính và định lượng, rằng các biểu diễn nội bộ của crate có thể diễn giải hơn so với transformer thị giác vanilla được huấn luyện trên phân loại hình ảnh.

Ở cuối bài báo, trong Phụ lục A đến C, chúng tôi cung cấp đầy đủ chi tiết kỹ thuật và chi tiết thí nghiệm cho các phần trên, để đảm bảo rằng tất cả các tuyên bố của chúng tôi trong phần chính có thể xác minh được và các thí nghiệm có thể tái tạo. Phụ lục D đưa ra mã giả giống PyTorch cho việc triển khai crate của chúng tôi.
13
