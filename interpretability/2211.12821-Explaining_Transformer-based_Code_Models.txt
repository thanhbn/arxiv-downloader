# 2211.12821.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/interpretability/2211.12821.pdf
# File size: 451648 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Explaining Transformer-based Code Models:
What Do They Learn? When They Do Not Work?
1stAhmad Haji Mohammadkhani
University of Calgary
Calgary, Canada
ahmad.hajimohammadkh@ucalgary.ca2ndChakkrit Tantithamthavorn
Monash University
Melbourne, Australia
Chakkrit@monash.edu3rdHadi Hemmatif
York University
Toronto, Canada
hemmati@yorku.ca
Abstract —In recent years, there has been a wide interest
in designing deep neural network-based models that automate
downstream software engineering tasks on source code, such
as code document generation, code search, and program repair.
Although the main objective of these studies is to improve the
effectiveness of the downstream task, many studies only attempt
to employ the next best neural network model, without a proper
in-depth analysis of why a particular solution works or does
not, on particular tasks or scenarios. In this paper, using an
example eXplainable AI (XAI) method (attention mechanism),
we study two recent large language models (LLMs) for code
(CodeBERT and GraphCodeBERT) on a set of software engi-
neering downstream tasks: code document generation (CDG),
code refinement (CR), and code translation (CT). Through
quantitative and qualitative studies, we identify what CodeBERT
and GraphCodeBERT learn (put the highest attention on, in
terms of source code token types), on these tasks. We also show
some of the common patterns when the model does not work as
expected (performs poorly even on easy problems) and suggest
recommendations that may alleviate the observed challenges.
Index Terms —Explainable AI (XAI), LLM, Code Models,
Interpretable, Attention, Transformer.
I. I NTRODUCTION
Large Language Models (LLMs) for code (in short: code
models) are proposed to analyze the big corpora of source
code and natural languages collected from open-source plat-
forms (e.g., GitHub and StackOverflow). Such pre-trained
code models have been used to automate various source code-
related tasks, e.g., code understanding, code generation, code
clone detection [1], defect detection [2], [3], and code
summarization [4]. Automating such software engineering
tasks has been shown to greatly improve software developers’
productivity and reduce the costs of software development.
Recent studies proposed Transformer-based code models,
e.g., CodeBERT [5], GraphCodeBERT [6], CodeGPT [7],
CodeT5 [8]. However, most of these studies often focus on
improving its accuracy— without considering its explainabil-
ity aspect . Thus, when deploying such models in practice,
practitioners still do not know why such models provide a
given recommendation or suggestion.
Let’s consider a given Python code snippet of a bubble
sort algorithm. A code summarization model may be able to
correctly summarize that the given code snippet is a bubble
sort algorithm. However, developers may not trust the models
if the models correctly generate the natural text based onindentation, white spaces, or parenthesis of the Python code
snippet, instead of the meaningful semantic information (i.e.,
bubble sort). Thus, the correct predictions generated by the
models do not guarantee that the models are learned correctly.
Therefore, a lack of explainability of the large and complex
code models could lead to a lack of adoption in practice.
In this paper , we conduct an empirical study to analyze
code models through the lens of Explainable AI. Particularly,
we focus on the two well-known code models, i.e., Code-
BERT and GraphCodeBERT with the three understanding
& generation-specific downstream tasks, i.e., Code Summa-
rization (Code →Text), Code Transformation (Code →Code),
and Code Translation (Code →Code) tasks. To explain the
predictions of these models, we leverage an attention mecha-
nism inside the Transformer architecture, which is an intrinsic
Explainable AI approach. The attention mechanism allows us
to understand what are the most important tokens in the input
sequence that contribute the most to the tokens in the output
sequences. In particular, we aim to address the following two
research questions:
What do the pre-trained code models learn?
Results: Analyzing attention scores and their distribution over
different token types shows that the models learn to focus on
specific types of tokens for each downstream task. In CDG, the
models learn to focus on the methods signatures (i.e., method
name and input arguments). While in CT, syntax, that is tokens
related to the programming language, attract more attention.
CR has a middle ground compared to the other two tasks and
has a more balanced distribution of attention. Also, it is shown
that GraphCodeBERT pays more attention to the structural
parts of the source code, rather than CodeBERT, which likely
is the result of the additional step of GraphCodeBERT for
parsing the code and leveraging the code’s data flow. These
observations are inline with what is expected from a code
model which led us to conclude that the two studied models
are indeed learning (in most cases) what they are supposed to.
When do the pre-trained code models not work?
Results . Our finding shows that there are certain situations that
cause the models to perform poorly across different tasks. For
instance, the models do not work well with samples having
long or complex source code and/or long expected answers
(model outputs). We also showed that poor performance of
the model is usually projected in its attention distribution.arXiv:2211.12821v2  [cs.SE]  28 Aug 2023

--- PAGE 2 ---
In other words, whenever the model fails to achieve a good
output for a model, it also has failed to pay enough attention to
the corresponding token types for the respective downstream
task. We have also provided some recommendations on how
to potentially alleviate these weaknesses in the paper.
These findings lead us to conclude that even though pre-
trained models have shown great results on software engineer-
ing tasks; none of them can be considered a closed problem
and there are certain aspects of these models that need more
focus through further studies. Explaining these models can
shed light on their weaknesses and provide directions for
future research.
Open Science . To foster the open science initiative, we made
the replication package publicly available at GitHub.1
II. B ACKGROUND & R ELATED WORK
Explainability is now becoming a critical concern in soft-
ware engineering. Many researchers often employed AI/ML
techniques for defect prediction, malware detection, and effort
estimation. While these AI/ML techniques can greatly improve
developers’ productivity, software quality, and end-user expe-
rience, practitioners still do not understand why such AI/ML
models made those predictions [9], [10], [11], [12], [13],
[2]. To address this challenge, researchers propose various
approaches to generate explanations at two levels:
(1) Global explanations can be generated using interpretable
machine learning techniques (e.g., decision tree, decision rules,
and logistic regression techniques) or intrinsic model-specific
techniques (e.g., ANOV A, variable importance) so the entire
predictions and recommendations process is transparent and
comprehensible. However, such intrinsic model-specific tech-
niques aim to provide global explainability, without providing
explanations to individual predictions.
(2) Local explanations , on the other hand, can be generated
using some techniques (e.g., LIME, SHAP) to explain the
predictions of complex black-box AI/ML models (e.g., neural
network, random forest). Such techniques can provide an
explanation for each individual prediction (i.e., an instance
to be explained), allowing users to better understand why the
prediction is made.
In software engineering, explainable AI has been recently
studied in the domain of defect prediction (i.e., a classification
model to predict if a file/class/method will be defective in the
future or not). In particular, the survey study by Jiarpakdee et
al.[11] found that explaining the predictions is as equally
important and useful as improving the accuracy of defect
prediction. However, their literature review found that 91%
(81/96) of the defect prediction studies only focus on improv-
ing the predictive accuracy, without considering explaining
the predictions, while only 4% of these 96 studies focus on
explaining the predictions.
Although XAI is still a very under-researched topic within
the software engineering community, very few existing XAI
studies have shown some successful usages e.g., in defect
1https://anonymous.4open.science/r/XAI-for-transformer-models-F26Aprediction. In one example, Wattanakriengkrai et al. [14]
and Pornprasit and Tantithamthavorn [15] employed model-
agnostic techniques (e.g., LIME) for line-level defect pre-
diction (e.g., predicting which lines will be defective in the
future), helping developers to localize defective lines in a cost-
effective manner. In another example, Jiarpakdee et al. [12]
and Khanan et al. [16] employed model-agnostic techniques
(e.g., LIME) for explaining defect prediction models, helping
developers better understand why a file is predicted as defec-
tive. Rajapaksha et al. [13] and Pornprasit et al. [15] proposed
local rule-based model-agnostic techniques to generate action-
able guidance to help managers chart the most effective quality
improvement plans.
A. Research Gaps
While there exist research efforts on the explainability of
classification tasks in SE domains (e.g., defect prediction),
little research is focused on transformer-based pre-trained code
models. Particularly, practitioners often raised concerns e.g.,
why this source code is generated? why this code token is
modified? . A lack of explainability of code models could
lead to a lack of trust, hindering the adoption in practice.
To address this challenge, this paper aims to address the
following research questions: (RQ1) What do the pre-trained
code models learn? and(RQ2) When do the pre-trained code
models not work?
III. E XPERIMENTAL SETUP
A. Selecting Pre-Trained Code Models
Pre-trained code models, like transformers, are deep learn-
ing models trained on extensive datasets (e.g., GitHub projects,
StackOverflow posts) for source code understanding and gen-
eration. These models, also known as language models of
code, employ self-supervision techniques, including BERT-
based architectures like Masked Language Modeling (MLM)
and Next Sentence Prediction (NSP). This training on large
corpora enables them to grasp universal representations of both
source code and programming-specific natural language. These
models offer valuable benefits for diverse downstream tasks,
eliminating the need for building new models from scratch
and enhancing reusability. Notably, recent advancements have
produced various Transformer-based pre-trained code models
(e.g., CodeBERT, GraphCodeBERT, CodeGPT, CodeT5). This
paper concentrates on two specific transformer models: Code-
BERT and GraphCodeBERT.
CodeBERT [5] is a bimodal multi-lingual pre-trained model
for a programming language (PL) and natural language (NL).
It has a multi-layer Transformer encoder, trained on Masked
Language Modeling (MLM), and Replaced Token Detection
(RTD) with both NL and PL as inputs. The model has a similar
architecture as BERT [17] and showed promising results on
multiple downstream tasks such as Code Translation, Clone
Detection, Defect Detection, etc [5]. CodeBERT has been
chosen as one of our code models since its popularity at the
time of conducting this study and the many related works that

--- PAGE 3 ---
either propose a technique based on it or use it as a comparison
baseline [18].
GraphCodeBERT [6] is a pre-trained model similar to
CodeBERT that considers the semantic-level structure of the
code as well. It uses data-flow in the pre-training stage and
uses MLM, alongside Edge Prediction and Node Alignment as
pre-training tasks. Having this feature included, the model is
able to improve the results on its benchmark tasks compared to
CodeBERT, but as we will show in our experiments, in tasks
like Code Document Generation, it shows a great drawback.
GraphCodeBERT has been selected as one of our code models
since at least in theory, it is a step up compared to CodeBERT
with information added to model from the code itself. In other
words, it belongs to another category of code models, which
helps with generalizability of our findings.
B. Fine-tuning Pre-Trained Code Models on Three Down-
stream Tasks
The existing pre-trained code models have been used for
various downstream software engineering tasks. which can be
categorized into four types: (1) Text →Text (e.g., language
translation of code documentation [7], query reformulation
[19]); (2) Text →Code (e.g., code search [20], [21]); (3)
Code→Text (e.g., code summarization [22], commit message
generation [23], [24]); and (4) Code →Code (e.g., automated
program repair [25], [26], [27], programming language trans-
lation [28], code completion [29]). In this paper, we will
focus on the following three downstream tasks.
Code Document Generation (CDG) or Code Summariza-
tion (Code →Text) is an NLP task designed to generate natural
language comments for a given source code, which could help
developers to better understand codes in software projects with
wrong or missing comments and decrease the extra time that
should be spent on reading the source code. For example,
given a Python method (“ def sum(x,y): ... ”), the NLP
model will generate natural language comments as (“This is
a summation function”).
Code Refinement (Code→Code) is an NLP task designed
to generate refined source code (e.g., a fixed version) for a
given source code (e.g., a buggy version). Code refinement
has been widely studied in the context of code review [30],
[31], [32], helping developers receive refined code that is likely
to be approved without waiting for reviewers’ feedback.
Code Translation (Code→Code) is an NLP task designed
to generate source code in one language (e.g., Java) for a given
source code in another language (e.g., C#).
C. Hyper parameter settings
During the model training, we use default parameter values
as follows: max source length of 256 and max target length
of 128 with the learning rate of 5e-4, with 16 as batch size
and training it for 100 epochs.
Both models follow the same steps to generate the output.
After training a model on a downstream task on the respective
training dataset, the model generates the output for each test
data item, token by token. That is, in the inference time, inTABLE I: Dataset Statistics.
Task Input→Output Training Validation Test Total
Code Translation Java →C# 10,300 500 1,000 11,800
Code Document
GenerationJava→NL 164,923 5,183 10,955 181,061
Python→NL 251,820 13,914 14,918 280,652
Code Refinement Java →Java 52,364 6,545 6,545 65,454
each step, some tokens (depending on the beam size, set in the
model) are chosen from the potential predicted candidates, and
this process is repeated (new tokens are added to the candidate
output string) until the model generates the end-of-sentence
token, which indicates the end of prediction.
D. Evaluating the Model Accuracy
To ensure that the explanations generated from our models
are reliable, the models should be accurate. To evaluate the
model accuracy, we use smoothed BLEU-4 score that is used
in CodeBERT’s original study [5] and is commonly used by
baseline document generation techniques [33]. For other tasks,
we use a BLEU-4 score. The BLEU-4 is the only evaluation
metric we use in this paper and from now on, unless we
explicitly say otherwise when we use the BLEU score we are
referring to the BLEU-4 score. BLEU score [34] calculates
the n-gram overlap of the prediction and the gold document
or code snippet. Since in CDG, the generated sentences are
usually short, and the higher-level n-gram is not likely to have
an overlap, CodeBERT uses a smoothed version [35] that
compensates it, by giving additional counts to higher-level n-
gram overlaps.
For the code document generation task, CodeBERT achieves
an overall BLEU score of 17.83 (19.06 and 17.65 per Python
and Java, respectively), while GraphCodeBERT achieves an
overall BLEU score of 5.3 and 4.21 for Java and Python, re-
spectively. For the code refinement task, CodeBERT achieves
a BLEU score of 91.07 with an exact match of 5.16, while
GraphCodeBERT achieves a BLEU score of 91.31 with an
exact match of 9.1. For the code translation task (Java to C#),
CodeBERT achieves a BLEU score of 79.92 with an exact
match of 59.0, while GraphCodeBERT achieves a BLEU score
of 80.58 with an exact match of 59.4.
E. Explaining the Models via Attention Scores
Transformer models are able to comprehend long dependen-
cies among words in a sentence (or tokens in a code snippet)
by benefiting from the attention mechanism. The attention
mechanism basically works with a key ( k), query ( q), and
value ( v), and dkdenotes the dimensionality of the key. In its
simplest form, it calculates the similarity between the query,
key, and values as:
Attention( q, k, v ) = softmaxqkT
√dk
V
where keys and queries are the elements in the sequence.
Calculating all the dot products of qi.kjwill result in a matrix
where each row represents the attention weight for a specific
element ito all other elements in the sequence. Afterward,

--- PAGE 4 ---
a softmax layer and multiplication with the value vector will
be applied to the matrix to obtain the weighted mean. This
means every dependency between every two elements will be
considered in the final output.
Attention is a reasonable XAI method to explain CodeBERT
and GraphCodeBERT. To calculate the attention per token,
we need the weights for the encoder-decoder attention layers.
Since we have six Transformer decoder layers stacked up, we
have six attention layers. Rather than somehow aggregating
these 6 layers’ attention values into one metric, we decided
to keep all layers’ data and analyze the different layers’ roles
in explaining the outputs. The attention weights are available
inside the model, but the Transformers library that is used
by CodeBERT and GraphCodeBERT doesn’t provide them by
default. Hence, we changed the model’s implementation to
collect them, as well. Note that the attention score for each
token is the output of a softmax layer, therefore it has a value
between 0 and 1.
IV. E XPERIMENTAL RESULTS
A. (RQ1) What do the pre-trained code models learn?
Approach . To answer this RQ, we analyze the attention
weights per token type and not individual tokens. To do so, we
first need to define a list of token types. This is a subjective
decision on what tokens types are of interest for our study. We
opt for a set of seven token types that cover all tokens and
group them according to their semantic relevance, as follows:
Method name: The method under study’s name can be one
of the main decision factors on perceiving what a method does
which is a very important step for the model, specially in some
tasks such as CDG. This only includes the main method’s
name in each sample (reminder that each input sample is the
source code for one method) and not the methods called within
the main method’s body.
Type identifiers: This category represents all the keywords
that are used for identifying token types in our source lan-
guages Python and Java. For more information on these tokens,
look at “type identifier” and “* type” node types in tree-sitter
for each language.
Language keywords: Control flow command tokens are
all bundled together for each language in this category. These
tokens are as follows: For Python: {False, None, True, and,
as, assert, async, await, break, class, continue, def, del, elif,
else, except, finally, for, from, global, if, import, in, is, lambda,
nonlocal, not, or, pass, raise, return, try, while, with, yield }and
for Java: {if, else, switch, case, while, class, enum, interface,
annotation, public, protected, private, static, abstract, final,
native, synchronized, transient, volatile, strictfp, assert, return,
throw, try, catch, finally, default, super, do, for, break, continue,
super, void, import, extends, implements, import, instanceof,
new, null, package, this, throws }.
Method calls: This category includes all the tokens that are
the name of methods invoked within the body of the method
under study. They can also be instrumental in describing what
the method is doing and may contain bugs to fix.TABLE II: Number of tokens in each category (1: CT, 2:
CDG Java, 3: CDG Python, 4: CR) for each task
Method
nameInput
variablesMethod
callVariableType
identifierLanguage
keywordsOthers Total
1 1,033 2,859 2,065 4,289 2,815 3,408 21,731 38,200
2 12,991 44,175 42,285 101,352 63,143 73,644 446,419 784,009
3 16,633 110,630 19,567 197,878 0 77,662 487,860 910,230
4 7,761 19,316 33,717 70,750 52,969 36,408 319,544 540,465
Local variables: Here we consider all variables that are
used only in the body of the method under study. That is, the
input arguments of the method are excluded.
Input variables: This category only contains the input
arguments of the method under study. We have separated it
from the Local variables category.
Others: This category represents all the tokens that are not
included in any of the categories above. Mostly tokens like
punctuation, constant values, parentheses, etc.
Note that although these token types are chosen subjectively,
the results will justify this design choice by showing that
they are among the most important tokens and not many
contributing tokens are left for the “Others” category. We
should also emphasize that the level of abstraction on what
constitutes a “token category” is up to the XAI user. For
instance, we decided to separate the Input Arguments category
from the Variable Names category, to better analyze their
effects individually, but merging the two categories is a valid
design choice as well (just in different level of abstraction).
For each sample in test data, we follow these steps to find
the distribution of attention scores over different categories:
For each generated token (each step), we take its attention
weights toward the input tokens and find their corresponding
type. Then, we accumulate the attention scores of all the tokens
in each category to get a total score for that category in that
sample. Our categories cover all tokens so the sum of all
scores for each step is equal to one. We go through the same
process for all output tokens in all code snippets of the testing
dataset and gather the accumulation of attention scores for
each category.
It’s noteworthy that the size of token categories is quite
imbalanced. For example, there is only one method name
for each sample but many tokens in the “others” category.
Therefore, we normalize the total score of each category,
according to its population as in Table II. This gives us
the attention score per token for each category. Finally, we
normalized scores of all categories, between 0 and 100 for the
purpose of easier comparison.
Among these defined categories, “Method name”, “Input
variables”, and “Local variables” are more representative of
the naming aspects of a source code. So we group them in
a higher-level category of “Naming”, while “Method calls”,
“Type identifiers”, and “Language keywords” are more related
to the structure of the code. Thus, we consider them as the
higher-level category of “Structure”. Also note that we only
report the average score of all six layers for each task and
model here, since reporting all results per layer would be too

--- PAGE 5 ---
TABLE III: Normalized attention score of the two high-level
categories of tokens, for different code models and tasks.
The results are the average of all six layers for each task.
Task Model Naming Structural Others
Code TranslationCodeXGLUE 42.36% 51.38% 6.27%
GraphCodeBERT 42.60% 51.30% 6.09%
CDG javaCodeXGLUE 63.31% 29.19% 7.50%
GraphCodeBERT 64.51% 28.19% 7.30%
CDG pythonCodeXGLUE 67.97% 23.75% 8.28%
GraphCodeBERT 74.63% 17.83% 7.54%
Code RefinementCodeXGLUE 56.46% 37.96% 5.58%
GraphCodeBERT 55.49% 38.86% 5.65%
lengthy and also the results of this RQ were quite similar over
different layers and they all followed the same patterns.
Results . In the three downstream tasks under study, we expect
different normalized attention scores per high-level category,
as follows: (a) CT is a task that heavily relies on the structure,
since the model must learn the source language structure, and
generate the equivalent structure in the target language. (b)
In CDG, the structure is less important (nested blocks and
syntax trees have less to do with the output document). On
the other hand, names are very important in this task, since
they basically describe the functionality of the source code. (c)
Finally, we expect code refinement to be in the middle of these
two ends, since both names and the structure are important in
debugging a code.
Table III shows the normalized total attention score of
each high-level category and validates our hypothesis. Code
Document Generation, as a task that heavily relies on naming,
has a considerably high normalized attention score of over
63% for the Naming category, while it pays much less attention
to the Structural token types category, compared to other tasks.
It has also a higher number for the Other category which can
be understandable considering the fact that NL comments in
the code are also part of this category. Code Translation, on the
other hand, is the only task that has more than 50% normalized
attention score for Structural tokens and less than any task
for the Naming category. Code Refinement in this comparison
holds the middle ground between the two mentioned tasks in
both categories.
In Table IV, we have a more detailed analysis for each of our
categories. We saw that both models pay more attention to the
structural tokens for CT. Getting into more details, the results
show that this attention is more focused on Method calls,
and Type identifiers rather than Language keywords, roughly
having only 6% of the normalized score. It is very interesting
that studying individually, the Method name category has
the second highest score after Method calls. Even though in
translating a code, the method name is usually unchanged, this
shows that the models considerably utilize the name of the
method to understand its functionality.
In the CDG task, we observed a great reliance on Naming
categories, the results show that the Method name category
plays the most significant role. It always has a normalizedscore of close to 40% or higher. In the absence of Type
identifiers, in GraphCodeBERT CDG python, this category
has the highest score ever among all the token types across
all tasks/models. Intuitively, this amount of importance is
justifiable, given that even humans rely a lot on the method
names to understand their functionality. In addition, in three
out of four different experiments for this task, the Input
variables have the second highest normalized attention score
(between 11% to 16%). This basically means that the models
have learned that while generating document for a method,
the main and most interesting part is the signature of the
method and not the body.
Code Refinement which holds a middle ground between
two other tasks has a very close score for four categories
of Method name, Input variable, Method call, and Variable.
Since in this task the model is supposed to look for bugs
and try to fix them, it seems that the models have learned
that fewer bugs happen in categories like Type identifier,
Language keywords, and Others. This makes sense because
we know that databases for this task are gathered from public
projects and they probably do not have syntactical errors. The
method name is also less likely to have a bug, but as we saw
in other tasks, this category always has a minimum appeal for
the models.
It is interesting that according to the table, Type identifiers
which is a category purely related to the syntax of the code
and do not include any naming, have the most contribution to
CT with a gap compared to others. It has a score of 20.67%
and 21.66% for CodeBERT and GraphCodeBERT respectively
while its score in other categories is below 14%. Also, it
is worth mentioning that GraphCodeBERT which uses the
dataflow of the code in order to capture its structure; always
has a higher score for this category compared to CodeBERT.
The same patterns appear to be valid for Variable names
and for CR. The score of this category for CR is 17.85% and
18.93%, while in other tasks the score is always below 12%.
Similarly, the Method name category has a harshly higher
score in CDG. For this task in python, this category has a
score of 39.44% and 41.04%, and in Java, it has 46.17% and
54.21% for CodeBERT and GraphCodeBERT, respectively.
Having all these observations, we can see a pattern of
importance comparing different tasks together. The method
name and input variable (basically the first line of the code
samples) are the most important categories for CDG; Method
calls and local variables play the most significant role on code
refinement, alongside the method name with slightly lower
importance. On the other hand, code translation is concerned
with type identifiers and language keywords, more than any
other task, while still caring about some naming categories, as
well. In Table III, we have aggregated the numbers for our two
main categories and we can see a pattern that we expected.
Naming tokens are important for all tasks, but less for code
translation which alternatively, cares more about structural
tokens compared to other tasks.

--- PAGE 6 ---
TABLE IV: Normalized attention score of different token
types, for different code models and tasks. The results are
the average of all six layers for each task.
Task/ModelMethod
nameInput
variablesMethod
callVariableType
identifierLanguage
keywordsOthers
CT-Code
XGLUE21.36% 9.63% 24.26% 11.36% 20.67% 6.45% 6.27%
CT-Graph
CodeBERT22.78% 7.89% 23.46% 11.93% 21.66% 6.18% 6.09%
CDG Java-Code
XGLUE39.44% 13.88% 10.49% 10.00% 13.07% 5.63% 7.50%
CDG Java-Graph
CodeBERT41.04% 15.10% 8.44% 8.38% 13.13% 6.62% 7.30%
CDG Python-Code
XGLUE46.17% 11.96% 16.00% 9.83% 0.00% 7.76% 8.28%
CDG Python-Graph
CodeBERT54.21% 12.79% 10.79% 7.64% 0.00% 7.03% 7.54%
CR-Code
XGLUE22.01% 16.60% 20.33% 17.85% 9.82% 7.81% 5.58%
CR-Graph
CodeBERT19.36% 17.19% 21.15% 18.93% 10.02% 7.69% 5.65%
B. (RQ2) When do the pre-trained code models not work?
Approach . To provide explanations on when CodeBERT and
GraphCodeBert perform well and when they fail, in this RQ,
we start by a qualitative analysis of some sample predictions.
Then we make hypotheses based on our observations and fi-
nally verify them quantitatively on the whole dataset. To define
the strong and weak performances of the models, we cannot
simply rely on the absolute values of the evaluation metric
(BLEU). Since the magnitude of the BLEU score partially
depends on how difficult or easy the document generation task
is, per sample code. Therefore, we need to somehow measure
the difficulty level of the document generation task, given a
source code.
To address this research question, we create metrics to
gauge sample complexity and evaluate model performance
accordingly.
In the context of CR and CT, where token copying predom-
inates, Levenshtein Distance (LD) between input and expected
output (’gold output’) serves as a suitable complexity metric.
By calculating LD for all dataset samples, we identify the
easiest third based on lower LDs.
For the CDG task, despite input-output disparity in program-
ming language (PL) and natural language (NL), our approach
remains similar. We determine difficulty by intersecting pre-
processed tokens in the gold output document per method with
tokens in the method’s source code.
To find the overlap between source code and output tokens,
we follow these preprocessing steps: First, we remove punc-
tuations and tokens shorter than three characters, in the output
document. Then, lemmatize those tokens using the standard
Wordnet [36] lemmatizer, offered in the NLTK package. Next,
we tokenize the source code using a parser for the respective
language. Note that due to CodeBERT and GraphCodeBERT’s
tokenizations, which may split one meaningful word into
multiple tokens, we do not use their tokenization for this
analysis. Finally, we create a set of case-insensitive tokensthat fall at the intersection of processed output and source
code tokens.
Now an easy/difficult document generation task is when the
overlap between the two sets is high/low. Therefore, the same
as the cut-offs for CR and CT, we consider the first one-third
samples with the highest overlaps as easy, and the one-third
cases with the least overlaps as hard problems, and ignore the
rest (average difficulty-level).
The above process tells us which samples are considered
hard and which ones are considered easy. Now we need to
measure the performance of models. To do so, we use BLEU
score since it is the most accepted and reliable score that is
applied in these tasks in the literature. For the accuracy, we
also take the one-third of the samples with the highest BLEU
scores as High and the samples with the one-third least BLEU
scores as Low .
Having these definitions, there will be four categories (for
the tuples of <easiness level, model accuracy >) as below:
Easy−High :This category contains test data items that
are easy problems (meaning high similarity between the input
and the gold data) that the models have achieved a High BLEU
score on them.
Hard−High :This category contains the samples labeled
asDifficult andHigh . This means even with the lack of
common tokens, the model was able to achieve a satisfactory
result in these cases.
Hard −Low :This category includes the cases that are
difficult again, and expectedly, end up with Low accuracy
for the model’s prediction.
Easy−Low :This category is the most interesting one in
this paper, since it can show the potential weaknesses of the
model and is very suitable for being analyzed and “explained”.
Samples in this group, are among the samples with higher
overlaps in their corresponding dataset that means that the
model is having rather an easy job predicting. However, the
BLEU score as our indicator of the model’s accuracy is
showing poor performance comparing to other samples.
In order to perform our manual observation (qualitative
study) for this RQ, after grouping our test dataset into these
four categories, we randomly choose 100 samples from our
target category ( Easy−Low ), and manually analyze their
outputs and attention weights.
For each sample, we record the most interesting findings
to identify the most frequent patterns. This way we develop
some hypotheses. Finally, we try to verify these hypotheses by
quantitatively studying the whole test dataset, with respect to
the hypotheses. The output of this quantitative phase is in the
form of some descriptive statistics to either confirm or reject
the observations made based on the 100 samples.
Results . Having the data divided according to the defined
groups, Table V shows the ratio of the target category popu-
lation to the whole dataset for each task-model. Next, we will
explain the observations from the manual analysis.
Observations 1: The pre-trained code models do not
work well, when the output gold document is long.

--- PAGE 7 ---
TABLE V: The ratio of the Easy-Low category population to
the whole dataset, for each task-model.
CodeBERT GraphCodeBERT
CT 11.70% 11.41%
CDG / Java 5.12% 5.59%
CDG / Python 5.87% 5.85%
CR 2.49% 4.37%
Fig. 1: BLEU scores vs. the gold document code length.
Our first observation regarding the CDG task is that in cases
with long gold documents the BLEU scores tend to be low! We
plotted the distribution of BLEU scores, according to the gold
document’s length, in Fig. 1. According to the plot, most high
BLEU scores happen when the length of the gold document
is less than 50 characters.
In general, the idea of the BLEU score is about counting the
number of n-grams that are common between the reference and
the output. Model-generated documents are usually short so for
longer sentences, there is less chance that the model chooses
the same phrasing and words with the same order. Another
plausible explanation for this observation is that a longer
document means the method implements a more complex
task and thus it is harder for the model to generate the right
documentation for the complex method.
One potential solution for this problem is forcing the model
to generate longer sequences as the output which will increase
the chance of a high BLEU score, in cases with long refer-
ence documents. But obviously, since this is not actually the
model’s fault and in these cases, a low BLEU score does not
necessarily indicate a bad prediction (like the example shown
in Fig. 2), the best way to handle this problem is considering
other evaluation metrics and ideally more subjective ones.
Observation 2: The pre-trained code models do not work
well, when the input source code is complex.
Another interesting observation is about the length of the
source code. The results show that in cases with longer code,
the BLEU score is usually lower. We started the initial analysis
with the CDG data and the results, which are summarized
in Fig. 3, show a decreasing trend of BLEU scores, by the
increase of the source code length. For example, the average
Gold document: Parse the cache control headers returning a
dictionary with values for the different directives .
Best prediction: Parse a dict of headers
BLEU score: 0.08
Overlap: 0.54
Fig. 2: A sample method broken into tokens, where the
attention values of the last layer of CodeBERT for the last
generated token(“headers” in this example) are highlighted as
shades of blue (the darker, the higher).
Fig. 3: BLEU scores vs. the source code length.
BLEU score for cases shorter and longer than 300 tokens is
0.161 and 0.149, respectively.
There are two explanations for this observation: (1) the
fixed length of the inputs in models, which basically means if
the source code’s length is higher than a fixed value (in our
experiments, 256 tokens), then the input will be truncated. This
means some tokens will not make it to the decoder, and thus
we have a potential degradation in the final scores. (2) the
increased complexity of the code. Similar to Observation 1,
longer source code means more complex logic, more objects,
and functionalities to consider for the model which probably
leads to poorer results.
According to these reasons, two basic solutions can be
suggested: (a) increasing the input threshold and (b) decreasing
the input’s length. The input threshold can be easily modified

--- PAGE 8 ---
in the training process of the model and only requires more
resources. The second option, however, is a more complex
solution that is already kind of naively implemented by the
truncation. Another potential alternative is to refactor long
methods to multiple smaller methods, then pass each method
to the models to generate documents for, and finally merge all
output documents as one document.
Next, to expand this observation on all tasks-models, and
find more statistical results, we used some common code
complexity metrics and conducted an analysis on all 8 model-
tasks to get a better understanding of the root cause of the poor
results for long code snippets. We chose ’number of tokens’,
’cyclomatic complexity’, ’nested block depth’, ’number of
variables’ as complexity metrics. To measure the difficulty of
the task, we also included the same ’Levenshtein distance’ for
CT and CR and ’overlap’ for CDG, in our analysis.
For each task-model, we have five different metrics to study,
so we have one plot per metric. In each plot, the distribution
of samples in the respective dataset, regarding that metric is
shown with blue bars, and the same distribution but only for
the target category (Easy-Low) is shown in red. With this
visualization, we can identify any difference in terms of trends
on a specific metric in the target category vs. the whole dataset.
Fig. 4 shows the results for code refinement (CR) for
CodeBERT for number of tokens and the number of variables
(all the plots for GraphCodeBERT and other metrics for
CodeBERT can be found in the public repo). As illustrated in
the plots, the Easy-Low category has a very similar distribution
to the whole dataset, except for a slightly increasing trend
for some reported metrics like the number of tokens and the
number of variables This means that the model tends to make
bad decisions, whenever the source code gets more complex
in terms of the number of tokens and variables, even if the
overlap of tokens is high. For instance, considering the number
of tokens, as a measure of code complexity, the proportion of
samples with more than 100 tokens is mostly higher in the
Easy-Low category than the share of the same samples in total.
It means that the samples with many tokens are more likely to
be assumed “Easy” in our categories (more overlaps between
input and outputs) but in fact, they are harder for the model
to understand (given the long length of the code snippet).
Fig. 5 shows the results for code translation (CT) for
the number of tokens, nested block depth, and cyclomatic
complexity for CodeBERT (all the plots for GraphCodeBERT
and other metrics for CodeBERT can be found in the public
repo). For this task, number of variables follow the same
pattern as the CR task, that is Easy-Low category is harder
based on those metrics. On the other hand, considering the
number of tokens, nested block depth, and even cyclomatic
complexity, there is a reversed connection. In other words,
samples that have smaller values of these metrics, have higher
density in Easy-Low category. For instance, in both models,
samples with tokens less than 20, are around 50% of the
target category population, even though they occupy a very
small portion of the total dataset. One plausible explanation is
that when the source code is too short (very small number of
(a)
(b)
Fig. 4: Distribution of the whole dataset (in blue) and the
Easy-Low category (in red), according to code complexity
metrics, for code refinement on CodeBERT
tokens and very few nested blocks), the model fails to translate
it properly, due to the lack of enough information/context.
Another explanation is the fact that it’s harder to maintain
a high BLEU score when the input is very short.
Fig. 6 and Fig. 7 illustrate some of the similar results for
code document generation (CDG) for CodeBERT (all the plots
for GraphCodeBERT and other metrics for CodeBERT can be
found in the public repo). In this downstream task, we can
see patterns more dependent on the language, rather than the
model. In all cases, the number of variables, number of tokens,
and nested block depth follow the same general pattern.
In these experiments, we see that both models struggle with
samples with low complexity in Java. However, they have
problems figuring out the more complex samples in Python,
too. For instance in Python, samples with more than 80 tokens
or 8 variables, have always a higher density in the Easy-Low
category compared to all of the dataset. Finally, it’s interesting
that considering the cyclomatic complexity, both models in
both languages struggle with samples with higher complexity.
So all-in-all, one can conclude that code models per-
form poorly on the code snippets with extreme values of
complexity-related metrics on either direction (i.e., both long

--- PAGE 9 ---
(a)
(b)
(c)
Fig. 5: Distribution of the whole dataset (in blue) and the
Easy-Low category (in red), according to code complexity
metrics, for code translation on CodeBERT
code with many nested blocks and tokens and also very short
code with only a few variables and tokens).
Observation 3: The pre-trained code models do not work
well, when the models fail to focus on important categories :
Finally, we analyzed the contribution of token categories
similar to RQ1, but specifically for the target category of
(Easy−Low ). In Table VI we have the normalized score of
two main categories for the target samples. We were interested
to compare the results for this category and the previous
results for the whole test dataset. Hence, we calculated the
difference between these two from Table VI and TableIII, and
the outcomes are shown in Table VII. The negative numbers
(a)
(b)
Fig. 6: Distribution of the whole dataset (blue) and the
Easy-Low category (red), based on code complexity metrics,
for code document generation on CodeBERT on Java
in this table indicate a decrease in the target category.
As the results show, there is a considerable decrease in the
scores for the Structural category in CT. While answering to
RQ1, we showed that this task mostly relied on this group of
tokens IV-A. Likewise, we have a slighter decrease in the score
of the Naming category in CDG while the naming category
also proved to be the more important category for CDG.
Both of these clues, lead us to the conclusion that the results
are less satisfying, whenever the model fails to pay enough
attention to the corresponding important token category for
a specific task. Based on this observation, potential research
questions to investigate in the future are: “Will the model work
better if we help it by tagging the token types? Can manually
amplifying the attention scores of specific categories according
to the task beneficial for the code models”.
V. L IMITATIONS
One limitation of this study is that our experiments are
only on Java and Python code. Including other datasets (as
well as Python code for CT and CR tasks) requires lots of
pre-processing to become consistent with our designs and
requirements and would go beyond one conference paper’s
scope and size. We plan to extend these analyses to other
languages in the future.
Another limitation is we used the BLEU score as our
evaluation metric for the accuracy of the model, which is

--- PAGE 10 ---
(a)
(b)
Fig. 7: Distribution of the whole dataset (blue) and the
Easy-Low category (red), based on code complexity metrics,
for code document generation on CodeBERT on Python
TABLE VI: Normalized attention score of the Easy-Low
samples, in three general categories of tokens, for different
code models and tasks. The results are the average of all six
layers for each task.
Task Model Naming Structural Others
CTCodeXGLUE 48.85% 44.51% 7.70%
GraphCodeBERT 49.51% 43.49% 7.57%
CDG javaCodeXGLUE 60.08% 33.28% 8.89%
GraphCodeBERT 63.85% 37.06% 7.43%
CDG pythonCodeXGLUE 66.37% 29.14% 8.61%
GraphCodeBERT 73.68% 24.16% 7.87%
CRCodeXGLUE 56.64% 46.29% 5.63%
GraphCodeBERT 55.82% 48.35% 5.62%
commonly used in document generation downstream tasks
to reduce the subjectivity of the results. However, as we
mentioned in the paper, it is not a comprehensive metric as it
is unable to find rephrasings or cases that the prediction is not
wrong, but doesn’t exactly match with the gold label.
The observations we made are also limited to the main
patterns we have observed in the 100 samples, manually.
Although we later quantitatively validate them, it is of course
possible that there exist some other explanations as well thatTABLE VII: The difference of normalized attention scores
for the Easy-Low samples and all samples, in two general
categories of tokens, for different code models and tasks.
The results are the average of all six layers for each task.
Task Model Naming Structural Others
CTCodeXGLUE 6.49% -7.92% 1.43%
GraphCodeBERT 6.90% -8.38% 1.48%
CDG javaCodeXGLUE -3.24% 1.85% 1.39%
GraphCodeBERT -0.66% 0.53% 0.13%
CDG pythonCodeXGLUE -1.59% 1.27% 0.33%
GraphCodeBERT -0.96% 0.63% 0.33%
CRCodeXGLUE 0.18% -0.23% 0.06%
GraphCodeBERT 0.33% -0.30% -0.03%
we have missed observing, due to the samples we have chosen.
In addition, the study is only limited to three downstream
tasks (CT, CDG, and CR) and two code models (CodeBERT
and GraphCodeBERT). More work is required to generalize
the findings for other Transformer-based models, in the future.
Worth mentioning is the implementation of the CDG task
within the GraphCodeBERT framework, which was executed
employing identical settings and hyperparameters as the other
tasks and models. This uniform approach might contribute to
the observed lower accuracy in the performance of the CDG
task within the model.
Finally, this study was conducted before ChatGPT gone
public. So a very relevant extension of this work would be
including GPT-4 model both as a code model as well as a
XAI method to provide explanations on the decisions of itself
and other models.
VI. C ONCLUSION AND FUTURE WORKS
This paper proposes an approach for explaining pre-trained
code models (e.g., CodeBert and GraphCodeBert), using their
internal end-to-end attention mechanism, as the XAI method.
Unlike most XAI research, where the explanation is applied to
high-accuracy models to make sure the results are trustworthy,
we have used XAI on both high (to find out what the models
learn) and low-accuracy scenarios (to see when they do not
perform well). Our findings not only provide observations
about what these state-of-the-art Transformer-based models
learn in terms of token type categories and why they un-
derperform in some scenarios, but also suggest actionable
recommendations, such as using more subjective evaluation
metrics for CDG task, giving token types as additional input
to the model, and manually amplifying attention scores for
specific token types. In the future, we plan to extend this
work by examining other downstream tasks, models, and XAI
methods. In addition, we also plan to work on pre-trained code
models by implementing the suggested recommendations from
our observations. Finally, we plan to use GPT-4 model to both
access its results as code model and use it as an XAI technique
to explain why a particular output was provided by the model.

--- PAGE 11 ---
REFERENCES
[1] G. Shobha, A. Rana, V . Kansal, and S. Tanwar, “Code clone detec-
tion—a systematic review,” Emerging Technologies in Data Mining and
Information Security , pp. 645–655, 2021.
[2] C. Pornprasit, C. Tantithamthavorn, J. Jiarpakdee, M. Fu, and P. Thong-
tanunam, “Pyexplainer: Explaining the predictions of just-in-time defect
models,” in 2021 36th IEEE/ACM International Conference on Auto-
mated Software Engineering (ASE) . IEEE, 2021, pp. 407–418.
[3] J. Humphreys and H. K. Dam, “An explainable deep model for defect
prediction,” in 2019 IEEE/ACM 7th International Workshop on Realiz-
ing Artificial Intelligence Synergies in Software Engineering (RAISE) .
IEEE, 2019, pp. 49–55.
[4] A. LeClair, S. Haque, L. Wu, and C. McMillan, “Improved code
summarization via a graph neural network,” in Proceedings of the 28th
international conference on program comprehension , 2020, pp. 184–195.
[5] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin,
T. Liu, D. Jiang et al. , “Codebert: A pre-trained model for programming
and natural languages,” in Findings of the Association for Computational
Linguistics: EMNLP 2020 , 2020, pp. 1536–1547.
[6] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan,
A. Svyatkovskiy, S. Fu et al. , “Graphcodebert: Pre-training code repre-
sentations with data flow,” arXiv preprint arXiv:2009.08366 , 2020.
[7] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco,
C. Clement, D. Drain, D. Jiang, D. Tang et al. , “Codexglue: A machine
learning benchmark dataset for code understanding and generation,”
arXiv preprint arXiv:2102.04664 , 2021.
[8] Y . Wang, W. Wang, S. Joty, and S. C. Hoi, “Codet5: Identifier-aware
unified pre-trained encoder-decoder models for code understanding and
generation,” arXiv preprint arXiv:2109.00859 , 2021.
[9] C. Tantithamthavorn, J. Jiarpakdee, and J. Grundy, “Actionable analytics:
Stop telling me what it is; please tell me what to do,” IEEE Software ,
vol. 38, no. 4, pp. 115–120, 2021.
[10] C. K. Tantithamthavorn and J. Jiarpakdee, “Explainable ai for software
engineering,” in 2021 36th IEEE/ACM International Conference on
Automated Software Engineering (ASE) . IEEE, 2021, pp. 1–2.
[11] J. Jiarpakdee, C. K. Tantithamthavorn, and J. Grundy, “Practitioners’
perceptions of the goals and visual explanations of defect prediction
models,” in 2021 IEEE/ACM 18th International Conference on Mining
Software Repositories (MSR) . IEEE, 2021, pp. 432–443.
[12] J. Jiarpakdee, C. K. Tantithamthavorn, H. K. Dam, and J. Grundy,
“An empirical study of model-agnostic techniques for defect prediction
models,” IEEE Transactions on Software Engineering , vol. 48, no. 1,
pp. 166–185, 2020.
[13] D. Rajapaksha, C. Tantithamthavorn, J. Jiarpakdee, C. Bergmeir,
J. Grundy, and W. Buntine, “Sqaplanner: Generating data-informed
software quality improvement plans,” IEEE Transactions on Software
Engineering , vol. 48, no. 8, pp. 2814–2835, 2021.
[14] S. Wattanakriengkrai, P. Thongtanunam, C. Tantithamthavorn, H. Hata,
and K. Matsumoto, “Predicting defective lines using a model-agnostic
technique,” IEEE Transactions on Software Engineering , vol. 48, no. 5,
pp. 1480–1496, 2020.
[15] C. Pornprasit and C. K. Tantithamthavorn, “Jitline: A simpler, better,
faster, finer-grained just-in-time defect prediction,” in 2021 IEEE/ACM
18th International Conference on Mining Software Repositories (MSR) .
IEEE, 2021, pp. 369–379.
[16] C. Khanan, W. Luewichana, K. Pruktharathikoon, J. Jiarpakdee, C. Tan-
tithamthavorn, M. Choetkiertikul, C. Ragkhitwetsagul, and T. Sunet-
nanta, “Jitbot: An explainable just-in-time defect prediction bot,” in
2020 35th IEEE/ACM International Conference on Automated Software
Engineering (ASE) . IEEE, 2020, pp. 1336–1339.
[17] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
of deep bidirectional transformers for language understanding,” arXiv
preprint arXiv:1810.04805 , 2018.
[18] C. Pan, M. Lu, and B. Xu, “An empirical study on software defect
prediction using codebert model,” Applied Sciences , vol. 11, no. 11, p.
4793, 2021.
[19] K. Cao, C. Chen, S. Baltes, C. Treude, and X. Chen, “Automated
query reformulation for efficient search based on query logs fromstack overflow,” in 2021 IEEE/ACM 43rd International Conference on
Software Engineering (ICSE) . IEEE, 2021, pp. 1273–1285.
[20] X. Gu, H. Zhang, and S. Kim, “Deep code search,” in 2018 IEEE/ACM
40th International Conference on Software Engineering (ICSE) . IEEE,
2018, pp. 933–944.
[21] T. Nguyen, P. C. Rigby, A. T. Nguyen, M. Karanfil, and T. N. Nguyen,
“T2api: Synthesizing api code usage templates from english texts with
statistical translation,” in Proceedings of the 2016 24th ACM SIGSOFT
International Symposium on Foundations of Software Engineering , 2016,
pp. 1013–1017.
[22] S. Haque, A. LeClair, L. Wu, and C. McMillan, “Improved auto-
matic summarization of subroutines via attention to file context,” in
Proceedings of the 17th International Conference on Mining Software
Repositories , 2020, pp. 300–310.
[23] S. Jiang, A. Armaly, and C. McMillan, “Automatically generating
commit messages from diffs using neural machine translation,” in
2017 32nd IEEE/ACM International Conference on Automated Software
Engineering (ASE) . IEEE, 2017, pp. 135–146.
[24] L. Liu, X. Hu, W. Song, R. Fu, T. Liu, and G. Hu, “Neural multitask
learning for simile recognition,” in Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Processing , 2018, pp. 1543–
1553.
[25] N. Jiang, T. Lutellier, and L. Tan, “Cure: Code-aware neural machine
translation for automatic program repair,” in 2021 IEEE/ACM 43rd
International Conference on Software Engineering (ICSE) . IEEE, 2021,
pp. 1161–1173.
[26] Y . Li, S. Wang, and T. N. Nguyen, “Dlfix: Context-based code trans-
formation learning for automated program repair,” in Proceedings of
the ACM/IEEE 42nd International Conference on Software Engineering ,
2020, pp. 602–614.
[27] Z. Chen, S. Kommrusch, M. Tufano, L.-N. Pouchet, D. Poshyvanyk,
and M. Monperrus, “Sequencer: Sequence-to-sequence learning for end-
to-end program repair,” IEEE Transactions on Software Engineering ,
vol. 47, no. 9, pp. 1943–1959, 2019.
[28] B. Roziere, M.-A. Lachaux, L. Chanussot, and G. Lample, “Unsu-
pervised translation of programming languages,” Advances in Neural
Information Processing Systems , vol. 33, pp. 20 601–20 611, 2020.
[29] A. Svyatkovskiy, S. K. Deng, S. Fu, and N. Sundaresan, “Intellicode
compose: Code generation using transformer,” in Proceedings of the
28th ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering , 2020, pp.
1433–1443.
[30] R. Tufano, S. Masiero, A. Mastropaolo, L. Pascarella, D. Poshyvanyk,
and G. Bavota, “Using pre-trained models to boost code review automa-
tion,” in Proceedings of the 44th International Conference on Software
Engineering , 2022, pp. 2291–2302.
[31] P. Thongtanunam, C. Pornprasit, and C. Tantithamthavorn, “Autotrans-
form: Automated code transformation to support modern code review
process,” in Proceedings of the 44th International Conference on Soft-
ware Engineering , 2022, pp. 237–248.
[32] Y . Liu, C. Tantithamthavorn, Y . Liu, P. Thongtanunam, and L. Li,
“Autoupdate: Automatically recommend code updates for android apps,”
arXiv preprint arXiv:2209.07048 , 2022.
[33] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, “Deep code comment gener-
ation,” in 2018 IEEE/ACM 26th International Conference on Program
Comprehension (ICPC) . IEEE, 2018, pp. 200–20 010.
[34] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for
automatic evaluation of machine translation,” in Proceedings of the 40th
annual meeting of the Association for Computational Linguistics , 2002,
pp. 311–318.
[35] C.-Y . Lin and F. J. Och, “Orange: a method for evaluating automatic
evaluation metrics for machine translation,” in COLING 2004: Proceed-
ings of the 20th International Conference on Computational Linguistics ,
2004, pp. 501–507.
[36] E. Loper and S. Bird, “Nltk: The natural language toolkit,” CoRR ,
vol. cs.CL/0205028, 2002. [Online]. Available: http://dblp.uni-trier.de/
db/journals/corr/corr0205.html#cs-CL-0205028
