# 2211.00251.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/interpretability/2211.00251.pdf
# Kích thước tệp: 1617732 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Lựa chọn Mô hình Khả vi cho Học tập Ensemble
James Kotary1,Vincenzo Di Vito1và Ferdinando Fioretto2
1Đại học Syracuse
2Đại học Virginia
fjkotary, vdivitofg@syr.edu, nandoﬁoretto@gmail.com
Tóm tắt
Lựa chọn mô hình là một chiến lược nhằm tạo ra các mô hình chính xác và mạnh mẽ. Một thử thách chính trong việc thiết kế các thuật toán này là xác định mô hình tối ưu để phân loại bất kỳ mẫu đầu vào cụ thể nào. Bài báo này giải quyết thử thách này và đề xuất một khung làm việc mới cho lựa chọn mô hình khả vi tích hợp học máy và tối ưu hóa tổ hợp. Khung làm việc được thiết kế riêng cho học tập ensemble, một chiến lược kết hợp các đầu ra của các mô hình được huấn luyện trước riêng lẻ, và học cách lựa chọn các thành viên ensemble phù hợp cho một mẫu đầu vào cụ thể bằng cách biến đổi nhiệm vụ học tập ensemble thành một chương trình lựa chọn khả vi được huấn luyện end-to-end trong mô hình học tập ensemble. Được thử nghiệm trên các nhiệm vụ khác nhau, khung làm việc được đề xuất thể hiện tính linh hoạt và hiệu quả của nó, vượt trội hơn các quy tắc đồng thuận thông thường và tiên tiến trên nhiều thiết lập và nhiệm vụ học tập khác nhau.

1 Giới thiệu
Lựa chọn mô hình liên quan đến quá trình xác định mô hình phù hợp nhất từ một tập hợp các ứng viên cho một nhiệm vụ học tập nhất định. Mô hình được chọn lý tưởng nên khái quát hóa tốt cho dữ liệu chưa thấy, với độ phức tạp của mô hình đóng vai trò quan trọng trong quá trình lựa chọn này. Tuy nhiên, việc cân bằng giữa underfitting và overfitting là một thử thách đáng kể.

Nhiều kỹ thuật khác nhau đã được trình bày trong tài liệu học máy để giải quyết vấn đề này. Đặc biệt có liên quan, học tập ensemble [Witten et al., 2005] là một meta-algorithm kết hợp các đầu ra của các mô hình được huấn luyện trước riêng lẻ, được gọi là base learners, để cải thiện hiệu suất tổng thể. Mặc dù được huấn luyện để thực hiện cùng một nhiệm vụ, các base learners này có thể thể hiện sự đa dạng lỗi, có nghĩa là chúng thất bại trên các mẫu khác nhau, và hồ sơ độ chính xác của chúng bổ sung cho nhau trên toàn bộ phân phối của các mẫu thử nghiệm.

Hiệu quả tiềm năng của một mô hình ensemble phụ thuộc mạnh vào mối tương quan giữa các lỗi của base learners trên các mẫu đầu vào và độ chính xác của chúng; những mô hình có độ chính xác cao hơn và sự đa dạng lỗi có tiềm năng cao hơn để cải thiện độ chính xác ensemble [Mienye and Sun, 2022].

Tuy nhiên, nhiệm vụ xác định ensemble tối ưu của các mô hình để phân loại bất kỳ mẫu đầu vào cụ thể nào là không tầm thường. Các phương pháp truyền thống thường tổng hợp dự đoán trên tất cả base learners của một ensemble, nhằm làm cho dự đoán mạnh mẽ hơn đối với lỗi của các base learners riêng lẻ. Trong khi các kỹ thuật này có thể được cải thiện bằng cách áp dụng có chọn lọc chúng cho một tập con của base learners được biết là đáng tin cậy hơn trên các đầu vào nhất định, việc thiết kế các thuật toán hiệu quả lựa chọn và kết hợp các dự đoán riêng lẻ của base learners vẫn là một nỗ lực phức tạp. Nhiều phương pháp dựa trên quy tắc đồng thuận áp dụng các sơ đồ tổng hợp kết hợp hoặc loại trừ dự đoán của base learners dựa trên các quy tắc tĩnh, do đó bỏ lỡ cơ hội thông báo cho việc lựa chọn ensemble dựa trên các đặc trưng của một đầu vào cụ thể.

Gần đây, khái niệm lựa chọn mô hình khả vi đã xuất hiện, nhằm tích hợp quá trình lựa chọn mô hình vào chính quá trình huấn luyện [Dona and Gallinari, 2021; Sheth and Fusi, 2020; Fu et al., 2016]. Phương pháp này tận dụng các phương pháp dựa trên gradient để tối ưu hóa lựa chọn mô hình, chứng minh đặc biệt có lợi trong các lĩnh vực như tìm kiếm kiến trúc mạng neural. Động lực đằng sau lựa chọn mô hình khả vi nằm ở tiềm năng tự động hóa và tối ưu hóa quá trình lựa chọn mô hình, do đó dẫn đến các mô hình ưu việt và các thủ tục lựa chọn hiệu quả hơn. Mặc dù có những hứa hẹn, tuy nhiên, việc thiết kế các chiến lược lựa chọn mô hình khả vi hiệu quả vẫn không tầm thường và việc sử dụng các phương pháp dựa trên gradient một mình còn tăng thêm rủi ro hội tụ đến các tối ưu cục bộ có thể dẫn đến lựa chọn mô hình dưới tối ưu.

Trong bối cảnh những thử thách này, bài báo này đề xuất một khung làm việc mới cho lựa chọn mô hình khả vi được thiết kế riêng cho học tập ensemble. Khung làm việc này tích hợp học máy và tối ưu hóa tổ hợp để học việc lựa chọn các thành viên ensemble bằng cách mô hình hóa quá trình lựa chọn như một bài toán tối ưu hóa dẫn đến các lựa chọn tối ưu trong bối cảnh được quy định.

Đóng góp. Chi tiết hơn, bài báo này đưa ra các đóng góp sau: (1) Nó đề xuất end-to-end Combinatorial Ensemble Learning (e2e-CEL), một khung làm việc học tập ensemble mới khai thác sự tích hợp của ML và tối ưu hóa tổ hợp để biên dịch quy tắc đồng thuận chuyên biệt cho một mẫu đầu vào cụ thể. (2) Nó cho thấy cách chuyển đổi việc lựa chọn và tổng hợp dự đoán của base learners ensemble thành một bài toán tối ưu hóa khả vi, được tham số hóa bởi một mạng neural sâu và được huấn luyện end-to-end trong nhiệm vụ học tập ensemble. (3) Một phân tích các nhiệm vụ học tập thử thách thể hiện điểm mạnh của ý tưởng này: e2e-arXiv:2211.00251v2 [cs.LG] 19 May 2023

--- TRANG 2 ---
CEL vượt trội hơn các mô hình cố gắng lựa chọn các thành viên ensemble riêng lẻ, chẳng hạn như sự kết hợp có trọng số tối ưu của các dự đoán của các thành viên ensemble riêng lẻ cũng như các quy tắc đồng thuận thông thường, ngụ ý khả năng cao hơn nhiều để tận dụng sự đa dạng lỗi.

Những kết quả này chứng minh việc tích hợp tối ưu hóa có ràng buộc và học tập là một yếu tố then chốt để nâng cao hiệu quả của lựa chọn mô hình trong các nhiệm vụ học máy.

2 Công trình Liên quan
Học tập ensemble bao gồm hai bước: huấn luyện các base learners riêng lẻ và kết hợp đầu ra của chúng để có dự đoán chính xác. Việc cấu tạo một ensemble từ các base learners với hồ sơ lỗi bổ sung thường được thực hiện thông qua bagging (tạo ngẫu nhiên các tập dữ liệu để huấn luyện từng thành viên) và boosting (tạo thích ứng các tập dữ liệu dựa trên phân phối lỗi để tăng sự đa dạng lỗi). Một khảo sát về huấn luyện các base learners riêng lẻ có thể được tìm thấy trong Mienye and Sun [2022]. Bước thứ hai thường được xử lý bởi các quy tắc tổng hợp cổ điển trên các dự đoán hoặc giá trị kích hoạt của các thành viên ensemble, chẳng hạn như bỏ phiếu đa số hoặc bỏ phiếu đa nguyên. Một số công trình cũng đã cố gắng mô hình hóa toán học các quy tắc tổng hợp hiệu quả hơn, chẳng hạn như thuật toán Super Learner [Ju et al., 2018] tạo thành một sự kết hợp có trọng số của các mô hình base learner tối đa hóa độ chính xác trên một tập validation. Thuật toán này đã được chứng minh là tối ưu tiệm cận để kết hợp dự đoán của các thành viên ensemble.

Bài báo này giải quyết khía cạnh sau, thử thách, của mô hình hóa ensemble: tối ưu hóa việc tổng hợp dự đoán từ các base learners ensemble riêng lẻ. Phương pháp e2e-CEL được đề xuất nhằm học các quy tắc tổng hợp một cách thích ứng ở mức độ của các mẫu đầu vào riêng lẻ, thay vì một quy tắc duy nhất cho tất cả các mẫu. Trong khi các quy tắc lựa chọn dựa trên heuristic để rút ra các ensemble phụ thuộc vào đầu vào không phải là mới đối với tài liệu, theo hiểu biết tốt nhất của chúng tôi, đây là đề xuất đầu tiên về một phương pháp học các quy tắc có điều kiện như vậy theo cách end-to-end. Một thảo luận về công trình bổ sung được hoãn lại đến Phụ lục A.

3 Thiết lập và Mục tiêu
Bài báo xem xét các ensemble như một bộ sưu tập n mô hình hoặc base learners được biểu diễn bởi các hàm fi; 1 ≤ i ≤ n, được huấn luyện độc lập trên các tập dữ liệu riêng biệt (nhưng có thể chồng lấp) (Xi, Yi), tất cả trên cùng một nhiệm vụ phân loại dự định. Trong mọi nhiệm vụ được nghiên cứu, nó giả định rằng (Xi, Yi) được cung cấp, cùng với một quy định để huấn luyện từng base learners, để fi được giả định là được cấu hình trước. Thiết lập này phổ biến trong các bối cảnh phân tích liên kết, nơi các base learners thường được huấn luyện trên các tập dữ liệu đa dạng với phân phối lệch [Kairouz et al., 2021], và trong các dịch vụ ML, nơi các nhà cung cấp cung cấp một loạt các mô hình được huấn luyện trước với các lựa chọn kiến trúc và siêu tham số khác nhau [Ribeiro et al., 2015].

Cho n ∈ N là số lượng base learners, c ∈ N số lượng lớp và d ∈ N kích thước đặc trưng đầu vào. Cho một mẫu z ∈ Rd, mỗi base learner fj: Rd → Rc tính toán fj(z) = ŷj. Đối với các nhiệm vụ phân loại được xem xét trong bài báo này, mỗi ŷj là đầu ra trực tiếp của một hàm softmax Rc → Rc,
softmax(c)i = eci / ∑k=1^c eck: (1)

Cụ thể, mỗi bộ phân loại fi(θi; x) được huấn luyện với tham số θi để tối thiểu hóa một loss phân loại L như
min θi E(x,y)∼(Xi,Yi)[L(fi(θi; x), y)]: (2)

Mục tiêu sau đó là kết hợp các base learners thành một ensemble, có bộ phân loại tổng hợp g thực hiện cùng một nhiệm vụ, nhưng với độ chính xác tổng thể lớn hơn trên một tập dữ liệu chủ (X, Y), trong đó Xi ⊆ X và Yi ⊆ Y cho tất cả i với 0 ≤ i ≤ n:
min θ E(x,y)∼(X,Y)[L(g(θ; x), y)]: (3)

Như điển hình trong học tập ensemble, các base learners có thể được huấn luyện theo cách tăng sự đa dạng lỗi thử nghiệm giữa fi trên X— xem Phần 5. Trong mỗi tập dữ liệu có một phân chia train/test/validation ngụ ý, để việc đánh giá một mô hình được huấn luyện luôn được thực hiện trên phần test của nó. Khi cần sự phân biệt này, các ký hiệu Xtrain, Xvalid, Xtest được sử dụng. Một danh sách các ký hiệu được sử dụng trong bài báo để mô tả các khía cạnh khác nhau của tính toán, cùng với ý nghĩa của chúng được cung cấp trong [Kotary et al., 2022], Bảng 4.

4 End-to-end Combinatorial Ensemble Learning
Lý tưởng nhất, cho một ensemble được huấn luyện trước fi; 1 ≤ i ≤ n và một mẫu z ∈ X, người ta sẽ lựa chọn từ ensemble một bộ phân loại được biết là tạo ra một dự đoán lớp chính xác cho z. Tuy nhiên, một đánh giá hiệu suất cho dự đoán của từng base learners không có sẵn tại thời điểm test. Do đó, các sơ đồ học tập ensemble thông thường sử dụng các tiêu chí lựa chọn chẳng hạn như bỏ phiếu đa nguyên (xem Phần 5 để mô tả các quy tắc tổng hợp được sử dụng ở đây như một benchmark).

Sơ đồ học tập end-to-end trong công trình này dựa trên ý tưởng rằng một dự đoán ensemble chính xác hơn có thể được thực hiện bằng cách sử dụng dự đoán dựa trên z, và việc lựa chọn một tập con được chọn tốt của ensemble, thay vì toàn bộ ensemble, có thể cung cấp kết quả đáng tin cậy hơn so với một base learner duy nhất. Kích thước của tập con, k, được coi như một siêu tham số. Trong khi có vẻ hợp lý khi chỉ chọn base learner được dự đoán tốt nhất cho một mẫu đầu vào nhất định (đặt k = 1), đã được quan sát một cách nhất quán trong các thí nghiệm rằng hiệu suất tối ưu đạt được khi 1 < k < n:

Cơ chế được đề xuất chuyển đổi việc lựa chọn sub-ensemble thành một chương trình tối ưu hóa end-to-end khả vi và do đó có thể được tích hợp với một mô hình học tập g để lựa chọn một tập con đáng tin cậy của các base learners ensemble để kết hợp cho dự đoán. Một end-to-end Combinatorial Ensemble Learning (e2e-CEL), hoặc đơn giản, smart ensemble, bao gồm một ensemble của các base learners cùng với một module được huấn luyện bởi e2e-CEL để lựa chọn sub-ensemble có kích thước k, tạo ra dự đoán kết hợp chính xác nhất cho một đầu vào nhất định. Mô hình g được gọi là selection net, và mô hình ensemble end-to-end được huấn luyện bằng cách tối ưu hóa tham số θ của nó.

--- TRANG 3 ---
Hình 1: Sơ đồ Học tập Ensemble End-to-end: Mũi tên đen và đỏ minh họa các hoạt động forward và backward, tương ứng.

Tổng quan E2e-CEL. E2e-CEL bao gồm ba bước chính:
1. Dự đoán một vector điểm số g(z) = ĉ, ước tính độ chính xác dự đoán cho mỗi base learner trên mẫu z.
2. Xác định các chỉ số base learner E ⊆ [n] tương ứng với k điểm số được dự đoán cao nhất.
3. Thu thập các dự đoán của sub-ensemble được chọn fj(z) và thực hiện một sơ đồ bỏ phiếu đa số xấp xỉ trên những dự đoán đó để xác định lớp của z.

Bằng cách huấn luyện trên tập chủ Xtrain, smart ensemble học cách đưa ra dự đoán tốt hơn bằng việc học cách lựa chọn sub-ensembles tốt hơn để bỏ phiếu trên các mẫu đầu vào của nó. Tuy nhiên, lưu ý rằng việc lựa chọn tập con và bỏ phiếu đa nguyên là các hoạt động rời rạc, và ở dạng đơn giản không cung cấp gradients hữu ích cho backpropagation và học tập. Các phần tiếp theo thảo luận chi tiết hơn về khung làm việc e2e-CEL, bao gồm các xấp xỉ khả vi cho từng bước của mô hình tổng thể.

Hình 1 minh họa mô hình e2e-CEL và quá trình huấn luyện của nó theo các hoạt động thành phần. Backpropagation được hiển thị với mũi tên đỏ, và nó chỉ áp dụng cho các hoạt động downstream từ selection net g, vì e2e-CEL được tham số hóa chỉ bởi các tham số của g.

4.1 Lựa chọn Mô hình Khả vi
Hệ thống e2e-CEL dựa trên việc học cách lựa chọn k < n dự đoán từ ensemble chủ, cho một tập hợp các đặc trưng đầu vào. Điều này có thể được thực hiện bằng cách dự đoán có cấu trúc của các giá trị nhị phân, sau đó được sử dụng để che dự đoán của các base learner riêng lẻ.

Xem xét bài toán knapsack không có trọng số
K(ĉ) = argmax b ĉᵀb (4a)
subject to 1ᵀb = k; (4b)
b ∈ {0,1}ⁿ; (4c)

có thể được xem như một bài toán lựa chọn có nghiệm tối ưu gán giá trị 1 cho các phần tử của b liên quan đến k giá trị cao nhất của ĉ. Nới lỏng ràng buộc (4c) thành 0 ≤ b ≤ 1 dẫn đến một chương trình tuyến tính (LP) tương đương với các nghiệm tối ưu rời rạc b ∈ {0,1}ⁿ, mặc dù vừa lồi vừa được cấu tạo từ các hàm liên tục. Tính chất hữu ích này tồn tại cho bất kỳ LP nào với các ràng buộc totally unimodular và các hệ số phía phải nguyên [Bazaraa et al., 2008].

Bài toán tối ưu hóa này có thể được xem như một ánh xạ từ ĉ đến một vector nhị phân chỉ ra k giá trị cao nhất của nó, và do đó đại diện cho một ứng viên tự nhiên để lựa chọn sub-ensemble tối ưu có kích thước k cho các điểm số được dự đoán của các base learners riêng lẻ, được xem như ĉ. Tuy nhiên, các đầu ra của Bài toán (4) định nghĩa một hàm hằng từng đoạn, K(ĉ), không thừa nhận gradients thông tin sẵn có, đặt ra thử thách cho tính khả vi. Để tích hợp vào hệ thống học tập end-to-end, hàm K(ĉ) phải cung cấp gradients thông tin đối với ĉ. Trong công trình này, thử thách này được khắc phục bằng cách làm mịn K(ĉ) dựa trên việc nhiễu ĉ với nhiễu ngẫu nhiên.

Như được quan sát bởi Berthet et al. [2020], bất kỳ bài toán lập trình tuyến tính liên tục, lồi nào cũng có thể được sử dụng để định nghĩa một perturbed optimizer khả vi, tạo ra các nghiệm xấp xỉ giống nhau nhưng khả vi đối với các hệ số mục tiêu tuyến tính của nó. Cho một biến nhiễu ngẫu nhiên Z có mật độ xác suất p(z) ∝ exp(-v(z)) trong đó v là một hàm khả vi hai lần,
K̄(ĉ) = EzĨZ[K(ĉ + z)]; (5)

là một perturbed optimizer khả vi liên quan đến K. Tham số nhiệt độ τ > 0 kiểm soát độ nhạy của gradients (hoặc đúng hơn, ma trận Jacobian), có thể tự nó được biểu diễn bởi giá trị kỳ vọng [Abernethy et al., 2016]:
∂K̄(ĉ)/∂ĉ = EzĨZ[K(ĉ + z)v'(z)ᵀ/τ]: (6)

Trong công trình này, Z được mô hình hóa như một biến ngẫu nhiên Normal chuẩn. Trong khi những giá trị kỳ vọng này không thể tính được một cách phân tích (do toán tử argmax có ràng buộc trong bài toán knapsack K), chúng có thể được ước tính với độ chính xác tùy ý bằng cách lấy mẫu theo cách Monte Carlo. Thủ tục này là một tổng quát hóa của Gumbel Max Trick [Gumbel, 1954].

--- TRANG 4 ---
Lưu ý rằng việc mô phỏng Phương trình (5) và (6) yêu cầu giải Bài toán (4) cho nhiều giá trị z có thể. Tuy nhiên, mặc dù lý thuyết về perturbed optimizers yêu cầu bài toán cơ bản phải là một chương trình tuyến tính, chỉ cần một triển khai blackbox để tạo ra K(ĉ) [Berthet et al., 2020], cho phép sử dụng một thuật toán hiệu quả thay cho một bộ giải LP (tốn kém hơn). Độ phức tạp của việc đánh giá perturbed optimizer khả vi K̄(ĉ) được thảo luận tiếp theo.

Định lý 1. Tổng tính toán cần thiết để giải Bài toán (4) là O(n log k), trong đó n và k tương ứng là kích thước ensemble và sub-ensembles.

Chứng minh. Kết quả này dựa trên quan sát rằng K(ĉ) có thể được tính toán hiệu quả bằng cách xác định k giá trị cao nhất của ĉ trong thời gian O(n log k) sử dụng kỹ thuật chia để trị. Xem, ví dụ, [Cormen et al., 2022].

Tạo ra m nghiệm như vậy để ước tính gradient sau đó yêu cầu O(mn log k) hoạt động. Tuy nhiên, lưu ý rằng những điều này có thể được thực hiện song song trên các mẫu, cho phép tạo ra đủ mẫu nhiễu để tính toán gradients chính xác, đặc biệt khi có tính toán GPU.

Để rõ ràng, cũng lưu ý rằng hàm K, như một ánh xạ chương trình tuyến tính, có không gian đầu ra rời rạc vì bất kỳ chương trình tuyến tính nào đều lấy nghiệm tối ưu tại một đỉnh của vùng khả thi [Bazaraa et al., 2008], có số lượng hữu hạn. Như vậy, nó là một hàm hằng từng đoạn và khả vi trừ trên một tập có độ đo không [Folland, 1999]. Tuy nhiên, ∂K/∂ĉ = 0 ở mọi nơi nó được định nghĩa, vì vậy các đạo hàm thiếu thông tin hữu ích cho gradient descent [Wilder et al., 2019]. Trong khi ∂K̄/∂ĉ không phải là đạo hàm thực của K̄ tại ĉ, nó cung cấp thông tin hữu ích về hướng descent của nó.

Trong thực tế, lượt đi forward được mô hình hóa như K(ĉ), và lượt đi backward được mô hình hóa như ∂K̄(ĉ)/∂ĉ. Điều này cho phép các hoạt động downstream tiếp theo, và các đạo hàm của chúng, được đánh giá tại K(ĉ) mà không cần xấp xỉ, cải thiện hiệu suất huấn luyện và test. Những lượt đi forward và backward này cùng nhau sau đây được gọi là Knapsack Layer. Lượt đi backward rõ ràng của nó được tính như
∂K̄(ĉ)/∂ĉ ≈ (1/m) Σᵢ₌₁ᵐ K(ĉ + zᵢ)v'(zᵢ)ᵀ/τ; (7)

trong đó zᵢ ∼ N(0,1)ⁿ là m mẫu độc lập mỗi mẫu được rút từ phân phối normal chuẩn.

4.2 Kết hợp Dự đoán
Ký hiệu P ∈ Rᶜˣⁿ là ma trận có cột thứ j là vector softmax ŷⱼ của base learner j,
P = (ŷ₁ ŷ₂ ... ŷₙ.) (8)

Để kết hợp dự đoán của base learners ensemble, K(ĉ) được coi như một vector che nhị phân b ∈ {0,1}ⁿ, lựa chọn tập con của base learners để đưa ra dự đoán. Ký hiệu B ∈ {0,1}ᶜˣⁿ là ma trận có cột thứ i là Bᵢ = ω1bᵢ; tức là,
B = [b1ᵀ ... bnᵀ]ᵀ.

Ma trận này được sử dụng để che dự đoán softmax P của các mô hình base learner bằng phép nhân theo phần tử. Tiếp theo, định nghĩa
Pₖ = B ⊙ P
= [b1ᵀ ... bnᵀ]ᵀ ⊙ [ŷ₁ ... ŷₙ]: (9)

Làm như vậy cho phép tính tổng dự đoán trên sub-ensemble được chọn E, nhưng theo cách tự động khả vi, đó là:
v̂ := Σᵢ∈E ŷᵢ = Σᵢ₌₁ⁿ Pₖ⁽ⁱ⁾: (10)

Dự đoán e2e-CEL đến từ việc áp dụng softmax cho tổng này:
ŷ = softmax(v̂) = softmax(Σᵢ₌₁ⁿ Pₖ⁽ⁱ⁾); (11)

xem softmax như một xấp xỉ mịn của hàm argmax được biểu diễn với các vector nhị phân one-hot. Hàm này được hiểu như một bỏ phiếu đa số được làm mịn để xác định dự đoán lớp: cho các chỉ số lớp nhị phân one-hot hᵢ, phiếu bầu đa số bằng argmax(Σᵢ hᵢ). Một minh họa của quá trình được đưa ra trong Hình 1.

Tại thời điểm test, dự đoán lớp được tính như
argmax₁≤ᵢ≤ᶜ ŷᵢ(x): (12)

Kết hợp dự đoán theo cách này cho phép một bỏ phiếu đa số xấp xỉ trên một sub-ensemble được chọn, nhưng theo cách khả vi để các tham số selection net có thể được huấn luyện trực tiếp để tạo ra các lựa chọn tối thiểu hóa loss nhiệm vụ phân loại, như được chi tiết trong phần tiếp theo.

4.3 Học Lựa chọn
Cơ chế smart ensemble học dự đoán lớp chính xác bằng cách học cách lựa chọn subensembles tốt hơn để bỏ phiếu trên các mẫu đầu vào của nó. Đổi lại, điều này được thực hiện bằng cách dự đoán các hệ số ĉ tốt hơn tham số hóa Knapsack Layer.

Nhiệm vụ dự đoán ĉ dựa trên đầu vào z tự nó được học bởi selection net, một mô hình mạng neural g sao cho ĉ = g(z). Vì g hoạt động trên cùng các mẫu đầu vào như mỗi fᵢ, nó nên có khả năng xử lý đầu vào từ z ít nhất cũng tốt như các mô hình base learners; trong Phần 5, selection net trong mỗi thí nghiệm sử dụng cùng kiến trúc CNN như của các mô hình base learner. Các giá trị được dự đoán của nó được xem như điểm số trên các thành viên ensemble, thay vì trên các lớp có thể. Điểm số cao tương ứng với base learners có trình độ cao để bỏ phiếu cho mẫu đang xét.

Trong thực tế, dự đoán ĉ của selection net được chuẩn hóa trước khi đầu vào cho ánh xạ K:
ĉ ← ĉ/||ĉ||₂: (13)

Điều này có tác dụng chuẩn hóa độ lớn của số hạng mục tiêu tuyến tính trong (4a), và có xu hướng cải thiện huấn luyện. Vì việc tỷ lệ mục tiêu của một bài toán tối ưu hóa không có tác dụng đến nghiệm của nó, điều này tương đương với việc chuẩn hóa độ lớn tương đối của mục tiêu tuyến tính và nhiễu ngẫu nhiên trong Phương trình (5) và (6), ngăn τ khỏi việc được hấp thụ hiệu quả vào ĉ được dự đoán.

Cho đầu vào huấn luyện x, cho ŷ(x) biểu diễn dự đoán e2e-CEL liên quan cho các tham số selection net θ. Trong quá trình huấn luyện, mô hình tối thiểu hóa loss phân loại giữa những dự đoán này và nhãn ground-truth:
min θ E(x,y)∼(X,Y)[L(ŷ(x), y)]: (14)

Nói chung, hàm loss L được chọn giống như loss được sử dụng để huấn luyện các mô hình base learner, vì các base learners được huấn luyện để thực hiện cùng nhiệm vụ phân loại.

4.4 Chi tiết Thuật toán e2e-CEL
Thuật toán 1 tóm tắt thủ tục e2e-CEL để huấn luyện một selection net. Lưu ý rằng chỉ các tham số của selection net được tối ưu hóa trong huấn luyện, và vì vậy chỉ các tính toán downstream của nó được backpropagated. Điều này được thực hiện bởi automatic differentiation chuẩn được sử dụng trong thư viện học máy [Paszke et al., 2019], ngoại trừ trường hợp của Knapsack Layer, có biến đổi gradient được chỉ định phân tích bởi Phương trình (7).

Để rõ ràng, Thuật toán 1 được viết theo các hoạt động áp dụng cho một mẫu đầu vào duy nhất. Trong thực tế, tuy nhiên, minibatch gradient descent được sử dụng. Mỗi lượt của huấn luyện bắt đầu đánh giá các mô hình base learner (dòng 4) và lấy mẫu các vector nhiễu Normal chuẩn (dòng 5). Selection net dự đoán từ đặc trưng đầu vào x một vector điểm số base learner g(x), định nghĩa một bài toán knapsack không có trọng số K(g(x)) được giải để tạo ra mặt nạ nhị phân b (dòng 6). Che được áp dụng cho dự đoán base learner trước khi được tổng và softmaxed cho dự đoán ensemble cuối cùng ŷ (dòng 8). Loss phân loại L được đánh giá đối với nhãn y và backpropagated trong 3 bước: (1) Gradient ∂L/∂b được tính bởi automatic differentiation backpropagated đến đầu ra của Knapsack Layer (dòng 9). (2) Yếu tố quy tắc chuỗi ∂b/∂ĉ được tính phân tích bởi phương pháp của Phần 4.1 (dòng 10). (3) Yếu tố quy tắc chuỗi còn lại ∂ĉ/∂θ được tính bởi automatic differentiation (dòng 11). Lưu ý rằng khi mỗi yếu tố quy tắc chuỗi được tính, nó cũng được áp dụng hướng tới tính toán ∂L/∂θ (dòng 12). Cuối cùng, một bước SGD [Ruder, 2016] hoặc một trong các biến thể của nó ([Diederik and Jimmy, 2014], [Zeiler, 2012]) được áp dụng để cập nhật θ (dòng 13).

Phần tiếp theo đánh giá độ chính xác của các mô hình ensemble được huấn luyện với thuật toán này, trên các nhiệm vụ phân loại sử dụng mạng neural sâu.

5 Đánh giá e2e-CEL
Huấn luyện e2e-CEL được đánh giá trên một số nhiệm vụ phân loại thị giác: phân loại chữ số trên tập dữ liệu MNIST [Deng, 2012], ước tính độ tuổi trên tập dữ liệu UTKFace [Zhifei Zhang, 2017], phân loại hình ảnh trên tập dữ liệu CIFAR10 [Krizhevsky, 2009], và phát hiện cảm xúc trên tập dữ liệu FER2013 [Liu et al., 2016].

Là một quy tắc tổng hợp được tối ưu hóa, e2e-CEL được so sánh với thuật toán Super Learner hiện đại [Ju et al., 2018] và các quy tắc tổng hợp baseline được áp dụng rộng rãi sau đây khi được ghép nối với một ensemble được huấn luyện trước:

Thuật toán 1: Huấn luyện Selection Net
input: X; Y; θ; k; m; epsilon
1 for epoch = 0, 1, ... do
2    foreach (x, y) ∼ (X, Y) do
3       ŷᵢ ← fᵢ(x) ∀1 ≤ i ≤ n
4       zᵢ ∼ N(0,1)ⁿ ∀1 ≤ i ≤ m
5       (b, ĉ) ← K(g(x)), g(x)/||g(x)||₂
6       Pₖ ← [b, ..., b]ᵀ ⊙ [ŷ₁, ..., ŷₙ]
7       ŷ ← softmax(∑ᵢ₌₁ⁿ Pₖ⁽ⁱ⁾)
8       ∂L(ŷ, y)/∂b ← autodiff
9       ∂b/∂ĉ ← (1/m)∑ᵢ₌₁ᵐ K(ĉ + zᵢ)v'(zᵢ)ᵀ/τ
10      ∂ĉ/∂θ ← autodiff
11      ∂L(ŷ, y)/∂θ ← ∂L(ŷ, y)/∂b ∂b/∂ĉ ∂ĉ/∂θ
12      θ ← θ - α∇_θ∂L(ŷ, y)/∂θ

• Super Learner: một mạng neural kết nối đầy đủ, cho các dự đoán của base learners, học các kết hợp có trọng số tối ưu chuyên biệt cho bất kỳ mẫu đầu vào nào.
• Unweighted Average: tính trung bình tất cả dự đoán softmax của base learners và sau đó tính chỉ số của điểm số nhãn cao nhất tương ứng như dự đoán cuối cùng.
• Plurality Voting: đưa ra dự đoán lớp rời rạc từ mỗi base learner và sau đó trả về lớp được dự đoán nhiều nhất.
• Random Selection: lựa chọn ngẫu nhiên một sub-ensemble có kích thước k của base learners để đưa ra dự đoán và sau đó áp dụng quy tắc trung bình không có trọng số cho dự đoán mềm của base learners được chọn.

Thiết lập thí nghiệm. Như được mô tả trong Phần 1 và trong Phụ lục A, các sơ đồ học tập ensemble hiệu quả nhất khi các mô hình base learner chính xác và có sự đa dạng lỗi cao. Trong công trình này, base learners được cố ý huấn luyện để có sự đa dạng lỗi cao đối với các mẫu đầu vào thuộc về các lớp khác nhau. Điều này được thực hiện bằng cách cấu tạo cho mỗi mô hình base learner fᵢ (1 ≤ i ≤ n) một tập huấn luyện Xᵢ trong đó một tập con của các lớp được đại diện quá mức, dẫn đến base learners chuyên về nhận dạng những lớp đó. Thành phần lớp chính xác của mỗi tập dữ liệu Xᵢ phụ thuộc vào nhiệm vụ phân loại cụ thể và vào chuyên môn dự định của base learner.

Đối với mỗi nhiệm vụ, mỗi base learners được thiết kế để chuyên về nhận dạng một hoặc hai lớp cụ thể. Để làm điều này, tập huấn luyện của mỗi base learner được phân vùng để có đa số mẫu thuộc về một lớp cụ thể, trong khi phần còn lại của tập dữ liệu huấn luyện được phân phối đều trên tất cả các lớp khác bằng lấy mẫu ngẫu nhiên. Cụ thể, để cấu tạo smart ensemble cho mỗi nhiệm vụ, một base learner duy nhất được huấn luyện để chuyên về mỗi lớp có thể, và trên mỗi cặp lớp (ví dụ, chữ số 1 và 2 trong MNIST). Khi c là số lượng lớp, smart ensemble thí nghiệm sau đó bao gồm c + C(c,2) tổng base learners. Huấn luyện một base learner chuyên biệt theo cách này thường dẫn đến độ chính xác cao trên các lớp chuyên môn của nó, nhưng độ chính xác thấp trên tất cả các lớp khác. Do đó trong thiết lập thí nghiệm này,

--- TRANG 5 ---
Độ chính xác (%)
Tập dữ liệu | Chuyên biệt | Bổ sung | Tổng thể
MNIST | 97.5 | 86.8 | 89.6
UTKFACE | 93.2 | 25.2 | 51.2
FER2013 | 79.4 | 38.1 | 47.8
CIFAR10 | 76.3 | 24.8 | 31.1

Bảng 1: Độ chính xác test của mô hình base learner chuyên biệt

không có base learner đơn lẻ nào có khả năng đạt được độ chính xác tổng thể cao trên tập test chủ Xtest. Đặc điểm này cũng tái diễn trong các mô hình phân tích liên kết [Kairouz et al., 2021].

Bảng 1 cho thấy độ chính xác trung bình của các mô hình base learner riêng lẻ trên các lớp chuyên môn của chúng và các lớp không chuyên môn của chúng; được báo cáo, tương ứng như độ chính xác chuyên biệt và độ chính xác bổ sung. Độ chính xác tổng thể được báo cáo được đo trên toàn bộ tập test chủ Xtest. Điều này tạo ra bối cảnh để chứng minh khả năng của huấn luyện e2e-CEL để cấu tạo một bộ phân loại vượt trội đáng kể so với các mô hình base learner của nó trên Xtest bằng cách lựa chọn thích ứng sub-ensembles dựa trên đặc trưng đầu vào; xem Phần 5.2.

Lưu ý rằng, trong mỗi thí nghiệm, thiết kế kiến trúc của các mô hình base learner, lựa chọn siêu tham số, và phương pháp huấn luyện không được chọn để tối ưu hóa hoàn toàn độ chính xác phân loại, đó không phải là mục tiêu trực tiếp của công trình này. Thay vào đó, các base learners đã được huấn luyện để tối đa hóa sự đa dạng lỗi, và chứng minh khả năng của e2e-CEL để tận dụng sự đa dạng lỗi và cấu tạo các mô hình ensemble có độ chính xác cao từ các mô hình base learner có độ chính xác thấp hơn nhiều, theo cách không được chia sẻ bởi các quy tắc tổng hợp thông thường. Cũng lưu ý rằng việc cải thiện độ chính xác mô hình base learner sẽ, tất nhiên, có xu hướng cải thiện độ chính xác của các bộ phân loại ensemble kết quả.

Trong mỗi trường hợp, xuyên suốt phần này, selection net e2e-CEL được cung cấp cùng kiến trúc CNN như các mô hình base learner tạo thành ensemble của nó.

5.1 Tập dữ liệu và Thiết lập
Đối với mỗi nhiệm vụ, các base learners được huấn luyện để chuyên về phân loại một hoặc hai lớp cụ thể, cho phép chương trình lựa chọn tận dụng sự đa dạng lỗi của chúng. Chi tiết bổ sung về các mô hình base learners và phân chia tập dữ liệu có thể được tìm thấy trong Phụ lục B.

Phân loại chữ số. MNIST là một tập dữ liệu hình ảnh thang xám 28x28 pixel của chữ số viết tay, chứa 60000 hình ảnh để huấn luyện và 10000 hình ảnh để kiểm tra. Ensemble bao gồm 55 base learners, 10 trong số đó chuyên về 1 lớp và C(10,2) = 45 trong số đó chuyên về 2 lớp.

Phân loại hình ảnh. CIFAR10 là một tập dữ liệu hình ảnh màu 32x32 pixel trong 10 lớp: máy bay, xe hơi, chim, mèo, hươu, chó, ếch, ngựa, tàu, và xe tải. Nó chứa 6000 hình ảnh của mỗi lớp. Ensemble bao gồm 55 base learners, 10 trong số đó chuyên về 1 lớp và C(10,2) = 45 trong số đó chuyên về 2 lớp.

Ước tính tuổi. UTKFace là một tập dữ liệu hình ảnh khuôn mặt bao gồm hơn 20000 mẫu và các phiên bản khác nhau của định dạng hình ảnh. Ở đây 9700 hình ảnh được cắt và căn chỉnh được chia thành 5 lớp: em bé (tới 5 tuổi), thiếu niên (từ 6 đến 19), trẻ (từ 20 đến 33), người lớn (từ 34 đến 56) và cao niên (hơn 56 tuổi). Các lớp không được phân phối đều theo số lượng tuổi, nhưng mỗi lớp chứa cùng số lượng mẫu. Mục tiêu là ước tính tuổi của người đó cho hình ảnh khuôn mặt. Ensemble bao gồm 15 base learners, 5 trong số đó chuyên về 1 lớp và C(5,2) = 10 về 2 lớp.

Phát hiện cảm xúc. Fer2013 là một tập dữ liệu hơn 30000 hình ảnh khuôn mặt thang xám 48x48 pixel, được nhóm thành 7 lớp: tức giận, ghê tởm, sợ hãi, vui vẻ, trung tính, buồn và ngạc nhiên. Mục tiêu là phân loại cảm xúc được thể hiện trong biểu cảm khuôn mặt thành một danh mục. Ensemble bao gồm 21 base learners, 7 trong số đó chuyên về 1 lớp và C(7,2) = 21 trong số đó chuyên về 2 lớp.

5.2 Phân tích e2e-CEL
Chiến lược e2e-CEL được thử nghiệm trên mỗi nhiệm vụ thí nghiệm cho kích thước sub-ensemble k biến thiên giữa 1 và n, và được so sánh với các phương pháp baseline được mô tả ở trên. Lưu ý trong mỗi trường hợp rằng độ chính xác được định nghĩa là tỷ lệ phần trăm các mẫu được phân loại chính xác trên tập test chủ.

Bảng 2 báo cáo độ chính xác tốt nhất trên tất cả kích thước ensemble k của các ensemble được huấn luyện bởi e2e-CEL cùng với của mỗi mô hình ensemble baseline, trong đó mỗi mô hình được hình thành sử dụng cùng các base learners được huấn luyện trước. Lưu ý cách sơ đồ e2e-CEL được đề xuất vượt trội hơn tất cả các phương pháp baseline, trong mỗi nhiệm vụ, cho tất cả trừ các giá trị thấp nhất của k.

Hình 2 báo cáo độ chính xác test được tìm thấy bởi e2e-CEL và các ensemble dựa trên Super Learner, trung bình có trọng số, bỏ phiếu đa số, hoặc sơ đồ lựa chọn ngẫu nhiên. Chúng tôi đưa ra hai quan sát chính: (1) Lưu ý từ mỗi subplot trong Hình 2 rằng smart ensembles có kích thước k > 1 cung cấp dự đoán chính xác hơn so với các mô hình baseline lựa chọn ngẫu nhiên sub-ensembles có cùng kích thước, một xu hướng giảm dần khi k tăng và lựa chọn base learner có ít hậu quả hơn (hai hoạt động bằng nhau khi k = n). (2) Trong mọi trường hợp, kích thước sub-ensemble dẫn đến hiệu suất tối ưu nằm nghiêm ngặt giữa 1 và n. Quan trọng, điều này minh họa trực giác thúc đẩy của huấn luyện ensemble e2e-CEL. Cả ensemble đầy đủ (k = n), hoặc lựa chọn thông minh của một mô hình base learner duy nhất (k = 1) đều không thể vượt trội hơn các mô hình sử dụng lựa chọn thông minh của một sub-ensemble có bất kỳ kích thước nào. Một sub-ensemble được chọn tốt có tiềm năng độ chính xác cao hơn ensemble chủ, và trung bình, đáng tin cậy hơn một base learner đơn lẻ được chọn tốt.

Tiếp theo, Bảng 3 (trái) báo cáo độ chính xác của mô hình e2e-CEL được huấn luyện trên mỗi nhiệm vụ, cùng với kích thước sub-ensemble dẫn đến độ chính xác cao nhất. Trong hai trường hợp, đối với nhiệm vụ phân loại chữ số

--- TRANG 6 ---
Độ chính xác (%)
Tập dữ liệu | e2e-CEL | SL | UA | PV | RS
MNIST | 98.55 | 96.88 | 96.81 | 95.99 | 96.83
UTKFACE | 90.97 | 85.07 | 84.60 | 80.78 | 84.60
FER2013 | 66.31 | 64.95 | 63.89 | 63.15 | 63.89
CIFAR10 | 64.09 | 60.13 | 60.59 | 60.35 | 60.59

Bảng 2: e2e-CEL so với super learner (SL), unweighted average (UA), plurality voting (MV), và random selection (RS), sử dụng base learners chuyên biệt.

và nhiệm vụ phân loại hình ảnh, e2e-CEL hoạt động tốt nhất khi kích thước sub-ensemble bằng số lượng lớp. Trong các nhiệm vụ còn lại, quan sát này đúng xấp xỉ. Điều này trực quan, vì số lượng base learners chuyên về bất kỳ lớp nào bằng số lượng lớp, và e2e-CEL có thể tăng độ chính xác ensemble bằng cách học cách lựa chọn những base learners này để dự đoán.

Cuối cùng, quan sát độ chính xác của e2e-CEL trong Bảng 3 (trái) và hiệu suất của các bộ dự đoán base learners riêng lẻ của ensemble được kiểm tra trên cả nhãn mà việc huấn luyện của chúng được chuyên biệt cũng như các nhãn khác. Lưu ý cách dự đoán e2e-CEL vượt trội hơn các base learners cấu thành của chúng với một khoảng cách rộng trên mỗi nhiệm vụ. Ví dụ, trên UTKFace, ensemble e2e-CEL đạt được độ chính xác cao hơn 40 điểm phần trăm so với base learner cấu thành trung bình của nó. Điều này minh họa khả năng của e2e-CEL để tận dụng sự đa dạng lỗi của base learners để hình thành các bộ phân loại chính xác bằng cách cấu tạo chúng dựa trên đặc trưng đầu vào, ngay cả khi độ chính xác của base learner riêng lẻ kém.

6 Kết luận
Bài báo này được thúc đẩy bởi mong muốn giải quyết những thử thách đáng kể trong lựa chọn mô hình và học tập ensemble: việc xác định mô hình tối ưu hoặc ensemble để phân loại bất kỳ mẫu đầu vào cụ thể nào. Giải pháp được đề xuất là một khung làm việc mới cho lựa chọn mô hình khả vi được thiết kế riêng cho học tập ensemble, tích hợp học máy và tối ưu hóa tổ hợp. Khung làm việc này được thúc đẩy bởi ý tưởng rằng các sub-ensembles được chọn tốt có thể tạo thành dự đoán chính xác hơn ensemble gốc của chúng. Nó lựa chọn thích ứng sub-ensembles theo các mẫu đầu vào riêng lẻ.

Bài báo cho thấy cách rút ra nhiệm vụ học tập ensemble thành một chương trình lựa chọn khả vi được huấn luyện end-to-end trong mô hình học tập ensemble. Phương pháp này cho phép khung làm việc được đề xuất cấu tạo các mô hình phân loại chính xác ngay cả từ các base learners ensemble có độ chính xác thấp, một đặc điểm không được chia sẻ bởi các phương pháp học tập ensemble hiện có. Kết quả trên các nhiệm vụ khác nhau chứng minh tính linh hoạt và hiệu quả của khung làm việc được đề xuất, vượt trội đáng kể so với các thuật toán đồng thuận hiện đại và thông thường trong nhiều thiết lập khác nhau.

Công trình này chứng minh rằng việc tích hợp học máy và tối ưu hóa tổ hợp là một bộ công cụ có giá trị không chỉ để nâng cao mà còn kết hợp các mô hình học máy để cải thiện hiệu suất trên các nhiệm vụ thông thường. Công trình này hy vọng thúc đẩy các giải pháp mới nơi decision-focused learning có thể được sử dụng để cải thiện khả năng của các hệ thống học máy. Khung làm việc được đề xuất đóng góp vào những nỗ lực đang diễn ra để cải thiện hiệu quả và tính hiệu quả của lựa chọn mô hình trong học máy, đặc biệt trong bối cảnh học tập ensemble.

Tuyên bố Đạo đức
Trong khi phương pháp e2e-CEL được đề xuất có tiềm năng cải thiện hiệu suất của học tập ensemble, việc xem xét những tác động đạo đức của việc sử dụng nó và thực hiện các bước để giảm thiểu bất kỳ tác động tiêu cực tiềm tàng nào là quan trọng. Một mối quan tâm có thể là tiềm năng lựa chọn sub-ensembles theo cách có thể duy trì hoặc khuếch đại các thiên kiến có mặt trong các base learners ensemble. Điều này có thể dẫn đến dự đoán không công bằng hoặc phân biệt đối xử đối với một số nhóm người nhất định.

Cũng quan trọng để xem xét lợi ích tiềm tàng của nghiên cứu này. Phương pháp này cho phép cấu tạo các mô hình phân loại chính xác ngay cả từ các base learners ensemble có độ chính xác thấp hoặc thiên kiến mạnh, đây là một đặc điểm không được chia sẻ bởi các phương pháp học tập ensemble hiện có. Giải pháp được đề xuất do đó nhằm nâng cao hiệu suất của các mô hình học tập ensemble và có thể thúc đẩy việc phát triển các bộ dự đoán hiệu quả hơn thể hiện ít thiên kiến hơn.

Lời cảm ơn
Nghiên cứu này được hỗ trợ một phần bởi các grant NSF 2007164, 2232054, và NSF CAREER Award 2143706. Fioretto cũng được hỗ trợ bởi Amazon Research Award và Google Research Scholar Award. Quan điểm và kết luận của nó chỉ là của các tác giả.

--- TRANG 7 ---
[Đây là trang chứa các biểu đồ và bảng so sánh hiệu suất]

0 10 20 30 40 50
Kích thước Ensemble 30 35 40 45 50 55 60 65 Độ chính xác trung bình %
Unweighted average
Plurality voting
Super Learner (*)
e2e-CEL (*)
Random selection

0 10 20 30 40 50
Kích thước Ensemble 90 92 94 96 98 Độ chính xác trung bình %
Unweighted average
Plurality voting
Super Learner (*)
e2e-CEL (*)
Random selection

0 2 4 6 8 10 12 14 16
Kích thước Ensemble 50 55 60 65 70 75 80 85 90 Độ chính xác trung bình %
Unweighted average
Plurality voting
Super Learner (*)
e2e-CEL (*)
Random selection

0 5 10 15 20 25
Kích thước Ensemble 47.5 50.0 52.5 55.0 57.5 60.0 62.5 65.0 67.5 Độ chính xác trung bình %
Unweighted average
Plurality voting
Super Learner (*)
e2e-CEL (*)
Random selection

Hình 2: So sánh giữa e2e-CEL và các mô hình ensemble khác tại các kích thước sub-ensemble k khác nhau trên phân loại hình ảnh–CIFAR10–(trên trái), phân loại chữ số–MNIST–(trên phải), ước tính tuổi–UTKFace–(dưới trái), và phát hiện cảm xúc–FER2013–(dưới phải). (*) trong nhãn xác định các phương pháp sử dụng quy tắc tổng hợp chuyên biệt cho mọi mẫu đầu vào.

Độ chính xác (%)
Tập dữ liệu | Lớp | k tốt nhất | e2e-CEL | Base learners riêng lẻ
MNIST | 10 | 10 | 98.55 | 89.6
UTKFACE | 5 | 7 | 90.97 | 51.2
FER2013 | 7 | 13 | 66.31 | 47.8
CIFAR10 | 10 | 10 | 64.09 | 31.1

Bảng 3: Trái: Kích thước ensemble tốt nhất (k tốt nhất) và độ chính xác test e2e-CEL liên quan đạt được trên mỗi tập dữ liệu. Phải: Độ chính xác trung bình cho các base learners ensemble cấu thành.

--- TRANG 8 ---
Tuyên bố Đóng góp
James Kotary và Vincenzo Di Vito đều được coi là tác giả đầu tiên. Tất cả tác giả có đóng góp bằng nhau.

Tài liệu tham khảo
Jacob Abernethy, Chansoo Lee, và Ambuj Tewari. Perturbation techniques in online learning and optimization. Perturbations, Optimization, and Statistics, 233, 2016.

Mokhtar S Bazaraa, John J Jarvis, và Hanis D Sherali. Linear programming and network flows. John Wiley & Sons, 2008.

Quentin Berthet, Mathieu Blondel, Olivier Teboul, Marco Cuturi, Jean-Philippe Vert, và Francis Bach. Learning with differentiable pertubed optimizers. Advances in neural information processing systems, 33:9508–9519, 2020.

Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, và Clifford Stein. Introduction to algorithms. MIT press, 2022.

Li Deng. The mnist database of handwritten digit images for machine learning research [best of the web]. IEEE Signal Processing Magazine, 29(6):141–142, 2012.

PK Diederik và B Jimmy. Adam: A method for stochastic optimization. iclr. arXiv preprint arXiv:1412.6980, 2014.

Jérémie Dona và Patrick Gallinari. Differentiable feature selection, a reparameterization approach. In Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13–17, 2021, Proceedings, Part III 21, pages 414–429. Springer, 2021.

Gerald B Folland. Real analysis: modern techniques and their applications, volume 40. John Wiley & Sons, 1999.

Jie Fu, Hongyin Luo, Jiashi Feng, Kian Hsiang Low, và Tat-Seng Chua. Drmad: Distilling reverse-mode automatic differentiation for optimizing hyperparameters of deep neural networks, 2016.

Emil Julius Gumbel. Statistical theory of extreme values and some practical applications: a series of lectures, volume 33. US Government Printing Office, 1954.

Cheng Ju, Aurélien Bibaut, và Mark van der Laan. The relative performance of ensemble methods with deep convolutional neural networks for image classification. Journal of Applied Statistics, 45(15):2800–2818, 2018.

Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D'Oliveira, Hubert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adrià Gascón, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Konečný, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Özgür, Rasmus Pagh, Hang Qi, Daniel Ramage, Ramesh Raskar, Mariana Raykova, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramèr, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, và Sen Zhao. Advances and open problems in federated learning. Foundations and Trends® in Machine Learning, 14(1–2):1–210, 2021.

James Kotary, Vincenzo Di Vito, và Ferdinando Fioretto. End-to-end optimization and learning for multiagent ensembles, 2022.

Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.

Kuang Liu, Mingmin Zhang, và Zhigeng Pan. Facial expression recognition with cnn ensemble. In 2016 International Conference on Cyberworlds (CW), pages 163–166, 2016.

Ibomoiye Domor Mienye và Yanxia Sun. A survey of ensemble learning: Concepts, algorithms, applications, and prospects. IEEE Access, 10:99129–99149, 2022.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, và Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019.

Mauro Ribeiro, Katarina Grolinger, và Miriam A.M. Capretz. Mlaas: Machine learning as a service. In 2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA), pages 896–902, 2015.

Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747, 2016.

Rishit Sheth và Nicolò Fusi. Differentiable feature selection by discrete relaxation. In International Conference on Artificial Intelligence and Statistics, pages 1564–1572. PMLR, 2020.

Bryan Wilder, Bistra Dilkina, và Milind Tambe. Melding the data-decisions pipeline: Decision-focused learning for combinatorial optimization. In AAAI, volume 33, pages 1658–1665, 2019.

Ian H Witten, Eibe Frank, Mark A Hall, Christopher J Pal, và MINING DATA. Practical machine learning tools and techniques. In Data Mining, volume 2, 2005.

Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.

Hairong Qi Zhifei Zhang, Yang Song. Age progression/regression by conditional adversarial autoencoder. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2017.

--- TRANG 9 ---
A Công trình Liên quan
Bài báo này đề xuất một khung làm việc để cấu tạo các mô hình học tập ensemble hiệu quả bằng cách thích ứng các kỹ thuật từ giao điểm của tối ưu hóa có ràng buộc và học máy. Phần này xem xét ngắn gọn hai lĩnh vực chưa riêng biệt:

Học tập ensemble. Một số công trình đã tập trung vào nghiên cứu các mô hình ensemble để học máy hiệu quả hơn. Chúng tôi giới thiệu độc giả đến ? để có phân loại hữu ích của các phương pháp ensemble, tùy thuộc vào nhiệm vụ học máy cơ bản cần được giải quyết.

Học tập ensemble thường bao gồm hai khía cạnh riêng biệt: (1) việc huấn luyện các base learners học tập ensemble riêng lẻ, và (2) việc tổng hợp các đầu ra riêng lẻ của chúng thành dự đoán ensemble chính xác. Khía cạnh trước liên quan đến việc cấu tạo một ensemble từ các base learners với hồ sơ lỗi bổ sung, và thường được xử lý bởi bagging, bao gồm việc cấu tạo ngẫu nhiên các tập dữ liệu để huấn luyện từng thành viên ensemble, và boosting, bao gồm việc tạo ra thích ứng một chuỗi các tập dữ liệu dựa trên phân phối lỗi của các mô hình kết quả của chúng để sự đa dạng lỗi được tăng lên. Nhiều biến thể và lựa chọn thay thế cụ thể theo nhiệm vụ cũng đã được đề xuất ?. Ví dụ, ? nghiên cứu tác động của các kỹ thuật pruning khác nhau để giảm kích thước ensemble từ một ensemble bagging ban đầu. Một khảo sát tập trung vào việc huấn luyện các base learners ensemble riêng lẻ được cung cấp trong Mienye and Sun [2022].

Khía cạnh sau thường được xử lý bởi các quy tắc tổng hợp cổ điển trên các dự đoán lớp rời rạc hoặc các giá trị kích hoạt liên tục của các thành viên ensemble. Bỏ phiếu đa số và đa nguyên là các tiêu chí đồng thuận để chọn dự đoán lớp rời rạc dựa trên phổ biến nhất trong các thành viên ensemble. Một số công trình đã cố gắng mô hình hóa toán học các quy tắc tổng hợp hiệu quả hơn: chúng bao gồm super-learners Ju et al. [2018], cố gắng tạo thành một sự kết hợp có trọng số của các mô hình base learner tối đa hóa độ chính xác trên một tập validation.

Decision focused learning. Một khối công trình đã được dành để nghiên cứu các bài toán tối ưu hóa có ràng buộc như các thành phần có thể học được trong mạng neural. Xem ví dụ khảo sát gần đây của ?. Như một ánh xạ giữa một số tham số định nghĩa bài toán và nghiệm tối ưu của chúng, một bài toán tối ưu hóa có thể được xem như một hàm có đầu ra tuân thủ các ràng buộc được định nghĩa rõ ràng. Theo cách này, cấu trúc đặc biệt có thể được đảm bảo trong các dự đoán ? hoặc embeddings đã học của một mạng neural ?. Thử thách chính là việc phân biệt của ánh xạ tối ưu hóa được yêu cầu cho backpropagation. Các phương pháp riêng biệt đã được đề xuất, phụ thuộc vào các xấp xỉ khác nhau, tùy thuộc vào dạng của bài toán tối ưu hóa ?, ?, và ?. Các bài toán lồi mịn, đặc biệt, thừa nhận gradients chính xác thông qua các phương trình tối ưu được định nghĩa rõ ràng của chúng, như được chỉ ra bởi ? và ?. Các bài toán lồi định nghĩa ánh xạ hằng từng đoạn, chẳng hạn như chương trình tuyến tính, yêu cầu xấp xỉ bởi các bài toán mịn trước khi phân biệt. Các kỹ thuật làm mịn thường được áp dụng cho hàm mục tiêu và bao gồm việc thêm các số hạng mục tiêu điều chỉnh, như được đề xuất bởi Wilder et al. [2019], và nhiễu ngẫu nhiên, như trong Berthet et al. [2020].

Ký hiệu | Ngữ nghĩa
n | Kích thước của ensemble
k | Kích thước của sub-ensemble
m | Số lượng mẫu nhiễu cho perturbed optimizer
fᵢ | Bộ phân loại được huấn luyện trước, 1 ≤ i ≤ n
ŷᵢ | Dự đoán softmax từ fᵢ
f | Bộ phân loại smart ensemble
ŷ | Dự đoán softmax smart ensemble
g | Selection net
θ | Tham số selection net
ĉ | Điểm số được dự đoán từ selection net
K | Hàm knapsack không có trọng số
K̄ | Hàm knapsack không có trọng số bị nhiễu
X | Dữ liệu đặc trưng đầu vào
Y | Dữ liệu lớp mục tiêu
L | Hàm loss phân loại

Bảng 4: Ký hiệu thông thường

Trong bài báo này, dự đoán có cấu trúc mong muốn chủ yếu có dạng của một vector nhị phân đại diện cho lựa chọn tập con, trong đó k < n trong số n mục được chỉ ra để lựa chọn. Công trình này khai thác thực tế rằng việc phân biệt cấu trúc này như một chương trình tuyến tính đối với một vector tham số hóa cho phép nó được áp dụng trong một sơ đồ che khả vi end-to-end được sử dụng để lựa chọn và kết hợp dự đoán base learners ensemble tốt nhất cho một mẫu đầu vào nhất định.

B Tập dữ liệu và thiết lập
Phần này cung cấp thông tin về kiến trúc mô hình base learner và cấu tạo tập dữ liệu huấn luyện cho mỗi nhiệm vụ.

Phân loại chữ số. Ở đây các base learners sử dụng mạng neural tích chập (CNN), mỗi mạng có cùng kiến trúc: 2 lớp tích chập và 3 lớp tuyến tính. Trong các lớp tích chập, với 10 và 20 kênh đầu ra tương ứng, thông tin được xử lý sử dụng bộ lọc kernel vuông có kích thước 5x5. Tập dữ liệu huấn luyện của mỗi base learner được cấu tạo, trung bình, bởi 73.2% mẫu thuộc về một (hoặc hai) lớp cụ thể, trong khi phần còn lại được phân phối đều trên tất cả các lớp khác bằng lấy mẫu ngẫu nhiên. Trung bình, điều này dẫn đến 97.5% độ chính xác trong việc phân loại một tập con cụ thể của các mẫu huấn luyện và 86.8% độ chính xác trong việc phân loại tập con bổ sung của các mẫu huấn luyện.

Phân loại hình ảnh
Ở đây các base learners sử dụng các mô hình CNN, mỗi mô hình có cùng kiến trúc, 2 lớp tích chập, mỗi lớp chứa bộ lọc kernel vuông có kích thước 5, với 6 và 16 kênh đầu ra tương ứng, và 3 lớp tuyến tính. Trung bình, tập dữ liệu huấn luyện của mỗi base learner được cấu tạo bởi 55.2% mẫu thuộc về một (hoặc hai) lớp cụ thể, trong khi phần còn lại được phân phối đều trên tất cả các lớp khác bằng lấy mẫu ngẫu nhiên. Điều này dẫn đến 76.3% độ chính xác để phân loại một tập con cụ thể của các mẫu huấn luyện và 24.8% độ chính xác để phân loại tập con bổ sung của các mẫu huấn luyện.

Ước tính tuổi. Ở đây kiến trúc mô hình base learner là một phiên bản được huấn luyện trước của Resnet18 trên tập dữ liệu ImageNet-1k, được huấn luyện lại hoàn toàn. Trung bình, tập dữ liệu huấn luyện của mỗi base learner được cấu tạo bởi 58.3% mẫu thuộc về một (hoặc hai) lớp cụ thể, trong khi phần còn lại được phân phối đều trên tất cả các lớp khác bằng lấy mẫu ngẫu nhiên. Điều này dẫn đến 93.2% độ chính xác để phân loại một tập con cụ thể của các mẫu huấn luyện và 25.2% độ chính xác để phân loại tập con bổ sung của các mẫu huấn luyện.

Phát hiện cảm xúc. Ở đây kiến trúc mô hình base learner bao gồm 8 lớp tích chập và một lớp tuyến tính. Đối với mỗi lớp tích chập, số lượng kênh đầu ra là 64; 128; 128; 128; 256; 512; 512 và 512; mỗi lớp sử dụng kernel vuông có kích thước 3x3. Tập dữ liệu huấn luyện của mỗi base learner được cấu tạo bởi 44.4% mẫu thuộc về một (hoặc hai) lớp cụ thể, trong khi phần còn lại được phân phối đều trên tất cả các lớp khác bằng lấy mẫu ngẫu nhiên. Điều này dẫn đến 79.4% độ chính xác để phân loại một tập con cụ thể của các mẫu huấn luyện và 38.1% độ chính xác để phân loại tập con bổ sung của các mẫu huấn luyện.

--- TRANG 10 ---
C Siêu tham số
C.1 Siêu tham số của việc huấn luyện mô hình base learners
Ở đây cho mỗi nhiệm vụ, chi tiết bổ sung về các siêu tham số của việc huấn luyện mô hình base learners được cung cấp.

Tập dữ liệu | Optimizer | Scheduler | Learning rate | Gamma | Epochs | Loss function
MNIST | Adadelta | StepLR | 1 | 0.9 | 14 | Cross entropy
UTKFACE | Adadelta | StepLR | 1 | 0.9 | 10 | Cross entropy
FER2013 | Adam | OneCycleLR | 0.001 | | 70 | Cross entropy
CIFAR10 | SGD | | 0.001 | | 3 | Cross entropy

C.2 Siêu tham số của việc huấn luyện mô hình selection net
Ở đây, cho mỗi nhiệm vụ, chi tiết bổ sung về các siêu tham số của việc huấn luyện mô hình selection net được cung cấp. Chúng tôi nhắc lại rằng cho mỗi nhiệm vụ, mô hình selection net chia sẻ cùng kiến trúc của mô hình base learner tương ứng.

Optimizer | Scheduler | Learning rate | Gamma | Epochs | Loss function
MNIST | Adadelta | | 1 | | 8 | Cross entropy
UTKFACE | Adadelta | | 1 | | 10 | Cross entropy
FER2013 | Adam | OneCycleLR | 0.001 | | 20 | Cross entropy
CIFAR10 | Adadelta | | 0.001 | | 3 | Cross entropy
