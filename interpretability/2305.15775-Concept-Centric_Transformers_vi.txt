# 2305.15775.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/interpretability/2305.15775.pdf
# Kích thước tệp: 43544320 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Transformer Tập Trung Khái Niệm:
Tăng Cường Khả Năng Diễn Giải Mô Hình thông qua Học Khái Niệm Tập Trung Đối Tượng
trong một Không Gian Làm Việc Toàn Cầu Được Chia Sẻ
Jinyung Hong1, Keun Hee Park1, và Theodore P. Pavlic1, 2
1Trường Tin Học và Trí Tuệ Tăng Cường
2Trường Khoa Học Sự Sống
Đại Học Bang Arizona
Tempe, AZ 85281
{jhong53, kpark53, tpavlic }@asu.edu
Tóm Tắt
Nhiều phương pháp AI có thể diễn giải đã được đề xuất
để cung cấp các giải thích hợp lý cho việc ra quyết định của
mô hình. Tuy nhiên, việc cấu hình một mô hình có thể giải
thích hiệu quả giao tiếp giữa các mô-đun tính toán đã nhận
được ít sự chú ý hơn. Một lý thuyết không gian làm việc toàn
cầu được chia sẻ được đề xuất gần đây đã chỉ ra rằng các
mạng của các mô-đun phân tán có thể được hưởng lợi từ việc
chia sẻ thông tin với bộ nhớ có điểm nghẽn vì các ràng buộc
giao tiếp khuyến khích chuyên môn hóa, tính tổ hợp và đồng
bộ hóa giữa các mô-đun. Được truyền cảm hứng từ điều này,
chúng tôi đề xuất Transformer Tập Trung Khái Niệm, một
cấu hình đơn giản nhưng hiệu quả của không gian làm việc
toàn cầu được chia sẻ cho khả năng diễn giải, bao gồm: i)
một mô-đun bộ nhớ dựa trên tập trung đối tượng để trích
xuất các khái niệm ngữ nghĩa từ các đặc trưng đầu vào, ii)
một cơ chế cross-attention giữa khái niệm đã học và nhúng
đầu vào, và iii) các hàm mất mát phân loại và giải thích tiêu
chuẩn để cho phép các nhà phân tích con người trực tiếp
đánh giá một giải thích cho lý luận phân loại của mô hình.
Chúng tôi kiểm tra phương pháp của mình so với các phương
pháp dựa trên khái niệm hiện có khác trên các tác vụ phân
loại cho nhiều bộ dữ liệu khác nhau, bao gồm CIFAR100,
CUB-200-2011, và ImageNet, và chúng tôi chỉ ra rằng mô
hình của chúng tôi đạt được độ chính xác phân loại tốt hơn
so với tất cả các đường cơ sở trên tất cả các bài toán mà còn
tạo ra các giải thích dựa trên khái niệm nhất quán hơn về
đầu ra phân loại.

1. Giới Thiệu
Mặc dù các mô hình học máy tối tân đã đạt được hiệu
suất đáng chú ý trên một loạt các ứng dụng, việc thiếu
minh bạch nội tại của chúng do nhiều bậc tự do trong huấn
luyện hạn chế việc sử dụng chúng trong các lĩnh vực quan
trọng về an toàn—như chẩn đoán y tế, chăm sóc sức khỏe, an
toàn cơ sở hạ tầng công cộng, và kiểm tra hình ảnh cho kỹ
thuật dân dụng—nơi kiến thức chuyên môn theo lĩnh vực
đáng tin cậy là quan trọng cho việc ra quyết định. Gần đây,
một số phương pháp đã được phát triển cung cấp các giải
thích hậu hoc xác định các đặc trưng liên quan mà một mô
hình đã huấn luyện sử dụng để đưa ra dự đoán [58, 64, 70,
78], nhưng những phương pháp này thường bị chỉ trích vì
chỉ tập trung vào các đặc trưng cấp thấp [1, 43, 44, 79]. Ngược
lại, các mô hình có thể diễn giải nội tại [67] đã được đề xuất
để đưa ra quyết định dựa trên "khái niệm" có thể hiểu được
bởi con người, nền tảng của chuyên môn lĩnh vực [1,6,12,13,
25,32,42,43,45,50,65,91,94]. Khoảng cách giữa khả năng
giải thích hậu hoc và các mô hình có thể diễn giải nội tại
cũng được thảo luận trong cộng đồng NLP liên quan đến
việc diễn giải các cơ chế attention [3]. Đặc biệt, cuộc tranh
luận về mức độ diễn giải nào có thể được gán cho trọng số
attention trên các token đầu vào vẫn cần được giải quyết để
giúp đáp ứng nhu cầu về khả năng diễn giải của các cơ chế
attention [1, 9, 40, 71, 86].

Lý tưởng nhất, một mô hình có thể diễn giải nội tại sẽ tạo
ra các giải thích là sự kết hợp của các mô-đun có ý nghĩa
riêng lẻ. Các giải thích mô-đun có thể cải thiện sự hiểu biết
của con người, và các hệ thống nơ-ron tự nhiên vẫn tiếp tục
truyền cảm hứng cho việc phát triển AI thường được mô tả
là có kiến trúc mô-đun [4, 5, 8, 66]. Do đó, việc cấu trúc
các thuật toán để thúc đẩy việc học các cấu trúc tiềm ẩn
mô-đun cũng có thể dẫn đến hiệu suất tổng thể tốt hơn.

Được thúc đẩy bởi việc cải thiện tính mô-đun trong các mô
hình có thể diễn giải, chúng tôi đề xuất Transformer Tập
Trung Khái Niệm (CCT), một khung của các mô hình có thể
diễn giải nội tại được truyền cảm hứng từ Không Gian Làm
Việc Toàn Cầu Được Chia Sẻ (SGW) [30], một khung khái
niệm mới có nghĩa là khuyến khích tính mô-đun nói chung
bằng cách buộc các thành phần chuyên môn song song phải
cạnh tranh để truy cập có điểm nghẽn vào một bộ nhớ được
chia sẻ. Cấu hình của CCT cho phép các mô hình đã huấn
luyện có cấu trúc đơn giản, mô-đun có thể trích xuất các khái
niệm ngữ nghĩa có hoặc không có sự hướng dẫn của các
giải thích sự thật cơ bản về các khái niệm.

arXiv:2305.15775v3  [cs.LG]  7 Nov 2023

--- TRANG 2 ---
Trong những gì sau đây, chúng tôi khung hóa CCT của chúng
tôi như một mở rộng mới lạ của khái niệm SGW để phát triển
mô hình có thể diễn giải, và chúng tôi mô tả cách CCT được
triển khai bằng ba thành phần chính: i) mô-đun Concept-
Slot-Attention (CSA) giao tiếp với nhúng hình ảnh từ một
mô hình xương sống và tạo ra một tập hợp các nhúng phụ
thuộc tác vụ cho các khái niệm, ii) mô-đun Cross-Attention
(CA) tạo ra các đầu ra phân loại sử dụng cross-attention
giữa các đặc trưng đầu vào và các nhúng của mô-đun CSA,
và iii) các hình phạt mất mát chuyên môn, bao gồm Mất Mát
Giải Thích, khi kiến thức chuyên gia có thể được tận dụng,
và Mất Mát Thưa Thớt, một mất mát dựa trên entropy để
thực thi tính thưa thớt để xác định tầm quan trọng của các
đặc trưng trong quá trình huấn luyện. Kiến trúc CCT được
thiết kế để tăng cường các xương sống học sâu hiện có để
thêm khả năng giải thích cho chúng. Do đó, chúng tôi xác
thực phương pháp của mình trên ba bộ dữ liệu điểm chuẩn
hình ảnh—CIFAR100 Super-class [23], CUB-200-2011 [83],
và ImageNet [19]—kết hợp nó với các xương sống học sâu
khác nhau, như Vision Transformer (ViT) [22], Swin
Transformer (SwinT) [55], và ConvNeXt [56].

2. Công Trình Liên Quan
Những tiến bộ đáng kể gần đây đã được thực hiện trong
việc thiết kế các mô hình có thể giải thích và diễn giải để
đo lường tầm quan trọng của từng đặc trưng cho đầu ra dự
đoán. Phân tích hậu hoc là một phương pháp chung để phân
tích một mô hình đã huấn luyện bằng cách khớp các giải
thích với các đầu ra phân loại [1, 58, 64]. Ví dụ, tối đa hóa
kích hoạt [62, 81, 92] và trực quan hóa saliency [70, 78, 80]
là các phương pháp nổi tiếng cho CNN. Các phương pháp
có thể diễn giải dựa trên attention cũng đã được giới thiệu
để xác định các phần liên quan nhất của đầu vào mà mạng
tập trung vào khi đưa ra quyết định [24, 26, 27, 37, 96–99].

Ngoài ra, việc thiết kế các phương pháp giải thích dự đoán
với các khái niệm cấp cao, có thể hiểu được bởi con người
[6, 12, 13, 25, 32, 42, 43, 45, 50, 65, 89, 91, 94] là một trong
những tiến bộ gần đây trong lĩnh vực khả năng diễn giải.
Những phương pháp có thể diễn giải nội tại này tập trung
vào việc xác định các mẫu kích hoạt chung trong các nút
của lớp cuối cùng của mạng nơ-ron tương ứng với các danh
mục có thể hiểu được bởi con người hoặc ràng buộc mạng
để học các khái niệm như vậy. Trong số đó, công việc của
chúng tôi tương tự nhất với Concept Transformers (CT)
[65], một khung học các khái niệm cấp cao được định nghĩa
với một tập hợp các chiều liên quan. Những khái niệm đó,
có thể là cụ thể theo phần hoặc toàn cục, thường có thể
tăng hiệu suất của tác vụ học trong khi cung cấp khả năng
giải thích mà không tốn thêm chi phí cho mạng. Tuy nhiên,
phương pháp đó dựa vào việc trích xuất khái niệm dựa trên
các mảnh hình ảnh được cung cấp mặc dù mỗi mảnh hình
ảnh có thể là một dự đoán không đáng tin cậy của các khái
niệm cấp cao. Công thức CCT của chúng tôi tổng quát hóa
phương pháp này vượt ra ngoài các mảnh hình ảnh.

Trong lịch sử, người ta đã tranh luận rằng tốt hơn là xây
dựng một hệ thống thông minh từ nhiều mô-đun chuyên môn
tương tác thay vì một thực thể "nguyên khối" duy nhất để
đối phó với một phổ rộng các điều kiện và tác vụ [29, 59,
66]. Do đó, đã có nỗ lực đáng kể về đồng bộ hóa giữa các
mô-đun chuyên môn tính toán thông qua một không gian
làm việc toàn cầu được chia sẻ [16,31,39,60,69]. Hơn nữa,
công việc về việc tích hợp các kiến trúc tính toán mô-đun
với bộ nhớ làm việc lấy cảm hứng từ sinh học, khoa học
thần kinh và khoa học nhận thức [35, 36, 63, 87]. Không
gian làm việc toàn cầu được chia sẻ được đề xuất gần đây
[30] cho thấy cách sử dụng cơ chế attention để khuyến khích
thông tin hữu ích nhất được chia sẻ giữa các mô-đun nơ-ron
trong các khung AI hiện đại. Phương pháp này là nguồn
cảm hứng cho việc chúng tôi sử dụng một bộ nhớ làm việc
rõ ràng trong CCT để cải thiện tính tổng quát của các mô
hình dựa trên Transformer và tập trung đối tượng trong
bối cảnh của các mô hình có thể giải thích.

3. Khái Niệm Cơ Bản
Không Gian Làm Việc Toàn Cầu Được Chia Sẻ trong
Các Mô Hình AI. Được truyền cảm hứng từ Lý Thuyết
Không Gian Làm Việc Toàn Cầu (GWT) từ khoa học nhận
thức [2, 17, 18, 72–75], Không Gian Làm Việc Toàn Cầu
Được Chia Sẻ (SGW) [30] khám phá cách GWT có thể được
thể hiện trong các mô hình AI hiện đại để sở hữu các sơ
đồ giao tiếp và phối hợp nơi một số chuyên gia giao tiếp
thưa thớt (các mô-đun tính toán cụ thể xử lý đầu vào) tương
tác thông qua một không gian làm việc được chia sẻ (một
mô-đun bộ nhớ làm việc được chia sẻ). Để làm như vậy,
transformer và các phương pháp dựa trên slot đã được mở
rộng bằng cách thêm một không gian làm việc được chia
sẻ và cho phép các mô-đun cạnh tranh để truy cập ghi trong
mỗi giai đoạn tính toán (Hình 1). Thay thế giao tiếp theo
cặp giữa các mô-đun bằng tương tác được tạo điều kiện
bởi không gian làm việc được chia sẻ cho phép: i) tương
tác bậc cao hơn giữa các mô-đun, ii) lọc động do sự bền
bỉ của bộ nhớ, và iii) sự tinh vi tính toán của việc sử dụng
không gian làm việc được chia sẻ để đồng bộ hóa các
chuyên gia khác nhau. Được thúc đẩy bởi SGW, chúng tôi
nhằm khám phá một cấu hình hiệu quả cho các khung AI
có thể diễn giải nội tại và đề xuất một cách đơn giản nhưng
hiệu quả để tương tác giữa một không gian làm việc được
chia sẻ và các chuyên gia để cải thiện khả năng diễn giải
và hiệu suất.

Các Biến Thể của Slot-Attention. Do thiết kế đơn giản
nhưng hiệu quả, Slot-Attention (SA) [57] đã nhận được sự
chú ý đáng kể trong việc học tập trung đối tượng không
giám sát để bắt chước sự phát triển của hiểu biết tượng
trưng trong nhận thức con người. Cơ chế attention lặp cho
phép SA học và cạnh tranh giữa các slot để giải thích các
phần của đầu vào, cho thấy hiệu ứng phân cụm mềm trên
các đầu vào trực quan [57]. Tuy nhiên, như được tiết lộ bởi
các nghiên cứu gần đây, mô-đun SA vanilla có giới hạn
nội tại ở chỗ: i) việc khởi tạo ngẫu nhiên cho các slot cản
trở việc giải quyết liên kết đối tượng trong đầu vào và ii) nó

--- TRANG 3 ---
Hình 1. Không gian làm việc toàn cầu được chia sẻ [30] xuất hiện
từ ba bước: 1) Một tập hợp các mô-đun tính toán (hoặc chuyên
gia) thực hiện xử lý tiêu chuẩn, và một tập con của các chuyên
gia trở nên hoạt động tại một giai đoạn tính toán cụ thể tùy thuộc
vào đầu vào; 2) Chuyên gia hoạt động ghi thông tin trong một
mô-đun bộ nhớ làm việc được chia sẻ (hoặc không gian làm việc
được chia sẻ); 3) Nội dung được cập nhật của không gian làm
việc được phát sóng đến tất cả các chuyên gia. Chúng tôi khám
phá cách những bước này có thể được sử dụng để thêm khả năng
giải thích trong các khung AI. Hình tổng quát ở trên được lấy
cảm hứng từ [30, Hình 1].

phụ thuộc nhiều vào việc điều chỉnh siêu tham số để nó
không thể được áp dụng chung trong nhiều lĩnh vực. Do
đó, một số biến thể của SA, bao gồm I-SA [10] và BO-QSA
[41], đã được đề xuất gần đây để giải quyết những vấn đề
đó¹.

Có một số ví dụ hiện có về việc tận dụng các phương pháp
dựa trên slot trong các mô hình có thể giải thích để trích
xuất các khái niệm ngữ nghĩa [49, 84]. Tuy nhiên, ít nghiên
cứu đã nhấn mạnh quan điểm của kiến trúc mô-đun để thúc
đẩy giao tiếp giữa mô hình dựa trên slot và các mô-đun khác.
Chúng tôi tận dụng ba biến thể SA ở trên như không gian
làm việc được chia sẻ của SGW và khám phá cách khuyến
khích các tương tác giữa chúng để đạt được khả năng diễn
giải và hiệu suất tốt hơn.

4. Transformer Tập Trung Khái Niệm
Đối với các tác vụ phân loại có giám sát, chúng tôi giới
thiệu Transformer Tập Trung Khái Niệm (CCT), một hiện
thực hóa của SGW để cấu hình một mô hình có thể diễn
giải nội tại, và chúng tôi sẽ mô tả mối liên hệ giữa các bước
SGW (Hình 1) và công thức của chúng tôi. Mô hình của
chúng tôi bao gồm: i) mô-đun Concept-Slot-Attention
(CSA) hoạt động như một mô-đun bộ nhớ được chia sẻ và
trích xuất nhúng khái niệm tiềm ẩn cụ thể cho mỗi lô đầu
vào, ii) mô-đun Cross-Attention (CA) để phát sóng giữa
nhúng đầu vào và nhúng khái niệm được trích xuất từ mô-
đun CSA để nó tạo ra đầu ra phân loại cũng như các giải
thích dựa trên khái niệm trung thực và hợp lý và khuyến
khích các tương tác theo cặp giữa chúng, và iii) các mất
mát chuyên môn, bao gồm Mất Mát Giải Thích và Mất Mát
Thưa Thớt, đây là sơ đồ phát sóng thông tin của chúng tôi
để khuyến khích khả năng diễn giải. Kiến trúc CCT, được
tóm tắt trong Hình 2, được mô tả trong các phần sau. Các
chi tiết thêm, bao gồm các hạn chế (Phụ lục D), được đưa
ra trong phụ lục.

¹Chúng tôi bỏ qua các chi tiết về sự khác biệt kỹ thuật của chúng vì chúng nằm ngoài phạm vi.

4.1. Giữa Các Chuyên Gia và Không Gian Làm Việc
Được Chia Sẻ
Theo cấu trúc chung của SGW, chúng tôi tận dụng các
chuyên gia, là các mô-đun tính toán của xương sống của
chúng tôi, và một không gian làm việc được chia sẻ bằng
cách sử dụng các phương pháp dựa trên slot trong mô-đun
CSA của chúng tôi, bao gồm SA [57], I-SA [10], và BO-QSA
[41]. Do tính mô-đun trong công thức của chúng tôi, ba biến
thể SA đó có thể hoán đổi cho nhau, và chúng tôi chứng
minh so sánh hiệu suất giữa chúng trong các thí nghiệm
của chúng tôi. Chúng tôi sử dụng cơ chế attention khóa–
truy vấn–giá trị thông thường để triển khai sự cạnh tranh
giữa các chuyên gia để ghi vào không gian làm việc, tương
tự như SGW. Mô-đun CSA mã hóa một tập hợp L vector
đặc trưng đầu vào E thành các biểu diễn khái niệm Sconcept,
mà chúng tôi gọi là các slot khái niệm.

Liên Kết Khái Niệm Cụ Thể cho Mỗi Lô Đầu Vào. Với
số lượng khái niệm C, các slot khái niệm Sconcept∈RC×d
đầu tiên thực hiện attention cạnh tranh [57] trên các đặc
trưng đầu vào E∈RL×D. Đối với điều này, chúng tôi áp
dụng phép chiếu tuyến tính qCSA trên các slot khái niệm
để thu được các truy vấn và các phép chiếu kCSA và vCSA
trên các đầu vào để thu được các khóa và các giá trị, tất cả
đều có cùng kích thước d². Sau đó, chúng tôi thực hiện tích
vô hướng giữa các truy vấn và khóa để có được ma trận
attention ACSA∈RC×L. Trong ACSA, mỗi mục ACSA_c,l
là trọng số attention của slot khái niệm c để chú ý đến
vector đầu vào l. Chúng tôi chuẩn hóa ACSA bằng cách
áp dụng softmax trên các slot khái niệm, tức là dọc theo
trục C. Điều này triển khai một hình thức cạnh tranh giữa
các slot để chú ý đến mỗi đầu vào l.

Sau đó, chúng tôi tìm cách nhóm và tổng hợp các đầu vào
được chú ý và thu được readout attention cho mỗi slot
khái niệm. Một cách trực quan, điều này đại diện cho mức
độ các đầu vào được chú ý đóng góp vào việc biểu diễn
ngữ nghĩa của mỗi khái niệm. Đối với điều này, chúng tôi
chuẩn hóa ma trận attention ACSA∈RC×L dọc theo trục L
và nhân nó với các giá trị đầu vào vCSA(E)∈RL×d. Điều
này tạo ra readout attention dưới dạng ma trận U∈RC×d
nơi mỗi hàng uc∈Rd là readout tương ứng với slot khái
niệm c; ACSA= softmax_C(qCSA(Sconcept)·kCSA(E)⊤/√d),
ACSA_c,l= ACSA_c,l/∑^L_l=1 ACSA_c,l, và U=ACSA·vCSA(E).
Chúng tôi sử dụng thông tin readout thu được từ liên kết
khái niệm và cập nhật mỗi slot khái niệm. Các cập nhật
tổng hợp U cuối cùng được sử dụng để cập nhật các slot
khái niệm thông qua một hàm tái phát học được, mà chúng
tôi sử dụng Đơn Vị Tái Phát Có Cổng (GRU) [14] với d
đơn vị ẩn để Sconcept= GRU(Sconcept,U). Các quá trình
trên tạo thành một lần lặp tinh chỉnh. Các slot khái niệm
thu được từ lần lặp cuối cùng

²Để đơn giản, kích thước nhúng d được chia sẻ đều trong phương pháp của chúng tôi.

--- TRANG 4 ---
(a) Transformer Tập Trung Khái Niệm thông qua Không Gian Làm Việc Được Chia Sẻ
(b) Phát Sóng Có Thể Diễn Giải đến Các Chuyên Gia
Hình 2. Kiến trúc tổng thể của Transformer Tập Trung Khái Niệm (CCT). (a) CCT (trong đường nét xám đặc) là một thay thế drop-in cho đầu phân loại của bất kỳ kiến trúc xương sống nào, như ViT hoặc CNN, và bao gồm hai mô-đun: (1) mô-đun Concept-Slot-Attention và (2) mô-đun Cross-Attention. (b) Quá trình huấn luyện sử dụng các mất mát để cảm ứng một sơ đồ phát sóng có thể diễn giải.

được coi là cuối cùng. Toàn bộ mô-đun được mô tả trong
Thuật toán 1 dưới dạng mã giả trong Phụ lục B.

4.2. Phát Sóng Bộ Nhớ Đã Cập Nhật đến Các Chuyên Gia
Ở giai đoạn này của SGW, mỗi chuyên gia phải cập nhật
trạng thái của mình bằng thông tin được phát sóng từ không
gian làm việc được chia sẻ. Chúng tôi cũng tận dụng cơ
chế cross-attention (gọi là mô-đun CA) để làm cho các
chuyên gia truy vấn (bước 3 trong Hình 1 và 2) và thực
hiện tích vô hướng giữa chúng và các giá trị từ các slot
khái niệm đã cập nhật để cập nhật trạng thái của mỗi
chuyên gia. Tuy nhiên, vì chúng tôi nhằm cấu hình một
mô hình có thể diễn giải và thực hiện các tác vụ phân loại,
chúng tôi sửa đổi quá trình lặp bằng cách kết hợp nó với
tác vụ phân loại downstream mong muốn của chúng tôi: i)
được hướng dẫn bởi kiến thức chuyên gia nếu các giải thích
khái niệm sự thật cơ bản có sẵn, hoặc ii) chỉ sử dụng mất
mát thưa thớt để thực thi việc tối thiểu hóa entropy của
thông tin phát sóng.

Một tập hợp L vector đặc trưng đầu vào E∈RL×D được
sử dụng lại với một phép chiếu tuyến tính qCA để đạt được
các truy vấn, và các phép chiếu kCA và vCA được áp dụng
cho các slot khái niệm được trích xuất với nhúng vị trí
Ŝ từ mô-đun CSA. Các khóa và giá trị kết quả được sử
dụng trong cơ chế cross-attention với các truy vấn, và
cross-attention sau đó xuất ra một trọng số attention
ACA= softmax_L(qCA(E)·kCA(Ŝconcept)⊤/√d)∈RL×C
giữa mỗi cặp patch–slot khái niệm. Đầu ra cuối cùng của
mô-đun CA là tích thu được bằng cách nhân bản đồ attention
ACA, các giá trị vCA(Ŝconcept)∈RC×d, và một ma trận
đầu ra O∈Rd×nc chiếu lên các logit (chưa chuẩn hóa) nc
trên các lớp đầu ra và sau đó tính trung bình trên các đặc
trưng đầu vào³; nghĩa là, đối với i= 1, . . . , nc,

logiti=1/L ∑^L_l=1 ACA_l·vCA(Ŝconcept)·O:,i (1)

Vậy, cho một đầu vào x vào mạng, xác suất có điều kiện
của lớp đầu ra i∈ {1, . . . , nc} là:

Pr(i|x) = softmax_i(∑^C_c=1 βc γc(x)) (2)

với các thành phần βc βci:= (vCA(Ŝconcept)·O)c,i nơi
γc(x) là điểm số liên quan không âm phụ thuộc vào x
thông qua các trọng số attention được tính trung bình;
nghĩa là, γc(x) = (1/L)∑^L_l=1 ACA_l,c. Chúng tôi có thể
diễn giải các phương trình trên từ hai quan điểm sau:

1) Giải Thích Dựa Trên Concept-slot Trung Thực Theo
Thiết Kế. Đầu ra mô-đun CA là một mô hình hồi quy logistic
đa thức trên các biến dương γc(x) đo lường đóng góp của
mỗi slot khái niệm. Tính trung thực là mức độ mà giải thích
phản ánh quyết định và nhằm đảm bảo rằng các giải thích
thực sự đang giải thích hoạt động của mô hình [33,48].
Như được chỉ ra trong [65], kết quả của mô-đun CA tuân
theo mối quan hệ tuyến tính giữa các vector giá trị và các
logit phân loại và đến từ các lựa chọn thiết kế của việc
tính toán đầu ra từ ma trận giá trị vCA(Ŝconcept) thông qua
phép chiếu tuyến tính vCA(Ŝconcept)·O và tổng hợp các
đóng góp patch bằng cách tính trung bình. Vậy, mô-đun
CA của chúng tôi cũng được đảm bảo trung thực theo thiết
kế bằng cách thỏa mãn Mệnh đề 1 trong [65] và các định
nghĩa kỹ thuật về tính trung thực từ [1].

³Để đơn giản, chúng tôi mô tả một mô hình attention đơn đầu ở đây; một phiên bản đa đầu [82] có sẵn và cũng được sử dụng trong các thí nghiệm của chúng tôi.

--- TRANG 5 ---
2) Cập Nhật Trạng Thái Động cho Các Chuyên Gia với
Phát Sóng Thông Tin. Trong định nghĩa ban đầu của SGW,
ACA·vCA(Ŝconcept) là tính toán chính thức của việc cập
nhật cho các chuyên gia (bước 3 từ [30, Mục 2.1]). Thay
vì áp dụng một quá trình lặp bổ sung để cập nhật các
chuyên gia, Phương trình 1 và 2 là để tạo ra các đầu ra
phân loại sử dụng trọng số O. Vậy, mặt nạ attention ACA
có thể chứa không chỉ thông tin từ bộ nhớ đã cập nhật mà
còn cả lỗi phân loại. Hơn nữa, bằng cách trực tiếp thao tác
mặt nạ ACA, cuối cùng chúng tôi định nghĩa mất mát giải
thích và mất mát thưa thớt để tăng cường khả năng giải
thích của mô hình.

4.3. Mục Tiêu Huấn Luyện cho Khả Năng Diễn Giải
Tính Hợp Lý theo Cấu Trúc với Mất Mát Giải Thích.
Tính hợp lý đề cập đến mức độ thuyết phục của việc diễn
giải đối với con người [9,33]. Để cung cấp các giải thích
có thể hiểu được bởi con người một cách hợp lý, chúng
tôi tận dụng ý tưởng hướng dẫn rõ ràng các đầu attention
để tập trung vào các khái niệm trong đầu vào dựa trên
chuyên môn lĩnh vực quan trọng để phân loại đầu vào một
cách chính xác. Tương tự như [20], cho một phân phối mong
muốn của attention H được cung cấp bởi kiến thức lĩnh
vực, các trọng số attention từ mô-đun CA của CCT ACA
được sử dụng như một thuật ngữ điều hòa bằng cách thêm
một chi phí giải thích vào hàm mục tiêu tỷ lệ với Lexpl=
∥A−H∥²F, nơi ∥·∥ là chuẩn Frobenius. Giải thích sự thật
cơ bản H có thể chỉ ra thông tin toàn cục (ví dụ, lớp con
hoặc thuộc tính chủ đạo của một con chim) hoặc thông tin
không gian/cấp độ mảnh hình ảnh (ví dụ, màu mắt của
một con chim). Dưới đây, chúng tôi chứng minh hiệu quả
của mất mát này trong các thí nghiệm của CIFAR100
Super-class và CUB-200-2011.

Mất Mát Thưa Thớt dựa trên Entropy. Lợi thế của các
nhãn phong phú, có nhiều thông tin hơn có thể tăng khả
năng diễn giải, nhưng điều này đi kèm với chi phí nỗ lực
chú thích bổ sung. Một phương pháp có thể bỏ qua sự đánh
đổi này sẽ đặc biệt có giá trị. Chúng tôi có thể đạt được
khả năng này thông qua cấu hình của chúng tôi liên quan
đến các tương tác giữa các chuyên gia và một không gian
làm việc được chia sẻ. Chúng tôi giới thiệu mất mát thưa
thớt dựa trên việc tối thiểu hóa entropy của mặt nạ attention
ACA; Lsparse =H(A) =H(a1, . . . , a|A|) = (1/|A|)∑i −ai·log(ai).
Mất mát này có thể được sử dụng trong các thí nghiệm có/
không có các giải thích sự thật cơ bản.

Mất Mát Cuối Cùng. Do đó, mất mát cuối cùng để huấn
luyện mô hình trở thành L=Lcls+λexplLexpl+λsparseLsparse,
nơi Lcls biểu thị mất mát phân loại thông thường. Lưu ý
rằng hằng số λexpl≥0 kiểm soát đóng góp tương đối của
mất mát giải thích vào tổng mất mát để mô hình của chúng
tôi có thể được áp dụng với các giải thích sự thật cơ bản
(λexpl>0) hoặc không có chúng (λexpl= 0). Cuối cùng, hằng
số λsparse xử lý cường độ của mất mát thưa thớt. Các thí
nghiệm của chúng tôi chứng minh rằng mô hình của chúng
tôi có thể hoạt động tốt mà không sử dụng các mất mát
phức tạp bổ sung, như mất mát tương phản hoặc tái tạo.

Bảng 1. Độ chính xác kiểm tra trên dự đoán nhãn lớp tinh tế (F.C.)
và siêu lớp (S.C.) trên CIFAR100. Lưu ý rằng việc phân loại siêu
lớp và lớp tinh tế được thực hiện đồng thời và loại thí nghiệm
này có thể được thực hiện trong học sâu với các ràng buộc, nhưng
phương pháp của chúng tôi và CT chỉ là trong số các phương pháp
dựa trên khái niệm. †chỉ ra kết quả từ [34].

| Mô Hình | F.C. Acc. (%) | S.C. Acc. (%) |
|---------|---------------|---------------|
| Vanilla ResNet† | NA | 83.2±0.2 |
| Vanilla ViT-T | NA | 86.2±0.3 |
| Hierarchical Model† | 71.2±0.2 | 84.7±0.1 |
| DL2† [23] | 75.3±0.1 | 84.3±0.1 |
| MultiplexNet† [34] | 74.4±0.2 | 85.4±0.3 |
| PIP-Net [61] | NA | 83.9±0.2 |
| ProtoPFormer [89] | NA | 81.7±0.1 |
| ProtoPool [68] | NA | 82.9±0.4 |
| ProtoPNet [11] | NA | 82.3±0.1 |
| Deform-ProtoPNet [21] | NA | 83.7±1.0 |
| BotCL [84] | NA | 56.9±10 |
| CT [65] | 73.3±2.9 | 92.1±0.2 |
| CCT: ViT-T+SA | 80.3±0.4 | 92.6±0.1 |
| CCT: ViT-T+I-SA | 83.3±0.1 | 92.8±0.1 |
| CCT: ViT-T+BO-QSA | 83.4±0.1 | 93.0±0.1 |

5. Thí Nghiệm
Chúng tôi đánh giá hiệu suất của CCT trên ba bộ dữ liệu
riêng biệt: CIFAR100 Super-class [23], CUB-200-2011 [83],
và ImageNet [19]. Các mục tiêu cụ thể hướng dẫn việc lựa
chọn những bộ dữ liệu này. Thứ nhất, CIFAR100 Super-
class là một bãi thử nghiệm để chứng minh khả năng đặc
biệt của mô hình chúng tôi trong điều kiện giám sát đầy
đủ, nơi các giải thích khái niệm toàn cục hoàn chỉnh có
sẵn. Thứ hai, CUB-200-2011 hoạt động như một bộ dữ liệu
trung gian, cho phép chúng tôi thể hiện sức mạnh của mô
hình trong cả thiết lập giải thích có giám sát và không giám
sát. Cuối cùng, chúng tôi thách thức CCT của mình với bộ
dữ liệu ImageNet, đại diện cho một kịch bản hoàn toàn
không giám sát do thiếu các giải thích khái niệm. Bất chấp
trở ngại này, chúng tôi điều chỉnh mô hình của mình để
đạt được hiệu suất đáng chú ý ngay cả khi không có bất
kỳ giải thích khái niệm nào. Thiết lập toàn diện này nhấn
mạnh khả năng thích ứng và tính linh hoạt của CCT trên
nhiều trường hợp sử dụng, xương sống trực quan và kịch
bản dữ liệu khác nhau. Kết quả thí nghiệm của chúng tôi
được xác thực mạnh mẽ thông qua ba hạt giống ngẫu nhiên
khác nhau và khoảng tin cậy 95%, với các chi tiết bổ sung
được cung cấp trong Phụ lục C.

5.1. Đánh Giá trên CIFAR100 Super-class
Bộ dữ liệu CIFAR100 Super-class là một biến thể của bộ
dữ liệu hình ảnh CIFAR100 [47]. Nó bao gồm 100 lớp hình
ảnh tinh tế (F.C.) được nhóm thêm thành 20

--- TRANG 6 ---
siêu lớp (S.C.). Ví dụ, năm lớp tinh tế baby, boy, girl, man,
và woman thuộc về siêu lớp people. Kể từ khi giới thiệu các
mô hình học sâu với các ràng buộc logic [23], bộ dữ liệu
này đã được sử dụng như một trong những bộ dữ liệu điểm
chuẩn để đánh giá hiệu quả của việc nhúng các ràng buộc
vào mạng nơ-ron (các khảo sát chuyên sâu có thể tìm thấy
trong [15, 28]). Nghiên cứu của chúng tôi nâng cao rằng
các khái niệm trong CCT của chúng tôi (và CT liên quan
chặt chẽ [65]) có thể được hiểu như các ràng buộc, với các
khác biệt được nêu trong Phụ lục C.4. Chúng tôi tận dụng
các lớp hình ảnh tinh tế riêng lẻ như các giải thích khái
niệm toàn cục cho một tác vụ dự đoán đa lớp với 20 siêu
lớp, sử dụng Vision Transformer-Tiny (ViT-T)⁴ như xương
sống⁵.

Theo [34], hiệu suất của CCT được so sánh với ba nhóm
đường cơ sở. Nhóm đầu tiên bao gồm các mô hình xương
sống vanilla như Wide ResNet 28-10 [93] và ViT-T. Nhóm
thứ hai liên quan đến các mô hình mạng nơ-ron với các
ràng buộc logic, bao gồm Hierarchical Model [34], DL2
[23], và MultiplexNet [34]. Nhóm thứ ba bao gồm các mô
hình có thể giải thích dựa trên khái niệm; ngoại trừ CT, các
mô hình này không thể xử lý cả tác vụ lớp tinh tế và siêu
lớp đồng thời. Đối với mô-đun CSA của CCT, chúng tôi
cấu hình ba biến thể SA—SA, I-SA, và BO-QSA—với xương
sống và đánh giá độ chính xác lớp tinh tế bằng cách so sánh
các giải thích khái niệm sự thật cơ bản với các khái niệm
dự đoán top-1 dựa trên điểm số attention kết quả.

Bảng 1 trình bày các kết quả thí nghiệm, cho thấy CCT
của chúng tôi vượt trội đáng kể so với tất cả các đường cơ
sở. Nó tăng cường đáng kể hiệu suất xương sống ViT-T và
đạt được sự gia tăng đáng chú ý trong độ chính xác lớp
tinh tế so với CT. Đáng chú ý, CT hoạt động kém hiệu quả
hơn so với các phương pháp dựa trên ràng buộc logic, nâng
cao vai trò vượt trội của mô-đun của chúng tôi trong việc
hình thành các khái niệm toàn cục.

Hình 3 cho thấy hai ví dụ chứng minh nơi học khái niệm
của CCT vượt trội cả về định lượng và định tính so với CT.
Độ chính xác lớp tinh tế kém của CT có thể là kết quả của
việc tạo ra các ảo giác để đạt được độ chính xác siêu lớp
một cách tham lam mà không hình thành các khái niệm
tiềm ẩn vững chắc. Mặc dù cả hai mô hình đều phân loại
đúng siêu lớp trên các ví dụ được hiển thị, các quá trình ra
quyết định giải thích hoạt động hoàn toàn khác nhau trong
hai mô hình. Mặc dù lớp sự thật cơ bản của hình ảnh bên
trái trong Hình 3 là boy, khái niệm lớp khớp tốt nhất với
điểm số attention cao nhất bởi CT là baby, đây là lớp tinh
tế không chính xác nhưng cũng thuộc về siêu lớp people
chính xác. Trong ví dụ hình ảnh bên phải, chúng ta quan
sát thấy hành vi tương tự của CT. Ngược lại, các khái niệm
dự đoán của CCT cho cả hai ví dụ đều khớp chính xác với
các lớp tinh tế sự thật cơ bản của chúng với điểm số attention
khái niệm thưa thớt hơn

⁴Đối với thí nghiệm này, Swin Transformer và ConvNeXt không được sử dụng
như xương sống cho mô hình của chúng tôi vì số lượng tham số của hai mô hình
(cả SwinT-T và ConvNeXt-T đều là 28M) lớn hơn một trong ViT-T.
⁵Đối với các khái niệm toàn cục, chúng tôi sử dụng nhúng của token CLS như đầu vào.

Hình 3. So sánh dự đoán lớp cho CT [65] và CCT ("Ours") trong
các ví dụ nơi cả hai đều đưa ra dự đoán siêu lớp CIFAR100 chính
xác. 100 lớp được đánh chỉ số từ 1 (trên cùng bên trái trong lưới
10x10) đến 100 (dưới cùng bên phải trong lưới 10x10). (Trái)
Nhãn lớp sự thật cơ bản là boy(12), nhưng CT dự đoán sai thành
baby (3), trong khi dự đoán của CCT là chính xác. (Phải) Nhãn
sự thật cơ bản là shark (84), nhưng CT chọn sai ray(78) trong khi
CCT lại đưa ra dự đoán lớp chính xác.

so với CT. Các chi tiết và kết quả thêm được mô tả trong
Phụ lục C.4.

5.2. Đánh Giá trên CUB-200-2011
Bộ dữ liệu CUB-200-2011 [83] bao gồm 11,788 hình ảnh
chim được phân loại thành 200 loài. Mỗi hình ảnh được
chú thích với nhiều khái niệm rời rạc khác nhau, ví dụ, hình
dạng của mỏ, hoặc màu sắc của cơ thể, hỗ trợ nhận dạng
loài. Bộ dữ liệu liên quan đến 312 khái niệm được phân phối
không đều trên các hình ảnh, vì vậy chúng tôi sử dụng
phương pháp tiền xử lý từ [65] để giải quyết điều này. Kết
quả và chi tiết bổ sung được mô tả trong Phụ lục C.5.

Với Giải Thích Khái Niệm. Chúng tôi xem xét một kịch
bản thế giới thực nơi mối quan hệ nhiều-nhiều và không
xác định giữa các khái niệm và đầu ra tồn tại, cùng với sự
kết hợp của các khái niệm toàn cục và được định vị không
gian. Chúng tôi sử dụng các mô-đun CSA và CA trong CCT
để xử lý cả khái niệm toàn cục và không gian, tính trung
bình các logit của chúng cho khả năng diễn giải. Chúng tôi
sử dụng nhiều xương sống khác nhau, bao gồm ViT [22],
Swin Transformer (SwinT) [55], và ConvNeXt [56] cho bộ
dữ liệu này. Chúng tôi sử dụng các nhúng của các patch
hình ảnh được token hóa, trong khi như đầu vào cho CCT
phụ trách các khái niệm, chúng tôi sử dụng nhúng của token
CLS.

Trong Bảng 2, chúng tôi so sánh CCT với các phương pháp
khác dựa trên huấn luyện Đa giai đoạn (tức là, huấn luyện
phức tạp) và End-to-end (tức là, huấn luyện với lan truyền
ngược). Tất cả các cấu hình của CCT đạt được hơn 87%
độ chính xác phân loại, vượt trội rõ ràng so với các phương
pháp khác. Điều này xác nhận rằng cấu hình tổng thể của
CCT tăng cường hiệu suất phân loại. Đáng chú ý, mô hình
của chúng tôi vượt qua các đường cơ sở không thể diễn
giải (B-CNN) và các phương pháp yêu cầu huấn luyện phức
tạp.

Hình 4 nêu bật sự khác biệt giữa CT và

--- TRANG 7 ---
Bảng 2. So sánh hiệu suất trên CUB-200-2011. Đối với B-CNN [53], Part R-CNN [96], PS-CNN [37], PN-CNN [7], SPDA-CNN [95], PA-CNN [46], MG-CNN [85], 2-level attn. [88], FCAN [54], Neural const. [76], ProtoPNet [12], CAM [98], DeepLAC [52], ST-CNN [38], MA-CNN [97], và RA-CNN [24], hiệu suất từ [65]. Đối với ProtoPFormer [89] và CEM [94], hiệu suất tốt nhất trực tiếp từ các công trình của họ. Đối với CT [65] và CCT, kết quả từ đánh giá của chúng tôi. Số lượng tham số của xương sống chúng tôi sử dụng là: 307M (ViT-L), 197M (SwinT-L), và 197M (ConvNeXt-L). (w/w.o) chỉ ra hiệu suất có/không có giải thích khái niệm sự thật cơ bản.

| Phương Pháp | Độ Chính Xác Kiểm Tra (%) |
|-------------|---------------------------|
| **Đa giai đoạn** | Part R-CNN: 76.4 PS-CNN: 76.2 PN-CNN: 85.4 |
| | SPDA-CNN: 85.1 PA-CNN: 82.8 MG-CNN: 83.0 |
| | 2-level attn.: 77.9 FCAN: 82.0 Neural const.: 81.0 |
| | ProtoPNet: 84.8 Deform-ProtoPNet: 86.5 PIP-net: 84.3±0.2 |
| **End-to-end** | B-CNN: 85.1 CAM: 70.5 DeepLAC: 80.3 |
| | ST-CNN: 84.1 MA-CNN: 86.5 RA-CNN: 85.3 |
| | CEM: 77.1 ProtoPFormer: 84.9 CT (w/w.o): 86.4±0.2/75.4±0.3 |
| **CCT (của chúng tôi)** | ViT-L+SA: 90.0±0.3 ViT-L+I-SA: 90.3±0.3 ViT-L+BO-QSA: 90.3±0.1 |
| | SwinT-L+SA: 90.7±0.02 SwinT-L+I-SA: 90.9±0.4 SwinT-L+BO-QSA (w/w.o): 91.2±0.2/90.9±0.4 |
| | ConvNeXt-L+SA: 87.8±0.3 ConvNeXt-L+I-SA: 89.3±0.6 ConvNeXt-L+BO-QSA: 89.4±0.4 |

Hình 4. So sánh dự đoán giữa CT và CCT của chúng tôi với giải
thích sự thật cơ bản. (Trái) Tất cả dự đoán của CT đều không
chính xác. Các giải thích được tô sáng màu tím là các thuộc tính
chính thường có mặt khi phân loại đúng Pelagic Cormorant,
nhãn không chính xác. (Phải) Dự đoán của CCT là chính xác.

CCT, minh họa rằng CT thiếu việc học các khái niệm toàn
cục. Chúng tôi nhấn mạnh các khái niệm với điểm số attention
cao nhất trong tất cả hình ảnh. Hình cho thấy một trường
hợp nơi dự đoán của CT hoàn toàn không chính xác bằng
cách hội tụ về nhãn sai duy nhất. Mặc dù lớp sự thật cơ
bản của các hình ảnh là Brandt Cormorant, tất cả dự đoán
của CT đều là Pelagic Cormorant. Hơn nữa, các thuộc tính
mà CT sử dụng để đưa ra phân loại không chính xác bao
gồm một tập con các thuộc tính (các thuộc tính màu tím
trong Hình 4) thường liên quan đến dự đoán chính xác của
nhãn Pelagic Cormorant không chính xác. Do đó, CT tập
trung vào patch hình ảnh tạo ra ảo giác về các khía cạnh
chính liên quan đến các nhãn không chính xác, trong khi
CCT của chúng tôi cho thấy các giải thích khái niệm mạnh
mẽ hơn về các phân loại chính xác.

Không Có Giải Thích Khái Niệm. Mặc dù kiến thức
chuyên gia lĩnh vực là công cụ hiệu quả nhất để hướng dẫn
khả năng giải thích của mô hình, bước tiền xử lý để định
nghĩa các khái niệm trực quan cho các tác vụ có thể yêu
cầu việc gán nhãn tốn thời gian và dựa vào phán đoán của
con người. Quan trọng là, chúng tôi chứng minh rằng CCT
của chúng tôi cũng hoạt động hiệu quả mà không có giải
thích khái niệm, một khả năng không được chia sẻ bởi các
mô hình khác. Chúng tôi có thể dễ dàng thiết lập mất mát
của mình với λexpl= 0 (Mất Mát Cuối Cùng được định
nghĩa trong phần 4.3) và đánh giá mô hình của chúng tôi
với cùng thiết lập siêu tham số của thí nghiệm với giải thích
chỉ sử dụng mất mát phân loại và mất mát thưa thớt.

Trong Bảng 2, cấu hình tốt nhất của CCT (Swin-L+BO-
QSA) không có giải thích đạt được 90% độ chính xác kiểm
tra, rất nhỏ so với cái có giải thích. Ngược lại, CT cũng có
thể được huấn luyện mà không có giải thích, nhưng hiệu
suất của nó bị suy giảm rõ rệt.

Hình 5 trực quan hóa việc kích hoạt các khái niệm tiềm
ẩn đã học trong mô hình của chúng tôi từ bộ dữ liệu. Theo
[84], chúng tôi bổ sung huấn luyện CCT bằng cách đặt số
lượng nhãn lớp (n) thành 50 và số lượng khái niệm tiềm
ẩn (k) thành 20. Hình thể hiện năm khái niệm tiềm ẩn—7,
3, 19, 5, và 4. Trong kết quả thí nghiệm của chúng tôi, chúng
tôi xác định các khái niệm chính mà CCT tập trung vào khi
phân loại hình ảnh chim. Khái niệm 7 tập trung vào mỏ và
thân trên, trong khi Khái niệm 3 xem xét nhiều đặc trưng
như mỏ, mắt, và đuôi. Khái niệm 19 nắm bắt các cấu trúc
đầu và lông độc đáo, Khái niệm 5 phác thảo toàn bộ cơ thể
chim, và Khái niệm 4 tách biệt chim khỏi nền phức tạp.
Những điều này chứng minh sự tập trung tinh tế và kỹ năng
phân loại của mô hình, ngay cả trong môi trường không
giám sát nơi chúng ta không có quyền truy cập vào giải
thích khái niệm của nó.

--- TRANG 8 ---
Hình 5. Việc kích hoạt các khái niệm tiềm ẩn đã học từ CUB-200-2011 và các ví dụ cho thấy mỗi khái niệm tiềm ẩn. Kết quả thêm có thể tìm thấy trong Phụ lục C.5.

Bảng 3. Độ chính xác kiểm tra trên ImageNet. Chúng tôi sử dụng 200 lớp đầu tiên theo [84]. †chỉ ra kết quả tốt nhất từ [84]. Số lượng tham số là: 22M (ViT-S), 45M (ResNet-101), 50M (ConvNeXt-S), và 86M (Deit-B).

| Mô Hình | Độ Chính Xác Kiểm Tra (%) |
|---------|---------------------------|
| Vanilla ViT-S | 83.3±0.2 |
| ProtoPFormer(Deit-B) [89] | 83.4±2.2 |
| ProtoPool [68] (ResNet-101) | 76.5±0.8 |
| ProtoPNet [11] (ResNet-101) | 77.7±0.3 |
| Deform-ProtoPNet [21] (ResNet-101) | 76.1±0.3 |
| CT [65] (ViT-S) | 27.0±0.2 |
| BotCL† [84] (ResNet-101) | 83.0 |
| CCT: ViT-S+SA | 76.3±0.2 |
| CCT: ViT-S+I-SA | 83.6±0.2 |
| CCT: ViT-S+BO-QSA | 83.7±0.2 |

5.3. Đánh Giá trên ImageNet
Cuối cùng, để xác thực rằng mô hình của chúng tôi có thể
học các khái niệm tiềm ẩn mà không có giải thích, chúng
tôi kiểm tra CCT trên ImageNet theo phương pháp trong
[84]. Chúng tôi sử dụng ViT tương đối nhỏ (ViT-S) như
xương sống⁶. Kết quả và chi tiết bổ sung có trong Phụ lục
C.6.

Bảng 3 cho thấy hiệu suất phân loại ImageNet. Sự kết
hợp của ViT-S với BO-QSA trong thí nghiệm có hiệu suất
tốt nhất, đạt được 83.7% độ chính xác kiểm tra mặc dù sử
dụng xương sống kích thước nhỏ, điều này đáp ứng hoặc
vượt qua hiệu suất của các phương pháp SOTA khác—bao
gồm một số phương pháp dựa trên khái niệm không thể
áp dụng hoặc đạt được hiệu suất rất kém.

Ngoài ra, chúng tôi huấn luyện một mô hình CCT đơn giản
bằng cách đặt số lượng lớp (n) thành 20 và số lượng khái
niệm tiềm ẩn (k) thành 10 như trong [84] để trực quan hóa
tính nhất quán của các khái niệm đã học. Hình 6 mô tả năm
khái niệm chúng tôi

⁶Chúng tôi tránh sử dụng xương sống lớn hơn ResNet-101 (45M), điều này loại trừ SwinT-S (50M) và ConvNeXt-S (50M).

Hình 6. Việc kích hoạt các khái niệm tiềm ẩn đã học từ ImageNet và các ví dụ cho thấy mỗi khái niệm tiềm ẩn. Kết quả thêm có thể tìm thấy trong Phụ lục C.6.

lựa chọn và năm cặp hình ảnh đại diện cho mỗi khái niệm
tiềm ẩn đã học từ ImageNet. CCT của chúng tôi cho thấy
rằng nó xuất sắc trong việc tách biệt và nhấn mạnh các
đặc trưng đối tượng cụ thể trên nhiều khái niệm khác nhau.
Khái niệm 10 tách biệt đường viền gà bằng cách tô sáng
nền, trong khi Khái niệm 8 tập trung vào vùng bụng của
chim. Khái niệm 3 khéo léo phân tách các thành phần cá,
Khái niệm 5 nắm bắt vùng miệng cá mập, và Khái niệm 6
phác thảo hình dạng cá vàng. Khả năng của CCT trong việc
tạo đường viền và tô sáng các vùng có ý nghĩa ngữ nghĩa
vượt qua những mô hình hiện có. Nó cũng xuất sắc trong
truy xuất hình ảnh không giám sát, nhóm các hình ảnh có
ý nghĩa ngữ nghĩa tương tự với nhau như rõ ràng trong
Hình 6. Điều này thể hiện sức mạnh của phương pháp tập
trung khái niệm của chúng tôi trong việc tạo ra kết quả
nhất quán về mặt ngữ nghĩa.

6. Kết Luận
Chúng tôi đề xuất Transformer Tập Trung Khái Niệm, một
mô hình có thể diễn giải nội tại thông qua Không Gian Làm
Việc Toàn Cầu Được Chia Sẻ, cho phép đạt được khả năng
diễn giải, hiệu suất và tính linh hoạt tốt hơn khi kiến thức
chuyên gia có sẵn hoặc không. Một hướng nghiên cứu tương
lai tự nhiên là mở rộng mô-đun trích xuất khái niệm để có
được các "mảnh" kiến thức có thể tổ hợp [77] và sau đó học
các quy tắc hoặc cơ chế tổ hợp cơ bản liên quan các mảnh
đã có được với nhau. Những quy tắc đó có thể được nắm
bắt một cách chính thức, như với logic bậc nhất [6]. Cũng
đầy hứa hẹn khi điều tra các ứng dụng khác, như gỡ lỗi
mô hình và chẩn đoán hình ảnh y tế.

Lời Cảm Ơn: Được hỗ trợ một phần bởi giải thưởng NSF
2223839 và giải thưởng USACE ERDC W912HZ-21-2-0040.

Tài Liệu Tham Khảo
[1] David Alvarez Melis và Tommi Jaakkola. Towards robust
interpretability with self-explaining neural networks. Ad-
vances in Neural Information Processing Systems, 31, 2018.
1, 2, 4

--- TRANG 9 ---
[2] Bernard J Baars. A cognitive theory of consciousness. Cam-
bridge University Press, 1993. 2
[3] Dzmitry Bahdanau, Kyunghyun Cho, và Yoshua Bengio.
Neural machine translation by jointly learning to align and
translate. arXiv preprint arXiv:1409.0473, 2014. 1
[4] Carliss Young Baldwin và Kim B Clark. Design rules: The
power of modularity, volume 1. MIT press, 2000. 1
[5] Dana H Ballard. Cortical connections and parallel process-
ing: Structure and function. Behavioral and brain sciences,
9(1):67–90, 1986. 1
[6] Pietro Barbiero, Gabriele Ciravegna, Francesco Giannini,
Pietro Liò, Marco Gori, và Stefano Melacci. Entropy-based
logic explanations of neural networks. In Proceedings of the
AAAI Conference on Artificial Intelligence, volume 36, pages
6046–6054, 2022. 1, 2, 8
[7] Steve Branson, Grant Van Horn, Serge Belongie, và Pietro
Perona. Bird species categorization using pose normalized
deep convolutional nets. arXiv preprint arXiv:1406.2952,
2014. 7
[8] Rodney A Brooks. Intelligence without representation. Arti-
ficial intelligence, 47(1-3):139–159, 1991. 1
[9] Diogo V Carvalho, Eduardo M Pereira, và Jaime S Cardoso.
Machine learning interpretability: A survey on methods and
metrics. Electronics, 8(8):832, 2019. 1, 5
[10] Michael Chang, Tom Griffiths, và Sergey Levine. Object
representations as fixed points: Training iterative refinement
algorithms with implicit differentiation. Advances in Neural
Information Processing Systems, 35:32694–32708, 2022. 3,
A
[11] Chaofan Chen, Oscar Li, Alina Barnett, Jonathan Su, và
Cynthia Rudin. This looks like that: deep learning for inter-
pretable image recognition. CoRR, abs/1806.10574, 2018. 5,
8
[12] Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia
Rudin, và Jonathan K Su. This looks like that: deep learn-
ing for interpretable image recognition. Advances in Neural
Information Processing Systems, 32, 2019. 1, 2, 7
[13] Zhi Chen, Yijie Bei, và Cynthia Rudin. Concept whitening
for interpretable image recognition. Nature Machine Intelli-
gence, 2(12):772–782, 2020. 1, 2
[14] Kyunghyun Cho, Bart van Merriënboer, Caglar Gulcehre,
Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, và
Yoshua Bengio. Learning phrase representations using RNN
encoder–decoder for statistical machine translation. In Pro-
ceedings of the 2014 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 1724–1734,
2014. 3
[15] Tirtharaj Dash, Sharad Chitlangia, Aditya Ahuja, và Ash-
win Srinivasan. A review of some techniques for inclusion
of domain-knowledge into deep neural networks. Scientific
Reports, 12(1):1040, 2022. 6
[16] Stanislas Dehaene và Jean-Pierre Changeux. Experimental
and theoretical approaches to conscious processing. Neuron,
70(2):200–227, 2011. 2
[17] Stanislas Dehaene, Michel Kerszberg, và Jean-Pierre
Changeux. A neuronal model of a global workspace in ef-
fortful cognitive tasks. Proceedings of the national Academy
of Sciences, 95(24):14529–14534, 1998. 2

[18] Stanislas Dehaene, Hakwan Lau, và Sid Kouider. What is
consciousness, and could machines have it? Robotics, AI,
and Humanity: Science, Ethics, and Policy, pages 43–56,
2021. 2
[19] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
và Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition, pages 248–255. Ieee, 2009. 2, 5
[20] Ameet Deshpande và Karthik Narasimhan. Guiding atten-
tion for self-supervised learning with transformers. arXiv
preprint arXiv:2010.02399, 2020. 5
[21] Jon Donnelly, Alina Jade Barnett, và Chaofan Chen. De-
formable protopnet: An interpretable image classifier using
deformable prototypes. CoRR, abs/2111.15000, 2021. 5, 8
[22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020. 2, 6, B, C
[23] Marc Fischer, Mislav Balunovic, Dana Drachsler-Cohen, Ti-
mon Gehr, Ce Zhang, và Martin Vechev. DL2: training
and querying neural networks with logic. In International
Conference on Machine Learning, pages 1931–1941. PMLR,
2019. 2, 5, 6, B, C
[24] Jianlong Fu, Heliang Zheng, và Tao Mei. Look closer to
see better: Recurrent attention convolutional neural network
for fine-grained image recognition. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 4438–4446, 2017. 2, 7
[25] Amirata Ghorbani, James Wexler, James Y Zou, và Been
Kim. Towards automatic concept-based explanations. Ad-
vances in Neural Information Processing Systems, 32, 2019.
1, 2
[26] Ross Girshick. Fast R-CNN. In Proceedings of the IEEE
International Conference on Computer Vision, pages 1440–
1448, 2015. 2
[27] Ross Girshick, Jeff Donahue, Trevor Darrell, và Jitendra
Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
580–587, 2014. 2
[28] Eleonora Giunchiglia, Mihaela Catalina Stoian, và Thomas
Lukasiewicz. Deep learning with logical constraints. arXiv
preprint arXiv:2205.00523, 2022. 6
[29] Anirudh Goyal và Yoshua Bengio. Inductive biases for deep
learning of higher-level cognition. Proceedings of the Royal
Society A, 478(2266):20210068, 2022. 2
[30] Anirudh Goyal, Aniket Rajiv Didolkar, Alex Lamb, Kar-
tikeya Badola, Nan Rosemary Ke, Nasim Rahaman,
Jonathan Binas, Charles Blundell, Michael Curtis Mozer,
và Yoshua Bengio. Coordination among neural modules
through a shared global workspace. In International Confer-
ence on Learning Representations, 2021. 1, 2, 3, 5
[31] Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun
Sodhani, Sergey Levine, Yoshua Bengio, và Bernhard
Schölkopf. Recurrent independent mechanisms. arXiv
preprint arXiv:1909.10893, 2019. 2

--- TRANG 10 ---
[32] Yash Goyal, Amir Feder, Uri Shalit, và Been Kim. Ex-
plaining classifiers with causal concept effect (CaCE). arXiv
preprint arXiv:1907.07165, 2019. 1, 2
[33] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri,
Franco Turini, Fosca Giannotti, và Dino Pedreschi. A sur-
vey of methods for explaining black box models. ACM Com-
puting Surveys (CSUR), 51(5):1–42, 2018. 4, 5
[34] Nick Hoernle, Rafael Michael Karampatsis, Vaishak Belle,
và Kobi Gal. MultiplexNet: Towards fully satisfied log-
ical constraints in neural networks. In Proceedings of the
AAAI Conference on Artificial Intelligence, volume 36, pages
5700–5709, 2022. 5, 6, B
[35] Jinyung Hong và Ted Pavlic. Representing prior knowledge
using randomly, weighted feature networks for visual rela-
tionship detection. In Combining Learning and Reasoning:
Programming Languages, Formalisms, and Representations,
2022. 2
[36] Jinyung Hong và Theodore P. Pavlic. An insect-inspired
randomly, weighted neural network with random fourier fea-
tures for neuro-symbolic relational learning. In Proceed-
ings of the 15th International Workshop on Neural-Symbolic
Learning and Reasoning (Ne/Sy 2022), October 25–27 2021.
2
[37] Shaoli Huang, Zhe Xu, Dacheng Tao, và Ya Zhang. Part-
stacked CNN for fine-grained visual categorization. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1173–1182, 2016. 2, 7
[38] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.
Spatial transformer networks. Advances in Neural Informa-
tion Processing Systems, 28, 2015. 7
[39] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,
Andrew Zisserman, và Joao Carreira. Perceiver: General
perception with iterative attention. In International confer-
ence on machine learning, pages 4651–4664. PMLR, 2021.
2
[40] Sarthak Jain và Byron C Wallace. Attention is not explana-
tion. arXiv preprint arXiv:1902.10186, 2019. 1
[41] Baoxiong Jia, Yu Liu, và Siyuan Huang. Improving object-
centric learning with query optimization. In The Eleventh In-
ternational Conference on Learning Representations, 2022.
3, A
[42] Dmitry Kazhdan, Botty Dimanov, Mateja Jamnik, Pietro Liò,
và Adrian Weller. Now you see me (CME): concept-based
model extraction. arXiv preprint arXiv:2010.13233, 2020.
1, 2
[43] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai,
James Wexler, Fernanda Viegas, et al. Interpretability be-
yond feature attribution: Quantitative testing with concept
activation vectors (TCAV). In International Conference on
Machine Learning, pages 2668–2677. PMLR, 2018. 1, 2
[44] Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maxi-
milian Alber, Kristof T Schütt, Sven Dähne, Dumitru Erhan,
và Been Kim. The (un) reliability of saliency methods. Ex-
plainable AI: Interpreting, explaining and visualizing deep
learning, pages 267–280, 2019. 1
[45] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen
Mussmann, Emma Pierson, Been Kim, và Percy Liang.

Concept bottleneck models. In International Conference on
Machine Learning, pages 5338–5348. PMLR, 2020. 1, 2
[46] Jonathan Krause, Hailin Jin, Jianchao Yang, và Li Fei-Fei.
Fine-grained recognition without part annotations. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 5546–5555, 2015. 7
[47] Alex Krizhevsky. Learning multiple layers of features from
tiny images. Master's thesis, University of Toronto, Toronto,
Canada, 2009. 5
[48] Himabindu Lakkaraju, Ece Kamar, Rich Caruana, và Jure
Leskovec. Faithful and customizable explanations of black
box models. In Proceedings of the 2019 AAAI/ACM Confer-
ence on AI, Ethics, and Society, pages 131–138, 2019. 4
[49] Liangzhi Li, Bowen Wang, Manisha Verma, Yuta
Nakashima, Ryo Kawasaki, và Hajime Nagahara. Scouter:
Slot attention-based classifier for explainable image recog-
nition. In Proceedings of the IEEE/CVF international
conference on computer vision, pages 1046–1055, 2021. 3
[50] Oscar Li, Hao Liu, Chaofan Chen, và Cynthia Rudin. Deep
learning for case-based reasoning through prototypes: A
neural network that explains its predictions. In Proceedings
of the AAAI Conference on Artificial Intelligence, volume 32,
2018. 1, 2
[51] Zenan Li, Zehua Liu, Yuan Yao, Jingwei Xu, Taolue Chen,
Xiaoxing Ma, L Jian, et al. Learning with logical constraints
but without shortcut satisfaction. In The Eleventh Interna-
tional Conference on Learning Representations, 2023. B
[52] Di Lin, Xiaoyong Shen, Cewu Lu, và Jiaya Jia. Deep
lac: Deep localization, alignment and classification for fine-
grained recognition. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 1666–
1674, 2015. 7
[53] Tsung-Yu Lin, Aruni RoyChowdhury, và Subhransu Maji.
Bilinear CNN models for fine-grained visual recognition. In
Proceedings of the IEEE International Conference on Com-
puter Vision, pages 1449–1457, 2015. 7
[54] Xiao Liu, Tian Xia, Jiang Wang, Yi Yang, Feng Zhou, và
Yuanqing Lin. Fully convolutional attention networks for
fine-grained recognition. arXiv preprint arXiv:1603.06765,
2016. 7
[55] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, và Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF international conference on
computer vision, pages 10012–10022, 2021. 2, 6, B, C
[56] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-
enhofer, Trevor Darrell, và Saining Xie. A convnet for the
2020s. In Proceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition, pages 11976–11986,
2022. 2, 6, B, C
[57] Francesco Locatello, Dirk Weissenborn, Thomas Un-
terthiner, Aravindh Mahendran, Georg Heigold, Jakob
Uszkoreit, Alexey Dosovitskiy, và Thomas Kipf. Object-
centric learning with slot attention. Advances in Neural In-
formation Processing Systems, 33:11525–11538, 2020. 2, 3,
A

--- TRANG 11 ---
[58] Scott M Lundberg và Su-In Lee. A unified approach to
interpreting model predictions. Advances in Neural Infor-
mation Processing Systems, 30, 2017. 1, 2
[59] Marvin Minsky. Society of mind. Simon and Schuster, 1988.
2
[60] Tsendsuren Munkhdalai, Alessandro Sordoni, Tong Wang,
và Adam Trischler. Metalearned neural memory. Advances
in Neural Information Processing Systems, 32, 2019. 2
[61] Meike Nauta, Jörg Schlötterer, Maurice van Keulen, và
Christin Seifert. Pip-net: Patch-based intuitive prototypes
for interpretable image classification. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pages 2744–2753, June 2023. 5
[62] Anh Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas
Brox, và Jeff Clune. Synthesizing the preferred inputs for
neurons in neural networks via deep generator networks. Ad-
vances in Neural Information Processing Systems, 29, 2016.
2
[63] Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner,
Philipp Seidl, Michael Widrich, Thomas Adler, Lukas
Gruber, Markus Holzleitner, Milena Pavlović, Geir Kjetil
Sandve, et al. Hopfield networks is all you need. arXiv
preprint arXiv:2008.02217, 2020. 2
[64] Marco Tulio Ribeiro, Sameer Singh, và Carlos Guestrin.
"why should i trust you?" explaining the predictions of any
classifier. In Proceedings of the 22nd ACM SIGKDD interna-
tional conference on knowledge discovery and data mining,
pages 1135–1144, 2016. 1, 2
[65] Mattia Rigotti, Christoph Miksovic, Ioana Giurgiu, Thomas
Gschwind, và Paolo Scotton. Attention-based interpretabil-
ity with concept transformers. In International Conference
on Learning Representations, 2021. 1, 2, 4, 5, 6, 7, 8, A, B
[66] Philip Robbins. Modularity of mind. In Edward N. Zalta, ed-
itor, The Stanford Encyclopedia of Philosophy. Metaphysics
Research Lab, Stanford University, Winter 2017 edition,
2017. 1, 2
[67] Cynthia Rudin. Stop explaining black box machine learn-
ing models for high stakes decisions and use interpretable
models instead. Nature Machine Intelligence, 1(5):206–215,
2019. 1
[68] Dawid Rymarczyk, Lukasz Struski, Michal Górczak, Ko-
ryna Lewandowska, Jacek Tabor, và Bartosz Zielinski. In-
terpretable image classification with differentiable proto-
types assignment. CoRR, abs/2112.02902, 2021. 5, 8
[69] Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae,
Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol
Vinyals, Razvan Pascanu, và Timothy Lillicrap. Relational
recurrent neural networks. Advances in neural information
processing systems, 31, 2018. 2
[70] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,
Ramakrishna Vedantam, Devi Parikh, và Dhruv Batra.
Grad-cam: Visual explanations from deep networks via
gradient-based localization. In Proceedings of the IEEE In-
ternational Conference on Computer Vision, pages 618–626,
2017. 1, 2
[71] Sofia Serrano và Noah A Smith. Is attention interpretable?
arXiv preprint arXiv:1906.03731, 2019. 1

[72] Murray Shanahan. A cognitive architecture that combines
internal simulation with a global workspace. Consciousness
and cognition, 15(2):433–449, 2006. 2
[73] Murray Shanahan. Embodiment and the inner life: Cognition
and Consciousness in the Space of Possible Minds. Oxford
University Press, 2010. 2
[74] Murray Shanahan. The brain's connective core and its role
in animal cognition. Philosophical Transactions of the Royal
Society B: Biological Sciences, 367(1603):2704–2714, 2012.
2
[75] Murray Shanahan và Bernard Baars. Applying global
workspace theory to the frame problem. Cognition,
98(2):157–176, 2005. 2
[76] Marcel Simon và Erik Rodner. Neural activation constella-
tions: Unsupervised part model discovery with convolutional
networks. In Proceedings of the IEEE International Confer-
ence on Computer Vision, pages 1143–1151, 2015. 7
[77] Gautam Singh, Yeongbin Kim, và Sungjin Ahn. Neural
systematic binder. In The Eleventh International Conference
on Learning Representations, 2023. 8
[78] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas,
và Martin Wattenberg. Smoothgrad: removing noise by
adding noise. arXiv preprint arXiv:1706.03825, 2017. 1, 2
[79] Jiawei Su, Danilo Vasconcellos Vargas, và Kouichi Sakurai.
One pixel attack for fooling deep neural networks. IEEE
Transactions on Evolutionary Computation, 23(5):828–841,
2019. 1
[80] Mukund Sundararajan, Ankur Taly, và Qiqi Yan. Axiomatic
attribution for deep networks. In International Conference
on Machine Learning, pages 3319–3328. PMLR, 2017. 2
[81] Aäron Van Den Oord, Nal Kalchbrenner, và Koray
Kavukcuoglu. Pixel recurrent neural networks. In Interna-
tional Conference on Machine Learning, pages 1747–1756.
PMLR, 2016. 2
[82] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia
Polosukhin. Attention is all you need. Advances in Neural
Information Processing Systems, 30, 2017. 4
[83] Catherine Wah, Steve Branson, Peter Welinder, Pietro
Perona, và Serge Belongie. Caltech–UCSD Birds-200-
2011 (CUB-200-2011) dataset. Technical Report CNS-TR-
2011-001, California Institute of Technology, 2011. 2, 5, 6
[84] Bowen Wang, Liangzhi Li, Yuta Nakashima, và Hajime Na-
gahara. Learning bottleneck concepts in image classification.
In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 10962–10971, 2023.
3, 5, 7, 8, B, C, D, F, G, H, I
[85] Dequan Wang, Zhiqiang Shen, Jie Shao, Wei Zhang, Xi-
angyang Xue, và Zheng Zhang. Multiple granularity de-
scriptors for fine-grained categorization. In Proceedings
of the IEEE International Conference on Computer Vision,
pages 2399–2406, 2015. 7
[86] Sarah Wiegreffe và Yuval Pinter. Attention is not not expla-
nation. arXiv preprint arXiv:1908.04626, 2019. 1
[87] Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi
Fan, Bo Xiong, Jitendra Malik, và Christoph Feichtenhofer.
Memvit: Memory-augmented multiscale vision transformer

--- TRANG 12 ---
for efficient long-term video recognition. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 13587–13597, 2022. 2
[88] Tianjun Xiao, Yichong Xu, Kuiyuan Yang, Jiaxing Zhang,
Yuxin Peng, và Zheng Zhang. The application of two-
level attention models in deep convolutional neural network
for fine-grained image classification. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 842–850, 2015. 7
[89] Mengqi Xue, Qihan Huang, Haofei Zhang, Lechao Cheng,
Jie Song, Minghui Wu, và Mingli Song. ProtoPFormer:
Concentrating on prototypical parts in vision transform-
ers for interpretable image recognition. arXiv preprint
arXiv:2208.10431, 2022. 2, 5, 7, 8
[90] Zhun Yang, Adam Ishay, và Joohyung Lee. Learning to
solve constraint satisfaction problems with recurrent trans-
former. In Proceedings of the Eleventh International Con-
ference on Learning Representations, 2023. K
[91] Chih-Kuan Yeh, Been Kim, Sercan Arik, Chun-Liang Li,
Tomas Pfister, và Pradeep Ravikumar. On completeness-
aware concept-based explanations in deep neural net-
works. Advances in Neural Information Processing Systems,
33:20554–20565, 2020. 1, 2
[92] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, và
Hod Lipson. Understanding neural networks through deep
visualization. arXiv preprint arXiv:1506.06579, 2015. 2
[93] Sergey Zagoruyko và Nikos Komodakis. Wide residual net-
works. arXiv preprint arXiv:1605.07146, 2016. 6
[94] Mateo Espinosa Zarlenga, Pietro Barbiero, Gabriele
Ciravegna, Giuseppe Marra, Francesco Giannini, Michelan-
gelo Diligenti, Frederic Precioso, Stefano Melacci, Adrian
Weller, Pietro Lio, et al. Concept embedding models. In
NeurIPS 2022-36th Conference on Neural Information Pro-
cessing Systems, 2022. 1, 2, 7
[95] Han Zhang, Tao Xu, Mohamed Elhoseiny, Xiaolei Huang,
Shaoting Zhang, Ahmed Elgammal, và Dimitris Metaxas.
Spda-cnn: Unifying semantic part detection and abstraction
for fine-grained recognition. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 1143–1152, 2016. 7
[96] Ning Zhang, Jeff Donahue, Ross Girshick, và Trevor Dar-
rell. Part-based R-CNNs for fine-grained category detection.
In Computer Vision–ECCV 2014: 13th European Confer-
ence, Zurich, Switzerland, September 6-12, 2014, Proceed-
ings, Part I 13, pages 834–849. Springer, 2014. 2, 7
[97] Heliang Zheng, Jianlong Fu, Tao Mei, và Jiebo Luo. Learn-
ing multi-attention convolutional neural network for fine-
grained image recognition. In Proceedings of the IEEE Inter-
national Conference on Computer Vision, pages 5209–5217,
2017. 2, 7
[98] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,
và Antonio Torralba. Learning deep features for discrimi-
native localization. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 2921–
2929, 2016. 2, 7
[99] Bolei Zhou, Yiyou Sun, David Bau, và Antonio Torralba.
Interpretable basis decomposition for visual explanation. In

Proceedings of the European Conference on Computer Vi-
sion (ECCV), pages 119–134, 2018. 2

--- TRANG 13 ---
Tài Liệu Bổ Sung
A. Khả Năng Tái Tạo
Tất cả mã nguồn, hình ảnh, mô hình, v.v., có sẵn tại
https://github.com/PavlicLab/WACV2024-
Hong-Concept_Centric_Transformers.git .

B. Chi Tiết Phương Pháp
Thuật Toán. Thuật toán 1 cho thấy các chi tiết của mô-đun
Concept-Slot-Attention (CSA) trong Transformer Tập Trung
Khái Niệm (CCT) của chúng tôi dưới dạng mã giả. Thuật
toán 1 được mô tả dựa trên SA [57], nhưng chúng tôi đơn
giản hóa nó bằng cách loại bỏ các lớp LayerNorm và MLP
cuối cùng. Do đặc tính mô-đun trong khung của chúng tôi,
chúng tôi tận dụng ba phương pháp dựa trên slot, bao gồm
SA [57], I-SA [10], và BO-QSA [41], có thể hoán đổi cho
nhau. Chúng tôi triển khai các phương pháp trên dựa trên
các kho lưu trữ/công trình chính thức của họ.

Nhúng Vị Trí cho Các Slot Khái Niệm. Vì tập hợp các
slot khái niệm kết quả không có thứ tự [57], rất khó cho
mô-đun sau này để: (i) nhận ra slot khái niệm nào đang
đại diện cho khái niệm cấp cao nào và (ii) xác định slot
khái niệm nào mà mỗi khái niệm cấp cao thuộc về. Do đó,
chúng tôi thêm một mã hóa vị trí pc vào mỗi slot khái niệm
để tránh những thách thức này; cụ thể, ŝc=sc+pc. Sau đó,
biểu diễn nhúng khái niệm Ŝconcept∈RC×d với nhúng vị
trí P được truyền như biểu diễn khái niệm cuối cùng cho
mô-đun tiếp theo.

Thuật toán 1 Mô-đun Concept-Slot-Attention (CSA). Mô-
đun nhận tập hợp các đặc trưng đầu vào E∈RL×D; số
lượng khái niệm C; và chiều của các khái niệm d. Các
tham số mô hình bao gồm: phép chiếu tuyến tính qCSA,
kCSA, vCSA với chiều đầu ra d; một mạng GRU; trung bình
và hiệp phương sai đường chéo của phân phối Gaussian
μ, σ∈Rd.

Sconcept=Tensor (C, d)▷concept −slots ∈RC×d
Sconcept∼ N(μ, σ)
E=LayerNorm (E)
for t= 0, . . . , T do
    Sconcept=LayerNorm (Sconcept)
    ACSA= softmax(1/√d qCSA(Sconcept)·
    kCSA(E)⊤,axis =′concept −slots′)
    ACSA=ACSA/ACSA.sum(axis =′inputs′)
    U=ACSA·vCSA(E)
    Sconcept=GRU(state =Sconcept,inputs =U)
end for
return Sconcept

Số Lần Lặp trong Mô-đun Concept Slot Attention. Trong
việc sử dụng ban đầu của các phương pháp dựa trên slot,
việc tinh chỉnh có thể được lặp lại nhiều lần tùy thuộc vào
các tác vụ. Để thiết lập số lần lặp T (Thuật toán 1), chúng
tôi đặt số lần lặp T khác nhau cho ba phương pháp dựa
trên slot mà chúng tôi sử dụng. Đối với SA vanilla, chúng
tôi đặt T thành 1 vì việc khởi tạo cho các slot trong SA có
một số hạn chế được tiết lộ bởi [10], và chúng tôi thấy rằng
một lần lặp duy nhất để cập nhật các slot khái niệm có lợi
trong đó các slot khái niệm đã học sở hữu nhiều thông tin
được thực hiện thông qua việc huấn luyện với lan truyền
ngược. Nói cách khác, sơ đồ phát sóng có thể diễn giải đề
xuất của chúng tôi (4.2 trong văn bản chính) đóng góp vào
hiệu suất tốt hơn cho SA so với các lần lặp tinh chỉnh nội
bộ được sử dụng trong các phương pháp dựa trên slot thông
thường. Hơn nữa, chúng tôi đặt T thành 3 cho BO-QSA và
I-SA bằng cách đơn giản tuân theo các giá trị tốt nhất của
lần lặp trong các công trình của họ.

So Sánh với Concept Transformers [65]. Như được chỉ ra
trong [65, Hình 1], Concept Transformers (CT) là việc sử
dụng hạn chế trong công thức của chúng tôi, tận dụng các
vector có thể học được cho việc học khái niệm thay vì sử
dụng "một không gian làm việc được chia sẻ". Chúng tôi
nhấn mạnh rằng biểu diễn nhúng khái niệm trong CCT của
chúng tôi tinh vi và có thể tổng quát hóa hơn so với biểu
diễn trong CT. Mỗi nhúng khái niệm trong CT được biểu
diễn như một vector có thể học đơn giản được chia sẻ với
tất cả các lô đầu vào để học. Do đó, có thể khó cho vector
nắm bắt được các đặc trưng hình ảnh nào có thể đóng góp
vào mỗi khái niệm nhiều hơn so với các đặc trưng khác.
Ví dụ, như được chỉ ra trong Hình 2 trong văn bản chính,
cho ba hình ảnh thuộc cùng lớp "Winter Wren", các đặc
trưng hình ảnh với cùng vị trí không gian từ mỗi hình ảnh
có thể có tầm quan trọng khác nhau để cung cấp thông tin
để học cùng một khái niệm toàn cục, như "màu sắc cơ thể
chính của Winter Wren là gì?" hoặc "hình dạng cơ thể của
nó là gì?". Ngược lại, mô-đun đề xuất của chúng tôi tự nhiên
tổng hợp mức độ mỗi đặc trưng hình ảnh đóng góp vào
mỗi khái niệm bằng cách sử dụng attention ACSA (Xem
trong Thuật toán 1) và cung cấp các biểu diễn nhúng khái
niệm cụ thể theo lô có thông tin có ý nghĩa ngữ nghĩa hơn.
Trong tất cả các thí nghiệm, chúng tôi chứng minh rằng
CCT của chúng tôi luôn vượt trội so với CT.

C. Kết Quả Thí Nghiệm và Chi Tiết Thêm
Trong phần này, chúng tôi giải thích các kết quả thí nghiệm
và chi tiết thêm. Tất cả các thí nghiệm được tiến hành với
ba hạt giống ngẫu nhiên khác nhau và khoảng tin cậy 95%.

C.1. Thống Kê Bộ Dữ Liệu
Bảng A-1 mô tả thống kê của tất cả các bộ dữ liệu điểm
chuẩn trong các thí nghiệm của chúng tôi. Vì tất cả các bộ
dữ liệu không có phần xác thực, chúng tôi chọn thủ công
phần của bộ dữ liệu xác thực để khám phá các siêu tham
số tốt nhất cho các mô hình.

--- TRANG 14 ---
Đối với bộ dữ liệu CIFAR-100 Super-class, nó phục vụ
như một ví dụ đáng chú ý cho việc phân loại hình ảnh tinh
tế. Bắt nguồn từ bộ dữ liệu CIFAR-100 gốc, nó chứa 100
lớp hình ảnh tinh tế được tổng hợp thành 20 siêu lớp riêng
biệt để phân loại rộng hơn. Ví dụ, siêu lớp "vehicles 1" bao
gồm các lớp tinh tế cụ thể như "bicycle," "bus," "motorcycle,"
"pickup truck," và "train." Hệ thống phân cấp có cấu trúc
này làm cho bộ dữ liệu trở thành một điểm chuẩn được sử
dụng rộng rãi trong việc đánh giá các mô hình học sâu, đặc
biệt là những mô hình kết hợp các ràng buộc logic.

Đối với CUB-200-2011, chúng tôi giải thích các bước tiền
xử lý theo [65]. Ban đầu, bộ dữ liệu có 312 thuộc tính nhị
phân, nhưng chúng tôi lọc để chỉ giữ lại những thuộc tính
xuất hiện trong ít nhất 45% tất cả các mẫu trong một lớp
nhất định và xuất hiện trong ít nhất 8 lớp. Do đó, chúng
tôi có tổng cộng 108 thuộc tính, và dựa trên điều này, chúng
tôi nhóm chúng thành hai loại khái niệm: khái niệm không
gian và toàn cục. Cuối cùng chúng tôi có 13 khái niệm toàn
cục và 95 khái niệm không gian bằng cách xem xét mỗi
thuộc tính. Ví dụ, hasshape::perching-like và
hasprimary color::black là các khái niệm toàn cục,
và haseyecolor::black và hasforehead color::
yellow là các khái niệm không gian.

Đối với bộ dữ liệu ImageNet, chúng tôi áp dụng phương
pháp được trình bày trong [84] để xác thực khả năng của
CCT trong việc học các khái niệm tiềm ẩn mà không cần
giải thích khái niệm rõ ràng. [84] sử dụng các bản đồ attention
để xác định một tập hợp các khái niệm được định nghĩa
trước trong bộ dữ liệu, tập trung vào 200 lớp đầu tiên của
ImageNet cho việc đánh giá của họ. Theo phương pháp
này, chúng tôi cũng sử dụng 200 lớp đầu tiên trong ImageNet
để huấn luyện và tiến hành đánh giá để xác định độ chính
xác kiểm tra trên bộ dữ liệu ImageNet. Thí nghiệm này
phục vụ để điểm chuẩn khả năng của mô hình CCT trong
việc học các khái niệm tiềm ẩn mà không có giải thích rõ
ràng, thể hiện thêm hiệu quả của nó trong một thiết lập
không giám sát.

C.2. Thông Số Kỹ Thuật Phần Cứng của Máy Chủ
Thông số kỹ thuật phần cứng của máy chủ mà chúng tôi
sử dụng để thí nghiệm như sau:
• CPU: Intel® Core™ i7-6950X CPU @ 3.00GHz (lên
đến 3.50 GHz)
• RAM: 128 GB (DDR4 2400MHz)
• GPU: NVIDIA GeForce Titan Xp GP102 (kiến trúc Pascal,
3840 CUDA Cores @ 1.6 GHz, độ rộng bus 384-bit, 12
GB bộ nhớ GDDR G5X)

C.3. Kiến Trúc Mô Hình
Chúng tôi tận dụng ba loại xương sống, bao gồm Vision
Transformers (ViT) [22], Swin Transformers (Swin) [55],
và ConvNeXt [56]. Chúng tôi sử dụng thư viện Python timm
được hỗ trợ bởi Hugging Face™.

Các Biến Thể của ViT. Trong các thí nghiệm của chúng
tôi, chúng tôi sử dụng ba biến thể của Vision Transformer
(ViT), ViT-Large, ViT-Small, và ViT-Tiny (vitlarge
patch16 224, vitsmall patch16 224, và
vittiny patch16 224, tương ứng trong timm). Những
biến thể này được định nghĩa bởi số lượng khối mã hóa,
số lượng đầu attention trên mỗi khối, và chiều của lớp ẩn.
ViT-Large có 24 khối mã hóa với 16 đầu, và chiều của lớp
ẩn là 1024. Ngoài ra, ViT-Small có 12 khối mã hóa với 6
đầu, và chiều của lớp ẩn là 384. Cuối cùng, ViT-Tiny có 12
khối mã hóa với 3 đầu, và chiều của lớp ẩn là 192, nhẹ
hơn nhiều. So sánh giữa các biến thể của ViT được hiển
thị trong Bảng A-2.

Kiến Trúc của Swin-Large. Trong thí nghiệm của chúng
tôi, chúng tôi sử dụng Swin Transformer (Swin)-Large
(swin large patch4 window7 224.msin22k in
timm). Xem Bảng A-3 cho kiến trúc tổng thể của Swin-Large.

Kiến Trúc của ConvNeXt-Large. Trong thí nghiệm của
chúng tôi, chúng tôi tận dụng ConvNeXt-Large (convnext
large.fbin22k ftin1k in timm). Xem Bảng A-4 cho
kiến trúc tổng thể của ConvNeXt-Large.

C.4. Thí Nghiệm CIFAR100 Super-class
Sự Khác Biệt Giữa CCT và Các Phương Pháp Học Sâu
Khác với Ràng Buộc Logic. Ràng buộc logic bởi [23] để
giải quyết tác vụ này đã được giới thiệu, tức là, một khi
mô hình phân loại một hình ảnh vào một lớp, các xác suất
dự đoán của siêu lớp đó phải đầu tiên đến khối lượng toàn
bộ. Tham khảo điều này, một số phương pháp học sâu tận
dụng nó [34, 51]. Ví dụ, năm lớp, baby, boy, girl, man, và
woman, thuộc về siêu lớp people. Do đó, Prpeople(x) = 1
nếu x được phân loại là baby. Vậy, các ràng buộc logic có
thể được công thức hóa như ∧s∈superclass (Prs(x) = 0∨Prs(x)
= 1), nơi xác suất của siêu lớp là tổng của các xác suất lớp
tương ứng, ví dụ, Prpeople(x) =Prbaby(x) +Prboy(x) +Prgirl(x)
+Prman(x) + Prwoman(x).

Tuy nhiên, vì CCT của chúng tôi (và CT) tuân theo Mệnh
đề 1 trong [65], xác suất chọn siêu lớp ưa thích là sc= arg
maxs(βc)s của lớp c (Tham khảo Eq.(2) trong văn bản chính).
Điều này có nghĩa là siêu lớp được xác định bởi điểm số
attention lớn nhất của lớp trong tất cả các lớp, khác với
định nghĩa của ràng buộc logic ở trên.

Kết Quả Bổ Sung. Hình A-1 thể hiện một số ví dụ về dự
đoán chính xác bởi CT và CCT của chúng tôi, nêu bật

--- TRANG 15 ---
Bảng A-1. Thống Kê Bộ Dữ Liệu Điểm Chuẩn. †chỉ ra kích thước được thay đổi tỷ lệ của đầu vào cho xương sống ViT, khác với kích thước gốc của các bộ dữ liệu. Đối với thiết lập thí nghiệm ImageNet, chúng tôi theo [84].

| Bộ Dữ Liệu | CIFAR100 Super-class | CUB-200-2011 | ImageNet |
|-------------|----------------------|---------------|----------|
| Kích thước đầu vào | 3×28×28 | 3×224×224† | 3×224×224† |
| # Lớp | 20 (siêu lớp) | 200 (loài chim) | 200 (đối tượng) |
| # Khái niệm | 100 (lớp) | 13 (toàn cục), 95 (không gian) | 50 (không gian tiềm ẩn) |
| # Mẫu huấn luyện | 55,000 | 5,994 | 255,224 |
| # Mẫu xác thực | 5,000 | 1,000 | 10,000 |
| # Mẫu kiểm tra | 10,000 | 4,794 | 20,000 |

| Mô Hình | Lớp | Dim Ẩn | Đầu | # Tham Số |
|---------|-----|---------|-----|-----------|
| ViT-Tiny | 12 | 192 | 3 | 5M |
| ViT-Small | 12 | 384 | 6 | 22M |
| ViT-Large | 24 | 1024 | 16 | 304 M |

Bảng A-2. So sánh giữa các biến thể của ViT [22]

| Mô Hình | Lớp (mỗi giai đoạn) | Dim Ẩn | Đầu Attention | # Tham Số |
|---------|---------------------|---------|---------------|-----------|
| Swin-Large | (2, 2, 18, 2) | 192 | 6 | 197M |

Bảng A-3. Kiến trúc tổng thể của Swin Transformer [55]

| Mô Hình | Lớp Conv. (mỗi khối) | Dim Ẩn | Đầu Attention | # Tham Số |
|---------|---------------------|---------|---------------|-----------|
| ConvNeXt-Large | 2 | 512 | 16 | 50M |

Bảng A-4. Kiến trúc tổng thể của ConvNeXt [56]

rằng so với CT, CCT của chúng tôi đạt được điểm số attention
giải thích thưa thớt hơn, điều này chỉ ra mức độ tin cậy và
mạnh mẽ hơn cho việc ra quyết định.

Thiết Lập Siêu Tham Số. Đối với các kết quả thí nghiệm,
bao gồm Bảng 1 và Hình 3 trong văn bản chính, tham khảo
[23], chúng tôi đầu tiên tìm các thiết lập thí nghiệm tốt nhất
của cả ViT-Tiny và CT vì chúng chưa được đánh giá trên
CIFAR100 Super-class trước đây. Một khi thiết lập siêu
tham số cho xương sống ViT-Tiny được tìm thấy để đạt
được hiệu suất tương tự như Wide ResNet (xương sống cho
nhóm đường cơ sở thứ hai), chúng tôi áp dụng cùng thiết
lập siêu tham số cho cả CT và CCT của chúng tôi. Bảng
A-5a trình bày tất cả các siêu tham số được chia sẻ cho
ViT-Tiny, CT, và tất cả các cấu hình của CCT. Vui lòng
tham khảo Bảng A-6 cho thiết lập siêu tham số được sử
dụng trong các thí nghiệm đường cơ sở của chúng tôi.

C.5. Thí Nghiệm CUB-200-2011
Kết Quả Bổ Sung với So Sánh với CT. Với các giải thích
khái niệm, chúng tôi giới thiệu các kết quả thí nghiệm bổ
sung để so sánh dự đoán giữa CT và CCT của chúng tôi.
Hình A-2 mô tả một trường hợp nơi dự đoán của CT hoàn
toàn khác nhau mặc dù tất cả hình ảnh có cùng lớp. Các
lớp dự đoán từ CT trong Hình A-2 là Prothonotary
Warbler, Pine Warbler, và Magnolia Warbler,
tương ứng. Sự khác biệt duy nhất giữa hai dự đoán đầu
tiên mà CT đưa ra là liệu khái niệm không gian hasunder
tail color::black có tồn tại hay không, chỉ ra độ nhạy
cảm hiệu suất của CT. Ngoài ra, dự đoán thứ ba từ CT chứa
các khái niệm không gian mới không liên quan đến hai ví
dụ đầu tiên, điều này cho thấy sự không chắc chắn phân
tích về lý do tại sao CT đưa ra quyết định như vậy. Ngược
lại, các dự đoán được thực hiện bởi CCT của chúng tôi đều
chính xác, với các giải thích toàn cục nhất quán. Điều này
chỉ ra rằng mặc dù việc kết hợp các khái niệm không gian
cung cấp thông tin bổ sung, việc xác định các khái niệm
toàn cục mạnh mẽ quan trọng hơn đối với tác vụ phân loại.

Hình A-3 cho thấy một số ví dụ nơi CCT của chúng tôi
vượt trội so với CT để phân loại Olive sided Flycatcher.
Tất cả dự đoán của CT đều là Western Wood Pewee, và
điều này được gây ra bởi các giải thích không gian của
chúng, bao gồm haseyecolor::black và haslegcolor::
black. Chúng tôi phát hiện rằng hai thuộc tính không gian
này quan trọng đối với Western Wood Pewee, và do đó
gây hiểu lầm cho dự đoán của mô hình. Ngược lại, các
giải thích đạt được bởi CCT mạnh mẽ và nhất quán hơn.

Cuối cùng, Hình A-4 và A-5 chứng minh các trường hợp
thất bại nơi cả CCT và CT đều dự đoán không chính xác.
Trong cả hai mô hình, nhiễu gây ra bởi các giải thích không
gian phổ biến trong một số lớp có xu hướng làm suy giảm
hiệu suất của kết quả dự đoán. Tuy nhiên, so với CT, CCT
đề xuất của chúng tôi nắm bắt các giải thích toàn cục phong
phú hơn, do đó giúp đạt được hiệu suất phân loại tốt hơn
nói chung.

Thiết Lập Siêu Tham Số. Chúng tôi tuân theo việc triển
khai chính thức của CT và tạo ra các kết quả thí nghiệm,
bao gồm Bảng 2, Hình 4, trong văn bản chính, Hình A-2,
và Hình A-3 trong Phụ lục. Sự khác biệt duy nhất trong
thiết lập siêu tham số giữa CCT của chúng tôi và CT là tốc
độ học cho bộ tối ưu AdamW, 5e-5 cho CT, 1e-5 cho các
cấu hình với xương sống ViT-Large của CCT chúng tôi,
2e-5 cho CCT của chúng tôi với SwinT-Large, và 5e-5 cho
CCT của chúng tôi với ConvNeXt-Large. Bảng A-5c cho
thấy các siêu tham số được chia sẻ cho cả hai phương pháp.
Đối với những đường cơ sở không được liệt kê trong Bảng
A-5c, chúng tôi tham khảo trực tiếp dữ liệu được trình bày
trong các ấn phẩm tương ứng của họ.

Trực Quan Hóa Khái Niệm Để có cái nhìn sâu sắc về các
khái niệm đã học trong bộ dữ liệu CUB-200-2011, chúng
tôi điều chỉnh mô hình CCT thành 50 lớp riêng biệt và 20
khái niệm tiềm ẩn theo [84]. Một trực quan hóa toàn diện
của những khái niệm tiềm ẩn được tinh chỉnh này có thể
xem trong Hình A-6, A-7 và A-8. Xin lưu ý rằng chúng tôi
trình bày những kết quả trực quan hóa này mà không cung
cấp giải thích khái niệm.

Trong bộ dữ liệu CUB-200-2011, khái niệm đầu tiên của
mô hình chúng tôi

--- TRANG 16 ---
Hình A-1. Ví dụ về dự đoán chính xác bởi CT và CCT trên CIFAR100 super-class. 100 lớp được đánh chỉ số từ 1 (trên cùng bên trái trong lưới 10x10) đến 100 (dưới cùng bên phải trong lưới 10x10).

nổi bật các đặc trưng chính của chim như vùng đầu và mỏ.
Khái niệm 2 tập trung vào đường viền của hải âu bằng cách
nhấn mạnh nền. Khái niệm 3 xem xét nhiều đặc trưng như
mỏ, mắt, và đuôi, trong khi Khái niệm 4 tách biệt chim
khỏi nền phức tạp. Tương tự, Khái niệm 5 phác thảo toàn
bộ cơ thể chim. Khái niệm 6 chuyên về việc tô sáng vùng
cơ thể của những chim có thân màu vàng, và Khái niệm 7
tập trung vào mỏ và thân trên. Khái niệm 8 không chỉ tô
sáng đường viền mà còn tập trung vào vùng mắt của chim.
Khái niệm 9 và 10 chia sẻ điểm tương đồng với Khái niệm
8, nhấn mạnh vùng mắt. Khái niệm 11 nổi bật bằng cách
tập trung vào vùng đầu của vẹt đỏ, đây cũng là một đặc
trưng thu hút sự chú ý của con người. Khái niệm 12 và 13
được dành riêng cho vùng chân và cơ thể dưới của chim,
tương ứng. Khái niệm 14 vạch ra toàn bộ cơ thể của hải
âu, trong khi Khái niệm 15 nắm bắt đường viền của chim
trong tư thế bay. Khái niệm 16, mặt khác, tập trung vào
đường viền của chim trong tư thế ngồi. Khái niệm 17 tập
trung vào vùng mắt và giữa cơ thể, trong khi Khái niệm
18 một phần nắm bắt các đặc trưng chính của chim cổ dài.
Khái niệm 19 nắm bắt các cấu trúc đầu và lông độc đáo.
Cuối cùng, Khái niệm 20 hoàn hảo phác thảo đường viền
của chim.

Những kết quả này chứng minh khả năng của mô hình tập
trung vào một loạt các đặc trưng, từ mỏ và mắt đến đuôi
và chân, củng cố kỹ năng phân loại của nó. Điều này đặc
biệt đáng chú ý trong một môi trường nơi các giải thích
khái niệm rõ ràng không có sẵn, nổi bật một chiều khác
nơi mô hình của chúng tôi vượt trội so với những mô hình
khác.

Truy Xuất và Phân Cụm Hình Ảnh. Ngoài khả năng phân
loại mạnh mẽ, mô hình CCT của chúng tôi xuất sắc trong
lĩnh vực truy xuất hình ảnh. Đáng chú ý, mô hình đạt được
điều này mà không yêu cầu giải thích rõ ràng cho 20 khái
niệm tiềm ẩn mà nó xác định trong bộ dữ liệu CUB-200-
2011. Việc phân cụm trực quan này của hình ảnh dựa trên
các đặc trưng nội tại—từ đường viền của hải âu đến các
đặc trưng độc đáo của đầu vẹt đỏ—chứng minh một chiều
khác mà mô hình của chúng tôi vượt qua những mô hình
khác trong lĩnh vực. Về cơ bản, nó phân cụm các hình ảnh
liên quan về mặt ngữ nghĩa dựa trên những khái niệm tiềm
ẩn này, cung cấp kết quả nhất quán ngay cả trong môi
trường nơi chúng ta không có quyền truy cập vào giải thích
khái niệm. Ví dụ, hình ảnh có vẹt được phân cụm lại với
nhau, được thúc đẩy bởi sự tập trung của Khái niệm 11 vào
vùng đầu của vẹt đỏ. Tương tự, hình ảnh của hải âu được
nhóm lại với nhau, được hướng dẫn bởi sự nhấn mạnh của
Khái niệm 2 vào đường viền hải âu.

Nghiên Cứu Loại Bỏ. Trong Hình A-9, chúng ta có thể
suy ra rằng hiệu suất của CCT nhạy cảm với siêu tham số
explanation lambda λexpl (4.3 trong văn bản chính). Dường
như có một phạm vi tối ưu cho explanation lambda, đâu đó
giữa 0 và 10, trong đó mô hình hoạt động tốt nhất. Các giá
trị của λexpl cao hơn 10 dẫn đến hiệu suất tệ hơn dần dần,
với sự suy giảm nghiêm trọng được quan sát ở các giá trị
lambda cao nhất được kiểm tra.

Trong Hình A-10, chúng ta có thể suy ra rằng tốc độ học
thấp hơn 1.00e-4 cung cấp hiệu suất mô hình tốt nhất, đặc
biệt khi explanation lambda λexpl được đặt thành 0.0. Khi
tốc độ học tăng, độ chính xác của mô hình giảm đáng kể.
Độ chính xác cực thấp ở tốc độ học cao hơn (1.00e-01 và
1.00e+00) gợi ý rằng mô hình không thể học hiệu quả, có
thể vượt qua các giá trị tối ưu trong quá trình huấn luyện
do các cập nhật quá lớn đối với các tham số của mô hình.

C.6. Thí Nghiệm ImageNet
Thiết Kế Thí Nghiệm. Để xác nhận thêm khả năng của
CCT trong việc rút ra các khái niệm tiềm ẩn một cách tự
động, chúng tôi tiến hành một thí nghiệm trên bộ dữ liệu
ImageNet, tuân thủ phương pháp được mô tả trong [84].
Chúng tôi chọn ViT-S như kiến trúc xương sống của mình,
tránh các mô hình có hơn 45M tham số, như ResNet-101,
Swin-S, và ConvNeXt-S.

Trực Quan Hóa Khái Niệm. Để có cái nhìn sâu sắc về
các khái niệm đã học, chúng tôi điều chỉnh mô hình CCT
thành 20 lớp và 10 khái niệm tiềm ẩn, như trong [84]. Hình
A-11 cung cấp một trực quan hóa chi tiết của những khái
niệm tiềm ẩn này. Xin lưu ý rằng chúng tôi trình bày những
kết quả trực quan hóa này mà không cung cấp giải thích
khái niệm.

Khái niệm đầu tiên mà mô hình của chúng tôi xác định tập
trung vào sứa thạch, cụ thể là tách biệt đường viền của
những sinh vật này khỏi môi trường tự nhiên của chúng.
Khái niệm thứ hai hướng sự chú ý về phía gà, đặc biệt nhấn
mạnh quào đỏ riêng biệt trên đầu chúng. Tiếp theo, khái
niệm thứ ba xuất sắc trong việc phân tách các thành phần
khác nhau của loài cá, hiệu quả phân định giữa các phần
khác nhau như vây, vảy, và đuôi. Chuyển sang sinh vật
biển, khái niệm thứ tư nhằm tập trung vào các thuộc tính
mặt của cá heo và cá mập, với sự tập trung đặc biệt vào

--- TRANG 17 ---
Hình A-2. So sánh dự đoán giữa CT và CCT của chúng tôi. (Trên) Dự đoán của CT không chính xác. (Dưới) Tất cả dự đoán đều chính xác bởi CCT của chúng tôi.

Hình A-3. So sánh dự đoán giữa CT và CCT. (Hàng Trên) Dự đoán của CT không chính xác. Các giải thích được tô sáng màu tím là các thuộc tính chính trong Western Wood Pewee. (Hàng Dưới) Tất cả dự đoán đều chính xác bởi CCT của chúng tôi.

Hình A-4. Ví dụ về dự đoán thất bại của cả CT (Hàng Trên) và CCT (Hàng Dưới).

miệng của chúng. Khái niệm thứ năm đi xa hơn bằng cách
cụ thể tô sáng các vùng xung quanh khoang miệng của cá
mập, tách biệt chúng khỏi các phần khác của sinh vật.
Trong một tĩnh mạch thủy sản tương tự, khái niệm thứ sáu
phác thảo các hình dạng và đường viền độc đáo của cá
vàng, nắm bắt hình thức của chúng một cách hiệu quả.
Khái niệm thứ bảy khác biệt bằng cách chỉ tập trung vào
chân của chim, dù chúng đang đậu hay bay. Điều này được
bổ sung bởi khái niệm thứ tám, có cách tiếp cận rộng hơn
đối với chim bằng cách tập trung vào vùng bụng của chúng,
nắm bắt các chi tiết như lông và bụng dưới. Khái niệm thứ
chín chuyên về đà điểu, đặc biệt tập trung vào các đặc
trưng riêng biệt tạo nên vùng đầu của chúng. Cuối cùng,
khái niệm thứ mười tập trung vào gà mái. Nó đặc biệt tập
trung vào việc tách biệt đường viền của chúng khỏi nhiều
nền khác nhau, do đó cho phép hiểu rõ hơn về hình thức
và cấu trúc của gà mái.

Những trực quan hóa này chứng minh khả năng vô song
của CCT trong việc tập trung vào các khía cạnh có ý nghĩa
ngữ nghĩa của hình ảnh.

--- TRANG 18 ---
Hình A-5. Ví dụ về dự đoán thất bại của cả CT (Hàng Trên) và CCT (Hàng Dưới).

Hình A-6. Từ trái sang phải, hình ảnh hiển thị Khái niệm 1 đến 20 như được xác định trong bộ dữ liệu CUB-200-2011 theo [84]. Mỗi cặp hình ảnh bao gồm phiên bản được che (với mặt nạ kích hoạt attention) bên trái và hình ảnh gốc bên phải. Để có biểu diễn trực quan tốt hơn, xem A-7 và A-8.

Hình A-7. 10 khái niệm đầu tiên (1-10) trong CUB-200-2011

Truy Xuất và Phân Cụm Hình Ảnh. Ngoài việc xác định
các khái niệm tiềm ẩn phức tạp, mô hình CCT của chúng
tôi chứng minh khả năng đặc biệt trong các tác vụ truy xuất
hình ảnh. Như được minh họa trong Hình A-11, mô hình
thành thạo trong việc phân cụm các hình ảnh chia sẻ những
điểm tương đồng ngữ nghĩa, do đó củng cố tiện ích của nó
trong việc tạo ra kết quả nhất quán về mặt ngữ nghĩa. Đáng
chú ý, CCT của chúng tôi đạt được mức độ phân cụm này
mà không cần bất kỳ giải thích khái niệm rõ ràng nào. Điều
này khiến nó khác biệt so với các mô hình khác trong lĩnh
vực, vì nó có thể trực quan nhóm hình ảnh dựa trên các
đặc trưng nội tại được nhận dạng thông qua các khái niệm
tiềm ẩn, từ đường viền của sứa thạch đến các đặc trưng
độc đáo của đầu đà điểu. Khả năng phân cụm các hình ảnh
liên quan về mặt ngữ nghĩa mà không có hướng dẫn khái
niệm chi tiết này nhấn mạnh một khía cạnh khác nơi mô
hình của chúng tôi xuất sắc so với những mô hình khác
trong miền.

Thiết Lập Siêu Tham Số. Chúng tôi tuân theo việc triển
khai chính thức của CT và tạo ra các kết quả thí nghiệm,
bao gồm Bảng 3 trong văn bản chính.

Sự khác biệt duy nhất trong thiết lập siêu tham số giữa
CCT của chúng tôi, CT, và ViT-S vanilla là tốc độ học cho
bộ tối ưu AdamW. Đối với ViT-S vanilla, tốc độ học là
0.0001. Đối với CT, tốc độ học là 5e-5, theo những gì trong
các thí nghiệm CUB-200-2011 của họ. Đối với CCT của
chúng tôi, tốc độ học là 0.0001. Bảng A-5d cho thấy các
siêu tham số được chia sẻ cho ViT-S vanilla, CT, và tất cả
các cấu hình của CCT chúng tôi. Vui lòng tham khảo Bảng
A-6 cho thiết lập siêu tham số được sử dụng trong các thí
nghiệm đường cơ sở của chúng tôi.

Hạn Chế của Các Phương Pháp Dựa Trên Prototype.
Quan sát từ việc kiểm tra các mô hình dựa trên prototype
như ProtoPFormer, ProtoPool, ProtoPNet, và Deform-
ProtoPNet trên ImageNet mang lại một số kết quả thú vị.
Trong khi chúng tôi ban đầu báo cáo kết quả chỉ cho 200
nhãn đầu tiên của ImageNet trong Bảng 3, theo phương
pháp từ [84], cần thiết cho chúng tôi phải giảm số lượng
prototype mỗi trường hợp—một siêu tham số quan trọng
cho mô hình dựa trên prototype—do các ràng buộc tính
toán, trong khi mô hình dựa trên transformer của chúng
tôi, CCT, quản lý tác vụ mà không gặp vấn đề. Chúng tôi
đưa ra giả thuyết rằng hạn chế này phát sinh từ thiết kế
nội tại của các mô hình dựa trên prototype, sử dụng 'prototype'
để cung cấp khả năng diễn giải cho các quá trình ra quyết
định của các mô hình học máy phức tạp trong các lĩnh vực
như phân loại hình ảnh, phát hiện đối tượng, hoặc phân
đoạn. Đây có thể là một vấn đề tầm thường khi kích thước
của bộ dữ liệu nhỏ, tuy nhiên, khi độ phức tạp của bộ dữ
liệu tăng,

--- TRANG 19 ---
Hình A-8. 10 khái niệm thứ hai (11-20) trong CUB-200-2011

Hình A-9. So sánh hiệu suất của CCT trên CUB-200-2011 với
giá trị khác nhau của explanation lambda λexpl.

Hình A-10. So sánh hiệu suất của CCT trên CUB-200-2011 với
giá trị khác nhau của tốc độ học dưới các giá trị explanation
lambda khác nhau.

như với ImageNet, chứa hàng triệu hình ảnh trên hàng nghìn
danh mục, số lượng prototype cần thiết để biểu diễn hiệu
quả tất cả các lớp tăng lên. Sự tăng trưởng này đòi hỏi
nhiều bộ nhớ và tài nguyên tính toán hơn đáng kể, đặc biệt
là RAM GPU, để lưu trữ và xử lý những prototype này
trong cả quá trình huấn luyện và suy luận.

Kết Quả Thí Nghiệm Bổ Sung. Hình A-12 cho thấy các
kết quả thí nghiệm trên ImageNet với số lượng nhãn khác
nhau để huấn luyện ViT-S vanilla và CCT. Chúng tôi kiểm
tra cả hai mô hình với cùng thiết lập siêu tham số trong
văn bản chính, và chọn cấu hình tốt nhất của CCT, sử dụng
BO-QSA. Mặc dù số lượng khái niệm tiềm ẩn trong CCT
là 50 theo [84], CCT chủ yếu vượt trội so với xương sống
được huấn luyện trước. Khi số lượng nhãn rất lớn (≥800),
hiệu suất của CCT tương tự hoặc tệ hơn so với xương sống.
Điều này điển hình vì số lượng 50 khái niệm tiềm ẩn mà
chúng tôi đặt không đủ để xử lý tất cả các nhãn.

Hình A-11. Tất cả 10 khái niệm (1-10) trong bộ dữ liệu ImageNet theo [84]

Hình A-12. So sánh hiệu suất trên ImageNet với số lượng nhãn
khác nhau.

Do đó, chúng tôi thực hiện một thí nghiệm bổ sung trên
ImageNet sử dụng tổng cộng 1000 lớp và số lượng khái
niệm tiềm ẩn khác nhau cho CCT. Hình A-13 chứng minh
so sánh hiệu suất của CCT tùy thuộc vào số lượng khái
niệm tiềm ẩn. Khi số lượng khái niệm được đặt thành 150,
CCT vượt trội so với ViT-S vanilla (đường màu xanh đứt
nét), chỉ ra sự đóng góp của việc đặt số lượng khái niệm
tiềm ẩn vào hiệu suất của CCT.

D. Hạn Chế
Trong phần này, chúng tôi giải thích các hạn chế của phương
pháp đề xuất của chúng tôi.

Thứ nhất, do các đặc tính kiến trúc của CCT trong Phần
4 trong văn bản chính, phương pháp đề xuất thực thi các
đóng góp cộng dồn từ các khái niệm do người dùng tùy
chỉnh vào các xác suất phân loại. Nói cách khác, chúng tôi
bỏ qua các mối quan hệ bậc cao tiềm ẩn giữa các khái niệm.

--- TRANG 20 ---
Hình A-13. So sánh hiệu suất của CCT trên ImageNet với số
lượng khái niệm tiềm ẩn khác nhau. Đường màu xanh đứt nét
chỉ ra độ chính xác kiểm tra của ViT-S vanilla.

| Tên | Giá Trị |
|-----|---------|
| Batch size | 64 |
| Epochs | 20 |
| Warmup Iters. | 10 |
| Learning rate | 5e-5 |
| Explanation loss λ | 1.0 |
| Weight decay | 1e-3 |
| Attention sparsity | 0.5 |

(a) ViT-Tiny, CT và tất cả cấu hình của CCT trên CIFAR100 Super-class

| Tên | Giá Trị |
|-----|---------|
| Epochs | 60 |
| Learning Rate | 1e-4 |
| Number of classes | 20 |
| Number of concepts | 10 |
| Quantity Bias | 0.1 |
| Distinctiveness Bias | 0.05 |
| Consistence Bias | 0.01 |
| Weak Supervision Bias | 0.1 |

(b) Cho botCL trên CIFAR100 Super-class

| Tên | Giá Trị |
|-----|---------|
| Batch size | 16 |
| Epochs | 50 |
| Warmup Iters. | 10 |
| Explanation loss λ | 1.0 |
| Weight decay | 1e-3 |
| Attention sparsity | 0.5 |

(c) Cho cả CT và tất cả cấu hình của CCT trên CUB-200-2011

| Tên | Giá Trị |
|-----|---------|
| Batch size | 256 |
| Epochs | 10 |
| Warmup Iters. | 10 |
| Explanation loss λ | 0. |
| Weight decay | 1e-3 |
| Attention sparsity | 0. |

(d) Cho cả CT và tất cả cấu hình của CCT trên ImageNet

Bảng A-5. Siêu tham số được chia sẻ trên các bộ dữ liệu khác nhau.

Ví dụ, trong các thí nghiệm CUB-200-2011 của chúng tôi,
chúng tôi giả định rằng các khái niệm toàn cục và không
gian có thể được biểu diễn và học song song trong khung
của chúng tôi và rằng không có mối tương quan giữa khái
niệm toàn cục và khái niệm không gian, điều này độc lập.
Tuy nhiên, điều này có hạn chế vì các mối quan hệ phức
tạp hơn, như thuộc tính phân cấp, có thể tồn tại giữa chúng.
Điều này có thể được giải quyết bằng cách giới thiệu các
thuộc tính kiến trúc được tinh chỉnh trong CCT của chúng
tôi. Ví dụ, Đơn Vị Tái Phát Hai Chiều có thể được

| Tên | CIFAR100 | ImageNet |
|-----|----------|----------|
| Batch size | 64 | 16 |
| Epochs | 10 | 10 |
| Freeze Epochs | 10 | 10 |
| Pre-train Epochs | 0 | 0 |
| Weight decay | 0.0 | 0.0 |
| LR (prototypes weights) | 5e-2 | 5e-2 |
| LR (backbone) | 5e-4 | 5e-4 |

(a) Cho PIP-Net trên CIFAR100 và ImageNet, LR có nghĩa là Learning Rate

| Tên | CIFAR100 | ImageNet |
|-----|----------|----------|
| Batch size | 64 | 64 |
| Epochs | 50 | 10 |
| Warmup Learning Rate | 1e-4 | 1e-4 |
| Feature Learning Rate | 4e-4 | 4e-4 |
| Prototype Learning Rate | 3e-3 | 3e-3 |
| Number of Prototype | 2000 | 2000 |

(b) Cho ProtoPFormer trên CIFAR100 và ImageNet

| Tên | CIFAR100 | ImageNet |
|-----|----------|----------|
| Batch size | 80 | 80 |
| Epochs | 30 | 20 |
| Learning Rate | 1e-3 | 1e-3 |
| Gumbel Time | 30 | 30 |
| Number of Classes | 20 | 200 |
| Number of Prototype | 202 | 202 |
| Prototype Depth | 256 | 256 |

(c) Cho ProtoPool trên CIFAR100 và ImageNet

| Tên | CIFAR100 | ImageNet |
|-----|----------|----------|
| Batch size | 80 | 80 |
| Epochs | 100 | 100 |
| Learning Rates | default | default |
| Number of Classes | 20 | 200 |
| Number of Prototype | 100 | 200 |

(d) Cho Deformable-ProtoPNet trên CIFAR100 và ImageNet

| Tên | CIFAR100 | ImageNet |
|-----|----------|----------|
| Batch size | 80 | 80 |
| Epochs | 100 | 100 |
| Learning Rates | default | default |
| Number of Classes | 20 | 200 |
| Number of Prototype | 1000 | 2000 |

(e) Cho ProtoPNet trên CIFAR100 và ImageNet

Bảng A-6. Siêu tham số được chia sẻ trên các bộ dữ liệu khác nhau.

giới thiệu để cho phép các khái niệm toàn cục và không
gian học ảnh hưởng khái niệm của nhau.

Thứ hai, mặc dù, trong các thí nghiệm của CUB-200-2011
và ImageNet mà không sử dụng giải thích, chúng tôi chứng
minh rằng CCT của chúng tôi có thể trực quan hóa các khái
niệm ngữ nghĩa đã học, chúng tôi thừa nhận rằng CCT của
chúng tôi có thể được tích hợp với các mất mát tinh vi hơn
để đạt được trực quan hóa tốt hơn của các khái niệm đã
học. Ví dụ, [90] tận dụng các mất mát, bao gồm tái tạo,
mất mát tương phản, và nhiều bộ điều hòa khác nhau, để
thực thi tính nhất quán cá nhân và sự khác biệt lẫn nhau
của các khái niệm. Trong nghiên cứu tương lai của chúng
tôi, chúng tôi có thể sử dụng những mất mát và bộ điều
hòa đó để cho phép mô hình của chúng tôi đạt được khả
năng trực quan hóa tốt hơn.

--- TRANG 21 ---

--- TRANG 22 ---

--- TRANG 23 ---
