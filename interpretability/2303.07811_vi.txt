# 2303.07811.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/interpretability/2303.07811.pdf
# Kích thước tệp: 2047573 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
ICICLE: Học Liên Tục Tăng Dần Lớp Có Thể Diễn Giải
Dawid Rymarczyk1,2,3,∗Joost van de Weijer4,5Bartosz Zieli ´nski1,3,6
Bartłomiej Twardowski4,5,6
1Khoa Toán học và Khoa học Máy tính, Đại học Jagiellonian
2Trường Tiến sĩ Khoa học Chính xác và Sự sống, Đại học Jagiellonian3Ardigen SA
4Đại học Tự trị Barcelona5Trung tâm Thị giác Máy tính6IDEAS NCBR
∗dawid.rymarczyk@doctoral.uj.edu.pl
Tóm tắt
Học liên tục cho phép học tăng dần các nhiệm vụ mới mà không quên những nhiệm vụ đã học trước đó, dẫn đến chuyển giao kiến thức tích cực có thể nâng cao hiệu suất trên cả nhiệm vụ mới và cũ. Tuy nhiên, học liên tục đặt ra những thách thức mới cho khả năng diễn giải, vì căn cứ đằng sau dự đoán của mô hình có thể thay đổi theo thời gian, dẫn đến sự trôi dạt khái niệm diễn giải. Chúng tôi giải quyết vấn đề này bằng cách đề xuất Học Tăng Dần Lớp Có Thể Diễn Giải (ICICLE), một phương pháp không cần mẫu điển hình áp dụng cách tiếp cận dựa trên phần nguyên mẫu. Nó bao gồm ba tính mới quan trọng: điều chuẩn khả năng diễn giải giúp chưng cất các khái niệm đã học trước đó trong khi bảo tồn lý luận tích cực thân thiện với người dùng; chiến lược khởi tạo nguyên mẫu dựa trên độ gần dành riêng cho thiết lập phân loại chi tiết; và bù trừ thiên lệch gần đây của nhiệm vụ dành cho các phần nguyên mẫu. Kết quả thí nghiệm của chúng tôi cho thấy ICICLE giảm sự trôi dạt khái niệm diễn giải và vượt trội so với các phương pháp học tăng dần lớp không cần mẫu hiện có khi được áp dụng cho các mô hình dựa trên khái niệm.

1. Giới thiệu
Với việc sử dụng ngày càng nhiều các mô hình học sâu trong các lĩnh vực đa dạng, bao gồm robot học [10], hình ảnh y tế [17], và lái xe tự động [43], có nhu cầu cấp thiết phát triển các mô hình có thể thích ứng với điều kiện luôn thay đổi và học các nhiệm vụ mới từ dữ liệu không dừng. Tuy nhiên, một thách thức đáng kể với mạng nơ-ron là xu hướng bị quên thảm khốc [26, 30, 44], nơi hiệu suất trên các nhiệm vụ trước đó giảm nhanh chóng khi những nhiệm vụ mới được tiếp thu. Học Liên tục (CL) [19] đã nổi lên như một kỹ thuật đầy hứa hẹn để giải quyết thách thức này bằng cách cho phép các mô hình học những nhiệm vụ mới mà không quên những nhiệm vụ đã học trước đó.

Trong khi các phương pháp CL hiện tại giảm đáng kể quên thảm khốc, chúng thường khó hiểu đối với con người. Điều này đặc biệt có vấn đề vì mạng nơ-ron sâu 

Task 4 Task 3 Task 2 
Fine-tuning LWF ICICLE EWC LWM Input image Task 1 
Hình 1: Chúng tôi xử lý hình ảnh đầu vào (trên trái) thông qua mạng và hiện thị cách các khu vực cụ thể của nó tương tự với một trong các nguyên mẫu. Sự trôi dạt khái niệm diễn giải xảy ra khi bản đồ tương tự như vậy khác nhau giữa các nhiệm vụ. ICICLE hoạt động tốt nhất, bảo tồn bản đồ tương tự tốt hơn so với các phương pháp học liên tục khác.

thường dự đoán câu trả lời đúng vì lý do sai (hiện tượng "Clever Hans"), dẫn đến hiệu suất xuất sắc trong huấn luyện nhưng hiệu suất kém trong thực tế [63]. Điều này dẫn đến các vấn đề xã hội nghiêm trọng ảnh hưởng sâu sắc đến sức khỏe, tự do, thiên lệch chủng tộc, và an toàn [11]. Kết quả là, một số bước đầu tiên đã được thực hiện trong tài liệu để giới thiệu các phương pháp có thể giải thích hậu hốc vào thiết lập CL [32, 52, 64].

Tuy nhiên, việc giải thích các hộp đen, thay vì thay thế chúng bằng các mô hình có thể diễn giải (tự giải thích), có thể làm trầm trọng thêm vấn đề bằng cách cung cấp các đặc tính sai lệch hoặc sai [70] hoặc thêm quyền lực không cần thiết vào hộp đen [12]. Do đó, có nhu cầu rõ ràng về các mô hình học máy sáng tạo vốn có thể diễn giải [11]. Theo hiểu biết tốt nhất của chúng tôi, chưa có phương pháp CL có thể diễn giải nào được đề xuất cho đến nay.

Trong công trình này, chúng tôi giới thiệu Học Tăng Dần Lớp Có Thể Diễn Giải arXiv:2303.07811v2  [cs.LG]  31 Jul 2023

--- TRANG 2 ---
(ICICLE), một phương pháp có thể diễn giải cho học tăng dần lớp dựa trên phương pháp luận phần nguyên mẫu. Tương tự như lý luận "Cái này trông giống cái kia" [16], ICICLE học một tập hợp các phần nguyên mẫu đại diện cho các khái niệm tham chiếu có nguồn gốc từ dữ liệu huấn luyện và đưa ra dự đoán bằng cách so sánh các phần hình ảnh đầu vào với các nguyên mẫu đã học. Tuy nhiên, việc chuyển giao kiến thức giữa các nhiệm vụ trong học liên tục đặt ra những thách thức mới cho khả năng diễn giải. Chủ yếu vì căn cứ đằng sau dự đoán của mô hình có thể thay đổi theo thời gian, dẫn đến sự trôi dạt khái niệm diễn giải và làm cho các giải thích không nhất quán (xem Hình 1 và Bảng 1). Do đó, ICICLE chứa nhiều cơ chế để ngăn chặn sự trôi dạt này và, cùng lúc, đạt được kết quả thỏa đáng. Đầu tiên, chúng tôi đề xuất điều chuẩn khả năng diễn giải phù hợp cho các mô hình dựa trên phần nguyên mẫu để giữ lại kiến thức đã có được trước đó trong khi duy trì tính dẻo của mô hình. Nó đảm bảo rằng các phần nguyên mẫu đã học trước đó được kích hoạt tương tự trong dữ liệu nhiệm vụ hiện tại, điều này làm cho các giải thích nhất quán theo thời gian. Hơn nữa, xem xét bản chất phân loại chi tiết của các tập dữ liệu được xem xét, chúng tôi giới thiệu khởi tạo nguyên mẫu dựa trên độ gần cho một nhiệm vụ mới. Nó tìm kiếm các khái niệm đại diện trong dữ liệu nhiệm vụ mới gần với các khái niệm đã học trước đó, cho phép mô hình nhận ra các đặc trưng cấp cao của nhiệm vụ mới và tập trung vào việc điều chỉnh chi tiết. Thứ ba, để vượt qua thiên lệch gần đây của nhiệm vụ trong các tình huống học tăng dần lớp, chúng tôi đề xuất một phương pháp đơn giản nhưng hiệu quả cân bằng các logits của tất cả nhiệm vụ dựa trên dữ liệu nhiệm vụ cuối cùng. Cuối cùng, chúng tôi giảm huấn luyện đa giai đoạn trong khi bảo tồn lý luận tích cực thân thiện với người dùng.

Chúng tôi đánh giá ICICLE trên hai tập dữ liệu, cụ thể là CUB-200-2011 [84] và Stanford Cars [46], và tiến hành loại bỏ triệt để để chứng minh hiệu quả của phương pháp của chúng tôi. Chúng tôi cho thấy rằng vấn đề này đầy thách thức nhưng mở ra một lĩnh vực nghiên cứu mới đầy hứa hẹn có thể thúc đẩy thêm sự hiểu biết của chúng ta về các phương pháp CL. Các đóng góp của chúng tôi có thể được tóm tắt như sau:

• Chúng tôi là những người đầu tiên giới thiệu học tăng dần lớp có thể diễn giải và đề xuất một phương pháp mới ICICLE, dựa trên phương pháp luận phần nguyên mẫu.
• Chúng tôi đề xuất điều chuẩn khả năng diễn giải ngăn chặn sự trôi dạt khái niệm diễn giải mà không sử dụng mẫu điển hình.
• Chúng tôi định nghĩa một chiến lược khởi tạo nguyên mẫu chuyên dụng và một phương pháp bù trừ thiên lệch gần đây của nhiệm vụ.

2. Các Công trình Liên quan
Học Liên tục và Học Tăng Dần Lớp
Các phương pháp học liên tục hiện tại có thể được phân loại rộng rãi thành ba loại: các phương pháp dựa trên phát lại, dựa trên kiến trúc, và dựa trên điều chuẩn [19, 54]. Các phương pháp dựa trên phát lại hoặc lưu một lượng nhỏ dữ liệu từ các nhiệm vụ đã thấy trước đó [5, 15] hoặc tạo ra dữ liệu tổng hợp với một mô hình sinh [86, 93]. Dữ liệu phát lại có thể được sử dụng IOU
PHƯƠNG PHÁP NHIỆM VỤ 1 NHIỆM VỤ 2 NHIỆM VỤ 3 TRUNG BÌNH
FINETUNING 0.115 0.149 0.260 0.151
EWC 0.192 0.481 0.467 0.334
LWF 0.221 0.193 0.077 0.188
LWM 0.332 0.312 0.322 0.325
ICICLE 0.705 0.753 0.742 0.728
Bảng 1: Kết quả định lượng cho sự trôi dạt khái niệm diễn giải được trình bày trong Hình 1. Chúng tôi tính IoU giữa các độ tương tự thu được sau mỗi nhiệm vụ và sau các nhiệm vụ tăng dần. Ví dụ trong cột "nhiệm vụ 1", chúng tôi tính IoU giữa bản đồ tương tự của các nguyên mẫu nhiệm vụ một sau mỗi tập huấn luyện.

trong quá trình huấn luyện cùng với dữ liệu hiện tại, như trong iCaRL [68] và LUCIR [37], hoặc để hạn chế hướng gradient trong khi huấn luyện, như trong AGEM [14]. Các phương pháp dựa trên kiến trúc kích hoạt các tập con khác nhau của các tham số mạng cho các nhiệm vụ khác nhau bằng cách cho phép các tham số mô hình tăng tuyến tính với số lượng nhiệm vụ. Các công trình trước đó theo chiến lược này bao gồm DER [89], Piggyback [50], PackNet [51]. Các phương pháp dựa trên điều chuẩn thêm một thuật ngữ điều chuẩn bổ sung có nguồn gốc từ kiến thức của các nhiệm vụ trước đó vào mất mát huấn luyện. Điều này có thể được thực hiện bằng cách điều chuẩn không gian trọng số, điều này hạn chế các tham số quan trọng [79, 82], hoặc không gian chức năng, điều này hạn chế dự đoán hoặc đặc trưng trung gian [23, 38]. EWC [44], MAS [3], REWC [49], SI [92], và RWalk [13] hạn chế tầm quan trọng của các tham số mạng để ngăn chặn quên lãng. Các phương pháp như LWF [48], LWM [21], và BiC [88] tận dụng chưng cất kiến thức để điều chuẩn đặc trưng hoặc dự đoán. Ngoài ra, các thiết lập thách thức hơn được xem xét trong lĩnh vực như học liên tục có thể diễn giải tập mở [57]. Khi nói đến CL có thể diễn giải, các phương pháp phát lại sinh [58] cung cấp một mức độ rõ ràng ẩn nhất định. Tuy nhiên, chúng yêu cầu một bộ giải mã (để hiển thị) và có thể thất bại trong việc tạo ra các hình ảnh nguyên mẫu thực tế [16].

Học tăng dần lớp (class-IL) là tình huống thách thức nhất trong đó bộ phân loại học các lớp mới một cách tuần tự. Mô hình cần duy trì hiệu suất tốt trên tất cả các lớp đã thấy cho đến nay [83]. Hai loại phương pháp đánh giá được định nghĩa [54]: không biết nhiệm vụ (không có quyền truy cập vào task-ID trong quá trình suy luận, ví dụ: BiC [88]) và biết nhiệm vụ (task-ID được cung cấp trong quá trình suy luận, ví dụ: HAT [78]).

Trí tuệ Nhân tạo Có thể Giải thích Trong lĩnh vực giải thích học sâu, hai loại mô hình đã được khám phá: mô hình hậu hốc và tự giải thích [70]. Các mô hình hậu hốc giải thích quá trình lý luận của các phương pháp hộp đen, bao gồm bản đồ hiển thị [53, 67, 76, 77, 80], vectơ kích hoạt khái niệm [18, 29, 40, 45, 90], ví dụ phản thực tế [1, 31, 56, 62, 87], và phân tích nhiễu loạn hình ảnh [7, 24, 25, 69]. Mặt khác, các mô hình tự giải thích nhằm làm cho quá trình quyết định minh bạch hơn và đã thu hút sự chú ý đáng kể [4, 9]. Gần đây, các nhà nghiên cứu đã tập trung vào việc nâng cao khái niệm

--- TRANG 3 ---
lớp tích chập 
 (backbone 𝑓, lớp add-on 𝑓A, và sigmoid) lớp phần nguyên mẫu 𝘨t -1(           ,                ) = 
gt-1(           ,                ) = gt-1(           ,                ) = 1.0
1.0
1.04.25 
5.72 
0.22 0.71 Zx
𝑓 𝑥
gt(           ,                ) = 
gt(           ,                ) = 
gt(           ,                ) = 0.12 
0.78 1.01.0
1.0
1.00.39 
lớp cuối nhãn lớp Wilson 
Warbler Prothonotary 
Warbler 
1.00.61 gt-1(           ,                ) = 1.00 Cardinal 1.47 
NHIỆM VỤ t 
0.49 gt(           ,                ) = 
0.90 Sooty 
Albatross 𝑓A
NHIỆM VỤ t-1 nguyên mẫu p
σ ht-1 
ht Hình 2: Kiến trúc của ICICLE với các lớp phần nguyên mẫu riêng biệt gt cho mỗi nhiệm vụ t. Trong ví dụ này, các nguyên mẫu của các lớp Prothonotary Warbler và Cardinal thuộc về nhiệm vụ t−1, trong khi các nguyên mẫu của Wilson Warbler và Sooty Albatross thuộc về nhiệm vụ t. Các lớp gt được đặt trước bởi backbone chung f, add-on fA, và sigmoid. Hơn nữa, chúng được theo sau bởi các lớp cuối ht với trọng số ht
ci= 1 nếu nguyên mẫu pi được gán cho lớp c và bằng 0 trong trường hợp ngược lại.

của các phần nguyên mẫu được giới thiệu trong ProtoPNet [16] để đại diện cho các mẫu kích hoạt của mạng. Một số phần mở rộng đã được đề xuất, bao gồm TesNet [85] và Deformable ProtoPNet [22], khai thác tính trực giao trong xây dựng nguyên mẫu. ProtoPShare [74], ProtoTree [59], và ProtoPool [73] giảm số lượng nguyên mẫu được sử dụng trong phân loại. Các phương pháp khác xem xét phân loại phân cấp với nguyên mẫu [33], biến đổi phần nguyên mẫu [47], và kỹ thuật chưng cất kiến thức từ nguyên mẫu [39]. Các giải pháp dựa trên nguyên mẫu đã được áp dụng rộng rãi trong các ứng dụng khác nhau như hình ảnh y tế [2, 6, 41, 72, 81], phân tích chuỗi thời gian [28], phân loại đồ thị [71, 94], học chuỗi [55], và phân đoạn nghĩa [75]. Trong công trình này, chúng tôi thích ứng cơ chế nguyên mẫu với học tăng dần lớp.

3. Phương pháp
Mục tiêu của phương pháp chúng tôi là tăng khả năng diễn giải trong tình huống tăng dần lớp. Để mục đích này, chúng tôi thích ứng các phần nguyên mẫu [16], trực tiếp tham gia vào tính toán mô hình, làm cho các giải thích trung thành với quyết định phân loại. Để làm cho công trình này độc lập, trước tiên chúng tôi nhớ lại phương pháp luận phần nguyên mẫu, và sau đó chúng tôi mô tả cách chúng tôi thích ứng chúng với tình huống tăng dần lớp. Chúng tôi nhằm bù trừ sự trôi dạt khái niệm diễn giải, điều mà chúng tôi định nghĩa ở cuối phần này.

3.1. Phương pháp luận phần nguyên mẫu
Kiến trúc. Việc triển khai ban đầu của các phần nguyên mẫu [16] giới thiệu một lớp phần nguyên mẫu bổ sung g được đặt trước bởi một mạng tích chập backbone f với một add-on fA và theo sau bởi lớp kết nối đầy đủ h. Add-on fA bao gồm hai lớp tích chập 1×1 và một kích hoạt sigmoid ở cuối, dịch đầu ra tích chập sang không gian phần nguyên mẫu. Lớp phần nguyên mẫu g bao gồm K nguyên mẫu pi∈RD cho mỗi lớp, và việc gán của chúng được xử lý bởi lớp kết nối đầy đủ h. Nếu nguyên mẫu pi được gán cho lớp c, thì hci= 1, nếu không, nó được đặt thành −0.5.

Suy luận. Với một hình ảnh đầu vào x, backbone f tạo ra biểu diễn f(x) của nó có hình dạng H×W×D, trong đó H và W là chiều cao và chiều rộng của biểu diễn thu được tại lớp tích chập cuối cùng, và D là số kênh. Biểu diễn này được dịch bởi fA sang không gian phần nguyên mẫu, một lần nữa có kích thước H×W×D. Sau đó, mỗi phần nguyên mẫu pi được so sánh với từng vectơ biểu diễn H×W để tính toán độ tương tự tối đa (tức là kích hoạt tối đa của nguyên mẫu này trên hình ảnh được phân tích) max j∈{1..HW}sim(pi, zj), trong đó

--- TRANG 4 ---
nguyên mẫu pt-1 
f  sau 
nhiệm vụ t-1
bản đồ 
tương tự nguyên mẫu 
sau nhiệm vụ tbản đồ 
tương tự nguyên mẫu 
sau nhiệm vụ t-1mặt nạ của 
độ tương tự cao nhấttối thiểu hóa 
MSE 
thấp caođộ tương tựf  sau 
nhiệm vụ tHình 3: Điều chuẩn khả năng diễn giải của chúng tôi nhằm tối thiểu hóa các thay đổi trong độ tương tự nguyên mẫu. Nó lấy một nguyên mẫu pt−1 của các nhiệm vụ trước đó và một hình ảnh từ nhiệm vụ t, chọn khu vực hình ảnh có độ tương tự cao nhất với nguyên mẫu này (mặt nạ nhị phân S), và phạt mô hình vì bất kỳ thay đổi nào trong khu vực này do huấn luyện nhiệm vụ t.

sim(pi, zj) = log|zj−pi|2+1
|zj−pi|2+η và η≪1. Để có được dự đoán cuối cùng, chúng tôi đẩy những giá trị đó qua lớp kết nối đầy đủ (và được khởi tạo thích hợp) h.

Huấn luyện. Huấn luyện được chia thành ba giai đoạn tối ưu hóa: khởi động, học kết hợp, và tối ưu hóa lồi của lớp cuối. Giai đoạn đầu tiên huấn luyện add-on fA và lớp phần nguyên mẫu g. Giai đoạn thứ hai học fA, g, và mạng backbone f. Giai đoạn cuối tinh chỉnh lớp kết nối đầy đủ h. Huấn luyện được tiến hành với mất mát cross-entropy được hỗ trợ bởi hai điều chuẩn, chi phí cụm và tách biệt [16]. Cụm khuyến khích mỗi hình ảnh huấn luyện có một vá ẩn gần với ít nhất một nguyên mẫu của lớp của nó. Ngược lại, chi phí tách biệt khuyến khích mọi vá ẩn của hình ảnh huấn luyện tránh xa các nguyên mẫu của các lớp còn lại.

3.2. ICICLE
Những sửa đổi đáng kể về kiến trúc và huấn luyện là cần thiết để sử dụng phương pháp luận phần nguyên mẫu cho học tăng dần lớp (suy luận là giống hệt). Chủ yếu vì học tăng dần có các phỏng đoán khá khác nhau. Nó giả định T nhiệm vụ (C1, X1),(C2, X2), . . . , (CT, XT), trong đó mỗi nhiệm vụ t chứa các lớp Ct và tập huấn luyện Xt. Hơn nữa, trong nhiệm vụ t, chỉ có dữ liệu huấn luyện Xt khả dụng, vì chúng tôi xem xét thiết lập không cần mẫu điển hình, nơi bị cấm lưu bất kỳ dữ liệu nào từ các nhiệm vụ trước đó (không cho phép bộ đệm phát lại).

Kiến trúc. Như trong mô hình cơ sở, ICICLE bao gồm backbone f và add-on fA. Tuy nhiên, nó không sử dụng một lớp phần nguyên mẫu g cố định và một lớp kết nối đầy đủ h. Thay vào đó, nó giới thiệu một lớp phần nguyên mẫu gt và một lớp kết nối đầy đủ ht cho mỗi nhiệm vụ liên tiếp. Các lớp gt bao gồm Mt phần nguyên mẫu, trong đó Mt= K·Ct và K là số nguyên mẫu trên mỗi lớp. Mặt khác, lớp ht có trọng số ht
ci= 1 nếu nguyên mẫu pi được gán cho lớp c và nó được đặt thành 0 trong trường hợp ngược lại. Chúng tôi đã loại bỏ trọng số âm (-0.5) từ lớp cuối vì

Không gian ẩn 
sau nhiệm vụ t-1 
nguyên mẫu từ nhiệm vụ t-1 
vị trí ban đầu cho 
nguyên mẫu nhiệm vụ t 
biểu diễn của dữ liệu nhiệm vụ t Khởi tạo ngẫu nhiên 
Khởi tạo độ gần 
cụm của biểu diễn Hình 4: Chúng tôi giới thiệu khởi tạo nguyên mẫu dựa trên độ gần mới. Nó bắt đầu bằng cách truyền các mẫu huấn luyện của nhiệm vụ t qua mạng (các chấm xanh lá) và chọn các biểu diễn gần nhất với các nguyên mẫu hiện tại (kim cương tím). Điều này dẫn đến nhiều điểm, mà chúng tôi phân cụm (hình tròn tím) để có được vị trí ban đầu của các phần nguyên mẫu nhiệm vụ t (kim cương vàng). Khởi tạo như vậy (dưới phải) được ưa thích hơn khởi tạo ngẫu nhiên (trên phải), nơi các nguyên mẫu mới có thể được tạo ra xa các nguyên mẫu cũ, mặc dù chúng chỉ khác nhau một chút.

huấn luyện đa giai đoạn không có lợi cho tình huống tăng dần lớp (xem Hình 6).

Huấn luyện. Để ngăn chặn quên thảm khốc, ICICLE sửa đổi hàm mất mát của giải pháp cơ sở. Ngoài ra, nó giới thiệu ba cơ chế: điều chuẩn khả năng diễn giải, khởi tạo nguyên mẫu dựa trên độ gần, và bù trừ thiên lệch gần đây của nhiệm vụ. Liên quan đến hàm mất mát cơ sở, cross-entropy được tính toán trên đầu ra đầy đủ của mô hình, bao gồm logits từ các lớp được học trong các nhiệm vụ trước đó. Tuy nhiên, chi phí cụm và tách biệt chỉ được tính toán trong head gt.

Điều chuẩn khả năng diễn giải. Chưng cất kiến thức [35] là một trong những phương pháp điều chuẩn mạnh được áp dụng để ngăn chặn quên lãng [48]. Tuy nhiên, kết quả thu được bằng ứng dụng trực tiếp của nó không thỏa đáng và dẫn đến sự trôi dạt giải thích đáng kể (xem Hình 1 và Phần 5). Do đó, chúng tôi giới thiệu một chi phí điều chuẩn bổ sung LIR (xem Hình 3), được lấy cảm hứng từ [39], tối thiểu hóa các thay đổi trong độ tương tự cho các phần nguyên mẫu của các nhiệm vụ trước đó. Nó được định nghĩa là:

LIR=HX
i=0WX
j=0|sim(pt−1, zt
i,j)−sim(pt, zt
i,j)|·Si,j(1)

trong đó sim(pt−1, zt
i,j) được tính toán cho mô hình được lưu trước khi huấn luyện nhiệm vụ t, và S là một mặt nạ nhị phân có kích thước H×W, chỉ ra các pixel biểu diễn có độ tương tự cao nhất (phân vị γ của những pixel đó). Chưng cất độ tương tự như vậy

--- TRANG 5 ---
tạo ra tính dẻo cao hơn khi học một nhiệm vụ mới nhưng, cùng lúc, giảm sự trôi dạt khả năng diễn giải.

Khởi tạo nguyên mẫu dựa trên độ gần. Khởi tạo ngẫu nhiên các nguyên mẫu, được đề xuất trong [16], thất bại trong học tăng dần (xem Bảng 5). Có lẽ vì các nguyên mẫu mới có thể được tạo ra xa các nguyên mẫu cũ, trong khi chúng chỉ khác nhau một chút (ví dụ: nguyên mẫu cánh của các loài chim khác nhau). Do đó, chúng tôi giới thiệu khởi tạo đặt các nguyên mẫu mới gần với các nguyên mẫu cũ (xem Hình 4). Chúng tôi bắt đầu bằng cách truyền các mẫu huấn luyện của nhiệm vụ t qua mạng và xác định những vá nào tương tự nhất với các nguyên mẫu hiện tại (chúng tôi chọn các vá từ phân vị α). Cụ thể hơn, chúng tôi tính toán tập {zt
j: max isim(pt−1
i, zt
j)∈αphân vị }. Điều này dẫn đến nhiều ứng viên cho các phần nguyên mẫu nhiệm vụ mới. Để có được K·Ct nguyên mẫu, chúng tôi thực hiện phân cụm KMeans++, và các trung tâm kết quả của các cụm được sử dụng để khởi tạo các phần nguyên mẫu trong gt.

Bù trừ thiên lệch gần đây của nhiệm vụ. Khi mô hình học nhiệm vụ t, độ tương tự với các nguyên mẫu của các nhiệm vụ trước đó giảm, chủ yếu do những thay đổi trong backbone (xem Hình 5). Đó là lý do tại sao, sau khi huấn luyện nhiệm vụ cuối cùng, chúng tôi bù trừ điều này bằng cách sử dụng T−1 hằng số thu được sử dụng dữ liệu nhiệm vụ cuối cùng. Chính xác hơn, đối với mỗi nhiệm vụ trước đó t < T, chúng tôi lấy logits yt=ht◦gt◦f(x) thu được cho tất cả x∈XT và tính toán thiên lệch ct sao cho |{x∈XT: max ( yt+ct)>maxyT}|=u|XT|. Một cách trực quan, chúng tôi điều chỉnh ct để mô hình thay đổi u% dự đoán của nó từ nhiệm vụ T sang nhiệm vụ t. Chúng tôi xác định thực nghiệm rằng u= 10% là tối ưu.

3.3. Sự Trôi dạt Khái niệm Diễn giải
Như đã lưu ý trong chú thích của Hình 1, sự trôi dạt khái niệm diễn giải xảy ra khi bản đồ tương tự khác nhau giữa các nhiệm vụ. Do đó, nó có thể được định nghĩa chính thức là:

ICD =EH,W
i,j=1sim(pt−1, zt
i,j)−sim(pt, zt
i,j),(2)

trong đó (zi,j)H,W
i,j=1 tương ứng với biểu diễn hình ảnh đầu vào, pt−1 và pt tương ứng với phần nguyên mẫu p trước và sau nhiệm vụ t, và sim là độ tương tự được định nghĩa trong Phần 3.1 của bài báo. Do đó, ICD càng lớn thì sự trôi dạt khái niệm diễn giải càng lớn.

4. Thiết lập Thí nghiệm
Chúng tôi đánh giá phương pháp của chúng tôi trên các tập dữ liệu CUB-200-2011 [84] và Stanford Cars [46] để phân loại 200 loài chim và 196 mẫu xe, tương ứng. Chúng tôi xem xét các tình huống học 4, 10, và 20 nhiệm vụ cho chim và 4, 7, và 14 tùy chọn cho xe. Như backbone f, chúng tôi lấy ResNet-34 [34] không có lớp cuối và được tiền huấn luyện trên ImageNet [20]. Chúng tôi đặt số nguyên mẫu trên mỗi lớp là 10. Hơn nữa, chúng tôi sử dụng các phần nguyên mẫu có kích thước 1×1×256 và 1×1×128 cho chim và xe, tương ứng. Trọng số của CE, cụm, tách biệt, và chi phí chưng cất trong hàm mất mát bằng 1.0, 0.8, −0.08, và 0.01. Trong chưng cất, chúng tôi lấy λ= 1/49 pixel biểu diễn có độ tương tự cao nhất. Đối với khởi tạo dựa trên độ gần, chúng tôi sử dụng α= 0.5. Đối với bù trừ thiên lệch gần đây của nhiệm vụ, chúng tôi lấy ct, thay đổi dự đoán của tập xác thực cuối cùng ít hơn 10%. Như khung triển khai, chúng tôi sử dụng FACIL [54] dựa trên thư viện PyTorch1. Chi tiết về thiết lập thí nghiệm được cung cấp trong Tài liệu Bổ sung2.

5. Kết quả
Hiệu suất. Chúng tôi đánh giá hiệu quả của ICICLE bằng cách so sánh nó với các phương pháp cơ sở không cần mẫu điển hình thường được sử dụng trong học tăng dần lớp, bao gồm LWF [48], LWM [21], và EWC[44]3. Ngoài ra, Fine-tuning, và Freezing của bộ trích xuất đặc trưng (không được huấn luyện chút nào) được cung cấp. Chúng tôi cũng báo cáo học đa nhiệm vụ như một cận trên nơi các nhiệm vụ khác nhau được học kết hợp theo cách đa nhiệm vụ. Để làm như vậy, chúng tôi phân tích độ chính xác biết nhiệm vụ và không biết nhiệm vụ cho mỗi nhiệm vụ sau nhiệm vụ cuối cùng (Bảng 3) và độ chính xác trung bình tăng dần tổng hợp sau khi học nhiệm vụ cuối cùng trong các tình huống liên quan đến 4, 10, và 20 nhiệm vụ cho CUB (Bảng 2) và 4, 7, và 14 nhiệm vụ cho Stanford Cars (Tài liệu Bổ sung). Tất cả các phương pháp sử dụng cùng kiến trúc mạng trích xuất đặc trưng và ProtoPNet cho học dựa trên phần nguyên mẫu. Phương pháp của chúng tôi vượt trội so với các phương pháp cơ sở trong tất cả các trường hợp, cho thấy hiệu suất vượt trội của nó cho học dựa trên phần nguyên mẫu theo cách liên tục. ICICLE giữ lại kiến thức từ các nhiệm vụ trước đó tốt hơn, dẫn đến độ chính xác cân bằng hơn giữa các nhiệm vụ và độ chính xác cao hơn cho nhiệm vụ đầu tiên so với tất cả các phương pháp khác. Tuy nhiên, bất chấp sự cải thiện đáng kể, phương pháp của chúng tôi vẫn còn chỗ để cải thiện so với cận trên của huấn luyện đa nhiệm vụ. Với số lượng nhiệm vụ lớn hơn, việc quên lãng của mô hình là

1https://pytorch.org
2Mã có sẵn tại: https://github.com/gmum/ICICLE
3Việc mở rộng các mô hình có thể diễn giải với các phương pháp không cần mẫu phức tạp hơn không đơn giản, và do đó chúng tôi đã loại trừ các phương pháp như SDC [91] (yêu cầu học với mất mát metric) và PASS [95] (yêu cầu kết hợp với học tự giám sát).

--- TRANG 6 ---
ĐỘ CHÍNH XÁC TRUNG BÌNH TĂNG DẦN BIẾT NHIỆM VỤ ĐỘ CHÍNH XÁC TRUNG BÌNH TĂNG DẦN KHÔNG BIẾT NHIỆM VỤ
PHƯƠNG PHÁP 4NHIỆM VỤ 10NHIỆM VỤ 20NHIỆM VỤ 4NHIỆM VỤ 10NHIỆM VỤ 20NHIỆM VỤ
FREEZING 0.560±0.027 0.531±0.042 0.452±0.055 0.309±0.024 0.115±0.028 0.078±0.004
FINETUNING 0.229±0.005 0.129±0.017 0.147±0.021 0.177±0.006 0.072±0.008 0.044±0.006
EWC 0.445±0.012 0.288±0.034 0.188±0.031 0.213±0.008 0.095±0.007 0.046±0.011
LWM 0.452±0.023 0.294±0.032 0.226±0.025 0.180±0.028 0.090±0.011 0.044±0.008
LWF 0.301±0.048 0.175±0.028 0.129±0.023 0.219±0.019 0.078±0.008 0.072±0.008
ICICLE 0.654±0.011 0.602±0.035 0.497±0.099 0.350±0.053 0.185±0.005 0.099±0.003
Đa nhiệm vụ 0.858±0.005 0.905±0.012 0.935±0.019 0.499±0.009 0.196±0.017 0.148±0.009
FeTrIL [65] 0.750±0.008 0.607±0.018 0.407±0.051 0.375±0.006 0.199±0.003 0.127±0.011
PASS [95] 0.775±0.006 0.647±0.003 0.518±0.012 0.395±0.001 0.233±0.009 0.139±0.017

Bảng 2: So sánh độ chính xác trung bình tăng dần cho số lượng nhiệm vụ khác nhau trên CUB-200-2011, chứng minh tác động tiêu cực của số lượng nhiệm vụ cao cần được học đến hiệu suất của mô hình. Bất chấp xu hướng này, ICICLE vượt trội so với các phương pháp cơ sở trên tất cả số lượng nhiệm vụ. Ngoài ra, chúng tôi cho thấy khoảng cách giữa các mô hình có thể diễn giải và hộp đen bằng cách so sánh ICICLE với FeTrIL và PASS.

ĐỘ CHÍNH XÁC BIẾT NHIỆM VỤ ĐỘ CHÍNH XÁC KHÔNG BIẾT NHIỆM VỤ
PHƯƠNG PHÁP NHIỆM VỤ 1 NHIỆM VỤ 2 NHIỆM VỤ 3 NHIỆM VỤ 4 NHIỆM VỤ 1 NHIỆM VỤ 2 NHIỆM VỤ 3 NHIỆM VỤ 4
FREEZING 0.806±0.024 0.462±0.037 0.517±0.041 0.455±0.027 0.570±0.031 0.195±0.017 0.258±0.019 0.213±0.020
FINETUNING 0.007±0.004 0.016±0.008 0.032±0.009 0.759±0.019 0.0±0.0 0.0±0.0 0.0±0.0 0.759±0.019
EWC 0.244±0.024 0.378±0.072 0.539±0.043 0.602±0.054 0.001±0.001 0.059±0.004 0.267±0.031 0.527±0.051
LWF 0.169±0.046 0.119±0.008 0.235±0.017 0.743±0.061 0.158±0.035 0.003±0.002 0.018±0.003 0.537±0.142
LWM 0.195±0.012 0.412±0.014 0.430±0.028 0.772±0.011 0.027±0.024 0.023±0.020 0.085±0.005 0.772±0.037
ICICLE 0.523±0.020 0.663±0.053 0.709±0.038 0.723±0.002 0.233±0.014 0.365±0.021 0.314±0.011 0.486±0.021

Bảng 3: So sánh độ chính xác nhiệm vụ cho kiến trúc ProtoPNet đã sửa đổi trong tình huống học tăng dần lớp sau 4 nhiệm vụ huấn luyện trên tập dữ liệu CUB-200-2011, trung bình qua 3 lần chạy với sai số chuẩn của trung bình. ICICLE của chúng tôi vượt trội so với các phương pháp cơ sở và đạt kết quả tốt nhất cho tất cả các nhiệm vụ tăng dần trước đó, chứng minh khả năng duy trì kiến thức trước đó trong khi học các nhiệm vụ mới. Freezing do sự cố định trọng số không thể học đúng cách các nhiệm vụ mới.

cao hơn, dẫn đến kết quả kém hơn, có thể được quy cho số lượng chi tiết mà các nguyên mẫu cần nắm bắt để phân loại một nhiệm vụ đúng cách. Hơn nữa, chúng tôi đã nhận thấy rằng freezing là một cơ sở mạnh mẽ cho tình huống biết nhiệm vụ vì bản chất cố định của mô hình và backbone được tiền huấn luyện.

Khả năng diễn giải Để đánh giá nếu biểu diễn đồ họa của khái niệm nguyên mẫu đã thay đổi và bao nhiêu, chúng tôi sử dụng metric IoU [75]. IoU đo lường sự trùng lặp của biểu diễn trực quan nguyên mẫu (như trong Hình 1) từ nhiệm vụ mà nó được học, qua tất cả các nhiệm vụ tiếp theo. Freezing là vượt trội trong việc bảo tồn thông tin nguyên mẫu vì tất cả trọng số từ các nhiệm vụ trước đó được cố định. Về các phương pháp cho phép thay đổi trong backbone và các nguyên mẫu đã học trước đó, ICICLE là vượt trội so với tất cả các cơ sở, như được hiển thị trong Bảng 1. ICICLE giữ các nguyên mẫu có thể diễn giải nhất quán với điều chuẩn khả năng diễn giải chưng cất các khái niệm đã học.

5.1. Nghiên cứu loại bỏ và phân tích
Tại sao cần thay đổi kiến trúc và huấn luyện ProtoPNet? ProtoPNet trong giai đoạn huấn luyện cuối (tối ưu hóa lồi lớp cuối) nhằm tinh chỉnh các kết nối tích cực và điều chuẩn các kết nối âm thành 0. Kết quả là, mô hình hội tụ trả về các giải thích dưới dạng lý luận tích cực, được mong muốn bởi người dùng cuối [11]. Trong thiết lập CL, bước cuối của huấn luyện thay đổi các kết nối âm theo cách khác (xem Hình 6). Mặt khác,

Điều chuẩn Khởi tạo Bù trừ Độ chính xác TAw Độ chính xác TAg
0.216 0.182
✓ 0.559 0.280
✓ ✓ 0.654 0.335
✓ ✓ ✓ 0.654 0.350

Bảng 4: Ảnh hưởng của các thành phần mới khác nhau đến độ chính xác trung bình tăng dần trong tình huống học bốn nhiệm vụ. Kết hợp tất cả các thành phần dẫn đến mô hình hoạt động tốt nhất.

Giá trị trọng số trung bình 
chỉ số lớp chỉ số lớp kết nối âm kết nối dương 

Hình 6: Trọng số trung bình của các kết nối dương và âm trên mỗi lớp trong tình huống học 4 nhiệm vụ. Các kết nối âm không cân bằng và mạnh giữa các nhiệm vụ dẫn đến các thuộc tính không mong muốn về khả năng diễn giải của mô hình.

trong tình huống học liên tục không cần mẫu, việc tiến hành giai đoạn học lớp cuối là không khả thi ở cuối huấn luyện. Đó là lý do tại sao, chúng tôi đã sửa đổi lớp cuối của ProtoPNet và chỉ giữ lại các kết nối dương được khởi tạo thành 1, loại bỏ nhu cầu về bước tối ưu hóa lồi.

--- TRANG 7 ---
điều chuẩn độ tương tự điều chuẩn khoảng cách điều chuẩn đặc trưng 
không gian mà hàm mất mát 
có cùng giá trị phần nguyên mẫu 
biểu diễn phần hình ảnh 
hàm độ tương tự ProtoPNet giá trị độ tương tự 
giá trị khoảng cách Hình 7: Hiện thị ba phương pháp có thể có cho độ tương tự khả năng diễn giải và ảnh hưởng của chúng đến tính dẻo của mô hình. Chỉ có điều chuẩn dựa trên độ tương tự tính đến cách một phần hình ảnh nhất định tương ứng với một phần nguyên mẫu. Nếu nó gần thì giá trị độ tương tự cao và những thay đổi nhỏ trong khoảng cách dẫn đến giảm lớn trong độ tương tự. Trong khi các vectơ ẩn xa các phần nguyên mẫu có thể được thay đổi tự do hơn bởi mô hình để đại diện tốt hơn dữ liệu nhiệm vụ hiện tại. Các phương pháp khác hạn chế tính dẻo của mô hình coi mỗi biểu diễn ẩn của phần hình ảnh như quan trọng ngang nhau.

Ảnh hưởng của từng thành phần được giới thiệu là gì? Bảng 4 trình bày ảnh hưởng của các thành phần khác nhau trong phương pháp của chúng tôi đến độ chính xác trung bình tăng dần cuối cùng của mô hình trong tập dữ liệu CUB-200-2011 với tình huống chia bốn nhiệm vụ. Kết hợp tất cả các thành phần dẫn đến mô hình hoạt động tốt nhất. Kết quả của chúng tôi cho thấy bù trừ thiên lệch gần đây của nhiệm vụ giúp trong đánh giá không biết nhiệm vụ và mang lại cải thiện bổ sung 4.5%. Tuy nhiên, hầu hết các cải thiện độ chính xác được quy cho điều chuẩn khả năng diễn giải và khởi tạo độ gần. Đáng chú ý, bù trừ thiên lệch gần đây của nhiệm vụ cải thiện đáng kể hiệu suất của các lớp nhiệm vụ một so với phương pháp không có nó, từ 0.028 đến 0.255 trong tình huống không biết nhiệm vụ, như được chi tiết trong Tài liệu Bổ sung.

Chúng ta nên thực hiện điều chuẩn khả năng diễn giải ở đâu? Lớp nguyên mẫu của mô hình ProtoPNet có thể được điều chuẩn theo ba cách khác nhau: điều chuẩn đặc trưng trên biểu diễn lớp add-on, điều chuẩn khoảng cách giữa các phần nguyên mẫu và vectơ dữ liệu ẩn, và điều chuẩn dựa trên độ tương tự. Phương pháp nghiêm ngặt nhất là điều chuẩn đặc trưng, không cho phép mô hình thay đổi cách nó biểu diễn dữ liệu từ nhiệm vụ mới, dẫn đến tính dẻo mô hình giảm đáng kể. Khi khoảng cách được điều chuẩn, mô hình có thể thay đổi biểu diễn của nó để duy trì cùng khoảng cách từ nguyên mẫu trên bề mặt của hình cầu. Mặt khác, điều chuẩn dựa trên độ tương tự cho phép mô hình giữ lại kiến thức chính từ các nhiệm vụ trước đó bằng cách chỉ bảo tồn thông tin liên quan đến các đặc trưng cụ thể gần với các phần nguyên mẫu trong không gian ẩn, cho phép linh hoạt lớn hơn để đổi lấy việc quên các đặc trưng không liên quan. Do đó, chúng tôi tuân theo điều chuẩn khả năng diễn giải trong ICICLE, dựa trên độ tương tự và duy trì kiến thức thiết yếu từ các nhiệm vụ trước đó trong khi giữ lại tính dẻo cao để học những nhiệm vụ mới. Hình 7 minh họa ba phương pháp này và so sánh chúng về độ chính xác trung bình tăng dần cho ProtoPNet chỉ với điều chuẩn (không thay đổi khởi tạo): 0.507, 0.535, và 0.559 trong biết nhiệm vụ và 0.261, 0.230, và 0.280 trong các tình huống không biết nhiệm vụ cho điều chuẩn đặc trưng, khoảng cách, và dựa trên độ tương tự, tương ứng, trên tập dữ liệu CUB-200-2011 với tình huống bốn nhiệm vụ.

Ảnh hưởng của các siêu tham số trong điều chuẩn khả năng diễn giải là gì? Trong Hình 8 và Hình 9, ảnh hưởng của λ và ngưỡng phân vị mặt nạ trong điều chuẩn khả năng diễn giải đến độ chính xác trung bình tăng dần được trình bày. Chúng tôi sử dụng tập dữ liệu CUB-200-2011 với thiết lập chia bốn nhiệm vụ. Đối với tập dữ liệu này, kết quả tiết lộ rằng điều chuẩn chỉ độ tương tự nguyên mẫu tối đa là hiệu quả nhất (Hình 9). Liên quan đến λIR, một giá trị quá nhỏ dẫn đến tính dẻo mạng cao, quên lãng tăng, và kết quả kém, trong khi một giá trị quá lớn giảm tính dẻo mô hình và có thể không đại diện tốt kiến thức mới.

Cách nào là tốt nhất để khởi tạo các phần nguyên mẫu mới? Trong phần loại bỏ này, chúng tôi điều tra chiến lược tối ưu để khởi tạo các phần nguyên mẫu ở đầu nhiệm vụ mới trong mô hình ProtoPNet. Chúng tôi đánh giá phương pháp khởi tạo của chúng tôi, khởi tạo các phần gần với các nguyên mẫu hiện tại, so với ba phương pháp khác: khởi tạo ngẫu nhiên, phân cụm tất cả biểu diễn phần hình ảnh, và phân cụm chỉ các vectơ ẩn xa. Kết quả được trình bày trong Bảng 5. Phương pháp khởi tạo độ gần vượt trội so với chiến lược xa, vì chiến lược sau có xu hướng gán các phần nguyên mẫu cho các vectơ ẩn tương ứng với nền của hình ảnh, dẫn đến học các khái niệm không liên quan có thể dễ dàng kích hoạt trên dữ liệu nhiệm vụ khác, như được hiển thị trong Tài liệu Bổ sung.

ICICLE có tổng quát hóa cho các kiến trúc khác không? Cuối cùng, chúng tôi cho thấy rằng ICICLE tổng quát hóa cho kiến trúc dựa trên khái niệm khác. Chúng tôi chứng minh rằng sử dụng mô hình TesNet [85], và cung cấp kết quả trong Bảng 6, nơi ICICLE đạt kết quả tốt nhất. Độ chính xác trung bình tăng dần của ICICLE với TesNet thậm chí còn tốt hơn ProtoPNet cho cả đánh giá biết nhiệm vụ và không biết nhiệm vụ.

Loại khởi tạo Ngẫu nhiên Xa Tất cả Độ gần
Độ chính xác biết nhiệm vụ 0.559 0.592 0.626 0.654
Độ chính xác không biết nhiệm vụ 0.280 0.290 0.297 0.335

Bảng 5: So sánh các chiến lược khởi tạo khác nhau cho các phần nguyên mẫu. Khởi tạo độ gần của chúng tôi cho các nguyên mẫu nhiệm vụ mới là vượt trội.

--- TRANG 8 ---
Freezing Finetuning EWC LWM LWF ICICLE
Độ chính xác TAw 0.637 0.355 0.592 0.648 0.581 0.746
Độ chính xác TAg 0.222 0.183 0.272 0.252 0.205 0.362

Bảng 6: Kết quả cho tình huống học bốn nhiệm vụ trên tập dữ liệu CUB-200-2011 với TesNet [85] như một kiến trúc dựa trên khái niệm. Bảng cho thấy tính linh hoạt của phương pháp ICICLE cho các mô hình có thể diễn giải.

Hình 8: Ảnh hưởng của λIR trong điều chuẩn khả năng diễn giải.

Hình 9: Ảnh hưởng của γ trong điều chuẩn khả năng diễn giải. Lưu ý rằng điều chuẩn chỉ tại vị trí của độ tương tự tối đa là có lợi nhất cho ICICLE trong tình huống học bốn nhiệm vụ trong CUB-200-2011.

6. Kết luận và công việc tương lai
Công trình này đề xuất một phương pháp mới gọi là ICICLE cho học tăng dần lớp có thể diễn giải. ICICLE dựa trên các phần nguyên mẫu và kết hợp điều chuẩn khả năng diễn giải, khởi tạo độ gần, và bù trừ thiên lệch gần đây của nhiệm vụ. Phương pháp được đề xuất vượt trội so với các phương pháp học tăng dần lớp cổ điển được áp dụng cho mạng dựa trên phần nguyên mẫu về độ chính xác biết nhiệm vụ và không biết nhiệm vụ trong khi duy trì khả năng diễn giải nguyên mẫu. Chúng tôi cũng tiến hành các nghiên cứu loại bỏ và nhiều phân tích để biện minh cho các lựa chọn của chúng tôi và làm nổi bật những thách thức liên quan đến việc kết hợp các khái niệm có thể diễn giải với CL. Công trình này được kỳ vọng sẽ truyền cảm hứng cho nghiên cứu về XAI và CL. Tiến về phía trước, chúng tôi dự định khám phá các phương pháp phù hợp cho học tăng dần lớp đơn với các mô hình có thể diễn giải. Chúng tôi cũng có ý định điều tra cách các kiến trúc có thể diễn giải khác, như B-COS [8], có thể được thích ứng với tình huống học tăng dần lớp.

Hạn chế. Công trình của chúng tôi bị hạn chế đối với các phương pháp phần nguyên mẫu, phù hợp cho nhận dạng hình ảnh phân loại chi tiết và kế thừa các nhược điểm của chúng đã được thảo luận trước đó trong [27, 36, 42, 60, 73]. Tuy nhiên, gần đây có những công trình đầu tiên tổng quát hóa chúng cho các tập dữ liệu tiêu chuẩn (không phân loại chi tiết) [61]. Ngoài ra, vì chúng tôi chỉ xem xét tình huống không cần mẫu và nhận dạng tập đóng, chúng tôi không phân tích cách có bộ đệm phát lại sẽ ảnh hưởng đến hiệu suất của phương pháp và cách phương pháp này sẽ phù hợp trong các thiết lập tập mở.

Tác động. ICICLE làm nổi bật rằng các phương pháp không cần mẫu truyền thống cho học liên tục không phù hợp tốt cho các mô hình hộp xám sử dụng khái niệm cho dự đoán. Phát hiện này có ý nghĩa đối với việc phát triển các phương pháp học liên tục, vì chúng phải cân bằng nhu cầu về tính tổng quát với nhu cầu được thích ứng cho các kiến trúc cụ thể. Hơn nữa, nó có tác động đến lĩnh vực các mô hình dựa trên khái niệm và AI có thể giải thích, chứng minh nhu cầu nghiên cứu thêm về các phương pháp CL cho XAI. Trong một số trường hợp, các thực hành viên biết rằng hệ thống của họ sẽ cần học các nhiệm vụ mới liên tục có thể chọn sử dụng các mô hình hộp đen và bộ giải thích thay vì các mô hình có thể diễn giải, hy sinh độ trung thực của giải thích để cải thiện hiệu suất mô hình.

Lời cảm ơn
Joost van de Weijer và Bartłomiej Twardowski thừa nhận sự hỗ trợ từ tài trợ của Chính phủ Tây Ban Nha cho các dự án PID2019-104174GB-I00, TED2021-132513B-I00, và tài trợ RYC2021-032765-I, Ủy ban Châu Âu theo Chương trình Chân trời 2020, được tài trợ bởi MCIN/AEI/10.13039/501100011033 và bởi Liên minh Châu Âu NextGenerationEU/PRTR. Công việc của Dawid Rymarczyk được hỗ trợ bởi Trung tâm Khoa học Quốc gia (Ba Lan), tài trợ số 2022/45/N/ST6/04147. Anh ấy cũng nhận được học bổng khuyến khích từ chương trình Sáng kiến Xuất sắc - Quỹ Đại học Nghiên cứu tại Đại học Jagiellonian ở Kraków. Công việc của Bartosz Zieliński được tài trợ bởi tài trợ của Trung tâm Khoa học Quốc gia (Ba Lan) số 2022/47/B/ST6/03397 và dự án nghiên cứu "Mạng nơ-ron nhân tạo lấy cảm hứng từ sinh học" (tài trợ số POIR.04.04.00-00-14DE/18-00) trong chương trình Team-Net của Quỹ Khoa học Ba Lan được đồng tài trợ bởi Liên minh Châu Âu theo Quỹ Phát triển Khu vực Châu Âu. Cuối cùng, một số thí nghiệm được thực hiện trên các máy chủ được mua bằng quỹ từ tài trợ Khu vực Nghiên cứu Ưu tiên (Trung tâm Cơ sở Trí tuệ Nhân tạo) theo Chương trình Chiến lược Sáng kiến Xuất sắc tại Đại học Jagiellonian.

Tài liệu tham khảo
[1] Ehsan Abbasnejad, Damien Teney, Amin Parvaneh, Javen Shi, and Anton van den Hengel. Counterfactual vision and language learning. In Proceedings of the IEEE/CVF Con-

--- TRANG 9 ---
ference on Computer Vision and Pattern Recognition , pages 10044–10054, 2020. 2
[2] Michael Anis Mihdi Afnan, Yanhe Liu, Vincent Conitzer, Cynthia Rudin, Abhishek Mishra, Julian Savulescu, and Masoud Afnan. Interpretable, not black-box, artificial intelligence should be used for embryo selection. Human Reproduction Open , 2021. 3
[3] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. In European Conference on Computer Vision , 2018. 2
[4] David Alvarez Melis and Tommi Jaakkola. Towards robust interpretability with self-explaining neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems , volume 31. Curran Associates, Inc., 2018. 2
[5] Jihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo Ha, and Jonghyun Choi. Rainbow memory: Continual learning with a memory of diverse samples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8218–8227, 2021. 2
[6] Alina Jade Barnett, Fides Regina Schwartz, Chaofan Tao, Chaofan Chen, Yinhao Ren, Joseph Y Lo, and Cynthia Rudin. A case-based interpretable deep learning model for classification of mass lesions in digital mammography. Nature Machine Intelligence , 3(12):1061–1070, 2021. 3
[7] Dominika Basaj, Witold Oleszkiewicz, Igor Sieradzki, Michał Gółrzzczak, B Rychalska, T Trzcinski, and B Zielinski. Explaining self-supervised image representations with visual probing. In International Joint Conference on Artificial Intelligence , 2021. 2
[8] Moritz Böhle, Mario Fritz, and Bernt Schiele. B-cos networks: alignment is all we need for interpretability. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 10329–10338, 2022. 8
[9] Wieland Brendel and Matthias Bethge. Approximating CNNs with bag-of-local-features models works surprisingly well on imagenet. In International Conference on Learning Representations , 2019. 2
[10] Lukas Brunke, Melissa Greeff, Adam W Hall, Zhaocong Yuan, Siqi Zhou, Jacopo Panerati, and Angela P Schoellig. Safe learning in robotics: From learning-based control to safe reinforcement learning. Annual Review of Control, Robotics, and Autonomous Systems , 5:411–444, 2022. 1
[11] Rudin C. et al. Interpretable machine learning: Fundamental principles and 10 grand challenges. Statistics Surveys , 16:1–85, 2022. 1, 6
[12] Rudin C. and Radin J. Why are we using black box models in ai when we don't need to? a lesson from an explainable ai competition. Harvard Data Science Review , 1(2):10–1162, 2019. 1
[13] Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian walk for incremental learning: understanding forgetting and intransigence. In European Conference on Computer Vision , 2018. 2
[14] Arslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient lifelong learning with a-gem. In International Conference on Learning Representations , 2019. 2
[15] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc'Aurelio Ranzato. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486 , 2019. 2
[16] Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan K Su. This looks like that: deep learning for interpretable image recognition. Advances in Neural Information Processing Systems , 32, 2019. 2, 3, 4, 5
[17] Xuxin Chen, Ximin Wang, Ke Zhang, Kar-Ming Fung, Theresa C Thai, Kathleen Moore, Robert S Mannel, Hong Liu, Bin Zheng, and Yuchen Qiu. Recent advances and clinical applications of deep learning in medical image analysis. Medical Image Analysis , page 102444, 2022. 1
[18] Zhi Chen, Yijie Bei, and Cynthia Rudin. Concept whitening for interpretable image recognition. Nature Machine Intelligence , 2(12):772–782, 2020. 2
[19] Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Greg Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2021. 1, 2
[20] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition , pages 248–255. Ieee, 2009. 5
[21] Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng, Ziyan Wu, and Rama Chellappa. Learning without memorizing. In Conference on Computer Vision and Pattern Recognition , 2019. 2, 5
[22] Jon Donnelly, Alina Jade Barnett, and Chaofan Chen. Deformable protopnet: An interpretable image classifier using deformable prototypes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 10265–10275, 2022. 3
[23] Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, and Eduardo Valle. Podnet: Pooled outputs distillation for small-tasks incremental learning. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XX 16 , pages 86–102. Springer, 2020. 2
[24] Ruth Fong, Mandela Patrick, and Andrea Vedaldi. Understanding deep networks via extremal perturbations and smooth masks. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 2950–2958, 2019. 2
[25] Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturbation. In Proceedings of the IEEE international conference on computer vision , pages 3429–3437, 2017. 2
[26] Robert M. French. Catastrophic forgetting in connectionist networks. Trends in cog. scie. , 1999. 1
[27] Srishti Gautam, Marina M-C Höhne, Stine Hansen, Robert Jenssen, and Michael Kampffmeyer. This looks more like that: Enhancing self-explaining models by prototypical relevance propagation. Pattern Recognition , 136:109172, 2023. 8
[28] Alan H Gee, Diego Garcia-Olano, Joydeep Ghosh, and David Paydarfar. Explaining deep classification of time-series data with learned prototypes. In CEUR workshop proceedings , volume 2429, page 15. NIH Public Access, 2019. 3
[29] Amirata Ghorbani, James Wexler, James Y Zou, and Been Kim. Towards automatic concept-based explanations. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019. 2
[30] Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks. In International Conference on Learning Representations , 2014. 1
[31] Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, and Stefan Lee. Counterfactual visual explanations. In International Conference on Machine Learning , pages 2376–2384. PMLR, 2019. 2
[32] Filip Guzy, Michał Woźniak, and Bartosz Krawczyk. Evaluating and explaining generative adversarial networks for continual learning under concept drift. In 2021 International Conference on Data Mining Workshops (ICDMW) , pages 295–303. IEEE, 2021. 1
[33] Peter Hase, Chaofan Chen, Oscar Li, and Cynthia Rudin. Interpretable image recognition with hierarchical prototypes. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing , volume 7, pages 32–40, 2019. 3
[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 770–778, 2016. 5
[35] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In NIPS Deep Learning Workshop , 2014. 4
[36] Adrian Hoffmann, Claudio Fanconi, Rahul Rade, and Jonas Kohler. This looks like that... does it? shortcomings of latent space prototype interpretability in deep networks, 2021. 8
[37] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In International Conference on Computer Vision , 2019. 2
[38] Xinting Hu, Kaihua Tang, Chunyan Miao, Xian-Sheng Hua, and Hanwang Zhang. Distilling causal effect of data in class-incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 3957–3966, 2021. 2
[39] Monish Keswani, Sriranjani Ramakrishnan, Nishant Reddy, and Vineeth N Balasubramanian. Proto2proto: Can you recognize the car, the way i do? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 10233–10243, 2022. 3, 4
[40] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International conference on machine learning , pages 2668–2677. PMLR, 2018. 2
[41] Eunji Kim, Siwon Kim, Minji Seo, and Sungroh Yoon. Xprotonet: Diagnosis in chest radiography with global and local explanations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15719–15728, 2021. 3
[42] Sunnie SY Kim, Nicole Meister, Vikram V Ramaswamy, Ruth Fong, and Olga Russakovsky. Hive: evaluating the human interpretability of visual explanations. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XII , pages 280–298. Springer, 2022. 8
[43] B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A Al Sallab, Senthil Yogamani, and Patrick Pérez. Deep reinforcement learning for autonomous driving: A survey. IEEE Transactions on Intelligent Transportation Systems , 23(6):4909–4926, 2021. 1
[44] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. National Academy of Sciences , 2017. 1, 2, 5
[45] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. Concept bottleneck models. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Research , pages 5338–5348. PMLR, 13–18 Jul 2020. 2
[46] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer vision workshops , pages 554–561, 2013. 2, 5
[47] Oscar Li, Hao Liu, Chaofan Chen, and Cynthia Rudin. Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 32, 2018. 3
[48] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2017. 2, 4, 5
[49] Xialei Liu, Marc Masana, Luis Herranz, Joost Van de Weijer, Antonio M Lopez, and Andrew D Bagdanov. Rotate your networks: Better weight consolidation and less catastrophic forgetting. In International Conference on Pattern Recognition, 2018. 2
[50] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks by learning to mask weights. In European Conference on Computer Vision , 2018. 2
[51] Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In Conference on Computer Vision and Pattern Recognition , 2018. 2

--- TRANG 10 ---
[52] Emanuele Marconato, Gianpaolo Bontempo, Stefano Teso, Elisa Ficarra, Simone Calderara, and Andrea Passerini. Catastrophic forgetting in continual concept bottleneck models. In Image Analysis and Processing. ICIAP 2022 Workshops: ICIAP International Workshops, Lecce, Italy, May 23–27, 2022, Revised Selected Papers, Part II , pages 539–547. Springer, 2022. 1
[53] Diego Marcos, Sylvain Lobry, and Devis Tuia. Semantically interpretable activation maps: what-where-how explanations within cnns. In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW) , pages 4207–4215. IEEE, 2019. 2
[54] Marc Masana, Xialei Liu, Bartlomiej Twardowski, Mikel Menta, Andrew D Bagdanov, and Joost van de Weijer. Class-incremental learning: Survey and performance evaluation on image classification. IEEE Transactions on Pattern Analysis and Machine Intelligence , pages 1–20, 2022. 2, 5
[55] Yao Ming, Panpan Xu, Huamin Qu, and Liu Ren. Interpretable and steerable sequence learning via prototypes. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pages 903–913, 2019. 3
[56] Ramaravind K Mothilal, Amit Sharma, and Chenhao Tan. Explaining machine learning classifiers through diverse counterfactual explanations. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency , pages 607–617, 2020. 2
[57] Martin Mundt, Yongwon Hong, Iuliia Pliushch, and Visvanathan Ramesh. A wholistic view of continual learning with deep neural networks: Forgotten lessons and the bridge to active and open world learning. Neural Networks , 160:306–336, 2023. 2
[58] Martin Mundt, Iuliia Pliushch, Sagnik Majumder, Yongwon Hong, and Visvanathan Ramesh. Unified probabilistic deep continual learning through generative replay and open set recognition. Journal of Imaging , 8(4):93, 2022. 2
[59] Meike Nauta et al. Neural prototype trees for interpretable fine-grained image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 14933–14943, 2021. 3
[60] Meike Nauta, Annemarie Jutte, Jesper Provoost, and Christin Seifert. This looks like that, because... explaining prototypes for interpretable image recognition. In Machine Learning and Principles and Practice of Knowledge Discovery in Databases: International Workshops of ECML PKDD 2021, Virtual Event, September 13-17, 2021, Proceedings, Part I , pages 441–456. Springer, 2022. 8
[61] Meike Nauta, Jörg Schlötterer, Maurice van Keulen, and Christin Seifert. Pip-net: Patch-based intuitive prototypes for interpretable image classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2744–2753, 2023. 8
[62] Yulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, Xian-Sheng Hua, and Ji-Rong Wen. Counterfactual vqa: A cause-effect look at language bias. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 12700–12710, 2021. 2
[63] Schramowski P. et al. Making deep neural networks right for the right scientific reasons by interacting with their explanations. Nature Machine Intelligence , 2(8):476–486, 2020. 1
[64] Arijit Patra and J Alison Noble. Incremental learning of fetal heart anatomies using interpretable saliency maps. In Medical Image Understanding and Analysis: 23rd Conference, MIUA 2019, Liverpool, UK, July 24–26, 2019, Proceedings 23, pages 129–141. Springer, 2020. 1
[65] Grégoire Petit, Adrian Popescu, Hugo Schindler, David Picard, and Bertrand Delezoide. Fetril: Feature translation for exemplar-free class-incremental learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , pages 3911–3920, 2023. 6
[66] Ameya Prabhu, Philip HS Torr, and Puneet K Dokania. Gdumb: A simple approach that questions our progress in continual learning. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16 , pages 524–540. Springer, 2020. 13
[67] Sylvestre-Alvise Rebuffi, Ruth Fong, Xu Ji, and Andrea Vedaldi. There and back again: Revisiting backpropagation saliency methods. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8839–8848, 2020. 2
[68] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In Conference on Computer Vision and Pattern Recognition , 2017. 2
[69] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. " why should i trust you?" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining , pages 1135–1144, 2016. 2
[70] Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence , 1(5):206–215, 2019. 1, 2
[71] Dawid Rymarczyk, Daniel Dobrowolski, and Tomasz Danel. Progrest: Prototypical graph regression soft trees for molecular property prediction. SIAM International Conference on Data Mining , 2023. 3
[72] Dawid Rymarczyk, Aneta Kaczyńska, Jarosław Kraus, Adam Pardyl, and Bartosz Zieliński. Protomil: Multiple instance learning with prototypical parts for fine-grained interpretability. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases . Springer, 2022. 3
[73] Dawid Rymarczyk, Łukasz Struski, Michał Górzczak, Koryna Lewandowska, Jacek Tabor, and Bartosz Zieliński. Interpretable image classification with differentiable prototypes assignment. In Proceedings of the European Conference on Computer Vision (ECCV) , 2022. 3, 8
[74] Dawid Rymarczyk, Łukasz Struski, Jacek Tabor, and Bartosz Zieliński. Protopshare: Prototypical parts sharing for similarity discovery in interpretable image classification. In Proceedings of the 27th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pages 1420–1430, 2021. 3

--- TRANG 11 ---
[75] Mikołaj Sacha, Dawid Rymarczyk, Łukasz Struski, Jacek Tabor, and Bartosz Zieliński. Protoseg: Interpretable semantic segmentation with prototypical parts. In Winter Conference on Applications of Computer Vision (WACV) , 2023. 3, 6
[76] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision , pages 618–626, 2017. 2
[77] Ramprasaath R Selvaraju, Stefan Lee, Yilin Shen, Hongxia Jin, Shalini Ghosh, Larry Heck, Dhruv Batra, and Devi Parikh. Taking a hint: Leveraging explanations to make vision and language models more grounded. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2591–2600, 2019. 2
[78] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In International Conference on Machine Learning , 2018. 2
[79] Yujun Shi, Li Yuan, Yunpeng Chen, and Jiashi Feng. Continual learning via bit-level information preserving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 16674–16683, 2021. 2
[80] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. In In Workshop at International Conference on Learning Representations . Citeseer, 2014. 2
[81] Gurmail Singh and Kin-Choong Yow. These do not look like those: An interpretable deep learning model for image recognition. IEEE Access , 9:41482–41493, 2021. 3
[82] Shixiang Tang, Dapeng Chen, Jinguo Zhu, Shijie Yu, and Wanli Ouyang. Layerwise optimization by gradient decomposition for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9634–9643, 2021. 2
[83] Gido M van de Ven and Andreas S Tolias. Three scenarios for continual learning. In NeurIPS Continual Learning Workshop , 2018. 2
[84] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011. 2, 5
[85] Jiaqi Wang et al. Interpretable image recognition by constructing transparent embedding space. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 895–904, 2021. 3, 7, 8
[86] Liyuan Wang, Kuo Yang, Chongxuan Li, Lanqing Hong, Zhenguo Li, and Jun Zhu. Ordisco: Effective and efficient usage of incremental unlabeled data for semi-supervised continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5383–5392, 2021. 2
[87] Pei Wang and Nuno Vasconcelos. Scout: Self-aware discriminant counterfactual explanations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8981–8990, 2020. 2
[88] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu. Large scale incremental learning. In Conference on Computer Vision and Pattern Recognition , 2019. 2
[89] Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynamically expandable representation for class incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 3014–3023, 2021. 2
[90] Chih-Kuan Yeh, Been Kim, Sercan Arik, Chun-Liang Li, Tomas Pfister, and Pradeep Ravikumar. On completeness-aware concept-based explanations in deep neural networks. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems , volume 33, pages 20554–20565. Curran Associates, Inc., 2020. 2
[91] Lu Yu, Bartlomiej Twardowski, Xialei Liu, Luis Herranz, Kai Wang, Yongmei Cheng, Shangling Jui, and Joost van de Weijer. Semantic drift compensation for class-incremental learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 6982–6991, 2020. 5
[92] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In International Conference on Machine Learning , 2017. 2
[93] Mengyao Zhai, Lei Chen, and Greg Mori. Hyper-lifelonggan: Scalable lifelong learning for image conditioned generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2246–2255, 2021. 2
[94] Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and Cheekong Lee. Protgnn: Towards self-explaining graph neural networks. 2022. 3
[95] Fei Zhu, Xu-Yao Zhang, Chuang Wang, Fei Yin, and Cheng-Lin Liu. Prototype augmentation and self-supervision for incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5871–5880, 2021. 5, 6

--- TRANG 12 ---
Chi tiết thiết lập thí nghiệm bổ sung
Ở đây chúng tôi trình bày chi tiết bổ sung về thiết lập thí nghiệm. Chúng tôi thực hiện tìm kiếm siêu tham số cho λdist (λdist ∈ {10.0,5.0,0.0,0.5,0.1,0.05,0.01,0.005,0.001,0.0005, 0.0001}). Chúng tôi sử dụng bộ tối ưu Adam với tốc độ học 0.001 và các tham số β1= 0.9 và β2= 0.999. Chúng tôi đặt kích thước batch là 75 và sử dụng hình ảnh đầu vào có độ phân giải 224×224×3. Trọng số của mạng được khởi tạo với bộ khởi tạo bình thường Xavier.

Chúng tôi thực hiện huấn luyện khởi động nơi trọng số của f được đóng băng trong 10 epochs, và sau đó chúng tôi huấn luyện mô hình cho đến khi hội tụ với 12 epochs dừng sớm. Chúng tôi sử dụng lược đồ học được trình bày trong Bảng 7. Tùy thuộc vào số lượng nhiệm vụ, chúng tôi thực hiện huấn luyện khởi động với {5,5,4} epochs và giai đoạn huấn luyện kết hợp cho {21,15,11}, dài hơn với ít nhiệm vụ hơn. Tương tự, chúng tôi thực hiện chiếu nguyên mẫu mỗi {10,7,5} epoch. Vì vậy với nhiều nhiệm vụ hơn, chúng tôi thực hiện ít epochs huấn luyện hơn (Bảng 7).

Kết quả trên Stanford Cars
Bảng 8 mô tả cách phương pháp ICICLE hoạt động trên tập dữ liệu Stanford Cars so với các phương pháp cơ sở khác. Kết quả nhất quán với những kết quả trên CUB-200-2011 và cho thấy ICICLE vượt trội so với tất cả các phương pháp học CL tiêu chuẩn được thích ứng cho kiến trúc ProtoPNet.

So sánh với GDumb
Ngoài ra, chúng tôi so sánh phương pháp của chúng tôi với GDumb [66], một phương pháp cơ sở trong học CL, trong các tình huống liên quan đến 3, 5, và 10 hình ảnh trên mỗi lớp với học 4 nhiệm vụ đạt 20.3, 34.2, 57.6, và 13.0, 26.7, 48.8 cho biết nhiệm vụ và không biết nhiệm vụ tương ứng. ICICLE vượt trội so với GDumb với số lượng ví dụ nhỏ, và biết nhiệm vụ cho GDumb-10 là ngoại lệ duy nhất nơi GDumb đạt điểm độ chính xác cao hơn.

Khởi tạo xa
Trong Bảng 5, chúng tôi cho thấy rằng khởi tạo dựa trên độ gần là có lợi nhất. Tuy nhiên, ở đây trong Hình 10, chúng tôi cho thấy cách khởi tạo các phần nguyên mẫu ở khoảng cách từ những phần đã tồn tại một lần tạo ra các khái niệm quá tổng quát hoặc mang thông tin về nền.

Bù trừ thiên lệch gần đây của nhiệm vụ
Ở đây, trong Bảng 9 chúng tôi cho thấy ảnh hưởng của bù trừ thiên lệch gần đây của nhiệm vụ đến độ chính xác từng nhiệm vụ cho tình huống không biết nhiệm vụ. Sau bù trừ, độ chính xác trên nhiệm vụ 1 tăng nhiều nhất, nhưng cùng lúc độ chính xác trên tất cả các nhiệm vụ khác bị hy sinh. Tuy nhiên, độ chính xác trung bình sau bù trừ được tăng lên.

Hình 10: Hình ảnh mô tả chiếu TSNE 2D của các nguyên mẫu. Người ta có thể quan sát rằng có một cụm các nguyên mẫu từ tất cả các nhiệm vụ ngoại trừ nhiệm vụ đầu tiên (hộp vàng). Điều này có thể quan sát được với khởi tạo xa và hình ảnh nguyên mẫu cho thấy rằng những phần nguyên mẫu đó đang đại diện cho nền hoặc các khái niệm mơ hồ.

Kết quả chi tiết sau khi học từng nhiệm vụ
Trong Bảng 10, và Bảng 11 chúng tôi cho thấy độ chính xác chi tiết của ICICLE sau khi học từng nhiệm vụ cho CUB-200-2011 trong một lần chạy duy nhất với cùng seed trong các tình huống học bốn và mười nhiệm vụ.

Phân tích các siêu tham số cho các cơ sở
Trong Bảng 12, Bảng 13, và Bảng 14 chúng tôi cho thấy ảnh hưởng của các siêu tham số cho từng phương pháp cơ sở, EWC, LWF, và LWM, tương ứng. Dựa trên đó, các tham số của những phương pháp này được chọn để so sánh với ICICLE.

--- TRANG 13 ---
Giai đoạn Lớp mô hình Tốc độ học Bộ lập lịch Suy giảm trọng số Thời lượng
Khởi động add-on tích chập 1×1 1·10−3
Không có Không có 5,5,4 epochs lớp nguyên mẫu 1·10−3
Kết hợp tích chập f 1·10−4
giảm một nửa mỗi 5 epochs 10−4 21,15,10 epochs dừng sớm add-on tích chập 1×1 1·10−3
lớp nguyên mẫu 1·10−3

Bảng 7: Lược đồ học cho phương pháp ICICLE.

ĐỘ CHÍNH XÁC TRUNG BÌNH TĂNG DẦN BIẾT NHIỆM VỤ ĐỘ CHÍNH XÁC TRUNG BÌNH TĂNG DẦN KHÔNG BIẾT NHIỆM VỤ
PHƯƠNG PHÁP 4NHIỆM VỤ 7NHIỆM VỤ 14NHIỆM VỤ 4NHIỆM VỤ 7NHIỆM VỤ 14NHIỆM VỤ
FREEZING 0.572±0.031 0.518±0.041 0.486±0.026 0.309±0.012 0.155±0.031 0.092±0.014
FINETUNING 0.216±0.009 0.167±0.011 0.149±0.012 0.182±0.006 0.124±0.013 0.057±0.001
EWC 0.456±0.021 0.315±0.037 0.287±0.041 0.258±0.019 0.152±0.022 0.011±0.009
LWM 0.459±0.072 0.416±0.048 0.305±0.022 0.233±0.026 0.171±0.016 0.080±0.008
LWF 0.375±0.021 0.356±0.024 0.250±0.020 0.230±0.011 0.171±0.005 0.092±0.008
ICICLE 0.654±0.014 0.645±0.003 0.583±0.048 0.335±0.005 0.203±0.010 0.116±0.018

Bảng 8: So sánh độ chính xác trung bình tăng dần cho số lượng nhiệm vụ khác nhau trên Stanford Cars, chứng minh tác động tiêu cực của số lượng nhiệm vụ cao cần được học đến hiệu suất của mô hình. Bất chấp xu hướng này, ICICLE vượt trội so với các phương pháp cơ sở trên tất cả số lượng nhiệm vụ.

NHIỆM VỤ 1 NHIỆM VỤ 2 NHIỆM VỤ 3 NHIỆM VỤ 4 TRUNG BÌNH
BIẾT NHIỆM VỤ 0.514 0.717 0.725 0.698 0.663
TRƯỚC 0.028 0.301 0.434 0.575 0.335
SAU 0.233 0.365 0.314 0.486 0.350

Bảng 9: Ảnh hưởng bù trừ thiên lệch gần đây của nhiệm vụ trong một lần chạy. Kết quả cho thấy phương pháp bù trừ của chúng tôi cân bằng hơn kết quả cho từng nhiệm vụ trong tình huống không biết nhiệm vụ.

--- TRANG 14 ---
ĐỘ CHÍNH XÁC BIẾT NHIỆM VỤ ĐỘ CHÍNH XÁC KHÔNG BIẾT NHIỆM VỤ
Sau NHIỆM VỤ 1 NHIỆM VỤ 2 NHIỆM VỤ 3 NHIỆM VỤ 4 TRUNG BÌNH NHIỆM VỤ 1 NHIỆM VỤ 2 NHIỆM VỤ 3 NHIỆM VỤ 4 TRUNG BÌNH
NHIỆM VỤ 1 0.806 NA NA NA 0.806 0.806 NA NA NA 0.806
NHIỆM VỤ 2 0.740 0.759 NA NA 0.750 0.089 0.747 NA NA 0.418
NHIỆM VỤ 3 0.622 0.736 0.759 NA 0.706 0.033 0.633 0.549 NA 0.404
NHIỆM VỤ 4 0.514 0.717 0.725 0.698 0.663 0.028 0.484 0.378 0.505 0.349

Bảng 10: Kết quả của phương pháp ICICLE trước bù trừ thiên lệch gần đây của nhiệm vụ cho tình huống học bốn nhiệm vụ sau mỗi tập huấn luyện.

ĐỘ CHÍNH XÁC BIẾT NHIỆM VỤ
Sau NHIỆM VỤ 1 NHIỆM VỤ 2 NHIỆM VỤ 3 NHIỆM VỤ 4 NHIỆM VỤ 5 NHIỆM VỤ 6 NHIỆM VỤ 7 NHIỆM VỤ 8 NHIỆM VỤ 9 NHIỆM VỤ 10 TRUNG BÌNH
NHIỆM VỤ 1 0.920 NA NA NA NA NA NA NA NA NA 0.920
NHIỆM VỤ 2 0.666 0.869 NA NA NA NA NA NA NA NA 0.767
NHIỆM VỤ 3 0.462 0.818 0.858 NA NA NA NA NA NA NA 0.713
NHIỆM VỤ 4 0.420 0.751 0.774 0.774 NA NA NA NA NA NA 0.680
NHIỆM VỤ 5 0.314 0.625 0.680 0.672 0.784 NA NA NA NA NA 0.615
NHIỆM VỤ 6 0.268 0.538 0.627 0.617 0.760 0.747 NA NA NA NA 0.593
NHIỆM VỤ 7 0.265 0.476 0.584 0.617 0.713 0.706 0.769 NA NA NA 0.590
NHIỆM VỤ 8 0.258 0.413 0.551 0.555 0.667 0.634 0.741 0.764 NA NA 0.573
NHIỆM VỤ 9 0.253 0.398 0.494 0.492 0.598 0.587 0.701 0.745 0.852 NA 0.569
NHIỆM VỤ 10 0.244 0.371 0.462 0.441 0.573 0.560 0.667 0.729 0.816 0.803 0.567

ĐỘ CHÍNH XÁC KHÔNG BIẾT NHIỆM VỤ
NHIỆM VỤ 1 NHIỆM VỤ 2 NHIỆM VỤ 3 NHIỆM VỤ 4 NHIỆM VỤ 5 NHIỆM VỤ 6 NHIỆM VỤ 7 NHIỆM VỤ 8 NHIỆM VỤ 9 NHIỆM VỤ 10 TRUNG BÌNH
NHIỆM VỤ 1 0.920 NA NA NA NA NA NA NA NA NA 0.920
NHIỆM VỤ 2 0.010 0.869 NA NA NA NA NA NA NA NA 0.439
NHIỆM VỤ 3 0.0 0.060 0.854 NA NA NA NA NA NA NA 0.305
NHIỆM VỤ 4 0.0 0.0 0.339 0.751 NA NA NA NA NA NA 0.273
NHIỆM VỤ 5 0.0 0.0 0.030 0.323 0.746 NA NA NA NA NA 0.220
NHIỆM VỤ 6 0.0 0.0 0.004 0.090 0.451 0.684 NA NA NA NA 0.205
NHIỆM VỤ 7 0.0 0.0 0.0 0.020 0.193 0.432 0.712 NA NA NA 0.194
NHIỆM VỤ 8 0.0 0.0 0.0 0.020 0.073 0.233 0.497 0.643 NA NA 0.181
NHIỆM VỤ 9 0.0 0.0 0.0 0.0 0.035 0.138 0.338 0.435 0.676 NA 0.180
NHIỆM VỤ 10 0.0 0.0 0.0 0.0 0.016 0.070 0.214 0.285 0.484 0.643 0.171

Bảng 11: Kết quả của phương pháp ICICLE trước bù trừ thiên lệch gần đây của nhiệm vụ cho tình huống học mười nhiệm vụ sau mỗi tập huấn luyện.

ĐỘ CHÍNH XÁC BIẾT NHIỆM VỤ ĐỘ CHÍNH XÁC KHÔNG BIẾT NHIỆM VỤ
α 0.01 0.1 1.0 5.0 10.0 0.01 0.1 1.0 5.0 10.0
0.185 0.329 0.441 0.197 0.167 0.170 0.185 0.213 0.168 0.144

Bảng 12: Ảnh hưởng của tham số alpha trong EWC đến độ chính xác của kiến trúc ProtoPNet trong tình huống học bốn nhiệm vụ.

ĐỘ CHÍNH XÁC BIẾT NHIỆM VỤ ĐỘ CHÍNH XÁC KHÔNG BIẾT NHIỆM VỤ
γ 0.001 0.01 0.1 1.0 10.0 0.001 0.01 0.1 1.0 10.0
0.240 0.240 0.431 0.355 0.231 0.209 0.209 0.212 0.209 0.209

Bảng 13: Ảnh hưởng của tham số γ trong LWM đến độ chính xác của kiến trúc ProtoPNet trong tình huống học bốn nhiệm vụ.

ĐỘ CHÍNH XÁC BIẾT NHIỆM VỤ ĐỘ CHÍNH XÁC KHÔNG BIẾT NHIỆM VỤ
λ 0.001 0.01 0.1 1.0 10.0 0.001 0.01 0.1 1.0 10.0
0.232 0.232 0.238 0.359 0.249 0.207 0.210 0.210 0.231 0.221

Bảng 14: Ảnh hưởng của tham số λ trong LWF đến độ chính xác của kiến trúc ProtoPNet trong tình huống học bốn nhiệm vụ.