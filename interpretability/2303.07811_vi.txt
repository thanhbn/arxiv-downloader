# 2303.07811.pdf
# ÄÃ£ chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/interpretability/2303.07811.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 2047573 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================


--- TRANG 1 ---
ICICLE: Há»c LiÃªn Tá»¥c TÄƒng Dáº§n Lá»›p CÃ³ Thá»ƒ Diá»…n Giáº£i
Dawid Rymarczyk1,2,3,âˆ—Joost van de Weijer4,5Bartosz Zieli Â´nski1,3,6
BartÅ‚omiej Twardowski4,5,6
1Khoa ToÃ¡n há»c vÃ  Khoa há»c MÃ¡y tÃ­nh, Äáº¡i há»c Jagiellonian
2TrÆ°á»ng Tiáº¿n sÄ© Khoa há»c ChÃ­nh xÃ¡c vÃ  Sá»± sá»‘ng, Äáº¡i há»c Jagiellonian3Ardigen SA
4Äáº¡i há»c Tá»± trá»‹ Barcelona5Trung tÃ¢m Thá»‹ giÃ¡c MÃ¡y tÃ­nh6IDEAS NCBR
âˆ—dawid.rymarczyk@doctoral.uj.edu.pl
TÃ³m táº¯t
Há»c liÃªn tá»¥c cho phÃ©p há»c tÄƒng dáº§n cÃ¡c nhiá»‡m vá»¥ má»›i mÃ  khÃ´ng quÃªn nhá»¯ng nhiá»‡m vá»¥ Ä‘Ã£ há»c trÆ°á»›c Ä‘Ã³, dáº«n Ä‘áº¿n chuyá»ƒn giao kiáº¿n thá»©c tÃ­ch cá»±c cÃ³ thá»ƒ nÃ¢ng cao hiá»‡u suáº¥t trÃªn cáº£ nhiá»‡m vá»¥ má»›i vÃ  cÅ©. Tuy nhiÃªn, há»c liÃªn tá»¥c Ä‘áº·t ra nhá»¯ng thÃ¡ch thá»©c má»›i cho kháº£ nÄƒng diá»…n giáº£i, vÃ¬ cÄƒn cá»© Ä‘áº±ng sau dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh cÃ³ thá»ƒ thay Ä‘á»•i theo thá»i gian, dáº«n Ä‘áº¿n sá»± trÃ´i dáº¡t khÃ¡i niá»‡m diá»…n giáº£i. ChÃºng tÃ´i giáº£i quyáº¿t váº¥n Ä‘á» nÃ y báº±ng cÃ¡ch Ä‘á» xuáº¥t Há»c TÄƒng Dáº§n Lá»›p CÃ³ Thá»ƒ Diá»…n Giáº£i (ICICLE), má»™t phÆ°Æ¡ng phÃ¡p khÃ´ng cáº§n máº«u Ä‘iá»ƒn hÃ¬nh Ã¡p dá»¥ng cÃ¡ch tiáº¿p cáº­n dá»±a trÃªn pháº§n nguyÃªn máº«u. NÃ³ bao gá»“m ba tÃ­nh má»›i quan trá»ng: Ä‘iá»u chuáº©n kháº£ nÄƒng diá»…n giáº£i giÃºp chÆ°ng cáº¥t cÃ¡c khÃ¡i niá»‡m Ä‘Ã£ há»c trÆ°á»›c Ä‘Ã³ trong khi báº£o tá»“n lÃ½ luáº­n tÃ­ch cá»±c thÃ¢n thiá»‡n vá»›i ngÆ°á»i dÃ¹ng; chiáº¿n lÆ°á»£c khá»Ÿi táº¡o nguyÃªn máº«u dá»±a trÃªn Ä‘á»™ gáº§n dÃ nh riÃªng cho thiáº¿t láº­p phÃ¢n loáº¡i chi tiáº¿t; vÃ  bÃ¹ trá»« thiÃªn lá»‡ch gáº§n Ä‘Ã¢y cá»§a nhiá»‡m vá»¥ dÃ nh cho cÃ¡c pháº§n nguyÃªn máº«u. Káº¿t quáº£ thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i cho tháº¥y ICICLE giáº£m sá»± trÃ´i dáº¡t khÃ¡i niá»‡m diá»…n giáº£i vÃ  vÆ°á»£t trá»™i so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p há»c tÄƒng dáº§n lá»›p khÃ´ng cáº§n máº«u hiá»‡n cÃ³ khi Ä‘Æ°á»£c Ã¡p dá»¥ng cho cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn khÃ¡i niá»‡m.

1. Giá»›i thiá»‡u
Vá»›i viá»‡c sá»­ dá»¥ng ngÃ y cÃ ng nhiá»u cÃ¡c mÃ´ hÃ¬nh há»c sÃ¢u trong cÃ¡c lÄ©nh vá»±c Ä‘a dáº¡ng, bao gá»“m robot há»c [10], hÃ¬nh áº£nh y táº¿ [17], vÃ  lÃ¡i xe tá»± Ä‘á»™ng [43], cÃ³ nhu cáº§u cáº¥p thiáº¿t phÃ¡t triá»ƒn cÃ¡c mÃ´ hÃ¬nh cÃ³ thá»ƒ thÃ­ch á»©ng vá»›i Ä‘iá»u kiá»‡n luÃ´n thay Ä‘á»•i vÃ  há»c cÃ¡c nhiá»‡m vá»¥ má»›i tá»« dá»¯ liá»‡u khÃ´ng dá»«ng. Tuy nhiÃªn, má»™t thÃ¡ch thá»©c Ä‘Ã¡ng ká»ƒ vá»›i máº¡ng nÆ¡-ron lÃ  xu hÆ°á»›ng bá»‹ quÃªn tháº£m khá»‘c [26, 30, 44], nÆ¡i hiá»‡u suáº¥t trÃªn cÃ¡c nhiá»‡m vá»¥ trÆ°á»›c Ä‘Ã³ giáº£m nhanh chÃ³ng khi nhá»¯ng nhiá»‡m vá»¥ má»›i Ä‘Æ°á»£c tiáº¿p thu. Há»c LiÃªn tá»¥c (CL) [19] Ä‘Ã£ ná»•i lÃªn nhÆ° má»™t ká»¹ thuáº­t Ä‘áº§y há»©a háº¹n Ä‘á»ƒ giáº£i quyáº¿t thÃ¡ch thá»©c nÃ y báº±ng cÃ¡ch cho phÃ©p cÃ¡c mÃ´ hÃ¬nh há»c nhá»¯ng nhiá»‡m vá»¥ má»›i mÃ  khÃ´ng quÃªn nhá»¯ng nhiá»‡m vá»¥ Ä‘Ã£ há»c trÆ°á»›c Ä‘Ã³.

Trong khi cÃ¡c phÆ°Æ¡ng phÃ¡p CL hiá»‡n táº¡i giáº£m Ä‘Ã¡ng ká»ƒ quÃªn tháº£m khá»‘c, chÃºng thÆ°á»ng khÃ³ hiá»ƒu Ä‘á»‘i vá»›i con ngÆ°á»i. Äiá»u nÃ y Ä‘áº·c biá»‡t cÃ³ váº¥n Ä‘á» vÃ¬ máº¡ng nÆ¡-ron sÃ¢u 

Task 4 Task 3 Task 2 
Fine-tuning LWF ICICLE EWC LWM Input image Task 1 
HÃ¬nh 1: ChÃºng tÃ´i xá»­ lÃ½ hÃ¬nh áº£nh Ä‘áº§u vÃ o (trÃªn trÃ¡i) thÃ´ng qua máº¡ng vÃ  hiá»‡n thá»‹ cÃ¡ch cÃ¡c khu vá»±c cá»¥ thá»ƒ cá»§a nÃ³ tÆ°Æ¡ng tá»± vá»›i má»™t trong cÃ¡c nguyÃªn máº«u. Sá»± trÃ´i dáº¡t khÃ¡i niá»‡m diá»…n giáº£i xáº£y ra khi báº£n Ä‘á»“ tÆ°Æ¡ng tá»± nhÆ° váº­y khÃ¡c nhau giá»¯a cÃ¡c nhiá»‡m vá»¥. ICICLE hoáº¡t Ä‘á»™ng tá»‘t nháº¥t, báº£o tá»“n báº£n Ä‘á»“ tÆ°Æ¡ng tá»± tá»‘t hÆ¡n so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p há»c liÃªn tá»¥c khÃ¡c.

thÆ°á»ng dá»± Ä‘oÃ¡n cÃ¢u tráº£ lá»i Ä‘Ãºng vÃ¬ lÃ½ do sai (hiá»‡n tÆ°á»£ng "Clever Hans"), dáº«n Ä‘áº¿n hiá»‡u suáº¥t xuáº¥t sáº¯c trong huáº¥n luyá»‡n nhÆ°ng hiá»‡u suáº¥t kÃ©m trong thá»±c táº¿ [63]. Äiá»u nÃ y dáº«n Ä‘áº¿n cÃ¡c váº¥n Ä‘á» xÃ£ há»™i nghiÃªm trá»ng áº£nh hÆ°á»Ÿng sÃ¢u sáº¯c Ä‘áº¿n sá»©c khá»e, tá»± do, thiÃªn lá»‡ch chá»§ng tá»™c, vÃ  an toÃ n [11]. Káº¿t quáº£ lÃ , má»™t sá»‘ bÆ°á»›c Ä‘áº§u tiÃªn Ä‘Ã£ Ä‘Æ°á»£c thá»±c hiá»‡n trong tÃ i liá»‡u Ä‘á»ƒ giá»›i thiá»‡u cÃ¡c phÆ°Æ¡ng phÃ¡p cÃ³ thá»ƒ giáº£i thÃ­ch háº­u há»‘c vÃ o thiáº¿t láº­p CL [32, 52, 64].

Tuy nhiÃªn, viá»‡c giáº£i thÃ­ch cÃ¡c há»™p Ä‘en, thay vÃ¬ thay tháº¿ chÃºng báº±ng cÃ¡c mÃ´ hÃ¬nh cÃ³ thá»ƒ diá»…n giáº£i (tá»± giáº£i thÃ­ch), cÃ³ thá»ƒ lÃ m tráº§m trá»ng thÃªm váº¥n Ä‘á» báº±ng cÃ¡ch cung cáº¥p cÃ¡c Ä‘áº·c tÃ­nh sai lá»‡ch hoáº·c sai [70] hoáº·c thÃªm quyá»n lá»±c khÃ´ng cáº§n thiáº¿t vÃ o há»™p Ä‘en [12]. Do Ä‘Ã³, cÃ³ nhu cáº§u rÃµ rÃ ng vá» cÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y sÃ¡ng táº¡o vá»‘n cÃ³ thá»ƒ diá»…n giáº£i [11]. Theo hiá»ƒu biáº¿t tá»‘t nháº¥t cá»§a chÃºng tÃ´i, chÆ°a cÃ³ phÆ°Æ¡ng phÃ¡p CL cÃ³ thá»ƒ diá»…n giáº£i nÃ o Ä‘Æ°á»£c Ä‘á» xuáº¥t cho Ä‘áº¿n nay.

Trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i giá»›i thiá»‡u Há»c TÄƒng Dáº§n Lá»›p CÃ³ Thá»ƒ Diá»…n Giáº£i arXiv:2303.07811v2  [cs.LG]  31 Jul 2023

--- TRANG 2 ---
(ICICLE), má»™t phÆ°Æ¡ng phÃ¡p cÃ³ thá»ƒ diá»…n giáº£i cho há»c tÄƒng dáº§n lá»›p dá»±a trÃªn phÆ°Æ¡ng phÃ¡p luáº­n pháº§n nguyÃªn máº«u. TÆ°Æ¡ng tá»± nhÆ° lÃ½ luáº­n "CÃ¡i nÃ y trÃ´ng giá»‘ng cÃ¡i kia" [16], ICICLE há»c má»™t táº­p há»£p cÃ¡c pháº§n nguyÃªn máº«u Ä‘áº¡i diá»‡n cho cÃ¡c khÃ¡i niá»‡m tham chiáº¿u cÃ³ nguá»“n gá»‘c tá»« dá»¯ liá»‡u huáº¥n luyá»‡n vÃ  Ä‘Æ°a ra dá»± Ä‘oÃ¡n báº±ng cÃ¡ch so sÃ¡nh cÃ¡c pháº§n hÃ¬nh áº£nh Ä‘áº§u vÃ o vá»›i cÃ¡c nguyÃªn máº«u Ä‘Ã£ há»c. Tuy nhiÃªn, viá»‡c chuyá»ƒn giao kiáº¿n thá»©c giá»¯a cÃ¡c nhiá»‡m vá»¥ trong há»c liÃªn tá»¥c Ä‘áº·t ra nhá»¯ng thÃ¡ch thá»©c má»›i cho kháº£ nÄƒng diá»…n giáº£i. Chá»§ yáº¿u vÃ¬ cÄƒn cá»© Ä‘áº±ng sau dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh cÃ³ thá»ƒ thay Ä‘á»•i theo thá»i gian, dáº«n Ä‘áº¿n sá»± trÃ´i dáº¡t khÃ¡i niá»‡m diá»…n giáº£i vÃ  lÃ m cho cÃ¡c giáº£i thÃ­ch khÃ´ng nháº¥t quÃ¡n (xem HÃ¬nh 1 vÃ  Báº£ng 1). Do Ä‘Ã³, ICICLE chá»©a nhiá»u cÆ¡ cháº¿ Ä‘á»ƒ ngÄƒn cháº·n sá»± trÃ´i dáº¡t nÃ y vÃ , cÃ¹ng lÃºc, Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ thá»a Ä‘Ã¡ng. Äáº§u tiÃªn, chÃºng tÃ´i Ä‘á» xuáº¥t Ä‘iá»u chuáº©n kháº£ nÄƒng diá»…n giáº£i phÃ¹ há»£p cho cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn pháº§n nguyÃªn máº«u Ä‘á»ƒ giá»¯ láº¡i kiáº¿n thá»©c Ä‘Ã£ cÃ³ Ä‘Æ°á»£c trÆ°á»›c Ä‘Ã³ trong khi duy trÃ¬ tÃ­nh dáº»o cá»§a mÃ´ hÃ¬nh. NÃ³ Ä‘áº£m báº£o ráº±ng cÃ¡c pháº§n nguyÃªn máº«u Ä‘Ã£ há»c trÆ°á»›c Ä‘Ã³ Ä‘Æ°á»£c kÃ­ch hoáº¡t tÆ°Æ¡ng tá»± trong dá»¯ liá»‡u nhiá»‡m vá»¥ hiá»‡n táº¡i, Ä‘iá»u nÃ y lÃ m cho cÃ¡c giáº£i thÃ­ch nháº¥t quÃ¡n theo thá»i gian. HÆ¡n ná»¯a, xem xÃ©t báº£n cháº¥t phÃ¢n loáº¡i chi tiáº¿t cá»§a cÃ¡c táº­p dá»¯ liá»‡u Ä‘Æ°á»£c xem xÃ©t, chÃºng tÃ´i giá»›i thiá»‡u khá»Ÿi táº¡o nguyÃªn máº«u dá»±a trÃªn Ä‘á»™ gáº§n cho má»™t nhiá»‡m vá»¥ má»›i. NÃ³ tÃ¬m kiáº¿m cÃ¡c khÃ¡i niá»‡m Ä‘áº¡i diá»‡n trong dá»¯ liá»‡u nhiá»‡m vá»¥ má»›i gáº§n vá»›i cÃ¡c khÃ¡i niá»‡m Ä‘Ã£ há»c trÆ°á»›c Ä‘Ã³, cho phÃ©p mÃ´ hÃ¬nh nháº­n ra cÃ¡c Ä‘áº·c trÆ°ng cáº¥p cao cá»§a nhiá»‡m vá»¥ má»›i vÃ  táº­p trung vÃ o viá»‡c Ä‘iá»u chá»‰nh chi tiáº¿t. Thá»© ba, Ä‘á»ƒ vÆ°á»£t qua thiÃªn lá»‡ch gáº§n Ä‘Ã¢y cá»§a nhiá»‡m vá»¥ trong cÃ¡c tÃ¬nh huá»‘ng há»c tÄƒng dáº§n lá»›p, chÃºng tÃ´i Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p Ä‘Æ¡n giáº£n nhÆ°ng hiá»‡u quáº£ cÃ¢n báº±ng cÃ¡c logits cá»§a táº¥t cáº£ nhiá»‡m vá»¥ dá»±a trÃªn dá»¯ liá»‡u nhiá»‡m vá»¥ cuá»‘i cÃ¹ng. Cuá»‘i cÃ¹ng, chÃºng tÃ´i giáº£m huáº¥n luyá»‡n Ä‘a giai Ä‘oáº¡n trong khi báº£o tá»“n lÃ½ luáº­n tÃ­ch cá»±c thÃ¢n thiá»‡n vá»›i ngÆ°á»i dÃ¹ng.

ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ ICICLE trÃªn hai táº­p dá»¯ liá»‡u, cá»¥ thá»ƒ lÃ  CUB-200-2011 [84] vÃ  Stanford Cars [46], vÃ  tiáº¿n hÃ nh loáº¡i bá» triá»‡t Ä‘á»ƒ Ä‘á»ƒ chá»©ng minh hiá»‡u quáº£ cá»§a phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i. ChÃºng tÃ´i cho tháº¥y ráº±ng váº¥n Ä‘á» nÃ y Ä‘áº§y thÃ¡ch thá»©c nhÆ°ng má»Ÿ ra má»™t lÄ©nh vá»±c nghiÃªn cá»©u má»›i Ä‘áº§y há»©a háº¹n cÃ³ thá»ƒ thÃºc Ä‘áº©y thÃªm sá»± hiá»ƒu biáº¿t cá»§a chÃºng ta vá» cÃ¡c phÆ°Æ¡ng phÃ¡p CL. CÃ¡c Ä‘Ã³ng gÃ³p cá»§a chÃºng tÃ´i cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ³m táº¯t nhÆ° sau:

â€¢ ChÃºng tÃ´i lÃ  nhá»¯ng ngÆ°á»i Ä‘áº§u tiÃªn giá»›i thiá»‡u há»c tÄƒng dáº§n lá»›p cÃ³ thá»ƒ diá»…n giáº£i vÃ  Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p má»›i ICICLE, dá»±a trÃªn phÆ°Æ¡ng phÃ¡p luáº­n pháº§n nguyÃªn máº«u.
â€¢ ChÃºng tÃ´i Ä‘á» xuáº¥t Ä‘iá»u chuáº©n kháº£ nÄƒng diá»…n giáº£i ngÄƒn cháº·n sá»± trÃ´i dáº¡t khÃ¡i niá»‡m diá»…n giáº£i mÃ  khÃ´ng sá»­ dá»¥ng máº«u Ä‘iá»ƒn hÃ¬nh.
â€¢ ChÃºng tÃ´i Ä‘á»‹nh nghÄ©a má»™t chiáº¿n lÆ°á»£c khá»Ÿi táº¡o nguyÃªn máº«u chuyÃªn dá»¥ng vÃ  má»™t phÆ°Æ¡ng phÃ¡p bÃ¹ trá»« thiÃªn lá»‡ch gáº§n Ä‘Ã¢y cá»§a nhiá»‡m vá»¥.

2. CÃ¡c CÃ´ng trÃ¬nh LiÃªn quan
Há»c LiÃªn tá»¥c vÃ  Há»c TÄƒng Dáº§n Lá»›p
CÃ¡c phÆ°Æ¡ng phÃ¡p há»c liÃªn tá»¥c hiá»‡n táº¡i cÃ³ thá»ƒ Ä‘Æ°á»£c phÃ¢n loáº¡i rá»™ng rÃ£i thÃ nh ba loáº¡i: cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn phÃ¡t láº¡i, dá»±a trÃªn kiáº¿n trÃºc, vÃ  dá»±a trÃªn Ä‘iá»u chuáº©n [19, 54]. CÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn phÃ¡t láº¡i hoáº·c lÆ°u má»™t lÆ°á»£ng nhá» dá»¯ liá»‡u tá»« cÃ¡c nhiá»‡m vá»¥ Ä‘Ã£ tháº¥y trÆ°á»›c Ä‘Ã³ [5, 15] hoáº·c táº¡o ra dá»¯ liá»‡u tá»•ng há»£p vá»›i má»™t mÃ´ hÃ¬nh sinh [86, 93]. Dá»¯ liá»‡u phÃ¡t láº¡i cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng IOU
PHÆ¯Æ NG PHÃP NHIá»†M Vá»¤ 1 NHIá»†M Vá»¤ 2 NHIá»†M Vá»¤ 3 TRUNG BÃŒNH
FINETUNING 0.115 0.149 0.260 0.151
EWC 0.192 0.481 0.467 0.334
LWF 0.221 0.193 0.077 0.188
LWM 0.332 0.312 0.322 0.325
ICICLE 0.705 0.753 0.742 0.728
Báº£ng 1: Káº¿t quáº£ Ä‘á»‹nh lÆ°á»£ng cho sá»± trÃ´i dáº¡t khÃ¡i niá»‡m diá»…n giáº£i Ä‘Æ°á»£c trÃ¬nh bÃ y trong HÃ¬nh 1. ChÃºng tÃ´i tÃ­nh IoU giá»¯a cÃ¡c Ä‘á»™ tÆ°Æ¡ng tá»± thu Ä‘Æ°á»£c sau má»—i nhiá»‡m vá»¥ vÃ  sau cÃ¡c nhiá»‡m vá»¥ tÄƒng dáº§n. VÃ­ dá»¥ trong cá»™t "nhiá»‡m vá»¥ 1", chÃºng tÃ´i tÃ­nh IoU giá»¯a báº£n Ä‘á»“ tÆ°Æ¡ng tá»± cá»§a cÃ¡c nguyÃªn máº«u nhiá»‡m vá»¥ má»™t sau má»—i táº­p huáº¥n luyá»‡n.

trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n cÃ¹ng vá»›i dá»¯ liá»‡u hiá»‡n táº¡i, nhÆ° trong iCaRL [68] vÃ  LUCIR [37], hoáº·c Ä‘á»ƒ háº¡n cháº¿ hÆ°á»›ng gradient trong khi huáº¥n luyá»‡n, nhÆ° trong AGEM [14]. CÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn kiáº¿n trÃºc kÃ­ch hoáº¡t cÃ¡c táº­p con khÃ¡c nhau cá»§a cÃ¡c tham sá»‘ máº¡ng cho cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau báº±ng cÃ¡ch cho phÃ©p cÃ¡c tham sá»‘ mÃ´ hÃ¬nh tÄƒng tuyáº¿n tÃ­nh vá»›i sá»‘ lÆ°á»£ng nhiá»‡m vá»¥. CÃ¡c cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã³ theo chiáº¿n lÆ°á»£c nÃ y bao gá»“m DER [89], Piggyback [50], PackNet [51]. CÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn Ä‘iá»u chuáº©n thÃªm má»™t thuáº­t ngá»¯ Ä‘iá»u chuáº©n bá»• sung cÃ³ nguá»“n gá»‘c tá»« kiáº¿n thá»©c cá»§a cÃ¡c nhiá»‡m vá»¥ trÆ°á»›c Ä‘Ã³ vÃ o máº¥t mÃ¡t huáº¥n luyá»‡n. Äiá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n báº±ng cÃ¡ch Ä‘iá»u chuáº©n khÃ´ng gian trá»ng sá»‘, Ä‘iá»u nÃ y háº¡n cháº¿ cÃ¡c tham sá»‘ quan trá»ng [79, 82], hoáº·c khÃ´ng gian chá»©c nÄƒng, Ä‘iá»u nÃ y háº¡n cháº¿ dá»± Ä‘oÃ¡n hoáº·c Ä‘áº·c trÆ°ng trung gian [23, 38]. EWC [44], MAS [3], REWC [49], SI [92], vÃ  RWalk [13] háº¡n cháº¿ táº§m quan trá»ng cá»§a cÃ¡c tham sá»‘ máº¡ng Ä‘á»ƒ ngÄƒn cháº·n quÃªn lÃ£ng. CÃ¡c phÆ°Æ¡ng phÃ¡p nhÆ° LWF [48], LWM [21], vÃ  BiC [88] táº­n dá»¥ng chÆ°ng cáº¥t kiáº¿n thá»©c Ä‘á»ƒ Ä‘iá»u chuáº©n Ä‘áº·c trÆ°ng hoáº·c dá»± Ä‘oÃ¡n. NgoÃ i ra, cÃ¡c thiáº¿t láº­p thÃ¡ch thá»©c hÆ¡n Ä‘Æ°á»£c xem xÃ©t trong lÄ©nh vá»±c nhÆ° há»c liÃªn tá»¥c cÃ³ thá»ƒ diá»…n giáº£i táº­p má»Ÿ [57]. Khi nÃ³i Ä‘áº¿n CL cÃ³ thá»ƒ diá»…n giáº£i, cÃ¡c phÆ°Æ¡ng phÃ¡p phÃ¡t láº¡i sinh [58] cung cáº¥p má»™t má»©c Ä‘á»™ rÃµ rÃ ng áº©n nháº¥t Ä‘á»‹nh. Tuy nhiÃªn, chÃºng yÃªu cáº§u má»™t bá»™ giáº£i mÃ£ (Ä‘á»ƒ hiá»ƒn thá»‹) vÃ  cÃ³ thá»ƒ tháº¥t báº¡i trong viá»‡c táº¡o ra cÃ¡c hÃ¬nh áº£nh nguyÃªn máº«u thá»±c táº¿ [16].

Há»c tÄƒng dáº§n lá»›p (class-IL) lÃ  tÃ¬nh huá»‘ng thÃ¡ch thá»©c nháº¥t trong Ä‘Ã³ bá»™ phÃ¢n loáº¡i há»c cÃ¡c lá»›p má»›i má»™t cÃ¡ch tuáº§n tá»±. MÃ´ hÃ¬nh cáº§n duy trÃ¬ hiá»‡u suáº¥t tá»‘t trÃªn táº¥t cáº£ cÃ¡c lá»›p Ä‘Ã£ tháº¥y cho Ä‘áº¿n nay [83]. Hai loáº¡i phÆ°Æ¡ng phÃ¡p Ä‘Ã¡nh giÃ¡ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a [54]: khÃ´ng biáº¿t nhiá»‡m vá»¥ (khÃ´ng cÃ³ quyá»n truy cáº­p vÃ o task-ID trong quÃ¡ trÃ¬nh suy luáº­n, vÃ­ dá»¥: BiC [88]) vÃ  biáº¿t nhiá»‡m vá»¥ (task-ID Ä‘Æ°á»£c cung cáº¥p trong quÃ¡ trÃ¬nh suy luáº­n, vÃ­ dá»¥: HAT [78]).

TrÃ­ tuá»‡ NhÃ¢n táº¡o CÃ³ thá»ƒ Giáº£i thÃ­ch Trong lÄ©nh vá»±c giáº£i thÃ­ch há»c sÃ¢u, hai loáº¡i mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c khÃ¡m phÃ¡: mÃ´ hÃ¬nh háº­u há»‘c vÃ  tá»± giáº£i thÃ­ch [70]. CÃ¡c mÃ´ hÃ¬nh háº­u há»‘c giáº£i thÃ­ch quÃ¡ trÃ¬nh lÃ½ luáº­n cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p há»™p Ä‘en, bao gá»“m báº£n Ä‘á»“ hiá»ƒn thá»‹ [53, 67, 76, 77, 80], vectÆ¡ kÃ­ch hoáº¡t khÃ¡i niá»‡m [18, 29, 40, 45, 90], vÃ­ dá»¥ pháº£n thá»±c táº¿ [1, 31, 56, 62, 87], vÃ  phÃ¢n tÃ­ch nhiá»…u loáº¡n hÃ¬nh áº£nh [7, 24, 25, 69]. Máº·t khÃ¡c, cÃ¡c mÃ´ hÃ¬nh tá»± giáº£i thÃ­ch nháº±m lÃ m cho quÃ¡ trÃ¬nh quyáº¿t Ä‘á»‹nh minh báº¡ch hÆ¡n vÃ  Ä‘Ã£ thu hÃºt sá»± chÃº Ã½ Ä‘Ã¡ng ká»ƒ [4, 9]. Gáº§n Ä‘Ã¢y, cÃ¡c nhÃ  nghiÃªn cá»©u Ä‘Ã£ táº­p trung vÃ o viá»‡c nÃ¢ng cao khÃ¡i niá»‡m

--- TRANG 3 ---
lá»›p tÃ­ch cháº­p 
 (backbone ğ‘“, lá»›p add-on ğ‘“A, vÃ  sigmoid) lá»›p pháº§n nguyÃªn máº«u ğ˜¨t -1(           ,                ) = 
gt-1(           ,                ) = gt-1(           ,                ) = 1.0
1.0
1.04.25 
5.72 
0.22 0.71 Zx
ğ‘“ ğ‘¥
gt(           ,                ) = 
gt(           ,                ) = 
gt(           ,                ) = 0.12 
0.78 1.01.0
1.0
1.00.39 
lá»›p cuá»‘i nhÃ£n lá»›p Wilson 
Warbler Prothonotary 
Warbler 
1.00.61 gt-1(           ,                ) = 1.00 Cardinal 1.47 
NHIá»†M Vá»¤ t 
0.49 gt(           ,                ) = 
0.90 Sooty 
Albatross ğ‘“A
NHIá»†M Vá»¤ t-1 nguyÃªn máº«u p
Ïƒ ht-1 
ht HÃ¬nh 2: Kiáº¿n trÃºc cá»§a ICICLE vá»›i cÃ¡c lá»›p pháº§n nguyÃªn máº«u riÃªng biá»‡t gt cho má»—i nhiá»‡m vá»¥ t. Trong vÃ­ dá»¥ nÃ y, cÃ¡c nguyÃªn máº«u cá»§a cÃ¡c lá»›p Prothonotary Warbler vÃ  Cardinal thuá»™c vá» nhiá»‡m vá»¥ tâˆ’1, trong khi cÃ¡c nguyÃªn máº«u cá»§a Wilson Warbler vÃ  Sooty Albatross thuá»™c vá» nhiá»‡m vá»¥ t. CÃ¡c lá»›p gt Ä‘Æ°á»£c Ä‘áº·t trÆ°á»›c bá»Ÿi backbone chung f, add-on fA, vÃ  sigmoid. HÆ¡n ná»¯a, chÃºng Ä‘Æ°á»£c theo sau bá»Ÿi cÃ¡c lá»›p cuá»‘i ht vá»›i trá»ng sá»‘ ht
ci= 1 náº¿u nguyÃªn máº«u pi Ä‘Æ°á»£c gÃ¡n cho lá»›p c vÃ  báº±ng 0 trong trÆ°á»ng há»£p ngÆ°á»£c láº¡i.

cá»§a cÃ¡c pháº§n nguyÃªn máº«u Ä‘Æ°á»£c giá»›i thiá»‡u trong ProtoPNet [16] Ä‘á»ƒ Ä‘áº¡i diá»‡n cho cÃ¡c máº«u kÃ­ch hoáº¡t cá»§a máº¡ng. Má»™t sá»‘ pháº§n má»Ÿ rá»™ng Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t, bao gá»“m TesNet [85] vÃ  Deformable ProtoPNet [22], khai thÃ¡c tÃ­nh trá»±c giao trong xÃ¢y dá»±ng nguyÃªn máº«u. ProtoPShare [74], ProtoTree [59], vÃ  ProtoPool [73] giáº£m sá»‘ lÆ°á»£ng nguyÃªn máº«u Ä‘Æ°á»£c sá»­ dá»¥ng trong phÃ¢n loáº¡i. CÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c xem xÃ©t phÃ¢n loáº¡i phÃ¢n cáº¥p vá»›i nguyÃªn máº«u [33], biáº¿n Ä‘á»•i pháº§n nguyÃªn máº«u [47], vÃ  ká»¹ thuáº­t chÆ°ng cáº¥t kiáº¿n thá»©c tá»« nguyÃªn máº«u [39]. CÃ¡c giáº£i phÃ¡p dá»±a trÃªn nguyÃªn máº«u Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng rá»™ng rÃ£i trong cÃ¡c á»©ng dá»¥ng khÃ¡c nhau nhÆ° hÃ¬nh áº£nh y táº¿ [2, 6, 41, 72, 81], phÃ¢n tÃ­ch chuá»—i thá»i gian [28], phÃ¢n loáº¡i Ä‘á»“ thá»‹ [71, 94], há»c chuá»—i [55], vÃ  phÃ¢n Ä‘oáº¡n nghÄ©a [75]. Trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i thÃ­ch á»©ng cÆ¡ cháº¿ nguyÃªn máº«u vá»›i há»c tÄƒng dáº§n lá»›p.

3. PhÆ°Æ¡ng phÃ¡p
Má»¥c tiÃªu cá»§a phÆ°Æ¡ng phÃ¡p chÃºng tÃ´i lÃ  tÄƒng kháº£ nÄƒng diá»…n giáº£i trong tÃ¬nh huá»‘ng tÄƒng dáº§n lá»›p. Äá»ƒ má»¥c Ä‘Ã­ch nÃ y, chÃºng tÃ´i thÃ­ch á»©ng cÃ¡c pháº§n nguyÃªn máº«u [16], trá»±c tiáº¿p tham gia vÃ o tÃ­nh toÃ¡n mÃ´ hÃ¬nh, lÃ m cho cÃ¡c giáº£i thÃ­ch trung thÃ nh vá»›i quyáº¿t Ä‘á»‹nh phÃ¢n loáº¡i. Äá»ƒ lÃ m cho cÃ´ng trÃ¬nh nÃ y Ä‘á»™c láº­p, trÆ°á»›c tiÃªn chÃºng tÃ´i nhá»› láº¡i phÆ°Æ¡ng phÃ¡p luáº­n pháº§n nguyÃªn máº«u, vÃ  sau Ä‘Ã³ chÃºng tÃ´i mÃ´ táº£ cÃ¡ch chÃºng tÃ´i thÃ­ch á»©ng chÃºng vá»›i tÃ¬nh huá»‘ng tÄƒng dáº§n lá»›p. ChÃºng tÃ´i nháº±m bÃ¹ trá»« sá»± trÃ´i dáº¡t khÃ¡i niá»‡m diá»…n giáº£i, Ä‘iá»u mÃ  chÃºng tÃ´i Ä‘á»‹nh nghÄ©a á»Ÿ cuá»‘i pháº§n nÃ y.

3.1. PhÆ°Æ¡ng phÃ¡p luáº­n pháº§n nguyÃªn máº«u
Kiáº¿n trÃºc. Viá»‡c triá»ƒn khai ban Ä‘áº§u cá»§a cÃ¡c pháº§n nguyÃªn máº«u [16] giá»›i thiá»‡u má»™t lá»›p pháº§n nguyÃªn máº«u bá»• sung g Ä‘Æ°á»£c Ä‘áº·t trÆ°á»›c bá»Ÿi má»™t máº¡ng tÃ­ch cháº­p backbone f vá»›i má»™t add-on fA vÃ  theo sau bá»Ÿi lá»›p káº¿t ná»‘i Ä‘áº§y Ä‘á»§ h. Add-on fA bao gá»“m hai lá»›p tÃ­ch cháº­p 1Ã—1 vÃ  má»™t kÃ­ch hoáº¡t sigmoid á»Ÿ cuá»‘i, dá»‹ch Ä‘áº§u ra tÃ­ch cháº­p sang khÃ´ng gian pháº§n nguyÃªn máº«u. Lá»›p pháº§n nguyÃªn máº«u g bao gá»“m K nguyÃªn máº«u piâˆˆRD cho má»—i lá»›p, vÃ  viá»‡c gÃ¡n cá»§a chÃºng Ä‘Æ°á»£c xá»­ lÃ½ bá»Ÿi lá»›p káº¿t ná»‘i Ä‘áº§y Ä‘á»§ h. Náº¿u nguyÃªn máº«u pi Ä‘Æ°á»£c gÃ¡n cho lá»›p c, thÃ¬ hci= 1, náº¿u khÃ´ng, nÃ³ Ä‘Æ°á»£c Ä‘áº·t thÃ nh âˆ’0.5.

Suy luáº­n. Vá»›i má»™t hÃ¬nh áº£nh Ä‘áº§u vÃ o x, backbone f táº¡o ra biá»ƒu diá»…n f(x) cá»§a nÃ³ cÃ³ hÃ¬nh dáº¡ng HÃ—WÃ—D, trong Ä‘Ã³ H vÃ  W lÃ  chiá»u cao vÃ  chiá»u rá»™ng cá»§a biá»ƒu diá»…n thu Ä‘Æ°á»£c táº¡i lá»›p tÃ­ch cháº­p cuá»‘i cÃ¹ng, vÃ  D lÃ  sá»‘ kÃªnh. Biá»ƒu diá»…n nÃ y Ä‘Æ°á»£c dá»‹ch bá»Ÿi fA sang khÃ´ng gian pháº§n nguyÃªn máº«u, má»™t láº§n ná»¯a cÃ³ kÃ­ch thÆ°á»›c HÃ—WÃ—D. Sau Ä‘Ã³, má»—i pháº§n nguyÃªn máº«u pi Ä‘Æ°á»£c so sÃ¡nh vá»›i tá»«ng vectÆ¡ biá»ƒu diá»…n HÃ—W Ä‘á»ƒ tÃ­nh toÃ¡n Ä‘á»™ tÆ°Æ¡ng tá»± tá»‘i Ä‘a (tá»©c lÃ  kÃ­ch hoáº¡t tá»‘i Ä‘a cá»§a nguyÃªn máº«u nÃ y trÃªn hÃ¬nh áº£nh Ä‘Æ°á»£c phÃ¢n tÃ­ch) max jâˆˆ{1..HW}sim(pi, zj), trong Ä‘Ã³

--- TRANG 4 ---
nguyÃªn máº«u pt-1 
f  sau 
nhiá»‡m vá»¥ t-1
báº£n Ä‘á»“ 
tÆ°Æ¡ng tá»± nguyÃªn máº«u 
sau nhiá»‡m vá»¥ tbáº£n Ä‘á»“ 
tÆ°Æ¡ng tá»± nguyÃªn máº«u 
sau nhiá»‡m vá»¥ t-1máº·t náº¡ cá»§a 
Ä‘á»™ tÆ°Æ¡ng tá»± cao nháº¥ttá»‘i thiá»ƒu hÃ³a 
MSE 
tháº¥p caoÄ‘á»™ tÆ°Æ¡ng tá»±f  sau 
nhiá»‡m vá»¥ tHÃ¬nh 3: Äiá»u chuáº©n kháº£ nÄƒng diá»…n giáº£i cá»§a chÃºng tÃ´i nháº±m tá»‘i thiá»ƒu hÃ³a cÃ¡c thay Ä‘á»•i trong Ä‘á»™ tÆ°Æ¡ng tá»± nguyÃªn máº«u. NÃ³ láº¥y má»™t nguyÃªn máº«u ptâˆ’1 cá»§a cÃ¡c nhiá»‡m vá»¥ trÆ°á»›c Ä‘Ã³ vÃ  má»™t hÃ¬nh áº£nh tá»« nhiá»‡m vá»¥ t, chá»n khu vá»±c hÃ¬nh áº£nh cÃ³ Ä‘á»™ tÆ°Æ¡ng tá»± cao nháº¥t vá»›i nguyÃªn máº«u nÃ y (máº·t náº¡ nhá»‹ phÃ¢n S), vÃ  pháº¡t mÃ´ hÃ¬nh vÃ¬ báº¥t ká»³ thay Ä‘á»•i nÃ o trong khu vá»±c nÃ y do huáº¥n luyá»‡n nhiá»‡m vá»¥ t.

sim(pi, zj) = log|zjâˆ’pi|2+1
|zjâˆ’pi|2+Î· vÃ  Î·â‰ª1. Äá»ƒ cÃ³ Ä‘Æ°á»£c dá»± Ä‘oÃ¡n cuá»‘i cÃ¹ng, chÃºng tÃ´i Ä‘áº©y nhá»¯ng giÃ¡ trá»‹ Ä‘Ã³ qua lá»›p káº¿t ná»‘i Ä‘áº§y Ä‘á»§ (vÃ  Ä‘Æ°á»£c khá»Ÿi táº¡o thÃ­ch há»£p) h.

Huáº¥n luyá»‡n. Huáº¥n luyá»‡n Ä‘Æ°á»£c chia thÃ nh ba giai Ä‘oáº¡n tá»‘i Æ°u hÃ³a: khá»Ÿi Ä‘á»™ng, há»c káº¿t há»£p, vÃ  tá»‘i Æ°u hÃ³a lá»“i cá»§a lá»›p cuá»‘i. Giai Ä‘oáº¡n Ä‘áº§u tiÃªn huáº¥n luyá»‡n add-on fA vÃ  lá»›p pháº§n nguyÃªn máº«u g. Giai Ä‘oáº¡n thá»© hai há»c fA, g, vÃ  máº¡ng backbone f. Giai Ä‘oáº¡n cuá»‘i tinh chá»‰nh lá»›p káº¿t ná»‘i Ä‘áº§y Ä‘á»§ h. Huáº¥n luyá»‡n Ä‘Æ°á»£c tiáº¿n hÃ nh vá»›i máº¥t mÃ¡t cross-entropy Ä‘Æ°á»£c há»— trá»£ bá»Ÿi hai Ä‘iá»u chuáº©n, chi phÃ­ cá»¥m vÃ  tÃ¡ch biá»‡t [16]. Cá»¥m khuyáº¿n khÃ­ch má»—i hÃ¬nh áº£nh huáº¥n luyá»‡n cÃ³ má»™t vÃ¡ áº©n gáº§n vá»›i Ã­t nháº¥t má»™t nguyÃªn máº«u cá»§a lá»›p cá»§a nÃ³. NgÆ°á»£c láº¡i, chi phÃ­ tÃ¡ch biá»‡t khuyáº¿n khÃ­ch má»i vÃ¡ áº©n cá»§a hÃ¬nh áº£nh huáº¥n luyá»‡n trÃ¡nh xa cÃ¡c nguyÃªn máº«u cá»§a cÃ¡c lá»›p cÃ²n láº¡i.

3.2. ICICLE
Nhá»¯ng sá»­a Ä‘á»•i Ä‘Ã¡ng ká»ƒ vá» kiáº¿n trÃºc vÃ  huáº¥n luyá»‡n lÃ  cáº§n thiáº¿t Ä‘á»ƒ sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p luáº­n pháº§n nguyÃªn máº«u cho há»c tÄƒng dáº§n lá»›p (suy luáº­n lÃ  giá»‘ng há»‡t). Chá»§ yáº¿u vÃ¬ há»c tÄƒng dáº§n cÃ³ cÃ¡c phá»ng Ä‘oÃ¡n khÃ¡ khÃ¡c nhau. NÃ³ giáº£ Ä‘á»‹nh T nhiá»‡m vá»¥ (C1, X1),(C2, X2), . . . , (CT, XT), trong Ä‘Ã³ má»—i nhiá»‡m vá»¥ t chá»©a cÃ¡c lá»›p Ct vÃ  táº­p huáº¥n luyá»‡n Xt. HÆ¡n ná»¯a, trong nhiá»‡m vá»¥ t, chá»‰ cÃ³ dá»¯ liá»‡u huáº¥n luyá»‡n Xt kháº£ dá»¥ng, vÃ¬ chÃºng tÃ´i xem xÃ©t thiáº¿t láº­p khÃ´ng cáº§n máº«u Ä‘iá»ƒn hÃ¬nh, nÆ¡i bá»‹ cáº¥m lÆ°u báº¥t ká»³ dá»¯ liá»‡u nÃ o tá»« cÃ¡c nhiá»‡m vá»¥ trÆ°á»›c Ä‘Ã³ (khÃ´ng cho phÃ©p bá»™ Ä‘á»‡m phÃ¡t láº¡i).

Kiáº¿n trÃºc. NhÆ° trong mÃ´ hÃ¬nh cÆ¡ sá»Ÿ, ICICLE bao gá»“m backbone f vÃ  add-on fA. Tuy nhiÃªn, nÃ³ khÃ´ng sá»­ dá»¥ng má»™t lá»›p pháº§n nguyÃªn máº«u g cá»‘ Ä‘á»‹nh vÃ  má»™t lá»›p káº¿t ná»‘i Ä‘áº§y Ä‘á»§ h. Thay vÃ o Ä‘Ã³, nÃ³ giá»›i thiá»‡u má»™t lá»›p pháº§n nguyÃªn máº«u gt vÃ  má»™t lá»›p káº¿t ná»‘i Ä‘áº§y Ä‘á»§ ht cho má»—i nhiá»‡m vá»¥ liÃªn tiáº¿p. CÃ¡c lá»›p gt bao gá»“m Mt pháº§n nguyÃªn máº«u, trong Ä‘Ã³ Mt= KÂ·Ct vÃ  K lÃ  sá»‘ nguyÃªn máº«u trÃªn má»—i lá»›p. Máº·t khÃ¡c, lá»›p ht cÃ³ trá»ng sá»‘ ht
ci= 1 náº¿u nguyÃªn máº«u pi Ä‘Æ°á»£c gÃ¡n cho lá»›p c vÃ  nÃ³ Ä‘Æ°á»£c Ä‘áº·t thÃ nh 0 trong trÆ°á»ng há»£p ngÆ°á»£c láº¡i. ChÃºng tÃ´i Ä‘Ã£ loáº¡i bá» trá»ng sá»‘ Ã¢m (-0.5) tá»« lá»›p cuá»‘i vÃ¬

KhÃ´ng gian áº©n 
sau nhiá»‡m vá»¥ t-1 
nguyÃªn máº«u tá»« nhiá»‡m vá»¥ t-1 
vá»‹ trÃ­ ban Ä‘áº§u cho 
nguyÃªn máº«u nhiá»‡m vá»¥ t 
biá»ƒu diá»…n cá»§a dá»¯ liá»‡u nhiá»‡m vá»¥ t Khá»Ÿi táº¡o ngáº«u nhiÃªn 
Khá»Ÿi táº¡o Ä‘á»™ gáº§n 
cá»¥m cá»§a biá»ƒu diá»…n HÃ¬nh 4: ChÃºng tÃ´i giá»›i thiá»‡u khá»Ÿi táº¡o nguyÃªn máº«u dá»±a trÃªn Ä‘á»™ gáº§n má»›i. NÃ³ báº¯t Ä‘áº§u báº±ng cÃ¡ch truyá»n cÃ¡c máº«u huáº¥n luyá»‡n cá»§a nhiá»‡m vá»¥ t qua máº¡ng (cÃ¡c cháº¥m xanh lÃ¡) vÃ  chá»n cÃ¡c biá»ƒu diá»…n gáº§n nháº¥t vá»›i cÃ¡c nguyÃªn máº«u hiá»‡n táº¡i (kim cÆ°Æ¡ng tÃ­m). Äiá»u nÃ y dáº«n Ä‘áº¿n nhiá»u Ä‘iá»ƒm, mÃ  chÃºng tÃ´i phÃ¢n cá»¥m (hÃ¬nh trÃ²n tÃ­m) Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c vá»‹ trÃ­ ban Ä‘áº§u cá»§a cÃ¡c pháº§n nguyÃªn máº«u nhiá»‡m vá»¥ t (kim cÆ°Æ¡ng vÃ ng). Khá»Ÿi táº¡o nhÆ° váº­y (dÆ°á»›i pháº£i) Ä‘Æ°á»£c Æ°a thÃ­ch hÆ¡n khá»Ÿi táº¡o ngáº«u nhiÃªn (trÃªn pháº£i), nÆ¡i cÃ¡c nguyÃªn máº«u má»›i cÃ³ thá»ƒ Ä‘Æ°á»£c táº¡o ra xa cÃ¡c nguyÃªn máº«u cÅ©, máº·c dÃ¹ chÃºng chá»‰ khÃ¡c nhau má»™t chÃºt.

huáº¥n luyá»‡n Ä‘a giai Ä‘oáº¡n khÃ´ng cÃ³ lá»£i cho tÃ¬nh huá»‘ng tÄƒng dáº§n lá»›p (xem HÃ¬nh 6).

Huáº¥n luyá»‡n. Äá»ƒ ngÄƒn cháº·n quÃªn tháº£m khá»‘c, ICICLE sá»­a Ä‘á»•i hÃ m máº¥t mÃ¡t cá»§a giáº£i phÃ¡p cÆ¡ sá»Ÿ. NgoÃ i ra, nÃ³ giá»›i thiá»‡u ba cÆ¡ cháº¿: Ä‘iá»u chuáº©n kháº£ nÄƒng diá»…n giáº£i, khá»Ÿi táº¡o nguyÃªn máº«u dá»±a trÃªn Ä‘á»™ gáº§n, vÃ  bÃ¹ trá»« thiÃªn lá»‡ch gáº§n Ä‘Ã¢y cá»§a nhiá»‡m vá»¥. LiÃªn quan Ä‘áº¿n hÃ m máº¥t mÃ¡t cÆ¡ sá»Ÿ, cross-entropy Ä‘Æ°á»£c tÃ­nh toÃ¡n trÃªn Ä‘áº§u ra Ä‘áº§y Ä‘á»§ cá»§a mÃ´ hÃ¬nh, bao gá»“m logits tá»« cÃ¡c lá»›p Ä‘Æ°á»£c há»c trong cÃ¡c nhiá»‡m vá»¥ trÆ°á»›c Ä‘Ã³. Tuy nhiÃªn, chi phÃ­ cá»¥m vÃ  tÃ¡ch biá»‡t chá»‰ Ä‘Æ°á»£c tÃ­nh toÃ¡n trong head gt.

Äiá»u chuáº©n kháº£ nÄƒng diá»…n giáº£i. ChÆ°ng cáº¥t kiáº¿n thá»©c [35] lÃ  má»™t trong nhá»¯ng phÆ°Æ¡ng phÃ¡p Ä‘iá»u chuáº©n máº¡nh Ä‘Æ°á»£c Ã¡p dá»¥ng Ä‘á»ƒ ngÄƒn cháº·n quÃªn lÃ£ng [48]. Tuy nhiÃªn, káº¿t quáº£ thu Ä‘Æ°á»£c báº±ng á»©ng dá»¥ng trá»±c tiáº¿p cá»§a nÃ³ khÃ´ng thá»a Ä‘Ã¡ng vÃ  dáº«n Ä‘áº¿n sá»± trÃ´i dáº¡t giáº£i thÃ­ch Ä‘Ã¡ng ká»ƒ (xem HÃ¬nh 1 vÃ  Pháº§n 5). Do Ä‘Ã³, chÃºng tÃ´i giá»›i thiá»‡u má»™t chi phÃ­ Ä‘iá»u chuáº©n bá»• sung LIR (xem HÃ¬nh 3), Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« [39], tá»‘i thiá»ƒu hÃ³a cÃ¡c thay Ä‘á»•i trong Ä‘á»™ tÆ°Æ¡ng tá»± cho cÃ¡c pháº§n nguyÃªn máº«u cá»§a cÃ¡c nhiá»‡m vá»¥ trÆ°á»›c Ä‘Ã³. NÃ³ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ :

LIR=HX
i=0WX
j=0|sim(ptâˆ’1, zt
i,j)âˆ’sim(pt, zt
i,j)|Â·Si,j(1)

trong Ä‘Ã³ sim(ptâˆ’1, zt
i,j) Ä‘Æ°á»£c tÃ­nh toÃ¡n cho mÃ´ hÃ¬nh Ä‘Æ°á»£c lÆ°u trÆ°á»›c khi huáº¥n luyá»‡n nhiá»‡m vá»¥ t, vÃ  S lÃ  má»™t máº·t náº¡ nhá»‹ phÃ¢n cÃ³ kÃ­ch thÆ°á»›c HÃ—W, chá»‰ ra cÃ¡c pixel biá»ƒu diá»…n cÃ³ Ä‘á»™ tÆ°Æ¡ng tá»± cao nháº¥t (phÃ¢n vá»‹ Î³ cá»§a nhá»¯ng pixel Ä‘Ã³). ChÆ°ng cáº¥t Ä‘á»™ tÆ°Æ¡ng tá»± nhÆ° váº­y

--- TRANG 5 ---
táº¡o ra tÃ­nh dáº»o cao hÆ¡n khi há»c má»™t nhiá»‡m vá»¥ má»›i nhÆ°ng, cÃ¹ng lÃºc, giáº£m sá»± trÃ´i dáº¡t kháº£ nÄƒng diá»…n giáº£i.

Khá»Ÿi táº¡o nguyÃªn máº«u dá»±a trÃªn Ä‘á»™ gáº§n. Khá»Ÿi táº¡o ngáº«u nhiÃªn cÃ¡c nguyÃªn máº«u, Ä‘Æ°á»£c Ä‘á» xuáº¥t trong [16], tháº¥t báº¡i trong há»c tÄƒng dáº§n (xem Báº£ng 5). CÃ³ láº½ vÃ¬ cÃ¡c nguyÃªn máº«u má»›i cÃ³ thá»ƒ Ä‘Æ°á»£c táº¡o ra xa cÃ¡c nguyÃªn máº«u cÅ©, trong khi chÃºng chá»‰ khÃ¡c nhau má»™t chÃºt (vÃ­ dá»¥: nguyÃªn máº«u cÃ¡nh cá»§a cÃ¡c loÃ i chim khÃ¡c nhau). Do Ä‘Ã³, chÃºng tÃ´i giá»›i thiá»‡u khá»Ÿi táº¡o Ä‘áº·t cÃ¡c nguyÃªn máº«u má»›i gáº§n vá»›i cÃ¡c nguyÃªn máº«u cÅ© (xem HÃ¬nh 4). ChÃºng tÃ´i báº¯t Ä‘áº§u báº±ng cÃ¡ch truyá»n cÃ¡c máº«u huáº¥n luyá»‡n cá»§a nhiá»‡m vá»¥ t qua máº¡ng vÃ  xÃ¡c Ä‘á»‹nh nhá»¯ng vÃ¡ nÃ o tÆ°Æ¡ng tá»± nháº¥t vá»›i cÃ¡c nguyÃªn máº«u hiá»‡n táº¡i (chÃºng tÃ´i chá»n cÃ¡c vÃ¡ tá»« phÃ¢n vá»‹ Î±). Cá»¥ thá»ƒ hÆ¡n, chÃºng tÃ´i tÃ­nh toÃ¡n táº­p {zt
j: max isim(ptâˆ’1
i, zt
j)âˆˆÎ±phÃ¢n vá»‹ }. Äiá»u nÃ y dáº«n Ä‘áº¿n nhiá»u á»©ng viÃªn cho cÃ¡c pháº§n nguyÃªn máº«u nhiá»‡m vá»¥ má»›i. Äá»ƒ cÃ³ Ä‘Æ°á»£c KÂ·Ct nguyÃªn máº«u, chÃºng tÃ´i thá»±c hiá»‡n phÃ¢n cá»¥m KMeans++, vÃ  cÃ¡c trung tÃ¢m káº¿t quáº£ cá»§a cÃ¡c cá»¥m Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ khá»Ÿi táº¡o cÃ¡c pháº§n nguyÃªn máº«u trong gt.

BÃ¹ trá»« thiÃªn lá»‡ch gáº§n Ä‘Ã¢y cá»§a nhiá»‡m vá»¥. Khi mÃ´ hÃ¬nh há»c nhiá»‡m vá»¥ t, Ä‘á»™ tÆ°Æ¡ng tá»± vá»›i cÃ¡c nguyÃªn máº«u cá»§a cÃ¡c nhiá»‡m vá»¥ trÆ°á»›c Ä‘Ã³ giáº£m, chá»§ yáº¿u do nhá»¯ng thay Ä‘á»•i trong backbone (xem HÃ¬nh 5). ÄÃ³ lÃ  lÃ½ do táº¡i sao, sau khi huáº¥n luyá»‡n nhiá»‡m vá»¥ cuá»‘i cÃ¹ng, chÃºng tÃ´i bÃ¹ trá»« Ä‘iá»u nÃ y báº±ng cÃ¡ch sá»­ dá»¥ng Tâˆ’1 háº±ng sá»‘ thu Ä‘Æ°á»£c sá»­ dá»¥ng dá»¯ liá»‡u nhiá»‡m vá»¥ cuá»‘i cÃ¹ng. ChÃ­nh xÃ¡c hÆ¡n, Ä‘á»‘i vá»›i má»—i nhiá»‡m vá»¥ trÆ°á»›c Ä‘Ã³ t < T, chÃºng tÃ´i láº¥y logits yt=htâ—¦gtâ—¦f(x) thu Ä‘Æ°á»£c cho táº¥t cáº£ xâˆˆXT vÃ  tÃ­nh toÃ¡n thiÃªn lá»‡ch ct sao cho |{xâˆˆXT: max ( yt+ct)>maxyT}|=u|XT|. Má»™t cÃ¡ch trá»±c quan, chÃºng tÃ´i Ä‘iá»u chá»‰nh ct Ä‘á»ƒ mÃ´ hÃ¬nh thay Ä‘á»•i u% dá»± Ä‘oÃ¡n cá»§a nÃ³ tá»« nhiá»‡m vá»¥ T sang nhiá»‡m vá»¥ t. ChÃºng tÃ´i xÃ¡c Ä‘á»‹nh thá»±c nghiá»‡m ráº±ng u= 10% lÃ  tá»‘i Æ°u.

3.3. Sá»± TrÃ´i dáº¡t KhÃ¡i niá»‡m Diá»…n giáº£i
NhÆ° Ä‘Ã£ lÆ°u Ã½ trong chÃº thÃ­ch cá»§a HÃ¬nh 1, sá»± trÃ´i dáº¡t khÃ¡i niá»‡m diá»…n giáº£i xáº£y ra khi báº£n Ä‘á»“ tÆ°Æ¡ng tá»± khÃ¡c nhau giá»¯a cÃ¡c nhiá»‡m vá»¥. Do Ä‘Ã³, nÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a chÃ­nh thá»©c lÃ :

ICD =EH,W
i,j=1sim(ptâˆ’1, zt
i,j)âˆ’sim(pt, zt
i,j),(2)

trong Ä‘Ã³ (zi,j)H,W
i,j=1 tÆ°Æ¡ng á»©ng vá»›i biá»ƒu diá»…n hÃ¬nh áº£nh Ä‘áº§u vÃ o, ptâˆ’1 vÃ  pt tÆ°Æ¡ng á»©ng vá»›i pháº§n nguyÃªn máº«u p trÆ°á»›c vÃ  sau nhiá»‡m vá»¥ t, vÃ  sim lÃ  Ä‘á»™ tÆ°Æ¡ng tá»± Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trong Pháº§n 3.1 cá»§a bÃ i bÃ¡o. Do Ä‘Ã³, ICD cÃ ng lá»›n thÃ¬ sá»± trÃ´i dáº¡t khÃ¡i niá»‡m diá»…n giáº£i cÃ ng lá»›n.

4. Thiáº¿t láº­p ThÃ­ nghiá»‡m
ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i trÃªn cÃ¡c táº­p dá»¯ liá»‡u CUB-200-2011 [84] vÃ  Stanford Cars [46] Ä‘á»ƒ phÃ¢n loáº¡i 200 loÃ i chim vÃ  196 máº«u xe, tÆ°Æ¡ng á»©ng. ChÃºng tÃ´i xem xÃ©t cÃ¡c tÃ¬nh huá»‘ng há»c 4, 10, vÃ  20 nhiá»‡m vá»¥ cho chim vÃ  4, 7, vÃ  14 tÃ¹y chá»n cho xe. NhÆ° backbone f, chÃºng tÃ´i láº¥y ResNet-34 [34] khÃ´ng cÃ³ lá»›p cuá»‘i vÃ  Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n trÃªn ImageNet [20]. ChÃºng tÃ´i Ä‘áº·t sá»‘ nguyÃªn máº«u trÃªn má»—i lá»›p lÃ  10. HÆ¡n ná»¯a, chÃºng tÃ´i sá»­ dá»¥ng cÃ¡c pháº§n nguyÃªn máº«u cÃ³ kÃ­ch thÆ°á»›c 1Ã—1Ã—256 vÃ  1Ã—1Ã—128 cho chim vÃ  xe, tÆ°Æ¡ng á»©ng. Trá»ng sá»‘ cá»§a CE, cá»¥m, tÃ¡ch biá»‡t, vÃ  chi phÃ­ chÆ°ng cáº¥t trong hÃ m máº¥t mÃ¡t báº±ng 1.0, 0.8, âˆ’0.08, vÃ  0.01. Trong chÆ°ng cáº¥t, chÃºng tÃ´i láº¥y Î»= 1/49 pixel biá»ƒu diá»…n cÃ³ Ä‘á»™ tÆ°Æ¡ng tá»± cao nháº¥t. Äá»‘i vá»›i khá»Ÿi táº¡o dá»±a trÃªn Ä‘á»™ gáº§n, chÃºng tÃ´i sá»­ dá»¥ng Î±= 0.5. Äá»‘i vá»›i bÃ¹ trá»« thiÃªn lá»‡ch gáº§n Ä‘Ã¢y cá»§a nhiá»‡m vá»¥, chÃºng tÃ´i láº¥y ct, thay Ä‘á»•i dá»± Ä‘oÃ¡n cá»§a táº­p xÃ¡c thá»±c cuá»‘i cÃ¹ng Ã­t hÆ¡n 10%. NhÆ° khung triá»ƒn khai, chÃºng tÃ´i sá»­ dá»¥ng FACIL [54] dá»±a trÃªn thÆ° viá»‡n PyTorch1. Chi tiáº¿t vá» thiáº¿t láº­p thÃ­ nghiá»‡m Ä‘Æ°á»£c cung cáº¥p trong TÃ i liá»‡u Bá»• sung2.

5. Káº¿t quáº£
Hiá»‡u suáº¥t. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ cá»§a ICICLE báº±ng cÃ¡ch so sÃ¡nh nÃ³ vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p cÆ¡ sá»Ÿ khÃ´ng cáº§n máº«u Ä‘iá»ƒn hÃ¬nh thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong há»c tÄƒng dáº§n lá»›p, bao gá»“m LWF [48], LWM [21], vÃ  EWC[44]3. NgoÃ i ra, Fine-tuning, vÃ  Freezing cá»§a bá»™ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng (khÃ´ng Ä‘Æ°á»£c huáº¥n luyá»‡n chÃºt nÃ o) Ä‘Æ°á»£c cung cáº¥p. ChÃºng tÃ´i cÅ©ng bÃ¡o cÃ¡o há»c Ä‘a nhiá»‡m vá»¥ nhÆ° má»™t cáº­n trÃªn nÆ¡i cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau Ä‘Æ°á»£c há»c káº¿t há»£p theo cÃ¡ch Ä‘a nhiá»‡m vá»¥. Äá»ƒ lÃ m nhÆ° váº­y, chÃºng tÃ´i phÃ¢n tÃ­ch Ä‘á»™ chÃ­nh xÃ¡c biáº¿t nhiá»‡m vá»¥ vÃ  khÃ´ng biáº¿t nhiá»‡m vá»¥ cho má»—i nhiá»‡m vá»¥ sau nhiá»‡m vá»¥ cuá»‘i cÃ¹ng (Báº£ng 3) vÃ  Ä‘á»™ chÃ­nh xÃ¡c trung bÃ¬nh tÄƒng dáº§n tá»•ng há»£p sau khi há»c nhiá»‡m vá»¥ cuá»‘i cÃ¹ng trong cÃ¡c tÃ¬nh huá»‘ng liÃªn quan Ä‘áº¿n 4, 10, vÃ  20 nhiá»‡m vá»¥ cho CUB (Báº£ng 2) vÃ  4, 7, vÃ  14 nhiá»‡m vá»¥ cho Stanford Cars (TÃ i liá»‡u Bá»• sung). Táº¥t cáº£ cÃ¡c phÆ°Æ¡ng phÃ¡p sá»­ dá»¥ng cÃ¹ng kiáº¿n trÃºc máº¡ng trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng vÃ  ProtoPNet cho há»c dá»±a trÃªn pháº§n nguyÃªn máº«u. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i vÆ°á»£t trá»™i so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p cÆ¡ sá»Ÿ trong táº¥t cáº£ cÃ¡c trÆ°á»ng há»£p, cho tháº¥y hiá»‡u suáº¥t vÆ°á»£t trá»™i cá»§a nÃ³ cho há»c dá»±a trÃªn pháº§n nguyÃªn máº«u theo cÃ¡ch liÃªn tá»¥c. ICICLE giá»¯ láº¡i kiáº¿n thá»©c tá»« cÃ¡c nhiá»‡m vá»¥ trÆ°á»›c Ä‘Ã³ tá»‘t hÆ¡n, dáº«n Ä‘áº¿n Ä‘á»™ chÃ­nh xÃ¡c cÃ¢n báº±ng hÆ¡n giá»¯a cÃ¡c nhiá»‡m vá»¥ vÃ  Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n cho nhiá»‡m vá»¥ Ä‘áº§u tiÃªn so vá»›i táº¥t cáº£ cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c. Tuy nhiÃªn, báº¥t cháº¥p sá»± cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i váº«n cÃ²n chá»— Ä‘á»ƒ cáº£i thiá»‡n so vá»›i cáº­n trÃªn cá»§a huáº¥n luyá»‡n Ä‘a nhiá»‡m vá»¥. Vá»›i sá»‘ lÆ°á»£ng nhiá»‡m vá»¥ lá»›n hÆ¡n, viá»‡c quÃªn lÃ£ng cá»§a mÃ´ hÃ¬nh lÃ 

1https://pytorch.org
2MÃ£ cÃ³ sáºµn táº¡i: https://github.com/gmum/ICICLE
3Viá»‡c má»Ÿ rá»™ng cÃ¡c mÃ´ hÃ¬nh cÃ³ thá»ƒ diá»…n giáº£i vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ´ng cáº§n máº«u phá»©c táº¡p hÆ¡n khÃ´ng Ä‘Æ¡n giáº£n, vÃ  do Ä‘Ã³ chÃºng tÃ´i Ä‘Ã£ loáº¡i trá»« cÃ¡c phÆ°Æ¡ng phÃ¡p nhÆ° SDC [91] (yÃªu cáº§u há»c vá»›i máº¥t mÃ¡t metric) vÃ  PASS [95] (yÃªu cáº§u káº¿t há»£p vá»›i há»c tá»± giÃ¡m sÃ¡t).

--- TRANG 6 ---
Äá»˜ CHÃNH XÃC TRUNG BÃŒNH TÄ‚NG Dáº¦N BIáº¾T NHIá»†M Vá»¤ Äá»˜ CHÃNH XÃC TRUNG BÃŒNH TÄ‚NG Dáº¦N KHÃ”NG BIáº¾T NHIá»†M Vá»¤
PHÆ¯Æ NG PHÃP 4NHIá»†M Vá»¤ 10NHIá»†M Vá»¤ 20NHIá»†M Vá»¤ 4NHIá»†M Vá»¤ 10NHIá»†M Vá»¤ 20NHIá»†M Vá»¤
FREEZING 0.560Â±0.027 0.531Â±0.042 0.452Â±0.055 0.309Â±0.024 0.115Â±0.028 0.078Â±0.004
FINETUNING 0.229Â±0.005 0.129Â±0.017 0.147Â±0.021 0.177Â±0.006 0.072Â±0.008 0.044Â±0.006
EWC 0.445Â±0.012 0.288Â±0.034 0.188Â±0.031 0.213Â±0.008 0.095Â±0.007 0.046Â±0.011
LWM 0.452Â±0.023 0.294Â±0.032 0.226Â±0.025 0.180Â±0.028 0.090Â±0.011 0.044Â±0.008
LWF 0.301Â±0.048 0.175Â±0.028 0.129Â±0.023 0.219Â±0.019 0.078Â±0.008 0.072Â±0.008
ICICLE 0.654Â±0.011 0.602Â±0.035 0.497Â±0.099 0.350Â±0.053 0.185Â±0.005 0.099Â±0.003
Äa nhiá»‡m vá»¥ 0.858Â±0.005 0.905Â±0.012 0.935Â±0.019 0.499Â±0.009 0.196Â±0.017 0.148Â±0.009
FeTrIL [65] 0.750Â±0.008 0.607Â±0.018 0.407Â±0.051 0.375Â±0.006 0.199Â±0.003 0.127Â±0.011
PASS [95] 0.775Â±0.006 0.647Â±0.003 0.518Â±0.012 0.395Â±0.001 0.233Â±0.009 0.139Â±0.017

Báº£ng 2: So sÃ¡nh Ä‘á»™ chÃ­nh xÃ¡c trung bÃ¬nh tÄƒng dáº§n cho sá»‘ lÆ°á»£ng nhiá»‡m vá»¥ khÃ¡c nhau trÃªn CUB-200-2011, chá»©ng minh tÃ¡c Ä‘á»™ng tiÃªu cá»±c cá»§a sá»‘ lÆ°á»£ng nhiá»‡m vá»¥ cao cáº§n Ä‘Æ°á»£c há»c Ä‘áº¿n hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh. Báº¥t cháº¥p xu hÆ°á»›ng nÃ y, ICICLE vÆ°á»£t trá»™i so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p cÆ¡ sá»Ÿ trÃªn táº¥t cáº£ sá»‘ lÆ°á»£ng nhiá»‡m vá»¥. NgoÃ i ra, chÃºng tÃ´i cho tháº¥y khoáº£ng cÃ¡ch giá»¯a cÃ¡c mÃ´ hÃ¬nh cÃ³ thá»ƒ diá»…n giáº£i vÃ  há»™p Ä‘en báº±ng cÃ¡ch so sÃ¡nh ICICLE vá»›i FeTrIL vÃ  PASS.

Äá»˜ CHÃNH XÃC BIáº¾T NHIá»†M Vá»¤ Äá»˜ CHÃNH XÃC KHÃ”NG BIáº¾T NHIá»†M Vá»¤
PHÆ¯Æ NG PHÃP NHIá»†M Vá»¤ 1 NHIá»†M Vá»¤ 2 NHIá»†M Vá»¤ 3 NHIá»†M Vá»¤ 4 NHIá»†M Vá»¤ 1 NHIá»†M Vá»¤ 2 NHIá»†M Vá»¤ 3 NHIá»†M Vá»¤ 4
FREEZING 0.806Â±0.024 0.462Â±0.037 0.517Â±0.041 0.455Â±0.027 0.570Â±0.031 0.195Â±0.017 0.258Â±0.019 0.213Â±0.020
FINETUNING 0.007Â±0.004 0.016Â±0.008 0.032Â±0.009 0.759Â±0.019 0.0Â±0.0 0.0Â±0.0 0.0Â±0.0 0.759Â±0.019
EWC 0.244Â±0.024 0.378Â±0.072 0.539Â±0.043 0.602Â±0.054 0.001Â±0.001 0.059Â±0.004 0.267Â±0.031 0.527Â±0.051
LWF 0.169Â±0.046 0.119Â±0.008 0.235Â±0.017 0.743Â±0.061 0.158Â±0.035 0.003Â±0.002 0.018Â±0.003 0.537Â±0.142
LWM 0.195Â±0.012 0.412Â±0.014 0.430Â±0.028 0.772Â±0.011 0.027Â±0.024 0.023Â±0.020 0.085Â±0.005 0.772Â±0.037
ICICLE 0.523Â±0.020 0.663Â±0.053 0.709Â±0.038 0.723Â±0.002 0.233Â±0.014 0.365Â±0.021 0.314Â±0.011 0.486Â±0.021

Báº£ng 3: So sÃ¡nh Ä‘á»™ chÃ­nh xÃ¡c nhiá»‡m vá»¥ cho kiáº¿n trÃºc ProtoPNet Ä‘Ã£ sá»­a Ä‘á»•i trong tÃ¬nh huá»‘ng há»c tÄƒng dáº§n lá»›p sau 4 nhiá»‡m vá»¥ huáº¥n luyá»‡n trÃªn táº­p dá»¯ liá»‡u CUB-200-2011, trung bÃ¬nh qua 3 láº§n cháº¡y vá»›i sai sá»‘ chuáº©n cá»§a trung bÃ¬nh. ICICLE cá»§a chÃºng tÃ´i vÆ°á»£t trá»™i so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p cÆ¡ sá»Ÿ vÃ  Ä‘áº¡t káº¿t quáº£ tá»‘t nháº¥t cho táº¥t cáº£ cÃ¡c nhiá»‡m vá»¥ tÄƒng dáº§n trÆ°á»›c Ä‘Ã³, chá»©ng minh kháº£ nÄƒng duy trÃ¬ kiáº¿n thá»©c trÆ°á»›c Ä‘Ã³ trong khi há»c cÃ¡c nhiá»‡m vá»¥ má»›i. Freezing do sá»± cá»‘ Ä‘á»‹nh trá»ng sá»‘ khÃ´ng thá»ƒ há»c Ä‘Ãºng cÃ¡ch cÃ¡c nhiá»‡m vá»¥ má»›i.

cao hÆ¡n, dáº«n Ä‘áº¿n káº¿t quáº£ kÃ©m hÆ¡n, cÃ³ thá»ƒ Ä‘Æ°á»£c quy cho sá»‘ lÆ°á»£ng chi tiáº¿t mÃ  cÃ¡c nguyÃªn máº«u cáº§n náº¯m báº¯t Ä‘á»ƒ phÃ¢n loáº¡i má»™t nhiá»‡m vá»¥ Ä‘Ãºng cÃ¡ch. HÆ¡n ná»¯a, chÃºng tÃ´i Ä‘Ã£ nháº­n tháº¥y ráº±ng freezing lÃ  má»™t cÆ¡ sá»Ÿ máº¡nh máº½ cho tÃ¬nh huá»‘ng biáº¿t nhiá»‡m vá»¥ vÃ¬ báº£n cháº¥t cá»‘ Ä‘á»‹nh cá»§a mÃ´ hÃ¬nh vÃ  backbone Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n.

Kháº£ nÄƒng diá»…n giáº£i Äá»ƒ Ä‘Ã¡nh giÃ¡ náº¿u biá»ƒu diá»…n Ä‘á»“ há»a cá»§a khÃ¡i niá»‡m nguyÃªn máº«u Ä‘Ã£ thay Ä‘á»•i vÃ  bao nhiÃªu, chÃºng tÃ´i sá»­ dá»¥ng metric IoU [75]. IoU Ä‘o lÆ°á»ng sá»± trÃ¹ng láº·p cá»§a biá»ƒu diá»…n trá»±c quan nguyÃªn máº«u (nhÆ° trong HÃ¬nh 1) tá»« nhiá»‡m vá»¥ mÃ  nÃ³ Ä‘Æ°á»£c há»c, qua táº¥t cáº£ cÃ¡c nhiá»‡m vá»¥ tiáº¿p theo. Freezing lÃ  vÆ°á»£t trá»™i trong viá»‡c báº£o tá»“n thÃ´ng tin nguyÃªn máº«u vÃ¬ táº¥t cáº£ trá»ng sá»‘ tá»« cÃ¡c nhiá»‡m vá»¥ trÆ°á»›c Ä‘Ã³ Ä‘Æ°á»£c cá»‘ Ä‘á»‹nh. Vá» cÃ¡c phÆ°Æ¡ng phÃ¡p cho phÃ©p thay Ä‘á»•i trong backbone vÃ  cÃ¡c nguyÃªn máº«u Ä‘Ã£ há»c trÆ°á»›c Ä‘Ã³, ICICLE lÃ  vÆ°á»£t trá»™i so vá»›i táº¥t cáº£ cÃ¡c cÆ¡ sá»Ÿ, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 1. ICICLE giá»¯ cÃ¡c nguyÃªn máº«u cÃ³ thá»ƒ diá»…n giáº£i nháº¥t quÃ¡n vá»›i Ä‘iá»u chuáº©n kháº£ nÄƒng diá»…n giáº£i chÆ°ng cáº¥t cÃ¡c khÃ¡i niá»‡m Ä‘Ã£ há»c.

5.1. NghiÃªn cá»©u loáº¡i bá» vÃ  phÃ¢n tÃ­ch
Táº¡i sao cáº§n thay Ä‘á»•i kiáº¿n trÃºc vÃ  huáº¥n luyá»‡n ProtoPNet? ProtoPNet trong giai Ä‘oáº¡n huáº¥n luyá»‡n cuá»‘i (tá»‘i Æ°u hÃ³a lá»“i lá»›p cuá»‘i) nháº±m tinh chá»‰nh cÃ¡c káº¿t ná»‘i tÃ­ch cá»±c vÃ  Ä‘iá»u chuáº©n cÃ¡c káº¿t ná»‘i Ã¢m thÃ nh 0. Káº¿t quáº£ lÃ , mÃ´ hÃ¬nh há»™i tá»¥ tráº£ vá» cÃ¡c giáº£i thÃ­ch dÆ°á»›i dáº¡ng lÃ½ luáº­n tÃ­ch cá»±c, Ä‘Æ°á»£c mong muá»‘n bá»Ÿi ngÆ°á»i dÃ¹ng cuá»‘i [11]. Trong thiáº¿t láº­p CL, bÆ°á»›c cuá»‘i cá»§a huáº¥n luyá»‡n thay Ä‘á»•i cÃ¡c káº¿t ná»‘i Ã¢m theo cÃ¡ch khÃ¡c (xem HÃ¬nh 6). Máº·t khÃ¡c,

Äiá»u chuáº©n Khá»Ÿi táº¡o BÃ¹ trá»« Äá»™ chÃ­nh xÃ¡c TAw Äá»™ chÃ­nh xÃ¡c TAg
0.216 0.182
âœ“ 0.559 0.280
âœ“ âœ“ 0.654 0.335
âœ“ âœ“ âœ“ 0.654 0.350

Báº£ng 4: áº¢nh hÆ°á»Ÿng cá»§a cÃ¡c thÃ nh pháº§n má»›i khÃ¡c nhau Ä‘áº¿n Ä‘á»™ chÃ­nh xÃ¡c trung bÃ¬nh tÄƒng dáº§n trong tÃ¬nh huá»‘ng há»c bá»‘n nhiá»‡m vá»¥. Káº¿t há»£p táº¥t cáº£ cÃ¡c thÃ nh pháº§n dáº«n Ä‘áº¿n mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t nháº¥t.

GiÃ¡ trá»‹ trá»ng sá»‘ trung bÃ¬nh 
chá»‰ sá»‘ lá»›p chá»‰ sá»‘ lá»›p káº¿t ná»‘i Ã¢m káº¿t ná»‘i dÆ°Æ¡ng 

HÃ¬nh 6: Trá»ng sá»‘ trung bÃ¬nh cá»§a cÃ¡c káº¿t ná»‘i dÆ°Æ¡ng vÃ  Ã¢m trÃªn má»—i lá»›p trong tÃ¬nh huá»‘ng há»c 4 nhiá»‡m vá»¥. CÃ¡c káº¿t ná»‘i Ã¢m khÃ´ng cÃ¢n báº±ng vÃ  máº¡nh giá»¯a cÃ¡c nhiá»‡m vá»¥ dáº«n Ä‘áº¿n cÃ¡c thuá»™c tÃ­nh khÃ´ng mong muá»‘n vá» kháº£ nÄƒng diá»…n giáº£i cá»§a mÃ´ hÃ¬nh.

trong tÃ¬nh huá»‘ng há»c liÃªn tá»¥c khÃ´ng cáº§n máº«u, viá»‡c tiáº¿n hÃ nh giai Ä‘oáº¡n há»c lá»›p cuá»‘i lÃ  khÃ´ng kháº£ thi á»Ÿ cuá»‘i huáº¥n luyá»‡n. ÄÃ³ lÃ  lÃ½ do táº¡i sao, chÃºng tÃ´i Ä‘Ã£ sá»­a Ä‘á»•i lá»›p cuá»‘i cá»§a ProtoPNet vÃ  chá»‰ giá»¯ láº¡i cÃ¡c káº¿t ná»‘i dÆ°Æ¡ng Ä‘Æ°á»£c khá»Ÿi táº¡o thÃ nh 1, loáº¡i bá» nhu cáº§u vá» bÆ°á»›c tá»‘i Æ°u hÃ³a lá»“i.

--- TRANG 7 ---
Ä‘iá»u chuáº©n Ä‘á»™ tÆ°Æ¡ng tá»± Ä‘iá»u chuáº©n khoáº£ng cÃ¡ch Ä‘iá»u chuáº©n Ä‘áº·c trÆ°ng 
khÃ´ng gian mÃ  hÃ m máº¥t mÃ¡t 
cÃ³ cÃ¹ng giÃ¡ trá»‹ pháº§n nguyÃªn máº«u 
biá»ƒu diá»…n pháº§n hÃ¬nh áº£nh 
hÃ m Ä‘á»™ tÆ°Æ¡ng tá»± ProtoPNet giÃ¡ trá»‹ Ä‘á»™ tÆ°Æ¡ng tá»± 
giÃ¡ trá»‹ khoáº£ng cÃ¡ch HÃ¬nh 7: Hiá»‡n thá»‹ ba phÆ°Æ¡ng phÃ¡p cÃ³ thá»ƒ cÃ³ cho Ä‘á»™ tÆ°Æ¡ng tá»± kháº£ nÄƒng diá»…n giáº£i vÃ  áº£nh hÆ°á»Ÿng cá»§a chÃºng Ä‘áº¿n tÃ­nh dáº»o cá»§a mÃ´ hÃ¬nh. Chá»‰ cÃ³ Ä‘iá»u chuáº©n dá»±a trÃªn Ä‘á»™ tÆ°Æ¡ng tá»± tÃ­nh Ä‘áº¿n cÃ¡ch má»™t pháº§n hÃ¬nh áº£nh nháº¥t Ä‘á»‹nh tÆ°Æ¡ng á»©ng vá»›i má»™t pháº§n nguyÃªn máº«u. Náº¿u nÃ³ gáº§n thÃ¬ giÃ¡ trá»‹ Ä‘á»™ tÆ°Æ¡ng tá»± cao vÃ  nhá»¯ng thay Ä‘á»•i nhá» trong khoáº£ng cÃ¡ch dáº«n Ä‘áº¿n giáº£m lá»›n trong Ä‘á»™ tÆ°Æ¡ng tá»±. Trong khi cÃ¡c vectÆ¡ áº©n xa cÃ¡c pháº§n nguyÃªn máº«u cÃ³ thá»ƒ Ä‘Æ°á»£c thay Ä‘á»•i tá»± do hÆ¡n bá»Ÿi mÃ´ hÃ¬nh Ä‘á»ƒ Ä‘áº¡i diá»‡n tá»‘t hÆ¡n dá»¯ liá»‡u nhiá»‡m vá»¥ hiá»‡n táº¡i. CÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c háº¡n cháº¿ tÃ­nh dáº»o cá»§a mÃ´ hÃ¬nh coi má»—i biá»ƒu diá»…n áº©n cá»§a pháº§n hÃ¬nh áº£nh nhÆ° quan trá»ng ngang nhau.

áº¢nh hÆ°á»Ÿng cá»§a tá»«ng thÃ nh pháº§n Ä‘Æ°á»£c giá»›i thiá»‡u lÃ  gÃ¬? Báº£ng 4 trÃ¬nh bÃ y áº£nh hÆ°á»Ÿng cá»§a cÃ¡c thÃ nh pháº§n khÃ¡c nhau trong phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i Ä‘áº¿n Ä‘á»™ chÃ­nh xÃ¡c trung bÃ¬nh tÄƒng dáº§n cuá»‘i cÃ¹ng cá»§a mÃ´ hÃ¬nh trong táº­p dá»¯ liá»‡u CUB-200-2011 vá»›i tÃ¬nh huá»‘ng chia bá»‘n nhiá»‡m vá»¥. Káº¿t há»£p táº¥t cáº£ cÃ¡c thÃ nh pháº§n dáº«n Ä‘áº¿n mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t nháº¥t. Káº¿t quáº£ cá»§a chÃºng tÃ´i cho tháº¥y bÃ¹ trá»« thiÃªn lá»‡ch gáº§n Ä‘Ã¢y cá»§a nhiá»‡m vá»¥ giÃºp trong Ä‘Ã¡nh giÃ¡ khÃ´ng biáº¿t nhiá»‡m vá»¥ vÃ  mang láº¡i cáº£i thiá»‡n bá»• sung 4.5%. Tuy nhiÃªn, háº§u háº¿t cÃ¡c cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c Ä‘Æ°á»£c quy cho Ä‘iá»u chuáº©n kháº£ nÄƒng diá»…n giáº£i vÃ  khá»Ÿi táº¡o Ä‘á»™ gáº§n. ÄÃ¡ng chÃº Ã½, bÃ¹ trá»« thiÃªn lá»‡ch gáº§n Ä‘Ã¢y cá»§a nhiá»‡m vá»¥ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u suáº¥t cá»§a cÃ¡c lá»›p nhiá»‡m vá»¥ má»™t so vá»›i phÆ°Æ¡ng phÃ¡p khÃ´ng cÃ³ nÃ³, tá»« 0.028 Ä‘áº¿n 0.255 trong tÃ¬nh huá»‘ng khÃ´ng biáº¿t nhiá»‡m vá»¥, nhÆ° Ä‘Æ°á»£c chi tiáº¿t trong TÃ i liá»‡u Bá»• sung.

ChÃºng ta nÃªn thá»±c hiá»‡n Ä‘iá»u chuáº©n kháº£ nÄƒng diá»…n giáº£i á»Ÿ Ä‘Ã¢u? Lá»›p nguyÃªn máº«u cá»§a mÃ´ hÃ¬nh ProtoPNet cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘iá»u chuáº©n theo ba cÃ¡ch khÃ¡c nhau: Ä‘iá»u chuáº©n Ä‘áº·c trÆ°ng trÃªn biá»ƒu diá»…n lá»›p add-on, Ä‘iá»u chuáº©n khoáº£ng cÃ¡ch giá»¯a cÃ¡c pháº§n nguyÃªn máº«u vÃ  vectÆ¡ dá»¯ liá»‡u áº©n, vÃ  Ä‘iá»u chuáº©n dá»±a trÃªn Ä‘á»™ tÆ°Æ¡ng tá»±. PhÆ°Æ¡ng phÃ¡p nghiÃªm ngáº·t nháº¥t lÃ  Ä‘iá»u chuáº©n Ä‘áº·c trÆ°ng, khÃ´ng cho phÃ©p mÃ´ hÃ¬nh thay Ä‘á»•i cÃ¡ch nÃ³ biá»ƒu diá»…n dá»¯ liá»‡u tá»« nhiá»‡m vá»¥ má»›i, dáº«n Ä‘áº¿n tÃ­nh dáº»o mÃ´ hÃ¬nh giáº£m Ä‘Ã¡ng ká»ƒ. Khi khoáº£ng cÃ¡ch Ä‘Æ°á»£c Ä‘iá»u chuáº©n, mÃ´ hÃ¬nh cÃ³ thá»ƒ thay Ä‘á»•i biá»ƒu diá»…n cá»§a nÃ³ Ä‘á»ƒ duy trÃ¬ cÃ¹ng khoáº£ng cÃ¡ch tá»« nguyÃªn máº«u trÃªn bá» máº·t cá»§a hÃ¬nh cáº§u. Máº·t khÃ¡c, Ä‘iá»u chuáº©n dá»±a trÃªn Ä‘á»™ tÆ°Æ¡ng tá»± cho phÃ©p mÃ´ hÃ¬nh giá»¯ láº¡i kiáº¿n thá»©c chÃ­nh tá»« cÃ¡c nhiá»‡m vá»¥ trÆ°á»›c Ä‘Ã³ báº±ng cÃ¡ch chá»‰ báº£o tá»“n thÃ´ng tin liÃªn quan Ä‘áº¿n cÃ¡c Ä‘áº·c trÆ°ng cá»¥ thá»ƒ gáº§n vá»›i cÃ¡c pháº§n nguyÃªn máº«u trong khÃ´ng gian áº©n, cho phÃ©p linh hoáº¡t lá»›n hÆ¡n Ä‘á»ƒ Ä‘á»•i láº¥y viá»‡c quÃªn cÃ¡c Ä‘áº·c trÆ°ng khÃ´ng liÃªn quan. Do Ä‘Ã³, chÃºng tÃ´i tuÃ¢n theo Ä‘iá»u chuáº©n kháº£ nÄƒng diá»…n giáº£i trong ICICLE, dá»±a trÃªn Ä‘á»™ tÆ°Æ¡ng tá»± vÃ  duy trÃ¬ kiáº¿n thá»©c thiáº¿t yáº¿u tá»« cÃ¡c nhiá»‡m vá»¥ trÆ°á»›c Ä‘Ã³ trong khi giá»¯ láº¡i tÃ­nh dáº»o cao Ä‘á»ƒ há»c nhá»¯ng nhiá»‡m vá»¥ má»›i. HÃ¬nh 7 minh há»a ba phÆ°Æ¡ng phÃ¡p nÃ y vÃ  so sÃ¡nh chÃºng vá» Ä‘á»™ chÃ­nh xÃ¡c trung bÃ¬nh tÄƒng dáº§n cho ProtoPNet chá»‰ vá»›i Ä‘iá»u chuáº©n (khÃ´ng thay Ä‘á»•i khá»Ÿi táº¡o): 0.507, 0.535, vÃ  0.559 trong biáº¿t nhiá»‡m vá»¥ vÃ  0.261, 0.230, vÃ  0.280 trong cÃ¡c tÃ¬nh huá»‘ng khÃ´ng biáº¿t nhiá»‡m vá»¥ cho Ä‘iá»u chuáº©n Ä‘áº·c trÆ°ng, khoáº£ng cÃ¡ch, vÃ  dá»±a trÃªn Ä‘á»™ tÆ°Æ¡ng tá»±, tÆ°Æ¡ng á»©ng, trÃªn táº­p dá»¯ liá»‡u CUB-200-2011 vá»›i tÃ¬nh huá»‘ng bá»‘n nhiá»‡m vá»¥.

áº¢nh hÆ°á»Ÿng cá»§a cÃ¡c siÃªu tham sá»‘ trong Ä‘iá»u chuáº©n kháº£ nÄƒng diá»…n giáº£i lÃ  gÃ¬? Trong HÃ¬nh 8 vÃ  HÃ¬nh 9, áº£nh hÆ°á»Ÿng cá»§a Î» vÃ  ngÆ°á»¡ng phÃ¢n vá»‹ máº·t náº¡ trong Ä‘iá»u chuáº©n kháº£ nÄƒng diá»…n giáº£i Ä‘áº¿n Ä‘á»™ chÃ­nh xÃ¡c trung bÃ¬nh tÄƒng dáº§n Ä‘Æ°á»£c trÃ¬nh bÃ y. ChÃºng tÃ´i sá»­ dá»¥ng táº­p dá»¯ liá»‡u CUB-200-2011 vá»›i thiáº¿t láº­p chia bá»‘n nhiá»‡m vá»¥. Äá»‘i vá»›i táº­p dá»¯ liá»‡u nÃ y, káº¿t quáº£ tiáº¿t lá»™ ráº±ng Ä‘iá»u chuáº©n chá»‰ Ä‘á»™ tÆ°Æ¡ng tá»± nguyÃªn máº«u tá»‘i Ä‘a lÃ  hiá»‡u quáº£ nháº¥t (HÃ¬nh 9). LiÃªn quan Ä‘áº¿n Î»IR, má»™t giÃ¡ trá»‹ quÃ¡ nhá» dáº«n Ä‘áº¿n tÃ­nh dáº»o máº¡ng cao, quÃªn lÃ£ng tÄƒng, vÃ  káº¿t quáº£ kÃ©m, trong khi má»™t giÃ¡ trá»‹ quÃ¡ lá»›n giáº£m tÃ­nh dáº»o mÃ´ hÃ¬nh vÃ  cÃ³ thá»ƒ khÃ´ng Ä‘áº¡i diá»‡n tá»‘t kiáº¿n thá»©c má»›i.

CÃ¡ch nÃ o lÃ  tá»‘t nháº¥t Ä‘á»ƒ khá»Ÿi táº¡o cÃ¡c pháº§n nguyÃªn máº«u má»›i? Trong pháº§n loáº¡i bá» nÃ y, chÃºng tÃ´i Ä‘iá»u tra chiáº¿n lÆ°á»£c tá»‘i Æ°u Ä‘á»ƒ khá»Ÿi táº¡o cÃ¡c pháº§n nguyÃªn máº«u á»Ÿ Ä‘áº§u nhiá»‡m vá»¥ má»›i trong mÃ´ hÃ¬nh ProtoPNet. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ phÆ°Æ¡ng phÃ¡p khá»Ÿi táº¡o cá»§a chÃºng tÃ´i, khá»Ÿi táº¡o cÃ¡c pháº§n gáº§n vá»›i cÃ¡c nguyÃªn máº«u hiá»‡n táº¡i, so vá»›i ba phÆ°Æ¡ng phÃ¡p khÃ¡c: khá»Ÿi táº¡o ngáº«u nhiÃªn, phÃ¢n cá»¥m táº¥t cáº£ biá»ƒu diá»…n pháº§n hÃ¬nh áº£nh, vÃ  phÃ¢n cá»¥m chá»‰ cÃ¡c vectÆ¡ áº©n xa. Káº¿t quáº£ Ä‘Æ°á»£c trÃ¬nh bÃ y trong Báº£ng 5. PhÆ°Æ¡ng phÃ¡p khá»Ÿi táº¡o Ä‘á»™ gáº§n vÆ°á»£t trá»™i so vá»›i chiáº¿n lÆ°á»£c xa, vÃ¬ chiáº¿n lÆ°á»£c sau cÃ³ xu hÆ°á»›ng gÃ¡n cÃ¡c pháº§n nguyÃªn máº«u cho cÃ¡c vectÆ¡ áº©n tÆ°Æ¡ng á»©ng vá»›i ná»n cá»§a hÃ¬nh áº£nh, dáº«n Ä‘áº¿n há»c cÃ¡c khÃ¡i niá»‡m khÃ´ng liÃªn quan cÃ³ thá»ƒ dá»… dÃ ng kÃ­ch hoáº¡t trÃªn dá»¯ liá»‡u nhiá»‡m vá»¥ khÃ¡c, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong TÃ i liá»‡u Bá»• sung.

ICICLE cÃ³ tá»•ng quÃ¡t hÃ³a cho cÃ¡c kiáº¿n trÃºc khÃ¡c khÃ´ng? Cuá»‘i cÃ¹ng, chÃºng tÃ´i cho tháº¥y ráº±ng ICICLE tá»•ng quÃ¡t hÃ³a cho kiáº¿n trÃºc dá»±a trÃªn khÃ¡i niá»‡m khÃ¡c. ChÃºng tÃ´i chá»©ng minh ráº±ng sá»­ dá»¥ng mÃ´ hÃ¬nh TesNet [85], vÃ  cung cáº¥p káº¿t quáº£ trong Báº£ng 6, nÆ¡i ICICLE Ä‘áº¡t káº¿t quáº£ tá»‘t nháº¥t. Äá»™ chÃ­nh xÃ¡c trung bÃ¬nh tÄƒng dáº§n cá»§a ICICLE vá»›i TesNet tháº­m chÃ­ cÃ²n tá»‘t hÆ¡n ProtoPNet cho cáº£ Ä‘Ã¡nh giÃ¡ biáº¿t nhiá»‡m vá»¥ vÃ  khÃ´ng biáº¿t nhiá»‡m vá»¥.

Loáº¡i khá»Ÿi táº¡o Ngáº«u nhiÃªn Xa Táº¥t cáº£ Äá»™ gáº§n
Äá»™ chÃ­nh xÃ¡c biáº¿t nhiá»‡m vá»¥ 0.559 0.592 0.626 0.654
Äá»™ chÃ­nh xÃ¡c khÃ´ng biáº¿t nhiá»‡m vá»¥ 0.280 0.290 0.297 0.335

Báº£ng 5: So sÃ¡nh cÃ¡c chiáº¿n lÆ°á»£c khá»Ÿi táº¡o khÃ¡c nhau cho cÃ¡c pháº§n nguyÃªn máº«u. Khá»Ÿi táº¡o Ä‘á»™ gáº§n cá»§a chÃºng tÃ´i cho cÃ¡c nguyÃªn máº«u nhiá»‡m vá»¥ má»›i lÃ  vÆ°á»£t trá»™i.

--- TRANG 8 ---
Freezing Finetuning EWC LWM LWF ICICLE
Äá»™ chÃ­nh xÃ¡c TAw 0.637 0.355 0.592 0.648 0.581 0.746
Äá»™ chÃ­nh xÃ¡c TAg 0.222 0.183 0.272 0.252 0.205 0.362

Báº£ng 6: Káº¿t quáº£ cho tÃ¬nh huá»‘ng há»c bá»‘n nhiá»‡m vá»¥ trÃªn táº­p dá»¯ liá»‡u CUB-200-2011 vá»›i TesNet [85] nhÆ° má»™t kiáº¿n trÃºc dá»±a trÃªn khÃ¡i niá»‡m. Báº£ng cho tháº¥y tÃ­nh linh hoáº¡t cá»§a phÆ°Æ¡ng phÃ¡p ICICLE cho cÃ¡c mÃ´ hÃ¬nh cÃ³ thá»ƒ diá»…n giáº£i.

HÃ¬nh 8: áº¢nh hÆ°á»Ÿng cá»§a Î»IR trong Ä‘iá»u chuáº©n kháº£ nÄƒng diá»…n giáº£i.

HÃ¬nh 9: áº¢nh hÆ°á»Ÿng cá»§a Î³ trong Ä‘iá»u chuáº©n kháº£ nÄƒng diá»…n giáº£i. LÆ°u Ã½ ráº±ng Ä‘iá»u chuáº©n chá»‰ táº¡i vá»‹ trÃ­ cá»§a Ä‘á»™ tÆ°Æ¡ng tá»± tá»‘i Ä‘a lÃ  cÃ³ lá»£i nháº¥t cho ICICLE trong tÃ¬nh huá»‘ng há»c bá»‘n nhiá»‡m vá»¥ trong CUB-200-2011.

6. Káº¿t luáº­n vÃ  cÃ´ng viá»‡c tÆ°Æ¡ng lai
CÃ´ng trÃ¬nh nÃ y Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p má»›i gá»i lÃ  ICICLE cho há»c tÄƒng dáº§n lá»›p cÃ³ thá»ƒ diá»…n giáº£i. ICICLE dá»±a trÃªn cÃ¡c pháº§n nguyÃªn máº«u vÃ  káº¿t há»£p Ä‘iá»u chuáº©n kháº£ nÄƒng diá»…n giáº£i, khá»Ÿi táº¡o Ä‘á»™ gáº§n, vÃ  bÃ¹ trá»« thiÃªn lá»‡ch gáº§n Ä‘Ã¢y cá»§a nhiá»‡m vá»¥. PhÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c Ä‘á» xuáº¥t vÆ°á»£t trá»™i so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p há»c tÄƒng dáº§n lá»›p cá»• Ä‘iá»ƒn Ä‘Æ°á»£c Ã¡p dá»¥ng cho máº¡ng dá»±a trÃªn pháº§n nguyÃªn máº«u vá» Ä‘á»™ chÃ­nh xÃ¡c biáº¿t nhiá»‡m vá»¥ vÃ  khÃ´ng biáº¿t nhiá»‡m vá»¥ trong khi duy trÃ¬ kháº£ nÄƒng diá»…n giáº£i nguyÃªn máº«u. ChÃºng tÃ´i cÅ©ng tiáº¿n hÃ nh cÃ¡c nghiÃªn cá»©u loáº¡i bá» vÃ  nhiá»u phÃ¢n tÃ­ch Ä‘á»ƒ biá»‡n minh cho cÃ¡c lá»±a chá»n cá»§a chÃºng tÃ´i vÃ  lÃ m ná»•i báº­t nhá»¯ng thÃ¡ch thá»©c liÃªn quan Ä‘áº¿n viá»‡c káº¿t há»£p cÃ¡c khÃ¡i niá»‡m cÃ³ thá»ƒ diá»…n giáº£i vá»›i CL. CÃ´ng trÃ¬nh nÃ y Ä‘Æ°á»£c ká»³ vá»ng sáº½ truyá»n cáº£m há»©ng cho nghiÃªn cá»©u vá» XAI vÃ  CL. Tiáº¿n vá» phÃ­a trÆ°á»›c, chÃºng tÃ´i dá»± Ä‘á»‹nh khÃ¡m phÃ¡ cÃ¡c phÆ°Æ¡ng phÃ¡p phÃ¹ há»£p cho há»c tÄƒng dáº§n lá»›p Ä‘Æ¡n vá»›i cÃ¡c mÃ´ hÃ¬nh cÃ³ thá»ƒ diá»…n giáº£i. ChÃºng tÃ´i cÅ©ng cÃ³ Ã½ Ä‘á»‹nh Ä‘iá»u tra cÃ¡ch cÃ¡c kiáº¿n trÃºc cÃ³ thá»ƒ diá»…n giáº£i khÃ¡c, nhÆ° B-COS [8], cÃ³ thá»ƒ Ä‘Æ°á»£c thÃ­ch á»©ng vá»›i tÃ¬nh huá»‘ng há»c tÄƒng dáº§n lá»›p.

Háº¡n cháº¿. CÃ´ng trÃ¬nh cá»§a chÃºng tÃ´i bá»‹ háº¡n cháº¿ Ä‘á»‘i vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p pháº§n nguyÃªn máº«u, phÃ¹ há»£p cho nháº­n dáº¡ng hÃ¬nh áº£nh phÃ¢n loáº¡i chi tiáº¿t vÃ  káº¿ thá»«a cÃ¡c nhÆ°á»£c Ä‘iá»ƒm cá»§a chÃºng Ä‘Ã£ Ä‘Æ°á»£c tháº£o luáº­n trÆ°á»›c Ä‘Ã³ trong [27, 36, 42, 60, 73]. Tuy nhiÃªn, gáº§n Ä‘Ã¢y cÃ³ nhá»¯ng cÃ´ng trÃ¬nh Ä‘áº§u tiÃªn tá»•ng quÃ¡t hÃ³a chÃºng cho cÃ¡c táº­p dá»¯ liá»‡u tiÃªu chuáº©n (khÃ´ng phÃ¢n loáº¡i chi tiáº¿t) [61]. NgoÃ i ra, vÃ¬ chÃºng tÃ´i chá»‰ xem xÃ©t tÃ¬nh huá»‘ng khÃ´ng cáº§n máº«u vÃ  nháº­n dáº¡ng táº­p Ä‘Ã³ng, chÃºng tÃ´i khÃ´ng phÃ¢n tÃ­ch cÃ¡ch cÃ³ bá»™ Ä‘á»‡m phÃ¡t láº¡i sáº½ áº£nh hÆ°á»Ÿng Ä‘áº¿n hiá»‡u suáº¥t cá»§a phÆ°Æ¡ng phÃ¡p vÃ  cÃ¡ch phÆ°Æ¡ng phÃ¡p nÃ y sáº½ phÃ¹ há»£p trong cÃ¡c thiáº¿t láº­p táº­p má»Ÿ.

TÃ¡c Ä‘á»™ng. ICICLE lÃ m ná»•i báº­t ráº±ng cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ´ng cáº§n máº«u truyá»n thá»‘ng cho há»c liÃªn tá»¥c khÃ´ng phÃ¹ há»£p tá»‘t cho cÃ¡c mÃ´ hÃ¬nh há»™p xÃ¡m sá»­ dá»¥ng khÃ¡i niá»‡m cho dá»± Ä‘oÃ¡n. PhÃ¡t hiá»‡n nÃ y cÃ³ Ã½ nghÄ©a Ä‘á»‘i vá»›i viá»‡c phÃ¡t triá»ƒn cÃ¡c phÆ°Æ¡ng phÃ¡p há»c liÃªn tá»¥c, vÃ¬ chÃºng pháº£i cÃ¢n báº±ng nhu cáº§u vá» tÃ­nh tá»•ng quÃ¡t vá»›i nhu cáº§u Ä‘Æ°á»£c thÃ­ch á»©ng cho cÃ¡c kiáº¿n trÃºc cá»¥ thá»ƒ. HÆ¡n ná»¯a, nÃ³ cÃ³ tÃ¡c Ä‘á»™ng Ä‘áº¿n lÄ©nh vá»±c cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn khÃ¡i niá»‡m vÃ  AI cÃ³ thá»ƒ giáº£i thÃ­ch, chá»©ng minh nhu cáº§u nghiÃªn cá»©u thÃªm vá» cÃ¡c phÆ°Æ¡ng phÃ¡p CL cho XAI. Trong má»™t sá»‘ trÆ°á»ng há»£p, cÃ¡c thá»±c hÃ nh viÃªn biáº¿t ráº±ng há»‡ thá»‘ng cá»§a há» sáº½ cáº§n há»c cÃ¡c nhiá»‡m vá»¥ má»›i liÃªn tá»¥c cÃ³ thá»ƒ chá»n sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh há»™p Ä‘en vÃ  bá»™ giáº£i thÃ­ch thay vÃ¬ cÃ¡c mÃ´ hÃ¬nh cÃ³ thá»ƒ diá»…n giáº£i, hy sinh Ä‘á»™ trung thá»±c cá»§a giáº£i thÃ­ch Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t mÃ´ hÃ¬nh.

Lá»i cáº£m Æ¡n
Joost van de Weijer vÃ  BartÅ‚omiej Twardowski thá»«a nháº­n sá»± há»— trá»£ tá»« tÃ i trá»£ cá»§a ChÃ­nh phá»§ TÃ¢y Ban Nha cho cÃ¡c dá»± Ã¡n PID2019-104174GB-I00, TED2021-132513B-I00, vÃ  tÃ i trá»£ RYC2021-032765-I, á»¦y ban ChÃ¢u Ã‚u theo ChÆ°Æ¡ng trÃ¬nh ChÃ¢n trá»i 2020, Ä‘Æ°á»£c tÃ i trá»£ bá»Ÿi MCIN/AEI/10.13039/501100011033 vÃ  bá»Ÿi LiÃªn minh ChÃ¢u Ã‚u NextGenerationEU/PRTR. CÃ´ng viá»‡c cá»§a Dawid Rymarczyk Ä‘Æ°á»£c há»— trá»£ bá»Ÿi Trung tÃ¢m Khoa há»c Quá»‘c gia (Ba Lan), tÃ i trá»£ sá»‘ 2022/45/N/ST6/04147. Anh áº¥y cÅ©ng nháº­n Ä‘Æ°á»£c há»c bá»•ng khuyáº¿n khÃ­ch tá»« chÆ°Æ¡ng trÃ¬nh SÃ¡ng kiáº¿n Xuáº¥t sáº¯c - Quá»¹ Äáº¡i há»c NghiÃªn cá»©u táº¡i Äáº¡i há»c Jagiellonian á»Ÿ KrakÃ³w. CÃ´ng viá»‡c cá»§a Bartosz ZieliÅ„ski Ä‘Æ°á»£c tÃ i trá»£ bá»Ÿi tÃ i trá»£ cá»§a Trung tÃ¢m Khoa há»c Quá»‘c gia (Ba Lan) sá»‘ 2022/47/B/ST6/03397 vÃ  dá»± Ã¡n nghiÃªn cá»©u "Máº¡ng nÆ¡-ron nhÃ¢n táº¡o láº¥y cáº£m há»©ng tá»« sinh há»c" (tÃ i trá»£ sá»‘ POIR.04.04.00-00-14DE/18-00) trong chÆ°Æ¡ng trÃ¬nh Team-Net cá»§a Quá»¹ Khoa há»c Ba Lan Ä‘Æ°á»£c Ä‘á»“ng tÃ i trá»£ bá»Ÿi LiÃªn minh ChÃ¢u Ã‚u theo Quá»¹ PhÃ¡t triá»ƒn Khu vá»±c ChÃ¢u Ã‚u. Cuá»‘i cÃ¹ng, má»™t sá»‘ thÃ­ nghiá»‡m Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn cÃ¡c mÃ¡y chá»§ Ä‘Æ°á»£c mua báº±ng quá»¹ tá»« tÃ i trá»£ Khu vá»±c NghiÃªn cá»©u Æ¯u tiÃªn (Trung tÃ¢m CÆ¡ sá»Ÿ TrÃ­ tuá»‡ NhÃ¢n táº¡o) theo ChÆ°Æ¡ng trÃ¬nh Chiáº¿n lÆ°á»£c SÃ¡ng kiáº¿n Xuáº¥t sáº¯c táº¡i Äáº¡i há»c Jagiellonian.

TÃ i liá»‡u tham kháº£o
[1] Ehsan Abbasnejad, Damien Teney, Amin Parvaneh, Javen Shi, and Anton van den Hengel. Counterfactual vision and language learning. In Proceedings of the IEEE/CVF Con-

--- TRANG 9 ---
ference on Computer Vision and Pattern Recognition , pages 10044â€“10054, 2020. 2
[2] Michael Anis Mihdi Afnan, Yanhe Liu, Vincent Conitzer, Cynthia Rudin, Abhishek Mishra, Julian Savulescu, and Masoud Afnan. Interpretable, not black-box, artificial intelligence should be used for embryo selection. Human Reproduction Open , 2021. 3
[3] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. In European Conference on Computer Vision , 2018. 2
[4] David Alvarez Melis and Tommi Jaakkola. Towards robust interpretability with self-explaining neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems , volume 31. Curran Associates, Inc., 2018. 2
[5] Jihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo Ha, and Jonghyun Choi. Rainbow memory: Continual learning with a memory of diverse samples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8218â€“8227, 2021. 2
[6] Alina Jade Barnett, Fides Regina Schwartz, Chaofan Tao, Chaofan Chen, Yinhao Ren, Joseph Y Lo, and Cynthia Rudin. A case-based interpretable deep learning model for classification of mass lesions in digital mammography. Nature Machine Intelligence , 3(12):1061â€“1070, 2021. 3
[7] Dominika Basaj, Witold Oleszkiewicz, Igor Sieradzki, MichaÅ‚ GÃ³Å‚rzzczak, B Rychalska, T Trzcinski, and B Zielinski. Explaining self-supervised image representations with visual probing. In International Joint Conference on Artificial Intelligence , 2021. 2
[8] Moritz BÃ¶hle, Mario Fritz, and Bernt Schiele. B-cos networks: alignment is all we need for interpretability. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 10329â€“10338, 2022. 8
[9] Wieland Brendel and Matthias Bethge. Approximating CNNs with bag-of-local-features models works surprisingly well on imagenet. In International Conference on Learning Representations , 2019. 2
[10] Lukas Brunke, Melissa Greeff, Adam W Hall, Zhaocong Yuan, Siqi Zhou, Jacopo Panerati, and Angela P Schoellig. Safe learning in robotics: From learning-based control to safe reinforcement learning. Annual Review of Control, Robotics, and Autonomous Systems , 5:411â€“444, 2022. 1
[11] Rudin C. et al. Interpretable machine learning: Fundamental principles and 10 grand challenges. Statistics Surveys , 16:1â€“85, 2022. 1, 6
[12] Rudin C. and Radin J. Why are we using black box models in ai when we don't need to? a lesson from an explainable ai competition. Harvard Data Science Review , 1(2):10â€“1162, 2019. 1
[13] Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian walk for incremental learning: understanding forgetting and intransigence. In European Conference on Computer Vision , 2018. 2
[14] Arslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient lifelong learning with a-gem. In International Conference on Learning Representations , 2019. 2
[15] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc'Aurelio Ranzato. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486 , 2019. 2
[16] Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan K Su. This looks like that: deep learning for interpretable image recognition. Advances in Neural Information Processing Systems , 32, 2019. 2, 3, 4, 5
[17] Xuxin Chen, Ximin Wang, Ke Zhang, Kar-Ming Fung, Theresa C Thai, Kathleen Moore, Robert S Mannel, Hong Liu, Bin Zheng, and Yuchen Qiu. Recent advances and clinical applications of deep learning in medical image analysis. Medical Image Analysis , page 102444, 2022. 1
[18] Zhi Chen, Yijie Bei, and Cynthia Rudin. Concept whitening for interpretable image recognition. Nature Machine Intelligence , 2(12):772â€“782, 2020. 2
[19] Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Greg Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2021. 1, 2
[20] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition , pages 248â€“255. Ieee, 2009. 5
[21] Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng, Ziyan Wu, and Rama Chellappa. Learning without memorizing. In Conference on Computer Vision and Pattern Recognition , 2019. 2, 5
[22] Jon Donnelly, Alina Jade Barnett, and Chaofan Chen. Deformable protopnet: An interpretable image classifier using deformable prototypes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 10265â€“10275, 2022. 3
[23] Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, and Eduardo Valle. Podnet: Pooled outputs distillation for small-tasks incremental learning. In Computer Visionâ€“ECCV 2020: 16th European Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part XX 16 , pages 86â€“102. Springer, 2020. 2
[24] Ruth Fong, Mandela Patrick, and Andrea Vedaldi. Understanding deep networks via extremal perturbations and smooth masks. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 2950â€“2958, 2019. 2
[25] Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturbation. In Proceedings of the IEEE international conference on computer vision , pages 3429â€“3437, 2017. 2
[26] Robert M. French. Catastrophic forgetting in connectionist networks. Trends in cog. scie. , 1999. 1
[27] Srishti Gautam, Marina M-C HÃ¶hne, Stine Hansen, Robert Jenssen, and Michael Kampffmeyer. This looks more like that: Enhancing self-explaining models by prototypical relevance propagation. Pattern Recognition , 136:109172, 2023. 8
[28] Alan H Gee, Diego Garcia-Olano, Joydeep Ghosh, and David Paydarfar. Explaining deep classification of time-series data with learned prototypes. In CEUR workshop proceedings , volume 2429, page 15. NIH Public Access, 2019. 3
[29] Amirata Ghorbani, James Wexler, James Y Zou, and Been Kim. Towards automatic concept-based explanations. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchÃ©-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019. 2
[30] Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks. In International Conference on Learning Representations , 2014. 1
[31] Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, and Stefan Lee. Counterfactual visual explanations. In International Conference on Machine Learning , pages 2376â€“2384. PMLR, 2019. 2
[32] Filip Guzy, MichaÅ‚ WoÅºniak, and Bartosz Krawczyk. Evaluating and explaining generative adversarial networks for continual learning under concept drift. In 2021 International Conference on Data Mining Workshops (ICDMW) , pages 295â€“303. IEEE, 2021. 1
[33] Peter Hase, Chaofan Chen, Oscar Li, and Cynthia Rudin. Interpretable image recognition with hierarchical prototypes. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing , volume 7, pages 32â€“40, 2019. 3
[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 770â€“778, 2016. 5
[35] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In NIPS Deep Learning Workshop , 2014. 4
[36] Adrian Hoffmann, Claudio Fanconi, Rahul Rade, and Jonas Kohler. This looks like that... does it? shortcomings of latent space prototype interpretability in deep networks, 2021. 8
[37] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In International Conference on Computer Vision , 2019. 2
[38] Xinting Hu, Kaihua Tang, Chunyan Miao, Xian-Sheng Hua, and Hanwang Zhang. Distilling causal effect of data in class-incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 3957â€“3966, 2021. 2
[39] Monish Keswani, Sriranjani Ramakrishnan, Nishant Reddy, and Vineeth N Balasubramanian. Proto2proto: Can you recognize the car, the way i do? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 10233â€“10243, 2022. 3, 4
[40] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International conference on machine learning , pages 2668â€“2677. PMLR, 2018. 2
[41] Eunji Kim, Siwon Kim, Minji Seo, and Sungroh Yoon. Xprotonet: Diagnosis in chest radiography with global and local explanations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15719â€“15728, 2021. 3
[42] Sunnie SY Kim, Nicole Meister, Vikram V Ramaswamy, Ruth Fong, and Olga Russakovsky. Hive: evaluating the human interpretability of visual explanations. In Computer Visionâ€“ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23â€“27, 2022, Proceedings, Part XII , pages 280â€“298. Springer, 2022. 8
[43] B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A Al Sallab, Senthil Yogamani, and Patrick PÃ©rez. Deep reinforcement learning for autonomous driving: A survey. IEEE Transactions on Intelligent Transportation Systems , 23(6):4909â€“4926, 2021. 1
[44] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. National Academy of Sciences , 2017. 1, 2, 5
[45] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. Concept bottleneck models. In Hal DaumÃ© III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Research , pages 5338â€“5348. PMLR, 13â€“18 Jul 2020. 2
[46] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer vision workshops , pages 554â€“561, 2013. 2, 5
[47] Oscar Li, Hao Liu, Chaofan Chen, and Cynthia Rudin. Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 32, 2018. 3
[48] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2017. 2, 4, 5
[49] Xialei Liu, Marc Masana, Luis Herranz, Joost Van de Weijer, Antonio M Lopez, and Andrew D Bagdanov. Rotate your networks: Better weight consolidation and less catastrophic forgetting. In International Conference on Pattern Recognition, 2018. 2
[50] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks by learning to mask weights. In European Conference on Computer Vision , 2018. 2
[51] Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In Conference on Computer Vision and Pattern Recognition , 2018. 2

--- TRANG 10 ---
[52] Emanuele Marconato, Gianpaolo Bontempo, Stefano Teso, Elisa Ficarra, Simone Calderara, and Andrea Passerini. Catastrophic forgetting in continual concept bottleneck models. In Image Analysis and Processing. ICIAP 2022 Workshops: ICIAP International Workshops, Lecce, Italy, May 23â€“27, 2022, Revised Selected Papers, Part II , pages 539â€“547. Springer, 2022. 1
[53] Diego Marcos, Sylvain Lobry, and Devis Tuia. Semantically interpretable activation maps: what-where-how explanations within cnns. In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW) , pages 4207â€“4215. IEEE, 2019. 2
[54] Marc Masana, Xialei Liu, Bartlomiej Twardowski, Mikel Menta, Andrew D Bagdanov, and Joost van de Weijer. Class-incremental learning: Survey and performance evaluation on image classification. IEEE Transactions on Pattern Analysis and Machine Intelligence , pages 1â€“20, 2022. 2, 5
[55] Yao Ming, Panpan Xu, Huamin Qu, and Liu Ren. Interpretable and steerable sequence learning via prototypes. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pages 903â€“913, 2019. 3
[56] Ramaravind K Mothilal, Amit Sharma, and Chenhao Tan. Explaining machine learning classifiers through diverse counterfactual explanations. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency , pages 607â€“617, 2020. 2
[57] Martin Mundt, Yongwon Hong, Iuliia Pliushch, and Visvanathan Ramesh. A wholistic view of continual learning with deep neural networks: Forgotten lessons and the bridge to active and open world learning. Neural Networks , 160:306â€“336, 2023. 2
[58] Martin Mundt, Iuliia Pliushch, Sagnik Majumder, Yongwon Hong, and Visvanathan Ramesh. Unified probabilistic deep continual learning through generative replay and open set recognition. Journal of Imaging , 8(4):93, 2022. 2
[59] Meike Nauta et al. Neural prototype trees for interpretable fine-grained image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 14933â€“14943, 2021. 3
[60] Meike Nauta, Annemarie Jutte, Jesper Provoost, and Christin Seifert. This looks like that, because... explaining prototypes for interpretable image recognition. In Machine Learning and Principles and Practice of Knowledge Discovery in Databases: International Workshops of ECML PKDD 2021, Virtual Event, September 13-17, 2021, Proceedings, Part I , pages 441â€“456. Springer, 2022. 8
[61] Meike Nauta, JÃ¶rg SchlÃ¶tterer, Maurice van Keulen, and Christin Seifert. Pip-net: Patch-based intuitive prototypes for interpretable image classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2744â€“2753, 2023. 8
[62] Yulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, Xian-Sheng Hua, and Ji-Rong Wen. Counterfactual vqa: A cause-effect look at language bias. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 12700â€“12710, 2021. 2
[63] Schramowski P. et al. Making deep neural networks right for the right scientific reasons by interacting with their explanations. Nature Machine Intelligence , 2(8):476â€“486, 2020. 1
[64] Arijit Patra and J Alison Noble. Incremental learning of fetal heart anatomies using interpretable saliency maps. In Medical Image Understanding and Analysis: 23rd Conference, MIUA 2019, Liverpool, UK, July 24â€“26, 2019, Proceedings 23, pages 129â€“141. Springer, 2020. 1
[65] GrÃ©goire Petit, Adrian Popescu, Hugo Schindler, David Picard, and Bertrand Delezoide. Fetril: Feature translation for exemplar-free class-incremental learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , pages 3911â€“3920, 2023. 6
[66] Ameya Prabhu, Philip HS Torr, and Puneet K Dokania. Gdumb: A simple approach that questions our progress in continual learning. In Computer Visionâ€“ECCV 2020: 16th European Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part II 16 , pages 524â€“540. Springer, 2020. 13
[67] Sylvestre-Alvise Rebuffi, Ruth Fong, Xu Ji, and Andrea Vedaldi. There and back again: Revisiting backpropagation saliency methods. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8839â€“8848, 2020. 2
[68] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In Conference on Computer Vision and Pattern Recognition , 2017. 2
[69] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. " why should i trust you?" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining , pages 1135â€“1144, 2016. 2
[70] Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence , 1(5):206â€“215, 2019. 1, 2
[71] Dawid Rymarczyk, Daniel Dobrowolski, and Tomasz Danel. Progrest: Prototypical graph regression soft trees for molecular property prediction. SIAM International Conference on Data Mining , 2023. 3
[72] Dawid Rymarczyk, Aneta KaczyÅ„ska, JarosÅ‚aw Kraus, Adam Pardyl, and Bartosz ZieliÅ„ski. Protomil: Multiple instance learning with prototypical parts for fine-grained interpretability. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases . Springer, 2022. 3
[73] Dawid Rymarczyk, Åukasz Struski, MichaÅ‚ GÃ³rzczak, Koryna Lewandowska, Jacek Tabor, and Bartosz ZieliÅ„ski. Interpretable image classification with differentiable prototypes assignment. In Proceedings of the European Conference on Computer Vision (ECCV) , 2022. 3, 8
[74] Dawid Rymarczyk, Åukasz Struski, Jacek Tabor, and Bartosz ZieliÅ„ski. Protopshare: Prototypical parts sharing for similarity discovery in interpretable image classification. In Proceedings of the 27th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pages 1420â€“1430, 2021. 3

--- TRANG 11 ---
[75] MikoÅ‚aj Sacha, Dawid Rymarczyk, Åukasz Struski, Jacek Tabor, and Bartosz ZieliÅ„ski. Protoseg: Interpretable semantic segmentation with prototypical parts. In Winter Conference on Applications of Computer Vision (WACV) , 2023. 3, 6
[76] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision , pages 618â€“626, 2017. 2
[77] Ramprasaath R Selvaraju, Stefan Lee, Yilin Shen, Hongxia Jin, Shalini Ghosh, Larry Heck, Dhruv Batra, and Devi Parikh. Taking a hint: Leveraging explanations to make vision and language models more grounded. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2591â€“2600, 2019. 2
[78] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In International Conference on Machine Learning , 2018. 2
[79] Yujun Shi, Li Yuan, Yunpeng Chen, and Jiashi Feng. Continual learning via bit-level information preserving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 16674â€“16683, 2021. 2
[80] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. In In Workshop at International Conference on Learning Representations . Citeseer, 2014. 2
[81] Gurmail Singh and Kin-Choong Yow. These do not look like those: An interpretable deep learning model for image recognition. IEEE Access , 9:41482â€“41493, 2021. 3
[82] Shixiang Tang, Dapeng Chen, Jinguo Zhu, Shijie Yu, and Wanli Ouyang. Layerwise optimization by gradient decomposition for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9634â€“9643, 2021. 2
[83] Gido M van de Ven and Andreas S Tolias. Three scenarios for continual learning. In NeurIPS Continual Learning Workshop , 2018. 2
[84] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011. 2, 5
[85] Jiaqi Wang et al. Interpretable image recognition by constructing transparent embedding space. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 895â€“904, 2021. 3, 7, 8
[86] Liyuan Wang, Kuo Yang, Chongxuan Li, Lanqing Hong, Zhenguo Li, and Jun Zhu. Ordisco: Effective and efficient usage of incremental unlabeled data for semi-supervised continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5383â€“5392, 2021. 2
[87] Pei Wang and Nuno Vasconcelos. Scout: Self-aware discriminant counterfactual explanations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8981â€“8990, 2020. 2
[88] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu. Large scale incremental learning. In Conference on Computer Vision and Pattern Recognition , 2019. 2
[89] Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynamically expandable representation for class incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 3014â€“3023, 2021. 2
[90] Chih-Kuan Yeh, Been Kim, Sercan Arik, Chun-Liang Li, Tomas Pfister, and Pradeep Ravikumar. On completeness-aware concept-based explanations in deep neural networks. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems , volume 33, pages 20554â€“20565. Curran Associates, Inc., 2020. 2
[91] Lu Yu, Bartlomiej Twardowski, Xialei Liu, Luis Herranz, Kai Wang, Yongmei Cheng, Shangling Jui, and Joost van de Weijer. Semantic drift compensation for class-incremental learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 6982â€“6991, 2020. 5
[92] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In International Conference on Machine Learning , 2017. 2
[93] Mengyao Zhai, Lei Chen, and Greg Mori. Hyper-lifelonggan: Scalable lifelong learning for image conditioned generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2246â€“2255, 2021. 2
[94] Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and Cheekong Lee. Protgnn: Towards self-explaining graph neural networks. 2022. 3
[95] Fei Zhu, Xu-Yao Zhang, Chuang Wang, Fei Yin, and Cheng-Lin Liu. Prototype augmentation and self-supervision for incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5871â€“5880, 2021. 5, 6

--- TRANG 12 ---
Chi tiáº¿t thiáº¿t láº­p thÃ­ nghiá»‡m bá»• sung
á» Ä‘Ã¢y chÃºng tÃ´i trÃ¬nh bÃ y chi tiáº¿t bá»• sung vá» thiáº¿t láº­p thÃ­ nghiá»‡m. ChÃºng tÃ´i thá»±c hiá»‡n tÃ¬m kiáº¿m siÃªu tham sá»‘ cho Î»dist (Î»dist âˆˆ {10.0,5.0,0.0,0.5,0.1,0.05,0.01,0.005,0.001,0.0005, 0.0001}). ChÃºng tÃ´i sá»­ dá»¥ng bá»™ tá»‘i Æ°u Adam vá»›i tá»‘c Ä‘á»™ há»c 0.001 vÃ  cÃ¡c tham sá»‘ Î²1= 0.9 vÃ  Î²2= 0.999. ChÃºng tÃ´i Ä‘áº·t kÃ­ch thÆ°á»›c batch lÃ  75 vÃ  sá»­ dá»¥ng hÃ¬nh áº£nh Ä‘áº§u vÃ o cÃ³ Ä‘á»™ phÃ¢n giáº£i 224Ã—224Ã—3. Trá»ng sá»‘ cá»§a máº¡ng Ä‘Æ°á»£c khá»Ÿi táº¡o vá»›i bá»™ khá»Ÿi táº¡o bÃ¬nh thÆ°á»ng Xavier.

ChÃºng tÃ´i thá»±c hiá»‡n huáº¥n luyá»‡n khá»Ÿi Ä‘á»™ng nÆ¡i trá»ng sá»‘ cá»§a f Ä‘Æ°á»£c Ä‘Ã³ng bÄƒng trong 10 epochs, vÃ  sau Ä‘Ã³ chÃºng tÃ´i huáº¥n luyá»‡n mÃ´ hÃ¬nh cho Ä‘áº¿n khi há»™i tá»¥ vá»›i 12 epochs dá»«ng sá»›m. ChÃºng tÃ´i sá»­ dá»¥ng lÆ°á»£c Ä‘á»“ há»c Ä‘Æ°á»£c trÃ¬nh bÃ y trong Báº£ng 7. TÃ¹y thuá»™c vÃ o sá»‘ lÆ°á»£ng nhiá»‡m vá»¥, chÃºng tÃ´i thá»±c hiá»‡n huáº¥n luyá»‡n khá»Ÿi Ä‘á»™ng vá»›i {5,5,4} epochs vÃ  giai Ä‘oáº¡n huáº¥n luyá»‡n káº¿t há»£p cho {21,15,11}, dÃ i hÆ¡n vá»›i Ã­t nhiá»‡m vá»¥ hÆ¡n. TÆ°Æ¡ng tá»±, chÃºng tÃ´i thá»±c hiá»‡n chiáº¿u nguyÃªn máº«u má»—i {10,7,5} epoch. VÃ¬ váº­y vá»›i nhiá»u nhiá»‡m vá»¥ hÆ¡n, chÃºng tÃ´i thá»±c hiá»‡n Ã­t epochs huáº¥n luyá»‡n hÆ¡n (Báº£ng 7).

Káº¿t quáº£ trÃªn Stanford Cars
Báº£ng 8 mÃ´ táº£ cÃ¡ch phÆ°Æ¡ng phÃ¡p ICICLE hoáº¡t Ä‘á»™ng trÃªn táº­p dá»¯ liá»‡u Stanford Cars so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p cÆ¡ sá»Ÿ khÃ¡c. Káº¿t quáº£ nháº¥t quÃ¡n vá»›i nhá»¯ng káº¿t quáº£ trÃªn CUB-200-2011 vÃ  cho tháº¥y ICICLE vÆ°á»£t trá»™i so vá»›i táº¥t cáº£ cÃ¡c phÆ°Æ¡ng phÃ¡p há»c CL tiÃªu chuáº©n Ä‘Æ°á»£c thÃ­ch á»©ng cho kiáº¿n trÃºc ProtoPNet.

So sÃ¡nh vá»›i GDumb
NgoÃ i ra, chÃºng tÃ´i so sÃ¡nh phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i vá»›i GDumb [66], má»™t phÆ°Æ¡ng phÃ¡p cÆ¡ sá»Ÿ trong há»c CL, trong cÃ¡c tÃ¬nh huá»‘ng liÃªn quan Ä‘áº¿n 3, 5, vÃ  10 hÃ¬nh áº£nh trÃªn má»—i lá»›p vá»›i há»c 4 nhiá»‡m vá»¥ Ä‘áº¡t 20.3, 34.2, 57.6, vÃ  13.0, 26.7, 48.8 cho biáº¿t nhiá»‡m vá»¥ vÃ  khÃ´ng biáº¿t nhiá»‡m vá»¥ tÆ°Æ¡ng á»©ng. ICICLE vÆ°á»£t trá»™i so vá»›i GDumb vá»›i sá»‘ lÆ°á»£ng vÃ­ dá»¥ nhá», vÃ  biáº¿t nhiá»‡m vá»¥ cho GDumb-10 lÃ  ngoáº¡i lá»‡ duy nháº¥t nÆ¡i GDumb Ä‘áº¡t Ä‘iá»ƒm Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n.

Khá»Ÿi táº¡o xa
Trong Báº£ng 5, chÃºng tÃ´i cho tháº¥y ráº±ng khá»Ÿi táº¡o dá»±a trÃªn Ä‘á»™ gáº§n lÃ  cÃ³ lá»£i nháº¥t. Tuy nhiÃªn, á»Ÿ Ä‘Ã¢y trong HÃ¬nh 10, chÃºng tÃ´i cho tháº¥y cÃ¡ch khá»Ÿi táº¡o cÃ¡c pháº§n nguyÃªn máº«u á»Ÿ khoáº£ng cÃ¡ch tá»« nhá»¯ng pháº§n Ä‘Ã£ tá»“n táº¡i má»™t láº§n táº¡o ra cÃ¡c khÃ¡i niá»‡m quÃ¡ tá»•ng quÃ¡t hoáº·c mang thÃ´ng tin vá» ná»n.

BÃ¹ trá»« thiÃªn lá»‡ch gáº§n Ä‘Ã¢y cá»§a nhiá»‡m vá»¥
á» Ä‘Ã¢y, trong Báº£ng 9 chÃºng tÃ´i cho tháº¥y áº£nh hÆ°á»Ÿng cá»§a bÃ¹ trá»« thiÃªn lá»‡ch gáº§n Ä‘Ã¢y cá»§a nhiá»‡m vá»¥ Ä‘áº¿n Ä‘á»™ chÃ­nh xÃ¡c tá»«ng nhiá»‡m vá»¥ cho tÃ¬nh huá»‘ng khÃ´ng biáº¿t nhiá»‡m vá»¥. Sau bÃ¹ trá»«, Ä‘á»™ chÃ­nh xÃ¡c trÃªn nhiá»‡m vá»¥ 1 tÄƒng nhiá»u nháº¥t, nhÆ°ng cÃ¹ng lÃºc Ä‘á»™ chÃ­nh xÃ¡c trÃªn táº¥t cáº£ cÃ¡c nhiá»‡m vá»¥ khÃ¡c bá»‹ hy sinh. Tuy nhiÃªn, Ä‘á»™ chÃ­nh xÃ¡c trung bÃ¬nh sau bÃ¹ trá»« Ä‘Æ°á»£c tÄƒng lÃªn.

HÃ¬nh 10: HÃ¬nh áº£nh mÃ´ táº£ chiáº¿u TSNE 2D cá»§a cÃ¡c nguyÃªn máº«u. NgÆ°á»i ta cÃ³ thá»ƒ quan sÃ¡t ráº±ng cÃ³ má»™t cá»¥m cÃ¡c nguyÃªn máº«u tá»« táº¥t cáº£ cÃ¡c nhiá»‡m vá»¥ ngoáº¡i trá»« nhiá»‡m vá»¥ Ä‘áº§u tiÃªn (há»™p vÃ ng). Äiá»u nÃ y cÃ³ thá»ƒ quan sÃ¡t Ä‘Æ°á»£c vá»›i khá»Ÿi táº¡o xa vÃ  hÃ¬nh áº£nh nguyÃªn máº«u cho tháº¥y ráº±ng nhá»¯ng pháº§n nguyÃªn máº«u Ä‘Ã³ Ä‘ang Ä‘áº¡i diá»‡n cho ná»n hoáº·c cÃ¡c khÃ¡i niá»‡m mÆ¡ há»“.

Káº¿t quáº£ chi tiáº¿t sau khi há»c tá»«ng nhiá»‡m vá»¥
Trong Báº£ng 10, vÃ  Báº£ng 11 chÃºng tÃ´i cho tháº¥y Ä‘á»™ chÃ­nh xÃ¡c chi tiáº¿t cá»§a ICICLE sau khi há»c tá»«ng nhiá»‡m vá»¥ cho CUB-200-2011 trong má»™t láº§n cháº¡y duy nháº¥t vá»›i cÃ¹ng seed trong cÃ¡c tÃ¬nh huá»‘ng há»c bá»‘n vÃ  mÆ°á»i nhiá»‡m vá»¥.

PhÃ¢n tÃ­ch cÃ¡c siÃªu tham sá»‘ cho cÃ¡c cÆ¡ sá»Ÿ
Trong Báº£ng 12, Báº£ng 13, vÃ  Báº£ng 14 chÃºng tÃ´i cho tháº¥y áº£nh hÆ°á»Ÿng cá»§a cÃ¡c siÃªu tham sá»‘ cho tá»«ng phÆ°Æ¡ng phÃ¡p cÆ¡ sá»Ÿ, EWC, LWF, vÃ  LWM, tÆ°Æ¡ng á»©ng. Dá»±a trÃªn Ä‘Ã³, cÃ¡c tham sá»‘ cá»§a nhá»¯ng phÆ°Æ¡ng phÃ¡p nÃ y Ä‘Æ°á»£c chá»n Ä‘á»ƒ so sÃ¡nh vá»›i ICICLE.

--- TRANG 13 ---
Giai Ä‘oáº¡n Lá»›p mÃ´ hÃ¬nh Tá»‘c Ä‘á»™ há»c Bá»™ láº­p lá»‹ch Suy giáº£m trá»ng sá»‘ Thá»i lÆ°á»£ng
Khá»Ÿi Ä‘á»™ng add-on tÃ­ch cháº­p 1Ã—1 1Â·10âˆ’3
KhÃ´ng cÃ³ KhÃ´ng cÃ³ 5,5,4 epochs lá»›p nguyÃªn máº«u 1Â·10âˆ’3
Káº¿t há»£p tÃ­ch cháº­p f 1Â·10âˆ’4
giáº£m má»™t ná»­a má»—i 5 epochs 10âˆ’4 21,15,10 epochs dá»«ng sá»›m add-on tÃ­ch cháº­p 1Ã—1 1Â·10âˆ’3
lá»›p nguyÃªn máº«u 1Â·10âˆ’3

Báº£ng 7: LÆ°á»£c Ä‘á»“ há»c cho phÆ°Æ¡ng phÃ¡p ICICLE.

Äá»˜ CHÃNH XÃC TRUNG BÃŒNH TÄ‚NG Dáº¦N BIáº¾T NHIá»†M Vá»¤ Äá»˜ CHÃNH XÃC TRUNG BÃŒNH TÄ‚NG Dáº¦N KHÃ”NG BIáº¾T NHIá»†M Vá»¤
PHÆ¯Æ NG PHÃP 4NHIá»†M Vá»¤ 7NHIá»†M Vá»¤ 14NHIá»†M Vá»¤ 4NHIá»†M Vá»¤ 7NHIá»†M Vá»¤ 14NHIá»†M Vá»¤
FREEZING 0.572Â±0.031 0.518Â±0.041 0.486Â±0.026 0.309Â±0.012 0.155Â±0.031 0.092Â±0.014
FINETUNING 0.216Â±0.009 0.167Â±0.011 0.149Â±0.012 0.182Â±0.006 0.124Â±0.013 0.057Â±0.001
EWC 0.456Â±0.021 0.315Â±0.037 0.287Â±0.041 0.258Â±0.019 0.152Â±0.022 0.011Â±0.009
LWM 0.459Â±0.072 0.416Â±0.048 0.305Â±0.022 0.233Â±0.026 0.171Â±0.016 0.080Â±0.008
LWF 0.375Â±0.021 0.356Â±0.024 0.250Â±0.020 0.230Â±0.011 0.171Â±0.005 0.092Â±0.008
ICICLE 0.654Â±0.014 0.645Â±0.003 0.583Â±0.048 0.335Â±0.005 0.203Â±0.010 0.116Â±0.018

Báº£ng 8: So sÃ¡nh Ä‘á»™ chÃ­nh xÃ¡c trung bÃ¬nh tÄƒng dáº§n cho sá»‘ lÆ°á»£ng nhiá»‡m vá»¥ khÃ¡c nhau trÃªn Stanford Cars, chá»©ng minh tÃ¡c Ä‘á»™ng tiÃªu cá»±c cá»§a sá»‘ lÆ°á»£ng nhiá»‡m vá»¥ cao cáº§n Ä‘Æ°á»£c há»c Ä‘áº¿n hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh. Báº¥t cháº¥p xu hÆ°á»›ng nÃ y, ICICLE vÆ°á»£t trá»™i so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p cÆ¡ sá»Ÿ trÃªn táº¥t cáº£ sá»‘ lÆ°á»£ng nhiá»‡m vá»¥.

NHIá»†M Vá»¤ 1 NHIá»†M Vá»¤ 2 NHIá»†M Vá»¤ 3 NHIá»†M Vá»¤ 4 TRUNG BÃŒNH
BIáº¾T NHIá»†M Vá»¤ 0.514 0.717 0.725 0.698 0.663
TRÆ¯á»šC 0.028 0.301 0.434 0.575 0.335
SAU 0.233 0.365 0.314 0.486 0.350

Báº£ng 9: áº¢nh hÆ°á»Ÿng bÃ¹ trá»« thiÃªn lá»‡ch gáº§n Ä‘Ã¢y cá»§a nhiá»‡m vá»¥ trong má»™t láº§n cháº¡y. Káº¿t quáº£ cho tháº¥y phÆ°Æ¡ng phÃ¡p bÃ¹ trá»« cá»§a chÃºng tÃ´i cÃ¢n báº±ng hÆ¡n káº¿t quáº£ cho tá»«ng nhiá»‡m vá»¥ trong tÃ¬nh huá»‘ng khÃ´ng biáº¿t nhiá»‡m vá»¥.

--- TRANG 14 ---
Äá»˜ CHÃNH XÃC BIáº¾T NHIá»†M Vá»¤ Äá»˜ CHÃNH XÃC KHÃ”NG BIáº¾T NHIá»†M Vá»¤
Sau NHIá»†M Vá»¤ 1 NHIá»†M Vá»¤ 2 NHIá»†M Vá»¤ 3 NHIá»†M Vá»¤ 4 TRUNG BÃŒNH NHIá»†M Vá»¤ 1 NHIá»†M Vá»¤ 2 NHIá»†M Vá»¤ 3 NHIá»†M Vá»¤ 4 TRUNG BÃŒNH
NHIá»†M Vá»¤ 1 0.806 NA NA NA 0.806 0.806 NA NA NA 0.806
NHIá»†M Vá»¤ 2 0.740 0.759 NA NA 0.750 0.089 0.747 NA NA 0.418
NHIá»†M Vá»¤ 3 0.622 0.736 0.759 NA 0.706 0.033 0.633 0.549 NA 0.404
NHIá»†M Vá»¤ 4 0.514 0.717 0.725 0.698 0.663 0.028 0.484 0.378 0.505 0.349

Báº£ng 10: Káº¿t quáº£ cá»§a phÆ°Æ¡ng phÃ¡p ICICLE trÆ°á»›c bÃ¹ trá»« thiÃªn lá»‡ch gáº§n Ä‘Ã¢y cá»§a nhiá»‡m vá»¥ cho tÃ¬nh huá»‘ng há»c bá»‘n nhiá»‡m vá»¥ sau má»—i táº­p huáº¥n luyá»‡n.

Äá»˜ CHÃNH XÃC BIáº¾T NHIá»†M Vá»¤
Sau NHIá»†M Vá»¤ 1 NHIá»†M Vá»¤ 2 NHIá»†M Vá»¤ 3 NHIá»†M Vá»¤ 4 NHIá»†M Vá»¤ 5 NHIá»†M Vá»¤ 6 NHIá»†M Vá»¤ 7 NHIá»†M Vá»¤ 8 NHIá»†M Vá»¤ 9 NHIá»†M Vá»¤ 10 TRUNG BÃŒNH
NHIá»†M Vá»¤ 1 0.920 NA NA NA NA NA NA NA NA NA 0.920
NHIá»†M Vá»¤ 2 0.666 0.869 NA NA NA NA NA NA NA NA 0.767
NHIá»†M Vá»¤ 3 0.462 0.818 0.858 NA NA NA NA NA NA NA 0.713
NHIá»†M Vá»¤ 4 0.420 0.751 0.774 0.774 NA NA NA NA NA NA 0.680
NHIá»†M Vá»¤ 5 0.314 0.625 0.680 0.672 0.784 NA NA NA NA NA 0.615
NHIá»†M Vá»¤ 6 0.268 0.538 0.627 0.617 0.760 0.747 NA NA NA NA 0.593
NHIá»†M Vá»¤ 7 0.265 0.476 0.584 0.617 0.713 0.706 0.769 NA NA NA 0.590
NHIá»†M Vá»¤ 8 0.258 0.413 0.551 0.555 0.667 0.634 0.741 0.764 NA NA 0.573
NHIá»†M Vá»¤ 9 0.253 0.398 0.494 0.492 0.598 0.587 0.701 0.745 0.852 NA 0.569
NHIá»†M Vá»¤ 10 0.244 0.371 0.462 0.441 0.573 0.560 0.667 0.729 0.816 0.803 0.567

Äá»˜ CHÃNH XÃC KHÃ”NG BIáº¾T NHIá»†M Vá»¤
NHIá»†M Vá»¤ 1 NHIá»†M Vá»¤ 2 NHIá»†M Vá»¤ 3 NHIá»†M Vá»¤ 4 NHIá»†M Vá»¤ 5 NHIá»†M Vá»¤ 6 NHIá»†M Vá»¤ 7 NHIá»†M Vá»¤ 8 NHIá»†M Vá»¤ 9 NHIá»†M Vá»¤ 10 TRUNG BÃŒNH
NHIá»†M Vá»¤ 1 0.920 NA NA NA NA NA NA NA NA NA 0.920
NHIá»†M Vá»¤ 2 0.010 0.869 NA NA NA NA NA NA NA NA 0.439
NHIá»†M Vá»¤ 3 0.0 0.060 0.854 NA NA NA NA NA NA NA 0.305
NHIá»†M Vá»¤ 4 0.0 0.0 0.339 0.751 NA NA NA NA NA NA 0.273
NHIá»†M Vá»¤ 5 0.0 0.0 0.030 0.323 0.746 NA NA NA NA NA 0.220
NHIá»†M Vá»¤ 6 0.0 0.0 0.004 0.090 0.451 0.684 NA NA NA NA 0.205
NHIá»†M Vá»¤ 7 0.0 0.0 0.0 0.020 0.193 0.432 0.712 NA NA NA 0.194
NHIá»†M Vá»¤ 8 0.0 0.0 0.0 0.020 0.073 0.233 0.497 0.643 NA NA 0.181
NHIá»†M Vá»¤ 9 0.0 0.0 0.0 0.0 0.035 0.138 0.338 0.435 0.676 NA 0.180
NHIá»†M Vá»¤ 10 0.0 0.0 0.0 0.0 0.016 0.070 0.214 0.285 0.484 0.643 0.171

Báº£ng 11: Káº¿t quáº£ cá»§a phÆ°Æ¡ng phÃ¡p ICICLE trÆ°á»›c bÃ¹ trá»« thiÃªn lá»‡ch gáº§n Ä‘Ã¢y cá»§a nhiá»‡m vá»¥ cho tÃ¬nh huá»‘ng há»c mÆ°á»i nhiá»‡m vá»¥ sau má»—i táº­p huáº¥n luyá»‡n.

Äá»˜ CHÃNH XÃC BIáº¾T NHIá»†M Vá»¤ Äá»˜ CHÃNH XÃC KHÃ”NG BIáº¾T NHIá»†M Vá»¤
Î± 0.01 0.1 1.0 5.0 10.0 0.01 0.1 1.0 5.0 10.0
0.185 0.329 0.441 0.197 0.167 0.170 0.185 0.213 0.168 0.144

Báº£ng 12: áº¢nh hÆ°á»Ÿng cá»§a tham sá»‘ alpha trong EWC Ä‘áº¿n Ä‘á»™ chÃ­nh xÃ¡c cá»§a kiáº¿n trÃºc ProtoPNet trong tÃ¬nh huá»‘ng há»c bá»‘n nhiá»‡m vá»¥.

Äá»˜ CHÃNH XÃC BIáº¾T NHIá»†M Vá»¤ Äá»˜ CHÃNH XÃC KHÃ”NG BIáº¾T NHIá»†M Vá»¤
Î³ 0.001 0.01 0.1 1.0 10.0 0.001 0.01 0.1 1.0 10.0
0.240 0.240 0.431 0.355 0.231 0.209 0.209 0.212 0.209 0.209

Báº£ng 13: áº¢nh hÆ°á»Ÿng cá»§a tham sá»‘ Î³ trong LWM Ä‘áº¿n Ä‘á»™ chÃ­nh xÃ¡c cá»§a kiáº¿n trÃºc ProtoPNet trong tÃ¬nh huá»‘ng há»c bá»‘n nhiá»‡m vá»¥.

Äá»˜ CHÃNH XÃC BIáº¾T NHIá»†M Vá»¤ Äá»˜ CHÃNH XÃC KHÃ”NG BIáº¾T NHIá»†M Vá»¤
Î» 0.001 0.01 0.1 1.0 10.0 0.001 0.01 0.1 1.0 10.0
0.232 0.232 0.238 0.359 0.249 0.207 0.210 0.210 0.231 0.221

Báº£ng 14: áº¢nh hÆ°á»Ÿng cá»§a tham sá»‘ Î» trong LWF Ä‘áº¿n Ä‘á»™ chÃ­nh xÃ¡c cá»§a kiáº¿n trÃºc ProtoPNet trong tÃ¬nh huá»‘ng há»c bá»‘n nhiá»‡m vá»¥.