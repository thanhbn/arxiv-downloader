# Phân tích Nhân quả cho Khả năng Diễn giải Mạnh mẽ của Mạng Nơ-ron

Ola Ahmad
Thales Digital Solutions
Montreal, Canada

Nicolas Bereux*
Université Paris Sud
Paris, France

Loïc Baret
Thales Digital Solutions
Quebec, Canada

Vahid Hashemi
AUDI
Ingolstadt, Germany

Freddy Lecue*
J.P. Morgan, Chase & Co
New York, USA

## Tóm tắt

Việc diễn giải chức năng bên trong của mạng nơ-ron là rất quan trọng cho việc phát triển và triển khai đáng tin cậy của các mô hình hộp đen này. Các phương pháp diễn giải trước đây tập trung vào các biện pháp dựa trên tương quan để gán quyết định của mô hình cho từng ví dụ riêng lẻ. Tuy nhiên, những biện pháp này dễ bị ảnh hưởng bởi nhiễu và các tương quan giả mạo được mã hóa trong mô hình trong giai đoạn huấn luyện (ví dụ: đầu vào thiên lệch, mô hình overfitting, hoặc xác định sai). Hơn nữa, quá trình này đã được chứng minh là dẫn đến các thuộc tính nhiễu và không ổn định ngăn cản mọi hiểu biết minh bạch về hành vi của mô hình. Trong bài báo này, chúng tôi phát triển một phương pháp can thiệp mạnh mẽ dựa trên phân tích nhân quả để nắm bắt các cơ chế nhân quả trong mạng nơ-ron đã được huấn luyện và mối quan hệ của chúng với dự đoán. Cách tiếp cận mới của chúng tôi dựa vào can thiệp đường dẫn để suy luận các cơ chế nhân quả trong các lớp ẩn và tách biệt thông tin liên quan và cần thiết (đối với dự đoán mô hình), tránh những thông tin nhiễu. Kết quả là các đồ thị giải thích nhân quả cụ thể cho từng tác vụ có thể kiểm tra hành vi mô hình và thể hiện các nguyên nhân thực tế làm nền tảng cho hiệu suất của nó. Chúng tôi áp dụng phương pháp của mình cho các mô hình thị giác được huấn luyện trên các tác vụ phân loại. Với các tác vụ phân loại hình ảnh, chúng tôi cung cấp các thí nghiệm định lượng rộng rãi để chỉ ra rằng cách tiếp cận của chúng tôi có thể nắm bắt các giải thích ổn định và trung thực hơn so với các phương pháp thuộc tính tiêu chuẩn. Hơn nữa, các đồ thị nhân quả cơ bản tiết lộ các tương tác nơ-ron trong mô hình, làm cho nó trở thành một công cụ có giá trị trong các ứng dụng khác (ví dụ: sửa chữa mô hình).

*Công việc này đã được thực hiện khi các tác giả đang làm việc tại Thales Digital Solutions Inc.

## 1. Giới thiệu

Khả năng giải thích và diễn giải là rất quan trọng đối với mạng nơ-ron sâu (DNN), được phổ biến trong nhiều ứng dụng, bao gồm thị giác và xử lý ngôn ngữ tự nhiên. Mặc dù phổ biến, bản chất mờ đục của chúng hạn chế việc áp dụng các mô hình "hộp đen" này trong các lĩnh vực yêu cầu quyết định quan trọng mà không có khả năng hiểu hành vi của chúng. Các nỗ lực cung cấp hiểu biết minh bạch về các hệ thống DNN đã dẫn đến việc phát triển nhiều phương pháp diễn giải. Hầu hết chúng tập trung vào việc diễn giải chức năng của DNN thông qua các biện pháp dựa trên tương quan, gán quyết định của mô hình cho các đầu vào riêng lẻ [31]. Những phương pháp phổ biến nhất là các phương pháp độ nổi bật (hoặc thuộc tính đặc trưng) [30, 28, 34, 32, 2, 26, 17, 8].

Các phương pháp độ nổi bật nhằm giúp người dùng hiểu tại sao DNN đưa ra quyết định cụ thể bằng cách giải thích toàn bộ mô hình. Tuy nhiên, chúng tôi quan sát thấy hai hạn chế đáng kể của những phương pháp này. Thứ nhất, chúng không thể giải thích chức năng bên trong của hệ thống nơ-ron đang được kiểm tra. Điều đó có nghĩa là cách các nơ-ron bên trong tương tác với nhau để đạt được dự đoán cụ thể. Như được báo cáo trong [6], rất khó để xác minh các tuyên bố về các mô hình hộp đen mà không có giải thích về hoạt động bên trong của chúng. Hạn chế thứ hai, chúng dễ bị ảnh hưởng bởi nhiễu và tương quan giả mạo. Cho dù do tính chất của hệ thống DNN thu được trong giai đoạn huấn luyện (ví dụ: đầu vào thiên lệch, overfitting, hoặc xác định sai) hay phương pháp được sử dụng để nắm bắt độ nổi bật [14, 15] như được hiển thị trong Hình 1). Thay vào đó, một số phương pháp tìm cách hình dung hành vi của các nơ-ron cụ thể [19] nhưng không thể cung cấp cái nhìn sâu sắc rõ ràng do số lượng lớn và kiến trúc tổng thể phức tạp của chúng.

Trong bài báo này, chúng tôi đề xuất một phương pháp mới giải quyết các hạn chế trên thông qua góc độ nhân quả. Chúng tôi chỉ ra rằng một kỹ thuật dựa trên lý thuyết suy luận nhân quả cung cấp các diễn giải mạnh mẽ và trung thực về hành vi mô hình trong khi có thể tiết lộ các tương tác nơ-ron của nó. Lấy cảm hứng từ khoa học thần kinh, chúng tôi phân tích tác động của từng nơ-ron lên dự đoán mô hình bằng cách can thiệp vào các kết nối của chúng (trọng số hoặc bộ lọc của mô hình).

Chúng tôi tóm tắt các đóng góp của mình như sau. a) Chúng tôi đề xuất một cách tiếp cận diễn giải mạnh mẽ để nắm bắt ngữ nghĩa có ý nghĩa và giải thích hoạt động bên trong của DNN. b) Phương pháp của chúng tôi dựa vào can thiệp đường dẫn và mối quan hệ nhân quả, cung cấp các giải thích ổn định và nhất quán. Cụ thể hơn, chúng tôi tìm cách trả lời các câu hỏi như: liệu dự đoán của mô hình có cao hơn nếu chúng ta ngăn chặn luồng tín hiệu qua các đường dẫn cụ thể không? hoặc, quyết định của mô hình sẽ là gì nếu chúng ta làm giảm hoặc loại bỏ một thành phần riêng lẻ hoặc một tập hợp các thành phần tại một lớp cụ thể?. Phân tích của chúng tôi sẽ dẫn đến việc xác định vị trí và tách biệt thông tin liên quan và cần thiết được kết nối mạnh mẽ và có nguyên nhân với dự đoán mô hình đến mức kiểm định ý nghĩa. c) Chúng tôi áp dụng phương pháp của mình cho các mô hình thị giác được huấn luyện để phân loại dữ liệu MNIST, CIFAR10 và ImageNet. d) Chúng tôi cung cấp một khung linh hoạt có thể được áp dụng cho các kiến trúc phức tạp và các tác vụ khác ngoài diễn giải.

## 2. Công trình liên quan

**Các phương pháp Diễn giải và Thuộc tính.** Khả năng diễn giải cho mạng nơ-ron sâu nhằm cung cấp cái nhìn sâu sắc về hành vi của các mô hình hộp đen. Một họ rộng lớn các phương pháp đã được phát triển trong vài năm qua. Các kỹ thuật phổ biến nhất là các phương pháp thuộc tính gán điểm cho các đặc trưng đầu vào chỉ ra sự đóng góp của từng đặc trưng vào dự đoán mô hình. Các phương pháp dựa trên gradient [30, 26, 2, 33, 29, 27] truyền gradient của các mô hình đã được huấn luyện từ đầu ra ngược về đầu vào. Các nghiên cứu gần đây đã chỉ ra rằng những phương pháp này tạo ra các thuộc tính nhiễu và không ổn định [14, 1]. Các phương pháp dựa trên nhiễu loạn [20, 32, 21] là những lựa chọn thay thế tập trung vào tương quan giữa các nhiễu loạn cục bộ của đầu vào thô và đầu ra mô hình. Chúng là các phương pháp hộp đen theo nghĩa là chúng không yêu cầu truy cập vào trạng thái bên trong của mô hình. Ngoài những kỹ thuật được sử dụng rộng rãi này, nhiều phương pháp diễn giải khác nhau đã được đề xuất [36]. Gần với công việc của chúng tôi là [35]. Họ đề xuất tách rời kiến thức ẩn trong cấu trúc bên trong của DNN bằng cách học một mô hình đồ thị. Công việc của họ tập trung vào mạng nơ-ron tích chập (CNN), nơi họ khớp các kích hoạt giữa các lớp lân cận. Cách tiếp cận của chúng tôi khác về những gì nó coi là đồ thị giải thích và cách nó suy luận chúng. Chúng tôi dựa vào phân tích nhân quả, gần đây đã được coi là một công cụ hiệu quả cho khả năng diễn giải và giải thích DNN. Khung của chúng tôi không giả định một loại mạng nơ-ron cụ thể, điều này làm cho cách tiếp cận trở nên chung chung và linh hoạt.

**Nắm bắt Giải thích với Nhân quả.** Gần đây hơn, các cách tiếp cận nhân quả đã được xem xét để diễn giải DNN. Cấu trúc bên trong của DNN đã được xem lần đầu tiên như một mô hình nhân quả cấu trúc (SCM) trong [7]. Họ sử dụng SCM để phát triển một phương pháp thuộc tính tính toán tác động nhân quả của mỗi đặc trưng đầu vào lên đầu ra của mạng nơ-ron hồi quy. Các cách tiếp cận nhân quả khác được phát triển đặc biệt để giải thích các mô hình ngôn ngữ dựa trên NLP, chẳng hạn như phân tích trung gian nhân quả [31] và trừu tượng nhân quả [9]. Ngược lại, [25] đã phát triển một cách tiếp cận bất khả tri mô hình (CXPlain) để ước tính tầm quan trọng của đặc trưng cho diễn giải mô hình. Họ sử dụng một mục tiêu nhân quả để huấn luyện một mô hình giám sát riêng biệt (U-net) để học các giải thích nhân quả cho một mô hình hộp đen khác. Một hạn chế quan trọng của phương pháp này là nó phải được huấn luyện để học cách giải thích mô hình mục tiêu. Một điểm khác là tính chất nhân quả của nó bị giới hạn ở tác động bên ngoài của đầu vào trong việc gây ra thay đổi biên trong đầu ra. Do đó, nó không thể liên kết các giải thích với cấu trúc bên trong của mô hình, vẫn là một hộp đen.

## 3. Suy luận Đồ thị Nhân quả của Mạng Nơ-ron

### 3.1. Ký hiệu và Trực giác

**Ký hiệu** Chúng tôi ký hiệu x là một hình ảnh đầu vào (không mất tính tổng quát), và y ∈ R^ny là nhãn đầu ra tương ứng, trong đó ny là số lượng lớp. Chúng tôi cũng ký hiệu ŷ ∈ R^ny là đầu ra dự đoán thu được bởi một mạng nơ-ron đã được huấn luyện N^(L) gồm L lớp. Chúng tôi định nghĩa mối quan hệ l → l+1 để chỉ các cạnh có hướng hoặc kết nối giữa các nút ẩn của lớp l và l+1, tương ứng. Tại mỗi nút ẩn j của lớp thứ l, chúng tôi định nghĩa các đặc trưng hoặc bản đồ kích hoạt a^l_j. Chúng tôi ký hiệu đồ thị nhân quả G là một trừu tượng của N^(L) như được hiển thị trong Hình 2 (b) và (d). Chúng tôi sử dụng thuật ngữ đồ thị giải thích để chỉ các đồ thị nhân quả có các nút chứa các đặc trưng quan trọng.

**Trực giác** Các tín hiệu được kích hoạt a^l chảy đến lớp tiếp theo l+1 thông qua các cạnh có trọng số W^l→l+1 kết nối các nút ẩn của lớp l và l+1. Những trọng số này kiểm soát cường độ luồng thông tin giữa hai lớp theo cách tương tự vật lý với một công tắc. Trong các hệ thống vật lý, việc thao tác trạng thái của công tắc (ví dụ: bật-tắt hoặc thông qua can thiệp liên tục) sẽ thay đổi trạng thái vật lý của hệ thống, từ đó cung cấp diễn giải về hành vi của nó. Chúng tôi đặt trực giác này để thúc đẩy công việc của mình. Theo hiểu biết của chúng tôi, có tương đối ít nghiên cứu về khả năng giải thích DNN bằng cách thao tác trọng số.

### 3.2. Công thức Bài toán

Mục tiêu của chúng tôi là khám phá các đồ thị giải thích nhân quả của N^(L) thông qua can thiệp đường dẫn (tương đương trọng số). Chính thức, chúng tôi đặt bài toán như sau. Cho W^l→l+1 ∈ R^nc×np là ma trận trọng số của các cạnh có hướng từ lớp l đến l+1, trong đó np là số lượng nút cha trong l và nc là số lượng nút con trong l+1. Những nút này định nghĩa một đồ thị con G^l→l+1. Cho w^l→l+1 ∈ R^nt×np, là các đường dẫn kết nối np nút trong l với nt nút mục tiêu trong l+1 (nt < nc). Bài toán của chúng tôi sau đó là ước tính mức độ ý nghĩa của tác động nhân quả hoặc tác động điều trị (TE) tạo ra từ việc can thiệp vào các trọng số w^l→l+1_j tại nút j:

P{TE(do(w^l→l+1_j);Ŷ,X,W\w^l→l+1_j) = 0} < α, (1)

trong đó do(w^l→l+1_j) là một phép toán học chỉ hành động can thiệp. X là một tập con của đầu vào trong đa tạp dữ liệu X, và Ŷ là dự đoán của chúng (lớp trước softmax). α > 0 là ngưỡng xác suất (tương đương p-value) đo lường "ý nghĩa". Công thức trong (1) định nghĩa một dạng kiểm định giả thuyết, trong đó giả thuyết không phát biểu rằng can thiệp vào các đường dẫn từ nút j sẽ không ảnh hưởng hoặc thay đổi dự đoán ban đầu của mô hình Ŷ. Công thức này có nghĩa là bác bỏ giả thuyết không, và sẽ dẫn chúng ta đến việc xác định các nút có ảnh hưởng nhất của l lên Ŷ.

### 3.3. Suy luận Nhân quả

Trong phần này, chúng tôi cung cấp chi tiết của phương pháp để giải quyết (1). Chúng tôi tập trung vào các mô hình thị giác bao gồm một tập hợp các lớp tích chập và MLP. Cụ thể, chúng tôi sử dụng LeNet [16] với dữ liệu MNIST để dễ giải thích. Phần thí nghiệm hiển thị các ứng dụng trên các tập dữ liệu phổ biến và kiến trúc phức tạp hơn. Ở đây, chúng tôi tìm cách nắm bắt đồ thị giải thích nhân quả của LeNet với đầu vào của chữ số k ∈ N9.

**Tác động Điều trị** Bước đầu tiên của cách tiếp cận của chúng tôi là tính toán tác động của can thiệp đường dẫn lên đầu ra mô hình. Hãy xem xét ví dụ MLP trong Hình 2 (a) và (c). Các can thiệp vào các đường dẫn trong lớp ẩn cuối cùng L-1 cho phép đo lường tác động lên các đầu ra (y1 và y2) trực tiếp. Trong khi đó, đối với lớp L-2, tác động của can thiệp được trung gian bởi phản ứng của các nơ-ron ẩn trong các lớp con cháu; trong ví dụ này, lớp con L-1. Cường độ phản ứng với can thiệp đường dẫn phụ thuộc vào cấu trúc và độ phức tạp của mạng nơ-ron. Mục tiêu của chúng tôi do đó là phân tích mức độ ý nghĩa của những tác động này. Đầu tiên, chúng tôi định nghĩa tác động điều trị như một biện pháp của sự khác biệt tương ứng với can thiệp đường dẫn.

**Định nghĩa 1 (Tác động Điều trị)** Cho X là một tập hợp các đặc trưng đầu vào và Ŷ là đầu ra tương ứng của một mạng nơ-ron N^(L). Cho w^l→l+1_j ∈ R^nt là vectơ trọng số có hướng từ nút j trong lớp l đến nt nút trong lớp l+1. Bằng cách giữ tất cả các trọng số khác W\(w^l→l+1_j) cố định và can thiệp vào w^l→l+1_j (tức là, do(w^l→l+1_j)), chúng tôi định nghĩa tác động của chúng như sau:

TE(do(w^l→l+1_j);Ŷ,X,W\w^l→l+1_j) = Ŷ_{w^l→l+1_j=u1}(X) - Ŷ_{w^l→l+1_j=u0}(X) (2)

trong đó u1 và u0 là các biến can thiệp được định nghĩa dưới đây.

Phương trình (2) đo lường sự thay đổi tương đối của phân phối đầu ra trên các đầu vào X với cùng các hành động tại nút j. Bằng cách xem xét tất cả np nút trong lớp l, chúng ta thu được tập hợp các phân phối {TE}_{j=1,...,np}. Hình 3, hiển thị các mẫu của tác động điều trị trung bình thu được trên X khi can thiệp tương ứng với việc loại bỏ các cạnh trong các lớp ẩn.

**Kiểm định Ý nghĩa** Để nắm bắt các nút có ảnh hưởng nhất trong lớp cha l, chúng tôi xem xét kiểm định giả thuyết như được công thức hóa trong phương trình (1). Chúng tôi quan sát thấy rằng phân phối không gần đúng là Gaussian, với số lượng mẫu đủ lớn (trong các tập huấn luyện). Điều này làm cho z-test trở thành lựa chọn phù hợp để giải quyết bài toán. Chúng tôi đặt ngưỡng xác suất α thành giá trị phổ biến 0.05. Điều đó có nghĩa là, tác động của việc can thiệp vào các đường dẫn đi ra từ nút j là có ý nghĩa khi phương trình (1) giữ với 5% cơ hội sai sót.

**Can thiệp Đường dẫn** Theo trực giác của công việc của chúng tôi, chúng tôi đề xuất các can thiệp w^l→l+1_j = u sao cho u = βw^l→l+1_j, trong đó β có thể là rời rạc (loại bỏ kết nối) hoặc liên tục (làm giảm tác động kết nối). Trong trường hợp rời rạc, β là nhị phân sao cho u1 = 0 và u0 = w^l→l+1_j. Trong trường hợp liên tục, chúng tôi đề xuất lấy mẫu β từ phân phối đều U(b-ε, b+ε), trong đó ε < b < 1.0 là một tham số được định nghĩa trước và ε = 0.01. Chúng tôi sử dụng can thiệp liên tục để đánh giá tính nhất quán của các tác động nhân quả và đồ thị ước tính.

### 3.4. Lựa chọn Đường dẫn

Cho đến nay, chúng tôi đã giải thích cách giải quyết (1) sử dụng w^l→l+1_j cho mỗi nút cha j. Những trọng số này tương ứng với một tập con của các mục tiêu mà chúng tôi xác định ở đây thông qua lựa chọn đường dẫn. Thật vậy, việc thao tác tất cả các kết nối có thể cho một nút j với lớp cha l là tốn kém về mặt tính toán và không thể thực hiện được cho các kiến trúc phức tạp với nhiều nơ-ron. Một cách hiệu quả là chú ý đến các đường dẫn và nút cụ thể thông qua tiêu chí lựa chọn. Chúng tôi đề xuất cách tiếp cận từ trên xuống bắt đầu từ một đầu ra cụ thể (ví dụ: lớp). Nó ngụ ý xử lý tuần tự bắt đầu từ lớp cuối cùng cho đến khi đạt được lớp l. Hãy xem xét chúng ta tìm cách tính toán tác động của can thiệp đường dẫn của lớp L-2 của LeNet cho chữ số 3 (như được hiển thị trong Hình 4). Chúng ta bắt đầu với các đường dẫn có hướng từ tất cả các nút trong lớp cha (L-1) đến nút 3 của lớp đầu ra L. Tính toán phương trình (1) tiết lộ các nút liên quan nhất trong (L-1), cho đến kiểm định ý nghĩa α = 5%. Để xác định tác động của những nút này lên mô hình, chúng ta phải xem xét hành vi của các tác động nhân quả. Các giá trị âm giải thích sự giảm trong dự đoán lớp khi loại bỏ các cạnh hoặc khấu hao trọng số, trong khi các giá trị dương giải thích dự đoán được cải thiện. Do đó, các nút được tiết lộ khi tác động nhân quả thấp hơn đáng kể so với không được coi là cần thiết cho đầu ra đó (những nút màu đỏ trong ví dụ này). Ngược lại, chúng ta khám phá các nút nhiễu hoặc gây xao nhãng khi can thiệp đường dẫn có tác động dương đáng kể. Do đó chúng ta chọn các nút cần thiết (màu đỏ) làm mục tiêu cho đồ thị con tiếp theo G^L-2→L-1. Chúng ta lặp lại quy trình tương tự trên L-2, nhưng lần này chúng ta đồng thời can thiệp vào tất cả các đường dẫn có hướng từ một nút cha (nút màu xanh lá cây) đến các mục tiêu. Với quy trình này, chúng ta có thể ước tính hiệu quả các nút liên quan trong tất cả các lớp trung gian trong khi tập trung vào các can thiệp có ý nghĩa. Thuật toán 1 hiển thị các bước thực hiện để khám phá các đồ thị giải thích nhân quả của một mạng nơ-ron phân loại. Chúng tôi cung cấp một số hình ảnh trực quan về đồ thị nhân quả của LeNet trong phần bổ sung.

**Thuật toán 1** Suy luận Đồ thị Giải thích Nhân quả (G) của một DNN

**Đầu vào:** N^(L) DNN đã được huấn luyện, W trọng số, X ví dụ cụ thể cho tác vụ, Ŷ đầu ra mô hình, (k) chỉ số tác vụ

**Đầu ra:** G (Từ điển các nút quan trọng và mối quan hệ của chúng), D (Từ điển các nút không liên quan)

l ← L-1, β ← {0,1}

**while** l > 0 **do**
    np ← dim(l)
    **for** j = 1 **to** np **do**
        u ← βw^l→l+1_j
        do(w^l→l+1_j = u)
        Tính TE(do(w^l→l+1_j),Ŷ,X,W) cho tất cả X
        Giải quyết (1) và nhận các nút (Jl, Il)
        G^l→l+1 ← Jl, D^l→l+1 ← Il
    **end for**
    l ← l-1
**end while**

## 4. Giải thích từ Đồ thị Nhân quả

Cấu trúc phân cấp của các đồ thị nhân quả cho phép trích xuất mạnh mẽ các thuộc tính và ngữ nghĩa cấp cao. Thay vì nắm bắt một bản đồ độ nổi bật duy nhất từ tất cả các kích hoạt, chúng tôi dựa vào phản ứng đặc trưng dọc theo các đường dẫn nhân quả. Chúng tôi chứng minh thực nghiệm rằng những đặc trưng này ổn định và nhất quán hơn so với các phương pháp thuộc tính truyền thống. Như được báo cáo trong [14], lý do để những phương pháp này tạo ra các thuộc tính nhiễu và không ổn định là do các đặc trưng gây xao nhãng trong DNN. Phương pháp của chúng tôi có thể loại bỏ các đặc trưng ảnh hưởng tiêu cực đến dự đoán của mô hình, và tách biệt các nơ-ron quan trọng trong đồ thị/đồ thị con nhân quả. Chính thức, với đồ thị con G^l→l+1, chúng tôi trích xuất các diễn giải nổi bật (s^l+1_i) tại một nút i trong l+1 như sau:

s^l+1_i = (1/|Jl|) Σ_{j=1}^{|Jl|} f(w^l→l+1_{ji}, a^l_j), (3)

trong đó a^l_j là tín hiệu được kích hoạt thứ j của lớp l, |Jl| là số lượng nút cha trong l được kết nối với nút con i trong lớp l+1. Phản ứng f phụ thuộc vào cấu trúc của lớp cha. Đối với các lớp tích chập, wji là một bộ lọc và f là hàm tích chập; trong khi đối với MLP, f là hàm tuyến tính. Hình 5 hiển thị các đồ thị con nhân quả, cho đến lớp conv2 (để trực quan hóa), và các thuộc tính cơ bản cho một mẫu hình ảnh được phân loại chính xác bởi mô hình. Chúng được tăng cường và chuẩn hóa để phản ánh xác suất theo pixel (Màu xanh lá cây đậm tương ứng với các đỉnh có điểm số cao nhất.).

Lưu ý rằng phương trình (3) tổng hợp tại mỗi nút i các phản ứng của các nút cha của nó với các bộ lọc/trọng số. Chúng ta cũng có thể quan tâm đến việc phân tích và diễn giải vai trò của mỗi bộ lọc giữa các cặp (i, j). Hình 6 là một ví dụ về phản ứng với các bộ lọc top-1 (w.r.t. biên độ của các tác động nhân quả của chúng) cho một tập hợp các nút liên quan trong lớp tích chập cuối cùng của ResNet18. Các thuộc tính nhân quả (của các phần đối tượng) được tinh chỉnh bằng cách trích xuất cực đại (và cực tiểu) cục bộ của phản ứng.

## 5. Thí nghiệm

Phần thí nghiệm chia thành hai phần: 1) chúng tôi đánh giá khả năng của thuật toán trong việc ước tính các đồ thị nhân quả ổn định và nhất quán; 2) chúng tôi đánh giá các giải thích được nắm bắt bởi đồ thị nhân quả và so sánh chúng với nhiều phương pháp thuộc tính khác nhau sử dụng các chỉ số giải thích tiêu chuẩn.

**Mô hình và tập dữ liệu** Chúng tôi đánh giá phương pháp của mình trên mô hình LeNet được huấn luyện trên dữ liệu MNIST và các kiến trúc sau: ResNet18 [11], ResNet50V2 [12], MobileNetV2 [24], và trên kiến trúc mới nhất ConvNext [18]; phiên bản tiny. Những mô hình này được huấn luyện trên dữ liệu ImageNet quy mô lớn (ILSVRC-2012) [23]. Chúng tôi cũng tinh chỉnh những kiến trúc này trên tập dữ liệu CIFAR10 sau khi cập nhật lớp phân loại cuối cùng của chúng. Chúng tôi chia các tập xác thực thành tập xác thực và tập kiểm tra. Chúng tôi sử dụng các mẫu trong tập xác thực để khám phá đồ thị giải thích nhân quả và tập kiểm tra để đánh giá các giải thích.

**Phương pháp so sánh** Chúng tôi đã chọn các phương pháp thuộc tính phổ biến nhất từ hai danh mục: các phương pháp bất khả tri mô hình (hộp đen) và dựa trên gradient (hộp trắng). Chúng tôi đã chọn RISE [20] và Occlusion [32] làm phương pháp hộp đen, và các phương pháp dựa trên gradient sau: Integrated-Gradient (IG) [30], Saliency [28], Gradient Shape [2], GradXInput [26], DeconvNet [33] và Excitation Backprob (MWP) [34].

### 5.1. Đánh giá Độ tin cậy của Đồ thị Nhân quả

Trong thí nghiệm này, chúng tôi đánh giá tính ổn định và nhất quán của việc ước tính đồ thị nhân quả của chúng tôi. Vì tác động nhân quả dựa trên can thiệp đường dẫn, chúng ta cần đảm bảo tính nhất quán trong kết quả kiểm định thống kê bất kể giá trị can thiệp nào được sử dụng (tức là nhị phân hoặc liên tục). Chúng tôi thực hiện điều này bằng cách chạy 1000 thí nghiệm với tham số can thiệp được lấy mẫu ngẫu nhiên từ phân phối đều U(b-0.01, b+0.01). Ở đây, b thay đổi đơn điệu mỗi 10 lần chạy trong khoảng (0.01, 0.5). Chúng tôi báo cáo độ tin cậy bằng cách đo tần suất phát hiện cùng các nút quan trọng trong mỗi lớp (tính bằng phần trăm). Trong Hình 7, chúng tôi hiển thị cho một vài mẫu của β phân phối của các nút so với tỷ lệ xuất hiện của chúng. Như chúng ta có thể thấy, tính ổn định của đồ thị không phụ thuộc vào giá trị được chọn cho tham số can thiệp. Bất kể giá trị của β, một tỷ lệ đáng kể (98% đến 100%) của các nút xuất hiện trong mọi thí nghiệm. Tính ổn định của đồ thị nhân quả chỉ ra hai sự thật: (1) tầm quan trọng của các tín hiệu được kích hoạt, bị ảnh hưởng bởi sự suy giảm trọng số. (2) phương pháp của chúng tôi không nhạy cảm với việc lựa chọn can thiệp (nhị phân hoặc liên tục). Hơn nữa, tác động nhân quả có ý nghĩa ngay cả khi giảm cường độ của tín hiệu dọc theo đường dẫn nhân quả chỉ bằng hệ số 1/2. Những kết quả này đảm bảo rằng các tính chất của từng nơ-ron thực sự có thể đại diện cho hành vi của mô hình.

### 5.2. Đánh giá giải thích nhân quả

Các đồ thị nhân quả được ước tính bởi phương pháp của chúng tôi tóm tắt kiến thức từ tất cả các lớp ẩn trong DNN và cho phép khả năng diễn giải tốt hơn. Ví dụ, Hình 5 hiển thị rằng để phân loại chữ số 3, có 8 nút liên quan trong lớp Conv2, mỗi nút mã hóa tín hiệu được kích hoạt tại các phần khác nhau của đối tượng. Để so sánh các giải thích thu được bởi phương pháp của chúng tôi với các phương pháp thuộc tính hiện có, chúng tôi tổng hợp các thuộc tính tại các nút liên quan trong một lớp cụ thể. Sau đó, chúng tôi đánh giá tính ổn định và trung thực của các giải thích sử dụng các chỉ số tiêu chuẩn hiện đại. Các đánh giá được thực hiện sử dụng thư viện Quantus [13]. Chi tiết về các chỉ số giải thích và trực quan hóa thuộc tính được cung cấp trong phần bổ sung.

**Tính ổn định:** Tính ổn định đo lường tính nhất quán của các giải thích chống lại các nhiễu loạn cục bộ của đầu vào. Ở đây, chúng tôi áp dụng Ước tính Lipschitz (LE) [1], tính toán phương sai tối đa giữa một đầu vào và ε-láng giềng của nó, trong đó ε chỉ mức độ nhiễu loạn. Chúng tôi tạo ra các nhiễu loạn bằng cách thêm nhiễu trắng vào đầu vào từ các tập kiểm tra. Chúng tôi tính toán các giải thích cho mỗi đầu vào trong lớp cụ thể và mẫu nhiễu của nó sử dụng các đồ thị được ước tính từ dữ liệu xác thực. Khoảng cách euclidean tối đa giữa các giải thích sau đó được thu được qua nhiều lần chạy trong đó các nhiễu loạn mới được tạo ra. Hình 8 báo cáo kết quả cho LeNet được huấn luyện trên MNIST và ResNet18 được tinh chỉnh trên CIFAR10, và Hình 9 hiển thị kết quả cho bốn kiến trúc khác nhau được huấn luyện trên dữ liệu ImageNet.

Kết quả (trong Hình 8 và 9) rõ ràng chỉ ra rằng các giải thích được tạo ra từ đồ thị nhân quả ổn định và nhất quán hơn so với các phương pháp thuộc tính khác. Các giải thích được tạo ra bởi những phương pháp này hiển thị phương sai cao hơn đối với nhiễu loạn tùy thuộc vào tập dữ liệu và mô hình. Ngược lại, giải thích từ đồ thị nhân quả hiển thị tính ổn định nhất quán. Phương pháp của chúng tôi có phương sai thấp nhất với biên đáng kể so với phương pháp tốt nhất trong mỗi thí nghiệm.

**Tính trung thực:** Đánh giá liên quan của thuộc tính đối với quyết định thu được bởi mô hình là cần thiết để đảm bảo tính chính xác và độ trung thực của các giải thích. Điều này thường được thực hiện bằng cách đo lường tác động của việc che khuất hoặc loại bỏ các đặc trưng từ đầu vào lên dự đoán của mô hình. Các kỹ thuật khác nhau đã được đề xuất để chấm điểm mức độ liên quan của các giải thích [5, 1, 3, 4, 22]. Ở đây, chúng tôi sử dụng loại bỏ lặp đi lặp lại các đặc trưng (IROF) [22]. Một hình ảnh được chia thành các mảnh sử dụng phân đoạn superpixel. Các mảnh được sắp xếp theo tầm quan trọng trung bình của chúng w.r.t các thuộc tính trong mỗi mảnh. Tại mỗi lần lặp, một số lượng ngày càng tăng các mảnh có mức độ liên quan cao nhất được thay thế bằng giá trị trung bình của chúng. IROC tính toán diện tích trung bình phía trên đường cong cho xác suất lớp (dự đoán bị nhiễu loạn so với dự đoán ban đầu). Chúng tôi áp dụng chỉ số này để đánh giá mỗi phương pháp giải thích bao gồm cả phương pháp của chúng tôi. Hình 8) hiển thị rằng phương pháp của chúng tôi vượt trội hơn các phương pháp khác và có thể so sánh với MWP [34] (với biên tương đối nhỏ giữa các trung vị của chúng). Đối với ResNet18 được huấn luyện trên CIFAR10, hầu hết các phương pháp thuộc tính hiển thị điểm số cao hơn LeNet trên MNIST. Hơn nữa, các giải thích thu được bởi phương pháp của chúng tôi và MWP hiển thị độ nhạy cảm ít hơn đối với dữ liệu và mô hình khác nhau, chỉ ra độ tin cậy tốt hơn. Hình 9 hiển thị kết quả IROF cho các kiến trúc khác nhau được huấn luyện trên ImageNet. Trên ImageNet, tất cả các phương pháp, bao gồm cả phương pháp của chúng tôi, đồng ý về sự khác biệt trong hành vi giữa bốn mô hình và rằng ConvNeXt đáng tin cậy hơn so với ConvNets tiêu chuẩn. Đối với những độc giả quan tâm, chúng tôi tham khảo [18] để biết thêm chi tiết về thiết kế cốt lõi của họ kiến trúc ConvNeXt.

### 5.3. Độ trung thực của các nơ-ron nhân quả cụ thể cho từng lớp

Các nơ-ron nhân quả được khám phá là quan trọng (hoặc liên quan) thông qua can thiệp nên mô tả chính xác hành vi mô hình. Chúng tôi đánh giá điều này bằng cách đo độ chính xác của mô hình trên một lớp cụ thể khi che các nơ-ron quan trọng được kết nối với lớp này. Điều đó có nghĩa là các nơ-ron có độ trung thực cao nên gây ra sự giảm mạnh về độ chính xác khi loại bỏ chúng. Chúng tôi minh họa hành vi này trên bốn mô hình được huấn luyện trên ImageNet trong Hình 10. Đầu tiên, sau khi khám phá các đồ thị nhân quả cụ thể cho từng lớp, chúng tôi xếp hạng các trọng số (và bộ lọc) trong mỗi đồ thị con theo các tác động cao nhất của chúng (như được mô tả trong phương trình (2)). Sau đó, chúng tôi sử dụng những thứ hạng này để chọn top-k nơ-ron quan trọng trong mỗi lớp. Như chúng ta quan sát trong Hình 10, độ chính xác của tất cả bốn mô hình giảm mạnh sau khi che một phần nhỏ (<20%) của các nơ-ron quan trọng hàng đầu, và nó rõ ràng hơn trên các kiến trúc nhỏ hơn như ResNet18 và MobileNetV2. Ngoài ra, những kết quả này mô tả một cách khác để đánh giá tính trung thực vì các nơ-ron quan trọng mã hóa các đặc trưng quan trọng để dự đoán một lớp cụ thể.

## 6. Ứng dụng

**Sửa chữa độ chính xác mô hình** Trong nhiều trường hợp thực tế, thực tế, chúng ta tìm kiếm các cách nhanh chóng và hiệu quả để sửa chữa hành vi của mô hình mà không yêu cầu huấn luyện lại rộng rãi với các tập dữ liệu lớn. Chúng ta có thể nhắm mục tiêu phương pháp giải thích được đề xuất để đạt được mục tiêu này. Mỗi đồ thị giải thích nhân quả đo lường đóng góp của các nơ-ron cho một lớp cụ thể (hoặc tác vụ) bằng cách can thiệp vào các trọng số kết nối các nơ-ron với lớp. Cụ thể hơn, khấu hao cường độ của các tín hiệu kích hoạt đi qua các đường dẫn cụ thể gây ra sự giảm hiệu suất của mô hình hoặc dự đoán sai. Đáng chú ý rằng hoạt động này khác với việc cắt tỉa mô hình vì chúng ta chỉ chặn những đường dẫn này tại thời điểm suy luận. Trên thực tế, chúng tôi thực hiện điều này bằng cách che các trọng số không liên quan (và bộ lọc cho các lớp tích chập). Các thí nghiệm chỉ ra rằng phương pháp của chúng tôi có thể cải thiện dự đoán lớp và sửa chữa dự đoán sai. Để minh họa những sự thật này, chúng tôi lấy bốn mô hình được huấn luyện trên ImageNet và xem xét 10 lớp đại diện (động vật) để đánh giá ngoài mô hình LeNet được huấn luyện trên dữ liệu MNIST. Đối với mỗi mô hình được huấn luyện, chúng tôi chọn che một phần của các trọng số không liên quan được khám phá bởi phương pháp của chúng tôi và đánh giá cách chúng hoạt động trên những mẫu này. Hình 11 hiển thị độ chính xác kiểm tra dưới một phần khác nhau của các trọng số bị che trong tất cả các lớp.

## 7. Kết luận và thảo luận

Chúng tôi đã trình bày một phương pháp mới để diễn giải hành vi mạng nơ-ron dựa trên suy luận nhân quả. Nó ước tính các đồ thị giải thích nhân quả tách rời kiến thức liên quan ẩn trong cấu trúc bên trong của DNN, vốn là bẩm sinh đối với dự đoán của chúng. Phương pháp của chúng tôi kiểm tra giả thuyết rằng can thiệp đường dẫn cho một nơ-ron cha được kết nối với các nơ-ron mục tiêu trong lớp tiếp theo sẽ ảnh hưởng đáng kể đến đầu ra của mô hình. Như một nghiên cứu trường hợp, chúng tôi áp dụng phương pháp của mình cho các mô hình thị giác để phân loại đối tượng. Các phản ứng của bộ lọc nhân quả được sử dụng để so sánh cách tiếp cận của chúng tôi với các phương pháp thuộc tính một cách định lượng. Công việc này không nhằm mục đích trích xuất các trừu tượng cấp cao có thể diễn giải được đối với con người, điều này có thể được coi là một hạn chế của phương pháp của chúng tôi. Tuy nhiên, chúng tôi tìm cách hiểu hoạt động bên trong của mô hình và do đó cung cấp một công cụ có giá trị cho việc giám sát và sửa chữa mô hình. Chúng tôi chỉ ra rằng phương pháp của chúng tôi có thể được sử dụng để cải thiện và sửa chữa mô hình mà không cần huấn luyện lại, điều này làm cho nó có giá trị và thực tế cho các trường hợp thực tế nơi dữ liệu huấn luyện rộng rãi không thể truy cập được, hoặc huấn luyện lại tốn kém về mặt tính toán. Trong công việc tương lai, chúng tôi sẽ xem xét việc điều tra thêm các ứng dụng của phương pháp của chúng tôi. Ví dụ, các nơ-ron quan trọng cụ thể cho từng lớp có thể được sử dụng với các phương pháp chính quy hóa trong học liên tục và học few-shot. Chi phí tính toán của phương pháp của chúng tôi là hợp lý, như được hiển thị trong phần bổ sung, điều này tạo điều kiện cho việc tích hợp nó vào các quy trình khác. Các hạn chế quan trọng của các phương pháp tầm quan trọng nơ-ron là chi phí tính toán cao và độ nhạy cảm với tương quan vượt trội giữa các nơ-ron [10]. Dựa vào suy luận nhân quả và can thiệp đường dẫn cho phép giảm thiểu những hạn chế này và cung cấp các diễn giải mạnh mẽ.

## Tài liệu tham khảo

[1] David Alvarez Melis and Tommi Jaakkola. Towards robust interpretability with self-explaining neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. 2, 6

[2] Marco Ancona, Enea Ceolini, Cengiz Öztireli, and Markus Gross. Towards better understanding of gradient-based attribution methods for deep neural networks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. 1, 2, 6

[3] Vijay Arya, Rachel K. E. Bellamy, Pin-Yu Chen, Amit Dhurandhar, Michael Hind, Samuel C. Hoffman, Stephanie Houde, Q. Vera Liao, Ronny Luss, Aleksandra Mojsilović, Sami Mourad, Pablo Pedemonte, Ramya Raghavendra, John Richards, Prasanna Sattigeri, Karthikeyan Shanmugam, Moninder Singh, Kush R. Varshney, Dennis Wei, and Yunfeng Zhang. One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques. arXiv e-prints, 2019. 6

[4] Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PLoS ONE, 10, 2015. 6

[5] Umang Bhatt, Adrian Weller, and José M. F. Moura. Evaluating and aggregating feature-based model explanations. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, pages 3016–3022, Jan. 2021. 6

[6] Miles Brundage, Shahar Avin, Jasmine Wang, Haydn Belfield, Gretchen Krueger, Gillian Hadfield, Heidy Khlaaf, and Others. Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims. arXiv e-prints, 2020. 1

[7] Aditya Chattopadhyay, Piyushi Manupriya, Anirban Sarkar, and Vineeth N Balasubramanian. Neural network attributions: A causal perspective. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pages 981–990, 2019. 2

[8] Ruth Fong and Andrea Vedaldi. Net2vec: Quantifying and explaining how concepts are encoded by filters in deep neural networks. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8730–8738, 2018. 1

[9] Atticus Geiger, Hanson Lu, Thomas Icard, and Christopher Potts. Causal Abstractions of Neural Networks. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 9574–9586. Curran Associates, Inc., 2021. 2

[10] Amirata Ghorbani and James Zou. Neuron shapley: discovering the responsible neurons. In Proceedings of the 34th International Conference on Neural Information Processing Systems, pages 5922–5932. Curran Associates Inc., Dec. 2020. 9

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016. 5

[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity Mappings in Deep Residual Networks. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, Computer Vision – ECCV 2016, pages 630–645, Cham, 2016. Springer International Publishing. 5

[13] Anna Hedström, Leander Weber, Dilyara Bareeva, Franz Motzkus, Wojciech Samek, Sebastian Lapuschkin, and Marina M.-C. Höhne. Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations. arXiv:2202.06861 [cs], Feb. 2022. arXiv: 2202.06861. 6

[14] Beomsu Kim, Junghoon Seo, Seunghyeon Jeon, Jamyoung Koo, Jeongyeol Choe, and Taegyun Jeon. Why are saliency maps noisy? cause of and solution to noisy saliency maps. In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), pages 4149–4157, 2019. 1, 2, 5

[15] Matthew L. Leavitt and Ari Morcos. Towards falsifiable interpretability research. arXiv e-prints, Oct. 2020. 1

[16] Yann LeCun, Bernhard Boser, John Denker, Donnie Henderson, R. Howard, Wayne Hubbard, and Lawrence Jackel. Handwritten digit recognition with a back-propagation network. In D. Touretzky, editor, Advances in Neural Information Processing Systems, volume 2. Morgan-Kaufmann, 1989. 3

[17] Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. Visualizing and understanding neural models in NLP. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 681–691, San Diego, California, June 2016. Association for Computational Linguistics. 1

[18] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s, 2022. 5, 7

[19] Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. Feature Visualization. Distill, 2(11):10.23915/distill.00007, Nov. 2017. 1

[20] Vitali Petsiuk, Abir Das, and Kate Saenko. rise: Randomized input sampling for explanation of black-box models. In Proceedings of the British Machine Vision Conference (BMVC), 2018. 2, 5

[21] Marco Ribeiro, Sameer Singh, and Carlos Guestrin. "why should I trust you?": Explaining the predictions of any classifier. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, pages 97–101. Association for Computational Linguistics, June 2016. 2

[22] Laura Rieger and Lars Kai Hansen. IROF: a low resource evaluation metric for explanation methods. CoRR, abs/2003.08747, 2020. 6, 7

[23] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015. 5

[24] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4510–4520, 2018. 5

[25] Patrick Schwab and Walter Karlen. CXPlain: causal explanations for model interpretation under uncertainty. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, number 917, pages 10220–10230. Curran Associates Inc., Red Hook, NY, USA, Dec. 2019. 2

[26] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, pages 3145–3153. JMLR.org, 2017. 1, 2, 6

[27] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, pages 3145–3153. JMLR.org, 2017. 2

[28] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. In Yoshua Bengio and Yann LeCun, editors, 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Workshop Track Proceedings, 2014. 1, 6

[29] Jost Tobias Springenberg, A. Dosovitskiy, T. Brox, and Martin A. Riedmiller. Striving for Simplicity: The All Convolutional Net. ICLR, 2015. 2

[30] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, pages 3319–3328. JMLR.org, 2017. 1, 2, 6

[31] Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. Investigating Gender Bias in Language Models Using Causal Mediation Analysis. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 12388–12401. Curran Associates, Inc., 2020. 1, 2

[32] Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Computer Vision – ECCV 2014, pages 818–833, Cham, 2014. Springer International Publishing. 1, 2, 5

[33] Matthew D. Zeiler and Rob Fergus. Visualizing and Understanding Convolutional Networks. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Computer Vision – ECCV 2014, pages 818–833. Springer International Publishing, 2014. 2, 6

[34] Jianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan Brandt, Xiaohui Shen, and Stan Sclaroff. Top-Down Neural Attention by Excitation Backprop. International Journal of Computer Vision, 126(10):1084–1102, Oct. 2018. 1, 6, 7

[35] Quanshi Zhang, Ruiming Cao, Feng Shi, Ying Nian Wu, and Song-Chun Zhu. Interpreting CNN knowledge via an explanatory graph. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence, pages 4454–4463. AAAI Press, Feb. 2018. 2

[36] Yu Zhang, Peter Tiňo, Aleš Leonardis, and Ke Tang. A survey on neural network interpretability. IEEE Transactions on Emerging Topics in Computational Intelligence, 5(5):726–742, 2021. 2
