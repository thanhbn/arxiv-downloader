# 2305.08950.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/interpretability/2305.08950.pdf
# File size: 3370129 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Causal Analysis for Robust Interpretability of Neural Networks
Ola Ahmad
Thales Digital Solutions
Montreal, CanadaNicolas Bereux*
Universit ´e Paris Sud
Paris, FranceLo¨ıc Baret
Thales Digital Solutions
Quebec, Canada
Vahid Hashemi
AUDI
Ingolstadt, GermanyFreddy Lecue*
J.P. Morgan, Chase & Co
New York, USA
Abstract
Interpreting the inner function of neural networks is cru-
cial for the trustworthy development and deployment of
these black-box models. Prior interpretability methods fo-
cus on correlation-based measures to attribute model de-
cisions to individual examples. However, these measures
are susceptible to noise and spurious correlations encoded
in the model during the training phase (e.g., biased in-
puts, model overfitting, or misspecification). Moreover,
this process has proven to result in noisy and unstable at-
tributions that prevent any transparent understanding of
the model’s behavior. In this paper, we develop a robust
interventional-based method grounded by causal analysis
to capture cause-effect mechanisms in pre-trained neural
networks and their relation to the prediction. Our novel
approach relies on path interventions to infer the causal
mechanisms within hidden layers and isolate relevant and
necessary information (to model prediction), avoiding noisy
ones. The result is task-specific causal explanatory graphs
that can audit model behavior and express the actual causes
underlying its performance. We apply our method to vision
models trained on classification tasks. On image classifica-
tion tasks, we provide extensive quantitative experiments to
show that our approach can capture more stable and faith-
ful explanations than standard attribution-based methods.
Furthermore, the underlying causal graphs reveal the neu-
ral interactions in the model, making it a valuable tool in
other applications (e.g., model repair).
*This work has been done when the authors were working at Thales
Digital Solutions Inc.1. Introduction
Explainability and interpretability are crucial for deep
neural networks (DNNs), which are disseminated in many
applications, including vision and natural language process-
ing. Despite their popularity, their opaque nature limits the
adoption of these ”black-box” models in domains requir-
ing critical decisions without the ability to understand their
behavior. Attempts to provide a transparent understanding
of DNN systems have led to the development of many in-
terpretability methods. Most of them focus on interpret-
ing the function of DNNs through correlation-based mea-
sures, which attribute the model’s decision to individual in-
puts [31]. The most popular ones are saliency (or feature
attribution) methods [30, 28, 34, 32, 2, 26, 17, 8].
Saliency methods aim at helping the user to understand
why a DNN made a particular decision by explaining the
entire model. However, we observe two considerable limi-
tations of these methods. First, they cannot explain the inner
function of the neural system being examined. That means
how internal neurons interact with each other to reach a
particular prediction. As reported in [6], it is difficult to
verify claims about black-box models without explanations
of their inner workings. A second limitation, they are sus-
ceptible to noise and spurious correlations. Whether due
to a property of the DNN system obtained during the train-
ing phase (e.g., biased inputs, overfitting, or misspecifica-
tion) or the method being used to capture saliency [14, 15]
as shown in Fig. 1). Alternatively, some methods seek to
visualize the behavior of specific neurons [19] but cannot
provide clear insights due to their large number and overall
complex architectures.
In this paper, we propose a novel method that addresses
the above limitations through the angle of causality. We
show that a technique grounded in the theory of causal in-
1arXiv:2305.08950v2  [cs.LG]  20 Jun 2023

--- PAGE 2 ---
ference provides robust and faithful interpretations of model
behavior while being able to reveal its neural interactions.
Inspired by neuroscience, we analyze individual neurons’
effects on model prediction by intervening in their connec-
tions (model’s weights or filters).
We summarize our contributions as follows. a) We pro-
pose a robust interpretability approach to capture meaning-
ful semantics and explain the inner working of DNNs. b)
Our methodology relies on path interventions and cause-
effect relations, providing stable and consistent explana-
tions. More specifically, we seek to answer questions such
as:would the model’s prediction have been higher if we
prevented the flow of signals through particular paths? or,
what would have been the decision of the model had we at-
tenuated or removed an individual or a set of components
at a particular layer? . Our analysis will lead to locating and
isolating relevant and necessary information strongly and
causally connected to model prediction up to a test of sig-
nificance. c) We apply our method to vision models trained
to classify MNIST, CIFAR10, and ImageNet data. d) We
provide a flexible framework that can be applied to com-
plex architectures and other tasks beyond interpretations.
2. Related Work
Interpretability and Attribution Methods. Inter-
pretability for deep neural networks aims to provide
insights into black box models’ behavior. A broad family
of methods has been developed in the past few years. The
most common techniques are attribution methods which
assign scores to input features indicating the contribution of
each one to the model prediction. Gradient-based methods
[30, 26, 2, 33, 29, 27] propagate gradients of pre-trained
models from output backward until input. Recent studies
have pointed out that these methods produce noisy and
unstable attributions [14, 1]. Perturbation-based methods
[20, 32, 21] are alternatives that focus on correlations be-
tween local perturbations of raw inputs and model output.
They are black-box methods in the sense that they don’t
require access to the inner state of the model. Beyond these
widely used techniques, various interpretability methods
have been proposed [36]. Close to our work is [35]. They
suggest disentangling knowledge hidden in the internal
structure of DNNs by learning a graphical model. Their
work focuses on convolutional neural networks (CNNs),
where they fit the activations between neighboring layers.
Our approach differs in what it considers explanatory
graphs and how it infers them. We rely on causal analyses,
which have been recently considered as an effective tool for
DNN interpretability and explainability. Our framework
does not assume a specific type of neural network, which
makes the approach generic and flexible.Capturing Explanations with Causality. More recently,
causal approaches have been considered for interpreting
DNNs. The inner structure of DNN has been viewed, for
the first time, as a structural causal model (SCM) in [7].
They use SCM to develop an attribution method that com-
putes the causal effect of each input feature on the output
of a recurrent neural network. Other causal approaches
were specifically developed to explain NLP-based language
models, such as causal mediation analyses [31] and causal
abstraction [9]. In contrast, [25] have developed a model-
agnostic approach (CXPlain) to estimate feature importance
for model interpretations. They use a causal objective to
train a separate supervised model (U-net) to learn causal
explanations for another black-box model. An important
limitation of this method is that it has to be trained to learn
to explain the target model. Another point is that its causal
property is limited to the extrinsic effect of input on causing
a marginal change in output. Therefore, it cannot link ex-
planations to the model’s internal structure, which remains
a black box.
3. Causal Graph Inference of Neural Networks
3.1. Notation and Intuition
Notation We denote by xan input image (without loss
of generality), and y∈Rnyits corresponding output la-
bel, where nyis the number of classes. We also denote
byˆy∈Rnyits predicted output obtained by a pre-trained
neural network N(L)composed of Llayers. We define the
relation l→l+ 1to refer to directed edges or connections
between hidden nodes of layers landl+ 1, respectively.
At every hidden node jof the l-th layer, we define features
or activation map al
j. We denote by causal graph Gan ab-
straction of N(L)as shown in Fig.2 (b) and (d). We use
the term explanatory graph to refer to causal graphs whose
nodes hold important features.
Intuition The activated signals alflow to the next layer
l+ 1through weighted edges Wl→l+1connecting hidden
nodes of layers landl+ 1. These weights control the
strength of information flow between two layers in a man-
ner physically analog to a switch. In physical systems, ma-
nipulating the state of a switch (e.g., on-off or via continu-
ous interventions) would change the system’s physical state,
thereby providing an interpretation of its behavior. We set
this intuition to motivate our work. To our knowledge, there
is relatively little research on DNN explainability by manip-
ulating weights.
3.2. Problem Formulation
Our goal is to discover causal explanatory graphs of
N(L)via path (equiv. weights) interventions. Formally,
we set the problem as follows. Let Wl→l+1∈Rnc×npbe

--- PAGE 3 ---
IG DeconvNet Saliency InputXGrad Occlusion Ours  (2 top nodes)  Original input  Figure 1. Features importance from different explanation methods on a hard example from ImageNet data. The actual class is ”white
wolf”. The predicted class by the pre-trained ResNet18 is ”Malamute”. To the right is our method showing top 2 semantics (top head and
body of wrong class) from the causal graph that explain the prediction. Other methods either fail or provide noisy features.
Figure 2. Causal connections within the last three layers of a neural network. (a) and (c) Coloured paths (red/yellow) transpose signals
between layers to labels y1/y2, respectively. (b) and (d) Two abstract graphs are obtained by causal inference. In each graph, neutral
neurons (marked by dots) hold variant information which don’t influence models’ behavior for the corresponding label.
the weight matrix of the directed edges from layer ltol+1,
where npis the number of parent nodes in landncis the
number of child nodes in l+ 1. These nodes define a sub-
graphGl→l+1. Let wl→l+1∈Rnt×np, be the paths con-
necting npnodes in ltonttarget nodes in l+ 1(nt< nc).
Our problem is then to estimate how significant the causal
or treatment effect (TE)resulting from intervening on the
weights wl→l+1
j at node j:
P{TE(do(wl→l+1
j );ˆY,X,W\wl→l+1
j ) = 0}< α, (1)
where do(wl→l+1
j )is a mathematical operation referring to
the action of interventions. Xis a subset of inputs in the
data manifold X, and ˆYare their predictions (pre-softmax
layer). α > 0is a probability threshold (equiv. p-value)
that measures ”significance”. The formula in (1) defines a
form of hypothesis testing, where the null hypothesis states
that interventions on the paths from node jwill not affect
or change the original predictions of the model ˆY. This
formula means rejecting the null hypothesis, and will lead
us to identify the most influential nodes of lonˆY.
3.3. Causal Inference
In this section, we provide the details of our method-
ology for solving (1). We focus on vision models which
encompass a set of convolution and MLP layers. Specifi-
cally, we use LeNet [16] with MNIST data for ease of ex-
planations. The experiments section shows applications on
common datasets and more complex architectures. Here,we seek to capture the causal explanatory graph of LeNet
given inputs of digit k∈N9.
Treatment Effects The first step of our approach is to
compute the effects of path interventions on model outputs.
Let us consider the MLP example in Fig. 2 (a) and (c). The
interventions on the paths in the last hidden layer L−1al-
low measuring the effect on the outputs ( y1andy2) directly.
Meanwhile, for layer L−2, the effects of interventions are
mediated by the responses of hidden neurons in descendant
layers; in this example, the child layer L−1. The strength of
response to path interventions depends on the structure and
complexity of neural networks. Our goal is thus to analyze
how significant these effects are. First, we define the treat-
ment effect as a measure of the difference corresponding to
path interventions.
Definition 1 (Treatment Effect ) LetXbe a set of input
features and ˆYthe corresponding output of a neural net-
work N(L). Let wl→l+1
j ∈Rntbe the weights vector di-
rected from node jin layer ltontnodes in layer l+ 1. By
holding all other weights W\(wl→l+1
j )fixed and interven-
ing on wl→l+1
j (i.e.,do(wl→l+1
j )), we define their effect as
follows:
TE(do(wl→l+1
j );ˆY,X,W\wl→l+1
j ) =
ˆYwl→l+1
j=u1(X)−ˆYwl→l+1
j=u0(X)(2)
where u1andu0are intervention variables defined below.
Equation (2) measures the relative change of the outputs

--- PAGE 4 ---
A TE A TE A TEFigure 3. Heatmap of the Average Treatment Effect (ATE). We show the effect of path interventions on the convolution and linear layers
of LeNet architecture for digits 3,7and8respectively. Y-axis indicates the total number of nodes overall layers. For instance, conv1 has 6
nodes (channels) and the last hidden layer fc2has 84 nodes. The colorbar indicates the relative ATE values w.r.t the original outputs.
distribution over the inputs Xgiven the same actions at
nodej. By considering all the npnodes in layer l, we obtain
the set of distributions {TE}j=1,...,n p. IFig. 3, shows sam-
ples of the average treatment effect obtained over Xwhen
interventions correspond to removing edges in the hidden
layers.
Test of Significance To capture the most influential nodes
in the parent layer l, we consider hypothesis testing as for-
mulated in eq. (1). We observe that the null distribution is
approximately Gaussian, given the sufficiently large num-
ber of samples (in training sets). This makes the z-test an
appropriate choice to solve the problem. We set the prob-
ability threshold αto its common value 0.05. That means,
the effect of intervening on the paths coming out from node
jis significant when eq. (1) holds with 5%chance of error.
Path Interventions Following the intuition of our work,
we propose the interventions wl→l+1
j =usuch that u=
βwl→l+1
j , where βcan either be discrete (remove connec-
tions) or continuous (attenuate connection’s effect). In the
discrete case, βis binary so that u1= 0andu0=wl→l+1
j .
In the continuous case, we propose to sample βfrom a uni-
form distribution U(b−ϵ, b+ϵ), where ϵ < b < 1.0is a
predefined parameter and ϵ= 0.01. We use continuous in-
terventions to evaluate the consistency of the causal effects
and estimated graphs.
3.4. Path Selection
So far, we explained how to solve (1) using wl→l+1
j for
each parent node j. These weights correspond to a subset
of targets we identify here via path selection. Indeed, ma-
nipulating all possible connections for a node jgiven par-
ent layer lis computationally expensive and intractable for
complex architectures with many neurons. An efficient way
is to pay attention to specific paths and nodes via selection
criterion. We propose a top-down approach starting from aspecific output (e.g., class). It implies sequential process-
ing starting from the last layer until reaching layer l. Let
us consider we seek to compute the effects of path interven-
tions of LeNet’s layer L−2for digit 3(as shown in Fig.
4). We start with the paths directed from all nodes in the
Figure 4. Path selection.
parent layer (L−1)to node 3of the output layer L. Com-
puting eq. (1) reveals the most relevant nodes in (L−1),
up to a significance test α= 5% . To identify the impact of
these nodes on the model, we must look at the behavior of
causal effects. Negative values explain a drop in class pre-
diction when removing edges or amortizing weights, while
positive values explain an improving prediction. Hence, the
nodes revealed when the causal effect is significantly below
zero are considered necessary for that output (the red ones
in this example). In contrast, we discover noisy or distract-
ing nodes when path interventions have a significant posi-
tive effect. We thereby select the necessary (red) nodes as
targets for the next sub-graph GL−2→L−1. We repeat the
same process on L−2, but this time we simultaneously
intervene on all paths directed from a parent node (green

--- PAGE 5 ---
Algorithm 1 Causal Explanatory Graph Inference ( G) of a
DNN
Input :N(L)pre-trained DNN, Wweights, Xtask-
specific examples, ˆYmodel outputs, (k)task index
Output :G(Dict. of important nodes and their relations), D
(Dict. of irrelevant nodes)
l←L−1,β← {0,1}
while l >0do
np←dim(l)
forj= 1tonpdo
u←βwl→l+1
j
do(wl→l+1
j =u)
Compute TE(do(wl→l+1
j ),ˆY,X,W)for all X
Solve (1) and get nodes (Jl, Il)
Gl→l+1←Jl,Dl→l+1←Il
end for
l←l−1
end while
node) to the targets. With this process, we can efficiently
estimate relevant nodes in all intermediate layers while fo-
cusing on meaningful interventions. Algorithm 1 shows the
implementation steps for discovering the causal explanatory
graphs of a classification neural network. We provide some
visualizations of LeNet’s causal graphs in the supplemen-
tary.
4. Explanations from Causal Graphs
The hierarchical structure of the causal graphs enables
robust extraction of attributions and high-level semantics.
Instead of capturing a single saliency map from all activa-
tions, we rely on features response along the causal path-
ways. We empirically show that these features are more sta-
ble and consistent compared to traditional attribution meth-
ods. As reported in [14], the reason for these methods to
produce noisy and unstable attributions is due to distracting
features in DNNs. Our method can remove the features that
negatively affect model’s prediction, and isolate important
neurons in causal graphs/sub-graphs. Formally, given the
sub-graph Gl→l+1, we extract salient interpretations ( sl+1
i)
at a node iinl+ 1as follows
sl+1
i=1
JlJlX
j=1f(wl→l+1
ji , al
j), (3)
where al
jis the j-th activated signal of layer l,Jlis the num-
ber of parent nodes in lconnected to the child node iin the
layer l+ 1. The response fdepends on the structure of the
parent layer. For convolution layers, wjiis a filter and fis
a convolution function; whereas for MLPs, fis linear func-
tion. Fig. 5 shows causal sub-graphs, up to conv2 layer (for
0 2 40
0 4 6 7 8 13 10 14Input
Conv1
Conv2
Figure 5. Illustration of LeNet’s causal sub-graphs
Ginput→conv 1,Gconv 1→conv 2for class 3.The resulted at-
tributes provide visual interpretations for a sample image
correctly classified by the model. They are up-sampled and
normalized to reflect pixel-wise probabilities (Dark greens
correspond to peaks with highest scores.).
visualization), and the underlying attributions for a LeNet
model successfully classified its input.
Note that eq. (3)aggregates at every node ithe responses
of its parent nodes to the filters/weights. We may also be
interested in analyzing and interpreting the role of each fil-
ter between the pairs ( i, j). Fig. 6 is an example of the
response to the top-1 filters (w.r.t. the amplitude of their
causal effects) for a set of relevant nodes in the last convo-
lution layer of ResNet18. Causal attributes (of object parts)
are refined by extracting the response’s local maxima (and
minima).
5. Experiments
The experiments section splits into two parts: 1) we eval-
uate our algorithm’s capacity to estimate stable and consis-
tent causal graphs; 2) we evaluate the explanations captured
by causal graphs and compare them to various attribution
methods using standard explanation metrics.
Models and datasets We evaluate our method on the
LeNet model trained on MNIST data and the follow-
ing architectures: ResNet18 [11], ResNet50V2 [12], Mo-
bileNetV2 [24], and on the latest architecture ConvNext
[18]; the tiny version. These models were trained on the
large-scale ImageNet data (ILSVRC-2012) [23]. We also
fine-tuned these architectures on CIFAR10 dataset after up-
dating their last classification layer. We divide the valida-
tion sets into validation and test sets. We use the samples
in validation sets to discover causal explanatory graphs and
the test set for evaluating the explanations.
Comparison methods We selected the most popular at-
tribution methods from two categories: model-agnostic
(black-box) and gradient-based (white-box) methods. We
chose RISE [20] and Occlusion [32] as black-box meth-
ods, and the following gradient-based methods: Integrated-

--- PAGE 6 ---
Node 25 Node 78 Node 147 Node 156Node 25 Node 1 Node 61 Node 177Figure 6. Visualizing explanations obtained by the top-1 causal
filters. We show four examples for two object classes (from Im-
ageNet). Important neurons belong to the causal sub-graph con-
necting the last Conv layers l=layer 4.1.conv 1andl+ 1 =
layer 4.1.conv 2of ResNet18. We can observe consistent at-
tributes for similar inputs. The red point indicates the location
of the peaks corresponding to the absolute maximum response.
Gradient (IG) [30], Saliency [28], Gradient Shape [2],
GradXInput [26], DeconvNet [33] and Excitation Backprob
(MWP) [34].
5.1. Evaluating the Reliability of Causal Graphs
In this experiment, we evaluate the stability and consis-
tency of our estimation of causal graphs. Since the causal
effect is based on path interventions, we need to ensure con-
sistency in the statistical test results no matter what inter-
vention values are used (i.e., binary or continuous). We
do so by running 1000 experiments with an intervention
parameter randomly sampled from a uniform distribution
U(b−0.01, b+ 0.01). Here, bchanges monotonically ev-
ery10runs in the range (0.01,0.5). We report reliability
by measuring the frequency of detecting the same impor-
tant nodes in each layer (in percentage). In Fig 7, we show
for a few samples of βthe distribution of the nodes versus
their appearance rate. As we can see, the stability of the
graph does not rely on the value chosen for the intervention
parameter. Regardless of the value of β, a considerable pro-
portion ( 98% to100% ) of the nodes appear in every experi-
ment. The stability of causal graphs indicates two facts: (1)
the importance of the activated signals, which are affected
by weights attenuation. (2) our method is not sensitive tothe choice of interventions (binary or continuous). Further-
more, the causal effect is significant even when reducing the
strength of the signal along the causal path by only a factor
of1/2. These results ensure that the properties of single
neurons might indeed be representative of model’s behav-
ior.
5.2. Evaluation of causal explanations
The causal graphs estimated by our method summarize
knowledge from all hidden layers in the DNN and enable
better interpretability. For example, Fig.5 shows that for
classifying digit 3, there exist 8relevant nodes in the Conv2
layer, each encoding signal activated at different parts of
the object. To compare the explanations obtained by our
method with existing attribution methods, we aggregate at-
tributions at the relevant nodes in a specific layer. Then, we
evaluate the stability and faithfulness of explanations using
standard state-of-the-art metrics. The evaluations are per-
formed using the Quantus library [13]. Details on expla-
nation metrics and attributions visualization are provided in
the supplementary.
Stability: Stability measures consistency of explanations
against local perturbations of inputs. Here, we adopt Lip-
schitz Estimate (LE) [1], which calculates the maximum
variance between an input and its ϵ-neighbourhood, where
ϵrefers to the level of perturbations. We generate pertur-
bations by adding white noise to inputs from the test sets.
We compute explanations for every input in specific class
and its noisy sample using the graphs estimated from the
validation data. The maximum euclidean distance between
explanations is then obtained over multiple runs where new
perturbations are generated. Fig. 8 reports the results for
LeNet trained on MNIST and ResNet18 fine-tuned on CI-
FAR10, and Fig.9 shows the results for four different archi-
tectures trained on ImageNet data.
The results (in Fig. 8 and 9) clearly indicate that the ex-
planations generated from the causal graphs are more stable
and consistent compared to other attribution methods. The
explanations generated by these methods show higher vari-
ance to perturbations depending on the dataset and model.
In contrast, the explanation from causal graph show con-
sistent stability. Our method has the lowest variance with
significant margin compared to the best method in each ex-
periment.
Faithfulness: Evaluating attributions relevance for the
decision obtained by the model is essential to ensure cor-
rectness and fidelity of explanations. This is commonly
done by measuring the effect of obscuring or removing fea-
tures from the input on model’s prediction. Different tech-
niques have been proposed to score the relevance of expla-
nations [5, 1, 3, 4, 22]. Here, we used iterative removal of

--- PAGE 7 ---
20 40 60 80 100
Appearence frequency (%)101
100101102Portion of nodes (%) ~ U(0.0, 0.1)
20 40 60 80 100
Appearence frequency (%)101
100101102 ~ U(0.1, 0.2)
20 40 60 80 100
Appearence frequency (%)101
100101102 ~ U(0.2, 0.3)
20 40 60 80 100
Appearence frequency (%)101
100101102 ~ U(0.3, 0.4)
20 40 60 80 100
Appearence frequency (%)101
100101102 ~ U(0.4, 0.5)
ResNet18
convnext
ResNet50
MobilenetFigure 7. Reliability assessment of causal graphs. We show results on four complex architectures. On the x-axis, we show the frequency
of appearance of a node (%). On the y-axis is the portion of all the nodes appearing at least once during the experiments (%).
101
100101102103 Lipschitz  Estimate MNIST/LeNet
101
100101102103 Lipschitz  Estimate CIFAR10/ResNet18
Occlusion
Deconv
Saliency
MWP
InpXGrad
GradShap
IG
Causal
 020406080100120IROF
Occlusion
Saliency
MWP
GradShap
InpXGrad
IG
Deconv
Causal
 020406080100120IROF
Figure 8. Quantitative evaluations of attribution methods for
LeNet on MNIST and ResNet18 on CIFAR10. For each metric,
we compare 7 attribution methods to the causal explanations ob-
tained by our method using test images. The bars show mean and
variance over samples. Lower Lipschitz Estimates (w.r.t. means)
indicate higher stability. Higher IROF values (w.r.t. means) indi-
cate strong relation between explanations and predictions.
features (IROF) [22]. An image is partitioned into patches
using superpixel segmentation. The patches are sorted by
their mean importance w.r.t the attributions in each patch.
At every iteration, an increasing number of patches with
highest relevance are replaced by their mean value. The
IROC computes the mean area above the curve for the class
probabilities (perturbed vs. original predictions). We ap-
plied this metric to evaluate each explanation method in-
cluding ours. Fig. 8) shows that our method outperforms
other methods and is comparable to MWP [34] (with a rela-
tively small margin between their medians). For ResNet18
trained on CIFAR10, most attribution methods show higher
scores than LeNet on MNIST. Furthermore, the explana-
tions obtained by our method and MWP show less sensitiv-
ity to the different data and models, indicating better trust-
RISE
Deconv
GradShap InpXGradIG
SaliencyGuidedBPMWPCausal
 104
103
102
101
100101102103 Lipschitz  Estimate ResNet18 ResNet50 MobileNetV2 Tiny-Convnext
RISE
Deconv
GradShap InpXGradIG
SaliencyGuidedBPMWPCausal
 0102030405060IROFFigure 9. Quantitative evaluations of explanations for complex
architectures on ImageNet. We evaluate 9 methods including
ours using 10 representative classes from the test set.
worthiness. Fig. 9 shows IROF results for different archi-
tectures trained on ImageNet. On ImageNet, all methods,
including ours, agree on the differences in behavior between
the four models and that ConvNeXt is more trustable than
standard ConvNets. For interested readers, we refer to [18]
for further details about the core design of the ConvNeXt
family of architectures.

--- PAGE 8 ---
0 10 20 30 40 50
Portion of masked critical neurons (%)0.00.20.40.60.81.0Normalized test accuracyResNet18
ResNet50V2MobileNetV2
Tiny-Convnext
10 20 30 40 50
Ratio (%)100200300400500#Top masked-out critical neuronsResNet18
ResNet50V2MobileNetV2
Tiny-ConvnextFigure 10. Fidelity of class-specific causal neurons to the model.
The left figure shows the test accuracy of four models when mask-
ing out the top-k ( %) portion of causal neurons discovered as crit-
ical using our path interventions method. The figure shows the
average accuracy over ten representative classes selected from Im-
ageNet. The right figure shows the absolute number of critical
neurons at each portion.
5.3. Fidelity of class-specific causal neurons
The causal neurons discovered as critical (or relevant)
through interventions should accurately describe model be-
havior. We evaluate this by measuring the model accuracy
on a specific class when masking out the critical neurons
connected to this class. That means high-fidelity neurons
should cause a drastic drop in accuracy under discarding
them. We illustrate this behavior on four models trained on
ImageNet in Fig. 10. First, after discovering class-specific
causal graphs, we rank the weights (and filters) in each sub-
graph according to their highest effects (as described in eq.
(2)). Then, we use these ranks to select the top-k critical
neurons in each layer. As we observe in Fig. 10, the ac-
curacy of all four models drastically drops after masking a
small portion ( <20%) of top critical neurons, and it is more
evident on smaller architectures such as ResNet18 and Mo-
bileNetV2. In addition, these results describe another way
of evaluating faithfulness since critical neurons encode the
important features for predicting a specific class.
6. Applications
Repairing model accuracy In many practical, real-world
cases, we seek fast and effective ways to repair the model’s
behavior without requiring extensive retraining with large
datasets. We can target the proposed explanation method to
achieve this goal. Each causal explanatory graph measures
the neurons’ contributions to a specific class (or task) by
intervening on the weights connecting the neurons to the
class. More specifically, amortizing the strength of acti-
vation signals passing through particular paths that cause
a drop in the model’s performance or a wrong prediction.
It is worth noting that this operation differs from model
pruning since we only block these paths at inference time.
0.0 0.2 0.4 0.6 0.8 1.0
Portion of masked noisy/irrelevant neurons (%)0.700.750.800.850.900.951.00Normalized test accuracyResNet18
ResNet50
Mobilenet
convnext
0.0 0.2 0.4 0.6 0.8 1.0
Portion of masked noisy/irrelevant neurons (%)0.860.880.900.920.940.960.981.00Normalized test accuracy0
1
2
3
4
5
6
7
8
9Figure 11. Repair of model performance. We show the test accu-
racy after masking out ( n%) of class-specific noisy filters in LeNet
model for all categories of MNIST data (right), and in the 4 models
for 10 representative (animal) categories of ImageNet data (left).
Each color in left figure points to one different category and is
fixed for each model.
Practically, we do this by masking out irrelevant weights
(and filters for convolutional layers). The experiments show
that our method can improve class prediction and correct
wrong predictions. To illustrate these facts, we took the
four models trained on ImageNet and considered 10 repre-
sentative (animal) classes for evaluation in addition to the
LeNet trained oon MNIST data. For each trained model,
we select to mask out a portion of the irrelevant weights dis-
covered by our method and evaluate how they perform on
these samples. Figs 11 shows test accuracy under a varying
portion of the masked weights in all layers.
7. Conclusion and discussions
We have presented a novel method for interpreting neural
network behavior based on causal inference. It estimates the
causal explanatory graphs that disentangle relevant knowl-
edge hidden in the internal structure of DNNs, which is
congenital to their predictions. Our methodology tests the
hypothesis that path interventions for a parent neuron con-
nected with target neurons in the subsequent layer will sig-
nificantly affect the model’s output. As a case study, we ap-
plied our method to vision models for object classification.
The responses of causal filters are used to compare our ap-
proach to attribution methods quantitatively. This work is
not aimed at extracting high-level abstractions that are in-
terpretable to humans, which might be considered a limi-
tation of our method. However, we seek to understand the
inner working of the model and therefore provide a valu-
able tool for model monitoring and repair. We show that our
method can be used to improve and fix the model without
retraining, which makes it worthwhile and practical for real-
world cases where extensive training data are not accessi-
ble, or retraining is computationally expensive. In future
work, we will consider investigating further applications of
our method. For instance, class-specific important neurons
can be used with regularization methods in continual and
few-shot learning. Our method’s computational cost is rea-

--- PAGE 9 ---
sonable, as shown in the supplementary, which facilitates
its integration into other processes. The critical limitations
of neuron importance methods are their high computational
costs and sensitivity to superior correlations between neu-
rons [10]. Relying on causal inference and path interven-
tions allows for mitigating these limitations and provides
robust interpretations.
References
[1] David Alvarez Melis and Tommi Jaakkola. Towards robust
interpretability with self-explaining neural networks. In S.
Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett, editors, Advances in Neural Infor-
mation Processing Systems , volume 31. Curran Associates,
Inc., 2018. 2, 6
[2] Marco Ancona, Enea Ceolini, Cengiz ¨Oztireli, and Markus
Gross. Towards better understanding of gradient-based at-
tribution methods for deep neural networks. In 6th Interna-
tional Conference on Learning Representations, ICLR 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference
Track Proceedings . OpenReview.net, 2018. 1, 2, 6
[3] Vijay Arya, Rachel K. E. Bellamy, Pin-Yu Chen, Amit
Dhurandhar, Michael Hind, Samuel C. Hoffman, Stephanie
Houde, Q. Vera Liao, Ronny Luss, Aleksandra Mojsilovi ´c,
Sami Mourad, Pablo Pedemonte, Ramya Raghavendra,
John Richards, Prasanna Sattigeri, Karthikeyan Shanmugam,
Moninder Singh, Kush R. Varshney, Dennis Wei, and Yun-
feng Zhang. One Explanation Does Not Fit All: A Toolkit
and Taxonomy of AI Explainability Techniques. arXiv e-
prints , 2019. 6
[4] Sebastian Bach, Alexander Binder, Gr ´egoire Montavon,
Frederick Klauschen, Klaus-Robert M ¨uller, and Wojciech
Samek. On pixel-wise explanations for non-linear classifier
decisions by layer-wise relevance propagation. PLoS ONE ,
10, 2015. 6
[5] Umang Bhatt, Adrian Weller, and Jos ´e M. F. Moura. Evalu-
ating and aggregating feature-based model explanations. In
Proceedings of the Twenty-Ninth International Joint Confer-
ence on Artificial Intelligence , pages 3016–3022, Jan. 2021.
6
[6] Miles Brundage, Shahar Avin, Jasmine Wang, Haydn
Belfield, Gretchen Krueger, Gillian Hadfield, Heidy Khlaaf,
and Others . Toward Trustworthy AI Development: Mech-
anisms for Supporting Verifiable Claims. arXiv e-prints ,
2020. 1
[7] Aditya Chattopadhyay, Piyushi Manupriya, Anirban Sarkar,
and Vineeth N Balasubramanian. Neural network attribu-
tions: A causal perspective. In Proceedings of the 36th In-
ternational Conference on Machine Learning , volume 97,
pages 981–990, 2019. 2
[8] Ruth Fong and Andrea Vedaldi. Net2vec: Quantifying and
explaining how concepts are encoded by filters in deep neu-
ral networks. In 2018 IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 8730–8738, 2018. 1
[9] Atticus Geiger, Hanson Lu, Thomas Icard, and Christopher
Potts. Causal Abstractions of Neural Networks. In M. Ran-zato, A. Beygelzimer, Y . Dauphin, P. S. Liang, and J. Wort-
man Vaughan, editors, Advances in Neural Information Pro-
cessing Systems , volume 34, pages 9574–9586. Curran As-
sociates, Inc., 2021. 2
[10] Amirata Ghorbani and James Zou. Neuron shapley: dis-
covering the responsible neurons. In Proceedings of the
34th International Conference on Neural Information Pro-
cessing Systems , pages 5922–5932. Curran Associates Inc.,
Dec. 2020. 9
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In 2016 IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 770–778, 2016. 5
[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Identity Mappings in Deep Residual Networks. In Bas-
tian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, edi-
tors, Computer Vision – ECCV 2016 , pages 630–645, Cham,
2016. Springer International Publishing. 5
[13] Anna Hedstr ¨om, Leander Weber, Dilyara Bareeva, Franz
Motzkus, Wojciech Samek, Sebastian Lapuschkin, and Ma-
rina M.-C. H ¨ohne. Quantus: An Explainable AI Toolkit
for Responsible Evaluation of Neural Network Explanations.
arXiv:2202.06861 [cs] , Feb. 2022. arXiv: 2202.06861. 6
[14] Beomsu Kim, Junghoon Seo, Seunghyeon Jeon, Jamyoung
Koo, Jeongyeol Choe, and Taegyun Jeon. Why are saliency
maps noisy? cause of and solution to noisy saliency maps.
In2019 IEEE/CVF International Conference on Computer
Vision Workshop (ICCVW) , pages 4149–4157, 2019. 1, 2, 5
[15] Matthew L. Leavitt and Ari Morcos. Towards falsifiable in-
terpretability research. arXiv e-prints , Oct. 2020. 1
[16] Yann LeCun, Bernhard Boser, John Denker, Donnie Hen-
derson, R. Howard, Wayne Hubbard, and Lawrence Jackel.
Handwritten digit recognition with a back-propagation net-
work. In D. Touretzky, editor, Advances in Neural Infor-
mation Processing Systems , volume 2. Morgan-Kaufmann,
1989. 3
[17] Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. Vi-
sualizing and understanding neural models in NLP. In Pro-
ceedings of the 2016 Conference of the North American
Chapter of the Association for Computational Linguistics:
Human Language Technologies , pages 681–691, San Diego,
California, June 2016. Association for Computational Lin-
guistics. 1
[18] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-
enhofer, Trevor Darrell, and Saining Xie. A convnet for the
2020s, 2022. 5, 7
[19] Chris Olah, Alexander Mordvintsev, and Ludwig Schubert.
Feature Visualization. Distill , 2(11):10.23915/distill.00007,
Nov. 2017. 1
[20] Vitali Petsiuk, Abir Das, and Kate Saenko. Rise: Random-
ized input sampling for explanation of black-box models.
InProceedings of the British Machine Vision Conference
(BMVC) , 2018. 2, 5
[21] Marco Ribeiro, Sameer Singh, and Carlos Guestrin. “why
should I trust you?”: Explaining the predictions of any clas-
sifier. In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computational Lin-

--- PAGE 10 ---
guistics: Demonstrations , pages 97–101. Association for
Computational Linguistics, June 2016. 2
[22] Laura Rieger and Lars Kai Hansen. IROF: a low re-
source evaluation metric for explanation methods. CoRR ,
abs/2003.08747, 2020. 6, 7
[23] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, Alexander C. Berg, and
Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal-
lenge. International Journal of Computer Vision (IJCV) ,
115(3):211–252, 2015. 5
[24] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-
moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted
residuals and linear bottlenecks. In 2018 IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
4510–4520, 2018. 5
[25] Patrick Schwab and Walter Karlen. CXPlain: causal expla-
nations for model interpretation under uncertainty. In Pro-
ceedings of the 33rd International Conference on Neural In-
formation Processing Systems , number 917, pages 10220–
10230. Curran Associates Inc., Red Hook, NY , USA, Dec.
2019. 2
[26] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje.
Learning important features through propagating activation
differences. In Proceedings of the 34th International Confer-
ence on Machine Learning - Volume 70 , pages 3145–3153.
JMLR.org, 2017. 1, 2, 6
[27] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje.
Learning important features through propagating activation
differences. In Proceedings of the 34th International Confer-
ence on Machine Learning - Volume 70 , pages 3145–3153.
JMLR.org, 2017. 2
[28] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.
Deep inside convolutional networks: Visualising image clas-
sification models and saliency maps. In Yoshua Bengio
and Yann LeCun, editors, 2nd International Conference on
Learning Representations, ICLR 2014, Banff, AB, Canada,
April 14-16, 2014, Workshop Track Proceedings , 2014. 1, 6
[29] Jost Tobias Springenberg, A. Dosovitskiy, T. Brox, and Mar-
tin A. Riedmiller. Striving for Simplicity: The All Convolu-
tional Net. ICLR , 2015. 2
[30] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic
attribution for deep networks. In Proceedings of the 34th
International Conference on Machine Learning - Volume 70 ,
pages 3319–3328. JMLR.org, 2017. 1, 2, 6
[31] Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon
Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. In-
vestigating Gender Bias in Language Models Using Causal
Mediation Analysis. In H. Larochelle, M. Ranzato, R. Had-
sell, M. F. Balcan, and H. Lin, editors, Advances in Neural
Information Processing Systems , volume 33, pages 12388–
12401. Curran Associates, Inc., 2020. 1, 2
[32] Matthew D. Zeiler and Rob Fergus. Visualizing and under-
standing convolutional networks. In David Fleet, Tomas Pa-
jdla, Bernt Schiele, and Tinne Tuytelaars, editors, Computer
Vision – ECCV 2014 , pages 818–833, Cham, 2014. Springer
International Publishing. 1, 2, 5[33] Matthew D. Zeiler and Rob Fergus. Visualizing and Under-
standing Convolutional Networks. In David Fleet, Tomas Pa-
jdla, Bernt Schiele, and Tinne Tuytelaars, editors, Computer
Vision – ECCV 2014 , pages 818–833. Springer International
Publishing, 2014. 2, 6
[34] Jianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan
Brandt, Xiaohui Shen, and Stan Sclaroff. Top-Down Neu-
ral Attention by Excitation Backprop. International Journal
of Computer Vision , 126(10):1084–1102, Oct. 2018. 1, 6, 7
[35] Quanshi Zhang, Ruiming Cao, Feng Shi, Ying Nian Wu, and
Song-Chun Zhu. Interpreting CNN knowledge via an ex-
planatory graph. In Proceedings of the Thirty-Second AAAI
Conference on Artificial Intelligence and Thirtieth Innova-
tive Applications of Artificial Intelligence Conference and
Eighth AAAI Symposium on Educational Advances in Artifi-
cial Intelligence , pages 4454–4463. AAAI Press, Feb. 2018.
2
[36] Yu Zhang, Peter Ti ˇno, Ale ˇs Leonardis, and Ke Tang. A sur-
vey on neural network interpretability. IEEE Transactions on
Emerging Topics in Computational Intelligence , 5(5):726–
742, 2021. 2
