# 2211.00251.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/interpretability/2211.00251.pdf
# File size: 1617732 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Differentiable Model Selection for Ensemble Learning
James Kotary1,Vincenzo Di Vito1and Ferdinando Fioretto2
1Syracuse University
2University of Virginia
fjkotary, vdivitofg@syr.edu, nandoﬁoretto@gmail.com
Abstract
Model selection is a strategy aimed at creating ac-
curate and robust models. A key challenge in de-
signing these algorithms is identifying the optimal
model for classifying any particular input sample.
This paper addresses this challenge and proposes a
novel framework for differentiable model selection
integrating machine learning and combinatorial op-
timization. The framework is tailored for ensemble
learning, a strategy that combines the outputs of in-
dividually pre-trained models, and learns to select
appropriate ensemble members for a particular in-
put sample by transforming the ensemble learning
task into a differentiable selection program trained
end-to-end within the ensemble learning model.
Tested on various tasks, the proposed framework
demonstrates its versatility and effectiveness, out-
performing conventional and advanced consensus
rules across a variety of settings and learning tasks.
1 Introduction
Model selection involves the process of identifying the most
suitable model from a set of candidates for a given learning
task. The chosen model should ideally generalize well to un-
seen data, with the complexity of the model playing a crucial
role in this selection process. However, striking a balance be-
tween underﬁtting and overﬁtting is a signiﬁcant challenge.
A variety of techniques have been presented in the ma-
chine learning literature to address this issue. Of particu-
lar relevance, ensemble learning [Witten et al. , 2005] is a
meta-algorithm that combines the outputs of individually pre-
trained models, known as base learners , to improve overall
performance. Despite being trained to perform the same task,
these base learners may exhibit error diversity, meaning they
fail on different samples, and their accuracy proﬁles comple-
ment each other across an overall distribution of test samples.
The potential effectiveness of an ensemble model strongly
depends on the correlation between the base learners’ errors
across input samples and their accuracy; those with higher
accuracy and error diversity have a higher potential for im-
proved ensemble accuracy [Mienye and Sun, 2022].
However, the task of identifying the optimal ensemble of
models for classifying any particular input sample is nontriv-ial. Traditional approaches often aggregate predictions across
all base learners of an ensemble, aiming to make predictions
more robust to the error of individual base learners. While
these techniques could be enhanced by selectively applying
them to a subset of base learners known to be more reliable
on certain inputs, the design of algorithms that effectively se-
lect and combine the base learners’ individual predictions re-
mains a complex endeavor. Many consensus rule-based meth-
ods apply aggregation schemes that combine or exclude base
learners’ predictions based on static rules, thereby missing an
opportunity to inform the ensemble selection based on a par-
ticular input’s features.
Recently, the concept of differentiable model selection has
emerged, aiming to incorporate the model selection process
into the training process itself [Dona and Gallinari, 2021;
Sheth and Fusi, 2020; Fu et al. , 2016]. This approach lever-
ages gradient-based methods to optimize model selection,
proving particularly beneﬁcial in areas like neural architec-
ture search. The motivation behind differentiable model se-
lection lies in the potential to automate and optimize the
model selection process, thereby leading to superior models
and more efﬁcient selection procedures. Despite its promises,
however, it remains non-trivial how to design effective differ-
entiable model selection strategies and the use of gradient-
based methods alone further enhances the risk of converging
to local optima which can lead to suboptimal model selection.
In light of these challenges, this paper proposes a novel
framework for differentiable model selection speciﬁcally tai-
lored for ensemble learning. This framework integrates ma-
chine learning and combinatorial optimization to learn the se-
lection of ensemble members by modeling the selection pro-
cess as an optimization problem leading to optimal selections
within the prescribed context.
Contributions. In more detail, this paper makes the fol-
lowing contributions: (1)It proposes end-to-end Combinato-
rial Ensemble Learning (e2e-CEL) , a novel ensemble learn-
ing framework that exploits an integration of ML and com-
binatorial optimization to compile specialized consensus rule
for a particular input sample .(2)It shows how to cast the se-
lection and aggregation of ensemble base learner predictions
as a differentiable optimization problem, which is parameter-
ized by a deep neural network and trained end-to-end within
the ensemble learning task. (3)An analysis of challenging
learning tasks demonstrates the strengths of this idea: e2e-arXiv:2211.00251v2  [cs.LG]  19 May 2023

--- PAGE 2 ---
CEL outperforms models that attempt to select individual en-
semble members, such as the optimal weighted combination
of the individual ensemble members’ predictions as well as
conventional consensus rules, implying a much higher ability
to leverage error diversity.
These results demonstrate the integration of constrained
optimization and learning to be a key enabler to enhance the
effectiveness of model selection in machine learning tasks.
2 Related Work
Ensemble learning involves two steps: training individual
base learners and combining their outputs for accurate pre-
dictions. The composition of an ensemble from base learners
with complementary error proﬁles is commonly done through
bagging (randomly creating datasets for training each mem-
ber) and boosting (adaptively creating datasets based on error
distributions to increase error diversity). A survey of train-
ing individual base learners can be found in Mienye and Sun
[2022]. The second step is typically handled by classical ag-
gregation rules over the predictions or activation values of en-
semble members, such as majority or plurality voting. Some
works have also attempted to mathematically model more ef-
fective aggregation rules, such as the Super Learner algo-
rithm [Ju et al. , 2018] which forms a weighted combination
of base learner models that maximizes accuracy over a valida-
tion set. This algorithm has been proven to be asymptotically
optimal for combining ensemble members predictions.
This paper addresses the latter, challenging, aspect of en-
semble modeling : optimizing the aggregation of predictions
from individual ensemble base learners. The proposed e2e-
CEL approach aims to learn aggregation rules adaptively at
the level of individual input samples, rather than a single rule
for all samples. While heuristic-based selection rules to de-
rive input-dependent ensembles are not new to the literature,
to the best of our knowledge, this is the ﬁrst proposal of a
method that learns such conditional rules in an end-to-end
manner. A discussion on additional work is deferred to Ap-
pendix A.
3 Setting and Goals
The paper considers ensembles as a collection of nmodels or
base learners represented by functions fi;1in, trained
independently on separate (but possibly overlapping) datasets
(Xi,Yi), all on the same intended classiﬁcation task. On ev-
ery task studied, it assumed that ( Xi,Yi) are given, along with
a prescription for training each base learners, so that fiare
assumed to be pre-conﬁgured. This setting is common in fed-
erated analytic contexts, where base learners are often trained
on diverse datasets with skewed distributions [Kairouz et al. ,
2021], and in ML services, where providers offer a range
of pre-trained models with different architectural and hyper-
parametrization choices [Ribeiro et al. , 2015].
Letn2Nbe the number of base learners, c2Nthe number
of classes and d2Nthe input feature size. Given a sample
z2Rd, each base learner fj:Rd!Rccomputesfj(z) = ^yj.
For the classiﬁcation tasks considered in this paper, each ^yjis the direct output of a softmax function Rc!Rc,
softmax (c)i=eci
Pc
k=1eck: (1)
Explicitly, each classiﬁer fi(i;x)is trained with respect
to its parameters ito minimize a classiﬁcation loss Las
min
iE(x;y)(Xi;Yi)[L(fi(i;x);y)]: (2)
The goal is then to combine the base learners into an ensem-
ble, whose aggregated classiﬁer gperforms the same task,
but with greater overall accuracy on a master dataset (X,Y),
whereXiX andYiY for alliwith0in:
min
E(x;y)(X;Y)[L(g(;x);y)]: (3)
As is typical in ensemble learning, the base learners may be
trained in a way that increases test-error diversity among fi
onX— see Section 5. In each dataset there is an implied
train/test/validation split, so that evaluation of a trained model
is always performed on its test portion. Where this distinction
is needed, the symbols Xtrain ,Xvalid ,Xtest are used. A
list of symbols used in the paper to describe various aspects
of the computation, along with their meanings is provided in
[Kotary et al. , 2022], Table 4.
4 End-to-end Combinatorial Ensemble
Learning
Ideally, given a pretrained ensemble fi;1inand a sam-
plez2X , one would select from the ensemble a classiﬁer
which is known to produce an accurate class prediction for z.
However, a performance assessment for each base learners’
predictions is not available at test time. Thus, conventional
ensemble learning schemes resort to selection criteria such as
plurality voting (see Section 5 for a description of the aggre-
gation rules here used as a benchmark).
The end-to-end learning scheme in this work is based on
the idea that a more accurate ensemble prediction can be
made by using predictions based on z, and that selecting a
well-chosen subset of the ensemble, rather than the entire en-
semble, can provide more reliable results than a single base
learner. The size of the subset, k, is treated as a hyperparam-
eter. While it may seem logical to only choose the best pre-
dicted base learner for a given input sample (setting k= 1),
it has been consistently observed in the experiments that the
optimal performance is achieved for 1<k<n:
The proposed mechanism casts the sub-ensemble selection
as an optimization program that is end-to-end differentiable
and can thus be integrated with a learning model gto select
a reliable subset of the ensemble base learners to combine for
predictions. An end-to-end Combinatorial Ensemble Learn-
ing(e2e-CEL), or simply, smart ensemble , consists of an en-
semble of base learners along with a module that is trained by
e2e-CEL to select the sub-ensemble of sizek, which produces
the most accurate combined prediction for a given input. The
modelgis called the selection net , and the end-to-end ensem-
ble model is trained by optimizing its parameters .

--- PAGE 3 ---
Figure 1: End-to-end Ensemble Learning scheme: Black and red arrows illustrate forward and backward operations, respectively.
E2e-CEL overview. E2e-CEL is composed of three main
steps:
1. Predict a vector of scores g(z) = ^c, estimating the pre-
diction accuracy for each base learner on sample z.
2. Identify the base learner indices E  [n]which corre-
spond the the top kpredicted scores.
3. Collect the predictions of the selected sub-ensemble
fj(z)and perform an approximate majority voting
scheme over those predictions to determine the z’s class.
By training on the master set Xtrain , the smart ensemble
learns to make better predictions by virtue of learning to se-
lect better sub-ensembles to vote on its input samples. How-
ever, note that subset selection and plurality voting are dis-
crete operations, and in plain form do not offer useful gradi-
ents for backpropagation and learning. The next sections dis-
cuss further details of the e2e-CEL framework, including dif-
ferentiable approximations for each step of the overall model.
Figure 1 illustrates the e2e-CEL model and its training pro-
cess in terms of its component operations. Backpropagation
is shown with red arrows, and it only applies to the opera-
tions downstream from the selection net g, since the e2e-CEL
is parameterized by the parameters of galone.
4.1 Differentiable Model Selection
The e2e-CEL system is based on learning to select k < n
predictions from the master ensemble, given a set of input
features. This can be done by way of a structured prediction
of binary values, which are then used to mask the individual
base learner predictions.
Consider the unweighted knapsack problem
K(^c) =argmax
b^c>b (4a)
subject to 1>b=k; (4b)
b2f0;1gn; (4c)
which can be viewed as a selection problem whose optimal
solution assigns the value 1to the elements of bassociated tothe topkvalues of ^c. Relaxing constraint (4c) to 0b1
results in an equivalent linear program (LP) with discrete
optimal solutions b2 f0;1gn, despite being both convex
and composed of continuous functions. This useful property
holds for any LP with totally unimodular constraints and in-
teger right-side coefﬁcients [Bazaraa et al. , 2008].
This optimization problem can be viewed as a mapping
from ^cto a binary vector indicating its top kvalues, and rep-
resents thus a natural candidate for selection of the optimal
sub-ensemble of size kgiven the individual base learners’
predicted scores, seen as ^c. However, the outputs of Problem
(4) deﬁne a piecewise constant function, K(^c), which does
not admit readily informative gradients, posing challenges to
differentiability. For integration into the end-to-end learning
system, the function K(^c)must provide informative gradients
with respect to ^c. In this work, this challenge is overcome by
smoothingK(^c)based on perturbing ^cwith random noise.
As observed by Berthet et al. [2020], any continuous, con-
vex linear programming problem can be used to deﬁne a dif-
ferentiable perturbed optimizer , which yields approximately
the same solutions but is differentiable with respect to its lin-
ear objective coefﬁcients. Given a random noise variable Z
with probability density p(z)/exp ( v(z))wherevis a
twice differentiable function,
K(^c) =EzZ[K(^c+z)]; (5)
is a differentiable perturbed optimizer associated to K. The
temperature parameter  > 0controls the sensitivity of its
gradients (or properly, Jacobian matrix), which can itself be
represented by the expected value [Abernethy et al. , 2016]:
@K(^c)
@^c=EzZ
K(^c+z)v0(z)>
: (6)
In this work, Zis modeled as a standard Normal random
variable. While these expected values are analytically in-
tractable (due to the constrained argmax operator within the
knapsack problem K), they can be estimated to arbitrary pre-
cision by sampling in Monte Carlo fashion. This procedure is
a generalization of the Gumbel Max Trick [Gumbel, 1954].

--- PAGE 4 ---
Note that simulating Equations (5) and (6) requires solving
Problem (4) for potentially many values of z. However, al-
though the theory of perturbed optimizers requires the under-
lying problem to be a linear program, only a blackbox imple-
mentation is required to produce K(^c)[Berthet et al. , 2020],
allowing for an efﬁcient algorithm to be used in place of a
(more costly) LP solver. The complexity of evaluating the
differentiable perturbed optimizer K(^c)is discussed next.
Theorem 1. The total computation required for solving
Problem (4)isO(nlogk), wherenandkare, respectively,
the ensemble and sub-ensembles sizes.
Proof. This result relies on the observation that K(^c)can be
computed efﬁciently by identifying the top kvalues of ^cin
O(nlogk)time using a divide-and-conquer technique. See,
for example, [Cormen et al. , 2022].
Generatingmsuch solutions for gradient estimation then
requiresO(mnlogk)operations. Note, however, that these
can be performed in parallel across samples, allowing for suf-
ﬁcient noise samples to be generated for computing accurate
gradients, especially when GPU computing is available.
For clarity, note also that the function K, as a linear pro-
gram mapping, has a discrete output space since any linear
program takes its optimal solution at a vertex of its feasible
region [Bazaraa et al. , 2008], which are ﬁnite in number. As
such, it is a piecewise constant function and is differentiable
except on a set of measure zero [Folland, 1999]. However,
@K=@^c= 0 everywhere it is deﬁned, so the derivatives lack
useful information for gradient descent [Wilder et al. , 2019].
While@K=@^cis not the true derivative of Kat^c, it supplies
useful information about its direction of descent.
In practice, the forward optimization pass is modeled as
K(^c), and the backward pass is modeled as @K(^c)=@^c. This al-
lows further downstream operations, and their derivatives, to
be evaluated atK(^c)without approximation, which improves
training and test performance. These forward and backward
passes together are henceforth referred to as the Knapsack
Layer . Its explicit backward pass is computed as
@K(^c)
@^c1
mmX
i=1
K(^c+zi)v0(zi)>
; (7)
whereziN(0;1)naremindependent samples each drawn
from a standard normal distribution.
4.2 Combining Predictions
Denote asP2Rcnthe matrix whose jthcolumn is the
softmax vector ^yjof base learner j,
P= (^y1^y2:::^yn:) (8)
For the purpose of combining the ensemble base learner
predictions,K(^c)is treated as a binary masking vector b2
f0;1gn, which selects the subset of base learners for making
a prediction. Denote as B2f0;1gcnthe matrix whose ith
column isBi=  !1bi; i.e.,
B= [b1::: bn]>:This matrix is used to mask the base learner models’ softmax
predictionsPby element-wise multiplication. Next, deﬁne
Pk=BP
= [b1::: bn]>[^y1::: ^yn]: (9)
Doing so allows compute the sum of predictions over the
selected sub-ensemble E, but in a way that is automatically
differentiable, that is:
^v:=X
i2E^yi=nX
i=1P(i)
k: (10)
The e2e-CEL prediction comes from applying softmax to this
sum:
^y=softmax (^v) =softmax nX
i=1P(i)
k!
; (11)
viewing the softmax as a smooth approximation to the argmax
function as represented with one-hot binary vectors. This
function is interpreted as a smoothed majority voting to deter-
mine a class prediction: given one-hot binary class indicators
hi, the majority vote is equal to argmax(P
ihi). An illustra-
tion of the process is given in Figure 1.
At test time, class predictions are calculated as
argmax
1ic^yi(x): (12)
Combining predictions in this way allows for an approxi-
mated majority voting over a selected sub-ensemble, but in a
differentiable way so that selection net parameters can be
directly trained to produce selections that minimize the clas-
siﬁcation task loss, as detailed in the next section.
4.3 Learning Selections
The smart ensemble mechanism learns accurate class predic-
tions by learning to select better subensembles to vote on its
input samples. In turn, this is done by predicting better coef-
ﬁcients ^cwhich parameterize the Knapsack Layer.
The task of predicting ^cbased on input zis itself learned by
theselection net , a neural network model gso that ^c=g(z).
Sincegacts on the same input samples as each fi, it should
be capable of processing inputs from zat least as well as
the base learners’ models; in Section 5, the selection net in
each experiment uses the same CNN architecture as that of
the base learner models. Its predicted values are viewed as
scores over the ensemble members, rather than over the pos-
sible classes. High scores correspond to base learners which
are well-qualiﬁed to vote on the sample in question.
In practice, the selection net’s predictions ^care normalized
before input to the mapping K:
^c ^c
k^ck2: (13)
This has the effect of standardizing the magnitude of the lin-
ear objective term in (4a), and tends to improve training.
Since scaling the objective of an optimization problem has no
effect on its solution, this is equivalent to standardizing the

--- PAGE 5 ---
relative magnitudes of the linear objective and random noise
perturbations in Equations (5) and (6), preventing from be-
ing effectively absorbed into the predicted ^c.
For training input x, let^y(x)represent the associated e2e-
CEL prediction given the selection net parameters . During
training, the model minimizes the classiﬁcation loss between
these predictions and the ground-truth labels:
min
E(x;y)(X;Y)[L(^y(x);y)]: (14)
Generally, the loss function Lis chosen to be the same
as the loss used to train the base learner models, as the base
learners are trained to perform the same classiﬁcation task.
4.4 e2e-CEL Algorithm Details
Algorithm 1 summarizes the e2e-CEL procedure for train-
ing a selection net. Note that only the parameters of the se-
lection net are optimized in training, and so only its down-
stream computations are backpropagated. This is done by
the standard automatic differentiation employed in machine
learning libraries [Paszke et al. , 2019], except in the case of
the Knapsack Layer, whose gradient transformation is analyt-
ically speciﬁed by Equation (7).
For clarity, Algorithm 1 is written in terms of operations
that apply to a single input sample. In practice, however,
minibatch gradient descent is used. Each pass of the train-
ing begins evaluating the base learner models (line 4) and
sampling standard Normal noise vectors (line 5). The selec-
tion net predicts from input features xa vector of base learner
scoresg(x), which deﬁnes an unweighted knapsack problem
K(g(x))that is solved to produce the binary mask b(line 6).
Masking is applied to the base learner predictions before
being summed and softmaxed for a ﬁnal ensemble prediction
^y(line 8). The classiﬁcation loss Lis evaluated with respect
to the labelyand backpropagated in 3 steps: (1)The gradi-
ent@L
@bis computed by automatic differentiation backpropa-
gated to the Knapsack Layer’s output (line 9). (2)The chain
rule factor@b
@^cis analytically computed by the methodology of
Section 4.1 (line 10). (3)The remaining chain rule factor@^c
@is computed by automatic differentiation (line 11). Note that
as each chain rule factor is computed, it is also applied toward
computing@L
@(line 12). Finally, a SGD step [Ruder, 2016]
or one of its variants ([Diederik and Jimmy, 2014], [Zeiler,
2012]) is applied to update (line 13).
The next section evaluates the accuracy of ensemble mod-
els trained with this algorithm, on classiﬁcation tasks using
deep neural networks.
5 e2e-CEL Evaluation
The e2e-CEL training is evaluated on several vision clas-
siﬁcation tasks: digit classiﬁcation on MNIST dataset
[Deng, 2012], age-range estimation on UTKFace dataset
[Zhifei Zhang, 2017], image classiﬁcation on CIFAR10
dataset [Krizhevsky, 2009], and emotion detection on
FER2013 dataset [Liu et al. , 2016].
Being an optimized aggregation rule, e2e-CEL is com-
pared with state-of-the-art Super Learner algorithm [Ju et al. ,
2018] and the following widely adopted baseline aggregation
rules when paired with a pre-trained ensemble :Algorithm 1: Training the Selection Net
input :X;Y;;k;m;epsilon
1forepochk= 0;1;:::do
2 foreach (x;y) (X;Y)do
3 ^yi fi(x)81in
4ziN(0;1)n81im
5 (b;^c) 
K(g(x));g(x)
kg(x)k2
6Pk [b;:::;b ]>[^y1;:::; ^yn]
7 ^p softmax (Pn
i=1P(i)
k)
8@L(^p;y)=@b autodiff
9@b=@^c 1
mPm
i=1
K(^c+zi)v0(zi)>
10 @^c=@ autodiff
11 @L(^p;y)=@ @L(^p;y)
@b@b
@^c@^c
@
12  @L(^p;y)
@
•Super Learner : a fully connected neural network that, given
the base learners’ predictions, learns the optimal weighted
combinations specialized for any input sample.
•Unweighted Average : averages all the base learners’ soft-
max predictions and then compute the index of the corre-
sponding highest label score as the ﬁnal prediction.
•Plurality Voting : makes a discrete class prediction from
each base learner and then returns the most-predicted class.
•Random Selection : randomly selects a size- ksub-ensemble
of base learners for making prediction and then applies the
unweighted average rule to the selected base learners’ soft
predictions.
Experimental settings. As described in Section 1 and in
Appendix A, ensemble learning schemes are most effective
when base learner models are accurate and have high error
diversity. In this work, base learners are deliberately trained
to have high error diversity with respect to input samples be-
longing to different classes. This is done by composing for
each base learner model fi(1in)a training setXiin
which a subset of classes is over-represented, resulting base
learners that specialize in identifying those classes. The ex-
act class composition of each dataset Xidepends on the par-
ticular classiﬁcation task and on the base learner’s intended
specialization.
For each task, each base learners is designed to be spe-
cialized for recognizing either one or two particular classes.
To this end, the training set of each base learner is parti-
tioned to have a majority of samples belonging to a partic-
ular class, while the remaining part of the training dataset is
uniformly distributed across all other classes by random sam-
pling. Speciﬁcally, to compose the smart ensemble for each
task, a single base learner is trained to specialize on each pos-
sible class, and on each pair of classes (e.g., digits 1and2
in MNIST). When cis the number of classes, the experimen-
tal smart ensemble then consists of c+ c
2
total base learners.
Training a specialized base learner in this way generally leads
to high accuracy over its specialty classes, but low accuracy
over all other classes. Therefore in this experimental setup,

--- PAGE 6 ---
Accuracy (%)
Dataset Specialized Complimentary Overall
MNIST 97.5 86.8 89.6
UTKFACE 93.2 25.2 51.2
FER2013 79.4 38.1 47.8
CIFAR10 76.3 24.8 31.1
Table 1: Specialized base learner model test accuracy
no single base learner is capable of achieving high overall
accuracy on the master test set Xtest. This feature is also
recurrent in federated analytic models [Kairouz et al. , 2021].
Table 1 shows the average accuracy of individual base
learner models on their specialty classes and their non-
specialty classes; reported, respectively as specialized accu-
racy andcomplementary accuracy . The reported overall ac-
curacy is measured over the entire master test set Xtest. This
sets the stage for demonstrating the ability of e2e-CEL train-
ing to compose a classiﬁer that substantially outperforms its
base learner models on Xtest by adaptively selecting sub-
ensembles based on input features; see Section 5.2.
Note that, in each experiment, the base learner models’
architecture design, hyperparameter selection, and training
methods have not been chosen to fully optimize classiﬁca-
tion accuracy, which is not the direct goal of this work. In-
stead the base learners have been trained to maximize error
diversity, and demonstrate the ability of e2e-CEL to leverage
error diversity and compose highly accurate ensemble models
from far less accurate base learner models, in a way that is not
shared by conventional aggregation rules. Note also that im-
proving base learner model accuracies would, of course, tend
to improve the accuracy of the resulting ensemble classiﬁers.
In each case, throughout this section, the e2e-CEL selection
net is given the same CNN architecture as the base learner
models which form its ensemble.
5.1 Datasets and Settings
For each task, the base learners are trained to specialize in
classifying one or two particular classes, which allows the
selection program to leverage their error diversity. Additional
details about the base learners’ models and the dataset split
can be found in Appendix B.
Digit classiﬁcation. MNIST is a 28x28 pixel greyscale im-
ages of handwritten digits dataset, which contains 60000 im-
ages for training and 10000 images for testing. The ensemble
consists of 55base learners, 10of which specialize on 1class
and 10
2
= 45 of which specialize on 2classes.
Image classiﬁcation. CIFAR10 is a 32x32 pixel color im-
ages dataset in 10 classes: airplanes, cars, birds, cats, deer,
dogs, frogs, horses, ships, and trucks. It contains 6000 im-
ages of each class. The ensemble consists of 55base learn-
ers,10of which specialize on 1class and 10
2
= 45 of which
specialize on 2classes.
Age estimation. UTKFace is a face images dataset consist-
ing of over 20000 samples and different version of images
format. Here 9700 cropped and aligned images are split in
5classes: baby (up to 5 years old), teenager (from 6to19),Accuracy (%)
Dataset e2e-CEL SL UA PV RS
MNIST 98.55 96.88 96.81 95.99 96.83
UTKFACE 90.97 85.07 84.60 80.78 84.60
FER2013 66.31 64.95 63.89 63.15 63.89
CIFAR10 64.09 60.13 60.59 60.35 60.59
Table 2: e2e-CEL vs super learner (SL), unweighted average (UA),
plurality voting (MV), and random selection (RS), using specialized
base learners.
young (from 20to33), adult (from 34to56) and senior (more
than56years old). The classes are not uniformly distributed
per number of ages, but each class contains the same number
of samples. The goal is to estimate the person age given the
face image. The ensemble consists of 15base learners, 5 of
which specialize on 1class and 5
2
= 10 on2classes.
Emotion detection. Fer2013 is a dataset of over 30000
48x48 pixel grayscale face images, which are grouped in 7
classes: angry, disgust, fear, happy, neutral, sad and surprises.
The goal is to categorize the emotion shown in the facial ex-
pression into one category. The ensemble consists of 21base
learners, 7of which specialize on 1class and 7
2
= 21 of
which specialize on 2classes.
5.2 e2e-CEL Analysis
Thee2e-CEL strategy is tested on each experimental task for
sub-ensemble size kvarying between 1andn, and compared
to the baseline methods described above. Note in each case
that accuracy is deﬁned as the percentage of correctly classi-
ﬁed samples over the master test set.
Table 2 reports the best accuracy over all the ensemble
sizeskof ensembles trained by e2e-CEL along with that of
each baseline ensemble model, where each are formed using
the same pre-trained base learners. Note how the proposed
e2e-CEL scheme outperforms all the baseline methods, in
each task, for all but the lowest values of k.
Figure 2 reports the test accuracy found by e2e-CEL and
ensembles based on the Super Learner, weighted average,
majority voting, or random selection scheme. We make two
key observations: (1)Note from each subplot in Figure 2
that smart ensembles of size k > 1provide more accurate
predictions than baseline models that randomly select sub-
ensembles of the same size, a trend that diminishes as kin-
creases and base learner selections have less consequence (the
two perform equally when k=n).(2)In every case, the sub-
ensemble size which results in optimal performance is strictly
between 1andn.Importantly, this illustrates the motivat-
ing intuition of the e2e-CEL ensemble training. Neither the
full ensemble ( k=n), nor smart selection of a single base
learner model ( k= 1) can outperform models that use smart
selection of a sub-ensemble of any size. A well-selected sub-
ensemble has higher potential accuracy than the master en-
semble, and is, on average, more reliable than a well-selected
single base learner.
Next, Table 3 (left) reports the accuracy of the e2e-CEL
model trained on each task, along with the sub-ensemble size
that resulted in highest accuracy. In two cases, for the digit

--- PAGE 7 ---
0 10 20 30 40 50
Ensemble size3035404550556065Avg. Accuracy %Unweighted average
Plurality voting
Super Learner (*)
e2e-CEL (*)
Random selection
0 10 20 30 40 50
Ensemble size9092949698Avg. Accuracy %Unweighted average
Plurality voting
Super Learner (*)
e2e-CEL (*)
Random selection
0 2 4 6 8 10 12 14 16
Ensemble size505560657075808590Avg. Accuracy %Unweighted average
Plurality voting
Super Learner (*)
e2e-CEL (*)
Random selection
0 5 10 15 20 25
Ensemble size47.550.052.555.057.560.062.565.067.5Avg. Accuracy %Unweighted average
Plurality voting
Super Learner (*)
e2e-CEL (*)
Random selectionFigure 2: Comparison between e2e-CEL and other ensemble models at varying of the sub-ensemble size kon image classiﬁcation–CIFAR10–
(top left), digit classiﬁcation–MNIST–(top right), age estimation–UTKFace–(bottom left), and emotion detection–FER2013–(bottom right)
The (*) in the label identiﬁes methods that use specialized aggregation rules for every input sample..
Accuracy (%)
Dataset Classes Best ke2e-CEL Individual base learners
MNIST 10 10 98.55 89.6
UTKFACE 5 7 90.97 51.2
FER2013 7 13 66.31 47.8
CIFAR10 10 10 64.09 31.1
Table 3: Left: Best ensemble size (Best k) and associated e2e-CEL
test accuracy attained on each dataset. Right: Average accuracy for
the constituent ensemble base learners.
classiﬁcation and the image classiﬁcation task, the e2e-CEL
performs best when the sub-ensemble size is equal to the
number of classes. In the remaining tasks, this observation
holds approximately. This is intuitive, since the number of
base learners specializing on any class is equal to the number
of classes, and e2e-CEL is able to increase ensemble accu-
racy by learning to select these base learners for prediction.
Finally, observe the accuracy of e2e-CEL in Table 3 (left)
and the performance of the individual base learners predictors
of the ensemble tested on both the labels in which their train-
ing was specialized as well as the other labels. Note how e2e-
CEL predictions outperform their constituent base learners by
a wide margin on each task. For example, on UTKFace, the
e2e-CEL ensemble reaches an accuracy 40percentage points
higher than its average constituent base learner. This illus-
trates the ability of e2e-CEL to leverage the error diversity
of base learners to form accurate classiﬁers by composing
them based on input features , even when the individual base
learner’s accuracies are poor.
6 Conclusion
This paper was motivated by the desire to address signif-
icant challenges in model selection and ensemble learning:
the identiﬁcation of the optimal model or ensemble for clas-
sifying any particular input sample. The proposed solution is
a novel framework for differentiable model selection tailored
for ensemble learning, integrating machine learning and com-
binatorial optimization. This framework is motivated by the
idea that well-selected sub-ensembles can form more accurate
predictions than their original ensemble. It adaptively selects
sub-ensembles according to individual input samples.
The paper shows how to derive the ensemble learning task
into a differentiable selection program which is trained end-
to-end within the ensemble learning model. This approachallows the proposed framework to compose accurate classi-
ﬁcation models even from ensemble base learners with low
accuracy, a feature not shared by existing ensemble learning
approaches. The results on various tasks demonstrate the ver-
satility and effectiveness of the proposed framework, substan-
tially outperforming state-of-the-art and conventional consen-
sus algorithms in a variety of settings.
This work demonstrates that the integration of machine
learning and combinatorial optimization is a valuable toolset
for not only enhancing but also combining machine learning
models to improve performance on common tasks. This work
hopes to motivate new solutions where decision-focused
learning may be used to improve the capabilities of machine
learning systems. The proposed framework contributes to the
ongoing efforts to improve the efﬁciency and effectiveness of
model selection in machine learning, particularly in the con-
text of ensemble learning.
Ethics Statement
While the proposed e2e-CEL method has the potential to im-
prove the performance of ensemble learning, it is important
to consider the ethical implications of its use and to take steps
to mitigate any potential negative impacts. One possible con-
cern is the potential to select sub-ensembles in a way that
could perpetuate or amplify biases present in the ensemble
base learners. This could lead to unfair or discriminatory pre-
dictions for certain groups of people.
It is also important to consider the potential beneﬁt of this
study. This approach allows for the composition of accurate
classiﬁcation models even from ensemble base learners with
low accuracy or strong biases, which is a feature not shared by
existing ensemble learning approaches. The proposed solu-
tion thus aims to enhance the performance of ensemble learn-
ing models and may advance the development of more effec-
tive predictors which exhibit fewer biases.
Acknowledgements
This research is partially supported by NSF grants 2007164,
2232054, and NSF CAREER Award 2143706. Fioretto is
also supported by an Amazon Research Award and a Google
Research Scholar Award. Its views and conclusions are those
of the authors only.

--- PAGE 8 ---
Contribution Statement
James Kotary and Vincenzo Di Vito are both considered ﬁrst
authors. All authors have equal contributions.
References
Jacob Abernethy, Chansoo Lee, and Ambuj Tewari. Pertur-
bation techniques in online learning and optimization. Per-
turbations, Optimization, and Statistics , 233, 2016.
Mokhtar S Bazaraa, John J Jarvis, and Hanis D Sherali. Lin-
ear programming and network ﬂows . John Wiley & Sons,
2008.
Quentin Berthet, Mathieu Blondel, Olivier Teboul, Marco
Cuturi, Jean-Philippe Vert, and Francis Bach. Learning
with differentiable pertubed optimizers. Advances in neu-
ral information processing systems , 33:9508–9519, 2020.
Thomas H Cormen, Charles E Leiserson, Ronald L Rivest,
and Clifford Stein. Introduction to algorithms . MIT press,
2022.
Li Deng. The mnist database of handwritten digit images for
machine learning research [best of the web]. IEEE Signal
Processing Magazine , 29(6):141–142, 2012.
PK Diederik and B Jimmy. Adam: A method for stochastic
optimization. iclr. arXiv preprint arXiv:1412.6980 , 2014.
J´er´emie Dona and Patrick Gallinari. Differentiable feature se-
lection, a reparameterization approach. In Machine Learn-
ing and Knowledge Discovery in Databases. Research
Track: European Conference, ECML PKDD 2021, Bilbao,
Spain, September 13–17, 2021, Proceedings, Part III 21 ,
pages 414–429. Springer, 2021.
Gerald B Folland. Real analysis: modern techniques and
their applications , volume 40. John Wiley & Sons, 1999.
Jie Fu, Hongyin Luo, Jiashi Feng, Kian Hsiang Low, and Tat-
Seng Chua. Drmad: Distilling reverse-mode automatic dif-
ferentiation for optimizing hyperparameters of deep neural
networks, 2016.
Emil Julius Gumbel. Statistical theory of extreme values
and some practical applications: a series of lectures , vol-
ume 33. US Government Printing Ofﬁce, 1954.
Cheng Ju, Aur ´elien Bibaut, and Mark van der Laan. The rel-
ative performance of ensemble methods with deep convo-
lutional neural networks for image classiﬁcation. Journal
of Applied Statistics , 45(15):2800–2818, 2018.
Peter Kairouz, H. Brendan McMahan, Brendan Avent,
Aur´elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,
Kallista Bonawitz, Zachary Charles, Graham Cormode,
Rachel Cummings, Rafael G. L. D’Oliveira, Hubert Eich-
ner, Salim El Rouayheb, David Evans, Josh Gardner,
Zachary Garrett, Adri `a Gasc ´on, Badih Ghazi, Phillip B.
Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He,
Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu,
Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak,
Jakub Konecn ´y, Aleksandra Korolova, Farinaz Koushan-
far, Sanmi Koyejo, Tancr `ede Lepoint, Yang Liu, PrateekMittal, Mehryar Mohri, Richard Nock, Ayfer ¨Ozg¨ur, Ras-
mus Pagh, Hang Qi, Daniel Ramage, Ramesh Raskar,
Mariana Raykova, Dawn Song, Weikang Song, Sebas-
tian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian
Tram `er, Praneeth Vepakomma, Jianyu Wang, Li Xiong,
Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen
Zhao. Advances and open problems in federated learn-
ing. Foundations and Trends® in Machine Learning ,
14(1–2):1–210, 2021.
James Kotary, Vincenzo Di Vito, and Ferdinando Fioretto.
End-to-end optimization and learning for multiagent en-
sembles, 2022.
Alex Krizhevsky. Learning multiple layers of features from
tiny images. 2009.
Kuang Liu, Mingmin Zhang, and Zhigeng Pan. Facial expres-
sion recognition with cnn ensemble. In 2016 International
Conference on Cyberworlds (CW) , pages 163–166, 2016.
Ibomoiye Domor Mienye and Yanxia Sun. A survey of en-
semble learning: Concepts, algorithms, applications, and
prospects. IEEE Access , 10:99129–99149, 2022.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,
Andreas Kopf, Edward Yang, Zachary DeVito, Martin Rai-
son, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An
imperative style, high-performance deep learning library.
InAdvances in Neural Information Processing Systems 32 ,
pages 8024–8035. Curran Associates, Inc., 2019.
Mauro Ribeiro, Katarina Grolinger, and Miriam A.M.
Capretz. Mlaas: Machine learning as a service. In 2015
IEEE 14th International Conference on Machine Learning
and Applications (ICMLA) , pages 896–902, 2015.
Sebastian Ruder. An overview of gradient descent optimiza-
tion algorithms. arXiv preprint arXiv:1609.04747 , 2016.
Rishit Sheth and Nicol ´o Fusi. Differentiable feature selection
by discrete relaxation. In International Conference on Arti-
ﬁcial Intelligence and Statistics , pages 1564–1572. PMLR,
2020.
Bryan Wilder, Bistra Dilkina, and Milind Tambe. Melding
the data-decisions pipeline: Decision-focused learning for
combinatorial optimization. In AAAI , volume 33, pages
1658–1665, 2019.
Ian H Witten, Eibe Frank, Mark A Hall, Christopher J Pal,
and MINING DATA. Practical machine learning tools and
techniques. In Data Mining , volume 2, 2005.
Matthew D Zeiler. Adadelta: an adaptive learning rate
method. arXiv preprint arXiv:1212.5701 , 2012.
Hairong Qi Zhifei Zhang, Yang Song. Age progres-
sion/regression by conditional adversarial autoencoder. In
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR) . IEEE, 2017.

--- PAGE 9 ---
A Related Work
This paper proposes a framework for composing effective en-
semble learning models by adapting techniques from the in-
tersection of constrained optimization and machine learning.
This section brieﬂy reviews the two as-yet distinct areas:
Ensemble learning. Several works have focused on study-
ing ensemble models for more effective machine learning.
We refer the reader to ?for a useful categorization of ensem-
ble methods, depending on the underlying machine learning
task to be solved.
Ensemble learning often involves two distinct aspects: (1)
the training of individual ensemble learning base learners,
and(2)the aggregation of their individual outputs into ac-
curate ensemble predictions. The former aspect concerns the
composition of an ensemble from base learners with comple-
mentary error proﬁles, and is commonly handled by bagging ,
which consists of randomly composing datasets for training
each ensemble member, and boosting , which involves adap-
tively contriving a sequence of datasets based on the error
distributions of their resulting models so that error diversity
is increased. Many variations and task-speciﬁc alternatives
have also been proposed ?. For example, ?studies the effects
of various pruning techniques for reducing the ensemble size
from an initial bagging ensemble. A survey focusing on the
training of individual ensemble base learners is provided in
Mienye and Sun [2022].
The latter aspect is typically handled by classical aggrega-
tion rules over either the discrete class predictions or the con-
tinuous activation values of ensemble members. Majority and
plurality voting are consensus criteria for choosing a discrete
class prediction based on the most popular among ensem-
ble members. Some works have attempted mathematically
model more effective aggregation rules: these include super-
learners Juet al. [2018], which attempt to form a weighted
combination of base learner models that maximizes accuracy
over a validation set.
Decision focused learning. A body of work has been de-
voted to studying constrained optimization problems as learn-
able components within neural networks. See for example
the recent survey of ?. As a mapping between some problem-
deﬁning parameters and their optimal solutions, an optimiza-
tion problem can be viewed as a function whose outputs
adhere to explicitly deﬁned constraints. In this way, spe-
cial structure can be guaranteed within the predictions ?or
learned embeddings of a neural network ?. The primary chal-
lenge is that differentiation of the optimization mapping is
required for backpropagation. Distinct approaches have been
proposed, which depend on various approximations, depend-
ing on the optimization problem’s form ?,?, and ?. Smooth
convex problems, in particular, admit exact gradients via their
well-deﬁned equations for optimality, as shown by ?and
?. Convex problems which deﬁne piecewise-constant map-
pings, such as linear programs, require approximation by
smooth problems before differentiation. Smoothing tech-
niques are commonly applied to the objective function and
include the addition of regularizing objective terms, as pro-
posed by Wilder et al. [2019], and random noise, as in Berthet
et al. [2020].Symbol Semantic
n Size of the ensemble
k Size of a sub-ensemble
m Number of noise samples for perturbed optimizer
fi Pre-trained classiﬁer, 1in
^yi Softmax prediction from fi
f Smart ensemble classiﬁer
^y Smart ensemble softmax prediction
g Selection net
 Selection net parameters
^c Predicted scores from the selection net
K Unweighted knapsack function
K Perturbed unweighted knapsack function
X Input feature data
Y Target class data
L Classiﬁcation loss function
Table 4: Common symbols
In this paper, the desired structured prediction primarily
takes the form of a binary vector representing subset selec-
tion, in which k < n amongnitems are indicated for selec-
tion. This work exploits the fact that differentiation of this
structure as a linear program with respect to a parameterizing
vector allows it to be applied in an end-to-end differentiable
masking scheme that is used to select and combine the best
ensemble base learners’ predictions for a given input sample.
B Datasets and settings
This section provides information about the base learner’s
model architecture and the training dataset composition for
each task.
Digit classiﬁcation. Here the base learners use convolu-
tional neural networks (CNN), each of those with the same ar-
chitecture: 2convolutional layers and 3linear layers. Within
the convolutional layers, of 10and20output channels respec-
tively, the information is processed using a square kernel ﬁlter
of size 5x5. The training dataset of each base learner is com-
posed, on average, by 73:2%of samples belonging to one (or
two) speciﬁc class(es), while the remaining part is uniformly
distributed over all the other classes by random sampling. On
average, this results in 97:5%of accuracy in classifying a par-
ticular subset of training samples and 86:8%of accuracy in
classifying the complementary subset of training samples.
Image classiﬁcation
Here the base learners use CNN models, each of those with
the same architecture, 2convolutional layer, each containing
a square kernel ﬁlter of size 5, with 6and16output chan-
nels respectively, and 3linear layers. On average, the training
dataset of each base learner is composed by 55:2%of samples
belonging to one (or two) speciﬁc class(es), while the remain-
ing part is uniformly distributed over all the other classes by
random sampling. This results in 76:3%accuracy for classi-
fying a particular subset of training samples and 24:8%ac-
curacy for classifying the complementary subset of training
samples.
Age estimation. Here the base learner model architecture is
a pretrained version of Resnet18 on the ImageNet-1k dataset,

--- PAGE 10 ---
which is entirely re-trained. On average, the training dataset
of each base learner is composed by 58:3%of samples be-
longing to one (or two) speciﬁc class(es), while the remaining
part is uniformly distributed over all the other classes by ran-
dom sampling. This results in 93:2%accuracy for classifying
a particular subset of training samples and 25:2%accuracy
for classifying the complementary subset of training samples.
Emotion detection. Here the base learner model architec-
ture consists of 8convolutional layer and a linear layer. For
each convolutional layer, the number of output channels is
64;128;128;128;256;512;512and512; each uses a square
kernel of size 3x3. The training dataset of each base learner
is composed by 44:4%of samples belonging to one (or two)
speciﬁc class(es), while the remaining part is uniformly dis-
tributed over all the other classes by random sampling. This
results in 79:4%accuracy for classifying a particular subset
of training samples and 38:1%accuracy for classifying the
complementary subset of training samples.
C Hyperparameters
C.1 Hyperparameters of the base learners’ model
training
Here for each task, additional details about the hyperparame-
ters of the base learners’ model training are provided .
Dataset Optimizer Scheduler Learning rate Gamma Epochs Loss function
MNIST Adadelta StepLR 1 0.9 14 Cross entropy
UTKFACE Adadelta StepLR 1 0.9 10 Cross entropy
FER2013 Adam OneCycleLR 0.001 70 Cross entropy
CIFAR10 SGD 0.001 3 Cross entropy
C.2 Hyperparameters of the selection net’s model
training
Here, for each task, additional details about the hyperparam-
eters of the selection net’s model training are provided. We
recall that for each task, the selection net’s model share the
same architecture of the corresponding base learner’s model.
Optimizer Scheduler Learning rate Gamma Epochs Loss function
MNIST Adadelta 1 8 Cross entropy
UTKFACE Adadelta 1 10 Cross entropy
FER2013 Adam OneCycleLR 0.001 20 Cross entropy
CIFAR10 Adadelta 0.001 3 Cross entropy
