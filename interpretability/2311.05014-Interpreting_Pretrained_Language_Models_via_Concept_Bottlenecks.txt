# 2311.05014.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/interpretability/2311.05014.pdf
# File size: 1672392 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Interpreting Pretrained Language Models via Concept Bottlenecks
Zhen Tan
Arizona State University
ztan36@asu.edu
Yuan Bo
Zhejiang University
byuan@zju.edu.cnLu Cheng
University of Illinois Chicago
lucheng@uic.edu
Jundong Li
University of Virginia
jundong@virginia.eduSong Wang
University of Virginia
sw3wv@virginia.edu
Huan Liu
Arizona State University
huanliu@asu.edu
Abstract
Pretrained language models (PLMs) have made
significant strides in various natural language
processing tasks. However, the lack of in-
terpretability due to their “black-box” nature
poses challenges for responsible implementa-
tion. Although previous studies have attempted
to improve interpretability by using, e.g., at-
tention weights in self-attention layers, these
weights often lack clarity, readability, and intu-
itiveness. In this research, we propose a novel
approach to interpreting PLMs by employing
high-level, meaningful concepts that are easily
understandable for humans. For example, we
learn the concept of “Food” and investigate how
it influences the prediction of a model’s senti-
ment towards a restaurant review. We intro-
duce C3M, which combines human-annotated
and machine-generated concepts to extract hid-
den neurons designed to encapsulate seman-
tically meaningful and task-specific concepts.
Through empirical evaluations on real-world
datasets, we manifest that our approach offers
valuable insights to interpret PLM behavior,
helps diagnose model failures, and enhances
model robustness amidst noisy concept labels.
1 Introduction
Although Pretrained Language Models (PLMs)
like BERT (Devlin et al., 2018) have achieved re-
markable success in various NLP tasks (Zhu et al.,
2020; Liu and Lapata, 2019), they are frequently
regarded as black boxes, posing significant obsta-
cles to their responsible deployment in real-world
scenarios, particularly in critical domains such as
healthcare (Koh et al., 2020). Therefore, enabling
PLMs’ interpretability is crucial to achieve socially
responsible AI (Cheng et al., 2021). To date, many
existing works (Belinkov and Glass, 2019; Madsen
et al., 2022) leverage attention weights extracted
from the self-attention layers to provide token-level
or phrase-level importance. These low-level ex-
planations are found unfaithful (Yin and Neubig,
Figure 1: The illustration of CBE-PLMs. Via PLMs, the
original texts xis first map into an intermediate layer
consisting of a set of human-comprehensible concepts
c, which are then used to predict the target label y.
2022) and lack readability and intuitiveness (Losch
et al., 2019), leading to unstable or even unrea-
sonable explanations. To address these limitations,
we seek to explain via human-comprehensible con-
cepts that use more abstract features (e.g., general
notions) as opposed to raw input features at the to-
ken level (Zarlenga et al., 2022; Liao and Vaughan,
2023). The foundation of this work is the Con-
cept Bottleneck Models (CBMs) (Koh et al., 2020)
that interprets deep models (e.g., ResNet (He et al.,
2016)) for image classification tasks using high-
level concepts (e.g., shape). For NLP tasks such
as sentiment analysis, concepts can be Food, Am-
biance, and Service as shown in Figure 1, where
each concept corresponds to a neuron in the con-
cept bottleneck layer. The final decision layer is
then a linear function of these concepts. Using
concepts greatly improves the readability and intu-
itiveness of the explanations compared to low-level
features such as “lobster”.
We propose to study Concept-Bottleneck-
Enabled Pretrained Language Models (CBE-
PLMs). There are three key challenges: First,
CBMs cannot be directly adapted since PLMs
are pre-trained and fine-tuned on separate corpora
while CBMs work on the same end-to-end im-
age classification tasks during training and test-
ing. Therefore, the corpora used for pre-training
PLMs may contain useful text-concept correlations
that are unseen in the downstream task. An in-
vestigation of the adaptability of CBMs to CBE-
PLMs is needed. Second, the majority of existingarXiv:2311.05014v1  [cs.CL]  8 Nov 2023

--- PAGE 2 ---
CBMs (Koh et al., 2020; Zarlenga et al., 2022) re-
quire human-annotated concepts. This can be chal-
lenging for natural language since the annotator
may need to read through the entire text to under-
stand the context and label one concept (Németh
et al., 2020). This limits the practical usage and
scalability of CBE-PLMs. Third, many studies
have identified the tradeoff between interpretability
and task accuracy using CBMs since the prede-
termined concepts may leave out important infor-
mation for target task prediction (Zarlenga et al.,
2022). Therefore, it is crucial to improve both inter-
pretability and task performance to achieve optimal
interpretability-utility tradeoff.
To tackle the first challenge, we adapt standard
training strategies in CBMs (Koh et al., 2020) to
learning CBE-PLMs and conduct comprehensive
analyses to identify the best way to adapt CBMs
to interpret PLMs. For the second challenge of
concept discovery and labeling, we propose lever-
aging Large Language Models (LLMs) trained on
extensive human-generated corpora and feedbacks,
such as ChatGPT (OpenAI, 2023), to identify novel
concepts in text and generate pseudo-labels (via
prompting) for unlabeled concepts. Recent stud-
ies (Bommasani et al., 2022; OpenAI, 2023) exhibit
that these LLMs encapsulate significant amounts
of human common sense knowledge. By augment-
ing the small set of human-specified concepts with
machine-generated concepts, we increase concept
diversity and useful information for prediction. In
addition, generated pseudo-labels offer us a large
set of instances with noisy concept labels, com-
plementing the smaller set of instances with clean
labels. To further improve interpretability-utility
tradeoff (third challenge), we propose to learn from
noisy concept labels and incorporate a concept-
level MixUp mechanism (Zhang et al., 2017) that
allows CBE-PLMs to cooperatively learn from both
noisy and clean concept sets. We name our frame-
work for training CBE-PLMs as ChatGPT-guided
Concept augmentation with Concept-level Mixup
(C3M). In summary, our contributions include:
•We provide the first comprehensive investigation
of standard training strategies of CBMs for inter-
preting PLMs and benchmark CBE-PLMs.
•We propose C3M, which leverages LLMs and
MixUp to help PLMs learn from human-
annotated and machine-generated concepts.
C3M liberates CBMs from predefined concepts
and enhances the interpretability-utility tradeoff.•We demonstrate the effectiveness and robustness
of test-time concept intervention for the learned
CBE-PLMs for common text classification tasks.
2 Related Work
2.1 Interpreting Pretrained Language Models
PLMs such as Word2Vec (Mikolov et al., 2013),
BERT (Devlin et al., 2018), and the more recent
GPT series (Radford et al., 2019; Brown et al.,
2020; OpenAI, 2023) have demonstrated impres-
sive performance in various NLP tasks. However,
their opaque nature poses a challenge in compre-
hending how PLMs work internally (Diao et al.,
2022). In order to improve the interpretability and
transparency of PLMs, researchers have explored
different approaches, such as visualizing attention
weights (Galassi et al., 2020), probing feature rep-
resentations (Mishra et al., 2017; Lundberg and
Lee, 2017; Bills et al., 2023), and using counterfac-
tuals (Wu et al., 2021; Ross et al., 2021), among
others, to provide explanations at the local token-
level, instance-level, or neuron-level. However,
these methods often lack faithfulness and intuitive-
ness, and are of poor readability, which undermines
their trustworthiness (Madsen et al., 2022).
Recently, researchers have turned to global
concept-level explanations that are naturally un-
derstandable to humans. Although this level of
interpretability has been less explored in NLP com-
pared to computer vision (Goyal et al., 2019; Kim
et al., 2018; Mu and Andreas, 2020), it has gained
attention. For instance, a study (Vig et al., 2020)
investigates gender classification bias by examin-
ing the association of occupation words such as
‘nurse’ with gender. In addition, the CBMs (Koh
et al., 2020; Zarlenga et al., 2022) have emerged
as novel frameworks for achieving concept-level
interpretability in lightweight image classification
systems. CBMs typically involve a layer preced-
ing the final fully connected classifier, where each
neuron corresponds to a concept that can be inter-
preted by humans. CBMs also show advantages
in improving accuracy through human intervention
during testing. Yet, the application of CBMs to
larger-scale PLMs interpretation is under-explored.
Implementing CBMs necessitates human involve-
ment in defining the concept set and annotating the
concept labels. Such requirements are challenging
for natural language as humans may need to read
through the entire text to understand the context
and label one concept (Németh et al., 2020).

--- PAGE 3 ---
2.2 Learning from Noisy Labels
Addressing inaccurately labeled or misclassified
data in real-world scenarios is the goal of learning
from noisy labels, with techniques including noise
transition matrix estimation (Liu et al., 2022), ro-
bust risk minimization (Englesson and Azizpour,
2021), and more. Recently, the resilience of semi-
supervised learning methods like MixMatch (Berth-
elot et al., 2019) and FixMatch (Sohn et al., 2020)
to label noise has been discovered by using pseudo-
labels for unlabeled data. Inspired by them, we por-
pose to utilize an LLM (ChatGPT) as a fixed-label
guesser, generating noisy intermediate concept la-
bels to potentially predict task labels.
Notably, CBMs specialize in the interpretation
and interactability of deep models for general clas-
sification tasks. While Multi-Aspect Sentiment
Analysis (Zhang et al., 2022) (MASA) shares simi-
lar goals when using aspects as concepts, it differs
as concepts are not confined to fine-grained aspec-
tual features and can be abstract ideas or broader
notions throughout entire contexts. Aspect labels in
MASA, primarily used for prediction accuracy, are
not always mandatory. To summarize, this study
pioneers the comprehensive exploration of utiliz-
ing concepts for interpreting large-scale PLMs, and
provids a robust framework for harnessing the noisy
signals from LLMs to achieve interpretable out-
comes from lighter-weight PLMs, which can be
easily understood by users.
3 Enable Concept Bottlenecks for
Pretrained Language Models
3.1 Problem Setup
We focus on interpreting the predictions of fine-
tuned PLMs for text classification tasks. Given
dataD={(x(i), y(i), c(i))n
i=1}, where x∈Rdis
the original text input, y∈Ris the target label
to predict, and c∈Rkis a vector of kconcepts
from the concept set Cwith|C|=k, we consider a
PLM fθparameterized by θthat encodes an input
textx∈Rdinto its latent representation z∈Re.
Vanilla fine-tuning strategy, concretely defined in
Appendix A, can be abstracted as x→z→y.
Concept-Bottleneck-Enabled Pretrained Lan-
guage Models. The original concept bottlenecks in
CBMs (Koh et al., 2020) come from resizing one
of the layers in the CNN encoder to match the num-
ber of concepts. However, since PLM encoders
typically provide text representations with much
higher dimensions than the number of concepts,directly reducing the neurons in the layer would
significantly impact the quality of learned text rep-
resentation. To address this issue, we instead add
a linear layer with the sigmoid activation, denoted
aspψ, that projects the learned latent representa-
tionz∈Reinto the concept space c∈Rk. This
process can be represented as x→z→c→y.
Note that, unlike the previous works for image
classification, each concept here does not need to
be binary (i.e., present or not). We allow multi-
class concepts, e.g., the concept “Food” in a restau-
rant review can be positive, negative, or unknown.
We refer to the PLM and the projector (fθ, pψ)to-
gether as the concept encoder and the complete
model (fθ, pψ, gϕ)asConcept-Bottleneck-Enabled
Pretrained Language Models (CBE-PLMs).
During training, CBE-PLMs seek to achieve two
goals: (1) align concept prediction ˆc=pψ(fθ(x)))
tox’s ground-truth concept labels cand (2) align la-
bel prediction ˆy=gϕ(pψ(fθ(x)))to ground-truth
task labels y. We accordingly adapt the three con-
ventional strategies, independent training, sequen-
tialtraining, and joint training, proposed in (Koh
et al., 2020) to learn the CBE-PLM. Their detailed
formulations are given in Appendix A.
3.2 Benchmarking CBE-PLMs
We propose to benchmark the performance of the
vanilla fine-tuning and the three training strate-
gies for CBE-PLMs using two text classifica-
tion datasets: CEBaB (Abraham et al., 2022) and
IMDB (Maas et al., 2011). Both datasets contain
human-labeled concepts. We consider four typical
PLMs following Abraham et al. (2022). Descrip-
tions of the PLM backbones, datasets, and concept
labels are detailed in Section 5.1, Section 5.2, and
Appendix G. We consider the target task scores and
concept prediction scores as the evaluation metrics
for utility and interpretability, respectively.
CBM for CBE-PLMs . In this experiment, we
aim to identify the optimal training strategy for
CBE-PLMs. The results depicted in Figure 2 con-
firm that standard-PLMs typically yield the highest
task scores, demonstrating that the implementation
of a concept bottleneck can indeed impact target
task performance negatively. However, without
considering the concept labels, standard-PLMs lack
interpretability. In contrast, CBE-PLMs trained
jointly exhibit higher task scores and superior con-
cept prediction scores compared to their counter-
parts. This divergence from CBMs in the image

--- PAGE 4 ---
0 20 40 60 80 100
Concept Macro F1 (%)4050607080T ask Macro F1 (%)
(a) CEBaB0 20 40 60 80
Concept Accuracy (%)607080T ask Accuracy (%)
(b) IMDBLSTM-standard
GPT2-standardBERT-standard
RoBERT a-standardLSTM-independent
GPT2-independentBERT-independent
RoBERT a-independentLSTM-sequential
GPT2-sequentialBERT-sequential
RoBERT a-sequentialLSTM-joint
GPT2-jointBERT-joint
RoBERT a-jointFigure 2: Illustration of the interpretability-accuracy trade-off using various backbones. The top-right indicates a
more favorable trade-off, i.e., a better interpretability-utility Pareto front. We also show the confidence intervals for
both dimensions.
domain, where all three strategies display similar
performance (Koh et al., 2020), is notable. We
attribute this to PLMs’ extensive pretraining on nu-
merous human-generated corpora and larger param-
eter numbers than the studied vision encoders such
as ResNets (He et al., 2016). Unlike independent or
sequential training where the PLM encoder is fixed
after training on the concept labels, joint training
allows PLMs to ultize their capacity to learn con-
cepts and target labels jointly, making the learned
concept activations from the bottleneck layer better
aligned with the task labels. Given this advantage
of joint training, we adopt it as the default strategy
for training CBE-PLMs in the subsequent sections.
While initial findings from applying vanilla
CBM (Koh et al., 2020) for interpreting PLMs ap-
pear encouraging, they require human-annotated
concepts during training. This proves to be imprac-
tical in real-world situations due to the vast number
of potential concepts and the time-intensive anno-
tation process (Németh et al., 2020). Often, only
a limited number of texts come with manually la-
beled concepts. Moreover, as humans continuously
acquire new concepts, it is desirable for the train-
ing framework to discover and incorporate new
concepts automatically. Thus, we aim to design a
general framework for training CBE-PLMs.
4 C3M: A General Framework for
Learning CBE-PLMs
We define the following data potions according to
the real-world scenarios. We refer to a dataset with
human-annotated concepts as the source concept
dataset , denoted as Ds={(x(i), y(i), c(i)
s)ns
i=1},
where nsdenotes the size and cs∈Rksis a vector
ofksconcepts from the pre-defined source concept
setCs. We also consider another dataset without
concept labels, referred to as the unlabeled con-
cept dataset , denoted as Du={(x(i), y(i))nu
i=1}.The complete dataset is then the combination of
these two datasets: D={Ds,Du}.nsandks
are typically small, limiting the effectiveness of
CBE-PLMs. Specifically, small nsleads to sparse
concept labels in D, and vanilla CBM cannot be
trained on datasets with unlabeled concepts Du.
Additionally, small ksindicates that we may not
have sufficient information for model prediction.
To address these limitations, we propose ChatGPT-
guided Concept augmentation with Concept-level
Mixup (C3M), a novel framework for training CBE-
PLMs effectively. As illustrated in Figure 3, at
the high level, we augment the concept set Csand
annotate pseudo concept labels for the unlabeled
concept dataset using ChatGPT. Since these pseudo
labels are noisy, we propose a novel concept-level
MixUp to train the CBE-PLMs effectively on the
augmented dataset with noisy concept labels.
4.1 ChatGPT-guided Concept Augmentation
In this section, we detail how to leverage ChatGPT
(GPT4) to automatically (1) augment the concept
set, and (2) annotate missing concept labels.
4.1.1 Concept Set Augmentation
The goal of concept set augmentation is to automat-
ically generate high-quality concepts using human-
specified concepts Csas references. These gener-
ated concepts should be semantic meaningful and
useful for target task prediction. Inspired by LF-
CBM (Oikarinen et al., 2023), we query ChatGPT
with appropriate prompts to generate additional
concepts. Our prompts are designed using “in-
context learning” (Brown et al., 2020; Min et al.,
2022; Xie et al., 2022), and include examples from
human annotations. Below is an example of a Chat-
GPT prompt designed for a sentiment classification
task using the IMDB dataset (Maas et al., 2011):

--- PAGE 5 ---
Besides {Acting ,Storyline ,Emotional Arousal ,
Cinematography }, what are the additional important
features to judge if a {movie }is good or not?
Parentheses represent fields that can be cus-
tomized for different tasks. The concepts
Acting ,Storyline ,Emotional Arousal , and
Cinematography are from the source concept
setCswith labels manually annotated following
procedures in Appendix B. Different from LF-
CBM which generates concepts merely relying on
GPT3 (Brown et al., 2020), we further include
a small set of human-specified concepts in the
prompt to improve the quality of generated con-
cepts. This additional information can help effec-
tively filter out undesired output without additional
operations (e.g., deletions). Rarely seen concepts
are discarded using a predefined threshold and the
remaining generated concepts are referred to as
augmented concepts set Cawith size ka. Results
are given in Table 5 in Appendix G.
4.1.2 Noisy Concept Label Annotation
The next step is to automatically annotate unlabeled
concepts using noisy labels. We again leverage the
power of ChatGPT which has been shown to en-
capsulate significant amounts of human common
sense knowledge (Bommasani et al., 2022; Ope-
nAI, 2023; Singh et al., 2023) and show strong per-
formance for some text annotation tasks (Gilardi
et al., 2023). As we will also show here, LLMs
are surprisingly proficient at identifying language
concepts when suitably prompted. Using the same
example of movie reviews, the prompt for this step
is designed as follows:
a. According to the review " {text 1}", the
"{concept 1}" of the movie is "positive".
b. According to the review " {text 2}", the
"{concept 2}" of the movie is "negative".
c. According to the review " {text 3}", the
"{concept 3}" of the movie is "unknown".
d. According to the review " {texti}", how is the
"{concept i}" of the movie? Please answer with one
option in "positive, negative, or unknown".
Following a similar “in-context learning” strategy
described in Section 4.1.1, Prompts a-c are three
human-annotated examples randomly selected to
represent positive, negative, and unknown concept
labels, respectively. Prompt d is the query instance.
The goal is to obtain noisy labels for any given
{text i}and{concept i}. There are three types of
noisy concept annotations:
1.Noisy labels for human-specified concepts in
Ds. The resulting dataset ˜Dsis used to validatethe quality of labels generated by ChatGPT only
(See Table 4 in Appendix F).
2.Noisy labels for ChatGPT-generated concepts
inDs. The augmented concept set is denoted as
csa= (cs||ca)∈Rks+ka, where ||refers to the
concatenation operator and ca∈Rkastands for
the generated concepts. For example, we iden-
tify new important concepts such as Soundtrack
using ChatGPT for the IMDB movie reviews.
3.Noisy labels for both human-specified and
ChatGPT-generated concepts in unlabeled con-
cept datasets Du. The augmented concept set
is denoted as ˜csa= ( ˜cs||˜ca)∈Rks+kaand
˜cs∈Rks,˜ca∈Rkastand for the generated con-
cept labels for human-specified and ChatGPT-
generated concepts, respectively.
In summary, we transform the original dataset
with sparse concept labels into an augmented
dataset with new concepts and noisy labels: D=
{Ds,Du} → ˜D={˜Dsa,˜Du}. Examples of these
two types of qeuries are illustrated in Appendix J.
Figure 3: Illustration of the proposed framework C3M.
4.2 Learning from Noisy Concept Labels
While directly training CBE-PLMs on the trans-
formed dataset ˜Dis straightforward, this method’s
drawback is its equal treatment of human anno-
tations and ChatGPT-generated noisy labels, po-
tentially leading to prediction and interpretation
inaccuracies. To improve interpretability and ac-
curacy, we introduce a novel Concept-level MixUp
(CM) approach. It advocates for a convex behavior
of PLMs between human-annotated and ChatGPT-
generated concepts, thereby enhancing its robust-
ness against noisy concept labels.

--- PAGE 6 ---
4.2.1 Concept-level MixUp
To better utilize the noisy concept labels, CM
first linearly interpolates the texts and concept la-
bels between human-annotated concepts ( ˜Dsa) and
ChatGPT-generated concepts ( ˜Du). Specifically,
we interpolate any two text-concept-label ternaries
(x(i), c(i), y(i)),(x(j), c(j), y(j))for both their la-
tent representation ( z(i), z(j)), concepts ( c(i), c(j)),
and the task labels ( y(i), y(j)) using the MixUp ( ·)
defined as follows:
λ∼Beta(α, α);ˆλ= max( λ,1−λ);
z(i)=fθ(xi);z(j)=fθ(xj);
ˆz(i,j)=ˆλz(i)+ (1−ˆλ)z(j);
ˆc(i,j)=ˆλc(i)+ (1−ˆλ)c(j);
ˆy(i,j)=ˆλy(i)+ (1−ˆλ)y(j),(1)
where αis a hyperparameter for the Beta distri-
bution. Notably, ˆλ≥0.5preserves the order of
human-annotated concepts and ChatGPT-generated
concepts for computing individual loss components
in Eq. (4)appropriately. Then, we combine and
shuffle human-annotated and ChatGPT-annotated
data in the transformed dataset ˜D={˜Dsa,˜Du}:
W=Shuffle (˜D) =Shuffle (˜Dsa||˜Du),(2)
where||indicates the concatenation of two potions
of datasets. Next, we perform MixUp ( ·) for the ith
instance as follows:
(ˆz(i)
sa,ˆc(i)
sa,ˆy(i)
sa) =MixUp (˜D(i)
sa,W(i)),
(ˆz(i)
u,ˆc(i)
u,ˆy(i)
u) =MixUp (˜D(i)
u,W(i)).(3)
Through these steps, we can generate a "mixed
version" for each instance in ˜Dsaand˜Du, while
preserving a larger portion of the original instance.
4.2.2 Loss Function
The loss function LjointMixUp for training CBE-
PLMs with the MixUped dataset is defined below:
Lsa=Ljoint(ˆz(i)
sa,ˆc(i)
sa,ˆy(i)
sa);
Lu=Ljoint(ˆz(i)
u,ˆc(i)
u,ˆy(i)
u);
LjointMixUp =Lsa+τLu,(4)
where τis a hyperparameter and Ljoint is the joint
training loss used in vanilla CBM formulated in Ap-
pendix A. In this way, We backpropagate gradients
of the mixed noisy concept labels and gold concept
labels to update the parameters in CBE-PLMs.5 Experiments
5.1 Datasets
In this section, we give detailed descriptions of
the experimented datasets. Each of the datasets
has two components: source concept dataset and
unlabeled concept dataset ( D={Ds,Du}). Exist-
ing datasets with human-annotated concept labels
are very limited. One source concept dataset is
CEBaB (Abraham et al., 2022; Wu et al., 2022), a
common sentiment classification dataset for restau-
rant reviews. Its corresponding Duis the restaurant
reviews from the Yelp Dataset1. We also curate
another dataset for movie reviews. Specifically,
we randomly sample two portions of reviews from
the IMDB datasets (Maas et al., 2011) to repre-
sentDsandDu, respectively. Following a pre-
vious NLP work (Cai et al., 2021), we manually
annotate the concept labels for Dsin the movie re-
views. More annotation details are included in Ap-
pendix B. For convenience, we still refer to these
two new datasets as CEBaB andIMDB . Each con-
cept contains three values, i.e., Negative, Positive,
and Unknown. As described in Section 4.1, each
dataset Dis then transformed into ˜D={˜Dsa,˜Du}.
The basic statistics of the transformed datasets and
their human-annotated concepts are given in Ta-
ble 3 in Appendix E and Table 4 in Appendix F,
respectively. Note that the last column in Table 4
indicates the accuracy of ChatGPT-labeled con-
cepts in Ds, as described in Section 4.1. Table 5
in Appendix G provides statistics about augmented
concepts. Both the human-annotated and ChatGPT-
generated data, alone with the framework imple-
mentation are released2.
5.2 PLM Backbones
We experiment with the same PLM backbones
as in the CEBaB paper (Abraham et al., 2022):
GPT2 (Radford et al., 2019), BERT (Devlin et al.,
2018), RoBERTa (Liu et al., 2019), and BiL-
STM (Hochreiter and Schmidhuber, 1997) with
CBOW (Mikolov et al., 2013). For better perfor-
mance, we obtain the representations of the input
texts by pooling the embedding of all tokens. Re-
ported scores are the averages of six independent
runs, each taking 5 to 40 minutes. More implemen-
tation details and parameter values are included in
Appendix C and Table 2 in Appendix D.
1https://www.kaggle.com/datasets/omkarsabnis/
yelp-reviews-dataset
2https://github.com/Zhen-Tan-dmml/CBM_NLP.git

--- PAGE 7 ---
Table 1: Comparisons of task accuracy and interpretability using CEBaB andIMDB datasets. Metrics for both task
and concept labels are written as Accuracy /Macro F1 . Scores are reported in %. Scores in bold indicate that the
CBE-PLM under the current setting outperforms its standard PLM counterpart. CM denotes Concept-level MixUp.
Dataset CEBaB IMDB
ModelD ˜D D ˜D
Task Concept Task Concept Task Concept Task Concept
PLMsLSTM 40.57/60.67 - 43.34/64.47 - 68.25/53.37 - 90.5/90.46 -
GPT2 66.69/77.25 - 67.26/78.81 - 71.67/67.53 - 97.64/97.55 -
BERT 68.75/78.71 - 71.81/82.58 - 80.5/78.4 - 98.89/98.68 -
RoBERTa 71.36/80.17 - 73.12/82.64 - 84.1/82.5 - 99.13/99.12 -
CBE-PLMsLSTM 56.47/67.82 86.46/85.24 54.54/65.84 83.46/84.74 68.5/55.4 72.5/77.5 93.02/91.53 76.92/75.41
GPT2 64.04/77.75 92.14/92.05 63.57/74.71 90.17/90.13 70.05/69.53 80.6/82.5 96.85/96.81 86.14/88.06
BERT 67.27/79.24 93.65/92.75 68.23/78.13 89.64/90.45 77.42/74.57 80.2/83.7 97.62/97.58 92.57/92.05
RoBERTa 70.98/79.89 96.12/95.34 69.85/79.29 91.45/92.23 82.33/80.13 86.7/85.3 98.45/98.12 93.99/94.28
CBE-PLMs-CMLSTM - - 59.67/70.53 88.75/86.67 - - 94.35/92.32 83.83/84.52
GPT2 - - 65.54/77.87 93.58/92.32 - - 97.89/97.88 89.64/88.25
BERT - - 70.58/80.07 94.43/93.26 - - 98.18/98.06 94.87/94.32
RoBERTa - - 72.88/81.91 96.3/98.5 - - 99.69/99.66 96.35/96.36
5.3 Task Accuracy vs Interpretability
Table 1 presents the results for the two original
datasets ( D) and their transformed versions ( ˜D).
We have the following observations:
CBE-PLMs offer interpretability and compet-
itive task prediction performance. Compared to
standard PLMs (trained solely with task labels),
CBE-PLMs provide concept-level interpretability
with only a minor decrease in task prediction.
Interestingly, a smaller PLM, i.e., LSTM with
CBOW embeddings, achieves improved task ac-
curacy when learning from concept labels. This
suggests that the accuracy-interpretability tradeoff
in concept learning is not necessary, as opposed to
the prevailing view. Concepts can help guide PLMs
trained on smaller corpora with fewer parameters
towards better prediction performance.
Noisy concept labels can facilitate the training
of CBE-PLMs on small datasets. The extremely
limited size of the IMDB source concept dataset (de-
liberately set to 100) yields unsurprisingly low test
scores. Transforming Dinto˜Dusing ChatGPT for
noisy labeled concept instances leads to significant
improvements in both concept and task predictions
for CBE-PLMs-CM.
Uncritical learning from noisy concept labels
can impair performance. Results for CEBaB in
Table 1 demonstrate that, learning from the trans-
formed dataset ˜Ddirectly leads to inferior perfor-
mance for CBE-PLMs. Unlike IMDB , the source
concept dataset in CEBaB contains sufficient train-
ing instances, therefore, enforcing CBE-PLMs to
learn from noisy concept labels will undesirably
mislead the model, exacerbating both the concept
and task prediction performance.CBE-PLMs-CM trained via the proposed
C3M framework consistently deliver superior
interpretability-utility trade-offs. By encourag-
ing the CBE-PLMs to linearly interpolate between
examples with gold-labeled concepts and those
with ChatGPT-generated concepts, the model is
able to extract useful semantic knowledge mean-
while becoming robust to noisy concept labels. The
result is promising: We achieve the best concept-
level prediction (interpretability measure) without
sacrificing the task prediction performance, and in
some cases, CBE-PLMs trained through C3M can
even outperform their standard PLM counterparts.
5.4 Explainable Predictions
A unique advantage of CBMs is that its decision
rules can be interpreted as a linear combination of
comprehensible variables (Koh et al., 2020). In-
heriting this strength, our proposed CBE-PLMs
can deliver intuitive concept-level explanations for
predictions by assessing the activations of each con-
cept. We measure concept contribution using the
product of activation and the corresponding weight
in the linear label predictor gϕ(Oikarinen et al.,
2023). Concepts with negative activation are desig-
nated as “Neg Concept”. We highlight the concepts
contributing the most in our visualizations. Visu-
alization results are demonstrated in Figure 4 for
a toy example, while real-world CEBaB andIMDB
case studies can be found in Appendix H. These
visualizations provide new intriguing insights into
real-world applications. For instance, negative con-
cepts (e.g., Service) contribute more to the final
prediction of positive sentiment in Figure 4, mak-
ing the predicted sentiment second highest ( Y= 4)
rather than the highest ( Y= 5). Moreover, inter-

--- PAGE 8 ---
pretability results such as Figure 6 in Appendix H
imply that concepts such as “Food” and “Ambiance”
weigh more heavily in customers’ restaurant evalu-
ations compared to “Noise” and “Menu Variety”.
Figure 4: Illustration of the explainable prediction for a
toy example in restaurant review sentiment analysis.
5.5 Test-time Intervention
0 2 4 6 8 10
Number of Concepts Intervened2040608090T est Accuracy (%)
NI RI(W/O CM)
 RI OI
(a) BERT
0 2 4 6 8 10
Number of Concepts Intervened2040608090T est Accuracy (%)
NI RI(W/O CM)
 RI OI (b) GPT2
Figure 5: The results of Test-time Intervention. "NI"
denotes "no intervention", "RI (W/O CM)" denotes "ran-
dom intervention on CBE-PLMs without the concept-
level MixUp", "RI" denotes "random intervention on
CBE-PLMs", and "OI" denotes "oracle intervention".
Another strength of CBE-PLMs is that they al-
low test-time concept intervention (inherited from
CBMs), facilitating deeper, user-friendly interac-
tions. To assess this strength, we follow Koh et al.
(2020) to intervene in the predicted concepts and
investigate the impact of such interventions on test-
time prediction accuracy. Concept mispredictions
arise from ChatGPT’s incorrect labels or inaccu-
rate concept activation. Recall that the input of the
task label predictor is the predicted concept acti-
vations ˆa=pϕ(fθ(x))rather than the predicted
ternary concepts ˆc. In a concept-level interven-
tionI, the activation ˆajof the jth concept with
a target concept cjis set to the 5th, 95th, or 50th
percentile of ˆajover the training distribution for
Negative, Positive, or Unknown cjrespectively.
Multiple concepts can be intervened upon by re-
placing all related predicted concept activations
and updating the prediction. Experiments were
conducted on the transformed version ˜Dof the
CEBaB dataset. Figure 5 exhibits results for CBE-
PLMs using BERT and GPT2 as the PLM back-
bones (with similar observations for LSTM andRoBERTa). A case study is further illustrated in
Appendix I. The results reveal that task accuracy
improves substantially when more concepts are
corrected by the oracle. Additionally, while the per-
formance of CBE-PLMs declines as more concepts
are intervened upon incorrectly (randomly), the pro-
posed concept-level MixUp effectively mitigates
this impact. Notably, the decline in performance is
marginal when only two concepts are erroneously
intervened upon. These findings underscore the
pronounced advantages of test-time intervention
for CBE-PLMs trained through C3M. First, domain
experts can interact with the model to rectify any
inaccurately predicted concept values. Second, in
reality, even experts might inadvertently implement
incorrect interventions. Yet, despite this suscepti-
bility, our proposed concept-level MixUp strategy
effectively curbs performance degradation, particu-
larly when inaccuracies affect only a small subset
of the intervention. This attests to the robustness
of the proposed framework.
6 Conclusion
Our analysis began with an exhaustive examination
of three training strategies, identifying joint train-
ing as the most efficacious. Further, we proposed
the C3M framework, designed to streamline the
training process of CBE-PLMs in the presence of
incomplete concept labels. Moreover, we show-
cased the interpretability of our models in their
decision-making process and elucidated how this
comprehensibility can be harnessed to boost test
accuracy via concept intervention.
Outlook: Our research lays the groundwork
for future studies focused on enhancing the trans-
parency and robustness of PLMs. We foresee that
CBE-PLMs could potentially show more resilience
to data biases compared to standard PLMs, which
have been known to display biased performance
due to spurious correlations between sensitive at-
tributes (e.g., gender) and task labels (Wang and
Culotta, 2021; Udomcharoenchaikit et al., 2022).
For instance, a biased PLM might wrongly infer
patterns like female users writing more extreme re-
views while male users tend towards moderate ones.
CBE-PLMs, by focusing on concept labels and re-
lying solely on these concepts for classifications,
might reduce such biases. If the concepts are not
associated with sensitive attributes and their rela-
tionship with task labels is consistent, CBE-PLMs
could offer enhanced fairness.

--- PAGE 9 ---
Limitations
While our approach presents a significant step to-
wards more interpretable pretrained language mod-
els, several limitations warrant further exploration.
First, our approach relies heavily on the accuracy
of the predefined concepts. Despite the promising
results, this dependency raises the issue of poten-
tial bias present in the concept selection process
(for both human-specified and ChatGPT-generated
concepts). If a concept is not well-defined or if
important concepts are missing, this could lead to
incomplete or skewed interpretations. Second, the
methodology proposed in this paper has not been
experimented on very large language models, such
as Bloom (Scao et al., 2022). The core idea of
this framework is to utilize large language models
(LLMs) to provide explanations for comparatively
lighter-weight pretrained language models (PLMs).
Nevertheless, the proposed framework is of a uni-
versal nature and should be compatible with any
PLMs. Investigations utilizing larger PLMs are
reserved for future research endeavors. Third, the
process of prompting large language models to gen-
erate concept labels remains somewhat of an art.
While we have proposed a systematic method for
constructing desired prompts, the performance of
the model may still be sensitive to the quality and
structure of these prompts. Lastly, while our pro-
posed method shows promising results in English
language tasks, it has not been tested extensively on
other languages. This restricts its applicability in
a multilingual setting. Future work should extend
this method to other languages and conduct cross-
lingual analysis. We hope future research will build
upon our work to address these limitations, moving
us closer to truly interpretable, responsible, and
universally applicable language models.
Ethics Statement
In conducting this research, we strictly adhered to
the ACL Ethics Policy. All data used in our work
were either publicly available or anonymized, en-
suring no personally identifiable information was
involved. The work presented in this paper signifi-
cantly contributes to the field of natural language
processing and machine learning. By improving
the interpretability of pre-trained language models,
we are contributing to the creation of more transpar-
ent and trustworthy AI systems. This advancement
is expected to have broad-ranging impacts across
numerous domains that increasingly rely on AI,including healthcare, education, business, and fi-
nance, enhancing decision-making processes and
user interaction with AI systems. However, the
increased efficacy of these models could also raise
potential societal concerns if not used responsibly.
The misuse of these advanced NLP technologies
could lead to privacy breaches, the propagation of
misinformation, or the amplification of existing
biases in data. As with any powerful technology,
it is essential to consider its ethical implications
and manage its deployment with care to ensure it’s
used for the betterment of society. Our work also
underscores the need for continual research into
strategies that mitigate potential bias in AI systems
and protect user privacy. As researchers, we are
committed to working towards these goals and urge
those employing this technology to adhere to the
same principles.
References
Eldar D Abraham, Karel D’Oosterlinck, Amir Feder,
Yair Gat, Atticus Geiger, Christopher Potts, Roi Re-
ichart, and Zhengxuan Wu. 2022. Cebab: Estimating
the causal effects of real-world concepts on nlp model
behavior. Advances in Neural Information Process-
ing Systems , 35:17582–17596.
Yonatan Belinkov and James Glass. 2019. Analysis
methods in neural language processing: A survey.
Transactions of the Association for Computational
Linguistics , 7:49–72.
David Berthelot, Nicholas Carlini, Ian Goodfellow,
Nicolas Papernot, Avital Oliver, and Colin A Raf-
fel. 2019. Mixmatch: A holistic approach to semi-
supervised learning. Advances in neural information
processing systems , 32.
Steven Bills, Nick Cammarata, Dan Moss-
ing, Henk Tillman, Leo Gao, Gabriel Goh,
Ilya Sutskever, Jan Leike, Jeff Wu, and
William Saunders. 2023. Language mod-
els can explain neurons in language models.
https://openaipublic.blob.core.windows.
net/neuron-explainer/paper/index.html .
Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ
Altman, Simran Arora, Sydney von Arx, Michael S.
Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas
Card, Rodrigo Castellon, Niladri Chatterji, Annie
Chen, Kathleen Creel, Jared Quincy Davis, Dora
Demszky, Chris Donahue, Moussa Doumbouya,
Esin Durmus, Stefano Ermon, John Etchemendy,
Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor
Gale, Lauren Gillespie, Karan Goel, Noah Goodman,
Shelby Grossman, Neel Guha, Tatsunori Hashimoto,
Peter Henderson, John Hewitt, Daniel E. Ho, Jenny

--- PAGE 10 ---
Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil
Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth
Karamcheti, Geoff Keeling, Fereshte Khani, Omar
Khattab, Pang Wei Koh, Mark Krass, Ranjay Kr-
ishna, Rohith Kuditipudi, Ananya Kumar, Faisal Lad-
hak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle
Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma,
Ali Malik, Christopher D. Manning, Suvir Mirchan-
dani, Eric Mitchell, Zanele Munyikwa, Suraj Nair,
Avanika Narayan, Deepak Narayanan, Ben Newman,
Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan,
Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Pa-
padimitriou, Joon Sung Park, Chris Piech, Eva Porte-
lance, Christopher Potts, Aditi Raghunathan, Rob
Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani,
Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa
Sadigh, Shiori Sagawa, Keshav Santhanam, Andy
Shih, Krishnan Srinivasan, Alex Tamkin, Rohan
Taori, Armin W. Thomas, Florian Tramèr, Rose E.
Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai
Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan
You, Matei Zaharia, Michael Zhang, Tianyi Zhang,
Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn
Zhou, and Percy Liang. 2022. On the opportunities
and risks of foundation models.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Hongjie Cai, Rui Xia, and Jianfei Yu. 2021. Aspect-
category-opinion-sentiment quadruple extraction
with implicit aspects and opinions. In Proceedings
of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers) , pages 340–350.
Lu Cheng, Kush R Varshney, and Huan Liu. 2021. So-
cially responsible ai algorithms: Issues, purposes,
and challenges. Journal of Artificial Intelligence Re-
search , 71:1137–1181.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805 .
Shizhe Diao, Zhichao Huang, Ruijia Xu, Xuechun Li,
Yong Lin, Xiao Zhou, and Tong Zhang. 2022. Black-
box prompt learning for pre-trained language models.
arXiv preprint arXiv:2201.08531 .
Erik Englesson and Hossein Azizpour. 2021. Gener-
alized jensen-shannon divergence loss for learning
with noisy labels. Advances in Neural Information
Processing Systems , 34:30284–30297.
Andrea Galassi, Marco Lippi, and Paolo Torroni. 2020.
Attention in natural language processing. IEEE trans-
actions on neural networks and learning systems ,
32(10):4291–4308.Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli.
2023. Chatgpt outperforms crowd-workers for text-
annotation tasks. arXiv preprint arXiv:2303.15056 .
Yash Goyal, Amir Feder, Uri Shalit, and Been Kim.
2019. Explaining classifiers with causal concept ef-
fect (cace). arXiv preprint arXiv:1907.07165 .
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 770–
778.
Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation , 9(8):1735–
1780.
Been Kim, Martin Wattenberg, Justin Gilmer, Carrie
Cai, James Wexler, Fernanda Viegas, et al. 2018. In-
terpretability beyond feature attribution: Quantitative
testing with concept activation vectors (tcav). In In-
ternational conference on machine learning , pages
2668–2677. PMLR.
Evgeny Kim and Roman Klinger. 2018. Who feels what
and why? annotation of a literature corpus with se-
mantic roles of emotions. In Proceedings of the 27th
International Conference on Computational Linguis-
tics, pages 1345–1359.
Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen
Mussmann, Emma Pierson, Been Kim, and Percy
Liang. 2020. Concept bottleneck models. In Inter-
national Conference on Machine Learning , pages
5338–5348. PMLR.
Q Vera Liao and Jennifer Wortman Vaughan. 2023. Ai
transparency in the age of llms: A human-centered
research roadmap. arXiv preprint arXiv:2306.01941 .
Yang Liu, Hao Cheng, and Kun Zhang. 2022. Identifia-
bility of label noise transition matrix. arXiv preprint
arXiv:2202.02016 .
Yang Liu and Mirella Lapata. 2019. Text summariza-
tion with pretrained encoders. In Proceedings of
the 2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 3730–3740.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Max Losch, Mario Fritz, and Bernt Schiele. 2019.
Interpretability beyond classification output: Se-
mantic bottleneck networks. arXiv preprint
arXiv:1907.10882 .
Scott M Lundberg and Su-In Lee. 2017. A unified ap-
proach to interpreting model predictions. Advances
in neural information processing systems , 30.

--- PAGE 11 ---
Andrew Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of the 49th annual meeting of the associ-
ation for computational linguistics: Human language
technologies , pages 142–150.
Andreas Madsen, Siva Reddy, and Sarath Chandar. 2022.
Post-hoc interpretability for neural nlp: A survey.
ACM Computing Surveys , 55(8):1–42.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,
Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-
moyer. 2022. Rethinking the role of demonstra-
tions: What makes in-context learning work? arXiv
preprint arXiv:2202.12837 .
Saumitra Mishra, Bob L Sturm, and Simon Dixon. 2017.
Local interpretable model-agnostic explanations for
music content analysis. In ISMIR , volume 53, pages
537–543.
Jesse Mu and Jacob Andreas. 2020. Compositional
explanations of neurons. Advances in Neural Infor-
mation Processing Systems , 33:17153–17163.
Renáta Németh, Domonkos Sik, and Fanni Máté.
2020. Machine learning of concepts hard even
for humans: The case of online depression fo-
rums. International Journal of Qualitative Methods ,
19:1609406920949338.
Tuomas Oikarinen, Subhro Das, Lam M Nguyen, and
Tsui-Wei Weng. 2023. Label-free concept bottleneck
models. In The Eleventh International Conference
on Learning Representations .
OpenAI. 2023. Gpt-4 technical report.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin,
Alban Desmaison, Luca Antiga, and Adam Lerer.
2017. Automatic differentiation in pytorch. In
NeurIPS .
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Alexis Ross, Ana Marasovi ´c, and Matthew E Peters.
2021. Explaining nlp models via minimal con-
trastive editing (mice). In Findings of the Association
for Computational Linguistics: ACL-IJCNLP 2021 ,
pages 3840–3852.
Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon,
Matthias Gallé, et al. 2022. Bloom: A 176b-
parameter open-access multilingual language model.
arXiv preprint arXiv:2211.05100 .Manmeet Singh, Vaisakh SB, Neetiraj Malviya, et al.
2023. Mind meets machine: Unravelling gpt-4’s cog-
nitive psychology. arXiv preprint arXiv:2303.11436 .
Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao
Zhang, Han Zhang, Colin A Raffel, Ekin Dogus
Cubuk, Alexey Kurakin, and Chun-Liang Li. 2020.
Fixmatch: Simplifying semi-supervised learning
with consistency and confidence. Advances in neural
information processing systems , 33:596–608.
Can Udomcharoenchaikit, Wuttikorn Ponwitayarat,
Patomporn Payoungkhamdee, Kanruethai Masuk,
Weerayut Buaphet, Ekapol Chuangsuwanich, and
Sarana Nutanong. 2022. Mitigating spurious cor-
relation in natural language understanding with coun-
terfactual inference. In Proceedings of the 2022 Con-
ference on Empirical Methods in Natural Language
Processing , pages 11308–11321.
Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov,
Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart
Shieber. 2020. Investigating gender bias in language
models using causal mediation analysis. Advances
in neural information processing systems , 33:12388–
12401.
Zhao Wang and Aron Culotta. 2021. Robustness to
spurious correlations in text classification via auto-
matically generated counterfactuals. In Proceedings
of the AAAI Conference on Artificial Intelligence ,
volume 35, pages 14024–14031.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. 2020. Hug-
gingface’s transformers: State-of-the-art natural lan-
guage processing.
T Wu, M Tulio Ribeiro, J Heer, and D Weld. 2021.
Polyjuice: Generating counterfactuals for explaining,
evaluating, and improving models. In Joint Confer-
ence of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th Interna-
tional Joint Conference on Natural Language Pro-
cessing (ACL-IJCNLP 2021) .
Zhengxuan Wu, Karel D’Oosterlinck, Atticus Geiger,
Amir Zur, and Christopher Potts. 2022. Causal proxy
models for concept-based model explanations. arXiv
preprint arXiv:2209.14279 .
Sang Michael Xie, Aditi Raghunathan, Percy Liang,
and Tengyu Ma. 2022. An explanation of in-context
learning as implicit bayesian inference. In Interna-
tional Conference on Learning Representations .
Jie Yang, Yue Zhang, Linwei Li, and Xingxuan Li. 2017.
Yedda: A lightweight collaborative text span annota-
tion tool. arXiv preprint arXiv:1711.03759 .

--- PAGE 12 ---
Kayo Yin and Graham Neubig. 2022. Interpreting lan-
guage models with contrastive explanations. arXiv
preprint arXiv:2202.10419 .
Mateo Espinosa Zarlenga, Pietro Barbiero, Gabriele
Ciravegna, Giuseppe Marra, Francesco Giannini,
Michelangelo Diligenti, Frederic Precioso, Stefano
Melacci, Adrian Weller, Pietro Lio, et al. 2022. Con-
cept embedding models. In NeurIPS 2022-36th Con-
ference on Neural Information Processing Systems .
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and
David Lopez-Paz. 2017. mixup: Beyond empirical
risk minimization. arXiv preprint arXiv:1710.09412 .
Wenxuan Zhang, Xin Li, Yang Deng, Lidong Bing, and
Wai Lam. 2022. A survey on aspect-based senti-
ment analysis: tasks, methods, and challenges. IEEE
Transactions on Knowledge and Data Engineering .
Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,
Wengang Zhou, Houqiang Li, and Tieyan Liu. 2020.
Incorporating bert into neural machine translation. In
International Conference on Learning Representa-
tions .
A Definitions of Training Strategies
Given a text input x∈Rd, concepts c∈Rkand
its label y, the strategies for fine-tuning the text
encoder fθ, the projector pψand the label predictor
gϕare defined as follows:
i) Vanilla fine-tuning a PLM: The concept labels
are ignored, and then the text encoder fθand the
label predictor gϕare fine-tuned either as follows:
θ, ϕ= argmin
θ,ϕLCE(gϕ(fθ(x), y),
or as follows (frozen text encoder fθ):
ϕ= argmin
ϕLCE(gϕ(fθ(x), y),
where LCEindicates the cross-entropy loss. In this
work we only consider the former option for its
significant better performance.
ii) Independently training PLM with the concept
and task labels: The text encoder fθ, the projector
pψand the label predictor gϕare trained seperately
with ground truth concepts labels and task labels
as follows:
θ, ψ= argmin
θ,ψLCE(pψ(fθ(x)), c),
ϕ= argmin
ϕLCE(gϕ(c), y).
During inference, the label predictor will use the
output from the projector rather than the ground-
truth concepts.iii) Sequentilally training PLM with the concept
and task labels: We first learn the concept encoder
as the independent training strategy above, and then
use its output to train the label predictor:
ϕ= argmin
ϕLCE(gϕ(pψ(fθ(x), y).
iv) Jointly training PLM with the concept and task
labels: Learn the concept encoder and label predic-
tor via a weighted sum Ljoint of the two objectives
described above:
θ, ψ, ϕ = argmin
θ,ψ,ϕLjoint(x, c, y )
= argmin
θ,ψ,ϕ[LCE(gϕ(pψ(fθ(x), y)
+γLCE(pψ(fθ(x)), c)].
It’s worth noting that the CBE-PLMs trained jointly
are sensitive to the loss weight γ. We report the
most effective results here, tested value for γare
given in Table 2 in Appendix D.
B Details of the Manual Concept
Annotation for the IMDB Dataset
Our annotation policy is following a previous
work (Cai et al., 2021) for NLP datasets annotat-
ing. For the IMDB dataset, we annotate the four
concepts (Acting, Stroyline, Emotional Arousal,
Cinematography) manually. Even though the con-
cepts are naturally understandable by humans, two
Master students familiar with sentiment analysis
are selected as annotators for independent annota-
tion with the annotation tool introduced by Yang
et al. (2017). The strict quadruple matching F1
score between two annotators is 85.74%, which
indicates a consistent agreement between the two
annotators (Kim and Klinger, 2018). In case of
disagreement, a third expert will be asked to make
the final decision.
C Implementation Detail
In this section, we provide more details on the im-
plementation settings of our experiments. Specif-
ically, we implement our framework with Py-
Torch (Paszke et al., 2017) and HuggingFace (Wolf
et al., 2020) and train our framework on a sin-
gle 80GB Nvidia A100 GPU. We follow a prior
work (Abraham et al., 2022) for backbone imple-
mentation. All backbone models have a maximum
token number of 512 and a batch size of 8. We use
the Adam optimizer to update the backbone, pro-
jector, and label predictor according to Section 3.1.

--- PAGE 13 ---
The values of other hyperparameters (Table 2 in
Appendix D) for each specific PLM type are deter-
mined through grid search. We run all the experi-
ments on an Nvidia A100 GPU with 80GB RAM.
D Parameters and Notations
In this section, we provide used notations in this
paper along with their descriptions for comprehen-
sive understanding. We also list their experimented
values and optimal ones, as shown in Table 2.
E Statistics of Data Splits
The Statistics and split policies of the experimented
datasets, including the source concept dataset Ds,
the unlabeled concept dataset Du, and their aug-
mented versions. The specific details are presented
in Table 3.
F Statistics of Human-Annotated
Concepts
The Statistics of Human-Annotated Concepts in
both CEBaB andIMDB datasets. We also include
the accuracy of ChatGPT’s concept prediction here.
The specific details are presented in Table 4.
G Statistics of Concepts in Transformed
Datasets
The Statistics and split policies of the transformed
datasets of experimented datasets are presented in
Table 5.
H More Results on Explanable
Predictions
Case studies on explanable predictions for both
CEBaB andIMDB datasets are given in Figure 6 and
Figure 7 respectively.
I A case study on Test-time Intervention
We present a case study of Test-time Intervention
using an example from the transformed unlabeled
concept data ˜Duof the CEBaB dataset, as shown
in Figure 8. The first row displays the target con-
cept labels generated by ChatGPT. The second row
shows the predictions from the trained CBE-PLM
model, which mispredicts two concepts ("Waiting
time" and "Waiting area"). The third row demon-
strates test-time intervention using ChatGPT as the
oracle, which corrects the predicted task labels.Finally, the fourth row implements test-time inter-
vention with a human oracle, rectifying the concept
that ChatGPT originally mislabeled.
J Examples of Querying ChatGPT
In this paper, we query ChatGPT for 1) augmenting
the concept set, and 2) annotate missing concept
labels. Note that in practice, we query ChatGPT
(GPT4) via OpenAI API. Here we demonstrate
examples from the ChatGPT (GPT4) GUI for better
illustration. The illustrations are given in Figure 9
and Figure 10.

--- PAGE 14 ---
Table 2: Key parameters in this paper with their annotations and evaluated values. Note that bold values indicate the
optimal ones.
Notations Specification Definitions or Descriptions Values
max_len - maximum token number of input 128 / 256 / 512
batch_size - batch size 8
plm_epoch - maximum training epochs for PLM and Projector 20
clf_epoch - maximum training epochs for the linear classifier 20
hidden_dim - hidden dimension size 128
emb_dim LSTM embedding dimension for LSTM 300
lrLSTM learning rate when the backbone is LSTM 1e-1 / 1e-2 / 5e-2 / 1e-3 / 1e-4
GPT2 learning rate when the backbone is GPT2 1e-3 / 5e-3/ 1e-4 / 5e-4/ 1e-5
BERT learning rate when the backbone is BERT 1e-4 / 5e-4/ 1e-5 / 3e-5/ 5e-5
RoBERTa learning rate when the backbone is RoBERTa 1e-4 / 5e-4/ 1e-5 / 3e-5/ 5e-5
\gamma - loss weight in the joint loss Ljoint 0.1 / 0.3 / 0.5/ 0.7 / 1.0
\tau - loss weight in the joint-MixUp loss LjointMixUp 0.1 / 0.5 / 1.0/ 1.5 / 2.0
Table 3: Statistics of experimented datasets. kdenotes the number of concepts.
DatasetDs Du˜Dsa˜DuTask
Train/Dev/Test k Train/Dev/Test k Train/Dev/Test k Train/Dev/Test k
CEBaB 1755/1673/1685 4 2000/500/500 0 1755/1673/1685 10 2000/500/500 10 5-way classification
IMDB 100/50/50 4 1000/1000/1000 0 100/50/50 8 1000/1000/1000 8 2-way classification
Table 4: Statistics of human-specified concepts in Dsand the accuracy of ChatGPT’s concept prediction.
Dataset (Ds) Concept Negative Positive Unknown Total ChatGPT Acc.
CEBaBFood 1693 (33.1%) 2087 (40.8%) 1333 (26.1%) 5113 77.9%
Ambiance 787 (15.4%) 994 (19.4%) 3332 (65.2%) 5113 69.2%
Service 1249 (24.4%) 1397 (27.3%) 2467 (48.2%) 5113 78.7%
Noise 645 (12.6%) 442 (8.6%) 4026 (78.7%) 5113 77.7%
IMDBActing 76 (38%) 66 (33%) 58 (29%) 200 73.0%
Storyline 80 (40%) 77 (38.5%) 43 (21.5%) 200 64.0%
Emotional Arousal 74 (37%) 73 (36.5%) 53 (26.5%) 200 60.5%
Cinematography 118 (59%) 43 (21.5%) 39 (19.4%) 200 66.5%
Figure 6: Illustration of the explanable prediction for an example from the CEBaB dataset.

--- PAGE 15 ---
Table 5: Statistics of concepts in transformed datasets ( ˜D). Human-specified concepts are underlined . Concepts
shown in gray are not used in experiments as the portion of the "Unknown" label is too large.
Dataset Concept Negative Positive Unknown Total
CEBaBFood 2043(25.2%) 4382(54.0%) 1688(20.8%) 8113
Ambiance 868(10.7%) 1659(20.4%) 5586(68.9%) 8113
Service 1543(19.0%) 2481(30.6%) 4089(50.4%) 8113
Noise 668(8.2%) 477(5.9%) 6968(85.9%) 8113
Cleanliness 55(0.7%) 610(7.5%) 7448(91.8%) 8113
Price 714(8.8%) 527(6.5%) 6872(84.7%) 8113
Location 303(3.7%) 2598(32.0%) 5212(64.2%) 8113
Menu Variety 238(2.9%) 2501(30.8%) 5374(66.2%) 8113
Waiting Time 572(7.1%) 608(7.5%) 6933(85.5%) 8113
Waiting Area 267(3.3%) 1136(14.0%) 6710(82.7%) 8113
Parking 53(0.7%) 107(1.3%) 7953(98.0%) 8113
Wi-Fi 9(0.1%) 39(0.5%) 8065(99.4%) 8113
Kids-Friendly 15(0.2%) 536(6.6%) 7562(93.2%) 8113
IMDBSentiment 1624(50.7%) 1576(49.2%) 0(0.0%) 3200
Acting 663(20.7%) 1200(37.5%) 1337(41.8%) 3200
Storyline 1287(40.2%) 1223(38.2%) 690(21.6%) 3200
Emotiona Arousal 1109(34.7%) 1136(35.5%) 955(29.8%) 3200
Cinematography 165(5.2%) 481(15.0%) 2554(79.8%) 3200
Soundtrack 107(3.3%) 316(9.9%) 2777(86.8%) 3200
Directing 537(16.8%) 850(26.6%) 1813(56.7%) 3200
Background Setting 288(9.0%) 581(18.2%) 2331(72.8%) 3200
Editing 304(9.5%) 240(7.5%) 2656(83.0%) 3200
Figure 7: Illustration of the explanable prediction for an example from the IMDB dataset.

--- PAGE 16 ---
Figure 8: Illustration of the explanable prediction for an example from the transformed unlabeled concept data ˜Du
of the CEBaB dataset. The brown box with dash lines indicates the test-time intervention on corresponding concepts.
Figure 9: The illustration of querying ChatGPT for additional concepts for the IMDB dataset.
Figure 10: The illustration of querying ChatGPT for annotating a missing concept label for the IMDB dataset.
