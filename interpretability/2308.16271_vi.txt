# Sự Xuất Hiện của Phân Đoạn với Các Transformer Hộp Trắng Tối Giản

Yaodong Yu1,⋆Tianzhe Chu1,2,⋆Shengbang Tong1,3Ziyang Wu1Druv Pai1
Sam Buchanan4Yi Ma1,5
1UC Berkeley2ShanghaiTech3NYU4TTIC5HKU
https://ma-lab-berkeley.github.io/CRATE

Hình 1: Bản đồ tự chú ý từ một crate có giám sát với các patch 8×8 được huấn luyện sử dụng phân loại. Kiến trúc crate tự động học cách thực hiện phân đoạn đối tượng mà không cần một công thức huấn luyện tự giám sát phức tạp hoặc bất kỳ tinh chỉnh nào với các chú thích liên quan đến phân đoạn. Đối với mỗi cặp hình ảnh, chúng tôi trực quan hóa hình ảnh gốc ở bên trái và bản đồ tự chú ý của hình ảnh ở bên phải.

## Tóm tắt

Các mô hình giống Transformer cho các tác vụ thị giác gần đây đã được chứng minh hiệu quả cho một loạt các ứng dụng downstream như phân đoạn và phát hiện. Các nghiên cứu trước đây đã cho thấy rằng các thuộc tính phân đoạn xuất hiện trong vision transformers (ViTs) được huấn luyện sử dụng các phương pháp tự giám sát như DINO, nhưng không xuất hiện trong những mô hình được huấn luyện trên các tác vụ phân loại có giám sát. Trong nghiên cứu này, chúng tôi tìm hiểu liệu phân đoạn có xuất hiện trong các mô hình dựa trên transformer chỉ đơn thuần là kết quả của các cơ chế học tự giám sát phức tạp, hay liệu cùng một sự xuất hiện có thể đạt được dưới các điều kiện rộng rãi hơn nhiều thông qua thiết kế kiến trúc mô hình phù hợp. Thông qua các kết quả thực nghiệm rộng rãi, chúng tôi chứng minh rằng khi sử dụng một kiến trúc giống transformer hộp trắng được gọi là crate, mà thiết kế của nó mô hình hóa rõ ràng và theo đuổi các cấu trúc chiều thấp trong phân phối dữ liệu, các thuộc tính phân đoạn, ở cả mức độ toàn thể và các phần, đã xuất hiện với một công thức huấn luyện có giám sát tối giản. Phân tích chi tiết theo từng lớp tiết lộ rằng các thuộc tính xuất hiện mạnh mẽ củng cố các chức năng toán học được thiết kế của mạng hộp trắng. Kết quả của chúng tôi gợi ý một con đường để thiết kế các mô hình nền tảng hộp trắng vừa có hiệu suất cao vừa có thể diễn giải toán học hoàn toàn. Mã nguồn tại https://github.com/Ma-Lab-Berkeley/CRATE .

## 1. Giới thiệu

Học biểu diễn trong một hệ thống thông minh nhằm mục đích chuyển đổi dữ liệu cảm quan đa chiều cao, đa phương thức của thế giới—hình ảnh, ngôn ngữ, giọng nói—thành một dạng compact bảo tồn cấu trúc chiều thấp thiết yếu của nó, cho phép nhận dạng hiệu quả (ví dụ, phân loại), nhóm (ví dụ, phân đoạn), và theo dõi [26, 31]. Các framework học biểu diễn cổ điển, được thiết kế thủ công cho các phương thức dữ liệu và tác vụ khác biệt sử dụng các mô hình toán học cho dữ liệu [12, 38, 39, 48, 49], phần lớn đã được thay thế bởi các cách tiếp cận dựa trên deep learning, huấn luyện các mạng sâu hộp đen với lượng lớn dữ liệu không đồng nhất trên các tác vụ đơn giản, sau đó điều chỉnh các biểu diễn đã học trên

⋆Đóng góp bình đẳng.arXiv:2308.16271v1  [cs.CV]  30 Aug 2023

--- TRANG 2 ---

Hình 2: (Trái) Trực quan hóa bản đồ tự chú ý cho một hình ảnh đầu vào sử dụng mô hình crate. Các token đầu vào cho crate bao gồm N patch hình ảnh không chồng lấp và một token [CLS]. Chúng tôi sử dụng mô hình crate để chuyển đổi các token này thành biểu diễn của chúng, và giải raster hóa bản đồ tự chú ý liên kết với token [CLS] và các token patch hình ảnh tại lớp gần cuối để tạo ra một trực quan hóa bản đồ nhiệt. Chi tiết được cung cấp trong Phần 3.1. (Phải) Tổng quan về một lớp của kiến trúc crate. Mô hình crate là một kiến trúc giống transformer hộp trắng được dẫn xuất thông qua tối ưu hóa unrolled trên mục tiêu sparse rate reduction (Phần 2). Mỗi lớp nén phân phối của các token đầu vào so với một mô hình tín hiệu cục bộ, và làm sparse hóa nó trong một từ điển có thể học. Điều này làm cho mô hình có thể diễn giải toán học và có hiệu suất cao [51].

các tác vụ downstream [3, 4, 35]. Cách tiếp cận dựa trên dữ liệu này đã dẫn đến những thành công thực nghiệm to lớn—đặc biệt, các mô hình nền tảng [3] đã chứng minh kết quả state-of-the-art trong các tác vụ thị giác cơ bản như phân đoạn [22] và theo dõi [45]. Trong số các mô hình nền tảng thị giác, DINO [6, 35] thể hiện một hiện tượng thuộc tính xuất hiện đáng ngạc nhiên trong vision transformers tự giám sát (ViTs [11])—ViTs chứa thông tin phân đoạn ngữ nghĩa rõ ràng ngay cả khi không được huấn luyện với giám sát phân đoạn. Các nghiên cứu tiếp theo đã điều tra cách tận dụng thông tin phân đoạn như vậy trong các mô hình DINO và đạt được hiệu suất state-of-the-art trên các tác vụ downstream, bao gồm phân đoạn, co-segmentation, và phát hiện [2, 46].

Như được chứng minh trong Caron et al. [6], các đặc trưng lớp gần cuối trong ViTs được huấn luyện với DINO tương quan mạnh với thông tin saliency trong đầu vào thị giác—ví dụ, sự phân biệt foreground-background và ranh giới đối tượng (tương tự như các trực quan hóa được hiển thị trong Hình 1)—cho phép các đặc trưng này được sử dụng cho phân đoạn hình ảnh và các tác vụ khác. Tuy nhiên, để mang lại sự xuất hiện của các thuộc tính phân đoạn này, DINO yêu cầu một sự pha trộn tinh tế của học tự giám sát, chưng cất kiến thức, và tính trung bình trọng số trong quá trình huấn luyện. Vẫn chưa rõ liệu mọi thành phần được giới thiệu trong DINO có thiết yếu cho sự xuất hiện của các mặt nạ phân đoạn hay không. Đặc biệt, không có hành vi phân đoạn như vậy được quan sát trong các mô hình ViT có giám sát vanilla được huấn luyện trên các tác vụ phân loại [6], mặc dù DINO sử dụng cùng kiến trúc ViT làm backbone.

Trong bài báo này, chúng tôi đặt câu hỏi về quan điểm phổ biến, xuất phát từ những thành công của DINO, rằng một pipeline học tự giám sát phức tạp là cần thiết để có được các thuộc tính xuất hiện trong các mô hình thị giác giống transformer. Chúng tôi cho rằng một cách tiếp cận đầy hứa hẹn tương đương để thúc đẩy các thuộc tính phân đoạn trong transformer là thiết kế kiến trúc transformer với cấu trúc của dữ liệu đầu vào trong tâm trí, đại diện cho việc kết hợp cách tiếp cận cổ điển với học biểu diễn với framework deep learning hiện đại, dựa trên dữ liệu. Chúng tôi gọi cách tiếp cận như vậy để thiết kế kiến trúc transformer là white-box transformer, trái ngược với các kiến trúc black-box transformer (ví dụ, ViTs [11]) hiện đang thịnh hành như backbone của các mô hình nền tảng thị giác. Chúng tôi thử nghiệm với kiến trúc white-box transformer crate được đề xuất bởi Yu et al. [51], một thay thế cho ViTs trong đó mỗi lớp có thể diễn giải toán học, và chứng minh thông qua các thí nghiệm rộng rãi rằng:

Thiết kế white-box của crate dẫn đến sự xuất hiện của các thuộc tính phân đoạn trong các bản đồ tự chú ý của mạng, chỉ thông qua một công thức huấn luyện có giám sát tối giản—huấn luyện phân loại có giám sát được sử dụng trong ViTs có giám sát vanilla [11].

Chúng tôi trực quan hóa các bản đồ tự chú ý của crate được huấn luyện với công thức này trong Hình 1, có chung các hành vi định tính tương tự (phân đoạn đối tượng) với những hành vi được hiển thị trong DINO [6]. Hơn nữa, như sẽ được hiển thị trong Hình 7, mỗi attention head trong crate white-box đã học dường như bắt giữ một phần ngữ nghĩa khác nhau của các đối tượng quan tâm. Điều này đại diện cho mô hình thị giác có giám sát đầu tiên với các thuộc tính phân đoạn xuất hiện, và thiết lập white-box transformers như một hướng đầy hứa hẹn cho học biểu diễn dựa trên dữ liệu có thể diễn giải trong các mô hình nền tảng.

Phác thảo. Phần còn lại của bài báo được tổ chức như sau. Trong Phần 2, chúng tôi xem xét thiết kế của crate, mô hình white-box transformer mà chúng tôi nghiên cứu trong các thí nghiệm của chúng tôi. Trong Phần 3, chúng tôi phác thảo các phương pháp thực nghiệm để nghiên cứu phân đoạn trong các kiến trúc giống transformer, và cung cấp một phân tích cơ bản so sánh phân đoạn trong crate có giám sát với ViT có giám sát vanilla và DINO. Trong Phần 4, chúng tôi trình bày các ablation rộng rãi và phân tích chi tiết hơn về thuộc tính phân đoạn sử dụng cấu trúc white-box của crate, và chúng tôi thu được bằng chứng mạnh mẽ rằng thiết kế white-box của crate là chìa khóa cho các thuộc tính xuất hiện mà chúng tôi quan sát.

Ký hiệu. Chúng tôi ký hiệu hình ảnh (được patchified) đầu vào bằng X= [x1, . . . ,xN]∈RD×N, trong đó xi∈RD×1 đại diện cho patch hình ảnh thứ i và N đại diện cho tổng số patch hình ảnh. xi được gọi là một token, và thuật ngữ này có thể được sử dụng thay thế với patch hình ảnh. Chúng tôi sử dụng f∈ F:RD×N→ Rd×(N+1) để ký hiệu ánh xạ được cảm sinh bởi mô hình; nó là một tổ hợp của L+ 1 lớp, sao cho f=fL◦ ··· ◦ fℓ◦ ··· ◦ f1◦f0, trong đó fℓ:Rd×(N+1)→Rd×(N+1),1≤ℓ≤L đại diện cho ánh xạ của lớp thứ ℓ, và f0:X∈RD×N→Z1∈Rd×(N+1) là lớp tiền xử lý chuyển đổi các patch hình ảnh X= [x1, . . . ,xN] thành tokens Z1=z1[CLS],z11, . . . ,z1N, trong đó z1[CLS] ký hiệu "class token", một tham số mô hình cuối cùng được sử dụng cho phân loại có giám sát trong thiết lập huấn luyện của chúng tôi. Chúng tôi để Zℓ=zℓ[CLS],zℓ1, . . . ,zℓN∈Rd×(N+1)(1) ký hiệu các token đầu vào của lớp thứ ℓ fℓ với 1≤ℓ≤L, sao cho zℓi∈Rd cho biểu diễn của patch hình ảnh thứ i xi trước lớp thứ ℓ, và zℓ[CLS]∈Rd cho biểu diễn của class token trước lớp thứ ℓ. Chúng tôi sử dụng Z=ZL+1 để ký hiệu các token đầu ra của lớp cuối cùng (thứ L).

## 2. Kiến thức cơ bản: White-Box Vision Transformers

Trong phần này, chúng tôi xem xét lại kiến trúc crate (Coding RAte reduction Transform Er)—một white-box vision transformer được đề xuất trong Yu et al. [51]. crate có nhiều đặc điểm khác biệt so với kiến trúc vision transformer (ViT) [11] làm nền tảng cho các biểu diễn thị giác xuất hiện mà chúng tôi quan sát trong các thí nghiệm của mình. Chúng tôi đầu tiên giới thiệu kiến trúc mạng của crate trong Phần 2.1, và sau đó trình bày cách học các tham số của crate thông qua học có giám sát trong Phần 2.2.

### 2.1. Thiết kế của crate—Một Mô hình White-Box Transformer

Học biểu diễn thông qua unrolling optimization. Như được mô tả trong Yu et al. [51], white-box transformer crate f:RD×N→Rd×(N+1) được thiết kế để chuyển đổi dữ liệu đầu vào X∈RD×N được rút ra từ một phân phối có khả năng phi tuyến và đa phương thức thành các biểu diễn đặc trưng tuyến tính từng phần và compact Z∈Rd×(N+1). Nó thực hiện điều này bằng cách đặt ra một mô hình tín hiệu cục bộ cho phân phối biên của các token zi. Cụ thể, nó khẳng định rằng các token được hỗ trợ gần đúng trên một hợp của nhiều, giả sử K, không gian con chiều thấp, giả sử có chiều p≪d, có các cơ sở trực chuẩn được cho bởi U[K]= (Uk)Kk=1 trong đó mỗi Uk∈Rd×p. Đối với mô hình tín hiệu cục bộ này, mô hình crate được thiết kế để tối ưu hóa mục tiêu sparse rate reduction [51]:

max f∈FEZ ∆R(Z|U[K])−λ∥Z∥0 = max f∈FEZ R(Z)−λ∥Z∥0−Rc(Z;U[K]), (2)

trong đó Z=f(X), coding rate R(Z) là (một xấp xỉ chặt cho [30]) số bit trung bình cần thiết để mã hóa các token zi với độ chính xác ε sử dụng một codebook Gaussian, và Rc(Z|U[K]) là một cận trên của số bit trung bình cần thiết để mã hóa các hình chiếu của token lên mỗi không gian con trong mô hình tín hiệu cục bộ, tức là, U∗kzi, với độ chính xác ε sử dụng một codebook Gaussian [51]. Khi các không gian con này đủ không coherent, các minimizer của mục tiêu (2) như một hàm của Z tương ứng với các sắp xếp không gian con axis-aligned và không coherent [52].

Do đó, một mạng được thiết kế để tối ưu hóa (2) bằng unrolled optimization [7, 16, 32] từng bước chuyển đổi phân phối của X hướng tới các dạng canonical mong muốn: mỗi lần lặp của unrolled optimization trở thành một lớp của biểu diễn f, cụ thể

Zℓ+1=fℓ(Zℓ), (3)

với biểu diễn tổng thể f được xây dựng như vậy là

f:X f0−−→Z1→ ··· → Zℓ fℓ−−→Zℓ+1→ ··· → ZL+1=Z. (4)

Quan trọng, trong paradigm unrolled optimization, mỗi lớp fℓ có mô hình tín hiệu cục bộ riêng, không ràng buộc, Uℓ[K]: mỗi lớp mô hình hóa phân phối của các token đầu vào Zℓ, cho phép tuyến tính hóa các cấu trúc phi tuyến trong phân phối đầu vào X ở quy mô toàn cầu trong quá trình áp dụng f.

Framework unrolled optimization ở trên cho phép nhiều lựa chọn thiết kế để thực hiện các lớp fℓ tối ưu hóa từng bước (2). crate sử dụng một cách tiếp cận alternating minimization hai giai đoạn với cơ sở khái niệm mạnh [51], mà chúng tôi tóm tắt ở đây và mô tả chi tiết bên dưới:

1. Đầu tiên, phân phối của các token Zℓ được nén so với mô hình tín hiệu cục bộ Uℓ[K] bằng một bước gradient gần đúng trên Rc(Z|Uℓ[K]) để tạo ra một biểu diễn trung gian Zℓ+1/2;

2. Thứ hai, biểu diễn trung gian này được mã hóa sparse sử dụng một từ điển có thể học Dℓ để tạo ra biểu diễn lớp tiếp theo Zℓ+1.

Các thí nghiệm chứng minh rằng ngay cả sau huấn luyện có giám sát, crate đạt được các mục tiêu thiết kế cho học biểu diễn được phát biểu ở trên [51].

Toán tử nén: Multi-Head Subspace Self-Attention (MSSA). Cho các mô hình cục bộ Uℓ[K], để hình thành chuyển đổi từng bước fℓ tối ưu hóa (2) tại lớp ℓ, crate đầu tiên nén tập token Zℓ so với các không gian con bằng cách cực tiểu hóa coding rate Rc(· |Uℓ[K]). Như Yu et al. [51] chỉ ra, thực hiện việc cực tiểu hóa này cục bộ bằng cách thực hiện một bước gradient descent trên Rc(· |Uℓ[K]) dẫn đến hoạt động multi-head subspace self-attention (MSSA) được gọi là:

MSSA(Z|U[K]).=√(N+ 1)ε2[U1, . . . ,UK] ((U∗1Z) softmax(( U∗1Z)∗(U∗1Z)) ... (U∗KZ) softmax(( U∗KZ)∗(U∗KZ))), (5)

và biểu diễn trung gian tiếp theo

Zℓ+1/2=Zℓ−κ∇ZRc(Zℓ|U[K])≈ (1−κ·√(N+ 1)ε2) Zℓ+κ·√(N+ 1)ε2·MSSA(Zℓ|U[K]), (6)

trong đó κ >0 là một hyperparameter tốc độ học. Block này có sự tương đồng đáng chú ý với multi-head self-attention block của ViT, với một khác biệt quan trọng: các ma trận hình chiếu query, key, và value thông thường trong một head đều giống hệt nhau, và được xác định bởi mô hình cục bộ của chúng ta cho phân phối của các token đầu vào. Chúng tôi sẽ chứng minh thông qua các nghiên cứu ablation cẩn thận rằng sự khác biệt này là quan trọng cho sự xuất hiện của các biểu diễn thị giác hữu ích trong crate.

Toán tử sparsification: Iterative Shrinkage-Thresholding Algorithm (ISTA). Thuật ngữ còn lại để tối ưu hóa trong (2) là hiệu số của global coding rate R(Z) và chuẩn ℓ0 của các token, cùng nhau khuyến khích các biểu diễn vừa sparse vừa không bị collapse. Yu et al. [51] chỉ ra rằng cực tiểu hóa cục bộ của mục tiêu này trong một lân cận của các biểu diễn trung gian Zℓ+1/2 được đạt được gần đúng bởi một bài toán LASSO đối với một từ điển trực giao sparsifying Dℓ. Thực hiện một bước lặp hướng tới giải quyết bài toán LASSO này cho ra thuật toán iterative shrinkage-thresholding (ISTA) [47, 51]:

Zℓ+1=fℓ(Zℓ) = ReLU( Zℓ+1/2+ηDℓ∗(Zℓ+1/2−DℓZℓ+1/2)−ηλ1).=ISTA(Zℓ+1/2|Dℓ). (7)

Ở đây, η > 0 là một step size, và λ > 0 là regularizer sparsification. Phi tuyến ReLU xuất hiện trong block này phát sinh từ một ràng buộc nonnegativity bổ sung trên các biểu diễn trong chương trình LASSO, được thúc đẩy bởi mục tiêu tách biệt tốt hơn các mode biến thiên khác biệt trong phân phối token [17]. Block ISTA gợi nhớ đến MLP block trong ViT, nhưng với một skip connection được di chuyển.

Kiến trúc crate tổng thể. Kết hợp MSSA và ISTA block, như trên, cùng với một lựa chọn phù hợp của các hyperparameter, chúng ta đến với định nghĩa của một lớp crate đơn:

Zℓ+1/2.=Zℓ+MSSA(Zℓ|Uℓ[K]), fℓ(Zℓ) =Zℓ+1.=ISTA(Zℓ+1/2|Dℓ). (8)

Các lớp này được tổ hợp để thu được biểu diễn f, như trong (4). Chúng tôi trực quan hóa kiến trúc crate trong Hình 2. Pseudocode đầy đủ (cả toán học và kiểu PyTorch) được cung cấp trong Phụ lục A.

Forward và backward pass của crate. Framework khái niệm ở trên tách vai trò của "optimization" forward, trong đó mỗi lớp từng bước chuyển đổi đầu vào của nó hướng tới một biểu diễn compact

--- TRANG 5 ---

CRATECRATECRATE ViTViTViT

Hình 3: Trực quan hóa các thành phần PCA. Chúng tôi tính PCA của các biểu diễn patch-wise của mỗi cột và trực quan hóa 3 thành phần đầu tiên cho đối tượng foreground. Mỗi thành phần được khớp với một kênh RGB khác nhau và background được loại bỏ bằng cách ngưỡng hóa thành phần PCA đầu tiên của toàn bộ hình ảnh. Các biểu diễn của crate được căn chỉnh tốt hơn, và với ít tương quan giả hơn, với các thành phần texture và shape của đầu vào so với những thành phần của ViT. Xem pipeline trong Phụ lục B.2 để biết thêm chi tiết.

và có cấu trúc thông qua nén và sparsification của các biểu diễn token sử dụng các mô hình tín hiệu cục bộ Uℓ[K] và từ điển sparsifying Dℓ tại mỗi lớp, và "learning" backward, trong đó các mô hình tín hiệu cục bộ và từ điển sparsifying được học từ huấn luyện có giám sát (như trong các thí nghiệm của chúng tôi) hoặc tự giám sát thông qua back propagation để nắm bắt các cấu trúc trong dữ liệu.

Chúng tôi tin rằng các thiết kế toán học rõ ràng như vậy của crate đóng vai trò chủ chốt trong sự xuất hiện của các thuộc tính có ý nghĩa ngữ nghĩa trong các mô hình đã học cuối cùng, như chúng ta sẽ sớm thấy.

### 2.2. Huấn luyện crate với Học có Giám sát

Như được mô tả trong phần trước, cho các mô hình tín hiệu cục bộ (Uℓ[K])Lℓ=1 và từ điển sparsifying (Dℓ)Lℓ=1, mỗi lớp của crate được thiết kế để tối ưu hóa mục tiêu sparse rate reduction (2). Để cho phép nén và sparsification hiệu quả hơn, các tham số của mô hình tín hiệu cục bộ cần được xác định. Nghiên cứu trước [51] đề xuất học các tham số (Uℓ[K],Dℓ)Lℓ=1 từ dữ liệu, cụ thể theo cách có giám sát bằng cách giải quyết bài toán phân loại sau:

min W,f ∑i ℓCE(WzL+1i,[CLS], yi) trong đó [zL+1i,[CLS],zL+1i,1, . . . ,zL+1i,N] =f(Xi), (9)

trong đó (Xi, yi) là cặp (hình ảnh, nhãn) huấn luyện thứ i, W∈Rd×C ánh xạ token [CLS] thành một vector của logits, C là số lượng lớp, và ℓCE(·,·) ký hiệu softmax cross-entropy loss.1

## 3. Đo lường Các Thuộc tính Xuất hiện trong crate

Bây giờ chúng tôi nghiên cứu các thuộc tính phân đoạn xuất hiện trong crate có giám sát cả định tính và định lượng. Như được chứng minh trong nghiên cứu trước [6], phân đoạn trong ViT [11] chỉ xuất hiện khi áp dụng DINO, một phương pháp học tự giám sát rất chuyên biệt [6]. Đặc biệt, một ViT vanilla được huấn luyện trên phân loại có giám sát không phát triển khả năng thực hiện phân đoạn. Ngược lại, như chúng tôi chứng minh cả định tính và định lượng trong Phần 3 và Phần 4, các thuộc tính phân đoạn xuất hiện trong crate ngay cả khi sử dụng huấn luyện phân loại có giám sát tiêu chuẩn.

Kết quả thực nghiệm của chúng tôi chứng minh rằng học tự giám sát, cũng như các tùy chọn thiết kế chuyên biệt trong DINO [6] (ví dụ, momentum encoder, mạng student và teacher, tự chưng cất, v.v.) không cần thiết cho sự xuất hiện của phân đoạn. Chúng tôi huấn luyện tất cả các mô hình (crate và ViT) với cùng số lượng dữ liệu và lần lặp, cũng như optimizer, để đảm bảo các thí nghiệm và ablation cung cấp một so sánh công bằng—chi tiết chính xác được cung cấp trong Phụ lục C.1.

1Điều này tương tự như huấn luyện ViT có giám sát được sử dụng trong Dosovitskiy et al. [11].

--- TRANG 6 ---

Supervised CRATE Supervised ViT

Supervised CRATE Supervised ViT

ViT CRATE (a) Trực quan hóa phân đoạn ngữ nghĩa thô.

| Model | Train | mIoU |
|--------|-------|------|
| crate-S/8 | Supervised | 23.9 |
| crate-B/8 | Supervised | 23.6 |
| ViT-S/8 | Supervised | 14.1 |
| ViT-B/8 | Supervised | 19.2 |
| ViT-S/8 | DINO | 27.0 |
| ViT-B/8 | DINO | 27.3 |

(b) Đánh giá mIoU.

Hình 4: Phân đoạn ngữ nghĩa thô thông qua bản đồ tự chú ý. (a) Chúng tôi trực quan hóa các mặt nạ phân đoạn cho cả crate và ViT có giám sát. Chúng tôi chọn attention head có hiệu suất phân đoạn tốt nhất cho crate và ViT riêng biệt. (b) Chúng tôi đánh giá định lượng mặt nạ phân đoạn thô bằng cách đánh giá điểm mIoU trên tập validation của PASCAL VOC12 [13]. Nhìn chung, crate thể hiện hiệu suất phân đoạn vượt trội so với ViT có giám sát cả định tính (ví dụ, trong (a), nơi các bản đồ phân đoạn sạch hơn nhiều và phác thảo đối tượng mong muốn), và định lượng (ví dụ, trong (b)).

### 3.1. Đo lường Định tính

Trực quan hóa bản đồ tự chú ý. Để đo lường định tính hiện tượng xuất hiện, chúng tôi áp dụng cách tiếp cận bản đồ attention dựa trên token [CLS], đã được sử dụng rộng rãi như một cách để diễn giải và trực quan hóa các kiến trúc giống transformer [1, 6]. Thực tế, chúng tôi sử dụng cùng phương pháp như [1, 6], lưu ý rằng trong crate các ma trận query-key-value đều giống nhau; một tài khoản chính thức hơn được hoãn lại đến Phụ lục B.1. Kết quả trực quan hóa của các bản đồ tự chú ý được tóm tắt trong Hình 1 và Hình 7. Chúng tôi quan sát rằng các bản đồ tự chú ý của mô hình crate tương ứng với các vùng ngữ nghĩa trong hình ảnh đầu vào. Kết quả của chúng tôi gợi ý rằng mô hình crate mã hóa một phân đoạn ngữ nghĩa rõ ràng của mỗi hình ảnh trong các biểu diễn nội bộ của mạng, tương tự như phương pháp tự giám sát DINO [6]. Ngược lại, như được hiển thị trong Hình 14 trong Phụ lục, ViT vanilla được huấn luyện trên phân loại có giám sát không thể hiện các thuộc tính phân đoạn tương tự.

Trực quan hóa PCA cho biểu diễn patch-wise. Theo nghiên cứu trước [2, 35] về trực quan hóa các đặc trưng sâu patch-wise đã học của hình ảnh, chúng tôi nghiên cứu phân tích thành phần chính (PCA) trên các biểu diễn token sâu của các mô hình crate và ViT. Một lần nữa, chúng tôi sử dụng cùng phương pháp như các nghiên cứu trước [2, 35], và một tài khoản đầy đủ hơn về phương pháp được hoãn lại đến Phụ lục B.2. Chúng tôi tóm tắt kết quả trực quan hóa PCA của crate có giám sát trong Hình 3. Không có giám sát phân đoạn, crate có thể nắm bắt ranh giới của đối tượng trong hình ảnh. Hơn nữa, các thành phần chính thể hiện sự căn chỉnh đặc trưng giữa các token tương ứng với các phần tương tự của đối tượng; ví dụ, kênh đỏ tương ứng với chân ngựa. Mặt khác, trực quan hóa PCA của mô hình ViT có giám sát ít có cấu trúc hơn đáng kể. Chúng tôi cũng cung cấp thêm kết quả trực quan hóa PCA trong Hình 9.

### 3.2. Đo lường Định lượng

Bên cạnh đánh giá định tính các thuộc tính phân đoạn thông qua trực quan hóa, chúng tôi cũng đánh giá định lượng thuộc tính phân đoạn xuất hiện của crate sử dụng các kỹ thuật phân đoạn và phát hiện đối tượng hiện có [6, 46]. Cả hai phương pháp đều áp dụng các biểu diễn sâu nội bộ của transformers, như các bản đồ tự chú ý đã thảo luận trước đó, để tạo ra các mặt nạ phân đoạn mà không cần huấn luyện thêm trên các chú thích đặc biệt (ví dụ, object boxes, masks, v.v.).

Phân đoạn thô thông qua bản đồ tự chú ý. Như được hiển thị trong Hình 1, crate rõ ràng nắm bắt ngữ nghĩa cấp độ đối tượng với ranh giới rõ ràng. Để đo lường định lượng chất lượng của phân đoạn được cảm sinh, chúng tôi sử dụng các bản đồ tự chú ý thô đã thảo luận trước đó để tạo ra các mặt nạ phân đoạn. Sau đó, chúng tôi đánh giá điểm mIoU (mean intersection over union) tiêu chuẩn [28] bằng cách so sánh các mặt nạ phân đoạn được tạo ra với các mặt nạ ground truth. Cách tiếp cận này đã được sử dụng trong nghiên cứu trước về đánh giá hiệu suất phân đoạn của các bản đồ tự chú ý [6]. Một tài khoản chi tiết hơn về phương pháp được tìm thấy trong Phụ lục B.3. Kết quả được tóm tắt trong Hình 4. crate phần lớn vượt trội ViT cả về mặt thị giác và về mặt mIoU, gợi ý rằng các biểu diễn nội bộ trong crate hiệu quả hơn nhiều để tạo ra các mặt nạ phân đoạn.

Phát hiện đối tượng và phân đoạn chi tiết. Để xác thực và đánh giá thêm thông tin ngữ nghĩa phong phú được nắm bắt bởi crate, chúng tôi sử dụng MaskCut [46], một cách tiếp cận hiệu quả gần đây cho phát hiện và phân đoạn đối tượng không yêu cầu chú thích của con người. Như thường lệ, chúng tôi cung cấp một mô tả phương pháp chi tiết hơn trong Phụ lục B.4. Quy trình này cho phép chúng tôi trích xuất thêm

--- TRANG 7 ---

ViT CRATE

Supervised CRATE Supervised ViT

Supervised CRATE Supervised ViT

Hình 5: Trực quan hóa trên COCO val2017 [27] với MaskCut. (Hàng Trên) Kiến trúc crate có giám sát rõ ràng phát hiện các đối tượng chính trong hình ảnh. (Hàng Dưới) ViT có giám sát đôi khi thất bại trong việc phát hiện các đối tượng chính trong hình ảnh (cột 2, 3, 4).

| Detection | | | Segmentation | | |
|-----------|--|--|-------------|--|--|
| Model | Train | AP 50 | AP75 | AP | AP 50 | AP75 | AP |
| crate-S/8 | Supervised | 2.9 | 1.0 | 1.1 | 1.8 | 0.7 | 0.8 |
| crate-B/8 | Supervised | 2.9 | 1.0 | 1.3 | 2.2 | 0.7 | 1.0 |
| ViT-S/8 | Supervised | 0.1 | 0.1 | 0.0 | 0.0 | 0.0 | 0.0 |
| ViT-B/8 | Supervised | 0.8 | 0.2 | 0.4 | 0.7 | 0.5 | 0.4 |
| ViT-S/8 | DINO | 5.0 | 2.0 | 2.4 | 4.0 | 1.3 | 1.7 |
| ViT-B/8 | DINO | 5.1 | 2.3 | 2.5 | 4.1 | 1.3 | 1.8 |

Bảng 1: Phát hiện đối tượng và phân đoạn chi tiết thông qua MaskCut trên COCO val2017 [27]. Chúng tôi xem xét các mô hình với các quy mô khác nhau và đánh giá average precision được đo bằng metric đánh giá chính thức của COCO. Bốn mô hình đầu tiên được pre-train với các tác vụ phân loại hình ảnh dưới sự giám sát nhãn; hai mô hình dưới cùng được pre-train thông qua kỹ thuật tự giám sát DINO [6]. crate kết luận có hiệu suất tốt hơn ViT tại các metric phát hiện và phân đoạn khi cả hai được huấn luyện sử dụng phân loại có giám sát.

phân đoạn chi tiết từ một hình ảnh dựa trên các biểu diễn token được học trong crate. Chúng tôi trực quan hóa các phân đoạn chi tiết được tạo ra bởi MaskCut trong Hình 5 và so sánh hiệu suất phân đoạn và phát hiện trong Bảng 1. Dựa trên các kết quả này, chúng tôi quan sát rằng MaskCut với các đặc trưng ViT có giám sát hoàn toàn thất bại trong việc tạo ra các mặt nạ phân đoạn trong một số trường hợp nhất định, ví dụ, hình ảnh đầu tiên trong Hình 5 và hàng ViT-S/8 trong Bảng 1. So với ViT, crate cung cấp các token biểu diễn nội bộ tốt hơn cho cả phân đoạn và phát hiện.

## 4. Phân tích Được Hỗ trợ bởi White-Box về Phân đoạn trong crate

Trong phần này, chúng tôi đi sâu vào các thuộc tính phân đoạn của crate sử dụng phân tích được hỗ trợ bởi quan điểm white-box của chúng tôi. Để bắt đầu, chúng tôi phân tích các biểu diễn token nội bộ từ các lớp khác nhau của crate và nghiên cứu sức mạnh của phân đoạn mạng như một hàm của độ sâu lớp. Sau đó chúng tôi thực hiện một nghiên cứu ablation về các cấu hình kiến trúc khác nhau của crate để cô lập các thành phần thiết yếu để phát triển các thuộc tính phân đoạn. Cuối cùng, chúng tôi điều tra cách xác định "ý nghĩa" ngữ nghĩa của các không gian con nhất định và trích xuất thông tin chi tiết hơn từ crate.

Chúng tôi sử dụng crate-B/8 và ViT-B/8 làm các mô hình mặc định để đánh giá trong phần này.

Vai trò của độ sâu trong crate. Mỗi lớp của crate được thiết kế cho cùng một mục đích khái niệm: tối ưu hóa sparse rate reduction và chuyển đổi phân phối token thành các dạng compact và có cấu trúc (Phần 2). Cho rằng sự xuất hiện của phân đoạn ngữ nghĩa trong crate tương tự như việc clustering các token thuộc về các danh mục ngữ nghĩa tương tự trong biểu diễn Z, do đó chúng tôi mong đợi hiệu suất phân đoạn của crate được cải thiện với độ sâu tăng. Để kiểm tra điều này, chúng tôi sử dụng pipeline MaskCut (được mô tả trong Phần 3.2) để đánh giá định lượng hiệu suất phân đoạn của các biểu diễn nội bộ trên các lớp khác nhau. Trong khi đó, chúng tôi áp dụng trực quan hóa PCA (được mô tả trong Phần 3.1) để hiểu cách phân đoạn xuất hiện đối với độ sâu. So với kết quả trong Hình 3, một khác biệt nhỏ trong trực quan hóa của chúng tôi là chúng tôi hiển thị bốn thành phần chính đầu tiên trong Hình 6 và không lọc ra các token background.

--- TRANG 8 ---

| 7 | 8 | 9 | 10 | 11 | 12 |
|---|---|---|----|----|----| 
Layer index 0.0 0.2 0.4 0.6 0.8 1.0 AP Score on Segmentation
CRATE-B/8
ViT-B/8

Shallow Deep layer 1 layer 4 layer 6 layer 8 layer 11

Hình 6: Ảnh hưởng của độ sâu đối với phân đoạn trong crate có giám sát. (Trái) Hiệu suất phân đoạn theo từng lớp của crate và ViT thông qua pipeline MaskCut trên COCO val2017 (Điểm AP cao hơn có nghĩa là hiệu suất phân đoạn tốt hơn). (Phải) Chúng tôi tuân theo implementation trong Amir et al. [2]: đầu tiên chúng tôi áp dụng PCA trên các đặc trưng patch-wise. Sau đó, đối với hình xám, chúng tôi trực quan hóa thành phần thứ 1, và đối với hình màu, chúng tôi trực quan hóa thành phần thứ 2, 3 và 4, tương ứng với các kênh màu RGB. Xem thêm kết quả trong Hình 9.

| COCO Detection | | | VOC Seg. |
|----------------|--|--|---------|
| Model | Attention | Nonlinearity | AP 50 | AP75 | AP | mIoU |
| crate | MSSA | ISTA | 2.1 | 0.7 | 0.8 | 23.9 |
| crate-MLP | MSSA | MLP | 0.2 | 0.2 | 0.2 | 22.0 |
| crate-MHSA | MHSA | ISTA | 0.1 | 0.1 | 0.0 | 18.4 |
| ViT | MHSA | MLP | 0.1 | 0.1 | 0.0 | 14.1 |

Bảng 2: Nghiên cứu ablation của các biến thể crate khác nhau. Chúng tôi sử dụng cấu hình mô hình Small-Patch8 (S-8) trên tất cả các thí nghiệm trong bảng này.

Kết quả được tóm tắt trong Hình 6. Chúng tôi quan sát rằng điểm phân đoạn được cải thiện khi sử dụng các biểu diễn từ các lớp sâu hơn, phù hợp tốt với thiết kế tối ưu hóa từng bước của crate. Ngược lại, mặc dù hiệu suất của ViT-B/8 cải thiện nhẹ ở các lớp sau, điểm phân đoạn của nó thấp hơn đáng kể so với crate (so sánh các thất bại trong Hình 5, hàng dưới). Kết quả PCA được trình bày trong Hình 6 (Phải). Chúng tôi quan sát rằng các biểu diễn được trích xuất từ các lớp sâu hơn của crate ngày càng tập trung vào đối tượng foreground và có thể nắm bắt các chi tiết cấp độ texture. Hình 9 trong Phụ lục có thêm kết quả trực quan hóa PCA.

Nghiên cứu ablation về kiến trúc trong crate. Cả attention block (MSSA) và MLP block (ISTA) trong crate đều khác với những thành phần trong ViT. Để hiểu ảnh hưởng của mỗi thành phần đối với các thuộc tính phân đoạn xuất hiện của crate, chúng tôi nghiên cứu ba biến thể khác nhau của crate: crate, crate-MHSA, crate-MLP, trong đó chúng tôi ký hiệu attention block và MLP block trong ViT lần lượt là MHSA và MLP. Chúng tôi tóm tắt các kiến trúc mô hình khác nhau trong Bảng 2.

Đối với tất cả các mô hình trong Bảng 2, chúng tôi áp dụng cùng thiết lập pre-training trên tập dữ liệu ImageNet-21k. Sau đó chúng tôi áp dụng đánh giá phân đoạn thô (Phần 3.2) và đánh giá MaskCut (Phần 3.2) để so sánh định lượng hiệu suất của các mô hình khác nhau. Như được hiển thị trong Bảng 2, crate vượt trội đáng kể so với các kiến trúc mô hình khác trên tất cả các tác vụ. Thú vị là, chúng tôi thấy rằng hiệu suất phân đoạn thô (tức là, VOC Seg) của ViT có thể được cải thiện đáng kể bằng cách đơn giản thay thế MHSA trong ViT bằng MSSA trong crate, bất chấp sự khác biệt kiến trúc giữa MHSA và MSSA là nhỏ. Điều này chứng minh hiệu quả của thiết kế white-box.

Xác định các thuộc tính ngữ nghĩa của attention heads. Như được hiển thị trong Hình 1, bản đồ tự chú ý giữa token [CLS] và các token patch chứa các mặt nạ phân đoạn rõ ràng. Chúng tôi quan tâm đến việc nắm bắt ý nghĩa ngữ nghĩa của các attention heads nhất định; đây là một tác vụ quan trọng cho khả năng diễn giải, và đã được nghiên cứu cho language transformers [34]. Một cách trực quan, mỗi head nắm bắt những đặc trưng nhất định của dữ liệu. Cho một mô hình crate, đầu tiên chúng tôi forward một hình ảnh đầu vào (ví dụ một hình ảnh ngựa như trong Hình 7) và chọn bốn attention heads có vẻ như có ý nghĩa ngữ nghĩa bằng kiểm tra thủ công. Sau khi xác định các attention heads, chúng tôi trực quan hóa bản đồ tự chú ý của các heads này trên các hình ảnh đầu vào khác. Chúng tôi trực quan hóa kết quả trong Hình 7. Thú vị là, chúng tôi thấy rằng mỗi attention head được chọn nắm bắt một phần khác nhau của đối tượng, và thậm chí một ý nghĩa ngữ nghĩa khác nhau. Ví dụ, attention head được hiển thị trong cột đầu tiên của Hình 7 nắm bắt chân của các loài động vật khác nhau,

--- TRANG 9 ---

Head 0
"Leg" Head 1
"Body" Head 3
"Face" Head 4
"Ear" Head 0
"Leg" Head 1
"Body" Head 3
"Face" Head 4
"Ear"

Hình 7: Trực quan hóa các semantic heads. Chúng tôi forward một mini-batch hình ảnh qua một crate có giám sát và kiểm tra các bản đồ attention từ tất cả các heads trong lớp gần cuối. Chúng tôi trực quan hóa một lựa chọn các attention heads để cho thấy rằng các heads nhất định truyền đạt ý nghĩa ngữ nghĩa cụ thể, tức là head 0↔"Legs", head 1↔"Body", head 3↔"Face", head 4↔"Ear".

và attention head được hiển thị trong cột cuối nắm bắt tai và đầu. Việc phân tích đầu vào thị giác thành một hierarchy part-whole này đã là một mục tiêu cơ bản của các kiến trúc nhận dạng dựa trên học kể từ deformable part models [14, 15] và capsule networks [20, 40]—đáng chú ý, nó xuất hiện từ thiết kế white-box của crate trong thiết lập huấn luyện có giám sát đơn giản của chúng tôi.2

## 5. Nghiên cứu Liên quan

Attention thị giác và sự xuất hiện của phân đoạn. Khái niệm attention đã trở nên ngày càng quan trọng trong trí tuệ, phát triển từ các mô hình tính toán sớm [21, 23, 41] đến các mạng neural hiện đại [11, 44]. Trong deep learning, cơ chế self-attention đã được sử dụng rộng rãi trong xử lý dữ liệu thị giác [11] với hiệu suất state-of-the-art trên các tác vụ thị giác khác nhau [6, 18, 35]. DINO [6] đã chứng minh rằng các bản đồ attention được tạo ra bởi Vision Transformers tự giám sát (ViT) [11] có thể ngầm thực hiện phân đoạn ngữ nghĩa của hình ảnh. Sự xuất hiện của khả năng phân đoạn này đã được củng cố bởi các nghiên cứu học tự giám sát tiếp theo [6, 18, 35]. Tận dụng các phát hiện này, các phương pháp phân đoạn gần đây [2, 22, 46] đã khai thác các phân đoạn xuất hiện này để đạt được kết quả state-of-the-art. Tuy nhiên, có một sự đồng thuận, như được nổi bật trong các nghiên cứu như Caron et al. [6], gợi ý rằng khả năng phân đoạn như vậy sẽ không biểu hiện trong một ViT có giám sát. Một động lực chính và đóng góp của nghiên cứu của chúng tôi là cho thấy các kiến trúc giống transformer, như trong crate, có thể thể hiện khả năng này ngay cả với huấn luyện có giám sát.

Các mô hình white-box. Trong phân tích dữ liệu, liên tục có sự quan tâm đáng kể trong việc phát triển các biểu diễn có thể diễn giải và có cấu trúc của tập dữ liệu. Những biểu hiện sớm nhất của sự quan tâm như vậy là trong sparse coding thông qua dictionary learning [47], là các mô hình white-box chuyển đổi dữ liệu (gần đúng tuyến tính) thành các dạng tiêu chuẩn có thể diễn giải bởi con người (các vector có độ sparse cao). Sự ra đời của deep learning không thay đổi nhiều mong muốn này, và thực tế các nỗ lực đã được thực hiện để kết hợp sức mạnh của deep learning với khả năng diễn giải của các mô hình white-box. Các nỗ lực như vậy bao gồm scattering networks [5], convolutional sparse coding networks [36], và sparse manifold transform [9]. Một hướng nghiên cứu khác xây dựng các mạng sâu từ unrolled optimization [7, 43, 50, 51]. Các mô hình như vậy hoàn toàn có thể diễn giải, nhưng chỉ gần đây chúng mới chứng minh hiệu suất cạnh tranh với các lựa chọn thay thế black-box như ViT ở quy mô ImageNet [51]. Nghiên cứu này xây dựng trên một mô hình white-box mạnh mẽ như vậy, crate [51], và chứng minh thêm các khả năng của nó, đồng thời phục vụ như một ví dụ cho phân tích chi tiết được tạo ra bởi các mô hình white-box.

## 6. Thảo luận và Nghiên cứu Tương lai

Trong nghiên cứu này, chúng tôi đã chứng minh rằng khi sử dụng mô hình white-box crate như một kiến trúc nền tảng thay cho ViT, có sự xuất hiện tự nhiên của các mặt nạ phân đoạn ngay cả khi sử dụng một cách tiếp cận huấn luyện có giám sát đơn giản. Các phát hiện thực nghiệm của chúng tôi nhấn mạnh tầm quan trọng của thiết kế kiến trúc có nguyên tắc để phát triển các mô hình nền tảng thị giác tốt hơn. Vì các mô hình đơn giản hơn có thể diễn giải và dễ phân tích hơn, chúng tôi lạc quan rằng những hiểu biết thu được từ white-box transformers trong nghiên cứu này sẽ đóng góp vào hiểu biết thực nghiệm và lý thuyết sâu sắc hơn về hiện tượng phân đoạn. Hơn nữa, các phát hiện của chúng tôi gợi ý rằng các nguyên tắc thiết kế white-box hứa hẹn trong việc cung cấp các hướng dẫn cụ thể để phát triển các mô hình nền tảng thị giác được cải thiện. Hai hướng nghiên cứu hấp dẫn sẽ là điều tra cách kỹ thuật tốt hơn các mô hình white-box như crate để phù hợp với hiệu suất của các phương pháp học tự giám sát (như DINO), và mở rộng phạm vi các tác vụ mà các mô hình white-box hữu ích trong thực tế.

Lời cảm ơn. Chúng tôi cảm ơn Xudong Wang và Baifeng Shi cho những thảo luận có giá trị về các thuộc tính phân đoạn trong vision transformers.

Tài liệu tham khảo

[1] Samira Abnar và Willem Zuidema. "Quantifying attention flow in transformers". arXiv preprint arXiv:2005.00928 (2020). 6, 16.

[2] Shir Amir, Yossi Gandelsman, Shai Bagon, và Tali Dekel. "Deep ViT Features as Dense Visual Descriptors". ECCVW What is Motion For? (2022). 2, 6, 8, 9, 16, 17.

[3] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. "On the opportunities and risks of foundation models". arXiv preprint arXiv:2108.07258 (2021). 2.

[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. "Language models are few-shot learners". Advances in neural information processing systems 33 (2020), pp. 1877–1901. 2.

[5] Joan Bruna và Stéphane Mallat. "Invariant scattering convolution networks". IEEE transactions on pattern analysis and machine intelligence 35.8 (Aug. 2013), pp. 1872–1886. 10.

[6] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, và Armand Joulin. "Emerging properties in self-supervised vision transformers". Proceedings of the IEEE/CVF international conference on computer vision. 2021, pp. 9650–9660. 2, 5–7, 9, 16, 19.

[7] Kwan Ho Ryan Chan, Yaodong Yu, Chong You, Haozhi Qi, John Wright, và Yi Ma. "ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction". Journal of Machine Learning Research 23.114 (2022), pp. 1–103. 3, 10.

[8] Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, et al. "Symbolic discovery of optimization algorithms". arXiv preprint arXiv:2302.06675 (2023). 18.

--- TRANG 11 ---

[9] Yubei Chen, Dylan Paiton, và Bruno Olshausen. "The sparse manifold transform". Advances in neural information processing systems 31 (2018). 10.

[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, và Li Fei-Fei. "Imagenet: A large-scale hierarchical image database". 2009 IEEE conference on computer vision and pattern recognition. Ieee. 2009, pp. 248–255. 18.

[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. "An image is worth 16x16 words: Transformers for image recognition at scale". arXiv preprint arXiv:2010.11929 (2020). 2, 3, 5, 9, 18.

[12] Michael Elad và Michal Aharon. "Image denoising via sparse and redundant representations over learned dictionaries". IEEE transactions on image processing: a publication of the IEEE Signal Processing Society 15.12 (Dec. 2006), pp. 3736–3745. 1.

[13] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, và A. Zisserman. The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html. 6, 18.

[14] Pedro Felzenszwalb, David McAllester, và Deva Ramanan. "A discriminatively trained, multiscale, deformable part model". 2008 IEEE Conference on Computer Vision and Pattern Recognition. ieeexplore.ieee.org, June 2008, pp. 1–8. 9.

[15] Pedro F Felzenszwalb và Daniel P Huttenlocher. "Pictorial Structures for Object Recognition". International journal of computer vision 61.1 (Jan. 2005), pp. 55–79. 9.

[16] Karol Gregor và Yann LeCun. "Learning fast approximations of sparse coding". Proceedings of the 27th International Conference on International Conference on Machine Learning. Omnipress. 2010, pp. 399–406. 3.

[17] Florentin Guth, John Zarka, và Stéphane Mallat. "Phase Collapse in Neural Networks". International Conference on Learning Representations. 2022. 4.

[18] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, và Ross Girshick. "Masked autoencoders are scalable vision learners". Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022, pp. 16000–16009. 9.

[19] Geoffrey Hinton. "How to represent part-whole hierarchies in a neural network" (Feb. 2021). arXiv: 2102.12627 [cs.CV]. 9.

[20] Geoffrey E Hinton, Alex Krizhevsky, và Sida D Wang. "Transforming Auto-Encoders". Artificial Neural Networks and Machine Learning–ICANN 2011. Springer Berlin Heidelberg, 2011, pp. 44–51. 9.

[21] Laurent Itti và Christof Koch. "Computational modelling of visual attention". Nature reviews neuroscience 2.3 (2001), pp. 194–203. 9.

[22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. "Segment anything". arXiv preprint arXiv:2304.02643 (2023). 2, 9.

[23] Christof Koch và Shimon Ullman. "Shifts in selective visual attention: towards the underlying neural circuitry." Human neurobiology 4.4 (1985), pp. 219–227. 9.

[24] Philipp Krähenbühl và Vladlen Koltun. "Efficient inference in fully connected crfs with gaussian edge potentials". Advances in neural information processing systems 24 (2011). 17.

[25] Alex Krizhevsky, Geoffrey Hinton, et al. "Learning multiple layers of features from tiny images" (2009). 18.

[26] Yann LeCun. "A path towards autonomous machine intelligence version 0.9.2, 2022-06-27". Open Review 62 (2022). 1.

[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, và C Lawrence Zitnick. "Microsoft coco: Common objects in context". Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13. Springer. 2014, pp. 740–755. 7, 18.

[28] Jonathan Long, Evan Shelhamer, và Trevor Darrell. "Fully convolutional networks for semantic segmentation". Proceedings of the IEEE conference on computer vision and pattern recognition. 2015, pp. 3431–3440. 6, 17.

[29] Ilya Loshchilov và Frank Hutter. "Decoupled weight decay regularization". arXiv preprint arXiv:1711.05101 (2017). 18.

--- TRANG 12 ---

[30] Yi Ma, Harm Derksen, Wei Hong, và John Wright. "Segmentation of multivariate mixed data via lossy data coding and compression". PAMI (2007). 3.

[31] Yi Ma, Doris Tsao, và Heung-Yeung Shum. "On the principles of parsimony and self-consistency for the emergence of intelligence". Frontiers of Information Technology & Electronic Engineering 23.9 (2022), pp. 1298–1323. 1.

[32] Vishal Monga, Yuelong Li, và Yonina C Eldar. "Algorithm Unrolling: Interpretable, Efficient Deep Learning for Signal and Image Processing". IEEE Signal Processing Magazine 38.2 (Mar. 2021), pp. 18–44. 3.

[33] Maria-Elena Nilsback và Andrew Zisserman. "Automated flower classification over a large number of classes". 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing. IEEE. 2008, pp. 722–729. 18.

[34] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, và Chris Olah. "In-context Learning and Induction Heads". Transformer Circuits Thread (2022). https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html. 8.

[35] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. "Dinov2: Learning robust visual features without supervision". arXiv preprint arXiv:2304.07193 (2023). 2, 6, 9, 16.

[36] Vardan Papyan, Yaniv Romano, Jeremias Sulam, và Michael Elad. "Theoretical Foundations of Deep Learning via Sparse Representations: A Multilayer Sparse Model and Its Connection to Convolutional Neural Networks". IEEE Signal Processing Magazine 35.4 (July 2018), pp. 72–89. 10.

[37] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, và CV Jawahar. "Cats and dogs". 2012 IEEE conference on computer vision and pattern recognition. IEEE. 2012, pp. 3498–3505. 18.

[38] Ignacio Ramirez, Pablo Sprechmann, và Guillermo Sapiro. "Classification and clustering via dictionary learning with structured incoherence and shared features". 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE. 2010, pp. 3501–3508. 1.

[39] Shankar Rao, Roberto Tron, René Vidal, và Yi Ma. "Motion segmentation in the presence of outlying, incomplete, or corrupted trajectories". IEEE transactions on pattern analysis and machine intelligence 32.10 (Oct. 2010), pp. 1832–1845. 1.

[40] Sara Sabour, Nicholas Frosst, và Geoffrey E Hinton. "Dynamic Routing Between Capsules". Advances in Neural Information Processing Systems. Ed. by I Guyon, U V Von Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, và R Garnett. Vol. 30. Curran Associates, Inc., 2017. 9.

[41] Brian J Scholl. "Objects and attention: The state of the art". Cognition 80.1-2 (2001), pp. 1–46. 9.

[42] Jianbo Shi và Jitendra Malik. "Normalized cuts and image segmentation". IEEE Transactions on pattern analysis and machine intelligence 22.8 (2000), pp. 888–905. 17.

[43] Bahareh Tolooshams và Demba Ba. "Stable and Interpretable Unrolled Dictionary Learning". arXiv preprint arXiv:2106.00058 (2021). 10.

[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. "Attention is all you need". Advances in neural information processing systems 30 (2017). 9.

[45] Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li, Bharath Hariharan, Aleksander Holynski, và Noah Snavely. "Tracking Everything Everywhere All at Once". arXiv:2306.05422 (2023). 2.

[46] Xudong Wang, Rohit Girdhar, Stella X Yu, và Ishan Misra. "Cut and learn for unsupervised object detection and instance segmentation". Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 3124–3134. 2, 6, 9, 17, 18.

[47] John Wright và Yi Ma. High-Dimensional Data Analysis with Low-Dimensional Models: Principles, Computation, and Applications. Cambridge University Press, 2022. 4, 10.

[48] Allen Y Yang, John Wright, Yi Ma, và S Shankar Sastry. "Unsupervised segmentation of natural images via lossy data compression". Computer Vision and Image Understanding 110.2 (2008), pp. 212–225. 1.

--- TRANG 13 ---

[49] Jianchao Yang, John Wright, Thomas S Huang, và Yi Ma. "Image super-resolution via sparse representation". IEEE transactions on image processing: a publication of the IEEE Signal Processing Society 19.11 (Nov. 2010), pp. 2861–2873. 1.

[50] Yongyi Yang, David P Wipf, et al. "Transformers from an optimization perspective". Advances in Neural Information Processing Systems 35 (2022), pp. 36958–36971. 10.

[51] Yaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, Shengbang Tong, Benjamin D. Haeffele, và Yi Ma. White-Box Transformers via Sparse Rate Reduction. 2023. arXiv: 2306.01129 [cs.LG]. 2–5, 10, 14, 18.

[52] Yaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, và Yi Ma. "Learning Diverse and Discriminative Representations via the Principle of Maximal Coding Rate Reduction". Advances in Neural Information Processing Systems 33 (2020), pp. 9422–9434. 3.

--- TRANG 14 ---

## Phụ lục

### A. Implementation của crate

Trong phần này, chúng tôi cung cấp chi tiết về implementation của crate, cả ở mức độ cao hơn để sử dụng trong phân tích toán học, và ở mức độ dựa trên code để sử dụng trong các implementation tham khảo. Trong khi chúng tôi sử dụng cùng implementation như trong Yu et al. [51], chúng tôi cung cấp chi tiết ở đây để hoàn thiện.

#### A.1. Thuật toán Forward-Pass

Chúng tôi cung cấp chi tiết về forward pass của crate trong Thuật toán 1.

**Thuật toán 1** CRATE Forward Pass.
**Hyperparameter:** Số lượng lớp L, chiều đặc trưng d, chiều không gian con p, chiều hình ảnh (C, H, W), chiều patch (PH, PW), regularizer sparsification λ > 0, lỗi quantization ε, tốc độ học η >0.
**Parameter:** Ma trận hình chiếu patch W∈Rd×D. ▷ D.=PHPW.
**Parameter:** Class token z0[CLS]∈Rd.
**Parameter:** Mã hóa vị trí Epos∈Rd×(N+1). ▷ N.=H/PH·W/PW.
**Parameter:** Các mô hình tín hiệu cục bộ (Uℓ[K])Lℓ=1 trong đó mỗi Uℓ[K]= (Uℓ1, . . . ,UℓK) và mỗi Uℓk∈Rd×p.
**Parameter:** Từ điển sparsifying (Dℓ)Lℓ=1 trong đó mỗi Dℓ∈Rd×d.
**Parameter:** Các tham số LayerNorm khác.

1: **function** MSSA(Z∈Rd×(N+1)|U[K]∈RK×d×p)
2:    **return** √(N+ 1)ε2∑Kk=1Uk(U∗kZ) softmax(( U∗kZ)∗(U∗kZ)) ▷Eq. (5)
3: **end function**
4: **function** ISTA(Z∈Rd×(N+1)|D×Rd×d)
5:    **return** ReLU( Z+ηD∗(Z−DZ)−ηλ1) ▷Eq. (7)
6: **end function**
7: **function** CRATEForwardPass (IMG∈RC×H×W)
8:    X.= [x1, . . . ,xN]←Patchify (IMG) ▷X∈RD×N và mỗi xi∈RD.
9:    #f0 Operator
10:   [z11, . . . ,z1N]←WX ▷z1i∈Rd.
11:   Z1←[z1[CLS],z11, . . . ,z1N]+Epos ▷Z1∈Rd×(N+1).
12:   #fℓ Operators
13:   **for** ℓ∈ {1, . . . , L } **do**
14:      Zℓn←LayerNorm (Zℓ) ▷Zℓn∈Rd×(N+1)
15:      Zℓ+1/2←Zℓn+MSSA(Zℓn|Uℓ[K]) ▷Zℓ+1/2∈Rd×(N+1)
16:      Zℓ+1/2n←LayerNorm (Zℓ+1/2) ▷Zℓ+1/2n∈Rd×(N+1)
17:      Zℓ+1←ISTA(Zℓ+1/2n|Dℓ) ▷Zℓ+1∈Rd×(N+1)
18:   **end for**
19:   **return** Z←ZL+1
20: **end function**

--- TRANG 15 ---

#### A.2. Code giống PyTorch cho Forward Pass

Tương tự như phần trước, chúng tôi cung cấp pseudocode cho MSSA block và ISTA block trong Thuật toán 2, và sau đó trình bày pseudocode cho forward pass của crate trong Thuật toán 3.

**Thuật toán 2** Code giống PyTorch cho MSSA và ISTA Forward Passes

```python
1 class ISTA:
2     # khởi tạo
3     def __init__(self, dim, hidden_dim, dropout = 0., step_size=0.1, lambd=0.1):
4         self.weight = Parameter(Tensor(dim, dim))
5         init.kaiming_uniform_(self.weight)
6         self.step_size = step_size
7         self.lambd = lambd
8     # forward pass
9     def forward(self, x):
10        x1 = linear(x, self.weight, bias=None)
11        grad_1 = linear(x1, self.weight.t(), bias=None)
12        grad_2 = linear(x, self.weight.t(), bias=None)
13        grad_update = self.step_size * (grad_2 - grad_1) - self.step_size * self.lambd
14        output = relu(x + grad_update)
15        return output
16 class MSSA:
17     # khởi tạo
18     def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):
19         inner_dim = dim_head * heads
20         project_out = not (heads == 1 and dim_head == dim)
21         self.heads = heads
22         self.scale = dim_head ** -0.5
23         self.attend = Softmax(dim = -1)
24         self.dropout = Dropout(dropout)
25         self.qkv = Linear(dim, inner_dim, bias=False)
26         self.to_out = Sequential(Linear(inner_dim, dim), Dropout(dropout)) if project_out else nn.Identity()
27     # forward pass
28     def forward(self, x):
29         w = rearrange(self.qkv(x), 'b n (h d) -> b h n d', h = self.heads)
30         dots = matmul(w, w.transpose(-1, -2)) * self.scale
31         attn = self.attend(dots)
32         attn = self.dropout(attn)
33         out = matmul(attn, w)
34         out = rearrange(out, 'b h n d -> b n (h d)')
35         return self.to_out(out)
```

**Thuật toán 3** Code giống PyTorch cho CRATE Forward Pass

```python
1 class CRATE:
2     # khởi tạo
3     def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):
4         # định nghĩa các lớp
5         self.layers = []
6         self.depth = depth
7         for _ in range(depth):
8             self.layers.extend([LayerNorm(dim), MSSA(dim, heads, dim_head, dropout)])
9             self.layers.extend([LayerNorm(dim), ISTA(dim, mlp_dim, dropout)])
10    # forward pass
11    def forward(self, x):
12        for ln1, attn, ln2, ff in self.layers:
13            x_ = attn(ln1(x)) + ln1(x)
14            x = ff(ln2(x_))
15        return x
```

--- TRANG 16 ---

## B. Phương pháp Thực nghiệm Chi tiết

Trong phần này chúng tôi mô tả chính thức từng phương pháp được sử dụng để đánh giá thuộc tính phân đoạn của crate trong Phần 3 và Phần 4, đặc biệt so với DINO và ViT có giám sát. Phần này lặp lại các phương pháp thực nghiệm được đề cập ít chính thức hơn trong các nghiên cứu khác; chúng tôi cố gắng định nghĩa nghiêm ngặt các phương pháp thực nghiệm trong phần này.

### B.1. Trực quan hóa Bản đồ Attention

Chúng tôi tóm tắt lại phương pháp trực quan hóa bản đồ attention trong Abnar và Zuidema [1] và Caron et al. [6], đầu tiên chuyên biệt hóa việc sử dụng chúng cho các thể hiện của mô hình crate trước khi tổng quát hóa cho ViT.

Đối với head thứ k tại lớp thứ ℓ của crate, chúng tôi tính ma trận tự chú ý Aℓk∈RN được định nghĩa như sau:

Aℓk = [Aℓk,1 ... Aℓk,N] ∈RN, trong đó Aℓk,i = exp(⟨Uℓ∗kzℓi,Uℓ∗kzℓ[CLS]⟩) / ∑Nj=1exp(⟨Uℓ∗kzℓj,Uℓ∗kzℓ[CLS]⟩). (10)

Sau đó chúng tôi reshape ma trận attention Aℓk thành ma trận √N×√N và trực quan hóa heatmap như được hiển thị trong Hình 1. Ví dụ, phần tử hàng i và cột j của heatmap trong Hình 1 tương ứng với thành phần thứ m của Aℓk nếu m= (i−1)·√N+j. Trong Hình 1, chúng tôi chọn một attention head của crate và trực quan hóa ma trận attention Aℓk cho mỗi hình ảnh.

Đối với ViT, toàn bộ phương pháp vẫn giống nhau, ngoại trừ bản đồ attention được định nghĩa theo cách hợp lý sau:

Aℓk = [Aℓk,1 ... Aℓk,N] ∈RN, trong đó Aℓk,i = exp(⟨Kℓ∗kzℓi,Qℓ∗kzℓ[CLS]⟩) / ∑Nj=1exp(⟨Kℓ∗kzℓj,Qℓ∗kzℓ[CLS]⟩). (11)

trong đó các tham số "query" và "key" của transformer tiêu chuẩn tại head k và lớp ℓ được ký hiệu lần lượt là Kℓk và Qℓk.

### B.2. Trực quan hóa PCA

Như trong phần trước, chúng tôi tóm tắt lại phương pháp trực quan hóa các biểu diễn patch sử dụng PCA từ Amir et al. [2] và Oquab et al. [35]. Như trước đây, chúng tôi chuyên biệt hóa việc sử dụng chúng cho các thể hiện của mô hình crate trước khi tổng quát hóa cho ViT.

Đầu tiên chúng tôi chọn J hình ảnh thuộc cùng một lớp, {Xj}Jj=1, và trích xuất các biểu diễn token cho mỗi hình ảnh tại lớp ℓ, tức là, [zℓj,[CLS],zℓj,1, . . . ,zℓj,N] với j∈[J]. Đặc biệt, zℓj,i đại diện cho biểu diễn token thứ i tại lớp thứ ℓ cho hình ảnh thứ j. Sau đó chúng tôi tính các thành phần PCA đầu tiên của Z̃ℓ={z̃ℓ1,1, . . . ,z̃ℓ1,N, . . . ,z̃ℓJ,1, . . . ,z̃ℓJ,N}, và sử dụng z̃ℓj,i để ký hiệu biểu diễn token tổng hợp cho token thứ i của Xj, tức là, z̃ℓj,i= [(U∗1z̃ℓj,i)⊤, . . . , (U∗Kz̃ℓj,i)⊤]⊤∈R(p·K)×1. Chúng tôi ký hiệu eigenvector đầu tiên của ma trận Z̃∗Z̃ bằng u0 và tính các giá trị hình chiếu là {σλ(⟨u0,zℓj,i⟩)}i,j, trong đó σλ(x) = x nếu |x| ≥λ, 0 nếu |x|< λ là hàm hard-thresholding. Sau đó chúng tôi chọn một tập con các biểu diễn token từ Z̃ với σλ(⟨u0,zℓj,i⟩)>0. tương ứng với các giá trị hình chiếu khác không sau thresholding, và chúng tôi ký hiệu tập con này là Z̃s⊆Z̃. Bước chọn lọc này được sử dụng để loại bỏ background [35]. Sau đó chúng tôi tính ba thành phần PCA đầu tiên của Z̃s với ba eigenvector đầu tiên của ma trận Z̃∗sZ̃s được ký hiệu là {u1,u2,u3}. Chúng tôi định nghĩa bộ ba RGB cho mỗi token là:

[rj,i, gj,i, bj,i] = [⟨u1,zℓj,i⟩,⟨u2,zℓj,i⟩,⟨u3,zℓj,i⟩], i∈[N], j∈[J],zℓj,i∈Z̃s. (12)

Tiếp theo, đối với mỗi hình ảnh Xj chúng tôi tính Rj,Gj,Bj, trong đó Rj= [rj,1, . . . , rj,N]⊤∈Rd×1 (tương tự cho Gj và Bj). Sau đó chúng tôi reshape ba ma trận thành √N×√N và trực quan hóa "thành phần PCA" của hình ảnh Xj thông qua hình ảnh RGB (Rj,Gj,Bj)∈R3×√N×√N.

--- TRANG 17 ---

Trực quan hóa PCA của ViTs được đánh giá tương tự, ngoại trừ việc sử dụng các đặc trưng "Key" z̃ℓj,i= [(K∗1z̃ℓj,i)⊤, . . . , (K∗Kz̃ℓj,i)⊤]⊤. Nghiên cứu trước [2] chứng minh rằng các đặc trưng "Key" dẫn đến các cấu trúc không gian ít nhiễu hơn so với các đặc trưng "Query". Trong các thí nghiệm (như trong Hình 3), chúng tôi đặt giá trị ngưỡng λ=1/2.

### B.3. Bản đồ Phân đoạn và Điểm mIoU

Bây giờ chúng tôi thảo luận các phương pháp được sử dụng để tính các bản đồ phân đoạn và điểm mean-Union-over-Intersection (mIoU) tương ứng.

Thực tế, giả sử chúng ta đã tính các bản đồ attention Aℓk∈RN cho một hình ảnh cho trước như trong Phụ lục B.1. Sau đó chúng tôi ngưỡng hóa mỗi bản đồ attention bằng cách đặt P= 60% các entry hàng đầu thành 1 và đặt phần còn lại thành 0. Ma trận còn lại, giả sử Ãℓk∈ {0,1}N, tạo thành một bản đồ phân đoạn tương ứng với head thứ k trong lớp thứ ℓ cho hình ảnh.

Giả sử các token có thể được phân chia thành M lớp ngữ nghĩa, và lớp ngữ nghĩa thứ m có một bản đồ phân đoạn ground truth boolean Sm∈ {0,1}N. Chúng tôi muốn tính chất lượng của bản đồ phân đoạn được tạo ra bởi attention ở trên, đối với các bản đồ ground truth. Để làm điều này, chúng tôi sử dụng metric mean-intersection-over-union (mIoU) [28] như được mô tả tiếp theo. Kết quả thực nghiệm cho thấy các heads tại một lớp cho trước tương ứng với các đặc trưng ngữ nghĩa khác nhau. Do đó, đối với mỗi lớp ngữ nghĩa m và lớp ℓ, chúng tôi cố gắng tìm head khớp tốt nhất tại lớp ℓ và sử dụng điều này để tính intersection-over-union, thu được

mIoUℓm.= maxk∈[K] ∥Sm⊙Aℓk∥0 / (∥Sm∥0+∥Aℓk∥0− ∥Sm⊙Aℓk∥0), (13)

trong đó ⊙ ký hiệu phép nhân từng phần tử và ∥·∥0 đếm số lượng phần tử khác không trong vector đầu vào (và vì các vector đầu vào là boolean, điều này tương đương với việc đếm số lượng số 1). Để báo cáo điểm mIoU tổng thể cho lớp ℓ (hoặc không có tham chiếu, cho các biểu diễn lớp cuối), chúng tôi tính lượng

mIoUℓ.=1/M ∑Mm=1mIoUℓm, (14)

và tính trung bình trong tất cả các hình ảnh mà chúng ta biết ground truth.

### B.4. MaskCut

Chúng tôi áp dụng pipeline MaskCut (Thuật toán 4) để tạo ra các mặt nạ phân đoạn và bounding box phát hiện (được thảo luận trong Phần 3.2). Như được mô tả bởi Wang et al. [46], chúng tôi áp dụng lặp đi lặp lại Normalized Cuts [42] trên ma trận affinity patch-wise Mℓ, trong đó Mℓij=∑Kk=1⟨Uℓ∗kzℓi,Uℓ∗kzℓj⟩. Tại mỗi bước lặp, chúng tôi mask out các entry patch-wise được xác định trên Mℓ. Để có được các mặt nạ phân đoạn chi tiết hơn, MaskCut sử dụng Conditional Random Fields (CRF) [24] để hậu xử lý các mặt nạ, làm mịn các cạnh và lọc ra các mặt nạ không hợp lý. Tương ứng, bounding box phát hiện được định nghĩa bởi vùng hình chữ nhật bao quanh chặt chẽ một mặt nạ phân đoạn.

**Thuật toán 4** MaskCut
**Hyperparameter:** n, số lượng đối tượng cần phân đoạn.
1: **function** MaskCut (M)
2:    **for** i∈ {1, . . . , n } **do**
3:       mask←NCut (M) ▷mask là một mảng boolean
4:       M←M⊙mask ▷Tương đương với việc áp dụng mask lên M
5:       masks [i]←mask
6:    **end for**
7:    **return** masks
8: **end function**

Theo implementation chính thức của Wang et al. [46], chúng tôi chọn các tham số là n= 3, τ= 0.15, trong đó n ký hiệu số lượng đối tượng mong đợi và τ ký hiệu giá trị ngưỡng cho ma trận affinity Mℓ, tức là các entry nhỏ hơn 0.15 sẽ được đặt thành 0. Trong Bảng 1, chúng tôi loại bỏ bước hậu xử lý CRF trong MaskCut khi so sánh các biến thể mô hình khác nhau.

--- TRANG 18 ---

## C. Thiết lập Thực nghiệm và Kết quả Bổ sung

Trong phần này, chúng tôi cung cấp các thiết lập thực nghiệm cho các thí nghiệm được trình bày trong Phần 3 và Phần 4, cũng như các kết quả thực nghiệm bổ sung. Cụ thể, chúng tôi cung cấp thiết lập thực nghiệm chi tiết cho huấn luyện và đánh giá trên Phụ lục C.1. Sau đó chúng tôi trình bày các kết quả thực nghiệm bổ sung về hiệu suất transfer learning của crate khi được pre-train trên ImageNet-21k [10] trong Phụ lục C.2. Trong Phụ lục C.3, chúng tôi cung cấp các trực quan hóa bổ sung về sự xuất hiện của các mặt nạ phân đoạn trong crate.

### C.1. Thiết lập

**Thiết lập mô hình** Chúng tôi sử dụng mô hình crate như được mô tả bởi Yu et al. [51] ở các quy mô -S/8 và -B/8. Theo cách tương tự, chúng tôi áp dụng mô hình ViT từ Dosovitskiy et al. [11] sử dụng cùng các quy mô (-S/8 và -B/8), đảm bảo cấu hình nhất quán giữa chúng. Có thể xem chi tiết của crate transformer trong Phụ lục A.

**Thiết lập huấn luyện** Tất cả các mô hình thị giác được huấn luyện cho các tác vụ phân loại (xem Phần 2.2) trên tập dữ liệu ImageNet hoàn chỉnh [10], thường được gọi là ImageNet-21k. Tập dữ liệu này bao gồm 14,197,122 hình ảnh được phân bố trên 21,841 lớp. Để huấn luyện, mỗi hình ảnh RGB được resize về kích thước 3×224×224, chuẩn hóa sử dụng mean (0.485,0.456,0.406) và độ lệch chuẩn (0.229,0.224,0.225), và sau đó được center cropping và random flipping. Chúng tôi đặt kích thước mini-batch là 4,096 và áp dụng optimizer Lion [8] với learning rate 9.6×10−5 và weight decay 0.05. Tất cả các mô hình, bao gồm crates và ViTs đều được pre-train với 90 epochs trên ImageNet-21K.

**Thiết lập đánh giá** Chúng tôi đánh giá phân đoạn thô, như được chi tiết trong Phần 3.2, sử dụng các bản đồ attention trên tập validation PASCAL VOC 2012 [13] bao gồm 1,449 hình ảnh RGB. Ngoài ra, chúng tôi implement pipeline MaskCut [46], như được mô tả trong Phần 3.2, trên COCO val2017 [27], bao gồm 5,000 hình ảnh RGB, và đánh giá hiệu suất của các mô hình cho cả tác vụ phát hiện đối tượng và phân đoạn instance. Tất cả các quy trình đánh giá đều không giám sát, và chúng tôi không cập nhật trọng số mô hình trong quá trình này.

### C.2. Đánh giá Transfer Learning

Chúng tôi đánh giá hiệu suất transfer learning của crate bằng cách fine-tune các mô hình được pre-train trên ImageNet-21k cho các tác vụ phân loại thị giác downstream sau: ImageNet-1k [10], CIFAR10/CIFAR100 [25], Oxford Flowers-102 [33], Oxford-IIIT-Pets [37]. Chúng tôi cũng finetune trên hai mô hình ViT được pre-train (-T/8 và -B/8) để tham khảo. Cụ thể, chúng tôi sử dụng optimizer AdamW [29] và cấu hình learning rate là 5×10−5, weight decay là 0.01. Do hạn chế bộ nhớ, chúng tôi đặt batch size là 128 cho tất cả các thí nghiệm được thực hiện cho các mô hình base và đặt là 256 cho các mô hình nhỏ hơn khác. Chúng tôi báo cáo kết quả trong Bảng 3.

| Datasets | crate-T | crate-S | crate-B | ViT-T | ViT-B |
|----------|---------|---------|---------|-------|-------|
| # parameters | 5.74M | 14.12M | 38.83M | 10.36M | 102.61M |
| ImageNet-1K | 62.7 | 74.2 | 79.5 | 71.8 | 85.8 |
| CIFAR10 | 94.1 | 97.2 | 98.1 | 97.2 | 98.9 |
| CIFAR100 | 76.7 | 84.1 | 87.9 | 84.4 | 90.1 |
| Oxford Flowers-102 | 82.2 | 92.2 | 96.7 | 92.1 | 99.5 |
| Oxford-IIIT-Pets | 77.0 | 86.4 | 90.7 | 86.2 | 91.8 |

Bảng 3: Top1 accuracy của crate trên các tập dữ liệu khác nhau với các quy mô mô hình khác nhau khi được pre-train trên ImageNet-21K và fine-tune trên tập dữ liệu cho trước.

--- TRANG 19 ---

### C.3. Trực quan hóa Bổ sung

Supervised CRATE
DINO

Hình 8: Trực quan hóa bổ sung của bản đồ attention của crate-S/8 và so sánh với DINO [6]. 2 hàng trên: trực quan hóa các bản đồ attention từ crate-S/8 có giám sát. 2 hàng dưới: trực quan hóa các bản đồ attention được mượn từ bài báo của DINO. Hình cho thấy rằng crate có giám sát có các bản đồ attention ít nhất là tương đương với DINO. Phương pháp chính xác được thảo luận trong Phụ lục B.1.

Supervised CRATE
Supervised ViT

Shallow Deep

Hình 9: Trực quan hóa PCA theo từng lớp bổ sung. 2 hàng trên: trực quan hóa PCA của các đặc trưng từ crate-B/8 có giám sát. 2 hàng dưới: trực quan hóa PCA của các đặc trưng từ ViT-B/8 có giám sát. Hình cho thấy rằng crate có giám sát hiển thị cấu trúc không gian đặc trưng tốt hơn với một đối tượng foreground được phân đoạn rõ ràng và thông tin background ít nhiễu hơn. Hình ảnh đầu vào được hiển thị ở góc trên bên trái của Hình 1. Phương pháp chính xác được thảo luận trong Phụ lục B.2.

--- TRANG 20 ---

| 2 | 3 | 5 | 9 | 20 | 50 | 90 |
|---|---|---|---|----|----|----| 
Epochs 0.0 0.2 0.4 0.6 0.8 1.0 AP Score on Detection
CRATE-S/8

Random Converged init epo. 2 epo. 5 epo. 20 epo. 50

Hình 10: Ảnh hưởng của epochs huấn luyện trong crate có giám sát. (Trái) Hiệu suất phát hiện được tính tại mỗi epoch thông qua pipeline MaskCut trên COCO val2017 (Điểm AP cao hơn có nghĩa là hiệu suất phát hiện tốt hơn). (Phải) Chúng tôi tuân theo implementation trong Amir et al. [2]: đầu tiên chúng tôi áp dụng PCA trên các đặc trưng patch-wise. Sau đó, đối với hình xám, chúng tôi trực quan hóa thành phần thứ 1, và đối với hình màu, chúng tôi trực quan hóa thành phần thứ 2, 3 và 4, tương ứng với các kênh màu RGB. Khi epochs huấn luyện tăng, các đối tượng foreground có thể được phân đoạn rõ ràng và tách thành các phần khác nhau với ý nghĩa ngữ nghĩa.

50%, N(0, 10)
50%, N(0, 25)
50%, N(0, 50)
50%, N(0, 75)

Hình 11: Thêm nhiễu Gaussian với độ lệch chuẩn khác nhau. Chúng tôi thêm nhiễu Gaussian vào hình ảnh đầu vào trên một tập 50% pixel được chọn ngẫu nhiên, với các độ lệch chuẩn khác nhau, và trực quan hóa tất cả 6 heads trong lớp 10 của crate-S/8. Giá trị của mỗi entry trong mỗi màu của hình ảnh nằm trong khoảng (0,255). 2 cột phải, chứa thông tin cạnh, vẫn không thay đổi với các quy mô nhiễu Gaussian khác nhau. Cột giữa cho thấy thông tin cấp độ texture sẽ bị mất khi đầu vào trở nên nhiều nhiễu hơn.

--- TRANG 21 ---

10%, N(0, 75)
25%, N(0, 75)
50%, N(0, 75)
75%, N(0, 75)

Hình 12: Thêm nhiễu Gaussian vào tỷ lệ phần trăm pixel khác nhau. Chúng tôi thêm nhiễu Gaussian với độ lệch chuẩn 75 vào một tập pixel được chọn ngẫu nhiên trong hình ảnh đầu vào, lấy số lượng pixel khác nhau trong mỗi thí nghiệm. Chúng tôi trực quan hóa tất cả 6 heads trong lớp 10 của crate-S/8. Giá trị của mỗi entry trong mỗi kênh của hình ảnh nằm trong khoảng (0,255). Ngoài quan sát trong Hình 11, chúng tôi thấy rằng crate chuyển focus của nó khi tỷ lệ phần trăm pixel nhiễu tăng. Ví dụ, trong cột giữa, head đầu tiên tập trung vào texture của cửa. Sau đó, nó bắt đầu tái tập trung vào các cạnh. Thú vị là cây xuất hiện trong các bản đồ attention của các trường hợp nhiễu hơn.

--- TRANG 22 ---

CRATE
CRATE-sth CRATE-MLP CRATE-MHSA

Hình 13: Bản đồ attention của các biến thể crate trong lớp thứ hai từ cuối. Ngoài kết quả định lượng được thảo luận trong Phần 4, chúng tôi cung cấp kết quả trực quan hóa cho nghiên cứu ablation kiến trúc. crate-MLP và crate-MHSA đã được thảo luận trong Phần 4 trong khi crate-sth duy trì cả MSSA và ISTA blocks, và thay vào đó chuyển hàm activation trong ISTA block từ ReLU sang soft thresholding, phù hợp với một công thức thay thế của ISTA block không áp đặt ràng buộc non-negativity trong LASSO (xem Phần 2.1 để biết thêm chi tiết). Các bản đồ attention với phân đoạn rõ ràng xuất hiện trong tất cả các kiến trúc với MSSA block.

CRATE ViT ViT CRATE

Hình 14: Thêm bản đồ attention của crate và ViT có giám sát trên hình ảnh từ COCO val2017. Chúng tôi chọn các bản đồ attention lớp thứ hai từ cuối cho crate và lớp cuối cho ViT.
