# Giả Thuyết Biểu Diễn Tuyến Tính và
Hình Học của Các Mô Hình Ngôn Ngữ Lớn
Kiho Park1Yo Joong Choe1Victor Veitch1
Tóm tắt
Một cách không chính thức, "giả thuyết biểu diễn tuyến 
tính" là ý tưởng rằng các khái niệm cấp cao được biểu 
diễn tuyến tính như các hướng trong một không gian biểu 
diễn nào đó. Trong bài báo này, chúng tôi giải quyết hai 
câu hỏi có liên quan chặt chẽ: "Biểu diễn tuyến tính" thực 
sự có nghĩa là gì? Và, làm thế nào để chúng ta hiểu được 
các khái niệm hình học (ví dụ: độ tương tự cosine và phép 
chiếu) trong không gian biểu diễn? Để trả lời những câu 
hỏi này, chúng tôi sử dụng ngôn ngữ của các phản thực 
để đưa ra hai cách chính thức hóa biểu diễn tuyến tính, 
một trong không gian biểu diễn đầu ra (từ) và một trong 
không gian đầu vào (ngữ cảnh). Sau đó chúng tôi chứng 
minh rằng những điều này kết nối với linear probing và 
model steering, tương ứng. Để hiểu được các khái niệm 
hình học, chúng tôi sử dụng cách chính thức hóa để xác 
định một tích vô hướng cụ thể (không Euclidean) mà tôn 
trọng cấu trúc ngôn ngữ theo nghĩa mà chúng tôi làm cho 
chính xác. Sử dụng tích vô hướng nhân quả này, chúng 
tôi chỉ ra cách thống nhất tất cả các khái niệm về biểu 
diễn tuyến tính. Đặc biệt, điều này cho phép xây dựng 
các probe và steering vector bằng cách sử dụng các cặp 
phản thực. Các thí nghiệm với LLaMA-2 chứng minh sự 
tồn tại của biểu diễn tuyến tính của các khái niệm, mối 
liên hệ với việc diễn giải và kiểm soát, và vai trò cơ bản 
của việc lựa chọn tích vô hướng. Mã nguồn có sẵn tại 
github.com/KihoPark/linear repgeometry.

1. Giới thiệu
Trong ngữ cảnh của các mô hình ngôn ngữ, "Giả Thuyết Biểu 
Diễn Tuyến Tính" là ý tưởng rằng các khái niệm cấp cao được 
biểu diễn tuyến tính trong không gian biểu diễn của một mô hình 
(ví dụ: Mikolov et al., 2013c; Arora et al., 2016; Elhage et al., 2022).
Các khái niệm cấp cao có thể bao gồm: văn bản có phải bằng tiếng 
Pháp hay tiếng Anh không? Nó có ở thì hiện tại hay quá khứ không? 
Nếu văn bản nói về một người, họ có phải là nam hay nữ không? 
Sức hấp dẫn của giả thuyết biểu diễn tuyến tính là—nếu nó đúng—
các nhiệm vụ diễn giải và kiểm soát hành vi mô hình có thể khai 
thác các phép toán đại số tuyến tính trên không gian biểu diễn. 
Mục tiêu của chúng tôi là chính thức hóa giả thuyết biểu diễn 
tuyến tính và làm rõ nó liên quan như thế nào đến việc diễn giải 
và kiểm soát.

1University of Chicago, Illinois, USA.
Proceedings of the 41stInternational Conference on Machine
Learning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by
the author(s).

Thách thức đầu tiên là không rõ "biểu diễn tuyến tính" có nghĩa 
là gì. Có (ít nhất) ba cách diễn giải:

1.Không gian con: (ví dụ: Mikolov et al., 2013c; Pennington
et al., 2014) Ý tưởng đầu tiên là mỗi khái niệm được biểu 
diễn như một không gian con (1 chiều). Ví dụ, trong ngữ 
cảnh của word embedding, người ta đã lập luận thực nghiệm 
rằng Rep( "woman" )−Rep( "man" ), Rep( "queen" )−Rep( "king" ), 
và tất cả các cặp tương tự thuộc về một không gian con chung 
(Mikolov et al., 2013c). Khi đó, tự nhiên ta lấy không gian 
con này là một biểu diễn của khái niệm Nam/Nữ.

2.Đo lường: (ví dụ: Nanda et al., 2023; Gurnee & Tegmark, 2023) 
Tiếp theo là ý tưởng rằng xác suất của một giá trị khái niệm 
có thể được đo bằng một linear probe. Ví dụ, xác suất rằng 
ngôn ngữ đầu ra là tiếng Pháp là logit-tuyến tính trong biểu 
diễn của đầu vào. Trong trường hợp này, chúng ta có thể lấy 
ánh xạ tuyến tính làm biểu diễn của khái niệm Tiếng Anh/Tiếng Pháp.

3.Can thiệp: (ví dụ: Wang et al., 2023; Turner et al., 2023) 
Ý tưởng cuối cùng là giá trị mà một khái niệm nhận có thể 
được thay đổi, mà không thay đổi các khái niệm khác, bằng 
cách thêm một steering vector phù hợp—ví dụ: chúng ta thay 
đổi đầu ra từ tiếng Anh sang tiếng Pháp bằng cách thêm một 
vector Tiếng Anh/Tiếng Pháp. Trong trường hợp này, chúng 
ta lấy vector được thêm này làm biểu diễn của khái niệm.

Không rõ ràng trước đó là những ý tưởng này liên quan với 
nhau như thế nào, cũng như khái niệm nào là khái niệm "đúng" 
về biểu diễn tuyến tính.

Tiếp theo, giả sử chúng ta đã tìm ra biểu diễn tuyến tính của 
các khái niệm khác nhau bằng cách nào đó. Sau đó chúng ta 
có thể sử dụng các phép toán đại số tuyến tính trên không gian 
biểu diễn để diễn giải và kiểm soát. Ví dụ, chúng ta có thể 
tính độ tương tự cosine giữa một biểu diễn và các hướng khái 
niệm đã biết, hoặc chỉnh sửa biểu diễn được chiếu lên các 
hướng mục tiêu. Tuy nhiên, độ tương tự và phép chiếu đều 
là các khái niệm hình học: chúng yêu cầu một tích vô hướng 
trên không gian biểu diễn. Thách thức thứ hai là không rõ 
tích vô hướng nào là phù hợp để hiểu biểu diễn mô hình ngôn ngữ.

Để giải quyết những vấn đề này, chúng tôi đưa ra các đóng góp sau:

1.Đầu tiên, chúng tôi chính thức hóa khái niệm không gian con 
về biểu diễn tuyến tính theo các cặp phản thực, trong cả không 
gian "embedding" (ngữ cảnh đầu vào) và "unembedding" (từ đầu ra). 
Sử dụng cách chính thức hóa này, chúng tôi chứng minh rằng khái 
niệm unembedding kết nối với đo lường, và khái niệm embedding 
với can thiệp.

2.Tiếp theo, chúng tôi giới thiệu khái niệm tích vô hướng nhân quả: 
một tích vô hướng có tính chất là các khái niệm có thể biến đổi tự 
do với nhau được biểu diễn như các vector trực giao. Chúng tôi chỉ ra 
rằng tích vô hướng như vậy có tính chất đặc biệt là nó thống nhất 
biểu diễn embedding và unembedding, như được minh họa trong Hình 1. 
Ngoài ra, chúng tôi chỉ ra cách ước tính tích vô hướng bằng cách sử 
dụng ma trận unembedding LLM.

3.Cuối cùng, chúng tôi nghiên cứu giả thuyết biểu diễn tuyến tính 
một cách thực nghiệm bằng cách sử dụng LLaMA-2 (Touvron et al., 2023). 
Chúng tôi tìm thấy khái niệm không gian con về biểu diễn tuyến tính 
cho nhiều khái niệm khác nhau. Sử dụng những điều này, chúng tôi 
đưa ra bằng chứng rằng tích vô hướng nhân quả tôn trọng cấu trúc 
ngữ nghĩa, và biểu diễn không gian con có thể được sử dụng để xây 
dựng biểu diễn đo lường và can thiệp.

Kiến thức nền về Mô hình Ngôn ngữ Chúng tôi sẽ cần một số 
kiến thức nền tối thiểu về các mô hình ngôn ngữ (lớn). Chính thức, 
một mô hình ngôn ngữ nhận văn bản ngữ cảnh x và lấy mẫu văn bản 
đầu ra. Việc lấy mẫu này được thực hiện từng từ một (hoặc từng token một). 
Theo đó, chúng tôi sẽ xem các đầu ra như những từ đơn lẻ. Để định 
nghĩa một phân phối xác suất trên các đầu ra, mô hình ngôn ngữ đầu 
tiên ánh xạ mỗi ngữ cảnh x thành một vector λ(x) trong không gian 
biểu diễn Λ≃Rd. Chúng tôi sẽ gọi những vector này là embedding vector. 
Mô hình cũng biểu diễn mỗi từ y như một unembedding vector γ(y) trong 
một không gian biểu diễn riêng biệt Γ≃Rd. Phân phối xác suất trên 
các từ tiếp theo sau đó được cho bởi phân phối softmax:
P(y|x)∝exp(λ(x)⊤γ(y)).

2. Giả Thuyết Biểu Diễn Tuyến Tính

Chúng tôi bắt đầu bằng cách chính thức hóa khái niệm không gian con 
về biểu diễn tuyến tính, một trong mỗi không gian unembedding và 
embedding của các mô hình ngôn ngữ, và sau đó liên kết các khái niệm 
không gian con với các khái niệm đo lường và can thiệp.

2.1. Khái niệm

Bước đầu tiên là chính thức hóa khái niệm về một khái niệm. Trực 
quan, một khái niệm là bất kỳ yếu tố biến đổi nào có thể được thay 
đổi một cách cô lập. Ví dụ, chúng ta có thể thay đổi đầu ra từ tiếng 
Pháp sang tiếng Anh mà không thay đổi ý nghĩa của nó, hoặc thay đổi 
đầu ra từ việc nói về một người đàn ông thành về một phụ nữ mà không 
thay đổi ngôn ngữ mà nó được viết.

Theo Wang et al. (2023), chúng tôi chính thức hóa ý tưởng này bằng 
cách lấy một biến khái niệm W là một biến tiềm ẩn được gây ra bởi 
ngữ cảnh X, và hoạt động như một nguyên nhân của đầu ra Y. Để đơn 
giản trong trình bày, chúng tôi sẽ giới hạn sự chú ý đến các khái 
niệm nhị phân. Dự đoán việc biểu diễn các khái niệm bằng vector, 
chúng tôi giới thiệu một thứ tự trên mỗi khái niệm nhị phân—ví dụ: 
nam⇒nữ. Thứ tự này làm cho dấu của một biểu diễn có ý nghĩa (ví dụ: 
biểu diễn của nữ⇒nam sẽ có dấu ngược lại).

Mỗi biến khái niệm W định nghĩa một tập hợp các đầu ra phản thực 
{Y(W=w)} chỉ khác nhau ở giá trị của W. Ví dụ, đối với khái niệm 
nam⇒nữ, (Y(0), Y(1)) là một phần tử ngẫu nhiên của tập hợp 
{("man", "woman"), ("king", "queen"), . . . }. Trong bài báo này, 
chúng tôi giả định giá trị của các khái niệm có thể được đọc một cách 
xác định từ đầu ra được lấy mẫu (ví dụ: đầu ra "king" ngụ ý W= 0). 
Sau đó, chúng ta có thể chỉ định các khái niệm bằng cách chỉ định 
các đầu ra phản thực tương ứng của chúng.

Cuối cùng chúng tôi sẽ cần lý luận về mối quan hệ giữa nhiều khái 
niệm. Chúng tôi nói rằng hai khái niệm W và Z là tách biệt nhân quả 
nếu Y(W=w, Z=z) được định nghĩa rõ ràng cho mỗi w, z. Nghĩa là, 
các khái niệm tách biệt nhân quả là những khái niệm có thể được biến 
đổi tự do và cô lập. Ví dụ, các khái niệm Tiếng Anh⇒Tiếng Pháp 
và nam⇒nữ là tách biệt nhân quả—xem xét {"king", "queen", "roi", "reine"}. 
Tuy nhiên, các khái niệm Tiếng Anh⇒Tiếng Pháp và Tiếng Anh⇒Tiếng Nga 
thì không phải vì chúng không thể biến đổi tự do.

Chúng tôi sẽ viết Y(W=w, Z=z) là Y(w, z) khi các khái niệm rõ ràng từ ngữ cảnh.

2.2. Biểu diễn Unembedding và Đo lường

Bây giờ chúng tôi chuyển sang chính thức hóa biểu diễn tuyến tính của 
một khái niệm. Quan sát đầu tiên là có hai không gian biểu diễn riêng 
biệt đang được xem xét—không gian embedding Λ và không gian unembedding Γ. 
Một khái niệm có thể được biểu diễn tuyến tính trong một trong hai không 
gian. Chúng tôi bắt đầu với không gian unembedding.

Định nghĩa hình nón của vector v là Cone(v) ={αv:α >0},

Định nghĩa 2.1 (Biểu diễn Unembedding). Chúng tôi nói rằng ¯γW là 
một biểu diễn unembedding của một khái niệm W nếu
γ(Y(1))−γ(Y(0))∈Cone(¯γW) hầu chắc chắn.

Định nghĩa này nắm bắt khái niệm không gian con trong không gian 
unembedding, ví dụ: γ("queen")−γ("king") song song với γ("woman")−γ("man"). 
Chúng tôi sử dụng hình nón thay vì không gian con vì dấu của hiệu là 
có ý nghĩa—tức là hiệu giữa "king" và "queen" theo hướng ngược lại 
với hiệu giữa "woman" và "man".

Biểu diễn unembedding (nếu tồn tại) là duy nhất đến phép nhân với 
số dương, phù hợp với giả thuyết không gian con tuyến tính rằng các 
khái niệm được biểu diễn như các hướng.

Kết nối với Đo lường Kết quả đầu tiên là biểu diễn unembedding liên 
quan chặt chẽ với khái niệm đo lường của biểu diễn tuyến tính:

Định lý 2.2 (Biểu diễn Đo lường). Cho W là một khái niệm, và cho ¯γW 
là biểu diễn unembedding của W. Khi đó, với bất kỳ context embedding 
λ∈Λ nào,
logitP(Y=Y(1)|Y∈ {Y(0), Y(1)}, λ) =αλ⊤¯γW,
trong đó α >0(h.c.c.) là một hàm của {Y(0), Y(1)}.

Tất cả các chứng minh được đưa ra trong Phụ lục B.

Bằng lời: nếu chúng ta biết token đầu ra là "king" hoặc "queen" (giả sử 
ngữ cảnh là về một vị vua), thì xác suất rằng đầu ra là "king" là logit-tuyến 
tính trong biểu diễn mô hình ngôn ngữ với các hệ số hồi quy ¯γW.

Vô hướng ngẫu nhiên α là một hàm của cặp phản thực cụ thể {Y(0), Y(1)}—
ví dụ: nó có thể khác nhau cho {"king", "queen"} và {"roi", "reine"}. 
Tuy nhiên, hướng được sử dụng để dự đoán là giống nhau cho tất cả các 
cặp phản thực thể hiện khái niệm.

Định lý 2.2 cho thấy mối liên hệ giữa biểu diễn không gian con và biểu 
diễn tuyến tính được học bằng cách khớp một linear probe để dự đoán khái 
niệm. Cụ thể, trong cả hai trường hợp, chúng ta nhận được một bộ dự đoán 
tuyến tính trên thang logit. Tuy nhiên, biểu diễn unembedding khác với 
biểu diễn dựa trên probe ở chỗ nó không kết hợp bất kỳ thông tin nào về 
các khái niệm tương quan nhưng ngoài mục tiêu. Ví dụ, nếu văn bản tiếng 
Pháp chủ yếu về nam giới, một probe có thể học thông tin này (và bao gồm 
nó trong biểu diễn), nhưng biểu diễn unembedding thì không. Theo nghĩa 
này, biểu diễn unembedding có thể được xem như một biểu diễn probing lý tưởng.

2.3. Biểu diễn Embedding và Can thiệp

Bước tiếp theo là định nghĩa một biểu diễn không gian con tuyến tính 
trong không gian embedding Λ. Chúng tôi sẽ lại sử dụng một khái niệm 
được neo trong các cặp thể hiện. Trong không gian embedding, mỗi λ(x) 
định nghĩa một phân phối trên các khái niệm. Chúng tôi xem xét các cặp 
câu như λ0=λ("He is the monarch of England, ") và λ1=λ("She is the monarch of England, ") 
mà tạo ra các phân phối khác nhau trên khái niệm mục tiêu, nhưng cùng 
phân phối trên tất cả các khái niệm ngoài mục tiêu. Một khái niệm được 
biểu diễn embedding nếu sự khác biệt giữa tất cả các cặp như vậy thuộc 
về một không gian con chung. Chính thức,

Định nghĩa 2.3 (Biểu diễn Embedding). Chúng tôi nói rằng ¯λW là một 
biểu diễn embedding của một khái niệm W nếu chúng ta có λ1−λ0∈Cone(¯λW) 
đối với bất kỳ context embedding λ0, λ1∈Λ nào thỏa mãn
P(W= 1|λ1)/P(W= 1|λ0)>1 và P(W, Z|λ1)/P(W, Z|λ0)=P(W|λ1)/P(W|λ0),
đối với mỗi khái niệm Z tách biệt nhân quả với W.

Điều kiện đầu tiên đảm bảo rằng hướng có liên quan đến khái niệm mục tiêu, 
và điều kiện thứ hai đảm bảo rằng hướng không liên quan đến các khái niệm 
ngoài mục tiêu.

Kết nối với Can thiệp Hóa ra biểu diễn embedding liên quan chặt chẽ với 
khái niệm can thiệp của biểu diễn tuyến tính. Để làm điều này, chúng ta 
cần bổ đề sau liên quan biểu diễn embedding và unembedding.

Bổ đề 2.4 (Mối quan hệ Unembedding-Embedding). Cho ¯λW là biểu diễn 
embedding của một khái niệm W, và cho ¯γW và ¯γZ là các biểu diễn unembedding 
cho W và bất kỳ khái niệm Z nào tách biệt nhân quả với W. Khi đó,
¯λ⊤W¯γW>0 và ¯λ⊤W¯γZ= 0. (2.1)

Ngược lại, nếu một biểu diễn ¯λW thỏa mãn (2.1), và nếu tồn tại các khái 
niệm {Zi}d−1i=1, sao cho mỗi Zi tách biệt nhân quả với W và {¯γW}∪{¯γZi}d−1i=1 
là cơ sở của Rd, thì ¯λW là biểu diễn embedding cho W.

Bây giờ chúng ta có thể kết nối với khái niệm can thiệp:

Định lý 2.5 (Biểu diễn Can thiệp). Cho ¯λW là biểu diễn embedding của 
một khái niệm W. Khi đó, đối với bất kỳ khái niệm Z nào tách biệt nhân 
quả với W,
P(Y=Y(W,1)|Y∈ {Y(W,0), Y(W,1)}, λ+c¯λW)
không đổi theo c∈R, và
P(Y=Y(1, Z)|Y∈ {Y(0, Z), Y(1, Z)}, λ+c¯λW)
tăng theo c∈R.

Bằng lời: việc thêm ¯λW vào biểu diễn mô hình ngôn ngữ của ngữ cảnh 
thay đổi xác suất của khái niệm mục tiêu (W), nhưng không thay đổi xác 
suất của các khái niệm ngoài mục tiêu (Z).

3. Tích Vô Hướng cho Biểu diễn Mô hình Ngôn ngữ

Với các biểu diễn tuyến tính, chúng tôi muốn sử dụng chúng bằng cách 
làm những việc như đo độ tương tự giữa các biểu diễn khác nhau, hoặc 
chỉnh sửa biểu diễn được chiếu lên một hướng mục tiêu. Độ tương tự và 
phép chiếu đều là các khái niệm yêu cầu một tích vô hướng. Bây giờ chúng 
tôi xem xét câu hỏi về tích vô hướng nào là phù hợp để hiểu biểu diễn 
mô hình ngôn ngữ.

Sơ bộ Chúng tôi định nghĩa ¯Γ là không gian của các hiệu giữa các phần 
tử của Γ. Khi đó, ¯Γ là một không gian vector thực d-chiều.1 Chúng tôi 
xem xét việc định nghĩa tích vô hướng trên ¯Γ. Các biểu diễn unembedding 
tự nhiên là các hướng (chỉ duy nhất đến phép co giãn). Khi chúng ta có 
một tích vô hướng, chúng tôi định nghĩa biểu diễn unembedding chính tắc ¯γW 
là phần tử của hình nón với ⟨¯γW,¯γW⟩= 1. Điều này cho phép chúng ta định 
nghĩa tích vô hướng giữa các biểu diễn unembedding.

Không thể xác định được tích vô hướng Chúng ta có thể hy vọng rằng có 
một tích vô hướng tự nhiên nào đó được chọn ra (xác định) bởi việc huấn 
luyện mô hình. Hóa ra điều này không phải như vậy. Để hiểu thách thức, 
hãy xem xét việc biến đổi các không gian unembedding và embedding theo
g(y)←Aγ(y) +β, l(x)←A−⊤λ(x), (3.1)
trong đó A∈Rd×d là một phép biến đổi tuyến tính khả nghịch nào đó và 
β∈Rd là một hằng số. Dễ thấy rằng phép biến đổi này bảo toàn phân phối 
softmax P(y|x):
exp(λ(x)⊤γ(y))/∑y′exp(λ(x)⊤γ(y′))=exp(l(x)⊤g(y))/∑y′exp(l(x)⊤g(y′)),∀x, y.

Tuy nhiên, hàm mục tiêu được sử dụng để huấn luyện mô hình chỉ phụ thuộc 
vào các biểu diễn thông qua xác suất softmax. Do đó, biểu diễn γ được xác 
định (tốt nhất) chỉ đến một phép biến đổi affine khả nghịch nào đó.

Điều này cũng có nghĩa là các biểu diễn khái niệm ¯γW chỉ được xác định 
đến một phép biến đổi tuyến tính khả nghịch A nào đó. Vấn đề là, với bất 
kỳ tích vô hướng cố định nào,
⟨¯γW,¯γZ⟩ ̸=⟨A¯γW, A¯γZ⟩,
nói chung. Theo đó, không có lý do rõ ràng nào để mong đợi rằng các thao 
tác đại số dựa trên, ví dụ: tích vô hướng Euclidean, nên có ý nghĩa ngữ nghĩa.

1Lưu ý rằng không gian unembedding Γ chỉ là một không gian affine, vì 
softmax bất biến đối với việc thêm một hằng số.

3.1. Tích Vô Hướng Nhân Quả

Chúng tôi yêu cầu một số nguyên tắc bổ sung để chọn một tích vô hướng 
trên không gian biểu diễn. Trực giác mà chúng tôi theo đuổi ở đây là 
các khái niệm tách biệt nhân quả nên được biểu diễn như các vector trực 
giao. Ví dụ, Tiếng Anh⇒Tiếng Pháp và Nam⇒Nữ, nên trực giao. Chúng 
tôi định nghĩa một tích vô hướng có tính chất này:

Định nghĩa 3.1 (Tích Vô Hướng Nhân Quả). Một tích vô hướng nhân quả 
⟨·,·⟩C trên ¯Γ≃Rd là một tích vô hướng sao cho
⟨¯γW,¯γZ⟩C= 0,
đối với bất kỳ cặp khái niệm tách biệt nhân quả W và Z nào.

Sự lựa chọn này hóa ra có tính chất chính là nó thống nhất các biểu diễn 
unembedding và embedding:

Định lý 3.2 (Thống nhất Biểu diễn). Giả sử rằng, đối với bất kỳ khái 
niệm W nào, tồn tại các khái niệm {Zi}d−1i=1 sao cho mỗi Zi tách biệt 
nhân quả với W và {¯γW} ∪ {¯γZi}d−1i=1 là một cơ sở của Rd. Nếu ⟨·,·⟩C 
là một tích vô hướng nhân quả, thì đồng cấu Riesz ¯γ→ ⟨¯γ,·⟩C, đối với 
¯γ∈¯Γ, ánh xạ biểu diễn unembedding ¯γW của mỗi khái niệm W thành biểu 
diễn embedding ¯λW của nó:
⟨¯γW,·⟩C=¯λ⊤W.

Để hiểu kết quả này một cách trực quan, hãy chú ý rằng chúng ta có thể 
biểu diễn embedding như vector hàng và unembedding như vector cột. Nếu 
tích vô hướng nhân quả là tích vô hướng Euclidean, đồng cấu sẽ đơn giản 
là phép chuyển vị. Định lý là tổng quát hóa (đồng cấu Riesz) của ý tưởng 
này: mỗi ánh xạ tuyến tính trên ¯Γ tương ứng với một λ∈Λ nào đó theo 
λ⊤: ¯γ→λ⊤¯γ. Vậy, chúng ta có thể ánh xạ ¯Γ đến Λ bằng cách ánh xạ 
mỗi ¯γW thành một hàm tuyến tính theo ¯γW→ ⟨¯γW,·⟩C. Định lý nói rằng 
ánh xạ này gửi mỗi biểu diễn unembedding của một khái niệm đến biểu diễn 
embedding của cùng khái niệm đó.

Trong các thí nghiệm, chúng tôi sẽ sử dụng kết quả này để xây dựng biểu 
diễn embedding từ biểu diễn unembedding. Đặc biệt, điều này cho phép 
chúng ta tìm biểu diễn can thiệp của các khái niệm. Điều này quan trọng 
vì trong thực tế rất khó tìm các cặp prompt trực tiếp thỏa mãn Định nghĩa 2.3.

3.2. Dạng Tường minh cho Tích Vô Hướng Nhân Quả

Vấn đề tiếp theo là: nếu một tích vô hướng nhân quả tồn tại, làm thế nào 
chúng ta có thể tìm thấy nó? Về nguyên tắc, điều này có thể được thực hiện 
bằng cách tìm biểu diễn unembedding của một số lượng lớn khái niệm, và 
sau đó tìm một tích vô hướng ánh xạ mỗi cặp hướng tách biệt nhân quả thành 
không. Trong thực tế, điều này không khả thi vì số lượng khái niệm cần thiết 
để tìm tích vô hướng, và khó khăn trong việc ước tính biểu diễn của mỗi khái niệm.

Bây giờ chúng tôi chuyển sang phát triển một cách tiếp cận dễ xử lý hơn 
dựa trên insight sau: biết giá trị của khái niệm W được thể hiện bởi một 
từ được chọn ngẫu nhiên cho chúng ta biết ít về giá trị của một khái niệm 
tách biệt nhân quả Z được thể hiện bởi từ đó. Ví dụ, nếu chúng ta biết 
rằng một từ được lấy mẫu ngẫu nhiên là tiếng Pháp (không phải tiếng Anh), 
điều này không cho chúng ta thông tin đáng kể về việc liệu nó có đề cập 
đến một người đàn ông hay phụ nữ.2

Chúng tôi chính thức hóa ý tưởng này như sau:

Giả định 3.3. Giả sử W, Z là các khái niệm tách biệt nhân quả và γ là 
một unembedding vector được lấy mẫu đều từ từ vựng. Khi đó, ¯λ⊤Wγ và 
¯λ⊤Zγ độc lập3 đối với bất kỳ biểu diễn embedding ¯λW và ¯λZ cho W và 
Z, tương ứng.

Giả định này cho phép chúng ta kết nối tính tách biệt nhân quả với một 
điều mà chúng ta thực sự có thể đo lường: sự phụ thuộc thống kê giữa 
các từ. Kết quả tiếp theo làm cho điều này chính xác.

Định lý 3.4 (Dạng Tường minh của Tích Vô Hướng Nhân Quả). Giả sử tồn 
tại một tích vô hướng nhân quả, được biểu diễn là ⟨¯γ,¯γ′⟩C= ¯γ⊤M¯γ′ 
đối với một ma trận đối xứng xác định dương M nào đó. Nếu có các khái 
niệm tách biệt nhân quả lẫn nhau {Wk}dk=1, sao cho các biểu diễn chính 
tắc của chúng G= [¯γW1,···,¯γWd] tạo thành một cơ sở cho ¯Γ≃Rd, thì 
dưới Giả định 3.3,
M−1=GG⊤ và G⊤Cov(γ)−1G=D, (3.2)
đối với một ma trận chéo D nào đó có các mục dương, trong đó γ là unembedding 
vector của một từ được lấy mẫu đều từ từ vựng.

2Lưu ý rằng giả định này là về các từ được lấy mẫu ngẫu nhiên từ từ vựng, 
không phải các từ được lấy mẫu ngẫu nhiên từ các nguồn ngôn ngữ tự nhiên. 
Trong trường hợp sau, có thể có các tương quan không nhân quả giữa các 
khái niệm tách biệt nhân quả.

3Thực tế, để chứng minh kết quả tiếp theo của chúng tôi, chúng tôi chỉ 
yêu cầu ¯λ⊤Wγ và ¯λ⊤Zγ không tương quan. Trong Phụ lục D.6, chúng tôi 
xác minh rằng tích vô hướng nhân quả mà chúng tôi tìm thấy thỏa mãn điều 
kiện không tương quan.

Chú ý rằng tính trực giao nhân quả chỉ áp đặt d(d−1)/2 ràng buộc trên 
tích vô hướng, nhưng có d(d−1)/2+d bậc tự do trong việc xác định ma trận 
xác định dương M (do đó, một tích vô hướng)—vì vậy, chúng ta mong đợi d 
bậc tự do trong việc chọn một tích vô hướng nhân quả. Định lý 3.4 đưa ra 
một đặc trưng của lớp tích vô hướng này, dưới dạng (3.2). Ở đây, D là 
một tham số tự do với d bậc tự do. Mỗi D định nghĩa tích vô hướng. Chúng 
tôi không có nguyên tắc để chọn ra một lựa chọn duy nhất của D. Trong 
các thí nghiệm của chúng tôi, chúng tôi sẽ làm việc với lựa chọn D=Id, 
điều này cho chúng ta M= Cov(γ)−1. Khi đó, chúng ta có một dạng đóng 
đơn giản cho tích vô hướng tương ứng:
⟨¯γ,¯γ′⟩C:= ¯γ⊤Cov(γ)−1¯γ′,∀¯γ,¯γ′∈¯Γ. (3.3)

Lưu ý rằng mặc dù chúng ta không có một tích vô hướng duy nhất, chúng ta 
có thể loại trừ hầu hết các tích vô hướng. Ví dụ: tích vô hướng Euclidean 
không phải là tích vô hướng nhân quả nếu M=Id không thỏa mãn (3.2) đối 
với bất kỳ D nào.

Biểu diễn thống nhất Sự lựa chọn tích vô hướng cũng có thể được xem như 
định nghĩa một sự lựa chọn biểu diễn g và l trong (3.1) (do đó, ¯g=A¯γ). 
Với A=M1/2, Định lý 3.2 tiếp tục ngụ ý rằng một tích vô hướng nhân quả 
làm cho biểu diễn embedding và unembedding của các khái niệm giống nhau, 
tức là ¯gW=¯lW. Hơn nữa, trong không gian được biến đổi, tích vô hướng 
Euclidean là tích vô hướng nhân quả: ⟨¯γ,¯γ′⟩C= ¯g⊤¯g′. Trong Hình 1, 
chúng tôi đã minh họa sự thống nhất này của biểu diễn unembedding và embedding.

Điều này thuận tiện cho các thí nghiệm, vì nó cho phép sử dụng các công 
cụ Euclidean chuẩn trên không gian được biến đổi.

4. Thí nghiệm

Bây giờ chúng tôi chuyển sang xác nhận thực nghiệm sự tồn tại của biểu 
diễn tuyến tính, tích vô hướng nhân quả được ước tính, và các mối quan 
hệ được dự đoán giữa các khái niệm không gian con, đo lường và can thiệp 
của biểu diễn tuyến tính. Mã nguồn có sẵn tại github.com/KihoPark/linear repgeometry.

Chúng tôi sử dụng mô hình LLaMA-2 với 7 tỷ tham số (Touvron et al., 2023) 
làm nền tảng thử nghiệm. Đây là một LLM Transformer chỉ có bộ giải mã 
(Vaswani et al., 2017; Radford et al., 2018), được huấn luyện bằng mục 
tiêu LM tiến và một từ vựng 32K token. Chúng tôi bao gồm thêm chi tiết 
về tất cả các thí nghiệm trong Phụ lục C.

Các khái niệm được biểu diễn như các hướng trong không gian unembedding 
Chúng tôi bắt đầu với giả thuyết rằng các khái niệm được biểu diễn như 
các hướng trong không gian biểu diễn unembedding (Định nghĩa 2.1). Khái 
niệm này dựa vào các cặp từ phản thực chỉ biến đổi về giá trị của khái 
niệm quan tâm. Chúng tôi xem xét 22 khái niệm được định nghĩa trong Big 
Analogy Test Set (BATS 3.0) (Gladkova et al., 2016), cung cấp các cặp 
phản thực như vậy.4 Chúng tôi cũng xem xét 4 khái niệm ngôn ngữ: Tiếng Anh⇒Tiếng Pháp, 
Tiếng Pháp⇒Tiếng Đức, Tiếng Pháp⇒Tiếng Tây Ban Nha, và Tiếng Đức⇒Tiếng Tây Ban Nha, 
trong đó chúng tôi sử dụng các từ và bản dịch của chúng làm cặp phản thực. 
Ngoài ra, chúng tôi xem xét khái niệm thường xuyên⇒không thường xuyên 
nắm bắt mức độ phổ biến của một từ—chúng tôi sử dụng các cặp từ đồng nghĩa 
phổ biến/không phổ biến (ví dụ: "bad" và "terrible") làm cặp phản thực. 
Chúng tôi cung cấp bảng tất cả 27 khái niệm mà chúng tôi xem xét trong Phụ lục C.

Nếu khái niệm không gian con của giả thuyết biểu diễn tuyến tính đúng, 
thì tất cả các cặp token phản thực nên chỉ về một hướng chung trong không 
gian unembedding. Trong thực tế, điều này sẽ chỉ đúng gần đúng. Tuy nhiên, 
nếu giả thuyết biểu diễn tuyến tính đúng, chúng ta vẫn mong đợi rằng, ví dụ: 
γ("queen")−γ("king") sẽ thẳng hàng với hướng nam⇒nữ (gần hơn so với sự 
khác biệt giữa các cặp từ ngẫu nhiên). Để xác nhận điều này, đối với mỗi 
khái niệm W, chúng tôi xem xét cách hướng được định nghĩa bởi mỗi cặp phản 
thực, γ(yi(1))−γ(yi(0)), được sắp xếp hình học với biểu diễn unembedding ¯γW. 
Chúng tôi ước tính ¯γW là (chuẩn hóa) trung bình5 giữa tất cả các cặp phản thực: 
¯γW:= ˜γW/√⟨˜γW,˜γW⟩C, trong đó
˜γW=1/nW ∑i=1^nW [γ(yi(1))−γ(yi(0))],
nW biểu thị số lượng cặp phản thực cho W, và ⟨·,·⟩C biểu thị tích vô hướng 
nhân quả được định nghĩa trong (3.3).

Hình 2 trình bày các biểu đồ của mỗi γ(yi(1))−γ(yi(0))) được chiếu lên 
¯γW đối với tích vô hướng nhân quả. Vì ¯γW được tính bằng γ(yi(1))−γ(yi(0)), 
chúng tôi tính mỗi phép chiếu bằng cách sử dụng ước tính loại-trừ-một (LOO) 
¯γW,(−i) của hướng khái niệm loại trừ (yi(0), yi(1)).

Qua ba khái niệm được hiển thị (và 23 khái niệm khác được hiển thị trong 
Phụ lục D.1), sự khác biệt giữa các cặp phản thực được sắp xếp đáng kể 
hơn với ¯γW so với những cặp giữa các cặp ngẫu nhiên. Ngoại lệ duy nhất 
là thing⇒part, dường như không có biểu diễn tuyến tính.

4Chúng tôi chỉ sử dụng các từ là token đơn trong mô hình LLaMA-2. Xem 
Phụ lục C để biết chi tiết.

5Nghiên cứu trước đây về word embedding (Drozd et al., 2016; Fournier et al., 2020) 
động lực lấy trung bình để cải thiện tính nhất quán của hướng khái niệm.

Các kết quả phù hợp với giả thuyết biểu diễn tuyến tính: sự khác biệt được 
tính bởi mỗi cặp phản thực chỉ về một hướng chung đại diện cho một không 
gian con tuyến tính (đến một số nhiễu). Hơn nữa, ¯γW là một bộ ước tính 
hợp lý cho hướng đó.

Tích vô hướng được ước tính tôn trọng tính tách biệt nhân quả Tiếp theo, 
chúng tôi kiểm tra trực tiếp liệu tích vô hướng ước tính (3.3) được chọn 
từ Định lý 3.4 có thực sự gần đúng là một tích vô hướng nhân quả không. 
Trong Hình 3, chúng tôi vẽ một heatmap của các tích vô hướng giữa tất cả 
các cặp biểu diễn unembedding ước tính cho 27 khái niệm. Nếu tích vô hướng 
ước tính là một tích vô hướng nhân quả, thì chúng ta mong đợi các giá trị 
gần 0 giữa các khái niệm tách biệt nhân quả.

Quan sát đầu tiên là hầu hết các cặp khái niệm gần như trực giao đối với 
tích vô hướng này. Thú vị là cũng có một cấu trúc khối chéo rõ ràng. Điều 
này phát sinh vì các khái niệm được nhóm theo độ tương tự ngữ nghĩa. Ví dụ, 
10 khái niệm đầu tiên liên quan đến động từ, và 4 khái niệm cuối cùng là 
các cặp ngôn ngữ. Cấu trúc khác không cũng thường có ý nghĩa. Ví dụ, 
lower⇒upper (viết hoa, khái niệm 19) có tích vô hướng không tầm thường 
với các cặp ngôn ngữ khác ngoài Tiếng Pháp⇒Tiếng Tây Ban Nha. Điều này 
có thể là do tiếng Pháp và tiếng Tây Ban Nha tuân theo các quy tắc viết 
hoa tương tự, trong khi tiếng Anh và tiếng Đức mỗi ngôn ngữ có các quy ước 
khác nhau (ví dụ: tiếng Đức viết hoa tất cả danh từ, nhưng tiếng Anh chỉ 
viết hoa danh từ riêng).

Trong Phụ lục D.2, chúng tôi so sánh tích vô hướng Euclidean với tích vô 
hướng nhân quả cho cả mô hình LLaMA-2 và mô hình ngôn ngữ lớn Gemma gần 
đây hơn (Mesnard et al., 2024).

Các hướng khái niệm hoạt động như linear probe Tiếp theo, chúng tôi kiểm 
tra mối liên hệ với khái niệm đo lường của biểu diễn tuyến tính. Chúng tôi 
xem xét khái niệm W=Tiếng Pháp⇒Tiếng Tây Ban Nha. Để xây dựng một tập 
dữ liệu của các ngữ cảnh tiếng Pháp và tiếng Tây Ban Nha, chúng tôi lấy 
mẫu các ngữ cảnh có độ dài ngẫu nhiên từ các trang Wikipedia trong mỗi 
ngôn ngữ. Lưu ý rằng đây không phải là các cặp phản thực. Theo Định lý 2.2, 
chúng ta mong đợi ¯γ⊤Wλ(xfr_j)<0 và ¯γ⊤Wλ(xes_j)>0. Hình 4 xác nhận 
kỳ vọng này, cho thấy rằng ¯γW là một linear probe cho khái niệm W trong 
Λ (trái). Cũng vậy, biểu diễn của một khái niệm ngoài mục tiêu Z=nam⇒nữ 
không có bất kỳ sức mạnh dự đoán nào cho nhiệm vụ này (phải). Phụ lục D.3 
bao gồm các kết quả tương tự sử dụng tất cả 27 khái niệm.

Các hướng khái niệm ánh xạ thành biểu diễn can thiệp Định lý 2.5 nói rằng 
chúng ta có thể xây dựng một biểu diễn can thiệp bằng cách xây dựng một 
biểu diễn embedding. Việc làm điều này trực tiếp yêu cầu tìm các cặp prompt 
chỉ biến đổi về phân phối mà chúng tạo ra trên khái niệm mục tiêu, điều 
này có thể khó tìm trong thực tế.

Ở đây, thay vào đó chúng tôi sẽ sử dụng đồng cấu giữa biểu diễn embedding 
và unembedding (Định lý 3.2) để xây dựng biểu diễn can thiệp từ biểu diễn 
unembedding. Chúng tôi lấy
¯λW:= Cov(γ)−1¯γW. (4.1)

Định lý 2.5 dự đoán rằng việc thêm ¯λW vào một biểu diễn ngữ cảnh nên 
tăng xác suất của W, trong khi để xác suất của tất cả các khái niệm tách 
biệt nhân quả không thay đổi.

Để kiểm tra điều này cho một cặp khái niệm tách biệt nhân quả W và Z, 
chúng tôi đầu tiên chọn một bộ bốn {Y(w, z)}w,z∈{0,1}, và sau đó tạo 
ngữ cảnh {xj} sao cho từ tiếp theo nên là Y(0,0). Ví dụ, nếu W=nam⇒nữ 
và Z=lower⇒upper, thì chúng tôi chọn bộ bốn ("king", "queen", "King", "Queen"), 
và tạo ngữ cảnh bằng ChatGPT-4 (ví dụ: "Long live the"). Sau đó chúng tôi 
can thiệp vào λ(xj) bằng cách sử dụng ¯λC thông qua
λC,α(xj) =λ(xj) +α¯λC, (4.2)
trong đó α >0 và C có thể là W, Z, hoặc một khái niệm tách biệt nhân quả 
khác (ví dụ: Tiếng Pháp⇒Tiếng Tây Ban Nha). Đối với các lựa chọn khác 
nhau của C, chúng tôi vẽ các thay đổi trong logitP(W=1|Z, λ) và logitP(Z= 1|W, λ), 
khi chúng tôi tăng α.

Chúng ta mong đợi thấy rằng, nếu chúng ta can thiệp theo hướng W, thì 
can thiệp nên tăng tuyến tính logitP(W=1|Z, λ), trong khi logit khác nên 
giữ nguyên; nếu chúng ta can thiệp theo hướng C tách biệt nhân quả với 
cả W và Z, thì chúng ta mong đợi cả hai logit đều giữ nguyên.

Hình 5 cho thấy kết quả của một thí nghiệm như vậy được hiển thị cho ba 
khái niệm mục tiêu (24 khái niệm khác được hiển thị trong Phụ lục D.4), 
xác nhận kỳ vọng của chúng tôi. Chúng ta thấy, ví dụ, rằng can thiệp theo 
hướng nam⇒nữ làm tăng logit để chọn "queen" thay vì "king" làm từ tiếp 
theo, nhưng không thay đổi logit cho "King" thay vì "king".

Một câu hỏi tiếp theo tự nhiên là xem liệu can thiệp theo hướng khái niệm 
(cho W) có đẩy xác suất của Y(W= 1) là từ tiếp theo trở thành lớn nhất 
trong tất cả các token không. Chúng ta mong đợi thấy rằng, khi chúng ta 
tăng giá trị của α, khái niệm mục tiêu cuối cùng nên được phản ánh trong 
các từ đầu ra có khả năng nhất theo LM.

Trong Bảng 1, chúng tôi hiển thị một ví dụ minh họa trong đó W là khái 
niệm nam⇒nữ và ngữ cảnh x là một đoạn câu có thể kết thúc bằng từ Y(W= 0) 
("king"). Đối với x="Long live the ", khi chúng tôi tăng thang α trên can 
thiệp, chúng ta thấy rằng từ mục tiêu Y(W= 1) ("queen ") trở thành từ tiếp 
theo có khả năng nhất, trong khi từ ban đầu Y(W= 0) rơi xuống dưới danh 
sách top-5. Điều này minh họa cách can thiệp có thể đẩy xác suất của từ 
mục tiêu đủ cao để làm cho nó trở thành từ có khả năng nhất trong khi giảm 
xác suất của từ ban đầu.

5. Thảo luận và Công trình Liên quan

Ý tưởng rằng các khái niệm cấp cao được mã hóa tuyến tính rất hấp dẫn vì—
nếu nó đúng—nó có thể mở ra các phương pháp đơn giản để diễn giải và kiểm 
soát LLM. Trong bài báo này, chúng tôi đã chính thức hóa 'biểu diễn tuyến 
tính', và chỉ ra rằng tất cả các biến thể tự nhiên của khái niệm này có thể 
được thống nhất.6 Sự tương đương này đã gợi ý một số cách tiếp cận cho 
diễn giải và kiểm soát—ví dụ: chúng tôi chỉ ra cách sử dụng các tập hợp 
cặp từ để định nghĩa hướng khái niệm, và sau đó sử dụng các hướng này để 
dự đoán đầu ra của mô hình sẽ là gì, và thay đổi đầu ra một cách có kiểm 
soát. Một chủ đề chính là vai trò được đóng bởi việc lựa chọn tích vô hướng.

Các không gian con tuyến tính trong biểu diễn ngôn ngữ Giả thuyết không 
gian con tuyến tính ban đầu được quan sát thực nghiệm trong ngữ cảnh của 
word embedding (ví dụ: Mikolov et al., 2013b;c; Levy & Goldberg, 2014; 
Goldberg & Levy, 2014; Vylomova et al., 2016; Gladkova et al., 2016; 
Chiang et al., 2020; Fournier et al., 2020). Cấu trúc tương tự đã được 
quan sát trong cross-lingual word embedding (Mikolov et al., 2013a; Lample et al., 2018; 
Ruder et al., 2019; Peng et al., 2022), sentence embedding (Bowman et al., 2016; 
Zhu & de Melo, 2020; Li et al., 2020; Ushio et al., 2021), không gian biểu 
diễn của Transformer LLM (Meng et al., 2022; Merullo et al., 2023; Hernandez et al., 2023), 
và các mô hình vision-language (Wang et al., 2023; Trager et al., 2023; 
Perera et al., 2023). Những quan sát này động lực cho Định nghĩa 2.1. Ý 
tưởng chính trong bài báo hiện tại là cung cấp sự chính thức hóa theo các 
cặp phản thực—đây là điều cho phép chúng ta kết nối với các khái niệm khác 
về biểu diễn tuyến tính, và xác định cấu trúc tích vô hướng.

6Trong Phụ lục A, chúng tôi tóm tắt những kết quả này trong một hình.

Đo lường, can thiệp và khả năng diễn giải cơ chế Có một khối lượng công 
trình đáng kể về biểu diễn tuyến tính để diễn giải (probing) (ví dụ: Alain & Bengio, 2017; 
Kim et al., 2018; nostalgebraist, 2020; Rogers et al., 2021; Belinkov, 2022; 
Li et al., 2022; Geva et al., 2022; Nanda et al., 2023) và kiểm soát (steering) 
(ví dụ: Wang et al., 2023; Turner et al., 2023; Merullo et al., 2023; 
Trager et al., 2023) các mô hình. Điều này đặc biệt nổi bật trong khả năng 
diễn giải cơ chế (Elhage et al., 2021; Meng et al., 2022; Hernandez et al., 2023; 
Turner et al., 2023; Zou et al., 2023; Todd et al., 2023; Hendel et al., 2023). 
Đối với khối lượng công trình này, đóng góp chính của bài báo hiện tại là 
làm rõ giả thuyết biểu diễn tuyến tính, và vai trò quan trọng của tích vô hướng. 
Tuy nhiên, chúng tôi không giải quyết khả năng diễn giải của các tham số 
mô hình, cũng như các activation của các lớp trung gian. Đây là trọng tâm 
chính của công trình hiện có. Đó là một hướng nghiên cứu thú vị cho công 
trình tương lai để hiểu cách các ý tưởng ở đây—đặc biệt là tích vô hướng 
nhân quả—chuyển đổi sang các cài đặt này.

Hình học của biểu diễn Có một dòng công trình nghiên cứu hình học của 
biểu diễn từ và câu (ví dụ: Arora et al., 2016; Mimno & Thompson, 2017; 
Ethayarajh, 2019; Reif et al., 2019; Li et al., 2020; Hewitt & Manning, 2019; 
Chen et al., 2021; Chang et al., 2022; Jiang et al., 2023). Công trình này 
xem xét, ví dụ: việc trực quan hóa và mô hình hóa cách các embedding được 
học được phân phối, hoặc cách cấu trúc phân cấp được mã hóa. Công trình 
của chúng tôi phần lớn trực giao với những công trình này, vì chúng tôi đang 
cố gắng định nghĩa một tích vô hướng phù hợp (và do đó, các khái niệm về 
độ tương tự và phép chiếu) tôn trọng cấu trúc ngữ nghĩa của ngôn ngữ.

Học biểu diễn nhân quả Cuối cùng, các ý tưởng ở đây kết nối với học biểu 
diễn nhân quả (ví dụ: Higgins et al., 2016; Hyvarinen & Morioka, 2016; 
Higgins et al., 2018; Khemakhem et al., 2020; Zimmermann et al., 2021; 
Schölkopf et al., 2021; Moran et al., 2021; Wang et al., 2023). Rõ ràng 
nhất, sự chính thức hóa nhân quả của chúng tôi về các khái niệm được lấy 
cảm hứng từ Wang et al. (2023), người thiết lập một đặc trưng của các khái 
niệm tiềm ẩn và đại số vector trong các mô hình khuếch tán. Riêng biệt, 
một chủ đề chính trong văn học này là khả năng xác định của các biểu diễn 
được học—tức là mức độ mà chúng nắm bắt cấu trúc thế giới thực bên dưới. 
Các kết quả tích vô hướng nhân quả của chúng tôi có thể được xem trong chủ 
đề này, cho thấy rằng một tích vô hướng tôn trọng sự gần gũi ngữ nghĩa không 
được xác định bởi quy trình huấn luyện thông thường, nhưng nó có thể được 
chọn ra với một giả định phù hợp.

Lời cảm ơn

Cảm ơn Gemma Moran vì những bình luận về bản thảo trước đó. Công trình này 
được hỗ trợ bởi tài trợ ONR N00014-23-1-2591 và Open Philanthropy.

Tài liệu tham khảo

Alain, G. and Bengio, Y. Understanding intermediate layers using linear 
classifier probes. In International Conference on Learning Representations, 2017. 
URL https://openreview.net/forum?id=ryF7rTqgl.

Arora, S., Li, Y., Liang, Y., Ma, T., and Risteski, A. A latent variable 
model approach to PMI-based word embeddings. Transactions of the Association 
for Computational Linguistics, 4:385–399, 2016.

Belinkov, Y. Probing classifiers: Promises, shortcomings, and advances. 
Computational Linguistics, 48(1):207–219, 2022.

Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A., Jozefowicz, R., and Bengio, S. 
Generating sentences from a continuous space. In Proceedings of the 20th SIGNLL 
Conference on Computational Natural Language Learning, pp. 10–21, Berlin, Germany, 
August 2016. Association for Computational Linguistics. doi: 10.18653/v1/K16-1002. 
URL https://aclanthology.org/K16-1002.

Chang, T., Tu, Z., and Bergen, B. The geometry of multilingual language model 
representations. In Proceedings of the 2022 Conference on Empirical Methods 
in Natural Language Processing, pp. 119–136, 2022.

Chen, B., Fu, Y., Xu, G., Xie, P., Tan, C., Chen, M., and Jing, L. Probing 
BERT in hyperbolic spaces. In International Conference on Learning Representations, 2021.

Chiang, H.-Y., Camacho-Collados, J., and Pardos, Z. Understanding the source 
of semantic regularities in word embeddings. In Proceedings of the 24th Conference 
on Computational Natural Language Learning, pp. 119–131, 2020.

Choe, Y. J., Park, K., and Kim, D. word2word: A collection of bilingual lexicons 
for 3,564 language pairs. In Proceedings of the Twelfth Language Resources 
and Evaluation Conference, pp. 3036–3045, 2020.

Drozd, A., Gladkova, A., and Matsuoka, S. Word embeddings, analogies, and 
machine learning: Beyond king - man + woman = queen. In Proceedings of COLING 2016, 
the 26th International Conference on Computational Linguistics: Technical papers, 
pp. 3519–3530, 2016.

Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., 
Bai, Y., Chen, A., Conerly, T., et al. A mathematical framework for transformer 
circuits. Transformer Circuits Thread, 1, 2021.

Elhage, N., Hume, T., Olsson, C., Schiefer, N., Henighan, T., Kravec, S., 
Hatfield-Dodds, Z., Lasenby, R., Drain, D., Chen, C., et al. Toy models of 
superposition. arXiv preprint arXiv:2209.10652, 2022.

Ethayarajh, K. How contextual are contextualized word representations? Comparing 
the geometry of BERT, ELMo, and GPT-2 embeddings. In Proceedings of the 2019 
Conference on Empirical Methods in Natural Language Processing and the 9th 
International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 
pp. 55–65, 2019.

Fournier, L., Dupoux, E., and Dunbar, E. Analogies minus analogy test: measuring 
regularities in word embeddings. In Proceedings of the 24th Conference on 
Computational Natural Language Learning, pp. 365–375, Online, 2020. Association 
for Computational Linguistics. doi: 10.18653/v1/2020.conll-1.29. URL https://aclanthology.org/2020.conll-1.29.

Geva, M., Caciularu, A., Wang, K., and Goldberg, Y. Transformer feed-forward 
layers build predictions by promoting concepts in the vocabulary space. In 
Proceedings of the Conference on Empirical Methods in Natural Language Processing, 
pp. 30–45, 2022.

Gladkova, A., Drozd, A., and Matsuoka, S. Analogy-based detection of morphological 
and semantic relations with word embeddings: what works and what doesn't. In 
Proceedings of the NAACL Student Research Workshop, pp. 8–15, 2016.

Goldberg, Y. and Levy, O. word2vec explained: deriving Mikolov et al.'s negative-sampling 
word-embedding method. arXiv preprint arXiv:1402.3722, 2014.

Gurnee, W. and Tegmark, M. Language models represent space and time. arXiv 
preprint arXiv:2310.02207, art. arXiv:2310.02207, October 2023. doi: 10.48550/arXiv.2310.02207.

Hendel, R., Geva, M., and Globerson, A. In-context learning creates task vectors. 
arXiv preprint arXiv:2310.15916, 2023.

Hernandez, E., Sharma, A. S., Haklay, T., Meng, K., Wattenberg, M., Andreas, J., 
Belinkov, Y., and Bau, D. Linearity of relation decoding in transformer language 
models. arXiv preprint arXiv:2308.09124, 2023.

Hewitt, J. and Manning, C. D. A structural probe for finding syntax in word 
representations. In Proceedings of the 2019 Conference of the North American 
Chapter of the Association for Computational Linguistics: Human Language Technologies, 
Volume 1 (Long and Short Papers), pp. 4129–4138, 2019.

Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., 
Mohamed, S., and Lerchner, A. beta-VAE: Learning basic visual concepts with 
a constrained variational framework. In International Conference on Learning 
Representations, 2016.

Higgins, I., Amos, D., Pfau, D., Racaniere, S., Matthey, L., Rezende, D., 
and Lerchner, A. Towards a definition of disentangled representations. arXiv 
preprint arXiv:1812.02230, 2018.

Hyvarinen, A. and Morioka, H. Unsupervised feature extraction by time-contrastive 
learning and nonlinear ICA. Advances in Neural Information Processing Systems, 29, 2016.

Jiang, Y., Aragam, B., and Veitch, V. Uncovering meanings of embeddings via 
partial orthogonality. arXiv preprint arXiv:2310.17611, 2023.

Khemakhem, I., Kingma, D., Monti, R., and Hyvarinen, A. Variational autoencoders 
and nonlinear ICA: A unifying framework. In International Conference on Artificial 
Intelligence and Statistics, pp. 2207–2217. PMLR, 2020.

Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F., et al. 
Interpretability beyond feature attribution: Quantitative testing with concept 
activation vectors (TCAV). In International Conference on Machine Learning, 
pp. 2668–2677. PMLR, 2018.

Kudo, T. and Richardson, J. SentencePiece: A simple and language independent 
subword tokenizer and detokenizer for neural text processing. In Proceedings 
of the 2018 Conference on Empirical Methods in Natural Language Processing: 
System Demonstrations, pp. 66–71, 2018.

Lample, G., Conneau, A., Ranzato, M., Denoyer, L., and Jégou, H. Word translation 
without parallel data. In International Conference on Learning Representations, 2018.

Levy, O. and Goldberg, Y. Linguistic regularities in sparse and explicit word 
representations. In Proceedings of the Eighteenth Conference on Computational 
Natural Language Learning, pp. 171–180, 2014.

Li, B., Zhou, H., He, J., Wang, M., Yang, Y., and Li, L. On the sentence embeddings 
from pre-trained language models. In Proceedings of the 2020 Conference on 
Empirical Methods in Natural Language Processing (EMNLP), pp. 9119–9130, 2020.

Li, K., Hopkins, A. K., Bau, D., Viégas, F., Pfister, H., and Wattenberg, M. 
Emergent world representations: Exploring a sequence model trained on a synthetic 
task. In International Conference on Learning Representations, 2022.

Meng, K., Bau, D., Andonian, A., and Belinkov, Y. Locating and editing factual 
associations in GPT. Advances in Neural Information Processing Systems, 35:17359–17372, 2022.

Merullo, J., Eickhoff, C., and Pavlick, E. Language models implement simple 
word2vec-style vector arithmetic. arXiv preprint arXiv:2305.16130, 2023.

Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L., 
Rivière, M., Kale, M. S., Love, J., et al. Gemma: Open models based on gemini 
research and technology. arXiv preprint arXiv:2403.08295, 2024.

Mikolov, T., Le, Q. V., and Sutskever, I. Exploiting similarities among languages 
for machine translation. arXiv preprint arXiv:1309.4168, 2013a.

Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. Distributed 
representations of words and phrases and their compositionality. Advances in 
Neural Information Processing Systems, 26, 2013b.

Mikolov, T., Yih, W.-T., and Zweig, G. Linguistic regularities in continuous 
space word representations. In Proceedings of the 2013 Conference of the North 
American Chapter of the Association for Computational Linguistics: Human Language 
Technologies, pp. 746–751, 2013c.

Mimno, D. and Thompson, L. The strange geometry of skip-gram with negative 
sampling. In Palmer, M., Hwa, R., and Riedel, S. (eds.), Proceedings of the 
2017 Conference on Empirical Methods in Natural Language Processing, pp. 2873–2878, 
Copenhagen, Denmark, 2017. Association for Computational Linguistics. doi: 
10.18653/v1/D17-1308. URL https://aclanthology.org/D17-1308.

Moran, G. E., Sridhar, D., Wang, Y., and Blei, D. M. Identifiable deep generative 
models via sparse decoding. arXiv preprint arXiv:2110.10804, art. arXiv:2110.10804, 
October 2021. doi: 10.48550/arXiv.2110.10804.

Nanda, N., Lee, A., and Wattenberg, M. Emergent linear representations in world 
models of self-supervised sequence models. arXiv preprint arXiv:2309.00941, 2023.

nostalgebraist. Interpreting GPT: the logit lens, 2020. URL https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens.

OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

Peng, X., Stevenson, M., Lin, C., and Li, C. Understanding linearity of cross-lingual 
word embedding mappings. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. 
URL https://openreview.net/forum?id=8HuyXvbvqX.

Pennington, J., Socher, R., and Manning, C. D. GloVe: Global vectors for word 
representation. In Proceedings of the 2014 Conference on Empirical Methods 
in Natural Language Processing (EMNLP), pp. 1532–1543, 2014.

Perera, P., Trager, M., Zancato, L., Achille, A., and Soatto, S. Prompt algebra 
for task composition. arXiv preprint arXiv:2306.00310, 2023.

Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language 
understanding by generative pre-training. 2018.

Reif, E., Yuan, A., Wattenberg, M., Viegas, F. B., Coenen, A., Pearce, A., 
and Kim, B. Visualizing and measuring the geometry of BERT. Advances in Neural 
Information Processing Systems, 32, 2019.

Rogers, A., Kovaleva, O., and Rumshisky, A. A primer in BERTology: What we 
know about how BERT works. Transactions of the Association for Computational 
Linguistics, 8:842–866, 2021.

Ruder, S., Vulić, I., and Søgaard, A. A survey of cross-lingual word embedding 
models. Journal of Artificial Intelligence Research, 65:569–631, 2019.

Schölkopf, B., Locatello, F., Bauer, S., Ke, N. R., Kalchbrenner, N., Goyal, A., 
and Bengio, Y. Toward causal representation learning. Proceedings of the IEEE, 
109(5):612–634, 2021.

Todd, E., Li, M. L., Sharma, A. S., Mueller, A., Wallace, B. C., and Bau, D. 
Function vectors in large language models. arXiv preprint arXiv:2310.15213, 2023.

Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., 
Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., 
Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., 
Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., 
Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., 
Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., 
Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., 
Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., 
Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., 
Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., 
Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned chat models. 
arXiv preprint arXiv:2307.09288, 2023.

Trager, M., Perera, P., Zancato, L., Achille, A., Bhatia, P., and Soatto, S. 
Linear spaces of meanings: Compositional structures in vision-language models. 
In Proceedings of the IEEE/CVF International Conference on Computer Vision, 
pp. 15395–15404, 2023.

Turner, A. M., Thiergart, L., Udell, D., Leech, G., Mini, U., and MacDiarmid, M. 
Activation addition: Steering language models without optimization. arXiv preprint 
arXiv:2308.10248, art. arXiv:2308.10248, August 2023. doi: 10.48550/arXiv.2308.10248.

Ushio, A., Anke, L. E., Schockaert, S., and Camacho-Collados, J. BERT is to 
NLP what AlexNet is to CV: Can pre-trained language models identify analogies? 
In Proceedings of the 59th Annual Meeting of the Association for Computational 
Linguistics and the 11th International Joint Conference on Natural Language 
Processing (Volume 1: Long Papers), pp. 3609–3624, 2021.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., 
Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in Neural 
Information Processing Systems, 30, 2017.

Vylomova, E., Rimell, L., Cohn, T., and Baldwin, T. Take and took, gaggle and 
goose, book and read: Evaluating the utility of vector differences for lexical 
relation learning. In Proceedings of the 54th Annual Meeting of the Association 
for Computational Linguistics (Volume 1: Long Papers), pp. 1671–1682, 2016.

Wang, Z., Gui, L., Negrea, J., and Veitch, V. Concept algebra for score-based 
conditional models. arXiv preprint arXiv:2302.03693, 2023.

Zhu, X. and de Melo, G. Sentence analogies: Linguistic regularities in sentence 
embeddings. In Proceedings of the 28th International Conference on Computational 
Linguistics, pp. 3389–3400, 2020.

Zimmermann, R. S., Sharma, Y., Schneider, S., Bethge, M., and Brendel, W. 
Contrastive learning inverts the data generating process. In International 
Conference on Machine Learning, pp. 12979–12990. PMLR, 2021.

Zou, A., Phan, L., Chen, S., Campbell, J., Guo, P., Ren, R., Pan, A., Yin, X., 
Mazeika, M., Dombrowski, A.-K., Goel, S., Li, N., Byun, M. J., Wang, Z., Mallen, A., 
Basart, S., Koyejo, S., Song, D., Fredrikson, M., Kolter, Z., and Hendrycks, D. 
Representation engineering: A top-down approach to AI transparency. arXiv preprint 
arXiv:2310.01405, 2023.

A. Tóm tắt Kết quả Chính

Trong Hình 6, chúng tôi đưa ra một tóm tắt cấp cao về các kết quả chính của chúng tôi. Trong Phần 2, chúng tôi đã đưa ra các định nghĩa về biểu diễn unembedding và embedding và cách chúng cũng tạo ra biểu diễn đo lường và can thiệp, tương ứng. Trong Phần 3, chúng tôi đã định nghĩa tích vô hướng nhân quả và chỉ ra cách nó thống nhất biểu diễn unembedding và embedding thông qua đồng cấu Riesz được tạo ra.

B. Chứng minh

B.1. Chứng minh Định lý 2.2

Định lý 2.2 (Biểu diễn Đo lường). Cho W là một khái niệm, và cho ¯γW là biểu diễn unembedding của W. Khi đó, với bất kỳ context embedding λ∈Λ nào,
logitP(Y=Y(1)|Y∈ {Y(0), Y(1)}, λ) =αλ⊤¯γW,
trong đó α >0(h.c.c.) là một hàm của {Y(0), Y(1)}.

Chứng minh. Chứng minh bao gồm việc viết ra phân phối lấy mẫu softmax và gọi Định nghĩa 2.1.

logitP(Y=Y(1)|Y∈ {Y(0), Y(1)}, λ) (B.1)
= logP(Y=Y(1)|Y∈ {Y(0), Y(1)}, λ)/P(Y=Y(0)|Y∈ {Y(0), Y(1)}, λ) (B.2)
=λ⊤{γ(Y(1))−γ(Y(0))} (B.3)
=α·λ⊤¯γW. (B.4)

Trong (B.3), chúng tôi đơn giản viết ra phân phối softmax, cho phép chúng tôi triệt tiêu các hằng số chuẩn hóa cho hai xác suất. Phương trình (B.4) theo trực tiếp từ Định nghĩa 2.1; lưu ý rằng tính ngẫu nhiên của α đến từ tính ngẫu nhiên của {Y(0), Y(1)}.

B.2. Chứng minh Bổ đề 2.4

Bổ đề B.1 (Mối quan hệ Unembedding-Embedding). Cho ¯λW là biểu diễn embedding của một khái niệm W, và cho ¯γW và ¯γZ là các biểu diễn unembedding cho W và bất kỳ khái niệm Z nào tách biệt nhân quả với W. Khi đó,
¯λ⊤W¯γW>0 và ¯λ⊤W¯γZ= 0. (2.1)

Ngược lại, nếu một biểu diễn ¯λW thỏa mãn (2.1), và nếu tồn tại các khái niệm {Zi}d−1i=1, sao cho mỗi Zi tách biệt nhân quả với W và {¯γW} ∪ {¯γZi}d−1i=1 là cơ sở của Rd, thì ¯λW là biểu diễn embedding cho W.

Chứng minh. Cho λ0, λ1 là một cặp embedding sao cho
P(W= 1|λ1)/P(W= 1|λ0)>1 và P(W, Z|λ1)/P(W, Z|λ0)=P(W|λ1)/P(W|λ0), (B.5)
đối với bất kỳ khái niệm Z nào tách biệt nhân quả với W. Khi đó, theo Định nghĩa 2.3,
λ1−λ0∈Cone(¯λW). (B.6)

Điều kiện (B.5) tương đương với
P(W= 1|λ1)/P(W= 1|λ0)>1 và P(Z= 1|W, λ1)/P(Z= 1|W, λ0)= 1. (B.7)

Hai điều kiện này cũng tương đương với cặp điều kiện sau, tương ứng:
P(Y=Y(1)|Y∈ {Y(0), Y(1)}, λ1)/P(Y=Y(1)|Y∈ {Y(0), Y(1)}, λ0)>1 (B.8)
và
P(Y=Y(W,1)|Y∈ {Y(W,0), Y(W,1)}, λ1)/P(Y=Y(W,1)|Y∈ {Y(W,0), Y(W,1)}, λ0)= 1 (B.9)

Lý do là, có điều kiện trên Y∈ {Y(0,0), Y(0,1), Y(1,0), Y(1,1)}, có điều kiện trên W tương đương với có điều kiện trên Y∈ {Y(W,0), Y(W,1)}. Và, sự kiện Z= 1 tương đương với sự kiện Y=Y(W,1). (Bằng lời: nếu chúng ta biết đầu ra là một trong "king", "queen", "roi", "reine" thì có điều kiện trên W= 1 tương đương với có điều kiện trên đầu ra là "king" hoặc "roi". Khi đó, dự đoán liệu từ có phải tiếng Anh không tương đương với dự đoán liệu từ có phải "king" không.)

Theo Định lý 2.2, hai điều kiện (B.8) và (B.9) tương đương với
α(Y(0), Y(1))(λ1−λ0)⊤¯γW>0 và α(Y(W,0), Y(W,1))(λ1−λ0)⊤¯γZ= 0, (B.10)
trong đó các α dương h.c.c. Những điều này lại tương đương với
¯λ⊤W¯γW>0 và ¯λ⊤W¯γZ= 0. (B.11)

Ngược lại, nếu một biểu diễn ¯λW thỏa mãn (B.11) và tồn tại các khái niệm {Zi}d−1i=1 sao cho mỗi khái niệm tách biệt nhân quả với W và {¯γW} ∪ {¯γZi}d−1i=1 là cơ sở của Rd, thì ¯λW duy nhất đến phép nhân với số dương. Nếu tồn tại λ0 và λ1 thỏa mãn (B.5), thì sự tương đương giữa (B.5) và (B.10) nói rằng
(λ1−λ0)⊤¯γW>0 và (λ1−λ0)⊤¯γZ= 0. (B.12)

Nói cách khác, λ1−λ0 cũng thỏa mãn (B.11), ngụ ý rằng nó phải giống như ¯λW đến phép nhân với số dương. Do đó, đối với bất kỳ λ0 và λ1 nào thỏa mãn (B.5), λ1−λ0∈Cone(¯λW).

B.3. Chứng minh Định lý 2.5

Định lý 2.5 (Biểu diễn Can thiệp). Cho ¯λW là biểu diễn embedding của một khái niệm W. Khi đó, đối với bất kỳ khái niệm Z nào tách biệt nhân quả với W,
P(Y=Y(W,1)|Y∈ {Y(W,0), Y(W,1)}, λ+c¯λW)
không đổi theo c∈R, và
P(Y=Y(1, Z)|Y∈ {Y(0, Z), Y(1, Z)}, λ+c¯λW)
tăng theo c∈R.

Chứng minh. Theo Định lý 2.2,
logitP(Y=Y(W,1)|Y∈ {Y(W,0), Y(W,1)}, λ+c¯λW) (B.13)
=α·(λ+c¯λW)⊤¯γZ (B.14)
=α·λ⊤¯γZ+αc·¯λ⊤W¯γZ (B.15)

Do đó, xác suất đầu tiên không đổi vì ¯λ⊤W¯γZ= 0 theo Bổ đề 2.4.

Cũng vậy, theo Định lý 2.2,
logitP(Y=Y(1, Z)|Y∈ {Y(0, Z), Y(1, Z)}, λ+c¯λW) (B.16)
=α·(λ+c¯λW)⊤¯γW (B.17)
=α·λ⊤¯γZ+αc·¯λ⊤W¯γW (B.18)

Do đó, xác suất thứ hai tăng vì ¯λ⊤W¯γW>0 theo Bổ đề 2.4.

B.4. Chứng minh Định lý 3.2

Định lý 3.2 (Thống nhất Biểu diễn). Giả sử rằng, đối với bất kỳ khái niệm W nào, tồn tại các khái niệm {Zi}d−1i=1 sao cho mỗi Zi tách biệt nhân quả với W và {¯γW} ∪ {¯γZi}d−1i=1 là một cơ sở của Rd. Nếu ⟨·,·⟩C là một tích vô hướng nhân quả, thì đồng cấu Riesz ¯γ→ ⟨¯γ,·⟩C, đối với ¯γ∈¯Γ, ánh xạ biểu diễn unembedding ¯γW của mỗi khái niệm W thành biểu diễn embedding ¯λW của nó:
⟨¯γW,·⟩C=¯λ⊤W.

Chứng minh. Tích vô hướng nhân quả định nghĩa đồng cấu Riesz ϕ sao cho ϕ(¯γ) =⟨¯γ,·⟩C. Khi đó, chúng ta có
ϕ(¯γW)(¯γW) =⟨¯γW,¯γW⟩C>0 và ϕ(¯γW)(¯γZ) =⟨¯γW,¯γZ⟩C= 0, (B.19)
trong đó đẳng thức thứ hai theo từ Định nghĩa 3.1. Theo Bổ đề 2.4, ϕ(¯γW) biểu thị biểu diễn unembedding duy nhất ¯λW (đến phép nhân với số dương); cụ thể, ϕ(¯γW) =¯λ⊤W trong đó ¯λ⊤W: ¯γ→¯λ⊤W¯γ.

B.5. Chứng minh Định lý 3.4

Định lý 3.4 (Dạng Tường minh của Tích Vô Hướng Nhân Quả). Giả sử tồn tại một tích vô hướng nhân quả, được biểu diễn là ⟨¯γ,¯γ′⟩C=¯γ⊤M¯γ′ đối với một ma trận đối xứng xác định dương M nào đó. Nếu có các khái niệm tách biệt nhân quả lẫn nhau {Wk}dk=1, sao cho các biểu diễn chính tắc của chúng G= [¯γW1,···,¯γWd] tạo thành một cơ sở cho ¯Γ≃Rd, thì dưới Giả định 3.3,
M−1=GG⊤ và G⊤Cov(γ)−1G=D, (3.2)
đối với một ma trận chéo D nào đó có các mục dương, trong đó γ là unembedding vector của một từ được lấy mẫu đều từ từ vựng.

Chứng minh. Vì ⟨·,·⟩C là một tích vô hướng nhân quả,
0 = ¯γ⊤WM¯γZ (B.20)
đối với bất kỳ khái niệm tách biệt nhân quả W và Z nào. Bằng cách áp dụng (B.20) cho các biểu diễn chính tắc G= [¯γW1,···,¯γWd], chúng ta có
I=G⊤MG. (B.21)

Điều này cho thấy rằng M=G−⊤G−1, chứng minh nửa đầu của (3.2).

Tiếp theo, quan sát rằng M¯γWi là một biểu diễn embedding cho mỗi khái niệm Wi cho i= 1,···, d theo chứng minh của Bổ đề 2.4 và Định lý 3.2. Khi đó, theo Giả định 3.3,
0 = Cov(¯γ⊤WiMγ,¯γ⊤WjMγ) (B.22)
= ¯γ⊤WiMCov(γ)M¯γWj. (B.23)

cho i̸=j. Vậy,
D−1=G⊤MCov(γ)MG, (B.24)
đối với một ma trận chéo D nào đó có các mục dương. Thay thế M=G−⊤G−1, chúng ta có
Cov(γ) =GD−1G⊤, (B.25)
chứng minh nửa thứ hai của (3.2).

Bảng 2. Tên khái niệm, một ví dụ về các cặp phản thực, và số lượng các cặp được sử dụng

# Khái niệm Ví dụ Số lượng
1 verb ⇒3pSg (accept, accepts) 32
2 verb ⇒Ving (add, adding) 31
3 verb ⇒Ved (accept, accepted) 47
4 Ving ⇒3pSg (adding, adds) 27
5 Ving ⇒Ved (adding, added) 34
6 3pSg ⇒Ved (adds, added) 29
7 verb ⇒V + able (accept, acceptable) 6
8 verb ⇒V + er (begin, beginner) 14
9 verb ⇒V + tion (compile, compilation) 8
10 verb ⇒V + ment (agree, agreement) 11
11 adj ⇒un + adj (able, unable) 5
12 adj ⇒adj + ly (according, accordingly) 18
13 small ⇒big (brief, long) 20
14 thing ⇒color (ant, black) 21
15 thing ⇒part (bus, seats) 13
16 country ⇒capital (Austria, Vienna) 15
17 pronoun ⇒possessive (he, his) 4
18 male ⇒female (actor, actress) 11
19 lower ⇒upper (always, Always) 34
20 noun ⇒plural (album, albums) 63
21 adj ⇒comparative (bad, worse) 19
22 adj ⇒superlative (bad, worst) 9
23 frequent ⇒infrequent (bad, terrible) 32
24 English ⇒French (April, avril) 46
25 French ⇒German (ami, Freund) 35
26 French ⇒Spanish (année, año) 35
27 German ⇒Spanish (Arbeit, trabajo) 22

C. Chi tiết Thí nghiệm

Mô hình LLaMA-2 Chúng tôi sử dụng biến thể llama-2-7b của mô hình LLaMA-2 (Touvron et al., 2023), có thể truy cập trực tuyến (với sự cho phép) thông qua thư viện huggingface.7 Bảy tỷ tham số của nó được huấn luyện trước trên hai nghìn tỷ token sentencepiece (Kudo & Richardson, 2018), 90% trong số đó bằng tiếng Anh. Mô hình này sử dụng 32.000 token và 4.096 chiều cho các token embedding của nó.

Các cặp phản thực Tokenization đặt ra một thách thức trong việc sử dụng một số từ nhất định. Đầu tiên, một từ có thể được token hóa thành nhiều hơn một token. Ví dụ, từ "princess" được token hóa thành "prin" + "cess", và γ("princess") không tồn tại. Vì vậy, chúng ta không thể có được ý nghĩa của từ chính xác "princess". Thứ hai, một từ có thể được sử dụng như một trong các token cho từ khác. Ví dụ, các từ tiếng Pháp "bas" và "est" ("down" và "east" trong tiếng Anh) nằm trong các token cho các từ "basalt", "baseline", "basil", "basilica", "basin", "estuary", "estrange", "estoppel", "estival", "esthetics", và "estrogen". Do đó, một từ có thể có ý nghĩa khác ngoài ý nghĩa của từ chính xác đó.

Khi chúng tôi thu thập các cặp phản thực để xác định ¯γW, vấn đề đầu tiên trong cặp có thể được xử lý bằng cách không sử dụng nó. Tuy nhiên, vấn đề thứ hai không thể được xử lý, và nó tạo ra rất nhiều nhiễu cho kết quả của chúng tôi. Bảng 2 trình bày số lượng các cặp phản thực cho mỗi khái niệm và một ví dụ về các cặp. Các cặp cho khái niệm thứ 13, 17, 19, 23-27 được tạo bởi ChatGPT-4 (OpenAI, 2023), và những cặp cho khái niệm thứ 16 dựa trên tệp csv8). Các khái niệm khác dựa trên The Bigger Analogy Test Set (BATS) (Gladkova et al., 2016), phiên bản 3.09, được sử dụng để đánh giá nhiệm vụ word analogy.

Mẫu ngữ cảnh Trong Phần 4, đối với một khái niệm W (ví dụ: Tiếng Anh⇒Tiếng Pháp), chúng tôi chọn một số cặp phản thực (Y(0), Y(1)) (ví dụ: (house, maison)), sau đó lấy mẫu ngữ cảnh {x0j} và {x1j} mà token tiếp theo là Y(0) và Y(1), tương ứng, từ Wikipedia. Những cặp token tiếp theo này được thu thập từ từ điển song ngữ word2word (Choe et al., 2020), là một từ điển dịch từ có sẵn công khai. Chúng tôi lấy tất cả các cặp từ giữa các ngôn ngữ là các tương ứng top-1 cho nhau trong từ điển song ngữ và lọc ra các cặp là token đơn trong từ vựng của mô hình LLaMA-2.

7https://huggingface.co/meta-llama/Llama-2-7b-hf
8https://github.com/jmerullo/lm_vector_arithmetic/blob/main/world_capitals.csv
9https://vecto.space/projects/BATS/

Bảng 3 trình bày số lượng các ngữ cảnh {x0j} và {x1j} cho mỗi khái niệm và một ví dụ về các cặp (Y(0), Y(1)).

Trong thí nghiệm cho khái niệm can thiệp, đối với một khái niệm W, Z, chúng tôi lấy mẫu các văn bản mà Y(0,0) (ví dụ: "king") nên theo sau, thông qua ChatGPT-4. Chúng tôi loại bỏ các ngữ cảnh sao cho Y(0,0) không phải là từ tiếp theo top 1. Bảng 4 trình bày các ngữ cảnh chúng tôi sử dụng.

D. Kết quả Bổ sung

D.1. Biểu đồ của các cặp ngẫu nhiên và phản thực cho tất cả các khái niệm

Trong Hình 7, chúng tôi bao gồm một tương tự của Hình 2 trong đó chúng tôi kiểm tra tích vô hướng nhân quả của sự khác biệt giữa các cặp phản thực và một biểu diễn unembedding được ước tính LOO cho mỗi khái niệm trong số 27 khái niệm. Trong khi hầu hết các khái niệm được mã hóa trong biểu diễn unembedding, một số khái niệm, như thing⇒part, không được mã hóa trong không gian unembedding Γ.

D.2. So sánh với các tích vô hướng Euclidean

Trong Hình 8, chúng tôi cũng vẽ các độ tương tự cosine được tạo ra bởi tích vô hướng Euclidean giữa các biểu diễn unembedding. Thật ngạc nhiên, tích vô hướng Euclidean có phần hoạt động trong mô hình LLaMA-2 vì hầu hết các khái niệm tách biệt nhân quả đều trực giao! Điều này có thể do một số khởi tạo hoặc hiệu ứng điều chỉnh ngầm ưa chuộng việc học unembedding với covariance gần đẳng hướng. Tuy nhiên, tích vô hướng nhân quả được ước tính rõ ràng cải thiện so với tích vô hướng Euclidean. Ví dụ, frequent⇒infrequent (khái niệm 23) có tích vô hướng Euclidean cao với nhiều khái niệm tách biệt, và những điều này nhỏ hơn nhiều đối với tích vô hướng nhân quả. Ngược lại, English⇒French (24) có tích vô hướng Euclidean thấp với các khái niệm ngôn ngữ khác (25-27), nhưng tích vô hướng nhân quả cao với French⇒German và French⇒Spanish (trong khi gần như trực giao với German⇒Spanish, không chia sẻ tiếng Pháp).

Thú vị là, các heatmap tương tự cho mô hình Gemma-2B gần đây hơn (Mesnard et al., 2024) trong Hình 9 minh họa rằng tích vô hướng Euclidean không nắm bắt ngữ nghĩa, trong khi tích vô hướng nhân quả vẫn hoạt động. Một lý do có thể là gốc của các unembedding có ý nghĩa vì mô hình Gemma liên kết các unembedding với các token embedding được sử dụng trước các lớp transformer.

D.3. Kết quả bổ sung từ thí nghiệm đo lường

Chúng tôi bao gồm các tương tự của Hình 4, cụ thể là nơi chúng tôi sử dụng mỗi khái niệm trong số 27 khái niệm như một linear probe trên ngữ cảnh French⇒Spanish (Hình 10) hoặc English⇒French (Hình 11).

D.4. Kết quả bổ sung từ thí nghiệm can thiệp

Trong Hình 12, chúng tôi bao gồm một tương tự của Hình 5 trong đó chúng tôi thêm biểu diễn embedding α¯λC (4.1) cho mỗi khái niệm trong số 27 khái niệm vào λ(xj) và xem sự thay đổi trong logit.

D.5. Bảng bổ sung của top-5 từ sau can thiệp

Bảng 5 và Bảng 6 là các tương tự của Bảng 1 trong đó chúng tôi sử dụng các ngữ cảnh khác nhau x="In a monarchy, the ruler usually is a " và x="The prince matured and eventually became the ". Đối với ví dụ đầu tiên, lưu ý rằng "r" và "em" là các token tiền tố cho các từ liên quan đến hoàng gia, như "ruler", "royal", và "emperor". Đối với ví dụ thứ hai, ngay cả khi từ mục tiêu "queen " không trở thành từ có khả năng nhất, các từ có khả năng nhất vẫn phản ánh hướng khái niệm ("woman", " queen ", "her", "female").

D.6. Kiểm tra tính hợp lý cho tích vô hướng nhân quả được ước tính

Trong các thí nghiệm trước đó, chúng tôi thấy rằng lựa chọn M= Cov(γ)−1 từ (3.3) tạo ra một tích vô hướng nhân quả và tạo ra một biểu diễn embedding ¯λW dưới dạng (4.1). Ở đây, chúng tôi chạy một thí nghiệm kiểm tra tính hợp lý trong đó chúng tôi xác minh rằng biểu diễn embedding được tạo ra thỏa mãn điều kiện không tương quan trong Giả định D.6. Trong Hình 13, chúng tôi chỉ ra thực nghiệm rằng ¯λ⊤Wγ và ¯λ⊤Zγ không tương quan đối với các khái niệm tách biệt nhân quả (biểu đồ trái), trong khi chúng tương quan đối với các khái niệm không tách biệt nhân quả (biểu đồ phải). Trong những biểu đồ này, mỗi điểm tương ứng với điểm (¯λ⊤Wγ,¯λ⊤Zγ), trong đó γ là một unembedding vector γ tương ứng với mỗi token trong từ vựng LLaMA-2 (tổng cộng 32K).

Bảng 3. Các khái niệm được sử dụng để điều tra khái niệm đo lường

Khái niệm Ví dụ Số lượng
English ⇒French (house, maison) (209, 231)
French ⇒German (déjà, bereits) (278, 205)
French ⇒Spanish (musique, música) (218, 214)
German ⇒Spanish (Krieg, guerra) (214, 213)

Bảng 4. Ngữ cảnh được sử dụng để điều tra khái niệm can thiệp

j xj
1 Long live the
2 The lion is the
3 In the hierarchy of medieval society, the highest rank was the
4 Arthur was a legendary
5 He was known as the warrior
6 In a monarchy, the ruler is usually a
7 He sat on the throne, the
8 A sovereign ruler in a monarchy is often a
9 His domain was vast, for he was a
10 The lion, in many cultures, is considered the
11 He wore a crown, signifying he was the
12 A male sovereign who reigns over a kingdom is a
13 Every kingdom has its ruler, typically a
14 The prince matured and eventually became the
15 In the deck of cards, alongside the queen is the

Bảng 5. Ngữ cảnh: "The prince matured and eventually became the "

Thứ hạng α=0 0.1 0.2 0.3 0.4
1 king king em queen queen
2 em em r em woman
3 leader r leader r lady
4 r leader king leader wife
5 King head queen woman em

Bảng 6. Ngữ cảnh: "In a monarchy, the ruler is usually a "

Thứ hạng α=0 0.1 0.2 0.3 0.4
1 king king her woman woman
2 monarch monarch monarch queen queen
3 member her member her female
4 her member woman monarch her
5 person person queen member member
