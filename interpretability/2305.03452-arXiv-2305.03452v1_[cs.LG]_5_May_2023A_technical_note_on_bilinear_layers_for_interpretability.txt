# 2305.03452.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/interpretability/2305.03452.pdf
# File size: 185495 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2305.03452v1  [cs.LG]  5 May 2023A technical note on bilinear layers for interpretability
Lee Sharkey
{lee@conjecture.dev, leedsharkey@gmail.com}
Conjecture
Abstract
The ability of neural networks to represent more features th an neurons makes inter-
preting them challenging. This phenomenon, known as superp osition [Olah et al.,
2020, Elhage et al., 2022b], has spurred efforts to ﬁnd archi tectures that are more
interpretable than standard multilayer perceptrons (MLPs ) with elementwise acti-
vation functions. In this note, I examine bilinear layers [S hazeer, 2020], which are
a type of MLP layer that are mathematically much easier to ana lyze while simulta-
neously performing better than standard MLPs. Although the y are nonlinear func-
tions of their input, I demonstrate that bilinear layers can be expressed using only
linear operations and third order tensors. We can integrate this expression for bi-
linear layers into a mathematical framework for transforme r circuits [Elhage et al.,
2021], which was previously limited to attention-only tran sformers. These results
suggest that bilinear layers are easier to analyze mathemat ically than current archi-
tectures and thus may lend themselves to deeper safety insig hts by allowing us to
talk more formally about circuits in neural networks. Addit ionally, bilinear layers
may offer an alternative path for mechanistic interpretabi lity through understand-
ing the mechanisms of feature construction instead of enumerating a (potentially
exponentially) large number of features in large models.
1 Introduction
Neural networks can learn to compute interesting and compli cated functions. To a ﬁrst approxima-
tion, these functions appear to be structured such that part icular computational roles or representa-
tions are assigned to particular directions in neural activ ation space [Olah et al., 2020]. We call these
representations features . Somewhat surprisingly, neural networks are believed to be able to represent
more features than they have neurons [Elhage et al., 2022b, G urnee et al., 2023]. This phenomenon
is known as superposition , since they assign features to non-orthogonal directions w hich ‘overlap’
in high-dimensional space.
We are particularly interested in mechanistically underst anding large language models that use the
transformer architecture [Vaswani et al., 2017]. This arch itecture mostly consists of a series of al-
ternating attention layers (which let activations at diffe rent points in a sequence interact with each
other) and MLP layers (which, at each point in the sequence, c onstruct useful output features that are
nonlinear transformations of the input features). About tw o thirds of the parameters in these models
are in the MLP layers, which are thought to make prodigious us e of superposition [Elhage et al.,
2022a, Gurnee et al., 2023].
Nonlinear elementwise activation functions (such as ReLU [ Nair and Hinton, 2010] or GeLU
[Hendrycks and Gimpel, 2020]) in MLP layers can remove small amounts of interference between
non-orthogonal features [Elhage et al., 2022b], thus makin g it possible for layers to represent fea-
tures in superposition without increasing the loss. Unfort unately, while the activation function is
very useful for the performance of neural networks, it makes it quite difﬁcult to analyze MLPs
mathematically because the powerful tools of linear algebr a can no longer be readily applied.
Work in progress

--- PAGE 2 ---
However, it turns out that another kind of MLP layer, the bili near layer [Shazeer, 2020,
Dauphin et al., 2016, Mnih and Hinton, 2007], is much easier t o analyze than MLPs with elemen-
twise activation functions. Even though bilinear layers ar e nonlinear functions of the input vector,
bilinear layers can be described using only linear operatio ns and third order tensors ! This nice
property lets us extend ‘A Mathematical Framework for Transformer Circuits ’ [Elhage et al.,
2021] to transformers with MLP layers as well as attention , not just attention-only transformers.
We hope that this simple change will give us a ﬁrmer analytica l footing to understand large models
on a deep, mechanistic level. This might eventually let us ma ke deeper claims about their safety,
since it could permit us to describe classes of circuits as ma thematical objects with certain proper-
ties (as induction heads were in Elhage et al. [2021]) and to a nalyze learning dynamics and predict
the emergence of particular kinds of circuits.
It has been hypothesized (though not yet observed) that neur al networks might represent a number of
features that is exponential in the number of neurons in a lay er [Elhage et al., 2022b]. If this is true,
it would not bode well for our ability to mechanistically und erstand large neural networks, which
in a sense relies on our being able to enumerate all their feat ures. However, as discussed in the last
section of this work, bilinear layers may offer a potential alternative path to ‘enumerative safety’
[Elhage et al., 2022b]. Instead of attempting to understand each of a large number of features, with
bilinear networks we may be able to understand a smaller numb er of primitive features that bilinear
layers use to ‘construct’ their (potentially exponential) larger number of features. Thus, in the same
way that we might be able to understand an exponentially larg e number of executed programs by un-
derstanding their code, we might be able to understand an exp onentially large number of features by
understanding the process by which features with certain pr operties are constructed. Here, we make
some preliminary steps toward understanding the mechanism s of feature construction in bilinear
layers; we show that in bilinear layers, output features are constructed through sums of pairwise
interactions between input features , whereas, in standard MLPs, output features are constructe d
using all-to-all interactions between input features that appear not to be decomposable.
2 Bilinear layers
2.1 Introducing bilinear layers
A standard MLP layer consist of an input vector x, a weight matrix Wand an elementwise nonlinear
activation function, σsuch as the ReLU function (and an optional bias term which is o mitted for
notational simplicity). The input vector is linearly trans formed by the weight matrix to yield the
pre-activation vector Wx, to which the activation function is applied elementwise:
MLPReLU(x) =σ(Wx)
Bilinear layers are slightly different. They take the form
MLPBilinear(x) = (W1x)⊙(W2x),
where⊙denotes elementwise multiplication. They have two weight m atrices, which each separately
transform the input vector. They were introduced in differe nt forms by Dauphin et al. [2016] and
Mnih and Hinton [2007]. They were later studied by Shazeer [2 020], who showed that bilinear
layers, when used as the MLP layer in transformer language mo dels, are surprisingly competitive1:
They are at least as performant per parameter than standard M LPs with ReLU or GELU activation
functions and only slightly less performant than state-of- the-art SwiGLU layers2.
2.2 Describing bilinear layers using only linear operation s and third order tensors
The lack of an elementwise activation function in bilinear l ayers makes them mathematically very
simple. In fact, despite being nonlinear functions of x, they can be expressed using only linear
operations and third order tensors.
1At least for the model size they explored, which was approxim ately 120M parameters, a similar size to
GPT2-small [Radford et al., 2019]. To my knowledge, it remai ns to be determined whether bilinear layers
continue to perform competitively at larger scales.
2A SwiGLU layer is equivalent to a bilinear layer but where an e lementwise Swish activation function
[Ramachandran et al., 2017] is applied to W1x.
2

--- PAGE 3 ---
First, we’ll deﬁne the tensor inner product (See appendix A for some examples of tensor inner
products which may help build intuitions). Unlike the inner product between vectors, the tensor
inner product needs to deﬁne the axes along which the inner pr oduct is taken. The tensor inner
product is thus deﬁned as
U(n)·jkV(m)=T(n+m−2)
where
Tγ1···γj−1γj+1···γnγ′
1···γ′
k−1γ′
k+1···γ′m=/summationdisplay
βUγ1···γj−1βγj+1···γnVγ′
1···γ′
k−1βγ′
k+1···γ′m(1)
For the tensor inner product between nthorder tensor UandmthorderVto be deﬁned, the dimen-
sion of axis jof tensor Umust be the same dimension as axis kof tensor V.
Now we show how bilinear layers can be expressed using linear operations and third order tensors.
Suppose we want to ﬁnd the third order tensor Bsuch that
(W1x)⊙(W2x) =x·12B·21x,
if it exists. We’ll ﬁrst identify the terms in the vector on th e right hand side,
((W1x)⊙(W2x))i= (/summationdisplay
jW1(ij)xj)(/summationdisplay
kW2(ik)xk)
=/summationdisplay
j/summationdisplay
kW1(ij)xjW2(ik)xk(2)
Now let’s express the terms of the third order tensor Busing tensor inner products. We have,
(x·12B·21x)i=/summationdisplay
jxj/summationdisplay
kxkBijk
=/summationdisplay
kxk/summationdisplay
jxjBijk
=/summationdisplay
j/summationdisplay
kxjxkBijk.(3)
Note that it doesn’t matter whether we take the tensor inner p roduct between Bandxon the2nd or
3rd axis ﬁrst, which is why x·12B·21xis associative, i.e. (x·12B)·21x=x·12(B·21x). We’ll use
this property when extending a Mathematical Framework for T ransformer Circuits [Elhage et al.,
2021] (Section 2.3).
Comparing the terms from equations 2 and 3, we can see they are equal ifBijk=W1(ij)W2(ik).
Thus, we can construct the tensor Busing the bilinear layer weights W1,W2∈Rm×nand a third
order tensor Zsuch that Zijk= 1wherei=j=kand0otherwise, because B=W1·12Z·21W2.
One helpful way to think about the m×n×ntensorBis that the column vector B:jkconsists of
the elementwise multiplication of the jthcolumn of W1with thekthcolumn of W2.
2.3 Extending a Mathematical Framework for Transformer Cir cuits
When Elhage et al. [2021] analyzed the equations for 1- and 2- layer attention-only transformers, it
offered interesting insights on the structure of these mode ls. It helped to reveal QK- and OV-circuits,
induction heads, and virtual attention heads, which formed the basis of much interesting follow-up
work in interpretability [Olsson et al., 2022, Wang et al., 2 022].
However, one of the biggest shortcomings of Elhage et al. [20 21] was that the transformers they
analyzed had no MLP layers. MLPs comprise around two thirds o f all parameters in standard trans-
former language models and are thought to be be necessary for a great deal of interesting behaviour
[Geva et al., 2021]. The reason MLPs were excluded was that th ey could not be linearised, which
made their analysis intractable. But, as we’ve seen, it is po ssible to describe bilinear layers using
3

--- PAGE 4 ---
only linear operations. This means we can write linearized e xpressions for transformers with both at-
tention and MLP layers! It’s important to stress that the MLP s we achieve this with are close to state
of the art [Shazeer, 2020]. This opens up the possibility tha t we may be able to formally analyze
some very capable language models. In this section, we’ll id entify the expression for a one-layer
transformer with attention and (bilinear) MLPs. The expres sions for two- and N-layer transformers
are left as lengthy exercises for the reader.
We’ll update our notation in order to be consistent with Elha ge et al. [2021], with which we expect
readers to be familiar. The inputs to the language model is a s equence of tokens tof length ncontext .
These are embedded by the dmodel×nvocab embedding matrix WE. The token embeddings x0=
WEt(which have shape ncontext×dmodel ) become the residual stream, which is passed through
multiple residual blocks, each consisting of a multihead at tention layer and an MLP layer, and each
added back into the residual stream. Finally, the residual s tream is unembedded by the unembedding
matrixWUto make the token logits.
In Elhage et al. [2021], they assumed MLPs that had an element wise GeLU activation function,
which are very difﬁcult to analyze. Here, we’ll instead use b ilinear layers. Deﬁne the bilinear MLP
layer as
F(x) =Wm
O(x·12Wm
I1·12Z·21Wm
I2·21x) (4)
whereWm
Ois thedmodel×dmlpoutput weight matrix for the MLP layer and Wm
I1,Wm
I2are the two
dmlp×dmodel input weight matrices for the bilinear layer.
Using the path expansion trick described by Elhage et al. [20 21], the input to the MLP in a one layer
transformer can be described as
x1= (Id+/summationdisplay
h∈HAh⊗Wh
OV)·WEt
= (WE+/summationdisplay
h∈HAh⊗Wh
OVWE)t(5)
whereWh
OV=Wh
OWh
VandAh=softmax*(tT·WT
EWQKWE·t)in which softmax* is the softmax
function with autoregressive masking and WQK=Wh⊤
QWh
K. Putting our deﬁnition of x1into our
deﬁnition of F(·)we get
F(x1) =Wm
O(((WE+/summationdisplay
h∈HAh⊗Wh
OVWE)t)·12Wm
I1·12Z·21Wm
I2·21
((WE+/summationdisplay
h∈HAh⊗Wh
OVWE)t))(6)
Note that for arbitrary matrices M,M′, it’s true that M·12M′=M⊤M′⊤. So we transpose the
left hand bracket and Wm
I1and move the weight matrix into the brackets:
=Wm
O((t⊤(W⊤
EWm⊤
I1+/summationdisplay
h∈HAh⊗W⊤
EWh⊤
OVWm⊤
I1))·12Z·21Wm
I2·21
((WE+/summationdisplay
h∈HAh⊗Wh
OVWE)t))(7)
And next, noting that M·21M′=MM′, we move Wm
I2into the right hand brackets:
=Wm
O((t⊤(W⊤
EWm⊤
I1+/summationdisplay
h∈HAh⊗W⊤
EWh⊤
OVWm⊤
I1))·12Z·21
((Wm
I2WE+/summationdisplay
h∈HAh⊗Wm
I2Wh
OVWE)t))(8)
Next, we move the Ztensor into the left hand brackets
4

--- PAGE 5 ---
=Wm
O((t⊤(W⊤
EWm⊤
I1·12Z+/summationdisplay
h∈HAh⊗W⊤
EWh⊤
OVWm⊤
I1·12Z))·21
((Wm
I2WE+/summationdisplay
h∈HAh⊗Wm
I2Wh
OVWE)t))(9)
And combining both the left hand and right hand brackets, we g et the expression for a bilinear
feedforward layer
=Wm
O(t⊤(
W⊤
EWm⊤
I1·12Z·21Wm
I2WE+
/summationdisplay
h∈HAh⊗(W⊤
EWh⊤
OVWm⊤
I1·12Z·21Wm
I2WE)+
/summationdisplay
h∈HAh⊗(W⊤
EWm⊤
I1·12Z·21Wm
I2Wh⊤
OVWE)+
/summationdisplay
h∈H/summationdisplay
h′∈HAhAh′⊗(W⊤
EWh⊤
OVWm⊤
I1·12Z·21Wm
I2Wh′⊤
OVWE)
)t)(10)
We can analyze each of the terms in this equation. The ﬁrst summand expresses a direct path
from the token embedding matrix straight to the MLP without p assing through any attention heads.
The second summand expresses the components of the token embeddings that pass t hrough the
attention head and then pass into only the ﬁrst MLP input matr ix. The third summand is similar, but
the embeddings pass through the attention heads and into the second MLP input matrix. The last
summand corresponds to token embeddings that pass through the atten tion heads and then into both
the ﬁrst and second MLP input matrices.
With this expression for the MLP layer, we can now express the path expansion for the full one
layer transformer, which is simply the above expression for F(x)added to the token embedding-
unembedding pathway (the ‘direct pathway’) and the pathways through the attention heads :
T(t) =(Id⊗WUWE)t+
/summationdisplay
h∈H(Ah⊗WUWh
OVWE)t+
Wm
O(t⊤(
W⊤
EWm⊤
I1·12Z·21Wm
I2WE+
/summationdisplay
h∈HAh⊗(W⊤
EWh⊤
OVWm⊤
I1·12Z·21Wm
I2WE)+
/summationdisplay
h∈HAh⊗(W⊤
EWm⊤
I1·12Z·21Wm
I2Wh⊤
OVWE)+
/summationdisplay
h∈H/summationdisplay
h′∈HAhAh′⊗
(W⊤
EWh⊤
OVWm⊤
I1·12Z·21Wm
I2Wh′⊤
OVWE)
)t)(11)
3 Understanding feature construction in bilinear layers
One of the problems we may face when trying to mechanisticall y understand neural networks is
that they may be able to represent an exponential number of fe atures. If this hypothesis resolves
5

--- PAGE 6 ---
true, then enumerating all the features in large networks ma y become computationally intractable.
One analogy that gives us hope is discussed by Olah [2022]: Ev en though the input space to a
particular computer program might be exponentially large, we can still say that we understand that
exponentially large space of executed programs if we unders tand its code. In the same way, if we
can understand the process by which features with certain pr operties are constructed from simpler
primitives, we may be able to overcome the issue of having to u nderstand an exponential number of
features. In this section, which is more speculative than ea rlier sections, I outline why this hopeful
vision seems very hard to realise in standard MLPs, but seems quite possible in bilinear layers.
3.1 Feature construction in standard MLPs is non-decomposa ble
Suppose we have a standard MLP layer MLPReLU(x) =σ(Wx)with a ReLU activation σ(where
the bias term is omitted). Also suppose that the input vector x∈Xconsists of sparse linear com-
binations of input features x=DI⊤aI, whereDIis a dictionary of input features represented as
anfeatures×dinputmatrix and aI∈AIis a sparse vector of coefﬁcients (with values in [0,∞)of
sizenfeatures ) such that the dataset Xcan be reconstructed from the features and their coefﬁcient s,
X=DI⊤AI. Similarly suppose there is a dictionary of output features for this layer DOand
that sparse linear combinations of those output features de scribe the activations observed in a large
representative sample from px(MLPReLU(x)), i.e.
MLPReLU(x) =σ(Wx) =σ(W(DI⊤aI)) =DO⊤aO(12)
Therefore DIandDOare overcomplete bases3for the input space Xand output space
MLPReLU(X)respectively.
One way to view the process of feature construction is to say t hat output features DOare all im-
plicitly represented in superposition in the weight matrix Wand that the nonlinearity, when applied
elementwise to the preactivation vector Wx, modiﬁes a set of default output features in order to
select particular output features. One candidate for the de fault output features are the left singular
vectors of W, i.e. the columns of a matrix U(We’ll discuss other candidates in the next section).
We can thus introduce a modiﬁer vector m(x)that is a function of xsuch that
MLPReLU(x) =m(x)⊙Wx= (m(x)⊙U)ΣV⊤x=DO⊤aO.
Therefore we can view linear combinations of the output feat ures (namely DO⊤aO) as consisting of
linear combinations of modiﬁed default output features (na mely(m(x)⊙U)ΣV⊤x).
With a ReLU activation function, m(x)is binary vector of ones and zero: m(x)i= 1 where
σ(Wx)i>0andm(x)i= 0 otherwise. In general, for vanilla MLPs with any elementwis e ac-
tivation function σ:
m(x)i=σ(Wx)i
(Wx)i4(13)
It is the modiﬁer vector that ‘selects’ from the features rep resented in superposition in W, or, equiva-
lently, ‘contructs’ them by modifying the default output fe atures. If we could understand how m(x)
is computed in terms of input features DI, then we could begin to understand why particular output
featuresDOare constructed not others. Unfortunately, in vanilla MLPs , the only way to calculate
the value of m(x)in general is Equation 13. In other words, to get the value of t he modiﬁer vec-
tor, we ﬁrst have to pass the input through the network to obse rve what the post-activations (the
numerator) and pre-activations are (the denominator) to ge tm(x). But this is circular: We would
have to already understand the nonlinear computation in the numerator in order to understand how
output features are constructed. This framing doesn’t simp lify anything at all! Feature construction
in standard MLPs can thus be considered ‘non-decomposable’ .
3.2 Feature construction in bilinear layers
In mechanistic interpretability, one of the major assumpti ons that we need to make is that we can
interpret linear transformations of almost arbitrary dime nsionality. They may still be large objects,
3In linear algebra, a basis of a vector space is a set of vectors from which every vector in that space can be
expressed as a linear combination. An overcomplete basis is a basis where at least one element of the basis set
can be removed yet the set remains a basis.
4Note that m(x)iis discontinuous at (Wx)i= 0.
6

--- PAGE 7 ---
but linear transformations are as simple as transformation s get. For large linear transformations with
non-sparse coefﬁcients, we may have to spend more time study ing them or prioritize analysis of the
largest coefﬁcients. But overall we assume that we can under stand them to a satisfying extent. If we
can’t, then the whole business of mechanistic intepretabil ity would be doomed even for large linear
regressions, never mind deep neural networks.
Granting this assumption, if we could describe the modiﬁer v ectorm(x)in the previous section as
a linear function of input features (instead of a nonlinear o ne), then we could begin to understand
how a layer constructs output features. Fortunately, in bil inear layers the modiﬁer vector is a linear
function of the input!
MLPBilinear(x) =m(x)⊙(W2x) where m(x) =W1x,
We’ll say that the modiﬁer vector modiﬁes the default output features represented in W2to construct
output features.
We still need to deﬁne what the default output feature direct ions and the modiﬁer feature directions
are concretely. Ultimately this choice will always be somew hat arbitrary because linear transforma-
tions do not imply any particular privileged basis. As befor e, perhaps the most obvious candidates
for the default output feature directions are the left singu lar vectors of W2. But the largest directions
in the output activations may not necessarily have a strong r elationship with the weights because
the output directions depend on both the weights and the inpu t directions. Therefore, we may be
able to do better than the left singular vectors of W2by incorporating the data distribution into the
choice of bases. One way might use the right singular vectors MLPBilinear(X)orW2X. Another
– perhaps better – way is to identify default output features that are maximally statistically indepen-
dent. This may be better because statistically independent directions tend to be activated somewhat
sparsely and therefore might be better default output featu res than singular vectors, since fewer
will be signiﬁcantly ‘activated’ at any one time. We could ac hieve this by performing linear ICA
Hyvärinen and Oja [2000] on the preactivations W2X. This would yield a matrix U(2), which is
the set of vectors that are maximally statistically indepen dent directions of the output dataset while
still being a basis of it. We can then use multiple linear regr ession to ﬁnd the corresponding matrix
V(2)⊤such that W2=U(2)V(2)⊤. Slightly abusing terminology, we’ll call U(2)andV(2)⊤the left
and right independent components of W2respectively. We can deﬁne the modiﬁer features using the
same procedure, identifying the left and right independent components of W1=U(1)V(1)⊤.
Armed with such features, we may be able to describe feature construction in bilinear n etworks
in terms of interactions between two relatively small, rela tively sparse sets of vectors (the de-
fault output features and the modiﬁer features) . We hope we can use this approach to tell mech-
anistic stories for how features with certain properties ar e constructed by the network. We might
be able to do this by understanding the functional propertie s of the default output features and how
modiﬁer features tend to modify them. Optimistically, mech anistic stories like these may let us un-
derstand an exponentially large space of features. Whether or not such an approach will work is
ultimately an empirical question, which we leave for future work. In the next section, we explore
the mathematical simplicity of feature construction in bil inear layers, which gives us some reason to
suspect that feature construction may be understandable.5
5We can make further modiﬁcations to the modiﬁer features and default output features that assist either the
intuitiveness or interpretability of bilinear networks. I ’ll note them here but won’t explore them further in this
work.
Improving intuitiveness : If, during training, we constrain W1xto be low L2norm and add the one vector
as bias, the modiﬁer vector would always be close to the one ve ctor. In other words: m(x) =W1x+1
where||W1x|| ≈0. This would mean that modiﬁer features simply cause slight m odiﬁcations of default output
features. This addition would also help us make a analysis pr ioritization decisions later (see section 3.4), but
fundamentally the modiﬁcation isn’t necessary. This addit ion also opens up an experimental avenue (which we
won’t explore here): By imposing more or less regularizatio n on the norm, it allows us to control the amount
of superposition a network is able to do. This would be an inte resting experimental lever to pull, since it would
allow us to directly test how much a network’s performance is due to superposition.
Improving interpretability : We could choose an L1penalty for the norm constraint on the modiﬁer vector
(instead of the L2norm); or we could constrain W1to be low rank; alternatively, we could quantize the output
ofW1xin order to put hard limits on the amount of superposition a ne twork can do.
7

--- PAGE 8 ---
3.3 Feature construction in bilinear layers decompose into a sum of pairwise interactions
between input features
Not all layers have the same ‘amount’ of nonlinearity. Some a re more nonlinear than others. Here
we characterize the amount of nonlinearity layers can have, which sheds light on how bilinear layers
differ from standard MLPs.
LetC(dI
i,dO
j,aI)quantify the contribution of input feature dI
i∈DIto the activation (or ‘selection’)
of output feature dO
j∈DO. We then have the following (non-comprehensive) set of degr ees of
nonlinearity.
•Linear : Fully linear layers have no nonlinearity. There are theref ore no interactions be-
tween input features during feature construction (since th ere is no modiﬁer vector). The
amount that input feature dI
icontributes to the selection of output feature dO
jis quanti-
ﬁed simply as C(dI
i,dO
j,aI) = [WdI
iaI
i]⊤dO
j, which is just the inner product between the
preactivation caused by that input feature and the output fe ature.
•Additively pairwise nonlinear : In this case, output features are determined by a sum of
pairwise interactions between features. For example, if in put features dI
1,dI
2,dI
3are active
in the input, the contribution of dI
i(wherei∈ {1,2,3}) to each output feature can be
described as a sum of pairwise nonlinear interactions, C(dI
i,dO
j,aI) = [f(dI
i;dI
1,aI
1) +
f(dI
i;dI
2,aI
2)+f(dI
i;dI
3,aI
3)]⊤dO
j, wheref(·)is some nonlinear function of the two inter-
acting features.
•Fully nonlinear : The contribution an input feature makes to the selection of an output
feature depends on every other feature in a way that can’t be d ecomposed into a sum. The
contribution of dI
ito each output feature can only be described as an all-to-all nonlinear
interaction between input features that cannot be broken do wn into linear components:
C(dI
i,dO
j,aI) =g(dI
i;dI
1,dI
2,dI
3,aI)⊤dO
j, whereg(·)is some (non-additively-pairwise)
nonlinear function.
The task of understanding additively pairwise nonlinearit y is easier than full nonlinearity because
we can study each pairwise interaction between features and sum them up. Understanding full
nonlinearity is signiﬁcantly harder because there is no way to linearly decompose the function g.
Sadly, standard MLPs are fully nonlinear. However, we show t hat bilinear layers are additively
pairwise nonlinear, making them signiﬁcantly easier to ana lyze.
Suppose the input to a bilinear layer x′consists of a linear combination of two input features dI
1and
dI
2, i.e.x′=a1dI
1+a2dI
2. Using the re-expression of the bilinear layer, inputting x′into equation
2 yields
(a1d1+a2d2)·12B·21(a1d1+a2d2) =
a1d1·12B·21a1d1+
a1d1·12B·21a2d2+
a2d2·12B·21a1d1+
a2d2·12B·21a2d2
(14)
More generally, for arbitrary linear combinations of input features:
(W1x)⊙(W2x) = (/summationdisplay
i∈Raidi)·12B·21(/summationdisplay
i∈Raidi) =/summationdisplay
i∈R/summationdisplay
j∈Raiajdi·12B·21dj(15)
whereRis the set of indices of nonzero feature coefﬁcients. Equati on 15 shows that, although all
features interact to determine the output features, these i nteractions can be understood as a sum of
pairwise interactions between features. Hence bilinear la yers are only additively pairwise nonlinear.
We hope that this simplicity can be leveraged to tell simple s tories about how particular input features
(hopefully sparsely) activate particular default output f eatures and modiﬁer features. Then, if we
understand the functional properties of those default outp ut features and the kinds of functional
modiﬁcations that those modiﬁer features make, then we may b e able to understand the properties
of the output features.
8

--- PAGE 9 ---
3.4 How should we study feature construction?
At this early stage, it’s not totally clear how best to analyz e the structure of bilinear networks. What
is clear is that doing so will be easier than analyzing fully n onlinear computations, since we’re simply
studying the structure of tensors, which is a relatively wel l understood domain in mathematics. In
advance of empirical results, I speculate on a few non-mutua lly exclusive ways to proceed in this
section.
1.Large coefﬁcients of B: As discussed at the beginning of section 2, when interpreti ng
any linear transformation, there may be so many coefﬁcients that it may be necessary to
prioritize our analyses by studying only the largest coefﬁc ients. One way to leverage this
is simply to study the largest coefﬁcients of Band how they would inﬂuence interactions
between commonly observed pairs or groups of input features .
2.Tensor decomposition : Building on (1), we could perform Higher Order Singular Val ue
Decomposition (HOSVD) and study the structure of the most in ﬂuential ranks of the tensor.
3.Maximally modiﬁed default output features : Recall that one way to view the bilinear
network is that one side of the elementwise multiplication m odiﬁes the linear transforma-
tion on the other side. This suggests an way to prioritize the analysis of how particular
features are constructed: For each input feature, we should prioritize analysis of the most
modiﬁed default output features. Concretely, deﬁne
U(2,di):=d⊤
iW1·12Z·21U(2).
This is the set of output features caused by the modiﬁcations that input feature dimakes to
default output feature U(2). Then, for each input feature diwe should study the top k most
modiﬁed default output features, i.e.
arg top-k
l(||U(2)
:,l−U(2,di)
:,l||) (16)
This would let us focus on the most signiﬁcant modiﬁcations t hat a given input feature
makes to the default output features. But we can prioritize o ur analyses further than that.
The modiﬁcations that an input feature makes to the default o utput features don’t matter
unless the default output feature is actually activated by t hat feature or some other feature
that is simultaneously present in x. Therefore we can identify pairs of features, (dl,dm)
that are correlated (or that have often appeared at the same t ime) and where U(2,dl)is both
one of the default output features that is most modiﬁed by dmand simultaneously one of
the default output features that is most activated by dm.
4 Conclusion
The simplicity of bilinear layers makes formal analysis muc h easier than for standard MLPs. One
of the most important things bilinear layers give us are anal ysable expressions for performant trans-
formers with both attention heads and MLP layers. I hope that this will eventually let us formally
analyze the structure of the representations of large langu age models in this class. This might reveal
interesting features and circuits in a similar way that the m athematical framework for attention-only
transformers introduced by Elhage et al. [2021] helped to re veal reveal QK- and OV-circuits, induc-
tion heads, and virtual attention heads. Curiosity aside, a n expression for models with bilinear layers
may let us make stronger claims about safety. For instance, i t may let us more directly compare cir-
cuit structure in different models, and enable us to make inf erences about model behaviour without
necessarily running the model.
Another potential research direction is analyzing learnin g dynamics. Models with bilinear layers
seem like they might lend themselves to mathematical analys is in a similar fashion to the deep
linear layers studied by Saxe et al. [2013]. Learning dynami cs may be important for safety, since
understanding them may be necessary to be able to predict dan gerous model behaviors before they
emerge.
Lastly, and most speculatively, bilinear layers offer the p otential to understand the mechanisms of
feature construction, which may be necessary for understan ding a potentially exponentially large
number of features represented in language models. There is still much empirical work to do to
9

--- PAGE 10 ---
evaluate whether intuiting the mechanisms of feature const ruction is possible. Overall, I hope that
this note might pique the interest of the interpretability c ommunity by highlighting an architecture
that is much gentler on the intuitions than standard MLPs.
Acknowledgements
I thank Trenton Bricken for helpful discussions that initia ted my search for layers that could be
described in terms of higher order tensors. I thank Beren Mil lidge, Sid Black, and Dan Braun for
helpful discussions and detailed feedback on this work.
References
Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangie r. Language mod-
eling with gated convolutional networks. CoRR , abs/1612.08083, 2016. URL
http://arxiv.org/abs/1612.08083 .
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,
Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasS arma, Dawn Drain, Deep
Ganguli, Zac Hatﬁeld-Dodds, Danny Hernandez, Andy Jones, J ackson Kernion, Liane Lovitt, Ka-
mal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kapl an, Sam McCandlish, and Chris
Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread , 2021.
https://transformer-circuits.pub/2021/framework/ind ex.html.
Nelson Elhage, Tristan Hume, Catherine Olsson, Neel Nanda, Tom Henighan, Scott Johnston, Sheer
ElShowk, Nicholas Joseph, Nova DasSarma, Ben Mann, Danny He rnandez, Amanda Askell, Ka-
mal Ndousse, , Jones, Dawn Drain, Anna Chen, Yuntao Bai, Deep Ganguli, Liane Lovitt, Zac
Hatﬁeld-Dodds, Jackson Kernion, Tom Conerly, Shauna Krave c, Stanislav Fort, Saurav Kada-
vath, Josh Jacobson, Eli Tran-Johnson, Jared Kaplan, Jack C lark, Tom Brown, Sam McCandlish,
Dario Amodei, and Christopher Olah. Softmax linear units. Transformer Circuits Thread , 2022a.
https://transformer-circuits.pub/2022/solu/index.ht ml.
Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Sc hiefer, Tom Henighan, Shauna
Kravec, Zac Hatﬁeld-Dodds, Robert Lasenby, Dawn Drain, Car ol Chen, Roger Grosse,
Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenb erg, and Christopher Olah.
Toy models of superposition. Transformer Circuits Thread , 2022b. https://transformer-
circuits.pub/2022/toy_model/index.html.
Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Tra nsformer feed-forward layers are
key-value memories, 2021.
Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dm itrii Troitskii, and Dimitris Bertsi-
mas. Finding neurons in a haystack: Case studies with sparse probing, 2023.
Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2020.
A. Hyvärinen and E. Oja. Independent component analysis: al gorithms
and applications. Neural Networks , 13(4):411–430, 2000. ISSN
0893-6080. doi: https://doi.org/10.1016/S0893-6080(00 )00026-5. URL
https://www.sciencedirect.com/science/article/pii/S 0893608000000265 .
Andriy Mnih and Geoffrey Hinton. Three new graphical models for statistical language
modelling. In Proceedings of the 24th International Conference on Machin e Learn-
ing, ICML ’07, page 641–648, New York, NY , USA, 2007. Associatio n for Com-
puting Machinery. ISBN 9781595937933. doi: 10.1145/12734 96.1273577. URL
https://doi.org/10.1145/1273496.1273577 .
Vinod Nair and Geoffrey E. Hinton. Rectiﬁed linear units imp rove restricted boltzmann machines. In
Proceedings of the 27th International Conference on Intern ational Conference on Machine Learn-
ing, ICML’10, page 807–814, Madison, WI, USA, 2010. Omnipress. ISBN 9781605589077.
Chris Olah. Mechanistic interpretability, variables, and the importance of interpretable bases, 2022.
URLhttps://transformer-circuits.pub/2022/mech-interp-e ssay/index.html .
10

--- PAGE 11 ---
Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, M ichael Petrov, and Shan Carter.
Zoom in: An introduction to circuits. Distill , 2020. doi: 10.23915/distill.00024.001.
https://distill.pub/2020/circuits/zoom-in.
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Jose ph, Nova DasSarma, Tom Henighan,
Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli,
Zac Hatﬁeld-Dodds, Danny Hernandez, Scott Johnston, Andy J ones, Jackson Kernion, Liane
Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish,
and Chris Olah. In-context learning and induction heads, 20 22.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei , and Ilya Sutskever. Language
models are unsupervised multitask learners. 2019.
Prajit Ramachandran, Barret Zoph, and Quoc V . Le. Searching for activation functions, 2017.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exac t solutions to the nonlinear dynam-
ics of learning in deep linear neural networks, 2013.
Noam Shazeer. Glu variants improve transformer, 2020. URL
https://arxiv.org/abs/2002.05202 .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit , Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you ne ed.CoRR , abs/1706.03762, 2017.
URLhttp://arxiv.org/abs/1706.03762 .
Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shleg eris, and Jacob Steinhardt. Inter-
pretability in the wild: a circuit for indirect object ident iﬁcation in gpt-2 small, 2022.
11

--- PAGE 12 ---
A Tensor inner product examples
The deﬁnition of tensor inner product we use is
U(n)·jkV(m)=T(n+m−2)
where
Tγ1···γj−1γj+1···γnγ′
1···γ′
k−1γ′
k+1···γ′m=/summationdisplay
βUγ1···γj−1βγj+1···γnVγ′
1···γ′
k−1βγ′
k+1···γ′m
Example 1 :U(1)·11V(1)=T(0)=/summationtext
βuβvβ=u⊤v, which is just the standard inner product,
resulting in a scalar.
Example 2 :U(2)·21V(1)=T(1)whereTi=/summationtext
βUiβvβ. This is multiplication of a matrix on the
right and a vector on the left: T=Uv.
Example 3 :U(2)·11V(1)=T(1)whereTi=/summationtext
βUβivβ. This is equivalent to multiplication of a
transposed matrix on the left and a vector on the right: T=U⊤v.
Example 4 :U(1)·12V(2)=T(1)whereTi=/summationtext
βuβViβ. This equivalent to multiplication of
transposed vector on the left and a matrix on the right: T=u⊤V⊤. Note that Tis a rank one
tensor, so T=u⊤V⊤=Vusince tensor notation disposes of the convention that vecto rs are
column vectors or row vectors; instead they are just rank-on e tensors. We somewhat abuse notation
in this work by assuming standard vector-matrix convention s for multiplication unless the tensors
we’re dealing with are rank-three or above, in which case we u se tensor inner product notation.
Example 5 :U(3)·11V(1)=T(2)which is the matrix that is a sum of matrices consisting of sli ces of
the rank-three tensor T=/summationtext
βUβ::vβ. If we imagine the rank-three tensor as a cube, this example
ﬂattens the tensor along its height by taking the inner produ ct between vand every 3-d column of
U.
Example 6 :U(2)·23V(3)=T(3)which is the rank-three tensor where Ti::=/summationtext
βU:βVi:β. If
we imagine tensor Vas a cube, here take each front-to-back-row and get its inner product with the
corresponding row iof matrix U.
12
