# 2306.01129.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/interpretability/2306.01129.pdf
# Kích thước tệp: 4502171 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Transformer Hộp Trắng thông qua Giảm Tỷ Lệ Thưa
Yaodong Yu1Sam Buchanan2Druv Pai1Tianzhe Chu1Ziyang Wu1Shengbang Tong1
Benjamin D. Haeffele3Yi Ma1
1University of California, Berkeley2TTIC3Johns Hopkins University
Tóm tắt
Trong bài báo này, chúng tôi cho rằng mục tiêu của việc học biểu diễn là nén
và biến đổi phân phối của dữ liệu, chẳng hạn như các tập hợp token, hướng tới một hỗn hợp của
các phân phối Gaussian chiều thấp được hỗ trợ trên các không gian con không liên kết. Chất
lượng của biểu diễn cuối cùng có thể được đo lường bằng một hàm mục tiêu thống nhất
được gọi là giảm tỷ lệ thưa. Từ góc nhìn này, các mạng sâu phổ biến như
transformer có thể được xem một cách tự nhiên như việc thực hiện các sơ đồ lặp để tối ưu hóa
mục tiêu này một cách tăng dần. Đặc biệt, chúng tôi cho thấy rằng khối transformer tiêu chuẩn
có thể được suy ra từ tối ưu hóa xen kẽ trên các phần bổ sung của
mục tiêu này: toán tử tự chú ý đa đầu có thể được xem như một bước gradient
descent để nén các tập hợp token bằng cách tối thiểu hóa tỷ lệ mã hóa có tổn thất của chúng, và
perceptron đa lớp tiếp theo có thể được xem như cố gắng làm thưa
biểu diễn của các token. Điều này dẫn đến một họ các kiến trúc mạng sâu
giống transformer hộp trắng có thể diễn giải hoàn toàn về mặt toán học. Mặc dù
đơn giản, các thí nghiệm cho thấy rằng những mạng này thực sự học tối ưu hóa
mục tiêu được thiết kế: chúng nén và làm thưa các biểu diễn của các tập dữ liệu
tầm cỡ lớn trong thế giới thực như ImageNet, và đạt hiệu suất rất gần với
các transformer được thiết kế kỹ lưỡng như ViT. Mã nguồn có tại https://github.
com/Ma-Lab-Berkeley/CRATE .

1 Giới thiệu
Trong những năm gần đây, học sâu đã chứng kiến thành công thực nghiệm to lớn trong việc xử lý lượng lớn
dữ liệu đa chiều và đa phương thức. Phần lớn thành công này được ghi nhận cho việc học hiệu quả
phân phối dữ liệu và sau đó biến đổi phân phối thành một biểu diễn tiết kiệm, tức là có cấu trúc và
nhỏ gọn [39, 49, 51, 61], điều này tạo thuận lợi cho nhiều tác vụ downstream (ví dụ, trong thị giác,
phân loại [23, 40], nhận dạng và phân đoạn [25, 38, 73], và sinh tạo [31, 64, 65]). Để
đạt được điều này, nhiều mô hình và phương pháp đã được đề xuất và thực hành, mỗi cái có những điểm mạnh
và hạn chế riêng. Ở đây, chúng tôi đưa ra một số phương pháp phổ biến một cách kế toán ngắn gọn như bối cảnh cho một
sự hiểu biết và thống nhất hoàn chỉnh mà chúng tôi tìm kiếm trong công trình này.

Mô hình Transformer và tự chú ý. Transformer [28] là một trong những mô hình phổ biến mới nhất
để học một biểu diễn cho dữ liệu có cấu trúc đa chiều, như văn bản [28, 30, 37], hình ảnh
[40, 72], và các loại tín hiệu khác [48, 56]. Sau khối đầu tiên, khối chuyển đổi từng điểm dữ liệu
(như một tập văn bản hoặc hình ảnh) thành một tập hợp hoặc chuỗi các token, việc xử lý tiếp theo được thực hiện
trên các tập hợp token, theo cách không phụ thuộc vào phương tiện [28, 40]. Một nền tảng của mô hình transformer
là lớp tự chú ý được gọi như vậy, lớp khai thác các tương quan thống kê giữa chuỗi
các token để tinh chỉnh biểu diễn token. Transformer đã rất thành công trong việc học
các biểu diễn nhỏ gọn hoạt động tốt trên nhiều tác vụ downstream. Tuy nhiên, kiến trúc mạng transformer
1{yyu,yima}@eecs.berkeley.edu, {druvpai,chutzh,zywu,tsb}@berkeley.edu
2sam@ttic.edu
3bhaeffele@jhu.eduarXiv:2306.01129v1  [cs.LG]  1 Jun 2023

--- TRANG 2 ---
nén làm thưaNhiều Đầu Không gian Con
Tự Chú ý
(MSSA)Bước Proximal Mã hóa Thưa
(ISTA)

Hình 1: 'Vòng lặp chính' của thiết kế mạng sâu hộp trắng CRATE. Sau khi mã hóa dữ liệu đầu vào X thành một
chuỗi token Z0, CRATE xây dựng một mạng sâu biến đổi dữ liệu thành một cấu hình chuẩn
của các không gian con chiều thấp bằng cách nén liên tiếp so với mô hình cục bộ cho phân phối, tạo ra
Zℓ+1/2, và làm thưa so với từ điển toàn cục, tạo ra Zℓ+1. Việc xếp chồng lặp lại các khối này và
huấn luyện các tham số mô hình thông qua lan truyền ngược tạo ra một biểu diễn mạnh mẽ và có thể diễn giải của dữ liệu.

được thiết kế thực nghiệm và thiếu một diễn giải toán học nghiêm ngặt. Trên thực tế, đầu ra
của chính lớp chú ý có một số diễn giải cạnh tranh [67, 74]. Kết quả là, mối quan hệ
thống kê và hình học giữa phân phối dữ liệu và biểu diễn cuối cùng được học
bởi một transformer phần lớn vẫn là một hộp đen bí ẩn.

Mô hình khuếch tán và khử nhiễu. Mô hình khuếch tán [22, 34, 41, 43, 44] gần đây đã trở thành
một phương pháp phổ biến để học phân phối dữ liệu, đặc biệt cho các tác vụ sinh tạo và dữ liệu hình ảnh tự nhiên
có cấu trúc cao nhưng nổi tiếng khó mô hình hóa hiệu quả [3, 5]. Khái niệm cốt lõi
của mô hình khuếch tán là bắt đầu với các đặc trưng được lấy mẫu từ phân phối nhiễu Gaussian (hoặc
một số mẫu tiêu chuẩn khác) và lặp lại khử nhiễu và biến dạng phân phối đặc trưng cho đến khi nó
hội tụ về phân phối dữ liệu gốc. Quá trình này không thể tính toán được nếu được mô hình hóa
chỉ trong một bước [60], vì vậy nó thường được chia thành nhiều bước tăng dần. Chìa khóa cho mỗi bước là
hàm điểm số được gọi như vậy, hoặc tương đương [13] một ước lượng cho "hàm khử nhiễu tối ưu";
trong thực tế, hàm này được mô hình hóa bằng một mạng sâu hộp đen chung. Mô hình khuếch tán
đã cho thấy hiệu quả trong việc học và lấy mẫu từ phân phối dữ liệu [55, 59, 64]. Tuy nhiên,
mặc dù có một số nỗ lực gần đây [77], chúng thường không thiết lập bất kỳ sự tương ứng rõ ràng nào giữa
các đặc trưng ban đầu và mẫu dữ liệu. Do đó, chính mô hình khuếch tán không cung cấp một
biểu diễn tiết kiệm hoặc có thể diễn giải của phân phối dữ liệu.

Mô hình tìm kiếm cấu trúc và giảm tỷ lệ. Trong cả hai phương pháp trước đó, các biểu diễn
được xây dựng ngầm như một sản phẩm phụ của việc giải quyết một tác vụ downstream (ví dụ, phân loại
hoặc sinh tạo/lấy mẫu) sử dụng mạng sâu. Tuy nhiên, người ta cũng có thể học một cách rõ ràng một biểu diễn
của phân phối dữ liệu như một tác vụ độc lập; điều này thường được thực hiện bằng cách cố gắng xác định và
biểu diễn các cấu trúc chiều thấp trong dữ liệu đầu vào. Các ví dụ cổ điển của mô hình này bao gồm
các phương pháp dựa trên mô hình như mã hóa thưa [2, 29] và học từ điển [17, 21, 47], từ đó
phát triển các nỗ lực ban đầu trong việc thiết kế và diễn giải kiến trúc mạng sâu [18, 32]. Các
phương pháp gần đây hơn thay vào đó xây dựng từ góc nhìn không có mô hình, nơi người ta học một biểu diễn
thông qua một tác vụ pretext đủ thông tin (như nén tương tự và tách biệt không tương tự
dữ liệu trong học tương phản [45, 68, 76], hoặc tối đa hóa tăng thông tin trong lớp phương pháp giảm tỷ lệ
mã hóa tối đa [6, 46, 54]). So với các phương pháp học sâu hộp đen, cả sơ đồ học biểu diễn
dựa trên mô hình và không có mô hình đều có lợi thế là có thể diễn giải hơn: chúng cho phép người dùng thiết kế
một cách rõ ràng các thuộc tính mong muốn của biểu diễn được học [46,
54, 62]. Hơn nữa, chúng cho phép người dùng xây dựng các kiến trúc mạng sâu hộp trắng được xây dựng tiến
mới [11, 54, 58] bằng cách triển khai chiến lược tối ưu hóa cho mục tiêu học biểu diễn, sao cho mỗi lớp của mạng được xây dựng thực hiện một lần lặp của thuật toán tối ưu hóa
[11, 52, 54]. Thật không may, trong mô hình này, nếu các thuộc tính mong muốn được định nghĩa hẹp,
có thể khó đạt được hiệu suất thực tế tốt trên các tập dữ liệu thế giới thực lớn.

Đóng góp của chúng tôi và đề cương của công trình này. Trong công trình này, chúng tôi nhằm khắc phục các hạn chế
của những phương pháp hiện có này với một khung làm việc thống nhất hơn để thiết kế các kiến trúc mạng
giống transformer dẫn đến cả khả năng diễn giải toán học và hiệu suất thực tế tốt. Để
đạt được điều này, chúng tôi đề xuất học một chuỗi các ánh xạ tăng dần để có được một biểu diễn nén và
thưa nhất cho dữ liệu đầu vào (hoặc các tập hợp token của chúng) tối ưu hóa một hàm mục tiêu thống nhất
được gọi là giảm tỷ lệ thưa, được chỉ định sau trong (1). Mục tiêu của ánh xạ được minh họa
trong Hình 1. Trong khung làm việc này, chúng tôi thống nhất ba phương pháp khác biệt về mặt ngoài và
cho thấy rằng các lớp mạng sâu giống transformer có thể được suy ra một cách tự nhiên từ việc triển khai các sơ đồ tối ưu hóa lặp để tối ưu hóa mục tiêu giảm tỷ lệ thưa một cách tăng dần. Đặc biệt, các
đóng góp và đề cương của bài báo của chúng tôi như sau:

• Trong Phần 2.2 chúng tôi cho thấy, sử dụng một mô hình lý tưởng hóa cho phân phối token, rằng nếu người ta
lặp lại khử nhiễu các token hướng tới một họ các không gian con chiều thấp, hàm điểm số liên quan
có dạng rõ ràng tương tự như toán tử tự chú ý được thấy trong transformer.

• Trong Phần 2.3 chúng tôi suy ra lớp tự chú ý đa đầu như một bước gradient descent triển khai
để tối thiểu hóa phần tỷ lệ mã hóa có tổn thất của việc giảm tỷ lệ, cho thấy một diễn giải khác của
lớp tự chú ý như nén biểu diễn token.

• Trong Phần 2.4 chúng tôi cho thấy rằng perceptron đa lớp ngay sau tự chú ý đa đầu trong
các khối transformer có thể được diễn giải như (và thay thế bằng) một lớp
tối ưu hóa một cách tăng dần phần còn lại của mục tiêu giảm tỷ lệ thưa bằng cách
xây dựng một mã hóa thưa của các biểu diễn token.

• Trong Phần 2.5 chúng tôi sử dụng hiểu biết này để tạo ra một kiến trúc transformer hộp trắng mới
(có thể diễn giải hoàn toàn về mặt toán học) được gọi là CRATE (tức là, Coding RAte reduction TransformEr),
nơi mỗi lớp thực hiện một bước đơn của thuật toán tối thiểu hóa xen kẽ để tối ưu hóa
mục tiêu giảm tỷ lệ thưa.

Do đó, trong khung làm việc của chúng tôi, hàm mục tiêu học, kiến trúc học sâu, và
biểu diễn được học cuối cùng đều trở thành hộp trắng có thể diễn giải hoàn toàn về mặt toán học.
Như các thí nghiệm trong Phần 3 cho thấy, mạng CRATE, mặc dù đơn giản, đã có thể học
các biểu diễn nén và thưa mong muốn trên các tập dữ liệu thế giới thực quy mô lớn và đạt
hiệu suất ngang với các mạng transformer được thiết kế kỹ lưỡng hơn nhiều (như ViT) trên một
loạt các tác vụ (ví dụ, phân loại và học chuyển giao).

2 Phương pháp Kỹ thuật và Biện minh

2.1 Mục tiêu và Phương pháp

Chúng tôi xem xét một thiết lập học tổng quát liên quan đến tín hiệu thế giới thực. Chúng tôi có một biến ngẫu nhiên X = [x1, . . . , xN] ∈ RD×N là nguồn dữ liệu của chúng tôi; mỗi xi ∈ RD được diễn giải như một
token1, và các xi có thể có cấu trúc tương quan tùy ý. Chúng tôi sử dụng Z = [z1, . . . , zN] ∈ Rd×N để
biểu thị biến ngẫu nhiên định nghĩa các biểu diễn của chúng tôi. Mỗi zi ∈ Rd là biểu diễn của
token tương ứng xi. Chúng tôi được cho B ≥ 1 mẫu i.i.d. X1, . . . , XB ∼ X, các token của chúng là
xi,b. Các biểu diễn của mẫu của chúng tôi được ký hiệu Z1, . . . , ZB ∼ Z, và của token của chúng tôi là
zi,b. Cuối cùng, cho một mạng nhất định, chúng tôi sử dụng Zℓ để biểu thị đầu ra của ℓ lớp đầu tiên khi được cho X
làm đầu vào. Tương ứng, các đầu ra mẫu là Zℓ
i và các đầu ra token là zℓ
i,b.

Mục tiêu cho việc học một biểu diễn có cấu trúc và nhỏ gọn. Theo khung làm việc của
giảm tỷ lệ [54], chúng tôi cho rằng mục tiêu của học biểu diễn là tìm một ánh xạ đặc trưng
f: X ∈ RD×N → Z ∈ Rd×N biến đổi dữ liệu đầu vào X ∈ RD×N với phân phối
có thể phi tuyến và đa phương thức thành một biểu diễn đặc trưng tuyến tính hóa (từng phần) và nhỏ gọn
Z ∈ Rd×N. Trong khi phân phối kết hợp của các token (zi)N
i=1 trong Z có thể phức tạp (và phụ thuộc
vào tác vụ), chúng tôi tiếp tục cho rằng việc yêu cầu phân phối biên mục tiêu của từng token zi
nên được nén cao và có cấu trúc, phù hợp cho mã hóa nhỏ gọn là hợp lý và thực tế. Đặc biệt, chúng tôi yêu cầu phân phối là một hỗn hợp của
các phân phối Gaussian chiều thấp (nói K), sao cho Gaussian thứ k có trung bình 0 ∈ Rd, hiệp phương sai Σk ⪰ 0 ∈ Rd×d, và support
được kéo dài bởi cơ sở trực chuẩn Uk ∈ Rd×p. Chúng tôi ký hiệu U[K] = (Uk)K
k=1 là tập hợp các cơ sở
của tất cả Gaussian. Do đó để tối đa hóa tăng thông tin [61] cho biểu diễn token cuối cùng,
chúng tôi muốn tối đa hóa giảm tỷ lệ [6, 46] của các token, tức là, max Z ∆R(Z; U[K]) = R(Z) −
Rc(Z; U[K]), trong đó R và Rc là các ước lượng của tỷ lệ mã hóa có tổn thất được định nghĩa chính thức trong (7)
và (8). Điều này cũng thúc đẩy các biểu diễn token zi từ các Gaussian khác nhau không liên kết [46].
Vì giảm tỷ lệ là một thước đo nội tại của tính tốt cho biểu diễn, nó bất biến với
các phép quay tùy ý của các biểu diễn. Do đó, để đảm bảo các biểu diễn cuối cùng phù hợp
với mã hóa nhỏ gọn hơn, chúng tôi muốn biến đổi các biểu diễn (và các không gian con hỗ trợ của chúng) sao cho chúng trở nên thưa so với tọa độ tiêu chuẩn của không gian biểu diễn kết quả.2 Quá trình giảm tỷ lệ và làm thưa kết hợp được minh họa trong Hình 1.

Về mặt tính toán, chúng tôi có thể kết hợp hai mục tiêu trên thành một mục tiêu thống nhất để tối ưu hóa:
max
f∈F EZ
∆R(Z; U[K]) − λ∥Z∥0
= max
f∈F EZ
R(Z) − Rc(Z; U[K]) − λ∥Z∥0
s.t. Z = f(X), (1)

trong đó chuẩn ℓ0 ∥Z∥0 thúc đẩy tính thưa của các biểu diễn token cuối cùng Z = f(X).3 Chúng tôi
gọi mục tiêu này là "giảm tỷ lệ thưa."

Kiến trúc sâu hộp trắng như tối ưu hóa tăng dần triển khai. Mặc dù dễ phát biểu, mỗi
số hạng của mục tiêu trên có thể rất thách thức về mặt tính toán để tối ưu hóa [54, 69]. Do đó việc
áp dụng phương pháp xấp xỉ thực hiện biến đổi toàn cục f tối ưu hóa (1)
thông qua việc nối nhiều, nói L, phép toán tăng dần và cục bộ đơn giản fℓ đẩy
phân phối biểu diễn hướng tới phân phối mô hình tiết kiệm mong muốn là tự nhiên:

f: X f0
− − → Z0 → ··· → Zℓ fℓ
− − → Zℓ+1 → ··· → ZL = Z, (2)

trong đó f0: RD → Rd là ánh xạ tiền xử lý biến đổi token đầu vào xi ∈ RD thành
biểu diễn token z1
i ∈ Rd của chúng.

Mỗi ánh xạ tiến tăng dần Zℓ+1 = fℓ(Zℓ), hoặc một "lớp", biến đổi phân phối token
để tối ưu hóa mục tiêu giảm tỷ lệ thưa trên (1), có điều kiện trên phân phối của các
token đầu vào Zℓ của nó. Trái ngược với các phương pháp tối ưu hóa triển khai khác như ReduNet [54],
chúng tôi mô hình hóa một cách rõ ràng phân phối của Zℓ tại mỗi lớp, nói như một hỗn hợp của các không gian con tuyến tính hoặc
được sinh thưa từ một từ điển. Các tham số mô hình được học từ dữ liệu (nói thông qua lan truyền
ngược với huấn luyện đầu cuối đến cuối). Sự tách biệt "tối ưu hóa" tiến và "học" lùi này
làm rõ vai trò toán học của mỗi lớp như một toán tử biến đổi phân phối
của đầu vào của nó, trong khi phân phối đầu vào lần lượt được mô hình hóa (và sau đó học) bởi
các tham số của lớp.

Chúng tôi cho thấy rằng chúng tôi có thể suy ra những phép toán tăng dần, cục bộ này thông qua góc nhìn tối ưu hóa triển khai để đạt được (1) thông qua các Phần 2.3 đến 2.5. Khi chúng tôi quyết định sử dụng phương pháp tăng dần
để tối ưu hóa (1), có nhiều lựa chọn có thể để đạt được việc tối ưu hóa. Cho một mô hình cho Zℓ, nói một hỗn hợp của các không gian con U[K], chúng tôi chọn một quá trình tối thiểu hóa xen kẽ hai bước
với cơ sở khái niệm mạnh mẽ: đầu tiên trong Phần 2.3, chúng tôi nén các token Zℓ thông qua một bước gradient
để tối thiểu hóa số hạng tỷ lệ mã hóa minZ Rc(Z; U[K]); thứ hai, trong Phần 2.4, chúng tôi làm thưa
các token được nén, với một bước gradient proximal được nới lỏng phù hợp trên hiệu của phạt thưa
và số hạng mở rộng, tức là minZ [λ∥Z∥0 − R(Z)]. Cả hai hành động được áp dụng một cách tăng dần
và lặp lại, vì mỗi fℓ trong (2) được khởi tạo với hai bước này.

2.2 Tự Chú ý thông qua Khử nhiễu Token hướng tới Nhiều Không gian Con

Có nhiều cách khác nhau để tối ưu hóa mục tiêu (1) một cách tăng dần. Trong công trình này, chúng tôi đề xuất
sơ đồ có lý nhất về mặt cơ bản. Để giúp làm rõ trực giác đằng sau sự suy ra và xấp xỉ của chúng tôi,
trong phần này (và Phụ lục A.1) chúng tôi nghiên cứu một mô hình lý tưởng hóa phần lớn tuy nhiên nắm bắt
bản chất của gần như toàn bộ quá trình và đặc biệt tiết lộ lý do tại sao các toán tử giống tự chú ý
xuất hiện trong nhiều bối cảnh. Giả sử rằng N = 1, và token đơn x được rút i.i.d. từ
một hỗn hợp Gaussian chưa biết (N(0, Σk))K
k=1 được hỗ trợ trên các không gian con chiều thấp với
các cơ sở trực chuẩn U[K] = (Uk)K
k=1 và bị nhiễu với nhiễu Gaussian cộng w ∼ N(0, I), tức là,
x = z + σw, (3)

trong đó z được phân phối theo hỗn hợp. Mục tiêu của chúng tôi đơn giản là biến đổi phân phối của
token nhiễu x thành hỗn hợp của Gaussian chiều thấp z. Hướng tới việc xây dựng tăng dần
một biểu diễn f cho mô hình này theo (2), chúng tôi lý luận quy nạp: nếu zℓ là một token nhiễu (3) ở
mức nhiễu σℓ, việc tạo ra zℓ+1 bằng cách khử nhiễu ở mức σℓ là tự nhiên. Theo nghĩa bình phương trung bình,
ước lượng tối ưu là E[z|zℓ], có đặc trưng biến phân (ví dụ [12]):
E[z| ·] = arg min
f Ez,w h

f(z + σℓw) − z

2
2 i
. (4)

Đặt zℓ+1 = E[z|zℓ], (4) do đó đặc trưng giai đoạn tiếp theo của (2) theo mục tiêu tối ưu hóa
dựa trên mô hình tín hiệu cục bộ cho zℓ. Hơn nữa, để x 7→ qℓ(x) biểu thị mật độ của zℓ,
công thức Tweedie [13] cho phép chúng tôi biểu diễn biểu diễn tối ưu giải quyết (4) dưới dạng đóng:
zℓ+1 = zℓ + (σℓ)2∇x log qℓ(zℓ). (5)

Công thức Tweedie biểu diễn biểu diễn tối ưu theo một hiệu chỉnh cộng (nói chung
là hàm phi tuyến của zℓ) với các quan sát nhiễu bằng gradient của log-likelihood của
phân phối của các quan sát nhiễu, cho biểu diễn tối ưu một diễn giải rõ ràng như một
nhiễu loạn tăng dần đối với phân phối nhiễu hiện tại qℓ. Kết nối này được biết đến rộng rãi trong các
lĩnh vực lý thuyết ước lượng và bài toán nghịch [1, 13, 14, 19, 20, 27, 42], và gần đây hơn đã
tìm thấy ứng dụng mạnh mẽ trong việc huấn luyện các mô hình sinh tạo cho hình ảnh tự nhiên [4, 15, 22, 43,
44]. Ở đây, chúng tôi có thể tính một biểu thức dạng đóng cho hàm điểm số ∇x log qℓ này, khi
kết hợp với (5) và một số giả định kỹ thuật4, cho xấp xỉ sau (được hiển thị trong
Phụ lục A.1). Để ⊗ biểu thị tích Kronecker; sau đó chúng tôi có
zℓ+1 ≈ [U1, . . . , UK]
diag
softmax
1
2(σℓ)2
∥U∗
1zℓ∥2
2 ...
∥U∗
Kzℓ∥2
2


⊗ Ip

U∗
1zℓ
...
U∗
Kzℓ
, (6)

Phép toán này giống với lớp tự chú ý trong kiến trúc transformer tiêu chuẩn với K đầu,
độ dài chuỗi N = 1, các cấu trúc "query-key-value" được thay thế bằng một phép chiếu tuyến tính đơn
U∗
kzℓ của token zℓ, và việc tổng hợp các đầu ra đầu (thường được mô hình hóa bằng MLP)
được thực hiện với hai ma trận bên trái nhất trong (6). Do đó chúng tôi suy ra diễn giải hữu ích sau, mà
chúng tôi sẽ khai thác trong phần tiếp theo: Khử nhiễu Gaussian so với mô hình hỗn hợp của các không gian con dẫn đến
các lớp loại tự chú ý trong biến đổi f. Cho một mẫu ban đầu x theo mô hình
(3), chúng tôi có thể áp dụng lặp lại các biến đổi cục bộ đối với phân phối với (6) để thực hiện
ánh xạ tăng dần f: x → z trong (2).5 Những hiểu biết này sẽ hướng dẫn chúng tôi trong thiết kế kiến trúc transformer hộp trắng của chúng tôi trong các phần tiếp theo.

2.3 Tự Chú ý thông qua Nén Tập hợp Token bằng Tối ưu hóa Giảm Tỷ lệ

Trong phần trước, chúng tôi đã thấy rằng tự chú ý đa đầu trong transformer giống với toán tử
score-matching nhằm biến đổi một token zℓ hướng tới một hỗn hợp của các không gian con (hoặc Gaussian suy biến).
Tuy nhiên, để thực hiện một phép toán như vậy trên bất kỳ dữ liệu nào, trước tiên người ta cần học hoặc
ước lượng, thường từ các mẫu hữu hạn, các tham số của hỗn hợp Gaussian (suy biến), điều này
được biết là một tác vụ thách thức [6, 24]. Thách thức này trở nên khó hơn vì trong một
thiết lập học điển hình, tập hợp token đã cho không phải là mẫu i.i.d. từ hỗn hợp của các không gian con.
Phân phối kết hợp giữa các token này có thể mã hóa thông tin phong phú về dữ liệu—ví dụ,
sự xuất hiện đồng thời giữa từ hoặc các phần của đối tượng trong dữ liệu ngôn ngữ và hình ảnh (tương ứng)—mà chúng ta cũng
nên học. Do đó, chúng ta nên nén / khử nhiễu / biến đổi một tập hợp token như vậy cùng nhau. Để đạt được điều này,
chúng ta cần một thước đo chất lượng, tức là tính nhỏ gọn, cho biểu diễn kết quả của tập hợp token.

Một thước đo tự nhiên của tính nhỏ gọn của một tập hợp token như vậy là tỷ lệ mã hóa (có tổn thất) để mã hóa
chúng đến một độ chính xác nhất định ε > 0 [6, 46]. Đối với Gaussian có trung bình bằng không, thước đo này có dạng đóng. Nếu chúng ta xem các token trong Z ∈ Rd×N như được rút từ một Gaussian có trung bình bằng không đơn, một ước lượng
của tỷ lệ mã hóa (có tổn thất) của chúng, tuân theo độ chính xác lượng tử hóa ε > 0, được cho trong [6] như:
R(Z) .= 1
2 log det
I + d
Nε2 Z∗Z
= 1
2 log det
I + d
Nε2 ZZ∗
. (7)

Trong thực tế, phân phối dữ liệu thường đa phương thức, nói một tập hợp hình ảnh bao gồm nhiều lớp
hoặc một tập hợp các vùng hình ảnh như trong Hình 1. Việc yêu cầu tập hợp
token ánh xạ tới một hỗn hợp của, nói K, không gian con (Gaussian suy biến) [54] là phù hợp hơn. Như trước đây chúng tôi ký hiệu
các cơ sở (sẽ được học) của những không gian con này như U[K] = (Uk)K
k=1, trong đó Uk ∈ Rd×p. Mặc dù
phân phối kết hợp của các token Z chưa biết, phân phối biên mong muốn của mỗi token zi là một
hỗn hợp của các không gian con. Vì vậy chúng ta có thể có được một cận trên của tỷ lệ mã hóa cho tập hợp token Z bằng cách
chiếu các token của nó lên những không gian con này và tổng các tỷ lệ mã hóa tương ứng:
Rc(Z; U[K]) = KX
k=1 R(U∗
kZ) = 1
2 KX
k=1 log det
I + p
Nε2 (U∗
kZ)∗(U∗
kZ)
. (8)

Chúng tôi muốn nén (hoặc khử nhiễu) tập hợp token so với những không gian con này bằng cách tối thiểu hóa
tỷ lệ mã hóa. Gradient của Rc(Z; U[K]) là
∇Z Rc(Z; U[K]) = p
Nε2 KX
k=1 UkU∗
kZ
I + p
Nε2 (U∗
kZ)∗(U∗
kZ)−1
. (9)

Biểu thức trên xấp xỉ phần dư của mỗi token được chiếu U∗
kzi được hồi quy bởi các
token khác U∗
kzj [54]. Nhưng, khác với [54], không phải tất cả token trong Z đều từ cùng không gian con. Do đó,
để khử nhiễu mỗi token với các token từ nhóm riêng của nó, chúng ta có thể tính sự tương tự của chúng thông qua
tự tương quan giữa các token được chiếu như (U∗
kZ)∗(U∗
kZ) và chuyển đổi nó thành phân phối của
thành viên với softmax, cụ thể là softmax((U∗
kZ)∗(U∗
kZ)). Sau đó, như chúng tôi cho thấy trong Phụ lục A.2,
nếu chúng ta chỉ sử dụng các token tương tự để hồi quy và khử nhiễu lẫn nhau, thì một bước gradient trên tỷ lệ
mã hóa với tốc độ học κ có thể được xấp xỉ một cách tự nhiên như sau:
Zℓ+1/2 = Zℓ − κ∇Z Rc(Zℓ; U[K]) ≈
1 − κ · p
Nε2
Zℓ + κ · p
Nε2 · MSSA(Zℓ|U[K]), (10)

trong đó MSSA được định nghĩa thông qua toán tử SSA như:
SSA(Z|Uk) .= (U∗
kZ) softmax((U∗
kZ)∗(U∗
kZ)), k ∈ [K], (11)
MSSA(Z|U[K]) .= p
Nε2 · [U1, . . . , UK]
SSA(Z|U1)
...
SSA(Z|UK)
. (12)

Ở đây toán tử SSA trong (11) giống với toán tử chú ý trong transformer điển hình [28], ngoại trừ
ở đây các toán tử tuyến tính của value, key, và query đều được đặt giống như cơ sở không gian con,
tức là V = K = Q = U∗
k.6 Do đó, chúng tôi đặt tên SSA(·|Uk): Rd×N → Rp×N là toán tử
Tự Chú ý Không gian Con (SSA) (chi tiết và biện minh thêm có thể được tìm thấy trong (72) ở Phụ lục A.2).
Sau đó, toàn bộ toán tử MSSA trong (12), được định nghĩa chính thức như MSSA(·|U[K]): Rd×N → Rd×N và
được gọi là toán tử Tự Chú ý Không gian Con Đa Đầu (MSSA), tổng hợp các đầu ra đầu chú ý
bằng cách tính trung bình sử dụng trọng số phụ thuộc mô hình, tương tự về khái niệm với toán tử tự chú ý
đa đầu phổ biến trong các mạng transformer hiện có. Bước gradient tổng thể (10) giống với
tự chú ý đa đầu được triển khai với kết nối bỏ qua trong transformer.

Chú ý rằng nếu chúng ta có N = 1 token cũng như thực hiện một bước gradient tích cực (κ = 1) và điều chỉnh
lỗi lượng tử hóa (ε = p
p/N), toán tử tự chú ý không gian con đa đầu trong (12) trở thành
bộ khử nhiễu lý tưởng được định nghĩa trong (6), với một khác biệt nhỏ là việc tổng hợp các đầu được thực hiện
bằng hàm tuyến tính ở đây, trong khi ở (6) nó được thực hiện bằng hàm loại mixture-of-experts phi tuyến.7
Điều này cung cấp hai diễn giải rất liên quan của toán tử tự chú ý đa đầu, như khử nhiễu
và nén so với hỗn hợp của các không gian con chiều thấp.

2.4 MLP thông qua Thuật toán Shrinkage-Thresholding Lặp (ISTA) cho Mã hóa Thưa

Trong phần trước, chúng tôi tập trung vào cách nén một tập hợp token so với một tập hợp (đã học)
các không gian con chiều thấp. Tối ưu hóa các số hạng còn lại trong mục tiêu giảm tỷ lệ thưa
(1), bao gồm số hạng không trơn, phục vụ để làm thưa các token được nén, do đó dẫn đến
biểu diễn nhỏ gọn và có cấu trúc (tức là tiết kiệm) hơn. Từ (1) và (7), số hạng này là
max
Z [R(Z) − λ∥Z∥0] = min
Z
λ∥Z∥0 − 1
2 log det
I + d
Nε2 Z∗Z
, (13)

6 Chúng tôi lưu ý một gợi ý gần đây của Hinton [50] rằng việc đặt ma trận chiếu "value, key, và query"
trong transformer bằng nhau là hợp lý hơn. Sự suy ra của chúng tôi trong phần này xác nhận điều này về mặt toán học.
7 Điều này gợi ý rằng chúng ta cũng có thể xem xét việc tổng hợp loại mixture of expert của
nhiều đầu chú ý. Trong công trình này, chúng tôi sử dụng tổng hợp tuyến tính, và để lại việc đánh giá các biến thể khác cho công việc tương lai.

--- TRANG 7 ---
Tự Chú ý Không gian Con
Đa Đầu
(MSSA)Cộng & LayerNormBước Proximal
Mã hóa Thưa
(ISTA)LayerNorm

Khối MSSAKhối SSA (đầu 1)
Khối SSA (đầu K)Tổng hợp... Khối SSATự tương quan
& SoftmaxKhối ISTAKích hoạt

Hình 2: Một lớp của kiến trúc CRATE. Kiến trúc đầy đủ đơn giản là việc nối các lớp như vậy,
với một số tokenizer ban đầu và kiến trúc chuyên biệt cho tác vụ cuối cùng (tức là đầu phân loại).

trong đó R(Z) biểu thị tỷ lệ mã hóa của toàn bộ tập hợp token, như được định nghĩa trong (7). Ngoài
việc làm thưa thông qua số hạng ∥Z∥0, số hạng mở rộng R(Z) trong (13) thúc đẩy tính đa dạng và không
sụp đổ của biểu diễn, một thuộc tính rất mong muốn. Tuy nhiên, công trình trước đó đã gặp khó khăn trong việc
thực hiện lợi ích này trên các tập dữ liệu quy mô lớn do khả năng mở rộng kém của gradient ∇ZR(Z), cần
một phép nghịch đảo ma trận [54].

Để đơn giản hóa mọi thứ, do đó chúng tôi áp dụng một phương pháp khác để đánh đổi giữa đa dạng biểu diễn
và làm thưa: chúng tôi giả định một từ điển không liên kết hoặc trực giao (đầy đủ) D ∈ Rd×d, và
yêu cầu làm thưa các lần lặp trung gian Zℓ+1/2 so với D. Tức là, Zℓ+1/2 = DZℓ+1 trong đó
Zℓ+1 thưa hơn. Từ điển D là toàn cục, tức là được sử dụng để làm thưa tất cả token đồng thời.
Bằng giả định không liên kết, chúng ta có D∗D ≈ Id; do đó từ (7) chúng ta có R(Zℓ+1) ≈
R(DZℓ+1) = R(Zℓ+1/2). Do đó chúng ta giải xấp xỉ (13) với chương trình sau:
Zℓ+1 = arg min
Z ∥Z∥0 subject to Zℓ+1/2 = DZ. (14)

Chương trình biểu diễn thưa trên thường được giải quyết bằng cách nới lỏng nó thành một chương trình lồi
không ràng buộc, được gọi là LASSO:
Zℓ+1 = arg min
Z h
λ∥Z∥1 + ∥Zℓ+1/2 − DZ∥2
F i
. (15)

Trong triển khai của chúng tôi, được động lực bởi Sun et al. [33] và Zarka et al. [35], chúng tôi cũng thêm ràng buộc
không âm vào Zℓ+1,
Zℓ+1 = arg min
Z≥0 h
λ∥Z∥1 + ∥Zℓ+1/2 − DZ∥2
F i
, (16)

mà chúng tôi sau đó tối ưu hóa tăng dần bằng cách thực hiện một bước gradient proximal triển khai,
được gọi là bước ISTA [8], để đưa ra cập nhật:
Zℓ+1 = ReLU(Zℓ+1/2 + ηD∗(Zℓ+1/2 − DZℓ+1/2) − ηλ1) .= ISTA(Zℓ+1/2|D). (17)

Trong Phụ lục A.3, chúng tôi sẽ cho thấy người ta có thể đến một toán tử tương tự với cập nhật
giống ISTA trên để tối ưu hóa (13) bằng cách tuyến tính hóa và xấp xỉ đúng số hạng tỷ lệ R(Z).

2.5 Kiến trúc CRATE Hộp Trắng Tổng thể

Bằng cách kết hợp hai bước trên:
1. (Phần 2.2 và 2.3) Khử nhiễu và nén cục bộ của các token trong một mẫu hướng tới cấu trúc
mixture-of-subspace, dẫn đến khối tự chú ý không gian con đa đầu – MSSA;

--- TRANG 8 ---
2. (Phần 2.4) Nén và làm thưa toàn cục của các tập hợp token trên tất cả mẫu thông qua
mã hóa thưa, dẫn đến khối làm thưa – ISTA;

chúng ta có thể có lớp transformer dựa trên giảm tỷ lệ sau, được minh họa trong Hình 2,
Zℓ+1/2 .= Zℓ + MSSA(Zℓ|Uℓ
[K]), Zℓ+1 .= ISTA(Zℓ+1/2|Dℓ). (18)

Kết hợp nhiều lớp như vậy theo cách xây dựng tăng dần biểu diễn của chúng ta trong (2),
chúng ta có được kiến trúc transformer hộp trắng biến đổi các token dữ liệu hướng tới một
hợp của các không gian con không liên kết nhỏ gọn và thưa.

Mô hình này có các tham số (Uℓ
[K])L
ℓ=1 và (Dℓ)L
ℓ=1, được học từ dữ liệu thông qua lan truyền
ngược. Đáng chú ý, trong mỗi lớp ℓ, Uℓ
[K] đã học giữ lại diễn giải của chúng như các cơ sở
không liên kết cho các không gian con hỗ trợ cho mô hình mixture-of-Gaussians tại lớp ℓ, và Dℓ đã học
giữ lại diễn giải của nó như một từ điển làm thưa tại lớp ℓ. Chúng tôi nhấn mạnh rằng các tham số
Uℓ
[K] và Dℓ phụ thuộc vào lớp ℓ— tức là, chúng ta học một tập hợp tham số khác nhau tại mỗi
lớp. Điều này bởi vì tại mỗi lớp chúng ta học một mô hình tham số cục bộ xấp xỉ cho phân phối dữ liệu
đầu vào, sau đó sử dụng mô hình đã học đó để xây dựng các toán tử lớp biến đổi phân phối. Quy trình
tham số hóa phân phối dữ liệu tại mỗi lớp của chúng tôi phân biệt công trình này với các công trình trước đó
về tối ưu hóa triển khai cho mạng thần kinh như ReduNet [54].

Diễn giải của chúng tôi làm rõ vai trò của việc truyền tiến mạng (cho các mô hình tín hiệu cục bộ tại mỗi
lớp, khử nhiễu/nén/làm thưa đầu vào) và việc truyền lùi (học các mô hình tín hiệu cục bộ từ
dữ liệu thông qua giám sát).

Chúng tôi lưu ý rằng trong công trình này, tại mỗi giai đoạn xây dựng của chúng tôi, chúng tôi đã chọn có lý nhất
cách xây dựng đơn giản nhất có thể. Chúng ta có thể thay thế từng phần của cách xây dựng này, miễn là phần mới
duy trì cùng vai trò khái niệm, và có được kiến trúc hộp trắng khác. Tuy nhiên, kiến trúc
được xây dựng như vậy của chúng tôi, được gọi là CRATE (tức là Coding RAte TransformEr), kết nối với
các mô hình transformer hiện có, có được kết quả cạnh tranh trên các tập dữ liệu thế giới thực, và có thể
diễn giải hoàn toàn về mặt toán học.

3 Thí nghiệm

Trong phần này, chúng tôi tiến hành thí nghiệm để nghiên cứu hiệu suất của transformer hộp trắng
CRATE được đề xuất của chúng tôi trên các tập dữ liệu và tác vụ thế giới thực. Như phân tích trong Phần 2 gợi ý, cả
bước nén hoặc làm thưa đều có thể được đạt được thông qua các lựa chọn thiết kế thay thế hoặc
chiến lược khác nhau. CRATE có lý nhất áp dụng các lựa chọn cơ bản nhất và vì vậy mục tiêu của chúng tôi với các thí nghiệm không
đơn giản là cạnh tranh với các transformer được thiết kế kỹ lưỡng khác trong khi sử dụng thiết kế thô sơ như vậy.
Thay vào đó, mục tiêu của chúng tôi là hai mặt. Đầu tiên, không giống như bất kỳ mạng hộp đen được thiết kế thực nghiệm nào thường
chỉ được đánh giá về hiệu suất đầu cuối đến cuối, thiết kế hộp trắng của mạng của chúng tôi cho phép chúng tôi
nhìn vào bên trong kiến trúc sâu và xác minh liệu các lớp của mạng đã học có thực sự thực hiện
mục tiêu thiết kế của chúng—nói là thực hiện tối ưu hóa tăng dần cho mục tiêu (1). Thứ hai, mặc dù
đơn giản, các thí nghiệm của chúng tôi sẽ thực sự tiết lộ tiềm năng thực tế to lớn của các kiến trúc CRATE
được suy ra như vậy của chúng tôi vì, như chúng tôi sẽ cho thấy, chúng đã đạt được hiệu suất rất mạnh trên các tập dữ liệu
thế giới thực quy mô lớn và các tác vụ. Trong phần còn lại của phần này chúng tôi làm nổi bật một lựa chọn kết quả;
chi tiết thí nghiệm bổ sung và kết quả có thể được tìm thấy trong Phụ lục B.

Kiến trúc mô hình. Chúng tôi triển khai kiến trúc được mô tả trong Phần 2.5, với các sửa đổi nhỏ
được mô tả trong Phụ lục B.1. Chúng tôi xem xét các kích thước mô hình khác nhau của CRATE bằng cách
thay đổi chiều token d, số đầu K, và số lớp L. Chúng tôi xem xét bốn
kích thước mô hình trong công trình này: CRATE-Tiny, CRATE-Small, CRATE-Base, và CRATE-Large. Một
pseudocode kiểu PyTorch có thể được tìm thấy trong Phụ lục B.1, chứa thêm chi tiết triển khai. Để
huấn luyện sử dụng phân loại có giám sát, trước tiên chúng tôi lấy token CLS zb = zL+1
1,b cho mỗi mẫu,
sau đó áp dụng một lớp tuyến tính; đầu ra của lớp tuyến tính này ub .= Wzb được sử dụng làm đầu vào cho
hàm mất mát cross-entropy tiêu chuẩn. Hàm mất mát tổng thể tính trung bình trên tất cả mẫu b ∈ [B].

Tập dữ liệu và tối ưu hóa. Chúng tôi chủ yếu xem xét ImageNet-1K [9] như bàn thử nghiệm cho kiến trúc của chúng tôi.
Cụ thể, chúng tôi áp dụng bộ tối ưu hóa Lion [71] để huấn luyện các mô hình CRATE với các kích thước mô hình khác nhau.
Trong khi đó, chúng tôi cũng đánh giá hiệu suất học chuyển giao của CRATE: bằng cách xem xét các mô hình
được huấn luyện trên ImageNet-1K như các mô hình được huấn luyện trước, chúng tôi tinh chỉnh CRATE trên một số tập dữ liệu
downstream thường được sử dụng (CIFAR10/100, Oxford Flowers, Oxford-IIT-Pets). Chi tiết thêm về
huấn luyện và tập dữ liệu có thể được tìm thấy trong Phụ lục B.1.

--- TRANG 9 ---
2 4 6 8 10 12
Chỉ số lớp - 
60070080090010001100Rc(Z) [khối SSA]
Đo tỷ lệ mã hóa qua các lớp
train
val
2 4 6 8 10 12
Chỉ số lớp - 
0.200.250.300.350.400.450.50Độ thưa [khối ISTA]
Đo độ thưa đầu ra qua các lớp
train
val

Hình 3: Trái: Số hạng nén Rc(Zℓ+1/2) của các đầu ra MSSA tại các lớp khác nhau. Phải: độ thưa
của đầu ra khối ISTA, ∥Zℓ+1∥0/(d·N), tại các lớp khác nhau. (Mô hình: CRATE-Small).

2 4 6 8 10 12
Chỉ số lớp - 
8009001000110012001300140015001600Rc(Z) [khối SSA]
Đo tỷ lệ mã hóa qua các lớp
rand init
epoch 1
epoch 20
epoch 150
2 4 6 8 10 12
Chỉ số lớp - 
0.150.200.250.300.350.400.450.50Độ thưa [khối ISTA]
Đo độ thưa đầu ra qua các lớp
rand init
epoch 1
epoch 20
epoch 150

Hình 4: Số hạng nén Rc(Z) (trái) và số hạng làm thưa ∥Z∥0/(d·N) (phải) qua các mô hình
được huấn luyện với số epoch khác nhau. (Mô hình: CRATE-Base).

3.1 Phân tích Theo Lớp Chuyên sâu của CRATE

Các lớp của CRATE có đạt được mục tiêu thiết kế của chúng không? Như được mô tả trong Phần 2.3 và Phần 2.4,
khối MSSA được thiết kế để tối ưu hóa số hạng nén Rc(Z) và khối ISTA để làm thưa
các biểu diễn token (tương ứng với số hạng làm thưa ∥Z∥0). Để hiểu liệu CRATE
có thực sự tối ưu hóa những số hạng này, cho mỗi lớp ℓ, chúng tôi đo (i) số hạng nén Rc(Zℓ+1/2)
trên các đầu ra khối MSSA Zℓ+1/2; và (ii) độ thưa ∥Zℓ+1∥0 trên các đầu ra khối ISTA Zℓ+1.
Cụ thể, chúng tôi đánh giá hai số hạng quan trọng này bằng cách sử dụng các mẫu huấn luyện/xác thực từ ImageNet-1K.
Cả hai số hạng đều được đánh giá ở mức mỗi mẫu và tính trung bình trên B = 103 mẫu.

Hình 3 cho thấy các biểu đồ của hai thước đo quan trọng này tại tất cả các lớp cho mô hình CRATE-small đã học.
Chúng tôi thấy rằng khi chỉ số lớp ℓ tăng, cả thước đo nén và làm thưa đều cải thiện
trong hầu hết trường hợp. Việc tăng thước đo độ thưa của lớp cuối cùng được gây ra bởi lớp tuyến tính
bổ sung cho phân loại.8 Những kết quả này gợi ý rằng CRATE phù hợp tốt với các mục tiêu thiết kế
ban đầu: một khi được học, nó về cơ bản học cách nén và làm thưa dần dần các biểu diễn thông qua
các lớp của nó. Ngoài ra, chúng tôi cũng đo các số hạng nén và làm thưa trên các mô hình CRATE
với các kích thước mô hình khác nhau cũng như các checkpoint mô hình trung gian và kết quả được hiển thị bằng
các biểu đồ trong Hình 5 của Phụ lục B.2. Các quan sát rất nhất quán trên tất cả các kích thước mô hình
khác nhau—cả số hạng nén và làm thưa đều cải thiện trong hầu hết các tình huống. Các mô hình với nhiều lớp hơn
có xu hướng tối ưu hóa các mục tiêu hiệu quả hơn, xác nhận hiểu biết của chúng tôi về vai trò của mỗi lớp.

Để thấy tác động của việc học, chúng tôi trình bày các đánh giá trên CRATE-Small được huấn luyện với số
epoch khác nhau trong Hình 4. Khi mô hình không được huấn luyện đủ (ví dụ chưa được huấn luyện), kiến trúc không
tối ưu hóa các mục tiêu một cách hiệu quả. Tuy nhiên, trong quá trình huấn luyện—học các không gian con Uℓ
[K]
và từ điển Dℓ tốt hơn—các khối được thiết kế bắt đầu tối ưu hóa các mục tiêu hiệu quả hơn nhiều.

Trực quan hóa các biểu diễn token theo lớp. Để hiểu rõ hơn về các biểu diễn token của CRATE,
chúng tôi trực quan hóa đầu ra của mỗi khối ISTA tại lớp ℓ trong Hình 6 của Phụ lục B.2.
Cụ thể, chúng tôi trực quan hóa Zℓ+1 thông qua các biểu đồ heatmap. Chúng tôi quan sát rằng đầu ra Zℓ+1 trở nên
thưa hơn khi lớp tăng. Hơn nữa, ngoài độ thưa, chúng tôi cũng thấy rằng Zℓ+1 trở nên

8 Lưu ý rằng các đặc trưng thưa (token) đã học cần được trộn trong lớp cuối cùng để dự đoán lớp.
Hiện tượng tăng thước đo độ thưa ở lớp cuối cùng gợi ý rằng mỗi lớp đối tượng có thể được
liên kết với một số đặc trưng, và một số đặc trưng này có khả năng được chia sẻ giữa các lớp khác nhau.

--- TRANG 10 ---
có cấu trúc hơn (tức là low-rank), điều này chỉ ra rằng tập hợp các biểu diễn token trở nên gần với
các không gian con tuyến tính hơn, xác nhận hình ảnh tinh thần của chúng tôi về hình học của mỗi lớp (như trong Hình 1).

Trực quan hóa các không gian con theo lớp trong tự chú ý đa đầu. Bây giờ chúng tôi trực quan hóa các ma trận Uℓ
[K]
được sử dụng trong khối MSSA. Trong Phần 2.3, chúng tôi giả định rằng Uℓ
[K] không liên kết để nắm bắt
các "góc nhìn" khác nhau của tập hợp token. Trong Hình 7 của Phụ lục B.2, trước tiên chúng tôi chuẩn hóa các cột
trong mỗi Uℓ
k, sau đó chúng tôi trực quan hóa [Uℓ
1, . . . , Uℓ
K]∗[Uℓ
1, . . . , Uℓ
K] ∈ RpK×pK. Khối (i, j)-th
trong mỗi tiểu hình tương ứng với (Uℓ
i)∗Uℓ
j cho i, j ∈ [K] tại một lớp cụ thể ℓ. Chúng tôi thấy rằng
Uℓ
[K] đã học xấp xỉ không liên kết, phù hợp tốt với các giả định của chúng tôi. Một quan sát thú vị
là Uℓ
[K] trở nên không liên kết hơn khi chỉ số lớp ℓ lớn hơn, điều này gợi ý rằng các biểu diễn token
có thể tách biệt hơn. Điều này phản ánh tình huống trong các mạng sâu phổ biến khác [57].

3.2 Đánh giá CRATE trên Tập dữ liệu và Tác vụ Thế giới Thực Lớn

Bây giờ chúng tôi nghiên cứu hiệu suất thực nghiệm của các mạng được đề xuất bằng cách đo độ chính xác top-1
của chúng trên ImageNet-1K cũng như hiệu suất học chuyển giao trên một số tập dữ liệu downstream được sử dụng rộng rãi.
Chúng tôi tóm tắt kết quả trong Bảng 1. Vì kiến trúc được thiết kế của chúng tôi tận dụng việc chia sẻ tham số trong
cả khối chú ý (MSSA) và khối MLP (ISTA), mô hình CRATE-Base của chúng tôi (22.08 triệu) có
số lượng tham số tương tự như ViT-Small (22.05 triệu).

Bảng 1: Độ chính xác Top 1 của CRATE trên các tập dữ liệu khác nhau với các quy mô mô hình khác nhau khi được huấn luyện trước trên ImageNet.
Đối với ImageNet/ImageNetReaL, chúng tôi trực tiếp đánh giá độ chính xác top-1. Đối với các tập dữ liệu khác, chúng tôi sử dụng các mô hình được
huấn luyện trước trên ImageNet làm khởi tạo và đánh giá hiệu suất học chuyển giao thông qua tinh chỉnh.

Tập dữ liệu CRATE-T CRATE-S CRATE-B CRATE-L ViT-T ViT-S
# tham số 6.09M 13.12M 22.80M 77.64M 5.72M 22.05M
ImageNet 66.7 69.2 70.8 71.3 71.5 72.4
ImageNet ReaL 74.0 76.0 76.5 77.4 78.3 78.4
CIFAR10 95.5 96.0 96.8 97.2 96.6 97.2
CIFAR100 78.9 81.0 82.7 83.6 81.8 83.2
Oxford Flowers-102 84.6 87.1 88.7 88.3 85.1 88.5
Oxford-IIIT-Pets 81.4 84.9 85.3 87.4 88.5 88.6

Từ Bảng 1, chúng tôi thấy rằng với số lượng tham số mô hình tương tự, mạng được đề xuất của chúng tôi
đạt được hiệu suất ImageNet-1K và học chuyển giao tương tự như ViT, mặc dù tính đơn giản và
khả năng diễn giải của thiết kế của chúng tôi. Hơn nữa, với cùng tập hợp siêu tham số huấn luyện, chúng tôi quan sát
hành vi mở rộng đầy hứa hẹn trong CRATE—chúng tôi liên tục cải thiện hiệu suất bằng cách mở rộng
kích thước mô hình. Để so sánh, việc mở rộng trực tiếp ViT trên ImageNet-1K không phải lúc nào cũng dẫn đến
cải thiện hiệu suất nhất quán được đo bằng độ chính xác top-1 [40]. Tóm lại, chúng tôi đạt được hiệu suất
đầy hứa hẹn trên các tập dữ liệu thế giới thực quy mô lớn bằng cách trực tiếp triển khai kiến trúc có nguyên tắc của chúng tôi.

4 Kết luận

Trong bài báo này, chúng tôi đề xuất một khung lý thuyết mới cho phép chúng tôi suy ra các kiến trúc mạng sâu
giống transformer như các sơ đồ tối ưu hóa tăng dần để học biểu diễn nén và thưa của dữ liệu đầu vào
(hoặc tập hợp token). Các kiến trúc sâu được suy ra và học như vậy không chỉ
có thể diễn giải hoàn toàn về mặt toán học, mà còn nhất quán ở cấp độ từng lớp với mục tiêu thiết kế của chúng.
Mặc dù có lý nhất là đơn giản nhất trong tất cả các thiết kế có thể, những mạng này đã
chứng minh hiệu suất trên các tập dữ liệu và tác vụ thế giới thực quy mô lớn gần với các transformer dày dặn kinh nghiệm.
Chúng tôi tin rằng công trình này thực sự giúp thu hẹp khoảng cách giữa lý thuyết và thực hành của mạng thần kinh sâu
cũng như giúp thống nhất các phương pháp có vẻ riêng biệt để học và biểu diễn phân phối dữ liệu.
Có lẽ quan trọng hơn đối với các nhà thực hành, khung làm việc của chúng tôi cung cấp hướng dẫn lý thuyết để thiết kế
và biện minh các kiến trúc sâu mới, có tiềm năng mạnh mẽ hơn, cho học biểu diễn.

--- TRANG 11 ---
Tài liệu tham khảo
[1] Charles M Stein. "Estimation of the Mean of a Multivariate Normal Distribution". The Annals
of Statistics 9.6 (Nov. 1981), pp. 1135–1151. 5.
[2] Bruno A Olshausen and David J Field. "Sparse coding with an overcomplete basis set: A
strategy employed by V1?" Vision research 37.23 (1997), pp. 3311–3325. 2.
[3] David L Donoho and Carrie Grimes. "Image Manifolds which are Isometric to Euclidean
Space". Journal of mathematical imaging and vision 23.1 (July 2005), pp. 5–24. 2.
[4] Aapo Hyvärinen. "Estimation of Non-Normalized Statistical Models by Score Matching".
Journal of machine learning research: JMLR 6.24 (2005), pp. 695–709. 5.
[5] Michael B Wakin, David L Donoho, Hyeokho Choi, and Richard G Baraniuk. "The multiscale
structure of non-differentiable image manifolds". Wavelets XI . V ol. 5914. SPIE. 2005, pp. 413–
429. 2.
[6] Yi Ma, Harm Derksen, Wei Hong, and John Wright. "Segmentation of multivariate mixed data
via lossy data coding and compression". PAMI (2007). 2, 3, 5.
[7] Maria-Elena Nilsback and Andrew Zisserman. "Automated flower classification over a large
number of classes". 2008 Sixth Indian Conference on Computer Vision, Graphics & Image
Processing . IEEE. 2008, pp. 722–729. 25.
[8] Amir Beck and Marc Teboulle. "A fast iterative shrinkage-thresholding algorithm for linear
inverse problems". SIAM journal on imaging sciences 2.1 (2009), pp. 183–202. 7.
[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. "Imagenet: A large-
scale hierarchical image database". 2009 IEEE conference on computer vision and pattern
recognition . Ieee. 2009, pp. 248–255. 8.
[10] Alex Krizhevsky, Geoffrey Hinton, et al. "Learning multiple layers of features from tiny
images" (2009). 25.
[11] Karol Gregor and Yann LeCun. "Learning fast approximations of sparse coding". Proceedings
of the 27th International Conference on International Conference on Machine Learning .
Omnipress. 2010, pp. 399–406. 2.
[12] László Györfi, Michael Kohler, Adam Krzyzak, and Harro Walk. A Distribution-Free Theory
of Nonparametric Regression . Springer New York, Dec. 2010. 4.
[13] Bradley Efron. "Tweedie's Formula and Selection Bias". Journal of the American Statistical
Association 106.496 (2011), pp. 1602–1614. 2, 5, 15.
[14] Martin Raphan and Eero P Simoncelli. "Least squares estimation without priors or supervision".
Neural computation 23.2 (Feb. 2011), pp. 374–420. 5.
[15] Pascal Vincent. "A connection between score matching and denoising autoencoders". Neural
computation 23.7 (July 2011), pp. 1661–1674. 5.
[16] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. "Cats and dogs". 2012
IEEE conference on computer vision and pattern recognition . IEEE. 2012, pp. 3498–3505. 25.
[17] Daniel A Spielman, Huan Wang, and John Wright. "Exact Recovery of Sparsely-Used Dictio-
naries" (June 2012). arXiv: 1206.5882 [cs.LG] . 2.
[18] Joan Bruna and Stéphane Mallat. "Invariant scattering convolution networks". IEEE transac-
tions on pattern analysis and machine intelligence 35.8 (Aug. 2013), pp. 1872–1886. 2.
[19] Peyman Milanfar. "A Tour of Modern Image Filtering: New Insights and Methods, Both
Practical and Theoretical". IEEE Signal Processing Magazine 30.1 (Jan. 2013), pp. 106–128.
5.
[20] Singanallur V Venkatakrishnan, Charles A Bouman, and Brendt Wohlberg. "Plug-and-Play pri-
ors for model based reconstruction". 2013 IEEE Global Conference on Signal and Information
Processing . Dec. 2013, pp. 945–948. 5.
[21] Rémi Gribonval, Rodolphe Jenatton, and Francis Bach. "Sparse and spurious: dictionary
learning with noise and outliers" (July 2014). arXiv: 1407.5155 [cs.LG] . 2.
[22] Jascha Sohl-Dickstein, Eric A Weiss, Niru Maheswaranathan, and Surya Ganguli. "Deep
Unsupervised Learning using Nonequilibrium Thermodynamics" (Mar. 2015). arXiv: 1503.
03585 [cs.LG] . 2, 5.
[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. "Deep Residual Learning for
Image Recognition". 2016 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR) . June 2016, pp. 770–778. 1.

--- TRANG 12 ---
[24] René Vidal, Yi Ma, and Shankar Sastry. Generalized Principal Component Analysis . Springer
Verlag, 2016. 5.
[25] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. "Mask R-CNN" (Mar. 2017).
arXiv: 1703.06870 [cs.CV] . 1.
[26] Ilya Loshchilov and Frank Hutter. "Decoupled weight decay regularization". arXiv preprint
arXiv:1711.05101 (2017). 25.
[27] Yaniv Romano, Michael Elad, and Peyman Milanfar. "The Little Engine That Could: Regular-
ization by Denoising (RED)". SIAM journal on imaging sciences 10.4 (Jan. 2017), pp. 1804–
1844. 5.
[28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. "Attention is all you need". Advances in neural informa-
tion processing systems 30 (2017). 1, 3, 6.
[29] Yubei Chen, Dylan Paiton, and Bruno Olshausen. "The sparse manifold transform". Advances
in neural information processing systems 31 (2018). 2.
[30] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. "Bert: Pre-training of
deep bidirectional transformers for language understanding". arXiv preprint arXiv:1810.04805
(2018). 1.
[31] Tero Karras, Samuli Laine, and Timo Aila. "A Style-Based Generator Architecture for Genera-
tive Adversarial Networks" (Dec. 2018). arXiv: 1812.04948 [cs.NE] . 1.
[32] Vardan Papyan, Yaniv Romano, Jeremias Sulam, and Michael Elad. "Theoretical Foundations
of Deep Learning via Sparse Representations: A Multilayer Sparse Model and Its Connection
to Convolutional Neural Networks". IEEE Signal Processing Magazine 35.4 (July 2018),
pp. 72–89. 2.
[33] Xiaoxia Sun, Nasser M Nasrabadi, and Trac D Tran. "Supervised deep sparse coding networks".
2018 25th IEEE International Conference on Image Processing (ICIP) . IEEE. 2018, pp. 346–
350. 7.
[34] Yang Song and Stefano Ermon. "Generative Modeling by Estimating Gradients of the Data
Distribution" (July 2019). arXiv: 1907.05600 [cs.LG] . 2.
[35] John Zarka, Louis Thiry, Tomás Angles, and Stéphane Mallat. "Deep network classification
by scattering and homotopy dictionary learning". arXiv preprint arXiv:1910.03561 (2019). 7.
[36] Lucas Beyer, Olivier J Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord.
"Are we done with imagenet?" arXiv preprint arXiv:2006.07159 (2020). 25.
[37] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. "Language
models are few-shot learners". Advances in neural information processing systems 33 (2020),
pp. 1877–1901. 1.
[38] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and
Sergey Zagoruyko. "End-to-End Object Detection with Transformers" (May 2020). arXiv:
2005.12872 [cs.CV] . 1.
[39] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. "A Simple Frame-
work for Contrastive Learning of Visual Representations". Proceedings of the 37th Interna-
tional Conference on Machine Learning . Ed. by Hal Daumé Iii and Aarti Singh. V ol. 119.
Proceedings of Machine Learning Research. PMLR, 2020, pp. 1597–1607. 1.
[40] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
et al. "An image is worth 16x16 words: Transformers for image recognition at scale". arXiv
preprint arXiv:2010.11929 (2020). 1, 3, 10.
[41] Jonathan Ho, Ajay Jain, and Pieter Abbeel. "Denoising diffusion probabilistic models". Ad-
vances in Neural Information Processing Systems 33 (2020), pp. 6840–6851. 2.
[42] Zahra Kadkhodaie and Eero P Simoncelli. "Solving Linear Inverse Problems Using the Prior
Implicit in a Denoiser" (July 2020). arXiv: 2007.13640 [cs.CV] . 5.
[43] Jiaming Song, Chenlin Meng, and Stefano Ermon. "Denoising Diffusion Implicit Models"
(Oct. 2020). arXiv: 2010.02502 [cs.LG] . 2, 5.
[44] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon,
and Ben Poole. "Score-Based Generative Modeling through Stochastic Differential Equations"
(Nov. 2020). arXiv: 2011.13456 [cs.LG] . 2, 5.

--- TRANG 13 ---
[45] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola.
"What makes for good views for contrastive learning?" Advances in neural information pro-
cessing systems 33 (2020), pp. 6827–6839. 2.
[46] Yaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. "Learning Diverse
and Discriminative Representations via the Principle of Maximal Coding Rate Reduction".
Advances in Neural Information Processing Systems 33 (2020), pp. 9422–9434. 2, 3, 5, 18, 23.
[47] Yuexiang Zhai, Zitong Yang, Zhenyu Liao, John Wright, and Yi Ma. "Complete dictionary
learning via l 4-norm maximization over the orthogonal group". The Journal of Machine
Learning Research 21.1 (2020), pp. 6622–6689. 2.
[48] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu ˇci´c, and Cordelia
Schmid. "Vivit: A video vision transformer". Proceedings of the IEEE/CVF international
conference on computer vision . 2021, pp. 6836–6846. 1.
[49] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. "Masked
Autoencoders Are Scalable Vision Learners" (Nov. 2021). arXiv: 2111.06377 [cs.CV] . 1.
[50] Geoffrey Hinton. How to represent part-whole hierarchies in a neural network . 2021. arXiv:
2102.12627 [cs.CV] . 6.
[51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya
Sutskever. "Learning Transferable Visual Models From Natural Language Supervision". Pro-
ceedings of the 38th International Conference on Machine Learning . Ed. by Marina Meila and
Tong Zhang. V ol. 139. Proceedings of Machine Learning Research. PMLR, 2021, pp. 8748–
8763. 1.
[52] Bahareh Tolooshams and Demba Ba. "Stable and Interpretable Unrolled Dictionary Learning".
arXiv preprint arXiv:2106.00058 (2021). 2.
[53] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas
Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic,
and Alexey Dosovitskiy. "MLP-Mixer: An all-MLP Architecture for Vision" (May 2021).
arXiv: 2105.01601 [cs.CV] . 22.
[54] Kwan Ho Ryan Chan, Yaodong Yu, Chong You, Haozhi Qi, John Wright, and Yi Ma. "ReduNet:
A White-box Deep Network from the Principle of Maximizing Rate Reduction". Journal of
Machine Learning Research 23.114 (2022), pp. 1–103. 2–8, 18, 19.
[55] Hongrui Chen, Holden Lee, and Jianfeng Lu. "Improved Analysis of Score-based Generative
Modeling: User-Friendly Bounds under Minimal Smoothness Assumptions". arXiv preprint
arXiv:2211.01916 (2022). 2.
[56] Yuan Gong, Andrew Rouditchenko, Alexander H Liu, David Harwath, Leonid Karlinsky, Hilde
Kuehne, and James R Glass. "Contrastive audio-visual masked autoencoder". The Eleventh
International Conference on Learning Representations . 2022. 1.
[57] Hangfeng He and Weijie J Su. "A law of data separation in deep learning". arXiv preprint
arXiv:2210.17020 (2022). 10.
[58] Geoffrey Hinton G The Forward-Forward Algorithm: Some Preliminary Investigations . 2022.
arXiv: 2212.13345 [cs.LG] . 2.
[59] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. "Elucidating the design space of
diffusion-based generative models". arXiv preprint arXiv:2206.00364 (2022). 2, 15.
[60] Frederic Koehler, Alexander Heckett, and Andrej Risteski. "Statistical Efficiency of Score
Matching: The View from Isoperimetry" (Oct. 2022). arXiv: 2210.00726 [cs.LG] . 2.
[61] Yi Ma, Doris Tsao, and Heung-Yeung Shum. "On the principles of parsimony and self-
consistency for the emergence of intelligence". Frontiers of Information Technology & Elec-
tronic Engineering 23.9 (2022), pp. 1298–1323. 1, 3.
[62] Druv Pai, Michael Psenka, Chih-Yuan Chiu, Manxi Wu, Edgar Dobriban, and Yi Ma. "Pursuit
of a discriminative representation for multiple subspaces via sequential games". arXiv preprint
arXiv:2206.09120 (2022). 2.
[63] Mary Phuong and Marcus Hutter. "Formal algorithms for transformers". arXiv preprint
arXiv:2207.09238 (2022). 20.
[64] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
"High-resolution image synthesis with latent diffusion models". Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition . 2022, pp. 10684–10695. 1, 2.

--- TRANG 14 ---
[65] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed
Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, Tim
Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. "Photorealistic Text-to-Image
Diffusion Models with Deep Language Understanding" (May 2022). arXiv: 2205.11487
[cs.CV] . 1.
[66] Asher Trockman, Devin Willmott, and J Zico Kolter. "Understanding the Covariance Structure
of Convolutional Filters" (Oct. 2022). arXiv: 2210.03651 [cs.CV] . 22.
[67] Rene Vidal. Attention: Self-Expression Is All You Need . Unpublished; available: https :
//openreview.net/forum?id=MmujBClawFo . 2022. 2.
[68] Haoqing Wang, Xun Guo, Zhi-Hong Deng, and Yan Lu. "Rethinking minimal sufficient
representation in contrastive learning". Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition . 2022, pp. 16041–16050. 2.
[69] John Wright and Yi Ma. High-Dimensional Data Analysis with Low-Dimensional Models:
Principles, Computation, and Applications . Cambridge University Press, 2022. 4, 21, 22.
[70] Sitan Chen, Giannis Daras, and Alexandros G Dimakis. "Restoration-Degradation Beyond
Linear Diffusions: A Non-Asymptotic Analysis For DDIM-Type Samplers" (Mar. 2023). arXiv:
2303.03384 [cs.LG] . 5.
[71] Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham,
Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, et al. "Symbolic discovery of optimization
algorithms". arXiv preprint arXiv:2302.06675 (2023). 8, 25.
[72] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin
Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al.
"Scaling vision transformers to 22 billion parameters". arXiv preprint arXiv:2302.05442
(2023). 1.
[73] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,
Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, Piotr Dollár, and Ross
Girshick. "Segment Anything" (Apr. 2023). arXiv: 2304.02643 [cs.CV] . 1.
[74] Hongkang Li, Meng Wang, Sijia Liu, and Pin-Yu Chen. "A Theoretical Understanding of
shallow Vision Transformers: Learning, Generalization, and Sample Complexity". arXiv
preprint arXiv:2302.06015 (2023). 2.
[75] Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li, Ankit Singh Rawat, Sashank J Reddi,
Ke Ye, Felix Chern, Felix Yu, Ruiqi Guo, and Sanjiv Kumar. "The Lazy Neuron Phenomenon:
On Emergence of Activation Sparsity in Transformers". The Eleventh International Conference
on Learning Representations . 2023. 22.
[76] Ravid Shwartz-Ziv and Yann LeCun. "To Compress or Not to Compress–Self-Supervised
Learning and Information Theory: A Review". arXiv preprint arXiv:2304.09355 (2023). 2.
[77] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. "Consistency models". arXiv
preprint arXiv:2303.01469 (2023). 2.

--- TRANG 15 ---
Phụ lục

A Chi tiết Kỹ thuật từ Phần 2

A.1 Phụ trợ cho Phần 2.2

Trước tiên chúng tôi muốn tái khẳng định các đóng góp cốt lõi của phương pháp chúng tôi trong Phần 2.2 ở mức
kỹ thuật hơi cao hơn. Các kết nối giữa khử nhiễu và score matching được hiểu rõ [59], và
tính toán hàm khử nhiễu tối ưu (tức là kỳ vọng có điều kiện) so với mô hình mixture-of-
Gaussians là một tính toán khá đơn giản cho các công cụ hiện có như công thức Tweedie [13].
Đây không phải là những đóng góp chính của chúng tôi. Thay vào đó, những đóng góp chính của Phần 2.2 có hai mặt:
• Đầu tiên, chúng tôi chứng minh một cơ chế để học biểu diễn thông qua khử nhiễu trong một
mô hình dữ liệu hỗn hợp Gaussian lý tưởng hóa cho một token đơn (tức là với độ dài chuỗi N = 1).
• Thứ hai, chúng tôi minh họa sự tương đồng giữa một sơ đồ học biểu diễn được suy ra như vậy
và các lớp tự chú ý hiện có trong transformer (với độ dài chuỗi 1), do đó
chứng minh một diễn giải của lớp tự chú ý như một cơ chế tổng quát để
khử nhiễu so với mô hình biên mixture-of-Gaussian cho một tập hợp token.

Bây giờ chúng tôi tạo ra các chứng minh được ám chỉ trong Phần 2.2, phần lớn tạo thành các khía cạnh kỹ thuật của
đóng góp được liệt kê đầu tiên. Để đơn giản hóa các chứng minh, chúng tôi sử dụng các tương ứng ký hiệu sau:
x 7→ zℓ, z 7→ zℓ+1, và σ 7→ σℓ.

Mệnh đề 1. Để u1, . . . , uK ∈ Rd độc lập và có phân phối uk ∼ N(0, Σk) cho
Σk ⪰ 0, và để z nhận giá trị uk với xác suất πk > 0. Để w ∼ N(0, Id) độc lập với z.
Để x .= z + σw. Để x 7→ q(x) là mật độ của x. Chúng tôi định nghĩa
Mk .= (Σk + σ2Id)−1/2 (19)
và giả sử rằng πi det(Mi) = πj det(Mj) cho tất cả 1 ≤ i ≤ j ≤ K. Khi đó chúng ta có
∇x log q(x) (20)
= −[M1, ···, MK]
diag
softmax
−1
2
∥M∗
1x∥2
2 ...
∥M∗
Kx∥2
2


⊗ Id

M∗
1x
...
M∗
Kx
, (21)

trong đó ⊗ biểu thị tích Kronecker, tức là ma trận khối được định nghĩa bởi
A ⊗ B =
A11B ··· A1nB
......... 
Am1B ··· AmnB
 (22)

Chứng minh. Để u là biến ngẫu nhiên đa thức sao cho z = zu, do đó u có hàm khối lượng xác suất
π. Khi đó theo luật xác suất toàn phần, chúng ta có
∇x log q(x) = ∇x log KX
k=1 q(x|k)πk (23)
= PK
k=1 πk∇xq(x|k)
PK
k=1 q(x|k)πk (24)

trong đó q(x|k) là mật độ có điều kiện của x cho sự kiện {u = k}. Để tính lượng này,
lưu ý rằng có điều kiện trên giá trị của u, chúng ta có
x = zu + σw ∼ N(0, Σu + σ2Id). (25)

Do đó chúng ta có
q(x|k) = 1p
(2π)d det(Σk + σ2Id) exp
−1
2 x∗(Σk + σ2Id)−1x
, (26)

Điều này cho
∇xq(x|k) = −q(x|k) · (Σk + σ2Id)−1x. (27)

Kết hợp tất cả lại với nhau, chúng ta có được
∇x log q(x) (28)
= −PK
k=1 q(x|k)πk · (Σk + σ2Id)−1x
PK
k=1 q(x|k)πk (29)
= −PK
k=1 πk det(Σk + σ2Id)−1/2 exp
−1
2 x∗(Σk + σ2Id)−1x
· (Σk + σ2Id)−1x
PK
k=1 πk det(Σk + σ2Id)−1/2 exp
−1
2 x∗(Σk + σ2Id)−1x . (30)

Bây giờ định nghĩa Mk .= (Σk + σ2Id)−1/2. Với ký hiệu này, chúng ta có
∇x log q(x) = −PK
k=1 πk det(Mk) exp
−1
2 x∗MkM∗
kx
· MkM∗
kx
PK
k=1 πk det(Mk) exp
−1
2 x∗MkM∗
kx (31)
= −PK
k=1 πk det(Mk) exp
−1
2 ∥M∗
kx∥2
2
· MkM∗
kx
PK
k=1 πk det(Mk) exp
−1
2 x∗MkM∗
kx . (32)

Cho giả định của chúng tôi rằng mỗi πk det(Mk) giống nhau, chúng ta có
∇x log q(x) (33)
= −PK
k=1 πk det(Mk) exp
−1
2 ∥M∗
kx∥2
2
· MkM∗
kx
PK
k=1 πk det(Mk) exp
−1
2 ∥M∗
kx∥2
2 (34)
= −PK
k=1 exp
−1
2 ∥M∗
kx∥2
2
· MkM∗
kx
PK
k=1 exp
−1
2 ∥M∗
kx∥2
2 (35)
= −KX
k=1 e∗
k softmax
−1
2
∥M∗
1x∥2
2 ...
∥M∗
Kx∥2
2

MkM∗
kx (36)
= −[M1, . . . , MK]
diag
softmax
−1
2
∥M∗
1x∥2
2 ...
∥M∗
Kx∥2
2


⊗ Id

M∗
1x
...
M∗
Kx
. (37)

Bây giờ chúng tôi cung cấp một biện minh cuối cùng cho kết quả được trích dẫn trong Phần 2.2.

Xấp xỉ 2. Trong thiết lập của Mệnh đề 1, chéo hóa Σk = UkΛkU∗
k trong đó Uk ∈ Rd×p
là trực giao và Λk ≻ 0 ∈ Rp×p là chéo.9 Khi đó chúng ta có xấp xỉ
E[z|x] ≈ [U1, . . . , UK]
diag
softmax
1
2σ2
∥U∗
1x∥2
2 ...
∥U∗
Kx∥2
2


⊗ Ip

U∗
1x
...
U∗
Kx
. (38)

Chứng minh. Chúng ta có
∇x log q(x) = −KX
k=1 e∗
k softmax
−1
2
∥M∗
1x∥2
2 ...
∥M∗
Kx∥2
2

MkM∗
kx (39)
= −KX
k=1 e∗
k softmax
−1
2σ2
∥σM∗
1x∥2
2 ...
∥σM∗
Kx∥2
2

MkM∗
kx (40)

9 Giả định này có thể được nới lỏng dễ dàng thành Λk ⪰ 0 cho tất cả k, nhưng cần thêm ký hiệu để xử lý, và
dạng của nghiệm không thay đổi. Do đó chúng tôi xử lý trường hợp tất cả ma trận có rank đầy đủ để đơn giản.

--- TRANG 16 ---
= −KX
k=1 e∗
k softmax
1
2σ2
∥x∥2
2 − ∥σM∗
1x∥2
2 ...
∥x∥2
2 − ∥σM∗
Kx∥2
2

MkM∗
kx. (41)

Bây giờ định nghĩa Pk .= Id − σMk, và để U⊥
k ∈ Rd×(d−p) là phần bù trực giao của Uk. Khi đó
chúng ta có
Pk = Id − σMk (42)
= Id − σ
Σk + σ2Id−1/2 (43)
= Id − σ
UkU⊥
k
Λk 0
0 0U∗
k
(U⊥
k)∗
+ σ2Id−1/2
(44)
= Id − σ
UkU⊥
k
Λk + σ2Ip 0
0 σ2Id−pU∗
k
(U⊥
k)∗−1/2
(45)
= Id −
UkU⊥
k
σ(Λk + σ2Ip)−1/2 0
0 σ · (σ2)−1/2Id−pU∗
k
(U⊥
k)∗
(46)
= Id −
UkU⊥
k
(σ−2Λk + Ip)−1/2 0
0 Id−pU∗
k
(U⊥
k)∗
(47)
=
UkU⊥
k
Ip − (σ−2Λk + Ip)−1/2 0
0 0U∗
k
(U⊥
k)∗
(48)
≈
UkU⊥
k
Ip 0
0 0U∗
k
(U⊥
k)∗
(49)
= UkU∗
k. (50)

Do đó Pk xấp xỉ một phép chiếu khi σ nhỏ. Dưới mối quan hệ đại số này, chúng ta có
∇x log q(x) (51)
= −KX
k=1 e∗
k softmax
1
2σ2
∥x∥2
2 − ∥σM∗
1x∥2
2 ...
∥x∥2
2 − ∥σM∗
Kx∥2
2

MkM∗
kx (52)
= −1
σ2 KX
k=1 e∗
k softmax
1
2σ2
∥x∥2
2 − ∥(Id − P1)∗x∥2
2 ...
∥x∥2
2 − ∥(Id − PK)∗x∥2
2

(Id − Pk)(Id − Pk)∗x (53)
≈ −1
σ2 KX
k=1 e∗
k softmax
1
2σ2
∥P∗
1x∥2
2 ...
∥P∗
Kx∥2
2

(Id − Pk)(Id − Pk)∗x (54)
≈ −1
σ2 KX
k=1 e∗
k softmax
1
2σ2
∥P∗
1x∥2
2 ...
∥P∗
Kx∥2
2

(Id − Pk)∗x (55)
= −x
σ2 KX
k=1 e∗
k softmax
1
2σ2
∥P∗
1x∥2
2 ...
∥P∗
Kx∥2
2

+ 1
σ2 KX
k=1 e∗
k softmax
1
2σ2
∥P∗
1x∥2
2 ...
∥P∗
Kx∥2
2

P∗
kx
(56)
= −1
σ2 x + 1
σ2 KX
k=1 e∗
k softmax
1
2σ2
∥P∗
1x∥2
2 ...
∥P∗
Kx∥2
2

P∗
kx (57)
≈ −1
σ2 x + 1
σ2 KX
k=1 e∗
k softmax
1
2σ2
∥U∗
1x∥2
2 ...
∥U∗
Kx∥2
2

UkU∗
kx (58)

--- TRANG 17 ---
= −1
σ2 x + 1
σ2 [U1, ···, UK]
diag
softmax
1
2σ2
∥U∗
1x∥2
2 ...
∥U∗
Kx∥2
2


⊗ Ip

U∗
1x
...
U∗
Kx
. (59)

Thay điều này vào công thức Tweedie, chúng ta có
E[z|x] ≈ [U1, ···, UK]
diag
softmax
1
2σ2
∥U∗
1x∥2
2 ...
∥U∗
Kx∥2
2


⊗ Ip

U∗
1x
...
U∗
Kx
. (60)

Nhận xét 3. Mặc dù Xấp xỉ 2 được phát biểu như một xấp xỉ thay vì như một mệnh đề, chúng tôi
tin rằng không quá khó khăn để chuyển đổi nó thành một phát biểu về tương đương tiệm cận khi σ → 0 (đặc biệt, giữ cho σ dưới giá trị eigen nhỏ nhất (khác không) của bất kỳ
Σk nào. Hầu hết các xấp xỉ được thực hiện trong việc suy ra Xấp xỉ 2 có thể ngay lập tức được biến thành
các khẳng định tiệm cận; điểm hơi tinh tế duy nhất là xử lý softmax, có thể được thực hiện
bằng cách sử dụng hành vi hội tụ "nhiệt độ cao" tiêu chuẩn của hàm softmax (đặc biệt, khi
σ → 0 trong các biểu thức của chúng tôi, softmax tập trung vào "đầu tốt nhất").

A.2 Phụ trợ cho Phần 2.3

Chúng tôi một lần nữa muốn tái khẳng định đóng góp cốt lõi của phương pháp chúng tôi trong Phần 2.3. Việc áp dụng
góc nhìn nén vào học biểu diễn đã được thảo luận trước đây, ví dụ trong dòng
công trình giảm tỷ lệ mã hóa tối đa [46]. Trong Phần 2.3, chúng tôi cung cấp các đóng góp
và phát triển sau cho góc nhìn này:
• Chúng tôi đề xuất hàm tỷ lệ mã hóa tổng quát Rc(·; U[K]) đo tỷ lệ mã hóa
so với một tập hợp các không gian con U[K] thay vì một tập hợp các lớp (như trong [46, 54]), làm cho
công thức cơ bản không có giám sát.
• Sau đó chúng tôi cho thấy nếu chúng ta áp dụng khung tối thiểu hóa xen kẽ của mục tiêu giảm tỷ lệ
thưa, thì việc triển khai bước xen kẽ đầu tiên — gradient descent trên mục tiêu tỷ lệ
mã hóa này — gần như chính xác khôi phục cơ chế chú ý đa đầu phổ biến được tìm thấy
trong mạng transformer (ngoại trừ các toán tử query/key/value đều là cùng một phép toán
U∗
k bây giờ, mà chúng tôi diễn giải như phép chiếu lên một không gian con đơn).

Trong quá trình đóng góp thứ hai, và trong các chứng minh sau, chúng tôi thực hiện một số xấp xỉ
đơn giản và giả định kỹ thuật. Tính hợp lệ của những giả định này có thể được khám phá, và
các xấp xỉ được tinh chỉnh, tất cả cung cấp một toán tử giống tự chú ý phức tạp hơn (và có thể hiệu suất hơn)
kết quả. Vì mục đích rõ ràng về kỹ thuật và đơn giản trong công trình này, chúng tôi thực hiện có lẽ
các lựa chọn đơn giản nhất có thể. Kết quả là, chúng tôi không khẳng định rằng mạng của chúng tôi được thiết kế
tối ưu, mà thay vào đó các nguyên tắc chúng tôi phát triển trong công trình này (nén, khử nhiễu, làm thưa,
tối ưu hóa triển khai) có thể cung cấp xương sống cho các kiến trúc mạng vượt trội và dễ diễn giải hơn nhiều
trong tương lai trên các tác vụ khác nhau. Như hiện tại, với thiết kế đơn giản, đơn giản và dễ diễn giải
của chúng tôi, chúng tôi vẫn có được kết quả khái niệm có ý nghĩa và hiệu suất thực nghiệm rất vững chắc.

Bây giờ chúng tôi đưa ra sự suy ra của xấp xỉ được ám chỉ trong Phần 2.3.

Xấp xỉ 4. Để Z ∈ Rd×N có các cột đơn vị chuẩn, và U[K] = (U1, . . . , UK) sao cho
mỗi Uk ∈ Rd×p là ma trận trực giao, (Uk)K
k=1 không liên kết, và các cột của Z
xấp xỉ nằm trên SK
k=1 Span(Uk). Để γ = p
Nε2. Để κ > 0. Khi đó
Z − κ∇Z Rc(Z|U[K]) ≈ (1 − κγ)Z + κγMSSA(Z|U[K]), (61)

trong đó như trong Phần 2.3 chúng ta có
SSA(Z|Uk) = (U∗
kZ) softmax((U∗
kZ)∗(U∗
kZ)), (62)
MSSA(Z|U[K]) = γ[U1, . . . , UK]
SSA(Z|U1)
...
SSA(Z|UK)
, (63)

trong đó softmax(·) là toán tử softmax (áp dụng cho mỗi cột của ma trận đầu vào), tức là,
softmax(v) = 1
Pn
i=1 evi
ev1
...
evn
, (64)
softmax([v1, . . . , vK]) = [softmax(v1), . . . , softmax(vK)]. (65)

Chứng minh. Theo (9), gradient ∇Z Rc(Z; U[K]) là
∇Z Rc(Z; U[K]) = γ KX
k=1 UkU∗
kZ(I + γ(U∗
kZ)∗(U∗
kZ))−1. (66)

Lưu ý rằng theo [54], gradient chính xác là phần dư của hồi quy ridge cho mỗi
token (được chiếu) U∗
k zi sử dụng các token được chiếu khác U∗
k zj như các biến hồi quy, do đó là phần dư
của tự hồi quy.

Tuy nhiên, như chúng ta đã thấy trong công trình của ReduNet [54], việc tính nghịch đảo
(I + γ(U∗
kZ)∗(U∗
kZ))−1 có thể tốn kém. Do đó vì hiệu quả tính toán, chúng ta có thể xấp xỉ
nó với số hạng bậc nhất của khai triển von Neumann của nó:
∇Z Rc(Z; U[K]) = γ KX
k=1 UkU∗
kZ
I + γ(U∗
kZ)∗(U∗
kZ)−1
(67)
≈ γ KX
k=1 UkU∗
kZ
I − γ(U∗
kZ)∗(U∗
kZ)
(68)
= γ KX
k=1 Uk
U∗
kZ − γU∗
kZ[(U∗
kZ)∗(U∗
kZ)]
(69)

Lưu ý rằng số hạng (U∗
kZ)∗(U∗
kZ) là tự tương quan giữa các token được chiếu. Vì
các token Z có thể từ các không gian con khác nhau, chúng ta muốn chỉ sử dụng các token thuộc về
cùng một không gian con để tự hồi quy và nén. Do đó chúng ta có thể chuyển đổi số hạng tương quan trên
thành chỉ báo thành viên không gian con với phép toán softmax, từ đó (69) trở thành
∇Z Rc(Z; U[K]) ≈ γ KX
k=1 Uk
U∗
kZ − γU∗
kZ[(U∗
kZ)∗(U∗
kZ)]
(70)
≈ γ KX
k=1 UkU∗
kZ − γ2 KX
k=1 Uk
U∗
kZ softmax((U∗
kZ)∗(U∗
kZ))
(71)

Sau đó, chúng ta có thể viết lại xấp xỉ trên cho gradient của Rc như:
∇Z Rc(Z; U[K]) ≈ γ KX
k=1 UkU∗
kZ − γ2 KX
k=1 Uk(U∗
kZ softmax((U∗
kZ)∗(U∗
kZ))) (72)
= γ KX
k=1 UkU∗
kZ − γ2 KX
k=1 UkSSA(Z|Uk) (73)
= 
γ KX
k=1 UkU∗
k!
Z
| {z }
≈γZ − γ2[U1, ···, UK]
SSA(Z|U1)
...
SSA(Z|UK)
 (74)
≈ γZ − γ2[U1, ···, UK]
SSA(Z|U1)
...
SSA(Z|UK)
. (75)

--- TRANG 19 ---
Do đó bước gradient descent với tốc độ học κ > 0 cho
Z − κ∇Z Rc(Z|U[K]) ≈ (1 − κγ)Z + κγ2[U1, . . . , UK]
SSA(Z|U1)
...
SSA(Z|UK)
. (76)

A.3 Phụ trợ cho Phần 2.4

Chúng tôi một lần nữa muốn tái khẳng định đóng góp cốt lõi của phương pháp chúng tôi trong Phần 2.4.
• Trong khung tối thiểu hóa xen kẽ của mục tiêu giảm tỷ lệ thưa, chúng tôi cho thấy rằng bước xen kẽ thứ hai — gradient descent trên tỷ lệ mã hóa tổng thể cộng với
số hạng chính quy thưa — có các kết nối heuristic với một tối ưu hóa LASSO cụ thể.
• Chúng tôi cho thấy rằng việc triển khai bước gradient proximal để giải quyết tối ưu hóa LASSO này
giống với MLP ngay sau lớp tự chú ý trong các khối transformer.

Trong văn bản chính, kết nối của chúng tôi giữa bước thứ hai của tối thiểu hóa xen kẽ và
tối ưu hóa LASSO là ở mức cao và heuristic. Theo một nghĩa nào đó, việc lựa chọn đặt bước tối thiểu hóa
như một LASSO là một lựa chọn đơn giản, đáng tin cậy và dễ diễn giải hoạt động tốt trong thực tế, nhưng
tuy nhiên không được hỗ trợ bởi biện minh lý thuyết nghiêm ngặt. Trong phần tiếp theo, chúng tôi
cung cấp một biện minh toán học cho việc tái công thức hóa bước tối thiểu hóa bằng khung
majorization-minimization. Chúng tôi tiếp tục cho thấy rằng bước tối ưu hóa triển khai liên quan có
sự giống nhau mạnh mẽ với bước ISTA. Điều này xác nhận thảo luận trước đó của chúng tôi — chúng tôi đã lấy
lựa chọn đơn giản nhất có thể trong việc thiết kế CRATE, nhưng bằng suy ra nghiêm ngặt hơn chúng ta có thể khám phá
các toán tử thay thế tuy nhiên có cùng chức năng khái niệm và có thể hoạt động tốt hơn trong thực tế.

Giả định. Trong phần này, chúng tôi trình bày một phân tích tối ưu hóa nghiêm ngặt của phương pháp
tối thiểu hóa tăng dần cho mục tiêu (13). Chúng tôi sẽ cho thấy rằng dưới hai giả định đơn giản hóa,
cụ thể là
1. Các cột của Zℓ+1/2 được chuẩn hóa, theo nghĩa là diag((Zℓ+1/2)∗Zℓ+1/2) = 1;10
2. Chúng ta có d ≥ N,11 và các cột của Zℓ+1/2 trực giao, sao cho (Zℓ+1/2)∗Zℓ+1/2 =
I.12

phương pháp dẫn đến một lần lặp cập nhật bằng một phiên bản hơi đơn giản hóa của
khối ISTA (17). Chúng tôi thấy điều này như một biện minh cho sự suy ra của chúng tôi trong Phần 2.4, đã có được
khối ISTA bằng cách đưa ra một giả định đơn giản hóa bổ sung về phân phối của dữ liệu tại lớp ℓ.

Phân tích. Theo (16), chúng tôi sẽ xem xét việc nới lỏng tự nhiên của chuẩn ℓ0 "norm" thành chuẩn ℓ1,
và kết hợp ràng buộc không âm. Xem xét mục tiêu
φ(Z) = λ∥Z∥1 + χ{Z≥0}(Z) − 1
2 log det (I + αZ∗Z)
| {z }
R(Z), (77)

trong đó Z ∈ Rd×N và α = d/Nε2, và χ{Z≥0} biểu thị hàm đặc trưng cho tập hợp các
ma trận Z không âm theo từng phần tử. Như trong Phụ lục A.2, chúng ta tính
∇ZR(Z) = αZ(I + αZ∗Z)−1. (78)

10 Đây là một giả định tự nhiên trong các kiến trúc kiểu transformer như CRATE do việc sử dụng các khối LayerNorm
—mặc dù các khối này (thực sự, như chúng tôi sử dụng chúng trong CRATE) bao gồm các offset trung bình và tỷ lệ có thể huấn luyện cũng như
một phép toán trừ trung bình bổ sung [63], chúng được khởi tạo để có trung bình bằng không và chuẩn đơn vị,
do đó giả định này tương ứng với một phân tích của mạng tại khởi tạo của nó.
11 Giả định này không mất tính tổng quát, như chúng ta sẽ thấy trong phân tích dưới đây. Lý do là Z∗Z
và ZZ∗ có cùng giá trị eigen khác không bất kể hình dạng của Z, có nghĩa là log det(I +
αZ∗Z) = log det(I + αZZ∗). Đặc biệt, diễn giải các chuẩn một cách phù hợp (với một sự lạm dụng ký hiệu nhẹ), chúng ta có φ(Z) = φ(Z∗), vì vậy cho mục đích phân tích chúng ta luôn có thể tiến hành như thể Z là một
ma trận cao (miễn là chúng ta không sử dụng bất kỳ thuộc tính đặc biệt nào của α trong suy ra của chúng ta).
12 Giả định này mạnh hơn nghiêm ngặt so với giả định trước đó, và mạnh hơn nghiêm ngặt so với giả định về
sự không liên kết trên các cột. Nó tương ứng với biểu diễn Zℓ+1/2 không sụp đổ, mà chúng ta mong đợi
sẽ giữ tại khởi tạo do các phép chiếu U[K] là ngẫu nhiên.

--- TRANG 20 ---
Chúng tôi xem xét một sơ đồ tối ưu hóa tăng dần cho mục tiêu φ rất phi tuyến và không lồi.
Theo Phần 2.3, chúng tôi tối ưu hóa cục bộ tại một lần lặp "sau nén" Zℓ+1/2. Chúng tôi theo
khung majorize-minimize proximal tiêu chuẩn [69] cho tối ưu hóa tăng dần/cục bộ: điều này bắt đầu
với khai triển Taylor bậc hai cho phần trơn của φ trong vùng lân cận của lần lặp hiện tại Zℓ+1/2:
R(Z) = R(Zℓ+1/2) + D
∇ZR(Zℓ+1/2), Z − Zℓ+1/2E
+ Z1
0 (1 − t)D
Z − Zℓ+1/2, ∇2R(Zt)
Z − Zℓ+1/2E
dt, (79)

trong đó cho bất kỳ Z ∈ Rd×N, Zt = tZℓ+1/2 + (1 − t)Z. Phương pháp majorization-minimization
proximal xen kẽ hai bước để tối thiểu hóa φ:
1. Đầu tiên, sử dụng các giả định về Zℓ+1/2 để suy ra một cận trên trên chuẩn toán tử của
Hessian ∇2R(Z) trên miền hiệu quả của bài toán tối ưu hóa. Chúng tôi sẽ viết L
cho cận trên (đồng nhất) này. Điều này tạo ra một cận trên bậc hai cho phần trơn của
mục tiêu φ.
2. Sau đó, tối thiểu hóa xen kẽ phần trơn của cận trên bậc hai như một hàm của Z,
và thực hiện một bước proximal trên phần không trơn. Có thể chỉ ra [69] rằng tương ứng với
lần lặp
Z+ = prox λ
L (∥·∥1+χ{Z≥0})
Z + 1
L ∇ZR(Z)
(80)
Trong thiết lập tối thiểu hóa xen kẽ của bài báo này để tối ưu hóa (1), chúng tôi chỉ thực hiện một
bước như vậy, bắt đầu tại Zℓ+1/2.

Chúng tôi sẽ khởi tạo chương trình này dưới đây, cho thấy các cận lỗi định lượng liên quan đến các giả định
của chúng tôi ở trên khi cần thiết. Thay vì áp dụng trực tiếp lần lặp (80), chúng tôi sẽ suy ra nó dưới đây dưới
các giả định đã nêu của chúng tôi.

Bắt đầu tại (79), nhiệm vụ đầu tiên của chúng tôi là cận trên phần dư bậc hai. Điều này tương ứng với việc ước lượng
D
Z − Zℓ+1/2, ∇2R(Zt)
Z − Zℓ+1/2E
(81)
≤ sup
t∈[0,1]

∇2R(Zt)

ℓ2→ℓ2


Z − Zℓ+1/2


2
F (82)

với Cauchy-Schwarz. Sử dụng Bổ đề 5, chúng ta có thể ước lượng số hạng chuẩn toán tử trong cận trước đó
theo các thuộc tính của Zℓ+1/2. Chúng ta cần cận
α sup
∥∆∥F≤1


∆ − αZt(I + αZ∗
tZt)−1(Z∗
t∆ + ∆∗Zt)
(I + αZ∗
tZt)−1

F, (83)

và Bổ đề 6 cho rằng số hạng này không lớn hơn 9α/4 cho bất kỳ Z và bất kỳ t nào. Với ước lượng này và
(79), chúng ta có một cận trên bậc hai cho −R(Z):
−R(Z) ≤ −R(Zℓ+1/2) + D
−∇ZR(Zℓ+1/2), Z − Zℓ+1/2E
+ 9α
8


Z − Zℓ+1/2


2
F. (84)

Trong khi đó, bằng các giả định của chúng tôi ở trên, chúng ta có
−∇ZR(Zℓ+1/2) = −αZℓ+1/2(I + αI)−1 = −α
1 + α Zℓ+1/2. (85)

Bây giờ chúng tôi tối thiểu hóa cận trên bậc hai trước đó như một hàm của Z. Lấy đạo hàm, bộ tối thiểu hóa Zopt
được tính như
Zopt = 1 + 4
9(1 + α) Zℓ+1/2, (86)

và được biết rằng toán tử proximal của tổng χ{Z≥0} và λ∥ · ∥1 đơn giản là
toán tử soft-thresholding một phía [69]
proxχ{Z≥0}+λ∥·∥1(Z) = max {Z − λ1, 0}, (87)

trong đó maximum được áp dụng theo từng phần tử. Như trong Phần 2.4, chúng ta có thể viết maximum theo từng phần tử này
đơn giản như ReLU. Do đó, một bước của proximal majorization-minimization dưới các
giả định đơn giản hóa của chúng tôi có dạng
Zℓ+1 = ReLU
1 + 4
9(1 + α) Zℓ+1/2 − 4λ
9α 1
. (88)

Cuối cùng, chúng tôi chỉ ra một mở rộng bổ sung đưa ra từ điển D xuất hiện trong
khối ISTA trong Phần 2.4. Lưu ý rằng cho bất kỳ D trực giao nào, người ta có R(DZ) = R(Z) cho mọi Z.
Đối xứng này ngụ ý các thuộc tính equivariance của ∇ZR(Z) và ∇2
ZR(Z): cho mọi Z và mọi ∆
và mọi D trực giao,
D∇ZR(Z) = ∇ZR(DZ), (89)
⟨D∆, ∇2
ZR(Z)(D∆)⟩ = ⟨∆, ∇2
ZR(DZ)(∆)⟩. (90)

Do đó khai triển Taylor bậc hai (79) có thể được viết tương đương như
R(Z) = R(D∗Zℓ+1/2) + D
∇ZR(D∗Zℓ+1/2), Z − Zℓ+1/2E
+ Z1
0 (1 − t)D
Z − Zℓ+1/2, ∇2R(D∗Zt)
Z − Zℓ+1/2E
dt, (91)

cho bất kỳ D trực giao nào. Ý nghĩa của điều này là chúng ta đã có được một biểu thức tương đương
với (79), nhưng với Zℓ+1/2 được thay thế bằng D∗Zℓ+1/2; hơn nữa, vì các đối số xấp xỉ
của chúng tôi ở trên không bị ảnh hưởng bởi việc nhân trái Zℓ+1/2 với một ma trận trực giao (phép toán này
không thay đổi các chuẩn của các cột của Zℓ+1/2, hoặc các tương quan của chúng, và do đó tính không liên kết của ma trận), chúng ta có thể áp dụng chính xác cùng một dòng lý luận ở trên để có được rằng một
lần lặp majorization-minimization proximal tương đương được cho bởi
Zℓ+1 = ReLU
1 + 4
9(1 + α) D∗Zℓ+1/2 − 4λ
9α 1
, (92)

cho bất kỳ từ điển trực giao D nào. Điều này cho một cập nhật khá tương tự với khối ISTA (17) trong
trường hợp từ điển được sử dụng trong Phần 2.4 là trực giao, nhưng không có kết nối bỏ qua.

Do đó chúng ta có được một phiên bản hộp trắng tự nhiên của phần này của kiến trúc, cùng với diễn giải
tự nhiên rằng mục đích của nó là làm thưa các token được nén Zℓ+1/2 trong một từ điển (có thể học),
phù hợp với các nghiên cứu thực nghiệm gần đây [75].

Các kiến trúc khác? Như chúng tôi đề cập ở đầu phần này, suy ra trước đó
được thực hiện trong thiết lập cơ bản nhất có thể để chứng minh phương pháp majorization-
minimization cho thiết kế lớp. Các xấp xỉ chính xác hơn hoặc giả định có thể dẫn đến
các thiết kế lớp vượt trội tối ưu hóa mục tiêu đích (1) tốt hơn (và đặc biệt (13)). Chúng tôi đề cập
hai ở đây:
1. Ngoài các đặc trưng chính xác-không liên kết: các suy ra của chúng tôi ở trên giả định rằng các biểu diễn
đến Zℓ+1/2 đã tối đa cho số hạng mở rộng R trong (13). Việc có được một suy ra
'perturbative' là mong muốn, áp dụng trong các trường hợp mà Zℓ+1/2 không
hoàn toàn trực giao, mà thay vào đó gần-trực giao, đặc biệt là không liên kết [69]. Các suy ra
ở trên có thể được điều chỉnh cho thiết lập này; các cận perturbation trở nên hơi tinh tế hơn,
và lớp cuối cùng (92) thay đổi để bao gồm chuẩn hóa bổ sung.
2. Ngoài từ điển trực giao: Các đối xứng của số hạng mở rộng R trong (13) có thể được
theo dõi để dẫn đến một cặp từ điển D và D′ và một mục tiêu làm thưa DZD′.
Loại biến đổi này gợi ý các kiến trúc phổ biến trộn trên token [53,
66], tuy nhiên chúng tôi xem xét dạng đơn giản hơn DZ trong công trình này. Ngoài ra, chúng tôi đã tập trung
vì đơn giản vào các từ điển trực giao D; như trong dấu đầu dòng trước đó, người ta có thể xem xét
theo cách tương tự các từ điển D hoàn chỉnh và gần-trực giao. Điều chỉnh
suy ra cho các từ điển overcomplete là một hướng tương lai thú vị mà chúng tôi mong đợi
cải thiện khả năng mở rộng của CRATE; một con đường để đạt được điều này có thể là tăng số lượng
phép chiếu U[K] và các chiều nhúng của chúng.

--- TRANG 21 ---
A.3.1 Bổ đề Phụ trợ

Bổ đề 5. Xem xét hàm
R(Z) = 1
2 log det (I + αZ∗Z), (93)

trong đó α > 0 là một hằng số. Khi đó chúng ta có
∇ZR(Z) = αZ(I + αZ∗Z)−1, (94)

và toán tử Hessian ∇2
ZR(Z): Rd×N → Rd×N thỏa mãn rằng cho bất kỳ ∆ ∈ Rd×N,
∇2
ZR(Z)(∆) (95)
= α∆(I + αZ∗Z)−1 − α2Z(I + αZ∗Z)−1(Z∗∆ + ∆∗Z)(I + αZ∗Z)−1. (96)

Chứng minh. Tính toán gradient theo từ [46], ví dụ. Cho Hessian, chúng ta sử dụng phương pháp thông thường
để tính đạo hàm: nếu ∆ là bất kỳ ma trận nào có cùng hình dạng với Z và t > 0,
∇2
ZR(Z)(∆) = ∂
∂t
t=0 [t 7→ ∇ZR(Z + t∆)], (97)

hợp lệ vì R trơn. Chúng ta có
∇ZR(Z + t∆)
= α(Z + t∆)(I + α(Z + t∆)∗(Z + t∆))−1
= α(Z + t∆)(I + αZ∗Z + αt[Z∗∆ + ∆∗Z + t∆∗∆])−1
= α(Z + t∆)
I + αt(I + αZ∗Z)−1[Z∗∆ + ∆∗Z + t∆∗∆]−1
(I + αZ∗Z)−1
= α(Z + t∆) ∞X
k=0 (−αt)k
(I + αZ∗Z)−1[Z∗∆ + ∆∗Z + t∆∗∆]k!
(I + αZ∗Z)−1,

trong đó ở dòng thứ tư chúng ta yêu cầu t đủ gần 0 để gọi chuỗi Neumann. Đầu tiên, lưu ý rằng số hạng liên quan đến ∆∗∆ không đóng vai trò trong biểu thức cuối cùng: sau khi
chúng ta lấy đạo hàm theo t và lấy giới hạn t → 0, các số hạng phát sinh do vi phân của
t 7→ t∆∗∆ về 0, vì bất cứ khi nào chỉ số tổng k > 0 chúng ta có một số hạng (−αt)k về 0 khi t → 0. Do đó chúng ta có được với quy tắc tích
∂
∂t
t=0 [t 7→ ∇ZR(Z + t∆)] (98)
= α∆(I + αZ∗Z)−1 − α2Z(I + αZ∗Z)−1(Z∗∆ + ∆∗Z)(I + αZ∗Z)−1. (99)

Bổ đề 6. Người ta có
sup
∥∆∥F≤1


∆ − αZt(I + αZ∗
tZt)−1(Z∗
t∆ + ∆∗Zt)
(I + αZ∗
tZt)−1

F ≤ 9
4. (100)

Chứng minh. Cố định ∆ thỏa mãn ∥∆∥F ≤ 1. Bằng bất đẳng thức tam giác,



∆ − αZt(I + αZ∗
tZt)−1(Z∗
t∆ + ∆∗Zt)
(I + αZ∗
tZt)−1

F (101)
≤

∆(I + αZ∗
tZt)−1

F + α

Zt(I + αZ∗
tZt)−1(Z∗
t∆ + ∆∗Zt)(I + αZ∗
tZt)−1

F. (102)

Cho số hạng đầu tiên, chúng ta lưu ý rằng


∆(I + αZ∗
tZt)−1

F =


(I + αZ∗
tZt)−1 ⊗ I
vec(∆)

F, (103)

và vì (I + αZ∗
tZt)−1 ⪯ I, chúng ta có được từ Cauchy-Schwarz13


∆(I + αZ∗
tZt)−1

F ≤ ∥∆∥F. (104)

Chúng ta có thể sử dụng một ý tưởng tương tự để kiểm soát số hạng thứ hai. Chúng ta có từ bất đẳng thức tam giác


Zt(I + αZ∗
tZt)−1(Z∗
t∆ + ∆∗Zt)(I + αZ∗
tZt)−1

F (105)
≤

Zt(I + αZ∗
tZt)−1Z∗
t∆(I + αZ∗
tZt)−1

F (106)
+

(I + αZ∗
tZt)−1Z∗
t∆(I + αZ∗
tZt)−1Z∗
t

F. (107)

Cho số hạng đầu tiên, chúng ta có


Zt(I + αZ∗
tZt)−1Z∗
t∆(I + αZ∗
tZt)−1

F (108)
=


(I + αZ∗
tZt)−1 ⊗ Zt(I + αZ∗
tZt)−1Z∗
t
vec(∆)

F (109)
≤ σmax
(I + αZ∗
tZt)−1
σmax
Zt(I + αZ∗
tZt)−1Z∗
t
∥∆∥F (110)
≤ 1
α ∥∆∥F. (111)

Ước lượng cuối cùng theo từ một tính toán sử dụng SVD của Zt. Trong khi đó, chúng ta có cho số hạng thứ hai bằng đối số tương tự (sử dụng thực tế rằng các giá trị kỳ dị của A và A∗ giống hệt nhau
cho bất kỳ ma trận A nào)


(I + αZ∗
tZt)−1Z∗
t∆(I + αZ∗
tZt)−1Z∗
t

F ≤ σmax
(I + αZ∗
tZt)−1Z∗
t2 ∥∆∥F (112)
≤ 1
4α ∥∆∥F, (113)

trong đó một lần nữa ước lượng theo từ một tính toán liên quan đến SVD của Zt (cùng với
thực tế rằng hàm σ 7→ σ/(1 + ασ2) bị chặn trên σ ≥ 0 bởi 1/(2√α)). Kết hợp lại,
chúng ta đã có được



∆ − αZt(I + αZ∗
tZt)−1(Z∗
t∆ + ∆∗Zt)
(I + αZ∗
tZt)−1

F ≤ 9
4 ∥∆∥F, (114)

điều này cho khẳng định sau khi lấy suprema.

13 Nhớ rằng các giá trị eigen của tích Kronecker của các ma trận đối xứng là tích tensor của các
giá trị eigen (với độ bội).

--- TRANG 22 ---
B Thí nghiệm và Chi tiết Bổ sung

Trong phần này, chúng tôi cung cấp chi tiết về các thí nghiệm của chúng tôi, và báo cáo kết quả của các thí nghiệm bổ sung
không được đề cập trong văn bản chính. CRATE có lẽ lấy các lựa chọn thiết kế cơ bản nhất
có thể, và vì vậy chúng tôi không cố gắng cạnh tranh trực tiếp với hiệu suất tiên tiến từ các
transformer được thiết kế kỹ lưỡng và thực nghiệm. Kết quả của các thí nghiệm của chúng tôi nhằm
truyền đạt một vài thông điệp cốt lõi:
• Mặc dù không được thiết kế để cạnh tranh với tiên tiến, CRATE hoạt động mạnh mẽ
trên các tập dữ liệu thế giới thực quy mô lớn, bao gồm phân loại trên ImageNet-1K. CRATE cũng
đạt được hiệu suất học chuyển giao mạnh mẽ.
• Vì mô hình của chúng tôi được thiết kế thông qua tối ưu hóa triển khai của một mục tiêu được hiểu rõ,
mỗi lớp có thể diễn giải. Đặc biệt, chúng tôi có thể phân tích hiệu suất của CRATE, cũng như
thiết kế các sửa đổi mạng, trên cơ sở từng lớp. Điều này được hỗ trợ bởi một mức độ hiểu biết
có lẽ vô song về vai trò của từng toán tử trong mạng của chúng tôi.
• Chúng tôi thực hiện các lựa chọn đơn giản nhất có thể trong quá trình thiết kế CRATE, nhưng những điều này có thể được thay đổi
dễ dàng trong khi giữ cùng khung làm việc. Chúng tôi nghiên cứu một vài sửa đổi sau trong phần này
(Phụ lục B.4) và cho thấy rằng chúng không làm tổn hại đáng kể hiệu suất thực nghiệm, nhưng
nhấn mạnh ở đây rằng có tiềm năng đáng kể cho cải thiện với các lựa chọn kiến trúc khác nhau
(và đặc biệt là một phân tích lý thuyết khác).

B.1 Chi tiết triển khai

Trong phần này, chúng tôi cung cấp thêm chi tiết để triển khai CRATE trên các tác vụ thị giác.

B.1.1 Kiến trúc của CRATE

Sửa đổi kiến trúc. So với kiến trúc khái niệm được đề xuất trong Phần 2.5
và 3, chúng tôi thực hiện thay đổi sau vì mục đích đơn giản triển khai:
• Trong bước nén, thay thế số hạng p
Nε2[U1, . . . , UK] trong toán tử MSSA bằng
một tham số có thể huấn luyện khác W ∈ Rd×pK. Do đó khối MSSA trở thành
MSSA(Z|U[K], W) .= W
SSA(Z|U1)
...
SSA(Z|UK)
. (115)

Mã PyTorch cho CRATE. Chúng tôi cung cấp mã kiểu PyTorch để triển khai kiến trúc mạng được đề xuất của chúng tôi. Thuật toán 1 định nghĩa kiến trúc tổng thể, Thuật toán 2 và Thuật toán 3 chứa
chi tiết cho khối transformer, khối tự chú ý (khối MSSA), và khối MLP (khối ISTA).

B.1.2 Thiết lập Huấn luyện

Huấn luyện trước trên ImageNet-1K. Chúng tôi áp dụng bộ tối ưu hóa Lion [71] để huấn luyện trước cả mô hình CRATE và ViT. Chúng tôi cấu hình tốc độ học là 2.4×10−4, weight decay là 0.5, và batch size là
2,048. Chúng tôi kết hợp chiến lược warm-up với tăng tuyến tính trong 5 epoch, tiếp theo bằng việc huấn luyện
các mô hình trong tổng cộng 150 epoch với cosine decay. Để tăng cường dữ liệu, chúng tôi chỉ áp dụng các
kỹ thuật tiêu chuẩn, cắt ngẫu nhiên và lật ngang ngẫu nhiên, trên tập dữ liệu ImageNet-1K.
Chúng tôi áp dụng label smoothing với tham số smoothing 0.1. Một epoch huấn luyện của CRATE−Base mất
khoảng 240 giây sử dụng 16 GPU A100 40GB.

Tinh chỉnh. Chúng tôi tinh chỉnh các mô hình CRATE và ViT được huấn luyện trước của chúng tôi trên các tập dữ liệu đích sau:
CIFAR10/CIFAR100 [10], Oxford Flowers-102 [7], Oxford-IIIT-Pets [16]. Chúng tôi cũng đánh giá các mô hình
được huấn luyện trước của chúng tôi trên benchmark ImageNet Real [36] thường được sử dụng. Cho mỗi tác vụ tinh chỉnh, chúng tôi sử dụng bộ tối ưu hóa AdamW [26]. Chúng tôi cấu hình tốc độ học là 5×10−5, weight decay
là 0.01, và batch size là 512. Để cho phép học chuyển giao, trước tiên chúng tôi thay đổi kích thước dữ liệu đầu vào của chúng tôi về
224. Để tăng cường dữ liệu, chúng tôi cũng áp dụng một số kỹ thuật tiêu chuẩn: cắt ngẫu nhiên, lật ngang
ngẫu nhiên, và tăng cường ngẫu nhiên (với số lượng biến đổi n = 2 và độ lớn của biến đổi
m = 14).14

14 https://github.com/huggingface/pytorch-image-models/blob/main/timm/data/auto_
augment.py

--- TRANG 23 ---
Thuật toán 1: Pseudocode kiểu PyTorch cho Mạng CRATE
# Định nghĩa lớp ViT_dictionary
CRATE:
# khởi tạo
def init(self, image_size, patch_size, num_classes, dim, depth, heads,
mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0.,
emb_dropout = 0.):
# định nghĩa patch, kích thước hình ảnh và số lượng patch
image_height, image_width = pair(image_size)
patch_height, patch_width = pair(patch_size)
num_patches = (image_height // patch_height) * (image_width //
patch_width)
patch_dim = channels * patch_height * patch_width
# định nghĩa patch embedding, positional embedding, dropout, và transformer
self.to_patch_embedding = Sequential(Rearrange, LayerNorm(patch_dim),
Linear(patch_dim, dim), LayerNorm(dim))
self.pos_embedding = Parameter(random(1, num_patches + 1, dim))
self.cls_token = Parameter(random(1, 1, dim))
self.dropout = Dropout(emb_dropout)
self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim,
dropout)
# định nghĩa pooling, latent layer, và MLP head
self.pool = pool
self.to_latent = Identity()
self.mlp_head = Sequential(LayerNorm(dim), Linear(dim, num_classes))
# forward pass
def forward(self, img):
x = self.to_patch_embedding(img)
b, n, _ = shape(x)
cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)
x = concatenate((cls_tokens, x), dim=1)
x += self.pos_embedding[:, :(n + 1)]
x = self.dropout(x)
x = self.transformer(x)
x = mean(x, dim = 1) if self.pool == 'mean' else x[:, 0]
x = self.to_latent(x)
return self.mlp_head(x)

Thuật toán 2: Pseudocode Kiểu Pytorch cho Khối Transformer trong CRATE
# Định nghĩa lớp Transformer
class Transformer:
# khởi tạo
def init(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):
# định nghĩa các lớp
self.layers = []
self.depth = depth
for _ in range(depth):
self.layers.append([LayerNorm(dim, Attention(dim, heads, dim_head,
dropout))])
self.layers.append([LayerNorm(dim, FeedForward(dim, mlp_dim,
dropout))])
# forward pass
def forward(self, x):
for attn, ff in self.layers:
x_ = attn(x) + x
x = ff(x_)
return x

--- TRANG 24 ---
Thuật toán 3: Pseudocode cho Attention và FeedForward
# Định nghĩa lớp FeedForward
class FeedForward:
# khởi tạo
def init(self, dim, hidden_dim, dropout = 0., step_size=0.1, lambd=0.1):
self.weight = Parameter(Tensor(dim, dim))
init.kaiming_uniform_(self.weight)
self.step_size = step_size
self.lambd = lambd
# forward pass
def forward(self, x):
x1 = linear(x, self.weight, bias=None)
grad_1 = linear(x1, self.weight.t(), bias=None)
grad_2 = linear(x, self.weight.t(), bias=None)
grad_update = self.step_size * (grad_2 - grad_1) - self.step_size *
self.lambd
output = relu(x + grad_update)
return output
# Định nghĩa lớp Attention
class Attention:
# khởi tạo
def init(self, dim, heads = 8, dim_head = 64, dropout = 0.):
inner_dim = dim_head * heads
project_out = not (heads == 1 and dim_head == dim)
self.heads = heads
self.scale = dim_head ** -0.5
self.attend = Softmax(dim = -1)
self.dropout = Dropout(dropout)
self.qkv = Linear(dim, inner_dim, bias=False)
self.to_out = Sequential(Linear(inner_dim, dim), Dropout(dropout)) if
project_out else nn.Identity()
# forward pass
def forward(self, x):
w = rearrange(self.qkv(x), 'b n (h d) -> b h n d', h = self.heads)
dots = matmul(w, w.transpose(-1, -2)) * self.scale
attn = self.attend(dots)
attn = self.dropout(attn)
out = matmul(attn, w)
out = rearrange(out, 'b h n d -> b n (h d)')
return self.to_out(out)

--- TRANG 25 ---
B.2 Kết quả Thí nghiệm

Trong phần này, chúng tôi cung cấp kết quả thí nghiệm bổ sung về CRATE, bao gồm các đo lường theo lớp,
trực quan hóa, cũng như các nghiên cứu ablation.

B.2.1 Đánh giá và Trực quan hóa Theo Lớp

Đánh giá theo lớp về nén và độ thưa. Tương tự như Hình 3, chúng tôi tiến hành đánh giá
theo lớp về số hạng nén và độ thưa cho CRATE-Tiny, CRATE-Base, và CRATE-Large.
Chúng tôi quan sát hành vi tương tự như đã đề cập trong Phần 3.1: cả số hạng nén và số hạng độ thưa
đều cải thiện khi chỉ số lớp tăng.

2 4 6 8 10 12
Chỉ số lớp - 
400450500550600650700Rc(Z) [khối SSA]
Đo tỷ lệ mã hóa qua các lớp

(a) Nén (Mô hình: CRATE-Tiny).

2 4 6 8 10 12
Chỉ số lớp - 
0.250.300.350.400.450.500.550.60Độ thưa [khối ISTA]
Đo độ thưa đầu ra qua các lớp

(b) Độ thưa (Mô hình: CRATE-Tiny).

2 4 6 8 10 12
Chỉ số lớp - 
600800100012001400Rc(Z) [khối SSA]
Đo tỷ lệ mã hóa qua các lớp

(c) Nén (Mô hình: CRATE-Base).

2 4 6 8 10 12
Chỉ số lớp - 
0.10.20.30.40.50.60.7Độ thưa [khối ISTA]
Đo độ thưa đầu ra qua các lớp

(d) Độ thưa (Mô hình: CRATE-Base).

0 5 10 15 20 25
Chỉ số lớp - 
100012001400160018002000Rc(Z) [khối SSA]
Đo tỷ lệ mã hóa qua các lớp

(e) Nén (Mô hình: CRATE-Large).

0 5 10 15 20 25
Chỉ số lớp - 
0.10.20.30.40.50.60.7Độ thưa [khối ISTA]
Đo độ thưa đầu ra qua các lớp

(f) Độ thưa (Mô hình: CRATE-Large).

Hình 5: Trái: Số hạng nén Rc(Zℓ+1/2) của các đầu ra MSSA tại các lớp khác nhau. Phải: độ thưa
của đầu ra khối ISTA, ∥Zℓ+1∥0/(d·N), tại các lớp khác nhau.

--- TRANG 26 ---
Trực quan hóa các biểu diễn token theo lớp. Trong Hình 6, chúng tôi trực quan hóa các biểu diễn token
Zℓ tại các lớp khác nhau ℓ ∈ {1, . . . , 12}. Chúng tôi cung cấp thêm kết quả được đánh giá trên các mẫu khác trong
Phụ lục B.2.2.

Trực quan hóa các không gian con theo lớp trong tự chú ý đa đầu. Chúng tôi cung cấp trực quan hóa của
Uℓ
[K] trong Hình 7.

Layer=1
0.000.250.500.751.001.251.501.752.00

(a) ℓ = 1.

Layer=2
0.000.250.500.751.001.251.501.752.00

(b) ℓ = 2.

Layer=3
0.000.250.500.751.001.251.501.752.00

(c) ℓ = 3.

Layer=4
0.000.250.500.751.001.251.501.752.00

(d) ℓ = 4.

Layer=5
0.000.250.500.751.001.251.501.752.00

(e) ℓ = 5.

Layer=6
0.000.250.500.751.001.251.501.752.00

(f) ℓ = 6.

Layer=7
0.000.250.500.751.001.251.501.752.00

(g) ℓ = 7.

Layer=8
0.000.250.500.751.001.251.501.752.00

(h) ℓ = 8.

Layer=9
0.000.250.500.751.001.251.501.752.00

(i) ℓ = 9.

Layer=10
0.000.250.500.751.001.251.501.752.00

(j) ℓ = 10.

Layer=11
0.000.250.500.751.001.251.501.752.00

(k) ℓ = 11.

Layer=12
0.000.250.500.751.001.251.501.752.00

(l) ℓ = 12.

Hình 6: Trực quan hóa các biểu diễn token Zℓ theo lớp tại mỗi lớp ℓ. Để tăng cường độ rõ nét trực quan, chúng tôi
ngẫu nhiên trích xuất một ma trận con 50 × 50 từ Zℓ để hiển thị. (Mô hình: CRATE-Tiny)

--- TRANG 27 ---
Layer=1
0.20.40.60.81.0

(a) ℓ = 1.

Layer=2
0.20.40.60.81.0

(b) ℓ = 2.

Layer=3
0.20.40.60.81.0

(c) ℓ = 3.

Layer=4
0.20.40.60.81.0

(d) ℓ = 4.

Layer=5
0.20.40.60.81.0

(e) ℓ = 5.

Layer=6
0.20.40.60.81.0

(f) ℓ = 6.

Layer=7
0.20.40.60.81.0

(g) ℓ = 7.

Layer=8
0.20.40.60.81.0

(h) ℓ = 8.

Layer=9
0.20.40.60.81.0

(i) ℓ = 9.

Layer=10
0.20.40.60.81.0

(j) ℓ = 10.

Layer=11
0.20.40.60.81.0

(k) ℓ = 11.

Layer=12
0.20.40.60.81.0

(l) ℓ = 12.

Hình 7: Chúng tôi trực quan hóa [Uℓ
1, . . . , Uℓ
K]∗[Uℓ
1, . . . , Uℓ
K] ∈ RpK×pK tại các lớp khác nhau. Khối (i, j)-th trong
mỗi tiểu hình tương ứng với (Uℓ
i)∗Uℓ
j cho i, j ∈ [K] tại một lớp cụ thể ℓ. Để tăng cường độ rõ nét trực quan, cho
mỗi không gian con Ui, chúng tôi ngẫu nhiên chọn 4 hướng để hiển thị. (Mô hình: CRATE-Tiny)

--- TRANG 28 ---
B.2.2 Trực quan hóa Theo Lớp Bổ sung

Chúng tôi cung cấp thêm kết quả của trực quan hóa biểu diễn token theo lớp trên các mẫu khác trong
Hình 8, Hình 9, Hình 10, và Hình 11 (Mô hình: CRATE-Base).

Layer=1
0.000.250.500.751.001.251.501.752.00

Layer=2
0.000.250.500.751.001.251.501.752.00

Layer=3
0.000.250.500.751.001.251.501.752.00

Layer=4
0.000.250.500.751.001.251.501.752.00

Layer=5
0.000.250.500.751.001.251.501.752.00

Layer=6
0.000.250.500.751.001.251.501.752.00

Layer=7
0.000.250.500.751.001.251.501.752.00

Layer=8
0.000.250.500.751.001.251.501.752.00

Layer=9
0.000.250.500.751.001.251.501.752.00

Layer=10
0.000.250.500.751.001.251.501.752.00

Layer=11
0.000.250.500.751.001.251.501.752.00

Layer=12
0.000.250.500.751.001.251.501.752.00

Hình 8: Trực quan hóa các biểu diễn token Zℓ theo lớp tại mỗi lớp ℓ. Để tăng cường độ rõ nét trực quan, chúng tôi
ngẫu nhiên trích xuất một ma trận con 50 × 50 từ Zℓ để hiển thị. (Mẫu 1)

Layer=1
0.000.250.500.751.001.251.501.752.00

Layer=2
0.000.250.500.751.001.251.501.752.00

Layer=3
0.000.250.500.751.001.251.501.752.00

Layer=4
0.000.250.500.751.001.251.501.752.00

Layer=5
0.000.250.500.751.001.251.501.752.00

Layer=6
0.000.250.500.751.001.251.501.752.00

Layer=7
0.000.250.500.751.001.251.501.752.00

Layer=8
0.000.250.500.751.001.251.501.752.00

Layer=9
0.000.250.500.751.001.251.501.752.00

Layer=10
0.000.250.500.751.001.251.501.752.00

Layer=11
0.000.250.500.751.001.251.501.752.00

Layer=12
0.000.250.500.751.001.251.501.752.00

Hình 9: Trực quan hóa các biểu diễn token Zℓ theo lớp tại mỗi lớp ℓ. Để tăng cường độ rõ nét trực quan, chúng tôi
ngẫu nhiên trích xuất một ma trận con 50 × 50 từ Zℓ để hiển thị. (Mẫu 2)

--- TRANG 29 ---
Layer=1
0.000.250.500.751.001.251.501.752.00

Layer=2
0.000.250.500.751.001.251.501.752.00

Layer=3
0.000.250.500.751.001.251.501.752.00

Layer=4
0.000.250.500.751.001.251.501.752.00

Layer=5
0.000.250.500.751.001.251.501.752.00

Layer=6
0.000.250.500.751.001.251.501.752.00

Layer=7
0.000.250.500.751.001.251.501.752.00

Layer=8
0.000.250.500.751.001.251.501.752.00

Layer=9
0.000.250.500.751.001.251.501.752.00

Layer=10
0.000.250.500.751.001.251.501.752.00

Layer=11
0.000.250.500.751.001.251.501.752.00

Layer=12
0.000.250.500.751.001.251.501.752.00

Hình 10: Trực quan hóa các biểu diễn token Zℓ theo lớp tại mỗi lớp ℓ. Để tăng cường độ rõ nét trực quan, chúng tôi
ngẫu nhiên trích xuất một ma trận con 50 × 50 từ Zℓ để hiển thị. (Mẫu 3)

Layer=1
0.000.250.500.751.001.251.501.752.00

Layer=2
0.000.250.500.751.001.251.501.752.00

Layer=3
0.000.250.500.751.001.251.501.752.00

Layer=4
0.000.250.500.751.001.251.501.752.00

Layer=5
0.000.250.500.751.001.251.501.752.00

Layer=6
0.000.250.500.751.001.251.501.752.00

Layer=7
0.000.250.500.751.001.251.501.752.00

Layer=8
0.000.250.500.751.001.251.501.752.00

Layer=9
0.000.250.500.751.001.251.501.752.00

Layer=10
0.000.250.500.751.001.251.501.752.00

Layer=11
0.000.250.500.751.001.251.501.752.00

Layer=12
0.000.250.500.751.001.251.501.752.00

Hình 11: Trực quan hóa các biểu diễn token Zℓ theo lớp tại mỗi lớp ℓ. Để tăng cường độ rõ nét trực quan, chúng tôi
ngẫu nhiên trích xuất một ma trận con 50 × 50 từ Zℓ để hiển thị. (Mẫu 4)

--- TRANG 30 ---
B.3 Ablation CRATE

Siêu tham số của CRATE. Trong Bảng 2, chúng tôi trình bày đánh giá CRATE được huấn luyện với các
tham số khác nhau. Cụ thể hơn, chúng tôi nghiên cứu tác động của số epoch, weight decay, tốc độ học,
step size (η) và số hạng chính quy (λ) trong khối ISTA. Như được hiển thị trong Bảng 2, CRATE
chứng minh hiệu suất thỏa đáng nhất quán trên một phạm vi đa dạng các siêu tham số.

Bảng 2: Độ chính xác Top 1 của CRATE trên các tập dữ liệu khác nhau với các biến thể thiết kế kiến trúc khác nhau khi được huấn luyện
trên ImageNet.

Mô hình epoch weight decay lr η(ISTA) λ(ISTA) ImageNet
CRATE-B 150 (mặc định) 0.5 (mặc định) 2.4×10−4 0.1 0.1 70.8
CRATE-B 150 0.5 2.4×10−4 0.02 0.1 70.7
CRATE-B 150 0.5 2.4×10−4 0.5 0.1 66.7
CRATE-B 150 0.5 2.4×10−4 0.1 0.02 70.8
CRATE-B 150 0.5 2.4×10−4 0.1 0.5 70.5
CRATE-B 90 0.5 2.4×10−4 0.1 0.1 69.5
CRATE-B 300 0.5 2.4×10−4 0.1 0.1 70.9
CRATE-B 150 1.0 2.4×10−4 0.1 0.1 70.3
CRATE-B 150 0.05 2.4×10−4 0.1 0.1 70.2
CRATE-B 150 0.5 4.8×10−4 0.1 0.1 70.2
CRATE-B 150 0.5 1.2×10−4 0.1 0.1 70.3

B.4 Khám phá Các Biến thể Kiến trúc

Trong phần này, chúng tôi khám phá hai kiến trúc thay thế sau. Một kiến trúc liên quan đến việc sửa đổi
cơ chế chú ý, trong khi cái khác liên quan đến việc sửa đổi cơ chế làm thưa. Một lần nữa, chúng tôi tái nhấn mạnh rằng những lựa chọn này, mặc dù có nguyên tắc, hoàn toàn mô-đun và
các lựa chọn chúng tôi thực hiện ở đây vẫn dẫn đến các kiến trúc rất đơn giản. Một phân tích tinh vi hơn có thể
dẫn đến các kiến trúc khác nhau, phức tạp hơn hoạt động tốt hơn trong thực tế. Các kiến trúc chúng tôi
thử nghiệm với là:
• Cơ chế chú ý lấy cảm hứng từ nén: hoàn nguyên thay đổi trong (115). Tức là, cơ chế chú ý
triển khai (11) và (12) trực tiếp.
• Bước làm thưa proximal majorization-minimization: thay vì (17), triển khai (92).

Chúng tôi có được kết quả phân loại sau trong Bảng 3. Sau khi tiến hành các đơn giản hóa bổ sung
cho kiến trúc mạng (tức là áp đặt các ràng buộc bổ sung cho thiết kế kiến trúc mạng),
chúng tôi phát hiện rằng CRATE duy trì hiệu suất hợp lý trên ImageNet-1K.

Bảng 3: Độ chính xác Top 1 của CRATE trên các tập dữ liệu khác nhau với các biến thể thiết kế kiến trúc khác nhau khi được huấn luyện
trên ImageNet.

Mô hình Khối MSSA Khối ISTA ImageNet
CRATE-B mặc định mặc định 70.8
CRATE-B Eq. (11) và (12) mặc định 63.3
CRATE-B mặc định Eq. (92) 68.6
