# 2211.12821.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/interpretability/2211.12821.pdf
# Kích thước tệp: 451648 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Giải thích các Mô hình Mã nguồn dựa trên Transformer:
Chúng Học được Gì? Khi nào Chúng Không Hoạt động?
1stAhmad Haji Mohammadkhani
University of Calgary
Calgary, Canada
ahmad.hajimohammadkh@ucalgary.ca2ndChakkrit Tantithamthavorn
Monash University
Melbourne, Australia
Chakkrit@monash.edu3rdHadi Hemmatif
York University
Toronto, Canada
hemmati@yorku.ca
Tóm tắt —Trong những năm gần đây, đã có sự quan tâm rộng rãi đến việc thiết kế các mô hình dựa trên mạng nơ-ron sâu để tự động hóa các tác vụ kỹ thuật phần mềm hạ nguồn trên mã nguồn, chẳng hạn như tạo tài liệu mã, tìm kiếm mã và sửa chữa chương trình. Mặc dù mục tiêu chính của các nghiên cứu này là cải thiện hiệu quả của tác vụ hạ nguồn, nhiều nghiên cứu chỉ cố gắng sử dụng mô hình mạng nơ-ron tốt nhất tiếp theo, mà không có phân tích sâu sắc thích hợp về lý do tại sao một giải pháp cụ thể hoạt động hoặc không hoạt động, trên các tác vụ hoặc tình huống cụ thể. Trong bài báo này, sử dụng một phương pháp AI có thể giải thích (XAI) ví dụ (cơ chế chú ý), chúng tôi nghiên cứu hai mô hình ngôn ngữ lớn (LLM) gần đây cho mã (CodeBERT và GraphCodeBERT) trên một tập hợp các tác vụ kỹ thuật phần mềm hạ nguồn: tạo tài liệu mã (CDG), tinh chỉnh mã (CR), và dịch mã (CT). Thông qua các nghiên cứu định lượng và định tính, chúng tôi xác định những gì CodeBERT và GraphCodeBERT học được (đặt sự chú ý cao nhất vào, về mặt các loại token mã nguồn), trong các tác vụ này. Chúng tôi cũng chỉ ra một số mẫu thông thường khi mô hình không hoạt động như mong đợi (hoạt động kém ngay cả trên các vấn đề dễ) và đề xuất các khuyến nghị có thể giảm thiểu các thách thức được quan sát.
Từ khóa chỉ mục —AI có thể giải thích (XAI), LLM, Mô hình Mã, Có thể diễn giải, Chú ý, Transformer.

I. GIỚI THIỆU
Các Mô hình Ngôn ngữ Lớn (LLM) cho mã (gọi tắt: mô hình mã) được đề xuất để phân tích các kho dữ liệu lớn về mã nguồn và ngôn ngữ tự nhiên được thu thập từ các nền tảng mã nguồn mở (ví dụ: GitHub và StackOverflow). Các mô hình mã được tiền huấn luyện như vậy đã được sử dụng để tự động hóa các tác vụ liên quan đến mã nguồn khác nhau, ví dụ: hiểu mã, tạo mã, phát hiện bản sao mã [1], phát hiện lỗi [2], [3], và tóm tắt mã [4]. Việc tự động hóa các tác vụ kỹ thuật phần mềm như vậy đã được chứng minh là cải thiện đáng kể năng suất của các nhà phát triển phần mềm và giảm chi phí phát triển phần mềm.

Các nghiên cứu gần đây đã đề xuất các mô hình mã dựa trên Transformer, ví dụ: CodeBERT [5], GraphCodeBERT [6], CodeGPT [7], CodeT5 [8]. Tuy nhiên, hầu hết các nghiên cứu này thường tập trung vào việc cải thiện độ chính xác của nó— mà không xem xét khía cạnh khả năng giải thích. Do đó, khi triển khai các mô hình như vậy trong thực tế, các nhà thực hành vẫn không biết tại sao các mô hình như vậy cung cấp một khuyến nghị hoặc gợi ý nhất định.

Hãy xem xét một đoạn mã Python đã cho của một thuật toán sắp xếp nổi bọt. Một mô hình tóm tắt mã có thể có khả năng tóm tắt chính xác rằng đoạn mã đã cho là một thuật toán sắp xếp nổi bọt. Tuy nhiên, các nhà phát triển có thể không tin tưởng các mô hình nếu các mô hình tạo ra văn bản tự nhiên chính xác dựa trên thụt lề, khoảng trắng, hoặc dấu ngoặc đơn của đoạn mã Python, thay vì thông tin ngữ nghĩa có ý nghĩa (tức là sắp xếp nổi bọt). Do đó, các dự đoán chính xác được tạo ra bởi các mô hình không đảm bảo rằng các mô hình được học chính xác.

Do đó, việc thiếu khả năng giải thích của các mô hình mã lớn và phức tạp có thể dẫn đến việc thiếu áp dụng trong thực tế.

Trong bài báo này, chúng tôi tiến hành một nghiên cứu thực nghiệm để phân tích các mô hình mã thông qua lăng kính của AI có thể giải thích. Đặc biệt, chúng tôi tập trung vào hai mô hình mã nổi tiếng, tức là CodeBERT và GraphCodeBERT với ba tác vụ hạ nguồn cụ thể về hiểu và tạo, tức là các tác vụ Tóm tắt Mã (Mã →Văn bản), Chuyển đổi Mã (Mã →Mã), và Dịch Mã (Mã →Mã). Để giải thích các dự đoán của các mô hình này, chúng tôi tận dụng cơ chế chú ý bên trong kiến trúc Transformer, đây là một phương pháp AI có thể giải thích nội tại. Cơ chế chú ý cho phép chúng ta hiểu những token quan trọng nhất trong chuỗi đầu vào đóng góp nhiều nhất cho các token trong chuỗi đầu ra. Cụ thể, chúng tôi nhằm giải quyết hai câu hỏi nghiên cứu sau:

Các mô hình mã được tiền huấn luyện học được gì?
Kết quả: Phân tích điểm chú ý và phân phối của chúng trên các loại token khác nhau cho thấy rằng các mô hình học cách tập trung vào các loại token cụ thể cho mỗi tác vụ hạ nguồn. Trong CDG, các mô hình học cách tập trung vào chữ ký phương thức (tức là tên phương thức và đối số đầu vào). Trong khi đó ở CT, cú pháp, đó là các token liên quan đến ngôn ngữ lập trình, thu hút nhiều sự chú ý hơn. CR có vị trí trung gian so với hai tác vụ kia và có phân phối chú ý cân bằng hơn. Ngoài ra, được chỉ ra rằng GraphCodeBERT chú ý nhiều hơn đến các phần cấu trúc của mã nguồn, hơn là CodeBERT, điều này có thể là kết quả của bước bổ sung của GraphCodeBERT để phân tích mã và tận dụng luồng dữ liệu của mã. Những quan sát này phù hợp với những gì được mong đợi từ một mô hình mã khiến chúng ta kết luận rằng hai mô hình được nghiên cứu thực sự đang học (trong hầu hết các trường hợp) những gì chúng được cho là phải học.

Khi nào các mô hình mã được tiền huấn luyện không hoạt động?
Kết quả. Phát hiện của chúng tôi cho thấy có những tình huống nhất định khiến các mô hình hoạt động kém trên các tác vụ khác nhau. Ví dụ, các mô hình không hoạt động tốt với các mẫu có mã nguồn dài hoặc phức tạp và/hoặc câu trả lời mong đợi dài (đầu ra mô hình). Chúng tôi cũng đã chỉ ra rằng hiệu suất kém của mô hình thường được phản ánh trong phân phối chú ý của nó. Nói cách khác, bất cứ khi nào mô hình không đạt được đầu ra tốt cho một mô hình, nó cũng đã không chú ý đủ đến các loại token tương ứng cho tác vụ hạ nguồn tương ứng. Chúng tôi cũng đã cung cấp một số khuyến nghị về cách có thể giảm thiểu những điểm yếu này trong bài báo.

Những phát hiện này khiến chúng ta kết luận rằng mặc dù các mô hình được tiền huấn luyện đã cho thấy kết quả tuyệt vời trong các tác vụ kỹ thuật phần mềm; không có mô hình nào trong số chúng có thể được coi là một vấn đề đã khép kín và có những khía cạnh nhất định của các mô hình này cần được tập trung nhiều hơn thông qua các nghiên cứu tiếp theo. Việc giải thích các mô hình này có thể làm sáng tỏ những điểm yếu của chúng và cung cấp hướng cho nghiên cứu tương lai.

Khoa học Mở. Để thúc đẩy sáng kiến khoa học mở, chúng tôi đã công khai gói tái tạo tại GitHub.¹

II. BỐI CẢNH & CÔNG TRÌNH LIÊN QUAN

Khả năng giải thích hiện đang trở thành một mối quan tâm quan trọng trong kỹ thuật phần mềm. Nhiều nhà nghiên cứu thường sử dụng các kỹ thuật AI/ML cho dự đoán lỗi, phát hiện phần mềm độc hại, và ước lượng nỗ lực. Trong khi những kỹ thuật AI/ML này có thể cải thiện đáng kể năng suất của nhà phát triển, chất lượng phần mềm, và trải nghiệm người dùng cuối, các nhà thực hành vẫn không hiểu tại sao các mô hình AI/ML như vậy đưa ra những dự đoán đó [9], [10], [11], [12], [13], [2]. Để giải quyết thách thức này, các nhà nghiên cứu đề xuất các phương pháp khác nhau để tạo ra các giải thích ở hai cấp độ:

(1) Giải thích toàn cục có thể được tạo ra bằng cách sử dụng các kỹ thuật học máy có thể diễn giải (ví dụ: cây quyết định, quy tắc quyết định, và kỹ thuật hồi quy logistic) hoặc các kỹ thuật nội tại cụ thể của mô hình (ví dụ: ANOVA, tầm quan trọng của biến) để toàn bộ quá trình dự đoán và khuyến nghị trở nên minh bạch và có thể hiểu được. Tuy nhiên, những kỹ thuật nội tại cụ thể của mô hình như vậy nhằm cung cấp khả năng giải thích toàn cục, mà không cung cấp giải thích cho các dự đoán cá nhân.

(2) Giải thích cục bộ, mặt khác, có thể được tạo ra bằng cách sử dụng một số kỹ thuật (ví dụ: LIME, SHAP) để giải thích các dự đoán của các mô hình AI/ML hộp đen phức tạp (ví dụ: mạng nơ-ron, rừng ngẫu nhiên). Những kỹ thuật như vậy có thể cung cấp giải thích cho mỗi dự đoán cá nhân (tức là một thể hiện cần được giải thích), cho phép người dùng hiểu rõ hơn tại sao dự đoán được đưa ra.

Trong kỹ thuật phần mềm, AI có thể giải thích gần đây đã được nghiên cứu trong lĩnh vực dự đoán lỗi (tức là một mô hình phân loại để dự đoán liệu một tệp/lớp/phương thức có bị lỗi trong tương lai hay không). Cụ thể, nghiên cứu khảo sát của Jiarpakdee et al.[11] phát hiện rằng việc giải thích các dự đoán cũng quan trọng và hữu ích như việc cải thiện độ chính xác của dự đoán lỗi. Tuy nhiên, đánh giá tài liệu của họ phát hiện rằng 91% (81/96) các nghiên cứu dự đoán lỗi chỉ tập trung vào việc cải thiện độ chính xác dự đoán, mà không xem xét việc giải thích các dự đoán, trong khi chỉ 4% trong số 96 nghiên cứu này tập trung vào việc giải thích các dự đoán.

Mặc dù XAI vẫn là một chủ đề rất ít được nghiên cứu trong cộng đồng kỹ thuật phần mềm, rất ít nghiên cứu XAI hiện có đã cho thấy một số cách sử dụng thành công ví dụ như trong dự đoán lỗi. Trong một ví dụ, Wattanakriengkrai et al. [14] và Pornprasit và Tantithamthavorn [15] đã sử dụng các kỹ thuật bất khả tri mô hình (ví dụ: LIME) cho dự đoán lỗi cấp dòng (ví dụ: dự đoán dòng nào sẽ bị lỗi trong tương lai), giúp các nhà phát triển định vị các dòng bị lỗi một cách hiệu quả về chi phí. Trong một ví dụ khác, Jiarpakdee et al. [12] và Khanan et al. [16] đã sử dụng các kỹ thuật bất khả tri mô hình (ví dụ: LIME) để giải thích các mô hình dự đoán lỗi, giúp các nhà phát triển hiểu rõ hơn tại sao một tệp được dự đoán là bị lỗi. Rajapaksha et al. [13] và Pornprasit et al. [15] đã đề xuất các kỹ thuật bất khả tri mô hình dựa trên quy tắc cục bộ để tạo ra hướng dẫn có thể hành động nhằm giúp các nhà quản lý lập kế hoạch cải thiện chất lượng hiệu quả nhất.

A. Khoảng Trống Nghiên Cứu

Trong khi tồn tại các nỗ lực nghiên cứu về khả năng giải thích của các tác vụ phân loại trong các lĩnh vực SE (ví dụ: dự đoán lỗi), ít nghiên cứu tập trung vào các mô hình mã được tiền huấn luyện dựa trên transformer. Đặc biệt, các nhà thực hành thường nặng các mối quan tâm ví dụ như tại sao mã nguồn này được tạo ra? tại sao token mã này được sửa đổi?. Việc thiếu khả năng giải thích của các mô hình mã có thể dẫn đến thiếu tin tưởng, cản trở việc áp dụng trong thực tế.

Để giải quyết thách thức này, bài báo này nhằm giải quyết các câu hỏi nghiên cứu sau: (RQ1) Các mô hình mã được tiền huấn luyện học được gì? và (RQ2) Khi nào các mô hình mã được tiền huấn luyện không hoạt động?

III. THIẾT LẬP THỰC NGHIỆM

A. Lựa chọn Các Mô hình Mã được Tiền huấn luyện

Các mô hình mã được tiền huấn luyện, giống như transformer, là các mô hình học sâu được huấn luyện trên các tập dữ liệu rộng lớn (ví dụ: các dự án GitHub, bài đăng StackOverflow) để hiểu và tạo mã nguồn. Những mô hình này, còn được gọi là mô hình ngôn ngữ của mã, sử dụng các kỹ thuật tự giám sát, bao gồm các kiến trúc dựa trên BERT như Mô hình Ngôn ngữ Che mặt (MLM) và Dự đoán Câu Tiếp theo (NSP). Việc huấn luyện này trên các kho dữ liệu lớn cho phép chúng nắm bắt các biểu diễn phổ quát của cả mã nguồn và ngôn ngữ tự nhiên cụ thể lập trình. Những mô hình này mang lại lợi ích có giá trị cho các tác vụ hạ nguồn đa dạng, loại bỏ nhu cầu xây dựng các mô hình mới từ đầu và tăng cường khả năng tái sử dụng. Đáng chú ý, những tiến bộ gần đây đã tạo ra các mô hình mã được tiền huấn luyện dựa trên Transformer khác nhau (ví dụ: CodeBERT, GraphCodeBERT, CodeGPT, CodeT5). Bài báo này tập trung vào hai mô hình transformer cụ thể: CodeBERT và GraphCodeBERT.

CodeBERT [5] là một mô hình được tiền huấn luyện đa phương thức đa ngôn ngữ cho ngôn ngữ lập trình (PL) và ngôn ngữ tự nhiên (NL). Nó có một bộ mã hóa Transformer đa lớp, được huấn luyện trên Mô hình Ngôn ngữ Che mặt (MLM), và Phát hiện Token Thay thế (RTD) với cả NL và PL làm đầu vào. Mô hình có kiến trúc tương tự như BERT [17] và cho thấy kết quả đầy hứa hẹn trên nhiều tác vụ hạ nguồn như Dịch Mã, Phát hiện Bản sao, Phát hiện Lỗi, v.v. [5]. CodeBERT đã được chọn làm một trong các mô hình mã của chúng tôi do tính phổ biến của nó vào thời điểm tiến hành nghiên cứu này và nhiều công trình liên quan đã đề xuất một kỹ thuật dựa trên nó hoặc sử dụng nó làm đường cơ sở so sánh [18].

GraphCodeBERT [6] là một mô hình được tiền huấn luyện tương tự như CodeBERT nhưng cũng xem xét cấu trúc cấp độ ngữ nghĩa của mã. Nó sử dụng luồng dữ liệu trong giai đoạn tiền huấn luyện và sử dụng MLM, cùng với Dự đoán Cạnh và Căn chỉnh Nút như các tác vụ tiền huấn luyện. Với tính năng này được bao gồm, mô hình có thể cải thiện kết quả trên các tác vụ chuẩn của nó so với CodeBERT, nhưng như chúng ta sẽ thấy trong các thí nghiệm của mình, trong các tác vụ như Tạo Tài liệu Mã, nó cho thấy một nhược điểm lớn. GraphCodeBERT đã được chọn làm một trong các mô hình mã của chúng tôi vì ít nhất về mặt lý thuyết, nó là một bước tiến so với CodeBERT với thông tin được thêm vào mô hình từ chính mã. Nói cách khác, nó thuộc về một loại mô hình mã khác, điều này giúp cho tính tổng quát của các phát hiện của chúng tôi.

B. Tinh chỉnh Các Mô hình Mã được Tiền huấn luyện trên Ba Tác vụ Hạ nguồn

Các mô hình mã được tiền huấn luyện hiện có đã được sử dụng cho các tác vụ kỹ thuật phần mềm hạ nguồn khác nhau. có thể được phân loại thành bốn loại: (1) Văn bản →Văn bản (ví dụ: dịch ngôn ngữ của tài liệu mã [7], cải cách truy vấn [19]); (2) Văn bản →Mã (ví dụ: tìm kiếm mã [20], [21]); (3) Mã→Văn bản (ví dụ: tóm tắt mã [22], tạo thông điệp commit [23], [24]); và (4) Mã →Mã (ví dụ: sửa chữa chương trình tự động [25], [26], [27], dịch ngôn ngữ lập trình [28], hoàn thành mã [29]). Trong bài báo này, chúng tôi sẽ tập trung vào ba tác vụ hạ nguồn sau.

Tạo Tài liệu Mã (CDG) hoặc Tóm tắt Mã (Mã →Văn bản) là một tác vụ NLP được thiết kế để tạo ra các bình luận ngôn ngữ tự nhiên cho một mã nguồn đã cho, điều này có thể giúp các nhà phát triển hiểu rõ hơn các mã trong các dự án phần mềm với các bình luận sai hoặc thiếu và giảm thời gian thêm cần được dành cho việc đọc mã nguồn. Ví dụ, cho một phương thức Python ("def sum(x,y): ..."), mô hình NLP sẽ tạo ra các bình luận ngôn ngữ tự nhiên như ("Đây là một hàm tổng").

Tinh chỉnh Mã (Mã→Mã) là một tác vụ NLP được thiết kế để tạo ra mã nguồn được tinh chỉnh (ví dụ: một phiên bản đã sửa) cho một mã nguồn đã cho (ví dụ: một phiên bản có lỗi). Tinh chỉnh mã đã được nghiên cứu rộng rãi trong bối cảnh đánh giá mã [30], [31], [32], giúp các nhà phát triển nhận được mã tinh chỉnh có khả năng được phê duyệt mà không cần chờ phản hồi của người đánh giá.

Dịch Mã (Mã→Mã) là một tác vụ NLP được thiết kế để tạo ra mã nguồn trong một ngôn ngữ (ví dụ: Java) cho một mã nguồn đã cho trong một ngôn ngữ khác (ví dụ: C#).

C. Cài đặt siêu tham số

Trong quá trình huấn luyện mô hình, chúng tôi sử dụng các giá trị tham số mặc định như sau: độ dài nguồn tối đa là 256 và độ dài đích tối đa là 128 với tốc độ học 5e-4, với kích thước lô là 16 và huấn luyện trong 100 epoch.

Cả hai mô hình đều tuân theo các bước giống nhau để tạo ra đầu ra. Sau khi huấn luyện một mô hình trên một tác vụ hạ nguồn trên tập dữ liệu huấn luyện tương ứng, mô hình tạo ra đầu ra cho mỗi mục dữ liệu kiểm tra, từng token một. Đó là, trong thời gian suy luận, trong mỗi bước, một số token (tùy thuộc vào kích thước chùm, được đặt trong mô hình) được chọn từ các ứng viên dự đoán tiềm năng, và quá trình này được lặp lại (các token mới được thêm vào chuỗi đầu ra ứng viên) cho đến khi mô hình tạo ra token kết thúc câu, báo hiệu kết thúc dự đoán.

D. Đánh giá Độ chính xác của Mô hình

Để đảm bảo rằng các giải thích được tạo ra từ các mô hình của chúng tôi là đáng tin cậy, các mô hình phải chính xác. Để đánh giá độ chính xác của mô hình, chúng tôi sử dụng điểm BLEU-4 được làm mượt được sử dụng trong nghiên cứu gốc của CodeBERT [5] và thường được sử dụng bởi các kỹ thuật tạo tài liệu cơ sở [33]. Đối với các tác vụ khác, chúng tôi sử dụng điểm BLEU-4. BLEU-4 là chỉ số đánh giá duy nhất chúng tôi sử dụng trong bài báo này và từ bây giờ, trừ khi chúng tôi nói rõ ràng khác đi khi chúng tôi sử dụng điểm BLEU, chúng tôi đang đề cập đến điểm BLEU-4. Điểm BLEU [34] tính toán sự chồng chéo n-gram của dự đoán và tài liệu hoặc đoạn mã vàng. Vì trong CDG, các câu được tạo ra thường ngắn, và n-gram cấp độ cao hơn không có khả năng có sự chồng chéo, CodeBERT sử dụng một phiên bản được làm mượt [35] để bù đắp điều đó, bằng cách cung cấp thêm số đếm cho các chồng chéo n-gram cấp độ cao hơn.

Đối với tác vụ tạo tài liệu mã, CodeBERT đạt được điểm BLEU tổng thể là 17.83 (19.06 và 17.65 cho Python và Java, tương ứng), trong khi GraphCodeBERT đạt được điểm BLEU tổng thể là 5.3 và 4.21 cho Java và Python, tương ứng. Đối với tác vụ tinh chỉnh mã, CodeBERT đạt được điểm BLEU là 91.07 với khớp chính xác là 5.16, trong khi GraphCodeBERT đạt được điểm BLEU là 91.31 với khớp chính xác là 9.1. Đối với tác vụ dịch mã (Java sang C#), CodeBERT đạt được điểm BLEU là 79.92 với khớp chính xác là 59.0, trong khi GraphCodeBERT đạt được điểm BLEU là 80.58 với khớp chính xác là 59.4.

E. Giải thích các Mô hình thông qua Điểm Chú ý

Các mô hình Transformer có thể hiểu các phụ thuộc dài giữa các từ trong một câu (hoặc các token trong một đoạn mã) bằng cách hưởng lợi từ cơ chế chú ý. Cơ chế chú ý về cơ bản hoạt động với một khóa (k), truy vấn (q), và giá trị (v), và dk biểu thị chiều của khóa. Trong dạng đơn giản nhất, nó tính toán sự tương tự giữa truy vấn, khóa, và giá trị như:

Attention(q, k, v) = softmaxqkT/√dk V

trong đó khóa và truy vấn là các phần tử trong chuỗi. Tính toán tất cả các tích vô hướng của qi.kj sẽ cho ra một ma trận trong đó mỗi hàng đại diện cho trọng số chú ý cho một phần tử cụ thể i đối với tất cả các phần tử khác trong chuỗi. Sau đó, một lớp softmax và phép nhân với vector giá trị sẽ được áp dụng cho ma trận để có được trung bình có trọng số. Điều này có nghĩa là mọi phụ thuộc giữa mọi hai phần tử sẽ được xem xét trong đầu ra cuối cùng.

Chú ý là một phương pháp XAI hợp lý để giải thích CodeBERT và GraphCodeBERT. Để tính toán sự chú ý cho mỗi token, chúng ta cần các trọng số cho các lớp chú ý encoder-decoder. Vì chúng ta có sáu lớp decoder Transformer được xếp chồng, chúng ta có sáu lớp chú ý. Thay vì tổng hợp các giá trị chú ý của 6 lớp này thành một chỉ số nào đó, chúng ta quyết định giữ dữ liệu của tất cả các lớp và phân tích vai trò của các lớp khác nhau trong việc giải thích các đầu ra. Các trọng số chú ý có sẵn bên trong mô hình, nhưng thư viện Transformers được sử dụng bởi CodeBERT và GraphCodeBERT không cung cấp chúng theo mặc định. Do đó, chúng tôi đã thay đổi cách triển khai của mô hình để thu thập chúng, cũng như vậy. Lưu ý rằng điểm chú ý cho mỗi token là đầu ra của một lớp softmax, do đó nó có giá trị từ 0 đến 1.

IV. KẾT QUẢ THỰC NGHIỆM

A. (RQ1) Các mô hình mã được tiền huấn luyện học được gì?

Phương pháp. Để trả lời RQ này, chúng tôi phân tích các trọng số chú ý theo loại token và không phải token cá nhân. Để làm như vậy, trước tiên chúng ta cần định nghĩa một danh sách các loại token. Đây là một quyết định chủ quan về những loại token nào được quan tâm cho nghiên cứu của chúng tôi. Chúng tôi chọn một tập hợp bảy loại token bao phủ tất cả các token và nhóm chúng theo tính liên quan ngữ nghĩa của chúng, như sau:

Tên phương thức: Tên của phương thức đang được nghiên cứu có thể là một trong những yếu tố quyết định chính trong việc nhận thức về những gì một phương thức làm, đây là một bước rất quan trọng cho mô hình, đặc biệt trong một số tác vụ như CDG. Điều này chỉ bao gồm tên phương thức chính trong mỗi mẫu (nhắc nhở rằng mỗi mẫu đầu vào là mã nguồn cho một phương thức) và không phải các phương thức được gọi trong thân phương thức chính.

Định danh loại: Loại này đại diện cho tất cả các từ khóa được sử dụng để xác định các loại token trong ngôn ngữ nguồn Python và Java của chúng ta. Để biết thêm thông tin về những token này, hãy xem "định danh loại" và "* loại" các loại nút trong tree-sitter cho mỗi ngôn ngữ.

Từ khóa ngôn ngữ: Các token lệnh điều khiển luồng đều được gói gọn lại với nhau cho mỗi ngôn ngữ trong loại này. Những token này như sau: Đối với Python: {False, None, True, and, as, assert, async, await, break, class, continue, def, del, elif, else, except, finally, for, from, global, if, import, in, is, lambda, nonlocal, not, or, pass, raise, return, try, while, with, yield} và cho Java: {if, else, switch, case, while, class, enum, interface, annotation, public, protected, private, static, abstract, final, native, synchronized, transient, volatile, strictfp, assert, return, throw, try, catch, finally, default, super, do, for, break, continue, super, void, import, extends, implements, import, instanceof, new, null, package, this, throws}.

Lời gọi phương thức: Loại này bao gồm tất cả các token là tên của các phương thức được gọi trong thân của phương thức đang được nghiên cứu. Chúng cũng có thể có tác dụng trong việc mô tả những gì phương thức đang làm và có thể chứa lỗi cần sửa.

Biến cục bộ: Ở đây chúng ta xem xét tất cả các biến chỉ được sử dụng trong thân của phương thức đang được nghiên cứu. Đó là, các đối số đầu vào của phương thức bị loại trừ.

Biến đầu vào: Loại này chỉ chứa các đối số đầu vào của phương thức đang được nghiên cứu. Chúng tôi đã tách nó khỏi loại Biến cục bộ.

Khác: Loại này đại diện cho tất cả các token không được bao gồm trong bất kỳ loại nào ở trên. Chủ yếu là các token như dấu chấm câu, giá trị hằng số, dấu ngoặc đơn, v.v.

Lưu ý rằng mặc dù những loại token này được chọn một cách chủ quan, kết quả sẽ biện minh cho lựa chọn thiết kế này bằng cách cho thấy rằng chúng là một trong những token quan trọng nhất và không có nhiều token đóng góp được để lại cho loại "Khác". Chúng tôi cũng nên nhấn mạnh rằng mức độ trừu tượng về những gì tạo thành một "loại token" tùy thuộc vào người dùng XAI. Ví dụ, chúng tôi quyết định tách loại Đối số Đầu vào khỏi loại Tên Biến, để phân tích tốt hơn tác động của chúng một cách riêng lẻ, nhưng việc hợp nhất hai loại là một lựa chọn thiết kế hợp lệ (chỉ ở mức độ trừu tượng khác).

Đối với mỗi mẫu trong dữ liệu kiểm tra, chúng tôi thực hiện các bước sau để tìm phân phối của điểm chú ý trên các loại khác nhau:

Đối với mỗi token được tạo (mỗi bước), chúng tôi lấy trọng số chú ý của nó đối với các token đầu vào và tìm loại tương ứng của chúng. Sau đó, chúng tôi tích lũy điểm chú ý của tất cả các token trong mỗi loại để có được tổng điểm cho loại đó trong mẫu đó. Các loại của chúng tôi bao phủ tất cả các token nên tổng của tất cả điểm số cho mỗi bước bằng một. Chúng tôi thực hiện cùng một quá trình cho tất cả các token đầu ra trong tất cả các đoạn mã của tập dữ liệu thử nghiệm và thu thập sự tích lũy của điểm chú ý cho mỗi loại.

Điều đáng chú ý là kích thước của các loại token khá mất cân bằng. Ví dụ, chỉ có một tên phương thức cho mỗi mẫu nhưng có nhiều token trong loại "khác". Do đó, chúng tôi chuẩn hóa tổng điểm của mỗi loại, theo dân số của nó như trong Bảng II. Điều này cho chúng ta điểm chú ý cho mỗi token cho mỗi loại. Cuối cùng, chúng tôi chuẩn hóa điểm của tất cả các loại, từ 0 đến 100 cho mục đích so sánh dễ dàng hơn.

Trong số những loại được định nghĩa này, "Tên phương thức", "Biến đầu vào", và "Biến cục bộ" đại diện hơn cho các khía cạnh đặt tên của mã nguồn. Vì vậy chúng tôi nhóm chúng trong một loại cấp cao hơn của "Đặt tên", trong khi "Lời gọi phương thức", "Định danh loại", và "Từ khóa ngôn ngữ" liên quan nhiều hơn đến cấu trúc của mã. Do đó, chúng tôi coi chúng là loại cấp cao hơn của "Cấu trúc". Cũng lưu ý rằng chúng tôi chỉ báo cáo điểm trung bình của tất cả sáu lớp cho mỗi tác vụ và mô hình ở đây, vì việc báo cáo tất cả kết quả theo lớp sẽ quá dài và cũng kết quả của RQ này khá tương tự trên các lớp khác nhau và tất cả chúng đều tuân theo các mẫu giống nhau.

Kết quả. Trong ba tác vụ hạ nguồn đang được nghiên cứu, chúng tôi mong đợi điểm chú ý chuẩn hóa khác nhau cho mỗi loại cấp cao, như sau: (a) CT là một tác vụ phụ thuộc rất nhiều vào cấu trúc, vì mô hình phải học cấu trúc ngôn ngữ nguồn, và tạo ra cấu trúc tương đương trong ngôn ngữ đích. (b) Trong CDG, cấu trúc ít quan trọng hơn (các khối lồng nhau và cây cú pháp có ít liên quan đến tài liệu đầu ra). Mặt khác, tên rất quan trọng trong tác vụ này, vì chúng về cơ bản mô tả chức năng của mã nguồn. (c) Cuối cùng, chúng tôi mong đợi tinh chỉnh mã ở giữa hai đầu này, vì cả tên và cấu trúc đều quan trọng trong việc gỡ lỗi mã.

Bảng III cho thấy tổng điểm chú ý chuẩn hóa của mỗi loại cấp cao và xác nhận giả thuyết của chúng tôi. Tạo Tài liệu Mã, như một tác vụ phụ thuộc rất nhiều vào đặt tên, có điểm chú ý chuẩn hóa đáng kể cao hơn 63% cho loại Đặt tên, trong khi nó chú ý ít hơn nhiều đến loại token Cấu trúc, so với các tác vụ khác. Nó cũng có số cao hơn cho loại Khác có thể hiểu được xem xét thực tế rằng các bình luận NL trong mã cũng là một phần của loại này. Dịch Mã, mặt khác, là tác vụ duy nhất có điểm chú ý chuẩn hóa hơn 50% cho token Cấu trúc và ít hơn bất kỳ tác vụ nào cho loại Đặt tên. Tinh chỉnh Mã trong so sánh này giữ vị trí trung gian giữa hai tác vụ được đề cập trong cả hai loại.

Trong Bảng IV, chúng tôi có phân tích chi tiết hơn cho mỗi loại của chúng tôi. Chúng tôi thấy rằng cả hai mô hình đều chú ý nhiều hơn đến các token cấu trúc cho CT. Đi vào chi tiết hơn, kết quả cho thấy rằng sự chú ý này tập trung nhiều hơn vào Lời gọi phương thức, và Định danh loại hơn là Từ khóa ngôn ngữ, có khoảng 6% điểm chuẩn hóa. Rất thú vị là nghiên cứu riêng lẻ, loại Tên phương thức có điểm cao thứ hai sau Lời gọi phương thức. Mặc dù trong việc dịch mã, tên phương thức thường không thay đổi, điều này cho thấy rằng các mô hình sử dụng đáng kể tên của phương thức để hiểu chức năng của nó.

Trong tác vụ CDG, chúng tôi quan sát thấy sự phụ thuộc lớn vào các loại Đặt tên, kết quả cho thấy rằng loại Tên phương thức đóng vai trò quan trọng nhất. Nó luôn có điểm chuẩn hóa gần 40% hoặc cao hơn. Trong trường hợp không có Định danh loại, trong GraphCodeBERT CDG python, loại này có điểm cao nhất từng có trong số tất cả các loại token trên tất cả các tác vụ/mô hình. Theo trực giác, mức độ quan trọng này có thể biện minh, cho rằng ngay cả con người cũng phụ thuộc rất nhiều vào tên phương thức để hiểu chức năng của chúng. Ngoài ra, trong ba trong số bốn thí nghiệm khác nhau cho tác vụ này, Biến đầu vào có điểm chú ý chuẩn hóa cao thứ hai (từ 11% đến 16%). Điều này về cơ bản có nghĩa là các mô hình đã học được rằng trong khi tạo tài liệu cho một phương thức, phần chính và thú vị nhất là chữ ký của phương thức chứ không phải thân.

Tinh chỉnh Mã giữ vị trí trung gian giữa hai tác vụ khác có điểm rất gần cho bốn loại Tên phương thức, Biến đầu vào, Lời gọi phương thức, và Biến. Vì trong tác vụ này mô hình được cho là tìm kiếm lỗi và cố gắng sửa chúng, có vẻ như các mô hình đã học được rằng ít lỗi hơn xảy ra trong các loại như Định danh loại, Từ khóa ngôn ngữ, và Khác. Điều này có ý nghĩa vì chúng ta biết rằng cơ sở dữ liệu cho tác vụ này được thu thập từ các dự án công cộng và chúng có thể không có lỗi cú pháp. Tên phương thức cũng ít có khả năng có lỗi, nhưng như chúng ta đã thấy trong các tác vụ khác, loại này luôn có sức hấp dẫn tối thiểu đối với các mô hình.

Thú vị là theo bảng, Định danh loại là một loại hoàn toàn liên quan đến cú pháp của mã và không bao gồm bất kỳ đặt tên nào, có đóng góp nhiều nhất cho CT với khoảng cách so với những loại khác. Nó có điểm là 20.67% và 21.66% cho CodeBERT và GraphCodeBERT tương ứng trong khi điểm của nó trong các loại khác dưới 14%. Ngoài ra, đáng đề cập rằng GraphCodeBERT sử dụng luồng dữ liệu của mã để nắm bắt cấu trúc của nó; luôn có điểm cao hơn cho loại này so với CodeBERT.

Các mẫu tương tự xuất hiện hợp lệ cho tên Biến và cho CR. Điểm của loại này cho CR là 17.85% và 18.93%, trong khi trong các tác vụ khác điểm luôn dưới 12%.

Tương tự, loại Tên phương thức có điểm cao đáng kể trong CDG. Đối với tác vụ này trong python, loại này có điểm là 39.44% và 41.04%, và trong Java, nó có 46.17% và 54.21% cho CodeBERT và GraphCodeBERT, tương ứng.

Với tất cả những quan sát này, chúng ta có thể thấy một mẫu tầm quan trọng so sánh các tác vụ khác nhau với nhau. Tên phương thức và biến đầu vào (về cơ bản là dòng đầu tiên của các mẫu mã) là những loại quan trọng nhất cho CDG; Lời gọi phương thức và biến cục bộ đóng vai trò quan trọng nhất trong tinh chỉnh mã, cùng với tên phương thức với tầm quan trọng thấp hơn một chút. Mặt khác, dịch mã quan tâm đến định danh loại và từ khóa ngôn ngữ, nhiều hơn bất kỳ tác vụ nào khác, trong khi vẫn quan tâm đến một số loại đặt tên, cũng như vậy. Trong Bảng III, chúng tôi đã tổng hợp các số cho hai loại chính của chúng tôi và chúng ta có thể thấy một mẫu mà chúng tôi mong đợi. Các token đặt tên quan trọng đối với tất cả các tác vụ, nhưng ít hơn đối với dịch mã mà thay vào đó, quan tâm nhiều hơn đến các token cấu trúc so với các tác vụ khác.

--- TRANG 5 ---
BẢNG III: Điểm chú ý chuẩn hóa của hai loại token cấp cao,
cho các mô hình mã và tác vụ khác nhau.
Kết quả là trung bình của tất cả sáu lớp cho mỗi tác vụ.

Tác vụ Mô hình Đặt tên Cấu trúc Khác
Dịch MãCodeXGLUE 42.36% 51.38% 6.27%
GraphCodeBERT 42.60% 51.30% 6.09%
CDG javaCodeXGLUE 63.31% 29.19% 7.50%
GraphCodeBERT 64.51% 28.19% 7.30%
CDG pythonCodeXGLUE 67.97% 23.75% 8.28%
GraphCodeBERT 74.63% 17.83% 7.54%
Tinh chỉnh MãCodeXGLUE 56.46% 37.96% 5.58%
GraphCodeBERT 55.49% 38.86% 5.65%

--- TRANG 6 ---
BẢNG IV: Điểm chú ý chuẩn hóa của các loại token khác nhau,
cho các mô hình mã và tác vụ khác nhau. Kết quả là
trung bình của tất cả sáu lớp cho mỗi tác vụ.

Tác vụ/MôhìnhTên
phương thứcBiến
đầu vàoLời gọi
phương thứcBiếnĐịnh danh
loạiTừ khóa
ngôn ngữKhác
CT-Code
XGLUE21.36% 9.63% 24.26% 11.36% 20.67% 6.45% 6.27%
CT-Graph
CodeBERT22.78% 7.89% 23.46% 11.93% 21.66% 6.18% 6.09%
CDG Java-Code
XGLUE39.44% 13.88% 10.49% 10.00% 13.07% 5.63% 7.50%
CDG Java-Graph
CodeBERT41.04% 15.10% 8.44% 8.38% 13.13% 6.62% 7.30%
CDG Python-Code
XGLUE46.17% 11.96% 16.00% 9.83% 0.00% 7.76% 8.28%
CDG Python-Graph
CodeBERT54.21% 12.79% 10.79% 7.64% 0.00% 7.03% 7.54%
CR-Code
XGLUE22.01% 16.60% 20.33% 17.85% 9.82% 7.81% 5.58%
CR-Graph
CodeBERT19.36% 17.19% 21.15% 18.93% 10.02% 7.69% 5.65%

B. (RQ2) Khi nào các mô hình mã được tiền huấn luyện không hoạt động?

Phương pháp. Để cung cấp giải thích về khi nào CodeBERT và GraphCodeBert hoạt động tốt và khi nào chúng thất bại, trong RQ này, chúng tôi bắt đầu bằng một phân tích định tính của một số dự đoán mẫu. Sau đó chúng tôi đưa ra giả thuyết dựa trên quan sát của chúng tôi và cuối cùng xác minh chúng một cách định lượng trên toàn bộ tập dữ liệu. Để định nghĩa hiệu suất mạnh và yếu của các mô hình, chúng ta không thể đơn giản dựa vào các giá trị tuyệt đối của chỉ số đánh giá (BLEU). Vì độ lớn của điểm BLEU một phần phụ thuộc vào mức độ khó hoặc dễ của tác vụ tạo tài liệu, cho mỗi mã mẫu. Do đó, chúng ta cần đo lường mức độ khó của tác vụ tạo tài liệu nào đó, cho một mã nguồn.

Để giải quyết câu hỏi nghiên cứu này, chúng tôi tạo ra các chỉ số để đo lường độ phức tạp của mẫu và đánh giá hiệu suất mô hình tương ứng.

Trong bối cảnh CR và CT, nơi sao chép token chiếm ưu thế, Khoảng cách Levenshtein (LD) giữa đầu vào và đầu ra mong đợi ('đầu ra vàng') phục vụ như một chỉ số độ phức tạp phù hợp. Bằng cách tính toán LD cho tất cả các mẫu tập dữ liệu, chúng tôi xác định một phần ba dễ nhất dựa trên LD thấp hơn.

Đối với tác vụ CDG, mặc dù có sự khác biệt giữa đầu vào-đầu ra trong ngôn ngữ lập trình (PL) và ngôn ngữ tự nhiên (NL), phương pháp của chúng tôi vẫn tương tự. Chúng tôi xác định độ khó bằng cách giao nhau các token được xử lý trước trong tài liệu đầu ra vàng cho mỗi phương thức với các token trong mã nguồn của phương thức.

Để tìm sự chồng chéo giữa mã nguồn và token đầu ra, chúng tôi thực hiện các bước tiền xử lý sau: Đầu tiên, chúng tôi loại bỏ dấu chấm câu và các token ngắn hơn ba ký tự, trong tài liệu đầu ra. Sau đó, lemmatize những token đó bằng cách sử dụng trình lemmatizer Wordnet [36] tiêu chuẩn, được cung cấp trong gói NLTK. Tiếp theo, chúng tôi tokenize mã nguồn bằng cách sử dụng một trình phân tích cú pháp cho ngôn ngữ tương ứng. Lưu ý rằng do tokenization của CodeBERT và GraphCodeBERT, có thể chia một từ có ý nghĩa thành nhiều token, chúng tôi không sử dụng tokenization của chúng cho phân tích này. Cuối cùng, chúng tôi tạo ra một tập hợp các token không phân biệt chữ hoa chữ thường rơi vào giao điểm của các token đầu ra và mã nguồn đã được xử lý.

Bây giờ một tác vụ tạo tài liệu dễ/khó là khi sự chồng chéo giữa hai tập hợp cao/thấp. Do đó, giống như các ngưỡng cắt cho CR và CT, chúng tôi coi một phần ba đầu tiên các mẫu với sự chồng chéo cao nhất là dễ, và một phần ba các trường hợp với sự chồng chéo ít nhất là các vấn đề khó, và bỏ qua phần còn lại (mức độ khó trung bình).

Quá trình trên cho chúng ta biết mẫu nào được coi là khó và mẫu nào được coi là dễ. Bây giờ chúng ta cần đo lường hiệu suất của các mô hình. Để làm như vậy, chúng tôi sử dụng điểm BLEU vì nó là điểm được chấp nhận và đáng tin cậy nhất được áp dụng trong những tác vụ này trong tài liệu. Đối với độ chính xác, chúng tôi cũng lấy một phần ba các mẫu với điểm BLEU cao nhất là Cao và các mẫu với một phần ba điểm BLEU ít nhất là Thấp.

Có những định nghĩa này, sẽ có bốn loại (cho các tuple của <mức độ dễ, độ chính xác mô hình>) như dưới đây:

Easy−High: Loại này chứa các mục dữ liệu kiểm tra là các vấn đề dễ (có nghĩa là sự tương tự cao giữa đầu vào và dữ liệu vàng) mà các mô hình đã đạt được điểm BLEU Cao trên chúng.

Hard−High: Loại này chứa các mẫu được gắn nhãn là Khó và Cao. Điều này có nghĩa là ngay cả với việc thiếu token chung, mô hình đã có thể đạt được kết quả thỏa đáng trong những trường hợp này.

Hard−Low: Loại này bao gồm các trường hợp khó một lần nữa, và như mong đợi, kết thúc với độ chính xác Thấp cho dự đoán của mô hình.

Easy−Low: Loại này là loại thú vị nhất trong bài báo này, vì nó có thể cho thấy những điểm yếu tiềm năng của mô hình và rất phù hợp để được phân tích và "giải thích". Các mẫu trong nhóm này, nằm trong số các mẫu với sự chồng chéo cao hơn trong tập dữ liệu tương ứng của chúng có nghĩa là mô hình đang có công việc tương đối dễ dự đoán. Tuy nhiên, điểm BLEU như chỉ số của chúng tôi về độ chính xác của mô hình đang cho thấy hiệu suất kém so với các mẫu khác.

Để thực hiện quan sát thủ công của chúng tôi (nghiên cứu định tính) cho RQ này, sau khi nhóm tập dữ liệu kiểm tra của chúng tôi thành bốn loại này, chúng tôi chọn ngẫu nhiên 100 mẫu từ loại mục tiêu của chúng tôi (Easy−Low), và phân tích thủ công đầu ra và trọng số chú ý của chúng.

Đối với mỗi mẫu, chúng tôi ghi lại những phát hiện thú vị nhất để xác định các mẫu thường xuyên nhất. Bằng cách này chúng tôi phát triển một số giả thuyết. Cuối cùng, chúng tôi cố gắng xác minh những giả thuyết này bằng cách nghiên cứu định lượng toàn bộ tập dữ liệu kiểm tra, liên quan đến các giả thuyết. Đầu ra của giai đoạn định lượng này ở dạng một số thống kê mô tả để xác nhận hoặc bác bỏ các quan sát được thực hiện dựa trên 100 mẫu.

Kết quả. Với dữ liệu được chia theo các nhóm đã định nghĩa, Bảng V cho thấy tỷ lệ dân số loại mục tiêu so với toàn bộ tập dữ liệu cho mỗi tác vụ-mô hình. Tiếp theo, chúng tôi sẽ giải thích các quan sát từ phân tích thủ công.

Quan sát 1: Các mô hình mã được tiền huấn luyện không hoạt động tốt, khi tài liệu vàng đầu ra dài.

--- TRANG 7 ---
BẢNG V: Tỷ lệ dân số loại Easy-Low so với
toàn bộ tập dữ liệu, cho mỗi tác vụ-mô hình.

CodeBERT GraphCodeBERT
CT 11.70% 11.41%
CDG / Java 5.12% 5.59%
CDG / Python 5.87% 5.85%
CR 2.49% 4.37%

Hình 1: Điểm BLEU so với độ dài mã tài liệu vàng.

Quan sát đầu tiên của chúng tôi về tác vụ CDG là trong các trường hợp có tài liệu vàng dài, điểm BLEU có xu hướng thấp! Chúng tôi đã vẽ phân phối của điểm BLEU, theo độ dài của tài liệu vàng, trong Hình 1. Theo biểu đồ, hầu hết điểm BLEU cao xảy ra khi độ dài của tài liệu vàng ít hơn 50 ký tự.

Nói chung, ý tưởng của điểm BLEU là về việc đếm số lượng n-gram chung giữa tham chiếu và đầu ra. Các tài liệu được tạo ra bởi mô hình thường ngắn nên đối với các câu dài hơn, có ít cơ hội hơn để mô hình chọn cùng cách diễn đạt và từ với cùng thứ tự. Một giải thích hợp lý khác cho quan sát này là tài liệu dài hơn có nghĩa là phương thức thực hiện một tác vụ phức tạp hơn và do đó khó hơn cho mô hình để tạo ra tài liệu đúng cho phương thức phức tạp.

Một giải pháp tiềm năng cho vấn đề này là buộc mô hình tạo ra các chuỗi dài hơn làm đầu ra sẽ tăng cơ hội có điểm BLEU cao, trong các trường hợp có tài liệu tham chiếu dài. Nhưng rõ ràng, vì đây thực sự không phải là lỗi của mô hình và trong những trường hợp này, điểm BLEU thấp không nhất thiết chỉ ra một dự đoán tồi (như ví dụ được hiển thị trong Hình 2), cách tốt nhất để xử lý vấn đề này là xem xét các chỉ số đánh giá khác và lý tưởng nhất là những chỉ số chủ quan hơn.

Quan sát 2: Các mô hình mã được tiền huấn luyện không hoạt động tốt, khi mã nguồn đầu vào phức tạp.

Một quan sát thú vị khác là về độ dài của mã nguồn. Kết quả cho thấy rằng trong các trường hợp có mã dài hơn, điểm BLEU thường thấp hơn. Chúng tôi bắt đầu phân tích ban đầu với dữ liệu CDG và kết quả, được tóm tắt trong Hình 3, cho thấy xu hướng giảm của điểm BLEU, bằng sự gia tăng độ dài mã nguồn. Ví dụ, điểm BLEU trung bình cho các trường hợp ngắn hơn và dài hơn 300 token là 0.161 và 0.149, tương ứng.

Tài liệu vàng: Parse the cache control headers returning a
dictionary with values for the different directives.
Dự đoán tốt nhất: Parse a dict of headers
Điểm BLEU: 0.08
Chồng chéo: 0.54

Hình 2: Một phương thức mẫu được chia thành các token, trong đó các giá trị chú ý của lớp cuối cùng của CodeBERT cho token được tạo cuối cùng ("headers" trong ví dụ này) được tô sáng dưới dạng các sắc thái màu xanh (càng tối, càng cao).

Hình 3: Điểm BLEU so với độ dài mã nguồn.

Có hai giải thích cho quan sát này: (1) độ dài cố định của đầu vào trong các mô hình, về cơ bản có nghĩa là nếu độ dài của mã nguồn cao hơn một giá trị cố định (trong thí nghiệm của chúng tôi, 256 token), thì đầu vào sẽ bị cắt ngắn. Điều này có nghĩa là một số token sẽ không đến được decoder, và do đó chúng ta có sự suy giảm tiềm năng trong điểm cuối cùng. (2) độ phức tạp tăng của mã. Tương tự như Quan sát 1, mã nguồn dài hơn có nghĩa là logic phức tạp hơn, nhiều đối tượng hơn, và chức năng để mô hình xem xét có thể dẫn đến kết quả kém hơn.

Theo những lý do này, hai giải pháp cơ bản có thể được đề xuất: (a) tăng ngưỡng đầu vào và (b) giảm độ dài đầu vào. Ngưỡng đầu vào có thể dễ dàng được sửa đổi trong quá trình huấn luyện của mô hình và chỉ yêu cầu nhiều tài nguyên hơn. Tùy chọn thứ hai, tuy nhiên, là một giải pháp phức tạp hơn đã được thực hiện một cách ngây thơ bằng cách cắt ngắn. Một lựa chọn tiềm năng khác là tái cấu trúc các phương thức dài thành nhiều phương thức nhỏ hơn, sau đó chuyển mỗi phương thức cho các mô hình để tạo tài liệu cho, và cuối cùng hợp nhất tất cả tài liệu đầu ra thành một tài liệu.

Tiếp theo, để mở rộng quan sát này trên tất cả các tác vụ-mô hình, và tìm thêm kết quả thống kê, chúng tôi đã sử dụng một số chỉ số độ phức tạp mã thông thường và tiến hành phân tích trên tất cả 8 mô hình-tác vụ để hiểu rõ hơn nguyên nhân gốc rễ của kết quả kém cho các đoạn mã dài. Chúng tôi đã chọn 'số lượng token', 'độ phức tạp cyclomatic', 'độ sâu khối lồng nhau', 'số lượng biến' làm chỉ số độ phức tạp. Để đo lường độ khó của tác vụ, chúng tôi cũng bao gồm cùng 'khoảng cách Levenshtein' cho CT và CR và 'chồng chéo' cho CDG, trong phân tích của chúng tôi.

Đối với mỗi tác vụ-mô hình, chúng tôi có năm chỉ số khác nhau để nghiên cứu, vì vậy chúng tôi có một biểu đồ cho mỗi chỉ số. Trong mỗi biểu đồ, phân phối các mẫu trong tập dữ liệu tương ứng, liên quan đến chỉ số đó được hiển thị bằng các thanh màu xanh, và cùng phân phối nhưng chỉ cho loại mục tiêu (Easy-Low) được hiển thị bằng màu đỏ. Với việc trực quan hóa này, chúng ta có thể xác định bất kỳ sự khác biệt nào về xu hướng trên một chỉ số cụ thể trong loại mục tiêu so với toàn bộ tập dữ liệu.

Hình 4 cho thấy kết quả cho tinh chỉnh mã (CR) cho CodeBERT cho số lượng token và số lượng biến (tất cả các biểu đồ cho GraphCodeBERT và các chỉ số khác cho CodeBERT có thể được tìm thấy trong repo công cộng). Như được minh họa trong các biểu đồ, loại Easy-Low có phân phối rất tương tự với toàn bộ tập dữ liệu, ngoại trừ xu hướng tăng nhẹ cho một số chỉ số được báo cáo như số lượng token và số lượng biến. Điều này có nghĩa là mô hình có xu hướng đưa ra quyết định tồi, bất cứ khi nào mã nguồn trở nên phức tạp hơn về số lượng token và biến, ngay cả khi sự chồng chéo của token cao. Ví dụ, xem xét số lượng token, như một thước đo độ phức tạp mã, tỷ lệ các mẫu có hơn 100 token chủ yếu cao hơn trong loại Easy-Low so với phần của cùng các mẫu trong tổng số. Nó có nghĩa là các mẫu có nhiều token có nhiều khả năng được giả định "Dễ" trong các loại của chúng tôi (nhiều chồng chéo hơn giữa đầu vào và đầu ra) nhưng trên thực tế, chúng khó hơn cho mô hình để hiểu (cho độ dài dài của đoạn mã).

Hình 5 cho thấy kết quả cho dịch mã (CT) cho số lượng token, độ sâu khối lồng nhau, và độ phức tạp cyclomatic cho CodeBERT (tất cả các biểu đồ cho GraphCodeBERT và các chỉ số khác cho CodeBERT có thể được tìm thấy trong repo công cộng). Đối với tác vụ này, số lượng biến tuân theo cùng mẫu như tác vụ CR, đó là loại Easy-Low khó hơn dựa trên những chỉ số đó. Mặt khác, xem xét số lượng token, độ sâu khối lồng nhau, và thậm chí độ phức tạp cyclomatic, có một kết nối ngược lại. Nói cách khác, các mẫu có giá trị nhỏ hơn của những chỉ số này, có mật độ cao hơn trong loại Easy-Low. Ví dụ, trong cả hai mô hình, các mẫu có token ít hơn 20, chiếm khoảng 50% dân số loại mục tiêu, mặc dù chúng chiếm một phần rất nhỏ của tổng tập dữ liệu. Một giải thích hợp lý là khi mã nguồn quá ngắn (số lượng token rất nhỏ và rất ít khối lồng nhau), mô hình không thể dịch nó đúng cách, do thiếu đủ thông tin/ngữ cảnh. Một giải thích khác là thực tế rằng khó hơn để duy trì điểm BLEU cao khi đầu vào rất ngắn.

Hình 6 và Hình 7 minh họa một số kết quả tương tự cho tạo tài liệu mã (CDG) cho CodeBERT (tất cả các biểu đồ cho GraphCodeBERT và các chỉ số khác cho CodeBERT có thể được tìm thấy trong repo công cộng). Trong tác vụ hạ nguồn này, chúng ta có thể thấy các mẫu phụ thuộc nhiều hơn vào ngôn ngữ, hơn là mô hình. Trong tất cả các trường hợp, số lượng biến, số lượng token, và độ sâu khối lồng nhau tuân theo cùng mẫu chung.

Trong những thí nghiệm này, chúng ta thấy rằng cả hai mô hình đều gặp khó khăn với các mẫu có độ phức tạp thấp trong Java. Tuy nhiên, chúng có vấn đề tìm ra các mẫu phức tạp hơn trong Python, cũng như vậy. Ví dụ trong Python, các mẫu có hơn 80 token hoặc 8 biến, luôn có mật độ cao hơn trong loại Easy-Low so với tất cả tập dữ liệu. Cuối cùng, thú vị là xem xét độ phức tạp cyclomatic, cả hai mô hình trong cả hai ngôn ngữ đều gặp khó khăn với các mẫu có độ phức tạp cao hơn.

Vì vậy tổng cộng, người ta có thể kết luận rằng các mô hình mã hoạt động kém trên các đoạn mã có giá trị cực đoan của các chỉ số liên quan đến độ phức tạp theo cả hai hướng (tức là cả mã dài với nhiều khối lồng nhau và token và cũng mã rất ngắn chỉ với một vài biến và token).

--- TRANG 8 ---
(a)

(b)

Hình 4: Phân phối của toàn bộ tập dữ liệu (màu xanh) và loại Easy-Low (màu đỏ), theo các chỉ số độ phức tạp mã, cho tinh chỉnh mã trên CodeBERT

(a)

(b)

(c)

Hình 5: Phân phối của toàn bộ tập dữ liệu (màu xanh) và loại Easy-Low (màu đỏ), theo các chỉ số độ phức tạp mã, cho dịch mã trên CodeBERT

--- TRANG 9 ---
(a)

(b)

Hình 6: Phân phối của toàn bộ tập dữ liệu (xanh) và loại Easy-Low (đỏ), dựa trên các chỉ số độ phức tạp mã, cho tạo tài liệu mã trên CodeBERT trên Java

(a)

(b)

Hình 7: Phân phối của toàn bộ tập dữ liệu (xanh) và loại Easy-Low (đỏ), dựa trên các chỉ số độ phức tạp mã, cho tạo tài liệu mã trên CodeBERT trên Python

Quan sát 3: Các mô hình mã được tiền huấn luyện không hoạt động tốt, khi các mô hình không tập trung vào các loại quan trọng:

Cuối cùng, chúng tôi phân tích sự đóng góp của các loại token tương tự như RQ1, nhưng đặc biệt cho loại mục tiêu của (Easy−Low). Trong Bảng VI chúng tôi có điểm chuẩn hóa của hai loại chính cho các mẫu mục tiêu. Chúng tôi quan tâm đến việc so sánh kết quả cho loại này và kết quả trước đó cho toàn bộ tập dữ liệu kiểm tra. Do đó, chúng tôi tính toán sự khác biệt giữa hai loại này từ Bảng VI và Bảng III, và kết quả được hiển thị trong Bảng VII. Các số âm trong bảng này chỉ ra sự giảm trong loại mục tiêu.

Như kết quả cho thấy, có sự giảm đáng kể trong điểm cho loại Cấu trúc trong CT. Trong khi trả lời RQ1, chúng tôi đã chỉ ra rằng tác vụ này chủ yếu dựa vào nhóm token này IV-A. Tương tự, chúng ta có sự giảm nhẹ hơn trong điểm của loại Đặt tên trong CDG trong khi loại đặt tên cũng đã được chứng minh là loại quan trọng hơn cho CDG.

Cả hai manh mối này, dẫn chúng ta đến kết luận rằng kết quả ít thỏa đáng hơn, bất cứ khi nào mô hình không chú ý đủ đến loại token quan trọng tương ứng cho một tác vụ cụ thể. Dựa trên quan sát này, các câu hỏi nghiên cứu tiềm năng để điều tra trong tương lai là: "Mô hình sẽ hoạt động tốt hơn nếu chúng ta giúp nó bằng cách gắn thẻ các loại token? Việc khuếch đại thủ công điểm chú ý của các loại cụ thể theo tác vụ có thể có lợi cho các mô hình mã".

V. HẠN CHẾ

Một hạn chế của nghiên cứu này là các thí nghiệm của chúng tôi chỉ trên mã Java và Python. Việc bao gồm các tập dữ liệu khác (cũng như mã Python cho các tác vụ CT và CR) yêu cầu rất nhiều tiền xử lý để trở nên nhất quán với các thiết kế và yêu cầu của chúng tôi và sẽ vượt quá phạm vi và kích thước của một bài báo hội nghị. Chúng tôi dự định mở rộng những phân tích này sang các ngôn ngữ khác trong tương lai.

Một hạn chế khác là chúng tôi đã sử dụng điểm BLEU làm chỉ số đánh giá cho độ chính xác của mô hình, đây là chỉ số thường được sử dụng trong các tác vụ tạo tài liệu hạ nguồn để giảm tính chủ quan của kết quả. Tuy nhiên, như chúng tôi đã đề cập trong bài báo, nó không phải là một chỉ số toàn diện vì nó không thể tìm ra các cách diễn đạt lại hoặc các trường hợp mà dự đoán không sai, nhưng không khớp chính xác với nhãn vàng.

Các quan sát chúng tôi đưa ra cũng giới hạn trong các mẫu chính chúng tôi đã quan sát trong 100 mẫu, một cách thủ công. Mặc dù sau đó chúng tôi xác nhận định lượng chúng, tất nhiên có thể tồn tại một số giải thích khác cũng như vậy mà chúng tôi đã bỏ lỡ quan sát, do các mẫu chúng tôi đã chọn.

Ngoài ra, nghiên cứu chỉ giới hạn trong ba tác vụ hạ nguồn (CT, CDG, và CR) và hai mô hình mã (CodeBERT và GraphCodeBERT). Cần nhiều công việc hơn để tổng quát hóa các phát hiện cho các mô hình dựa trên Transformer khác, trong tương lai.

Đáng đề cập là việc triển khai tác vụ CDG trong khung GraphCodeBERT, được thực hiện sử dụng các cài đặt và siêu tham số giống hệt như các tác vụ và mô hình khác. Phương pháp thống nhất này có thể góp phần vào độ chính xác thấp hơn được quan sát trong hiệu suất của tác vụ CDG trong mô hình.

Cuối cùng, nghiên cứu này được tiến hành trước khi ChatGPT ra mắt công chúng. Vì vậy một mở rộng rất liên quan của công việc này sẽ là bao gồm mô hình GPT-4 cả như một mô hình mã cũng như một phương pháp XAI để cung cấp giải thích về các quyết định của chính nó và các mô hình khác.

VI. KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI

Bài báo này đề xuất một phương pháp để giải thích các mô hình mã được tiền huấn luyện (ví dụ: CodeBert và GraphCodeBert), sử dụng cơ chế chú ý đầu cuối đến đầu cuối nội bộ của chúng, như phương pháp XAI. Không giống như hầu hết nghiên cứu XAI, nơi giải thích được áp dụng cho các mô hình có độ chính xác cao để đảm bảo kết quả đáng tin cậy, chúng tôi đã sử dụng XAI trên cả kịch bản độ chính xác cao (để tìm hiểu những gì các mô hình học) và thấp (để xem khi nào chúng không hoạt động tốt). Các phát hiện của chúng tôi không chỉ cung cấp quan sát về những gì các mô hình dựa trên Transformer tiên tiến này học theo các loại token và tại sao chúng hoạt động kém trong một số kịch bản, mà còn đề xuất các khuyến nghị có thể thực hiện, chẳng hạn như sử dụng các chỉ số đánh giá chủ quan hơn cho tác vụ CDG, cung cấp các loại token như đầu vào bổ sung cho mô hình, và khuếch đại thủ công điểm chú ý cho các loại token cụ thể. Trong tương lai, chúng tôi dự định mở rộng công việc này bằng cách kiểm tra các tác vụ hạ nguồn, mô hình, và phương pháp XAI khác. Ngoài ra, chúng tôi cũng dự định làm việc trên các mô hình mã được tiền huấn luyện bằng cách triển khai các khuyến nghị được đề xuất từ các quan sát của chúng tôi. Cuối cùng, chúng tôi dự định sử dụng mô hình GPT-4 để cả truy cập kết quả của nó như mô hình mã và sử dụng nó như một kỹ thuật XAI để giải thích tại sao một đầu ra cụ thể được cung cấp bởi mô hình.

--- TRANG 10 ---
BẢNG VI: Điểm chú ý chuẩn hóa của các mẫu Easy-Low,
trong ba loại token tổng quát, cho các mô hình mã và tác vụ khác nhau.
Kết quả là trung bình của tất cả sáu lớp cho mỗi tác vụ.

Tác vụ Mô hình Đặt tên Cấu trúc Khác
CTCodeXGLUE 48.85% 44.51% 7.70%
GraphCodeBERT 49.51% 43.49% 7.57%
CDG javaCodeXGLUE 60.08% 33.28% 8.89%
GraphCodeBERT 63.85% 37.06% 7.43%
CDG pythonCodeXGLUE 66.37% 29.14% 8.61%
GraphCodeBERT 73.68% 24.16% 7.87%
CRCodeXGLUE 56.64% 46.29% 5.63%
GraphCodeBERT 55.82% 48.35% 5.62%

BẢNG VII: Sự khác biệt của điểm chú ý chuẩn hóa
cho các mẫu Easy-Low và tất cả mẫu, trong hai loại token tổng quát,
cho các mô hình mã và tác vụ khác nhau.
Kết quả là trung bình của tất cả sáu lớp cho mỗi tác vụ.

Tác vụ Mô hình Đặt tên Cấu trúc Khác
CTCodeXGLUE 6.49% -7.92% 1.43%
GraphCodeBERT 6.90% -8.38% 1.48%
CDG javaCodeXGLUE -3.24% 1.85% 1.39%
GraphCodeBERT -0.66% 0.53% 0.13%
CDG pythonCodeXGLUE -1.59% 1.27% 0.33%
GraphCodeBERT -0.96% 0.63% 0.33%
CRCodeXGLUE 0.18% -0.23% 0.06%
GraphCodeBERT 0.33% -0.30% -0.03%

--- TRANG 11 ---
TÀI LIỆU THAM KHẢO

[1] G. Shobha, A. Rana, V. Kansal, và S. Tanwar, "Code clone detection—a systematic review," Emerging Technologies in Data Mining and Information Security, pp. 645–655, 2021.

[2] C. Pornprasit, C. Tantithamthavorn, J. Jiarpakdee, M. Fu, và P. Thongtanunam, "Pyexplainer: Explaining the predictions of just-in-time defect models," trong 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2021, pp. 407–418.

[3] J. Humphreys và H. K. Dam, "An explainable deep model for defect prediction," trong 2019 IEEE/ACM 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering (RAISE). IEEE, 2019, pp. 49–55.

[4] A. LeClair, S. Haque, L. Wu, và C. McMillan, "Improved code summarization via a graph neural network," trong Proceedings of the 28th international conference on program comprehension, 2020, pp. 184–195.

[5] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang et al., "Codebert: A pre-trained model for programming and natural languages," trong Findings of the Association for Computational Linguistics: EMNLP 2020, 2020, pp. 1536–1547.

[6] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan, A. Svyatkovskiy, S. Fu et al., "Graphcodebert: Pre-training code representations with data flow," arXiv preprint arXiv:2009.08366, 2020.

[7] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement, D. Drain, D. Jiang, D. Tang et al., "Codexglue: A machine learning benchmark dataset for code understanding and generation," arXiv preprint arXiv:2102.04664, 2021.

[8] Y. Wang, W. Wang, S. Joty, và S. C. Hoi, "Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation," arXiv preprint arXiv:2109.00859, 2021.

[9] C. Tantithamthavorn, J. Jiarpakdee, và J. Grundy, "Actionable analytics: Stop telling me what it is; please tell me what to do," IEEE Software, vol. 38, no. 4, pp. 115–120, 2021.

[10] C. K. Tantithamthavorn và J. Jiarpakdee, "Explainable ai for software engineering," trong 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2021, pp. 1–2.

[11] J. Jiarpakdee, C. K. Tantithamthavorn, và J. Grundy, "Practitioners' perceptions of the goals and visual explanations of defect prediction models," trong 2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR). IEEE, 2021, pp. 432–443.

[12] J. Jiarpakdee, C. K. Tantithamthavorn, H. K. Dam, và J. Grundy, "An empirical study of model-agnostic techniques for defect prediction models," IEEE Transactions on Software Engineering, vol. 48, no. 1, pp. 166–185, 2020.

[13] D. Rajapaksha, C. Tantithamthavorn, J. Jiarpakdee, C. Bergmeir, J. Grundy, và W. Buntine, "Sqaplanner: Generating data-informed software quality improvement plans," IEEE Transactions on Software Engineering, vol. 48, no. 8, pp. 2814–2835, 2021.

[14] S. Wattanakriengkrai, P. Thongtanunam, C. Tantithamthavorn, H. Hata, và K. Matsumoto, "Predicting defective lines using a model-agnostic technique," IEEE Transactions on Software Engineering, vol. 48, no. 5, pp. 1480–1496, 2020.

[15] C. Pornprasit và C. K. Tantithamthavorn, "Jitline: A simpler, better, faster, finer-grained just-in-time defect prediction," trong 2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR). IEEE, 2021, pp. 369–379.

[16] C. Khanan, W. Luewichana, K. Pruktharathikoon, J. Jiarpakdee, C. Tantithamthavorn, M. Choetkiertikul, C. Ragkhitwetsagul, và T. Sunetnanta, "Jitbot: An explainable just-in-time defect prediction bot," trong 2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2020, pp. 1336–1339.

[17] J. Devlin, M.-W. Chang, K. Lee, và K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018.

[18] C. Pan, M. Lu, và B. Xu, "An empirical study on software defect prediction using codebert model," Applied Sciences, vol. 11, no. 11, p. 4793, 2021.

[19] K. Cao, C. Chen, S. Baltes, C. Treude, và X. Chen, "Automated query reformulation for efficient search based on query logs from stack overflow," trong 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE, 2021, pp. 1273–1285.

[20] X. Gu, H. Zhang, và S. Kim, "Deep code search," trong 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE). IEEE, 2018, pp. 933–944.

[21] T. Nguyen, P. C. Rigby, A. T. Nguyen, M. Karanfil, và T. N. Nguyen, "T2api: Synthesizing api code usage templates from english texts with statistical translation," trong Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering, 2016, pp. 1013–1017.

[22] S. Haque, A. LeClair, L. Wu, và C. McMillan, "Improved automatic summarization of subroutines via attention to file context," trong Proceedings of the 17th International Conference on Mining Software Repositories, 2020, pp. 300–310.

[23] S. Jiang, A. Armaly, và C. McMillan, "Automatically generating commit messages from diffs using neural machine translation," trong 2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2017, pp. 135–146.

[24] L. Liu, X. Hu, W. Song, R. Fu, T. Liu, và G. Hu, "Neural multitask learning for simile recognition," trong Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018, pp. 1543–1553.

[25] N. Jiang, T. Lutellier, và L. Tan, "Cure: Code-aware neural machine translation for automatic program repair," trong 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE, 2021, pp. 1161–1173.

[26] Y. Li, S. Wang, và T. N. Nguyen, "Dlfix: Context-based code transformation learning for automated program repair," trong Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, 2020, pp. 602–614.

[27] Z. Chen, S. Kommrusch, M. Tufano, L.-N. Pouchet, D. Poshyvanyk, và M. Monperrus, "Sequencer: Sequence-to-sequence learning for end-to-end program repair," IEEE Transactions on Software Engineering, vol. 47, no. 9, pp. 1943–1959, 2019.

[28] B. Roziere, M.-A. Lachaux, L. Chanussot, và G. Lample, "Unsupervised translation of programming languages," Advances in Neural Information Processing Systems, vol. 33, pp. 20 601–20 611, 2020.

[29] A. Svyatkovskiy, S. K. Deng, S. Fu, và N. Sundaresan, "Intellicode compose: Code generation using transformer," trong Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2020, pp. 1433–1443.

[30] R. Tufano, S. Masiero, A. Mastropaolo, L. Pascarella, D. Poshyvanyk, và G. Bavota, "Using pre-trained models to boost code review automation," trong Proceedings of the 44th International Conference on Software Engineering, 2022, pp. 2291–2302.

[31] P. Thongtanunam, C. Pornprasit, và C. Tantithamthavorn, "Autotransform: Automated code transformation to support modern code review process," trong Proceedings of the 44th International Conference on Software Engineering, 2022, pp. 237–248.

[32] Y. Liu, C. Tantithamthavorn, Y. Liu, P. Thongtanunam, và L. Li, "Autoupdate: Automatically recommend code updates for android apps," arXiv preprint arXiv:2209.07048, 2022.

[33] X. Hu, G. Li, X. Xia, D. Lo, và Z. Jin, "Deep code comment generation," trong 2018 IEEE/ACM 26th International Conference on Program Comprehension (ICPC). IEEE, 2018, pp. 200–20 010.

[34] K. Papineni, S. Roukos, T. Ward, và W.-J. Zhu, "Bleu: a method for automatic evaluation of machine translation," trong Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, pp. 311–318.

[35] C.-Y. Lin và F. J. Och, "Orange: a method for evaluating automatic evaluation metrics for machine translation," trong COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics, 2004, pp. 501–507.

[36] E. Loper và S. Bird, "Nltk: The natural language toolkit," CoRR, vol. cs.CL/0205028, 2002. [Trực tuyến]. Có sẵn: http://dblp.uni-trier.de/db/journals/corr/corr0205.html#cs-CL-0205028
