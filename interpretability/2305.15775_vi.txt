# Concept-Centric Transformers:
Enhancing Model Interpretability through Object-Centric Concept Learning
within a Shared Global Workspace
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/interpretability/2305.15775.pdf
# File size: 43544320 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Transformers Tập Trung Khái Niệm:
Tăng Cường Khả Năng Diễn Giải Mô Hình thông qua Học Khái Niệm Tập Trung Đối Tượng
trong Không Gian Làm Việc Toàn Cục Chia Sẻ
Jinyung Hong1, Keun Hee Park1, và Theodore P. Pavlic1, 2
1Trường Tin Học và Trí Tuệ Tăng Cường
2Trường Khoa Học Sự Sống
Đại Học Arizona State
Tempe, AZ 85281
{jhong53, kpark53, tpavlic }@asu.edu
Tóm Tắt
Nhiều phương pháp AI có thể diễn giải đã được đề xuất
để cung cấp các giải thích hợp lý cho việc ra quyết định
của mô hình. Tuy nhiên, việc cấu hình một mô hình có thể
giải thích hiệu quả giao tiếp giữa các mô-đun tính toán đã
nhận được ít sự chú ý hơn. Một lý thuyết không gian làm
việc toàn cục chia sẻ được đề xuất gần đây cho thấy rằng
các mạng của các mô-đun phân tán có thể hưởng lợi từ
việc chia sẻ thông tin với một bộ nhớ có cổ chai vì các ràng
buộc giao tiếp khuyến khích sự chuyên môn hóa, tính tổ
hợp và đồng bộ hóa giữa các mô-đun. Được truyền cảm
hứng từ điều này, chúng tôi đề xuất Transformers Tập
Trung Khái Niệm, một cấu hình đơn giản nhưng hiệu quả
của không gian làm việc toàn cục chia sẻ cho khả năng
diễn giải, bao gồm: i) một mô-đun bộ nhớ dựa trên tập
trung đối tượng để trích xuất các khái niệm ngữ nghĩa từ
các đặc trưng đầu vào, ii) một cơ chế cross-attention giữa
khái niệm đã học và embeddings đầu vào, và iii) các hàm
mất mát phân loại và giải thích tiêu chuẩn để cho phép
các nhà phân tích con người trực tiếp đánh giá một giải
thích cho lý luận phân loại của mô hình. Chúng tôi kiểm
tra phương pháp của mình so với các phương pháp dựa
trên khái niệm khác trên các tác vụ phân loại cho nhiều
bộ dữ liệu khác nhau, bao gồm CIFAR100, CUB-200-
2011, và ImageNet, và chúng tôi chỉ ra rằng mô hình của
chúng tôi đạt được độ chính xác phân loại tốt hơn so với
tất cả các baseline trên tất cả các bài toán mà còn tạo ra
các giải thích dựa trên khái niệm nhất quán hơn về đầu
ra phân loại.

1. Giới Thiệu
Mặc dù các mô hình machine-learning tiên tiến đã đạt
được hiệu suất đáng chú ý trên một loạt các ứng dụng rộng
rãi, sự thiếu minh bạch bẩm sinh của chúng do nhiều bậc
tự do huấn luyện hạn chế việc sử dụng trong các lĩnh vực
quan trọng về an toàn—như chẩn đoán y tế, chăm sóc sức
khỏe, an toàn cơ sở hạ tầng công cộng, và kiểm tra thị giác
cho kỹ thuật dân dụng—nơi mà kiến thức chuyên môn đáng
tin cậy là quan trọng cho việc ra quyết định. Gần đây, một
số phương pháp được phát triển cung cấp các giải thích
hậu kiểm xác định các đặc trưng liên quan mà một mô hình
đã huấn luyện sử dụng để đưa ra dự đoán [58, 64, 70, 78],
nhưng những phương pháp này thường được chỉ trích vì
chỉ tập trung vào các đặc trưng cấp thấp [1, 43, 44, 79].
Ngược lại, các mô hình có thể diễn giải nội tại [67] đã được
đề xuất để đưa ra quyết định dựa trên các "khái niệm" có
thể hiểu được của con người, nền tảng của chuyên môn
lĩnh vực [1,6,12,13,25,32,42,43,45,50,65,91,94]. Khoảng
cách giữa khả năng giải thích hậu kiểm và các mô hình có
thể diễn giải nội tại cũng được thảo luận trong cộng đồng
NLP liên quan đến việc diễn giải các cơ chế attention [3].
Đặc biệt, cuộc tranh luận về mức độ diễn giải nào có thể
được quy cho các trọng số attention trên các token đầu vào
vẫn cần được giải quyết để giúp đáp ứng nhu cầu diễn giải
các cơ chế attention [1, 9, 40, 71, 86].

Lý tưởng nhất, một mô hình có thể diễn giải nội tại sẽ tạo
ra các giải thích là sự tổng hợp của các mô-đun có ý nghĩa
riêng lẻ. Các giải thích theo mô-đun có thể cải thiện sự hiểu
biết của con người, và các hệ thống thần kinh tự nhiên tiếp
tục truyền cảm hứng cho việc phát triển AI thường được
mô tả là có kiến trúc mô-đun [4, 5, 8, 66]. Do đó, việc cấu
trúc hóa các thuật toán để thúc đẩy việc học các cấu trúc
tiềm ẩn mô-đun cũng có thể dẫn đến hiệu suất tổng thể tốt
hơn. Được thúc đẩy bởi việc cải thiện tính mô-đun trong
các mô hình có thể diễn giải, chúng tôi đề xuất Concept-
Centric Transformer (CCT), một framework của các mô
hình có thể diễn giải nội tại được truyền cảm hứng bởi
Shared Global Workspace (SGW) [30], một framework
khái niệm mới có ý định thúc đẩy tính mô-đun một cách
tổng quát bằng cách buộc các thành phần chuyên biệt song
song phải cạnh tranh để có quyền truy cập có cổ chai vào
một bộ nhớ chia sẻ. Cấu hình của CCT cho phép các mô
hình đã huấn luyện có cấu trúc đơn giản, mô-đun
arXiv:2305.15775v3  [cs.LG]  7 Nov 2023

--- TRANG 2 ---
tures có thể trích xuất các khái niệm ngữ nghĩa có hoặc
không có sự hướng dẫn của các giải thích cơ sở thực tế về
các khái niệm.

Trong phần tiếp theo, chúng tôi khung hóa CCT của
chúng tôi như một phần mở rộng mới của khái niệm SGW
cho việc phát triển mô hình có thể diễn giải, và chúng tôi
mô tả cách CCT được triển khai sử dụng ba thành phần
chính: i) mô-đun Concept-Slot-Attention (CSA) giao tiếp
với image embedding từ một mô hình backbone và tạo ra
một tập hợp các embeddings phụ thuộc tác vụ cho các khái
niệm, ii) mô-đun Cross-Attention (CA) tạo ra các đầu ra
phân loại sử dụng cross-attention giữa các đặc trưng đầu
vào và embeddings của mô-đun CSA, và iii) các hình phạt
mất mát chuyên biệt, bao gồm Explanation Loss, khi kiến
thức chuyên gia có thể được tận dụng, và Sparsity Loss,
một mất mát dựa trên entropy để thực thi sự thưa thớt nhằm
xác định tầm quan trọng của các đặc trưng trong quá trình
huấn luyện. Kiến trúc CCT được thiết kế để tăng cường
các backbone deep-learning hiện có để thêm khả năng giải
thích cho chúng. Do đó, chúng tôi xác thực phương pháp
của mình trên ba bộ dữ liệu benchmark hình ảnh—CIFAR100
Super-class [23], CUB-200-2011 [83], và ImageNet [19]—
kết hợp nó với các backbone deep-learning khác nhau, như
Vision Transformer (ViT) [22], Swin Transformer (SwinT)
[55], và ConvNeXt [56].

2. Công Trình Liên Quan
Những tiến bộ đáng kể gần đây đã được thực hiện trong
việc phát minh các mô hình có thể giải thích và diễn giải
để đo lường tầm quan trọng của các đặc trưng riêng lẻ cho
đầu ra dự đoán. Phân tích hậu kiểm là một phương pháp
tổng quát để phân tích một mô hình đã huấn luyện bằng
cách khớp các giải thích với đầu ra phân loại [1, 58, 64].
Ví dụ, tối ưu hóa kích hoạt [62, 81, 92] và trực quan hóa
saliency [70, 78, 80] là các phương pháp nổi tiếng cho
CNNs. Các phương pháp có thể diễn giải dựa trên attention
cũng đã được giới thiệu để xác định các phần liên quan
nhất của đầu vào mà mạng tập trung vào khi đưa ra quyết
định [24, 26, 27, 37, 96–99].

Ngoài ra, việc thiết kế các phương pháp giải thích dự
đoán với các khái niệm cấp cao, có thể hiểu được của con
người [6, 12, 13, 25, 32, 42, 43, 45, 50, 65, 89, 91, 94] là
một trong những tiến bộ gần đây trong lĩnh vực khả năng
diễn giải. Những phương pháp có thể diễn giải nội tại này
tập trung vào việc xác định các mẫu kích hoạt chung trong
các node của lớp cuối của mạng nơ-ron tương ứng với các
danh mục có thể hiểu được của con người hoặc ràng buộc
mạng để học các khái niệm như vậy. Trong số đó, công
trình của chúng tôi tương tự nhất với Concept Transformers
(CTs) [65], một framework học các khái niệm cấp cao được
định nghĩa với một tập hợp các chiều liên quan. Những
khái niệm đó, có thể là cụ thể theo phần hoặc toàn cục,
thường có thể tăng cường hiệu suất của tác vụ học trong
khi cung cấp khả năng giải thích mà không tốn thêm chi
phí nào cho mạng. Tuy nhiên, phương pháp đó dựa vào
việc trích xuất khái niệm dựa trên các mảnh hình ảnh được
cung cấp mặc dù mỗi mảnh hình ảnh có thể là một yếu tố
dự đoán không đáng tin cậy của các khái niệm cấp cao.
Công thức CCT của chúng tôi tổng quát hóa phương pháp
này vượt ra ngoài các mảnh hình ảnh.

Từ lịch sử, người ta đã lập luận rằng tốt hơn là xây dựng
một hệ thống thông minh từ nhiều mô-đun chuyên biệt
tương tác thay vì một thực thể "nguyên khối" duy nhất để
đối phó với một phổ rộng các điều kiện và tác vụ [29, 59,
66]. Do đó, đã có nỗ lực đáng kể về đồng bộ hóa giữa các
mô-đun chuyên biệt tính toán thông qua một không gian
làm việc toàn cục chia sẻ [16,31,39,60,69]. Hơn nữa, công
trình về việc tích hợp các kiến trúc tính toán mô-đun với
bộ nhớ làm việc lấy cảm hứng từ sinh học, khoa học thần
kinh, và khoa học nhận thức [35, 36, 63, 87]. Không gian
làm việc toàn cục chia sẻ được đề xuất gần đây [30] cho
thấy cách sử dụng cơ chế attention để khuyến khích thông
tin hữu ích nhất được chia sẻ giữa các mô-đun thần kinh
trong các framework AI hiện đại. Phương pháp này là cảm
hứng cho việc sử dụng một bộ nhớ làm việc rõ ràng trong
CCT để cải thiện khả năng tổng quát của các mô hình dựa
trên Transformer- và object-centric trong bối cảnh các mô
hình có thể giải thích.

3. Kiến Thức Cơ Bản
Không Gian Làm Việc Toàn Cục Chia Sẻ trong Các
Mô Hình AI. Được truyền cảm hứng bởi Global Workspace
Theory (GWT) từ khoa học nhận thức [2, 17, 18, 72–75],
Shared Global Workspace (SGW) [30] khám phá cách GWT
có thể được thể hiện trong các mô hình AI hiện đại để sở
hữu các sơ đồ giao tiếp và phối hợp nơi một số chuyên gia
giao tiếp thưa thớt (các mô-đun tính toán cụ thể xử lý đầu
vào) tương tác thông qua một không gian làm việc chia sẻ
(một mô-đun bộ nhớ làm việc chia sẻ). Để làm như vậy,
các phương pháp transformer và dựa trên slot đã được mở
rộng bằng cách thêm một không gian làm việc chia sẻ và
cho phép các mô-đun cạnh tranh để có quyền truy cập ghi
trong mỗi giai đoạn tính toán (Hình 1). Việc thay thế giao
tiếp từng cặp giữa các mô-đun bằng tương tác được tạo
thuận lợi bởi không gian làm việc chia sẻ cho phép: i) tương
tác bậc cao hơn giữa các mô-đun, ii) lọc động do tính bền
vững của bộ nhớ, và iii) sự tinh vi tính toán của việc sử
dụng không gian làm việc chia sẻ để đồng bộ hóa các
chuyên gia khác nhau. Được thúc đẩy bởi SGW, chúng tôi
nhằm khám phá một cấu hình hiệu quả cho các framework
AI có thể diễn giải nội tại và đề xuất một cách đơn giản
nhưng hiệu quả để tương tác giữa một không gian làm việc
chia sẻ và các chuyên gia để cải thiện khả năng diễn giải
và hiệu suất.

Các Biến Thể của Slot-Attention. Do thiết kế đơn giản
nhưng hiệu quả, Slot-Attention (SA) [57] đã thu hút sự
chú ý đáng kể trong học tập tập trung đối tượng không
giám sát để bắt chước sự phát triển của sự hiểu biết tượng
trưng trong nhận thức con người. Cơ chế attention lặp cho
phép SA học và cạnh tranh giữa các slot để giải thích các
phần của đầu vào, cho thấy hiệu ứng phân cụm mềm trên
đầu vào thị giác [57]. Tuy nhiên, như được tiết lộ bởi các
nghiên cứu gần đây, mô-đun SA gốc có giới hạn bẩm sinh
trong đó: i) khởi tạo ngẫu nhiên cho các slot cản trở việc
giải quyết ràng buộc đối tượng trong đầu vào và ii) nó

--- TRANG 3 ---
Hình 1. Không gian làm việc toàn cục chia sẻ [30] xuất hiện từ
ba bước: 1) Một tập hợp các mô-đun tính toán (hoặc chuyên gia)
thực hiện xử lý tiêu chuẩn, và một tập con của các chuyên gia
trở nên hoạt động tại một giai đoạn tính toán cụ thể tùy thuộc
vào đầu vào; 2) Chuyên gia hoạt động ghi thông tin trong một
mô-đun bộ nhớ làm việc chia sẻ (hoặc không gian làm việc chia
sẻ); 3) Nội dung được cập nhật của không gian làm việc được
phát sóng đến tất cả các chuyên gia. Chúng tôi khám phá cách
những bước này có thể được sử dụng để thêm khả năng giải
thích trong các framework AI. Hình chung chung ở trên được
truyền cảm hứng bởi [30, Hình 1].

phụ thuộc nhiều vào việc điều chỉnh siêu tham số để nó
không thể được áp dụng chung trong nhiều lĩnh vực. Do
đó, một số biến thể của SA, bao gồm I-SA [10] và BO-QSA
[41], đã được đề xuất gần đây để giải quyết những vấn đề
đó1.

Có một số ví dụ hiện có về việc tận dụng các phương
pháp dựa trên slot trong các mô hình có thể giải thích để
trích xuất các khái niệm ngữ nghĩa [49, 84]. Tuy nhiên, ít
nghiên cứu nhấn mạnh góc độ kiến trúc mô-đun để thúc
đẩy giao tiếp giữa mô hình dựa trên slot và các mô-đun
khác. Chúng tôi tận dụng ba biến thể SA ở trên như không
gian làm việc chia sẻ của SGW và khám phá cách khuyến
khích các tương tác giữa chúng để đạt được khả năng diễn
giải và hiệu suất tốt hơn.

4. Transformers Tập Trung Khái Niệm
Đối với các tác vụ phân loại có giám sát, chúng tôi giới
thiệu Concept-Centric Transformers (CCTs), một thể hiện
của SGW để cấu hình một mô hình có thể diễn giải nội tại,
và chúng tôi sẽ mô tả kết nối giữa các bước SGW (Hình 1)
và công thức của chúng tôi. Mô hình của chúng tôi bao
gồm: i) mô-đun Concept-Slot-Attention (CSA) hoạt động
như một mô-đun bộ nhớ chia sẻ và trích xuất embedding
khái niệm tiềm ẩn cụ thể cho từng batch đầu vào, ii) mô-
đun Cross-Attention (CA) để phát sóng giữa embedding
đầu vào và embedding khái niệm được trích xuất từ mô-
đun CSA để nó tạo ra đầu ra phân loại cũng như các giải
thích dựa trên khái niệm trung thực và hợp lý và khuyến
khích các tương tác từng cặp giữa chúng, và iii) các mất
mát chuyên biệt, bao gồm Explanation Loss và Sparsity
Loss, đây là sơ đồ phát sóng thông tin của chúng tôi để
khuyến khích khả năng diễn giải. Kiến trúc CCT, được
tóm tắt trong Hình 2, được mô tả trong các phần sau. Các
chi tiết thêm, bao gồm cả hạn chế (Phụ lục D), được đưa
ra trong phụ lục.

4.1. Giữa Các Chuyên Gia và Không Gian Làm Việc Chia Sẻ
Theo cấu trúc chung của SGW, chúng tôi tận dụng các
chuyên gia, là các mô-đun tính toán của backbone của
chúng tôi, và một không gian làm việc chia sẻ bằng cách
sử dụng các phương pháp dựa trên slot trong mô-đun CSA
của chúng tôi, bao gồm SA [57], I-SA [10], và BO-QSA
[41]. Do tính mô-đun trong công thức của chúng tôi, ba
biến thể SA đó có thể hoán đổi cho nhau, và chúng tôi
chứng minh so sánh hiệu suất giữa chúng trong các thí
nghiệm của chúng tôi. Chúng tôi sử dụng cơ chế attention
key–query–value thông thường để triển khai sự cạnh tranh
giữa các chuyên gia để ghi vào không gian làm việc, tương
tự như SGW. Mô-đun CSA mã hóa một tập hợp L vector
đặc trưng đầu vào E thành các biểu diễn khái niệm Sconcept,
mà chúng tôi gọi là concept slots.

Ràng Buộc Khái Niệm Cụ Thể cho Từng Batch Đầu Vào.
Với số lượng khái niệm C, concept slots Sconcept∈RC×d
đầu tiên thực hiện competitive attention [57] trên các đặc
trưng đầu vào E∈RL×D. Để làm điều này, chúng tôi áp
dụng phép chiếu tuyến tính qCSA trên concept slots để có
được các query và các phép chiếu kCSA và vCSA trên đầu
vào để có được các key và value, tất cả đều có cùng kích
thước d2. Sau đó, chúng tôi thực hiện tích vô hướng giữa
các query và key để có được ma trận attention ACSA∈RC×L.
Trong ACSA, mỗi mục ACSA c,l là trọng số attention của
concept slot c để attend over vector đầu vào l. Chúng tôi
chuẩn hóa ACSA bằng cách áp dụng softmax qua concept
slots, tức là dọc theo trục C. Điều này triển khai một dạng
cạnh tranh giữa các slot để attend to mỗi đầu vào l.

Sau đó chúng tôi tìm cách nhóm và tổng hợp các đầu
vào được attend và thu được attention readout cho mỗi
concept slot. Một cách trực quan, điều này đại diện cho
mức độ các đầu vào được attend đóng góp vào việc biểu
diễn ngữ nghĩa mỗi khái niệm. Để làm điều này, chúng tôi
chuẩn hóa ma trận attention ACSA∈RC×L dọc theo trục L
và nhân nó với các giá trị đầu vào vCSA(E)∈RL×d. Điều
này tạo ra attention readout dưới dạng ma trận U∈RC×d
nơi mỗi hàng uc∈Rd là readout tương ứng với concept
slot c; ACSA= softmax C(qCSA(Sconcept)·kCSA(E)⊤/√d),
ACSA c,l= ACSA c,l/∑L l=1ACSA c,l, và U=ACSA·vCSA(E).
Chúng tôi sử dụng thông tin readout thu được từ ràng buộc
khái niệm và cập nhật mỗi concept slot. Các cập nhật tổng
hợp U cuối cùng được sử dụng để cập nhật concept slots
thông qua một hàm lặp đã học, mà chúng tôi sử dụng một
Gated Recurrent Unit (GRU) [14] với dhidden units để
Sconcept= GRU(Sconcept,U). Các quá trình trên tạo thành
một vòng lặp tinh chỉnh. Concept slots thu được từ vòng
lặp cuối

2Để đơn giản, kích thước embedding d được chia sẻ đều trong phương pháp của chúng tôi.

--- TRANG 4 ---
(a) Transformers Tập Trung Khái Niệm qua Không Gian Làm Việc Chia Sẻ
(b) Phát Sóng Có Thể Diễn Giải đến Các Chuyên Gia
Hình 2. Kiến trúc tổng thể của Concept-Centric Transformers (CCTs). (a) CCT (trong đường nét liền màu xám) là một thay thế drop-in
cho đầu phân loại của bất kỳ kiến trúc backbone nào, chẳng hạn như ViT hoặc CNN, và bao gồm hai mô-đun: (1) mô-đun Concept-Slot-
Attention và (2) mô-đun Cross-Attention. (b) Quá trình huấn luyện sử dụng các hàm mất mát để tạo ra một sơ đồ phát sóng có thể diễn giải.

iteration được coi là cuối cùng. Mô-đun tổng thể được mô
tả trong Thuật toán 1 ở dạng mã giả trong Phụ lục B.

4.2. Phát Sóng Bộ Nhớ Đã Cập Nhật đến Các Chuyên Gia
Tại giai đoạn này của SGW, mỗi chuyên gia phải cập
nhật trạng thái của mình bằng cách sử dụng thông tin phát
sóng từ không gian làm việc chia sẻ. Chúng tôi cũng tận
dụng cơ chế cross-attention (được gọi là mô-đun CA) để
làm cho các chuyên gia query (bước 3 trong Hình 1 và 2)
và thực hiện tích vô hướng giữa chúng và các giá trị từ
concept slots đã cập nhật để cập nhật trạng thái của mỗi
chuyên gia. Tuy nhiên, vì chúng tôi nhằm cấu hình một
mô hình có thể diễn giải và thực hiện các tác vụ phân loại,
chúng tôi sửa đổi quá trình lặp bằng cách kết hợp nó với
tác vụ phân loại downstream mong muốn của chúng tôi:
i) được hướng dẫn bởi kiến thức chuyên gia nếu các giải
thích khái niệm cơ sở thực tế có sẵn, hoặc ii) chỉ sử dụng
sparsity loss để thực thi việc tối thiểu hóa entropy của thông
tin phát sóng.

Một tập hợp L vector đặc trưng đầu vào E∈RL×D được
sử dụng lại với một phép chiếu tuyến tính qCA để đạt được
các query, và các phép chiếu kCA và vCA được áp dụng
cho concept slots đã trích xuất với position embedding Ŝ
từ mô-đun CSA. Các key và value kết quả được sử dụng
trong cơ chế cross-attention với các query, và cross-attention
sau đó xuất ra một trọng số attention ACA= softmax L
(qCA(E)·kCA(Ŝconcept)⊤/√d) ∈RL×C giữa mỗi cặp patch–
concept slot. Đầu ra cuối cùng của mô-đun CA là tích thu
được bằng cách nhân bản đồ attention ACA, các giá trị
vCA(Ŝconcept)∈RC×d, và một ma trận đầu ra O∈Rd×nc
chiếu lên nc logits (chưa chuẩn hóa) qua các lớp đầu ra và
sau đó lấy trung bình qua các đặc trưng đầu vào3; tức là,
với i= 1, . . . , nc,

logiti=1/L ∑L l=1ACA l·vCA(Ŝconcept)·O:,i (1)

Vậy, cho một đầu vào x vào mạng, xác suất có điều kiện
của lớp đầu ra i∈ {1, . . . , nc} là:

Pr(i|x) = softmax i(∑C c=1βcγc(x)) (2)

với các thành phần βc là βci:= (vCA(Ŝconcept)·O)c,i nơi
γc(x) là các điểm liên quan không âm phụ thuộc vào x
thông qua các trọng số attention được lấy trung bình; tức
là, γc(x) = (1/L)∑L l=1ACA l,c. Chúng tôi có thể diễn giải
các phương trình trên từ hai góc độ sau:

1) Giải Thích Dựa Trên Concept-slot Trung Thực Theo
Thiết Kế. Đầu ra mô-đun CA là một mô hình hồi quy logistic
đa thức trên các biến dương γc(x) đo lường đóng góp của
mỗi concept slot. Tính trung thực là mức độ mà giải thích
phản ánh quyết định và nhằm đảm bảo rằng các giải thích
thực sự đang giải thích hoạt động của mô hình [33,48].
Như được chỉ ra trong [65], kết quả của mô-đun CA tuân
theo mối quan hệ tuyến tính giữa các vector giá trị và các
logit phân loại và đến từ các lựa chọn thiết kế của việc
tính toán đầu ra từ ma trận giá trị vCA(Ŝconcept) thông qua
phép chiếu tuyến tính vCA(Ŝconcept)·O và tổng hợp các
đóng góp patch bằng cách lấy trung bình. Vậy, mô-đun CA
của chúng tôi cũng được đảm bảo trung thực theo thiết kế
bằng cách thỏa mãn Mệnh đề 1 trong [65] và các định
nghĩa kỹ thuật về tính trung thực từ [1].

3Để đơn giản, chúng tôi mô tả một mô hình attention đơn đầu ở đây; một phiên bản đa đầu [82] có sẵn và cũng được sử dụng trong các thí nghiệm của chúng tôi.

--- TRANG 5 ---
2) Cập Nhật Trạng Thái Động cho Các Chuyên Gia với
Phát Sóng Thông Tin. Trong định nghĩa gốc của SGW,
ACA·vCA(Ŝconcept) là phép tính chính thức của việc cập
nhật cho các chuyên gia (bước 3 từ [30, Mục 2.1]). Thay
vì áp dụng một quy trình lặp bổ sung để cập nhật các
chuyên gia, Phương trình 1 và 2 là để tạo ra đầu ra phân
loại sử dụng trọng số O. Vậy, mặt nạ attention ACA có thể
chứa không chỉ thông tin từ bộ nhớ được cập nhật mà còn
lỗi phân loại. Hơn nữa, bằng cách thao tác trực tiếp mặt
nạ ACA, chúng tôi cuối cùng định nghĩa explanation loss
và sparsity loss để tăng cường khả năng giải thích của mô
hình.

4.3. Mục Tiêu Huấn Luyện cho Khả Năng Diễn Giải
Tính Hợp Lý Theo Cấu Trúc với Explanation Loss. Tính
hợp lý đề cập đến mức độ thuyết phục của việc diễn giải
đối với con người [9,33]. Để cung cấp các giải thích có thể
hiểu được của con người một cách hợp lý, chúng tôi tận
dụng ý tưởng hướng dẫn rõ ràng các đầu attention để tập
trung vào các khái niệm trong đầu vào dựa trên chuyên
môn lĩnh vực quan trọng để phân loại đầu vào một cách
chính xác. Tương tự như [20], cho một phân phối mong
muốn của attention H được cung cấp bởi kiến thức lĩnh
vực, các trọng số attention từ mô-đun CA của CCT ACA
được sử dụng như một thuật ngữ chính quy hóa bằng cách
thêm một chi phí giải thích vào hàm mục tiêu tỷ lệ với
Lexpl=∥A−H∥2 F, nơi ∥·∥ là chuẩn Frobenius. Giải thích
cơ sở thực tế H có thể chỉ ra thông tin toàn cục (ví dụ: lớp
con hoặc thuộc tính chủ đạo của một con chim) hoặc không
gian/cấp độ mảnh hình ảnh (ví dụ: màu mắt của một con
chim). Dưới đây, chúng tôi chứng minh hiệu quả của hàm
mất mát này trong các thí nghiệm của CIFAR100 Super-
class và CUB-200-2011.

Sparsity Loss dựa trên Entropy. Lợi thế của các nhãn
phong phú, nhiều thông tin hơn có thể tăng khả năng diễn
giải, nhưng điều này đi kèm với chi phí nỗ lực chú thích
bổ sung. Một phương pháp có thể bỏ qua sự đánh đổi này
sẽ đặc biệt đáng giá. Chúng tôi có thể đạt được khả năng
này thông qua cấu hình của chúng tôi liên quan đến các
tương tác giữa các chuyên gia và một không gian làm việc
chia sẻ. Chúng tôi giới thiệu sparsity loss dựa trên việc
tối thiểu hóa entropy của mặt nạ attention ACA; Lsparse =
H(A) =H(a1, . . . , a|A|) = (1/|A|)∑i−ai·log(ai). Hàm mất
mát này có thể được sử dụng trong các thí nghiệm có/
không có các giải thích cơ sở thực tế.

Hàm Mất Mát Cuối Cùng. Do đó, hàm mất mát cuối
cùng để huấn luyện mô hình trở thành L=Lcls+λexplLexpl+
λsparseLsparse, nơi Lcls biểu thị hàm mất mát phân loại
thông thường. Lưu ý rằng hằng số λexpl≥0 kiểm soát đóng
góp tương đối của explanation loss vào tổng mất mát để
mô hình của chúng tôi có thể được áp dụng với các giải
thích cơ sở thực tế (λexpl>0) hoặc không có chúng (λexpl=
0). Cuối cùng, hằng số λsparse xử lý cường độ của sparsity
loss. Các thí nghiệm của chúng tôi chứng minh rằng mô
hình của chúng tôi có thể hoạt động tốt mà không sử dụng
các hàm mất mát phức tạp bổ sung, chẳng hạn như contrastive
hoặc reconstruct losses.

[Bảng 1 với kết quả thử nghiệm CIFAR100]

5. Thí Nghiệm
Chúng tôi đánh giá hiệu suất của CCT trên ba bộ dữ liệu
riêng biệt: CIFAR100 Super-class [23], CUB-200-2011
[83], và ImageNet [19]. Các mục tiêu cụ thể hướng dẫn
việc lựa chọn những bộ dữ liệu này. Thứ nhất, CIFAR100
Super-class là một nền tảng thử nghiệm để chứng minh
khả năng đặc biệt của mô hình chúng tôi trong điều kiện
có giám sát đầy đủ, nơi các giải thích khái niệm toàn cục
hoàn chỉnh có sẵn. Thứ hai, CUB-200-2011 hoạt động như
một bộ dữ liệu trung gian, cho phép chúng tôi thể hiện
sức mạnh của mô hình trong cả thiết lập giải thích có giám
sát và không giám sát. Cuối cùng, chúng tôi thử thách CCT
với bộ dữ liệu ImageNet, đại diện cho một kịch bản hoàn
toàn không giám sát do không có giải thích khái niệm. Bất
chấp trở ngại này, chúng tôi điều chỉnh mô hình để đạt
được hiệu suất đáng chú ý ngay cả khi không có bất kỳ
giải thích khái niệm nào. Thiết lập toàn diện này nhấn
mạnh khả năng thích ứng và tính linh hoạt của CCT qua
nhiều trường hợp sử dụng, backbone thị giác, và kịch bản
dữ liệu khác nhau. Kết quả thí nghiệm của chúng tôi được
xác nhận mạnh mẽ thông qua ba hạt giống ngẫu nhiên
khác nhau và khoảng tin cậy 95%, với các chi tiết bổ sung
được cung cấp trong Phụ lục C.

5.1. Đánh Giá trên CIFAR100 Super-class
Bộ dữ liệu CIFAR100 Super-class là một biến thể của
bộ dữ liệu hình ảnh CIFAR100 [47]. Nó bao gồm 100 lớp
hình ảnh chi tiết (F.C.) được nhóm thêm vào 20

--- TRANG 6 ---
super-classes (S.C.) riêng biệt. Ví dụ, năm lớp chi tiết baby,
boy, girl, man, và woman thuộc về super-class people.
Kể từ khi giới thiệu các mô hình deep-learning với ràng
buộc logic [23], bộ dữ liệu này đã được sử dụng như một
trong những bộ dữ liệu benchmark để đánh giá hiệu quả
của việc nhúng các ràng buộc vào mạng nơ-ron (các khảo
sát chuyên sâu có thể được tìm thấy trong [15, 28]). Nghiên
cứu của chúng tôi nổi bật rằng các khái niệm trong CCT
của chúng tôi (và CT liên quan chặt chẽ [65]) có thể được
hiểu như các ràng buộc, với các khác biệt được nêu trong
Phụ lục C.4. Chúng tôi tận dụng các lớp hình ảnh chi tiết
riêng lẻ như các giải thích khái niệm toàn cục cho một tác
vụ dự đoán đa lớp với 20 super-classes, sử dụng Vision
Transformer-Tiny (ViT-T)4 như backbone5.

Theo [34], hiệu suất của CCT được so sánh với ba nhóm
baseline. Nhóm đầu tiên bao gồm các mô hình backbone
gốc như Wide ResNet 28-10 [93] và ViT-T. Nhóm thứ hai
liên quan đến các mô hình mạng nơ-ron với ràng buộc
logic, bao gồm Hierarchical Model [34], DL2 [23], và
MultiplexNet [34]. Nhóm thứ ba bao gồm các mô hình có
thể giải thích dựa trên khái niệm; ngoại trừ CT, những mô
hình này không thể xử lý cả tác vụ chi tiết và super-class
đồng thời. Đối với mô-đun CSA của CCT, chúng tôi cấu
hình ba biến thể SA—SA, I-SA, và BO-QSA—với backbone
và đánh giá độ chính xác lớp chi tiết bằng cách so sánh
các giải thích khái niệm cơ sở thực tế với các khái niệm
được dự đoán top-1 dựa trên điểm attention kết quả.

Bảng 1 trình bày kết quả thí nghiệm, cho thấy CCT của
chúng tôi vượt trội đáng kể so với tất cả các baseline. Nó
tăng cường đáng kể hiệu suất backbone ViT-T và đạt được
sự gia tăng đáng chú ý trong độ chính xác lớp chi tiết so
với CT. Đáng chú ý, CT hoạt động kém hiệu quả hơn so
với các phương pháp dựa trên ràng buộc logic, nổi bật vai
trò ưu việt của mô-đun chúng tôi trong việc hình thành
các khái niệm toàn cục.

Hình 3 cho thấy hai ví dụ chứng minh nơi học khái niệm
của CCT xuất sắc cả về mặt định lượng và định tính so với
CT. Độ chính xác lớp chi tiết kém của CT có thể là kết quả
của việc tạo ra ảo giác để đạt được độ chính xác super-
class một cách tham lam mà không hình thành các khái
niệm tiềm ẩn vững chắc. Mặc dù cả hai mô hình đều phân
loại super-classes một cách chính xác trên các ví dụ được
hiển thị, các quá trình ra quyết định giải thích hoạt động
hoàn toàn khác nhau trong hai mô hình. Mặc dù lớp cơ sở
thực tế của hình ảnh bên trái trong Hình 3 là boy, lớp khái
niệm khớp tốt nhất với điểm attention cao nhất bởi CT là
baby, đây là lớp chi tiết không chính xác nhưng cũng thuộc
về super-class people chính xác. Trong ví dụ hình ảnh bên
phải, chúng tôi quan sát một hành vi tương tự của CT. Ngược
lại, các khái niệm được dự đoán của CCT cho cả hai ví dụ
đều khớp chính xác với các lớp chi tiết cơ sở thực tế của
chúng với điểm attention khái niệm thưa thớt hơn

4Đối với thí nghiệm này, Swin Transformer và ConvNeXt không được sử dụng làm backbone cho mô hình của chúng tôi vì số lượng tham số của hai mô hình (cả SwinT-T và ConvNeXt-T đều là 28M) lớn hơn một trong ViT-T.
5Đối với các khái niệm toàn cục, chúng tôi sử dụng embedding của token CLS làm đầu vào.

Hình 3. So sánh dự đoán lớp cho CT [65] và CCT ("Ours") trong
các ví dụ nơi cả hai đều đưa ra dự đoán super-class CIFAR100
chính xác. 100 lớp được đánh chỉ số từ 1 (trên cùng bên trái trong
lưới 10x10) đến 100 (dưới cùng bên phải trong lưới 10x10). (Trái)
Nhãn lớp cơ sở thực tế là boy (12), nhưng CT dự đoán sai thành
baby (3), trong khi dự đoán của CCT là chính xác. (Phải) Nhãn
cơ sở thực tế là shark (84), nhưng CT chọn sai ray (78) trong khi
CCT lại đưa ra dự đoán lớp chính xác.

so với CT. Các chi tiết thêm và kết quả được mô tả trong
Phụ lục C.4.

5.2. Đánh Giá trên CUB-200-2011
Bộ dữ liệu CUB-200-2011 [83] bao gồm 11,788 hình
ảnh chim được phân loại thành 200 loài. Mỗi hình ảnh
được chú thích với nhiều khái niệm rời rạc khác nhau, ví
dụ: hình dạng của mỏ, hoặc màu sắc của cơ thể, hỗ trợ
nhận dạng loài. Bộ dữ liệu bao gồm 312 khái niệm được
phân phối không đều qua các hình ảnh, vì vậy chúng tôi
sử dụng phương pháp tiền xử lý từ [65] để giải quyết điều
này. Kết quả bổ sung và chi tiết được mô tả trong Phụ lục
C.5.

Với Giải Thích Khái Niệm. Chúng tôi xem xét một kịch
bản thực tế nơi tồn tại mối quan hệ nhiều-nhiều và không
xác định giữa các khái niệm và đầu ra, cùng với sự kết
hợp của các khái niệm toàn cục và được bản địa hóa không
gian. Chúng tôi sử dụng các mô-đun CSA và CA trong
CCT để xử lý cả khái niệm toàn cục và không gian, lấy
trung bình logits của chúng cho khả năng diễn giải. Chúng
tôi sử dụng nhiều backbone khác nhau, bao gồm ViT [22],
Swin Transformer (SwinT) [55], và ConvNeXt [56] cho
bộ dữ liệu này. Chúng tôi sử dụng embeddings của các
patch hình ảnh được token hóa, trong khi làm đầu vào cho
CCT phụ trách các khái niệm, chúng tôi sử dụng embedding
của token CLS.

Trong Bảng 2, chúng tôi so sánh CCT với các phương
pháp khác dựa trên huấn luyện Multi-stage (tức là, huấn
luyện phức tạp) và End-to-end (tức là, huấn luyện với
backpropagation). Tất cả các cấu hình của CCT đạt được
hơn 87% độ chính xác phân loại, vượt trội rõ ràng so với
các phương pháp khác. Điều này xác nhận rằng cấu hình
tổng thể của CCT tăng cường hiệu suất phân loại. Đáng
chú ý, mô hình của chúng tôi vượt qua các baseline không
thể diễn giải (B-CNN) và các phương pháp yêu cầu huấn
luyện phức tạp.

Hình 4 nổi bật sự khác biệt giữa CT và CCT của chúng

--- TRANG 7 ---
[Bảng 2 - So sánh hiệu suất trên CUB-200-2011]

tôi, minh họa rằng CT thiếu học các khái niệm toàn cục.
Chúng tôi nhấn mạnh các khái niệm với điểm attention cao
nhất trong tất cả hình ảnh. Hình cho thấy một trường hợp
nơi dự đoán của CT hoàn toàn không chính xác bằng cách
hội tụ về nhãn sai duy nhất. Mặc dù lớp cơ sở thực tế của
các hình ảnh là Brandt Cormorant, tất cả dự đoán của CT
đều là Pelagic Cormorant. Hơn nữa, các thuộc tính mà CT
sử dụng để đưa ra phân loại không chính xác bao gồm một
tập con các thuộc tính (các thuộc tính màu tím trong Hình
4) thường liên kết với dự đoán chính xác của nhãn không
chính xác Pelagic Cormorant. Do đó, CT tập trung vào
patch hình ảnh tạo ảo giác các khía cạnh chính liên kết
với các nhãn không chính xác, trong khi CCT của chúng
tôi cho thấy các giải thích khái niệm mạnh mẽ hơn về phân
loại chính xác.

Hình 4. So sánh dự đoán giữa CT và CCT của chúng tôi với giải
thích cơ sở thực tế. (Trái) Tất cả dự đoán của CT đều không chính
xác. Các giải thích được làm nổi bật màu tím là các thuộc tính
chính thường có mặt khi phân loại chính xác Pelagic Cormorant,
nhãn không chính xác. (Phải) Dự đoán của CCT là chính xác.

Không Có Giải Thích Khái Niệm. Mặc dù kiến thức của
chuyên gia lĩnh vực là công cụ hiệu quả nhất để hướng
dẫn khả năng giải thích của mô hình, bước tiền xử lý để
định nghĩa các khái niệm thị giác cho các tác vụ có thể
yêu cầu ghi nhãn tốn thời gian và dựa vào phán đoán của
con người. Quan trọng, chúng tôi chứng minh rằng CCT
cũng hoạt động hiệu quả mà không có giải thích khái niệm,
một khả năng không được chia sẻ bởi các mô hình khác.
Chúng tôi có thể dễ dàng thiết lập hàm mất mát với λexpl=
0 (Final Loss được định nghĩa trong mục 4.3) và đánh giá
mô hình với cùng thiết lập siêu tham số của thí nghiệm
với giải thích chỉ sử dụng classification loss và sparsity
loss.

Trong Bảng 2, cấu hình tốt nhất của CCT (Swin-L+BO-
QSA) không có giải thích đạt được 90% độ chính xác kiểm
tra, rất sát so với cái có giải thích. Ngược lại, CT cũng có
thể được huấn luyện không có giải thích, nhưng hiệu suất
của nó bị suy giảm mạnh.

Hình 5 trực quan hóa việc kích hoạt của các khái niệm
tiềm ẩn đã học trong mô hình của chúng tôi từ bộ dữ liệu.
Theo [84], chúng tôi đào tạo thêm CCT bằng cách đặt số
lượng nhãn lớp (n) thành 50 và số lượng khái niệm tiềm
ẩn (k) thành 20. Hình thể hiện năm khái niệm tiềm ẩn—7,
3, 19, 5, và 4. Trong kết quả thí nghiệm của chúng tôi,
chúng tôi xác định các khái niệm chính mà CCT tập trung
vào khi phân loại hình ảnh chim. Khái niệm 7 tập trung
vào mỏ và phần thân trên, trong khi Khái niệm 3 xem xét
nhiều đặc trưng như mỏ, mắt, và đuôi. Khái niệm 19 nắm
bắt cấu trúc đầu và lông độc đáo, Khái niệm 5 phác thảo
toàn bộ cơ thể chim, và Khái niệm 4 tách chim khỏi nền
phức tạp. Những điều này chứng minh sự tập trung tinh
tế và kỹ năng phân loại của mô hình, ngay cả trong môi
trường không giám sát nơi chúng tôi không có quyền truy
cập vào giải thích khái niệm của nó.

--- TRANG 8 ---
Hình 5. Việc kích hoạt của các khái niệm tiềm ẩn được học từ
CUB-200-2011 và các ví dụ cho thấy mỗi khái niệm tiềm ẩn. Kết
quả thêm có thể được tìm thấy trong Phụ lục C.5.

[Bảng 3 - Độ chính xác kiểm tra trên ImageNet]

5.3. Đánh Giá trên ImageNet
Cuối cùng, để xác thực rằng mô hình của chúng tôi có
thể học các khái niệm tiềm ẩn mà không có giải thích,
chúng tôi kiểm tra CCT trên ImageNet theo phương pháp
trong [84]. Chúng tôi sử dụng ViT tương đối nhỏ (ViT-S)
làm backbone6. Kết quả bổ sung và chi tiết trong Phụ lục
C.6.

Bảng 3 cho thấy hiệu suất phân loại ImageNet. Sự kết
hợp của ViT-S với BO-QSA trong thí nghiệm có hiệu suất
tốt nhất, đạt được 83.7% độ chính xác kiểm tra bất chấp
sử dụng backbone kích thước nhỏ, đáp ứng hoặc vượt qua
hiệu suất của các phương pháp SOTA khác—bao gồm một
số phương pháp dựa trên khái niệm không thể áp dụng
hoặc đạt hiệu suất rất kém.

Ngoài ra, chúng tôi huấn luyện một mô hình CCT đơn
giản bằng cách đặt số lượng lớp (n) thành 20 và số lượng
khái niệm tiềm ẩn (k) thành 10 như trong [84] để trực quan
hóa tính nhất quán của các khái niệm đã học. Hình 6 mô
tả năm khái niệm chúng tôi

6Chúng tôi tránh sử dụng backbone lớn hơn ResNet-101 (45M), điều này loại trừ SwinT-S (50M) và ConvNeXt-S (50M).

Hình 6. Việc kích hoạt của các khái niệm tiềm ẩn được học từ
ImageNet và các ví dụ cho thấy mỗi khái niệm tiềm ẩn. Kết quả
thêm có thể được tìm thấy trong Phụ lục C.6.

lựa chọn và năm cặp hình ảnh đại diện cho mỗi khái niệm
tiềm ẩn được học từ ImageNet. CCT của chúng tôi cho
thấy rằng nó xuất sắc trong việc tách biệt và nhấn mạnh
các đặc trưng đối tượng cụ thể qua nhiều khái niệm khác
nhau. Khái niệm 10 tách biệt đường viền gà mái bằng cách
làm nổi bật nền, trong khi Khái niệm 8 tập trung vào vùng
bụng của chim. Khái niệm 3 khéo léo tách các thành phần
cá, Khái niệm 5 nắm bắt các vùng miệng cá mập, và Khái
niệm 6 phác thảo hình dạng cá vàng. Khả năng của CCT
trong việc vẽ đường viền và làm nổi bật các khu vực có ý
nghĩa ngữ nghĩa vượt qua những mô hình hiện có. Nó cũng
xuất sắc trong truy xuất hình ảnh không giám sát, nhóm
các hình ảnh tương tự ngữ nghĩa lại với nhau như được
thể hiện trong Hình 6. Điều này thể hiện sức mạnh của
phương pháp tập trung khái niệm của chúng tôi trong việc
tạo ra kết quả nhất quán ngữ nghĩa.

6. Kết Luận
Chúng tôi đề xuất Concept-Centric Transformers, một
mô hình có thể diễn giải nội tại thông qua Shared Global
Workspace, cho phép đạt được khả năng diễn giải, hiệu
suất, và tính linh hoạt tốt hơn cho khi kiến thức chuyên
gia có sẵn hoặc không. Một hướng nghiên cứu tương lai
tự nhiên là mở rộng mô-đun trích xuất khái niệm để thu
thập các "mảnh" kiến thức có thể tổng hợp [77] và sau đó
học các quy tắc tổng hợp cơ bản hoặc cơ chế liên quan
các mảnh đã thu thập với nhau. Những quy tắc đó có thể
được nắm bắt một cách chính thức, như với logic bậc nhất
[6]. Cũng hứa hẹn khi điều tra các ứng dụng khác, chẳng
hạn như gỡ lỗi mô hình và chẩn đoán hình ảnh y tế.

Lời Cảm Ơn: Được hỗ trợ một phần bởi giải thưởng NSF
2223839 và giải thưởng USACE ERDC W912HZ-21-2-
0040.

Tài Liệu Tham Khảo
[1] David Alvarez Melis and Tommi Jaakkola. Towards robust
interpretability with self-explaining neural networks. Ad-
vances in Neural Information Processing Systems , 31, 2018.
1, 2, 4

--- TRANG 9 ---
[2] Bernard J Baars. A cognitive theory of consciousness . Cam-
bridge University Press, 1993. 2
[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
Neural machine translation by jointly learning to align and
translate. arXiv preprint arXiv:1409.0473 , 2014. 1
[4] Carliss Young Baldwin and Kim B Clark. Design rules: The
power of modularity , volume 1. MIT press, 2000. 1
[5] Dana H Ballard. Cortical connections and parallel process-
ing: Structure and function. Behavioral and brain sciences ,
9(1):67–90, 1986. 1
[6] Pietro Barbiero, Gabriele Ciravegna, Francesco Giannini,
Pietro Li ´o, Marco Gori, and Stefano Melacci. Entropy-based
logic explanations of neural networks. In Proceedings of the
AAAI Conference on Artificial Intelligence , volume 36, pages
6046–6054, 2022. 1, 2, 8
[7] Steve Branson, Grant Van Horn, Serge Belongie, and Pietro
Perona. Bird species categorization using pose normalized
deep convolutional nets. arXiv preprint arXiv:1406.2952 ,
2014. 7
[8] Rodney A Brooks. Intelligence without representation. Arti-
ficial intelligence , 47(1-3):139–159, 1991. 1
[9] Diogo V Carvalho, Eduardo M Pereira, and Jaime S Cardoso.
Machine learning interpretability: A survey on methods and
metrics. Electronics , 8(8):832, 2019. 1, 5
[10] Michael Chang, Tom Griffiths, and Sergey Levine. Object
representations as fixed points: Training iterative refinement
algorithms with implicit differentiation. Advances in Neural
Information Processing Systems , 35:32694–32708, 2022. 3,
A
[11] Chaofan Chen, Oscar Li, Alina Barnett, Jonathan Su, and
Cynthia Rudin. This looks like that: deep learning for inter-
pretable image recognition. CoRR , abs/1806.10574, 2018. 5,
8
[12] Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia
Rudin, and Jonathan K Su. This looks like that: deep learn-
ing for interpretable image recognition. Advances in Neural
Information Processing Systems , 32, 2019. 1, 2, 7
[13] Zhi Chen, Yijie Bei, and Cynthia Rudin. Concept whitening
for interpretable image recognition. Nature Machine Intelli-
gence , 2(12):772–782, 2020. 1, 2
[14] Kyunghyun Cho, Bart van Merri ¨enboer, Caglar Gulcehre,
Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. Learning phrase representations using RNN
encoder–decoder for statistical machine translation. In Pro-
ceedings of the 2014 Conference on Empirical Methods in
Natural Language Processing (EMNLP) , pages 1724–1734,
2014. 3
[15] Tirtharaj Dash, Sharad Chitlangia, Aditya Ahuja, and Ash-
win Srinivasan. A review of some techniques for inclusion
of domain-knowledge into deep neural networks. Scientific
Reports , 12(1):1040, 2022. 6
[16] Stanislas Dehaene and Jean-Pierre Changeux. Experimental
and theoretical approaches to conscious processing. Neuron ,
70(2):200–227, 2011. 2
[17] Stanislas Dehaene, Michel Kerszberg, and Jean-Pierre
Changeux. A neuronal model of a global workspace in ef-
fortful cognitive tasks. Proceedings of the national Academy
of Sciences , 95(24):14529–14534, 1998. 2

[18] Stanislas Dehaene, Hakwan Lau, and Sid Kouider. What is
consciousness, and could machines have it? Robotics, AI,
and Humanity: Science, Ethics, and Policy , pages 43–56,
2021. 2
[19] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248–255. Ieee, 2009. 2, 5
[20] Ameet Deshpande and Karthik Narasimhan. Guiding atten-
tion for self-supervised learning with transformers. arXiv
preprint arXiv:2010.02399 , 2020. 5
[21] Jon Donnelly, Alina Jade Barnett, and Chaofan Chen. De-
formable protopnet: An interpretable image classifier using
deformable prototypes. CoRR , abs/2111.15000, 2021. 5, 8
[22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 2, 6, B, C
[23] Marc Fischer, Mislav Balunovic, Dana Drachsler-Cohen, Ti-
mon Gehr, Ce Zhang, and Martin Vechev. DL2: training
and querying neural networks with logic. In International
Conference on Machine Learning , pages 1931–1941. PMLR,
2019. 2, 5, 6, B, C
[24] Jianlong Fu, Heliang Zheng, and Tao Mei. Look closer to
see better: Recurrent attention convolutional neural network
for fine-grained image recognition. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 4438–4446, 2017. 2, 7
[25] Amirata Ghorbani, James Wexler, James Y Zou, and Been
Kim. Towards automatic concept-based explanations. Ad-
vances in Neural Information Processing Systems , 32, 2019.
1, 2
[26] Ross Girshick. Fast R-CNN. In Proceedings of the IEEE
International Conference on Computer Vision , pages 1440–
1448, 2015. 2
[27] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition , pages
580–587, 2014. 2
[28] Eleonora Giunchiglia, Mihaela Catalina Stoian, and Thomas
Lukasiewicz. Deep learning with logical constraints. arXiv
preprint arXiv:2205.00523 , 2022. 6
[29] Anirudh Goyal and Yoshua Bengio. Inductive biases for deep
learning of higher-level cognition. Proceedings of the Royal
Society A , 478(2266):20210068, 2022. 2
[30] Anirudh Goyal, Aniket Rajiv Didolkar, Alex Lamb, Kar-
tikeya Badola, Nan Rosemary Ke, Nasim Rahaman,
Jonathan Binas, Charles Blundell, Michael Curtis Mozer,
and Yoshua Bengio. Coordination among neural modules
through a shared global workspace. In International Confer-
ence on Learning Representations , 2021. 1, 2, 3, 5
[31] Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun
Sodhani, Sergey Levine, Yoshua Bengio, and Bernhard
Sch¨olkopf. Recurrent independent mechanisms. arXiv
preprint arXiv:1909.10893 , 2019. 2

--- TRANG 10 ---
[32] Yash Goyal, Amir Feder, Uri Shalit, and Been Kim. Ex-
plaining classifiers with causal concept effect (CaCE). arXiv
preprint arXiv:1907.07165 , 2019. 1, 2
[33] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri,
Franco Turini, Fosca Giannotti, and Dino Pedreschi. A sur-
vey of methods for explaining black box models. ACM Com-
puting Surveys (CSUR) , 51(5):1–42, 2018. 4, 5
[34] Nick Hoernle, Rafael Michael Karampatsis, Vaishak Belle,
and Kobi Gal. MultiplexNet: Towards fully satisfied log-
ical constraints in neural networks. In Proceedings of the
AAAI Conference on Artificial Intelligence , volume 36, pages
5700–5709, 2022. 5, 6, B
[35] Jinyung Hong and Ted Pavlic. Representing prior knowledge
using randomly, weighted feature networks for visual rela-
tionship detection. In Combining Learning and Reasoning:
Programming Languages, Formalisms, and Representations ,
2022. 2
[36] Jinyung Hong and Theodore P. Pavlic. An insect-inspired
randomly, weighted neural network with random fourier fea-
tures for neuro-symbolic relational learning. In Proceed-
ings of the 15th International Workshop on Neural-Symbolic
Learning and Reasoning (Ne/Sy 2022) , October 25–27 2021.
2
[37] Shaoli Huang, Zhe Xu, Dacheng Tao, and Ya Zhang. Part-
stacked CNN for fine-grained visual categorization. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pages 1173–1182, 2016. 2, 7
[38] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.
Spatial transformer networks. Advances in Neural Informa-
tion Processing Systems , 28, 2015. 7
[39] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,
Andrew Zisserman, and Joao Carreira. Perceiver: General
perception with iterative attention. In International confer-
ence on machine learning , pages 4651–4664. PMLR, 2021.
2
[40] Sarthak Jain and Byron C Wallace. Attention is not explana-
tion. arXiv preprint arXiv:1902.10186 , 2019. 1
[41] Baoxiong Jia, Yu Liu, and Siyuan Huang. Improving object-
centric learning with query optimization. In The Eleventh In-
ternational Conference on Learning Representations , 2022.
3, A
[42] Dmitry Kazhdan, Botty Dimanov, Mateja Jamnik, Pietro Li `o,
and Adrian Weller. Now you see me (CME): concept-based
model extraction. arXiv preprint arXiv:2010.13233 , 2020.
1, 2
[43] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai,
James Wexler, Fernanda Viegas, et al. Interpretability be-
yond feature attribution: Quantitative testing with concept
activation vectors (TCA V). In International Conference on
Machine Learning , pages 2668–2677. PMLR, 2018. 1, 2
[44] Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maxi-
milian Alber, Kristof T Sch ¨utt, Sven D ¨ahne, Dumitru Erhan,
and Been Kim. The (un) reliability of saliency methods. Ex-
plainable AI: Interpreting, explaining and visualizing deep
learning , pages 267–280, 2019. 1
[45] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen
Mussmann, Emma Pierson, Been Kim, and Percy Liang.

Concept bottleneck models. In International Conference on
Machine Learning , pages 5338–5348. PMLR, 2020. 1, 2
[46] Jonathan Krause, Hailin Jin, Jianchao Yang, and Li Fei-Fei.
Fine-grained recognition without part annotations. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pages 5546–5555, 2015. 7
[47] Alex Krizhevsky. Learning multiple layers of features from
tiny images. Master's thesis, University of Toronto, Toronto,
Canada, 2009. 5
[48] Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure
Leskovec. Faithful and customizable explanations of black
box models. In Proceedings of the 2019 AAAI/ACM Confer-
ence on AI, Ethics, and Society , pages 131–138, 2019. 4
[49] Liangzhi Li, Bowen Wang, Manisha Verma, Yuta
Nakashima, Ryo Kawasaki, and Hajime Nagahara. Scouter:
Slot attention-based classifier for explainable image recog-
nition. In Proceedings of the IEEE/CVF international
conference on computer vision , pages 1046–1055, 2021. 3
[50] Oscar Li, Hao Liu, Chaofan Chen, and Cynthia Rudin. Deep
learning for case-based reasoning through prototypes: A
neural network that explains its predictions. In Proceedings
of the AAAI Conference on Artificial Intelligence , volume 32,
2018. 1, 2
[51] Zenan Li, Zehua Liu, Yuan Yao, Jingwei Xu, Taolue Chen,
Xiaoxing Ma, L Jian, et al. Learning with logical constraints
but without shortcut satisfaction. In The Eleventh Interna-
tional Conference on Learning Representations , 2023. B
[52] Di Lin, Xiaoyong Shen, Cewu Lu, and Jiaya Jia. Deep
lac: Deep localization, alignment and classification for fine-
grained recognition. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages 1666–
1674, 2015. 7
[53] Tsung-Yu Lin, Aruni RoyChowdhury, and Subhransu Maji.
Bilinear CNN models for fine-grained visual recognition. In
Proceedings of the IEEE International Conference on Com-
puter Vision , pages 1449–1457, 2015. 7
[54] Xiao Liu, Tian Xia, Jiang Wang, Yi Yang, Feng Zhou, and
Yuanqing Lin. Fully convolutional attention networks for
fine-grained recognition. arXiv preprint arXiv:1603.06765 ,
2016. 7
[55] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 10012–10022, 2021. 2, 6, B, C
[56] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-
enhofer, Trevor Darrell, and Saining Xie. A convnet for the
2020s. In Proceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition , pages 11976–11986,
2022. 2, 6, B, C
[57] Francesco Locatello, Dirk Weissenborn, Thomas Un-
terthiner, Aravindh Mahendran, Georg Heigold, Jakob
Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-
centric learning with slot attention. Advances in Neural In-
formation Processing Systems , 33:11525–11538, 2020. 2, 3,
A

--- TRANG 11 ---
[58] Scott M Lundberg and Su-In Lee. A unified approach to
interpreting model predictions. Advances in Neural Infor-
mation Processing Systems , 30, 2017. 1, 2
[59] Marvin Minsky. Society of mind . Simon and Schuster, 1988.
2
[60] Tsendsuren Munkhdalai, Alessandro Sordoni, Tong Wang,
and Adam Trischler. Metalearned neural memory. Advances
in Neural Information Processing Systems , 32, 2019. 2
[61] Meike Nauta, J ¨org Schl ¨otterer, Maurice van Keulen, and
Christin Seifert. Pip-net: Patch-based intuitive prototypes
for interpretable image classification. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 2744–2753, June 2023. 5
[62] Anh Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas
Brox, and Jeff Clune. Synthesizing the preferred inputs for
neurons in neural networks via deep generator networks. Ad-
vances in Neural Information Processing Systems , 29, 2016.
2
[63] Hubert Ramsauer, Bernhard Sch ¨afl, Johannes Lehner,
Philipp Seidl, Michael Widrich, Thomas Adler, Lukas
Gruber, Markus Holzleitner, Milena Pavlovi ´c, Geir Kjetil
Sandve, et al. Hopfield networks is all you need. arXiv
preprint arXiv:2008.02217 , 2020. 2
[64] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
" why should i trust you?" explaining the predictions of any
classifier. In Proceedings of the 22nd ACM SIGKDD interna-
tional conference on knowledge discovery and data mining ,
pages 1135–1144, 2016. 1, 2
[65] Mattia Rigotti, Christoph Miksovic, Ioana Giurgiu, Thomas
Gschwind, and Paolo Scotton. Attention-based interpretabil-
ity with concept transformers. In International Conference
on Learning Representations , 2021. 1, 2, 4, 5, 6, 7, 8, A, B
[66] Philip Robbins. Modularity of mind. In Edward N. Zalta, ed-
itor,The Stanford Encyclopedia of Philosophy . Metaphysics
Research Lab, Stanford University, Winter 2017 edition,
2017. 1, 2
[67] Cynthia Rudin. Stop explaining black box machine learn-
ing models for high stakes decisions and use interpretable
models instead. Nature Machine Intelligence , 1(5):206–215,
2019. 1
[68] Dawid Rymarczyk, Lukasz Struski, Michal G ´orszczak, Ko-
ryna Lewandowska, Jacek Tabor, and Bartosz Zielinski. In-
terpretable image classification with differentiable proto-
types assignment. CoRR , abs/2112.02902, 2021. 5, 8
[69] Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae,
Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol
Vinyals, Razvan Pascanu, and Timothy Lillicrap. Relational
recurrent neural networks. Advances in neural information
processing systems , 31, 2018. 2
[70] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,
Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.
Grad-cam: Visual explanations from deep networks via
gradient-based localization. In Proceedings of the IEEE In-
ternational Conference on Computer Vision , pages 618–626,
2017. 1, 2
[71] Sofia Serrano and Noah A Smith. Is attention interpretable?
arXiv preprint arXiv:1906.03731 , 2019. 1

[72] Murray Shanahan. A cognitive architecture that combines
internal simulation with a global workspace. Consciousness
and cognition , 15(2):433–449, 2006. 2
[73] Murray Shanahan. Embodiment and the inner life: Cognition
and Consciousness in the Space of Possible Minds . Oxford
University Press, 2010. 2
[74] Murray Shanahan. The brain's connective core and its role
in animal cognition. Philosophical Transactions of the Royal
Society B: Biological Sciences , 367(1603):2704–2714, 2012.
2
[75] Murray Shanahan and Bernard Baars. Applying global
workspace theory to the frame problem. Cognition ,
98(2):157–176, 2005. 2
[76] Marcel Simon and Erik Rodner. Neural activation constella-
tions: Unsupervised part model discovery with convolutional
networks. In Proceedings of the IEEE International Confer-
ence on Computer Vision , pages 1143–1151, 2015. 7
[77] Gautam Singh, Yeongbin Kim, and Sungjin Ahn. Neural
systematic binder. In The Eleventh International Conference
on Learning Representations , 2023. 8
[78] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi ´egas,
and Martin Wattenberg. Smoothgrad: removing noise by
adding noise. arXiv preprint arXiv:1706.03825 , 2017. 1, 2
[79] Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai.
One pixel attack for fooling deep neural networks. IEEE
Transactions on Evolutionary Computation , 23(5):828–841,
2019. 1
[80] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic
attribution for deep networks. In International Conference
on Machine Learning , pages 3319–3328. PMLR, 2017. 2
[81] A ¨aron Van Den Oord, Nal Kalchbrenner, and Koray
Kavukcuoglu. Pixel recurrent neural networks. In Interna-
tional Conference on Machine Learning , pages 1747–1756.
PMLR, 2016. 2
[82] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in Neural
Information Processing Systems , 30, 2017. 4
[83] Catherine Wah, Steve Branson, Peter Welinder, Pietro
Perona, and Serge Belongie. Caltech–UCSD Birds-200-
2011 (CUB-200-2011) dataset. Technical Report CNS-TR-
2011-001, California Institute of Technology, 2011. 2, 5, 6
[84] Bowen Wang, Liangzhi Li, Yuta Nakashima, and Hajime Na-
gahara. Learning bottleneck concepts in image classification.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 10962–10971, 2023.
3, 5, 7, 8, B, C, D, F, G, H, I
[85] Dequan Wang, Zhiqiang Shen, Jie Shao, Wei Zhang, Xi-
angyang Xue, and Zheng Zhang. Multiple granularity de-
scriptors for fine-grained categorization. In Proceedings
of the IEEE International Conference on Computer Vision ,
pages 2399–2406, 2015. 7
[86] Sarah Wiegreffe and Yuval Pinter. Attention is not not expla-
nation. arXiv preprint arXiv:1908.04626 , 2019. 1
[87] Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi
Fan, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer.
Memvit: Memory-augmented multiscale vision transformer

--- TRANG 12 ---
for efficient long-term video recognition. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 13587–13597, 2022. 2
[88] Tianjun Xiao, Yichong Xu, Kuiyuan Yang, Jiaxing Zhang,
Yuxin Peng, and Zheng Zhang. The application of two-
level attention models in deep convolutional neural network
for fine-grained image classification. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 842–850, 2015. 7
[89] Mengqi Xue, Qihan Huang, Haofei Zhang, Lechao Cheng,
Jie Song, Minghui Wu, and Mingli Song. ProtoPFormer:
Concentrating on prototypical parts in vision transform-
ers for interpretable image recognition. arXiv preprint
arXiv:2208.10431 , 2022. 2, 5, 7, 8
[90] Zhun Yang, Adam Ishay, and Joohyung Lee. Learning to
solve constraint satisfaction problems with recurrent trans-
former. In Proceedings of the Eleventh International Con-
ference on Learning Representations , 2023. K
[91] Chih-Kuan Yeh, Been Kim, Sercan Arik, Chun-Liang Li,
Tomas Pfister, and Pradeep Ravikumar. On completeness-
aware concept-based explanations in deep neural net-
works. Advances in Neural Information Processing Systems ,
33:20554–20565, 2020. 1, 2
[92] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and
Hod Lipson. Understanding neural networks through deep
visualization. arXiv preprint arXiv:1506.06579 , 2015. 2
[93] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-
works. arXiv preprint arXiv:1605.07146 , 2016. 6
[94] Mateo Espinosa Zarlenga, Pietro Barbiero, Gabriele
Ciravegna, Giuseppe Marra, Francesco Giannini, Michelan-
gelo Diligenti, Frederic Precioso, Stefano Melacci, Adrian
Weller, Pietro Lio, et al. Concept embedding models. In
NeurIPS 2022-36th Conference on Neural Information Pro-
cessing Systems , 2022. 1, 2, 7
[95] Han Zhang, Tao Xu, Mohamed Elhoseiny, Xiaolei Huang,
Shaoting Zhang, Ahmed Elgammal, and Dimitris Metaxas.
Spda-cnn: Unifying semantic part detection and abstraction
for fine-grained recognition. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition ,
pages 1143–1152, 2016. 7
[96] Ning Zhang, Jeff Donahue, Ross Girshick, and Trevor Dar-
rell. Part-based R-CNNs for fine-grained category detection.
InComputer Vision–ECCV 2014: 13th European Confer-
ence, Zurich, Switzerland, September 6-12, 2014, Proceed-
ings, Part I 13 , pages 834–849. Springer, 2014. 2, 7
[97] Heliang Zheng, Jianlong Fu, Tao Mei, and Jiebo Luo. Learn-
ing multi-attention convolutional neural network for fine-
grained image recognition. In Proceedings of the IEEE Inter-
national Conference on Computer Vision , pages 5209–5217,
2017. 2, 7
[98] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,
and Antonio Torralba. Learning deep features for discrimi-
native localization. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages 2921–
2929, 2016. 2, 7
[99] Bolei Zhou, Yiyou Sun, David Bau, and Antonio Torralba.
Interpretable basis decomposition for visual explanation. In

Proceedings of the European Conference on Computer Vi-
sion (ECCV) , pages 119–134, 2018. 2

--- TRANG 13 ---
Tài Liệu Bổ Sung
A. Khả Năng Tái Tạo
Tất cả mã nguồn, hình ảnh, mô hình, v.v., có sẵn tại
https://github.com/PavlicLab/WACV2024-
Hong-Concept_Centric_Transformers.git .

B. Chi Tiết Phương Pháp
Thuật Toán. Thuật toán 1 cho thấy chi tiết của mô-đun
Concept-Slot-Attention (CSA) trong Concept-Centric
Transformer (CCT) của chúng tôi ở dạng mã giả. Thuật
toán 1 được mô tả dựa trên SA [57], nhưng chúng tôi đơn
giản hóa nó bằng cách loại bỏ các lớp LayerNorm và MLP
cuối cùng. Do đặc tính mô-đun trong framework của chúng
tôi, chúng tôi tận dụng ba phương pháp dựa trên slot, bao
gồm SA [57], I-SA [10], và BO-QSA [41], có thể hoán đổi
cho nhau. Chúng tôi triển khai các phương pháp trên dựa
trên các repository/công trình chính thức của chúng.

Positional Embedding cho Concept Slots. Vì tập hợp
concept slots kết quả không có thứ tự [57], khó cho mô-
đun sau để: (i) nhận biết khái niệm cấp cao nào mà mỗi
concept slot đang đại diện và (ii) xác định concept slot
nào mà mỗi khái niệm cấp cao thuộc về. Do đó, chúng tôi
thêm một mã hóa vị trí pc vào mỗi concept slot để tránh
những thách thức này; cụ thể, ŝc=sc+pc. Sau đó, biểu
diễn embedding khái niệm Ŝconcept∈RC×d với positional
embedding P được truyền như biểu diễn khái niệm cuối
cùng cho mô-đun tiếp theo.

Thuật toán 1 Mô-đun Concept-Slot-Attention (CSA). Mô-
đun nhận tập hợp các đặc trưng đầu vào E∈RL×D; số
lượng khái niệm C; và chiều của khái niệm d. Các tham số
mô hình bao gồm: phép chiếu tuyến tính qCSA, kCSA, vCSA
với chiều đầu ra d; một mạng GRU; trung bình và hiệp
phương sai đường chéo của phân phối Gauss μ, σ∈Rd.

Sconcept=Tensor (C, d)▷concept −slots ∈RC×d
Sconcept∼ N(μ, σ)
E=LayerNorm (E)
for t= 0, . . . , T do
    Sconcept=LayerNorm (Sconcept)
    ACSA= softmax(1/√d qCSA(Sconcept)·
        kCSA(E)⊤,axis =′concept −slots′)
    ACSA=ACSA/ACSA.sum(axis =′inputs′)
    U=ACSA·vCSA(E)
    Sconcept=GRU(state =Sconcept,inputs =U)
end for
return Sconcept

Số Lần Lặp trong Mô-đun Concept Slot Attention. Trong
việc sử dụng gốc của các phương pháp dựa trên slot, việc
tinh chỉnh có thể được lặp lại nhiều lần tùy thuộc vào các
tác vụ. Để thiết lập số lần lặp T (Thuật toán 1), chúng tôi
đặt số lần lặp T khác nhau cho ba phương pháp dựa trên
slot mà chúng tôi sử dụng. Đối với SA gốc, chúng tôi đặt
T thành 1 vì việc khởi tạo cho các slot trong SA có một số
hạn chế được tiết lộ bởi [10], và chúng tôi thấy rằng một
lần lặp duy nhất để cập nhật concept slots có lợi trong đó
concept slots đã học sở hữu nhiều thông tin được thực hiện
thông qua huấn luyện với backpropagation. Nói cách khác,
sơ đồ phát sóng có thể diễn giải được đề xuất của chúng
tôi (4.2 trong văn bản chính) đóng góp vào hiệu suất tốt
hơn cho SA so với các vòng lặp tinh chỉnh nội bộ được sử
dụng trong các phương pháp dựa trên slot thông thường.
Hơn nữa, chúng tôi đặt T thành 3 cho BO-QSA và I-SA
bằng cách đơn giản làm theo các giá trị tốt nhất của các
vòng lặp trong công trình của chúng.

So Sánh với Concept Transformers [65]. Như được hiển
thị trong [65, Hình 1], Concept Transformers (CTs) là việc
sử dụng hạn chế trong công thức của chúng tôi, tận dụng
các vector có thể học để học khái niệm thay vì sử dụng
"một không gian làm việc chia sẻ". Chúng tôi nổi bật rằng
biểu diễn embedding khái niệm trong CCT của chúng tôi
tinh vi và tổng quát hơn so với cái trong CTs. Mỗi embedding
khái niệm trong CT được đại diện như một vector có thể
học đơn giản được chia sẻ với tất cả các batch đầu vào để
học. Như vậy, có thể khó cho vector nắm bắt đặc trưng
hình ảnh nào có thể đóng góp cho mỗi khái niệm nhiều
hơn những cái khác. Ví dụ, như được hiển thị trong Hình
2 trong văn bản chính, cho ba hình ảnh thuộc cùng lớp
"Winter Wren", các đặc trưng hình ảnh với cùng vị trí không
gian từ mỗi hình ảnh có thể có tầm quan trọng khác nhau
để cung cấp thông tin học cùng khái niệm toàn cục, chẳng
hạn như "màu cơ thể chính của Winter Wren là gì?" hoặc
"hình dạng cơ thể của nó là gì?". Ngược lại, mô-đun được
đề xuất của chúng tôi tự nhiên tổng hợp mức độ mỗi đặc
trưng hình ảnh đóng góp cho mỗi khái niệm sử dụng attention
ACSA (Xem trong Thuật toán 1) và cung cấp các biểu diễn
embedding khái niệm cụ thể theo batch có thông tin ngữ
nghĩa có ý nghĩa hơn. Trong tất cả các thí nghiệm, chúng
tôi chứng minh rằng CCTs của chúng tôi luôn vượt trội so
với CTs.

C. Kết Quả Thí Nghiệm Thêm và Chi Tiết
Trong phần này, chúng tôi giải thích kết quả thí nghiệm
thêm và chi tiết. Tất cả các thí nghiệm được tiến hành với
ba hạt giống ngẫu nhiên khác nhau và khoảng tin cậy 95%.

C.1. Thống Kê Bộ Dữ Liệu
Bảng A-1 mô tả thống kê của tất cả các bộ dữ liệu benchmark
trong các thí nghiệm của chúng tôi. Vì tất cả các bộ dữ liệu
không có phần validation, chúng tôi thủ công chọn phần
của bộ dữ liệu validation để khám phá các siêu tham số tốt
nhất cho các mô hình.

--- TRANG 14 ---
Đối với bộ dữ liệu CIFAR-100 Super-class, nó phục vụ
như một ví dụ đáng chú ý cho phân loại hình ảnh tinh tế.
Xuất phát từ bộ dữ liệu CIFAR-100 gốc, nó chứa 100 lớp
hình ảnh chi tiết được tổng hợp thành 20 super-classes riêng
biệt để phân loại rộng hơn. Ví dụ, super-class "vehicles 1"
bao gồm các lớp chi tiết cụ thể như "bicycle," "bus,"
"motorcycle," "pickup truck," và "train." Cấu trúc phân cấp
này làm cho bộ dữ liệu trở thành một benchmark được sử
dụng rộng rãi trong việc đánh giá các mô hình deep-learning,
đặc biệt là những mô hình kết hợp ràng buộc logic.

Đối với CUB-200-2011, chúng tôi giải thích các bước
tiền xử lý theo [65]. Ban đầu, bộ dữ liệu có 312 thuộc tính
nhị phân, nhưng chúng tôi lọc để chỉ giữ lại những thuộc
tính xuất hiện trong ít nhất 45% của tất cả các mẫu trong
một lớp nhất định và xuất hiện trong ít nhất 8 lớp. Do đó,
chúng tôi có tổng cộng 108 thuộc tính, và dựa trên điều
này, chúng tôi nhóm chúng thành hai loại khái niệm: không
gian và toàn cục. Cuối cùng chúng tôi có 13 khái niệm
toàn cục và 95 khái niệm không gian bằng cách xem xét
mỗi thuộc tính. Ví dụ, has shape::perching-like và has
primary color::black là các khái niệm toàn cục, và has
eye color::black và has forehead color::yellow là các
khái niệm không gian.

Đối với bộ dữ liệu ImageNet, chúng tôi áp dụng phương
pháp được trình bày trong [84] để xác thực khả năng của
CCT học các khái niệm tiềm ẩn mà không cần giải thích
khái niệm rõ ràng. [84] sử dụng các bản đồ attention để
xác định một tập hợp khái niệm được định nghĩa trước
trong bộ dữ liệu, tập trung vào 200 lớp đầu tiên của ImageNet
cho đánh giá của họ. Theo phương pháp này, chúng tôi
cũng sử dụng 200 lớp đầu tiên trong ImageNet để huấn
luyện và tiến hành đánh giá để xác định độ chính xác kiểm
tra trên bộ dữ liệu ImageNet. Thí nghiệm này phục vụ để
benchmark khả năng của mô hình CCT trong việc học các
khái niệm tiềm ẩn mà không có giải thích rõ ràng, thể hiện
thêm hiệu quả của nó trong thiết lập không giám sát.

C.2. Thông Số Kỹ Thuật Phần Cứng của Máy Chủ
Thông số kỹ thuật phần cứng của máy chủ mà chúng tôi
sử dụng để thí nghiệm như sau:
• CPU: Intel® Core™ i7-6950X CPU @ 3.00GHz (lên
đến 3.50 GHz)
• RAM: 128 GB (DDR4 2400MHz)
• GPU: NVIDIA GeForce Titan Xp GP102 (kiến trúc
Pascal, 3840 CUDA Cores @ 1.6 GHz, độ rộng bus
384-bit, bộ nhớ GDDR G5X 12 GB)

C.3. Kiến Trúc Mô Hình
Chúng tôi tận dụng ba loại backbone, bao gồm Vision
Transformers (ViT) [22], Swin Transformers (Swin) [55],
và ConvNeXt [56]. Chúng tôi sử dụng thư viện Python
timm được hỗ trợ bởi Hugging Face™.

Các Biến Thể của ViT. Trong các thí nghiệm của chúng
tôi, chúng tôi sử dụng ba biến thể của Vision Transformer
(ViT), ViT-Large, ViT-Small, và ViT-Tiny (vit_large_patch16_224,
vit_small_patch16_224, và vit_tiny_patch16_224, tương
ứng trong timm). Những biến thể này được định nghĩa
bởi số lượng khối encoder, số lượng đầu attention trên mỗi
khối, và chiều của lớp ẩn. ViT-Large có 24 khối encoder
với 16 đầu, và chiều của lớp ẩn là 1024. Ngoài ra, ViT-
Small có 12 khối encoder với 6 đầu, và chiều của lớp ẩn
là 384. Cuối cùng, ViT-Tiny có 12 khối encoder với 3 đầu,
và chiều của lớp ẩn là 192, nhẹ hơn nhiều. So sánh giữa
các biến thể của ViT được hiển thị trong Bảng A-2.

Kiến Trúc của Swin-Large. Trong thí nghiệm của chúng
tôi, chúng tôi sử dụng Swin Transformer (Swin)-Large
(swin_large_patch4_window7_224.ms_in22k_ft_in1k trong
timm). Xem Bảng A-3 cho kiến trúc tổng thể của Swin-
Large.

Kiến Trúc của ConvNeXt-Large. Trong thí nghiệm của
chúng tôi, chúng tôi tận dụng ConvNeXt-Large (convnext_large.
fb_in22k_ft_in1k trong timm). Xem Bảng A-4 cho kiến
trúc tổng thể của ConvNeXt-Large.

C.4. Thí Nghiệm CIFAR100 Super-class
Sự Khác Biệt Giữa CCT và Các Phương Pháp Deep-
Learning Khác với Ràng Buộc Logic. Ràng buộc logic
bởi [23] để giải quyết tác vụ này đã được giới thiệu, tức
là, một khi mô hình phân loại một hình ảnh thành một
lớp, các xác suất được dự đoán của super-class đó phải
đến trước với toàn bộ khối lượng. Tham chiếu đến điều
này, một số phương pháp deep-learning tận dụng nó [34,
51]. Ví dụ, năm lớp, baby, boy, girl, man, và woman, thuộc
về super-class people. Do đó, Pr_people(x) = 1 nếu x được
phân loại là baby. Vậy, ràng buộc logic có thể được công
thức hóa là ∧s∈superclass (Prs(x) = 0∨Prs(x) = 1), nơi
xác suất của super-class là tổng của các xác suất lớp tương
ứng, ví dụ: Pr_people(x) =Pr_baby(x) +Pr_boy(x) +Pr_girl(x)
+Pr_man(x) + Pr_woman(x).

Tuy nhiên, vì CCT của chúng tôi (và CT) tuân theo Mệnh
đề 1 trong [65], xác suất chọn super-class ưa thích là sc=
arg max s(βc)s của lớp c (Tham khảo Phương trình (2)
trong văn bản chính). Điều này có nghĩa là super-class
được xác định bởi điểm attention lớn nhất của lớp trong
tất cả các lớp, khác với định nghĩa của ràng buộc logic ở
trên.

Kết Quả Bổ Sung. Hình A-1 thể hiện một số ví dụ về
dự đoán chính xác bởi CT và CCT của chúng tôi, làm nổi
bật

--- TRANG 15 ---
[Bảng A-1. Thống kê Bộ dữ liệu Benchmark và các bảng khác]

rằng so với CT, CCT của chúng tôi đạt được điểm attention
giải thích thưa thớt hơn, cho thấy mức độ tin tưởng và
mạnh mẽ hơn cho việc ra quyết định.

Thiết Lập Siêu Tham Số. Đối với kết quả thí nghiệm,
bao gồm Bảng 1 và Hình 3 trong văn bản chính, tham
chiếu đến [23], trước tiên chúng tôi tìm thiết lập thí nghiệm
tốt nhất của cả ViT-Tiny và CT vì chúng chưa được đánh
giá trên CIFAR100 Super-class trước đây. Khi thiết lập
siêu tham số cho backbone ViT-Tiny được tìm thấy để
đạt hiệu suất tương tự như Wide ResNet (backbone cho
nhóm baseline thứ hai), chúng tôi áp dụng cùng thiết lập
siêu tham số cho cả CT và CCT của chúng tôi. Bảng A-5a
trình bày tất cả các siêu tham số chia sẻ cho ViT-Tiny,
CT, và tất cả các cấu hình của CCT. Vui lòng tham khảo
Bảng A-6 cho thiết lập siêu tham số được sử dụng trong
các thí nghiệm baseline của chúng tôi.

C.5. Thí Nghiệm CUB-200-2011
Kết Quả Bổ Sung với So Sánh với CT. Với các giải thích
khái niệm, chúng tôi giới thiệu kết quả thí nghiệm bổ sung
để so sánh dự đoán giữa CT và CCT của chúng tôi. Hình
A-2 mô tả một trường hợp nơi dự đoán của CT hoàn toàn
khác nhau mặc dù tất cả hình ảnh có cùng lớp. Các lớp
được dự đoán từ CT trong Hình A-2 là Prothonotary
Warbler, Pine Warbler, và Magnolia Warbler, tương ứng.
Sự khác biệt duy nhất giữa hai dự đoán đầu tiên mà CT
đưa ra là liệu khái niệm không gian has under tail color::black
có tồn tại hay không, cho thấy độ nhạy hiệu suất của CT.
Ngoài ra, dự đoán thứ ba từ CT chứa các khái niệm không
gian mới không liên quan đến hai ví dụ đầu tiên, cho thấy
sự không chắc chắn phân tích về lý do tại sao CT đưa ra
quyết định như vậy. Ngược lại, các dự đoán được đưa ra
bởi CCT của chúng tôi đều chính xác, với các giải thích
toàn cục nhất quán. Điều này chỉ ra rằng mặc dù việc kết
hợp các khái niệm không gian cung cấp thông tin bổ sung,
việc xác định các khái niệm toàn cục mạnh mẽ quan trọng
hơn cho tác vụ phân loại.

[Tiếp tục với các phần còn lại của tài liệu bổ sung...]

--- TRANG 16 ---
[Nội dung tiếp tục với các hình ảnh và mô tả thí nghiệm bổ sung]

--- TRANG 17 ---
[Nội dung tiếp tục]

--- TRANG 18 ---
[Nội dung tiếp tục]

--- TRANG 19 ---
[Nội dung tiếp tục với các hình ảnh trực quan hóa khái niệm]

--- TRANG 20 ---
[Nội dung tiếp tục]

--- TRANG 21 ---
[Nội dung tiếp tục với kết quả thí nghiệm bổ sung]

--- TRANG 22 ---
[Nội dung kết thúc với các bảng siêu tham số]

--- TRANG 23 ---
[Nội dung kết thúc với phần hạn chế]

strated ở một cấu hình được sử dụng trong mô-đun để
cho phép các khái niệm toàn cục và không gian học ảnh
hưởng khái niệm của nhau.

Thứ hai, mặc dù trong các thí nghiệm của CUB-200-
2011 và ImageNet mà không sử dụng giải thích, chúng
tôi chứng minh rằng CCT của chúng tôi có thể trực quan
hóa các khái niệm ngữ nghĩa đã học, chúng tôi thừa nhận
rằng CCT của chúng tôi có thể được tích hợp với các hàm
mất mát tinh vi hơn để đạt được khả năng trực quan hóa
tốt hơn của các khái niệm đã học. Ví dụ, [90] tận dụng các
hàm mất mát, bao gồm reconstruction, contrastive losses,
và nhiều regularizer khác nhau, để thực thi tính nhất quán
riêng lẻ và sự khác biệt lẫn nhau của các khái niệm. Trong
nghiên cứu tương lai của chúng tôi, chúng tôi có thể sử
dụng những hàm mất mát và regularizer đó để cho phép
mô hình của chúng tôi đạt được khả năng trực quan hóa
tốt hơn.
