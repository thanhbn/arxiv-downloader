# 2306.01129.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/interpretability/2306.01129.pdf
# Kích thước tệp: 4502171 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Transformers Hộp Trắng thông qua Giảm Tỷ Lệ Thưa Thớt
Yaodong Yu1Sam Buchanan2Druv Pai1Tianzhe Chu1Ziyang Wu1Shengbang Tong1
Benjamin D. Haeffele3Yi Ma1
1University of California, Berkeley2TTIC3Johns Hopkins University
Tóm tắt
Trong bài báo này, chúng tôi cho rằng mục tiêu của việc học biểu diễn là nén
và biến đổi phân phối của dữ liệu, chẳng hạn như các tập hợp token, về phía một hỗn hợp của
các phân phối Gaussian thấp chiều được hỗ trợ trên các không gian con không liên kết. Chất
lượng của biểu diễn cuối cùng có thể được đo bằng một hàm mục tiêu thống nhất
được gọi là giảm tỷ lệ thưa thớt. Từ góc độ này, các mạng sâu phổ biến như
transformers có thể được xem một cách tự nhiên như thực hiện các sơ đồ lặp để tối ưu hóa
mục tiêu này một cách gia tăng. Đặc biệt, chúng tôi chỉ ra rằng khối transformer tiêu chuẩn
có thể được dẫn xuất từ tối ưu hóa luân phiên trên các phần bổ sung của
mục tiêu này: toán tử tự chú ý đa đầu có thể được xem như một bước gradient descent
để nén các tập token bằng cách tối thiểu hóa tỷ lệ mã hóa có tổn thất của chúng, và
perceptron đa lớp tiếp theo có thể được xem như nỗ lực làm thưa thớt
biểu diễn của các token. Điều này dẫn đến một họ các kiến trúc mạng sâu
giống transformer hộp trắng hoàn toàn có thể diễn giải về mặt toán học. Mặc dù
đơn giản, các thí nghiệm cho thấy rằng những mạng này thực sự học để tối ưu hóa
mục tiêu được thiết kế: chúng nén và làm thưa thớt các biểu diễn của các tập dữ liệu
thị giác thế giới thực quy mô lớn như ImageNet, và đạt được hiệu suất rất gần với
các transformers được thiết kế kỹ lưỡng như ViT. Code tại https://github.
com/Ma-Lab-Berkeley/CRATE.
1 Giới thiệu
Trong những năm gần đây, học sâu đã chứng kiến thành công thực nghiệm to lớn trong việc xử lý lượng lớn
dữ liệu chiều cao và đa phương thức. Phần lớn thành công này được ghi nhận cho việc học hiệu quả
phân phối dữ liệu và sau đó biến đổi phân phối thành một biểu diễn tiết kiệm, tức là có cấu trúc và
compact [39, 49, 51, 61], điều này tạo thuận lợi cho nhiều tác vụ downstream (ví dụ, trong thị giác,
phân loại [23, 40], nhận dạng và phân đoạn [25, 38, 73], và sinh tạo [31, 64, 65]). Với
mục tiêu này, nhiều mô hình và phương pháp đã được đề xuất và thực hành, mỗi cái với những điểm mạnh
và hạn chế riêng. Ở đây, chúng tôi đưa ra một số phương pháp phổ biến một cách ngắn gọn như bối cảnh cho sự hiểu biết và thống nhất hoàn chỉnh mà chúng tôi tìm kiếm trong công trình này.
Mô hình Transformer và tự chú ý. Transformers [28] là một trong những mô hình phổ biến mới nhất
để học biểu diễn cho dữ liệu có cấu trúc chiều cao, như văn bản [28, 30, 37], hình ảnh
[40, 72], và các loại tín hiệu khác [48, 56]. Sau khối đầu tiên, chuyển đổi mỗi điểm dữ liệu
(như một tập văn bản hoặc hình ảnh) thành một tập hợp hoặc chuỗi các token, việc xử lý tiếp theo được thực hiện
trên các tập token, theo cách bất khả tri về phương tiện [28, 40]. Một nền tảng của mô hình transformer
là lớp tự chú ý được gọi, khai thác các tương quan thống kê giữa chuỗi
các token để tinh chỉnh biểu diễn token. Transformers đã rất thành công trong việc học
các biểu diễn compact thực hiện tốt trên nhiều tác vụ downstream. Tuy nhiên, kiến trúc mạng transformer
1{yyu,yima}@eecs.berkeley.edu, {druvpai,chutzh,zywu,tsb}@berkeley.edu
2sam@ttic.edu
3bhaeffele@jhu.eduarXiv:2306.01129v1  [cs.LG]  1 Jun 2023

--- TRANG 2 ---
nén thưa thớt hóaKhông Gian Con Đa Đầu
Tự Chú Ý
(MSSA)Bước Proximal Mã Hóa Thưa Thớt
(ISTA)
Hình 1: 'Vòng lặp chính' của thiết kế mạng sâu hộp trắng CRATE. Sau khi mã hóa dữ liệu đầu vào X như một
chuỗi các token Z0, CRATE xây dựng một mạng sâu biến đổi dữ liệu thành cấu hình chính tắc
của các không gian con thấp chiều bằng cách nén liên tiếp chống lại một mô hình cục bộ cho phân phối, tạo ra
Zℓ+1/2, và thưa thớt hóa chống lại một từ điển toàn cục, tạo ra Zℓ+1. Việc xếp chồng lặp lại các khối này và
huấn luyện các tham số mô hình thông qua lan truyền ngược tạo ra một biểu diễn mạnh mẽ và có thể diễn giải của dữ liệu.
được thiết kế thực nghiệm và thiếu diễn giải toán học nghiêm ngặt. Trên thực tế, đầu ra
của chính lớp attention có một số diễn giải cạnh tranh [67, 74]. Kết quả là, mối quan hệ
thống kê và hình học giữa phân phối dữ liệu và biểu diễn cuối cùng được học
bởi một transformer phần lớn vẫn là một hộp đen bí ẩn.
Mô hình khuếch tán và khử nhiễu. Mô hình khuếch tán [22, 34, 41, 43, 44] gần đây đã trở thành
một phương pháp phổ biến để học phân phối dữ liệu, đặc biệt cho các tác vụ sinh tạo và dữ liệu hình ảnh tự nhiên
có cấu trúc cao nhưng khó mô hình hóa hiệu quả [3, 5]. Khái niệm cốt lõi
của mô hình khuếch tán là bắt đầu với các đặc trưng được lấy mẫu từ phân phối nhiễu Gaussian (hoặc
một số template tiêu chuẩn khác) và lặp đi lặp lại khử nhiễu và biến dạng phân phối đặc trưng cho đến khi nó
hội tụ về phân phối dữ liệu gốc. Quá trình này không thể tính toán được nếu được mô hình hóa trong
chỉ một bước [60], vì vậy nó thường được chia thành nhiều bước gia tăng. Chìa khóa cho mỗi bước là
hàm điểm số được gọi, hoặc tương đương [13] một ước tính cho "hàm khử nhiễu tối ưu";
trong thực tế hàm này được mô hình hóa bằng một mạng sâu hộp đen chung. Mô hình khuếch tán
đã cho thấy hiệu quả trong việc học và lấy mẫu từ phân phối dữ liệu [55, 59, 64]. Tuy nhiên,
mặc dù có một số nỗ lực gần đây [77], chúng thường không thiết lập bất kỳ tương ứng rõ ràng nào giữa
các đặc trưng ban đầu và mẫu dữ liệu. Do đó, bản thân mô hình khuếch tán không cung cấp một
biểu diễn tiết kiệm hoặc có thể diễn giải của phân phối dữ liệu.
Mô hình tìm kiếm cấu trúc và giảm tỷ lệ. Trong cả hai phương pháp trước đó, các biểu diễn
được xây dựng ngầm như một sản phẩm phụ của việc giải quyết một tác vụ downstream (ví dụ, phân loại
hoặc sinh tạo/lấy mẫu) bằng mạng sâu. Tuy nhiên, người ta cũng có thể học một cách rõ ràng một biểu diễn
của phân phối dữ liệu như một tác vụ riêng lẻ; điều này thường được thực hiện bằng cách cố gắng xác định và
biểu diễn các cấu trúc thấp chiều trong dữ liệu đầu vào. Các ví dụ cổ điển của mô hình này bao gồm
các phương pháp dựa trên mô hình như mã hóa thưa thớt [2, 29] và học từ điển [17, 21, 47], từ đó
đã phát triển những nỗ lực ban đầu trong việc thiết kế và diễn giải kiến trúc mạng sâu [18, 32]. Những
phương pháp gần đây hơn xây dựng thay vào đó từ góc độ không mô hình, nơi người ta học một biểu diễn
thông qua một tác vụ pretext đủ thông tin (như nén các dữ liệu tương tự và tách biệt các dữ liệu khác biệt
trong học tương phản [45, 68, 76], hoặc tối đa hóa thu hoạch thông tin trong lớp các phương pháp giảm tỷ lệ mã hóa tối đa [6, 46, 54]). So với các phương pháp học sâu hộp đen, cả hai
sơ đồ học biểu diễn dựa trên mô hình và không mô hình đều có lợi thế là có thể diễn giải hơn: chúng cho phép người dùng thiết kế một cách rõ ràng các tính chất mong muốn của biểu diễn đã học [46,
54, 62]. Hơn nữa, chúng cho phép người dùng xây dựng các kiến trúc mạng sâu hộp trắng được xây dựng từ trước mới [11, 54, 58] bằng cách mở rộng chiến lược tối ưu hóa cho mục tiêu học biểu diễn, sao cho mỗi lớp của mạng được xây dựng thực hiện một lần lặp của thuật toán tối ưu hóa [11, 52, 54]. Thật không may, trong mô hình này, nếu các tính chất mong muốn được định nghĩa hẹp, có thể khó đạt được hiệu suất thực tế tốt trên các tập dữ liệu thế giới thực lớn.
Đóng góp của chúng tôi, và phác thảo công trình này. Trong công trình này, chúng tôi nhằm khắc phục các hạn chế
của những phương pháp hiện có này với một khung làm việc thống nhất hơn để thiết kế các kiến trúc mạng
giống transformer dẫn đến cả khả năng diễn giải toán học và hiệu suất thực tế tốt. Với
mục tiêu này, chúng tôi đề xuất học một chuỗi các ánh xạ gia tăng để có được một biểu diễn
nén và thưa thớt nhất cho dữ liệu đầu vào (hoặc các tập token của chúng) tối ưu hóa một hàm mục tiêu thống nhất
được biết như giảm tỷ lệ thưa thớt, được chỉ định sau trong (1). Mục tiêu của ánh xạ được minh họa
trong Hình 1. Trong khung làm việc này, chúng tôi thống nhất ba phương pháp dường như khác biệt ở trên và
2

--- TRANG 3 ---
chỉ ra rằng các lớp mạng sâu giống transformer có thể được dẫn xuất một cách tự nhiên từ việc mở rộng các
sơ đồ tối ưu hóa lặp để tối ưu hóa mục tiêu giảm tỷ lệ thưa thớt một cách gia tăng. Đặc biệt, các
đóng góp và phác thảo của bài báo như sau:
•Trong Mục 2.2 chúng tôi chỉ ra, sử dụng một mô hình lý tưởng hóa cho phân phối token, rằng nếu người ta
lặp đi lặp lại khử nhiễu các token về phía một họ các không gian con thấp chiều, hàm điểm số liên quan
giả định một dạng rõ ràng tương tự như một toán tử tự chú ý được thấy trong transformers.
•Trong Mục 2.3 chúng tôi dẫn xuất lớp tự chú ý đa đầu như một bước gradient descent được mở rộng
để tối thiểu hóa phần tỷ lệ mã hóa có tổn thất của việc giảm tỷ lệ, cho thấy một diễn giải khác của
lớp tự chú ý như nén biểu diễn token.
•Trong Mục 2.4 chúng tôi chỉ ra rằng perceptron đa lớp ngay sau tự chú ý đa đầu
trong các khối transformer có thể được diễn giải như (và thay thế bằng) một lớp
tối ưu hóa một cách gia tăng phần còn lại của mục tiêu giảm tỷ lệ thưa thớt bằng cách
xây dựng một mã hóa thưa thớt của các biểu diễn token.
•Trong Mục 2.5 chúng tôi sử dụng hiểu biết này để tạo ra một kiến trúc transformer hộp trắng mới
(hoàn toàn có thể diễn giải về mặt toán học) được gọi là CRATE (tức là, Coding RAte reduction TransformEr),
nơi mỗi lớp thực hiện một bước duy nhất của thuật toán tối thiểu hóa luân phiên để tối ưu hóa
mục tiêu giảm tỷ lệ thưa thớt.
Do đó, trong khung làm việc của chúng tôi, hàm mục tiêu học, kiến trúc học sâu, và
biểu diễn cuối cùng đã học tất cả trở thành hộp trắng hoàn toàn có thể diễn giải về mặt toán học.
Như các thí nghiệm trong Mục 3 cho thấy, các mạng CRATE, mặc dù đơn giản, đã có thể học
các biểu diễn nén và thưa thớt mong muốn trên các tập dữ liệu thế giới thực quy mô lớn và đạt được
hiệu suất ngang với các mạng transformer được thiết kế kỹ lưỡng hơn nhiều (như ViT) trên
nhiều tác vụ khác nhau (ví dụ, phân loại và học chuyển giao).
2 Phương Pháp Kỹ Thuật và Biện Minh
2.1 Mục Tiêu và Phương Pháp
Chúng tôi xem xét một thiết lập học tổng quát liên quan đến các tín hiệu thế giới thực. Chúng tôi có một biến ngẫu nhiên X= [x1, . . . ,xN]∈RD×N là nguồn dữ liệu của chúng tôi; mỗi xi∈RD được diễn giải như một
token1, và các xi có thể có cấu trúc tương quan tùy ý. Chúng tôi sử dụng Z= [z1, . . . ,zN]∈Rd×N để
ký hiệu biến ngẫu nhiên định nghĩa các biểu diễn của chúng tôi. Mỗi zi∈Rd là biểu diễn của
token tương ứng xi. Chúng tôi được cho B≥1 mẫu i.i.d. X1, . . . ,XB∼X, có các token là
xi,b. Các biểu diễn của các mẫu chúng tôi được ký hiệu Z1, . . . ,ZB∼Z, và của các token chúng tôi là
zi,b. Cuối cùng, cho một mạng nhất định, chúng tôi sử dụng Zℓ để ký hiệu đầu ra của ℓ lớp đầu tiên khi cho X
làm đầu vào. Tương ứng, các đầu ra mẫu là Zℓ
i và các đầu ra token là zℓ
i,b.
Mục tiêu để học một biểu diễn có cấu trúc và compact. Theo khung làm việc của
giảm tỷ lệ [54], chúng tôi cho rằng mục tiêu của việc học biểu diễn là tìm một ánh xạ đặc trưng
f:X∈RD×N→Z∈Rd×N biến đổi dữ liệu đầu vào X∈RD×N với một phân phối
có thể phi tuyến và đa phương thức thành một biểu diễn đặc trưng tuyến tính hóa (từng mảnh) và compact
Z∈Rd×N. Trong khi phân phối kết hợp của các token (zi)N
i=1 trong Z có thể phức tạp (và phụ thuộc
nhiệm vụ), chúng tôi tiếp tục cho rằng hợp lý và thực tế khi yêu cầu phân phối biên mục tiêu
của các token riêng lẻ zi nên được nén cao và có cấu trúc, phù hợp cho
mã hóa compact. Đặc biệt, chúng tôi yêu cầu phân phối là một hỗn hợp của các phân phối Gaussian thấy chiều (giả sử K), sao cho Gaussian thứ k có mean 0∈Rd, covariance Σk⪰0∈Rd×d, và hỗ trợ
được bao trùm bởi cơ sở trực chuẩn Uk∈Rd×p. Chúng tôi ký hiệu U[K]= (Uk)K
k=1 là tập hợp các cơ sở
của tất cả Gaussians. Do đó để tối đa hóa thu hoạch thông tin [61] cho biểu diễn token cuối cùng,
chúng tôi muốn tối đa hóa giảm tỷ lệ [6, 46] của các token, tức là, max Z∆R(Z;U[K]) =R(Z)−
Rc(Z;U[K]), nơi R và Rc là các ước tính của tỷ lệ mã hóa có tổn thất được định nghĩa chính thức trong (7)
và (8). Điều này cũng thúc đẩy các biểu diễn token zi từ các Gaussians khác nhau không liên kết [46].
Vì giảm tỷ lệ là một thước đo nội tại về tính tốt cho biểu diễn, nó bất biến với
các phép quay tùy ý của các biểu diễn. Do đó, để đảm bảo các biểu diễn cuối cùng phù hợp
cho mã hóa compact hơn, chúng tôi muốn biến đổi các biểu diễn (và các không gian con hỗ trợ
1Đối với transformers ngôn ngữ, tokens gần như tương ứng với từ [28], trong khi đối với transformers thị giác, tokens
tương ứng với các patch hình ảnh [40].
3

--- TRANG 4 ---
của chúng) sao cho chúng trở thành thưa thớt đối với các tọa độ tiêu chuẩn của không gian biểu diễn
kết quả.2 Quá trình giảm tỷ lệ và thưa thớt hóa kết hợp được minh họa trong Hình 1.
Về mặt tính toán, chúng tôi có thể kết hợp hai mục tiêu trên thành một mục tiêu thống nhất để tối ưu hóa:
max
f∈FEZ
∆R(Z;U[K])−λ∥Z∥0
= max
f∈FEZ
R(Z)−Rc(Z;U[K])−λ∥Z∥0
s.t.Z=f(X),(1)
nơi chuẩn ℓ0 ∥Z∥0 thúc đẩy tính thưa thớt của các biểu diễn token cuối cùng Z=f(X).3 Chúng tôi
gọi mục tiêu này "giảm tỷ lệ thưa thớt."
Kiến trúc sâu hộp trắng như tối ưu hóa gia tăng được mở rộng. Mặc dù dễ phát biểu, mỗi
hạng của mục tiêu trên có thể rất thách thức về mặt tính toán để tối ưu hóa [54, 69]. Do đó tự nhiên
là áp dụng một phương pháp xấp xỉ thực hiện biến đổi toàn cục f tối ưu hóa (1)
thông qua một chuỗi nhiều, giả sử L, hoạt động gia tăng và cục bộ đơn giản fℓ đẩy
phân phối biểu diễn về phía phân phối mô hình tiết kiệm mong muốn:
f:Xf0
− − →Z0→ ··· → Zℓfℓ
− − →Zℓ+1→ ··· → ZL=Z, (2)
nơi f0:RD→Rd là ánh xạ tiền xử lý biến đổi các token đầu vào xi∈RD thành
biểu diễn token z1
i∈Rd của chúng.
Mỗi ánh xạ tiến gia tăng Zℓ+1=fℓ(Zℓ), hoặc một "lớp", biến đổi phân phối token
để tối ưu hóa mục tiêu giảm tỷ lệ thưa thớt (1) trên, có điều kiện trên phân phối của
token đầu vào Zℓ của nó. Trái ngược với các phương pháp tối ưu hóa mở rộng khác như ReduNet [54],
chúng tôi mô hình hóa một cách rõ ràng phân phối của Zℓ tại mỗi lớp, giả sử như một hỗn hợp của các không gian con tuyến tính hoặc
được tạo ra một cách thưa thớt từ một từ điển. Các tham số mô hình được học từ dữ liệu (giả sử thông qua lan truyền
ngược với huấn luyện end-to-end). Sự tách biệt này của "tối ưu hóa" tiến và "học" lùi
làm rõ vai trò toán học của mỗi lớp như một toán tử biến đổi phân phối
của đầu vào của nó, trong khi phân phối đầu vào lần lượt được mô hình hóa (và sau đó được học) bởi
các tham số của lớp.
Chúng tôi chỉ ra rằng chúng tôi có thể dẫn xuất những hoạt động gia tăng, cục bộ này thông qua một góc độ tối ưu hóa mở rộng
để đạt được (1) thông qua các Mục 2.3 đến 2.5. Một khi chúng tôi quyết định sử dụng một phương pháp gia tăng
để tối ưu hóa (1), có nhiều lựa chọn khả thi khác nhau để đạt được tối ưu hóa. Cho một mô hình
cho Zℓ, giả sử một hỗn hợp của các không gian con U[K], chúng tôi chọn một quá trình tối thiểu hóa luân phiên hai bước
với cơ sở khái niệm mạnh mẽ: đầu tiên trong Mục 2.3, chúng tôi nén các token Zℓ thông qua một bước gradient
để tối thiểu hóa hạng tỷ lệ mã hóa minZRc(Z;U[K]); thứ hai, trong Mục 2.4, chúng tôi thưa thớt hóa các
token đã nén, với một bước gradient proximal được nới lỏng phù hợp trên hiệu của hình phạt thưa thớt
và hạng mở rộng, tức là, minZ[λ∥Z∥0−R(Z)]. Cả hai hành động được áp dụng một cách gia tăng
và lặp lại, như mỗi fℓ trong (2) được khởi tạo với hai bước này.
2.2 Tự Chú Ý thông qua Khử Nhiễu Token về Nhiều Không Gian Con
Có nhiều cách khác nhau để tối ưu hóa mục tiêu (1) một cách gia tăng. Trong công trình này, chúng tôi đề xuất
sơ đồ cơ bản có lẽ nhất. Để giúp làm rõ trực giác đằng sau dẫn xuất và xấp xỉ của chúng tôi,
trong mục này (và Phụ lục A.1) chúng tôi nghiên cứu một mô hình phần lớn lý tưởng hóa nhưng vẫn nắm bắt
bản chất của gần như toàn bộ quá trình và đặc biệt tiết lộ lý do tại sao các toán tử giống tự chú ý
phát sinh trong nhiều bối cảnh. Giả sử rằng N= 1, và token đơn x được rút i.i.d. từ
một hỗn hợp chưa biết của Gaussians (N(0,Σk))K
k=1 được hỗ trợ trên các không gian con thấp chiều với
các cơ sở trực chuẩn U[K]= (Uk)K
k=1 và bị hỏng bởi nhiễu Gaussian cộng w∼ N(0,I), tức là,
x=z+σw, (3)
nơi z được phân phối theo hỗn hợp. Mục tiêu của chúng tôi đơn giản là biến đổi phân phối của
token nhiễu x thành hỗn hợp của Gaussians thấp chiều z. Hướng tới việc xây dựng gia tăng
một biểu diễn f cho mô hình này theo (2), chúng tôi lý luận một cách quy nạp: nếu zℓ là một token nhiễu (3) tại
mức nhiễu σℓ, tự nhiên là tạo ra zℓ+1 bằng cách khử nhiễu ở mức σℓ. Theo nghĩa trung bình bình phương,
ước tính tối ưu là E[z|zℓ], có đặc trưng biến phân (ví dụ [12]):
E[z| ·] = arg min
fE
z,wh

f(z+σℓw)−z

2
2i
. (4)
2Tức là, có ít mục khác không nhất.
3Để đơn giản hóa ký hiệu, chúng tôi sẽ thảo luận mục tiêu cho một mẫu X tại một thời điểm với hiểu biết
rằng chúng tôi luôn có ý nghĩa tối ưu hóa kỳ vọng.
4

--- TRANG 5 ---
Đặt zℓ+1=E[z|zℓ], (4) do đó đặc trưng giai đoạn tiếp theo của (2) theo một mục tiêu tối ưu hóa
dựa trên một mô hình tín hiệu cục bộ cho zℓ. Hơn nữa, đặt x7→qℓ(x) ký hiệu mật độ của zℓ,
công thức Tweedie [13] cho phép chúng tôi biểu hiện biểu diễn tối ưu giải quyết (4) ở dạng đóng:
zℓ+1=zℓ+ (σℓ)2∇xlogqℓ(zℓ). (5)
Công thức Tweedie biểu hiện biểu diễn tối ưu theo một hiệu chỉnh cộng (nói chung là
một hàm phi tuyến của zℓ) cho các quan sát nhiễu bởi gradient của log-likelihood của
phân phối của các quan sát nhiễu, cho biểu diễn tối ưu một diễn giải rõ ràng như một
nhiễu loạn gia tăng cho phân phối nhiễu hiện tại qℓ. Kết nối này được biết đến rộng rãi trong các
lĩnh vực lý thuyết ước tính và bài toán nghịch [1, 13, 14, 19, 20, 27, 42], và gần đây hơn đã
tìm thấy các ứng dụng mạnh mẽ trong việc huấn luyện mô hình sinh tạo cho hình ảnh tự nhiên [4, 15, 22, 43,
44]. Ở đây, chúng tôi có thể tính toán một biểu thức dạng đóng cho hàm điểm số ∇xlogqℓ này, khi
kết hợp với (5) và một số giả định kỹ thuật4, cho xấp xỉ sau (được trình bày trong
Phụ lục A.1). Đặt ⊗ ký hiệu tích Kronecker; sau đó chúng tôi có
zℓ+1≈[U1, . . . ,UK]
diag
softmax
1
2(σℓ)2
∥U∗
1zℓ∥2
2...
∥U∗
Kzℓ∥2
2


⊗Ip

U∗
1zℓ
...
U∗
Kzℓ
,(6)
Hoạt động này giống với một lớp tự chú ý trong kiến trúc transformer tiêu chuẩn với K đầu,
độ dài chuỗi N= 1, các cấu trúc "query-key-value" được thay thế bằng một phép chiếu tuyến tính đơn
U∗
kzℓ của token zℓ, và việc tổng hợp các đầu ra head (theo quy ước được mô hình hóa bởi một MLP)
được thực hiện với hai ma trận bên trái nhất trong (6). Do đó chúng tôi dẫn xuất diễn giải hữu ích sau, mà
chúng tôi sẽ khai thác trong phần tiếp theo: Khử nhiễu Gaussian chống lại một mô hình hỗn hợp của các không gian con dẫn đến
các lớp kiểu tự chú ý trong biến đổi f. Cho một mẫu ban đầu x theo mô hình
(3), chúng tôi có thể áp dụng lặp lại các biến đổi cục bộ cho phân phối với (6) để thực hiện
ánh xạ gia tăng f:x→z trong (2).5 Những hiểu biết này sẽ hướng dẫn chúng tôi trong thiết kế kiến trúc transformer hộp trắng của chúng tôi trong các mục tiếp theo.
2.3 Tự Chú Ý thông qua Nén Tập Token qua Tối Ưu Hóa Giảm Tỷ Lệ
Trong mục cuối, chúng tôi đã thấy rằng tự chú ý đa đầu trong một transformer giống với toán tử
khớp điểm số nhằm biến đổi một token zℓ về phía một hỗn hợp của các không gian con (hoặc degenerate
Gaussians). Tuy nhiên, để thực hiện hoạt động như vậy trên bất kỳ dữ liệu nào, người ta cần phải đầu tiên học hoặc
ước tính, thường từ các mẫu hữu hạn, các tham số của hỗn hợp của (degenerate) Gaussians,
điều này được biết là một nhiệm vụ thách thức [6, 24]. Thách thức này trở nên khó khăn hơn bởi vì trong một
thiết lập học tập điển hình, tập hợp các token đã cho không phải là các mẫu i.i.d. từ hỗn hợp của các không gian con.
Phân phối kết hợp giữa những token này có thể mã hóa thông tin phong phú về dữ liệu—ví dụ,
sự đồng xuất hiện giữa từ hoặc các phần đối tượng trong dữ liệu ngôn ngữ và hình ảnh (tương ứng)—mà chúng ta cũng
nên học. Do đó, chúng ta nên nén / khử nhiễu / biến đổi một tập hợp như vậy của các token cùng nhau. Với mục tiêu này,
chúng ta cần một thước đo chất lượng, tức là, độ compactness, cho biểu diễn kết quả của tập hợp các token.
Một thước đo tự nhiên về độ compactness của một tập hợp token như vậy là tỷ lệ mã hóa (có tổn thất) để mã hóa
chúng đến một độ chính xác nhất định ϵ >0[6, 46]. Đối với một Gaussian trung bình không, thước đo này có một
dạng đóng. Nếu chúng ta xem các token trong Z∈Rd×N như được rút từ một Gaussian trung bình không duy nhất, một ước tính
của tỷ lệ mã hóa (có tổn thất) của chúng, tuân theo độ chính xác lượng tử ϵ >0, được cho trong [6] như:
R(Z).=1
2logdet
I+d
Nϵ2Z∗Z
=1
2logdet
I+d
Nϵ2ZZ∗
. (7)
Trong thực tế, phân phối dữ liệu thường là đa phương thức, giả sử một tập hình ảnh bao gồm nhiều lớp
hoặc một tập hợp các patch hình ảnh như trong Hình 1. Phù hợp hơn là yêu cầu rằng tập hợp các
token ánh xạ đến một hỗn hợp của, giả sử K, không gian con (degenerate Gaussians) [54]. Như trước chúng ta ký hiệu
các cơ sở (được học) của những không gian con này là U[K]= (Uk)K
k=1, nơi Uk∈Rd×p. Mặc dù
phân phối kết hợp của các token Z chưa biết, phân phối biên mong muốn của mỗi token zi là một
hỗn hợp của các không gian con. Vì vậy chúng ta có thể có được một cận trên của tỷ lệ mã hóa cho tập token Z bằng cách
chiếu các token của nó lên những không gian con này và tổng các tỷ lệ mã hóa tương ứng:
Rc(Z;U[K]) =KX
k=1R(U∗
kZ) =1
2KX
k=1logdet
I+p
Nϵ2(U∗
kZ)∗(U∗
kZ)
. (8)
Chúng ta muốn nén (hoặc khử nhiễu) tập hợp các token chống lại những không gian con này bằng cách tối thiểu hóa
tỷ lệ mã hóa. Gradient của Rc(Z;U[K]) là
∇ZRc(Z;U[K]) =p
Nϵ2KX
k=1UkU∗
kZ
I+p
Nϵ2(U∗
kZ)∗(U∗
kZ)−1
. (9)
Biểu thức trên xấp xỉ phần dư của mỗi token được chiếu U∗
kzi được hồi quy bởi các
token khác U∗
kzj[54]. Nhưng, khác với [54], không phải tất cả token trong Z đều từ cùng một không gian con. Do đó,
để khử nhiễu mỗi token với các token từ nhóm riêng của nó, chúng ta có thể tính toán độ tương tự của chúng thông qua một
tự tương quan giữa các token được chiếu như (U∗
kZ)∗(U∗
kZ) và chuyển đổi nó thành một phân phối của
thành viên với một softmax, cụ thể là, softmax(( U∗
kZ)∗(U∗
kZ)). Sau đó, như chúng tôi chỉ ra trong Phụ lục A.2,
nếu chúng ta chỉ sử dụng các token tương tự để hồi quy và khử nhiễu lẫn nhau, thì một bước gradient trên
tỷ lệ mã hóa với tỷ lệ học κ có thể được xấp xỉ một cách tự nhiên như sau:
Zℓ+1/2=Zℓ−κ∇ZRc(Zℓ;U[K])≈
1−κ·p
Nϵ2
Zℓ+κ·p
Nϵ2·MSSA(Zℓ|U[K]),(10)
nơi MSSA được định nghĩa thông qua một toán tử SSA như:
SSA(Z|Uk).= (U∗
kZ) softmax(( U∗
kZ)∗(U∗
kZ)), k∈[K], (11)
MSSA(Z|U[K]).=p
Nϵ2·[U1, . . . ,UK]
SSA(Z|U1)
...
SSA(Z|UK)
. (12)
Ở đây toán tử SSA trong (11) giống với toán tử attention trong một transformer điển hình [28], ngoại trừ
rằng ở đây các toán tử tuyến tính của value, key, và query đều được đặt giống như
cơ sở không gian con, tức là, V=K=Q=U∗
k.6 Do đó, chúng tôi đặt tên SSA(·|Uk) :Rd×N→Rp×N là
toán tử Tự Chú Ý Không Gian Con (SSA) (chi tiết và biện minh thêm có thể tìm thấy trong (72) ở Phụ lục A.2).
Sau đó, toàn bộ toán tử MSSA trong (12), được định nghĩa chính thức là MSSA(·|U[K]):Rd×N→Rd×N và
được gọi là toán tử Tự Chú Ý Không Gian Con Đa Đầu (MSSA), tổng hợp các đầu ra attention head
bằng cách tính trung bình sử dụng các trọng số phụ thuộc mô hình, tương tự về khái niệm với toán tử tự chú ý đa đầu
phổ biến trong các mạng transformer hiện có. Bước gradient tổng thể (10) giống với
tự chú ý đa đầu được thực hiện với một kết nối bỏ qua trong transformers.
Lưu ý rằng nếu chúng ta có N= 1 token cũng như thực hiện một bước gradient tích cực (κ= 1) và điều chỉnh
lỗi lượng tử (ϵ=p
p/N), toán tử tự chú ý không gian con đa đầu trong (12) trở thành
bộ khử nhiễu lý tưởng được định nghĩa trong (6), với một khác biệt nhỏ là việc tổng hợp các head được thực hiện
bởi một hàm tuyến tính ở đây, trong khi ở (6) nó được thực hiện bởi một hàm kiểu mixture-of-experts phi tuyến.7
Điều này cung cấp hai diễn giải rất liên quan của toán tử tự chú ý đa đầu, như khử nhiễu
và nén chống lại một hỗn hợp của các không gian con thấp chiều.
2.4 MLP thông qua Thuật Toán Shrinkage-Thresholding Lặp (ISTA) cho Mã Hóa Thưa Thớt
Trong mục trước, chúng tôi tập trung vào cách nén một tập hợp các token chống lại một tập hợp các
không gian con thấp chiều (đã học). Tối ưu hóa các hạng còn lại trong mục tiêu giảm tỷ lệ thưa thớt
(1), bao gồm hạng không trơn, phục vụ để thưa thớt hóa các token đã nén, do đó dẫn đến một
biểu diễn compact và có cấu trúc hơn (tức là, tiết kiệm). Từ (1) và (7), hạng này là
max
Z[R(Z)−λ∥Z∥0] = min
Z
λ∥Z∥0−1
2logdet
I+d
Nϵ2Z∗Z
, (13)
6Chúng tôi lưu ý một gợi ý gần đây của Hinton [50] rằng việc đặt các ma trận chiếu "value, key, và query"
trong một transformer bằng nhau là hợp lý hơn. Dẫn xuất của chúng tôi trong mục này xác nhận điều này về mặt toán học.
7Điều này cho thấy rằng chúng ta cũng có thể xem xét việc tổng hợp kiểu mixture of expert cho các
attention head đa đầu. Trong công trình này, chúng tôi sử dụng tổng hợp tuyến tính, và để lại việc đánh giá các biến thể khác cho công việc tương lai.
6

--- TRANG 7 ---
Tự Chú Ý Không Gian Con
Đa Đầu
(MSSA)Cộng & LayerNormBước Proximal
Mã Hóa Thưa Thớt
(ISTA)LayerNorm
Khối MSSAKhối SSA (head 1
Khối SSA (head K)Tổng hợp . . .Khối SSAAutocorrelation
& SoftmaxKhối ISTAActivation
Hình 2: Một lớp của kiến trúc CRATE. Kiến trúc đầy đủ đơn giản là một chuỗi của những lớp như vậy,
với một số tokenizer ban đầu và kiến trúc cuối cùng đặc thù tác vụ (tức là, một classification head).
nơi R(Z) ký hiệu tỷ lệ mã hóa của toàn bộ tập token, như được định nghĩa trong (7). Ngoài
thưa thớt hóa thông qua hạng ∥Z∥0, hạng mở rộng R(Z) trong (13) thúc đẩy tính đa dạng và không
sụp đổ của biểu diễn, một tính chất rất mong muốn. Tuy nhiên, công việc trước đây đã gặp khó khăn để
thực hiện lợi ích này trên các tập dữ liệu quy mô lớn do khả năng mở rộng kém của gradient ∇ZR(Z), 
yêu cầu một nghịch đảo ma trận [54].
Để đơn giản hóa mọi thứ, do đó chúng tôi áp dụng một phương pháp khác để đánh đổi giữa tính đa dạng biểu diễn
và thưa thớt hóa: chúng tôi đặt ra một từ điển (hoàn chỉnh) không liên kết hoặc trực giao D∈Rd×d, và
yêu cầu thưa thớt hóa các lần lặp trung gian Zℓ+1/2 đối với D. Tức là, Zℓ+1/2=DZℓ+1 nơi
Zℓ+1 thưa thớt hơn. Từ điển D là toàn cục, tức là, được sử dụng để thưa thớt hóa tất cả token đồng thời.
Bởi giả định không liên kết, chúng ta có D∗D≈Id; do đó từ (7) chúng ta có R(Zℓ+1)≈
R(DZℓ+1) =R(Zℓ+1/2). Do đó chúng ta xấp xỉ giải quyết (13) với chương trình sau:
Zℓ+1= arg min
Z∥Z∥0subject to Zℓ+1/2=DZ. (14)
Chương trình biểu diễn thưa thớt trên thường được giải quyết bằng cách nới lỏng nó thành một chương trình lồi
không ràng buộc, được biết như LASSO:
Zℓ+1= arg min
Zh
λ∥Z∥1+∥Zℓ+1/2−DZ∥2
Fi
. (15)
Trong triển khai của chúng tôi, được động lực bởi Sun et al. [33] và Zarka et al. [35], chúng tôi cũng thêm một ràng buộc
không âm cho Zℓ+1,
Zℓ+1= arg min
Z≥0h
λ∥Z∥1+∥Zℓ+1/2−DZ∥2
Fi
, (16)
mà chúng tôi sau đó tối ưu hóa một cách gia tăng bằng cách thực hiện một bước gradient proximal được mở rộng,
được biết như một bước ISTA [8], để đưa ra cập nhật:
Zℓ+1= ReLU( Zℓ+1/2+ηD∗(Zℓ+1/2−DZℓ+1/2)−ηλ1).=ISTA(Zℓ+1/2|D). (17)
Trong Phụ lục A.3, chúng tôi sẽ chỉ ra người ta có thể đến một toán tử tương tự với cập nhật giống ISTA ở trên cho
việc tối ưu hóa (13) bằng cách tuyến tính hóa và xấp xỉ một cách thích hợp hạng tỷ lệ R(Z).
2.5 Kiến Trúc CRATE Hộp Trắng Tổng Thể
Bằng cách kết hợp hai bước trên:
1.(Mục 2.2 và 2.3) Khử nhiễu và nén cục bộ của các token trong một mẫu về phía một
cấu trúc mixture-of-subspace, dẫn đến khối tự chú ý không gian con đa đầu – MSSA;
7

--- TRANG 8 ---
2.(Mục 2.4) Nén toàn cục và thưa thớt hóa của các tập token qua tất cả mẫu thông qua
mã hóa thưa thớt, dẫn đến khối thưa thớt hóa – ISTA;
chúng ta có thể có được lớp transformer dựa trên giảm tỷ lệ sau, được minh họa trong Hình 2,
Zℓ+1/2.=Zℓ+MSSA(Zℓ|Uℓ
[K]),Zℓ+1.=ISTA(Zℓ+1/2|Dℓ). (18)
Tổng hợp nhiều lớp như vậy theo việc xây dựng gia tăng biểu diễn của chúng ta trong (2),
chúng ta có được một kiến trúc transformer hộp trắng biến đổi các token dữ liệu về phía một hợp
compact và thưa thớt của các không gian con không liên kết.
Mô hình này có các tham số (Uℓ
[K])L
ℓ=1và(Dℓ)L
ℓ=1, được học từ dữ liệu thông qua lan truyền
ngược. Đáng chú ý, trong mỗi lớp ℓ, các Uℓ
[K] đã học giữ lại diễn giải của chúng như các
cơ sở không liên kết cho các không gian con hỗ trợ cho mô hình mixture-of-Gaussians tại lớp ℓ, và Dℓ đã học
giữ lại diễn giải của nó như một từ điển thưa thớt hóa tại lớp ℓ. Chúng tôi nhấn mạnh rằng các tham số
Uℓ
[K] và Dℓ phụ thuộc vào lớp ℓ— tức là, chúng ta học một tập hợp tham số khác nhau tại mỗi
lớp. Điều này là do tại mỗi lớp chúng ta học một mô hình tham số cục bộ xấp xỉ cho phân phối dữ liệu đầu vào,
sau đó sử dụng mô hình đã học đó để xây dựng các toán tử lớp biến đổi phân phối. Quy trình
tham số hóa phân phối dữ liệu tại mỗi lớp của chúng tôi phân biệt công việc này với các công việc
trước đây về tối ưu hóa mở rộng cho mạng neural như ReduNet [54].
Diễn giải của chúng tôi làm rõ vai trò của forward pass mạng (cho các mô hình tín hiệu cục bộ tại mỗi
lớp, khử nhiễu/nén/thưa thớt hóa đầu vào) và backward pass (học các mô hình tín hiệu cục bộ từ
dữ liệu thông qua giám sát).
Chúng tôi lưu ý rằng trong công việc này, tại mỗi giai đoạn xây dựng của chúng tôi, chúng tôi đã chọn có lẽ
việc xây dựng đơn giản nhất có thể để sử dụng. Chúng ta có thể thay thế mỗi phần của việc xây dựng này, miễn là phần mới
duy trì cùng vai trò khái niệm, và có được một kiến trúc hộp trắng khác. Tuy nhiên, kiến trúc
được xây dựng như vậy của chúng tôi, được gọi là CRATE (tức là, Coding RAte TransformEr), kết nối với các
mô hình transformer hiện có, có được kết quả cạnh tranh trên các tập dữ liệu thế giới thực, và hoàn toàn có thể diễn giải về mặt toán học.
3 Thí Nghiệm
Trong mục này, chúng tôi tiến hành thí nghiệm để nghiên cứu hiệu suất của transformer hộp trắng CRATE
được đề xuất của chúng tôi trên các tập dữ liệu và tác vụ thế giới thực. Như phân tích trong Mục 2 gợi ý, hoặc
bước nén hoặc bước thưa thớt hóa có thể được đạt được thông qua các lựa chọn thiết kế hoặc chiến lược
thay thế khác nhau. CRATE có lẽ áp dụng những lựa chọn cơ bản nhất và vì vậy mục tiêu của chúng tôi với các thí nghiệm không
đơn thuần là cạnh tranh với các transformers được thiết kế kỹ lưỡng khác trong khi sử dụng một thiết kế thô sơ như vậy.
Thay vào đó, mục tiêu của chúng tôi là hai mặt. Đầu tiên, không giống như bất kỳ mạng hộp đen được thiết kế thực nghiệm nào thường
được đánh giá chỉ trên hiệu suất end-to-end, thiết kế hộp trắng của mạng chúng tôi cho phép chúng tôi
nhìn vào bên trong kiến trúc sâu và xác minh liệu các lớp của mạng đã học thực sự thực hiện
mục tiêu thiết kế của chúng—giả sử thực hiện tối ưu hóa gia tăng cho mục tiêu (1). Thứ hai, mặc dù
đơn giản, các thí nghiệm của chúng tôi sẽ thực sự tiết lộ tiềm năng thực tế to lớn của các kiến trúc CRATE
được dẫn xuất như vậy của chúng tôi vì, như chúng tôi sẽ chỉ ra, chúng đã đạt được hiệu suất rất mạnh trên các
tập dữ liệu và tác vụ thế giới thực quy mô lớn. Trong phần còn lại của mục này chúng tôi làm nổi bật một số kết quả;
chi tiết thí nghiệm bổ sung và kết quả có thể tìm thấy trong Phụ lục B.
Kiến trúc mô hình. Chúng tôi triển khai kiến trúc được mô tả trong Mục 2.5, với những
sửa đổi nhỏ được mô tả trong Phụ lục B.1. Chúng tôi xem xét các kích thước mô hình CRATE khác nhau bằng cách
thay đổi chiều token d, số head K, và số lớp L. Chúng tôi xem xét bốn
kích thước mô hình trong công việc này: CRATE-Tiny, CRATE-Small, CRATE-Base, và CRATE-Large. Pseudocode kiểu PyTorch
có thể tìm thấy trong Phụ lục B.1, chứa chi tiết triển khai thêm. Để
huấn luyện sử dụng phân loại có giám sát, đầu tiên chúng tôi lấy token CLS zb=zL+1
1,b cho mỗi mẫu,
sau đó áp dụng một lớp tuyến tính; đầu ra của lớp tuyến tính này ub.=Wzb được sử dụng như đầu vào cho
hàm mất mát cross-entropy tiêu chuẩn. Hàm mất mát tổng thể tính trung bình trên tất cả mẫu b∈[B].
Tập dữ liệu và tối ưu hóa. Chúng tôi chủ yếu xem xét ImageNet-1K [9] như testbed cho kiến trúc của chúng tôi.
Cụ thể, chúng tôi áp dụng bộ tối ưu Lion [71] để huấn luyện các mô hình CRATE với các kích thước mô hình khác nhau.
Trong khi đó, chúng tôi cũng đánh giá hiệu suất học chuyển giao của CRATE: bằng cách xem xét các mô hình
được huấn luyện trên ImageNet-1K như các mô hình pre-trained, chúng tôi fine-tune CRATE trên một số
tập dữ liệu downstream thường được sử dụng (CIFAR10/100, Oxford Flowers, Oxford-IIT-Pets). Chi tiết thêm về
huấn luyện và tập dữ liệu có thể tìm thấy trong Phụ lục B.1.
8

--- TRANG 9 ---
2 4 6 8 10 12
Chỉ số lớp - 
60070080090010001100Rc(Z) [khối SSA]
Đo tỷ lệ mã hóa qua các lớp
train
val
2 4 6 8 10 12
Chỉ số lớp - 
0.200.250.300.350.400.450.50Độ thưa thớt [khối ISTA]
Đo độ thưa thớt đầu ra qua các lớp
train
val
Hình 3: Trái: Hạng nén Rc(Zℓ+1/2) của các đầu ra MSSA tại các lớp khác nhau. Phải: độ thưa thớt
của đầu ra khối ISTA, ∥Zℓ+1∥0/(d·N), tại các lớp khác nhau. (Mô hình: CRATE-Small).
2 4 6 8 10 12
Chỉ số lớp - 
8009001000110012001300140015001600Rc(Z) [khối SSA]
Đo tỷ lệ mã hóa qua các lớp
rand init
epoch 1
epoch 20
epoch 150
2 4 6 8 10 12
Chỉ số lớp - 
0.150.200.250.300.350.400.450.50Độ thưa thớt [khối ISTA]
Đo độ thưa thớt đầu ra qua các lớp
rand init
epoch 1
epoch 20
epoch 150
Hình 4: Hạng nén Rc(Z) (trái) và hạng thưa thớt hóa ∥Z∥0/(d·N) (phải) qua các mô hình
được huấn luyện với số epoch khác nhau. (Mô hình: CRATE-Base).
3.1 Phân Tích Theo Lớp Sâu của CRATE
Các lớp của CRATE có đạt được mục tiêu thiết kế của chúng không? Như được mô tả trong Mục 2.3 và Mục 2.4,
khối MSSA được thiết kế để tối ưu hóa hạng nén Rc(Z) và khối ISTA để thưa thớt hóa
các biểu diễn token (tương ứng với hạng thưa thớt hóa ∥Z∥0). Để hiểu liệu CRATE
thực sự có tối ưu hóa những hạng này không, cho mỗi lớp ℓ, chúng tôi đo (i) hạng nén Rc(Zℓ+1/2)
trên các đầu ra khối MSSA Zℓ+1/2; và (ii) độ thưa thớt ∥Zℓ+1∥0 trên các đầu ra khối ISTA Zℓ+1.
Cụ thể, chúng tôi đánh giá hai hạng quan trọng này bằng cách sử dụng các mẫu huấn luyện/validation từ ImageNet-1K.
Cả hai hạng đều được đánh giá ở mức mỗi mẫu và tính trung bình trên B= 103 mẫu.
Hình 3 cho thấy các biểu đồ của hai thước đo quan trọng này tại tất cả các lớp cho mô hình CRATE-small đã học.
Chúng tôi thấy rằng khi chỉ số lớp ℓ tăng, cả hạng nén và thưa thớt hóa đều cải thiện
trong hầu hết trường hợp. Sự tăng trong thước đo độ thưa thớt của lớp cuối cùng được gây ra bởi lớp tuyến tính
bổ sung cho phân loại.8 Những kết quả này cho thấy rằng CRATE phù hợp tốt với các mục tiêu thiết kế ban đầu:
một khi được học, nó về cơ bản học để nén và thưa thớt hóa các biểu diễn một cách dần dần thông qua
các lớp của nó. Ngoài ra, chúng tôi cũng đo các hạng nén và thưa thớt hóa trên các mô hình CRATE
với các kích thước mô hình khác nhau cũng như các checkpoint mô hình trung gian và kết quả được hiển thị bằng
các biểu đồ trong Hình 5 của Phụ lục B.2. Các quan sát rất nhất quán qua tất cả các kích thước mô hình khác nhau—cả
hạng nén và thưa thớt hóa đều cải thiện trong hầu hết các tình huống. Các mô hình với nhiều lớp hơn có xu hướng tối ưu hóa
các mục tiêu hiệu quả hơn, xác nhận hiểu biết của chúng tôi về vai trò của mỗi lớp.
Để xem hiệu ứng của việc học, chúng tôi trình bày các đánh giá trên CRATE-Small được huấn luyện với số
epoch khác nhau trong Hình 4. Khi mô hình không được huấn luyện đủ (ví dụ không được huấn luyện), kiến trúc không
tối ưu hóa các mục tiêu hiệu quả. Tuy nhiên, trong quá trình huấn luyện—học các không gian con Uℓ
[K]
và từ điển Dℓ tốt hơn—các khối được thiết kế bắt đầu tối ưu hóa các mục tiêu hiệu quả hơn nhiều.
Trực quan hóa các biểu diễn token theo lớp. Để hiểu rõ hơn về các biểu diễn token của CRATE, chúng tôi trực quan hóa
đầu ra của mỗi khối ISTA tại lớp ℓ trong Hình 6 của Phụ lục B.2.
Cụ thể, chúng tôi trực quan hóa Zℓ+1 thông qua các biểu đồ heatmap. Chúng tôi quan sát rằng đầu ra Zℓ+1 trở nên
thưa thớt hơn khi lớp tăng. Hơn nữa, ngoài độ thưa thớt, chúng tôi cũng thấy rằng Zℓ+1 trở nên
8Lưu ý rằng các đặc trưng thưa thớt đã học (tokens) cần được trộn trong lớp cuối cùng để dự đoán lớp.
Hiện tượng tăng trong thước đo độ thưa thớt tại lớp cuối cùng cho thấy rằng mỗi lớp đối tượng có thể được
liên kết với một số đặc trưng, và một số đặc trưng này có khả năng được chia sẻ qua các lớp khác nhau.
9

--- TRANG 10 ---
Bảng 1: Độ chính xác Top 1 của CRATE trên các tập dữ liệu khác nhau với các quy mô mô hình khác nhau khi pre-trained trên ImageNet.
Đối với ImageNet/ImageNetReaL, chúng tôi trực tiếp đánh giá độ chính xác top-1. Đối với các tập dữ liệu khác, chúng tôi sử dụng các mô hình được
pre-trained trên ImageNet như khởi tạo và đánh giá hiệu suất học chuyển giao thông qua fine-tuning.
Tập dữ liệu CRATE-T CRATE-S CRATE-B CRATE-L ViT-T ViT-S
# tham số 6.09M 13.12M 22.80M 77.64M 5.72M 22.05M
ImageNet 66.7 69.2 70.8 71.3 71.5 72.4
ImageNet ReaL 74.0 76.0 76.5 77.4 78.3 78.4
CIFAR10 95.5 96.0 96.8 97.2 96.6 97.2
CIFAR100 78.9 81.0 82.7 83.6 81.8 83.2
Oxford Flowers-102 84.6 87.1 88.7 88.3 85.1 88.5
Oxford-IIIT-Pets 81.4 84.9 85.3 87.4 88.5 88.6
có cấu trúc hơn (tức là, low-rank), điều này cho thấy rằng tập hợp các biểu diễn token trở nên gần với
các không gian con tuyến tính hơn, xác nhận hình ảnh tinh thần của chúng tôi về hình học của mỗi lớp (như trong Hình 1).
Trực quan hóa các không gian con theo lớp trong tự chú ý đa đầu. Bây giờ chúng tôi trực quan hóa các ma trận Uℓ
[K]
được sử dụng trong khối MSSA. Trong Mục 2.3, chúng tôi giả định rằng Uℓ
[K] không liên kết để nắm bắt
các "view" khác nhau của tập hợp các token. Trong Hình 7 của Phụ lục B.2, đầu tiên chúng tôi chuẩn hóa các cột
trong mỗi Uℓ
k, sau đó chúng tôi trực quan hóa [Uℓ
1, . . . ,Uℓ
K]∗[Uℓ
1, . . . ,Uℓ
K]∈RpK×pK. Khối (i, j) thứ
trong mỗi hình phụ tương ứng với (Uℓ
i)∗Uℓ
j cho i, j∈[K] tại một lớp ℓ cụ thể. Chúng tôi thấy rằng các
Uℓ
[K] đã học gần như không liên kết, phù hợp tốt với các giả định của chúng tôi. Một quan sát thú vị
là Uℓ
[K] trở nên không liên kết hơn khi chỉ số lớp ℓ lớn hơn, điều này cho thấy rằng
các biểu diễn token có thể tách biệt hơn. Điều này phản ánh tình huống trong các mạng sâu phổ biến khác [57].
3.2 Đánh Giá CRATE trên Tập Dữ Liệu và Tác Vụ Thế Giới Thực Lớn
Bây giờ chúng tôi nghiên cứu hiệu suất thực nghiệm của các mạng được đề xuất bằng cách đo độ chính xác top-1
của chúng trên ImageNet-1K cũng như hiệu suất học chuyển giao trên một số tập dữ liệu downstream được sử dụng rộng rãi.
Chúng tôi tóm tắt kết quả trong Bảng 1. Vì kiến trúc được thiết kế của chúng tôi tận dụng việc chia sẻ tham số trong
cả khối attention (MSSA) và khối MLP (ISTA), mô hình CRATE-Base của chúng tôi (22.08 triệu)
có số lượng tham số tương tự với ViT-Small (22.05 triệu).
Từ Bảng 1, chúng tôi thấy rằng với số lượng tham số mô hình tương tự, mạng được đề xuất của chúng tôi
đạt được hiệu suất ImageNet-1K và học chuyển giao tương tự như ViT, mặc dù tính đơn giản và
khả năng diễn giải của thiết kế chúng tôi. Hơn nữa, với cùng một tập hợp các siêu tham số huấn luyện, chúng tôi quan sát
hành vi mở rộng đầy hứa hẹn trong CRATE—chúng tôi liên tục cải thiện hiệu suất bằng cách mở rộng
kích thước mô hình. Để so sánh, việc mở rộng ViT trực tiếp trên ImageNet-1K không luôn dẫn đến cải thiện hiệu suất
nhất quán được đo bằng độ chính xác top-1 [40]. Tóm lại, chúng tôi đạt được hiệu suất đầy hứa hẹn
trên các tập dữ liệu thế giới thực quy mô lớn bằng cách triển khai trực tiếp kiến trúc có nguyên tắc của chúng tôi.
4 Kết Luận
Trong bài báo này, chúng tôi đề xuất một khung lý thuyết mới cho phép chúng tôi dẫn xuất các kiến trúc mạng sâu
giống transformer như các sơ đồ tối ưu hóa gia tăng để học biểu diễn nén và thưa thớt
của dữ liệu đầu vào (hoặc tập token). Các kiến trúc sâu được dẫn xuất và học như vậy không chỉ
hoàn toàn có thể diễn giải về mặt toán học, mà còn nhất quán ở mức từng lớp với mục tiêu thiết kế
của chúng. Mặc dù có thể là đơn giản nhất trong tất cả các thiết kế có thể, những mạng này đã
thể hiện hiệu suất trên các tập dữ liệu và tác vụ thế giới thực quy mô lớn gần với các transformers dày dạn kinh nghiệm.
Chúng tôi tin rằng công việc này thực sự giúp bắc cầu khoảng cách giữa lý thuyết và thực hành của mạng neural sâu
cũng như giúp thống nhất các phương pháp dường như tách biệt để học và biểu diễn phân phối dữ liệu.
Có lẽ quan trọng hơn cho các nhà thực hành, khung làm việc của chúng tôi cung cấp hướng dẫn lý thuyết để thiết kế
và biện minh các kiến trúc sâu mới, có khả năng mạnh mẽ hơn, cho việc học biểu diễn.
10

--- TRANG 11 ---
Tài Liệu Tham Khảo
[1] Charles M Stein. "Estimation of the Mean of a Multivariate Normal Distribution". The Annals
of Statistics 9.6 (Nov. 1981), pp. 1135–1151. 5.
[2] Bruno A Olshausen and David J Field. "Sparse coding with an overcomplete basis set: A
strategy employed by V1?" Vision research 37.23 (1997), pp. 3311–3325. 2.
[3] David L Donoho and Carrie Grimes. "Image Manifolds which are Isometric to Euclidean
Space". Journal of mathematical imaging and vision 23.1 (July 2005), pp. 5–24. 2.
[4] Aapo Hyvärinen. "Estimation of Non-Normalized Statistical Models by Score Matching".
Journal of machine learning research: JMLR 6.24 (2005), pp. 695–709. 5.
[5] Michael B Wakin, David L Donoho, Hyeokho Choi, and Richard G Baraniuk. "The multiscale
structure of non-differentiable image manifolds". Wavelets XI . V ol. 5914. SPIE. 2005, pp. 413–
429. 2.
[6] Yi Ma, Harm Derksen, Wei Hong, and John Wright. "Segmentation of multivariate mixed data
via lossy data coding and compression". PAMI (2007). 2, 3, 5.
[7] Maria-Elena Nilsback and Andrew Zisserman. "Automated flower classification over a large
number of classes". 2008 Sixth Indian Conference on Computer Vision, Graphics & Image
Processing . IEEE. 2008, pp. 722–729. 25.
[8] Amir Beck and Marc Teboulle. "A fast iterative shrinkage-thresholding algorithm for linear
inverse problems". SIAM journal on imaging sciences 2.1 (2009), pp. 183–202. 7.
[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. "Imagenet: A large-
scale hierarchical image database". 2009 IEEE conference on computer vision and pattern
recognition . Ieee. 2009, pp. 248–255. 8.
[10] Alex Krizhevsky, Geoffrey Hinton, et al. "Learning multiple layers of features from tiny
images" (2009). 25.
[11] Karol Gregor and Yann LeCun. "Learning fast approximations of sparse coding". Proceedings
of the 27th International Conference on International Conference on Machine Learning .
Omnipress. 2010, pp. 399–406. 2.
[12] László Györfi, Michael Kohler, Adam Krzyzak, and Harro Walk. A Distribution-Free Theory
of Nonparametric Regression . Springer New York, Dec. 2010. 4.
[13] Bradley Efron. "Tweedie's Formula and Selection Bias". Journal of the American Statistical
Association 106.496 (2011), pp. 1602–1614. 2, 5, 15.
[14] Martin Raphan and Eero P Simoncelli. "Least squares estimation without priors or supervision".
Neural computation 23.2 (Feb. 2011), pp. 374–420. 5.
[15] Pascal Vincent. "A connection between score matching and denoising autoencoders". Neural
computation 23.7 (July 2011), pp. 1661–1674. 5.
[16] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. "Cats and dogs". 2012
IEEE conference on computer vision and pattern recognition . IEEE. 2012, pp. 3498–3505. 25.
[17] Daniel A Spielman, Huan Wang, and John Wright. "Exact Recovery of Sparsely-Used Dictio-
naries" (June 2012). arXiv: 1206.5882 [cs.LG] . 2.
[18] Joan Bruna and Stéphane Mallat. "Invariant scattering convolution networks". IEEE transac-
tions on pattern analysis and machine intelligence 35.8 (Aug. 2013), pp. 1872–1886. 2.
[19] Peyman Milanfar. "A Tour of Modern Image Filtering: New Insights and Methods, Both
Practical and Theoretical". IEEE Signal Processing Magazine 30.1 (Jan. 2013), pp. 106–128.
5.
[20] Singanallur V Venkatakrishnan, Charles A Bouman, and Brendt Wohlberg. "Plug-and-Play pri-
ors for model based reconstruction". 2013 IEEE Global Conference on Signal and Information
Processing . Dec. 2013, pp. 945–948. 5.
[21] Rémi Gribonval, Rodolphe Jenatton, and Francis Bach. "Sparse and spurious: dictionary
learning with noise and outliers" (July 2014). arXiv: 1407.5155 [cs.LG] . 2.
[22] Jascha Sohl-Dickstein, Eric A Weiss, Niru Maheswaranathan, and Surya Ganguli. "Deep
Unsupervised Learning using Nonequilibrium Thermodynamics" (Mar. 2015). arXiv: 1503.
03585 [cs.LG] . 2, 5.
[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. "Deep Residual Learning for
Image Recognition". 2016 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR) . June 2016, pp. 770–778. 1.
11

--- TRANG 12 ---
[24] René Vidal, Yi Ma, and Shankar Sastry. Generalized Principal Component Analysis . Springer
Verlag, 2016. 5.
[25] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. "Mask R-CNN" (Mar. 2017).
arXiv: 1703.06870 [cs.CV] . 1.
[26] Ilya Loshchilov and Frank Hutter. "Decoupled weight decay regularization". arXiv preprint
arXiv:1711.05101 (2017). 25.
[27] Yaniv Romano, Michael Elad, and Peyman Milanfar. "The Little Engine That Could: Regular-
ization by Denoising (RED)". SIAM journal on imaging sciences 10.4 (Jan. 2017), pp. 1804–
1844. 5.
[28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. "Attention is all you need". Advances in neural informa-
tion processing systems 30 (2017). 1, 3, 6.
[29] Yubei Chen, Dylan Paiton, and Bruno Olshausen. "The sparse manifold transform". Advances
in neural information processing systems 31 (2018). 2.
[30] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. "Bert: Pre-training of
deep bidirectional transformers for language understanding". arXiv preprint arXiv:1810.04805
(2018). 1.
[31] Tero Karras, Samuli Laine, and Timo Aila. "A Style-Based Generator Architecture for Genera-
tive Adversarial Networks" (Dec. 2018). arXiv: 1812.04948 [cs.NE] . 1.
[32] Vardan Papyan, Yaniv Romano, Jeremias Sulam, and Michael Elad. "Theoretical Foundations
of Deep Learning via Sparse Representations: A Multilayer Sparse Model and Its Connection
to Convolutional Neural Networks". IEEE Signal Processing Magazine 35.4 (July 2018),
pp. 72–89. 2.
[33] Xiaoxia Sun, Nasser M Nasrabadi, and Trac D Tran. "Supervised deep sparse coding networks".
2018 25th IEEE International Conference on Image Processing (ICIP) . IEEE. 2018, pp. 346–
350. 7.
[34] Yang Song and Stefano Ermon. "Generative Modeling by Estimating Gradients of the Data
Distribution" (July 2019). arXiv: 1907.05600 [cs.LG] . 2.
[35] John Zarka, Louis Thiry, Tomás Angles, and Stéphane Mallat. "Deep network classification
by scattering and homotopy dictionary learning". arXiv preprint arXiv:1910.03561 (2019). 7.
[36] Lucas Beyer, Olivier J Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord.
"Are we done with imagenet?" arXiv preprint arXiv:2006.07159 (2020). 25.
[37] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. "Language
models are few-shot learners". Advances in neural information processing systems 33 (2020),
pp. 1877–1901. 1.
[38] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and
Sergey Zagoruyko. "End-to-End Object Detection with Transformers" (May 2020). arXiv:
2005.12872 [cs.CV] . 1.
[39] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. "A Simple Frame-
work for Contrastive Learning of Visual Representations". Proceedings of the 37th Interna-
tional Conference on Machine Learning . Ed. by Hal Daumé Iii and Aarti Singh. V ol. 119.
Proceedings of Machine Learning Research. PMLR, 2020, pp. 1597–1607. 1.
[40] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
et al. "An image is worth 16x16 words: Transformers for image recognition at scale". arXiv
preprint arXiv:2010.11929 (2020). 1, 3, 10.
[41] Jonathan Ho, Ajay Jain, and Pieter Abbeel. "Denoising diffusion probabilistic models". Ad-
vances in Neural Information Processing Systems 33 (2020), pp. 6840–6851. 2.
[42] Zahra Kadkhodaie and Eero P Simoncelli. "Solving Linear Inverse Problems Using the Prior
Implicit in a Denoiser" (July 2020). arXiv: 2007.13640 [cs.CV] . 5.
[43] Jiaming Song, Chenlin Meng, and Stefano Ermon. "Denoising Diffusion Implicit Models"
(Oct. 2020). arXiv: 2010.02502 [cs.LG] . 2, 5.
[44] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon,
and Ben Poole. "Score-Based Generative Modeling through Stochastic Differential Equations"
(Nov. 2020). arXiv: 2011.13456 [cs.LG] . 2, 5.
12

--- TRANG 13 ---
[45] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola.
"What makes for good views for contrastive learning?" Advances in neural information pro-
cessing systems 33 (2020), pp. 6827–6839. 2.
[46] Yaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. "Learning Diverse
and Discriminative Representations via the Principle of Maximal Coding Rate Reduction".
Advances in Neural Information Processing Systems 33 (2020), pp. 9422–9434. 2, 3, 5, 18, 23.
[47] Yuexiang Zhai, Zitong Yang, Zhenyu Liao, John Wright, and Yi Ma. "Complete dictionary
learning via l 4-norm maximization over the orthogonal group". The Journal of Machine
Learning Research 21.1 (2020), pp. 6622–6689. 2.
[48] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu ˇci´c, and Cordelia
Schmid. "Vivit: A video vision transformer". Proceedings of the IEEE/CVF international
conference on computer vision . 2021, pp. 6836–6846. 1.
[49] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. "Masked
Autoencoders Are Scalable Vision Learners" (Nov. 2021). arXiv: 2111.06377 [cs.CV] . 1.
[50] Geoffrey Hinton. How to represent part-whole hierarchies in a neural network . 2021. arXiv:
2102.12627 [cs.CV] . 6.
[51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya
Sutskever. "Learning Transferable Visual Models From Natural Language Supervision". Pro-
ceedings of the 38th International Conference on Machine Learning . Ed. by Marina Meila and
Tong Zhang. V ol. 139. Proceedings of Machine Learning Research. PMLR, 2021, pp. 8748–
8763. 1.
[52] Bahareh Tolooshams and Demba Ba. "Stable and Interpretable Unrolled Dictionary Learning".
arXiv preprint arXiv:2106.00058 (2021). 2.
[53] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas
Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic,
and Alexey Dosovitskiy. "MLP-Mixer: An all-MLP Architecture for Vision" (May 2021).
arXiv: 2105.01601 [cs.CV] . 22.
[54] Kwan Ho Ryan Chan, Yaodong Yu, Chong You, Haozhi Qi, John Wright, and Yi Ma. "ReduNet:
A White-box Deep Network from the Principle of Maximizing Rate Reduction". Journal of
Machine Learning Research 23.114 (2022), pp. 1–103. 2–8, 18, 19.
[55] Hongrui Chen, Holden Lee, and Jianfeng Lu. "Improved Analysis of Score-based Generative
Modeling: User-Friendly Bounds under Minimal Smoothness Assumptions". arXiv preprint
arXiv:2211.01916 (2022). 2.
[56] Yuan Gong, Andrew Rouditchenko, Alexander H Liu, David Harwath, Leonid Karlinsky, Hilde
Kuehne, and James R Glass. "Contrastive audio-visual masked autoencoder". The Eleventh
International Conference on Learning Representations . 2022. 1.
[57] Hangfeng He and Weijie J Su. "A law of data separation in deep learning". arXiv preprint
arXiv:2210.17020 (2022). 10.
[58] Geoffrey Hinton. The Forward-Forward Algorithm: Some Preliminary Investigations . 2022.
arXiv: 2212.13345 [cs.LG] . 2.
[59] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. "Elucidating the design space of
diffusion-based generative models". arXiv preprint arXiv:2206.00364 (2022). 2, 15.
[60] Frederic Koehler, Alexander Heckett, and Andrej Risteski. "Statistical Efficiency of Score
Matching: The View from Isoperimetry" (Oct. 2022). arXiv: 2210.00726 [cs.LG] . 2.
[61] Yi Ma, Doris Tsao, and Heung-Yeung Shum. "On the principles of parsimony and self-
consistency for the emergence of intelligence". Frontiers of Information Technology & Elec-
tronic Engineering 23.9 (2022), pp. 1298–1323. 1, 3.
[62] Druv Pai, Michael Psenka, Chih-Yuan Chiu, Manxi Wu, Edgar Dobriban, and Yi Ma. "Pursuit
of a discriminative representation for multiple subspaces via sequential games". arXiv preprint
arXiv:2206.09120 (2022). 2.
[63] Mary Phuong and Marcus Hutter. "Formal algorithms for transformers". arXiv preprint
arXiv:2207.09238 (2022). 20.
[64] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
"High-resolution image synthesis with latent diffusion models". Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition . 2022, pp. 10684–10695. 1, 2.
13

--- TRANG 14 ---
[65] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed
Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, Tim
Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. "Photorealistic Text-to-Image
Diffusion Models with Deep Language Understanding" (May 2022). arXiv: 2205.11487
[cs.CV] . 1.
[66] Asher Trockman, Devin Willmott, and J Zico Kolter. "Understanding the Covariance Structure
of Convolutional Filters" (Oct. 2022). arXiv: 2210.03651 [cs.CV] . 22.
[67] Rene Vidal. Attention: Self-Expression Is All You Need . Unpublished; available: https :
//openreview.net/forum?id=MmujBClawFo . 2022. 2.
[68] Haoqing Wang, Xun Guo, Zhi-Hong Deng, and Yan Lu. "Rethinking minimal sufficient
representation in contrastive learning". Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition . 2022, pp. 16041–16050. 2.
[69] John Wright and Yi Ma. High-Dimensional Data Analysis with Low-Dimensional Models:
Principles, Computation, and Applications . Cambridge University Press, 2022. 4, 21, 22.
[70] Sitan Chen, Giannis Daras, and Alexandros G Dimakis. "Restoration-Degradation Beyond
Linear Diffusions: A Non-Asymptotic Analysis For DDIM-Type Samplers" (Mar. 2023). arXiv:
2303.03384 [cs.LG] . 5.
[71] Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham,
Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, et al. "Symbolic discovery of optimization
algorithms". arXiv preprint arXiv:2302.06675 (2023). 8, 25.
[72] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin
Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al.
"Scaling vision transformers to 22 billion parameters". arXiv preprint arXiv:2302.05442
(2023). 1.
[73] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,
Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, Piotr Dollár, and Ross
Girshick. "Segment Anything" (Apr. 2023). arXiv: 2304.02643 [cs.CV] . 1.
[74] Hongkang Li, Meng Wang, Sijia Liu, and Pin-Yu Chen. "A Theoretical Understanding of
shallow Vision Transformers: Learning, Generalization, and Sample Complexity". arXiv
preprint arXiv:2302.06015 (2023). 2.
[75] Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li, Ankit Singh Rawat, Sashank J Reddi,
Ke Ye, Felix Chern, Felix Yu, Ruiqi Guo, and Sanjiv Kumar. "The Lazy Neuron Phenomenon:
On Emergence of Activation Sparsity in Transformers". The Eleventh International Conference
on Learning Representations . 2023. 22.
[76] Ravid Shwartz-Ziv and Yann LeCun. "To Compress or Not to Compress–Self-Supervised
Learning and Information Theory: A Review". arXiv preprint arXiv:2304.09355 (2023). 2.
[77] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. "Consistency models". arXiv
preprint arXiv:2303.01469 (2023). 2.
14

--- TRANG 15 ---
Phụ Lục
A Chi Tiết Kỹ Thuật từ Mục 2
A.1 Bổ Sung cho Mục 2.2
Đầu tiên chúng tôi muốn nhắc lại các đóng góp cốt lõi của phương pháp chúng tôi trong Mục 2.2 ở mức kỹ thuật
hơi cao hơn. Các kết nối giữa khử nhiễu và score matching được hiểu rõ [59], và
tính toán hàm khử nhiễu tối ưu (tức là, kỳ vọng có điều kiện) chống lại một mô hình mixture-of-
Gaussians là một tính toán khá đơn giản cho các công cụ hiện có như công thức Tweedie [13].
Những điều này không phải là đóng góp chính của chúng tôi. Thay vào đó, các đóng góp chính của Mục 2.2 có hai mặt:
•Đầu tiên, chúng tôi thể hiện một cơ chế để học biểu diễn thông qua khử nhiễu trong một mô hình dữ liệu
hỗn hợp Gaussian lý tưởng hóa cho một token đơn (tức là, với độ dài chuỗi N= 1).
•Thứ hai, chúng tôi minh họa những điểm tương đồng giữa một sơ đồ học biểu diễn được dẫn xuất như vậy
và các lớp tự chú ý hiện có trong transformer (với độ dài chuỗi 1), do đó
thể hiện một diễn giải của lớp tự chú ý như một cơ chế tổng quát để
khử nhiễu chống lại một mô hình mixture-of-Gaussian-marginal cho một tập hợp các token.
Bây giờ chúng tôi tạo ra các chứng minh được ám chỉ trong Mục 2.2, chủ yếu tạo thành các khía cạnh kỹ thuật của
đóng góp đầu tiên được liệt kê. Để đơn giản hóa các chứng minh, chúng tôi sử dụng các tương ứng ký hiệu sau:
x7→zℓ,z7→zℓ+1, và σ7→σℓ.
Mệnh đề 1. Cho u1, . . . ,uK∈Rd độc lập và có phân phối uk∼ N(0,Σk) cho
Σk⪰0, và cho z nhận giá trị uk với xác suất πk>0. Cho w∼ N(0,Id) độc lập với z.
Cho x.=z+σw. Cho x7→q(x) là mật độ của x. Chúng tôi định nghĩa
Mk.= (Σk+σ2Id)−1/2(19)
và giả định rằng πidet(Mi) =πjdet(Mj) cho tất cả 1≤i≤j≤K. Sau đó chúng ta có
∇xlogq(x) (20)
=−[M1,···,MK]
diag
softmax
−1
2
∥M∗
1x∥2
2...
∥M∗
Kx∥2
2


⊗Id

M∗
1x
...
M∗
Kx
, (21)
nơi ⊗ ký hiệu tích Kronecker, tức là, ma trận khối được định nghĩa bởi
A⊗B=
A11B··· A1nB
.........
Am1B··· AmnB
 (22)
Chứng minh. Cho u là biến ngẫu nhiên đa thức sao cho z=zu, vì vậy u có hàm khối lượng xác suất
π. Sau đó bởi quy luật xác suất toàn phần, chúng ta có
∇xlogq(x) =∇xlogKX
k=1q(x|k)πk (23)
=PK
k=1πk∇xq(x|k)
PK
k=1q(x|k)πk(24)
nơi q(x|k) là mật độ có điều kiện của x cho sự kiện {u=k}. Để tính toán lượng này,
lưu ý rằng có điều kiện trên giá trị của u, chúng ta có
x=zu+σw∼ N(0,Σu+σ2Id). (25)
Do đó chúng ta có
q(x|k) =1p
(2π)ddet(Σk+σ2Id)exp
−1
2x∗(Σk+σ2Id)−1x
, (26)
Điều này cho
∇xq(x|k) =−q(x|k)·(Σk+σ2Id)−1x. (27)
Đưa tất cả này lại với nhau, chúng ta có
∇xlogq(x) (28)
=−PK
k=1q(x|k)πk·(Σk+σ2Id)−1x
PK
k=1q(x|k)πk(29)
=−PK
k=1πkdet(Σk+σ2Id)−1/2exp
−1
2x∗(Σk+σ2Id)−1x
·(Σk+σ2Id)−1x
PK
k=1πkdet(Σk+σ2Id)−1/2exp
−1
2x∗(Σk+σ2Id)−1x .(30)
Bây giờ định nghĩa Mk.= (Σk+σ2Id)−1/2. Với ký hiệu này, chúng ta có
∇xlogq(x) =−PK
k=1πkdet(Mk) exp
−1
2x∗MkM∗
kx
·MkM∗
kx
PK
k=1πkdet(Mk) exp
−1
2x∗MkM∗
kx (31)
=−PK
k=1πkdet(Mk) exp
−1
2∥M∗
kx∥2
2
·MkM∗
kx
PK
k=1πkdet(Mk) exp
−1
2x∗MkM∗
kx . (32)
Cho giả định của chúng ta rằng mỗi πkdet(Mk) là giống nhau, chúng ta có
∇xlogq(x) (33)
=−PK
k=1πkdet(Mk) exp
−1
2∥M∗
kx∥2
2
·MkM∗
kx
PK
k=1πkdet(Mk) exp
−1
2∥M∗
kx∥2
2 (34)
=−PK
k=1exp
−1
2∥M∗
kx∥2
2
·MkM∗
kx
PK
k=1exp
−1
2∥M∗
kx∥2
2 (35)
=−KX
k=1e∗
ksoftmax
−1
2
∥M∗
1x∥2
2...
∥M∗
Kx∥2
2

MkM∗
kx (36)
=−[M1, . . . ,MK]
diag
softmax
−1
2
∥M∗
1x∥2
2...
∥M∗
Kx∥2
2


⊗Id

M∗
1x
...
M∗
Kx
. (37)
Bây giờ chúng tôi cung cấp một biện minh cuối cùng cho kết quả được trích dẫn trong Mục 2.2.
Xấp xỉ 2. Trong thiết lập của Mệnh đề 1, chéo hóa Σk=UkΛkU∗
k nơi Uk∈Rd×p
là trực giao và Λk≻0∈Rp×p là đường chéo.9 Sau đó chúng ta có xấp xỉ
E[z|x]≈[U1, . . . ,UK]
diag
softmax
1
2σ2
∥U∗
1x∥2
2...
∥U∗
Kx∥2
2


⊗Ip

U∗
1x
...
U∗
Kx
. (38)
Chứng minh. Chúng ta có
∇xlogq(x) =−KX
k=1e∗
ksoftmax
−1
2
∥M∗
1x∥2
2...
∥M∗
Kx∥2
2

MkM∗
kx (39)
=−KX
k=1e∗
ksoftmax
−1
2σ2
∥σM∗
1x∥2
2...
∥σM∗
Kx∥2
2

MkM∗
kx (40)
9Giả định này có thể dễ dàng được nới lỏng thành Λk⪰0 cho tất cả k, nhưng yêu cầu thêm ký hiệu để xử lý, và
dạng của nghiệm không thay đổi. Do đó chúng tôi xử lý trường hợp nơi tất cả ma trận đều full rank để đơn giản.
16

--- TRANG 16 ---
=−KX
k=1e∗
ksoftmax
1
2σ2
∥x∥2
2− ∥σM∗
1x∥2
2...
∥x∥2
2− ∥σM∗
Kx∥2
2

MkM∗
kx. (41)
Bây giờ định nghĩa Pk.=Id−σMk, và cho U⊥
k∈Rd×(d−p) là một bổ trực giao của Uk. Sau đó
chúng ta có
Pk=Id−σMk (42)
=Id−σ
Σk+σ2Id−1/2(43)
=Id−σ
UkU⊥
k
Λk0
0 0U∗
k
(U⊥
k)∗
+σ2Id−1/2
(44)
=Id−σ
UkU⊥
k
Λk+σ2Ip 0
0 σ2Id−pU∗
k
(U⊥
k)∗−1/2
(45)
=Id−
UkU⊥
k
σ(Λk+σ2Ip)−1/20
0 σ·(σ2)−1/2Id−pU∗
k
(U⊥
k)∗
(46)
=Id−
UkU⊥
k
(σ−2Λk+Ip)−1/20
0 Id−pU∗
k
(U⊥
k)∗
(47)
=
UkU⊥
k
Ip−(σ−2Λk+Ip)−1/20
0 0U∗
k
(U⊥
k)∗
(48)
≈
UkU⊥
k
Ip0
0 0U∗
k
(U⊥
k)∗
(49)
=UkU∗
k. (50)
Do đó Pk gần như là một phép chiếu khi σ nhỏ. Dưới mối quan hệ đại số này, chúng ta có
∇xlogq(x) (51)
=−KX
k=1e∗
ksoftmax
1
2σ2
∥x∥2
2− ∥σM∗
1x∥2
2...
∥x∥2
2− ∥σM∗
Kx∥2
2

MkM∗
kx (52)
=−1
σ2KX
k=1e∗
ksoftmax
1
2σ2
∥x∥2
2− ∥(Id−P1)∗x∥2
2...
∥x∥2
2− ∥(Id−PK)∗x∥2
2

(Id−Pk)(Id−Pk)∗x (53)
≈ −1
σ2KX
k=1e∗
ksoftmax
1
2σ2
∥P∗
1x∥2
2...
∥P∗
Kx∥2
2

(Id−Pk)(Id−Pk)∗x (54)
≈ −1
σ2KX
k=1e∗
ksoftmax
1
2σ2
∥P∗
1x∥2
2...
∥P∗
Kx∥2
2

(Id−Pk)∗x (55)
=−x
σ2KX
k=1e∗
ksoftmax
1
2σ2
∥P∗
1x∥2
2...
∥P∗
Kx∥2
2

+1
σ2KX
k=1e∗
ksoftmax
1
2σ2
∥P∗
1x∥2
2...
∥P∗
Kx∥2
2

P∗
kx
(56)
=−1
σ2x+1
σ2KX
k=1e∗
ksoftmax
1
2σ2
∥P∗
1x∥2
2...
∥P∗
Kx∥2
2

P∗
kx (57)
≈ −1
σ2x+1
σ2KX
k=1e∗
ksoftmax
1
2σ2
∥U∗
1x∥2
2...
∥U∗
Kx∥2
2

UkU∗
kx (58)
17

--- TRANG 17 ---
=−1
σ2x+1
σ2[U1,···,UK]
diag
softmax
1
2σ2
∥U∗
1x∥2
2...
∥U∗
Kx∥2
2


⊗Ip

U∗
1x
...
U∗
Kx
.(59)
Cắm điều này vào công thức Tweedie, chúng ta có
E[z|x]≈[U1,···,UK]
diag
softmax
1
2σ2
∥U∗
1x∥2
2...
∥U∗
Kx∥2
2


⊗Ip

U∗
1x
...
U∗
Kx
. (60)
Nhận xét 3. Mặc dù Xấp xỉ 2 được phát biểu như một xấp xỉ thay vì như một mệnh đề, chúng tôi tin rằng nó có thể được chuyển đổi thành một phát biểu về tương đương tiệm cận như σ→0 mà không cần quá nhiều công việc bổ sung (đặc biệt, đúng cho σ dưới giá trị eigen nhỏ nhất (khác không) của bất kỳ Σk nào. Hầu hết các xấp xỉ được thực hiện trong việc dẫn xuất Xấp xỉ 2 có thể ngay lập tức được chuyển thành các khẳng định tiệm cận; điểm hơi tinh tế duy nhất là xử lý softmax, có thể được thực hiện bằng cách sử dụng hành vi hội tụ "nhiệt độ cao" tiêu chuẩn của hàm softmax (đặc biệt, khi σ→0 trong các biểu thức của chúng ta, softmax tập trung vào "head tốt nhất").
A.2 Bổ Sung cho Mục 2.3
Chúng tôi một lần nữa muốn nhắc lại đóng góp cốt lõi của phương pháp chúng tôi trong Mục 2.3. Việc áp dụng góc độ nén cho việc học biểu diễn đã được thảo luận trước đây, ví dụ trong dòng công việc giảm tỷ lệ mã hóa tối đa [46]. Trong Mục 2.3, chúng tôi cung cấp các đóng góp và phát triển sau cho góc độ này:
•Chúng tôi đề xuất một hàm tỷ lệ mã hóa tổng quát Rc(·;U[K]) đo tỷ lệ mã hóa đối với một tập các không gian con U[K] thay vì một tập các lớp (như trong [46, 54]), làm cho công thức cơ bản không giám sát.
•Sau đó chúng tôi chỉ ra cách nếu chúng ta áp dụng khung làm việc của tối thiểu hóa luân phiên của mục tiêu giảm tỷ lệ thưa thớt, thì việc mở rộng bước luân phiên đầu tiên — gradient descent trên mục tiêu tỷ lệ mã hóa này — gần như chính xác khôi phục cơ chế tự chú ý đa đầu thông thường được tìm thấy trong các mạng transformer (ngoại trừ các toán tử query/key/value đều là cùng một phép toán U∗
k bây giờ, mà chúng tôi diễn giải như phép chiếu lên một không gian con duy nhất).
Trong quá trình của đóng góp thứ hai, và trong các chứng minh sau, chúng tôi thực hiện một số xấp xỉ đơn giản và giả định kỹ thuật. Tính hợp lệ của những giả định này có thể được khám phá, và các xấp xỉ được tinh chỉnh, tất cả cung cấp một toán tử giống tự chú ý phức tạp hơn (và có thể hiệu suất hơn) kết quả. Vì lợi ích của sự rõ ràng kỹ thuật và đơn giản trong công việc này, chúng tôi thực hiện có lẽ những lựa chọn đơn giản nhất có thể. Kết quả là, chúng tôi không khẳng định rằng mạng của chúng tôi được thiết kế tối ưu, mà là các nguyên tắc chúng tôi phát triển trong công việc này (nén, khử nhiễu, thưa thớt hóa, tối ưu hóa mở rộng) có thể cung cấp xương sống cho các kiến trúc mạng vượt trội và có thể diễn giải hơn nhiều trong tương lai trên các tác vụ khác nhau. Như vậy, với thiết kế đơn giản, đơn giản và có thể diễn giải của chúng tôi, chúng tôi vẫn có được kết quả khái niệm có ý nghĩa và hiệu suất thực nghiệm rất vững chắc.
Bây giờ chúng tôi đưa ra việc dẫn xuất của xấp xỉ được ám chỉ trong Mục 2.3.
Xấp xỉ 4. Cho Z∈Rd×N có các cột chuẩn đơn vị, và U[K]= (U1, . . . ,UK) sao cho mỗi Uk∈Rd×p là một ma trận trực giao, các (Uk)K
k=1 không liên kết, và các cột của Z gần như nằm trên SK
k=1Span(Uk). Cho γ=p
Nϵ2. Cho κ >0. Sau đó
Z−κ∇ZRc(Z|U[K])≈(1−κγ)Z+κγMSSA(Z|U[K]), (61)
nơi như trong Mục 2.3 chúng ta có
SSA(Z|Uk) = (U∗
kZ) softmax(( U∗
kZ)∗(U∗
kZ)), (62)
MSSA(Z|U[K]) =γ[U1, . . . ,UK]
SSA(Z|U1)
...
SSA(Z|UK)
, (63)
nơi softmax( ·) là toán tử softmax (được áp dụng cho mỗi cột của một ma trận đầu vào), tức là,
softmax( v) =1Pn
i=1evi
ev1
...
evn
, (64)
softmax([ v1, . . . ,vK]) = [ softmax( v1), . . . , softmax( vK)]. (65)
Chứng minh. Theo (9), gradient ∇ZRc(Z;U[K]) là
∇ZRc(Z;U[K]) =γKX
k=1UkU∗
kZ(I+γ(U∗
kZ)∗(U∗
kZ))−1. (66)
Lưu ý rằng theo [54], gradient chính xác là phần dư của một hồi quy ridge cho mỗi
token được chiếu (U∗
kzi) sử dụng các token được chiếu khác U∗
kzj như các regressor, do đó là phần dư
của một auto-regression.
Tuy nhiên, như chúng ta đã thấy trong công việc của ReduNet [54], việc tính toán nghịch đảo
(I+γ(U∗
kZ)∗(U∗
kZ))−1 có thể tốn kém. Do đó để hiệu quả tính toán, chúng ta có thể xấp xỉ
nó với hạng bậc nhất của khai triển von Neumann của nó:
∇ZRc(Z;U[K]) =γKX
k=1UkU∗
kZ
I+γ(U∗
kZ)∗(U∗
kZ)−1
(67)
≈γKX
k=1UkU∗
kZ
I−γ(U∗
kZ)∗(U∗
kZ)
(68)
=γKX
k=1Uk
U∗
kZ−γU∗
kZ[(U∗
kZ)∗(U∗
kZ)]
(69)
Lưu ý rằng hạng (U∗
kZ)∗(U∗
kZ) là tự tương quan giữa các token được chiếu. Vì các token Z có thể từ các không gian con khác nhau, chúng ta sẽ thích sử dụng chỉ các token thuộc cùng một không gian con để hồi quy và nén chính chúng. Do đó chúng ta có thể chuyển đổi hạng tương quan trên thành một chỉ thị thành viên không gian con với một phép toán softmax, từ đó (69) trở thành
∇ZRc(Z;U[K])≈γKX
k=1Uk
U∗
kZ−γU∗
kZ[(U∗
kZ)∗(U∗
kZ)]
(70)
≈γKX
k=1UkU∗
kZ−γ2KX
k=1Uk
U∗
kZsoftmax(( U∗
kZ)∗(U∗
kZ))
(71)
Sau đó, chúng ta có thể viết lại xấp xỉ trên cho gradient của Rc như:
∇ZRc(Z;U[K])≈γKX
k=1UkU∗
kZ−γ2KX
k=1Uk(U∗
kZsoftmax(( U∗
kZ)∗(U∗
kZ))) (72)
=γKX
k=1UkU∗
kZ−γ2KX
k=1UkSSA(Z|Uk) (73)
= 
γKX
k=1UkU∗
k!
Z
| {z }
≈γZ−γ2[U1,···,UK]
SSA(Z|U1)
...
SSA(Z|UK)
 (74)
≈γZ−γ2[U1,···,UK]
SSA(Z|U1)
...
SSA(Z|UK)
. (75)
18

--- TRANG 18 ---
Do đó bước gradient descent với tỷ lệ học κ >0 cho
Z−κ∇ZRc(Z|U[K])≈(1−κγ)Z+κγ2[U1, . . . ,UK]
SSA(Z|U1)
...
SSA(Z|UK)
. (76)
A.3 Bổ Sung cho Mục 2.4
Chúng tôi một lần nữa muốn nhắc lại đóng góp cốt lõi của phương pháp chúng tôi trong Mục 2.4.
•Trong khung làm việc của tối thiểu hóa luân phiên của mục tiêu giảm tỷ lệ thưa thớt, chúng tôi chỉ ra rằng bước luân phiên thứ hai — gradient descent trên tỷ lệ mã hóa tổng thể cộng với một hạng chính quy thưa thớt — có các kết nối heuristic với một tối ưu hóa LASSO cụ thể.
•Chúng tôi chỉ ra rằng việc mở rộng của bước gradient proximal để giải quyết tối ưu hóa LASSO này giống với MLP ngay sau lớp tự chú ý trong các khối transformer.
Trong văn bản chính, kết nối của chúng tôi giữa bước thứ hai của tối thiểu hóa luân phiên và tối ưu hóa LASSO là mức độ cao và heuristic. Theo một nghĩa nào đó, lựa chọn đặt ra bước tối thiểu hóa như một LASSO là một lựa chọn đơn giản, đáng tin cậy và có thể diễn giải hoạt động tốt trong thực tế, nhưng vẫn không được hỗ trợ bởi biện minh lý thuyết nghiêm ngặt. Trong mục tiếp theo, chúng tôi cung cấp một biện minh toán học cho việc tái công thức hóa bước tối thiểu hóa sử dụng một khung làm việc majorization-minimization. Chúng tôi tiếp tục chỉ ra rằng bước tối ưu hóa mở rộng liên quan mang một sự giống nhau mạnh mẽ với bước ISTA. Điều này xác nhận cuộc thảo luận trước đó của chúng tôi — chúng tôi đã chọn lựa chọn đơn giản nhất có thể trong việc thiết kế CRATE, nhưng bằng việc dẫn xuất nghiêm ngặt hơn chúng ta có thể khám phá các toán tử thay thế vẫn có cùng chức năng khái niệm và có thể hoạt động tốt hơn trong thực tế.
Giả định. Trong mục này, chúng tôi trình bày một phân tích tối ưu hóa nghiêm ngặt của một phương pháp tối thiểu hóa gia tăng cho mục tiêu (13). Chúng tôi sẽ chỉ ra rằng dưới hai giả định đơn giản hóa, cụ thể là
1. Các cột của Zℓ+1/2 được chuẩn hóa, theo nghĩa rằng diag((Zℓ+1/2)∗Zℓ+1/2) =1;10
2. Chúng ta có d≥N,11 và các cột của Zℓ+1/2 trực giao, sao cho (Zℓ+1/2)∗Zℓ+1/2=
I.12
phương pháp dẫn đến một lần lặp cập nhật bằng với một phiên bản đơn giản hóa hơi của khối ISTA (17). Chúng tôi xem điều này như một biện minh cho việc dẫn xuất của chúng tôi trong Mục 2.4, đã có được khối ISTA bằng cách giới thiệu một giả định đơn giản hóa bổ sung về phân phối của dữ liệu tại lớp ℓ.
Phân tích. Theo (16), chúng tôi sẽ xem xét việc nới lỏng tự nhiên của "chuẩn" ℓ0 thành chuẩn ℓ1, và kết hợp một ràng buộc không âm. Xem xét mục tiêu
φ(Z) =λ∥Z∥1+χ{Z≥0}(Z)−1
2log det ( I+αZ∗Z)
| {z }
R(Z), (77)
nơi Z∈Rd×N và α=d/Nε2, và χ{Z≥0} ký hiệu hàm đặc trưng cho tập hợp các ma trận Z không âm theo từng phần tử. Như trong Phụ lục A.2, chúng ta tính toán
∇ZR(Z) =αZ(I+αZ∗Z)−1. (78)
10Đây là một giả định tự nhiên trong các kiến trúc kiểu transformer như CRATE do việc sử dụng các khối LayerNorm—mặc dù những khối này (thực sự, như chúng tôi sử dụng chúng trong CRATE) bao gồm các offset trung bình và tỷ lệ có thể huấn luyện cũng như một phép toán trừ trung bình bổ sung [63], chúng được khởi tạo để có trung bình không và chuẩn đơn vị, do đó giả định này tương ứng với một phân tích của mạng tại khởi tạo của nó.
11Giả định này không mất tính tổng quát, như chúng ta sẽ thấy trong phân tích dưới đây. Lý do là Z∗Z và Z∗Z có cùng các giá trị eigen khác không bất kể hình dạng của Z, điều này có nghĩa rằng log det( I+ αZ∗Z) = log det( I+αZZ∗). Đặc biệt, diễn giải các chuẩn một cách thích hợp (với một sự lạm dụng ký hiệu nhỏ), chúng ta có φ(Z) =φ(Z∗), vì vậy với mục đích phân tích chúng ta luôn có thể tiến hành như thể Z là một ma trận cao (miễn là chúng ta không sử dụng bất kỳ tính chất đặc biệt nào của α trong việc dẫn xuất của chúng ta).
12Giả định này nghiêm ngặt hơn giả định trước đó, và nghiêm ngặt hơn một giả định về tính không liên kết trên các cột. Nó tương ứng với biểu diễn Zℓ+1/2 không sụp đổ, mà chúng ta mong đợi giữ tại khởi tạo do các phép chiếu U[K] là ngẫu nhiên.
20

--- TRANG 19 ---
Chúng tôi xem xét một sơ đồ tối ưu hóa gia tăng cho mục tiêu φ phi tuyến cao và không lồi.
Theo Mục 2.3, chúng tôi tối ưu hóa cục bộ tại một lần lặp "sau nén" Zℓ+1/2. Chúng tôi theo
khung làm việc majorize-minimize proximal tiêu chuẩn [69] cho tối ưu hóa gia tăng/cục bộ: điều này bắt đầu
với khai triển Taylor bậc hai cho phần trơn của φ trong một lân cận của lần lặp hiện tại Zℓ+1/2:
R(Z) =R(Zℓ+1/2) +D
∇ZR(Zℓ+1/2),Z−Zℓ+1/2E
+Z1
0(1−t)D
Z−Zℓ+1/2,∇2R(Zt)
Z−Zℓ+1/2E
dt,(79)
nơi với bất kỳ Z∈Rd×N nào, Zt=tZℓ+1/2+ (1−t)Z. Phương pháp majorization-minimization
proximal luân phiên hai bước để tối thiểu hóa φ:
1. Đầu tiên, sử dụng các giả định về Zℓ+1/2 để dẫn xuất một cận trên cho chuẩn toán tử của
Hessian ∇2R(Z) trên miền hiệu lực của bài toán tối ưu hóa. Chúng tôi sẽ viết L
cho cận trên (thống nhất) này. Điều này tạo ra một cận trên bậc hai cho phần trơn của
mục tiêu φ.
2. Sau đó, luân phiên tối thiểu hóa phần trơn của cận trên bậc hai như một hàm của Z,
và thực hiện một bước proximal trên phần không trơn. Có thể chỉ ra [69] rằng tương ứng với
lần lặp
Z+= prox λ
L(∥ · ∥1+χ{Z≥0})
Z+1
L∇ZR(Z)
(80)
Trong thiết lập tối thiểu hóa luân phiên của bài báo này để tối ưu hóa (1), chúng tôi chỉ thực hiện một
bước như vậy, bắt đầu tại Zℓ+1/2.
Chúng tôi sẽ khởi tạo chương trình này dưới đây, chỉ ra các cận lỗi định lượng liên quan đến các giả định
của chúng tôi ở trên khi cần thiết. Thay vì áp dụng trực tiếp lần lặp (80), chúng tôi sẽ dẫn xuất nó dưới đây dưới các
giả định đã nêu của chúng tôi.
Bắt đầu tại (79), nhiệm vụ đầu tiên của chúng tôi là ước lượng cận trên cho phần dư bậc hai. Điều này tương ứng với việc ước tính
D
Z−Zℓ+1/2,∇2R(Zt)
Z−Zℓ+1/2E
(81)
≤sup
t∈[0,1]

∇2R(Zt)

ℓ2→ℓ2


Z−Zℓ+1/2


2
F(82)
với Cauchy-Schwarz. Sử dụng Bổ đề 5, chúng ta có thể ước tính hạng chuẩn toán tử trong cận trước đó
theo các tính chất của Zℓ+1/2. Chúng ta cần ước lượng
αsup
∥∆∥F≤1


∆−αZt(I+αZ∗
tZt)−1(Z∗
t∆+∆∗Zt)
(I+αZ∗
tZt)−1

F, (83)
và Bổ đề 6 cho rằng hạng này không lớn hơn 9α/4 cho bất kỳ Z và bất kỳ t nào. Với ước tính này và
(79), chúng ta có một cận trên bậc hai cho −R(Z):
−R(Z)≤ −R(Zℓ+1/2) +D
−∇ZR(Zℓ+1/2),Z−Zℓ+1/2E
+9α
8


Z−Zℓ+1/2


2
F. (84)
Trong khi đó, bởi các giả định của chúng tôi ở trên, chúng ta có
−∇ZR(Zℓ+1/2) =−αZℓ+1/2(I+αI)−1=−α
1 +αZℓ+1/2. (85)
Bây giờ chúng tôi tối thiểu hóa cận trên bậc hai trước đó như một hàm của Z. Lấy đạo hàm, bộ tối thiểu hóa Zopt được tính toán như
Zopt=
1 +4
9(1 + α)
Zℓ+1/2, (86)
và được biết rõ rằng toán tử proximal của tổng của χ{Z≥0} và λ∥ · ∥ 1 đơn giản là
toán tử thresholding mềm một phía [69]
proxχ{Z≥0}+λ∥ · ∥1(Z) = max {Z−λ1,0}, (87)
nơi maximum được áp dụng theo từng phần tử. Như trong Mục 2.4, chúng ta có thể viết maximum theo từng phần tử này đơn giản như ReLU. Do đó, một bước của majorization-minimization proximal dưới các giả định đơn giản hóa của chúng tôi có dạng
Zℓ+1= ReLU
1 +4
9(1 + α)
Zℓ+1/2−4λ
9α1
. (88)
Cuối cùng, chúng tôi chỉ ra một chi tiết bổ sung giới thiệu từ điển D xuất hiện trong khối ISTA trong Mục 2.4. Lưu ý rằng với bất kỳ D trực giao nào, người ta có R(DZ) =R(Z) cho mọi Z.
Tính đối xứng này ngụ ý các tính chất equivariance của ∇ZR(Z) và ∇2
ZR(Z): cho mọi Z và mọi ∆
và mọi D trực giao,
D∇ZR(Z) =∇ZR(DZ), (89)
⟨D∆,∇2
ZR(Z) (D∆)⟩=⟨∆,∇2
ZR(DZ) (∆)⟩. (90)
Do đó khai triển Taylor bậc hai (79) có thể được viết tương đương như
R(Z) =R(D∗Zℓ+1/2) +D
∇ZR(D∗Zℓ+1/2),Z−Zℓ+1/2E
+Z1
0(1−t)D
Z−Zℓ+1/2,∇2R(D∗Zt)
Z−Zℓ+1/2E
dt,(91)
cho bất kỳ D trực giao nào. Ý nghĩa của điều này là chúng ta đã có được một biểu thức tương đương
với (79), nhưng với Zℓ+1/2 được thay thế bằng D∗Zℓ+1/2; hơn nữa, vì các lập luận xấp xỉ
của chúng tôi ở trên không bị ảnh hưởng bởi phép nhân trái của Zℓ+1/2 bằng một ma trận trực giao (phép toán này
không thay đổi các chuẩn của các cột của Zℓ+1/2, hoặc các tương quan của chúng, và do đó tính không liên kết của ma trận), chúng ta có thể áp dụng chính xác cùng dòng lý luận ở trên để có được rằng một
lần lặp majorization-minimization proximal tương đương được cho bởi
Zℓ+1= ReLU
1 +4
9(1 + α)
D∗Zℓ+1/2−4λ
9α1
, (92)
cho bất kỳ từ điển trực giao D nào. Điều này cho một cập nhật khá tương tự với khối ISTA (17) trong
trường hợp nơi từ điển được sử dụng trong Mục 2.4 là trực giao, nhưng không có kết nối bỏ qua.
Do đó chúng ta có được một phiên bản hộp trắng tự nhiên của phần này của kiến trúc, cùng với diễn giải tự nhiên
rằng mục đích của nó là thưa thớt hóa các token đã nén Zℓ+1/2 trong một từ điển (có thể học),
phù hợp với các nghiên cứu thực nghiệm gần đây [75].
Các kiến trúc khác? Như chúng tôi đã đề cập ở đầu mục này, việc dẫn xuất trước đó
được thực hiện trong thiết lập cơ bản nhất có thể để thể hiện phương pháp majorization-
minimization cho thiết kế lớp. Các xấp xỉ hoặc giả định chính xác hơn có thể dẫn đến
các thiết kế lớp vượt trội tối ưu hóa mục tiêu mục tiêu (1) tốt hơn (và đặc biệt (13)). Chúng tôi đề cập
hai ở đây:
1. Ngoài các đặc trưng chính xác-không liên kết: các dẫn xuất của chúng tôi ở trên giả định rằng các biểu diễn đến Zℓ+1/2 đã tối đa cho hạng mở rộng R trong (13). Mong muốn là có được một dẫn xuất 'perturbative', áp dụng trong các trường hợp nơi Zℓ+1/2 không
hoàn toàn trực giao, mà thay vào đó gần-trực giao, đặc biệt không liên kết [69]. Các dẫn xuất
ở trên có thể được điều chỉnh cho thiết lập này; các cận perturbation trở nên hơi tinh tế hơn,
và lớp cuối cùng (92) thay đổi để liên quan đến chuẩn hóa bổ sung.
2. Ngoài các từ điển trực giao: Các tính đối xứng của hạng mở rộng R trong (13) có thể được
theo dõi để dẫn đến một cặp từ điển D và D′ và một mục tiêu thưa thớt hóa DZD′.
Loại biến đổi này gợi ý các kiến trúc phổ biến trộn trên các token [53,
66], tuy nhiên chúng tôi xem xét dạng đơn giản hơn DZ trong công việc này. Ngoài ra, chúng tôi đã tập trung
cho đơn giản vào các từ điển trực giao D; như trong bullet trước đó, người ta có thể xem xét
theo cách tương tự các từ điển D hoàn chỉnh và gần-trực giao. Điều chỉnh
dẫn xuất cho các từ điển overcomplete là một hướng tương lai thú vị mà chúng tôi mong đợi sẽ
cải thiện khả năng mở rộng của CRATE; một con đường để đạt được điều này có thể là tăng số lượng
phép chiếu U[K] và các chiều embedding của chúng.
22

--- TRANG 20 ---
A.3.1 Bổ Đề Phụ Trợ
Bổ đề 5. Xem xét hàm
R(Z) =1
2log det ( I+αZ∗Z), (93)
nơi α >0 là một hằng số. Sau đó chúng ta có
∇ZR(Z) =αZ(I+αZ∗Z)−1, (94)
và toán tử Hessian ∇2
ZR(Z):Rd×N→Rd×N thỏa mãn rằng với bất kỳ ∆∈Rd×N nào,
∇2
ZR(Z) (∆) (95)
=α∆(I+αZ∗Z)−1−α2Z(I+αZ∗Z)−1(Z∗∆+∆∗Z) (I+αZ∗Z)−1. (96)
Chứng minh. Việc tính toán gradient theo từ [46], ví dụ. Đối với Hessian, chúng tôi sử dụng phương pháp thông thường
để tính toán đạo hàm: nếu ∆ là bất kỳ ma trận nào có cùng hình dạng với Z và t >0,
∇2
ZR(Z) (∆) =∂
∂t
t=0[t7→ ∇ ZR(Z+t∆)], (97)
hợp lệ vì R trơn. Chúng ta có
∇ZR(Z+t∆)
=α(Z+t∆) (I+α(Z+t∆)∗(Z+t∆))−1
=α(Z+t∆) (I+αZ∗Z+αt[Z∗∆+∆∗Z+t∆∗∆])−1
=α(Z+t∆)
I+αt(I+αZ∗Z)−1[Z∗∆+∆∗Z+t∆∗∆]−1
(I+αZ∗Z)−1
=α(Z+t∆) ∞X
k=0(−αt)k
(I+αZ∗Z)−1[Z∗∆+∆∗Z+t∆∗∆]k!
(I+αZ∗Z)−1,
nơi trong dòng thứ tư chúng ta yêu cầu rằng t đủ gần 0 để gọi chuỗi Neumann. Đầu tiên, lưu ý rằng hạng liên quan đến ∆∗∆ không đóng vai trò trong biểu thức cuối cùng: sau khi
chúng ta lấy đạo hàm theo t và lấy giới hạn t→0, các hạng phát sinh do việc lấy đạo hàm của
t7→t∆∗∆ đi về không, bởi vì bất cứ khi nào chỉ số tổng k >0 chúng ta có một hạng (−αt)k đi về không khi t→0. Do đó chúng ta có được với quy tắc tích
∂
∂t
t=0[t7→ ∇ ZR(Z+t∆)] (98)
=α∆(I+αZ∗Z)−1−α2Z(I+αZ∗Z)−1(Z∗∆+∆∗Z) (I+αZ∗Z)−1. (99)
Bổ đề 6. Người ta có
sup
∥∆∥F≤1


∆−αZt(I+αZ∗
tZt)−1(Z∗
t∆+∆∗Zt)
(I+αZ∗
tZt)−1

F≤9
4. (100)
Chứng minh. Cố định ∆ thỏa mãn ∥∆∥F≤1. Bởi bất đẳng thức tam giác,



∆−αZt(I+αZ∗
tZt)−1(Z∗
t∆+∆∗Zt)
(I+αZ∗
tZt)−1

F(101)
≤

∆(I+αZ∗
tZt)−1

F+α

Zt(I+αZ∗
tZt)−1(Z∗
t∆+∆∗Zt)(I+αZ∗
tZt)−1

F.(102)
Cho hạng đầu tiên, chúng ta lưu ý rằng


∆(I+αZ∗
tZt)−1

F=


(I+αZ∗
tZt)−1⊗I
vec(∆)

F, (103)
và vì (I+αZ∗
tZt)−1⪯I, chúng ta có được từ Cauchy-Schwarz13


∆(I+αZ∗
tZt)−1

F≤ ∥∆∥F. (104)
Chúng ta có thể sử dụng một ý tưởng tương tự để kiểm soát hạng thứ hai. Chúng ta có từ bất đẳng thức tam giác


Zt(I+αZ∗
tZt)−1(Z∗
t∆+∆∗Zt)(I+αZ∗
tZt)−1

F(105)
≤

Zt(I+αZ∗
tZt)−1Z∗
t∆(I+αZ∗
tZt)−1

F(106)
+

(I+αZ∗
tZt)−1Z∗
t∆(I+αZ∗
tZt)−1Z∗
t

F. (107)
Cho hạng đầu tiên, chúng ta có


Zt(I+αZ∗
tZt)−1Z∗
t∆(I+αZ∗
tZt)−1

F(108)
=


(I+αZ∗
tZt)−1⊗Zt(I+αZ∗
tZt)−1Z∗
t
vec(∆)

F(109)
≤σmax
(I+αZ∗
tZt)−1
σmax
Zt(I+αZ∗
tZt)−1Z∗
t
∥∆∥F (110)
≤1
α∥∆∥F. (111)
Ước tính cuối cùng theo từ một tính toán sử dụng SVD của Zt. Trong khi đó, chúng ta có cho hạng thứ hai bằng một lập luận tương tự (sử dụng thực tế rằng các giá trị đơn của A và A∗ là giống hệt nhau
cho bất kỳ ma trận A nào)


(I+αZ∗
tZt)−1Z∗
t∆(I+αZ∗
tZt)−1Z∗
t

F≤σmax
(I+αZ∗
tZt)−1Z∗
t2∥∆∥F (112)
≤1
4α∥∆∥F, (113)
nơi một lần nữa ước tính theo từ một tính toán liên quan đến SVD của Zt (cùng với
thực tế rằng hàm σ7→σ/(1 +ασ2) bị chặn trên σ≥0 bởi 1/(2√α)). Đưa nó lại với nhau,
chúng ta đã có được



∆−αZt(I+αZ∗
tZt)−1(Z∗
t∆+∆∗Zt)
(I+αZ∗
tZt)−1

F≤9
4∥∆∥F, (114)
điều này cho khẳng định sau khi lấy suprema.
24

--- TRANG 21 ---
B Thí Nghiệm và Chi Tiết Bổ Sung
Trong mục này, chúng tôi cung cấp chi tiết về các thí nghiệm của chúng tôi, và báo cáo kết quả của các thí nghiệm bổ sung
mà không được đề cập trong văn bản chính. CRATE áp dụng có lẽ những lựa chọn thiết kế cơ bản nhất
có thể, và vì vậy chúng tôi không cố gắng cạnh tranh trực tiếp với hiệu suất tối tân từ các
transformers được thiết kế kỹ lưỡng và thực nghiệm. Kết quả của các thí nghiệm chúng tôi nhằm
truyền đạt một vài thông điệp cốt lõi:
•Mặc dù không được thiết kế để cạnh tranh với tối tân, CRATE hoạt động mạnh mẽ
trên các tập dữ liệu thế giới thực quy mô lớn, bao gồm phân loại trên ImageNet-1K. CRATE cũng
đạt được hiệu suất học chuyển giao mạnh mẽ.
•Vì mô hình của chúng tôi được thiết kế thông qua tối ưu hóa mở rộng của một mục tiêu được hiểu rõ,
mỗi lớp có thể diễn giải. Đặc biệt, chúng ta có thể phân tích hiệu suất của CRATE, cũng như
thiết kế các sửa đổi mạng, trên cơ sở từng lớp. Điều này được hỗ trợ bởi một mức độ hiểu biết
không thể so sánh về vai trò của mỗi toán tử trong mạng của chúng tôi.
•Chúng tôi thực hiện những lựa chọn đơn giản nhất có thể trong quá trình thiết kế CRATE, nhưng những điều này có thể được thay đổi
dễ dàng trong khi giữ cùng khung làm việc. Chúng tôi nghiên cứu một vài sửa đổi sau trong mục này
(Phụ lục B.4) và chỉ ra rằng chúng không làm tổn hại đáng kể hiệu suất thực nghiệm, nhưng
nhấn mạnh ở đây rằng có tiềm năng đáng kể cho cải thiện với các lựa chọn kiến trúc khác nhau (và đặc biệt một phân tích lý thuyết khác).
B.1 Chi tiết triển khai
Trong mục này, chúng tôi cung cấp chi tiết thêm để triển khai CRATE trên các tác vụ thị giác.
B.1.1 Kiến trúc của CRATE
Sửa đổi kiến trúc. So với kiến trúc khái niệm được đề xuất trong Mục 2.5 và 3, chúng tôi thực hiện thay đổi sau vì lợi ích của sự đơn giản triển khai:
•Trong bước nén, thay thế hạng p
Nϵ2[U1, . . . ,UK] trong toán tử MSSA bằng
một tham số có thể huấn luyện khác W∈Rd×pK. Do đó khối MSSA trở thành
MSSA(Z|U[K],W).=W
SSA(Z|U1)
...
SSA(Z|UK)
. (115)
Code PyTorch cho CRATE. Chúng tôi cung cấp code kiểu PyTorch để triển khai kiến trúc mạng được đề xuất của chúng tôi. Thuật toán 1 định nghĩa kiến trúc tổng thể, Thuật toán 2 và Thuật toán 3 chứa
chi tiết cho khối transformer, khối tự chú ý (khối MSSA), và khối MLP (khối ISTA).
B.1.2 Thiết lập Huấn luyện
Pre-training trên ImageNet-1K. Chúng tôi áp dụng bộ tối ưu Lion [71] để pre-train cả mô hình CRATE và ViT. Chúng tôi cấu hình tỷ lệ học là 2.4×10−4, weight decay là 0.5, và batch size là
2,048. Chúng tôi kết hợp chiến lược khởi động với sự tăng tuyến tính trong 5 epoch, sau đó huấn luyện
các mô hình tổng cộng 150 epoch với cosine decay. Để tăng cường dữ liệu, chúng tôi chỉ áp dụng các
kỹ thuật tiêu chuẩn, random cropping và random horizontal flipping, trên tập dữ liệu ImageNet-1K.
Chúng tôi áp dụng label smoothing với tham số smoothing 0.1. Một epoch huấn luyện của CRATE−Base mất
khoảng 240 giây sử dụng 16 GPU A100 40GB.
Fine-tuning. Chúng tôi fine-tune các mô hình CRATE và ViT đã pre-trained của chúng tôi trên các tập dữ liệu đích sau:
CIFAR10/CIFAR100 [10], Oxford Flowers-102 [7], Oxford-IIIT-Pets [16]. Chúng tôi cũng đánh giá các
mô hình đã pre-trained của chúng tôi trên benchmark ImageNet Real [36] thường được sử dụng. Cho mỗi tác vụ fine-tuning, chúng tôi sử dụng bộ tối ưu AdamW [26]. Chúng tôi cấu hình tỷ lệ học là 5×10−5, weight decay
là 0.01, và batch size là 512. Để cho phép học chuyển giao, đầu tiên chúng tôi thay đổi kích thước dữ liệu đầu vào của chúng tôi thành
224. Cho tăng cường dữ liệu, chúng tôi cũng áp dụng một số kỹ thuật tiêu chuẩn: random cropping, random
horizontal flipping, và random augmentation (với số lượng biến đổi n= 2 và độ lớn
của biến đổi m= 14).14
14https://github.com/huggingface/pytorch-image-models/blob/main/timm/data/auto_
augment.py
25

--- TRANG 22 ---
Thuật toán 1: Pseudocode kiểu PyTorch cho Mạng CRATE
# Định nghĩa lớp ViT_dictionary
CRATE:
# khởi tạo
def init(self, image_size, patch_size, num_classes, dim, depth, heads,
mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0.,
emb_dropout = 0.):
# định nghĩa patch, các chiều hình ảnh và số lượng patch
image_height, image_width = pair(image_size)
patch_height, patch_width = pair(patch_size)
num_patches = (image_height // patch_height) * (image_width //
patch_width)
patch_dim = channels * patch_height * patch_width
# định nghĩa patch embedding, positional embedding, dropout, và transformer
self.to_patch_embedding = Sequential(Rearrange, LayerNorm(patch_dim),
Linear(patch_dim, dim), LayerNorm(dim))
self.pos_embedding = Parameter(random(1, num_patches + 1, dim))
self.cls_token = Parameter(random(1, 1, dim))
self.dropout = Dropout(emb_dropout)
self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim,
dropout)
# định nghĩa pooling, latent layer, và MLP head
self.pool = pool
self.to_latent = Identity()
self.mlp_head = Sequential(LayerNorm(dim), Linear(dim, num_classes))
# forward pass
def forward(self, img):
x = self.to_patch_embedding(img)
b, n, _ = shape(x)
cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)
x = concatenate((cls_tokens, x), dim=1)
x += self.pos_embedding[:, :(n + 1)]
x = self.dropout(x)
x = self.transformer(x)
x = mean(x, dim = 1) if self.pool == 'mean' else x[:, 0]
x = self.to_latent(x)
return self.mlp_head(x)
Thuật toán 2: Pseudocode Kiểu Pytorch cho Khối Transformer trong CRATE
# Định nghĩa lớp Transformer
class Transformer:
# khởi tạo
def init(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):
# định nghĩa các lớp
self.layers = []
self.depth = depth
for _ in range(depth):
self.layers.append([LayerNorm(dim, Attention(dim, heads, dim_head,
dropout))])
self.layers.append([LayerNorm(dim, FeedForward(dim, mlp_dim,
dropout))])
# forward pass
def forward(self, x):
for attn, ff in self.layers:
x_ = attn(x) + x
x = ff(x_)
return x
26

--- TRANG 23 ---
Thuật toán 3: Pseudocode cho Attention và FeedForward
# Định nghĩa lớp FeedForward
class FeedForward:
# khởi tạo
def init(self, dim, hidden_dim, dropout = 0., step_size=0.1, lambd=0.1):
self.weight = Parameter(Tensor(dim, dim))
init.kaiming_uniform_(self.weight)
self.step_size = step_size
self.lambd = lambd
# forward pass
def forward(self, x):
x1 = linear(x, self.weight, bias=None)
grad_1 = linear(x1, self.weight.t(), bias=None)
grad_2 = linear(x, self.weight.t(), bias=None)
grad_update = self.step_size * (grad_2 - grad_1) - self.step_size *
self.lambd
output = relu(x + grad_update)
return output
# Định nghĩa lớp Attention
class Attention:
# khởi tạo
def init(self, dim, heads = 8, dim_head = 64, dropout = 0.):
inner_dim = dim_head * heads
project_out = not (heads == 1 and dim_head == dim)
self.heads = heads
self.scale = dim_head ** -0.5
self.attend = Softmax(dim = -1)
self.dropout = Dropout(dropout)
self.qkv = Linear(dim, inner_dim, bias=False)
self.to_out = Sequential(Linear(inner_dim, dim), Dropout(dropout)) if
project_out else nn.Identity()
# forward pass
def forward(self, x):
w = rearrange(self.qkv(x), 'b n (h d) -> b h n d', h = self.heads)
dots = matmul(w, w.transpose(-1, -2)) * self.scale
attn = self.attend(dots)
attn = self.dropout(attn)
out = matmul(attn, w)
out = rearrange(out, 'b h n d -> b n (h d)')
return self.to_out(out)
27

--- TRANG 24 ---
B.2 Kết Quả Thí Nghiệm
Trong mục này, chúng tôi cung cấp kết quả thí nghiệm bổ sung trên CRATE, bao gồm các đo lường theo lớp,
trực quan hóa, cũng như các nghiên cứu ablation.
B.2.1 Đánh Giá và Trực Quan Hóa Theo Lớp
Đánh giá theo lớp về nén và độ thưa thớt. Tương tự như Hình 3, chúng tôi tiến hành đánh giá theo lớp
về hạng nén và độ thưa thớt cho CRATE-Tiny, CRATE-Base, và CRATE-Large.
Chúng tôi quan sát hành vi tương tự như đã đề cập trong Mục 3.1: cả hạng nén và hạng
thưa thớt đều cải thiện khi chỉ số lớp tăng.
2 4 6 8 10 12
Chỉ số lớp - 
400450500550600650700Rc(Z) [khối SSA]
Đo tỷ lệ mã hóa qua các lớp
(a)Nén (Mô hình: CRATE-Tiny).
2 4 6 8 10 12
Chỉ số lớp - 
0.250.300.350.400.450.500.550.60Độ thưa thớt [khối ISTA]
Đo độ thưa thớt đầu ra qua các lớp (b)Độ thưa thớt (Mô hình: CRATE-Tiny).
2 4 6 8 10 12
Chỉ số lớp - 
600800100012001400Rc(Z) [khối SSA]
Đo tỷ lệ mã hóa qua các lớp
(c)Nén (Mô hình: CRATE-Base).
2 4 6 8 10 12
Chỉ số lớp - 
0.10.20.30.40.50.60.7Độ thưa thớt [khối ISTA]
Đo độ thưa thớt đầu ra qua các lớp (d)Độ thưa thớt (Mô hình: CRATE-Base).
0 5 10 15 20 25
Chỉ số lớp - 
100012001400160018002000Rc(Z) [khối SSA]
Đo tỷ lệ mã hóa qua các lớp
(e)Nén (Mô hình: CRATE-Large).
0 5 10 15 20 25
Chỉ số lớp - 
0.10.20.30.40.50.60.7Độ thưa thớt [khối ISTA]
Đo độ thưa thớt đầu ra qua các lớp (f)Độ thưa thớt (Mô hình: CRATE-Large).
Hình 5: Trái: Hạng nén Rc(Zℓ+1/2) của các đầu ra MSSA tại các lớp khác nhau. Phải: độ thưa thớt
của đầu ra khối ISTA, ∥Zℓ+1∥0/(d·N), tại các lớp khác nhau.
28

--- TRANG 25 ---
Trực quan hóa biểu diễn token theo lớp. Trong Hình 6, chúng tôi trực quan hóa các biểu diễn token
Zℓ tại các lớp khác nhau ℓ∈ {1, . . . , 12}. Chúng tôi cung cấp thêm kết quả được đánh giá trên các mẫu khác trong
Phụ lục B.2.2.
Trực quan hóa các không gian con theo lớp trong tự chú ý đa đầu. Chúng tôi cung cấp trực quan hóa của
Uℓ
[K] trong Hình 7.
Lớp=1
0.000.250.500.751.001.251.501.752.00
(a)ℓ= 1.
Lớp=2
0.000.250.500.751.001.251.501.752.00 (b)ℓ= 2.
Lớp=3
0.000.250.500.751.001.251.501.752.00 (c)ℓ= 3.
Lớp=4
0.000.250.500.751.001.251.501.752.00 (d)ℓ= 4.
Lớp=5
0.000.250.500.751.001.251.501.752.00
(e)ℓ= 5.
Lớp=6
0.000.250.500.751.001.251.501.752.00 (f)ℓ= 6.
Lớp=7
0.000.250.500.751.001.251.501.752.00 (g)ℓ= 7.
Lớp=8
0.000.250.500.751.001.251.501.752.00 (h)ℓ= 8.
Lớp=9
0.000.250.500.751.001.251.501.752.00
(i)ℓ= 9.
Lớp=10
0.000.250.500.751.001.251.501.752.00 (j)ℓ= 10.
Lớp=11
0.000.250.500.751.001.251.501.752.00 (k)ℓ= 11.
Lớp=12
0.000.250.500.751.001.251.501.752.00 (l)ℓ= 12.
Hình 6: Trực quan hóa biểu diễn token Zℓ theo lớp tại mỗi lớp ℓ. Để tăng cường độ rõ nét thị giác, chúng tôi
ngẫu nhiên trích xuất một ma trận con 50 ×50 từ Zℓ để hiển thị. (Mô hình: CRATE-Tiny)
29

--- TRANG 26 ---
Lớp=1
0.20.40.60.81.0(a)ℓ= 1.
Lớp=2
0.20.40.60.81.0 (b)ℓ= 2.
Lớp=3
0.20.40.60.81.0 (c)ℓ= 3.
Lớp=4
0.20.40.60.81.0 (d)ℓ= 4.
Lớp=5
0.20.40.60.81.0
(e)ℓ= 5.
Lớp=6
0.20.40.60.81.0 (f)ℓ= 6.
Lớp=7
0.20.40.60.81.0 (g)ℓ= 7.
Lớp=8
0.20.40.60.81.0 (h)ℓ= 8.
Lớp=9
0.20.40.60.81.0
(i)ℓ= 9.
Lớp=10
0.20.40.60.81.0 (j)ℓ= 10.
Lớp=11
0.20.40.60.81.0 (k)ℓ= 11.
Lớp=12
0.20.40.60.81.0 (l)ℓ= 12.
Hình 7: Chúng tôi trực quan hóa [Uℓ
1, . . . ,Uℓ
K]∗[Uℓ
1, . . . ,Uℓ
K]∈RpK×pK tại các lớp khác nhau. Khối (i, j) thứ
trong mỗi hình phụ tương ứng với (Uℓ
i)∗Uℓ
j cho i, j∈[K] tại một lớp ℓ cụ thể. Để tăng cường độ rõ nét thị giác, cho
mỗi không gian con Ui, chúng tôi ngẫu nhiên chọn 4 hướng để hiển thị. (Mô hình: CRATE-Tiny)
30

--- TRANG 27 ---
B.2.2 Trực Quan Hóa Theo Lớp Bổ Sung
Chúng tôi cung cấp thêm kết quả của trực quan hóa biểu diễn token theo lớp trên các mẫu khác trong
Hình 8, Hình 9, Hình 10, và Hình 11 (Mô hình: CRATE-Base).
Lớp=1
0.000.250.500.751.001.251.501.752.00
Lớp=2
0.000.250.500.751.001.251.501.752.00
Lớp=3
0.000.250.500.751.001.251.501.752.00
Lớp=4
0.000.250.500.751.001.251.501.752.00
Lớp=5
0.000.250.500.751.001.251.501.752.00
Lớp=6
0.000.250.500.751.001.251.501.752.00
Lớp=7
0.000.250.500.751.001.251.501.752.00
Lớp=8
0.000.250.500.751.001.251.501.752.00
Lớp=9
0.000.250.500.751.001.251.501.752.00
Lớp=10
0.000.250.500.751.001.251.501.752.00
Lớp=11
0.000.250.500.751.001.251.501.752.00
Lớp=12
0.000.250.500.751.001.251.501.752.00
Hình 8: Trực quan hóa biểu diễn token Zℓ theo lớp tại mỗi lớp ℓ. Để tăng cường độ rõ nét thị giác, chúng tôi
ngẫu nhiên trích xuất một ma trận con 50 ×50 từ Zℓ để hiển thị. (Mẫu 1)
Lớp=1
0.000.250.500.751.001.251.501.752.00
Lớp=2
0.000.250.500.751.001.251.501.752.00
Lớp=3
0.000.250.500.751.001.251.501.752.00
Lớp=4
0.000.250.500.751.001.251.501.752.00
Lớp=5
0.000.250.500.751.001.251.501.752.00
Lớp=6
0.000.250.500.751.001.251.501.752.00
Lớp=7
0.000.250.500.751.001.251.501.752.00
Lớp=8
0.000.250.500.751.001.251.501.752.00
Lớp=9
0.000.250.500.751.001.251.501.752.00
Lớp=10
0.000.250.500.751.001.251.501.752.00
Lớp=11
0.000.250.500.751.001.251.501.752.00
Lớp=12
0.000.250.500.751.001.251.501.752.00
Hình 9: Trực quan hóa biểu diễn token Zℓ theo lớp tại mỗi lớp ℓ. Để tăng cường độ rõ nét thị giác, chúng tôi
ngẫu nhiên trích xuất một ma trận con 50 ×50 từ Zℓ để hiển thị. (Mẫu 2)
31

--- TRANG 28 ---
Lớp=1
0.000.250.500.751.001.251.501.752.00
Lớp=2
0.000.250.500.751.001.251.501.752.00
Lớp=3
0.000.250.500.751.001.251.501.752.00
Lớp=4
0.000.250.500.751.001.251.501.752.00
Lớp=5
0.000.250.500.751.001.251.501.752.00
Lớp=6
0.000.250.500.751.001.251.501.752.00
Lớp=7
0.000.250.500.751.001.251.501.752.00
Lớp=8
0.000.250.500.751.001.251.501.752.00
Lớp=9
0.000.250.500.751.001.251.501.752.00
Lớp=10
0.000.250.500.751.001.251.501.752.00
Lớp=11
0.000.250.500.751.001.251.501.752.00
Lớp=12
0.000.250.500.751.001.251.501.752.00Hình 10: Trực quan hóa biểu diễn token Zℓ theo lớp tại mỗi lớp ℓ. Để tăng cường độ rõ nét thị giác, chúng tôi
ngẫu nhiên trích xuất một ma trận con 50 ×50 từ Zℓ để hiển thị. (Mẫu 3)
Lớp=1
0.000.250.500.751.001.251.501.752.00
Lớp=2
0.000.250.500.751.001.251.501.752.00
Lớp=3
0.000.250.500.751.001.251.501.752.00
Lớp=4
0.000.250.500.751.001.251.501.752.00
Lớp=5
0.000.250.500.751.001.251.501.752.00
Lớp=6
0.000.250.500.751.001.251.501.752.00
Lớp=7
0.000.250.500.751.001.251.501.752.00
Lớp=8
0.000.250.500.751.001.251.501.752.00
Lớp=9
0.000.250.500.751.001.251.501.752.00
Lớp=10
0.000.250.500.751.001.251.501.752.00
Lớp=11
0.000.250.500.751.001.251.501.752.00
Lớp=12
0.000.250.500.751.001.251.501.752.00
Hình 11: Trực quan hóa biểu diễn token Zℓ theo lớp tại mỗi lớp ℓ. Để tăng cường độ rõ nét thị giác, chúng tôi
ngẫu nhiên trích xuất một ma trận con 50 ×50 từ Zℓ để hiển thị. (Mẫu 4)
32

--- TRANG 29 ---
B.3 Ablation CRATE
Siêu tham số của CRATE. Trong Bảng 2, chúng tôi trình bày đánh giá CRATE được huấn luyện với các
tham số khác nhau. Cụ thể hơn, chúng tôi nghiên cứu hiệu ứng của số epoch, weight decay, tỷ lệ học,
step size (η) và hạng chính quy (λ) trong khối ISTA. Như được hiển thị trong Bảng 2, CRATE
thể hiện hiệu suất đạt yêu cầu nhất quán qua một phạm vi đa dạng các siêu tham số.
Bảng 2: Độ chính xác Top 1 của CRATE trên các tập dữ liệu khác nhau với các biến thể thiết kế kiến trúc khác nhau khi được huấn luyện
trên ImageNet.
Mô hình epoch weight decay lr η(ISTA) λ(ISTA) ImageNet
CRATE-B 150 (mặc định) 0.5 (mặc định) 2.4×10−40.1 0.1 70.8
CRATE-B 150 0.5 2.4×10−40.02 0.1 70.7
CRATE-B 150 0.5 2.4×10−40.5 0.1 66.7
CRATE-B 150 0.5 2.4×10−40.1 0.02 70.8
CRATE-B 150 0.5 2.4×10−40.1 0.5 70.5
CRATE-B 90 0.5 2.4×10−40.1 0.1 69.5
CRATE-B 300 0.5 2.4×10−40.1 0.1 70.9
CRATE-B 150 1.0 2.4×10−40.1 0.1 70.3
CRATE-B 150 0.05 2.4×10−40.1 0.1 70.2
CRATE-B 150 0.5 4.8×10−40.1 0.1 70.2
CRATE-B 150 0.5 1.2×10−40.1 0.1 70.3
B.4 Khám Phá Các Biến Thể Kiến Trúc
Trong mục này, chúng tôi khám phá hai kiến trúc thay thế sau. Một kiến trúc liên quan đến một
sửa đổi cho cơ chế attention, trong khi cái khác liên quan đến một sửa đổi cho cơ chế thưa thớt hóa. Một lần nữa, chúng tôi nhấn mạnh lại rằng những lựa chọn này, mặc dù có nguyên tắc, hoàn toàn modular và
những lựa chọn chúng tôi thực hiện ở đây vẫn dẫn đến các kiến trúc rất đơn giản. Một phân tích tinh vi hơn có thể
dẫn đến các kiến trúc khác nhau, phức tạp hơn hoạt động tốt hơn trong thực tế. Các kiến trúc chúng tôi
thí nghiệm với là:
•Cơ chế attention lấy cảm hứng từ nén: hoàn nguyên thay đổi trong (115). Tức là, cơ chế attention
thực hiện (11) và (12) trực tiếp.
•Bước thưa thớt hóa proximal majorization-minimization: thay vì (17), thực hiện (92).
Chúng tôi có được các kết quả phân loại sau trong Bảng 3. Sau khi tiến hành các đơn giản hóa bổ sung
cho kiến trúc mạng (tức là, áp đặt các ràng buộc bổ sung cho thiết kế kiến trúc mạng),
chúng tôi phát hiện rằng CRATE duy trì hiệu suất hợp lý trên ImageNet-1K.
Bảng 3: Độ chính xác Top 1 của CRATE trên các tập dữ liệu khác nhau với các biến thể thiết kế kiến trúc khác nhau khi được huấn luyện
trên ImageNet.
Mô hình Khối MSSA Khối ISTA ImageNet
CRATE-B mặc định mặc định 70.8
CRATE-B Phương trình (11) và (12) mặc định 63.3
CRATE-B mặc định Phương trình (92) 68.6
33
