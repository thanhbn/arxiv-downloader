# 2311.03658.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/interpretability/2311.03658.pdf
# Kích thước tệp: 2534772 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Giả thuyết Biểu diễn Tuyến tính và
Hình học của các Mô hình Ngôn ngữ Lớn
Kiho Park1Yo Joong Choe1Victor Veitch1
Tóm tắt
Một cách không chính thức, "giả thuyết biểu diễn tuyến tính" là ý tưởng rằng các khái niệm cấp cao được biểu diễn tuyến tính như các hướng trong một không gian biểu diễn nào đó. Trong bài báo này, chúng tôi giải quyết hai câu hỏi liên quan chặt chẽ: "Biểu diễn tuyến tính" thực sự có nghĩa là gì? Và, làm thế nào chúng ta hiểu được các khái niệm hình học (ví dụ: độ tương tự cosine và phép chiếu) trong không gian biểu diễn? Để trả lời những câu hỏi này, chúng tôi sử dụng ngôn ngữ của các phản thực để đưa ra hai cách hình thức hóa của biểu diễn tuyến tính, một trong không gian biểu diễn đầu ra (từ), và một trong không gian đầu vào (ngữ cảnh). Sau đó chúng tôi chứng minh rằng những cách này kết nối với việc thăm dò tuyến tính và điều khiển mô hình, tương ứng. Để hiểu được các khái niệm hình học, chúng tôi sử dụng cách hình thức hóa để xác định một tích vô hướng cụ thể (không phải Euclidean) tôn trọng cấu trúc ngôn ngữ theo nghĩa chúng tôi làm rõ. Sử dụng tích vô hướng nhân quả này, chúng tôi chỉ ra cách thống nhất tất cả các khái niệm về biểu diễn tuyến tính. Đặc biệt, điều này cho phép xây dựng các đầu dò và vector điều khiển sử dụng các cặp phản thực. Các thí nghiệm với LLaMA-2 chứng minh sự tồn tại của các biểu diễn tuyến tính của khái niệm, kết nối với việc giải thích và kiểm soát, và vai trò cơ bản của việc lựa chọn tích vô hướng. Mã nguồn có sẵn tại github.com/KihoPark/linear_repgeometry.

1. Giới thiệu
Trong bối cảnh của các mô hình ngôn ngữ, "Giả thuyết Biểu diễn Tuyến tính" là ý tưởng rằng các khái niệm cấp cao được biểu diễn tuyến tính trong không gian biểu diễn của một mô hình (ví dụ: Mikolov et al., 2013c; Arora et al., 2016; Elhage et al., 2022). Các khái niệm cấp cao có thể bao gồm: văn bản bằng tiếng Pháp hay tiếng Anh? Nó ở thì hiện tại hay quá khứ? Nếu văn bản nói về một người, họ là nam hay nữ? Sự hấp dẫn của giả thuyết biểu diễn tuyến tính là rằng—nếu nó đúng—các tác vụ giải thích và kiểm soát hành vi mô hình có thể khai thác các phép toán đại số tuyến tính trên không gian biểu diễn. Mục tiêu của chúng tôi là hình thức hóa giả thuyết biểu diễn tuyến tính, và làm rõ cách nó liên quan đến giải thích và kiểm soát.

1University of Chicago, Illinois, USA.
Kỷ yếu Hội nghị Quốc tế lần thứ 41 về Học máy, Vienna, Austria. PMLR 235, 2024. Bản quyền 2024 thuộc về (các) tác giả.

Thách thức đầu tiên là không rõ "biểu diễn tuyến tính" có nghĩa là gì. Có (ít nhất) ba cách diễn giải:

1. Không gian con: (ví dụ: Mikolov et al., 2013c; Pennington et al., 2014) Ý tưởng đầu tiên là mỗi khái niệm được biểu diễn như một không gian con (1 chiều). Ví dụ, trong bối cảnh của nhúng từ, đã được lập luận thực nghiệm rằng Rep("woman") − Rep("man"), Rep("queen") − Rep("king"), và tất cả các cặp tương tự thuộc về một không gian con chung (Mikolov et al., 2013c). Khi đó, tự nhiên ta lấy không gian con này là một biểu diễn của khái niệm Nam/Nữ.

2. Đo lường: (ví dụ: Nanda et al., 2023; Gurnee & Tegmark, 2023) Tiếp theo là ý tưởng rằng xác suất của một giá trị khái niệm có thể được đo bằng một đầu dò tuyến tính. Ví dụ, xác suất ngôn ngữ đầu ra là tiếng Pháp có dạng logit-tuyến tính trong biểu diễn của đầu vào. Trong trường hợp này, chúng ta có thể lấy ánh xạ tuyến tính là một biểu diễn của khái niệm Tiếng Anh/Tiếng Pháp.

3. Can thiệp: (ví dụ: Wang et al., 2023; Turner et al., 2023) Ý tưởng cuối cùng là giá trị mà một khái niệm nhận có thể được thay đổi, mà không thay đổi các khái niệm khác, bằng cách thêm một vector điều khiển phù hợp—ví dụ, chúng ta thay đổi đầu ra từ tiếng Anh sang tiếng Pháp bằng cách thêm một vector Tiếng Anh/Tiếng Pháp. Trong trường hợp này, chúng ta lấy vector được thêm này là một biểu diễn của khái niệm.

Không rõ a priori cách những ý tưởng này liên quan đến nhau, cũng không rõ cái nào là khái niệm "đúng" về biểu diễn tuyến tính.

Tiếp theo, giả sử chúng ta đã tìm được các biểu diễn tuyến tính của các khái niệm khác nhau. Sau đó chúng ta có thể sử dụng các phép toán đại số tuyến tính trên không gian biểu diễn để giải thích và kiểm soát. Ví dụ, chúng ta có thể tính toán độ tương tự cosine giữa một biểu diễn và các hướng khái niệm đã biết, hoặc chỉnh sửa các biểu diễn được chiếu lên các hướng mục tiêu. Tuy nhiên, độ tương tự và phép chiếu là các khái niệm hình học: chúng cần một tích vô hướng trên không gian biểu diễn. Thách thức thứ hai là không rõ tích vô hướng nào

1arXiv:2311.03658v2 [cs.CL] 17 Jul 2024

--- TRANG 2 ---
Giả thuyết Biểu diễn Tuyến tính và Hình học của các Mô hình Ngôn ngữ Lớn

γ̄male⇒female
≈γ("queen") −γ("king")
ḡmale⇒female
=l̄male⇒female ḡEnglish ⇒French
=l̄English ⇒French

Tích Vô hướng Nhân quả γ̄English ⇒French
≈γ("roi")−γ("king")
λ̄English ⇒French
≈λ("Il est le")−λ("He is the") λ̄male⇒female
≈λ("She is the")−λ("He is the")

Hình 1. Hình học của các biểu diễn tuyến tính có thể được hiểu thông qua một tích vô hướng nhân quả tôn trọng cấu trúc ngữ nghĩa của các khái niệm. Trong một mô hình ngôn ngữ, mỗi khái niệm có hai biểu diễn tuyến tính riêng biệt, λ̄ (đỏ) trong không gian nhúng (ngữ cảnh đầu vào) và γ̄ (xanh) trong không gian giải nhúng (từ đầu ra), như được vẽ ở bên trái. Tích vô hướng nhân quả tạo ra một phép biến đổi tuyến tính cho các không gian biểu diễn sao cho các biểu diễn tuyến tính được biến đổi trùng nhau (tím), như được vẽ ở bên phải. Trong không gian biểu diễn thống nhất này, các khái niệm có thể tách biệt nhân quả được biểu diễn bởi các vector trực giao.

là phù hợp để hiểu các biểu diễn mô hình.

Để giải quyết những vấn đề này, chúng tôi đóng góp như sau:

1. Đầu tiên, chúng tôi hình thức hóa khái niệm không gian con của biểu diễn tuyến tính theo các cặp phản thực, trong cả không gian "nhúng" (ngữ cảnh đầu vào) và "giải nhúng" (từ đầu ra). Sử dụng cách hình thức hóa này, chúng tôi chứng minh rằng khái niệm giải nhúng kết nối với đo lường, và khái niệm nhúng với can thiệp.

2. Tiếp theo, chúng tôi giới thiệu khái niệm về tích vô hướng nhân quả: một tích vô hướng có tính chất là các khái niệm có thể biến đổi tự do với nhau được biểu diễn như các vector trực giao. Chúng tôi chỉ ra rằng một tích vô hướng như vậy có tính chất đặc biệt là nó thống nhất các biểu diễn nhúng và giải nhúng, như được minh họa trong Hình 1. Ngoài ra, chúng tôi chỉ ra cách ước lượng tích vô hướng sử dụng ma trận giải nhúng LLM.

3. Cuối cùng, chúng tôi nghiên cứu giả thuyết biểu diễn tuyến tính bằng thực nghiệm sử dụng LLaMA-2 (Touvron et al., 2023). Chúng tôi tìm thấy khái niệm không gian con của các biểu diễn tuyến tính cho nhiều khái niệm khác nhau. Sử dụng những cái này, chúng tôi đưa ra bằng chứng rằng tích vô hướng nhân quả tôn trọng cấu trúc ngữ nghĩa, và rằng các biểu diễn không gian con có thể được sử dụng để xây dựng các biểu diễn đo lường và can thiệp.

Kiến thức nền về Mô hình Ngôn ngữ Chúng tôi sẽ cần một số kiến thức nền tối thiểu về các mô hình ngôn ngữ (lớn). Chính thức, một mô hình ngôn ngữ nhận văn bản ngữ cảnh x và lấy mẫu văn bản đầu ra. Việc lấy mẫu này được thực hiện từng từ (hoặc từng token). Theo đó, chúng tôi sẽ xem các đầu ra như các từ đơn lẻ. Để định nghĩa một phân phối xác suất trên các đầu ra, mô hình ngôn ngữ đầu tiên ánh xạ mỗi ngữ cảnh x thành một vector λ(x) trong một không gian biểu diễn Λ ≃ Rd. Chúng tôi sẽ gọi những cái này là các vector nhúng. Mô hình cũng biểu diễn mỗi từ y như một vector giải nhúng γ(y) trong một không gian biểu diễn riêng biệt Γ ≃ Rd. Phân phối xác suất trên các từ tiếp theo sau đó được đưa ra bởi phân phối softmax:

P(y|x) ∝ exp(λ(x)⊤γ(y)).

2. Giả thuyết Biểu diễn Tuyến tính

Chúng tôi bắt đầu bằng cách hình thức hóa khái niệm không gian con của biểu diễn tuyến tính, một trong mỗi không gian giải nhúng và nhúng của các mô hình ngôn ngữ, và sau đó liên kết các khái niệm không gian con với các khái niệm đo lường và can thiệp.

2.1. Khái niệm

Bước đầu tiên là hình thức hóa khái niệm về một khái niệm. Một cách trực quan, một khái niệm là bất kỳ yếu tố biến đổi nào có thể được thay đổi một cách độc lập. Ví dụ, chúng ta có thể thay đổi đầu ra từ tiếng Pháp sang tiếng Anh mà không thay đổi ý nghĩa của nó, hoặc thay đổi đầu ra từ việc nói về một người đàn ông thành về một người phụ nữ mà không thay đổi ngôn ngữ mà nó được viết.

Theo Wang et al. (2023), chúng tôi hình thức hóa ý tưởng này bằng cách lấy một biến khái niệm W là một biến ẩn được gây ra bởi ngữ cảnh X, và hoạt động như một nguyên nhân của đầu ra Y. Để đơn giản trong trình bày, chúng tôi sẽ giới hạn chú ý vào các khái niệm nhị phân. Dự đoán việc biểu diễn các khái niệm bằng vector, chúng tôi giới thiệu một thứ tự trên mỗi khái niệm nhị phân—ví dụ: nam ⇒ nữ. Thứ tự này làm cho dấu của một biểu diễn có ý nghĩa (ví dụ: biểu diễn của nữ ⇒ nam sẽ có dấu ngược lại).

Mỗi biến khái niệm W định nghĩa một tập hợp các đầu ra phản thực {Y(W = w)} chỉ khác nhau ở giá trị của W. Ví dụ, đối với khái niệm nam ⇒ nữ, (Y(0), Y(1)) là một phần tử ngẫu nhiên của tập hợp {("man", "woman"), ("king",

2

--- TRANG 3 ---
Giả thuyết Biểu diễn Tuyến tính và Hình học của các Mô hình Ngôn ngữ Lớn

"queen"), . . . }. Trong bài báo này, chúng tôi giả sử giá trị của các khái niệm có thể được đọc một cách xác định từ đầu ra được lấy mẫu (ví dụ: đầu ra "king" ngụ ý W = 0). Khi đó, chúng ta có thể chỉ định các khái niệm bằng cách chỉ định các đầu ra phản thực tương ứng của chúng.

Cuối cùng chúng ta sẽ cần lập luận về mối quan hệ giữa nhiều khái niệm. Chúng ta nói rằng hai khái niệm W và Z có thể tách biệt nhân quả nếu Y(W = w, Z = z) được định nghĩa rõ ràng cho mỗi w, z. Nghĩa là, các khái niệm có thể tách biệt nhân quả là những khái niệm có thể được biến đổi tự do và một cách độc lập. Ví dụ, các khái niệm Tiếng Anh ⇒ Tiếng Pháp và nam ⇒ nữ có thể tách biệt nhân quả—xem xét {"king", "queen", "roi", "reine"}. Tuy nhiên, các khái niệm Tiếng Anh ⇒ Tiếng Pháp và Tiếng Anh ⇒ Tiếng Nga thì không thể vì chúng không thể biến đổi tự do.

Chúng tôi sẽ viết Y(W = w, Z = z) là Y(w, z) khi các khái niệm rõ ràng từ ngữ cảnh.

2.2. Biểu diễn Giải nhúng và Đo lường

Bây giờ chúng tôi chuyển sang hình thức hóa các biểu diễn tuyến tính của một khái niệm. Quan sát đầu tiên là có hai không gian biểu diễn khác nhau đang tham gia—không gian nhúng Λ và không gian giải nhúng Γ. Một khái niệm có thể được biểu diễn tuyến tính trong một trong hai không gian. Chúng tôi bắt đầu với không gian giải nhúng.

Định nghĩa hình nón của vector v là Cone(v) = {αv : α > 0},

Định nghĩa 2.1 (Biểu diễn Giải nhúng). Chúng ta nói rằng γ̄W là một biểu diễn giải nhúng của một khái niệm W nếu
γ(Y(1)) − γ(Y(0)) ∈ Cone(γ̄W) hầu như chắc chắn.

Định nghĩa này nắm bắt khái niệm không gian con trong không gian giải nhúng, ví dụ: γ("queen") − γ("king") song song với γ("woman") − γ("man"). Chúng tôi sử dụng hình nón thay vì không gian con vì dấu của sự khác biệt có ý nghĩa—tức là sự khác biệt giữa "king" và "queen" ngược hướng với sự khác biệt giữa "woman" và "man".

Biểu diễn giải nhúng (nếu tồn tại) là duy nhất đến việc chia tỷ lệ dương, phù hợp với giả thuyết không gian con tuyến tính rằng các khái niệm được biểu diễn như các hướng.

Kết nối với Đo lường Kết quả đầu tiên là biểu diễn giải nhúng được liên kết chặt chẽ với khái niệm đo lường của biểu diễn tuyến tính:

Định lý 2.2 (Biểu diễn Đo lường). Cho W là một khái niệm, và cho γ̄W là biểu diễn giải nhúng của W. Khi đó, với bất kỳ nhúng ngữ cảnh λ ∈ Λ,
logit P(Y = Y(1)|Y ∈ {Y(0), Y(1)}, λ) = αλ⊤γ̄W,
trong đó α > 0 (h.k.c.c.) là một hàm của {Y(0), Y(1)}.

Tất cả các chứng minh được đưa ra trong Phụ lục B.

Nói cách khác: nếu chúng ta biết token đầu ra là "king" hoặc "queen" (giả sử, ngữ cảnh là về một vị vua), thì xác suất đầu ra là "king" có dạng logit-tuyến tính trong biểu diễn mô hình ngôn ngữ với các hệ số hồi quy γ̄W.

Vô hướng ngẫu nhiên α là một hàm của cặp phản thực cụ thể {Y(0), Y(1)}—ví dụ: nó có thể khác nhau cho {"king", "queen"} và {"roi", "reine"}. Tuy nhiên, hướng được sử dụng để dự đoán là giống nhau cho tất cả các cặp phản thực thể hiện khái niệm.

Định lý 2.2 cho thấy một kết nối giữa biểu diễn không gian con và biểu diễn tuyến tính được học bởi việc khớp một đầu dò tuyến tính để dự đoán khái niệm. Cụ thể, trong cả hai trường hợp, chúng ta có được một bộ dự đoán tuyến tính trên thang logit. Tuy nhiên, biểu diễn giải nhúng khác với biểu diễn dựa trên đầu dò ở chỗ nó không kết hợp bất kỳ thông tin nào về các khái niệm tương quan nhưng ngoài mục tiêu. Ví dụ, nếu văn bản tiếng Pháp chủ yếu về đàn ông một cách không cân đối, một đầu dò có thể học thông tin này (và bao gồm nó trong biểu diễn), nhưng biểu diễn giải nhúng sẽ không. Theo nghĩa này, biểu diễn giải nhúng có thể được xem như một biểu diễn thăm dò lý tưởng.

2.3. Biểu diễn Nhúng và Can thiệp

Bước tiếp theo là định nghĩa một biểu diễn không gian con tuyến tính trong không gian nhúng Λ. Chúng tôi sẽ lại đi với một khái niệm được neo bằng các cặp minh họa. Trong không gian nhúng, mỗi λ(x) định nghĩa một phân phối trên các khái niệm. Chúng tôi xem xét các cặp câu như λ0 = λ("He is the monarch of England, ") và λ1 = λ("She is the monarch of England, ") tạo ra các phân phối khác nhau trên khái niệm mục tiêu, nhưng cùng phân phối trên tất cả các khái niệm ngoài mục tiêu. Một khái niệm được biểu diễn-nhúng nếu sự khác biệt giữa tất cả các cặp như vậy thuộc về một không gian con chung. Chính thức,

Định nghĩa 2.3 (Biểu diễn Nhúng). Chúng ta nói rằng λ̄W là một biểu diễn nhúng của một khái niệm W nếu chúng ta có λ1 − λ0 ∈ Cone(λ̄W) cho bất kỳ nhúng ngữ cảnh λ0, λ1 ∈ Λ thỏa mãn

P(W = 1|λ1)/P(W = 1|λ0) > 1 và P(W, Z|λ1)/P(W, Z|λ0) = P(W|λ1)/P(W|λ0),

cho mỗi khái niệm Z có thể tách biệt nhân quả với W.

Điều kiện đầu tiên đảm bảo rằng hướng có liên quan đến khái niệm mục tiêu, và điều kiện thứ hai đảm bảo rằng hướng không liên quan đến các khái niệm ngoài mục tiêu.

Kết nối với Can thiệp Hóa ra biểu diễn nhúng được liên kết chặt chẽ với khái niệm can thiệp của biểu diễn tuyến tính. Để làm điều này, chúng ta cần bổ đề sau liên quan biểu diễn nhúng và giải nhúng.

3

--- TRANG 4 ---
Giả thuyết Biểu diễn Tuyến tính và Hình học của các Mô hình Ngôn ngữ Lớn

Bổ đề 2.4 (Mối quan hệ Giải nhúng-Nhúng). Cho λ̄W là biểu diễn nhúng của một khái niệm W, và cho γ̄W và γ̄Z là các biểu diễn giải nhúng cho W và bất kỳ khái niệm Z nào có thể tách biệt nhân quả với W. Khi đó,
λ̄⊤W γ̄W > 0 và λ̄⊤W γ̄Z = 0. (2.1)

Ngược lại, nếu một biểu diễn λ̄W thỏa mãn (2.1), và nếu tồn tại các khái niệm {Zi}d−1i=1, sao cho mỗi Zi có thể tách biệt nhân quả với W và {γ̄W} ∪ {γ̄Zi}d−1i=1 là cơ sở của Rd, thì λ̄W là biểu diễn nhúng cho W.

Bây giờ chúng ta có thể kết nối với khái niệm can thiệp:

Định lý 2.5 (Biểu diễn Can thiệp). Cho λ̄W là biểu diễn nhúng của một khái niệm W. Khi đó, với bất kỳ khái niệm Z nào có thể tách biệt nhân quả với W,

P(Y = Y(W,1)|Y ∈ {Y(W,0), Y(W,1)}, λ + cλ̄W)

là hằng số theo c ∈ R, và

P(Y = Y(1,Z)|Y ∈ {Y(0,Z), Y(1,Z)}, λ + cλ̄W)

tăng theo c ∈ R.

Nói cách khác: thêm λ̄W vào biểu diễn mô hình ngôn ngữ của ngữ cảnh thay đổi xác suất của khái niệm mục tiêu (W), nhưng không thay đổi xác suất của các khái niệm ngoài mục tiêu (Z).

3. Tích Vô hướng cho Biểu diễn Mô hình Ngôn ngữ

Với các biểu diễn tuyến tính, chúng ta muốn sử dụng chúng bằng cách thực hiện những việc như đo lường độ tương tự giữa các biểu diễn khác nhau, hoặc chỉnh sửa các biểu diễn được chiếu lên một hướng mục tiêu. Độ tương tự và phép chiếu đều là các khái niệm đòi hỏi một tích vô hướng. Bây giờ chúng tôi xem xét câu hỏi về tích vô hướng nào phù hợp để hiểu các biểu diễn mô hình ngôn ngữ.

Kiến thức cơ bản Chúng tôi định nghĩa Γ̄ là không gian của các hiệu giữa các phần tử của Γ. Khi đó, Γ̄ là một không gian vector thực d chiều.¹ Chúng tôi xem xét việc định nghĩa các tích vô hướng trên Γ̄. Các biểu diễn giải nhúng tự nhiên là các hướng (duy nhất chỉ đến tỷ lệ). Khi chúng ta có một tích vô hướng, chúng tôi định nghĩa biểu diễn giải nhúng chính tắc γ̄W là phần tử của hình nón với ⟨γ̄W, γ̄W⟩ = 1. Điều này cho phép chúng ta định nghĩa các tích vô hướng giữa các biểu diễn giải nhúng.

Không thể xác định của tích vô hướng Chúng ta có thể hy vọng rằng có một tích vô hướng tự nhiên nào đó được chọn ra (xác định) bởi việc huấn luyện mô hình. Hóa ra điều này không phải như vậy. Để hiểu thách thức, hãy xem xét việc biến đổi các không gian giải nhúng và nhúng theo

g(y) ← Aγ(y) + β, l(x) ← A−⊤λ(x), (3.1)

trong đó A ∈ Rd×d là một phép biến đổi tuyến tính khả nghịch nào đó và β ∈ Rd là một hằng số. Dễ thấy rằng phép biến đổi này bảo toàn phân phối softmax P(y|x):

exp(λ(x)⊤γ(y))/∑y′ exp(λ(x)⊤γ(y′)) = exp(l(x)⊤g(y))/∑y′ exp(l(x)⊤g(y′)), ∀x, y.

Tuy nhiên, hàm mục tiêu được sử dụng để huấn luyện mô hình chỉ phụ thuộc vào các biểu diễn thông qua các xác suất softmax. Do đó, biểu diễn γ được xác định (tốt nhất) chỉ đến một phép biến đổi affine khả nghịch nào đó.

Điều này cũng có nghĩa là các biểu diễn khái niệm γ̄W chỉ được xác định đến một phép biến đổi tuyến tính khả nghịch A nào đó. Vấn đề là, với bất kỳ tích vô hướng cố định nào,
⟨γ̄W, γ̄Z⟩ ≠ ⟨Aγ̄W, Aγ̄Z⟩,

nói chung. Theo đó, không có lý do rõ ràng nào để mong đợi rằng các thao tác đại số dựa trên, ví dụ: tích vô hướng Euclidean, có ý nghĩa ngữ nghĩa.

3.1. Tích Vô hướng Nhân quả

Chúng tôi cần một số nguyên tắc bổ sung để chọn một tích vô hướng trên không gian biểu diễn. Trực giác mà chúng tôi theo đuổi ở đây là các khái niệm có thể tách biệt nhân quả nên được biểu diễn như các vector trực giao. Ví dụ, Tiếng Anh ⇒ Tiếng Pháp và Nam ⇒ Nữ, nên trực giao. Chúng tôi định nghĩa một tích vô hướng với tính chất này:

Định nghĩa 3.1 (Tích Vô hướng Nhân quả). Một tích vô hướng nhân quả ⟨·,·⟩C trên Γ̄ ≃ Rd là một tích vô hướng sao cho
⟨γ̄W, γ̄Z⟩C = 0,
cho bất kỳ cặp khái niệm có thể tách biệt nhân quả W và Z.

Lựa chọn này hóa ra có tính chất chủ chốt là nó thống nhất các biểu diễn giải nhúng và nhúng:

Định lý 3.2 (Thống nhất các Biểu diễn). Giả sử rằng, với bất kỳ khái niệm W, tồn tại các khái niệm {Zi}d−1i=1 sao cho mỗi Zi có thể tách biệt nhân quả với W và {γ̄W} ∪ {γ̄Zi}d−1i=1 là một cơ sở của Rd. Nếu ⟨·,·⟩C là một tích vô hướng nhân quả, thì đồng cấu Riesz γ̄ ↦ ⟨γ̄,·⟩C, với γ̄ ∈ Γ̄, ánh xạ biểu diễn giải nhúng γ̄W của mỗi khái niệm W thành biểu diễn nhúng λ̄W của nó:
⟨γ̄W,·⟩C = λ̄⊤W.

Để hiểu kết quả này một cách trực quan, chú ý rằng chúng ta có thể biểu diễn các nhúng như các vector hàng và các giải nhúng như các vector cột.

¹Lưu ý rằng không gian giải nhúng Γ chỉ là một không gian affine, vì softmax bất biến với việc thêm một hằng số.

4

--- TRANG 5 ---
Giả thuyết Biểu diễn Tuyến tính và Hình học của các Mô hình Ngôn ngữ Lớn

Nếu tích vô hướng nhân quả là tích vô hướng Euclidean, đồng cấu sẽ đơn giản là phép toán chuyển vị. Định lý là sự tổng quát hóa (đồng cấu Riesz) của ý tưởng này: mỗi ánh xạ tuyến tính trên Γ̄ tương ứng với một λ ∈ Λ nào đó theo λ⊤: γ̄ ↦ λ⊤γ̄. Vậy, chúng ta có thể ánh xạ Γ̄ thành Λ bằng cách ánh xạ mỗi γ̄W thành một hàm tuyến tính theo γ̄W → ⟨γ̄W,·⟩C. Định lý nói rằng ánh xạ này gửi mỗi biểu diễn giải nhúng của một khái niệm thành biểu diễn nhúng của cùng khái niệm đó.

Trong các thí nghiệm, chúng tôi sẽ sử dụng kết quả này để xây dựng các biểu diễn nhúng từ các biểu diễn giải nhúng. Đặc biệt, điều này cho phép chúng ta tìm các biểu diễn can thiệp của các khái niệm. Điều này quan trọng vì trên thực tế rất khó tìm các cặp lời nhắc thỏa mãn trực tiếp Định nghĩa 2.3.

3.2. Một Dạng Tường minh cho Tích Vô hướng Nhân quả

Vấn đề tiếp theo là: nếu một tích vô hướng nhân quả tồn tại, làm thế nào chúng ta có thể tìm thấy nó? Về nguyên tắc, điều này có thể được thực hiện bằng cách tìm các biểu diễn giải nhúng của một số lượng lớn các khái niệm, và sau đó tìm một tích vô hướng ánh xạ mỗi cặp hướng có thể tách biệt nhân quả thành không. Trên thực tế, điều này không khả thi vì số lượng khái niệm cần thiết để tìm tích vô hướng, và khó khăn trong việc ước lượng các biểu diễn của mỗi khái niệm.

Bây giờ chúng tôi chuyển sang phát triển một cách tiếp cận dễ xử lý hơn dựa trên insight sau: biết giá trị của khái niệm W được thể hiện bởi một từ được chọn ngẫu nhiên cho chúng ta biết ít về giá trị của một khái niệm có thể tách biệt nhân quả Z được thể hiện bởi từ đó. Ví dụ, nếu chúng ta biết rằng một từ được lấy mẫu ngẫu nhiên là tiếng Pháp (không phải tiếng Anh), điều này không cho chúng ta thông tin đáng kể về việc nó chỉ một người đàn ông hay phụ nữ.²

Chúng tôi hình thức hóa ý tưởng này như sau:

Giả định 3.3. Giả sử W, Z là các khái niệm có thể tách biệt nhân quả và γ là một vector giải nhúng được lấy mẫu đồng đều từ từ vựng. Khi đó, λ̄⊤W γ và λ̄⊤Z γ là độc lập³ cho bất kỳ biểu diễn nhúng λ̄W và λ̄Z cho W và Z, tương ứng.

Giả định này cho phép chúng ta kết nối tính tách biệt nhân quả với một điều gì đó chúng ta thực sự có thể đo lường: sự phụ thuộc thống kê giữa các từ. Kết quả tiếp theo làm rõ điều này.

Định lý 3.4 (Dạng Tường minh của Tích Vô hướng Nhân quả). Giả sử tồn tại một tích vô hướng nhân quả, được biểu diễn như ⟨γ̄, γ̄′⟩C = γ̄⊤Mγ̄′ cho một ma trận xác định dương đối xứng M nào đó. Nếu có các khái niệm có thể tách biệt nhân quả lẫn nhau {Wk}dk=1, sao cho các biểu diễn chính tắc của chúng G = [γ̄W1,···, γ̄Wd] tạo thành một cơ sở cho Γ̄ ≃ Rd, thì dưới Giả định 3.3,

M⁻¹ = GG⊤ và G⊤Cov(γ)⁻¹G = D, (3.2)

cho một ma trận chéo D nào đó với các phần tử dương, trong đó γ là vector giải nhúng của một từ được lấy mẫu đồng đều ngẫu nhiên từ từ vựng.

Chú ý rằng tính trực giao nhân quả chỉ đặt ra d(d−1)/2 ràng buộc trên tích vô hướng, nhưng có d(d−1)/2 + d bậc tự do trong việc xác định ma trận xác định dương M (do đó, một tích vô hướng)—vậy, chúng ta mong đợi d bậc tự do trong việc chọn một tích vô hướng nhân quả. Định lý 3.4 đưa ra một đặc trưng của lớp tích vô hướng này, dưới dạng (3.2). Ở đây, D là một tham số tự do với d bậc tự do. Mỗi D định nghĩa tích vô hướng. Chúng tôi không có nguyên tắc để chọn ra một lựa chọn duy nhất của D. Trong các thí nghiệm của chúng tôi, chúng tôi sẽ làm việc với lựa chọn D = Id, điều này cho chúng ta M = Cov(γ)⁻¹. Khi đó, chúng ta có một dạng đóng đơn giản cho tích vô hướng tương ứng:

⟨γ̄, γ̄′⟩C := γ̄⊤Cov(γ)⁻¹γ̄′, ∀γ̄, γ̄′ ∈ Γ̄. (3.3)

Lưu ý rằng mặc dù chúng ta không có một tích vô hướng duy nhất, chúng ta có thể loại trừ hầu hết các tích vô hướng. Ví dụ: tích vô hướng Euclidean không phải là một tích vô hướng nhân quả nếu M = Id không thỏa mãn (3.2) cho bất kỳ D nào.

Biểu diễn thống nhất Lựa chọn tích vô hướng cũng có thể được xem như việc định nghĩa một lựa chọn các biểu diễn g và l trong (3.1) (do đó, ḡ = Aγ̄). Với A = M^(1/2), Định lý 3.2 hơn nữa ngụ ý rằng một tích vô hướng nhân quả làm cho các biểu diễn nhúng và giải nhúng của các khái niệm giống nhau, nghĩa là, ḡW = l̄W. Hơn nữa, trong không gian được biến đổi, tích vô hướng Euclidean chính là tích vô hướng nhân quả: ⟨γ̄, γ̄′⟩C = ḡ⊤ḡ′. Trong Hình 1, chúng tôi đã minh họa sự thống nhất các biểu diễn giải nhúng và nhúng này. Điều này thuận tiện cho các thí nghiệm, vì nó cho phép sử dụng các công cụ Euclidean tiêu chuẩn trên không gian được biến đổi.

4. Thí nghiệm

Bây giờ chúng tôi chuyển sang xác thực thực nghiệm sự tồn tại của các biểu diễn tuyến tính, tích vô hướng nhân quả được ước lượng, và các mối quan hệ được dự đoán giữa các khái niệm không gian con, đo lường, và can thiệp của biểu diễn tuyến tính. Mã nguồn có sẵn tại github.com/KihoPark/linear_repgeometry.

Chúng tôi sử dụng mô hình LLaMA-2 với 7 tỷ tham số (Touvron et al., 2023) làm bệ thử nghiệm. Đây là một LLM Transformer chỉ giải mã (Vaswani et al., 2017; Radford et al., 2018), được huấn luyện sử dụng mục tiêu LM thuận và một từ vựng 32K

²Lưu ý rằng giả định này là về các từ được lấy mẫu ngẫu nhiên từ từ vựng, không phải các từ được lấy mẫu ngẫu nhiên từ các nguồn ngôn ngữ tự nhiên. Trong trường hợp sau, có thể có các tương quan không nhân quả giữa các khái niệm có thể tách biệt nhân quả.

³Thực tế, để chứng minh kết quả tiếp theo của chúng tôi, chúng tôi chỉ cần λ̄⊤W γ và λ̄⊤Z γ không tương quan. Trong Phụ lục D.6, chúng tôi xác minh rằng tích vô hướng nhân quả mà chúng tôi tìm thấy thỏa mãn điều kiện không tương quan.

5

--- TRANG 6 ---
Giả thuyết Biểu diễn Tuyến tính và Hình học của các Mô hình Ngôn ngữ Lớn

token. Chúng tôi bao gồm thêm chi tiết về tất cả các thí nghiệm trong Phụ lục C.

Các khái niệm được biểu diễn như các hướng trong không gian biểu diễn giải nhúng Chúng tôi bắt đầu với giả thuyết rằng các khái niệm được biểu diễn như các hướng trong không gian biểu diễn giải nhúng (Định nghĩa 2.1). Khái niệm này dựa vào các cặp phản thực của các từ chỉ thay đổi về giá trị của khái niệm quan tâm. Chúng tôi xem xét 22 khái niệm được định nghĩa trong Bộ Kiểm tra Tương tự Lớn (BATS 3.0) (Gladkova et al., 2016), cung cấp các cặp phản thực như vậy.⁴ Chúng tôi cũng xem xét 4 khái niệm ngôn ngữ: Tiếng Anh ⇒ Tiếng Pháp, Tiếng Pháp ⇒ Tiếng Đức, Tiếng Pháp ⇒ Tiếng Tây Ban Nha, và Tiếng Đức ⇒ Tiếng Tây Ban Nha, trong đó chúng tôi sử dụng các từ và bản dịch của chúng như các cặp phản thực. Ngoài ra, chúng tôi xem xét khái niệm thường xuyên ⇒ không thường xuyên nắm bắt mức độ phổ biến của một từ—chúng tôi sử dụng các cặp từ đồng nghĩa phổ biến/không phổ biến (ví dụ: "bad" và "terrible") như các cặp phản thực. Chúng tôi cung cấp một bảng tất cả 27 khái niệm mà chúng tôi xem xét trong Phụ lục C.

Nếu khái niệm không gian con của giả thuyết biểu diễn tuyến tính đúng, thì tất cả các cặp token phản thực nên chỉ tới một hướng chung trong không gian giải nhúng. Trên thực tế, điều này sẽ chỉ đúng một cách gần đúng. Tuy nhiên, nếu giả thuyết biểu diễn tuyến tính đúng, chúng ta vẫn mong đợi rằng, ví dụ: γ("queen") − γ("king") sẽ thẳng hàng với hướng nam ⇒ nữ (gần hơn so với sự khác biệt giữa các cặp từ ngẫu nhiên). Để xác thực điều này, với mỗi khái niệm W, chúng tôi xem xét cách hướng được định nghĩa bởi mỗi cặp phản thực, γ(yi(1)) − γ(yi(0)), được thẳng hàng hình học với biểu diễn giải nhúng γ̄W. Chúng tôi ước lượng γ̄W như trung bình (chuẩn hóa)⁵ giữa tất cả các cặp phản thực: γ̄W := γ̃W/√⟨γ̃W, γ̃W⟩C, trong đó

γ̃W = (1/nW) ∑(i=1 to nW) [γ(yi(1)) − γ(yi(0))],

nW biểu thị số lượng cặp phản thực cho W, và ⟨·,·⟩C biểu thị tích vô hướng nhân quả được định nghĩa trong (3.3).

Hình 2 trình bày các biểu đồ tần số của mỗi γ(yi(1)) − γ(yi(0)) được chiếu lên γ̄W đối với tích vô hướng nhân quả. Vì γ̄W được tính bằng γ(yi(1)) − γ(yi(0)), chúng tôi tính mỗi phép chiếu sử dụng một ước lượng leave-one-out (LOO) γ̄W,(−i) của hướng khái niệm loại trừ (yi(0), yi(1)).

Qua ba khái niệm được hiển thị (và 23 khái niệm khác được hiển thị trong Phụ lục D.1), sự khác biệt giữa các cặp phản thực thẳng hàng với γ̄W một cách đáng kể hơn so với những cặp giữa các cặp ngẫu nhiên. Ngoại lệ duy nhất là thing ⇒ part, dường như không có biểu diễn tuyến tính.

⁴Chúng tôi chỉ sử dụng các từ là token đơn trong mô hình LLaMA-2. Xem Phụ lục C để biết chi tiết.
⁵Các nghiên cứu trước về nhúng từ (Drozd et al., 2016; Fournier et al., 2020) động lực việc lấy trung bình để cải thiện tính nhất quán của hướng khái niệm.

[THIS IS FIGURE: Figure 2 showing histograms comparing counterfactual pairs vs random pairs for three concepts: verb3pSg, thingcolor, and countrycapital]

Hình 2. Việc chiếu các cặp phản thực lên hướng khái niệm tương ứng của chúng cho thấy một độ nghiêng phải mạnh, như chúng ta mong đợi nếu giả thuyết biểu diễn tuyến tính đúng. Các phép chiếu của các cặp phản thực, ⟨γ̄W,(−i), γ(yi(1)) − γ(yi(0))⟩C, được hiển thị bằng màu đỏ. Để tham khảo, chúng tôi cũng chiếu sự khác biệt giữa 100K cặp từ được lấy mẫu ngẫu nhiên lên hướng khái niệm được ước lượng, như được hiển thị bằng màu xanh. Xem Bảng 2 để biết chi tiết về mỗi khái niệm W (tiêu đề của mỗi biểu đồ).

[THIS IS FIGURE: Figure 3 showing a heatmap of causal separability between concepts]

Hình 3. Các khái niệm có thể tách biệt nhân quả được biểu diễn một cách gần đúng trực giao dưới tích vô hướng nhân quả được ước lượng dựa trên (3.3). Bản đồ nhiệt hiển thị |⟨γ̄W, γ̄Z⟩C| cho các biểu diễn giải nhúng được ước lượng của mỗi cặp khái niệm (W, Z). Chi tiết cho mỗi khái niệm được đưa ra trong Bảng 2.

Kết quả phù hợp với giả thuyết biểu diễn tuyến tính: sự khác biệt được tính bởi mỗi cặp phản thực chỉ tới một hướng chung đại diện cho một không gian con tuyến tính (đến một số nhiễu). Hơn nữa, γ̄W là một bộ ước lượng hợp lý cho hướng đó.

Tích vô hướng được ước lượng tôn trọng tính tách biệt nhân quả Tiếp theo, chúng tôi trực tiếp kiểm tra liệu tích vô hướng được ước lượng (3.3) được chọn từ Định lý 3.4 có thực sự gần như là một tích vô hướng nhân quả không. Trong Hình 3, chúng tôi vẽ một bản đồ nhiệt của các tích vô hướng giữa tất cả các cặp của các biểu diễn giải nhúng được ước lượng cho 27 khái niệm. Nếu tích vô hướng được ước lượng là một tích vô hướng nhân quả, thì chúng ta mong đợi các giá trị gần 0 giữa các khái niệm có thể tách biệt nhân quả.

Quan sát đầu tiên là hầu hết các cặp khái niệm gần như trực giao đối với tích vô hướng này. Thú vị là, cũng có một cấu trúc chéo khối rõ ràng.

6

--- TRANG 7 ---
Giả thuyết Biểu diễn Tuyến tính và Hình học của các Mô hình Ngôn ngữ Lớn

[THIS IS FIGURE: Two scatter plots showing French/Spanish vs male/female concepts with data points]

Hình 4. Biểu diễn không gian con γ̄W hoạt động như một đầu dò tuyến tính cho W. Các biểu đồ tần số hiển thị γ̄⊤W λ(x^fr_j) vs. γ̄⊤W λ(x^es_j) (trái) và γ̄⊤Z λ(x^fr_j) vs. γ̄⊤Z λ(x^es_j) (phải) cho W = Tiếng Pháp ⇒ Tiếng Tây Ban Nha và Z = nam ⇒ nữ, trong đó {x^fr_j} và {x^es_j} là các ngữ cảnh ngẫu nhiên từ Wikipedia tiếng Pháp và tiếng Tây Ban Nha, tương ứng. Chúng ta cũng thấy rằng γ̄Z không hoạt động như một đầu dò tuyến tính cho W, như mong đợi.

Điều này phát sinh vì các khái niệm được nhóm theo độ tương tự ngữ nghĩa. Ví dụ, 10 khái niệm đầu tiên liên quan đến động từ, và 4 khái niệm cuối cùng là các cặp ngôn ngữ. Cấu trúc không-zero bổ sung cũng thường có ý nghĩa. Ví dụ, lower ⇒ upper (viết hoa, khái niệm 19) có tích vô hướng không tầm thường với các cặp ngôn ngữ khác ngoài Tiếng Pháp ⇒ Tiếng Tây Ban Nha. Điều này có thể là vì tiếng Pháp và tiếng Tây Ban Nha tuân theo các quy tắc viết hoa tương tự, trong khi tiếng Anh và tiếng Đức mỗi cái có các quy ước khác nhau (ví dụ: tiếng Đức viết hoa tất cả danh từ, nhưng tiếng Anh chỉ viết hoa danh từ riêng).

Trong Phụ lục D.2, chúng tôi so sánh tích vô hướng Euclidean với tích vô hướng nhân quả cho cả mô hình LLaMA-2 và một mô hình ngôn ngữ lớn Gemma gần đây hơn (Mesnard et al., 2024).

Các hướng khái niệm hoạt động như các đầu dò tuyến tính Tiếp theo, chúng tôi kiểm tra kết nối với khái niệm đo lường của biểu diễn tuyến tính. Chúng tôi xem xét khái niệm W = Tiếng Pháp ⇒ Tiếng Tây Ban Nha. Để xây dựng một tập dữ liệu các ngữ cảnh tiếng Pháp và tiếng Tây Ban Nha, chúng tôi lấy mẫu các ngữ cảnh có độ dài ngẫu nhiên từ các trang Wikipedia trong mỗi ngôn ngữ. Lưu ý rằng đây không phải là các cặp phản thực. Theo Định lý 2.2, chúng ta mong đợi γ̄⊤W λ(x^fr_j) < 0 và γ̄⊤W λ(x^es_j) > 0. Hình 4 xác nhận kỳ vọng này, cho thấy rằng γ̄W là một đầu dò tuyến tính cho khái niệm W trong Λ (trái). Ngoài ra, biểu diễn của một khái niệm ngoài mục tiêu Z = nam ⇒ nữ không có bất kỳ khả năng dự đoán nào cho nhiệm vụ này (phải). Phụ lục D.3 bao gồm các kết quả tương tự sử dụng tất cả 27 khái niệm.

Các hướng khái niệm ánh xạ thành các biểu diễn can thiệp Định lý 2.5 nói rằng chúng ta có thể xây dựng một biểu diễn can thiệp bằng cách xây dựng một biểu diễn nhúng. Thực hiện điều này trực tiếp đòi hỏi tìm các cặp lời nhắc chỉ thay đổi trên phân phối mà chúng tạo ra trên khái niệm mục tiêu, điều này có thể khó tìm trong thực tế.

Ở đây, chúng tôi sẽ thay vào đó sử dụng đồng cấu giữa các biểu diễn nhúng và giải nhúng (Định lý 3.2) để xây dựng các biểu diễn can thiệp từ các biểu diễn giải nhúng.

[THIS IS FIGURE: Three arrow plots showing intervention effects on different concepts]

Hình 5. Thêm αλ̄C vào λ thay đổi khái niệm mục tiêu C mà không thay đổi các khái niệm ngoài mục tiêu. Các biểu đồ minh họa sự thay đổi trong log(P("queen"|x)/P("king"|x)) và log(P("King"|x)/P("king"|x)), sau khi thay đổi λ(xj) thành λC,α(xj) khi α tăng từ 0 đến 0.4, cho C = nam ⇒ nữ (trái), lower ⇒ upper (giữa), Tiếng Pháp ⇒ Tiếng Tây Ban Nha (phải). Hai đầu của mũi tên là λ(xj) và λC,0.4(xj), tương ứng. Mỗi ngữ cảnh xj được trình bày trong Bảng 4.

Chúng tôi lấy
λ̄W := Cov(γ)^(-1)γ̄W. (4.1)

Định lý 2.5 dự đoán rằng thêm λ̄W vào một biểu diễn ngữ cảnh nên tăng xác suất của W, trong khi để nguyên xác suất của tất cả các khái niệm có thể tách biệt nhân quả.

Để kiểm tra điều này cho một cặp khái niệm có thể tách biệt nhân quả W và Z, trước tiên chúng tôi chọn một bộ bốn {Y(w,z)}w,z∈{0,1}, và sau đó tạo ra các ngữ cảnh {xj} sao cho từ tiếp theo nên là Y(0,0). Ví dụ, nếu W = nam ⇒ nữ và Z = lower ⇒ upper, thì chúng tôi chọn bộ bốn ("king", "queen", "King", "Queen"), và tạo ra các ngữ cảnh sử dụng ChatGPT-4 (ví dụ: "Long live the"). Sau đó chúng tôi can thiệp vào λ(xj) sử dụng λ̄C qua

λC,α(xj) = λ(xj) + αλ̄C, (4.2)

trong đó α > 0 và C có thể là W, Z, hoặc một khái niệm có thể tách biệt nhân quả khác (ví dụ: Tiếng Pháp ⇒ Tiếng Tây Ban Nha). Với các lựa chọn khác nhau của C, chúng tôi vẽ các thay đổi trong logit P(W = 1|Z, λ) và logit P(Z = 1|W, λ), khi chúng tôi tăng α.

Chúng ta mong đợi thấy rằng, nếu chúng ta can thiệp theo hướng W, thì can thiệp nên tăng tuyến tính logit P(W = 1|Z, λ), trong khi logit khác nên giữ không đổi; nếu chúng ta can thiệp theo hướng C có thể tách biệt nhân quả với cả W và Z, thì chúng ta mong đợi cả hai logit đều giữ không đổi.

Hình 5 hiển thị kết quả của một thí nghiệm như vậy được hiển thị cho ba khái niệm mục tiêu (24 khái niệm khác được hiển thị trong Phụ lục D.4), xác nhận kỳ vọng của chúng tôi. Chúng ta thấy, ví dụ, rằng can thiệp theo hướng nam ⇒ nữ làm tăng logit để chọn "queen" thay vì "king" như từ tiếp theo, nhưng không thay đổi logit cho "King" thay vì "king".

Một câu hỏi tiếp theo tự nhiên là xem liệu can thiệp theo hướng khái niệm (cho W) có đẩy xác suất của Y(W = 1) là từ tiếp theo lên cao nhất trong tất cả

7

--- TRANG 8 ---
Giả thuyết Biểu diễn Tuyến tính và Hình học của các Mô hình Ngôn ngữ Lớn

Bảng 1. Thêm biểu diễn can thiệp αλ̄W đẩy xác suất trên các hoàn thành để phản ánh khái niệm W. Khi quy mô can thiệp tăng, xác suất thấy Y(W = 1) ("queen") tăng trong khi xác suất thấy Y(W = 0) ("king") giảm. Chúng tôi hiển thị top-5 từ có xác suất cao nhất trên toàn bộ từ vựng sau can thiệp (4.2) theo hướng W = nam ⇒ nữ, tức là λW,α(x) = λ(x) + αλ̄W, cho α ∈ {0, 0.1, 0.2, 0.3, 0.4}. Ngữ cảnh gốc x = "Long live the " là một đoạn câu kết thúc bằng từ Y(W = 0) ("king"). Các từ có khả năng cao nhất phản ánh khái niệm, với "queen" đứng top-1. Trong Phụ lục D.5, chúng tôi cung cấp thêm ví dụ.

[THIS IS TABLE:
Ranking table showing top 5 words for different alpha values (0, 0.1, 0.2, 0.3, 0.4):
Rank | α=0  | 0.1   | 0.2   | 0.3   | 0.4
1    | king | Queen | queen | queen | queen
2    | King | queen | Queen | Queen | Queen
3    | Queen| king  | lady  | lady  |
4    | queen| King  | lady  | woman | woman
5    |      | king  | women | women |]

token. Chúng ta mong đợi thấy rằng, khi chúng ta tăng giá trị của α, khái niệm mục tiêu cuối cùng nên được phản ánh trong các từ đầu ra có khả năng cao nhất theo LM.

Trong Bảng 1, chúng tôi hiển thị một ví dụ minh họa trong đó W là khái niệm nam ⇒ nữ và ngữ cảnh x là một đoạn câu có thể kết thúc bằng từ Y(W = 0) ("king"). Với x = "Long live the ", khi chúng tôi tăng quy mô α trên can thiệp, chúng ta thấy rằng từ mục tiêu Y(W = 1) ("queen") trở thành từ tiếp theo có khả năng cao nhất, trong khi từ gốc Y(W = 0) rơi xuống dưới danh sách top-5. Điều này minh họa cách can thiệp có thể đẩy xác suất của từ mục tiêu lên đủ cao để làm cho nó trở thành từ có khả năng cao nhất trong khi giảm xác suất của từ gốc.

5. Thảo luận và Công trình Liên quan

Ý tưởng rằng các khái niệm cấp cao được mã hóa tuyến tính là hấp dẫn vì—nếu nó đúng—nó có thể mở ra các phương pháp đơn giản để giải thích và kiểm soát các LLM. Trong bài báo này, chúng tôi đã hình thức hóa 'biểu diễn tuyến tính', và chỉ ra rằng tất cả các biến thể tự nhiên của khái niệm này có thể được thống nhất.⁶ Sự tương đương này đã gợi ý một số cách tiếp cận cho giải thích và kiểm soát—ví dụ: chúng tôi chỉ ra cách sử dụng các bộ sưu tập các cặp từ để định nghĩa các hướng khái niệm, và sau đó sử dụng những hướng này để dự đoán đầu ra của mô hình sẽ là gì, và để thay đổi đầu ra một cách có kiểm soát. Một chủ đề chính là vai trò được chơi bởi lựa chọn tích vô hướng.

Các không gian con tuyến tính trong biểu diễn ngôn ngữ Giả thuyết không gian con tuyến tính ban đầu được quan sát thực nghiệm trong bối cảnh nhúng từ (ví dụ: Mikolov et al., 2013b;c; Levy & Goldberg, 2014; Goldberg & Levy, 2014; Vylomova et al., 2016; Gladkova et al., 2016; Chiang et al., 2020; Fournier et al., 2020). Cấu trúc tương tự đã được quan sát trong nhúng từ đa ngôn ngữ (Mikolov et al., 2013a; Lample et al., 2018; Ruder et al., 2019; Peng et al., 2022), nhúng câu (Bowman et al., 2016; Zhu & de Melo, 2020; Li et al., 2020; Ushio et al., 2021), không gian biểu diễn của Transformer LLM (Meng et al., 2022; Merullo et al., 2023; Hernandez et al., 2023), và các mô hình thị giác-ngôn ngữ (Wang et al., 2023; Trager et al., 2023; Perera et al., 2023). Những quan sát này làm động lực cho Định nghĩa 2.1. Ý tưởng chính trong bài báo hiện tại là cung cấp hình thức hóa theo các cặp phản thực—đây là điều cho phép chúng tôi kết nối với các khái niệm khác về biểu diễn tuyến tính, và xác định cấu trúc tích vô hướng.

Đo lường, can thiệp, và khả năng giải thích cơ học Có một khối lượng lớn công trình về biểu diễn tuyến tính để giải thích (thăm dò) (ví dụ: Alain & Bengio, 2017; Kim et al., 2018; nostalgebraist, 2020; Rogers et al., 2021; Belinkov, 2022; Li et al., 2022; Geva et al., 2022; Nanda et al., 2023) và kiểm soát (điều khiển) (ví dụ: Wang et al., 2023; Turner et al., 2023; Merullo et al., 2023; Trager et al., 2023) các mô hình. Điều này đặc biệt nổi bật trong khả năng giải thích cơ học (Elhage et al., 2021; Meng et al., 2022; Hernandez et al., 2023; Turner et al., 2023; Zou et al., 2023; Todd et al., 2023; Hendel et al., 2023). Đối với khối lượng công trình này, đóng góp chính của bài báo hiện tại là làm rõ giả thuyết biểu diễn tuyến tính, và vai trò quan trọng của tích vô hướng. Tuy nhiên, chúng tôi không giải quyết khả năng giải thích của các tham số mô hình, cũng không phải các kích hoạt của các lớp trung gian. Đây là trọng tâm chính của công trình hiện có. Đó là một hướng thú vị cho công trình tương lai để hiểu cách các ý tưởng ở đây—đặc biệt là tích vô hướng nhân quả—chuyển sang những bối cảnh này.

Hình học của biểu diễn Có một dòng công trình nghiên cứu hình học của biểu diễn từ và câu (ví dụ: Arora et al., 2016; Mimno & Thompson, 2017; Ethayarajh, 2019; Reif et al., 2019; Li et al., 2020; Hewitt & Manning, 2019; Chen et al., 2021; Chang et al., 2022; Jiang et al., 2023). Công trình này xem xét, ví dụ: việc trực quan hóa và mô hình hóa cách các nhúng được học được phân phối, hoặc cách cấu trúc phân cấp được mã hóa. Công trình của chúng tôi phần lớn trực giao với những cái này, vì chúng tôi đang cố gắng định nghĩa một tích vô hướng phù hợp (và do đó, các khái niệm về độ tương tự và phép chiếu) tôn trọng cấu trúc ngữ nghĩa của ngôn ngữ.

Học biểu diễn nhân quả Cuối cùng, các ý tưởng ở đây kết nối với học biểu diễn nhân quả (ví dụ: Higgins et al., 2016; Hyvarinen & Morioka, 2016; Higgins et al., 2018; Khemakhem et al., 2020; Zimmermann et al., 2021; Schölkopf et al., 2021; Moran et al., 2021; Wang et al., 2023). Rõ ràng nhất, hình thức hóa nhân quả của chúng tôi về các khái niệm được lấy cảm hứng từ Wang et al. (2023), người thiết lập một đặc trưng của các khái niệm ẩn và đại số vector trong các mô hình khuếch tán. Riêng biệt, một chủ đề chính trong văn học này là khả năng xác định của các biểu diễn được học—tức là mức độ chúng nắm bắt cấu trúc thế giới thực bên dưới. Kết quả tích vô hướng nhân quả của chúng tôi có thể được xem trong chủ đề này, cho thấy rằng một tích vô hướng tôn trọng sự gần gũi ngữ nghĩa không được xác định bởi quy trình huấn luyện thông thường, nhưng nó có thể được chọn ra với một giả định phù hợp.

Lời cảm ơn
Cảm ơn Gemma Moran vì các nhận xét về bản thảo trước đó. Công trình này được hỗ trợ bởi tài trợ ONR N00014-23-1-2591 và Open Philanthropy.

Tài liệu tham khảo

⁶Trong Phụ lục A, chúng tôi tóm tắt những kết quả này trong một hình.

8

--- TRANG 9 ---
Giả thuyết Biểu diễn Tuyến tính và Hình học của các Mô hình Ngôn ngữ Lớn

Alain, G. và Bengio, Y. Understanding intermediate layers using linear classifier probes. Trong International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=ryF7rTqgl.

Arora, S., Li, Y., Liang, Y., Ma, T., và Risteski, A. A latent variable model approach to PMI-based word embeddings. Transactions of the Association for Computational Linguistics, 4:385–399, 2016.

Belinkov, Y. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1):207–219, 2022.

Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A., Jozefowicz, R., và Bengio, S. Generating sentences from a continuous space. Trong Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, pp. 10–21, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/K16-1002. URL https://aclanthology.org/K16-1002.

Chang, T., Tu, Z., và Bergen, B. The geometry of multilingual language model representations. Trong Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 119–136, 2022.

Chen, B., Fu, Y., Xu, G., Xie, P., Tan, C., Chen, M., và Jing, L. Probing BERT in hyperbolic spaces. Trong International Conference on Learning Representations, 2021.

Chiang, H.-Y., Camacho-Collados, J., và Pardos, Z. Understanding the source of semantic regularities in word embeddings. Trong Proceedings of the 24th Conference on Computational Natural Language Learning, pp. 119–131, 2020.

Choe, Y. J., Park, K., và Kim, D. word2word: A collection of bilingual lexicons for 3,564 language pairs. Trong Proceedings of the Twelfth Language Resources and Evaluation Conference, pp. 3036–3045, 2020.

Drozd, A., Gladkova, A., và Matsuoka, S. Word embeddings, analogies, and machine learning: Beyond king - man + woman = queen. Trong Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical papers, pp. 3519–3530, 2016.

Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., et al. A mathematical framework for transformer circuits. Transformer Circuits Thread, 1, 2021.

Elhage, N., Hume, T., Olsson, C., Schiefer, N., Henighan, T., Kravec, S., Hatfield-Dodds, Z., Lasenby, R., Drain, D., Chen, C., et al. Toy models of superposition. arXiv preprint arXiv:2209.10652, 2022.

Ethayarajh, K. How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings. Trong Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 55–65, 2019.

Fournier, L., Dupoux, E., và Dunbar, E. Analogies minus analogy test: measuring regularities in word embeddings. Trong Proceedings of the 24th Conference on Computational Natural Language Learning, pp. 365–375, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.conll-1.29. URL https://aclanthology.org/2020.conll-1.29.

Geva, M., Caciularu, A., Wang, K., và Goldberg, Y. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. Trong Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 30–45, 2022.

Gladkova, A., Drozd, A., và Matsuoka, S. Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't. Trong Proceedings of the NAACL Student Research Workshop, pp. 8–15, 2016.

Goldberg, Y. và Levy, O. word2vec explained: deriving Mikolov et al.'s negative-sampling word-embedding method. arXiv preprint arXiv:1402.3722, 2014.

Gurnee, W. và Tegmark, M. Language models represent space and time. arXiv preprint arXiv:2310.02207, art. arXiv:2310.02207, October 2023. doi: 10.48550/arXiv.2310.02207.

Hendel, R., Geva, M., và Globerson, A. In-context learning creates task vectors. arXiv preprint arXiv:2310.15916, 2023.

9

--- TRANG 10 ---
Giả thuyết Biểu diễn Tuyến tính và Hình học của các Mô hình Ngôn ngữ Lớn

Hernandez, E., Sharma, A. S., Haklay, T., Meng, K., Wattenberg, M., Andreas, J., Belinkov, Y., và Bau, D. Linearity of relation decoding in transformer language models. arXiv preprint arXiv:2308.09124, 2023.

Hewitt, J. và Manning, C. D. A structural probe for finding syntax in word representations. Trong Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4129–4138, 2019.

Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., và Lerchner, A. beta-VAE: Learning basic visual concepts with a constrained variational framework. Trong International Conference on Learning Representations, 2016.

Higgins, I., Amos, D., Pfau, D., Racaniere, S., Matthey, L., Rezende, D., và Lerchner, A. Towards a definition of disentangled representations. arXiv preprint arXiv:1812.02230, 2018.

Hyvarinen, A. và Morioka, H. Unsupervised feature extraction by time-contrastive learning and nonlinear ICA. Advances in Neural Information Processing Systems, 29, 2016.

Jiang, Y., Aragam, B., và Veitch, V. Uncovering meanings of embeddings via partial orthogonality. arXiv preprint arXiv:2310.17611, 2023.

Khemakhem, I., Kingma, D., Monti, R., và Hyvarinen, A. Variational autoencoders and nonlinear ICA: A unifying framework. Trong International Conference on Artificial Intelligence and Statistics, pp. 2207–2217. PMLR, 2020.

Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F., et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV). Trong International Conference on Machine Learning, pp. 2668–2677. PMLR, 2018.

Kudo, T. và Richardson, J. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. Trong Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 66–71, 2018.

Lample, G., Conneau, A., Ranzato, M., Denoyer, L., và Jégou, H. Word translation without parallel data. Trong International Conference on Learning Representations, 2018.

Levy, O. và Goldberg, Y. Linguistic regularities in sparse and explicit word representations. Trong Proceedings of the Eighteenth Conference on Computational Natural Language Learning, pp. 171–180, 2014.

Li, B., Zhou, H., He, J., Wang, M., Yang, Y., và Li, L. On the sentence embeddings from pre-trained language models. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 9119–9130, 2020.

Li, K., Hopkins, A. K., Bau, D., Viégas, F., Pfister, H., và Wattenberg, M. Emergent world representations: Exploring a sequence model trained on a synthetic task. Trong International Conference on Learning Representations, 2022.

Meng, K., Bau, D., Andonian, A., và Belinkov, Y. Locating and editing factual associations in GPT. Advances in Neural Information Processing Systems, 35:17359–17372, 2022.

Merullo, J., Eickhoff, C., và Pavlick, E. Language models implement simple word2vec-style vector arithmetic. arXiv preprint arXiv:2305.16130, 2023.

Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L., Rivière, M., Kale, M. S., Love, J., et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.

Mikolov, T., Le, Q. V., và Sutskever, I. Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168, 2013a.

Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., và Dean, J. Distributed representations of words and phrases and their compositionality. Advances in Neural Information Processing Systems, 26, 2013b.

Mikolov, T., Yih, W.-T., và Zweig, G. Linguistic regularities in continuous space word representations. Trong Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 746–751, 2013c.

Mimno, D. và Thompson, L. The strange geometry of skip-gram with negative sampling. Trong Palmer, M., Hwa, R., và Riedel, S. (eds.), Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2873–2878, Copenhagen, Denmark, 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1308. URL https://aclanthology.org/D17-1308.

Moran, G. E., Sridhar, D., Wang, Y., và Blei, D. M. Identifiable deep generative models via sparse decoding. arXiv preprint arXiv:2110.10804, art. arXiv:2110.10804, October 2021. doi: 10.48550/arXiv.2110.10804.

Nanda, N., Lee, A., và Wattenberg, M. Emergent linear representations in world models of self-supervised sequence models. arXiv preprint arXiv:2309.00941, 2023.

10

--- TRANG 11 ---
Giả thuyết Biểu diễn Tuyến tính và Hình học của các Mô hình Ngôn ngữ Lớn

nostalgebraist. Interpreting GPT: the logit lens, 2020. URL https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens.

OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

Peng, X., Stevenson, M., Lin, C., và Li, C. Understanding linearity of cross-lingual word embedding mappings. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https://openreview.net/forum?id=8HuyXvbvqX.

Pennington, J., Socher, R., và Manning, C. D. GloVe: Global vectors for word representation. Trong Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1532–1543, 2014.

Perera, P., Trager, M., Zancato, L., Achille, A., và Soatto, S. Prompt algebra for task composition. arXiv preprint arXiv:2306.00310, 2023.

Radford, A., Narasimhan, K., Salimans, T., và Sutskever, I. Improving language understanding by generative pre-training. 2018.

Reif, E., Yuan, A., Wattenberg, M., Viegas, F. B., Coenen, A., Pearce, A., và Kim, B. Visualizing and measuring the geometry of BERT. Advances in Neural Information Processing Systems, 32, 2019.

Rogers, A., Kovaleva, O., và Rumshisky, A. A primer in BERTology: What we know about how BERT works. Transactions of the Association for Computational Linguistics, 8:842–866, 2021.

Ruder, S., Vulić, I., và Søgaard, A. A survey of cross-lingual word embedding models. Journal of Artificial Intelligence Research, 65:569–631, 2019.

Schölkopf, B., Locatello, F., Bauer, S., Ke, N. R., Kalchbrenner, N., Goyal, A., và Bengio, Y. Toward causal representation learning. Proceedings of the IEEE, 109(5):612–634, 2021.

Todd, E., Li, M. L., Sharma, A. S., Mueller, A., Wallace, B. C., và Bau, D. Function vectors in large language models. arXiv preprint arXiv:2310.15213, 2023.

Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., và Scialom, T. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Trager, M., Perera, P., Zancato, L., Achille, A., Bhatia, P., và Soatto, S. Linear spaces of meanings: Compositional structures in vision-language models. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 15395–15404, 2023.

Turner, A. M., Thiergart, L., Udell, D., Leech, G., Mini, U., và MacDiarmid, M. Activation addition: Steering language models without optimization. arXiv preprint arXiv:2308.10248, art. arXiv:2308.10248, August 2023. doi: 10.48550/arXiv.2308.10248.

Ushio, A., Anke, L. E., Schockaert, S., và Camacho-Collados, J. BERT is to NLP what AlexNet is to CV: Can pre-trained language models identify analogies? Trong Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 3609–3624, 2021.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., và Polosukhin, I. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017.

Vylomova, E., Rimell, L., Cohn, T., và Baldwin, T. Take and took, gaggle and goose, book and read: Evaluating the utility of vector differences for lexical relation learning. Trong Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1671–1682, 2016.

Wang, Z., Gui, L., Negrea, J., và Veitch, V. Concept algebra for score-based conditional models. arXiv preprint arXiv:2302.03693, 2023.

Zhu, X. và de Melo, G. Sentence analogies: Linguistic regularities in sentence embeddings. Trong Proceedings of the 28th International Conference on Computational Linguistics, pp. 3389–3400, 2020.

Zimmermann, R. S., Sharma, Y., Schneider, S., Bethge, M., và Brendel, W. Contrastive learning inverts the data generating process. Trong International Conference on Machine Learning, pp. 12979–12990. PMLR, 2021.

Zou, A., Phan, L., Chen, S., Campbell, J., Guo, P., Ren, R., Pan, A., Yin, X., Mazeika, M., Dombrowski, A.-K.,

11

--- TRANG 12 ---
Giả thuyết Biểu diễn Tuyến tính và Hình học của các Mô hình Ngôn ngữ Lớn

Goel, S., Li, N., Byun, M. J., Wang, Z., Mallen, A., Basart, S., Koyejo, S., Song, D., Fredrikson, M., Kolter, Z., và Hendrycks, D. Representation engineering: A top-down approach to AI transparency. arXiv preprint arXiv:2310.01405, 2023.

12

--- TRANG 13 ---
Giả thuyết Biểu diễn Tuyến tính và Hình học của các Mô hình Ngôn ngữ Lớn

A. Tóm tắt Kết quả Chính

Trong Hình 6, chúng tôi đưa ra một tóm tắt cấp cao về các kết quả chính của chúng tôi. Trong Phần 2, chúng tôi đã đưa ra định nghĩa của các biểu diễn giải nhúng và nhúng và cách chúng cũng tạo ra các biểu diễn đo lường và can thiệp, tương ứng. Trong Phần 3, chúng tôi đã định nghĩa tích vô hướng nhân quả và chỉ ra cách nó thống nhất các biểu diễn giải nhúng và nhúng thông qua đồng cấu Riesz được tạo ra.

[THIS IS FIGURE: Diagram showing connections between Embedding (λ̄W in Definition 2.3) and Unembedding (γ̄W in Definition 2.1), with arrows pointing to Intervention and Measurement respectively through Theorem 2.5 and Theorem 2.2, plus a "Unification via Causal Inner Product (Theorem 3.2)" connection between them]

Hình 6. Một tóm tắt cấp cao về các kết quả chính của chúng tôi, minh họa các kết nối giữa các khái niệm khác nhau về biểu diễn tuyến tính.

B. Chứng minh

B.1. Chứng minh Định lý 2.2

Định lý 2.2 (Biểu diễn Đo lường). Cho W là một khái niệm, và cho γ̄W là biểu diễn giải nhúng của W. Khi đó, với bất kỳ nhúng ngữ cảnh λ ∈ Λ,
logit P(Y = Y(1)|Y ∈ {Y(0), Y(1)}, λ) = αλ⊤γ̄W,
trong đó α > 0 (h.k.c.c.) là một hàm của {Y(0), Y(1)}.

Chứng minh. Chứng minh bao gồm việc viết ra phân phối lấy mẫu softmax và gọi Định nghĩa 2.1.

logit P(Y = Y(1)|Y ∈ {Y(0), Y(1)}, λ) (B.1)
= log P(Y = Y(1)|Y ∈ {Y(0), Y(1)}, λ)/P(Y = Y(0)|Y ∈ {Y(0), Y(1)}, λ) (B.2)
= λ⊤{γ(Y(1)) − γ(Y(0))} (B.3)
= α · λ⊤γ̄W. (B.4)

Trong (B.3), chúng tôi đơn giản viết ra phân phối softmax, cho phép chúng ta hủy các hằng số chuẩn hóa cho hai xác suất. Phương trình (B.4) theo trực tiếp từ Định nghĩa 2.1; lưu ý rằng tính ngẫu nhiên của α đến từ tính ngẫu nhiên của {Y(0), Y(1)}.

B.2. Chứng minh Bổ đề 2.4

Bổ đề B.1 (Mối quan hệ Giải nhúng-Nhúng). Cho λ̄W là biểu diễn nhúng của một khái niệm W, và cho γ̄W và γ̄Z là các biểu diễn giải nhúng cho W và bất kỳ khái niệm Z nào có thể tách biệt nhân quả với W. Khi đó,
λ̄⊤W γ̄W > 0 và λ̄⊤W γ̄Z = 0. (2.1)

Ngược lại, nếu một biểu diễn λ̄W thỏa mãn (2.1), và nếu tồn tại các khái niệm {Zi}d−1i=1, sao cho mỗi Zi có thể tách biệt nhân quả với W và {γ̄W} ∪ {γ̄Zi}d−1i=1 là cơ sở của Rd, thì λ̄W là biểu diễn nhúng cho W.

Chứng minh. Cho λ0, λ1 là một cặp nhúng sao cho

P(W = 1|λ1)/P(W = 1|λ0) > 1 và P(W, Z|λ1)/P(W, Z|λ0) = P(W|λ1)/P(W|λ0), (B.5)

cho bất kỳ khái niệm Z nào có thể tách biệt nhân quả với W. Khi đó, theo Định nghĩa 2.3,
λ1 − λ0 ∈ Cone(λ̄W). (B.6)

Điều kiện (B.5) tương đương với

P(W = 1|λ1)/P(W = 1|λ0) > 1 và P(Z = 1|W, λ1)/P(Z = 1|W, λ0) = 1. (B.7)

Hai điều kiện này cũng tương đương với cặp điều kiện sau, tương ứng:

P(Y = Y(1)|Y ∈ {Y(0), Y(1)}, λ1)/P(Y = Y(1)|Y ∈ {Y(0), Y(1)}, λ0) > 1 (B.8)

và

P(Y = Y(W,1)|Y ∈ {Y(W,0), Y(W,1)}, λ1)/P(Y = Y(W,1)|Y ∈ {Y(W,0), Y(W,1)}, λ0) = 1 (B.9)

Lý do là, có điều kiện trên Y ∈ {Y(0,0), Y(0,1), Y(1,0), Y(1,1)}, có điều kiện trên W tương đương với có điều kiện trên Y ∈ {Y(W,0), Y(W,1)}. Và, sự kiện Z = 1 tương đương với sự kiện Y = Y(W,1). (Nói cách khác: nếu chúng ta biết đầu ra là một trong "king", "queen", "roi", "reine" thì có điều kiện trên W = 1 tương đương với có điều kiện trên đầu ra là "king" hoặc "roi". Khi đó, dự đoán liệu từ có phải tiếng Anh không tương đương với dự đoán liệu từ có phải "king" không.)

Theo Định lý 2.2, hai điều kiện (B.8) và (B.9) tương đương với
α(Y(0), Y(1))(λ1 − λ0)⊤γ̄W > 0 và α(Y(W,0), Y(W,1))(λ1 − λ0)⊤γ̄Z = 0, (B.10)

trong đó các α là dương h.k.c.c. Những cái này lần lượt tương đương với
λ̄⊤W γ̄W > 0 và λ̄⊤W γ̄Z = 0. (B.11)

Ngược lại, nếu một biểu diễn λ̄W thỏa mãn (B.11) và tồn tại các khái niệm {Zi}d−1i=1 sao cho mỗi khái niệm có thể tách biệt nhân quả với W và {γ̄W} ∪ {γ̄Zi}d−1i=1 là cơ sở của Rd, thì λ̄W là duy nhất đến việc chia tỷ lệ dương. Nếu tồn tại λ0 và λ1 thỏa mãn (B.5), thì sự tương đương giữa (B.5) và (B.10) nói rằng
(λ1 − λ0)⊤γ̄W > 0 và (λ1 − λ0)⊤γ̄Z = 0. (B.12)

Nói cách khác, λ1 − λ0 cũng thỏa mãn (B.11), ngụ ý rằng nó phải giống với λ̄W đến việc chia tỷ lệ dương. Do đó, với bất kỳ λ0 và λ1 thỏa mãn (B.5), λ1 − λ0 ∈ Cone(λ̄W).

B.3. Chứng minh Định lý 2.5

Định lý 2.5 (Biểu diễn Can thiệp). Cho λ̄W là biểu diễn nhúng của một khái niệm W. Khi đó, với bất kỳ khái niệm Z nào có thể tách biệt nhân quả với W,

P(Y = Y(W,1)|Y ∈ {Y(W,0), Y(W,1)}, λ + cλ̄W)

là hằng số theo c ∈ R, và

P(Y = Y(1,Z)|Y ∈ {Y(0,Z), Y(1,Z)}, λ + cλ̄W)

tăng theo c ∈ R.

Chứng minh. Theo Định lý 2.2,

logit P(Y = Y(W,1)|Y ∈ {Y(W,0), Y(W,1)}, λ + cλ̄W) (B.13)
= α · (λ + cλ̄W)⊤γ̄Z (B.14)
= α · λ⊤γ̄Z + αc · λ̄⊤W γ̄Z (B.15)

Do đó, xác suất đầu tiên là hằng số vì λ̄⊤W γ̄Z = 0 theo Bổ đề 2.4.

Ngoài ra, theo Định lý 2.2,

logit P(Y = Y(1,Z)|Y ∈ {Y(0,Z), Y(1,Z)}, λ + cλ̄W) (B.16)
= α · (λ + cλ̄W)⊤γ̄W (B.17)
= α · λ⊤γ̄Z + αc · λ̄⊤W γ̄W (B.18)

Do đó, xác suất thứ hai tăng vì λ̄⊤W γ̄W > 0 theo Bổ đề 2.4.

13

--- TRANG 14 ---
Giả thuyết Biểu diễn Tuyến tính và Hình học của các Mô hình Ngôn ngữ Lớn

B.4. Chứng minh Định lý 3.2

Định lý 3.2 (Thống nhất các Biểu diễn). Giả sử rằng, với bất kỳ khái niệm W, tồn tại các khái niệm {Zi}d−1i=1 sao cho mỗi Zi có thể tách biệt nhân quả với W và {γ̄W} ∪ {γ̄Zi}d−1i=1 là một cơ sở của Rd. Nếu ⟨·,·⟩C là một tích vô hướng nhân quả, thì đồng cấu Riesz ϕ sao cho ϕ(γ̄) = ⟨γ̄,·⟩C ánh xạ biểu diễn giải nhúng γ̄W của mỗi khái niệm W thành biểu diễn nhúng λ̄W của nó:
⟨γ̄W,·⟩C = λ̄⊤W.

Chứng minh. Tích vô hướng nhân quả định nghĩa đồng cấu Riesz ϕ sao cho ϕ(γ̄) = ⟨γ̄,·⟩C. Khi đó, chúng ta có
ϕ(γ̄W)(γ̄W) = ⟨γ̄W, γ̄W⟩C > 0 và ϕ(γ̄W)(γ̄Z) = ⟨γ̄W, γ̄Z⟩C = 0, (B.19)

trong đó đẳng thức thứ hai theo từ Định nghĩa 3.1. Theo Bổ đề 2.4, ϕ(γ̄W) thể hiện biểu diễn giải nhúng duy nhất λ̄W (đến việc chia tỷ lệ dương); cụ thể, ϕ(γ̄W) = λ̄⊤W trong đó λ̄⊤W: γ̄ ↦ λ̄⊤W γ̄.

B.5. Chứng minh Định lý 3.4

Định lý 3.4 (Dạng Tường minh của Tích Vô hướng Nhân quả). Giả sử tồn tại một tích vô hướng nhân quả, được biểu diễn như ⟨γ̄, γ̄′⟩C = γ̄⊤Mγ̄′ cho một ma trận xác định dương đối xứng M nào đó. Nếu có các khái niệm có thể tách biệt nhân quả lẫn nhau {Wk}dk=1, sao cho các biểu diễn chính tắc của chúng G = [γ̄W1,···, γ̄Wd] tạo thành một cơ sở cho Γ̄ ≃ Rd, thì dưới Giả định 3.3,

M−1 = GG⊤ và G⊤Cov(γ)−1G = D, (3.2)

cho một ma trận chéo D nào đó với các phần tử dương, trong đó γ là vector giải nhúng của một từ được lấy mẫu đồng đều ngẫu nhiên từ từ vựng.

Chứng minh. Vì ⟨·,·⟩C là một tích vô hướng nhân quả,
0 = γ̄⊤W Mγ̄Z (B.20)

cho bất kỳ khái niệm có thể tách biệt nhân quả W và Z. Bằng cách áp dụng (B.20) cho các biểu diễn chính tắc G = [γ̄W1,···, γ̄Wd], chúng ta có được
I = G⊤MG. (B.21)

Điều này cho thấy rằng M = G−⊤G−1, chứng minh nửa đầu của (3.2).

Tiếp theo, quan sát rằng Mγ̄Wi là một biểu diễn nhúng cho mỗi khái niệm Wi với i = 1,···, d theo chứng minh của Bổ đề 2.4 và Định lý 3.2. Khi đó, theo Giả định 3.3,
0 = Cov(γ̄⊤Wi Mγ, γ̄⊤Wj Mγ) (B.22)
= γ̄⊤Wi MCov(γ)Mγ̄Wj. (B.23)

cho i ≠ j. Vậy,
D−1 = G⊤MCov(γ)MG, (B.24)

cho một ma trận chéo D nào đó với các phần tử dương. Thay thế vào M = G−⊤G−1, chúng ta có được
Cov(γ) = GD−1G⊤, (B.25)

chứng minh nửa thứ hai của (3.2).

15

--- TRANG 15 ---
Giả thuyết Biểu diễn Tuyến tính và Hình học của các Mô hình Ngôn ngữ Lớn

Bảng 2. Tên khái niệm, một ví dụ về các cặp phản thực, và số lượng các cặp được sử dụng

| # | Khái niệm | Ví dụ | Số lượng |
|---|-----------|-------|----------|
| 1 | verb ⇒ 3pSg | (accept, accepts) | 32 |
| 2 | verb ⇒ Ving | (add, adding) | 31 |
| 3 | verb ⇒ Ved | (accept, accepted) | 47 |
| 4 | Ving ⇒ 3pSg | (adding, adds) | 27 |
| 5 | Ving ⇒ Ved | (adding, added) | 34 |
| 6 | 3pSg ⇒ Ved | (adds, added) | 29 |
| 7 | verb ⇒ V+able | (accept, acceptable) | 6 |
| 8 | verb ⇒ V+er | (begin, beginner) | 14 |
| 9 | verb ⇒ V+tion | (compile, compilation) | 8 |
| 10 | verb ⇒ V+ment | (agree, agreement) | 11 |
| 11 | adj ⇒ un+adj | (able, unable) | 5 |
| 12 | adj ⇒ adj+ly | (according, accordingly) | 18 |
| 13 | small ⇒ big | (brief, long) | 20 |
| 14 | thing ⇒ color | (ant, black) | 21 |
| 15 | thing ⇒ part | (bus, seats) | 13 |
| 16 | country ⇒ capital | (Austria, Vienna) | 15 |
| 17 | pronoun ⇒ possessive | (he, his) | 4 |
| 18 | male ⇒ female | (actor, actress) | 11 |
| 19 | lower ⇒ upper | (always, Always) | 34 |
| 20 | noun ⇒ plural | (album, albums) | 63 |
| 21 | adj ⇒ comparative | (bad, worse) | 19 |
| 22 | adj ⇒ superlative | (bad, worst) | 9 |
| 23 | frequent ⇒ infrequent | (bad, terrible) | 32 |
| 24 | English ⇒ French | (April, avril) | 46 |
| 25 | French ⇒ German | (ami, Freund) | 35 |
| 26 | French ⇒ Spanish | (année, año) | 35 |
| 27 | German ⇒ Spanish | (Arbeit, trabajo) | 22 |

C. Chi tiết Thí nghiệm

Mô hình LLaMA-2 Chúng tôi sử dụng biến thể llama-2-7b của mô hình LLaMA-2 (Touvron et al., 2023), có thể truy cập trực tuyến (với sự cho phép) qua thư viện huggingface.⁷ Bảy tỷ tham số của nó được huấn luyện trước trên hai nghìn tỷ token sentencepiece (Kudo & Richardson, 2018), 90% trong số đó bằng tiếng Anh. Mô hình này sử dụng 32,000 token và 4,096 chiều cho các nhúng token của nó.

Các cặp phản thực Tokenization đặt ra một thách thức trong việc sử dụng một số từ. Đầu tiên, một từ có thể được tokenize thành nhiều hơn một token. Ví dụ, một từ "princess" được tokenize thành "prin" + "cess", và γ("princess") không tồn tại. Vậy, chúng ta không thể có được ý nghĩa của từ chính xác "princess". Thứ hai, một từ có thể được sử dụng như một trong các token cho từ khác. Ví dụ, các từ tiếng Pháp "bas" và "est" ("down" và "east" trong tiếng Anh) nằm trong các token cho các từ "basalt", "baseline", "basil", "basilica", "basin", "estuary", "estrange", "estoppel", "estival", "esthetics", và "estrogen". Do đó, một từ có thể có ý nghĩa khác ngoài ý nghĩa của từ chính xác.

Khi chúng tôi thu thập các cặp phản thực để xác định γ̄W, vấn đề đầu tiên trong cặp có thể được xử lý bằng cách không sử dụng nó. Tuy nhiên, vấn đề thứ hai không thể được xử lý, và nó tạo ra nhiều nhiễu cho kết quả của chúng tôi. Bảng 2 trình bày số lượng các cặp phản thực cho mỗi khái niệm và một ví dụ về các cặp. Các cặp cho khái niệm thứ 13, 17, 19, 23-27 được tạo ra bởi ChatGPT-4 (OpenAI, 2023), và những cái cho khái niệm thứ 16 dựa trên tệp csv⁸). Các khái niệm khác dựa trên Bộ Kiểm tra Tương tự Lớn hơn (BATS) (Gladkova et al., 2016), phiên bản 3.0⁹, được sử dụng để đánh giá tác vụ tương tự từ.

Mẫu ngữ cảnh Trong Phần 4, cho một khái niệm W (ví dụ: Tiếng Anh ⇒ Tiếng Pháp), chúng tôi chọn một số cặp phản thực (Y(0), Y(1)) (ví dụ: (house, maison)), sau đó lấy mẫu ngữ cảnh {x⁰ⱼ} và {x¹ⱼ} mà token tiếp theo là Y(0) và Y(1), tương ứng, từ Wikipedia. Những cặp token tiếp theo này được thu thập từ từ điển song ngữ word2word (Choe et al., 2020), là một

⁷https://huggingface.co/meta-llama/Llama-2-7b-hf
⁸https://github.com/jmerullo/lm_vector_arithmetic/blob/main/world_capitals.csv
⁹https://vecto.space/projects/BATS/

16

--- TRANG 16 ---
Giả thuyết Biểu diễn Tuyến tính và Hình học của các Mô hình Ngôn ngữ Lớn

Bảng 3. Các khái niệm được sử dụng để điều tra khái niệm đo lường

| Khái niệm | Ví dụ | Số lượng |
|-----------|-------|----------|
| English ⇒ French | (house, maison) | (209, 231) |
| French ⇒ German | (déjà, bereits) | (278, 205) |
| French ⇒ Spanish | (musique, música) | (218, 214) |
| German ⇒ Spanish | (Krieg, guerra) | (214, 213) |

Bảng 4. Các ngữ cảnh được sử dụng để điều tra khái niệm can thiệp

| j | xⱼ |
|---|-----|
| 1 | Long live the |
| 2 | The lion is the |
| 3 | In the hierarchy of medieval society, the highest rank was the |
| 4 | Arthur was a legendary |
| 5 | He was known as the warrior |
| 6 | In a monarchy, the ruler is usually a |
| 7 | He sat on the throne, the |
| 8 | A sovereign ruler in a monarchy is often a |
| 9 | His domain was vast, for he was a |
| 10 | The lion, in many cultures, is considered the |
| 11 | He wore a crown, signifying he was the |
| 12 | A male sovereign who reigns over a kingdom is a |
| 13 | Every kingdom has its ruler, typically a |
| 14 | The prince matured and eventually became the |
| 15 | In the deck of cards, alongside the queen is the |

từ điển dịch từ có sẵn công khai. Chúng tôi lấy tất cả các cặp từ giữa các ngôn ngữ là những tương ứng top-1 với nhau trong từ điển song ngữ và lọc ra các cặp là token đơn trong từ vựng của mô hình LLaMA-2.

Bảng 3 trình bày số lượng các ngữ cảnh {x⁰ⱼ} và {x¹ⱼ} cho mỗi khái niệm và một ví dụ về các cặp (Y(0), Y(1)).

Trong thí nghiệm cho khái niệm can thiệp, cho một khái niệm W, Z, chúng tôi lấy mẫu các văn bản mà Y(0,0) (ví dụ: "king") nên theo sau, qua ChatGPT-4. Chúng tôi loại bỏ các ngữ cảnh sao cho Y(0,0) không phải là từ tiếp theo top 1. Bảng 4 trình bày các ngữ cảnh chúng tôi sử dụng.

D. Kết quả Bổ sung

D.1. Biểu đồ tần số của các cặp ngẫu nhiên và phản thực cho tất cả các khái niệm

Trong Hình 7, chúng tôi bao gồm một analog của Hình 2 trong đó chúng tôi kiểm tra tích vô hướng nhân quả của sự khác biệt giữa các cặp phản thực và một biểu diễn giải nhúng được ước lượng LOO cho mỗi trong 27 khái niệm. Trong khi hầu hết các khái niệm được mã hóa trong biểu diễn giải nhúng, một số khái niệm, như thing ⇒ part, không được mã hóa trong không gian giải nhúng Γ.

D.2. So sánh với các tích vô hướng Euclidean

Trong Hình 8, chúng tôi cũng vẽ các độ tương tự cosine được tạo ra bởi tích vô hướng Euclidean giữa các biểu diễn giải nhúng. Ngạc nhiên là, tích vô hướng Euclidean phần nào hoạt động trong mô hình LLaMA-2 vì hầu hết các khái niệm có thể tách biệt nhân quả đều trực giao! Điều này có thể do một số khởi tạo hoặc hiệu ứng điều hòa ngầm ẩn ưu ái việc học các giải nhúng với hiệp phương sai gần như đồng hướng. Tuy nhiên, tích vô hướng nhân quả được ước lượng rõ ràng cải thiện so với tích vô hướng Euclidean. Ví dụ, frequent ⇒ infrequent (khái niệm 23) có tích vô hướng Euclidean cao với nhiều khái niệm có thể tách biệt, và những cái này nhỏ hơn nhiều cho tích vô hướng nhân quả. Ngược lại, English ⇒ French (24) có tích vô hướng Euclidean thấp với các khái niệm ngôn ngữ khác (25-27), nhưng tích vô hướng nhân quả cao với French ⇒ German và French ⇒ Spanish (trong khi gần như trực giao với German ⇒ Spanish, không chia sẻ tiếng Pháp).

17

--- TRANG 17 ---
Giả thuyết Biểu diễn Tuyến tính và Hình học của các Mô hình Ngôn ngữ Lớn

Bảng 5. Ngữ cảnh: "The prince matured and eventually became the "

| Rank | α=0 | 0.1 | 0.2 | 0.3 | 0.4 |
|------|-----|-----|-----|-----|-----|
| 1 | king | king | em | queen | queen |
| 2 | em | em | r | em | woman |
| 3 | leader | r | leader | r | lady |
| 4 | r | leader | king | leader | wife |
| 5 | King | head | queen | woman | em |

Bảng 6. Ngữ cảnh: "In a monarchy, the ruler is usually a "

| Rank | α=0 | 0.1 | 0.2 | 0.3 | 0.4 |
|------|-----|-----|-----|-----|-----|
| 1 | king | king | her | woman | woman |
| 2 | monarch | monarch | monarch | queen | queen |
| 3 | member | her | member | her | female |
| 4 | her | member | woman | monarch | her |
| 5 | person | person | queen | member | member |

Thú vị là, các bản đồ nhiệt tương tự cho một mô hình Gemma-2B gần đây hơn (Mesnard et al., 2024) trong Hình 9 minh họa rằng tích vô hướng Euclidean không nắm bắt ngữ nghĩa, trong khi tích vô hướng nhân quả vẫn hoạt động. Một lý do có thể là gốc của các giải nhúng có ý nghĩa vì mô hình Gemma gắn các giải nhúng với các nhúng token được sử dụng trước các lớp transformer.

D.3. Kết quả bổ sung từ thí nghiệm đo lường

Chúng tôi bao gồm các analog của Hình 4, cụ thể là nơi chúng tôi sử dụng mỗi trong 27 khái niệm như một đầu dò tuyến tính trên các ngữ cảnh French ⇒ Spanish (Hình 10) hoặc English ⇒ French (Hình 11).

D.4. Kết quả bổ sung từ thí nghiệm can thiệp

Trong Hình 12, chúng tôi bao gồm một analog của Hình 5 trong đó chúng tôi thêm biểu diễn nhúng αλ̄C (4.1) cho mỗi trong 27 khái niệm vào λ(xⱼ) và xem sự thay đổi trong logits.

D.5. Các bảng bổ sung của top-5 từ sau can thiệp

Bảng 5 và Bảng 6 là các analog của Bảng 1 trong đó chúng tôi sử dụng các ngữ cảnh khác nhau x = "In a monarchy, the ruler usually is a " và x = "The prince matured and eventually became the ". Đối với ví dụ đầu tiên, lưu ý rằng "r" và "em" là các token tiền tố cho các từ liên quan đến hoàng gia, như "ruler", "royal", và "emperor". Đối với ví dụ thứ hai, ngay cả khi từ mục tiêu "queen" không trở thành từ có khả năng cao nhất, các từ có khả năng cao nhất vẫn phản ánh hướng khái niệm ("woman", "queen", "her", "female").

D.6. Một kiểm tra tính hợp lý cho tích vô hướng nhân quả được ước lượng

Trong các thí nghiệm trước đó, chúng tôi thấy rằng lựa chọn M = Cov(γ)⁻¹ từ (3.3) tạo ra một tích vô hướng nhân quả và tạo ra một biểu diễn nhúng λ̄W dưới dạng (4.1). Ở đây, chúng tôi chạy một thí nghiệm kiểm tra tính hợp lý trong đó chúng tôi xác minh rằng biểu diễn nhúng được tạo ra thỏa mãn điều kiện không tương quan trong Giả định D.6. Trong Hình 13, chúng tôi chỉ ra thực nghiệm rằng λ̄⊤W γ và λ̄⊤Z γ không tương quan cho các khái niệm có thể tách biệt nhân quả (biểu đồ trái), trong khi chúng tương quan cho các khái niệm không thể tách biệt nhân quả (biểu đồ phải). Trong những biểu đồ này, mỗi điểm tương ứng với điểm (λ̄⊤W γ, λ̄⊤Z γ), trong đó γ là một vector giải nhúng γ tương ứng với mỗi token trong từ vựng LLaMA-2 (tổng cộng 32K).

18

--- TRANG 18 ---
Giả thuyết Biểu diễn Tuyến tính và Hình học của các Mô hình Ngôn ngữ Lớn

[Biểu đồ tần số hiển thị 27 khái niệm khác nhau với các cặp ngẫu nhiên và phản thực]

Hình 7. Biểu đồ tần số của các phép chiếu của các cặp phản thực ⟨γ̄W,(−i), γ(yi(1)) − γ(yi(0))⟩C (đỏ), và các phép chiếu của sự khác biệt giữa 100K cặp từ được lấy mẫu ngẫu nhiên lên hướng khái niệm được ước lượng (xanh). Xem Bảng 2 để biết chi tiết về mỗi khái niệm W (tiêu đề của mỗi biểu đồ).

19

--- TRANG 19 ---
Giả thuyết Biểu diễn Tuyến tính và Hình học của các Mô hình Ngôn ngữ Lớn

[THIS IS FIGURE: Two heatmaps side by side showing causal inner product (left) and Euclidean inner product (right) for LLaMA-2-7B model with numbered concepts 1-27]

Hình 8. Đối với mô hình LLaMA-2-7B, các khái niệm có thể tách biệt nhân quả gần như trực giao dưới tích vô hướng nhân quả được ước lượng và, ngạc nhiên là, dưới tích vô hướng Euclidean. Các bản đồ nhiệt hiển thị |⟨γ̄W, γ̄Z⟩| cho các biểu diễn giải nhúng được ước lượng của mỗi cặp khái niệm (W, Z). Biểu đồ bên trái hiển thị tích vô hướng được ước lượng dựa trên (3.3), và biểu đồ bên phải đại diện cho tích vô hướng Euclidean. Chi tiết cho các khái niệm được đưa ra trong Bảng 2.

[THIS IS FIGURE: Two heatmaps side by side showing causal inner product (left) and Euclidean inner product (right) for Gemma-2B model with numbered concepts 1-27]

Hình 9. Đối với mô hình Gemma-2B, các khái niệm có thể tách biệt nhân quả gần như trực giao dưới tích vô hướng nhân quả được ước lượng; tuy nhiên, tích vô hướng Euclidean không nắm bắt ngữ nghĩa. Các bản đồ nhiệt hiển thị |⟨γ̄W, γ̄Z⟩| cho các biểu diễn giải nhúng được ước lượng của mỗi cặp khái niệm (W, Z). Biểu đồ bên trái hiển thị tích vô hướng được ước lượng dựa trên (3.3), và biểu đồ bên phải đại diện cho tích vô hướng Euclidean. Chi tiết cho các khái niệm được đưa ra trong Bảng 2.

20

--- TRANG 20 ---
Giả thuyết Biểu diễn Tuyến tính và Hình học của các Mô hình Ngôn ngữ Lớn

[THIS IS FIGURE: A large collection of histograms arranged in a grid format showing various linguistic concepts. Each histogram shows French vs Spanish distributions. The concepts shown include:

verb3pSg, verbVing, verbVed, Ving3pSg, VingVed, 3pSgVed, verbV+able, verbV+er, verbV+tion, verbV+ment, adjun+adj, adjadj+ly, smallbig, thingcolor, thingpart, countrycapital, pronounpossessive, malefemale, lowerupper, nounplural, adjcomparative, adjsuperlative, frequentinfrequent, EnglishFrench, FrenchGerman, FrenchSpanish, GermanSpanish]

Hình 10. Biểu đồ tần số của γ̄ᵀC λ(xʲᶠʳ) vs γ̄ᵀC λ(xʲᵉˢ) cho tất cả các khái niệm C, trong đó {xʲᶠʳ} là các ngữ cảnh ngẫu nhiên từ Wikipedia tiếng Pháp, và {xʲᵉˢ} là các ngữ cảnh ngẫu nhiên từ Wikipedia tiếng Tây Ban Nha.

21

--- TRANG 21 ---
Giả thuyết Biểu diễn Tuyến tính và Hình học của các Mô hình Ngôn ngữ Lớn

[THIS IS FIGURE: A large grid of histograms showing frequency distributions for various linguistic concepts. Each histogram shows English vs French distributions. The concepts include:

verb3pSg, verbVing, verbVed, Ving3pSg, VingVed, 3pSgVed, verbV+able, verbV+er, verbV+tion, verbV+ment, adjun+adj, adjadj+ly, smallbig, thingcolor, thingpart, countrycapital, pronounpossessive, malefemale, lowerupper, nounplural, adjcomparative, adjsuperlative, frequentinfrequent, EnglishFrench, FrenchGerman, FrenchSpanish, GermanSpanish]

Hình 11. Biểu đồ tần số của γ̄ᵀC λ(xʲᵉⁿ) vs γ̄ᵀC λ(xʲᶠʳ) cho tất cả các khái niệm C, trong đó {xʲᵉⁿ} là các ngữ cảnh ngẫu nhiên từ Wikipedia tiếng Anh, và {xʲᶠʳ} là các ngữ cảnh ngẫu nhiên từ Wikipedia tiếng Pháp.

22

--- TRANG 22 ---
Giả thuyết Biểu diễn Tuyến tính và Hình học của các Mô hình Ngôn ngữ Lớn

[THIS IS FIGURE: A grid of 27 small plots showing arrows indicating changes in log probabilities. Each plot is labeled with linguistic concepts like "verb3pSg", "verbVing", etc. The plots show changes in log(P("queen"|x)/P("king"|x)) and log(P("King"|x)/P("king"|x)).]

Hình 12. Thay đổi trong log(P("queen"|x)/P("king"|x)) và log(P("King"|x)/P("king"|x)), sau khi thay đổi λ(xⱼ) thành λC,α(xⱼ) cho α ∈ [0, 0.4] và bất kỳ khái niệm C nào. Điểm bắt đầu và điểm kết thúc của mỗi mũi tên tương ứng với λ(xⱼ) và λC,0.4(xⱼ), tương ứng.

23

--- TRANG 23 ---
Giả thuyết Biểu diễn Tuyến tính và Hình học của các Mô hình Ngôn ngữ Lớn

[THIS IS FIGURE: Two scatter plots side by side. Left plot shows uncorrelated data points scattered randomly. Right plot shows correlated data points forming a diagonal pattern.]

Hình 13. Biểu đồ bên trái cho thấy rằng λ̄ᵀW γ và λ̄ᵀZ γ không tương quan cho các khái niệm có thể tách biệt nhân quả W = nam ⇒ nữ và Z = Tiếng Anh ⇒ Tiếng Pháp. Mặt khác, biểu đồ bên phải cho thấy rằng λ̄ᵀW γ và λ̄ᵀZ γ tương quan cho các khái niệm không thể tách biệt nhân quả W = verb ⇒ 3pSg và Z = verb ⇒ Ving. Mỗi điểm tương ứng với vector giải nhúng γ cho mỗi token trong từ vựng.

24
