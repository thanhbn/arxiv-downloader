# 2312.02179.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/cot/2312.02179.pdf
# Kích thước tệp: 598793 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Huấn luyện Chuỗi Suy nghĩ thông qua Suy luận
Biến Tiềm ẩn
Du Phan∗Matthew D. Hoffman∗David Dohan†Sholto Douglas Tuan Anh Le
Aaron Parisi Pavel Sountsov Charles Sutton Sharad Vikram Rif A. Saurous
Google

Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) giải quyết các vấn đề chính xác và dễ diễn giải hơn
khi được hướng dẫn làm ra đáp án từng bước bằng cách sử dụng prompt "chuỗi suy nghĩ"
(CoT). Người ta cũng có thể cải thiện hiệu suất của LLM trên một nhiệm vụ cụ thể bằng
cách tinh chỉnh có giám sát, tức là sử dụng gradient ascent trên một số tham số có thể
điều chỉnh để tối đa hóa log-likelihood trung bình của các câu trả lời đúng từ tập huấn
luyện được gán nhãn. Việc kết hợp đơn giản CoT với tinh chỉnh có giám sát đòi hỏi sự
giám sát không chỉ về các câu trả lời đúng mà còn về các lý lẽ chi tiết dẫn đến những
câu trả lời đó; những lý lẽ này tốn kém để sản xuất bằng tay. Thay vào đó, chúng tôi
đề xuất một chiến lược tinh chỉnh cố gắng tối đa hóa log-likelihood biên của việc tạo
ra câu trả lời đúng bằng cách sử dụng prompt CoT, xấp xỉ tính trung bình trên tất cả
các lý lẽ có thể. Thách thức cốt lõi là lấy mẫu từ posterior trên các lý lẽ có điều kiện
trên câu trả lời đúng; chúng tôi giải quyết nó bằng cách sử dụng thuật toán kỳ vọng-
tối đa hóa (EM) Markov-chain Monte Carlo (MCMC) đơn giản lấy cảm hứng từ bộ
lý luận tự học (STaR), wake-sleep có ghi nhớ, leo điểm Markovian, và divergence tương
phản liên tục. Thuật toán này cũng cho phép một kỹ thuật kiểm soát-variate mới đẩy
phương sai của các ước lượng gradient của chúng tôi về không khi mô hình được cải thiện.
Áp dụng kỹ thuật của chúng tôi cho GSM8K và các nhiệm vụ trong BIG-Bench Hard,
chúng tôi thấy rằng kỹ thuật tinh chỉnh MCMC-EM này thường cải thiện độ chính xác
của mô hình trên các ví dụ được giữ lại nhiều hơn so với STaR hoặc prompt-tuning có
hoặc không có CoT.

1 Giới thiệu
Đối với nhiều vấn đề lý luận toán học, logic và thường thức, các mô hình ngôn ngữ lớn giải quyết
vấn đề chính xác hơn khi được hướng dẫn làm ra đáp án từng bước trong một chuỗi suy nghĩ
hoặc bảng nháp (Wei et al., 2022; Nye et al., 2021; Kojima et al., 2022; Rajani et al., 2019; Shwartz
et al., 2020). Những phương pháp này khuyến khích mô hình tạo ra một lý lẽ, tức là văn bản mô tả
một chuỗi các bước lý luận dẫn đến một câu trả lời; động cơ là dường như dễ dàng hơn cho
mô hình tạo ra một chuỗi các bước lý luận đúng hơn là tạo ra câu trả lời cuối cùng trực tiếp.
Vì hiệu suất nổi bật của các phương pháp chuỗi suy nghĩ, nhiều biến thể đã được đề xuất
(Wang et al., 2022b; Zhou et al., 2022; Creswell et al., 2022; Ye & Durrett, 2023), nhưng vẫn còn
nhiều trường hợp mà các lý lẽ không đúng.

Một cách để cải thiện những phương pháp này là tinh chỉnh các mô hình để tạo ra các lý lẽ tốt hơn.
Nếu có thể có được các lý lẽ tiêu chuẩn vàng, chẳng hạn như thông qua crowdsourcing (Rajani et al., 2019)
hoặc tự động (Nye et al., 2021), thì có thể áp dụng các phương pháp có giám sát, nhưng việc thu thập
dữ liệu này có thể khó khăn. Một lựa chọn hấp dẫn khác là bắt đầu từ các tập dữ liệu chỉ chứa câu hỏi
và câu trả lời đúng, dễ có hơn, và bootstrap các lý lẽ trong quá trình học. Một phiên bản của chiến lược
này đã được đề xuất dưới dạng bộ lý luận tự học (STaR) (Zelikman et al., 2022), tạo ra các lý lẽ được
đề xuất từ một LLM, và sau đó tinh chỉnh trên các lý lẽ dẫn đến câu trả lời đúng.

∗Tác giả liên hệ: {mhoffman,phandu}@google.com. Hai tác giả đầu tiên đóng góp như nhau,
và thứ tự của họ được chọn ngẫu nhiên.
†Thuộc tổ chức hiện tại: OpenAI.
Hội nghị lần thứ 37 về Hệ thống Xử lý Thông tin Neural (NeurIPS 2023).arXiv:2312.02179v1 [cs.LG] 28 Nov 2023

--- TRANG 2 ---
Trong bài báo này, chúng tôi tiếp cận vấn đề bootstrap các lý lẽ từ một hướng khái niệm khác:
các phương pháp chuỗi suy nghĩ là các mô hình biến tiềm ẩn xác suất. LLM định nghĩa một
phân phối xác suất kết hợp trên câu hỏi, lý lẽ và câu trả lời; phân phối kết hợp này ngụ ý một
phân phối biên của câu trả lời cho trước câu hỏi, tính trung bình trên tất cả các lý lẽ có thể được
trọng số bởi xác suất của chúng cho trước câu hỏi. Vấn đề tự huấn luyện cho lý luận sau đó trở
thành một vấn đề học với dữ liệu không đầy đủ, một nhiệm vụ cốt lõi trong học máy xác suất
(Murphy, 2022) mà chúng ta có thể áp dụng các phương pháp từ một văn liệu lớn và tinh vi.

Quan điểm này đặt ra một thách thức kỹ thuật, vì việc tính toán phân phối biên đòi hỏi tính trung
bình trên một tập hợp rộng lớn các lý lẽ tiềm năng. Để giải quyết điều này, chúng tôi giới thiệu
một thuật toán học cho việc tạo lý lẽ, mà chúng tôi gọi là TRICE. TRICE là một thuật toán
kỳ vọng-tối đa hóa (EM) Markov-chain Monte Carlo (MCMC) đơn giản kết hợp với một sơ đồ
kiểm soát-variate mới, lấy cảm hứng từ ý tưởng từ STaR (Zelikman et al., 2022), wake-sleep có
ghi nhớ (Hewitt et al., 2020), leo điểm Markovian (Naesseth et al., 2020), và divergence tương
phản liên tục (Tieleman, 2008).

Quan điểm này thống nhất một số luồng công việc trong lý luận sử dụng LLM: Nó cung cấp một
diễn giải thay thế của STaR như một loại thuật toán kỳ vọng-tối đa hóa ngẫu nhiên thiên lệch
(Nielsen, 2000) làm giảm trọng số các ví dụ khó khi quá trình lý luận hóa của nó thất bại. Tính
nhất quán bản thân (Wang et al., 2022a) có thể được xem như một thuật toán Monte Carlo để
tính toán câu trả lời có khả năng nhất dưới phân phối biên. So với tính nhất quán bản thân, cách
tiếp cận học xác suất của TRICE cho phép chúng ta tính trung bình trên các lý lẽ không chỉ ở
thời gian suy luận mà còn ở thời gian huấn luyện. So với STaR, TRICE ít có khả năng bỏ qua
các ví dụ khó (điều này ổn định hội tụ và cải thiện hiệu suất), và cũng có thể học từ các lý lẽ
không đúng cũng như các lý lẽ đúng.

Chúng tôi áp dụng kỹ thuật của mình cho tập dữ liệu GSM8K (Cobbe et al., 2021) và cho điểm
chuẩn BIG-Bench Hard (Suzgun et al., 2022a). Chúng tôi thấy rằng TRICE cải thiện hiệu suất
của mô hình một cách đáng kể, vượt trội hơn các mô hình được tinh chỉnh với STaR, tinh chỉnh
trực tiếp có hoặc không có CoT, và thậm chí cả tinh chỉnh có giám sát trên các lý lẽ do con người
tạo ra.

2 Phương pháp
Cho một tập huấn luyện gồm N câu hỏi x₁:N và câu trả lời y₁:N, chúng tôi hình thức hóa việc
tinh chỉnh CoT như việc tối ưu hóa một vector tham số θ để tối đa hóa log-likelihood biên trung
bình của câu trả lời cho trước câu hỏi:

L(θ) ≜ 1/N ∑ₙ log pθ(yₙ|xₙ) = 1/N ∑ₙ log ∑z pθ(z|xₙ)p(yₙ|z, xₙ), (1)

trong đó z là một lý lẽ tiềm ẩn không quan sát được, pθ(z|x) là xác suất thu được lý lẽ z bằng
cách prompt một LLM với câu hỏi x và các tham số có thể điều chỉnh θ, và pθ(y|z, x) là xác suất
thu được câu trả lời y cho trước lý lẽ z, câu hỏi x, và tham số θ. Chúng tôi sẽ đặc biệt quan tâm
đến các mô hình trong đó likelihood pθ(y|x, z) ∈ {0,1}, tức là trong đó câu trả lời y là một hàm
xác định của z. Ví dụ, chúng ta có thể nói rằng câu trả lời của mô hình là y="(a)" nếu z kết thúc
bằng chuỗi "Câu trả lời là (a)." Đối với mô hình xác định này, chúng tôi định nghĩa p(y|z, x) =
c(z, y) ∈ {0,1}. Chi tiết về c(z, y) cho mỗi nhiệm vụ có thể tìm thấy trong Phụ lục F. Chúng
tôi tin rằng mô hình likelihood nhị phân như vậy phù hợp cho các nhiệm vụ trả lời câu hỏi trong
đó z là một lý lẽ - một lý lẽ tốt không nên để lại sự mơ hồ về câu trả lời đúng. Các dẫn xuất
dưới đây do đó sẽ giả định một hàm likelihood nhị phân. Việc tổng quát hóa các phương pháp
của chúng tôi cho các trường hợp mà mối quan hệ giữa z và y yếu hơn và do đó p(y|x, z) phức
tạp hơn là đơn giản; Phụ lục A cho thấy cách thức.

Thuật toán 1 phác thảo phương pháp. Một notebook với triển khai tham chiếu có thể tìm thấy
tại https://github.com/google-research/cascades/tree/main/cascades/examples/notebooks/trice.ipynb.

Chúng tôi bắt đầu bằng cách khởi tạo một bộ nhớ chứa một lý lẽ tiềm ẩn zₙ cho mỗi cặp ví dụ
xₙ, yₙ bằng cách lấy mẫu zₙ từ một phân phối hướng dẫn có gợi ý q(z|xₙ, yₙ) có thể có điều kiện
trên câu trả lời đúng yₙ cũng như câu hỏi xₙ. Ví dụ, bộ hướng dẫn có thể prompt một LLM cụ
thể để đưa ra lý lẽ cho câu trả lời; thêm chi tiết về các prompt chính xác được sử dụng bởi bộ
hướng dẫn có trong Phụ lục F. Trong một số trường hợp, lấy mẫu từ bộ hướng dẫn thay vì mô
hình pθ(z|xₙ) tăng cơ hội tạo ra lý lẽ đúng (Zelikman et al., 2022).

³TRICE là viết tắt của "Tuning Rationales with Independence-Chain Expectation-maximization."
⁴Trừ khi được nói khác, chúng tôi lấy mẫu ở nhiệt độ 1 xuyên suốt.

--- TRANG 3 ---
Sau đó chúng tôi tiến hành vòng lặp tối ưu hóa chính. Mỗi lần lặp, chúng tôi lấy mẫu một minibatch
gồm M câu hỏi và câu trả lời từ tập dữ liệu, và truy xuất các lý lẽ liên quan đến những ví dụ đó
từ bộ nhớ. Sau đó chúng tôi đề xuất các lý lẽ mới z̃ từ mô hình hiện tại pθ(z|x), và bất cứ khi
nào lý lẽ mới z̃ đúng (tức là c(z̃, y) = 1) thì thay thế lý lẽ cũ trong bộ nhớ bằng lý lẽ mới.

Tại thời điểm này chúng tôi có tất cả những gì cần thiết để tính toán một ước lượng gradient; chúng
tôi chỉ cần tính trung bình các gradient ∇θ log pθ(zᵢₘ|xᵢₘ) mà chúng tôi thu được từ những lý lẽ
trong bộ nhớ cập nhật mà đúng (tức là chúng tôi bỏ qua các ví dụ trong đó cả lý lẽ đề xuất và lý
lẽ trước đó đều sai). BASIC_GRADIENT_ESTIMATE trong Thuật toán 1 cho thấy cách thức.

Nhưng chúng tôi cũng có thể giảm phương sai của bộ ước lượng gradient bằng cách kết hợp một
kiểm soát variate, như trong CONTROL_VARIATE_GRADIENT_ESTIMATE trong Thuật toán 1.
Trước tiên chúng tôi tính toán các ước lượng leave-one-out β₁:M của xác suất trung bình chấp
nhận một lý lẽ mới. Đối với mỗi ví dụ m, chúng tôi trừ đi một kiểm soát variate có tỷ lệ βₘ∇θ log pθ(z̃ₘ|xᵢₘ)
có giá trị kỳ vọng là không (vì nó là một hàm điểm). Nếu lý lẽ đề xuất z̃ₘ cho ví dụ m đúng, thì
zᵢₘ = z̃ₘ, và đóng góp gradient thứ m trở thành (1-βₘ)∇θ log pθ(zᵢₘ|xᵢₘ), tức là nó được
giảm tỷ lệ bởi 1-βₘ. Nếu z̃ₘ không đúng, thì chúng tôi điều chỉnh ước lượng gradient để cố gắng
làm cho z̃ₘ ít có khả năng hơn dưới pθ. Khi mô hình trở nên chính xác hơn (tức là β tiến gần đến 1),
chúng tôi đưa ra trọng số lớn hơn cho các lý lẽ không đúng (khi chúng xảy ra) và trọng số nhỏ hơn
cho các lý lẽ đúng (hầu hết thời gian).

CONTROL_VARIATE_GRADIENT_ESTIMATE đắt hơn BASIC_GRADIENT_ESTIMATE, vì
chúng tôi phải tính toán gradient không chỉ cho các lý lẽ trong bộ nhớ mà còn cho bất kỳ lý lẽ
không đúng nào chúng tôi tạo ra. Điều này có thể lãng phí, đặc biệt nếu nhiều trọng số trên những
gradient đó (1-β cho các đề xuất đúng, β cho các đề xuất không đúng) gần bằng không vì β gần
bằng không hoặc một. Để giảm chi phí này, trong SUBSAMPLED_CONTROL_VARIATE_GRADIENT_ESTIMATE,
chúng tôi sử dụng lấy mẫu có hệ thống (Hol et al., 2006) để tạo ra một mẫu con gồm L cặp câu
hỏi-lý lẽ, từ đó chúng tôi có được một ước lượng không thiên lệch của đầu ra của CONTROL_VARIATE_GRADIENT_ESTIMATE.
Chúng tôi ưu tiên lấy mẫu các gradient có trọng số vô hướng cao hơn; nếu β nhỏ, chúng tôi ít có
khả năng lấy mẫu các lý lẽ không đúng (có trọng số β), và nếu β lớn, chúng tôi ít có khả năng lấy
mẫu các lý lẽ đề xuất đúng (có trọng số 1-β). Điều này có thể được xem như một sự tổng quát hóa
của chiến lược của Burda et al. (2015, Phần 3) để giảm chi phí tính toán gradient IWAE.

Dưới đây, chúng tôi dẫn xuất thủ tục MCMC-EM ngẫu nhiên giảm phương sai này chi tiết hơn.

2.1 Dẫn xuất
Gradient thực. Gradient của log-likelihood biên log pθ(y|x) đối với θ là

∇θ log ∑z pθ(z, y|x) = ∑z pθ(z,y|x)∇θ log pθ(z,y|x) / ∑z' pθ(z',y|x) = ∑z pθ(z|x, y)∇θ log pθ(z|x), (2)

tức là nó là kỳ vọng đối với posterior pθ(z|x, y) của gradient của log-prior có điều kiện log pθ(z|x),
vì likelihood p(y|z, x) = c(z, y) không phụ thuộc vào θ. Vậy nếu chúng ta có thể lấy mẫu từ
posterior trên lý lẽ z có điều kiện trên cặp câu hỏi-câu trả lời x, y, thì chúng ta có thể tính toán
một ước lượng không thiên lệch của gradient của log-likelihood biên log pθ(y|x). Chúng ta có thể
diễn giải điều này như "bootstrap" các lý lẽ z nhất quán với cả prior trên lý lẽ pθ(z|x) và câu
trả lời quan sát được y (tham khảo Zelikman et al., 2022).

Bộ lấy mẫu độc lập cho pθ(z|x, y). Chúng ta không thể trực tiếp lấy mẫu từ pθ(z|x, y), vì vậy
chúng ta phải sử dụng Markov chain Monte Carlo (MCMC). Chúng tôi duy trì một bộ nhớ (tham khảo
Hewitt et al., 2020) của một lý lẽ duy nhất zₙ cho mỗi cặp câu hỏi-câu trả lời xₙ, yₙ, và mỗi lần
lặp chúng tôi áp dụng một cập nhật ngẫu nhiên cho zₙ để giữ cho posterior pθ(zₙ|xₙ, yₙ) bất biến
(tham khảo Tieleman, 2008). Mỗi cập nhật MCMC đưa các zₙ gần hơn trong phân phối đến
pθ(zₙ|xₙ, yₙ) (Cover, 1999; Murray & Salakhutdinov, 2008). Tuy nhiên, các cập nhật cho θ có thể
thay đổi posterior pθ(zₙ|xₙ, yₙ), vì vậy chúng ta phải tiếp tục cập nhật các chuỗi để kiểm soát
thiên lệch của các ước lượng gradient.

Để cập nhật các chuỗi, chúng tôi sử dụng một bộ lấy mẫu độc lập đơn giản, không có siêu tham số
(Tierney, 1994); một cập nhật Metropolis-Hastings (Hastings, 1970) đề xuất cập nhật trạng thái
hiện tại z với một mẫu z̃ từ phân phối rₓ,ᵧ không phụ thuộc vào z, và chấp nhận cập nhật với
xác suất

--- TRANG 4 ---
Thuật toán 1 TRICE
Đầu vào: Mô hình sinh pθ(z, y|x), hàm kiểm tra đúng c(z, y), tập dữ liệu x₁:N, y₁:N, phân phối hướng dẫn có gợi ý q(z|x, y), tham số ban đầu θ, hàm cập nhật optimizer h(θ, g, t), kích thước minibatch M, kích thước minibatch gradient L, số lần lặp T.
Đầu ra: Tham số được tinh chỉnh θ, lý lẽ z₁:N.

1: for n ∈ 1, . . . , N do (song song) // Khởi tạo trạng thái chuỗi Markov.
2:    Lấy mẫu zₙ từ q(z|xₙ, yₙ). // Lấy mẫu lý lẽ "dự phòng" từ hướng dẫn q.
3: end for
4: for t ∈ 1, . . . , T do // Vòng lặp tối ưu hóa chính.
5:    Lấy minibatch tiếp theo gồm M chỉ số i₁:M vào tập dữ liệu.
6:    for m ∈ 1, . . . , M do (song song) // Thực hiện một bước MCMC để cập nhật trạng thái chuỗi Markov.
7:       Lấy mẫu z̃ₘ từ pθ(z|xᵢₘ).
8:       if c(z̃ₘ, yᵢₘ) then // Chấp nhận hoặc từ chối đề xuất.
9:          Cập nhật zᵢₘ ← z̃ₘ.
10:      end if
11:      Đặt c̃ₘ = c(z̃ₘ, yᵢₘ). // Liệu đề xuất có đúng không.
12:      Đặt c'ₘ = c(zᵢₘ, yᵢₘ). // Liệu lý lẽ cập nhật có đúng không.
13:   end for
14:   Tính toán ĝ sử dụng một trong BASIC_GRADIENT_ESTIMATE(z, x, c'),
      CONTROL_VARIATE_GRADIENT_ESTIMATE(z, x, z̃, c̃, c'),
      hoặc SUBSAMPLED_CONTROL_VARIATE_GRADIENT_ESTIMATE(z, x, z̃, c̃, c').
15:   Cập nhật θ ← h(θ, ĝ, t). // Áp dụng cập nhật gradient.
16: end for
17: return θ, z₁:N.

18: procedure BASIC_GRADIENT_ESTIMATE(z, x, c')
19:    return 1/(∑ₘ c'ₘ) ∑ₘ c'ₘ ∇θ log pθ(zᵢₘ|xᵢₘ).
20: end procedure

21: procedure CONTROL_VARIATE_GRADIENT_ESTIMATE(z, x, z̃, c̃, c')
22:    for m ∈ 1, . . . , M do (song song)
23:       Đặt βₘ = (∑ₘ'≠ₘ c'ₘ' c̃ₘ')/(∑ₘ'≠ₘ c'ₘ'). // Tính toán tỷ lệ kiểm soát-variate leave-one-out.
24:    end for
25:    return 1/(∑ₘ c'ₘ) ∑ₘ c'ₘ (∇θ log pθ(zᵢₘ|xᵢₘ) - βₘ∇θ log pθ(z̃ₘ|xᵢₘ)).
26: end procedure

27: procedure SUBSAMPLED_CONTROL_VARIATE_GRADIENT_ESTIMATE(z, x, z̃, c̃, c')
28:    for m ∈ 1, . . . , M do (song song)
29:       Đặt βₘ = (∑ₘ'≠ₘ c'ₘ' c̃ₘ')/(∑ₘ'≠ₘ c'ₘ'). // Tính toán tỷ lệ kiểm soát-variate leave-one-out.
30:       Đặt w̃ₘ = c'ₘ(1 - c̃ₘ βₘ), // Tính toán trọng số chưa chuẩn hóa cho lấy mẫu con.
           w̃ₘ₊ₘ = c'ₘ(1 - c̃ₘ)βₘ.
31:    end for
32:    Chọn một tập con gồm L chỉ số j₁:L sử dụng lấy mẫu có hệ thống với xác suất w̃/(∑²ᴹₘ₌₁ w̃ₘ).
33:    for ℓ ∈ 1, . . . , L do (song song)
34:       if jℓ ≤ M then // Đã chọn lý lẽ đúng.
35:          Đặt m̂ = jℓ, ẑ = zᵢₘ̂, s = 1.
36:       else // Đã chọn lý lẽ không đúng.
37:          Đặt m̂ = jℓ - M, ẑ = z̃ₘ̂, s = -1.
38:       end if
39:       Tính toán ĝℓ = s∇θ log pθ(ẑ|xᵢₘ̂).// Đảo dấu gradient nếu lý lẽ thứ ℓ không đúng.
40:    end for
41:    return (∑²ᴹₘ₌₁ w̃ₘ)/(∑ₘ c'ₘ) · 1/L ∑ᴸℓ₌₁ ĝℓ.
42: end procedure

α(z̃|z) = min{1, (pθ(z̃,y|x)/rₓ,ᵧ(z̃))/(pθ(z,y|x)/rₓ,ᵧ(z))}. Chúng tôi chọn rₓ,ᵧ(z) = pθ(z|x), điều này đơn giản hóa xác suất chấp nhận thành α(z̃|z) = min{1, pθ(y|x,z̃)/pθ(y|x,z)}. Điều này là 1 nếu c(z̃, y) = 1, 0 nếu c(z̃, y) = 0 và c(z, y) = 1, và không xác định (ngụ ý rằng chúng ta phải từ chối) nếu cả c(z, y) = 0 và c(z̃, y) = 0. Vậy chúng tôi chấp nhận bất cứ khi nào đề xuất z̃ đúng, và từ chối trong trường hợp khác.

--- TRANG 5 ---
Nhận xét: Bộ lấy mẫu độc lập có thể được hiểu như các bộ lấy mẫu quan trọng "Metropolized" phân tán công việc tạo ra và đánh giá các đề xuất theo thời gian. Trong bối cảnh của chúng tôi, việc cập nhật cũng có thể được diễn giải như cố gắng lấy mẫu từ posterior bằng lấy mẫu từ chối, sau đó dự phòng bằng một mẫu cũ nếu điều đó thất bại. Số lần lặp kỳ vọng giữa các cập nhật thành công là p(y|x)⁻¹, vì vậy việc trộn sẽ nhanh hơn đối với các câu hỏi x dễ hơn, và sẽ tăng tốc khi mô hình cải thiện.

Bộ ước lượng gradient cơ bản. Thủ tục MCMC/lấy mẫu từ chối này cho phép chúng tôi xấp xỉ gradient của log-likelihood biên trong Phương trình (2). Ký hiệu z là trạng thái của chuỗi Markov cho một ví dụ x, y trước khi cập nhật, chúng tôi lấy mẫu một đề xuất z̃ từ pθ(z|x), chấp nhận trạng thái mới nếu nó đúng (tức là nếu c(z̃, y) = 1), và tính toán gradient của log-probability của kết quả:

z' = c(z̃, y)z̃ + (1-c(z̃, y))z; ĝ = ∇θ log pθ(z'|x); E_z,z̃[ĝ|θ] ≈ E_pθ(z|x,y)[∇θ log pθ(z|x)], (3)

trong đó E_z,z̃[·|θ] biểu thị kỳ vọng đối với cả đề xuất z̃ và trạng thái trước đó z.

Nhận xét: Ước lượng sẽ có thiên lệch thấp nếu phân phối của z' gần với posterior p(z|x, y), mà chúng tôi kỳ vọng đúng nếu chuỗi trộn đủ nhanh so với tốc độ thay đổi θ. Điều này sẽ xảy ra nếu hoặc là xác suất có được câu trả lời đúng cao, hoặc θ thay đổi chậm do tốc độ học nhỏ và/hoặc gradient. Nếu độ chính xác tập huấn luyện của mô hình cải thiện với việc huấn luyện và chúng tôi sử dụng lịch trình tốc độ học giảm dần, thì khi huấn luyện tiến triển, cả hai yếu tố này sẽ hoạt động để giảm thiên lệch của ước lượng gradient.

Thêm kiểm soát variate. Trung bình của một bộ ước lượng ĝ không bị ảnh hưởng bởi việc trừ đi một biến ngẫu nhiên có trung bình không b khỏi nó. Nếu b tương quan dương với ĝ, thì ĝ - b có thể có phương sai thấp hơn ĝ, và chúng ta nói rằng b có thể được sử dụng như một "kiểm soát variate" (Owen & Zhou, 2000). Vì theo đồng nhất thức hàm điểm, E_pz|x[β∇θ log pθ(z|x)] = 0 (cho bất kỳ β vô hướng nào độc lập với z), chúng ta có thể sử dụng các mẫu đề xuất z̃ để tạo ra kiểm soát variate cho bộ ước lượng gradient:

E_z,z̃[ĝ|θ] = E_z[E_z̃[∇θ log pθ(z'|x)]|θ]
           = E_z[E_z̃[∇θ log pθ(z'|x) - β∇θ log pθ(z̃|x)]|θ]. (4)

Nhận xét: Giá trị của bộ ước lượng này sẽ phụ thuộc vào việc chúng ta có chấp nhận đề xuất z̃ hay không:

∇θ log pθ(z'|x) - β∇θ log pθ(z̃|x) = {
    (1-β)∇θ log pθ(z'|x) nếu c̃ = 1,
    ∇θ log pθ(z'|x) - β∇θ log pθ(z̃|x) nếu c̃ = 0,
} (5)

trong đó chúng tôi sử dụng ký hiệu viết tắt c̃ ≜ c(z̃, y).

Kiểm soát variate này có thể đẩy phương sai của bộ ước lượng gradient về không khi mô hình hội tụ đến độ chính xác hoàn hảo trên tập huấn luyện (tham khảo Roeder et al., 2017). Nếu chúng ta đặt β = π, trong đó π là xác suất của câu trả lời đúng (tức là c̃ = 1), thì khi π trở nên lớn, hầu hết thời gian c̃ = 1 và chúng ta nhân bộ ước lượng gradient với 1-π (nhân phương sai của nó với một hệ số (1-π)²). Nếu c̃ = 0, thì chúng ta sử dụng cả lý lẽ đúng và không đúng; các trọng số gắn với những cập nhật này sẽ không nhỏ, nhưng nếu các lý lẽ không đúng tương đối hiếm thì đóng góp của chúng vào phương sai của bộ ước lượng gradient sẽ tương ứng nhỏ. Mặt khác, nếu mô hình chưa học được cách thường xuyên tạo ra lý lẽ tốt cho các ví dụ huấn luyện, thì chúng ta nên đặt β gần với 0 hơn, vì trong trường hợp này tín hiệu từ lý lẽ không đúng ít thông tin hơn - trong Phụ lục C.1 chúng tôi cho thấy rằng phương sai của các bộ ước lượng gradient dựa trên lý lẽ không đúng phụ thuộc mạnh vào độ chính xác π của mô hình. Trong Phụ lục B, chúng tôi cho thấy rằng việc chọn β = π thực sự tối ưu đến các số hạng O((1-π)²), và phương sai của bộ ước lượng kết quả tỷ lệ với 1-π.

Ước lượng β. Đối với mỗi ví dụ xₘ, yₘ, chúng ta cần tính toán βₘ ≈ E[c̃ₘ] theo cách đảm bảo rằng βₘ độc lập với ∇θ log pθ(z̃ₘ|xₘ). Chúng tôi giả định rằng E[c̃ₘ] ≈ 1/M ∑ₘ E[c̃ₘ] (tức là xác suất chấp nhận theo ví dụ gần với xác suất chấp nhận trung bình trên minibatch⁶), và tính toán ước lượng leave-one-out βₘ = (∑ₘ'≠ₘ c'ₘ' c̃ₘ')/(∑ₘ'≠ₘ c'ₘ'), trong đó c'ₘ := c(z'ₘ, y). Chúng tôi hạn chế ước lượng chỉ xem xét các ví dụ mà chúng tôi có lý lẽ đúng (tức là c'ₘ = 1), vì đây là những ví dụ duy nhất ảnh hưởng đến ước lượng gradient của chúng tôi. Loại bỏ c̃ₘ và c'ₘ khỏi ước lượng βₘ đảm bảo rằng βₘ độc lập với z̃ₘ.

⁵Có thể có một số ví dụ (đặc biệt là sớm trong quá trình huấn luyện) mà chúng tôi chưa tạo ra bất kỳ lý lẽ đúng nào. Chúng tôi bỏ qua những ví dụ này khỏi ước lượng gradient, vì chúng có likelihood 0 và do đó không thể là các mẫu đại diện từ posterior.

⁶Chúng tôi cũng đã thử giữ một ước lượng chạy của xác suất chấp nhận trung bình theo ví dụ, nhưng chúng tôi không thấy rằng sơ đồ phức tạp hơn này mang lại bất kỳ lợi thế thực nghiệm nào.

--- TRANG 6 ---
Lấy mẫu con gradient. Cuối cùng, như đã mô tả ở trên, chúng ta có thể giảm chi phí của bộ ước lượng gradient bằng cách sử dụng lấy mẫu có hệ thống để chọn một tập con các lý lẽ. Điều này không ảnh hưởng đến giá trị kỳ vọng của bộ ước lượng miễn là xác suất biên của việc chọn một lý lẽ tỷ lệ với trọng số tương ứng w̃ₘ, và gradient trung bình được tính trọng số lại bởi (∑²ᴹₘ₌₁ w̃ₘ)/(∑ₘ c'ₘ).

2.2 Tại sao không phải suy luận biến phân, reweighted wake-sleep, hoặc lấy mẫu từ chối?

Chúng tôi đã xem xét ba lựa chọn thay thế cho cách tiếp cận MCMC-EM mà chúng tôi theo đuổi trong bài báo này: EM biến phân (ví dụ, Bishop, 2006), reweighted wake-sleep (RWS; Bornschein & Bengio, 2015; Le et al., 2019), và lấy mẫu từ chối.

Kỳ vọng-tối đa hóa biến phân là một chiến lược phổ biến để huấn luyện các mô hình biến tiềm ẩn, nhưng suy luận biến phân với các biến tiềm ẩn rời rạc là thách thức (ví dụ, Tucker et al., 2017).

RWS là một lựa chọn hấp dẫn tránh gradient hàm điểm có phương sai cao; nó tiến hành bằng cách lấy mẫu M mẫu z₁:M từ một mô hình hướng dẫn qᵩ(z|x, y), gán các mẫu trọng số wₘ ∝ pθ(y,z|x)/qᵩ(z|x,y), và cập nhật cả tham số mô hình θ và tham số hướng dẫn ϕ để tối đa hóa log-probability có trọng số ∑ₘ wₘ log pθ(zₘ|x) và ∑ₘ wₘ log qᵩ(zₘ|x, y). Thật không may, chúng tôi thấy rằng huấn luyện RWS đôi khi dẫn đến các lý lẽ z có độ dài không thoái hóa. Hình 1 đề xuất một lời giải thích một phần: các chuỗi ngắn hơn nhận được trọng số cao hơn, vì vậy mô hình và hướng dẫn học tạo ra các chuỗi ngắn hơn và ngắn hơn cho đến khi chúng nhất quán tạo ra các lý lẽ rỗng.

[Hình 1 mô tả việc độ dài lý lẽ giảm trong quá trình huấn luyện RWS]

Tại sao các chuỗi dài hơn lại có xu hướng nhận được trọng số thấp hơn? Chúng ta có thể viết các trọng số chưa chuẩn hóa như w̃ₘ = c(y, zₘ)pθ(zₘ|x)/qᵩ(zₘ|x,y) = c(y, zₘ)∏ᵀᵐₜ₌₁ pθ(zₘ,ₜ|x,zₘ,₁:(ₜ₋₁))/qᵩ(zₘ,ₜ|x,y,zₘ,₁:(ₜ₋₁)), trong đó Tₘ là độ dài của zₘ và ε được thêm vào để giải quyết trường hợp không có mẫu nào đúng. Nếu có sự không khớp giữa q(zₘ,ₜ|x, zₘ,₁:(ₜ₋₁)) và p(zₘ,ₜ|x, zₘ,₁:(ₜ₋₁)), thì pθ(zₘ,ₜ|x,zₘ,₁:(ₜ₋₁))/qᵩ(zₘ,ₜ|x,y,zₘ,₁:(ₜ₋₁)) thường sẽ nhỏ hơn một, với các ngoại lệ trọng số cao hiếm hoi đảm bảo rằng Eq[p(z|x)/q(z|x)] = 1. Nếu những ngoại lệ này đủ hiếm để không thường xuất hiện trong một mẫu M chuỗi z₁:M, thì các trọng số chuẩn hóa w₁:M = w̃₁:M/∑ₘ w̃ₘ sẽ có xu hướng gán khối lượng cao hơn cho các chuỗi ngắn hơn trừ khi những chuỗi ngắn hơn đó ít có khả năng đúng hơn nhiều.

Với việc khởi tạo cẩn thận và điều chỉnh tốc độ học, chúng tôi đôi khi có thể làm cho RWS tránh được vấn đề lý lẽ rỗng này. Nhưng điều này dẫn đến một vấn đề mới: hướng dẫn qᵩ(z|x, y) học bắt chước chặt chẽ prior p(z|x) cho đến cuối lý lẽ, và sau đó chỉ đơn giản dán vào câu trả lời đúng cho dù nó có liên quan gì đến lý lẽ đến thời điểm đó hay không (tham khảo Turpin et al., 2023). Hình 5 trong Phụ lục E cho thấy một ví dụ đại diện trong đó mô hình hướng dẫn bỏ qua câu trả lời mà nó đã đi đến thông qua lý luận không chính xác và dán vào câu trả lời đúng.

Về mặt định lượng, ký hiệu t là chỉ số của token mà tại đó phần "câu trả lời cuối cùng" của lý lẽ bắt đầu, trong một lần chạy chúng tôi thấy rằng KL trung bình giữa q(z₁:ₜ|x, y) và p(z₁:ₜ|x) là khoảng 0.61 nats, trong khi KL có điều kiện giữa q(z₍ₜ₊₁₎:T|x, y, z₁:ₜ) và p(z₍ₜ₊₁₎:T|x, z₁:ₜ) là khoảng 42.5 nats, xác nhận rằng hướng dẫn không "lý luận ngược", chỉ sao chép câu trả lời đúng.

Cuối cùng, chúng tôi đã xem xét một sơ đồ lấy mẫu từ chối⁷ trong đó chúng tôi lấy mẫu K lý lẽ đề xuất z₁:K từ p(z|x), và tính trung bình các gradient từ những lý lẽ dẫn đến câu trả lời đúng. Chúng tôi sẽ trình bày kết quả định lượng trong Phần 4; phát hiện chính của chúng tôi là, mặc dù sơ đồ này có thể hoạt động, nó đòi hỏi giảm kích thước minibatch theo hệ số K để giữ chi phí mỗi lần lặp không đổi so với TRICE, điều này lại dẫn đến hội tụ chậm hơn và/hoặc kết quả cuối cùng tệ hơn.

⁷Chúng tôi cũng đã xem xét tối ưu hóa một ràng buộc có trọng số quan trọng (Burda et al., 2015) sử dụng prior p(z|x) như một phân phối đề xuất, nhưng thay vào đó đã chọn sơ đồ lấy mẫu từ chối đơn giản vì điều này ít thiên lệch hơn và tương đương khả thi trong bối cảnh của chúng tôi.

--- TRANG 7 ---
3 Công trình liên quan

Một số phương pháp đã đề xuất tạo lý lẽ cho các nhiệm vụ giải quyết vấn đề trong các mô hình chuỗi neural, bao gồm cả các cách tiếp cận có giám sát đầy đủ và few-shot (Wei et al., 2022; Nye et al., 2021; Kojima et al., 2022; Rajani et al., 2019; Shwartz et al., 2020; Wang et al., 2022b; Zhou et al., 2022; Creswell et al., 2022; Ye & Durrett, 2023). Đặc biệt liên quan đến cách tiếp cận của chúng tôi là chuỗi suy nghĩ tự nhất quán (Wang et al., 2022b), vì điều này có thể được xem gần đúng như việc biên hóa trên các lý lẽ ở thời gian kiểm tra. Kỹ thuật này đã được áp dụng thành công cho một loạt các nhiệm vụ lý luận định lượng (Lewkowycz et al., 2022). Có tương đối ít công trình hơn nhiều làm việc với việc quy kết hoặc tính trung bình trên các lý lẽ ở thời gian huấn luyện; có lẽ trường hợp chính là STaR (Zelikman et al., 2022), mà chúng tôi thảo luận trong Phần 3.1.

Dohan et al. (2022) trình bày một bài báo quan điểm ủng hộ việc biểu diễn một thành phần của các tương tác mô hình ngôn ngữ thông qua lập trình xác suất. Việc xử lý các lý lẽ như các biến tiềm ẩn của chúng tôi được lấy cảm hứng từ công trình đó. Lievin (2022) đưa ra một ví dụ khác về việc diễn giải LLM với CoT như các mô hình biến tiềm ẩn.

Suy luận biến phân (ví dụ, Kingma & Welling, 2013) và các phương pháp wake-sleep (ví dụ, Bornschein & Bengio, 2015) là những công cụ chính của cộng đồng mô hình hóa biến tiềm ẩn, nhưng như chúng tôi thảo luận trong Phần 2.2, chúng tôi thấy rằng thiên lệch của những phương pháp này gây ra các vấn đề nghiêm trọng. MCMC-EM là một chiến lược ít phổ biến hơn ngày nay, mặc dù một phiên bản của nó dựa trên lấy mẫu Gibbs (Geman & Geman, 1984) đã được áp dụng rộng rãi cho việc huấn luyện các mô hình đồ thị vô hướng (Tieleman, 2008). TRICE cũng có thể được đúc như một trường hợp của leo điểm Markovian (Naesseth et al., 2020).

ReAct (Yao et al., 2023) đã chứng minh rằng việc tiêm lý luận vào một vòng lặp quan sát-và-hành động theo kiểu RL làm tăng hiệu suất đáng kể. Cách tiếp cận này được mở rộng trong Reflexion (Shinn et al., 2023), trong đó một agent có thể phản chiếu có điều kiện về một quỹ đạo RL, tăng cường các ví dụ kết quả có thể được sử dụng như các ví dụ few-shot trong các rollout tiếp theo. Những cách tiếp cận này báo cáo cải thiện đáng kể trên các nhiệm vụ đánh giá tương ứng của chúng nhưng vẫn dựa vào mô hình có khả năng tạo ra phản hồi hữu ích và có thể thực hiện thông qua prompt few-shot thuần túy, trong khi phương pháp của chúng tôi tích cực tinh chỉnh mô hình để tạo ra các suy nghĩ phù hợp với nhiệm vụ.

Công trình gần đây về sử dụng công cụ trong các mô hình ngôn ngữ cũng hoạt động thông qua quy kết, suy luận nơi chèn các lời gọi đến công cụ (Parisi et al., 2022; Schick et al., 2023). Các hàm mất mát của chúng tương tự về tinh thần với của chúng tôi, lọc ra các quỹ đạo không dẫn đến câu trả lời hợp lệ. Trong bài báo này, chúng tôi đã xử lý các lý lẽ như các biến tiềm ẩn; người ta cũng có thể xử lý việc sử dụng công cụ như một biến tiềm ẩn.

3.1 Bộ lý luận tự học
Công trình liên quan gần nhất là bộ lý luận tự học (STaR; Zelikman et al., 2022). Ngoài các lập luận trong dẫn xuất của chúng, có ba sự khác biệt đáng kể giữa TRICE và STaR. Đầu tiên, STaR sử dụng giải mã tham lam, điều này làm giảm tính đa dạng của các lý lẽ mà nó huấn luyện. Các tác giả đã đưa ra lựa chọn này để giảm nguy cơ mô hình có được câu trả lời đúng bất chấp có lý lẽ xấu. Mặc dù chúng tôi thật sự thấy rằng thủ tục của chúng tôi đôi khi tạo ra câu trả lời đúng vì lý do sai, điều này dường như không cản trở mô hình cải thiện trên hầu hết các nhiệm vụ. Một lý do có thể là các mô hình cơ sở của chúng tôi mạnh hơn mô hình GPT-J 6B tham số được sử dụng trong bài báo STaR, vì vậy chúng có nhiều khả năng tạo ra lý lẽ tốt từ đầu hơn.

Một sự khác biệt thứ hai là TRICE lấy mẫu lại các lý lẽ mỗi lần lặp, vì vậy nó ít có khả năng overfit với bất kỳ lý lẽ cụ thể nào. STaR có một vòng lặp trong chạy nhiều lần lặp huấn luyện trên một tập lý lẽ đơn, có nghĩa là nó sử dụng các lý lẽ cũ để ước lượng gradient của likelihood biên. Trong các thí nghiệm của chúng tôi, chúng tôi quan sát thấy rằng điều này dẫn đến mô hình thực sự ghi nhớ một tập lý lẽ cố định cho tập huấn luyện. Một khi điều này xảy ra, thủ tục giải mã tham lam hầu như chắc chắn sẽ tái tạo chính xác các lý lẽ tương tự ở đầu vòng lặp ngoài tiếp theo. Nếu những lý lẽ này đều dẫn đến câu trả lời đúng, và STaR có lý lẽ cho mỗi câu hỏi, thì đây là một tối ưu toàn cục của likelihood biên trên tập huấn luyện! Nhưng về mặt thực nghiệm, STaR thường không tìm thấy lý lẽ tốt cho mỗi câu hỏi, và do đó nó bỏ qua một phần của tập huấn luyện (xem Phần 4).

Xu hướng bỏ qua các câu hỏi khó nhất trong tập huấn luyện này xuất phát từ dẫn xuất của STaR như một thuật toán gradient chính sách xấp xỉ cố gắng trực tiếp tối thiểu hóa mất mát 0-1 Ep[1-c(z, y)] = 1-pθ(y|x). Đạo hàm của likelihood biên này là pθ(y|x)∇θ log pθ(y|x), tức là nó là đạo hàm của log-likelihood biên (mà TRICE cố gắng tối đa hóa) được trọng số bởi pθ(y|x). Việc trọng số này khiến các ví dụ khó đóng góp ít vào gradient được sử dụng để cập nhật mô hình, vì vậy mô hình có thể "từ bỏ" các câu hỏi mà nó chưa thể giải quyết. Đây là một lập luận để cố gắng tối đa hóa log-likelihood thay vì likelihood.

Một sự khác biệt cuối cùng, nhỏ hơn là khi STaR cập nhật các lý lẽ của nó, nó có thể thay thế một lý lẽ từ mô hình p(z|x) bằng một lý lẽ từ một surrogate qθ(z|x, y). Khi mô hình ghi nhớ một tập các lý lẽ đúng cho tập huấn luyện, STaR trở nên ít có khả năng dự phòng vào surrogate, nhưng lựa chọn này có thể ảnh hưởng đến động lực huấn luyện sớm.

--- TRANG 8 ---
4 Thí nghiệm

Chúng tôi đánh giá TRICE trên tập dữ liệu GSM8K (Cobbe et al., 2021) và 27 nhiệm vụ BigBench-Hard (BBH) (Suzgun et al., 2022b) sử dụng LLM dựa trên Transformer cỡ trung PaLM 2-M (Anil et al., 2023) (Vaswani et al., 2017). Đối với các thí nghiệm BBH, chúng tôi sử dụng phiên bản Flan instruction-tuned (Chung et al., 2022) của PaLM 2; đối với GSM8K, chúng tôi sử dụng mô hình PaLM 2 cơ sở, vì GSM8K được bao gồm trong các tập dữ liệu huấn luyện Flan. Tất cả các thí nghiệm đều được chạy trên chip TPU v4 và v5e (Jouppi et al., 2023). Các ví dụ về lý lẽ được tạo ra có thể tìm thấy trong Phụ lục E.

Thay vì tinh chỉnh trọng số mô hình, chúng tôi sử dụng prompt tuning (Lester et al., 2021); chúng tôi đặt trước một chuỗi các vector embedding θ (một "soft prompt") vào các embedding tương ứng với prompt CoT đã token hóa được sử dụng để điều kiện hóa mô hình. Prompt tuning có thể đạt được cải thiện độ chính xác tương tự như tinh chỉnh đầy đủ, nhưng sử dụng một phần nhỏ các tham số. Chúng tôi khởi tạo soft prompt với chuỗi embedding thu được từ một loạt ba (cho BBH) hoặc năm (cho GSM8K) prompt CoT mẫu, mỗi prompt có dạng "Question: <QUESTION>\nAnswer: Let's think step by step.\n<RATIONALE>". Chúng tôi xem xét hai sơ đồ khởi tạo: một trong đó chúng tôi sử dụng các prompt CoT few-shot tiêu chuẩn được cung cấp với BBH, và một trong đó chúng tôi cố gắng bootstrap một prompt CoT few-shot bằng cách lấy mẫu các câu hỏi ngẫu nhiên từ tập huấn luyện, tạo ra các lý lẽ ngẫu nhiên từ mô hình cơ sở, và chọn ba hoặc năm ví dụ trong đó các lý lẽ ngẫu nhiên dẫn đến câu trả lời đúng. Sơ đồ đầu tiên có thể được xem như một cách tinh chỉnh một prompt few-shot ban đầu tốt, nhưng nó đòi hỏi một lượng nhỏ sự giám sát CoT chi tiết, trong khi sơ đồ thứ hai chỉ đòi hỏi sự giám sát nhãn.

Trên mỗi nhiệm vụ BBH, chúng tôi chia các ví dụ thành 60% tập huấn luyện và 40% tập kiểm tra. Đối với tất cả trừ ba nhiệm vụ, đây là 150 ví dụ huấn luyện và 100 ví dụ kiểm tra. Đối với GSM8K, chúng tôi sử dụng tập huấn luyện tiêu chuẩn 7473 ví dụ và tập kiểm tra 1319 ví dụ. Chúng tôi đánh giá độ chính xác của các mô hình CoT theo hai cách: đầu tiên, sử dụng giải mã tham lam (nhiệt độ 0), và thứ hai, sử dụng "tính nhất quán bản thân" (Wang et al., 2022b). Trong đánh giá tính nhất quán bản thân, chúng tôi rút 40 mẫu và kiểm tra xem câu trả lời phổ biến nhất có đúng không; đây là một bộ ước lượng plug-in cho dự đoán arg max_y p(y|x) tối thiểu hóa mất mát 0-1 dưới mô hình (mặc dù đây không phải là cách Wang et al. (2022b) ban đầu động cơ hóa thủ tục).

Chúng tôi so sánh với bốn phương pháp prompt-tuning cơ sở: direct prompt tuning, CoT prompt tuning, rejection sampling, và STaR (Zelikman et al., 2022). Tất cả các phương pháp đều được đánh giá trên cùng các tập validation, và sử dụng cùng các nhãn huấn luyện, prompt few-shot (trừ direct tuning, nơi chúng tôi chỉ sử dụng các cặp câu hỏi-câu trả lời), và các chiến lược khởi tạo tùy theo thích hợp. Chi tiết cho mỗi phương pháp và các siêu tham số thí nghiệm tương ứng có thể tìm thấy trong Phụ lục F.

Bảng 1 và Bảng 2 tóm tắt kết quả; tóm tắt BBH chi tiết theo từng nhiệm vụ có trong Phụ lục D. Ngay cả không có lý lẽ mẫu do con người tạo ra, TRICE có thể học tạo ra các lý lẽ dẫn đến câu trả lời đúng. TRICE cũng vượt trội hơn một mô hình được huấn luyện trực tiếp trên các lý lẽ do con người tạo ra trên GSM8K (tham khảo Uesato et al., 2022), có lẽ vì mất mát cross-entropy được sử dụng trong tinh chỉnh có giám sát có thể đặt trọng số nhiều hơn vào phong cách hơn là nội dung; cần nhiều bit hơn để mã hóa cách một người thể hiện một chuỗi lý luận hơn là để mã hóa chính các lý do.

Việc khởi tạo soft prompt với một prompt câu hỏi-lý lẽ-câu trả lời mẫu 3-shot do con người tạo ra cải thiện hiệu suất một chút trên BBH, cũng như đánh giá với tính nhất quán bản thân. Đến cuối huấn luyện, TRICE đã quản lý tạo ra ít nhất một lý lẽ hợp lệ cho hầu như tất cả các ví dụ huấn luyện, trong khi STaR thất bại trong việc tạo ra lý lẽ hợp lệ cho gần 10% các ví dụ huấn luyện. Không giống như trong các thí nghiệm được thực hiện trên Commonsense QA (Talmor et al., 2019) bởi Zelikman et al. (2022), STaR không vượt trội hơn mô hình direct-prompted prompt-tuned trên BBH. Điều này có thể là do mỗi nhiệm vụ BBH bao gồm tương đối ít dữ liệu huấn luyện (150 ví dụ so với 9,741 của CommonsenseQA), và do đó trong vòng lặp trong của nó STaR overfit với tập lý lẽ bootstrap tương đối nhỏ. Mặt khác, TRICE có thể overfit với tập câu hỏi nhỏ nhưng ít nhất có cơ hội tạo ra một tập lý lẽ hơi đa dạng từ những câu hỏi đó.

Một bằng chứng cho giả thuyết overfitting-rationales này là ở bước cuối cùng của vòng lặp trong cuối cùng, STaR (với khởi tạo bootstrap) đạt được mất mát cross-entropy cấp chuỗi huấn luyện (không phải per-token) dưới 0.06 trên tất cả các nhiệm vụ, và dưới 0.01 trên 19 trong số 27 nhiệm vụ. Điều này ngụ ý rằng nó đã học tái tạo chính xác một tập lý lẽ đơn với xác suất rất cao, điều này làm cho nó rất có khả năng tạo ra cùng những lý lẽ đó trong lần lặp tiếp theo.

Hình 2 so sánh các ước lượng cho GSM8K của likelihood biên huấn luyện trung bình (tức là tần suất một đề xuất được chấp nhận) và độ chính xác validation với giải mã tham lam như một hàm của số

--- TRANG 9 ---
[Bảng 1: Độ chính xác trung bình trên 27 nhiệm vụ BIG-Bench Hard (BBH)]

Chiến lược Prompt-Tuning | Khởi tạo | Độ chính xác Giải mã Tham lam (%) | Độ chính xác Tự nhất quán (%) | % Lý lẽ Hợp lệ
---|---|---|---|---
STaR | 3-shot Q-R-A Bootstrap | 62.0 | 62.1 | 91.6
Rejection Sampling | | 64.6 | 65.3 | -
TRICE không CV | | 67.8 | 68.0 | 98.7
TRICE có CV | | 72.8 | 73.1 | 98.8
Direct Prompt Tuning | 3-shot Q-A | 70.4 | - | -
TRICE không CV | 3-shot Q-R-A | 73.4 | 75.2 | 98.2
TRICE có CV | | 76.7 | 77.6 | 98.6

[Bảng 2: Độ chính xác trung bình trên GSM8K]

Chiến lược Prompt-Tuning | Độ chính xác Giải mã Tham lam (%) | Độ chính xác Tự nhất quán (%) | % Lý lẽ Hợp lệ
---|---|---|---
STaR | 53.5 | 60.1 | 80.2
CoT Prompt Tuning | 58.6 | 73.8 | -
Rejection Sampling | 77.9 | 87.0 | -
Direct Prompt Tuning | 19.4 | - | -
TRICE không CV | 72.8 | 81.5 | 98.9
TRICE có CV | 74.7 | 82.3 | 98.8
TRICE có CV (không bootstrap) | 77.7 | 86.6 | 98.4

các bước huấn luyện⁸ cho rejection sampling và cho TRICE có và không có sơ đồ control-variate. Sơ đồ control-variate cải thiện tốc độ hội tụ trung bình, đặc biệt là cuối quá trình huấn luyện khi xác suất tạo ra câu trả lời đúng trên tập huấn luyện tăng. Cả hai phiên bản của TRICE đều hội tụ đến độ chính xác huấn luyện cao nhanh hơn nhiều so với rejection sampling.

[Hình 2 hiển thị ước lượng biến thiên theo thời gian của độ chính xác tập huấn luyện trung bình và độ chính xác tập validation]

⁸Chúng tôi đặt chi phí mỗi lần lặp của rejection sampling và TRICE có và không có sơ đồ control-variate trực tiếp so sánh được: đối với rejection sampling, chúng tôi giảm kích thước minibatch theo hệ số bốn và tạo ra gấp bốn lần đề xuất cho mỗi ví dụ; đối với TRICE với sơ đồ control-variate, chúng tôi đặt kích thước minibatch gradient L bằng với số ví dụ mỗi minibatch M (lưu ý rằng điều này vẫn liên quan đến lấy mẫu con, vì mỗi ví dụ có thể đóng góp cả lý lẽ đúng và không đúng vào ước lượng gradient).

5 Thảo luận

Chúng tôi đã đề xuất TRICE, một phương pháp tinh chỉnh LLM để tốt hơn trong việc giải quyết các nhiệm vụ trả lời câu hỏi sử dụng prompt chuỗi suy nghĩ (CoT). Bằng cách định khung LLM được prompt CoT như một mô hình biến tiềm ẩn, chúng tôi có thể dẫn xuất một phương pháp tinh chỉnh có nguyên tắc và hiệu quả. Khi áp dụng cho GSM8K và các nhiệm vụ BIG-Bench Hard (BBH), TRICE vượt trội hơn ba baseline mạnh: direct prompt-tuning, STaR, và rejection sampling. Mặc dù chúng tôi dẫn xuất TRICE trong bối cảnh trả lời câu hỏi CoT, chiến lược MCMC-EM cơ bản của nó có thể được sử dụng rộng rãi hơn, ví dụ cho các vấn đề sử dụng công cụ.

Hạn chế: Chúng tôi chỉ đánh giá TRICE với prompt-tuning trên một LLM cỡ trung; có thể nó hoạt động khác đi trên các mô hình nhỏ hơn, lớn hơn, hoặc khi sử dụng các chiến lược tinh chỉnh khác. TRICE là một thuật toán tinh chỉnh dựa trên gradient, nhưng nhiều LLM có khả năng nhất là độc quyền, và chủ sở hữu của chúng thường không cung cấp bất kỳ cơ chế công khai nào cho tinh chỉnh dựa trên gradient. Điều này làm cho việc đánh giá TRICE hoạt động tốt như thế nào khi sử dụng với, chẳng hạn, GPT-4 (OpenAI, 2023) trở nên khó khăn. Cuối cùng, các đánh giá định lượng của chúng tôi tập trung vào việc liệu LLM có thể tạo ra câu trả lời đúng hay không; chúng tôi không đánh giá chính thức chất lượng lý luận trong chính các lý lẽ (tham khảo Uesato et al., 2022).

Tác động rộng hơn: Công trình này nhằm cải thiện khả năng của LLM bằng cách làm cho chúng có thể trả lời câu hỏi chính xác và minh bạch hơn. Tuy nhiên, các LLM có khả năng cao hơn có thể được sử dụng theo cách độc hại hoặc không an toàn, tinh chỉnh trên các tập dữ liệu trả lời câu hỏi không được tuyển chọn có thể đưa thiên lệch vào các mô hình, và các LLM được sử dụng rộng rãi hơn sẽ đóng góp dấu chân carbon lớn hơn.

Các lý lẽ có thể làm cho người dùng có động cơ dễ dàng hơn trong việc đánh giá độ tin cậy của đầu ra LLM. Nhưng nhiều người dùng có thể không đọc và phê phán lý lẽ của LLM, coi sự tồn tại đơn thuần của một lý lẽ như bằng chứng của sự thật. Nếu các lý lẽ chuỗi suy nghĩ thúc đẩy niềm tin không phê phán, chúng có thể dẫn đến tổn hại.

--- TRANG 10 ---
Lời cảm ơn: Chúng tôi đánh giá cao những đóng góp của Daniel Freeman và Enrique Piqueras đối với cơ sở hạ tầng mà chúng tôi sử dụng trong các thí nghiệm. Chúng tôi cảm ơn Kevin Murphy, Ben Lee, Brian Patton, và Jascha Sohl-Dickstein vì những thảo luận hữu ích.

Tài liệu tham khảo

Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.

Bishop, C. M. Pattern recognition and machine learning. Springer, 2006.

Bornschein, J. and Bengio, Y. Reweighted wake-sleep, 2015.

Burda, Y., Grosse, R., and Salakhutdinov, R. Importance weighted autoencoders. arXiv preprint arXiv:1509.00519, 2015.

Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.

Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

Cover, T. M. Elements of information theory. John Wiley & Sons, 1999.

Creswell, A., Shanahan, M., and Higgins, I. Selection-Inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712, May 2022.

Dohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D., Lopes, R. G., Wu, Y., Michalewski, H., Saurous, R. A., Sohl-dickstein, J., Murphy, K., and Sutton, C. Language model cascades, 2022.

Geman, S. and Geman, D. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Transactions on pattern analysis and machine intelligence, 6:721–741, 1984.

Hastings, W. K. Monte Carlo sampling methods using Markov chains and their applications. Biometrika, 57(1):97–109, 04 1970. ISSN 0006-3444. doi: 10.1093/biomet/57.1.97. URL https://doi.org/10.1093/biomet/57.1.97.

Hewitt, L. B., Le, T. A., and Tenenbaum, J. B. Learning to learn generative programs with memoised wake-sleep. In Uncertainty in Artificial Intelligence, 2020.

Hol, J. D., Schon, T. B., and Gustafsson, F. On resampling algorithms for particle filters. In 2006 IEEE Nonlinear Statistical Signal Processing Workshop, pp. 79–82, 2006. doi: 10.1109/NSSPW.2006.4378824.

Jouppi, N., Kurian, G., Li, S., Ma, P., Nagarajan, R., Nai, L., Patil, N., Subramanian, S., Swing, A., Towles, B., et al. Tpu v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings. In Proceedings of the 50th Annual International Symposium on Computer Architecture, pp. 1–14, 2023.

Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In Bengio, Y. and LeCun, Y. (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.

Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.

Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are Zero-Shot reasoners. arXiv preprint arXiv:2205.11916, May 2022.

Le, T. A., Kosiorek, A. R., Siddharth, N., Teh, Y. W., and Wood, F. Revisiting reweighted wake-sleep for models with stochastic control flow, 2019.

--- TRANG 11 ---
Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045–3059, 2021.

Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B., Gur-Ari, G., and Misra, V. Solving quantitative reasoning problems with language models, 2022.

Lievin, V. Deep Latent Variable Models for Natural Language Processing. PhD thesis, 2022.

Loshchilov, I. and Hutter, F. SGDR: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=Skq89Scxx.

Murphy, K. P. Probabilistic Machine Learning: An introduction. MIT Press, 2022. URL probml.ai.

Murray, I. and Salakhutdinov, R. Notes on the kl-divergence between a markov chain and its equilibrium distribution. preprint, 2008.

Naesseth, C., Lindsten, F., and Blei, D. Markovian score climbing: Variational inference with kl(p|| q). Advances in Neural Information Processing Systems, 33:15499–15510, 2020.

Nielsen, S. F. The stochastic EM algorithm: Estimation and asymptotic results. Bernoulli, 6(3): 457–489, June 2000.

Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., Sutton, C., and Odena, A. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, November 2021.

OpenAI. Gpt-4 technical report, 2023.

Owen, A. and Zhou, Y. Safe and effective importance sampling. Journal of the American Statistical Association, 95(449):135–143, 2000.

Parisi, A., Zhao, Y., and Fiedel, N. Talm: Tool augmented language models, 2022.

Rajani, N. F., McCann, B., Xiong, C., and Socher, R. Explain yourself! leveraging language models for commonsense reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4932–4942, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1487. URL https://aclanthology.org/P19-1487.

Roeder, G., Wu, Y., and Duvenaud, D. K. Sticking the landing: Simple, lower-variance gradient estimators for variational inference. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/e91068fff3d7fa1594dfdf3b4308433a-Paper.pdf.

Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools, 2023.

Shinn, N., Labash, B., and Gopinath, A. Reflexion: an autonomous agent with dynamic memory and self-reflection, 2023.

Shwartz, V., West, P., Le Bras, R., Bhagavatula, C., and Choi, Y. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4615–4629, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.373. URL https://aclanthology.org/2020.emnlp-main.373.

Suzgun, M., Scales, N., Schärli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., and Wei, J. Challenging BIG-Bench tasks and whether Chain-of-Thought can solve them. "arXiv preprint arXiv:2210.09261", October 2022a.

--- TRANG 12 ---
Suzgun, M., Scales, N., Schärli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022b.

Talmor, A., Herzig, J., Lourie, N., and Berant, J. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4149–4158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https://aclanthology.org/N19-1421.

Tieleman, T. Training restricted boltzmann machines using approximations to the likelihood gradient. In Proceedings of the 25th international conference on Machine learning, pp. 1064–1071, 2008.

Tierney, L. Markov chains for exploring posterior distributions. Annals of Statistics, 22(4):1701–1728, December 1994.

Tucker, G., Mnih, A., Maddison, C. J., Lawson, J., and Sohl-Dickstein, J. Rebar: Low-variance, unbiased gradient estimates for discrete latent variable models. Advances in Neural Information Processing Systems, 30, 2017.

Turpin, M., Michael, J., Perez, E., and Bowman, S. R. Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting. arXiv preprint arXiv:2305.04388, 2023.

Uesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N., Wang, L., Creswell, A., Irving, G., and Higgins, I. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017.

Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-Consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, March 2022a.

Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022b.

Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. "arXiv preprint arXiv:2201.11903", January 2022.

Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. React: Synergizing reasoning and acting in language models, 2023.

Ye, X. and Durrett, G. Explanation selection using unlabeled data for In-Context learning. "arXiv preprint arXiv:2302.04813", February 2023.

Zelikman, E., Wu, Y., and Goodman, N. D. Star: Bootstrapping reasoning with reasoning. arXiv preprint arXiv:2203.14465, 2022.

Zhou, D., Schärli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Bousquet, O., Le, Q., and Chi, E. Least-to-Most prompting enables complex reasoning in large language models. "arXiv preprint arXiv:2205.10625", May 2022.

--- TRANG 13 ---
Tài liệu Bổ sung cho "Huấn luyện Chuỗi Suy nghĩ thông qua Suy luận Biến Tiềm ẩn"

A Tổng quát hóa TRICE cho Các Mô hình Likelihood Không xác định

Để áp dụng TRICE vượt ra ngoài các vấn đề trả lời câu hỏi, chúng ta có thể muốn sử dụng một mô hình likelihood không xác định. Ví dụ, đầu ra mong muốn yₘ của chúng ta có thể là một tóm tắt của văn bản x, và zₘ có thể là một bảng nháp hoặc đề cương. Trong những tình huống như thế này, có thể có nhiều y thích hợp cho một x và z cho trước. Vậy từ góc độ mô hình hóa, có thể hợp lý khi làm cho p(y|x, z) có entropy khác không. Nhưng cũng có một lý do tính toán để ưu tiên mô hình như vậy: khi số lượng giá trị hợp lý mà y có thể nhận cho trước x tăng lên, xác suất lấy mẫu chính xác y đã được quan sát sẽ giảm xuống với tốc độ có thể là hàm mũ của kích thước không gian đầu ra.

May mắn thay, chúng ta có thể dễ dàng mở rộng TRICE để thích nghi với các likelihood không xác định. Các sự khác biệt là:

1. Như đã thảo luận trong Phần 2.1, xác suất chấp nhận cho một bộ lấy mẫu độc lập trong đó các đề xuất được tạo ra từ p(z|x) là α(z̃|z) = min{1, p(y|z̃,x)/p(y|z,x)}. Vậy thay vì chấp nhận xác định các latent được đề xuất đúng z̃, chúng ta cập nhật bộ nhớ một cách xác suất, luôn chấp nhận các đề xuất làm cho y quan sát được có khả năng cao hơn, nhưng đôi khi cũng chấp nhận các đề xuất làm cho y ít có khả năng hơn một chút.

2. Các trọng số control-variate βₘ bây giờ là một hàm của xác suất chấp nhận α: βₘ = (1/(M-1))∑ₘ'≠ₘ α(z̃ₘ'|zᵢₘ').

Giá trị của control variate trong bối cảnh này có thể ít hơn so với trong bối cảnh likelihood xác định. Ngay cả khi chúng ta học một mô hình nhất quán tạo ra các latent z tốt (theo nghĩa chúng dẫn đến đầu ra y hợp lệ), điều này không đảm bảo rằng nó sẽ nhất quán tạo ra các latent nhất quán với y cụ thể đã được quan sát. Ví dụ, có thể có nhiều cách hợp lý để phác thảo một văn bản dài, một số trong đó dẫn đến các đoạn tóm tắt khác nhau. Trong kịch bản này, xác suất chấp nhận có thể không hội tụ đến cái gì đó gần 1, và hiệu ứng giảm phương sai từ control variate sẽ khiêm tốn.

B Dẫn xuất Heuristic Tỷ lệ Control Variate

Phân tích của chúng tôi sẽ tập trung vào phương sai của phần tử thứ k ĝₖ của bộ ước lượng gradient. Để giảm sự lộn xộn, chúng tôi bỏ qua các chỉ số phụ và định nghĩa g' = ∇θₖ log p(z'|x), g̃ = ∇θₖ log p(z̃|x), và g = ∇θₖ log p(z|x), trong đó z' là lý lẽ được cập nhật, z̃ là lý lẽ được đề xuất, và z là lý lẽ trước đó trong bộ nhớ. Lưu ý rằng g' = g̃ nếu đề xuất z̃ được chấp nhận, và g' = g nếu z̃ bị từ chối. Chúng tôi giả định rằng lý lẽ trước đó z là một mẫu từ p(z|x, y). Chúng tôi cũng định nghĩa các ký hiệu viết tắt g₊ = E[g̃|c(z̃, y) = 1], g₋ = E[g̃|c(z̃, y) = 0], v = Var(g̃) = E[g̃²], v₊ = Var(g̃|c(z̃, y) = 1), v₋ = Var(g̃|c(z̃, y) = 0). Cuối cùng, chúng tôi định nghĩa xác suất chấp nhận một đề xuất π = E[c(z̃, y)].

Bộ ước lượng gradient của chúng tôi là
ĝ = g' - βg̃; E[ĝ] = g₊. (6)

Phương sai của ĝ là
E[(g' - g₊ - βg̃)²] = E[(g' - g₊)²] + β²E[g̃²] - 2βE[(g' - g₊)g̃] (7)
= v₊ + β²v - 2βE[g'g̃]. (8)

Trong Phương trình (8) chúng tôi sử dụng thực tế rằng E[g̃] = 0 và g₊ không ngẫu nhiên và do đó độc lập với g̃. Chia ra các trường hợp trong đó c(z̃, y) = 0 (vậy g' = g) và c(z̃, y) = 1 (vậy g' = g̃), kỳ vọng ngoài cùng bên phải là

E[g'g̃] = πE[g̃²|c(z̃, y) = 1] + (1-π)E[gg̃|c(z̃, y) = 0] (9)
= π(v₊ + g₊²) + (1-π)g₊g₋. (10)

Chúng ta có thể đơn giản hóa số hạng bên phải. Vì 0 = E[g̃] = πg₊ + (1-π)g₋, (1-π)g₊g₋ = -πg₊². Thay vào Phương trình (10), chúng ta có

E[g'g̃] = π(v₊ + g₊²) - πg₊² = πv₊. (11)

Vậy phương sai của ĝ đơn giản hóa thành
Var(ĝ) = (1 - 2βπ)v₊ + β²v. (12)

Lấy đạo hàm theo β cho thấy điều này được tối thiểu hóa khi
2(βv - πv₊) = 0; β⋆ = πv₊/v. (13)

Thay điều này trở lại Phương trình (12) cho phương sai tối ưu v⋆:
v⋆ = v₊ + π²v₊²/v - 2π²v₊²/v = v₊(1 - π²v₊/v). (14)

Chúng ta có thể mở rộng v bằng cách xem xét trường hợp c(z̃, y) = 1 (xảy ra với xác suất π) và trường hợp c(z̃, y) = 0 (xảy ra với xác suất 1-π):

v = E[g̃²] = πv₊ + πg₊² + (1-π)v₋ + (1-π)g₋²
= πv₊ - (1-π)g₊g₋ + (1-π)v₋ - πg₊g₋
= πv₊ + (1-π)v₋ - g₊g₋
= πv₊ + (1-π)v₋ + (1-π)/π g₊²
= πv₊ + (1-π)v₋ + (1-π)(2-π)g₊² + O((1-π)³)
= πv₊ + (1-π)v₋ + (1-π)(1 + 1-π)g₊² + O((1-π)³)
= πv₊ + (1-π)(v₋ + g₊²) + O((1-π)²), (15)

trong đó trong dòng thứ hai chúng tôi lại sử dụng thực tế rằng πg₊ = -(1-π)g₋, và trong dòng thứ ba đến cuối cùng chúng tôi xấp xỉ 1/π với xấp xỉ Taylor bậc nhất 1/π = 2-π + O((1-π)²). Như vậy, chúng ta có thể viết

πv₊/v = πv₊/(πv₊ + (1-π)(v₋ + g₊²) + O((1-π)²))
= 1/(1 + (1-π)/(πv₊)(v₋ + g₊²) + O((1-π)²))
= 1 - (1-π)/(πv₊)(v₋ + g₊²) + O((1-π)²). (16)

Vậy phương sai tối ưu v⋆ là
v⋆ = v₊(1 - π²v₊/v)
= v₊(1 - π + (1-π)/v₊(v₋ + g₊²) + O((1-π)²))
= (1-π)(v₊ + v₋ + g₊²) + O((1-π)²). (17)

Ngược lại, thay giá trị heuristic β = π của chúng tôi vào Phương trình (12) cho phương sai không tối ưu vπ:

vπ = (1-2π²)v₊ + π²v
= (1-2π²)v₊ + π²(πv₊ + (1-π)(v₋ + g₊²) + O((1-π)²))
= (1-2π²+π³)v₊ + π²(1-π)(v₋ + g₊²) + O((1-π)²)
= (1-π)(v₊ + v₋ + g₊²) + O((1-π)²), (18)

trong đó chúng tôi sử dụng xấp xỉ πᵏ = (1-(1-π))ᵏ = 1-k(1-π) + O((1-π)²) để đơn giản hóa các số hạng π² và π³. Như vậy, chúng tôi kết luận rằng v⋆ và vπ giống nhau đến số hạng O((1-π)²), và do đó khi xác suất π có được câu trả lời đúng tăng, tính không tối ưu của việc đặt β = π giảm nhanh hơn phương sai.

--- TRANG 14 ---
C Về Các Bộ Ước lượng Gradient Dựa Duy nhất trên Lý lẽ Không đúng

Chúng tôi áp dụng cùng các ký hiệu viết tắt như trong Phụ lục B.

Phân tích của chúng tôi tập trung vào đồng nhất thức
0 = E[g̃] = πg₊ + (1-π)g₋, (19)

liên hệ gradient mà chúng ta muốn ước lượng g₊ (gradient kỳ vọng cho trước lý lẽ đúng) với g₋ (gradient kỳ vọng cho trước lý lẽ không đúng).

C.1 Phương sai của các bộ ước lượng gradient lý lẽ không đúng

Phương trình (19) ngụ ý rằng
g₊ = -(1-π)/π g₋. (20)

Trì hoãn lúc này khó khăn trong việc ước lượng π⁻¹ (xem Phụ lục C.2 bên dưới), chúng ta có thể xem xét phương sai của một bộ ước lượng dựa trên vế phải của Phương trình (20):

Var[-(1-c(z̃, y))/π g̃] = E[(1-c(z̃, y))²]/π² g̃² - g₊²
= (1-π)/π² (v₋ + g₋²) - g₊²
= (1-π)/π² (v₋ + g₋²) - (1-π)²/π² g₋²
= (1-π)/π² (v₋ + πg₋²). (21)

Nếu π nhỏ, thì điều này trở thành
(1-π)/π² (v₋ + πg₋²) = v₋/π² + O(π⁻¹), (22)

vậy trừ khi phương sai v₋ của lý lẽ không đúng rất thấp, phương sai của bộ ước lượng này sẽ là O(π⁻²), rất cao. Ngược lại, phương sai của bộ ước lượng gradient dựa thuần túy trên lý lẽ đúng chỉ đơn giản là v₊, vậy trừ khi phương sai gradient cho lý lẽ không đúng thấp hơn đáng kể so với lý lẽ đúng, thì nếu π nhỏ thì lý lẽ không đúng sẽ dẫn đến ước lượng gradient nhiều nhiễu hơn.

Mặt khác, nếu 1-π nhỏ, thì chúng ta có
(1-π)/π² (v₋ + πg₋²) = (1-π)(v₋ + g₋²) + O((1-π)²), (23)

có thể là một cải thiện đáng kể so với phương sai v₊ của bộ ước lượng lý lẽ đúng; đặc biệt, nó tiến về không khi π tiến về 1.

C.2 TRICE control variate như một bộ ước lượng không thiên lệch dựa trên lý lẽ không đúng

Phương trình (20) ngụ ý rằng, về nguyên tắc, cho các ước lượng không thiên lệch của 1-π và π⁻¹, chúng ta có thể tính toán cập nhật gradient thuần túy dựa trên lý lẽ thất bại trong việc có được câu trả lời đúng. Các ước lượng không thiên lệch của 1-π dễ có được; chúng ta chỉ cần tính toán 1-c(z̃, y). Nhưng các ước lượng không thiên lệch của π⁻¹ khó có được hơn.

Thay vào đó, chúng ta có thể tính toán một bộ ước lượng thiên lệch bỏ qua số hạng π⁻¹ và sau đó điều chỉnh thiên lệch:

E[-(1-c(z̃, y))g̃] = -(1-π)g₋
= πg₊
= g₊ - (1-π)g₊. (24)

Vậy giá trị kỳ vọng của bộ ước lượng -(1-c(z̃, y))g̃ quá nhỏ theo hệ số (1-π)g₊. Chúng ta có thể điều chỉnh thiên lệch này bằng cách thêm vào một bộ ước lượng không thiên lệch của nó sử dụng g (gradient cho lý lẽ đúng từ bộ nhớ):

E[-(1-c(z̃, y))(g̃ - g)] = g₊ - (1-π)g₊ + (1-π)g₊ = g₊. (25)

Đây chính xác là bộ ước lượng gradient mà TRICE sử dụng khi lý lẽ lấy mẫu g̃ không đúng và β = 1. Các giá trị nhỏ hơn của β nội suy giữa bộ ước lượng này và bộ ước lượng dựa thuần túy trên lý lẽ đúng.

--- TRANG 15 ---
[Bảng 3: Kết quả BIG-Bench Hard theo từng nhiệm vụ - hiển thị chi tiết cho 27 nhiệm vụ khác nhau với các phương pháp khác nhau]

D Kết quả Thí nghiệm BBH Theo Từng Nhiệm vụ

Bảng 3 tóm tắt kết quả thí nghiệm của chúng tôi cho mỗi nhiệm vụ trong BBH.

E Các Lý lẽ Ví dụ

Hình 3 minh họa một số ví dụ về lý lẽ được tạo ra bởi LLM được tinh chỉnh TRICE trên nhiệm vụ BBH Logical Deduction Three Objects.

Hình 4 minh họa một số ví dụ về lý lẽ được tạo ra bởi LLM được tinh chỉnh TRICE trên GSM8K. Mặc dù chúng tôi thật sự tìm thấy các ví dụ trong đó LLM có được câu trả lời đúng vì lý do sai, điều này ít phổ biến hơn nhiều trên GSM8K so với BBH, vì không gian đầu ra số cho GSM8K lớn hơn nhiều so với nhiệm vụ BBH trắc nghiệm điển hình.

Hình 5 cho thấy một ví dụ trong đó mô hình hướng dẫn qφ(z|x, y) trong reweighted wake-sleep học bắt chước chặt chẽ mô hình prior pθ(z|x) cho đến cuối lý lẽ, tại thời điểm đó nó dán vào câu trả lời đúng.

F Chi tiết Phương pháp và Template

Trong phần này, chúng tôi trình bày thêm chi tiết về các phương pháp và template mà chúng tôi sử dụng trong các thí nghiệm.

Để lấy mẫu từ pθ(z|x), chúng tôi prompt LLM với template "Question: <QUESTION>\nAnswer: Let's think step by step.\n". Chúng tôi giới hạn độ dài của lý lẽ được tạo ra ở 1.25 lần độ dài của lý lẽ mẫu dài nhất được sử dụng để khởi tạo soft prompt. Để khởi tạo bộ nhớ (tức là lấy mẫu từ q(z|x, y) trong dòng 2 của Thuật toán 1), trên BBH chúng tôi lấy mẫu từ mô hình cơ sở với prompt "hướng dẫn" có dạng "Question: <QUESTION>\nAnswer: The answer is <ANSWER>. Let's think step by step.\n". Chúng tôi sử dụng cùng prompt hướng dẫn để tạo ra lý lẽ hóa trong STaR, nhưng với nhiệt độ 0 (xem bên dưới). Trên GSM8K, chúng tôi thay vào đó khởi tạo bộ nhớ với các mẫu từ pθ(z|x), vì chúng tôi thấy rằng khởi tạo bộ nhớ bằng prompt bao gồm câu trả lời dẫn đến

--- TRANG 16 ---
[Hình 3: Các ví dụ về lý lẽ cho nhiệm vụ BBH Logical Deduction Three Objects]

Ví dụ TRICE có câu trả lời đúng với lý lẽ tốt:
Câu hỏi: Trên kệ, có ba cuốn sách: một cuốn sách nâu, một cuốn sách vàng, và một cuốn sách cam. Cuốn sách vàng ở ngoài cùng bên trái. Cuốn sách cam ở bên phải cuốn sách nâu.
Lựa chọn:
(A) Cuốn sách nâu ở ngoài cùng bên phải
(B) Cuốn sách vàng ở ngoài cùng bên phải
(C) Cuốn sách cam ở ngoài cùng bên phải
Câu trả lời TRICE: Cuốn sách nâu ở bên phải cuốn sách vàng. Và cuốn sách cam ở bên phải cuốn sách nâu. Vậy câu trả lời là (C).

Ví dụ TRICE có câu trả lời đúng với lý lẽ xấu:
Câu hỏi: Một cửa hàng trái cây bán ba loại trái cây: mận, táo, và nhãn lồng. Nhãn lồng rẻ nhất. Mận ít đắt hơn táo.
Lựa chọn:
(A) Mận đắt thứ hai
(B) Táo đắt thứ hai
(C) Nhãn lồng đắt thứ hai
Câu trả lời TRICE: Táo đắt hơn mận, nhưng ít hơn nhãn lồng. Vậy câu trả lời là (A).

Ví dụ TRICE có câu trả lời sai:
Câu hỏi: Một cửa hàng trái cây bán ba loại trái cây: xoài, dưa hấu, và kiwi. Dưa hấu ít đắt hơn kiwi. Kiwi đắt thứ hai.
Lựa chọn:
(A) Xoài đắt nhất
(B) Dưa hấu đắt nhất
(C) Kiwi đắt nhất
Câu trả lời TRICE: Xoài đắt hơn Dưa hấu. Và đắt hơn kiwi. Vậy câu trả lời là (C).

--- TRANG 17 ---
[Hình 4: Các ví dụ về lý lẽ cho GSM8K]

Ví dụ TRICE có câu trả lời đúng với lý lẽ tốt:
Câu hỏi: Gabrielle bán trứng. Thứ Hai cô ấy bán 5 thùng trứng. Thứ Ba cô ấy bán gấp 2 lần số thùng trứng so với thứ Hai. Thứ Tư cô ấy bán ít hơn 2 thùng so với thứ Ba. Thứ Năm cô ấy bán bằng nửa số thùng trứng so với thứ Ba. Tổng cộng cô ấy bán bao nhiêu thùng trứng trong 4 ngày?
Câu trả lời TRICE: Thứ Hai = 5 thùng
Thứ Ba = 2 x 5 = 10 thùng
Thứ Tư = 10 - 2 = 8 thùng
Thứ Năm = 1/2 x 10 = 5 thùng
Tổng = 5 + 10 + 8 + 5 = 28 thùng
<answer>
28
</answer>

Ví dụ TRICE có câu trả lời đúng với lý lẽ xấu:
Câu hỏi: Họ của Samantha có ít hơn 3 chữ cái so với họ của Bobbie. Nếu Bobbie bỏ 2 chữ cái khỏi họ của mình, cô ấy sẽ có một họ dài gấp đôi họ của Jamie. Tên đầy đủ của Jamie là Jamie Grey. Họ của Samantha có bao nhiêu chữ cái?
Câu trả lời TRICE: Tên đầy đủ của Jamie gồm 5 chữ cái.
Nếu Bobbie bỏ 2 chữ cái khỏi họ của mình, cô ấy sẽ có một họ dài gấp đôi họ của Jamie, tức là 10 chữ cái.
Vậy họ của Bobbie có 10 chữ cái.
Nếu họ của Bobbie có 10 chữ cái, và họ của Samantha có ít hơn 3 chữ cái so với Bobbie, thì họ của Samantha có 10-3 = 7 chữ cái.
<answer>
7
</answer>

Ví dụ TRICE có câu trả lời sai:
Câu hỏi: John lái xe 3 giờ với tốc độ 60 mph rồi quay lại vì anh ấy nhận ra mình đã quên cái gì đó rất quan trọng ở nhà. Anh ấy cố gắng về nhà trong 4 giờ nhưng dành 2 giờ đầu trong tắc đường. Anh ấy dành nửa giờ tiếp theo lái xe với tốc độ 30mph, trước khi có thể lái xe thời gian còn lại trong 4 giờ với tốc độ 80 mph. Anh ấy cách nhà bao xa vào cuối 4 giờ đó?
Câu trả lời TRICE: Đầu tiên, anh ấy lái xe 3 giờ * 60 mph = 180 dặm cách xa nhà.
Sau đó, anh ấy quay lại và dành 2 giờ trong tắc đường (0 mph)
Sau đó, anh ấy dành 0.5 giờ ở 30 mph = 15 dặm
Sau đó, anh ấy dành 1.5 giờ ở 80 mph = 120 dặm
Vậy vào cuối 4 giờ, anh ấy cách nhà 180 - 15 + 120 = 285 dặm
<answer>
285
</answer>

--- TRANG 18 ---
[Hình 5: Ví dụ về lý lẽ cho GSM8K được tạo ra bởi RWS Guide]

Câu hỏi: Josh quyết định thử lật một căn nhà. Anh ấy mua một căn nhà với giá $80,000 rồi đầu tư $50,000 vào sửa chữa. Điều này làm tăng giá trị của căn nhà lên 150%. Anh ấy thu được bao nhiêu lợi nhuận?
Câu trả lời Guide:
Anh ấy mua căn nhà với giá 80000 và làm sửa chữa trị giá 50000.
Vậy tổng chi phí = 80000 + 50000 = 130000.
Bây giờ, giá trị tăng của căn nhà là 150% của 80000 = 120000.
Vậy lợi nhuận của anh ấy = 120000 - 130000 = -10000.
<answer>
70000
</answer>

Câu trả lời Model:
Anh ấy mua căn nhà với giá 80,000
Anh ấy chi 50,000 cho sửa chữa.
Vậy tổng đầu tư là 80,000 + 50,000 = 130,000
Giá trị của căn nhà sau sửa chữa là 150% của 80,000 = 1.5 * 80,000 = 120,000
Vậy tổng lợi nhuận là 120,000 - 130,000 = -10,000
<answer>
-10000
</answer>

--- TRANG 19 ---
hội tụ chậm hơn và kết quả tệ hơn; có thể việc bao gồm câu trả lời trong prompt đôi khi khuyến khích mô hình tạo ra giải thích không đáng tin cậy (Turpin et al., 2023).

Để đánh giá tính đúng đắn c(z, y) của lý lẽ z cho trước câu trả lời y, trong BBH chúng tôi tìm kiếm phần cuối của lý lẽ cho câu trả lời cuối cùng có dạng "the answer is <ANSWER>.". Trong GSM8K, chúng tôi khởi tạo soft prompt để khuyến khích mô hình bao câu trả lời trong thẻ "<answer>" và "</answer>", và sau đó trích xuất câu trả lời từ những thẻ đó. Để khuyến khích các ví dụ few-shot bootstrap trong GSM8K tuân theo template này, chúng tôi thêm ví dụ sau vào prompt CoT: "Question: What is 1 plus 1?\nAnswer: Let's think step by step.\n1 plus 1 is 2.\n<answer>\n2\n</answer>\n\n".

Hình 6 và Hình 7 cho thấy các ví dụ CoT few-shot bootstrap mà chúng tôi đã sử dụng để khởi tạo soft prompt trong các thí nghiệm.

TRICE. Đối với tất cả các nhiệm vụ BBH, chúng tôi chạy TRICE trong 500 bước với kích thước batch M = 32 và không sử dụng lấy mẫu con (tức là tính toán L = 64 gradient mỗi batch). Chúng tôi sử dụng optimizer Adam (Kingma & Ba, 2015) với tốc độ học ban đầu 1.0 và lịch trình giảm cosine (Loshchilov & Hutter, 2017) giảm tốc độ học 10 lần trong 450 bước đầu tiên. Đối với GSM8K, chúng tôi chạy TRICE trong 5000 bước với tốc độ học không đổi 1.0, kích thước batch M = 128, và tính toán L = 128 gradient mỗi batch.

STaR. Chúng tôi sử dụng một sự điều chỉnh của chiến lược STaR được đề xuất bởi Zelikman et al. (2022), trong đó chúng tôi làm prompt-tuning thay vì tinh chỉnh trên tất cả trọng số. Phương pháp xen kẽ giữa việc cập nhật bộ nhớ và tinh chỉnh lại mô hình từ đầu trên bộ nhớ đã cập nhật trong vòng lặp trong. Chúng tôi áp dụng thủ tục này cho 10 bước vòng lặp ngoài. Theo Zelikman et al. (2022), chúng tôi bắt đầu với 40 bước tối ưu hóa vòng lặp trong, tăng số bước vòng lặp trong lên 20% mỗi lần lặp vòng lặp ngoài lên đến tối đa 200 bước. Nếu mất mát huấn luyện xuống dưới 0.01, chúng tôi thoát khỏi vòng lặp trong. Đối với tối ưu hóa vòng lặp trong của STaR, chúng tôi sử dụng cùng khởi tạo prompt-tuning, siêu tham số Adam như trên, nhưng với giảm cosine từ 1.0 xuống 0.1 trong suốt mỗi vòng lặp trong. Để cập nhật bộ nhớ STaR, trước tiên chúng tôi thử tạo ra lý lẽ từ mô hình prompt-tuned bằng giải mã tham lam, sau đó nếu lý lẽ đó không đúng, dự phòng bằng lý lẽ hóa được tạo ra bởi giải mã tham lam từ cùng mô hình hướng dẫn mà chúng tôi sử dụng trong TRICE để khởi tạo bộ nhớ MCMC, và cuối cùng nếu cả hai thủ tục đều không tạo ra lý lẽ hợp lệ, chúng tôi bỏ qua ví dụ khỏi bộ nhớ.

Rejection Sampling. Chúng tôi giảm kích thước mini-batch theo 4 và rút 4 lý lẽ cho mỗi ví dụ trong mini-batch. Chúng tôi sử dụng cùng kích thước mini-batch, bước huấn luyện, và optimizer như trong TRICE cho tất cả các thí nghiệm BBH và GSM8K. Trong BBH, chúng tôi sử dụng tốc độ học ban đầu 1.0 như trong TRICE. Trong GSM8K, chúng tôi sử dụng tốc độ học 0.1 vì nó đạt được kết quả tốt hơn so với tốc độ học 0.3, và thủ tục huấn luyện trở nên không ổn định với tốc độ học 1.0.

CoT Prompt Tuning. Để thực hiện tinh chỉnh CoT có giám sát, chúng tôi prompt-tune mô hình để tối đa hóa log-likelihood của lý lẽ huấn luyện cho trước câu hỏi. Các tập dữ liệu BBH bao gồm rất ít lý lẽ mẫu, vì vậy chúng tôi không thể áp dụng chiến lược này cho BBH. Trên GSM8K, chúng tôi sử dụng cùng siêu tham số như trong TRICE trừ việc chúng tôi dừng sớm thuật toán sau chỉ 1000 bước huấn luyện, vì mô hình overfit tệ khi chúng tôi chạy lâu hơn.

Direct Prompt Tuning. Trong phương pháp này, mô hình cố gắng đoán câu trả lời trực tiếp mà không tạo ra lý lẽ; prompt-tuning để tối đa hóa log-likelihood của câu trả lời trong thiết lập này đơn giản, vì không có lý lẽ tiềm ẩn để tích phân. Chúng tôi khởi tạo soft prompt sử dụng 3 ví dụ từ tập huấn luyện và cắt ngắn độ dài của nó xuống 64. Thủ tục tối ưu hóa được thực hiện trong 150 bước với kích thước batch 16 và cùng siêu tham số Adam như trên, trừ việc chu kỳ giảm cosine được đặt thành 150 thay vì 450. Chúng tôi thấy rằng những điều chỉnh siêu tham số này từ các sơ đồ huấn luyện khác nhau cần thiết để giảm overfitting.

--- TRANG 20 ---
[Hình 6: Prompt 3-shot Q-R-A bootstrap được sử dụng trong các thí nghiệm formal_fallacies BBH]

Câu hỏi: "Không phải lúc nào cũng dễ dàng để hiểu ai đang tiêu thụ sản phẩm nào. Lập luận sau đây liên quan đến câu hỏi này: Trước tiên, việc là người tiêu dùng thường xuyên dầu gội KMS là cần thiết để trở thành người tiêu dùng thường xuyên xà phòng Yardley London. Tiếp theo, bất cứ ai là người dùng thường xuyên dầu gội Joico đều không phải là người tiêu dùng thường xuyên dầu gội KMS. Tất cả điều này dẫn đến việc không có người tiêu dùng thường xuyên xà phòng Yardley London nào là người dùng thường xuyên dầu gội Joico."

Lập luận, với các tiền đề được nêu một cách rõ ràng, có hợp lệ suy diễn hay không hợp lệ?
Lựa chọn:
- hợp lệ
- không hợp lệ

Câu trả lời: Hãy suy nghĩ từng bước.
Việc sử dụng thường xuyên một sản phẩm luôn đòi hỏi việc là người tiêu dùng thường xuyên sản phẩm đó. Mặc dù người dùng dầu gội Joico vẫn có thể mua chai xà phòng Yardley London đôi khi, anh ta/cô ta không bao giờ có thể là người tiêu dùng thường xuyên của cả hai loại xà phòng. Vậy câu trả lời là hợp lệ.

Câu hỏi: "Đây là một lập luận hoàn toàn hợp lệ: Để bắt đầu, bất cứ ai là chuyên gia về BSC Young Boys đều không phải là cựu fan của Real Betis Balompié. Bây giờ, bất cứ ai là bạn của FC Dynamo Kyiv đều không phải là chuyên gia về BSC Young Boys. Do đó, không có bạn nào của FC Dynamo Kyiv là cựu fan của Real Betis Balompié."

Lập luận, với các tiền đề được nêu một cách rõ ràng, có hợp lệ suy diễn hay không hợp lệ?
Lựa chọn:
- hợp lệ
- không hợp lệ

Câu trả lời: Hãy suy nghĩ từng bước.
Bạn có thể là bạn của FC Dynamo Kyiv nhưng không phải là chuyên gia về BSC Young Boys. Vậy câu trả lời là không hợp lệ.

Câu hỏi: "Không phải lúc nào cũng dễ dàng để thấy ai liên quan đến ai – và theo những cách nào. Lập luận sau đây liên quan đến câu hỏi này: Một số bạn cùng lớp của Terri là đồng nghiệp của Dolores. Bất cứ ai không phải là đồng nghiệp của Dolores đều là tổ tiên của Cheryl. Vậy, nhất thiết, một số tổ tiên của Cheryl không phải là bạn cùng lớp của Terri."

Lập luận, với các tiền đề được nêu một cách rõ ràng, có hợp lệ suy diễn hay không hợp lệ?
Lựa chọn:
- hợp lệ
- không hợp lệ

Câu trả lời: Hãy suy nghĩ từng bước.
Lý luận: Một số bạn cùng lớp của Terri không phải là đồng nghiệp của Dolores. Bất cứ ai không phải là đồng nghiệp của Dolores đều là tổ tiên của Cheryl. Vậy, nhất thiết, một số tổ tiên của Cheryl không phải là bạn cùng lớp của Terri.
Nhớ rằng bạn cùng lớp của Terri và đồng nghiệp của Dolores có thể trong bất kỳ trường hợp nào: Một trong ba có thể là bất kỳ người nào. Vậy câu trả lời là không hợp lệ.

--- TRANG 21 ---
[Hình 7: Prompt CoT 5-shot bootstrap được sử dụng trong các thí nghiệm GSM8K]

Câu hỏi: Có 3 quả thịt viên trên mỗi đĩa mì ống. Nếu 3 con trai của Theresa mỗi đứa ăn hai phần ba số thịt viên trên đĩa tương ứng của mình, thì tổng cộng còn lại bao nhiêu quả thịt viên trên đĩa của họ?

Câu trả lời: Hãy suy nghĩ từng bước.
Có 3x3 quả thịt viên trên mỗi đĩa trong 3 đĩa.
3 con trai mỗi đứa ăn 2/3 số thịt viên trên đĩa tương ứng của mình.
<answer>
3
</answer>

Câu hỏi: Joyce, Michael, Nikki và Ryn có một bộ phim yêu thích. Bộ phim yêu thích của Joyce dài hơn 2 giờ so với phim của Michael. Phim của Nikki dài gấp ba lần phim của Michael, và phim yêu thích của Ryn dài bằng 4/5 lần phim yêu thích của Nikki. Nếu phim yêu thích của Nikki dài 30 giờ, hãy tính tổng số giờ của các bộ phim yêu thích của họ cộng lại.

Câu trả lời: Hãy suy nghĩ từng bước.
Phim của Nikki = 30 giờ
Phim của Ryn = Nikki's * 4/5 = 4/5 * 30 = 24
Phim của Michael = Nikki's / 3 = 10  
Phim của Joyce = Michael's + 2 = 10 + 2 = 12
Tổng Giờ = Michael's + Nikki's + Ryn's + Joyce's = 10 + 30 + 24 + 12 = 76
<answer>
76
</answer>

Câu hỏi: Susan có một bó hoa gồm 3 tá hồng. Cô ấy đưa nửa cho con gái mình, và sau đó đặt phần còn lại vào bình. Ngày hôm sau, một phần ba số hoa trong bình bị héo. Sau khi bỏ những bông hoa héo, còn lại bao nhiêu bông hoa trong bình?

Câu trả lời: Hãy suy nghĩ từng bước.
Câu trả lời BƯỚC 1: 3 tá là 36
Câu trả lời BƯỚC 2: 1/2 của 36 là 18
Câu trả lời BƯỚC 3: 18-6 = 12
<answer>
12
</answer>

Câu hỏi: Jessie đã giành được gấp 3 lần số giải thưởng thể thao so với bạn Scott của mình, người đã giành được 4 giải thưởng. Vận động viên giỏi nhất ở trường trung học đối thủ đã giành được gấp đôi số giải thưởng so với Jessie. Người đối thủ đã giành được bao nhiêu giải thưởng?

Câu trả lời: Hãy suy nghĩ từng bước.
1. Jessie giành được gấp 3 lần số giải thưởng so với Scott, vậy Jessie giành được 3 x 4 = 12 giải thưởng trong khi Scott giành được 4. <sentence>
2. Vận động viên giỏi nhất ở trường đối thủ giành được gấp đôi so với Jessie. Vậy anh ta giành được 2 x 12 = 24 giải thưởng, nhiều hơn 12 so với Jessie, trong khi Scott giành được 4. <sentence>
3. Vận động viên giỏi nhất ở trường đối thủ giành được 24 giải thưởng. <answer>
<answer>
24
</answer>

Câu hỏi: James mua 5 gói nước ngọt, mỗi gói có 12 lon. Anh ấy đã có 10 lon. Anh ấy uống hết tất cả nước ngọt trong 1 tuần. Anh ấy uống bao nhiêu lon mỗi ngày?

Câu trả lời: Hãy suy nghĩ từng bước.
Có 5 gói với 12 lon mỗi gói và anh ấy còn 10 lon.
5 x 12 = 60 + 10 = 70 (tổng số lon)
70 / (tuần 7 ngày) = 10 lon mỗi ngày
<answer>
10
</answer>
