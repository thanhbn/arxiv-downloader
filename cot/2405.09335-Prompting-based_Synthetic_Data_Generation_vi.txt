# Tạo dữ liệu tổng hợp dựa trên prompting cho trả lời câu hỏi few-shot

Maximilian Schmidt1, Andrea Bartezzaghi2, Ngoc Thang Vu1
1Đại học Stuttgart, 2IBM Research Zurich
1{maximilian.schmidt, thang.vu}@ims.uni-stuttgart.de, 2abt@zurich.ibm.com

## Tóm tắt

Mặc dù các mô hình ngôn ngữ (LM) đã thúc đẩy hiệu suất của hệ thống trả lời câu hỏi, chúng vẫn cần nhiều dữ liệu. Ngược lại, việc chú thích dữ liệu là một quá trình tốn thời gian. Điều này đặc biệt áp dụng cho trả lời câu hỏi, nơi có thể các tài liệu lớn phải được phân tích và chú thích với các câu hỏi cùng câu trả lời tương ứng. Hơn nữa, các mô hình trả lời câu hỏi thường chỉ hoạt động tốt cho miền mà chúng được huấn luyện. Vì việc chú thích tốn kém, chúng tôi lập luận rằng kiến thức domain-agnostic từ LM, chẳng hạn như hiểu biết ngôn ngữ, là đủ để tạo ra một tập dữ liệu được tuyển chọn tốt. Với động lực này, chúng tôi chỉ ra rằng việc sử dụng các mô hình ngôn ngữ lớn có thể cải thiện hiệu suất trả lời câu hỏi trên nhiều tập dữ liệu khác nhau trong thiết lập few-shot so với các phương pháp tiên tiến hiện tại. Để làm điều này, chúng tôi thực hiện tạo dữ liệu tận dụng khung prompting, cho thấy rằng các mô hình ngôn ngữ chứa kiến thức có giá trị không phụ thuộc vào tác vụ có thể được sử dụng vượt ra ngoài lược đồ pre-training/fine-tuning thông thường. Kết quả là, chúng tôi nhất quán vượt trội hơn các phương pháp trước đây trong trả lời câu hỏi few-shot.

## 1. Giới thiệu

Trả lời câu hỏi đọc hiểu máy (MRQA) là một tác vụ quan trọng trong xử lý ngôn ngữ tự nhiên và cho phép dễ dàng truy cập thông tin bằng cách cung cấp câu trả lời cho các câu hỏi cụ thể. Trong khi có một số tác vụ phụ liên quan đến MRQA như QA miền mở, lựa chọn nhị phân/đa lựa chọn, QA hội thoại hoặc QA tạo sinh, chúng tôi tập trung vào QA trích xuất trong công việc này. Trong QA trích xuất, mục tiêu là tìm câu trả lời cho một câu hỏi bằng cách trích xuất nó từ một bối cảnh đã cho. MRQA cũng đã thu hút sự chú ý trong cộng đồng như một đại diện nơi các tác vụ khác được đúc thành các bài toán trả lời câu hỏi, do đó cho phép một loạt ứng dụng rộng rãi. Điều này bao gồm, ví dụ, nhận dạng thực thể có tên (NER, Li et al., 2020; Arora và Park, 2023), trích xuất quan hệ thực thể (Levy et al., 2017; Li et al., 2019; Zhang et al., 2022) và điền slot (Gao et al., 2019).

Pre-training các mô hình ngôn ngữ (LM) trên các mục tiêu hiểu ngôn ngữ tự nhiên (NLU) như mô hình hóa ngôn ngữ có mặt nạ (MLM, Devlin et al., 2019) đã dẫn đến các mô hình MRQA mạnh mẽ (Rajpurkar et al., 2016) và thậm chí vượt qua mức độ con người. Vì tác vụ downstream sử dụng một hàm mục tiêu khác, việc fine-tuning các PLM được pre-training là cần thiết để thích nghi với tác vụ. Có thể lập luận rằng, sự không khớp này dẫn đến kết quả kém nếu dữ liệu có nhãn cho tác vụ downstream khan hiếm. Tuy nhiên, việc chú thích dữ liệu cho MRQA tốn thời gian và đắt đỏ. Ngoài ra, MRQA few-shot đặt ra một thách thức thú vị, đặc biệt đối với các miền cụ thể, nơi cần nỗ lực cao để chú thích dữ liệu hoặc thiếu các chuyên gia miền.

Để giải quyết thiết lập MRQA low-resource, công việc trước đây đã đề xuất tạo dữ liệu tổng hợp để tăng cường tập huấn luyện (ví dụ: Alberti et al., 2019; Puri et al., 2020; Shakeri et al., 2020, 2021). Với mục tiêu tương tự, PLM đã được sử dụng cho các tác vụ khác trong xử lý ngôn ngữ tự nhiên (ví dụ: Anaby-Tavor et al., 2020; Schick và Schütze, 2021). Tuy nhiên, MRQA thách thức hơn khi tạo dữ liệu tổng hợp: Chúng ta không thể chỉ đơn giản tạo văn bản từ một nhãn, mà phải nghĩ ra đầu vào của mẫu cũng như nhãn của nó dưới dạng câu hỏi và câu trả lời. Ngoài ra, cả câu trả lời và câu hỏi đều phụ thuộc lẫn nhau.

Trong công việc này, chúng tôi khám phá một phương pháp cho phép điều này; một tổng quan cấp cao được đưa ra trong hình 1. Cụ thể hơn, chúng tôi nhằm trả lời các câu hỏi nghiên cứu sau:

**RQ1:** Làm thế nào chúng ta có thể sử dụng LM để tạo dữ liệu tổng hợp để cải thiện tác vụ MRQA few-shot?

**RQ1.1:** Dữ liệu tổng hợp có thể cải thiện hiệu suất đến mức nào?

**RQ1.2:** Việc lựa chọn câu trả lời ảnh hưởng đến hiệu suất như thế nào?

**RQ1.3:** Chúng ta có cần dữ liệu có nhãn hay LM có thể tạo ra dữ liệu hữu ích ngay lập tức?

**RQ2:** Phương pháp được đề xuất tổng quát hóa cho các miền khác như thế nào?

Để đạt được điều này, chúng tôi tin rằng có một cách hiệu quả hơn để sử dụng LM: Chúng tôi đề xuất sử dụng kiến thức ngôn ngữ được mã hóa trong các mô hình này để tạo dữ liệu tổng hợp cho miền mục tiêu để chống lại tác động của việc khan hiếm dữ liệu. Để làm điều này, chúng tôi sử dụng khả năng của LM để tạo câu hỏi có điều kiện trên đầu vào, và chúng tôi lập luận rằng điều này có thể dễ dàng được thực hiện trong bất kỳ miền mục tiêu nào vì chúng tôi xây dựng trên các PLM không giám sát.

Tóm lại, những đóng góp của chúng tôi như sau: 1) Chúng tôi đề xuất một phương pháp tạo dữ liệu có nhãn có giá trị cho miền mục tiêu bằng cách sử dụng kiến thức ngôn ngữ được mã hóa trong LM; 2) chúng tôi cải thiện hiệu suất của QA few-shot cho nhiều kích thước tập dữ liệu khác nhau trên các miền khác nhau để thu hẹp thêm khoảng cách hiệu suất giữa thiết lập few-shot và dữ liệu đầy đủ; 3) chúng tôi chứng minh chất lượng cao của các câu hỏi được tạo bởi phương pháp của chúng tôi trong một nghiên cứu người dùng.

Trong khi giới thiệu một phương pháp mới, mạnh mẽ vượt trội hơn nhiều phương pháp tiên tiến hiện tại trong MRQA few-shot, mô hình của chúng tôi thậm chí vượt trội hơn thiết lập dữ liệu đầy đủ của TextbookQA với 64% F1 chỉ với 64 mẫu có nhãn.

## 2. Công việc liên quan

Trong phần này, chúng tôi xem xét các công việc hiện có liên quan đến thiết lập của chúng tôi, tức là few-shot, cũng như các ứng dụng cho prompting.

### MRQA low-resource

Mặc dù không có thỏa thuận giữa các nghiên cứu về thiết lập few-shot có thể bao gồm bao nhiêu dữ liệu (Hedderich et al., 2021), mục tiêu khi xử lý thiết lập few-shot là giảm chi phí và thời gian chú thích đắt đỏ, tùy thuộc vào miền, có thể yêu cầu kiến thức chuyên gia miền. Hơn nữa, đối với một số miền, việc tìm kiếm chuyên gia hoặc các tài nguyên khác là một thách thức (Otegi et al., 2020).

Một số phương pháp xử lý các thiết lập nơi lượng dữ liệu bị hạn chế. Nhiều trong số chúng áp dụng kỹ thuật pre-training không giám sát. Trong khi Ram et al. (2021) đưa ra một mục tiêu pre-training cụ thể cho QA, nhiều người khác thích nghi LM với miền mục tiêu sử dụng mục tiêu pre-training của nó (Zhang et al., 2020; Nishida et al., 2020; Pergola et al., 2021; Chen et al., 2023).

Mặc dù pre-training LM tự giám sát cũng được coi là tăng cường dữ liệu, cũng tồn tại một số phương pháp xử lý tăng cường dữ liệu cụ thể cho tác vụ và miền. Ví dụ, các instance huấn luyện có thể được thao tác bằng cách thực hiện các thao tác trên đầu vào giữ nguyên nhãn (Zhang et al., 2020), hoặc dữ liệu có nhãn mới có thể được tổng hợp (Alberti et al., 2019; Shakeri et al., 2020, 2021).

Khi con người tích cực tham gia vào phát triển mô hình, học tích cực trở nên khả thi (Settles, 2012; Schmidt et al., 2022).

### (L)LM, Prompting

Như đã đề cập ở đầu, prompting (Liu et al., 2021) nhằm cải thiện các tác vụ downstream bằng cách căn chỉnh mục tiêu pre-training với mục tiêu downstream. Cũng có một số công việc sử dụng prompting cho thiết lập few-shot (Liu et al., 2021; Schick và Schütze, 2021). Theo kinh nghiệm, prompting giảm bớt nhu cầu dữ liệu có nhãn (Radford et al., 2019; Brown et al., 2020) và cũng thúc đẩy hiệu suất QA trong thiết lập few-shot (Chada và Natarajan, 2021; Castel et al., 2022). Ví dụ, Chada và Natarajan (2021) và Wang et al. (2022a) căn chỉnh tác vụ MRQA với mục tiêu pre-training bằng cách đúc các bộ ba context-question-answer thành tái tạo câu trả lời, nơi câu trả lời được giải mã bằng LM từ bối cảnh và câu hỏi. Castel et al. (2022) thích nghi phương pháp này cho MRQA trích xuất bằng cách chỉ giải mã từ bối cảnh đã cho, tức là tính toán xác suất trên tất cả các span có thể từ bối cảnh.

Một số phương pháp nhằm cải thiện prompting: Ví dụ, các token mềm (Liu et al., 2022; Li và Liang, 2021; Zhong et al., 2021) có thể cho phép mô hình thích nghi tốt hơn với các tác vụ downstream và dữ liệu đầu vào, và học minh họa (Gao et al., 2021) có thể giúp mô hình hoạt động tốt trong thiết lập few-shot.

Prompting cũng có thể được sử dụng thêm cho tăng cường dữ liệu (Anaby-Tavor et al., 2020).

## 3. Phương pháp

Ở đây, chúng tôi đưa ra tổng quan về vấn đề và mô tả chi tiết phương pháp của chúng tôi.

Chính thức, MRQA được định nghĩa là cho bối cảnh c và câu hỏi q, mục tiêu là dự đoán câu trả lời a=f(c, q). Chúng tôi tiếp tục tập trung vào MRQA trích xuất, tức là, a là một span liên tục đơn trong c.

Tiếp theo, chúng tôi giới thiệu phương pháp của chúng tôi tận dụng khung prompting. Ý tưởng cấp cao của phương pháp chúng tôi bao gồm hai bước: Đầu tiên, chúng tôi lấy mẫu các ứng cử viên câu trả lời từ một tài liệu. Trong bước thứ hai, chúng tôi sau đó truy vấn một LM được pre-training để tạo câu hỏi sử dụng tài liệu và các câu trả lời được lấy mẫu trước đó. Một tổng quan về pipeline tạo dữ liệu của chúng tôi được đưa ra trong hình 2. Trong phần sau, chúng tôi mô tả chi tiết từng bước của phương pháp.

### 3.1. Lấy mẫu câu trả lời

Để lấy mẫu các ứng cử viên câu trả lời, chúng tôi áp dụng NER cho bối cảnh c và các thực thể kết quả được sử dụng làm câu trả lời văn bản ac với các span s (một tuple của chỉ số bắt đầu và kết thúc ký tự). Chúng tôi chọn kỹ thuật này vì nó là một phương pháp đơn giản, ít tài nguyên và không cần có kiến thức về chủ đề của miền (tức là, bất kỳ mô hình NER tiếng Anh nào cũng đủ cho các miền của chúng tôi). Hơn nữa, NER khả thi trong nhiều ngôn ngữ và các tập dữ liệu mà chúng tôi đánh giá phương pháp của mình, benchmark MRQA few-shot, hoạt động với phong cách như vậy. Kết quả là, nó có thể được áp dụng cho bất kỳ miền nào mà một mô hình NER tồn tại trong ngôn ngữ đã cho. Lưu ý rằng NER không nhất thiết phải dựa vào nhiều mẫu có nhãn hoặc dữ liệu có nhãn (ví dụ: các phương pháp dựa trên quy tắc, sử dụng giám sát yếu (Lison et al., 2020) hoặc prompting (Liu et al., 2022; Ma et al., 2022)).

### 3.2. Tạo câu hỏi

Trong prompting, một LM nhận một đầu vào văn bản và, tùy thuộc vào mục tiêu huấn luyện, dự đoán token tiếp theo (như trong trường hợp mô hình hóa ngôn ngữ) hoặc một hoặc nhiều token có mặt nạ. Ví dụ, T5 (Raffel et al., 2020) là một mô hình encoder-decoder được cung cấp một đầu vào văn bản có thể chứa nhiều token có mặt nạ. Đối với mỗi token có mặt nạ, một hoặc nhiều token có thể xuất hiện trong đầu ra được đặt tiền tố bởi một token sentinel, đánh dấu token có mặt nạ trong đầu vào mà các token sau thuộc về. Cho mục đích của chúng tôi, chúng tôi chỉ sử dụng một mặt nạ duy nhất trong đầu vào.

Để tạo câu hỏi, chúng tôi biến đổi các đầu vào mẫu thành prompt cho LM. Cho mục đích này, chúng tôi áp dụng một template, do đó thay thế các placeholder (được đánh dấu bắt đầu bằng < và kết thúc bằng >) bằng các giá trị thực tế từ mẫu. Vì chúng tôi nhằm tạo một câu hỏi cho một bối cảnh và một ứng cử viên câu trả lời được lấy mẫu, template được công thức hóa để bao gồm bối cảnh và câu trả lời, và đầu ra mong đợi là câu hỏi. Do đó câu hỏi được định nghĩa chính thức là:

p(q|c, ac) = ∏[t=1 to T] log p(qt|q<t, c, ac)    (1)

Trong thời gian huấn luyện, chúng tôi sử dụng mục tiêu gốc được sử dụng để pre-training mô hình ngôn ngữ cơ bản để mô hình hóa xác suất của câu hỏi bằng cách chỉ tính toán loss trên câu hỏi q trong đầu ra. Trong các thí nghiệm sơ bộ, chúng tôi đã thấy rằng việc dựa vào các mô hình sequence-to-sequence là quan trọng, vì chúng cho phép điều kiện hóa đầu ra không chỉ trên các token trước mà trên toàn bộ chuỗi. Chúng tôi tin rằng điều này là do công thức tự nhiên hơn nơi một câu hỏi xảy ra trước câu trả lời của nó trong một câu. Ngược lại, chỉ sử dụng mô hình giải mã từ trái sang phải, đầu vào sẽ phải được công thức hóa sao cho câu hỏi được tạo có thể được trả lời bởi câu trả lời được đưa ra trước đó (tức là, bên trái câu hỏi). Rõ ràng, điều này không chỉ mang lại prompt dài hơn mà còn tăng độ phức tạp của nó. Hơn nữa, chúng tôi sử dụng các token mềm trong đầu vào. Tức là, tất cả các token văn bản từ template được huấn luyện thêm vào các trọng số mô hình còn lại và chúng tôi khởi tạo chúng với các trọng số tương ứng từ word embedding được pre-training.

Để tạo sinh, chúng tôi giải mã câu hỏi q từng token một và áp dụng lọc. Chúng tôi làm điều này vì các lý do sau: 1) câu hỏi được tạo có thể nhiễu, ví dụ: không phải là một câu hỏi hợp lệ, và 2) câu hỏi được tạo có thể không hữu ích cho tác vụ downstream trả lời câu hỏi, có thể bị under-specified. Ví dụ, câu hỏi được tạo có thể có - ngoài câu trả lời được cung cấp - một số câu trả lời chính xác khác trong một bối cảnh đã cho.

Như kỹ thuật lọc, chúng tôi áp dụng một quy trình hai bước. Đầu tiên, chúng tôi loại bỏ các mẫu được tạo dựa trên lọc dựa trên quy tắc. Sau đó chúng tôi áp dụng lọc nhất quán (Alberti et al., 2019; Anaby-Tavor et al., 2020) mà một mô hình tương tự như mô hình MRQA cuối cùng được sử dụng. Các mẫu được tạo bị loại bỏ tùy thuộc vào điểm F1 của câu trả lời dự đoán (nơi tham chiếu là câu trả lời được tạo) sử dụng mô hình MRQA. Chúng tôi không sử dụng lọc nhất quán lặp (Wang et al., 2022b) vì điều này liên quan đến việc huấn luyện lại mô hình MRQA trong mỗi lần lặp, dẫn đến tăng độ phức tạp điều chỉnh siêu tham số và cạn kiệt tài nguyên.

## 4. Thiết lập thí nghiệm

Ở đây, chúng tôi đầu tiên giới thiệu tập dữ liệu SQuAD few-shot theo sau là các chi tiết triển khai của phương pháp chúng tôi và mô tả các baseline mà chúng tôi xem xét.

### 4.1. Thiết lập few-shot

Chúng tôi thực hiện các thí nghiệm trên một số tập dữ liệu để so sánh với các phương pháp hiện có. Để làm điều này, chúng tôi dựa vào các split huấn luyện và kiểm tra được lấy mẫu con từ benchmark Few-Shot MRQA từ Ram et al. (2021) dựa trên các phiên bản được xử lý trước từ MRQA Shared Task 2019 (Fisch et al., 2019). Cụ thể hơn, điều này bao gồm các miền SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NaturalQuestionsShort (NQ) (Kwiatkowski et al., 2019), NewsQA (Trischler et al., 2017), HotpotQA (Yang et al., 2018), BioASQ (Tsatsaronis et al., 2015) và TextbookQA (Kembhavi et al., 2017) và chúng tôi đánh giá phương pháp của mình sử dụng các split với 16, 32, 64 và 128 mẫu huấn luyện. Chúng tôi thực hiện các thí nghiệm cho RQ1 trên SQuAD trong khi các tập dữ liệu còn lại được sử dụng để kiểm tra khả năng tổng quát hóa của phương pháp chúng tôi (RQ2).

### 4.2. Tạo dữ liệu

Để tạo câu hỏi, chúng tôi đã thử nghiệm các mô hình, template và chiến lược xử lý trước khác nhau trong các thí nghiệm sơ bộ. Chúng tôi thấy T5 (Raffel et al., 2020) hoạt động tốt nhất. Thay vì mô hình v1 gốc, sử dụng dữ liệu có nhãn trong quá trình pre-training do đó vi phạm thiết lập few-shot của chúng tôi, chúng tôi sử dụng mô hình v1.1 trong biến thể lớn (∼800M tham số). Các mô hình chỉ decoder như GPT-2 (Radford et al., 2019) hoạt động kém hơn như đã đề cập trong phần 3.2. Trong các template, chúng tôi đã xem xét độ nhạy cảm chữ hoa chữ thường cũng như các cách diễn đạt khác nhau. Kết quả của việc điều tra thủ công, context: <context> question: <mask> answer: <answer>. hoạt động tốt cho mục đích của chúng tôi và tương tự với các phát hiện của Castel et al. (2022).

#### 4.2.1. Huấn luyện mô hình tạo câu hỏi

Để huấn luyện mô hình tạo dữ liệu, tương tự như Castel et al. (2022), chúng tôi tạo một tập phát triển học thuật để phục vụ thiết lập few-shot nơi có một split phát triển riêng dẫn đến khả năng tổng quát hóa kém do kích thước nhỏ của nó. Do đó, chúng tôi điều chỉnh learning rate và số bước huấn luyện trên một tập validation của 2048 mẫu từ dữ liệu huấn luyện của SQuAD, và chọn tập siêu tham số có hiệu suất chuẩn hóa tốt nhất trên tất cả các kích thước few-shot như mô tả trong Castel et al. (2022). Kết quả là, mô hình tạo câu hỏi được huấn luyện trong 130 bước huấn luyện với batch size 32 sử dụng learning rate tuyến tính 1e-4 với optimizer Adafactor. Hơn nữa, các token mềm thêm 8192 trọng số cho mô hình tạo câu hỏi. Chúng tôi cũng chia nhỏ các bối cảnh được cung cấp sử dụng stride 100 token để tối đa 450 token của bối cảnh được bao gồm trong một đầu vào duy nhất để cho phép không gian đủ cho câu hỏi. Vì việc chia nhỏ có thể tạo ra các instance nơi câu trả lời không phải là một phần của bối cảnh, chúng tôi loại bỏ chúng vì chúng tôi không thể mong đợi chúng mang lại một câu hỏi đúng về mặt ngữ nghĩa.

#### 4.2.2. Tạo dữ liệu tổng hợp

Để tạo dữ liệu tổng hợp, trong trường hợp của SQuAD, TriviaQA, NQ, NewsQA, SearchQA và HotpotQA, chúng tôi sử dụng các tài liệu từ corpus huấn luyện. Vì BioASQ và TextbookQA đều bao gồm khá ít tài liệu, chúng tôi thu thập tóm tắt từ PubMed và các bài học từ CK-12, tương ứng, cho mục đích tạo dữ liệu. Sau đó chúng tôi áp dụng NER của stanza sử dụng tất cả các loại thực thể của nó để lấy mẫu câu trả lời từ các tài liệu này. Sau đó, tương tự như thời gian huấn luyện, chúng tôi áp dụng chia nhỏ với stride 100 token để cung cấp các tài liệu vào mô hình (bằng cách thực hiện template) trong đó chúng tôi chỉ giữ lại các instance nơi câu trả lời được chứa trong bối cảnh vì lý do tương tự như trên. Trong bước tiếp theo, câu hỏi được tạo được giải mã tham lam với beam size 5, top-k sampling với k bằng 20 và nucleus sampling (Holtzman et al., 2020) giữ các token bao gồm 95% khối lượng xác suất trong mỗi bước.

Các token đặc biệt của mô hình ngôn ngữ được loại bỏ trong bước tiếp theo. Vì chúng tôi chỉ cho phép một mặt nạ trong đầu vào (cho câu hỏi), chúng tôi đảm bảo rằng chỉ các token đầu ra tương ứng với mặt nạ này được sử dụng.

Để lọc dựa trên quy tắc trong bước tiếp theo, chúng tôi chọn ngẫu nhiên 1.000.000 mẫu và loại bỏ các mẫu được tạo nơi câu trả lời được chứa trong câu hỏi, hoặc câu hỏi chỉ chứa các từ vô nghĩa hoặc trống. Sau đó, chúng tôi áp dụng lọc nhất quán loại bỏ các mẫu được tạo với điểm F1 dưới 80% sử dụng mô hình MRQA dựa trên prompting được huấn luyện tương tự như được mô tả trong phần tiếp theo.

### 4.3. Mô hình MRQA

Cho bước cuối cùng của phương pháp chúng tôi, chúng tôi huấn luyện một mô hình MRQA sử dụng dữ liệu tổng hợp và có nhãn có sẵn như được hiển thị trong hình 2. Vì một phương pháp dựa trên prompting hoạt động tốt hơn một đầu trích xuất span trên đầu mô hình encoder dựa trên Transformer (Vaswani et al., 2017), chúng tôi cũng sử dụng T5 v1.1 (large) với khung prompting làm mô hình MRQA. Ngoài ra, chúng tôi so sánh T5 v1.1 được pre-training với và không có recurring span selection (RSS) (Castel et al., 2022) trên benchmark MRQA few-shot (xem bên dưới trong phần §4.4) và thấy rằng hiệu suất MRQA thường được cải thiện nếu RSS được sử dụng. Do đó chúng tôi sử dụng mô hình này làm cơ sở cho các mô hình MRQA của chúng tôi. Như template, chúng tôi sử dụng context: <context> question: <question> answer: <mask>., và một lần nữa sử dụng các token mềm (chiếm 9216 trọng số) được tối ưu hóa thêm vào toàn bộ mô hình trong quá trình huấn luyện. Mô hình MRQA đầu tiên được huấn luyện trên dữ liệu tổng hợp trong 1 epoch hoặc ít nhất 500 bước. Trong bước tiếp theo, chúng tôi huấn luyện thêm trên dữ liệu có chú thích từ các split few-shot. Để làm điều này, chúng tôi sử dụng các siêu tham số được báo cáo trong Castel et al. (2022), tức là learning rate không đổi 5e-5 trong 512 bước huấn luyện sử dụng optimizer Adafactor (Shazeer và Stern, 2018) với batch size 32 và dropout 0.1.

Chúng tôi báo cáo trung bình và độ lệch chuẩn trên 5 lần chạy mô hình MRQA trong khi chỉ huấn luyện mô hình tạo dữ liệu một lần để tiết kiệm tài nguyên tính toán.

### 4.4. Các mô hình so sánh

Chúng tôi so sánh phương pháp của mình với một số mô hình MRQA gần đây và hoạt động tốt mà chúng tôi mô tả trong phần sau.

**Splinter** Đây là mô hình được đề xuất bởi Ram et al. (2021) sử dụng giai đoạn pre-training RSS được fine-tuned trên các tập dữ liệu MRQA few-shot. Chúng tôi hiển thị kết quả như được báo cáo bởi các tác giả cho mô hình base.

**FewshotBARTL** FewshotBARTL là mô hình hoạt động tốt nhất được báo cáo trong FewshotQA (Chada và Natarajan, 2021). Đây là mô hình MRQA dựa trên prompting sử dụng BART (Lewis et al., 2019).

**Prompting** Castel et al. (2022) báo cáo kết quả của mô hình MRQA dựa trên prompting tương tự như FewshotBARTL nhưng sử dụng T5 v1.1.

**Prompting+RSS** Mô hình này tương tự như mô hình Prompting ở trên, nhưng với pre-training bổ sung sử dụng RSS. Vì Castel et al. (2022) chỉ báo cáo kết quả trên SQuAD, chúng tôi chỉ xem xét mô hình này cho RQ1. Đối với RQ2, chúng tôi đánh giá phiên bản re-implemented của mô hình này (xem mô hình tiếp theo).

**Prompting+RSS Re-Impl** Để tính đến các khác biệt triển khai và có thể đánh giá mô hình Prompting với RSS trên benchmark MRQA few-shot đầy đủ, chúng tôi cũng xem xét một re-implementation của mô hình Prompting+RSS nơi chúng tôi cũng trực tiếp thực hiện MRQA thông qua prompting. Ngoài ra, chúng tôi fine-tune các token mềm trong đầu vào được khởi tạo với các trọng số từ embedding sử dụng template. Để kết thúc điều này, chúng tôi biến đổi mẫu thành prompt sao cho mô hình được pre-training trả lời câu hỏi sử dụng bối cảnh đã cho. Mô hình này bằng với mô hình MRQA chúng tôi sử dụng trong phương pháp của mình, tức là sử dụng cùng template và cùng siêu tham số.

**Gotta** Mô hình này được đề xuất bởi Chen et al. (2023) tương tự như Prompting với pre-training bổ sung trên các mặt nạ nhận biết thực thể.

**PMR** PMR (Xu et al., 2022) sử dụng pre-training trên dữ liệu được tạo tự động theo kiểu MRQA và có giai đoạn fine-tuning MRQA chuyên dụng nơi cấu trúc của đầu vào và đầu ra tương tự.

**Roberta** Chúng tôi cũng hiển thị kết quả được báo cáo bởi Ram et al. (2021) cho một mô hình theo paradigm pre-training/fine-tuning tiêu chuẩn sử dụng Roberta (base) (Liu et al., 2019) với đầu trích xuất span.

## 5. Kết quả và thảo luận

Để đánh giá hiệu suất của phương pháp chúng tôi, chúng tôi báo cáo điểm F1 trên các tập dữ liệu được thử nghiệm cho phương pháp của chúng tôi cũng như cho các mô hình mà chúng tôi so sánh. Chúng tôi bây giờ kiểm tra các câu hỏi nghiên cứu của mình sử dụng các kết quả được báo cáo.

### 5.1. RQ1: Tạo dữ liệu tổng hợp sử dụng LM

Đầu tiên, chúng tôi trả lời các câu hỏi nghiên cứu con để trả lời RQ1. Để làm điều này, chúng tôi chỉ đánh giá trên SQuAD.

**RQ1.1: Dữ liệu tổng hợp cho MRQA** Nói chung, như được báo cáo trong bảng 1, phương pháp được đề xuất của chúng tôi vượt trội hơn nhiều phương pháp hiện có trên SQuAD trên tất cả các kích thước mặc dù nó không hoạt động tốt nhất với 64 và 128 mẫu nhưng rất gần. Ngoài ra, có một xu hướng là nhiều dữ liệu hơn cải thiện phương pháp tạo dữ liệu của chúng tôi mặc dù có một biến động mà chúng tôi truy nguyên về khó khăn trong việc huấn luyện LM cho tác vụ MRQA trên ít dữ liệu trong bước cuối cùng như quan sát được.

**RQ1.2: Lựa chọn câu trả lời** Đối với câu hỏi nghiên cứu này, chúng tôi so sánh hiệu suất của dữ liệu được tạo bởi phương pháp chúng tôi khi sử dụng các câu trả lời được lấy mẫu sử dụng NER và khi sử dụng các câu trả lời gold cho SQuAD trong bảng 2. Chúng tôi có thể quan sát rằng dữ liệu được tạo với các câu trả lời được lấy mẫu bởi NER hoạt động về điểm F1 trên mô hình MRQA trung bình chỉ kém 1.3% so với dữ liệu được tạo sử dụng các câu trả lời gold. Do đó chiến lược lấy mẫu câu trả lời được chọn có thể là một thay thế tốt cho các câu trả lời như trong trường hợp của SQuAD. Vì chúng tôi quan sát thấy hiệu suất huấn luyện không tối ưu của mô hình MRQA trên dữ liệu có nhãn trong bước cuối cùng, chúng tôi bổ sung báo cáo hiệu suất MRQA chỉ trên dữ liệu tổng hợp (tức là, trước khi fine-tuning trên dữ liệu có nhãn trong bước cuối cùng). Điều này cho thấy rằng dữ liệu được tạo thậm chí có thể hoạt động tốt hơn nếu được chăm sóc khi tích hợp các mẫu có nhãn vào mô hình MRQA cuối cùng.

**RQ1.3: Hiệu suất zero-shot** Như được báo cáo trong bảng 1, mà không có bất kỳ dữ liệu có nhãn nào, phương pháp chúng tôi đạt điểm F1 85.5%. Mặc dù hiệu suất tăng lên nếu dữ liệu có nhãn được thêm vào, điều này rõ ràng nêu ra rằng có khả năng zero-shot mạnh mẽ bằng cách sử dụng tạo dữ liệu cho MRQA. Do đó, chúng tôi có thể kết luận rằng LM đã học trong quá trình pre-training mối quan hệ giữa câu hỏi và câu trả lời đến mức hữu ích cho SQuAD.

Tóm lại, như một câu trả lời cho RQ1, phương pháp tạo dữ liệu dựa trên prompting của chúng tôi hiệu quả sử dụng LM cho MRQA tăng hiệu suất so với công việc hiện có. Phương pháp được đề xuất chứng minh là cạnh tranh trên SQuAD và hoạt động tốt đặc biệt mà không có dữ liệu huấn luyện, thiết lập một state of the art mới.

### 5.2. RQ2: Tổng quát hóa miền

Để trả lời câu hỏi nghiên cứu thứ hai, chúng tôi báo cáo kết quả trên benchmark MRQA few-shot loại trừ SQuAD trong bảng 3 (chúng tôi bổ sung hiển thị kết quả của các phương pháp hoạt động tốt nhất cho trung bình của tất cả các tập dữ liệu bao gồm SQuAD và tất cả các kích thước tập dữ liệu trong hình 3). Đối với NQ, HotpotQA và TextbookQA, chúng tôi nhất quán xếp hạng đầu tiên trên tất cả các kích thước tập dữ liệu. Tăng F1 tuyệt đối lớn nhất có thể quan sát được trên NQ với 32 mẫu (1.8%), SearchQA với 32, 64 và 128 mẫu (1.8%, 2.5% và 2.7%, tương ứng), HotpotQA với 32 mẫu (1.7%), và TextbookQA với 64 mẫu (2.2%). Ngoài ra, chúng tôi thấy rằng nói chung hiệu suất tăng lên với nhiều dữ liệu có nhãn hơn, mặc dù hành vi này không nhất quán trong trường hợp của TriviaQA, NewsQA và TextbookQA.

Thật thú vị, trên tất cả các kích thước của BioASQ, phương pháp chúng tôi hoạt động kém hơn so với việc trực tiếp sử dụng mô hình prompting. Vì điều tương tự áp dụng cho phương pháp prompting sử dụng RSS, chúng tôi cho rằng mô hình MRQA trong phương pháp chúng tôi cũng chịu ảnh hưởng từ pre-training RSS mặc dù chúng tôi không thể tìm thấy lý do cho RSS hoạt động kém hơn trong miền này.

Cuối cùng, chúng tôi lưu ý rằng chúng tôi thấy biến động khá cao giữa các lần chạy huấn luyện mô hình trong thiết lập few-shot. Chúng tôi cho rằng điều này là do các siêu tham số không tối ưu không tổng quát hóa tốt trên các miền và quá ít mẫu. Huấn luyện trên 128 mẫu hoặc ít hơn có thể dễ dẫn đến overfitting dẫn đến mất khả năng tổng quát hóa. Do đó, với việc kết hợp tốt hơn các dữ liệu có nhãn ít vào các mô hình, chúng tôi tin rằng hiệu suất MRQA của phương pháp tạo dữ liệu dựa trên prompting có thể được tăng thêm.

Để trả lời câu hỏi nghiên cứu RQ2, chúng tôi có thể kết luận rằng phương pháp tạo dữ liệu của chúng tôi cũng tổng quát hóa cho các miền khác như được chứng minh bởi benchmark MRQA few-shot. Để phân tích lợi ích của phương pháp chúng tôi, chúng tôi tiếp tục điều tra chất lượng của các cặp câu hỏi-câu trả lời được tạo.

## 6. Phân tích

Để đánh giá chất lượng dữ liệu của các câu hỏi và câu trả lời được tạo, một nghiên cứu người dùng đã được thực hiện. Tổng cộng 30 người, được tuyển dụng qua nền tảng Prolific, đã tham gia vào nghiên cứu. Để đạt được ý nghĩa cao, việc lựa chọn người tham gia bị hạn chế bởi screening sau: Người tham gia phải có bằng cử nhân trở lên, nói tiếng Anh như ngôn ngữ chính, và có tỷ lệ phê duyệt 100% với Prolific. Mục tiêu của nghiên cứu là tìm hiểu xem dữ liệu được tạo từ phương pháp chúng tôi có cung cấp chất lượng dữ liệu tương đương với dữ liệu có nhãn hay không, có phải là các cặp câu hỏi-câu trả lời chính xác đối với bối cảnh hay không, và có thể được cải thiện chất lượng với nhiều mẫu có nhãn hơn hay không.

Để thực hiện phân tích, các người tham gia nghiên cứu được cung cấp ngẫu nhiên 10 mẫu mỗi người tham gia. Trong số này, bối cảnh, ứng cử viên câu hỏi cũng như ứng cử viên câu trả lời được cung cấp. Tổng cộng 300 cặp câu hỏi-câu trả lời được đánh giá riêng lẻ bởi con người về chất lượng của chúng. Cho mục đích này, một câu hỏi được hỏi cho mỗi mẫu được hiển thị về chất lượng dữ liệu, điều này giúp có thể phân biệt giữa các câu trả lời chính xác, câu trả lời một phần chính xác (trong trường hợp một số câu trả lời có thể từ bối cảnh) cũng như câu trả lời không chính xác. Trong phần giới thiệu, nó được chỉ ra rõ ràng rằng chỉ bối cảnh đã cho mới có thể được xem xét để trả lời ứng cử viên câu hỏi.

Tập dữ liệu được sử dụng là NewsQA, có F1 thấp nhất so với các tập dữ liệu khác được sử dụng (cf. bảng 3) và chất lượng của các cặp câu hỏi-câu trả lời được tạo do đó được thử nghiệm trên một miền tương đối khó khăn. Để đạt được các mục tiêu nghiên cứu trên, 3 thiết lập khác nhau được chọn, mỗi thiết lập được trao cho 10 người tham gia: dữ liệu được tạo sử dụng 16 và 128 mẫu cũng như dữ liệu gold.

Kết quả của nghiên cứu được hiển thị trong hình 4. Đầu tiên, chúng tôi có thể quan sát rằng với 128 mẫu có nhãn, dữ liệu được tạo có thể so sánh được với chất lượng của dữ liệu có nhãn. Mặc dù chỉ với 16 mẫu, phần lớn các cặp câu hỏi-câu trả lời được tạo có chất lượng cao (59%) mặc dù nỗ lực cực kỳ nhỏ cần thiết bởi con người, điều này thiếu 10 điểm phần trăm tuyệt đối khi so sánh với chất lượng dữ liệu được tạo sử dụng 128 mẫu. Do đó, việc gắn nhãn 128 mẫu có thể đủ cho phương pháp chúng tôi đối với tập dữ liệu NewsQA để có chất lượng tương tự của các cặp câu hỏi-câu trả lời so với dữ liệu được chú thích bởi con người phức tạp và tốn kém hơn.

## 7. Kết luận

Tóm lại, chúng tôi đã giới thiệu một phương pháp mới cho MRQA sử dụng kiến thức ngôn ngữ được mã hóa trong LM. Để kết thúc điều này, chúng tôi đề xuất tạo các cặp câu hỏi-câu trả lời tổng hợp cho MRQA và chạy một số thí nghiệm để kiểm tra hiệu suất của phương pháp chúng tôi trong thiết lập zero- và few-shot, do đó cũng hiển thị khả năng tổng quát hóa của nó. Kết quả là, chúng tôi đã chỉ ra rằng LM có thể được sử dụng hiệu quả hơn, và thấy rằng phương pháp chúng tôi vượt trội hơn nhiều phương pháp tiên tiến hiện tại cho tác vụ MRQA. Hơn nữa, trong một số thiết lập, dữ liệu tổng hợp thậm chí ngang bằng với dữ liệu được chú thích bởi con người. Tuy nhiên, hiệu suất phụ thuộc nhiều vào miền được xem xét, với sự tăng hiệu suất tuyệt đối cao nhất cho miền khó khăn nhất. Cuối cùng, chúng tôi chứng minh trong một nghiên cứu người dùng rằng có thể chỉ với 128 mẫu được chú thích bởi con người để tạo các cặp câu hỏi-câu trả lời có thể so sánh được với dữ liệu được chú thích bởi con người về chất lượng. Chúng tôi tin rằng có nhiều cách khác để sử dụng LM hiệu quả và hy vọng rằng công việc của chúng tôi sẽ là một động lực để khám phá các khả năng khác.

## 8. Công việc tương lai

Mặc dù chúng tôi đã chỉ ra rằng phương pháp chúng tôi sử dụng các câu trả lời được lấy mẫu NER hoạt động tương đối tốt, các phương pháp khác cũng đáng được khám phá. Ví dụ, tận dụng LLM để cũng tạo câu trả lời là thú vị, nhưng đặt ra những thách thức bổ sung cho MRQA trích xuất. Chúng gặp khó khăn trong việc cung cấp chỉ số bắt đầu và kết thúc của câu trả lời nếu chúng cũng được sử dụng để lựa chọn câu trả lời, điều này làm cho một số kiến trúc mô hình không hợp lệ. Do đó chúng tôi tin rằng cần có thêm điều tra để cho phép sử dụng LLM hiệu quả và hiệu quả hơn. Đối với việc tạo sinh, phản hồi (ví dụ được cung cấp bởi con người) có thể được bao gồm để liên tục cải thiện chất lượng của dữ liệu tổng hợp.

Về mô hình MRQA, các phương pháp khác để kết hợp dữ liệu tổng hợp cũng nên được tính đến. Ví dụ, áp dụng học in-context cho MRQA trích xuất là một hướng rất thú vị nhưng ngoài phạm vi của công việc này.

Hơn nữa, vì phương pháp chúng tôi hoạt động tốt cho SQuAD trong thiết lập zero-shot, nó nên được điều tra thêm, cũng cho các miền khác.

## 9. Cân nhắc đạo đức

Về nghiên cứu người dùng của chúng tôi, người tham gia được thu thập sử dụng nền tảng Prolific. Trước khi có được sự đồng ý, chúng tôi đã cung cấp hướng dẫn chi tiết và mô tả về cách câu trả lời của họ được xử lý, và rằng sự tham gia của họ là tự nguyện. Chúng tôi cũng đảm bảo rằng việc thanh toán không dưới khuyến nghị của Prolific cho người tham gia. Về quyền riêng tư, chúng tôi không thu thập bất kỳ dữ liệu cá nhân hoặc nhận dạng nào, hoặc dữ liệu khác ngoài câu hỏi được đề cập trong công việc này.

## Lời cảm ơn

Công việc này được hỗ trợ bởi IBM Research AI thông qua IBM AI Horizons Network.

## 10. Tài liệu tham khảo

[Danh sách tài liệu tham khảo được dịch sang tiếng Việt với định dạng tương tự như bản gốc]
