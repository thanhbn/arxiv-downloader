# 2407.03181.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/cot/2407.03181.pdf
# Kích thước file: 764470 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Tinh chỉnh trên các Chuỗi Lý luận Đa dạng Thúc đẩy
việc Cải tiến CoT trong quá trình Suy luận của LLM
Haritz Puerto1, Tilek Chubakov1
Xiaodan Zhu2,Harish Tayyar Madabushi3,Iryna Gurevych1
1Phòng thí nghiệm Xử lý Tri thức Phổ quát (UKP Lab), TU Darmstadt và
Trung tâm Nghiên cứu Quốc gia ATHENE về An ninh mạng Ứng dụng, Đức
2Khoa ECE & Viện Nghiên cứu Ingenuity Labs, Đại học Queen's, Canada
3Đại học Bath, Anh
https://www.ukp.tu-darmstadt.de

Tóm tắt
Yêu cầu một mô hình ngôn ngữ lớn (LLM) tạo ra các bước lý luận trung gian, được gọi là Chuỗi Tư duy (CoT), đã được chứng minh là một cách hiệu quả để tăng cường hiệu suất. Các phương pháp trước đây đã tập trung vào việc tạo ra nhiều CoT độc lập, kết hợp chúng thông qua ensemble hoặc các chiến lược hậu xử lý khác để nâng cao khả năng lý luận. Trong công trình này, chúng tôi giới thiệu một phương pháp mới mà LLM được tinh chỉnh để tạo ra một chuỗi các Chuỗi Tư duy Đa dạng (DCoT) trong một bước suy luận duy nhất, điều này khác biệt cơ bản so với công trình trước đây chủ yếu hoạt động trên các thế hệ CoT song song. DCoT cho phép LLM có khả năng thực hiện cải tiến chuỗi lý luận trong quá trình suy luận mà không cần phản hồi từ bên ngoài. Thông qua một loạt thí nghiệm nghiêm ngặt trên nhiều tác vụ đòi hỏi các loại lý luận khác nhau, chúng tôi cho thấy rằng việc tinh chỉnh trên DCoT cải thiện hiệu suất so với đường cơ sở CoT trên các họ mô hình và quy mô (1.3B đến 70B). Những cải thiện này đặc biệt có tác động đối với các tác vụ có không gian trạng thái kết quả lớn, chẳng hạn như những tác vụ liên quan đến câu trả lời số. Công trình của chúng tôi cũng có ý nghĩa quan trọng vì cả phân tích định lượng và đánh giá thủ công đều cho thấy những cải thiện quan sát được bắt nguồn từ khả năng của mô hình trong việc cải tiến một chuỗi lý luận ban đầu bằng cách tạo ra một chuỗi thứ hai, cải thiện hơn trong cùng một bước suy luận, thể hiện sự tự cải thiện mà trước đây khó nắm bắt. Mã và dữ liệu của chúng tôi được công khai1.

1 Giới thiệu
Chuỗi Tư duy (CoT; Wei et al. 2022b), phương pháp prompting để tạo ra các bước lý luận trung gian để trả lời câu hỏi, được công nhận là một cơ chế đơn giản nhưng hiệu quả để cải thiện hiệu suất của các mô hình ngôn ngữ lớn (LLM). Cho rằng việc yêu cầu mô hình tạo ra các bước trung gian cải thiện hiệu suất, điều hợp lý là yêu cầu mô hình tạo ra nhiều chuỗi có thể cải thiện hiệu suất hơn nữa. Công trình trước đây khám phá ý tưởng này bao gồm công trình của Wang et al. (2023), trong đó họ tạo ra nhiều CoT và ensemble chúng bằng cơ chế bầu chọn. Tuy nhiên, các phương pháp này và những phương pháp tương tự khác (xem Mục 2), dựa vào các CoT được tạo ra độc lập, điều này ngăn cản mô hình truy cập vào các chuỗi đã tạo ra trước đó trong quá trình suy luận. Sự độc lập này hạn chế tiềm năng cải tiến trong quá trình suy luận và khả năng xây dựng dựa trên các đường lý luận trước đó.

Do đó, chúng tôi trình bày một phương pháp huấn luyện cho phép LLM tạo ra nhiều chuỗi lý luận đa dạng một cách tuần tự trong một bước suy luận duy nhất. Thông qua các thí nghiệm của chúng tôi, chúng tôi nhận thấy rằng phương pháp này thúc đẩy cải thiện hiệu suất bằng cách cho phép mô hình cải tiến các CoT tiếp theo dựa trên những chuỗi trước đó. Để đạt được điều này, chúng tôi xây dựng một tập dữ liệu huấn luyện của các CoT Đa dạng (DCoT), trong đó một câu hỏi duy nhất được liên kết với nhiều CoT hợp lệ. Trong khi các nghiên cứu trước đây coi mỗi cặp (câu hỏi, CoT) là một điểm dữ liệu độc lập (Ho et al., 2023; Huang et al., 2023), chúng tôi đề xuất nối các CoT thành một chuỗi duy nhất, tạo thành các cặp huấn luyện theo định dạng (câu hỏi, [CoTs]). Bằng cách này, các mô hình học cách tạo ra nhiều CoT trong một bước suy luận duy nhất.

1https://github.com/UKPLab/acl2025-diverse-cot

[Figure 1: Diverse CoT (k=2) tạo ra k CoT trong một bước suy luận duy nhất và chọn câu trả lời đúng.]

--- TRANG 2 ---
Trong khi tất cả các CoT huấn luyện của chúng tôi đại diện cho các chuỗi lý luận đúng, chúng tôi giả thuyết rằng chế độ huấn luyện này sẽ cho phép LLM tạo ra các CoT tốt hơn một cách tuần tự—cho đến một số lần lặp nhất định—vì chúng có quyền truy cập vào các chuỗi lý luận trước đó, dẫn đến cải thiện hiệu suất.

Chúng tôi chứng minh rằng việc tinh chỉnh sử dụng DCoT cải thiện hiệu suất LLM so với đường cơ sở CoT bằng cách kiểm tra nghiêm ngặt trên một loạt tác vụ yêu cầu các loại lý luận khác nhau trên các họ mô hình và quy mô (1.3B đến 70B). Chúng tôi còn xác định tập con các tác vụ, cụ thể là những tác vụ có không gian trạng thái kết quả lớn, chẳng hạn như những tác vụ liên quan đến câu trả lời số, đặc biệt hưởng lợi từ phương pháp của chúng tôi. Ngoài ra, chúng tôi cho thấy rằng việc tạo ra một CoT duy nhất trên mô hình được tinh chỉnh DCoT cho kết quả tương đương với đường cơ sở CoT, trong khi tạo ra hai hoặc nhiều CoT hơn mang lại cải thiện rõ ràng trung bình trên tất cả các tác vụ cho tất cả các mô hình. Điều này, cùng với đánh giá thủ công mà chúng tôi trình bày dưới đây, chứng minh rằng những cải thiện do DCoT mang lại không phát sinh từ nhiễu loạn ngẫu nhiên mà từ cải tiến lặp lại trong một bước suy luận duy nhất. Ngoài ra, chúng tôi cho thấy rằng các mô hình được tinh chỉnh DCoT có thể được tăng cường thêm bằng các phương pháp tương tự như những phương pháp tăng cường CoT, chẳng hạn như tự-ensemble của CoT (Wei et al., 2022b). Những kết quả này cho thấy rằng các tập dữ liệu instruction-tuning có thể dễ dàng được tăng cường với dữ liệu DCoT, cho rằng nhiều tập dữ liệu hiện có đã bao gồm các ví dụ CoT, thường có nhiều CoT cho mỗi câu hỏi (Ho et al., 2023; Huang et al., 2023). Điều này làm cho việc tạo ra các tập dữ liệu huấn luyện DCoT vừa thực tế vừa hiệu quả. Cụ thể, những đóng góp của công trình này như sau:

• Chúng tôi giới thiệu một phương pháp mới tinh chỉnh LLM để tạo ra nhiều chuỗi lý luận trong một bước suy luận duy nhất, nâng cao các chuỗi tiếp theo và tăng cường hiệu suất.

• Chúng tôi chứng minh nghiêm ngặt tính hiệu quả của phương pháp trên một loạt họ LLM và kích thước trên nhiều tác vụ lý luận, xác định các loại tác vụ mà nó hoạt động tốt nhất—những tác vụ có không gian trạng thái kết quả lớn.

• Thông qua kết hợp phân tích thực nghiệm và thủ công, chúng tôi cho thấy rằng DCoT đạt được cải thiện thông qua việc sửa đổi trong quá trình suy luận CoT đầu tiên của nó mà không cần phản hồi từ bên ngoài hoặc tối ưu hóa prompt, điều này, theo hiểu biết tốt nhất của chúng tôi, là công trình đầu tiên làm được điều này.

2 Các Công trình Liên quan
Trong mục này, chúng tôi xem xét các công trình liên quan từ ba góc độ riêng biệt: (i) các phương pháp prompting nâng cao CoT thông qua tính đa dạng, (ii) nghiên cứu tập trung vào instruction tuning các mô hình sử dụng CoT, và (iii) các nghiên cứu về tự sửa lỗi trong LLM.

Cải thiện Prompting thông qua tính đa dạng. Nhiều công trình đã chỉ ra lợi ích của việc tạo ra các CoT đa dạng và tổng hợp chúng (Wang et al., 2023; Zhang et al., 2024; Yoran et al., 2023; Li et al., 2023b; Weng et al., 2023; Zhao et al., 2023a,b). Đặc biệt, Wang et al. (2023) đề xuất việc tạo ra tự-ensemble của CoT để cải thiện hiệu suất của LLM, mà họ gọi là tự-nhất quán. Họ lấy mẫu một loạt CoT, chọn câu trả lời nhất quán nhất, và cho thấy cải thiện hiệu suất lớn trên các tác vụ lý luận. Yoran et al. (2023) mở rộng công trình này bằng cách tạo ra một meta prompt tổng hợp các đường lý luận thay vì chọn câu trả lời phổ biến nhất. Zhang et al. (2024) đề xuất các bước rõ ràng để đối chiếu mỗi CoT và phản ánh về câu trả lời cuối cùng. Tuy nhiên, không có công trình nào trong số này thúc đẩy LLM tạo ra nhiều CoT trong cùng một bước suy luận.

Tinh chỉnh trên các CoT Đa dạng. Thành công của CoT prompting dẫn đến việc tạo ra các tập dữ liệu instruction-tuning với CoT (Chung et al., 2024). Kim et al. (2023) lập luận rằng các LM nhỏ hoạt động kém trên CoT trên các tác vụ chưa thấy so với các LM lớn. Do đó, họ tạo ra một tập dữ liệu instruction-tuning của CoT để trang bị cho các LM nhỏ khả năng CoT. Những người khác đề xuất chưng cất CoT từ các mô hình ngôn ngữ rất lớn (vLLM) (Hsieh et al., 2023; Li et al., 2023a). Ho et al. (2023) cũng cho thấy lợi ích của việc chưng cất CoT từ các vLLM này và tuyên bố rằng việc lấy mẫu nhiều CoT cho mỗi câu hỏi và huấn luyện trên các CoT đa dạng này là một kỹ thuật tăng cường dữ liệu hiệu quả cải thiện hiệu suất của các mô hình được chưng cất. Tuy nhiên, họ không sử dụng tính đa dạng này tại thời điểm suy luận, và không giống như chúng tôi, phương pháp của họ chỉ tạo ra một CoT cho mỗi câu hỏi. Ranaldi và Freitas (2024) đề xuất một bước thứ hai trong đó các mô hình học sinh được chưng cất tạo ra nhiều CoT và với học tăng cường, học sinh tự huấn luyện. Huang et al. (2023) cho thấy rằng vLLM có thể cải thiện hiệu suất trên các tác vụ lý luận bằng cách tự huấn luyện trên các thế hệ CoT của chính chúng từ việc lấy mẫu.

Tự Sửa lỗi. Huang et al. (2023) định nghĩa nó là khả năng của LLM sửa chữa phản hồi ban đầu của nó mà không dựa vào phản hồi từ bên ngoài. Hầu hết các công trình tiếp cận tự sửa lỗi trong LLM với một hệ thống có hai bước: một bước tạo ra câu trả lời và một bước khác xác định lỗi (Shinn et al., 2024; Madaan et al., 2023; Pan et al., 2024; Kim et al., 2024; Weng et al., 2023; Jiang et al., 2024; Du et al., 2024; Paul et al., 2024; Saunders et al., 2022; Akyurek et al., 2023; Welleck et al., 2023; Estornell et al., 2025). Tuy nhiên, Hong et al. (2024) tuyên bố rằng LLM không thể xác định lỗi của chính chúng và Huang et al. (2024); Stechly et al. (2025); Tyen et al. (2024) lập luận rằng các cải thiện tự sửa lỗi bắt nguồn từ việc sử dụng phản hồi từ bên ngoài. Phương pháp của chúng tôi khác với các phương pháp này ở chỗ chúng tôi tạo ra nhiều CoT trong một bước suy luận duy nhất. Như chúng tôi sẽ chứng minh, quyền truy cập vào các CoT trước đó này cho phép mô hình cải tiến các chuỗi lý luận tiếp theo mà không cần xác định lỗi một cách rõ ràng.

3 Phương pháp
DCoT. Chúng tôi instruction-tune LLM trên các CoT đa dạng để tạo ra nhiều CoT trong một bước suy luận duy nhất. Để làm điều này, chúng tôi tạo ra một mẫu instruction DCoT, trong đó chúng tôi giới thiệu một tập các lệnh (trong dấu ngoặc) để yêu cầu số lượng CoT cần tạo ra:

Prompt: [Question] Câu hỏi [Options] Tùy chọn [Number of answers] k

Response: [Answer 1] CoT 1 [Answer 2] ... [Answer k] CoT k [Final answer] câu trả lời

--- TRANG 3 ---
Tạo Dữ liệu
Câu hỏi CoT Triggers CoT1 CoT2 … CoTk
Huấn luyện DCoT
Huấn luyện CoT
(q1, CoT11)
(q1, CoT12)
…
(q2, CoT21)
…
LLM
(q1, CoT11)
(q1, CoT11, CoT12)
…
(q2, CoT21, CoT22, CoT23)
…
LLM
Suy luận DCoT
Câu hỏi, k=2
LLM
CoT1
CoT2
Câu trả lời
Bước suy luận duy nhất

Hình 2: Chúng tôi huấn luyện trên một loạt CoT để làm cho mô hình học cách tạo ra nhiều CoT trong một bước suy luận. DCoT và CoT có cùng số lượng CoT. Tuy nhiên, DCoT được huấn luyện với số lượng k CoT khác nhau cho một truy vấn nhất định. Tại thời điểm suy luận, người dùng có thể chọn bất kỳ k nào.

Trong prompt đầu vào, instruction [Options] cung cấp các câu trả lời ứng viên cho các tác vụ trả lời câu hỏi trắc nghiệm. Đối với các tác vụ khác, chẳng hạn như trích xuất span, điều này được bỏ qua. Trong phản hồi, instruction [Final answer] là cơ chế hội tụ điều kiện mô hình tạo ra câu trả lời cuối cùng. Chúng tôi tạo ra dữ liệu DCoT theo định dạng yêu cầu bằng các phương pháp được mô tả trong Mục 3.1. Để ngắn gọn, chúng tôi gọi các mô hình được instruction-tuned trên dữ liệu DCoT là DCoT.

CoT (Đường cơ sở). Để thiết lập một đường cơ sở có thể so sánh, chúng tôi instruction-tune các LLM tương tự sử dụng định dạng CoT truyền thống hơn. Để đảm bảo so sánh công bằng, chúng tôi sử dụng các chuỗi lý luận tương tự như trong DCoT. Như được hiển thị trong Hình 2, mỗi điểm dữ liệu được tạo thành từ một câu hỏi và một CoT, và một câu hỏi có thể xuất hiện trong nhiều hơn một điểm dữ liệu nhưng với một CoT khác. Bằng cách này, mô hình tận dụng tính đa dạng CoT tại thời điểm huấn luyện, nhưng, không giống như trong DCoT, nó không làm như vậy tại thời điểm suy luận. Để ngắn gọn, chúng tôi gọi các mô hình này là CoT.

Với hai phương pháp này, chúng tôi nhằm mục đích so sánh hai chế độ huấn luyện sử dụng cùng một lượng CoT huấn luyện và nơi sự khác biệt duy nhất nằm ở định dạng phản hồi. Chúng tôi cũng thực hiện phân tích khám phá về việc liệu chúng tôi có thể tái tạo kết quả của huấn luyện DCoT với học trong ngữ cảnh trong các mô hình ngôn ngữ thương mại rất lớn trong Phụ lục C.

3.1 Tạo Dữ liệu Huấn luyện
Chúng tôi tuân theo các phương pháp do Ott et al. (2023) đề ra để tạo ra CoT cho các tập dữ liệu CoT và DCoT của chúng tôi. Chúng tôi sử dụng GPT 3.5 turbo trong thiết lập zero-shot với nhiều trigger để tạo ra CoT. Cụ thể, CoT Triggers là các hậu tố prompt, chẳng hạn như "Let's think step by step" mà 'kích hoạt' LLM tạo ra CoT. Chúng tôi sử dụng các trigger tương tự như trong (Ott et al., 2023). Đối với mỗi câu hỏi, chúng tôi chọn bốn trigger CoT ngẫu nhiên. Chúng tôi giới hạn số lượng CoT ở mức bốn để đảm bảo rằng các mục tiêu phù hợp với cửa sổ ngữ cảnh của LLM. Chúng tôi hạn chế dữ liệu huấn luyện đối với những chuỗi lý luận dẫn đến câu trả lời đúng như được xác định bởi các nhãn do tập dữ liệu tương ứng cung cấp2. Chúng tôi báo cáo các mẫu prompt và trigger trong Phụ lục J.

Bảng 8 trong Phụ lục A liệt kê các tập dữ liệu chúng tôi sử dụng để tạo ra CoT và huấn luyện các mô hình. Các tập dữ liệu này được chọn theo các công trình trước đây (Wang et al., 2023; Yoran et al., 2023). Chúng tôi đã thêm BoardgameQA (Kazemi et al., 2023) để bao gồm logic và ConditionalQA (Sun et al., 2022) để bao gồm lý luận điều kiện tự nhiên, điều này rất phức tạp và do đó việc sửa đổi câu trả lời có thể có lợi. Với sự lựa chọn này, chúng tôi bao gồm nhiều lĩnh vực, không gian đầu ra và khả năng lý luận. Theo các công trình trước đây (Khashabi et al., 2020; Longpre et al., 2023; Wei et al., 2022a; Tafjord và Clark, 2021), chúng tôi huấn luyện tất cả các mô hình trên tất cả các tập dữ liệu cùng lúc để hướng tới khả năng tổng quát. Chúng tôi cung cấp thêm chi tiết trong Phụ lục A.

3.2 Các Mô hình
Chúng tôi huấn luyện một loạt mô hình bao gồm các quy luật tỷ lệ và các họ khác nhau. Cụ thể, chúng tôi sử dụng Phi 1.5 (1.3B; Li et al. 2023c), Phi 2 (2.7B; Abdin et al. 2023), LLaMA-2 7B, LLaMA-2 13B (Touvron et al., 2023). Với sự lựa chọn này, chúng tôi có thể phân tích hai họ và các quy luật tỷ lệ trong các họ. Đối với tất cả các thí nghiệm của chúng tôi, chúng tôi chọn các mô hình không dựa trên instruction tuned để đảm bảo rằng việc so sánh giữa DCoT và CoT là công bằng. Điều này là do các tập dữ liệu instruction-tuning chứa dữ liệu CoT (Touvron et al., 2023), điều này sẽ làm cho đường cơ sở CoT được huấn luyện trên các CoT dài hơn và đa dạng hơn, và do đó, việc so sánh giữa hai chế độ huấn luyện có thể không công bằng. Chúng tôi cũng tiến hành một thí nghiệm nhỏ hơn trên LLaMA-2 13B Chat để phân tích xem phương pháp instruction-tuning DCoT của chúng tôi có thể được áp dụng cho các mô hình đã được instruction-tuned hay không. Cuối cùng, chúng tôi cũng chạy các thí nghiệm chính trên LLaMA-2 70B. Tuy nhiên, do chi phí suy luận, chúng tôi huấn luyện nó trên ít dữ liệu hơn các mô hình khác và chỉ đánh giá nó trên các tập con của tập đánh giá để hiển thị gợi ý về tính hiệu quả trên các LM rất lớn. Chúng tôi tham khảo Phụ lục B để biết chi tiết về thiết lập huấn luyện của các mô hình.

2Các thí nghiệm ban đầu bao gồm CoT sai dẫn đến kết quả tệ hơn, có thể vì trong kịch bản tinh chỉnh có giám sát, chúng làm cho mô hình bối rối, vì vậy chúng tôi chỉ giữ lại các CoT khác nhau dẫn đến câu trả lời đúng.

3.3 Đánh giá
Phương pháp của chúng tôi có khả năng hiệu quả nhất trong các kịch bản mà quyền truy cập vào các CoT trước đó và câu trả lời tương ứng hữu ích, cụ thể là những tác vụ có không gian đầu ra lớn. Do đó, để đánh giá điều này một cách nghiêm ngặt, chúng tôi kiểm tra phương pháp của chúng tôi trên các tác vụ có kích thước không gian đầu ra khác nhau. Cụ thể, chúng tôi đánh giá các mô hình trên các loại tác vụ sau: Số, Trích xuất Span, Trắc nghiệm, Nhị phân và Tượng trưng. Chúng tôi sử dụng độ đo F1 trung bình macro cho tất cả các tác vụ phân loại trong miền này và squad-metric (Rajpurkar et al., 2016) cho các tác vụ trích xuất span trong miền này (tức là, ConditionalQA và HotpotQA). Để chọn giá trị của siêu tham số k, chúng tôi chạy DCoT với k∈[1,4] và chọn k tốt nhất cho mỗi tập dữ liệu dựa trên tập dev (Bảng 16 trong Phụ lục I báo cáo chúng). Tất cả kết quả được báo cáo trên tập test, ngoại trừ LLaMA-2 70B. Đối với LLaMA-2 70B, chúng tôi chỉ huấn luyện trên một tập con nhỏ của tập huấn luyện và cũng báo cáo kết quả trên các tập con của tập dev mà không có tối ưu hóa siêu tham số nào cả bằng cách sử dụng k là 2 (số lượng tối thiểu của các cải tiến), do chi phí. Thảo luận thêm được cung cấp trong Phụ lục B.

3.3.1 Tác vụ Chưa thấy
Tổng quát hóa cho các tác vụ mới vẫn là một vấn đề thách thức. Ví dụ, Chung et al. (2024) và Kim et al. (2023) cho thấy nhu cầu huấn luyện trên hàng nghìn tác vụ để đạt được cải thiện hiệu suất trên các tác vụ chưa thấy với CoT. Vì chúng tôi ủng hộ việc tăng cường dữ liệu instruction-tuning với DCoT, chúng tôi cần đánh giá rằng DCoT ít nhất không gây ra suy giảm hiệu suất trên các tác vụ chưa thấy (tức là, các tác vụ không được sử dụng để huấn luyện). Do đó, chúng tôi chọn bốn tác vụ chưa thấy thách thức bao gồm lý luận thông thường (CSQA; Talmor et al. 2019), toán trắc nghiệm (AQuA, Ling et al. 2017), tạo số cho toán (SV AMP, Patel et al. 2021), và tạo số cho đếm đối tượng (Suzgun et al. 2023).

--- TRANG 4 ---
Số Trích xuất-Span Trắc nghiệm Nhị phân Tượng trưng
LLM Phương pháp Trung bình GSM8K CQA HQA Trung bình ARC BGQA Quartz Trung bình StrQA LLC†
CoT 47.2 34.95 61.21 32.56 46.88 48.7 32.39 72.69 51.26 54.08 41
Phi 1.5 DCoT (Chúng tôi) 49.39 36.85 62.48 34.81 48.64 50.01 38.6 77.39 55.34 55.97 39
(1.3B) CoT + SC 46.48 40.33 63.39 33.63 48.51 53.81 21.59 75.11 50.17 51.96 32
DCoT + SC 49.01 40.18 65.23 37.79 51.51 53.24 27.6 81.08 53.97 55.97 31
CoT 60.85 56.71 65.13 52.65 58.89 70.87 39.48 82.91 64.42 61.06 58
Phi 2 DCoT 62.6 60.73 68.61 55.15 61.88 73.77 47.07 83.16 68.00 54.34 58
(2.7B) CoT + SC 61.5 64.97 68.14 55.82 61.98 74.36 28.99 85.2 62.85 59.51 55
DCoT + SC 65.12 68.08 70.53 58.61 64.57 76.06 44.16 86.09 68.77 51.43 66
CoT 58.97 28.51 65.73 53.88 59.80 61.63 43.13 79.32 61.36 64.59 75
LLaMA2 DCoT 60.8 29.57 70.99 56.26 63.62 62.7 41.91 81.37 61.99 61.64 82
7B CoT + SC 62.9 33.97 69.92 57.05 63.48 65.98 46.04 83.28 65.10 65.99 81
DCoT + SC 61.09 36.01 71.36 58.35 64.85 68.53 28.2 84.05 60.26 59.22 83
CoT 64.39 42.53 70.25 60.23 65.24 71.79 42.63 84.82 66.41 61.85 81
LLaMA2 DCoT 66.18 44.28 71.56 63.52 67.54 71.41 50.21 83.29 68.30 65.16 80
13B CoT + SC 66.82 50.27 72.72 62.34 67.53 74.82 40.8 85.84 67.15 67.75 80
DCoT + SC 68.12 54.51 72.61 65.92 69.26 74.89 41.27 85.07 67.08 64.65 86
LLaMA2 CoT 66.96 56 73.59 55.94 64.76 81.69 44.34 81.99 69.34 66.15 76
70B* DCoT 68.63 66 69.57 49.78 59.67 89.04 38.3 85.99 71.11 68.34 82
LLaMA2 CoT 64.87 42.76 71.71 60.83 66.27 70.43 44.39 84.04 66.29 66.78 78
13B Chat‡ DCoT 64.62 44.2 71.59 63.87 67.73 72.22 40.94 85.43 66.20 67.68 71

Bảng 1: So sánh DCoT với CoT trên các tập test. Chúng tôi lưu ý những cải thiện lớn hơn trên các tác vụ có không gian đầu ra lớn hơn (số và trích xuất span) phù hợp với giả thuyết của chúng tôi về nơi phương pháp của chúng tôi hiệu quả hơn. *Kết quả 70B trên tập dev. ‡CoT trong các mô hình chat bao gồm dữ liệu CoT không được sử dụng trong DCoT. †Kết quả trên LLC không đáng tin cậy do kích thước tập huấn luyện và test nhỏ.

3.3.2 Kiểm tra Độ mạnh
Cuối cùng, chúng tôi sử dụng Big Bench Hard (Suzgun et al., 2023) cho một thí nghiệm kiểm soát để đánh giá xem phương pháp của chúng tôi có cản trở lý luận trên các tác vụ phức tạp mà CoT chỉ có lợi trong các mô hình lớn hơn đáng kể so với những mô hình chúng tôi kiểm tra; nói cách khác, không sử dụng CoT tốt hơn cho các mô hình nhỏ. Điều này ngụ ý rằng việc tạo ra CoT đúng cho các tác vụ này cực kỳ khó khăn đối với các mô hình nhỏ, và do đó, việc tạo ra nhiều hơn một CoT thậm chí còn khó khăn hơn, vì vậy việc đặt câu hỏi liệu DCoT có thể làm giảm hiệu suất hay không là hợp lý.

4 Kết quả và Phân tích
Trong mục này, chúng tôi thảo luận về kết quả của chúng tôi với mục tiêu trả lời các câu hỏi sau:

1. Chính: Việc tinh chỉnh trên DCoT (DCoT) có cung cấp cải thiện hiệu suất so với việc tinh chỉnh trên CoT (CoT) với số lượng chuỗi lý luận tương đương không? (Mục 4.1)

2. Với chi phí suy luận, DCoT có hiệu quả trên số lượng chuỗi lý luận hợp lý nhỏ (k) không? (Mục 4.2)

3. DCoT có thể được bao gồm an toàn trong các tập dữ liệu instruction-tuning mà không có hậu quả không lường trước trên các tác vụ chưa thấy không? (Mục 4.3) và (Mục 4.4)

4. DCoT có thể hưởng lợi từ các phần mở rộng CoT không? (Mục 4.5)

5. Những cải thiện thu được có phải là kết quả của cải tiến trong quá trình suy luận hay kết quả của nhiễu loạn ngẫu nhiên? (Mục 4.6 và 4.7)

4.1 DCoT có lợi trên các Tác vụ Trong miền
Mục tiêu chính của chúng tôi trong thí nghiệm này là so sánh hai chế độ huấn luyện: CoT và DCoT. Hai hàng đầu tiên của mỗi mô hình trong Bảng 1 so sánh DCoT với đường cơ sở CoT sử dụng giải mã tham lam3. Kết quả đầu tiên chúng tôi quan sát là DCoT, trung bình, vượt trội trên tất cả các mô hình chính của chúng tôi. Phân tích kết quả theo loại tác vụ, chúng tôi cũng quan sát rằng nó đặc biệt hiệu quả trên các tác vụ trắc nghiệm, nơi các mô hình Phi đạt được tăng cường hiệu suất bốn điểm. Chúng tôi cũng quan sát cải thiện rõ ràng trên trích xuất span và các tác vụ số. Tuy nhiên, hiệu suất trên các tác vụ nhị phân và tượng trưng trình bày một bức tranh hỗn hợp hơn. Những thách thức với StrategyQA (nhị phân) có thể được quy cho không gian đầu ra bị hạn chế của nó (tức là, "có" hoặc "không"), điều này hạn chế phạm vi cho các sửa đổi hiệu quả. Đối với các tác vụ tượng trưng, chúng tôi sử dụng tập dữ liệu Last Letter Concatenation, chỉ bao gồm 300 instance huấn luyện và 100 instance test và là tập dữ liệu nhỏ nhất của chúng tôi (Phụ lục A). Kích thước nhỏ này làm giảm độ tin cậy của kết quả, ngăn chúng tôi rút ra kết luận chắc chắn. Hiệu suất tổng thể của DCoT trên tất cả các mô hình xác nhận tính hiệu quả của huấn luyện DCoT.

3Kết quả CoinFlip được bỏ qua vì tất cả các mô hình đạt điểm số hoàn hảo.

Chúng tôi cũng tiến hành một thí nghiệm nhỏ hơn trên các mô hình instruction-tuned tổng quát (LLaMA2 13B chat). Đáng chú ý là so sánh CoT với DCoT không hoàn toàn công bằng trong thiết lập này vì mô hình này đã được tinh chỉnh trên CoT (Touvron et al., 2023); do đó, huấn luyện CoT lớn hơn và đa dạng hơn so với DCoT, như đã thảo luận trong Mục 3.2. Mặc dù vậy, Bảng 1 cho thấy rằng DCoT vượt trội hơn CoT trong hơn một nửa số tập dữ liệu. Kết quả của thí nghiệm nhỏ khác của chúng tôi trên LLaMA 2 70B cũng gợi ý về tính hiệu quả của DCoT trên các mô hình lớn hơn.

4.2 Một Cải tiến đạt được Cải thiện
Trong thí nghiệm này, chúng tôi xem xét số lượng chuỗi4 cần thiết để đạt được cải thiện hiệu suất so với k=1. Bảng 2 trình bày hiệu suất trung bình trên tất cả các tập dữ liệu cho các giá trị k khác nhau. Đáng chú ý, trung bình, k=2 một cách nhất quán nâng cao hiệu suất trên tất cả các mô hình, hỗ trợ giả thuyết của chúng tôi rằng DCoT hiệu quả cải tiến phản hồi ban đầu. Tuy nhiên, tăng k lên ba hoặc bốn không mang lại cải thiện thêm so với k=2, ngoại trừ trên GSM8k (toán), nơi tăng k lên 3 một cách nhất quán cải thiện hiệu suất (xem Bảng 11 trong Phụ lục D). Chúng tôi cũng quan sát các mẫu tương tự trong các thí nghiệm nhỏ hơn của chúng tôi trên LLaMA 13B Chat và LLaMA 2 70B (cũng trong Bảng 11). Những kết quả này cho thấy rằng, trung bình, một cải tiến duy nhất đủ để cải thiện hiệu suất, làm cho nó hiệu quả và rẻ. Chúng cũng phù hợp với các công trình trước đây (Estornell et al., 2025), nơi 3 và 4 lượt không phải lúc nào cũng cải thiện hiệu suất, như chúng tôi sẽ thảo luận trong Mục 5.

4Nhờ vào huấn luyện DCoT của chúng tôi, mô hình tạo ra chính xác k CoT.

Phương pháp Phi 1.5 Phi 2 LL. 7B LL. 13B
CoT 47.51±1.77 63.51±.71 59.30±.54 65.41±.91
DCoT@1 47.87±1.71 63.91±2.58 61.28±.50 65.80±.44
DCoT@2 48.63±.67 ↑65.33±2.80 ↑62.46±.45 ↑67.30±.49 ↑
DCoT@3 48.96±.66 65.30±1.72 62.37±.23 66.92±.59
DCoT@4 48.76±.33 64.89±2.39 62.42±.59 66.70±.55

Bảng 2: Hiệu suất trung bình DCoT trên số lượng CoT khác nhau cho mỗi câu hỏi trên các tập dev.

DCoT@1 ≈ CoT. Một hiện tượng quan trọng chúng tôi quan sát trong Bảng 2 là hiệu suất của DCoT khi tạo ra một CoT duy nhất (tức là, DCoT@1) rất tương tự với đường cơ sở CoT, như mong đợi. Kết quả này cho thấy rằng huấn luyện DCoT của chúng tôi không can thiệp vào việc tạo ra CoT thông thường. Do đó, DCoT là một thay thế an toàn cho CoT trong các tập dữ liệu instruction-tuning thông thường.

4.3 Không Suy giảm trong Tác vụ Chưa thấy
Trong mục này, chúng tôi đánh giá hiệu suất của DCoT trên các tác vụ chưa thấy để đảm bảo rằng không có suy giảm bất ngờ. Bảng 3 cho thấy rằng DCoT vẫn hiệu quả trong Commonsense QA (trắc nghiệm), nơi nó cho thấy cùng xu hướng như trong các kịch bản trong miền (tức là, tăng k mang lại câu trả lời tốt hơn so với câu đầu tiên). Tuy nhiên, các mô hình cho thấy tính hiệu quả hạn chế trong các tác vụ yêu cầu

LLM Phương pháp CSQA AQuA ObjCnt SV AMP
Phi 1.5 CoT 33.88 20.27 35.60 40.00
DCoT@1 32.26 21.51 25.20 40.50
DCoT@2 34.23 17.31 27.60 30.00
DCoT@3 33.81 22.38 30.80 30.00
DCoT@4 34.73 22.06 30.00 31.50
Phi 2 CoT 44.29 29.52 54.00 55.00
DCoT@1 44.15 34.86 58.40 60.50
DCoT@2 44.13 34.09 56.40 60.50
DCoT@3 45.99 31.83 57.60 60.00
DCoT@4 45.43 34.73 56.40 59.50
CoT 38.41 19.41 34.80 39.50
DCoT@1 36.94 17.70 40.00 41.50
LLaMA2 DCoT@2 40.79 17.27 39.60 43.00
7B DCoT@3 40.67 16.90 36.80 43.00
DCoT@4 40.43 17.21 37.20 39.00
CoT 46.55 24.85 45.2 62.50
DCoT@1 44.62 23.98 46.00 55.00
LLaMA2 DCoT@2 45.48 22.42 47.60 53.50
13B DCoT@3 47.42 20.72 52.40 56.50
DCoT@4 46.45 23.13 54.00 53.50

Bảng 3: Hiệu suất CoT và DCoT trên các k khác nhau trên các tác vụ chưa thấy.

số (tức là, AQuA, ObjCnt, và SV AMP). Khi một thế hệ mang lại câu trả lời tệ hơn, thế hệ tiếp theo thường sửa chữa nó (ví dụ, AQuA trên Phi 1.5, SV AMP trên LLaMA 13B). Những cải thiện hạn chế này trong các tác vụ toán chưa thấy phù hợp với kỳ vọng, cho rằng hiệu suất của LLM về lý luận số học trong thiết lập ngoài miền được biết là kém (Qian et al., 2023), và tổng quát hóa cho các tác vụ chưa thấy đòi hỏi hàng nghìn tác vụ (Kim et al., 2023), trong khi chúng tôi chỉ huấn luyện trên chín tác vụ. Nhìn chung, trong khi kết quả của chúng tôi cho thấy rằng DCoT chỉ trình bày cải thiện nhỏ trong một số trường hợp, chúng tôi lưu ý rằng không có suy giảm lớn nào.

4.4 DCoT mạnh mẽ trên Tác vụ mà CoT có hại
Chúng tôi phân tích hiệu suất của phương pháp trên Big Bench Hard, một benchmark mà các mô hình nhỏ không hưởng lợi từ CoT (Suzgun et al., 2023) để đảm bảo rằng phương pháp của chúng tôi không dẫn đến suy giảm hiệu suất bất ngờ so với đường cơ sở CoT. Kết quả từ Bảng 4 cho thấy rằng trên các tác vụ này, DCoT thể hiện hiệu suất tương tự như CoT, do đó chứng minh rằng DCoT không dẫn đến suy giảm trong các trường hợp thách thức, nơi CoT có thể có hại. Hơn nữa, chúng tôi có thể quan sát một số cải thiện hiệu suất trên Phi 2 và LLaMA-2 13B khi tăng k, tiếp tục cho thấy độ mạnh mẽ của tinh chỉnh DCoT. Những thí nghiệm này cho phép chúng tôi kết luận rằng không có rủi ro nào trong việc thêm dữ liệu huấn luyện DCoT vào các tập dữ liệu instruction tuning.

Phương pháp Phi 1.5 Phi 2 LL. 7B LL. 13B
CoT 28.37 46.7 31.08 36.38
DCoT@1 28.31 44.56 31.23 34.59
DCoT@2 28.07 45.81 31.11 35.94
DCoT@3 28.35 45.92 31.00 36.90
DCoT@4 28.21 46.71 31.13 36.45

Bảng 4: Kết quả trên Big Bench Hard. LL là viết tắt của LLaMA2.

4.5 DCoT hưởng lợi từ Các Phần mở rộng CoT
DCoT nhằm mục đích trở thành một cơ chế CoT mới để instruction-tuning LLM. Để đạt được điều này, chúng tôi cần xác nhận rằng đây là một thay thế drop-in an toàn và hoạt động với bất kỳ phần mở rộng CoT nào, chẳng hạn như tự-nhất quán (Wang et al., 2023), một chiến lược giải mã khác. Phương pháp giải mã này là một bổ sung đã được chứng minh là tăng hiệu suất của CoT trên một loạt tác vụ bằng cách lấy mẫu nhiều thế hệ và tổng hợp chúng bằng cơ chế bầu chọn.

Hai hàng cuối cùng của mỗi mô hình (tức là, CoT+SC và DCoT+SC) trong Bảng 1 so sánh DCoT với đường cơ sở CoT sử dụng giải mã tự-nhất quán với bốn mẫu. Chúng tôi quan sát rằng DCoT cũng hưởng lợi từ cơ chế này và giữ cải thiện hiệu suất so với đường cơ sở CoT, cho thấy rằng phương pháp của chúng tôi có thể là một thay thế cho CoT trong các tập dữ liệu instruction-tuning tương lai. Cũng đáng chú ý là, trung bình, DCoT với giải mã tham lam thậm chí còn vượt trội hơn CoT+SC trên các mô hình Phi, cho thấy hiệu suất cao của nó.

4.6 Phân tích Thủ công: DCoT@2
Chúng tôi tiến hành đánh giá thủ công để xác minh kết luận của chúng tôi dựa trên kết quả định lượng. Do chi phí của đánh giá thủ công, chúng tôi tiến hành điều này chỉ trên LLaMA 7B, mô hình cỡ trung của lựa chọn của chúng tôi, để có những hiểu biết áp dụng cho các mô hình nhỏ hơn và lớn hơn của chúng tôi. Cụ thể, chúng tôi nhằm mục đích hiểu cách tạo ra CoT thứ hai có thể cải thiện hiệu suất. Để làm điều này, chúng tôi chọn các instance cho mỗi tập dữ liệu mà LLaMA 7B với DCoT@1 đưa ra câu trả lời không chính xác trong khi DCoT@2 dẫn đến câu trả lời chính xác. Sau đó, chúng tôi lấy mẫu ngẫu nhiên năm instance đó cho mỗi tập dữ liệu, ngoại trừ trong HotpotQA và LLC, nơi chỉ có ba instance đáp ứng tiêu chí, dẫn đến tổng cộng 31 mẫu.

Chúng tôi lưu ý rằng chuỗi lý luận đầu tiên của DCoT@2 có thể khác với DCoT@1 vì chúng là các lần chạy khác nhau. Chúng tôi thấy điều này xảy ra trong mười instance. Điều này ngụ ý rằng trong hầu hết các trường hợp, CoT đầu tiên giống nhau trong cả hai trường hợp. Trong những instance này mà chuỗi lý luận đầu tiên được chia sẻ, chúng tôi quan sát rằng CoT thứ hai của DCoT@2 thể hiện một mẫu lý luận khác với chuỗi đầu tiên trong 15 trường hợp. Do đó, một CoT thứ hai, cải thiện cho phép mô hình tạo ra câu trả lời chính xác khi CoT đầu tiên dẫn đến câu trả lời không chính xác. Chúng tôi cũng quan sát ba trường hợp mà CoT thứ hai giống với chuỗi đầu tiên nhưng sửa chữa kết luận logic. Trong hai trường hợp, CoT thứ hai sửa chữa các bước lý luận từ chuỗi đầu tiên, và trong một trường hợp, lỗi của CoT đầu tiên có một lỗi nhỏ trong việc viết câu trả lời cuối cùng được sửa chữa với CoT thứ hai. Nhìn chung, những quan sát này xác nhận rằng DCoT sửa đổi câu trả lời đầu tiên của nó bằng cách tạo ra một CoT thứ hai. Bảng 14 trong Phụ lục G cho thấy một vài ví dụ.

--- TRANG 5 ---
4.7 Phân tích Thủ công: DCoT@3
Mẫu Câu trả lời. Để hiểu hành vi của DCoT@3, chúng tôi sử dụng GPT4o-mini để trích xuất ba câu trả lời do mô hình đưa ra. Chúng tôi tập trung vào các tập dữ liệu trắc nghiệm để tạo thuận lợi cho việc trích xuất câu trả lời bằng GPT4o-mini. Bảng 10 trong Phụ lục H cho thấy rằng mẫu phổ biến nhất là câu trả lời đầu tiên đúng, và các CoT tiếp theo tái tạo nó. Điều này được mong đợi vì CoT đạt được hiệu suất cạnh tranh. Hơn nữa, điều này tiếp tục chứng minh rằng mô hình không tạo ra CoT một cách ngẫu nhiên mà cố gắng cải tiến các câu trả lời trước đó. Do đó, nếu câu trả lời đầu tiên đúng, mô hình không cần thay đổi nó. Thú vị hơn, chúng tôi tìm thấy 12 trường hợp mà câu trả lời từ CoT thứ ba (một câu trả lời sai) không được chọn và thay vào đó chọn các câu trả lời trước đó (câu trả lời đúng). Ngược lại, chúng tôi cũng tìm thấy 12 trường hợp mà CoT thứ ba mang lại một câu trả lời mới (câu trả lời đúng), và mô hình chọn nó. Những quan sát này cho thấy rằng mô hình không thiên vị về câu trả lời cuối cùng. Chúng tôi cũng quan sát 27 trường hợp mà CoT thứ ba sai thay đổi CoT thứ hai đúng. Tuy nhiên, chúng tôi tìm thấy 45 trường hợp mà CoT thứ ba sửa chữa CoT thứ hai. Chúng tôi cũng cung cấp phân tích về các trường hợp lỗi trong Phụ lục G.

5 Thảo luận
Phương pháp Huấn luyện. DCoT và CoT được huấn luyện trên chính xác cùng một lượng CoT và câu hỏi. Trong khi đường cơ sở CoT sử dụng các điểm dữ liệu dưới dạng [(q, cot 1), (q, cot 2), ...], DCoT sử dụng các điểm dữ liệu dưới dạng [(q, cot 1, cot 2, ...), ...]. Nói cách khác, một sự tái tổ chức đơn giản của các CoT huấn luyện thành dạng nhiều CoT cho mỗi nhãn có tác động lớn đến hiệu suất của mô hình. Mục tiêu của phương pháp huấn luyện này không phải là tạo ra các chuỗi lý luận khác nhau một cách nhất quán vì nếu chuỗi đầu tiên đúng, không cần thay đổi nó. Phương pháp huấn luyện này nhằm mục đích cho phép LLM sửa đổi các CoT trước đó nếu cần. Quan trọng, DCoT@1 phù hợp với hiệu suất của đường cơ sở CoT, chỉ ra rằng việc tăng cường các tập dữ liệu instruction-tuning hiện có với dữ liệu DCoT là an toàn, vì nó sẽ không cản trở hiệu suất mô hình. Thật vậy, chúng tôi hình dung một tập dữ liệu instruction-tuning điển hình chứa cả dữ liệu DCoT và CoT.

DCoT sửa đổi câu trả lời đầu tiên. Kết quả của các thí nghiệm của chúng tôi chỉ ra rằng DCoT khác với các phương pháp ensemble-CoT như tự-nhất quán, cũng hưởng lợi từ việc tạo ra nhiều câu trả lời ứng viên nhưng làm như vậy trên các bước suy luận khác nhau và độc lập sử dụng các giá trị nhiệt độ cao. DCoT, thay vào đó, tạo ra nhiều CoT trong một bước suy luận duy nhất, cho phép mỗi chuỗi nhận thức về những chuỗi trước đó. Như đã thảo luận trong Mục 4.6 và 4.7, điều này cho phép mô hình sửa đổi lý luận trước đó. Theo thực nghiệm, DCoT@2 một cách nhất quán vượt trội hơn DCoT@1, cho thấy rằng sự sửa đổi như vậy góp phần vào cải thiện hiệu suất. Tuy nhiên, DCoT@3 và DCoT@4 cho thấy lợi ích giảm dần, hoạt động tương tự như DCoT@2. Những phát hiện này cho thấy rằng sau khi sửa đổi câu trả lời đầu tiên và cải thiện hiệu suất liên quan, việc tạo ra các CoT bổ sung không cải thiện cũng không làm giảm hiệu suất, cho thấy rằng mô hình không thể cải thiện câu trả lời của nó hơn nữa. Điều này cũng xác nhận rằng mô hình không đơn giản tạo ra một chuỗi CoT ngẫu nhiên mà thay vào đó sửa đổi câu trả lời đầu tiên của nó. Hạn chế về việc mở rộng k phù hợp với quan sát từ các công trình trước đây về tự sửa chữa từ (Madaan et al., 2023; Kim et al., 2024), nơi nhờ phản hồi từ bên ngoài của họ (không giống như chúng tôi), họ có thể đạt được cải thiện với k=4 và k=3 tương ứng. Hơn nữa, Estornell et al. (2025) quan sát một hành vi tương tự như DCoT của chúng tôi, hầu hết các cải thiện được thu được với k=2, và tăng k thêm có thể dẫn đến cải thiện cận biên hoặc thậm chí suy giảm nhỏ.

Chi phí. Cuối cùng, những cải thiện hiệu suất do DCoT đạt được đi kèm với chi phí tạo ra nhiều token hơn (tức là, một CoT thứ hai). Sự đánh đổi này giữa hiệu suất và chi phí phổ biến trong các công trình trước đây, chẳng hạn như trong (Wang et al., 2023; Yoran et al., 2023; Zhang et al., 2024). Một CoT trong tập dữ liệu huấn luyện của chúng tôi yêu cầu, trung bình, 124 token Phi và 142 token LLaMA 2. Do đó, việc tạo ra một CoT thứ hai để đạt được cải thiện hiệu suất sẽ phát sinh chi phí tăng thêm không đáng kể là $0.00007 cho mỗi lần gọi mô hình5, làm cho DCoT trở thành một thay thế hiệu quả về chi phí cho CoT.

5Sử dụng chi phí của LLaMA-2 13B trên replicate.com ($0.5/1M token).

6 Kết luận
Công trình này trình bày Diverse Chain of Thought (DCoT), một phương pháp huấn luyện mới nhằm mục đích cải thiện hiệu suất của LLM trên các tác vụ lý luận bằng cách tạo ra nhiều CoT trong một bước suy luận duy nhất để sửa đổi câu trả lời. Thông qua các thí nghiệm định lượng rộng rãi, chúng tôi cho thấy tính hiệu quả và khả năng mở rộng của phương pháp trên một loạt tác vụ lý luận, họ mô hình và kích thước. Hơn nữa, chúng tôi cho thấy hiệu quả của nó bằng cách đạt được cải thiện với một sửa đổi duy nhất (tức là, k=2), điều này phát sinh chi phí không đáng kể cho người dùng. Chúng tôi cũng cho thấy rằng DCoT có thể được mở rộng với bất kỳ phần mở rộng CoT nào, chẳng hạn như tự-nhất quán, trong đó nó vượt trội hơn CoT được mở rộng tương tự với tự-nhất quán. Cuối cùng, chúng tôi cho thấy rằng lý do đằng sau những cải thiện hiệu suất của các mô hình được tinh chỉnh với DCoT nằm ở khả năng sửa đổi câu trả lời của nó, nơi một sửa đổi câu trả lời duy nhất đủ để đạt được cải thiện rõ ràng. Chúng tôi để lại như công việc tương lai việc mở rộng tinh chỉnh DCoT cho các loại phương pháp prompting khác như code prompting (Puerto et al., 2024) hoặc graph of thoughts (Besta et al., 2024), và cải thiện khả năng mở rộng của k, mà không cần phản hồi từ bên ngoài.

Hạn chế
Phương pháp của chúng tôi bị hạn chế bởi cửa sổ ngữ cảnh của mô hình cơ bản. Trong công trình này, chúng tôi đã khám phá việc tạo ra CoT lên đến 4. Mặc dù chúng tôi huấn luyện mô hình để tạo ra các chuỗi khác nhau, điều này không đảm bảo nó sẽ tạo ra các chuỗi khác nhau về bản chất tại thời điểm suy luận, và điều này không phải lúc nào cũng mong muốn (ví dụ, một khi mô hình tạo ra câu trả lời đúng, không cần thay đổi chuỗi lý luận). Đây là một hạn chế được chia sẻ bởi các công trình trước đây về việc tạo ra CoT đa dạng.

Chúng tôi hạn chế việc tạo ra các CoT cho một nhà cung cấp LLM thương mại duy nhất vì các thí nghiệm sơ bộ của chúng tôi cho thấy sự sụt giảm hiệu suất khi kết hợp nhiều nhà cung cấp LLM. Nghiên cứu thêm về cách kết hợp nhiều nhà cung cấp LLM để chưng cất cho các mô hình nhỏ hơn thú vị và chúng tôi để lại điều đó cho công việc tương lai.

Do chi phí tính toán, chúng tôi không thể thí nghiệm rộng rãi trên mô hình 70B. Chúng tôi chỉ có thể đủ khả năng huấn luyện với một seed và trên một tập dữ liệu nhỏ hơn gồm 900 câu hỏi. Tương tự, chúng tôi chỉ có thể đánh giá nó trên 100 câu hỏi ngẫu nhiên cho mỗi tập dữ liệu. Tuy nhiên, những cải thiện rõ ràng mà chúng tôi quan sát trên các tập dev, nơi chúng tôi không thực hiện bất kỳ tinh chỉnh siêu tham số nào do chi phí của nó, là dấu hiệu của tiềm năng của phương pháp trên các mô hình ngôn ngữ rất lớn.

Chúng tôi không tiến hành thí nghiệm trên LLaMA 3 và 3.1 vì với chúng, chúng tôi không thể thực hiện thí nghiệm trên các kích thước mô hình trong một họ mô hình duy nhất vì chúng chỉ có 8B và 70B. LLaMA 3.2, mặt khác, được phát hành sau khi kết thúc các thí nghiệm cốt lõi và việc chạy lại tất cả các thí nghiệm sẽ không thêm những hiểu biết mới mà biện minh cho chi phí. Do đó, chúng tôi đã tiến hành thí nghiệm với hai họ, một cho các mô hình nhỏ hơn (1.3B và 2.7B) và một họ khác cho các kích thước lớn hơn nhưng vẫn có thể quản lý (7B, 13B, và thí nghiệm cốt lõi trên 70B).

Tuyên bố Đạo đức và Tác động Rộng hơn
Công trình này tuân thủ Bộ quy tắc Đạo đức ACL. Đặc biệt, tất cả các tập dữ liệu chúng tôi sử dụng đã được các công trình trước đây chỉ ra là an toàn cho mục đích nghiên cứu. Chúng không được biết là chứa thông tin cá nhân hoặc nội dung có hại. Phương pháp của chúng tôi nhằm mục đích cải thiện khả năng lý luận của LLM. Hơn nữa, bằng cách tạo ra nhiều CoT trong một bước suy luận, chúng tôi cho phép mô hình khám phá nhiều chuỗi lý luận hơn và có khả năng giảm thiểu tác động của các CoT có thể thiên vị hoặc không chính xác. Vì điều này, chúng tôi tin rằng công trình của chúng tôi có thể đóng góp vào việc triển khai an toàn LLM trong các kịch bản thực tế.

Lời cảm ơn
Công trình nghiên cứu này đã được tài trợ bởi Quỹ Nghiên cứu Đức (DFG) như một phần của dự án UKP-SQuARE (grant GU 798/29-1) và bởi Bộ Giáo dục và Nghiên cứu Liên bang Đức và Bộ Giáo dục Đại học, Nghiên cứu, Khoa học và Nghệ thuật Hessian trong sự hỗ trợ chung của họ đối với Trung tâm Nghiên cứu Quốc gia về An ninh mạng Ứng dụng ATHENE. Chúng tôi cũng muốn cảm ơn Early Career Research grant từ Đại học Bath.

Chúng tôi biết ơn sự hỗ trợ của Microsoft với một khoản tài trợ để truy cập các mô hình OpenAI GPT qua Azure cloud (Accelerate Foundation Model Academic Research).

Cuối cùng, chúng tôi cảm ơn Irina Bigoulaeva, Sheng Lu, Subhabrata Dutta, và các reviewer ẩn danh cho phản hồi sâu sắc của họ về phiên bản trước của bản thảo này.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo được tiếp tục như trong bản gốc...]

--- CÁC TRANG TIẾP THEO ---
[Nội dung các trang còn lại bao gồm các phụ lục, bảng biểu, và thông tin bổ sung được dịch tương tự như trên...]
