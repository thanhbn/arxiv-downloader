# 2111.15664.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/document-parsing/2111.15664.pdf
# File size: 7105978 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
OCR-free Document Understanding Transformer
Geewook Kim1‚àó, Teakgyu Hong4‚Ä†, Moonbin Yim2‚Ä†, Jeongyeon Nam1,
Jinyoung Park5‚Ä†, Jinyeong Yim6‚Ä†, Wonseok Hwang7‚Ä†, Sangdoo Yun3,
Dongyoon Han3, and Seunghyun Park1
1NAVER CLOVA2NAVER Search3NAVER AI Lab
4Upstage5Tmax6Google7LBox
Abstract. Understanding document images ( e.g., invoices) is a core but
challenging task since it requires complex functions such as reading text
and a holistic understanding of the document . Current Visual Document
Understanding (VDU) methods outsource the task of reading text to off-
the-shelf Optical Character Recognition (OCR) engines and focus on the
understanding task with the OCR outputs. Although such OCR-based
approaches have shown promising performance, they suffer from 1) high
computational costs for using OCR; 2) inflexibility of OCR models on
languages or types of documents; 3) OCR error propagation to the sub-
sequent process. To address these issues, in this paper, we introduce a
novel OCR-free VDU model named Donut , which stands for Document
understanding transformer. As the first step in OCR-free VDU research,
we propose a simple architecture ( i.e., Transformer) with a pre-training
objective ( i.e.,cross-entropy loss). Donut is conceptually simple yet ef-
fective. Through extensive experiments and analyses, we show a simple
OCR-free VDU model, Donut, achieves state-of-the-art performances on
various VDU tasks in terms of both speed and accuracy. In addition, we
offer a synthetic data generator that helps the model pre-training to be
flexible in various languages and domains. The code, trained model, and
synthetic data are available at https://github.com/clovaai/donut .
Keywords: Visual Document Understanding, Document Information
Extraction, Optical Character Recognition, End-to-End Transformer
1 Introduction
Document images, such as commercial invoices, receipts, and business cards,
are easy to find in modern working environments. To extract useful informa-
tion from such document images, Visual Document Understanding (VDU) has
not been only an essential task for industry, but also a challenging topic for re-
searchers, with applications including document classification [27,1], information
extraction [22,42], and visual question answering [44,57].
‚àóCorresponding author: gwkim.rsrch@gmail.com
‚Ä†This work was done while the authors were at NAVER CLOVA.arXiv:2111.15664v5  [cs.LG]  6 Oct 2022

--- PAGE 2 ---
2 G. Kim et al.
Document Image{ "items": [     {      "name": "3002-Kyoto Choco Mochi",      "count": 2,      "priceInfo": {        "unitPrice": 14000,        "price": 28000      }    }, ...  ],  "total": [ {       "menuqty_cnt": 4,       "total_price": 50000    }   ]}{ "words": [        {            "bbox":[[0.11,0.21],...,[0.19,0.22]],            "text": "3002-Kyoto"        }, {            "bbox":[[0.21,0.22],...,[0.45,0.23]],            "text": "Choco"        }, {            "bbox":[[0.46,0.22],...,[0.52,0.23]],            "text": "Mochi"        }, ‚Ä¶, {            "bbox":[[0.66,0.31],...,[0.72,0.32]],            "text": "50.000"        }    ]}Structured Information(a)(b)(c)(d)
Fig. 1. The schema of the conventional document information extraction
(IE) pipeline. (a) The goal is to extract the structured information from a given semi-
structured document image. In the pipeline, (b) text detection is conducted to obtain
text locations and (c) each box is passed to the recognizer to comprehend characters.
(d) Finally, the recognized texts and its locations are passed to the following module
to be processed for the desired structured form of the information
ImageOCRDownstream ModelOutputAS-IS (OCR + BERT, Layout LM, ‚Ä¶)
ImageE2E ModelOutputDonut üç©
(Proposed)1.20.9Time (sec/img)9182Accuracy (%)0.8
~0.6 sec143179+aMemory (M)
(a) Pipeline Overview. (b) System Benchmarks.
Fig. 2. The pipeline overview and benchmarks. The proposed end-to-end model,
Donut , outperforms the recent OCR-dependent VDU models in memory, time cost
and accuracy. Performances on visual document IE [45] are shown in (b). More results
on various VDU tasks are available at Section 3 showing the same trend
Current VDU methods [22,24,65,64,18] solve the task in a two-stage manner:
1) reading the texts in the document image; 2) holistic understanding of the doc-
ument. They usually rely on deep-learning-based Optical Character Recognition
(OCR) [4,3] for the text reading task and focus on modeling the understanding
part. For example, as shown in Figure 1, a conventional pipeline for extracting
structured information from documents (a.k.a. document parsing) consists of
three separate modules for text detection, text recognition, and parsing [22,24].
However, the OCR-dependent approach has critical problems. First of all, us-
ing OCR as a pre-processing method is expensive. We can utilize pre-trained off-
the-shelf OCR engines; however, the computational cost for inference would be
expensive for high-quality OCR results. Moreover, the off-the-shelf OCR meth-
ods rarely have flexibility dealing with different languages or domain changes,
which may lead to poor generalization ability. If we train an OCR model, it also
requires extensive training costs and large-scale datasets [4,3,39,46]. Another
problem is, OCR errors would propagate to the VDU system and negatively
influence subsequent processes [54,23]. This problem becomes more severe in

--- PAGE 3 ---
OCR-free Document Understanding Transformer 3
languages with complex character sets, such as Korean or Chinese, where the
quality of OCR is relatively low [50]. To deal with this, post-OCR correction
module [51,50,10] is usually adopted. However, it is not a practical solution for
real application environments since it increases the entire system size and main-
tenance cost.
We go beyond the traditional framework by modeling a direct mapping from
a raw input image to the desired output without OCR. We introduce a new
OCR-free VDU model to address the problems induced by the OCR-dependency.
Our model is based on Transformer-only architecture, referred to as Document
understanding transformer ( Donut ), following the huge success in vision and
language [8,9,29]. We present a minimal baseline including a simple architecture
and pre-training method. Despite its simplicity, Donut shows comparable or
better overall performance than previous methods as shown in Figure 2.
We take pre-train-and-fine-tune scheme [8,65] on Donut training. In the
pre-training phase, Donut learns how to read the texts by predicting the next
words by conditioning jointly on the image and previous text contexts. Donut
is pre-trained with document images and their text annotations. Since our pre-
training objective is simple ( i.e., reading the texts), we can realize domain and
language flexibility straightforwardly pre-training with synthetic data. During
fine-tuning stage, Donut learns how to understand the whole document accord-
ing to the downstream task. We demonstrate Donut has a strong understanding
ability through extensive evaluation on various VDU tasks and datasets. The
experiments show a simple OCR-free VDU model can achieve state-of-the-art
performance in terms of both speed and accuracy.
The contributions are summarized as follows:
1. We propose a novel OCR-free approach for VDU. To the best of our knowl-
edge, this is the first method based on an OCR-free Transformer trained in
end-to-end manner.
2. We introduce a simple pre-training scheme that enables the utilization of
synthetic data. By using our generator SynthDoG, we show Donut can
easily be extended to a multi-lingual setting, which is not applicable for the
conventional approaches that need to retrain an off-the-shelf OCR engine.
3. We conduct extensive experiments and analyses on both public benchmarks
and private industrial datasets, showing that the proposed method achieves
not only state-of-the-art performances on benchmarks but also has many
practical advantages (e.g., cost-effective ) in real-world applications.
4. The codebase, pre-trained model, and synthetic data are available at GitHub.1
2 Method
2.1 Preliminary: background
There have been various visual document understanding (VDU) methods to un-
derstand and extract essential information from the semi-structured documents
such as receipts [20,25,18], invoices [49], and form documents [14,6,43].
1https://github.com/clovaai/donut .

--- PAGE 4 ---
4 G. Kim et al.
<vqa><question>what is the price of choco mochi?</question><answer>Converted JSONtransformer encoder
Input Image and Prompt
transformer decoderDonut üç©
<classification><parsing><class>receipt</class></classification>14,000</answer></vqa><item><name>3002-Kyoto Choco Mochi</name>„Éª„Éª„Éª </parsing>{ "items": [{"name": "3002-Kyoto Choco Mochi",      "count": 2,      "unitprice": 14000, ‚Ä¶}], ‚Ä¶ }Output Sequence{ "class":"receipt" }{ "question": "what is the price of choco mochi?",   "answer": "14,000" }
Fig. 3. The pipeline of Donut. The encoder maps a given document image into
embeddings. With the encoded embeddings, the decoder generates a sequence of tokens
that can be converted into a target type of information in a structured form
Earlier VDU attempts have been done with OCR-independent visual back-
bones [27,1,15,12,31], but the performances are limited. Later, with the remark-
able advances of OCR [4,3] and BERT [8], various OCR-dependent VDU models
have been proposed by combining them [22,24,23]. More recently, in order to get
a more general VDU, most state-of-the-arts [64,18] use both powerful OCR en-
gines and large-scale real document image data (e.g., IIT-CDIP [32]) for a model
pre-training. Although they showed remarkable advances in recent years, extra
effort is required to ensure the performance of an entire VDU model by using
the off-the-shelf OCR engine.
2.2 Document Understanding Transformer
Donut is an end-to-end (i.e., self-contained) VDU model for general understand-
ing of document images. The architecture of Donut is quite simple, which con-
sists of a Transformer [58,9]-based visual encoder and textual decoder modules.
Note that Donut does not rely on any modules related to OCR functionality
but uses a visual encoder for extracting features from a given document im-
age. The following textual decoder maps the derived features into a sequence
of subword tokens to construct a desired structured format (e.g., JSON). Each
model component is Transformer-based, and thus the model is trained easily in
an end-to-end manner. The overall process of Donut is illustrated in Figure 3.
Encoder. The visual encoder converts the input document image x‚ààRH√óW√óC
into a set of embeddings {zi|zi‚ààRd,1‚â§i‚â§n}, where nis feature map size or the
number of image patches and dis the dimension of the latent vectors of the
encoder. Note that CNN-based models [17] or Transformer-based models [9,40]
can be used as the encoder network. In this study, we use Swin Transformer [40]
because it shows the best performance in our preliminary study in document
parsing. Swin Transformer first splits the input image xinto non-overlapping
patches. Swin Transformer blocks, consist of a shifted window-based multi-head
self-attention module and a two-layer MLP, are applied to the patches. Then,
patch merging layers are applied to the patch tokens at each stage. The output
of the final Swin Transformer block {z}is fed into the following textual decoder.

--- PAGE 5 ---
OCR-free Document Understanding Transformer 5
Decoder. Given the {z}, the textual decoder generates a token sequence ( yi)m
i=1,
where yi‚ààRvis an one-hot vector for the i-th token, vis the size of token vo-
cabulary, and mis a hyperparameter, respectively. We use BART [33] as the
decoder architecture. Specifically, we initialize the decoder model weights with
those from the publicly available2pre-trained multi-lingual BART model[38].
Model Input. Following the original Transformer [58], we use a teacher-forcing
scheme [62], which is a model training strategy that uses the ground truth as
input instead of model output from a previous time step. In the test phase,
inspired by GPT-3 [5], the model generates a token sequence given a prompt.
We add new special tokens for the prompt for each downstream task in our
experiments. The prompts that we use for our applications are shown with the
desired output sequences in Figure 3. Illustrative explanations for the teacher-
forcing strategy and the decoder output format are available in Appendix A.4.
Output Conversion. The output token sequence is converted to a desired
structured format. We adopt a JSON format due to its high representation
capacity. As shown in Figure 3, a token sequence is one-to-one invertible to a
JSON data. We simply add two special tokens [START ‚àó]and [END ‚àó], where ‚àó
indicates each field to extract. If the output token sequence is wrongly structured,
we simply treat the field is lost. For example, if there is only [START name] exists
but no [END name] , we assume the model fails to extract ‚Äúname‚Äù field. This
algorithm can easily be implemented with simple regular expressions [11].
2.3 Pre-training
Task. The model is trained to read all texts in the image in reading order (from
top-left to bottom-right, basically). The objective is to minimize cross-entropy
loss of next token prediction by jointly conditioning on the image and previous
contexts. This task can be interpreted as a pseudo-OCR task. The model is
trained as a visual language model over the visual corpora, i.e., document images.
Visual Corpora. We use IIT-CDIP [32], which is a set of 11M scanned english
document images. A commercial CLOVA OCR API is applied to get the pseudo
text labels. As aforementioned, however, this kind of dataset is not always avail-
able, especially for languages other than English. To alleviate the dependencies,
we build a scalable Synth eticDocument Generator , referred to as SynthDoG .
Using the SynthDog and Chinese, Japanese, Korean and English Wikipedia, we
generated 0.5M samples per language.
Synthetic Document Generator. The pipeline of image rendering basically
follows Yim et al. [67]. As shown in Figure 4, the generated sample consists of
2https://huggingface.co/hyunwoongko/asian-bart-ecjk .

--- PAGE 6 ---
6 G. Kim et al.
Fig. 4. Generated English, Chinese, Japanese, and Korean samples with
SynthDoG. Heuristic random patterns are applied to mimic the real documents
several components; background, document, text, and layout. Background image
is sampled from ImageNet [7], and a texture of document is sampled from the
collected paper photos. Words and phrases are sampled from Wikipedia. Layout
is generated by a simple rule-based algorithm that randomly stacks grids. In
addition, several image rendering techniques [13,41,67] are applied to mimic
real documents. The generated examples are shown in Figure 4. More details of
SynthDoG are available in the code1and Appendix A.2.
2.4 Fine-tuning
After the model learns how to read , in the application stage (i.e., fine-tuning), we
teach the model how to understand the document image. As shown in Figure 3,
we interpret all downstream tasks as a JSON prediction problem.
The decoder is trained to generate a token sequence that can be converted
into a JSON that represents the desired output information. For example, in the
document classification task, the decoder is trained to generate a token sequence
[START class][memo][END class] which is 1-to-1 invertible to a JSON {‚Äúclass‚Äù:
‚Äúmemo‚Äù }. We introduce some special tokens (e.g., [memo] is used for representing
the class ‚Äúmemo‚Äù), if such replacement is available in the target task.
3 Experiments and Analyses
In this section, we present Donut fine-tuning results on three VDU applications
on six different datasets including both public benchmarks and private industrial
service datasets. The samples are shown in Figure 5.
3.1 Downstream Tasks and Datasets
Document Classification. To see whether the model can distinguish across
different types of documents, we test a classification task. Unlike other models
that predict the class label via a softmax on the encoded embedding, Donut
generate a JSON that contains class information to maintain the uniformity of
the task-solving method. We report overall classification accuracy on a test set.
RVL-CDIP. The RVL-CDIP dataset [16] consists of 400K images in 16 classes,
with 25K images per class. The classes include letter, memo, email, and so on.
There are 320K training, 40K validation, and 40K test images.

--- PAGE 7 ---
OCR-free Document Understanding Transformer 7
form
handwritten(a)(b)
(c)
Q: What is the Extension Number as per the voucher? A: (910) 741-0673
Fig. 5. Samples of the downstream datasets. (a) Document Classification. (b)
Document Information Extraction. (c) Document Visual Question Answering
Document Information Extraction. To see the model fully understands
the complex layouts and contexts in documents, we test document information
extraction (IE) tasks on various real document images including both public
benchmarks and real industrial datasets. In this task, the model aims to map
each document to a structured form of information that is consistent with the
target ontology or database schema. See Figure 1 for an illustrative example. The
model should not only read the characters well, but also understand the layouts
and semantics to infer the groups and nested hierarchies among the texts.
We evaluate the models with two metrics; field-level F1 score [22,65,18] and
Tree Edit Distance (TED) based accuracy [68,70,23]. The F1 checks whether the
extracted field information is in the ground truth. Even if a single character is
missed, the score assumes the field extraction is failed. Although F1 is simple
and easy to understand, there are some limitations. First, it does not take into
account partial overlaps. Second, it can not measure the predicted structure (e.g.,
groups and nested hierarchy). To assess overall accuracy, we also use another
metric based on TED [68], that can be used for any documents represented as
trees. It is calculated as, max(0 ,1‚àíTED(pr ,gt)/TED( œï,gt)), where gt, pr, and œï
stands for ground truth, predicted, and empty trees respectively. Similar metrics
are used in recent works on document IE [70,23]
We use two public benchmark datasets as well as two private industrial
datasets which are from our active real-world service products. Each dataset
is explained in the followings.
CORD. The Consolidated Receipt Dataset (CORD)3[45] is a public benchmark
that consists of 0.8K train, 0.1K valid, 0.1K test receipt images. The letters
of receipts is in Latin alphabet. The number of unique fields is 30 containing
menu name, count, total price, and so on. There are complex structures (i.e.,
3https://huggingface.co/datasets/naver-clova-ix/cord-v1 .

--- PAGE 8 ---
8 G. Kim et al.
nested groups and hierarchies such as items>item> {name, count, price }) in the
information. See Figure 1 for more details.
Ticket. This is a public benchmark dataset [12] that consists of 1.5K train and
0.4K test Chinese train ticket images. We split 10% of the train set as a validation
set. There are 8 fields which are ticket number, starting station, train number,
and so on. The structure of information is simple and all keys are guaranteed to
appear only once and the location of each field is fixed.
Business Card (In-Service Data). This dataset is from our active products that
are currently deployed. The dataset consists of 20K train, 0.3K valid, 0.3K test
Japanese business cards. The number of fields is 11, including name, company,
address, and so on. The structure of information is similar to the Ticket dataset.
Receipt (In-Service Data). This dataset is also from one of our real products.
The dataset consists of 40K train, 1K valid, 1K test Korean receipt images.
The number of unique field is 81, which includes store information, payment
information, price information, and so on. Each sample has complex structures
compared to the aforementioned datasets. Due to industrial policies, not all
samples can publicly be available. Some real-like high-quality samples are shown
in Figure 5 and in the supplementary material.
Document Visual Question Answering. To validate the further capacity of
the model, we conduct a document visual question answering task (DocVQA). In
this task, a document image and question pair is given and the model predicts the
answer for the question by capturing both visual and textual information within
the image. We make the decoder generate the answer by setting the question as
a starting prompt to keep the uniformity of the method (See Figure 3).
DocVQA. The dataset is from Document Visual Question Answering competi-
tion4and consists of 50K questions defined on more than 12K documents [44].
There are 40K train, 5K valid, and 5K test questions. The evaluation metric is
ANLS (Average Normalized Levenshtein Similarity) which is an edit-distance-
based metric. The score on the test set is measured via the evaluation site.
3.2 Setups
We use Swin-B [40] as a visual encoder of Donut with slight modification.
We set the layer numbers and window size as {2,2,14,2}and 10. In further
consideration of the speed-accuracy trade-off, we use the first four layers of BART
as a decoder. As explained in Section 2.3, we train the multi-lingual Donut using
the 2M synthetic and 11M IIT-CDIP scanned document images. We pre-train the
model for 200K steps with 64 A100 GPUs and a mini-batch size of 196. We use
Adam [30] optimizer, the learning rate is scheduled and the initial rate is selected
4https://rrc.cvc.uab.es/?ch=17 .

--- PAGE 9 ---
OCR-free Document Understanding Transformer 9
Table 1. Classification results on the RVL-CDIP dataset. Donut achieves
state-of-the-are performance with reasonable speed and model size efficiency. Donut
is a general purpose backbone but does not rely on OCR while other recent backbones
(e.g., LayoutLM) do.‚Ä†# parameters for OCR should be considered for non-E2E models
OCR #Params Time (ms) Accuracy (%)
BERT ‚úì110M + Œ±‚Ä†1392 89.81
RoBERTa ‚úì125M + Œ±‚Ä†1392 90.06
LayoutLM ‚úì113M + Œ±‚Ä†1396 91.78
LayoutLM (w/ image) ‚úì160M + Œ±‚Ä†1426 94.42
LayoutLMv2 ‚úì200M + Œ±‚Ä†1489 95.25
Donut (Proposed) 143M 752 95.30
from 1e-5 to 1e-4. The input resolution is set to 2560 √ó1920 and a max length
in the decoder is set to 1536. All fine-tuning results are achieved by starting
from the pre-trained multi-lingual model. Some hyperparameters are adjusted
at fine-tuning and in ablation studies. We use 960 √ó1280 for Train Tickets and
Business Card parsing tasks. We fine-tune the model while monitoring the edit
distance over token sequences. The speed of Donut is measured on a P40 GPU,
which is much slower than A100. For the OCR based baselines, states-of-the-art
OCR engines are used, including MS OCR API used in [64] and CLOVA OCR
API5used in [24,23]. An analysis on OCR engines is available in Section 3.4.
More details of OCR and training setups are available in Appendix A.1 and A.5.
3.3 Experimental Results
Document Classification. The results are shown in Table 1. Without re-
lying on any other resource (e.g., off-the-shelf OCR engine), Donut shows a
state-of-the-art performance among the general-purpose VDU models such as
LayoutLM [65] and LayoutLMv2 [64]. In particular, Donut surpasses the Lay-
outLMv2 accuracy reported in [64], while using fewer parameters with the 2x
faster speed. Note that the OCR-based models must consider additional model
parameters and speed for the entire OCR framework, which is not small in gen-
eral. For example, a recent advanced OCR-based model [4,3] requires more than
80M parameters. Also, training and maintaining the OCR-based systems are
costly [23], leading to needs for the Donut-like end-to-end approach.
Document Information Extraction. Table 2 shows the results on the four
different document IE tasks. The first group uses a conventional BIO-tagging-
based IE approach [22]. We follows the conventions in IE [65,18]. OCR extracts
texts and bounding boxes from the image, and then the serialization module sorts
all texts with geometry information within the bounding box. The BIO-tagging-
based named entity recognition task performs token-level tag classification upon
5https://clova.ai/ocr .

--- PAGE 10 ---
10 G. Kim et al.
Table 2. Performances on various document IE tasks. The field-level F1 scores
and tree-edit-distance-based accuracies are reported. Donut shows the best accuracies
for all domains with significantly faster inference speed.‚Ä†Parameters for vocabulary
are omitted for fair comparisons among multi-lingual models.‚Ä°# parameters for OCR
should be considered.‚àóOfficial multi-lingual extension models are used
CORD [45] Ticket [12] Business Card Receipt
OCR #Params Time (s) F1 Acc. Time (s) F1 Acc. Time (s) F1 Acc. Time (s) F1 Acc.
BERT‚àó[22] ‚úì86‚Ä†
M+Œ±‚Ä°1.6 73.0 65.5 1.7 74.3 82.4 1.5 40.8 72.1 2.5 70.3 54.1
BROS [18] ‚úì86‚Ä†
M+Œ±‚Ä°1.7 74.7 70.0
LayoutLM [65] ‚úì89‚Ä†
M+Œ±‚Ä°1.7 78.4 81.3
LayoutLMv2‚àó[64,66] ‚úì179‚Ä†
M+Œ±‚Ä°1.7 78.9 82.4 1.8 87.2 90.1 1.6 52.2 83.0 2.6 72.9 78.0
Donut 143‚Ä†
M 1.2 84.1 90.9 0.6 94.1 98.7 1.4 57.8 84.4 1.9 78.6 88.6
SPADE‚àó[25] ‚úì93‚Ä†
M+Œ±‚Ä°4.0 74.0 75.8 4.5 14.9 29.4 4.3 32.3 51.3 7.3 64.1 53.2
WYVERN‚àó[21] ‚úì106‚Ä†
M+Œ±‚Ä°1.2 43.3 46.9 1.5 41.8 54.8 1.7 29.9 51.5 3.4 71.5 82.9
the ordered texts to generate a structured form. We test three general-purpose
VDU backbones, BERT [8], BROS [18], LayoutLM [65], and LayoutLMv2 [64,66].
We also test two recently proposed IE models, SPADE [24] and WYVERN [23].
SPADE is a graph-based IE method that predicts relations between bounding
boxes. WYVERN is an Transformer encoder-decoder model that directly gen-
erates entities with structure given OCR outputs. WYVERN is different from
Donut in that it takes the OCR output as its inputs.
For all domains, including public and private in-service datasets, Donut shows
the best scores among the comparing models. By measuring both F1 and TED-
based accuracy, we observe not only Donut can extract key information but
also predict complex structures among the field information. We observe that
a large input resolution gives robust accuracies but makes the model slower.
For example, the performance on the CORD with 1280 √ó960 was 0.7 sec./image
and 91.1 accuracy. But, the large resolution showed better performances on the
low-resource situation. The detailed analyses are in Section 3.4. Unlike other
baselines, Donut shows stable performance regardless of the size of datasets and
complexity of the tasks (See Figure 5). This is a significant impact as the target
tasks are already actively used in industries.
Document Visual Question Answering. Table 3 shows the results on the
DocVQA dataset. The first group is the general-purposed VDU backbones whose
scores are from the LayoutLMv2 paper [64]. We measure the running time with
MS OCR API used in [64]. The model in the third group is a DocVQA-specific-
purposed fine-tuning model of LayoutLMv2, whose inference results are available
in the official leader-board.6
As can be seen, Donut achieves competitive scores with the baselines that
are dependent on external OCR engines. Especially, Donut shows that it is robust
to the handwritten documents, which is known to be challenging to process. In
the conventional approach, adding a post-processing module that corrects OCR
6https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=1 .

--- PAGE 11 ---
OCR-free Document Understanding Transformer 11
Table 3. Average Normalized Levenshtein Similarity (ANLS) scores on
DocVQA. Donut shows a promising result without OCR.‚àóDonut shows a high
ANLS score on the handwritten documents which are known to be challenging due to
the difficulty of handwriting OCR (See Figure 6).‚Ä†Token embeddings for English is
counted for a fair comparison.‚Ä°# parameters for OCR should be considered
Fine-tuning set OCR #Params‚Ä†Time (ms)ANLS
test setANLS‚àó
handwritten
BERT [64] train set ‚úì110M + Œ±‚Ä°1517 63.5 n/a
LayoutLM[65] train set ‚úì113M + Œ±‚Ä°1519 69.8 n/a
LayoutLMv2[64] train set ‚úì200M + Œ±‚Ä°1610 78.1 n/a
Donut train set 176M 782 67.5 72.1
LayoutLMv2-Large-QG[64] train + dev + QG ‚úì390M + Œ±‚Ä°1698 86.7 67.3
Q: What is the name of the passenger? 
Answer: DR. William J. Darby 
Donut: DR. William J. Darby 
LayoutLMv2-Large-QG: DR. William J. Jarry Q: What is the Publication No.? 
Answer: 540
Donut: 943  (another number in the image is extracted) 
LayoutLMv2-Large-QG: 540
Q: What is the phone number given? 
Answer: 336-723-6100 
Donut: 336-723-6100 
LayoutLMv2-Large-QG: 336-723- 4100 
Fig. 6. Examples of Donut and LayoutLMv2 outputs on DocVQA. The OCR-
errors make a performance upper-bound of the OCR-dependent baselines, e.g., Lay-
outLMv2 (left and middle examples). Due to the input resolution constraint of the
end-to-end pipeline, Donut miss some tiny texts in large-scale images (right example)
but this could be mitigated by scaling the input image size (See Section 3.4)
errors is an option to strengthen the pipeline [51,50,10] or adopting an encoder-
decoder architecture on the OCR outputs can mitigate the problems of OCR
errors [23]. However, this kind of approaches tend to increase the entire system
size and maintenance cost. Donut shows a completely different direction. Some
inference results are shown in Figure 6. The samples show the current strengths
of Donut as well as the left challenges in the Donut-like end-to-end approach.
Further analysis and ablation is available in Section 3.4.
3.4 Further Studies
In this section, we study several elements of understanding Donut. We show some
striking characteristics of Donut through the experiments and visualization.
On Pre-training Strategy. We test several pre-training tasks for VDUs. Fig-
ure 7(a) shows that the Donut pre-training task (i.e., text reading) is the most

--- PAGE 12 ---
12 G. Kim et al.
No pretraining Classification CaptioningRead (SynthDoG)Read (CDIP) Read (Both) ResNet-152 EffNetv2ViT-BSwin-B 18 Swin-B 14 640x640 960x960 1280x960 2560√ó1920
 30507090Accuracy
020406080DocVQA score(a) Pretrain Strategy                                          (b) Backbone                                         (c) Resolution
CORD (Accuracy)
DocVQA Score (ANLS)
Fig. 7. Analysis on (a) pre-training strategies, (b) image backbones, and (c)
input resolutions. Performances on CORD [45] and DocVQA [44] are shown
simple yet effective approach. Other tasks that impose a general knowledge of
images and texts on models, e.g., image captioning, show little gains in the
fine-tuning tasks. For the text reading tasks, we verify three options, SynthDoG
only, IIT-CDIP only, and both. Note that synthetic images were enough for the
document IE task in our analysis. However, in the DocVQA task, it was impor-
tant to see the real images. This is probably because the image distributions of
IIT-CDIP and DocVQA are similar [44].
On Encoder Backbone. Here, we study popular image classification back-
bones that show superior performance in traditional vision tasks to measure
their performance in VDU tasks. The Figure 7(b) shows the comparison results.
We use all the backbones pre-trained on ImageNet [7]. EfficientNetV2 [55] and
Swin Transformer [40] outperform others on both datasets. We argue that this is
due to the high expressiveness of the backbones, which were shown by the strik-
ing scores on several downstream tasks as well. We choose Swin Transformer
due to the high scalability of the Transformer-based architecture and higher
performance over the EfficientNetV2‚Äôs.
On Input Resolution. The Figure 7(c) shows the performance of Donut grows
rapidly as we set a larger input size. This gets clearer in the DocVQA where the
images are larger with many tiny texts. But, increasing the size for a precise
result incurs bigger computational costs. Using an efficient attention mecha-
nism [60] may avoid the matter in architectural design, but we use the original
Transformer [58] as we aim to present a simpler architecture in this work.
On Text Localization. To see how the model behaves, we visualize the corss
attention maps of the decoder given an unseen document image. As can be seen
in Figure 8, the model shows meaningful results that can be used as an auxiliary
indicator. The model attends to a desired location in the given image.
On OCR System. We test four widely-used public OCR engines (See Fig-
ure 9). The results show that the performances (i.e., speed and accuracy) of the
conventional OCR-based methods heavily rely on the off-the-shelf OCR engine.
More details of the OCR engines are available in Appendix A.1.

--- PAGE 13 ---
OCR-free Document Understanding Transformer 13
K
ChocoMochiyoto3002-
Fig. 8. Visualization of cross-attention maps in the decoder and its applica-
tion to text localization. Donut is trained without any supervision for the localiza-
tion. The Donut decoder attends proper text regions to process the image
LayoutLMv2 BERT Donut405060708090 Accuracy
1280   2560EasyOCR
PaddleOCR
MSAzure
ClovaOCR
LayoutLMv2 BERT Donut0.51.01.5 time(s/image)
1280   2560EasyOCR
PaddleOCR
MSAzure
ClovaOCR
80 160 400 800
number of samples5060708090 Accuracy
Donut, 2560
Donut, 1280LayoutLMv2
BERT
Fig. 9. Comparison of BERT, LayoutLMv2 and Donut on CORD. The perfor-
mances (i.e., speed and accuracy) of the OCR-based models extremely varies depending
on what OCR engine is used (left and center). Donut shows robust performances even
in a low resourced situation showing the higher score only with 80 samples (right)
On Low Resourced Situation. We evaluate the models by limiting the size
of training set of CORD [45]. The performance curves are shown in the right
Figure 9. Donut shows a robust performances. We also observe that a larger
input resolution, 2560 √ó1920, shows more robust scores on the extremely low-
resourced situation, e.g., 80 samples. As can be seen, Donut outperformed the
LayoutLMv2 accuracy only with 10% of the data, which is only 80 samples.
4 Related Work
4.1 Optical Character Recognition
Recent trends of OCR study are to utilize deep learning models in its two sub-
steps: 1) text areas are predicted by a detector; 2) a text recognizer then rec-
ognizes all characters in the cropped image instances. Both are trained with a
large-scale datasets including the synthetic images [26,13] and real images [28,47].
Early detection methods used CNNs to predict local segments and apply
heuristics to merge them [19,69]. Later, region proposal and bounding box regres-
sion based methods were proposed [36]. Recently, focusing on the homogeneity
and locality of texts, component-level approaches were proposed [56,4].
Many modern text recognizer share a similar approach [37,53,52,59] that can
be interpreted into a combination of several common deep modules [3]. Given the
cropped text instance image, most recent text recognition models apply CNNs
to encode the image into a feature space. A decoder is then applied to extract
characters from the features.

--- PAGE 14 ---
14 G. Kim et al.
4.2 Visual Document Understanding
Classification of the document type is a core step towards automated document
processing. Early methods treated the problem as a general image classification,
so various CNNs were tested [27,1,15]. Recently, with BERT [8], the methods
based on a combination of CV and NLP were widely proposed [65,34]. As a
common approach, most methods rely on an OCR engine to extract texts; then
the OCR-ed texts are serialized into a token sequence; finally they are fed into
a language model (e.g., BERT) with some visual features if available. Although
the idea is simple, the methods showed remarkable performance improvements
and became a main trend in recent years [64,35,2].
Document IE covers a wide range of real applications [22,42], for example,
given a bunch of raw receipt images, a document parser can automate a major
part of receipt digitization, which has been required numerous human-labors
in the traditional pipeline. Most recent models [25,23] take the output of OCR
as their input. The OCR results are then converted to the final parse through
several processes, which are often complex. Despite the needs in the industry,
only a few works have been attempted on end-to-end parsing. Recently, some
works are proposed to simplify the complex parsing processes [25,23]. But they
still rely on a separate OCR to extract text information.
Visual QA on documents seeks to answer questions asked on document im-
ages. This task requires reasoning over visual elements of the image and general
knowledge to infer the correct answer [44]. Currently, most state-of-the-arts fol-
low a simple pipeline consisting of applying OCR followed by BERT-like trans-
formers [65,64]. However, the methods work in an extractive manner by their
nature. Hence, there are some concerns for the question whose answer does not
appear in the given image [57]. To tackle the concerns, generation-based methods
have also been proposed [48].
5 Conclusions
In this work, we propose a novel end-to-end framework for visual document un-
derstanding. The proposed method, Donut , directly maps an input document
image into a desired structured output. Unlike conventional methods, Donut
does not depend on OCR and can easily be trained in an end-to-end fashion. We
also propose a synthetic document image generator, SynthDoG, to alleviate the
dependency on large-scale real document images and we show that Donut can
be easily extended to a multi-lingual setting. We gradually trained the model
from how to read tohow to understand through the proposed training pipeline.
Our extensive experiments and analysis on both external public benchmarks
and private internal service datasets show higher performance and better cost-
effectiveness of the proposed method. This is a significant impact as the target
tasks are already practically used in industries. Enhancing the pre-training ob-
jective could be a future work direction. We believe our work can easily be
extended to other domains/tasks regarding document understanding.

--- PAGE 15 ---
OCR-free Document Understanding Transformer 15
References
1. Afzal, M.Z., Capobianco, S., Malik, M.I., Marinai, S., Breuel, T.M.,
Dengel, A., Liwicki, M.: Deepdocclassifier: Document classification with
deep convolutional neural network. In: 2015 13th International Conference
on Document Analysis and Recognition (ICDAR). pp. 1111‚Äì1115 (2015).
https://doi.org/10.1109/ICDAR.2015.7333933 1, 4, 14
2. Appalaraju, S., Jasani, B., Kota, B.U., Xie, Y., Manmatha, R.: Docformer: End-to-
end transformer for document understanding. In: Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV). pp. 993‚Äì1003 (October
2021) 14
3. Baek, J., Kim, G., Lee, J., Park, S., Han, D., Yun, S., Oh, S.J., Lee, H.: What is
wrong with scene text recognition model comparisons? dataset and model analysis.
In: Proceedings of the IEEE/CVF International Conference on Computer Vision
(ICCV) (October 2019) 2, 4, 9, 13, 22
4. Baek, Y., Lee, B., Han, D., Yun, S., Lee, H.: Character region awareness for text
detection. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition (CVPR). pp. 9357‚Äì9366 (2019). https://doi.org/10.1109/CVPR.2019.00959
2, 4, 9, 13, 22
5. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-
lakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A.,
Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C.,
Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner,
C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are
few-shot learners. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.F., Lin,
H. (eds.) Advances in Neural Information Processing Systems. vol. 33, pp. 1877‚Äì
1901. Curran Associates, Inc. (2020), https://proceedings.neurips.cc/paper/
2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf 5
6. Davis, B., Morse, B., Cohen, S., Price, B., Tensmeyer, C.: Deep visual template-free
form parsing. In: 2019 International Conference on Document Analysis and Recog-
nition (ICDAR). pp. 134‚Äì141 (2019). https://doi.org/10.1109/ICDAR.2019.00030
3
7. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-
scale hierarchical image database. In: 2009 IEEE conference on computer vision
and pattern recognition. pp. 248‚Äì255. Ieee (2009) 6, 12, 23
8. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep
bidirectional transformers for language understanding. In: Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers).
pp. 4171‚Äì4186. Association for Computational Linguistics, Minneapolis, Minnesota
(Jun 2019). https://doi.org/10.18653/v1/N19-1423, https://aclanthology.org/
N19-1423 3, 4, 10, 14, 27, 29
9. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby,
N.: An image is worth 16x16 words: Transformers for image recognition at scale.
In: 9th International Conference on Learning Representations, ICLR 2021, Virtual
Event, Austria, May 3-7, 2021. OpenReview.net (2021), https://openreview.net/
forum?id=YicbFdNTTy 3, 4
10. Duong, Q., H¬® am¬® al¬® ainen, M., Hengchen, S.: An unsupervised method for OCR
post-correction and spelling normalisation for Finnish. In: Proceedings of the

--- PAGE 16 ---
16 G. Kim et al.
23rd Nordic Conference on Computational Linguistics (NoDaLiDa). pp. 240‚Äì248.
Link¬® oping University Electronic Press, Sweden, Reykjavik, Iceland (Online) (May
31‚Äì2 Jun 2021), https://aclanthology.org/2021.nodalida-main.24 3, 11
11. Friedl, J.E.F.: Mastering Regular Expressions. O‚ÄôReilly, Beijing, 3
edn. (2006), https://www.safaribooksonline.com/library/view/
mastering-regular-expressions/0596528124/ 5
12. Guo, H., Qin, X., Liu, J., Han, J., Liu, J., Ding, E.: Eaten: Entity-aware at-
tention for single shot visual text extraction. In: 2019 International Confer-
ence on Document Analysis and Recognition (ICDAR). pp. 254‚Äì259 (2019).
https://doi.org/10.1109/ICDAR.2019.00049 4, 8, 10, 22, 24, 25, 26
13. Gupta, A., Vedaldi, A., Zisserman, A.: Synthetic data for text localisation in nat-
ural images. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) (June 2016) 6, 13
14. Hammami, M., H¬¥ eroux, P., Adam, S., d‚ÄôAndecy, V.P.: One-shot field spotting
on colored forms using subgraph isomorphism. In: 2015 13th International Con-
ference on Document Analysis and Recognition (ICDAR). pp. 586‚Äì590 (2015).
https://doi.org/10.1109/ICDAR.2015.7333829 3
15. Harley, A.W., Ufkes, A., Derpanis, K.G.: Evaluation of deep convolutional nets
for document image classification and retrieval. In: 2015 13th International Con-
ference on Document Analysis and Recognition (ICDAR). pp. 991‚Äì995 (2015).
https://doi.org/10.1109/ICDAR.2015.7333910 4, 14
16. Harley, A.W., Ufkes, A., Derpanis, K.G.: Evaluation of deep convolutional nets
for document image classification and retrieval. In: 2015 13th International Con-
ference on Document Analysis and Recognition (ICDAR). pp. 991‚Äì995 (2015).
https://doi.org/10.1109/ICDAR.2015.7333910 6
17. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
pp. 770‚Äì778 (2016). https://doi.org/10.1109/CVPR.2016.90 4
18. Hong, T., Kim, D., Ji, M., Hwang, W., Nam, D., Park, S.: Bros: A pre-trained
language model focusing on text and layout for better key information extrac-
tion from documents. Proceedings of the AAAI Conference on Artificial Intelli-
gence 36(10), 10767‚Äì10775 (Jun 2022). https://doi.org/10.1609/aaai.v36i10.21322,
https://ojs.aaai.org/index.php/AAAI/article/view/21322 2, 3, 4, 7, 9, 10, 22,
27
19. Huang, W., Qiao, Y., Tang, X.: Robust scene text detection with convolution
neural network induced mser trees. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars,
T. (eds.) Computer Vision ‚Äì ECCV 2014. pp. 497‚Äì511. Springer International
Publishing, Cham (2014) 13
20. Huang, Z., Chen, K., He, J., Bai, X., Karatzas, D., Lu, S., Jawahar, C.V.: Ic-
dar2019 competition on scanned receipt ocr and information extraction. In: 2019
International Conference on Document Analysis and Recognition (ICDAR). pp.
1516‚Äì1520 (2019). https://doi.org/10.1109/ICDAR.2019.00244 3
21. Hwang, A., Frey, W.R., McKeown, K.: Towards augmenting lexical resources for
slang and African American English. In: Proceedings of the 7th Workshop on
NLP for Similar Languages, Varieties and Dialects. pp. 160‚Äì172. International
Committee on Computational Linguistics (ICCL), Barcelona, Spain (Online) (Dec
2020), https://aclanthology.org/2020.vardial-1.15 10
22. Hwang, W., Kim, S., Yim, J., Seo, M., Park, S., Park, S., Lee, J., Lee, B., Lee, H.:
Post-ocr parsing: building simple and robust parser via bio tagging. In: Workshop
on Document Intelligence at NeurIPS 2019 (2019) 1, 2, 4, 7, 9, 10, 14, 28, 29

--- PAGE 17 ---
OCR-free Document Understanding Transformer 17
23. Hwang, W., Lee, H., Yim, J., Kim, G., Seo, M.: Cost-effective end-to-end infor-
mation extraction for semi-structured document images. In: Proceedings of the
2021 Conference on Empirical Methods in Natural Language Processing. pp. 3375‚Äì
3383. Association for Computational Linguistics, Online and Punta Cana, Do-
minican Republic (Nov 2021). https://doi.org/10.18653/v1/2021.emnlp-main.271,
https://aclanthology.org/2021.emnlp-main.271 2, 4, 7, 9, 10, 11, 14, 27
24. Hwang, W., Yim, J., Park, S., Yang, S., Seo, M.: Spatial depen-
dency parsing for semi-structured document information extraction. In:
Findings of the Association for Computational Linguistics: ACL-IJCNLP
2021. pp. 330‚Äì343. Association for Computational Linguistics, Online (Aug
2021). https://doi.org/10.18653/v1/2021.findings-acl.28, https://aclanthology.
org/2021.findings-acl.28 2, 4, 9, 10
25. Hwang, W., Yim, J., Park, S., Yang, S., Seo, M.: Spatial depen-
dency parsing for semi-structured document information extraction. In:
Findings of the Association for Computational Linguistics: ACL-IJCNLP
2021. pp. 330‚Äì343. Association for Computational Linguistics, Online (Aug
2021). https://doi.org/10.18653/v1/2021.findings-acl.28, https://aclanthology.
org/2021.findings-acl.28 3, 10, 14, 27
26. Jaderberg, M., Simonyan, K., Vedaldi, A., Zisserman, A.: Synthetic data and ar-
tificial neural networks for natural scene text recognition. In: Workshop on Deep
Learning, NIPS (2014) 13
27. Kang, L., Kumar, J., Ye, P., Li, Y., Doermann, D.S.: Convolutional neural networks
for document image classification. 2014 22nd International Conference on Pattern
Recognition pp. 3168‚Äì3172 (2014) 1, 4, 14
28. Karatzas, D., Gomez-Bigorda, L., Nicolaou, A., Ghosh, S., Bagdanov, A., Iwamura,
M., Matas, J., Neumann, L., Chandrasekhar, V.R., Lu, S., Shafait, F., Uchida,
S., Valveny, E.: Icdar 2015 competition on robust reading. In: 2015 13th Interna-
tional Conference on Document Analysis and Recognition (ICDAR). pp. 1156‚Äì1160
(2015). https://doi.org/10.1109/ICDAR.2015.7333942 13
29. Kim, W., Son, B., Kim, I.: Vilt: Vision-and-language transformer without con-
volution or region supervision. In: Meila, M., Zhang, T. (eds.) Proceedings of
the 38th International Conference on Machine Learning. Proceedings of Ma-
chine Learning Research, vol. 139, pp. 5583‚Äì5594. PMLR (18‚Äì24 Jul 2021),
http://proceedings.mlr.press/v139/kim21k.html 3
30. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: Bengio,
Y., LeCun, Y. (eds.) 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings
(2015), http://arxiv.org/abs/1412.6980 8, 25
31. Klaiman, S., Lehne, M.: Docreader: Bounding-box free training of a document
information extraction model. In: Document Analysis and Recognition ‚Äì IC-
DAR 2021: 16th International Conference, Lausanne, Switzerland, September
5‚Äì10, 2021, Proceedings, Part I. p. 451‚Äì465. Springer-Verlag, Berlin, Heidel-
berg (2021). https://doi.org/10.1007/978-3-030-86549-8 29,https://doi.org/10.
1007/978-3-030-86549-8_29 4
32. Lewis, D., Agam, G., Argamon, S., Frieder, O., Grossman, D., Heard, J.: Building
a test collection for complex document information processing. In: Proceedings of
the 29th Annual International ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval. p. 665‚Äì666. SIGIR ‚Äô06, Association for Computing
Machinery, New York, NY, USA (2006). https://doi.org/10.1145/1148170.1148307,
https://doi.org/10.1145/1148170.1148307 4, 5

--- PAGE 18 ---
18 G. Kim et al.
33. Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoy-
anov, V., Zettlemoyer, L.: BART: Denoising sequence-to-sequence pre-training
for natural language generation, translation, and comprehension. In: Proceed-
ings of the 58th Annual Meeting of the Association for Computational Lin-
guistics. pp. 7871‚Äì7880. Association for Computational Linguistics, Online (Jul
2020). https://doi.org/10.18653/v1/2020.acl-main.703, https://aclanthology.
org/2020.acl-main.703 5
34. Li, C., Bi, B., Yan, M., Wang, W., Huang, S., Huang, F., Si, L.: Struc-
turalLM: Structural pre-training for form understanding. In: Proceedings of the
59th Annual Meeting of the Association for Computational Linguistics and
the 11th International Joint Conference on Natural Language Processing (Vol-
ume 1: Long Papers). pp. 6309‚Äì6318. Association for Computational Linguis-
tics, Online (Aug 2021). https://doi.org/10.18653/v1/2021.acl-long.493, https:
//aclanthology.org/2021.acl-long.493 14
35. Li, P., Gu, J., Kuen, J., Morariu, V.I., Zhao, H., Jain, R., Manjunatha, V., Liu,
H.: Selfdoc: Self-supervised document representation learning. In: 2021 IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR). pp. 5648‚Äì5656
(2021). https://doi.org/10.1109/CVPR46437.2021.00560 14
36. Liao, M., Shi, B., Bai, X., Wang, X., Liu, W.: Textboxes: A fast text detector with
a single deep neural network. Proceedings of the AAAI Conference on Artificial
Intelligence 31(1) (Feb 2017). https://doi.org/10.1609/aaai.v31i1.11196, https:
//ojs.aaai.org/index.php/AAAI/article/view/11196 13
37. Liu, W., Chen, C., Wong, K.Y.K., Su, Z., Han, J.: Star-net: A spatial attention
residue network for scene text recognition. In: Richard C. Wilson, E.R.H., Smith,
W.A.P. (eds.) Proceedings of the British Machine Vision Conference (BMVC).
pp. 43.1‚Äì43.13. BMVA Press (September 2016). https://doi.org/10.5244/C.30.43,
https://dx.doi.org/10.5244/C.30.43 13
38. Liu, Y., Gu, J., Goyal, N., Li, X., Edunov, S., Ghazvininejad, M., Lewis, M.,
Zettlemoyer, L.: Multilingual denoising pre-training for neural machine translation.
Transactions of the Association for Computational Linguistics 8, 726‚Äì742 (2020),
https://aclanthology.org/2020.tacl-1.47 5
39. Liu, Y., Chen, H., Shen, C., He, T., Jin, L., Wang, L.: Abcnet: Real-time scene text
spotting with adaptive bezier-curve network. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) (June 2020) 2
40. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin trans-
former: Hierarchical vision transformer using shifted windows. In: Proceedings of
the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 10012‚Äì
10022 (October 2021) 4, 8, 12
41. Long, S., Yao, C.: Unrealtext: Synthesizing realistic scene text images from the
unreal world. arXiv preprint arXiv:2003.10608 (2020) 6
42. Majumder, B.P., Potti, N., Tata, S., Wendt, J.B., Zhao, Q., Najork, M.: Rep-
resentation learning for information extraction from form-like documents. In:
Proceedings of the 58th Annual Meeting of the Association for Computational
Linguistics. pp. 6495‚Äì6504. Association for Computational Linguistics, Online
(Jul 2020). https://doi.org/10.18653/v1/2020.acl-main.580, https://www.aclweb.
org/anthology/2020.acl-main.580 1, 14
43. Majumder, B.P., Potti, N., Tata, S., Wendt, J.B., Zhao, Q., Najork, M.: Repre-
sentation learning for information extraction from form-like documents. In: Pro-
ceedings of the 58th Annual Meeting of the Association for Computational Lin-
guistics. pp. 6495‚Äì6504. Association for Computational Linguistics, Online (Jul

--- PAGE 19 ---
OCR-free Document Understanding Transformer 19
2020). https://doi.org/10.18653/v1/2020.acl-main.580, https://aclanthology.
org/2020.acl-main.580 3
44. Mathew, M., Karatzas, D., Jawahar, C.: Docvqa: A dataset for vqa on document
images. In: Proceedings of the IEEE/CVF Winter Conference on Applications of
Computer Vision. pp. 2200‚Äì2209 (2021) 1, 8, 12, 14
45. Park, S., Shin, S., Lee, B., Lee, J., Surh, J., Seo, M., Lee, H.: Cord: A consolidated
receipt dataset for post-ocr parsing. In: Workshop on Document Intelligence at
NeurIPS 2019 (2019) 2, 7, 10, 12, 13, 22, 24, 26
46. Peng, D., Wang, X., Liu, Y., Zhang, J., Huang, M., Lai, S., Zhu, S., Li, J., Lin,
D., Shen, C., Jin, L.: SPTS: Single-Point Text Spotting. CoRR abs/2112.07917
(2021), https://arxiv.org/abs/2112.07917 2
47. Phan, T.Q., Shivakumara, P., Tian, S., Tan, C.L.: Recognizing text with per-
spective distortion in natural scenes. In: Proceedings of the IEEE International
Conference on Computer Vision (ICCV) (December 2013) 13
48. Powalski, R., Borchmann,  L., Jurkiewicz, D., Dwojak, T., Pietruszka, M., Pa lka,
G.: Going full-tilt boogie on document understanding with text-image-layout trans-
former. In: Llad¬¥ os, J., Lopresti, D., Uchida, S. (eds.) Document Analysis and
Recognition ‚Äì ICDAR 2021. pp. 732‚Äì747. Springer International Publishing, Cham
(2021) 14
49. Riba, P., Dutta, A., Goldmann, L., Forn¬¥ es, A., Ramos, O., Llad¬¥ os, J.: Table de-
tection in invoice documents by graph neural networks. In: 2019 International
Conference on Document Analysis and Recognition (ICDAR). pp. 122‚Äì127 (2019).
https://doi.org/10.1109/ICDAR.2019.00028 3
50. Rijhwani, S., Anastasopoulos, A., Neubig, G.: OCR Post Correction for
Endangered Language Texts. In: Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Processing (EMNLP). pp.
5931‚Äì5942. Association for Computational Linguistics, Online (Nov 2020).
https://doi.org/10.18653/v1/2020.emnlp-main.478, https://aclanthology.org/
2020.emnlp-main.478 3, 11
51. Schaefer, R., Neudecker, C.: A two-step approach for automatic OCR post-
correction. In: Proceedings of the The 4th Joint SIGHUM Workshop on Com-
putational Linguistics for Cultural Heritage, Social Sciences, Humanities and Lit-
erature. pp. 52‚Äì57. International Committee on Computational Linguistics, Online
(Dec 2020), https://aclanthology.org/2020.latechclfl-1.6 3, 11
52. Shi, B., Bai, X., Yao, C.: An end-to-end trainable neural network for image-based
sequence recognition and its application to scene text recognition. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence 39, 2298‚Äì2304 (2017) 13
53. Shi, B., Wang, X., Lyu, P., Yao, C., Bai, X.: Robust scene text recog-
nition with automatic rectification. In: 2016 IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR). pp. 4168‚Äì4176 (2016).
https://doi.org/10.1109/CVPR.2016.452 13
54. Taghva, K., Beckley, R., Coombs, J.: The effects of ocr error on the extraction of
private information. In: Bunke, H., Spitz, A.L. (eds.) Document Analysis Systems
VII. pp. 348‚Äì357. Springer Berlin Heidelberg, Berlin, Heidelberg (2006) 2
55. Tan, M., Le, Q.: Efficientnetv2: Smaller models and faster training. In: Meila, M.,
Zhang, T. (eds.) Proceedings of the 38th International Conference on Machine
Learning. Proceedings of Machine Learning Research, vol. 139, pp. 10096‚Äì10106.
PMLR (18‚Äì24 Jul 2021), https://proceedings.mlr.press/v139/tan21a.html 12
56. Tian, Z., Huang, W., He, T., He, P., Qiao, Y.: Detecting text in natural image with
connectionist text proposal network. In: Leibe, B., Matas, J., Sebe, N., Welling,

--- PAGE 20 ---
20 G. Kim et al.
M. (eds.) Computer Vision ‚Äì ECCV 2016. pp. 56‚Äì72. Springer International Pub-
lishing, Cham (2016) 13
57. Tito, R., Mathew, M., Jawahar, C.V., Valveny, E., Karatzas, D.: Icdar 2021 compe-
tition on document visual question answering. In: Llad¬¥ os, J., Lopresti, D., Uchida,
S. (eds.) Document Analysis and Recognition ‚Äì ICDAR 2021. pp. 635‚Äì649. Springer
International Publishing, Cham (2021) 1, 14
58. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
Kaiser, L.u., Polosukhin, I.: Attention is all you need. In: Guyon, I., Luxburg,
U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R.
(eds.) Advances in Neural Information Processing Systems. vol. 30. Curran
Associates, Inc. (2017), https://proceedings.neurips.cc/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf 4, 5, 12, 24, 27
59. Wang, J., Hu, X.: Gated recurrent convolution neural network for ocr. In: Guyon,
I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Gar-
nett, R. (eds.) Advances in Neural Information Processing Systems. vol. 30.
Curran Associates, Inc. (2017), https://proceedings.neurips.cc/paper/2017/
file/c24cd76e1ce41366a4bbe8a49b02a028-Paper.pdf 13
60. Wang, S., Li, B., Khabsa, M., Fang, H., Ma, H.: Linformer: Self-attention with
linear complexity. arXiv preprint arXiv:2006.04768 (2020) 12, 27
61. Wightman, R.: Pytorch image models. https://github.com/rwightman/
pytorch-image-models (2019). https://doi.org/10.5281/zenodo.4414861 25
62. Williams, R.J., Zipser, D.: A learning algorithm for continually running fully re-
current neural networks. Neural computation 1(2), 270‚Äì280 (1989) 5
63. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac,
P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen,
P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M.,
Lhoest, Q., Rush, A.: Transformers: State-of-the-art natural language processing.
In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing: System Demonstrations. pp. 38‚Äì45. Association for Computational
Linguistics, Online (Oct 2020). https://doi.org/10.18653/v1/2020.emnlp-demos.6,
https://aclanthology.org/2020.emnlp-demos.6 25
64. Xu, Y., Xu, Y., Lv, T., Cui, L., Wei, F., Wang, G., Lu, Y., Florencio,
D., Zhang, C., Che, W., Zhang, M., Zhou, L.: LayoutLMv2: Multi-modal
pre-training for visually-rich document understanding. In: Proceedings of the
59th Annual Meeting of the Association for Computational Linguistics and
the 11th International Joint Conference on Natural Language Processing (Vol-
ume 1: Long Papers). pp. 2579‚Äì2591. Association for Computational Linguis-
tics, Online (Aug 2021). https://doi.org/10.18653/v1/2021.acl-long.201, https:
//aclanthology.org/2021.acl-long.201 2, 4, 9, 10, 11, 14, 22, 27, 28
65. Xu, Y., Li, M., Cui, L., Huang, S., Wei, F., Zhou, M.: Layoutlm: Pre-training of
text and layout for document image understanding. In: Proceedings of the 26th
ACM SIGKDD International Conference on Knowledge Discovery & Data Min-
ing. p. 1192‚Äì1200. KDD ‚Äô20, Association for Computing Machinery, New York,
NY, USA (2020). https://doi.org/10.1145/3394486.3403172, https://doi.org/
10.1145/3394486.3403172 2, 3, 7, 9, 10, 11, 14, 22, 28
66. Xu, Y., Lv, T., Cui, L., Wang, G., Lu, Y., Florencio, D., Zhang, C., Wei, F.:
Layoutxlm: Multimodal pre-training for multilingual visually-rich document un-
derstanding. arXiv preprint arXiv:2104.08836 (2021) 10, 27
67. Yim, M., Kim, Y., Cho, H.C., Park, S.: Synthtiger: Synthetic text image generator
towards better text recognition models. In: Llad¬¥ os, J., Lopresti, D., Uchida, S.

--- PAGE 21 ---
OCR-free Document Understanding Transformer 21
(eds.) Document Analysis and Recognition ‚Äì ICDAR 2021. pp. 109‚Äì124. Springer
International Publishing, Cham (2021) 5, 6, 23
68. Zhang, K., Shasha, D.: Simple fast algorithms for the editing distance be-
tween trees and related problems. SIAM J. Comput. 18, 1245‚Äì1262 (12 1989).
https://doi.org/10.1137/0218082 7
69. Zhang, Z., Zhang, C., Shen, W., Yao, C., Liu, W., Bai, X.: Multi-oriented
text detection with fully convolutional networks. In: 2016 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR). pp. 4159‚Äì4167 (2016).
https://doi.org/10.1109/CVPR.2016.451 13
70. Zhong, X., ShafieiBavani, E., Jimeno Yepes, A.: Image-based table recognition:
Data, model, and evaluation. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.M.
(eds.) Computer Vision ‚Äì ECCV 2020. pp. 564‚Äì580. Springer International Pub-
lishing, Cham (2020) 7

--- PAGE 22 ---
22 G. Kim et al.
A Appendix
A.1 Details of OCR Engines (MS, CLOVA, Easy, Paddle)
Current state-of-the-art visual document understanding (VDU) backbones, such
as BROS [18], LayoutLM [65] and LayoutLMv2 [64], are dependent on off-the-
shelf OCR engines. These backbones take the output of OCR as their (one of)
input features. For the OCR-dependent methods, in our experiments, we use
state-of-the-art OCR engines that are publicly available, including 2 OCR API
products (i.e., MS OCR7and CLOVA OCR8) and 2 open-source OCR models
(i.e., Easy OCR9and Paddle OCR10). In the main paper, Paddle OCR is used
for the Chinese train ticket dataset [12] and CLOVA OCR is used for the rest
datasets in the document information extraction (IE) tasks. MS OCR is used
to measure the running time of the LayoutLM family in document classification
and visual question answering (VQA) tasks, following the previous work of Xu
et al. [64]. Each OCR engine is explained in the following.
MS OCR MS OCR7is the latest OCR API product from Microsoft and used in
several recent VDU methods, e.g., LayoutLMv2 [64]. This engine supports 164
languages for printed text and 9 languages for handwritten text (until 2022/03).
CLOVA OCR CLOVA OCR8is an API product from NAVER CLOVA and
is specialized in document IE tasks. This engine supports English, Japanese and
Korean (until 2022/03). In the ablation experiments on the CORD dataset [45]
(Figure 9 in the main paper), the CLOVA OCR achieved the best accuracy.
Easy OCR Easy OCR9is a ready-to-use OCR engine that is publicly available
at GitHub. This engine supports more than 80 languages (until 2022/03). Un-
like the aforementioned two OCR products (i.e., MS OCR and CLOVA OCR),
this engine is publicly opened and downloadable.9The entire model architec-
ture is based on the modern deep-learning-based OCR modules [4,3] with some
modifications to make the model lighter and faster. The total number of model
parameters is 27M which is small compared to the state-of-the-art models [4,3].
Paddle OCR Paddle OCR10is an open-source OCR engine available at GitHub.
We used a lightweight (i.e., mobile) version of the model which is specially de-
signed for a fast and light OCR of English and Chinese texts. The model is served
on a CPU environment and the size of the model is extremely small, which is
approximately 10M.
7https://docs.microsoft.com/en-us/azure/cognitive-services/
computer-vision/overview-ocr .
8https://clova.ai/ocr/en .
9https://github.com/JaidedAI/EasyOCR .
10https://github.com/PaddlePaddle/PaddleOCR .

--- PAGE 23 ---
OCR-free Document Understanding Transformer 23
Fig. A. Examples of SynthDoG. English, Chinese, Japanese and Korean samples
are shown (from top to bottom). Although the idea is simple, these synthetic samples
play an important role in the pre-training of Donut. Please, see Figure 7 in the main
paper for details
A.2 Details of Synthetic Document Generator (SynthDoG)
In this section, we explain the components of the proposed Synthetic Document
Generator (SynthDoG) in detail. The entire pipeline basically follows Yim et
al. [67]. Our source code is available at https://github.com/clovaai/donut .
More samples are shown in Figure A.
Background Background images are sampled from ImageNet [7]. Gaussian blur
is randomly applied to the background image to represent out-of-focus effects.
Document Paper textures are sampled from the photos that we collected. The
texture is applied to an white background. In order to make the texture realistic,
random elastic distortion and Gaussian noise are applied. To represent various
view angles in photographs, a random perspective transformation is applied to
the image.

--- PAGE 24 ---
24 G. Kim et al.
Text Layout and Pattern To mimic the layouts in real-world documents, a
heuristic rule-based pattern generator is applied to the document image region
to generate text regions. The main idea is to set multiple squared regions to rep-
resent text paragraphs. Each squared text region is then interpreted as multiple
lines of text. The size of texts and text region margins are chosen randomly.
Text Content and Style We prepare the multi-lingual text corpora from
Wikipedia.11We use Noto fonts12since it supports various languages. SynthDoG
samples texts and fonts from these resources and the sampled texts are rendered
in the regions that are generated by the layout pattern generator. The text colors
are randomly assigned.
Post-processing Finally, some post-processing techniques are applied to the
output image. In this process, the color, brightness, and contrast of the image
are adjusted. In addition, shadow effect, motion blur, Gaussian blur, and JPEG
compression are applied to the image.
A.3 Details of Document Information Extraction
Information Extraction (IE) on documents is an arduous task since it requires (a)
reading texts, (b) understanding the meaning of the texts, and (c) predicting the
relations and structures among the extracted information. Some previous works
have only focused on extracting several pre-defined key information [12]. In that
case, only (a) and (b) are required for IE models. We go beyond the previous
works by considering (c) also. Although the task is complex, its interface (i.e., the
format of input and output) is simple. In this section, for explanation purposes,
we show some sample images (which are the raw input of the IE pipeline) with
the output of Donut.
In the main paper, we test four datasets including two public benchmarks
(i.e., CORD [45] and Ticket [12]) and two private industrial datasets (i.e., Busi-
ness Card andReceipt ). Figure B shows examples of Ticket with the outputs
of Donut. Figure C shows examples of CORD with the outputs of Donut. Due
to strict industrial policies on the private industrial datasets, we instead show
some real-like high-quality samples of Business Card andReceipt in Figure D.
A.4 Details of Model Training Scheme and Output Format
In the model architecture and training objective, we basically followed the orig-
inal Transformer [58], which uses a Transformer encoder-decoder architecture
and a teacher-forcing training scheme. The teacher-forcing scheme is a model
training strategy that uses the ground truth as input instead of model output
from a previous time step. Figure E shows a details of the model training scheme
and decoder output format.
11https://dumps.wikimedia.org .
12https://fonts.google.com/noto .

--- PAGE 25 ---
OCR-free Document Understanding Transformer 25
(a) Input Image(b) Prediction(c) Ground Truth
Ticket
{    "date":"2017Âπ¥11Êúà15Êó•",    "destination_station": "Á¶èÁî∞Á´ô",    "name": "ÁèÇ",    "seat_category": "‰∫åÁ≠âÂ∫ß",    "starting_station": "ÂπøÂ∑ûÂçóÁ´ô",    "ticket_num": "C068987",    "ticket_rates": ¬•82.0ÂÖÉ",    "train_num": "G79‚Äù}{    "date": "2017Âπ¥12Êúà05Êó•",    "destination_station": "ÂπøÂ∑û‰∏úÁ´ô",    "name": "Âª∂Ëæâ",    "seat_category": "‰∏ÄÁ≠âÂ∫ß",    "starting_station": "Ê∑±Âú≥Á´ô",    "ticket_num": "E019154",    "ticket_rates": "¬•99.5ÂÖÉ",    "train_num": "C7128"}
{    "date": "2018Âπ¥02Êúà13Êó•",    "destination_station": "ÊâéÂÖ∞Âπ≥Á´ô",    "name": "Êµ∑Èπè",    "seat_category": "Êñ∞Á©∫Ë∞ÉÁ°¨Âçß",    "starting_station": "Âåó‰∫¨Á´ô",    "ticket_num": ‚ÄùJ033534",    "ticket_rates": "¬•367.0ÂÖÉ",    "train_num": "K1301‚Äù}{    "date": "2018Âπ¥02Êúà13Êó•",    "destination_station": "ÊâéÂÖ∞Â±ØÁ´ô",    "name": "Êµ∑Èπè",    "seat_category": "Êñ∞Á©∫Ë∞ÉÁ°¨Âçß",    "starting_station": "Âåó‰∫¨Á´ô",    "ticket_num": ‚ÄùJ033534",    "ticket_rates": "¬•367.0ÂÖÉ",    "train_num": "K1301‚Äù}TED Acc. 97.7TED Acc. 97.5{    "date": "2017Âπ¥12Êúà05Êó•",    "destination_station": "ÂπøÂ∑û‰∏úÁ´ô",    "name": "Âª∂Ë§•",    "seat_category": "‰∏ÄÁ≠âÂ∫ß",    "starting_station": "Ê∑±Âú≥Á´ô",    "ticket_num": "E019154",    "ticket_rates": "¬•99.5ÂÖÉ",    "train_num": "C7128"}TED Acc. 100{    "date":"2017Âπ¥11Êúà15Êó•",    "destination_station": "Á¶èÁî∞Á´ô",    "name": "ÁèÇ",    "seat_category": "‰∫åÁ≠âÂ∫ß",    "starting_station": "ÂπøÂ∑ûÂçóÁ´ô",    "ticket_num": "C068987",    "ticket_rates": ¬•82.0ÂÖÉ",    "train_num": "G79‚Äù}
Fig. B. Examples of Ticket [12] with Donut predictions. There is no hierarchy in
the structure of information (i.e., depth = 1) and the location of each key information
is almost fixed. Failed predictions are marked and bolded (red)
A.5 Implementation and Training Hyperparameters
The codebase and settings are available at GitHub.13We implement the en-
tire model pipeline with Huggingface‚Äôs transformers14[63] and an open-source
library TIMM (PyTorch image models)15[61].
For all model training, we use a half-precision (fp16) training. We train Donut
using Adam optimizer [30] by decreasing the learning rate as the training pro-
gresses. The initial learning rate of pre-training is set to 1e-4 and that of fine-
tuning is selected from 1e-5 to 1e-4. We pre-train the model for 200K steps with
64 NVIDIA A100 GPUs and a mini-batch size of 196, which takes about 2-3
GPU days. We also apply a gradient clipping technique where a maximum gra-
dient norm is selected from 0.05 to 1.0. The input resolution of Donut is set
to 2560 √ó1920 at the pre-training phase. In downstream tasks, the input reso-
lutions are controlled. In some downstream document IE experiments, such as,
13https://github.com/clovaai/donut .
14https://github.com/huggingface/transformers .
15https://github.com/rwightman/pytorch-image-models .

--- PAGE 26 ---
26 G. Kim et al.
{    "menu": [        {            "cnt": ["1"],            "nm": ["cashew nuts chkn"],            "price": ["64,500"]        },          ‚Ä¶        {            "cnt": ["4"],            "nm": ["steamed rice"],            "price": ["47,600"]        }    ],    "sub_total": [        {            "service_price": ["17,908"],            "subtotal_price": ["325,600"],            "tax_price": ["34,351"]        }    ],    "total": [        {            "total_price": ["377,859"]        } ]}TED Acc. 100
(a) Input Image(b) Prediction(c) Ground Truth
CORD{    "menu": [        {            "cnt": ["2"],            "nm": ["TWIST DONUT"],            "price": ["18,000"]        },         ‚Ä¶        {            "cnt": ["1"],            "nm": ["FRANKFRUT SAUSAGE ROLL"],            "price": ["12,000"]        }    ],    "total": [        {            "cashprice": ["104.000"],            "changeprice": ["56.000"],            "total_price": ["54.000"]        } ]}TED Acc. 99.0{    "menu": [        {            "cnt": ["2"],            "nm": ["TWIST DONUT"],            "price": ["18,000"]        },         ‚Ä¶        {            "cnt": ["1"],            "nm": ["FRANKFRUT SAUSAGE ROLL"],            "price": ["12,000"]        }    ],    "total": [        {            "cashprice": ["104.000"],            "changeprice": ["50.000"],            "total_price": ["54.000"]        } ]}{    "menu": [        {            "nm": ["TRAD KY TOAST CARTE"],            "price": ["28.182"]        }    ],    "sub_total": [        {            "subtotal_price": ["28.182"],            "tax_price": ["2.818"]        }    ],    "total": [        {            "cashprice": ["31.000"],            "menuqty_cnt": ["1.00"],            "total_price": ["31.000"]        } ]}{    "menu": [        {            "nm": ["TRAD KY TOAST CARTE"],            "price": ["28.182"]        }    ],    "sub_total": [        {            "subtotal_price": ["28.182"],            "tax_price": ["2.818"]        }    ],    "total": [        {            "cashprice": ["31.000"],            "total_price": ["31.000"]        } ]}
TED Acc. 89.6{    "menu": [        {            "cnt": ["1"],            "nm": ["cashew nuts chkn"],            "price": ["64,500"]        },          ‚Ä¶        {            "cnt": ["4"],            "nm": ["steamed rice"],            "price": ["47,600"]        }    ],    "sub_total": [        {            "service_price": ["17,908"],            "subtotal_price": ["325,600"],            "tax_price": ["34,351"]        }    ],    "total": [        {            "total_price": ["377,859"]        } ]}
Fig. C. Examples of CORD [45] with Donut predictions. There is a hierarchy
in the structure of information (i.e., depth = 2). Donut not only reads some important
key information from the image, but also predicts the relationship among the extracted
information (e.g., the name, price, and quantity of each menu item are grouped)
CORD [45], Ticket [12] and Business Card , smaller size of input resolution,
e.g., 1280 √ó960, is tested. With the 1280 √ó960 setting, the model training cost
of Donut was small. For example, the model fine-tuning on CORD orTicket
took approximately 0.5 hours with one A100 GPU. However, when we set the

--- PAGE 27 ---
OCR-free Document Understanding Transformer 27
Fig. D. Examples of Business Card (top) and Receipt (bottom). Due to strict
industrial policies on the private industrial datasets from our active products, real-like
high-quality samples are shown instead
2560√ó1920 setting for larger datasets, e.g., RVL-CDIP orDocVQA , the cost
increased rapidly. With 64 A100 GPUs, DocVQA requires one GPU day and
RVL-CDIP requires two GPU days approximately. This is not surprising in that
increasing the input size for a precise result incurs higher computational costs
in general. Using an efficient attention mechanism [60] may avoid the prob-
lem in architectural design, but we use the original Transformer [58] as we aim
to present a simpler architecture in this work. Our preliminary experiments in
smaller resources are available in Appendix A.6.
For the implementation of document IE baselines, we use the transformers
library for BERT [8], BROS [18], LayoutLMv2 [64,66] and WYVERN [23]. For
the SPADE [25] baseline, the official implementation16is used. The models are
trained using NVIDIA P40, V100, or A100 GPUs. The major hyperparameters,
such as initial learning rate and number of epochs, are adjusted by monitoring
the scores on the validation set. The architectural details of the OCR-dependent
VDU backbone baselines (e.g., LayoutLM and LayoutLMv2) are available in
Appendix A.7.
A.6 Preliminary Experiments in Smaller Resources
In our preliminary experiments, we pre-trained Donut with smaller resources
(denoted as Donut Proto), i.e., smaller data (SynthDoG 1.2M) and fewer GPUs
16https://github.com/clovaai/spade .

--- PAGE 28 ---
28 G. Kim et al.
*URXQG7UXWK
,QSXW7RNHQV'HFRGHU2XWSXW'LVWULEXWLRQV'HFRGHU¬´¬´¬´7UDLQLQJ,QIHUHQFH
7RNHQFODVVLILFDWLRQDWHDFKVWHSSDUVLQJ!LWHP!QDPH!LWHP!QDPH!0LQLPL]H&URVV(QWURS\3UHGLFWHG¬´¬´SDUVLQJ!LWHP!QDPH!
¬´LWHP!QDPH!'HFRGHU
3UHGLFWHGWRNHQVHTXHQFHLVFRQYHUWHGLQWRD-621IRUPDW^¬≥,WHP¬¥^¬≥QDPH¬¥¬≥.\RWR&KRFR0RFKL¬¥¬≥SULFH¬¥¬≥¬¥¬≥FRXQW¬¥¬≥¬¥``SDUVLQJ!LWHP!QDPH!.\RWR&KRFR0RFKLQDPH!SULFH!SULFH!FRXQW!FRXQW!LWHP!HQG!
Fig. E. Donut training scheme with teacher forcing and decoder output
format examples. The model is trained to minimize cross-entropy loss of the token
classifications simultaneously. At inference, the predicted token from the last step is
fed to the next
(8 V100 GPUs for 5 days). The input size was 2048 √ó1536. In this setting,
Donut Proto also achieved comparable results on RVL-CDIP andCORD . The
accuracy on RVL-CDIP was 94.5 and CORD was 85.4. After the preliminaries,
we have scaled the model training with more data.
A.7 Details of OCR-dependent Baseline Models
In this section, we provide a gentle introduction to the general-purpose VDU
backbones, such as LayoutLM [65] and LayoutLMv2 [64]. To be specific, we
explain how the conventional backbones perform downstream VDU tasks; docu-
ment classification, IE, and VQA. Common to all tasks, the output of the OCR
engine is used as input features of the backbone. That is, the extracted texts
are sorted and converted to a sequence of text tokens. The sequence is passed to
the Transformer encoder to get contextualized output vectors. The vectors are
used to get the desired output. The difference in each task depends on a slight
modification on the input sequence or on the utilization of the output vectors.
Document Classification At the start of the input sequence, a special token
[CLS] is appended. The sequence is passed to the backbone to get the output
vectors. With a linear mapping and softmax operation, the output vector of the
special token [CLS] is used to get a class-label prediction.
Document IE With a linear mapping and softmax operation, the output vector
sequence is converted to a BIO-tag sequence [22].

--- PAGE 29 ---
OCR-free Document Understanding Transformer 29
IE on 1-depth structured documents When there is no hierarchical structure in
the document (See Figure B), the tag set is defined as {‚ÄúBk‚Äù, ‚ÄúI k‚Äù, ‚ÄúO‚Äù |k‚àà
pre-defined keys }. ‚ÄúBk‚Äù and ‚ÄúI k‚Äù are tags that represent the beginning (B) and
the inside (I) token of the key krespectively. The ‚ÄúO‚Äù tag indicates that the
token belongs to no key information.
IE on n-depth structured documents When there are hierarchies in the structure
(See Figure C), the BIO-tags are defined for each hierarchy level. In this section,
we explain a case where the depth of structure is n= 2. The tag set is defined
as{‚ÄúBg.Bk‚Äù, ‚ÄúB g.Ik‚Äù, ‚ÄúI g.Bk‚Äù, ‚ÄúI g.Ik‚Äù, ‚ÄúO‚Äù |g‚ààpre-defined parent keys, k‚àà
pre-defined child keys }. For instance, the Figure C shows an example where a
parent key is ‚Äúmenu‚Äù and related child keys are {‚Äúcnt‚Äù, ‚Äúnm‚Äù, ‚Äúprice‚Äù }. ‚ÄúB g‚Äù
represents that one group (i.e., a parent key such as ‚Äúmenu‚Äù) starts, and ‚ÄúI g‚Äù
represents that the group is continuing. Separately from the BI tags of the parent
key (i.e., ‚ÄúB g‚Äù and ‚ÄúI g‚Äù), the BI tags of each child key (i.e., ‚ÄúB k‚Äù and ‚ÄúI k‚Äù) work
the same as in the case of n= 1. This BIO-tagging method is also known as
Group BIO-tagging and the details are also available in Hwang et al. [22].
Document VQA With a linear mapping and softmax operation, the output
vector sequence is converted to a span-tag sequence. For the input token se-
quence, the model finds the beginning and the end of the answer span. Details
can also be found in the Section 4.2 of Devlin et al. [8].
