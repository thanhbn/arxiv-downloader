# 2309.01131.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/document-parsing/2309.01131.pdf
# Kích thước tệp: 6200066 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Chú ý đến nơi quan trọng: Suy nghĩ lại về hiểu biết tài liệu thị giác với tập trung vùng chọn lọc

Haoyu Cao1*, Changcun Bao1*, Chaohu Liu1,2†, Huang Chen1, Kun Yin1, Hao Liu1,
Yinsong Liu1, Deqiang Jiang1, Xing Sun1
Tencent YouTu Lab1, Đại học Khoa học và Công nghệ Trung Quốc2
{rechycao, changcunbao, huaangchen, zhanyin, ivanhliu, jasonysliu, dqiangjiang }@tencent.com
liuchaohu@mail.ustc.edu.cn, winfred.sun@gmail.com

Tóm tắt
Chúng tôi đề xuất một mô hình hiểu biết tài liệu đầu cuối mới được gọi là SeRum (Mô hình hiểu biết vùng chọn lọc) để trích xuất thông tin có ý nghĩa từ hình ảnh tài liệu, bao gồm phân tích tài liệu, tìm kiếm và tự động hóa văn phòng. Không giống như các phương pháp tiên tiến hiện tại dựa vào các sơ đồ kỹ thuật đa giai đoạn và tốn kém về mặt tính toán, SeRum chuyển đổi các tác vụ hiểu biết và nhận dạng hình ảnh tài liệu thành một quy trình giải mã cục bộ của các token thị giác quan tâm, sử dụng một mô-đun hợp nhất token nhận thức nội dung. Cơ chế này cho phép mô hình chú ý nhiều hơn đến các vùng quan tâm được tạo ra bởi bộ giải mã truy vấn, cải thiện hiệu quả của mô hình và tăng tốc độ giải mã của sơ đồ sinh tạo. Chúng tôi cũng thiết kế một số tác vụ tiền huấn luyện để tăng cường khả năng hiểu biết và nhận thức cục bộ của mô hình. Kết quả thực nghiệm cho thấy SeRum đạt được hiệu suất tiên tiến trên các tác vụ hiểu biết tài liệu và kết quả cạnh tranh trên các tác vụ phát hiện văn bản. SeRum đại diện cho một bước tiến đáng kể hướng tới việc cho phép hiểu biết tài liệu đầu cuối hiệu quả và có hiệu lực.

1. Giới thiệu
Hiểu biết hình ảnh tài liệu là một tác vụ cơ bản liên quan đến việc trích xuất thông tin có ý nghĩa từ chúng, chẳng hạn như trích xuất thông tin tài liệu [51] hoặc trả lời các câu hỏi thị giác liên quan đến tài liệu [34]. Trong thế giới ngày nay, nơi khối lượng tài liệu kỹ thuật số đang tăng theo cấp số nhân, tác vụ này đã trở nên quan trọng hơn nữa trong các ứng dụng khác nhau, bao gồm phân tích tài liệu [6], tìm kiếm tài liệu [33], và tự động hóa quy trình robot văn phòng (RPA) [1].

Các phương pháp tiên tiến hiện tại [15] dựa vào các sơ đồ kỹ thuật đa giai đoạn liên quan đến nhận dạng ký tự quang học

*Đóng góp bằng nhau.†Công việc được thực hiện trong thời gian thực tập tại YouTu Lab.

(a) Phương pháp đa giai đoạn
Mô hình hiểu biết tài liệu
OCR → Thứ tự đọc → Chuỗi hóa
Fax：335-7733

Bộ mã hóa thị giác → Bộ giải mã truy vấn → Token quan tâm
Bộ giải mã văn bản → 335-7733
Hợp nhất token nhận thức nội dung → số fax là gì? → Token đã hợp nhất

(b) Phương pháp đầu cuối được đề xuất của chúng tôi
SeRum

Hình 1. So sánh các luồng xử lý đa giai đoạn và SeRum đầu cuối cho hiểu biết tài liệu thị giác. Phương pháp của SeRum đơn giản hóa đường ống bằng cách trực tiếp tạo ra đầu ra văn bản cho các token thị giác chính từ hình ảnh, loại bỏ nhu cầu OCR và cung cấp phân tích tài liệu hiệu quả và có hiệu lực cao. Xem tốt nhất trong màu.

(OCR) [42] và các mô-đun khác [37] để trích xuất thông tin chính, như được hiển thị trong Hình 1. Tuy nhiên, các phương pháp này không tối ưu và tốn kém về mặt tính toán, phụ thuộc quá nhiều vào các mô-đun tiền tố như nhận dạng OCR chính xác và sắp xếp nội dung tài liệu [18].

Để giải quyết những thách thức này, chúng tôi đề xuất một mô hình hiểu biết tài liệu đầu cuối mới với tập trung vùng chọn lọc được gọi là SeRum (Mô hình hiểu biết vùng chọn lọc). Như được hiển thị trong Hình 1, SeRum chuyển đổi các tác vụ hiểu biết và nhận dạng hình ảnh tài liệu thành một quy trình giải mã cục bộ của các token thị giác quan tâm, bao gồm một bộ mã hóa thị giác, một bộ giải mã truy vấn-văn bản, và một mô-đun hợp nhất token nhận thức nội dung.

Đối với các tác vụ hiểu biết tài liệu, nội dung cần trích xuất thường chiếm một tỷ lệ nhỏ của toàn bộ khu vực tài liệu nhưng có thể thay đổi rất lớn về quy mô. Do đó, việc xác định chính xác khu vực quan tâm chính trước tiên là rất quan trọng. SeRum trích xuất các tính năng hình ảnh tài liệu bằng cách sử dụng một bộ mã hóa dựa trên Transformer thị giác. Chúng tôi sử dụng một bộ giải mã truy vấn Transform tự mã hóa lấy cảm hứng từ MaskFormer [5], mà giải mã truy vấn đầu vào (câu hỏi cho các tác vụ) và thực hiện cơ chế chú ý chéo với các tính năng hình ảnh để tạo thành các nhúng của các truy vấn. Sau đó, chúng tôi thu được mặt nạ khu vực quan tâm thông qua tích vô hướng với các tính năng hình ảnh được nâng cấp mẫu. Vì số lượng truy vấn lớn hơn số lượng vị trí văn bản cần thiết, chúng tôi sử dụng ghép cặp nhị phân để ghép đôi, theo DETR [4].

--- TRANG 2 ---
Chuỗi đầu ra cuối cùng được tự động tạo ra bởi bộ giải mã văn bản thông qua chú ý chéo với token thị giác được mã hóa. Tuy nhiên, sự hiện diện của nhiễu trong chuỗi token thị giác dài có thể ảnh hưởng tiêu cực đến quá trình giải mã. Để giải quyết vấn đề này, chúng tôi đề xuất một cơ chế hợp nhất token nhận thức nội dung chọn lọc các token thị giác liên quan đến truy vấn trong khi hợp nhất phần còn lại. Cơ chế này hạn chế sự chú ý đến các vùng quan tâm được tạo ra bởi bộ giải mã truy vấn, đồng thời bảo tồn thông tin toàn cục và tăng cường thông tin vùng quan tâm. Cơ chế đa truy vấn được sử dụng trong phương pháp của chúng tôi cho phép tạo ra văn bản cục bộ, do đó dẫn đến nội dung văn bản ngắn hơn và chính xác hơn.

Để tăng cường hơn nữa khả năng hiểu biết và nhận thức cục bộ của mô hình, chúng tôi thiết kế ba tác vụ tiền huấn luyện, bao gồm truy vấn đến phân đoạn, văn bản đến phân đoạn và phân đoạn đến văn bản. Tóm lại, chúng tôi đề xuất một mô hình hiểu biết tài liệu đầu cuối mới được gọi là SeRum cải thiện khả năng nhận dạng của các mô hình đầu cuối trong khi đạt được kết quả cạnh tranh trong nhận dạng từ. Cơ chế hợp nhất token nhận thức nội dung của chúng tôi giới hạn sự chú ý của bộ giải mã đến các chi tiết cục bộ quan tâm, cải thiện hiệu ứng của mô hình và tăng tốc độ giải mã của sơ đồ sinh tạo. Chúng tôi tin rằng mô hình SeRum cung cấp một bước có giá trị hướng tới hiểu biết tài liệu đầu cuối hiệu quả và có hiệu lực, với các ứng dụng tiềm năng trong nhiều lĩnh vực khác nhau như phân tích tài liệu tự động, trích xuất thông tin, nhận dạng văn bản, v.v. Kết luận, các đóng góp của chúng tôi được tóm tắt thành ba khía cạnh:

• Chúng tôi đề xuất một mô hình hiểu biết tài liệu đầu cuối mới được gọi là SeRum, chuyển đổi các tác vụ hiểu biết và nhận dạng hình ảnh tài liệu thành một quy trình giải mã cục bộ của các token thị giác quan tâm.

• Chúng tôi giới thiệu một cơ chế hợp nhất token nhận thức nội dung cải thiện khả năng nhận thức chi tiết hình ảnh của mô hình và tăng tốc độ giải mã của sơ đồ sinh tạo.

• Kết quả thực nghiệm trên nhiều bộ dữ liệu công cộng cho thấy phương pháp của chúng tôi đạt được hiệu suất tiên tiến trên các tác vụ hiểu biết tài liệu và kết quả cạnh tranh trên các tác vụ phát hiện văn bản.

2. Công trình liên quan
Hiểu biết hình ảnh tài liệu đã là một chủ đề quan trọng trong thị giác máy tính trong nhiều năm và đã thu hút sự chú ý đáng kể từ cộng đồng nghiên cứu [6, 25]. Các phương pháp dựa trên quy tắc truyền thống [12, 40] và các kỹ thuật dựa trên học máy [32, 39] thường liên quan đến việc lựa chọn tính năng thủ công, có thể không đủ khi xử lý các tài liệu bố cục phức tạp. Với sự ra đời của công nghệ học sâu, các phương pháp trích xuất thông tin tài liệu đã chứng kiến những cải tiến đáng chú ý về cả hiệu quả và độ bền. Trong những năm gần đây, các phương pháp dựa trên học sâu cho hiểu biết tài liệu có thể được phân loại thành ba nhóm chính: phương pháp phụ thuộc OCR với hậu xử lý, phương pháp phụ thuộc OCR không có hậu xử lý, và phương pháp đầu cuối không có OCR.

Các phương pháp phụ thuộc OCR với hậu xử lý liên quan đến việc trích xuất thông tin văn bản từ hình ảnh tài liệu bằng cách sử dụng các công cụ OCR và các mô-đun phụ trợ khác. Nói chung, công cụ OCR trước tiên phát hiện và nhận dạng nội dung văn bản của tài liệu, sau đó sắp xếp văn bản theo thứ tự đọc, và cuối cùng đánh dấu từng từ theo cách chú thích chuỗi thông qua mô hình hiểu biết tài liệu. Theo cách này, mô hình hiểu biết tài liệu tập trung vào biểu diễn nội dung tài liệu và nhiều công trình quan trọng đã xuất hiện trong lĩnh vực này. Một ví dụ như vậy là họ LayoutLM [45, 47, 46, 15], sử dụng mạng đa phương thức để tích hợp các tính năng thị giác và văn bản. GCN (Mạng tích chập đồ thị) là một kỹ thuật khác được sử dụng để mô hình hóa mối quan hệ phức tạp giữa văn bản và hình ảnh [37, 26, 44, 48, 30, 31]. Ngoài ra, Chargrid [21, 8] tận dụng thông tin bố cục để trích xuất mối quan hệ thực thể.

Các phương pháp phụ thuộc OCR không có hậu xử lý được thiết kế để giải quyết vấn đề lỗi chuỗi hóa token trong hậu xử lý và giảm thiểu lỗi OCR ở một mức độ nhất định. Ví dụ, GMN [3] sử dụng một bộ mã hóa không gian có cấu trúc tối ưu và một mô-đun che mặt chú ý phương thức để giải quyết các tài liệu có cấu trúc phức tạp khó chuỗi hóa. QGN [2] sử dụng một sơ đồ sinh tạo dựa trên truy vấn để tăng cường quá trình sinh tạo ngay cả với nhiễu OCR. Các công trình khác như [38, 36, 41] mã hóa thông tin tài liệu được rút ra như một chuỗi token theo định dạng XML và xuất ra các thẻ XML phân định các loại thông tin.

Các phương pháp đầu cuối không có OCR nhằm tiếp tục loại bỏ sự phụ thuộc vào mô-đun nhận dạng OCR, thể hiện tốc độ lý luận nhanh hơn và cần ít tham số hơn. Các phương pháp này gần đây đã được chú ý do khả năng đạt được hiệu quả và hiệu lực cao hơn. Ví dụ, Donut [22] và Dessurt [7] sử dụng Swin Transformer [29] như một bộ mã hóa để trích xuất tính năng hình ảnh, và Transformer giống BART [24] như một bộ giải mã để sinh ra chuỗi văn bản.

Trong khi các giải pháp trên đã giải quyết sự phụ thuộc hiểu biết tài liệu vào OCR, tất cả đều là mã hóa đồ thị đầy đủ và giải mã toàn cục. Do đó, chúng có thể không

--- TRANG 3 ---
Bộ giải mã truy vấn
Q1: VQA: ngày nộp là gì?
Q2: IE: số Fax
Qn: Phát hiện văn bản: truy vấn1…n

Nâng cấp mẫu
Xương sống
MLP ×

Bộ giải mã văn bản
Bộ giải mã văn bản
Bộ giải mã văn bản

A1: DEC13
A2: 335-7733
A3: NGÀYNỘP

ℱ𝜺𝒑𝒊𝒙𝒆𝒍
𝓠𝜺𝒔𝒄𝒐𝒓𝒆

Hợp nhất token nhận thức nội dung
Bộ mã hóa thị giác
𝜺𝒎𝒂𝒔𝒌

Hình 2. Kiến trúc của SeRum được đề xuất. Cho một hình ảnh đầu vào và truy vấn, bộ mã hóa thị giác và bộ giải mã truy vấn mã hóa hình ảnh và trích xuất mặt nạ vùng quan tâm, tương ứng. Mặt nạ kết quả sau đó được truyền qua mô-đun hợp nhất token nhận thức nội dung, lọc ra các token thị giác Top-K quan tâm và hợp nhất những token còn lại. Cuối cùng, các token đã hợp nhất được truyền qua bộ giải mã văn bản để thu được kết quả hiểu biết tài liệu. Xem tốt nhất trong màu.

phù hợp với các tình huống mà lỗi nhận dạng OCR xảy ra do không thể chú ý đến các chi tiết cục bộ của tài liệu, dẫn đến hiệu quả sinh tạo thấp. Mô hình SeRum được đề xuất trong công trình này cung cấp một bước có giá trị hướng tới hiểu biết tài liệu đầu cuối hiệu quả và có hiệu lực và phục vụ như một cầu nối cho nhận dạng OCR và hiểu biết tài liệu.

3. Phương pháp

3.1. Kiến trúc tổng thể
SeRum là một kiến trúc đầu cuối được thiết kế để giải quyết các tác vụ hiểu biết tài liệu bằng cách tận dụng một quy trình giải mã các vùng thị giác quan tâm. Như được hiển thị trong Hình 2, nó bao gồm ba thành phần chính: bộ mã hóa thị giác, bộ giải mã truy vấn-văn bản, và các mô-đun hợp nhất token nhận thức nội dung. Bộ mã hóa thị giác sử dụng một xương sống transformer thị giác để trích xuất các tính năng hình ảnh F sau đó được nâng cấp mẫu để tạo thành các nhúng tính năng độ phân giải cao Epixel. Bộ giải mã truy vấn là một bộ giải mã transformer mã hóa các truy vấn đầu vào và chú ý đến các tính năng hình ảnh. Nó tạo ra N nhúng mỗi phân đoạn Q độc lập tạo ra N dự đoán lớp với N nhúng mặt nạ tương ứng Emask. Mô hình sau đó dự đoán N dự đoán mặt nạ nhị phân có thể chồng chéo Escore thông qua tích vô hướng giữa các nhúng thị giác Epixel và nhúng mặt nạ Emask, theo sau bởi kích hoạt sigmoid. Mô-đun hợp nhất token nhận thức nội dung hợp nhất các token không liên quan, và thông qua bộ giải mã văn bản, tạo ra kết quả token văn bản cuối cùng. Bằng cách sử dụng bộ giải mã transformer để mã hóa truy vấn đầu vào, mô hình có thể lý luận hiệu quả về nội dung ngữ nghĩa của tài liệu và tạo ra các dự đoán thông tin. Các mô-đun hợp nhất token nhận thức nội dung tận dụng sự chú ý của mô hình để kết hợp các token liên quan và cải thiện độ chính xác của các dự đoán cuối cùng.

3.2. Bộ mã hóa thị giác
Mô-đun bộ mã hóa thị giác đóng vai trò quan trọng trong việc chuyển đổi hình ảnh tài liệu đầu vào x∈RH×W×C thành bản đồ tính năng F ∈Rh×w×d có thể được xử lý bởi các mô-đun tiếp theo. Bản đồ tính năng được chuỗi hóa tiếp thành một tập hợp các nhúng
zi|zi∈Rd,1≤i≤n	
trong đó n đại diện cho kích thước bản đồ tính năng hoặc số lượng vá hình ảnh, và d là chiều của các vector tiềm ẩn của bộ mã hóa. Các mô hình dựa trên CNN [13] hoặc các mô hình dựa trên Transformer như Swin Transformer [29] có thể được sử dụng như mạng mã hóa. Đối với nghiên cứu này, chúng tôi sử dụng Swin Transformer với các sửa đổi vì nó thể hiện hiệu suất vượt trội trong nghiên cứu sơ bộ của chúng tôi về phân tích tài liệu. Swin Transformer chia hình ảnh đầu vào x thành các vá không chồng chéo, và áp dụng các khối Swin Transformer gồm một mô-đun chú ý đa đầu dựa trên cửa sổ dịch chuyển và MLP hai lớp cho các vá này. Các lớp hợp nhất vá được áp dụng tiếp theo cho các token vá ở mỗi giai đoạn, và đầu ra của khối Swin Transformer cuối cùng z được đưa vào bộ giải mã truy vấn-văn bản. Chúng tôi cũng nâng cấp mẫu bản đồ tính năng lên kích thước lớn hơn, Epixel∈Rsh×sw×d để thu nhận vùng mặt nạ tinh vi, trong đó s là hệ số nâng cấp mẫu và chúng tôi đặt s= 6 làm mặc định. Ngoài kiến trúc Swin Transformer, chúng tôi giới thiệu một nhúng vị trí có thể học được trong lớp cuối cùng của mô-đun bộ mã hóa thị giác, giúp tăng cường khả năng nhận thức vị trí của mô hình. Nhúng vị trí được thêm vào bản đồ tính năng đã nâng cấp mẫu: E′pixel=Epixel+P, trong đó E′pixel là bản đồ tính năng đã nâng cấp mẫu với nhúng vị trí, Epixel là bản đồ tính năng đã nâng cấp mẫu gốc, và P là nhúng vị trí có thể học được.

3.3. Bộ giải mã truy vấn-văn bản
Bộ giải mã truy vấn-văn bản là một mô-đun thiết yếu trong lĩnh vực phân tích ngữ cảnh thị giác. Nó bao gồm hai mô-đun con, cụ thể là bộ giải mã truy vấn và bộ giải mã văn bản, hoạt động với các trọng số được chia sẻ.

Bộ giải mã truy vấn sử dụng Transformer tiêu chuẩn [43] để tính toán các nhúng mỗi phân đoạn Q ∈ RCQ×N, Emask∈RN×d mã hóa thông tin toàn cục về từng dự đoán phân đoạn từ các tính năng hình ảnh F và N nhúng token có thể học được hoặc câu hỏi. Theo phương pháp của [5], bộ giải mã tạo ra tất cả các dự đoán đồng thời.

Bộ giải mã văn bản được thiết kế để giải mã các chuỗi token văn bản tương ứng cho nhiều tác vụ khác nhau, bao gồm Trả lời câu hỏi thị giác (VQA), trích xuất thông tin và phát hiện văn bản. Nó sinh ra văn bản theo cách tự hồi quy bằng cách chú ý đến nhúng truy vấn, các token được sinh ra trước đó, và các tính năng thị giác được mã hóa. Dạng toán học của bộ giải mã văn bản như sau:

ht= Decoder ( ht−1,F,Q,Tt−1) (1)

Ở đây, ht đại diện cho trạng thái ẩn của bộ giải mã tại thời điểm t, Q đại diện cho nhúng truy vấn, Tt−1 đại diện cho nhúng token trước đó, và F đại diện cho bản đồ tính năng thị giác. Ngược lại, bộ giải mã truy vấn là một dạng tự mã hóa, trong đó mỗi truy vấn chú ý đến tất cả các truy vấn khác. Dạng toán học như sau:

hi= Decoder ( F,Qi,Qj|j̸=i) (2)

Ở đây, hi đại diện cho trạng thái ẩn của bộ giải mã cho truy vấn i, và Qj|j̸=i là tập hợp tất cả các truy vấn khác.

3.4. Hợp nhất token nhận thức nội dung
Các tác vụ hiểu biết thị giác tài liệu khác với các tác vụ thị giác thông thường ở chỗ một lượng thông tin thường tập trung trong một khu vực rất nhỏ, thường bao gồm ít hơn 5% của toàn bộ hình ảnh. Do đó, các token thị giác được trích xuất bởi mô-đun thị giác chứa một số lượng lớn các khu vực nền, làm tăng khó khăn giải mã và giảm tốc độ giải mã. Để giải quyết vấn đề này, chúng tôi giới thiệu một mô-đun gọi là hợp nhất token nhận thức nội dung tập trung động vào phần nền quan trọng hơn của các token thị giác và giảm sự tập trung vào phần nền của các token.

Khu vực nền trước: Như đã đề cập ở trên, mối tương quan giữa truy vấn và token thị giác có thể được biểu diễn bởi Escore∈RN×sh×sw. Điểm số càng cao, thông tin tương quan của token thị giác đại diện cho vị trí càng mạnh. Chúng tôi sắp xếp Escore và chọn các tính năng tương ứng với các token top-K có điểm số cao để tạo thành một chuỗi khu vực nền trước cho mỗi truy vấn. Elscore biểu thị điểm số trung bình liên quan đến token l.

Ff= zl∗ Elscore|l= 1, . . . , K (3)

Khu vực nền sau: Các tính năng khu vực nền trước đại diện cho các tính năng thị giác có ưu thế tuyệt đối, trong khi phần còn lại của các tính năng được coi là tính năng khu vực nền sau. Các tính năng này thường là tính năng toàn cục hoặc tính năng được sử dụng để hỗ trợ ngoại suy. Để bảo tồn phần tính năng này và tránh can thiệp với các tính năng nền trước do quá nhiều tính năng của phần này, trong trường hợp của chúng tôi, chúng tôi chiếu các token nền sau sử dụng cơ chế chú ý cơ bản lên các token nền trước, dẫn đến một biểu diễn tính năng K chiều nắm bắt thông tin quan trọng từ cả token nền trước và nền sau. Công thức chính thức cho cơ chế chú ý như sau:

Fb=softmax (Qf⊺r)(Wvf⊺r)⊺(4)

trong đó Fb∈RK×d là các tính năng nền sau được hợp nhất, Q∈RK×d là các tính năng truy vấn nền trước, fr∈R(L−K)×d là các tính năng nền sau nguyên thủy, và Wv∈Rd×d là trọng số chiếu. Hàm softmax được sử dụng để chuẩn hóa tích vô hướng của các ma trận truy vấn và khóa. Tiếp theo, các trọng số kết quả được áp dụng cho ma trận giá trị để rút ra đầu ra. Quá trình này tạo ra một vector tính năng mới Fb, bao gồm thông tin ngữ cảnh nền sau. Ngữ cảnh thị giác được biểu diễn bởi F=Ff+Fb.

Để thích ứng với các thay đổi kích thước khác nhau, chúng tôi định nghĩa số lượng token nền trước, được ký hiệu bởi K, như một hàm của tổng số token, được biểu diễn bởi L, tức là K=αL. Trong quá trình huấn luyện, chúng tôi lấy mẫu α từ một phân phối đồng đều trong khoảng từ 0.02 đến 1.0. Cụ thể, α= 1.0 cho biết không có hợp nhất token nào xảy ra. Trong quá trình suy luận, giá trị của α có thể được cố định dựa trên yêu cầu hiệu suất.

3.5. Tiền huấn luyện
Trong nghiên cứu này, chúng tôi sử dụng tiền huấn luyện đa tác vụ để tăng cường khả năng hiểu biết vị trí và tạo ra văn bản của mô hình. Quá trình tiền huấn luyện bao gồm ba tác vụ phụ: truy vấn đến phân đoạn, văn bản đến phân đoạn, và phân đoạn đến văn bản.

Truy vấn đến phân đoạn nhằm trang bị cho mô hình khả năng phát hiện văn bản. Chúng tôi áp dụng phương pháp tạo truy vấn được sử dụng trong DETR [4] nhưng áp dụng nó cho một tác vụ phân đoạn thể hiện thay vì, vì tạo ra văn bản là một tác vụ dự đoán rất dày đặc. Chúng tôi sử dụng bộ mã hóa thị giác và bộ giải mã truy vấn để xử lý một hình ảnh x cho trước với N nhúng token có thể học được, dẫn đến N dự đoán mặt nạ Escore cho khu vực văn bản, và chúng tôi đặt N= 50 làm mặc định.

Văn bản đến phân đoạn giúp mô hình hiểu các vị trí riêng biệt của từng phần tử văn bản, do đó đóng vai trò quan trọng trong việc căn chỉnh văn bản-phân đoạn. Chúng tôi đưa đoạn văn bản của hình ảnh vào bộ giải mã truy vấn và biến điều này thành một tác vụ phân đoạn thể hiện. Vì hình ảnh tài liệu thường chứa tỷ lệ pixel tương đối thấp với văn bản, việc nhấn mạnh hiệu suất phân đoạn vùng văn bản có thể tăng cường chất lượng của đầu ra được tạo ra bởi bộ giải mã.

--- TRANG 4 ---
Phân đoạn đến văn bản nhằm tạo ra văn bản từ các phân đoạn hình ảnh theo cách tương tự như OCR. Trong giai đoạn này, các tính năng nền trước và nền sau F được tạo ra từ các lớp trước được sử dụng như ngữ cảnh thị giác trong cơ chế chú ý chéo của bộ giải mã văn bản, tự hồi quy tạo ra các token.

Điều quan trọng cần lưu ý là ba tác vụ phụ tiền huấn luyện được thực hiện đồng thời và không có thứ tự phân cấp cụ thể. Nói cách khác, quá trình huấn luyện bao gồm dữ liệu từ cả ba tác vụ phụ trong cùng một lô.

3.6. Chiến lược huấn luyện
Hàm mất mát. SeRum được huấn luyện đầu cuối với một hàm mất mát gồm nhiều phần. Phần đầu tiên là mất mát ghép cặp Hungary giữa truy vấn và mục tiêu, được sử dụng để xử lý trường hợp số lượng truy vấn lớn hơn số lượng mục tiêu. Cụ thể, chúng tôi tiến hành ghép cặp vị trí ở mỗi lớp của bộ mã hóa truy vấn và ghép cặp vị trí và danh mục ở lớp cuối cùng. Mất mát cho phần này được định nghĩa là:

Lmatch =NeX
i=1L(i)match(5)

trong đó Ne là số lượng lớp bộ mã hóa truy vấn, và L(i)match là mất mát ghép cặp ở lớp i.

Phần thứ hai của mất mát là mất mát bộ giải mã tự hồi quy cho bộ giải mã văn bản, được định nghĩa là:

Ldecoder =−logP(y1:T|h1:T,e1) (6)

trong đó y1:T là văn bản sự thật nền, h1:T là các trạng thái ẩn của bộ giải mã, và e1 là nhúng của token bắt đầu chuỗi.

Để tăng tốc việc học các khu vực quan tâm, chúng tôi thêm một mất mát ràng buộc văn bản vào các tính năng của hình ảnh được nâng cấp mẫu ở trên, trong đó mặt nạ của ràng buộc mất mát được thay đổi để bao phủ tất cả các khu vực văn bản trong toàn bộ hình ảnh. Mất mát cho phần này được định nghĩa là:

Ltext=1
|Ω|X
(i,j)∈ΩLtext(Ei,j
score) (7)

trong đó Ω là tập hợp tất cả các khu vực văn bản trong hình ảnh, Ltext là mất mát ràng buộc văn bản, và Ei,j
score là mặt nạ phân đoạn tại vị trí (i, j).

Tổng mất mát là tổng có trọng số của các phần mất mát trên:

Ltotal=λ1Lmatch +λ2Ldecoder +λ3Ltext (8)

trong đó λ1, λ2, và λ3 là các siêu tham số kiểm soát trọng số của mỗi phần mất mát.

Chi tiết khác. Trong quá trình tiền huấn luyện, SeRum sử dụng chế độ ghép cặp nhị phân giữa truy vấn và mục tiêu và có giám sát rõ ràng trong mặt nạ khu vực quan tâm. Tuy nhiên, trong quá trình huấn luyện tác vụ hạ lưu, mô hình bị ràng buộc bởi tỷ lệ khu vực quan tâm, dẫn đến chế độ giám sát ngầm.

4. Thí nghiệm
Chúng tôi đánh giá mô hình được đề xuất trên ba tác vụ: trích xuất thông tin tài liệu, trả lời câu hỏi thị giác tài liệu, và phát hiện văn bản. Để đánh giá hiệu suất của mô hình, chúng tôi so sánh nó với các phương pháp trích xuất thông tin tài liệu hai giai đoạn tiên tiến cũng như các phương pháp đầu cuối khác.

4.1. Tác vụ và bộ dữ liệu
Trích xuất thông tin tài liệu (DIE) là một quá trình trích xuất dữ liệu có cấu trúc cặp khóa-giá trị từ tài liệu. Chúng tôi đánh giá hiệu suất của mô hình trên ba bộ dữ liệu chuẩn thường được sử dụng để xác nhận hiệu quả của các mô hình trong DIE.

Bộ dữ liệu vé [10] bao gồm 300.000 hình ảnh tổng hợp và 1.900 hình ảnh thật của vé tàu Trung Quốc. Có tám thực thể, như ga xuất phát, ga cuối, thời gian, giá, v.v., cần được trích xuất.

Bộ dữ liệu CORD [35] là một chuẩn tiếng Anh được sử dụng rộng rãi cho các tác vụ trích xuất thông tin. Nó bao gồm 800 hình ảnh hóa đơn huấn luyện, 100 xác nhận, và 100 thử nghiệm, với 30 lớp phụ riêng biệt để trích xuất, như tên menu, số menu, giá menu, v.v. CORD phức tạp do cấu trúc đa lớp, lồng ghép, yêu cầu mô hình không chỉ trích xuất văn bản liên quan mà còn hiểu sâu hơn về cấu trúc của nó.

Bộ dữ liệu SROIE [16] gồm 973 hình ảnh hóa đơn được quét, chia thành 626 để huấn luyện và 347 để thử nghiệm. Thành phần của nó đơn giản, chỉ bao gồm bốn thực thể chính: công ty, tổng cộng, ngày, và địa chỉ. Tuy nhiên, do nội dung văn bản dày đặc có trong hình ảnh, việc trích xuất thông tin liên quan là phức tạp, vì một số thực thể trải rộng trên nhiều dòng.

Chúng tôi sử dụng hai chỉ số thông thường, điểm F1 [14, 47] và Khoảng cách chỉnh sửa cây (TED) [22, 50], để chứng minh hiệu suất của mô hình trên các bộ dữ liệu. Điểm F1 là trung bình điều hòa của độ chính xác và khả năng nhớ lại của một mô hình phân loại, trong đó độ chính xác đo khả năng của mô hình xác định chính xác các trường hợp tích cực, và khả năng nhớ lại đo khả năng của mô hình xác định tất cả các trường hợp tích cực. Điểm F1 cao cho thấy mô hình có độ chính xác và độ chính xác cao trong việc phân loại các trường hợp tích cực. Mặc dù điểm F1 rất trực quan, nó tương đối nghiêm ngặt và không thể phản ánh chính xác độ chính xác dự đoán ở cấp độ ký tự. TED là số lượng tối thiểu các thao tác chỉnh sửa ký tự đơn cần thiết để chuyển đổi một chuỗi thành chuỗi khác. Chúng tôi tính điểm của nó thông qua max(0,1−TED (pr, gt )/TED (ϕ, gt)), 

--- TRANG 5 ---
trong đó gt, pr, và ϕ đại diện cho sự thật nền, được dự đoán, và chuỗi rỗng.

DocVQA là một tác vụ kết hợp hiểu biết tài liệu và trả lời câu hỏi thị giác. Mục tiêu là trả lời một câu hỏi về một hình ảnh tài liệu cho trước. Chúng tôi đánh giá mô hình của chúng tôi trên bộ dữ liệu DocVQA [34], chứa 12.767 hình ảnh tài liệu và 99.000 câu hỏi tổng cộng. Bộ dữ liệu được chia thành các tập huấn luyện, xác nhận, và thử nghiệm, với 80%, 10%, và 10% dữ liệu trong mỗi tập, tương ứng. Các câu hỏi thuộc nhiều loại khác nhau, như có/không, đếm, lý luận, và so sánh, và yêu cầu cả hiểu biết văn bản và lý luận thị giác. Chỉ số ANLS (Độ tương tự Levenshtein chuẩn hóa trung bình), là một chỉ số dựa trên khoảng cách chỉnh sửa, được sử dụng để đánh giá.

4.2. Chi tiết triển khai
Kiến trúc mô hình. Mô hình được đề xuất của chúng tôi, SeRum, sử dụng Swin-B [29] làm bộ mã hóa thị giác với những sửa đổi nhỏ. Cụ thể, chúng tôi đặt số lượng lớp và kích thước cửa sổ là 2, 2, 14, 2 và 10 để trích xuất tính năng từ hình ảnh đầu vào. Hơn nữa, độ phân giải hình ảnh đầu vào được đặt thành 1280×960. Để cân bằng sự đánh đổi giữa tốc độ và độ chính xác, chúng tôi sử dụng bốn lớp đầu tiên của mBART làm bộ giải mã để tạo ra văn bản đầu ra.

Tiền huấn luyện. Để cải thiện hiệu suất của mô hình, chúng tôi tiền huấn luyện SeRum trên một bộ dữ liệu tổng hợp quy mô lớn được tạo ra bằng cách sử dụng Nhận dạng văn bản tổng hợp (SynthText) [11], Synth90K [20], IIT-CDIP [23] và bộ dữ liệu tổng hợp đa ngôn ngữ theo Donut. Bộ dữ liệu SynthText được sử dụng để tạo ra các thể hiện văn bản với nền phức tạp, trong khi bộ dữ liệu Synth90K được sử dụng cho các thể hiện văn bản với nền đơn giản. Bộ dữ liệu IIT-CDIP bao gồm hơn 11 triệu hình ảnh quét của tài liệu tiếng Anh. Chúng tôi sử dụng chiến lược tiền huấn luyện đa tác vụ, như được mô tả trong Mục 3.5.

Tinh chỉnh. Đối với giai đoạn tinh chỉnh, chúng tôi sử dụng trình tối ưu hóa Adam với tỷ lệ học ban đầu là 5×10−5 cho tất cả các bộ dữ liệu, và kết hợp hệ số phân rã tỷ lệ học là 0.1 sau mỗi 30 epoch để tăng cường hiệu suất của mô hình. Đối với các tác vụ trích xuất thông tin, chúng tôi đặt kích thước lô thành 24, trong khi đối với tác vụ DocVQA, chúng tôi đặt thành 8. Để đảm bảo hiệu suất tối ưu và hội tụ, chúng tôi huấn luyện mô hình tối đa 300 epoch. Ngoài ra, chúng tôi sử dụng hai cơ chế tạo ra, SeRum-total và SeRum-prompt, cho các tác vụ trích xuất thông tin. SeRum-total tạo ra một chuỗi token hoàn chỉnh của tất cả thông tin chính sử dụng định dạng được xác định trước, như Donut, với tên tác vụ như truy vấn. SeRum-Prompt sử dụng các khóa như truy vấn và tạo ra từng thông tin song song. Chúng tôi đặt tỷ lệ giữ token α thành 0.5 cho cơ chế tạo ra SeRum-total và α thành 0.1 cho cơ chế tạo ra SeRum-Prompt trong giai đoạn thử nghiệm. Cuối cùng, để cân bằng trọng số hàm mất mát, chúng tôi đặt λ1=λ2=λ3.

4.3. So sánh với các phương pháp trước đó
Trích xuất thông tin tài liệu. Chúng tôi trình bày một đánh giá so sánh của phương pháp được đề xuất với một số phương pháp tiên tiến được báo cáo trong những năm gần đây trên ba bộ dữ liệu được sử dụng rộng rãi, cụ thể là Ticket, CORD, và SROIE. Sự so sánh của chúng tôi bao gồm các phương pháp sử dụng OCR cũng như các phương pháp đầu cuối hoàn toàn. Trong phương pháp trước, OCR được sử dụng để trích xuất văn bản và vị trí của hình ảnh, và thông tin được sắp xếp và phân loại ở cấp độ token, sau đó thông tin mục tiêu được xác định dựa trên kết quả phân loại. Các phương pháp như BERT [18], LayoutLMv2 [47], và SPADE [19] có thể tạo ra kết quả thỏa mãn khi kết quả OCR hoàn toàn chính xác. Chúng tôi sử dụng API công cụ OCR được báo cáo trong Donut, bao gồm MS OCR và các công cụ khác, để đảm bảo tính nhất quán. Ngoài ra, chúng tôi đánh giá một phương pháp sinh tạo được gọi là WYVERN [17] sử dụng kiến trúc mã hóa và giải mã của Transformer và yêu cầu OCR. Các phương pháp đầu cuối, có đường ống được tối ưu hóa, ngày càng phổ biến trong cả ngành công nghiệp và học thuật. Donut [22] và Dessurt [7] là hai phương pháp đầu cuối nổi bật, và chúng tôi chỉ nghiên cứu kết quả của Donut trong bài báo này do hiệu suất kém của Dessurt.

Phương pháp được đề xuất của chúng tôi đạt được kết quả tiên tiến mới trên ba chuẩn mở của trích xuất thông tin tài liệu, như được hiển thị trong Bảng 1. Mô hình của chúng tôi xuất sắc trong bộ dữ liệu Ticket, đạt được điểm số trên 99% và chứng minh thành công gần như hoàn toàn trong việc giải quyết tác vụ này, vượt trội hơn phương pháp đầu cuối tốt thứ hai Donut 5%. Hơn nữa, mô hình của chúng tôi thể hiện khả năng nhận dạng ký tự mạnh mẽ và hiểu biết ngữ cảnh, như được hiển thị trong Hình 3. Đáng chú ý, SeRum là một phương pháp đầu cuối không yêu cầu mô-đun OCR, làm cho nó hiệu quả hơn cho huấn luyện và suy luận, và do đó phù hợp cho các ứng dụng công nghiệp. Đặc biệt, trên bộ dữ liệu SROIE, điểm F1 của SeRum vượt quá phương pháp đa giai đoạn LayoutLMv2 24%.

DocVQA. Chúng tôi tiến hành đánh giá mô hình SeRum được đề xuất trên bộ dữ liệu DocVQA đầy thách thức và so sánh hiệu suất của nó với một số phương pháp tiên tiến. Kết quả thực nghiệm được tóm tắt trong Bảng 1. Mô hình SeRum của chúng tôi chứng minh hiệu suất vượt trội so với baseline mạnh của các phương pháp đầu cuối và đạt được kết quả cạnh tranh với các phương pháp đa giai đoạn. Đặc biệt, trong bộ dữ liệu ANLS* với văn bản viết tay, các phương pháp đầu cuối thể hiện ưu thế so với các phương pháp đa giai đoạn do khả năng tối ưu hóa cùng lúc phát hiện và nhận dạng văn bản. Chúng tôi tiến hành các nghiên cứu sâu hơn về đặc tính này trong phần thảo luận sâu hơn và phụ lục.

4.4. Nghiên cứu khử bỏ
Trong các thí nghiệm khử bỏ, chúng tôi tiến hành phân tích kỹ lưỡng về hiệu quả của từng đóng góp của chúng tôi, bao gồm tiền huấn luyện, tạo ra gợi ý, và cơ chế hợp nhất token nhận thức nội dung

--- TRANG 6 ---
CORD [35] | Ticket [10] | SROIE [16] | DocVQA [34]
OCR | #Params | F1 | Acc. | F1 | Acc. | F1 | Acc. | ANLS | ANLS∗
SPADE [19] | ✓ | 93M+α‡ | 74.0 | 75.8 | 14.9 | 29.4 | - | - | - | -
WYVERN [17] | ✓ | 106M+α‡ | 43.3 | 46.9 | 41.8 | 54.8 | - | - | - | -
BERT [18] | ✓ | 86M+α‡ | 73.0 | 65.5 | 74.3 | 82.4 | - | - | 63.5 | -
LayoutLMv2 [47] | ✓ | 179M+α‡ | 78.9 | 82.4 | 87.2 | 90.1 | 61.0 | 91.1 | 78.1 | 67.3
Donut [22] | | 143M | 84.1 | 90.9 | 94.1 | 98.7 | 83.2 | 92.8 | 67.5 | 72.1
SeRum-total | | 136M | 80.5 | 85.8 | 97.9 | 99.6 | 85.6 | 92.8 | - | -
SeRum-prompt | | 136M | 84.9 | 91.5 | 99.2 | 99.8 | 85.8 | 95.4 | 71.9 | 77.9

Bảng 1. Hiệu suất trên các tác vụ hiểu biết tài liệu khác nhau. Điểm F1 cấp độ trường và độ chính xác dựa trên khoảng cách chỉnh sửa cây được báo cáo. SeRum-prompt cho thấy cạnh tranh trong các tác vụ khác nhau. Tham số cho từ vựng được bỏ qua để so sánh công bằng giữa các mô hình đa ngôn ngữ. ‡# tham số đại diện cho việc cần sử dụng OCR.

phương pháp | F1
truy vấn đến phân đoạn | 59.3
+ văn bản đến phân đoạn | 82.5
+ phân đoạn đến văn bản | 85.8

Bảng 2. Hiệu ứng của phương pháp tiền huấn luyện trên bộ dữ liệu SROIE.

mô-đun hợp nhất token.

Ảnh hưởng của tiền huấn luyện. Vai trò của tiền huấn luyện trong việc tăng cường hiệu suất của các mô hình quy mô lớn được công nhận rộng rãi trong cộng đồng nghiên cứu. Trong bài báo này, chúng tôi điều tra hiệu ứng của ba tác vụ tiền huấn luyện trên hiệu suất của mô hình trong bối cảnh bộ dữ liệu SROIE. Chúng tôi trình bày kết quả thực nghiệm trong Bảng 2, chứng minh hiệu quả của tiền huấn luyện trong việc tăng cường hiệu suất trích xuất thông tin của mô hình.

Ảnh hưởng của cách thức tạo ra. Như một mô hình sinh tạo đầu cuối, SeRum cho phép trích xuất hoàn toàn thông tin thiết yếu bằng cách tạo ra một chuỗi dây chứa tất cả các khóa liên quan, được gọi là SeRum-total. Để đạt được điều này, chúng tôi sử dụng phương pháp giống Donut [22] để chuỗi hóa một chuỗi kiểu json, với mỗi cặp khóa-giá trị được biểu diễn như <skey>value<ekey>, trong đó cả <skey> và <ekey> đều được thêm vào tokenizer như các token đặc biệt. Json hoàn chỉnh được mã hóa thành một chuỗi duy nhất, sau đó được giải mã để trích xuất tất cả thông tin cần thiết trong một lần.

Bảng 1 hiển thị sự khác biệt hiệu suất giữa SeRum-total và SeRum-prompt trong tác vụ trích xuất thông tin. Kết quả của chúng tôi cho thấy phương pháp trích xuất một lần của SeRum-total cho thấy các mức độ suy giảm khác nhau so với phương pháp dựa trên gợi ý, đặc biệt trên bộ dữ liệu CORD. Sự suy giảm này có thể được quy cho việc tiền huấn luyện của chúng tôi là một tác vụ pseudo-OCR, hạn chế khả năng của mô hình hiểu các cấu trúc phức tạp. Tuy nhiên, việc phân tích bộ dữ liệu CORD thành truy vấn từ điển dẫn đến sự tăng cường đáng kể hiệu suất của mô hình. Hơn nữa, sự cải thiện đáng kể được quan sát trong các bộ dữ liệu Ticket và SROIE tiếp tục xác nhận hiệu

Tỷ lệ giữ token α | F1 | Độ trễ bộ giải mã văn bản(ms)
2% | 72.5 | 194
5% | 83.2 | 198
10% | 85.8 | 209
20% | 84.8 | 225
30% | 84.9 | 231
50% | 84.9 | 234
100% | 84.9 | 306

Bảng 3. Hiệu ứng của tỷ lệ giữ token trên bộ dữ liệu SROIE. Độ trễ đo trên GPU T4.

quả của tạo ra gợi ý.

Ảnh hưởng của hợp nhất token nhận thức nội dung. Bảng 3 trình bày ảnh hưởng của cơ chế hợp nhất token nhận thức nội dung trên độ chính xác nhận dạng và tốc độ giải mã của SeRum. Chúng tôi quan sát thấy với tỷ lệ giữ token tăng lên, độ chính xác nhận dạng của SeRum cải thiện dần dần, đạt đỉnh ở tỷ lệ hợp nhất 10%. Tuy nhiên, vượt quá tỷ lệ này gây suy giảm độ chính xác nhận dạng. Về tốc độ giải mã, khi tỷ lệ giữ token giảm, tốc độ giải mã của SeRum cải thiện dần dần. Những phát hiện này cho thấy cơ chế hợp nhất token nhận thức nội dung trong SeRum có khả năng tăng cường cả độ chính xác nhận dạng và tốc độ giải mã. Hơn nữa, tỷ lệ có thể được điều chỉnh động để đạt được tốc độ khác nhau trong quá trình suy luận.

4.5. Thảo luận sâu hơn
Nghiên cứu trường hợp. Để điều tra sâu hơn hiệu quả và các vấn đề của sơ đồ, chúng tôi tiến hành phân tích thêm về sự khác biệt với phương pháp SOTA trên nhiều bộ dữ liệu, cũng như độ chính xác của mặt nạ vùng được chọn. Như được hiển thị trong Hình 3, lợi ích của mô hình SeRum là độ chính xác nhận dạng từ cao hơn bằng cách tập trung vào các chi tiết liên quan quan trọng. Đặc biệt trong một số cảnh gây nhầm lẫn, thường tốt hơn khi kết hợp thông tin ngữ cảnh của văn bản. Xem phụ lục để biết thêm chi tiết và trường hợp.

Khái quát hóa cho các tác vụ khác. Khả năng khái quát hóa của SeRum cho các tác vụ khác được đánh giá bằng cách thử nghiệm hiệu suất của nó trên phát hiện văn bản, như được minh họa trong Hình 4. Phát hiện văn bản liên quan đến việc phát hiện và nhận dạng văn bản trong hình ảnh tự nhiên, như cảnh đường phố hoặc biển báo. Trong đánh giá của chúng tôi, chúng tôi sử dụng bộ dữ liệu CTW-1500 [49] được sử dụng rộng rãi, và kết quả thực nghiệm được tóm tắt trong Bảng 4. Kết quả thực nghiệm cho thấy SeRum đạt được hiệu suất cạnh tranh trên nhiều tác vụ, nhấn mạnh hiệu quả của nó trong các ứng dụng đa dạng. Chúng tôi gợi ý rằng phương pháp này có thể hứa hẹn cho nhiều tác vụ hiểu biết tài liệu khác nhau.

phương pháp | F1
TextDragon [9] | 39.7
ABCNet [27] | 45.2
SPTS v2 [28] | 63.6
SeRum | 41.8

Bảng 4. Hiệu suất trên bộ dữ liệu phát hiện văn bản CTW-1500.

5. Kết luận
Tóm lại, chúng tôi đã trình bày một phương pháp dựa trên truy vấn mới cho hiểu biết tài liệu dựa trên hình ảnh không dựa vào OCR, mà thay vào đó sử dụng một bộ mã hóa thị giác và một bộ giải mã văn bản. Phương pháp của chúng tôi mã hóa gợi ý và trích xuất các vùng token liên quan thông qua cơ chế hợp nhất token nhận thức ngữ cảnh giữa gợi ý và tính năng hình ảnh, và với một bộ giải mã để tạo ra kết quả hiểu biết tài liệu cục bộ.

Phương pháp của chúng tôi đạt được hiệu suất cạnh tranh trên một số bộ dữ liệu công cộng cho trích xuất thông tin tài liệu và phát hiện văn bản, và gần như có thể so sánh với giải pháp hai giai đoạn trong khi hiệu quả hơn và cho phép tạo ra song song dựa trên gợi ý. Mô hình được đề xuất có tiềm năng được áp dụng trong các tình huống thực tế nơi các phương pháp dựa trên OCR truyền thống không khả thi hoặc hiệu quả.

--- TRANG 7 ---
SROIE:ngày | CORD:menu.nm | Ticket:gaxuatphat | DocVQA:Tiêu đề của tài liệu là gì?

Hình ảnh
Mặt nạ văn bản
Mặt nạ vùng chọn lọc
Kết quả của SeRum | 05/02/2018 | TICKET CP | 肇庆东站 | FINANCIAL HIGHLIGHTS

Hình 3. Trực quan hóa của SeRum trên các bộ dữ liệu khác nhau. Hình mô tả năm hàng từ trên xuống dưới, thể hiện tác vụ và truy vấn, hình ảnh gốc, vùng văn bản của mô hình tiền huấn luyện, vùng quan tâm được chọn bởi truy vấn, và kết quả văn bản tương ứng mà SeRum đã trích xuất. Kết quả cho thấy SeRum hiệu quả nắm bắt các vùng quan tâm và giải mã chúng một cách chính xác. Xem tốt nhất trong màu.

ARTHUR KAY xBR0AS ADA VERTISED
WEDDING RINGS CHARGED ONLY BY WEIGHT
ILKLEY CIVIC SOCIETY CRESCENT HOTEL
Opened in 1861 as a commercial hotel with 30 bedrooms and stabling for 20 horses

Hình 4. Kết quả phát hiện văn bản. SeRum chứng minh lợi thế đáng kể trong việc xử lý các tài liệu có định dạng phức tạp, bao gồm WordArt, văn bản cong, v.v. Xem tốt nhất trong màu.

--- TRANG 8 ---
Tài liệu tham khảo

[1] Bernhard Axmann và Harmoko Harmoko. Robotic process automation: An overview and comparison to other technology in industry 4.0. Trong 10th International Conference on Advanced Computer Information Technologies, ACIT 2020, Deggendorf, Germany, September 16-18, 2020, trang 559–562. IEEE, 2020. 1

[2] Haoyu Cao, Xin Li, Jiefeng Ma, Deqiang Jiang, Antai Guo, Yiqing Hu, Hao Liu, Yinsong Liu, và Bo Ren. Query-driven generative network for document information extraction in the wild. Trong João Magalhães, Alberto Del Bimbo, Shin'ichi Satoh, Nicu Sebe, Xavier Alameda-Pineda, Qin Jin, Vincent Oria, và Laura Toni, biên tập, MM '22: The 30th ACM International Conference on Multimedia, Lisboa, Portugal, October 10 - 14, 2022, trang 4261–4271. ACM, 2022. 2

[3] Haoyu Cao, Jiefeng Ma, Antai Guo, Yiqing Hu, Hao Liu, Deqiang Jiang, Yinsong Liu, và Bo Ren. GMN: generative multi-modal network for practical document information extraction. Trong Marine Carpuat, Marie-Catherine de Marneffe, và Iván Vladimir Meza Ruíz, biên tập, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, trang 3768–3778. Association for Computational Linguistics, 2022. 2

[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, và Sergey Zagoruyko. End-to-end object detection with transformers. Trong Andrea Vedaldi, Horst Bischof, Thomas Brox, và Jan-Michael Frahm, biên tập, Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part I, tập 12346 của Lecture Notes in Computer Science, trang 213–229. Springer, 2020. 2, 4

[5] Bowen Cheng, Alexander G. Schwing, và Alexander Kirillov. Per-pixel classification is not all you need for semantic segmentation. Trong Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, và Jennifer Wortman Vaughan, biên tập, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, trang 17864–17875, 2021. 1, 4

[6] Lei Cui, Yiheng Xu, Tengchao Lv, và Furu Wei. Document AI: benchmarks, models and applications. CoRR, abs/2111.08609, 2021. 1, 2

[7] Brian L. Davis, Bryan S. Morse, Brian L. Price, Chris Tensmeyer, Curtis Wigington, và Vlad I. Morariu. End-to-end document recognition and understanding with dessurt. Trong Leonid Karlinsky, Tomer Michaeli, và Ko Nishino, biên tập, Computer Vision - ECCV 2022 Workshops - Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part IV, tập 13804 của Lecture Notes in Computer Science, trang 280–296. Springer, 2022. 2, 6

[8] Timo I. Denk và Christian Reisswig. Bertgrid: Contextualized embedding for 2d document representation and understanding. CoRR, abs/1909.04948, 2019. 2

[9] Wei Feng, Wenhao He, Fei Yin, Xu-Yao Zhang, và Cheng-Lin Liu. Textdragon: An end-to-end framework for arbitrary shaped text spotting. Trong 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, trang 9075–9084. IEEE, 2019. 8

[10] He Guo, Xiameng Qin, Jiaming Liu, Junyu Han, Jingtuo Liu, và Errui Ding. EATEN: entity-aware attention for single shot visual text extraction. Trong 2019 International Conference on Document Analysis and Recognition, ICDAR 2019, Sydney, Australia, September 20-25, 2019, trang 254–259. IEEE, 2019. 5, 7, 12

[11] Ankush Gupta, Andrea Vedaldi, và Andrew Zisserman. Synthetic data for text localisation in natural images. Trong 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, trang 2315–2324. IEEE Computer Society, 2016. 6

[12] Jaekyu Ha, R.M. Haralick, và I.T. Phillips. Recursive x-y cut using bounding boxes of connected components. Trong Proceedings of 3rd International Conference on Document Analysis and Recognition, tập 2, trang 952–955 vol.2, 1995. 2

[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image recognition. Trong 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, jun 2016. 3

[14] Teakgyu Hong, Donghyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, và Sungrae Park. BROS: A pre-trained language model focusing on text and layout for better key information extraction from documents. Trong Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March 1, 2022, trang 10767–10775. AAAI Press, 2022. 5

[15] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, và Furu Wei. Layoutlmv3: Pre-training for document AI with unified text and image masking. Trong João Magalhães, Alberto Del Bimbo, Shin'ichi Satoh, Nicu Sebe, Xavier Alameda-Pineda, Qin Jin, Vincent Oria, và Laura Toni, biên tập, MM '22: The 30th ACM International Conference on Multimedia, Lisboa, Portugal, October 10 - 14, 2022, trang 4083–4091. ACM, 2022. 1, 2

[16] Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, và C. V. Jawahar. ICDAR2019 competition on scanned receipt OCR and information extraction. CoRR, abs/2103.10213, 2021. 5, 7, 12

[17] Alyssa Hwang, William R. Frey, và Kathleen R. McKeown. Towards augmenting lexical resources for slang and african american english. Trong Marcos Zampieri, Preslav Nakov, Nikola Ljubesic, Jörg Tiedemann, và Yves Scherrer, biên tập, Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects, VarDial@COLING 2020, Barcelona, Spain (Online), December 13, 2020, trang 160–172. International Committee on Computational Linguistics (ICCL), 2020. 6, 7

[18] Wonseok Hwang, Seonghyeon Kim, Minjoon Seo, Jinyeong Yim, Seunghyun Park, Sungrae Park, Junyeop Lee, Bado Lee, và Hwalsuk Lee. Post-ocr parsing: building simple and robust parser via bio tagging. Trong Workshop on Document Intelligence at NeurIPS 2019, 2019. 1, 6, 7

[19] Wonseok Hwang, Jinyeong Yim, Seunghyun Park, Sohee Yang, và Minjoon Seo. Spatial dependency parsing for semi-structured document information extraction. Trong Chengqing Zong, Fei Xia, Wenjie Li, và Roberto Navigli, biên tập, Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, tập ACL/IJCNLP 2021 của Findings of ACL, trang 330–343. Association for Computational Linguistics, 2021. 6, 7

[20] Max Jaderberg, Karen Simonyan, Andrea Vedaldi, và Andrew Zisserman. Synthetic data and artificial neural networks for natural scene text recognition. CoRR, abs/1406.2227, 2014. 6

[21] Anoop R. Katti, Christian Reisswig, Cordula Guder, Sebastian Brarda, Steffen Bickel, Johannes Höhne, và Jean Baptiste Faddoul. Chargrid: Towards understanding 2d documents. Trong Ellen Riloff, David Chiang, Julia Hockenmaier, và Jun'ichi Tsujii, biên tập, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, trang 4459–4469. Association for Computational Linguistics, 2018. 2

[22] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, và Seunghyun Park. Ocr-free document understanding transformer. Trong Shai Avidan, Gabriel J. Brostow, Moustapha Cissé, Giovanni Maria Farinella, và Tal Hassner, biên tập, Computer Vision - ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXVIII, tập 13688 của Lecture Notes in Computer Science, trang 498–517. Springer, 2022. 2, 5, 6, 7, 12

[23] David D. Lewis, Gady Agam, Shlomo Argamon, Ophir Frieder, David A. Grossman, và Jefferson Heard. Building a test collection for complex document information processing. Trong Efthimis N. Efthimiadis, Susan T. Dumais, David Hawking, và Kalervo Järvelin, biên tập, SIGIR 2006: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Seattle, Washington, USA, August 6-11, 2006, trang 665–666. ACM, 2006. 6

[24] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, và Luke Zettlemoyer. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Trong Dan Jurafsky, Joyce Chai, Natalie Schluter, và Joel R. Tetreault, biên tập, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, trang 7871–7880. Association for Computational Linguistics, 2020. 2

[25] Xin Li, Yan Zheng, Yiqing Hu, Haoyu Cao, Yunfei Wu, Deqiang Jiang, Yinsong Liu, và Bo Ren. Relational representation learning in visually-rich documents. Trong Proceedings of the 30th ACM International Conference on Multimedia, MM'22, trang 4614–4624, New York, NY, USA, 2022. Association for Computing Machinery. 2

[26] Xiaojing Liu, Feiyu Gao, Qiong Zhang, và Huasha Zhao. Graph convolution for multimodal information extraction from visually rich documents. Trong Anastassia Loukina, Michelle Morales, và Rohit Kumar, biên tập, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 2 (Industry Papers), trang 32–39. Association for Computational Linguistics, 2019. 2

[27] Yuliang Liu, Hao Chen, Chunhua Shen, Tong He, Lianwen Jin, và Liangwei Wang. Abcnet: Real-time scene text spotting with adaptive bezier-curve network. Trong 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, trang 9806–9815. Computer Vision Foundation / IEEE, 2020. 8

[28] Yuliang Liu, Jiaxin Zhang, Dezhi Peng, Mingxin Huang, Xinyu Wang, Jingqun Tang, Can Huang, Dahua Lin, Chunhua Shen, Xiang Bai, et al. Spts v2: single-point scene text spotting. arXiv preprint arXiv:2301.01635, 2023. 8

[29] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, và Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. Trong 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, trang 9992–10002. IEEE, 2021. 2, 3, 6

[30] Colin Lockard, Prashant Shiralkar, và Xin Luna Dong. Openceres: When open information extraction meets the semi-structured web. Trong Jill Burstein, Christy Doran, và Thamar Solorio, biên tập, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), trang 3047–3056. Association for Computational Linguistics, 2019. 2

[31] Colin Lockard, Prashant Shiralkar, Xin Luna Dong, và Hannaneh Hajishirzi. Zeroshotceres: Zero-shot relation extraction from semi-structured webpages. Trong Dan Jurafsky, Joyce Chai, Natalie Schluter, và Joel R. Tetreault, biên tập, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, trang 8105–8117. Association for Computational Linguistics, 2020. 2

[32] Simone Marinai, Marco Gori, và Giovanni Soda. Artificial neural networks for document analysis and recognition. IEEE Trans. Pattern Anal. Mach. Intell., 27(1):23–35, 2005. 2

[33] Yosi Mass và Haggai Roitman. Ad-hoc document retrieval using weak-supervision with BERT and GPT2. Trong Bonnie Webber, Trevor Cohn, Yulan He, và Yang Liu, biên tập, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, trang 4191–4197. Association for Computational Linguistics, 2020. 1

[34] Minesh Mathew, Dimosthenis Karatzas, và C. V. Jawahar. Docvqa: A dataset for VQA on document images. Trong

--- TRANG 9 ---
IEEE Winter Conference on Applications of Computer Vision, WACV 2021, Waikoloa, HI, USA, January 3-8, 2021, trang 2199–2208. IEEE, 2021. 1, 6, 7, 12

[35] Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung Surh, Minjoon Seo, và Hwalsuk Lee. Cord: A consolidated receipt dataset for post-ocr parsing. 2019. 5, 7, 12

[36] Rafal Powalski, Lukasz Borchmann, Dawid Jurkiewicz, Tomasz Dwojak, Michal Pietruszka, và Gabriela Palka. Going full-tilt boogie on document understanding with text-image-layout transformer. Trong Josep Lladós, Daniel Lopresti, và Seiichi Uchida, biên tập, 16th International Conference on Document Analysis and Recognition, ICDAR 2021, Lausanne, Switzerland, September 5-10, 2021, Proceedings, Part II, tập 12822 của Lecture Notes in Computer Science, trang 732–747. Springer, 2021. 2

[37] Yujie Qian, Enrico Santus, Zhijing Jin, Jiang Guo, và Regina Barzilay. Graphie: A graph-based framework for information extraction. Trong Jill Burstein, Christy Doran, và Thamar Solorio, biên tập, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), trang 751–761. Association for Computational Linguistics, 2019. 1, 2

[38] Clément Sage, Alex Aussem, Véronique Eglin, Haytham Elghazel, và Jérémy Espinas. End-to-end extraction of structured information from business documents with pointer-generator networks. Trong Priyanka Agrawal, Zornitsa Kozareva, Julia Kreutzer, Gerasimos Lampouras, André F. T. Martins, Sujith Ravi, và Andreas Vlachos, biên tập, Proceedings of the Fourth Workshop on Structured Prediction for NLP@EMNLP 2020, Online, November 20, 2020, trang 43–52. Association for Computational Linguistics, 2020. 2

[39] M. Shilman, P. Liang, và P. Viola. Learning nongenerative grammatical models for document analysis. Trong Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1, tập 2, trang 962–969 Vol. 2, 2005. 2

[40] Anikó Simon, Jean-Christophe Pret, và A. Peter Johnson. A fast algorithm for bottom-up document layout analysis. IEEE Trans. Pattern Anal. Mach. Intell., 19(3):273–277, 1997. 2

[41] Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, và Mohit Bansal. Unifying vision, text, and layout for universal document processing. CoRR, abs/2212.02623, 2022. 2

[42] Daniel van Strien, Kaspar Beelen, Mariona Coll Ardanuy, Kasra Hosseini, Barbara McGillivray, và Giovanni Colavizza. Assessing the impact of OCR quality on downstream NLP tasks. Trong Ana Paula Rocha, Luc Steels, và H. Jaap van den Herik, biên tập, Proceedings of the 12th International Conference on Agents and Artificial Intelligence, ICAART 2020, Volume 1, Valletta, Malta, February 22-24, 2020, trang 484–496. SCITEPRESS, 2020. 1

[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, và Illia Polosukhin. Attention is all you need. Trong Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, và Roman Garnett, biên tập, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, trang 5998–6008, 2017. 4

[44] Mengxi Wei, Yifan He, và Qiong Zhang. Robust layout-aware IE for visually rich documents with pre-trained language models. Trong Jimmy X. Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, và Yiqun Liu, biên tập, Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020, trang 2367–2376. ACM, 2020. 2

[45] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, và Ming Zhou. Layoutlm: Pre-training of text and layout for document image understanding. Trong Rajesh Gupta, Yan Liu, Jiliang Tang, và B. Aditya Prakash, biên tập, KDD '20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, trang 1192–1200. ACM, 2020. 2

[46] Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florêncio, Cha Zhang, và Furu Wei. Layoutxlm: Multimodal pre-training for multilingual visually-rich document understanding. CoRR, abs/2104.08836, 2021. 2

[47] Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei A. F. Florêncio, Cha Zhang, Wanxiang Che, Min Zhang, và Lidong Zhou. Layoutlmv2: Multi-modal pre-training for visually-rich document understanding. Trong Chengqing Zong, Fei Xia, Wenjie Li, và Roberto Navigli, biên tập, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, trang 2579–2591. Association for Computational Linguistics, 2021. 2, 5, 6, 7

[48] Wenwen Yu, Ning Lu, Xianbiao Qi, Ping Gong, và Rong Xiao. PICK: processing key information extraction from documents using improved graph learning-convolutional networks. Trong 25th International Conference on Pattern Recognition, ICPR 2020, Virtual Event / Milan, Italy, January 10-15, 2021, trang 4363–4370. IEEE, 2020. 2

[49] Tai-Ling Yuan, Zhe Zhu, Kun Xu, Cheng-Jun Li, Tai-Jiang Mu, và Shi-Min Hu. A large chinese text dataset in the wild. J. Comput. Sci. Technol., 34(3):509–521, 2019. 8

[50] Kaizhong Zhang và Dennis E. Shasha. Simple fast algorithms for the editing distance between trees and related problems. SIAM J. Comput., 18(6):1245–1262, 1989. 5

[51] Peng Zhang, Yunlu Xu, Zhanzhan Cheng, Shiliang Pu, Jing Lu, Liang Qiao, Yi Niu, và Fei Wu. TRIE: end-to-end text reading and information extraction for document understanding. Trong Chang Wen Chen, Rita Cucchiara, Xian-Sheng Hua, Guo-Jun Qi, Elisa Ricci, Zhengyou Zhang, và Roger Zimmermann, biên tập, MM '20: The 28th ACM International Conference on Multimedia, Virtual Event / Seattle, WA, USA, October 12-16, 2020, trang 1413–1422. ACM, 2020. 1

--- TRANG 10 ---
--- TRANG 11 ---
--- TRANG 12 ---
Phụ lục

Tài liệu bổ sung này trình bày một nghiên cứu trường hợp so sánh của SeRum với các phương pháp đầu cuối và các phương pháp phụ thuộc OCR để xử lý các hình ảnh đầy thách thức. Đánh giá sử dụng một số tập thử nghiệm, như SROIE [16], CORD [35], Ticket [10], và DocVQA [34].

Kết quả so sánh với các phương pháp đầu cuối. SeRum được so sánh với Donut [22], phương pháp hiểu biết tài liệu đầu cuối tiên tiến hiện tại, giải mã văn bản trực tiếp từ tính năng hình ảnh. Tuy nhiên, Donut gặp khó khăn trong việc tạo ra kết quả quá dài, dẫn đến không ổn định, và sự lệch hướng và nhầm lẫn cơ chế chú ý. Ngược lại, SeRum xuất sắc trong việc giải mã dạng token thị giác cục bộ quan tâm, dẫn đến những cải thiện đáng kể trong cả hai nhược điểm này.

Như được minh họa trong Hình 5.(a) đến (c), Donut tạo ra một chuỗi văn bản bất thường do sự can thiệp của các ký tự dư thừa, và nó không thể phân tích chính xác tất cả thông tin chính. Ngược lại, SeRum sở hữu khả năng xác định khu vực quan tâm chính và thực hiện quá trình giải mã một cách tách biệt. Ngoài ra, như được hiển thị trong Hình 5.(d) đến (f), Donut thể hiện xu hướng hiểu sai vị trí của văn bản, trong khi SeRum có khả năng xác định chính xác văn bản và vị trí của nó trong hình ảnh. Tổng thể, SeRum chứng minh hiệu suất vượt trội so với Donut.

Kết quả so sánh với các phương pháp phụ thuộc OCR. Phần này đánh giá hiệu suất của SeRum trên hình ảnh văn bản viết tay hoặc mờ. Nhận dạng văn bản viết tay đặt ra một thách thức đáng kể cho các hệ thống OCR do tính phức tạp và tính biến đổi vốn có của phông chữ viết tay. Các ký tự viết tay thể hiện mức độ biến đổi cao về hình dạng, kích thước, độ nghiêng, v.v. như được hiển thị trong Hình 6.(a) đến (g). Bên cạnh đó, sự ổn định của hệ thống cũng có thể bị ảnh hưởng đáng kể bởi sự hiện diện của văn bản mờ, như được minh họa trong Hình 6.(h) đến (o).

Mô hình SeRum đơn giản hóa đường ống nhận dạng ký tự bằng cách tích hợp tất cả các giai đoạn thành một mô hình duy nhất, đạt được tối ưu hóa đầu cuối cải thiện độ chính xác và giảm sự lan truyền lỗi. Ngoài ra, mô hình sử dụng các cơ chế chú ý để trích xuất các tính năng mạnh mẽ từ hình ảnh đầu vào và sử dụng hiệu quả thông tin ngữ cảnh.

Các phát hiện của chúng tôi cho thấy phương pháp SeRum có thể tổng hợp ngữ cảnh và giúp cải thiện kết quả nhận dạng OCR. Ví dụ, trong Hình 6.(d), mô hình SeRum xác định từ 'home' sau một số điện thoại, cho thấy đó là điện thoại nhà chứ không phải một thuật ngữ ít phổ biến hơn như 'hame'. Như được hiển thị trong Hình 6.(n), mô hình SeRum phân biệt 'MART' từ 'MAPT', mặc dù sự tương đồng thị giác của cái sau do sự mơ hồ.

(a)
(b)
(e)(d)
SeRum:TAHUGORENG…(nm)
Donut: <s_nm> E T E T E T E ……..
Donut: None(nm)
SeRum:ICEDGT(nm)

Donut: <cnt> 4
SeRum:(Qty=4.00)(cnt)
Donut: <s_nm>FOOD Quality/k
SeRum:BLACKPAPER…..（nm）

(c)
Donut: None (ticket_num)
SeRum:F029746(ticket_num)

(f)
Donut: <s_total> 5.00
SeRum:4.90(total)

Hình 5. Các trường hợp đầy thách thức gặp phải trong nghiên cứu của chúng tôi, bao gồm các thể hiện của văn bản dư thừa trên viền, hình ảnh chồng chéo, v.v. Các hộp màu đỏ và xanh lá cây đại diện cho đầu ra của Donut và SeRum, tương ứng. Xem tốt nhất trong màu.

--- TRANG 13 ---
(g)(h)(i)
(j)(k)(l)
(m)(n)(o)
(a)(b)(c)
OCR:/230 | SeRum:1230
OCR:hame | SeRum:home
OCR:bave | SeRum:Dave

OCR:12/1aa | SeRum:12/1/99
OCR:LCE | SeRum:ICE
OCR:A11 | SeRum:All

OCR:208055692 | SeRum:20B055692
OCR:MAPT | SeRum:MART
OCR:-60,000 | SeRum:-60.000

OCR:12/1aa | SeRum:12/1/99
OCR:DaThacsr | SeRum:DanThacker

(d)(e)(f)
OCR:-leard | SeRum:Heard
SeRum:Date:3/21 | OCR:Dat:3/21
OCR:AE0NC0. | SeRum:AEONCO.
OCR:cheesa | SeRum:Cheese

Hình 6. Ví dụ về các trường hợp đầy thách thức, như viết tay, mờ, và thiếu ký tự, minh họa những khó khăn mà các hệ thống OCR gặp phải. Những ký tự này được biết đến là gây khó khăn trong nhận dạng chính xác. Các hộp màu đỏ và xanh lá cây đại diện cho đầu ra của công cụ OCR và SeRum, tương ứng. Xem tốt nhất trong màu.
