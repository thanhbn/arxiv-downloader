# Về Việc Xây Dựng Corpus Hình Ảnh Dựa Trên Web
# Cho Hiểu Biết Tài Liệu Hình Ảnh

Donghyun Kim1, Teakgyu Hong2?,
Moonbin Yim1, Yoonsik Kim1, và Geewook Kim1??
1NAVER CLOVA
2Upstage AI

Tóm tắt. Trong những năm gần đây, nghiên cứu về hiểu biết tài liệu hình ảnh (VDU) đã phát triển đáng kể, với sự nhấn mạnh đặc biệt vào việc phát triển các phương pháp học tự giám sát. Tuy nhiên, một trong những thách thức đáng kể gặp phải trong lĩnh vực này là sự hạn chế về tính khả dụng của các corpus hình ảnh có thể truy cập công khai hoặc các bộ sưu tập hình ảnh rộng lớn với các chú thích văn bản chi tiết, đặc biệt là cho các ngôn ngữ không phải Latin hoặc các ngôn ngữ khan hiếm tài nguyên. Để giải quyết thách thức này, chúng tôi đề xuất Web-based Visual Corpus Builder (Webvicob), một công cụ tạo tập dữ liệu có khả năng xây dựng các corpus hình ảnh đa ngôn ngữ quy mô lớn từ các dump HTML Wikipedia thô. Các thí nghiệm của chúng tôi chứng minh rằng dữ liệu được tạo ra bởi Webvicob có thể được sử dụng để huấn luyện các mô hình VDU mạnh mẽ hoạt động tốt trên các nhiệm vụ downstream khác nhau, chẳng hạn như DocVQA và phân tích hậu OCR. Hơn nữa, khi sử dụng một tập dữ liệu gồm 1 triệu hình ảnh được tạo ra bởi Webvicob, chúng tôi đã quan sát thấy một cải thiện hơn 13% trên DocVQA Task 3 so với một tập dữ liệu gồm 11 triệu hình ảnh từ IIT-CDIP. Việc triển khai công cụ của chúng tôi được công khai tại https://github.com/clovaai/webvicob.

Từ khóa: Hiểu Biết Tài Liệu Hình Ảnh · Nhận Dạng Ký Tự Quang Học · Xử Lý Hình Ảnh Tài Liệu.

1 Giới Thiệu

Mô hình hóa ngôn ngữ đã là một nhiệm vụ cơ bản lâu đời trong xử lý ngôn ngữ tự nhiên (NLP). Các mô hình ngôn ngữ (LM) được huấn luyện được sử dụng trong một loạt các ứng dụng NLP downstream, chẳng hạn như trích xuất thông tin (IE) [17] và trả lời câu hỏi (QA) [9]. Để xây dựng một LM mạnh mẽ, các phương pháp gần đây sử dụng corpus văn bản quy mô lớn trong giai đoạn tiền huấn luyện. Corpus văn bản thường được xây dựng bằng một công cụ hoặc phần mềm chuyên biệt. Ví dụ, WikiExtractor [1] trích xuất văn bản từ các dump HTML Wikipedia và xây dựng một corpus văn bản sạch.

Hiểu Biết Tài Liệu Hình Ảnh (VDU) [42,15,22] đã được phát triển để thực hiện một loạt các nhiệm vụ thực tế trên hình ảnh tài liệu. Ví dụ, Document Parsing [42,22] nhằm mục đích trích xuất một số văn bản quan trọng từ hình ảnh tài liệu [17,18]. Hầu hết các backbone VDU gần đây có thể được coi là một phần mở rộng của LM.

Được truyền cảm hứng bởi những tiến bộ gần đây trong LM [40,9], các phương pháp VDU gần đây chia sẻ một cách tiếp cận tương tự là (1) trước tiên thu thập hình ảnh tài liệu thực quy mô lớn, (2) thực hiện OCR trên hình ảnh để trích xuất văn bản, và (3) huấn luyện một backbone LM giống BERT trên các văn bản đã trích xuất [42,41,15,16].

Mặc dù các phương pháp VDU thông thường đã cho thấy kết quả khả quan, nhưng tồn tại một số thách thức thực tế, đặc biệt là trong giai đoạn chuẩn bị tập dữ liệu huấn luyện. Hầu hết các công trình gần đây [42,15] dựa vào một tập dữ liệu hình ảnh thực quy mô lớn. Ví dụ, tập dữ liệu IIT-CDIP [27] bao gồm 11 triệu hình ảnh tài liệu công nghiệp và được sử dụng trong một loạt các công trình VDU. Tuy nhiên, trong hầu hết các ngôn ngữ ít tài nguyên, không có tập dữ liệu công khai như IIT-CDIP. Ngoài ra, các công cụ OCR có sẵn (ví dụ: CLOVA OCR API, MS Read API, Amazon Textract) được yêu cầu để trích xuất văn bản trong quá trình tiền xử lý. Điều này thường đòi hỏi chi phí rất lớn.

Hơn nữa, việc sử dụng OCR có thể có những hệ quả tiêu cực khác; dữ liệu được xây dựng bị ràng buộc chặt chẽ với công cụ OCR, và lỗi OCR được lan truyền trong toàn bộ quá trình. Vấn đề này trở nên nghiêm trọng, đặc biệt là trong một số ngôn ngữ không phải Latin như tiếng Hàn và tiếng Nhật, nơi OCR được biết là phức tạp.

Trong công trình này, chúng tôi đề xuất một công cụ để xây dựng corpus hình ảnh dựa trên web. Như được trình bày trong Hình 2 và Hình 3, Web-based Visual Corpus Builder (Webvicob) được đề xuất render một trang web thành một file hình ảnh và tạo ra các chú thích văn bản phong phú. Với các API Document Object Model (DOM) có tính biểu đạt, Webvicob tạo ra các bounding box chính xác cho tất cả các ký tự.

Hơn nữa, Webvicob bao phủ một loạt các ngữ cảnh từ rộng lớn. Wikipedia là một tập dữ liệu khổng lồ bao gồm hơn 270 ngôn ngữ và chứa 60 triệu tài liệu HTML. Kết quả phân tích PCA trên 3,626 mẫu có sẵn trong Phần 5.3.

So với mô hình truyền thống được huấn luyện trên IIT-CDIP, mô hình được huấn luyện bằng Webvicob cho thấy kết quả cạnh tranh trên DocVQA Task 1 và điểm số cao hơn trên Task 3, cho thấy tính hiệu quả của corpus hình ảnh dựa trên Webvicob đã được khử thiên vị.

Phương pháp được đề xuất đơn giản nhưng hiệu quả. Chúng tôi cho thấy rằng corpus được tạo ra bởi Webvicob là rất quan trọng trong việc xây dựng một backbone VDU mạnh mẽ. Thông qua các thí nghiệm và phân tích mở rộng, chúng tôi chứng minh tính hiệu quả của Webvicob. Những đóng góp của công trình này có thể được tóm tắt như sau:

1. Chúng tôi đề xuất Webvicob, có thể được sử dụng để tiền huấn luyện các mô hình hiểu biết tài liệu hình ảnh. Webvicob cung cấp các chú thích phong phú, bao gồm thông tin ký tự, từ, dòng và đoạn văn.

2. Webvicob cung cấp hỗ trợ cho một loạt các font chữ, cho phép các tài liệu hình ảnh có nội dung giống hệt nhau xuất hiện khác nhau. Ngoài ra, chúng tôi đã tính đến các đặc điểm của từng font chữ để xây dựng các chú thích bounding box cấp độ ký tự chính xác.

3. Chúng tôi thực hiện các thí nghiệm mở rộng để xác minh tính hiệu quả của công cụ và tập dữ liệu được đề xuất.

4. Mã nguồn của công cụ của chúng tôi được công khai để thúc đẩy nghiên cứu về VDU trong các ngôn ngữ ít tài nguyên.

2 Bối Cảnh và Công Trình Liên Quan

Trong phần này, chúng tôi giới thiệu pipeline VDU truyền thống và các tập dữ liệu. Hầu hết các phương pháp hiện tại chia sẻ một cách tiếp cận tương tự là (1) thu thập hình ảnh thực quy mô lớn, (2) thực hiện OCR trên hình ảnh, và (3) huấn luyện một backbone giống BERT trên các văn bản đã trích xuất [42,15].

2.1 Backbone VDU

Được truyền cảm hứng bởi BERT [9] và những tiến bộ gần đây trong mô hình hóa ngôn ngữ, một loạt các backbone VDU dựa trên Transformer giống BERT đã được đề xuất [42,15,41,16,8]. Để xử lý thông tin bố cục của hình ảnh tài liệu, tọa độ không gian của các hộp văn bản OCR được đưa vào backbone VDU [42,15,43]. Sử dụng các bộ mã hóa hình ảnh như ResNet [14], các đặc trưng hình ảnh của hình ảnh đầu vào cũng đang được tích hợp vào các backbone VDU gần đây [41,16]. Gần đây hơn, với những tiến bộ trong Vision Transformer (ViT) [10], việc huấn luyện một backbone VDU mã hóa-giải mã Transformer mà không cần OCR cũng đã được thử nghiệm [22,7,26]. Công cụ của chúng tôi có thể được sử dụng cùng với các backbone VDU khác nhau. Trong bài báo này, chúng tôi đã xác minh hiệu suất của công cụ Webvicob bằng cách tiền huấn luyện BROS [15], LayoutXLM [43], và Donut [22].

2.2 Xây Dựng Corpus Hình Ảnh cho VDU

Hầu hết các tập dữ liệu được chú thích OCR hiện có có kích thước nhỏ, dẫn đến khó khăn trong việc huấn luyện các backbone VDU. Để xây dựng một corpus phong phú, trong pipeline truyền thống, hình ảnh tài liệu thực tế quy mô lớn (ví dụ: IIT-CDIP) và một công cụ OCR (ví dụ: CLOVA OCR API) được sử dụng.

Chất lượng của công cụ OCR ảnh hưởng đáng kể đến các quá trình downstream [22,7]. Do đó, đã có những khó khăn trong việc huấn luyện và kiểm thử backbone VDU. Ví dụ, vì BROS [15] và LayoutLM [43] sử dụng các OCR nội bộ khác nhau, việc so sánh công bằng đã trở nên khó khăn.

LayoutXLM [43] thu thập dữ liệu PDF sinh ra từ kỹ thuật số quy mô lớn từ world wide web và trích xuất chú thích văn bản từ PDF thông qua một trình render PDF mã nguồn mở. Mặc dù điều này cho thấy một hướng đi khả quan khác, nhưng trong thực tế không dễ dàng làm theo pipeline này. Một người thực hành phải thu thập dữ liệu PDF vì không có tập dữ liệu có sẵn công khai ([43] không công khai tập dữ liệu). Hơn nữa, vì các file PDF không thể dễ dàng chỉnh sửa, việc augmentation bị hạn chế.

Khác với cách tiếp cận hiện có, Webvicob có thể hiệu quả sửa đổi và augment dữ liệu (tức là bố cục, hình ảnh nền) bằng javascript vì Webvicob render HTML trực tiếp. Hơn nữa, Webvicob có thể dễ dàng được tích hợp với các dump HTML (ví dụ: dump Wikipedia), có thể truy cập dễ dàng và đã được sử dụng rộng rãi trong việc xây dựng các backbone NLP mạnh mẽ [9].

3 Web-based Visual Corpus Builder

Webvicob sử dụng dump HTML và sửa đổi Document Object Model (DOM) để tạo ra dữ liệu để tiền huấn luyện các backbone VDU với corpus phong phú. Như được thấy trong Hình 2, Webvicob cung cấp chú thích hộp của ký tự, từ, LaTeX [34], hình ảnh, dòng, và đoạn văn, và cũng cho hình ảnh và LaTeX.

Trong phần này, chúng tôi giải thích chi tiết các quy trình tạo ra (Thuật toán 1).

Thuật toán 1 Lấy chú thích từ HTML
Input: html
Output: image, annotations
procedure get_annotations_from_html (html)
1:html add_spans (html ){3.1}
// Từ <p>ab</p> thành <p><span>a</span><span>b</span></p>
2:driver get_selenium_driver (html )
3:remove_unusable_elements (driver ){3.1}
4:change_paragraph_fonts (driver ){3.2}
5:image capture (driver )
6:boxes get_glyph_box (driver ){3.2}
7:annotations get_annots (boxes ){3.1}
return image, annotations

3.1 Chú Thích

Thêm Spans Chúng ta có thể truy cập từng Element bằng DOM và tìm ra bounding box (bbox) của Element thông qua hàm getBoundingClientRect(). Webvicob sửa đổi HTML để tất cả ký tự có thể được bao quanh bởi thẻ <span> để lấy bbox của từng ký tự (tức là từ <p>abc</p> thành <p><span>a</span><span>b</span><span>c</span></p>). Với quy trình này, hàm getBoundingClientRect() có thể được áp dụng cho tất cả các span, cho phép chúng ta có được chú thích bounding box cho tất cả các ký tự.

Loại Bỏ Các Element Không Sử Dụng Được Bước quan trọng trước khi tạo dữ liệu là loại bỏ các Element không sử dụng được. Ví dụ, có nhiều cách khác nhau để ẩn một Element cụ thể trong tài liệu web. Ngay cả khi Element không hiển thị, hàm getBoundingClientRect() vẫn trả về kết quả vì Element chiếm không gian. Cụ thể, các Element sau được loại bỏ:

Pseudo Elements.
Child Elements có kích thước Element lớn hơn node cha.
Elements có style invisible được áp dụng.
(display: none / visibility: hidden / visibility: collapse / opacity: 0)
Elements nằm ngoài màn hình render.
Element có thuộc tính placeholder.

Xây Dựng Chú Thích Chúng tôi xây dựng chú thích từ và chú thích dòng bằng cách tính toán khoảng cách giữa các hộp ký tự và các hộp dòng. Chúng tôi định nghĩa LaTeX Elements với className "mwe-math-fallback-image-inline", và định nghĩa image Elements với các thẻ {image, canvas, SVG, video}. Như MarkupLM [21] đã làm, chú thích đoạn văn được trích xuất bằng cách sử dụng cấu trúc cây có thứ tự tốt. Các Elements có cùng độ sâu được nhóm thành một đoạn văn.

3.2 Render với Các Font Khác Nhau

Font Đoạn Văn Ngẫu Nhiên Sự đa dạng hình ảnh trong tập dữ liệu tiền huấn luyện thường được liên kết với hiệu suất cải thiện của các backbone VDU [19,12,45,22]. Để có sự đa dạng hình ảnh, Webvicob render HTML với các font khác nhau cho từng đoạn văn (Xem Hình 4a và 4b). Chúng tôi chọn ngẫu nhiên font từ 3,567 Google-Fonts trong các thí nghiệm và phân tích của chúng tôi.

Bounding Box Chính Xác Bounding box thực tế của glyph và kết quả của getBoundingClientRect() là khác nhau. Như được thấy trong Hình 4a và 4b, kết quả của getBoundingClientRect() có margin lớn. Chúng tôi trích xuất tỷ lệ của glyph box thực tế so với bounding box thông qua việc render hình ảnh vector trong file font bằng Pygame FreeType handler. Sử dụng tỷ lệ này, glyph box chặt chẽ cuối cùng có thể được thu được (Hình 4c).

4 Thí Nghiệm và Phân Tích

4.1 Thiết Lập

Các thí nghiệm Donut được thực hiện bằng 8 GPU NVIDIA V100 để so sánh công bằng với Donut Proto, trong khi các thí nghiệm khác được thực hiện bằng 4 GPU NVIDIA A100 80G. Chúng tôi sử dụng kỹ thuật huấn luyện mixed precision [33].

Các Mô Hình Sử Dụng Để cho thấy tính hiệu quả của Webvicob, chúng tôi sử dụng ba mô hình với các thuộc tính khác nhau.

BROS [15] được đề xuất với một phương pháp tiền huấn luyện hiệu quả (tức là area masking) và một relative positional encoding. Để xác thực tính hiệu quả của dữ liệu do Webvicob tạo ra, chúng tôi tiền huấn luyện BROS BASE và đo hiệu suất trên DocVQA Task 1 [39] và Task 3 [32].

LayoutXLM [43] là một backbone VDU đa ngôn ngữ được sử dụng rộng rãi. Chúng tôi tái triển khai và tiền huấn luyện LayoutXLM BASE cho tám ngôn ngữ để xác thực Webvicob trong tình huống đa ngôn ngữ. Các tập dữ liệu FUNSD [20] và XFUND [43] được sử dụng làm tập dữ liệu benchmark.

Donut [22] đã giới thiệu một cách tiếp cận mới sử dụng hình ảnh một mình mà không dựa vào kết quả OCR làm đầu vào. Chúng tôi tiền huấn luyện Donut Proto sử dụng dữ liệu do Webvicob tạo ra để chứng minh tính linh hoạt của dữ liệu cho các kiến trúc khác nhau. Điều này đã được xác minh thông qua các thí nghiệm trên các tập dữ liệu DocVQA Task 1, DocVQA Task 3, FUNSD, và XFUND (tiếng Nhật).

Donut cho Entity Recognition Vì Donut chỉ nhận đầu vào hình ảnh và không thể sử dụng phương pháp phổ biến của token classification bằng thông tin OCR, chúng tôi đã huấn luyện mô hình để giải mã chuỗi theo định dạng cụ thể (như được mô tả trong Hình 5) để trích xuất tất cả các trường entity trong tài liệu.

Để đánh giá khả năng của mô hình trong việc xác định tập entity, bất kể thứ tự, chúng tôi đã sử dụng Hungarian Matching [25] để điều chỉnh chuỗi sao cho Tree Edit Distance (TED) được tối thiểu hóa. Điểm đánh giá cuối cùng dựa trên độ chính xác dựa trên TED [47,18,48,22].

Tiền Huấn Luyện Trong việc tiền huấn luyện BROS BASE, chúng tôi sử dụng 6,4 triệu mẫu tiếng Anh do Webvicob tạo ra và huấn luyện mô hình trong 5 epoch. Quy trình huấn luyện bao gồm việc chọn ngẫu nhiên 512 token liên tiếp từ mỗi tài liệu, tương tự như LayoutLMv2 [41]. Các siêu tham số huấn luyện còn lại được giữ nguyên như BROS BASE.

Như có thể thấy trong Bảng 1 và Bảng 2, chúng tôi đã tiền huấn luyện LayoutXLM BASE sử dụng 18,6 triệu dữ liệu đa ngôn ngữ do Webvicob tạo ra, bao gồm tám ngôn ngữ: tiếng Anh, tiếng Trung, tiếng Nhật, tiếng Tây Ban Nha, tiếng Pháp, tiếng Bồ Đào Nha, tiếng Ý, và tiếng Đức. Mô hình được tiền huấn luyện trong 5 epoch với kích thước batch 64, theo quy trình được mô tả trong LayoutXLM. Theo chiến lược sampling đa ngôn ngữ [6,3,43], mỗi batch được lấy mẫu dựa trên tần suất xuất hiện của ngôn ngữ trong dữ liệu, với xác suất pl α/(Nl α/N), trong đó N là tổng số dữ liệu và Nl là số dữ liệu cho ngôn ngữ l. Tham số α được đặt là 0,7 để tính đến phân phối dữ liệu không cân bằng.

Donut Proto mã nguồn mở đã được tiền huấn luyện với 1,2 triệu mẫu do SynthDoG tạo ra, bao gồm 400.000 mẫu mỗi loại cho tiếng Hàn, tiếng Nhật, và tiếng Anh. Việc tiền huấn luyện được thực hiện trong 5 ngày sử dụng 8 GPU NVIDIA V100, tổng cộng 40 GPU ngày, với kích thước batch 8. Một cách tiếp cận tương tự được thực hiện, trong đó 400.000 mẫu mỗi loại cho tiếng Hàn, tiếng Nhật, và tiếng Anh được tạo ra bởi Webvicob và Donut Proto được huấn luyện trong 3 ngày trên 8 GPU NVIDIA V100 (24 GPU ngày) với kích thước batch 16. Hình ảnh được cắt và sau đó thay đổi kích thước thành 2,048 chiều rộng và 1,536 chiều cao cho mục đích huấn luyện.

Fine-tuning Chúng tôi fine-tune BROS với tập dữ liệu DocVQA trong 16K iteration. Kích thước batch 64, bộ tối ưu hóa Adam [23], tốc độ học 5e-5, và bộ lập lịch tốc độ học cosine annealing [29] được áp dụng.

Vì lịch trình fine-tuning chính xác chưa được tiết lộ, chúng tôi thiết lập một lịch trình fine-tuning "thô" và so sánh kết quả. Cho XFUND Semantic Entity Recognition (SER), chúng tôi fine-tune LayoutXLM với 10K iteration, kích thước batch 64, bộ tối ưu hóa AdamW [30], tốc độ học 1e-4, decay tốc độ học tuyến tính, và warmup [11] tốc độ học cho 10% tổng số iteration. Chúng tôi sử dụng thư viện unilm cho các thí nghiệm fine-tuning LayoutXLM.

Donut Proto được fine-tuned trong 300 epoch cho các nhiệm vụ DocVQA với kích thước hình ảnh 2,560 chiều rộng và 2,048 chiều cao. Cho các nhiệm vụ FUNSD và XFUND, chúng tôi fine-tuned mô hình trong 2,000 epoch sử dụng hình ảnh với chiều rộng 2,048 và chiều cao 1,536. Cho tất cả các nhiệm vụ downstream, kích thước batch 8, bộ tối ưu hóa Adam [23], tốc độ học 1,5e-5, và bộ lập lịch tốc độ học cosine annealing được áp dụng.

4.2 Kết Quả Thí Nghiệm

BROS Corpus của DocVQA Task 1 và IIT-CDIP rất tương tự, trong khi dữ liệu do Webvicob tạo ra có corpus đa dạng hơn (sẽ được thể hiện trong Phần 5.3). Kết quả cũng phản ánh mô hình này. Kết quả của Bảng 3 được phân loại dựa trên lượng dữ liệu được sử dụng trong tiền huấn luyện. Do lượng dữ liệu đáng kể cụ thể cho một miền cụ thể trong IIT-CDIP, có thể xây dựng một mô hình chuyên biệt cho miền. Mặt khác, sự đa dạng rộng lớn của các miền được đại diện trong dữ liệu do Webvicob tạo ra đã dẫn đến việc phát triển một mô hình xuất sắc trong DocVQA Task 3.

Việc sử dụng dữ liệu IIT-CDIP cho tiền huấn luyện dẫn đến hiệu suất thuận lợi trên DocVQA Task 1. Ngược lại, khi dữ liệu Webvicob được sử dụng, DocVQA Task 3 hoạt động tốt hơn nhiều. Đáng ngạc nhiên, hiệu suất của Task 3 sử dụng Webvicob 1M có hiệu suất tốt hơn BROS sử dụng IIT-CDIP 11M. Khi một sự kết hợp của cả dữ liệu IIT-CDIP và dữ liệu do Webvicob tạo ra được sử dụng theo tỷ lệ bằng nhau cho tiền huấn luyện, có một cải thiện vừa phải về hiệu suất cho cả hai nhiệm vụ.

LayoutXLM Chúng tôi báo cáo điểm f1 của Semantic Entity Recognition (SER) cho các tập dữ liệu FUNSD và XFUND. Bảng 4 hiển thị điểm f1 của multitask fine-tuning với tám ngôn ngữ. Dưới cùng thiết lập fine-tuning, dữ liệu Webvicob cho thấy hiệu suất tương đương. Xin lưu ý rằng tổng số iteration tiền huấn luyện của LayoutXLM (Webvicob) là 1,45M, nhỏ hơn nhiều so với iteration của LayoutXLM (PDF 22M + IIT 8M), là 2,34M.

Donut Chúng tôi báo cáo kết quả fine-tuning của Donut Proto trong Bảng 5. Donut Proto được huấn luyện bằng dữ liệu Webvicob đã đạt được hiệu suất vượt trội trên tất cả bốn nhiệm vụ, mặc dù chỉ 60% ngày huấn luyện GPU so với Donut Proto được huấn luyện với dữ liệu do SynthDoG tạo ra. Trong khi SynthDoG cũng sử dụng corpus Wikipedia, hiệu suất nổi bật của dữ liệu do Webvicob tạo ra là đáng chú ý. Chúng tôi suy đoán rằng có hai lý do cho điều này. Thứ nhất, sự hiện diện của văn bản và hình ảnh có liên quan về mặt ngữ cảnh trong Webvicob giúp phản ánh chính xác mối quan hệ giữa hình ảnh và văn bản. Thứ hai, việc sử dụng nhiều loại tài liệu thực, chẳng hạn như bảng, trong quá trình học có thể cải thiện ngầm việc học thông tin semantic, chẳng hạn như key-value pairing.

5 Thảo Luận

5.1 Mở Rộng Quy Mô Sử Dụng CommonCrawl

Mục tiêu ban đầu của chúng tôi là xử lý hàng tấn dữ liệu HTML, chẳng hạn như tập dữ liệu CommonCrawl bao gồm petabyte dữ liệu.

Tuy nhiên, sự đa dạng khổng lồ trong định dạng HTML đã làm cho việc thiết kế một giải pháp tích hợp trở nên khó khăn. Hiện tại, Webvicob chỉ chuyên biệt trong định dạng Wikipedia. Việc mở rộng phạm vi có thể là công việc trong tương lai.

5.2 Webvicob với Augraphy

Chúng tôi mong đợi Webvicob có thể dễ dàng được tích hợp với bất kỳ dự án augmentation nào, chẳng hạn như Augraphy [31], để tăng cường hiệu suất hơn nữa. Như có thể thấy trong Hình 6, việc kết hợp Webvicob và Augraphy có thể tạo ra hình ảnh tài liệu với các hiệu ứng hình ảnh phong phú.

5.3 Phân Tích PCA

Chúng tôi lấy mẫu 3,626 điểm dữ liệu từ mỗi bốn tập dữ liệu (Webvicob, IIT-CDIP, DocVQA Task 1, và DocVQA Task 3), tạo ra tổng cộng 14,504 vector cho phân tích PCA. Chúng tôi xây dựng từ vựng gồm 100,000 từ sử dụng dữ liệu Wikipedia 1M và biểu diễn dữ liệu bằng biểu diễn Bag of Words (BOW), loại trừ stop words. Các hình [7, 8] trong hàng thứ i hiển thị phân tích giữa thành phần chính thứ i và (i+ 1). Ví dụ, hàng đầu tiên hiển thị biểu đồ của thành phần chính thứ 0 và thứ 1.

Chúng tôi trực quan hóa dữ liệu bằng 10 thành phần chính và nhận thấy rằng các vector BOW của IIT-CDIP và DocVQA Task 1 rất tương tự, trong khi Webvicob chứa một loạt các vector BOW đa dạng.

Trực quan hóa thêm với từ vựng Google có thể được tìm thấy trong các hình [9, 10].

6 Kết Luận

Trong công trình này, chúng tôi đề xuất một công cụ, Webvicob, xây dựng corpus hình ảnh từ tài nguyên web. Corpus hình ảnh được xây dựng có thể được sử dụng để xây dựng backbone VDU. Trong các thí nghiệm và phân tích của chúng tôi, chúng tôi quan sát thấy rằng dữ liệu do Webvicob tạo ra giúp backbone VDU hoạt động mạnh mẽ trên nhiều định dạng và miền tài liệu khác nhau.

7 Lời Cảm Ơn

Các tác giả cảm ơn NAVER CLOVA Text Vision Team và Information Extraction Team.
