# 2306.01733.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/document-parsing/2306.01733.pdf
# File size: 2759793 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
DocFormerv2: Local Features for Document Understanding
Srikar Appalaraju1*, Peng Tang1, Qi Dong1, Nishant Sankaran1, Yichu Zhou2†, R. Manmatha1
1AWS AI Labs2School of Computing at University of Utah
{srikara, tangpen, qdon, nishsank, manmatha }@amazon.com, flyaway@cs.utah.edu
Abstract
We propose DocFormerv2, a multi-modal transformer
for Visual Document Understanding (VDU). The VDU do-
main entails understanding documents (beyond mere OCR
predictions) e.g., extracting information from a form, VQA
for documents and other tasks. VDU is challenging as it
needs a model to make sense of multiple modalities (visual,
language and spatial) to make a prediction. Our approach,
termed DocFormerv2 is an encoder-decoder transformer
which takes as input - vision, language and spatial fea-
tures. DocFormerv2 is pre-trained with unsupervised tasks
employed asymmetrically i.e., two novel document tasks on
encoder and one on the auto-regressive decoder. The un-
supervised tasks have been carefully designed to ensure
that the pre-training encourages local-feature alignment
between multiple modalities. DocFormerv2 when evalu-
ated on nine datasets shows state-of-the-art performance
over strong baselines e.g. TabFact (4.3%), InfoVQA (1.4%),
FUNSD (1%). Furthermore, to show generalization ca-
pabilities, on three VQA tasks involving scene-text, Doc-
Formerv2 outperforms previous comparably-sized models
and even does better than much larger models (such as
GIT2, PaLi and Flamingo) on some tasks. Extensive ab-
lations show that due to its pre-training, DocFormerv2 un-
derstands multiple modalities better than prior-art in VDU.
1. Introduction
Documents have become ubiquitous carriers of informa-
tion, including forms, tables, invoices, and other structured
documents. Many such documents require visual and layout
understanding to make sense (just the text string is insuffi-
cient). Visual Document Understanding (VDU) is the task
of leveraging machine learning techniques to comprehend
such scanned documents, such as PDFs or images. Popular
VDU tasks include Document and Tables VQA [9, 49], se-
quence labeling for key-value identification in forms [27],
entity extraction [58], and document classification [19].
*Corresponding author.
†Work conducted during an internship at Amazon.
Figure 1: Visual Document Understanding . Snippet of a doc-
ument receipt from DocVQA [48]. VDU tasks could include a
model asked to predict ”SOLD TO” address (VQA) or predict all
relations (”SOLD TO” →<address >, ”SHIP TO” →<address >)
or asked to infer info from table (at the top).
While modern deep-learning based OCR models [41] have
proven to be effective in extracting text from documents,
the naive approach of linearizing the OCR-text and feed-
ing it to a language model is sub-optimal. This is because
the content of a document is presented according to a visual
layout and structure that must be taken into account for ac-
curate understanding. Naively linearizing the text from left-
to-right will result in sub-optimal performance as the se-
mantic meaning alters based on layout, as shown in Figure
1 - Table 6,5 has experiments demonstrating this. Instead,
VDU requires a multi-modal approach that can comprehend
text and visual features in the context of a document’s 2D
layout.
Multi-modal training in general entails feature align-
ment. Specific to vision-language learning this means align-
ing a piece of text with an arbitrary span of pixels in visual
space [1–3, 5, 10, 18, 21, 31, 37, 54, 64]. How those features
are aligned makes a big difference. In VDU, a majority of
the tasks require local and layout-relative understanding of
the document. For example, in document VQA, semantic
labeling or entity extraction, a model needs to make sense
of text in-relation to where the text is placed in a document.
E.g.: ”1” when placed at the top-right/bottom-left of a doc-
ument is to be interpreted as a page-number vs as a number
when placed anywhere else.
Based on this domain understanding of VDU and its
challenges, we present DocFormerv2 (DFv2) which is an
encoder-decoder multi-modal transformer. In this work,
we meticulously devise two novel unsupervised pre-trainingarXiv:2306.01733v1  [cs.CV]  2 Jun 2023

--- PAGE 2 ---
Model Year Conf. Arch. Input Mod. Vision Branch
LayoutLMv1 [66] 2020 KDD E T + S -
DocFormerv1 [2] 2021 ICCV E T + V + S Resnet50
LayoutLMv2 [67] 2021 ACL E T + V + S ResNeXt 101
SelfDoc [39] 2021 CVPR E - -
LayoutLMv3 [25] 2022 ACM E T + V + S Linear
BROS [22] 2022 AAAI E T + S -
XYLayoutLM [16] 2022 CVPR E T + V + S ResNeXt 101
FormNet [33] 2022 ACL E - -
ERNIE-Layout [52] 2022 EMNLP E T + V + S F-RCNN
LiLT [63] 2022 ACL E T + S -
XDoc [8] 2022 EMNLP E T -
TILT [53] 2021 ICDAR E + D T + V + S U-Net
DocFormerv2 (ours) 2023 - E + D T + V + S Linear
Table 1: VDU Related Work : In this table, a summary of VDU
prior art is presented with their architecture (E: Encoder, D: De-
coder), the input (T: text, V: vision, S: spatial features), the vision
features branch and core idea behind the work.
Figure 2: VDU Paradigms: Broad state of Visual Document Un-
derstanding (VDU) approaches. In a)E-only LayoutLM [66] and
variants. b)E+D but only language-task TILT [53]. c)Ours
tasks with the objective of incorporating local semantic in-
formation of a document into the model. These pre-training
tasks impart the ability to the model to accurately locate
relevant information within the document. We also depart
from VDU prior-art [53, 60] as we introduce a novel asym-
metrical method of pre-training. i.e., multi-task pre-training
on encoder (two tasks) and decoder (one task). We pro-
pose two novel pre-training tasks on encoder with the in-
tent to enrich the encoder with local semantic information.
The tasks aid in by fusing and aligning multi-modal in-
put and generating efficient representations for the decoder.
We show that these pre-training tasks are necessary for ef-
fective VDU (see §5). Furthermore, we demonstrate that
a simplified linear visual layer is sufficient to encapsulate
visual features, simplifying the architecture from previous
VDU research [39, 53, 67] which required specific visual
encoders [13, 20, 44].
Experimentally we demonstrate that DocFormerv2
achieves state-of-the-art performance on five VDU tasks.
In addition, we demonstrate the versatility of DocFormerv2
by utilizing its pre-trained model and fine-tuning it on text-
VQA tasks from a completely different domain. Our ap-
proach yields superior performance on three distinct text-
VQA datasets, surpassing comparable models and in somedatasets much bigger models like GIT2 [64], PaLi [10] and
Flamingo [1]. Therefore, the primary contributions of this
paper are as follows:
• Asymmetrical method of pre-training for VDU: Two
novel tasks on the encoder which encourage local multi-
modal feature collaboration ( Token-to-Line task and
Token-to-Grid task) and one on the decoder §3.2.
• Simplified Visual branch: DocFormerv2 is end-to-end
trainable and it does not rely on a pre-trained object de-
tection network for visual features simplifying its archi-
tecture. On five varied downstream VDU tasks, Doc-
Formerv2 achieves state of the art results §4.
• We also show DocFormerv2 versatility by fine-tuning it
on a totally different domain - text-VQA datasets with-
out changing the pre-training. DocFormerv2 beats strong
baselines and achieves state-of-the-art numbers on three
text-VQA datasets amongst similar model sizes. Selec-
tively, on Text-VQA it out-performs much larger models
like PaLi-3B +6.8%, PaLi-15B +1.5% and Flamingo [1]
(+9.9%) (106x DocFormerv2 size in the num. of param-
eters) by absolute accuracy §4.5.
Furthermore, we conducted comprehensive ablation exper-
iments to demonstrate the advantages of our pre-training
tasks, the model’s resilience to input noise, and the efficacy
of the simplified visual branch.
2. Related Work
VDU research has attracted considerable attention over
the past few years [2,8,15,16,23,25,33,36,38,39,52,53,60,
63, 65–67, 70]. Prominent published research papers in this
area are catalogued in Table 1. As can be observed, the re-
search focus has been lopsided towards encoder-only mod-
els. While TILT [53] proposed a encoder-decoder trans-
former for VDU, they only train it on one pre-training task
(masked language modeling) and also use a bulky visual
CNN. Our approach DocFormerv2 , not only simplifies the
architecture by not using a separate visual module (CNN
or Transformer based) and has multiple unsupervised pre-
training tasks.
3. Approach
3.1. Architecture
DocFormerv2 (DFv2 ) is a multi-modal encoder-decoder
transformer architecture (see fig. 3). Three variations of
DFv2 are designed - small, base and large variants (see table
2). DFv2 takes multi-modal inputs, the image of the docu-
mentI, textTextracted by an OCR model along with OCR
bounding box co-ordinates as spatial features S. DFv2 has

--- PAGE 3 ---
Figure 3: DocFormerv2 Pre-train Architecture . After pre-train, the two prediction heads (token-to-line and grid) on encoder are removed,
rest of the architecture remains the same for down-stream tasks. Read section 3.1 for more details on TsandVs. All components are end-
to-end trainable. Best viewed in color.
a unified multi-modal encoder where the multi-modal fea-
tures fuse and align with the help of novel pre-training tasks
(see §3.2).
Visual features: DFv2 has a simplified visual branch con-
trary to most VDU prior-art (fig. 2). DFv2 consumes
a flattened image sequence as visual input. Specifically,
letv∈R3×h×wbe the image of a document. A sim-
pleV=linear (conv 2×2(v))is used to create an image
embedding. The weights are randomly initialized for pre-
training. As documents tend to have lots of white-space,
the linear down-sampling layer gives an opportunity for
the model to only keep relevant visual features. Based on
our ablation experiments (see §12), this simple approach
gives better results than using expensive image encoders
such as Swin, ViT [13,44,57] or bulky object-detection net-
works like FRCNN variants [56] as was used in VDU prior-
art [2, 53, 68]. Since transformer layers are permutation-
invariant, a learnable 2D-positional encoding Psis also
computed. Finally, Vs=V+Ps
Language features: Lettbe the predicted text extracted
via an OCR model for a document image. DFv2 uses a
sentence-piece sub-word tokenizer [32] to get tokens ttok.
A maximum sequence limit sis applied during training and
testing, so if the number of OCR tokens is greater than s,
the rest is ignored. If the sequence length is less than s,
the sequence is padded. The OCR tokens ttokare sent to
a learnable embedding layer Wtto create a text embedding
T=Wt(ttok).
Spatial features: For each OCR word ti, the OCR modelmodel dim ff # attn. H # layers (E,D) # params
small 512 2048 8 6,6 66M
base 768 3072 12 12,12 232M
large 1024 4096 16 24,24 750M
Table 2: DocFormerv2 variants : dim is embedding dimensional-
ity. ff is output dim of feed-forward layer. E is encoder and D is
decoder. attn. H is attention heads.
predicts its bounding-box location in the normalized form
bi= (x1, y1, x3, y3). This information is encoded using
four learnable spatial embedding layers - Wxfor encoding
a word horizontal spatial information xi,Wyfor the vertical
coordinate yi,Whfor word height hiandWwfor the width
wi. The spatial features not only encode the location of a
word in the document but also provides cues about a word’s
font-size and thereby its importance in a document (via hi
andwi). Specifically, spatial features S=Wx(x1, x3) +
Wy(y1, y3) +Wh(y3−y1) +Ww(x3−x1). Finally, Ts=
T+S.
Other features: TsandVsfeatures are different modal-
ities (fig. 3). As the model has no idea it is being fed
multi-modal input, another learnable embedding Wmis
used to provide cues to the model about the multi-modal
input. A modality-embedding Wmlearns nuances of dif-
ferent modalities, which generates Mvembedding for vi-
sual modality and Mtfor text. Finally, T=Ts+Mtand
V=Vs+Mv.TandVare concatenated in the sequence
dimension to form the input sequence to the DFv2 encoder.

--- PAGE 4 ---
Figure 4: Token-to-Line
 Figure 5: Token-to-Grid 4x4
3.2. Unsupervised Document Pre-training
In DocFormerv2 we follow the now well established
practice of unsupervised pre-training followed by down-
stream task fine-tuning. Furthermore, with the intent of elic-
iting the maximum benefit from unsupervised pre-training,
we designed the pre-training tasks as a close proxy for
downstream tasks. We now describe the two novel pre-
training tasks employed on the encoder and the language
modeling task on decoder. All three tasks are performed at
the same time and the final loss is a linear combination of
all three losses for each iteration.
Encoder Token-to-Line Task: We share the intuition that
for VDU tasks local feature semantic alignment is impor-
tant. Most of the related information for key-value predic-
tion in a form or VQA is either on the same line or adja-
cent lines of a document e.g., see fig. 4, in order to predict
the value for "TOTAL" (box a), the model has to look in
the same line (to its right - "$4.32" box d). We teach
the model the relative position information between tokens.
For implementation, we randomly pick two language tokens
and ask the model to predict the number of lines between
them. Furthermore, as a document could have an arbitrary
number of lines of text, the task is quantized. i.e., there
are only three labels: {0, 1, 2} . All token pairs that
are more than 2 lines apart are labelled as 2 because distant
tokens are not likely related and the model should learn to
ignore them. Assume that a, b, c, d (fig. 4) are lines. Let F
be the DFv2 encoder head function trying to predict a label
for this task. then:
F(a, d) = 0; F(a, b) = 1; F(b, c) = 2 (1)
Based on the ablation (table 10), this task gives +2.2% ben-
efit on DocVQA task. The loss for this task is tracked as
Ltol.
Encoder Token-to-Grid Task: Different semantic infor-
mation is concentrated in different regions of the document.
For example, a) In a financial document, the top block con-
tains the header, the middle block contains information tobe filled and the bottom block typically contains footer el-
ements/instructions. b) Page numbers are typically at the
top or the bottom. c) In a receipt/invoice the company name
is typically at the top. The content of a document is pre-
sented according to a visual layout and structure that must
be taken into account for accurate understanding. Based on
this intuition, this task pairs language semantics with the lo-
cation (visual, spatial or both) in a document. Specifically,
the document is virtually divided into a m x n grid. Each
OCR token is assigned a grid number and DFv2 is tasked
with predicting the grid number for each token. For each
OCR token ti, its top-left location (x1, y1)is used to deter-
mine its grid-number gi. Grids are in raster-scan order, so
if a particular token falls on the boundary of multiple grids,
the scan-order is used to disambiguate. If a token falls on
the boundary of normalized image co-ordinates, they are
ignored for prediction. See fig. 5 for viz. Specifically, we
have:
gi=⌊x1
∆x⌋+⌊y1
∆y⌋ ·m,
where ∆xand∆yare the widths and heights of each grid,
respectively, and mis the number of grids in a row. The
lossLtog.
Decoder Language Modeling: Since VDU predictions are
in the language domain, language understanding forms an
important component of DFv2 pre-training. We do the de-
noising masked language modeling popularized by T5 [55].
During pre-training, not only are the input tokens randomly
MASKED it’s spatial features (mentioned in §3.1) are also
masked. Masking the spatial features Sfor the masked to-
kens makes the grid prediction and line prediction hard be-
cause the model does not have 2D-position information of
the masked tokens. It has to infer from other available con-
text. The loss for this operation is denoted Ldlm.
Final pre-training loss: The final loss is a linear combina-
tion of all three pre-training losses i.e., Lfinal =k∗Ltol+
l∗Ltog+m∗Ldlm, where k, l, m are empirically deter-
mined.
Downstream Tasks: Once pre-training is done, we remove
the token-to-line and token-to-grid linear prediction heads.
The rest of the pre-trained model is fine-tuned on the re-
spective downstream train data.
4. Experiments
Implementation details : Following prior-art [2, 5, 25, 53,
66, 68] we use the Industrial Document Library (IDL)1
dataset for pre-training. The IDL is a collection of indus-
try documents hosted by UCSF. It hosts millions of doc-
uments publicly disclosed from various industries like to-
bacco, drug, food etc. The data from the website amounts
to about 13M documents, translating to about 70M pages
of various document images. We further extracted OCR
1https://www.industrydocuments.ucsf.edu/

--- PAGE 5 ---
for each document. Data was cleaned and about 6M doc-
uments were pruned, the resulting 64M document images
and OCR-text (with spatial co-ordinates) is used for unsu-
pervised pre-training. The data distribution for IDL 64M is
presented in supplemental section.
Downstream experiments: The model is fine-tuned on the
provided training set and numbers are reported on the cor-
responding validation/test set. No dataset specific hyper-
parameter tuning is done. This is an advantage of our
approach and we believe that the numbers may be higher
if dataset specific fine-tuning is done. Details about fine-
tuning datasets are in the supplemental section. We used
Pytorch [51] and the Huggingface library [61].
Evaluation Metrics: A dataset specific evaluation metric is
adopted. For DocVQA [49], InfoVQA [47], ST-VQA [7],
Average Normalized Levenshtein Similarity (ANLS) [6]
is used. ANLS measures the similarity between the pre-
dicted results and ground truth and ranges from (0,100). For
FUNSD [27], CORD [58] F1-score is used. For TextVQA
[59] and OCR-VQA [50] accuracy is used. In all metrics,
higher the better.
4.1. Table VQA
TabFact [9, 70]: This dataset studies table understand-
ing and fact verification with semi-structured evidence over
tables collected from Wikipedia. Entailed and refuted state-
ments corresponding to a single row or cell were prepared
by the authors of TabFact [9]. This task poses challenges
due to the complex linguistic and spatial reasoning in-
volved. In Table 3, we can see that DocFormerv2 out-
performs prior art by a large margin (+4.3%).
Modelpre-train
data#paramTabFact
Acc. (%)
methods based on only text / (text + spatial) features:
T5large[55] - 750M 58.9
T5large+U [70] - 750M 76.0
T5large+2D [70] - 750M 58.0
T5large+2D+U [70] - 750M 78.6
methods based on image + text + spatial features:
LayoutLMv3 large[25] 11M 368M 78.1
UDOP [60] 11M 794M 78.9
DocFormerv2 large 64M 750M 83.2 (+4.3%)
Table 3: Comparison on Table VQA Datasets [70]: Our work,
DocFormerv2 outperforms the previous state of the art.
4.2. Document VQA
DocVQA [49] and InfographicsVQA [47] are datasets
for the document VQA task. DocVQA [49] focuses on
VQA for real-world industry documents and requires that
the model understand images, texts, tables, forms, etc. In-
fographicsVQA [47] focuses on VQA for infographics and
requires that the model understand plots/graphs, texts, lay-out, figures. A model needs to reason multi-modally to gen-
erate an answer for this data. Please see the supplemental
for data statistics and samples.
Modelpre-train
data#paramDocVQA
test ANLS (%)InfoVQA
test ANLS (%)
methods based on only image:
Donut base[30] 13M 143M 67.5 11.5
Pix2Struct large[34] 80M 1.3B 76.6 40.0
methods based on only text / (text + spatial) features:
T5large[55] - 750M 70.4 36.7
T5large+U [70] - 750M 76.3 37.1
T5large+2D [70] - 750M 69.8 39.2
T5large+2D+U [70] - 750M 81.0 46.1
methods based on image + text + spatial features:
LayoutLMv3 large[25] 11M 368M 83.4 45.1
UDOP [60] 11M 794M 84.7 47.4
LayoutLMv2†
large[67] 11M 426M 86.7 -
TILT†
large[53] 1.1M 780M 87.05 -
DocFormerv2 large 64M 750M 87.2 -
DocFormerv2†
large 64M 750M 87.84 (+0.79%) 48.8 (+1.4%)
Table 4: Comparison on Document VQA Datasets [47,49]: Our
work, DocFormerv2 outperforms the previous state of the art.†
indicates training with extra document VQA data.
Following common practice [53, 67, 70], we train Doc-
Formerv2 on the combination of the training and validation
sets and do evaluation on the test set for each dataset. In
addition, we also follow [53, 67] to train DocFormerv2 on
an extra document VQA dataset with 850k question-answer
pairs and then fine-tune on DocVQA/InfographicsVQA for
higher accuracy.
DocFormerv2 outperforms (Table 4) the previous state
of the art for document VQA even without using any extra
document VQA pre-training data. After pre-training on the
extra data, DocFormerv2 surpasses the previous state of the
art by 0.79% on DocVQA and 1.4% on InfographicsVQA,
which confirms the effectiveness of our approach.
4.3. Sequence Labeling Task
We study the performance of DocFormerv2 on the se-
mantic entity-labeling task (i.e., group tokens which belong
to the same class). We test the model on FUNSD dataset
[27], which is a forms dataset containing 199 noisy docu-
ments (149 images for train, 50 images for test). There are
four classes: question, answer, header , and other . We mea-
sure entity-level performance using F1 score (Table 5). The
input sequence to Docformerv2 includes individual texts
as prompts and all document texts as context, and the de-
coder sequence contains the entity texts and predicted la-
bels. Docformerv2 achieves 88.89% F1 score (Table 5), and
outperforms the existing methods without using entity box
priors in pretraining and finetuning (grayed models in the
table).

--- PAGE 6 ---
Model #param Precision Recall F1
methods based on only image:
Dessurt base[11] 127M - - 65.0
methods based on only text / (text + spatial) features:
BERT base[12] 109M 54.69 61.71 60.26
RoBERTa base[42] 125M 63.49 69.75 66.48
UniLMv2 base[4] 125M 63.49 69.75 66.48
LayoutLMv1 base[66] 113M 76.12 81.55 78.66
BROS base[22] 139M 80.56 81.88 81.21
BERT large[12] 340M 61.13 70.85 65.63
RoBERTa large[42] 355M 67.80 73.91 70.72
UniLMv2 large[4] 355M 67.80 73.91 70.72
LayoutLMv1 large[66] 343M 75.36 80.61 77.89
StructuralLM large[36] 355M 83.52 86.81 85.14
FormNet [33] 217M 85.21 84.18 84.69
methods based on image + text + spatial features:
LayoutLMv1 base[66] 160M 76.77 81.95 79.27
LayoutLMv2 base[67] 200M 80.29 85.39 82.76
LayoutLMv2 large[67] 426M 83.24 85.19 84.20
DocFormer base[2] 183M 80.76 86.09 83.34
DocFormer large[2] 536M 82.29 86.94 84.55
SelfDoc [39] - - - 83.36
UDoc [15] 272M - - 87.93
StrucTexT [40] ✢ 107M 85.68 80.97 83.09
LayoutLMv3 base[25]❅ 133M 77.39 81.65 79.46
LayoutLMv3 large[25]❅ 368M 81.35 83.75 82.53
LayoutLMv3 base[25]❍ 133M 89.55 91.65 90.29
LayoutLMv3 large[25]❍ 368M 92.19 92.10 92.08
UDOP [60] ❍ 794M - - 91.62
DocFormerv2 base 232M 89.15 87.6 88.37
DocFormerv2 large 750M 89.88 87.92 88.89
Table 5: FUNSD comparison : DocFormerv2 does better than
models its size and compares well with even larger models. ✢
does not use standard train/test split, and the results are not di-
rectly compared with others. ❍use OCR lines (not word box)
as 2D position for words, and use entity boxes as 2D position for
each word during finetuning and test, and thus the results are not
directly comparable. ❅are results by using the word boxes as 2D
position for each word as other competitors do.
4.4. Entity Extraction Task
We evaluate DocFormerv2 for the entity extraction task
on the CORD dataset. CORD [58] consists of 1000 receipts
(800/100/100 images for train/val/test). It defines 30 fine-
grained fields under 4 coarse-grained categories. To ex-
tract all entities, in the input sequence, we add a question
of “What are entities of <CLASS >?” in front of all text
context tokens. The output of the decoder includes all enti-
ties which are separated by a separator token. Following the
standard evaluation metric for entity extraction, we measure
entity-level performance using F1 score. Docformerv2 (Ta-
ble 6) achieves 97.7% F1 score, and outperforms existing
methods. Docformerv2 enables multiple entities decoding
in an auto-regressive way which shows that the model is
able to learn both intra-entity and inter-entity structures.
4.5. Generalization Experiments - Scene-Text VQA
In this section, we show the strength of DocFormerv2
on a different task - Text-VQA. Unlike document under-Model #param Precision Recall F1
methods based on only text / (text + spatial) features:
BERT base[12] 109M 88.33 91.07 89.68
UniLMv2 base[4] 125M 89.87 91.98 90.92
SPADE [26] - - - 91.50
LayoutLMv1 base[66] 113M 94.37 95.08 94.72
BROS base[22] 139M 95.58 95.14 95.36
BERT large[12] 340M 88.86 91.68 90.25
RoBERTa large[42] 355M - - 93.80
UniLMv2 large[4] 355M 91.23 92.89 92.05
LayoutLMv1 large[66] 343M 94.32 95.54 94.93
FormNet [33] 345M 98.02 96.55 97.28
methods based on image + text + spatial features:
LayoutLMv2 base[67] 200M 94.53 95.39 94.95
LayoutLMv2 large[67] 426M 95.65 96.37 96.01
TILT base[53]❍ 230M - - 95.11
TILT large[53]❍ 780M - - 96.33
DocFormer base[2] 183M 96.52 96.14 96.33
DocFormer large[2] 536M 97.25 96.74 96.99
UDoc [15] 272M - - 96.86
LayoutLMv3 base[25]❅ 133M 92.92 94.31 93.61
LayoutLMv3 large[25]❅ 368M 96.78 96.78 96.78
LayoutLMv3 base[25]❍ 133M - - 96.56
LayoutLMv3 large[25]❍ 368M - - 97.46
UDOP [60] ❍ 794M - - 97.58
DocFormerv2 base 232M 97.51 96.10 96.80
DocFormerv2 large 750M 97.71 97.70 97.70
Table 6: CORD dataset [58]comparison . We present entity-level
Precision, Recall, F1 on test set. ❍use OCR lines (not word box)
as 2D position for words, and use entity boxes as 2D position for
each word during finetuning and testing, and thus the results are
not directly comparable. ❅are results by using the word boxes as
2D position for each word as the other competitors do.
Modelpre-train
data#paramVal
Acc. (%)Test
Acc. (%)
Blk+CNN+W2V - - - 48.3
M4C [24] - 200M 63.5 63.9
LaAP [17] - - 63.8 64.1
LaTr base[5] 64M 311 67.5 67.9
GIT base 10M 129M 57.3 57.5
GIT large 20M 347M 62.4 62.9
GIT 800M 681M 67.8 68.1
GIT2✢[64] 12.9B 5.1B - 70.3
DocFormerv2 base 64M 232M 69.7 70.3
DocFormerv2 large 64M 750M 71.1 71.5 (+3.4%)
Table 7: Comparison on OCR-VQA [50]: DocFormerv2 is bet-
ter than the previous SOTA by (+3.4%). Bold indicates best and
underline indicates the previous state of the art. GIT2 ✢: uses ex-
tra VQA data (aggregation of 8 VQA datasets).
standing which focuses on document images, the Text-VQA
task answers questions for natural images with scene text.
We fine-tune our document pre-trained models on three
Text-VQA datasets. We emphasize that no image-text pre-
training was performed on DocFormerv2, it was merely
fine-tuned on the respective Text-VQA training dataset.
Three popular Text-VQA datasets are used - OCR-VQA

--- PAGE 7 ---
Modelpre-train
data#paramVal
Acc. (%)Test
Acc. (%)
M4C [24] - 200M 47.8 -
LaAP [17] - - 41.0 41.4
SA-M4C [28] - 200M 45.4 44.6
SMA - - 44.6 45.5
SceneGate [46] - - 42.4 44.0
SC-Net [14] - - 44.8 45.7
LOGOS [45] - - 51.5 51.1
TAP + TAG [62] - - 53.6 53.7
TAP [69] - 200M 54.7 54.0
TAP Two-Stage [35] - 200M 55.9 55.3
Flamingo [1] 2.3B★ 80B 57.1 54.1
PreSTU [29] 13M 237M - 56.3
LaTr base[5] 64M 311M 58.0 58.9
LaTr†
base[5] 64M 311 59.5 59.6
LaTr†
large[5] 64M 856M 61.0 61.6
GIT base 10M❍ 129M 18.8 -
GIT large 20M❍ 347M 37.5 -
GIT 800M❍ 681M 59.9 59.8
GIT2✢[64] 12.9B❍ 5.1B 68.4 67.3
PaLi-3B [10] 1.6B❍ 3B 58.8 -
PaLi-15B [10] 1.6B❍ 15B 64.1 -
PaLi-17B [10] 1.6B❍ 17B 70.5 73.1
DocFormerv2†
base 64M 232M 61.6 60.0
DocFormerv2†
large 64M 750M 65.6 64.0 (+2.4%)
Table 8: Comparison on TextVQA [59]:†indicates the model
used the combination of ST-VQA and TextVQA training sets to
train the model. GIT2 ✢: extra data used (aggregation of 8 VQA
datasets) ★: video-text data. ❍: proprietary image-text data. Grey
rows shows models which are much bigger (# parameters ≥3x
DFv2 largeparameters) and use large amounts of external data.
DFv2 largestill outperforms Flamingo (+9.9%), Pali-3B (+6.8%)
and Pali-15B (+1.5%) models.
Modelpre-train
data#paramVal
ANLS (%)Test
ANLS (%)
M4C [24] - 200M 47.2 46.2
LaAP [17] - - 49.7 48.5
SA-M4C [28] - 200M 51.2 50.4
SceneGate [46] - - 52.5 51.6
LOGOS [45] - - 58.1 57.9
TAP [69] 200M 59.8 59.7
TAP + TAG [62] - - 62.0 60.2
PreSTU [29] 13M 237M - 65.5
LaTr base[5] 64M 311M 67.5 66.8
LaTr†
base[5] 64M 311M 68.3 68.4
LaTr†
large[5] 64M 856M 70.2 69.6
GIT base 10M❍ 129M 20.7 -
GIT large 20M❍ 347M 44.6 -
GIT 800M❍ 681M 69.1 69.6
DocFormerv2†
base 64M 232M 70.1 68.4
DocFormerv2†
large 64M 750M 72.9 71.8 (+2.2%)
Table 9: Comparison on ST-VQA [7]:†indicates the combina-
tion of the ST-VQA and TextVQA training sets is used.
[50], TextVQA [59] and ST-VQA [7], each with strong
baselines from the vision-language community (as is stan-dard practice by Text-VQA we mean any scene text VQA
dataset while TextVQA refers to a specific dataset). Please
see the supplemental for a dataset breakdown. For OCR-
VQA, we fine-tune our models on the training set and do
evaluation on the validation and test sets. For TextVQA
and ST-VQA, following the previous state-of-the-art meth-
ods [5, 69], we fine-tune our models on the combination of
the TextVQA and ST-VQA training sets and do evaluation
on the validation and test sets of each dataset. Tables 7, 8, 9
show that our large size model outperforms the comparably
sized previous state-of-the-art method LaTr [5] by +3.4%,
+2.4% and +2.2% on the OCR-VQA, TextVQA, and ST-
VQA test sets respectively. These results show that our
method generalizes beyond document understanding tasks.
Analysis: Surprisingly, on OCR-VQA,
DocFormerv2 large even performs better than GIT2 [64]
which is a 5.1B size model ( vs. 750M for DocFormerv2 large)
and uses 12.9B data for pre-training ( vs. 64M for
DocFormerv2 large). On TextVQA, DocFormerv2 does
better than several vision-language models which are much
bigger and have been pre-trained on much more data. On
the test set, it is (+9.9%) better than Flamingo (which at
80B has 106x the number of parameters). On the validation
set, it is better than PaLi-3B and 15B (+2.2%, +6.8%)
respectively. GIT2 and PaLi-17B do perform better than it.
(GIT2 also uses 8 VQA datasets to train). DocFormerv2
gets this performance without any natural image-text
pre-training. We present this as evidence that DocFormerv2
is a good approach to solving this problem with a much
smaller model and much less data.
5. Ablation Experiments
Ablation of DFv2 novel pre-training tasks : Table 10 shows
DFv2 ablation on the proposed novel pre-training tasks and
multi-modal training. The denoising language modeling
task and spatial features mentioned in §3.1 are applied to all
architectures. Note, this ablation was performed on DFv2
-small with 1M doc pre-training.
Model AblationDatasets
DocVQA (ANLS)
baseline B 69
B + V 70.5 (+1.5)
B + V + L 71.2 (+2.2)
B + V + G 71.7 (+2.7)
B + V + L + G 73.0 (+4.0)
Table 10: DocFormerv2 Pre-training Tasks Ablation : Impact
of three pre-training tasks on four downstream tasks over baseline.
B:baseline, V:only with Visual features §3.1, L:with Token-to-
Line prediction pre-training §3.2, G:with Token-to-Grid predic-
tion pre-training §3.2.
Pre-training Impact or Better Approach? DFv2 was pre-
trained with 64M documents whereas prior-art like Lay-
outLMv2 [67] was pre-trained with only 11M documents.

--- PAGE 8 ---
Model# pre-train data Datasets
FUNSD CORD
LayoutLMv2 base[67] 11M 82.7 94.9
DocFormerv2 base 11M 86.1 (+3.4%) 96.2 (+1.3%)
DocFormerv2 base 64M 87.9 (+5.2%) 96.8 (+1.9%)
DocFormerv2†
base 64M 88.3 (+5.6%) 96.8 (+1.9%)
Table 11: DocFormerv2 Pre-training Data Ablation : Impact
of training with different # of pre-training data on various down-
stream tasks. The F1 scores are reported.†indicates the combina-
tion of the ST-VQA and TextVQA training sets is used.
In order to see if DFv2 benefits come from more pre-
training data or a better approach, we ablate. Table 11
shows that DFv2 baseis superior to LayoutLMv2 basewhen
pre-trained on the same quantity of data. We also see DFv2
improve in performance as more pre-training data is pro-
vided (64M). The table shows that the novel DFv2 asym-
metric pre-training approach is a superior VDU approach.
Robustness to OCR errors .DFv2 consumes OCR text
which can have errors. Since it also has a generative de-
coder, in theory it is robust to certain distortions and noise
from OCR-text. To quantify the degree of robustness, we
conduct a study using the FUNSD dataset and artificially in-
troduce noise/typographical errors to the input words, sim-
ulating OCR errors. Specifically, for every character in the
text, we randomly replace it with an erroneous character
with a probability p, limiting to a maximum of 1 character
error per word. We then evaluate the performance of the
DocFormerv2 baseand LayoutLMv2 basemodels on the error
injected text to observe their resilience to such noise. From
Figure 6, we see that for increasing amount of injected OCR
errors. Specifically, 20% OCR errors only decreases the
performance by -1.68% whereas an encoder-only model de-
creases by a wide margin -9.84%. This shows the benefit of
our approach (in having a generative decoder).
81.9881.180.0678.4274.2272.1466.8388.3788.0687.787.487.0986.6985.96065707580859005101520301InducedOCRError(%)F1 score (%)LayoutLMv2DocFormerv2
Figure 6: Induced OCR Error Ablation . F1 score performance
evaluated on FUNSD for varying orders of injected OCR errors.
Varying Image Tokens. Concatenating the image tokens
along with the text tokens is a simple and intuitive approach
for the model to learn to jointly capture multi-modal infor-
mation. However, since we are limited on the total number
of tokens we can use, it begs the question - what is a suit-
able proportion of vision-to-text tokens to be used for this
design? We perform ablations in this regard to measure the
performance obtained by finetuning the model on FUNSD
Figure 7: Image token
length ablation . Effect on
model performance w.r.t
variation of the proportion
of vision tokens to text to-
kens provided as input to
the model. 128 works best
and was used as the final
model design.
Figure 8: Token-to-
Grid Ablation . How
different grid sizes used
for the Token-to-Grid
pre-training task affects
model performance on
DocVQA. 4x4 seems best
and was used for all final
pre-training.
dataset with varying ratios of vision tokens to text tokens.
Figure 7 shows that 128image tokens appears to provide
the best performance compared to the other settings.
Do we need a separate Image encoder? We investi-
gate if DocFormerv2 ’s way of consuming visual features
is optimal for VDU. Instead of using linear features, we
use Swinv2 [43] and pre-train this setup on 1M docu-
ments. When fine-tuned on DocVQA we observe that the
setup with Swinv2 as visual backbone, substantially under-
performs our approach by (+4.3%). For this task, the com-
plex visual features from Swinv2 are less beneficial than our
simple linear features.
Model image encoder DocVQA [49] eval ANLS (%)
baseline - 69.0
DocFormerv2 small Swinv2 small[43] 66.2
DocFormerv2 small Linear (ours) 70.5 (+4.3%)
Table 12: Image Encoder Ablation : All models pre-trained on
1M docs from IDL. Swinv2 too was pre-trained and fine-tuned.
Correct grid size for Token-to-Grid pre-training? In
§3.2, we presented the novel Token-to-Grid pre-training
task. In this pre-training ablation §10 this task was observed
to provide benefits. Here the appropriate virtual grid-size is
empirically determined. From Fig. 8, 4x4 grid seems op-
timal. Smaller or asymmetric grid structures (4x1) seem to
cause harm. On the other end, if the grid is too granular
(12x12, 8x8), the performance seems to hurt as well. All
models pre-trained on DFv2 small and 1M documents from
IDL, with the Vision and Token-to-line enabled.
6. Conclusion
Our work DocFormerv2 highlights the importance of
two novel pre-training tasks and the efficacy of enriching
encoder representations with local semantic information via
pre-training tasks. We perform experiments on eight var-
ied datasets (five on VDU and three on scene-text VQA)

--- PAGE 9 ---
achieving state-of-the-art numbers on all datasets. Based
on ablations, we also show the various design choices and
its impact on downstream performance.
References
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-
toine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur
Mensch, Katie Millican, Malcolm Reynolds, Roman Ring,
Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,
Sina Samangooei, Marianne Monteiro, Jacob Menick, Se-
bastian Borgeaud, Andy Brock, Aida Nematzadeh, Sa-
hand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,
Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.
Flamingo: a visual language model for few-shot learning.
ArXiv , abs/2204.14198, 2022. 1, 2, 7
[2] Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota,
Yusheng Xie, and R Manmatha. Docformer: End-to-end
transformer for document understanding. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 993–1003, 2021. 1, 2, 3, 4, 6
[3] Srikar Appalaraju, Yi Zhu, Yusheng Xie, and Istv ´an
Feh´erv´ari. Towards good practices in self-supervised repre-
sentation learning. Neural Information Processing Systems
(NeurIPS Self-Supervision Workshop 2020) , 2020. 1
[4] Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang,
Xiaodong Liu, Yu Wang, Songhao Piao, Jianfeng Gao, Ming
Zhou, and Hsiao-Wuen Hon. Unilmv2: Pseudo-masked lan-
guage models for unified language model pre-training, 2020.
6
[5] Ali Furkan Biten, Ron Litman, Yusheng Xie, Srikar Ap-
palaraju, and R Manmatha. Latr: Layout-aware transformer
for scene-text vqa. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
16548–16558, 2022. 1, 4, 6, 7
[6] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez,
Marc ¸al Rusinol, Minesh Mathew, CV Jawahar, Ernest Val-
veny, and Dimosthenis Karatzas. Icdar 2019 competition
on scene text visual question answering. In 2019 Interna-
tional Conference on Document Analysis and Recognition
(ICDAR) , pages 1563–1570, 2019. 5
[7] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez,
Marc ¸al Rusinol, Ernest Valveny, CV Jawahar, and Dimos-
thenis Karatzas. Scene text visual question answering. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 4291–4301, 2019. 5, 7
[8] Jingye Chen, Tengchao Lv, Lei Cui, Changrong Zhang, and
Furu Wei. Xdoc: Unified pre-training for cross-format doc-
ument understanding. In Conference on Empirical Methods
in Natural Language Processing , 2022. 2
[9] Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang,
Hong Wang, SHIYANG LI, Xiyou Zhou, and William Yang
Wang. Tabfact: A large-scale dataset for table-based fact
verification. ArXiv , abs/1909.02164, 2019. 1, 5
[10] Xi Chen, Xiao Wang, Soravit Changpinyo, A. J. Piergio-
vanni, Piotr Padlewski, Daniel M. Salz, Sebastian Good-
man, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander
Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, HassanAkbari, Gaurav Mishra, Linting Xue, Ashish V . Thapliyal,
James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini,
Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas
Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and
Radu Soricut. Pali: A jointly-scaled multilingual language-
image model. ArXiv , abs/2209.06794, 2022. 1, 2, 7
[11] Brian L. Davis, B. Morse, Bryan Price, Chris Tensmeyer,
Curtis Wigington, and Vlad I. Morariu. End-to-end docu-
ment recognition and understanding with dessurt. In ECCV
Workshops , 2022. 6
[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018. 6
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. In International Con-
ference on Learning Representations , 2020. 2, 3
[14] Chengyang Fang, Gangyan Zeng, Yu Zhou, Daiqing Wu,
Can Ma, Dayong Hu, and Weiping Wang. Towards escaping
from language bias and ocr error: Semantics-centered text
visual question answering. 2022 IEEE International Confer-
ence on Multimedia and Expo (ICME) , pages 01–06, 2022.
7
[15] Jiuxiang Gu, Jason Kuen, Vlad I. Morariu, Handong Zhao,
Nikolaos Barmpalios, R. Jain, Ani Nenkova, and Tong Sun.
Unified pretraining framework for document understanding.
ArXiv , abs/2204.10939, 2022. 2, 6
[16] Zhangxuan Gu, Changhua Meng, Ke Wang, Jun Lan,
Weiqiang Wang, Ming Gu, and Liqing Zhang. Xylayoutlm:
Towards layout-aware multimodal networks for visually-rich
document understanding. 2022 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , pages
4573–4582, 2022. 2
[17] Wei Han, Hantao Huang, and Tao Han. Finding the ev-
idence: Localization-aware answer prediction for text vi-
sual question answering. In Proceedings of the 28th In-
ternational Conference on Computational Linguistics , pages
3118–3131, 2020. 6, 7
[18] Xiaoshuai Hao, Yi Zhu, Srikar Appalaraju, Aston Zhang,
Wanqian Zhang, Boyang Li, and Mu Li. Mixgen: A new
multi-modal data augmentation. In IEEE WACV 2023 - Pre
train Workshop , volume abs/2206.08358, 2023. 1
[19] Adam W Harley, Alex Ufkes, and Konstantinos G Derpanis.
Evaluation of deep convolutional nets for document image
classification and retrieval. In International Conference on
Document Analysis and Recognition (ICDAR) . 1
[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep Residual Learning for Image Recognition. In CVPR .
2016. 2
[21] Chih-Hui Ho, Srikar Appalaraju, Bhavan Jasani, R Man-
matha, and Nuno Vasconcelos. Yoro-lightweight end to end
visual grounding. In European Conference on Computer Vi-
sion - ECCV CAMP Workshop , 2022. 1
[22] Teakgyu Hong, DongHyun Kim, Mingi Ji, Won-
seok Hwang, Daehyun Nam, and Sungrae Park.

--- PAGE 10 ---
Bros: A pre-trained language model for un-
derstanding texts in document. under review
https://openreview.net/references/pdf?id=uCz3OR6CJT ,
2020. 2, 6
[23] Teakgyu Hong, DongHyun Kim, Mingi Ji, Wonseok
Hwang, Daehyun Nam, and Sungrae Park. Bros: A pre-
trained language model for understanding texts in document.
https://openreview.net/forum?id=punMXQEsPr0 , 2020. 2
[24] Ronghang Hu, Amanpreet Singh, Trevor Darrell, and Mar-
cus Rohrbach. Iterative answer prediction with pointer-
augmented multimodal transformers for textvqa. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 9992–10002, 2020. 6, 7
[25] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu
Wei. Layoutlmv3: Pre-training for document ai with unified
text and image masking. arXiv preprint arXiv:2204.08387 ,
2022. 2, 4, 5, 6
[26] Wonseok Hwang, Jinyeong Yim, Seunghyun Park, Sohee
Yang, and Minjoon Seo. Spatial dependency parsing for
semi-structured document information extraction, 2020. 6
[27] Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe
Thiran. Funsd: A dataset for form understanding in noisy
scanned documents. 2019 International Conference on Doc-
ument Analysis and Recognition Workshops (ICDARW) , 2:1–
6, 2019. 1, 5
[28] Yash Kant, Dhruv Batra, Peter Anderson, Alexander
Schwing, Devi Parikh, Jiasen Lu, and Harsh Agrawal. Spa-
tially aware multimodal transformers for textvqa. In Euro-
pean Conference on Computer Vision , pages 715–732, 2020.
7
[29] Jihyung Kil, Soravit Changpinyo, Xi Chen, Hexiang Hu,
Sebastian Goodman, Wei-Lun Chao, and Radu Soricut.
Prestu: Pre-training for scene-text understanding. ArXiv ,
abs/2209.05534, 2022. 7
[30] Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon
Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sang-
doo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free
document understanding transformer. In European Confer-
ence on Computer Vision , 2021. 5
[31] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-
and-language transformer without convolution or region su-
pervision. In ICML , 2021. 1
[32] Taku Kudo and John Richardson. Sentencepiece: A sim-
ple and language independent subword tokenizer and detok-
enizer for neural text processing. In Conference on Empirical
Methods in Natural Language Processing , 2018. 3
[33] Chen-Yu Lee, Chun-Liang Li, Timothy Dozat, Vincent
Perot, Guolong Su, Nan Hua, Joshua Ainslie, Renshen
Wang, Yasuhisa Fujii, and Tomas Pfister. Formnet: Struc-
tural encoding beyond sequential modeling in form docu-
ment information extraction. In Annual Meeting of the Asso-
ciation for Computational Linguistics , 2022. 2, 6
[34] Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu
Liu, Julian Martin Eisenschlos, Urvashi Khandelwal, Peter
Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct:
Screenshot parsing as pretraining for visual language under-
standing. ArXiv , abs/2210.03347, 2022. 5[35] Bingjia Li, Jie Wang, Minyi Zhao, and Shuigeng Zhou. Two-
stage multimodality fusion for high-performance text-based
visual question answering. In Asian Conference on Com-
puter Vision , 2022. 7
[36] Chenliang Li, Bin Bi, Ming Yan, Wei Wang, Songfang
Huang, Fei Huang, and Luo Si. Structurallm: Structural pre-
training for form understanding. In Proceedings of the 59th
Annual Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Papers) , pages
6309–6318, 2021. 2, 6
[37] Chenge Li, Istv ´an Feh ´erv´ari, Xiaonan Zhao, Ives Mac ˆedo,
and Srikar Appalaraju. Seetek: Very large-scale open-set
logo recognition with text-aware metric learning. 2022
IEEE/CVF Winter Conference on Applications of Computer
Vision (WACV) , pages 587–596, 2022. 1
[38] Junlong Li, Yiheng Xu, Lei Cui, and Furu Wei. Markuplm:
Pre-training of text and markup language for visually rich
document understanding. In Annual Meeting of the Associa-
tion for Computational Linguistics , 2021. 2
[39] Peizhao Li, Jiuxiang Gu, Jason Kuen, Vlad I. Morariu, Han-
dong Zhao, R. Jain, Varun Manjunatha, and Hongfu Liu.
Selfdoc: Self-supervised document representation learning.
2021 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) , pages 5648–5656, 2021. 2, 6
[40] Yulin Li, Yuxi Qian, Yuechen Yu, Xiameng Qin, Chengquan
Zhang, Yan Liu, Kun Yao, Junyu Han, Jingtuo Liu, and Errui
Ding. Structext: Structured text understanding with multi-
modal transformers. In Proceedings of the 29th ACM Inter-
national Conference on Multimedia , 2021. 6
[41] Ron Litman, Oron Anschel, Shahar Tsiper, Roee Litman,
Shai Mazor, and R Manmatha. Scatter: selective con-
text attentional scene text recognizer. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 11962–11972, 2020. 1
[42] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar
Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-
moyer, and Veselin Stoyanov. Roberta: A robustly optimized
bert pretraining approach. arXiv preprint arXiv:1907.11692 ,
2019. 6
[43] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,
Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong,
Furu Wei, and Baining Guo. Swin transformer v2: Scal-
ing up capacity and resolution. 2022 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
11999–12009, 2021. 8
[44] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. 2021
IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 9992–10002, 2021. 2, 3
[45] Xiaopeng Lu, Zhen Fan, Yansen Wang, Jean Oh, and Car-
olyn P Ros ´e. Localize, group, and select: Boosting text-vqa
by scene text modeling. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 2631–
2639, 2021. 7
[46] Siwen Luo, Feiqi Cao, Felipe Weason N ´u˜nez, Zean Wen,
Josiah Poon, and Caren Han. Scenegate: Scene-graph based

--- PAGE 11 ---
co-attention networks for text visual question answering.
ArXiv , abs/2212.08283, 2022. 7
[47] Minesh Mathew, Viraj Bagal, Rub `en Tito, Dimosthenis
Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa.
InProceedings of the IEEE/CVF Winter Conference on Ap-
plications of Computer Vision , pages 1697–1706, 2022. 5
[48] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar.
Docvqa: A dataset for vqa on document images. In Proceed-
ings of the IEEE/CVF Winter Conference on Applications of
Computer Vision , pages 2200–2209, 2021. 1
[49] Minesh Mathew, Dimosthenis Karatzas, R. Manmatha, and
C. V . Jawahar. DocVQA: A dataset for vqa on document
images, 2020. 1, 5, 8
[50] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and
Anirban Chakraborty. Ocr-vqa: Visual question answering
by reading text in images. In 2019 international conference
on document analysis and recognition (ICDAR) , pages 947–
952. IEEE, 2019. 5, 6, 7
[51] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-
perative style, high-performance deep learning library. arXiv
preprint arXiv:1912.01703 , 2019. 5
[52] Qiming Peng, Yinxu Pan, Wenjin Wang, Bin Luo, Zhenyu
Zhang, Zhengjie Huang, Teng Hu, Weichong Yin, Yongfeng
Chen, Yin Zhang, et al. Ernie-layout: Layout knowledge en-
hanced pre-training for visually-rich document understand-
ing.arXiv preprint arXiv:2210.06155 , 2022. 2
[53] Rafał Powalski, Łukasz Borchmann, Dawid Jurkiewicz,
Tomasz Dwojak, Michał Pietruszka, and Gabriela Pałka. Go-
ing full-tilt boogie on document understanding with text-
image-layout transformer. In International Conference on
Document Analysis and Recognition , pages 732–747, 2021.
2, 3, 4, 5, 6
[54] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In International
Conference on Machine Learning , 2021. 1
[55] Colin Raffel, Noam M. Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. Exploring the limits of trans-
fer learning with a unified text-to-text transformer. ArXiv ,
abs/1910.10683, 2019. 4, 5
[56] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. IEEE Transactions on Pattern Analysis
and Machine Intelligence , 39:1137–1149, 2015. 3
[57] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
ArXiv , abs/1505.04597, 2015. 3
[58] Park Seunghyun, Shin Seung, Lee Bado, Lee Junyeop, Surh
Jaeheung, Seo Minjoon, and Lee Hwalsuk. Cord: A con-
solidated receipt dataset for post-ocr parsing. 2019. 1, 5,
6
[59] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,
Xinlei Chen, Dhruv Batra, Devi Parikh, and MarcusRohrbach. Towards vqa models that can read. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition , pages 8317–8326, 2019. 5, 7
[60] Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang
Liu, Chenguang Zhu, Michael Zeng, Chao-Yue Zhang, and
Mohit Bansal. Unifying vision, text, and layout for universal
document processing. ArXiv , abs/2212.02623, 2022. 2, 5, 6
[61] Wolf Thomas, Debut Lysandre, Sanh Victor, Chaumond
Julien, Delangue Clement, Moi Anthony, Cistac Pierric,
Rault Tim, Louf R ´emi, Morgan Funtowicz, et al. Hugging-
face’s transformers: State-of-the-art natural language pro-
cessing. arXiv preprint arXiv:1910.03771 , 2019. 5
[62] Jun Wang, Mingfei Gao, Yuqian Hu, Ramprasaath R
Selvaraju, Chetan Ramaiah, Ran Xu, Joseph F JaJa,
and Larry S Davis. Tag: Boosting text-vqa via text-
aware visual question-answer generation. arXiv preprint
arXiv:2208.01813 , 2022. 7
[63] Jiapeng Wang, Lianwen Jin, and Kai Ding. Lilt: A simple yet
effective language-independent layout transformer for struc-
tured document understanding. In Annual Meeting of the
Association for Computational Linguistics , 2022. 2
[64] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,
Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang.
Git: A generative image-to-text transformer for vision and
language. arXiv preprint arXiv:2205.14100 , 2022. 1, 2, 6, 7
[65] Zilong Wang, Yichao Zhou, Wei Wei, Chen-Yu Lee, and
Sandeep Tata. A benchmark for structured extractions from
complex documents. ArXiv , abs/2211.15421, 2022. 2
[66] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei,
and Ming Zhou. Layoutlm: Pre-training of text and layout
for document image understanding. In Proceedings of the
26th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining , pages 1192–1200, 2020. 2, 4, 6
[67] Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei,
Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang,
Wanxiang Che, et al. Layoutlmv2: Multi-modal pre-training
for visually-rich document understanding. arXiv preprint
arXiv:2012.14740 , 2020. 2, 5, 6, 7, 8
[68] Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei,
Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang,
Wanxiang Che, et al. Layoutlmv2: Multi-modal pre-training
for visually-rich document understanding. In Proceedings
of the 59th Annual Meeting of the Association for Compu-
tational Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1: Long Pa-
pers) , pages 2579–2591, 2021. 3, 4
[69] Zhengyuan Yang, Yijuan Lu, Jianfeng Wang, Xi Yin, Dinei
Florencio, Lijuan Wang, Cha Zhang, Lei Zhang, and Jiebo
Luo. Tap: Text-aware pre-training for text-vqa and text-
caption. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 8751–8761,
2021. 7
[70] Łukasz Borchmann, Michal Pietruszka, Tomasz Sta-
nisławek, Dawid Jurkiewicz, Michał P. Turski, Karolina
Szyndler, and Filip Gralinski. Due: End-to-end document
understanding benchmark. In NeurIPS Datasets and Bench-
marks , 2021. 2, 5
