TrOCR: Nhận dạng ký tự quang học dựa trên Transformer
với các mô hình được huấn luyện trước
Minghao Li1*, Tengchao Lv2, Jingye Chen2, Lei Cui2,
Yijuan Lu2, Dinei Florencio2, Cha Zhang2, Zhoujun Li1, Furu Wei2
1Đại học Beihang
2Tập đoàn Microsoft
fliminghao1630, lizj g@buaa.edu.cn
ftengchaolv, v-jingyechen, lecu, yijlu, dinei, chazhang, fuwei g@microsoft.com

Tóm tắt
Nhận dạng văn bản là một vấn đề nghiên cứu lâu đời cho việc số hóa tài liệu. Các phương pháp tiếp cận hiện tại thường được xây dựng dựa trên CNN cho hiểu biết hình ảnh và RNN cho việc tạo văn bản ở cấp độ ký tự. Ngoài ra, một mô hình ngôn ngữ khác thường được cần thiết để cải thiện độ chính xác tổng thể như một bước xử lý hậu kỳ. Trong bài báo này, chúng tôi đề xuất một phương pháp nhận dạng văn bản đầu cuối đến đầu cuối với các mô hình Transformer hình ảnh và văn bản được huấn luyện trước, cụ thể là TrOCR, mà tận dụng kiến trúc Transformer cho cả hiểu biết hình ảnh và tạo văn bản ở cấp độ wordpiece. Mô hình TrOCR đơn giản nhưng hiệu quả, và có thể được huấn luyện trước với dữ liệu tổng hợp quy mô lớn và tinh chỉnh với các bộ dữ liệu được gán nhãn bởi con người. Các thí nghiệm cho thấy mô hình TrOCR vượt trội hơn các mô hình hiện đại hiện tại trên các tác vụ nhận dạng văn bản in, viết tay và cảnh. Các mô hình TrOCR và mã nguồn được công bố công khai tại https://aka.ms/trocr.

Giới thiệu
Nhận dạng Ký tự Quang học (OCR) là việc chuyển đổi điện tử hoặc cơ khí các hình ảnh văn bản được đánh máy, viết tay hoặc in thành văn bản được mã hóa máy tính, dù từ một tài liệu được quét, ảnh chụp tài liệu, ảnh cảnh hay từ văn bản phụ đề được chồng lên hình ảnh. Thông thường, một hệ thống OCR bao gồm hai module chính: module phát hiện văn bản và module nhận dạng văn bản. Phát hiện văn bản nhằm định vị tất cả các khối văn bản trong hình ảnh văn bản, ở cấp từ hoặc cấp dòng văn bản. Tác vụ phát hiện văn bản thường được coi là một vấn đề phát hiện đối tượng mà các mô hình phát hiện đối tượng thông thường như YoLOv5 và DBNet (Liao et al. 2019) có thể được áp dụng. Trong khi đó, nhận dạng văn bản nhằm hiểu nội dung hình ảnh văn bản và chuyển đổi các tín hiệu thị giác thành các token ngôn ngữ tự nhiên. Tác vụ nhận dạng văn bản thường được xây dựng như một vấn đề encoder-decoder mà các phương pháp hiện tại tận dụng encoder dựa trên CNN cho hiểu biết hình ảnh và decoder dựa trên RNN cho việc tạo văn bản. Trong bài báo này, chúng tôi tập trung vào tác vụ nhận dạng văn bản cho hình ảnh tài liệu và để phát hiện văn bản làm công việc tương lai.

Tiến bộ gần đây trong nhận dạng văn bản (Diaz et al. 2021) đã chứng kiến những cải tiến đáng kể bằng cách tận dụng các kiến trúc Transformer (Vaswani et al. 2017). Tuy nhiên, các phương pháp hiện tại vẫn dựa trên CNN làm backbone, nơi self-attention được xây dựng trên đầu các backbone CNN làm encoder để hiểu hình ảnh văn bản. Đối với decoder, Connectionist Temporal Classification (CTC) (Graves et al. 2006) thường được sử dụng kết hợp với một mô hình ngôn ngữ bên ngoài ở cấp độ ký tự để cải thiện độ chính xác tổng thể. Mặc dù có thành công lớn đạt được bởi phương pháp encoder/decoder lai, vẫn còn nhiều chỗ để cải thiện với các mô hình CV và NLP được huấn luyện trước:

1) các tham số mạng trong các phương pháp hiện tại được huấn luyện từ đầu với các bộ dữ liệu tổng hợp/được gán nhãn bởi con người, để các mô hình được huấn luyện trước quy mô lớn chưa được khám phá. 2) khi các Transformer hình ảnh trở nên ngày càng phổ biến (Dosovitskiy et al. 2021; Touvron et al. 2021), đặc biệt là huấn luyện trước hình ảnh tự giám sát gần đây (Bao, Dong, and Wei 2021), việc điều tra xem các Transformer hình ảnh được huấn luyện trước có thể thay thế các backbone CNN hay không là điều dễ hiểu, trong khi tận dụng các Transformer hình ảnh được huấn luyện trước để làm việc cùng với các Transformer văn bản được huấn luyện trước trong một framework duy nhất trên tác vụ nhận dạng văn bản.

Để đạt được mục tiêu này, chúng tôi đề xuất TrOCR, một mô hình OCR đầu cuối đến đầu cuối dựa trên Transformer cho nhận dạng văn bản với các mô hình CV và NLP được huấn luyện trước, được thể hiện trong Hình 1. Khác biệt với các mô hình nhận dạng văn bản hiện tại, TrOCR là một mô hình đơn giản nhưng hiệu quả không sử dụng CNN làm backbone. Thay vào đó, theo (Dosovitskiy et al. 2021), nó đầu tiên thay đổi kích thước hình ảnh văn bản đầu vào thành 384×384 và sau đó hình ảnh được chia thành một chuỗi các patch 16×16 được sử dụng làm đầu vào cho các Transformer hình ảnh. Kiến trúc Transformer chuẩn với cơ chế self-attention được tận dụng ở cả phần encoder và decoder, nơi các đơn vị wordpiece được tạo ra làm văn bản được nhận dạng từ hình ảnh đầu vào. Để huấn luyện mô hình TrOCR một cách hiệu quả, encoder có thể được khởi tạo với các mô hình kiểu ViT được huấn luyện trước (Dosovitskiy et al. 2021; Touvron et al. 2021; Bao, Dong, and Wei 2021) trong khi decoder có thể được khởi tạo với các mô hình kiểu BERT được huấn luyện trước (Devlin et al. 2019; Liu et al. 2019; Dong et al. 2019; Wang et al. 2020b), tương ứng. Do đó, ưu điểm của TrOCR là ba mặt. Thứ nhất, TrOCR sử dụng các mô hình Transformer hình ảnh và văn bản được huấn luyện trước, mà tận dụng dữ liệu không gán nhãn quy mô lớn cho hiểu biết hình ảnh và mô hình hóa ngôn ngữ, mà không cần một mô hình ngôn ngữ bên ngoài. Thứ hai, TrOCR không yêu cầu bất kỳ mạng tích chập nào cho backbone và không giới thiệu bất kỳ bias quy nạp đặc thù hình ảnh nào, điều này làm cho mô hình rất dễ thực hiện và duy trì. Cuối cùng, kết quả thí nghiệm trên các bộ dữ liệu benchmark OCR cho thấy TrOCR có thể đạt được kết quả tiên tiến nhất trên các bộ dữ liệu hình ảnh văn bản in, viết tay và cảnh mà không cần bất kỳ bước tiền xử lý/hậu xử lý phức tạp nào. Hơn nữa, chúng tôi có thể dễ dàng mở rộng TrOCR cho nhận dạng văn bản đa ngôn ngữ với nỗ lực tối thiểu, nơi chỉ cần tận dụng các mô hình đa ngôn ngữ được huấn luyện trước ở phía decoder và mở rộng từ điển.

Các đóng góp của bài báo này được tóm tắt như sau:
1. Chúng tôi đề xuất TrOCR, một mô hình OCR đầu cuối đến đầu cuối dựa trên Transformer cho nhận dạng văn bản với các mô hình CV và NLP được huấn luyện trước. Theo hiểu biết tốt nhất của chúng tôi, đây là công trình đầu tiên kết hợp tận dụng các Transformer hình ảnh và văn bản được huấn luyện trước cho tác vụ nhận dạng văn bản trong OCR.
2. TrOCR đạt được kết quả tiên tiến nhất với mô hình encoder-decoder dựa trên Transformer chuẩn, mà không có tích chập và không dựa vào bất kỳ bước tiền xử lý/hậu xử lý phức tạp nào.
3. Các mô hình TrOCR và mã nguồn được công bố công khai tại https://aka.ms/trocr.

TrOCR

Kiến trúc Mô hình
TrOCR được xây dựng với kiến trúc Transformer, bao gồm một Transformer hình ảnh để trích xuất các đặc trưng thị giác và một Transformer văn bản để mô hình hóa ngôn ngữ. Chúng tôi áp dụng cấu trúc encoder-decoder Transformer vanilla trong TrOCR. Encoder được thiết kế để có được biểu diễn của các patch hình ảnh và decoder để tạo ra chuỗi wordpiece với sự hướng dẫn của các đặc trưng thị giác và các dự đoán trước đó.

Encoder Encoder nhận một hình ảnh đầu vào ximg ∈ ℝ3×H'×W', và thay đổi kích thước nó thành một kích thước cố định (H, W). Vì encoder Transformer không thể xử lý các hình ảnh thô trừ khi chúng là một chuỗi các token đầu vào, encoder phân rã hình ảnh đầu vào thành một lô N = HW/P² patch hình vuông với kích thước cố định (P, P), trong khi chiều rộng W và chiều cao H của hình ảnh được thay đổi kích thước được đảm bảo chia hết cho kích thước patch P. Sau đó, các patch được làm phẳng thành các vector và được chiếu tuyến tính thành các vector D-chiều, aka các embedding patch. D là kích thước ẩn của Transformer qua tất cả các lớp của nó.

Tương tự như ViT (Dosovitskiy et al. 2021) và DeiT (Touvron et al. 2021), chúng tôi giữ token đặc biệt "[CLS]" thường được sử dụng cho các tác vụ phân loại hình ảnh. Token "[CLS]" tập hợp tất cả thông tin từ tất cả các embedding patch và đại diện cho toàn bộ hình ảnh. Trong khi đó, chúng tôi cũng giữ token chưng cất trong chuỗi đầu vào khi sử dụng các mô hình DeiT được huấn luyện trước để khởi tạo encoder, điều này cho phép mô hình học từ mô hình giáo viên. Các embedding patch và hai token đặc biệt được cung cấp các embedding vị trí 1D có thể học được theo vị trí tuyệt đối của chúng.

Khác với các đặc trưng được trích xuất bởi mạng giống CNN, các mô hình Transformer không có bias quy nạp đặc thù hình ảnh và xử lý hình ảnh như một chuỗi các patch, điều này làm cho mô hình dễ dàng chú ý khác nhau đến toàn bộ hình ảnh hoặc các patch độc lập.

Decoder Chúng tôi sử dụng decoder Transformer gốc cho TrOCR. Decoder Transformer chuẩn cũng có một chồng các lớp giống hệt nhau, có cấu trúc tương tự như các lớp trong encoder, ngoại trừ decoder chèn "attention encoder-decoder" giữa multi-head self-attention và mạng feed-forward để phân phối attention khác nhau trên đầu ra của encoder. Trong module attention encoder-decoder, các key và value đến từ đầu ra encoder, trong khi các query đến từ đầu vào decoder. Ngoài ra, decoder tận dụng masking attention trong self-attention để ngăn bản thân nhận được thêm thông tin trong quá trình huấn luyện hơn dự đoán. Dựa trên sự thật rằng đầu ra của decoder sẽ dịch chuyển phải một chỗ từ đầu vào của decoder, mask attention cần đảm bảo đầu ra cho vị trí i chỉ có thể chú ý đến đầu ra trước đó, đó là đầu vào ở các vị trí nhỏ hơn i:

hi = Proj(Emb(Token_i))
α(hij) = e^hij / ∑k=1^V e^hik cho j = 1, 2, ..., V

Các trạng thái ẩn từ decoder được chiếu bởi một lớp tuyến tính từ chiều mô hình đến chiều của kích thước từ vựng V, trong khi các xác suất trên từ vựng được tính toán trên đó bởi hàm softmax. Chúng tôi sử dụng beam search để có được đầu ra cuối cùng.

Khởi tạo Mô hình
Cả encoder và decoder đều được khởi tạo bởi các mô hình công cộng được huấn luyện trước trên các bộ dữ liệu có gán nhãn và không gán nhãn quy mô lớn.

Khởi tạo Encoder Các mô hình DeiT (Touvron et al. 2021) và BEiT (Bao, Dong, and Wei 2021) được sử dụng để khởi tạo encoder trong các mô hình TrOCR. DeiT huấn luyện Transformer hình ảnh với ImageNet (Deng et al. 2009) làm tập huấn luyện duy nhất. Các tác giả thử các siêu tham số khác nhau và data augmentation để làm cho mô hình hiệu quả về dữ liệu. Hơn nữa, họ chưng cất kiến thức của một bộ phân loại hình ảnh mạnh vào một token chưng cất trong embedding ban đầu, dẫn đến kết quả cạnh tranh so với các mô hình dựa trên CNN.

Tham khảo tác vụ huấn luyện trước Masked Language Model, BEiT đề xuất tác vụ Masked Image Modeling để huấn luyện trước Transformer hình ảnh. Mỗi hình ảnh sẽ được chuyển đổi thành hai góc nhìn: các patch hình ảnh và các token thị giác. Họ tokenize hình ảnh gốc thành các token thị giác bằng các mã tiềm ẩn của VAE rời rạc (Ramesh et al. 2021), ngẫu nhiên mask một số patch hình ảnh, và làm cho mô hình khôi phục các token thị giác gốc. Cấu trúc của BEiT giống như Transformer hình ảnh và thiếu token chưng cất khi so sánh với DeiT.

Khởi tạo Decoder Chúng tôi sử dụng các mô hình RoBERTa (Liu et al. 2019) và các mô hình MiniLM (Wang et al. 2020b) để khởi tạo decoder. Nói chung, RoBERTa là một nghiên cứu tái tạo của (Devlin et al. 2019) mà cẩn thận đo lường tác động của nhiều siêu tham số quan trọng và kích thước dữ liệu huấn luyện. Dựa trên BERT, họ loại bỏ mục tiêu dự đoán câu tiếp theo và thay đổi động mẫu masking của Masked Language Model.

Các MiniLM là các mô hình nén của các mô hình Transformer lớn được huấn luyện trước trong khi vẫn giữ 99% hiệu suất. Thay vì sử dụng các xác suất mục tiêu mềm của dự đoán mô hình ngôn ngữ masked hoặc biểu diễn trung gian của các mô hình giáo viên để hướng dẫn việc huấn luyện các mô hình học sinh trong công trình trước đây. Các mô hình MiniLM được huấn luyện bằng cách chưng cất module self-attention của lớp Transformer cuối cùng của các mô hình giáo viên và giới thiệu một trợ lý giáo viên để hỗ trợ việc chưng cất.

Khi tải các mô hình trên vào decoder, các cấu trúc không khớp chính xác vì cả hai đều chỉ là encoder của kiến trúc Transformer. Ví dụ, các lớp attention encoder-decoder không có trong các mô hình này. Để giải quyết điều này, chúng tôi khởi tạo các decoder với các mô hình RoBERTa và MiniLM bằng cách thiết lập thủ công ánh xạ tham số tương ứng, và các tham số không có được khởi tạo ngẫu nhiên.

Pipeline Tác vụ
Trong công trình này, pipeline của tác vụ nhận dạng văn bản là khi được cho các hình ảnh dòng văn bản, mô hình trích xuất các đặc trưng thị giác và dự đoán các token wordpiece dựa vào hình ảnh và ngữ cảnh được tạo ra trước đó. Chuỗi các token ground truth được theo sau bởi một token "[EOS]", biểu thị kết thúc của một câu. Trong quá trình huấn luyện, chúng tôi dịch chuyển chuỗi lùi về một chỗ và thêm token "[BOS]" vào đầu biểu thị bắt đầu của việc tạo ra. Chuỗi ground truth được dịch chuyển được đưa vào decoder, và đầu ra của đó được giám sát bởi chuỗi ground truth gốc với loss cross-entropy. Đối với suy luận, decoder bắt đầu từ token "[BOS]" để dự đoán đầu ra một cách lặp đi lặp lại trong khi liên tục lấy đầu ra mới được tạo ra làm đầu vào tiếp theo.

Huấn luyện trước
Chúng tôi sử dụng tác vụ nhận dạng văn bản cho giai đoạn huấn luyện trước, vì tác vụ này có thể làm cho các mô hình học kiến thức của cả việc trích xuất đặc trưng thị giác và mô hình ngôn ngữ. Quá trình huấn luyện trước được chia thành hai giai đoạn khác nhau bởi bộ dữ liệu được sử dụng. Trong giai đoạn đầu tiên, chúng tôi tổng hợp một bộ dữ liệu quy mô lớn bao gồm hàng trăm triệu hình ảnh dòng văn bản in và huấn luyện trước các mô hình TrOCR trên đó. Trong giai đoạn thứ hai, chúng tôi xây dựng hai bộ dữ liệu tương đối nhỏ tương ứng với các tác vụ downstream in và viết tay, chứa hàng triệu hình ảnh dòng văn bản mỗi bộ. Chúng tôi sử dụng các bộ dữ liệu văn bản cảnh tổng hợp hiện có và được áp dụng rộng rãi cho tác vụ nhận dạng văn bản cảnh. Sau đó, chúng tôi huấn luyện trước các mô hình riêng biệt trên các bộ dữ liệu đặc thù tác vụ này trong giai đoạn thứ hai, tất cả được khởi tạo bởi mô hình giai đoạn đầu tiên.

Tinh chỉnh
Ngoại trừ các thí nghiệm liên quan đến nhận dạng văn bản cảnh, các mô hình TrOCR được huấn luyện trước được tinh chỉnh trên các tác vụ nhận dạng văn bản downstream. Các đầu ra của các mô hình TrOCR dựa trên Byte Pair Encoding (BPE) (Sennrich, Haddow, and Birch 2015) và SentencePiece (Kudo and Richardson 2018) và không dựa vào bất kỳ từ vựng liên quan đến tác vụ nào.

Tăng cường Dữ liệu
Chúng tôi tận dụng data augmentation để tăng cường sự đa dạng của dữ liệu huấn luyện trước và tinh chỉnh. Sáu loại biến đổi hình ảnh cộng với việc giữ nguyên bản được áp dụng cho các bộ dữ liệu in và viết tay, đó là xoay ngẫu nhiên (-10 đến 10 độ), làm mờ Gaussian, giãn nở hình ảnh, xói mòn hình ảnh, giảm kích thước, và gạch dưới. Chúng tôi quyết định ngẫu nhiên loại biến đổi hình ảnh nào để thực hiện với khả năng bằng nhau cho mỗi mẫu. Đối với các bộ dữ liệu văn bản cảnh, RandAugment (Cubuk et al. 2020) được áp dụng theo (Atienza 2021), và các loại augmentation bao gồm đảo ngược, cong, mờ, nhiễu, biến dạng, xoay, v.v.

Thí nghiệm

Dữ liệu
Bộ dữ liệu Huấn luyện trước Để xây dựng một bộ dữ liệu chất lượng cao quy mô lớn, chúng tôi lấy mẫu hai triệu trang tài liệu từ các tệp PDF có sẵn công khai trên Internet. Vì các tệp PDF được sinh ra kỹ thuật số, chúng tôi có thể có được các hình ảnh dòng văn bản in khá đẹp bằng cách chuyển đổi chúng thành hình ảnh trang và trích xuất các dòng văn bản với các hình ảnh được cắt của chúng. Tổng cộng, bộ dữ liệu huấn luyện trước giai đoạn đầu tiên chứa 684M dòng văn bản.

Chúng tôi sử dụng 5.427 font viết tay để tổng hợp các hình ảnh dòng văn bản viết tay bằng TRDG, một trình tạo dữ liệu nhận dạng văn bản mã nguồn mở. Văn bản được sử dụng để tạo ra được thu thập từ các trang ngẫu nhiên của Wikipedia. Bộ dữ liệu viết tay cho huấn luyện trước giai đoạn thứ hai bao gồm 17.9M dòng văn bản, bao gồm bộ dữ liệu IIIT-HWS (Krishnan and Jawahar 2016). Ngoài ra, chúng tôi thu thập khoảng 53K hình ảnh biên lai trong thế giới thực và nhận dạng văn bản trên chúng bằng các engine OCR thương mại. Theo kết quả, chúng tôi cắt các dòng văn bản theo tọa độ của chúng và chỉnh sửa chúng thành hình ảnh được chuẩn hóa. Chúng tôi cũng sử dụng TRDG để tổng hợp 1M hình ảnh dòng văn bản in với hai font biên lai và các font in tích hợp. Tổng cộng, bộ dữ liệu in bao gồm 3.3M dòng văn bản. Dữ liệu huấn luyện trước giai đoạn thứ hai cho nhận dạng văn bản cảnh là MJSynth (MJ) (Jaderberg et al. 2014) và SynthText (ST) (Gupta, Vedaldi, and Zisserman 2016), tổng cộng khoảng 16M hình ảnh văn bản.

Benchmark Bộ dữ liệu SROIE (Scanned Receipts OCR and Information Extraction) (Task 2) tập trung vào nhận dạng văn bản trong hình ảnh biên lai. Có 626 hình ảnh biên lai và 361 hình ảnh biên lai trong tập huấn luyện và tập test của SROIE. Vì tác vụ phát hiện văn bản không được bao gồm trong công trình này, chúng tôi sử dụng hình ảnh được cắt của các dòng văn bản để đánh giá, được thu được bằng cách cắt toàn bộ hình ảnh biên lai theo các hộp giới hạn ground truth.

Cơ sở dữ liệu Viết tay IAM được cấu thành từ văn bản tiếng Anh viết tay, đây là bộ dữ liệu phổ biến nhất cho nhận dạng văn bản viết tay. Chúng tôi sử dụng phân vùng của Aachen của bộ dữ liệu: 6.161 dòng từ 747 biểu mẫu trong tập train, 966 dòng từ 115 biểu mẫu trong tập validation và 2.915 dòng từ 336 biểu mẫu trong tập test.

Nhận dạng hình ảnh văn bản cảnh thách thức hơn so với hình ảnh văn bản in, vì nhiều hình ảnh trong tự nhiên gặp phải các vấn đề mờ, che khuất, hoặc độ phân giải thấp. Ở đây chúng tôi tận dụng một số benchmark được sử dụng rộng rãi, bao gồm IIIT5K-3000 (Mishra, Alahari, and Jawahar 2012), SVT-647 (Wang, Babenko, and Belongie 2011), IC13-857, IC13-1015 (Karatzas et al. 2013), IC15-1811, IC15-2077 (Karatzas et al. 2015), SVTP-645 (Phan et al. 2013), và CT80-288 (Risnumawan et al. 2014) để đánh giá khả năng của TrOCR được đề xuất.

Cài đặt
Các mô hình TrOCR được xây dựng trên Fairseq (Ott et al. 2019) là một toolkit mô hình hóa chuỗi phổ biến. Đối với khởi tạo mô hình, các mô hình DeiT được thực hiện và khởi tạo bởi mã và các mô hình được huấn luyện trước từ thư viện timm (Wightman 2019) trong khi các mô hình BEiT và các mô hình MiniLM từ repository chính thức của UniLM. Các mô hình RoBERTa đến từ trang tương ứng trong repository GitHub Fairseq. Chúng tôi sử dụng 32 GPU V100 với bộ nhớ 32GB để huấn luyện trước và 8 GPU V100 để tinh chỉnh. Đối với tất cả các mô hình, kích thước batch được đặt thành 2.048 và tốc độ học là 5e-5. Chúng tôi sử dụng tokenizer BPE và sentencepiece từ Fairseq để tokenize các dòng văn bản thành wordpiece.

Chúng tôi sử dụng độ phân giải 384×384 và kích thước patch 16×16 cho các encoder DeiT và BEiT. DeiT SMALL có 12 lớp với 384 kích thước ẩn và 6 head. Cả DeiT BASE và BEiT BASE đều có 12 lớp với 768 kích thước ẩn và 12 head trong khi BEiT LARGE có 24 lớp với 1024 kích thước ẩn và 16 head. Chúng tôi sử dụng 6 lớp, 256 kích thước ẩn và 8 attention head cho các decoder nhỏ, 512 kích thước ẩn cho các decoder base và 12 lớp, 1.024 kích thước ẩn và 16 head cho các decoder lớn. Đối với tác vụ này, chúng tôi chỉ sử dụng nửa cuối của tất cả các lớp từ mô hình RoBERTa tương ứng, đó là 6 lớp cuối cho RoBERTa BASE và 12 lớp cuối cho RoBERTa LARGE. Kích thước beam được đặt thành 10 cho các mô hình TrOCR.

Chúng tôi lấy mô hình CRNN (Shi, Bai, and Yao 2016) làm mô hình baseline. Mô hình CRNN được cấu thành từ các lớp tích chập để trích xuất đặc trưng hình ảnh, các lớp tái phát để mô hình hóa chuỗi và dự đoán nhãn khung cuối cùng, và một lớp phiên âm để chuyển đổi các dự đoán khung thành chuỗi nhãn cuối cùng. Để giải quyết vấn đề căn chỉnh ký tự, họ sử dụng loss CTC để huấn luyện mô hình CRNN. Trong một thời gian dài, mô hình CRNN là mô hình chi phối cho nhận dạng văn bản. Chúng tôi sử dụng implementation PyTorch và khởi tạo các tham số bằng mô hình được huấn luyện trước được cung cấp.

Chỉ số Đánh giá
Bộ dữ liệu SROIE được đánh giá sử dụng precision, recall và f1 score ở cấp từ. Nếu các từ lặp lại xuất hiện trong ground truth, chúng cũng được cho là xuất hiện trong dự đoán. Precision, recall và f1 score được mô tả như sau:

Precision = Kết quả khớp đúng / Số từ được phát hiện
Recall = Kết quả khớp đúng / Số từ ground truth
F1 = 2 × Precision × Recall / (Precision + Recall)

Bộ dữ liệu IAM được đánh giá bằng Character Error Rate (CER) phân biệt chữ hoa chữ thường. Các bộ dữ liệu văn bản cảnh được đánh giá bằng Word Accuracy. Để so sánh công bằng, chúng tôi lọc chuỗi đầu ra cuối cùng để phù hợp với bộ ký tự 36 ký tự phổ biến (chữ và số thường) trong tác vụ này.

Kết quả
So sánh Kiến trúc Chúng tôi so sánh các kết hợp khác nhau của encoder và decoder để tìm ra cài đặt tốt nhất. Đối với encoder, chúng tôi so sánh DeiT, BEiT và mạng ResNet-50. Cả DeiT và BEiT đều là các mô hình base trong các bài báo gốc của chúng. Đối với decoder, chúng tôi so sánh các decoder base được khởi tạo bởi RoBERTa BASE và các decoder lớn được khởi tạo bởi RoBERTa LARGE. Để so sánh thêm, chúng tôi cũng đánh giá mô hình baseline CRNN và Tesseract OCR trong phần này, trong khi phần sau là một Engine OCR mã nguồn mở sử dụng mạng LSTM.

Bảng 1 cho thấy kết quả của các mô hình kết hợp. Từ kết quả, chúng tôi quan sát thấy rằng các encoder BEiT cho thấy hiệu suất tốt nhất trong ba loại encoder trong khi các decoder tốt nhất là các decoder RoBERTa LARGE. Rõ ràng, các mô hình được huấn luyện trước trên tác vụ thị giác cải thiện hiệu suất của các mô hình nhận dạng văn bản, và các mô hình Transformer thuần túy tốt hơn so với các mô hình CRNN và Tesseract trên tác vụ này. Theo kết quả, chúng tôi chủ yếu sử dụng ba cài đặt trong các thí nghiệm tiếp theo: TrOCR SMALL (tổng tham số=62M) bao gồm encoder của DeiT SMALL và decoder của MiniLM, TrOCR BASE (tổng tham số=334M) bao gồm encoder của BEiT BASE và decoder của RoBERTa LARGE, TrOCR LARGE (tổng tham số=558M) bao gồm encoder của BEiT LARGE và decoder của RoBERTa LARGE.

Trong Bảng 2, chúng tôi cũng đã thực hiện một số thí nghiệm ablation để xác minh tác dụng của khởi tạo mô hình được huấn luyện trước, data augmentation, và hai giai đoạn huấn luyện trước. Tất cả chúng đều có cải thiện lớn đối với các mô hình TrOCR.

SROIE Task 2 Bảng 3 cho thấy kết quả của các mô hình TrOCR và các phương pháp SOTA hiện tại trên bảng xếp hạng của bộ dữ liệu SROIE. Để nắm bắt thông tin thị giác, tất cả các baseline này tận dụng các mạng dựa trên CNN làm bộ trích xuất đặc trưng trong khi các mô hình TrOCR sử dụng Transformer hình ảnh để nhúng thông tin từ các patch hình ảnh. Đối với mô hình hóa ngôn ngữ, MSO Lab (Sang and Cuong 2019) và CLOV A OCR (Sang and Cuong 2019) sử dụng các lớp LSTM và H&H Lab (Shi, Bai, and Yao 2016) sử dụng các lớp GRU trong khi các mô hình TrOCR sử dụng decoder Transformer với cơ chế attention thuần túy. Theo kết quả, các mô hình TrOCR vượt trội hơn các mô hình SOTA hiện tại với cấu trúc Transformer thuần túy. Nó cũng được xác nhận rằng các mô hình nhận dạng văn bản dựa trên Transformer có được hiệu suất cạnh tranh so với các mạng dựa trên CNN trong trích xuất đặc trưng thị giác và các mạng dựa trên RNN trong mô hình hóa ngôn ngữ trên tác vụ này mà không cần bất kỳ bước tiền xử lý/hậu xử lý phức tạp nào.

Cơ sở dữ liệu Viết tay IAM Bảng 4 cho thấy kết quả của các mô hình TrOCR và các phương pháp hiện tại trên Cơ sở dữ liệu Viết tay IAM. Theo kết quả, các phương pháp với decoder CTC cho thấy hiệu suất tốt trên tác vụ này và LM bên ngoài sẽ dẫn đến giảm đáng kể CER. Bằng cách so sánh các phương pháp (Bluche and Messina 2017) với các mô hình TrOCR, TrOCR LARGE đạt được kết quả tốt hơn, điều này cho thấy rằng decoder Transformer cạnh tranh hơn so với decoder CTC trong nhận dạng văn bản và có đủ khả năng mô hình hóa ngôn ngữ thay vì dựa vào một LM bên ngoài. Hầu hết các phương pháp sử dụng các mô hình chuỗi trong encoder của chúng sau backbone dựa trên CNN ngoại trừ các encoder FCN trong (Wang et al. 2020a), dẫn đến cải thiện đáng kể về CER. Thay vì dựa vào các đặc trưng từ backbone dựa trên CNN, các mô hình TrOCR sử dụng thông tin từ các patch hình ảnh có được kết quả tương tự và thậm chí tốt hơn, minh họa rằng các cấu trúc Transformer có khả năng trích xuất đặc trưng thị giác tốt sau huấn luyện trước. Từ kết quả thí nghiệm, các mô hình TrOCR vượt trội hơn tất cả các phương pháp chỉ sử dụng tổng hợp/IAM làm tập huấn luyện duy nhất với cấu trúc Transformer thuần túy và đạt được CER tiên tiến nhất mới là 2.89. Mà không tận dụng bất kỳ dữ liệu được gán nhãn bởi con người nào khác, TrOCR thậm chí có được kết quả tương đương với các phương pháp trong (Diaz et al. 2021) sử dụng bộ dữ liệu được gán nhãn bởi con người nội bộ bổ sung.

Các bộ dữ liệu Văn bản Cảnh Trong Bảng 6, chúng tôi so sánh các mô hình TrOCR BASE và TrOCR LARGE của việc tinh chỉnh chỉ với dữ liệu tổng hợp và tinh chỉnh với dữ liệu tổng hợp và các bộ dữ liệu benchmark (các tập huấn luyện của IC13, IC15, IIIT5K, SVT) với các phương pháp SOTA phổ biến và gần đây. So sánh với tất cả, các mô hình TrOCR thiết lập năm kết quả SOTA mới trong tám thí nghiệm trong khi có được kết quả tương đương trên phần còn lại. Mô hình của chúng tôi kém hiệu suất trên bộ dữ liệu IIIT5K, và chúng tôi thấy một số hình ảnh mẫu văn bản cảnh chứa các ký hiệu, nhưng ground truth thì không. Nó không nhất quán với hành vi trong dữ liệu huấn luyện trước của chúng tôi (giữ lại các ký hiệu trong ground truth), khiến mô hình có xu hướng vẫn xử lý các ký hiệu. Có hai loại lỗi: xuất ra các ký hiệu nhưng cắt ngắn đầu ra trước để đảm bảo rằng số lượng wordpiece nhất quán với ground truth, hoặc nhận dạng các ký hiệu như các ký tự tương tự.

Tốc độ Suy luận Bảng 5 cho thấy tốc độ suy luận của các cài đặt khác nhau của mô hình TrOCR trên Cơ sở dữ liệu Viết tay IAM. Chúng tôi có thể kết luận rằng không có sự khác biệt đáng kể về tốc độ suy luận giữa các mô hình base và các mô hình lớn. Ngược lại, mô hình nhỏ cho thấy kết quả tương đương cho nhận dạng văn bản in và viết tay mặc dù số lượng tham số nhỏ hơn một bậc và tốc độ suy luận nhanh gấp đôi. Số lượng tham số thấp và tốc độ suy luận cao có nghĩa là ít tài nguyên tính toán hơn và thời gian chờ người dùng, làm cho nó phù hợp hơn để triển khai trong các ứng dụng công nghiệp.

Công trình Liên quan

Nhận dạng Văn bản Cảnh
Đối với nhận dạng văn bản, các phương pháp tiếp cận phổ biến nhất thường dựa trên các mô hình dựa trên CTC. (Shi, Bai, and Yao 2016) đề xuất CRNN chuẩn, một kiến trúc đầu cuối đến đầu cuối kết hợp bởi CNN và RNN. Các lớp tích chập được sử dụng để trích xuất các đặc trưng thị giác và chuyển đổi chúng thành chuỗi bằng cách nối các cột, trong khi các lớp tái phát dự đoán các nhãn mỗi khung. Họ sử dụng chiến lược giải mã CTC để loại bỏ các ký hiệu lặp lại và tất cả các khoảng trống từ các nhãn để đạt được dự đoán cuối cùng. (Su and Lu 2014) sử dụng các đặc trưng Histogram of Oriented Gradient (HOG) được trích xuất từ các patch hình ảnh trong cùng một cột của hình ảnh đầu vào, thay vì các đặc trưng từ mạng CNN. Một BiLSTM sau đó được huấn luyện để gán nhãn dữ liệu tuần tự với kỹ thuật CTC để tìm ra sự khớp tốt nhất. (Gao et al. 2019) trích xuất đặc trưng bằng mạng kết nối dày đặc kết hợp khối attention dư và nắm bắt thông tin ngữ cảnh và phụ thuộc tuần tự bằng mạng CNN. Họ tính toán phân phối xác suất trên đầu ra của mạng CNN thay vì sử dụng mạng RNN để mô hình hóa chúng. Sau đó, CTC chuyển đổi các phân phối xác suất thành chuỗi nhãn cuối cùng.

Các mô hình Sequence-to-Sequence (Zhang et al. 2020b; Wang et al. 2019; Sheng, Chen, and Xu 2019; Bleeker and de Rijke 2019; Lee et al. 2020; Atienza 2021) đang dần thu hút nhiều sự chú ý hơn, đặc biệt là sau sự ra đời của kiến trúc Transformer (Vaswani et al. 2017). SaHAN (Zhang et al. 2020b), viết tắt của mạng attention phân cấp nhận thức quy mô, được đề xuất để giải quyết vấn đề biến đổi quy mô ký tự. Các tác giả sử dụng mạng FPN và các mô hình CRNN làm encoder cũng như decoder attention phân cấp để giữ lại các đặc trưng đa quy mô. (Wang et al. 2019) trích xuất một chuỗi các đặc trưng thị giác từ hình ảnh đầu vào bằng CNN với module attention và BiLSTM. Decoder được cấu thành từ Gated Cascade Attention Module (GCAM) được đề xuất và tạo ra các ký tự mục tiêu từ chuỗi đặc trưng được trích xuất bởi encoder. Đối với các mô hình Transformer, (Sheng, Chen, and Xu 2019) đầu tiên áp dụng Transformer cho Nhận dạng Văn bản Cảnh. Vì đầu vào của kiến trúc Transformer được yêu cầu là một chuỗi, một khối biến đổi phương thức dựa trên CNN được sử dụng để biến đổi hình ảnh đầu vào 2D thành chuỗi 1D. (Bleeker and de Rijke 2019) thêm embedding hướng vào đầu vào của decoder để giải mã văn bản hai chiều với một decoder duy nhất, trong khi (Lee et al. 2020) sử dụng embedding vị trí động hai chiều để giữ cấu trúc không gian của các bản đồ đặc trưng trung gian để nhận dạng văn bản với sắp xếp tùy ý và khoảng cách giữa ký tự lớn. (Yu et al. 2020) đề xuất mạng lý luận ngữ nghĩa để thay thế các cấu trúc giống RNN cho nhận dạng văn bản chính xác hơn. (Atienza 2021) chỉ sử dụng Transformer hình ảnh mà không có Transformer văn bản để nhận dạng văn bản theo cách không tự hồi quy.

Các văn bản trong hình ảnh tự nhiên có thể xuất hiện ở dạng bất thường do biến dạng phối cảnh. (Shi et al. 2016; Baek et al. 2019; Litman et al. 2020; Shi et al. 2018; Zhan and Lu 2019) giải quyết vấn đề này bằng cách xử lý hình ảnh đầu vào với bước chỉnh sửa ban đầu. Ví dụ, biến đổi thin-plate spline (Shi et al. 2016; Baek et al. 2019; Litman et al. 2020; Shi et al. 2018) được áp dụng để tìm ra phép nội suy spline mượt giữa một tập hợp các điểm chuẩn và chuẩn hóa vùng văn bản thành hình chữ nhật được định nghĩa trước, trong khi (Zhan and Lu 2019) đề xuất mạng chỉnh sửa lặp để mô hình hóa đường giữa của văn bản cảnh cũng như hướng và ranh giới của các dòng văn bản. (Baek et al. 2019; Diaz et al. 2021) đề xuất các kiến trúc phổ quát để so sánh các mô hình nhận dạng khác nhau.

Nhận dạng Văn bản Viết tay
(Memon et al. 2020) đưa ra một đánh giá tài liệu hệ thống về các phương pháp hiện đại cho nhận dạng chữ viết tay. Các cơ chế attention và encoding vị trí khác nhau được so sánh trong (Michael et al. 2019) để giải quyết sự căn chỉnh giữa chuỗi đầu vào và đầu ra. Sự kết hợp của encoder RNN (chủ yếu là LSTM) và decoder CTC (Bluche and Messina 2017; Graves and Schmidhuber 2008; Pham et al. 2014) chiếm một phần lớn trong các công trình liên quan trong một thời gian dài. Bên cạnh đó, (Graves and Schmidhuber 2008; Voigtlaender, Doetsch, and Ney 2016; Puigcerver 2017) cũng đã thử các encoder LSTM đa chiều. Tương tự như nhận dạng văn bản cảnh, các phương pháp seq2seq và sơ đồ cho giải mã attention đã được xác minh trong (Michael et al. 2019; Kang et al. 2020; Chowdhury and Vig 2018; Bluche 2016). (Ingle et al. 2019) giải quyết các vấn đề trong việc xây dựng một hệ thống quy mô lớn.

Kết luận
Trong bài báo này, chúng tôi trình bày TrOCR, một mô hình OCR đầu cuối đến đầu cuối dựa trên Transformer để nhận dạng văn bản với các mô hình được huấn luyện trước. Khác biệt với các phương pháp tiếp cận hiện tại, TrOCR không dựa vào các mô hình CNN thông thường để hiểu hình ảnh. Thay vào đó, nó tận dụng mô hình Transformer hình ảnh làm encoder thị giác và mô hình Transformer văn bản làm decoder văn bản. Hơn nữa, chúng tôi sử dụng wordpiece làm đơn vị cơ bản cho đầu ra được nhận dạng thay vì các phương pháp dựa trên ký tự, điều này tiết kiệm chi phí tính toán được giới thiệu bởi mô hình hóa ngôn ngữ bổ sung. Kết quả thí nghiệm cho thấy TrOCR đạt được kết quả tiên tiến nhất trên nhận dạng văn bản in, viết tay và cảnh chỉ với một mô hình encoder-decoder đơn giản, mà không cần bất kỳ bước hậu xử lý nào.
