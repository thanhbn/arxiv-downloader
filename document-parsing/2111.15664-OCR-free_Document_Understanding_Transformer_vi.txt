# Máy Biến Đổi Hiểu Tài Liệu Không Cần OCR

Geewook Kim1∗, Teakgyu Hong4†, Moonbin Yim2†, Jeongyeon Nam1,
Jinyoung Park5†, Jinyeong Yim6†, Wonseok Hwang7†, Sangdoo Yun3,
Dongyoon Han3, and Seunghyun Park1

1NAVER CLOVA 2NAVER Search 3NAVER AI Lab
4Upstage 5Tmax 6Google 7LBox

Tóm tắt. Hiểu hình ảnh tài liệu (ví dụ: hóa đơn) là một nhiệm vụ cốt lõi nhưng đầy thách thức vì nó đòi hỏi các chức năng phức tạp như đọc văn bản và hiểu toàn diện tài liệu. Các phương pháp Hiểu Tài Liệu Trực Quan (VDU) hiện tại ủy thác nhiệm vụ đọc văn bản cho các công cụ Nhận Dạng Ký Tự Quang Học (OCR) có sẵn và tập trung vào nhiệm vụ hiểu với đầu ra OCR. Mặc dù các phương pháp dựa trên OCR như vậy đã cho thấy hiệu suất đầy hứa hẹn, chúng gặp phải 1) chi phí tính toán cao khi sử dụng OCR; 2) tính không linh hoạt của các mô hình OCR đối với ngôn ngữ hoặc loại tài liệu; 3) lan truyền lỗi OCR đến quá trình tiếp theo. Để giải quyết những vấn đề này, trong bài báo này, chúng tôi giới thiệu một mô hình VDU không cần OCR mới có tên Donut, viết tắt của Document understanding transformer (Máy biến đổi hiểu tài liệu). Như bước đầu tiên trong nghiên cứu VDU không cần OCR, chúng tôi đề xuất một kiến trúc đơn giản (tức là Transformer) với một mục tiêu tiền huấn luyện (tức là cross-entropy loss). Donut có khái niệm đơn giản nhưng hiệu quả. Thông qua các thí nghiệm và phân tích sâu rộng, chúng tôi cho thấy một mô hình VDU không cần OCR đơn giản, Donut, đạt được hiệu suất tối ưu trên các nhiệm vụ VDU khác nhau về cả tốc độ và độ chính xác. Ngoài ra, chúng tôi cung cấp một trình tạo dữ liệu tổng hợp giúp quá trình tiền huấn luyện mô hình linh hoạt trong nhiều ngôn ngữ và lĩnh vực khác nhau. Mã nguồn, mô hình đã huấn luyện và dữ liệu tổng hợp có sẵn tại https://github.com/clovaai/donut.

Từ khóa: Hiểu Tài Liệu Trực Quan, Trích Xuất Thông Tin Tài Liệu, Nhận Dạng Ký Tự Quang Học, Transformer Đầu-Cuối

1 Giới thiệu

Hình ảnh tài liệu, như hóa đơn thương mại, biên lai và danh thiếp, dễ dàng tìm thấy trong môi trường làm việc hiện đại. Để trích xuất thông tin hữu ích từ những hình ảnh tài liệu này, Hiểu Tài Liệu Trực Quan (VDU) không chỉ là một nhiệm vụ thiết yếu cho ngành công nghiệp mà còn là một chủ đề đầy thách thức đối với các nhà nghiên cứu, với các ứng dụng bao gồm phân loại tài liệu [27,1], trích xuất thông tin [22,42], và trả lời câu hỏi trực quan [44,57].

Các phương pháp VDU hiện tại [22,24,65,64,18] giải quyết nhiệm vụ theo cách hai giai đoạn: 1) đọc văn bản trong hình ảnh tài liệu; 2) hiểu toàn diện tài liệu. Chúng thường dựa vào Nhận Dạng Ký Tự Quang Học (OCR) dựa trên deep learning [4,3] cho nhiệm vụ đọc văn bản và tập trung vào việc mô hình hóa phần hiểu. Ví dụ, như thể hiện trong Hình 1, một đường ống truyền thống để trích xuất thông tin có cấu trúc từ tài liệu (còn gọi là phân tích tài liệu) bao gồm ba mô-đun riêng biệt cho phát hiện văn bản, nhận dạng văn bản và phân tích [22,24].

Tuy nhiên, phương pháp phụ thuộc OCR có những vấn đề nghiêm trọng. Trước tiên, sử dụng OCR như một phương pháp tiền xử lý là đắt đỏ. Chúng ta có thể sử dụng các công cụ OCR có sẵn đã được tiền huấn luyện; tuy nhiên, chi phí tính toán cho suy luận sẽ đắt đỏ đối với kết quả OCR chất lượng cao. Hơn nữa, các phương pháp OCR có sẵn hiếm khi có tính linh hoạt để xử lý các ngôn ngữ khác nhau hoặc thay đổi lĩnh vực, có thể dẫn đến khả năng tổng quát hóa kém. Nếu chúng ta huấn luyện một mô hình OCR, nó cũng đòi hỏi chi phí huấn luyện lớn và bộ dữ liệu quy mô lớn [4,3,39,46]. Một vấn đề khác là các lỗi OCR sẽ lan truyền đến hệ thống VDU và ảnh hưởng tiêu cực đến các quá trình tiếp theo [54,23]. Vấn đề này trở nên nghiêm trọng hơn ở các ngôn ngữ có bộ ký tự phức tạp, như tiếng Hàn hoặc tiếng Trung, nơi chất lượng OCR tương đối thấp [50]. Để giải quyết điều này, mô-đun hiệu chỉnh sau-OCR [51,50,10] thường được áp dụng. Tuy nhiên, đây không phải là giải pháp thực tế cho môi trường ứng dụng thực tế vì nó làm tăng kích thước toàn bộ hệ thống và chi phí bảo trì.

Chúng tôi vượt ra ngoài khuôn khổ truyền thống bằng cách mô hình hóa ánh xạ trực tiếp từ hình ảnh đầu vào thô đến đầu ra mong muốn mà không cần OCR. Chúng tôi giới thiệu một mô hình VDU không cần OCR mới để giải quyết các vấn đề do phụ thuộc OCR gây ra. Mô hình của chúng tôi dựa trên kiến trúc chỉ sử dụng Transformer, được gọi là Document understanding transformer (Donut), theo sau thành công lớn trong thị giác và ngôn ngữ [8,9,29]. Chúng tôi trình bày một đường cơ sở tối thiểu bao gồm một kiến trúc đơn giản và phương pháp tiền huấn luyện. Bất chấp tính đơn giản của nó, Donut cho thấy hiệu suất tổng thể tương đương hoặc tốt hơn các phương pháp trước đó như thể hiện trong Hình 2.

Chúng tôi áp dụng sơ đồ tiền-huấn-luyện-và-tinh-chỉnh [8,65] cho việc huấn luyện Donut. Trong giai đoạn tiền huấn luyện, Donut học cách đọc văn bản bằng cách dự đoán từ tiếp theo bằng cách điều kiện hóa đồng thời trên hình ảnh và ngữ cảnh văn bản trước đó. Donut được tiền huấn luyện với hình ảnh tài liệu và chú thích văn bản của chúng. Vì mục tiêu tiền huấn luyện của chúng tôi là đơn giản (tức là đọc văn bản), chúng tôi có thể thực hiện tính linh hoạt về lĩnh vực và ngôn ngữ một cách trực tiếp bằng cách tiền huấn luyện với dữ liệu tổng hợp. Trong giai đoạn tinh chỉnh, Donut học cách hiểu toàn bộ tài liệu theo nhiệm vụ downstream. Chúng tôi chứng minh Donut có khả năng hiểu mạnh mẽ thông qua đánh giá mở rộng trên các nhiệm vụ và bộ dữ liệu VDU khác nhau. Các thí nghiệm cho thấy một mô hình VDU không cần OCR đơn giản có thể đạt được hiệu suất tối ưu về cả tốc độ và độ chính xác.

Các đóng góp được tóm tắt như sau:
1. Chúng tôi đề xuất một phương pháp không cần OCR mới cho VDU. Theo hiểu biết của chúng tôi, đây là phương pháp đầu tiên dựa trên Transformer không cần OCR được huấn luyện theo cách đầu-cuối.
2. Chúng tôi giới thiệu một sơ đồ tiền huấn luyện đơn giản cho phép sử dụng dữ liệu tổng hợp. Bằng cách sử dụng trình tạo SynthDoG của chúng tôi, chúng tôi cho thấy Donut có thể dễ dàng được mở rộng thành cài đặt đa ngôn ngữ, điều này không áp dụng được cho các phương pháp truyền thống cần phải huấn luyện lại công cụ OCR có sẵn.
3. Chúng tôi tiến hành các thí nghiệm và phân tích sâu rộng trên cả điểm chuẩn công khai và bộ dữ liệu công nghiệp riêng tư, cho thấy phương pháp đề xuất không chỉ đạt được hiệu suất tối ưu trên điểm chuẩn mà còn có nhiều lợi thế thực tế (ví dụ: hiệu quả về chi phí) trong các ứng dụng thực tế.
4. Mã nguồn, mô hình tiền huấn luyện và dữ liệu tổng hợp có sẵn tại GitHub.

2 Phương pháp

2.1 Bước đầu: nền tảng

Đã có nhiều phương pháp hiểu tài liệu trực quan (VDU) khác nhau để hiểu và trích xuất thông tin thiết yếu từ các tài liệu bán cấu trúc như biên lai [20,25,18], hóa đơn [49] và tài liệu biểu mẫu [14,6,43].

Các nỗ lực VDU ban đầu đã được thực hiện với các backbone trực quan độc lập OCR [27,1,15,12,31], nhưng hiệu suất bị hạn chế. Sau đó, với những tiến bộ đáng kể của OCR [4,3] và BERT [8], nhiều mô hình VDU phụ thuộc OCR đã được đề xuất bằng cách kết hợp chúng [22,24,23]. Gần đây nhất, để có được VDU tổng quát hơn, hầu hết các công nghệ tiên tiến [64,18] sử dụng cả công cụ OCR mạnh mẽ và dữ liệu hình ảnh tài liệu thực quy mô lớn (ví dụ: IIT-CDIP [32]) cho tiền huấn luyện mô hình. Mặc dù chúng đã cho thấy những tiến bộ đáng kể trong những năm gần đây, cần có nỗ lực thêm để đảm bảo hiệu suất của toàn bộ mô hình VDU bằng cách sử dụng công cụ OCR có sẵn.

2.2 Máy Biến Đổi Hiểu Tài Liệu

Donut là một mô hình VDU đầu-cuối (tức là tự chứa) để hiểu tổng quát hình ảnh tài liệu. Kiến trúc của Donut khá đơn giản, bao gồm một bộ mã hóa trực quan và các mô-đun giải mã văn bản dựa trên Transformer [58,9]. Lưu ý rằng Donut không dựa vào bất kỳ mô-đun nào liên quan đến chức năng OCR mà sử dụng bộ mã hóa trực quan để trích xuất đặc trưng từ hình ảnh tài liệu đã cho. Bộ giải mã văn bản tiếp theo ánh xạ các đặc trưng dẫn xuất thành một chuỗi token subword để xây dựng định dạng có cấu trúc mong muốn (ví dụ: JSON). Mỗi thành phần mô hình đều dựa trên Transformer, và do đó mô hình được huấn luyện dễ dàng theo cách đầu-cuối. Toàn bộ quá trình của Donut được minh họa trong Hình 3.

Bộ mã hóa. Bộ mã hóa trực quan chuyển đổi hình ảnh tài liệu đầu vào x∈R^(H×W×C) thành một tập hợp các embedding {zi|zi∈R^d,1≤i≤n}, trong đó n là kích thước feature map hoặc số lượng patch hình ảnh và d là chiều của các vector tiềm ẩn của bộ mã hóa. Lưu ý rằng các mô hình dựa trên CNN [17] hoặc các mô hình dựa trên Transformer [9,40] có thể được sử dụng làm mạng mã hóa. Trong nghiên cứu này, chúng tôi sử dụng Swin Transformer [40] vì nó cho thấy hiệu suất tốt nhất trong nghiên cứu sơ bộ của chúng tôi về phân tích tài liệu. Swin Transformer đầu tiên chia hình ảnh đầu vào x thành các patch không chồng lấp. Các khối Swin Transformer, bao gồm một mô-đun multi-head self-attention dựa trên cửa sổ dịch chuyển và một MLP hai lớp, được áp dụng cho các patch. Sau đó, các lớp patch merging được áp dụng cho các token patch ở mỗi giai đoạn. Đầu ra của khối Swin Transformer cuối cùng {z} được đưa vào bộ giải mã văn bản tiếp theo.

Bộ giải mã. Với {z}, bộ giải mã văn bản tạo ra một chuỗi token (yi)^m_(i=1), trong đó yi∈R^v là một vector one-hot cho token thứ i, v là kích thước của từ vựng token, và m là một siêu tham số. Chúng tôi sử dụng BART [33] làm kiến trúc giải mã. Cụ thể, chúng tôi khởi tạo các trọng số mô hình giải mã với những trọng số từ mô hình BART đa ngôn ngữ tiền huấn luyện có sẵn công khai.

Đầu vào mô hình. Theo Transformer gốc [58], chúng tôi sử dụng sơ đồ teacher-forcing [62], đây là một chiến lược huấn luyện mô hình sử dụng ground truth làm đầu vào thay vì đầu ra mô hình từ bước thời gian trước đó. Trong giai đoạn kiểm thử, được lấy cảm hứng từ GPT-3 [5], mô hình tạo ra một chuỗi token với một prompt đã cho. Chúng tôi thêm các token đặc biệt mới cho prompt cho mỗi nhiệm vụ downstream trong các thí nghiệm của chúng tôi. Các prompt mà chúng tôi sử dụng cho các ứng dụng của chúng tôi được hiển thị cùng với các chuỗi đầu ra mong muốn trong Hình 3. Giải thích minh họa cho chiến lược teacher-forcing và định dạng đầu ra giải mã có sẵn trong Phụ lục A.4.

Chuyển đổi đầu ra. Chuỗi token đầu ra được chuyển đổi thành định dạng có cấu trúc mong muốn. Chúng tôi áp dụng định dạng JSON do khả năng biểu diễn cao của nó. Như thể hiện trong Hình 3, một chuỗi token có thể chuyển đổi một-một thành dữ liệu JSON. Chúng tôi đơn giản thêm hai token đặc biệt [START ∗] và [END ∗], trong đó ∗ chỉ ra mỗi trường cần trích xuất. Nếu chuỗi token đầu ra có cấu trúc sai, chúng tôi đơn giản coi trường đó bị mất. Ví dụ, nếu chỉ có [START name] tồn tại mà không có [END name], chúng tôi giả định mô hình không thể trích xuất trường "name". Thuật toán này có thể dễ dàng được triển khai với các biểu thức chính quy đơn giản [11].

2.3 Tiền huấn luyện

Nhiệm vụ. Mô hình được huấn luyện để đọc tất cả văn bản trong hình ảnh theo thứ tự đọc (từ trên-trái đến dưới-phải, cơ bản). Mục tiêu là tối thiểu hóa cross-entropy loss của việc dự đoán token tiếp theo bằng cách điều kiện hóa đồng thời trên hình ảnh và ngữ cảnh trước đó. Nhiệm vụ này có thể được hiểu như một nhiệm vụ pseudo-OCR. Mô hình được huấn luyện như một mô hình ngôn ngữ trực quan trên các kho dữ liệu trực quan, tức là hình ảnh tài liệu.

Kho dữ liệu trực quan. Chúng tôi sử dụng IIT-CDIP [32], là một tập hợp 11M hình ảnh tài liệu tiếng Anh được quét. Một API OCR thương mại CLOVA được áp dụng để có được các nhãn văn bản giả. Tuy nhiên, như đã đề cập, loại bộ dữ liệu này không phải lúc nào cũng có sẵn, đặc biệt là cho các ngôn ngữ khác tiếng Anh. Để giảm bớt sự phụ thuộc, chúng tôi xây dựng một Trình Tạo Tài Liệu Tổng Hợp có thể mở rộng, được gọi là SynthDoG. Sử dụng SynthDoG và Wikipedia tiếng Trung, Nhật, Hàn và Anh, chúng tôi tạo ra 0,5M mẫu mỗi ngôn ngữ.

Trình tạo tài liệu tổng hợp. Đường ống render hình ảnh cơ bản theo Yim et al. [67]. Như thể hiện trong Hình 4, mẫu được tạo bao gồm nhiều thành phần; nền, tài liệu, văn bản và bố cục. Hình ảnh nền được lấy mẫu từ ImageNet [7], và kết cấu của tài liệu được lấy mẫu từ các ảnh giấy đã thu thập. Từ và cụm từ được lấy mẫu từ Wikipedia. Bố cục được tạo bởi một thuật toán dựa trên quy tắc đơn giản xếp chồng lưới một cách ngẫu nhiên. Ngoài ra, một số kỹ thuật render hình ảnh [13,41,67] được áp dụng để bắt chước tài liệu thực. Các ví dụ được tạo ra được hiển thị trong Hình 4. Chi tiết thêm về SynthDoG có sẵn trong mã nguồn và Phụ lục A.2.

2.4 Tinh chỉnh

Sau khi mô hình học cách đọc, trong giai đoạn ứng dụng (tức là tinh chỉnh), chúng tôi dạy mô hình cách hiểu hình ảnh tài liệu. Như thể hiện trong Hình 3, chúng tôi diễn giải tất cả các nhiệm vụ downstream như một bài toán dự đoán JSON.

Bộ giải mã được huấn luyện để tạo ra một chuỗi token có thể được chuyển đổi thành JSON biểu diễn thông tin đầu ra mong muốn. Ví dụ, trong nhiệm vụ phân loại tài liệu, bộ giải mã được huấn luyện để tạo ra chuỗi token [START class][memo][END class] có thể chuyển đổi 1-1 thành JSON {"class": "memo"}. Chúng tôi giới thiệu một số token đặc biệt (ví dụ: [memo] được sử dụng để biểu diễn lớp "memo"), nếu việc thay thế như vậy có sẵn trong nhiệm vụ mục tiêu.

3 Thí nghiệm và phân tích

Trong phần này, chúng tôi trình bày kết quả tinh chỉnh Donut trên ba ứng dụng VDU trên sáu bộ dữ liệu khác nhau bao gồm cả điểm chuẩn công khai và bộ dữ liệu dịch vụ công nghiệp riêng tư. Các mẫu được hiển thị trong Hình 5.

3.1 Nhiệm vụ downstream và bộ dữ liệu

Phân loại tài liệu. Để xem liệu mô hình có thể phân biệt các loại tài liệu khác nhau hay không, chúng tôi kiểm tra một nhiệm vụ phân loại. Khác với các mô hình khác dự đoán nhãn lớp thông qua softmax trên embedding được mã hóa, Donut tạo ra JSON chứa thông tin lớp để duy trì tính đồng nhất của phương pháp giải quyết nhiệm vụ. Chúng tôi báo cáo độ chính xác phân loại tổng thể trên tập kiểm tra.

RVL-CDIP. Bộ dữ liệu RVL-CDIP [16] bao gồm 400K hình ảnh trong 16 lớp, với 25K hình ảnh mỗi lớp. Các lớp bao gồm thư, ghi nhớ, email, v.v. Có 320K hình ảnh huấn luyện, 40K xác thực và 40K kiểm tra.

Trích xuất thông tin tài liệu. Để xem mô hình có hiểu đầy đủ các bố cục và ngữ cảnh phức tạp trong tài liệu hay không, chúng tôi kiểm tra các nhiệm vụ trích xuất thông tin tài liệu (IE) trên nhiều hình ảnh tài liệu thực khác nhau bao gồm cả điểm chuẩn công khai và bộ dữ liệu công nghiệp thực. Trong nhiệm vụ này, mô hình nhằm mục đích ánh xạ mỗi tài liệu thành một dạng thông tin có cấu trúc phù hợp với ontology mục tiêu hoặc schema cơ sở dữ liệu. Xem Hình 1 để có ví dụ minh họa. Mô hình không chỉ phải đọc ký tự tốt mà còn phải hiểu bố cục và ngữ nghĩa để suy luận các nhóm và cấu trúc phân cấp lồng nhau giữa các văn bản.

Chúng tôi đánh giá các mô hình với hai số liệu; điểm F1 cấp trường [22,65,18] và độ chính xác dựa trên Tree Edit Distance (TED) [68,70,23]. F1 kiểm tra xem thông tin trường được trích xuất có trong ground truth hay không. Ngay cả khi một ký tự bị thiếu, điểm số giả định rằng việc trích xuất trường đã thất bại. Mặc dù F1 đơn giản và dễ hiểu, có một số hạn chế. Thứ nhất, nó không tính đến sự chồng lấp một phần. Thứ hai, nó không thể đo lường cấu trúc dự đoán (ví dụ: nhóm và cấu trúc phân cấp lồng nhau). Để đánh giá độ chính xác tổng thể, chúng tôi cũng sử dụng một số liệu khác dựa trên TED [68], có thể được sử dụng cho bất kỳ tài liệu nào được biểu diễn dưới dạng cây. Nó được tính như: max(0, 1-TED(pr,gt)/TED(ϕ,gt)), trong đó gt, pr và ϕ đại diện cho ground truth, dự đoán và cây rỗng tương ứng. Các số liệu tương tự được sử dụng trong các công trình gần đây về IE tài liệu [70,23].

Chúng tôi sử dụng hai bộ dữ liệu điểm chuẩn công khai cũng như hai bộ dữ liệu công nghiệp riêng tư từ các sản phẩm dịch vụ thực tế đang hoạt động của chúng tôi. Mỗi bộ dữ liệu được giải thích như sau.

CORD. Consolidated Receipt Dataset (CORD) [45] là một điểm chuẩn công khai bao gồm 0,8K hình ảnh biên lai huấn luyện, 0,1K hợp lệ, 0,1K kiểm tra. Các chữ cái của biên lai sử dụng bảng chữ cái Latin. Số lượng trường duy nhất là 30 bao gồm tên menu, số lượng, tổng giá, v.v. Có các cấu trúc phức tạp (tức là nhóm và cấu trúc phân cấp lồng nhau như items>item>{name, count, price}) trong thông tin. Xem Hình 1 để biết chi tiết.

Ticket. Đây là một bộ dữ liệu điểm chuẩn công khai [12] bao gồm 1,5K hình ảnh vé tàu Trung Quốc huấn luyện và 0,4K kiểm tra. Chúng tôi tách 10% tập huấn luyện làm tập xác thực. Có 8 trường là số vé, ga xuất phát, số tàu, v.v. Cấu trúc thông tin đơn giản và tất cả các khóa được đảm bảo chỉ xuất hiện một lần và vị trí của mỗi trường được cố định.

Business Card (Dữ liệu trong dịch vụ). Bộ dữ liệu này từ các sản phẩm đang hoạt động của chúng tôi hiện đang được triển khai. Bộ dữ liệu bao gồm 20K danh thiếp Nhật Bản huấn luyện, 0,3K hợp lệ, 0,3K kiểm tra. Số lượng trường là 11, bao gồm tên, công ty, địa chỉ, v.v. Cấu trúc thông tin tương tự với bộ dữ liệu Ticket.

Receipt (Dữ liệu trong dịch vụ). Bộ dữ liệu này cũng từ một trong những sản phẩm thực của chúng tôi. Bộ dữ liệu bao gồm 40K hình ảnh biên lai Hàn Quốc huấn luyện, 1K hợp lệ, 1K kiểm tra. Số lượng trường duy nhất là 81, bao gồm thông tin cửa hàng, thông tin thanh toán, thông tin giá, v.v. Mỗi mẫu có cấu trúc phức tạp so với các bộ dữ liệu đã đề cập. Do chính sách công nghiệp, không phải tất cả mẫu đều có thể công bố. Một số mẫu chất lượng cao giống thật được hiển thị trong Hình 5 và trong tài liệu bổ sung.

Trả lời câu hỏi trực quan tài liệu. Để xác thực khả năng sâu hơn của mô hình, chúng tôi tiến hành một nhiệm vụ trả lời câu hỏi trực quan tài liệu (DocVQA). Trong nhiệm vụ này, một cặp hình ảnh tài liệu và câu hỏi được đưa ra và mô hình dự đoán câu trả lời cho câu hỏi bằng cách nắm bắt cả thông tin trực quan và văn bản trong hình ảnh. Chúng tôi làm cho bộ giải mã tạo ra câu trả lời bằng cách đặt câu hỏi làm prompt khởi đầu để giữ tính đồng nhất của phương pháp (Xem Hình 3).

DocVQA. Bộ dữ liệu từ cuộc thi Document Visual Question Answering và bao gồm 50K câu hỏi được định nghĩa trên hơn 12K tài liệu [44]. Có 40K câu hỏi huấn luyện, 5K hợp lệ và 5K kiểm tra. Số liệu đánh giá là ANLS (Average Normalized Levenshtein Similarity) là một số liệu dựa trên khoảng cách chỉnh sửa. Điểm số trên tập kiểm tra được đo thông qua trang web đánh giá.

3.2 Thiết lập

Chúng tôi sử dụng Swin-B [40] làm bộ mã hóa trực quan của Donut với một số sửa đổi nhỏ. Chúng tôi đặt số lượng lớp và kích thước cửa sổ là {2,2,14,2} và 10. Trong việc xem xét thêm về sự đánh đổi tốc độ-độ chính xác, chúng tôi sử dụng bốn lớp đầu tiên của BART làm bộ giải mã. Như đã giải thích trong Phần 2.3, chúng tôi huấn luyện Donut đa ngôn ngữ sử dụng 2M hình ảnh tài liệu tổng hợp và 11M hình ảnh tài liệu quét IIT-CDIP. Chúng tôi tiền huấn luyện mô hình trong 200K bước với 64 GPU A100 và kích thước mini-batch là 196. Chúng tôi sử dụng bộ tối ưu hóa Adam [30], tốc độ học được lập lịch và tốc độ ban đầu được chọn từ 1e-5 đến 1e-4. Độ phân giải đầu vào được đặt ở 2560×1920 và độ dài tối đa trong bộ giải mã được đặt ở 1536. Tất cả kết quả tinh chỉnh đều đạt được bằng cách bắt đầu từ mô hình đa ngôn ngữ tiền huấn luyện. Một số siêu tham số được điều chỉnh khi tinh chỉnh và trong các nghiên cứu ablation. Chúng tôi sử dụng 960×1280 cho các nhiệm vụ phân tích Train Tickets và Business Card. Chúng tôi tinh chỉnh mô hình trong khi giám sát khoảng cách chỉnh sửa trên chuỗi token. Tốc độ của Donut được đo trên GPU P40, chậm hơn nhiều so với A100. Đối với các baseline dựa trên OCR, các công cụ OCR tối ưu được sử dụng, bao gồm MS OCR API được sử dụng trong [64] và CLOVA OCR API được sử dụng trong [24,23]. Phân tích về các công cụ OCR có sẵn trong Phần 3.4. Chi tiết thêm về thiết lập OCR và huấn luyện có sẵn trong Phụ lục A.1 và A.5.

3.3 Kết quả thí nghiệm

Phân loại tài liệu. Kết quả được hiển thị trong Bảng 1. Mà không dựa vào bất kỳ tài nguyên nào khác (ví dụ: công cụ OCR có sẵn), Donut cho thấy hiệu suất tối ưu giữa các mô hình VDU đa năng như LayoutLM [65] và LayoutLMv2 [64]. Đặc biệt, Donut vượt qua độ chính xác LayoutLMv2 được báo cáo trong [64], trong khi sử dụng ít tham số hơn với tốc độ nhanh gấp 2 lần. Lưu ý rằng các mô hình dựa trên OCR phải xem xét thêm các tham số mô hình và tốc độ cho toàn bộ framework OCR, điều này nói chung không nhỏ. Ví dụ, một mô hình dựa trên OCR tiến bộ gần đây [4,3] yêu cầu hơn 80M tham số. Ngoài ra, huấn luyện và duy trì các hệ thống dựa trên OCR rất tốn kém [23], dẫn đến nhu cầu cho phương pháp đầu-cuối như Donut.

Trích xuất thông tin tài liệu. Bảng 2 cho thấy kết quả trên bốn nhiệm vụ IE tài liệu khác nhau. Nhóm đầu tiên sử dụng phương pháp IE dựa trên BIO-tagging truyền thống [22]. Chúng tôi theo các quy ước trong IE [65,18]. OCR trích xuất văn bản và bounding box từ hình ảnh, và sau đó mô-đun serialization sắp xếp tất cả văn bản với thông tin hình học trong bounding box. Nhiệm vụ nhận dạng thực thể có tên dựa trên BIO-tagging thực hiện phân loại tag cấp token trên các văn bản được sắp xếp để tạo ra dạng có cấu trúc. Chúng tôi kiểm tra ba backbone VDU đa năng, BERT [8], BROS [18], LayoutLM [65] và LayoutLMv2 [64,66]. Chúng tôi cũng kiểm tra hai mô hình IE được đề xuất gần đây, SPADE [24] và WYVERN [23]. SPADE là một phương pháp IE dựa trên đồ thị dự đoán quan hệ giữa các bounding box. WYVERN là một mô hình Transformer encoder-decoder trực tiếp tạo ra các thực thể với cấu trúc dựa trên đầu ra OCR. WYVERN khác với Donut ở chỗ nó lấy đầu ra OCR làm đầu vào.

Đối với tất cả các lĩnh vực, bao gồm cả bộ dữ liệu công khai và riêng tư trong dịch vụ, Donut cho thấy điểm số tốt nhất giữa các mô hình so sánh. Bằng cách đo cả F1 và độ chính xác dựa trên TED, chúng tôi quan sát thấy không chỉ Donut có thể trích xuất thông tin chính mà còn dự đoán cấu trúc phức tạp giữa thông tin trường. Chúng tôi quan sát thấy rằng độ phân giải đầu vào lớn cho độ chính xác mạnh mẽ nhưng làm mô hình chậm hơn. Ví dụ, hiệu suất trên CORD với 1280×960 là 0,7 giây/hình ảnh và 91,1 độ chính xác. Nhưng độ phân giải lớn cho thấy hiệu suất tốt hơn trong tình huống ít tài nguyên. Các phân tích chi tiết ở Phần 3.4. Khác với các baseline khác, Donut cho thấy hiệu suất ổn định bất kể kích thước bộ dữ liệu và độ phức tạp của nhiệm vụ (Xem Hình 5). Đây là một tác động đáng kể vì các nhiệm vụ mục tiêu đã được sử dụng tích cực trong các ngành công nghiệp.

Trả lời câu hỏi trực quan tài liệu. Bảng 3 cho thấy kết quả trên bộ dữ liệu DocVQA. Nhóm đầu tiên là các backbone VDU đa năng có điểm số từ bài báo LayoutLMv2 [64]. Chúng tôi đo thời gian chạy với MS OCR API được sử dụng trong [64]. Mô hình trong nhóm thứ ba là mô hình tinh chỉnh dành riêng cho DocVQA của LayoutLMv2, có kết quả suy luận có sẵn trong bảng xếp hạng chính thức.

Như có thể thấy, Donut đạt được điểm số cạnh tranh với các baseline phụ thuộc vào công cụ OCR bên ngoài. Đặc biệt, Donut cho thấy rằng nó mạnh mẽ với các tài liệu viết tay, được biết là khó xử lý. Trong phương pháp truyền thống, việc thêm mô-đun xử lý sau hiệu chỉnh lỗi OCR là một tùy chọn để tăng cường pipeline [51,50,10] hoặc áp dụng kiến trúc encoder-decoder trên đầu ra OCR có thể giảm thiểu các vấn đề của lỗi OCR [23]. Tuy nhiên, loại phương pháp này có xu hướng tăng kích thước toàn bộ hệ thống và chi phí bảo trì. Donut cho thấy một hướng hoàn toàn khác. Một số kết quả suy luận được hiển thị trong Hình 6. Các mẫu cho thấy điểm mạnh hiện tại của Donut cũng như các thách thức còn lại trong phương pháp đầu-cuối như Donut. Phân tích và ablation thêm có sẵn trong Phần 3.4.

3.4 Nghiên cứu thêm

Trong phần này, chúng tôi nghiên cứu một số yếu tố hiểu Donut. Chúng tôi cho thấy một số đặc điểm nổi bật của Donut thông qua các thí nghiệm và trực quan hóa.

Về chiến lược tiền huấn luyện. Chúng tôi kiểm tra một số nhiệm vụ tiền huấn luyện cho VDU. Hình 7(a) cho thấy rằng nhiệm vụ tiền huấn luyện Donut (tức là đọc văn bản) là phương pháp đơn giản nhất nhưng hiệu quả nhất. Các nhiệm vụ khác áp đặt kiến thức tổng quát về hình ảnh và văn bản lên mô hình, ví dụ: image captioning, cho thấy ít cải thiện trong các nhiệm vụ tinh chỉnh. Đối với các nhiệm vụ đọc văn bản, chúng tôi xác minh ba tùy chọn, chỉ SynthDoG, chỉ IIT-CDIP, và cả hai. Lưu ý rằng hình ảnh tổng hợp đã đủ cho nhiệm vụ IE tài liệu trong phân tích của chúng tôi. Tuy nhiên, trong nhiệm vụ DocVQA, việc xem hình ảnh thực rất quan trọng. Điều này có thể là do phân phối hình ảnh của IIT-CDIP và DocVQA tương tự [44].

Về backbone mã hóa. Ở đây, chúng tôi nghiên cứu các backbone phân loại hình ảnh phổ biến cho thấy hiệu suất vượt trội trong các nhiệm vụ thị giác truyền thống để đo hiệu suất của chúng trong các nhiệm vụ VDU. Hình 7(b) cho thấy kết quả so sánh. Chúng tôi sử dụng tất cả backbone được tiền huấn luyện trên ImageNet [7]. EfficientNetV2 [55] và Swin Transformer [40] vượt trội hơn những backbone khác trên cả hai bộ dữ liệu. Chúng tôi cho rằng điều này do khả năng biểu đạt cao của các backbone, điều này được chứng minh bằng điểm số nổi bật trên một số nhiệm vụ downstream. Chúng tôi chọn Swin Transformer do khả năng mở rộng cao của kiến trúc dựa trên Transformer và hiệu suất cao hơn EfficientNetV2.

Về độ phân giải đầu vào. Hình 7(c) cho thấy hiệu suất của Donut tăng nhanh khi chúng tôi đặt kích thước đầu vào lớn hơn. Điều này rõ ràng hơn trong DocVQA nơi hình ảnh lớn hơn với nhiều văn bản nhỏ. Nhưng việc tăng kích thước để có kết quả chính xác gây ra chi phí tính toán lớn hơn. Sử dụng cơ chế attention hiệu quả [60] có thể tránh vấn đề này trong thiết kế kiến trúc, nhưng chúng tôi sử dụng Transformer gốc [58] vì chúng tôi nhằm mục đích trình bày kiến trúc đơn giản hơn trong công trình này.

Về định vị văn bản. Để xem mô hình hoạt động như thế nào, chúng tôi trực quan hóa các bản đồ cross attention của bộ giải mã với hình ảnh tài liệu chưa thấy. Như có thể thấy trong Hình 8, mô hình cho thấy kết quả có ý nghĩa có thể được sử dụng như một chỉ số phụ trợ. Mô hình chú ý đến vị trí mong muốn trong hình ảnh đã cho.

Về hệ thống OCR. Chúng tôi kiểm tra bốn công cụ OCR công khai được sử dụng rộng rãi (Xem Hình 9). Kết quả cho thấy rằng hiệu suất (tức là tốc độ và độ chính xác) của các phương pháp dựa trên OCR truyền thống phụ thuộc rất nhiều vào công cụ OCR có sẵn. Chi tiết thêm về các công cụ OCR có sẵn trong Phụ lục A.1.

Về tình huống ít tài nguyên. Chúng tôi đánh giá các mô hình bằng cách hạn chế kích thước tập huấn luyện của CORD [45]. Các đường cong hiệu suất được hiển thị trong Hình 9 bên phải. Donut cho thấy hiệu suất mạnh mẽ. Chúng tôi cũng quan sát thấy rằng độ phân giải đầu vào lớn hơn, 2560×1920, cho thấy điểm số mạnh mẽ hơn trong tình huống cực kỳ ít tài nguyên, ví dụ: 80 mẫu. Như có thể thấy, Donut vượt qua độ chính xác LayoutLMv2 chỉ với 10% dữ liệu, tức là chỉ 80 mẫu.

4 Công trình liên quan

4.1 Nhận dạng ký tự quang học

Xu hướng gần đây của nghiên cứu OCR là sử dụng các mô hình deep learning trong hai bước phụ của nó: 1) các vùng văn bản được dự đoán bởi detector; 2) text recognizer sau đó nhận dạng tất cả ký tự trong các thực thể hình ảnh đã cắt. Cả hai đều được huấn luyện với bộ dữ liệu quy mô lớn bao gồm hình ảnh tổng hợp [26,13] và hình ảnh thực [28,47].

Các phương pháp phát hiện ban đầu sử dụng CNN để dự đoán các phân khúc cục bộ và áp dụng heuristic để hợp nhất chúng [19,69]. Sau đó, các phương pháp dựa trên region proposal và bounding box regression được đề xuất [36]. Gần đây, tập trung vào tính đồng nhất và địa phương của văn bản, các phương pháp cấp thành phần đã được đề xuất [56,4].

Nhiều text recognizer hiện đại chia sẻ phương pháp tương tự [37,53,52,59] có thể được hiểu thành sự kết hợp của một số mô-đun deep learning phổ biến [3]. Với hình ảnh thực thể văn bản đã cắt, hầu hết các mô hình nhận dạng văn bản gần đây áp dụng CNN để mã hóa hình ảnh thành không gian đặc trưng. Sau đó một bộ giải mã được áp dụng để trích xuất ký tự từ các đặc trưng.

4.2 Hiểu tài liệu trực quan

Phân loại loại tài liệu là một bước cốt lõi hướng tới xử lý tài liệu tự động. Các phương pháp ban đầu xử lý vấn đề như phân loại hình ảnh tổng quát, vì vậy nhiều CNN khác nhau đã được kiểm tra [27,1,15]. Gần đây, với BERT [8], các phương pháp dựa trên sự kết hợp CV và NLP đã được đề xuất rộng rãi [65,34]. Như một phương pháp phổ biến, hầu hết các phương pháp dựa vào công cụ OCR để trích xuất văn bản; sau đó các văn bản được OCR hóa được serialize thành chuỗi token; cuối cùng chúng được đưa vào mô hình ngôn ngữ (ví dụ: BERT) với một số đặc trưng trực quan nếu có sẵn. Mặc dù ý tưởng đơn giản, các phương pháp cho thấy cải thiện hiệu suất đáng kể và trở thành xu hướng chính trong những năm gần đây [64,35,2].

IE tài liệu bao gồm một loạt các ứng dụng thực tế [22,42], ví dụ: với một loạt hình ảnh biên lai thô, một parser tài liệu có thể tự động hóa phần lớn quá trình số hóa biên lai, điều này đã đòi hỏi vô số lao động con người trong pipeline truyền thống. Hầu hết các mô hình gần đây [25,23] lấy đầu ra của OCR làm đầu vào của chúng. Kết quả OCR sau đó được chuyển đổi thành phân tích cuối cùng thông qua một số quá trình, thường phức tạp. Bất chấp nhu cầu trong ngành công nghiệp, chỉ có một số ít công trình đã được thử nghiệm về phân tích đầu-cuối. Gần đây, một số công trình được đề xuất để đơn giản hóa các quá trình phân tích phức tạp [25,23]. Nhưng chúng vẫn dựa vào OCR riêng biệt để trích xuất thông tin văn bản.

QA trực quan trên tài liệu tìm cách trả lời câu hỏi được hỏi trên hình ảnh tài liệu. Nhiệm vụ này đòi hỏi lý luận trên các yếu tố trực quan của hình ảnh và kiến thức tổng quát để suy luận câu trả lời đúng [44]. Hiện tại, hầu hết các công nghệ tiên tiến theo pipeline đơn giản bao gồm áp dụng OCR theo sau bởi các transformer giống BERT [65,64]. Tuy nhiên, các phương pháp hoạt động theo cách trích xuất theo bản chất của chúng. Do đó, có một số mối quan tâm về câu hỏi mà câu trả lời không xuất hiện trong hình ảnh đã cho [57]. Để giải quyết các mối quan tâm này, các phương pháp dựa trên generation cũng đã được đề xuất [48].

5 Kết luận

Trong công trình này, chúng tôi đề xuất một framework đầu-cuối mới cho hiểu tài liệu trực quan. Phương pháp được đề xuất, Donut, trực tiếp ánh xạ một hình ảnh tài liệu đầu vào thành đầu ra có cấu trúc mong muốn. Khác với các phương pháp truyền thống, Donut không phụ thuộc vào OCR và có thể dễ dàng được huấn luyện theo cách đầu-cuối. Chúng tôi cũng đề xuất một trình tạo hình ảnh tài liệu tổng hợp, SynthDoG, để giảm bớt sự phụ thuộc vào hình ảnh tài liệu thực quy mô lớn và chúng tôi cho thấy rằng Donut có thể dễ dàng được mở rộng thành cài đặt đa ngôn ngữ. Chúng tôi đã huấn luyện mô hình từ từ từ cách đọc đến cách hiểu thông qua pipeline huấn luyện đề xuất. Các thí nghiệm và phân tích sâu rộng của chúng tôi trên cả điểm chuẩn công khai bên ngoài và bộ dữ liệu dịch vụ nội bộ riêng tư cho thấy hiệu suất cao hơn và hiệu quả chi phí tốt hơn của phương pháp đề xuất. Đây là một tác động đáng kể vì các nhiệm vụ mục tiêu đã được sử dụng thực tế trong các ngành công nghiệp. Tăng cường mục tiêu tiền huấn luyện có thể là một hướng công trình tương lai. Chúng tôi tin rằng công trình của chúng tôi có thể dễ dàng được mở rộng sang các lĩnh vực/nhiệm vụ khác liên quan đến hiểu tài liệu.
