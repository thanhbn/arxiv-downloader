# 2404.12022.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/speculative/2404.12022.pdf
# File size: 1101648 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Parallel Decoding via Hidden Transfer for
Lossless Large Language Model Acceleration
Pengfei Wu1,2∗, Jiahao Liu3∗, Zhuocheng Gong1, Qifan Wang4, Jinpeng Li1
Jingang Wang3, Xunliang Cai3, Dongyan Zhao1,2,5,6 †
1Wangxuan Institute of Computer Technology, Peking University
2Center for Data Science, AAIS, Peking University;3Meituan;4Meta AI
5National Key Laboratory of General Artificial Intelligence;6BIGAI, Beijing, China
{pengfeiwu1999,lijinpeng}@stu.pku.edu.cn
{liujiahao12,wangjingang02,caixunliang}@meituan.com
wqfcr@fb.com ,{zhaody,gzhch}@pku.edu.cn
Abstract
Large language models (LLMs) have recently
shown remarkable performance across a wide
range of tasks. However, the substantial num-
ber of parameters in LLMs contributes to sig-
nificant latency during model inference. This is
particularly evident when utilizing autoregres-
sive decoding methods, which generate one
token in a single forward process, thereby not
fully capitalizing on the parallel computing ca-
pabilities of GPUs. In this paper, we propose
a novel parallel decoding approach, namely
hidden transfer , which decodes multiple suc-
cessive tokens simultaneously in a single for-
ward pass. The idea is to transfer the interme-
diate hidden states of the previous context to
thepseudo hidden states of the future tokens
to be generated, and then the pseudo hidden
states will pass the following transformer layers
thereby assimilating more semantic informa-
tion and achieving superior predictive accuracy
of the future tokens.
Besides, we use the novel tree attention mech-
anism to simultaneously generate and verify
multiple candidates of output sequences, which
ensure the lossless generation and further im-
proves the generation efficiency of our method.
Experiments demonstrate the effectiveness of
our method. We conduct a lot of analytic ex-
periments to prove our motivation. In terms
of acceleration metrics, we outperform all the
single-model acceleration techniques, includ-
ing Medusa and Self-Speculative decoding.
1 Introduction
Recent developments in Transformer-based large
language models (Vaswani et al., 2017; Radford
et al., 2018, 2019; Brown et al., 2020; Zeng
et al., 2022; Zhang et al., 2022; Touvron et al.,
2023a,b; Roziere et al., 2023) have demonstrated
∗Equal contribution.
†Corresponding authors: Dongyan Zhao
(zhaody@pku.edu.cn).
Figure 1: A single forward time consumption for vari-
ous LLMs with different size under different KV cache
length and different number of input tokens, each data is
the average of randomly 100 samples. The result shows
that under the setting of KV cache, increasing the length
of input sequence in a certain range will not increase
the time of forward propagation, and then prove that the
traditional autoregressive decoding has a waste in GPU
utilization efficiency
remarkable performance across a broad spectrum
of tasks. However, these models grapple with ex-
cessive inference latency due to the inherently se-
rial process of generating one token per forward
pass, the inference process is typically memory
bandwidth-bound, which means most of the time
model inference is spent loading billion parameters
from memory rather than computing, resulting in
the waste of the parallel computational power of
GPUs (Shazeer, 2019). In our experiments, We
find that due to the parallelism of GPU comput-
ing, the time for multiple tokens to propagate in
parallel is nearly the same as the time for one to-
ken to propagate(as shown in Table 1). Therefore,arXiv:2404.12022v1  [cs.CL]  18 Apr 2024

--- PAGE 2 ---
the low efficiency of the inference stage becomes
the biggest bottleneck to broaden the application
scenarios of LLMs.
To address this bottleneck, contemporary work
proposes speculative decoding (Leviathan et al.,
2023; Chen et al., 2023a; Zhang et al., 2023), which
utilizes a small language model to draft a few to-
kens ahead. then the LLM verifies the drafted
tokens and accepts the correct ones. While this
acceleration technique achieves promising perfor-
mance, it has its limitations: speculative decoding
requires another model to do the draft thing. It is
inconvenient to cooperate with an extra model in
some scenarios as it requires more sophisticated
scheduling and the draft model might consume
extra GPU and memory resources. Thus, some
researchers have been investigating single-model
acceleration. That is, to speed up the inference of
LLMs without auxiliary models. Self-speculative
decoding (Zhang et al., 2023) and Medusa (Cai
et al., 2023) are typical methods within this line of
research. Medusa predicts not only the next token
within a single forward propagation but also a few
tokens ahead of the next token. These extra tokens
are predicted based on the last hidden states of the
input tokens through the trainable Medusa heads.
Our method aligns with the line of single-model
acceleration. We design a novel method called Hid-
den Transfer , to predict the pseudo hidden states
of future tokens in the intermediate layers. We use
a trainable linear projection to transfer the hidden
states of input tokens to the pseudo hidden states
of future tokens in a certain intermediate layer, and
the synthesized pseudo hidden states pass the sub-
sequent layers and interact with hidden states of the
whole sequence as normal, in the last layer, we use
the original lm-head and decode the draft tokens
of the future positions. In this way, we can predict
more than merely the next token but also a few to-
kens ahead in a single forward propagation. In the
training stage we employ KL-divergence as the su-
pervised signal, which minimizes the distribution
between tokens predicted by the pseudo hidden
states and the real ones. In addition to the novel
design of Hidden Transfer, we also use the tree
attention mechanism (Cai et al., 2023; Miao et al.,
2023; Spector and Re, 2023) to simultaneously per-
form the token prediction and token verification to
ensure the lossless generation of our method.
The motivation for hidden transfer is that the
synthesized pseudo hidden states will interact with
themselves and previous hidden states of the con-text during the forward propagation in which gain
more semantic information to boost the success rate
of predicting future tokens. Our experiments show
that this motivation is fulfilled, and our method can
achieve the best draft token prediction accuracy and
inference acceleration ratio compared with other
methods under a single-model setting.
Our key contributions are: (1) To our best knowl-
edge, we are the first to study the prediction of
pseudo hidden states of the future tokens in LLMs,
our experiments prove that intermediate hidden
states could be predicted directly and refined in
the forward propagation. (2) We propose Hidden
Transfer, a novel single-model lossless accelera-
tion method for improving the inference efficiency
of LLMs. Our method predicts multiple draft to-
kens with synthetic pseudo hidden states. (3) We
conduct various experiments to prove the effec-
tiveness of our method, including some analytic
experiments to prove our motivation.
2 Related Work
To solve the problem of inference latency in LLMs,
the existing works can be divided into the following
two categories: we call the first category Model
Compression , including model distillation (Sanh
et al., 2019), model pruning (Frantar and Alis-
tarh, 2023; Wang et al., 2021) and model quan-
tization (Liu et al., 2023a), aiming to replace the
original large language model with a small model
which have the similar but not identical outputs
in certain field. We refer the second category of
methods as Speculative Decoding , the key idea of
is to reduce the number of forward propagation of
the LLMs under the condition that the generated
results remain unchanged.
2.1 Model Compression
There are plenty of works focus on the model
lightweight, including model quantization (Han
et al., 2015; Zhao et al., 2019; Jacob et al., 2018;
Yao et al., 2022; Frantar et al., 2022; Liu et al.,
2023a), knowledge distillation (Hinton et al., 2015;
Cho and Hariharan, 2019; Hsieh et al., 2023),
model pruning (Xia et al., 2023; Guo et al., 2023;
Chen et al., 2023b) and model sparsification (Hoe-
fler et al., 2021; Liu et al., 2023b). Model quantiza-
tion is to convert the model parameters to floating-
point numbers with lower precision or integers;
Model pruning and sparsification removes redun-
dant components in the LLMs; Knowledge dis-

--- PAGE 3 ---
tillation works by transferring knowledge from a
teacher model to a student model (small model).
These model compression methods have a wide
range of applications, but they do not guarantee
that the output is strictly consistent with the origi-
nal LLMs.
2.2 Speculative Decoding
This series of methods aim to quickly generate
some draft tokens and use the LLMs to verify them
in parallel to keep lossless generation theoretically.
We can divide this class of methods into two types
based on the number of deployed models, single
model and multiple models. The single models’ ap-
proach is represented by Medusa (Cai et al., 2023),
and Self-speculative decoding method (Zhang et al.,
2023), Medusa and train extra multiple heads to
predict subsequent tokens based on the last hid-
den state of the input tokens after a single forward
propagation. Self-speculative methods use a sub-
set of intermediate layers of the whole LLM as
the draft model to generate draft tokens The multi-
ple models’ approach is represented by traditional
speculative decoding (Leviathan et al., 2023; Chen
et al., 2023a), which uses a small language mod-
els (SLMs) as the draft model to generate draft
tokens, the LLMs verify the tokens in parallel.
3 Methodology
In this section, we first define the problem formula-
tion, including the overview of traditional autore-
gressive decoding algorithm and parallel decoding
algorithm, then we provide a detailed description
of the training and testing process of our hidden
transfer method.
3.1 Problem Formulation
Transformer-based auto-regressive language mod-
els aim to construct the the (n+ 1)thtoken’s dis-
tribution given the prefix ntokens, denoted as
P(xn+1|x1, x2, . . . , x n)These models are capable
of processing entire sequences in parallel during the
training stage. However, during inference stage, the
generation process becomes serial naturally. This
is due to the requirement of the preceding nto-
kens’ semantic information to predict the probabil-
ity distribution of the (n+ 1)thtoken. Traditional
autoregressive generation techniques, whereby a
single token is produced per model forward pro-
cess, fail to capitalize on the parallel processing
capabilities of GPUs, consequently impacting the
response efficiency of various AI systems. Theessence of parallel decoding algorithms, exempli-
fied by speculative decoding, lies in their ambition
to increase the expected number of tokens gener-
ated in one single forward propagation of the LLMs
while maintain the generation consistency. Thus,
the optimization goal of this class of methods can
be written to find a maximum positive integer k
that satisfies the following conditions:
˜P(Xn+k+1. . . X n+1|X≤n) =P(Xn+k+1. . . X n+1|X≤n)
Where Pand˜Prepresent the token distribution
given by the original language model and parallel
decoding algorithm respectively. For most parallel
decoding algorithms, ˜Pis obtained by a draft and
verify process, tree attention is wildly adopted to
verify multiple candidate sequences simultaneous,
so a lot of works focus on how to generate better
candidates in the draft stage, the process can be
described as the following formula:
Xn+k+1, . . . , X n+1=M(X≤n)
In some speculative decoding algorithms, Mrep-
resents small language models, or part of the origi-
nal LLMs (Zhang et al., 2023), in the block-wise
series of methods (represented by Medusa (Cai
et al., 2023)), Mcan be viewed as the extra heads
in the last layer of LLMs, In our method, Mcan be
viewed as the hidden transfer linear projection in
some intermediate layers, in the following section,
we concisely introduce our method, including the
training and inference stages.
3.2 Hidden Transfer
The core idea of our hidden transfer is to predict
the future k+ 1tokens(including kdraft tokens
and the next token generated correctly by language
model) by one step of LLM forward propagation
without the deployment of SLMs, existing works
under this setting either use earlying exiting method
to directly predict the token distribution (Bae et al.,
2023) or train extra klm-heads to predict the k
draft tokens in the last layer, we believe that the
first method will lose a lot of information at higher
layers, assuming that we use Xnto represent the
nthtoken of the input sequence, and hj
nrepresents
thejthlayer’s hidden state of the nthtoken, the first
method trains an early lm-head to predict Xn+1in
advance, but once the Xn+1is predicted, it should
be used as the input in the next round of forward
propagation, the previous forward propagation will

--- PAGE 4 ---
Figure 2: Overview of our method. The upper-left is the training process of hidden transfer, NandMrepresent the
numbers of transformer layers. The upper-right and the bottom of the figure are the inference process of Hidden
transfer and Medusa respectively, assuming that the generation of both methods starts from the same context and
their inputs are the candidate token sequences generated in the last round, Medusa and Hidden transfer both verify
the candidate token sequences to find the last accepted token position(i.e the fourth token of the input) and generate
the next token and new draft tokens at the same time(for simplicity we only consider 2 transfer steps/medusa heads),
the next token and draft tokens construct to a tree structure and are then flatted into a sequence to be verified with
tree attention, after the verification stage, result shows Hidden transfer has more prediction accuracy and more draft
tokens accepted
stop in the middle layer, as a result the higher lay-
ers’ hidden state of Xnwill be lost(i.e. hj+1
n,hj+2
n
. . .), although there’re some works claim that they
can simply use copy mechanism to simulate the
higher layers’ hidden state (Elbayad et al., 2019),
but other experiments still show that the copy mech-
anism performs badly in some cases (Bae et al.,
2023). The second method which use extra lm-
heads to predict the future kdraft tokens is very
simple and effective, but we believe this method
lacks the interaction of the kdraft tokens and the
previous tokens because the draft tokens do not
interact with the previous tokens through the atten-
tion mechanism, for example it only use the last
hidden states of Xnto predict the Xn+2without
using the Xn+1’s information, but some times the
Xn+2depends on the Xn+1, so our method choose
to train multiple transfer functions (simple linear
projections) to predict the future kdraft token’s
hidden states in some intermediate layers by map-
ping the hj
ntohj
n+1...hj
n+k, so we can continue
the forward propagation with n+kintermediate
hidden states, During the following transformerlayers, the last khidden states will pass the original
lm-head to predict the token distribution normally,
the whole training and inference process compared
with Hidden transfer and Medusa is shown in Fig-
ure2.
3.3 Training stage
At the training stage, it is required to train multiple
linear projections in multiple fixed layers, where
the locations of the corresponding transfer layers
and the number of transfer step are considered as
hyperparameters. The number of transfer step is
equal to the number of pseudo hidden states pre-
dicted in a single forward process for one token;
hence, we denote this number as k. Because we
trainklinear projections separately so we need
to conduct the training process ktimes(we train
one linear projection for one transfer step in a cer-
tain layer). For simplicity we discuss the training
process of the ithtransfer step, we denote the in-
dex of transfer layer for step iasti. so we can
useWi
ti∈Rd×d(1≤i≤k) to denote the train-
able linear projection for the ithstep(drepresent
the hidden dimension of the LLMs). Assume we

--- PAGE 5 ---
have original token sequences X1,X2,...,Xn, we
first do the forward process to the tilayer to get
their corresponding hidden state hti
1,hti
2,...,htin,
then we transfer all of the nhidden states into their
corresponding pseudo hidden states, which can be
formulated as below:
ehtin,ehti
n−1, . . . , ehti
1=Wi
ti·(htin, hti
n−1, ..., hti
1)
After transferring the hti
jintoehti
j(1≤j≤
n), we concat the original hidden states and
the pseudo hidden states into a new sequence
(ie.hti
1, ..., hti
n−1, htin,ehti
1, . . . , ehti
n−1,ehtin). It’s easy
to show that the ehti
jis the pseudo hidden state of
hi
j+1, so in the self-attention layers it can only view
the hidden states from hti
1tohti
j+i−1and itself in
the sequence, we design attention mask to achieve
the goal. In order to ensure the consistency be-
tween the training stage and inference stage, we set
the position id j+itoehti
jto construct the position
embedding.
We then continue to forward the new se-
quence, and get the final representations of the
last layer (i.e., hl
1, . . . , hl
n−1, hl
n,ehl
1, . . . , ehl
n−1,ehl
n,
where ldenotes the number of layers of the
LLMs). We then pass the hidden states se-
quence through the original lm-head and fi-
nally get the token distribution of each position
(Pl
1, . . . , Pl
n−1, Pl
n,ePl
1, . . . , ePl
n−1,ePl
n). We use the
KL-divergence between token distributions given
by the pseudo hidden states and the original hidden
states as the supervised signal. The loss can be
formulated as below:
Loss distll =n−i−1X
q=1KL−divergence (ePl
n+q, Pl
q+i)
3.4 Inference stage
In the inference stage, the process is to first gener-
ate some sequence candidates and then verify them,
the purpose of the verification stage is to ensure
the tokens consistency with the auto-regressive de-
coding, We construct multiple sequence candidates
into a tree structure by merging their common an-
cestors, then we flat the tree into a sequence and
construct attention mask correctly to keep their or-
ders, finally we send the whole sequence into the
LLMs to verify the candidates and generate the
new candidates at the same time. Figure2 shows
the inference stage of both Hidden transfer and
Medusa.3.4.1 Tree attention
When predicting the draft tokens for the following
several steps, it becomes clear that the draft to-
kens belonging to different steps form a tree struc-
ture according to their order. The tree attention
mechanism transforms this hierarchical tree into a
linear sequence while preserving the original posi-
tional indices of each token. Moreover, it employs
a specialized attention mask to ensure that a to-
ken only attends to its ancestors within the tree
structure, thereby upholding the causal language
model’s properties. During inference, a pre-defined
tree structure and parser facilitate the rapid trans-
formation of token candidates between the tree and
sequence representations, obviating the need for
additional computations.
4 Experiment
4.1 Setup
We evaluate the our method on two different series
of models with different size, including LLaMA-
2-CHAT-13B (Touvron et al., 2023b), LLaMA-
2-CHAT-7B (Touvron et al., 2023b) and Vicuna-
13B (Chiang et al., 2023), Vicuna-7B (Chiang et al.,
2023). We use greedy sampling strategy for the
LLMs and the tokens generated using our method
are identical to those generated by standard auto-
regressive decoding theoretically. We divide the
experiments into main experiments subsection, an-
alytical and ablation study. The main experiments
are conducted on all models and the analytical and
ablation study only conducted on 7B model for
simplicity.
In the main experiments subsection, we com-
pare our end-to-end time acceleration ratio with
Medusa (Cai et al., 2023) and Self-speculative de-
coding (Zhang et al., 2023) to show the effective-
ness of our method(more details in Appendix), Be-
cause the self-speculative decoding method (Zhang
et al., 2023) need to carefully select the transformer
layers skipped for each model, and it doesn’t of-
fer the initial skip layers of 7B model in its open
source code for their optimizer algorithm, so we
only compare with them on LLaMA-2-Chat-13B
and vicuna-13B, we use the acceleration ratio re-
ported in their paper of LLaMA-2-Chat-13B for
the Xsum dataset, and run its open source code to
evaluate their effectiveness on the Gsm8K dataset
and vicuna-13B. we use the same Tree structure
and predict the next three draft tokens (exclude the
next token generated by the original lm-head) for

--- PAGE 6 ---
Model Decoding Algorithm XSum Gsm8k
LLaMA-2-Chat-13BAuto-regressive 1.000 × 1.000×
Self-speculative (Zhang et al., 2023) 1.241 × 1.216×
Medusa (Cai et al., 2023) 1.325 × 1.976×
Ours 1.532 × 2.275×
Vicuna-13BAuto-regressive 1.000 × 1.000×
Self-speculative (Zhang et al., 2023) 1.125 × 1.118×
Medusa (Cai et al., 2023) 1.247 × 1.869×
Ours 1.419 × 2.150×
LlaMA-2-Chat-7BAuto-regressive 1.000 × 1.000×
Medusa (Cai et al., 2023) 1.465 × 1.762×
Ours 1.816 × 2.135×
Vicuna-7BAuto-regressive 1.000 × 1.000×
Medusa (Cai et al., 2023) 1.388 × 1.732×
Ours 1.786 × 2.219×
Table 1: The acceleration ratio for both forward times and end-to-end time
Medusa and our method, and we do hidden transfer
for our method on the 25 30 35 layers respectively.
Analytical experiments subsection and ablation
study aim to verify our motivation, So we first com-
pare the draft tokens prediction accuracy between
our method and two baselines (e.g. Medusa heads
and Early existing on different intermediate layers),
result shows that we have the best prediction accu-
racy for the future draft tokens in a single forward
propagation; To verify our motivation, we also ana-
lyze how the hidden states similarity between the
pseudo hidden states predicted and the original hid-
den states changing along with the forward prop-
agation and prove the refinement of transformer
layers, finally we train multiple transfer projections
on different layers for different transfer steps to
explore how to select the transfer layers. Finally
we also compare the transfer prediction accuracy of
the second transfer step between the setting of first
pseudo hidden states masked or not in the ablation
study
All experiments are conducted on a single
NVIDIA A100-80GB GPU and all implementa-
tions are based on PyTorch using HuggingFace’s
architecture (Wolf et al., 2020; Lhoest et al., 2021)
4.2 Datasets
We use ShareGPT dataset as our training dataset
for all models, and we use the test split of Ex-
treme Sum marization (XSum) (Narayan et al.,
2018), Gsm8k (Cobbe et al., 2021) as our test
dataset. ShareGPT is a multi-round conversations
dataset comprises nearly 70,000 samples, We trainone epoch for all the models. XSum (Narayan
et al., 2018) is a dataset for evaluation of abstract
single-document summary systems, it’s test split
has 11,334 samples. we only sample 1000 sen-
tences followed (Zhang et al., 2023). The Gsm8k
dataset (Cobbe et al., 2021) encompasses a col-
lection of 8,500 linguistically varied, high-quality
math word problems for grade school students, all
of which were meticulously crafted by human au-
thors, we use its whole test split with 1000 samples.
All the two datasets are evaluated under 1-shot set-
ting followed (Zhang et al., 2023).
4.3 Main Results
Table 1 shows that the acceleration ratio of our
method is significantly better than other baselines
in end-to-end time for all the test dataset(more de-
tails in appendix), it has at most 1.28x acceleration
ratio compared with Medusa, which is similar to
our method. Using hidden transfer to predict the
pseudo hidden states in the intermediate layer gain
more benefit in the overall performance than us-
ing Medusa head to predict the token distribution
directly, this improvement is more obvious in 7B
models, and we find that the acceleration ratio on
Gsm8k is higher because the answer in Gsm8k is
more logical and predictable with more mathemati-
cal symbols.
4.4 Analytical Study
In this subsection, we conduct some analytical ex-
periments to further verify the effectiveness and
motivation of our method, the key idea of our

--- PAGE 7 ---
Figure 3: TopK tokens’ prediction accuracy using three prediction methods on LLaMA-2-Chat-13B model including
directly train different lm-heads on some intermediate layers (denoted as Early exit in the figure), Medusa method
and our hidden transfer method (We transfer the pseudo intermediate hidden states of the next 3 tokens on the 25th,
30thand35thlayers respectively), The Nin the figure is the prediction step (N=2 means we predict the first draft
token). It’s clear that our method achieve the best prediction accuracy
method is to predict draft tokens more correctly
in a single forward propagation by predicting the
pseudo hidden states in intermediate layers. So we
first compare our method with Medusa and early
exiting to show that we have better draft tokens pre-
diction accuracy; then we analyze how the hidden
states change during the forward propagation to
verify the refinement we proposed. We also verify
the draft tokens prediction dependency and show
how to select the transfer layers.
Draft tokens prediction accuracy In a single
forward propagation (given the token sequence X1,
...Xnand model need to predict the future Kdraft
tokens eXn+2, ... eXn+k+1), so we first compare
the draft tokens’ prediction accuracy on three dif-
ferent methods: early exiting in the intermediate
layers, using medusa heads and our hidden transfer
method. Early exiting method trains independent
lm-heads in several intermediate transformer lay-
ers to directly predict the draft tokens(for example
train a lm-head and use it to map hj
ntoeXn+2),
We use the LLaMA-2-Chat-13B as the base model
for this experiment, three different methods are
trained on ShareGPT dataset for one epoch and
we set Kas 3. We random sample 100 sequences
from the test split of XSum and Gsm8K dataset re-
spectively, for each sequence, we random split 50
points at its output part, and for each split-point, we
use the token sequence before as the prompt input
and predict Kdraft tokens using different methods
and compare with the tokens generated greedily by
the original model in the following Ksteps.(We
choose five random seeds and average the results
to better eliminate random errors) Figure 3 shows
that our hidden transfer method achieve the best
Figure 4: Hidden states similarity between the virtual
hidden states predicted and the original hidden states.
prediction accuracy among them.
Pseudo hidden states refinement we conduct an-
other experiment to prove that the forward propaga-
tion in transformer layers can refine the predicted
hidden states by given more semantic information
using self-attention mechanism. We compare the
cosine similarity of the pseudo hidden states pre-
dicted with the original hidden states, and trace
how the cosine similarity changes with the forward
process (for example, we denote the prompt se-
quence as X1, ...Xnand we calculate the cosine
similarity between pseudo hidden states eht
n+1and
realht
n+1,ht
n+1is the hidden state of Xn+1on
thetthlayer and Xn+1is the greedy decoding out-
put token given the nprefix context). We random
sample 100 sequences for two datasets and random
split 50 times for each sequence as well. Figure 4
shows the cosine similarity get closer along with
the forward process, which proves that with the
forward process, the hidden states can be refined
by the transformer layers.
How to choose transfer layers In the inference
stage of our method, we need to choose which layer

--- PAGE 8 ---
Figure 5: The first transfer step prediction accuracy on
different layers for Vicuna-7b and LlaMa-2-Chat-7b.
TopK means the topk tokens predicted by the transfer
step include
to transfer and generate the pseudo hidden states,
there’s a trade-off between accuracy and efficient:
if we transfer on the lower layers, the pseudo hid-
den states will pass more subsequent layers and
take more computing resource but gain more se-
mantic information by interacting with the hidden
states of context; if we transfer on higher layers,
the pseudo hidden states will take less computing
resource with less semantic information. So we
train the first and second transfer step on different
layers of a fixed LLM to study the impact of the
transfer layer selection. We choose Vicuna-7B and
LlaMa-2-7b-chat model. For the first transfer step,
we train different transfer structure on the 5th,10th,
15th,20th,25thlayers seperately, and report the
token prediction accuracy in figure 5, we found
that from the lower layer to the middle layer, the
prediction accuracy is basically unchanged, and
from the middle layer to the high layer, the pre-
diction accuracy drops rapidly, which proves that
the middle layer to do tranfer is an optimal choice
for both accuracy and computational efficiency, so
we choose the 15thlayer to conduct the first trans-
fer step. After fixed the first transfer layer, we
also train different transfer structures on the layers
higher than it as our second transfer layer. Fig-
ure 6 shows that the second transfer step have the
same rule, starting from the 15thlayer, the accu-
racy rate remains unchanged within a range, and
then rapidly decline, so we choose 20thlayer to
conduct the second transfer step.
4.5 Ablation study
It’s clear that Medusa (Cai et al., 2023) predict the
draft tokens in parallel which means the genera-
tion between different draft tokens is independent.
Our motivation is that serialized generation of draft
tokens will gain better performance and we use
experiment to prove it. We compare the predic-
tion accuracy of the second transfer step under two
Figure 6: The second transfer step prediction accuracy
on different layers for Vicuna-7b and LlaMa-2-Chat-7b
with the fixed transfer step 15. TopK means the topk
tokens predicted by the transfer step include
Figure 7: The second transfer step prediction accuracy
under masked and no masked setting, we transfer the
hidden states on 15thand20threspectively
setting: Masked andNo masked ,Masked means
the second pseudo hidden state can not see the
first pseudo hidden state in the forward process,
andNo masked is the normal self-attention mech-
anism. Figure 4 shows that in all the models and
datasets, Masked performs worse which prove that
the semantic information of the first draft token
is important to the generation of the second draft
token.
5 Conclusion
In this paper, we introduce a novel parallel decod-
ing approach, named hidden transfer, designed for
accelerating inference in large language models.
By training a linear transformation projection in the
intermediate layers, our model is capable of predict-
ing the pseudo hidden states of multiple subsequent
tokens in a single forward propagation. These pre-
dicted hidden states obtain additional semantic in-
formation through subsequent transformer layers,
resulting in enhanced prediction accuracy. Through
analytical experiments, we have proved that the
hidden states predicted by the intermediary layers
are progressively refined, gaining increased seman-
tic information in the subsequent transformer lay-
ers by interacting with context. Our experiments
demonstrate that our method outperforms existing
approaches in terms of predictive precision within
a single forward iteration and also achieves sub-

--- PAGE 9 ---
stantial gains in generation velocity.
Limitation
In the verification stage of our method, we simply
utilized vanilla tree attention without specific opti-
mization. However, the structural choices of tree
attention significantly impact the generation speed.
Therefore, future work will focus on optimizing
it. Furthermore, we aim to conduct a more exten-
sive set of analytical experiments to elucidate the
underlying mechanisms of hidden transfer better
and design improved training methodologies to en-
hance the quality of draft token generation. Our
approach necessitates the expansion of the input
sequence during both training and inference, which
may lead to an increase in computational resource
requirements. This issue will be addressed in the
future research.
References
Sangmin Bae, Jongwoo Ko, Hwanjun Song, and Se-
Young Yun. 2023. Fast and robust early-exiting
framework for autoregressive language models with
synchronized parallel decoding. arXiv preprint
arXiv:2310.05424 .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng,
and Tri Dao. 2023. Medusa: Simple framework for
accelerating llm generation with multiple decoding
heads. https://github.com/FasterDecoding/
Medusa .
Charlie Chen, Sebastian Borgeaud, Geoffrey Irving,
Jean-Baptiste Lespiau, Laurent Sifre, and John
Jumper. 2023a. Accelerating large language model
decoding with speculative sampling. arXiv preprint
arXiv:2302.01318 .
Tianyi Chen, Tianyu Ding, Badal Yadav, Ilya Zharkov,
and Luming Liang. 2023b. Lorashear: Efficient large
language model structured pruning and knowledge
recovery. arXiv preprint arXiv:2310.18356 .
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Jang Hyun Cho and Bharath Hariharan. 2019. On the
efficacy of knowledge distillation. In Proceedings ofthe IEEE/CVF international conference on computer
vision , pages 4794–4802.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, et al. 2021. Training verifiers to solve math
word problems. arXiv preprint arXiv:2110.14168 .
Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael
Auli. 2019. Depth-adaptive transformer. arXiv
preprint arXiv:1910.10073 .
Elias Frantar and Dan Alistarh. 2023. Sparsegpt: Mas-
sive language models can be accurately pruned in
one-shot. In International Conference on Machine
Learning , pages 10323–10337. PMLR.
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and
Dan Alistarh. 2022. Gptq: Accurate post-training
quantization for generative pre-trained transformers.
arXiv preprint arXiv:2210.17323 .
Song Guo, Jiahang Xu, Li Lyna Zhang, and Mao Yang.
2023. Compresso: Structured pruning with collabora-
tive prompting learns compact large language models.
arXiv preprint arXiv:2310.05015 .
Song Han, Huizi Mao, and William J Dally. 2015. Deep
compression: Compressing deep neural networks
with pruning, trained quantization and huffman cod-
ing.arXiv preprint arXiv:1510.00149 .
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531 .
Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dry-
den, and Alexandra Peste. 2021. Sparsity in deep
learning: Pruning and growth for efficient inference
and training in neural networks. The Journal of Ma-
chine Learning Research , 22(1):10882–11005.
Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh,
Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner,
Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.
2023. Distilling step-by-step! outperforming larger
language models with less training data and smaller
model sizes. arXiv preprint arXiv:2305.02301 .
Benoit Jacob, Skirmantas Kligys, Bo Chen, Meng-
long Zhu, Matthew Tang, Andrew Howard, Hartwig
Adam, and Dmitry Kalenichenko. 2018. Quanti-
zation and training of neural networks for efficient
integer-arithmetic-only inference. In Proceedings of
the IEEE conference on computer vision and pattern
recognition , pages 2704–2713.
Yaniv Leviathan, Matan Kalman, and Yossi Matias.
2023. Fast inference from transformers via spec-
ulative decoding. In International Conference on
Machine Learning , pages 19274–19286. PMLR.
Quentin Lhoest, Albert Villanova del Moral, Yacine
Jernite, Abhishek Thakur, Patrick von Platen, Suraj
Patil, Julien Chaumond, Mariama Drame, Julien Plu,

--- PAGE 10 ---
Lewis Tunstall, et al. 2021. Datasets: A commu-
nity library for natural language processing. arXiv
preprint arXiv:2109.02846 .
Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie
Chang, Pierre Stock, Yashar Mehdad, Yangyang
Shi, Raghuraman Krishnamoorthi, and Vikas Chan-
dra. 2023a. Llm-qat: Data-free quantization aware
training for large language models. arXiv preprint
arXiv:2305.17888 .
Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang
Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang,
Yuandong Tian, Christopher Re, et al. 2023b. Deja
vu: Contextual sparsity for efficient llms at infer-
ence time. In International Conference on Machine
Learning , pages 22137–22176. PMLR.
Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao
Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuom-
ing Chen, Daiyaan Arfeen, Reyna Abhyankar, and
Zhihao Jia. 2023. Specinfer: Accelerating generative
llm serving with speculative inference and token tree
verification. arXiv preprint arXiv:2305.09781 .
Shashi Narayan, Shay B Cohen, and Mirella Lap-
ata. 2018. Don’t give me the details, just the
summary! topic-aware convolutional neural net-
works for extreme summarization. arXiv preprint
arXiv:1808.08745 .
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya
Sutskever, et al. 2018. Improving language under-
standing by generative pre-training.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,
Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.
Code llama: Open foundation models for code. arXiv
preprint arXiv:2308.12950 .
Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019. Distilbert, a distilled version
of bert: smaller, faster, cheaper and lighter. arXiv
preprint arXiv:1910.01108 .
Noam Shazeer. 2019. Fast transformer decoding:
One write-head is all you need. arXiv preprint
arXiv:1911.02150 .
Benjamin Spector and Chris Re. 2023. Accelerating llm
inference with staged speculative decoding. arXiv
preprint arXiv:2308.04623 .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023a. Llama: Open
and efficient foundation language models.Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023b. Llama 2: Open foundation and
fine-tuned chat models.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Hanrui Wang, Zhekai Zhang, and Song Han. 2021. Spat-
ten: Efficient sparse attention architecture with cas-
cade token and head pruning. In 2021 IEEE Interna-
tional Symposium on High-Performance Computer
Architecture (HPCA) , pages 97–110. IEEE.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
et al. 2020. Transformers: State-of-the-art natural
language processing. In Proceedings of the 2020 con-
ference on empirical methods in natural language
processing: system demonstrations , pages 38–45.
Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi
Chen. 2023. Sheared llama: Accelerating language
model pre-training via structured pruning. arXiv
preprint arXiv:2310.06694 .
Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang,
Xiaoxia Wu, Conglong Li, and Yuxiong He. 2022.
Zeroquant: Efficient and affordable post-training
quantization for large-scale transformers. Advances
in Neural Information Processing Systems , 35:27168–
27183.
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,
Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b:
An open bilingual pre-trained model. arXiv preprint
arXiv:2210.02414 .
Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen,
Gang Chen, and Sharad Mehrotra. 2023. Draft

--- PAGE 11 ---
& verify: Lossless large language model accelera-
tion via self-speculative decoding. arXiv preprint
arXiv:2309.08168 .
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.
Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068 .
Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa,
and Zhiru Zhang. 2019. Improving neural network
quantization without retraining using outlier channel
splitting. In International conference on machine
learning , pages 7543–7552. PMLR.
