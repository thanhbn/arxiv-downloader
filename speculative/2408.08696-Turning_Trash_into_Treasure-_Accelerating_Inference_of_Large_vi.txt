# 2408.08696.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/speculative/2408.08696.pdf
# Kích thước tệp: 884780 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Biến Rác Thành Kho Báu: Tăng Tốc Suy Luận của Các Mô Hình Ngôn Ngữ Lớn
với Tái Chế Token

Xianzhen Luo1, Yixuan Wang1, Qingfu Zhu1*, Zhiming Zhang1,
Xuanyu Zhang2,Qing Yang2,Dongliang Xu2
1Đại học Công nghệ Harbin, Harbin, Trung Quốc
2Du Xiaoman (Bắc Kinh) Science Technology Co., Ltd.
{xzluo, wyx, qfzhu, zmzhang}@ir.hit.edu.cn
{zhangxuanyu, yangqing, xudongliang}@duxiaoman.com

Tóm tắt
Số lượng tham số khổng lồ của LLM đã khiến độ trễ suy luận trở thành một nút thắt cơ bản. Giải mã suy đoán đại diện cho một phương pháp không mất mát để tăng tốc suy luận thông qua mô hình đoán-và-xác minh. Một số phương pháp dựa vào các kiến trúc bổ sung để đoán token nháp, cần đào tạo thêm trước khi sử dụng. Ngoài ra, các kỹ thuật không cần đào tạo dựa trên truy xuất xây dựng thư viện từ các kho ngữ liệu có sẵn hoặc bằng cách tạo n-gram. Tuy nhiên, chúng phải đối mặt với những thách thức như yêu cầu lưu trữ lớn, truy xuất tốn thời gian và khả năng thích ứng hạn chế. Quan sát thấy rằng các token ứng viên được tạo ra trong quá trình giải mã có khả năng xuất hiện lại trong các chuỗi tương lai, chúng tôi đề xuất Tái Chế Token. Nó lưu trữ các token ứng viên trong một ma trận kề và sử dụng thuật toán giống tìm kiếm theo chiều rộng (BFS) để xây dựng cây nháp, sau đó được xác thực thông qua attention cây. Các token ứng viên mới từ quá trình giải mã sau đó được sử dụng để cập nhật ma trận. Tái Chế Token yêu cầu <2MB bộ nhớ bổ sung và đạt được tốc độ tăng khoảng 2 lần trên tất cả các kích thước LLM. Nó vượt trội hơn đáng kể so với các phương pháp không cần đào tạo hiện có 30% và thậm chí một phương pháp đào tạo được công nhận rộng rãi 25%.

1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLM) (Brown et al., 2020; Gemini Team et al., 2023; Touvron et al., 2023; Meta, 2024) đã trở thành nền tảng của nhiều ứng dụng như chatbot, trợ lý mã và agent (OpenAI, 2023; Chen et al., 2021; Wang et al., 2024a). Tuy nhiên, do chiến lược giải mã tự hồi quy, LLM chỉ có thể tạo ra một token duy nhất ở mỗi bước giải mã, dẫn đến độ trễ suy luận cao (Brown et al., 2020). Độ trễ chủ yếu đến từ việc truyền hàng tỷ tham số từ bộ nhớ băng thông cao đến bộ nhớ cache của bộ tăng tốc ở mỗi bước giải mã,

*Tác giả liên hệ

Thông tin bị
Bỏ qua

[ range values [ k
for i range in (i in ( zip xs
LLM

Mô hình Nháp/Thư viện Truy xuất

for

Giải mã Suy đoán Điển hình

Tái Chế Token (Của chúng tôi)for i range in (i in ( zip keys
LLMLoại bỏ
Truy xuấtCập nhật

token nháp/ứng viên
token được chấp nhận
token bị từ chối

Token Ứng viên Top-k
Ma trận Kềfor i k
... ... ...
( keys valuesCập nhật
for ... k ( keysfor i j
... ... ...
( a b

Các Thế hệ Tương lai

Hình 1: So sánh giữa giải mã suy đoán điển hình và Tái Chế Token (TR). Các phương pháp điển hình tạo nháp một số token và xác minh chúng song song trong một bước giải mã. Không giống như các phương pháp khác loại bỏ token ứng viên, TR lưu trữ chúng trong ma trận kề. Trong các thế hệ tương lai, token nháp được truy xuất từ ma trận được cập nhật với token ứng viên mới. TR hiệu quả tái chế token trong quá trình giải mà.

chứ không phải tính toán số học (Kim et al., 2024; Shazeer, 2019; Cai et al., 2024).

Nhiều phương pháp (Xu et al., 2024; Frantar và Alistarh, 2023; Dao, 2024; DeepSeek-AI, 2024) tìm cách giảm độ trễ, với giải mã suy đoán như một kỹ thuật không mất mát chính. Phương pháp này sử dụng quy trình đoán và xác minh để có được nhiều token trong một bước giải mã duy nhất (Chen et al., 2023; Leviathan et al., 2023; Miao et al., 2024; Xia et al., 2023). Đầu tiên nó suy đoán một số token nháp tiếp theo và sau đó xác minh chúng bằng LLM gốc. Chi phí thời gian xác minh trên nhiều token có thể so sánh với việc tạo ra một

arXiv:2408.08696v3 [cs.CL] 25 May 2025

--- TRANG 2 ---
token do hiệu suất song song cao của các bộ tăng tốc. Một khi một số token nháp đúng, các bước giải mã được rút ngắn đáng kể mà không hy sinh chất lượng. Để tận dụng đầy đủ tính song song của các bộ tăng tốc, attention cây điều chỉnh nhẹ mặt nạ attention để xác minh nhiều chuỗi token trong một lần forward mô hình (Cai et al., 2024; Miao et al., 2024).

Để tăng tốc hiệu quả, giải mã suy đoán phải đảm bảo dự đoán nháp chính xác trong khi giữ chi phí suy đoán thấp. Các kiến trúc mô hình bổ sung được xây dựng để đoán token nháp, bao gồm các mô hình nháp nhỏ (Leviathan et al., 2023; Chen et al., 2023) và cấu trúc hiệu quả tham số (Cai et al., 2024; Lin et al., 2024). Tuy nhiên, các phương pháp này yêu cầu tài nguyên để đào tạo bổ sung trên mỗi LLM. Phương pháp điển hình để đạt được giải mã suy đoán không cần đào tạo là dựa trên truy xuất. Trong trường hợp này, một thư viện truy xuất được định nghĩa trước để lấy token theo sau hậu tố của nội dung hiện tại như token nháp. Một số phương pháp đã được đề xuất trong danh mục này, mỗi phương pháp có những đánh đổi riêng: (i)REST (He et al., 2023) biến đổi các kho ngữ liệu hiện có thành thư viện truy xuất, nhưng việc lưu trữ lớn, truy xuất tốn thời gian, và thư viện thiếu linh hoạt vì nó tĩnh đối với bất kỳ truy vấn nào. (ii)PLD (Saxena, 2023) chỉ truy xuất nội dung trước đó với chi phí tối thiểu. Tuy nhiên, nó không thể dự đoán token mới hoặc tổ hợp token mới. (iii)Lookhead (Fu et al., 2024) xây dựng và cập nhật thư viện n-gram bằng cách giải mã n lần với LLM. Tuy nhiên, LLM phải tạo n-gram trong khi suy luận, gây ra hiệu quả thấp.

Hơn nữa, tất cả các phương pháp giải mã suy đoán đều không tận dụng đầy đủ token ứng viên, là nhiều token có thể tiếp theo được tạo bởi LLM ở mỗi bước giải mã. Trong giải mã tham lam, chỉ token ứng viên top-1 của token được chấp nhận được chọn làm đầu ra, trong khi các token ứng viên khác, bao gồm tất cả token ứng viên từ token bị từ chối, đều bị loại bỏ, như 'k' và 'keys' trong Hình 1. Tuy nhiên, chúng tôi quan sát thấy rằng khi token đầu vào hiện tại xuất hiện lại trong các thế hệ tương lai, các token tiếp theo có thể là token ứng viên được tạo ra một vài bước trước đó. Dựa trên quan sát này, chúng tôi đề xuất Tái Chế Token (TR), sử dụng token ứng viên làm token nháp. Nó lưu trữ token ứng viên trong ma trận kề. Trước mỗi bước giải mã, phương pháp giống BFS truy xuất cây nháp từ ma trận, sau đó được xác minh bằng attention cây. Sau khi xác minh, token ứng viên mới được tạo ra cập nhật ma trận. (i)Ma trận cung cấp thư viện truy xuất linh hoạt được tùy chỉnh cho mỗi truy vấn và đưa ra chi phí truy xuất thấp do kích thước nhỏ (<2MB). (ii)So với việc chỉ sử dụng nội dung trước đó, token ứng viên tự nhiên bao gồm nhiều token hơn, cung cấp nhiều sự tiếp tục có thể. (iii)Việc xây dựng và cập nhật thư viện (ma trận) của chúng tôi sử dụng token 'rác' mà không yêu cầu bất kỳ thế hệ bổ sung nào.

Chúng tôi tiến hành thí nghiệm toàn diện trên benchmark chung SpecBench (Xia et al., 2024), và tập dữ liệu chuyên biệt về lĩnh vực code, MBPP (Chen et al., 2021) với Vicuna (Zheng et al., 2023) và Code Llama (Roziere et al., 2023). Kết quả cho thấy TR vượt xa các phương pháp không cần đào tạo trước đây, và cải thiện hơn 30% trên tất cả kích thước (7b, 13b, 33b/34b). Tỷ lệ tăng tốc thậm chí vượt qua phương pháp đào tạo được sử dụng rộng rãi–Medusa, chứng minh hiệu quả cao của nó.

Những đóng góp của chúng tôi được tóm tắt dưới đây:
•Một phương pháp giải mã suy đoán cắm-và-chạy, Tái Chế Token được đề xuất. Nó đầu tiên nhận ra giá trị của token 'rác' và chuyển đổi chúng thành token 'kho báu' để tăng tốc.
•TR yêu cầu không gian lưu trữ tối thiểu (<2MB) với chi phí truy xuất thấp và bao phủ nhiều token mới. Cập nhật liên tục cung cấp không gian truy xuất động.
•TR đạt được tốc độ tăng khoảng 2 lần trên tất cả kích thước LLM. Nó đạt được SOTA mới với cải thiện lớn hơn 31% so với các phương pháp không cần đào tạo trước đây và thậm chí vượt qua một phương pháp đào tạo.

2 Nền tảng
Trong phần này, chúng tôi tổng quan về giải mã suy đoán. Đầu tiên chúng tôi định nghĩa chính thức giải mã tự hồi quy (AR), sau đó thảo luận về giải mã suy đoán, tập trung vào hai chiến lược chính: đoán-và-xác minh và attention cây.

2.1 Giải mã Tự Hồi quy
AR là chiến lược giải mã mặc định của LLM. Ở mỗi bước t, LLM tính toán phân phối xác suất của token tiếp theo cho nội dung hiện tại s= (x0, x1,···, xt) trong đó xi∈ V:
pt+1=P(x|s;θ).
Ở đây V là từ vựng và θ biểu thị tham số LLM. Token tiếp theo được chọn từ pt+1 dựa trên phương pháp lấy mẫu. Theo Kou et al.

--- TRANG 3 ---
(2024), chúng tôi tập trung vào giải mã tham lam trong bài báo này, trong đó token tiếp theo là:
xt+1=argmax pt+1.
Token ứng viên là k token hàng đầu với xác suất cao nhất
(x0t+1, x1t+1, . . . , xk−1t+1) =argtop k(pt+1)
trong đó k là số lượng token ứng viên,
argtop k(·) trả về các chỉ số của k giá trị cao nhất trong pt+1 và x0t+1=xt+1.

2.2 Giải mã Suy đoán
Đoán và Xác minh Giải mã suy đoán hiệu quả tận dụng khả năng song song của bộ tăng tốc. Cho s, đầu tiên nó đoán n token nháp tiếp theo (˜xt+1,···,˜xt+n). Tổ hợp (s,˜xt+1,···,˜xt+n) sau đó được gửi đến LLM cho một lần forward, dẫn đến:
pt+1=P(x|s;θ),
˜pt+i=P(x|s,˜xt+1, . . . , ˜xt+i−1;θ), i= 2, . . . , n.
pt+1 giống như giải mã AR nên sự thật cơ bản xt+1 có thể xác định được. Nếu token nháp ˜xt+1 khớp với xt+1, thì ˜pt+2 được giả định giống hệt với pt+2. Do đó, sự thật cơ bản tiếp theo được chọn:
xt+2=argmax ˜pt+2. Quá trình xác minh này tiếp tục cho đến khi token nháp không khớp với sự thật cơ bản, được chỉ ra bởi:
xt+j=argmax ˜pt+j̸= ˜xt+j.
Cuối cùng, j token mới được xác nhận trong một lần forward. Chi phí thời gian của một lần forward với (s,˜xt+1,···,˜xt+n) gần như bằng với s do hiệu suất song song cao của bộ tăng tốc. Hình 1 cho thấy một ví dụ. Token nháp là ['i', 'in', 'range', '('] và token đầu ra là ['i', 'in', 'zip', '(', 'xs'] sau lần forward. Mặc dù 'zip' không khớp với 'range', ba token ['i', 'in', 'zip'] được xác nhận trong một lần forward.

Attention Cây Mặt nạ attention nhân quả truyền thống được thiết kế cho chuỗi tuyến tính, điều này hạn chế giải mã suy đoán chỉ xác minh một chuỗi tại một thời điểm. Tuy nhiên, khi độ dài chuỗi tăng lên trong quá trình tạo token nháp, số lượng tiếp tục có thể tăng lên. Ví dụ, trong cây nháp ở Hình 2, token theo sau 'guest' có thể là 'speaker' hoặc 'speak'. Attention cây sửa đổi mặt nạ attention để xác minh nhiều chuỗi nháp đồng thời. Nó nén nhiều chuỗi thành một chuỗi hợp nhất duy nhất, như ['guest', 'speaker', 'speak'], trong khi bảo toàn cấu trúc cây thông qua mặt nạ attention cây. Mỗi nút con chỉ chú ý đến các nút cha của nó, ngăn token anh em không can thiệp lẫn nhau. Sau khi LLM xử lý chuỗi hợp nhất, tất cả các chuỗi có thể như 'guest speaker' và 'guest speak', cùng với token đầu ra tương ứng của chúng được trích xuất dựa trên cấu trúc cây và xác minh song song. Chuỗi đúng dài nhất được chọn làm đầu ra cuối cùng. Trong trường hợp hiếm hoi, khi token có xác suất giống hệt nhau, attention cây và giải mã AR có thể chọn token khác nhau, nhưng điều này ảnh hưởng đến chất lượng phản hồi tối thiểu. Giải thích chi tiết có trong Phụ lục A.1.

Tóm lại, giải mã suy đoán, thông qua đoán và xác minh và attention cây, cải thiện độ trễ suy luận một cách mạnh mẽ và hiệu quả.

3 Phương pháp
Hình 2 cung cấp tổng quan về Tái Chế Token (TR). Nó tận dụng ma trận kề khởi động nóng để lưu trữ token ứng viên và sử dụng thuật toán giống BFS để xây dựng cây nháp. Nó sử dụng attention cây để xác minh chuỗi nháp và liên tục cập nhật ma trận với token ứng viên mới được tạo ra trong quá trình giải mã.

3.1 Khởi tạo Ma trận Kề
Ma trận kề M là thành phần chính trong TR, được sử dụng để lưu trữ k token ứng viên hàng đầu cho mỗi token trong từ vựng:
M ∈ V|V|×k
trong đó k là siêu tham số do người dùng định nghĩa. Mỗi phần tử M[i, j] chỉ ra rằng token VM[i,j] là token ứng viên thứ j được liên kết với Vi. Việc sử dụng định dạng ma trận, thay vì các cấu trúc khác như trie, cho phép xử lý song song hiệu quả các token ứng viên, điều này quan trọng để giảm thời gian truy xuất và cập nhật.

Ban đầu, tất cả các phần tử được đặt thành không, có nghĩa là một token phải xuất hiện trong token nháp trước khi nó có token ứng viên hợp lệ. Việc khởi tạo này dẫn đến ma trận bắt đầu với khả năng dự đoán hạn chế, có thể gây ra không hiệu quả trong giai đoạn đầu của suy luận. Để giảm thiểu hạn chế này, chúng tôi triển khai chiến lược khởi động nóng. Điều này bao gồm việc tiếp tục sử dụng ma trận hiện có, từ đó tận dụng kiến thức trước đó. Ngay cả

--- TRANG 4 ---
4. Cập nhật  với token ứng viênevent is at anTiền tố: ... volunteering as a guest
guest
guest
speaker
atspeaker speak Spe
speaker at for is
speak ings in ers
speakat ~ ~ ~
for ~ ~ ~
for... ... ... ...Token Hiện tại Token Ứng viênNội dung Hiện tại
1. Truy xuất  dựa trên 'guest'
Mặt nạ Attention Cây
Tiền tố guest speaker speak at forChuỗi Hợp nhấtTiền tố _guest _speaker _at _aa local nearby guest
2. Forward Mô hình
guest speaker event speaking
speaker at is event
speak ers at ER
at a an the
for a an multicol
... ... ... ...Token Hiện tại Token Ứng viên
a local nearby guestToken Ứng viên Top-kspeaker at ers a aLLM5. Chọn  chuỗi đúng dài nhấtguest
speaker
speak
at
forguestspeakeratspeakfor
ings inSpe
is ers
Giai đoạn 3: Xác minh và Cập nhậtGiai đoạn 1 : Khởi tạo Ma trận Kề Giai đoạn 2 : Truy xuất Cây Nháp
speaker
speaker at
speaker for
speakguest
guest
guest
guest
3. Xác minh  tất cả chuỗiings
ings
0. Kế thừa  từ ma trận hiện cóings
rare,
ings , rare fairspeak guest ings

Hình 2: Tổng quan về Tái Chế Token (TR). Ma trận kề, được khởi tạo bởi ma trận hiện có, lưu trữ token ứng viên. TR đầu tiên truy xuất cây nháp từ ma trận sau đó được xác minh thông qua attention cây. Sau khi thêm chuỗi đúng dài nhất vào nội dung, token ứng viên top-k mới cập nhật ma trận.

nếu các truy vấn khác nhau về lĩnh vực, token ứng viên thường bao gồm các biểu thức và mẫu phổ biến xuất hiện thường xuyên trên các truy vấn khác nhau. Do đó, khởi động nóng đảm bảo rằng ma trận có điểm bắt đầu rộng hơn, bao phủ một loạt tiếp tục tiềm năng.

3.2 Truy xuất Cây Nháp
Ma trận kề M lưu trữ token ứng viên, có thể được sử dụng làm token nháp khi token tương ứng của chúng xuất hiện sau đó. Sử dụng trực tiếp ma trận chỉ có thể xác định token ngay tiếp theo, như tìm 'speaker' theo sau 'guest' (xem Hình 2). Ngay cả khi 'speaker' đúng, nó chỉ cải thiện nhẹ so với giải mã AR, chỉ thêm một token bổ sung. Thực tế, ma trận cũng chứa các tiếp tục có thể cho những token ứng viên này, gợi ý các token tiếp theo như 'at' theo sau 'speaker'. Mở rộng chuỗi từng bước cho phép chuỗi nháp dài hơn. Hơn nữa, bằng cách lưu trữ k token ứng viên hàng đầu, nhiều tiếp tục tiềm năng có thể được khám phá song song cho mỗi token, như 'at' và 'for' theo sau 'speaker'. Quy trình BFS này cho phép xây dựng cây nháp chỉ với ma trận kề, có thể được áp dụng trực tiếp cho attention cây.

Không giống như BFS hoàn chỉnh, chúng tôi sử dụng quy tắc phỏng đoán để định nghĩa cấu trúc cây tĩnh và không cân bằng. Cấu trúc cây này và quy trình xây dựng của nó được chi tiết trong Phụ lục A.2. Tĩnh: Số lượng con cho mỗi nút vẫn không đổi trên tất cả các bước giải mã, điều này tạo điều kiện cho tiền xử lý và cho phép các hoạt động song song hiệu quả trong quá trình duyệt lớp. Việc tránh cần phải duyệt từng nút riêng lẻ giảm đáng kể thời gian truy xuất. Không cân bằng: Các nút được đặt sớm hơn trong mỗi lớp có nhiều con hơn và mở rộng sâu hơn. Điều này phân bổ tài nguyên tính toán cho các tiếp tục có khả năng nhất vì token ứng viên được sắp xếp theo xác suất trong ma trận.

Phương pháp giống BFS để truy xuất cây nháp bắt đầu với ma trận M và cấu trúc cây Tree. Gốc là token cuối cùng của nội dung hiện tại, như 'guest' trong Hình 2. Khi gốc tạo thành lớp đầu tiên, tất cả token ứng viên cho 'guest' được trích xuất từ M, dẫn đến ['speaker', 'speak', 'Spe']. Theo Tree, lớp đầu tiên cho phép mỗi token có hai con, Do đó, 'speaker' và 'speak', có xác suất top-2, được thêm vào lớp thứ hai. Quy trình sau đó tiếp tục mở rộng lớp mới. Tất cả token ứng viên của lớp thứ hai được truy xuất song song, dẫn đến ['at', 'for', 'is'] và ['ings', 'in', 'ers']. Tree chỉ định rằng nút đầu tiên ('speaker') có thể có hai

--- TRANG 5 ---
con, trong khi nút tiếp theo ('speak') chỉ có thể có một con. Do đó, token lớp mới là ['at', 'for'], và ['ings']. Quy trình này lặp lại cho đến khi đạt được độ sâu chỉ định. Thuật toán 1 chi tiết được cung cấp trong Phụ lục A.2.

Phương pháp truy xuất này xây dựng cây nháp hiệu quả và hữu hiệu với độ dài và đa dạng mong muốn, sau đó có thể được xác minh bởi attention cây.

3.3 Xác minh và Cập nhật
Việc xác minh cây nháp phù hợp với Phần 2.2. Chuỗi hợp nhất S được xây dựng thông qua duyệt cây nháp theo lớp. Tất cả chuỗi nháp tiềm năng sau đó được xác minh và chuỗi đúng dài nhất được chọn.

Sau xác minh, ma trận kề M được cập nhật song song dựa trên phân phối đầu ra ˜pi+1 của mỗi token nháp xi∈S:
M[˜xi] =argtop k(˜pi+1).

Vì nhiều token đứng trước có thể có cùng token ứng viên, các bản sao có thể xuất hiện trong S, và phân phối đầu ra của chúng có khả năng khác nhau. Khi thực hiện cập nhật song song, các hoạt động CUDA có thể hợp nhất những cập nhật này, dẫn đến sự khác biệt trong kết quả cuối cùng. Ví dụ, nếu xi xuất hiện hai lần và có hai token đầu ra top-2 khác nhau, [y0, y1],[z0, z1], thì M[xi] có thể được cập nhật thành chính xác một trong các kết quả sau: [y0, z1],[y0, y1],[z0, z1] hoặc [z0, y1]. Chúng tôi không giải quyết việc hợp nhất này, vì thêm kiểm soát làm giảm hiệu suất tổng thể, như thảo luận sau trong Phần 5.2.

Quy trình cập nhật ghi đè trực tiếp token ứng viên trước đó và tận dụng những token mới làm token nháp cho các bước giải mã tiếp theo. Điều này cho phép không gian truy xuất thích ứng động với nội dung hiện tại, tập trung vào các tiếp tục có liên quan và có khả năng nhất. Nó cũng loại bỏ sự cần thiết cho các hoạt động bổ sung ngoài giải mã tiêu chuẩn để cập nhật không gian truy xuất.

Tóm lại, TR tận dụng 'rác' có trong giải mã suy đoán bằng cách triển khai quy trình tuần hoàn giữa token ứng viên và nháp. Nó tăng tốc suy luận mà không cần cấu trúc mô hình bổ sung hoặc đào tạo, làm cho nó có khả năng thích ứng cao và tích hợp liền mạch với bất kỳ kiến trúc hoặc kích thước mô hình nào.

4 Thí nghiệm
4.1 Thiết lập Thí nghiệm
Phù hợp với công việc trước đây (Kou et al., 2024), chúng tôi tập trung vào các kịch bản dư thừa tính toán phổ biến, cụ thể là giải mã tham lam với kích thước batch bằng một. Các metric đánh giá sau được sử dụng:
Mean Accepted Token (MAT) (Xia et al., 2024) đại diện cho số lượng token trung bình được xác nhận trong một bước giải mã duy nhất; Tokens per Second (Ts/s) đo số lượng token được xử lý mỗi giây; Tỷ lệ tăng tốc so sánh hiệu suất tương đối với triển khai HuggingFace của giải mã AR. Chúng tôi đặt k= 8 cho M (<2MB lưu trữ tổng cộng) và cấu trúc cây nháp được hiển thị trong Phụ lục A.2. Tất cả thí nghiệm được tiến hành sử dụng Pytorch 2.3 với một GPU A100-80GB và 128 CPU dưới CUDA 12.2.

Tập dữ liệu và LLM Chúng tôi tiến hành thí nghiệm trên SpecBench (Xia et al., 2024) và MBPP (Austin et al., 2021). SpecBench là benchmark toàn diện bao gồm các kịch bản đa dạng bao gồm Hội thoại Đa lượt (MT), Dịch thuật (Trans), Tóm tắt (Sum), Hỏi đáp (QA), Lý luận Toán học (Math), và Tạo sinh Tăng cường Truy xuất (RAG). MBPP là tập dữ liệu được sử dụng rộng rãi trong tạo mã, có nhu cầu ngày càng tăng về tạo sinh hiệu quả. Những tập dữ liệu này cho phép phân tích so sánh với công việc trước đây trên cả lĩnh vực chung và chuyên biệt.

Chúng tôi tuân theo thực hành tiêu chuẩn sử dụng Vicuna (Chiang et al., 2023) cho SpecBench và Code Llama (Roziere et al., 2023) cho MBPP trên ba quy mô khác nhau: 7B, 13B, và 33B1.

Baseline Chúng tôi so sánh TR với ba phương pháp dựa trên truy xuất không cần đào tạo. Lookahead (Lade) xây dựng thư viện truy xuất n-gram thông qua tạo n-gram bổ sung trong quá trình giải mã, tiêu tốn tài nguyên tính toán đáng kể. PLD coi nội dung trước đó như thư viện truy xuất, bị hạn chế và không thể giới thiệu token mới hoặc tổ hợp token mới. REST xây dựng thư viện truy xuất từ các tập dữ liệu đào tạo hiện có, yêu cầu lưu trữ lớn và thời gian truy xuất đáng kể. Bản chất tĩnh của thư viện cũng ngăn nó thích ứng với các truy vấn riêng lẻ. Hơn nữa, chúng tôi cũng bao gồm baseline cần đào tạo để so sánh biên giới. Medusa thêm nhiều đầu LM bổ sung trong lớp cuối để dự đoán token nháp. Chúng tôi

1Mô hình lớn nhất của Code Llama là 34B, để nhất quán và thuận tiện trong so sánh của chúng tôi, chúng tôi gọi nó là 33B.

--- TRANG 6 ---
#Para PhươngphápSpecBench MBPP
MT Trans Sum QA Math RAG MAT Ts/s Tốc độ MAT Ts/s Tốc độ
7BAR 1.00 1.00 1.00 1.00 1.00 1.00 1.00 54.30 1.00 1.00 56.15 1.00
Lade 1.42 1.12 1.21 1.21 1.52 1.13 1.64 69.03 1.27 1.66 79.16 1.41
PLD 1.53 0.98 2.36 1.10 1.50 1.74 1.75 83.30 1.53 1.39 66.65 1.19
REST 1.37 1.05 1.12 1.42 1.06 1.30 1.84 66.29 1.22 2.08 87.08 1.55
Medusa 1.90 1.57 1.48 1.58 1.87 1.45 2.31 89.41 1.65 - - -
TR 2.17 1.90 1.94 1.95 2.40 1.78 2.70 110.06 2.03 2.93 131.20 2.34
13BAR 1.00 1.00 1.00 1.00 1.00 1.00 1.00 39.41 1.00 1.00 41.31 1.00
Lade 1.29 1.06 1.16 1.12 1.48 1.09 1.63 47.50 1.21 1.73 56.87 1.38
PLD 1.45 1.01 2.10 1.02 1.55 1.65 1.67 57.01 1.45 1.48 52.20 1.26
REST 1.51 1.14 1.31 1.50 1.17 1.50 1.82 53.34 1.35 2.05 70.13 1.70
Medusa 1.94 1.66 1.57 1.62 1.98 1.53 2.39 67.92 1.72 - - -
TR 1.98 1.77 1.89 1.75 2.21 1.73 2.72 74.57 1.89 3.08 93.42 2.26
33BAR 1.00 1.00 1.00 1.00 1.00 1.00 1.00 18.44 1.00 1.00 19.44 1.00
Lade 1.32 1.09 1.20 1.17 1.55 1.14 1.61 23.03 1.25 1.70 29.22 1.50
PLD 1.43 1.06 1.94 1.08 1.55 1.41 1.55 25.89 1.40 1.41 25.89 1.33
REST 1.63 1.27 1.42 1.61 1.29 1.57 1.81 26.99 1.46 2.10 36.85 1.90
Medusa 1.98 1.75 1.63 1.68 2.09 1.61 2.32 33.11 1.80 - - -
TR 1.95 1.75 1.92 1.77 2.24 1.78 2.63 35.16 1.91 3.05 45.43 2.34

Bảng 1: Hiệu suất của các phương pháp khác nhau trên SpecBench (Vicuna) và trên MBPP (Code Llama) trên tất cả kích thước tham số. Tốc độ là metric hiển thị cho các danh mục của SpecBench. Kết quả MBPP loại trừ Medusa vì nó thiếu biến thể Code Llama. Medusa liên quan đến đào tạo trong khi những phương pháp khác không cần đào tạo. In đậm đại diện cho hiệu suất cao nhất.

tập trung vào losses Medusa-1 vì Medusa-2 có mất mát. Tất cả baseline sử dụng siêu tham số mặc định của chúng.

4.2 Kết quả Chính
Bảng 1 cho thấy hiệu suất của TR so với các phương pháp khác. Trên SpecBench, nó đạt được tốc độ tăng hơn 2 lần trên mô hình 7B, cao hơn gần 30% so với các phương pháp không cần đào tạo trước đây. Ngay cả so với tuning Medusa, nó cho thấy cải thiện gần 25%. Đối với mô hình 13B và 33B, nó nhất quán cung cấp tốc độ tăng gần 2 lần, duy trì lợi thế tăng tốc 30%. Những kết quả này chứng minh rằng TR là phương pháp không cần đào tạo hiệu quả nhất trên SpecBench, đưa ra tốc độ tăng đáng kể và nhất quán trên tất cả kích thước mô hình.

Đáng chú ý, TR đạt được tốc độ tăng tốt nhất trên hầu hết các nhiệm vụ phụ, ngoại trừ nó hơi thua PLD trên Sum. Điều này có thể do nhiệm vụ này thường liên quan đến nhiều lặp lại nội dung trước đó. Tuy nhiên, khoảng cách hiệu suất giữa TR và PLD thu hẹp khi kích thước mô hình tăng, chỉ đạt 1% khác biệt với mô hình 33B. Điều này do các mô hình lớn hơn có xu hướng tạo token mới thay vì lặp lại nội dung trước đó. Trong các nhiệm vụ khác như MT, Trans, QA, và Math, TR cho thấy cải thiện đáng kể khoảng 40%~70% cho mô hình 7B. Điều này chứng minh tính tổng quát mạnh của phương pháp chúng tôi trên các kịch bản khác nhau.

Mặc dù cải thiện trên RAG ít hơn 3% cho mô hình 7B, nó tăng với kích thước mô hình, vượt quá 10% cho mô hình 33B. Cải thiện này phù hợp với sự ưa thích của các mô hình lớn hơn đối với token mới. So với lĩnh vực chung, tất cả phương pháp đạt được tăng tốc lớn hơn trên lĩnh vực code do dư thừa nội dung cao hơn. TR cung cấp tốc độ tăng khoảng 2.3 lần trên tất cả quy mô mô hình, đạt hiệu suất SOTA.

Hơn nữa, hiệu suất trên Trans cho thấy lợi thế của phương pháp chúng tôi so với PLD và REST. Trong khi PLD cho thấy tốc độ tăng không đáng kể (gần 1x) và REST đạt tốc độ tăng thấp nhất trên các nhiệm vụ, TR nhất quán mang lại tốc độ tăng hơn 1.75x trên tất cả kích thước mô hình. Đáng chú ý, trên mô hình 7B, PLD dẫn đến chậm lại, và REST chỉ đạt 1.05x, trong khi TR đạt 1.9x. Trans yêu cầu tạo token mới liên tục, liên quan đến lặp lại tối thiểu nội dung trước đó. Ngoài ra, nó rất nhạy cảm với ngữ cảnh, làm cho việc tìm kiếm khớp chính xác từ bất kỳ cơ sở dữ liệu có sẵn nào trở nên thách thức. Những điều này đặt ra thách thức cho PLD và REST. Ngược lại, không gian truy xuất thích ứng và đa dạng của TR dẫn đến hiệu suất vượt trội.

Ngoài Tốc độ, TR đạt được MAT cao nhất trên cả hai benchmark. Điều này được cho là do thời gian truy xuất ngắn hơn và tránh các thế hệ bổ sung như Lade. Điều này cho phép cây nháp sâu hơn và rộng hơn, cho phép nhiều token được chấp nhận trong một bước giải mã duy nhất.

Bảng 2 tóm tắt yêu cầu bộ nhớ GPU cho tất cả phương pháp. So với REST và

--- TRANG 7 ---
Phương pháp Bộ nhớ (MB) Tốc độ
Lade 105 1.27
PLD 0 1.53
REST 465 1.22
Medusa >800 1.65
TR 1.95 2.03

Bảng 2: Chi phí bộ nhớ bổ sung cho tất cả phương pháp. Medusa thêm đầu LM bổ sung vào mô hình, vì vậy việc sử dụng bộ nhớ phụ thuộc vào kích thước ẩn và độ chính xác. 800MB dựa trên LLM 7B và độ chính xác fp16.

Lade, TR đạt được tốc độ tăng cao hơn với bộ nhớ ít hơn nhiều. Trong khi PLD không yêu cầu bộ nhớ bổ sung, tốc độ tăng của nó bị hạn chế. Không giống như Medusa, phương pháp của chúng tôi không cần đào tạo, yêu cầu bộ nhớ tối thiểu, và vẫn đạt được hiệu suất vượt trội.

TR chứng minh cải thiện đáng kể trên tất cả kịch bản, nổi bật tính hiệu quả và khả năng ứng dụng rộng của nó. Quan trọng, TR không cần đào tạo và tự tạo nháp, cho phép tốc độ tăng khoảng 2 lần có thể được áp dụng liền mạch như 'bữa trưa miễn phí' cho bất kỳ LLM hiện có nào.

5 Phân tích
5.1 Cấu trúc Cây
Như đã nêu trước đó trong Phần 3.2, cấu trúc cây của chúng tôi tĩnh và không cân bằng. Kích thước cây là yếu tố quan trọng để tăng tốc. Cây lớn hơn cho phép nhiều token được xác nhận trong một bước giải mã nhưng cũng giới thiệu chi phí tính toán nhiều hơn, tăng thời gian cần thiết cho mỗi bước giải mã.

Để điều tra tác động của kích thước cây, cụ thể là độ sâu và chiều rộng của nó, thí nghiệm được tiến hành trên MT-Bench sử dụng Vicuna-7B.

Chiều rộng Tăng chiều rộng của cây cho phép bao phủ nhiều khả năng hơn. Trong Hình 3(a), chiều rộng được mở rộng bằng cách thêm nút trong khi giữ độ sâu cố định ở sáu lớp. Điều này dẫn đến cải thiện nhất quán trong MAT. Tuy nhiên, khi chiều rộng vượt quá 80, Tokens/s bắt đầu giảm. Chi phí tính toán bổ sung cuối cùng vượt quá lợi ích của MAT cao hơn.

Độ sâu Tăng độ sâu của cây cho phép chấp nhận chuỗi dài hơn trong quá trình giải mã. Trong Hình 3(b), với số lượng nút cố định ở 80, độ sâu được tăng dần. MAT ban đầu tăng nhanh nhưng cuối cùng cho thấy cải thiện tối thiểu, trong khi Tokens/s dao động đáng chú ý. Bởi vì ma trận lưu trữ token ứng viên chỉ cho các bước kề nhau, chuỗi dài hơn làm yếu các kết nối giữa token xa. Hạn chế này làm giảm hiệu quả của việc tăng độ sâu, khiến Tokens/s dao động.

5.2 Nghiên cứu Loại bỏ
Khởi động Nóng Trong TR, ma trận kề kế thừa từ ma trận trước đó. Trong Bảng 3, chúng tôi khám phá tác động của các chiến lược khởi tạo khác nhau. Random có nghĩa là chọn ngẫu nhiên token từ từ vựng, trong khi Zero đặt tất cả phần tử ma trận về không. Fixed chọn 100 truy vấn từ AlpacaEval (Li et al., 2023) (không liên quan đến tập test), thực thi chúng, và lưu trữ ma trận kết quả. Ma trận này sau đó được sử dụng để khởi tạo mỗi truy vấn trong

Tokens/s Tốc độ
AR 54.98 1.00
Random 95.07 1.73
Zero 102.68 1.87
Fixed 117.43 2.12
Shuffle 118.78 2.16
TR 119.56 2.17

Bảng 3: Tác động của các chiến lược khởi tạo khác nhau của ma trận kề. Random nghĩa là chọn ngẫu nhiên từ từ vựng, Zero nghĩa là tất cả đặt về không, Fixed nghĩa là kế thừa từ ma trận cố định và Shuffle nghĩa là trộn tập test.

tập test. Shuffle đề cập đến việc trộn tập test. So với Zero, tiếng ồn không liên quan được giới thiệu bởi Random dẫn đến giảm mạnh hiệu suất. Fixed, Shuffle và TR cho thấy cải thiện đáng kể so với Zero, gợi ý rằng ma trận trước đó có thể nắm bắt các mẫu phổ biến hỗ trợ hiệu quả các truy vấn tiếp theo. Sự khác biệt tương đối nhỏ giữa chúng chỉ ra rằng những mẫu này có thể tổng quát hóa và không gắn liền với nhiệm vụ hoặc nội dung cụ thể.

Chiến lược Cập nhật Phần 3.1 thảo luận về token trùng lặp trong chuỗi hợp nhất trong quá trình cập nhật ma trận. Chúng tôi so sánh ba chiến lược cập nhật: sử dụng token ứng viên từ lần xuất hiện đầu tiên, từ lần xuất hiện cuối cùng, và phương pháp hiện tại (hợp nhất qua các hoạt động CUDA song song). Giải thích chi tiết về ba chiến lược trong Phụ lục A.5

Hình 3(c) chỉ ra rằng sử dụng lần xuất hiện cuối cùng mang lại MAT cao nhất, có thể được hưởng lợi từ thông tin ngữ cảnh nhiều hơn. Tuy nhiên, sự khác biệt giữa các chiến lược khác nhau trong MAT là tối thiểu. Về Tokens/s, phương pháp hiện tại vượt trội đáng kể so với hai phương pháp kia, vì nó tránh xử lý bổ sung cần thiết để quản lý vị trí token, từ đó giảm độ trễ. Giải mã suy đoán rất nhạy cảm với độ trễ, bất kỳ hoạt động bổ sung nào phải cung cấp lợi ích đáng kể để vượt qua chi phí thời gian của nó.

Tác động của Token Bị từ chối Trong quá trình cập nhật, chúng tôi làm mới token ứng viên cho tất cả token nháp, bao gồm cả token được chấp nhận và bị từ chối. Để minh họa thêm tác động đáng kể của token rác, chúng tôi so sánh hai cài đặt: chỉ cập nhật ứng viên của token được chấp nhận so với tất cả token nháp.

Như được hiển thị trong Bảng 4, bao gồm ứng viên của token bị từ chối cải thiện đáng kể MAT. Điều này chỉ ra rằng token bị từ chối cũng mang thông tin có giá trị cần thiết cho giải mã tiếp theo. Chúng tôi bao gồm nghiên cứu trường hợp trong Phụ lục A.4.

Lấy mẫu Nhiệt độ Để tăng cường đa dạng, nhiệt độ lớn hơn 0 thường được sử dụng trong quá trình tạo LLM. Chúng tôi tiến hành thí nghiệm trên SpecBench ở các cài đặt nhiệt độ khác nhau để điều tra tác động của sự ngẫu nhiên này đối với tăng tốc.

SpecBench MBPP
Chỉ Được chấp nhận 1.63 1.99
Tất cả Nháp 2.69 2.93

Bảng 4: Mean Accepted Token (MAT) cho việc cập nhật token ứng viên từ chỉ token được chấp nhận hoặc tất cả token nháp.

Hình 4 cho thấy rằng ở nhiệt độ 0.3, hiệu suất vẫn không bị ảnh hưởng khi so sánh với giải mã tham lam. Tuy nhiên, khi nhiệt độ tăng lên 0.5 và 1, một sự suy giảm hiệu suất nhẹ được quan sát. Điều này có thể do chúng tôi chỉ lưu trữ k token ứng viên hàng đầu. Ở nhiệt độ lấy mẫu cao hơn, xác suất chọn token ngoài top-k tăng lên. Mặc dù vậy, tỷ lệ tăng tốc của TR vẫn nhất quán khoảng 2.0. Hiệu suất tăng tốc duy trì dưới các cài đặt lấy mẫu khác nhau chứng minh tính mạnh mẽ và khả năng ứng dụng rộng của TR.

5.3 So sánh với Eagle
Bảng 1 chứng minh rằng TR vượt trội đáng kể so với Medusa cần đào tạo. Chúng tôi tò mò muốn so sánh TR với các phương pháp giải mã suy đoán tiên tiến nhất - Eagle1 (Li et al., 2024b) và Eagle2 (Li et al., 2024a). Eagle1 là phương pháp phụ thuộc đào tạo thu thập dữ liệu đào tạo và đào tạo mô hình nháp riêng biệt sử dụng trạng thái ẩn và embedding token từ LLM lớn. Eagle2 cải thiện Eagle1 bằng cách kết hợp cây động vào quy trình xây dựng cây nháp. Trong Bảng 5, TR được so sánh với Eagle1/2 trên SpecBench. Thật ngạc nhiên rằng, mặc dù hoàn toàn không cần đào tạo và tự tạo nháp, TR vượt trội Eagle1. Hơn nữa, TR đạt được 91.23% hiệu suất tăng tốc của Eagle2 trong khi chỉ yêu cầu 0.39%

Phương pháp Bộ nhớ (MB) Tokens/s Tốc độ
Eagle1 500 106.94 2.08
Eagle2 500 116.95 2.28
TR 1.95 107.52 2.09

Bảng 5: So sánh Eagle1/2 với TR về chi phí bộ nhớ và tỷ lệ tăng tốc.

--- TRANG 8 ---
của bộ nhớ được sử dụng bởi Eagle2. Đáng chú ý rằng TR vẫn sử dụng cây tĩnh, thay vì cây động được sử dụng trong Eagle2. Điều này nổi bật hiệu quả và hiệu suất đáng chú ý của TR và cho thấy tiềm năng cải thiện thêm khi kết hợp TR với cây động.

5.4 Phân bổ Thời gian
Giải mã suy đoán hiệu quả yêu cầu không chỉ tỷ lệ trúng cao mà còn thời gian tối thiểu cho các hoạt động bổ sung. Mỗi bước giải mã có thể được chia thành một số thành phần: tiền xử lý, truy xuất token nháp, forward mô hình, xác minh chuỗi nháp, và cập nhật ma trận, token đầu vào, và bộ nhớ cache key-value. Hình 5 cho thấy phần lớn thời gian được tiêu tốn bởi forward mô hình. Thứ hai nhiều nhất là xác minh liên quan đến việc trích xuất và xác thực tất cả đường dẫn khả thi. Ngược lại, TR chỉ giới thiệu độ trễ không đáng kể trong các bước tiền xử lý, truy xuất, và cập nhật chuyên dụng của nó. Điều này nổi bật hiệu quả của thiết kế TR.

6 Công việc Liên quan
Suy luận hiệu quả là quan trọng cho các ứng dụng thời gian thực và kịch bản tài nguyên thấp. Nhiều chiến lược đã được phát triển để giảm độ trễ (Zhou et al., 2024b). Trong số này, giải mã suy đoán (Chen et al., 2023; Leviathan et al., 2023; Miao et al., 2024; Xia et al., 2023) là kỹ thuật không mất mát dự đoán nhiều tiếp tục có thể đồng thời. Nó giảm số lượng bước giải mã cần thiết mà không ảnh hưởng đến độ chính xác.

Một số phương pháp giải mã suy đoán dựa vào các mô hình nháp bổ sung để đoán token nháp. Những phương pháp này thường liên quan đến việc sử dụng các mô hình nhỏ hơn từ cùng series (Zhao et al., 2024; Spector và Re; Sun et al., 2023; Liu et al., 2024b; Yuan et al., 2024; Gong et al., 2024) hoặc đào tạo mô hình mới với từ vựng chung (Leviathan et al., 2023; Chen et al., 2023; Zhou et al., 2024a; Li et al., 2024b). Đáng chú ý rằng Zhao et al. (2024) cũng sử dụng token bị từ chối nhưng không bao gồm token ứng viên. Ngoài ra, Kou et al. (2024); Wang et al. (2024b) đề xuất đào tạo LLM gốc để cho phép giải mã không tích cực. Mặc dù hiệu quả, những phương pháp này yêu cầu quản lý hoặc đào tạo nhiều mô hình, có thể không tầm thường và tốn tài nguyên. Các phương pháp khác tập trung vào cấu trúc hiệu quả tham số. Những phương pháp này giảm thiểu nhu cầu đào tạo lại hoàn toàn nhưng vẫn yêu cầu đào tạo và thích ứng cụ thể mô hình, hạn chế khả năng mở rộng và khả năng ứng dụng chung của chúng (Lin et al., 2024; Liu et al., 2024a).

Các phương pháp không cần đào tạo xây dựng thư viện truy xuất để lấy token nháp (Yang et al., 2023). Lookahead (Fu et al., 2024) tạo n-gram thông qua nhiều giải mã, xây dựng thư viện truy xuất có thể trúng nhiều token trong một bước. Tuy nhiên, nó yêu cầu LLM tạo n-gram trong khi phản hồi truy vấn, làm giảm hiệu quả. PLD (Saxena, 2023) chỉ truy xuất từ nội dung trước đó, dẫn đến chi phí tối thiểu và tốc độ tăng đáng kể trong các nhiệm vụ dư thừa cao như tóm tắt. Tuy nhiên, nó cung cấp ít tăng tốc cho các nhiệm vụ yêu cầu tạo nội dung mới, như dịch thuật. REST (He et al., 2023) xây dựng thư viện truy xuất sử dụng kho ngữ liệu hiện có và hoạt động tốt trong các kịch bản phổ biến. Tuy nhiên, phương pháp này yêu cầu lưu trữ lớn, truy xuất tốn thời gian, và không thể thích ứng với mỗi truy vấn.

TR là phương pháp không cần đào tạo, dựa trên truy xuất. Nó không yêu cầu thế hệ bổ sung, bao phủ phạm vi rộng hơn các tiếp tục có thể, và đòi hỏi lưu trữ tối thiểu với chi phí truy xuất thấp. Quy trình cập nhật đảm bảo không gian truy xuất có thể thích ứng.

7 Kết luận
Trong công việc này, chúng tôi giới thiệu Tái Chế Token, phương pháp giải mã suy đoán để tăng tốc suy luận của LLM. Nó sử dụng ma trận kề để lưu trữ token ứng viên và truy xuất cây nháp, sau đó được xác minh với attention cây. Ma trận được cập nhật với token ứng viên mới được tạo ra trong quá trình giải mã. Tái Chế Token có thể được tích hợp liền mạch với LLM và nhiệm vụ hiện có. Như một phương pháp không cần đào tạo, nó đạt được tốc độ tăng gần 2 lần với <2MB lưu trữ, cải thiện hơn 31% so với các phương pháp không cần đào tạo trước đây.

--- TRANG 9 ---
Hạn chế
Nghiên cứu của chúng tôi toàn diện, nhưng có những hạn chế nhất định mà chúng tôi dự định giải quyết trong nghiên cứu tương lai. Trong việc xây dựng cây nháp, chúng tôi sử dụng cấu trúc cây tĩnh. Tuy nhiên, cây động có thể được sử dụng thay thế. Mặc dù cây động giới thiệu độ phức tạp bổ sung, chúng cho phép thích ứng tốt hơn với mỗi bước giải mã, có thể cải thiện hiệu suất bằng cách tùy chỉnh cấu trúc cây theo yêu cầu cụ thể của mỗi truy vấn.

Cân nhắc Đạo đức
Dữ liệu cho các phương pháp đề xuất được rút ra hoàn toàn từ tài nguyên dự án có thể truy cập công khai trên các trang web có uy tín, đảm bảo rằng không có thông tin nhạy cảm nào được bao gồm. Hơn nữa, tất cả tập dữ liệu và mô hình baseline được sử dụng trong thí nghiệm của chúng tôi cũng có sẵn cho công chúng. Chúng tôi đã cẩn thận thừa nhận các tác giả gốc bằng cách trích dẫn đúng công việc của họ.

Lời cảm ơn
Chúng tôi biết ơn sự hỗ trợ của Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (NSFC) qua grant 62236004, 62206078 và 62476073.

Tài liệu tham khảo
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. 2020. Language models are few-shot learners. Trong Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.

Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, và Tri Dao. 2024. Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads. Preprint, arXiv:2401.10774.

Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, và John Jumper. 2023. Accelerating Large Language Model Decoding with Speculative Sampling. Preprint, arXiv:2302.01318.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, và Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. Preprint, arXiv:2107.03374.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, và Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.

Tri Dao. 2024. FlashAttention-2: Faster attention with better parallelism and work partitioning. Trong International Conference on Learning Representations (ICLR).

DeepSeek-AI. 2024. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. Preprint, arXiv:2405.04434.

Elias Frantar và Dan Alistarh. 2023. SparseGPT: Massive language models can be accurately pruned in one-shot. Trong Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 10323–10337. PMLR.

Yichao Fu, Peter Bailis, Ion Stoica, và Hao Zhang. 2024. Break the Sequential Dependency of LLM Inference Using Lookahead Decoding. Preprint, arXiv:2402.02057.

Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.

Zhuocheng Gong, Jiahao Liu, Ziyue Wang, Pengfei Wu, Jingang Wang, Xunliang Cai, Dongyan Zhao, và Rui Yan. 2024. Graph-structured speculative decoding. Trong Findings of the Association for Computational Linguistics ACL 2024, pages 11404–11415.

--- TRANG 10 ---
Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee, và Di He. 2023. Rest: Retrieval-based speculative decoding. arXiv preprint arXiv:2311.08252.

Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W. Mahoney, và Kurt Keutzer. 2024. SqueezeLLM: Dense-and-Sparse Quantization. Preprint, arXiv:2306.07629.

Siqi Kou, Lanxiang Hu, Zhezhi He, Zhijie Deng, và Hao Zhang. 2024. CLLMs: Consistency Large Language Models. Preprint, arXiv:2403.00835.

Yaniv Leviathan, Matan Kalman, và Yossi Matias. 2023. Fast Inference from Transformers via Speculative Decoding. Trong Proceedings of the 40th International Conference on Machine Learning, pages 19274–19286. PMLR.

Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto. 2023. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval.

Yuhui Li, Fangyun Wei, Chao Zhang, và Hongyang Zhang. 2024a. Eagle-2: Faster inference of language models with dynamic draft trees. Trong Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 7421–7432.

Yuhui Li, Fangyun Wei, Chao Zhang, và Hongyang Zhang. 2024b. Eagle: Speculative sampling requires rethinking feature uncertainty. Trong Forty-first International Conference on Machine Learning.

Feng Lin, Hanling Yi, Hongbin Li, Yifan Yang, Xiaotian Yu, Guangming Lu, và Rong Xiao. 2024. BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models. Preprint, arXiv:2401.12522.

Fangcheng Liu, Yehui Tang, Zhenhua Liu, Yunsheng Ni, Kai Han, và Yunhe Wang. 2024a. Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting. Preprint, arXiv:2404.18911.

Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Alvin Cheung, Zhijie Deng, Ion Stoica, và Hao Zhang. 2024b. Online speculative decoding. Trong Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 31131–31146. PMLR.

Meta. 2024. Introducing meta llama 3: The most capable openly available llm to date.

Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, và Zhihao Jia. 2024. SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification. Trong Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, volume 3 of ASPLOS '24, pages 932–949. Association for Computing Machinery.

OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774.

Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950.

Apoorv Saxena. 2023. Prompt lookup decoding.

Noam Shazeer. 2019. Fast Transformer Decoding: One Write-Head is All You Need. Preprint, arXiv:1911.02150.

Benjamin Frederick Spector và Christopher Re. Accelerating llm inference with staged speculative decoding. Trong Workshop on Efficient Systems for Foundation Models@ ICML2023.

Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, và Felix Yu. 2023. Spectr: Fast speculative decoding via optimal transport. Trong Thirty-seventh Conference on Neural Information Processing Systems.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, và Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288.

Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, và Jirong Wen. 2024a. A survey on large language model based autonomous agents. Frontiers of Computer Science, 18:186345.

Yixuan Wang, Xianzhen Luo, Fuxuan Wei, Yijun Liu, Qingfu Zhu, Xuanyu Zhang, Qing Yang, Dongliang Xu, và Wanxiang Che. 2024b. Make some noise: Unlocking language model parallel inference capability through noisy training. arXiv preprint arXiv:2406.17404.

--- TRANG 11 ---
Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu Wei, và Zhifang Sui. 2023. Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation. Trong Findings of the Association for Computational Linguistics: EMNLP 2023, pages 3909–3925, Singapore. Association for Computational Linguistics.

Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, và Zhifang Sui. 2024. Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding. Preprint, arXiv:2401.07851.

Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, và Mike Lewis. 2024. Efficient streaming language models with attention sinks. Trong The Twelfth International Conference on Learning Representations.

Yuzhuang Xu, Xu Han, Zonghan Yang, Shuo Wang, Qingfu Zhu, Zhiyuan Liu, Weidong Liu, và Wanxiang Che. 2024. OneBit: Towards Extremely Low-bit Large Language Models. Preprint, arXiv:2402.11295.

Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, và Furu Wei. 2023. Inference with reference: Lossless acceleration of large language models. Preprint, arXiv:2304.04487.

Hongyi Yuan, Keming Lu, Fei Huang, Zheng Yuan, và Chang Zhou. 2024. Speculative contrastive decoding. Trong Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Bangkok, Thailand. Association for Computational Linguistics.

Weilin Zhao, Yuxiang Huang, Xu Han, Chaojun Xiao, Zhiyuan Liu, và Maosong Sun. 2024. Ouroboros: Speculative decoding with large model enhanced drafting. arXiv preprint arXiv:2402.13720.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, và Ion Stoica. 2023. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. Preprint, arXiv:2306.05685.

Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-François Kagy, và Rishabh Agarwal. 2024a. Distillspec: Improving speculative decoding via knowledge distillation. Trong The Twelfth International Conference on Learning Representations.

Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, et al. 2024b. A survey on efficient inference for large language models. arXiv preprint arXiv:2404.14294.

--- TRANG 12 ---
A Phụ lục
A.1 Token Xác suất Giống nhau

Phương pháp MT-Bench GSM8K
Giải mã AR 6.17 35.2
Attention Cây 6.23 35.2

Bảng 6: So sánh Chất lượng/Độ chính xác của Giải mã AR và Attention Cây trên MT-Bench và GSM8K. Kết quả MT-Bench được lấy từ Cai et al. (2024). Nó cho thấy rằng Attention Cây có tác động tối thiểu đến cả độ chính xác câu trả lời và chất lượng.

Biểu diễn số thực trong máy tính có lỗi độ chính xác, thường được gọi là 'lỗi làm tròn số thực'. Cụ thể, độ chính xác của số thực được xác định bởi số bit trong mantissa. Trong tiêu chuẩn IEEE 754, kiểu float32 có mantissa 23-bit, có nghĩa là sự khác biệt nhỏ nhất có thể biểu diễn là 2−23, khoảng 1.19×10−7. Kiểu float16, với mantissa 10-bit, có thể biểu diễn sự khác biệt nhỏ như 2−10, hoặc khoảng 9.77×10−4. Nếu sự khác biệt giữa hai xác suất token nhỏ hơn giới hạn độ chính xác của biểu diễn số thực, hai xác suất này sẽ được làm tròn về cùng giá trị, và những token này sẽ được coi là có xác suất giống hệt nhau trong quá trình lấy mẫu.

Giải mã AR sử dụng 'torch.argmax' để trả về token có xác suất cao nhất. Khi xác suất giống nhau, 'torch.argmax' mặc định trả về token có chỉ số nhỏ nhất. Trong Attention Cây, số lượng token mặt nạ được tăng so với Giải mã AR, và điểm attention của token mặt nạ sau hoạt động softmax không hoàn toàn bằng không, mà thay vào đó là giá trị rất nhỏ gần không. Những giá trị không bằng không rất nhỏ này làm nhiễu biểu diễn ẩn, khiến token ban đầu có xác suất giống hệt nhau bây giờ khác nhau một chút, dẫn đến kết quả argmax khác so với Giải mã AR.

Tuy nhiên, như được hiển thị trong Bảng 6, do sự xuất hiện cực kỳ hiếm của vấn đề này và xác suất bị ảnh hưởng rất gần với nhau, tác động đến độ chính xác thí nghiệm và hiệu suất mô hình là không đáng kể.

A.2 Thuật toán và Cấu trúc Cây Nháp

Sử dụng attention cây (Miao et al., 2024) để mở rộng đường dẫn trong giai đoạn xác minh đã trở thành chiến lược được áp dụng rộng rãi cho giải mã suy đoán

Thuật toán 1 BFS Dựa trên Cây Tĩnh
Require: Ma trận kề M, Cấu trúc cây tĩnh Tree, token prompt cuối cùng xt
Ensure: Chuỗi Hợp nhất S
1:Khởi tạo S← ∅
2:Khởi tạo root←xt
3:Khởi tạo lớp hiện tại L←(root)
4:Khởi tạo độ sâu hiện tại d←0
5:while d < Tree.depth do
6: Khởi tạo lớp tiếp theo Lnext← ∅
7: Lấy tất cả token ứng viên của L từ M song song
8:xs=M[L]
9: Trích xuất token lớp tiếp theo từ xs với Tree
10: Lnext=xs[Tree[d].index]
11: Nối S và L
12: S←(S;L)
13: L←Lnext
14:end while
15:return S

các phương pháp.

Trong Tái Chế Token, chúng tôi cũng sử dụng cây token được xây dựng phỏng đoán để thực hiện xác minh. Như được hiển thị trong Hình 6, chúng tôi xây dựng cây tĩnh và không cân bằng lấy cảm hứng từ Cai et al. (2024). Số k trên một nút chỉ ra rằng nó là token ứng viên thứ k cho nút cha của nó. Quy trình xây dựng dưới đây. Chúng tôi bắt đầu với cây cân bằng hoàn toàn 10-nhánh và sử dụng tập xác thực độc lập để xác định k nút hàng đầu thường xuyên nhất mang lại token đúng. Những k nút hàng đầu này và con của chúng được giữ lại để tạo thành cây mới, và quy trình được lặp lại để xác định tập k nút hàng đầu tiếp theo. Quy trình lặp này tiếp tục cho đến khi hiệu suất không còn cho thấy cải thiện đáng kể.

Cây cuối cùng được xác định, và k được đặt để xem xét số lượng tối đa con trên tất cả nút và yêu cầu bộ nhớ. Mặc dù thực nghiệm, phương pháp lặp này đã được chứng minh là hiệu quả. Chi tiết thêm về điều chỉnh n được cung cấp trong Phần 5.1. Tổng thể, cây chúng tôi xây dựng chứa 81 nút (bao gồm nút gốc) trong 6 lớp. Điều này có nghĩa là mỗi lần forward yêu cầu đầu vào nháp bổ sung của 79 token với độ dài chấp nhận tối đa là 6.

Dựa trên cấu trúc cây được mô tả ở trên, chúng tôi xây dựng cây nháp cho nội dung hiện tại bằng thuật toán giống BFS trong giai đoạn suy luận. Như được mô tả trong Thuật toán 1, chúng tôi điền các nút con của mỗi lớp lần lượt theo ma trận. Cuối cùng, chuỗi hợp nhất S được trả về và gửi đến attention cây với Tree.

A.3 Phân tích Cơ chế Tái sử dụng
Đầu ra của LLM phụ thuộc ngữ cảnh, nó khiến chúng tôi tò mò tại sao việc tái sử dụng token ứng viên từ các thế hệ trước đó lại hoạt động. Xiao et al. (2024) phân tích phân phối logit attention trong Transformer và thấy rằng hai lớp đầu tiên tập trung nhiều hơn vào các mẫu 'cục bộ', với token gần đây nhận được nhiều attention hơn. Trong các lớp sau, mô hình chuyển trọng tâm sang token ở đầu chuỗi. Để tăng tốc, token ứng viên cần thỏa mãn cả ngữ nghĩa cục bộ và phụ thuộc tầm xa. Như được hiển thị trong Hình 1, token nháp được chia thành token được chấp nhận và bị từ chối. Token được chấp nhận là những token xuất hiện trong phản hồi truy vấn trước đó. Trong Tái Chế Token, token ứng viên cho tất cả token nháp được lưu trữ, không chỉ những token được chấp nhận. Mỗi bước giải mã liên quan đến 79 token nháp, có nghĩa là 79 token nhận/cập nhật token ứng viên của chúng ở mỗi bước. Số lượng token bị từ chối nhiều hơn token được chấp nhận, và token ứng viên của chúng cũng được lưu trữ trong ma trận. Nói cách khác, các thế hệ trước đó thực sự cung cấp số lượng lớn các mẫu phổ biến được lưu trữ trong ma trận, và những mẫu này thường đáp ứng nhu cầu ngữ nghĩa cục bộ.

Từ hai góc độ, việc tái sử dụng những mẫu phổ biến này được biện minh:
Đối với các kịch bản như chuyển đổi câu, kết hợp động từ, dấu câu, hoặc từ được chia thành nhiều token, thường không cần phụ thuộc tầm xa. Các mẫu phổ biến được lưu trữ trong ma trận có thể tăng tốc đáng kể quá trình giải mã.

Token bị từ chối có thể không thỏa mãn phụ thuộc tầm xa của trước đó, nhưng điều này không có nghĩa là chúng không đáp ứng phụ thuộc tầm xa của hiện tại. Những token này có thể được chấp nhận trong tương lai. Như được hiển thị trong Bảng 4, lợi ích hiệu suất từ token ứng viên của token bị từ chối lớn hơn đáng kể so với lợi ích từ việc chỉ sử dụng token được chấp nhận trong thí nghiệm SpecBench.

Chúng tôi cũng bao gồm nghiên cứu trường hợp trong A.4.

A.4 Nghiên cứu Trường hợp
Dưới đây, chúng tôi trình bày ví dụ thực từ MT-Bench, minh họa cách token được chấp nhận và token bị từ chối đóng góp vào tăng tốc của Tái Chế Token.

Vòng Đầu tiên:
• Token cuối tiền tố: ['guest']
• Chuỗi hợp nhất với token nháp: ['guest', 'speaker', '</s>', 'speak', '<0x0A>', 'Spe', 'lect', 'speaking', 'spe', 'at', 'for', '</s>', 'is', 'could', 'can', 'would', '<0x0A>', '<s>', 'sime', 'multicol', 'bolds', '</s>', 'ings', 'in', '<0x0A>', 'a', 'aking', 'ures', 'engag', 'aking', 'a', 'an', 'the', '[', 'our', 'local', '</s>', 'up', 'a', 'an', '</s>', '<s>', 'sime', 'a', 'be', 'help', 'like', '<0x0A>', 'The', 'Home', 'guest', '<s>', '<0x0A>', 'opportunity', '</s>', 'local', 'nearby', 'guest', 'up', 'public', 'guest', '</s>', 'guest', 'public', 'local', 'local', 'The', 'first', 'The', '<0x0A>', 'event', 'community', 'Toast', 'Buddh', 'speaker', 'speaker', 'event', 'time', '.', ',', 'ism']
• Chuỗi được chấp nhận: ['guest', 'speaker', 'at', 'a', 'local', 'event', 'could']

Quan sát chính:
• Token bị từ chối vẫn nhận token ứng viên: Ví dụ, ['be'] không được chấp nhận trực tiếp, nhưng token ứng viên của nó đã được lưu trữ.
• Token ứng viên của token được chấp nhận được lưu trữ: ['local'] cho ['a'] được chọn trong vòng này, nhưng token ứng viên khác cũng được giữ lại.

Tại thời điểm này, ma trận kề lưu trữ:
• 'be': ['able', 'onto', 'mistaken', 'wrong', 'the', 'interested', 'persu', 'a']
• 'a': ['local', 'time', 'personal', 'professional', 'few', 'unique', 'great', 'low']

Vòng Thứ hai:
• Token cuối tiền tố hiện tại: ['could']
• Chuỗi hợp nhất với token nháp: ['could', 'be', 'provide', '</s>', 'help', 'offer', 'present', 'not', 'actually', 'a', 'an', 'the', '</s>', 'just', 'one', 'benef', 'both', ',', "'", 'you', 'for', '<s>', 'sime', 'multicol', 'you', 'overcome', 'some', '</s>', 'public', '<unk>', 'great', 'fant', 'wonderful', 'valuable', 'unique', 'perfect', '</s>', 'ter', 'up', 'event', 'local', 'up', 'local', '<s>', 'not', 'of', 'ited', 'models', 'such', 'like', 'my', 'The', 'as', 'comp', '<s>', 'opportunity', 'guest', '</s>', 'speaker', 'way', 'astic', 'asy', 'bl',

--- TRANG 13 ---
'ins', 'guest', 'coming', 'coming', 'as', 'first', 'a', 'at', 'is', 'would', 'event', '<s>', 'natural', 'public', 'a', 'a', 'an', 'could']
• Chuỗi được chấp nhận: ['could', 'be', 'a', 'great']

Tại thời điểm này:
• Token ứng viên từ token bị từ chối được khớp: 'be' dự đoán đúng 'a'.
• Token ứng viên từ token được chấp nhận được khớp: 'a' thành công dự đoán 'great'.

A.5 Giải thích về Ba Chiến lược Cập nhật
Giả sử chuỗi hợp nhất của chúng tôi là:
S= [a, b, c, a, d]
, trong đó token a xuất hiện hai lần. Hiện tại, token đầu ra tương ứng được lưu trữ trong ma trận O:
O[0] = [a1, a2, a3, a4]
O[1] = [b1, b2, b3, b4]
O[2] = [c1, c2, c3, c4]
O[3] = [a5, a6, a7, a8] (lần xuất hiện thứ hai của a)
O[4] = [d1, d2, d3, d4]

Ba chiến lược cập nhật ma trận kề M là:
• Cur (không kiểm soát): Cập nhật trực tiếp mà không kiểm soát vị trí, có thể trộn đầu ra từ token lặp lại. Ví dụ: M[S] = O, M[a] = [a1, a2, a3, a8] (a8 cuối cùng từ lần xuất hiện thứ hai).
• First: Ghi lại vị trí lần xuất hiện đầu tiên của mỗi token: pos= [0(a),1(b),2(c),4(d)]. Dẫn đến: M[S[pos]] = O[pos], M[a] = [a1, a2, a3, a4]
• Last: Ghi lại vị trí lần xuất hiện cuối cùng của mỗi token: pos= [1(b),2(c),3(a),4(d)]. Dẫn đến: M[S[pos]] = O[pos], M[a] = [a5, a6, a7, a8]

Những chiến lược cập nhật khác nhau này có tác động nhẹ đến Mean Accepted Token. Tuy nhiên, việc trích xuất những vị trí này giới thiệu độ trễ bổ sung, cuối cùng làm giảm hiệu quả tăng tốc tổng thể so với phương pháp không kiểm soát đơn giản hơn.

--- TRANG 14 ---
[Hình 6: Cây tĩnh được sử dụng trong Tái Chế Token.]
