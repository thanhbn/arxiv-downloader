# 2405.18628.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/speculative/2405.18628.pdf
# Kích thước tệp: 3431242 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Giải Mã Prompt Song Song Nhận Biết Phần Cứng cho
Tăng Tốc Tiết Kiệm Bộ Nhớ của Suy Luận LLM
Hao (Mark) Chen1Wayne Luk1Ka Fai Cedric Yiu2
Rui Li3Konstantin Mishchenko3Stylianos I. Venieris3Hongxiang Fan1,3
1Imperial College London, UK
2Hong Kong Polytechnic University, Hong Kong
3Samsung AI Center, Cambridge, UK
{hc1620,w.luk}@ic.ac.uk {rui.li,s.venieris}@samsung.com
konsta.mish@gmail.com cedric.yiu@polyu.edu.hk hongxiangfan@ieee.org
Tóm tắt
Việc giải mã tự hồi quy của các Mô hình Ngôn ngữ Lớn (LLM) dẫn đến các chi phí phụ đáng kể trong hiệu suất phần cứng của chúng. Trong khi nghiên cứu gần đây đã điều tra các kỹ thuật giải mã suy đoán khác nhau cho việc tạo nhiều token, những nỗ lực này chủ yếu tập trung vào việc cải thiện tốc độ xử lý như thông lượng. Điều quan trọng là chúng thường bỏ qua các chỉ số khác cần thiết cho triển khai thực tế, như tiêu thụ bộ nhớ và chi phí đào tạo. Để vượt qua những hạn chế này, chúng tôi đề xuất một giải mã prompt song song mới chỉ yêu cầu 0.0002% tham số có thể đào tạo, cho phép đào tạo hiệu quả trên một GPU A100-40GB trong chỉ 16 giờ. Lấy cảm hứng từ quá trình tạo ngôn ngữ tự nhiên của con người, PPD xấp xỉ các đầu ra được tạo tại các bước thời gian tương lai song song bằng cách sử dụng nhiều token prompt. Phương pháp này khôi phục một phần thông tin phụ thuộc có điều kiện cần thiết cho việc tạo nhiều token, dẫn đến tỷ lệ chấp nhận cao hơn 28% cho các dự đoán tầm xa. Hơn nữa, chúng tôi trình bày một kỹ thuật cây thưa động nhận biết phần cứng tối ưu hóa một cách thích ứng sơ đồ giải mã này để tận dụng đầy đủ khả năng tính toán trên các GPU khác nhau. Thông qua các thí nghiệm rộng rãi trên các LLM từ MobileLlama đến Vicuna-13B trên nhiều benchmark, phương pháp của chúng tôi chứng minh tăng tốc lên đến 2.49× và duy trì chi phí phụ bộ nhớ runtime tối thiểu chỉ 0.0004%. Quan trọng hơn, giải mã prompt song song của chúng tôi có thể phục vụ như một tối ưu hóa trực giao để tích hợp hiệp đồng với giải mã suy đoán hiện có, cho thấy cải thiện tốc độ lên đến 1.22× thêm nữa. Mã của chúng tôi có sẵn tại https://github.com/hmarkc/parallel-prompt-decoding.

1 Giới thiệu
Chi phí phụ bộ nhớ 0.0004% Đào tạo GPU 16 giờ
Tăng tốc 2.24×
Hình 1: So sánh bộ nhớ, tăng tốc, và chi phí đào tạo trên MT-Bench với Vicuna-7B. Đường kính vòng tròn cho thấy giờ GPU đào tạo. Những tiến bộ gần đây trong các mô hình ngôn ngữ lớn (LLM) đang ngày càng có ảnh hưởng trong các ứng dụng AI khác nhau. Tuy nhiên, tạo tự hồi quy, phương pháp de facto được sử dụng trong suy luận LLM, gặp phải hiệu suất phần cứng không đầy đủ do bản chất tuần tự vốn có của nó [23]. Giải mã suy đoán [13,2,11], một kỹ thuật tăng tốc mới nổi, sử dụng khung dự đoán và xác minh cho suy luận LLM, trong đó một mô hình nháp nhỏ hơn trước tiên dự đoán nhiều token tuần tự và sau đó LLM gốc xác minh chúng song song. Mặc dù có tiềm năng, hiệu quả của giải mã suy đoán bị hạn chế bởi độ phức tạp và chi phí đào tạo một mô hình nháp có khả năng đạt được tỷ lệ chấp nhận cao một cách nhất quán trên các mô hình cơ sở và bộ dữ liệu đa dạng. Thêm vào đó, chi phí phụ bộ nhớ runtime bổ sung để thực thi các mô hình nháp tạo ra rào cản đáng kể cho việc áp dụng rộng rãi hơn của giải mã suy đoán, đặc biệt trong môi trường edge và di động nơi dung lượng bộ nhớ bị hạn chế. Xem xét nhu cầu ngày càng tăng về quyền riêng tư và cá nhân hóa của người dùng, việc triển khai LLM trên thiết bị đòi hỏi một giải pháp tiết kiệm bộ nhớ và chi phí hơn để tăng tốc suy luận LLM. Những nỗ lực gần đây đã khám phá khả năng tạo nhiều token song song mà không dựa vào một mô hình nháp transformer riêng biệt [20]. Các phương pháp như chèn các đầu giải mã bổ sung [1] và truy xuất các token được sử dụng thường xuyên [9] được sử dụng để tăng cường hiệu suất. Tuy nhiên, các phương pháp này hoặc giả định một cách tích cực về tính độc lập có điều kiện giữa các token được tạo trong một bước duy nhất [1,9], hoặc sử dụng các token giữ chỗ (ví dụ: token [PAD]) không truyền đạt đủ thông tin ngữ cảnh [20]. Do đó, chúng thường gặp phải tỷ lệ chấp nhận thấp hoặc suy giảm chất lượng đầu ra do thiếu thông tin có điều kiện đầy đủ trong quá trình suy luận.

Để giảm bớt độ phức tạp và chi phí phụ liên quan đến việc sử dụng các mô hình nháp đồng thời duy trì tỷ lệ chấp nhận cao, chúng tôi đề xuất Giải Mã Prompt Song Song (PPD), một khung không phụ thuộc kiến trúc và tiết kiệm bộ nhớ mới áp dụng điều chỉnh prompt cho suy luận LLM không tự hồi quy. Lấy cảm hứng từ quá trình tạo ngôn ngữ tự nhiên của con người nơi các từ liên tục như các biểu thức phổ biến và cụm từ được tạo ra đồng thời, PPD giới thiệu việc sử dụng các token prompt, các embedding được đào tạo tỉ mỉ, cho dự đoán nhiều token. Cụ thể, các token prompt được đào tạo này được nối vào chuỗi đầu vào gốc song song, cho phép tạo ra nhiều token đầu ra đồng thời trong một lần forward pass duy nhất. Trực giác chính của PPD nằm ở quan sát rằng nếu được đào tạo đúng cách, các token prompt được nối vào đầu vào có thể xấp xỉ các token được tạo tại các bước thời gian tương lai, do đó khôi phục một phần thông tin phụ thuộc có điều kiện bị thiếu cho việc tạo nhiều token. Bằng cách định vị chiến lược các token prompt được đào tạo, PPD đạt được tỷ lệ chấp nhận cao hơn 28% khi dự đoán các token tầm xa. Để tăng thêm tỷ lệ chấp nhận token, chúng tôi tạo ra nhiều ứng viên tiếp tục với mỗi token prompt và sử dụng chúng kết hợp với một mask attention cây tùy chỉnh để giảm thiểu chi phí tính toán và bộ nhớ. Khả năng của PPD sử dụng các token prompt chi phí thấp cho dự đoán nhiều token chính xác tạo nền tảng cho việc tăng tốc suy luận LLM. Như được thể hiện trong Hình 1, PPD đạt được tăng tốc tương đương với các phương pháp giải mã suy đoán tiên tiến với chi phí phụ bộ nhớ không đáng kể và chi phí đào tạo giảm. Hơn nữa, để tạo điều kiện cho việc triển khai tối ưu của PPD trên các nền tảng phần cứng khác nhau, chúng tôi đề xuất một kỹ thuật cây thưa động nhận biết phần cứng tinh chỉnh cấu trúc prompt một cách thích ứng trong runtime dựa trên tài nguyên tính toán có sẵn trên phần cứng cụ thể.

Để chứng minh hiệu quả của phương pháp, chúng tôi đánh giá PPD trên MobileLLaMA [6], Vicuna-7b và Vicuna-13b [5]. Chạy trên một GPU duy nhất sử dụng A100-40GB và RTX 4090, phương pháp của chúng tôi đạt được tỷ lệ tăng tốc cho suy luận từ 2.12× đến 2.49× trên một loạt các bộ dữ liệu phổ biến đa dạng bao gồm MT-Bench, HumanEval, và GSM8K. Các thí nghiệm của chúng tôi chứng minh rằng PPD không chỉ đạt được thông lượng tương đương với phương pháp giải mã suy đoán tiên tiến, mà còn quản lý điều này với ít tham số có thể đào tạo hơn đáng kể - cụ thể là 0.0002% tham số có thể đào tạo - và chỉ phát sinh chi phí phụ bộ nhớ tối thiểu (0.0004%), cho thấy PPD cực kỳ tiết kiệm chi phí và bộ nhớ. Việc đào tạo các token prompt có thể được hoàn thành trong 16 giờ

--- TRANG 2 ---
Lớp Embedding
Khối Transformer
Token Prompt
Đào tạo
LLM Đông lạnh
Đào tạo Hiệu quả: 0.0002% tham số 16 giờ GPU
Phương pháp Động Nhận biết Phần cứng
LLM A B C D X
Pha Dự đoán S1 S2
Token Dự đoán (Ứng viên) Token prompt Token được chấp nhận
LLM C
Pha Xác minh (+ Dự đoán Tiếp theo) S1 S2 C S1 S2 C S1 S2 D D X D E F E Y F Z V G
Ứng viên 3 Ứng viên 2 Ứng viên 1
Y != X, bị từ chối Token dự đoán khớp với token được tạo chấp nhận
Đào tạo Sơ đồ Suy luận Triển khai
P=0.7 P=0.1 P=0.2 P=0.3 P=0.6 P=0.1
Token Prompt Ngân sách: 4
Token Prompt Ngân sách: 2
P=0.3 P=0.5 P=0.2
Token Prompt Ngân sách: 3
Token Prompt Song song Vào Dự đoán Song song Ra
Hợp nhất
NeurIPS là một trong những hội nghị tốt nhất đối với các tiến bộ trong AI.
Con người Cảm hứng (Tạo Song song) (Biểu thức Phổ biến)

Hình 2: Tổng quan về PPD. Phần bên trái hiển thị vị trí của các tham số có thể đào tạo và phần giữa hiển thị quá trình dự đoán và xác minh kết hợp trong quá trình suy luận. "Token prompt" biểu thị token đặc biệt với các embedding được đào tạo riêng để thực hiện dự đoán song song.

sử dụng một GPU A100, 8 giờ sử dụng bốn GPU GeForce RTX 3090, so với 1-2 ngày trên bốn GPU A100 yêu cầu cho Eagle [16]. Hơn nữa, vì PPD không yêu cầu sửa đổi LLM gốc hoặc thêm các mạng bổ sung, nó có tính thích ứng cao và trực giao với các kỹ thuật giải mã khác. Ví dụ, nó có thể được kết hợp hiệu quả với một mô hình nháp để giảm thêm độ trễ suy luận.

Đóng góp của chúng tôi được tóm tắt như sau:
• Một Giải Mã Prompt Song Song (PPD) mới áp dụng các token prompt hiệu quả về chi phí cho suy luận LLM không tự hồi quy, đạt được tỷ lệ chấp nhận cao cho dự đoán token tầm xa với chất lượng đầu ra được bảo toàn.
• Một kỹ thuật cây thưa động nhận biết phần cứng tối ưu hóa một cách thích ứng cấu trúc prompt của PPD tại runtime dựa trên tài nguyên tính toán và bộ nhớ có sẵn, tạo điều kiện cho việc triển khai hiệu quả trên các nền tảng phần cứng khác nhau.
• Một triển khai mã nguồn mở của PPD, kèm theo các đánh giá toàn diện trên các mô hình và benchmark khác nhau. Các thí nghiệm của chúng tôi chứng minh rằng PPD đạt được cải thiện tốc độ đáng kể với chi phí phụ bộ nhớ không đáng kể và chi phí đào tạo giảm.

2 Bối cảnh và Công trình Liên quan

Để tăng cường tốc độ suy luận của LLM, các phương pháp khác nhau áp dụng chiến lược dự đoán và xác minh lặp lại để cho phép tạo nhiều token. Trong pha dự đoán, các token tương lai tiềm năng được đề xuất với tốc độ nhanh hơn so với các triển khai tự hồi quy truyền thống. Sau đó, một quá trình xác minh song song đánh giá những token dự đoán nào nên được chấp nhận. Tùy thuộc vào cách các token được tạo ra trong giai đoạn dự đoán, các phương pháp này thường có thể được phân loại là i) giải mã suy đoán và ii) giải mã song song.

2.1 Giải Mã Suy đoán

Pha dự đoán của giải mã suy đoán áp dụng một mô hình nháp nhẹ để tạo ra nhiều token với tốc độ tăng [11]. Trong giai đoạn xác minh, LLM gốc sau đó xác định việc chấp nhận các token dự đoán. Đáng chú ý là cả mô hình nháp và mô hình gốc vẫn tuân theo sơ đồ suy luận tự hồi quy. Tăng tốc đến từ hai yếu tố: i) mô hình nháp chạy nhanh hơn nhiều so với mô hình gốc và có thể tạo ra nhiều token hơn trong cùng đơn vị thời gian; và ii) xác minh token được thực hiện đồng thời, hoặc bằng cách batching hoặc bằng cách kết hợp nhiều ứng viên vào một đầu vào duy nhất sử dụng các mask attention thưa tùy chỉnh [18]. Do đó, tăng tốc tổng thể phụ thuộc vào tỷ lệ chấp nhận và độ trễ suy luận của các mô hình nháp.

Dựa trên sơ đồ giải mã suy đoán, các nghiên cứu khác nhau đã được tiến hành để tối ưu hóa thêm tốc độ suy luận của nó. Để cải thiện độ chính xác của mô hình nháp và tỷ lệ chấp nhận token của nó, Eagle [16] kết hợp các đặc trưng ẩn vào lần forward pass của mô hình nháp. SpecInfer [18] áp dụng suy luận và xác minh suy đoán dựa trên cây, cải thiện tính đa dạng của các ứng viên suy đoán. Sequoia [4] tối ưu hóa cấu trúc cây thưa bằng cách xem xét khả năng của các nền tảng phần cứng cơ bản. Tuy nhiên, hầu hết các phương pháp này đều yêu cầu lưu trữ và duy trì một mô hình nháp riêng biệt. Hơn nữa, có độ phức tạp bổ sung trong việc thiết kế một mô hình nháp hiệu quả.

2.2 Giải Mã Song song

Để vượt qua những hạn chế vốn có của suy luận tự hồi quy và chi phí phụ bộ nhớ liên quan đến việc sử dụng một mô hình nháp riêng biệt, một số nỗ lực đã được thực hiện để tích hợp cả dự đoán và xác minh bằng một mô hình thống nhất. Medusa¹ [1] giới thiệu các đầu mô hình ngôn ngữ (LM) tại lớp cuối cùng của LLM gốc, tạo điều kiện cho việc tạo ra nhiều token trong một lần forward pass duy nhất. Nó cũng sử dụng các mask attention cây trong quá trình xác minh để tăng tốc độ hơn nữa. Để tăng cường việc soạn thảo token với tạo tăng cường truy xuất [10], Rest [9] giới thiệu giải mã dựa trên truy xuất được điều chỉnh cho các tình huống cụ thể. Lấy cảm hứng từ giải mã Jacobi [20] áp dụng nhiều token đặc biệt để tăng tốc dịch máy, Lookahead Decoding [8] cải thiện phương pháp này bằng cách tạo các n-gram song song và sử dụng một pool bộ nhớ cache. Để nắm bắt thêm thông tin trong khi sử dụng nhiều token đặc biệt ở các vị trí khác nhau, PaSS [19] đào tạo các token bổ sung với các lớp embedding cho giải mã song song. Giải mã song song phân cấp [17] giới thiệu việc sử dụng các token [Fork] và [Join], cho phép thực thi song song của nhiều subroutine cấu trúc.

Phương pháp của chúng tôi có thể được phân loại là giải mã song song, với ba đặc điểm mới để phân biệt nó với các phương pháp khác: 1) PPD đào tạo các embedding của các token prompt ensemble có tham số, 2) nó sử dụng một cây thưa động, thích ứng cấu trúc của nó tại mỗi bước suy luận, và 3) chúng tôi đề xuất một thuật toán nhận biết phần cứng để thiết kế một cây thưa động phù hợp với từng nền tảng phần cứng.

3 Giải Mã Prompt Song Song (PPD)

PPD đào tạo các embedding cho các token prompt thay vì phát triển một mô hình riêng biệt. Phương pháp của chúng tôi tích hợp ba bước con vào một bước giải mã duy nhất, tuân theo chiến lược dự đoán và xác minh: (1) tạo ứng viên, trong đó nhiều ứng viên tiếp tục² được dự đoán bằng cách chèn chiến lược các token prompt vào chuỗi đầu vào. Tree attention [18] hợp nhất việc xử lý của nhiều ứng viên vào một lần forward pass duy nhất; (2) xác minh ứng viên, trong đó hai sơ đồ xác minh, khớp chính xác [8] và chấp nhận điển hình [1], được triển khai; (3) chấp nhận ứng viên, trong đó các ứng viên đã được xác thực được tích hợp vào đầu vào và KV cache được cập nhật tương ứng. Sơ đồ suy luận trong Hình 2 minh họa việc tạo và xác minh được kết hợp trong một lần forward pass duy nhất.

3.1 Token Prompt

Các token prompt là thành phần chính của PPD để thực hiện tạo nhiều token. Ban đầu được giới thiệu trong [12] để thích ứng LLM cho các tác vụ cụ thể, các token prompt thường được đặt trước đầu vào, với các đầu ra được tạo theo cách tự hồi quy. Trong công trình này, chúng tôi đề xuất một phương pháp mới sử dụng các token prompt bằng cách định vị chúng một cách chiến lược tại các vị trí nơi các token được dự kiến sẽ được tạo ra song song. Đối với các kỹ thuật giải mã song song thông thường [23,1] giả định tính độc lập có điều kiện hoàn toàn giữa các token được giải mã trong một bước duy nhất, xác suất có điều kiện chính xác p(yi+k+1|x, y1:i+k) được xấp xỉ bởi ptheta(yi+k+1|x, y1:i), trong đó x là đầu vào, y1:i+k biểu thị các đầu ra được tạo cho đến nay, k > 0 chỉ ra khoảng cách token.³ Tuy nhiên, chúng tôi quan sát thấy rằng khi k tăng, khoảng cách giữa xác suất thực tế và xấp xỉ của nó mở rộng, chủ yếu do sự vắng mặt của các phụ thuộc có điều kiện liên quan. Chúng tôi lập luận rằng các token prompt có thể thu hẹp khoảng cách này bằng cách mô hình hóa chính xác hơn xác suất có điều kiện như ptheta(yi+k+1|x, y1:i, ti+1:i+k) trong đó ti là token prompt với khoảng cách token i. Thông qua lần forward pass này trong các lớp decoder, các token prompt liên kết nhân quả này tạo điều kiện cho luồng thông tin dọc theo chuỗi các token suy đoán, do đó khôi phục xác suất có điều kiện.

3.2 Token Prompt Ensemble

Lấy cảm hứng từ prompt ensembling [12], sử dụng nhiều prompt để tạo ra các phản hồi đa dạng và tổng hợp chúng để đưa ra một câu trả lời duy nhất, chúng tôi giới thiệu khái niệm token prompt ensemble (EPT). Khái niệm trừu tượng bổ sung này cho phép chúng tôi tách rời mỗi token prompt khỏi chiều embedding cố định. Đối với mỗi token prompt, tồn tại nhiều EPT tương ứng, mỗi EPT có embedding riêng biệt. Chúng tôi sửa đổi mask attention để đảm bảo rằng mỗi EPT thứ n chỉ phụ thuộc vào các EPT thứ n tương ứng từ các token prompt trước đó. Khả năng hiển thị có chọn lọc này được duy trì cho cả đào tạo và suy luận, trong đó token dự đoán cho mỗi token prompt được xác định bằng cách tính trung bình các logit của các EPT của nó. Việc sử dụng EPT không chỉ cho phép kiểm soát trực tiếp và linh hoạt các tham số có thể đào tạo, mà còn dẫn đến sự gia tăng độ chính xác dự đoán. Xác suất được xấp xỉ là 1/n Σⁿⱼ₌₁ ptheta(yi+k+1|x, y1:i, vʲi+1:i+k), trong đó vʲi+m biểu thị EPT thứ j tại khoảng cách token m.⁴

3.3 Đào tạo

Trong quá trình đào tạo, chỉ các embedding của các token prompt được thay đổi, với các tham số của LLM gốc vẫn được đông lạnh. Chúng tôi áp dụng hai kỹ thuật đào tạo sau:

² Một token ứng viên, còn được gọi là "token dự đoán", là một token nháp được tạo từ một token prompt.
³ Khoảng cách token là số lượng token giữa token được chấp nhận cuối cùng và token được dự đoán.
⁴ Chi tiết thêm về EPT có thể được tìm thấy trong Phụ lục B.

--- TRANG 3 ---
Tạo Mask Cây Thưa Động
Mask Attention cho độ sâu tối đa=2
A B b C D c
Cây Thưa Đầy đủ
A 0 B b 1 C 3 2 D c 4 7 5 6 8
độ sâu tối đa=2
A 0 B b 1 3 2 4 7
độ sâu tối đa=1
Token được chấp nhận Token dự đoán (Ứng viên) Token prompt
Cắt tỉa

Hình 3: Cây thưa động.

Chèn Ngẫu nhiên Token Prompt: Chèn ngẫu nhiên các token prompt khắp chuỗi đầu vào giảm thiểu thiên lệch ngữ cảnh từ việc chỉ nối chúng ở cuối. Phương pháp này mở rộng khả năng dự đoán của các token prompt vượt ra ngoài từ vựng hạn chế như <eos> và dấu câu.

Chưng cất Kiến thức: Để điều chỉnh hành vi dự đoán của các token prompt với LLM gốc, chúng tôi sử dụng chưng cất kiến thức. Thay vì sử dụng nhãn cứng, các token prompt được đào tạo dựa trên các logit được tạo bởi LLM gốc. Theo Medusa [1], hàm mất mát được công thức hóa như:

LPD = 1/N Σᴺᵢ₌₁ DKL(Pi||Qi)·alphaⁱ⁻¹, (1)

trong đó DKL là độ phân kỳ KL, Pi là phân phối dự đoán của token prompt thứ i, Qi là phân phối tương ứng từ LLM gốc, và alpha là tỷ lệ suy giảm.

4 Cây Thưa Động

4.1 Động lực

Để đạt được tăng tốc cao hơn, PPD sử dụng một tree attention chuyên biệt [1,18] để xử lý nhiều ứng viên trong một bước giải mã duy nhất mà không mở rộng kích thước batch. Đáng chú ý, PPD sử dụng một cây thưa [1,4], được thiết kế để ưu tiên các ứng viên có độ chính xác dự đoán cao hơn. Một sự khác biệt chính so với cây thưa được sử dụng trong các công trình trước đó là việc nối một chuỗi token prompt vào mỗi nút cây như được thể hiện trong Hình 3. Để tối ưu hóa độ dài chấp nhận được tính trung bình qua các bước giải mã, việc cân bằng cẩn thận số lượng token ứng viên và token prompt là rất quan trọng. Thay vì nối một số lượng đồng đều token prompt vào mọi token ứng viên, chúng tôi phân bổ chúng dựa trên xác suất của mỗi ứng viên, khiến độ sâu tối đa và cấu trúc của cây thay đổi tại mỗi bước giải mã.

4.2 Thuật toán Xây dựng

Chúng tôi nhằm mục đích xây dựng một cây thưa động tối đa hóa số lượng token được tạo ra được tính trung bình với số lượng token ứng viên và token prompt hạn chế. Chúng tôi đầu tiên định nghĩa thuật toán xây dựng cây như một bài toán tối ưu hóa có ràng buộc.

Định nghĩa 4.1. Cho m là số lượng tối đa token prompt trên mỗi nút cây. Cây thưa động T có thể tồn tại trong m trạng thái, mỗi trạng thái được biểu diễn bởi Tk tương ứng với trạng thái sk, trong đó 1<=k<=m. Cho C(Tk) biểu thị cây con của Tk chỉ bao gồm các token ứng viên. Độ sâu tối đa của C(Tk) là k.

Mệnh đề 4.1. Đối với trạng thái cây thưa động Tk, trong đó mỗi token ứng viên v tuân theo đường dẫn Path(v) từ gốc, và xác suất chấp nhận pk tại mỗi vị trí đường dẫn k, số lượng token dự kiến f(Tk) được tạo ra được cho bởi f(Tk) = Σv∈C(Tk) Π i∈Path(v) pi, trong đó Π i∈Path(v) pi biểu thị đóng góp của một token v vào số lượng token dự kiến.

Sau đó chúng tôi đề xuất một xấp xỉ của số lượng token được tạo ra được tính trung bình, bằng cách xem xét các token được tạo ra tại bước giải mã hiện tại và bước tiếp theo.

--- TRANG 4 ---
Mệnh đề 4.2. Số lượng token tổng dự kiến F(Tk) được tạo ra cho trạng thái cây thưa động F(Tk) tại bước giải mã hiện tại và bước tiếp theo được cho bởi F(Tk) = f(Tk) + Σᵐᵢ₌₁ p(si|sk)f(Ti), trong đó p(si|sk) biểu thị xác suất chuyển trạng thái từ trạng thái sk sang trạng thái si.

Bây giờ chúng tôi sẵn sàng giới thiệu Mệnh đề 4.3, mà chúng tôi sử dụng trong thuật toán cắt tỉa.

Mệnh đề 4.3. Đối với trạng thái cây thưa động Tk với cây con ứng viên ck=C(Tk), sự thay đổi trong tổng số token dự kiến F(Tk) do việc loại bỏ một token prompt tại token ứng viên c được cho bởi ΔF=p(c)·(f(Ti)−f(Ti−1)), trong đó p(c) là xác suất chấp nhận của ứng viên c, i biểu thị số lượng token prompt trước khi loại bỏ. Chúng tôi giả định rằng i > 1.

Để xây dựng một cây thưa động tối ưu gần đúng với số lượng token ứng viên và token prompt được chỉ định, quá trình bao gồm: (1) Cây Ứng viên Tối ưu: Xây dựng các cây chỉ sử dụng token ứng viên ở các độ sâu khác nhau, sử dụng thuật toán từ Medusa [1] và Sequoia [4] để tối đa hóa f(Tk) như được nêu trong Mệnh đề 4.1; (2) Nối Token Prompt: Gắn số lượng token prompt tối đa cho phép vào mỗi token ứng viên từ bước đầu tiên; (3) Loại bỏ Token Prompt Tham lam: Loại bỏ các token prompt một cách tham lam để giảm thiểu ΔF (Mệnh đề 4.3), tiếp tục cho đến khi đạt được số lượng token prompt mong muốn.

Bây giờ chúng tôi giới thiệu công thức của số lượng token thực được tính trung bình được tạo ra.

Mệnh đề 4.4. Số lượng token được tính trung bình R(Tk) được tạo ra cho trạng thái cây thưa động F(Tk) được cho bởi R(T) = Σᵐᵢ₌₁ p(si)f(Ti), trong đó p(si) là xác suất trạng thái ổn định của trạng thái si, và f là hàm được định nghĩa trong Mệnh đề 4.1.

Nhận biết phần cứng. Tất cả các xác suất được sử dụng ở trên có thể được xấp xỉ trên một bộ dữ liệu xác thực. Thuật toán xây dựng cây thưa động bây giờ có thể được công thức hóa như việc tìm cây thưa động T với nc token ứng viên và np token prompt để tối đa hóa R(T):

c(nc, np) = max T,|C(T)|=nc,|T|=nc+np R(T).

Đối với một kích thước cây cố định n, chúng tôi khám phá tất cả các kết hợp của nc và np trong đó n=nc+np, để xác định cây thưa động tối đa hóa R(Tk). Để xác định kích thước cây tối ưu n, chúng tôi định nghĩa hai hàm chính: 1. Độ dài chấp nhận tau(n) (độc lập phần cứng) và 2. Độ trễ forward pass Lfp(n) (phụ thuộc phần cứng). Tỷ lệ tăng tốc, Speedup(n) = tau(n)/Lfp(n), được ước tính bằng cách sử dụng một bộ dữ liệu xác thực, với tau(n) được đánh giá một lần và Lfp(n) được kiểm tra trên các loại phần cứng khác nhau. Chúng tôi nhằm mục đích tìm giá trị của n tối đa hóa Speedup(n).

5 Thí nghiệm

Mô hình và testbed. Chúng tôi đã tiến hành tất cả các thí nghiệm sử dụng MobileLLaMA-1.4B [6], Vicuna-7B và Vicuna-13B [5]. Chúng tôi sử dụng 3 token prompt và 1 EPT cho mỗi token prompt cho tất cả các thí nghiệm suy luận. Thông lượng suy luận của các mô hình được đánh giá trên một GPU NVIDIA A100 duy nhất với 40GB bộ nhớ và một GeForce RTX 4090 sử dụng kích thước batch là 1 và độ chính xác FP16. Chi tiết thêm về thiết lập thí nghiệm có thể được tìm thấy trong Phụ lục C.

Đào tạo. Chúng tôi đông lạnh tất cả các tham số có thể đào tạo của LLM gốc. Các embedding token prompt được đào tạo bằng cách sử dụng các logit chưng cất được tạo từ bộ dữ liệu ShareGPT [22], với độ dài ngữ cảnh tối đa là 1024, một bộ lập lịch tốc độ học cosine bắt đầu từ 0.01, và không có warmup. Các embedding token prompt được khởi tạo với các embedding token văn bản bình thường.

Bộ dữ liệu. Chúng tôi đánh giá hiệu suất thông lượng của PPD trên các tác vụ và bộ dữ liệu khác nhau. Cụ thể, chúng tôi đánh giá PPD bằng cách sử dụng bộ dữ liệu MT-Bench [25], chứa các câu hỏi nhiều lượt với một loạt các chủ đề, trong cả cài đặt không tham lam (nhiệt độ tuân theo cấu hình mặc định) và tham lam (nhiệt độ=0). Chúng tôi sử dụng các bộ dữ liệu GSM8K [7] và HumanEval [3] chỉ trong cài đặt tham lam. Bộ dữ liệu GSM8K bao gồm các bài toán toán học cấp tiểu học và chúng tôi sử dụng 500 câu hỏi đầu tiên của phần test split cho các đánh giá của chúng tôi. HumanEval bao gồm các tác vụ lập trình, mà chúng tôi đặt giới hạn token mới tối đa là 512 để kiểm soát độ dài của các chuỗi được tạo. Chúng tôi sử dụng bộ dữ liệu Alpaca [15] làm bộ dữ liệu xác thực để tạo ra các độ trễ và độ dài chấp nhận được sử dụng cho việc xây dựng cây thưa động.

--- TRANG 5 ---
Mô hình Phương pháp T tau Lfp(s) Chất lượng Ptr(%) Str Sinput
M Vanilla 50.2 1.00 0.020 - NA NA 1
PPD 108.7 2.43 0.022 Giống 4.50e−4 (10,84,89) (40,285,285)
V-7B Vanilla 39.2 1.00 0.026 5.99 NA NA 1
Medusa 82.0 2.51 0.0307 5.98 8.07 63 63
PPD 88.0 2.54 0.029 5.93 1.82e−4 (10,33,34) (40,105,105)
V-13B Vanilla 30.4 1.00 0.0330 6.38 NA NA 1
Medusa 63.4 2.59 0.0408 - 5.52 63 63
PPD 66.1 2.44 0.0379 6.32 7.87e−5 (10,20,20) (40,60,60)

Bảng 1: So sánh các chỉ số hiệu suất của MobileLLaMA (M) cho cài đặt tham lam, Vicuna-7B (V-7B) và Vicuna-13B (V-13B) cho cài đặt không tham lam sử dụng các phương pháp giải mã khác nhau. Bảng chi tiết thông lượng (T trong token/s), độ dài chấp nhận trung bình (tau trong token), độ trễ forward pass (Lfp trong giây), điểm chất lượng trên MT-benchmark, tỷ lệ phần trăm tham số có thể đào tạo bổ sung (Ptr) và độ dài đầu vào (Sinput) sau pha prefilling. PPD sử dụng cây thưa động với kích thước cây biến đổi (Str), được biểu diễn dưới dạng tuple. Giống nghĩa là đầu ra khớp với đầu ra của LLM gốc.

5.1 So sánh Tăng tốc với Phương pháp Giải mã Song song

[Biểu đồ cột so sánh throughput giữa các phương pháp trên V-7B và V-13B]

Hình 4: Đánh giá so sánh tăng tốc độ trễ giữa PPD và các phương pháp giải mã song song khác. Các thí nghiệm được tiến hành sử dụng bộ dữ liệu MT-Bench, với nhiệt độ được đặt theo cấu hình mặc định của MT-Bench cho Medusa và PPD.

Chúng tôi so sánh tỷ lệ tăng tốc của PPD với các phương pháp giải mã song song tiên tiến trên MT-Bench trong cài đặt không tham lam trong Hình 4. PPD đạt được tăng tốc lên đến 13.8% cao hơn Medusa và giữa 2 lần và 3 lần cao hơn các phương pháp giải mã song song khác. Chúng tôi xem xét các yếu tố góp phần vào tỷ lệ tăng tốc được cải thiện và các chỉ số hiệu suất khác, như được trình bày trong Bảng 1. Lý do cho sự gia tăng tỷ lệ tăng tốc có hai mặt. Thứ nhất, PPD tạo ra các token ứng viên với tỷ lệ chấp nhận cao hơn Medusa khi sử dụng cây thưa cùng kích thước. Đáng chú ý, PPD tiếp tục đạt được tỷ lệ chấp nhận tương đương hoặc hơi tốt hơn ngay cả khi sử dụng cây thưa nhỏ hơn nhiều - từ một phần ba đến một nửa kích thước. Thứ hai, PPD hưởng lợi từ độ trễ forward pass thấp hơn do khả năng sử dụng kích thước cây thưa nhỏ hơn và do đó độ dài đầu vào ngắn hơn. PPD cũng loại bỏ chi phí tính toán liên quan đến các đầu giải mã riêng biệt. PPD duy trì chất lượng đầu ra giống nhau, đạt được điểm tương tự trên MT-Bench trong khi sử dụng ít tham số có thể đào tạo hơn đáng kể.

Hình 5 hiển thị thông lượng của PPD trên MT-Bench, HumanEval, và GSM8K với nhiệt độ bằng 0. PPD đạt được tỷ lệ tăng tốc walltime nhất quán từ 2.12× đến 2.49× trên các GPU khác nhau. Nói chung, PPD hoạt động tốt hơn trong các tác vụ lập trình và lý luận toán học, đạt được tăng tốc giữa 2.21× và 2.49×. Điều này có thể được quy cho thực tế rằng cả mã và phương trình toán thường chứa các mẫu cố định và các ký hiệu lặp lại, điều này thu hẹp phạm vi của các ứng viên có thể và đơn giản hóa việc dự đoán. Chúng tôi cũng thấy rằng với chấp nhận điển hình, tăng tốc tăng với nhiệt độ. Một xu hướng đáng chú ý khác là các mô hình nhỏ hơn, như Vicuna-7B, thường đạt được tỷ lệ tăng tốc đáng kể hơn so với các mô hình lớn hơn, như Vicuna-13B. PPD nhằm mục đích tạo ra nhiều token hơn mỗi bước, điều này đi kèm với nhu cầu tính toán tăng. Đối với các mô hình lớn hơn đã yêu cầu tài nguyên tính toán đáng kể, cần thiết phải giới hạn kích thước cây thưa để tránh vượt quá giới hạn sử dụng GPU và gây ra độ trễ tăng. Kết quả là, số lượng token được chấp nhận mỗi bước giảm, dẫn đến tăng tốc thấp hơn. Tuy nhiên, điều này có thể được tính trung bình khi sử dụng các GPU mạnh hơn NVIDIA A100 và RTX 4090, như NVIDIA H100.

5.2 Dự đoán Token Tầm xa

Đối với một cây thưa cụ thể, độ chính xác tích lũy cung cấp giới hạn trên lý thuyết cho số lượng token được tạo ra mỗi bước và tỷ lệ tăng tốc tối đa có thể. Do đó, tối đa hóa

--- TRANG 6 ---
[Hàng loạt biểu đồ hiển thị throughput của PPD và vanilla models trên các nhiệm vụ khác nhau]

Hình 5: Thông lượng của PPD và các mô hình vanilla trên các tác vụ khác nhau. Nhiệt độ cho các thí nghiệm được đặt là 0 và đầu ra được tạo của PPD khớp chính xác với đầu ra của LLM gốc. Chúng tôi không hiển thị kết quả của Vicuna-13B trên RTX 4090 vì nó không vừa với bộ nhớ GPU.

độ chính xác tích lũy là rất quan trọng cho hiệu quả của PPD. Hình 6 chứng minh độ chính xác tích lũy của các token được dự đoán tại các vị trí khác nhau. Chúng tôi tóm tắt ba hiểu biết chính sau từ kết quả.

PPD xuất sắc trong việc dự đoán các token xa hơn. Như được mô tả trong Hình 6a, PPD luôn vượt trội hơn Medusa về độ chính xác trên tất cả các vị trí token. Khoảng cách độ chính xác giữa PPD và Medusa mở rộng với khoảng cách token tăng (ví dụ, sự khác biệt độ chính xác top-10 là 0.03 cho từ 'tiếp theo tiếp theo' so với 0.12 cho từ 'tiếp theo tiếp theo tiếp theo tiếp theo'). Sự cải thiện này có thể được quy cho khả năng của PPD khôi phục một phần thông tin phụ thuộc có điều kiện thông qua các token prompt được kết nối nhân quả.

PPD hoạt động tốt trong việc tạo ra một loạt các ứng viên token hợp lý rộng hơn. Ví dụ, trong việc dự đoán token tại khoảng cách token 3, 10 ứng viên hàng đầu thể hiện sự cải thiện độ chính xác 0.1 so với Medusa, so với chỉ 0.02 cho ứng viên hàng đầu 1. Điều này chứng minh giá trị của việc sử dụng tree attention và kích thước cây khả thi lớn nhất trong quá trình suy luận, vì nhiều ứng viên tiếp tục làm tăng thêm cải thiện độ chính xác.

Nhiều EPT cho mỗi token prompt và kích thước mô hình lớn hơn mang lại những cải thiện khiêm tốn về độ chính xác dự đoán. Hình 6b cho thấy rằng việc sử dụng 100 EPT cho mỗi token prompt dẫn đến cải thiện độ chính xác, dao động từ 0.018 đến 0.045. Hình 6c hiển thị rằng PPD với Vicuna-13B vượt trội hơn Vicuna-7B với mức tăng độ chính xác 0.011∼0.038. Sự gia tăng này là do chiều embedding lớn hơn và các lớp sâu hơn của Vicuna-13B, điều này tăng cường sức mạnh biểu đạt của các token prompt. Tuy nhiên, những mức tăng này là khiêm tốn và có thể bị bù đắp bởi gánh nặng tính toán tăng của các mô hình lớn hơn.

[Ba biểu đồ so sánh độ chính xác tích lũy]

Hình 6: So sánh độ chính xác tích lũy trên các cấu hình mô hình khác nhau và khoảng cách dự đoán. 'V7' cho Vicuna-7B, và 'V13' cho Vicuna-13B. Ký hiệu '@ i' đề cập đến khoảng cách token i. '100 EPT' biểu thị 100 EPT cho mỗi token prompt. Độ chính xác tích lũy được định nghĩa là độ chính xác top-k (ví dụ: một dự đoán là đúng nếu các ứng viên top-k chứa sự thật cơ bản). Các phép đo này được thu thập từ bộ dữ liệu Alpaca Eval với tối đa 20 bước.

--- TRANG 7 ---
5.3 Hiệu quả Bộ nhớ và Tích hợp Hiệp đồng với Giải mã Suy đoán

[Biểu đồ hiển thị mức sử dụng bộ nhớ mô hình]

Hình 7: Mức sử dụng bộ nhớ mô hình.

Hiệu quả bộ nhớ. Như được thể hiện trong Hình 7, chúng tôi so sánh chi phí phụ bộ nhớ của PPD với các phương pháp giải mã song song hàng đầu (Medusa) và giải mã suy đoán (Eagle). Chi phí phụ bộ nhớ của PPD chỉ là 0.004% của Medusa và 0.007% của Eagle. Hiệu quả này xuất phát từ việc sử dụng hiệu quả các embedding trong PPD, vốn nhỏ hơn đáng kể so với các đầu giải mã và mô hình nháp, cả hai đều mở rộng theo kích thước từ vựng.

PPD + Giải mã Suy đoán. Như một tối ưu hóa trực giao trong việc tăng tốc LLM, PPD có thể được tích hợp dễ dàng với giải mã suy đoán [11]. Để chứng minh điều này, chúng tôi đã áp dụng PPD cho Vicuna-68M [24] và sử dụng nó làm mô hình nháp cho Vicuna-7B. Sự kết hợp này dẫn đến tăng tốc lên đến 1.22× cho giải mã suy đoán trên Vicuna-7B so với chỉ sử dụng giải mã suy đoán một mình.

5.4 Nghiên cứu Ablation

Cây Thưa Động. Hình 8a cho thấy rằng các cây thưa động liên tục đạt được độ dài chấp nhận dài hơn so với các cây tĩnh và ngẫu nhiên trên các kích thước khác nhau. Độ dài chấp nhận cho các cây thưa động cho thấy sự gia tăng ổn định khi kích thước cây mở rộng, gợi ý khả năng mở rộng tốt của nó. Sự hội tụ của các cây thưa động và tĩnh ở kích thước lớn hơn gợi ý một sự tương đồng cấu trúc xuất hiện từ các ràng buộc về độ sâu cây và số lượng nút cây.

Kích thước Cây Nhận biết Phần cứng. Hình 8b trình bày tăng tốc lý thuyết trên các GPU khác nhau. Hình 8c xác thực rằng kích thước cây thưa tối ưu, được suy ra từ các mô hình tăng tốc lý thuyết, thực sự dẫn đến tăng tốc thực tế lớn nhất được quan sát.

[Ba biểu đồ đánh giá hiệu suất của Dynamic Sparse Tree]

Hình 8: Đánh giá Hiệu suất Cây Thưa Động. Các cây thưa tĩnh trong (a) luôn sử dụng số lượng token prompt lớn nhất có thể cho mỗi ứng viên. Tăng tốc lý thuyết trong (b) được tính là tỷ lệ của độ dài chấp nhận (độc lập phần cứng) so với chi phí phụ độ trễ (phụ thuộc phần cứng). Kích thước cây tối ưu được thu từ giá trị đỉnh của tăng tốc lý thuyết. Các độ trễ trong (b) được thu từ suy luận trên cùng một prompt cho 512 lần forward pass. (c) hiển thị tăng tốc thực tế thu được bằng cách chạy suy luận trên các GPU khác nhau với độ dài cây khác nhau trên bộ dữ liệu Alpaca Eval.

6 Kết luận

Chúng tôi đã giới thiệu PPD, một phương pháp giải mã song song tiết kiệm bộ nhớ, hiệu quả về chi phí và mạnh mẽ kết hợp một cây thưa động nhận biết phần cứng. Sử dụng các token prompt được đào tạo đặc biệt để dự đoán các token tầm xa một cách chính xác, PPD đạt được tăng tốc lên đến 2.49× trong suy luận trong khi chỉ sử dụng 0.0002% tham số có thể đào tạo bổ sung và không kết hợp các mô hình hoặc thành phần kiến trúc mới. Chúng tôi tin rằng PPD cung cấp một góc nhìn mới về khả năng của giải mã song song. Trong công việc tương lai, nó có thể được hiệp đồng với các kỹ thuật giải mã suy đoán hoặc song song khác để đẩy nhanh suy luận hơn nữa.

--- TRANG 8 ---
Tài liệu Tham khảo

[1]Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, và Tri Dao. Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads. arXiv preprint arXiv:2401.10774, 2024.

[2] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, và John Jumper. Accelerating Large Language Model Decoding with Speculative Sampling. arXiv preprint arXiv:2302.01318, 2023.

[3]Mark Chen et al. Evaluating Large Language Models Trained on Code. arXiv preprint arXiv:2107.03374, 2021.

[4] Zhuoming Chen, Avner May, Ruslan Svirschevski, Yuhsun Huang, Max Ryabinin, Zhihao Jia, và Beidi Chen. Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding. arXiv preprint arXiv:2402.12374, 2024.

[5]Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, và Eric P. Xing. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality, Tháng 3 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.

[6]Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, và Chunhua Shen. MobileVLM: A Fast, Strong and Open Vision Language Assistant for Mobile Devices. arXiv preprint arXiv:2312.16886, 2023.

[7]Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, và John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

[8]Yichao Fu, Peter Bailis, Ion Stoica, và Hao Zhang. Breaking the Sequential Dependency of LLM Inference Using Lookahead Decoding, Tháng 11 2023. URL https://lmsys.org/blog/2023-11-21-lookahead-decoding/.

[9]Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D. Lee, và Di He. Rest: Retrieval-based Speculative Decoding. arXiv preprint arXiv:2311.08252, 2023.

[10] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, và Wen-tau Yih. Dense Passage Retrieval for Open-Domain Question Answering. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), trang 6769-6781, 2020.

[11] Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W. Mahoney, Amir Gholami, và Kurt Keutzer. Speculative Decoding with Big Little Decoder. Advances in Neural Information Processing Systems (NeurIPS), 36, 2024.

[12] Brian Lester, Rami Al-Rfou, và Noah Constant. The Power of Scale for Parameter-Efficient Prompt Tuning. Trong Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021.

[13] Yaniv Leviathan, Matan Kalman, và Yossi Matias. Fast Inference from Transformers via Speculative Decoding. Trong International Conference on Machine Learning (ICML), 2023.

[14] Xiang Lisa Li và Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. Trong Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), trang 4582-4597. Association for Computational Linguistics, 2021.

[15] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto. AlpacaEval: An Automatic Evaluator of Instruction-following Models. https://github.com/tatsu-lab/alpaca_eval, 2023.

--- TRANG 9 ---
[16] Yuhui Li, Fangyun Wei, Chao Zhang, và Hongyang Zhang. EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty. arXiv preprint arXiv:2401.15077, 2024.

[17] Mingdao Liu, Aohan Zeng, Bowen Wang, Peng Zhang, Jie Tang, và Yuxiao Dong. APAR: LLMs can do auto-parallel auto-regressive decoding. arXiv preprint arXiv:2401.06761, 2024.

[18] Xupeng Miao et al. SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification. Trong ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), 2024.

[19] Giovanni Monea, Armand Joulin, và Edouard Grave. PaSS: Parallel Speculative Sampling. arXiv preprint arXiv:2311.13581, 2023.

[20] Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, và Emanuele Rodolà. Accelerating Transformer Inference for Translation via Parallel Decoding. Trong Annual Meeting of the Association for Computational Linguistics (ACL), 2023.

[21] Apoorv Saxena. Prompt Lookup Decoding, Tháng 11 2023. URL https://github.com/apoorvumang/prompt-lookup-decoding/.

[22] ShareGPT. ShareGPT, 2023. URL https://huggingface.co/datasets/Aeala/ShareGPT_Vicuna_unfiltered.

[23] Mitchell Stern, Noam Shazeer, và Jakob Uszkoreit. Blockwise Parallel Decoding for Deep Autoregressive Models. Trong Advances in Neural Information Processing Systems (NeurIPS), 2018.

[24] Sen Yang, Shujian Huang, Xinyu Dai, và Jiajun Chen. Multi-Candidate Speculative Decoding. arXiv preprint arXiv:2401.06706, 2024.

[25] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, và Ion Stoica. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. Trong Advances in Neural Information Processing Systems (NeurIPS), 2023.

--- TRANG 10 ---
Tài liệu Bổ sung
Giải Mã Prompt Song Song Nhận Biết Phần Cứng cho Tăng Tốc Tiết Kiệm Bộ Nhớ của Suy Luận LLM

Mục lục
A Mất mát Đào tạo 13
B Nghiên cứu Ablation Mở rộng 13
B.1 Ảnh hưởng của EPT đến Độ chính xác Dự đoán . . . . . . . . . . . . . . . . . . . . . . 13
B.2 Tác động của Chưng cất Kiến thức (KD), Epochs, và Kích thước Batch đến Độ chính xác Dự đoán . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
B.3 Prefix Tuning + Prompt Token . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
B.4 Custom Decoding Heads + Prompt Token . . . . . . . . . . . . . . . . . . . . . 15
B.5 Attention Masking cho EPT . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
B.6 Phương pháp Tổng hợp cho EPT . . . . . . . . . . . . . . . . . . . . . . . . . . 17
B.7 Multi-exit Ensemble . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
C Chi tiết Thí nghiệm 18
D Hạn chế 19
E Tác động Xã hội 19

--- TRANG 11 ---
A Mất mát Đào tạo

[Hai biểu đồ hiển thị training loss theo epoch]

Hình 9: Mất mát Đào tạo

Chúng tôi nghiên cứu mất mát đào tạo của PPD với các EPT khác nhau. Hình 9a cho thấy rằng, với 3 token prompt và 1 EPT, mất mát ban đầu khá cao, bắt đầu trên 5. Có sự giảm mạnh trong mất mát trong epoch đầu tiên, giảm xuống dưới 2. Sau sự giảm ban đầu này, mất mát ổn định và dao động xung quanh một giá trị hơi dưới 2 cho phần còn lại của các epoch đào tạo (lên đến epoch 12). Các dao động mất mát vẫn trong phạm vi hẹp, cho thấy hiệu suất nhất quán. Sự dao động có thể được quy cho việc chèn các token prompt tại các vị trí ngẫu nhiên. Mặt khác, Hình 9b, với 3 token prompt và 100 EPT, cho thấy mất mát ban đầu bắt đầu dưới 3, thấp hơn đáng kể so với PPD với 1 EPT. Tương tự, có sự giảm mạnh trong epoch đầu tiên, với mất mát giảm xuống khoảng 2.5. Tuy nhiên, không giống như PPD với 1 EPT, mất mát tiếp tục giảm dần qua các epoch, cho thấy xu hướng giảm. Điều này gợi ý rằng việc tăng số lượng EPT cải thiện khả năng học tập của mô hình và giảm mất mát đào tạo hiệu quả hơn theo thời gian.

B Nghiên cứu Ablation Mở rộng

B.1 Ảnh hưởng của EPT đến Độ chính xác Dự đoán

[Bảng hiển thị độ chính xác dự đoán với các EPT khác nhau]

Bảng 2: Độ chính xác Dự đoán của PPD với các EPT khác nhau. '@i' biểu thị khoảng cách token i. 'Top-k' biểu thị độ chính xác dự đoán top-k. Kết quả được thu trên bộ dữ liệu Alpaca với 20 bước.

Bảng 2 trình bày độ chính xác dự đoán của PPD sử dụng các EPT khác nhau. Kết quả cho thấy rằng việc tăng số lượng EPT thường tăng cường độ chính xác dự đoán của PPD, đặc biệt cho các dự đoán token tầm xa. Số lượng EPT cao hơn (ví dụ: 100 và 50) liên tục tạo ra độ chính xác dự đoán tốt hơn so với số lượng EPT thấp hơn.

B.2 Tác động của Chưng cất Kiến thức (KD), Epochs, và Kích thước Batch đến Độ chính xác Dự đoán

Bảng 3 tóm tắt kết quả của chúng tôi với các cài đặt khác nhau. Chúng tôi phân tích ảnh hưởng của mỗi yếu tố đến độ chính xác dự đoán trong cuộc thảo luận sau.

--- TRANG 12 ---
[Bảng hiển thị độ chính xác dự đoán với và không có knowledge distillation]

Bảng 3: Độ chính xác Dự đoán cho PPD với và không có chưng cất kiến thức (KD) cho các EPT, epoch, và kích thước batch khác nhau.

B.2.1 Epochs Đào tạo

Chúng tôi đầu tiên điều tra ảnh hưởng của số lượng epoch đào tạo đến độ chính xác dự đoán. Đối với các mô hình sử dụng 100 EPT với KD được bật và kích thước batch là 4, chúng tôi quan sát sự cải thiện ổn định trong độ chính xác dự đoán khi số lượng epoch tăng. Cụ thể, độ chính xác Top-1 tại khoảng cách 1-token tăng từ 0.504 ở 1 epoch lên 0.525 ở 12 epoch, trong khi độ chính xác Top-5 tại khoảng cách 1-token cải thiện từ 0.793 lên 0.805. Tương tự, độ chính xác Top-1 tại khoảng cách 2-token tăng từ 0.273 lên 0.308, và độ chính xác Top-5 tại khoảng cách 2-token cải thiện từ 0.598 lên 0.625 trong cùng phạm vi epoch. Xu hướng này chứng minh tác động tích cực của việc đào tạo kéo dài đến hiệu suất của PPD khi KD được áp dụng.

B.2.2 Chưng cất Kiến thức

Khi KD không được áp dụng, như được thể hiện cho 100 EPT ở 12 epoch với kích thước batch là 4, các chỉ số hiệu suất thường thấp hơn. Sự cải thiện độ chính xác dự đoán với KD lên đến 12%. Điều này gợi ý rằng KD đóng góp đáng kể vào độ chính xác dự đoán cho PPD.

B.2.3 Ảnh hưởng của Kích thước Batch

Chúng tôi cũng xem xét tác động của kích thước batch đến độ chính xác dự đoán. Đối với mô hình được đào tạo với 100 EPT, KD được bật, và 12 epoch, việc giảm kích thước batch từ 4 xuống 1 dẫn đến sự cải thiện nhẹ trong độ chính xác dự đoán lên đến 1%.

B.3 Prefix Tuning + Prompt Token

Prefix tuning [14], tương tự như prompt tuning, cung cấp một phương pháp hiệu quả về tham số để fine-tune một mô hình được pre-train. Không giống như prompt tuning, nó sửa đổi KV cache của mọi lớp attention bằng cách prepend các vector được đào tạo. Chúng tôi giả thuyết rằng sự kết hợp của prefix tuning và prompt token có thể dẫn đến khả năng học tập lớn hơn và độ chính xác dự đoán cao hơn. Giả thuyết này dựa trên trực giác rằng các prompt token nên nhìn thấy một ngữ cảnh khác với các input token khi dự đoán các token tầm xa. Ví dụ, nếu chuỗi đầu vào là "Once upon a time", thì việc tăng cường đầu vào với một template prompt có thể cung cấp ngữ cảnh ngữ nghĩa phù hợp hơn cho dự đoán tầm xa. Một đầu vào được tăng cường như "Predict the next-next token. Once upon a time" có thể trao quyền cho prompt token dự đoán đúng next-next token. Prefix tuning phục vụ như template prompt để tăng cường các hidden state hiển thị với các prompt token.

Để giữ lại phân phối của mô hình gốc, chúng tôi sửa đổi attention mask để các prefix token chỉ hiển thị với các prompt token. Điều này đảm bảo rằng chúng ta có thể tạo ra các đầu ra bảo toàn phân phối của mô hình gốc. Chúng tôi đặt ra rằng các prompt token ở các vị trí khác nhau nên nhìn thấy các ngữ cảnh khác nhau nên chúng tôi cho phép một prompt token tại một vị trí cụ thể nhìn thấy một tập hợp riêng biệt các prefix token, như được thể hiện trong Hình 10.

--- TRANG 13 ---
[Hình vẽ mô tả attention mask với prefix tokens]

Hình 10: 'P1' là prefix token cho prompt token 'S1' và 'P2' cho 'S2'. 'C' là input token. Dấu tick xanh có nghĩa là khả năng nhìn thấy trong quá trình tính toán attention. Ví dụ, 'S1' có thể nhìn thấy 'P1' nhưng không thể nhìn thấy 'P2'. 'C' không nhìn thấy bất kỳ prefix token nào nên đầu ra được tạo tương ứng với 'C' không bị thay đổi bởi việc sử dụng prefix tuning.

[Bảng so sánh độ chính xác dự đoán với và không có prefix tuning]

Bảng 4: Độ chính xác Dự đoán của PPD với và không có prefix tuning. 1 EPT được sử dụng cho tất cả mô hình và 1 prefix token được sử dụng cho prefix tuning.

Bảng 4 so sánh độ chính xác dự đoán của PPD với và không có việc sử dụng prefix tuning. Kết quả cho thấy rằng các mô hình không có prefix tuning vượt trội hơn những mô hình có prefix tuning lên đến 28%, điều này gợi ý rằng, trong thiết lập này, prefix tuning không tăng cường độ chính xác dự đoán của PPD. Thay vào đó, nó dường như làm giảm hiệu suất, có thể do độ phức tạp được giới thiệu bởi việc sửa đổi KV cache của các lớp attention với prefix token. Không giống như prompt token, prefix token không tương tác với input token, có nghĩa là chúng không thay đổi một cách động thông qua các lớp transformer dựa trên ngữ cảnh đầu vào. Sự thiếu tương tác và điều chỉnh động này có thể là một yếu tố góp phần vào việc giảm độ chính xác dự đoán quan sát được với prefix tuning.

B.4 Custom Decoding Heads + Prompt Token

Đã được chứng minh rằng một decoding head được fine-tune riêng biệt có thể dự đoán hiệu quả các token tầm xa [23,1]. Do đó, chúng tôi giả thuyết rằng việc kết hợp một decoding head được fine-tune riêng biệt với các prompt token có thể tăng cường thêm tiềm năng của PPD. Như được thể hiện trong Hình 11, chúng tôi đã đào tạo một decoding head riêng biệt để chuyển đổi chỉ các hidden state của prompt token thành logit. Một sự khác biệt chính so với Medusa là decoding head này chịu trách nhiệm tạo ra token tại nhiều vị trí, thay vì chỉ một vị trí.

Chúng tôi đề xuất hai phương pháp đào tạo. Trong phương pháp đầu tiên, custom decoding head và prompt token được đào tạo cùng nhau từ đầu trong một giai đoạn duy nhất. Trong phương pháp thứ hai, các prompt token ban đầu được đào tạo trong 2 epoch, tiếp theo là đào tạo cả prompt token và decoding head với learning rate nhỏ hơn trong một quá trình hai giai đoạn.

[Bảng so sánh độ chính xác dự đoán với và không có custom decoding head]

Bảng 5: Độ chính xác Dự đoán của PPD với và không có custom decoding head. 1 EPT được sử dụng cho tất cả mô hình. 1-stage và 2-stage đề cập đến các chiến lược đào tạo của custom decoding head.

Bảng 5 trình bày độ chính xác dự đoán của PPD với và không có custom decoding head. Khi được đào tạo bằng phương pháp một giai đoạn, PPD với custom decoding head cho thấy sự giảm 12%-21% trong độ chính xác dự đoán so với PPD cơ bản không có custom decoding head. Điều này gợi ý rằng phương pháp một giai đoạn không dẫn đến đào tạo ổn định hoặc hiệu quả.

--- TRANG 14 ---
[Hình vẽ mô tả custom decoding head với PPD]

Hình 11: Custom decoding head với PPD. Feature extractor đề cập đến các LLM không có decoding head. 'H1' là hidden state được tạo cho input token 'C'. 'H2' là hidden state cho prompt token 'S1' và 'H3' cho 'S2'. 'LM1' là decoding head gốc của LLM và nó nhận vào các hidden state của input token. 'LM2' là custom decoding head cho PPD và chỉ nhận vào các hidden state của prompt token.

Ngược lại, phương pháp đào tạo hai giai đoạn dẫn đến sự cải thiện hạn chế 2.1%-4.3% trong độ chính xác dự đoán so với baseline. Điều này gợi ý rằng việc thêm custom decoding head có thể không cần thiết, xem xét các tham số có thể đào tạo bổ sung và sự cải thiện hạn chế trong độ chính xác dự đoán.

B.5 Attention Masking cho EPT

Trong bài báo này, chúng tôi đã đề xuất một attention mask chuyên biệt cho EPT để đạt được hiệu ứng của prompt ensemble. Tuy nhiên, có các chiến lược masking thay thế có sẵn. Ở đây, chúng tôi mô tả và so sánh ba loại attention mask mà chúng tôi đã triển khai và thí nghiệm.

[Ba hình vẽ hiển thị các chiến lược mask khác nhau cho EPT]

Hình 12: Các Chiến lược Mask Khác nhau cho EPT. 'C' là một input token. 'V1' và 'V2' là các EPT cho prompt token 'S1' và 'V3' và 'V4' cho 'S2'.

B.5.1 Ensemble Attention Masking

Ensemble attention masking là chiến lược masking mà chúng tôi đã mô tả trước đó. Trong phương pháp này, các EPT được chia thành n nhóm rời rạc, trong đó n là số lượng EPT trên mỗi prompt token. Tất cả EPT thứ k trên các prompt token được đặt trong cùng một nhóm. Một EPT v trong nhóm i chỉ có thể attend đến các EPT đáp ứng hai tiêu chí sau: 1) chúng phải thuộc nhóm i, và 2) chỉ số vị trí của chúng phải nhỏ hơn chỉ số vị trí của v. Vì chiến lược masking này hiệu quả tính trung bình kết quả của các nhóm EPT rời rạc, chúng tôi gọi nó là "ensemble attention masking". Hình 12a cung cấp một ví dụ về ensemble attention masking.

B.5.2 Decoder-like Attention Masking

Decoder-like attention masking là một chiến lược đơn giản trong đó các EPT chỉ có thể attend đến các EPT với chỉ số vị trí nhỏ hơn. Điều này dẫn đến một attention mask hình tam giác, tương tự như mask được sử dụng trong các lớp decoder, do đó có tên "decoder-like attention masking". Hình 12b cung cấp một ví dụ về chiến lược masking này.

B.5.3 Encoder-like Attention Masking

Trong encoder-like attention masking, một EPT tương ứng với prompt token P có thể attend đến tất cả EPT với chỉ số vị trí nhỏ hơn cũng như tất cả EPT liên kết với P. Điều này cho phép các EPT nhìn thấy cả EPT trước và sau, tương tự như khả năng hiển thị token trong một lớp encoder, do đó có tên "encoder-like attention masking". Hình 12c minh họa chiến lược masking này.

B.5.4 Kết quả

[Bảng so sánh độ chính xác dự đoán với các chiến lược attention masking khác nhau]

Bảng 6: Độ chính xác Dự đoán của PPD với các chiến lược attention masking khác nhau cho EPT. 100 EPT được sử dụng cho tất cả mô hình.

Kết quả trong Bảng 6 cho thấy rằng ensemble attention mask vượt trội hơn các chiến lược masking khác. So sánh, PPD với decoder attention mask cho thấy độ chính xác dự đoán thấp hơn 4.9%-8.0%. PPD với encoder attention mask cũng kém hiệu suất về độ chính xác dự đoán so với ensemble attention mask từ 3.7%-7.2%.

Những kết quả này gợi ý rằng ensemble attention mask là chiến lược hiệu quả nhất trong ba chiến lược, có thể do khả năng tính trung bình hiệu quả các vote của các nhóm EPT rời rạc, do đó cải thiện độ chính xác dự đoán. Các decoder-like và encoder-like attention mask, mặc dù đơn giản hơn, không cung cấp cùng mức hiệu suất, cho thấy rằng cấu trúc và tính đặc thù của ensemble attention mask tạo điều kiện tốt hơn cho dự đoán token tầm xa chính xác. Thêm vào đó, ensemble attention masking thưa hơn, điều này cung cấp tiềm năng lớn hơn cho tối ưu hóa.

B.6 Phương pháp Tổng hợp cho EPT

Ngoài việc chỉ tính trung bình các logit từ EPT, chúng tôi đã khám phá các phương pháp tổng hợp tiên tiến hơn. Ví dụ, chúng tôi áp dụng trọng số đã học để tổng hợp các logit. Logit cuối cùng p có thể được biểu diễn như:

p = Σⁿᵢ₌₁ wᵢ·pᵢ,

trong đó n là số lượng EPT và wᵢ là trọng số scalar đã học cho EPT thứ i.

[Bảng so sánh độ chính xác dự đoán với các phương pháp tổng hợp khác nhau]

Bảng 7: Độ chính xác Dự đoán của PPD với các phương pháp tổng hợp khác nhau cho EPT. 100 EPT được sử dụng cho tất cả mô hình.

--- TRANG 15 ---
Kết quả trong Bảng 7 cho thấy độ chính xác dự đoán của PPD với hai phương pháp tổng hợp khác nhau cho EPT: tính trung bình đơn giản và trọng số đã học. Khi sử dụng trọng số đã học để tổng hợp logit, mô hình cho thấy sự giảm nhẹ 0.6%-9.4% trong độ chính xác dự đoán.

Những kết quả này gợi ý rằng mặc dù trọng số đã học cung cấp một phương pháp tổng hợp linh hoạt hơn, chúng không nhất thiết dẫn đến cải thiện độ chính xác dự đoán trong ngữ cảnh này. Tính đơn giản và ổn định của phương pháp tính trung bình dường như mang lại hiệu suất tốt hơn, có thể do độ phức tạp bổ sung và khả năng overfitting được giới thiệu bởi việc học các trọng số.

B.7 Multi-exit Ensemble

Mặc dù việc sử dụng EPT cho prompt ensemble cải thiện độ chính xác dự đoán, nó cũng tăng độ dài đầu vào, dẫn đến chi phí tính toán cao hơn và độ trễ forward pass. Để giải quyết điều này, chúng tôi đề xuất việc sử dụng phương pháp multi-exit ensemble. Trong multi-exit ensemble, các hidden state của một prompt token từ k lớp decoder cuối cùng được trích xuất và tính trung bình để tạo ra hidden state cuối cùng, sau đó được giải mã bởi decoding head thành một guess token, như được minh họa trong Hình 13. Phương pháp này đạt được prompt ensemble mà không có chi phí tính toán liên quan.

[Hình vẽ mô tả multi-exit ensemble]

Hình 13: Multi-exit ensemble. 'D1', 'D10', 'D11', và 'D12' là các lớp decoder theo thứ tự. 'S1' là một prompt token và 'H1', 'H2', 'H3' là các hidden state tương ứng từ 3 lớp decoder cuối cùng. 'H4' được thu từ việc tính trung bình 3 hidden state này. Decoding head 'LM' dịch 'H4' thành một token 'E'.

Giả thuyết là việc lấy các hidden state từ một vài lớp decoder cuối cùng cho ensemble có thể hoạt động vì các lớp này nắm bắt các biểu diễn ngày càng trừu tượng và cấp cao của chuỗi đầu vào. Bằng cách tính trung bình các hidden state từ nhiều lớp, chúng ta có thể kết hợp thông tin đa dạng nhưng bổ sung, dẫn đến một hidden state cuối cùng mạnh mẽ và chính xác hơn. Thêm vào đó, vì các lớp cuối cùng gần với đầu ra nhất, chúng có nhiều khả năng chứa thông tin được tinh chỉnh và liên quan đến ngữ cảnh, làm cho ensemble hiệu quả hơn.

[Bảng so sánh độ chính xác dự đoán với và không có multi-exit ensemble]

Bảng 8: Độ chính xác Dự đoán của PPD với và không có multi-exit ensemble. 1 EPT được sử dụng cho tất cả mô hình. k exit đề cập đến số lượng exit được sử dụng.

Bảng 8 cho thấy so sánh độ chính xác dự đoán của PPD với và không có multi-exit ensemble. Kết quả cho thấy rằng việc giới thiệu multi-exit ensemble với cả 2 và 3 exit dẫn đến sự giảm 7%-18% trong độ chính xác dự đoán so với mô hình baseline không có multi-exit.

Những phát hiện này gợi ý rằng phương pháp multi-exit ensemble, như được triển khai, không tăng cường độ chính xác dự đoán và thay vào đó dẫn đến sự giảm đáng chú ý trong hiệu suất. Điều này có thể do việc tính trung bình các hidden state từ nhiều lớp giới thiệu nhiễu hoặc giảm tính đặc thù của các biểu diễn cần thiết cho dự đoán chính xác. Cần tinh chỉnh thêm multi-exit ensemble để đạt được những cải thiện mong muốn về độ chính xác.

C Chi tiết Thí nghiệm

Đối với các thí nghiệm thông lượng, mỗi kết quả được thu bằng cách tính trung bình ba lần chạy riêng biệt. Các độ lệch chuẩn của những lần chạy này được báo cáo dưới dạng thanh lỗi trong các biểu đồ cột. Để đảm bảo so sánh công bằng trong các thí nghiệm so sánh của chúng tôi, chúng tôi duy trì các cài đặt phần cứng và phiên bản phần mềm nhất quán.

--- TRANG 16 ---
Chúng tôi đã chọn 3 prompt token vì việc thêm nhiều hơn sẽ không tăng thêm độ dài chấp nhận dự kiến do giới hạn kích thước cây. Số lượng EPT trên mỗi prompt token được tối ưu hóa để tối đa hóa thông lượng.

Trong Hình 2, các cài đặt nhiệt độ cho PPD, Eagle [16], và Medusa [1] tuân theo cấu hình mặc định, trong khi các mô hình khác sử dụng cài đặt tham lam (nhiệt độ=0). Lựa chọn này dựa trên phát hiện rằng các phương pháp dựa trên truy xuất hoạt động kém đáng kể trong cài đặt không tham lam. Tương tự, LOOKAHEAD DECODING [8], REST [9], và PLD [21] trong Hình 4 cũng sử dụng cài đặt nhiệt độ 0 vì những lý do tương tự.

D Hạn chế

Mặc dù hiệu quả, chúng tôi đã xác định những hạn chế sau của PPD:

1. Độ chính xác dự đoán thấp cho các mô hình rất nhỏ. Chúng tôi thấy rằng đối với các mô hình rất nhỏ như Vicuna-68M [24], chỉ có 2 lớp decoder và chiều embedding nhỏ hơn 1000, PPD gặp phải độ chính xác dự đoán thấp. Điều này là do chiều embedding xác định sức mạnh biểu đạt của một prompt token, và độ sâu của kiến trúc transformer rất quan trọng cho luồng thông tin hiệu quả đến các prompt token.

2. Ràng buộc tài nguyên tính toán GPU. Vì PPD đánh đổi tài nguyên tính toán bổ sung cho thông lượng tăng, hiệu quả của nó phụ thuộc vào tính khả dụng của tài nguyên tính toán GPU nhàn rỗi. Trên một GPU với tài nguyên tính toán hạn chế, tỷ lệ tăng tốc đạt được bởi PPD dự kiến sẽ giảm.

3. Độ dài đầu vào mở rộng. Sự cải thiện trong độ dài chấp nhận với PPD không đáng kể như mức tăng độ chính xác dự đoán so với Medusa. Điều này là do PPD phải dành một phần đáng kể của đầu vào cho các prompt token, điều này giới hạn kích thước cây thưa có thể được sử dụng.

E Tác động Xã hội

Trong bài báo này, chúng tôi đề xuất PPD để tăng tốc LLM một cách dễ dàng và rẻ. Vì PPD giảm thời gian cần thiết để xử lý một yêu cầu suy luận duy nhất, nó có thể làm giảm chi phí triển khai LLM cho cả các công ty và công chúng. Điều này có thể dẫn đến tăng khả năng tiếp cận của các dịch vụ LLM. Hơn nữa, các ứng dụng nhạy cảm với độ trễ như chatbot sẽ hưởng lợi rất lớn từ việc sử dụng PPD vì nó giảm độ trễ suy luận đáng kể, do đó tăng cường trải nghiệm người dùng.

Mặc dù PPD nhằm mục đích làm cho AI dễ tiếp cận hơn, vẫn có thể có sự phân chia kỹ thuật số nơi một số cộng đồng thiếu cơ sở hạ tầng cần thiết, như kết nối internet ổn định hoặc phần cứng hiện đại, để hưởng lợi đầy đủ từ những tiến bộ này. Điều này có thể làm mở rộng thêm khoảng cách giữa các nhóm dân số được ưu đãi về công nghệ và những nhóm thiếu thốn dịch vụ. Mặt khác, PPD có thể bị lạm dụng bởi các bên độc hại để thao túng đầu ra của LLM gốc, dẫn đến việc tạo ra thông tin không đáng tin cậy và dữ liệu giả mạo.
