OPT-Tree: Giải mã suy đoán với cấu trúc cây nháp thích ứng

Jikai Wang1*, Yi Su1*, Juntao Li1†,
Qingrong Xia2, Zi Ye2, Xinyu Duan2, Zhefeng Wang2, Min Zhang1,
1Viện Khoa học và Công nghệ Máy tính, Đại học Soochow, Trung Quốc
2Huawei Cloud
risus254@gmail.com, yisunlp@outlook.com
ljt@suda.edu.cn

Tóm tắt
Các mô hình ngôn ngữ tự hồi quy thể hiện hiệu suất xuất sắc trong nhiều tình huống khác nhau. Tuy nhiên, hiệu quả suy luận bị hạn chế bởi chế độ tạo từng bước một từ, điều này đã trở thành vấn đề cấp bách gần đây khi các mô hình ngày càng lớn hơn. Giải mã suy đoán sử dụng cơ chế "nháp rồi xác minh" để cho phép nhiều token được tạo trong một bước, thực hiện tăng tốc không mất mát. Các phương pháp hiện có chủ yếu áp dụng cấu trúc nháp heuristic cố định, không thể thích ứng với các tình huống khác nhau để tối đa hóa độ dài chấp nhận trong quá trình xác minh. Để giảm thiểu vấn đề này, chúng tôi đề xuất OPT-Tree, một thuật toán để xây dựng cây nháp thích ứng và có thể mở rộng. Nó tìm kiếm cấu trúc cây tối ưu để tối đa hóa kỳ vọng toán học của độ dài chấp nhận trong mỗi bước giải mã. Kết quả thực nghiệm cho thấy OPT-Tree vượt trội hơn các cấu trúc nháp hiện có và đạt được tỷ lệ tăng tốc lên đến 3.2 so với giải mã tự hồi quy. Nếu mô hình nháp đủ mạnh và ngân sách nút đủ, nó có thể tạo ra hơn mười token trong một bước duy nhất. Mã của chúng tôi có sẵn tại https://github.com/Jikai0Wang/OPT-Tree.

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLMs) (Black et al., 2022; Touvron et al., 2023; Achiam et al., 2023; Zheng et al., 2024) đã đạt được hiệu suất đáng chú ý trong nhiều tình huống NLP khác nhau. Khi các mô hình tăng về kích thước và độ phức tạp, nhu cầu tính toán cho suy luận tăng đáng kể. Do đó, việc tăng tốc giải mã để tiết kiệm chi phí tính toán ngày càng trở nên quan trọng.

Các mô hình tự hồi quy (Black et al., 2022; Zhang et al., 2022; Touvron et al., 2023) thường tạo một token trong một bước giải mã, dẫn đến hiệu quả giải mã hạn chế. Trong các nghiên cứu gần đây, giải mã suy đoán (Leviathan et al., 2023; He et al., 2023; Fu et al., 2024; Cai et al., 2024; Li et al., 2024) đã cho thấy tiềm năng lớn cho việc tăng tốc giải mã không mất mát. Nó áp dụng cơ chế "nháp rồi xác minh" để duy trì phân phối đầu ra ban đầu của mô hình đích cần được tăng tốc. Việc nháp được thực hiện bởi một mô hình nháp ít chi phí hơn. Bản nháp được tạo ra được xác minh song song bởi mô hình đích để tạo nhiều token trong một bước giải mã, mang lại sự tăng tốc đầy hứa hẹn.

Các nghiên cứu hiện có như EAGLE (Li et al., 2024) đã đề xuất các phương pháp để huấn luyện các mô hình nháp nhỏ nhưng hiệu quả. Theo hiểu biết của chúng tôi, các nghiên cứu trước đây chủ yếu áp dụng các bản nháp với cấu trúc Chuỗi hoặc cây cố định. Tuy nhiên, chúng tôi cho rằng cả hai đều không phải là cấu trúc nháp tối ưu dưới ngân sách nút hạn chế. Các bản nháp cấu trúc chuỗi (Stern et al., 2018; Leviathan et al., 2023; Xia et al., 2023; Yang et al., 2023; Zhang et al., 2023; Fu et al., 2024) chứa các nút dư thừa. Ví dụ, "A-B-C-D-E" và "A-B-C-F-G" có cùng tiền tố "A-B-C", được tính toán hai lần trong quá trình xác minh. Do đó, chỉ có 7 token hợp lệ trong số 10 nút của hai chuỗi này. Các bản nháp với cấu trúc cây (He et al., 2023; Cai et al., 2024; Li et al., 2024; Jeon et al., 2024; Chen et al., 2024) đã giải quyết vấn đề này. Cùng một token chỉ có thể xuất hiện một lần trong cùng một lớp cây. Một mặt nạ attention cây tương ứng được thiết kế cho việc xác minh song song. Cấu trúc cụ thể của cây thường là heuristic và không đổi. Tuy nhiên, với một ngân sách nút nhất định, cấu trúc tốt nhất để tối đa hóa độ dài chấp nhận trong quá trình xác minh sẽ thay đổi theo các đầu vào khác nhau trong mỗi bước giải mã.

Bài báo này đề xuất một cấu trúc cây thích ứng và có thể mở rộng được gọi là OPT-Tree. Nó có thể được áp dụng cho bất kỳ mô hình nháp tự hồi quy nào. Như được thể hiện trong Hình 1, cấu trúc cây thích ứng thay đổi trong mỗi bước giải mã để tối đa hóa kỳ vọng toán học của độ dài chấp nhận. Chúng tôi áp dụng một thuật toán tham lam để xây dựng OPT-Tree trong mỗi bước. Chi tiết được trình bày trong Phần 3. Chúng tôi thực hiện các thí nghiệm toàn diện trong Phần 4 để đánh giá hiệu quả của OPT-Tree. Kết quả thực nghiệm cho thấy OPT-Tree vượt trội hơn các baseline và có thể nhanh hơn đến 3.2 lần so với giải mã tự hồi quy vanilla. Kỳ vọng toán học của độ dài chấp nhận thường có tương quan dương với độ dài chấp nhận thực tế trong thực tế. Hơn nữa, OPT-Tree hoạt động tốt khi kích thước cây tăng lên. Sử dụng LLaMA-2-7B làm mô hình nháp, LLaMA-2-7B có thể tạo 10 token trong một bước giải mã duy nhất với OPT-Tree khi số lượng nút vượt quá 500, điều này cho thấy tiềm năng lớn để thích ứng với tài nguyên tính toán mạnh hơn và các mô hình nháp hiệu quả hơn trong tương lai.

2 Khái niệm cơ bản
Chúng tôi cung cấp các định nghĩa cần thiết trong phần này.

Suy luận. Sau khi nhập x = (x1, x2, ..., xl), trong đó l là độ dài chuỗi hiện tại, mô hình đích M và mô hình nháp Md trả về phân phối từ tiếp theo p(yl+1|x1, x2, ..., xl) và pd(ŷl+1|x1, x2, ..., xl) tương ứng, trong đó yl+1 và ŷl+1 là các từ tiếp theo được lấy mẫu.

Giải mã suy đoán. Trong giải mã suy đoán với nháp cấu trúc cây, Md đầu tiên suy luận d bước để tạo một cây nháp T có độ sâu d và sau đó M xác minh bản nháp. Việc xác minh phụ thuộc vào phương pháp lấy mẫu. Đối với lấy mẫu tham lam, sự thật cơ sở là chuỗi các token có xác suất cao nhất cho mỗi vị trí được đầu ra bởi M. Đối với tất cả các nhánh trong cây chứa nút gốc, nhánh dài nhất có cùng tiền tố với sự thật cơ sở được chấp nhận. Do đó, nhiều token có thể được tạo trong một bước giải mã trong khi đảm bảo rằng các chuỗi được tạo nhất quán với các chuỗi ban đầu.

3 OPT-Tree
Phần này giới thiệu OPT-Tree, một thuật toán để xây dựng cấu trúc cây nháp tối ưu đã định nghĩa của chúng tôi cho bất kỳ chuỗi đầu vào nào trong giải mã suy đoán với các mô hình nháp tự hồi quy.

Cây nháp T được định nghĩa như sau:
T = (V, E)
V = ⋃(i=l+1 to l+d) ⋃(j=1 to ni) (ŷij, p̂ij),    (1)

trong đó V và E là tập hợp của tất cả các nút và cạnh. ni biểu thị số lượng token được lấy mẫu trong lớp thứ i của T. p̂ij được tính bởi:

p̂ij = ∏(ŷ∈P(ŷij)) pd(ŷ),    (2)

trong đó P(ŷij) là tập hợp của tất cả các nút cha của ŷij (bao gồm chính nó). p̂ij của nút gốc được coi là dương vô cùng. Đối với mỗi nút trong T, nếu nó có k con, chúng là k token được lấy mẫu tham lam theo pd từ phân phối token tiếp theo của nó. Mục đích của việc tính p̂ là để đơn giản hóa các hoạt động tiếp theo.

Định lý 3.1. Đối với hai nút vi và vj bất kỳ trong cây, nếu vi là một nút trong cây con của vj, thì p̂ của vi nhỏ hơn p̂ của vj.

Xem xét một bước nhất định trong giải mã suy đoán có đầu vào là x, mô hình nháp Md tạo một cây nháp dựa trên x và cấu trúc cây T đã cho. Sau đó, mô hình đích nhập cây nháp và mặt nạ attention cây tương ứng và trả về các token tiếp theo của mỗi token trong T. Chúng ta có được ứng viên được chấp nhận dài nhất với độ dài A bằng cách so sánh các token tiếp theo và cây nháp. Cho M, Md và n, đối với đầu vào x, một cấu trúc cây tối ưu Topt nên tối đa hóa kỳ vọng toán học của độ dài chấp nhận E(A). Lưu ý rằng Topt thay đổi khi đầu vào thay đổi. Vì mục tiêu tối ưu hóa của mô hình nháp là làm cho phân phối đầu ra của nó gần với phân phối mô hình đích, đối với mỗi nút, p̂ sẽ có mối quan hệ tích cực với xác suất được chấp nhận của nó trong quá trình xác minh khi sử dụng mô hình nháp hiệu quả cho giải mã suy đoán. Do đó, E(A) có thể được tính gần đúng bởi p̂:

E(A) = ∑((ŷij,p̂ij)∈T) ∏(ŷ∈P(ŷij)) pd(ŷ)
     = ∑((ŷij,p̂ij)∈T) p̂ij.    (3)

Hình 2 cho thấy một ví dụ đơn giản về việc tính p̂ và E(A). E(A) nên có tương quan dương với độ dài chấp nhận. Chúng tôi thảo luận về mối tương quan của chúng trong Phần 4.2.

Chúng tôi sử dụng Esub(T, n) để biểu thị giá trị tối đa của E(A) cho tất cả các cây con của T chứa nút gốc và có n nút. Lưu ý rằng nút gốc không được xem xét khi tính cây nút và kỳ vọng toán học.

Sau đó, chúng tôi đề xuất Thuật toán 1 để xây dựng Topt trong giai đoạn nháp cho mỗi bước giải mã. Chúng tôi khởi tạo T với một nút gốc. Tại mỗi bước nháp, chúng tôi lấy mẫu tham lam n token với p̂ lớn nhất trong phân phối token tiếp theo của các nút trong lớp cuối cùng của T để xây dựng lớp tiếp theo. T có d*n nút tại thời điểm này. Cuối cùng, chúng tôi chọn n nút trong T với p lớn nhất. Dễ dàng chứng minh rằng n nút này là một cây con của T, chứa nút gốc:

Chứng minh. (1) Nếu các nút này không thể tạo thành một cây với gốc, có ít nhất một nút vi mà nút cha vj không nằm trong số các nút này. (2) Theo Định lý 3.1, p̂ của vj lớn hơn p̂ của vi. Do đó, vj cũng được chọn. (1) và (2) mâu thuẫn, vì vậy các nút này phải có thể tạo thành một cây con của T chứa nút gốc.

Thuật toán 1 Xây dựng OPT-Tree Topt
Đầu vào: Chuỗi đầu vào x = (x1, x2, ..., xl), mô hình nháp Md, số lượng nút n, ngưỡng δ.
Đầu ra: Một cây nháp Topt.
Khởi tạo một cây T với nút gốc xl
E ← 0
Phân phối đầu ra Pd(T) ← Md(T)
T ← topk(Pd(T), n)
while Độ sâu của cây D(T) < n và Esub(T, n) - E > δ do
    //Bước nháp
    E ← Esub(T, n)
    Phân phối đầu ra Pd(T) ← Md(T)
    T ← topk(Pd(T), n)
end while
Topt ← Chọn n nút với p̂ lớn nhất từ T

Định lý 3.2. Khi bước nháp tăng, Esub(T, n) là đơn điệu không giảm.

Thuật toán 2 Giải mã suy đoán với cấu trúc cây nháp thích ứng
Đầu vào: Chuỗi đầu vào x = (x1, x2, ..., xl), mô hình đích M, mô hình nháp Md, số lượng nút n, ngưỡng δ.
Đầu ra: Chuỗi đầu vào mới x' = (x1, x2, ..., xl+A)
Topt ← Xây dựng cây nháp với n nút
mask ← Tính mặt nạ attention cây tương ứng
P ← M(Topt, mask)
(yl+1, yl+2, ..., yl+A) ← Verify(Topt, P)
//Tìm ứng viên được chấp nhận dài nhất. Nếu một chuỗi có độ dài A-1 thành công, từ tiếp theo của nó cũng sẽ được chấp nhận. Vì vậy, tổng độ dài chấp nhận là A.
x' ← Concat(x, (yl+1, yl+2, ..., yl+A))

Theo Định lý 3.2, chúng ta có thể có được Topt mong muốn về mặt lý thuyết bằng cách dừng nháp khi E(T) không còn tăng. Tuy nhiên, mô hình nháp mang lại chi phí bổ sung trong thực tế. Đối với các mô hình nháp tự hồi quy, chi phí nháp tỷ lệ thuận với độ sâu của cây nháp. Xem xét điều này, chúng tôi giới thiệu một ngưỡng δ khi đặt điều kiện để chấm dứt nháp. Giá trị của δ nên được kiểm soát giữa μ và 1, trong đó μ là thời gian của một bước nháp chia cho thời gian của một bước giải mã.

Một bước giải mã hoàn chỉnh của M được thể hiện trong Thuật toán 2. Trong thực tế, cả M và Md đều sử dụng bộ nhớ cache key và value để tính attention. Do đó, độ dài đầu vào thực tế của mỗi bước nháp là n, điều này tránh các nút thắt cổ chai tính toán trong suy luận của mô hình nháp dưới ngân sách lớn hơn của kích thước cây.

4 Thí nghiệm
4.1 Kết quả chính

Thiết lập. Chúng tôi áp dụng LLaMA-2-7B, LLaMA-2-13B, LLaMA-2-70B (Touvron et al., 2023) và Vicuna-33B (Zheng et al., 2024) làm mô hình đích để xác minh hiệu quả của OPT-Tree. Chúng tôi sử dụng một GPU GeForce RTX 4090 duy nhất cho LLaMA-2-7B, một GPU L20 duy nhất cho LLaMA-2-13B và 4 GPU A100-PCIE-40GB cho LLaMA-2-70B và Vicuna-33B. Chúng tôi chọn một hoặc hai mô hình nhỏ hơn trong cùng phiên bản làm mô hình nháp cho mỗi mô hình đích. Hơn nữa, chúng tôi áp dụng một mô hình nháp EAGLE tương ứng cho mỗi mô hình đích. Nhiệt độ được đặt thành không. EAGLE (Li et al., 2024) là một phương pháp giải mã suy đoán hiệu quả huấn luyện các đầu tự hồi quy bổ sung làm mô hình nháp. Nó sử dụng một cấu trúc cây nháp heuristic được thiết kế tốt với 25 nút. Trong các thí nghiệm của chúng tôi, chúng tôi coi nó là cây nháp EAGLE. EAGLE được chứng nhận bởi Xia et al. (2024) là phương pháp suy đoán nhanh nhất trong các thí nghiệm của họ. Đối với mỗi nhóm mô hình đích và nháp, chúng tôi thực hiện giải mã suy đoán với lấy mẫu tham lam và so sánh OPT-Tree với cây Binary và cây EAGLE.

Chúng tôi so sánh độ dài chấp nhận trung bình và số lượng token được tạo mỗi giây khi giải mã với các cấu trúc cây khác nhau. Tỷ lệ tăng tốc được tính theo tốc độ tạo. Ngân sách nút được xác định bởi mô hình đích và tài nguyên tính toán vì thời gian suy luận thường giữ nguyên trong một độ dài đầu vào nhất định. Hình 3 hiển thị thời gian suy luận cho đầu vào với các độ dài khác nhau cho 4 mô hình đích được sử dụng trong các thí nghiệm. Số lượng nút cần được kiểm soát trong một phạm vi nhất định để tránh tiêu thụ thời gian quá mức trong giai đoạn xác minh. Nó được coi là một siêu tham số được chọn trong [25,50,60] để tối đa hóa tỷ lệ tăng tốc theo các mô hình đích và tài nguyên GPU khác nhau ngoại trừ cây EAGLE. Chúng tôi tiến hành đánh giá trên MT-Bench (Zheng et al., 2024) và GSM8K (Cobbe et al., 2021).

Kết quả. Kết quả thực nghiệm được thể hiện trong Bảng 1 và Bảng 2. Lưu ý rằng việc sử dụng LLaMA-2-1B làm mô hình nháp hầu như không thể tăng tốc giải mã khi mô hình đích là LLaMA-2-7B vì sự khác biệt về thời gian suy luận giữa hai mô hình quá nhỏ. Các mô hình nháp EAGLE đạt được hiệu suất mạnh với ít tham số hơn, do đó cung cấp tăng tốc tốt hơn so với các mô hình nhỏ trong cùng series với các mô hình đích. OPT-Tree vượt trội hơn các cấu trúc cây khác về độ dài chấp nhận trung bình trong mỗi nhóm thí nghiệm, đặc biệt khi hiệu suất của mô hình nháp gần với mô hình đích (ví dụ: LLaMA-2-70B kết hợp với L-7B và Vicuna-33B kết hợp với Vicuna-7B), cho thấy giới hạn trên cao của nó. Vì OPT-Trees thường sâu hơn các cây nhị phân và cây EAGLE, chúng phát sinh nhiều chi phí hơn khi nháp. Do đó, từ góc độ token mỗi giây, sự cải thiện không đáng kể như từ độ dài chấp nhận trung bình. Token mỗi giây bị ảnh hưởng bởi các tài nguyên phần cứng khác nhau và lỗi ngẫu nhiên. Ngoài ra, một số kỹ thuật độc lập với phương pháp cũng có thể được sử dụng để giảm thời gian tính toán. Ví dụ, phần không thay đổi của mặt nạ attention trong giai đoạn nháp có thể được khởi tạo chỉ một lần và gọi nhiều lần, do đó tiết kiệm thời gian khởi tạo nhiều lần. Để so sánh công bằng hơn trong các thí nghiệm của chúng tôi, chúng tôi tránh những thủ thuật này để nhất quán với thực hành của EAGLE. Nhìn chung, OPT-Tree vượt trội hơn các baseline. Nó có thể nhanh hơn khoảng 3.2 lần so với giải mã tự hồi quy vanilla. Hiệu suất tương tự trên cả hai bộ dữ liệu xác minh tính mạnh mẽ của phương pháp được đề xuất.

4.2 Tương quan giữa E(A) và A
Lý thuyết của OPT-Tree dựa trên tiền đề rằng E(A) có tương quan dương với A thực tế. Chúng tôi ghi lại các giá trị của E(A) và A của OPT-Tree trong khoảng 8000 bước giải mã cho 4 nhóm M và Md. Hình 4 cho thấy kết quả. Giá trị của E(A) được làm tròn. Các vùng tối hơn trong bốn hình cơ bản được phân bố dọc theo đường chéo chính. Khi E(A) của cây lớn hơn, nó cũng có xu hướng có được độ dài chấp nhận đáng kể hơn sau khi xác minh. Một mô hình nháp mạnh hơn chuyển phân phối đến góc phải dưới. Những hiện tượng này chứng thực phân tích lý thuyết của chúng tôi. Ngoài ra, trong nhóm LLaMA-2-70B+LLaMA-2-7B, các giá trị cao của E(A) và A (ví dụ: E(A) = 14, A = 15) thường được tìm thấy, điều này chứng minh tiềm năng của OPT-Tree để thích ứng với các mô hình nháp mạnh hơn và kích thước cây nháp lớn hơn.

4.3 Mở rộng kích thước cây nháp
Chúng tôi tiến hành thí nghiệm để khám phá những thay đổi trong độ dài chấp nhận trung bình với kích thước cây lớn hơn. Chúng tôi so sánh OPT-Tree với Sequoia (Chen et al., 2024) sử dụng LLaMA-2-7B và LLaMA-2-70B làm mô hình đích. Sequoia là một cây nháp có thể mở rộng sử dụng lập trình động để giải quyết cấu trúc cây. Nó yêu cầu các mô hình đích và nháp được sử dụng trước để suy luận một số mẫu để xác định cấu trúc tốt nhất. Cấu trúc cây được cố định khi thực hiện giải mã suy đoán. Chúng tôi sử dụng 200 mẫu trong C4 (Raffel et al., 2020) để xây dựng các cây Sequoia. Nhiệt độ được đặt thành 0 trong các thí nghiệm.

Kết quả thực nghiệm được thể hiện trong Hình 5. OPT-Tree vượt trội hơn Sequoia dưới các kích thước cây khác nhau. Đối với LLaMA-2-7B+LLaMA-2-68M, độ dài chấp nhận trung bình với cả OPT-Tree và Sequoia tăng nhanh khi số lượng nút nhỏ hơn 130. Khi số lượng nút vượt quá 140, độ dài chấp nhận trung bình tăng chậm. Đối với LLaMA-2-70B+LLaMA-2-7B, sự tăng trưởng của độ dài chấp nhận trung bình với Sequoia có xu hướng phẳng khi số lượng nút vượt quá 150. Tuy nhiên, OPT-Tree có thể tiếp tục cải thiện độ dài chấp nhận trung bình ngay cả khi số lượng nút vượt quá 500. Vì LLaMA-2-7B là một mô hình nháp mạnh cho LLaMA-2-70B, độ dài chấp nhận trung bình có thể đạt 10 với OPT-Tree có 500 nút. Một cây với 500 nút tiêu tốn một lượng lớn thời gian tính toán cho LLaMA-2-70B với GPU A100-PCIE-40GB, do đó không thể tăng tốc giải mã trong thực tế của chúng tôi. Tuy nhiên, chi phí này có thể chấp nhận được nếu các tài nguyên tính toán mạnh hơn được trang bị trong tương lai.

4.4 Tác động của ngưỡng
Xem xét chi phí của mô hình nháp tỷ lệ thuận với độ sâu của cây, cây tối đa hóa độ dài chấp nhận không nhất thiết có tỷ lệ tăng tốc cao nhất. Do đó, chúng tôi thực nghiệm để nghiên cứu độ dài chấp nhận trung bình và token/s dưới các ngưỡng khác nhau.

Hình 6 cho thấy kết quả thực nghiệm trên LLaMA-2-7B. Độ dài chấp nhận trung bình giảm khi ngưỡng tăng khi sử dụng LLaMA-2-68M làm mô hình nháp. Tuy nhiên, có một dao động nhẹ đối với mô hình nháp EAGLE. Điều này là do E(A) và A không hoàn toàn tương đương. Chúng tôi tính μ cho mỗi nhóm mô hình, đó là thời gian của một bước nháp chia cho thời gian của một bước giải mã. Một ngưỡng quá lớn sẽ giảm độ sâu của cây, do đó giảm giá trị của A. Mặt khác, một ngưỡng quá nhỏ có thể làm cho cây quá sâu và tăng chi phí nháp. Khi độ sâu của cây tăng một nhưng sự gia tăng của E(A) không vượt quá μ, việc tăng độ sâu không đáng giá. Vì vậy, chúng tôi đặt một ngưỡng giữa μ và 1 trong thực tế. LLaMA-2-68M và EAGLE đạt được tăng tốc cao nhất khi δ = 0.2 và δ = 0.8, tương ứng.

4.5 Hiệu suất trong các thiết lập không tham lam
Trong thiết lập giải mã của lấy mẫu không tham lam (lấy mẫu ngẫu nhiên), chúng tôi chỉ sửa đổi các token có thể chấp nhận được trong giai đoạn xác minh. Chúng tôi tiến hành thí nghiệm để đánh giá OPT-Tree trên các thiết lập không tham lam này, trong đó nhiệt độ vượt quá 0. Chúng tôi thực hiện giải mã suy đoán với OPT-Tree trên bộ dữ liệu MT-Bench cho tất cả các nhóm mô hình trong 4.1 với nhiệt độ đặt thành 1. Bảng 3 hiển thị kết quả thực nghiệm. Độ dài chấp nhận trung bình và tỷ lệ tăng tốc của giải mã suy đoán với OPT-Tree thấp hơn một chút khi nhiệt độ được đặt thành 1 so với khi nhiệt độ được đặt thành 0. Vì cây nháp lấy mẫu tham lam các token với xác suất cao hơn, mối tương quan dương giữa E(A) và A sẽ bị suy yếu trong việc giải mã lấy mẫu ngẫu nhiên. Do đó, việc tăng tốc của giải mã suy đoán giảm khi nhiệt độ lớn hơn 0 là điển hình. Hình 7 cho thấy những thay đổi cụ thể trong độ dài chấp nhận trung bình và token/s với các giá trị nhiệt độ khác nhau. Cả hai chỉ số đều giảm khi nhiệt độ tăng nói chung. Nhưng ngay cả khi nhiệt độ được đặt thành 1, opt-tree vẫn có thể cung cấp tăng tốc cao so với giải mã tự hồi quy vanilla.

4.6 Nghiên cứu trường hợp
Chúng tôi thể hiện một ví dụ về giải mã suy đoán với OPT-Tree có 50 nút trên LLaMA-2-70B với LLaMA-2-7B làm mô hình nháp trong Hình 8. Ngưỡng là 0.7, và nhiệt độ là 0. Độ dài chấp nhận trung bình là 9.34, và tốc độ tạo là 12.07 token mỗi giây. Hầu hết các từ (văn bản màu xanh) được tạo bởi mô hình nháp và sau đó được xác minh bởi mô hình đích. Mỗi cặp từ màu đỏ và văn bản màu xanh liên tục phía trước nó được tạo trong một bước giải mã duy nhất của mô hình đích. Sự xuất hiện của các từ màu đỏ là do độ sâu của cây nháp bị hạn chế hoặc vì không có ứng viên nào cho vị trí này trúng đích. Các giới từ (ví dụ: in, for và with), liên từ (ví dụ: and và or), mạo từ (ví dụ: a và the), dấu câu và các từ khác không có ý nghĩa thực tế rõ ràng dễ bị thiếu trong bản nháp. Ngoài ra, phần đầu của các câu mới trong bản nháp có xu hướng bị từ chối vì nó không có liên kết tuần tự chắc chắn với từ trước đó.

5 Nghiên cứu liên quan
Giải mã suy đoán (Stern et al., 2018; Xia et al., 2023; Leviathan et al., 2023; Chen et al., 2023a) tăng tốc giải mã tự hồi quy bằng cách nháp và sau đó xác minh trong khi đảm bảo đầu ra nhất quán. Các phương pháp nháp chủ yếu được chia thành nháp độc lập và tự nháp. Nháp độc lập tận dụng một mô hình bên ngoài chi phí thấp. SpecDec (Xia et al., 2023) huấn luyện một mô hình không tự hồi quy cho việc nháp trong khi những người khác (Leviathan et al., 2023; Chen et al., 2023a; Spector and Re, 2023; Chen et al., 2023b, 2024) trực tiếp sử dụng một phiên bản nhỏ hơn của mô hình đích. Ngoài ra, REST (He et al., 2023) đề xuất một phương pháp nháp dựa trên truy xuất. Tự nháp sử dụng thông tin ban đầu của mô hình đích để nháp. Yang et al. (2023) áp dụng một cơ chế thoát sớm cho việc nháp. Tương tự, Zhang et al. (2023) thực hiện bỏ qua lớp thích ứng trong giai đoạn nháp. Lookahead Decoding (Fu et al., 2024) thiết kế một thuật toán cho việc nháp và xác minh song song. MEDUSA (Cai et al., 2024) huấn luyện nhiều đầu giải mã để có được các ứng viên cho nhiều bước từ các tính năng ban đầu song song. Xem xét rằng các kết quả lấy mẫu khác nhau tại mỗi bước trong việc nháp sẽ ảnh hưởng đến phân phối của các đầu ra tiếp theo, EAGLE (Li et al., 2024) thiết kế một đầu tự hồi quy, giới thiệu embedding của mỗi từ trong giai đoạn nháp.

Phương pháp xác minh đã phát triển từ xác minh có cấu trúc chuỗi sang xác minh có cấu trúc cây. Các nghiên cứu sớm (Stern et al., 2018; Leviathan et al., 2023; Xia et al., 2023; Yang et al., 2023; Zhang et al., 2023; Fu et al., 2024) xác minh các bản nháp dưới dạng một hoặc nhiều chuỗi. Tuy nhiên, khi số lượng token xác minh tăng, có một số lượng lớn các trùng lặp tiền tố giữa các chuỗi, dẫn đến các tính toán dư thừa. Để giảm thiểu vấn đề này, các nghiên cứu gần đây (He et al., 2023; Cai et al., 2024; Li et al., 2024; Jeon et al., 2024) sử dụng các bản nháp có cấu trúc cây heuristic và thiết kế các mặt nạ attention tương ứng cho việc xác minh song song. Chen et al. (2024) đề xuất Sequoia, một thuật toán để xây dựng cây nháp, hoạt động tốt khi kích thước cây tăng lên.

6 Kết luận
Trong bài báo này, chúng tôi đề xuất một phương pháp mới và hiệu quả được gọi là OPT-Tree để xây dựng cấu trúc cây nháp thích ứng cho giải mã suy đoán. OPT-Tree tối đa hóa kỳ vọng toán học của độ dài chấp nhận dưới bất kỳ kích thước cây nháp hạn chế nào. Kết quả thực nghiệm với mười nhóm mô hình đích và mô hình nháp trên hai bộ dữ liệu cho thấy opt-tree vượt trội hơn các cấu trúc nháp hiện có. Nó đạt được tăng tốc không mất mát lên đến 3.2 lần so với giải mã tự hồi quy vanilla và cho thấy tính mạnh mẽ trên các bộ dữ liệu khác nhau và với các nhiệt độ khác nhau. Ngoài ra, nếu được trang bị một mô hình nháp mạnh, độ dài chấp nhận trung bình với OPT-Tree tiếp tục tăng ngay cả khi số lượng nút vượt quá 500, chứng minh tiềm năng lớn của nó để thích ứng với các tình huống có tài nguyên tính toán mạnh hơn.

Hạn chế
Các tài nguyên phần cứng và môi trường khác nhau sẽ ảnh hưởng đến tốc độ thông lượng được báo cáo trong các thí nghiệm trong bài viết này. Các thí nghiệm trong bài báo này áp dụng cùng một khung giải mã như EAGLE (Li et al., 2024) để so sánh công bằng. Trong thực tế, thuật toán giải mã có thể được tối ưu hóa từ các góc độ khác để cải thiện thêm tốc độ giải mã, điều này không được khám phá trong bài báo này.

Tài liệu tham khảo
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.

Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An open-source autoregressive language model. In Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models, pages 95–136, virtual+Dublin. Association for Computational Linguistics.

Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee, Deming Chen, and Tri Dao. 2024. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774.

Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. 2023a. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318.

Zhuoming Chen, Avner May, Ruslan Svirschevski, Yuhsun Huang, Max Ryabinin, Zhihao Jia, and Beidi Chen. 2024. Sequoia: Scalable, robust, and hardware-aware speculative decoding. arXiv preprint arXiv:2402.12374.

Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun, Jie Huang, and Kevin Chen-Chuan Chang. 2023b. Cascade speculative drafting for even faster llm inference. arXiv preprint arXiv:2312.11462.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.

Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. 2024. Break the sequential dependency of llm inference using lookahead decoding. arXiv preprint arXiv:2402.02057.

Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee, and Di He. 2023. Rest: Retrieval-based speculative decoding. arXiv preprint arXiv:2311.08252.

Wonseok Jeon, Mukul Gagrani, Raghavv Goel, Junyoung Park, Mingu Lee, and Christopher Lott. 2024. Recursive speculative decoding: Accelerating LLM inference via sampling without replacement. In ICLR 2024 Workshop on Large Language Model (LLM) Agents.

Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 19274–19286. PMLR.

Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. 2024. Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1–67.

Benjamin Spector and Chris Re. 2023. Accelerating llm inference with staged speculative decoding. arXiv preprint arXiv:2308.04623.

Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. 2018. Blockwise parallel decoding for deep autoregressive models. Advances in Neural Information Processing Systems, 31.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.

Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu Wei, and Zhifang Sui. 2023. Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 3909–3925.

Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. 2024. Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding. arXiv preprint arXiv:2401.07851.

Seongjun Yang, Gibbeum Lee, Jaewoong Cho, Dimitris Papailiopoulos, and Kangwook Lee. 2023. Predictive pipelined decoding: A compute-latency trade-off for exact llm decoding. arXiv preprint arXiv:2307.05908.

Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, and Sharad Mehrotra. 2023. Draft & verify: Lossless large language model acceleration via self-speculative decoding. arXiv preprint arXiv:2309.08168.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36.
