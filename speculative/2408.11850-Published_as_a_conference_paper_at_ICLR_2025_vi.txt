# 2408.11850.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/speculative/2408.11850.pdf
# Kích thước tệp: 649973 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Xuất bản như một bài báo hội nghị tại ICLR 2025
PEARL: GIẢI MÃ ĐỜN ĐỊN SONG SONG VỚI
ĐỘ DÀI BẢN THẢO THÍCH ỨNG
Tianyu Liu1,3*Yun Li2†Qitan Lv1Kai Liu2Jianchen Zhu2Winston Hu2Xiao Sun3†
1Đại học Khoa học và Công nghệ Trung Quốc
2Tencent
3OpenGVLab, Phòng thí nghiệm AI Thượng Hải
{tianyu liu, qitanlv }@mail.ustc.edu.cn
yunli.charles@gmail.com
{raccoonliu, dickzhu, winstony }@tencent.com
sunxiao@pjlab.org.cn
TÓM TẮT
Giải mã đón đoán (SD), nơi một mô hình thảo luận bổ sung được sử dụng để cung cấp
nhiều token thảo luận trước, và sau đó mô hình mục tiêu ban đầu xác minh các token này
song song, đã cho thấy sức mạnh lớn trong việc tăng tốc suy luận LLM. Tuy nhiên, các
phương pháp SD hiện tại gặp phải vấn đề chờ đợi lẫn nhau, tức là mô hình mục tiêu bị
kẹt khi mô hình thảo luận đang đoán token, và ngược lại. Vấn đề này được gây ra trực
tiếp bởi việc thực thi không đồng bộ của mô hình thảo luận và mô hình mục tiêu và được
làm trầm trọng thêm do độ dài thảo luận cố định trong giải mã đón đoán. Để giải quyết
những thách thức này, chúng tôi đề xuất một framework đơn giản về mặt khái niệm, linh
hoạt và tổng quát để thúc đẩy giải mã đón đoán, cụ thể là Giải mã đón đoán song song với
Độ dài thảo luận thích ứng (PEARL). Cụ thể, PEARL đề xuất pre-verify để xác minh token
thảo luận đầu tiên trước trong giai đoạn thảo luận, và post-verify để tạo thêm token thảo
luận trong giai đoạn xác minh. PEARL song song hóa giai đoạn thảo luận và giai đoạn xác
minh thông qua việc áp dụng hai chiến lược này, và đạt được độ dài thảo luận thích ứng
cho các tình huống khác nhau, điều này giảm thiểu hiệu quả vấn đề chờ đợi lẫn nhau. Các
thí nghiệm trên nhiều benchmark tạo văn bản khác nhau chứng minh hiệu quả của PEARL
của chúng tôi, dẫn đến hiệu suất tăng tốc vượt trội lên tới 4.43× và 1.50×, so với giải mã
tự hồi quy và giải mã đón đoán vanilla, tương ứng. Mã của chúng tôi có sẵn tại https:
//github.com/smart-lty/ParallelSpeculativeDecoding .

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLM) như GPT-4, LlaMA và DeepSeek (Achiam et al., 2023;
DeepSeek-AI, 2024; Bommasani et al., 2021; Touvron et al., 2023) đã thống trị việc hiểu và tạo
ngôn ngữ tự nhiên (Khurana et al., 2023) trên một loạt ứng dụng rộng. Tuy nhiên, độ trễ suy luận
đáng kể của những LLM này đã nổi lên như một trở ngại lớn giới hạn ứng dụng rộng rãi hơn của
chúng trong các tình huống có tài nguyên tính toán hạn chế. Độ trễ này chủ yếu bắt nguồn từ quá
trình giải mã token-by-token tự hồi quy trong đó việc giải mã K token đòi hỏi K lần chạy nối tiếp
của LLM, gây ra độ trễ trầm trọng với cả độ dài của token được tạo và quy mô mô hình.

Để giải quyết thách thức này, các nỗ lực nghiên cứu rộng rãi đã được dành để tăng tốc suy luận
LLM. Cho rằng suy luận từ các mô hình lớn thường bị ràng buộc nhiều hơn bởi băng thông bộ nhớ
và giao tiếp hơn là các phép toán số học Leviathan et al. (2023), một mô hình suy luận sáng tạo,
Giải mã Đón đoán (SD), đã nổi lên như một xu hướng mới và cho thấy hiệu suất vượt trội bằng
cách cho phép sử dụng GPU tốt hơn một cách hiệu quả. Như được hiển thị trong phần trên của
Hình 1, ý tưởng chính của thuật toán SD là sử dụng một mô hình nhỏ bổ sung (được gọi là mô hình
thảo luận) để tạo gamma token thảo luận đầu tiên

*Công việc này được thực hiện khi Tianyu Liu làm thực tập sinh tại Tencent.
†Các tác giả tương ứng.
1arXiv:2408.11850v3  [cs.CL]  17 Feb 2025

--- TRANG 2 ---
Xuất bản như một bài báo hội nghị tại ICLR 2025
Tiền tố: Trong lĩnh vực giải mã đón đoán, PEARL là một
Nhanh lên! Tôi đang chờ!
 Bước 1 D: thuật toán không phụ thuộc mô hình
Nhanh lên! Tôi đang chờ!
 Bước 2 V: thuật toán không phụ thuộc mô hình song song
Nhanh lên! Tôi đang chờ!
 Bước 3 D: phương pháp tăng tốc suy luận
Nhanh lên! Tôi đang chờ!
 Bước 4 V: phương pháp tăng tốc suy luận mà
Bước 1 Pre: song song
 Bước 1 Pre: thuật toán không phụ thuộc mô hình
Bước 2 Pre: phương pháp tăng tốc suy luận
 Bước 2 Pre: Suy luận
Bước 3 Post: phương pháp tăng tốc suy luận
 Bước 3 Post: có thể đạt được
Bước 4 Post: độ dài thảo luận thích ứng
 Bước 4 Post: có thể đạt được PEARL là thuật toán không phụ thuộc mô hình song song phương pháp tăng tốc suy luận mà
PEARL là thuật toán không phụ thuộc mô hình song song phương pháp tăng tốc suy luận có thể đạt được độ dài thảo luận thích ứng

Hình 1: Tổng quan về giải mã đón đoán (phần trên) và PEARL của chúng tôi (phần dưới).
SD sử dụng mô hình thảo luận để cung cấp nhiều bản thảo và sau đó mô hình mục tiêu xác minh
các bản thảo song song. Tuy nhiên, SD gặp phải vấn đề chờ đợi lẫn nhau, tức là mô hình mục tiêu
bị kẹt khi mô hình thảo luận đang đoán token, và ngược lại (hộp thoại đứt nét). PEARL song song
hóa quá trình thảo luận và xác minh để giảm thiểu vấn đề chờ đợi lẫn nhau. Hơn nữa, PEARL có
thể tận dụng độ dài thảo luận thích ứng để tạo thêm token trong cùng một khoảng thời gian để tiếp
tục giảm thiểu vấn đề chờ đợi lẫn nhau. Cụ thể, PEARL tạo ít token thảo luận hơn nếu chúng sẽ bị
từ chối (bước 1 trong phần dưới), và nhiều token thảo luận hơn nếu chúng có thể được chấp nhận
(bước 3 và 4).

token thảo luận cho mô hình lớn ban đầu (được gọi là mô hình mục tiêu), và sau đó mô hình mục
tiêu xác minh các token thảo luận này song song trong một lần forward duy nhất. Ở đây, gamma là
một tham số siêu kích thước cửa sổ cố định. Độ dài thảo luận là số lượng token được tạo bởi mô
hình thảo luận trong một lần thực thi liên tục. Do đó, độ dài thảo luận được đặt thành gamma trong
SD. Các công trình tiếp theo mở rộng hiệu quả framework này bằng cách loại bỏ sự cần thiết của
mô hình thảo luận (Cai et al., 2024; Fu et al., 2024; Zhang et al., 2023) hoặc xác định một mô hình
thảo luận compact với sự căn chỉnh phân phối cao (Zhou et al., 2023; Zhao et al., 2024; Miao et al.,
2023). Các thí nghiệm rộng rãi chứng minh rằng framework draft-then-verify này nâng cao hiệu
quả tính đồng thời của mô hình mục tiêu, do đó tăng tốc đáng kể quá trình suy luận.

Mặc dù có nhiều lợi ích của framework draft-then-verify này, nó đối mặt với một thách thức đáng
kể có thể cản trở hiệu suất và triển khai của nó - vấn đề chờ đợi lẫn nhau. Tức là, mô hình mục tiêu
sẽ không hoạt động khi mô hình thảo luận đang tạo token thảo luận và mô hình thảo luận sẽ không
hoạt động khi mô hình mục tiêu đang xác minh các token được thảo luận trước đó. Vấn đề chờ đợi
lẫn nhau này chủ yếu bắt nguồn từ hai hạn chế vốn có trong giải mã đón đoán: (i) việc thực thi
không đồng bộ của các giai đoạn thảo luận và xác minh, điều này trực tiếp dẫn đến vấn đề chờ đợi
lẫn nhau; và (ii) độ dài thảo luận cố định, không thể thích ứng với hầu hết các bước giải mã và do
đó làm trầm trọng thêm vấn đề chờ đợi lẫn nhau.

Do đó, trong bài báo này, chúng tôi tìm cách trả lời câu hỏi: Liệu chúng ta có thể thảo luận và xác
minh song song và điều chỉnh độ dài thảo luận một cách thích ứng? Với sự xem xét này, chúng tôi
đề xuất một framework đơn giản về mặt khái niệm, linh hoạt và tổng quát để thúc đẩy giải mã đón
đoán, cụ thể là Giải mã đón đoán song song với Độ dài thảo luận thích ứng (PEARL). Cụ thể,
PEARL bao gồm hai chiến lược pre-verify và post-verify: (i) pre-verify sử dụng mô hình mục tiêu
để xác minh token thảo luận đầu tiên trong giai đoạn thảo luận, điều này cho phép mô hình thảo
luận tạo ít token thảo luận hơn trong các tình huống khó khăn; (ii) post-verify sử dụng mô hình
thảo luận để tiếp tục tạo token thảo luận trong giai đoạn xác minh, điều này cung cấp thêm token
thảo luận trong các tình huống đơn giản. Như được hiển thị trong phần dưới của Hình 1, PEARL
giảm thiểu hiệu quả vấn đề chờ đợi lẫn nhau với tính song song và độ dài thảo luận thích ứng thông
qua hai chiến lược này. Chúng tôi tiến hành các thí nghiệm rộng rãi trên nhiều benchmark tạo văn
bản khác nhau, dẫn đến hiệu suất tăng tốc vượt trội lên tới 4.43× và 1.50×, so với giải mã tự hồi
quy và giải mã đón đoán, tương ứng.

--- TRANG 3 ---
Xuất bản như một bài báo hội nghị tại ICLR 2025

Mô hình bị kẹt×××××××
√√√√√√√√√√√√√√×√√√√√√
Độ dài thảo luận giải mã đón đoán
PEARL
(a)
deepseek 1.3&33 deepseek 7&33 codellama 7&34 codellama 7&70llama2 7&700.000.020.040.060.080.100.120.14Thời gian (s)0.080.08 0.080.14 0.14
0.05 0.05
0.040.07 0.07
(a)Thời gian Thảo luận và Mục tiêu Trung bình
Thời gian Thảo luận Trung bình
Thời gian Mục tiêu Trung bình
0 20 40 60 80 100
Lần lặp0102050100200
(b)Ảnh hưởng của Độ dài Thảo luận
độ dài thảo luận tối ưu tĩnh
độ dài thảo luận tối ưu lần lặp (b)

Hình 2: Quan sát động cơ. (a) Thời gian của cả giai đoạn thảo luận và giai đoạn xác minh đều
không thể bỏ qua, do đó việc thực thi không đồng bộ của mô hình thảo luận và mô hình mục tiêu
trực tiếp gây ra vấn đề chờ đợi lẫn nhau. (b) Chúng tôi quan sát thấy rằng độ dài thảo luận tối ưu
thay đổi đáng kể trong các lần lặp khác nhau, điều này làm trầm trọng thêm vấn đề chờ đợi lẫn
nhau.

2 KIẾN THỨC NỀN TẢNG

Ký hiệu. Trong bài báo này, chúng tôi sử dụng Mq để biểu thị mô hình thảo luận và Mp để biểu thị
mô hình mục tiêu. Mq(·), Mp(·) biểu thị logits của token tiếp theo của một lần forward duy nhất
của Mq, Mp tương ứng. gamma là một tham số siêu để kiểm soát kích thước cửa sổ trong giải mã
đón đoán. Chúng tôi biểu thị tốc độ chạy giữa Mq và Mp là c, được định nghĩa là tỷ lệ giữa thời
gian cho một lần forward duy nhất của Mp và thời gian cho một lần forward duy nhất của Mq, tức
là c=T(Mp(·))/T(Mq(·)).

Giải mã đón đoán. Cho một chuỗi đầu vào x như một tiền tố, một bước giải mã đón đoán bao gồm
một giai đoạn thảo luận và một giai đoạn xác minh. Trong giai đoạn thảo luận, mô hình thảo luận
Mq được sử dụng để cung cấp gamma token thảo luận x1, x2, ..., x_gamma bằng cách chạy gamma
lần forward mô hình và lấy mẫu. Ở đây, chúng tôi biểu thị Mq(x+ [x1, ..., x_i−1]) là qi, sau đó
mỗi token thảo luận được cho bởi xi∼qi, i= 1, ..., gamma. Trong giai đoạn xác minh, tiền tố x cùng
với gamma token thảo luận được gửi đến Mp để xác minh. Mô hình mục tiêu Mp nhập x+[x1, ...,
x_gamma] và xuất ra logits p1, p2, ..., p_gamma+1. Sau đó SD xác minh tuần tự xi thông qua lấy
mẫu đón đoán, trong đó tỷ lệ chấp nhận được cho bởi:

alphai= {
1 pi[xi]>=qi[xi],
pi[xi]/qi[xi] pi[xi]< qi[xi],
}(1)

Nếu SD từ chối xi, nó sẽ lấy mẫu lại một token từ norm (max(0, pi−qi)), ngược lại, SD chấp nhận
tất cả token thảo luận và lấy mẫu một token bổ sung từ p_gamma+1. Bằng cách này, mỗi bước SD
tạo token với số lượng ít nhất 1 và nhiều nhất gamma+ 1, dẫn đến tăng tốc hiệu quả.

Kích thước cửa sổ và độ dài thảo luận. Chúng tôi nhấn mạnh rằng kích thước cửa sổ là một tham
số siêu kiểm soát hành vi thảo luận. Độ dài thảo luận là số lượng token được tạo bởi mô hình thảo
luận trong một lần thực thi liên tục, được cố định và giống như kích thước cửa sổ trong SD, trong
khi độ dài thảo luận là thích ứng và có thể không bằng kích thước cửa sổ trong PEARL.

3 PHƯƠNG PHÁP LUẬN

3.1 QUAN SÁT ĐỘNG CƠ

Như được minh họa trong Hình 2(a), vấn đề chờ đợi lẫn nhau được gây ra trực tiếp bởi việc thực
thi không đồng bộ của mô hình thảo luận và mô hình mục tiêu. Trong các thí nghiệm của chúng
tôi, chúng tôi quan sát thấy rằng thời gian tiêu thụ trong giai đoạn thảo luận và giai đoạn xác minh
thường không thể bỏ qua. Lấy ví dụ Codellama 7B & 34B, tại mỗi bước giải mã, mặc dù tốc độ
chạy của Codellama 7B nhanh gần 3 lần so với Codellama 34B, tổng thời gian tiêu thụ để tạo 6
token thảo luận

--- TRANG 4 ---
Xuất bản như một bài báo hội nghị tại ICLR 2025

tiền tố
t=3t=4 Mô hình Thảo luận Mô hình Mục tiêu Giải mã Đón đoán Song song với Độ dài Thảo luận Thích ứng &!?&"?&#?&(?&)?&*?
t=6t=7t=8 &+?&,?&-?&!.?&!!?&!"?
t=9 Pre-Verify: 
❌Pre-Verify: 
✅Post-Verify: 
✅Post-Verify: 
❌"%&&(
✅"'!&"("&"(#&"($&&)
✅&*
✅&+
✅"(%&"(&&"('&&,
✅&-
❌("&!
❌(!

Hình 3: Minh họa PEARL của chúng tôi. Tại T= 0, Mq tạo x1, x2, x3 và Mp từ chối x1 với chiến
lược pre-verify. Tại T= 3t, Mp chấp nhận x4 và chuyển sang chiến lược post-verify. Tại T= 6t, Mp
chấp nhận tất cả token thảo luận x4, x5, x6 trong bước giải mã cuối cùng, trong khi Mq tiếp tục
thảo luận x7, x8, x9. Tại T= 9t, Mp từ chối x9, loại bỏ x10, x11, x12 và chuyển sang chiến lược
pre-verify. Đầu ra cuối cùng là [y1, x4, x5, x6, x7, x8, y2].

thậm chí còn gấp 2 lần thời gian tiêu thụ cho một bước xác minh. Do đó, vấn đề chờ đợi lẫn nhau
tồn tại tại bất kỳ thời điểm nào và ảnh hưởng nghiêm trọng đến hiệu quả tăng tốc của SD.

Việc thực thi không đồng bộ của mô hình thảo luận và mô hình mục tiêu là nguyên nhân trực tiếp
của vấn đề chờ đợi lẫn nhau, được quyết định bởi hai yêu cầu của giải mã đón đoán: (1) giai đoạn
thảo luận yêu cầu tiền tố đầu vào được xác minh; (2) giai đoạn xác minh yêu cầu mô hình thảo
luận hoàn thành việc tạo token thảo luận. Điều này ngụ ý tiềm năng lớn để giảm thiểu vấn đề chờ
đợi lẫn nhau thông qua tính song song: nếu chúng ta có thể loại bỏ hai yêu cầu này và song song hóa
giai đoạn thảo luận và giai đoạn xác minh, một sự tăng tốc đáng kể có thể xảy ra.

Một hạn chế khác làm trầm trọng thêm vấn đề chờ đợi lẫn nhau là độ dài thảo luận cố định trong
SD, không phù hợp cho tất cả các bước giải mã. Như được hiển thị trong Hình 2(b), độ dài thảo
luận tối ưu thay đổi đáng kể trong các lần lặp khác nhau. Một mặt, khi độ dài thảo luận tối ưu nhỏ
hơn độ dài thảo luận cố định, mô hình thảo luận sẽ tạo các token thảo luận vô nghĩa làm cản trở
mô hình mục tiêu. Mặt khác, khi độ dài thảo luận tối ưu lớn hơn độ dài thảo luận cố định, mô hình
thảo luận có thể đã tạo thêm token thảo luận có thể được chấp nhận bởi mô hình mục tiêu với một
lần forward duy nhất. Tuy nhiên, độ dài thảo luận cố định sẽ làm gián đoạn giai đoạn thảo luận dài
hơn và thực hiện thêm một giai đoạn xác minh, điều này cũng củng cố vấn đề chờ đợi lẫn nhau.
Điều này thúc đẩy PEARL của chúng tôi để tiếp tục giảm thiểu vấn đề chờ đợi lẫn nhau với độ dài
thảo luận thích ứng.

Cùng với hai động cơ này, chúng tôi đề xuất hai chiến lược đơn giản và hiệu quả, pre-verify và
post-verify. Pre-verify loại bỏ yêu cầu 2 và cho phép mô hình mục tiêu xác minh token thảo luận
đầu tiên trước. Post-verify loại bỏ yêu cầu 1 và cho phép mô hình thảo luận tiếp tục tạo token thảo
luận trong giai đoạn xác minh. Hai chiến lược cho phép tính song song và đạt được độ dài thảo
luận thích ứng để giảm thiểu hiệu quả vấn đề chờ đợi lẫn nhau.

3.2 PRE-VERIFY: XÁC MINH TOKEN THẢO LUẬN ĐẦU TIÊN TRƯỚC.

Chiến lược pre-verify nhằm loại bỏ yêu cầu rằng giai đoạn xác minh yêu cầu mô hình thảo luận
hoàn thành việc tạo token thảo luận. Do đó, chúng tôi tìm cách xác minh một số token thảo luận
trước trong giai đoạn thảo luận. Chúng tôi đi sâu vào giai đoạn thảo luận một cách rõ ràng. Trong
giai đoạn thảo luận, mô hình thảo luận cố gắng cung cấp gamma token thảo luận bằng cách chạy
gamma lần forward mô hình. Chúng tôi thấy rằng đầu vào của mô hình thảo luận trong gamma lần
forward là x, x+ [x1], ..., x+ [x1, x2, ..., x_gamma−1], tương ứng.

--- TRANG 5 ---
Xuất bản như một bài báo hội nghị tại ICLR 2025

Chỉ có tiền tố gốc x có thể được thu thập bởi mô hình mục tiêu để xác minh song song. Do đó,
chúng tôi đề xuất chạy mô hình mục tiêu để xuất ra logits Mp(x) song song. Bằng cách này, chúng
ta có thể xác minh token đầu tiên x1 trước giai đoạn xác minh. Chúng tôi triển khai cùng phương
pháp xác minh không mất mát theo (Leviathan et al., 2023) như được minh họa trong Mục 2.

Bằng cách áp dụng chiến lược pre-verify như vậy, chúng ta có thể xác minh token thảo luận đầu
tiên trước giai đoạn xác minh. Nếu token đầu tiên bị từ chối, tất cả các token thảo luận tiếp theo
đều vô nghĩa và nên được loại bỏ. Do đó chúng ta có thể bỏ qua giai đoạn xác minh và trực tiếp
tiến hành giai đoạn thảo luận tiếp theo với tiền tố x+ [y1]. Nếu token đầu tiên được chấp nhận, tất
cả token thảo luận sẽ được gửi đến mô hình mục tiêu trong giai đoạn xác minh. Trong Hình 3, tại
thời điểm T= 0, mô hình thảo luận tạo x1, x2, x3 trong khi mô hình mục tiêu xuất ra pt_0, từ chối
token đầu tiên x1 và lấy mẫu token khác y1. Tại thời điểm T= 3t, mô hình thảo luận tạo x4, x5, x6
trong khi mô hình mục tiêu chấp nhận token đầu tiên x4. Sau đó x4, x5, x6 được gửi đến mô hình
mục tiêu trong giai đoạn xác minh tiếp theo.

3.3 POST-VERIFY: TIẾP TỤC THẢO LUẬN TRONG XÁC MINH.

Chiến lược post-verify nhằm loại bỏ yêu cầu rằng giai đoạn thảo luận yêu cầu tiền tố đầu vào được
xác minh. Tuy nhiên, giả định này mang lại hạn chế rằng mô hình thảo luận nên bị kẹt cho đến khi
mô hình mục tiêu hoàn thành xác minh.

Do đó, chúng tôi loại bỏ giả định này và đưa ra giả định khác: chúng tôi trực tiếp giả định rằng tất
cả token thảo luận có thể được chấp nhận. Bằng cách này, chúng tôi thấy rằng khi tất cả gamma
token thảo luận được chấp nhận, việc lấy mẫu một token mới từ Mp(x+ [x1, ..., x_gamma]) không
cần thiết, vì mô hình thảo luận có thể đã tạo thêm token thảo luận có thể được chấp nhận. Do đó
chúng ta có thể sử dụng mô hình thảo luận để tiếp tục thảo luận x_gamma+1, ..., x_2gamma trong
giai đoạn xác minh.

Thuật toán 1 Giải mã Đón đoán Song song với Độ dài Thảo luận Thích ứng.
Yêu cầu: mô hình thảo luận Mq, mô hình mục tiêu Mp, tiền tố đầu vào x, số token tạo tối đa L,
kích thước cửa sổ gamma.
Khởi tạo: mode ←"pre-verify"
while len(x)< L do
ifmode = "pre-verify" then
x, mode ←Pre-verify( Mq, Mp,x, gamma)
else
x, mode ←Post-verify( Mq, Mp,x, gamma)
end if
end while

Nếu tất cả gamma token thảo luận được chấp nhận, chúng ta có thể bỏ qua giai đoạn thảo luận tiếp
theo vì chúng ta đã có token thảo luận trong giai đoạn thảo luận tiếp theo. Logit cuối cùng Mp(x+
[x1, ..., x_gamma]) có thể được sử dụng để xác minh x_gamma+1, đây cũng là quá trình "pre-verify".
Trong Hình 3, tại thời điểm T= 6t, mô hình mục tiêu nhận x4, x5, x6 và xuất ra pt_x4, pt_x5,
pt_x6, trong khi mô hình thảo luận tiếp tục đoán token thảo luận tiếp theo x7, x8, x9. May mắn
thay, tất cả token thảo luận đều được chấp nhận, và chúng ta có thể trực tiếp tiến hành giai đoạn
xác minh tiếp theo với tiền tố x+ [y1, x4, x5, x6, x7, x8, x9]. Tại thời điểm T= 9t, mô hình mục
tiêu nhận x7, x8, x9 và xuất ra pt_x7, pt_x8, pt_x9, trong khi mô hình thảo luận tiếp tục đoán token
thảo luận tiếp theo x10, x11, x12. Không may, chỉ có x8 được chấp nhận, và các token thảo luận
x10, x11, x12 sẽ bị loại bỏ. Cuối cùng, tiền tố x+ [y1, x4, x5, x6, x7, x8, y2] được nhập vào giai
đoạn thảo luận tiếp theo.

3.4 PEARL: GIẢI MÃ ĐỜN ĐỊN SONG SONG VỚI ĐỘ DÀI THẢO LUẬN THÍCH ỨNG

Kết hợp hai chiến lược, framework PEARL của chúng tôi bao gồm một mô hình thảo luận, một mô
hình mục tiêu và hai chiến lược để giải mã token. Hai chiến lược được chuyển đổi theo kết quả xác
minh trong bước giải mã trước đó. Thuật toán 1 cung cấp tóm tắt PEARL của chúng tôi. Chúng tôi
cũng cung cấp thêm chi tiết trong Thuật toán 2. Lưu ý rằng các chiến lược pre-verify và post-verify
không được thực thi chỉ một lần trong quá trình tạo câu và sẽ được chuyển đổi liên tục theo tình
hình chấp nhận token trong toàn bộ quá trình tạo. Chúng tôi cung cấp một ví dụ profiling từng bước
đơn giản trong Phụ lục B để hiểu rõ hơn. Sau đó chúng tôi chỉ ra cách PEARL của chúng tôi đạt
được tính song song và độ dài thảo luận thích ứng để giảm thiểu vấn đề chờ đợi lẫn nhau.

Tính song song. Với hai chiến lược pre-verify và post-verify, tại bất kỳ thời điểm nào, mô hình
thảo luận và mô hình mục tiêu đang chạy song song, điều này trực tiếp phá vỡ việc thực thi không
đồng bộ của mô hình thảo luận và mô hình mục tiêu.

Độ dài thảo luận thích ứng. Trong PEARL của chúng tôi, quá trình thảo luận có thể được xem như
một quá trình thảo luận phân đoạn. Nếu mô hình thảo luận không thể tạo bất kỳ token "đúng" nào,
chiến lược pre-verify sẽ tránh quá trình thảo luận bổ sung. Nếu mô hình thảo luận có thể đã tạo
thêm token "đúng", mô hình mục tiêu sẽ không làm gián đoạn giai đoạn thảo luận, nơi mô hình
thảo luận có thể tạo thêm token thảo luận với chiến lược post-verify. Do đó, PEARL có thể sử dụng
hai chiến lược đơn giản nhưng hiệu quả để triển khai độ dài thảo luận thích ứng để giảm thiểu vấn
đề chờ đợi lẫn nhau.

4 THÍ NGHIỆM

4.1 THIẾT LẬP THÍ NGHIỆM

Nhiệm vụ và Bộ dữ liệu. Chúng tôi tiến hành thí nghiệm trên nhiều tác vụ tạo văn bản khác nhau
để đánh giá hiệu quả của PEARL, bao gồm HumanEval (tác vụ tạo mã) (Chen et al., 2021), GSM8K
& MGSM (tác vụ lý luận số học đa ngôn ngữ, MGSM là bản dịch đa ngôn ngữ của GSM8K) (Cobbe
et al., 2021; Shi et al.), và MT-bench (tác vụ đối thoại nhiều vòng) (Zheng et al., 2024). Những
tác vụ và bộ dữ liệu này là các benchmark đại diện cho đánh giá. Thêm chi tiết có thể tìm thấy trong
Phụ lục C.1.

Chi tiết Đánh giá. Chúng tôi đánh giá hiệu quả của PEARL với một số họ LLM tiên tiến, bao gồm
CodeLlama (Roziere et al., 2023), Deepseek-Coder (Guo et al., 2024), Llama 2 (Touvron et al.,
2023) và Llama 3.1 (Dubey et al., 2024). Trong các thí nghiệm của chúng tôi, các mô hình có kích
thước nhỏ hơn 7B được sử dụng làm mô hình thảo luận và các mô hình có kích thước lớn hơn 33B
được sử dụng làm mô hình mục tiêu. Chúng tôi báo cáo tỷ lệ tăng tốc walltime làm metric. Chi tiết
đánh giá bổ sung được cung cấp trong Phụ lục C.2 và C.3.

Phương pháp Baseline. Chúng tôi triển khai bốn phương pháp tăng tốc suy luận không cần huấn
luyện làm baseline. (i) Giải mã đón đoán: các phương pháp SD độc lập (Leviathan et al., 2023;
Chen et al., 2023) sử dụng mô hình thảo luận để thảo luận token tương lai và sau đó xác minh chúng
song song. (ii) Ouroboros: ouroboros (Zhao et al., 2024) đề xuất phrase candidate pool từ quá
trình xác minh để tạo bản thảo chính xác và dài hơn. (iii) Lookahead Decoding: look ahead
decoding (Fu et al., 2024) cache quỹ đạo tạo (n-grams) làm bản thảo để giảm tổng số bước giải
mã. (iv) Assisted generation: assisted generation (Joao Gante, 2023) sử dụng phương pháp heuristic
để xác định số lượng token thảo luận trong lần lặp tiếp theo, dựa trên kết quả xác minh của token
được tạo bởi mô hình thảo luận trong vòng trước.

Bảng 1: Kết quả thí nghiệm trên tác vụ tạo mã. Một phần kết quả của Lookahead Decoding và
Ouroboros được lấy từ (Zhao et al., 2024). Chúng tôi in đậm kết quả tốt nhất cho mỗi tổ hợp mô
hình.*Một số kết quả của ouroboros và lookahead decoding được tái tạo trong triển khai chính thức
của họ với tham số mặc định. Các kết quả khác được tái tạo trong triển khai của chúng tôi. Ký hiệu
'-' biểu thị rằng các phương pháp không hỗ trợ cấu hình mô hình hiện tại.2

Phương pháp CodeLlama CodeLlama Llama2 Llama3.1 DeepSeek DeepSeek
7&34B 7&70B 7&70B 8&70B 1.3&33B 6.7&33B
Auto Regressive 1.00 × 1.00× 1.00× 1.00× 1.00× 1.00×
Speculative Decoding (Leviathan et al., 2023) 1.76 × 3.03× 2.35× 2.60× 2.32× 1.94×
Ouroboros (Zhao et al., 2024) 2.14 ×*3.28×*2.10× -*3.25×*2.66×
Lookahead Decoding (Fu et al., 2024) 1.72 ×*1.57×*1.80× -*1.82×*1.82×
Assisted Generation (Joao Gante, 2023) 1.37 × 2.49× 2.27× 2.72× 1.88× 1.52×
PEARL (ours) 2.48 × 4.43× 3.29× 3.87× 3.48× 2.79×

2Triển khai của họ yêu cầu phiên bản transformers 4.36.2, trong khi Llama 3.1 yêu cầu transformers >=
4.43.0

--- TRANG 6 ---
[Tiếp tục dịch...]

Bảng 2: Kết quả thí nghiệm trên tác vụ lý luận số học đa ngôn ngữ với Llama 2 7&70B. Chúng tôi
in đậm kết quả tốt nhất cho mỗi danh mục. Kết quả của ouroboros và lookahead decoding được
tái tạo trong triển khai chính thức của họ với tham số mặc định. Các kết quả khác được tái tạo
trong triển khai của chúng tôi.

Phương pháp Tiếng Anh (GSM8K) Bengali Đức Tây Ban Nha Pháp Nhật Nga Swahili Tegulu Thái Trung Quốc Trung bình
Auto Regressive 1.00 × 1.00× 1.00× 1.00× 1.00× 1.00× 1.00× 1.00× 1.00× 1.00× 1.00× 1.00×
Speculative Decoding 2.48 × 2.69× 2.77× 2.64× 2.71× 2.71× 2.72× 2.81× 2.65× 2.71× 2.78× 2.70×
Ouroboros 1.60 × 1.75× 1.88× 1.69× 1.80× 1.95× 1.65× 1.68× 2.45× 1.92× 1.81× 1.84×
Lookahead Decoding 1.23 × 1.34× 1.51× 1.50× 1.48× 1.29× 1.43× 1.60× 1.28× 1.23× 1.48× 1.39×
Assisted Generation 1.96 × 1.69× 1.75× 1.70× 1.67× 2.02× 1.68× 1.58× 3.07× 2.17× 1.97× 1.93×
PEARL (ours) 3.82 × 3.94× 4.00× 3.81× 3.76× 3.94× 3.85× 4.18× 4.10× 3.93× 4.06× 3.95×

Bảng 3: Kết quả thí nghiệm trên tác vụ đối thoại nhiều vòng với Llama 2 7&70B. Chúng tôi in
đậm kết quả tốt nhất cho mỗi danh mục. Kết quả của ouroboros và lookahead decoding được tái
tạo trong triển khai chính thức của họ với tham số mặc định. Các kết quả khác được tái tạo trong
triển khai của chúng tôi.

Phương pháp Viết Nhập vai Lý luận Toán Lập trình Trích xuất Khoa học Nhân văn Trung bình
Auto Regressive 1.00 × 1.00× 1.00× 1.00× 1.00× 1.00× 1.00× 1.00× 1.00×
Speculative Decoding 1.70 × 1.73× 1.96× 2.00× 1.93× 2.14× 1.87× 1.81× 1.89×
Ouroboros 1.42 × 1.35× 1.40× 1.61× 1.35× 1.67× 1.44× 1.36× 1.45×
Lookahead Decoding 1.31 × 1.24× 1.50× 1.51× 1.38× 1.40× 1.29× 1.27× 1.36×
Assisted Generation 1.41 × 1.40× 1.39× 1.64× 1.74× 1.92× 1.57× 1.47× 1.55×
PEARL (ours) 2.40 × 2.45× 2.85× 2.79× 2.67× 2.92× 2.58× 2.50× 2.64×

4.2 KẾT QUẢ CHÍNH.

Chúng tôi tiến hành các thí nghiệm rộng rãi trên các benchmark nói trên. Như được hiển thị trong
Bảng 1, PEARL vượt trội đáng kể so với giải mã đón đoán vanilla, Ouroboros, Lookahead decoding
và assisted generation trong tất cả cấu hình mô hình backbone trên bộ dữ liệu HumanEval, bao
gồm các quy mô cấu hình mô hình khác nhau bao gồm 1.3&33B, 6.7&33B (7&34B) và 7&70B.
Cụ thể, PEARL có thể đạt được tăng tốc lên tới 4.43× và 1.50× so với các phương pháp tự hồi quy
vanilla và giải mã đón đoán vanilla, tương ứng. Những kết quả này chỉ ra sự tồn tại phổ biến của
vấn đề chờ đợi lẫn nhau, và chứng minh rằng PEARL giải quyết hiệu quả vấn đề chờ đợi lẫn nhau,
do đó đạt được kết quả tăng tốc suy luận đáng kể so với các phương pháp dựa trên framework
draft-then-verify truyền thống. Hơn nữa, như được hiển thị trong Bảng 2, 3, PEARL cũng có thể
đạt được tăng tốc suy luận đáng kể trên 12 tác vụ số học đa ngôn ngữ và 8 tác vụ đối thoại nhiều
vòng, trong khi PEARL có thể đạt được tăng tốc 1.36∼1.55× so với giải mã đón đoán vanilla.
Những kết quả này chứng minh tiềm năng vượt trội của framework giải mã đón đoán song song để
khai thác tài nguyên tính toán một cách đầy đủ hơn. Chúng tôi cung cấp thêm kết quả đánh giá
trên MGSM và MT-bench với Llama 3.1 8&70B tiên tiến hơn trong Phụ lục D.

4.3 NGHIÊN CỨU LOẠI BỎ

Để cung cấp thêm thông tin chi tiết về hai chiến lược được đề xuất, chúng tôi tiến hành nghiên cứu
loại bỏ. Chúng tôi biểu thị PEARL không có pre-verify là PEARL w/o pre-verify và PEARL không
có post-verify là PEARL w/o post-verify và trình bày kết quả chính của nghiên cứu loại bỏ.

Như được hiển thị trong Bảng 4, việc thiếu bất kỳ chiến lược nào của PEARL dẫn đến suy giảm
hiệu suất của toàn bộ framework. Việc thiếu chiến lược post-verify thể hiện tác động rõ rệt hơn đến
hiệu suất của PEARL so với chiến lược pre-verify. Chúng tôi giải thích lý do cho hiện tượng này
như sau. Theo trực giác, chiến lược pre-verify đóng góp nhiều hơn khi tỷ lệ chấp nhận tương đối
thấp. Chiến lược pre-verify có thể tiết kiệm một lần forward của mô hình mục tiêu khi token thảo
luận đầu tiên bị từ chối bởi mô hình mục tiêu. Biểu thị tỷ lệ chấp nhận là alpha, và chiến lược
pre-verify sẽ có hiệu lực với xác suất 1−alpha. Do đó, sự căn chỉnh tốt hơn giữa mô hình thảo luận
và mô hình mục tiêu sẽ làm cho chiến lược pre-verify ít hiệu quả hơn. Tuy nhiên, chiến lược
post-verify đóng góp nhiều hơn khi hai mô hình được căn chỉnh, tức là có nhiều tình huống hơn
trong đó tất cả token thảo luận được chấp nhận bởi mô hình mục tiêu. Do đó, hai chiến lược bổ
sung cho nhau và tăng tốc suy luận cùng nhau.

--- TRANG 7 ---
Xuất bản như một bài báo hội nghị tại ICLR 2025

Bảng 4: Kết quả loại bỏ của PEARL trên bộ dữ liệu HumanEval và GSM8K.

HumanEval GSM8K
Phương pháp CodeLlama 7B&34B CodeLlama 7B&70B DeepSeek 1.3B&33B Llama 2 7B&70B
PEARL w/o pre-verify 2.21×(↓0.14) 3.53×(↓0.26) 3.19×(↓0.29) 2.51×(↓0.26)
PEARL w/o post-verify 1.64×(↓0.71) 2.57×(↓1.22) 2.37×(↓1.11) 2.15×(↓0.72)
PEARL 2.35× 3.79× 3.48× 2.87×

Trong các thí nghiệm của chúng tôi, tất cả các tổ hợp mô hình đều cho thấy sự căn chỉnh tuyệt vời,
điều này dẫn đến sự vượt trội của chiến lược post-verify. Khi các mô hình ngôn ngữ phát triển và
có thêm các phương pháp giải mã đón đoán, sự căn chỉnh giữa mô hình thảo luận và mô hình mục
tiêu sẽ tốt hơn, điều này tiếp tục làm nổi bật tầm quan trọng của chiến lược post-verify. Trong khi
đó, chúng ta có thể cải thiện thêm chiến lược pre-verify bằng cách pre-verify nhiều token thảo luận
(tương tự như cache pool trong Ouroboros và Lookahead Decoding) để tăng tốc nhiều hơn. Chúng
tôi để điều này làm công việc tương lai.

4.4 NGHIÊN CỨU TRƯỜNG HỢP

Trong phần này, chúng tôi trình bày hai nghiên cứu trường hợp quan trọng để thảo luận về phân
tích nhạy cảm của gamma và số token chấp nhận trung bình của PEARL. Chúng tôi cung cấp thêm
kết quả thí nghiệm trong Phụ lục D.

4.4.1 PHÂN TÍCH NHẠY CẢM CỦA KÍCH THƯỚC CỬA SỔ gamma

Theo trực giác, giá trị tối ưu của kích thước cửa sổ gamma nên là tỷ lệ tốc độ c giữa mô hình thảo
luận và mô hình mục tiêu, nơi mô hình thảo luận và mô hình mục tiêu có thể đạt được tính song
song tốt nhất và giảm thiểu hoàn toàn vấn đề chờ đợi lẫn nhau. Tuy nhiên, thường thì c có thể
không phải là số nguyên. Do đó, chúng tôi đề xuất chọn số nguyên làm tròn của c làm kích thước
cửa sổ gamma. Chúng tôi tiến hành một số nghiên cứu trường hợp để xem tác động của các giá trị
gamma khác nhau trong các cấu hình mô hình khác nhau và các tác vụ khác nhau. Như được hiển
thị trong Bảng 5 (chúng tôi đánh dấu số nguyên làm tròn bên dưới mỗi cấu hình mô hình), việc
trực tiếp chọn số nguyên làm tròn của c làm kích thước cửa sổ có thể đạt được tăng tốc suy luận
tối đa, điều này ổn định với các cấu hình mô hình. Trong khi đó, như được hiển thị trong Bảng 6
(số nguyên làm tròn của c của Llama 2 7&70B là 5), việc đặt kích thước cửa sổ là 5 cũng có thể
đạt được tăng tốc suy luận tối đa, điều này ổn định với các tác vụ. Những kết quả này gợi ý sự
thuận tiện lớn của framework PEARL giảm bớt gánh nặng điều chỉnh gamma cho các cấu hình mô
hình khác nhau và các tác vụ khác nhau.

Bảng 5: Gamma tối ưu của các tổ hợp mô hình khác
nhau trên HumanEval. (đơn vị: tok/sec)

gamma CodeLlama 7&34 CodeLlama 7&70 DeepSeek 6.7&33
(c=3) (c=5) (c=3)
2 33.25 16.28 30.82
3 46.06 23.14 48.46
4 44.12 29.65 47.22
5 44.93 40.72 46.91
6 41.83 35.39 44.36

Bảng 6: Gamma tối ưu cho các tác vụ khác nhau của
Llama 2 7B&70B. (c=5)

gamma HumanEval GSM8K MT-Bench
3 20.39 18.23 17.67
4 24.58 21.81 20.69
5 30.34 26.47 24.25
6 28.02 24.59 22.71
7 28.09 24.23 22.54

4.4.2 SỐ TOKEN CHẤP NHẬN TRUNG BÌNH

Trong Mục 3.4, chúng tôi khẳng định rằng PEARL có thể đạt được độ dài thảo luận thích ứng để
tăng tốc. Để minh họa thêm số token chấp nhận trung bình thực tế trong PEARL dưới điều kiện
phức tạp thế giới thực, chúng tôi tiến hành thí nghiệm trên bộ dữ liệu HumanEval, GSM8K và
MT-Bench. Như được hiển thị trong Bảng 7, chúng tôi vẫn quan sát thực nghiệm rằng PEARL đạt
được nhiều token chấp nhận hơn so với các phương pháp SD vanilla, điều này tiếp tục chứng minh
hiệu quả của framework PEARL. Cụ thể, PEARL đạt được số lượng token chấp nhận trung bình
tối đa là 39.9, vượt trội đáng kể so với các phương pháp SD vanilla với biên độ lớn. Lưu ý rằng số
token chấp nhận trung bình (MAT) và tỷ lệ tốc độ c giữa mô hình thảo luận và mô hình mục tiêu
đều ảnh hưởng đến kết quả tăng tốc cuối cùng. Ví dụ, trong trường hợp Deepseek 6.7&33B, mô
hình thảo luận chạy nhanh hơn khoảng ba lần so với mô hình mục tiêu. Ngay cả khi MAT tiếp cận
vô cùng, nơi tất cả token được tạo bởi mô hình 6.7B, tốc độ tăng tốc tối đa lý thuyết sẽ bị giới hạn
ở 3×. Do đó, với MAT là 39.9, PEARL

--- TRANG 8 ---
Xuất bản như một bài báo hội nghị tại ICLR 2025

đạt được tăng tốc 2.75×, gần với mức tối ưu lý thuyết này. Những kết quả này chứng minh rằng
PEARL của chúng tôi có thể khai thác đầy đủ khả năng suy luận của mô hình thảo luận để tăng tốc
thêm.

Bảng 7: So sánh số token chấp nhận trung bình của các phương pháp SD vanilla và PEARL.

Phương pháp CodeLlama 7B&34B CodeLlama 7B&70B DeepSeek 1.3B&33B DeepSeek 6.7B&33B
Speculative Decoding 5.27 8.32 7.23 5.69
Ours 27.95 26.53 29.65 39.90

4.5 THÊM THẢO LUẬN VỀ PEARL

Làm rõ các tình huống ứng dụng. Đầu tiên, chúng tôi muốn làm rõ các tình huống ứng dụng của
PEARL. Ý tưởng chính của giải mã đón đoán là khai thác sự dư thừa tính toán để tăng tốc
Leviathan et al. (2023). Dựa trên ý tưởng này, chúng tôi quan sát vấn đề chờ đợi lẫn nhau, cản
trở giải mã đón đoán trong việc sử dụng đầy đủ tài nguyên tính toán dư thừa. Do đó, các tình
huống ứng dụng chính của PEARL tập trung vào các tình huống có tài nguyên tính toán đầy đủ,
nơi giải mã đón đoán không thể sử dụng đủ những tài nguyên này.

PEARL trong các tình huống đầy đủ tài nguyên. Trong những tình huống như vậy, mô hình thảo
luận và mô hình mục tiêu có thể được triển khai riêng biệt. Việc chạy đồng thời mô hình thảo luận
và mô hình mục tiêu sẽ không mang lại độ trễ bổ sung trong cả giai đoạn thảo luận và xác minh.
Các thí nghiệm chính của chúng tôi cùng với tất cả các phương pháp baseline được tiến hành dưới
những tình huống này, nơi chúng tôi triển khai mô hình thảo luận và mô hình mục tiêu trên các
thiết bị khác nhau. Bên cạnh đó, việc tích hợp tensor parallelism (TP) với PEARL trong những
tình huống này là khả thi. Chúng tôi cung cấp giải pháp trong Phụ lục F.

PEARL trong các tình huống hạn chế tài nguyên. Tuy nhiên, chúng tôi thừa nhận rằng trong nhiều
tình huống, tài nguyên GPU bị hạn chế, và mô hình thảo luận và mô hình mục tiêu được triển khai
trên cùng thiết bị. Chúng tôi gọi đây là thiết lập "co-locate" hoặc cạnh tranh tài nguyên (RC). Vấn
đề chính nằm ở bản chất thiết kế phần cứng GPU - hai quá trình chạy trên cùng GPU sẽ cạnh tranh
tài nguyên GPU, có thể dẫn đến chậm lại. Chúng tôi cung cấp giải pháp để giải quyết vấn đề này.

Nói chung, trong các ứng dụng LLM thế giới thực, mô hình mục tiêu quy mô lớn thường được đặt
với hơn 1 GPU để xử lý nhiều yêu cầu hơn và suy luận ngữ cảnh dài, trong khi mô hình thảo luận
quy mô nhỏ chỉ cần 1 GPU để suy luận. Trong trường hợp này, chúng ta có thể áp dụng pipeline
parallelism (PP) để phục vụ mô hình mục tiêu với nhiều GPU. Lấy cảm hứng từ quan sát này,
chúng tôi đề xuất phiên bản cải tiến của PEARL để sử dụng hiệu quả tài nguyên tính toán GPU với
PP mà không có cạnh tranh tài nguyên. Ý tưởng chính là chuyển tính toán của mô hình thảo luận
sang GPU khác khi mô hình mục tiêu đang chạy trên GPU cụ thể. Cụ thể, chúng tôi chuyển việc
tạo ⌈gamma/2⌉ token thảo luận đầu tiên sang thiết bị cuối cùng, trong khi ⌊gamma/2⌋ token thảo
luận cuối cùng được tạo với thiết bị đầu tiên. Vì tính toán của mô hình mục tiêu được tiến hành
tuần tự với nhiều GPU, điều này có thể sử dụng hiệu quả tài nguyên GPU để tránh RC. Chúng tôi
tiến hành một số thí nghiệm trong Bảng 8 và thấy rằng chiến lược này cho phép PEARL giữ lại
89%∼99% hiệu suất ban đầu, chứng minh hiệu quả của PEARL trong những điều kiện như vậy.
Chúng tôi cung cấp triển khai chi tiết và kết quả thí nghiệm bổ sung của chiến lược này trong Phụ
lục G.

Bảng 8: So sánh các mô hình Llama cho PEARL trên MT-bench với và không có tình huống RC.

Mô hình Viết Nhập vai Lý luận Toán Lập trình Trích xuất Khoa học Nhân văn Trung bình
Llama 2 7b&70b 22.10 22.47 26.16 25.74 24.63 26.11 23.84 23.09 24.28
Llama 2 7b&70b (RC) 19.88 20.24 25.06 24.47 23.47 25.79 21.55 22.03 22.83
hiệu suất giữ lại 89.95% 90.08% 95.80% 95.07% 95.29% 98.77% 90.39% 95.41% 94.03%
Llama 3.1 8b&70b 31.23 30.08 35.09 36.59 31.95 34.60 30.06 27.51 32.14
Llama 3.1 8b&70b (RC) 29.65 27.54 35.01 36.31 29.85 33.99 26.77 26.10 30.78
hiệu suất giữ lại 94.94% 91.56% 99.77% 99.23% 93.43% 98.24% 89.06% 94.87% 95.77%

5 CÔNG TRÌNH LIÊN QUAN

Tăng tốc suy luận Transformer. Tăng tốc suy luận là một lĩnh vực đã được nghiên cứu rộng rãi
trong một thời gian dài (Liu et al., 2024). Tồn tại các công trình rộng rãi về tăng tốc suy luận
transformer với sự nổi lên của LLM (Xia et al., 2024; Lv et al., 2024; Chen et al., 2024).

--- TRANG 9 ---
Xuất bản như một bài báo hội nghị tại ICLR 2025

Điều này bao gồm các nỗ lực về nén mô hình (Zhu et al., 2023), thiết kế kiến trúc hiệu quả
(Chitty-Venkata & Somani, 2022), và tối ưu hóa và triển khai phần cứng (Dao et al., 2022). Các
phương pháp nén mô hình như lượng tử hóa (Choi et al., 2018), chưng cất kiến thức (Hinton et al.,
2015), và cắt tỉa cấu trúc (Han et al., 2015) nhằm giảm số lượng phép toán tính toán. Thiết kế kiến
trúc hiệu quả được đề xuất để phát triển kiến trúc transformer nhẹ. Tối ưu hóa và triển khai phần
cứng được đề xuất để thực thi hiệu quả nhằm khai thác đầy đủ các thiết bị phần cứng. Những
phương pháp này đã đạt được thành công lớn, trong khi chúng trực giao với các thuật toán giải mã
đón đoán, có thể được tích hợp để tăng tốc thêm.

Framework draft-then-verify. Trong khi SD thể hiện hiệu quả tăng tốc lớn và chất lượng tổng quát
không mất mát, vẫn là một thách thức để tìm mô hình thảo luận compact với sự căn chỉnh phân
phối cao. Một số công trình tập trung vào việc loại bỏ sự cần thiết của mô hình thảo luận. Self-
speculative decoding (Zhang et al., 2023) đề xuất bỏ qua một số lớp trung gian của mô hình mục
tiêu để thảo luận. Medusa (Cai et al., 2024) thêm các đầu giải mã bổ sung ở đầu mô hình mục tiêu
để tạo bản thảo. Lookahead decoding (Fu et al., 2024) cache quỹ đạo tạo (n-grams) làm bản thảo.
Eagle (Li et al., 2024) sử dụng lớp decoder transformer bổ sung để tạo bản thảo ở cấp độ tính năng.
Glide (Du et al., 2024) tái sử dụng kv cache từ mô hình mục tiêu để giải mã token thảo luận chính
xác hơn. DistillSpec (Zhou et al., 2023) sử dụng phương pháp chưng cất để xác định mô hình thảo
luận compact. Ouroboros (Zhao et al., 2024) kết hợp SD tiêu chuẩn và lookahead decoding để tạo
bản thảo chính xác và dài hơn. Bên cạnh những công trình này, SpecInfer (Miao et al., 2023) đề
xuất tree attention, được sử dụng rộng rãi để xác minh thêm bản thảo và tăng tỷ lệ chấp nhận. Tuy
nhiên, tất cả chúng đều không giải quyết vấn đề tính song song. Từ góc độ này, PEARL của chúng
tôi trực giao với những phương pháp này và có thể được tích hợp với những phương pháp này,
điều này được để lại làm công việc tương lai.

6 KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI

Hạn chế và tác động rộng rãi. Vì PEARL của chúng tôi là framework tăng tốc song song, vẫn là
thách thức để lên lịch tài nguyên GPU để tránh cạnh tranh tài nguyên, có thể làm tăng tiêu thụ
điện năng. Chúng tôi khẳng định cam kết đóng góp tích cực cho xã hội, tránh tác hại và duy trì sự
trung thực và đáng tin cậy. Chúng tôi trích dẫn đúng các phương pháp và bộ dữ liệu trước đây mà
chúng tôi sử dụng, và đảm bảo rằng tất cả dữ liệu liên quan đều hoàn toàn công khai, không có dữ
liệu riêng tư nào được sử dụng. Hơn nữa, chúng tôi cam kết duy trì đúng đắn các kỹ thuật tăng tốc
suy luận mà chúng tôi đã phát triển, mà không gây ra bất kỳ hình thức phân biệt đối xử nào.

Kết luận. Trong bài báo này, chúng tôi đề xuất một framework tăng tốc suy luận mới, được gọi là
PEARL, cải thiện đáng kể hiệu quả suy luận LLM. PEARL bao gồm hai chiến lược đơn giản và hiệu
quả, tức là pre-verify và post-verify, giảm thiểu hiệu quả vấn đề chờ đợi lẫn nhau với tính song
song và độ dài thảo luận thích ứng. Các thí nghiệm rộng rãi chứng minh rằng PEARL được đề
xuất của chúng tôi vượt trội so với các phương pháp tiên tiến hiện tại trên nhiều benchmark tạo văn
bản khác nhau.

Công việc tương lai. Đối với nghiên cứu tương lai, chúng tôi nhằm tích hợp PEARL với các phương
pháp suy luận tăng tốc hiện tại để khám phá các phương pháp tăng tốc hiệu quả và thân thiện với
tài nguyên hơn cho suy luận LLM. Hy vọng rằng PEARL sẽ tạo điều kiện cho sự phát triển tương
lai của tăng tốc suy luận LLM.

LỜI CẢM ƠN

Các tác giả muốn cảm ơn tất cả các nhà đánh giá ẩn danh vì những nhận xét sâu sắc của họ.

TÀI LIỆU THAM KHẢO

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical
report. arXiv preprint arXiv:2303.08774, 2023.

Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,
Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportu-
nities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.

--- TRANG 10 ---
Xuất bản như một bài báo hội nghị tại ICLR 2025

Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee, Deming Chen, and Tri
Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv
preprint arXiv:2401.10774, 2024.

Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John
Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint
arXiv:2302.01318, 2023.

Hanzhu Chen, Xu Shen, Qitan Lv, Jie Wang, Xiaoqi Ni, and Jieping Ye. Sac-kg: Exploiting large
language models as skilled automatic constructors for domain knowledge graph. In Proceedings
of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pp. 4345-4360, 2024.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

Krishna Teja Chitty-Venkata and Arun K Somani. Neural architecture search survey: A hardware
perspective. ACM Computing Surveys, 55(4):1-36, 2022.

Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srini-
vasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural
networks. arXiv preprint arXiv:1805.06085, 2018.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to
solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-
efficient exact attention with io-awareness. Advances in Neural Information Processing Systems,
35:16344-16359, 2022.

DeepSeek-AI. Deepseek-v3 technical report, 2024. URL https://arxiv.org/abs/2412.
19437.

Cunxiao Du, Jing Jiang, Xu Yuanchen, Jiawei Wu, Sicheng Yu, Yongqi Li, Shenggui Li, Kai Xu,
Liqiang Nie, Zhaopeng Tu, et al. Glide with a cape: A low-hassle method to accelerate speculative
decoding. arXiv preprint arXiv:2402.02082, 2024.

Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha
Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.
arXiv preprint arXiv:2407.21783, 2024.

Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Break the sequential dependency of llm infer-
ence using lookahead decoding. arXiv preprint arXiv:2402.02057, 2024.

Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao
Bi, Y Wu, YK Li, et al. Deepseek-coder: When the large language model meets programming-the
rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024.

Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.

Joao Gante. Assisted generation: a new direction toward low-latency text generation, 2023. URL
https://huggingface.co/blog/assisted-generation.

Diksha Khurana, Aditya Koli, Kiran Khatter, and Sukhdev Singh. Natural language processing:
State of the art, current trends and challenges. Multimedia tools and applications, 82(3):3713-
3744, 2023.

--- TRANG 11 ---
Xuất bản như một bài báo hội nghị tại ICLR 2025

Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.
Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model
serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating
Systems Principles, 2023.

Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative
decoding. In International Conference on Machine Learning, pp. 19274-19286. PMLR, 2023.

Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling requires
rethinking feature uncertainty. arXiv preprint arXiv:2401.15077, 2024.

Tianyu Liu, Qitan Lv, Jie Wang, Shuling Yang, and Hanzhu Chen. Learning rule-induced subgraph
representations for inductive relation prediction. Advances in Neural Information Processing
Systems, 36, 2024.

Qitan Lv, Jie Wang, Hanzhu Chen, Bin Li, Yongdong Zhang, and Feng Wu. Coarse-to-fine high-
lighting: Reducing knowledge hallucination in large language models. In Forty-first International
Conference on Machine Learning, 2024.

Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong,
Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating
generative llm serving with speculative inference and token tree verification. arXiv preprint
arXiv:2305.09781, 2023.

Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi
Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code.
arXiv preprint arXiv:2308.12950, 2023.

Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi,
Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. Language models are multi-
lingual chain-of-thought reasoners, 2022. URL https://arxiv.org/abs/2210.03057.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée
Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and
Zhifang Sui. Unlocking efficiency in large language model inference: A comprehensive survey
of speculative decoding. arXiv preprint arXiv:2401.07851, 2024.

Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, and Sharad Mehrotra. Draft &
verify: Lossless large language model acceleration via self-speculative decoding. arXiv preprint
arXiv:2309.08168, 2023.

Weilin Zhao, Yuxiang Huang, Xu Han, Chaojun Xiao, Zhiyuan Liu, and Maosong Sun. Ouroboros:
Speculative decoding with large model enhanced drafting. arXiv preprint arXiv:2402.13720,
2024.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and
chatbot arena. Advances in Neural Information Processing Systems, 36, 2024.

Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao
Zhang. Distserve: Disaggregating prefill and decoding for goodput-optimized large language
model serving. arXiv preprint arXiv:2401.09670, 2024.

Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh,
Sanjiv Kumar, Jean-François Kagy, and Rishabh Agarwal. Distillspec: Improving speculative
decoding via knowledge distillation. arXiv preprint arXiv:2310.08461, 2023.

Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. A survey on model compression for
large language models. arXiv preprint arXiv:2308.07633, 2023.

--- TRANG 12 ---
Xuất bản như một bài báo hội nghị tại ICLR 2025

A THUẬT TOÁN CỦA PEARL

Ở đây, chúng tôi đưa ra toàn bộ thuật toán của PEARL trong chi tiết trong Thuật toán 2.

Thuật toán 2 Giải mã Đón đoán Song song với Độ dài Thảo luận Thích ứng.
Đầu vào: mô hình thảo luận Mq, mô hình mục tiêu Mp, tiền tố đầu vào x, số token tạo tối đa L,
kích thước cửa sổ gamma.
▷ Chiến lược pre-verify được sử dụng đầu tiên.
1: Khởi tạo: mode ←"pre-verify"
2: while len(x)< L do
3: ifmode = "pre-verify" then
4: ▷ Chiến lược Pre-verify
5: fori= 1togammado
6: qi←Mq(x+ [x1, ..., xi−1])
7: xi∼qi
8: end for
9: ▷ chạy mô hình mục tiêu song song để xác minh token thảo luận đầu tiên trước.
10: p←Mp(x)
11: ifr∼U(0,1)<=p[x1]/q1[x1]then
12: ✓ chấp nhận token đầu tiên
13: x←x+ [x1, ..., xgamma]
14: mode←"post-verify"
15: else
16: × từ chối token đầu tiên
17: y∼norm (max(0, p−q1))
18: x←x+ [y]
19: mode←"pre-verify"
20: end if
21: else
22: ▷ Chiến lược Post-verify
23: x,[x1, x2, ..., xgamma]←x ▷ tách tiền tố để lấy gamma token thảo luận cuối cùng
24: fori=gamma+ 1to2gammado
25: ▷ chạy mô hình thảo luận song song để tiếp tục thảo luận.
26: qi←Mq(x+ [x1, ..., xi−1])
27: xi∼qi
28: end for
29: p1, p2, ..., pgamma←Mp(x+ [x1]), Mp(x+ [x1, x2]), ..., Mp(x+ [x1, ..., xgamma])
30: truy xuất q1, q2, ..., qgamma từ cache
31: r1∼U(0,1), ..., rgamma∼U(0,1)
32: n←min({i−1|1<=i<=gamma, ri>pi[xi]/qi[xi]} ∪ {gamma})
33: ifn=gammathen
34: ✓ chấp nhận tất cả token thảo luận
35: x←x+ [x1, ..., x2gamma]
36: mode←"post-verify"
37: else
38: × từ chối ai đó
39: y∼norm (max(0, pn+1−qn+1))
40: x←x+ [x1, ..., xn, y]
41: mode←"pre-verify"
42: end if
43: end if
44: end while

--- TRANG 13 ---
Xuất bản như một bài báo hội nghị tại ICLR 2025

B VÍ DỤ PROFILING TỪNG BƯỚC ĐƠN GIẢN

Chúng tôi cung cấp profiling từng bước đơn giản của PEARL với prompt dữ liệu thực "x+y = 4z,
x*y = 4z^2, express x-y in z" trong Bảng 9.

Bảng 9: Profiling từng bước đơn giản của PEARL với prompt "x+y = 4z, x*y = 4z^2, express x-y
in z". Chúng tôi chỉ báo cáo 7 bước đầu tiên để đơn giản. Prompt được chọn từ MT-bench, và
chúng tôi sử dụng Llama 2 7&70b làm cặp mô hình cơ sở.

bước tiền tố đầu vào chế độ
hiện tại đầu ra mô hình
thảo luận đầu ra mô hình
mục tiêu lý do đánh giá tiền tố đầu ra
0 x+y = 4z, x*y = 4z^2,
express x-y in z pre-
verify Great, I'm I great không phải I,
do đó great bị từ
chối, chuyển về
pre-verify I
1 I pre-
verify 'm glad
you ' ' được chấp nhận,
chuyển về post-
verify I'
2 I'm glad you post-
verify are inter-
ested in
exploring m m được chấp nhận,
nhưng glad bị từ
chối, chuyển về
pre-verify I'm happy
3 I'm happy pre-
verify to help
you with
this to to được chấp nhận,
chuyển về post-
verify I'm happy to
4 I'm happy to help
you with this post-
verify equation!
However,
I help you
with 3 token đầu được
chấp nhận, nhưng
this bị từ chối,
chuyển về pre-
verify I'm happy to
help you with
your
5 I'm happy to help
you with your pre-
verify question!
However,
I question question được chấp
nhận, chuyển về
post-verify I'm happy to
help you with
your question
6 I'm happy to help
you with your ques-
tion! However, I post-
verify notice that
the equa-
tion you ! How-
ever, I no-
tice tất cả token thảo
luận trước được
chấp nhận, giữ
post-verify I'm happy
to help you
with your
question!
However, I
notice
7 I'm happy to help
you with your ques-
tion! However, I no-
tice that the equation
you post-
verify provided
is not
correct. that the equation bị từ
chối. chuyển về
pre-verify I'm happy
to help you
with your
question!
However, I
notice that the
equations

chúng tôi đưa ra một số giải thích về toàn bộ quá trình.
1) Tại bước 0, prompt "x+y = 4z, x*y = 4z^2, express x-y in z" được nhập vào mô hình thảo luận
và mô hình mục tiêu đồng thời với chiến lược pre-verify. Trong phần còn lại của giải thích này,
chúng tôi bỏ qua prompt gốc này để đơn giản. Mô hình thảo luận xuất ra [Great, I'm], trong khi
mô hình mục tiêu xuất ra [I]. Sau đó, chúng ta sẽ sử dụng [I] để xác minh token đầu tiên [Great]
trong các token thảo luận. Vì nó không giống nhau, chúng ta từ chối các token thảo luận, tiết kiệm
một giai đoạn xác minh của các token thảo luận khác để tăng tốc, và tiền tố đầu ra là [I]. Vì tồn
tại token bị từ chối, chiến lược tiếp theo vẫn là pre-verify.

--- TRANG 14 ---
Xuất bản như một bài báo hội nghị tại ICLR 2025

2) Tại bước 1, tiền tố [I] được nhập, và mô hình thảo luận xuất ra ['m glad you] và mô hình mục
tiêu xuất ra [']. Lần này, ['] được chấp nhận bởi mô hình mục tiêu, và chiến lược tiếp theo được
điều chỉnh thành post-verify.
3) Tại bước 2, tiền tố cùng với các token thảo luận khác [I'm glad you] được nhập. Mô hình thảo
luận xuất ra [are interested in exploring] trong khi mô hình mục tiêu xuất ra [m], tức là token thảo
luận đầu tiên [m] được chấp nhận, nhưng token thảo luận thứ hai [glad] bị từ chối. Mô hình mục
tiêu bổ sung thêm [happy] vào tiền tố. Vì tồn tại token bị từ chối, chiến lược tiếp theo là pre-verify.
4) Tại bước 3, tiền tố [I'm happy] được nhập, và mô hình thảo luận xuất ra [to help you with this]
và mô hình mục tiêu xuất ra [to]. Lần này, [to] được chấp nhận bởi mô hình mục tiêu, và chiến
lược tiếp theo được điều chỉnh thành post-verify.
5) Tại bước 4, tiền tố cùng với các token thảo luận khác [I'm happy to help you with this] được
nhập. Mô hình thảo luận xuất ra [equation! However, I] trong khi mô hình mục tiêu xuất ra [help
you with], tức là ba token thảo luận đầu tiên [help you with] được chấp nhận, nhưng token thảo
luận thứ tư [you] bị từ chối. Mô hình mục tiêu bổ sung thêm [your] vào tiền tố. Vì tồn tại token bị
từ chối, chiến lược tiếp theo là pre-verify.
6) Tại bước 5, tiền tố [I'm happy to help you with your] được nhập, và mô hình thảo luận xuất ra
[question! However, I] và mô hình mục tiêu xuất ra [question]. Lần này, [question] được chấp
nhận bởi mô hình mục tiêu, và chiến lược tiếp theo được điều chỉnh thành post-verify.
7) Tại bước 6, tiền tố cùng với các token thảo luận khác [I'm happy to help you with your ques-
tion! However, I] được nhập. Mô hình thảo luận xuất ra [notice that the equation you] trong khi
mô hình mục tiêu xuất ra [! However, I notice]. Tất cả token thảo luận đều được chấp nhận, và
chiến lược tiếp theo vẫn là post-verify. Do đó, chúng ta tiết kiệm thời gian forward của mô hình
thảo luận để tăng tốc.
8) Tại bước 7, tiền tố cùng với các token thảo luận khác [I'm happy to help you with your ques-
tion! However, I notice that the equation you] được nhập. Mô hình thảo luận xuất ra [provided is
not correct] trong khi mô hình mục tiêu xuất ra [that the], tức là hai token thảo luận đầu tiên [that
the] được chấp nhận, nhưng token thảo luận thứ ba [equation] bị từ chối. Mô hình mục tiêu bổ
sung thêm [equations] vào tiền tố. Vì tồn tại token bị từ chối, chiến lược tiếp theo là pre-verify.

C CHI TIẾT ĐÁNH GIÁ

C.1 CẤU HÌNH BỘ DỮ LIỆU

Trong các thí nghiệm của chúng tôi, chúng tôi đánh giá hiệu quả của PEARL trên 4 danh mục tác
vụ tạo văn bản, bao gồm tạo mã, lý luận số học, suy luận đa ngôn ngữ và đối thoại nhiều vòng.
Đối với tác vụ tạo mã, chúng tôi sử dụng HumanEval (Chen et al., 2021), một benchmark tạo mã
nổi tiếng gồm 164 mục. Đối với lý luận số học và suy luận đa ngôn ngữ, chúng tôi sử dụng GSM8K
và MGSM (Cobbe et al., 2021; Shi et al.) làm benchmark đánh giá. Vì GSM8K là phiên bản tiếng
Anh của MGSM, chúng tôi báo cáo kết quả của chúng trong cùng một bảng. Đối với GSM8K,
chúng tôi lấy mẫu 100 mục đầu tiên để đánh giá. Đối với 10 danh mục khác trong MGSM, chúng
tôi chọn 10 mục cho mỗi ngôn ngữ. Đối với đối thoại nhiều vòng, chúng tôi sử dụng MT-bench
(Zheng et al., 2024) làm benchmark. Độ dài tạo tối đa của các tác vụ này được đặt tương ứng là
1024, 256, 256 và 256.

C.2 CẤU HÌNH MÔ HÌNH

Chúng tôi chọn một số mô hình đại diện để đánh giá, bao gồm Llama 2 Touvron et al. (2023),
Codellama Roziere et al. (2023), và Deepseek-Coder Guo et al. (2024). Chúng tôi tóm tắt cấu hình
mô hình trong Bảng 10. Trong các thí nghiệm của chúng tôi, tất cả mô hình đều được tải với độ
chính xác bfloat-16. PEARL của chúng tôi không đưa ra bất kỳ huấn luyện bổ sung nào, và trực
tiếp sử dụng những mô hình này để đánh giá thuật toán của chúng tôi. Tốc độ chạy được đo trên
các tác vụ tạo mã.

--- TRANG 15 ---
Xuất bản như một bài báo hội nghị tại ICLR 2025

Bảng 10: Cấu hình mô hình chi tiết.

Mô hình Lớp dim FFN dim tốc độ (tok/s)
Codellama-7B 32 4096 11008 49.34
Codellama-34B 48 8192 22016 18.58
Codellama-70B 80 8192 28672 9.20
Deepseek-1.3B 24 2048 5504 63.20
Deepseek-6.7B 32 4096 11008 50.05
Deepseek-33B 62 7168 19200 17.37
Llama-2-7B 32 4096 11008 49.94
Llama-2-70B 80 8192 28672 9.22
Llama-3.1-8B 32 4096 14336 44.37
Llama-3.1-70B 80 8192 28672 9.00

C.3 CHI TIẾT ĐÁNH GIÁ

Tất cả các thí nghiệm của chúng tôi bao gồm đo độ trễ, nghiên cứu loại bỏ và nghiên cứu trường
hợp đều được tiến hành trên GPU NVIDIA A100-SXM4-80G. Đối với các mô hình có kích thước
1.3B và 7B, chúng tôi đặt chúng trên một A100 duy nhất, trong khi các mô hình 34B được triển
khai trên 2 A100, và các mô hình 70B được triển khai trên 3 A100. Đối với suy luận, chúng tôi sử
dụng batch size 1, thường được sử dụng trong các công trình giải mã đón đoán khác. Đối với các
baseline so sánh, bao gồm Lookahead decoding và Ouroboros, chúng tôi tái tạo kết quả của chúng
trên các tác vụ tạo mã với các tham số mặc định như được mô tả trong bài báo hoặc mã của chúng.
Khi đánh giá những phương pháp này, cấu hình mô hình và việc sử dụng GPU giống như PEARL
của chúng tôi.

Bảng 11: Số lần chạy mô hình của mô hình thảo luận và mô hình mục tiêu với các cấu hình mô
hình khác nhau trên HumanEval

Mô hình Thảo luận (SD) Mô hình Mục tiêu (SD) Mô hình Thảo luận (PEARL) Mô hình Mục tiêu (PEARL)
Deepseek 1.3B&33B 140500 35125 181864 (1.29 ×) 45466 (1.29 ×)
Deepseek 6.7B&33B 128973 42991 174855 (1.35 ×) 58285 (1.36 ×)
Codellama 7B&34B 132054 44018 181020 (1.37 ×) 60340 (1.37 ×)
Codellama 7B&70B 151960 30392 198370 (1.30 ×) 39674 (1.30 ×)
Llama2 7B&70B 175460 35092 248720 (1.41 ×) 49744 (1.42 ×)

Vì PEARL của chúng tôi là framework tăng tốc suy luận song song, chúng tôi triển khai thuật toán
song song trong accelerate, có thể được tối ưu hóa thêm với các kỹ thuật song song khác. Chúng
tôi để điều này làm công việc tương lai tiềm năng để có được tăng tốc nhiều hơn.

D THÊM KẾT QUẢ THÍ NGHIỆM

D.1 KẾT QUẢ ĐÁNH GIÁ CỦA LLAMA 3.1 TRÊN MT-BENCH VÀ MGSM

Như được minh họa trong Mục 4.2, chúng tôi cung cấp thêm kết quả đánh giá của PEARL trong
Bảng 12 và 13 với cả Llama 2 7&70B và Llama 3.1 8&70B. Đáng chú ý, Llama 3.1 là dòng LLM
tiên tiến hơn yêu cầu phiên bản transformers phải lớn hơn 4.43.0. Do đó, chúng tôi không thể tái
tạo kết quả của baseline Ouroboros và Lookahead Decoding.

Bảng 12: Kết quả thí nghiệm đa ngôn ngữ sử dụng Llama 3.1 8B&70B trên GSM8K và MGSM
(Cobbe et al., 2021; Shi et al.). Chúng tôi in đậm kết quả tốt nhất cho mỗi ngôn ngữ.

Phương pháp Tiếng Anh (GSM8K) Bengali Đức Tây Ban Nha Pháp Nhật Nga Swahili Tegulu Thái Trung Quốc Trung bình
Auto Regressive 1.00 × 1.00× 1.00× 1.00× 1.00× 1.00× 1.00× 1.00× 1.00× 1.00× 1.00× 1.00×
Speculative Decoding 2.48 × 2.69× 2.77× 2.64× 2.71× 2.71× 2.72× 2.81× 2.65× 2.71× 2.78× 2.70×
Ours 3.82 × 3.94× 4.00× 3.81× 3.76× 3.94× 3.85× 4.18× 4.10× 3.93× 4.06× 3.95×

--- TRANG 16 ---
[Tiếp tục với phần còn lại của tài liệu...]
