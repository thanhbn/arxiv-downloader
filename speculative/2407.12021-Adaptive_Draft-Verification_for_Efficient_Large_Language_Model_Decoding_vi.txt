Giải mã Kiểm tra-Dự thảo Thích ứng cho Giải mã Mô hình Ngôn ngữ Lớn Hiệu quả

Xukun Liu1, Bowen Lei2, Ruqi Zhang3Dongkuan Xu4
1Đại học Northwestern
2Đại học Texas A&M
3Đại học Purdue
4Đại học Bang North Carolina
xukunliu2025@u.northwestern.edu, bowenlei@stat.tamu.edu, ruqiz@purdue.edu, dxu27@ncsu.edu

Tóm tắt
Giải mã mô hình ngôn ngữ lớn (LLM) bao gồm việc tạo ra một chuỗi token dựa trên ngữ cảnh cho trước, trong đó mỗi token được dự đoán từng cái một sử dụng các xác suất đã học của mô hình. Phương pháp giải mã tự hồi quy thông thường yêu cầu một lượt chuyển tiến riêng biệt qua mô hình cho mỗi token được tạo ra, điều này không hiệu quả về mặt tính toán và đặt ra những thách thức cho việc triển khai LLM trong các tình huống nhạy cảm với độ trễ. Những hạn chế chính của các phương pháp giải mã hiện tại xuất phát từ tính không hiệu quả và yêu cầu tài nguyên của chúng. Các cách tiếp cận hiện có hoặc cần thiết phải tinh chỉnh các mô hình nhỏ hơn, điều này tốn nhiều tài nguyên, hoặc dựa vào các lược đồ truy xuất cố định để xây dựng dự thảo cho các token tiếp theo, thiếu khả năng thích ứng và không thể tổng quát hóa trên các mô hình và ngữ cảnh khác nhau. Để giải quyết những vấn đề này, chúng tôi giới thiệu một phương pháp luận mới được gọi là ADED1, giúp tăng tốc giải mã LLM mà không cần tinh chỉnh. Cách tiếp cận của chúng tôi bao gồm một quy trình kiểm tra-dự thảo thích ứng phát triển theo thời gian để cải thiện hiệu quả. Chúng tôi sử dụng một biểu diễn LLM dựa trên ma trận tri-gram để ước lượng động phân phối đầu ra của LLM, cho phép mô hình điều chỉnh theo xác suất token thay đổi trong quá trình giải mã. Ngoài ra, chúng tôi triển khai một cơ chế xây dựng dự thảo cân bằng hiệu quả giữa khám phá và khai thác, đảm bảo rằng các dự thảo được tạo ra vừa đa dạng vừa gần với phân phối đầu ra thực sự của LLM. Tầm quan trọng của thiết kế này nằm ở khả năng tối ưu hóa phân phối dự thảo một cách thích ứng, dẫn đến giải mã nhanh hơn và chính xác hơn. Thông qua các thí nghiệm mở rộng trên nhiều tập dữ liệu chuẩn và kiến trúc LLM khác nhau, chúng tôi chứng minh rằng ADED tăng tốc quá trình giải mã trong khi duy trì độ chính xác cao, làm cho nó phù hợp để triển khai trong một loạt các ứng dụng thực tế.

Giới thiệu
Giải mã mô hình ngôn ngữ lớn (LLM) bao gồm việc tạo ra một chuỗi token dựa trên ngữ cảnh cho trước, trong đó mỗi token được dự đoán từng cái một sử dụng các xác suất đã học của mô hình (Brown et al. 2020; Zhang et al. 2022; Touvron et al. 2023a,b). Cơ chế cốt lõi là tự hồi quy, trong đó mỗi token mới được tạo ra có điều kiện trên các token đã tạo ra trước đó và ngữ cảnh cho trước. Quá trình này rất quan trọng cho các ứng dụng như tạo văn bản (Li et al. 2024a; Peng et al. 2023; Chang et al. 2023), dịch máy (Zhang, Haddow, and Birch 2023; Moslem et al. 2023; Hendy et al. 2023), và AI hội thoại (Shanahan 2024; Wu et al. 2023; Saka et al. 2023). Tuy nhiên, mỗi bước giải mã đều bao gồm một lượt chuyển tiến qua mô hình, làm cho quá trình vốn dĩ tuần tự và tốn kém về mặt tính toán. Sự không hiệu quả phát sinh do nhu cầu phải tải lại mô hình cho mỗi dự đoán token, dẫn đến chi phí tính toán cao và sử dụng băng thông bộ nhớ. Bản chất tuần tự này của giải mã là một nút thắt cổ chai đáng kể, đặc biệt đối với các ứng dụng thời gian thực (Liu et al. 2023a; Mandvikar 2023; Antoniol et al. 1994) nơi độ trễ là quan trọng. Do đó, việc tối ưu hóa tốc độ giải mã của LLM là cần thiết cho việc triển khai thực tế.

Nghiên cứu gần đây đã khám phá nhiều chiến lược khác nhau để giảm thiểu sự không hiệu quả của giải mã LLM. Giải mã Suy đoán (Leviathan, Kalman, and Matias 2023; Spector and Re 2023; Chen et al. 2023) giới thiệu một cách tiếp cận trong đó một mô hình nhỏ hơn, hiệu quả hơn tạo ra nhiều dự đoán token song song, sau đó được xác minh bởi mô hình mục tiêu lớn hơn. Phương pháp này tận dụng hiệu quả của các mô hình nhỏ hơn để giảm số lượng lượt chuyển tiến tuần tự cần thiết, đạt được tăng tốc đáng kể mà không thay đổi phân phối đầu ra. Giải mã Nhìn trước (Fu et al. 2024a) sử dụng toàn bộ ngữ cảnh để dự đoán nhiều token tương lai, tạo ra một bộ đệm giảm sự phụ thuộc vào xử lý tuần tự. REST (He et al. 2024) sử dụng cách tiếp cận dựa trên truy xuất trong đó các token liên quan được lấy từ một kho dữ liệu được xây dựng sẵn sử dụng ngữ cảnh hiện tại, tạo thành các dự thảo được xác minh bởi LLM. Các phương pháp này có thể được tóm tắt trong pipeline kiểm tra-dự thảo, như được hiển thị trong Hình 1.

Giải mã Suy đoán và Giải mã Nhìn trước đều tạo ra các token dự thảo thông qua các mô hình dự đoán, trong khi REST xây dựng dự thảo từ các token được truy xuất dựa trên ngữ cảnh. Trong mỗi trường hợp, các dự thảo sau đó được xác minh bởi LLM chính, đảm bảo rằng đầu ra cuối cùng tuân thủ các xác suất đã học của mô hình. Bất chấp những tiến bộ của chúng, các cách tiếp cận này đối mặt với những hạn chế đáng chú ý. Chúng thường yêu cầu đào tạo hoặc tinh chỉnh bổ sung, có thể tốn nhiều tài nguyên. Các lược đồ truy xuất cố định thiếu khả năng thích ứng, làm cho việc điều chỉnh phân phối dự thảo theo thời gian thực dựa trên đầu ra LLM đang phát triển trở nên thách thức. Ngoài ra, các phương pháp này có thể không tổng quát hóa tốt trên các mô hình và ngữ cảnh khác nhau, hạn chế hiệu quả của chúng trong môi trường động.

Trong công trình này, trọng tâm của chúng tôi là kiểm tra-dự thảo không cần tinh chỉnh để giải quyết những hạn chế này. Pipeline kiểm tra-dự thảo có thể được xem như một quy trình lấy mẫu từ chối trong đó sự tương tự giữa phân phối đề xuất (dự thảo) và phân phối mục tiêu (đầu ra LLM) là quan trọng cho tỷ lệ chấp nhận và tốc độ hội tụ. Sự tương tự cao hơn dẫn đến tỷ lệ chấp nhận cao hơn và tốc độ giải mã nhanh hơn. Rất ít cách tiếp cận không cần tinh chỉnh, ví dụ REST (He et al. 2024), thường sử dụng các lược đồ dựa trên truy xuất cố định để xây dựng dự thảo. Các lược đồ này thiếu khả năng thích ứng để điều chỉnh phân phối dự thảo dựa trên phân phối đầu ra LLM đang phát triển, dẫn đến một khoảng cách dai dẳng giữa dự thảo và đầu ra LLM thực tế. Khoảng cách này làm giảm tỷ lệ chấp nhận dự thảo và hạn chế tiềm năng cải thiện tốc độ giải mã.

Để giải quyết vấn đề này, chúng tôi đặt ra câu hỏi nghiên cứu sau:
Câu hỏi Nghiên cứu: Làm thế nào để thiết kế một quy trình xây dựng dự thảo thích ứng có thể phát triển bản thân và ước lượng chính xác đầu ra LLM trong quá trình giải mã?

Để giới thiệu khả năng thích ứng và tìm ra các dự thảo ngày càng gần với phân phối đầu ra LLM trong quá trình giải mã, chúng tôi không chỉ cần có một pipeline xây dựng dự thảo thích ứng mà còn cần duy trì sự cân bằng giữa khám phá và khai thác. Sự cân bằng này đảm bảo rằng việc tăng tốc có thể đạt được bằng cách tận dụng kiến thức hiện có về xây dựng dự thảo trong khi liên tục khám phá khả năng xây dựng dự thảo tốt hơn. Để đạt được điều này, chúng tôi đề xuất một phương pháp luận mới được gọi là ADED (Kiểm tra-Dự thảo Thích ứng cho Giải mã LLM Hiệu quả). ADED kết hợp một đại diện LLM thích ứng dựa trên ma trận tri-gram để kiểm soát phân phối xác suất có điều kiện của token tiếp theo, có thể được cập nhật trong quá trình giải mã để điều chỉnh việc xây dựng dự thảo tương ứng. Để cân bằng khám phá và khai thác, chúng tôi thiết kế một bộ tạo dự thảo lấy cảm hứng từ Tìm kiếm Cây Monte Carlo (MCTS)(Coulom 2007; Browne et al. 2012; James, Konidaris, and Rosman 2017; Świechowski et al. 2023). Bộ tạo dự thảo này sử dụng điểm ưu tiên token để duy trì sự cân bằng trong quá trình tìm kiếm. Điểm số bao gồm hai phần: phần đầu tiên dựa trên phân phối xác suất có điều kiện ước lượng của token tiếp theo thu được từ đại diện LLM, phản ánh kiến thức hiện tại của bộ tạo dự thảo về đầu ra LLM; phần thứ hai khuyến khích bộ tạo dự thảo khám phá các không gian dự thảo chưa được khám phá hoặc ít được khám phá. Về mặt lý thuyết, chúng tôi chỉ ra rằng phương pháp của chúng tôi có thể được xem như một bài toán tối ưu hóa có ràng buộc để khuyến khích phân phối dự thảo hội tụ về phân phối đầu ra LLM. Sử dụng điểm ưu tiên token, bộ tạo dự thảo có thể tìm kiếm hiệu quả không gian dự thảo và tạo ra các token ứng cử viên. Sau khi việc xây dựng và xác minh dự thảo hoàn tất, thông tin được phản hồi lại cho đại diện LLM để cập nhật ước lượng của nó về đầu ra LLM. Vòng phản hồi này làm phong phú kiến thức của bộ tạo dự thảo trong các vòng kiểm tra-dự thảo tiếp theo, cho phép khả năng thích ứng và tự cải thiện trong quy trình xây dựng dự thảo.

Tóm lại, đóng góp của chúng tôi được kết luận như sau:
• Chúng tôi thiết kế một biểu diễn dựa trên ma trận tri-gram động ước lượng phân phối đầu ra LLM, tăng cường khả năng thích ứng mà không cần tinh chỉnh. Nó giải quyết hạn chế của các lược đồ truy xuất cố định bằng cách liên tục phát triển với các dự đoán của mô hình.
• Chúng tôi phát triển một bộ tạo dự thảo cân bằng hiệu quả giữa khám phá và khai thác để tạo ra các dự thảo chất lượng cao. Cơ chế này cải thiện tốc độ và độ chính xác giải mã bằng cách đảm bảo rằng các dự thảo được căn chỉnh chặt chẽ với phân phối đầu ra của LLM. Các thí nghiệm của chúng tôi cho thấy cải thiện 2.5 lần về tốc độ giải mã so với các baseline.
• Thông qua các thí nghiệm mở rộng trên nhiều tập dữ liệu chuẩn và kiến trúc LLM khác nhau, chúng tôi chứng minh rằng ADED thành công trong việc tăng tốc quá trình giải mã trong khi duy trì độ chính xác cao. Cụ thể, chúng tôi đạt được tăng tốc lên đến 2.5 lần về độ trễ và cải thiện tỷ lệ chấp nhận trung bình 20% so với các phương pháp hiện có.
• Khả năng thích ứng với đầu ra LLM đang phát triển và liên tục tinh chỉnh việc xây dựng dự thảo của phương pháp chúng tôi làm cho nó khác biệt so với các phương pháp hiện có, giải quyết nhu cầu về các giải pháp giải mã linh hoạt và động hơn.

Phương pháp luận
Chúng tôi đề xuất một phương pháp giải mã LLM kiểm tra-dự thảo nhanh không cần tinh chỉnh mới bằng cách giới thiệu khả năng thích ứng vào việc giải mã và học từ LLM, được minh họa trong Hình 2. Các thuật toán giải mã tăng tốc hiện có hoặc yêu cầu tinh chỉnh bổ sung hoặc thiếu khả năng thích ứng với phân phối đầu ra của LLM, dẫn đến chi phí bổ sung hoặc tăng tốc không đủ. Để giải quyết những vấn đề này, chúng tôi thiết kế một biểu diễn LLM thích ứng dựa trên ma trận tri-gram để ước lượng thích ứng phân phối đầu ra của LLM, phát triển một bộ tạo dự thảo cân bằng khám phá và khai thác để tự cải thiện hướng tới các dự thảo chất lượng cao, và xác minh các dự thảo sử dụng tree attention.

Kiến thức cơ bản
Giải mã suy đoán là một phương pháp để tăng tốc suy luận mô hình ngôn ngữ bằng cách sử dụng một mô hình phụ nhỏ hơn để tạo ra một chuỗi dự thảo, giảm tải tính toán trên mô hình lớn hơn (Leviathan, Kalman, and Matias 2023). Giải mã suy đoán dựa trên truy xuất mở rộng điều này bằng cách kết hợp một hệ thống truy xuất thay vì mô hình nhỏ hơn, tận dụng các đoạn corpus được lưu trữ sẵn để tạo văn bản liên quan. Tìm kiếm Cây Monte Carlo (MCTS) (Coulom 2007; Browne et al. 2012; James, Konidaris, and Rosman 2017; Świechowski et al. 2023) là một thuật toán tối ưu hóa việc ra quyết định bằng cách cân bằng khám phá và khai thác các trạng thái tương lai. Nó chọn các nút để khám phá thêm sử dụng kết hợp số lần thăm nút và giá trị ước lượng, nhằm tối đa hóa kết quả tổng thể. Để thảo luận toàn diện về các phương pháp này, vui lòng tham khảo Phụ lục E.

Đại diện LLM Thích ứng
Để ước lượng phân phối token đầu ra của LLM mà không cần tinh chỉnh mô hình nhỏ, chúng tôi chưng cất kiến thức ngôn ngữ từ một corpus nhỏ và xây dựng một ma trận tri-gram như một biểu diễn ban đầu của LLM, cho phép chúng tôi tận dụng các quy luật thống kê của ngôn ngữ ở mức độ chi tiết. Cụ thể, chúng tôi tóm tắt và đếm mỗi tập hợp ba token xuất hiện trong corpus và tính toán xác suất của token thứ ba xuất hiện có điều kiện trên hai token đầu tiên. Công thức được định nghĩa trong Eq. (1):

P(wi|wi−2, wi−1) = C(wi−2, wi−1, wi) / C(wi−2, wi−1), (1)

trong đó P(wi|wi−2, wi−1) là xác suất có điều kiện của một từ wi cho trước hai từ đi trước wi−2 và wi−1, C(wi−2, wi−1, wi) là số lần xuất hiện của tri-gram trong corpus, và C(wi−2, wi−1) là số lần xuất hiện của bi-gram đi trước (Mori, Nishimura, and Itoh 1998).

Bằng cách này, chúng tôi có thể thu được một đại diện LLM ban đầu tốt với chi phí thấp hơn nhiều, có thể tạo ra một phân phối ước lượng của token tiếp theo dựa trên các token trước đó. Đại diện LLM này sẽ hợp tác với bộ tạo dự thảo của chúng tôi để tạo ra các dự thảo và nhận phản hồi để cập nhật ma trận tri-gram cho khả năng thích ứng và tự cải thiện. Vui lòng xem Phần 2.3 để biết thêm chi tiết.

Bộ tạo Dự thảo và Tự cải thiện
Với sự trợ giúp của đại diện LLM, chúng tôi tiếp tục đề xuất một bộ tạo dự thảo cân bằng khám phá và khai thác trong khi tìm kiếm các dự thảo ứng cử viên gần hơn với đầu ra LLM. Một mặt, bộ tạo dự thảo tận dụng các xác suất có điều kiện từ đại diện LLM, bao gồm kiến thức hiện tại về đầu ra LLM. Mặt khác, bộ tạo dự thảo được khuyến khích tìm kiếm nhiều hơn trong không gian dự thảo chưa được khám phá hoặc ít được khám phá để tìm các ứng cử viên dự thảo tốt hơn. Sau đó, với phản hồi từ đầu ra LLM, đại diện LLM có thể cập nhật hiểu biết của mình về đầu ra LLM, cải thiện tìm kiếm của bộ tạo dự thảo, và đạt được tự cải thiện. Chi tiết được cung cấp dưới đây.

Điểm Tìm kiếm Dự thảo: Cho các token ban đầu, chúng tôi khai thác Tìm kiếm Cây Monte Carlo (MCTS) (Coulom 2007) để hướng dẫn quá trình tìm kiếm các dự thảo của các token tiếp theo, trong đó chúng tôi ưu tiên các token ứng cử viên theo xác suất có điều kiện từ đại diện LLM dựa trên ma trận tri-gram và số lần thăm nút trong quá trình tìm kiếm cây. Điểm số đóng vai trò quan trọng trong việc cân bằng khám phá và sử dụng trong tìm kiếm cây Monte Carlo và được định nghĩa như Eq. (2).

PUCT (s, a) = Q(s, a) + E·P(s, a)·√(Σ_b N(s, b)) / (1 + N(s, a)). (2)

Thiết kế điểm số được thúc đẩy bởi Điểm PUCT (Rosin 2011; Silver et al. 2017). Cụ thể, Q(s, a) đánh giá chất lượng của việc thực hiện hành động a trong trạng thái s, trong khi P(s, a) đại diện cho xác suất tiên nghiệm của việc chọn hành động a trong trạng thái s. Thuật ngữ N(s, a) biểu thị số lần hành động a đã được thực hiện từ trạng thái s, và Σ_b N(s, b) tính tổng số lần đếm cho tất cả các hành động từ trạng thái s. Eq. (3) đóng vai trò quan trọng trong việc xác định sự cân bằng giữa khám phá và khai thác trong framework MCTS.

E = C1 + log(Σ_b N(s, b)) + C2 + 1 / C2, (3)

Hằng số C1 hoạt động như một điều chỉnh mức cơ bản, trong khi C2 điều chế thuật ngữ logarithm để mở rộng yếu tố khám phá một cách động dựa trên tổng số lần thăm. Công thức này đảm bảo rằng các lựa chọn dự thảo của chúng tôi phù hợp với ngữ cảnh và tối ưu hóa tính mạnh mẽ và tính nhất quán của việc tạo văn bản.

Chiến lược Chuyển giao Tự cải thiện: Dựa trên điểm tìm kiếm cuối cùng thu được trong quá trình tìm kiếm, chúng tôi có thể xây dựng các ứng cử viên dự thảo và xác minh chúng để có được đầu ra giải mã cuối cùng (vui lòng xem Phần 2.4) và phản hồi lại để tự cải thiện. Đầu ra giải mã cuối cùng này đại diện cho phân phối đầu ra của LLM, sẽ là tài liệu học tập tốt cho đại diện LLM. Do đó, chúng tôi đưa kiến thức này vào đại diện LLM để có được các phân phối xác suất có điều kiện được cập nhật, từ đó cung cấp cho bộ tạo dự thảo kiến thức chính xác và có thể khai thác hơn, được minh họa trong Hình 2. Cụ thể, kỹ thuật này hoạt động bằng cách đầu tiên trích xuất tri-gram từ các đầu ra gần đây của LLM. Tần suất của mỗi tri-gram sau đó được sử dụng để cập nhật xác suất của nó như đầu ra tiềm năng. Các xác suất điều chỉnh này được đưa vào MCTS như một phần của mạng chính sách, ảnh hưởng đến giai đoạn lựa chọn của tìm kiếm cây. Các xác suất tri-gram được cập nhật về cơ bản phục vụ như một hướng dẫn chính sách động, tăng cường khả năng của mô hình để tạo ra các chuỗi liên quan về ngữ cảnh và nhất quán. Bằng cách kết hợp các xác suất tri-gram đã học vào thuật toán tìm kiếm cây, chúng tôi hiệu quả tạo ra một vòng phản hồi trong đó chính chiến lược tìm kiếm phát triển theo thời gian. Điều chỉnh chiến lược này được thực hiện bằng cách hiệu chỉnh lại sự cân bằng khám phá-khai thác dựa trên dữ liệu thực nghiệm được suy ra từ các đầu ra của chính mô hình.

Xây dựng và Xác minh Dự thảo
Điều quan trọng cần lưu ý là các dự thảo ứng cử viên được tạo ra bởi bộ tạo dự thảo thường có các đoạn bắt đầu chung có thể gây ra việc tính toán lại dư thừa trong các lớp Transformer nếu không được quản lý đúng cách. Để giải quyết vấn đề này, một chuỗi giả đảm bảo rằng mỗi dự thảo là một chuỗi con và bất kỳ tiền tố chung nào chỉ xuất hiện một lần được tạo ra (He et al. 2024). Được thúc đẩy bởi quan sát này, chúng tôi sử dụng một mặt nạ attention cụ thể cho mỗi lớp attention, được gọi là tree attention (Miao et al. 2023; Cai et al. 2024). Mặt nạ này căn chỉnh các tính toán cho mỗi token với các phụ thuộc của nó theo chuỗi dự thảo gốc, bảo toàn tính toàn vẹn ngữ cảnh của dự thảo và ngăn chặn các tính toán không cần thiết. Việc phê duyệt dự thảo dựa vào so sánh với phân phối có điều kiện từ LLM. Tại mỗi vị trí, các token mới được lấy mẫu và so sánh với các token dự thảo. Nếu một token được lấy mẫu tương ứng với token dự thảo, nó được phê duyệt; nếu không, dự thảo bị loại bỏ từ điểm đó. Việc phê duyệt có chọn lọc này đảm bảo rằng chuỗi đầu ra phù hợp với những gì sẽ được tạo ra bởi một quá trình tự hồi quy điển hình, từ đó duy trì tính xác thực của văn bản được tạo ra.

Hiểu biết Lý thuyết: Tại sao ADED sử dụng MCTS
Trong phần này, chúng tôi cung cấp một biện minh lý thuyết cho thiết kế của phương pháp ADED. Chúng tôi chỉ ra rằng tìm kiếm dự thảo trong ADED sử dụng MCTS có thể được xem như một dạng tối ưu hóa chính sách, trong khi cơ chế suy luận của LLM có thể được xem như một dạng tối ưu hóa phạt tương tự.

MCTS trong ADED: Quy trình lựa chọn token trong giải mã ADED có thể được xem như một quá trình lựa chọn hành động. Thuật toán MCTS tối ưu hóa chính sách của mình bằng cách lặp đi lặp lại xây dựng một cây tìm kiếm và cập nhật số lần thăm cho mỗi nút (cặp trạng thái-hành động) dựa trên các đường dẫn tìm kiếm. Phân phối số lần thăm π̂(a|x) được định nghĩa như:

π̂(a|x) ≜ (1 + n(x, a)) / (|A| + Σ_b n(x, b)), (4)

trong đó n(x, a) đại diện cho số lần thăm cho hành động a trong trạng thái x, và |A| đại diện cho tổng số hành động có thể ở trạng thái x. Sau đó, việc lựa chọn hành động trong MCTS có thể được viết như việc chọn hành động a*:

a*(x) ≜ arg max_a [Q(x, a) + λN·πθ(a|x) / π̂(a|x)] (5)

Theo (Grill et al. 2020), chúng tôi sử dụng q ∈ R|A| để biểu thị vector của hàm Q Q(x, a). Với việc lựa chọn thích hợp các siêu tham số, thuật toán MCTS có thể được xem như tìm kiếm giải pháp tối ưu cho một bài toán tối ưu hóa chính sách (Grill et al. 2020) như dưới đây:

π̄ ≜ arg max_{y∈S} q^T y - λN KL[πθ, y], (6)

trong đó S là simplex |A|-chiều, λN là một tham số regularization phụ thuộc vào các siêu tham số và cân bằng khám phá và khai thác, và KL là độ phân kỳ KL.

Cơ chế Suy luận LLM: Các mô hình ngôn ngữ lớn, đặc biệt là những mô hình dựa trên kiến trúc Transformer, tạo ra văn bản bằng cách dự đoán phân phối xác suất của token tiếp theo cho trước các token trước đó. Trong quá trình đào tạo, mô hình tối đa hóa log-likelihood của dữ liệu quan sát được, tương đương với việc tối thiểu hóa cross-entropy loss:

L(θ) = -Σ_{t=1}^T log P(wt|w1:t−1;θ) + λ/2 ∥θ∥²₂, (7)

trong đó P biểu thị xác suất có điều kiện của LLM, w biểu thị các token, và θ biểu thị các tham số mô hình.

Phân tích So sánh: Như được hiển thị trong Eq (6) và Eq. (7), cả MCTS và LLM đều có thể được xem như các bài toán tối ưu hóa có regularization để lựa chọn phân phối của các token tiếp theo. Một mặt, hàm Q trong MCTS cho ADED có thể được xem như một ước lượng cho log-likelihood của LLM:

Q(x, a) = -Σ_{t=2}^T log P̂(wt|wt−1, wt−2;θ)
≈ log P(w0, w1,···, wT;θ)
= -Σ_{t=2}^T log P(wt|w1:t−1;θ), (8)

trong đó P̂ và P là phân phối xác suất có điều kiện từ đại diện LLM dựa trên ma trận tri-gram và LLM, tương ứng. Mặt khác, cả MCTS và LLM đều sử dụng regularization để cải thiện quy trình tối ưu hóa. Kết quả là, chúng tôi xác minh sự tương tự giữa MCTS và Suy luận LLM về mặt tối ưu hóa và regularization.

Thí nghiệm

Thiết lập Thí nghiệm
Mô hình và Tập dữ liệu. Chúng tôi tiến hành một loạt thí nghiệm với năm mô hình khác biệt trên ba tập dữ liệu để đánh giá hiệu quả của ADED. Cụ thể, chúng tôi sử dụng ba mô hình Vicuna (Chiang et al. 2023) (7B, 13B, 33B) và hai mô hình LLaMA2-chat (Touvron et al. 2023b) (7B, 13B) để đánh giá khả năng tăng tốc trên các kích thước và loại mô hình khác nhau. Đánh giá của chúng tôi kết hợp HumanEval (Chen et al. 2021), MT-Bench (Zheng et al. 2023), và tập dữ liệu Alpaca (Taori et al. 2023) để xác định năng lực hiểu và tạo ngôn ngữ tự nhiên tổng quát. Các tập dữ liệu này được lựa chọn một cách tỉ mỉ để đảm bảo phân tích toàn diện về các kỹ thuật tăng tốc trên nhiều nhiệm vụ khác nhau.

Corpus. Chúng tôi xây dựng hai corpus. Corpus đầu tiên được xây dựng sử dụng một phần mã Python pre-training từ The Stack (Kocetkov et al. 2022), bao gồm khoảng 2.7M mẫu mã Python với kích thước kết quả là 1007MB. Corpus thứ hai được xây dựng sử dụng dữ liệu được suy ra từ UltraChat (Ding et al. 2023), bao gồm khoảng 774K cuộc hội thoại ChatGPT, tạo ra một corpus với kích thước 574MB. Các thí nghiệm trên MT-Bench và Alpaca được tiến hành sử dụng corpus UltraChat, trong khi benchmark Human-Eval sử dụng corpus từ The Stack.

Metrics. Để đánh giá hiệu suất tăng tốc trên các mô hình ngôn ngữ lớn, chúng tôi sử dụng hai metrics chính: tỷ lệ tăng tốc và độ dài chấp nhận trung bình. Tỷ lệ tăng tốc, được tính như tỷ lệ thời gian cần thiết bởi các mô hình baseline để hoàn thành các nhiệm vụ suy luận mà không có tăng tốc so với thời gian cần thiết bởi ADED của chúng tôi, đo lường lợi ích hiệu quả được giới thiệu bởi thuật toán. Metric thứ hai, độ dài chấp nhận trung bình, đo lường số lượng token trung bình được chấp nhận mỗi lần chuyển tiến bởi các mô hình ngôn ngữ lớn mục tiêu, loại trừ bất kỳ overhead nào của việc truy xuất và xây dựng các token dự thảo, chỉ ra tăng tốc tối đa có thể.

Baselines. Chúng tôi so sánh các cách tiếp cận cơ bản khác nhau để cải thiện tốc độ giải mã của các mô hình ngôn ngữ lớn. Chúng tôi kiểm tra Lookahead Decoding (Fu et al. 2024a), một thuật toán giải mã chính xác và song song giảm độ trễ mà không dựa vào các mô hình dự thảo. Chúng tôi so sánh REST (He et al. 2024) (Retrieval-Based Speculative Decoding), áp dụng chiến lược dựa trên truy xuất để tạo các token dự thảo, trái ngược với các phương pháp giải mã suy đoán thông thường dựa vào mô hình dự thảo. Để công bằng trong so sánh, chúng tôi bao gồm REST Single, một phiên bản đơn luồng của REST, để đánh giá hiệu suất trong điều kiện xử lý hạn chế. Chúng tôi cũng bao gồm phương pháp Autoregressive truyền thống, đại diện cho cách tiếp cận giải mã tiêu chuẩn, phục vụ như một baseline để làm nổi bật những cải thiện được cung cấp bởi các phương pháp khác. Tất cả các thí nghiệm được tiến hành trên GPU NVIDIA A6000, ngoại trừ mô hình 33B, sử dụng NVIDIA H100. Các thí nghiệm mặc định cho Greedy sampling.

Cấu hình của ADED. Để đảm bảo khả năng tái tạo, chúng tôi cung cấp các siêu tham số chi tiết, corpus được sử dụng, và môi trường runtime cho mỗi bộ thí nghiệm trong Phụ lục F. Điều này cho phép các nhà nghiên cứu quan tâm tái tạo chính xác các phát hiện của chúng tôi và so sánh chúng với các thiết lập của họ. Do hạn chế về không gian, những chi tiết này không được bao gồm trong văn bản chính nhưng được ghi chép toàn diện trong phụ lục.

Kết quả Chính
Trong các thí nghiệm, chúng tôi so sánh hiệu quả của các baseline khác nhau được áp dụng cho các mô hình khác nhau, sử dụng ba tập dữ liệu: MT-Bench, Human-Eval, và Alpaca. Chúng tôi tập trung vào các metrics của Độ dài Chấp nhận, Độ trễ, và Tỷ lệ Tăng tốc. Bảng 1 tóm tắt độ trễ và độ dài chấp nhận trung bình trên ba tập dữ liệu. ADED nhất quán chứng minh độ trễ thấp hơn, đặc biệt cho các mô hình vicuna-7B và llama2-13B. Ví dụ, trên MT-Bench, ADED đạt được độ trễ 12.95 ms cho vicuna-7B, thấp hơn REST (16.31 ms), REST Single Thread (17.36 ms), và Lookahead (18.93 ms). Đáng chú ý, bộ nhớ cần thiết cho ADED (574MB) chỉ là 5.6% so với yêu cầu của REST (12GB). Theo Bảng 2, ngay cả khi ADED sử dụng một corpus nhỏ hơn (253MB, chỉ 2.5% yêu cầu của REST), nó vẫn đạt được độ trễ thấp hơn REST. Xu hướng này cũng được quan sát trên Alpaca, nơi ADED đạt được độ trễ 12.81 ms cho vicuna-7B, so với 14.24 ms cho REST, 14.58 ms cho REST Single Thread, và 18.73 ms cho Lookahead.

Kết quả độ dài chấp nhận trong Bảng 1 chỉ ra chất lượng của các đầu ra được tạo ra, với độ dài chấp nhận dài hơn gợi ý văn bản nhất quán và liên quan về ngữ cảnh hơn. Phương pháp của chúng tôi, ADED, vượt trội hơn các phương pháp khác trên các mô hình khác nhau trên cả tập dữ liệu MT-Bench và Alpaca. Ví dụ, trên MT-Bench, ADED đạt được độ dài chấp nhận cao nhất cho các mô hình vicuna-33B và llama2-13B, thể hiện khả năng tạo ngôn ngữ vượt trội.

Tỷ lệ tăng tốc được sử dụng để đánh giá hiệu quả. ADED nhất quán cho thấy cải thiện đáng kể về tốc độ trên tất cả các tập dữ liệu trong Hình 3. Hiệu quả này đáng chú ý trên các tập dữ liệu MT-Bench, Alpaca và Human-Eval, nơi ADED không chỉ giảm độ trễ mà còn tăng cường tốc độ xử lý tổng thể. Ví dụ, ADED đạt được tăng tốc 1.92x trên MT-Bench với mô hình vicuna-13B, vượt trội hơn REST, REST Single Thread, và Lookahead. Trên tập dữ liệu HumanEval, mô hình vicuna-33B, chẳng hạn, chứng minh tăng tốc gần 2.5x khi sử dụng ADED.

Tính ổn định của ADED
Trong phần này, chúng tôi phân tích tính ổn định của ADED trên các danh mục nhiệm vụ khác nhau. Các danh mục bao gồm writing, roleplay, reasoning, math, coding, extraction, STEM, và humanities. Kết quả thí nghiệm được hiển thị trong Hình 4b chỉ ra rằng ADED duy trì hiệu suất nhất quán trên các danh mục. Độ dài chấp nhận trung bình vẫn ổn định, chứng minh rằng ADED có thể xử lý hiệu quả một loạt các nhiệm vụ đa dạng mà không có biến thiên đáng kể trong hiệu suất.

Để đánh giá thêm tính mạnh mẽ của ADED, chúng tôi kiểm tra ảnh hưởng của việc thay đổi các tham số top-p và temperature đối với hiệu suất. Hình 5a và 5b tóm tắt tác động của các tham số này đối với độ dài chấp nhận trung bình. Cụ thể, Hình 5a cho thấy rằng những thay đổi của top-p không ảnh hưởng đáng kể đến hiệu suất của ADED. Độ dài chấp nhận trung bình vẫn tương đối ổn định trên các giá trị khác nhau của top-p, cho thấy rằng ADED không quá nhạy cảm với tham số này. Hình 5b chứng minh rằng các biến thiên trong tham số temperature có tác động không đáng kể đến hiệu suất của ADED. Tính nhất quán trong độ dài chấp nhận trung bình trên các giá trị temperature khác nhau tiếp tục hỗ trợ tính mạnh mẽ.

Nghiên cứu Loại bỏ
Để có hiểu biết sâu sắc về phương pháp của chúng tôi, chúng tôi tiến hành một loạt nghiên cứu loại bỏ. Nghiên cứu đầy đủ được tóm tắt trong Phụ lục D.

Tác động của chiến lược thích ứng. Hình 4a minh họa hiệu suất của chiến lược thích ứng của chúng tôi trên Vicuna-7B, với phân tích độ dài chấp nhận trung bình trên số lượng token khác nhau. Chúng tôi thấy rằng chiến lược thích ứng duy trì độ dài chấp nhận trung bình cao hơn trên toàn bộ phạm vi so với chiến lược không thích ứng. Thành công của chiến lược thích ứng được quy cho việc điều chỉnh động các phân phối xác suất của mô hình dựa trên tần suất tri-gram từ các đầu ra trước đó. Điều này cho phép mô hình quản lý tốt hơn các ngữ cảnh dài hơn và duy trì sự liên quan, tăng cường tính ổn định và tính nhất quán trong các tương tác dài hơn.

Tác động của kích thước corpus. Bảng 2 cho thấy tác động của việc tăng kích thước corpus từ 121k lên 774k đối với các metrics hiệu suất khác nhau. Với việc mở rộng corpus, có sự cải thiện dần dần trong Độ dài Chấp nhận từ 2.30 lên 2.42. Sự tăng này gợi ý rằng các tập dữ liệu lớn hơn cung cấp một loạt các mẫu ngôn ngữ rộng hơn, tăng cường khả năng của mô hình để tạo ra các đầu ra nhất quán và liên quan về ngữ cảnh hơn. Bất chấp việc tăng kích thước dữ liệu từ 253 MB lên 574 MB, hệ thống duy trì khả năng xử lý dữ liệu hiệu quả. Những khác biệt nhỏ trong độ trễ khẳng định hiệu suất nhất quán của ADED, ngay cả với kích thước corpus nhỏ hơn, điều này tiếp tục mở rộng tiềm năng sử dụng trên các thiết bị hạn chế tài nguyên. Sự tăng nhẹ trong thời gian truy xuất nhấn mạnh hiệu quả của các thuật toán truy xuất, có thể quản lý các tập dữ liệu lớn hơn mà không làm tổn hại đáng kể tốc độ phản hồi. Tóm lại, kết quả cho thấy rằng kích thước corpus lớn hơn có thể cải thiện chất lượng đầu ra của mô hình trong khi duy trì hiệu suất hệ thống tốt.

Tác động của MCTS. Hình 6 trình bày kết quả cho các mô hình Vicuna-7B và Vicuna-13B trên tập dữ liệu MT-Bench, cho thấy tác động của số lần tìm kiếm MCTS khác nhau đối với hiệu suất. Chúng tôi thấy rằng đối với cả hai mô hình, việc tăng số lần tìm kiếm cải thiện hiệu suất, trong khi số lượng tối ưu khác nhau theo kích thước mô hình. Độ dài exact trung bình và độ trễ được vẽ theo số lần tìm kiếm, minh họa sự cân bằng giữa hiệu suất và chi phí tính toán. Như được hiển thị trong hình, có sự cải thiện đáng chú ý trong độ dài exact trung bình khi số lần tìm kiếm tăng, cùng với sự tăng độ trễ, cho thấy sự cân bằng giữa độ sâu tìm kiếm và thời gian tạo. Chúng tôi tiếp tục so sánh tìm kiếm greedy với MCTS (duyệt đầy đủ không khả thi do không gian tìm kiếm khổng lồ) trong khi giữ số lần lặp tìm kiếm là 150 không đổi. Kết quả cho thấy rằng độ dài chấp nhận trung bình của tìm kiếm greedy chỉ là 1.493, thấp hơn đáng kể so với kết quả thu được bằng MCTS, chứng minh sự vượt trội của MCTS trong việc quản lý hiệu quả các không gian quyết định rộng lớn.

Tác động của Lựa chọn Mô hình N-gram. Các nghiên cứu của chúng tôi đánh giá mở rộng tác động của các cấu hình n-gram khác nhau đối với hiệu suất giải mã. Trong các thử nghiệm được tiến hành trên tập dữ liệu MT-Bench sử dụng mô hình Vicuna-7B, bi-gram và 4-gram dẫn đến độ dài chấp nhận lần lượt là 1.80 và 1.82. Những kết quả này thấp hơn đáng kể so với độ dài chấp nhận 2.30 đạt được với tri-gram. Các mô hình Bi-gram chứng minh khả năng hạn chế trong việc sử dụng hiệu quả thông tin ngữ cảnh, thường dẫn đến các đầu ra có vẻ ngẫu nhiên hơn và ít nhất quán hơn. Ngược lại, 4-gram thể hiện hành vi quá quyết định, hạn chế sự đa dạng của văn bản được tạo ra do bản chất hạn chế của chúng trong việc nắm bắt ngữ cảnh trước đó mở rộng. Tri-gram đạt được sự cân bằng tối ưu, cung cấp đủ độ sâu ngữ cảnh để tăng cường tính nhất quán và sự liên quan của đầu ra trong khi vẫn cho phép đủ biến thiên và đa dạng trong việc tạo văn bản. Sự cân bằng này làm cho tri-gram đặc biệt hiệu quả trong giải mã mô hình ngôn ngữ lớn, vì chúng bao gồm sự thỏa hiệp lý tưởng giữa tính ngẫu nhiên và nhận thức ngữ cảnh.

Công trình Liên quan
Một số nỗ lực nghiên cứu về các chiến lược giải mã cho các mô hình ngôn ngữ lớn đã sử dụng các mô hình dự thảo để cải thiện hiệu quả giải mã. Các kỹ thuật như Giải mã Suy đoán (Leviathan, Kalman, and Matias 2023; Spector and Re 2023; Chen et al. 2023; Stern, Shazeer, and Uszkoreit 2018), Madusa (Cai et al. 2024), Eagle (Li et al. 2024b), các cách tiếp cận khác yêu cầu mô hình dự thảo (Zhang et al. 2024; Liu et al. 2023b; Kim et al. 2024; Fu et al. 2024b) thuộc về danh mục này, sử dụng các mô hình để tạo ra dự thảo. Cụ thể, Giải mã Suy đoán sử dụng một kỹ thuật lấy mẫu tiến tiến trong đó mô hình phụ tạo ra một tập hợp các chuỗi token tiềm năng, và mô hình chính chọn các chuỗi tốt nhất, dẫn đến sự cân bằng tốt giữa tốc độ và độ chính xác. Mặc dù các phương pháp này chủ yếu nhằm tăng cường độ chính xác của các văn bản được tạo ra và tăng tốc đáng kể thời gian phản hồi trong quá trình tạo văn bản ban đầu, việc áp dụng chúng đi kèm với những nhược điểm. Vấn đề chính là sự cần thiết của đào tạo bổ sung cụ thể cho các mô hình dự thảo, có thể tốn nhiều tài nguyên. Hơn nữa, các kỹ thuật này thường phụ thuộc vào tài nguyên GPU (Kwon et al. 2023; Sheng et al. 2023; Park et al. 2024) cho suy luận, có thể hạn chế ứng dụng của chúng trong các môi trường nơi phần cứng như vậy không có sẵn hoặc khi hoạt động dưới các ràng buộc tài nguyên nghiêm ngặt.

Một phần đáng kể của những tiến bộ gần đây đã tập trung vào việc cải thiện hiệu quả mà không dựa vào các mô hình dự thảo (Fu et al. 2024a; He et al. 2024). Hai cách tiếp cận đáng chú ý trong lĩnh vực này là Giải mã Lookahead (Fu et al. 2024a) và Giải mã Suy đoán Dựa trên Truy xuất (REST) (He et al. 2024). Giải mã Lookahead là một cách tiếp cận tăng cường hiệu quả của quá trình giải mã thông qua dự đoán các token tiếp theo qua Jacobi Iteration (Sleijpen and Van der Vorst 2000). Nó sử dụng một heuristic để ước lượng chi phí tương lai của một chuỗi mà không cần tạo ra một dự thảo một cách rõ ràng. REST giới thiệu một mô hình tạo tăng cường truy xuất giải mã suy đoán các chuỗi mà không cần tạo ra các dự thảo sơ bộ. Thay vào đó, nó tìm kiếm và ưu tiên các tiếp tục có thể từ một cơ sở dữ liệu chuỗi đã được thiết lập. Tuy nhiên, các phương pháp này thể hiện độ chính xác thấp hơn và sử dụng tài nguyên lớn hơn so với cách tiếp cận của chúng tôi. Chúng đòi hỏi nhiều bộ nhớ và sức mạnh xử lý GPU hơn, đặt ra thách thức trong các cài đặt khan hiếm tài nguyên.

Kết luận
ADED cải thiện quá trình giải mã LLM bằng cách giới thiệu khả năng thích ứng và hiệu quả, giảm đáng kể độ trễ và yêu cầu tính toán. Phương pháp này đạt được tăng tốc lên đến 2.5X trong giải mã và cải thiện 20% tỷ lệ chấp nhận, vượt trội hơn các kỹ thuật truyền thống. Không giống như các cách tiếp cận hiện có, ADED điều chỉnh động phân phối dự thảo sử dụng ma trận tri-gram và tăng cường chất lượng dự thảo thông qua MCTS, loại bỏ nhu cầu tinh chỉnh. Vòng phản hồi liên tục đảm bảo những cải thiện liên tục trong việc tạo dự thảo. Trong khi ADED chứng minh hiệu suất mạnh mẽ trên nhiều benchmark khác nhau, công việc tương lai sẽ tập trung vào khám phá ứng dụng của nó trong các tình huống thế giới thực đa dạng hơn. Ngoài ra, việc giải quyết các hạn chế tiềm năng trong các triển khai quy mô cực lớn sẽ là ưu tiên.
