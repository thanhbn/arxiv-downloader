# 2408.08696.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/speculative/2408.08696.pdf
# File size: 884780 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Turning Trash into Treasure: Accelerating Inference of Large
Language Models with Token Recycling
Xianzhen Luo1, Yixuan Wang1, Qingfu Zhu1*, Zhiming Zhang1,
Xuanyu Zhang2,Qing Yang2,Dongliang Xu2
1Harbin Institute of Technology, Harbin, China
2Du Xiaoman (Beijing) Science Technology Co., Ltd.
{xzluo, wyx, qfzhu, zmzhang}@ir.hit.edu.cn
{zhangxuanyu, yangqing, xudongliang}@duxiaoman.com
Abstract
Massive parameters of LLMs have made infer-
ence latency a fundamental bottleneck. Specu-
lative decoding represents a lossless approach
to accelerate inference through a guess-and-
verify paradigm. Some methods rely on addi-
tional architectures to guess draft tokens, which
need extra training before use. Alternatively,
retrieval-based training-free techniques build li-
braries from pre-existing corpora or by n-gram
generation. However, they face challenges like
large storage requirements, time-consuming re-
trieval, and limited adaptability. Observing that
candidate tokens generated during the decod-
ing process are likely to reoccur in future se-
quences, we propose Token Recycling. It stores
candidate tokens in an adjacency matrix and
employs a breadth-first-search (BFS)-like algo-
rithm to construct a draft tree, which is then
validated through tree attention. New candi-
date tokens from the decoding process are then
used to update the matrix. Token Recycling re-
quires <2MB of additional storage and achieves
approximately 2x speedup across all sizes of
LLMs. It significantly outperforms existing
train-free methods by 30% and even a widely
recognized training method by 25%.
1 Introduction
Large Language Models (LLMs) (Brown et al.,
2020; Gemini Team et al., 2023; Touvron et al.,
2023; Meta, 2024) have becoming the foundation
of numerous applications such as chatbots, code
assistants, and agents (OpenAI, 2023; Chen et al.,
2021; Wang et al., 2024a). However, due to the
auto-regressive decoding strategy, LLMs can only
generate a single token at each decoding step, lead-
ing to high inference latency (Brown et al., 2020).
The latency mainly comes from transferring bil-
lions of parameters from high bandwidth mem-
ory to the accelerator cache at each decoding step,
*Corresponding author
Neglected
Information
[ range values [ k
for i range in (i in ( zip xs
LLM
Draft Model/Retrieval Library
for
Typical Speculative Decoding
Token Recycling (Ours)for i range in (i in ( zip keys
LLMDiscard
RetrieveUpdate
draft/candidate token
accepted token
rejected token
Top-k Candidate Tokens
Adjacency Matrixfor i k
... ... ...
( keys valuesUpdate
for ... k ( keysfor i j
... ... ...
( a b
Future GenerationsFigure 1: A comparison of typical speculative decoding
and Token Recycling (TR). Typical methods draft some
tokens and verify them in parallel in one decoding step.
Unlike other methods that discard candidate tokens, TR
stores them in an adjacency matrix. In future genera-
tions, draft tokens are retrieved from the matrix which
is updated with new candidate tokens. TR effectively
recycles tokens in the decoding process.
rather than arithmetic computations (Kim et al.,
2024; Shazeer, 2019; Cai et al., 2024).
Many approaches (Xu et al., 2024; Frantar and
Alistarh, 2023; Dao, 2024; DeepSeek-AI, 2024)
seek to reduce the latency, with speculative decod-
ingas a key lossless technique. This approach em-
ploys a guess and verify process to obtain multiple
tokens during a single decoding step (Chen et al.,
2023; Leviathan et al., 2023; Miao et al., 2024; Xia
et al., 2023). It first speculates several subsequent
draft tokens and then verifies them using the origi-
nal LLMs. The time cost of verification on multi-
ple tokens is comparable to that of generating onearXiv:2408.08696v3  [cs.CL]  25 May 2025

--- PAGE 2 ---
token due to the high parallelism of accelerators.
Once some draft tokens are correct, the decoding
steps is significantly shortened without sacrificing
quality. To fully utilize the parallelism of accel-
erators, tree attention slightly adjust the attention
mask to verify multiple token sequences in one
model forward (Cai et al., 2024; Miao et al., 2024).
For effective acceleration, speculative decoding
must ensure accurate draft predictions while keep-
ing speculation overhead low. Additional model
architectures are constructed to guess the draft to-
kens, including small draft models (Leviathan et al.,
2023; Chen et al., 2023) and parameter-efficient
structures (Cai et al., 2024; Lin et al., 2024). How-
ever, these approaches require resources for ad-
ditional training on each LLM. The typical ap-
proach to achieve train-free speculative decoding
is retrieve-based. In this case, a retrieval library is
pre-defined to obtain tokens following the suffix of
current content as draft tokens. Several methods
have been proposed in this category, each with its
trade-offs: (i)REST (He et al., 2023) transforms
existing corpora into a retrieval library, but the stor-
age is large, retrieval is time-consuming , and the
library lacks flexibility as it’s static to any queries.
(ii)PLD (Saxena, 2023) only retrieves the previ-
ous content with minimal cost. However, it can
not predict new tokens or new token combinations .
(iii)Lookhead (Fu et al., 2024) construct and up-
date an n-gram library by decoding n times with
LLMs. However, LLMs have to generate n-grams
while in inference, causing low efficiency .
Furthermore, all speculative decoding ap-
proaches fail to fully utilize candidate tokens ,
which are multiple possible next tokens generated
by LLMs at each decoding step. In greedy decod-
ing, only the top-1 candidate token of accepted to-
kens is selected as the output, while other candidate
tokens, including all candidate tokens from rejected
tokens, are discarded, such as ‘k’ and ‘keys’ in Fig-
ure 1. However, we observe that when current
input tokens reappear in future generations, the
following tokens could be candidate tokens gen-
erated several steps prior . Based on the observa-
tion, we propose Token Recycling (TR) , which uti-
lizes candidate tokens as draft tokens. It stores can-
didate tokens in an adjacency matrix. Before each
decoding step, a BFS-like approach retrieves a draft
tree from the matrix, which is then verified using
tree attention. Once verified, the newly generated
candidate tokens update the matrix. (i)The matrix
provides a flexible retrieval library that is tailoredto each query and offers low retrieval costs due to
itssmall size (<2MB) .(ii)Compared to using the
previous content solely, candidate tokens naturally
include more tokens, providing many possible con-
tinuations .(iii)The construction and update of our
library (matrix) utilize the ‘trash’ tokens without
requiring any additional generation .
We conduct comprehensive experiments on gen-
eral benchmark SpecBench (Xia et al., 2024), and
specialized dataset on code domain, MBPP (Chen
et al., 2021) with Vicuna (Zheng et al., 2023) and
Code Llama (Roziere et al., 2023) . The results
show that TR greatly exceeds previous train-free ap-
proaches, and improves more than 30% on all sizes
(7b, 13b, 33b/34b). The speed-up ratio even ex-
ceeds the widely used training approach–Medusa,
demonstrating its high efficiency.
Our contributions are summarized below:
•A plug-and-play speculative decoding method,
Token Recycling is proposed. It firstly recog-
nizes the value of ‘trash’ tokens and converts
them into ‘treasure’ tokens for acceleration.
•TR requires minimal storage space (<2MB)
with a low retrieval cost and covers many new
tokens. Continuously updating provides a dy-
namic retrieval space.
•TR achieves approximately 2x speedup on all
sizes of LLMs. It achieves a new SOTA with
an improvement greater than 31% compared
to previous train-free approaches and even
exceeding a training approach.
2 Background
In this section, we overview the speculative decod-
ing. We first define auto-regressive (AR) decoding
formally, then discuss speculative decoding, focus-
ing on two key strategies: guess-and-verify and
tree attention.
2.1 Auto-Regressive Decoding
AR is the default decoding strategy of LLMs. At
each step t, LLMs calculate the probability distri-
bution of the next token given the current content
s= (x0, x1,···, xt)which xi∈ V:
pt+1=P(x|s;θ).
HereVis the vocabulary and θdenotes LLM pa-
rameters. The next token is selected from pt+1
based on the sampling method. Followed Kou et al.

--- PAGE 3 ---
(2024), we focus on greedy decoding in this paper,
where the next token is:
xt+1=argmax pt+1.
Candidate tokens are the top- ktokens with the high-
est probabilities
(x0
t+1, x1
t+1, . . . , xk−1
t+1) =argtop k(pt+1)
where kis the number of candidate tokens,
argtop k(·)returns the indices of the top- khighest
values in pt+1andx0
t+1=xt+1.
2.2 Speculative Decoding
Guess and Verify Speculative decoding effec-
tively utilizes the parallel capability of acceler-
ators. Given s, it first guesses nsubsequent
draft tokens (˜xt+1,···,˜xt+n). The combination
(s,˜xt+1,···,˜xt+n)is then sent to LLMs for one
forward pass, resulting in:
pt+1=P(x|s;θ),
˜pt+i=P(x|s,˜xt+1, . . . , ˜xt+i−1;θ), i= 2, . . . , n.
pt+1is the same as AR decoding so the ground
truthxt+1is determinable. If the draft token ˜xt+1
matches xt+1, then ˜pt+2is assumed to identical
topt+2. Thus, the next ground truth is selected:
xt+2=argmax ˜pt+2. This verification process
continues until the draft token does not match the
ground truth, indicated by:
xt+j=argmax ˜pt+j̸= ˜xt+j.
Ultimately, jnew tokens are confirmed in one for-
ward pass. The time cost of one forward pass with
(s,˜xt+1,···,˜xt+n)is nearly the same as with s
due to the high parallel performance of accelera-
tors. Figure 1 shows an example. The draft tokens
are [‘i’, ‘in’, ‘range’, ‘(’] and the output tokens
are [‘i’, ‘in’, ‘zip’, ‘(’, ‘xs’] after the forward pass.
Though ‘zip’ fails to match ‘range’, three tokens
[‘i’, ‘in’, ‘zip’] are confirmed in one forward pass.
Tree Attention Traditional causal attention
masks are designed for linear sequences, which
restricts speculative decoding to verifying one se-
quence at a time. However, as the sequence length-
ens during draft token generation, the number of
potential continuations increases. For example, in
the draft tree in Figure 2, the token following ‘guest’
could be ‘speaker’ or ‘speak’. Tree attention mod-
ifies the attention mask to verify multiple draftsequences simultaneously. It compresses multiple
sequences into a single merged sequence, such as
[‘guest’, ‘speaker’, ‘speak’], while preserving the
tree structure through tree attention mask. Each
child node attends only to its parent nodes, prevent-
ing sibling tokens from interfering with each other.
After the LLM processes the merged sequence, all
possible sequences such as ‘guest speaker’ and
‘guest speak’, along with their corresponding out-
put tokens are extracted based on the tree structure
and verified in parallel. The longest correct se-
quence is selected as the final output. In rare cases,
when tokens have identical probabilities, tree atten-
tion and AR decoding may select different tokens,
but this affects the response quality minimally. The
detailed explanation is in Appendix A.1.
In summary, speculative decoding, through
guess and verify andtree attention , improves the
inference latency robustly and efficiently.
3 Methodology
Figure 2 provides an overview of Token Recycling
(TR). It leverages a hot-start adjacency matrix to
store candidate tokens and employs a BFS-like al-
gorithm to construct a draft tree. It utilizes tree at-
tention to verify draft sequences and continuously
updates the matrix with new candidate tokens gen-
erated during the decoding process.
3.1 Adjacency Matrix Initialization
The adjacency matrix Mis a key component in
TR, used to store top- kcandidate tokens for each
token in the vocabulary:
M ∈ V|V|×k
where kis a user-defined hyperparameter. Each
element M[i, j]indicates that the token VM[i,j]is
thej-th candidate token associated with Vi. The
use of matrix format, as opposed to other struc-
tures like tries, enables efficient parallel processing
of candidate tokens, which is crucial for reducing
retrieval and update times.
Initially, all elements are set to zero, meaning
that a token must appear in draft tokens before
it has valid candidate tokens. This initialization
leads to the matrix starting with limited predic-
tive capability, potentially causing inefficiencies
during the early stages of inference. To mitigate
this limitation, we implement a hot start strategy.
This involves continuing to use the existing ma-
trix, thereby leveraging prior knowledge. Even

--- PAGE 4 ---
4. Update  with candidate tokensevent is at an anPrefix: ... volunteering as a guest
guest
guest
speaker
atspeaker speak Spe
speaker at for is
speak ings in ers
speakat ~ ~ ~
for ~ ~ ~
for... ... ... ...Curr ent Token Candidate TokensCurr ent Content
1. Retrieve  based on 'guest'
Tree Attention Mask
Prefix guest speaker speak at forMerged SequencePrefix _guest _speaker _at _aa local nearby guest
2. Model F orward
guest speaker event speaking
speaker at is event
speak ers at ER
at a an the
for a an multicol
... ... ... ...Curr ent Token Candidate Tokens
a local nearby guestTop-k Candidate Tokensspeaker at ers a aLLM5. Select  the longest correct sequenceguest
speaker
speak
at
forguestspeakeratspeakfor
ings inSpe
is ers
Stage3: Verification and UpdateStage1 : Adjacency Matrix Initialization Stage2 : Draft Tree Retrieval
speaker
speaker at
speaker for
speakguest
guest
guest
guest
3. Verify  all sequencesings
ings
0. Inherit  from existing matrix
ings
rare,
ings , rare fairspeak guest ings
Figure 2: An overview of Token Recycling (TR). The adjacency matrix, initialized by the existing matrix, stores
candidate tokens. TR first retrieves a draft tree from the matrix which is then verified through tree attention. After
add the longest correct sequence to the content, the new top-k candidate tokens update the matrix.
if queries differ in the domain, candidate tokens
often include common expressions and patterns
that frequently appear across various queries. Con-
sequently, hot start ensures that the matrix has a
broader starting point, covering a wide range of
potential continuations.
3.2 Draft Tree Retrieval
The adjacency matrix Mstores candidate tokens,
which can be used as draft tokens when their cor-
responding tokens appear later. Directly using the
matrix could only determine the immediate next
token, such as finding ‘speaker’ following ‘guest’
(see Figure 2). Even if ‘speaker’ is correct, it only
slightly improves upon AR decoding, adding just
one additional token. In fact, the matrix also holds
possible continuations for these candidate tokens,
suggesting subsequent tokens like ‘at’ following
‘speaker’. Extending the sequence step by step al-
lows for longer draft sequences. Furthermore, by
storing top- kcandidate tokens, multiple potential
continuations can be explored in parallel for each
token, such as ‘at’ and ‘for’ following ‘speaker’.
This BFS process enables the construction of a
draft tree with only the adjacency matrix, which
can be directly applied to tree attention.
Unlike a complete BFS, we use heuristic rulesto define a static and imbalanced tree structure.
This tree structure and its construction process are
detailed in the Appendix A.2. Static : The num-
ber of children for each node remains constant
across all decoding steps, which facilitates pre-
processing and enables efficient parallel operations
during layer traversal. Avoiding the need to tra-
verse each node individually significantly reduces
retrieval time. Imbalance : Nodes positioned ear-
lier in each layer have more children and extend
deeper. This allocates computational resources to
the most probable continuations since candidate
tokens are ordered by probabilities in the matrix.
The BFS-like approach for retrieving the draft
tree begins with the matrix Mand the tree structure
Tree . The root is the last token of current content,
like ‘guest’ in Figure 2. As the root forms the first
layer, all candidate tokens for ‘guest’ are extracted
fromM, resulting in [‘speaker’, ‘speak’, ‘Spe’].
According to Tree , the first layer allows each to-
ken to have two children, Therefore, ‘speaker’ and
‘speak’, which have the top-2 probabilities, are
added to the second layer. The process then pro-
ceeds to expand a new layer. All candidate tokens
of the second layer are retrieved in parallel, result-
ing in [‘at’, ‘for’, ‘is’] and [‘ings’, ‘in’, ‘ers’]. Tree
specifies that the first node (‘speaker’) can have two

--- PAGE 5 ---
children, while the subsequent node (‘speak’) can
only have one child. Consequently, the new layer
tokens are [‘at’, ‘for’], and [‘ings’]. This process
repeats until the specified depth is reached. The
detailed Algorithm 1 is provided in Appendix A.2.
This retrieval method constructs a draft tree ef-
fectively and efficiently with the desired length and
variety, which can later be verified by tree attention.
3.3 Verification and Update
The verification of the draft tree aligns with Sec-
tion 2.2. Merged sequence Sis constructed through
traversing the draft tree by layers. All potential
draft sequences are then verified and the longest
correct sequence is selected.
Following verification, the adjacency matrix M
is updated in parallel based on the output distribu-
tions ˜pi+1of each draft token xi∈S:
M[˜xi] =argtop k(˜pi+1).
Since multiple preceding tokens may have the
same candidate token, duplicates may appear in
S, and their output distributions are likely to dif-
fer. When performing updates in parallel, CUDA
operations may merge these updates, leading to
variations in the final result. For example, if xi
appears twice and has two different top-2 out-
put tokens, [y0, y1],[z0, z1], thenM[xi]could be
updated to exactly one of the following results:
[y0, z1],[y0, y1],[z0, z1]or[z0, y1]. We do not re-
solve this merging, as adding controls reduces over-
all performance, as discussed later in Section 5.2.
The update process directly overwrites the pre-
vious candidate tokens and leverages the new ones
as draft tokens for subsequent decoding steps. This
allows the retrieval space to dynamically adapt to
the current content, focusing on the most relevant
and probable continuations. It also eliminates the
necessity for extra operations beyond the standard
decoding to update the retrieval space.
In summary, TR capitalizes on the ‘trash’ present
in speculative decoding by implementing a cycling
process between candidate and draft tokens. It ac-
celerates inference without the need for additional
model structures or training, making it highly adapt-
able and seamlessly integrated with any architec-
ture or model size.4 Experiment
4.1 Experimental Setup
Align with previous work (Kou et al., 2024), we fo-
cus on common computational redundancy scenar-
ios, specifically greedy decoding with a batch size
of one. The following evaluation metrics are used:
Mean Accepted Token (MAT) (Xia et al., 2024)
represents the average number of tokens confirmed
in a single decoding step; Tokens per Second
(Ts/s) measures the number of tokens processed per
second; Speed up ratio compares the performance
relative to HuggingFace’s implementation of AR
decoding. We set k= 8forM(<2MB storage in
sum) and the draft tree structure is shown in Ap-
pendix A.2. All experiments are conducted using
Pytorch 2.3 with a single A100-80GB GPU and
128 CPUs under CUDA 12.2.
Datasets and LLMs We conduct experiments on
SpecBench (Xia et al., 2024) and MBPP (Austin
et al., 2021). SpecBench is a comprehensive bench-
mark encompassing diverse scenarios including
Multi-turn Conversation (MT), Translation (Trans),
Summarization (Sum), Question Answering (QA),
Mathematical Reasoning (Math), and Retrieval-
Augmented Generation (RAG). MBPP is a widely
used dataset in code generation, which has a
growing demand for efficient generation. These
datasets enable a comparative analysis with prior
work across both general and specialized domains.
We follow the standard practice of utilizing Vi-
cuna (Chiang et al., 2023) for SpecBench and Code
Llama (Roziere et al., 2023) for MBPP across three
different scales: 7B, 13B, and 33B1.
Baseline We compare TR with three train-free
retrieval-based methods. Lookahead (Lade) con-
structs an n-gram retrieval library through addi-
tional n-gram generation during decoding, con-
suming significant computational resources. PLD
treats previous content as the retrieval library,
which is constrained and cannot introduce new to-
kens or new token combinations. REST builds
the retrieval library from existing training datasets,
requiring large storage and considerable retrieval
time. The static nature of the library also prevents it
from adapting to individual queries. Furthermore,
we also include a train-need baseline for border
comparison. Medusa adds multiple additional LM
heads in the final layer to predict draft tokens. We
1The largest model of Code Llama is 34B, for consistency
and convenience in our comparisons, we refer to it as 33B.

--- PAGE 6 ---
#Para MethodSpecBench MBPP
MT Trans Sum QA Math RAG MAT Ts/s Speed MAT Ts/s Speed
7BAR 1.00 1.00 1.00 1.00 1.00 1.00 1.00 54.30 1.00 1.00 56.15 1.00
Lade 1.42 1.12 1.21 1.21 1.52 1.13 1.64 69.03 1.27 1.66 79.16 1.41
PLD 1.53 0.98 2.36 1.10 1.50 1.74 1.75 83.30 1.53 1.39 66.65 1.19
REST 1.37 1.05 1.12 1.42 1.06 1.30 1.84 66.29 1.22 2.08 87.08 1.55
Medusa 1.90 1.57 1.48 1.58 1.87 1.45 2.31 89.41 1.65 - - -
TR 2.17 1.90 1.94 1.95 2.40 1.78 2.70 110.06 2.03 2.93 131.20 2.34
13BAR 1.00 1.00 1.00 1.00 1.00 1.00 1.00 39.41 1.00 1.00 41.31 1.00
Lade 1.29 1.06 1.16 1.12 1.48 1.09 1.63 47.50 1.21 1.73 56.87 1.38
PLD 1.45 1.01 2.10 1.02 1.55 1.65 1.67 57.01 1.45 1.48 52.20 1.26
REST 1.51 1.14 1.31 1.50 1.17 1.50 1.82 53.34 1.35 2.05 70.13 1.70
Medusa 1.94 1.66 1.57 1.62 1.98 1.53 2.39 67.92 1.72 - - -
TR 1.98 1.77 1.89 1.75 2.21 1.73 2.72 74.57 1.89 3.08 93.42 2.26
33BAR 1.00 1.00 1.00 1.00 1.00 1.00 1.00 18.44 1.00 1.00 19.44 1.00
Lade 1.32 1.09 1.20 1.17 1.55 1.14 1.61 23.03 1.25 1.70 29.22 1.50
PLD 1.43 1.06 1.94 1.08 1.55 1.41 1.55 25.89 1.40 1.41 25.89 1.33
REST 1.63 1.27 1.42 1.61 1.29 1.57 1.81 26.99 1.46 2.10 36.85 1.90
Medusa 1.98 1.75 1.63 1.68 2.09 1.61 2.32 33.11 1.80 - - -
TR 1.95 1.75 1.92 1.77 2.24 1.78 2.63 35.16 1.91 3.05 45.43 2.34
Table 1: Performance of different methods on SpecBench (Vicuna) and on MBPP (Code Llama) across all parameter
sizes. Speed is the displayed metric for categories of SpecBench. MBPP results exclude Medusa as it lacks a Code
Llama variant. Medusa involves training while others are training-free. Bold represents the highest performance.
focus on losses Medusa-1 since Medusa-2 is lossy.
All baselines use their default hyperparameters.
4.2 Main Results
Table 1 shows the performance of TR compared to
other methods. On SpecBench, it achieves more
than a 2x speedup on the 7B model, nearly 30%
higher than the previous train-free methods. Even
compared to tuning Medusa, it shows an improve-
ment of almost 25%. For the 13B and 33B models,
it consistently provides nearly 2x speedup, main-
taining the 30% acceleration advantage. These
results demonstrate that TR is the most effective
train-free method on SpecBench, offering substan-
tial and consistent speedup across all model sizes.
Notably, TR achieves the best speedup across
most sub-tasks as well, except it slightly trails PLD
on Sum. This may be due to this task often in-
volves many repetitions of previous content. How-
ever, the performance gap between TR and PLD
narrows as the model size increases, reaching only
a 1% difference with the 33B model. This is due
to larger models tending to generate new tokens
rather than repeat previous content. In other tasks
such as MT, Trans, QA, and Math, TR shows a
significant improvement of about 40%~70% for
the 7B model. This demonstrates the strong gen-
eralization of our method across various scenarios.
Although the improvement on RAG is less than 3%
for the 7B model, it increases with model size, ex-ceeding 10% for the 33B one. This improvement is
consistent with the preference of larger models for
new tokens. Compared to the general domain, all
methods achieve greater acceleration on the code
domain due to its higher content redundancy. TR
provides approximately 2.3x speedup across all
model scales, achieving the SOTA performance.
Furthermore, performances on Trans show the
advantages of our method compared to PLD and
REST. While PLD shows negligible speedup (close
to 1x) and REST achieves its lowest speedup across
tasks, TR consistently delivers over 1.75x speedup
across all model sizes. Notably, on the 7B model,
PLD results in a slowdown, and REST achieves
just 1.05x, whereas TR reaches 1.9x. Trans re-
quires generating new tokens continuously, involv-
ing minimal repetition of previous content. Addi-
tionally, it is highly context-sensitive, making it
challenging to find exact matches from any pre-
existing database. These pose challenges for PLD
and REST. In contrast, the adaptive and diverse re-
trieval space of TR leads to superior performance.
In addition to Speed, TR achieves the highest MAT
across both benchmarks. This is attributed to its
shorter retrieval times and the avoidance of addi-
tional generations like Lade. This allows for deeper
and wider draft trees, enabling more tokens to be
accepted in a single decoding step.
Table 2 summarizes the GPU memory require-
ment for all methods. Compared to REST and

--- PAGE 7 ---
/uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013 /uni0000001c/uni00000013
/uni00000025/uni00000055/uni00000048/uni00000044/uni00000047/uni00000057/uni0000004b/uni00000015/uni00000011/uni00000015/uni00000013/uni00000015/uni00000011/uni00000017/uni00000013/uni00000015/uni00000011/uni00000019/uni00000013/uni00000015/uni00000011/uni0000001b/uni00000013/uni00000030/uni00000024/uni00000037/uni0000000b/uni00000044/uni0000000c
/uni00000017 /uni00000018 /uni00000019 /uni0000001a /uni0000001b
/uni00000027/uni00000048/uni00000053/uni00000057/uni0000004b/uni00000015/uni00000011/uni00000019/uni0000001a/uni00000015/uni00000011/uni00000019/uni0000001c/uni00000015/uni00000011/uni0000001a/uni00000014/uni00000015/uni00000011/uni0000001a/uni00000016/uni0000000b/uni00000045/uni0000000c
/uni00000029/uni0000004c/uni00000055/uni00000056/uni00000057 /uni00000026/uni00000058/uni00000055 /uni0000002f/uni00000044/uni00000056/uni00000057
/uni00000036/uni00000057/uni00000055/uni00000044/uni00000057/uni00000048/uni0000004a/uni0000005c/uni00000015/uni00000011/uni00000019/uni0000001b/uni00000015/uni00000011/uni0000001a/uni00000014/uni00000015/uni00000011/uni0000001a/uni00000017/uni00000015/uni00000011/uni0000001a/uni0000001a/uni0000000b/uni00000046/uni0000000c
/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni0000001b/uni00000014/uni00000014/uni00000017/uni00000014/uni00000015/uni00000013
/uni00000014/uni00000014/uni00000018/uni00000014/uni00000014/uni0000001a/uni00000014/uni00000014/uni0000001c/uni00000014/uni00000015/uni00000014
/uni00000014/uni00000014/uni00000014/uni00000014/uni00000014/uni00000017/uni00000014/uni00000014/uni0000001a/uni00000014/uni00000015/uni00000013
/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000056/uni00000012/uni00000056
/uni00000030/uni00000024/uni00000037
/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000056/uni00000012/uni00000056Figure 3: Effects of tree breadth, depth and updating strategies on MAT and Tokens/s are in (a), (b), and (c).
Method Memory (MB) Speed
Lade 105 1.27
PLD 0 1.53
REST 465 1.22
Medusa >800 1.65
TR 1.95 2.03
Table 2: The additional memory costs for all methods.
Medusa adds extra LM heads to the model, so the mem-
ory usage depends on the hidden size and the precision.
800MB is based on a 7B LLM and fp16 precision.
Lade, TR achieves higher speedup with far less
memory. While PLD requires no additional mem-
ory, its speedup is limited. Unlike Medusa, our
approach is training-free, requires minimal mem-
ory, and still achieves superior performance.
TR demonstrates significant improvements
across all scenarios, highlighting its efficiency and
broad applicability. Importantly, TR is train-free
and self-drafting, allowing for an approximate
2x speedup that can be seamlessly applied as a
‘free lunch’ to any existing LLM .
5 Analysis
5.1 Tree Structure
As previously outlined in Section 3.2, our tree struc-
ture is static and imbalanced. The tree size is a
crucial factor to accelerate. A larger tree allows
more tokens confirmed in one decoding step but
also introduces more computational overhead, in-
creasing the time required for each decoding step.
To investigate the impact of tree size, specifically
its depth and breadth, experiments are conducted
on MT-Bench using Vicuna-7B.
Breadth Increasing the breadth of the tree allows
for covering more possibilities. In Figure 3(a), the
breadth is expanded by adding nodes while keeping
the depth fixed at six layers. This leads to a con-Tokens/s Speed
AR 54.98 1.00
Random 95.07 1.73
Zero 102.68 1.87
Fixed 117.43 2.12
Shuffle 118.78 2.16
TR 119.56 2.17
Table 3: The impact of different initialization strategies
of the adjacency matrix. Random means randomly se-
lected from the vocabulary, Zero means all set to zero,
Fixed means inherited from a fixed matrix and Shuffle
means shuffle the test set.
sistent improvement in MAT. However, when the
breadth exceeds 80, Tokens/s begins to decrease.
The additional computational overhead eventually
outweighs the benefits of a higher MAT.
Depth Increasing the depth of the tree allows
for accepting longer sequences during decoding.
In Figure 3(b), with the number of nodes fixed
at 80, the depth is gradually increased. MAT ini-
tially rises rapidly but eventually shows minimal
improvement, while Tokens/s noticeably fluctuates.
Because the matrix stores candidate tokens for only
adjacent steps, longer sequences weaken the con-
nections between distant tokens. This limitation
reduces the effectiveness of increased depth, caus-
ing Tokens/s to fluctuate.
5.2 Ablation Study
Hot Start In TR, the adjacency matrix inherits
from the previous one. In Table 3, we explore the
impact of different initialization strategies. Ran-
dom means randomly selecting tokens from the
vocabulary, while Zero sets all matrix elements
to zero. Fixed selects 100 queries from AlpacaE-
val (Li et al., 2023) (unrelated to the test set), ex-
ecutes them, and stores the resulting matrix. This
matrix is then used to initialize each query in the

--- PAGE 8 ---
SpecBench MBPP
Only Accepted 1.63 1.99
All Draft 2.69 2.93
Table 4: Mean Accepted Token (MAT) for updating
candidate tokens from only accepted or all draft tokens.
test set. Shuffle refers to shuffling the test set. Com-
pared to the Zero, the irrelevant noise introduced by
Random leads to a sharp decrease in performance.
Fixed, Shuffle and TR show significant improve-
ments over Zero, suggesting that the prior matrix
may capture common patterns that effectively assist
subsequent queries. The relatively small difference
among them indicates that these patterns are gener-
alizable and not tied to specific tasks or content.
Update Strategies Section 3.1 discusses dupli-
cate tokens in the merged sequence during matrix
updates. We compare three updating strategies: us-
ing candidate tokens from the first occurrence, from
the last occurrence, and the current method (merg-
ing via parallel CUDA operations). A detailed
explanation of three strategies is in Appendix A.5
Figure 3(c) indicates that using the last occurrence
yields the highest MAT, which may benefit from
more contextual information. However, the differ-
ences among different strategies in MAT are min-
imal. In terms of Tokens/s, the current approach
significantly outperforms the other two, as it avoids
the additional processing required to manage token
positions, thereby reducing delays. Speculative
decoding is highly sensitive to latency, any extra
operation must provide substantial benefits to out-
weigh its time cost.
Effect of Rejected Tokens During the update,
we refresh the candidate tokens for all draft tokens,
including both accepted and rejected tokens. To fur-
ther illustrate the significant effect of trash tokens,
we compare two settings: updating only the candi-
dates of accepted tokens versus of all draft tokens.
As shown in Table 4, including candidates of re-
jected tokens significantly improves the MAT. This
indicates that rejected tokens also carry valuable in-
formation necessary for subsequent decoding. We
include a case study in Appendix A.4.
Temperature Sampling To enhance diversity, a
temperature greater than 0 is often employed dur-
ing LLM generation. We conducted experiments
on SpecBench at different temperature settings to
investigate the impact of this randomness on ac-
0 0.3 0.5 1
Temperature0.00.51.01.52.02.53.0MATMAT
Speedup
0.00.51.01.52.0
SpeedupFigure 4: MAT and Speedup ratio under different tem-
peratures during generation.
Method Memory (MB) Tokens/s Speed
Eagle1 500 106.94 2.08
Eagle2 500 116.95 2.28
TR 1.95 107.52 2.09
Table 5: The comparison of Eagle1/2 with TR about
memory costs and speedup ratio.
celeration. Figure 4 shows that at a temperature
of 0.3, the performance remains unaffected when
compared to greedy decoding. However, as the
temperature increases to 0.5 and 1, a slight perfor-
mance degradation is observed. This is likely due to
we only store the top-k candidate tokens. At higher
sampling temperatures, the probability of selecting
tokens outside the top-k increases. Despite this, the
speedup ratio of TR remains consistently around
2.0. The sustained acceleration performance under
various sampling settings demonstrates the robust-
ness and broad applicability of TR.
5.3 Compare with Eagle
Table 1 demonstrates that TR significantly outper-
forms the train-need Medusa. We were curious to
compare TR with the state-of-the-art speculative de-
coding methods - Eagle1 (Li et al., 2024b) and Ea-
gle2 (Li et al., 2024a). Eagle1 is a train-dependent
method that collects training data and trains a sepa-
rate draft model using the hidden states and token
embeddings from a large LLM. Eagle2 improves
upon Eagle1 by incorporating dynamic trees into
the draft tree construction process. In Table 5, TR
is compared with Eagle1/2 on SpecBench. It is a
surprise that, despite being completely training-free
and self-drafting, TR outperforms Eagle1. More-
over, TR achieves 91.23% of the acceleration per-
formance of Eagle2 while only requiring 0.39%

--- PAGE 9 ---
/uni00000033/uni00000055/uni00000048/uni00000053/uni00000055/uni00000052/uni00000046/uni00000048/uni00000056/uni00000056/uni00000003/uni0000000b/uni00000014/uni00000011/uni00000019/uni00000008/uni0000000c
/uni00000035/uni00000048/uni00000057/uni00000055/uni0000004c/uni00000048/uni00000059/uni00000044/uni0000004f/uni00000003/uni0000000b/uni00000014/uni00000011/uni00000019/uni00000008/uni0000000c
/uni00000038/uni00000053/uni00000047/uni00000044/uni00000057/uni00000048/uni00000003/uni0000000b/uni00000014/uni00000011/uni00000018/uni00000008/uni0000000c/uni00000039/uni00000048/uni00000055/uni0000004c/uni00000049/uni0000005c/uni00000003/uni0000000b/uni00000018/uni00000011/uni00000015/uni00000008/uni0000000c
/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000003/uni00000029/uni00000052/uni00000055/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni0000000b/uni0000001c/uni00000013/uni00000011/uni00000014/uni00000008/uni0000000cFigure 5: Time allocation for each operation when
LLMs respond to a query.
of the memory used by Eagle2. It is worth not-
ing that TR still employs a static tree, rather than
the dynamic tree used in Eagle2. This highlights
the remarkable effectiveness and efficiency of TR
and shows potential for further improvement when
combine TR with dynamic trees.
5.4 Time Allocation
Effective speculative decoding requires not only a
high hit rate but also minimal time on additional
operations. Each decoding step can be divided
into several components: preprocessing ,retriev-
ingdraft tokens, model forward ,verifying draft se-
quences, and updating the matrix, input tokens, and
key-value cache. Figure 5 shows that the majority
of the time is consumed by the model forward. The
second most is verification which involves extract-
ing and validating all feasible paths. In contrast,
TR introduces only negligible latency in its ded-
icated preprocessing, retrieval, and update steps.
This highlights the efficiency of TR’s design.
6 Related Work
Efficient inference is crucial for real-time appli-
cations and low-resource scenarios. Many strate-
gies have been developed to reduce latency (Zhou
et al., 2024b). Among these, speculative decod-
ing (Chen et al., 2023; Leviathan et al., 2023; Miao
et al., 2024; Xia et al., 2023) is a losses tech-
nique that predicts multiple possible continuations
simultaneously. It reduces the number of decod-
ing steps needed without compromising accuracy.
Some speculative decoding methods rely on addi-
tional draft models to guess draft tokens. These
typically involve using smaller models from the
same series (Zhao et al., 2024; Spector and Re;
Sun et al., 2023; Liu et al., 2024b; Yuan et al.,2024; Gong et al., 2024) or training new models
with a shared vocabulary (Leviathan et al., 2023;
Chen et al., 2023; Zhou et al., 2024a; Li et al.,
2024b). It is worth noting that Zhao et al. (2024)
also uses rejected tokens but does not include can-
didate tokens. Additionally, Kou et al. (2024);
Wang et al. (2024b) propose training the original
LLMs to enable non-aggressive decoding. While
effective, these approaches require managing or
training multiple models, which can be non-trivial
and resource-intensive. Other methods focus on
parameter-efficient structures. These approaches
minimize the need for complete retraining but still
require model-specific training and adaptation, lim-
iting their scalability and general applicability (Lin
et al., 2024; Liu et al., 2024a).
Train-free methods construct retrieval libraries
to obtain draft tokens (Yang et al., 2023). Looka-
head (Fu et al., 2024) generates n-grams through
multiple decodings, building a retrieval library that
can hit multiple tokens in one step. However,
it requires the LLM to generate n-grams while
responding to queries, which reduces efficiency.
PLD (Saxena, 2023) retrieves only from previous
content, resulting in minimal overhead and signif-
icant speedup in high-redundancy tasks like sum-
marization. However, it provides little acceleration
for tasks requiring the generation of new content,
like translation. REST (He et al., 2023) constructs
retrieval libraries using existing corpora and per-
forms well in common scenarios. However, this
approach requires large storage, time-consuming
retrieval, and cannot adapt to each query.
TR is a train-free, retrieval-based method. It re-
quires no additional generation, covers a broader
range of possible continuations, and demands min-
imal storage with low retrieval costs. The update
process ensures an adaptable retrieval space.
7 Conclusion
In this work, we introduce Token Recycling, a spec-
ulative decoding method for accelerating the in-
ference of LLMs. It utilizes an adjacency matrix
to store candidate tokens and retrieve a draft tree,
which is then verified with tree attention. The ma-
trix is updated with new candidate tokens generated
during decoding. Token Recycling could be inte-
grated seamlessly with existing LLMs and tasks.
As a train-free approach, it achieves a speedup of
nearly 2x with <2MB of storage, improving over
31% compared to previous train-free approaches.

--- PAGE 10 ---
Limitations
Our study is comprehensive, but has certain limita-
tions that we plan to address in future research. In
constructing the draft tree, we use a static tree struc-
ture. However, a dynamic tree could be employed
instead. While dynamic trees introduce additional
complexity, they allow for better adaptation to each
decoding step, potentially improving performance
by tailoring the tree structure to the specific require-
ments of each query.
Ethical Considerations
The data for the proposed methods is drawn solely
from publicly accessible project resources on rep-
utable websites, ensuring that no sensitive informa-
tion is included. Moreover, all datasets and baseline
models used in our experiments are also available
to the public. We have taken care to acknowledge
the original authors by properly citing their work.
Acknowledge
We gratefully acknowledge the support of the Na-
tional Natural Science Foundation of China (NSFC)
via grant 62236004, 62206078 and 62476073.
References
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten
Bosma, Henryk Michalewski, David Dohan, Ellen
Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.
Program synthesis with large language models. arXiv
preprint arXiv:2108.07732 .
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Process-
ing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual .
Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu
Peng, Jason D. Lee, Deming Chen, and Tri Dao.
2024. Medusa: Simple LLM Inference Acceleration
Framework with Multiple Decoding Heads. Preprint ,
arXiv:2401.10774.
Charlie Chen, Sebastian Borgeaud, Geoffrey Irving,
Jean-Baptiste Lespiau, Laurent Sifre, and JohnJumper. 2023. Accelerating Large Language Model
Decoding with Speculative Sampling. Preprint ,
arXiv:2302.01318.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, Gretchen
Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-
try, Pamela Mishkin, Brooke Chan, Scott Gray,
Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz
Kaiser, Mohammad Bavarian, Clemens Winter,
Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza-
beth Barnes, Ariel Herbert-V oss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N.
Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welinder,
Bob McGrew, Dario Amodei, Sam McCandlish, Ilya
Sutskever, and Wojciech Zaremba. 2021. Evaluating
Large Language Models Trained on Code. Preprint ,
arXiv:2107.03374.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Tri Dao. 2024. FlashAttention-2: Faster attention with
better parallelism and work partitioning. In Inter-
national Conference on Learning Representations
(ICLR) .
DeepSeek-AI. 2024. Deepseek-v2: A strong, economi-
cal, and efficient mixture-of-experts language model.
Preprint , arXiv:2405.04434.
Elias Frantar and Dan Alistarh. 2023. SparseGPT: Mas-
sive language models can be accurately pruned in
one-shot. In Proceedings of the 40th International
Conference on Machine Learning , volume 202 of
Proceedings of Machine Learning Research , pages
10323–10337. PMLR.
Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang.
2024. Break the Sequential Dependency of LLM
Inference Using Lookahead Decoding. Preprint ,
arXiv:2402.02057.
Gemini Team, Rohan Anil, Sebastian Borgeaud,
Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,
Radu Soricut, Johan Schalkwyk, Andrew M Dai,
Anja Hauth, et al. 2023. Gemini: a family of
highly capable multimodal models. arXiv preprint
arXiv:2312.11805 .
Zhuocheng Gong, Jiahao Liu, Ziyue Wang, Pengfei
Wu, Jingang Wang, Xunliang Cai, Dongyan Zhao,
and Rui Yan. 2024. Graph-structured speculative
decoding. In Findings of the Association for Compu-
tational Linguistics ACL 2024 , pages 11404–11415.

--- PAGE 11 ---
Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee,
and Di He. 2023. Rest: Retrieval-based speculative
decoding. arXiv preprint arXiv:2311.08252 .
Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen
Dong, Xiuyu Li, Sheng Shen, Michael W. Mahoney,
and Kurt Keutzer. 2024. SqueezeLLM: Dense-and-
Sparse Quantization. Preprint , arXiv:2306.07629.
Siqi Kou, Lanxiang Hu, Zhezhi He, Zhijie Deng, and
Hao Zhang. 2024. CLLMs: Consistency Large Lan-
guage Models. Preprint , arXiv:2403.00835.
Yaniv Leviathan, Matan Kalman, and Yossi Matias.
2023. Fast Inference from Transformers via Spec-
ulative Decoding. In Proceedings of the 40th Inter-
national Conference on Machine Learning , pages
19274–19286. PMLR.
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori,
Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and
Tatsunori B. Hashimoto. 2023. Alpacaeval: An au-
tomatic evaluator of instruction-following models.
https://github.com/tatsu-lab/alpaca_eval .
Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang
Zhang. 2024a. Eagle-2: Faster inference of language
models with dynamic draft trees. In Proceedings
of the 2024 Conference on Empirical Methods in
Natural Language Processing , pages 7421–7432.
Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang
Zhang. 2024b. Eagle: Speculative sampling requires
rethinking feature uncertainty. In Forty-first Interna-
tional Conference on Machine Learning .
Feng Lin, Hanling Yi, Hongbin Li, Yifan Yang, Xi-
aotian Yu, Guangming Lu, and Rong Xiao. 2024.
BiTA: Bi-Directional Tuning for Lossless Accel-
eration in Large Language Models. Preprint ,
arXiv:2401.12522.
Fangcheng Liu, Yehui Tang, Zhenhua Liu, Yunsheng
Ni, Kai Han, and Yunhe Wang. 2024a. Kangaroo:
Lossless Self-Speculative Decoding via Double Early
Exiting. Preprint , arXiv:2404.18911.
Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Alvin Che-
ung, Zhijie Deng, Ion Stoica, and Hao Zhang. 2024b.
Online speculative decoding. In Proceedings of the
41st International Conference on Machine Learning ,
volume 235 of Proceedings of Machine Learning
Research , pages 31131–31146. PMLR.
Meta. 2024. Introducing meta llama 3: The most capa-
ble openly available llm to date.
Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao
Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee
Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan
Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Ab-
hyankar, and Zhihao Jia. 2024. SpecInfer: Accelerat-
ing Large Language Model Serving with Tree-based
Speculative Inference and Verification. In Proceed-
ings of the 29th ACM International Conference on
Architectural Support for Programming Languagesand Operating Systems, Volume 3 , volume 3 of ASP-
LOS ’24 , pages 932–949. Association for Computing
Machinery.
OpenAI. 2023. GPT-4 technical report. CoRR ,
abs/2303.08774.
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,
Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.
Code llama: Open foundation models for code. arXiv
preprint arXiv:2308.12950 .
Apoorv Saxena. 2023. Prompt lookup decoding.
Noam Shazeer. 2019. Fast Transformer Decoding:
One Write-Head is All You Need. Preprint ,
arXiv:1911.02150.
Benjamin Frederick Spector and Christopher Re. Accel-
erating llm inference with staged speculative decod-
ing. In Workshop on Efficient Systems for Foundation
Models@ ICML2023 .
Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ah-
mad Beirami, Himanshu Jain, and Felix Yu. 2023.
Spectr: Fast speculative decoding via optimal trans-
port. In Thirty-seventh Conference on Neural Infor-
mation Processing Systems .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurélien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-
tuned chat models. CoRR , abs/2307.09288.
Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao
Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang,
Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei,
and Jirong Wen. 2024a. A survey on large language
model based autonomous agents. Frontiers of Com-
puter Science , 18:186345.
Yixuan Wang, Xianzhen Luo, Fuxuan Wei, Yijun Liu,
Qingfu Zhu, Xuanyu Zhang, Qing Yang, Dongliang
Xu, and Wanxiang Che. 2024b. Make some noise:
Unlocking language model parallel inference ca-
pability through noisy training. arXiv preprint
arXiv:2406.17404 .

--- PAGE 12 ---
Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu
Wei, and Zhifang Sui. 2023. Speculative decod-
ing: Exploiting speculative execution for accelerat-
ing seq2seq generation. In Findings of the Associa-
tion for Computational Linguistics: EMNLP 2023 ,
pages 3909–3925, Singapore. Association for Com-
putational Linguistics.
Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang,
Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhi-
fang Sui. 2024. Unlocking Efficiency in Large Lan-
guage Model Inference: A Comprehensive Survey of
Speculative Decoding. Preprint , arXiv:2401.07851.
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song
Han, and Mike Lewis. 2024. Efficient streaming lan-
guage models with attention sinks. In The Twelfth
International Conference on Learning Representa-
tions .
Yuzhuang Xu, Xu Han, Zonghan Yang, Shuo Wang,
Qingfu Zhu, Zhiyuan Liu, Weidong Liu, and
Wanxiang Che. 2024. OneBit: Towards Ex-
tremely Low-bit Large Language Models. Preprint ,
arXiv:2402.11295.
Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin
Jiang, Linjun Yang, Rangan Majumder, and Furu
Wei. 2023. Inference with reference: Lossless
acceleration of large language models. Preprint ,
arXiv:2304.04487.
Hongyi Yuan, Keming Lu, Fei Huang, Zheng Yuan, and
Chang Zhou. 2024. Speculative contrastive decoding.
InProceedings of the 62nd Annual Meeting of the
Association for Computational Linguistics (Volume
2: Short Papers) , Bangkok, Thailand. Association
for Computational Linguistics.
Weilin Zhao, Yuxiang Huang, Xu Han, Chaojun Xiao,
Zhiyuan Liu, and Maosong Sun. 2024. Ouroboros:
Speculative decoding with large model enhanced
drafting. arXiv preprint arXiv:2402.13720 .
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,
Joseph E. Gonzalez, and Ion Stoica. 2023. Judging
LLM-as-a-Judge with MT-Bench and Chatbot Arena.
Preprint , arXiv:2306.05685.
Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat,
Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv
Kumar, Jean-François Kagy, and Rishabh Agarwal.
2024a. Distillspec: Improving speculative decoding
via knowledge distillation. In The Twelfth Interna-
tional Conference on Learning Representations .
Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Ji-
aming Xu, Shiyao Li, Yuming Lou, Luning Wang,
Zhihang Yuan, Xiuhong Li, et al. 2024b. A survey on
efficient inference for large language models. arXiv
preprint arXiv:2404.14294 .

--- PAGE 13 ---
A Appendix
A.1 Identical Probability Tokens
Method MT-Bench GSM8K
AR Decoding 6.17 35.2
Tree Attention 6.23 35.2
Table 6: Quality/Accuracy comparison of AR-Decoding
and Tree Attention on MT-Bench and GSM8K. MT-
Bench results are taken from Cai et al. (2024). It shows
that Tree Attention has minimal impact on both answer
accuracy and quality.
Floating-point representation in the computer
has precision errors, commonly known as ‘floating-
point rounding errors’. Specifically, the precision
of floating-point numbers is determined by the
number of bits in the mantissa. In the IEEE 754
standard, the float32 type has a 23-bit mantissa,
meaning the smallest representable difference is
2−23, approximately 1.19×10−7. The float16 type,
with a 10-bit mantissa, can represent differences
as small as 2−10, or about 9.77×10−4. If the dif-
ference between two token probabilities is smaller
than the precision limit of floating-point represen-
tation, these two probabilities will be rounded to
the same value, and these tokens will be treated as
having identical probabilities during sampling.
AR Decoding uses ‘torch.argmax’ to return the
token with the highest probability. When the prob-
abilities are the same, ‘torch.argmax’ defaults to
returning the one with the smallest index. In Tree
Attention, the number of mask tokens is increased
compared to AR Decoding, and the attention score
of the mask tokens after the softmax operation is
not strictly zero, but rather a very small value close
to zero. These tiny non-zero values perturb the hid-
den representations, causing tokens that originally
had identical probabilities to now differ slightly,
resulting in a different argmax outcome compared
to AR Decoding.
Nevertheless, as shown in Table 6, due to the
extremely rare occurrence of this issue and the
affected probabilities being so close to each other,
the impact on experimental accuracy and model
performance is negligible.
A.2 Draft Tree Algorithm and Structure
Utilizing tree attention (Miao et al., 2024) to
extend the path in the verification phase has become
a widely adopted strategy for speculative decodingAlgorithm 1 Static Tree Based BFS
Require: Adjacency matrix M, Static tree struc-
tureTree , the last prompt token xt
Ensure: Merged Sequence S
1:Initialize S← ∅
2:Initialize root←xt
3:Initialize the current layer L←(root)
4:Initialize the current depth d←0
5:while d < Tree.depth do
6: Initialize next layer Lnext← ∅
7: Get all candidate tokens of LfromMin
parallel
8:xs=M[L]
9: Extract next layer tokens from xswithTree
10: Lnext=xs[Tree [d].index ]
11: Concatenate SandL
12: S←(S;L)
13: L←Lnext
14:end while
15:return S
methods.
In Token Recycling, we also use a heuristically
constructed token tree to perform the verification.
As shown in Figure 6, we construct a static and
unbalanced tree inspired by Cai et al. (2024). The
number kon a node indicates that it is the k-th can-
didate token for its parent node. The construction
process is below. We begin with a fully balanced
10-branch tree and use an independent validation
set to identify the top knodes that most frequently
yield correct tokens. These top knodes and their
children are retained to form a new tree, and the
process is repeated to identify the next set of top k
nodes. This iterative process continues until perfor-
mance no longer shows significant improvement.
The final tree is determined, and the kis set to
consider the maximum number of children across
all nodes and the memory requirement. While em-
pirical, this iterative approach has proven to be
effective. Further details on tuning the nare pro-
vided in Section 5.1. Overall, the tree we construct
contains 81 nodes (including the root node) in 6
layers. This means that each forward requires an
additional draft input of 79 tokens with a maximum
acceptance length of 6.
Building on the tree structure described above,
we construct a draft tree for the current content by
a BFS-like algorithm in the inference phase. As
described in Algorithm 1, we infill the child nodes

--- PAGE 14 ---
of each layer in turn according to the matrix. At
last, the merged sequence Sis returned and sent to
tree attention with Tree .
A.3 Reuse Mechanism Analysis
The output of LLM is context-dependent, it makes
us curious why reusing candidate tokens from pre-
vious generations works. Xiao et al. (2024) ana-
lyzed the distribution of attention logits in Trans-
formers and found that the first two layers focus
more on ‘local’ patterns, with recent tokens receiv-
ing much more attention. In later layers, the model
shifts its focus to the tokens at the beginning of the
sequence. To accelerate, candidate tokens need to
satisfy both local semantics and long-range depen-
dencies. As shown in Figure 1, draft tokens are di-
vided into accepted and rejected tokens. Accepted
tokens are those that appear in the previous query
response. In Token Recycling, candidate tokens for
all draft tokens are stored, not only accepted ones.
Each decoding step involves 79 draft tokens, mean-
ing 79 tokens receive/update their candidate tokens
at each step. The quantity of rejected tokens is
far more than accepted tokens, and their candidate
tokens are also stored in the matrix. In other words,
previous generations actually provide a large num-
ber of common patterns stored in the matrix, and
these patterns often meet local semantic needs.
From two perspectives, the reuse of these com-
mon patterns is justified:
For scenarios like sentence transitions, verb col-
locations, punctuation, or words split into multiple
tokens, there is often no need for long-range depen-
dencies. The common patterns stored in the matrix
can significantly accelerate the decoding process.
Rejected tokens may not satisfy the long-range
dependencies of the previous, but this does not
mean they do not meet the long-range dependencies
of the current. These tokens may be accepted in
the future. As shown in Table 4, the performance
gain from the candidate tokens of rejected tokens is
significantly greater than the gain from using only
the accepted tokens in the SpecBench experiments.
We also include a case study in A.4.
A.4 Case Study
Below, we present a real example from MT-Bench,
illustrating how accepted tokens and rejected to-
kens contribute to the acceleration of Token Recy-
cling. First Round:
• Prefix last token: [’guest’]•Merged sequence with draft tokens: [’guest’,
’speaker’, ’</s>’, ’speak’, ’<0x0A>’, ’Spe’,
’lect’, ’speaking’, ’spe’, ’at’, ’for’, ’</s>’,
’is’, ’could’, ’can’, ’would’, ’<0x0A>’, ’<s>’,
’sime’, ’multicol’, ’bolds’, ’</s>’, ’ings’,
’in’, ’<0x0A>’, ’a’, ’aking’, ’ures’, ’en-
gag’, ’aking’, ’a’, ’an’, ’the’, ’[’, ’our’, ’lo-
cal’, ’</s>’, ’up’, ’a’, ’an’, ’</s>’, ’<s>’,
’sime’, ’a’, ’be’, ’help’, ’like’, ’<0x0A>’,
’The’, ’Home’, ’guest’, ’<s>’, ’<0x0A>’, ’op-
portunity’, ’</s>’, ’local’, ’nearby’, ’guest’,
’up’, ’public’, ’guest’, ’</s>’, ’guest’, ’pub-
lic’, ’local’, ’local’, ’The’, ’first’, ’The’,
’<0x0A>’, ’event’, ’community’, ’Toast’,
’Buddh’, ’speaker’, ’speaker’, ’event’, ’time’,
’.’, ’,’, ’ism’]
•Accepted sequence: [’guest’, ’speaker’, ’at’,
’a’, ’local’, ’event’, ’could’]
Key observations:
•Rejected tokens still receive candidate to-
kens : For example, [’be’] was not directly
accepted, but its candidate tokens were stored.
•Candidate tokens of accepted tokens are
stored : [’local’] for [’a’] was chosen in this
round, but other candidate tokens were also
retained.
At this point, the adjacency matrix stores:
•’be’: [’able’, ’onto’, ’mistaken’, ’wrong’,
’the’, ’interested’, ’persu’, ’a’]
•’a’: [’local’, ’time’, ’personal’, ’professional’,
’few’, ’unique’, ’great’, ’low’]
Second Round:
• Current prefix last token: [’could’]
•Merged sequence with draft tokens: [’could’,
’be’, ’provide’, ’</s>’, ’help’, ’offer’,
’present’, ’not’, ’actually’, ’a’, ’an’, ’the’,
’</s>’, ’just’, ’one’, ’benef’, ’both’, ’,’,
"’", ’you’, ’for’, ’<s>’, ’sime’, ’multicol’,
’you’, ’overcome’, ’some’, ’</s>’, ’public’,
’<unk>’, ’great’, ’fant’, ’wonderful’, ’valu-
able’, ’unique’, ’perfect’, ’</s>’, ’ter’, ’up’,
’event’, ’local’, ’up’, ’local’, ’<s>’, ’not’, ’of’,
’ited’, ’models’, ’such’, ’like’, ’my’, ’The’,
’as’, ’comp’, ’<s>’, ’opportunity’, ’guest’,
’</s>’, ’speaker’, ’way’, ’astic’, ’asy’, ’bl’,

--- PAGE 15 ---
’ins’, ’guest’, ’coming’, ’coming’, ’as’, ’first’,
’a’, ’at’, ’is’, ’would’, ’event’, ’<s>’, ’natural’,
’public’, ’a’, ’a’, ’an’, ’could’]
•Accepted sequence: [’could’, ’be’, ’a’,
’great’]
At this point:
•Candidate tokens from rejected tokens are
matched : ’be’ correctly predicted ’a’.
•Candidate tokens from accepted tokens are
matched : ’a’ successfully predicted ’great’.
A.5 Explanation of Three Update Strategies
Suppose our merged sequence is:
S= [a, b, c, a , d]
, which token aappears twice. Currently, corre-
sponding output tokens are stored in matrix O:
O[0] = [ a1, a2, a3, a4]
O[1] = [ b1, b2, b3, b4]
O[2] = [ c1, c2, c3, c4]
O[3] = [ a5, a6, a7, a8](second occurrence of a)
O[4] = [ d1, d2, d3, d4]
The three strategies for updating the adjacency
matrix Mare:
•Cur (uncontrolled): Update directly without
position control, potentially mixing outputs
from repeated tokens. For example: M[S] =
O, M [a] = [a1, a2, a3, a8](the last a8is from
the second occurrence).
•First: Record positions of each token’s first oc-
currence: pos= [0( a),1(b),2(c),4(d)]. Re-
sulting in: M[S[pos]] = O[pos], M[a] =
[a1, a2, a3, a4]
•Last: Record positions of each token’s last oc-
currence: pos= [1( b),2(c),3(a),4(d)]. Re-
sulting in: M[S[pos]] = O[pos], M[a] =
[a5, a6, a7, a8]
These different update strategies have a slight
impact on Mean Accepted Tokens. However, ex-
tracting these positions introduces additional la-
tency, ultimately reducing overall acceleration ef-
fectiveness compared to the simpler uncontrolled
approach.

--- PAGE 16 ---
/uni00000035/uni00000052/uni00000052/uni00000057
/uni00000013
/uni00000013
/uni00000013
/uni00000013
/uni00000013
/uni00000013 /uni00000014/uni00000014 /uni00000015/uni00000014
/uni00000013
/uni00000013/uni00000015
/uni00000013/uni00000016 /uni00000017/uni00000014
/uni00000013
/uni00000013/uni00000014/uni00000015
/uni00000013/uni00000016
/uni00000013/uni00000017
/uni00000013/uni00000018 /uni00000019 /uni0000001a/uni00000014
/uni00000013
/uni00000013
/uni00000013/uni00000014 /uni00000015/uni00000015
/uni00000013
/uni00000013/uni00000014/uni00000016
/uni00000013/uni00000017
/uni00000013/uni00000018
/uni00000013/uni00000019
/uni00000013/uni0000001a
/uni00000013/uni00000014
/uni00000013
/uni00000013
/uni00000013
/uni00000013/uni00000014/uni00000014
/uni00000013/uni00000015 /uni00000016/uni00000015
/uni00000013
/uni00000013
/uni00000013/uni00000014 /uni00000015/uni00000016
/uni00000013
/uni00000013
/uni00000013/uni00000014/uni00000017
/uni00000013
/uni00000013/uni00000018
/uni00000013
/uni00000013/uni00000019
/uni00000013/uni0000001a
/uni00000013Figure 6: The static tree used in Token Recycling.
