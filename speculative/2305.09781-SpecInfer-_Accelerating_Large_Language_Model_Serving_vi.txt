SpecInfer: Tăng tốc Phục vụ Mô hình Ngôn ngữ Lớn
với Suy luận Suy đoán Dựa trên Cây và Xác minh

Xupeng Miao∗
Carnegie Mellon University
Pittsburgh, PA, USA
xupeng@cmu.edu

Gabriele Oliaro∗
Carnegie Mellon University
Pittsburgh, PA, USA
goliaro@cs.cmu.edu

Zhihao Zhang∗
Carnegie Mellon University
Pittsburgh, PA, USA
zhihaoz3@andrew.cmu.edu

Xinhao Cheng∗
Carnegie Mellon University
Pittsburgh, PA, USA
xinhaoc@andrew.cmu.edu

Zeyu Wang
Carnegie Mellon University
Pittsburgh, PA, USA
zeyuwang@alumni.cmu.edu

Zhengxin Zhang
Tsinghua University
Beijing, China
zhang-zx21@mails.tsinghua.edu.cn

Rae Ying Yee Wong
Stanford University
Stanford, CA, USA
raewong@stanford.edu

Alan Zhu
Carnegie Mellon University
Pittsburgh, PA, USA
aczhu@andrew.cmu.edu

Lijie Yang
Carnegie Mellon University
Pittsburgh, PA, USA
lijiey@andrew.cmu.edu

Xiaoxiang Shi
Shanghai Jiao Tong University
Shanghai, China
lambda7shi@sjtu.edu.cn

Chunan Shi
Peking University
Beijing, China
spirited_away@pku.edu.cn

Zhuoming Chen
Carnegie Mellon University
Pittsburgh, PA, USA
zhuominc@andrew.cmu.edu

Daiyaan Arfeen
Carnegie Mellon University
Pittsburgh, PA, USA
marfeen@andrew.cmu.edu

Reyna Abhyankar
University of California, San Diego
San Diego, CA, USA
vabhyank@ucsd.edu

Zhihao Jia
Carnegie Mellon University
Pittsburgh, PA, USA
zhihao@cmu.edu

Tóm tắt
Bài báo này giới thiệu SpecInfer, một hệ thống tăng tốc phục vụ mô hình ngôn ngữ lớn sinh tạo (LLM) với suy luận suy đoán dựa trên cây và xác minh. Ý tưởng chính đằng sau SpecInfer là tận dụng các mô hình suy đoán nhỏ để dự đoán đầu ra của LLM; các dự đoán được tổ chức dưới dạng cây token, mà các nút của nó mỗi nút đại diện cho một chuỗi token ứng viên. Tính đúng đắn của tất cả các chuỗi token ứng viên được biểu diễn bởi một cây token được xác minh so với LLM song song bằng một cơ chế giải mã song song dựa trên cây mới. SpecInfer sử dụng một LLM như một bộ xác minh cây token thay vì một bộ giải mã tăng dần, điều này giảm đáng kể độ trễ end-to-end và yêu cầu tính toán để phục vụ các LLM sinh tạo trong khi có thể chứng minh bảo toàn chất lượng mô hình. Đánh giá của chúng tôi cho thấy SpecInfer vượt trội hơn các hệ thống phục vụ LLM hiện có từ 1.5-2.8× cho suy luận LLM phân tán và từ 2.6-3.5× cho suy luận LLM dựa trên offloading, trong khi bảo toàn cùng hiệu suất sinh tạo. SpecInfer có sẵn công khai tại https://github.com/flexflow/FlexFlow/

Từ khóa: phục vụ mô hình ngôn ngữ lớn, giải mã suy đoán, xác minh cây token

Định dạng Tham chiếu ACM:
Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, và Zhihao Jia. 2024. SpecInfer: Tăng tốc Phục vụ Mô hình Ngôn ngữ Lớn với Suy luận Suy đoán Dựa trên Cây và Xác minh. Trong Hội nghị Quốc tế ACM lần thứ 29 về Hỗ trợ Kiến trúc cho Ngôn ngữ Lập trình và Hệ điều hành, Tập 3 (ASPLOS '24), 27 tháng 4 - 1 tháng 5, 2024, La Jolla, CA, USA. ACM, New York, NY, USA, 18 trang. https://doi.org/10.1145/3620666.3651335

1 Giới thiệu
Các mô hình ngôn ngữ lớn sinh tạo (LLM), như ChatGPT [3] và GPT-4 [33], đã được chứng minh là mạnh mẽ trong nhiều lĩnh vực ứng dụng khác nhau, bao gồm trả lời câu hỏi, tổng hợp chương trình và tự động hóa tác vụ [26,56]. Tuy nhiên, việc phục vụ những LLM này một cách nhanh chóng và rẻ tiền là một thách thức do khối lượng tham số lớn, kiến trúc phức tạp và yêu cầu tính toán cao của chúng. Ví dụ, kiến trúc GPT-3 lớn nhất có 175 tỷ tham số, đòi hỏi hơn tám GPU NVIDIA 40GB A100 để lưu trữ ở dạng số thực độ chính xác nửa, và mất vài giây để phục vụ một yêu cầu suy luận duy nhất [3].

Một LLM thường nhận đầu vào là một chuỗi token, được gọi là prompt, và sinh ra các token tiếp theo từng cái một, như được hiển thị trong Hình 1a. Việc sinh ra mỗi token trong chuỗi được điều kiện bởi prompt đầu vào và các token được sinh ra trước đó và không xem xét các token tương lai. Cách tiếp cận này cũng được gọi là giải mã tự hồi quy vì mỗi token được sinh ra cũng được sử dụng làm đầu vào để sinh ra các token tương lai. Sự phụ thuộc này giữa các token là quan trọng đối với nhiều tác vụ NLP đòi hỏi bảo toàn thứ tự và ngữ cảnh của các token được sinh ra, chẳng hạn như hoàn thành văn bản [55].

Các hệ thống LLM hiện có thường sử dụng cách tiếp cận giải mã tăng dần để phục vụ một yêu cầu, trong đó hệ thống tính toán các kích hoạt cho tất cả các token prompt trong một bước duy nhất và sau đó lặp lại giải mã một token mới bằng cách sử dụng prompt đầu vào và tất cả các token được sinh ra trước đó [27]. Cách tiếp cận này tôn trọng các phụ thuộc dữ liệu giữa các token, nhưng đạt được hiệu suất thời gian chạy không tối ưu và sử dụng GPU hạn chế, vì mức độ song song trong mỗi yêu cầu bị hạn chế rất nhiều trong giai đoạn tăng dần. Ngoài ra, cơ chế attention của Transformer [48] đòi hỏi truy cập các key và value của tất cả các token trước đó để tính toán đầu ra attention của một token mới. Để tránh tính toán lại các key và value cho tất cả các token đi trước, các hệ thống LLM ngày nay sử dụng cơ chế caching để lưu trữ các key và value của chúng để tái sử dụng trong các lần lặp tương lai. Đối với các tác vụ sinh tạo chuỗi dài (ví dụ, GPT-4 hỗ trợ lên đến 32K token trong một yêu cầu), việc caching key và value tạo ra chi phí bộ nhớ đáng kể, ngăn cản các hệ thống hiện có phục vụ một số lượng lớn yêu cầu song song do yêu cầu bộ nhớ để cache key và value của chúng.

Được thúc đẩy bởi ý tưởng thực thi suy đoán trong tối ưu hóa bộ xử lý [13,42], các nghiên cứu gần đây giới thiệu suy luận suy đoán dựa trên chuỗi, tận dụng một mô hình suy đoán nhỏ (SSM) để sinh ra một chuỗi token và sử dụng một LLM để kiểm tra tính đúng đắn của chúng song song [5,22,25,44,51]. Những nỗ lực này chỉ xem xét một chuỗi token được sinh ra bởi một SSM duy nhất để suy đoán, điều này không thể phù hợp tốt với một LLM do khoảng cách năng lực mô hình giữa chúng, vì các SSM thường nhỏ hơn hàng bậc độ lớn so với LLM để duy trì chi phí bộ nhớ và thời gian chạy thấp.

Bài báo này giới thiệu SpecInfer, một hệ thống cải thiện độ trễ end-to-end và hiệu quả tính toán của việc phục vụ LLM với suy luận suy đoán dựa trên cây và xác minh. Hình 1b minh họa sự so sánh giữa giải mã tăng dần hiện có, suy luận suy đoán dựa trên chuỗi, và suy luận suy đoán dựa trên cây của chúng tôi. Một insight chính đằng sau SpecInfer là đồng thời xem xét sự đa dạng của các ứng viên suy đoán (thay vì chỉ một như trong các cách tiếp cận hiện có) để tối đa hóa hiệu suất suy đoán. Những ứng viên này được tổ chức như một cây token, mà các nút của nó mỗi nút đại diện cho một chuỗi token được suy đoán. Tính đúng đắn của tất cả các chuỗi token ứng viên được xác minh so với LLM song song, điều này cho phép SpecInfer tăng đáng kể số lượng token được sinh ra trong một bước giải mã LLM.

So với suy luận suy đoán dựa trên chuỗi, tận dụng cấu trúc cây có thể cải thiện đáng kể tỷ lệ thành công của việc xác minh một token (ví dụ, từ 52-57% lên 96-97% cho giải mã ngẫu nhiên như được hiển thị trong Bảng 1). Tuy nhiên, việc thực hiện cải thiện này đòi hỏi giải quyết hai thách thức độc đáo. Tiếp theo, chúng tôi trình bày chi tiết về những thách thức này và các ý tưởng chính mà SpecInfer sử dụng để giải quyết chúng.

Đầu tiên, SpecInfer phải khám phá một không gian tìm kiếm cực kỳ lớn của các chuỗi token ứng viên để tối đa hóa hiệu suất suy đoán. Mặc dù ý tưởng thực thi suy đoán đã được triển khai rộng rãi trong nhiều tác vụ tối ưu hóa khác nhau trong kiến trúc máy tính và hệ thống, bao gồm dự đoán nhánh trong các bộ xử lý pipeline hiện đại và dự đoán giá trị để pre-fetch bộ nhớ và tệp [13,42], không gian tìm kiếm được SpecInfer xem xét lớn hơn đáng kể do hai lý do: (1) các LLM hiện đại thường liên quan đến từ vựng rất lớn, và (2) tối đa hóa hiệu suất suy đoán đòi hỏi dự đoán nhiều token tương lai (thay vì chỉ token tiếp theo). Ví dụ, tất cả các LLM trong họ mô hình OPT xem xét 50,272 token khác nhau có thể trong từ vựng của chúng, trong khi SpecInfer có thể dự đoán đúng 4 token tiếp theo trung bình. Đạt được mục tiêu này đòi hỏi xem xét một không gian tìm kiếm của 502724≈6×1018 tổ hợp token khác nhau.

SpecInfer tận dụng các biến thể được chưng cất, lượng tử hóa và/hoặc cắt tỉa hiện có của một LLM, mà chúng tôi gọi là các mô hình suy đoán nhỏ (SSM), để hướng dẫn suy đoán. Một thách thức chính của việc sử dụng SSM cho suy luận suy đoán là sự phù hợp giữa một SSM và một LLM vốn bị giới hạn bởi khoảng cách năng lực mô hình, vì một SSM thường nhỏ hơn 100-1000× so với một LLM. Thay vì sử dụng một SSM duy nhất cho suy đoán dựa trên chuỗi, SpecInfer tối đa hóa hiệu suất suy đoán bằng cách đồng thời xem xét nhiều chuỗi token được tổ chức trong cấu trúc cây cho một prompt đầu vào nhất định. SpecInfer giới thiệu một cơ chế dựa trên mở rộng và hợp nhất để xây dựng cây token bằng cách khai thác sự đa dạng trong một SSM duy nhất và trên nhiều SSM, tương ứng.

Thách thức thứ hai mà SpecInfer phải giải quyết là xác minh các token được suy đoán. Nhiều ứng dụng LLM thực hiện giải mã ngẫu nhiên, lấy mẫu token tiếp theo từ một phân phối xác suất thay vì sinh ra một token một cách quyết định. Để bảo toàn hiệu suất sinh tạo của LLM, SpecInfer phải đảm bảo rằng cơ chế suy luận suy đoán dựa trên cây và xác minh của nó sinh ra token tiếp theo bằng cách tuân theo chính xác cùng phân phối xác suất như giải mã tăng dần. Để đạt được mục tiêu này, chúng tôi đề xuất lấy mẫu suy đoán đa bước, một cách tiếp cận lấy mẫu mới cho các SSM đảm bảo tương đương trong khi tối đa hóa số lượng token được suy đoán có thể được xác minh. Để tối thiểu hóa chi phí xác minh cây token, SpecInfer giới thiệu một cơ chế giải mã song song dựa trên cây, đồng thời xác minh tất cả các token của một cây token so với đầu ra của LLM trong một bước giải mã LLM duy nhất.

Bằng cách tận dụng suy luận suy đoán dựa trên cây và xác minh, SpecInfer tăng tốc cả suy luận LLM phân tán trên nhiều GPU và suy luận LLM dựa trên offloading trên một GPU. Đánh giá của chúng tôi cho thấy SpecInfer vượt trội hơn các hệ thống phục vụ LLM hiện có từ 1.5-2.8× cho suy luận LLM phân tán và từ 2.6-3.5× cho suy luận LLM dựa trên offloading, trong khi bảo toàn cùng độ chính xác sinh tạo.

Để tóm tắt, chúng tôi đóng góp như sau:
• Chúng tôi trình bày SpecInfer, một hệ thống suy luận suy đoán dựa trên cây và xác minh cho việc phục vụ LLM.
• Để tối đa hóa hiệu suất suy đoán, chúng tôi đề xuất một phương pháp dựa trên hợp nhất và mở rộng để xây dựng cây token bằng cách khai thác sự đa dạng trong và giữa các SSM, tương ứng.
• Để tối thiểu hóa chi phí xác minh, chúng tôi giới thiệu một cơ chế giải mã song song dựa trên cây để đồng thời xác minh tất cả các token của một cây token.
• Chúng tôi đánh giá SpecInfer và cho thấy nó vượt trội hơn các hệ thống hiện có lên đến 2.8× cho suy luận phân tán và lên đến 3.5× cho suy luận dựa trên offloading.

2 Tổng quan về SpecInfer
Hình 2 cho thấy tổng quan về SpecInfer, bao gồm một bộ suy đoán dựa trên học máy nhận đầu vào là một chuỗi token và tạo ra một cây token được suy đoán. Mục tiêu của bộ suy đoán là dự đoán đầu ra của LLM bằng cách tối đa hóa sự chồng lặp giữa cây token được suy đoán và các token được sinh ra bởi LLM sử dụng giải mã tăng dần (Thuật toán 1).

Thuật toán 1 Thuật toán giải mã tăng dần được sử dụng trong các hệ thống phục vụ LLM hiện có.
1: Đầu vào: Một chuỗi token đầu vào ℐ
2: Đầu ra: Một chuỗi token được sinh ra
3: 𝒮=ℐ
4: while true do
5: 𝑡=Decode(LLM,𝒮)
6: 𝒮.append(𝑡)
7: if 𝑡=⟨EOS⟩ then
8: Return 𝒮

Có một số cách để chuẩn bị SSM cho suy luận suy đoán. Đầu tiên, các LLM hiện đại thường có nhiều kiến trúc nhỏ hơn được pre-train cùng với LLM bằng cách sử dụng cùng bộ dữ liệu. Ví dụ, ngoài mô hình OPT-175B với 175 tỷ tham số, họ mô hình OPT cũng bao gồm OPT-125M và OPT-350M, hai biến thể với 125 triệu và 350 triệu tham số, được pre-train bằng cách sử dụng cùng bộ dữ liệu như OPT-175B [57]. Những mô hình nhỏ được pre-train này có thể được sử dụng trực tiếp làm SSM. Thứ hai, để cải thiện độ bao phủ của các token được suy đoán từ SSM, SpecInfer sử dụng một phương pháp suy đoán dựa trên mở rộng và dựa trên hợp nhất như được hiển thị ở đầu Hình 2. Các token được suy đoán được tổ chức trong cấu trúc cây token.

Việc sử dụng LLM của SpecInfer cũng khác với các hệ thống phục vụ LLM hiện có. Thay vì sử dụng LLM như một bộ giải mã tăng dần dự đoán token đơn tiếp theo, SpecInfer sử dụng LLM như một bộ xác minh cây token xác minh một cây token được suy đoán so với đầu ra của LLM. Đối với mỗi token, SpecInfer tính toán các kích hoạt của nó bằng cách xem xét tất cả các tổ tiên của nó trong cây token như các token đi trước của nó. Ví dụ, trong Hình 2, đầu ra attention của token 𝑡3,0 được tính toán dựa trên chuỗi (𝑡0,𝑡1,0,𝑡2,1,𝑡3,0), trong đó 𝑡0, 𝑡1,0, và 𝑡2,1 là các tổ tiên của 𝑡3,0 trong cây token. SpecInfer bao gồm một cơ chế giải mã song song dựa trên cây mới để đồng thời xác minh tất cả các token của một cây token trong một bước giải mã LLM duy nhất.

Suy luận suy đoán và xác minh cây token của SpecInfer cung cấp hai lợi thế chính so với cách tiếp cận giải mã tăng dần của các hệ thống suy luận LLM hiện có.

Giảm truy cập bộ nhớ đến tham số LLM. Hiệu suất suy luận LLM phần lớn bị hạn chế bởi việc truy cập bộ nhớ GPU. Trong cách tiếp cận giải mã tăng dần hiện có, việc sinh ra một token duy nhất đòi hỏi truy cập tất cả tham số của một LLM. Vấn đề được làm trầm trọng hơn đối với các hệ thống suy luận LLM dựa trên offloading, sử dụng tài nguyên tính toán hạn chế như một GPU thông thường duy nhất để phục vụ LLM bằng cách sử dụng CPU DRAM và lưu trữ bền vững để lưu tham số mô hình và tải những tham số này vào bộ nhớ băng thông cao (HBM) của GPU để tính toán. So với cách tiếp cận giải mã tăng dần, SpecInfer giảm đáng kể việc truy cập tham số LLM bất cứ khi nào sự chồng lặp giữa một cây token được suy đoán và đầu ra thực tế của LLM không rỗng. Việc giảm truy cập bộ nhớ thiết bị GPU và giảm truyền dữ liệu giữa bộ nhớ GPU và CPU cũng có thể trực tiếp dẫn đến giảm tiêu thụ năng lượng, vì việc truy cập GPU HBM tiêu thụ năng lượng nhiều hơn hai hoặc ba bậc độ lớn so với các phép toán số học dấu phẩy động.

Giảm độ trễ suy luận end-to-end. Phục vụ LLM gặp phải độ trễ suy luận end-to-end dài. Ví dụ, kiến trúc GPT-3 bao gồm 175 tỷ tham số và đòi hỏi nhiều giây để phục vụ một yêu cầu. Trong cách tiếp cận giải mã tăng dần hiện có, việc tính toán để sinh ra mỗi token phụ thuộc vào các key và value của tất cả các token được sinh ra trước đó, điều này tạo ra các phụ thuộc tuần tự giữa các token và đòi hỏi các hệ thống phục vụ LLM hiện đại phải tuần tự hóa việc sinh ra các token khác nhau cho mỗi yêu cầu. Trong SpecInfer, các LLM được sử dụng như một bộ xác minh nhận một cây token được suy đoán làm đầu vào và có thể đồng thời kiểm tra tất cả các token trong cây token bằng cách thực hiện một lần xác minh duy nhất qua LLM. Cách tiếp cận này cho phép song song hóa giữa các token khác nhau trong một yêu cầu duy nhất và giảm độ trễ suy luận end-to-end của LLM.

3 Bộ suy đoán dựa trên học máy
Các phương pháp giải mã suy đoán hiện có thực hiện suy đoán dựa trên chuỗi, trong đó một SSM dự đoán một chuỗi token duy nhất để được xác minh bởi một LLM. Tuy nhiên, một hạn chế chính của một chuỗi được suy đoán duy nhất là xác suất của sự phù hợp thành công giữa LLM và chuỗi token được suy đoán giảm theo cấp số nhân với độ dài phù hợp dự kiến. Điều này có thể được làm trầm trọng hơn bởi thực tế là suy đoán chỉ bao gồm một token ứng viên duy nhất để xác minh mỗi bước, dẫn đến hiệu suất suy đoán không tối ưu. Mặt khác, bằng cách khuyến khích các ứng viên được suy đoán đa dạng hơn mỗi bước, xác suất của một kết quả phù hợp thành công mỗi bước (tức là, token được giải mã bởi LLM nằm trong pool ứng viên này) có thể được cải thiện rất nhiều. Để đạt được điều này, SpecInfer nhằm xây dựng một cây các ứng viên được suy đoán bằng cách khai thác sự đa dạng trong một SSM duy nhất và trên nhiều SSM. Cụ thể, bộ suy đoán dựa trên học máy của SpecInfer tổng hợp các dự đoán của một hoặc nhiều SSM để tối đa hóa hiệu suất suy đoán trong khi duy trì chi phí bộ nhớ thấp và độ trễ suy luận. SpecInfer sử dụng một cây token để tổ chức các token được tạo ra bởi bộ suy đoán và giới thiệu hai phương pháp để xây dựng cây token: xây dựng cây dựa trên mở rộng và hợp nhất.

Định nghĩa 3.1 (Cây Token). Một cây token 𝒩 là một cấu trúc cây, trong đó mỗi nút 𝑢∈𝒩 được gắn nhãn với một token 𝑡𝑢, và 𝑝𝑢 đại diện cho nút cha của 𝑢 trong cây token. Đối với mỗi nút 𝑢, 𝑆𝑢 đại diện cho một chuỗi token được xác định bằng cách nối 𝑆𝑝𝑢 và {𝑡𝑢}1.

Xây dựng cây token dựa trên mở rộng. Một cách tiếp cận để tạo ra một cây token liên quan đến việc tạo ra nhiều token từ một SSM trong một bước giải mã duy nhất. Cách tiếp cận này được thúc đẩy bởi một quan sát quan trọng rằng khi một SSM không phù hợp với một LLM (tức là, hai mô hình chọn các token top-1 khác nhau), token được chọn bởi LLM thường nằm trong số các token top-𝑘 từ SSM cho các giá trị 𝑘 rất nhỏ. Bảng 1 cho thấy tỷ lệ thành công của việc xác minh một token bằng cách sử dụng các token top-𝑘 tạo ra từ một SSM, trong đó một xác minh thành công nếu token được chọn bởi LLM nằm trong số các token top-𝑘 từ SSM. So với chỉ sử dụng token top-1 từ một SSM, sử dụng các token top-5 có thể tăng tỷ lệ thành công từ 70% lên 89% cho giải mã tham lam và từ 57% lên 97% cho giải mã ngẫu nhiên.

Việc chọn trực tiếp các token top-𝑘 ở mỗi bước dẫn đến sự tăng theo cấp số nhân trong số lượng chuỗi token tiềm năng, điều này làm tăng đáng kể độ trễ suy luận và chi phí bộ nhớ. Do đó, chúng tôi áp dụng một chiến lược tĩnh mở rộng cây token theo một cấu hình mở rộng được đặt trước được biểu diễn như một vector số nguyên ⟨𝑘1,𝑘2,...,𝑘𝑚⟩, trong đó 𝑚 biểu thị số bước giải mã suy đoán tối đa, và 𝑘𝑖 chỉ ra số lượng token để mở rộng cho mỗi token trong bước thứ 𝑖. Ví dụ, Hình 3 minh họa cấu hình mở rộng ⟨2,2,1⟩, dẫn đến bốn chuỗi token. Đánh giá của chúng tôi (xem Phần 6.4) cho thấy rằng ngay cả một chiến lược đơn giản cũng có thể tạo ra kết quả suy đoán có độ chính xác cao. Chúng tôi thừa nhận rằng việc mở rộng động một cây token từ một SSM là một vấn đề nghiên cứu mở nằm ngoài phạm vi của bài báo này, mà chúng tôi để lại như công việc tương lai.

Xây dựng cây token dựa trên hợp nhất. Ngoài việc sử dụng một SSM duy nhất, SpecInfer cũng có thể kết hợp nhiều SSM để cùng nhau dự đoán đầu ra của một LLM. SpecInfer sử dụng một phương pháp không giám sát để tập thể boost-tune một pool các SSM để điều chỉnh đầu ra của chúng với đầu ra của LLM bằng cách tận dụng adaptive boosting [12]. SpecInfer sử dụng các SSM để dự đoán vài token tiếp theo mà một LLM sẽ sinh ra, và sử dụng các bộ dữ liệu văn bản tổng quát (ví dụ, corpus OpenWebText [15] trong đánh giá của chúng tôi) để thích ứng điều chỉnh đầu ra tổng hợp của nhiều SSM với LLM theo cách hoàn toàn không giám sát. Cụ thể, SpecInfer chuyển đổi một corpus văn bản thành một tập hợp các mẫu prompt và sử dụng LLM để sinh ra một chuỗi token cho mỗi prompt. SpecInfer đầu tiên fine-tune một SSM tại một thời điểm đến mức tối đa và đánh dấu tất cả các mẫu prompt nơi SSM và LLM sinh ra các token tiếp theo giống hệt nhau. Tiếp theo, SpecInfer lọc tất cả các mẫu prompt được đánh dấu và sử dụng tất cả các mẫu còn lại trong corpus để fine-tune SSM tiếp theo đến mức tối đa.

Bằng cách lặp lại quá trình này cho mọi SSM trong pool, SpecInfer thu được một tập hợp đa dạng các SSM mà đầu ra tổng hợp của chúng chồng lặp phần lớn với đầu ra của LLM trên corpus huấn luyện. Tất cả các SSM có độ trễ suy luận giống hệt nhau, và do đó việc chạy tất cả các SSM trên các GPU khác nhau song song không làm tăng độ trễ của suy luận suy đoán so với việc sử dụng một SSM duy nhất. Ngoài ra, SpecInfer sử dụng song song dữ liệu để phục vụ các SSM trên nhiều GPU, và do đó việc sử dụng nhiều SSM không làm tăng chi phí bộ nhớ trên mỗi GPU. Trong trường hợp nhiều SSM được sử dụng, đầu ra của mỗi SSM được xem như một cây token, và SpecInfer thực hiện hợp nhất cây token để tổng hợp tất cả các token được suy đoán trong một cấu trúc cây duy nhất.

Định nghĩa 3.2 (Hợp nhất Cây Token). ℳ là sự hợp nhất cây của 𝑚 cây token {𝒩𝑖}(1≤𝑖≤𝑚) khi và chỉ khi ∀1≤𝑖≤𝑚,∀𝑢∈𝒩𝑖,∃𝑣∈ℳ sao cho 𝑆𝑣=𝑆𝑢 và ngược lại.

Một cách trực quan, mỗi cây token đại diện cho một tập hợp các chuỗi token. Việc hợp nhất nhiều cây token tạo ra một cây mới bao gồm tất cả các chuỗi token của các cây gốc. Ví dụ, Hình 3 cho thấy cây token được tạo ra bằng cách hợp nhất bốn chuỗi token. Mỗi chuỗi token được xác định bởi một nút trong cây token đã hợp nhất.

Lưu ý rằng, ngoài boosting, có một số phương pháp học tập tập hợp khác (ví dụ, voting, bagging, và stacking) [14] có thể được sử dụng để kết hợp các đầu ra từ nhiều SSM, và chúng tôi để lại việc khám phá như công việc tương lai.

4 Bộ xác minh cây token
Phần này giới thiệu bộ xác minh cây token của SpecInfer, nhận đầu vào là một cây token được tạo ra bởi bộ suy đoán và xác minh tính đúng đắn của các token của nó so với đầu ra của LLM. Một ý tưởng chính đằng sau thiết kế của SpecInfer là đồng thời xác minh tất cả các chuỗi của một cây token so với đầu ra gốc của LLM bằng cách thực hiện một lần duy nhất qua các tham số của LLM. Chức năng này cho phép SpecInfer cơ hội giải mã nhiều token (thay vì một token duy nhất trong giải mã tăng dần), dẫn đến giảm truy cập bộ nhớ đến các tham số của LLM. Một thách thức mà SpecInfer phải giải quyết trong xác minh cây token là tính toán hiệu quả các điểm attention cho tất cả các chuỗi của một cây token. Để đạt được điều này, chúng tôi giới thiệu tree attention, tổng quát hóa cơ chế attention [48] từ chuỗi sang cấu trúc cây. Ngoài ra, chúng tôi phát triển một cơ chế giải mã song song dựa trên cây có thể giải mã tất cả các token trong một cây token song song.

§4.1 và §4.2 mô tả tree attention và giải mã song song dựa trên cây. §4.3 giới thiệu cơ chế để xác minh một cây token so với đầu ra của LLM.

4.1 Tree Attention
Các mô hình ngôn ngữ dựa trên Transformer sử dụng cơ chế attention để suy luận về thông tin tuần tự [48]. Các LLM thường sử dụng self-attention đa đầu chỉ decoder, nhận một tensor đầu vào 𝑋 duy nhất và tính toán một tensor đầu ra 𝑂 thông qua các công thức nhân tỷ lệ như sau.

𝑄𝑖=𝑋×𝑊𝑄𝑖, 𝐾𝑖=𝑋×𝑊𝐾𝑖, (1)
𝑉𝑖=𝑋×𝑊𝑉𝑖, 𝐴𝑖=(𝑄𝑖×𝐾𝑇𝑖)/√𝑑, (2)
𝐻𝑖=softmaxmask(𝐴𝑖)𝑉𝑖, 𝑂=(𝐻1,...,𝐻ℎ)𝑊𝑂 (3)

trong đó 𝑄𝑖,𝐾𝑖, và 𝑉𝑖 biểu thị các tensor query, key, và value của đầu attention thứ 𝑖 (1≤𝑖≤ℎ), 𝑊𝑄𝑖,𝑊𝐾𝑖, và 𝑊𝑉𝑖 là các ma trận trọng số tương ứng. 𝐴𝑖 là một ma trận 𝑙×𝑙 đại diện cho các điểm attention giữa các token khác nhau trong chuỗi đầu vào, trong đó 𝑙 là độ dài chuỗi. Để bảo toàn tính nhân quả khi sinh ra token (tức là, một token trong chuỗi không nên ảnh hưởng đến trạng thái ẩn của bất kỳ token đi trước nào), hàm mask nhân quả sau được áp dụng:

mask(𝐴)𝑗𝑘={𝐴𝑗𝑘 nếu 𝑗≥𝑘, −∞ nếu 𝑗<𝑘} (4)

Một cách trực quan, khi tính toán đầu ra attention của token thứ 𝑗 trong chuỗi, tất cả các token tiếp theo nên có điểm attention là −∞ để chỉ ra rằng các token tiếp theo sẽ không ảnh hưởng đến đầu ra attention của token thứ 𝑗2. Trong Phương trình 3, 𝐻𝑖 đại diện cho đầu ra của đầu attention thứ 𝑖, và 𝑊𝑂 là một ma trận trọng số được sử dụng để tính toán đầu ra cuối cùng của lớp attention.

Lưu ý rằng cơ chế attention được mô tả ở trên chỉ áp dụng cho một chuỗi token. Chúng tôi tổng quát hóa cơ chế attention cho các cấu trúc cây tùy ý.

Định nghĩa 4.1 (Tree Attention). Đối với một cây token 𝒩 và một nút tùy ý 𝑢∈𝒩, tree attention của nó được định nghĩa như đầu ra của việc tính toán attention chuỗi dựa trên Transformer gốc trên 𝑆𝑢 (tức là, chuỗi token được đại diện bởi 𝑢):
TreeAttention(𝑢)=Attention(𝑆𝑢)∀𝑢∈𝒩 (5)

Đối với một tập hợp nhất định các chuỗi token, vì mỗi chuỗi 𝑆 được bao phủ bởi một nút của cây token đã hợp nhất, việc thực hiện tree attention trên cây token cho phép SpecInfer thu được đầu ra attention cho tất cả các chuỗi token.

4.2 Giải mã song song dựa trên cây
Phần này mô tả cơ chế giải mã song song dựa trên cây của SpecInfer để tính toán tree attention cho tất cả các token trong một cây token song song. Một thách thức chính mà SpecInfer phải giải quyết trong việc tính toán tree attention là quản lý key-value cache. Cụ thể, cơ chế attention của Transformer [48] đòi hỏi truy cập các key và value của tất cả các token đi trước để tính toán đầu ra attention của mỗi token mới, như được hiển thị trong Phương trình 3. Để tránh tính toán lại những key và value này, các hệ thống suy luận LLM ngày nay thường cache các key và value của tất cả các token để tái sử dụng trong các lần lặp tương lai, vì mối quan hệ nhân quả đảm bảo rằng key và value của một token vẫn không thay đổi trong các lần lặp tiếp theo (tức là, mask(A)jk=−∞ đối với bất kỳ 𝑗<𝑘 nào). Tuy nhiên, khi tính toán tree attention, các chuỗi khác nhau trong một cây token có thể bao gồm các key-value cache xung đột. Ví dụ, đối với cây token được suy đoán trong Hình 4, hai chuỗi token (𝑡2,𝑡3,𝑡4,𝑡5) và (𝑡2,𝑡3,𝑡8,𝑡9) có các key và value khác nhau cho vị trí thứ ba và thứ tư.

Một cách tiếp cận đơn giản để hỗ trợ key-value cache là sử dụng giải mã dựa trên chuỗi của các hệ thống suy luận LLM hiện có và sử dụng một key-value cache khác nhau cho mỗi chuỗi của một cây token, như được hiển thị ở bên trái Hình 4. Tuy nhiên, cách tiếp cận này rất tốn kém về mặt tính toán và liên quan đến tính toán dư thừa, vì hai chuỗi token chia sẻ một tiền tố chung có cùng đầu ra attention cho tiền tố chung do mask nhân quả trong Phương trình 3. Ngoài ra, việc khởi chạy một kernel cho mỗi chuỗi token tạo ra chi phí khởi chạy kernel bổ sung.

SpecInfer giới thiệu hai kỹ thuật chính để thực hiện giải mã song song dựa trên cây.

Tìm kiếm theo chiều sâu để cập nhật key-value cache. Thay vì cache các key và value cho các chuỗi token riêng lẻ của một cây token, SpecInfer tái sử dụng cùng key-value cache trên tất cả các chuỗi token bằng cách tận dụng cơ chế tìm kiếm theo chiều sâu để duyệt cây token, như được hiển thị trong Hình 4, trong đó SpecInfer truy cập 𝑡2,𝑡3,...,𝑡9 bằng cách tuân theo thứ tự chiều sâu để duyệt cây token và cập nhật key-value cache chia sẻ. Cách tiếp cận này cho phép SpecInfer duy trì các key và value đúng cho tất cả các token đi trước khi tính toán đầu ra attention của một token mới.

Mask nhân quả nhận biết topology. Một cách tiếp cận đơn giản để tính toán tree attention là tính toán đầu ra tree attention cho các token riêng lẻ bằng cách tuân theo thứ tự chiều sâu được mô tả trước đó. Tuy nhiên, cách tiếp cận này sẽ dẫn đến chi phí khởi chạy kernel GPU cao vì mỗi kernel chỉ tính toán tree attention cho một chuỗi token. Ngoài ra, việc thực thi những kernel này song song đòi hỏi bộ nhớ GPU bổ sung để lưu trữ key-value cache của chúng riêng biệt do xung đột cache. Một thách thức chính ngăn cản SpecInfer gộp nhiều token là việc tính toán attention cho các token khác nhau đòi hỏi các key-value cache khác nhau và do đó không thể được xử lý song song.

Chúng tôi giới thiệu mask nhân quả nhận biết topology để hợp nhất tính toán tree attention của tất cả các token trong một kernel duy nhất. Để gộp tính toán attention, SpecInfer sử dụng topology cây thay vì topology chuỗi gốc để lưu trữ các key và value của tất cả các token trong một cây token trong key-value cache. Ví dụ, để tính toán tree attention cho cây token được suy đoán được hiển thị trong Hình 4, SpecInfer lấy cả token đã được xác minh (tức là, 𝑡2) và tất cả các token được suy đoán (tức là, 𝑡3,𝑡4,...,𝑡9) làm đầu vào. Cách tiếp cận này cho phép SpecInfer hợp nhất tính toán attention thành một kernel duy nhất nhưng cũng dẫn đến các điểm attention vi phạm phụ thuộc nhân quả (ví dụ, tính toán attention của 𝑡7 sử dụng tất cả các token trước đó, bao gồm 𝑡5 không nằm trong chuỗi token của 𝑡7). Để sửa các điểm attention cho những cặp này, SpecInfer cập nhật mask nhân quả dựa trên topology của cây token. Cách tiếp cận này tính toán chính xác cùng đầu ra attention như giải mã tăng dần, trong khi dẫn đến ít lần khởi chạy kernel hơn nhiều so với giải mã dựa trên chuỗi.

4.3 Xác minh Token
Đối với một cây token được suy đoán 𝒩 nhất định, SpecInfer sử dụng giải mã song song dựa trên cây (xem Phần 4.2) để tính toán tree attention của nó và tạo ra một tensor đầu ra 𝒪 bao gồm một token cho mỗi nút 𝑢∈𝒩. Tiếp theo, bộ xác minh cây token của SpecInfer kiểm tra tính đúng đắn của các token được suy đoán so với LLM. SpecInfer hỗ trợ cả lấy mẫu tham lam và ngẫu nhiên như được hiển thị trong Thuật toán 2.

Giải mã tham lam. Nhiều ứng dụng LLM sinh ra token bằng cách sử dụng giải mã tham lam, tham lam chọn token có khả năng cao nhất trong mỗi bước giải mã. Hàm VerifyGreedy trong Thuật toán 2 cho thấy cách SpecInfer xác minh một cây token được suy đoán 𝒩 với giải mã tham lam. SpecInfer bắt đầu từ gốc của 𝒩 và lặp lại kiểm tra kết quả suy đoán của một nút so với đầu ra gốc của LLM. Đối với một nút 𝑢∈𝒩, SpecInfer thành công suy đoán token tiếp theo của nó nếu 𝑢 bao gồm một nút con 𝑣 (tức là, 𝑝𝑣=𝑢) mà token của nó khớp với đầu ra của LLM (tức là, 𝑡𝑣=𝒪(𝑢)). Trong trường hợp này, SpecInfer hoàn thành xác minh của nó cho nút 𝑢 và chuyển sang kiểm tra con 𝑣 của nó. Khi nút 𝑢 không bao gồm một con chứa đầu ra của LLM, SpecInfer thêm 𝒪(𝑢) như một nút đã được xác minh trong 𝒩 và chấm dứt quá trình xác minh. Cuối cùng, tất cả các nút đã được xác minh được thêm vào chuỗi token được sinh ra hiện tại 𝒱. Xác minh cây token cho phép SpecInfer cơ hội giải mã nhiều token (thay vì một token duy nhất trong cách tiếp cận giải mã tăng dần), trong khi bảo toàn cùng hiệu suất sinh tạo như giải mã tăng dần.

Giải mã ngẫu nhiên. Để cải thiện sự đa dạng của các token được sinh ra, nhiều ứng dụng LLM thực hiện giải mã ngẫu nhiên, lấy mẫu một token từ một phân phối xác suất 𝑃(𝑢𝑖|𝑢0,...,𝑢𝑖−1;Θ𝐿𝐿𝑀), trong đó 𝑈=𝑢0,...,𝑢𝑖−1 là các token được sinh ra trước đó, 𝑢𝑖 là token tiếp theo để sinh ra, và Θ𝐿𝐿𝑀 đại diện cho một LLM được tham số hóa.

Để xác minh một cây token được suy đoán với giải mã ngẫu nhiên, chúng tôi giới thiệu một thuật toán lấy mẫu suy đoán đa bước (MSS) để tiến hành xác minh, mã giả của nó được hiển thị trong hàm VerifyStochastic trong Thuật toán 2 và được minh họa trong Hình 5. Phương pháp của chúng tôi có thể chứng minh bảo toàn hiệu suất sinh tạo của LLM như giải mã tăng dần trong khi tối ưu hóa số lượng token được suy đoán có thể được xác minh. Định lý 4.2 chứng minh tính đúng đắn của nó.

Định lý 4.2. Đối với một 𝐿𝐿𝑀 và 𝑚 SSM cho trước (tức là, 𝑆𝑆𝑀1,...,𝑆𝑆𝑀𝑚), đặt 𝑃(𝑢𝑖|𝑈;Θ𝐿𝐿𝑀) là phân phối xác suất của việc lấy mẫu một token bằng cách sử dụng giải mã ngẫu nhiên, trong đó 𝑈=𝑢0,...,𝑢𝑖−1 là các token được sinh ra trước đó, 𝑢𝑖 là token tiếp theo để sinh ra, Θ𝐿𝐿𝑀 đại diện cho LLM được tham số hóa.

Đặt 𝑃SpecInfer(𝑢𝑖|𝑈;Θ𝐿𝐿𝑀,{Θ𝑆𝑆𝑀𝑗}) là phân phối xác suất của việc lấy mẫu token 𝑢𝑖 bằng cách sử dụng lấy mẫu suy đoán đa bước của SpecInfer (xem hàm VerifyStochastic trong Thuật toán 2), trong đó Θ𝑆𝑆𝑀𝑗 là SSM được tham số hóa thứ 𝑗.

Thì ∀𝑈,𝑢𝑖,Θ𝐿𝐿𝑀,Θ𝑆𝑆𝑀𝑗 chúng ta có
𝑃(𝑢𝑖|𝑈;Θ𝐿𝐿𝑀)=𝑃SpecInfer(𝑢𝑖|𝑈;Θ𝐿𝐿𝑀,{Θ𝑆𝑆𝑀𝑗}) (6)

Một chứng minh của định lý này được trình bày trong [28].

Chúng tôi thừa nhận rằng một cách tiếp cận đơn giản hơn để bảo toàn phân phối xác suất của giải mã ngẫu nhiên là trực tiếp lấy mẫu token tiếp theo 𝑥∼𝑃(𝑢𝑖|𝑈;Θ𝐿𝐿𝑀) và kiểm tra xem 𝑥 có phải là một nút con của 𝑢𝑖−1 trong cây token được suy đoán hay không. Chúng tôi gọi cách tiếp cận này là lấy mẫu ngây thơ (NS) và cho thấy rằng lấy mẫu suy đoán đa bước của SpecInfer có xác suất từ chối thấp hơn đều đặn so với lấy mẫu ngây thơ.

Định lý 4.3. Đặt 𝑃reject|MSS,𝑈,ΘLLM,{ΘSSM𝑗} biểu thị xác suất từ chối suy đoán tuân theo lấy mẫu suy đoán đa bước với viết tắt 𝑃(reject|MSS), và 𝑃reject|NS,𝑈,ΘLLM,{ΘSSM𝑗} xác suất từ chối suy đoán tuân theo Lấy mẫu Ngây thơ (NS) với viết tắt 𝑃(reject|NS). Thì ∀𝑈,ΘLLM,{ΘSSM𝑗}, chúng ta có

𝑃(reject|MSS)≤𝑃(reject|NS)

Chúng tôi trình bày một chứng minh của Định lý 4.3 trong [28].

Lưu ý rằng công trình trước đây đã giới thiệu lấy mẫu suy đoán một bước cho suy luận suy đoán dựa trên chuỗi [5, 25]. Khác với những cách tiếp cận này, SpecInfer tận dụng cây token để cải thiện hiệu suất suy đoán, điều này đòi hỏi một thuật toán xác minh khác. Kết quả là, SpecInfer thực hiện xác minh đa bước (xem VerifyStochastic trong Thuật toán 2) trên tất cả các nhánh của một token để tối đa hóa tỷ lệ thành công trong khi bảo toàn tương đương như giải mã tăng dần. Thuật toán MSS được đề xuất không chỉ hoạt động cho phương pháp dựa trên hợp nhất với nhiều SSM, mà còn hỗ trợ phương pháp dựa trên mở rộng với một SSM và lấy mẫu top-𝑘.

5 Thiết kế và Triển khai Hệ thống
Phần này mô tả thiết kế và triển khai của hệ thống runtime phân tán của SpecInfer (§5.1 và §5.2), phân tích chi phí tính toán và bộ nhớ của suy đoán và xác minh (§5.3), và giới thiệu các ứng dụng LLM tiềm năng có thể hưởng lợi từ các kỹ thuật của SpecInfer (§5.4).

5.1 Thiết kế Runtime của SpecInfer
Hình 6 cho thấy quy trình làm việc cho một lần lặp của suy luận suy đoán và xác minh. Trình quản lý yêu cầu của SpecInfer nhận các yêu cầu phục vụ LLM và lên lịch những yêu cầu này để phục vụ bằng cách thích ứng chính sách lên lịch cấp độ lần lặp từ Orca [55]. Cụ thể, SpecInfer lặp lại chọn các yêu cầu từ một pool các yêu cầu đang chờ và thực hiện một lần lặp suy luận suy đoán và xác minh cây token cho các yêu cầu được chọn. Vì các SSM nhỏ và có thể vừa trong một GPU, SpecInfer phân phối đều các GPU trên các SSM và phục vụ những SSM này bằng cách sử dụng song song dữ liệu. Ví dụ, Hình 6 cho thấy cách SpecInfer phục vụ hai SSM và bốn yêu cầu (tức là, 𝑟1,𝑟2,𝑟3, và 𝑟4) trên bốn GPU. Các token được tạo ra bởi SSM được gửi trở lại trình quản lý yêu cầu, tạo ra một cây token được suy đoán cho mỗi yêu cầu bằng cách sử dụng thuật toán hợp nhất cây được giới thiệu trong §4.

SpecInfer phục vụ một LLM bằng cách sử dụng chiến lược song song hóa lai được giới thiệu trong Megatron-LM [41], sử dụng song song mô hình tensor để song song hóa mỗi lớp Transformer trên các GPU trong một nút, và sử dụng song song mô hình pipeline để phân chia các lớp Transformer trên các nút. Tất cả các GPU thực hiện giải mã song song dựa trên cây (xem §4.2) để tính toán các điểm tree attention và gửi các token được tạo ra bởi LLM trở lại trình quản lý yêu cầu, cuối cùng xác minh các token được suy đoán so với đầu ra của LLM (xem §4.3).

Lưu ý rằng chi phí được giới thiệu bởi trình quản lý yêu cầu (tức là, lên lịch yêu cầu, hợp nhất cây token, và xác minh) là không đáng kể so với thời gian thực thi của suy luận LLM. Ngoài ra, trình quản lý yêu cầu và các worker GPU của SpecInfer chỉ giao tiếp token và không truyền các biểu diễn vector của những token này, điều này một lần nữa tạo ra chi phí giao tiếp không đáng kể.

Batching liên tục. SpecInfer sử dụng batching liên tục được giới thiệu trong Orca [55] để phục vụ nhiều yêu cầu suy luận LLM song song. Cụ thể, SpecInfer lên lịch thực thi LLM ở mức độ chi tiết của các lần lặp thay vì các yêu cầu. Sau mỗi lần lặp giải mã LLM, SpecInfer kiểm tra trạng thái của mỗi yêu cầu và gửi kết quả được tạo ra của tất cả các yêu cầu đã hoàn thành đến client. Thiết kế này cũng cho phép SpecInfer bắt đầu xử lý các yêu cầu mới đến mà không cần chờ tất cả các yêu cầu hiện tại hoàn thành.

5.2 Triển khai của SpecInfer
SpecInfer được triển khai trên FlexFlow [21,47], một runtime đa GPU phân tán cho tính toán DNN. FlexFlow tiết lộ một API cho phép người dùng định nghĩa một mô hình DNN theo các lớp của nó. Nó tương thích với định nghĩa mô hình của PyTorch do sự phù hợp của các toán tử cơ bản. Ví dụ, các LLM mã nguồn mở từ HuggingFace [19] có thể được nhập trực tiếp vào SpecInfer để phục vụ mà không cần sửa đổi. Người dùng cũng có thể cung cấp một chiến lược song song hóa, chỉ định mức độ song song dữ liệu, mô hình, và pipeline cho mỗi lớp. Một DNN được biểu diễn như một đồ thị tính toán trong đó mỗi nút là một vùng bộ nhớ, và mỗi cạnh là một hoạt động trên một hoặc nhiều vùng. Các hoạt động có thể được biểu diễn bằng ba mức độ trừu tượng: lớp, toán tử, và tác vụ. Trình biên dịch FlexFlow biến đổi đồ thị tính toán từ mức trừu tượng cao nhất (tức là, lớp) đến thấp nhất (tức là, tác vụ). Các tác vụ cũng là đơn vị của song song hóa; chúng không thể bị ngắt, và được thực thi bất đồng bộ.

Tối ưu hóa kernel CUDA. Việc khởi chạy trực tiếp các kernel cuBLAS và cuDNN để tính toán attention dẫn đến chi phí khởi chạy kernel cao và không tận dụng bộ nhớ chia sẻ có sẵn trên các GPU hiện đại. Để giải quyết sự kém hiệu quả này, SpecInfer sử dụng một kernel tùy chỉnh được xây dựng trên FasterTransformer [32] để tính toán attention. Trong kernel này, mỗi thread block tính toán một đầu duy nhất cho một yêu cầu duy nhất. Quá trình bắt đầu với việc tải tensor query vào bộ nhớ chia sẻ GPU có thể truy cập bởi tất cả các thread trong một thread block. Sau đó mỗi thread thực hiện một đoạn của tích query/key và phát sóng kết quả đến các thread khác để tính toán tích query/key tối đa và tổng exponential. Để hỗ trợ giải mã song song dựa trên cây, SpecInfer tính toán tất cả các token trong một cây song song và tận dụng mask nhân quả nhận biết topology để bảo toàn tính nhân quả.

5.3 Chi phí của Suy đoán và Xác minh
SpecInfer tăng tốc suy luận LLM sinh tạo với chi phí của chi phí bộ nhớ và tính toán bổ sung. Phần này phân tích những chi phí này và cho thấy chúng thường nhỏ hơn một hoặc hai bậc độ lớn so với chi phí bộ nhớ và tính toán của việc thực thi suy luận LLM.

Chi phí bộ nhớ. Chi phí bộ nhớ của cách tiếp cận suy đoán-xác minh của SpecInfer đến từ hai khía cạnh. Đầu tiên, ngoài việc phục vụ một LLM, SpecInfer cũng cần phân bổ bộ nhớ để lưu các tham số của một hoặc nhiều SSM, cùng nhau suy đoán đầu ra của LLM. Đánh giá của chúng tôi cho thấy SpecInfer có thể đạt được cải thiện hiệu suất đáng kể bằng cách sử dụng các SSM nhỏ hơn 100-1000× so với LLM. Kết quả là, việc lưu trữ mỗi SSM làm tăng yêu cầu bộ nhớ tổng thể ít hơn 1%. Một nguồn chi phí bộ nhớ thứ hai đến từ engine xác minh cây token, xác minh toàn bộ cây token thay vì giải mã một token duy nhất. Do đó, bộ nhớ bổ sung cần thiết để cache các key và value, và lưu trữ các điểm attention cho tất cả các token. Do tính cần thiết để hỗ trợ độ dài chuỗi rất dài trong việc phục vụ LLM ngày nay, chúng tôi quan sát thấy rằng chi phí bộ nhớ liên quan đến cây token là không đáng kể so với key-value cache.

Chi phí tính toán. Tương tự, chi phí tính toán được giới thiệu bởi suy đoán và xác minh cũng đến từ hai khía cạnh. Đầu tiên, SpecInfer cần chạy các SSM trong chế độ giải mã tăng dần để tạo ra các token ứng viên. Khi nhiều SSM được sử dụng, SpecInfer xử lý những SSM này song song trên các GPU để tối thiểu hóa độ trễ suy đoán. Thứ hai, SpecInfer xác minh một cây token bằng cách tính toán các đầu ra attention cho toàn bộ cây token, hầu hết trong số đó không khớp với đầu ra của LLM và do đó là không cần thiết trong suy luận giải mã tăng dần. Tuy nhiên, cơ chế key-value cache của các hệ thống suy luận LLM hiện có ngăn cản chúng phục vụ một số lượng lớn yêu cầu song song, dẫn đến tài nguyên tính toán chưa được sử dụng đầy đủ trên các GPU khi phục vụ LLM trong giải mã tăng dần. Xác minh cây token của SpecInfer tận dụng những tài nguyên chưa được sử dụng đầy đủ này và do đó tạo ra chi phí runtime không đáng kể so với giải mã tăng dần.

5.4 Ứng dụng
Các kỹ thuật suy luận suy đoán và xác minh cây token của chúng tôi có thể được áp dụng trực tiếp cho nhiều ứng dụng LLM khác nhau. Chúng tôi xác định hai tình huống thực tế mà suy luận LLM có thể hưởng lợi đáng kể từ các kỹ thuật của chúng tôi.

Suy luận LLM phân tán. Yêu cầu bộ nhớ của các LLM hiện đại vượt quá khả năng của một nút tính toán duy nhất với một hoặc nhiều GPU, và cách tiếp cận hiện tại để giải quyết yêu cầu bộ nhớ cao là phân phối các tham số của LLM trên nhiều GPU [29]. Ví dụ, việc phục vụ một pipeline suy luận duy nhất cho GPT-3 với 175 tỷ tham số đòi hỏi hơn 16 GPU NVIDIA A100-40GB để lưu trữ các tham số mô hình ở dạng số thực độ chính xác đơn. Suy luận LLM phân tán phần lớn bị hạn chế bởi độ trễ để truyền các kích hoạt trung gian giữa các GPU cho mỗi bước giải mã LLM. Mặc dù cách tiếp cận của SpecInfer không trực tiếp giảm lượng giao tiếp giữa các GPU, cơ chế xác minh của nó có thể tăng độ chi tiết giao tiếp và giảm số bước giải mã.

Suy luận LLM dựa trên offloading. Một tình huống thực tế khác có thể hưởng lợi từ các kỹ thuật của SpecInfer là suy luận LLM dựa trên offloading, tận dụng CPU DRAM để lưu trữ các tham số của LLM và tải một tập hợp con của những tham số này vào GPU để tính toán theo cách pipeline [40]. Bằng cách cơ hội xác minh nhiều token, SpecInfer có thể giảm số bước giải mã LLM và giao tiếp tổng thể giữa CPU DRAM và GPU HBM.

6 Đánh giá
6.1 Thiết lập Thực nghiệm
LLM. Để so sánh hiệu suất runtime của SpecInfer với các hệ thống phục vụ LLM hiện có, chúng tôi đánh giá những hệ thống này bằng cách sử dụng hai họ LLM có sẵn công khai: OPT [57] và LLaMA [46]. Cụ thể hơn, chúng tôi chọn LLaMA-7B, OPT-13B, OPT-30B, và LLaMA-65B làm LLM, và LLaMA-68M và OPT-125M làm SSM. Các tham số mô hình được pre-train cho LLM và SSM được lấy từ các repository HuggingFace của chúng [19], và chúng tôi mô tả cách SpecInfer tập thể boost-tune nhiều SSM trong [28].

Bộ dữ liệu. Chúng tôi đánh giá SpecInfer trên năm bộ dữ liệu: Chatbot Instruction Prompts (CIP) [34], ChatGPT Prompts (CP) [30], WebQA [1], Alpaca [36,45], và PIQA [2]. Chúng tôi chỉ sử dụng các prompt/câu hỏi từ những bộ dữ liệu này để tạo thành các prompt đầu vào của chúng tôi để mô phỏng các trace cuộc trò chuyện thực tế.

Nền tảng. Các thí nghiệm được tiến hành trên hai instance AWS g5.12xlarge, mỗi instance được trang bị bốn GPU NVIDIA A10 24GB, 48 lõi CPU, và 192 GB DRAM. Các nút được kết nối bằng Ethernet 100 Gbps.

Các thí nghiệm của chúng tôi sử dụng phương pháp dựa trên mở rộng (xem Phần 3) để xây dựng cây token và sử dụng cấu hình mở rộng ⟨1,1,3,1,1,1,1,1⟩, cung cấp kết quả tốt cho các benchmark của chúng tôi. Chúng tôi phân tích tác động của các cấu hình mở rộng trong §6.4, đánh giá giải mã song song dựa trên cây và lấy mẫu suy đoán đa bước trong §6.5 và §6.6, và cuối cùng so sánh các phương pháp xây dựng cây dựa trên mở rộng và hợp nhất trong [28].

6.2 Suy luận LLM Phân tán
Chúng tôi so sánh hiệu suất suy luận LLM phân tán end-to-end giữa SpecInfer, vLLM [24], HuggingFace Text Generation Inference (TGI) [18], và FasterTransformer [32] trên LLaMA-7B, OPT-30B, và LLaMA-65B. Đối với LLaMA-7B và OPT-30B, tất cả các hệ thống phục vụ hai LLM ở dạng số thực độ chính xác nửa trên một và bốn GPU A10 bằng cách sử dụng song song mô hình tensor. LLaMA-65B không vừa trên bốn GPU trên một nút duy nhất, do đó cả FasterTransformer và SpecInfer đều phục vụ nó trên tám GPU A10 trên hai nút bằng cách kết hợp song song mô hình tensor trong mỗi nút và song song mô hình pipeline trên các nút. vLLM và HuggingFace TGI không hỗ trợ song song mô hình pipeline và không thể phục vụ một LLM trên nhiều nút.

Để loại bỏ các hiệu ứng tiềm năng của việc triển khai hệ thống của chúng tôi, chúng tôi cũng đánh giá SpecInfer với hai cấu hình bổ sung. Đầu tiên, SpecInfer với giải mã tăng dần đánh giá hiệu suất runtime của việc triển khai của chúng tôi khi bộ suy đoán tạo ra các cây token rỗng, và bộ xác minh xác minh chính xác một token trong mỗi bước giải mã. Thứ hai, SpecInfer với suy luận suy đoán dựa trên chuỗi phục vụ như một tham chiếu cho hệ thống suy luận suy đoán hiện có và được kích hoạt bằng cách sử dụng một SSM được pre-train duy nhất và giải mã dựa trên chuỗi.

Chúng tôi sử dụng các prompt từ năm bộ dữ liệu được mô tả trong §6.1. Đối với mỗi prompt, chúng tôi để tất cả các hệ thống tạo ra lên đến 128 token mới và báo cáo độ trễ trung bình mỗi token trong Hình 7. Lưu ý rằng SpecInfer có thể tạo ra nhiều hơn 128 token mới vì bộ xác minh có thể xác minh nhiều token trong mỗi lần lặp. Trong trường hợp này, chúng tôi cắt đầu ra của SpecInfer thành 128 token.

SpecInfer với giải mã tăng dần đạt được hiệu suất tương đương với các hệ thống hiện có. Điều này là do tất cả các hệ thống sử dụng cùng chiến lược để song song hóa suy luận LLM trên các GPU và sử dụng cùng thư viện kernel (tức là, cuDNN, cuBLAS, và cuTLASS) để thực thi tính toán suy luận trên GPU. Với suy luận suy đoán dựa trên cây và xác minh, SpecInfer vượt trội hơn các hệ thống giải mã tăng dần từ 1.5-2.5× cho suy luận đa GPU nút đơn và từ 2.4-2.8× cho suy luận đa GPU đa nút, trong khi tạo ra chính xác cùng chuỗi token như giải mã tăng dần cho tất cả các prompt. Sự tăng tốc đến từ việc tận dụng tài nguyên GPU dự phòng để thực hiện giải mã song song dựa trên cây trong khi duy trì cùng độ trễ mỗi lần lặp như giải mã tăng dần.

So với suy luận suy đoán dựa trên chuỗi, cách tiếp cận dựa trên cây của SpecInfer tiếp tục giảm độ trễ phục vụ LLM từ 1.2-1.5×. Sự cải thiện được đạt được bằng (1) tận dụng cây token để tối ưu hóa hiệu suất suy đoán, (2) sử dụng giải mã song song dựa trên cây để xác minh toàn bộ cây token song song, và (3) thực hiện lấy mẫu suy đoán đa bước để cải thiện hiệu suất xác minh. Chúng tôi đánh giá thêm những khía cạnh này trong §6.4, §6.5, và §6.6.

Lưu ý rằng sự cải thiện hiệu suất của SpecInfer so với các hệ thống hiện có giảm khi kích thước batch (tức là, số yêu cầu đồng thời) tăng. Điều này là do SpecInfer tận dụng tài nguyên GPU dự phòng để thực hiện giải mã song sang dựa trên cây trong khi duy trì cùng độ trễ mỗi lần lặp như giải mã tăng dần. Kích thước batch lớn hơn tạo ra nhiều tính toán có thể song song hóa hơn cho giải mã tăng dần, và do đó ít tài nguyên GPU dự phòng hơn có thể được tận dụng bởi SpecInfer. Mặt khác, kích thước batch lớn hơn cũng tăng độ trễ end-to-end của mỗi yêu cầu, như được hiển thị trong Hình 7. Nhìn chung, SpecInfer có lợi nhất cho suy luận LLM độ trễ thấp.

6.3 Suy luận LLM dựa trên Offloading
Một ứng dụng quan trọng khác của SpecInfer là suy luận LLM dựa trên offloading, trong đó hệ thống offload các tham số của LLM vào CPU DRAM và tải một tập hợp con của những tham số này vào GPU để tính toán suy luận theo cách pipeline. Chúng tôi so sánh hiệu suất suy luận LLM dựa trên offloading end-to-end giữa SpecInfer và FlexGen [39] bằng cách sử dụng một GPU A10 24GB duy nhất và hai LLM (tức là, OPT-13B và OPT-30B), cả hai đều vượt quá dung lượng bộ nhớ của GPU A10 và đòi hỏi offloading để phục vụ. Cả SpecInfer và FlexGen đều giữ tất cả tham số mô hình trên CPU DRAM. Trong quá trình tính toán, các trọng số cần thiết được tải từ CPU vào GPU. Hình 8 cho thấy kết quả. So với FlexGen, SpecInfer giảm độ trễ mỗi token từ 2.6-3.5×. Vì suy luận LLM dựa trên offloading chủ yếu bị nghẽn cổ chai bởi giao tiếp giữa CPU DRAM và GPU HBM để tải các tham số của LLM, sự cải thiện của SpecInfer so với các hệ thống hiện có được đạt được bằng cách cơ hội xác minh nhiều token, điều này lần lượt giảm số bước giải mã LLM và truyền dữ liệu giữa CPU và GPU.

6.4 Xây dựng Cây Token
Phần này đánh giá cơ chế xây dựng cây token dựa trên mở rộng. Chúng tôi đầu tiên nghiên cứu cách độ rộng cây token ảnh hưởng đến hiệu suất suy đoán của SpecInfer. Trong thí nghiệm này, chúng tôi sử dụng LLaMA-7B và LLaMA-68M làm LLM và SSM, và sử dụng cấu hình mở rộng ⟨1,1,𝑘,1,1,1,1,1⟩ (tức là, mở rộng ở token thứ ba), trong đó 𝑘 là độ rộng cây token. Hình 9 cho thấy hàm phân phối tích lũy (CDF) của số lượng token được xác minh trung bình mỗi bước giải mã cho tất cả các prompt trong bộ dữ liệu Alpaca [45]. So với suy đoán dựa trên chuỗi (tức là, độ rộng cây = 1), tận dụng cây token có thể giảm các bước giải mã LLM từ 1.2-1.5× cho giải mã tham lam và từ 1.3-1.4× cho giải mã ngẫu nhiên.

Độ rộng cây token lớn hơn giảm các bước giải mã LLM để xử lý một yêu cầu với chi phí của chi phí xác minh tăng lên, vì SpecInfer phải xác minh nhiều token hơn. Hình 10 so sánh độ trễ suy luận end-to-end của SpecInfer sử dụng các độ rộng cây khác nhau. Đối với kích thước batch nhỏ (tức là, BS = 1 và 2), sử dụng độ rộng cây lớn có thể liên tục giảm độ trễ mỗi token, vì SpecInfer có thể tận dụng tài nguyên GPU thưa thớt để xác minh nhiều token song song trong khi duy trì cùng độ trễ mỗi lần lặp. Đối với kích thước batch lớn (tức là, BS ≥4), sử dụng độ rộng cây lớn làm tăng độ trễ để xác minh một cây token do ít tài nguyên GPU thưa thớt hơn có thể được tận dụng bởi SpecInfer, và độ rộng cây 2 hoặc 3 đạt được hiệu suất tốt nhất bằng cách tạo ra sự cân bằng hoàn hảo giữa hiệu suất suy đoán và độ trễ xác minh.

6.5 Giải mã Song song Dựa trên Cây
Bây giờ chúng tôi đánh giá hiệu quả của cơ chế giải mã song song dựa trên cây của SpecInfer, giải mã tất cả các token của một cây token song song. Để so sánh, tất cả các hệ thống suy luận LLM hiện có sử dụng giải mã dựa trên chuỗi, đòi hỏi phân tách một cây token thành nhiều chuỗi token và xử lý những chuỗi này bằng cách sử dụng tài nguyên riêng biệt do các xung đột key-value cache tiềm năng (xem §4.2). Như được hiển thị trong Hình 11, giải mã song song dựa trên cây của SpecInfer đạt được hiệu suất tương đương với cơ chế giải mã dựa trên chuỗi hiện có cho kích thước batch nhỏ và vượt trội hơn lên đến 1.8× cho kích thước batch lớn. Sự cải thiện được thực hiện bằng (1) loại bỏ tính toán attention dư thừa cho các chuỗi với tiền tố chia sẻ, và (2) hợp nhất tree attention của tất cả các token trong một kernel duy nhất thông qua mask nhân quả nhận biết topology (xem §4.2).

6.6 Lấy mẫu Suy đoán Đa bước
Phần này đánh giá cách lấy mẫu suy đoán đa bước (MSS) của chúng tôi và thuật toán VerifyStochastic cải thiện hiệu suất suy đoán của SpecInfer khi thực hiện giải mã ngẫu nhiên. Chúng tôi sử dụng lấy mẫu ngây thơ như một baseline trong đó SpecInfer trực tiếp lấy mẫu token tiếp theo từ LLM và kiểm tra xem token được lấy mẫu có được bao gồm trong cây token được suy đoán hay không (xem §4.3). Vì các thuật toán lấy mẫu khác nhau liên quan đến chi phí suy đoán và xác minh giống hệt nhau, chúng tôi tập trung vào số lượng token trung bình có thể được xác minh trong mỗi bước giải mã ngẫu nhiên trong thí nghiệm này. Bảng 3 cho thấy kết quả. So với lấy mẫu ngây thơ, MSS có thể liên tục cải thiện số lượng token được xác minh từ 1.2-1.3× trung bình trên nhiều bộ dữ liệu prompt khác nhau, trong khi đảm bảo cùng phân phối đầu ra với LLM.

7 Công trình Liên quan
Các LLM dựa trên Transformer đã chứng minh tiềm năng đáng kể trong nhiều tác vụ mô hình hóa ngôn ngữ ở cấp độ con người bằng cách liên tục tăng kích thước của chúng [7,9,37,43,48]. Khi GPT-3 trở thành mô hình đầu tiên vượt quá 100B tham số [3], nhiều LLM (>100B) đã được phát hành, bao gồm OPT-175B [57], Bloom-176B [38], và PaLM [7]. Công trình gần đây đã đề xuất nhiều cách tiếp cận khác nhau để tăng tốc suy luận LLM sinh tạo, có thể được phân loại thành hai lớp.

Tăng tốc không mất mát. Công trình trước đây đã khám phá ý tưởng sử dụng một LLM như một bộ xác minh thay vì một bộ giải mã để tăng cường suy luận. Ví dụ, Yang et al. [53] giới thiệu suy luận với tham chiếu, tận dụng sự chồng lặp giữa đầu ra của LLM và các tham chiếu thu được bằng cách truy xuất tài liệu, và kiểm tra tính phù hợp của mỗi tham chiếu bằng cách kiểm tra kết quả giải mã của LLM. Được thúc đẩy bởi ý tưởng thực thi suy đoán trong tối ưu hóa bộ xử lý [4,16], công trình gần đây đề xuất giải mã suy đoán, sử dụng một mô hình ngôn ngữ nhỏ để tạo ra một chuỗi token và kiểm tra tính đúng đắn của những token này bằng cách sử dụng một LLM [5,22,25,44,51].

Có hai khác biệt chính giữa SpecInfer và những công trình trước đây này. Đầu tiên, thay vì chỉ xem xét một chuỗi token duy nhất, SpecInfer tạo ra và xác minh một cây token, mà các nút của nó mỗi nút đại diện cho một chuỗi token độc đáo. SpecInfer thực hiện tree attention để tính toán đầu ra attention của những chuỗi token này song song và sử dụng một thuật toán giải mã dựa trên cây mới để tái sử dụng kết quả trung gian được chia sẻ trên những chuỗi này. Thứ hai, những nỗ lực trước đây thường xem xét một mô hình ngôn ngữ nhỏ duy nhất để suy đoán, không thể phù hợp tốt với một LLM do khoảng cách năng lực mô hình giữa chúng. SpecInfer giới thiệu hai phương pháp suy đoán mới, bao gồm 1) mở rộng từ một SSM duy nhất và 2) hợp nhất từ nhiều SSM được fine-tune, và cây token được tạo ra làm tăng đáng kể độ bao phủ của đầu ra LLM.

Công trình trước đây cũng đã giới thiệu nhiều kỹ thuật khác nhau để tối ưu hóa tính toán ML trên các nền tảng phần cứng hiện đại. Ví dụ, TVM [6] và Ansor [58] tự động tạo ra kernel cho một chương trình tensor nhất định. TASO [20] và PET [50] tự động khám phá các biến đổi cấp độ đồ thị để tối ưu hóa đồ thị tính toán của một DNN. Các kỹ thuật của SpecInfer là trực giao và có thể được kết hợp với những hệ thống này để tăng tốc tính toán LLM sinh tạo, mà chúng tôi tin là một hướng nghiên cứu hứa hẹn cho công việc tương lai.

Tăng tốc có mất mát. BiLD [23] là một framework giải mã suy đoán sử dụng một SSM duy nhất để tăng tốc giải mã LLM. Không giống như các hệ thống được đề cập ở trên, việc tăng tốc có mất mát: tốc độ đến với chi phí của sự suy giảm có thể trong các token được tạo ra. Một hướng nghiên cứu khác tận dụng nén mô hình để giảm độ trễ suy luận LLM trong khi làm tổn hại hiệu suất dự đoán của LLM. Ví dụ, công trình trước đây đề xuất tận dụng lượng tử hóa trọng số/kích hoạt của LLM để giảm yêu cầu bộ nhớ và tính toán của việc phục vụ những LLM này [8,11,35,52,54]. Công trình gần đây tiếp tục khám phá nhiều kỹ thuật cắt tỉa có cấu trúc khác nhau để tăng tốc các kiến trúc dựa trên Transformer [10,17,49]. Một khác biệt chính giữa SpecInfer và những công trình trước đây này là SpecInfer không trực tiếp giảm yêu cầu tính toán để thực hiện suy luận LLM, mà thay vào đó tái tổ chức tính toán suy luận LLM theo cách có thể song song hóa hơn, giảm truy cập bộ nhớ và độ trễ suy luận với chi phí của chi phí bộ nhớ và tính toán có thể quản lý.

Tree-structured attention. Nguyen et al. [31] giới thiệu tree-structured attention, một kỹ thuật cho phép một mô hình Transformer nắm bắt thành phần phân cấp của văn bản đầu vào bằng cách chạy mô hình trên cây phân tích cú pháp của văn bản. Để xử lý với attention, nó sử dụng ánh xạ một-một để mã hóa và giải mã cây. Có hai khác biệt chính từ giải mã dựa trên cây của SpecInfer. Đầu tiên, SpecInfer sử dụng một cây để kết hợp các chuỗi ứng viên để nén tiền tố, trong khi Nguyen et al. biểu diễn một chuỗi duy nhất với cây phân tích cú pháp của nó. SpecInfer không kết hợp cây phân tích cú pháp vào LLM, mà tăng tốc suy luận bằng cách xác minh các chuỗi được giải mã song song. Thứ hai, attention của SpecInfer xuất ra một chuỗi token, không phải một cây.

Kỹ thuật giải mã đa mẫu. Giống như suy luận suy đoán dựa trên cây, beam search, lấy mẫu top-k, và lấy mẫu top-p xem xét nhiều chuỗi token ứng viên ở mỗi bước và có thể cắt tỉa các lựa chọn xác suất thấp. Tuy nhiên, giải mã dựa trên cây trong SpecInfer suy đoán dự đoán và xác minh nhiều ứng viên song song so với một LLM để giảm các lần lặp giải mã và độ trễ, tận dụng các mô hình suy đoán nhỏ (SSM). Ngược lại, beam search và lấy mẫu top-k/top-p là các chiến lược giải mã được áp dụng trực tiếp cho các xác suất đầu ra của LLM để tạo ra các chuỗi xác suất cao mà không giảm các bước giải mã. SpecInfer hỗ trợ beam search, lấy mẫu top-k, và lấy mẫu top-p. Những kỹ thuật này là các tối ưu hóa giải mã trực giao và có thể được kết hợp với giải mã suy đoán dựa trên cây.

8 Kết luận
Bài báo này giới thiệu SpecInfer, một hệ thống tăng tốc suy luận LLM sinh tạo với suy luận suy đoán dựa trên cây và xác minh. Một insight chính đằng sau SpecInfer là đồng thời xem xét sự đa dạng của các ứng viên suy đoán để dự đoán hiệu quả đầu ra của LLM, được tổ chức như một cây token và được xác minh so với LLM song song bằng cách sử dụng cơ chế giải mã song song dựa trên cây. SpecInfer giảm đáng kể việc truy cập bộ nhớ đến các tham số của LLM và độ trễ suy luận LLM end-to-end cho cả suy luận LLM phân tán và dựa trên offloading.

Lời cảm ơn
Chúng tôi cảm ơn Tianqi Chen, Bohan Hou, Hongyi Jin, các reviewer ASPLOS ẩn danh, và shepherd Shan Lu của chúng tôi vì phản hồi của họ về công việc này. Nghiên cứu này được hỗ trợ một phần bởi các giải thưởng NSF CNS-2147909, CNS-2211882, và CNS-2239351, và các giải thưởng nghiên cứu từ Amazon, Cisco, Google, Meta, Oracle, Qualcomm, và Samsung.

A Phụ lục Artifact
A.1 Tóm tắt
Artifact chứa mã để chạy SpecInfer, cũng như các bộ dữ liệu và script có thể được sử dụng để tái tạo các thí nghiệm trong bài báo.

A.2 Danh sách kiểm tra artifact (meta-information)
• Thuật toán: Tree-based Speculative Inference
• Chương trình: spec_infer.cc, incr_decoding.cc
• Biên dịch: CMake
• Môi trường run-time: CUDA, NCCL, MPI, UCX, Python3.
• Phần cứng: Hai instance AWS g5.12xlarge, mỗi instance có 4 GPU NVIDIA A10 24GB, 48 lõi CPU, và 192 GB DRAM.
• Metrics: Độ trễ trung bình End-to-end
• Đầu ra: Độ trễ end-to-end
• Thí nghiệm: Suy luận GPU cấp server, Suy luận dựa trên offloading
• Cần bao nhiêu dung lượng đĩa (xấp xỉ)?: 350GB mỗi nút
• Cần bao nhiêu thời gian để chuẩn bị workflow (xấp xỉ)?: 2h
• Cần bao nhiêu thời gian để hoàn thành các thí nghiệm (xấp xỉ)?: 6h
• Có sẵn công khai?: Có
• Giấy phép mã (nếu có sẵn công khai)?: Apache License v2.0
• Giấy phép dữ liệu (nếu có sẵn công khai)?: LLAMA nằm dưới giấy phép GNU và OPT nằm dưới giấy phép Non-commercial
• Được lưu trữ (cung cấp DOI)?: https://doi.org/10.5281/zenodo.10854410 .

A.3 Mô tả
A.3.1 Cách truy cập. Artifact được phát hành trên Github: https://github.com/goliaro/specinfer-ae . Repository chứa mã nguồn của SpecInfer, và hướng dẫn để xây dựng framework. Chúng tôi cũng cung cấp script để tái tạo các thí nghiệm từ bài báo. Để clone repository, sử dụng lệnh sau (đảm bảo truyền cờ –recursive):

git clone --recursive \
https://github.com/goliaro/specinfer-ae.git

A.3.2 Phụ thuộc phần cứng. Chúng tôi chạy các thí nghiệm trên hai instance AWS g5.12xlarge, mỗi instance có 4 GPU NVIDIA A10 24GB, 48 lõi CPU, và 192 GB DRAM. Chúng tôi cung cấp hướng dẫn để tạo và thiết lập các instance.

A.3.3 Phụ thuộc phần mềm. Phần mềm sau là bắt buộc: CUDA 12.1, NCCL, Rust, CMake và Python3. Hơn nữa, UCX và MPI được yêu cầu cho các thí nghiệm đa nút. Các phụ thuộc Python bổ sung được liệt kê ở đây: https://github.com/flexflow/FlexFlow/blob/inference/requirements.txt. Chúng tôi khuyến nghị sử dụng Deep Learning OSS Nvidia Driver AMI GPU PyTorch 2.1.0 (Ubuntu 20.04) AMI, và cung cấp script và môi trường conda để cài đặt tất cả các phụ thuộc còn lại.

A.3.4 Mô hình. Chúng tôi sử dụng các mô hình LLM/SSM sau cho các thí nghiệm của chúng tôi (đối với mỗi mô hình, chúng tôi chỉ định trong ngoặc đơn repository HuggingFace tương ứng): LLaMA-68M (JackFram/llama-68m), LLaMA-7B (huggyllama/llama-7b), LLaMA-65B (huggyllama/llama-65b), OPT-125M (facebook/opt-125m), OPT-13B (facebook/opt-13b), OPT-125M (facebook/opt-30b). Bạn có thể tải xuống tất cả các mô hình này với script:
./download_models.sh

A.4 Cài đặt
Để tái tạo các thí nghiệm, bạn sẽ cần truy cập vào hai instance AWS g5.12xlarge (hoặc các máy khác có cùng thông số kỹ thuật GPU/CPU/mạng). Nếu bạn đang sử dụng các instance được cấu hình trước mà chúng tôi cung cấp, bạn có thể bỏ qua bước này.

Khởi chạy các instance. Khởi chạy hai instance AWS g5.12xlarge sử dụng Deep Learning OSS Nvidia Driver AMI GPU PyTorch 2.1.0 (Ubuntu 20.04) AMI. Đảm bảo đặt các instance trong một placement group sử dụng chiến lược cluster để đạt được hiệu suất mạng độ trễ thấp. Gắn cùng security group vào tất cả các instance và thêm một quy tắc inbound trong security group để cho phép tất cả lưu lượng đến từ cùng security group. Ví dụ, bạn có thể thêm quy tắc sau: Type: All TCP, Source: Anywhere-IPv4.

Cài đặt các điều kiện tiên quyết. Sau khi truy cập vào các instance AWS, cài đặt các điều kiện tiên quyết bằng cách làm theo các bước dưới đây. Đầu tiên, kích hoạt hỗ trợ conda shell bằng cách chạy conda init bash, và sau đó khởi động lại phiên shell.

Tiếp theo, tạo môi trường conda với tất cả các phụ thuộc bắt buộc bằng cách chạy:
conda env create -f FlexFlow/conda/flexflow.yml
conda activate flexflow

Thiết lập đa nút. Tải xuống và xây dựng UCX bằng cách chạy script install_ucx.sh. Tiếp theo, nếu bạn đang chạy SpecInfer trên hai instance AWS, bạn sẽ cần cấu hình MPI để hai instance có thể truy cập lẫn nhau. Chọn một nút chính, và tạo một cặp khóa SSH với:
ssh-keygen -t ed25519

Nối nội dung của khóa công khai (~/.ssh/id_ed25519.pub) vào tệp ~/.ssh/authorized_keys trên CẢ máy chính và máy phụ. Lưu ý rằng nếu thư mục .ssh hoặc tệp authorized_keys không tồn tại, bạn sẽ cần tạo chúng thủ công. Cuối cùng, tạo một tệp tại đường dẫn ~/hostfile với nội dung sau:
<main_node_private_ip> slots=4
<secondary_node_private_ip> slots=4

thay thế <main_node_private_ip> và <secondary_node_private_ip> bằng địa chỉ IP riêng tư của hai máy, và số slots với số GPU có sẵn (nếu bạn đang sử dụng các instance AWS được khuyến nghị, bạn sẽ sử dụng giá trị 4). Bạn có thể tìm địa chỉ IP riêng tư của mỗi máy bằng cách chạy lệnh (và sử dụng giá trị IP đầu tiên được in):
hostname -I

Cài đặt SpecInfer. Để cài đặt SpecInfer, chạy script:
./install_specinfer.sh

A.5 Kiểm tra Cơ bản
Để đảm bảo rằng SpecInfer được cài đặt đúng và hoạt động, chạy script basic_test.sh. Script này sẽ kiểm tra các chức năng giải mã tăng dần cơ bản và suy luận suy đoán, trên cả nút đơn và đa nút. Nó cũng sẽ kiểm tra hỗ trợ cho offloading. Kiểm tra thành công nếu nó in thông báo "Test passed!".

A.6 Quy trình thí nghiệm
Artifact đi kèm với các script để thu thập dữ liệu có thể được sử dụng để tái tạo kết quả từ bài báo. Nó cũng đi kèm với các script có thể được sử dụng để chuyển đổi dữ liệu đầu ra thành định dạng CSV để vẽ biểu đồ.

Chạy Thí nghiệm. Chúng tôi chạy hai thí nghiệm sau để đánh giá SpecInfer dưới các thiết lập phần cứng khác nhau. Dữ liệu đầu ra sẽ được lưu vào đường dẫn FlexFlow/inference/output.

• Đánh giá GPU cấp server. Thí nghiệm này kiểm tra hiệu suất của SpecInfer trên GPU cấp server. Các LLM và SSM được tải trong bộ nhớ GPU, và chúng tôi đo độ trễ suy luận end-to-end sử dụng 1 nút, và 2 nút. Trong trường hợp nút đơn, chúng tôi đo hiệu suất sử dụng 1 GPU, hoặc 4 GPU. Trong trường hợp đa nút, chúng tôi sử dụng 4 GPU mỗi nút. Các thí nghiệm sử dụng LLAMA-7B, OPT-30B và LLAMA-65B làm LLM, và LLAMA-68M và OPT-125M làm SSM. Thí nghiệm chạy SpecInfer trong ba chế độ khác nhau: giải mã tăng dần, giải mã suy đoán dựa trên chuỗi, và giải mã suy đoán dựa trên cây. Hai chế độ đầu được sử dụng để thu được dữ liệu cho nghiên cứu ablation, và chế độ sau là chế độ suy luận mới được đề xuất bởi SpecInfer, và sẽ được triển khai bởi người dùng. Để chạy đánh giá GPU cấp server, chạy:
./server_gpu_experiments.sh

• Đánh giá offloading. Thí nghiệm này kiểm tra hiệu suất của SpecInfer khi chỉ tải một tập hợp con tham số trong bộ nhớ GPU, trong khi offload những tham số còn lại trên CPU DRAM. Kỹ thuật này được sử dụng để thực hiện suy luận khi mô hình đích lớn hơn bộ nhớ GPU có sẵn. Trong thí nghiệm, SpecInfer sử dụng một GPU duy nhất và hoán đổi trọng số của mô hình đến và từ CPU. Để chạy đánh giá offloading, chạy:
./offloading_experiments.sh

Framework bên thứ ba. Vui lòng làm theo tài liệu chính thức của vLLM, FasterTransformer, và HuggingFace TGI, và FlexGen để tái tạo hiệu suất của các framework bên thứ ba dưới các tình huống thí nghiệm.

Dữ liệu đầu ra. Các script ở trên sẽ tạo ra dữ liệu tại đường dẫn FlexFlow/inference/output. Đối với mỗi tình huống, một tệp .txt chứa đầu ra được tạo ra cho mỗi prompt, và một tệp .out chứa các log stdout. Chất lượng của đầu ra được tạo ra có thể được đánh giá trực quan và so sánh với đầu ra từ các framework suy luận bên thứ ba. Chúng tôi cung cấp script để phân tích dữ liệu đầu ra thô và tạo ra các tệp CSV có thể được sử dụng để tạo ra các hình trong bài báo. README cung cấp tất cả chi tiết về các script và ánh xạ giữa các tệp CSV và hình.

A.7 Đánh giá và kết quả mong đợi
Dữ liệu từ các tệp CSV nên cho thấy hiệu suất tương tự với các hình từ bài báo. Một số biến động là mong đợi, nhưng nhìn chung, SpecInfer nên hoạt động theo Hình 7-11 từ bài báo.

A.8 Tùy chỉnh thí nghiệm
Người dùng có thể chỉnh sửa các tham số cấu hình từ các script đánh giá để thay đổi các tham số khác nhau, chẳng hạn như số lượng GPU/CPU, bộ nhớ GPU/CPU, kích thước batch, mô hình LLM/SSM được sử dụng, bộ dữ liệu prompt, độ chính xác đầy đủ vs nửa, và số lượng token tối đa để tạo ra.

Tài liệu tham khảo
[1] Jonathan Berant, Andrew Chou, Roy Frostig, và Percy Liang. Phân tích cú pháp ngữ nghĩa trên Freebase từ các cặp câu hỏi-trả lời. Trong Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, trang 1533–1544, Seattle, Washington, USA, tháng 10 năm 2013. Association for Computational Linguistics.

[2] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, và Yejin Choi. Piqa: Suy luận về common sense vật lý trong ngôn ngữ tự nhiên. Trong Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020.

[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. Các mô hình ngôn ngữ là những người học few-shot, 2020.

[4] F Warren Burton. Tính toán suy đoán, song song, và lập trình hàm. IEEE Transactions on Computers, 100(12):1190–1193, 1985.

[5] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, và John Jumper. Tăng tốc giải mã mô hình ngôn ngữ lớn với lấy mẫu suy đoán. arXiv preprint arXiv:2302.01318, 2023.

[6] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, và Arvind Krishnamurthy. TVM: Một trình biên dịch tối ưu hóa tự động end-to-end cho deep learning. Trong 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), trang 578–594, 2018.

[7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Mở rộng mô hình ngôn ngữ với pathways. arXiv preprint arXiv:2204.02311, 2022.

[8] Tim Dettmers, Mike Lewis, Younes Belkada, và Luke Zettlemoyer. Gpt3.int8(): nhân ma trận 8-bit cho transformer ở quy mô lớn. Advances in Neural Information Processing Systems, 35:30318–30332, 2022.

[9] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Mở rộng hiệu quả của các mô hình ngôn ngữ với mixture-of-experts. Trong International Conference on Machine Learning, trang 5547–5569. PMLR, 2022.

[10] Elias Frantar và Dan Alistarh. Sparsegpt: Các mô hình ngôn ngữ lớn có thể được cắt tỉa chính xác trong một lần, 2023.

[11] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, và Dan Alistarh. Gptq: Lượng tử hóa chính xác cho transformer sinh tạo được pre-train. Trong International Conference on Learning Representations, 2023.

[12] Yoav Freund, Robert Schapire, và Naoki Abe. Một giới thiệu ngắn gọn về boosting. Journal-Japanese Society For Artificial Intelligence, 14(771-780):1612, 1999.

[13] Freddy Gabbay và Avi Mendelson. Thực thi suy đoán dựa trên dự đoán giá trị. Citeseer, 1996.

[14] Mudasir A Ganaie, Minghui Hu, AK Malik, M Tanveer, và PN Suganthan. Ensemble deep learning: Một đánh giá. Engineering Applications of Artificial Intelligence, 115:105151, 2022.

[15] Aaron Gokaslan*, Vanya Cohen*, Ellie Pavlick, và Stefanie Tellex. Openwebtext corpus. http://Skylion007.github.io/OpenWebTextCorpus, 2019.

[16] John L Hennessy và David A Patterson. Kiến trúc máy tính: một cách tiếp cận định lượng. Elsevier, 2011.

[17] Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, Joseph Naor, và Daniel Soudry. Huấn luyện mạng neural thưa thớt được tăng tốc: Một phương pháp có thể chứng minh và hiệu quả để tìm mask chuyển vị n:m. Advances in Neural Information Processing Systems, 34:21099–21111, 2021.

[18] HuggingFace. Large language model text generation inference. https://github.com/huggingface/text-generation-inference. (Truy cập ngày 08/09/2023).

[19] Hugging Face Inc. Hugging face. https://huggingface.co, 2023.

[20] Zhihao Jia, Oded Padon, James Thomas, Todd Warszawski, Matei Zaharia, và Alex Aiken. Taso: tối ưu hóa tính toán deep learning với tạo ra tự động các thay thế đồ thị. Trong Proceedings of the 27th ACM Symposium on Operating Systems Principles, trang 47–62, 2019.

[21] Zhihao Jia, Matei Zaharia, và Alex Aiken. Vượt ra ngoài song song dữ liệu và mô hình cho mạng neural sâu. Trong Proceedings of the 2nd Conference on Systems and Machine Learning, SysML'19, 2019.

[22] Joao Gante. Assisted generation: một hướng mới hướng tới tạo văn bản độ trễ thấp, 2023.

[23] Sehoon Kim, Karttikeya Mangalam, Suhong Moon, John Canny, Jitendra Malik, Michael W. Mahoney, Amir Gholami, và Kurt Keutzer. Big little transformer decoder, 2023.

[24] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Yu, Joseph E Gonzalez, Hao Zhang, và Ion Stoica. vllm: Phục vụ llm dễ dàng, nhanh chóng và rẻ tiền với pagedattention. Xem https://vllm.ai/ (truy cập 9 tháng 8 năm 2023), 2023.

[25] Yaniv Leviathan, Matan Kalman, và Yossi Matias. Suy luận nhanh từ transformer thông qua giải mã suy đoán. arXiv preprint arXiv:2211.17192, 2022.

[26] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, và Weizhu Chen. Điều gì làm cho các ví dụ trong ngữ cảnh tốt cho gpt-3? arXiv preprint arXiv:2101.06804, 2021.

[27] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Hongyi Jin, Tianqi Chen, và Zhihao Jia. Hướng tới phục vụ mô hình ngôn ngữ lớn sinh tạo hiệu quả: Một khảo sát từ thuật toán đến hệ thống. arXiv preprint arXiv:2312.15234, 2023.

[28] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, và Zhihao Jia. Specinfer: Tăng tốc phục vụ llm sinh tạo với suy luận suy đoán và xác minh cây token. arXiv preprint arXiv:2305.09781, 2023.

[29] Xupeng Miao, Chunan Shi, Jiangfei Duan, Xiaoli Xi, Dahua Lin, Bin Cui, và Zhihao Jia. Spotserve: Phục vụ mô hình ngôn ngữ lớn sinh tạo trên các instance có thể bị ngắt. arXiv preprint arXiv:2311.15566, 2023.

[30] MohamedRashad. Chatgpt-prompts. https://huggingface.co/datasets/MohamedRashad/ChatGPT-prompts, 2023.

[31] Xuan-Phi Nguyen, Shafiq Joty, Steven Hoi, và Richard Socher. Tree-structured attention với tích lũy phân cấp. Trong International Conference on Learning Representations, 2020.

[32] NVIDIA. Fastertransformer. https://github.com/NVIDIA/FasterTransformer. (Truy cập ngày 08/09/2023).

[33] OpenAI. Báo cáo kỹ thuật gpt-4, 2023.

[34] Alessandro Palla. chatbot instruction prompts. https://huggingface.co/datasets/alespalla/chatbot_instruction_prompts, 2023.

[35] Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, và Dongsoo Lee. nuqmm: Nhân ma trận lượng tử hóa cho suy luận hiệu quả của mô hình ngôn ngữ sinh tạo quy mô lớn. arXiv preprint arXiv:2206.09557, 2022.

[36] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, và Jianfeng Gao. Instruction tuning với gpt-4. arXiv preprint arXiv:2304.03277, 2023.

[37] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Mở rộng mô hình ngôn ngữ: Phương pháp, phân tích & insight từ huấn luyện gopher. arXiv preprint arXiv:2112.11446, 2021.

[38] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: Một mô hình ngôn ngữ đa ngôn ngữ truy cập mở 176b-parameter. arXiv preprint arXiv:2211.05100, 2022.

[39] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gonzalez, Percy Liang, Christopher Ré, Ion Stoica, và Ce Zhang. Flexgen: Suy luận sinh tạo thông lượng cao của mô hình ngôn ngữ lớn với một gpu duy nhất, 2023.

[40] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gonzalez, Percy Liang, Christopher Ré, Ion Stoica, và Ce Zhang. Suy luận sinh tạo thông lượng cao của mô hình ngôn ngữ lớn với một gpu duy nhất, 2023.

[41] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, và Bryan Catanzaro. Megatron-lm: Huấn luyện mô hình ngôn ngữ đa tỷ tham số bằng cách sử dụng song song mô hình, 2020.

[42] James E Smith. Một nghiên cứu về các chiến lược dự đoán nhánh. Trong 25 years of the international symposia on Computer architecture (selected papers), trang 202–215, 1998.

[43] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Sử dụng deepspeed và megatron để huấn luyện megatron-turing nlg 530b, một mô hình ngôn ngữ sinh tạo quy mô lớn. arXiv preprint arXiv:2201.11990, 2022.

[44] Mitchell Stern, Noam Shazeer, và Jakob Uszkoreit. Giải mã song song theo khối cho mô hình tự hồi quy sâu. Advances in Neural Information Processing Systems, 31, 2018.

[45] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto. Stanford alpaca: Một mô hình llama tuân theo hướng dẫn. https://github.com/tatsu-lab/stanford_alpaca, 2023.

[46] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Mô hình ngôn ngữ nền tảng mở và hiệu quả. arXiv preprint arXiv:2302.13971, 2023.

[47] Colin Unger, Zhihao Jia, Wei Wu, Sina Lin, Mandeep Baines, Carlos Efrain Quintero Narvaez, Vinay Ramakrishnaiah, Nirmal Prajapati, Pat McCormick, Jamaludin Mohd-Yusof, Xi Luo, Dheevatsa Mudigere, Jongsoo Park, Misha Smelyanskiy, và Alex Aiken. Unity: Tăng tốc huấn luyện DNN thông qua tối ưu hóa kết hợp các biến đổi đại số và song song hóa. Trong 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), trang 267–284, Carlsbad, CA, tháng 7 năm 2022. USENIX Association.

[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, và Illia Polosukhin. Attention is all you need, 2017.

[49] Hanrui Wang, Zhekai Zhang, và Song Han. Spatten: Kiến trúc attention thưa thớt hiệu quả với cascade token và head pruning. Trong 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA), trang 97–110. IEEE, 2021.

[50] Haojie Wang, Jidong Zhai, Mingyu Gao, Zixuan Ma, Shizhi Tang, Liyan Zheng, Yuanzhi Li, Kaiyuan Rong, Yuanyong Chen, và Zhihao Jia. PET: Tối ưu hóa chương trình tensor với các biến đổi tương đương một phần và sửa chữa tự động. Trong 15th USENIX Symposium on Operating Systems Design and Implementation (OSDI 21), trang 37–54. USENIX Association, tháng 7 năm 2021.

[51] Heming Xia, Tao Ge, Si-Qing Chen, Furu Wei, và Zhifang Sui. Giải mã suy đoán: Tăng tốc không mất mát của dịch tự hồi quy.

[52] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, và Song Han. Smoothquant: Lượng tử hóa sau huấn luyện chính xác và hiệu quả cho mô hình ngôn ngữ lớn. arXiv preprint arXiv:2211.10438, 2022.

[53] Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, và Furu Wei. Suy luận với tham chiếu: Tăng tốc không mất mát của mô hình ngôn ngữ lớn. arXiv preprint arXiv:2304.04487, 2023.

[54] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, và Yuxiong He. Zeroquant: Lượng tử hóa sau huấn luyện hiệu quả và giá cả phải chăng cho transformer quy mô lớn. Advances in Neural Information Processing Systems, 35:27168–27183, 2022.

[55] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, và Byung-Gon Chun. Orca: Một hệ thống phục vụ phân tán cho mô hình sinh tạo dựa trên Transformer. Trong 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), trang 521–538, Carlsbad, CA, tháng 7 năm 2022. USENIX Association.

[56] Haoyu Zhang, Jianjun Xu, và Ji Wang. Tạo ra ngôn ngữ tự nhiên dựa trên pretraining cho tóm tắt văn bản. arXiv preprint arXiv:1902.09243, 2019.

[57] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Mô hình ngôn ngữ transformer được pre-train mở. arXiv preprint arXiv:2205.01068, 2022.

[58] Lianmin Zheng, Chengfan Jia, Minmin Sun, Zhao Wu, Cody Hao Yu, Ameer Haj-Ali, Yida Wang, Jun Yang, Danyang Zhuo, Koushik Sen, et al. Ansor: Tạo ra chương trình tensor hiệu suất cao cho deep learning. Trong 14th{USENIX}Symposium on Operating Systems Design and Implementation ({OSDI}20), trang 863–879, 2020.
