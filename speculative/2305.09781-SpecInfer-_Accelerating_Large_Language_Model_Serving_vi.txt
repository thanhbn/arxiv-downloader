SpecInfer: TÄƒng tá»‘c Phá»¥c vá»¥ MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n
vá»›i Suy luáº­n Suy Ä‘oÃ¡n Dá»±a trÃªn CÃ¢y vÃ  XÃ¡c minh

Xupeng Miaoâˆ—
Carnegie Mellon University
Pittsburgh, PA, USA
xupeng@cmu.edu

Gabriele Oliaroâˆ—
Carnegie Mellon University
Pittsburgh, PA, USA
goliaro@cs.cmu.edu

Zhihao Zhangâˆ—
Carnegie Mellon University
Pittsburgh, PA, USA
zhihaoz3@andrew.cmu.edu

Xinhao Chengâˆ—
Carnegie Mellon University
Pittsburgh, PA, USA
xinhaoc@andrew.cmu.edu

Zeyu Wang
Carnegie Mellon University
Pittsburgh, PA, USA
zeyuwang@alumni.cmu.edu

Zhengxin Zhang
Tsinghua University
Beijing, China
zhang-zx21@mails.tsinghua.edu.cn

Rae Ying Yee Wong
Stanford University
Stanford, CA, USA
raewong@stanford.edu

Alan Zhu
Carnegie Mellon University
Pittsburgh, PA, USA
aczhu@andrew.cmu.edu

Lijie Yang
Carnegie Mellon University
Pittsburgh, PA, USA
lijiey@andrew.cmu.edu

Xiaoxiang Shi
Shanghai Jiao Tong University
Shanghai, China
lambda7shi@sjtu.edu.cn

Chunan Shi
Peking University
Beijing, China
spirited_away@pku.edu.cn

Zhuoming Chen
Carnegie Mellon University
Pittsburgh, PA, USA
zhuominc@andrew.cmu.edu

Daiyaan Arfeen
Carnegie Mellon University
Pittsburgh, PA, USA
marfeen@andrew.cmu.edu

Reyna Abhyankar
University of California, San Diego
San Diego, CA, USA
vabhyank@ucsd.edu

Zhihao Jia
Carnegie Mellon University
Pittsburgh, PA, USA
zhihao@cmu.edu

TÃ³m táº¯t
BÃ i bÃ¡o nÃ y giá»›i thiá»‡u SpecInfer, má»™t há»‡ thá»‘ng tÄƒng tá»‘c phá»¥c vá»¥ mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n sinh táº¡o (LLM) vá»›i suy luáº­n suy Ä‘oÃ¡n dá»±a trÃªn cÃ¢y vÃ  xÃ¡c minh. Ã tÆ°á»Ÿng chÃ­nh Ä‘áº±ng sau SpecInfer lÃ  táº­n dá»¥ng cÃ¡c mÃ´ hÃ¬nh suy Ä‘oÃ¡n nhá» Ä‘á»ƒ dá»± Ä‘oÃ¡n Ä‘áº§u ra cá»§a LLM; cÃ¡c dá»± Ä‘oÃ¡n Ä‘Æ°á»£c tá»• chá»©c dÆ°á»›i dáº¡ng cÃ¢y token, mÃ  cÃ¡c nÃºt cá»§a nÃ³ má»—i nÃºt Ä‘áº¡i diá»‡n cho má»™t chuá»—i token á»©ng viÃªn. TÃ­nh Ä‘Ãºng Ä‘áº¯n cá»§a táº¥t cáº£ cÃ¡c chuá»—i token á»©ng viÃªn Ä‘Æ°á»£c biá»ƒu diá»…n bá»Ÿi má»™t cÃ¢y token Ä‘Æ°á»£c xÃ¡c minh so vá»›i LLM song song báº±ng má»™t cÆ¡ cháº¿ giáº£i mÃ£ song song dá»±a trÃªn cÃ¢y má»›i. SpecInfer sá»­ dá»¥ng má»™t LLM nhÆ° má»™t bá»™ xÃ¡c minh cÃ¢y token thay vÃ¬ má»™t bá»™ giáº£i mÃ£ tÄƒng dáº§n, Ä‘iá»u nÃ y giáº£m Ä‘Ã¡ng ká»ƒ Ä‘á»™ trá»… end-to-end vÃ  yÃªu cáº§u tÃ­nh toÃ¡n Ä‘á»ƒ phá»¥c vá»¥ cÃ¡c LLM sinh táº¡o trong khi cÃ³ thá»ƒ chá»©ng minh báº£o toÃ n cháº¥t lÆ°á»£ng mÃ´ hÃ¬nh. ÄÃ¡nh giÃ¡ cá»§a chÃºng tÃ´i cho tháº¥y SpecInfer vÆ°á»£t trá»™i hÆ¡n cÃ¡c há»‡ thá»‘ng phá»¥c vá»¥ LLM hiá»‡n cÃ³ tá»« 1.5-2.8Ã— cho suy luáº­n LLM phÃ¢n tÃ¡n vÃ  tá»« 2.6-3.5Ã— cho suy luáº­n LLM dá»±a trÃªn offloading, trong khi báº£o toÃ n cÃ¹ng hiá»‡u suáº¥t sinh táº¡o. SpecInfer cÃ³ sáºµn cÃ´ng khai táº¡i https://github.com/flexflow/FlexFlow/

Tá»« khÃ³a: phá»¥c vá»¥ mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n, giáº£i mÃ£ suy Ä‘oÃ¡n, xÃ¡c minh cÃ¢y token

Äá»‹nh dáº¡ng Tham chiáº¿u ACM:
Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, vÃ  Zhihao Jia. 2024. SpecInfer: TÄƒng tá»‘c Phá»¥c vá»¥ MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n vá»›i Suy luáº­n Suy Ä‘oÃ¡n Dá»±a trÃªn CÃ¢y vÃ  XÃ¡c minh. Trong Há»™i nghá»‹ Quá»‘c táº¿ ACM láº§n thá»© 29 vá» Há»— trá»£ Kiáº¿n trÃºc cho NgÃ´n ngá»¯ Láº­p trÃ¬nh vÃ  Há»‡ Ä‘iá»u hÃ nh, Táº­p 3 (ASPLOS '24), 27 thÃ¡ng 4 - 1 thÃ¡ng 5, 2024, La Jolla, CA, USA. ACM, New York, NY, USA, 18 trang. https://doi.org/10.1145/3620666.3651335

1 Giá»›i thiá»‡u
CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n sinh táº¡o (LLM), nhÆ° ChatGPT [3] vÃ  GPT-4 [33], Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh lÃ  máº¡nh máº½ trong nhiá»u lÄ©nh vá»±c á»©ng dá»¥ng khÃ¡c nhau, bao gá»“m tráº£ lá»i cÃ¢u há»i, tá»•ng há»£p chÆ°Æ¡ng trÃ¬nh vÃ  tá»± Ä‘á»™ng hÃ³a tÃ¡c vá»¥ [26,56]. Tuy nhiÃªn, viá»‡c phá»¥c vá»¥ nhá»¯ng LLM nÃ y má»™t cÃ¡ch nhanh chÃ³ng vÃ  ráº» tiá»n lÃ  má»™t thÃ¡ch thá»©c do khá»‘i lÆ°á»£ng tham sá»‘ lá»›n, kiáº¿n trÃºc phá»©c táº¡p vÃ  yÃªu cáº§u tÃ­nh toÃ¡n cao cá»§a chÃºng. VÃ­ dá»¥, kiáº¿n trÃºc GPT-3 lá»›n nháº¥t cÃ³ 175 tá»· tham sá»‘, Ä‘Ã²i há»i hÆ¡n tÃ¡m GPU NVIDIA 40GB A100 Ä‘á»ƒ lÆ°u trá»¯ á»Ÿ dáº¡ng sá»‘ thá»±c Ä‘á»™ chÃ­nh xÃ¡c ná»­a, vÃ  máº¥t vÃ i giÃ¢y Ä‘á»ƒ phá»¥c vá»¥ má»™t yÃªu cáº§u suy luáº­n duy nháº¥t [3].

Má»™t LLM thÆ°á»ng nháº­n Ä‘áº§u vÃ o lÃ  má»™t chuá»—i token, Ä‘Æ°á»£c gá»i lÃ  prompt, vÃ  sinh ra cÃ¡c token tiáº¿p theo tá»«ng cÃ¡i má»™t, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 1a. Viá»‡c sinh ra má»—i token trong chuá»—i Ä‘Æ°á»£c Ä‘iá»u kiá»‡n bá»Ÿi prompt Ä‘áº§u vÃ o vÃ  cÃ¡c token Ä‘Æ°á»£c sinh ra trÆ°á»›c Ä‘Ã³ vÃ  khÃ´ng xem xÃ©t cÃ¡c token tÆ°Æ¡ng lai. CÃ¡ch tiáº¿p cáº­n nÃ y cÅ©ng Ä‘Æ°á»£c gá»i lÃ  giáº£i mÃ£ tá»± há»“i quy vÃ¬ má»—i token Ä‘Æ°á»£c sinh ra cÅ©ng Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m Ä‘áº§u vÃ o Ä‘á»ƒ sinh ra cÃ¡c token tÆ°Æ¡ng lai. Sá»± phá»¥ thuá»™c nÃ y giá»¯a cÃ¡c token lÃ  quan trá»ng Ä‘á»‘i vá»›i nhiá»u tÃ¡c vá»¥ NLP Ä‘Ã²i há»i báº£o toÃ n thá»© tá»± vÃ  ngá»¯ cáº£nh cá»§a cÃ¡c token Ä‘Æ°á»£c sinh ra, cháº³ng háº¡n nhÆ° hoÃ n thÃ nh vÄƒn báº£n [55].

CÃ¡c há»‡ thá»‘ng LLM hiá»‡n cÃ³ thÆ°á»ng sá»­ dá»¥ng cÃ¡ch tiáº¿p cáº­n giáº£i mÃ£ tÄƒng dáº§n Ä‘á»ƒ phá»¥c vá»¥ má»™t yÃªu cáº§u, trong Ä‘Ã³ há»‡ thá»‘ng tÃ­nh toÃ¡n cÃ¡c kÃ­ch hoáº¡t cho táº¥t cáº£ cÃ¡c token prompt trong má»™t bÆ°á»›c duy nháº¥t vÃ  sau Ä‘Ã³ láº·p láº¡i giáº£i mÃ£ má»™t token má»›i báº±ng cÃ¡ch sá»­ dá»¥ng prompt Ä‘áº§u vÃ o vÃ  táº¥t cáº£ cÃ¡c token Ä‘Æ°á»£c sinh ra trÆ°á»›c Ä‘Ã³ [27]. CÃ¡ch tiáº¿p cáº­n nÃ y tÃ´n trá»ng cÃ¡c phá»¥ thuá»™c dá»¯ liá»‡u giá»¯a cÃ¡c token, nhÆ°ng Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t thá»i gian cháº¡y khÃ´ng tá»‘i Æ°u vÃ  sá»­ dá»¥ng GPU háº¡n cháº¿, vÃ¬ má»©c Ä‘á»™ song song trong má»—i yÃªu cáº§u bá»‹ háº¡n cháº¿ ráº¥t nhiá»u trong giai Ä‘oáº¡n tÄƒng dáº§n. NgoÃ i ra, cÆ¡ cháº¿ attention cá»§a Transformer [48] Ä‘Ã²i há»i truy cáº­p cÃ¡c key vÃ  value cá»§a táº¥t cáº£ cÃ¡c token trÆ°á»›c Ä‘Ã³ Ä‘á»ƒ tÃ­nh toÃ¡n Ä‘áº§u ra attention cá»§a má»™t token má»›i. Äá»ƒ trÃ¡nh tÃ­nh toÃ¡n láº¡i cÃ¡c key vÃ  value cho táº¥t cáº£ cÃ¡c token Ä‘i trÆ°á»›c, cÃ¡c há»‡ thá»‘ng LLM ngÃ y nay sá»­ dá»¥ng cÆ¡ cháº¿ caching Ä‘á»ƒ lÆ°u trá»¯ cÃ¡c key vÃ  value cá»§a chÃºng Ä‘á»ƒ tÃ¡i sá»­ dá»¥ng trong cÃ¡c láº§n láº·p tÆ°Æ¡ng lai. Äá»‘i vá»›i cÃ¡c tÃ¡c vá»¥ sinh táº¡o chuá»—i dÃ i (vÃ­ dá»¥, GPT-4 há»— trá»£ lÃªn Ä‘áº¿n 32K token trong má»™t yÃªu cáº§u), viá»‡c caching key vÃ  value táº¡o ra chi phÃ­ bá»™ nhá»› Ä‘Ã¡ng ká»ƒ, ngÄƒn cáº£n cÃ¡c há»‡ thá»‘ng hiá»‡n cÃ³ phá»¥c vá»¥ má»™t sá»‘ lÆ°á»£ng lá»›n yÃªu cáº§u song song do yÃªu cáº§u bá»™ nhá»› Ä‘á»ƒ cache key vÃ  value cá»§a chÃºng.

ÄÆ°á»£c thÃºc Ä‘áº©y bá»Ÿi Ã½ tÆ°á»Ÿng thá»±c thi suy Ä‘oÃ¡n trong tá»‘i Æ°u hÃ³a bá»™ xá»­ lÃ½ [13,42], cÃ¡c nghiÃªn cá»©u gáº§n Ä‘Ã¢y giá»›i thiá»‡u suy luáº­n suy Ä‘oÃ¡n dá»±a trÃªn chuá»—i, táº­n dá»¥ng má»™t mÃ´ hÃ¬nh suy Ä‘oÃ¡n nhá» (SSM) Ä‘á»ƒ sinh ra má»™t chuá»—i token vÃ  sá»­ dá»¥ng má»™t LLM Ä‘á»ƒ kiá»ƒm tra tÃ­nh Ä‘Ãºng Ä‘áº¯n cá»§a chÃºng song song [5,22,25,44,51]. Nhá»¯ng ná»— lá»±c nÃ y chá»‰ xem xÃ©t má»™t chuá»—i token Ä‘Æ°á»£c sinh ra bá»Ÿi má»™t SSM duy nháº¥t Ä‘á»ƒ suy Ä‘oÃ¡n, Ä‘iá»u nÃ y khÃ´ng thá»ƒ phÃ¹ há»£p tá»‘t vá»›i má»™t LLM do khoáº£ng cÃ¡ch nÄƒng lá»±c mÃ´ hÃ¬nh giá»¯a chÃºng, vÃ¬ cÃ¡c SSM thÆ°á»ng nhá» hÆ¡n hÃ ng báº­c Ä‘á»™ lá»›n so vá»›i LLM Ä‘á»ƒ duy trÃ¬ chi phÃ­ bá»™ nhá»› vÃ  thá»i gian cháº¡y tháº¥p.

BÃ i bÃ¡o nÃ y giá»›i thiá»‡u SpecInfer, má»™t há»‡ thá»‘ng cáº£i thiá»‡n Ä‘á»™ trá»… end-to-end vÃ  hiá»‡u quáº£ tÃ­nh toÃ¡n cá»§a viá»‡c phá»¥c vá»¥ LLM vá»›i suy luáº­n suy Ä‘oÃ¡n dá»±a trÃªn cÃ¢y vÃ  xÃ¡c minh. HÃ¬nh 1b minh há»a sá»± so sÃ¡nh giá»¯a giáº£i mÃ£ tÄƒng dáº§n hiá»‡n cÃ³, suy luáº­n suy Ä‘oÃ¡n dá»±a trÃªn chuá»—i, vÃ  suy luáº­n suy Ä‘oÃ¡n dá»±a trÃªn cÃ¢y cá»§a chÃºng tÃ´i. Má»™t insight chÃ­nh Ä‘áº±ng sau SpecInfer lÃ  Ä‘á»“ng thá»i xem xÃ©t sá»± Ä‘a dáº¡ng cá»§a cÃ¡c á»©ng viÃªn suy Ä‘oÃ¡n (thay vÃ¬ chá»‰ má»™t nhÆ° trong cÃ¡c cÃ¡ch tiáº¿p cáº­n hiá»‡n cÃ³) Ä‘á»ƒ tá»‘i Ä‘a hÃ³a hiá»‡u suáº¥t suy Ä‘oÃ¡n. Nhá»¯ng á»©ng viÃªn nÃ y Ä‘Æ°á»£c tá»• chá»©c nhÆ° má»™t cÃ¢y token, mÃ  cÃ¡c nÃºt cá»§a nÃ³ má»—i nÃºt Ä‘áº¡i diá»‡n cho má»™t chuá»—i token Ä‘Æ°á»£c suy Ä‘oÃ¡n. TÃ­nh Ä‘Ãºng Ä‘áº¯n cá»§a táº¥t cáº£ cÃ¡c chuá»—i token á»©ng viÃªn Ä‘Æ°á»£c xÃ¡c minh so vá»›i LLM song song, Ä‘iá»u nÃ y cho phÃ©p SpecInfer tÄƒng Ä‘Ã¡ng ká»ƒ sá»‘ lÆ°á»£ng token Ä‘Æ°á»£c sinh ra trong má»™t bÆ°á»›c giáº£i mÃ£ LLM.

So vá»›i suy luáº­n suy Ä‘oÃ¡n dá»±a trÃªn chuá»—i, táº­n dá»¥ng cáº¥u trÃºc cÃ¢y cÃ³ thá»ƒ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ tá»· lá»‡ thÃ nh cÃ´ng cá»§a viá»‡c xÃ¡c minh má»™t token (vÃ­ dá»¥, tá»« 52-57% lÃªn 96-97% cho giáº£i mÃ£ ngáº«u nhiÃªn nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 1). Tuy nhiÃªn, viá»‡c thá»±c hiá»‡n cáº£i thiá»‡n nÃ y Ä‘Ã²i há»i giáº£i quyáº¿t hai thÃ¡ch thá»©c Ä‘á»™c Ä‘Ã¡o. Tiáº¿p theo, chÃºng tÃ´i trÃ¬nh bÃ y chi tiáº¿t vá» nhá»¯ng thÃ¡ch thá»©c nÃ y vÃ  cÃ¡c Ã½ tÆ°á»Ÿng chÃ­nh mÃ  SpecInfer sá»­ dá»¥ng Ä‘á»ƒ giáº£i quyáº¿t chÃºng.

Äáº§u tiÃªn, SpecInfer pháº£i khÃ¡m phÃ¡ má»™t khÃ´ng gian tÃ¬m kiáº¿m cá»±c ká»³ lá»›n cá»§a cÃ¡c chuá»—i token á»©ng viÃªn Ä‘á»ƒ tá»‘i Ä‘a hÃ³a hiá»‡u suáº¥t suy Ä‘oÃ¡n. Máº·c dÃ¹ Ã½ tÆ°á»Ÿng thá»±c thi suy Ä‘oÃ¡n Ä‘Ã£ Ä‘Æ°á»£c triá»ƒn khai rá»™ng rÃ£i trong nhiá»u tÃ¡c vá»¥ tá»‘i Æ°u hÃ³a khÃ¡c nhau trong kiáº¿n trÃºc mÃ¡y tÃ­nh vÃ  há»‡ thá»‘ng, bao gá»“m dá»± Ä‘oÃ¡n nhÃ¡nh trong cÃ¡c bá»™ xá»­ lÃ½ pipeline hiá»‡n Ä‘áº¡i vÃ  dá»± Ä‘oÃ¡n giÃ¡ trá»‹ Ä‘á»ƒ pre-fetch bá»™ nhá»› vÃ  tá»‡p [13,42], khÃ´ng gian tÃ¬m kiáº¿m Ä‘Æ°á»£c SpecInfer xem xÃ©t lá»›n hÆ¡n Ä‘Ã¡ng ká»ƒ do hai lÃ½ do: (1) cÃ¡c LLM hiá»‡n Ä‘áº¡i thÆ°á»ng liÃªn quan Ä‘áº¿n tá»« vá»±ng ráº¥t lá»›n, vÃ  (2) tá»‘i Ä‘a hÃ³a hiá»‡u suáº¥t suy Ä‘oÃ¡n Ä‘Ã²i há»i dá»± Ä‘oÃ¡n nhiá»u token tÆ°Æ¡ng lai (thay vÃ¬ chá»‰ token tiáº¿p theo). VÃ­ dá»¥, táº¥t cáº£ cÃ¡c LLM trong há» mÃ´ hÃ¬nh OPT xem xÃ©t 50,272 token khÃ¡c nhau cÃ³ thá»ƒ trong tá»« vá»±ng cá»§a chÃºng, trong khi SpecInfer cÃ³ thá»ƒ dá»± Ä‘oÃ¡n Ä‘Ãºng 4 token tiáº¿p theo trung bÃ¬nh. Äáº¡t Ä‘Æ°á»£c má»¥c tiÃªu nÃ y Ä‘Ã²i há»i xem xÃ©t má»™t khÃ´ng gian tÃ¬m kiáº¿m cá»§a 502724â‰ˆ6Ã—1018 tá»• há»£p token khÃ¡c nhau.

SpecInfer táº­n dá»¥ng cÃ¡c biáº¿n thá»ƒ Ä‘Æ°á»£c chÆ°ng cáº¥t, lÆ°á»£ng tá»­ hÃ³a vÃ /hoáº·c cáº¯t tá»‰a hiá»‡n cÃ³ cá»§a má»™t LLM, mÃ  chÃºng tÃ´i gá»i lÃ  cÃ¡c mÃ´ hÃ¬nh suy Ä‘oÃ¡n nhá» (SSM), Ä‘á»ƒ hÆ°á»›ng dáº«n suy Ä‘oÃ¡n. Má»™t thÃ¡ch thá»©c chÃ­nh cá»§a viá»‡c sá»­ dá»¥ng SSM cho suy luáº­n suy Ä‘oÃ¡n lÃ  sá»± phÃ¹ há»£p giá»¯a má»™t SSM vÃ  má»™t LLM vá»‘n bá»‹ giá»›i háº¡n bá»Ÿi khoáº£ng cÃ¡ch nÄƒng lá»±c mÃ´ hÃ¬nh, vÃ¬ má»™t SSM thÆ°á»ng nhá» hÆ¡n 100-1000Ã— so vá»›i má»™t LLM. Thay vÃ¬ sá»­ dá»¥ng má»™t SSM duy nháº¥t cho suy Ä‘oÃ¡n dá»±a trÃªn chuá»—i, SpecInfer tá»‘i Ä‘a hÃ³a hiá»‡u suáº¥t suy Ä‘oÃ¡n báº±ng cÃ¡ch Ä‘á»“ng thá»i xem xÃ©t nhiá»u chuá»—i token Ä‘Æ°á»£c tá»• chá»©c trong cáº¥u trÃºc cÃ¢y cho má»™t prompt Ä‘áº§u vÃ o nháº¥t Ä‘á»‹nh. SpecInfer giá»›i thiá»‡u má»™t cÆ¡ cháº¿ dá»±a trÃªn má»Ÿ rá»™ng vÃ  há»£p nháº¥t Ä‘á»ƒ xÃ¢y dá»±ng cÃ¢y token báº±ng cÃ¡ch khai thÃ¡c sá»± Ä‘a dáº¡ng trong má»™t SSM duy nháº¥t vÃ  trÃªn nhiá»u SSM, tÆ°Æ¡ng á»©ng.

ThÃ¡ch thá»©c thá»© hai mÃ  SpecInfer pháº£i giáº£i quyáº¿t lÃ  xÃ¡c minh cÃ¡c token Ä‘Æ°á»£c suy Ä‘oÃ¡n. Nhiá»u á»©ng dá»¥ng LLM thá»±c hiá»‡n giáº£i mÃ£ ngáº«u nhiÃªn, láº¥y máº«u token tiáº¿p theo tá»« má»™t phÃ¢n phá»‘i xÃ¡c suáº¥t thay vÃ¬ sinh ra má»™t token má»™t cÃ¡ch quyáº¿t Ä‘á»‹nh. Äá»ƒ báº£o toÃ n hiá»‡u suáº¥t sinh táº¡o cá»§a LLM, SpecInfer pháº£i Ä‘áº£m báº£o ráº±ng cÆ¡ cháº¿ suy luáº­n suy Ä‘oÃ¡n dá»±a trÃªn cÃ¢y vÃ  xÃ¡c minh cá»§a nÃ³ sinh ra token tiáº¿p theo báº±ng cÃ¡ch tuÃ¢n theo chÃ­nh xÃ¡c cÃ¹ng phÃ¢n phá»‘i xÃ¡c suáº¥t nhÆ° giáº£i mÃ£ tÄƒng dáº§n. Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c má»¥c tiÃªu nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t láº¥y máº«u suy Ä‘oÃ¡n Ä‘a bÆ°á»›c, má»™t cÃ¡ch tiáº¿p cáº­n láº¥y máº«u má»›i cho cÃ¡c SSM Ä‘áº£m báº£o tÆ°Æ¡ng Ä‘Æ°Æ¡ng trong khi tá»‘i Ä‘a hÃ³a sá»‘ lÆ°á»£ng token Ä‘Æ°á»£c suy Ä‘oÃ¡n cÃ³ thá»ƒ Ä‘Æ°á»£c xÃ¡c minh. Äá»ƒ tá»‘i thiá»ƒu hÃ³a chi phÃ­ xÃ¡c minh cÃ¢y token, SpecInfer giá»›i thiá»‡u má»™t cÆ¡ cháº¿ giáº£i mÃ£ song song dá»±a trÃªn cÃ¢y, Ä‘á»“ng thá»i xÃ¡c minh táº¥t cáº£ cÃ¡c token cá»§a má»™t cÃ¢y token so vá»›i Ä‘áº§u ra cá»§a LLM trong má»™t bÆ°á»›c giáº£i mÃ£ LLM duy nháº¥t.

Báº±ng cÃ¡ch táº­n dá»¥ng suy luáº­n suy Ä‘oÃ¡n dá»±a trÃªn cÃ¢y vÃ  xÃ¡c minh, SpecInfer tÄƒng tá»‘c cáº£ suy luáº­n LLM phÃ¢n tÃ¡n trÃªn nhiá»u GPU vÃ  suy luáº­n LLM dá»±a trÃªn offloading trÃªn má»™t GPU. ÄÃ¡nh giÃ¡ cá»§a chÃºng tÃ´i cho tháº¥y SpecInfer vÆ°á»£t trá»™i hÆ¡n cÃ¡c há»‡ thá»‘ng phá»¥c vá»¥ LLM hiá»‡n cÃ³ tá»« 1.5-2.8Ã— cho suy luáº­n LLM phÃ¢n tÃ¡n vÃ  tá»« 2.6-3.5Ã— cho suy luáº­n LLM dá»±a trÃªn offloading, trong khi báº£o toÃ n cÃ¹ng Ä‘á»™ chÃ­nh xÃ¡c sinh táº¡o.

Äá»ƒ tÃ³m táº¯t, chÃºng tÃ´i Ä‘Ã³ng gÃ³p nhÆ° sau:
â€¢ ChÃºng tÃ´i trÃ¬nh bÃ y SpecInfer, má»™t há»‡ thá»‘ng suy luáº­n suy Ä‘oÃ¡n dá»±a trÃªn cÃ¢y vÃ  xÃ¡c minh cho viá»‡c phá»¥c vá»¥ LLM.
â€¢ Äá»ƒ tá»‘i Ä‘a hÃ³a hiá»‡u suáº¥t suy Ä‘oÃ¡n, chÃºng tÃ´i Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p dá»±a trÃªn há»£p nháº¥t vÃ  má»Ÿ rá»™ng Ä‘á»ƒ xÃ¢y dá»±ng cÃ¢y token báº±ng cÃ¡ch khai thÃ¡c sá»± Ä‘a dáº¡ng trong vÃ  giá»¯a cÃ¡c SSM, tÆ°Æ¡ng á»©ng.
â€¢ Äá»ƒ tá»‘i thiá»ƒu hÃ³a chi phÃ­ xÃ¡c minh, chÃºng tÃ´i giá»›i thiá»‡u má»™t cÆ¡ cháº¿ giáº£i mÃ£ song song dá»±a trÃªn cÃ¢y Ä‘á»ƒ Ä‘á»“ng thá»i xÃ¡c minh táº¥t cáº£ cÃ¡c token cá»§a má»™t cÃ¢y token.
â€¢ ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ SpecInfer vÃ  cho tháº¥y nÃ³ vÆ°á»£t trá»™i hÆ¡n cÃ¡c há»‡ thá»‘ng hiá»‡n cÃ³ lÃªn Ä‘áº¿n 2.8Ã— cho suy luáº­n phÃ¢n tÃ¡n vÃ  lÃªn Ä‘áº¿n 3.5Ã— cho suy luáº­n dá»±a trÃªn offloading.

2 Tá»•ng quan vá» SpecInfer
HÃ¬nh 2 cho tháº¥y tá»•ng quan vá» SpecInfer, bao gá»“m má»™t bá»™ suy Ä‘oÃ¡n dá»±a trÃªn há»c mÃ¡y nháº­n Ä‘áº§u vÃ o lÃ  má»™t chuá»—i token vÃ  táº¡o ra má»™t cÃ¢y token Ä‘Æ°á»£c suy Ä‘oÃ¡n. Má»¥c tiÃªu cá»§a bá»™ suy Ä‘oÃ¡n lÃ  dá»± Ä‘oÃ¡n Ä‘áº§u ra cá»§a LLM báº±ng cÃ¡ch tá»‘i Ä‘a hÃ³a sá»± chá»“ng láº·p giá»¯a cÃ¢y token Ä‘Æ°á»£c suy Ä‘oÃ¡n vÃ  cÃ¡c token Ä‘Æ°á»£c sinh ra bá»Ÿi LLM sá»­ dá»¥ng giáº£i mÃ£ tÄƒng dáº§n (Thuáº­t toÃ¡n 1).

Thuáº­t toÃ¡n 1 Thuáº­t toÃ¡n giáº£i mÃ£ tÄƒng dáº§n Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c há»‡ thá»‘ng phá»¥c vá»¥ LLM hiá»‡n cÃ³.
1: Äáº§u vÃ o: Má»™t chuá»—i token Ä‘áº§u vÃ o â„
2: Äáº§u ra: Má»™t chuá»—i token Ä‘Æ°á»£c sinh ra
3: ğ’®=â„
4: while true do
5: ğ‘¡=Decode(LLM,ğ’®)
6: ğ’®.append(ğ‘¡)
7: if ğ‘¡=âŸ¨EOSâŸ© then
8: Return ğ’®

CÃ³ má»™t sá»‘ cÃ¡ch Ä‘á»ƒ chuáº©n bá»‹ SSM cho suy luáº­n suy Ä‘oÃ¡n. Äáº§u tiÃªn, cÃ¡c LLM hiá»‡n Ä‘áº¡i thÆ°á»ng cÃ³ nhiá»u kiáº¿n trÃºc nhá» hÆ¡n Ä‘Æ°á»£c pre-train cÃ¹ng vá»›i LLM báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¹ng bá»™ dá»¯ liá»‡u. VÃ­ dá»¥, ngoÃ i mÃ´ hÃ¬nh OPT-175B vá»›i 175 tá»· tham sá»‘, há» mÃ´ hÃ¬nh OPT cÅ©ng bao gá»“m OPT-125M vÃ  OPT-350M, hai biáº¿n thá»ƒ vá»›i 125 triá»‡u vÃ  350 triá»‡u tham sá»‘, Ä‘Æ°á»£c pre-train báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¹ng bá»™ dá»¯ liá»‡u nhÆ° OPT-175B [57]. Nhá»¯ng mÃ´ hÃ¬nh nhá» Ä‘Æ°á»£c pre-train nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng trá»±c tiáº¿p lÃ m SSM. Thá»© hai, Ä‘á»ƒ cáº£i thiá»‡n Ä‘á»™ bao phá»§ cá»§a cÃ¡c token Ä‘Æ°á»£c suy Ä‘oÃ¡n tá»« SSM, SpecInfer sá»­ dá»¥ng má»™t phÆ°Æ¡ng phÃ¡p suy Ä‘oÃ¡n dá»±a trÃªn má»Ÿ rá»™ng vÃ  dá»±a trÃªn há»£p nháº¥t nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ á»Ÿ Ä‘áº§u HÃ¬nh 2. CÃ¡c token Ä‘Æ°á»£c suy Ä‘oÃ¡n Ä‘Æ°á»£c tá»• chá»©c trong cáº¥u trÃºc cÃ¢y token.

Viá»‡c sá»­ dá»¥ng LLM cá»§a SpecInfer cÅ©ng khÃ¡c vá»›i cÃ¡c há»‡ thá»‘ng phá»¥c vá»¥ LLM hiá»‡n cÃ³. Thay vÃ¬ sá»­ dá»¥ng LLM nhÆ° má»™t bá»™ giáº£i mÃ£ tÄƒng dáº§n dá»± Ä‘oÃ¡n token Ä‘Æ¡n tiáº¿p theo, SpecInfer sá»­ dá»¥ng LLM nhÆ° má»™t bá»™ xÃ¡c minh cÃ¢y token xÃ¡c minh má»™t cÃ¢y token Ä‘Æ°á»£c suy Ä‘oÃ¡n so vá»›i Ä‘áº§u ra cá»§a LLM. Äá»‘i vá»›i má»—i token, SpecInfer tÃ­nh toÃ¡n cÃ¡c kÃ­ch hoáº¡t cá»§a nÃ³ báº±ng cÃ¡ch xem xÃ©t táº¥t cáº£ cÃ¡c tá»• tiÃªn cá»§a nÃ³ trong cÃ¢y token nhÆ° cÃ¡c token Ä‘i trÆ°á»›c cá»§a nÃ³. VÃ­ dá»¥, trong HÃ¬nh 2, Ä‘áº§u ra attention cá»§a token ğ‘¡3,0 Ä‘Æ°á»£c tÃ­nh toÃ¡n dá»±a trÃªn chuá»—i (ğ‘¡0,ğ‘¡1,0,ğ‘¡2,1,ğ‘¡3,0), trong Ä‘Ã³ ğ‘¡0, ğ‘¡1,0, vÃ  ğ‘¡2,1 lÃ  cÃ¡c tá»• tiÃªn cá»§a ğ‘¡3,0 trong cÃ¢y token. SpecInfer bao gá»“m má»™t cÆ¡ cháº¿ giáº£i mÃ£ song song dá»±a trÃªn cÃ¢y má»›i Ä‘á»ƒ Ä‘á»“ng thá»i xÃ¡c minh táº¥t cáº£ cÃ¡c token cá»§a má»™t cÃ¢y token trong má»™t bÆ°á»›c giáº£i mÃ£ LLM duy nháº¥t.

Suy luáº­n suy Ä‘oÃ¡n vÃ  xÃ¡c minh cÃ¢y token cá»§a SpecInfer cung cáº¥p hai lá»£i tháº¿ chÃ­nh so vá»›i cÃ¡ch tiáº¿p cáº­n giáº£i mÃ£ tÄƒng dáº§n cá»§a cÃ¡c há»‡ thá»‘ng suy luáº­n LLM hiá»‡n cÃ³.

Giáº£m truy cáº­p bá»™ nhá»› Ä‘áº¿n tham sá»‘ LLM. Hiá»‡u suáº¥t suy luáº­n LLM pháº§n lá»›n bá»‹ háº¡n cháº¿ bá»Ÿi viá»‡c truy cáº­p bá»™ nhá»› GPU. Trong cÃ¡ch tiáº¿p cáº­n giáº£i mÃ£ tÄƒng dáº§n hiá»‡n cÃ³, viá»‡c sinh ra má»™t token duy nháº¥t Ä‘Ã²i há»i truy cáº­p táº¥t cáº£ tham sá»‘ cá»§a má»™t LLM. Váº¥n Ä‘á» Ä‘Æ°á»£c lÃ m tráº§m trá»ng hÆ¡n Ä‘á»‘i vá»›i cÃ¡c há»‡ thá»‘ng suy luáº­n LLM dá»±a trÃªn offloading, sá»­ dá»¥ng tÃ i nguyÃªn tÃ­nh toÃ¡n háº¡n cháº¿ nhÆ° má»™t GPU thÃ´ng thÆ°á»ng duy nháº¥t Ä‘á»ƒ phá»¥c vá»¥ LLM báº±ng cÃ¡ch sá»­ dá»¥ng CPU DRAM vÃ  lÆ°u trá»¯ bá»n vá»¯ng Ä‘á»ƒ lÆ°u tham sá»‘ mÃ´ hÃ¬nh vÃ  táº£i nhá»¯ng tham sá»‘ nÃ y vÃ o bá»™ nhá»› bÄƒng thÃ´ng cao (HBM) cá»§a GPU Ä‘á»ƒ tÃ­nh toÃ¡n. So vá»›i cÃ¡ch tiáº¿p cáº­n giáº£i mÃ£ tÄƒng dáº§n, SpecInfer giáº£m Ä‘Ã¡ng ká»ƒ viá»‡c truy cáº­p tham sá»‘ LLM báº¥t cá»© khi nÃ o sá»± chá»“ng láº·p giá»¯a má»™t cÃ¢y token Ä‘Æ°á»£c suy Ä‘oÃ¡n vÃ  Ä‘áº§u ra thá»±c táº¿ cá»§a LLM khÃ´ng rá»—ng. Viá»‡c giáº£m truy cáº­p bá»™ nhá»› thiáº¿t bá»‹ GPU vÃ  giáº£m truyá»n dá»¯ liá»‡u giá»¯a bá»™ nhá»› GPU vÃ  CPU cÅ©ng cÃ³ thá»ƒ trá»±c tiáº¿p dáº«n Ä‘áº¿n giáº£m tiÃªu thá»¥ nÄƒng lÆ°á»£ng, vÃ¬ viá»‡c truy cáº­p GPU HBM tiÃªu thá»¥ nÄƒng lÆ°á»£ng nhiá»u hÆ¡n hai hoáº·c ba báº­c Ä‘á»™ lá»›n so vá»›i cÃ¡c phÃ©p toÃ¡n sá»‘ há»c dáº¥u pháº©y Ä‘á»™ng.

Giáº£m Ä‘á»™ trá»… suy luáº­n end-to-end. Phá»¥c vá»¥ LLM gáº·p pháº£i Ä‘á»™ trá»… suy luáº­n end-to-end dÃ i. VÃ­ dá»¥, kiáº¿n trÃºc GPT-3 bao gá»“m 175 tá»· tham sá»‘ vÃ  Ä‘Ã²i há»i nhiá»u giÃ¢y Ä‘á»ƒ phá»¥c vá»¥ má»™t yÃªu cáº§u. Trong cÃ¡ch tiáº¿p cáº­n giáº£i mÃ£ tÄƒng dáº§n hiá»‡n cÃ³, viá»‡c tÃ­nh toÃ¡n Ä‘á»ƒ sinh ra má»—i token phá»¥ thuá»™c vÃ o cÃ¡c key vÃ  value cá»§a táº¥t cáº£ cÃ¡c token Ä‘Æ°á»£c sinh ra trÆ°á»›c Ä‘Ã³, Ä‘iá»u nÃ y táº¡o ra cÃ¡c phá»¥ thuá»™c tuáº§n tá»± giá»¯a cÃ¡c token vÃ  Ä‘Ã²i há»i cÃ¡c há»‡ thá»‘ng phá»¥c vá»¥ LLM hiá»‡n Ä‘áº¡i pháº£i tuáº§n tá»± hÃ³a viá»‡c sinh ra cÃ¡c token khÃ¡c nhau cho má»—i yÃªu cáº§u. Trong SpecInfer, cÃ¡c LLM Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° má»™t bá»™ xÃ¡c minh nháº­n má»™t cÃ¢y token Ä‘Æ°á»£c suy Ä‘oÃ¡n lÃ m Ä‘áº§u vÃ o vÃ  cÃ³ thá»ƒ Ä‘á»“ng thá»i kiá»ƒm tra táº¥t cáº£ cÃ¡c token trong cÃ¢y token báº±ng cÃ¡ch thá»±c hiá»‡n má»™t láº§n xÃ¡c minh duy nháº¥t qua LLM. CÃ¡ch tiáº¿p cáº­n nÃ y cho phÃ©p song song hÃ³a giá»¯a cÃ¡c token khÃ¡c nhau trong má»™t yÃªu cáº§u duy nháº¥t vÃ  giáº£m Ä‘á»™ trá»… suy luáº­n end-to-end cá»§a LLM.

3 Bá»™ suy Ä‘oÃ¡n dá»±a trÃªn há»c mÃ¡y
CÃ¡c phÆ°Æ¡ng phÃ¡p giáº£i mÃ£ suy Ä‘oÃ¡n hiá»‡n cÃ³ thá»±c hiá»‡n suy Ä‘oÃ¡n dá»±a trÃªn chuá»—i, trong Ä‘Ã³ má»™t SSM dá»± Ä‘oÃ¡n má»™t chuá»—i token duy nháº¥t Ä‘á»ƒ Ä‘Æ°á»£c xÃ¡c minh bá»Ÿi má»™t LLM. Tuy nhiÃªn, má»™t háº¡n cháº¿ chÃ­nh cá»§a má»™t chuá»—i Ä‘Æ°á»£c suy Ä‘oÃ¡n duy nháº¥t lÃ  xÃ¡c suáº¥t cá»§a sá»± phÃ¹ há»£p thÃ nh cÃ´ng giá»¯a LLM vÃ  chuá»—i token Ä‘Æ°á»£c suy Ä‘oÃ¡n giáº£m theo cáº¥p sá»‘ nhÃ¢n vá»›i Ä‘á»™ dÃ i phÃ¹ há»£p dá»± kiáº¿n. Äiá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c lÃ m tráº§m trá»ng hÆ¡n bá»Ÿi thá»±c táº¿ lÃ  suy Ä‘oÃ¡n chá»‰ bao gá»“m má»™t token á»©ng viÃªn duy nháº¥t Ä‘á»ƒ xÃ¡c minh má»—i bÆ°á»›c, dáº«n Ä‘áº¿n hiá»‡u suáº¥t suy Ä‘oÃ¡n khÃ´ng tá»‘i Æ°u. Máº·t khÃ¡c, báº±ng cÃ¡ch khuyáº¿n khÃ­ch cÃ¡c á»©ng viÃªn Ä‘Æ°á»£c suy Ä‘oÃ¡n Ä‘a dáº¡ng hÆ¡n má»—i bÆ°á»›c, xÃ¡c suáº¥t cá»§a má»™t káº¿t quáº£ phÃ¹ há»£p thÃ nh cÃ´ng má»—i bÆ°á»›c (tá»©c lÃ , token Ä‘Æ°á»£c giáº£i mÃ£ bá»Ÿi LLM náº±m trong pool á»©ng viÃªn nÃ y) cÃ³ thá»ƒ Ä‘Æ°á»£c cáº£i thiá»‡n ráº¥t nhiá»u. Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y, SpecInfer nháº±m xÃ¢y dá»±ng má»™t cÃ¢y cÃ¡c á»©ng viÃªn Ä‘Æ°á»£c suy Ä‘oÃ¡n báº±ng cÃ¡ch khai thÃ¡c sá»± Ä‘a dáº¡ng trong má»™t SSM duy nháº¥t vÃ  trÃªn nhiá»u SSM. Cá»¥ thá»ƒ, bá»™ suy Ä‘oÃ¡n dá»±a trÃªn há»c mÃ¡y cá»§a SpecInfer tá»•ng há»£p cÃ¡c dá»± Ä‘oÃ¡n cá»§a má»™t hoáº·c nhiá»u SSM Ä‘á»ƒ tá»‘i Ä‘a hÃ³a hiá»‡u suáº¥t suy Ä‘oÃ¡n trong khi duy trÃ¬ chi phÃ­ bá»™ nhá»› tháº¥p vÃ  Ä‘á»™ trá»… suy luáº­n. SpecInfer sá»­ dá»¥ng má»™t cÃ¢y token Ä‘á»ƒ tá»• chá»©c cÃ¡c token Ä‘Æ°á»£c táº¡o ra bá»Ÿi bá»™ suy Ä‘oÃ¡n vÃ  giá»›i thiá»‡u hai phÆ°Æ¡ng phÃ¡p Ä‘á»ƒ xÃ¢y dá»±ng cÃ¢y token: xÃ¢y dá»±ng cÃ¢y dá»±a trÃªn má»Ÿ rá»™ng vÃ  há»£p nháº¥t.

Äá»‹nh nghÄ©a 3.1 (CÃ¢y Token). Má»™t cÃ¢y token ğ’© lÃ  má»™t cáº¥u trÃºc cÃ¢y, trong Ä‘Ã³ má»—i nÃºt ğ‘¢âˆˆğ’© Ä‘Æ°á»£c gáº¯n nhÃ£n vá»›i má»™t token ğ‘¡ğ‘¢, vÃ  ğ‘ğ‘¢ Ä‘áº¡i diá»‡n cho nÃºt cha cá»§a ğ‘¢ trong cÃ¢y token. Äá»‘i vá»›i má»—i nÃºt ğ‘¢, ğ‘†ğ‘¢ Ä‘áº¡i diá»‡n cho má»™t chuá»—i token Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh báº±ng cÃ¡ch ná»‘i ğ‘†ğ‘ğ‘¢ vÃ  {ğ‘¡ğ‘¢}1.

XÃ¢y dá»±ng cÃ¢y token dá»±a trÃªn má»Ÿ rá»™ng. Má»™t cÃ¡ch tiáº¿p cáº­n Ä‘á»ƒ táº¡o ra má»™t cÃ¢y token liÃªn quan Ä‘áº¿n viá»‡c táº¡o ra nhiá»u token tá»« má»™t SSM trong má»™t bÆ°á»›c giáº£i mÃ£ duy nháº¥t. CÃ¡ch tiáº¿p cáº­n nÃ y Ä‘Æ°á»£c thÃºc Ä‘áº©y bá»Ÿi má»™t quan sÃ¡t quan trá»ng ráº±ng khi má»™t SSM khÃ´ng phÃ¹ há»£p vá»›i má»™t LLM (tá»©c lÃ , hai mÃ´ hÃ¬nh chá»n cÃ¡c token top-1 khÃ¡c nhau), token Ä‘Æ°á»£c chá»n bá»Ÿi LLM thÆ°á»ng náº±m trong sá»‘ cÃ¡c token top-ğ‘˜ tá»« SSM cho cÃ¡c giÃ¡ trá»‹ ğ‘˜ ráº¥t nhá». Báº£ng 1 cho tháº¥y tá»· lá»‡ thÃ nh cÃ´ng cá»§a viá»‡c xÃ¡c minh má»™t token báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c token top-ğ‘˜ táº¡o ra tá»« má»™t SSM, trong Ä‘Ã³ má»™t xÃ¡c minh thÃ nh cÃ´ng náº¿u token Ä‘Æ°á»£c chá»n bá»Ÿi LLM náº±m trong sá»‘ cÃ¡c token top-ğ‘˜ tá»« SSM. So vá»›i chá»‰ sá»­ dá»¥ng token top-1 tá»« má»™t SSM, sá»­ dá»¥ng cÃ¡c token top-5 cÃ³ thá»ƒ tÄƒng tá»· lá»‡ thÃ nh cÃ´ng tá»« 70% lÃªn 89% cho giáº£i mÃ£ tham lam vÃ  tá»« 57% lÃªn 97% cho giáº£i mÃ£ ngáº«u nhiÃªn.

Viá»‡c chá»n trá»±c tiáº¿p cÃ¡c token top-ğ‘˜ á»Ÿ má»—i bÆ°á»›c dáº«n Ä‘áº¿n sá»± tÄƒng theo cáº¥p sá»‘ nhÃ¢n trong sá»‘ lÆ°á»£ng chuá»—i token tiá»m nÄƒng, Ä‘iá»u nÃ y lÃ m tÄƒng Ä‘Ã¡ng ká»ƒ Ä‘á»™ trá»… suy luáº­n vÃ  chi phÃ­ bá»™ nhá»›. Do Ä‘Ã³, chÃºng tÃ´i Ã¡p dá»¥ng má»™t chiáº¿n lÆ°á»£c tÄ©nh má»Ÿ rá»™ng cÃ¢y token theo má»™t cáº¥u hÃ¬nh má»Ÿ rá»™ng Ä‘Æ°á»£c Ä‘áº·t trÆ°á»›c Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° má»™t vector sá»‘ nguyÃªn âŸ¨ğ‘˜1,ğ‘˜2,...,ğ‘˜ğ‘šâŸ©, trong Ä‘Ã³ ğ‘š biá»ƒu thá»‹ sá»‘ bÆ°á»›c giáº£i mÃ£ suy Ä‘oÃ¡n tá»‘i Ä‘a, vÃ  ğ‘˜ğ‘– chá»‰ ra sá»‘ lÆ°á»£ng token Ä‘á»ƒ má»Ÿ rá»™ng cho má»—i token trong bÆ°á»›c thá»© ğ‘–. VÃ­ dá»¥, HÃ¬nh 3 minh há»a cáº¥u hÃ¬nh má»Ÿ rá»™ng âŸ¨2,2,1âŸ©, dáº«n Ä‘áº¿n bá»‘n chuá»—i token. ÄÃ¡nh giÃ¡ cá»§a chÃºng tÃ´i (xem Pháº§n 6.4) cho tháº¥y ráº±ng ngay cáº£ má»™t chiáº¿n lÆ°á»£c Ä‘Æ¡n giáº£n cÅ©ng cÃ³ thá»ƒ táº¡o ra káº¿t quáº£ suy Ä‘oÃ¡n cÃ³ Ä‘á»™ chÃ­nh xÃ¡c cao. ChÃºng tÃ´i thá»«a nháº­n ráº±ng viá»‡c má»Ÿ rá»™ng Ä‘á»™ng má»™t cÃ¢y token tá»« má»™t SSM lÃ  má»™t váº¥n Ä‘á» nghiÃªn cá»©u má»Ÿ náº±m ngoÃ i pháº¡m vi cá»§a bÃ i bÃ¡o nÃ y, mÃ  chÃºng tÃ´i Ä‘á»ƒ láº¡i nhÆ° cÃ´ng viá»‡c tÆ°Æ¡ng lai.

XÃ¢y dá»±ng cÃ¢y token dá»±a trÃªn há»£p nháº¥t. NgoÃ i viá»‡c sá»­ dá»¥ng má»™t SSM duy nháº¥t, SpecInfer cÅ©ng cÃ³ thá»ƒ káº¿t há»£p nhiá»u SSM Ä‘á»ƒ cÃ¹ng nhau dá»± Ä‘oÃ¡n Ä‘áº§u ra cá»§a má»™t LLM. SpecInfer sá»­ dá»¥ng má»™t phÆ°Æ¡ng phÃ¡p khÃ´ng giÃ¡m sÃ¡t Ä‘á»ƒ táº­p thá»ƒ boost-tune má»™t pool cÃ¡c SSM Ä‘á»ƒ Ä‘iá»u chá»‰nh Ä‘áº§u ra cá»§a chÃºng vá»›i Ä‘áº§u ra cá»§a LLM báº±ng cÃ¡ch táº­n dá»¥ng adaptive boosting [12]. SpecInfer sá»­ dá»¥ng cÃ¡c SSM Ä‘á»ƒ dá»± Ä‘oÃ¡n vÃ i token tiáº¿p theo mÃ  má»™t LLM sáº½ sinh ra, vÃ  sá»­ dá»¥ng cÃ¡c bá»™ dá»¯ liá»‡u vÄƒn báº£n tá»•ng quÃ¡t (vÃ­ dá»¥, corpus OpenWebText [15] trong Ä‘Ã¡nh giÃ¡ cá»§a chÃºng tÃ´i) Ä‘á»ƒ thÃ­ch á»©ng Ä‘iá»u chá»‰nh Ä‘áº§u ra tá»•ng há»£p cá»§a nhiá»u SSM vá»›i LLM theo cÃ¡ch hoÃ n toÃ n khÃ´ng giÃ¡m sÃ¡t. Cá»¥ thá»ƒ, SpecInfer chuyá»ƒn Ä‘á»•i má»™t corpus vÄƒn báº£n thÃ nh má»™t táº­p há»£p cÃ¡c máº«u prompt vÃ  sá»­ dá»¥ng LLM Ä‘á»ƒ sinh ra má»™t chuá»—i token cho má»—i prompt. SpecInfer Ä‘áº§u tiÃªn fine-tune má»™t SSM táº¡i má»™t thá»i Ä‘iá»ƒm Ä‘áº¿n má»©c tá»‘i Ä‘a vÃ  Ä‘Ã¡nh dáº¥u táº¥t cáº£ cÃ¡c máº«u prompt nÆ¡i SSM vÃ  LLM sinh ra cÃ¡c token tiáº¿p theo giá»‘ng há»‡t nhau. Tiáº¿p theo, SpecInfer lá»c táº¥t cáº£ cÃ¡c máº«u prompt Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u vÃ  sá»­ dá»¥ng táº¥t cáº£ cÃ¡c máº«u cÃ²n láº¡i trong corpus Ä‘á»ƒ fine-tune SSM tiáº¿p theo Ä‘áº¿n má»©c tá»‘i Ä‘a.

Báº±ng cÃ¡ch láº·p láº¡i quÃ¡ trÃ¬nh nÃ y cho má»i SSM trong pool, SpecInfer thu Ä‘Æ°á»£c má»™t táº­p há»£p Ä‘a dáº¡ng cÃ¡c SSM mÃ  Ä‘áº§u ra tá»•ng há»£p cá»§a chÃºng chá»“ng láº·p pháº§n lá»›n vá»›i Ä‘áº§u ra cá»§a LLM trÃªn corpus huáº¥n luyá»‡n. Táº¥t cáº£ cÃ¡c SSM cÃ³ Ä‘á»™ trá»… suy luáº­n giá»‘ng há»‡t nhau, vÃ  do Ä‘Ã³ viá»‡c cháº¡y táº¥t cáº£ cÃ¡c SSM trÃªn cÃ¡c GPU khÃ¡c nhau song song khÃ´ng lÃ m tÄƒng Ä‘á»™ trá»… cá»§a suy luáº­n suy Ä‘oÃ¡n so vá»›i viá»‡c sá»­ dá»¥ng má»™t SSM duy nháº¥t. NgoÃ i ra, SpecInfer sá»­ dá»¥ng song song dá»¯ liá»‡u Ä‘á»ƒ phá»¥c vá»¥ cÃ¡c SSM trÃªn nhiá»u GPU, vÃ  do Ä‘Ã³ viá»‡c sá»­ dá»¥ng nhiá»u SSM khÃ´ng lÃ m tÄƒng chi phÃ­ bá»™ nhá»› trÃªn má»—i GPU. Trong trÆ°á»ng há»£p nhiá»u SSM Ä‘Æ°á»£c sá»­ dá»¥ng, Ä‘áº§u ra cá»§a má»—i SSM Ä‘Æ°á»£c xem nhÆ° má»™t cÃ¢y token, vÃ  SpecInfer thá»±c hiá»‡n há»£p nháº¥t cÃ¢y token Ä‘á»ƒ tá»•ng há»£p táº¥t cáº£ cÃ¡c token Ä‘Æ°á»£c suy Ä‘oÃ¡n trong má»™t cáº¥u trÃºc cÃ¢y duy nháº¥t.

Äá»‹nh nghÄ©a 3.2 (Há»£p nháº¥t CÃ¢y Token). â„³ lÃ  sá»± há»£p nháº¥t cÃ¢y cá»§a ğ‘š cÃ¢y token {ğ’©ğ‘–}(1â‰¤ğ‘–â‰¤ğ‘š) khi vÃ  chá»‰ khi âˆ€1â‰¤ğ‘–â‰¤ğ‘š,âˆ€ğ‘¢âˆˆğ’©ğ‘–,âˆƒğ‘£âˆˆâ„³ sao cho ğ‘†ğ‘£=ğ‘†ğ‘¢ vÃ  ngÆ°á»£c láº¡i.

Má»™t cÃ¡ch trá»±c quan, má»—i cÃ¢y token Ä‘áº¡i diá»‡n cho má»™t táº­p há»£p cÃ¡c chuá»—i token. Viá»‡c há»£p nháº¥t nhiá»u cÃ¢y token táº¡o ra má»™t cÃ¢y má»›i bao gá»“m táº¥t cáº£ cÃ¡c chuá»—i token cá»§a cÃ¡c cÃ¢y gá»‘c. VÃ­ dá»¥, HÃ¬nh 3 cho tháº¥y cÃ¢y token Ä‘Æ°á»£c táº¡o ra báº±ng cÃ¡ch há»£p nháº¥t bá»‘n chuá»—i token. Má»—i chuá»—i token Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh bá»Ÿi má»™t nÃºt trong cÃ¢y token Ä‘Ã£ há»£p nháº¥t.

LÆ°u Ã½ ráº±ng, ngoÃ i boosting, cÃ³ má»™t sá»‘ phÆ°Æ¡ng phÃ¡p há»c táº­p táº­p há»£p khÃ¡c (vÃ­ dá»¥, voting, bagging, vÃ  stacking) [14] cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ káº¿t há»£p cÃ¡c Ä‘áº§u ra tá»« nhiá»u SSM, vÃ  chÃºng tÃ´i Ä‘á»ƒ láº¡i viá»‡c khÃ¡m phÃ¡ nhÆ° cÃ´ng viá»‡c tÆ°Æ¡ng lai.

4 Bá»™ xÃ¡c minh cÃ¢y token
Pháº§n nÃ y giá»›i thiá»‡u bá»™ xÃ¡c minh cÃ¢y token cá»§a SpecInfer, nháº­n Ä‘áº§u vÃ o lÃ  má»™t cÃ¢y token Ä‘Æ°á»£c táº¡o ra bá»Ÿi bá»™ suy Ä‘oÃ¡n vÃ  xÃ¡c minh tÃ­nh Ä‘Ãºng Ä‘áº¯n cá»§a cÃ¡c token cá»§a nÃ³ so vá»›i Ä‘áº§u ra cá»§a LLM. Má»™t Ã½ tÆ°á»Ÿng chÃ­nh Ä‘áº±ng sau thiáº¿t káº¿ cá»§a SpecInfer lÃ  Ä‘á»“ng thá»i xÃ¡c minh táº¥t cáº£ cÃ¡c chuá»—i cá»§a má»™t cÃ¢y token so vá»›i Ä‘áº§u ra gá»‘c cá»§a LLM báº±ng cÃ¡ch thá»±c hiá»‡n má»™t láº§n duy nháº¥t qua cÃ¡c tham sá»‘ cá»§a LLM. Chá»©c nÄƒng nÃ y cho phÃ©p SpecInfer cÆ¡ há»™i giáº£i mÃ£ nhiá»u token (thay vÃ¬ má»™t token duy nháº¥t trong giáº£i mÃ£ tÄƒng dáº§n), dáº«n Ä‘áº¿n giáº£m truy cáº­p bá»™ nhá»› Ä‘áº¿n cÃ¡c tham sá»‘ cá»§a LLM. Má»™t thÃ¡ch thá»©c mÃ  SpecInfer pháº£i giáº£i quyáº¿t trong xÃ¡c minh cÃ¢y token lÃ  tÃ­nh toÃ¡n hiá»‡u quáº£ cÃ¡c Ä‘iá»ƒm attention cho táº¥t cáº£ cÃ¡c chuá»—i cá»§a má»™t cÃ¢y token. Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y, chÃºng tÃ´i giá»›i thiá»‡u tree attention, tá»•ng quÃ¡t hÃ³a cÆ¡ cháº¿ attention [48] tá»« chuá»—i sang cáº¥u trÃºc cÃ¢y. NgoÃ i ra, chÃºng tÃ´i phÃ¡t triá»ƒn má»™t cÆ¡ cháº¿ giáº£i mÃ£ song song dá»±a trÃªn cÃ¢y cÃ³ thá»ƒ giáº£i mÃ£ táº¥t cáº£ cÃ¡c token trong má»™t cÃ¢y token song song.

Â§4.1 vÃ  Â§4.2 mÃ´ táº£ tree attention vÃ  giáº£i mÃ£ song song dá»±a trÃªn cÃ¢y. Â§4.3 giá»›i thiá»‡u cÆ¡ cháº¿ Ä‘á»ƒ xÃ¡c minh má»™t cÃ¢y token so vá»›i Ä‘áº§u ra cá»§a LLM.

4.1 Tree Attention
CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ dá»±a trÃªn Transformer sá»­ dá»¥ng cÆ¡ cháº¿ attention Ä‘á»ƒ suy luáº­n vá» thÃ´ng tin tuáº§n tá»± [48]. CÃ¡c LLM thÆ°á»ng sá»­ dá»¥ng self-attention Ä‘a Ä‘áº§u chá»‰ decoder, nháº­n má»™t tensor Ä‘áº§u vÃ o ğ‘‹ duy nháº¥t vÃ  tÃ­nh toÃ¡n má»™t tensor Ä‘áº§u ra ğ‘‚ thÃ´ng qua cÃ¡c cÃ´ng thá»©c nhÃ¢n tá»· lá»‡ nhÆ° sau.

ğ‘„ğ‘–=ğ‘‹Ã—ğ‘Šğ‘„ğ‘–, ğ¾ğ‘–=ğ‘‹Ã—ğ‘Šğ¾ğ‘–, (1)
ğ‘‰ğ‘–=ğ‘‹Ã—ğ‘Šğ‘‰ğ‘–, ğ´ğ‘–=(ğ‘„ğ‘–Ã—ğ¾ğ‘‡ğ‘–)/âˆšğ‘‘, (2)
ğ»ğ‘–=softmaxmask(ğ´ğ‘–)ğ‘‰ğ‘–, ğ‘‚=(ğ»1,...,ğ»â„)ğ‘Šğ‘‚ (3)

trong Ä‘Ã³ ğ‘„ğ‘–,ğ¾ğ‘–, vÃ  ğ‘‰ğ‘– biá»ƒu thá»‹ cÃ¡c tensor query, key, vÃ  value cá»§a Ä‘áº§u attention thá»© ğ‘– (1â‰¤ğ‘–â‰¤â„), ğ‘Šğ‘„ğ‘–,ğ‘Šğ¾ğ‘–, vÃ  ğ‘Šğ‘‰ğ‘– lÃ  cÃ¡c ma tráº­n trá»ng sá»‘ tÆ°Æ¡ng á»©ng. ğ´ğ‘– lÃ  má»™t ma tráº­n ğ‘™Ã—ğ‘™ Ä‘áº¡i diá»‡n cho cÃ¡c Ä‘iá»ƒm attention giá»¯a cÃ¡c token khÃ¡c nhau trong chuá»—i Ä‘áº§u vÃ o, trong Ä‘Ã³ ğ‘™ lÃ  Ä‘á»™ dÃ i chuá»—i. Äá»ƒ báº£o toÃ n tÃ­nh nhÃ¢n quáº£ khi sinh ra token (tá»©c lÃ , má»™t token trong chuá»—i khÃ´ng nÃªn áº£nh hÆ°á»Ÿng Ä‘áº¿n tráº¡ng thÃ¡i áº©n cá»§a báº¥t ká»³ token Ä‘i trÆ°á»›c nÃ o), hÃ m mask nhÃ¢n quáº£ sau Ä‘Æ°á»£c Ã¡p dá»¥ng:

mask(ğ´)ğ‘—ğ‘˜={ğ´ğ‘—ğ‘˜ náº¿u ğ‘—â‰¥ğ‘˜, âˆ’âˆ náº¿u ğ‘—<ğ‘˜} (4)

Má»™t cÃ¡ch trá»±c quan, khi tÃ­nh toÃ¡n Ä‘áº§u ra attention cá»§a token thá»© ğ‘— trong chuá»—i, táº¥t cáº£ cÃ¡c token tiáº¿p theo nÃªn cÃ³ Ä‘iá»ƒm attention lÃ  âˆ’âˆ Ä‘á»ƒ chá»‰ ra ráº±ng cÃ¡c token tiáº¿p theo sáº½ khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n Ä‘áº§u ra attention cá»§a token thá»© ğ‘—2. Trong PhÆ°Æ¡ng trÃ¬nh 3, ğ»ğ‘– Ä‘áº¡i diá»‡n cho Ä‘áº§u ra cá»§a Ä‘áº§u attention thá»© ğ‘–, vÃ  ğ‘Šğ‘‚ lÃ  má»™t ma tráº­n trá»ng sá»‘ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh toÃ¡n Ä‘áº§u ra cuá»‘i cÃ¹ng cá»§a lá»›p attention.

LÆ°u Ã½ ráº±ng cÆ¡ cháº¿ attention Ä‘Æ°á»£c mÃ´ táº£ á»Ÿ trÃªn chá»‰ Ã¡p dá»¥ng cho má»™t chuá»—i token. ChÃºng tÃ´i tá»•ng quÃ¡t hÃ³a cÆ¡ cháº¿ attention cho cÃ¡c cáº¥u trÃºc cÃ¢y tÃ¹y Ã½.

Äá»‹nh nghÄ©a 4.1 (Tree Attention). Äá»‘i vá»›i má»™t cÃ¢y token ğ’© vÃ  má»™t nÃºt tÃ¹y Ã½ ğ‘¢âˆˆğ’©, tree attention cá»§a nÃ³ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a nhÆ° Ä‘áº§u ra cá»§a viá»‡c tÃ­nh toÃ¡n attention chuá»—i dá»±a trÃªn Transformer gá»‘c trÃªn ğ‘†ğ‘¢ (tá»©c lÃ , chuá»—i token Ä‘Æ°á»£c Ä‘áº¡i diá»‡n bá»Ÿi ğ‘¢):
TreeAttention(ğ‘¢)=Attention(ğ‘†ğ‘¢)âˆ€ğ‘¢âˆˆğ’© (5)

Äá»‘i vá»›i má»™t táº­p há»£p nháº¥t Ä‘á»‹nh cÃ¡c chuá»—i token, vÃ¬ má»—i chuá»—i ğ‘† Ä‘Æ°á»£c bao phá»§ bá»Ÿi má»™t nÃºt cá»§a cÃ¢y token Ä‘Ã£ há»£p nháº¥t, viá»‡c thá»±c hiá»‡n tree attention trÃªn cÃ¢y token cho phÃ©p SpecInfer thu Ä‘Æ°á»£c Ä‘áº§u ra attention cho táº¥t cáº£ cÃ¡c chuá»—i token.

4.2 Giáº£i mÃ£ song song dá»±a trÃªn cÃ¢y
Pháº§n nÃ y mÃ´ táº£ cÆ¡ cháº¿ giáº£i mÃ£ song song dá»±a trÃªn cÃ¢y cá»§a SpecInfer Ä‘á»ƒ tÃ­nh toÃ¡n tree attention cho táº¥t cáº£ cÃ¡c token trong má»™t cÃ¢y token song song. Má»™t thÃ¡ch thá»©c chÃ­nh mÃ  SpecInfer pháº£i giáº£i quyáº¿t trong viá»‡c tÃ­nh toÃ¡n tree attention lÃ  quáº£n lÃ½ key-value cache. Cá»¥ thá»ƒ, cÆ¡ cháº¿ attention cá»§a Transformer [48] Ä‘Ã²i há»i truy cáº­p cÃ¡c key vÃ  value cá»§a táº¥t cáº£ cÃ¡c token Ä‘i trÆ°á»›c Ä‘á»ƒ tÃ­nh toÃ¡n Ä‘áº§u ra attention cá»§a má»—i token má»›i, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong PhÆ°Æ¡ng trÃ¬nh 3. Äá»ƒ trÃ¡nh tÃ­nh toÃ¡n láº¡i nhá»¯ng key vÃ  value nÃ y, cÃ¡c há»‡ thá»‘ng suy luáº­n LLM ngÃ y nay thÆ°á»ng cache cÃ¡c key vÃ  value cá»§a táº¥t cáº£ cÃ¡c token Ä‘á»ƒ tÃ¡i sá»­ dá»¥ng trong cÃ¡c láº§n láº·p tÆ°Æ¡ng lai, vÃ¬ má»‘i quan há»‡ nhÃ¢n quáº£ Ä‘áº£m báº£o ráº±ng key vÃ  value cá»§a má»™t token váº«n khÃ´ng thay Ä‘á»•i trong cÃ¡c láº§n láº·p tiáº¿p theo (tá»©c lÃ , mask(A)jk=âˆ’âˆ Ä‘á»‘i vá»›i báº¥t ká»³ ğ‘—<ğ‘˜ nÃ o). Tuy nhiÃªn, khi tÃ­nh toÃ¡n tree attention, cÃ¡c chuá»—i khÃ¡c nhau trong má»™t cÃ¢y token cÃ³ thá»ƒ bao gá»“m cÃ¡c key-value cache xung Ä‘á»™t. VÃ­ dá»¥, Ä‘á»‘i vá»›i cÃ¢y token Ä‘Æ°á»£c suy Ä‘oÃ¡n trong HÃ¬nh 4, hai chuá»—i token (ğ‘¡2,ğ‘¡3,ğ‘¡4,ğ‘¡5) vÃ  (ğ‘¡2,ğ‘¡3,ğ‘¡8,ğ‘¡9) cÃ³ cÃ¡c key vÃ  value khÃ¡c nhau cho vá»‹ trÃ­ thá»© ba vÃ  thá»© tÆ°.

Má»™t cÃ¡ch tiáº¿p cáº­n Ä‘Æ¡n giáº£n Ä‘á»ƒ há»— trá»£ key-value cache lÃ  sá»­ dá»¥ng giáº£i mÃ£ dá»±a trÃªn chuá»—i cá»§a cÃ¡c há»‡ thá»‘ng suy luáº­n LLM hiá»‡n cÃ³ vÃ  sá»­ dá»¥ng má»™t key-value cache khÃ¡c nhau cho má»—i chuá»—i cá»§a má»™t cÃ¢y token, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ á»Ÿ bÃªn trÃ¡i HÃ¬nh 4. Tuy nhiÃªn, cÃ¡ch tiáº¿p cáº­n nÃ y ráº¥t tá»‘n kÃ©m vá» máº·t tÃ­nh toÃ¡n vÃ  liÃªn quan Ä‘áº¿n tÃ­nh toÃ¡n dÆ° thá»«a, vÃ¬ hai chuá»—i token chia sáº» má»™t tiá»n tá»‘ chung cÃ³ cÃ¹ng Ä‘áº§u ra attention cho tiá»n tá»‘ chung do mask nhÃ¢n quáº£ trong PhÆ°Æ¡ng trÃ¬nh 3. NgoÃ i ra, viá»‡c khá»Ÿi cháº¡y má»™t kernel cho má»—i chuá»—i token táº¡o ra chi phÃ­ khá»Ÿi cháº¡y kernel bá»• sung.

SpecInfer giá»›i thiá»‡u hai ká»¹ thuáº­t chÃ­nh Ä‘á»ƒ thá»±c hiá»‡n giáº£i mÃ£ song song dá»±a trÃªn cÃ¢y.

TÃ¬m kiáº¿m theo chiá»u sÃ¢u Ä‘á»ƒ cáº­p nháº­t key-value cache. Thay vÃ¬ cache cÃ¡c key vÃ  value cho cÃ¡c chuá»—i token riÃªng láº» cá»§a má»™t cÃ¢y token, SpecInfer tÃ¡i sá»­ dá»¥ng cÃ¹ng key-value cache trÃªn táº¥t cáº£ cÃ¡c chuá»—i token báº±ng cÃ¡ch táº­n dá»¥ng cÆ¡ cháº¿ tÃ¬m kiáº¿m theo chiá»u sÃ¢u Ä‘á»ƒ duyá»‡t cÃ¢y token, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 4, trong Ä‘Ã³ SpecInfer truy cáº­p ğ‘¡2,ğ‘¡3,...,ğ‘¡9 báº±ng cÃ¡ch tuÃ¢n theo thá»© tá»± chiá»u sÃ¢u Ä‘á»ƒ duyá»‡t cÃ¢y token vÃ  cáº­p nháº­t key-value cache chia sáº». CÃ¡ch tiáº¿p cáº­n nÃ y cho phÃ©p SpecInfer duy trÃ¬ cÃ¡c key vÃ  value Ä‘Ãºng cho táº¥t cáº£ cÃ¡c token Ä‘i trÆ°á»›c khi tÃ­nh toÃ¡n Ä‘áº§u ra attention cá»§a má»™t token má»›i.

Mask nhÃ¢n quáº£ nháº­n biáº¿t topology. Má»™t cÃ¡ch tiáº¿p cáº­n Ä‘Æ¡n giáº£n Ä‘á»ƒ tÃ­nh toÃ¡n tree attention lÃ  tÃ­nh toÃ¡n Ä‘áº§u ra tree attention cho cÃ¡c token riÃªng láº» báº±ng cÃ¡ch tuÃ¢n theo thá»© tá»± chiá»u sÃ¢u Ä‘Æ°á»£c mÃ´ táº£ trÆ°á»›c Ä‘Ã³. Tuy nhiÃªn, cÃ¡ch tiáº¿p cáº­n nÃ y sáº½ dáº«n Ä‘áº¿n chi phÃ­ khá»Ÿi cháº¡y kernel GPU cao vÃ¬ má»—i kernel chá»‰ tÃ­nh toÃ¡n tree attention cho má»™t chuá»—i token. NgoÃ i ra, viá»‡c thá»±c thi nhá»¯ng kernel nÃ y song song Ä‘Ã²i há»i bá»™ nhá»› GPU bá»• sung Ä‘á»ƒ lÆ°u trá»¯ key-value cache cá»§a chÃºng riÃªng biá»‡t do xung Ä‘á»™t cache. Má»™t thÃ¡ch thá»©c chÃ­nh ngÄƒn cáº£n SpecInfer gá»™p nhiá»u token lÃ  viá»‡c tÃ­nh toÃ¡n attention cho cÃ¡c token khÃ¡c nhau Ä‘Ã²i há»i cÃ¡c key-value cache khÃ¡c nhau vÃ  do Ä‘Ã³ khÃ´ng thá»ƒ Ä‘Æ°á»£c xá»­ lÃ½ song song.

ChÃºng tÃ´i giá»›i thiá»‡u mask nhÃ¢n quáº£ nháº­n biáº¿t topology Ä‘á»ƒ há»£p nháº¥t tÃ­nh toÃ¡n tree attention cá»§a táº¥t cáº£ cÃ¡c token trong má»™t kernel duy nháº¥t. Äá»ƒ gá»™p tÃ­nh toÃ¡n attention, SpecInfer sá»­ dá»¥ng topology cÃ¢y thay vÃ¬ topology chuá»—i gá»‘c Ä‘á»ƒ lÆ°u trá»¯ cÃ¡c key vÃ  value cá»§a táº¥t cáº£ cÃ¡c token trong má»™t cÃ¢y token trong key-value cache. VÃ­ dá»¥, Ä‘á»ƒ tÃ­nh toÃ¡n tree attention cho cÃ¢y token Ä‘Æ°á»£c suy Ä‘oÃ¡n Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 4, SpecInfer láº¥y cáº£ token Ä‘Ã£ Ä‘Æ°á»£c xÃ¡c minh (tá»©c lÃ , ğ‘¡2) vÃ  táº¥t cáº£ cÃ¡c token Ä‘Æ°á»£c suy Ä‘oÃ¡n (tá»©c lÃ , ğ‘¡3,ğ‘¡4,...,ğ‘¡9) lÃ m Ä‘áº§u vÃ o. CÃ¡ch tiáº¿p cáº­n nÃ y cho phÃ©p SpecInfer há»£p nháº¥t tÃ­nh toÃ¡n attention thÃ nh má»™t kernel duy nháº¥t nhÆ°ng cÅ©ng dáº«n Ä‘áº¿n cÃ¡c Ä‘iá»ƒm attention vi pháº¡m phá»¥ thuá»™c nhÃ¢n quáº£ (vÃ­ dá»¥, tÃ­nh toÃ¡n attention cá»§a ğ‘¡7 sá»­ dá»¥ng táº¥t cáº£ cÃ¡c token trÆ°á»›c Ä‘Ã³, bao gá»“m ğ‘¡5 khÃ´ng náº±m trong chuá»—i token cá»§a ğ‘¡7). Äá»ƒ sá»­a cÃ¡c Ä‘iá»ƒm attention cho nhá»¯ng cáº·p nÃ y, SpecInfer cáº­p nháº­t mask nhÃ¢n quáº£ dá»±a trÃªn topology cá»§a cÃ¢y token. CÃ¡ch tiáº¿p cáº­n nÃ y tÃ­nh toÃ¡n chÃ­nh xÃ¡c cÃ¹ng Ä‘áº§u ra attention nhÆ° giáº£i mÃ£ tÄƒng dáº§n, trong khi dáº«n Ä‘áº¿n Ã­t láº§n khá»Ÿi cháº¡y kernel hÆ¡n nhiá»u so vá»›i giáº£i mÃ£ dá»±a trÃªn chuá»—i.

4.3 XÃ¡c minh Token
Äá»‘i vá»›i má»™t cÃ¢y token Ä‘Æ°á»£c suy Ä‘oÃ¡n ğ’© nháº¥t Ä‘á»‹nh, SpecInfer sá»­ dá»¥ng giáº£i mÃ£ song song dá»±a trÃªn cÃ¢y (xem Pháº§n 4.2) Ä‘á»ƒ tÃ­nh toÃ¡n tree attention cá»§a nÃ³ vÃ  táº¡o ra má»™t tensor Ä‘áº§u ra ğ’ª bao gá»“m má»™t token cho má»—i nÃºt ğ‘¢âˆˆğ’©. Tiáº¿p theo, bá»™ xÃ¡c minh cÃ¢y token cá»§a SpecInfer kiá»ƒm tra tÃ­nh Ä‘Ãºng Ä‘áº¯n cá»§a cÃ¡c token Ä‘Æ°á»£c suy Ä‘oÃ¡n so vá»›i LLM. SpecInfer há»— trá»£ cáº£ láº¥y máº«u tham lam vÃ  ngáº«u nhiÃªn nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Thuáº­t toÃ¡n 2.

Giáº£i mÃ£ tham lam. Nhiá»u á»©ng dá»¥ng LLM sinh ra token báº±ng cÃ¡ch sá»­ dá»¥ng giáº£i mÃ£ tham lam, tham lam chá»n token cÃ³ kháº£ nÄƒng cao nháº¥t trong má»—i bÆ°á»›c giáº£i mÃ£. HÃ m VerifyGreedy trong Thuáº­t toÃ¡n 2 cho tháº¥y cÃ¡ch SpecInfer xÃ¡c minh má»™t cÃ¢y token Ä‘Æ°á»£c suy Ä‘oÃ¡n ğ’© vá»›i giáº£i mÃ£ tham lam. SpecInfer báº¯t Ä‘áº§u tá»« gá»‘c cá»§a ğ’© vÃ  láº·p láº¡i kiá»ƒm tra káº¿t quáº£ suy Ä‘oÃ¡n cá»§a má»™t nÃºt so vá»›i Ä‘áº§u ra gá»‘c cá»§a LLM. Äá»‘i vá»›i má»™t nÃºt ğ‘¢âˆˆğ’©, SpecInfer thÃ nh cÃ´ng suy Ä‘oÃ¡n token tiáº¿p theo cá»§a nÃ³ náº¿u ğ‘¢ bao gá»“m má»™t nÃºt con ğ‘£ (tá»©c lÃ , ğ‘ğ‘£=ğ‘¢) mÃ  token cá»§a nÃ³ khá»›p vá»›i Ä‘áº§u ra cá»§a LLM (tá»©c lÃ , ğ‘¡ğ‘£=ğ’ª(ğ‘¢)). Trong trÆ°á»ng há»£p nÃ y, SpecInfer hoÃ n thÃ nh xÃ¡c minh cá»§a nÃ³ cho nÃºt ğ‘¢ vÃ  chuyá»ƒn sang kiá»ƒm tra con ğ‘£ cá»§a nÃ³. Khi nÃºt ğ‘¢ khÃ´ng bao gá»“m má»™t con chá»©a Ä‘áº§u ra cá»§a LLM, SpecInfer thÃªm ğ’ª(ğ‘¢) nhÆ° má»™t nÃºt Ä‘Ã£ Ä‘Æ°á»£c xÃ¡c minh trong ğ’© vÃ  cháº¥m dá»©t quÃ¡ trÃ¬nh xÃ¡c minh. Cuá»‘i cÃ¹ng, táº¥t cáº£ cÃ¡c nÃºt Ä‘Ã£ Ä‘Æ°á»£c xÃ¡c minh Ä‘Æ°á»£c thÃªm vÃ o chuá»—i token Ä‘Æ°á»£c sinh ra hiá»‡n táº¡i ğ’±. XÃ¡c minh cÃ¢y token cho phÃ©p SpecInfer cÆ¡ há»™i giáº£i mÃ£ nhiá»u token (thay vÃ¬ má»™t token duy nháº¥t trong cÃ¡ch tiáº¿p cáº­n giáº£i mÃ£ tÄƒng dáº§n), trong khi báº£o toÃ n cÃ¹ng hiá»‡u suáº¥t sinh táº¡o nhÆ° giáº£i mÃ£ tÄƒng dáº§n.

Giáº£i mÃ£ ngáº«u nhiÃªn. Äá»ƒ cáº£i thiá»‡n sá»± Ä‘a dáº¡ng cá»§a cÃ¡c token Ä‘Æ°á»£c sinh ra, nhiá»u á»©ng dá»¥ng LLM thá»±c hiá»‡n giáº£i mÃ£ ngáº«u nhiÃªn, láº¥y máº«u má»™t token tá»« má»™t phÃ¢n phá»‘i xÃ¡c suáº¥t ğ‘ƒ(ğ‘¢ğ‘–|ğ‘¢0,...,ğ‘¢ğ‘–âˆ’1;Î˜ğ¿ğ¿ğ‘€), trong Ä‘Ã³ ğ‘ˆ=ğ‘¢0,...,ğ‘¢ğ‘–âˆ’1 lÃ  cÃ¡c token Ä‘Æ°á»£c sinh ra trÆ°á»›c Ä‘Ã³, ğ‘¢ğ‘– lÃ  token tiáº¿p theo Ä‘á»ƒ sinh ra, vÃ  Î˜ğ¿ğ¿ğ‘€ Ä‘áº¡i diá»‡n cho má»™t LLM Ä‘Æ°á»£c tham sá»‘ hÃ³a.

Äá»ƒ xÃ¡c minh má»™t cÃ¢y token Ä‘Æ°á»£c suy Ä‘oÃ¡n vá»›i giáº£i mÃ£ ngáº«u nhiÃªn, chÃºng tÃ´i giá»›i thiá»‡u má»™t thuáº­t toÃ¡n láº¥y máº«u suy Ä‘oÃ¡n Ä‘a bÆ°á»›c (MSS) Ä‘á»ƒ tiáº¿n hÃ nh xÃ¡c minh, mÃ£ giáº£ cá»§a nÃ³ Ä‘Æ°á»£c hiá»ƒn thá»‹ trong hÃ m VerifyStochastic trong Thuáº­t toÃ¡n 2 vÃ  Ä‘Æ°á»£c minh há»a trong HÃ¬nh 5. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i cÃ³ thá»ƒ chá»©ng minh báº£o toÃ n hiá»‡u suáº¥t sinh táº¡o cá»§a LLM nhÆ° giáº£i mÃ£ tÄƒng dáº§n trong khi tá»‘i Æ°u hÃ³a sá»‘ lÆ°á»£ng token Ä‘Æ°á»£c suy Ä‘oÃ¡n cÃ³ thá»ƒ Ä‘Æ°á»£c xÃ¡c minh. Äá»‹nh lÃ½ 4.2 chá»©ng minh tÃ­nh Ä‘Ãºng Ä‘áº¯n cá»§a nÃ³.

Äá»‹nh lÃ½ 4.2. Äá»‘i vá»›i má»™t ğ¿ğ¿ğ‘€ vÃ  ğ‘š SSM cho trÆ°á»›c (tá»©c lÃ , ğ‘†ğ‘†ğ‘€1,...,ğ‘†ğ‘†ğ‘€ğ‘š), Ä‘áº·t ğ‘ƒ(ğ‘¢ğ‘–|ğ‘ˆ;Î˜ğ¿ğ¿ğ‘€) lÃ  phÃ¢n phá»‘i xÃ¡c suáº¥t cá»§a viá»‡c láº¥y máº«u má»™t token báº±ng cÃ¡ch sá»­ dá»¥ng giáº£i mÃ£ ngáº«u nhiÃªn, trong Ä‘Ã³ ğ‘ˆ=ğ‘¢0,...,ğ‘¢ğ‘–âˆ’1 lÃ  cÃ¡c token Ä‘Æ°á»£c sinh ra trÆ°á»›c Ä‘Ã³, ğ‘¢ğ‘– lÃ  token tiáº¿p theo Ä‘á»ƒ sinh ra, Î˜ğ¿ğ¿ğ‘€ Ä‘áº¡i diá»‡n cho LLM Ä‘Æ°á»£c tham sá»‘ hÃ³a.

Äáº·t ğ‘ƒSpecInfer(ğ‘¢ğ‘–|ğ‘ˆ;Î˜ğ¿ğ¿ğ‘€,{Î˜ğ‘†ğ‘†ğ‘€ğ‘—}) lÃ  phÃ¢n phá»‘i xÃ¡c suáº¥t cá»§a viá»‡c láº¥y máº«u token ğ‘¢ğ‘– báº±ng cÃ¡ch sá»­ dá»¥ng láº¥y máº«u suy Ä‘oÃ¡n Ä‘a bÆ°á»›c cá»§a SpecInfer (xem hÃ m VerifyStochastic trong Thuáº­t toÃ¡n 2), trong Ä‘Ã³ Î˜ğ‘†ğ‘†ğ‘€ğ‘— lÃ  SSM Ä‘Æ°á»£c tham sá»‘ hÃ³a thá»© ğ‘—.

ThÃ¬ âˆ€ğ‘ˆ,ğ‘¢ğ‘–,Î˜ğ¿ğ¿ğ‘€,Î˜ğ‘†ğ‘†ğ‘€ğ‘— chÃºng ta cÃ³
ğ‘ƒ(ğ‘¢ğ‘–|ğ‘ˆ;Î˜ğ¿ğ¿ğ‘€)=ğ‘ƒSpecInfer(ğ‘¢ğ‘–|ğ‘ˆ;Î˜ğ¿ğ¿ğ‘€,{Î˜ğ‘†ğ‘†ğ‘€ğ‘—}) (6)

Má»™t chá»©ng minh cá»§a Ä‘á»‹nh lÃ½ nÃ y Ä‘Æ°á»£c trÃ¬nh bÃ y trong [28].

ChÃºng tÃ´i thá»«a nháº­n ráº±ng má»™t cÃ¡ch tiáº¿p cáº­n Ä‘Æ¡n giáº£n hÆ¡n Ä‘á»ƒ báº£o toÃ n phÃ¢n phá»‘i xÃ¡c suáº¥t cá»§a giáº£i mÃ£ ngáº«u nhiÃªn lÃ  trá»±c tiáº¿p láº¥y máº«u token tiáº¿p theo ğ‘¥âˆ¼ğ‘ƒ(ğ‘¢ğ‘–|ğ‘ˆ;Î˜ğ¿ğ¿ğ‘€) vÃ  kiá»ƒm tra xem ğ‘¥ cÃ³ pháº£i lÃ  má»™t nÃºt con cá»§a ğ‘¢ğ‘–âˆ’1 trong cÃ¢y token Ä‘Æ°á»£c suy Ä‘oÃ¡n hay khÃ´ng. ChÃºng tÃ´i gá»i cÃ¡ch tiáº¿p cáº­n nÃ y lÃ  láº¥y máº«u ngÃ¢y thÆ¡ (NS) vÃ  cho tháº¥y ráº±ng láº¥y máº«u suy Ä‘oÃ¡n Ä‘a bÆ°á»›c cá»§a SpecInfer cÃ³ xÃ¡c suáº¥t tá»« chá»‘i tháº¥p hÆ¡n Ä‘á»u Ä‘áº·n so vá»›i láº¥y máº«u ngÃ¢y thÆ¡.

Äá»‹nh lÃ½ 4.3. Äáº·t ğ‘ƒreject|MSS,ğ‘ˆ,Î˜LLM,{Î˜SSMğ‘—} biá»ƒu thá»‹ xÃ¡c suáº¥t tá»« chá»‘i suy Ä‘oÃ¡n tuÃ¢n theo láº¥y máº«u suy Ä‘oÃ¡n Ä‘a bÆ°á»›c vá»›i viáº¿t táº¯t ğ‘ƒ(reject|MSS), vÃ  ğ‘ƒreject|NS,ğ‘ˆ,Î˜LLM,{Î˜SSMğ‘—} xÃ¡c suáº¥t tá»« chá»‘i suy Ä‘oÃ¡n tuÃ¢n theo Láº¥y máº«u NgÃ¢y thÆ¡ (NS) vá»›i viáº¿t táº¯t ğ‘ƒ(reject|NS). ThÃ¬ âˆ€ğ‘ˆ,Î˜LLM,{Î˜SSMğ‘—}, chÃºng ta cÃ³

ğ‘ƒ(reject|MSS)â‰¤ğ‘ƒ(reject|NS)

ChÃºng tÃ´i trÃ¬nh bÃ y má»™t chá»©ng minh cá»§a Äá»‹nh lÃ½ 4.3 trong [28].

LÆ°u Ã½ ráº±ng cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y Ä‘Ã£ giá»›i thiá»‡u láº¥y máº«u suy Ä‘oÃ¡n má»™t bÆ°á»›c cho suy luáº­n suy Ä‘oÃ¡n dá»±a trÃªn chuá»—i [5, 25]. KhÃ¡c vá»›i nhá»¯ng cÃ¡ch tiáº¿p cáº­n nÃ y, SpecInfer táº­n dá»¥ng cÃ¢y token Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t suy Ä‘oÃ¡n, Ä‘iá»u nÃ y Ä‘Ã²i há»i má»™t thuáº­t toÃ¡n xÃ¡c minh khÃ¡c. Káº¿t quáº£ lÃ , SpecInfer thá»±c hiá»‡n xÃ¡c minh Ä‘a bÆ°á»›c (xem VerifyStochastic trong Thuáº­t toÃ¡n 2) trÃªn táº¥t cáº£ cÃ¡c nhÃ¡nh cá»§a má»™t token Ä‘á»ƒ tá»‘i Ä‘a hÃ³a tá»· lá»‡ thÃ nh cÃ´ng trong khi báº£o toÃ n tÆ°Æ¡ng Ä‘Æ°Æ¡ng nhÆ° giáº£i mÃ£ tÄƒng dáº§n. Thuáº­t toÃ¡n MSS Ä‘Æ°á»£c Ä‘á» xuáº¥t khÃ´ng chá»‰ hoáº¡t Ä‘á»™ng cho phÆ°Æ¡ng phÃ¡p dá»±a trÃªn há»£p nháº¥t vá»›i nhiá»u SSM, mÃ  cÃ²n há»— trá»£ phÆ°Æ¡ng phÃ¡p dá»±a trÃªn má»Ÿ rá»™ng vá»›i má»™t SSM vÃ  láº¥y máº«u top-ğ‘˜.

5 Thiáº¿t káº¿ vÃ  Triá»ƒn khai Há»‡ thá»‘ng
Pháº§n nÃ y mÃ´ táº£ thiáº¿t káº¿ vÃ  triá»ƒn khai cá»§a há»‡ thá»‘ng runtime phÃ¢n tÃ¡n cá»§a SpecInfer (Â§5.1 vÃ  Â§5.2), phÃ¢n tÃ­ch chi phÃ­ tÃ­nh toÃ¡n vÃ  bá»™ nhá»› cá»§a suy Ä‘oÃ¡n vÃ  xÃ¡c minh (Â§5.3), vÃ  giá»›i thiá»‡u cÃ¡c á»©ng dá»¥ng LLM tiá»m nÄƒng cÃ³ thá»ƒ hÆ°á»Ÿng lá»£i tá»« cÃ¡c ká»¹ thuáº­t cá»§a SpecInfer (Â§5.4).

5.1 Thiáº¿t káº¿ Runtime cá»§a SpecInfer
HÃ¬nh 6 cho tháº¥y quy trÃ¬nh lÃ m viá»‡c cho má»™t láº§n láº·p cá»§a suy luáº­n suy Ä‘oÃ¡n vÃ  xÃ¡c minh. TrÃ¬nh quáº£n lÃ½ yÃªu cáº§u cá»§a SpecInfer nháº­n cÃ¡c yÃªu cáº§u phá»¥c vá»¥ LLM vÃ  lÃªn lá»‹ch nhá»¯ng yÃªu cáº§u nÃ y Ä‘á»ƒ phá»¥c vá»¥ báº±ng cÃ¡ch thÃ­ch á»©ng chÃ­nh sÃ¡ch lÃªn lá»‹ch cáº¥p Ä‘á»™ láº§n láº·p tá»« Orca [55]. Cá»¥ thá»ƒ, SpecInfer láº·p láº¡i chá»n cÃ¡c yÃªu cáº§u tá»« má»™t pool cÃ¡c yÃªu cáº§u Ä‘ang chá» vÃ  thá»±c hiá»‡n má»™t láº§n láº·p suy luáº­n suy Ä‘oÃ¡n vÃ  xÃ¡c minh cÃ¢y token cho cÃ¡c yÃªu cáº§u Ä‘Æ°á»£c chá»n. VÃ¬ cÃ¡c SSM nhá» vÃ  cÃ³ thá»ƒ vá»«a trong má»™t GPU, SpecInfer phÃ¢n phá»‘i Ä‘á»u cÃ¡c GPU trÃªn cÃ¡c SSM vÃ  phá»¥c vá»¥ nhá»¯ng SSM nÃ y báº±ng cÃ¡ch sá»­ dá»¥ng song song dá»¯ liá»‡u. VÃ­ dá»¥, HÃ¬nh 6 cho tháº¥y cÃ¡ch SpecInfer phá»¥c vá»¥ hai SSM vÃ  bá»‘n yÃªu cáº§u (tá»©c lÃ , ğ‘Ÿ1,ğ‘Ÿ2,ğ‘Ÿ3, vÃ  ğ‘Ÿ4) trÃªn bá»‘n GPU. CÃ¡c token Ä‘Æ°á»£c táº¡o ra bá»Ÿi SSM Ä‘Æ°á»£c gá»­i trá»Ÿ láº¡i trÃ¬nh quáº£n lÃ½ yÃªu cáº§u, táº¡o ra má»™t cÃ¢y token Ä‘Æ°á»£c suy Ä‘oÃ¡n cho má»—i yÃªu cáº§u báº±ng cÃ¡ch sá»­ dá»¥ng thuáº­t toÃ¡n há»£p nháº¥t cÃ¢y Ä‘Æ°á»£c giá»›i thiá»‡u trong Â§4.

SpecInfer phá»¥c vá»¥ má»™t LLM báº±ng cÃ¡ch sá»­ dá»¥ng chiáº¿n lÆ°á»£c song song hÃ³a lai Ä‘Æ°á»£c giá»›i thiá»‡u trong Megatron-LM [41], sá»­ dá»¥ng song song mÃ´ hÃ¬nh tensor Ä‘á»ƒ song song hÃ³a má»—i lá»›p Transformer trÃªn cÃ¡c GPU trong má»™t nÃºt, vÃ  sá»­ dá»¥ng song song mÃ´ hÃ¬nh pipeline Ä‘á»ƒ phÃ¢n chia cÃ¡c lá»›p Transformer trÃªn cÃ¡c nÃºt. Táº¥t cáº£ cÃ¡c GPU thá»±c hiá»‡n giáº£i mÃ£ song song dá»±a trÃªn cÃ¢y (xem Â§4.2) Ä‘á»ƒ tÃ­nh toÃ¡n cÃ¡c Ä‘iá»ƒm tree attention vÃ  gá»­i cÃ¡c token Ä‘Æ°á»£c táº¡o ra bá»Ÿi LLM trá»Ÿ láº¡i trÃ¬nh quáº£n lÃ½ yÃªu cáº§u, cuá»‘i cÃ¹ng xÃ¡c minh cÃ¡c token Ä‘Æ°á»£c suy Ä‘oÃ¡n so vá»›i Ä‘áº§u ra cá»§a LLM (xem Â§4.3).

LÆ°u Ã½ ráº±ng chi phÃ­ Ä‘Æ°á»£c giá»›i thiá»‡u bá»Ÿi trÃ¬nh quáº£n lÃ½ yÃªu cáº§u (tá»©c lÃ , lÃªn lá»‹ch yÃªu cáº§u, há»£p nháº¥t cÃ¢y token, vÃ  xÃ¡c minh) lÃ  khÃ´ng Ä‘Ã¡ng ká»ƒ so vá»›i thá»i gian thá»±c thi cá»§a suy luáº­n LLM. NgoÃ i ra, trÃ¬nh quáº£n lÃ½ yÃªu cáº§u vÃ  cÃ¡c worker GPU cá»§a SpecInfer chá»‰ giao tiáº¿p token vÃ  khÃ´ng truyá»n cÃ¡c biá»ƒu diá»…n vector cá»§a nhá»¯ng token nÃ y, Ä‘iá»u nÃ y má»™t láº§n ná»¯a táº¡o ra chi phÃ­ giao tiáº¿p khÃ´ng Ä‘Ã¡ng ká»ƒ.

Batching liÃªn tá»¥c. SpecInfer sá»­ dá»¥ng batching liÃªn tá»¥c Ä‘Æ°á»£c giá»›i thiá»‡u trong Orca [55] Ä‘á»ƒ phá»¥c vá»¥ nhiá»u yÃªu cáº§u suy luáº­n LLM song song. Cá»¥ thá»ƒ, SpecInfer lÃªn lá»‹ch thá»±c thi LLM á»Ÿ má»©c Ä‘á»™ chi tiáº¿t cá»§a cÃ¡c láº§n láº·p thay vÃ¬ cÃ¡c yÃªu cáº§u. Sau má»—i láº§n láº·p giáº£i mÃ£ LLM, SpecInfer kiá»ƒm tra tráº¡ng thÃ¡i cá»§a má»—i yÃªu cáº§u vÃ  gá»­i káº¿t quáº£ Ä‘Æ°á»£c táº¡o ra cá»§a táº¥t cáº£ cÃ¡c yÃªu cáº§u Ä‘Ã£ hoÃ n thÃ nh Ä‘áº¿n client. Thiáº¿t káº¿ nÃ y cÅ©ng cho phÃ©p SpecInfer báº¯t Ä‘áº§u xá»­ lÃ½ cÃ¡c yÃªu cáº§u má»›i Ä‘áº¿n mÃ  khÃ´ng cáº§n chá» táº¥t cáº£ cÃ¡c yÃªu cáº§u hiá»‡n táº¡i hoÃ n thÃ nh.

5.2 Triá»ƒn khai cá»§a SpecInfer
SpecInfer Ä‘Æ°á»£c triá»ƒn khai trÃªn FlexFlow [21,47], má»™t runtime Ä‘a GPU phÃ¢n tÃ¡n cho tÃ­nh toÃ¡n DNN. FlexFlow tiáº¿t lá»™ má»™t API cho phÃ©p ngÆ°á»i dÃ¹ng Ä‘á»‹nh nghÄ©a má»™t mÃ´ hÃ¬nh DNN theo cÃ¡c lá»›p cá»§a nÃ³. NÃ³ tÆ°Æ¡ng thÃ­ch vá»›i Ä‘á»‹nh nghÄ©a mÃ´ hÃ¬nh cá»§a PyTorch do sá»± phÃ¹ há»£p cá»§a cÃ¡c toÃ¡n tá»­ cÆ¡ báº£n. VÃ­ dá»¥, cÃ¡c LLM mÃ£ nguá»“n má»Ÿ tá»« HuggingFace [19] cÃ³ thá»ƒ Ä‘Æ°á»£c nháº­p trá»±c tiáº¿p vÃ o SpecInfer Ä‘á»ƒ phá»¥c vá»¥ mÃ  khÃ´ng cáº§n sá»­a Ä‘á»•i. NgÆ°á»i dÃ¹ng cÅ©ng cÃ³ thá»ƒ cung cáº¥p má»™t chiáº¿n lÆ°á»£c song song hÃ³a, chá»‰ Ä‘á»‹nh má»©c Ä‘á»™ song song dá»¯ liá»‡u, mÃ´ hÃ¬nh, vÃ  pipeline cho má»—i lá»›p. Má»™t DNN Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° má»™t Ä‘á»“ thá»‹ tÃ­nh toÃ¡n trong Ä‘Ã³ má»—i nÃºt lÃ  má»™t vÃ¹ng bá»™ nhá»›, vÃ  má»—i cáº¡nh lÃ  má»™t hoáº¡t Ä‘á»™ng trÃªn má»™t hoáº·c nhiá»u vÃ¹ng. CÃ¡c hoáº¡t Ä‘á»™ng cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng ba má»©c Ä‘á»™ trá»«u tÆ°á»£ng: lá»›p, toÃ¡n tá»­, vÃ  tÃ¡c vá»¥. TrÃ¬nh biÃªn dá»‹ch FlexFlow biáº¿n Ä‘á»•i Ä‘á»“ thá»‹ tÃ­nh toÃ¡n tá»« má»©c trá»«u tÆ°á»£ng cao nháº¥t (tá»©c lÃ , lá»›p) Ä‘áº¿n tháº¥p nháº¥t (tá»©c lÃ , tÃ¡c vá»¥). CÃ¡c tÃ¡c vá»¥ cÅ©ng lÃ  Ä‘Æ¡n vá»‹ cá»§a song song hÃ³a; chÃºng khÃ´ng thá»ƒ bá»‹ ngáº¯t, vÃ  Ä‘Æ°á»£c thá»±c thi báº¥t Ä‘á»“ng bá»™.

Tá»‘i Æ°u hÃ³a kernel CUDA. Viá»‡c khá»Ÿi cháº¡y trá»±c tiáº¿p cÃ¡c kernel cuBLAS vÃ  cuDNN Ä‘á»ƒ tÃ­nh toÃ¡n attention dáº«n Ä‘áº¿n chi phÃ­ khá»Ÿi cháº¡y kernel cao vÃ  khÃ´ng táº­n dá»¥ng bá»™ nhá»› chia sáº» cÃ³ sáºµn trÃªn cÃ¡c GPU hiá»‡n Ä‘áº¡i. Äá»ƒ giáº£i quyáº¿t sá»± kÃ©m hiá»‡u quáº£ nÃ y, SpecInfer sá»­ dá»¥ng má»™t kernel tÃ¹y chá»‰nh Ä‘Æ°á»£c xÃ¢y dá»±ng trÃªn FasterTransformer [32] Ä‘á»ƒ tÃ­nh toÃ¡n attention. Trong kernel nÃ y, má»—i thread block tÃ­nh toÃ¡n má»™t Ä‘áº§u duy nháº¥t cho má»™t yÃªu cáº§u duy nháº¥t. QuÃ¡ trÃ¬nh báº¯t Ä‘áº§u vá»›i viá»‡c táº£i tensor query vÃ o bá»™ nhá»› chia sáº» GPU cÃ³ thá»ƒ truy cáº­p bá»Ÿi táº¥t cáº£ cÃ¡c thread trong má»™t thread block. Sau Ä‘Ã³ má»—i thread thá»±c hiá»‡n má»™t Ä‘oáº¡n cá»§a tÃ­ch query/key vÃ  phÃ¡t sÃ³ng káº¿t quáº£ Ä‘áº¿n cÃ¡c thread khÃ¡c Ä‘á»ƒ tÃ­nh toÃ¡n tÃ­ch query/key tá»‘i Ä‘a vÃ  tá»•ng exponential. Äá»ƒ há»— trá»£ giáº£i mÃ£ song song dá»±a trÃªn cÃ¢y, SpecInfer tÃ­nh toÃ¡n táº¥t cáº£ cÃ¡c token trong má»™t cÃ¢y song song vÃ  táº­n dá»¥ng mask nhÃ¢n quáº£ nháº­n biáº¿t topology Ä‘á»ƒ báº£o toÃ n tÃ­nh nhÃ¢n quáº£.

5.3 Chi phÃ­ cá»§a Suy Ä‘oÃ¡n vÃ  XÃ¡c minh
SpecInfer tÄƒng tá»‘c suy luáº­n LLM sinh táº¡o vá»›i chi phÃ­ cá»§a chi phÃ­ bá»™ nhá»› vÃ  tÃ­nh toÃ¡n bá»• sung. Pháº§n nÃ y phÃ¢n tÃ­ch nhá»¯ng chi phÃ­ nÃ y vÃ  cho tháº¥y chÃºng thÆ°á»ng nhá» hÆ¡n má»™t hoáº·c hai báº­c Ä‘á»™ lá»›n so vá»›i chi phÃ­ bá»™ nhá»› vÃ  tÃ­nh toÃ¡n cá»§a viá»‡c thá»±c thi suy luáº­n LLM.

Chi phÃ­ bá»™ nhá»›. Chi phÃ­ bá»™ nhá»› cá»§a cÃ¡ch tiáº¿p cáº­n suy Ä‘oÃ¡n-xÃ¡c minh cá»§a SpecInfer Ä‘áº¿n tá»« hai khÃ­a cáº¡nh. Äáº§u tiÃªn, ngoÃ i viá»‡c phá»¥c vá»¥ má»™t LLM, SpecInfer cÅ©ng cáº§n phÃ¢n bá»• bá»™ nhá»› Ä‘á»ƒ lÆ°u cÃ¡c tham sá»‘ cá»§a má»™t hoáº·c nhiá»u SSM, cÃ¹ng nhau suy Ä‘oÃ¡n Ä‘áº§u ra cá»§a LLM. ÄÃ¡nh giÃ¡ cá»§a chÃºng tÃ´i cho tháº¥y SpecInfer cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c cáº£i thiá»‡n hiá»‡u suáº¥t Ä‘Ã¡ng ká»ƒ báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c SSM nhá» hÆ¡n 100-1000Ã— so vá»›i LLM. Káº¿t quáº£ lÃ , viá»‡c lÆ°u trá»¯ má»—i SSM lÃ m tÄƒng yÃªu cáº§u bá»™ nhá»› tá»•ng thá»ƒ Ã­t hÆ¡n 1%. Má»™t nguá»“n chi phÃ­ bá»™ nhá»› thá»© hai Ä‘áº¿n tá»« engine xÃ¡c minh cÃ¢y token, xÃ¡c minh toÃ n bá»™ cÃ¢y token thay vÃ¬ giáº£i mÃ£ má»™t token duy nháº¥t. Do Ä‘Ã³, bá»™ nhá»› bá»• sung cáº§n thiáº¿t Ä‘á»ƒ cache cÃ¡c key vÃ  value, vÃ  lÆ°u trá»¯ cÃ¡c Ä‘iá»ƒm attention cho táº¥t cáº£ cÃ¡c token. Do tÃ­nh cáº§n thiáº¿t Ä‘á»ƒ há»— trá»£ Ä‘á»™ dÃ i chuá»—i ráº¥t dÃ i trong viá»‡c phá»¥c vá»¥ LLM ngÃ y nay, chÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng chi phÃ­ bá»™ nhá»› liÃªn quan Ä‘áº¿n cÃ¢y token lÃ  khÃ´ng Ä‘Ã¡ng ká»ƒ so vá»›i key-value cache.

Chi phÃ­ tÃ­nh toÃ¡n. TÆ°Æ¡ng tá»±, chi phÃ­ tÃ­nh toÃ¡n Ä‘Æ°á»£c giá»›i thiá»‡u bá»Ÿi suy Ä‘oÃ¡n vÃ  xÃ¡c minh cÅ©ng Ä‘áº¿n tá»« hai khÃ­a cáº¡nh. Äáº§u tiÃªn, SpecInfer cáº§n cháº¡y cÃ¡c SSM trong cháº¿ Ä‘á»™ giáº£i mÃ£ tÄƒng dáº§n Ä‘á»ƒ táº¡o ra cÃ¡c token á»©ng viÃªn. Khi nhiá»u SSM Ä‘Æ°á»£c sá»­ dá»¥ng, SpecInfer xá»­ lÃ½ nhá»¯ng SSM nÃ y song song trÃªn cÃ¡c GPU Ä‘á»ƒ tá»‘i thiá»ƒu hÃ³a Ä‘á»™ trá»… suy Ä‘oÃ¡n. Thá»© hai, SpecInfer xÃ¡c minh má»™t cÃ¢y token báº±ng cÃ¡ch tÃ­nh toÃ¡n cÃ¡c Ä‘áº§u ra attention cho toÃ n bá»™ cÃ¢y token, háº§u háº¿t trong sá»‘ Ä‘Ã³ khÃ´ng khá»›p vá»›i Ä‘áº§u ra cá»§a LLM vÃ  do Ä‘Ã³ lÃ  khÃ´ng cáº§n thiáº¿t trong suy luáº­n giáº£i mÃ£ tÄƒng dáº§n. Tuy nhiÃªn, cÆ¡ cháº¿ key-value cache cá»§a cÃ¡c há»‡ thá»‘ng suy luáº­n LLM hiá»‡n cÃ³ ngÄƒn cáº£n chÃºng phá»¥c vá»¥ má»™t sá»‘ lÆ°á»£ng lá»›n yÃªu cáº§u song song, dáº«n Ä‘áº¿n tÃ i nguyÃªn tÃ­nh toÃ¡n chÆ°a Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘áº§y Ä‘á»§ trÃªn cÃ¡c GPU khi phá»¥c vá»¥ LLM trong giáº£i mÃ£ tÄƒng dáº§n. XÃ¡c minh cÃ¢y token cá»§a SpecInfer táº­n dá»¥ng nhá»¯ng tÃ i nguyÃªn chÆ°a Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘áº§y Ä‘á»§ nÃ y vÃ  do Ä‘Ã³ táº¡o ra chi phÃ­ runtime khÃ´ng Ä‘Ã¡ng ká»ƒ so vá»›i giáº£i mÃ£ tÄƒng dáº§n.

5.4 á»¨ng dá»¥ng
CÃ¡c ká»¹ thuáº­t suy luáº­n suy Ä‘oÃ¡n vÃ  xÃ¡c minh cÃ¢y token cá»§a chÃºng tÃ´i cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng trá»±c tiáº¿p cho nhiá»u á»©ng dá»¥ng LLM khÃ¡c nhau. ChÃºng tÃ´i xÃ¡c Ä‘á»‹nh hai tÃ¬nh huá»‘ng thá»±c táº¿ mÃ  suy luáº­n LLM cÃ³ thá»ƒ hÆ°á»Ÿng lá»£i Ä‘Ã¡ng ká»ƒ tá»« cÃ¡c ká»¹ thuáº­t cá»§a chÃºng tÃ´i.

Suy luáº­n LLM phÃ¢n tÃ¡n. YÃªu cáº§u bá»™ nhá»› cá»§a cÃ¡c LLM hiá»‡n Ä‘áº¡i vÆ°á»£t quÃ¡ kháº£ nÄƒng cá»§a má»™t nÃºt tÃ­nh toÃ¡n duy nháº¥t vá»›i má»™t hoáº·c nhiá»u GPU, vÃ  cÃ¡ch tiáº¿p cáº­n hiá»‡n táº¡i Ä‘á»ƒ giáº£i quyáº¿t yÃªu cáº§u bá»™ nhá»› cao lÃ  phÃ¢n phá»‘i cÃ¡c tham sá»‘ cá»§a LLM trÃªn nhiá»u GPU [29]. VÃ­ dá»¥, viá»‡c phá»¥c vá»¥ má»™t pipeline suy luáº­n duy nháº¥t cho GPT-3 vá»›i 175 tá»· tham sá»‘ Ä‘Ã²i há»i hÆ¡n 16 GPU NVIDIA A100-40GB Ä‘á»ƒ lÆ°u trá»¯ cÃ¡c tham sá»‘ mÃ´ hÃ¬nh á»Ÿ dáº¡ng sá»‘ thá»±c Ä‘á»™ chÃ­nh xÃ¡c Ä‘Æ¡n. Suy luáº­n LLM phÃ¢n tÃ¡n pháº§n lá»›n bá»‹ háº¡n cháº¿ bá»Ÿi Ä‘á»™ trá»… Ä‘á»ƒ truyá»n cÃ¡c kÃ­ch hoáº¡t trung gian giá»¯a cÃ¡c GPU cho má»—i bÆ°á»›c giáº£i mÃ£ LLM. Máº·c dÃ¹ cÃ¡ch tiáº¿p cáº­n cá»§a SpecInfer khÃ´ng trá»±c tiáº¿p giáº£m lÆ°á»£ng giao tiáº¿p giá»¯a cÃ¡c GPU, cÆ¡ cháº¿ xÃ¡c minh cá»§a nÃ³ cÃ³ thá»ƒ tÄƒng Ä‘á»™ chi tiáº¿t giao tiáº¿p vÃ  giáº£m sá»‘ bÆ°á»›c giáº£i mÃ£.

Suy luáº­n LLM dá»±a trÃªn offloading. Má»™t tÃ¬nh huá»‘ng thá»±c táº¿ khÃ¡c cÃ³ thá»ƒ hÆ°á»Ÿng lá»£i tá»« cÃ¡c ká»¹ thuáº­t cá»§a SpecInfer lÃ  suy luáº­n LLM dá»±a trÃªn offloading, táº­n dá»¥ng CPU DRAM Ä‘á»ƒ lÆ°u trá»¯ cÃ¡c tham sá»‘ cá»§a LLM vÃ  táº£i má»™t táº­p há»£p con cá»§a nhá»¯ng tham sá»‘ nÃ y vÃ o GPU Ä‘á»ƒ tÃ­nh toÃ¡n theo cÃ¡ch pipeline [40]. Báº±ng cÃ¡ch cÆ¡ há»™i xÃ¡c minh nhiá»u token, SpecInfer cÃ³ thá»ƒ giáº£m sá»‘ bÆ°á»›c giáº£i mÃ£ LLM vÃ  giao tiáº¿p tá»•ng thá»ƒ giá»¯a CPU DRAM vÃ  GPU HBM.

6 ÄÃ¡nh giÃ¡
6.1 Thiáº¿t láº­p Thá»±c nghiá»‡m
LLM. Äá»ƒ so sÃ¡nh hiá»‡u suáº¥t runtime cá»§a SpecInfer vá»›i cÃ¡c há»‡ thá»‘ng phá»¥c vá»¥ LLM hiá»‡n cÃ³, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ nhá»¯ng há»‡ thá»‘ng nÃ y báº±ng cÃ¡ch sá»­ dá»¥ng hai há» LLM cÃ³ sáºµn cÃ´ng khai: OPT [57] vÃ  LLaMA [46]. Cá»¥ thá»ƒ hÆ¡n, chÃºng tÃ´i chá»n LLaMA-7B, OPT-13B, OPT-30B, vÃ  LLaMA-65B lÃ m LLM, vÃ  LLaMA-68M vÃ  OPT-125M lÃ m SSM. CÃ¡c tham sá»‘ mÃ´ hÃ¬nh Ä‘Æ°á»£c pre-train cho LLM vÃ  SSM Ä‘Æ°á»£c láº¥y tá»« cÃ¡c repository HuggingFace cá»§a chÃºng [19], vÃ  chÃºng tÃ´i mÃ´ táº£ cÃ¡ch SpecInfer táº­p thá»ƒ boost-tune nhiá»u SSM trong [28].

Bá»™ dá»¯ liá»‡u. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ SpecInfer trÃªn nÄƒm bá»™ dá»¯ liá»‡u: Chatbot Instruction Prompts (CIP) [34], ChatGPT Prompts (CP) [30], WebQA [1], Alpaca [36,45], vÃ  PIQA [2]. ChÃºng tÃ´i chá»‰ sá»­ dá»¥ng cÃ¡c prompt/cÃ¢u há»i tá»« nhá»¯ng bá»™ dá»¯ liá»‡u nÃ y Ä‘á»ƒ táº¡o thÃ nh cÃ¡c prompt Ä‘áº§u vÃ o cá»§a chÃºng tÃ´i Ä‘á»ƒ mÃ´ phá»ng cÃ¡c trace cuá»™c trÃ² chuyá»‡n thá»±c táº¿.

Ná»n táº£ng. CÃ¡c thÃ­ nghiá»‡m Ä‘Æ°á»£c tiáº¿n hÃ nh trÃªn hai instance AWS g5.12xlarge, má»—i instance Ä‘Æ°á»£c trang bá»‹ bá»‘n GPU NVIDIA A10 24GB, 48 lÃµi CPU, vÃ  192 GB DRAM. CÃ¡c nÃºt Ä‘Æ°á»£c káº¿t ná»‘i báº±ng Ethernet 100 Gbps.

CÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p dá»±a trÃªn má»Ÿ rá»™ng (xem Pháº§n 3) Ä‘á»ƒ xÃ¢y dá»±ng cÃ¢y token vÃ  sá»­ dá»¥ng cáº¥u hÃ¬nh má»Ÿ rá»™ng âŸ¨1,1,3,1,1,1,1,1âŸ©, cung cáº¥p káº¿t quáº£ tá»‘t cho cÃ¡c benchmark cá»§a chÃºng tÃ´i. ChÃºng tÃ´i phÃ¢n tÃ­ch tÃ¡c Ä‘á»™ng cá»§a cÃ¡c cáº¥u hÃ¬nh má»Ÿ rá»™ng trong Â§6.4, Ä‘Ã¡nh giÃ¡ giáº£i mÃ£ song song dá»±a trÃªn cÃ¢y vÃ  láº¥y máº«u suy Ä‘oÃ¡n Ä‘a bÆ°á»›c trong Â§6.5 vÃ  Â§6.6, vÃ  cuá»‘i cÃ¹ng so sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p xÃ¢y dá»±ng cÃ¢y dá»±a trÃªn má»Ÿ rá»™ng vÃ  há»£p nháº¥t trong [28].

6.2 Suy luáº­n LLM PhÃ¢n tÃ¡n
ChÃºng tÃ´i so sÃ¡nh hiá»‡u suáº¥t suy luáº­n LLM phÃ¢n tÃ¡n end-to-end giá»¯a SpecInfer, vLLM [24], HuggingFace Text Generation Inference (TGI) [18], vÃ  FasterTransformer [32] trÃªn LLaMA-7B, OPT-30B, vÃ  LLaMA-65B. Äá»‘i vá»›i LLaMA-7B vÃ  OPT-30B, táº¥t cáº£ cÃ¡c há»‡ thá»‘ng phá»¥c vá»¥ hai LLM á»Ÿ dáº¡ng sá»‘ thá»±c Ä‘á»™ chÃ­nh xÃ¡c ná»­a trÃªn má»™t vÃ  bá»‘n GPU A10 báº±ng cÃ¡ch sá»­ dá»¥ng song song mÃ´ hÃ¬nh tensor. LLaMA-65B khÃ´ng vá»«a trÃªn bá»‘n GPU trÃªn má»™t nÃºt duy nháº¥t, do Ä‘Ã³ cáº£ FasterTransformer vÃ  SpecInfer Ä‘á»u phá»¥c vá»¥ nÃ³ trÃªn tÃ¡m GPU A10 trÃªn hai nÃºt báº±ng cÃ¡ch káº¿t há»£p song song mÃ´ hÃ¬nh tensor trong má»—i nÃºt vÃ  song song mÃ´ hÃ¬nh pipeline trÃªn cÃ¡c nÃºt. vLLM vÃ  HuggingFace TGI khÃ´ng há»— trá»£ song song mÃ´ hÃ¬nh pipeline vÃ  khÃ´ng thá»ƒ phá»¥c vá»¥ má»™t LLM trÃªn nhiá»u nÃºt.

Äá»ƒ loáº¡i bá» cÃ¡c hiá»‡u á»©ng tiá»m nÄƒng cá»§a viá»‡c triá»ƒn khai há»‡ thá»‘ng cá»§a chÃºng tÃ´i, chÃºng tÃ´i cÅ©ng Ä‘Ã¡nh giÃ¡ SpecInfer vá»›i hai cáº¥u hÃ¬nh bá»• sung. Äáº§u tiÃªn, SpecInfer vá»›i giáº£i mÃ£ tÄƒng dáº§n Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t runtime cá»§a viá»‡c triá»ƒn khai cá»§a chÃºng tÃ´i khi bá»™ suy Ä‘oÃ¡n táº¡o ra cÃ¡c cÃ¢y token rá»—ng, vÃ  bá»™ xÃ¡c minh xÃ¡c minh chÃ­nh xÃ¡c má»™t token trong má»—i bÆ°á»›c giáº£i mÃ£. Thá»© hai, SpecInfer vá»›i suy luáº­n suy Ä‘oÃ¡n dá»±a trÃªn chuá»—i phá»¥c vá»¥ nhÆ° má»™t tham chiáº¿u cho há»‡ thá»‘ng suy luáº­n suy Ä‘oÃ¡n hiá»‡n cÃ³ vÃ  Ä‘Æ°á»£c kÃ­ch hoáº¡t báº±ng cÃ¡ch sá»­ dá»¥ng má»™t SSM Ä‘Æ°á»£c pre-train duy nháº¥t vÃ  giáº£i mÃ£ dá»±a trÃªn chuá»—i.

ChÃºng tÃ´i sá»­ dá»¥ng cÃ¡c prompt tá»« nÄƒm bá»™ dá»¯ liá»‡u Ä‘Æ°á»£c mÃ´ táº£ trong Â§6.1. Äá»‘i vá»›i má»—i prompt, chÃºng tÃ´i Ä‘á»ƒ táº¥t cáº£ cÃ¡c há»‡ thá»‘ng táº¡o ra lÃªn Ä‘áº¿n 128 token má»›i vÃ  bÃ¡o cÃ¡o Ä‘á»™ trá»… trung bÃ¬nh má»—i token trong HÃ¬nh 7. LÆ°u Ã½ ráº±ng SpecInfer cÃ³ thá»ƒ táº¡o ra nhiá»u hÆ¡n 128 token má»›i vÃ¬ bá»™ xÃ¡c minh cÃ³ thá»ƒ xÃ¡c minh nhiá»u token trong má»—i láº§n láº·p. Trong trÆ°á»ng há»£p nÃ y, chÃºng tÃ´i cáº¯t Ä‘áº§u ra cá»§a SpecInfer thÃ nh 128 token.

SpecInfer vá»›i giáº£i mÃ£ tÄƒng dáº§n Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i cÃ¡c há»‡ thá»‘ng hiá»‡n cÃ³. Äiá»u nÃ y lÃ  do táº¥t cáº£ cÃ¡c há»‡ thá»‘ng sá»­ dá»¥ng cÃ¹ng chiáº¿n lÆ°á»£c Ä‘á»ƒ song song hÃ³a suy luáº­n LLM trÃªn cÃ¡c GPU vÃ  sá»­ dá»¥ng cÃ¹ng thÆ° viá»‡n kernel (tá»©c lÃ , cuDNN, cuBLAS, vÃ  cuTLASS) Ä‘á»ƒ thá»±c thi tÃ­nh toÃ¡n suy luáº­n trÃªn GPU. Vá»›i suy luáº­n suy Ä‘oÃ¡n dá»±a trÃªn cÃ¢y vÃ  xÃ¡c minh, SpecInfer vÆ°á»£t trá»™i hÆ¡n cÃ¡c há»‡ thá»‘ng giáº£i mÃ£ tÄƒng dáº§n tá»« 1.5-2.5Ã— cho suy luáº­n Ä‘a GPU nÃºt Ä‘Æ¡n vÃ  tá»« 2.4-2.8Ã— cho suy luáº­n Ä‘a GPU Ä‘a nÃºt, trong khi táº¡o ra chÃ­nh xÃ¡c cÃ¹ng chuá»—i token nhÆ° giáº£i mÃ£ tÄƒng dáº§n cho táº¥t cáº£ cÃ¡c prompt. Sá»± tÄƒng tá»‘c Ä‘áº¿n tá»« viá»‡c táº­n dá»¥ng tÃ i nguyÃªn GPU dá»± phÃ²ng Ä‘á»ƒ thá»±c hiá»‡n giáº£i mÃ£ song song dá»±a trÃªn cÃ¢y trong khi duy trÃ¬ cÃ¹ng Ä‘á»™ trá»… má»—i láº§n láº·p nhÆ° giáº£i mÃ£ tÄƒng dáº§n.

So vá»›i suy luáº­n suy Ä‘oÃ¡n dá»±a trÃªn chuá»—i, cÃ¡ch tiáº¿p cáº­n dá»±a trÃªn cÃ¢y cá»§a SpecInfer tiáº¿p tá»¥c giáº£m Ä‘á»™ trá»… phá»¥c vá»¥ LLM tá»« 1.2-1.5Ã—. Sá»± cáº£i thiá»‡n Ä‘Æ°á»£c Ä‘áº¡t Ä‘Æ°á»£c báº±ng (1) táº­n dá»¥ng cÃ¢y token Ä‘á»ƒ tá»‘i Æ°u hÃ³a hiá»‡u suáº¥t suy Ä‘oÃ¡n, (2) sá»­ dá»¥ng giáº£i mÃ£ song song dá»±a trÃªn cÃ¢y Ä‘á»ƒ xÃ¡c minh toÃ n bá»™ cÃ¢y token song song, vÃ  (3) thá»±c hiá»‡n láº¥y máº«u suy Ä‘oÃ¡n Ä‘a bÆ°á»›c Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t xÃ¡c minh. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ thÃªm nhá»¯ng khÃ­a cáº¡nh nÃ y trong Â§6.4, Â§6.5, vÃ  Â§6.6.

LÆ°u Ã½ ráº±ng sá»± cáº£i thiá»‡n hiá»‡u suáº¥t cá»§a SpecInfer so vá»›i cÃ¡c há»‡ thá»‘ng hiá»‡n cÃ³ giáº£m khi kÃ­ch thÆ°á»›c batch (tá»©c lÃ , sá»‘ yÃªu cáº§u Ä‘á»“ng thá»i) tÄƒng. Äiá»u nÃ y lÃ  do SpecInfer táº­n dá»¥ng tÃ i nguyÃªn GPU dá»± phÃ²ng Ä‘á»ƒ thá»±c hiá»‡n giáº£i mÃ£ song sang dá»±a trÃªn cÃ¢y trong khi duy trÃ¬ cÃ¹ng Ä‘á»™ trá»… má»—i láº§n láº·p nhÆ° giáº£i mÃ£ tÄƒng dáº§n. KÃ­ch thÆ°á»›c batch lá»›n hÆ¡n táº¡o ra nhiá»u tÃ­nh toÃ¡n cÃ³ thá»ƒ song song hÃ³a hÆ¡n cho giáº£i mÃ£ tÄƒng dáº§n, vÃ  do Ä‘Ã³ Ã­t tÃ i nguyÃªn GPU dá»± phÃ²ng hÆ¡n cÃ³ thá»ƒ Ä‘Æ°á»£c táº­n dá»¥ng bá»Ÿi SpecInfer. Máº·t khÃ¡c, kÃ­ch thÆ°á»›c batch lá»›n hÆ¡n cÅ©ng tÄƒng Ä‘á»™ trá»… end-to-end cá»§a má»—i yÃªu cáº§u, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 7. NhÃ¬n chung, SpecInfer cÃ³ lá»£i nháº¥t cho suy luáº­n LLM Ä‘á»™ trá»… tháº¥p.

6.3 Suy luáº­n LLM dá»±a trÃªn Offloading
Má»™t á»©ng dá»¥ng quan trá»ng khÃ¡c cá»§a SpecInfer lÃ  suy luáº­n LLM dá»±a trÃªn offloading, trong Ä‘Ã³ há»‡ thá»‘ng offload cÃ¡c tham sá»‘ cá»§a LLM vÃ o CPU DRAM vÃ  táº£i má»™t táº­p há»£p con cá»§a nhá»¯ng tham sá»‘ nÃ y vÃ o GPU Ä‘á»ƒ tÃ­nh toÃ¡n suy luáº­n theo cÃ¡ch pipeline. ChÃºng tÃ´i so sÃ¡nh hiá»‡u suáº¥t suy luáº­n LLM dá»±a trÃªn offloading end-to-end giá»¯a SpecInfer vÃ  FlexGen [39] báº±ng cÃ¡ch sá»­ dá»¥ng má»™t GPU A10 24GB duy nháº¥t vÃ  hai LLM (tá»©c lÃ , OPT-13B vÃ  OPT-30B), cáº£ hai Ä‘á»u vÆ°á»£t quÃ¡ dung lÆ°á»£ng bá»™ nhá»› cá»§a GPU A10 vÃ  Ä‘Ã²i há»i offloading Ä‘á»ƒ phá»¥c vá»¥. Cáº£ SpecInfer vÃ  FlexGen Ä‘á»u giá»¯ táº¥t cáº£ tham sá»‘ mÃ´ hÃ¬nh trÃªn CPU DRAM. Trong quÃ¡ trÃ¬nh tÃ­nh toÃ¡n, cÃ¡c trá»ng sá»‘ cáº§n thiáº¿t Ä‘Æ°á»£c táº£i tá»« CPU vÃ o GPU. HÃ¬nh 8 cho tháº¥y káº¿t quáº£. So vá»›i FlexGen, SpecInfer giáº£m Ä‘á»™ trá»… má»—i token tá»« 2.6-3.5Ã—. VÃ¬ suy luáº­n LLM dá»±a trÃªn offloading chá»§ yáº¿u bá»‹ ngháº½n cá»• chai bá»Ÿi giao tiáº¿p giá»¯a CPU DRAM vÃ  GPU HBM Ä‘á»ƒ táº£i cÃ¡c tham sá»‘ cá»§a LLM, sá»± cáº£i thiá»‡n cá»§a SpecInfer so vá»›i cÃ¡c há»‡ thá»‘ng hiá»‡n cÃ³ Ä‘Æ°á»£c Ä‘áº¡t Ä‘Æ°á»£c báº±ng cÃ¡ch cÆ¡ há»™i xÃ¡c minh nhiá»u token, Ä‘iá»u nÃ y láº§n lÆ°á»£t giáº£m sá»‘ bÆ°á»›c giáº£i mÃ£ LLM vÃ  truyá»n dá»¯ liá»‡u giá»¯a CPU vÃ  GPU.

6.4 XÃ¢y dá»±ng CÃ¢y Token
Pháº§n nÃ y Ä‘Ã¡nh giÃ¡ cÆ¡ cháº¿ xÃ¢y dá»±ng cÃ¢y token dá»±a trÃªn má»Ÿ rá»™ng. ChÃºng tÃ´i Ä‘áº§u tiÃªn nghiÃªn cá»©u cÃ¡ch Ä‘á»™ rá»™ng cÃ¢y token áº£nh hÆ°á»Ÿng Ä‘áº¿n hiá»‡u suáº¥t suy Ä‘oÃ¡n cá»§a SpecInfer. Trong thÃ­ nghiá»‡m nÃ y, chÃºng tÃ´i sá»­ dá»¥ng LLaMA-7B vÃ  LLaMA-68M lÃ m LLM vÃ  SSM, vÃ  sá»­ dá»¥ng cáº¥u hÃ¬nh má»Ÿ rá»™ng âŸ¨1,1,ğ‘˜,1,1,1,1,1âŸ© (tá»©c lÃ , má»Ÿ rá»™ng á»Ÿ token thá»© ba), trong Ä‘Ã³ ğ‘˜ lÃ  Ä‘á»™ rá»™ng cÃ¢y token. HÃ¬nh 9 cho tháº¥y hÃ m phÃ¢n phá»‘i tÃ­ch lÅ©y (CDF) cá»§a sá»‘ lÆ°á»£ng token Ä‘Æ°á»£c xÃ¡c minh trung bÃ¬nh má»—i bÆ°á»›c giáº£i mÃ£ cho táº¥t cáº£ cÃ¡c prompt trong bá»™ dá»¯ liá»‡u Alpaca [45]. So vá»›i suy Ä‘oÃ¡n dá»±a trÃªn chuá»—i (tá»©c lÃ , Ä‘á»™ rá»™ng cÃ¢y = 1), táº­n dá»¥ng cÃ¢y token cÃ³ thá»ƒ giáº£m cÃ¡c bÆ°á»›c giáº£i mÃ£ LLM tá»« 1.2-1.5Ã— cho giáº£i mÃ£ tham lam vÃ  tá»« 1.3-1.4Ã— cho giáº£i mÃ£ ngáº«u nhiÃªn.

Äá»™ rá»™ng cÃ¢y token lá»›n hÆ¡n giáº£m cÃ¡c bÆ°á»›c giáº£i mÃ£ LLM Ä‘á»ƒ xá»­ lÃ½ má»™t yÃªu cáº§u vá»›i chi phÃ­ cá»§a chi phÃ­ xÃ¡c minh tÄƒng lÃªn, vÃ¬ SpecInfer pháº£i xÃ¡c minh nhiá»u token hÆ¡n. HÃ¬nh 10 so sÃ¡nh Ä‘á»™ trá»… suy luáº­n end-to-end cá»§a SpecInfer sá»­ dá»¥ng cÃ¡c Ä‘á»™ rá»™ng cÃ¢y khÃ¡c nhau. Äá»‘i vá»›i kÃ­ch thÆ°á»›c batch nhá» (tá»©c lÃ , BS = 1 vÃ  2), sá»­ dá»¥ng Ä‘á»™ rá»™ng cÃ¢y lá»›n cÃ³ thá»ƒ liÃªn tá»¥c giáº£m Ä‘á»™ trá»… má»—i token, vÃ¬ SpecInfer cÃ³ thá»ƒ táº­n dá»¥ng tÃ i nguyÃªn GPU thÆ°a thá»›t Ä‘á»ƒ xÃ¡c minh nhiá»u token song song trong khi duy trÃ¬ cÃ¹ng Ä‘á»™ trá»… má»—i láº§n láº·p. Äá»‘i vá»›i kÃ­ch thÆ°á»›c batch lá»›n (tá»©c lÃ , BS â‰¥4), sá»­ dá»¥ng Ä‘á»™ rá»™ng cÃ¢y lá»›n lÃ m tÄƒng Ä‘á»™ trá»… Ä‘á»ƒ xÃ¡c minh má»™t cÃ¢y token do Ã­t tÃ i nguyÃªn GPU thÆ°a thá»›t hÆ¡n cÃ³ thá»ƒ Ä‘Æ°á»£c táº­n dá»¥ng bá»Ÿi SpecInfer, vÃ  Ä‘á»™ rá»™ng cÃ¢y 2 hoáº·c 3 Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tá»‘t nháº¥t báº±ng cÃ¡ch táº¡o ra sá»± cÃ¢n báº±ng hoÃ n háº£o giá»¯a hiá»‡u suáº¥t suy Ä‘oÃ¡n vÃ  Ä‘á»™ trá»… xÃ¡c minh.

6.5 Giáº£i mÃ£ Song song Dá»±a trÃªn CÃ¢y
BÃ¢y giá» chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ cá»§a cÆ¡ cháº¿ giáº£i mÃ£ song song dá»±a trÃªn cÃ¢y cá»§a SpecInfer, giáº£i mÃ£ táº¥t cáº£ cÃ¡c token cá»§a má»™t cÃ¢y token song song. Äá»ƒ so sÃ¡nh, táº¥t cáº£ cÃ¡c há»‡ thá»‘ng suy luáº­n LLM hiá»‡n cÃ³ sá»­ dá»¥ng giáº£i mÃ£ dá»±a trÃªn chuá»—i, Ä‘Ã²i há»i phÃ¢n tÃ¡ch má»™t cÃ¢y token thÃ nh nhiá»u chuá»—i token vÃ  xá»­ lÃ½ nhá»¯ng chuá»—i nÃ y báº±ng cÃ¡ch sá»­ dá»¥ng tÃ i nguyÃªn riÃªng biá»‡t do cÃ¡c xung Ä‘á»™t key-value cache tiá»m nÄƒng (xem Â§4.2). NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 11, giáº£i mÃ£ song song dá»±a trÃªn cÃ¢y cá»§a SpecInfer Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i cÆ¡ cháº¿ giáº£i mÃ£ dá»±a trÃªn chuá»—i hiá»‡n cÃ³ cho kÃ­ch thÆ°á»›c batch nhá» vÃ  vÆ°á»£t trá»™i hÆ¡n lÃªn Ä‘áº¿n 1.8Ã— cho kÃ­ch thÆ°á»›c batch lá»›n. Sá»± cáº£i thiá»‡n Ä‘Æ°á»£c thá»±c hiá»‡n báº±ng (1) loáº¡i bá» tÃ­nh toÃ¡n attention dÆ° thá»«a cho cÃ¡c chuá»—i vá»›i tiá»n tá»‘ chia sáº», vÃ  (2) há»£p nháº¥t tree attention cá»§a táº¥t cáº£ cÃ¡c token trong má»™t kernel duy nháº¥t thÃ´ng qua mask nhÃ¢n quáº£ nháº­n biáº¿t topology (xem Â§4.2).

6.6 Láº¥y máº«u Suy Ä‘oÃ¡n Äa bÆ°á»›c
Pháº§n nÃ y Ä‘Ã¡nh giÃ¡ cÃ¡ch láº¥y máº«u suy Ä‘oÃ¡n Ä‘a bÆ°á»›c (MSS) cá»§a chÃºng tÃ´i vÃ  thuáº­t toÃ¡n VerifyStochastic cáº£i thiá»‡n hiá»‡u suáº¥t suy Ä‘oÃ¡n cá»§a SpecInfer khi thá»±c hiá»‡n giáº£i mÃ£ ngáº«u nhiÃªn. ChÃºng tÃ´i sá»­ dá»¥ng láº¥y máº«u ngÃ¢y thÆ¡ nhÆ° má»™t baseline trong Ä‘Ã³ SpecInfer trá»±c tiáº¿p láº¥y máº«u token tiáº¿p theo tá»« LLM vÃ  kiá»ƒm tra xem token Ä‘Æ°á»£c láº¥y máº«u cÃ³ Ä‘Æ°á»£c bao gá»“m trong cÃ¢y token Ä‘Æ°á»£c suy Ä‘oÃ¡n hay khÃ´ng (xem Â§4.3). VÃ¬ cÃ¡c thuáº­t toÃ¡n láº¥y máº«u khÃ¡c nhau liÃªn quan Ä‘áº¿n chi phÃ­ suy Ä‘oÃ¡n vÃ  xÃ¡c minh giá»‘ng há»‡t nhau, chÃºng tÃ´i táº­p trung vÃ o sá»‘ lÆ°á»£ng token trung bÃ¬nh cÃ³ thá»ƒ Ä‘Æ°á»£c xÃ¡c minh trong má»—i bÆ°á»›c giáº£i mÃ£ ngáº«u nhiÃªn trong thÃ­ nghiá»‡m nÃ y. Báº£ng 3 cho tháº¥y káº¿t quáº£. So vá»›i láº¥y máº«u ngÃ¢y thÆ¡, MSS cÃ³ thá»ƒ liÃªn tá»¥c cáº£i thiá»‡n sá»‘ lÆ°á»£ng token Ä‘Æ°á»£c xÃ¡c minh tá»« 1.2-1.3Ã— trung bÃ¬nh trÃªn nhiá»u bá»™ dá»¯ liá»‡u prompt khÃ¡c nhau, trong khi Ä‘áº£m báº£o cÃ¹ng phÃ¢n phá»‘i Ä‘áº§u ra vá»›i LLM.

7 CÃ´ng trÃ¬nh LiÃªn quan
CÃ¡c LLM dá»±a trÃªn Transformer Ä‘Ã£ chá»©ng minh tiá»m nÄƒng Ä‘Ã¡ng ká»ƒ trong nhiá»u tÃ¡c vá»¥ mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯ á»Ÿ cáº¥p Ä‘á»™ con ngÆ°á»i báº±ng cÃ¡ch liÃªn tá»¥c tÄƒng kÃ­ch thÆ°á»›c cá»§a chÃºng [7,9,37,43,48]. Khi GPT-3 trá»Ÿ thÃ nh mÃ´ hÃ¬nh Ä‘áº§u tiÃªn vÆ°á»£t quÃ¡ 100B tham sá»‘ [3], nhiá»u LLM (>100B) Ä‘Ã£ Ä‘Æ°á»£c phÃ¡t hÃ nh, bao gá»“m OPT-175B [57], Bloom-176B [38], vÃ  PaLM [7]. CÃ´ng trÃ¬nh gáº§n Ä‘Ã¢y Ä‘Ã£ Ä‘á» xuáº¥t nhiá»u cÃ¡ch tiáº¿p cáº­n khÃ¡c nhau Ä‘á»ƒ tÄƒng tá»‘c suy luáº­n LLM sinh táº¡o, cÃ³ thá»ƒ Ä‘Æ°á»£c phÃ¢n loáº¡i thÃ nh hai lá»›p.

TÄƒng tá»‘c khÃ´ng máº¥t mÃ¡t. CÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y Ä‘Ã£ khÃ¡m phÃ¡ Ã½ tÆ°á»Ÿng sá»­ dá»¥ng má»™t LLM nhÆ° má»™t bá»™ xÃ¡c minh thay vÃ¬ má»™t bá»™ giáº£i mÃ£ Ä‘á»ƒ tÄƒng cÆ°á»ng suy luáº­n. VÃ­ dá»¥, Yang et al. [53] giá»›i thiá»‡u suy luáº­n vá»›i tham chiáº¿u, táº­n dá»¥ng sá»± chá»“ng láº·p giá»¯a Ä‘áº§u ra cá»§a LLM vÃ  cÃ¡c tham chiáº¿u thu Ä‘Æ°á»£c báº±ng cÃ¡ch truy xuáº¥t tÃ i liá»‡u, vÃ  kiá»ƒm tra tÃ­nh phÃ¹ há»£p cá»§a má»—i tham chiáº¿u báº±ng cÃ¡ch kiá»ƒm tra káº¿t quáº£ giáº£i mÃ£ cá»§a LLM. ÄÆ°á»£c thÃºc Ä‘áº©y bá»Ÿi Ã½ tÆ°á»Ÿng thá»±c thi suy Ä‘oÃ¡n trong tá»‘i Æ°u hÃ³a bá»™ xá»­ lÃ½ [4,16], cÃ´ng trÃ¬nh gáº§n Ä‘Ã¢y Ä‘á» xuáº¥t giáº£i mÃ£ suy Ä‘oÃ¡n, sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ nhá» Ä‘á»ƒ táº¡o ra má»™t chuá»—i token vÃ  kiá»ƒm tra tÃ­nh Ä‘Ãºng Ä‘áº¯n cá»§a nhá»¯ng token nÃ y báº±ng cÃ¡ch sá»­ dá»¥ng má»™t LLM [5,22,25,44,51].

CÃ³ hai khÃ¡c biá»‡t chÃ­nh giá»¯a SpecInfer vÃ  nhá»¯ng cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y nÃ y. Äáº§u tiÃªn, thay vÃ¬ chá»‰ xem xÃ©t má»™t chuá»—i token duy nháº¥t, SpecInfer táº¡o ra vÃ  xÃ¡c minh má»™t cÃ¢y token, mÃ  cÃ¡c nÃºt cá»§a nÃ³ má»—i nÃºt Ä‘áº¡i diá»‡n cho má»™t chuá»—i token Ä‘á»™c Ä‘Ã¡o. SpecInfer thá»±c hiá»‡n tree attention Ä‘á»ƒ tÃ­nh toÃ¡n Ä‘áº§u ra attention cá»§a nhá»¯ng chuá»—i token nÃ y song song vÃ  sá»­ dá»¥ng má»™t thuáº­t toÃ¡n giáº£i mÃ£ dá»±a trÃªn cÃ¢y má»›i Ä‘á»ƒ tÃ¡i sá»­ dá»¥ng káº¿t quáº£ trung gian Ä‘Æ°á»£c chia sáº» trÃªn nhá»¯ng chuá»—i nÃ y. Thá»© hai, nhá»¯ng ná»— lá»±c trÆ°á»›c Ä‘Ã¢y thÆ°á»ng xem xÃ©t má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ nhá» duy nháº¥t Ä‘á»ƒ suy Ä‘oÃ¡n, khÃ´ng thá»ƒ phÃ¹ há»£p tá»‘t vá»›i má»™t LLM do khoáº£ng cÃ¡ch nÄƒng lá»±c mÃ´ hÃ¬nh giá»¯a chÃºng. SpecInfer giá»›i thiá»‡u hai phÆ°Æ¡ng phÃ¡p suy Ä‘oÃ¡n má»›i, bao gá»“m 1) má»Ÿ rá»™ng tá»« má»™t SSM duy nháº¥t vÃ  2) há»£p nháº¥t tá»« nhiá»u SSM Ä‘Æ°á»£c fine-tune, vÃ  cÃ¢y token Ä‘Æ°á»£c táº¡o ra lÃ m tÄƒng Ä‘Ã¡ng ká»ƒ Ä‘á»™ bao phá»§ cá»§a Ä‘áº§u ra LLM.

CÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y cÅ©ng Ä‘Ã£ giá»›i thiá»‡u nhiá»u ká»¹ thuáº­t khÃ¡c nhau Ä‘á»ƒ tá»‘i Æ°u hÃ³a tÃ­nh toÃ¡n ML trÃªn cÃ¡c ná»n táº£ng pháº§n cá»©ng hiá»‡n Ä‘áº¡i. VÃ­ dá»¥, TVM [6] vÃ  Ansor [58] tá»± Ä‘á»™ng táº¡o ra kernel cho má»™t chÆ°Æ¡ng trÃ¬nh tensor nháº¥t Ä‘á»‹nh. TASO [20] vÃ  PET [50] tá»± Ä‘á»™ng khÃ¡m phÃ¡ cÃ¡c biáº¿n Ä‘á»•i cáº¥p Ä‘á»™ Ä‘á»“ thá»‹ Ä‘á»ƒ tá»‘i Æ°u hÃ³a Ä‘á»“ thá»‹ tÃ­nh toÃ¡n cá»§a má»™t DNN. CÃ¡c ká»¹ thuáº­t cá»§a SpecInfer lÃ  trá»±c giao vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c káº¿t há»£p vá»›i nhá»¯ng há»‡ thá»‘ng nÃ y Ä‘á»ƒ tÄƒng tá»‘c tÃ­nh toÃ¡n LLM sinh táº¡o, mÃ  chÃºng tÃ´i tin lÃ  má»™t hÆ°á»›ng nghiÃªn cá»©u há»©a háº¹n cho cÃ´ng viá»‡c tÆ°Æ¡ng lai.

TÄƒng tá»‘c cÃ³ máº¥t mÃ¡t. BiLD [23] lÃ  má»™t framework giáº£i mÃ£ suy Ä‘oÃ¡n sá»­ dá»¥ng má»™t SSM duy nháº¥t Ä‘á»ƒ tÄƒng tá»‘c giáº£i mÃ£ LLM. KhÃ´ng giá»‘ng nhÆ° cÃ¡c há»‡ thá»‘ng Ä‘Æ°á»£c Ä‘á» cáº­p á»Ÿ trÃªn, viá»‡c tÄƒng tá»‘c cÃ³ máº¥t mÃ¡t: tá»‘c Ä‘á»™ Ä‘áº¿n vá»›i chi phÃ­ cá»§a sá»± suy giáº£m cÃ³ thá»ƒ trong cÃ¡c token Ä‘Æ°á»£c táº¡o ra. Má»™t hÆ°á»›ng nghiÃªn cá»©u khÃ¡c táº­n dá»¥ng nÃ©n mÃ´ hÃ¬nh Ä‘á»ƒ giáº£m Ä‘á»™ trá»… suy luáº­n LLM trong khi lÃ m tá»•n háº¡i hiá»‡u suáº¥t dá»± Ä‘oÃ¡n cá»§a LLM. VÃ­ dá»¥, cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y Ä‘á» xuáº¥t táº­n dá»¥ng lÆ°á»£ng tá»­ hÃ³a trá»ng sá»‘/kÃ­ch hoáº¡t cá»§a LLM Ä‘á»ƒ giáº£m yÃªu cáº§u bá»™ nhá»› vÃ  tÃ­nh toÃ¡n cá»§a viá»‡c phá»¥c vá»¥ nhá»¯ng LLM nÃ y [8,11,35,52,54]. CÃ´ng trÃ¬nh gáº§n Ä‘Ã¢y tiáº¿p tá»¥c khÃ¡m phÃ¡ nhiá»u ká»¹ thuáº­t cáº¯t tá»‰a cÃ³ cáº¥u trÃºc khÃ¡c nhau Ä‘á»ƒ tÄƒng tá»‘c cÃ¡c kiáº¿n trÃºc dá»±a trÃªn Transformer [10,17,49]. Má»™t khÃ¡c biá»‡t chÃ­nh giá»¯a SpecInfer vÃ  nhá»¯ng cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y nÃ y lÃ  SpecInfer khÃ´ng trá»±c tiáº¿p giáº£m yÃªu cáº§u tÃ­nh toÃ¡n Ä‘á»ƒ thá»±c hiá»‡n suy luáº­n LLM, mÃ  thay vÃ o Ä‘Ã³ tÃ¡i tá»• chá»©c tÃ­nh toÃ¡n suy luáº­n LLM theo cÃ¡ch cÃ³ thá»ƒ song song hÃ³a hÆ¡n, giáº£m truy cáº­p bá»™ nhá»› vÃ  Ä‘á»™ trá»… suy luáº­n vá»›i chi phÃ­ cá»§a chi phÃ­ bá»™ nhá»› vÃ  tÃ­nh toÃ¡n cÃ³ thá»ƒ quáº£n lÃ½.

Tree-structured attention. Nguyen et al. [31] giá»›i thiá»‡u tree-structured attention, má»™t ká»¹ thuáº­t cho phÃ©p má»™t mÃ´ hÃ¬nh Transformer náº¯m báº¯t thÃ nh pháº§n phÃ¢n cáº¥p cá»§a vÄƒn báº£n Ä‘áº§u vÃ o báº±ng cÃ¡ch cháº¡y mÃ´ hÃ¬nh trÃªn cÃ¢y phÃ¢n tÃ­ch cÃº phÃ¡p cá»§a vÄƒn báº£n. Äá»ƒ xá»­ lÃ½ vá»›i attention, nÃ³ sá»­ dá»¥ng Ã¡nh xáº¡ má»™t-má»™t Ä‘á»ƒ mÃ£ hÃ³a vÃ  giáº£i mÃ£ cÃ¢y. CÃ³ hai khÃ¡c biá»‡t chÃ­nh tá»« giáº£i mÃ£ dá»±a trÃªn cÃ¢y cá»§a SpecInfer. Äáº§u tiÃªn, SpecInfer sá»­ dá»¥ng má»™t cÃ¢y Ä‘á»ƒ káº¿t há»£p cÃ¡c chuá»—i á»©ng viÃªn Ä‘á»ƒ nÃ©n tiá»n tá»‘, trong khi Nguyen et al. biá»ƒu diá»…n má»™t chuá»—i duy nháº¥t vá»›i cÃ¢y phÃ¢n tÃ­ch cÃº phÃ¡p cá»§a nÃ³. SpecInfer khÃ´ng káº¿t há»£p cÃ¢y phÃ¢n tÃ­ch cÃº phÃ¡p vÃ o LLM, mÃ  tÄƒng tá»‘c suy luáº­n báº±ng cÃ¡ch xÃ¡c minh cÃ¡c chuá»—i Ä‘Æ°á»£c giáº£i mÃ£ song song. Thá»© hai, attention cá»§a SpecInfer xuáº¥t ra má»™t chuá»—i token, khÃ´ng pháº£i má»™t cÃ¢y.

Ká»¹ thuáº­t giáº£i mÃ£ Ä‘a máº«u. Giá»‘ng nhÆ° suy luáº­n suy Ä‘oÃ¡n dá»±a trÃªn cÃ¢y, beam search, láº¥y máº«u top-k, vÃ  láº¥y máº«u top-p xem xÃ©t nhiá»u chuá»—i token á»©ng viÃªn á»Ÿ má»—i bÆ°á»›c vÃ  cÃ³ thá»ƒ cáº¯t tá»‰a cÃ¡c lá»±a chá»n xÃ¡c suáº¥t tháº¥p. Tuy nhiÃªn, giáº£i mÃ£ dá»±a trÃªn cÃ¢y trong SpecInfer suy Ä‘oÃ¡n dá»± Ä‘oÃ¡n vÃ  xÃ¡c minh nhiá»u á»©ng viÃªn song song so vá»›i má»™t LLM Ä‘á»ƒ giáº£m cÃ¡c láº§n láº·p giáº£i mÃ£ vÃ  Ä‘á»™ trá»…, táº­n dá»¥ng cÃ¡c mÃ´ hÃ¬nh suy Ä‘oÃ¡n nhá» (SSM). NgÆ°á»£c láº¡i, beam search vÃ  láº¥y máº«u top-k/top-p lÃ  cÃ¡c chiáº¿n lÆ°á»£c giáº£i mÃ£ Ä‘Æ°á»£c Ã¡p dá»¥ng trá»±c tiáº¿p cho cÃ¡c xÃ¡c suáº¥t Ä‘áº§u ra cá»§a LLM Ä‘á»ƒ táº¡o ra cÃ¡c chuá»—i xÃ¡c suáº¥t cao mÃ  khÃ´ng giáº£m cÃ¡c bÆ°á»›c giáº£i mÃ£. SpecInfer há»— trá»£ beam search, láº¥y máº«u top-k, vÃ  láº¥y máº«u top-p. Nhá»¯ng ká»¹ thuáº­t nÃ y lÃ  cÃ¡c tá»‘i Æ°u hÃ³a giáº£i mÃ£ trá»±c giao vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c káº¿t há»£p vá»›i giáº£i mÃ£ suy Ä‘oÃ¡n dá»±a trÃªn cÃ¢y.

8 Káº¿t luáº­n
BÃ i bÃ¡o nÃ y giá»›i thiá»‡u SpecInfer, má»™t há»‡ thá»‘ng tÄƒng tá»‘c suy luáº­n LLM sinh táº¡o vá»›i suy luáº­n suy Ä‘oÃ¡n dá»±a trÃªn cÃ¢y vÃ  xÃ¡c minh. Má»™t insight chÃ­nh Ä‘áº±ng sau SpecInfer lÃ  Ä‘á»“ng thá»i xem xÃ©t sá»± Ä‘a dáº¡ng cá»§a cÃ¡c á»©ng viÃªn suy Ä‘oÃ¡n Ä‘á»ƒ dá»± Ä‘oÃ¡n hiá»‡u quáº£ Ä‘áº§u ra cá»§a LLM, Ä‘Æ°á»£c tá»• chá»©c nhÆ° má»™t cÃ¢y token vÃ  Ä‘Æ°á»£c xÃ¡c minh so vá»›i LLM song song báº±ng cÃ¡ch sá»­ dá»¥ng cÆ¡ cháº¿ giáº£i mÃ£ song song dá»±a trÃªn cÃ¢y. SpecInfer giáº£m Ä‘Ã¡ng ká»ƒ viá»‡c truy cáº­p bá»™ nhá»› Ä‘áº¿n cÃ¡c tham sá»‘ cá»§a LLM vÃ  Ä‘á»™ trá»… suy luáº­n LLM end-to-end cho cáº£ suy luáº­n LLM phÃ¢n tÃ¡n vÃ  dá»±a trÃªn offloading.

Lá»i cáº£m Æ¡n
ChÃºng tÃ´i cáº£m Æ¡n Tianqi Chen, Bohan Hou, Hongyi Jin, cÃ¡c reviewer ASPLOS áº©n danh, vÃ  shepherd Shan Lu cá»§a chÃºng tÃ´i vÃ¬ pháº£n há»“i cá»§a há» vá» cÃ´ng viá»‡c nÃ y. NghiÃªn cá»©u nÃ y Ä‘Æ°á»£c há»— trá»£ má»™t pháº§n bá»Ÿi cÃ¡c giáº£i thÆ°á»Ÿng NSF CNS-2147909, CNS-2211882, vÃ  CNS-2239351, vÃ  cÃ¡c giáº£i thÆ°á»Ÿng nghiÃªn cá»©u tá»« Amazon, Cisco, Google, Meta, Oracle, Qualcomm, vÃ  Samsung.

A Phá»¥ lá»¥c Artifact
A.1 TÃ³m táº¯t
Artifact chá»©a mÃ£ Ä‘á»ƒ cháº¡y SpecInfer, cÅ©ng nhÆ° cÃ¡c bá»™ dá»¯ liá»‡u vÃ  script cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ¡i táº¡o cÃ¡c thÃ­ nghiá»‡m trong bÃ i bÃ¡o.

A.2 Danh sÃ¡ch kiá»ƒm tra artifact (meta-information)
â€¢ Thuáº­t toÃ¡n: Tree-based Speculative Inference
â€¢ ChÆ°Æ¡ng trÃ¬nh: spec_infer.cc, incr_decoding.cc
â€¢ BiÃªn dá»‹ch: CMake
â€¢ MÃ´i trÆ°á»ng run-time: CUDA, NCCL, MPI, UCX, Python3.
â€¢ Pháº§n cá»©ng: Hai instance AWS g5.12xlarge, má»—i instance cÃ³ 4 GPU NVIDIA A10 24GB, 48 lÃµi CPU, vÃ  192 GB DRAM.
â€¢ Metrics: Äá»™ trá»… trung bÃ¬nh End-to-end
â€¢ Äáº§u ra: Äá»™ trá»… end-to-end
â€¢ ThÃ­ nghiá»‡m: Suy luáº­n GPU cáº¥p server, Suy luáº­n dá»±a trÃªn offloading
â€¢ Cáº§n bao nhiÃªu dung lÆ°á»£ng Ä‘Ä©a (xáº¥p xá»‰)?: 350GB má»—i nÃºt
â€¢ Cáº§n bao nhiÃªu thá»i gian Ä‘á»ƒ chuáº©n bá»‹ workflow (xáº¥p xá»‰)?: 2h
â€¢ Cáº§n bao nhiÃªu thá»i gian Ä‘á»ƒ hoÃ n thÃ nh cÃ¡c thÃ­ nghiá»‡m (xáº¥p xá»‰)?: 6h
â€¢ CÃ³ sáºµn cÃ´ng khai?: CÃ³
â€¢ Giáº¥y phÃ©p mÃ£ (náº¿u cÃ³ sáºµn cÃ´ng khai)?: Apache License v2.0
â€¢ Giáº¥y phÃ©p dá»¯ liá»‡u (náº¿u cÃ³ sáºµn cÃ´ng khai)?: LLAMA náº±m dÆ°á»›i giáº¥y phÃ©p GNU vÃ  OPT náº±m dÆ°á»›i giáº¥y phÃ©p Non-commercial
â€¢ ÄÆ°á»£c lÆ°u trá»¯ (cung cáº¥p DOI)?: https://doi.org/10.5281/zenodo.10854410 .

A.3 MÃ´ táº£
A.3.1 CÃ¡ch truy cáº­p. Artifact Ä‘Æ°á»£c phÃ¡t hÃ nh trÃªn Github: https://github.com/goliaro/specinfer-ae . Repository chá»©a mÃ£ nguá»“n cá»§a SpecInfer, vÃ  hÆ°á»›ng dáº«n Ä‘á»ƒ xÃ¢y dá»±ng framework. ChÃºng tÃ´i cÅ©ng cung cáº¥p script Ä‘á»ƒ tÃ¡i táº¡o cÃ¡c thÃ­ nghiá»‡m tá»« bÃ i bÃ¡o. Äá»ƒ clone repository, sá»­ dá»¥ng lá»‡nh sau (Ä‘áº£m báº£o truyá»n cá» â€“recursive):

git clone --recursive \
https://github.com/goliaro/specinfer-ae.git

A.3.2 Phá»¥ thuá»™c pháº§n cá»©ng. ChÃºng tÃ´i cháº¡y cÃ¡c thÃ­ nghiá»‡m trÃªn hai instance AWS g5.12xlarge, má»—i instance cÃ³ 4 GPU NVIDIA A10 24GB, 48 lÃµi CPU, vÃ  192 GB DRAM. ChÃºng tÃ´i cung cáº¥p hÆ°á»›ng dáº«n Ä‘á»ƒ táº¡o vÃ  thiáº¿t láº­p cÃ¡c instance.

A.3.3 Phá»¥ thuá»™c pháº§n má»m. Pháº§n má»m sau lÃ  báº¯t buá»™c: CUDA 12.1, NCCL, Rust, CMake vÃ  Python3. HÆ¡n ná»¯a, UCX vÃ  MPI Ä‘Æ°á»£c yÃªu cáº§u cho cÃ¡c thÃ­ nghiá»‡m Ä‘a nÃºt. CÃ¡c phá»¥ thuá»™c Python bá»• sung Ä‘Æ°á»£c liá»‡t kÃª á»Ÿ Ä‘Ã¢y: https://github.com/flexflow/FlexFlow/blob/inference/requirements.txt. ChÃºng tÃ´i khuyáº¿n nghá»‹ sá»­ dá»¥ng Deep Learning OSS Nvidia Driver AMI GPU PyTorch 2.1.0 (Ubuntu 20.04) AMI, vÃ  cung cáº¥p script vÃ  mÃ´i trÆ°á»ng conda Ä‘á»ƒ cÃ i Ä‘áº·t táº¥t cáº£ cÃ¡c phá»¥ thuá»™c cÃ²n láº¡i.

A.3.4 MÃ´ hÃ¬nh. ChÃºng tÃ´i sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh LLM/SSM sau cho cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i (Ä‘á»‘i vá»›i má»—i mÃ´ hÃ¬nh, chÃºng tÃ´i chá»‰ Ä‘á»‹nh trong ngoáº·c Ä‘Æ¡n repository HuggingFace tÆ°Æ¡ng á»©ng): LLaMA-68M (JackFram/llama-68m), LLaMA-7B (huggyllama/llama-7b), LLaMA-65B (huggyllama/llama-65b), OPT-125M (facebook/opt-125m), OPT-13B (facebook/opt-13b), OPT-125M (facebook/opt-30b). Báº¡n cÃ³ thá»ƒ táº£i xuá»‘ng táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh nÃ y vá»›i script:
./download_models.sh

A.4 CÃ i Ä‘áº·t
Äá»ƒ tÃ¡i táº¡o cÃ¡c thÃ­ nghiá»‡m, báº¡n sáº½ cáº§n truy cáº­p vÃ o hai instance AWS g5.12xlarge (hoáº·c cÃ¡c mÃ¡y khÃ¡c cÃ³ cÃ¹ng thÃ´ng sá»‘ ká»¹ thuáº­t GPU/CPU/máº¡ng). Náº¿u báº¡n Ä‘ang sá»­ dá»¥ng cÃ¡c instance Ä‘Æ°á»£c cáº¥u hÃ¬nh trÆ°á»›c mÃ  chÃºng tÃ´i cung cáº¥p, báº¡n cÃ³ thá»ƒ bá» qua bÆ°á»›c nÃ y.

Khá»Ÿi cháº¡y cÃ¡c instance. Khá»Ÿi cháº¡y hai instance AWS g5.12xlarge sá»­ dá»¥ng Deep Learning OSS Nvidia Driver AMI GPU PyTorch 2.1.0 (Ubuntu 20.04) AMI. Äáº£m báº£o Ä‘áº·t cÃ¡c instance trong má»™t placement group sá»­ dá»¥ng chiáº¿n lÆ°á»£c cluster Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t máº¡ng Ä‘á»™ trá»… tháº¥p. Gáº¯n cÃ¹ng security group vÃ o táº¥t cáº£ cÃ¡c instance vÃ  thÃªm má»™t quy táº¯c inbound trong security group Ä‘á»ƒ cho phÃ©p táº¥t cáº£ lÆ°u lÆ°á»£ng Ä‘áº¿n tá»« cÃ¹ng security group. VÃ­ dá»¥, báº¡n cÃ³ thá»ƒ thÃªm quy táº¯c sau: Type: All TCP, Source: Anywhere-IPv4.

CÃ i Ä‘áº·t cÃ¡c Ä‘iá»u kiá»‡n tiÃªn quyáº¿t. Sau khi truy cáº­p vÃ o cÃ¡c instance AWS, cÃ i Ä‘áº·t cÃ¡c Ä‘iá»u kiá»‡n tiÃªn quyáº¿t báº±ng cÃ¡ch lÃ m theo cÃ¡c bÆ°á»›c dÆ°á»›i Ä‘Ã¢y. Äáº§u tiÃªn, kÃ­ch hoáº¡t há»— trá»£ conda shell báº±ng cÃ¡ch cháº¡y conda init bash, vÃ  sau Ä‘Ã³ khá»Ÿi Ä‘á»™ng láº¡i phiÃªn shell.

Tiáº¿p theo, táº¡o mÃ´i trÆ°á»ng conda vá»›i táº¥t cáº£ cÃ¡c phá»¥ thuá»™c báº¯t buá»™c báº±ng cÃ¡ch cháº¡y:
conda env create -f FlexFlow/conda/flexflow.yml
conda activate flexflow

Thiáº¿t láº­p Ä‘a nÃºt. Táº£i xuá»‘ng vÃ  xÃ¢y dá»±ng UCX báº±ng cÃ¡ch cháº¡y script install_ucx.sh. Tiáº¿p theo, náº¿u báº¡n Ä‘ang cháº¡y SpecInfer trÃªn hai instance AWS, báº¡n sáº½ cáº§n cáº¥u hÃ¬nh MPI Ä‘á»ƒ hai instance cÃ³ thá»ƒ truy cáº­p láº«n nhau. Chá»n má»™t nÃºt chÃ­nh, vÃ  táº¡o má»™t cáº·p khÃ³a SSH vá»›i:
ssh-keygen -t ed25519

Ná»‘i ná»™i dung cá»§a khÃ³a cÃ´ng khai (~/.ssh/id_ed25519.pub) vÃ o tá»‡p ~/.ssh/authorized_keys trÃªn Cáº¢ mÃ¡y chÃ­nh vÃ  mÃ¡y phá»¥. LÆ°u Ã½ ráº±ng náº¿u thÆ° má»¥c .ssh hoáº·c tá»‡p authorized_keys khÃ´ng tá»“n táº¡i, báº¡n sáº½ cáº§n táº¡o chÃºng thá»§ cÃ´ng. Cuá»‘i cÃ¹ng, táº¡o má»™t tá»‡p táº¡i Ä‘Æ°á»ng dáº«n ~/hostfile vá»›i ná»™i dung sau:
<main_node_private_ip> slots=4
<secondary_node_private_ip> slots=4

thay tháº¿ <main_node_private_ip> vÃ  <secondary_node_private_ip> báº±ng Ä‘á»‹a chá»‰ IP riÃªng tÆ° cá»§a hai mÃ¡y, vÃ  sá»‘ slots vá»›i sá»‘ GPU cÃ³ sáºµn (náº¿u báº¡n Ä‘ang sá»­ dá»¥ng cÃ¡c instance AWS Ä‘Æ°á»£c khuyáº¿n nghá»‹, báº¡n sáº½ sá»­ dá»¥ng giÃ¡ trá»‹ 4). Báº¡n cÃ³ thá»ƒ tÃ¬m Ä‘á»‹a chá»‰ IP riÃªng tÆ° cá»§a má»—i mÃ¡y báº±ng cÃ¡ch cháº¡y lá»‡nh (vÃ  sá»­ dá»¥ng giÃ¡ trá»‹ IP Ä‘áº§u tiÃªn Ä‘Æ°á»£c in):
hostname -I

CÃ i Ä‘áº·t SpecInfer. Äá»ƒ cÃ i Ä‘áº·t SpecInfer, cháº¡y script:
./install_specinfer.sh

A.5 Kiá»ƒm tra CÆ¡ báº£n
Äá»ƒ Ä‘áº£m báº£o ráº±ng SpecInfer Ä‘Æ°á»£c cÃ i Ä‘áº·t Ä‘Ãºng vÃ  hoáº¡t Ä‘á»™ng, cháº¡y script basic_test.sh. Script nÃ y sáº½ kiá»ƒm tra cÃ¡c chá»©c nÄƒng giáº£i mÃ£ tÄƒng dáº§n cÆ¡ báº£n vÃ  suy luáº­n suy Ä‘oÃ¡n, trÃªn cáº£ nÃºt Ä‘Æ¡n vÃ  Ä‘a nÃºt. NÃ³ cÅ©ng sáº½ kiá»ƒm tra há»— trá»£ cho offloading. Kiá»ƒm tra thÃ nh cÃ´ng náº¿u nÃ³ in thÃ´ng bÃ¡o "Test passed!".

A.6 Quy trÃ¬nh thÃ­ nghiá»‡m
Artifact Ä‘i kÃ¨m vá»›i cÃ¡c script Ä‘á»ƒ thu tháº­p dá»¯ liá»‡u cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ¡i táº¡o káº¿t quáº£ tá»« bÃ i bÃ¡o. NÃ³ cÅ©ng Ä‘i kÃ¨m vá»›i cÃ¡c script cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u Ä‘áº§u ra thÃ nh Ä‘á»‹nh dáº¡ng CSV Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“.

Cháº¡y ThÃ­ nghiá»‡m. ChÃºng tÃ´i cháº¡y hai thÃ­ nghiá»‡m sau Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ SpecInfer dÆ°á»›i cÃ¡c thiáº¿t láº­p pháº§n cá»©ng khÃ¡c nhau. Dá»¯ liá»‡u Ä‘áº§u ra sáº½ Ä‘Æ°á»£c lÆ°u vÃ o Ä‘Æ°á»ng dáº«n FlexFlow/inference/output.

â€¢ ÄÃ¡nh giÃ¡ GPU cáº¥p server. ThÃ­ nghiá»‡m nÃ y kiá»ƒm tra hiá»‡u suáº¥t cá»§a SpecInfer trÃªn GPU cáº¥p server. CÃ¡c LLM vÃ  SSM Ä‘Æ°á»£c táº£i trong bá»™ nhá»› GPU, vÃ  chÃºng tÃ´i Ä‘o Ä‘á»™ trá»… suy luáº­n end-to-end sá»­ dá»¥ng 1 nÃºt, vÃ  2 nÃºt. Trong trÆ°á»ng há»£p nÃºt Ä‘Æ¡n, chÃºng tÃ´i Ä‘o hiá»‡u suáº¥t sá»­ dá»¥ng 1 GPU, hoáº·c 4 GPU. Trong trÆ°á»ng há»£p Ä‘a nÃºt, chÃºng tÃ´i sá»­ dá»¥ng 4 GPU má»—i nÃºt. CÃ¡c thÃ­ nghiá»‡m sá»­ dá»¥ng LLAMA-7B, OPT-30B vÃ  LLAMA-65B lÃ m LLM, vÃ  LLAMA-68M vÃ  OPT-125M lÃ m SSM. ThÃ­ nghiá»‡m cháº¡y SpecInfer trong ba cháº¿ Ä‘á»™ khÃ¡c nhau: giáº£i mÃ£ tÄƒng dáº§n, giáº£i mÃ£ suy Ä‘oÃ¡n dá»±a trÃªn chuá»—i, vÃ  giáº£i mÃ£ suy Ä‘oÃ¡n dá»±a trÃªn cÃ¢y. Hai cháº¿ Ä‘á»™ Ä‘áº§u Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ thu Ä‘Æ°á»£c dá»¯ liá»‡u cho nghiÃªn cá»©u ablation, vÃ  cháº¿ Ä‘á»™ sau lÃ  cháº¿ Ä‘á»™ suy luáº­n má»›i Ä‘Æ°á»£c Ä‘á» xuáº¥t bá»Ÿi SpecInfer, vÃ  sáº½ Ä‘Æ°á»£c triá»ƒn khai bá»Ÿi ngÆ°á»i dÃ¹ng. Äá»ƒ cháº¡y Ä‘Ã¡nh giÃ¡ GPU cáº¥p server, cháº¡y:
./server_gpu_experiments.sh

â€¢ ÄÃ¡nh giÃ¡ offloading. ThÃ­ nghiá»‡m nÃ y kiá»ƒm tra hiá»‡u suáº¥t cá»§a SpecInfer khi chá»‰ táº£i má»™t táº­p há»£p con tham sá»‘ trong bá»™ nhá»› GPU, trong khi offload nhá»¯ng tham sá»‘ cÃ²n láº¡i trÃªn CPU DRAM. Ká»¹ thuáº­t nÃ y Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ thá»±c hiá»‡n suy luáº­n khi mÃ´ hÃ¬nh Ä‘Ã­ch lá»›n hÆ¡n bá»™ nhá»› GPU cÃ³ sáºµn. Trong thÃ­ nghiá»‡m, SpecInfer sá»­ dá»¥ng má»™t GPU duy nháº¥t vÃ  hoÃ¡n Ä‘á»•i trá»ng sá»‘ cá»§a mÃ´ hÃ¬nh Ä‘áº¿n vÃ  tá»« CPU. Äá»ƒ cháº¡y Ä‘Ã¡nh giÃ¡ offloading, cháº¡y:
./offloading_experiments.sh

Framework bÃªn thá»© ba. Vui lÃ²ng lÃ m theo tÃ i liá»‡u chÃ­nh thá»©c cá»§a vLLM, FasterTransformer, vÃ  HuggingFace TGI, vÃ  FlexGen Ä‘á»ƒ tÃ¡i táº¡o hiá»‡u suáº¥t cá»§a cÃ¡c framework bÃªn thá»© ba dÆ°á»›i cÃ¡c tÃ¬nh huá»‘ng thÃ­ nghiá»‡m.

Dá»¯ liá»‡u Ä‘áº§u ra. CÃ¡c script á»Ÿ trÃªn sáº½ táº¡o ra dá»¯ liá»‡u táº¡i Ä‘Æ°á»ng dáº«n FlexFlow/inference/output. Äá»‘i vá»›i má»—i tÃ¬nh huá»‘ng, má»™t tá»‡p .txt chá»©a Ä‘áº§u ra Ä‘Æ°á»£c táº¡o ra cho má»—i prompt, vÃ  má»™t tá»‡p .out chá»©a cÃ¡c log stdout. Cháº¥t lÆ°á»£ng cá»§a Ä‘áº§u ra Ä‘Æ°á»£c táº¡o ra cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ trá»±c quan vÃ  so sÃ¡nh vá»›i Ä‘áº§u ra tá»« cÃ¡c framework suy luáº­n bÃªn thá»© ba. ChÃºng tÃ´i cung cáº¥p script Ä‘á»ƒ phÃ¢n tÃ­ch dá»¯ liá»‡u Ä‘áº§u ra thÃ´ vÃ  táº¡o ra cÃ¡c tá»‡p CSV cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº¡o ra cÃ¡c hÃ¬nh trong bÃ i bÃ¡o. README cung cáº¥p táº¥t cáº£ chi tiáº¿t vá» cÃ¡c script vÃ  Ã¡nh xáº¡ giá»¯a cÃ¡c tá»‡p CSV vÃ  hÃ¬nh.

A.7 ÄÃ¡nh giÃ¡ vÃ  káº¿t quáº£ mong Ä‘á»£i
Dá»¯ liá»‡u tá»« cÃ¡c tá»‡p CSV nÃªn cho tháº¥y hiá»‡u suáº¥t tÆ°Æ¡ng tá»± vá»›i cÃ¡c hÃ¬nh tá»« bÃ i bÃ¡o. Má»™t sá»‘ biáº¿n Ä‘á»™ng lÃ  mong Ä‘á»£i, nhÆ°ng nhÃ¬n chung, SpecInfer nÃªn hoáº¡t Ä‘á»™ng theo HÃ¬nh 7-11 tá»« bÃ i bÃ¡o.

A.8 TÃ¹y chá»‰nh thÃ­ nghiá»‡m
NgÆ°á»i dÃ¹ng cÃ³ thá»ƒ chá»‰nh sá»­a cÃ¡c tham sá»‘ cáº¥u hÃ¬nh tá»« cÃ¡c script Ä‘Ã¡nh giÃ¡ Ä‘á»ƒ thay Ä‘á»•i cÃ¡c tham sá»‘ khÃ¡c nhau, cháº³ng háº¡n nhÆ° sá»‘ lÆ°á»£ng GPU/CPU, bá»™ nhá»› GPU/CPU, kÃ­ch thÆ°á»›c batch, mÃ´ hÃ¬nh LLM/SSM Ä‘Æ°á»£c sá»­ dá»¥ng, bá»™ dá»¯ liá»‡u prompt, Ä‘á»™ chÃ­nh xÃ¡c Ä‘áº§y Ä‘á»§ vs ná»­a, vÃ  sá»‘ lÆ°á»£ng token tá»‘i Ä‘a Ä‘á»ƒ táº¡o ra.

TÃ i liá»‡u tham kháº£o
[1] Jonathan Berant, Andrew Chou, Roy Frostig, vÃ  Percy Liang. PhÃ¢n tÃ­ch cÃº phÃ¡p ngá»¯ nghÄ©a trÃªn Freebase tá»« cÃ¡c cáº·p cÃ¢u há»i-tráº£ lá»i. Trong Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, trang 1533â€“1544, Seattle, Washington, USA, thÃ¡ng 10 nÄƒm 2013. Association for Computational Linguistics.

[2] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, vÃ  Yejin Choi. Piqa: Suy luáº­n vá» common sense váº­t lÃ½ trong ngÃ´n ngá»¯ tá»± nhiÃªn. Trong Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020.

[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, vÃ  Dario Amodei. CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lÃ  nhá»¯ng ngÆ°á»i há»c few-shot, 2020.

[4] F Warren Burton. TÃ­nh toÃ¡n suy Ä‘oÃ¡n, song song, vÃ  láº­p trÃ¬nh hÃ m. IEEE Transactions on Computers, 100(12):1190â€“1193, 1985.

[5] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, vÃ  John Jumper. TÄƒng tá»‘c giáº£i mÃ£ mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n vá»›i láº¥y máº«u suy Ä‘oÃ¡n. arXiv preprint arXiv:2302.01318, 2023.

[6] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, vÃ  Arvind Krishnamurthy. TVM: Má»™t trÃ¬nh biÃªn dá»‹ch tá»‘i Æ°u hÃ³a tá»± Ä‘á»™ng end-to-end cho deep learning. Trong 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), trang 578â€“594, 2018.

[7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Má»Ÿ rá»™ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ vá»›i pathways. arXiv preprint arXiv:2204.02311, 2022.

[8] Tim Dettmers, Mike Lewis, Younes Belkada, vÃ  Luke Zettlemoyer. Gpt3.int8(): nhÃ¢n ma tráº­n 8-bit cho transformer á»Ÿ quy mÃ´ lá»›n. Advances in Neural Information Processing Systems, 35:30318â€“30332, 2022.

[9] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Má»Ÿ rá»™ng hiá»‡u quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ vá»›i mixture-of-experts. Trong International Conference on Machine Learning, trang 5547â€“5569. PMLR, 2022.

[10] Elias Frantar vÃ  Dan Alistarh. Sparsegpt: CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n cÃ³ thá»ƒ Ä‘Æ°á»£c cáº¯t tá»‰a chÃ­nh xÃ¡c trong má»™t láº§n, 2023.

[11] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, vÃ  Dan Alistarh. Gptq: LÆ°á»£ng tá»­ hÃ³a chÃ­nh xÃ¡c cho transformer sinh táº¡o Ä‘Æ°á»£c pre-train. Trong International Conference on Learning Representations, 2023.

[12] Yoav Freund, Robert Schapire, vÃ  Naoki Abe. Má»™t giá»›i thiá»‡u ngáº¯n gá»n vá» boosting. Journal-Japanese Society For Artificial Intelligence, 14(771-780):1612, 1999.

[13] Freddy Gabbay vÃ  Avi Mendelson. Thá»±c thi suy Ä‘oÃ¡n dá»±a trÃªn dá»± Ä‘oÃ¡n giÃ¡ trá»‹. Citeseer, 1996.

[14] Mudasir A Ganaie, Minghui Hu, AK Malik, M Tanveer, vÃ  PN Suganthan. Ensemble deep learning: Má»™t Ä‘Ã¡nh giÃ¡. Engineering Applications of Artificial Intelligence, 115:105151, 2022.

[15] Aaron Gokaslan*, Vanya Cohen*, Ellie Pavlick, vÃ  Stefanie Tellex. Openwebtext corpus. http://Skylion007.github.io/OpenWebTextCorpus, 2019.

[16] John L Hennessy vÃ  David A Patterson. Kiáº¿n trÃºc mÃ¡y tÃ­nh: má»™t cÃ¡ch tiáº¿p cáº­n Ä‘á»‹nh lÆ°á»£ng. Elsevier, 2011.

[17] Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, Joseph Naor, vÃ  Daniel Soudry. Huáº¥n luyá»‡n máº¡ng neural thÆ°a thá»›t Ä‘Æ°á»£c tÄƒng tá»‘c: Má»™t phÆ°Æ¡ng phÃ¡p cÃ³ thá»ƒ chá»©ng minh vÃ  hiá»‡u quáº£ Ä‘á»ƒ tÃ¬m mask chuyá»ƒn vá»‹ n:m. Advances in Neural Information Processing Systems, 34:21099â€“21111, 2021.

[18] HuggingFace. Large language model text generation inference. https://github.com/huggingface/text-generation-inference. (Truy cáº­p ngÃ y 08/09/2023).

[19] Hugging Face Inc. Hugging face. https://huggingface.co, 2023.

[20] Zhihao Jia, Oded Padon, James Thomas, Todd Warszawski, Matei Zaharia, vÃ  Alex Aiken. Taso: tá»‘i Æ°u hÃ³a tÃ­nh toÃ¡n deep learning vá»›i táº¡o ra tá»± Ä‘á»™ng cÃ¡c thay tháº¿ Ä‘á»“ thá»‹. Trong Proceedings of the 27th ACM Symposium on Operating Systems Principles, trang 47â€“62, 2019.

[21] Zhihao Jia, Matei Zaharia, vÃ  Alex Aiken. VÆ°á»£t ra ngoÃ i song song dá»¯ liá»‡u vÃ  mÃ´ hÃ¬nh cho máº¡ng neural sÃ¢u. Trong Proceedings of the 2nd Conference on Systems and Machine Learning, SysML'19, 2019.

[22] Joao Gante. Assisted generation: má»™t hÆ°á»›ng má»›i hÆ°á»›ng tá»›i táº¡o vÄƒn báº£n Ä‘á»™ trá»… tháº¥p, 2023.

[23] Sehoon Kim, Karttikeya Mangalam, Suhong Moon, John Canny, Jitendra Malik, Michael W. Mahoney, Amir Gholami, vÃ  Kurt Keutzer. Big little transformer decoder, 2023.

[24] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Yu, Joseph E Gonzalez, Hao Zhang, vÃ  Ion Stoica. vllm: Phá»¥c vá»¥ llm dá»… dÃ ng, nhanh chÃ³ng vÃ  ráº» tiá»n vá»›i pagedattention. Xem https://vllm.ai/ (truy cáº­p 9 thÃ¡ng 8 nÄƒm 2023), 2023.

[25] Yaniv Leviathan, Matan Kalman, vÃ  Yossi Matias. Suy luáº­n nhanh tá»« transformer thÃ´ng qua giáº£i mÃ£ suy Ä‘oÃ¡n. arXiv preprint arXiv:2211.17192, 2022.

[26] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, vÃ  Weizhu Chen. Äiá»u gÃ¬ lÃ m cho cÃ¡c vÃ­ dá»¥ trong ngá»¯ cáº£nh tá»‘t cho gpt-3? arXiv preprint arXiv:2101.06804, 2021.

[27] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Hongyi Jin, Tianqi Chen, vÃ  Zhihao Jia. HÆ°á»›ng tá»›i phá»¥c vá»¥ mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n sinh táº¡o hiá»‡u quáº£: Má»™t kháº£o sÃ¡t tá»« thuáº­t toÃ¡n Ä‘áº¿n há»‡ thá»‘ng. arXiv preprint arXiv:2312.15234, 2023.

[28] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, vÃ  Zhihao Jia. Specinfer: TÄƒng tá»‘c phá»¥c vá»¥ llm sinh táº¡o vá»›i suy luáº­n suy Ä‘oÃ¡n vÃ  xÃ¡c minh cÃ¢y token. arXiv preprint arXiv:2305.09781, 2023.

[29] Xupeng Miao, Chunan Shi, Jiangfei Duan, Xiaoli Xi, Dahua Lin, Bin Cui, vÃ  Zhihao Jia. Spotserve: Phá»¥c vá»¥ mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n sinh táº¡o trÃªn cÃ¡c instance cÃ³ thá»ƒ bá»‹ ngáº¯t. arXiv preprint arXiv:2311.15566, 2023.

[30] MohamedRashad. Chatgpt-prompts. https://huggingface.co/datasets/MohamedRashad/ChatGPT-prompts, 2023.

[31] Xuan-Phi Nguyen, Shafiq Joty, Steven Hoi, vÃ  Richard Socher. Tree-structured attention vá»›i tÃ­ch lÅ©y phÃ¢n cáº¥p. Trong International Conference on Learning Representations, 2020.

[32] NVIDIA. Fastertransformer. https://github.com/NVIDIA/FasterTransformer. (Truy cáº­p ngÃ y 08/09/2023).

[33] OpenAI. BÃ¡o cÃ¡o ká»¹ thuáº­t gpt-4, 2023.

[34] Alessandro Palla. chatbot instruction prompts. https://huggingface.co/datasets/alespalla/chatbot_instruction_prompts, 2023.

[35] Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, vÃ  Dongsoo Lee. nuqmm: NhÃ¢n ma tráº­n lÆ°á»£ng tá»­ hÃ³a cho suy luáº­n hiá»‡u quáº£ cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯ sinh táº¡o quy mÃ´ lá»›n. arXiv preprint arXiv:2206.09557, 2022.

[36] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, vÃ  Jianfeng Gao. Instruction tuning vá»›i gpt-4. arXiv preprint arXiv:2304.03277, 2023.

[37] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Má»Ÿ rá»™ng mÃ´ hÃ¬nh ngÃ´n ngá»¯: PhÆ°Æ¡ng phÃ¡p, phÃ¢n tÃ­ch & insight tá»« huáº¥n luyá»‡n gopher. arXiv preprint arXiv:2112.11446, 2021.

[38] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana IliÄ‡, Daniel Hesslow, Roman CastagnÃ©, Alexandra Sasha Luccioni, FranÃ§ois Yvon, Matthias GallÃ©, et al. Bloom: Má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘a ngÃ´n ngá»¯ truy cáº­p má»Ÿ 176b-parameter. arXiv preprint arXiv:2211.05100, 2022.

[39] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gonzalez, Percy Liang, Christopher RÃ©, Ion Stoica, vÃ  Ce Zhang. Flexgen: Suy luáº­n sinh táº¡o thÃ´ng lÆ°á»£ng cao cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n vá»›i má»™t gpu duy nháº¥t, 2023.

[40] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gonzalez, Percy Liang, Christopher RÃ©, Ion Stoica, vÃ  Ce Zhang. Suy luáº­n sinh táº¡o thÃ´ng lÆ°á»£ng cao cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n vá»›i má»™t gpu duy nháº¥t, 2023.

[41] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, vÃ  Bryan Catanzaro. Megatron-lm: Huáº¥n luyá»‡n mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘a tá»· tham sá»‘ báº±ng cÃ¡ch sá»­ dá»¥ng song song mÃ´ hÃ¬nh, 2020.

[42] James E Smith. Má»™t nghiÃªn cá»©u vá» cÃ¡c chiáº¿n lÆ°á»£c dá»± Ä‘oÃ¡n nhÃ¡nh. Trong 25 years of the international symposia on Computer architecture (selected papers), trang 202â€“215, 1998.

[43] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Sá»­ dá»¥ng deepspeed vÃ  megatron Ä‘á»ƒ huáº¥n luyá»‡n megatron-turing nlg 530b, má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ sinh táº¡o quy mÃ´ lá»›n. arXiv preprint arXiv:2201.11990, 2022.

[44] Mitchell Stern, Noam Shazeer, vÃ  Jakob Uszkoreit. Giáº£i mÃ£ song song theo khá»‘i cho mÃ´ hÃ¬nh tá»± há»“i quy sÃ¢u. Advances in Neural Information Processing Systems, 31, 2018.

[45] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, vÃ  Tatsunori B. Hashimoto. Stanford alpaca: Má»™t mÃ´ hÃ¬nh llama tuÃ¢n theo hÆ°á»›ng dáº«n. https://github.com/tatsu-lab/stanford_alpaca, 2023.

[46] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: MÃ´ hÃ¬nh ngÃ´n ngá»¯ ná»n táº£ng má»Ÿ vÃ  hiá»‡u quáº£. arXiv preprint arXiv:2302.13971, 2023.

[47] Colin Unger, Zhihao Jia, Wei Wu, Sina Lin, Mandeep Baines, Carlos Efrain Quintero Narvaez, Vinay Ramakrishnaiah, Nirmal Prajapati, Pat McCormick, Jamaludin Mohd-Yusof, Xi Luo, Dheevatsa Mudigere, Jongsoo Park, Misha Smelyanskiy, vÃ  Alex Aiken. Unity: TÄƒng tá»‘c huáº¥n luyá»‡n DNN thÃ´ng qua tá»‘i Æ°u hÃ³a káº¿t há»£p cÃ¡c biáº¿n Ä‘á»•i Ä‘áº¡i sá»‘ vÃ  song song hÃ³a. Trong 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), trang 267â€“284, Carlsbad, CA, thÃ¡ng 7 nÄƒm 2022. USENIX Association.

[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, vÃ  Illia Polosukhin. Attention is all you need, 2017.

[49] Hanrui Wang, Zhekai Zhang, vÃ  Song Han. Spatten: Kiáº¿n trÃºc attention thÆ°a thá»›t hiá»‡u quáº£ vá»›i cascade token vÃ  head pruning. Trong 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA), trang 97â€“110. IEEE, 2021.

[50] Haojie Wang, Jidong Zhai, Mingyu Gao, Zixuan Ma, Shizhi Tang, Liyan Zheng, Yuanzhi Li, Kaiyuan Rong, Yuanyong Chen, vÃ  Zhihao Jia. PET: Tá»‘i Æ°u hÃ³a chÆ°Æ¡ng trÃ¬nh tensor vá»›i cÃ¡c biáº¿n Ä‘á»•i tÆ°Æ¡ng Ä‘Æ°Æ¡ng má»™t pháº§n vÃ  sá»­a chá»¯a tá»± Ä‘á»™ng. Trong 15th USENIX Symposium on Operating Systems Design and Implementation (OSDI 21), trang 37â€“54. USENIX Association, thÃ¡ng 7 nÄƒm 2021.

[51] Heming Xia, Tao Ge, Si-Qing Chen, Furu Wei, vÃ  Zhifang Sui. Giáº£i mÃ£ suy Ä‘oÃ¡n: TÄƒng tá»‘c khÃ´ng máº¥t mÃ¡t cá»§a dá»‹ch tá»± há»“i quy.

[52] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, vÃ  Song Han. Smoothquant: LÆ°á»£ng tá»­ hÃ³a sau huáº¥n luyá»‡n chÃ­nh xÃ¡c vÃ  hiá»‡u quáº£ cho mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n. arXiv preprint arXiv:2211.10438, 2022.

[53] Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, vÃ  Furu Wei. Suy luáº­n vá»›i tham chiáº¿u: TÄƒng tá»‘c khÃ´ng máº¥t mÃ¡t cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n. arXiv preprint arXiv:2304.04487, 2023.

[54] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, vÃ  Yuxiong He. Zeroquant: LÆ°á»£ng tá»­ hÃ³a sau huáº¥n luyá»‡n hiá»‡u quáº£ vÃ  giÃ¡ cáº£ pháº£i chÄƒng cho transformer quy mÃ´ lá»›n. Advances in Neural Information Processing Systems, 35:27168â€“27183, 2022.

[55] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, vÃ  Byung-Gon Chun. Orca: Má»™t há»‡ thá»‘ng phá»¥c vá»¥ phÃ¢n tÃ¡n cho mÃ´ hÃ¬nh sinh táº¡o dá»±a trÃªn Transformer. Trong 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), trang 521â€“538, Carlsbad, CA, thÃ¡ng 7 nÄƒm 2022. USENIX Association.

[56] Haoyu Zhang, Jianjun Xu, vÃ  Ji Wang. Táº¡o ra ngÃ´n ngá»¯ tá»± nhiÃªn dá»±a trÃªn pretraining cho tÃ³m táº¯t vÄƒn báº£n. arXiv preprint arXiv:1902.09243, 2019.

[57] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: MÃ´ hÃ¬nh ngÃ´n ngá»¯ transformer Ä‘Æ°á»£c pre-train má»Ÿ. arXiv preprint arXiv:2205.01068, 2022.

[58] Lianmin Zheng, Chengfan Jia, Minmin Sun, Zhao Wu, Cody Hao Yu, Ameer Haj-Ali, Yida Wang, Jun Yang, Danyang Zhuo, Koushik Sen, et al. Ansor: Táº¡o ra chÆ°Æ¡ng trÃ¬nh tensor hiá»‡u suáº¥t cao cho deep learning. Trong 14th{USENIX}Symposium on Operating Systems Design and Implementation ({OSDI}20), trang 863â€“879, 2020.
