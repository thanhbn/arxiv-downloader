# 2401.07851.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/speculative/2401.07851.pdf
# File size: 1986392 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Unlocking Efficiency in Large Language Model Inference:
A Comprehensive Survey of Speculative Decoding
Heming Xia1, Zhe Yang2, Qingxiu Dong2, Peiyi Wang2,
Yongqi Li1,Tao Ge3,Tianyu Liu4,Wenjie Li1,Zhifang Sui2
1Department of Computing, The Hong Kong Polytechnic University
2National Key Laboratory for Multimedia Information Processing, Peking University
3Microsoft Research Asia4Alibaba Group
{he-ming.xia}@connect.polyu.hk; {yz_young}@pku.edu.cn
Abstract
To mitigate the high inference latency stem-
ming from autoregressive decoding in Large
Language Models (LLMs), Speculative Decod-
ing has emerged as a novel decoding paradigm
for LLM inference. In each decoding step, this
method first drafts several future tokens effi-
ciently and then verifies them in parallel. Un-
like autoregressive decoding, Speculative De-
coding facilitates the simultaneous decoding of
multiple tokens per step, thereby accelerating
inference. This paper presents a comprehensive
overview and analysis of this promising decod-
ing paradigm. We begin by providing a formal
definition and formulation of Speculative De-
coding. Then, we organize in-depth discussions
on its key facets, such as drafter selection and
verification strategies. Furthermore, we present
a comparative analysis of leading methods un-
der third-party testing environments. We aim
for this work to serve as a catalyst for further
research on Speculative Decoding, ultimately
contributing to more efficient LLM inference.1
1 Introduction
Large Language Models (LLMs) have achieved
remarkable proficiency in a range of downstream
tasks (OpenAI, 2023; Touvron et al., 2023a,b; Chi-
ang et al., 2023; Jiang et al., 2023). They are pro-
gressively evolving as the cornerstone of compre-
hensive API interfaces (e.g., ChatGPT2), offering
human life services and guidance through real-time
human-machine interactions. However, the infer-
ence latency of these sizable models has emerged
as a substantial obstacle restricting their broader
applications. This latency primarily arises from
the token-by-token generation necessitated by au-
toregressive decoding, resulting in an escalation of
the inference latency with both the length of the
generated sequence and the model’s scale.
1The relevant papers will be regularly updated at https:
//github.com/hemingkx/SpeculativeDecodingPapers .
2https://chat.openai.com
Verify in  Parallel
Efficiently  Draft
Autoregressive
Decoding
Figure 1: In contrast to autoregressive decoding ( left)
that generates sequentially, Speculative Decoding ( right )
firstefficiently drafts multiple tokens and then verifies
them in parallel using the target LLM. Drafted tokens
after the bifurcation position ( e.g.,
 ) will be discarded
to guarantee the generation quality.
To accelerate LLM inference, an innovative in-
ference paradigm, Speculative Decoding has been
introduced (Stern et al., 2018; Xia et al., 2023;
Leviathan et al., 2023; Chen et al., 2023a). As
shown in Figure 1, in each decoding step, Specula-
tive Decoding first efficiently drafts multiple tokens
as speculation of future decoding steps of the target
LLM and then utilizes the LLM to verify all drafted
tokens in parallel. Only those tokens that meet the
LLM’s verification criterion are accepted as final
outputs to guarantee generation quality.
Speculative Decoding is founded upon two key
observations about LLM inference: 1) many easy
tokens can be predicted with less computational
overhead (e.g., using a smaller model), and 2) LLM
inference is highly memory bandwidth bound (Pat-
terson, 2004; Shazeer, 2019) with the main latency
bottleneck arising from memory reads/writes of
LLM parameters rather than arithmetic computa-
tions. Drawing on these observations, Speculative
Decoding adapts the concept of speculative execu-
tion3to focus LLMs’ efforts on the validation of
3Speculative execution (Burton, 1985; Hennessy and Pat-
terson, 2012) is an optimization technique used in computer
architecture where tasks are performed in advance and subse-
quently verified for their necessity, thereby circumventing the
delays inherent in sequential task execution.arXiv:2401.07851v3  [cs.CL]  4 Jun 2024

--- PAGE 2 ---
2022.11
Speculative Decoding
2023.02Speculative Sampling
2023.05Parallel DecodingSpecInfer
Assistant Generation
2023.08
PPD
SpecTrStagedSpec2023.09Medusa
Draft & Verify
BiLD
LLMCad
2018Blockwise Decoding
2021
Aggressive Decoding2022.03Speculative Decoding
Proposing the draft-then-
verify paradigm with
specialized drafting headsProposing the concept of "Speculative
Decoding" with an independent Non-
Auto LM as the drafter
Using off-the-shelf small LMs
for drafting and supporting
speculative samplingApplying the paradigm to LLM
inference and supporting sampling
as concurrent work
~2023.04
LLMA
Figure 2: Timeline illustrating the evolution of Speculative Decoding. After 2022, Speculative Decoding was
formally introduced as a general decoding paradigm to accelerate LLM inference and garnered widespread attention.
pre-drafted tokens, substantially diminishing the
need for frequent memory operations of LLM pa-
rameters, thereby improving inference efficiency.
While Speculative Decoding shows promise, it
raises several critical questions that warrant further
investigation. For instance, how to design an opti-
mal drafter to strike a balance between speculation
accuracy and drafting efficiency (Xia et al., 2023;
Zhou et al., 2023; Li et al., 2024). Additionally, it
is essential to assess whether the verification crite-
rion can maintain both generation parallelism and
output quality (Miao et al., 2024; Cai et al., 2024).
Furthermore, since existing methods are evaluated
under disparate testing conditions, a unified bench-
mark is needed to facilitate realistic speedup expec-
tations within the research community.
Amid the rapid expansion of research in Specula-
tive Decoding, this work makes the first attempt to
present a survey of this field, aiming to raise aware-
ness among academics about the latest advance-
ments. We provide a systematic categorization of
current research and an in-depth analysis of rele-
vant studies. Moreover, we introduce Spec-Bench,
a comprehensive benchmark to assess Speculative
Decoding methods in diverse application scenarios.
Our contributions can be summarized as follows:
(1)First survey: To our knowledge, we are the
first to present a comprehensive survey on
Speculative Decoding;
(2)Formal definition: We furnish a formal defini-
tion and formulation of Speculative Decoding,
laying the groundwork for future research.
(3)New taxonomy: We provide a systematic tax-
onomy for Speculative Decoding, offering an
organized categorization of existing work.
(4)Spec-Bench: We introduce Spec-Bench, an
extensive benchmark designed for assessing
Speculative Decoding, enabling a comparative
evaluation of leading methodologies.We hope that this work can serve as an essential
guide for newcomers and motivate future research.
2 Overview
This paper offers a comprehensive survey of Specu-
lative Decoding. We commence by introducing the
early stages of Speculative Decoding research (§3),
illustrated by a timeline of its evolution (as shown
in Figure 2). This is followed by a formal defini-
tion and formulation of Speculative Decoding (§4).
Then, we delve into a detailed discussion of leading
techniques, including the selection of draft mod-
els (§5), verification strategies (§6), and alignment
between the drafter and the target LLM (§7). More-
over, we introduce Spec-Bench, an extensive evalu-
ation benchmark designed for assessing the accel-
eration effect of Speculative Decoding (§8).
3 Evolution of Speculative Decoding
This section discusses the motivation behind Specu-
lative Decoding (§3.1) and then provides a detailed
introduction to early attempts in this field (§3.2).
3.1 Motivation
The widespread adoption of LLMs has established
autoregressive decoding as the de facto standard to
LLM inference (Chowdhery et al., 2023; OpenAI,
2023; Jiang et al., 2024). However, autoregressive
decoding is limited by its inference latency, which
primarily stems from the memory-bound compu-
tation of LLMs (Patterson, 2004; Shazeer, 2019).
Specifically, the main latency bottleneck of each
decoding step is not due to computational opera-
tions but arises from the necessity to transfer all
LLM parameters from High-Bandwidth Memory
(HBM) to the on-chip cache of modern accelerators
like GPUs. This process, which generates only one
token per step, leads to the underutilization of these
accelerators and results in inefficiencies.

--- PAGE 3 ---
Algorithm 1 Autoregressive Decoding
Require: Language model Mq, input sequence x1, . . . , x t,
and target sequence length T;
1:initialize n←t
2:while n < T do
3: Set qn+1← M q(x|x<n+1)
4: Sample xn+1∼qn+1
5: n←n+ 1
6:end while
3.2 Pioneering Draft-then-Verify Efforts
To mitigate the above issue, an intuitive way in-
volves leveraging idle computational resources to
enhance parallelism in LLM inference. To this end,
Stern et al. (2018) introduced Blockwise Decoding,
an approach that incorporates extra feedforward
neural (FFN) heads atop the Transformer decoder,
enabling the simultaneous drafting of multiple to-
kens per step. These tokens are then verified by the
original LLM in parallel , ensuring that the outputs
align with those of the original LLM. As a pioneer-
ing work proposing the Draft-then-Verify paradigm,
Blockwise Decoding effectively reduces the num-
ber of required LLM calls by increasing generation
parallelism, thereby accelerating inference.
To further unleash the potential of this paradigm,
Xia et al. (2023) introduced Speculative Decoding
(SpecDec), which utilizes an independent drafter,
notably a specialized Non-Autoregressive Trans-
former, to perform the drafting task both accurately
and efficiently. Moreover, this method presented
an innovative strategy that relaxes the rigid ver-
ification criterion, thereby increasing the accep-
tance rate of drafted tokens. Impressively, SpecDec
achieves around 5 ×speedup over autoregressive
decoding with comparable quality, underscoring
the substantial potential of Speculative Decoding.
Following SpecDec, Leviathan et al. (2023) and
Chen et al. (2023a) made concurrent contributions
by proposing Speculative Sampling, expanding this
paradigm to encompass the lossless acceleration
of various sampling methods. These approaches
employed smaller LMs from the same series (e.g.,
T5-small ) to speed up the inference of their larger
counterparts (e.g., T5-XXL ). Unlike previous work,
these off-the-shelf small LMs do not require ad-
ditional training, enabling the rapid adoption of
Speculative Decoding in LLM acceleration. This
advancement has elevated Speculative Decoding to
the forefront of LLM efficiency research, attracting
widespread interest within the NLP community.
To sum up, these pioneering efforts in Specula-Algorithm 2 Speculative Decoding
Require: Target language model Mq, draft model Mp, in-
put sequence x1, . . . , x t, block size K, target sequence
length T, drafting strategy DRAFT , verification criterion
VERIFY , and correction strategy C ORRECT ;
1:initialize n←t
2:while n < T do
//Drafting: obtain distributions from Mpefficiently
3: Set p1, . . . , p K←DRAFT (x≤n,Mp)
//Drafting: sample Kdrafted tokens
4: Sample exi∼pi, i= 1, . . . , K
//Verification: compute K+1distributions in parallel
5: Set qi← M q(x|x≤n,ex<i), i= 1, . . . , K + 1
//Verification: verify each drafted token
6: fori= 1 : Kdo
7: ifVERIFY (exi, pi, qi)then
8: Set xn+i←exiandn←n+ 1
9: else
10: xn+i←CORRECT (pi, qi)
11: and Exit for loop.
12: end if
13: end for
14: If all drafted tokens are accepted, sample next token
xn+1∼qK+1and set n←n+ 1.
15:end while
tive Decoding have gradually solidified the Draft-
then-Verify paradigm, showcasing its promising
potential in LLM acceleration. We provide a de-
tailed categorization and discussion of these studies
and subsequent research in the following sections.
4 Formulation and Definition
In this section, we first provide a concise overview
of standard autoregressive decoding (§4.1). Then,
we offer an in-depth exposition of Speculative De-
coding (§4.2), which encompasses a formal defini-
tion, a comprehensive description of the methodol-
ogy, and a detailed elaboration of the algorithm.
4.1 Autoregressive Decoding
Transformer-based LLMs typically make genera-
tions in an autoregressive manner. Given an input
sequence x1, . . . , x t, an autoregressive language
model Mqgenerates the next token according to:
xt+1∼qt+1=Mq(x|x<t+1), (1)
where qis the conditional probability distribution
calculated by Mqandxt+1denotes the next token
sampled from qt+1. We illustrate a detailed process
in Algorithm 1.
As discussed in Section 3, while the standard au-
toregressive decoding offers desirable generation
quality, it is bounded by memory bandwidth, result-
ing in low utilization of modern accelerators. In
this process, each memory-bound LLM call (i.e.,
an LLM forward step) produces merely a single

--- PAGE 4 ---
Speculative Decoding
Drafting (§5)Independent
Drafting (§5.1)Fine-tuned
DrafterSpecDec (Xia et al., 2023), BiLD (Kim et al., 2023), SpecInfer (Miao et al., 2024),
Online Speculative (Liu et al., 2023), DistillSpec (Zhou et al., 2023)
Tuning-free
DrafterSpeculative Decoding (Leviathan et al., 2023), StagedSpec (Spector and Re, 2023),
SpS (Chen et al., 2023a), SpecTr (Sun et al., 2023), REST (He et al., 2023),
CS. Drafting (Chen et al., 2023b), MCSD (Yang et al., 2024)
Self-Drafting (§5.2)FFN Heads Blockwise (Stern et al., 2018), Medusa (Cai et al., 2024), EAGLE (Li et al., 2024)
Early ExitingPPD (Yang et al., 2023b), Self-Speculative (Zhang et al., 2023a),
SPEED (Hooper et al., 2023)
Mask-PredictParallel Decoding (Santilli et al., 2023), Lookahead Decoding (Fu et al., 2024),
PaSS (Monea et al., 2023)Verification (§6)Greedy
Decoding (§6.1)LosslessBlockwise (Stern et al., 2018), SpecDec (Xia et al., 2023), Parallel Decoding
(Santilli et al., 2023), PPD (Yang et al., 2023b), SPEED (Hooper et al., 2023),
Self-Speculative (Zhang et al., 2023a), Lookahead Decoding (Fu et al., 2024)
Approximate Blockwise (Stern et al., 2018), SpecDec (Xia et al., 2023), BiLD (Kim et al., 2023)
Speculative
Sampling (§6.2)LosslessSpeculative Decoding (Leviathan et al., 2023), DistillSpec (Zhou et al., 2023),
Online Speculative (Liu et al., 2023), SpS (Chen et al., 2023a), CS. Drafting
(Chen et al., 2023b), PaSS (Monea et al., 2023), MCSD (Yang et al., 2024)
Approximate Speculative Decoding (Leviathan et al., 2023), DistillSpec (Zhou et al., 2023)
Token Tree
Verification (§6.3)SpecInfer (Miao et al., 2024), StagedSpec (Spector and Re, 2023), SpecTr (Sun et al., 2023),
REST (He et al., 2023), Medusa (Cai et al., 2024), EAGLE (Li et al., 2024)
Figure 3: Taxonomy of Speculative Decoding.
token for the entire sequence, making the whole
generation inefficient and time-consuming.
4.2 Speculative Decoding
Following Xia et al. (2023), Leviathan et al. (2023),
and Chen et al. (2023a), we here provide a formal
definition of Speculative Decoding:
Speculative Decoding is a Draft-then-
Verify decoding paradigm in which, at
each decoding step, it first efficiently
drafts multiple future tokens and then
verifies all these tokens in parallel using
the target LLM to speed up inference.
We formulate a detailed Speculative Decoding
process in Algorithm 2. Subsequently, we delve
into the two fundamental substeps integral to this
paradigm – drafting andverification :
Drafting At each decoding step, Speculative De-
coding first efficiently drafts multiple future tokens,
as a speculation of the target LLM’s output. For-
mally, given an input sequence x1, . . . , x tand the
target LLM Mq, this paradigm employs an effi-
cient draft model Mp(e.g., a smaller LM) to de-
code the next Kdrafted tokens:
p1, . . . , p K=DRAFT (x≤t,Mp),
exi∼pi, i = 1, . . . , K,(2)where DRAFT (·)denotes various drafting strate-
gies that we will discuss in Section 5, pis the con-
ditional probability distribution calculated by Mp,
andexidenotes the drafted token sampled from pi.
Verification Subsequently, these drafted tokens
are verified by the target LLM Mqin parallel. For-
mally, given the input sequence x1, . . . , x tand the
draftex1, . . . ,exK, Speculative Decoding utilizes
Mqto compute K+ 1 probability distributions
simultaneously:
qi=Mq(x|x≤t,ex<i), i= 1, . . . , K + 1.(3)
Then, each drafted token exiis verified by a spe-
cific criterion VERIFY (exi, pi, qi). Only those to-
kens that meet the criterion are selected as final
outputs, ensuring quality consistent with the target
LLM’s standards. Otherwise, the first drafted token
excthat fails the verification will be corrected by
the strategy CORRECT (pc, qc). All drafted tokens
after position cwill be discarded, to guarantee the
high quality of the final outputs. If all tokens pass
verification, an additional token xt+K+1will be
sampled from qK+1as Eq. (1).
The drafting and verification substeps will be
iterated until the termination condition is met, i.e.,
the[EOS] token is decoded or the sentence reaches
the maximal length.
Notably, the acceleration effect of Speculative
Decoding primarily hinges on the acceptance rate

--- PAGE 5 ---
Methods D RAFT (x≤t,Mp) Drafter Type
Parallel
Draftingp1, . . . , p K=Mp(x|x≤t)FFN Heads (Stern et al., 2018; Cai et al., 2024), Non-Autoregressive
LM (Xia et al., 2023), Mask-Predict (Santilli et al., 2023; Fu et al., 2024)
Autoregressive
Draftingpi=Mp(x|x≤t,ex<i), i=
1, . . . , KSmall LMs (Leviathan et al., 2023; Chen et al., 2023a), Early Exiting (Yang
et al., 2023b), Layer Skipping (Zhang et al., 2023a)
Table 1: Summary of formulations for various drafting strategies in Speculative Decoding. We categorize these
methods into two distinct groups based on their formulations: parallel drafting andautoregressive drafting .
of drafted tokens at each step. This rate is influ-
enced by several factors, including the draft quality,
verification criteria, and the behavior alignment be-
tween the drafter and the target LLM. Additionally,
the intrinsic efficiency of the drafter itself also con-
tributes to the overall speedup. In subsequent sec-
tions, we will delve into these pivotal components
of Speculative Decoding, as depicted in Figure 3,
to systematically categorize recent research trends
within this promising paradigm.
5
 Drafting
As a vital component of Speculative Decoding,
the drafting process has a crucial impact on the
speedup of the paradigm. The impact is determined
by two key factors: the speculation accuracy of
the drafter Mp, measured by the average number
of accepted tokens per step, and the drafting la-
tency (Stern et al., 2018; Xia et al., 2023). How to
trade off high speculation accuracy and low draft-
ing latency presents a major challenge in this pro-
cess. In this section, we classify various drafting
strategies into two categories: independent draft-
ing (§5.1) and self-drafting (§5.2), and summarize
their formulations D RAFT (x≤t,Mp)in Table 1.
5.1 Independent Drafting
To strike a balance between speculation accuracy
and efficiency, SpecDec (Xia et al., 2023) first
proposed utilizing an independent model for draft-
ing. Specifically, it employed a specialized Non-
Autoregressive Transformer that drafts multiple to-
kens simultaneously per step. This model has a
deep-shallow encoder-decoder architecture to run
efficiently. Despite its strengths, SpecDec requires
training a draft model from scratch, which demands
an increased computational budget.
Considering the available models in existing
LLM series (e.g., OPT (Zhang et al., 2022) and
LLaMA (Touvron et al., 2023a,b)), a more straight-
forward and efficient approach is directly employ-
ing a small LM from the same series as the drafterto accelerate the inference of its larger counter-
parts (Leviathan et al., 2023; Chen et al., 2023a;
Spector and Re, 2023; Sun et al., 2023; Chen et al.,
2023b). For instance, Leviathan et al. (2023) uti-
lized T5-small as the drafter, to accelerate the in-
ference of T5-XXL . These off-the-shelf small LMs
do not require additional training or any modifica-
tion on model architectures, facilitating the quick
adoption of Speculative Decoding. Moreover, since
models in the same series share tokenizers, pretrain-
ing corpora, and similar training processes, they
inherently have an alignment in prediction behav-
iors.
5.2 Self-Drafting
While leveraging an external draft model offers
considerable advantages, this approach necessitates
extra effort to either train or identify a draft model
that closely aligns with the target LLM. This chal-
lenge is intensified when no smaller counterparts
of the LLM are available, e.g., LLaMA-7B (Tou-
vron et al., 2023a,b). Furthermore, integrating two
distinct models within a single system introduces
additional computational complexity, particularly
in distributed settings (Cai et al., 2024).
To address the above issues, numerous studies
have suggested leveraging the target LLM itself
for efficient drafting (Stern et al., 2018; Santilli
et al., 2023; Hooper et al., 2023; Cai et al., 2024;
Fu et al., 2024; Du et al., 2024). Particularly, Block-
wise Decoding (Stern et al., 2018) and Medusa (Cai
et al., 2024) incorporated FFN heads atop the Trans-
former decoder, enabling the parallel token gener-
ation per step. Compared with external drafters,
these lightweight heads reduce extra computational
overhead and are friendly to distributed inference.
Another line of research has explored the potential
ofearly exiting andlayer skipping within the target
LLM for drafting (Yang et al., 2023b; Zhang et al.,
2023a; Hooper et al., 2023). For instance, Yang
et al. (2023b) introduced additional subprocesses
that exit early during the current decoding step,
thereby initiating the drafting of future tokens in

--- PAGE 6 ---
Methods V ERIFY (exi, pi, qi) CORRECT (pc, qc) Representative Work
Greedy
Decodingexi= arg max qi xt+c←arg max qcBlockwise Decoding (Stern et al.,
2018), SpecDec (Xia et al., 2023)
Speculative
Samplingr <min
1,qi(exi)
pi(exi)
, r∼U[0,1] xt+c∼norm(max (0 , qc−pc))Speculative Decoding (Leviathan
et al., 2023), SpS (Chen et al., 2023a)
Table 2: Summary of formulations for various verification strategies in Speculative Decoding.
advance. Similarly, Self-Speculative (Zhang et al.,
2023a) proposed to adaptively skip several interme-
diate layers during inference to draft efficiently.
In contrast to prior work that focused on extend-
ing model architectures or altering the inference
process, Santilli et al. (2023) introduced a sim-
ple drafting strategy that directly appends multiple
[PAD] tokens to the end of the input prompt to en-
able parallel generation. However, this approach
deviates from LLMs’ autoregressive pretraining
pattern, leading to suboptimal drafting quality. To
tackle this, Fu et al. (2024) proposed transforming
low-quality drafts into multiple n-grams to improve
the speculation accuracy; Monea et al. (2023) intro-
duced multiple learnable [LA] tokens and finetuned
these token embeddings on a small training dataset
to enhance the parallel decoding performance.
6
 Verification
In each decoding step, the drafted tokens are ver-
ified in parallel to ensure the outputs align with
the target LLM. This process also determines the
number of tokens accepted per step, a vital factor
impacting the speedup. This section summarizes
various verification criteria VERIFY (exi, pi, qi)(as
shown in Table 2), encompassing those support-
ing greedy decoding (§6.1) and speculative sam-
pling (§6.2) in LLM inference. Besides, we in-
troduce token tree verification (§6.3), an effective
strategy to increase the token acceptance rate.
6.1 Greedy Decoding
Early attempts at Speculative Decoding focused on
the verification criterion supporting greedy decod-
ing, which guarantees that the outputs are exactly
the same as the greedy decoding results of the tar-
get LLM (Stern et al., 2019; Sun et al., 2021; Xia
et al., 2023). Formally, given the input sequence
x1, . . . , x t, the drafted tokens ex1, . . . ,exK, and
the computed probability distributions p1, . . . , p K,
q1, . . . , q Kas obtained from Eq. (2) and (3), respec-
tively, the verification criterion on the ithdraftedtoken is formulated as
exi= arg max qi, (4)
where i= 1, . . . , K . The first position cthat the
drafted token excfails the verification denotes the
bifurcation position. The output token at this posi-
tionxt+cwill be adjusted by the correction strategy,
which simply replaces the drafted token with the
LLM’s top-1 prediction:
xt+c←arg max qc. (5)
The verification criterion of greedy decoding is
straightforward and clear. Thus, multiple subse-
quent studies have adopted this criterion to demon-
strate the efficacy of their methodologies (Santilli
et al., 2023; Yang et al., 2023b; Hooper et al., 2023;
Zhang et al., 2023a; Fu et al., 2024). However,
the strict matching requirement of this criterion of-
ten results in the rejection of high-quality drafted
tokens, simply because they differ from the top-1
predictions of the target LLM, thereby constraining
the speedup of the paradigm.
To tackle this problem, multiple studies have
proposed various approximate verification crite-
ria (Stern et al., 2018; Xia et al., 2023; Kim et al.,
2023). Compared with the lossless criterion, these
methods slightly relax the matching requirement
to trust the drafts more, leading to higher accep-
tance of drafted tokens. For instance, SpecDec (Xia
et al., 2023) only requires the drafted tokens to fall
in top-k candidates of the target LLM; BiLD (Kim
et al., 2023) proposed a rollback criterion that only
rejects drafted tokens when the number of consecu-
tive mismatch tokens exceeds a fixed threshold.
6.2 Speculative Sampling
Following Stern et al. (2019), subsequent work ex-
tended Speculative Decoding to support various
sampling methods (Leviathan et al., 2023; Chen
et al., 2023a), accelerating the target LLM’s in-
ference without changing its output distribution.
Formally, given the initial sequence x1, . . . , x t, the

--- PAGE 7 ---
MethodsDrafting VerificationTarget LLMSpeedup
(reported)Approach Alignment Tuning-free Greedy Sampling Token TreeIndependent-DSpecDec (Xia et al., 2023) Non-Auto LM Seq-KD ✗ ✓ ✗ ✗ Transformer-base (65M) 3.9× ∼ 5.1×
SpS (Chen et al., 2023a) Small LM - ✓ ✓ ✓ ✗ Chinchilla (70B) 1.9× ∼ 2.5×
SpecInfer (Miao et al., 2024) Boost-tuned LMs Col-BT ✗ ✓ ✓ ✓ LLaMA (30B-65B) 2.0× ∼ 2.4×
DistillSpec (Zhou et al., 2023) Small LM KD ✗ ✓ ✓ ✗ T5-XL (3B) -
Online Speculative (Liu et al., 2023) Small LM Online-KD ✗ ✓ ✓ ✗ Vicuna (7B) -
CS. Drafting (Chen et al., 2023b) Cascaded LMs - ✓ ✓ ✓ ✗ FLAN-T5-xxl (11B) -
REST (He et al., 2023) Context Retrieval - ✓ ✓ ✓ ✓ Vicuna (7B-13B) 1.6× ∼ 1.8×Self-DBlockwise Decoding (Stern et al., 2018) FFN Heads Seq-KD ✗ ✓ ✗ ✗ Transformer-big (213M) 1.7× ∼ 3.0×
Medusa (Cai et al., 2024) FFN Heads Seq-KD ✗ ✓ ✓ ✓ Vicuna (7B-13B) 2.2× ∼ 2.3×
PPD (Yang et al., 2023b) Early Exiting - ✗ ✓ ✗ ✗ Vicuna (13B) 1.1× ∼ 1.5×
Self-Speculative (Zhang et al., 2023a) Layer Skipping - ✓ ✓ ✓ ✗ LLaMA-2 (13B-70B) 1.4× ∼ 1.7×
Parallel Decoding (Santilli et al., 2023) Mask-Predict - ✓ ✓ ✗ ✗ MBart50 (610M) 1.0× ∼ 1.1×
Lookahead Decoding (Fu et al., 2024) Mask-P & N-grams - ✓ ✓ ✗ ✗ LLaMA-2 (7B-70B) 1.5× ∼ 2.3×
EAGLE (Li et al., 2024) Auto-regression Head KD ✗ ✓ ✓ ✓ Vicuna (7B-33B) 2.9× ∼ 3.1×
Table 3: Summary of Speculative Decoding methods. “ Independent-D ” and “ Self-D ” denote independent drafting
and self-drafting, respectively. “ Greedy ”, “Sampling ”, and “ Token Tree ” denote whether the method supports greedy
decoding, speculative sampling, and token tree verification, respectively. We list the most representative target
LLMs for each method and the speedups in the original paper (if reported), which is obtained with a batch size of 1.
drafted tokens ex1, . . . ,exKand the computed dis-
tributions p1, . . . , p K,q1, . . . , q K, the verification
criterion on the ithdrafted token is
r <min
1,qi(exi)
pi(exi)
, r∼U[0,1], (6)
where rdenotes a random number drawn from a
uniform distribution U[0,1];qi(exi)andpi(exi)are
the probability of exiaccording to MqandMp, re-
spectively; and i= 1, . . . , K . In other words, this
criterion accepts the token exiifqi(exi)≥pi(exi),
and in case qi(exi)< pi(exi)it rejects the token with
probability 1−qi(exi)
pi(exi). The correction strategy re-
samples the output token at the bifurcation position
cfrom an adjusted distribution:
xt+c∼norm(max (0 , qc−pc)). (7)
Leviathan et al. (2023) and Chen et al. (2023a)
have theoretically proved that this criterion main-
tains identical output distributions to the target
LLM. Thus, it has been widely adopted in subse-
quent research (Liu et al., 2023; Zhou et al., 2023;
Monea et al., 2023; Chen et al., 2023b). In addition
to the strict requirement, some work has also ex-
plored approximate strategies to improve the token
acceptance rate (Leviathan et al., 2023; Zhou et al.,
2023). For instance, Leviathan et al. (2023) pro-
posed multiplying pi(exi)in Eq. (6) by a lenience
parameter l∈[0,1]to slightly relax the criterion.
6.3 Token Tree Verification
Contrary to prior verification strategies that focused
on a single draft sequence, SpecInfer (Miao et al.,
2024) proposed token tree verification , an effective
strategy enabling the target LLM to verify multi-
ple draft sequences in parallel. As illustrated in
RootIt
I the
averybe
likeamwill✔
✔✔
✔✔
✔✔✔I am like very
I
am
like
veryFigure 4: Illustration of the token tree sequences ( left)
and tree attention mask ( right ). For simplicity, we only
visualize the attention mask of tokens in white colors.
Figure 4, this method first merges multiple candi-
date draft sequences into a token tree by sharing
prefixes. It then utilizes a specially designed tree
attention mask to facilitate the LLM verifying the
whole structure in parallel. Recent research has ex-
plored various approaches to obtain these candidate
draft sequences (Miao et al., 2024; Cai et al., 2024;
He et al., 2023; Li et al., 2024). For instance, Miao
et al. (2024) generated diverse draft sequences from
different boost-tuned LMs; Cai et al. (2024) con-
sidered the top-k predictions from each FFN head
to obtain multiple candidate sequences.
7 Alignment
As illustrated in Section 5, the speedup of Specula-
tive Decoding primarily depends on the speculation
accuracy, which in turn is influenced by the behav-
ior similarity between the drafter and the target
LLM. To enhance this, existing research has ex-
plored various knowledge distillation (KD) strate-
gies to align the drafter’s outputs with those of the
target LLM (Stern et al., 2018; Xia et al., 2023;
Miao et al., 2024; Liu et al., 2023; Kim et al., 2023;

--- PAGE 8 ---
Zhou et al., 2023). Particularly, Blockwise Decod-
ing adopted sequence-level knowledge distillation
(Seq-KD) (Kim and Rush, 2016) for alignment,
which trained the drafter on the sentences gener-
ated by the target LLM. Miao et al. (2024) proposed
a collective boost-tuning (Col-BT) strategy, apply-
ing Seq-KD to finetune multiple small LMs on the
training data and utilizing their aggregated output
as drafts to improve the speculation accuracy.
Although Seq-KD is effective, it ignores the
probability distributions of the target LLM, leading
to performance degradation with sampling methods.
To rectify this, recent studies have explored other
KD strategies for Speculative Decoding (Zhou
et al., 2023; Liu et al., 2023). Notably, Distill-
Spec (Zhou et al., 2023) conducted a comprehen-
sive comparison of different KD strategies on Spec-
ulative Decoding across various downstream tasks.
Liu et al. (2023) proposed an online KD strategy
that dynamically aligns the drafter with the target
LLM on the fly using the query data.
We summarize the main features of existing
Speculative Decoding methods in Table 3, includ-
ing the drafter type or the drafting strategy, the
alignment approach, supported verification strate-
gies, and the reported speedup, etc.
8 Spec-Bench
With the rapid research progress in Speculative De-
coding, there is an increasing demand for compara-
tive analysis of leading methods. However, existing
approaches are tested using disparate benchmarks,
devices, and environments, making fair compar-
isons impractical. To address this gap, we intro-
duce Spec-Bench – a comprehensive benchmark
for Speculative Decoding covering diverse applica-
tion scenarios. Based on Spec-Bench, we present a
systematic comparison of open-source approaches
under third-party testing conditions. Experiments
were executed on the same device and testing envi-
ronment to ensure a fair comparison.
8.1 Benchmark Construction
To assess Speculative Decoding methods across
various scenarios, Spec-Bench encompasses six
distinct subtasks: multi-turn conversation, trans-
lation, summarization, question answering, math-
ematical reasoning, and retrieval-augmented gen-
eration. We composed Spec-Bench by randomly
selecting 80 instances from each of six widely
used datasets, including MT-bench (Zheng et al.,
1 1.2 1.4 1.6 1.8 2 2.2 2.4Multi-turn
ConversationTranslation
Summarization
Question
Answering
Mathematical
ReasoningRetrieval-
augmented
Generation
EAGLE SpS Medusa
PLD REST LookaheadFigure 5: Speedup comparison of various Speculative
Decoding methods on Spec-Bench with greedy settings
(T= 0). Evaluations were conducted on Vicuna-7B
with a batch size of 1. We present the mean speedup over
3 runs. The detailed results are shown in Appendix C.
2023), WMT14 DE-EN, CNN/Daily Mail (Nallap-
ati et al., 2016), Natural Questions (Kwiatkowski
et al., 2019), GSM8K (Cobbe et al., 2021), and
DPR (Karpukhin et al., 2020). For details on Spec-
Bench and the specific experimental setup, please
refer to Appendix B.
8.2 Comparative Evaluation
Our main evaluations were conducted on Vicuna-
7B at FP16 precision using a single consumer-
grade 3090 GPU4. As depicted in Figure 5, under
greedy settings, EAGLE (Li et al., 2024) achieves
the highest speedup ratio (1.8 ×∼2.4×) over autore-
gressive decoding across most subtasks, especially
in mathematical reasoning (with a ∼2.4×speedup).
EAGLE’s success is mainly due to two factors: 1)
it reuses the KV cache of LLMs to predict drafted
tokens, substantially reducing the drafting compu-
tational overhead; 2) compared with Medusa (Cai
et al., 2024), EAGLE drafts in an autoregressive
way, providing more stable and accurate specula-
tion results. PLD (Saxena, 2023) excels in subtasks
with high similarities between input and output,
such as summarization (with a ∼2.4×speedup).
However, its performance diminishes in other sub-
tasks like translation and question answering, with
speedup ratios falling between 1.1 ×∼1.3×.
We also compare the speedups of Speculative
Decoding methods at different sampling tempera-
tures. As illustrated in Figure 6, EAGLE consis-
4For comparative analysis on a more powerful A100 GPU,
please refer to Appendix D.

--- PAGE 9 ---
T=0 T=0.5 T=1.0020406080T okens per Second1.00x 1.00x 1.00x2.08x
1.91x
1.74x1.77x
1.64x
1.49x1.39x1.36x 1.35xVanilla EAGLE SpS RESTFigure 6: Speedup comparison of various methods on
Spec-Bench at different temperatures. The speedup
effect diminishes as the sampling temperature increases.
tently outperforms other methods across various
settings, achieving a speedup ratio ranging from
1.7×to 2.1×. Besides, it is observed that the ac-
celeration effect of all methods decreases with an
increase in sampling temperature. This is attributed
to the increased computational complexity of the
speculative sampling criterion at higher tempera-
tures, as revealed in prior research (Joao Gante,
2023; Spector and Re, 2023).
9 Challenges and Future Directions
How to trade off speculation accuracy and draft-
ing efficiency? As discussed in Sections 5, scal-
ing up the drafter can effectively enhance specu-
lation accuracy, yet it largely reduces the drafting
efficiency and even the overall speedup. Therefore,
it is essential to strike a balance between specula-
tion accuracy and drafting latency. Among existing
strategies, behavior alignment is a promising ap-
proach to address this issue, as it improves specula-
tion accuracy without increasing latency. However,
despite recent advancements (Miao et al., 2024;
Zhou et al., 2023; Liu et al., 2023), there is still
considerable room for improvement to align the
drafter with the target LLM. For example, given
that the drafted tokens after the bifurcation posi-
tion are all discarded, one potential direction could
involve encouraging the drafter to prioritize the
generation quality of early-position tokens. Be-
yond alignment, other factors such as the quality of
drafting (Fu et al., 2024) and the determination of
speculation length (Su et al., 2023) also influence
speculation accuracy and merit further exploration.
How to apply Speculative Decoding in batched
inference scenarios? Currently, only a few Spec-
ulative Decoding implementations have supported
batched inference, such as EAGLE5and SpS6.
5https://github.com/SafeAILab/EAGLE
6https://github.com/lucidrains/
speculative-decodingHowever, batched inference is a crucial technique
for efficiently managing user inputs in LLM real-
time services. The primary challenges in batched
Speculative Decoding lie in two aspects: (1) Each
decoded sentence in Speculative Decoding varies
in decoding steps due to different speculation accu-
racy. Thus, the inference latency of a batch depends
on the slowest sample in the batch; (2) The extra
computational complexity introduced by Specu-
lative Decoding, especially in sampling settings,
increases with larger batch sizes. How to maintain
a promising speedup of Speculative Decoding in
batched inference, and combine it with advanced
techniques such as continuous batching (Yu et al.,
2022), warrants further investigation.
How to integrate Speculative Decoding with
other leading techniques? As a general decod-
ing paradigm, Speculative Decoding has already
demonstrated its potential in conjunction with other
advanced techniques (Yang et al., 2023a; Zhang
et al., 2023b; Li et al., 2023). For instance, Yuan
et al. (2023) combined Speculative Decoding with
Contrastive Decoding (Li et al., 2023), which not
only speeds up the inference but also substan-
tially improves the generation quality. In addi-
tion to the acceleration of text-only LLMs, ap-
plying Speculative Decoding in multimodal infer-
ence, such as image synthesis, text-to-speech syn-
thesis, and video generation, is also an intriguing
and valuable direction for future research. Another
promising research direction is to integrate Specu-
lative Decoding with other efficient methods such
as vLLM (Kwon et al., 2023), Non-Auregressive
Generation (Du et al., 2021, 2022) and Flash-
Attention (Dao et al., 2022; Dao, 2023), further
boosting the inference efficiency of LLM services.
10 Conclusion
This paper presents a comprehensive survey of
Speculative Decoding, including the evolution of
this promising paradigm, its formal definition and
formulation, a systematic categorization of exist-
ing methods, and an in-depth review of leading
techniques. Moreover, we introduce Spec-Bench,
an extensive evaluation benchmark for Speculative
Decoding methods, and present a comparative eval-
uation of prominent methods. To our knowledge,
this is the first survey dedicated to Speculative De-
coding. Our aim for this paper is to clarify the
current research landscape and provide insights
into future research directions.

--- PAGE 10 ---
Limitations
This paper provides a thorough examination and
categorization of current methodologies and emerg-
ing trends in Speculative Decoding. We have also
conducted a comparative analysis of leading open-
source methods to offer researchers deeper insights
into the advantages and limitations of different
models. Beyond Speculative Decoding, we ac-
knowledge additional efficient NLP strategies such
as vLLM (Kwon et al., 2023) and continuous batch-
ing (Yu et al., 2022). In the future, we intend to
expand the discussion to encompass the integration
of Speculative Decoding with these advanced tech-
niques. Moreover, due to the absence of an avail-
able implementation of batched Speculative De-
coding, our evaluations could not cover this aspect.
We plan to undertake subsequent experiments to as-
sess the speedup of Speculative Decoding methods
across various batch sizes.
Ethics Statement
The datasets used in our experiment are publicly
released and labeled through interaction with hu-
mans in English. In this process, user privacy is
protected, and no personal information is contained
in the dataset. The scientific artifacts that we used
are available for research with permissive licenses.
And the use of these artifacts in this paper is consis-
tent with their intended use. Therefore, we believe
that our research work meets the ethics of ACL.
Acknowledgements
We thank all anonymous reviewers for their valu-
able comments during the review process. This
work is partially supported by Research Grants
Council of Hong Kong (15207122 and 15213323).
References
Christopher Bryant, Zheng Yuan, Muhammad Reza
Qorib, Hannan Cao, Hwee Tou Ng, and Ted Briscoe.
2023. Grammatical error correction: A survey of the
state of the art. Comput. Linguistics , 49(3):643–701.
F. Warren Burton. 1985. Speculative computation, par-
allelism, and functional programming. IEEE Trans.
Computers, 34(12):1190–1193.
Deng Cai, Yan Wang, Lemao Liu, and Shum-
ing Shi. 2022. Recent advances in retrieval-
augmented text generation. In SIGIR ’22: The45th
International ACM SIGIR Conference onResearch
andDevelopment inInformation Retrieval, Madrid,
Spain, July 11-15,2022, pages 3417–3419. ACM.Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu
Peng, Jason D. Lee, Deming Chen, and Tri Dao.
2024. Medusa: Simple LLM inference acceleration
framework with multiple decoding heads. CoRR ,
abs/2401.10774.
Charlie Chen, Sebastian Borgeaud, Geoffrey Irv-
ing, Jean-Baptiste Lespiau, Laurent Sifre, and
John Jumper. 2023a. Accelerating large language
model decoding with speculative sampling. CoRR ,
abs/2302.01318.
Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun,
Jie Huang, and Kevin Chen-Chuan Chang. 2023b.
Cascade speculative drafting for even faster llm infer-
ence.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pil-
lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2023. Palm: Scaling language mod-
eling with pathways. J.Mach. Learn. Res., 24:240:1–
240:113.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
2021. Training verifiers to solve math word prob-
lems. CoRR, abs/2110.14168.
Tri Dao. 2023. Flashattention-2: Faster attention with
better parallelism and work partitioning. CoRR ,
abs/2307.08691.
Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra,
and Christopher Ré. 2022. Flashattention: Fast and
memory-efficient exact attention with io-awareness.
InAdvances inNeural Information Processing
Systems 35: Annual Conference onNeural

--- PAGE 11 ---
Information Processing Systems 2022, NeurIPS
2022, New Orleans, LA, USA, November 28-
December 9,2022.
Cunxiao Du, Jing Jiang, Yuanchen Xu, Jiawei Wu,
Sicheng Yu, Yongqi Li, Shenggui Li, Kai Xu, Liqiang
Nie, Zhaopeng Tu, and Yang You. 2024. Glide with a
cape: A low-hassle method to accelerate speculative
decoding. CoRR, abs/2402.02082.
Cunxiao Du, Zhaopeng Tu, and Jing Jiang. 2021.
Order-agnostic cross entropy for non-autoregressive
machine translation. In Proceedings ofthe38th
International Conference onMachine Learning,
ICML 2021, 18-24 July 2021, Virtual Event , volume
139 of Proceedings ofMachine Learning Research ,
pages 2849–2859. PMLR.
Cunxiao Du, Zhaopeng Tu, Longyue Wang, and
Jing Jiang. 2022. ngram-oaxe: Phrase-based
order-agnostic cross entropy for non-autoregressive
machine translation. In Proceedings ofthe
29th International Conference onComputational
Linguistics, COLING 2022, Gyeongju, Republic of
Korea, October 12-17, 2022 , pages 5035–5045. In-
ternational Committee on Computational Linguistics.
Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang.
2024. Break the sequential dependency of llm infer-
ence using lookahead decoding.
Tao Ge, Heming Xia, Xin Sun, Si-Qing Chen,
and Furu Wei. 2022. Lossless acceleration for
seq2seq generation with aggressive decoding. CoRR ,
abs/2205.10350.
Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D. Lee,
and Di He. 2023. REST: retrieval-based speculative
decoding. CoRR, abs/2311.08252.
John L. Hennessy and David A. Patterson. 2012.
Computer Architecture -AQuantitative Approach,
5thEdition. Morgan Kaufmann.
Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh,
Hasan Genc, Kurt Keutzer, Amir Gholami, and
Yakun Sophia Shao. 2023. SPEED: speculative
pipelined execution for efficient decoding. CoRR ,
abs/2310.12072.
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de Las Casas, Florian Bressand, Gianna Lengyel,
Guillaume Lample, Lucile Saulnier, Lélio Re-
nard Lavaud, Marie-Anne Lachaux, Pierre Stock,
Teven Le Scao, Thibaut Lavril, Thomas Wang, Timo-
thée Lacroix, and William El Sayed. 2023. Mistral
7b.CoRR, abs/2310.06825.
Albert Q. Jiang, Alexandre Sablayrolles, Antoine
Roux, Arthur Mensch, Blanche Savary, Chris
Bamford, Devendra Singh Chaplot, Diego de las
Casas, Emma Bou Hanna, Florian Bressand, Gi-
anna Lengyel, Guillaume Bour, Guillaume Lam-
ple, Lélio Renard Lavaud, Lucile Saulnier, Marie-
Anne Lachaux, Pierre Stock, Sandeep Subramanian,Sophia Yang, Szymon Antoniak, Teven Le Scao,
Théophile Gervet, Thibaut Lavril, Thomas Wang,
Timothée Lacroix, and William El Sayed. 2024. Mix-
tral of experts.
Joao Gante. 2023. Assisted generation: a new direction
toward low-latency text generation.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi
Chen, and Wen-tau Yih. 2020. Dense passage re-
trieval for open-domain question answering. In
Proceedings ofthe2020 Conference onEmpirical
Methods inNatural Language Processing, EMNLP
2020, Online, November 16-20, 2020 , pages 6769–
6781. Association for Computational Linguistics.
Sehoon Kim, Karttikeya Mangalam, Suhong Moon,
Jitendra Malik, Michael W. Mahoney, Amir Gho-
lami, and Kurt Keutzer. 2023. Speculative de-
coding with big little decoder. In Advances in
Neural Information Processing Systems 36:Annual
Conference onNeural Information Processing
Systems 2023, NeurIPS 2023, New Orleans, LA,
USA, December 10-16,2023.
Yoon Kim and Alexander M. Rush. 2016. Sequence-
level knowledge distillation. In Proceedings ofthe
2016 Conference onEmpirical Methods inNatural
Language Processing, EMNLP 2016, Austin, Texas,
USA, November 1-4, 2016 , pages 1317–1327. The
Association for Computational Linguistics.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, Kristina Toutanova, Llion Jones, Matthew
Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. 2019. Nat-
ural questions: A benchmark for question answer-
ing research. Transactions oftheAssociation for
Computational Linguistics, 7:452–466.
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying
Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gon-
zalez, Hao Zhang, and Ion Stoica. 2023. Efficient
memory management for large language model serv-
ing with pagedattention. In Proceedings ofthe29th
Symposium onOperating Systems Principles, SOSP
2023, Koblenz, Germany, October 23-26, 2023 ,
pages 611–626. ACM.
Yaniv Leviathan, Matan Kalman, and Yossi Matias.
2023. Fast inference from transformers via spec-
ulative decoding. In International Conference on
Machine Learning, ICML 2023, 23-29 July 2023,
Honolulu, Hawaii, USA , volume 202 of Proceedings
ofMachine Learning Research , pages 19274–19286.
PMLR.
Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy
Liang, Jason Eisner, Tatsunori Hashimoto, Luke
Zettlemoyer, and Mike Lewis. 2023. Contrastive de-
coding: Open-ended text generation as optimization.
InProceedings ofthe61st Annual Meeting ofthe

--- PAGE 12 ---
Association forComputational Linguistics (V olume
1:Long Papers), ACL 2023, Toronto, Canada, July
9-14, 2023 , pages 12286–12312. Association for
Computational Linguistics.
Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang
Zhang. 2024. Eagle: Speculative sampling requires
rethinking feature uncertainty.
Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Sto-
ica, Zhijie Deng, Alvin Cheung, and Hao Zhang.
2023. Online speculative decoding. CoRR ,
abs/2310.07177.
Xupeng Miao, Gabriele Oliaro, Zhihao Zhang,
Xinhao Cheng, Zeyu Wang, Zhengxin Zhang,
Rae Ying Yee Wong, Alan Zhu, Lijie Yang,
Xiaoxiang Shi, Chunan Shi, Zhuoming Chen,
Daiyaan Arfeen, Reyna Abhyankar, and Zhihao
Jia. 2024. Specinfer: Accelerating large language
model serving with tree-based speculative infer-
ence and verification. In Proceedings ofthe29th
ACM International Conference onArchitectural
Support forProgramming Languages andOperating
Systems, V olume 3, ASPLOS ’24, page 932–949,
New York, NY , USA. Association for Computing
Machinery.
Giovanni Monea, Armand Joulin, and Edouard Grave.
2023. Pass: Parallel speculative sampling. CoRR ,
abs/2311.13581.
Ramesh Nallapati, Bowen Zhou, Cícero Nogueira dos
Santos, Çaglar Gülçehre, and Bing Xiang. 2016.
Abstractive text summarization using sequence-to-
sequence rnns and beyond. In Proceedings of
the20th SIGNLL Conference onComputational
Natural Language Learning, CoNLL 2016, Berlin,
Germany, August 11-12, 2016 , pages 280–290.
ACL.
OpenAI. 2023. GPT-4 technical report. CoRR ,
abs/2303.08774.
David A. Patterson. 2004. Latency lags bandwith.
Commun. ACM, 47(10):71–75.
Andrea Santilli, Silvio Severino, Emilian Postolache,
Valentino Maiorca, Michele Mancusi, Riccardo
Marin, and Emanuele Rodolà. 2023. Accelerating
transformer inference for translation via parallel de-
coding. In Proceedings ofthe61st Annual Meeting
oftheAssociation forComputational Linguistics
(V olume 1:Long Papers), ACL 2023, Toronto,
Canada, July 9-14, 2023 , pages 12336–12355. As-
sociation for Computational Linguistics.
Apoorv Saxena. 2023. Prompt lookup decoding.
Noam Shazeer. 2019. Fast transformer decoding: One
write-head is all you need. CoRR, abs/1911.02150.
Benjamin Spector and Chris Re. 2023. Accelerating
LLM inference with staged speculative decoding.
CoRR, abs/2308.04623.Mitchell Stern, William Chan, Jamie Kiros, and Jakob
Uszkoreit. 2019. Insertion transformer: Flexible
sequence generation via insertion operations. In
Proceedings ofthe36th International Conference
onMachine Learning, ICML 2019, 9-15 June
2019, Long Beach, California, USA , volume 97 of
Proceedings ofMachine Learning Research , pages
5976–5985. PMLR.
Mitchell Stern, Noam Shazeer, and Jakob Uszko-
reit. 2018. Blockwise parallel decoding for
deep autoregressive models. In Advances in
Neural Information Processing Systems 31:Annual
Conference onNeural Information Processing
Systems 2018, NeurIPS 2018, December 3-8,2018,
Montréal, Canada, pages 10107–10116.
Qidong Su, Christina Giannoula, and Gennady Pekhi-
menko. 2023. The synergy of speculative decod-
ing and batching in serving large language models.
CoRR, abs/2310.18813.
Xin Sun, Tao Ge, Furu Wei, and Houfeng Wang.
2021. Instantaneous grammatical error correc-
tion with shallow aggressive decoding. In
Proceedings ofthe59th Annual Meeting ofthe
Association forComputational Linguistics andthe
11th International Joint Conference onNatural
Language Processing, ACL/IJCNLP 2021, (V olume
1:Long Papers), Virtual Event, August 1-6, 2021 ,
pages 5937–5947. Association for Computational
Linguistics.
Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ah-
mad Beirami, Himanshu Jain, and Felix X. Yu. 2023.
Spectr: Fast speculative decoding via optimal trans-
port. In Advances inNeural Information Processing
Systems 36: Annual Conference onNeural
Information Processing Systems 2023, NeurIPS
2023, New Orleans, LA, USA, December 10-16,
2023.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurélien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023a. Llama: Open
and efficient foundation language models. CoRR ,
abs/2302.13971.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,

--- PAGE 13 ---
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurélien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023b. Llama 2: Open foundation and
fine-tuned chat models. CoRR, abs/2307.09288.
Yu Wang, Yuelin Wang, Kai Dang, Jie Liu, and Zhuo
Liu. 2021. A comprehensive survey of grammatical
error correction. ACM Trans. Intell. Syst. Technol. ,
12(5):65:1–65:51.
Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen,
Furu Wei, and Zhifang Sui. 2023. Speculative de-
coding: Exploiting speculative execution for ac-
celerating seq2seq generation. In Findings ofthe
Association forComputational Linguistics: EMNLP
2023, Singapore, December 6-10, 2023 , pages 3909–
3925. Association for Computational Linguistics.
Daliang Xu, Wangsong Yin, Xin Jin, Ying Zhang,
Shiyun Wei, Mengwei Xu, and Xuanzhe Liu. 2023.
Llmcad: Fast and scalable on-device large language
model inference. CoRR, abs/2309.04255.
Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin
Jiang, Linjun Yang, Rangan Majumder, and Furu
Wei. 2023a. Inference with reference: Lossless ac-
celeration of large language models. arXiv preprint
arXiv:2304.04487.
Sen Yang, Shujian Huang, Xinyu Dai, and Jiajun Chen.
2024. Multi-candidate speculative decoding. CoRR ,
abs/2401.06706.
Seongjun Yang, Gibbeum Lee, Jaewoong Cho, Dim-
itris S. Papailiopoulos, and Kangwook Lee. 2023b.
Predictive pipelined decoding: A compute-latency
trade-off for exact LLM decoding. CoRR ,
abs/2307.05908.
Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soo-
jeong Kim, and Byung-Gon Chun. 2022. Orca:
A distributed serving system for transformer-based
generative models. In 16th USENIX Symposium
onOperating Systems Design andImplementation,
OSDI 2022, Carlsbad, CA, USA, July 11-13, 2022 ,
pages 521–538. USENIX Association.
Hongyi Yuan, Keming Lu, Fei Huang, Zheng Yuan, and
Chang Zhou. 2023. Speculative contrastive decoding.
CoRR, abs/2311.08981.
Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen,
Gang Chen, and Sharad Mehrotra. 2023a. Draft
& verify: Lossless large language model accel-
eration via self-speculative decoding. CoRR ,
abs/2309.08168.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher
Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin,Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shus-
ter, Daniel Simig, Punit Singh Koura, Anjali Srid-
har, Tianlu Wang, and Luke Zettlemoyer. 2022.
OPT: open pre-trained transformer language mod-
els.CoRR, abs/2205.01068.
Zhihao Zhang, Alan Zhu, Lijie Yang, Yihua Xu, Lant-
ing Li, Phitchaya Mangpo Phothilimthana, and Zhi-
hao Jia. 2023b. Accelerating retrieval-augmented
language model serving with speculation. In
Submitted toTheTwelfth International Conference
onLearning Representations. Under review.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,
Joseph E. Gonzalez, and Ion Stoica. 2023. Judg-
ing LLM-as-a-judge with MT-bench and chat-
bot arena. In Thirty-seventh Conference on
Neural Information Processing Systems Datasets
andBenchmarks Track.
Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat,
Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv
Kumar, Jean-François Kagy, and Rishabh Agarwal.
2023. Distillspec: Improving speculative decoding
via knowledge distillation.

--- PAGE 14 ---
Appendix
A Applications
In addition to serving as a general paradigm, recent
work has revealed that some variants of Specula-
tive Decoding demonstrate extraordinary effective-
ness in specific tasks. Furthermore, other research
has applied this paradigm to address latency issues
unique to certain application scenarios, achieving
inference acceleration. Below, we will provide a
detailed introduction to these promising works.
Recent studies have highlighted Speculative De-
coding is particularly well suited for tasks where
model inputs and outputs are highly similar (Sun
et al., 2021; Ge et al., 2022; Yang et al., 2023a),
such as Grammatical Error Correction (Wang et al.,
2021; Bryant et al., 2023) and Retrieval-augmented
Generation (Cai et al., 2022). These methods intro-
duced a specialized form of Speculative Decoding,
where the initial user input or the retrieved con-
text is directly employed as drafts. For instance,
SAD (Sun et al., 2021), an early attempt at Specu-
lative Decoding on Grammatical Error Correction,
utilized the input sentence with grammatical er-
rors as a draft and leveraged the LLM to verify the
whole sentence in parallel, achieving a 9×∼12×
speedup. Similarly, LLMA (Yang et al., 2023a)
selected text spans from the reference as drafts,
demonstrating a 2×∼3×speedup across various
practical application scenarios including Retrieval-
augmented Generation, Cache-assisted Generation,
and Multi-turn Conversations.
Beyond these works, RaLMSpec (Zhang et al.,
2023b) adopted Speculative Decoding to acceler-
ate retrieval-augmented language models (RaLMs).
It pointed out that the main latency bottleneck of
iterative RaLMs is the frequent retrieval from a
vast knowledge base. To accelerate inference, this
method proposed to maintain a local cache for spec-
ulative retrieval, achieving around 2×speedup with
identical model outputs. LLMCad (Xu et al., 2023)
applied Speculative Decoding to on-device LLM in-
ference. Concretely, it proposed to generate drafts
with a smaller real-time LM that can be hosted in
device memory, and only utilize the target LLM
for parallel verification. This approach effectively
reduces repetitive releasing and loading of model
weights, achieving a 9.3×speedup compared to
existing inference engines.B Experimental Details
B.1 Details of Spec-Bench
To assess the acceleration performance of Specu-
lative Decoding methods in various scenarios, we
developed Spec-Bench, a comprehensive bench-
mark encompassing six distinct tasks. Spec-Bench
integrates MT-bench (Zheng et al., 2023), a multi-
turn conversation benchmark previously adopted
in research (Cai et al., 2024; Li et al., 2024), to
provide a basis for comparison with earlier studies.
Additionally, it includes two input-guided tasks:
summarization and retrieval-augmented generation
(RAG), both of which exhibit a significant over-
lap between the input prompts and the target out-
puts. We selected CNN/Daily Mail (Nallapati et al.,
2016) and Natural Questions (Kwiatkowski et al.,
2019) as the dataset for these two tasks, respec-
tively. Specifically, in the RAG subtask, the top-5
documents retrieved from DPR (Karpukhin et al.,
2020) were concatenated with each question to con-
struct the input prompt.
Moreover, Spec-Bench incorporates three fur-
ther subtasks – translation, question answering, and
mathematical reasoning – to provide a thorough
evaluation of Speculative Decoding’s speedup ca-
pabilities in diverse contexts. We utilized WMT14
DE-EN, Natural Questions, and GSM8K (Cobbe
et al., 2021) as the primary datasets for these tasks,
respectively. We randomly selected 80 instances
from each subtask’s test set for evaluation. The
detailed composition is summarized in Table 4.
Subtask Dataset #Samples
Multi-turn Conversation MT-bench 80
Retrieval-aug. Generation Natural Questions 80
Summarization CNN/Daily Mail 80
Translation WMT14 DE-EN 80
Question Answering Natural Questions 80
Mathematical Reasoning GSM8K 80
Overall - 480
Table 4: Detailed Composition of Spec-Bench. Spec-
Bench includes 6 distinct subtasks to encompass diverse
application scenarios.
B.2 Implementation Details
We have selected six representative Speculative De-
coding methods for our comparative analysis on
Spec-Bench. These methods are open-source and
free of bugs. Specifically, SpS (Chen et al., 2023a)
stands as the pioneering work in this field, utilizing
a smaller LM from the same model series as the

--- PAGE 15 ---
ModelsMulti-turn
ConversationTranslation SummarizationQuestion
AnsweringMathematical
ReasoningRetrieval-aug.
Generation#tokens/s Avg.T= 0Autoregressive Decoding 1.00×±0.00 1.00×±0.00 1.00×±0.00 1.00×±0.00 1.00×±0.00 1.00×±0.00 36.74±0.31 1.00×
Lookahead (Fu et al., 2024) 1.15×±0.01 0.98×±0.02 1.07×±0.02 1.06×±0.02 1.32×±0.02 1.03×±0.02 40.64±0.26 1.11×
REST (He et al., 2023) 1.49×±0.02 1.23×±0.04 1.26×±0.03 1.39×±0.04 1.34×±0.03 1.71×±0.05 51.12±0.78 1.39×
PLD (Saxena, 2023) 1.63×±0.02 1.11×±0.02 2.41×±0.04 1.27×±0.03 1.70×±0.03 1.66×±0.04 59.42±0.55 1.62×
SpS (Leviathan et al., 2023) 1.92×±0.04 1.33×±0.02 1.93×±0.01 1.81×±0.04 1.84×±0.00 1.76×±0.01 64.85±0.70 1.77×
Medusa (Cai et al., 2024) 1.65×±0.03 1.41×±0.02 1.33×±0.01 1.44×±0.03 1.69×±0.01 1.29×±0.02 54.30±0.34 1.48×
EAGLE (Li et al., 2024) 2.35×±0.03 1.79×±0.03 2.04×±0.02 1.96×±0.03 2.44×±0.02 1.80×±0.03 76.30±0.36 2.08×T= 1Autoregressive Decoding 1.00×±0.00 1.00×±0.00 1.00×±0.00 1.00×±0.00 1.00×±0.00 1.00×±0.00 36.24±0.43 1.00×
REST (He et al., 2023) 1.43×±0.01 1.19×±0.02 1.24×±0.00 1.36×±0.02 1.34×±0.02 1.61×±0.02 49.04±0.30 1.35×
SpS (Leviathan et al., 2023) 1.55×±0.01 1.20×±0.01 1.57×±0.01 1.54×±0.03 1.56×±0.03 1.52×±0.02 53.94±0.43 1.49×
EAGLE (Li et al., 2024) 1.79×±0.02 1.61×±0.03 1.74×±0.03 1.66×±0.04 1.95×±0.06 1.63×±0.03 62.88±0.54 1.74×
Table 5: Speedup comparison of various Speculative Decoding methods on Spec-Bench. The results were obtained
using Vicuna-7B-v1.3 at FP16 precision. Evaluations were conducted on a single NVIDIA 3090 GPU with a batch
size of 1. We report the mean speedup ratio over 3 different runs. We show the best results in boldface .
drafter to accelerate LLM inference. Medusa (Cai
et al., 2024) and EAGLE (Li et al., 2024) integrate
additional lightweight heads into the target LLM
to facilitate efficient drafting. Lookahead (Fu
et al., 2024) introduces multiple special tokens to
the end of the input prompt for parallel drafting
and transforms the drafts into n-gram candidates.
PLD (Saxena, 2023) is the code implementation7
of LLMA (Yang et al., 2023a), which selects text
spans from the input as drafts. REST (He et al.,
2023) retrieves relevant drafts from text corpora
based on the input prompt.
We conducted our experimental evaluations us-
ing the Vicuna-v1.3 model series (Zheng et al.,
2023). For SpS, we employed the Huggingface im-
plementation8and utilized the vicuna-68m-v1.3
model provided by Yang et al. (2024) as the drafter.
We followed the default parameters of Lookahead9
and PLD for our evaluations. The main experi-
ments were conducted using Pytorch 2.0.1 with
a single consumer-grade NVIDIA GeForce RTX
3090 GPU (24GB) of 12 CPU cores under CUDA
11.8. Further analysis was performed on a more
powerful NVIDIA A100 GPU (80GB) of 64 CPU
cores under CUDA 11.4.
C Details of Main Experimental Results
The detailed results of our main analysis are shown
in Table 5, including the experimental settings of
greedy decoding ( T= 0) and speculative sampling
(T= 1). The findings indicate that EAGLE (Li
et al., 2024) excels across various Spec-Bench sub-
tasks, achieving an overall speedup ranging from
7https://github.com/apoorvumang/
prompt-lookup-decoding
8https://huggingface.co/blog/
assisted-generation
9https://github.com/hao-ai-lab/
LookaheadDecoding
Medusa EAGLE Lookahead SpS PLD REST1.001.251.501.752.002.252.50Speedup Ratio1.48x2.08x
1.11x1.77x
1.62x
1.39x2.42x2.39x
1.77x
1.59x1.66x
1.59x3090
A100Figure 7: Speedup comparison of various methods on
Spec-Bench with different computational devices.
1.6×to 2.4×. PLD (Saxena, 2023) shows notable
efficiency in scenarios where the input and out-
put have a significant overlap. For instance, the
speedup ratio of PLD increases from 1.27 ×in
the question answering subtask to 1.66 ×in the
retrieval-augmented generation subtask, highlight-
ing its effectiveness when the input includes rele-
vant documents. Notably, most methods achieve
a suboptimal speedup on the translation subtask.
We suspect that it is due to the potential lack of
multilingual data in the pretraining corpora.
D Further Analysis on A100
This section presents a comprehensive analysis of
leading Speculative Decoding methods on Spec-
Bench, utilizing a single NVIDIA A100 GPU. The
discussion delves into the influence of computa-
tional hardware, model scale, and computational
precision on the performance of Speculative Decod-
ing. All experiments were performed on the same
device andenvironment to ensure fair comparison.
D.1 Computational Devices
We first discuss the impact of evolving computa-
tional devices on Speculative Decoding. As de-
picted in Figure 7, the acceleration effect of most
Speculative Decoding methods is notably enhanced
when employed on high-performance GPUs, such

--- PAGE 16 ---
ModelsMulti-turn
ConversationTranslation SummarizationQuestion
AnsweringMathematical
ReasoningRetrieval-aug.
Generation#tokens/s Avg.Vicuna-7BAutoregressive Decoding 1.00×±0.00 1.00×±0.00 1.00×±0.00 1.00×±0.00 1.00×±0.00 1.00×±0.00 40.24±0.30 1.00×
Lookahead (Fu et al., 2024) 1.95×±0.01 1.61×±0.05 1.63×±0.02 1.73×±0.04 2.16×±0.04 1.50×±0.00 71.20±1.30 1.77×
REST (He et al., 2023) 1.72×±0.06 1.38×±0.05 1.46×±0.04 1.80×±0.04 1.31×±0.03 1.87×±0.06 63.81±1.00 1.59×
PLD (Saxena, 2023) 1.67×±0.03 1.06×±0.03 2.59×±0.06 1.16×±0.03 1.63×±0.03 1.83×±0.02 66.61±1.15 1.66×
SpS (Leviathan et al., 2023) 1.78×±0.03 1.19×±0.02 1.78×±0.03 1.58×±0.03 1.54×±0.02 1.69×±0.02 64.07±0.41 1.59×
Medusa (Cai et al., 2024) 2.79×±0.07 2.36×±0.07 2.14×±0.04 2.36×±0.08 2.77×±0.08 2.05×±0.01 97.27±2.04 2.42×
EAGLE (Li et al., 2024) 2.75×±0.05 2.08×±0.05 2.32×±0.05 2.23×±0.03 2.79×±0.04 2.15×±0.01 96.23±1.15 2.39×Vicuna-13BAutoregressive Decoding 1.00×±0.00 1.00×±0.00 1.00×±0.00 1.00×±0.00 1.00×±0.00 1.00×±0.00 31.38±0.22 1.00×
Lookahead (Fu et al., 2024) 1.57×±0.01 1.34×±0.01 1.39×±0.00 1.40×±0.01 1.82×±0.02 1.32×±0.01 46.42±0.12 1.48×
REST (He et al., 2023) 1.68×±0.01 1.31×±0.05 1.51×±0.01 1.67×±0.02 1.29×±0.00 1.96×±0.01 48.89±0.26 1.56×
PLD (Saxena, 2023) 1.53×±0.02 1.08×±0.01 2.25×±0.00 1.09×±0.02 1.65×±0.03 1.72×±0.00 48.42±0.17 1.54×
SpS (Leviathan et al., 2023) 1.73×±0.02 1.25×±0.02 1.76×±0.00 1.53×±0.01 1.68×±0.00 1.73×±0.00 50.48±0.28 1.61×
Medusa (Cai et al., 2024) 2.39×±0.02 2.12×±0.02 1.92×±0.00 2.07×±0.02 2.49×±0.02 1.88×±0.00 67.64±0.07 2.16×
EAGLE (Li et al., 2024) 2.88×±0.05 2.24×±0.04 2.52×±0.03 2.24×±0.04 2.90×±0.03 2.34×±0.01 79.35±1.18 2.53×Vicuna-33BAutoregressive Decoding 1.00×±0.00 1.00×±0.00 1.00×±0.00 1.00×±0.00 1.00×±0.00 1.00×±0.00 16.34±0.01 1.00×
Lookahead (Fu et al., 2024) 1.46×±0.00 1.21×±0.00 1.32×±0.00 1.29×±0.00 1.71×±0.00 1.28×±0.00 22.58±0.08 1.38×
REST (He et al., 2023) 1.71×±0.01 1.39×±0.00 1.57×±0.00 1.69×±0.01 1.34×±0.01 1.89×±0.00 25.98±0.07 1.59×
PLD (Saxena, 2023) 1.45×±0.00 1.06×±0.00 1.98×±0.00 1.07×±0.00 1.54×±0.00 1.43×±0.00 23.07±0.01 1.41×
SpS (Leviathan et al., 2023) 1.79×±0.00 1.31×±0.00 1.80×±0.00 1.57×±0.00 1.73×±0.00 1.69×±0.00 26.89±0.03 1.65×
Medusa (Cai et al., 2024) 2.22×±0.00 1.95×±0.00 1.85×±0.00 1.87×±0.01 2.32×±0.01 1.84×±0.00 32.92±0.06 2.01×
EAGLE (Li et al., 2024) 2.81×±0.00 2.14×±0.00 2.53×±0.00 2.19×±0.00 3.01×±0.00 2.31×±0.00 40.91±0.03 2.50×
Table 6: Speedup comparison of Speculative Decoding methods across various model scales on Spec-Bench. The
results were obtained using Vicuna-v1.3 at FP16 precision with greedy settings ( T= 0). Evaluations were
conducted on a single NVIDIA A100 GPU with a batch size of 1. We report the mean speedup over 3 different runs.
11.21.41.61.822.22.42.62.8Multi-turn
ConversationTranslation
Summarization
Question
Answering
Mathematical
ReasoningRetrieval-
augmented
Generation
EAGLE SpS Medusa
PLD REST Lookahead
Figure 8: Speedup comparison of various Speculative
Decoding methods on a single A100 GPU with greedy
settings ( T= 0). Evaluations were conducted on Spec-
Bench using Vicuna-7B at FP16 precision.
as NVIDIA A100s. This enhancement is primar-
ily due to the increased availability of idle com-
putational resources on more advanced computa-
tional devices, which Speculative Decoding can
leverage to accelerate inference processes. Among
the methods evaluated, Medusa (Cai et al., 2024)
and Lookahead (Fu et al., 2024) demonstrate the
most significant improvements. Specifically, the
speedup ratio for Medusa escalates from 1.48 ×to
2.42×, and for Lookahead, it rises from 1.11 ×to
1.77×. This finding underscores that Speculative
Decoding methods will benefit more from evolving
computational hardware, such as H100 GPUs.
7B 13B 33B020406080100T okens per Second2.37x
2.53x
2.50x1.61x
1.65x1.66xVanilla
Medusa
EAGLE
LookaheadSpS
PLD
RESTFigure 9: Speedup comparison of various methods on
Spec-Bench at different model scales.
We illustrate the comparison of various Specu-
lative Decoding methods evaluated with a single
A100 GPU in Figure 8. The detailed experimental
results are shown in Table 6. The results indicate
that Medusa (Cai et al., 2024) and EAGLE (Li et al.,
2024) excel in this experimental setting, achieving
an overall speedup of 2.4 ×. These two methods
perform particularly well on the multi-turn conver-
sation and mathematical reasoning subtasks, with
a∼2.8×speedup.
D.2 Model Scale
We present the speedup comparison of Speculative
Decoding methods across various model scales in
Figure 9. The detailed experimental results are
shown in Table 6. Among all the evaluated meth-
ods, EAGLE (Li et al., 2024) maintains a high
speedup ratio over autoregressive decoding across
all model scales, achieving a speedup ratio rang-
ing from 2.4 ×to 2.5×. While Medusa (Cai et al.,
2024) demonstrates superior acceleration perfor-
mance on Vicuna-7B, its speedup ratio degrades

--- PAGE 17 ---
Medusa EAGLE Lookahead SpS PLD REST0.500.751.001.251.501.752.002.252.50Speedup Ratio2.42x2.39x
1.77x
1.59x1.66x1.59x1.72x 1.74x
1.36x
1.15x
1.01x1.21xFP16
FP32Figure 10: Speedup comparison of various methods on
Spec-Bench with different computational precision.
from 2.4 ×to 2.0×as the model scale increases.
D.3 Computational Precision
It is noteworthy that most Speculative Decoding ap-
proaches are predominantly evaluated using FP16
precision (Fu et al., 2024; Cai et al., 2024; Li et al.,
2024; He et al., 2023). However, it is critical to un-
derscore that the outputs generated by Speculative
Decoding in FP16 precision may not consistently
align with those derived from autoregressive decod-
ing. This divergence stems from the accumulation
of floating-point errors inherent in FP16 computa-
tions, which can result in discrepancies between
the outputs of the two decoding methods, particu-
larly in the context of longer sequences. In FP32
precision, the outputs of Speculative Decoding are
guaranteed to be exactly the same as autoregressive
decoding.
We compare the speedup performance of Specu-
lative Decoding methods with FP16/FP32 precision
in Figure 10. The experimental results reveal a no-
ticeable reduction in speedup for all methods under
FP32 precision. Specifically, PLD (Saxena, 2023)
achieves merely 1.01 ×speedup in FP32 precision,
and the acceleration effect of EAGLE (Li et al.,
2024) also diminishes, with its speedup falling
from 2.39 ×to 1.74 ×. To furnish the research com-
munity with a comprehensive understanding of the
acceleration impact, we advocate for future stud-
ies to report speedup metrics across both precision
settings.
