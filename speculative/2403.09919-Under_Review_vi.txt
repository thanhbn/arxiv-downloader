# 2403.09919.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/speculative/2403.09919.pdf
# Kích thước tệp: 5243129 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Đang được đánh giá
RECURRENT DRAFTER CHO GIẢI MÃ ĐỐ ĐOÁN NHANH
TRONG CÁC MÔ HÌNH NGÔN NGỮ LỚN
Yunfei Cheng Aonan Zhang Xuanyu Zhang Chong Wang Yi Wang
Apple
{yunfei_cheng,aonan_zhang,xuanyu_zhang,mr.chongwang,wyi}@apple.com
https://github.com/apple/ml-recurrent-drafter
TÓM TẮT
Chúng tôi trình bày Recurrent Drafter (ReDrafter), một phương pháp giải mã đốt đoán tiên tiến đạt được tốc độ tăng tốc hiệu suất hàng đầu cho suy luận các mô hình ngôn ngữ lớn (LLM). Hiệu suất được thúc đẩy bởi ba khía cạnh chính: (1) tận dụng mạng nơ-ron hồi quy (RNN) làm mô hình dự thảo có điều kiện trên các trạng thái ẩn của LLM, (2) áp dụng thuật toán attention cây động trên kết quả tìm kiếm chùm để loại bỏ các tiền tố trùng lặp trong các chuỗi ứng cử viên, và (3) huấn luyện thông qua chưng cất kiến thức từ LLM. ReDrafter tăng tốc suy luận Vicuna trong MT-Bench lên tới 2.8x với triển khai PyTorch trên GPU Nvidia H100. Để chứng minh tính thực tiễn trong môi trường thực tế, chúng tôi cũng đã xác nhận hiệu quả của nó cho các ứng dụng trên thiết bị bằng cách triển khai phương pháp này trong MLX và đánh giá hiệu suất trên GPU Metal trong chip Apple Silicon, đạt được tăng tốc lên tới 2.3x. Chúng tôi tóm tắt kết quả thực nghiệm trong Hình 1.

1 GIỚI THIỆU
Hình 1: Tăng tốc suy luận trên MT-bench cho các mô hình Vicuna được hiển thị cho ba triển khai ReDrafter: (trái) PyTorch trên GPU Nvidia H100, (phải) và MLX trên GPU Metal M2 Ultra của Apple. Chúng tôi so sánh với EAGLE (Li et al., 2024a), Medusa (Cai et al., 2024), và auto-regressive trong triển khai PyTorch.

Giải mã đốt đoán (Leviathan et al., 2023; Spector & Re, 2023; Cai et al., 2024; Bhendawade et al., 2024) đã được nghiên cứu như một kỹ thuật đầy hứa hẹn để tăng tốc suy luận mô hình ngôn ngữ lớn (LLM) (Brown et al., 2020; Touvron et al., 2023; Achiam et al., 2023; Anil et al., 2023a;b; Gunter et al., 2024). Nó sử dụng các mô hình nhỏ hơn, hiệu quả hơn (thường được gọi là mô hình dự thảo) để dự đoán các chuỗi ứng cử viên, sau đó được xác minh bởi LLM. Ý tưởng cơ bản là cho phép LLM tập trung vào việc xác thực những ứng cử viên đó thay vì tạo ra từng token tuần tự. Phương pháp này giúp giảm thiểu nút thắt băng thông bộ nhớ bằng cách giảm nhu cầu lặp lại nhiều lần qua LLM. Vì mô hình dự thảo có thể tạo ra chi phí phụ, việc giảm số lần gọi LLM phải đủ để bù đắp chi phí này nhằm đạt được tăng tốc ròng.

Gần đây, Medusa (Cai et al., 2024) đạt được tăng tốc đáng kể bằng cách sử dụng các đầu dự thảo nhỏ gắn vào trạng thái ẩn của LLM, thay vì duy trì một mô hình dự thảo riêng biệt. Tuy nhiên, Medusa đòi hỏi nhiều đầu dự thảo với các tham số riêng biệt để chỉ ra vị trí dự đoán. Cơ chế dự đoán độc lập của nó không tận dụng cấu trúc tuần tự, dẫn đến độ chính xác dự đoán hạn chế và một tập hợp các chuỗi token ứng cử viên khả thi lớn theo cấp số mũ.

1arXiv:2403.09919v5  [cs.CL]  13 Dec 2024

--- TRANG 2 ---
Đang được đánh giá
trên   cô ấy[BẮT ĐẦU] Cô   tận hưởng   buổi   sáng   yên tĩnh,   nhâm nhi   cà phê   và   đọc   một   cuốn tiểu thuyết   bên   bờ hồ   dưới   bóng râm   của   những   cây   cổ thụ   [KẾT THÚC] yêu   cách   mùa thuAnh   liếc nhìn   đồng hồBước 1:Cô tận hưởng   ánh nắng mặt trời   trên nghe niềm vui buổi sáng,nhâm nhi   cà phé   và   xem[BẮT ĐẦU] Cô   tận hưởng   buổi   sáng   yên tĩnh,   nhâm nhi   cà phê   và   đọc   một   cuốn tiểu thuyết   bên   bờ hồ   dưới   bóng râm   của   những   cây   cổ thụ   [KẾT THÚC] [BẮT ĐẦU] Cô   tận hưởng   buổi   sáng   yên tĩnh,   nhâm nhi   cà phê   và   đọc   một   cuốn tiểu thuyết   bên   bờ hồ   dưới   bóng râm   của   những   cây   cổ thụ   [KẾT THÚC] [BẮT ĐẦU] Cô   tận hưởng   buổi   sáng   yên tĩnh,   nhâm nhi   cà phê   và   đọc   một   cuốn tiểu thuyết   bên   bờ hồ   dưới   bóng râm   của   những   cây   cổ thụ   [KẾT THÚC] Bước 2:Bước 3:Bước 4:[BẮT ĐẦU] Cô   tận hưởng   buổi   sáng   yên tĩnh,   nhâm nhi   cà phê   và   đọc   một   cuốn tiểu thuyết   bên   bờ hồ   dưới   bóng râm   của   những   cây   cổ thụ   [KẾT THÚC] Bước 5:

Hình 2: Minh họa quy trình giải mã ReDrafter. Tại mỗi bước suy luận, LLM tạo ra một token ban đầu (được tô màu xanh lá). Sau đó, mô hình dự thảo thực hiện tìm kiếm chùm (được chỉ ra bởi hộp đứt nét) sử dụng token được đảm bảo và các trạng thái ẩn lớp cuối từ LLM làm đầu vào. LLM sau đó xác minh chùm, và tiền tố được chấp nhận dài nhất (được tô màu xanh dương) được thêm vào ngữ cảnh. Điều này cho phép mỗi bước chấp nhận nhiều token thông qua một lần truyền xuôi duy nhất từ LLM. Quy trình lặp lại cho đến khi chuỗi kết thúc. ReDrafter đảm bảo rằng các token nó tạo ra giống hệt với những token được tạo ra bởi LLM (được đánh dấu màu xám).

Trong bài báo này, chúng tôi giới thiệu Recurrent Drafter (ReDrafter), để suy luận LLM nhanh. Hình 2 minh họa quy trình tạo sinh của nó. Hiệu suất của ReDrafter được thúc đẩy bởi ba khía cạnh chính:
(1) Sử dụng mạng nơ-ron hồi quy (RNN) (Mikolov et al., 2010) có điều kiện trên các trạng thái ẩn của LLM làm mô hình dự thảo khai thác các phụ thuộc thời gian cục bộ, cải thiện độ chính xác dự đoán của drafter và hiệu quả chuyển đổi tài nguyên tính toán thành tăng tốc. (2) Bằng cách sử dụng tìm kiếm chùm để khám phá nhiều chuỗi ứng cử viên và áp dụng thuật toán attention cây động để loại bỏ các tiền tố trùng lặp giữa các ứng cử viên, chúng tôi giảm đáng kể chi phí tính toán. (3) Huấn luyện thông qua chưng cất kiến thức (Zhou et al., 2023) từ LLM cải thiện sự liên kết của dự đoán mô hình dự thảo với những dự đoán của LLM, hiệu quả chuyển gánh nặng tính toán từ thời gian suy luận sang thời gian huấn luyện.

Trong triển khai PyTorch, ReDrafter tăng tốc suy luận Vicuna lên tới 2.8x so với phương pháp tự hồi quy trong MT-Bench trên GPU Nvidia H100, đạt được hiệu suất hiện đại nhất. Ngoài ra, chúng tôi xác nhận hiệu quả của ReDrafter sử dụng MLX trên GPU Metal trong chip Apple Silicon. Mặc dù tài nguyên tính toán hạn chế trong thiết lập này, chúng tôi quan sát thấy nút thắt bộ nhớ. ReDrafter hiệu quả giảm thiểu nút thắt này, dẫn đến tăng tốc lên tới 2.3x, chứng minh khả năng tối ưu hóa hiệu suất trong môi trường hạn chế tài nguyên.

2 NGHIÊN CỨU LIÊN QUAN VỀ GIẢI MÃ ĐỐT ĐOÁN
Được công nhận rộng rãi rằng việc tạo sinh LLM bị hạn chế bởi nút thắt bộ nhớ. Giải mã đốt đoán giảm thiểu nút thắt này bằng cách tăng cường độ tính toán, sử dụng mô hình dự thảo nhỏ hơn để dự đoán cục bộ các token có khả năng trong tương lai.

Gần đây, thiết kế mô hình dự thảo đã trải qua nhiều lần lặp. Leviathan et al. (2023); Chen et al. (2023); Spector & Re (2023); Sun et al. (2024) chọn sử dụng các mô hình dự thảo riêng biệt tách rời khỏi LLM. Đây là lựa chọn tiện lợi khi có một mô hình có sẵn xấp xỉ gần với LLM. Ví dụ, một phương pháp phổ biến là sử dụng biến thể nhỏ hơn từ cùng họ mô hình làm mô hình dự thảo. Nếu không có ứng cử viên có sẵn nào, mô hình dự thảo phải được huấn luyện riêng biệt khỏi LLM, với nỗ lực tập trung vào việc căn chỉnh nó càng gần càng tốt với LLM. Ngoài ra, triển khai hai mô hình riêng biệt thêm độ phức tạp cho việc tích hợp chúng trong một hệ thống phục vụ thống nhất.

2

--- TRANG 3 ---
Đang được đánh giá
Một hướng tiếp cận khác sử dụng chiến lược thống nhất bằng cách gắn mô hình dự thảo vào LLM, làm cho chúng phụ thuộc (Stern et al., 2018; Santilli et al., 2023; Cai et al., 2024). Đây là lựa chọn thiết kế hiệu quả khi mô hình dự thảo không được dự định sử dụng độc lập, cho phép nó tận dụng tài nguyên tính toán bổ sung cho dự đoán cục bộ bằng cách có điều kiện trên LLM. Trong số các phương pháp này, Cai et al. (2024) đề xuất sử dụng T đầu dự thảo độc lập để dự đoán T token tiếp theo, sử dụng sức mạnh tính toán song song bổ sung của GPU. Nỗ lực tính toán quá mức này có thể không mang lại tăng tốc tương xứng, vì dự đoán độc lập trở nên kém chính xác hơn khi T tăng, dẫn đến độ chính xác dự đoán không tối ưu và tỷ lệ chấp nhận thấp hơn từ LLM. Một phương pháp thay thế là tăng cường độ chính xác dự đoán bằng cách kết hợp tính hồi quy vào mô hình dự thảo để nắm bắt các phụ thuộc cục bộ (Bhendawade et al., 2024; Li et al., 2024a;b; ?). Tuy nhiên, việc giảm song song hóa do tính hồi quy dẫn đến sử dụng GPU thấp hơn, tạo ra chi phí phụ làm giảm lợi ích tăng tốc, ngay cả khi tỷ lệ chấp nhận cao hơn đáng kể.

ReDrafter sử dụng RNN nhẹ làm mô hình dự thảo để dự đoán các token sắp tới trong chuỗi. Nó phân bổ tài nguyên tính toán cho tìm kiếm chùm, dẫn đến tỷ lệ chấp nhận cao hơn. Cường độ tìm kiếm chùm được kiểm soát bởi độ rộng chùm và độ dài, có thể được điều chỉnh dựa trên khả năng phần cứng và triển khai cụ thể. ReDrafter áp dụng chưng cất kiến thức (Xia et al., 2023; Miao et al., 2023; Liu et al., 2023; Zhou et al., 2023) từ LLM, cải thiện hiệu quả thời gian suy luận bằng cách đầu tư nhiều tài nguyên hơn vào thời gian huấn luyện. Kết quả thực nghiệm của chúng tôi tiết lộ rằng ReDrafter sử dụng tính toán hiệu quả hơn so với các phương pháp trước đây, mang lại tăng tốc hiện đại nhất trên các triển khai và nền tảng phần cứng khác nhau.

3 REDRAFTER
3.1 MÔ HÌNH DỰ THẢO

đọc sáng buổi sáng,   nhâm nhi     cà phê         và      xem        ayên tĩnh
yên tĩnh      buổi sáng,   nhâm nhi     cà phê         và        đọch0p[BẮT ĐẦU] Cô   tận hưởng   buổie0e1h1qh2qe2e3h3qh4qe4h5qh6qe5pqpBước 2:Bước 3:h0pq

[BẮT ĐẦU] Cô   tận hưởng   buổi   sáng   yên tĩnh,   nhâm nhi   cà phê   và   đọcsáng nghe nhạcnhâm nhiCà phê và xemtrên cô ấy sáng nghe nhạcnhâm nichà phé và xemtrên cô ấy trên   cô ấy sáng,nhâm nhi   cà phê   và   xem[BẮT ĐẦU] Cô   tận hưởng   buổi   sáng   yên tĩnh,   nhâm nhi   cà phê   và   đọcsángNhâm nichà phéVànghe nhạcnhâm nichà phéVàxemtrên cô ấysángNhâm nichà phéVànghe nhạcnhâm nichà phéVàxemtrên cô ấy sáng,sáng,nhâm nhi   cà phêsángNhâm nichà phésángsángNhâm nichà phésáng

(a)(b)

trên   cô ấy
[BẮT ĐẦU] Cô   tận hưởng   buổi   sáng   yên tĩnh,   nhâm nhi   cà phê   và   đọc   một   cuốn tiểu thuyết   bên   bờ hồ   dưới   bóng râm   của   những   cây   cổ thụ   [KẾT THÚC] yêu   cách   mùa thuAnh   liếc nhìn   đồng hồBước 1:Cô tận hưởng   ánh nắng mặt trời   trên nghe nhạcsáng,nhâm nhi   cà phê   và   xem[BẮT ĐẦU] Cô   tận hưởng   buổi   sáng   yên tĩnh,   nhâm nhi   cà phê   và   đọc   một   cuốn tiểu thuyết   bên   bờ hồ   dưới   bóng râm   của   những   cây   cổ thụ   [KẾT THÚC] [BẮT ĐẦU] Cô   tận hưởng   buổi   sáng   yên tĩnh,   nhâm nhi   cà phê   và   đọc   một   cuốn tiểu thuyết   bên   bờ hồ   dưới   bóng râm   của   những   cây   cổ thụ   [KẾT THÚC] [BẮT ĐẦU] Cô   tận hưởng   buổi   sáng   yên tĩnh,   nhâm nhi   cà phê   và   đọc   một   cuốn tiểu thuyết   bên   bờ hồ   dưới   bóng râm   của   những   cây   cổ thụ   [KẾT THÚC] 

Bước 2:Bước 3:Bước 4:[BẮT ĐẦU] Cô   tận hưởng   buổi   sáng   yên tĩnh,   nhâm nhi   cà phê   và   đọc   một   cuốn tiểu thuyết   bên   bờ hồ   dưới   bóng râm   của   những   cây   cổ thụ   [KẾT THÚC] nhâm nhi   cà phê   và   nghe nhạc

Bước 5:trên   cô ấynghe nhạcsáng,nhâm nhi   cà phê   và   xem

yên tĩnhyên tĩnh      buổi sáng,   nhâm nhi      cà phê        vàht[BẮT ĐẦU] Cô   tận hưởng   buổiet+1et+2et+3et+4et+5LLMDrafter trên           cô ấysáng,nhâm nhi     cà phêgt+1gt+2gt+3gt+4gt+5sángNhâm nichà phévànghe nhạcsángNhâm nichà phévàxemsángNhâm nichà phêtrên cô ấy000000000100022dedup_prefixbeamprefix_treepack_beamsángNhâm nichà phévànghe nhạcxemtrên cô ấypacked_beam(c)

yên tĩnh      buổi sáng,   nhâm nhi      cà phê        vàh[BẮT ĐẦU] Cô   tận hưởng   buổie1e2e3e4e5LLMMô hình dự thảo và       đọcsáng,nhâm nhi      cà phêg1g2g3g4g5

Hình 3: Mô hình dự thảo lấy trạng thái ẩn cuối cùng của LLM làm đầu vào để dự báo một vài token tiếp theo. Để ngắn gọn, chúng tôi bỏ qua các tham số mô hình và trạng thái ẩn LLM trước h.

Chúng tôi phác thảo công thức của mô hình dự thảo trong Hình 3. Tương tự như phương pháp Medusa, chúng tôi sử dụng đầu ra lớp cuối của transformer từ LLM làm đầu vào cho mô hình dự thảo. Ngoài ra, chúng tôi kết hợp nhúng của các token lịch sử làm đầu vào hồi quy cho đầu dự thảo.

Chúng tôi sử dụng thiết kế RNN tiêu chuẩn để dự đoán token tiếp theo. Ví dụ, xem xét ngữ cảnh "Cô tận hưởng" , một khi LLM tạo ra token "yên tĩnh" với đầu ra lớp cuối h, mô hình dự thảo sử dụng nhúng của token "yên tĩnh" để cập nhật trạng thái ẩn RNN của nó và kết hợp với đầu ra h để dự đoán token tiếp theo "buổi sáng" . Chiến lược này được áp dụng hồi quy cho các token tiếp theo. Trong bài báo này, chúng tôi chọn thiết kế hồi quy đơn giản để mô hình hóa các kết nối giữa các đầu dự thảo được chia sẻ, hoãn các lựa chọn mô hình phức tạp hơn cho các nghiên cứu tương lai. Cụ thể, chúng tôi khởi tạo trạng thái ẩn của mô hình dự thảo là g1= [s1, h], trong đó s1:=e1 là nhúng của token cuối cùng mà LLM tạo ra. Để dự đoán token thứ t sử dụng mô hình dự thảo, trước tiên chúng tôi cập nhật trạng thái ẩn của nó,
gt= [st, h], s t=f(Ust−1+Wet+b),
trong đó f là hàm kích hoạt và U,W và b là tham số cho RNN (Mikolov & Zweig, 2012). Chúng tôi chỉ sử dụng RNN một lớp để làm cho mô hình đơn giản. Sau đó chúng tôi áp dụng một vài lớp MLP với kết nối bỏ qua, theo sau là lớp softmax tiêu chuẩn ở cuối. Vì các tham số của đầu dự thảo được chia sẻ, số lượng tham số vẫn không đổi ngay cả khi mô hình được huấn luyện để dự đoán nhiều hơn một token.

3.2 TÌM KIẾM CHÙM
Mô hình dự thảo kích hoạt thuật toán tìm kiếm chùm trong quá trình suy luận. Điều này cho phép mô hình khám phá nhiều loại tiếp tục có thể cho trước ngữ cảnh, xếp hạng chúng theo xác suất trong khi theo dõi nhiều chuỗi tiềm năng. Nó giúp duy trì sự cân bằng giữa đa dạng và tối ưu trong việc tạo ứng cử viên.

3

--- TRANG 4 ---
Đang được đánh giá
đọcsáng buổi sáng,   nhâm nhi     cà phê         và      xem        ayên tĩnh
yên tĩnh      buổi sáng,   nhâm nhi     cà phê         và        đọch0p[BẮT ĐẦU] Cô   tận hưởng   buổie0e1h1qh2qe2e3h3qh4qe4h5qh6qe5pqpBước 2:Bước 3:h0pq

[BẮT ĐẦU] Cô   tận hưởng   buổi   sáng   yên tĩnh,   nhâm nhi   cà phê   và   đọcsáng nghe nhạcnhâm nhicà phêvàxemtrên cô ấysáng nghe nhạcnhâm nichà phêvàxemtrên cô ấy trên   cô ấy sáng,nhâm nhi   cà phê   và   xem[BẮT ĐẦU] Cô   tận hưởng   buổi   sáng   yên tĩnh,   nhâm nhi   cà phê   và   đọcsángNhâm nichà phêVànghe nhạcnhâm nichà phêVàxemtrên cô ấysángNhâm nichà phêVànghe nhạcnhâm nichà phêVàxemtrên cô ấy sáng,sáng,nhâm nhi   cà phêsángNhâm nichà phêsángsángNhâm nichà phêsáng

(a)(b)

trên   cô ấy
[BẮT ĐẦU] Cô   tận hưởng   buổi   sáng   yên tĩnh,   nhâm nhi   cà phê   và   đọc   một   cuốn tiểu thuyết   bên   bờ hồ   dưới   bóng râm   của   những   cây   cổ thụ   [KẾT THÚC] yêu   cách   mùa thuAnh   liếc nhìn   đồng hồBước 1:Cô tận hưởng   ánh nắng mặt trời   trên nghe nhạcsáng,nhâm nhi   cà phê   và   xem[BẮT ĐẦU] Cô   tận hưởng   buổi   sáng   yên tĩnh,   nhâm nhi   cà phê   và   đọc   một   cuốn tiểu thuyết   bên   bờ hồ   dưới   bóng râm   của   những   cây   cổ thụ   [KẾT THÚC] [BẮT ĐẦU] Cô   tận hưởng   buổi   sáng   yên tĩnh,   nhâm nhi   cà phê   và   đọc   một   cuốn tiểu thuyết   bên   bờ hồ   dưới   bóng râm   của   những   cây   cổ thụ   [KẾT THÚC] [BẮT ĐẦU] Cô   tận hưởng   buổi   sáng   yên tĩnh,   nhâm nhi   cà phê   và   đọc   một   cuốn tiểu thuyết   bên   bờ hồ   dưới   bóng râm   của   những   cây   cổ thụ   [KẾT THÚC] 
Bước 2:Bước 3:Bước 4:[BẮT ĐẦU] Cô   tận hưởng   buổi   sáng   yên tĩnh,   nhâm nhi   cà phê   và   đọc   một   cuốn tiểu thuyết   bên   bờ hồ   dưới   bóng râm   của   những   cây   cổ thụ   [KẾT THÚC] nhâm nhi   cà phê   và   nghe nhạc

Bước 5:trên   cô ấynghe nhạcsáng,nhâm nhi   cà phê   và   xem

yên tĩnhyên tĩnh      buổi sáng,   nhâm nhi      cà phê        vàht[BẮT ĐẦU] Cô   tận hưởng   buổiet+1et+2et+3et+4et+5LLMDrafter trên           cô ấysáng,nhâm nhi     cà phêgt+1gt+2gt+3gt+4gt+5sángNhâm nichà phêvànghe nhạcsángNhâm nichà phêvàxemsángNhâm nichà phêtrên cô ấy000000000100022dedup_prefixbeamprefix_treepack_beamsángNhâm nichà phêvànghe nhạcxemtrên cô ấypacked_beam(c)

Hình 4: Tìm kiếm chùm tạo ra ba chuỗi ứng cử viên. (a) Không có attention cây động, sau khi làm phẳng các token trên tất cả ứng cử viên, 15 token cần được gửi đến LLM để xác minh. (b) Với attention cây động, chúng ta có thể cắt bỏ tiền tố được chia sẻ, giảm tổng số xuống 8 token sau khi làm phẳng. Chúng tôi điều chỉnh mặt nạ attention cây tương ứng để phản ánh các phụ thuộc token. (c) Minh họa cho việc xây dựng attention cây động cho kích thước lô bằng 1. (Mở rộng đến kích thước lô lớn hơn 1 là đơn giản.) Chúng tôi sử dụng thuật toán dựa trên tensor, thân thiện với GPU để đóng gói chùm thành "chùm được đóng gói". Các mặt nạ attention có thể được xử lý tương ứng theo cách thân thiện với GPU.

Tham số độ rộng chùm đề cập đến số lượng chuỗi token dự thảo. Độ rộng chùm lớn hơn tăng khả năng chuỗi có tiền tố chấp nhận được dài nhất được bao gồm trong chùm. Điều này cho phép LLM chấp nhận nhiều token hơn trong mỗi bước giải mã, giảm tổng số bước giải mã cần thiết—hoặc tương đương, giảm số lần gọi LLM. Trong khi chùm rộng hơn đòi hỏi nhiều FLOP hơn cho tìm kiếm chùm và xác minh LLM, GPU mạnh mẽ có thể xử lý các FLOP này song song, giảm thiểu sự gia tăng thời gian wall.

3.3 ATTENTION CÂY ĐỘNG
Tìm kiếm chùm trả về các ứng cử viên dự thảo với tiền tố được chia sẻ. Ví dụ, trong Hình 4(a), ứng cử viên thứ hai "sáng nhâm nhi cà phê và xem" và ứng cử viên thứ ba "sáng nhâm nhi cà phê trên cô ấy" chia sẻ tiền tố "sáng nhâm nhi cà phê" . Gửi những token trùng lặp đó đến LLM dẫn đến chi phí tính toán. Kết quả là, chúng tôi loại bỏ những tiền tố được chia sẻ đó, tiết lộ cấu trúc cây trên kết quả tìm kiếm chùm. Khi gửi ứng cử viên khử trùng đến LLM để xác minh, chúng tôi sửa đổi mặt nạ attention để tiết lộ các phụ thuộc giữa các token (Hình 4(b)). Chúng tôi gọi cơ chế này là "attention cây động".

Việc sử dụng cấu trúc cây để tiết kiệm tính toán giống với các phương pháp thấy trong Miao et al. (2023); Spector & Re (2023); Cai et al. (2024), nơi các phương pháp tương tự được sử dụng để tránh các tính toán dư thừa cho tiền tố được chia sẻ. Tuy nhiên, không giống như việc sử dụng cấu trúc cây được đề cập ở trên, chúng ta phải xác định của chúng ta một cách động vì nó dựa vào kết quả tìm kiếm chùm cá nhân tại thời gian chạy. Thuật toán dựa trên trie tiêu chuẩn chậm vì khó song song hóa. Chúng tôi nhận thấy rằng một tính chất độc đáo của vấn đề của chúng tôi là tất cả chuỗi ứng cử viên có cùng độ dài. Với cái nhìn sâu sắc này, chúng tôi phát hiện rằng attention cây động của chúng tôi có thể được xây dựng hiệu quả bằng cách sử dụng các toán tử tensor tiêu chuẩn, một khía cạnh quan trọng để tận dụng các gia tốc hiện đại với chi phí phụ ít. Điều này dẫn đến tăng tốc suy luận LLM trong giải mã đốt đoán.

Quan sát chính là chúng ta có thể tìm tất cả tiền tố được chia sẻ trong kết quả tìm kiếm chùm bằng cách sử dụng các phép toán tensor tiêu chuẩn mà không xây dựng trie tiền tố. Chúng tôi đã phát triển các hàm gọi là dedup_prefix để xác định tiền tố được chia sẻ, và một hàm khác pack_beam để nén chùm thành chùm "được đóng gói" khử trùng (được minh họa trong Hình 4(c)). Chúng tôi cung cấp mã giả cho dedup_prefix trong Phụ lục A.1 để cung cấp cái nhìn thoáng qua về cách thuật toán dựa trên tensor này hoạt động. Các phép toán tiếp theo có thể được thiết kế tương tự để sử dụng các tiền tố được tìm thấy ở đây để xây dựng attention cây động. Điều này dẫn đến tăng tốc suy luận LLM trong giải mã đốt đoán. Việc sử dụng attention cây động

4

--- TRANG 5 ---
Đang được đánh giá
attention không chỉ giới hạn ở ReDrafter. Nó cũng có thể được sử dụng trong các phương pháp giải mã đốt đoán tách rời trong khi mô hình dự thảo riêng biệt thực hiện tìm kiếm chùm và sau đó áp dụng attention cây động.

3.4 GIẢI MÃ ĐỐT ĐOÁN VỚI REDRAFTER
Chúng tôi mô tả ngắn gọn các bước sử dụng ReDrafter cho giải mã đốt đoán. Trong mỗi bước tạo sinh trong quá trình suy luận, ReDrafter luân phiên giữa việc sử dụng mô hình dự thảo để tạo token và gọi LLM để xác minh và chấp nhận chúng. Chúng tôi bắt đầu với tất cả các token được tạo trước đó với trạng thái ẩn cuối cùng. Chúng tôi sử dụng tìm kiếm chùm để tạo ra một chùm, bao gồm một tập hợp các chuỗi ứng cử viên. Tiếp theo, attention cây động được áp dụng để làm phẳng và nén chùm thành chùm được đóng gói, trong khi hình thành mặt nạ attention phù hợp. LLM sau đó tiến hành với một lần truyền xuôi để tính toán xác suất log cho tất cả các token được đề xuất. Sau đó, chúng tôi chọn ứng cử viên tốt nhất với tiền tố được chấp nhận dài nhất. Phương pháp lựa chọn có thể từ phương pháp tham lam (aka. token khớp), đến lấy mẫu từ chối. Đồng thời, LLM cung cấp trạng thái ẩn và token ban đầu cho lần gọi mô hình dự thảo tiếp theo. Chúng tôi thêm các token được chấp nhận vào cuối các token được tạo trước đó và chạy lần lặp tiếp theo cho đến khi tiêu chí dừng được thỏa mãn. ReDrafter đảm bảo chuỗi được tạo khớp với đầu ra của LLM.

3.5 HUẤN LUYỆN REDRAFTER VỚI CHƯNG CẤT KIẾN THỨC
Hiệu quả của ReDrafter được tối ưu hóa khi tất cả các token ứng cử viên được chấp nhận. Có nghĩa là, mô hình dự thảo đưa ra dự đoán giống như LLM trong phạm vi dự đoán T của nó. Hàm mất tự nhiên là divergence KL:
min
pdraftKL(pllm(y1:T)|pdraft(y1:T)=min
pdraftEpllm(y1:T)−logpdraft(y1:T) (1)

Điều này có nghĩa là, thay vì sử dụng token sự thật ground truth làm nhãn cho mô hình dự thảo, chúng ta nên sử dụng dự đoán xác suất từ LLM, theo cách tương tự như các phương pháp chưng cất kiến thức truyền thống (Kim & Rush, 2016). Tại mỗi vị trí t của chuỗi huấn luyện, chúng tôi lấy mẫu byt+1:t+T có điều kiện trên y1:t từ LLM và tối ưu hóa mất thực nghiệm sau đây
min
pdraftLdistill= min
pdraftX
t−logpdraft(byt+1:t+T|y1:t). (2)

Các phương pháp giải mã đốt đoán khác, như Medusa2 (Cai et al., 2024), cũng kết hợp chưng cất kiến thức. Ngược lại, ReDrafter chỉ lan truyền ngược qua mô hình dự thảo, giữ LLM không thay đổi để đảm bảo kết quả giải mã vẫn nhất quán. Ngoài ra, chúng tôi áp dụng chưng cất cục bộ bằng cách cho LLM dự đoán T token tiếp theo sử dụng các token sự thật ground truth làm ngữ cảnh.

4 THỰC NGHIỆM
Chúng tôi tiến hành thực nghiệm trong môi trường thực nghiệm và sẵn sàng sản xuất, sử dụng các mô hình Vicuna 7B, 13B, 33B làm LLM cơ sở. Đầu tiên, sử dụng PyTorch, chúng tôi so sánh ReDrafter với các phương pháp giải mã đốt đoán hiện đại nhất trên GPU Nvidia H100 trong Phần 4.1. Tiếp theo, chúng tôi nghiên cứu triển khai trên thiết bị của ReDrafter trên GPU Metal sử dụng MLX, chứng minh khả năng tăng tốc suy luận trên thiết bị với tài nguyên tính toán hạn chế trong Phần 4.2.

Ngoài ra, chúng tôi tiến hành nghiên cứu ablation sử dụng PyTorch. Trong Phần 4.3.1, chúng tôi khám phá sự đánh đổi hiệu suất của ReDrafter khi điều chỉnh độ rộng chùm và kích thước lô. Chúng tôi đánh giá lợi ích của attention cây động trong Phần 4.3.2 và kiểm tra cải thiện hiệu suất thông qua chưng cất kiến thức trong Phần 4.3.3.

4.1 BENCHMARK PYTORCH
Chúng tôi so sánh ReDrafter với Medusa (Cai et al., 2024) và EAGLE (Li et al., 2024a) sử dụng PyTorch. Đối với mỗi phương pháp, chúng tôi báo cáo tăng tốc so với giải mã tự hồi quy, cũng như số lượng token trung bình được LLM chấp nhận mỗi bước tạo sinh (Tokens/Step) trên MT-Bench (Zheng et al., 2024) và Alpaca (Dubois et al., 2024). Chúng tôi chủ yếu tiến hành thực nghiệm với temperature = 0, tức là giải mã tham lam.

5

--- TRANG 6 ---
Đang được đánh giá
Bảng 1: Tăng tốc và Tokens/Step trên MT-Bench và AlpacaEval.

MT-bench
PhươngphápVicuna 7B Vicuna 13B Vicuna 33B
Tăng tốc Tokens/Step Tăng tốc Tokens/Step Tăng tốc Tokens/Step
Medusa 2.39x 2.55 2.40x 2.61 2.51x 2.53
EAGLE 2.69x 3.96 2.74x 4.00 2.80x 3.71
ReDrafter 2.80x 4.20 2.80x 4.21 2.61x 3.87

AlpacaEval
PhươngphápVicuna 7B Vicuna 13B Vicuna 33B
Tăng tốc Tokens/Step Tăng tốc Tokens/Step Tăng tốc Tokens/Step
Medusa 2.19x 2.42 2.26x 2.45 2.31x 2.31
EAGLE 2.43x 3.61 2.49x 3.62 2.59x 3.29
ReDrafter 2.69x 4.06 2.78x 4.02 2.43x 3.61

Hình 5: Tăng tốc ReDrafter và Tokens/Step trên các danh mục phụ trong MT-Bench và AlpacaEval.

Bảng 1 so sánh các phương pháp khác nhau. ReDrafter đạt được tăng tốc và Tokens/Step cao nhất với Vicuna 7B và 13B. Đối với 33B, ReDrafter đạt được Tokens/Step cao nhất, mặc dù tăng tốc của nó thấp hơn một chút so với EAGLE.

Hình 5 minh họa rằng ReDrafter hoạt động nhất quán tốt trên tất cả kích thước mô hình và danh mục bộ dữ liệu trong cả MT-Bench và Alpaca. Có một khoảng cách giữa Tokens/Second và tăng tốc, điều này được dự đoán và phát sinh từ chi phí phụ liên quan đến quá trình giải mã đốt đoán.

4.2 SỬY LUẬN TRÊN THIẾT BỊ DỰA TRÊN MLX
Bộ nhớ, băng thông và sức mạnh tính toán ngày càng tăng của các thiết bị cá nhân khiến nó trở thành con đường đầy hứa hẹn để triển khai trợ lý AI cục bộ. Mặc dù được biết rằng GPU trên thiết bị có ít sức mạnh tính toán và băng thông hơn so với các hệ thống dựa trên CUDA, kịch bản trên thiết bị đơn giản hơn, với một người dùng duy nhất tương tác với LLM được triển khai cục bộ. Thiết lập này cung cấp cơ hội để khai thác tài nguyên tính toán có sẵn cho giải mã đốt đoán.

Chúng tôi đánh giá ReDrafter trên GPU Metal trong chip Apple Silicon, cụ thể là M1 Max và M2 Ultra mạnh mẽ hơn, sử dụng triển khai MLX. Bảng 2 cho thấy tăng tốc đầy hứa hẹn 1.37x trên M1 Max và tăng tốc cao hơn trên M2 Ultra, chứng minh tính khả thi của ReDrafter cho trường hợp sử dụng trên thiết bị. Chúng tôi bỏ qua thực nghiệm cho các mô hình 13B và 33B trên M1 Max, vì chúng vượt quá dung lượng bộ nhớ của thiết bị. Trong phần sau, chúng tôi thảo luận ngắn gọn về kết quả thực nghiệm liên quan đến độ rộng chùm, trong khi để chi tiết triển khai, kết quả bổ sung và các cái nhìn quan trọng khác cho độc giả quan tâm trong Phụ lục A.2.

Kết quả trong Bảng 2 cho thấy rằng mặc dù độ rộng chùm cao hơn luôn mang lại nhiều token hơn mỗi bước và giảm số lần gọi LLM, tăng tốc tối ưu được đạt tại độ rộng chùm thấp hơn. Ví dụ, hiệu suất tốt nhất xảy ra tại BW=1 trên M1 Max cho mô hình 7B, và BW=3 trên M2 Ultra cho cả mô hình 7B và 13B. Như đã thảo luận trong Phần 3.2, chùm rộng hơn đòi hỏi

6

--- TRANG 7 ---
Đang được đánh giá
Bảng 2: Thực nghiệm MLX với độ rộng chùm (BW) khác nhau. Chúng tôi cố định batch size=1, beam length=4. Xem A.2 để biết thêm chi tiết thiết lập thực nghiệm.

ChipMô hìnhBW=1 BW=2 BW=3 BW=4
TPS Tăng tốc Tokens/Step TPS Tăng tốc Tokens/Step TPS Tăng tốc Tokens/Step TPS Tăng tốc Tokens/Step
M1 Max V 7B 28.22 1.32x 2.15 27.69 1.30x 2.38 27.54 1.29x 2.44 20.16 0.94x 2.44
M2 UltraV 7B 57.14 1.43x 2.15 60.24 1.51x 2.38 60.40 1.52x 2.44 30.05 0.75x 2.44
V 13B 41.94 1.87x 2.53 43.52 1.94x 2.82 43.55 1.94x 2.82 21.83 0.97x 2.94
V 33B 1.15 1.97x 2.17 1.22 2.08x 2.33 1.28 2.19x 2.47 1.33 2.28x 2.56

Bảng 3: So sánh tokens trên giây cho mỗi yêu cầu (TPS), tokens trên giây của hệ thống (TPS ×BSZ) cho ReDrafter Vicuna 7B với độ rộng chùm (BW) và kích thước lô (BSZ) khác nhau trên MT-Bench.

BSZBW=1 BW=2 BW=4 BW=8 BW=16 BW=32 BW=64
TPS TPS ×BSZ TPS TPS ×BSZ TPS TPS ×BSZ TPS TPS ×BSZ TPS TPS ×BSZ TPS TPS ×BSZ TPS TPS ×BSZ
1 62.55 62.55 73.32 73.32 80.47 80.47 88.31 88.31 100.16 100.16 104.51 104.51 110.64 110.64
2 58.56 117.13 70.25 140.49 75.77 151.54 86.64 173.29 99.45 198.90 107.52 215.04 111.42 222.83
4 57.51 230.03 67.42 269.67 74.77 299.09 81.95 327.81 97.13 388.51 99.11 396.46 85.23 340.94
8 53.14 425.14 60.48 483.85 69.49 555.90 81.14 649.11 83.44 667.52 73.41 587.30 54.19 433.48
10 53.61 536.14 58.79 587.85 66.29 662.88 71.55 715.48 73.57 735.68 65.31 653.10 44.23 442.25
20 45.16 903.20 49.05 980.95 57.35 1146.95 53.87 1077.45 48.76 975.14 35.66 713.17 23.28 465.70
40 32.69 1307.65 33.59 1343.41 36.68 1467.04 34.33 1373.08 26.43 1057.20 19.15 766.03 12.09 483.58
80 18.94 1515.40 20.45 1636.36 19.65 1571.89 OOM OOM OOM OOM OOM OOM OOM OOM

nhiều phép toán dấu phẩy động (FLOP) để LLM xác minh các token dự thảo. Mặc dù GPU mạnh mẽ có thể xử lý các FLOP này song song, một khi chi phí tính toán đạt đến giới hạn phần cứng, lợi ích hiệu suất từ giải mã đốt đoán giảm dần. Điều này giải thích tại sao tăng tốc tối ưu cho M1 Max được đạt tại BW=1, trong khi đối với M2 Ultra, nó xảy ra tại BW=3, làm nổi bật sức mạnh tính toán lớn hơn của M2 Ultra. Tuy nhiên, hiệu suất giảm tại BW=4 cho cả hai thiết bị do chi phí tăng của việc xác minh token dự thảo.

Chúng tôi quan sát thấy TPS giảm mạnh từ 43.55 xuống 1.33 khi kích thước mô hình tăng từ 13B lên 33B, nhưng vẫn đạt được tăng tốc đáng kể. Phỏng đoán của chúng tôi là mô hình 33B gặp phải nút thắt bộ nhớ/IO do kích thước mô hình tăng, khiến suy luận bị chi phối bởi hoán đổi bộ nhớ và truyền dữ liệu, làm giảm đáng kể TPS. Với nghiên cứu trên, chúng tôi tin rằng ReDrafter có tiềm năng đáng kể để cải thiện thêm khi phần cứng trên thiết bị tiếp tục phát triển. Tuy nhiên, đối với các mô hình lớn hơn, các kỹ thuật nén như lượng tử hóa có thể cần thiết để đạt được độ trễ chấp nhận được.

4.3 NGHIÊN CỨU ABLATION TRONG PYTORCH
4.3.1 ĐỘ RỘNG CHÙM VÀ KÍCH THƯỚC LÔ
Trong ReDrafter, độ rộng chùm và kích thước lô là hai yếu tố quan trọng để tận dụng tài nguyên tính toán dư thừa. Mặc dù tăng một trong hai có thể đẩy hệ thống đến giới hạn tính toán, chúng tác động đến hệ thống khác nhau: độ rộng chùm lớn hơn làm tăng khả năng tạo ra chuỗi dự thảo có nhiều khả năng được chấp nhận hơn, trong khi kích thước lô lớn hơn cải thiện thông lượng tổng thể của hệ thống.

Chúng tôi kiểm tra sự đánh đổi giữa độ rộng chùm và kích thước lô bằng cách tiến hành tìm kiếm lưới trên cả hai tham số trên GPU Nvidia H100 sử dụng MT-Bench, với Vicuna-7B làm LLM. Chúng tôi đo tokens-trên-giây cho mỗi yêu cầu (TPS) để đánh giá độ trễ (nghịch đảo của độ trễ) và tokens-trên-giây cho mỗi yêu cầu nhân với kích thước lô (TPS ×BSZ) để đánh giá thông lượng tổng thể của hệ thống. Kết quả được tóm tắt trong Bảng 3.

Khi độ rộng chùm được giữ không đổi và kích thước lô tăng, TPS ban đầu vẫn ổn định nhưng cuối cùng giảm, trong khi TPS ×BSZ tiếp tục tăng. Điều này cho thấy việc tăng kích thước lô cải thiện sử dụng GPU và cho phép xử lý lô hiệu quả hơn, mặc dù ít tài nguyên hơn được phân bổ cho mỗi yêu cầu cá nhân. Sự giảm TPS xảy ra sớm hơn ở độ rộng chùm cao hơn do chi phí tính toán bổ sung. Tại kích thước lô 80, lỗi hết bộ nhớ (OOM) xảy ra ở độ rộng chùm lớn hơn, chỉ ra dung lượng bộ nhớ hạn chế.

Cấu hình tối ưu cho TPS mỗi yêu cầu và TPS × BSZ khác nhau. TPS mỗi yêu cầu cao nhất, khoảng 110, được đạt với độ rộng chùm 64 và kích thước lô 1 hoặc 2, trong khi TPS×BSZ cao nhất, khoảng 1636, được đạt với độ rộng chùm 2 và kích thước lô 80. Điều này

7

--- TRANG 8 ---
Đang được đánh giá
100 150 200 250 300 350
Độ dài đầu ra được tạo2345Số token được tạo mỗi lần chuyển tiếpbeam=5
beam=20
beam=40
50 100 150 200 250 300 350
Số token trước nén050100150200Số token sau nén

99 pct
Trung vị
1 pct
100101
Kích thước lô20406080100Tokens/giây
RD w TA
RD w/o TA
0 10 20 30 40 50 605101520253035

10020030040050060070080090010001100
Kích thước lô * Tokens/giây

Hình 6: Trái: Số token để được xác minh bởi LLM trước và sau nén sử dụng attention cây động. Phải: Tokens mỗi giây cho mỗi yêu cầu và tokens tổng thể mỗi giây cho ReDrafter có, không có attention cây động. RD là viết tắt của ReDrafter.

nhấn mạnh tầm quan trọng của việc điều chỉnh độ rộng chùm và kích thước lô dựa trên các trường hợp sử dụng cụ thể. Đối với các kịch bản ưu tiên độ trễ thấp, độ rộng chùm lớn hơn với kích thước lô nhỏ hơn được khuyến nghị. Ngược lại, nếu thông lượng cao là mục tiêu chính, kích thước lô lớn hơn kết hợp với độ rộng chùm vừa phải hiệu quả hơn.

4.3.2 ATTENTION CÂY ĐỘNG
Như được nêu trong Phần 3.3, attention cây động giảm đáng kể chi phí tính toán bằng cách khử trùng token dự thảo. Chúng tôi nghiên cứu hiệu quả của nó và chứng minh kết quả thực nghiệm trong Hình 6.

Lợi ích tính toán từ việc sử dụng attention train được xác định bởi tỷ lệ nén, là số token trong chùm chia cho số token trong chùm được đóng gói. Chúng tôi tiến hành nghiên cứu thực nghiệm trên MT-Bench, sử dụng Vicuna 7B làm LLM, với kích thước lô cố định là 1, độ dài chùm là 5 và thay đổi độ rộng chùm từ 5 đến 70. Điều này dẫn đến số lượng token mỗi chùm từ 25 đến 350. Sau đó chúng tôi đo số lượng token trung bình trong chùm được đóng gói sau khi áp dụng attention cây động. Như được hiển thị trong Hình 6 (trái), attention cây động hiệu quả giảm số lượng token dự thảo từ 30% đến 60%. Nó cũng chứng minh tỷ lệ nén nhất quán trên các kích thước chùm khác nhau, với tỷ lệ vẫn có thể dự đoán ngay cả trong các trường hợp cực đoan (percentile thứ 99 và 1).

Chúng tôi chứng minh rằng attention cây động có thể tăng cường hiệu suất thêm dưới các ràng buộc tính toán. Chúng tôi cố định độ dài chùm là 5, độ rộng chùm là 45, và điều chỉnh kích thước lô để đẩy giới hạn tính toán, Hình 6 (phải) cho thấy khi kích thước lô dưới 4, tài nguyên tính toán dồi dào, và không có sự khác biệt đáng kể trong TPS hoặc TPS ×BSZ. Tuy nhiên, khi kích thước lô vượt quá 4, chúng ta gặp phải nút thắt tính toán. Trong kịch bản này, ReDrafter với attention cây động (RD w TA) vượt trội đáng kể so với ReDrafter không có tree attention, mang lại thông lượng cao hơn và nhiều token hơn mỗi giây. Trong triển khai thực tế, cả tốc độ và thông lượng nên được xem xét để đưa ra quyết định cân bằng.

4.3.3 CHƯNG CẤT KIẾN THỨC
Bảng 4: So sánh Tăng tốc và Tokens được chấp nhận trung bình mỗi bước (Tokens/Step) cho ReDrafter sử dụng Vicuna 7B có và không có chưng cất. Kích thước lô là 1.

Chưng cấtBW=1 BW=2 BW=4 BW=16 BW=64
Tăng tốc Tokens/Step Tăng tốc Tokens/Step Tăng tốc Tokens/Step Tăng tốc Tokens/Step Tăng tốc Tokens/Step
K 1.47 2.21 1.52 2.31 1.54 2.48 1.80 2.87 1.99 3.30
C 1.54 2.35 1.60 2.50 1.72 2.73 1.92 3.09 2.18 3.58

Như đã thảo luận trong Phần 3.5, mục tiêu hiệu quả hơn để huấn luyện mô hình dự thảo là căn chỉnh với dự đoán LLM thông qua chưng cất kiến thức, thay vì đơn giản là khớp với token ground-truth. Để kiểm tra giả thuyết này, chúng tôi huấn luyện một mô hình dự thảo sử dụng bộ dữ liệu chưng cất và một mô hình khác sử dụng token ground-truth, cả hai dựa trên Vicuna 7B. Bộ dữ liệu chưng cất được tạo bằng cách cho LLM tạo ra 5 token tương lai tại mỗi vị trí của phản hồi ground-truth sử dụng temperature 0. Bảng 4 cho thấy chưng cất dẫn đến tăng khoảng 10% trong tăng tốc và tokens được chấp nhận trung bình mỗi bước. Điều này chứng minh rằng huấn luyện với chưng cất mang lại tăng cường hiệu suất hữu hình, cải thiện cả hiệu quả tạo sinh và độ chính xác dự đoán.

8

--- TRANG 9 ---
Đang được đánh giá
5 NGHIÊN CỨU TƯƠNG LAI
ReDrafter đặt nền móng để đẩy giới hạn tăng tốc của giải mã đốt đoán trên các phần cứng và triển khai khác nhau thông qua thiết kế mới lạ của nó, sử dụng mô hình dự thảo RNN cùng với các thuật toán huấn luyện và suy luận được thiết kế riêng. Mặc dù đạt được hiệu suất hiện đại nhất, chúng tôi xác định các lĩnh vực để cải thiện thêm, chẳng hạn như nâng cao huấn luyện mô hình dự thảo thông qua các kỹ thuật chưng cất tiên tiến hơn, và tối ưu hóa triển khai để đảm bảo lợi ích hiệu suất nhất quán và ít chi phí phụ hơn. Chúng tôi để dành những điều đó cho nghiên cứu tương lai.

Lời cảm ơn. Chúng tôi muốn cảm ơn Barry Theobald, Frank Chu, Liangliang Cao, Ruoming Pang, Sam Wiseman, Xiujun Li và Zhiyun Lu vì những bình luận và đề xuất của họ về bản thảo. Chúng tôi cảm ơn Tianle Cai, Yuhong Li vì đã chia sẻ chi tiết của Medusa với chúng tôi. Để làm cho ReDrafter sẵn sàng sản xuất cho phần cứng Nvidia, chúng tôi đã hợp tác với Nvidia để tích hợp ReDrafter vào khung tăng tốc suy luận Nvidia TensorRT-LLM.

TÀI LIỆU THAM KHẢO
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical
report. arXiv preprint arXiv:2303.08774 , 2023.

Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,
Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: A family of highly
capable multimodal models. arXiv preprint arXiv:2312.11805 , 1, 2023a.

Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,
Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.
arXiv preprint arXiv:2305.10403 , 2023b.

Nikhil Bhendawade, Irina Belousova, Qichen Fu, Henry Mason, Mohammad Rastegari, and Mah-
yar Najibi. Speculative streaming: Fast llm inference without auxiliary models. arXiv preprint
arXiv:2402.11131 , 2024.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.

Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee, Deming Chen, and Tri
Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv
preprint arXiv:2401.10774 , 2024.

Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John
Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint
arXiv:2302.01318 , 2023.

Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos
Guestrin, Percy S Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for
methods that learn from human feedback. Advances in Neural Information Processing Systems ,
36, 2024.

Zhifu Gao, Shiliang Zhang, Ian McLoughlin, and Zhijie Yan. Paraformer: Fast and accu-
rate parallel transformer for non-autoregressive end-to-end speech recognition. arXiv preprint
arXiv:2206.08317 , 2022.

Tom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen
Zhang, Chen Chen, Chung-Cheng Chiu, David Qiu, et al. Apple intelligence foundation language
models. arXiv preprint arXiv:2407.21075 , 2024.

Can Karakus, Rahul Huilgol, Fei Wu, Anirudh Subramanian, Cade Daniel, Derya Cavdar, Teng Xu,
Haohan Chen, Arash Rahnama, and Luis Quintela. Amazon sagemaker model parallelism: A
general and flexible framework for large model training. arXiv preprint arXiv:2111.05972 , 2021.

9

--- TRANG 10 ---
Đang được đánh giá
Yoon Kim and Alexander M Rush. Sequence-level knowledge distillation. arXiv preprint
arXiv:1606.07947 , 2016.

Hyunwoong Ko, Soohwan Kim, and Kyubyong Park. Oslo: Open source framework for large-scale
transformer optimization. https://github.com/tunib-ai/oslo , 2021.

Abhishek Vijaya Kumar, Gianni Antichi, and Rachee Singh. Responsive ml inference in multi-
tenanted environments using aqua. arXiv preprint arXiv:2407.21255 , 2024.

Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph
Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model
serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Prin-
ciples , pp. 611–626, 2023.

Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative
decoding. In International Conference on Machine Learning , pp. 19274–19286. PMLR, 2023.

Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling requires
rethinking feature uncertainty. arXiv preprint arXiv:2401.15077 , 2024a.

Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle-2: Faster inference of language
models with dynamic draft trees. arXiv preprint arXiv:2406.16858 , 2024b.

Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, and Hao Zhang.
Online speculative decoding. arXiv preprint arXiv:2310.07177 , 2023.

Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong,
Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating
generative llm serving with speculative inference and token tree verification. arXiv preprint
arXiv:2305.09781 , 2023.

Tomas Mikolov and Geoffrey Zweig. Context dependent recurrent neural network language model.
In2012 IEEE Spoken Language Technology Workshop (SLT) , pp. 234–239, 2012. doi: 10.1109/
SLT.2012.6424228.

Tomas Mikolov, Martin Karafi ´at, Luk ´as Burget, Jan Cernock ´y, and Sanjeev Khudanpur. Re-
current neural network based language model. In INTERSPEECH , pp. 1045–1048. ISCA,
2010. URL http://dblp.uni-trier.de/db/conf/interspeech/interspeech2010.
html#MikolovKBCK10 .

Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Ric-
cardo Marin, and Emanuele Rodol `a. Accelerating transformer inference for translation via paral-
lel decoding. arXiv preprint arXiv:2305.10427 , 2023.

Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan
Catanzaro. Megatron-lm: Training multi-billion parameter language models using model par-
allelism. arXiv preprint arXiv:1909.08053 , 2019.

Benjamin Spector and Chris Re. Accelerating llm inference with staged speculative decoding. arXiv
preprint arXiv:2308.04623 , 2023.

Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autore-
gressive models. Advances in Neural Information Processing Systems , 31, 2018.

Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, and Felix
Yu. Spectr: Fast speculative decoding via optimal transport. Advances in Neural Information
Processing Systems , 36, 2024.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee
Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.

Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu Wei, and Zhifang Sui. Speculative decod-
ing: Exploiting speculative execution for accelerating seq2seq generation. In Findings of the
Association for Computational Linguistics: EMNLP 2023 , pp. 3909–3925, 2023.

10

--- TRANG 11 ---
Đang được đánh giá
Siyan Zhao, Daniel Israel, Guy Van den Broeck, and Aditya Grover. Prepacking: A simple
method for fast prefilling and increased throughput in large language models. arXiv preprint
arXiv:2404.09529 , 2024.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and
chatbot arena. Advances in Neural Information Processing Systems , 36, 2024.

Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh,
Sanjiv Kumar, Jean-Franc ¸ois Kagy, and Rishabh Agarwal. Distillspec: Improving speculative
decoding via knowledge distillation. arXiv preprint arXiv:2310.08461 , 2023.

11

--- TRANG 12 ---
Đang được đánh giá
A PHỤ LỤC
A.1 TRIỂN KHAI ATTENTION CÂY ĐỘNG THÂN THIỆN VỚI GPU
Trong Danh sách 1, chúng tôi chứng minh triển khai thân thiện với GPU của hàm dedup_prefix từ Phần 3.3 với năm dòng code. Chúng tôi giả định kích thước lô bằng 1 trong ví dụ này để ngắn gọn. Mở rộng đến kích thước lô lớn hơn 1 là đơn giản bằng cách thêm trước một chiều kích thước lô bổ sung trong tensor.

Hàm xử lý tensor beam ban đầu, tạo ra tensor prefix_tree làm đầu ra. Trong tensor này, prefix_tree[i][j]=k chỉ ra rằng ứng cử viên tại chỉ số nhỏ nhất k chia sẻ tiền tố giống hệt beam[i][:j+1] . Ví dụ, prefix_tree[2][1]=0 có nghĩa là tiền tố được chia sẻ "sáng nhâm nhi " giữa beam[0] và beam[2] từ ví dụ trong Hình 4. Có thể nén các token nơi prefix_tree[i][j]<i .

Danh sách 1: Một ví dụ triển khai cho dedup prefix .
def dedup_prefix(beam):
"""Cho mỗi tiền tố trong mỗi ứng cử viên, tìm chỉ số ứng cử viên nhỏ nhất
chia sẻ cùng tiền tố.
Args:
- beam: (beam_width, beam_length) chùm đầu vào.
Returns:
- prefix_tree: [beam_width, beam_length] prefix_tree[i][j]=k
chỉ ra rằng các chuỗi ứng cử viên i và k chia sẻ cùng
tiền tố, hoặc nói cách khác, beam[i][:j+1]== beam[k][:j+1]
Examples:
beam = tensor([[91, 92, 93, 95],
[91, 92, 94, 96],
[91, 92, 93, 97]])
prefix_tree = tensor([[0, 0, 0, 0],
[0, 0, 1, 1],
[0, 0, 0, 2]]
"""
beam_length = beam.shape[1]
prefix_target = torch.arange(1, beam_length+1)
# Xây dựng ma trận boolean vuông matches.
# Nếu matches[i][j][k]==True, thì token thứ k của chuỗi thứ i
giống với token thứ k trong chuỗi thứ j.
# Vậy, i và j trong phạm vi [0,beam_width), k trong [0,beam_length).
matches = beam[:, :, None]==beam[:, None, :]
seq_matches = (torch.cumsum(matches, dim=2)
== prefix_target[None, None, :])
# Ứng cử viên trước đó với chỉ số nhỏ nhất chia sẻ cùng
tiền tố.
prefix_tree = torch.argmax(seq_matches, dim=2)
return prefix_tree

A.2 BENCHMARK RECURRENT DRAFTING , MỘT PHƯƠNG PHÁP GIẢI MÃ ĐỐT ĐOÁN NHANH ,TRONG
MLX
A.2.1 TĂNG TỐC TỐI ƯU VÀ TIỀM NĂNG GPU
Để có được cái nhìn chi tiết về hành vi của Recurrent Drafting trên chip Apple Silicon, chúng ta cần phân tích tăng tốc liên quan đến hai tham số thời gian suy luận chính: độ rộng chùm và độ dài chùm trong tìm kiếm chùm drafter.

Tham số độ rộng chùm đề cập đến số lượng chuỗi token dự thảo. Độ rộng chùm lớn hơn tăng khả năng chuỗi có tiền tố chấp nhận được dài nhất được bao gồm trong chùm. Điều này cho phép LLM chấp nhận nhiều token hơn trong mỗi bước giải mã, giảm tổng số bước giải mã cần thiết—hoặc tương đương, giảm số lần gọi LLM.

Tuy nhiên, chùm rộng hơn đòi hỏi nhiều FLOP hơn để LLM xác minh các token dự thảo. Mặc dù GPU mạnh mẽ có thể xử lý các FLOP này song song, giảm thiểu sự gia tăng thời gian wall, nó cũng có nghĩa là tiêu thụ năng lượng và chi phí tính toán cao hơn.

Tham số độ dài chùm đề cập đến số bước trong thuật toán tìm kiếm chùm, số lần gọi mô hình dự thảo RNN, hoặc tương đương, độ dài của các chuỗi token dự thảo trong chùm. Chùm dài hơn tăng độ dài tiền tố có thể được chấp nhận, giảm số bước giải mã hoặc gọi LLM. Tuy nhiên, độ dài này bị ràng buộc bởi độ dài chuỗi token được sử dụng trong quá trình huấn luyện mô hình dự thảo RNN.

Hình 7 minh họa tăng tốc liên quan đến độ rộng chùm và độ dài chùm trên M1 Max và M2 Ultra. Chúng ta có thể quan sát rằng tăng tốc tối ưu (được tô màu vàng) được đạt khi độ dài chùm gần với độ dài của các chuỗi huấn luyện được sử dụng cho mô hình dự thảo RNN. Điều này tối đa hóa sức mạnh dự đoán của RNN. Tuy nhiên, tăng tốc tối ưu không nhất thiết được đạt tại độ dài huấn luyện chính xác là 5; nó có thể xảy ra ở độ dài ngắn hơn một chút, chẳng hạn như 4, vì RNN có thể không luôn dự đoán chính xác token thứ 5.

Chúng tôi cũng quan sát rằng tăng tốc tối ưu được đạt với các chùm hẹp hơn—1 cho M1 Max và 3 cho M2 Ultra. Ngược lại, các thực nghiệm của chúng tôi trên GPU A100 và H100 sử dụng PyTorch cho thấy tăng tốc tối ưu được đạt với độ rộng chùm 50 hoặc hơn. Sự khác biệt này do khoảng cách hiệu suất vốn có giữa GPU cấp máy chủ và GPU di động, điều này được mong đợi. Nó cũng giải thích tại sao M2 Ultra có thể xử lý chùm tối ưu rộng hơn là 3, so với 1 của M1 Max, vì M2 Ultra được trang bị GPU mạnh mẽ hơn.

Hình 7: Tokens mỗi giây và Tăng tốc của ReDrafter với Vicuna 7B trên M1 Max và M2 Ultra.

A.2.2 ĐIỀU CHỈNH HIỆU SUẤT : MLX VÀ APPLE SILICON
Trước khi bắt đầu triển khai MLX, chúng tôi có kinh nghiệm với PyTorch với CUDA. Tuy nhiên, làm việc trên dự án MLX đầu tiên của chúng tôi tiết lộ rằng nhiều bài học chúng tôi học được từ lập trình CUDA không còn áp dụng cho MLX và Apple Silicon. Dưới đây, chúng tôi chia sẻ một số bài học quan trọng rút ra từ hành trình này.

Sử dụng Dtype gốc và Low-Bits
Mã benchmark của chúng tôi khám phá các yếu tố khác nhau, bao gồm các loại dữ liệu (dtype). Từ khám phá của chúng tôi, chạy autoregression và recurrent drafting trong float16 luôn nhanh hơn bfloat16 .
Chúng tôi cũng quan sát rằng cả float16 và bfloat16 đều vượt trội hơn float32 , vì chúng sử dụng ít băng thông truy cập bộ nhớ hơn. Tương tự, như được báo cáo bởi các nghiên cứu khác, lượng tử hóa 4-bit tăng tốc đáng kể các chương trình MLX so với float16 .

MLX thực hiện đánh giá lười biếng
Trong khi các chương trình PyTorch thực thi háo hức, các chương trình MLX chạy lười biếng. Trong MLX, mã Python có thể có vẻ như đang thực thi các phép toán tensor tuần tự, nhưng những phép toán này có thể không thực sự chạy cho đến khi kết quả cuối cùng được in. Việc thực thi lười biếng này mở rộng ra ngoài các phép toán số; ngay cả việc tải tham số mô hình từ hệ thống tệp vào tensor trong bộ nhớ cũng được thực hiện lười biếng. Để đảm bảo rằng một mô hình được tải đầy đủ trước khi đánh giá thuật toán suy luận, bạn phải gọi mlx.core.eval (model.parameters()) trước khi gọi mô hình.

Đừng phá vỡ JIT Compilation
Việc đánh giá lười biếng trong MLX cho phép nó âm thầm theo dấu các lời gọi phép toán tensor và biên dịch các dấu vết này just-in-time thành các kernel GPU Metal để cải thiện hiệu suất runtime. Mặc dù điều này cung cấp sự tiện lợi, nó cũng có thể ảnh hưởng đến cách chúng ta lập trình. Ví dụ, trong một so sánh thời gian thực thi giữa các hàm MLX và các đối tác PyTorch của chúng, chúng tôi nhận thấy rằng một hàm MLX tiêu thụ một phần không cân xứng lớn của tổng thời gian chạy, trong khi tương đương PyTorch của nó thì không. Chúng tôi phát hiện rằng hàm này bao gồm nhiều lời gọi đến array.item() . MLX phải biên dịch mã Python trước mỗi lời gọi này, khiến mã bị chia thành nhiều đoạn, làm chậm đáng kể thời gian thực thi.

Đo hiệu suất ở tất cả mức độ chi tiết
Ứng dụng Instruments, được bao gồm với Xcode, cung cấp hình ảnh hóa của các luồng CPU và các phép toán Metal GPU, tương tự như NVIDIA Nsight. Công cụ này giúp các nhà phát triển có hiểu biết chung về các nút thắt tiềm năng trong mã của họ. để nắm bắt các số liệu hiệu suất chi tiết, chúng tôi đã phát triển các tiện ích tùy chỉnh để đo và ghi lại thời gian thực thi của các hàm cá nhân.

14
