# 2406.14066.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/speculative/2406.14066.pdf
# Kích thước tệp: 1567184 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Tối ưu hóa Giải mã Suy đoán để Phục vụ Mô hình Ngôn ngữ Lớn
Sử dụng Goodput
Xiaoxuan Liu1Cade Daniel2Langxiang Hu3Woosuk Kwon1Zhuohan Li1Xiangxi Mo1
Alvin Cheung1Zhijie Deng4Ion Stoica1Hao Zhang3
1UC Berkeley2Anyscale Inc.3UCSD4SJTU
Tóm tắt
Giảm độ trễ suy luận của các mô hình ngôn ngữ lớn
(LLM) là rất quan trọng, và giải mã suy đoán (SD) nổi bật
như một trong những kỹ thuật hiệu quả nhất. Thay vì để
LLM tạo ra tất cả các token trực tiếp, giải mã suy đoán
sử dụng các proxy hiệu quả để dự đoán các đầu ra tiềm năng,
mà LLM sau đó xác minh mà không ảnh hưởng đến chất
lượng tạo ra. Tuy nhiên, triển khai SD trong các hệ thống
phục vụ LLM trực tuyến thực tế (với batching liên tục) không
luôn mang lại cải thiện - dưới tỷ lệ yêu cầu cao hơn hoặc độ
chính xác suy đoán thấp, nó có thể tăng độ trễ một cách
nghịch lý. Hơn nữa, không có độ dài suy đoán tốt nhất nào
hoạt động cho tất cả khối lượng công việc dưới các tải hệ
thống khác nhau. Dựa trên các quan sát này, chúng tôi phát
triển một khung làm việc động SmartSpec. SmartSpec động
xác định độ dài suy đoán tốt nhất cho mỗi yêu cầu
(từ 0, tức là không suy đoán, đến nhiều token) - do đó chi
phí thực thi suy đoán liên quan - dựa trên một số liệu mới
gọi là goodput, đặc trưng cho tải quan sát hiện tại của toàn
bộ hệ thống và độ chính xác suy đoán. Chúng tôi cho thấy
rằng SmartSpec luôn giảm độ trễ yêu cầu trung bình lên đến
3.2× so với các baseline giải mã không suy đoán trên các
kích thước khác nhau của mô hình đích, mô hình nháp, tỷ
lệ yêu cầu và bộ dữ liệu. Hơn nữa, SmartSpec có thể được
áp dụng cho các phong cách giải mã suy đoán khác nhau,
bao gồm các phương pháp truyền thống dựa trên mô hình
cũng như các phương pháp không sử dụng mô hình như tra
cứu prompt và giải mã kiểu cây.

1 Giới thiệu
Độ trễ là rất quan trọng khi triển khai Mô hình Ngôn ngữ
Lớn (LLM) cho các dịch vụ trực tuyến như công cụ tìm kiếm
[25,32], chatbot [30], và trợ lý ảo [27,38,39]. Tuy nhiên,
việc tạo ra LLM vốn chậm do tính chất tự hồi quy của nó,
trong đó mỗi token được tạo ra phụ thuộc vào tất cả các
token trước đó. Sự phụ thuộc dữ liệu tuần tự này hạn chế
các token chỉ có thể được tạo ra từng cái một, dẫn đến tốc
độ tạo ra chậm.

Giải mã suy đoán nhằm giải quyết vấn đề này bằng cách
sử dụng các proxy nhẹ, chẳng hạn như một mô hình nháp
nhỏ [18, 20, 24,26,46] hoặc các đầu mô hình bổ sung [4,21,22,43],
tạo ra nhiều token sau đó được xác minh bởi LLM chính/đích
song song. Giải mã Suy đoán có thể giảm độ trễ tạo ra chủ
yếu vì hai lý do. Thứ nhất, proxy hiệu quả chạy nhanh hơn
nhiều so với việc chạy một lần chuyển tiếp duy nhất trên
mô hình đích và do đó nó có thể tạo ra token nhanh hơn
nhiều. Hơn nữa, việc xác minh mô hình nháp được thực
hiện trong một lần chuyển tiếp duy nhất. Việc xác minh như
vậy chỉ chậm hơn một chút so với việc để LLM tạo ra một
token mới, nhưng nó cho phép LLM có thể tạo ra nhiều
token mới (nếu các token đoán được đúng), hoặc ít nhất
cho phép LLM tạo ra một token mới (nếu tất cả các token
đoán được đều sai).

Trong khi SD đã chứng minh tiềm năng trong việc tăng
tốc suy luận yêu cầu đơn (tức là kích thước batch = 1),
việc tích hợp nó vào các hệ thống phục vụ trực tuyến đặt
ra những thách thức đáng kể. Trong các ứng dụng thực tế,
các hệ thống gộp nhiều token để đạt được việc sử dụng GPU
cao và SD kém hiệu quả hơn hoặc thậm chí có thể tăng độ
trễ truy vấn như thể hiện trong Hình 1. Sự gia tăng này chủ
yếu do chi phí tính toán bổ sung của việc chạy các mô hình
proxy và xác minh các token cuối cùng không được chấp
nhận. Việc công việc tính toán bổ sung có chuyển thành
giảm độ trễ thực tế hay không phụ thuộc vào hai yếu tố
chính: độ chính xác suy đoán của mỗi yêu cầu và tải hiện
tại của hệ thống phục vụ.

--- TRANG 2 ---
Thứ hai, do sự không hoàn hảo vốn có trong độ chính xác
suy đoán, giải mã suy đoán không thể tránh khỏi việc tiêu
tốn một số tài nguyên tính toán cho các token cuối cùng
không được chấp nhận. Dưới tải hệ thống thấp, việc lãng
phí tính toán này có thể chấp nhận được. Tuy nhiên, dưới
các điều kiện tải hệ thống cao - trong đó hệ thống tiếp cận
khả năng tính toán của nó và xử lý các kích thước batch
lớn - tính khả dụng của tính toán dư thừa cho suy đoán bị
giảm đáng kể. Trong những tình huống như vậy, việc phân
bổ các tài nguyên tính toán hạn chế trực tiếp cho việc tạo
token bình thường thay vì tiếp tục suy đoán sẽ hiệu quả
hơn, do đó giảm thiểu rủi ro và tối đa hóa việc sử dụng
tính toán có sẵn.

Lý tưởng nhất, giải mã suy đoán nên được thực hiện theo
cách thích ứng và tự động, có thể so sánh với các kỹ thuật
streaming thích ứng được sử dụng trong các hệ thống phân
phối video. Trong các tình huống có ít người dùng, hệ thống
có thể đủ khả năng tham gia vào "giải mã suy đoán độ phân
giải cao", giống như streaming video độ phân giải cao, trong
đó nó sử dụng tài nguyên tính toán dồi dào để đưa ra dự
đoán rộng rãi hơn cho mỗi yêu cầu. Ngược lại, khi nhu cầu
người dùng tăng và tài nguyên hệ thống trở nên căng thẳng,
hệ thống chuyển sang "giải mã suy đoán độ phân giải thấp".
Chiến lược này, giống như việc giảm độ phân giải video
trong giờ cao điểm, bao gồm việc đưa ra ít dự đoán hơn
cho mỗi yêu cầu để bảo tồn tài nguyên trong khi duy trì
chức năng tổng thể của hệ thống.

Mặc dù được công nhận rộng rãi, giải mã suy đoán vẫn
chưa được tích hợp hiệu quả vào các hệ thống phục vụ cấp
sản xuất. Hầu hết nghiên cứu trước đây đã khám phá giải
mã suy đoán với kích thước batch bằng một [4,20]. Các
nghiên cứu gần đây đã mở rộng cuộc điều tra này sang các
kích thước batch lớn hơn, nhưng những kỹ thuật này đã
được thử nghiệm chủ yếu trên các LLM tương đối nhỏ [35]
hoặc trong các thiết lập ngoại tuyến [37].

Trong công trình này, chúng tôi tích hợp giải mã suy đoán
vào một hệ thống phục vụ cấp sản xuất vLLM [19], đánh
dấu việc tích hợp đầu tiên như vậy theo hiểu biết tốt nhất
của chúng tôi. Chúng tôi cũng khám phá sự đánh đổi giữa
chi phí suy đoán và độ chính xác giải mã dưới các tải hệ
thống khác nhau. Một đổi mới quan trọng trong hệ thống
của chúng tôi là việc giới thiệu một số liệu gọi là "goodput",
được định nghĩa là tỷ lệ token được tạo ra mỗi giây. Không
giống như throughput, goodput trong bối cảnh giải mã suy
đoán chỉ đo những token vừa được xác minh vừa được tạo
ra bởi mô hình đích. Điều này phản ánh một sự khác biệt
quan trọng - không phải tất cả token đầu ra đều đủ điều
kiện là token được tạo ra.

Goodput là một số liệu thiết yếu để xác định mức độ suy
đoán. Nó được tạo ra từ hai yếu tố: tỷ lệ chấp nhận token
và kích thước batch, với yếu tố sau chỉ ra tải hệ thống. Số
liệu này tuân thủ hai nguyên tắc cốt lõi. Thứ nhất, nó giới
hạn suy đoán dưới các tài nguyên tính toán hạn chế để tối
đa hóa hiệu quả hệ thống. Ví dụ, dưới tải hệ thống cực cao,
goodput sẽ tự động tắt suy đoán để tránh lãng phí bất kỳ
tài nguyên tính toán nào. Thứ hai, số liệu này tăng độ dài
đề xuất cho các yêu cầu dễ dự đoán, như được chỉ ra bởi
tỷ lệ chấp nhận token cao trong các bước tạo ra trước đó.
Bằng cách tận dụng khả năng dự đoán của các truy vấn
này, hệ thống có thể nâng cao hiệu suất tổng thể.

Tuy nhiên, chúng ta không thể đo goodput trực tiếp vì
quyết định cần được đưa ra trước khi biết goodput. Chúng
ta phải xác định độ dài đề xuất và những yêu cầu nào cần
chạy (kích thước batch) dựa trên ước tính goodput, vì hai
yếu tố này ảnh hưởng đến giá trị của nó. Để ước tính goodput,
chúng tôi dự đoán độ dài token được chấp nhận cho tất cả
các yêu cầu trong một bước tạo ra duy nhất bằng cách sử
dụng tỷ lệ chấp nhận token. Sau đó, một mô hình tuyến
tính đơn giản được sử dụng để ước tính thời gian thực thi
batch. Bằng cách kết hợp độ dài token được dự đoán và
thời gian thực thi, chúng ta có thể xấp xỉ goodput.

Tận dụng goodput, chúng tôi đã phát triển khung làm việc
giải mã suy đoán động SmartSpec. SmartSpec điều chỉnh
động độ dài suy đoán của mỗi yêu cầu - từ không suy đoán
(tức là không) đến nhiều token - dựa trên goodput ước tính,
điều chỉnh cường độ suy đoán để đảm bảo giảm nhất quán
(thay vì tăng) độ trễ yêu cầu. SmartSpec cũng phù hợp với
các phương pháp giải mã suy đoán khác nhau, bao gồm các
phương pháp dựa trên mô hình nháp và các kỹ thuật không
sử dụng mô hình như tra cứu prompt [33] và giải mã dựa
trên cây [4]. Phù hợp với các phương pháp SD đa dạng là
rất quan trọng vì các kỹ thuật khác nhau phù hợp với các
khối lượng công việc khác nhau. Ví dụ, giải mã tra cứu
prompt có lợi hơn cho các tác vụ tóm tắt, trong khi các
phương pháp dựa trên cây như Medusa hữu ích hơn cho
trò chuyện trực tuyến. Đối với tất cả các phương pháp SD
được đánh giá trên tất cả các bộ dữ liệu, SmartSpec đảm
bảo hiệu suất được cải thiện mà không có bất kỳ suy giảm
nào, đây là một tính năng quan trọng để làm cho SD hữu
ích trong hệ thống phục vụ trực tuyến sẵn sàng sản xuất.

Tóm lại, bài báo đóng góp những điều sau:
• Chúng tôi thực hiện nghiên cứu đầu tiên về giải mã suy
đoán trong một hệ thống phục vụ trực tuyến thực tế với
lập lịch batching liên tục (§3).
• Chúng tôi định nghĩa goodput cho giải mã suy đoán,
tính đến cả throughput hệ thống và độ chính xác suy
đoán (§4).
• Chúng tôi thiết kế và triển khai một khung làm việc lập
lịch giải mã suy đoán sử dụng goodput làm số liệu chính
để xác định độ dài đề xuất tối ưu cho các khối lượng
yêu cầu khác nhau (§5, §6). Đánh giá SmartSpec trên
năm mô hình qua các tác vụ khác nhau cho thấy SmartSpec
luôn giảm độ trễ dưới các tải hệ thống khác nhau, mang
lại giảm độ trễ lên đến 3.2× so với baseline giải mã không
suy đoán (§7).

2 Kiến thức nền
Cho một danh sách các token (x₁,...,xₙ), một mô hình ngôn
ngữ lớn (LLM) [1,3] được huấn luyện để dự đoán phân phối
xác suất có điều kiện cho token tiếp theo: P(xₙ₊₁|x₁,...,xₙ).
Khi được triển khai như một dịch vụ [19,31], LLM nhận
vào một danh sách

--- TRANG 3 ---
Mô hình Nháp x₁ x₂ x₃ x₄ Mô hình Đích x₁ x₂ x₃ x₄ Token Đề xuất Token Được chấp nhận Token Bonus Pool Yêu cầu R1 R3 R2 R4 x₄ x₂ x₁ x₃ Đề xuất tự hồi quy Chấm điểm trong một lần chuyển tiếp x₁ x₂ x₃ x₄
Chấp nhận Token Bộ lấy mẫu từ chối

Hình 2. Một bước tạo ra duy nhất khi kết hợp batching liên tục với giải mã suy đoán. Mô hình nháp chạy theo cách tự hồi quy. Các token đề xuất được gửi đến mô hình đích để chấm điểm trong một lần chuyển tiếp duy nhất. Một bước tạo ra duy nhất có thể tạo ra nhiều hơn một token cho mỗi yêu cầu.

token từ yêu cầu người dùng và tạo ra một chuỗi đầu ra (xₙ₊₁,...,xₙ₊ₜ). Quá trình tạo ra yêu cầu đánh giá tuần tự xác suất và lấy mẫu token tại mọi vị trí trong T lần.

Do sự phụ thuộc dữ liệu tuần tự, tính toán này thường gặp phải việc sử dụng thiết bị thấp khi chạy trên GPU, dẫn đến độ trễ suy luận cao và throughput phục vụ thấp [31]. Do đó, nhiều công trình trước đây đề xuất các thuật toán khác nhau để giảm độ trễ hoặc tăng throughput khi phục vụ LLM. Trong bài báo này, chúng tôi tập trung vào hai loại thuật toán tối ưu hóa, giải mã suy đoán và batching liên tục.

2.1 Giải mã Suy đoán
Mặc dù LLM chỉ có thể tạo ra token đầu ra tuần tự, khi đối mặt với một danh sách token đầu ra (xₙ₊₁,...,xₙ₊ₜ), LLM có thể đánh giá hiệu quả các xác suất cho mỗi token P(xₙ₊₁|x₁,...,xₙ),...,P(xₙ₊ₜ|x₁,...,xₙ₊ₜ₋₁) song song.

Giải mã suy đoán [5,20] sử dụng tính chất này để giảm độ trễ tạo ra của LLM.

Cụ thể, trong giải mã suy đoán, chúng ta biến LLM đích thành một bộ đánh giá. Tại mỗi bước, chúng ta sử dụng một mô hình nháp hiệu quả hơn khác để đề xuất một danh sách token ứng viên (yₙ₊₁,...,yₙ₊ₖ), trong đó k là số lượng ứng viên được đề xuất. Sau đó, chúng ta đưa k token này vào LLM đích để đánh giá các xác suất P(yₙ₊₁|x₁,...,xₙ),...,P(yₙ₊ₖ|x₁,...,xₙ,yₙ₊₁,...,yₙ₊ₖ₋₁) song song. Dựa trên các xác suất và phương pháp lấy mẫu, chúng ta sẽ chấp nhận một tập con token y₁,...,yₘ, trong đó m là số lượng token được chấp nhận.

Ví dụ, đối với lấy mẫu tham lam, chúng ta kiểm tra xem mỗi yₙ₊ᵢ có phải là token tối đa hóa phân phối xác suất P(·|x₁,...,xₙ,yₙ₊₁,...,yₙ₊ᵢ₋₁) hay không và chấp nhận m token đầu tiên y₁,...,yₘ thỏa mãn điều kiện. Lưu ý rằng đối với vị trí m+1, chúng ta có thể trực tiếp lấy mẫu yₘ₊₁ từ phân phối P(·|x₁,...,xₙ,yₙ₊₁,...,yₙ₊ₘ₋₁). Cuối cùng, chúng ta sẽ lấy m+1 token y₁,...,yₘ₊₁ làm đầu ra LLM cho bước này.

Giải mã suy đoán có hai tính chất cốt lõi: (1) Giải mã suy đoán không thay đổi hành vi của quá trình lấy mẫu LLM, và do đó tạo ra đầu ra chính xác giống như các thuật toán giải mã thông thường mà không mất độ chính xác. (2) Hiệu quả và tăng tốc hiệu quả của các thuật toán giải mã suy đoán phụ thuộc vào hai yếu tố: độ chính xác của mô hình nháp khớp với đầu ra của mô hình đích và hiệu quả của mô hình nháp.

Nhiều công trình trước đây tập trung vào cải thiện độ chính xác và hiệu quả của giải mã suy đoán và có thể được phân loại thành hai phần: (1) Giải mã suy đoán dựa trên LLM nháp, sử dụng một LLM nhỏ làm mô hình nháp để đề xuất token ứng viên [6,24,26,40,46]. (2) Giải mã suy đoán không sử dụng mô hình nháp, sử dụng một nhánh của mô hình đích hoặc sử dụng các nguồn khác (ví dụ, từ cơ sở dữ liệu bên ngoài) để tạo ra token ứng viên [4,10,21,22,33]. Trong công trình này, chúng tôi nghiên cứu hành vi của cả hai loại phương pháp giải mã suy đoán.

2.2 Batching Liên tục
Do sự phụ thuộc tuần tự, khi tạo ra đầu ra cho một đầu ra duy nhất, LLM sử dụng GPU rất kém. Để tăng việc sử dụng GPU, người ta có thể gộp nhiều yêu cầu trong một bước và xử lý chúng song song. Tuy nhiên, việc gộp các yêu cầu cho một LLM không đơn giản: Thứ nhất, các yêu cầu có thể đến vào những thời điểm khác nhau. Một chiến lược gộp ngây thơ sẽ làm cho các yêu cầu sớm hơn phải đợi các yêu cầu muộn hơn hoặc trì hoãn các yêu cầu đến cho đến khi các yêu cầu sớm hơn hoàn thành, dẫn đến độ trễ xếp hàng đáng kể. Thứ hai, các yêu cầu có thể có độ dài đầu vào và đầu ra rất khác nhau. Một kỹ thuật gộp đơn giản sẽ đệm các đầu vào và đầu ra của các yêu cầu để cân bằng độ dài của chúng, lãng phí tính toán và bộ nhớ GPU.

Batching liên tục [11,41] được đề xuất để giải quyết vấn đề này. Thay vì gộp ở cấp độ yêu cầu, batching liên tục gộp ở cấp độ bước. Đối với mỗi bước, các yêu cầu đã hoàn thành từ bước trước được loại bỏ khỏi batch, và các yêu cầu mới nhận được sẽ được thêm vào. Do đó, một yêu cầu mới có thể ngay lập tức bắt đầu được xử lý sau khi nó được nhận. Điều này dẫn đến kích thước batch lớn hơn tại mỗi bước, cải thiện việc sử dụng GPU và do đó throughput phục vụ. Hơn nữa, với các kernel GPU đặc biệt, batching liên tục có thể loại bỏ nhu cầu đệm các yêu cầu có độ dài khác nhau, điều này càng cải thiện throughput phục vụ. Kỹ thuật này đã được

--- TRANG 4 ---
tích hợp trong tất cả các engine suy luận LLM phổ biến, như vLLM [19] và TensorRT-LLM [29].

3 Giải mã Suy đoán với Batching Liên tục
Giải mã suy đoán thay đổi batching liên tục bằng cách cho phép mỗi bước tạo ra sản xuất nhiều thay vì một token duy nhất cho mỗi yêu cầu. Nó sử dụng một mô hình nháp để đề xuất một loạt token có thể có cho một yêu cầu tại mỗi bước tạo ra. Các token đề xuất này cho tất cả các yêu cầu sau đó được xử lý tập thể trong một batch bởi mô hình đích để xác minh.

Hình 4 minh họa ba giai đoạn của giải mã suy đoán: đề xuất, chấm điểm và chấp nhận. Trong đề xuất, mô hình nháp kiểm tra pool yêu cầu và tạo ra token theo cách tự hồi quy. Trong chấm điểm, tất cả các token đề xuất được đánh giá tập thể trong một lần chuyển tiếp duy nhất. Sau khi chấp nhận các token bằng lấy mẫu từ chối, mỗi yêu cầu có thể mang lại nhiều token trong một lần pass. Các token được tạo ra bao gồm những token được đề xuất bởi mô hình nháp và sau đó được chấp nhận bởi mô hình đích, cộng với một token bonus. Token bonus này hoặc sửa một dự đoán được đưa ra bởi mô hình nháp hoặc được tạo ra bởi mô hình đích khi nó chấp nhận tất cả các token đề xuất.

3.1 Độ trễ Giải mã Suy đoán Thông thường
Để hiểu tác động hiệu suất của giải mã suy đoán thông thường trong bối cảnh batching liên tục, chúng tôi tiến hành một phân tích được thể hiện trong Hình 1, thể hiện tăng tốc đạt được dưới các tỷ lệ yêu cầu khác nhau. Trong phân tích này, để giảm thiểu các biến số gây nhiễu, chúng tôi đặt tỷ lệ chấp nhận token (0.7) và chuẩn hóa độ dài đầu vào và đầu ra trên tất cả các yêu cầu (độ dài đầu vào = độ dài đầu ra = 128). Kết quả cho thấy rằng ở tỷ lệ yêu cầu thấp (cụ thể, tỷ lệ yêu cầu là 4), việc đề xuất 3 hoặc 5 token dẫn đến tăng tốc đáng kể nhất. Tuy nhiên, khi tỷ lệ yêu cầu tăng, lợi thế của việc đề xuất nhiều token giảm nhanh chóng: khi tỷ lệ yêu cầu vượt quá 12, việc đề xuất 5 token không mang lại cải thiện hiệu suất. Tương tự, ở tỷ lệ yêu cầu lớn hơn 16, việc đề xuất 3 token mang lại suy giảm hiệu suất.

Một số insight nổi lên từ thí nghiệm này. Thứ nhất, giải mã suy đoán không luôn dẫn đến hiệu suất được cải thiện; thực tế, nó có thể ảnh hưởng bất lợi đến hiệu suất ở tỷ lệ yêu cầu cao hơn. Thứ hai, độ dài tối ưu cho suy đoán thay đổi theo tỷ lệ yêu cầu. Ở tỷ lệ yêu cầu thấp hơn, suy đoán nhiều hơn là tốt hơn, nhưng ở tỷ lệ yêu cầu cao hơn, thậm chí việc suy đoán có thể không có ý nghĩa gì.

3.2 Phân tích Độ trễ
Để hiểu hiện tượng này, chúng ta có thể xấp xỉ độ trễ yêu cầu như:
độ trễ yêu cầu ~= độ trễ batch × bước tạo ra (1)

trong đó độ trễ batch đề cập đến thời gian cần thiết để xử lý một batch duy nhất và bước tạo ra là số lần lặp trung bình cần thiết cho mô hình đích để hoàn thành một yêu cầu. Để đơn giản, chúng tôi loại trừ độ trễ liên quan đến mô hình nháp, giả định độ trễ đồng nhất trên các bước tạo ra khác nhau, và chỉ tập trung vào độ trễ batch cho mỗi bước tạo ra phát sinh bởi mô hình đích. Hình 3a minh họa rằng việc đề xuất nhiều token hơn tại mỗi bước dẫn đến tích lũy lớn hơn các token trong cùng một batch và do đó độ trễ batch cao hơn. Mặt khác, như thể hiện trong Hình 3b, việc tạo ra nhiều token hơn trong một lần chuyển tiếp duy nhất của mô hình đích giảm số lượng bước tạo ra. Cho xấp xỉ được trình bày trong Phương trình 1, tính khả thi của việc đạt được tăng tốc được xác định bởi sự tương tác giữa hai yếu tố này.

3.3 Mức độ Chi tiết của Độ dài Đề xuất
Cuối cùng, batching liên tục cho phép lập lịch linh hoạt. Như thể hiện trong Hình 4, có ba mức độ chi tiết cho độ dài đề xuất: (1) Toàn cầu: Đây là cách đơn giản nhất để triển khai giải mã suy đoán với batching liên tục, trong đó độ dài đề xuất cho tất cả các yêu cầu trên tất cả các bước tạo ra là đồng nhất. Tuy nhiên, phương pháp này bỏ qua hành vi hệ thống; như đã đề cập trước đó, giải mã suy đoán có thể làm giảm hiệu suất. (2) Cấp độ Bước: Ở đây tất cả các yêu cầu trong cùng một batch có cùng độ dài đề xuất, mặc dù độ dài có thể thay đổi giữa các bước. Điều này cho phép độ dài đề xuất thích ứng với các tải hệ thống khác nhau. Ví dụ, khi số lượng yêu cầu cao, độ dài đề xuất cho một bước nhất định có thể được giảm để bảo tồn tài nguyên tính toán. (3) Cấp độ Yêu cầu: Đây là mức độ lập lịch chi tiết nhất, trong đó mỗi yêu cầu có thể có độ dài đề xuất riêng. Nó cho phép ưu tiên các yêu cầu 'dễ hơn' bằng cách đề xuất số lượng token cao hơn cho chúng, dựa trên giả định rằng có nhiều khả năng nhiều token sẽ được tạo ra trong một bước duy nhất cho các yêu cầu này.

Trong phần này, chúng tôi phân tích đặc điểm hiệu suất của giải mã suy đoán ngây thơ với batching liên tục trên các tỷ lệ yêu cầu khác nhau. Chúng tôi khám phá lý do cho suy giảm hiệu suất và nhấn mạnh khả năng triển khai lập lịch linh hoạt. Xác định độ dài đề xuất tối ưu để đạt được độ trễ tối thiểu trên các khối lượng công việc đa dạng dưới các khối lượng yêu cầu khác nhau là một thách thức đáng kể mà chúng tôi giải quyết trong cuộc thảo luận sau.

--- TRANG 5 ---
x₁ x₂ x₃ x₄ x'₁ x'₂ x'₃ x'₄ x₁ x₂ x₃ x₄ x'₁ x'₂ x'₃ x'₄ x₁ x₂ x₃ x₄ x'₁ x'₂ x'₃ x'₄ Bước 1 Bước 2 Bước 1 Bước 2 Bước 1 Bước 2 Độ dài Đề xuất Toàn cầu Độ dài Đề xuất Cấp độ Bước Độ dài Đề xuất Cấp độ Yêu cầu

Hình 4. Độ dài đề xuất linh hoạt.

4 Goodput của Giải mã Suy đoán
Bây giờ chúng tôi định nghĩa goodput của việc phục vụ LLM với giải mã suy đoán và chi tiết về cách nó kết nối với hiệu quả tổng thể của hệ thống. Sau đó chúng tôi mô tả cách goodput có thể được ước tính với các phương pháp dự đoán khác nhau về việc chấp nhận các token được suy đoán.

4.1 Định nghĩa Goodput
Chúng tôi định nghĩa throughput mỗi bước của việc phục vụ một yêu cầu sử dụng LLM là:
Throughput = Số lượng Token Đầu ra / Thời gian Thực thi (2)

Throughput đề cập đến tỷ lệ đầu ra của các token được tạo ra bởi mô hình mỗi đơn vị thời gian. Các hệ thống hiện tại như vLLM [19] và Orca [41] đều nhằm tối đa hóa throughput, vì làm như vậy sẽ nâng cao hiệu quả tổng thể của hệ thống.

Goodput trong giải mã suy đoán. Trong giải mã suy đoán, không phải tất cả các token được đầu ra bởi mô hình đích trong giai đoạn chấm điểm (Hình 4) đều được đảm bảo vượt qua cơ chế lấy mẫu từ chối. Do đó, những token này có thể không đại diện cho các token thực tế được tạo ra trong một bước duy nhất. Để giải quyết sự khác biệt này, chúng tôi định nghĩa goodput là:
Goodput = Số lượng Token Được tạo ra / Thời gian Thực thi (3)

Ở đây, goodput đề cập đến tỷ lệ mà các token được tạo ra, đo bằng token mỗi giây. Điều này bao gồm cả các token đề xuất sau đó được chấp nhận và các token bonus mà mô hình đích tạo ra trong quá trình xác minh.

Tham số của Goodput. Trong khi định nghĩa trên là tổng quát trên các thuật toán giải mã suy đoán khác nhau, chúng tôi tập trung vào ba tham số cấu hình có tác động đặc biệt trong bối cảnh lập lịch giải mã suy đoán:
1. Độ dài đề xuất: số lượng token được đề xuất bởi mô hình nháp trong mỗi bước.
2. Yêu cầu để chạy: yêu cầu nào để chạy trong mỗi bước.

4.2 Hiểu Goodput
Goodput, về cơ bản được định nghĩa là tỷ lệ lợi ích mong đợi trên chi phí, cung cấp những insight có giá trị về cách kích thước batch và độ dài đề xuất nên tương tác để tối ưu hóa hiệu suất hệ thống.

Để chứng minh trực giác đằng sau goodput, Hình 5 cho thấy các giá trị goodput trên các kích thước batch và độ dài đề xuất khác nhau, giả định tỷ lệ chấp nhận token đồng nhất.

Đề xuất nhiều hơn cho batch nhỏ. Trong Hình 5, đường màu đỏ chỉ ra độ dài đề xuất tối ưu cho mỗi kích thước batch. Đáng chú ý, các kích thước batch nhỏ yêu cầu đề xuất hơn 4 token mỗi yêu cầu để đạt được goodput tối đa. Khi kích thước batch tăng, độ dài đề xuất tối ưu giảm.

Đề xuất ít hơn cho batch lớn. Đối với các kích thước batch lớn, việc không suy đoán hoàn toàn có thể dẫn đến goodput cao hơn. Điều này xảy ra khi chi phí của các suy đoán không thành công tăng đáng kể với các kích thước batch lớn hơn, vượt qua các lợi ích tiềm năng.

Ưu tiên batching hơn suy đoán. Xem xét một tình huống trong đó việc chấp nhận token là độc lập, với mỗi token có xác suất chấp nhận 0.7. Trong trường hợp này, xác suất chấp nhận token đầu tiên là 0.7, trong khi

--- TRANG 6 ---
xác suất chấp nhận cả token đầu tiên và thứ hai là 0.7×0.7 = 0.49. Do đó, việc tăng kích thước batch có xu hướng tạo ra nhiều token hơn với cùng chi phí. Việc tăng gấp đôi kích thước batch dẫn đến gấp đôi số token được tạo ra, trong khi việc tăng gấp đôi độ dài đề xuất không nhất thiết mang lại sự gia tăng tỷ lệ trong đầu ra.

Tối ưu hóa goodput giảm độ trễ yêu cầu. Độ trễ yêu cầu bao gồm độ trễ xếp hàng của yêu cầu và tổng thời gian thực thi. Khi tỷ lệ yêu cầu thấp, việc cải thiện goodput hiệu quả giảm tổng thời gian thực thi bằng cách sử dụng giải mã suy đoán với độ dài đề xuất tối ưu. Mặt khác, ở tỷ lệ yêu cầu cao, việc tối ưu hóa goodput giúp giảm độ trễ xếp hàng bằng cách tăng khả năng của hệ thống xử lý yêu cầu thông qua các kích thước batch lớn và giải mã suy đoán vừa phải. Nhìn chung, việc điều chỉnh chiến lược kích thước batch và độ dài đề xuất dựa trên goodput cho phép quản lý hiệu quả cả tình huống nhu cầu cao và thấp.

4.3 Mô hình hóa Thời gian Thực thi Batch
Thời gian thực thi batch được định nghĩa là tổng thời gian cần thiết để hoàn thành một bước giải mã suy đoán, kết hợp cả thời gian thực thi mô hình nháp và đích. Điều này có thể được biểu diễn toán học như:
T_batch = T_draft + T_target (4)

Mô hình hóa T_draft và T_target. Đối với giải mã suy đoán không sử dụng mô hình nháp, chúng tôi gán một hệ số hằng số nhỏ T_draft = C. Đối với giải mã suy đoán dựa trên mô hình nháp, mô hình nháp hoạt động theo cách tự hồi quy và hỗ trợ batching liên tục. Do đó, tổng thời gian thực thi cho mô hình nháp là tổng của thời gian thực thi trên tất cả các bước nháp. Mỗi bước bao gồm một lần chuyển tiếp duy nhất của mô hình nháp. Cho sự biến đổi trong độ dài đề xuất trên các yêu cầu khác nhau, thời gian thực thi mỗi bước có thể khác nhau. Thời gian thực thi mô hình nháp là tổng của thời gian thực thi của mỗi lần chuyển tiếp:

T_draft = Σ(i=1 to s) T_fwd(M, N_context(s), N_batched(s)) (5)

Ở đây, s là số bước tự hồi quy. Cụ thể, s = max(s₁, s₂, ...sₙ), trong đó sᵢ là độ dài đề xuất của yêu cầu i trong batch. Thời gian thực thi chuyển tiếp, T_fwd, thay đổi trên các bước khác nhau do số lượng token ngữ cảnh khác nhau (N_context) và token được gộp (N_batched) tại mỗi bước. Nó cũng phụ thuộc vào mô hình M mà lần chuyển tiếp đang chạy trên đó. Những biến đổi này ảnh hưởng trực tiếp đến thời lượng của thời gian thực thi chuyển tiếp, như được nêu trong Mô hình hóa T_fwd bên dưới.

Mô hình đích thực thi một lần chuyển tiếp duy nhất cho hoạt động của nó:
T_target = T_fwd(N_context, N_batched) (6)

Mô hình hóa T_fwd. Sau đó chúng tôi định nghĩa T_fwd, thời gian cho một lần chuyển tiếp, áp dụng cho cả mô hình nháp và đích:
T_fwd(M, N_context, N_batched) = α_M · N_context + γ_M · N_batched + δ_M (7)

trong đó N_context đại diện cho số lượng token ngữ cảnh trong batch và N_batched biểu thị số lượng token được gộp, như minh họa trong Hình 7. Thuật ngữ α_M · N_context phản ánh thời gian cần thiết để tải bộ nhớ đệm key-value, tỷ lệ tuyến tính với số lượng token ngữ cảnh. Thuật ngữ γ_M · N_batched chiếm thời gian tính toán, trong khi δ_M đại diện cho thời gian tải các tham số mô hình. Các hệ số α_M, γ_M và δ_M phụ thuộc vào mô hình và cấu hình phần cứng, đòi hỏi các bộ riêng biệt (α_M, γ_M, δ_M) cho các mô hình hoặc môi trường phần cứng khác nhau. Trong thực tế, chúng tôi hệ thống hóa việc profiling thời gian thực thi batch cho mỗi kết hợp mô hình và phần cứng cụ thể, và sau đó điều chỉnh các giá trị của α_M, γ_M và δ_M để phản ánh chính xác các profile này.

Mô hình hóa N_batched. Nếu tại mỗi vị trí phương pháp đề xuất chỉ đề xuất một token (ứng viên top 1), số lượng token được gửi để xác minh chỉ đơn giản là tổng của độ dài đề xuất của mỗi yêu cầu. Đối với giải mã suy đoán kiểu cây top-k, giả định xác minh cây đầy đủ, giả định xác minh cây đầy đủ, có H đầu và chúng ta đề xuất kᵢ token cho đầu i, số lượng token được gộp là Σ(h=1 to H) Π(i=1 to h) kᵢ [4]. Hình 8 minh họa một ví dụ về số lượng token được gộp trong Medusa. Như thể hiện, một đặc điểm của giải mã suy đoán cây đầy đủ là chi phí cao của nó. Trong ví dụ, mặc dù tối đa bốn token có thể được chấp nhận (ba token đề xuất cộng với một token bonus), tổng cộng 27 token được gửi để xác minh. Điều này có thể có vấn đề đối với các kích thước batch lớn. Các phương pháp thông minh hơn để xây dựng cây, như những phương pháp được đề xuất bởi [6] và SmartSpec, cần thiết để làm cho ứng viên topk suy đoán có thể áp dụng trong hệ thống phục vụ thực tế.

Xác thực mô hình hiệu suất. Hình 7 minh họa việc áp dụng hàm T_fwd của chúng tôi, được thiết kế để ước tính thời gian thực thi batch. Điều này được chứng minh dưới điều kiện khối lượng công việc đồng nhất, trong đó mỗi yêu cầu duy trì kích thước đầu vào/đầu ra giống hệt nhau (độ dài đầu vào = độ dài đầu ra = 128). Hơn nữa, chúng tôi điều chỉnh tỷ lệ yêu cầu để đánh giá hiệu suất của hàm trên một phổ kích thước batch, với mỗi điểm dữ liệu đại diện cho một bước thực thi. Phân tích của chúng tôi bao gồm so sánh giữa các mô hình 7B và 70B tham số, sử dụng cài đặt song song tensor là 1 và 4, tương ứng. Nhìn chung, kết quả cho thấy rằng mô hình của chúng tôi nắm bắt chính xác các xu hướng hiện tại trong dữ liệu quan sát, thích ứng hiệu quả với các biến đổi trong tỷ lệ yêu cầu, kích thước mô hình và mức độ song song.

4.4 Mô hình hóa Độ dài Được tạo ra
Để dự đoán chính xác goodput, như được định nghĩa trong Phương trình 3, phương pháp luận của chúng tôi đòi hỏi việc mô hình hóa số lượng token được sản xuất trong mỗi bước tạo ra. Khả năng dự đoán chính xác độ dài của nội dung được tạo ra là rất quan trọng để giảm thiểu lãng phí tính toán và nâng cao hiệu quả lập lịch, vì các dự đoán ảnh hưởng trực tiếp đến việc xác định độ dài đề xuất cho việc tạo ra và các quyết định lập lịch tiếp theo. SmartSpec khám phá ba phương pháp để mô hình hóa độ dài được chấp nhận.

Độ dài được tạo ra ứng viên Top1. SmartSpec sử dụng phương pháp trung bình động để ước tính tỷ lệ chấp nhận token cho các cặp nháp và đích được chỉ định trên một bộ dữ liệu nhất định. Cụ thể, SmartSpec ghi lại tỷ lệ chấp nhận token từ các bước tạo ra trước đó. Để dự đoán tỷ lệ trong bước hiện tại, nó tính toán trung bình từ những tỷ lệ quá khứ này. Phương pháp trung bình động được sử dụng yêu cầu kích thước cửa sổ; tuy nhiên, chúng tôi thấy rằng hiệu suất tương đối không nhạy cảm với việc lựa chọn kích thước cửa sổ này. Phương pháp này giả định hành vi chấp nhận token đồng nhất trên các yêu cầu đa dạng. Độ dài chấp nhận được dự đoán bằng công thức được giới thiệu trong bài báo giải mã suy đoán gốc [20].

l(α, k) = (1 - α^(k+1)) / (1 - α) (8)

Trong bối cảnh này, l đại diện cho độ dài được tạo ra cho mỗi yêu cầu, bao gồm token bonus. Biến α biểu thị tỷ lệ chấp nhận token trung bình quan sát được trong bộ dữ liệu được hiệu chỉnh, trong khi k tương ứng với số lượng token được đề xuất. Sau đó chúng ta có thể viết ra tổng số token được tạo ra trong một batch duy nhất:

token được tạo ra = Σ(i trong R) (1 - α_i^(k_i+1)) / (1 - α_i) (9)

Ở đây, chúng ta có thể có mức độ chi tiết khác nhau của tỷ lệ chấp nhận token. (1) Tỷ lệ chấp nhận token toàn cầu: giả định rằng mỗi yêu cầu thể hiện hành vi chấp nhận giống hệt nhau. Do đó, các yêu cầu khác nhau trong cùng một batch chia sẻ cùng tỷ lệ chấp nhận token và độ dài đề xuất, k₁=k₂=....=kₙ, α₁=α₂=....=αₙ. (2) Tỷ lệ chấp nhận token cấp độ yêu cầu: các yêu cầu riêng lẻ thể hiện tỷ lệ chấp nhận (α) và độ dài đề xuất (k) khác biệt do các mức độ khó khăn khác nhau, đòi hỏi việc đề xuất số lượng token khác nhau cho mỗi yêu cầu.

Độ dài được tạo ra kiểu cây Topk. Trong giải mã suy đoán kiểu cây, chúng ta có thể có nhiều ứng viên cho cùng một vị trí. Trong Medusa, mỗi đầu chịu trách nhiệm đề xuất cho một vị trí. Hình 8 cho thấy một ví dụ, trong đó có ba ứng viên cho vị trí 1, hai ứng viên cho vị trí 2, và ba ứng viên cho vị trí 3.

Để ước tính độ dài được chấp nhận, chúng tôi đưa ra các giả định sau: 1. Tỷ lệ chấp nhận token cho mỗi token đề xuất tại đầu i được ký hiệu là αᵢ. Tất cả các token trong top-k chia sẻ cùng tỷ lệ chấp nhận token. 2. Hành vi chấp nhận độc lập trên các đầu khác nhau; tức là, việc chấp nhận một token tại đầu i không ảnh hưởng đến việc chấp nhận một token tại đầu i+1. Giả sử có h đầu, và chúng ta đề xuất k₁, k₂, ..., kₕ token cho các đầu 1, 2, ..., h, tương ứng. Tỷ lệ chấp nhận token cho các đầu này là α₁, α₂, ..., αₕ. Để đơn giản trong công thức dưới đây, chúng tôi định nghĩa αₕ₊₁=0. Độ dài chấp nhận mong đợi cho cấu trúc có thể được công thức hóa như:

l(α₁...αₕ, k₁...kₕ) = Σ(i=1..h) (i+1) × (1-αᵢ₊₁) × Π(j=1···i) [1-(1-αⱼ)^kⱼ] (10)

Ở đây, (1-αᵢ₊₁) × Π(j=1 to i) [1-(1-αⱼ)^kⱼ] đại diện cho xác suất chấp nhận (i+1) token, trong đó "+1" chiếm token bonus. Để hiểu xác suất này, nó phụ thuộc vào hai điều kiện: (1) Ít nhất một token được chấp nhận từ các đầu 1 đến i. (2) Không có token đề xuất nào tại đầu i+1 được chấp nhận. Do đó, xác suất được tính như tích của xác suất của điều kiện (1) và (2).

Vì SmartSpec là một khung làm việc giải mã suy đoán đa năng, người dùng có thể tích hợp các phương pháp ước tính khác nhau, như

--- TRANG 7 ---
Prompt: Tên bạn là gì? LLM Đầu 1 Đầu 2 Đầu 3 My Hi I is name are you is

Hình 8. Một ví dụ về giải mã suy đoán kiểu Medusa sử dụng ba đầu riêng biệt. Mỗi đầu được giao nhiệm vụ tạo ra đề xuất cho một vị trí. Trong tình huống này, đầu đầu tiên đề xuất ba token có khả năng nhất, đầu thứ hai chọn hai token hàng đầu, và đầu thứ ba cũng chọn ba token hàng đầu. Trong lần chạy chuyển tiếp này, ba token ['My', 'name', 'is'] được tạo ra - hai từ các đề xuất cộng với một token bonus. Tổng cộng, thiết lập này đại diện cho tổng cộng 18 (3x2x3) khả năng tiếp tục có thể.

phương pháp dựa trên độ tin cậy được thảo luận trong Phụ lục. Ngoài ra, người dùng thậm chí có thể sử dụng các mô hình machine learning để dự đoán. Chúng tôi để lại việc phát triển một bộ dự đoán chính xác và hiệu quả cho độ dài được chấp nhận như công việc tương lai.

5 Phục vụ Yêu cầu Sử dụng SmartSpec
Bây giờ chúng tôi mô tả luồng của một bước giải mã duy nhất được nêu trong Thuật toán 2. Ban đầu, chúng tôi liệt kê các yêu cầu tiềm năng cho một batch duy nhất (dòng 2). SmartSpec sử dụng chiến lược đến trước, phục vụ trước. Giả sử n yêu cầu trong batch đang chờ, chúng tôi xây dựng các batch ứng viên bằng cách chọn các tiền tố có độ dài tăng dần: batch với 1 yêu cầu, 2 yêu cầu, v.v., lên đến n yêu cầu. Đối với mỗi batch tiềm năng, chúng tôi sử dụng goodput để xác định độ dài đề xuất tối ưu (dòng 4). Ngoài ra, chúng tôi xác minh xem có đủ không gian trong bộ nhớ đệm KV hay không (dòng 5-7). Đối với giải mã suy đoán ứng viên Top-1, SmartSpec sẽ xác định độ dài đề xuất tối ưu. Đối với giải mã suy đoán kiểu cây Top-k, SmartSpec sẽ xác định giá trị Top-k tối ưu cho mỗi đầu đề xuất. Trong cả hai trường hợp, tỷ lệ chấp nhận token được tính vào Phương trình 8 để tính toán độ dài tạo ra ước tính. SmartSpec sau đó sử dụng độ dài ước tính này cùng với Phương trình 3 và một mô hình hiệu suất (được chi tiết trong Phần 4.3) để ước tính goodput. Sau khi xác định độ dài đề xuất tối ưu hoặc giá trị Top-k, SmartSpec thực thi các bước này tuần tự. Đối với giải mã suy đoán dựa trên mô hình nháp, giai đoạn đề xuất hoạt động theo cách tự hồi quy và kết hợp batching liên tục (dòng 12). Sau đó SmartSpec xác minh các token đề xuất và ghi lại tỷ lệ chấp nhận token của bước hiện tại (dòng 13).

Để ước tính tỷ lệ chấp nhận token hiện tại, SmartSpec ghi lại tỷ lệ chấp nhận từ các bước chấm điểm trước đó (dòng 13 trong Thuật toán 2) và tính toán trung bình động của các tỷ lệ này (dòng 5-8 trong Thuật toán 1). Mặc dù trung bình động là một bộ ước tính không hoàn hảo cho tỷ lệ chấp nhận token, goodput vẫn mạnh mẽ và dẫn đến giảm độ trễ sẽ được thảo luận trong Phần 7.2.1.

Vô hiệu hóa prefill. Trong giải mã suy đoán dựa trên mô hình nháp, giai đoạn prefill của mô hình nháp đang chạy có thể gây ra overhead, đặc biệt khi tỷ lệ yêu cầu cao. Theo mặc định, giải mã suy đoán bị vô hiệu hóa trong giai đoạn prefill. Tuy nhiên, để đồng bộ hóa bộ nhớ đệm KV giữa các mô hình nháp và đích, hệ thống vẫn thực thi giai đoạn prefill của mô hình nháp theo mặc định, ngay cả khi không có token nào sau đó được đề xuất bởi SmartSpec. Điều này có thể dẫn đến tiêu thụ lãng phí bộ nhớ và tài nguyên tính toán. Để giải quyết vấn đề này, chúng tôi sử dụng phương pháp dựa trên phản hồi tự động vô hiệu hóa việc chạy prefill của mô hình nháp. Đối với mỗi bước tạo ra, SmartSpec ghi lại độ dài đề xuất. Trong mỗi giai đoạn prefill, SmartSpec kiểm tra độ dài đề xuất từ các bước giải mã trước đó. Nếu tỷ lệ phần trăm lần không có token nào được đề xuất vượt quá ngưỡng xác định trước, SmartSpec sẽ vô hiệu hóa việc chạy prefill của mô hình nháp cho yêu cầu hiện tại và phân loại yêu cầu là không suy đoán. Vì bộ nhớ đệm KV của mô hình nháp không được duy trì cho những

Thuật toán 1 Ước tính Goodput cho Độ dài Đề xuất Cấp độ Bước
Yêu cầu: Độ dài đề xuất k cho tất cả các yêu cầu trong batch cho giải mã suy đoán ứng viên Top-1. Số lượng token được lấy mẫu k₁...kₕ cho mỗi đầu cho giải mã suy đoán kiểu cây Top-k. Phương pháp ước tính Method. Tỷ lệ chấp nhận token prev_alphas của các bước trước đó. Tất cả các yêu cầu trong batch R.
1: n ← len(R)
2: if Method == Giải mã suy đoán ứng viên Top-1 then
3:    α ← MovingAvg(prev_alphas)
4:    generated_len = n × (1-α^(k+1))/(1-α) × n
5:    batch_execution_time = T(R, [k])
6: else if Method == Giải mã suy đoán kiểu cây Top-k then
7:    α₁, α₂...αₕ ← MovingAvg(prev_alphas)
8:    generated_len = n × Σ(i=1..h) (i+1) × (1-αᵢ₊₁) × Π(j=1···i) [1-(1-αⱼ)^kⱼ]
9:    batch_execution_time = T(R, [k₁, k₂....kₕ])
10: end if
11: return generated_len / batch_execution_time

Thuật toán 2 Đề xuất và xác minh dựa trên tỷ lệ chấp nhận token SmartSpec.
Yêu cầu: Các yêu cầu đang chờ R. Độ dài đề xuất tối đa V. Tỷ lệ chấp nhận token của các bước giải mã trước đó prev_alphas.
1: best_goodput, best_proposed_lens ← -1, []
2: batch_candidates ← GetBatchCandidates()
3: for batch in batch_candidates do
4:    cur_goodput, cur_proposed_lens ← Argmax_{k₁,k₂...kₙ}(Goodput(k₁,k₂...kₙ, prev_alphas))
5:    if not HasSlot(batch) then
6:        continue.
7:    end if
8:    if cur_goodput > best_goodput then
9:        best_goodput, best_proposed_lens ← cur_goodput, cur_proposed_lens
10:   end if
11: end for
12: Propose(R, best_proposed_lens)
13: α_cur = Score(R, best_proposed_lens)
14: prev_alphas.append(α_cur)

yêu cầu này, giải mã suy đoán cũng bị vô hiệu hóa cho tất cả các bước giải mã tiếp theo cho những yêu cầu này. Ngưỡng để vô hiệu hóa việc chạy prefill có thể điều chỉnh, cho phép người dùng điều chỉnh mức độ giải mã suy đoán bảo thủ theo nhu cầu của họ. Theo kinh nghiệm, việc đặt ngưỡng này thành 0.7 đã mang lại hiệu suất tốt, cân bằng hiệu quả tài nguyên với hiệu quả giải mã.

Thảo luận và Phân tích Độ phức tạp. Đối với mỗi batch, overhead của SmartSpec bao gồm ba thành phần: ước tính độ dài được chấp nhận, mô hình hóa thời gian thực thi batch, và tối ưu hóa độ dài đề xuất hướng dẫn bởi goodput. Tính toán độ dài được chấp nhận và thời gian thực thi batch là O(1), vì chúng sử dụng tỷ lệ chấp nhận token dựa trên trung bình động và các hệ số mô hình được profiling ngoại tuyến, như chi tiết trong Phần 4.4 và 4.3.

Độ phức tạp của tối ưu hóa hướng dẫn goodput thay đổi tùy thuộc vào việc nó sử dụng tỷ lệ chấp nhận token cấp độ yêu cầu hay toàn cầu. Trong công trình này, chúng tôi thấy rằng cả hai mức độ chi tiết đều mang lại lợi ích hiệu suất tương tự. Tuy nhiên, do tỷ lệ chấp nhận token cấp độ yêu cầu gây ra độ phức tạp kỹ thuật đáng kể, chúng tôi chọn sử dụng tỷ lệ chấp nhận token toàn cầu để ước tính độ dài được chấp nhận. Vì độ dài đề xuất tối đa V thường nhỏ hơn 10, chúng ta có thể liệt kê hiệu quả tất cả các độ dài đề xuất có thể để tìm ra độ dài tối đa hóa goodput. Đây là một overhead rất nhỏ so với lần chuyển tiếp LLM: cho s là độ dài chuỗi, ℓ là số lớp decoder trong mô hình, h là kích thước chiều ẩn, n là kích thước batch, độ phức tạp của mỗi lần chuyển tiếp ở bậc O(ℓn(sh²+s²h)) ≫ O(nV).

6 Thiết kế Hệ thống và Kiến trúc
Chúng tôi triển khai SmartSpec trong vllm [19] và minh họa kiến trúc hệ thống trong Hình 9. Ban đầu, khi nhận được một yêu cầu, bộ lập lịch lookahead đảm nhận. Bộ lập lịch này được giao nhiệm vụ điều phối các yêu cầu để thực thi ngay lập tức và quản lý phân bổ tài nguyên trong bộ nhớ đệm key-value (KV). Nó được gọi là bộ lập lịch "lookahead" vì nó chủ động dành nhiều vị trí bộ nhớ đệm KV cho các token chưa được tạo ra. Sau đó, engine chuyển tiếp các yêu cầu hoạt động này đến worker giải mã suy đoán, lần lượt kích hoạt worker nháp. Worker nháp vận hành mô hình nháp thông qua nhiều bước, mỗi bước tạo ra một token đề xuất. Nó cung cấp giao diện chuẩn hóa hỗ trợ các phương pháp giải mã suy đoán khác nhau, như mô hình nháp nhỏ hoặc mô hình n-gram. Sau đó, worker đích sử dụng mô hình đích để chấm điểm và xác minh. Lưu ý ở đây cả mô hình nháp và đích đều tương tác với bộ nhớ đệm KV trong quá trình chuyển tiếp của chúng.

Để quản lý bộ nhớ hiệu quả trong giải mã suy đoán, việc duy trì bộ nhớ đệm key-value (KV) cho cả worker nháp và đích là thiết yếu. Trong các tình huống sử dụng giải mã suy đoán dựa trên mô hình nháp, chúng tôi chia tổng bộ nhớ được phân bổ cho bộ nhớ đệm KV thành hai phân đoạn riêng biệt: một dành riêng cho mô hình nháp và một cho mô hình đích. Các quan sát của chúng tôi đã luôn cho thấy rằng số lượng slot cần thiết cho cả mô hình nháp và đích vẫn không đổi cả trước và sau bước thực thi. Do đó, chúng tôi phân bổ bộ nhớ đệm KV để cung cấp số lượng slot bằng nhau cho mỗi mô hình, với bộ lập lịch gán cùng số lượng slot cho cả hai mô hình. Phương pháp này không chỉ đơn giản hóa kiến trúc của hệ thống mà còn giảm độ phức tạp của nó. Mặt khác, khi thực hiện giải mã suy đoán không sử dụng mô hình nháp, SmartSpec duy trì bộ nhớ đệm KV như không có giải mã suy đoán.

Để điều chỉnh động độ dài đề xuất, chúng tôi sử dụng bộ adaptor trực tuyến kết hợp với profiler ngoại tuyến. Profiler ngoại tuyến chịu trách nhiệm phân tích cả mô hình nháp (nếu có) và đích để rút ra các hệ số hiệu suất quan trọng cho mô hình hiệu suất, như chi tiết trong Phần 4.3. Những hệ số này sau đó được sử dụng bởi bộ adaptor trực tuyến, tổng hợp thông tin batch. Dựa trên dữ liệu này, bộ adaptor trực tuyến điều chỉnh độ dài đề xuất cho worker nháp và độ dài xác minh cho mô hình đích.

7 Đánh giá
Cấu hình mô hình và máy chủ. Chúng tôi thử nghiệm trên hai họ mô hình mã nguồn mở phổ biến: Llama và Mistral. Đối với Llama, chúng tôi sử dụng Vicuna-7B-v1.5, Vicuna-33B-v1.3 [44], và Llama-2-70b-chat-hf [36]. Đối với Mistral, chúng tôi sử dụng Mistral-7B-Instruct-v0.1 [13] và Mixtral-8x7B-Instruct-v0.1 [14]. Chúng tôi sử dụng một GPU A100-80G [28] duy nhất cho mô hình 7B, 4 × A100-80G GPU cho mô hình 33B, và 8 × A100-80G GPU cho mô hình 70B. Đối với phương pháp dựa trên mô hình nháp, mô hình nháp hoạt động với song song tensor đặt thành 1, và chỉ mô hình đích được chia nhỏ trên nhiều GPU. Chúng tôi cung cấp thông số kỹ thuật chi tiết của thiết lập trong Bảng 1.

Các phương pháp và baseline được đánh giá. Chúng tôi kiểm tra hiệu quả của SmartSpec trên cả giải mã suy đoán tiêu chuẩn, trong đó một mô hình nháp được sử dụng để đưa ra đề xuất, và giải mã tra cứu prompt, trong đó đề xuất được thực hiện bằng cách tra cứu ngram trong prompt. Cả hai cơ chế đều được chi tiết trong Phần 2.1. Đối với giải mã suy đoán tiêu chuẩn, chúng tôi fine-tune Llama-160M trên bộ dữ liệu shareGPT để cải thiện khả năng đề xuất chung và sử dụng nó làm mô hình nháp cho Vicuna-7B. Đối với mô hình Llama2-70B, chúng tôi sử dụng Vicuna-7B làm mô hình nháp. Đối với tất cả các thiết lập, mô hình nháp chia sẻ cùng mức độ song song tensor với mô hình đích.

Chúng tôi so sánh SmartSpec với hai baseline: suy luận tự hồi quy vanilla, không kết hợp giải mã suy đoán, và sử dụng giải mã suy đoán với độ dài đề xuất cố định là 1, 3, và 5 trên tất cả các bước thực thi.

Khối lượng công việc. Trong nghiên cứu của chúng tôi, chúng tôi tập trung vào bốn loại khối lượng công việc, trò chuyện trực tuyến, text-to-SQL, tóm tắt, và trả lời câu hỏi có ngữ cảnh. Đối với trò chuyện trực tuyến, chúng tôi sử dụng các bộ dữ liệu từ ShareGPT [2], Chatbot Arena [44]. Đối với text-to-SQL, chúng tôi sử dụng bộ dữ liệu spider [42]. Đối với tóm tắt và

--- TRANG 8 ---
--- TRANG 9 ---
Nhiệm vụ	Bộ dữ liệu	Phương pháp SD	Mô hình Nháp (TP)	Mô hình Đích (TP)	Phần cứng (Tổng Bộ nhớ)
Trò chuyện Trực tuyến	Arena[44]	VSD[20]	Llama-160M(1)	Vicuna-7B(1)	A100 (80G)
		Llama-160M(1)	Vicuna-33B(4)	4×A100 (320G)
		TinyLlama-1.1B (1)	Llama2-70B (8)	8×A100 (640G)
	ShareGPT[2]	Llama-160M(1)	Vicuna-7B(1)	A100 (80G)
		Llama-160M(1)	Vicuna-33B(4)	4×A100 (320G)
		TinyLlama-1.1B (1)	Llama2-70B (8)	8×A100 (640G)
Text-to-SQL	Spider[42]	Llama-160M(1)	Vicuna-7B(1)	A100 (80G)
		Llama-160M(1)	Vicuna-33B(4)	4×A100 (320G)
Tóm tắt	CNN/Daily Mail[12, 34]	PTL[33]	Không có	Mistral-7B (1)	A100 (80G)
			Mixture-8×7B (8)	8×A100 (640G)
Trả lời câu hỏi Ngữ cảnh	HAGRID[16]		Mistral-7B (1)	A100 (80G)
			Mixture-8×7B(8)	8×A100 (640G)

Bảng 1. Cấu hình bộ dữ liệu, mô hình và máy chủ. VSD: Giải mã suy đoán vanilla. PTL: Giải mã tra cứu prompt.

trả lời câu hỏi, chúng tôi sử dụng bộ dữ liệu gốc CNN/Daily Mail [34] và HAGRID [16] từ công trình giải mã tra cứu prompt. Chúng tôi thể hiện đặc điểm khối lượng công việc (độ dài đầu vào trung bình và độ dài đầu ra trung bình) trong Hình 10. Đối với trò chuyện trực tuyến, đầu vào có độ dài trung bình trong khi độ dài đầu ra dài hơn và cho thấy phương sai cao hơn. Đối với tóm tắt và trả lời câu hỏi, độ dài đầu vào dài và đầu ra ngắn. Đối với trả lời câu hỏi, vì chúng tôi không có ground truth trong bộ dữ liệu, chúng tôi đặt độ dài đầu ra cố định là 128. Đối với tất cả khối lượng công việc, chúng tôi tạo ra thời gian đến yêu cầu sử dụng phân phối Poisson với các tỷ lệ yêu cầu khác nhau. Chúng tôi sử dụng lấy mẫu tham lam cho tất cả các yêu cầu được phục vụ.

7.1 Đo lường Độ trễ
Trước tiên chúng tôi đánh giá hiệu quả của SmartSpec trong việc giảm độ trễ yêu cầu cho cả giải mã suy đoán dựa trên mô hình nháp và không sử dụng mô hình nháp.

7.1.1 Giải mã suy đoán dựa trên mô hình nháp. Chúng tôi đánh giá độ trễ yêu cầu trung bình trên các bộ dữ liệu khác nhau, tập trung vào hiệu suất của SmartSpec so với các chiến lược baseline trong Hình 11 và 12. Trong môi trường có tỷ lệ yêu cầu thấp, SmartSpec thể hiện hiệu suất tương tự như việc tham lam dự đoán số lượng token cao hơn (ví dụ, k=3 hoặc 5) để tối đa hóa tăng tốc. Sự tương tự này cho thấy rằng SmartSpec dự đoán đủ token dưới tải nhẹ để hiệu quả giảm độ trễ yêu cầu thông qua giải mã suy đoán. Tuy nhiên, trong một vài trường hợp khi tỷ lệ yêu cầu thấp, có sự khác biệt hiệu suất nhỏ giữa SmartSpec và SD tiêu chuẩn. Trong những trường hợp đó, SD tiêu chuẩn, dự đoán số lượng token cao hơn, vượt trội hơn SmartSpec một chút với tăng tốc rõ rệt hơn. Sự khác biệt này có thể được quy cho bộ dự đoán độ dài chấp nhận không hoàn hảo của chúng tôi, dẫn đến SD tiêu chuẩn với độ dài đề xuất dài hơn hoạt động tốt hơn vì nhiều token có thể được chấp nhận.

Ngược lại, trong các tình huống có tỷ lệ yêu cầu cao, hiệu suất của hệ thống phản ánh các tình huống trong đó không có token nào được dự đoán, hiệu quả chỉ ra rằng giải mã suy đoán bị vô hiệu hóa dưới điều kiện khối lượng yêu cầu cao. Điều quan trọng cần lưu ý là trong chế độ này, độ trễ liên quan đến việc sử dụng SD tiêu chuẩn và đề xuất số lượng token cao leo thang nhanh chóng, dẫn đến suy giảm hiệu suất đáng kể. Điều này được minh họa trong Hình 11 (c) khi tỷ lệ yêu cầu ở 32 và (f) khi tỷ lệ yêu cầu vượt quá 5.

Trong các trường hợp như Hình 11 (d) và (e), tăng tốc tương đối cho các baseline này phục hồi sau khi ban đầu giảm khi tỷ lệ yêu cầu vượt quá 2. Sự phục hồi này xảy ra vì, khi tỷ lệ yêu cầu tiếp tục tăng, nó đạt đến điểm mà ngay cả giải mã baseline không có suy đoán cũng bắt đầu gặp phải độ trễ xếp hàng. Hiện tượng này tương đối cải thiện tăng tốc của các baseline SD tiêu chuẩn, mặc dù chúng trải qua độ trễ tổng thể cao hơn.

Nói chung, giải mã suy đoán hiệu quả nhất khi hệ thống có đủ tài nguyên tính toán: mô hình càng lớn, vùng tỷ lệ yêu cầu càng nhỏ mà chúng ta thấy tăng tốc từ suy đoán. Điều này được mong đợi vì giải mã suy đoán yêu cầu thêm sức mạnh tính toán; khi mô hình lớn và tài nguyên tính toán không mở rộng tỷ lệ, hệ thống có khả năng bị ràng buộc tính toán. Điều này nhấn mạnh tầm quan trọng của SmartSpec. Việc SmartSpec luôn khớp hoặc vượt trội so với baseline đã thiết lập trên các tình huống khác nhau là rất quan trọng. Đặc điểm này quan trọng để triển khai các kỹ thuật giải mã suy đoán trong môi trường sản xuất thực tế, vì nó đảm bảo rằng việc áp dụng

--- TRANG 10 ---
DSD K=1 K=3 K=5

giải mã suy đoán sẽ không làm ảnh hưởng đến hiệu suất hệ thống.

7.1.2 Giải mã suy đoán không sử dụng mô hình nháp. Tiếp theo chúng tôi đánh giá hiệu quả của SmartSpec với giải mã tra cứu prompt. Giống như các thử nghiệm của chúng tôi với giải mã suy đoán dựa trên mô hình nháp, chúng tôi so sánh SmartSpec sử dụng độ dài đề xuất cố định là 1, 3, và 5, với baseline không có giải mã suy đoán. Như thể hiện trong Hình 12, SmartSpec luôn đạt được tăng tốc tốt nhất. Đáng chú ý, giải mã tra cứu prompt với Mistral-7B (Hình 12 (a) và (b)) cho thấy tăng tốc đáng kể ngay cả với tỷ lệ chấp nhận token tương đối thấp (tỷ lệ chấp nhận token đo được trên hai thiết lập đó nằm giữa 0.3 và 0.4). Không giống như các tình huống liên quan đến mô hình nháp, tra cứu prompt không gây ra overhead đáng kể cho việc đề xuất token, dẫn đến lợi ích tốc độ đáng chú ý ngay cả với độ chính xác suy đoán thấp hơn.

Giống như giải mã suy đoán dựa trên mô hình nháp, SmartSpec không làm ảnh hưởng đến hiệu suất ngay cả khi tỷ lệ yêu cầu cao. Điều này là do SmartSpec áp dụng phương pháp bảo thủ dưới tỷ lệ yêu cầu cao, giảm thiểu tính toán lãng phí của việc xác minh các token không chính xác. Chiến lược này đảm bảo rằng SmartSpec duy trì hiệu quả của nó trên các tải hệ thống khác nhau.

7.2 Thí nghiệm Mô phỏng
Chúng tôi tiến hành các thí nghiệm sau sử dụng một trình mô phỏng vì một số lý do. Thứ nhất, việc tích hợp hiệu quả giải mã suy đoán vào hệ thống thực tế đặt ra thách thức kỹ thuật đáng kể. Ví dụ, việc thiết kế một kernel xác minh hiệu quả cho giải mã suy đoán có cấu trúc cây tỏ ra khó khăn. Việc thiếu kernel như vậy làm mất đi lợi thế của giải mã suy đoán. Ngoài ra, chúng tôi nhằm khám phá cách hệ thống hoạt động dưới các mô hình, khối lượng công việc và cấu hình tài nguyên khác nhau. Đặc biệt, chúng tôi quan tâm đến việc đánh giá cách độ chính xác của dự đoán chấp nhận token ảnh hưởng đến hiệu suất tổng thể. Hơn nữa, việc thực hiện các thí nghiệm toàn diện trên tất cả các cấu hình có thể vừa tốn thời gian vừa tốn kém. Do đó, chúng tôi sử dụng trình mô phỏng cho tất cả các thí nghiệm tiếp theo.

Xây dựng trình mô phỏng. Thiết kế của trình mô phỏng được căn chỉnh chặt chẽ với luồng hoạt động của vLLM [19], sao chép chính xác logic lập lịch và điều khiển mô hình của nó. Sự khác biệt chính giữa trình mô phỏng và phần cứng thực tế nằm ở việc sử dụng đồng hồ sự kiện để mô phỏng thời gian thực thi kernel, cho phép nó hoạt động hiệu quả trên CPU mà không cần phần cứng GPU thực tế. Về cơ bản, trình mô phỏng sử dụng đồng hồ sự kiện để thay thế tất cả thời gian thực thi kernel trong khi bảo tồn tất cả logic hoạt động khác. Để đảm bảo đồng hồ sự kiện tiến triển chính xác và phản ánh thời gian thực thi thực tế, chúng tôi đã profiling thời gian thực thi GPU trên các cấu hình phần cứng và thiết lập mô hình khác nhau được sử dụng trong các thí nghiệm. Phương pháp này cho phép chúng tôi mô phỏng các hoạt động thực tế với độ trung thực cao.

Độ trung thực của trình mô phỏng. Dữ liệu chúng tôi đã thu thập cho mỗi công việc cho phép trình mô phỏng của chúng tôi mô hình hóa chính xác một số hiệu ứng hệ thống. Điều này bao gồm tác động hiệu suất của các chính sách lập lịch khác nhau và overhead hệ thống như lấy mẫu chậm và overhead Python, được xác định thông qua profiling. Tuy nhiên,

trình mô phỏng của chúng tôi không tính đến độ trễ mạng. Sau khi hiệu chỉnh, như thể hiện trong Hình 13, trình mô phỏng cho thấy tỷ lệ lỗi dưới 10% khi tỷ lệ yêu cầu thấp hơn tỷ lệ dịch vụ. Độ chính xác này nhất quán trên các độ dài đầu vào/đầu ra và tỷ lệ yêu cầu khác nhau. Điều quan trọng cần lưu ý là trình mô phỏng có xu hướng dự đoán thấp độ trễ khi tỷ lệ yêu cầu vượt quá tỷ lệ dịch vụ do khả năng hạn chế trong việc mô phỏng độ trễ xếp hàng. Đối với các thí nghiệm tiếp theo được trình bày trong bài báo này, chúng tôi sẽ tập trung độc quyền vào các tình huống trong đó tỷ lệ yêu cầu nhỏ hơn tỷ lệ dịch vụ.

Sử dụng trình mô phỏng, chúng tôi ban đầu xác định sự khác biệt giữa tăng tốc hiện tại và tăng tốc tối ưu, trong đó "tối ưu" ngụ ý biết trước về độ dài được chấp nhận. Sau đó chúng tôi triển khai giải mã suy đoán kiểu cây Medusa với batching liên tục để kiểm tra tính tổng quát của SmartSpec.

7.2.1 Dự đoán Độ dài Được chấp nhận và Tăng tốc Trong phần này, chúng tôi khám phá cách độ chính xác của mô hình hóa chấp nhận ảnh hưởng đến tăng tốc tính toán. Chúng tôi giữ tỷ lệ yêu cầu không đổi để đảm bảo so sánh có kiểm soát các kỹ thuật dự đoán chấp nhận khác nhau. Chúng tôi đánh giá hiệu quả của dự đoán độ dài được chấp nhận của SmartSpec, sử dụng trung bình động dựa trên tỷ lệ chấp nhận token lịch sử, và so sánh nó với một oracle. Oracle này giả định truy cập vào bộ dự đoán hoàn hảo và sử dụng độ dài được chấp nhận thực tế để tính toán goodput và xác định độ dài đề xuất tối ưu.

Như thể hiện trong Hình 14, có khoảng cách hiệu suất đáng chú ý giữa tăng tốc của SmartSpec và của oracle. Việc phát triển bộ dự đoán độ dài được chấp nhận hiệu quả hơn vẫn là lĩnh vực cho nghiên cứu tương lai. Tuy nhiên, điều quan trọng cần lưu ý là, ngay cả với phương pháp trung bình động, tăng tốc đạt được bởi SmartSpec là đáng kể và đại diện cho sự cải thiện lớn so với các chiến lược dựa vào đề xuất ngẫu nhiên.

7.2.2 Giải mã Suy đoán Kiểu Cây Trong phần này, chúng tôi đánh giá khả năng áp dụng của SmartSpec cho Medusa [4], một phương pháp giải mã suy đoán kiểu cây. Trước khi tích hợp SmartSpec, Medusa chỉ có thể được triển khai với kích thước batch bằng 1. Để kiểm tra khả năng nâng cao, chúng tôi mô phỏng Medusa với batching liên tục và đánh giá hiệu suất của nó cả có và không có tích hợp SmartSpec. Đối với các thí nghiệm của chúng tôi, chúng tôi duy trì tỷ lệ chấp nhận token nhất quán trên tất cả các đầu Medusa và cho tất cả các token trong lựa chọn top-k. Ngoài ra, chúng tôi mô hình hóa Medusa với kết nối dày đặc, đảm bảo rằng mỗi nút top-k được liên kết với các token top-k tương ứng tại vị trí tiếp theo.

Chúng tôi minh họa độ trễ yêu cầu trung bình trên các giá trị k khác nhau dưới tỷ lệ chấp nhận token khác nhau trong Hình 16. Như được chứng minh trong hình, phương pháp giải mã suy đoán kiểu cây tăng đáng kể độ trễ yêu cầu ở tỷ lệ yêu cầu cao. Kết quả này phù hợp với kỳ vọng được nêu trong [4], mô tả cách kết nối dày đặc giữa các đầu khác nhau làm tăng rất nhiều số lượng token được gộp. Như thể hiện trong Hình 16a và 16b, Medusa top2/top3 cố định nhanh chóng làm nổ kích thước batch. Cụ thể, số lượng token được gộp mỗi yêu cầu được biểu diễn bởi Σ(h=1 to H) Π(i=1 to h) sᵢ, trong đó sᵢ biểu thị số lượng token được lấy mẫu bởi đầu i. Ví dụ, việc chọn 3, 2, và 2 token cho các đầu 1, 2, và 3, tương ứng, dẫn đến việc thêm 21 token trong batch tiếp theo để xác minh (tính như 3+3×2+3×2×2).

Ngược lại, với chỉ ba đầu tương ứng với ba vị trí, đối với mỗi yêu cầu, tối đa 4 token (3 cộng với 1 token bonus) có thể được xử lý trong một lần chuyển tiếp duy nhất. Sự không hiệu quả này nhấn mạnh nhu cầu cho các tiến bộ tương lai như những tiến bộ được đề xuất bởi sequoia [6], nhằm phát triển cây xác minh có cấu trúc hơn để hiệu quả cắt tỉa các ứng viên ít có khả năng dưới khối lượng yêu cầu cao.

Cuối cùng, chúng tôi cho thấy hiệu suất của SmartSpec khi được tích hợp với Medusa. Chúng tôi mô hình hóa độ dài token được chấp nhận như được nêu trong Phần 4.4 và đánh giá hiệu suất bằng cách sử dụng goodput để quyết định số lượng token được lấy mẫu mỗi đầu. Như minh họa trong Hình 16, SmartSpec hiệu quả duy trì độ trễ yêu cầu có thể quản lý được ngay cả dưới tỷ lệ yêu cầu cao. Ngoài ra, chúng tôi mô tả số lượng token trung bình mỗi yêu cầu trong Hình 16c. Trong cả hai tình huống (α là 0.6 hoặc 0.8), SmartSpec nhanh chóng quay về lấy mẫu top-1. Đáng chú ý rằng số lượng token được gộp trung bình xấp xỉ bốn; điều này bao gồm một token đầu vào cộng với một token được lấy mẫu mỗi đầu. Cho rằng có ba đầu Medusa, tổng số token được gộp mỗi yêu cầu vẫn là bốn khi sử dụng lấy mẫu top-1 cho mỗi đầu.

8 Công trình Liên quan
Ngoài giải mã suy đoán (Phần 2.1) và batching liên tục (Phần 2.2), cộng đồng hệ thống đã đề xuất nhiều phương pháp trực giao để cải thiện hiệu suất suy luận LLM.

Phương pháp Lượng tử hóa. Các công trình như (LLM.int8() [7], GPTQ [9], Marlin [8], AWQ [23], SqueezeLLM [17]) giảm độ trễ của suy luận LLM bằng cách sử dụng các kiểu dữ liệu độ chính xác thấp hơn như số nguyên 2/3/4/6/8 bit. GPU Ví dụ, một GPU A100 duy nhất có thể hỗ trợ 624 Teraops tính toán INT8 thông qua tensor core của nó, trong khi chỉ có thể hỗ trợ 312 TFLOPS cho FLOAT16. Tuy nhiên, lớp phương pháp này đánh đổi độ chính xác cho hiệu suất và thường yêu cầu bước hiệu chỉnh. Trong bối cảnh SmartSpec, các tối ưu hóa lượng tử hóa có thể được áp dụng cho cả mô hình nháp và đích để cải thiện thêm hiệu suất của chúng tôi.

Các kỹ thuật Bộ nhớ đệm Tiền tố tiết kiệm tính toán của các tiền tố lặp lại thường xuyên trên các yêu cầu. Các hệ thống như SGLang [45], Cascade Inference [40], và Hydragen [15] đề xuất các kernel GPU hiệu quả để tính toán và cache tính toán cho các tiền tố được chia sẻ trên yêu cầu và cung cấp độ trễ suy luận thấp hơn. Trong bối cảnh SmartSpec, bộ nhớ đệm tiền tố có thể được áp dụng cho cả worker nháp và đích.

--- TRANG 11 ---
--- TRANG 12 ---
--- TRANG 13 ---
9 Kết luận
Giải mã suy đoán gần đây đã nổi lên như một phương tiện để giảm độ trễ suy luận với chi phí tăng overhead tính toán. Để khai thác lợi ích của giải mã suy đoán mà không làm ảnh hưởng đến hiệu quả, chúng tôi giới thiệu một khung làm việc ra quyết định thích ứng SmartSpec được hướng dẫn bởi khái niệm goodput. Đánh giá của chúng tôi trên ba bộ dữ liệu riêng biệt cho thấy rằng SmartSpec có thể giảm độ trễ với hệ số từ 1.2× đến 3.2× khi tỷ lệ yêu cầu thấp, trong khi duy trì mức hiệu suất ngay cả dưới tỷ lệ yêu cầu cao.

Tài liệu tham khảo
[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023).
[2] anon8231489123. 2024. ShareGPT dataset. https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered
[3] Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. 2000. A neural probabilistic language model. Advances in neural information processing systems 13 (2000).
[4] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee, Deming Chen, and Tri Dao. 2024. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774 (2024).
[5] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. 2023. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318 (2023).
[6] Zhuoming Chen, Avner May, Ruslan Svirschevski, Yuhsun Huang, Max Ryabinin, Zhihao Jia, and Beidi Chen. 2024. Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding. arXiv preprint arXiv:2402.12374 (2024).
[7] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale. arXiv:2208.07339 [cs.LG]
[8] Elias Frantar and Dan Alistarh. 2024. Marlin: a fast 4-bit inference kernel for medium batchsizes. https://github.com/IST-DASLab/marlin.
[9] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2023. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers. arXiv:2210.17323 [cs.LG]
[10] Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. 2024. Break the sequential dependency of llm inference using lookahead decoding. arXiv preprint arXiv:2402.02057 (2024).
[11] Pin Gao, Lingfan Yu, Yongwei Wu, and Jinyang Li. 2018. Low latency rnn inference with cellular batching. In Proceedings of the Thirteenth EuroSys Conference. 1-15.
[12] Karl Moritz Hermann, Tomás Kociský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching Machines to Read and Comprehend. In NIPS. 1693-1701. http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend
[13] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7B. arXiv preprint arXiv:2310.06825 (2023).
[14] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. arXiv preprint arXiv:2401.04088 (2024).
[15] Jordan Juravsky, Bradley Brown, Ryan Ehrlich, Daniel Y. Fu, Christopher Ré, and Azalia Mirhoseini. 2024. Hydragen: High-Throughput LLM Inference with Shared Prefixes. arXiv:2402.05099 [cs.LG]
[16] Ehsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan Thakur, and Jimmy Lin. 2023. HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution. arXiv:2307.16883 (2023).
[17] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W. Mahoney, and Kurt Keutzer. 2024. SqueezeLLM: Dense-and-Sparse Quantization. arXiv:2306.07629 [cs.CL]
[18] Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W Mahoney, Amir Gholami, and Kurt Keutzer. 2024. Speculative decoding with big little decoder. Advances in Neural Information Processing Systems 36 (2024).
[19] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin

--- TRANG 14 ---
Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles. 611-626.
[20] Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning. PMLR, 19274-19286.
[21] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. 2024. Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077 (2024).
[22] Feng Lin, Hanling Yi, Hongbin Li, Yifan Yang, Xiaotian Yu, Guangming Lu, and Rong Xiao. 2024. BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models. arXiv preprint arXiv:2401.12522 (2024).
[23] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Chuang Gan, and Song Han. 2023. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. arXiv:2306.00978 [cs.CL]
[24] Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, and Hao Zhang. 2023. Online speculative decoding. arXiv preprint arXiv:2310.07177 (2023).
[25] Yusuf Mehdi. 2023. Reinventing search with a new AI-powered Microsoft Bing and Edge, your copilot for the web. https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/ Accessed: 2024-02-21.
[26] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. 2023. Specinfer: Accelerating generative llm serving with speculative inference and token tree verification. arXiv preprint arXiv:2305.09781 (2023).
[27] Microsoft. 2023. Copilot. https://copilot.microsoft.com/ Accessed: 2024-02-21.
[28] Nvidia. 2024. A100 GPU Spec. https://www.nvidia.com/en-us/data-center/a100/ Accessed: 2024-03-10.
[29] NVIDIA. 2024. TensorRT-LLM. https://github.com/NVIDIA/TensorRT-LLM.
[30] OpenAI. 2022. ChatGPT. https://chat.openai.com/ Accessed: 2024-02-21.
[31] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. 2023. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems 5 (2023).
[32] Elizabeth Reid. 2023. Supercharging Search with generative AI. https://blog.google/products/search/generative-ai-search/ Accessed: 2024-02-21.
[33] Apoorv Saxena. 2023. Prompt Lookup Decoding. https://github.com/apoorvumang/prompt-lookup-decoding/
[34] Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get To The Point: Summarization with Pointer-Generator Networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Vancouver, Canada, 1073-1083. https://doi.org/10.18653/v1/P17-1099
[35] Qidong Su, Christina Giannoula, and Gennady Pekhimenko. 2023. The synergy of speculative decoding and batching in serving large language models. arXiv preprint arXiv:2310.18813 (2023).
[36] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).
[37] Siqi Wang, Hailong Yang, Xuezhu Wang, Tongxuan Liu, Pengbo Wang, Xuning Liang, Kejie Ma, Tianyu Feng, Xin You, Yongjun Bao, et al. 2024. Minions: Accelerating Large Language Model Inference with Adaptive and Collective Speculative Decoding. arXiv preprint arXiv:2402.15678 (2024).
[38] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, and Chi Wang. 2023. AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation. arXiv:2308.08155 [cs.AI]
[39] Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, and Chi Wang. 2023. An Empirical Study on Challenging Math Problem Solving with GPT-4. In ArXiv preprint arXiv:2306.01337.
[40] Zihao Ye, Ruihang Lai, Bo-Ru Lu, Chien-Yu Lin, Size Zheng, Lequn Chen, Tianqi Chen, and Luis Ceze. 2024. Cascade Inference: Memory Bandwidth Efficient Shared Prefix Batch Decoding. https://flashinfer.ai/2024/02/02/cascade-inference.html
[41] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. 2022. Orca: A distributed serving system for {Transformer-Based} generative models. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22). 521-538.
[42] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. 2018. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. arXiv preprint arXiv:1809.08887 (2018).
[43] Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, and Sharad Mehrotra. 2023. Draft & verify: Lossless large language model acceleration via self-speculative decoding. arXiv preprint arXiv:2309.08168 (2023).
[44] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems 36 (2024).
[45] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. 2023. Efficiently Programming Large Language Models using SGLang. arXiv:2312.07104 [cs.AI]
[46] Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-François Kagy, and Rishabh Agarwal. 2023. Distillspec: Improving speculative decoding via knowledge distillation. arXiv preprint arXiv:2310.08461 (2023).

--- TRANG 15 ---
--- TRANG 16 ---
Thuật toán 3 Đề xuất và xác minh dựa trên độ tin cậy SmartSpec.
Yêu cầu: Các yêu cầu đang chờ R. Ngưỡng độ tin cậy T. Độ dài đề xuất tối đa P.
1: best_goodput, best_verify_lens ← -1, 0
2: foreach r in R do
3:    // Khởi tạo độ tin cậy trước khi đề xuất token đầu tiên cho mỗi yêu cầu
4:    r.conf = 1
5: end for
6: propose_steps ← 0
7: while len(R) > 0 and propose_steps <= P do
8:    R' = {r for r in R if r.conf > T}
9:    // Propose sẽ cập nhật thuộc tính conf cho mỗi yêu cầu r trong R'
10:   R = Propose(R')
11:   propose_steps++
12: end while
13: ArgmaxGoodput(R)
14: Score(R)

10 Phụ lục
Trong phần này, chúng tôi khám phá việc sử dụng độ tin cậy làm tiêu chí cho việc chấp nhận token. Độ tin cậy được định nghĩa là xác suất đầu ra của token đề xuất được tạo ra bởi mô hình nháp. Như được mô tả trong Hình 17, một sự khác biệt rõ ràng tồn tại giữa phân phối độ tin cậy của các token được chấp nhận và bị từ chối. Sự khác biệt này gợi ý một cách trực quan rằng các token được đề xuất bởi mô hình nháp, khi đi kèm với mức độ tin cậy cao, có khả năng chính xác. Ngược lại, các đề xuất được đưa ra với độ tin cậy thấp ít có khả năng được chấp nhận bởi mô hình đích. Trong thực tế, chúng tôi thiết lập một ngưỡng T. Các token có mức độ tin cậy vượt quá T được dự đoán sẽ được chấp nhận, trong khi những token dưới ngưỡng này được dự đoán sẽ bị từ chối.

Ban đầu, SmartSpec đặt mức độ tin cậy của mỗi yêu cầu thành 1, tuân thủ định nghĩa trong Phần 4.4 trong đó độ tin cậy được coi như một xác suất và do đó không thể vượt quá 1. Điều này đảm bảo rằng mô hình nháp sẽ đề xuất ít nhất một token, kích hoạt quy trình được mô tả trong dòng 7-12 ít nhất một lần, với điều kiện P > 0. Trong mỗi bước đề xuất (dòng 7-12), SmartSpec chọn lọc chỉ xử lý những yêu cầu, được ký hiệu là R', có mức độ tin cậy vượt qua ngưỡng được chỉ định T từ bước đề xuất trước đó. Sau đó, SmartSpec gộp những yêu cầu R' này để thực thi lần chuyển tiếp. Theo chiến lược được nêu ở trên, SmartSpec cũng khám phá tất cả các độ dài tiềm năng để xác minh và chọn độ dài tối đa hóa goodput.
