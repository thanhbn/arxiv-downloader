# 2312.11462.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/speculative/2312.11462.pdf
# File size: 490267 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Cascade Speculative Drafting for Even Faster LLM Inference
Ziyi Chen1Xiaocong Yang1Jiacheng Lin1Chenkai Sun1Kevin Chen-Chuan Chang1Jie Huang1
Abstract
Introduced to enhance the efficiency of large lan-
guage model (LLM) inference, speculative decod-
ing operates by having a smaller model generate
a draft. A larger target model then reviews this
draft to align with its output, and any acceptance
by the target model results in a reduction of the
number of the target model runs, ultimately im-
proving efficiency. However, the drafting process
in speculative decoding includes slow autoregres-
sive generation and allocates equal time to gen-
erating tokens, irrespective of their importance.
These inefficiencies collectively contribute to the
suboptimal performance of speculative decoding.
To further improve LLM inference, we introduce
Cascade Speculative Drafting (CS Drafting), a
speculative execution algorithm that incorporates
two types of cascades. The Vertical Cascade
eliminates autoregressive generation from neu-
ral models, while the Horizontal Cascade opti-
mizes time allocation in drafting for improved ef-
ficiency. Combining both cascades, CS Drafting
achieves up to an 81 percent additional speedup
over speculative decoding in our experiments,
while maintaining the same output distribution
as the target model. Our code is publicly avail-
able at https://github.com/lfsszd/CS-Drafting.
1. Introduction
The advent of Large Language Models (LLMs), like GPT-4
(OpenAI, 2023), has marked a significant milestone in the
field of natural language processing (NLP). These models
have not only excelled in various NLP tasks but have also
found widespread applications in user-interactive settings,
such as chatbots and virtual assistants. However, these ap-
plications involve an extremely high number of users, up to
hundreds of millions daily. To serve in real-time at this scale,
1Department of Computer Science, University of Illi-
nois at Urbana-Champaign. Correspondence to: Ziyi
Chen <ziyic2@illinois.edu >, Kevin Chen-Chuan Chang <kc-
chang@illinois.edu >, Jie Huang <jeffhj@illinois.edu >.
Preprint.a low-latency system is not only cost-saving but also crucial
for keeping the service running. In addition, the sheer scale
of the service means that even a slight improvement in the
latency of LLMs can greatly contribute to both the service
provider and the community. Consequently, optimizing the
latency of LLMs has become a critical area of research.
Unfortunately, the ever-growing size of LLMs significantly
increases the latency, especially in long-form generation, as
autoregressive LLMs generate tokens one by one. An emerg-
ing solution, known as speculative decoding (Leviathan
et al., 2023; Chen et al., 2023; Xia et al., 2023), shows po-
tential to mitigate this issue. In speculative decoding, a draft
model (which is smaller and faster) generates ktokens in
each step (with kbeing a hyperparameter) autoregressively,
and these tokens are then reviewed by a target model (which
is larger and slower) in parallel. In one single run, the target
model will accept any tokens aligned with its output and fur-
ther generate one token. The drafting process in speculative
decoding enables the target model to generate multiple to-
kens in a single run while maintaining its output distribution
unchanged. With a properly sized draft model, speculative
decoding achieves a speedup of 2 to 3 times, making it a
potential method for solving high latency issues.
However, since draft models are typically required to gen-
erate multiple tokens in multiple steps, where each gener-
ation still involves inefficient autoregressive decoding, the
performance of speculative decoding could be limited by
the drafting latency. This inefficiency is also indicated by
Leviathan et al. (2023)’s experiments, where it was observed
that very small models (e.g., around two orders of magnitude
smaller than the target model) are usually the best choice
for drafting because their inference cost is lower compared
to that of a larger draft model, despite the fact that larger
draft models usually have higher-quality generation. This
underscores that improving drafting efficiency is crucial for
further enhancing the performance of speculative decoding.
In light of this, one key strategy to address this bottleneck
is to avoid the inefficient autoregressive generation of neu-
ral draft models. Based on this consideration, it is noted
that statistical language models, such as bigram language
models, incur negligible latency and computational resource
costs compared to neural language models, owing to their
simple structure. However, because the tokens generated by
1arXiv:2312.11462v4  [cs.LG]  27 Feb 2024

--- PAGE 2 ---
Cascade Speculative Drafting for Even Faster LLM Inference
Figure 1. The figure shows an example of Cascade Speculative Drafting with target model Mtand draft models Md1,Md2, andMd3.
The horizontal cascade involves using larger draft models to generate the earlier tokens and smaller models to generate the later tokens.
The vertical cascade requires each model to review drafts from smaller models with the exception of the smallest model which is a
statistical language model. As the horizontal cascade and vertical cascade are orthogonal to each other, CS Drafting combines both
approaches for optimal efficiency.
0 5 10 15 20 25 30
Token position0.10.20.30.40.50.60.70.8Acceptance rate
FLAN-T5-small
FLAN-T5-base
FLAN-T5-large������������������������������������������������
Figure 2. The probability of acceptance of draft tokens in relation
to their positions in a single step of speculative decoding, evaluated
on FLAN-T5- SMALL ,BASE , and LARGE models on GSM8K. The
draft model generates 30 tokens at each step. The results on
MMLU are shown in Figure 3 of the Appendix.
statistical language models usually do not have a high prob-
ability of being accepted by the target model, speculative
decoding with statistical language models alone may not
yield optimal results compared to using a well-sized neural
language model from the same family as the draft model.
Nonetheless, we notice that it is not necessary to use only
one draft model in speculative decoding—statistical lan-
guage models can serve as the “draft” model for the neural
draft model, thereby eliminating autoregressive generation
from the neural draft model.Furthermore, our analysis in Figure 2 reveals a pattern dur-
ing the drafting step: tokens generated later in the sequence
by the draft model show a progressively lower probability
of being accepted by the target model. This is because the
probability of a token being accepted is conditioned on the
acceptance of the previous tokens. It indicates that later
tokens from draft models are more prone to rejection, con-
tributing less to the expected number of accepted tokens per
draft step, yet incurring the same latency.
Inspired by the above observations, we propose Cascade
Speculative Drafting (CS Drafting) , a speculative execu-
tion algorithm that comprises multiple draft models, starting
with the smallest being a statistical language model. Each
neural draft model reviews generations from a smaller model
and then proposes its reviewed content to either a larger
draft model or the target model. In this design, the drafting
of each neural model will be accelerated by drafting from
a smaller model, avoiding the inefficiency of autoregres-
sive generation from neural models. We refer to this tiered
speculative decoding approach as the Vertical Cascade . In
addition, we suggest the use of smaller, faster draft models
for generating high-rejection tokens that are trailing in draft-
ing generation, forming the Horizontal Cascade . Along
with the aforementioned Vertical Cascade , these strategies
compose our complete CS Drafting approach, as illustrated
in Figure 1.
Through theoretical analysis and empirical studies, we
demonstrate that the CS Drafting algorithm outperforms
speculative decoding in terms of latency across various tasks
2

--- PAGE 3 ---
Cascade Speculative Drafting for Even Faster LLM Inference
and settings, achieving an additional speedup of up to 81%
over speculative decoding. These findings highlight the
practical advantages and efficiency enhancements offered
by both vertical and horizontal cascades.
The main contributions are summarized as follows:
•We introduce Cascade Speculative Drafting (CS Drafting),
a speculative-execution-based algorithm that improves
language model inference speed without sacrificing gen-
eration quality.
•We provide theoretical analyses supporting the effective-
ness of the proposed CS Drafting approach.
•We conduct empirical experiments showing that CS Draft-
ing achieves further speedup over speculative decoding
across different tasks and settings.
2. Preliminary
The core concept of speculative decoding (Leviathan et al.,
2023) involves the utilization of a small draft model for se-
quential token generation with validation by a larger target
model resulting in reduced latency. This design acceler-
ates sampling from autoregressive models without altering
output distributions. At its heart, there are two key obser-
vations: 1) certain generations in language modeling are
simpler than others and can be predicted by more efficient
models correctly, and 2) using speculative execution along
with a new sampling method enables faster, exact decoding
from large models.
Specifically, let xbe the input tokens at a run and Mtand
Mdare the target and the draft model respectively, kbe the
number of draft tokens generated per step, and Mt(x)[i]and
Md(x)[i]be their probability output at i-th token when in-
put is x. We interpret speculative sampling as a two-stage op-
eration. In the proposing stage, we sample {xt+1, ..., x t+k}
from draft model Mdautoregressively and append them
tox. In the reviewing stage, let xi∈ {xt+1, ..., x t+k}
represents the token at the current position, and we ac-
cept it if Md(x)[i−1]≤ M t(x)[i−1]; in the event that
Md(x)[i−1]>Mt(x)[i−1], we reject xiwith a proba-
bility of 1−Mt(x)[i−1]
Md(x)[i−1]and proceed to resample xifrom
a recalibrated distribution norm (max(0 ,Mt(x)[i−1]−
Md(x)[i−1]))and reject any token following xi. At the
end, the target model will generate one additional token
following the accepted tokens. Such a design guarantees the
output is the same as sampling autoregressively using the
target model alone, as proven in Leviathan et al. (2023).
Speculative decoding was empirically validated on various
tasks and model sizes, demonstrating a significant accelera-
tion in inference times (2x-3x faster) compared to standard
implementations, without affecting the outputs. Importantly,it does not require task-specific training, altering model ar-
chitectures, or changing training procedures, making it a
practical solution for reducing the latency of LLM inference.
3. Cascade Speculative Drafting
In this section, we introduce our proposed method, Cascade
Speculative Drafting (CS Drafting), a speculative execution
algorithm that incorporates two types of cascades: vertical
cascade andhorizontal cascade .
3.1. Vertical Cascade
A notable inefficiency of the speculative decoding algorithm
is the reliance on the autoregressive generation of a smaller
draft model. Since the draft model must run ktimes for each
target model run, the cost can still be significant despite
its smaller size. In light of this, we reduce the drafting
inefficiency by using an even smaller model to assist in
drafting and employing the original draft model to review
the generation of this smaller model. In addition, since
this process can be performed again on the draft model
that drafts for the original model, we recursively perform
this process until it reaches a statistical draft model that
involves negligent cost, such as a bigram language model.
In this approach, we expect each recursion step will reduce
the drafting latency without altering the output distribution.
We refer to this recursive speculative approach as Vertical
Cascade .
Additionally, we incorporate lenience , a hyperparameter that
loosens the review process by the target model, allowing for
faster speed at the trade-off of potentially differing results
from the target model (Leviathan et al., 2023). Lenience
can be adopted during sampling or greedy decoding with
speculative decoding. Let lenience l∈[1,∞). When sam-
pling, the acceptance condition for token xiis transformed
toMd(x)[i]≤l×M t(x)[i]. If the acceptance condition is
not satisfied, with a probability of 1−l×M t(x)
Md(x), we reject
xiand any following tokens.1When performing greedy de-
coding, the acceptance condition becomes deterministic and
is simply either argmax Md(x)[i] =argmax Mt(x)[i]or
Md(x)[i]≤l× M t(x)[i].
For the speculative decoding algorithm, the reduced quality
introduced by lenience is generally undesirable. However,
for the vertical cascade approach, lenience affects the final
output only if it is applied when the target model reviews.
Therefore, we can limit the application of lenience in the ver-
tical cascade only when draft models review and do not ap-
ply lenience when the target model reviews. This can ensure
the final output is not altered while further reducing latency.
1For simplicity, we assume the probability outputs are not
standardized. We refer the readers to Leviathan et al. (2023) for
the discussion on standardized sampling.
3

--- PAGE 4 ---
Cascade Speculative Drafting for Even Faster LLM Inference
Algorithm 1 CascadeSpeculativeDraftingStep
Require: draft models {Md1, ...,Mdn}, target mode Mt,prefix , flag isFirstCall , hyperparameters Knn,l
draftList ←[Md1, ...,Mdn]
▷Initialize curGen and curProb.
curGen ←prefix , curProbs ←a list of ones with the same length as prefix
▷Unpack the a list of kfor the current function call.
[k1, ..., k n−1]←first row of Knn
▷Generate using MaG for the Base case of the recursive call.
ifdraftList is empty then
M ← first element of draftList
res← M (curGen )
return res.generation, res.logits
end if
▷Perform the horizontal cascade with the for loop.
fori←1tondo
▷Prepare the arguments for the next recursive call.
curTarget ←thei-th item of draftList
curDraftList ←the sublist of draftList starting from index i+ 1
curK←the submatrix of Knnfrom with the top-left corner at (i+ 1, i+ 1) extending to the bottom-right corner
curPrefix ←curGen
while curGen.length - curPrefix.length is less than kido
curPrefix ←curGen
▷Perform the vertical cascade with the recursive call.
[x1, .., x u],[p1, p2, ..., p v]←CascadeSpeculativeDraftingStep (curDraftList, curTarget, curPrefix, False, curK, l)
curGen ←[x1, .., x u]
s←curProbs.length + 1
Add elements of [p1, p2, ..., p v]to curProbs
end while
end for
▷Set lenience to 1when the original target model reviews.
ifisFirstCall then
l←1
end if
▷UseMtto review the draft generation.
[x1, ..., x out],[p′
1, p′
2, ..., p′
out]= review( Mt, curGen, curProbs, l)
return [x1, ..., x out],[p′
1, p′
2, ..., p′
out]
3.2. Horizontal Cascade
Another key observation is that during the drafting steps
of speculative decoding, not all drafting tokens are created
equal, as illustrated in Figure 2. The first draft token is
more likely to be accepted as it only depends on itself; the
last token is rarely accepted, as it has a chance of being
reviewed only if all preceding tokens are accepted. From a
theoretical perspective, assume the event of acceptance of
each token being a Bernoulli distribution with probably p,
the probability of n-th token being accepted is pn, implying
an exponential decrease of value for tokens generated later
in the sequence.
Inspired by this observation, we designed Horizontal Cas-
cade , an approach that improves time allocation by drafttoken allocation. Horizontal Cascade assigns the largest
draft model to perform the generation of the first draft token
due to its highest output alignment with the target model,
and it progressively uses a smaller as the new draft token to
be generated is less likely to be accepted. This process stops
after the smallest model, i.e., a statistical language model
finishes. This design reduces the time cost of generating
unimportant draft tokens with a costly draft model, leading
to a reduction in overall latency.
3.3. Max-Gram for Better Statistical Drafting
As both Vertical Cascade and Horizontal Cascade remark
cascade toward faster draft models, a statistical language
model, which is the basis of the cascade, becomes essential
for the efficiency of both approaches. In our pursuit of a
4

--- PAGE 5 ---
Cascade Speculative Drafting for Even Faster LLM Inference
more effective statistical language model, we noticed a gen-
eral pattern: in language model generation, some words and
phrases from the input query frequently reappear in the gen-
erated content. In light of this observation, we designed the
Max-Gram (MaG) algorithm. It greedily identifies maximal
matches between the initial input (or existing generation)
and tokens from the end of the generation. In cases where
there is no match, we resort to a bigram model based on the
probability distribution of Wikipedia (chosen to maintain
the generality). We include a GPU-friendly version of the
Max-Gram algorithm in Appendix A.
3.4. Algorithm
Combining the horizontal and vertical cascades, the algo-
rithm of cascade speculative decoding is presented in Al-
gorithm 1. At its center, the horizontal cascade is realized
by the for loop, while the vertical cascade is implemented
through recursive calls. Notably, the MaG model is incor-
porated as the smallest draft model to avoid autoregressive
generation from a neural model. An example of CS Drafting
is shown in Figure 1.
The algorithm requires an upper-triangular hyperparameter,
Knn, with each row serving as the stop criteria for a layer
of recursive calls. For simplicity, we assume the lenience l
is universal for the algorithm, except when the target model
is under review; thus, the algorithm can benefit from the
speedup of lenience without altering the output distribution.
4. Analysis
We begin with some notions. Let Mtbe the target model,
Mdbe the draft model, and kbe the number of draft tokens
generated per step.
Expected acceptance rate α(Mt,Md)is the probability
of draft generation by Mdbeing accepted by target model
Mt.
Cost coefficient c(Mt,Md)is the ratio of time for a single
run of draft model Mdover target model Mt.
Expected walltime improvement factor (EWIF) is the
expected time improvement achieved by an algorithm under
the i.i.d. assumption of token acceptance.
Despite the simple setting of EWIF, Leviathan et al. (2023)
have demonstrated that it aligns with the experimental re-
sults in most instances. Therefore, our analysis will concen-
trate on EWIF.
4.1. Vertical Cascade
We analyze EWIF of vertical cascade using generating func-
tions, a well-studied topic in combinatorial mathematics
(West, 2021). The properties of generating functions areuseful in the recursion and evaluation process making the
our final expression simple.
We begin with the derivation of the probability generating
function for speculative decoding.
Theorem 4.1. For speculative decoding between Mtand
Md, letpibe the probability of generating itokens. The
probability generating function of pisatisfies the following
equation:
ϕ(α,k)(x) = 1 + ( x−1)1−αk+1xk+1
(1−αx), (1)
where α=α(Mt,Md).
Proof in Appendix C.1.
Corollary 4.2. The EWIF of speculative decoding is
ϕ′
(α,k)(1)
(ck+1)=1−αk+1
(1−α)(ck+1).
We use the generating function to derive the EWIF of a
vertical cascade and analyze the case involving two draft
models, Md1andMd2.
Theorem 4.3. Assume kto be the speculative decoding
parameter between Md1andMd2, and nto be the number
of steps Mtreviews. The EWIF by this system is
1−αϕn(α)
(1−α)(1 + ncd1+nkcd2), (2)
where ϕ(x) =ϕ(α(Md1,Md2),k)(x),α=α(Mt,Md), and
cd1, cd2bec(Mt,Md1), c(Mt,Md2)respectively.
Corollary 4.4. ϕα′,k(α)< α for any 1> α > 0,1> α′>
0, k > 0, so if cd2<<1, the EWIF of Md1andMd2is
higher than EWIF of Md1alone.
Proof in Appendix C.2.
Therefore, with the statistical model having negligible cost
(i.e.,cd2<<1), it can almost always improve the efficiency
of an SD system.
4.2. Horizontal Cascade
We also present an analysis of the walltime improvement
offered by the horizontal cascade.
To assist the analysis, we establish the notions. Let Mt
be the target model, {Mi}be the draft models assisting
generation with Mibeing the draft model generating the
i-th token. In the simpler case of the speculative decoding,
Mi=Mdfor any i. Let xbe the input to the model at
a single run, Mt(x)andMi(x)are then the output prob-
ability distribution with input x. To simplify notation, let
αi=α(Mt(x),Mi(x))andci=c(Mt(x),Mi(x)).
Theorem 4.5. The expected walltime improve-
ment factor (EWIF) of the horizontal cascade is
T(k, α1, ..., α k, c1, ..., c k) =Pk
i=0Qi
j=1αj
1+Pk
i=1ci.
5

--- PAGE 6 ---
Cascade Speculative Drafting for Even Faster LLM Inference
Dataset Md1Md2 k1k2EWIF
CNNDM SMALL - 9 - 2.65
CNNDM BASE - 8 - 2.96
CNNDM BASE SMALL 5 3 3.03
ENDE SMALL - 12 - 3.61
ENDE BASE - 11 - 3.75
ENDE BASE SMALL 5 8 3.93
Table 1. Simulated EWIF under the assumption that the acceptance
distribution is a Bernoulli distribution. BASE and SMALL refer to
FLAN-T5- BASE and FLAN-T5- SMALL . In the simulation, specu-
lative sampling with horizontal cascade exceeded the performance
of the vanilla speculative decoding on both CNN Dailymail (Nalla-
pati et al., 2016) and WMT EnDe (Bojar et al., 2018) datasets.
Furthermore, theorem 4.5 can be used to analyze the impor-
tance of the tokens in the drafting step.
Corollary 4.6. The probability of i-th token being ac-
cepted isQi
j=1αj. The derivative of EWIF with re-
spect to αlisdT(k,α1,...,α k,c1,...,ck)
dαl=Pk
i=lQi
j=1,j̸=lαj
1+Pk
i=1ci.
Specifically,dT(k,α1,...,α k,c1,...,ck)
dα1=Pk
i=1Qi
j=2αj
1+Pk
i=1ciand
dT(k,α1,...,α k,c1,...,ck)
dαk=Qk−1
j=1αj
1+Pk
i=1ci.
Using the information provided by Leviathan et al. (2023),
we calculate a simulated EWIF under the assumption that
the event of acceptance by the target model is a Bernoulli
trial. The results, shown in Table 1, indicate that speculative
sampling with a horizontal cascade achieved better EWIF
than vanilla speculative sampling under this assumption.
5. Experiments
5.1. Evaluation Metric
Previous works on speculative decoding and related meth-
ods relied on walltime as an evaluation method. However,
there are standardization and legitimacy concerns related
to walltime. Leviathan et al. (2023) mentioned that cost
coefficiency, the ratio between the time for a single run of
draft model and target model will vary from system to sys-
tem which can result in significant variation in wall time
improvement across different systems, making comparisons
across different research work difficult.
In addition, a recent analysis suggests GPU speed can vary
for the same GPU model with one being 1.5x faster than
another GPU of the same model (Sinha et al., 2022); there-
fore, theoretical walltime improvement up to 1.5x can be
undermined by GPU variation, and methods without any
improvement can be measured with an improvement by
running on a different GPU. The lack of standardization
and stability makes walltime unfit for measuring algorith-
mic improvement. To resolve this issue, we design a new
metric to measure speculative decoding and its related al-gorithms which can reduce the impact of varying/unstable
GPU performance.
Our proposed method, standardized walltime improvement
(SWI), calculates the GPU times of the models, assuming
that each run of a language model costs the same amount
of time, an assumption made when inventing the specu-
lative sampling algorithm (Chen et al., 2023). Given a
speculative decoding system Scontaining kdraft models
Md1, ...,Mdk, target model Mtsolving a task T, and let
count (S, m, T )be the count of runs of model mduring the
execution of task Twith system S, letcibe the expected
cost coefficient being the time ratio between a single run of
MiandMt, and let nbe the total of tokens generated, the
standardized walltime improvement is defined as
SWI (Md1, ...,Mdk,Mt, T) =
n
count (S,Mt, T) +Pn
i=1count (S,Mdi, T)ci,
which is simply the sum of GPU time by all models
when ciis measured during a local experiment. The
benefit of the SWI evaluation is that count (S,Mt, T)
andcount (S,Mdi, T)will be the consistent across
different systems, so by using the same ciacross different
experiments the GPU performance variation will be elim-
inated; therefore, it provides a standardized, reproducible
measurement.
During our experiment, we will use two sets of {ci}, exist-
ing{ci}reported by Leviathan et al. (2023) and an heuristic
{ci}being the ratio of number of tunable parameters be-
tweenMdiandMt.
5.2. Setup
To ensure the generality of our findings, we perform
experiments on both encoder-decoder and decoder-only
models. For encoder-decoder models, we choose our target
and draft models from the FLAN-T5 (Chung et al., 2022)
family for our experiment, as there is a large variation in
model sizes within the FLAN-T5 family (ranging from 77
million to 11 billion parameters). We use FLAN-T5- XXL as
our target model, FLAN-T5- BASE and FLAN-T5- SMALL
as our reviewing draft models. For decoder-only models,
we select LLAMA-2-chat-7B (Touvron et al., 2023) as the
target model. Since there is no official release of a smaller
model in LLAMA family, we use a pretrained 160M model
with the same tokenizer as the reviewing draft model (Miao
et al., 2024). In both cases, the Max-Gram algorithm is
used as the generating draft model. To align our experiment
with current common usage, we do not perform fine-tuning,
and the generation is conducted in a zero-shot manner.
Since we do not observe any significant difference between
sampling with temperature 1and greedy decoding in pre-
vious speculative decoding experiments (Leviathan et al.,
6

--- PAGE 7 ---
Cascade Speculative Drafting for Even Faster LLM Inference
Dataset Algorithm {Mdi} Speedup (MS) k11k12k22l
GSM8K Autoregressive - 1 - - - -
GSM8K S Decoding BASE 3.38 10 - - -
GSM8K S Decoding SMALL 3.06 11 - - -
GSM8K CS Drafting BASE ,MAG 3.70 10 - - -
GSM8K CS Drafting SMALL ,MAG 3.19 11 - - -
GSM8K CS Drafting BASE ,SMALL ,MAG 3.88 8 13 1 3
MMLU Autoregressive - 1 - - - -
MMLU S Decoding BASE 3.97 13 - - -
MMLU S Decoding SMALL 4.12 19 - - -
MMLU CS Drafting BASE ,MAG 4.56 13 - - -
MMLU CS Drafting SMALL ,MAG 4.39 14 - - -
MMLU CS Drafting BASE ,SMALL ,MAG 4.88 5 19 1 5
Dataset Algorithm {Mdi} Speedup (PW) k11k12k22l
GSM8K Autoregressive - 1 - - - -
GSM8K S Decoding BASE 2.99 8 - - -
GSM8K S Decoding SMALL 2.76 8 - - -
GSM8K CS Drafting BASE ,MAG 3.27 9 - - -
GSM8K CS Drafting SMALL ,MAG 2.82 11 - - -
GSM8K CS Drafting BASE ,SMALL ,MAG 3.43 5 9 1 3
MMLU Autoregressive - 1 - - - -
MMLU S Decoding BASE 3.42 10 - - -
MMLU S Decoding SMALL 3.51 11 - - -
MMLU CS Drafting BASE ,MAG 4.21 6 - - -
MMLU CS Drafting SMALL ,MAG 3.99 13 - - -
MMLU CS Drafting BASE ,SMALL ,MAG 4.32 5 8 1 5
Table 2. The experimental results on FLAN-T5. Speedup (MS) is the standardized walltime improvement with the assumption that the
latency of each run of a model is its number of parameters (model size). Speedup (PW) is the SWI with the assumption that the latency of
each run of a model is the time cost data reported from previous work (Leviathan et al., 2023). k11,k12,k22,lare the hyperparameters.
k11andk12represent the step limitation target model and the draft models, k22is the step limitations between the first and second draft
model, and lis lenience as shown in algorithm 1. For speculative decoding, the k11is simply the k.
2023), and to ensure our experiments are fully reproducible,
we perform sampling at temperature 0, i.e., using greedy
decoding by default.
Cascade Speculative Drafting Inference To reduce the
number of hyperparameters to tune, we use MaG to generate
10 tokens at once, as it is rare for more than 10 tokens to be
accepted. We do not use lenience when the reviewer is Mt
to ensure the output distribution does not change. We also
avoid lenience between MaG and its reviewer, since there
is still a significant performance gap between MaG and a
neural model.
With these constraints, we are left with at most four hyper-
parameters: k11,k12,k22, and l. For the CS Drafting step
where the target model is the reviewer, k11andk12are used.
k21andlare used in the step where Md1is the reviewer.
Baselines We consider both speculative decoding and
vanilla autoregressive generation as our baselines. Whenperforming speculative decoding with the target model as
FLAN-T5- XXL, we opt for FLAN-T5- SMALL and FLAN-
T5-BASE as the draft models, as they perform better than
FLAN-T5- LARGE , according to Leviathan et al. (2023) and
our preliminary experiments. When LLAMA-2-chat-7B is
the target model, we use the 160M model as the draft model.
We test a wide range of k-values, from 2 to 30, to ensure
that we find the best result for each model serving as our
draft model to establish our baseline.
Dataset We chose two commonly used datasets for our
experiments. For both datasets, we conducted experiments
in a zero-shot chain-of-thought setup (Kojima et al., 2022;
Wei et al., 2023):
•GSM8K (Cobbe et al., 2021) is a dataset comprising
8,500 high-quality, linguistically diverse, grade-school
math word problems. It focuses on multi-step reason-
ing with problems that are typically solvable using basic
7

--- PAGE 8 ---
Cascade Speculative Drafting for Even Faster LLM Inference
Dataset Algorithm {Mdi} Speedup k11
GSM8K Auto. - 1 -
GSM8K S Decoding 160 M 2.48 12
GSM8K CS Drafting 160 M,MAG 2.86 15
MMLU Auto. - 1 -
MMLU S Decoding 160 M 2.12 10
MMLU CS Drafting 160 M,MAG 2.64 13
Table 3. The experimental results on LLAMA.
arithmetic in 2-8 steps.
•MMLU (Hendrycks et al., 2021), or Massive Multitask
Language Understanding, is a benchmark for testing how
well large language models grasp knowledge. It encom-
passes 57 diverse subjects, ranging from elementary sci-
ence to advanced law.
5.3. Experimental Results
Comparison to Baselines Table 2 presents the experi-
mental results. In two settings of SWI, Cascade Specula-
tive Decoding has outperformed the speculative decoding
algorithm. For GSM8K, Cascade Speculative Decoding
achieved a maximum additional speedup of 44% over the
fastest speculative algorithm; for MMLU, the maximum
additional speedup improvement over speculative decoding
is 81%. Overall, CS Decoding achieves a 3-4x speedup
over autoregressive generation, establishing itself as a faster
algorithm over the baselines.
Effectiveness of MaG When comparing CS Drafting with
one neural model and MaG against the fastest speculative
decoding setup, we found that CS Drafting with one neu-
ral model gained up to a 70% speedup on MMLU and a
32% speedup on GSM8K. Notably, the MaG algorithm only
involves a bigram model with as many parameters as the
size of the tokenizer, making its memory cost negligible.
Notably, the speedup gained using CS Drafting with one
neural model involves no additional deployment overhead
while reducing both latency and computational cost, making
it a superior choice over speculative decoding.
Draft Model Size Despite FLAN-T5- SMALL mostly outper-
forming FLAN-T5- BASE as a draft model for speculative
decoding, in CS Drafting with the aid of MaG, FLAN-T5-
BASE consistently outperforms FLAN-T5- SMALL . This
implies that with the limitation of a single draft model, the
ideal size of the draft model might increase with the assis-
tance of the MaG model.
Results of Decoder-only Models As shown in Table 3, the
results for decoder-only models exhibit similar traits to those
observed in the experiments on encoder-decoder models.
Therefore, for speculative decoding-based algorithms, webelieve there is no distinct difference between an encoder-
decoder model and a decoder-only model, aligning with the
results in Leviathan et al. (2023).
6. Related Work
6.1. Efficienct Methods for Language Model Inference
In the era of large language models, efficiency during infer-
ence becomes a key to model service. To reduce the model
inference cost and speed up, several efficient methods have
been proposed, including pruning, knowledge distillation
and quantization (Treviso et al., 2023). Model pruning
takes structured (Xia et al., 2022; V oita et al., 2019) or un-
structured (Guo et al., 2021; Chen et al., 2020) methods
to remove the redundant model parameters to reduce the
storage memory and increase inference speed. Knowledge
distillation takes the approach of transferring knowledge
from a superior teacher model to a smaller student model
(Hinton et al., 2015; Gou et al., 2021). Quantization maps
high-precision data representations (e.g. 32 bits) into low-
precision ones (e.g. 8 bits) to reduce memory consumption
(Bhandare et al., 2019; Schaefer et al., 2023).
6.2. Speculative Decoding
With the success of Speculative Decoding (Chen et al.,
2023; Leviathan et al., 2023) in reducing the large
language model inference latency, some recent works
have attempted to improve Speculative Decoding by
reducing the rejection rate. Zhou et al. (2023) propose using
generalized knowledge distillation and achieve a lower
rejection rate compared to other knowledge distillation
methods. Avoiding an additional draft model, self-drafting
is an approach to speculative decoding by reusing part of
the target model together with added weight to perform
drafting (Zhang et al., 2023; Hooper et al., 2024). Tree
attention involves generating multiple candidates during
drafting to increase the chance of acceptance (Spector &
Re, 2023; Miao et al., 2024). Besides reducing the rejection
rate, improving drafting efficiency can also reduce latency.
Spector & Re (2023) propose using speculative decoding
for drafting, showing similarities to the vertical cascade;
however, their method only has two layers of speculative
decoding and does not observe the recursive nature of the
vertical cascade nor the lenience among draft models, two
crucial aspects for the performance of vertical cascade.
7. Conclusion
In this work, we propose a novel algorithm, CS Drafting,
which involves two cascades: the vertical cascade and the
horizontal cascade. The vertical cascade eliminates the ne-
cessity of autoregressive generation from a neural language
model, while the horizontal cascade effectively allocates the
8

--- PAGE 9 ---
Cascade Speculative Drafting for Even Faster LLM Inference
cost of drafting tokens at different positions. Our experi-
ments show that CS Drafting achieves up to an 81 percent
additional speedup over speculative decoding, while main-
taining the same output distribution as the target model. Our
work demonstrates that LLM inference can be further sped
up by cascades without sacrificing generation quality or
requiring additional task-specific training.
Impact Statement
This paper presents work whose goal is to advance the field
of Machine Learning. There are many potential societal
consequences of our work, none which we feel must be
specifically highlighted here.
References
Bhandare, A., Sripathi, V ., Karkada, D., Menon, V ., Choi, S.,
Datta, K., and Saletore, V . Efficient 8-bit quantization of
transformer neural machine language translation model,
2019.
Bojar, O., Federmann, C., Fishel, M., Graham, Y ., Haddow,
B., Huck, M., Koehn, P., and Monz, C. Findings of the
2018 conference on machine translation (WMT18). In
Bojar, O., Chatterjee, R., Federmann, C., Fishel, M., Gra-
ham, Y ., Haddow, B., Huck, M., Yepes, A. J., Koehn, P.,
Monz, C., Negri, M., N ´ev´eol, A., Neves, M., Post, M.,
Specia, L., Turchi, M., and Verspoor, K. (eds.), Proceed-
ings of the Third Conference on Machine Translation:
Shared Task Papers , pp. 272–303, Belgium, Brussels, Oc-
tober 2018. Association for Computational Linguistics.
doi: 10.18653/v1/W18-6401.
Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre,
L., and Jumper, J. Accelerating large language model
decoding with speculative sampling. arXiv preprint
arXiv:2302.01318 , 2023.
Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y ., Wang,
Z., and Carbin, M. The lottery ticket hypothesis for pre-
trained bert networks, 2020.
Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y .,
Fedus, W., Li, Y ., Wang, X., Dehghani, M., Brahma,
S., et al. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416 , 2022.
Cobbe, K., Kosaraju, V ., Bavarian, M., Chen, M., Jun, H.,
Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,
R., Hesse, C., and Schulman, J. Training verifiers to solve
math word problems, 2021.
Gou, J., Yu, B., Maybank, S. J., and Tao, D. Knowledge
distillation: A survey. International Journal of Computer
Vision , 129(6):1789–1819, March 2021. ISSN 1573-1405.
doi: 10.1007/s11263-021-01453-z.Guo, D., Rush, A. M., and Kim, Y . Parameter-efficient
transfer learning with diff pruning, 2021.
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,
Song, D., and Steinhardt, J. Measuring massive multitask
language understanding, 2021.
Hinton, G., Vinyals, O., and Dean, J. Distilling the knowl-
edge in a neural network, 2015.
Hooper, C., Kim, S., Mohammadzadeh, H., Genc, H.,
Keutzer, K., Gholami, A., and Shao, S. Speed: Spec-
ulative pipelined execution for efficient decoding, 2024.
Kojima, T., Gu, S. S., Reid, M., Matsuo, Y ., and Iwasawa,
Y . Large language models are zero-shot reasoners. Ad-
vances in neural information processing systems , 35:
22199–22213, 2022.
Langley, P. Crafting papers on machine learning. In Langley,
P. (ed.), Proceedings of the 17th International Conference
on Machine Learning (ICML 2000) , pp. 1207–1216, Stan-
ford, CA, 2000. Morgan Kaufmann.
Leviathan, Y ., Kalman, M., and Matias, Y . Fast inference
from transformers via speculative decoding. In Inter-
national Conference on Machine Learning , pp. 19274–
19286. PMLR, 2023.
Miao, X., Oliaro, G., Zhang, Z., Cheng, X., Wang, Z.,
Zhang, Z., Wong, R. Y . Y ., Zhu, A., Yang, L., Shi, X.,
Shi, C., Chen, Z., Arfeen, D., Abhyankar, R., and Jia,
Z. Specinfer: Accelerating generative large language
model serving with tree-based speculative inference and
verification, 2024.
Nallapati, R., Zhou, B., dos santos, C. N., Gulcehre, C., and
Xiang, B. Abstractive text summarization using sequence-
to-sequence rnns and beyond, 2016.
OpenAI. Gpt-4 technical report, 2023.
Schaefer, C. J., Guo, E., Stanton, C., Zhang, X., Jablin,
T., Lambert-Shirzad, N., Li, J., Chou, C., Joshi, S., and
Wang, Y . E. Mixed precision post training quantization
of neural networks with sensitivity guided search, 2023.
Sinha, P., Guliani, A., Jain, R., Tran, B., Sinclair, M. D.,
and Venkataraman, S. Not all gpus are created equal:
characterizing variability in large-scale, accelerator-rich
systems. In SC22: International Conference for High Per-
formance Computing, Networking, Storage and Analysis ,
pp. 01–15. IEEE, 2022.
Spector, B. and Re, C. Accelerating llm inference
with staged speculative decoding. arXiv preprint
arXiv:2308.04623 , 2023.
9

--- PAGE 10 ---
Cascade Speculative Drafting for Even Faster LLM Inference
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
A., Babaei, Y ., Bashlykov, N., Batra, S., Bhargava, P.,
Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,
M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,
Fuller, B., Gao, C., Goswami, V ., Goyal, N., Hartshorn,
A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,
V ., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,
Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y .,
Mao, Y ., Martinet, X., Mihaylov, T., Mishra, P., Molybog,
I., Nie, Y ., Poulton, A., Reizenstein, J., Rungta, R., Saladi,
K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R.,
Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,
Xu, P., Yan, Z., Zarov, I., Zhang, Y ., Fan, A., Kambadur,
M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S.,
and Scialom, T. Llama 2: Open foundation and fine-tuned
chat models, 2023.
Treviso, M., Lee, J.-U., Ji, T., van Aken, B., Cao, Q., Ciosici,
M. R., Hassid, M., Heafield, K., Hooker, S., Raffel, C.,
Martins, P. H., Martins, A. F. T., Forde, J. Z., Milder, P.,
Simpson, E., Slonim, N., Dodge, J., Strubell, E., Balasub-
ramanian, N., Derczynski, L., Gurevych, I., and Schwartz,
R. Efficient methods for natural language processing: A
survey, 2023.
V oita, E., Talbot, D., Moiseev, F., Sennrich, R., and Titov, I.
Analyzing multi-head self-attention: Specialized heads
do the heavy lifting, the rest can be pruned, 2019.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,
Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thought
prompting elicits reasoning in large language models,
2023.
West, D. Combinatorial Mathematics . Cambridge Univer-
sity Press, 2021. ISBN 9781107058583.
Xia, H., Ge, T., Wang, P., Chen, S.-Q., Wei, F., and Sui,
Z. Speculative decoding: Exploiting speculative execu-
tion for accelerating seq2seq generation. In Findings of
the Association for Computational Linguistics: EMNLP
2023 , pp. 3909–3925, 2023.
Xia, M., Zhong, Z., and Chen, D. Structured pruning learns
compact and accurate models, 2022.
Zhang, J., Wang, J., Li, H., Shou, L., Chen, K., Chen, G.,
and Mehrotra, S. Draft & verify: Lossless large language
model acceleration via self-speculative decoding, 2023.
Zhou, Y ., Lyu, K., Rawat, A. S., Menon, A. K., Ros-
tamizadeh, A., Kumar, S., Kagy, J.-F., and Agarwal, R.
Distillspec: Improving speculative decoding via knowl-
edge distillation, 2023.
10

--- PAGE 11 ---
Cascade Speculative Drafting for Even Faster LLM Inference
A. Max-Gram Implementation
Listing 1. Max-Gram Algorithm
1def torch_index(t, value):
2 return (t == value).nonzero(as_tuple=True)[0][0]
3
4
5def max_gram(input_ids, encoder_ids, n=1):
6 matches = (encoder_ids[0] == input_ids[0, -1]).int()
7 if matches.sum() < 1:
8 return None
9 for i in range(2, input_ids.shape[-1] + 1):
10 new_matches = (encoder_ids[0, :(-1 *(i - 1))] == input_ids[0, -1 *i]).int()
11 combined_matches = (2 - new_matches == matches[1:]).int()
12 if combined_matches.sum() < 1:
13 index = torch_index(torch.cat(
14 (
15 torch.tensor([0] *(i - 1), device=torch.device(encoder_ids.device)),
16 matches
17 ),
18 dim=-1
19 ), 1)
20 return encoder_ids[:, index:index + n]
21 else:
22 matches = combined_matches
23 index = torch_index(torch.cat((
24 torch.tensor([0] *(encoder_ids.shape[-1] - matches.shape[-1])), matches), dim=-1
25 ), 1)
26 return encoder_ids[:, index+1:index + n+1]
B. Additonal Results
0 5 10 15 20 25 30
Token position0.10.20.30.40.50.60.7Acceptance rateAcceptance rate of draft tokens on MMLU dataset
FLAN-T5-small
FLAN-T5-base
FLAN-T5-large
Figure 3. The probability of acceptance of draft tokens in relation to their positions in a single step of speculative decoding, evaluated on
FLAN-T5- SMALL ,BASE , and LARGE models on MMLU.
11

--- PAGE 12 ---
Cascade Speculative Drafting for Even Faster LLM Inference
C. Proof
C.1. Proof for Theorem 4.1
Proof. The probability of accepting itokens is αi−αi+1, with the exception of the k+ 1-th token, which has a probability
ofαkof being accepted. This is because it requires all the first itokens to be accepted and the i+ 1-th token to be rejected
for this to happen. Therefore,
ϕ(α,k)(x) =αkxk+1+k−1X
i=0(αi−αi+1)xi+1. (3)
By rearranging the terms, we can achieve an expression much easier to work with
ϕ(α,k)(x) =x+kX
i=1αi(xi+1−xi) (4)
=x+ (x−1)kX
i=1αi(xi) (5)
=x+ (x−1)1−αi+1xi+1
1−αx. (6)
C.2. Proof for Theorem 4.3
Proof. Letα′=α(Md1,Md2). We first calculate the expected number of tokens being generated in a step of vertical
cascade with Md1,Md2. With the property of generating function, the coefficient of term xjofϕn(x)is the probability of
the sum of acceptance length of nspeculative step being j. Therefore, ϕn(x)represents the probability generating function
right before Mtperforms the generation.
To achieve the expected number of token acceptances of a probability-generating function, we seek for an operator that can
map the probability-generating function into the desired expectation.
To achieve the operator, we begin with a single polynomial term of xj. Fortunately, given the end result of1−αj+1
(1−α)(Leviathan
et al., 2023), the operator Tα(f(x)) =1−αf(α)
(1−α)will convert xjto1−αj+1
(1−α). In addition, due to the linearity of the operator,
this can be extended to any polynomial. Therefore, we achieved the desired operator to map a probability-generating
function into the desired expectation.
Apply operator Tαtoϕn(x), we achieved the result of1−αϕn(α)
(1−α)for the expected number of accepted tokens. Furthermore,
since the number of Md1calls is n, andMd2is called ktime for each Md1call given a total of nkcalls of Md2. The time
cost is 1 +ncd1+nkcd2which implied the EWIF of the system being1−αϕn(α)
(1−α)(1+ncd1+nkcd2).
For Corollary 4.4, since both 0< α < 1and0< α′<1, we have αi+1α′i+1< αα′, meaning that1−αk+1α′k+1
1−αα′<1.
Together with α−1<0, we have ϕα′,k(α)<1 + (α−1) = α. If we also let nkcd2= 0, we have1−αϕn(α)
(1−α)(1+ncd1+nkcd2)>
1−ααn
(1−α)(1+ncd1), which is the EWIF for speculative decoding with step size n.
12
