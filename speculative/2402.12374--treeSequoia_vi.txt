# 2402.12374.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/speculative/2402.12374.pdf
# Kích thước tệp: 1005015 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
/treeSequoia :
Giải mã suy đoán có thể mở rộng, mạnh mẽ và nhận biết phần cứng
Zhuoming Chen∗†, Avner May∗‡, Ruslan Svirschevski∗§,
Yuhsun Huang†, Max Ryabinin‡, Zhihao Jia†, và Beidi Chen†♯
†Đại học Carnegie Mellon
‡Together AI
§Yandex
♯Meta AI
{zhuominc,yuhsunh,zhihaoj2,beidic }@andrew.cmu.edu ,
{avner,mryab }@together.ai ,ruslansv@gmail.com
1 tháng 3, 2024

Tóm tắt
Khi việc sử dụng các mô hình ngôn ngữ lớn (LLM) ngày càng tăng, việc thực hiện suy luận hiệu quả với các mô hình này trở nên ngày càng quan trọng. Trong khi giải mã suy đoán gần đây đã nổi lên như một hướng đầy hứa hẹn để tăng tốc suy luận, các phương pháp hiện tại bị hạn chế trong khả năng mở rộng đến ngân sách suy đoán lớn hơn và thích ứng với các siêu tham số và phần cứng khác nhau. Bài báo này giới thiệu Sequoia, một thuật toán có thể mở rộng, mạnh mẽ và nhận biết phần cứng cho giải mã suy đoán. Để đạt được khả năng mở rộng tốt hơn, Sequoia giới thiệu một thuật toán quy hoạch động để tìm cấu trúc cây tối ưu cho các token được suy đoán. Để đạt được hiệu suất suy đoán mạnh mẽ, Sequoia sử dụng một phương pháp lấy mẫu và xác minh mới vượt trội hơn các công trình trước đó qua các nhiệt độ giải mã khác nhau. Cuối cùng, Sequoia giới thiệu một bộ tối ưu hóa cây nhận biết phần cứng tối đa hóa hiệu suất suy đoán bằng cách tự động chọn kích thước cây token và độ sâu cho một nền tảng phần cứng nhất định. Sequoia cải thiện tốc độ giải mã của Llama2-7B, Llama2-13B và Vicuna-33B trên GPU A100 lên đến 4.04×, 3.73× và 2.27×. Đối với thiết lập offloading trên L40, Sequoia đạt được độ trễ suy luận Llama2-70B chính xác thấp nhất là 0.56 s/token, nhanh hơn 9.96× trên hệ thống offloading được tối ưu hóa của chúng tôi (5.6 s/token), nhanh hơn 9.7× so với DeepSpeed-Zero-Inference [2], nhanh hơn 19.5× so với Huggingface Accelerate [16, 45]. Mã nguồn có sẵn tại https://github.com/Infini-AI-Lab/Sequoia.

1 Giới thiệu
Khi các mô hình ngôn ngữ lớn (LLM) được áp dụng rộng rãi [3,7,43], việc phục vụ hiệu quả các LLM này trở nên ngày càng quan trọng. Tuy nhiên, việc tăng tốc suy luận LLM là một thách thức vì việc tạo ra một token mới duy nhất đòi hỏi phải truy cập tất cả các tham số của LLM [34]. Kết quả của tắc nghẽn I/O này, phần cứng được sử dụng kém trong quá trình tạo. Vấn đề này trở nên trầm trọng hơn trong cả các thiết lập suy luận batch nhỏ và dựa trên offloading, nơi việc tạo một token mất nhiều thời gian như việc xử lý một prompt với hàng trăm hoặc hàng nghìn token trên các GPU hiện đại.

Để giải quyết thách thức này, các công trình gần đây đã giới thiệu giải mã suy đoán để tăng tốc suy luận LLM trong khi bảo toàn phân phối đầu ra của LLM [5,25,28,40]. Các phương pháp này tận dụng một hoặc nhiều mô hình nháp để dự đoán đầu ra của LLM; các dự đoán được tổ chức trong một cây token, có các nút đại diện cho các chuỗi token được suy đoán khác nhau. Tính đúng đắn của các token được suy đoán này sau đó được xác minh song song thông qua một lần forward pass duy nhất của LLM. Sử dụng cây token—thay vì chuỗi—có thể tăng số lượng token được LLM chấp nhận bằng cách cung cấp nhiều tùy chọn cho mỗi vị trí token.

∗Đóng góp bằng nhau.
1arXiv:2402.12374v2 [cs.CL] 29 Feb 2024

--- TRANG 2 ---
Token đầu vào Token được suy đoán
...
(b) Chuỗi đơn lẻ các token
(c) Cây k-ary đầy đủ các token (d) k chuỗi độc lập các token
(e) Cấu trúc liên kết cây tối ưu (Sequoia)
(a) Chuỗi đơn lẻ các token
L40 A100 H100 Offloading TươngLai

Hình 1: Sequoia là một phương pháp có thể mở rộng cho giải mã suy đoán. Trái: Thuật toán xây dựng cây Sequoia có thể tạo ra các cây có số lượng token được tạo trung bình (sau xác minh) tiếp tục tăng theo kích thước cây, trong khi các cấu trúc cây hiện tại có xu hướng tiệm cận. Điều này cho phép Sequoia hoạt động tốt hơn nhiều so với các phương pháp hiện tại trong các chế độ rất bị ràng buộc về bộ nhớ như offloading. Phải: Một hình ảnh hóa để so sánh cấu trúc cây Sequoia với các cấu trúc được thiết kế thủ công phổ biến khác.

Trong khi có nhiều nghiên cứu đáng kể về các phương pháp giải mã suy đoán dựa trên cây [28,40], chúng tôi thấy trong các thí nghiệm của mình rằng chúng có những hạn chế quan trọng. Đầu tiên, chúng tôi quan sát thấy rằng các thuật toán xây dựng cây token hiện tại hoạt động tốt cho các cây token nhỏ nhưng không tối ưu cho kích thước cây lớn. Ví dụ, SpecInfer xây dựng cây token sử dụng k chuỗi độc lập, một cấu trúc liên kết bị ràng buộc trong số lượng token dự kiến mà nó có thể chấp nhận, bất kể kích thước cây (Hình 1). Thứ hai, chúng tôi quan sát thấy rằng các thuật toán lấy mẫu và xác minh cây token hiện tại không thể hoạt động tốt qua các cấu hình siêu tham số suy luận; ví dụ, SpecInfer [28] và SpecTr [40] thường hoạt động kém ở nhiệt độ thấp (Hình 3), do thực tế là chúng có thể lặp đi lặp lại lấy mẫu một token không chính xác với xác suất mô hình nháp cao. Cuối cùng, chúng tôi quan sát thấy rằng các hệ thống hiện tại không thể tối ưu hóa hiệu quả—cho bất kỳ cấu hình phần cứng nào—kích thước và hình dạng của các cây được suy đoán của chúng (Bảng 3). Điều này là do các mô hình hiện tại mô tả tốc độ tăng từ giải mã suy đoán [25,40] giả định thời gian xác minh là hằng số, điều này không đúng đối với các cây được suy đoán lớn, do đó làm cho các mô hình này không hiệu quả trong việc chọn các kích thước cây tốt nhất.

Trong bài báo này, chúng tôi nhằm trả lời câu hỏi nghiên cứu sau: làm thế nào chúng ta có thể thiết kế một phương pháp giải mã suy đoán dựa trên cây tối ưu để tối đa hóa tốc độ tăng trên phần cứng hiện đại? Việc thực hiện mục tiêu này đòi hỏi giải quyết một số thách thức kỹ thuật. Đầu tiên, đối với bất kỳ kích thước và độ sâu cây nào, chúng ta phải có thể tìm kiếm hiệu quả không gian có kích thước mũ của các cấu trúc liên kết cây để tìm ra cấu trúc tối đa hóa số lượng token được tạo dự kiến. Thứ hai, chúng ta phải thiết kế một quy trình lấy mẫu và xác minh cây hoạt động tốt qua các siêu tham số suy luận, và tránh lặp đi lặp lại lấy mẫu các token không chính xác, trong khi duy trì phân phối đầu ra chính xác. Thứ ba, đối với bất kỳ phần cứng nào, chúng ta phải có thể chọn kích thước và độ sâu cây sẽ cung cấp tốc độ tăng lớn nhất, khi được ghép với cây tối ưu của những kích thước đó.

Bài báo này giới thiệu Sequoia, một thuật toán giải mã suy đoán có thể mở rộng, mạnh mẽ và nhận biết phần cứng. Như được hiển thị trong Hình 1, Sequoia có thể đạt được tốc độ tăng lên đến 10× so với giải mã tăng dần và giới thiệu một số kỹ thuật chính để giải quyết các thách thức nêu trên.

• Trong Phần 3.1, để giải quyết thách thức đầu tiên, chúng tôi xây dựng việc xây dựng cây như một bài toán tối ưu hóa có ràng buộc và sử dụng một thuật toán quy hoạch động để khám phá cây token suy đoán tối ưu. Chúng tôi chứng minh, về mặt lý thuyết và thực nghiệm, rằng số lượng token được tạo với cấu trúc cây này là không bị ràng buộc, tăng trưởng gần như logarithmic với kích thước của cây.

• Trong Phần 3.2, để giải quyết thách thức thứ hai, chúng tôi xây dựng trên thuật toán SpecInfer [28] bằng cách thực hiện lấy mẫu không thay thế từ mô hình nháp—do đó ngăn chặn mô hình nháp mắc cùng một lỗi hai lần, trong khi duy trì phân phối đầu ra của mô hình đích. Chúng tôi chứng minh rằng phương pháp lấy mẫu và xác minh mới này có thể đạt được tỷ lệ chấp nhận cao ở cả nhiệt độ cao và thấp, và xác nhận tuyên bố này bằng thực nghiệm.

2

--- TRANG 3 ---
• Trong Phần 3.3, để giải quyết thách thức cuối cùng, chúng tôi đề xuất một bộ tối ưu hóa cây nhận biết phần cứng, coi thời gian xác minh như một hàm phụ thuộc phần cứng của số lượng token được xác minh, và sử dụng hàm này để giải quyết cho hình dạng và độ sâu cây tối ưu. Chúng tôi cho thấy phương pháp này mang lại tốc độ tăng so với các phương pháp không nhận biết phần cứng.

Trong Phần 4, chúng tôi thực hiện các thí nghiệm end-to-end và các nghiên cứu ablation rộng rãi để chứng minh hiệu quả của Sequoia. Chúng tôi triển khai Sequoia trên Hugging Face (và Accelerate) [16,45] với CUDA Graphs [31,32]. Chúng tôi cho thấy Sequoia đạt được tốc độ tăng lên đến 4.04× cho Llama2-7B trên một GPU A100 duy nhất và 9.96× cho Llama2-70B trong thiết lập offloading trên GPU L40. Độ trễ của Llama2-70B offloading trên L40 có thể được giảm xuống 0.56 s/token với Sequoia trong khi tốc độ suy luận của hệ thống offloading hiện đại nhất (DeepSpeed-Zero-Inference [2]) là 5.5 s/token và 11 s/token cho API cpu offload Huggingface Accelerate [16, 45]. Chúng tôi cũng trình bày các nghiên cứu ablation để chỉ ra rằng: (1) cấu trúc cây Sequoia có thể tạo ra nhiều hơn đến 33% token mỗi bước giải mã so với k chuỗi độc lập (kích thước cây ≤512), chứng minh khả năng mở rộng tốt hơn; (2) thuật toán lấy mẫu và xác minh Sequoia mạnh mẽ với việc chọn siêu tham số (nhiệt độ, top-p), cung cấp tốc độ tăng lên đến 65% và 27% so với SpecInfer và thuật toán lấy mẫu và xác minh top-k, tương ứng; (3) bộ tối ưu hóa cây nhận biết phần cứng Sequoia có thể tự động chọn kích thước và độ sâu cây tốt nhất cho phần cứng khác nhau.

2 Kiến thức nền tảng
Ở đây, chúng tôi xem xét các phương pháp giải mã suy đoán dựa trên cây. Cụ thể, chúng tôi thảo luận về cách các phương pháp hiện tại chọn cấu trúc của cây suy đoán (Phần 2.1), các thuật toán mà chúng sử dụng để lấy mẫu và xác minh các cây token (Phần 2.2), và cách các phương pháp này có thể tự động chọn hình dạng của cây token (Phần 2.3).

2.1 Xây dựng cây
Cấu trúc cây chính được sử dụng bởi các phương pháp hiện tại là một cấu trúc được tạo thành từ k chuỗi độc lập có độ dài L phân nhánh từ gốc cây (tương ứng với tiền tố hiện tại [x1,x2,...,xn−1]). Bài báo SpecTr bổ sung xem xét các mẫu phân nhánh tùy ý (k1,k2,...,kt), nhưng nói rằng điều này không hoạt động tốt hơn trong các thí nghiệm của họ so với các chuỗi độc lập. Medusa xây dựng một cây k-ary đầy đủ, điều này tăng tỷ lệ thành công ở mỗi tầng nhưng không thể tạo thành một cây sâu dưới ngân sách token vừa phải [4].

2.2 Lấy mẫu và xác minh cây
Bây giờ chúng tôi xem xét cách SpecInfer [28], SpecTr [40], lấy mẫu ngây thơ [28], và lấy mẫu top-k¹ thực hiện lấy mẫu và xác minh cây token. Về mặt lấy mẫu, SpecInfer, SpecTr, và lấy mẫu ngây thơ đều thực hiện lấy mẫu i.i.d. có thay thế từ mô hình nháp, trong khi lấy mẫu top-k chọn k token có xác suất cao nhất từ mô hình nháp. Về mặt xác minh, SpecInfer và SpecTr so sánh xác suất mô hình nháp và mô hình đích cho các token được lấy mẫu để quyết định token nào (nếu có) sẽ chấp nhận; mặt khác, lấy mẫu ngây thơ và top-k lấy mẫu một token từ phân phối mô hình đích và chấp nhận nó nếu nó tương ứng với một trong các token từ cây được suy đoán. Tất cả các phương pháp này đều xác minh một cây token được suy đoán theo cách đệ quy—bắt đầu từ gốc của cây—chỉ khác nhau trong thuật toán xác minh mà chúng áp dụng tại mỗi nút.

SpecInfer: Phương pháp SpecInfer lặp đi lặp lại xác minh các token được lấy mẫu từ một hoặc nhiều mô hình nháp. Giống như phương pháp giải mã suy đoán gốc [25], nó so sánh xác suất mô hình nháp với những xác suất từ mô hình đích để quyết định có chấp nhận hay không. Lưu ý rằng trong khi phương pháp SpecInfer cho phép lấy mẫu từ k mô hình nháp khác nhau để tạo k con cho một nút, trong công việc này chúng tôi xem xét thiết lập phổ biến hơn nơi chỉ có một mô hình nháp khả dụng. Do đó, chúng tôi so sánh với phiên bản SpecInfer lấy mẫu từ một mô hình nháp duy nhất k lần thay thế. Chúng tôi trình bày mã giả cho SpecInfer trong Phụ lục B.2.

SpecTr: Thuật toán SpecTr tương tự về tinh thần với thuật toán SpecInfer. Nó lặp qua các con của một nút, và sử dụng một quy trình lấy mẫu để quyết định có chấp nhận một con hay không, theo cách mà phân phối đầu ra không thay đổi. Một tính chất quan trọng của thuật toán này là nó nằm trong một yếu tố (1 −1/e) của thuật toán xác minh tốt nhất có thể (có nghĩa là, thuật toán với tỷ lệ chấp nhận cao nhất có thể). Để ngắn gọn, chúng tôi tham khảo người đọc đến Thuật toán 3 trong bài báo SpecTr cho mã giả chính xác cho thuật toán này.

¹Lấy mẫu Top-k là một phiên bản cải tiến của lấy mẫu ngây thơ mà chúng tôi giới thiệu như một baseline trong công việc này.

3

--- TRANG 4 ---
100101102103104
Kích thước Cây123456# Token Được Tạo

Nháp: Pythia-410m, Đích: Pythia-12b (Nhiệt độ: 1.0)
Sequoia (của chúng tôi)
Chuỗi Độc lập
Chuỗi Đơn
Cây Nhị phân

100101102103104
Kích thước Cây13579# Token Được Tạo

Nháp: Pythia-2.8b, Đích: Pythia-12b (Nhiệt độ: 1.0)
Sequoia (của chúng tôi)
Chuỗi Độc lập
Chuỗi Đơn
Cây Nhị phân

100101102103104
Kích thước Cây13579111315# Token Được Tạo

Nháp: Llama2-7b, Đích: Llama2-70b (Nhiệt độ: 1.0)
Sequoia (của chúng tôi)
Chuỗi Độc lập
Chuỗi Đơn
Cây Nhị phân

Hình 2: Số token được tạo so với kích thước cây: Chúng tôi vẽ số lượng token trung bình được tạo cho các cấu trúc cây khác nhau mỗi bước giải mã của mô hình đích, như một hàm của kích thước cây, cho các cặp mô hình nháp và đích khác nhau. Số lượng token được tạo cho các cây Sequoia tiếp tục tăng theo kích thước cây, trong khi các cấu trúc cây khác có xu hướng tiệm cận.

Lấy mẫu ngây thơ và lấy mẫu top-k: Cho một nút trong cây token, thuật toán xác minh cho lấy mẫu ngây thơ và lấy mẫu top-k đầu tiên lấy mẫu từ phân phối P(·|x<n) của mô hình đích tại nút đó, và sau đó chấp nhận mẫu này nếu nó bằng một trong các con của nút đó. Thuật toán xác minh này duy trì phân phối đầu ra của mô hình đích một cách tầm thường—bất kể cách cây token được tạo ra—cho rằng người ta luôn lấy mẫu từ mô hình đích trong thuật toán này (trái ngược với từ mô hình nháp, như trong SpecTr và SpecInfer). Quan sát này thúc đẩy lựa chọn của chúng tôi—cho phương pháp lấy mẫu top-k—để điền cây bằng cách lấy k con hàng đầu của mỗi nút, thay vì phương pháp lấy mẫu ngây thơ lấy k mẫu i.i.d. (có thay thế). Chúng tôi sử dụng phương pháp lấy mẫu top-k trong các thí nghiệm của chúng tôi trong Phần 3.2, để hiểu rõ hơn về giới hạn của thuật toán xác minh này.

2.3 Bộ tối ưu hóa cây
Bài báo giải mã suy đoán gốc [25] đề xuất chọn số lượng token d để suy đoán bằng cách tối đa hóa biểu thức sau cho tốc độ tăng: Tốc độ tăng(n) = G(n)/(1+c·n). Ở đây, G(n) = (1−α^(n+1))/(1−α) là số lượng token được tạo dự kiến trong một lần lặp của giải mã suy đoán (nếu n token được suy đoán), c biểu thị thời gian để thực hiện một forward pass của mô hình nháp so với forward pass của mô hình đích, và 1 đại diện cho thời gian để xác minh cây token. Phương pháp này có thể được mở rộng dễ dàng cho thiết lập dựa trên cây, bằng cách thay thế G(n) bằng G(n,d), số lượng token được tạo dự kiến cho cây tốt nhất có kích thước n và độ sâu d. Điều quan trọng cần lưu ý là bất kể giá trị của n, thời gian xác minh được coi là hằng số O(1) bởi các công trình trước đó (ví dụ, mô hình tính toán trong SpecTr [40]).

3 Sequoia
Bây giờ chúng tôi trình bày Sequoia, một thuật toán giải mã suy đoán có thể mở rộng, mạnh mẽ và nhận biết phần cứng.

• Trong Phần 3.1, chúng tôi trình bày thuật toán xây dựng cây có thể mở rộng của chúng tôi, sử dụng quy hoạch động để giải quyết cho cấu trúc cây tối ưu. Chúng tôi chứng minh cả về mặt lý thuyết và thực nghiệm rằng số lượng token được tạo bởi việc xác minh các cây Sequoia tỷ lệ gần như logarithmic trong kích thước của cây, trong khi các cấu trúc cây hiện tại có xu hướng tiệm cận trong số lượng token mà chúng có thể tạo ra.

• Trong Phần 3.2, chúng tôi trình bày thuật toán xác minh cây mạnh mẽ của chúng tôi, sửa đổi thuật toán SpecInfer bằng cách lấy mẫu không thay thế từ mô hình nháp. Chúng tôi cho thấy cả về mặt lý thuyết và thực nghiệm rằng Sequoia mạnh mẽ, hoạt động tốt qua các giá trị nhiệt độ, trong khi các phương pháp xác minh hiện tại thì không.

• Trong Phần 3.3, chúng tôi trình bày cách chúng ta có thể làm cho Sequoia nhận biết phần cứng bằng cách tối ưu hóa kích thước và độ sâu của cây được suy đoán dựa trên phần cứng được sử dụng. Chúng tôi chứng minh về mặt lý thuyết và thực nghiệm rằng việc chọn các siêu tham số này theo cách này có thể mang lại sự gia tăng tốc độ so với việc chọn kích thước cây cố định.

3.1 Xây dựng cây
Bây giờ chúng tôi trình bày thuật toán xây dựng cây Sequoia, và chứng minh rằng số lượng token được tạo dự kiến bởi việc xác minh các cây Sequoia này tỷ lệ tốt với kích thước cây.

4

--- TRANG 5 ---
3.1.1 Thuật toán
Để dẫn xuất thuật toán xây dựng cây Sequoia, trước tiên chúng tôi biểu thị bài toán xây dựng cây như một bài toán tối ưu hóa có ràng buộc, và sau đó sử dụng quy hoạch động để giải quyết bài toán này một cách tối ưu và hiệu quả. Trong bài toán tối ưu hóa này, chúng tôi nhằm tối đa hóa số lượng token F(T) được tạo dự kiến bởi việc xác minh một cây token T, dưới một ràng buộc về kích thước của T. Chúng tôi bắt đầu bằng cách trình bày một biểu thức dạng đóng cho F(T) (Mệnh đề 3.4). Sau đó chúng tôi trình bày thuật toán xây dựng cây của chúng tôi, sử dụng quy hoạch động để tìm cây có kích thước n tối đa hóa biểu thức này (cho bất kỳ giá trị nào của ngân sách suy đoán n).

Chúng tôi đầu tiên trình bày một số định nghĩa quan trọng:

Định nghĩa 3.1. Dưới giả định chấp nhận vị trí, xác suất của một thuật toán xác minh chấp nhận một token t là con thứ k của một token đã được chấp nhận chỉ phụ thuộc vào giá trị của k.

Định nghĩa 3.2. Vector chấp nhận là vector p=(p1,p2,...,pk,...) chứa các xác suất pk rằng thuật toán xác minh chấp nhận một token tại vị trí con k. Dưới giả định chấp nhận vị trí, động lực chấp nhận của một thuật toán xác minh có thể được mô tả hoàn toàn bởi vector chấp nhận.

Định nghĩa 3.3. Cho một vector chấp nhận p và một cây T, chúng tôi định nghĩa hàm điểm f(v) cho một nút v∈T là f(v)=∏i∈Path(v) pi. nơi Path(v) bằng danh sách các chỉ số con dọc theo đường dẫn từ gốc đến một nút v∈T. Ví dụ, nếu v là con thứ 3 của con thứ 2 của gốc, thì Path(v)=[2,3]. Chúng tôi định nghĩa f(root)=1.

Bây giờ chúng tôi sẵn sàng trình bày Mệnh đề 3.4 (chứng minh trong Phụ lục C.2), cho thấy nghiệm dạng đóng cho số lượng token được tạo dự kiến bởi việc xác minh một cây token T, dưới giả định chấp nhận vị trí.

Mệnh đề 3.4. Cho T là một cây token được xác minh với giả định chấp nhận vị trí, và cho f(v) biểu thị hàm điểm cho một nút v∈T. Thì số lượng token F(T) được tạo dự kiến bởi việc xác minh T bằng

F(T)=∑v∈T f(v).

Thuật toán xây dựng cây Sequoia sau đó đơn giản tương ứng với việc tìm cây T có kích thước n tối đa hóa F(T), sử dụng quy hoạch động. Đặt

c(n)= max(T,|T|=n) F(T),

chúng ta có thể biểu thị c(n) theo các giá trị c(n′) cho n′<n:

c(n)= max(ai,∑(i=1 to n-1)ai=n-1) [1+∑(i=1 to n-1) pi·c(ai)].

Cấu trúc con đệ quy của bài toán tối ưu hóa này cho phép chúng ta giải quyết bài toán này bằng quy hoạch động. Chúng tôi lưu ý rằng vì thời gian để suy đoán cây token tỷ lệ với độ sâu của cây, nên việc có thể tìm cây T có kích thước n, và độ sâu tối đa d, tối đa hóa F(T) cũng quan trọng. Trong Phụ lục C.2, chúng tôi cung cấp chi tiết về các công thức quy hoạch động và nghiệm cho cả hai phiên bản của bài toán (độ sâu bị ràng buộc/không bị ràng buộc).

3.1.2 Kết quả lý thuyết
Bây giờ chúng tôi chứng minh rằng thuật toán xây dựng cây Sequoia tỷ lệ tốt với kích thước của cây được suy đoán. Cụ thể, chúng tôi cho thấy rằng dưới các giả định nhất định về tỷ lệ chấp nhận của thuật toán xác minh, số lượng token được tạo ra được giới hạn dưới bởi một hàm (gần như) logarithmic trong kích thước của cây. Điều này trái ngược với các thuật toán xây dựng cây hiện tại, mà chúng tôi cho thấy (Bảng 1) được giới hạn trên trong số lượng token dự kiến mà chúng tạo ra.

Chúng tôi đầu tiên định nghĩa ý nghĩa của việc một thuật toán xác minh có tỷ lệ chấp nhận power-law b, và sau đó trình bày định lý của chúng tôi về khả năng mở rộng của các cây Sequoia, dưới giả định rằng thuật toán xác minh có tỷ lệ chấp nhận power-law b.

Định nghĩa 3.5. Chúng ta nói rằng một thuật toán xác minh cây có tỷ lệ chấp nhận power-law b nếu cơ hội rk của thuật toán xác minh cây từ chối tất cả k con được suy đoán của một nút trong cây được giới hạn trên bởi một power-law của k với số mũ b—có nghĩa là, rk≤1/k^b ∀k∈N, cho b>0∈R.

5

--- TRANG 6 ---
Hình 3: Tỷ lệ từ chối so với số token được suy đoán: Chúng tôi vẽ tỷ lệ từ chối trung bình (1 − tỷ lệ chấp nhận) cho các thuật toán xác minh khác nhau, như một hàm của số token được suy đoán k. Chúng ta có thể thấy rằng qua các thiết lập nhiệt độ ({0.2,0.6,1.0}, từ trái sang phải), thuật toán xác minh Sequoia đạt được tỷ lệ từ chối thấp nhất, và nhất quán có tỷ lệ chấp nhận power-law (Định nghĩa 3.5).

Bảng 1: Chúng tôi trình bày giới hạn trên về số lượng token được tạo trong mỗi lần lặp của thuật toán giải mã suy đoán dựa trên cây, giả định rằng Pk biểu thị xác suất rằng nếu một nút có k con, thì một trong những con đó sẽ được chấp nhận.

| Cấu trúc cây | Giới hạn trên |
|---|---|
| Chuỗi | 1/(1−P1) |
| k chuỗi độc lập | (1+Pk)/(1−P1) |
| Cây nhị phân | 1/(1−P2) |
| Cây k-ary | 1/(1−Pk) |
| Sequoia (của chúng tôi) | ∞ |

Định nghĩa trên được thúc đẩy bởi quan sát của chúng tôi (Hình 3) rằng thuật toán lấy mẫu và xác minh Sequoia thỏa mãn tỷ lệ chấp nhận power-law trong thực tế. Bây giờ chúng tôi phát biểu định lý (chứng minh trong Phụ lục C.4).

Định lý 3.6. Giả sử một thuật toán xác minh cây có tỷ lệ chấp nhận power-law b. Thì số lượng token G(n) được tạo dự kiến bởi việc xác minh cây Sequoia có kích thước n với thuật toán này là trong Ω(b·log(n)/log(log(n))).

3.1.3 Xác thực thực nghiệm
Trong Hình 2, chúng tôi vẽ số lượng token trung bình được tạo bởi các cây Sequoia so với các cấu trúc cây khác nhau được trình bày trong Bảng 1, như một hàm của số lượng token n trong cây, cho một số cặp mô hình nháp và đích, sử dụng dữ liệu từ WikiText-103. Chúng ta thấy rằng số lượng token được tạo cho các cây Sequoia là không bị ràng buộc—tỷ lệ gần như logarithmic với kích thước cây—trong khi các cấu trúc cây khác có xu hướng tiệm cận.

3.2 Lấy mẫu và xác minh cây
Chúng tôi trình bày thuật toán lấy mẫu và xác minh cây token của chúng tôi, và chứng minh rằng đây là thuật toán đầu tiên thỏa mãn hai tính chất mạnh mẽ quan trọng, trong khi duy trì phân phối đầu ra của mô hình đích.

3.2.1 Thuật toán
Chúng tôi trình bày mã giả cho thuật toán lấy mẫu và xác minh Cây Sequoia trong Thuật toán 1. Như đã thảo luận trong Phần 2, một động lực quan trọng để thiết kế thuật toán xác minh Sequoia là quan sát rằng SpecInfer và SpecTr đều hoạt động kém ở nhiệt độ thấp, do thực tế là chúng có thể lặp đi lặp lại lấy mẫu (và sau đó từ chối) một token chất lượng thấp mà mô hình nháp tin tưởng. Do đó, chúng tôi muốn thiết kế một thuật toán không bao giờ mắc cùng một lỗi hai lần—có nghĩa là, một khi một token bị từ chối, nó sẽ không bao giờ đề xuất token đó nữa. Hướng tới mục tiêu này, Sequoia giới thiệu hai thay đổi đối với thuật toán SpecInfer: Đầu tiên, nó thực hiện lấy mẫu không thay thế sử dụng phân phối mô hình nháp. Thứ hai, nếu tất cả các token có xác suất mô hình nháp khác không đã được lấy mẫu và từ chối, nó sử dụng phân phối đồng đều trên tất cả các token chưa được lấy mẫu làm phân phối mô hình nháp mới. Những thay đổi này cải thiện đáng kể tính mạnh mẽ của Sequoia so với SpecInfer, trong khi duy trì đảm bảo rằng phân phối đầu ra giống hệt với mô hình đích (chứng minh trong Phụ lục C.1).

6

--- TRANG 7 ---
Thuật toán 1 Lấy mẫu và Xác minh Sequoia
1: Đầu vào: Tiền tố [x1,x2,...,xn−1], xác suất mô hình đích P(·|x<n), xác suất mô hình nháp Q(·|x<n), và số nhánh k≤ kích thước từ vựng.
2: Đầu ra: Một token x được lấy mẫu sử dụng Sequoia.
3: Khởi tạo phần dư R với P, nháp D với Q, và tập hợp các token bị từ chối S với ∅
4: for i=1→k do
5: lấy mẫu xi∼D, ri∼Uniform(0,1)
6: if ri<R[xi]/D[xi] then ▷ Chấp nhận xi
7: Return xi
8: else ▷ Từ chối xi
9: R←norm(max(R−D,0))
10: D[xi]←0
11: S.add(xi)
12: if sum(D)=0 then
13: # Cho D đồng đều trên tập hợp không bị từ chối
14: D[t]←0 nếu t∈S, ngược lại 1
15: end if
16: D←norm(D)
17: end if
18: end for
19: Return x∼R

3.2.2 Kết quả lý thuyết
Bây giờ chúng tôi chứng minh rằng thuật toán xác minh Sequoia mạnh mẽ, theo nghĩa là nó thỏa mãn cả hai tính chất dưới đây, trong khi các thuật toán xác minh hiện tại thì không.

• Tính chất vận chuyển tối ưu: Khi k=1, tỷ lệ chấp nhận bằng 1 − ∥P−Q∥1/2.²
• Tính chất bao phủ: Nếu hỗ trợ của phân phối xác suất mô hình nháp Q có kích thước k và là tập cha của hỗ trợ của phân phối xác suất mô hình đích P, tối đa k suy đoán sẽ cần thiết để đạt được tỷ lệ chấp nhận là 1. Hơn nữa, nếu k bằng kích thước từ vựng, tỷ lệ chấp nhận cũng phải luôn là 1, bất kể mô hình nháp được sử dụng.

Trực quan, việc thỏa mãn tính chất vận chuyển tối ưu dẫn đến hiệu suất mạnh ở nhiệt độ cao (vì P và Q sẽ tiếp cận phân phối đồng đều), trong khi việc thỏa mãn tính chất bao phủ dẫn đến hiệu suất mạnh ở nhiệt độ thấp (giả sử token mô hình đích hàng đầu nằm trong k token mô hình nháp hàng đầu).

Bây giờ chúng tôi trình bày kết quả mạnh mẽ chính của chúng tôi (chứng minh trong Phụ lục C.4):

Định lý 3.7. Thuật toán xác minh Sequoia thỏa mãn cả tính chất vận chuyển tối ưu và tính chất bao phủ, trong khi SpecInfer và SpecTr chỉ thỏa mãn tính chất vận chuyển tối ưu, và lấy mẫu top-k chỉ thỏa mãn tính chất bao phủ.

3.2.3 Xác thực thực nghiệm
Trong Hình 3, chúng tôi vẽ tỷ lệ từ chối trung bình (bằng 1 − tỷ lệ chấp nhận) cho các thuật toán xác minh khác nhau, như một hàm của số token con được suy đoán cho một tiền tố token cố định, cho các nhiệt độ khác nhau (0.2, 0.6, 1.0), được đo trên WikiText-103. Chúng ta có thể thấy rằng qua tất cả các thiết lập nhiệt độ, tỷ lệ từ chối cho Sequoia giảm nhanh hơn so với các thuật toán khác. Nói chung, chúng tôi quan sát thấy rằng tỷ lệ từ chối rk cho Sequoia tuân theo power-law, nơi rk≈1/k^b cho một số b>0. Chúng ta cũng có thể thấy rằng trong khi SpecTr và SpecInfer hoạt động tương đối tốt ở nhiệt độ cao, chúng gặp khó khăn ở nhiệt độ thấp hơn, và điều ngược lại đúng cho phương pháp lấy mẫu top-k.

²Bài báo SpecTr [40] đã chỉ ra rằng 1 − ∥P−Q∥1/2 là tỷ lệ chấp nhận được đạt bởi thuật toán xác minh tối ưu cho k=1.

7

--- TRANG 8 ---
Bảng 2: Các thành phần của phương trình tốc độ tăng (Phương trình 1) cho Sequoia, cũng như cho các công trình trước đó.

| | Sequoia | 1 chuỗi | k chuỗi độc lập |
|---|---|---|---|
| G(n,d) | Ω(b·log(n)/log(log(n))) | (1−P^(n+1))/(1−P1) | ≤1+(1)/(1−Pk) |
| t(n) | Ω(n) | 1 | 1 |
| d(n) | d | n | ⌊n/k⌋ |

Bảng 3: Tổng tốc độ tăng (số token được tạo trong ngoặc đơn) trên phần cứng khác nhau. Chúng tôi so sánh bộ tối ưu hóa cây nhận biết phần cứng của chúng tôi với việc sử dụng kích thước cây cố định. Bộ tối ưu hóa của chúng tôi có thể mang lại tốc độ tăng lên đến 38%.

| Phần cứng | Đích | Nháp | Nhận biết phần cứng | n cố định=128 | n cố định=256 |
|---|---|---|---|---|---|
| L40 | Llama2-13B | JF68M | 3.26×(4.40) | 3.16×(4.77) | 2.51×(5.09) |
| A100 | Vicuna-33B | SL1.3B | 2.37×(4.41) | 2.09×(4.99) | 1.72×(5.30) |

3.3 Bộ tối ưu hóa cây nhận biết phần cứng
Chúng tôi trình bày phương pháp đề xuất của chúng tôi để chọn kích thước và độ sâu cây Sequoia theo cách nhận biết phần cứng để tối ưu hóa tốc độ tăng thực tế được đạt bởi Sequoia trên phần cứng khác nhau. Về mặt lý thuyết, chúng tôi cho thấy rằng trong khi các công trình trước có thể thoát khỏi việc tối ưu hóa kích thước của ngân sách suy đoán theo cách không nhận biết phần cứng, điều này không khả thi trong Sequoia, do thực tế là đối với các cây lớn, thời gian xác minh không còn có thể được xấp xỉ như một hằng số, độc lập với kích thước cây. Về mặt thực nghiệm, chúng tôi chứng minh rằng việc chọn kích thước và độ sâu cây theo cách nhận biết phần cứng này mang lại tốc độ tăng lên đến 40% so với các baseline.

3.3.1 Thuật toán
Bây giờ chúng tôi cho thấy cách chúng ta có thể chọn kích thước và độ sâu cây một cách tối ưu, tùy thuộc vào phần cứng được sử dụng cho suy luận. Đặt G(n,d) biểu thị số lượng token được tạo dự kiến bởi việc xác minh cây Sequoia có kích thước n và độ sâu d (được tính qua quy hoạch động), t(n) biểu thị lượng thời gian (phụ thuộc phần cứng) mà mô hình đích cần để xác minh n token chia cho thời gian để xác minh 1 token, và c biểu thị thời gian (phụ thuộc phần cứng) để nháp 1 token chia cho thời gian để xác minh 1 token, tốc độ tăng được đạt bởi Sequoia có thể được biểu thị như:

Tốc độ tăng(n,d) = G(n,d)/(t(n)+d·c). (1)

Chúng tôi đề xuất tối ưu hóa phương trình này bằng cách đo t(n) và c một cách thực nghiệm trên phần cứng suy luận được sử dụng, và sau đó thực hiện tìm kiếm lưới trên các giá trị có thể của n và d để tìm sự kết hợp mang lại tốc độ tăng lớn nhất. Quan trọng, đối với kích thước batch b>1, mẫu số có thể được cập nhật thành t(b·n)+d·c, vì trình xác minh phải xác minh một cây có kích thước n cho mỗi phần tử của batch.

3.3.2 Kết quả lý thuyết

[THIS IS FIGURE: Biểu đồ thời gian forward pass cho các kết hợp mô hình/phần cứng khác nhau như một hàm của số token n được xử lý. Chúng tôi sử dụng các giá trị này để chọn cây tối ưu.]

Bây giờ chúng tôi nghiên cứu các tính chất của phương trình tốc độ tăng Sequoia, để hiểu rõ hơn kích thước và độ sâu cây tối ưu được chọn bởi phương trình này, và tại sao việc chọn các tham số này theo cách nhận biết phần cứng là quan trọng.

Trong Bảng 2, chúng tôi phân tích phương trình tốc độ tăng theo các thành phần của nó, cho Sequoia và các công trình trước đó. Chúng tôi đầu tiên quan sát thấy rằng đối với Sequoia, vì tử số cho phương trình tốc độ tăng tăng trưởng gần như logarithmic trong n, trong khi mẫu số tăng trưởng tuyến tính với n, phải tồn tại một kích thước cây tối ưu n<∞ tối đa hóa biểu thức tốc độ tăng này.

Bổ sung, chúng ta có thể quan sát thấy rằng các công trình trước có thể thoát khỏi việc đặt t(n)=1, vì sự lựa chọn tối ưu của n thường

8

--- TRANG 9 ---
xảy ra trước khi t(n) bắt đầu tăng trưởng có ý nghĩa lớn hơn 1. Tuy nhiên, giả định này không phải lúc nào cũng đúng với Sequoia. Trong Sequoia, thực tế là chúng tôi đã cải thiện G(n,d) để nó tăng trưởng (hơi dưới-) logarithmic trong n, đòi hỏi mô hình hóa t(n) nhận biết phần cứng.

3.3.3 Xác thực thực nghiệm
Trong Bảng 3, chúng tôi cho thấy tốc độ tăng được đạt trên một số loại phần cứng khi chúng tôi chọn kích thước và độ sâu cây sử dụng hàm t(n) chính xác (ước tính thực nghiệm, xem Hình 4) so với đơn giản chọn một giá trị cố định của n. Chúng ta có thể thấy rằng sử dụng hàm t(n) nhận biết phần cứng có thể mang lại tốc độ tăng có ý nghĩa lên đến 40% so với sử dụng các giá trị cố định của n qua phần cứng khác nhau.

4 Đánh giá
Trong phần này, chúng tôi nhằm chứng minh rằng Sequoia có thể tăng tốc suy luận LLM bằng một biên độ lớn trong thời gian wall-clock. Chúng tôi đầu tiên trình bày kết quả hệ thống end-to-end của chúng tôi cho thấy tổng tốc độ tăng, tiếp theo là xác thực ba tuyên bố của chúng tôi rằng Sequoia có thể mở rộng, mạnh mẽ và nhận biết phần cứng.

• Trong Phần 4.1, chúng tôi chứng minh hiệu suất end-to-end vượt trội của Sequoia. Cụ thể, Sequoia đạt được tốc độ tăng lên đến 4.04× cho Llama2-7B trên A100 và 9.96× cho Llama2-70B trên L40 offloading (đạt được độ trễ thấp nhất là 0.56 s/token).

• Trong Phần 4.2.1, chúng tôi cho thấy rằng cây Sequoia có thể tạo ra trung bình 33% token nhiều hơn so với cây của 16 chuỗi độc lập (kích thước cây 512).

• Trong Phần 4.2.2, chúng tôi trình bày rằng thuật toán lấy mẫu và xác minh của Sequoia mạnh mẽ với nhiệt độ và top-p, nhất quán vượt trội hơn SpecInfer (lên đến 1.65×) và lấy mẫu top-k (lên đến 1.27×).

• Trong Phần 4.2.3, chúng tôi cho thấy rằng bộ tối ưu hóa cây nhận biết phần cứng của Sequoia có thể chọn kích thước và độ sâu cây tốt nhất cho các thiết lập phần cứng khác nhau để tối đa hóa tốc độ tăng.

4.1 Kết quả End-to-end
Bây giờ chúng tôi chứng minh rằng Sequoia tăng tốc giải mã LLM trong thiết lập on-chip lên đến 4.04× trên GPU A100, và lên đến 9.96× với offloading trên GPU L40.

Thiết lập. Các thí nghiệm của chúng tôi dựa trên các mô hình Llama và Vicuna. Đối với thiết lập on-chip, chúng tôi sử dụng JackFram/Llama-68m (JF68m) [28] và princeton-nlp/Sheared-Llama-1.3B (SL1.3B) [46] làm mô hình nháp, và Llama2-7B [43], Llama2-13B, và Vicuna-33B [6] làm mô hình đích. Đối với thiết lập offloading, chúng tôi sử dụng Llama2-7B làm mô hình nháp và Llama2-70B làm mô hình đích. Chúng tôi đánh giá kết quả của chúng tôi trên tập dữ liệu validation C4(en) [35], OpenWebText [13] và CNN DailyMail [36]. Đối với mỗi thí nghiệm, chúng tôi sử dụng 200 ví dụ để đo vector tỷ lệ chấp nhận (được đề cập trong Phần 3.1) và lấy mẫu 200 ví dụ khác để đánh giá (100 cho offloading). Độ dài prompt và độ dài tạo đều được đặt thành 128 token. Chúng tôi đánh giá Sequoia trên phần cứng khác nhau bao gồm các thí nghiệm on-chip trên GPU L40 và A100, cũng như các thí nghiệm offloading trên GPU L40. Chúng tôi cũng so sánh Sequoia với SpecInfer [28] với cây 5×8 (5 chuỗi độc lập có độ dài 8, cấu trúc cây được sử dụng trong [28] cho kích thước batch 1) và cây 8×8 cho thiết lập on-chip, và cây 16×48 cho thiết lập offloading.

Chi tiết triển khai. Chúng tôi triển khai các mô hình nháp và đích sử dụng Transformers [45]. Vì chúng tôi xác định cấu trúc cây tối ưu trước, chúng tôi có thể sử dụng PyTorch CUDA graphs [31,32] để giảm overhead của việc khởi chạy kernel trong quá trình giải mã suy đoán. Để tăng tốc lấy mẫu không thay thế—không hiệu quả trong PyTorch 2.1 [32]—chúng tôi sử dụng thuật toán exponential-sort [44], kết hợp với PyTorch CUDA graphs [31,32]. Đối với thiết lập offloading, chúng tôi triển khai một hệ thống offloading làm baseline, cũng có thể hỗ trợ xác minh cây cho giải mã suy đoán. Độ trễ của hệ thống offloading của chúng tôi là 5.6 s/token, phù hợp với tốc độ suy luận của hiện đại nhất (DeepSpeed-Zero-Inference [2]), là 5.5 s/token.

9

--- TRANG 10 ---
Bảng 4: Kết quả on-chip (A100): Cấu hình cây tối ưu và tốc độ tăng cho các cặp mô hình nháp và đích khác nhau, và các nhiệt độ khác nhau, cho Sequoia so với SpecInfer. Chúng tôi chỉ định số lượng token được tạo trung bình mỗi bước giải mã trong ngoặc đơn, bên cạnh hệ số tốc độ tăng. Sequoia đạt được tốc độ tăng lên đến 4.04× trên A100.

| LLM Đích | Mô hình Nháp | T | Tập dữ liệu | Cấu hình Cây (kích thước, độ sâu) | Tốc độ tăng | SpecInfer 5×8 | SpecInfer 8×8 |
|---|---|---|---|---|---|---|---|
| Llama2-7B | JF68M | 0 | C4 | (128,10) | 4.04×(5.08) | 3.45×(3.96) | 3.70×(4.11) |
| Llama2-7B | JF68M | 0.6 | C4 | (128,7) | 3.18×(3.92) | 2.47×(2.97) | 2.45×(3.05) |
| Llama2-7B | JF68M | 0 | OpenWebText | (128,7) | 3.22×(3.86) | 2.79×(3.15) | 2.96×(3.24) |
| Llama2-7B | JF68M | 0.6 | OpenWebText | (128,6) | 2.71×(3.33) | 2.10×(2.54) | 2.08×(2.55) |
| Llama2-7B | JF68M | 0 | CNN Daily | (128,7) | 3.41×(4.05) | 2.95×(3.27) | 3.10×(3.37) |
| Llama2-7B | JF68M | 0.6 | CNN Daily | (128,6) | 2.83×(3.45) | 2.11×(2.58) | 2.22×(2.69) |
| Llama2-13B | JF68M | 0 | C4 | (64,9) | 3.73×(4.20) | 3.30×(3.64) | 3.10×(3.75) |
| Llama2-13B | JF68M | 0.6 | C4 | (64,7) | 3.19×(3.57) | 2.48×(2.87) | 2.42×(3.00) |
| Llama2-13B | JF68M | 0 | OpenWebText | (64,7) | 3.18×(3.49) | 2.77×(3.05) | 2.59×(3.14) |
| Llama2-13B | JF68M | 0.6 | OpenWebText | (64,6) | 2.77×(3.06) | 2.17×(2.49) | 2.01×(2.52) |
| Llama2-13B | JF68M | 0 | CNN Daily | (64,7) | 3.33×(3.68) | 2.95×(3.22) | 2.75×(3.32) |
| Llama2-13B | JF68M | 0.6 | CNN Daily | (64,6) | 2.88×(3.17) | 2.17×(2.54) | 2.09×(2.60) |
| Llama2-13B | JF160M | 0 | C4 | (64,7) | 3.10×(4.69) | 2.74×(4.33) | 2.58×(4.42) |
| Llama2-13B | JF160M | 0.6 | C4 | (64,6) | 2.83×(4.06) | 2.07×(3.46) | 2.02×(3.53) |
| Llama2-13B | JF160M | 0 | OpenWebText | (64,6) | 2.72×(3.90) | 2.26×(3.58) | 2.15×(3.66) |
| Llama2-13B | JF160M | 0.6 | OpenWebText | (64,5) | 2.49×(3.38) | 1.80×(2.96) | 1.77×(3.07) |
| Llama2-13B | JF160M | 0 | CNN Daily | (64,6) | 2.84×(4.05) | 2.36×(3.73) | 2.25×(3.83) |
| Llama2-13B | JF160M | 0.6 | CNN Daily | (64,5) | 2.55×(3.47) | 1.79×(2.97) | 1.74×(3.03) |
| Vicuna-33B | SL1.3B | 0 | C4 | (64,6) | 2.27×(4.28) | 1.83×(3.86) | 1.73×(3.96) |
| Vicuna-33B | SL1.3B | 0.6 | C4 | (64,6) | 2.19×(4.16) | 1.64×(3.53) | 1.52×(3.56) |
| Vicuna-33B | SL1.3B | 0 | OpenWebText | (64,5) | 2.21×(3.93) | 1.75×(3.70) | 1.65×(3.79) |
| Vicuna-33B | SL1.3B | 0.6 | OpenWebText | (64,5) | 2.13×(3.82) | 1.57×(3.36) | 1.47×(3.43) |
| Vicuna-33B | SL1.3B | 0 | CNN Daily | (64,5) | 2.21×(3.93) | 1.75×(3.71) | 1.65×(3.79) |
| Vicuna-33B | SL1.3B | 0.6 | CNN Daily | (64,5) | 2.16×(3.86) | 1.58×(3.40) | 1.46×(3.43) |

Bảng 5: Kết quả offloading (L40): Cấu hình cây tối ưu và tốc độ tăng cho các cặp mô hình nháp và đích khác nhau, và các nhiệt độ khác nhau, cho Sequoia so với SpecInfer. Chúng tôi chỉ định số lượng token được tạo trung bình mỗi bước giải mã trong ngoặc đơn, bên cạnh hệ số tốc độ tăng. Sequoia đạt được tốc độ tăng lên đến 9.96× trong thiết lập offloading trên L40.

| LLM Đích | Mô hình Nháp | T | Tập dữ liệu | Cấu hình Cây (kích thước, độ sâu) | Tốc độ tăng | Độ trễ E2E s/token | SpecInfer 16×48 |
|---|---|---|---|---|---|---|---|
| Llama2-70B | Llama2-7B | 0 | C4 | (768,22) | 9.96×(12.18) | 0.56 | 6.46×(8.66) |
| Llama2-70B | Llama2-7B | 0.6 | C4 | (768,23) | 8.26×(10.12) | 0.69 | 5.20×(6.93) |
| Llama2-70B | Llama2-7B | 0 | OpenWebText | (768,18) | 8.14×(9.83) | 0.69 | 5.50×(7.36) |
| Llama2-70B | Llama2-7B | 0.6 | OpenWebText | (768,19) | 7.39×(9.05) | 0.76 | 4.64×(6.18) |
| Llama2-70B | Llama2-7B | 0 | CNN Daily | (768,17) | 8.78×(10.46) | 0.64 | 5.91×(7.87) |
| Llama2-70B | Llama2-7B | 0.6 | CNN Daily | (768,18) | 8.03×(9.58) | 0.70 | 4.68×(6.24) |

Kết quả chính. Chúng tôi đánh giá Sequoia sử dụng các nhiệt độ khác nhau, cặp mô hình nháp và đích, và cấu hình phần cứng. Kết quả được hiển thị trong Bảng 4 (A100 on-chip) và Bảng 5 (L40 offloading). Chúng tôi quan sát thấy rằng Sequoia nhất quán tăng tốc giải mã LLM trong một phạm vi rộng các thiết lập. Sequoia đạt được tốc độ tăng lên đến 4.04× cho thiết lập on-chip, và lên đến 9.96× tốc độ tăng cho thiết lập offloading, kết quả của khoảng cách lớn giữa khả năng tính toán và băng thông bộ nhớ. Đáng chú ý, đối với thiết lập offloading trên L40, Sequoia có thể đạt được độ trễ thấp nhất là 0.56 s/token. Chúng tôi trình bày kết quả on-chip bổ sung (GPU L40) trong Phụ lục D.

Phân tích. Chúng tôi đã có một số quan sát thú vị về sự tương tác giữa xây dựng cây Sequoia, lấy mẫu và xác minh, và bộ tối ưu hóa nhận biết phần cứng. (1) Sequoia chọn các cây lớn hơn nhiều trong thiết lập offloading (768 token) so với trong thiết lập on-chip (64 đến 128 token). (2) Nói chung, số lượng token được tạo trung bình gần với tốc độ tăng thời gian wall-clock (đặc biệt khi JF68M được sử dụng làm nháp) kết quả của bộ tối ưu hóa cây nhận biết phần cứng. (3) Các cây tối ưu được tìm thấy bởi Sequoia cho các cấu hình hơi khác nhau—ví dụ, các nhiệt độ và cặp mô hình khác nhau—có thể rất khác nhau với nhau. (4) Sequoia chọn các cây sâu hơn ở nhiệt độ thấp so với nhiệt độ cao, do tỷ lệ chấp nhận cao hơn cho nhiệt độ thấp. (5) Kích thước cây tối ưu cho các mô hình lớn hơn như 33B hoặc 13B nói chung nhỏ hơn so với các mô hình nhỏ hơn vì thời gian xác minh tăng nhanh hơn với số lượng token ứng cử viên, như được minh họa trong Hình 4.

4.2 Ablation
Chúng tôi trình bày các thí nghiệm ablation của chúng tôi xác thực khả năng mở rộng của thuật toán xây dựng cây Sequoia (Phần 4.2.1), tính mạnh mẽ của thuật toán lấy mẫu và xác minh cây Sequoia (Phần 4.2.2), và nhận biết phần cứng của bộ tối ưu hóa cây Sequoia (Phần 4.2.3). Đối với mỗi thí nghiệm này, chúng tôi chỉ thay đổi một yếu tố tại một thời điểm (ví dụ, cấu trúc cây cho Phần 4.2.1) để nghiên cứu lợi ích được đạt bởi mỗi thành phần của Sequoia.

4.2.1 Khả năng mở rộng của Sequoia
Trong phần này, chúng tôi đánh giá khả năng mở rộng của phương pháp xây dựng cây Sequoia về số lượng token được tạo trung bình ở các kích thước ngân sách khác nhau, so với các baseline. Trong Hình 5a, chúng tôi so sánh cây Sequoia với k chuỗi độc lập, nơi chúng tôi sử dụng thuật toán lấy mẫu và xác minh của Sequoia cho cả hai cấu trúc cây. Cây Sequoia có thể tạo ra nhiều hơn đến 33% token mỗi bước giải mã, chứng minh hiệu quả của thuật toán xây dựng cây Sequoia. Đối với những thí nghiệm này, chúng tôi sử dụng JackFram/Llama-68m làm mô hình nháp, Llama2-13B làm mô hình đích, 0.6 làm nhiệt độ, và CNN Daily Mail làm tập dữ liệu.

4.2.2 Tính mạnh mẽ của thuật toán lấy mẫu Sequoia
Ở đây, chúng tôi so sánh thuật toán lấy mẫu và xác minh Sequoia với SpecInfer và lấy mẫu top-k qua các siêu tham số suy luận khác nhau (nhiệt độ và top-p [20]), giữ cấu trúc cây cố định. Trong Hình 5b, chúng tôi trình bày kết quả cho các phương pháp này khi chúng tôi thay đổi nhiệt độ (cho top-p=1), trong khi trong Bảng 6 chúng tôi trình bày

11

--- TRANG 11 ---
Bảng 6: Chúng tôi so sánh tính mạnh mẽ của thuật toán lấy mẫu và xác minh Sequoia với siêu tham số top-p, so với SpecInfer và lấy mẫu top-k. Chúng tôi trình bày tổng tốc độ tăng trên GPU A100 cho các phương pháp khác nhau (số token được tạo trong ngoặc đơn). Chúng tôi giữ cấu trúc cây cố định qua các phương pháp, sử dụng JF68M làm mô hình nháp, và Llama2-7B làm mô hình đích.

| Top-p | Sequoia (Của chúng tôi) | SpecInfer | lấy mẫu top-k |
|---|---|---|---|
| 0.8 | 2.54×(3.18) | 2.35×(2.93) | 2.43×(2.90) |
| 0.9 | 2.61×(3.27) | 2.42×(3.01) | 2.27×(2.71) |
| 1.0 | 2.69×(3.26) | 2.55×(3.10) | 2.12×(2.44) |

kết quả thay đổi tham số top-p (cho temp=1). Chúng ta có thể thấy rằng Sequoia đạt được tốc độ tăng lớn nhất qua tất cả các thiết lập nhiệt độ và top-p, đạt được tốc độ tăng lên đến 1.65× và 1.27× so với SpecInfer và lấy mẫu top-k, tương ứng. Bổ sung, chúng ta có thể quan sát thấy rằng thuật toán lấy mẫu và xác minh Sequoia hoạt động tốt qua các thiết lập nhiệt độ và top-p khác nhau, trong khi lấy mẫu top-k và SpecInfer xuất sắc trong các chế độ khác nhau³. Đối với những thí nghiệm này, chúng tôi sử dụng JackFram/Llama-68m làm mô hình nháp, Llama2-7B làm mô hình đích, 0.6 làm nhiệt độ, CNN Daily Mail làm tập dữ liệu, và cây Sequoia tương ứng từ Bảng 4 làm cấu trúc cây.

4.2.3 Sequoia trên các thiết lập phần cứng khác nhau
Trong phần này, chúng tôi chứng minh hiệu quả của bộ tối ưu hóa cây nhận biết phần cứng Sequoia. Trong Hình 5c, chúng tôi so sánh tốc độ tăng được đạt bởi các cây Sequoia có kích thước khác nhau từ Hình 5a với các cây được chọn bởi bộ tối ưu hóa cây nhận biết phần cứng. Vì bộ tối ưu hóa cây có thể hạn chế độ sâu cây để làm cho suy đoán nhanh hơn, nó có thể đạt được tốc độ tăng end-to-end lớn hơn so với bất kỳ cây Sequoia nào từ Hình 5a, có cấu trúc được chọn để tối đa hóa số lượng token được tạo dự kiến (không phải tốc độ tăng). Bộ tối ưu hóa cũng có thể tự động tìm kích thước cây tạo ra tốc độ tăng tổng thể lớn nhất.

5 Kết luận
Chúng tôi đã trình bày Sequoia, một phương pháp giải mã suy đoán có thể mở rộng, mạnh mẽ và nhận biết phần cứng. Bằng cách cải thiện cấu trúc liên kết của cây token, các thuật toán lấy mẫu, và sự lựa chọn kích thước cây, Sequoia có thể tăng tốc suy luận LLM tự hồi quy lên đến 4.04× trên GPU và 9.96× với offloading. Bổ sung để cung cấp tốc độ tăng thực, chúng tôi tin rằng Sequoia cũng cung cấp hiểu biết về cả tiềm năng lớn và giới hạn cơ bản của các hệ thống giải mã suy đoán. Chúng tôi hy vọng rằng sự hiểu biết này sẽ truyền cảm hứng cho công việc tương lai trong lĩnh vực này, hoặc thậm chí thông báo thiết kế của các chip tùy chỉnh cho suy luận LLM.

Tài liệu tham khảo
[1] Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Ontañón, Siddhartha Brahma, Yury Zemlyanskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, et al. Colt5: Faster long-range transformers with conditional computation. arXiv preprint arXiv:2303.09752, 2023.

[2] Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, et al. Deepspeed inference: Enabling efficient inference of transformer models at unprecedented scale. arXiv preprint arXiv:2207.00032, 2022.

[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

[4] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads, 2024.

³Top-p không được hỗ trợ hiệu quả trong việc triển khai hiện tại, điều này ảnh hưởng đến tổng tốc độ tăng.

12

--- TRANG 12 ---
4 8 16 32 64 128 256 512
Kích thước Cây1.52.02.53.03.54.0# Token Được Tạo

Khả năng mở rộng của Sequoia
Cây Sequoia
4 chuỗi
8 chuỗi
16 chuỗi (a)

0.0 0.2 0.4 0.6 0.8 1.0
Nhiệt độ2.02.252.52.753.0Tốc độ tăng

Tính mạnh mẽ của Sequoia
Sequoia
SpecInfer
Lấy mẫu Top-k (b)

4 8 16 32 64 128 256 512
Kích thước Cây1.001.251.501.752.002.252.502.753.00Tốc độ tăng

Nhận biết phần cứng của Sequoia
Cây tối ưu hóa Sequoia (A100)
Sequoia w/o bộ tối ưu hóa cây (A100)
Cây tối ưu hóa Sequoia (L40)
Sequoia w/o bộ tối ưu hóa cây (L40) (c)

Hình 5: Trái: Chúng tôi so sánh số lượng token được tạo trung bình bởi các cây Sequoia so với k chuỗi độc lập, nơi chúng tôi sử dụng lấy mẫu và xác minh Sequoia cho cả hai cấu trúc cây. Khoảng cách giữa các cây Sequoia và các baseline chứng minh khả năng mở rộng được cải thiện của thuật toán xây dựng cây Sequoia. Giữa: Chúng tôi so sánh tốc độ tăng được đạt bởi thuật toán lấy mẫu và xác minh Sequoia so với SpecInfer và lấy mẫu top-k, qua các nhiệt độ khác nhau, giữ cấu trúc cây cố định. Chúng ta có thể thấy Sequoia mạnh mẽ với sự lựa chọn nhiệt độ, hoạt động tốt qua các thiết lập khác nhau. Phải: Chúng tôi so sánh tốc độ tăng thời gian wall-clock của các cây Sequoia có kích thước khác nhau (đường cam)—được chọn để tối đa hóa # token được tạo—với tốc độ tăng của các cây được chọn bởi bộ tối ưu hóa cây nhận biết phần cứng (đường ngang xanh lá)—được chọn để tối đa hóa tốc độ tăng—trên GPU A100 và L40. Bộ tối ưu hóa có thể chọn kích thước và độ sâu cây tối ưu cho mỗi loại phần cứng; bằng cách hạn chế độ sâu của cây, nó có thể làm cho suy đoán nhanh hơn và do đó đạt được tốc độ tăng lớn hơn so với các cây có độ sâu không bị ràng buộc (đường cam).

[5] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. CoRR, abs/2302.01318, 2023. doi: 10.48550/ARXIV.2302.01318. URL https://doi.org/10.48550/arXiv.2302.01318.

[6] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.

[7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

[8] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR, abs/2307.08691, 2023. doi: 10.48550/ARXIV.2307.08691. URL https://doi.org/10.48550/arXiv.2307.08691.

[9] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022.

[10] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix multiplication for transformers at scale. CoRR, abs/2208.07339, 2022. doi: 10.48550/ARXIV.2208.07339. URL https://doi.org/10.48550/arXiv.2208.07339.

[11] Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023.

[12] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: accurate post-training quantization for generative pre-trained transformers. CoRR, abs/2210.17323, 2022. doi: 10.48550/ARXIV.2210.17323. URL https://doi.org/10.48550/arXiv.2210.17323.

[13] Aaron Gokaslan and Vanya Cohen. Openwebtext corpus, 2019.

[14] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. CoRR, abs/2312.00752, 2023. doi: 10.48550/ARXIV.2312.00752. URL https://doi.org/10.48550/arXiv.2312.00752.

[15] Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured state spaces. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=uYLFoz1vlAC.

[16] Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller, Sourab Mangrulkar, Marc Sun, and Benjamin Bossan. Accelerate: Training and inference at scale made simple, efficient and adaptable. https://github.com/huggingface/accelerate, 2022.

13

--- TRANG 13 ---
[17] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.

[18] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015.

[19] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. J. Mach. Learn. Res., 22(241):1–124, 2021.

[20] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019.

[21] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2704–2713, 2018.

[22] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 5156–5165. PMLR, 2020. URL http://proceedings.mlr.press/v119/katharopoulos20a.html.

[23] Sehoon Kim, Karttikeya Mangalam, Jitendra Malik, Michael W. Mahoney, Amir Gholami, and Kurt Keutzer. Big little transformer decoder. CoRR, abs/2302.07863, 2023. doi: 10.48550/ARXIV.2302.07863. URL https://doi.org/10.48550/arXiv.2302.07863.

[24] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Jason Flinn, Margo I. Seltzer, Peter Druschel, Antoine Kaufmann, and Jonathan Mace, editors, Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 2023, Koblenz, Germany, October 23-26, 2023, pages 611–626. ACM, 2023. doi: 10.1145/3600006.3613165. URL https://doi.org/10.1145/3600006.3613165.

[25] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 19274–19286. PMLR, 2023.

[26] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. AWQ: activation-aware weight quantization for LLM compression and acceleration. CoRR, abs/2306.00978, 2023. doi: 10.48550/ARXIV.2306.00978. URL https://doi.org/10.48550/arXiv.2306.00978.

[27] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of network pruning. arXiv preprint arXiv:1810.05270, 2018.

[28] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative llm serving with speculative inference and token tree verification. arXiv preprint arXiv:2305.09781, 2023.

[29] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440, 2016.

[30] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1325–1334, 2019.

[31] NVIDIA, Péter Vingelmann, and Frank H.P. Fitzek. Cuda, release: 10.2.89, 2020. URL https://developer.nvidia.com/cuda-toolkit.

[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.

[33] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. ArXiv, abs/2211.05102, 2022. URL https://api.semanticscholar.org/CorpusID:253420623.

[34] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. arXiv preprint arXiv:2211.05102, 2022.

14

--- TRANG 14 ---
[35] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019.

[36] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073–1083, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1099. URL https://www.aclweb.org/anthology/P17-1099.

[37] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single GPU. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 31094–31116. PMLR, 2023. URL https://proceedings.mlr.press/v202/sheng23a.html.

[38] Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autoregressive models. Advances in Neural Information Processing Systems, 31, 2018.

[39] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695, 2023.

[40] Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, and Felix Yu. Spectr: Fast speculative decoding via optimal transport. arXiv preprint arXiv:2310.15141, 2023.

[41] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-specific knowledge from bert into simple neural networks. arXiv preprint arXiv:1903.12136, 2019.

[42] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pages 10347–10357. PMLR, 2021.

[43] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.

[44] Tim Vieira. Gumbel-max trick and weighted reservoir sampling, 2014.

[45] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.

[46] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared llama: Accelerating language model pre-training via structured pruning. arXiv preprint arXiv:2310.06694, 2023.

[47] Daliang Xu, Wangsong Yin, Xin Jin, Ying Zhang, Shiyun Wei, Mengwei Xu, and Xuanzhe Liu. Llmcad: Fast and scalable on-device large language model inference. arXiv preprint arXiv:2309.04255, 2023.

15

--- TRANG 15 ---
[48] Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Mykola Pechenizkiy, Yi Liang, Zhangyang Wang, and Shiwei Liu. Outlier weighed layerwise sparsity (owl): A missing secret sauce for pruning llms to high sparsity. arXiv preprint arXiv:2310.05175, 2023.

[49] Gyeong-In Yu and Joo Seong Jeong. Orca: A distributed serving system for transformer-based generative models. In USENIX Symposium on Operating Systems Design and Implementation, 2022. URL https://api.semanticscholar.org/CorpusID:251734964.

[50] Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, and Sharad Mehrotra. Draft & verify: Lossless large language model acceleration via self-speculative decoding. CoRR, abs/2309.08168, 2023. doi: 10.48550/ARXIV.2309.08168. URL https://doi.org/10.48550/arXiv.2309.08168.

[51] Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural network quantization without retraining using outlier channel splitting. In International conference on machine learning, pages 7543–7552. PMLR, 2019.

[52] Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-François Kagy, and Rishabh Agarwal. Distillspec: Improving speculative decoding via knowledge distillation. arXiv preprint arXiv:2310.08461, 2023.

Lời cảm ơn
Chúng tôi cảm ơn Xinyu Yang, Harry Dong, Ranajoy Sadhukhan, Hanshi Sun và Silong Yong vì những cuộc thảo luận hữu ích và phản hồi về các bản nháp đầu của bài báo, cũng như giúp tái tạo các kết quả thực nghiệm.

16

--- TRANG 16 ---
A Công trình liên quan
Công trình này giới thiệu một thuật toán mới trong họ các phương pháp giải mã suy đoán nhằm duy trì phân phối đầu ra chính xác của mô hình đích bằng cách cải thiện cấu trúc và thuật toán lấy mẫu/xác minh cho cây token được suy đoán. Tồn tại nhiều hướng khác trong dòng công việc này—ví dụ, các phương pháp giới thiệu sự khoan dung vào thuật toán giải mã suy đoán để đạt được tốc độ tăng với chi phí độ chính xác [23,38], các phương pháp tái sử dụng các lớp hoặc biểu diễn từ mô hình đích làm mô hình nháp [4,50], v.v. Thay vào đó, mô hình nháp có thể được chưng cất để xấp xỉ tốt hơn mô hình đích; DistillSpec [18,41,42,52] cải thiện quá trình đó bằng cách sử dụng dữ liệu được tạo bởi mô hình và điều chỉnh mục tiêu tùy thuộc vào nhiệm vụ và chiến lược giải mã. Cuối cùng, LLMCad [47] đề xuất một thuật toán tiên tiến cho việc tạo và xác minh cây token trong bối cảnh suy luận LLM trên thiết bị.

Bổ sung cho giải mã suy đoán, tồn tại nhiều phương pháp khác nhằm cải thiện tốc độ suy luận LLM. Ví dụ, lượng tử hóa mô hình là một cách rất hứa hẹn khác để xử lý tắc nghẽn I/O trong quá trình suy luận, bằng cách giảm số bit mỗi tham số. Tuy nhiên, không giống như giải mã suy đoán, các phương pháp này thường làm giảm chất lượng của mô hình ở một mức độ nào đó, tùy thuộc vào lượng lượng tử hóa [10, 12, 17, 21, 26, 30, 51] hoặc độ thưa [19, 27, 29].

Trong khi đó, các công trình khác nhau [1,11,39,48] đã nghiên cứu các cách để cải thiện thông lượng phục vụ LLM. Pope et al. [33] đã điều tra hiệu ứng batching trong việc mở rộng LLM. Orca [49] đã đề xuất một hệ thống phục vụ LLM phân tán sử dụng chính sách lập lịch tinh tế để cải thiện việc sử dụng GPU dưới các độ dài yêu cầu khác nhau. vLLM [24] đã sử dụng bảng trang để quản lý bộ nhớ GPU để tăng việc sử dụng bộ nhớ, điều này tăng đáng kể thông lượng suy luận. FlexGen [37] đã đề xuất một cơ chế offloading để hỗ trợ các batch lớn hơn để đạt được thông lượng cao.

FlashAttention [8,9] là một thuật toán khác nhằm cải thiện tốc độ của LLM (ở cả thời gian huấn luyện và suy luận) bằng cách xem xét chi phí I/O của các phép toán khác nhau.

Một phương pháp tiềm năng khác để tăng tốc suy luận là thay đổi các khối xây dựng cơ bản của mô hình. Gần đây, nhiều kiến trúc dưới-bậc hai—bao gồm SSM [14,15] và các mô hình attention tuyến tính [22]—đã được đề xuất. Các mô hình này đặc biệt có lợi cho các đầu vào dài.

B Kiến thức nền tảng
B.1 Giải mã suy đoán dựa trên chuỗi
Phương pháp giải mã suy đoán gốc [5,25] đề xuất sử dụng một "mô hình nháp" nhỏ để suy đoán γ token vào tương lai, và sau đó sử dụng "mô hình đích" để xử lý song song các token này và quyết định token nào sẽ "chấp nhận", theo cách mà phân phối đầu ra của mô hình đích không thay đổi. Thuật toán này được trình bày trong Thuật toán 2.

Leviathan et al. [25] phân tích hiệu suất của thuật toán này, trình bày các phương trình cho số lượng token được chấp nhận dự kiến từ một lần chạy của thuật toán, và tốc độ tăng thời gian wall-clock dự kiến từ việc sử dụng giải mã suy đoán (so với suy luận tự hồi quy tiêu chuẩn với mô hình đích). Trong phân tích này, họ giới thiệu tỷ lệ chấp nhận α∈[0,1], tương ứng với xác suất rằng một token xi được chấp nhận bởi Thuật toán 2, dưới giả định đơn giản hóa rằng các quyết định chấp nhận là i.i.d.⁴ Dưới giả định này, họ cho thấy rằng số lượng token được tạo dự kiến trong mỗi lần chạy của Thuật toán 2 là (1−α^(γ+1))/(1−α). Bổ sung, đặt c biểu thị tỷ lệ giữa thời gian chạy mô hình nháp và thời gian chạy mô hình đích, họ cho thấy rằng tốc độ tăng thời gian wall-clock dự kiến từ việc sử dụng thuật toán này là (1−α^(γ+1))/((1−α)(γc+1)).

B.2 SpecInfer
Ở đây chúng tôi trình bày mã giả cho thuật toán SpecInfer trong Thuật toán 3.

⁴Người ta có thể nghĩ α như tỷ lệ chấp nhận trung bình qua nhiều lần chạy thuật toán này trên một tập dữ liệu đại diện.

17

--- TRANG 17 ---
Thuật toán 2 Giải mã suy đoán dựa trên chuỗi
1: Đầu vào: Tiền tố [x1,x2,...,xn−1], Mô hình đích Mp, mô hình nháp Mq, và số token γ để suy đoán.
2: Đầu ra: Một chuỗi token được tạo sử dụng giải mã suy đoán.
3: for i=n→n+γ-1 do ▷ Lấy mẫu chuỗi γ token từ mô hình nháp
4: qi(x)←Mq([x1,...,xi−1])
5: xi∼qi(x)
6: end for
7: for i=n→n+γ do ▷ Vòng lặp for dưới đây có thể được chạy song song với một forward pass duy nhất của Mp
8: pi(x)←Mq([x1,...,xi−1])
9: end for
10: s←n−1 ▷ Chọn số lượng token n để chấp nhận
11: for i=n→n+γ-1 do
12: ri∼Uniform(0,1)
13: if ri<pi(xi)/qi(xi) then
14: s←s+1
15: else
16: break
17: end if
18: end for
19: p′(x)←ps+1(x)
20: if t<n+γ−1 then
21: p′(x)←norm(max(0,ps+1(x)−qs+1(x)))
22: end if
23: t∼p′(x) ▷ Lấy mẫu một token cuối cùng từ p′(x)
24: Return x1,...,xs,t

Thuật toán 3 Lấy mẫu và Xác minh SpecInfer
1: Đầu vào: Tiền tố [x1,x2,...,xn−1], xác suất mô hình đích P(·|x<n), xác suất mô hình nháp Q(·|x<n), và số nhánh k.
2: Đầu ra: Một token x được lấy mẫu sử dụng SpecInfer.
3: Khởi tạo phần dư R với P và nháp D với Q
4: for i=1→k do
5: lấy mẫu xi∼D, ri∼Uniform(0,1)
6: if ri<R[xi]/D[xi] then ▷ Chấp nhận xi
7: Return xi
8: else ▷ Từ chối xi
9: R←norm(max(R−D,0))
10: end if
11: end for
12: lấy mẫu x∼R
13: Return x

C Kết quả lý thuyết
C.1 Tính đúng đắn của thuật toán xác minh Sequoia
Bây giờ chúng tôi chứng minh rằng thuật toán xác minh Sequoia duy trì phân phối đầu ra của mô hình đích.
Chúng tôi giả định chúng ta có một mô hình đích t, và một danh sách các mô hình nháp (d1,...dn,dn+1,...), nơi di trong trường hợp này phụ thuộc vào các mẫu bị từ chối trước đó x1,...,xi−1, và nơi di(u) và t(u) biểu thị xác suất lấy mẫu token u∈V từ di hoặc t tương ứng (nơi V là từ vựng token). Chúng tôi đặt ti biểu thị phần dư tại lần lặp i của vòng lặp Sequoia, (sau khi i−1 nút đã bị từ chối (vậy t1=t, như có thể thấy trong Thuật toán 1)

Chúng tôi sẽ chứng minh bằng quy nạp về số token được đề xuất n rằng thuật toán xác minh Sequoia đúng.

18

--- TRANG 18 ---
Trường hợp cơ sở (n=0): Sequoia đúng một cách tầm thường, vì nó sẽ đơn giản lấy mẫu từ phần dư t1, bằng với t.

Trường hợp đệ quy: Chúng tôi giả định Sequoia đúng cho n−1 mẫu được đề xuất và chứng minh nó đúng cho n mẫu được đề xuất.

Chúng tôi đầu tiên cho thấy rằng tại giai đoạn i trong thuật toán giải mã suy đoán, cơ hội Sequoia chọn từ chối mẫu được đề xuất bằng ∑x max(0,ti(x)−di(x)):

Bổ đề C.1. P(Không có token nào được chấp nhận tại lần lặp i) = ∑x max(0,ti(x)−di(x)).

Chứng minh.
P(Không có token nào được chấp nhận tại lần lặp i) = ∑x P(lấy mẫu x)·P(từ chối x|x được lấy mẫu)
= ∑x di(x)·(1−min(ti(x)/di(x),1))
= ∑x di(x)−∑x min(ti(x),di(x))
= ∑x ti(x)−∑x min(ti(x),di(x))
= ∑x ti(x)+max(−ti(x),−di(x))
= ∑x ti(x)−ti(x)+max(0,ti(x)−di(x))
= ∑x max(0,ti(x)−di(x))

Bây giờ chúng tôi sẵn sàng chứng minh trường hợp đệ quy của thuật toán Sequoia. Bởi giả thuyết quy nạp, chúng ta biết rằng đối với tất cả u∈V,

t(u) = P(u được chấp nhận trong n−1 lần lặp đầu tiên) + P(Không có token nào được chấp nhận trong n−1 lần lặp đầu tiên)·tn(u)

Điều này có nghĩa là trong trường hợp chúng ta chạy Sequoia cho n−1 lần lặp (và nếu không có token nào được chấp nhận, chúng ta lấy mẫu từ phần dư tn), điều này tương đương với việc lấy mẫu từ phân phối đích t trực tiếp. Chúng tôi muốn chỉ ra rằng phân phối đầu ra này tương đương với phân phối mà chúng ta sẽ nhận được nếu chúng ta chạy Sequoia cho n lần lặp (và nếu không có token nào được chấp nhận, chúng ta lấy mẫu từ phần dư tn+1). Phân phối đầu ra của kịch bản này có thể được viết như sau:

P(u được chấp nhận trong n−1 lần lặp đầu tiên) + P(Không có token nào được chấp nhận trong n−1 lần lặp đầu tiên)·
[dn(u)·P(u được chấp nhận tại lần lặp n)+P(Không có token nào được chấp nhận tại lần lặp n)·tn+1(u)]

Do đó, tất cả những gì chúng ta phải chỉ ra là

tn(u) = dn(u)·P(u được chấp nhận tại lần lặp n)+P(Không có token nào được chấp nhận tại lần lặp n)·tn+1(u)

Bây giờ chúng tôi chỉ ra kết quả mong muốn này. Chúng tôi sẽ sử dụng Bổ đề C.1, và thực tế rằng theo định nghĩa của Thuật toán B.2, chúng ta biết rằng tn+1(u) = max(0,tn(u)−dn(u))/∑x max(0,tn(x)−dn(x)).

19

--- TRANG 19 ---
dn(u)·P(u được chấp nhận tại lần lặp n)+P(Không có token nào được chấp nhận tại lần lặp n)·tn+1(u)

= dn(u)·min(1,tn(u)/dn(u)) + (∑x max(0,tn(x)−dn(x)))tn+1(u)

= min(dn(u),tn(u)) + (∑x max(0,tn(x)−dn(x)))·(max(0,tn(u)−dn(u))/(∑x max(0,tn(x)−dn(x))))

= min(dn(u),tn(u)) + max(0,tn(u)−dn(u))

= tn(u)

Để thấy rằng đẳng thức cuối cùng này đúng, chúng ta xem xét hai trường hợp:

1. Trường hợp 1 tn(u)≥dn(u): min(dn(u),tn(u))+max(0,tn(u)−dn(u))=dn(u)+tn(u)−dn(u)=tn(u).

2. Trường hợp 2 tn(u)<dn(u): min(dn(u),tn(u))+max(0,tn(u)−dn(u))=tn(u)+0=tn(u).

Điều này hoàn thành chứng minh.

C.2 Chi tiết xây dựng cây Sequoia
Bây giờ chúng tôi chứng minh Mệnh đề 3.4 bằng cách dẫn xuất biểu thức dạng đóng cho F(T) (số lượng token được tạo dự kiến bởi việc xác minh cây T), và chỉ ra cách sử dụng quy hoạch động để tìm cây T tối ưu dưới ngân sách kích thước cây.

Mệnh đề C.2. Cho T là một cây token được xác minh với giả định chấp nhận vị trí, và cho f(v) biểu thị hàm điểm cho một nút v∈T. Thì số lượng token D(T) được tạo dự kiến bởi việc xác minh cây T bằng

F(T) = ∑v∈T f(v).

Chứng minh. Đặt D(T) biểu thị số lượng token được tạo dự kiến bởi việc xác minh cây T. Chúng tôi muốn chứng minh rằng D(T)=F(T) ∀T. Chúng tôi sẽ chứng minh điều này bằng quy nạp về kích thước của T.

Trường hợp cơ sở (n=1): Một cây có kích thước 1 chỉ được tạo thành từ nút gốc. Theo định nghĩa của hàm điểm f(v) (Định nghĩa 3.3), chúng ta biết rằng f(v)=1 cho nút gốc, vậy F(T)=1. D(T)=1 cũng vậy, vì việc xác minh một cây được tạo thành từ nút gốc không có con sẽ đơn giản lấy mẫu từ mô hình đích, và tạo ra 1 token.

Bước quy nạp (n>1): Đối với |T|=n>1, đặt v là một lá của T tại chỉ số con iv có độ sâu d với cha vp và anh em Sv (tập hợp các chỉ số anh em). Chúng ta sau đó có thể xem xét cây T′=T−{v}. Dựa trên giả định quy nạp, chúng ta biết rằng g(T′)=D(T′). Sử dụng giả định này, chúng ta có thể biểu thị D(T) theo D(T′):

D(T) = D(T′)−(d−1)·f(vp)·(1−∑i∈Sv pi)+(d−1)·f(vp)·(1−∑i∈Sv∪{iv} pi)+d·f(v)
= D(T′)−(d−1)f(vp)piv+d·f(v)
= ∑v′∈T′ f(v′)−(d−1)f(v)+d·f(v)
= F(T′)+f(v)
= F(T)

Lưu ý rằng chúng tôi sử dụng giả thuyết quy nạp, cùng với thực tế là f(vp)·piv=f(v) (theo định nghĩa của f(v)).

20

--- TRANG 20 ---
C.2.1 Quy hoạch động (Độ sâu cây không bị ràng buộc)
Bây giờ chúng tôi sẽ mô tả nghiệm quy hoạch động của chúng tôi để tìm cây T có kích thước n với giá trị F(T) cao nhất. Nhớ lại rằng chúng tôi đã định nghĩa

c(n) = max(T,|T|=n) F(T),

và rằng chúng tôi đã chỉ ra rằng chúng ta có thể biểu thị c(n) theo các giá trị c(n′) cho n′<n:

c(n) = max(ai,∑(i=1 to n-1)ai=n-1) [1+∑(i=1 to n-1) pic(ai)].

Sử dụng cấu trúc con đệ quy này, chúng tôi chỉ ra rằng c(n) có thể được tính bằng quy hoạch động. Để làm cho bài toán đơn giản, chúng tôi giả định rằng số lượng ứng cử viên (tức là tập hợp kế tiếp của bất kỳ đỉnh nào) có một giới hạn trên k. Sau đó xem xét nhánh trong tầng đầu tiên, chúng ta có thể có

c(n+1) = max [1+ p1c(a1)+p2c(a2)+...+pkc(ak)] s.t. ∑(i=1 to k)ai=n, I(ai>0)≥I(ai+1>0) i∈[k−1]

Chúng tôi giả định rằng p1≥p2≥...≥pk (điều này hợp lý do cơ chế của Sequoia và SpecInfer). Sau đó chúng ta có thể hủy các ràng buộc cuối cùng, và hàm chúng ta đang tối ưu hóa trở thành:

c(n+1) = max [1+ p1c(a1)+p2c(a2)+...+pkc(ak)] s.t. ∑(i=1 to k)ai=n

Dựa trên giả định rằng p1≥p2≥...≥pk, chúng ta có thể đạt được rằng trong nghiệm tối ưu a1≥a2...≥ak.

Để đơn giản hóa mục tiêu tối ưu hóa, chúng tôi định nghĩa một biến mới cho n≥L≥1

cL(n+1) = max [1+ p1c(a1)+p2c(a2)+...+pkc(ak)] s.t. ∑(i=1 to k)ai=n, a1,a2,...,aL>0, aL+1=aL+2=...=ak=0

và c0(1)=1. Sau đó rõ ràng,

c(n) = max(L∈[n−1]) cL(n)

Sau đó chúng ta xem xét hàm chuyển,

cL+1(n) = max [1+ p1c(a1)+p2c(a2)+...+pkc(ak)]
= max [1+ p1c(a1)+p2c(a2)+...+pL+1c(aL+1)]
= max(x∈{L+1,L+2,...,n−1}) cL(x)+pL+1c(n−x)

c1(n) = 1+ p1c(n−1)

Thứ tự tính toán được hình ảnh hóa trong Hình 6.

[THIS IS FIGURE: Hình 6: Thứ tự Tính toán - A triangular grid showing computation order with numbers arranged in rows]

Độ phức tạp có thể được ước tính bởi ∑(1≤l≤k)∑(l+1≤m≤n)(m−l) = ∑(1≤l≤k)((n−l+1)(n−l))/2 = O(n²k)

21

--- TRANG 21 ---
C.2.2 Quy hoạch động (Độ sâu cây bị ràng buộc)
Chúng tôi viết lại quy hoạch động dưới đây với hạn chế độ sâu,

max F(T) s.t. |T|≤M, Depth(T)≤L

Để làm cho bài toán đơn giản, chúng tôi thêm một ràng buộc bổ sung về số nhánh của nút gốc FirstBranch(T)=B. Thực tế, chúng tôi giải quyết bài toán sau

T(M,L,B) ≡ max F(T) s.t. |T|=M, Depth(T)≤L, FirstBranch(T)=B

Đầu tiên, chúng ta nên chú ý rằng, không phải tất cả M,L,B có thể dẫn đến một nghiệm khả thi, chưa nói đến một nghiệm tối ưu. Chúng tôi sử dụng R(M,L,B)=1 hoặc 0 để biểu thị rằng, liệu có tồn tại ít nhất một cây T, thỏa mãn

|T|=M
Depth(T)≤L  
FirstBranch(T)=B

Giả sử MaxBranch(T)=K, đối với bất kỳ T nào, thì chúng ta có thể viết trực tiếp (M,L,B) khả thi cho L≤2, đó là (1,1,0),(2,2,1),(3,2,2)...,(K+1,2,K).

Nếu R(M,L,B)=1 và T thỏa mãn các ràng buộc do (M,L,B) mang lại, thì nó phải có B≤K. Chúng ta có thể chia nó thành ba trường hợp.

Trường hợp 1: B=0. Trong trường hợp này, chúng ta trực tiếp có
R(M,L,0) = {1, M=1; 0, M>1.

Trường hợp 2: B=1. Điều này gợi ý rằng nếu chúng ta xóa nút gốc của T, phần còn lại vẫn là một cây hợp lệ thỏa mãn một trong các ràng buộc sau, (M−1,L−1,0),(M−1,L−1,1),...,(M−1,L−1,K). Mặt khác, nếu tồn tại một cây thỏa mãn (M−1,L−1,j), 0≤j≤K, chúng ta có thể ngay lập tức xây dựng một cây thỏa mãn (M,L,1) bằng cách thêm một nút gốc. Thì chúng ta có

R(M,L,1) = max(0≤j≤K) R(M−1,L−1,j)

Trường hợp 3: B≥2. Hãy xem xét nhánh cuối cùng của tầng đầu tiên và phần còn lại. Rõ ràng, cả hai đều là cây hợp lệ. Nhánh cuối cùng của tầng đầu tiên, T1 thỏa mãn một trong (|T1|,L−1,0),(|T1|,L−1,1),...,(|T1|,L−1,K). Phần còn lại T2 thỏa mãn (|T2|,L,B−1). Mặt khác, nếu chúng ta tìm R(x,L−1,j)=1 và R(y,L,B−1)=1, nơi x+y=M, chúng ta có thể ngay lập tức xây dựng một cây thỏa mãn (M,L,B) bằng cách để cây đầu tiên là nhánh thứ B của cây thứ hai. Thì chúng ta có

R(M,L,B) = max(1≤y≤M−1) (R(y,L,B−1)×max(0≤j≤K) R(M−y,L−1,j))

Bên cạnh điều này, có một điều kiện ban đầu khác, dựa trên đó chúng ta có thể thực hiện quy hoạch động để giải quyết F.

R(M,1,B) = {1, M=1,B=0; 0, ngược lại.

Thuật toán để giải quyết F trong Thuật toán 4.

Tương tự, chúng ta có thể tính T trong Thuật toán 5. Đối với (m,l,b) thỏa mãn R(m,l,b)=0, chúng tôi giữ T(m,l,b)=−∞.

C.3 Kết quả khả năng mở rộng cho các cây Sequoia
Bây giờ chúng tôi chứng minh rằng, dưới các giả định nhất định về tỷ lệ chấp nhận của thuật toán xác minh cây, số lượng token được tạo dự kiến bởi việc xác minh cây Sequoia được giới hạn dưới bởi một hàm gần như logarithmic trong kích thước của cây. Chúng tôi sẽ làm điều này bằng cách chỉ ra rằng một cây đơn giản hơn—cây k*(n) (được định nghĩa dưới đây)—cũng có giới hạn dưới này, và sử dụng thực tế rằng cây Sequoia theo cấu trúc là cây có số lượng token được tạo dự kiến lớn nhất.

Chúng tôi định nghĩa cây k*(n) là cây k-ary⁵ có ≤n nút có độ dài chuỗi được chấp nhận dự kiến cao nhất. Đặt G(n) biểu thị độ dài chuỗi được chấp nhận dự kiến cho cây k*(n), chúng tôi bây giờ sẽ chứng minh rằng G(n)∈Ω(b·log(n)/log(log(n))) (có nghĩa là, nó được giới hạn dưới bởi một bội số vô hướng của b·log(n)/log(log(n))), dưới giả định rằng tỷ lệ từ chối rk được giới hạn trên bởi một power-law của k. Sau đó nó theo trực tiếp (như một hệ quả) rằng tỷ lệ tăng trưởng của cây được tạo bởi thuật toán Sequoia cũng sẽ trong Ω(b·log(n)/log(log(n))).

⁵Nhớ lại rằng một cây k-ary là cây nơi mỗi nút không phải lá có k con.

22

--- TRANG 22 ---
Thuật toán 4 Tính toán R
1: Đầu vào: M cho số nút cây tối đa, L cho độ sâu tối đa và K cho số nhánh tối đa của bất kỳ nút nào.
2: Đầu ra: R(m,l,b) ∀1≤m≤M,1≤l≤L,0≤b≤K.
3: Khởi tạo mảng F[M][L][K] với 0
4: # Khởi tạo các ranh giới
5: for l=1→L do
6: for b=0→K do
7: F[1][l][b]=¬b
8: end for
9: end for
10: for m=2→M do
11: for l=2→L do
12: F[m][l][1]=max(0≤j≤K) F[m−1][l−1][j]
13: for b=2→K do
14: F[m][l][b]=max(1≤y≤m−1) (F[y][l][b−1]×max(0≤j≤K) F[m−y][l−1][j]))
15: end for
16: end for
17: end for

Thuật toán 5 Tính toán T
1: Đầu vào: M cho số nút cây tối đa, L cho độ sâu tối đa và K cho số nhánh tối đa của bất kỳ nút nào. p[1],p[2],...p[K] cho xác suất của mỗi nhánh.
2: Đầu ra: T(m,l,b) ∀1≤m≤M,1≤l≤L,0≤b≤K.
3: Khởi tạo mảng T[M][L][K]=−∞
4: # Khởi tạo các ranh giới
5: for l=1→L do
6: for b=0→K do
7: T[1][l][b]=¬b
8: end for
9: end for
10: for m=2→M do
11: for l=2→L do
12: T[m][l][1]=1+ p[1]×max(0≤j≤K) T[m−1][l−1][j]
13: for b=2→K do
14: T[m][l][b]=max(1≤y≤m−1) (T[y][l][b−1]+p[b]×max(0≤j≤K) T[m−y][l−1][j]))
15: end for
16: end for
17: end for
18: Return mảng T

Định lý C.3. Giả sử cơ hội rk của một thuật toán xác minh cây token từ chối tất cả k token được suy đoán (k nút con của một số nút trong cây) được giới hạn trên bởi một power-law của k; vậy rk≤1/k^b cho một số b>0∈R. Thì tỷ lệ tăng trưởng G(n) cho cây k*(n) là trong Ω(b·log(n)/log(log(n))).

Chứng minh. Chúng tôi sẽ đặt k(n)=⌊log(n)^(1/b)⌋ biểu thị chiều rộng nhánh được chọn cho kích thước cây n, và chỉ ra rằng dưới giả định này, tỷ lệ tăng trưởng G′(n) của cây k(n) tương ứng ít nhất là b·log(n)/(10·log(log(n))), giả sử rằng n đủ lớn. Cho rằng G′(n) là một giới hạn dưới trên G(n) (vì sự lựa chọn k(n) ở trên có thể không hoàn toàn tối ưu), và sử dụng định nghĩa của Ω, điều này chứng minh rằng G(n)∈Ω(b·log(n)/log(log(n))). Lưu ý rằng chúng tôi sẽ viết tắt k(n) là k ở nhiều nơi trong suốt chứng minh, để ngắn gọn.

Nếu chúng ta đặt d biểu thị độ sâu của cây, số nút trong cây là 1+k+k²+...+k^d=(k^(d+1)−1)/(k−1)≤n.

23

--- TRANG 23 ---
Điều này có nghĩa là d≤logk(n), mà chúng ta có thể chứng minh như sau:

k^(d+1)−1≤n(k−1)
⇒k^(d+1)≤nk−n+1≤nk
⇒d+1≤logk(nk)=logk(n)+1
⇒d≤logk(n)

Chúng ta có thể giả định d là số nguyên lớn nhất sao cho d≤logk(n), vậy nó cũng theo đó d+1≥logk(n).

Đặt αk:=1−rk, độ dài G′(n) dự kiến của chuỗi token được chấp nhận có thể được biểu thị như 1·(1−αk)+2αk·(1−αk)+3αk²(1−αk)+...+(d+1)αk^d = 1+αk+αk²+...+αk^d = (1−αk^(d+1))/(1−αk) (đẳng thức đầu tiên là kết quả của tổng telescoping, thứ hai từ tổng của chuỗi hình học hữu hạn). Bây giờ chúng tôi sẽ giới hạn dưới biểu thức này, sử dụng Bổ đề C.5 (được định nghĩa và chứng minh dưới đây).

G(n)≥G′(n) = (1−αk^(d+1))/(1−αk) = (1−(1−rk)^(d+1))/rk ≥ (d+1)/10 áp dụng Bổ đề C.5, và giả sử rk·(d+1)≤log(1.9)
≥ logk(n)/10 = log(n)/(10log(k)) ≤ log(n)/(10log(log(n)^(1/b))) = (b·log(n))/(10log(log(n)))

Bây giờ chúng ta chỉ cần hiểu khi nào rk·(d+1)≤log(1.9):

rk·(d+1)≤(1/k^b)·(logk(n)+1) ≤ (2logk(n))/((log(n)^(1/b)−1)^b) sử dụng k(n)=⌊log(n)^(1/b)⌋≥log(n)^(1/b)−1
≤ (2logk(n))/((1/2·log(n)^(1/b))^b) giả sử log(n)^(1/b)≥2⇔n≥exp(2^b)
= (2^(b+1)log(n))/(log(k)log(n)) = 2^(b+1)/log(k)

Vậy nếu 2^(b+1)/log(k)≤log(1.9), thì theo đó rk·(d+1)≤log(1.9).

2^(b+1)/log(k)≤log(1.9)⇔2^(b+1)/log(1.9)≤log(k)⇔exp(2^(b+1)/log(1.9))≤k

Cho rằng k(n)=⌊log(n)^(1/b)⌋≥log(n)^(1/b)−1, chúng ta biết rằng nếu log(n)^(1/b)−1≥exp(2^(b+1)/log(1.9)), thì nó phải đúng rằng k(n)≥exp(2^(b+1)/log(1.9)) cũng vậy. Chúng ta có thể thấy rằng điều này đúng nếu:

log(n)^(1/b)−1≥exp(2^(b+1)/log(1.9)) ⇔ n≥exp((1+exp(2^(b+1)/log(1.9)))^b)

Do đó, chúng tôi đã chỉ ra rằng miễn là n lớn hơn biểu thức trên, thì G′(n)≥(b·log(n))/(10log(log(n))). Vì chúng ta biết rằng G(n)≥G′(n), điều này kết thúc chứng minh rằng G(n) trong Ω(b·log(n)/log(log(n))).

Bây giờ chúng tôi chứng minh, như một hệ quả của Định lý C.3, rằng tỷ lệ tăng trưởng của cây Sequoia cũng trong Ω(b·log(n)/log(log(n))).

Hệ quả C.4. Dưới cùng giả định về tỷ lệ từ chối như Định lý C.3, nó đúng rằng tỷ lệ tăng trưởng cho cây Sequoia trong Ω(b·log(n)/log(log(n))).

Chứng minh. Theo cấu trúc, đối với mỗi kích thước cây n, cây Sequoia là cây có số lượng token được tạo dự kiến lớn nhất. Do đó, đối với mỗi giá trị của n, số lượng token được tạo dự kiến cho cây Sequoia phải lớn hơn của cây k*(n), được chỉ ra trong Định lý C.3 là trong Ω(b·log(n)/log(log(n))). Điều này kết thúc chứng minh.

Bây giờ chúng tôi chứng minh bổ đề mà chúng tôi đã sử dụng để chứng minh Định lý C.3:

Bổ đề C.5. Đối với bất kỳ số thực x∈(0,1] và số nguyên m>0 sao cho mx≤log(1.9), nó đúng rằng (1−(1−x)^m)/x≥m/10.

Chứng minh.
(1−(1−x)^m)/x = (1−(1−mx+m(m-1)/2·x²−m(m-1)(m-2)/6·x³+m(m-1)(m-2)(m-3)/24·x⁴−...+(−1)^m·x^m))/x
= (mx−m(m-1)/2·x²+m(m-1)(m-2)/6·x³−m(m-1)(m-2)(m-3)/24·x⁴+...−(−1)^m·x^m)/x
= m−m(m-1)/2·x+m(m-1)(m-2)/6·x²−m(m-1)(m-2)(m-3)/24·x³+...−(−1)^m·x^(m-1)
≥ m−m(m-1)/2·x−m(m-1)(m-2)/6·x²−m(m-1)(m-2)(m-3)/24·x³−...−x^(m-1)
≥ m−m²/2!·x−m³/3!·x²−m⁴/4!·x³−...
= m(1−mx/2!−(mx)²/3!−(mx)³/4!−(mx)⁴/5!−(mx)⁵/6!−...)
= m(2−(1+mx/2!+(mx)²/3!+(mx)³/4!+(mx)⁴/5!+(mx)⁵/6!+...))/2
≥ m(2−(1+mx+(mx)²/2!+(mx)³/3!+(mx)⁴/4!+(mx)⁵/5!+...))/2
= m(2−e^(mx))/2
≥ m/10 Giả sử e^(mx)≤1.9, điều này đúng theo giả định ban đầu của chúng tôi.

C.4 Kết quả mạnh mẽ cho xác minh Sequoia
Bây giờ chúng tôi chứng minh kết quả mạnh mẽ cho thuật toán xác minh Sequoia.

Định lý C.6. Thuật toán xác minh Sequoia thỏa mãn cả tính chất vận chuyển tối ưu và tính chất bao phủ, trong khi SpecInfer và SpecTr chỉ thỏa mãn tính chất vận chuyển tối ưu, và lấy mẫu ngây thơ (top-k) chỉ thỏa mãn tính chất bao phủ.

Chứng minh. Chứng minh này khá đơn giản:

25

--- TRANG 24 ---
• Sequoia thỏa mãn tính chất vận chuyển tối ưu: Rõ ràng rằng Sequoia thỏa mãn tính chất vận chuyển tối ưu, vì tại k=1, nó giống hệt với thuật toán giải mã suy đoán gốc [25].

• Sequoia thỏa mãn tính chất bao phủ: Để thấy tại sao Sequoia thỏa mãn tính chất bao phủ, chúng tôi sẽ sử dụng hai thực tế sau:
  - Nếu hỗ trợ của Q có kích thước k và k token được suy đoán bởi mô hình nháp, tập hợp các token được suy đoán sẽ luôn chính xác bằng k token trong hỗ trợ của Q (vì Sequoia thực hiện lấy mẫu không thay thế từ mô hình nháp).
  - Trong vòng lặp xác minh for trong Thuật toán 1, hỗ trợ của phần dư sẽ luôn được chứa trong hỗ trợ của P giao với tập hợp các token chưa bị từ chối. Điều này là vì hỗ trợ của phần dư không bao giờ có thể tăng (vì pi(x)=0⇒pi+1(x)=norm(max(pi−qi,0))(x)=0, nơi pi và qi biểu thị xác suất phần dư và nháp tại lần lặp i, tương ứng), và vì nếu một token x bị từ chối, nó sẽ "thoát" khỏi phần dư (vì x bị từ chối có nghĩa là qi(x)>pi(x) có nghĩa là pi+1(x)=norm(max(pi−qi,0))(x)=0).

Kết hợp hai thực tế này, chúng ta có thể thấy rằng nếu k−1 token đầu tiên bị từ chối, thì token thứ k phải được chấp nhận, vì phần dư phải là một vector one-hot với xác suất 1 tại token còn lại duy nhất, và xác suất nháp (được cập nhật) cũng sẽ là cùng vector one-hot này (và do đó, được chấp nhận với xác suất 1). Bổ sung, chúng ta có thể thấy rằng nếu V token được lấy mẫu (nơi V là kích thước từ vựng), chúng phải chính xác bằng V token trong từ vựng, và do đó một trong những token đó phải được chấp nhận. Trong trường hợp hỗ trợ của Q bằng toàn bộ từ vựng, kết quả này theo trực tiếp từ thảo luận trên. Trong trường hợp hỗ trợ của Q không bằng toàn bộ từ vựng, điều này là kết quả của thực tế rằng một khi tất cả token trong hỗ trợ của Q đã được lấy mẫu và từ chối, chúng ta bắt đầu lấy mẫu (không thay thế) từ phân phối đồng đều trên tất cả các token không bị từ chối.

• SpecInfer thỏa mãn tính chất vận chuyển tối ưu: Đối với k=1, SpecInfer giống hệt với thuật toán giải mã suy đoán gốc [25].

• SpecInfer không thỏa mãn tính chất bao phủ: Dễ dàng thấy rằng SpecInfer không thỏa mãn tính chất bao phủ, với ví dụ phản chứng sau. Đặt Q=[0.5,0.5] và P=[1.0,0]. Chúng ta có thể thấy rằng hỗ trợ của Q có kích thước 2 và chứa hỗ trợ của P. Nhưng với xác suất 25%, SpecInfer sẽ lấy mẫu token thứ hai hai lần liên tiếp, và sẽ từ chối cả hai.

• SpecTr thỏa mãn tính chất vận chuyển tối ưu: Đối với k=1, SpecTr giống hệt với thuật toán giải mã suy đoán gốc [25], vì γ=1 theo định nghĩa.

• SpecTr không thỏa mãn tính chất bao phủ: Chúng ta có thể chỉ ra rằng SpecTr (cụ thể, thuật toán 'k-sequential selection' từ [40]) không thỏa mãn tính chất bao phủ, với ví dụ phản chứng sau. Đặt P=[1,0] và Q=[0.5,0.5]. Thì βp,q(γ)=∑x=0¹min(Q(x),P(x)/γ)=min(0.5,1/γ)+min(0.5,0/γ)=0.5 (vì γ∈[1,2] theo giả định). Chúng ta biết tỷ lệ chấp nhận của SpecTr là 1−(1−βp,q(γ))²=1−(1−0.5)²=0.75≠1. Do đó, SpecTr không thỏa mãn tính chất bao phủ.

• Lấy mẫu ngây thơ top-k không thỏa mãn tính chất vận chuyển tối ưu: Đặt Q=[0.6,0.4] và P=[0.6,0.4], chúng ta có thể thấy rằng lấy mẫu ngây thơ top-k sẽ chấp nhận với xác suất 0.6, trong khi 1−∥P−Q∥/2=1.0.

• Lấy mẫu ngây thơ top-k thỏa mãn tính chất bao phủ: Dễ dàng thấy rằng nếu hỗ trợ của Q có kích thước k và chứa hỗ trợ của P, thì lấy mẫu ngây thơ top-k sẽ luôn chấp nhận (vì nó sẽ lấy mẫu từ mô hình đích và chấp nhận nếu token được lấy mẫu nằm trong k token hàng đầu theo mô hình nháp). Tương tự, nếu k=V, nó phải chấp nhận cũng vậy (vì k token hàng đầu phải là toàn bộ từ vựng, và vì vậy bất kỳ mẫu nào từ mô hình đích phải chấp nhận).

D Thí nghiệm bổ sung
Chúng tôi cung cấp kết quả end-to-end bổ sung so sánh Sequoia với các baseline, mở rộng kết quả từ Phần 4.1. Ở đây, chúng tôi cung cấp kết quả on-chip trên GPU L40, tương tự như kết quả trong Bảng 4, nhưng trên phần cứng khác nhau.

26

--- TRANG 25 ---
Bảng 7: Kết quả on-chip (L40): Cấu hình cây tối ưu và tốc độ tăng cho các cặp mô hình nháp và đích khác nhau, và các nhiệt độ khác nhau, cho Sequoia so với SpecInfer. Chúng tôi chỉ định số lượng token được tạo trung bình mỗi bước giải mã trong ngoặc đơn, bên cạnh hệ số tốc độ tăng. Sequoia đạt được tốc độ tăng lên đến 3.95× trên L40.

| LLM Đích | Mô hình Nháp | T | Tập dữ liệu | Cấu hình Cây (kích thước, độ sâu) | Tốc độ tăng | SpecInfer 5×8 | SpecInfer 8×8 |
|---|---|---|---|---|---|---|---|
| Llama2-7B | JF68M | 0 | C4 | (64,10) | 3.95×(4.68) | 3.50×(3.98) | 3.30×(4.10) |
| Llama2-7B | JF68M | 0.6 | C4 | (64,7) | 3.10×(3.63) | 2.28×(2.89) | 2.31×(3.08) |
| Llama2-7B | JF68M | 0 | OpenWebText | (64,7) | 3.12×(3.58) | 2.79×(3.16) | 2.62×(3.24) |
| Llama2-7B | JF68M | 0.6 | OpenWebText | (64,6) | 2.68×(3.12) | 2.08×(2.54) | 1.99×(2.64) |
| Llama2-7B | JF68M | 0 | CNN Daily | (64,7) | 3.30×(3.79) | 2.89×(3.28) | 2.73×(3.40) |
| Llama2-7B | JF68M | 0.6 | CNN Daily | (64,6) | 2.81×(3.27) | 2.09×(2.59) | 1.99×(2.65) |
| Llama2-13B | JF68M | 0 | C4 | (64,10) | 3.15×(4.25) | 2.76×(3.61) | 2.68×(3.72) |
| Llama2-13B | JF68M | 0.6 | C4 | (64,8) | 2.62×(3.57) | 2.06×(2.81) | 2.03×(2.93) |
| Llama2-13B | JF68M | 0 | OpenWebText | (64,8) | 2.64×(3.52) | 2.34×(3.05) | 2.26×(3.14) |
| Llama2-13B | JF68M | 0.6 | OpenWebText | (64,6) | 2.28×(3.07) | 1.79×(2.44) | 1.78×(2.57) |
| Llama2-13B | JF68M | 0 | CNN Daily | (64,7) | 2.78×(3.68) | 2.47×(3.21) | 2.40×(3.33) |
| Llama2-13B | JF68M | 0.6 | CNN Daily | (64,7) | 2.37×(3.22) | 1.85×(2.51) | 1.80×(2.60) |
| Llama2-13B | JF160M | 0 | C4 | (64,9) | 3.07×(5.04) | 2.71×(4.34) | 2.62×(4.43) |
| Llama2-13B | JF160M | 0.6 | C4 | (64,7) | 2.65×(4.18) | 2.07×(3.43) | 2.05×(3.56) |
| Llama2-13B | JF160M | 0 | OpenWebText | (64,7) | 2.60×(4.05) | 2.24×(3.57) | 2.18×(3.67) |
| Llama2-13B | JF160M | 0.6 | OpenWebText | (64,6) | 2.30×(3.54) | 1.82×(2.99) | 1.73×(2.99) |
| Llama2-13B | JF160M | 0 | CNN Daily | (64,7) | 2.71×(4.21) | 2.34×(3.72) | 2.27×(3.83) |
| Llama2-13B | JF160M | 0.6 | CNN Daily | (64,6) | 2.36×(3.63) | 1.82×(2.99) | 1.72×(2.99) |

27
