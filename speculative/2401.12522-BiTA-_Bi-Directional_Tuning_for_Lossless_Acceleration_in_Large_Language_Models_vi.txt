BiTA: Điều chỉnh Hai chiều cho Gia tốc Không mất mát trong các Mô hình Ngôn ngữ Lớn

Feng Lin1,2,Hanling Yi1,Hongbin Li1,Yifan Yang1,Xiaotian YU1,
Guangming Lu2và Rong Xiao1
1Intellifusion Inc.
2Viện Công nghệ Harbin, Thâm Quyến
lin1993@mail.ustc.edu.cn,
{hanling.cuhk, lee.blingner, yifan.yang.cn, xiaotianyu.ac, rongxiao }@gmail.com,
luguangm@hit.edu.cn

Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) thường sử dụng sinh tự hồi quy trong quá trình suy luận, dẫn đến nhu cầu băng thông bộ nhớ cao và do đó độ trễ kéo dài. Để giảm thiểu sự kém hiệu quả này, chúng tôi trình bày Điều chỉnh Hai chiều cho Gia tốc Không mất mát (BiTA), một phương pháp đổi mới để đẩy nhanh LLM thông qua sinh bán tự hồi quy được tinh giản và xác minh bản thảo. Lấy cảm hứng từ khái niệm điều chỉnh prompt, chúng tôi tăng cường LLM với thiết kế hiệu quả tham số được gọi là điều chỉnh hai chiều cho khả năng sinh bán tự hồi quy. Sử dụng giải mã dựa trên cây hiệu quả, các mô hình thực hiện sinh ứng viên bản thảo và xác minh song song, đảm bảo đầu ra giống hệt với các bản tương ứng tự hồi quy dưới lấy mẫu tham lam. BiTA hoạt động như một mô-đun plug-in nhẹ, tăng cường một cách liền mạch hiệu quả suy luận của các LLM hiện có mà không yêu cầu các mô hình hỗ trợ bổ sung hoặc phát sinh chi phí bộ nhớ thêm đáng kể. Áp dụng BiTA được đề xuất, LLaMA-2-70B-Chat đạt được tăng tốc 2.7× trên benchmark MT-Bench. Các thí nghiệm mở rộng xác nhận phương pháp của chúng tôi vượt trội các kỹ thuật gia tốc tiên tiến.

1 Giới thiệu
Những năm gần đây đã chứng kiến sự phát triển nhanh chóng trong các mô hình ngôn ngữ lớn (LLM) dựa trên kiến trúc transformer. Các tham số của LLM đã nhanh chóng phát triển, trải dài từ vài tỷ đến hàng chục nghìn tỷ, như được minh họa bởi các mô hình như Chat-GPT [Brown et al., 2020], LLaMA-2 [Touvron et al., 2023], và những mô hình khác. Trong khi LLM thể hiện khả năng sinh đa dạng và mạnh mẽ, chúng gặp phải thách thức về độ trễ suy luận do gánh nặng tính toán đáng kể phát sinh từ nhiều tham số. Kết quả là, việc gia tốc quá trình suy luận của LLM đã trở thành một trọng tâm quan trọng, đặc biệt trong các tình huống hạn chế tài nguyên như thiết bị biên và ứng dụng thời gian thực như chatbot.

Các LLM chỉ có bộ giải mã phổ biến, được nổi bật trong các công trình gần đây [Zhang et al., 2022; Scao et al., 2022; Almazrouei et al., 2023], tuân theo cách sinh token-by-token. Mỗi token được sinh ra cần một lần thực thi suy luận riêng biệt, phản ánh bản chất sinh tự hồi quy (AR) của chúng và dẫn đến một số lượng lớn các lời gọi transformer trong quá trình suy luận. Những lời gọi này thường gặp phải các ràng buộc liên quan đến băng thông bộ nhớ, gây ra hiệu quả tính toán giảm và thời gian wall-clock kéo dài [Hooper et al., 2023; Zhang et al., 2023; Shazeer, 2019]. Do đó, một chiến lược chính để đẩy nhanh LLM là tối thiểu hóa số lần thực thi suy luận.

Giải mã bán tự hồi quy (SAR), như được giới thiệu trong tài liệu dịch máy [Wang et al., 2018], giảm thiểu nhu cầu cao cho các lần thực thi suy luận bằng cách tạo ra nhiều token song song với một bước suy luận mô hình duy nhất. Tuy nhiên, phần lớn các LLM hiện tại là các mô hình AR, thiếu khả năng sinh SAR. Việc đào tạo lại cho một mô hình SAR có vẻ khó khăn do sự không khớp giữa các mục tiêu SAR và pretraining AR. Ngoài ra, đào tạo từ đầu theo cách SAR có vẻ gần như không thực tế xem xét mức tiêu thụ tài nguyên đáng kể liên quan. Hơn nữa, các mô hình SAR thường gặp phải sự suy giảm chất lượng so với các bản tương ứng AR [Wang et al., 2018; Gu và Kong, 2021; Huang et al., 2022].

Liệu có khả thi để trao quyền cho một mô hình ngôn ngữ AR hiện có hoạt động như một mô hình SAR với sự thích ứng tối thiểu trong khi đảm bảo hiệu suất mô hình thỏa đáng? Chúng tôi chia nhỏ câu hỏi thành hai khía cạnh và giải quyết từng cái một cách riêng biệt. Tận dụng những tiến bộ trong các kỹ thuật điều chỉnh hiệu quả tham số (PET), đặc biệt là điều chỉnh prompt [Lester et al., 2021], chúng tôi một cách liền mạch tăng cường các mô hình AR với khả năng sinh SAR. Đồng thời, rút ra những hiểu biết từ giải mã suy đoán [Leviathan et al., 2023], thường tuân theo mô hình "draft-then-verify", chúng tôi thực hiện xác minh trên các đầu ra SAR. Việc xác minh cẩn thận này đảm bảo rằng các đầu ra SAR vẫn nhất quán khi được suy luận theo cách AR, từ đó ngăn chặn sự suy giảm điển hình được quan sát trong các mô hình SAR tiêu chuẩn.

Trong bài báo này, chúng tôi đề xuất Điều chỉnh Hai chiều cho Gia tốc Không mất mát (BiTA), một sơ đồ gia tốc mới nhằm đạt được giải mã SAR không mất mát cho các mô hình ngôn ngữ AR bằng cách học các tham số có thể học được rất hạn chế. Cụ thể, BiTA bao gồm hai thành phần: sinh bản thảo SAR thông qua điều chỉnh hai chiều được đề xuất và xác minh được tinh giản cho các ứng viên bản thảo được sinh ra. Điều chỉnh hai chiều liên quan đến việc kết hợp cả prompt token và mask token để cho phép dự đoán các token tương lai, mở rộng vượt ra ngoài token tiếp theo cho một mô hình AR. Phương pháp này được ẩn dụ gọi là các embedding prefix và suffix có thể học được trong chuỗi token. Thông qua một cơ chế attention dựa trên cây tinh xảo, sinh và xác minh hoạt động đồng thời trong một lần forward pass duy nhất trong mô hình AR được chuyển đổi. Thiết kế phổ quát loại bỏ nhu cầu về các bước xác thực bổ sung hoặc các mô hình xác minh bên ngoài. Hưởng lợi từ khái niệm điều chỉnh prompt, phương pháp được đề xuất có thể hoạt động như một mô-đun plug-and-play để đẩy nhanh bất kỳ LLM dựa trên transformer công khai nào, đặc biệt là những chatbot được hướng dẫn tốt [Touvron et al., 2023; Chiang et al., 2023; Almazrouei et al., 2023], mà không làm tổn hại khả năng sinh mạnh mẽ của chúng. Những đóng góp chính của chúng tôi có thể được tóm tắt như sau:

• Để giảm các lời gọi transformer trong sinh AR, chúng tôi thích ứng các mô hình ngôn ngữ AR cho sinh SAR sử dụng điều chỉnh hai chiều được đề xuất, giới thiệu ít nhất 0.01% tham số có thể đào tạo bổ sung.

• Chúng tôi giới thiệu một giải mã dựa trên cây hiệu quả cho các đầu ra SAR, cho phép sinh bản thảo và xác minh đồng thời, từ đó loại bỏ sự cần thiết cho các bước xác thực bổ sung hoặc các mô hình bên ngoài.

• BiTA hoạt động như một mô-đun plug-and-play, có thể áp dụng cho bất kỳ LLM dựa trên transformer công khai nào mà không làm thay đổi các đầu ra gốc. Với sự hỗ trợ của BiTA, LLaMA-2-70B-Chat đạt được tăng tốc 2.7× trên benchmark MT-Bench, vượt trội các kỹ thuật tiên tiến trong các thí nghiệm mở rộng (Một so sánh ngắn gọn được trình bày trong Hình 1).

2 Công trình liên quan

2.1 Gia tốc LLM
Gia tốc LLM có thể được tiếp cận thông qua các chiều khác nhau, bao gồm nén mô hình [Hinton et al., 2015; Liu et al., 2018], đơn giản hóa kiến trúc [Dao et al., 2022], lượng tử hóa [Gholami et al., 2022], quản lý bộ nhớ [Kwon et al., 2023], tối ưu hóa kernel [Wang et al., 2021], lập lịch suy luận [Kwon et al., 2023], giải mã hiệu quả [Santilli et al., 2023], và nhiều hơn nữa. Những kỹ thuật này trải dài từ các sửa đổi thuật toán tiên tiến đến các thay đổi đột phá trong thiết kế hệ thống, tìm thấy ứng dụng rộng rãi trong các tình huống thực tế [Miao et al., 2023a].

Trong bài báo này, một sự nhấn mạnh cụ thể được đặt vào giải mã SAR như một trong những phương pháp điển hình cho giải mã hiệu quả. Giải mã SAR, được phát triển từ giải mã không tự hồi quy (NAR) [Gu et al., 2018], ban đầu được giới thiệu cho dịch máy [Stern et al., 2018]. Nó khác biệt với mô hình sinh AR thông thường bằng cách giải mã các token đầu ra song song, với mục tiêu đạt được chất lượng đầu ra AR thông qua các chiến lược hậu xử lý [Xiao et al., 2023].

2.2 Giải mã Suy đoán
Giải mã suy đoán nổi bật như một phương pháp giải mã hiệu quả điển hình khác, liên quan đến việc dự đoán phân phối token của các mô hình AR tương ứng theo cách suy đoán. Một phương pháp sớm [Stern et al., 2018] sinh ra các dự đoán tương lai như bản thảo bằng các đầu dự đoán phụ trợ, sau đó xác thực chúng bằng một mô hình chấm điểm. Các nghiên cứu gần đây [Leviathan et al., 2023; Chen et al., 2023] sử dụng các mô hình bản thảo bên ngoài để lấy mẫu phân phối token từ các mô hình đích lớn. SpecDec [Xia et al., 2023] khám phá các nguyên tắc thiết kế cho các mô hình bản thảo hiệu quả. OSD [Liu et al., 2023b] tăng cường các mô hình bản thảo thông qua đào tạo lại trực tuyến.

Không sử dụng các mô hình bản thảo bên ngoài, SPEED [Hooper et al., 2023] thiết kế một pipeline suy đoán nhanh hơn với chia sẻ tham số vòng. Trong khi Self-SpecDec [Zhang et al., 2023] đẩy nhanh việc tạo bản thảo bằng cách chọn lọc bỏ qua các lớp trung gian cụ thể. Medusa [Cai et al., 2023] áp dụng nhiều đầu dự đoán bổ sung, tương tự như tài liệu [Stern et al., 2018]. PaSS [Monea et al., 2023] thu được các bản thảo SAR bằng phương tiện "look-ahead" embedding. REST [He et al., 2023] sử dụng việc truy xuất kiến thức. Lookahead [Fu et al., 2023] dựa hoàn toàn vào n-gram được sinh ra bởi LLM như các ứng viên bản thảo suy đoán. Tối ưu hóa xác minh là một cách khác. SpecInfer [Miao et al., 2023b] sử dụng một cây token ứng viên bản thảo cho xác minh song song. SSD [Spector và Re, 2023] tái cấu trúc các bản thảo thành một cây và tiến hành giải mã batch. SpecTr [Sun et al., 2023] tìm kiếm một sự đánh đổi tối ưu giữa nhiều ứng viên bản thảo hơn và chi phí liên quan.

Phương pháp của chúng tôi thuộc về giải mã suy đoán hoạt động mà không có các mô hình bản thảo bên ngoài. Nghiên cứu gần đây, Medusa [Cai et al., 2023], có điểm tương đồng với BiTA trong việc sinh ra các token tương lai mà không làm thay đổi các tham số mô hình gốc. Tuy nhiên, một sự khác biệt đáng chú ý nằm ở cấu trúc: BiTA sử dụng soft embedding, trong khi Medusa sử dụng nhiều đầu. Một nghiên cứu gần đây khác phù hợp chặt chẽ với BiTA là PaSS [Monea et al., 2023], sử dụng "look-ahead" embedding (được gọi là "mask token" trong BiTA) cho các dự đoán tương lai, trong khi BiTA kết hợp các prompt token bổ sung, điều này chứng minh có lợi trong các thí nghiệm. Hơn nữa, cả hai công trình đều yêu cầu gọi mô hình lần thứ hai để xác thực các ứng viên bản thảo, trong khi BiTA một cách liền mạch tiến hành sinh suy đoán và xác minh.

2.3 Điều chỉnh Prompt
Như một kỹ thuật điều chỉnh hiệu quả tham số (PET) được áp dụng rộng rãi, Điều chỉnh Prompt [Lester et al., 2021], cùng với các phương pháp khác nhau tiếp theo [Li và Liang, 2021; Liu et al., 2023a], tối ưu hóa các transformer đã được đào tạo trước bằng cách cập nhật một tập hợp tối thiểu các prompt token, tăng cường tùy chỉnh mô hình cho các nhiệm vụ, lĩnh vực hoặc yêu cầu cụ thể. Trong nghiên cứu này, chúng tôi tận dụng lợi ích của điều chỉnh prompt, giới thiệu deep soft prompting từ prefix tuning [Li và Liang, 2021] để thích ứng hiệu quả các mô hình ngôn ngữ AR cho giải mã SAR mà không sửa đổi các tham số mô hình gốc.

3 Phương pháp

Trong phần này, chúng tôi giới thiệu BiTA, một phương pháp đổi mới cho gia tốc LLM không mất mát. Kết hợp điều chỉnh hai chiều được đề xuất, BiTA cho phép sự thích ứng liền mạch của một mô hình AR dựa trên transformer để có được phong cách sinh SAR thông qua điều chỉnh hiệu quả. Ngoài ra, chúng tôi phát triển một chiến lược sinh và xác minh được tinh giản được tạo điều kiện bởi một cơ chế attention dựa trên cây hiệu quả cho suy luận không mất mát. Các chi tiết phức tạp của hai thành phần then chốt này được giải thích trong các phần phụ tiếp theo.

3.1 Điều chỉnh Hai chiều

Nhờ các kiến trúc transformer của LLM, chúng tôi tận dụng nhiều token placeholder có thể học được được gọi là mask token, trao quyền cho các mô hình ngôn ngữ để sinh ra các token tương lai liên tiếp tiếp theo vượt ra ngoài token đầu vào cuối cùng. Trong quá trình đào tạo và suy luận, các mask token được giao nhiệm vụ tạo ra xác suất của token tiếp theo của chúng ở các vị trí tương ứng.

Để duy trì tính nhất quán với các đầu ra AR, chúng tôi chọn giữ các tham số mô hình gốc không thay đổi. Rút ra cảm hứng từ ý tưởng điều chỉnh prompt [Li và Liang, 2021], chúng tôi sử dụng các prefix soft prompt token trong các LLM đông cứng, hợp tác với các mask token mới được thêm vào để đạt được sinh SAR (tham khảo Hình 2 cho một sơ đồ minh họa). Ngược lại hoàn toàn, thiết kế của chúng tôi sử dụng một cơ chế attention riêng biệt, cụ thể cho các mask token, tách biệt khỏi các token đầu vào thông thường. Như được minh họa trong Hình 3, attention riêng biệt cho phép prefix prompting chỉ ảnh hưởng đến các mask token chứ không phải tất cả các token ở bên phải của chúng, từ đó tránh các thay đổi đối với các đầu ra của các token đầu vào gốc. Ở một mức độ nào đó, các prompt token phục vụ như một dạng soft prompting chuyên biệt, kích hoạt các mô hình AR để có được khả năng sinh SAR.

Trong quá trình đào tạo, chúng tôi thực hiện điều chỉnh hai chiều trong dữ liệu hướng dẫn tự sinh giống SFT, kết hợp một hàm mất mát SAR. Tiếp theo, chúng tôi mô tả cách tạo ra dữ liệu đào tạo SAR và sau đó giới thiệu mất mát được sử dụng. Cho một chuỗi token câu hỏi X được chuẩn bị trước, chúng tôi sử dụng LLM dự định gia tốc để sinh ra chuỗi token trả lời Y={y0, y1, ..., yN} bằng lấy mẫu tham lam. Do đó, một câu hỏi và câu trả lời tự sinh tương ứng tạo thành một mẫu đào tạo. Trong một giải thích chi tiết về đào tạo, xem xét số lượng mask token là M, chúng tôi lấy mẫu ngẫu nhiên một chỉ số k từ phạm vi (0, N-M). Tiếp theo, chúng tôi hình thành chuỗi token câu hỏi X bằng cách nối nó với Ykinst={yi, i≤k} như một hướng dẫn nhiệm vụ SAR. Trong khi Yk,Mgt={yi, k+1≤i≤k+M} tiếp theo phục vụ như ground truth của nó. Theo quá trình sản xuất dữ liệu ở trên, chúng tôi dễ dàng sản xuất một lượng lớn dữ liệu đào tạo sử dụng các chuỗi câu hỏi được chuẩn bị trước từ nhiều bộ dữ liệu đào tạo SFT. Dựa trên dữ liệu đào tạo SAR, công thức của mất mát SAR như sau:

LSAR=−Σi=k+1k+M logP(yi|X, Ykinst;θ, ϕp, ϕm),    (1)

trong đó yi là nhãn của các mask token, được rút ra từ Yk,Mgt, θ biểu thị các tham số mô hình đông cứng, và ϕp, ϕm biểu thị các embedding của prompt và mask token, tương ứng.

Điều quan trọng cần nhấn mạnh việc sử dụng dữ liệu đào tạo tự sinh, đảm bảo rằng phân phối của các token đầu ra SAR phù hợp chặt chẽ với phân phối của các đầu ra AR gốc của LLM được gia tốc. Chiến lược đơn giản nhưng hiệu quả này thúc đẩy các dự đoán chính xác, tăng cường khả năng "đoán" các token tương lai một cách chính xác của mô hình.

3.2 Sinh & Xác minh Được tinh giản

BiTA đơn giản hóa sinh SAR và xác minh trong một lần forward pass duy nhất trong một mô hình phổ quát. Để minh họa, trước tiên chúng tôi trình bày một ví dụ đơn giản về quá trình sinh và xác minh được tinh giản. Sau đó chúng tôi tăng cường nó với giải mã hiệu quả hơn kết hợp một cơ chế attention dựa trên cây lấy cảm hứng từ tài liệu [Miao et al., 2023b].

Một ví dụ minh họa của giải mã đơn giản được mô tả trong Hình 4, và chúng tôi mô tả quá trình như sau. Cho một chuỗi token đầu vào X0={x0, x1, ..., xN-1}, trong lần forward pass đầu tiên, mô hình đồng thời xuất ra dự đoán của token tiếp theo ŷN và M dự đoán tương lai Ĉ1={ŷN+1, ŷN+2, ..., ŷN+M}. Như một đầu ra AR đáng tin cậy dựa trên ngữ cảnh của X0, ŷN chắc chắn được chấp nhận, trong khi Ĉ1 phục vụ như các ứng viên token bản thảo dự định cho xác minh trong lần forward pass tiếp theo. Sau đó, trong lần forward pass thứ i (i>1), chuỗi đầu vào bao gồm các token truy vấn, các token được sinh ra trước đó, các ứng viên token bản thảo Ĉi-1 (được bao quanh trong các hộp đứt nét màu cam trong Hình 4), và các mask token. Các ứng viên token bản thảo được xác minh tuần tự dựa trên dự đoán AR của token cuối cùng. Các ứng viên bản thảo được chấp nhận được thêm vào chuỗi token được sinh ra trước đó. Nếu một ứng viên bản thảo bị từ chối, dự đoán của nó, cùng với các tiếp theo của nó, bị loại bỏ, và dự đoán của ứng viên được chấp nhận cuối cùng phục vụ như đầu ra token AR được chấp nhận. Đối với sinh bản thảo, mỗi mask token xuất ra dự đoán của token tiếp theo của nó ở các vị trí tương ứng. Lưu ý rằng chúng không "attend" đến các ứng viên token bản thảo hiện tại, sinh ra các dự đoán tương lai trong ngữ cảnh của chỉ cả các token truy vấn và các token được sinh ra trước đó. Các token tương lai được dự đoán được sử dụng trong lần forward pass tiếp theo để xác minh.

Phương pháp sinh và xác minh được tinh giản đơn giản gặp phải hai thách thức khó khăn. Một mặt, tính đúng đắn của các ứng viên token bản thảo không phải lúc nào cũng được đảm bảo. Như đã nêu, các ứng viên token bản thảo được dự đoán dựa trên ngữ cảnh của chuỗi đầu ra cho đến lần forward pass cuối cùng, thay vì kết hợp các token bản thảo hiện được chấp nhận. Mặt khác, số lượng ứng viên token bản thảo có thể rất ít, vì một số ứng viên token bản thảo không còn hiệu quả do được chấp nhận trong lần forward pass cuối cùng, hoặc thậm chí bằng không nếu tất cả ứng viên được chấp nhận trong lần forward pass cuối cùng. Hai thiếu sót này trở thành những trở ngại đáng kể cho hiệu quả giải mã. Hơn nữa, trong khi các ứng viên token bản thảo hiện được rút ra từ từ có điểm số top-1 của các dự đoán tương lai, khám phá các từ khả thi bổ sung cho mỗi dự đoán có thể làm tăng xác suất chấp nhận, dẫn đến tăng tốc suy luận cao hơn [Xia et al., 2023].

Để khắc phục những thiếu sót được xác định này, chúng tôi trình bày một phương pháp giải mã dựa trên cây hiệu quả sử dụng một cơ chế attention tinh xảo để đảm bảo một số lượng đầy đủ và tính đúng đắn của các ứng viên token bản thảo. Đối với mỗi mask token, chúng tôi chọn các từ được dự đoán top-k như các ứng viên bản thảo ở các vị trí tương ứng. Tất cả các nhóm ứng viên bản thảo top-k có thể được tổ chức thành một cây token, trong đó mỗi nút lá chỉ ra một chuỗi được sinh có thể để xác minh. Bằng cách khám phá phân phối của các token tương lai đầu ra, chúng tôi quan sát rằng từ có điểm số top-1 đóng vai trò quan trọng nhất trong các dự đoán tiếp theo: ứng viên có khả năng cao nhất có cơ hội cao hơn tạo thành một chuỗi hiệu quả với các từ tiếp theo. Do đó, thay vì xây dựng một cây đầy đủ để khám phá tất cả các chuỗi ứng viên có thể, chúng tôi xây dựng một cây token hiệu quả đủ đại diện, tập trung vào những chuỗi mà các nút không phải lá trong cây token là ứng viên có điểm số top-1, như được minh họa trong Hình 5.

Hơn nữa, chúng tôi đính kèm một nhóm mask token vào mỗi nút trong cây token hiệu quả. Điều này đảm bảo rằng bất kể số lượng ứng viên bản thảo hiện được chấp nhận, chúng tôi luôn có thể chọn cùng một số lượng mask token theo sau token ứng viên được chấp nhận cuối cùng. Các mask token được chọn được sử dụng để tổ chức cây token ứng viên bản thảo cho bước tiếp theo. Bằng cách có các mask token được chọn dự đoán các token tương lai dựa trên ngữ cảnh của các token ứng viên được chấp nhận, có tiềm năng cải thiện độ chính xác. Việc cải thiện này được kỳ vọng sẽ dẫn đến hiệu quả cao hơn cho giải mã, như được chứng minh trong Phần 4.3.

Để có được một đầu vào tuần tự, chúng tôi làm phẳng cây token ứng viên bản thảo, cùng với các mask token đính kèm, thành một chuỗi token. Chuỗi này sau đó được nối vào chuỗi token kết quả hiện tại bao gồm các token truy vấn và các token được sinh ra trước đó, để xây dựng đầu vào mô hình. Trong quá trình suy luận, các token ứng viên bản thảo và các mask token đính kèm "attend" đến cả "tổ tiên" của chúng trong cây và các token kết quả hiện tại, trong khi các mask token "attend" đến các prompt token bổ sung. Nhờ attention dựa trên cây, sinh AR và SAR, cũng như xác minh bản thảo, được tiến hành trong một lần forward pass duy nhất đồng thời. Một minh họa không đầy đủ nhưng đủ biểu cảm của attention dựa trên cây được cung cấp trong Hình 6 (ví dụ được hiển thị được rút ra từ cây token được mô tả trong Hình 5).

Trong phần phụ này, chúng tôi phác thảo phương pháp giải mã được tinh giản từng bước. Bắt đầu với một ví dụ đồ chơi đơn giản được đơn giản hóa, chúng tôi giới thiệu khái niệm sinh và xác minh được tinh giản song song. Để cải thiện tỷ lệ chấp nhận của các bản thảo, chúng tôi thiết kế một cây token ứng viên bản thảo top-k hiệu quả với các nhóm mask token theo ứng viên. Cuối cùng, chúng tôi sử dụng một cơ chế attention dựa trên cây để đạt được sinh và xác minh được tinh giản. Mặc dù giới thiệu độ phức tạp tính toán bổ sung với nhiều token được sử dụng trong suy luận, chiến lược này vẫn đạt được tăng tốc ấn tượng trong LLM. Bởi vì nút thắt cổ chai của suy luận LLM thường là chi phí memory-bandwidth [Shazeer, 2019] chứ không phải chi phí tính toán. Ngoài ra, chúng tôi khám phá kích thước của cây token ứng viên bản thảo, đạt được sự đánh đổi chấp nhận-độ phức tạp thuận lợi (chi tiết trong Phần 4.3).

4 Thí nghiệm

4.1 Thiết lập thí nghiệm

Bộ dữ liệu
Như được mô tả trong Phần 3.1, chúng tôi sử dụng dữ liệu đào tạo tự sinh giống SFT, bao gồm các câu hỏi được chuẩn bị trước và các câu trả lời được sinh ra bởi LLM để gia tốc. Để đa dạng, các câu hỏi được chuẩn bị trước được lấy từ năm bộ dữ liệu SFT: LIMA [Zhou et al., 2023], Alpaca-GPT4 [Peng et al., 2023], CodeAlpaca [Chaudhary, 2023], OpenPlatypus [Lee et al., 2023], và CIP [Palla, 2023]. Lưu ý rằng chúng tôi sử dụng một tập mẫu 50k từ tập đào tạo CIP trong quá trình đào tạo, dẫn đến khoảng 150k mẫu đào tạo tổng cộng.

Để đánh giá, chúng tôi sử dụng bốn bộ dữ liệu: XSum [Narayan et al., 2018], MT-Bench [Zheng et al., 2023a], tập kiểm tra CIP, và HumanEval-X [Zheng et al., 2023b]. Đánh giá này bao gồm khả năng gia tốc qua tóm tắt, câu hỏi mở, hội thoại, và mã, tương ứng.

Chi tiết Triển khai
Thí nghiệm được tiến hành trên LLM có kích thước khác nhau, bao gồm dòng mô hình chat LLaMA-2 (7B, 13B, 70B) [Touvron et al., 2023], dòng Vicuna (7B, 13B, 33B) [Chiang et al., 2023], và Falcon-40B-Chat [Almazrouei et al., 2023], từ 7B đến 70B tham số. Để tối ưu hóa, một lịch trình tỷ lệ học cosine được sử dụng với giá trị ban đầu 3e-2 và kích thước batch 128 cho 4 epoch. Các embedding có thể học được được khởi tạo với phân phối bình thường có mean bằng không và độ lệch chuẩn 0.02. Trừ khi được chỉ định khác, số lượng prompt token được thiết lập ở 16, với mask token được đặt thành 3 cho các mô hình có kích thước tham số không vượt quá 13B và được điều chỉnh thành 4 nếu không. Đào tạo được tiến hành trên một cụm bốn máy chủ, mỗi máy được trang bị tám GPU NVIDIA A800 (80GB). Đối với một mô hình cơ sở có quy mô 7B tham số, quá trình đào tạo mất khoảng 7 giờ. Ngoài ra, đối với giải mã dựa trên cây hiệu quả của chúng tôi, 5 dự đoán hàng đầu trong mỗi mask token được chọn làm ứng viên token bản thảo.

Thiết lập Đánh giá
Chúng tôi tiến hành đánh giá BiTA được đề xuất và các phương pháp so sánh trên GPU NVIDIA A800 (80GB). Đối với các mô hình có kích thước tham số không lớn hơn 13B, một GPU A800 được sử dụng; nếu không, 8 GPU A800 được sử dụng. Các mô hình được đánh giá thực hiện suy luận với kích thước batch bằng 1 để đảm bảo đánh giá chính xác thời gian phản hồi. Để tạo điều kiện so sánh, chúng tôi sử dụng "greedy speedup" làm thước đo [Cai et al., 2023], được định nghĩa là tỷ lệ tốc độ của mô hình được đánh giá sử dụng lấy mẫu tham lam so với baseline AR, với tốc độ được đo bằng token được sinh ra mỗi giây. Baseline AR được triển khai sử dụng mã sinh AR trong thư viện Huggingface Transformers [Wolf et al., 2020].

4.2 Kết quả Chính

Tăng tốc Qua Các LLM và Nhiệm vụ Đa dạng
Bảng 1 trình bày tăng tốc của BiTA được đề xuất qua bốn bộ dữ liệu: XSum, MT-Bench, CIP, và HumanEval-X. Khi BiTA được áp dụng, các LLM được đẩy nhanh thể hiện tăng tốc từ 2.1× đến 3.3× qua các nhiệm vụ sinh khác nhau, bao gồm tóm tắt, câu hỏi mở, hội thoại, và mã. Đáng chú ý, các LLM lớn hơn có xu hướng thể hiện tăng tốc đáng kể hơn, có thể được quy cho ngữ cảnh phong phú hơn được mã hóa bởi các embedding cho mỗi token, tạo điều kiện cho các dự đoán tương lai được cải thiện. Một giả thuyết khác là các phương pháp dựa trên prompting của chúng tôi hưởng lợi từ các mô hình lớn hơn, phù hợp với các quan sát tương tự trong các nhiệm vụ NLP được thảo luận trong các công trình khác về điều chỉnh hiệu quả tham số [Lester et al., 2021; Liu et al., 2023a]. Ngoài ra, đáng chú ý rằng BiTA đạt được tăng tốc đặc biệt ấn tượng (2.7∼3.3×) trong sinh mã so với các nhiệm vụ khác. Khi kiểm tra các mẫu kiểm tra, chúng tôi giả thuyết rằng nội dung có cấu trúc và logic trong các nhiệm vụ sinh mã có thể đóng vai trò quan trọng trong việc tăng cường các dự đoán tương lai. Ngoài hiệu suất tăng tốc đáng chú ý, chúng tôi quan sát rằng đối với một mô hình cơ sở quy mô 7B, số lượng tham số có thể đào tạo cho các embedding prompt và mask là khoảng 0.06% tổng tham số mô hình, trong khi nó ít hơn 10 lần đối với các mô hình quy mô 70B.

So sánh với Giải mã Suy đoán
Chúng tôi so sánh hiệu suất tăng tốc của phương pháp chúng tôi với một số phương pháp dựa trên giải mã suy đoán tiên tiến trong Bảng 2. Đánh giá được tiến hành trên MT-Bench, sử dụng cả LLaMA-2-7B và LLaMA-2-70B. Để so sánh công bằng, chúng tôi triển khai bốn phương pháp so sánh sử dụng mã nguồn công khai trong cùng môi trường phần cứng như BiTA của chúng tôi. Vì bốn phương pháp so sánh này không yêu cầu đào tạo hoặc tinh chỉnh đặc biệt, chúng tôi có thể dễ dàng tái triển khai chúng trong môi trường thí nghiệm của chúng tôi. Tuy nhiên, đối với HF AssistGen [Joao Gante, 2023] và SpecDec [Leviathan et al., 2023], chúng tôi gặp khó khăn trong việc có được một mô hình bản thảo nhỏ gọn với phân phối token đầu ra tương tự như mô hình đích, do đó không đáp ứng được yêu cầu gia tốc cho LLaMA-2-7B, và chúng tôi không thể cung cấp các kết quả tương ứng. Để đẩy nhanh LLaMA-2-70B, chúng tôi sử dụng LLaMA-2-7B làm mô hình bản thảo cho chúng. Như được hiển thị trong Bảng 2, BiTA đạt được tăng tốc lên đến 2.7×, vượt trội đáng kể các phương pháp so sánh.

Ngoài việc so sánh với bốn phương pháp giải mã suy đoán được đề cập ở trên, chúng tôi cũng đánh giá BiTA so với một nghiên cứu gần đây, Medusa [Cai et al., 2023], vì động lực tương tự với phương pháp của chúng tôi cho sinh và xác minh SAR. Chúng tôi áp dụng BiTA cho Vicuna-7B, 13B, và 33B trên MT-Bench để so sánh hiệu suất tăng tốc được báo cáo trong Medusa. Hình 7 chứng minh rằng phương pháp của chúng tôi vượt trội Medusa với cải thiện từ 19∼32% qua các mô hình cơ sở khác nhau (mặc dù điều quan trọng cần lưu ý rằng khoảng cách giữa hai phương pháp có thể không hoàn toàn đáng tin cậy do những khác biệt tiềm ẩn trong thiết lập thí nghiệm). Chúng tôi quy sự vượt trội của phương pháp chúng tôi cho điều chỉnh hai chiều mạnh mẽ, trong đó các mask token có thể nắm bắt ngữ cảnh đặc trưng phong phú hơn trong quá trình forward pass. Hơn nữa, chiến lược sinh và xác minh đồng thời cũng đóng góp vào việc gia tốc.

4.3 Nghiên cứu Ablation

Tác động của Thiết kế Prompting
Để xác thực hiệu quả của thiết kế điều chỉnh hai chiều, chúng tôi khám phá tác động của các thiết kế prompting khác nhau, bao gồm không prompting, điều chỉnh prompt nông, và điều chỉnh prompt sâu. Các thí nghiệm so sánh được tiến hành sử dụng LLaMA-2-7B cho các nhiệm vụ sinh hội thoại (CIP) và mã (HumanEval-X). Như được chỉ ra trong Bảng 3, chúng tôi quan sát một sự gia tăng tăng tốc tuần tự. Khi chỉ sử dụng mask token cho dự đoán tương lai, một tăng tốc thấp hơn đáng kể (1.94× trên CIP và 2.12× trên HumanEval-X) được quan sát. Tuy nhiên, khi thêm prompt token như các embedding mềm có thể học được trong chuỗi đầu vào [Lester et al., 2021], được gọi là điều chỉnh prompt nông, hiệu suất tăng tốc cải thiện nhẹ (+0.07 trên CIP và +0.19 trên HumanEval-X), nhưng nó không đạt được mức độ đạt được với điều chỉnh hai chiều (2.29× trên CIP và 2.73× trên HumanEval-X), trong đó điều chỉnh soft prompt được áp dụng trong mọi khối transformer.

Tăng tốc vs. Prompt Token
Rõ ràng, sử dụng nhiều prompt token hơn liên quan đến nhiều tham số mô hình có thể đào tạo hơn, dẫn đến sự gia tăng nhẹ trong độ phức tạp tính toán trong quá trình suy luận. Chúng tôi khám phá ảnh hưởng của số lượng prompt token để hiểu ảnh hưởng của nó đến tốc độ. Nghiên cứu của chúng tôi tập trung vào LLaMA-2-7B qua bốn bộ dữ liệu: XSum, MT-Bench, CIP, và HumanEval-X. Trong Hình 8, chúng tôi quan sát rằng tăng tốc tăng gần như với số lượng prompt token được sử dụng, nhất quán qua tất cả bốn bộ dữ liệu, và đạt bão hòa ở 16. Thú vị, tăng tốc với 8∼64 prompt token không khác biệt đáng kể. Điều này nhấn mạnh rằng thông qua điều chỉnh hai chiều, một khi một số lượng ngưỡng cụ thể của các embedding prompt sâu có thể học được được kết hợp, các mô hình AR có thể được thích ứng hiệu quả thành phong cách SAR.

Tăng tốc vs. Mask Token
Tăng số lượng mask token cung cấp nhiều ứng viên token bản thảo hơn, tăng cường tiềm năng sinh ra các token đầu ra bổ sung cho giải mã SAR trong một bước forward duy nhất. Tuy nhiên, số lượng ứng viên bản thảo cao hơn giới thiệu overhead tính toán nặng hơn, làm chậm suy luận mô hình. Một nghiên cứu ablation khám phá tác động của việc thay đổi số lượng mask token, như được trình bày trong Hình 9. Các thí nghiệm được tiến hành sử dụng LLaMA-2-7B. Quan sát tăng tốc qua bốn bộ dữ liệu cho thấy rằng chọn 3 hoặc 4 mask token đạt được sự đánh đổi thuận lợi giữa khả năng giải mã SAR và overhead tính toán. Do đó, thông qua tìm kiếm thí nghiệm toàn diện, chúng tôi đặt số lượng mask token thành 3 cho các mô hình có không quá 13 tỷ tham số và thành 4 cho các mô hình lớn hơn, nhằm đạt tăng tốc cao nhất qua các nhiệm vụ sinh đa dạng.

Sự Vượt trội của Giải mã Dựa trên Cây Hiệu quả
Như được giải thích trong Phần 3.2, chúng tôi phát triển giải mã dựa trên cây hiệu quả bắt đầu từ một phương pháp giải mã đơn giản được đơn giản hóa. Để chứng minh hiệu quả của phương pháp được đề xuất, chúng tôi so sánh giải mã dựa trên cây hiệu quả với giải mã đơn giản và giải mã dựa trên cây đầy đủ thông thường. Tăng tốc đạt được với các phương pháp giải mã khác nhau dựa trên LLaMA-2-7B được trình bày trong Bảng 4. Đối với giải mã dựa trên cây đầy đủ, chúng tôi tiến hành khám phá toàn diện số k cho các dự đoán top-k trong mỗi mask token, như các ứng viên bản thảo. Các kết quả tăng tốc cho thấy rằng giải mã với một cây token hiệu quả nhất quán vượt trội giải mã với một cây token đầy đủ trong tất cả các trường hợp k trên cả XSum và MT-Bench, vượt trội giải mã đơn giản với cải thiện vượt quá 30%.

5 Kết luận

Chúng tôi trình bày một phương pháp mới có tên BiTA để đạt được gia tốc không mất mát trong LLM. Để giảm các lời gọi transformer trong LLM tự hồi quy trong quá trình suy luận, BiTA một cách liền mạch thích ứng các mô hình AR hiện có cho phong cách sinh SAR thông qua điều chỉnh hai chiều được đề xuất, sử dụng các tham số có thể đào tạo rất hạn chế. Dựa trên chiến lược giải mã hiệu quả dựa trên cây, mô hình tiến hành sinh và xác minh được tinh giản đồng thời. Hai đặc điểm này của BiTA cùng đóng góp vào việc đẩy nhanh LLM mà không làm thay đổi các đầu ra gốc. Các kết quả thí nghiệm mở rộng chứng minh tăng tốc đáng chú ý từ 2.1× đến 3.3× qua các LLM có kích thước khác nhau và các nhiệm vụ sinh đa dạng. Hơn nữa, do thiết kế prompting linh hoạt, BiTA phục vụ như một kỹ thuật plug-and-play có thể áp dụng cho bất kỳ LLM công khai nào để gia tốc, điều này có ý nghĩa lớn trong các tình huống hạn chế tài nguyên và ứng dụng thời gian thực.
