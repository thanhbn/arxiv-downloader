# Mở khóa Hiệu quả trong Suy luận Mô hình Ngôn ngữ Lớn:
Một Khảo sát Toàn diện về Giải mã Đầu cơ

Heming Xia1, Zhe Yang2, Qingxiu Dong2, Peiyi Wang2,
Yongqi Li1,Tao Ge3,Tianyu Liu4,Wenjie Li1,Zhifang Sui2
1Khoa Tin học, Đại học Bách khoa Hồng Kông
2Phòng thí nghiệm Trọng điểm Quốc gia về Xử lý Thông tin Đa phương tiện, Đại học Bắc Kinh
3Microsoft Research Asia4Alibaba Group
{he-ming.xia}@connect.polyu.hk; {yz_young}@pku.edu.cn

Tóm tắt
Để giảm thiểu độ trễ suy luận cao phát sinh từ giải mã tự hồi quy trong Mô hình Ngôn ngữ Lớn (LLM), Giải mã Đầu cơ đã nổi lên như một mô hình giải mã mới cho suy luận LLM. Trong mỗi bước giải mã, phương pháp này đầu tiên soạn thảo hiệu quả một số token tương lai rồi xác minh chúng song song. Khác với giải mã tự hồi quy, Giải mã Đầu cơ tạo điều kiện cho việc giải mã đồng thời nhiều token mỗi bước, từ đó tăng tốc suy luận. Bài báo này trình bày một tổng quan và phân tích toàn diện về mô hình giải mã đầy hứa hẹn này. Chúng tôi bắt đầu bằng việc cung cấp định nghĩa chính thức và công thức hóa Giải mã Đầu cơ. Sau đó, chúng tôi tổ chức các cuộc thảo luận sâu sắc về các khía cạnh chính của nó, như lựa chọn trình soạn thảo và chiến lược xác minh. Hơn nữa, chúng tôi trình bày phân tích so sánh các phương pháp hàng đầu trong môi trường thử nghiệm của bên thứ ba. Chúng tôi mong muốn công trình này sẽ đóng vai trò như chất xúc tác cho nghiên cứu sâu hơn về Giải mã Đầu cơ, cuối cùng góp phần vào suy luận LLM hiệu quả hơn.

1 Giới thiệu
Mô hình Ngôn ngữ Lớn (LLM) đã đạt được trình độ xuất sắc trong một loạt các nhiệm vụ hạ lưu (OpenAI, 2023; Touvron et al., 2023a,b; Chiang et al., 2023; Jiang et al., 2023). Chúng đang tiến hóa dần trở thành nền tảng của các giao diện API toàn diện (ví dụ: ChatGPT2), cung cấp dịch vụ cuộc sống con người và hướng dẫn thông qua tương tác người-máy thời gian thực. Tuy nhiên, độ trễ suy luận của những mô hình lớn này đã nổi lên như một rào cản đáng kể hạn chế các ứng dụng rộng rãi hơn của chúng. Độ trễ này chủ yếu phát sinh từ việc sinh token từng token được đòi hỏi bởi giải mã tự hồi quy, dẫn đến sự gia tăng độ trễ suy luận với cả chiều dài của chuỗi được sinh ra và quy mô của mô hình.

1Các bài báo có liên quan sẽ được cập nhật thường xuyên tại https://github.com/hemingkx/SpeculativeDecodingPapers .
2https://chat.openai.com

Xác minh Song song
Soạn thảo Hiệu quả
Giải mã Tự hồi quy

Hình 1: Trái ngược với giải mã tự hồi quy (trái) sinh tuần tự, Giải mã Đầu cơ (phải) đầu tiên soạn thảo hiệu quả nhiều token rồi xác minh chúng song song bằng LLM đích. Các token được soạn thảo sau vị trí phân nhánh (ví dụ: ) sẽ bị loại bỏ để đảm bảo chất lượng sinh.

Để tăng tốc suy luận LLM, một mô hình suy luận sáng tạo, Giải mã Đầu cơ đã được giới thiệu (Stern et al., 2018; Xia et al., 2023; Leviathan et al., 2023; Chen et al., 2023a). Như được hiển thị trong Hình 1, trong mỗi bước giải mã, Giải mã Đầu cơ đầu tiên soạn thảo hiệu quả nhiều token như sự suy đoán về các bước giải mã tương lai của LLM đích và sau đó sử dụng LLM để xác minh tất cả các token đã soạn thảo song song. Chỉ những token đáp ứng tiêu chí xác minh của LLM mới được chấp nhận làm đầu ra cuối cùng để đảm bảo chất lượng sinh.

Giải mã Đầu cơ được dựa trên hai quan sát chính về suy luận LLM: 1) nhiều token dễ có thể được dự đoán với chi phí tính toán ít hơn (ví dụ: sử dụng mô hình nhỏ hơn), và 2) suy luận LLM bị ràng buộc mạnh bởi băng thông bộ nhớ (Patterson, 2004; Shazeer, 2019) với nút thắt độ trễ chính phát sinh từ việc đọc/ghi bộ nhớ của các tham số LLM chứ không phải từ các tính toán số học. Dựa trên những quan sát này, Giải mã Đầu cơ áp dụng khái niệm thực thi đầu cơ3 để tập trung nỗ lực của LLM vào việc xác nhận các token đã được soạn thảo trước, giảm đáng kể nhu cầu thao tác bộ nhớ thường xuyên của các tham số LLM, từ đó cải thiện hiệu quả suy luận.

3Thực thi đầu cơ (Burton, 1985; Hennessy và Patterson, 2012) là một kỹ thuật tối ưu hóa được sử dụng trong kiến trúc máy tính nơi các tác vụ được thực hiện trước và sau đó được xác minh về sự cần thiết của chúng, từ đó tránh được các độ trễ vốn có trong thực thi tác vụ tuần tự.

Mặc dù Giải mã Đầu cơ cho thấy triển vọng, nó đặt ra một số câu hỏi quan trọng cần được điều tra thêm. Ví dụ, làm thế nào để thiết kế một trình soạn thảo tối ưu để đạt được sự cân bằng giữa độ chính xác suy đoán và hiệu quả soạn thảo (Xia et al., 2023; Zhou et al., 2023; Li et al., 2024). Ngoài ra, điều cần thiết là đánh giá liệu tiêu chí xác minh có thể duy trì cả tính song song sinh và chất lượng đầu ra (Miao et al., 2024; Cai et al., 2024). Hơn nữa, vì các phương pháp hiện tại được đánh giá trong các điều kiện thử nghiệm khác nhau, cần có một chuẩn mực thống nhất để tạo điều kiện cho kỳ vọng tăng tốc thực tế trong cộng đồng nghiên cứu.

Giữa sự mở rộng nhanh chóng của nghiên cứu trong Giải mã Đầu cơ, công trình này thực hiện nỗ lực đầu tiên để trình bày một khảo sát về lĩnh vực này, nhằm nâng cao nhận thức trong giới học thuật về những tiến bộ mới nhất. Chúng tôi cung cấp phân loại hệ thống nghiên cứu hiện tại và phân tích sâu sắc các nghiên cứu liên quan. Hơn nữa, chúng tôi giới thiệu Spec-Bench, một chuẩn mực toàn diện để đánh giá các phương pháp Giải mã Đầu cơ trong các kịch bản ứng dụng đa dạng.

Những đóng góp của chúng tôi có thể được tóm tắt như sau:
(1) Khảo sát đầu tiên: Theo hiểu biết của chúng tôi, chúng tôi là những người đầu tiên trình bày khảo sát toàn diện về Giải mã Đầu cơ;
(2) Định nghĩa chính thức: Chúng tôi cung cấp định nghĩa chính thức và công thức hóa Giải mã Đầu cơ, đặt nền móng cho nghiên cứu tương lai.
(3) Phân loại mới: Chúng tôi cung cấp phân loại hệ thống cho Giải mã Đầu cơ, đưa ra sự phân loại có tổ chức của công trình hiện tại.
(4) Spec-Bench: Chúng tôi giới thiệu Spec-Bench, một chuẩn mực rộng lớn được thiết kế để đánh giá Giải mã Đầu cơ, cho phép đánh giá so sánh các phương pháp hàng đầu.

Chúng tôi hy vọng rằng công trình này có thể đóng vai trò như hướng dẫn thiết yếu cho người mới và thúc đẩy nghiên cứu tương lai.

2 Tổng quan
Bài báo này cung cấp khảo sát toàn diện về Giải mã Đầu cơ. Chúng tôi bắt đầu bằng việc giới thiệu giai đoạn đầu của nghiên cứu Giải mã Đầu cơ (§3), được minh họa bằng dòng thời gian về sự tiến hóa của nó (như được hiển thị trong Hình 2). Tiếp theo là định nghĩa chính thức và công thức hóa Giải mã Đầu cơ (§4). Sau đó, chúng tôi đi sâu vào thảo luận chi tiết về các kỹ thuật hàng đầu, bao gồm lựa chọn mô hình soạn thảo (§5), chiến lược xác minh (§6), và sự liên kết giữa trình soạn thảo và LLM đích (§7). Hơn nữa, chúng tôi giới thiệu Spec-Bench, một chuẩn mực đánh giá rộng lớn được thiết kế để đánh giá hiệu ứng tăng tốc của Giải mã Đầu cơ (§8).

3 Tiến hóa của Giải mã Đầu cơ
Phần này thảo luận về động lực đằng sau Giải mã Đầu cơ (§3.1) và sau đó cung cấp giới thiệu chi tiết về các nỗ lực ban đầu trong lĩnh vực này (§3.2).

3.1 Động lực
Việc áp dụng rộng rãi các LLM đã thiết lập giải mã tự hồi quy như tiêu chuẩn thực tế cho suy luận LLM (Chowdhery et al., 2023; OpenAI, 2023; Jiang et al., 2024). Tuy nhiên, giải mã tự hồi quy bị hạn chế bởi độ trễ suy luận, chủ yếu xuất phát từ tính toán bị ràng buộc bộ nhớ của LLM (Patterson, 2004; Shazeer, 2019). Cụ thể, nút thắt độ trễ chính của mỗi bước giải mã không phải do các hoạt động tính toán mà phát sinh từ sự cần thiết phải chuyển tất cả các tham số LLM từ Bộ nhớ Băng thông Cao (HBM) đến bộ đệm trên chip của các bộ tăng tốc hiện đại như GPU. Quá trình này, chỉ sinh ra một token mỗi bước, dẫn đến việc sử dụng dưới mức của những bộ tăng tốc này và gây ra sự không hiệu quả.

Thuật toán 1 Giải mã Tự hồi quy
Yêu cầu: Mô hình ngôn ngữ Mq, chuỗi đầu vào x1, . . . , xt,
và chiều dài chuỗi đích T;
1:khởi tạo n←t
2:trong khi n < T làm
3: Đặt qn+1← Mq(x|x<n+1)
4: Lấy mẫu xn+1∼qn+1
5: n←n+ 1
6:kết thúc trong khi

3.2 Những Nỗ lực Tiên phong Soạn thảo-sau đó-Xác minh
Để giảm thiểu vấn đề trên, một cách trực quan liên quan đến việc tận dụng tài nguyên tính toán nhàn rỗi để tăng cường tính song song trong suy luận LLM. Để đạt được điều này, Stern et al. (2018) đã giới thiệu Giải mã Khối, một cách tiếp cận kết hợp các đầu mạng nơron feedforward (FFN) bổ sung trên bộ giải mã Transformer, cho phép soạn thảo đồng thời nhiều token mỗi bước. Những token này sau đó được xác minh bởi LLM gốc song song, đảm bảo rằng các đầu ra phù hợp với những của LLM gốc. Như một công trình tiên phong đề xuất mô hình Soạn thảo-sau đó-Xác minh, Giải mã Khối hiệu quả giảm số lần gọi LLM cần thiết bằng cách tăng tính song song sinh, từ đó tăng tốc suy luận.

Để tiếp tục khai phá tiềm năng của mô hình này, Xia et al. (2023) đã giới thiệu Giải mã Đầu cơ (SpecDec), sử dụng một trình soạn thảo độc lập, đáng chú ý là một Transformer Phi-Tự hồi quy chuyên biệt, để thực hiện nhiệm vụ soạn thảo cả chính xác và hiệu quả. Hơn nữa, phương pháp này trình bày một chiến lược sáng tạo làm giảm bớt tiêu chí xác minh cứng nhắc, từ đó tăng tỷ lệ chấp nhận của các token đã soạn thảo. Ấn tượng là SpecDec đạt được tăng tốc khoảng 5× so với giải mã tự hồi quy với chất lượng tương đương, nhấn mạnh tiềm năng đáng kể của Giải mã Đầu cơ.

Tiếp theo SpecDec, Leviathan et al. (2023) và Chen et al. (2023a) đã có những đóng góp đồng thời bằng cách đề xuất Lấy mẫu Đầu cơ, mở rộng mô hình này để bao gồm việc tăng tốc không mất mát của các phương pháp lấy mẫu khác nhau. Những cách tiếp cận này sử dụng các LM nhỏ hơn từ cùng series (ví dụ: T5-small) để tăng tốc suy luận của các đối tác lớn hơn (ví dụ: T5-XXL). Khác với công trình trước đó, những LM nhỏ có sẵn này không yêu cầu đào tạo bổ sung, cho phép áp dụng nhanh chóng Giải mã Đầu cơ trong tăng tốc LLM. Tiến bộ này đã đưa Giải mã Đầu cơ lên hàng đầu của nghiên cứu hiệu quả LLM, thu hút sự quan tâm rộng rãi trong cộng đồng NLP.

Tóm lại, những nỗ lực tiên phong này trong Giải mã Đầu cơ đã dần củng cố mô hình Soạn thảo-sau đó-Xác minh, thể hiện tiềm năng đầy hứa hẹn trong tăng tốc LLM. Chúng tôi cung cấp phân loại chi tiết và thảo luận về những nghiên cứu này và nghiên cứu tiếp theo trong các phần sau.

Thuật toán 2 Giải mã Đầu cơ
Yêu cầu: Mô hình ngôn ngữ đích Mq, mô hình soạn thảo Mp, chuỗi đầu vào x1, . . . , xt, kích thước khối K, chiều dài chuỗi đích T, chiến lược soạn thảo DRAFT, tiêu chí xác minh VERIFY, và chiến lược sửa chữa CORRECT;
1:khởi tạo n←t
2:trong khi n < T làm
//Soạn thảo: thu được phân phối từ Mp hiệu quả
3: Đặt p1, . . . , pK←DRAFT(x≤n,Mp)
//Soạn thảo: lấy mẫu K token đã soạn thảo
4: Lấy mẫu êxi∼pi, i= 1, . . . , K
//Xác minh: tính toán K+1 phân phối song song
5: Đặt qi← Mq(x|x≤n,êx<i), i= 1, . . . , K + 1
//Xác minh: xác minh từng token đã soạn thảo
6: cho i= 1 : K làm
7: nếu VERIFY(êxi, pi, qi) thì
8: Đặt xn+i←êxi và n←n+ 1
9: ngược lại
10: xn+i←CORRECT(pi, qi)
11: và Thoát khỏi vòng lặp for.
12: kết thúc nếu
13: kết thúc cho
14: Nếu tất cả token đã soạn thảo được chấp nhận, lấy mẫu token tiếp theo xn+1∼qK+1 và đặt n←n+ 1.
15:kết thúc trong khi

4 Công thức hóa và Định nghĩa
Trong phần này, trước tiên chúng tôi cung cấp tổng quan ngắn gọn về giải mã tự hồi quy tiêu chuẩn (§4.1). Sau đó, chúng tôi đưa ra trình bày sâu sắc về Giải mã Đầu cơ (§4.2), bao gồm định nghĩa chính thức, mô tả toàn diện về phương pháp luận, và elaboration chi tiết về thuật toán.

4.1 Giải mã Tự hồi quy
Các LLM dựa trên Transformer thường tạo ra các sinh theo cách tự hồi quy. Cho một chuỗi đầu vào x1, . . . , xt, một mô hình ngôn ngữ tự hồi quy Mq sinh token tiếp theo theo:
xt+1∼qt+1=Mq(x|x<t+1), (1)
trong đó q là phân phối xác suất có điều kiện được tính bởi Mq và xt+1 biểu thị token tiếp theo được lấy mẫu từ qt+1. Chúng tôi minh họa một quá trình chi tiết trong Thuật toán 1.

Như đã thảo luận trong Phần 3, mặc dù giải mã tự hồi quy tiêu chuẩn cung cấp chất lượng sinh mong muốn, nó bị ràng buộc bởi băng thông bộ nhớ, dẫn đến việc sử dụng thấp các bộ tăng tốc hiện đại. Trong quá trình này, mỗi lần gọi LLM bị ràng buộc bộ nhớ (tức là một bước forward LLM) chỉ tạo ra một token duy nhất cho toàn bộ chuỗi, làm cho việc sinh toàn bộ không hiệu quả và tốn thời gian.

4.2 Giải mã Đầu cơ
Theo Xia et al. (2023), Leviathan et al. (2023), và Chen et al. (2023a), chúng tôi ở đây cung cấp định nghĩa chính thức của Giải mã Đầu cơ:

Giải mã Đầu cơ là một mô hình giải mã Soạn thảo-sau đó-Xác minh trong đó, tại mỗi bước giải mã, nó đầu tiên soạn thảo hiệu quả nhiều token tương lai và sau đó xác minh tất cả những token này song song bằng LLM đích để tăng tốc suy luận.

Chúng tôi công thức hóa một quá trình Giải mã Đầu cơ chi tiết trong Thuật toán 2. Tiếp theo, chúng tôi đi sâu vào hai bước con cơ bản tích hợp vào mô hình này – soạn thảo và xác minh:

Soạn thảo Tại mỗi bước giải mã, Giải mã Đầu cơ đầu tiên soạn thảo hiệu quả nhiều token tương lai, như một suy đoán về đầu ra của LLM đích. Chính thức, cho một chuỗi đầu vào x1, . . . , xt và LLM đích Mq, mô hình này sử dụng một mô hình soạn thảo hiệu quả Mp (ví dụ: một LM nhỏ hơn) để giải mã K token đã soạn thảo tiếp theo:
p1, . . . , pK=DRAFT(x≤t,Mp),
êxi∼pi, i = 1, . . . , K,(2)

trong đó DRAFT(·) biểu thị các chiến lược soạn thảo khác nhau mà chúng tôi sẽ thảo luận trong Phần 5, p là phân phối xác suất có điều kiện được tính bởi Mp, và êxi biểu thị token đã soạn thảo được lấy mẫu từ pi.

Xác minh Tiếp theo, những token đã soạn thảo này được xác minh bởi LLM đích Mq song song. Chính thức, cho chuỗi đầu vào x1, . . . , xt và bản thảo êx1, . . . ,êxK, Giải mã Đầu cơ sử dụng Mq để tính toán K+ 1 phân phối xác suất đồng thời:
qi=Mq(x|x≤t,êx<i), i= 1, . . . , K + 1.(3)

Sau đó, mỗi token đã soạn thảo êxi được xác minh bởi một tiêu chí cụ thể VERIFY(êxi, pi, qi). Chỉ những token đáp ứng tiêu chí mới được chọn làm đầu ra cuối cùng, đảm bảo chất lượng phù hợp với tiêu chuẩn của LLM đích. Ngược lại, token đã soạn thảo đầu tiên êxc không đạt xác minh sẽ được sửa chữa bằng chiến lược CORRECT(pc, qc). Tất cả các token đã soạn thảo sau vị trí c sẽ bị loại bỏ, để đảm bảo chất lượng cao của đầu ra cuối cùng. Nếu tất cả token đều vượt qua xác minh, một token bổ sung xt+K+1 sẽ được lấy mẫu từ qK+1 như Phương trình (1).

Các bước con soạn thảo và xác minh sẽ được lặp lại cho đến khi điều kiện kết thúc được đáp ứng, tức là token [EOS] được giải mã hoặc câu đạt chiều dài tối đa.

Đáng chú ý, hiệu ứng tăng tốc của Giải mã Đầu cơ chủ yếu phụ thuộc vào tỷ lệ chấp nhận của các token đã soạn thảo ở mỗi bước. Tỷ lệ này bị ảnh hưởng bởi một số yếu tố, bao gồm chất lượng bản thảo, tiêu chí xác minh, và sự liên kết hành vi giữa trình soạn thảo và LLM đích. Ngoài ra, hiệu quả vốn có của chính trình soạn thảo cũng góp phần vào tăng tốc tổng thể. Trong các phần tiếp theo, chúng tôi sẽ đi sâu vào những thành phần then chốt này của Giải mã Đầu cơ, như được mô tả trong Hình 3, để phân loại hệ thống các xu hướng nghiên cứu gần đây trong mô hình đầy hứa hẹn này.

5 Soạn thảo
Như một thành phần quan trọng của Giải mã Đầu cơ, quá trình soạn thảo có tác động quan trọng đến tăng tốc của mô hình. Tác động được xác định bởi hai yếu tố chính: độ chính xác suy đoán của trình soạn thảo Mp, được đo bằng số lượng token được chấp nhận trung bình mỗi bước, và độ trễ soạn thảo (Stern et al., 2018; Xia et al., 2023). Làm thế nào để cân bằng giữa độ chính xác suy đoán cao và độ trễ soạn thảo thấp đặt ra một thách thức lớn trong quá trình này. Trong phần này, chúng tôi phân loại các chiến lược soạn thảo khác nhau thành hai loại: soạn thảo độc lập (§5.1) và tự soạn thảo (§5.2), và tóm tắt các công thức DRAFT(x≤t,Mp) của chúng trong Bảng 1.

5.1 Soạn thảo Độc lập
Để đạt được sự cân bằng giữa độ chính xác suy đoán và hiệu quả, SpecDec (Xia et al., 2023) đầu tiên đề xuất sử dụng một mô hình độc lập để soạn thảo. Cụ thể, nó sử dụng một Transformer Phi-Tự hồi quy chuyên biệt soạn thảo nhiều token đồng thời mỗi bước. Mô hình này có kiến trúc encoder-decoder sâu-nông để chạy hiệu quả. Mặc dù có điểm mạnh, SpecDec yêu cầu đào tạo mô hình soạn thảo từ đầu, đòi hỏi ngân sách tính toán tăng thêm.

Xem xét các mô hình có sẵn trong các series LLM hiện tại (ví dụ: OPT (Zhang et al., 2022) và LLaMA (Touvron et al., 2023a,b)), một cách tiếp cận đơn giản và hiệu quả hơn là trực tiếp sử dụng một LM nhỏ từ cùng series làm trình soạn thảo để tăng tốc suy luận của các đối tác lớn hơn (Leviathan et al., 2023; Chen et al., 2023a; Spector và Re, 2023; Sun et al., 2023; Chen et al., 2023b). Ví dụ, Leviathan et al. (2023) sử dụng T5-small làm trình soạn thảo, để tăng tốc suy luận của T5-XXL. Những LM nhỏ có sẵn này không yêu cầu đào tạo bổ sung hoặc bất kỳ sửa đổi nào về kiến trúc mô hình, tạo điều kiện cho việc áp dụng nhanh chóng Giải mã Đầu cơ. Hơn nữa, vì các mô hình trong cùng series chia sẻ tokenizer, corpus pretraining, và quy trình đào tạo tương tự, chúng vốn có sự liên kết trong hành vi dự đoán.

5.2 Tự Soạn thảo
Mặc dù tận dụng mô hình soạn thảo bên ngoài mang lại lợi thế đáng kể, cách tiếp cận này đòi hỏi nỗ lực bổ sung để đào tạo hoặc xác định mô hình soạn thảo gần gũi với LLM đích. Thách thức này được tăng cường khi không có đối tác nhỏ hơn của LLM, ví dụ: LLaMA-7B (Touvron et al., 2023a,b). Hơn nữa, việc tích hợp hai mô hình riêng biệt trong một hệ thống duy nhất giới thiệu độ phức tạp tính toán bổ sung, đặc biệt trong cài đặt phân tán (Cai et al., 2024).

Để giải quyết các vấn đề trên, nhiều nghiên cứu đã đề xuất tận dụng chính LLM đích để soạn thảo hiệu quả (Stern et al., 2018; Santilli et al., 2023; Hooper et al., 2023; Cai et al., 2024; Fu et al., 2024; Du et al., 2024). Đặc biệt, Giải mã Khối (Stern et al., 2018) và Medusa (Cai et al., 2024) kết hợp các đầu FFN trên bộ giải mã Transformer, cho phép sinh token song song mỗi bước. So với các trình soạn thảo bên ngoài, những đầu nhẹ này giảm chi phí tính toán bổ sung và thân thiện với suy luận phân tán. Một hướng nghiên cứu khác đã khám phá tiềm năng của thoát sớm và bỏ qua lớp trong LLM đích để soạn thảo (Yang et al., 2023b; Zhang et al., 2023a; Hooper et al., 2023). Ví dụ, Yang et al. (2023b) giới thiệu các quy trình con bổ sung thoát sớm trong bước giải mã hiện tại, từ đó bắt đầu soạn thảo các token tương lai trước.

Trái ngược với công trình trước đó tập trung vào mở rộng kiến trúc mô hình hoặc thay đổi quy trình suy luận, Santilli et al. (2023) giới thiệu một chiến lược soạn thảo đơn giản trực tiếp nối nhiều token [PAD] vào cuối lời nhắc đầu vào để cho phép sinh song song. Tuy nhiên, cách tiếp cận này lệch khỏi mô hình pretraining tự hồi quy của LLM, dẫn đến chất lượng soạn thảo không tối ưu. Để giải quyết điều này, Fu et al. (2024) đề xuất chuyển đổi các bản thảo chất lượng thấp thành nhiều n-gram để cải thiện độ chính xác suy đoán; Monea et al. (2023) giới thiệu nhiều token [LA] có thể học được và tinh chỉnh các embedding token này trên một tập dữ liệu đào tạo nhỏ để tăng cường hiệu suất giải mã song song.

6 Xác minh
Trong mỗi bước giải mã, các token đã soạn thảo được xác minh song song để đảm bảo các đầu ra phù hợp với LLM đích. Quá trình này cũng xác định số lượng token được chấp nhận mỗi bước, một yếu tố quan trọng tác động đến tăng tốc. Phần này tóm tắt các tiêu chí xác minh khác nhau VERIFY(êxi, pi, qi) (như được hiển thị trong Bảng 2), bao gồm những tiêu chí hỗ trợ giải mã tham lam (§6.1) và lấy mẫu đầu cơ (§6.2) trong suy luận LLM. Bên cạnh đó, chúng tôi giới thiệu xác minh cây token (§6.3), một chiến lược hiệu quả để tăng tỷ lệ chấp nhận token.

6.1 Giải mã Tham lam
Những nỗ lực ban đầu tại Giải mã Đầu cơ tập trung vào tiêu chí xác minh hỗ trợ giải mã tham lam, đảm bảo rằng các đầu ra hoàn toàn giống với kết quả giải mã tham lam của LLM đích (Stern et al., 2019; Sun et al., 2021; Xia et al., 2023). Chính thức, cho chuỗi đầu vào x1, . . . , xt, các token đã soạn thảo êx1, . . . ,êxK, và các phân phối xác suất đã tính p1, . . . , pK, q1, . . . , qK như thu được từ Phương trình (2) và (3), tương ứng, tiêu chí xác minh trên token đã soạn thảo thứ i được công thức hóa như
êxi= arg max qi, (4)
trong đó i= 1, . . . , K. Vị trí đầu tiên c mà token đã soạn thảo êxc không đạt xác minh biểu thị vị trí phân nhánh. Token đầu ra tại vị trí này xt+c sẽ được điều chỉnh bởi chiến lược sửa chữa, đơn giản thay thế token đã soạn thảo bằng dự đoán top-1 của LLM:
xt+c←arg max qc. (5)

Tiêu chí xác minh của giải mã tham lam đơn giản và rõ ràng. Do đó, nhiều nghiên cứu tiếp theo đã áp dụng tiêu chí này để chứng minh hiệu quả của phương pháp luận của họ (Santilli et al., 2023; Yang et al., 2023b; Hooper et al., 2023; Zhang et al., 2023a; Fu et al., 2024). Tuy nhiên, yêu cầu khớp nghiêm ngặt của tiêu chí này thường dẫn đến việc từ chối các token đã soạn thảo chất lượng cao, đơn giản vì chúng khác với dự đoán top-1 của LLM đích, từ đó hạn chế tăng tốc của mô hình.

Để giải quyết vấn đề này, nhiều nghiên cứu đã đề xuất các tiêu chí xác minh gần đúng khác nhau (Stern et al., 2018; Xia et al., 2023; Kim et al., 2023). So với tiêu chí không mất mát, những phương pháp này hơi nới lỏng yêu cầu khớp để tin tưởng các bản thảo hơn, dẫn đến việc chấp nhận cao hơn các token đã soạn thảo. Ví dụ, SpecDec (Xia et al., 2023) chỉ yêu cầu các token đã soạn thảo nằm trong các ứng viên top-k của LLM đích; BiLD (Kim et al., 2023) đề xuất tiêu chí rollback chỉ từ chối các token đã soạn thảo khi số lượng token không khớp liên tiếp vượt quá một ngưỡng cố định.

6.2 Lấy mẫu Đầu cơ
Theo Stern et al. (2019), công trình tiếp theo mở rộng Giải mã Đầu cơ để hỗ trợ các phương pháp lấy mẫu khác nhau (Leviathan et al., 2023; Chen et al., 2023a), tăng tốc suy luận của LLM đích mà không thay đổi phân phối đầu ra của nó. Chính thức, cho chuỗi ban đầu x1, . . . , xt, các token đã soạn thảo êx1, . . . ,êxK và các phân phối đã tính p1, . . . , pK, q1, . . . , qK, tiêu chí xác minh trên token đã soạn thảo thứ i là
r <min(1,qi(êxi)/pi(êxi)), r∼U[0,1], (6)
trong đó r biểu thị một số ngẫu nhiên được rút từ phân phối đều U[0,1]; qi(êxi) và pi(êxi) là xác suất của êxi theo Mq và Mp, tương ứng; và i= 1, . . . , K. Nói cách khác, tiêu chí này chấp nhận token êxi nếu qi(êxi)≥pi(êxi), và trong trường hợp qi(êxi)< pi(êxi) nó từ chối token với xác suất 1−qi(êxi)/pi(êxi). Chiến lược sửa chữa lấy mẫu lại token đầu ra tại vị trí phân nhánh c từ một phân phối đã điều chỉnh:
xt+c∼norm(max(0, qc−pc)). (7)

Leviathan et al. (2023) và Chen et al. (2023a) đã chứng minh lý thuyết rằng tiêu chí này duy trì các phân phối đầu ra giống hệt với LLM đích. Do đó, nó đã được áp dụng rộng rãi trong nghiên cứu tiếp theo (Liu et al., 2023; Zhou et al., 2023; Monea et al., 2023; Chen et al., 2023b). Ngoài yêu cầu nghiêm ngặt, một số công trình cũng đã khám phá các chiến lược gần đúng để cải thiện tỷ lệ chấp nhận token (Leviathan et al., 2023; Zhou et al., 2023). Ví dụ, Leviathan et al. (2023) đề xuất nhân pi(êxi) trong Phương trình (6) với một tham số khoan dung l∈[0,1] để hơi nới lỏng tiêu chí.

6.3 Xác minh Cây Token
Trái ngược với các chiến lược xác minh trước đó tập trung vào một chuỗi bản thảo duy nhất, SpecInfer (Miao et al., 2024) đề xuất xác minh cây token, một chiến lược hiệu quả cho phép LLM đích xác minh nhiều chuỗi bản thảo song song. Như được minh họa trong Hình 4, phương pháp này đầu tiên hợp nhất nhiều chuỗi bản thảo ứng viên thành một cây token bằng cách chia sẻ tiền tố. Sau đó nó sử dụng mặt nạ attention cây được thiết kế đặc biệt để tạo điều kiện cho LLM xác minh toàn bộ cấu trúc song song. Nghiên cứu gần đây đã khám phá các cách tiếp cận khác nhau để thu được những chuỗi bản thảo ứng viên này (Miao et al., 2024; Cai et al., 2024; He et al., 2023; Li et al., 2024). Ví dụ, Miao et al. (2024) sinh các chuỗi bản thảo đa dạng từ các LM boost-tuned khác nhau; Cai et al. (2024) xem xét các dự đoán top-k từ mỗi đầu FFN để thu được nhiều chuỗi ứng viên.

7 Liên kết
Như được minh họa trong Phần 5, tăng tốc của Giải mã Đầu cơ chủ yếu phụ thuộc vào độ chính xác suy đoán, điều này lần lượt bị ảnh hưởng bởi sự tương tự hành vi giữa trình soạn thảo và LLM đích. Để tăng cường điều này, nghiên cứu hiện tại đã khám phá các chiến lược chưng cất kiến thức (KD) khác nhau để liên kết đầu ra của trình soạn thảo với những của LLM đích (Stern et al., 2018; Xia et al., 2023; Miao et al., 2024; Liu et al., 2023; Kim et al., 2023; Zhou et al., 2023). Đặc biệt, Giải mã Khối áp dụng chưng cất kiến thức cấp chuỗi (Seq-KD) (Kim và Rush, 2016) để liên kết, đào tạo trình soạn thảo trên các câu được sinh bởi LLM đích. Miao et al. (2024) đề xuất chiến lược boost-tuning tập thể (Col-BT), áp dụng Seq-KD để tinh chỉnh nhiều LM nhỏ trên dữ liệu đào tạo và sử dụng đầu ra tổng hợp của chúng làm bản thảo để cải thiện độ chính xác suy đoán.

Mặc dù Seq-KD hiệu quả, nó bỏ qua các phân phối xác suất của LLM đích, dẫn đến suy giảm hiệu suất với các phương pháp lấy mẫu. Để sửa chữa điều này, các nghiên cứu gần đây đã khám phá các chiến lược KD khác cho Giải mã Đầu cơ (Zhou et al., 2023; Liu et al., 2023). Đáng chú ý, DistillSpec (Zhou et al., 2023) thực hiện so sánh toàn diện các chiến lược KD khác nhau trên Giải mã Đầu cơ qua các nhiệm vụ hạ lưu khác nhau. Liu et al. (2023) đề xuất chiến lược KD trực tuyến liên kết động trình soạn thảo với LLM đích một cách nhanh chóng sử dụng dữ liệu truy vấn.

Chúng tôi tóm tắt các tính năng chính của các phương pháp Giải mã Đầu cơ hiện tại trong Bảng 3, bao gồm loại trình soạn thảo hoặc chiến lược soạn thảo, cách tiếp cận liên kết, các chiến lược xác minh được hỗ trợ, và tăng tốc được báo cáo, v.v.

8 Spec-Bench
Với tiến bộ nghiên cứu nhanh chóng trong Giải mã Đầu cơ, có nhu cầu tăng cao về phân tích so sánh các phương pháp hàng đầu. Tuy nhiên, các cách tiếp cận hiện tại được thử nghiệm sử dụng các chuẩn mực, thiết bị và môi trường khác nhau, làm cho việc so sánh công bằng trở nên không thực tế. Để giải quyết khoảng trống này, chúng tôi giới thiệu Spec-Bench – một chuẩn mực toàn diện cho Giải mã Đầu cơ bao gồm các kịch bản ứng dụng đa dạng. Dựa trên Spec-Bench, chúng tôi trình bày so sánh hệ thống các cách tiếp cận mã nguồn mở trong điều kiện thử nghiệm của bên thứ ba. Các thí nghiệm được thực hiện trên cùng thiết bị và môi trường thử nghiệm để đảm bảo so sánh công bằng.

8.1 Xây dựng Chuẩn mực
Để đánh giá các phương pháp Giải mã Đầu cơ qua các kịch bản khác nhau, Spec-Bench bao gồm sáu nhiệm vụ con riêng biệt: hội thoại đa lượt, dịch thuật, tóm tắt, trả lời câu hỏi, lý luận toán học, và sinh tăng cường truy xuất. Chúng tôi tạo Spec-Bench bằng cách ngẫu nhiên chọn 80 trường hợp từ mỗi trong sáu tập dữ liệu được sử dụng rộng rãi, bao gồm MT-bench (Zheng et al., 2023), WMT14 DE-EN, CNN/Daily Mail (Nallapati et al., 2016), Natural Questions (Kwiatkowski et al., 2019), GSM8K (Cobbe et al., 2021), và DPR (Karpukhin et al., 2020). Để biết chi tiết về Spec-Bench và thiết lập thí nghiệm cụ thể, vui lòng tham khảo Phụ lục B.

8.2 Đánh giá So sánh
Các đánh giá chính của chúng tôi được tiến hành trên Vicuna-7B với độ chính xác FP16 sử dụng một GPU 3090 cấp tiêu dùng4. Như được mô tả trong Hình 5, trong cài đặt tham lam, EAGLE (Li et al., 2024) đạt được tỷ lệ tăng tốc cao nhất (1.8×∼2.4×) so với giải mã tự hồi quy qua hầu hết các nhiệm vụ con, đặc biệt trong lý luận toán học (với tăng tốc ∼2.4×). Thành công của EAGLE chủ yếu do hai yếu tố: 1) nó tái sử dụng KV cache của LLM để dự đoán các token đã soạn thảo, giảm đáng kể chi phí tính toán soạn thảo; 2) so với Medusa (Cai et al., 2024), EAGLE soạn thảo theo cách tự hồi quy, cung cấp kết quả suy đoán ổn định và chính xác hơn. PLD (Saxena, 2023) xuất sắc trong các nhiệm vụ con có độ tương tự cao giữa đầu vào và đầu ra, như tóm tắt (với tăng tốc ∼2.4×). Tuy nhiên, hiệu suất của nó giảm trong các nhiệm vụ con khác như dịch thuật và trả lời câu hỏi, với tỷ lệ tăng tốc rơi vào khoảng 1.1×∼1.3×.

Chúng tôi cũng so sánh tăng tốc của các phương pháp Giải mã Đầu cơ ở các nhiệt độ lấy mẫu khác nhau. Như được minh họa trong Hình 6, EAGLE nhất quán vượt trội hơn các phương pháp khác qua các cài đặt khác nhau, đạt được tỷ lệ tăng tốc từ 1.7× đến 2.1×. Bên cạnh đó, quan sát thấy rằng hiệu ứng tăng tốc của tất cả các phương pháp giảm với sự tăng nhiệt độ lấy mẫu. Điều này được quy cho độ phức tạp tính toán tăng của tiêu chí lấy mẫu đầu cơ ở nhiệt độ cao hơn, như được tiết lộ trong nghiên cứu trước đó (Joao Gante, 2023; Spector và Re, 2023).

9 Thách thức và Hướng Tương lai
Làm thế nào để cân bằng giữa độ chính xác suy đoán và hiệu quả soạn thảo? Như đã thảo luận trong Phần 5, việc mở rộng quy mô trình soạn thảo có thể tăng cường hiệu quả độ chính xác suy đoán, nhưng nó phần lớn làm giảm hiệu quả soạn thảo và thậm chí tăng tốc tổng thể. Do đó, điều cần thiết là đạt được sự cân bằng giữa độ chính xác suy đoán và độ trễ soạn thảo. Trong số các chiến lược hiện tại, liên kết hành vi là một cách tiếp cận đầy hứa hẹn để giải quyết vấn đề này, vì nó cải thiện độ chính xác suy đoán mà không tăng độ trễ. Tuy nhiên, mặc dù có những tiến bộ gần đây (Miao et al., 2024; Zhou et al., 2023; Liu et al., 2023), vẫn còn chỗ để cải thiện đáng kể để liên kết trình soạn thảo với LLM đích. Ví dụ, cho rằng các token đã soạn thảo sau vị trí phân nhánh đều bị loại bỏ, một hướng tiềm năng có thể liên quan đến việc khuyến khích trình soạn thảo ưu tiên chất lượng sinh của các token vị trí đầu. Ngoài liên kết, các yếu tố khác như chất lượng soạn thảo (Fu et al., 2024) và xác định chiều dài suy đoán (Su et al., 2023) cũng ảnh hưởng đến độ chính xác suy đoán và xứng đáng khám phá thêm.

Làm thế nào để áp dụng Giải mã Đầu cơ trong các kịch bản suy luận theo batch? Hiện tại, chỉ một số ít triển khai Giải mã Đầu cơ hỗ trợ suy luận theo batch, như EAGLE5 và SpS6. Tuy nhiên, suy luận theo batch là một kỹ thuật quan trọng để quản lý hiệu quả đầu vào của người dùng trong các dịch vụ LLM thời gian thực. Những thách thức chính trong Giải mã Đầu cơ theo batch nằm ở hai khía cạnh: (1) Mỗi câu được giải mã trong Giải mã Đầu cơ khác nhau về các bước giải mã do độ chính xác suy đoán khác nhau. Do đó, độ trễ suy luận của một batch phụ thuộc vào mẫu chậm nhất trong batch; (2) Độ phức tạp tính toán bổ sung được giới thiệu bởi Giải mã Đầu cơ, đặc biệt trong cài đặt lấy mẫu, tăng với kích thước batch lớn hơn. Làm thế nào để duy trì tăng tốc đầy hứa hẹn của Giải mã Đầu cơ trong suy luận theo batch, và kết hợp nó với các kỹ thuật tiên tiến như batching liên tục (Yu et al., 2022), cần được điều tra thêm.

Làm thế nào để tích hợp Giải mã Đầu cơ với các kỹ thuật hàng đầu khác? Như một mô hình giải mã chung, Giải mã Đầu cơ đã chứng minh tiềm năng của nó kết hợp với các kỹ thuật tiên tiến khác (Yang et al., 2023a; Zhang et al., 2023b; Li et al., 2023). Ví dụ, Yuan et al. (2023) kết hợp Giải mã Đầu cơ với Giải mã Tương phản (Li et al., 2023), không chỉ tăng tốc suy luận mà còn cải thiện đáng kể chất lượng sinh. Ngoài việc tăng tốc LLM chỉ văn bản, việc áp dụng Giải mã Đầu cơ trong suy luận đa phương thức, như tổng hợp hình ảnh, tổng hợp văn bản thành lời nói, và sinh video, cũng là một hướng nghiên cứu tương lai hấp dẫn và có giá trị. Một hướng nghiên cứu đầy hứa hẹn khác là tích hợp Giải mã Đầu cơ với các phương pháp hiệu quả khác như vLLM (Kwon et al., 2023), Sinh Phi-Tự hồi quy (Du et al., 2021, 2022) và Flash-Attention (Dao et al., 2022; Dao, 2023), tăng cường thêm hiệu quả suy luận của các dịch vụ LLM.

10 Kết luận
Bài báo này trình bày khảo sát toàn diện về Giải mã Đầu cơ, bao gồm sự tiến hóa của mô hình đầy hứa hẹn này, định nghĩa chính thức và công thức hóa của nó, phân loại hệ thống các phương pháp hiện tại, và đánh giá sâu sắc các kỹ thuật hàng đầu. Hơn nữa, chúng tôi giới thiệu Spec-Bench, một chuẩn mực đánh giá rộng lớn cho các phương pháp Giải mã Đầu cơ, và trình bày đánh giá so sánh các phương pháp nổi bật. Theo hiểu biết của chúng tôi, đây là khảo sát đầu tiên dành riêng cho Giải mã Đầu cơ. Mục tiêu của chúng tôi cho bài báo này là làm rõ bối cảnh nghiên cứu hiện tại và cung cấp thông tin chi tiết về hướng nghiên cứu tương lai.
