Tăng tốc các LLM sản xuất với các Speculator kết hợp Token/Embedding

Báo cáo kỹ thuật này mô tả việc thiết kế và huấn luyện các mô hình draft speculative decoding mới nhằm tăng tốc độ suy luận của các mô hình ngôn ngữ lớn trong môi trường sản xuất. Bằng cách điều kiện hóa các dự đoán draft trên cả vector ngữ cảnh và token được lấy mẫu, chúng tôi có thể huấn luyện các speculator của mình để dự đoán hiệu quả các n-gram chất lượng cao, mà mô hình cơ sở sau đó chấp nhận hoặc từ chối. Điều này cho phép chúng tôi dự đoán hiệu quả nhiều token cho mỗi lượt forward pass suy luận, tăng tốc độ suy luận wall-clock của các triển khai mô hình cơ sở được tối ưu hóa cao lên 2-3 lần. Chúng tôi khám phá những kết quả ban đầu này và mô tả các bước tiếp theo để cải thiện thêm.

Khi các Mô hình Ngôn ngữ Lớn (LLM) sinh tạo ngày càng lớn hơn, khả năng của chúng cũng tăng theo. Tuy nhiên, sự tăng trưởng về kích thước mô hình để đổi lấy sức mạnh biểu đạt gây ra vấn đề cho việc triển khai trong môi trường sản xuất: dù mạnh mẽ đến đâu, các mô hình ngôn ngữ lớn vẫn phải chịu chi phí bộ nhớ và tính toán đáng kể. Các mô hình nổi tiếng như Llama2-13B chứa 13 tỷ tham số, chiếm khoảng 24 GB bộ nhớ khi sử dụng biểu diễn trọng số 16-bit thông thường. Tại thời điểm suy luận, khối lượng tính toán đáng kể này hoàn toàn dành cho việc tạo ra một chỉ số token duy nhất mỗi lần, trong khoảng từ 0 đến 32k. Do đó trong trường hợp cực đoan, chúng ta đang phải chịu chi phí 24 gigabyte để tạo ra ít hơn 2 byte thông tin!

Một cách rõ ràng để khắc phục sự mất cân bằng này là dự đoán nhiều token cùng một lúc. Thật vậy, lý thuyết NLP cổ điển đã chứng minh rằng ngay cả một mô hình ngôn ngữ 2/3-gram đơn giản cũng có khả năng dự đoán tuyệt vời, điều này cho chúng ta biết rằng các mô hình ngôn ngữ học được có thể dự đoán nhiều hơn một token mỗi lần với độ chính xác hợp lý. Tuy nhiên, chúng ta cũng muốn tránh thay đổi chính các mô hình cơ sở, vì các mô hình nổi tiếng - như (một lần nữa) Llama-13B - là những đại lượng đã biết, có các thuộc tính mong muốn không thể mạo hiểm trong môi trường sản xuất, nơi tính dự đoán được và độ tin cậy là tối quan trọng.

Một cách tiếp cận để giải quyết vấn đề này là speculative decoding, trong đó một mô hình draft nhỏ hơn hoặc speculator được huấn luyện để dự đoán nhiều token dựa trên một chuỗi đầu vào. Các token speculative này được tạo ra với chi phí thấp và độ chính xác thấp hơn so với LLM cơ sở. Tuy nhiên, chúng ta có thể tận dụng tính song song GPU trong quá trình forward pass LLM để đánh giá đầu ra cho mỗi token mới này với chi phí bổ sung tối thiểu. Sau đó, bằng cách so sánh các đầu ra với các đầu vào được suy đoán, chúng ta có thể chấp nhận tất cả các token dự đoán khớp với đầu ra của mô hình cơ sở, đồng thời từ chối những token không khớp. Bằng cách này, chúng ta có thể dự đoán nhiều token cho mỗi lượt forward pass LLM với chi phí bổ sung tối thiểu. Một giải thích sâu hơn về speculative decoding có thể tìm thấy trong [3, 6].

Bài báo này mô tả những nỗ lực gần đây của IBM Research để tăng tốc suy luận LLM sản xuất bằng speculative decoding với các kiến trúc mới. Trong khi các phương pháp trước đây sử dụng các LLM riêng biệt làm mô hình draft, tiêu thụ toàn bộ chuỗi đầu vào token [6,8], Medusa [3] đã chỉ ra rằng một kiến trúc MLP đa đầu đơn giản có thể đạt được hiệu suất tương tự. Kiến trúc này chỉ sử dụng vector embedding của mô hình cơ sở từ token gần nhất, vì vector embedding này ngầm chứa thông tin ngữ cảnh được rút ra từ đầu vào trước đó. Chúng tôi giới thiệu một kiến trúc cải tiến dựa trên Medusa, với các đóng góp sau:

1. Chúng tôi chỉ ra rằng chất lượng đầu ra của speculator có thể được cải thiện đáng kể bằng cách điều kiện hóa trên các token được lấy mẫu, ngoài vector ngữ cảnh mô hình cơ sở.

2. Chúng tôi giới thiệu một phương án huấn luyện hai giai đoạn hiệu quả, đầu tiên căn chỉnh các speculator của chúng tôi với hành vi đầu vào mô hình cơ sở, sau đó với hành vi đầu ra.

3. Sử dụng pipeline huấn luyện speculator này, chúng tôi tăng tốc bốn LLM sản xuất được tối ưu hóa cao lên 2-3 lần.

4. Chúng tôi khám phá các hạn chế của speculative decoding trong môi trường sản xuất, chỉ ra rằng tốc độ tăng tốc hứa hẹn giảm dần khi mức tính toán và hiệu quả cơ sở tăng lên.

5. Chúng tôi phác thảo các bước tiếp theo và các lĩnh vực nghiên cứu thêm.

Mã nguồn cho công việc của chúng tôi được mở và có sẵn tại https://github.com/foundation-model-stack/fms-fsdp, và các speculator cho các mô hình cơ sở 13B-parameter được phát hành trên HuggingFace: https://huggingface.co/ibm-fms.

Các triển khai gốc của speculative decoding dựa vào một LLM nhỏ hơn, độc lập để cung cấp draft cho mô hình cơ sở lớn hơn [6,8]. Mặc dù có chức năng, cách tiếp cận này mang theo một số hạn chế thực tế. Đầu tiên, bằng cách tiêu thụ toàn bộ lịch sử token trong quá khứ, ngay cả các mô hình ngôn ngữ nhỏ cũng có thể nhanh chóng phình to thành chi phí lớn. Thứ hai, các mô hình nhỏ hơn được huấn luyện trên văn bản ground-truth và do đó không được căn chỉnh với hành vi của mô hình cơ sở lớn hơn. Do đó, LLM draft có thể tạo ra các ứng viên hậu tố mà, mặc dù phản ánh dữ liệu huấn luyện ground truth, lại không chính xác đối với hành vi mô hình cơ sở, là hành vi mà cuối cùng chúng ta muốn tái tạo. Và thứ ba, nếu một LLM nhỏ hơn được huấn luyện trên cùng domain, với cùng tokenizer, không có sẵn, việc huấn luyện nó từ đầu có thể đòi hỏi thời gian và nỗ lực không hợp lý.

Mô hình Medusa [3] gần đây nhằm giải quyết cả ba vấn đề này bằng cách tạo draft thông qua một MLP đơn, đa đầu. MLP này lấy làm đầu vào vector embedding cuối cùng cho dự đoán token gần nhất của mô hình cơ sở. Điều này cho phép nó truy cập ngữ cảnh ngữ nghĩa mà không cần toàn bộ lịch sử token, vì mô hình cơ sở đã nỗ lực để ngữ cảnh hóa vector token này, làm cho Medusa nhanh tại thời điểm suy luận cho bất kỳ độ dài prompt nào. Medusa cũng tương đối nhanh và dễ huấn luyện, vì nó là một mô hình đơn giản với đầu vào đã giàu ngữ nghĩa.

Chúng tôi xác định một thiếu sót lớn trong kiến trúc Medusa: bằng cách chỉ sử dụng vector ngữ cảnh từ mô hình cơ sở, Medusa thất bại trong việc điều kiện hóa trên các lựa chọn token được lấy mẫu, dẫn đến các dự đoán n-gram chất lượng kém. EAGLE [7], Hydra [1] và Recurrent Drafter [10] là các công trình đồng thời, độc lập đi đến cùng một insight. Điều này xác thực thiết kế speculator của chúng tôi, trong khi ngược lại với những công trình đó, chúng tôi kiểm tra cụ thể hiệu suất của các mô hình như vậy trong môi trường sản xuất được tối ưu hóa cao. Đặc biệt, mô hình 7B cơ sở của chúng tôi không có tăng tốc tạo ra nhiều token hơn 70% mỗi giây so với baseline precision 16-bit tương đương trong trường hợp nghiên cứu EAGLE (94.9 tokens/s so với 55.1 [7]), cao hơn khoảng 90% so với baseline của Hydra (khoảng 50 tokens/s [1]), và 170% so với Recurrent Drafter (35.6 tokens/s [10]). Điều này dẫn đến một cảnh quan triển khai và đánh giá rất khác - và thú vị hơn.

Trong phần này, chúng tôi mô tả kiến trúc speculator của mình, sau đó thảo luận về các phương án huấn luyện hiệu quả.

Chúng tôi bắt đầu với kiến trúc Medusa, triển khai ba MLP prediction head song song dự đoán token n+1, n+2, n+3 sử dụng vector embedding từ mô hình cơ sở được sử dụng để dự đoán token n. Tuy nhiên, khi nhìn vào các dự đoán, chúng tôi nhanh chóng xác định được một hành vi đáng lo ngại: các chuỗi ứng viên được dự đoán bởi kiến trúc Medusa này thường là các trigram rất kém. Điều này là do MLP Medusa sử dụng vector embedding mô hình cơ sở cho token n làm đầu vào duy nhất, và hoàn toàn bỏ qua token thực tế được lấy mẫu từ phân phối được tạo bởi vector đó. Điều này có nghĩa là các dự đoán của Medusa cho token n+1, n+2, n+3 thực tế không được điều kiện hóa trên token n, mà thay vào đó trên kỳ vọng trên tất cả các mẫu có thể của token n, đồng thời. Tương tự, token n+2 không được điều kiện hóa trên token n+1, mà thay vào đó trên kỳ vọng trên tất cả giá trị cho n+1, và tương tự cho token n+3. Điều này dẫn đến hành vi phản trực giác.

Như một ví dụ đơn giản được tạo ra, hãy tưởng tượng rằng mô hình cơ sở đã tạo ra một đầu vào kết thúc bằng "follow these steps:\n". Để ba tiếp tục có khả năng nhất của đầu vào này là (không cần quá nhiều sự đình chỉ không tin):

• "Step 1:"
• "\n Step 1:"
• "\n\n Step 1:"

Hơn nữa, trong mỗi tiếp tục này, "Step" có khả năng cao (như chúng ta biết nó phải xuất hiện), trong khi "\n" có khả năng có độ tin cậy thấp hơn. Trong tình huống này, dự đoán Medusa cho ba token tiếp theo sau đó là "Step Step Step", vì đó rõ ràng là ba token có khả năng nhất tại mỗi vị trí n+1, n+2, n+3. Tuy nhiên, điều này cũng rõ ràng là một dự báo kém về hành vi mô hình cơ sở.

Để giảm thiểu vấn đề này, chúng tôi sửa đổi kiến trúc Medusa thành một MLP đa giai đoạn, trong đó mỗi giai đoạn dự đoán một token duy nhất trong chuỗi draft. Mỗi giai đoạn của MLP lấy làm đầu vào cả vector trạng thái và một (hoặc nhiều) embedding cho (các) token được lấy mẫu từ giai đoạn trước (mô hình cơ sở có thể được coi là giai đoạn 0). Mỗi cặp đầu vào (trạng thái cộng với sampled token embedding) được chiếu và truyền qua LayerNorm/GeLU activation, tạo thành một vector trạng thái mới. Vector trạng thái này được sử dụng để dự đoán tập token draft tiếp theo, mà cùng với vector trạng thái mới, hoạt động như đầu vào cho giai đoạn dự đoán tiếp theo. Do đó mô hình phát ra toàn bộ cây các ứng viên, và chúng tôi cho phép người dùng chọn top-k dự đoán có khả năng nhất từ cây đó để sử dụng trong thực tế. Kiến trúc mô hình 3-giai đoạn được trực quan hóa trong Hình 1.

Tại thời điểm suy luận, chúng tôi triển khai multi-candidate decoding [9] và Tree Attention [3, 8] để đánh giá k ứng viên được chọn đồng thời song song. Thay vì so sánh token đầu vào với token đầu ra cho một chuỗi ứng viên duy nhất, và chỉ lấy các token đúng, chúng tôi thay vào đó thực hiện quá trình chấm điểm này cho tất cả ứng viên đồng thời, và chỉ giữ các token từ ứng viên có điểm tốt nhất. Đây là một cách đơn giản để tăng độ chính xác speculator (và do đó số lượng token được tạo mỗi bước), với chi phí là tăng tính song song. Khi chúng tôi báo cáo kết quả token per step, do đó, chúng tôi cũng sẽ báo cáo số lượng ứng viên được đánh giá song song.

Bằng cách điều kiện hóa trên các token trước đó cũng như trạng thái nội bộ của mô hình cơ sở, speculator của chúng tôi có thể tạo ra các draft n-gram thực tế hơn. Đồng thời, nó cũng chính xác hơn trong việc dự đoán hành vi mô hình cơ sở. Đối với Llama2-13B, chúng tôi có thể tăng tốc kiến trúc Medusa sandbox ba đầu ở trên từ 2.28 token per step, lên 2.63 token per step (tốt nhất trong 10 ứng viên), chỉ đơn giản bằng cách điều kiện hóa mỗi MLP head trên các token được lấy mẫu trước đó, ngoài trạng thái mô hình cơ sở.

Lưu ý rằng Hình 1 cũng có thể dễ dàng mô tả một RNN, và thật vậy chúng tôi có thể triển khai MLP đa giai đoạn của mình như một RNN bằng cách đơn giản ràng buộc các trọng số qua các giai đoạn khác nhau. Mặc dù việc chia sẻ trọng số này sẽ thu nhỏ số lượng tham số của speculator của chúng tôi, nó không làm cho speculator nhanh hơn tại thời điểm suy luận. Do đó chúng tôi triển khai speculator của mình mà không chia sẻ trọng số để có khả năng thích ứng tối đa. Số lượng tham số kết quả do đó xuất hiện lớn (800M-1.1B cho LLM cơ sở quy mô 7B-13B). Nhưng mặc dù có ~1/10 tham số của LLM cơ sở, các speculator của chúng tôi thực thi trong vòng dưới 1/30 thời gian, do độ sâu nông và kiến trúc đơn giản của chúng.

Để huấn luyện các speculator của chúng tôi, chúng tôi tận dụng nền tảng huấn luyện phân tán Foundation Model Stack hiệu quả được giới thiệu trong [2]. Chúng tôi điều chỉnh script huấn luyện Llama để thay vào đó đóng băng mô hình Llama cơ sở và thay vào đó huấn luyện speculator ở trên. Chúng tôi giữ cùng dataloader, tiện ích checkpoint, và các thành phần pipeline thuận tiện khác.

Vì speculator được huấn luyện từ đầu, chúng tôi có cơ hội căn chỉnh hoàn toàn nó với mô hình cơ sở: thay vì dự đoán văn bản ground truth, speculator nên huấn luyện trực tiếp đối với đầu ra mô hình cơ sở được tạo. Thật không may, tạo văn bản với Llama không thể song song hóa hiệu quả - toàn bộ lý do chúng tôi đang huấn luyện một speculator ngay từ đầu. Do đó chúng tôi áp dụng một mô hình huấn luyện 2-giai đoạn. Trong giai đoạn đầu tiên, chúng tôi huấn luyện trên các batch nhỏ với độ dài ngữ cảnh dài (4k token). Mô hình cơ sở tạo ra các vector embedding cho mỗi token song song, và speculator sử dụng các vector này, cộng với các token ground truth, để dự đoán các token ground truth tiếp theo. Đây là một nhiệm vụ tương tự như huấn luyện mô hình ngôn ngữ causal tiêu chuẩn, chỉ là chúng tôi đang dự đoán nhiều token tại mỗi vị trí (một cho mỗi giai đoạn của MLP speculator).

Trong giai đoạn 2, chúng tôi chuyển sang các batch lớn với độ dài chuỗi ngắn (256 token), được tạo từ chính mô hình cơ sở. Bằng cách chỉ tạo một chuỗi token ngắn, chúng tôi có thể tiếp tục huấn luyện một cách hợp lý hiệu quả song song, trong khi tinh chỉnh speculator để khớp rõ ràng với đầu ra mô hình cơ sở. Chúng tôi không quan sát thấy bất kỳ overfitting nhất quán nào đối với độ dài chuỗi ngắn - thực tế, trong một số trường hợp, speculator trở nên chính xác hơn trên các đầu vào dài hơn. Trong thiết lập của chúng tôi, tỷ lệ khoảng 5:2 các bước cho giai đoạn 1 so với giai đoạn 2 chuyển thành tỷ lệ khoảng 1:1 trong thời gian wall clock, vì vậy chúng tôi sử dụng tỷ lệ này để thuận tiện. Speculator huấn luyện hiệu quả trên dữ liệu văn bản proxy ground-truth trong nửa thời gian, và sau đó dành nửa còn lại cho việc căn chỉnh mô hình kém hiệu quả hơn.

Đến cuối huấn luyện giai đoạn 2, chúng tôi có thể tăng tốc một speculator Llama2-13B ba đầu từ 2.41 token per step ở cuối giai đoạn 1, lên 2.63 (tốt nhất trong 10 ứng viên). Các đường cong huấn luyện cho lần chạy này qua cả hai giai đoạn huấn luyện được cung cấp trong Hình 2.

Chúng tôi huấn luyện bốn vòng mô hình speculator, và thảo luận kết quả cho mỗi vòng. Đầu tiên là speculator 3-đầu proof-of-concept cho Llama2-7B, mà chúng tôi sử dụng để minh họa cả thành công và hạn chế của phương pháp của chúng tôi trong môi trường sản xuất. Thứ hai là speculator 3-đầu tương tự cho Llama2-13B, mà chúng tôi triển khai đến một server LLM nội bộ. Thứ ba là speculator 7-đầu cho Codellama-13B-instruct, đạt được tăng tốc tốt hơn nhiều do tính chất dự đoán được hơn của code. Cuối cùng, chúng tôi huấn luyện speculator 5-đầu cho mô hình Granite-20B code nội bộ, chứng minh khả năng mở rộng thêm với kích thước mô hình.

Tất cả các speculator được huấn luyện trên 32 NVIDIA A100 gpu, với latent dimensionality 4096, 15k bước cho huấn luyện giai đoạn 1, và 6k cho giai đoạn 2. Đối với các mô hình ngôn ngữ tự nhiên, các batch giai đoạn 1 chứa 1M token, và các batch giai đoạn 2 bao gồm 786M token được tạo từ 197M token của các tài liệu huấn luyện bị cắt ngắn. Bởi vì các mô hình code tính toán phức tạp hơn (7 hoặc 5 đầu so với 3) chúng tôi thu nhỏ kích thước batch xuống 786M token cho giai đoạn 1, và 524M/131M token được tạo/ground truth cho giai đoạn 2. Tất cả các layer được khởi tạo với độ lệch chuẩn 4096^(-1/2), và các phép chiếu state/embedding được weighted sao cho đầu vào vector trạng thái của mô hình cơ sở chiếm 50% phương sai của trạng thái tại đầu cuối cùng, theo kỳ vọng. Learning rate được đặt thành 1e-3 cho giai đoạn 1, với warmup trong 5% đầu của huấn luyện và cosine decay xuống 1e-4. Giai đoạn 2 tuân theo cùng phương án LR, nhưng với max LR là 1e-4 và annealing xuống 1e-5, vì chúng tôi thấy rằng các giá trị cao hơn khiến loss tăng vọt mạnh trong quá trình chuyển đổi từ giai đoạn 1 sang 2.

Trong Bảng 1 chúng tôi theo dõi độ trễ per-token của greedy inference với mô hình Llama2-7B và speculator được huấn luyện của chúng tôi trên một A100 duy nhất, ở độ chính xác fp16, như một hàm của kích thước batch (b), độ dài prompt (p), và số lượng ứng viên đồng thời (k). Speculator tạo ra một cây với 6 nhánh tại đầu đầu tiên, 3 tại đầu thứ hai, và 2 tại đầu thứ ba, và chúng tôi cắt tỉa cây 6*3*2 = 36 ứng viên này xuống k mong muốn dựa trên độ tin cậy. Để tham khảo, chúng tôi bao gồm non-speculative inference dưới tiêu đề k=0, và trung bình token được tạo mỗi bước (τ) qua tất cả kích thước batch. Độ trễ được tính toán trên 256 thử nghiệm của chính xác 100 token được tạo. Prompt là các tài liệu Webhose được cắt ngắn đến độ dài prompt mong muốn.

Từ bảng chúng ta có thể đưa ra một vài quan sát đáng chú ý. Đầu tiên, các tăng tốc logic của chúng tôi (τ, tăng asymptotically với k) thực sự chuyển thành tăng tốc wall-clock, nhưng không một-đến-một. Điều này có ý nghĩa trong môi trường sản xuất, vì triển khai suy luận của chúng tôi đã được tối ưu hóa cao và torch compiled [5]. Độ trễ baseline b=1, k=0 của chúng tôi là 10.54 millisecond per token chuyển thành 94.9 token per second cho Llama2-7B, so với baseline triển khai gpt-fast của EAGLE là 55.1 token per second [7]. Lưu ý rằng speculative decoding dựa vào băng thông GPU chưa được sử dụng để song song hóa việc đánh giá nhiều ứng viên và token trong quá trình forward pass mô hình cơ sở. Nếu throughput GPU đã được tối đa hóa, điều đó để lại ít tài nguyên hơn để dành cho tăng tốc dựa trên speculative decoding. Ở đây chúng tôi quan sát được tăng tốc dưới 2x cho độ dài prompt 64, kích thước batch 1, và 4 ứng viên mỗi bước. Điều này ít hơn token per step đo được của chúng tôi (2.67), nhưng vẫn là một cải thiện khổng lồ trong môi trường sản xuất!

Thứ hai, và phù hợp với trực giác trên về băng thông GPU hạn chế: khi độ dài prompt và kích thước batch tăng, tăng tốc từ speculative decoding bị xói mòn. Đối với prompt lớn và kích thước batch, speculative decoding có thể chậm hơn gấp đôi so với non-speculative decoding, mặc dù tạo ra gần ba token cho mỗi lần forward pass đơn. Điều này ít đáng ngạc nhiên hơn khi chúng ta xem xét rằng đối với b=4, k=32 và 4 token per ứng viên (1 token ground truth từ mô hình cơ sở cộng với 3 token hậu tố speculated), chúng tôi đang chạy 512 token per step. Về mặt FLOP thô, khối lượng công việc non-speculative tương đương là kích thước batch là 512, vượt xa khả năng của gpu A100, mà chúng tôi quan sát thất bại tại b≈128 do giới hạn bộ nhớ. Do đó trong khi là một kỹ thuật đầy hứa hẹn cho tăng tốc suy luận, tăng tốc wall-clock từ speculative decoding mở rộng nghịch đảo với cả khối lượng công việc tính toán và mức độ tối ưu hóa có sẵn của mô hình cơ sở.

Cuối cùng, do sự suy giảm hiệu suất này, số lượng tối ưu các ứng viên song song cho một kích thước batch nhất định thường khá nhỏ trong thiết lập của chúng tôi: ít hơn 8 trong tất cả trường hợp cho độ dài prompt 64, và ít hơn 4 khi độ dài prompt là 2048. Điều này trái ngược với công việc trước đây trong speculative decoding, thường có các tập ứng viên lớn hơn. Ví dụ, Lookahead Decoding [4] có được kết quả tốt nhất của họ cho k=15, trong khi [9] công bố kết quả cho k=32. Trong trường hợp của chúng tôi, việc thổi phồng tensor đầu vào đến kích thước này gây hại nhiều hơn là giúp ích, và cũng đặt câu hỏi về đóng góp của tree attention trong triển khai của chúng tôi, vì có khả năng sẽ có rất ít token dư thừa khi k chỉ là 4 hoặc ít hơn.

Tóm lại, kết quả speculator Llama2-7B chỉ ra rằng speculative decoding tạo ra tăng tốc vật chất đáng kể trong môi trường sản xuất. Tuy nhiên, người thực hành phải cẩn thận tính đến tải tính toán. Việc lựa chọn hyperparameter và thậm chí thuật toán có thể đòi hỏi thích ứng động với khối lượng công việc tại runtime.

Chuyển ra ngoài proof of concept trong Llama2-7B, chúng tôi huấn luyện một speculator mới, sử dụng cùng hyperparameter, cho Llama2-13B. Kiến trúc speculator vẫn giữ nguyên, ngoại trừ đầu vào vector trạng thái ban đầu bây giờ có dimensionality 5120 thay vì 4096, vì độ rộng mô hình cơ sở đã tăng. Sau đó chúng tôi triển khai mô hình cơ sở được tăng tốc đến một server suy luận mô hình IBM nội bộ. Benchmark về độ trễ và throughput cho k=[1,5] và b khác nhau được hiển thị trong Hình 3. Chúng tôi quan sát được sự giảm 2x tương tự trong độ trễ cho b=1 và k=5. Tuy nhiên, phù hợp với hành vi quan sát được cho 7B, sự cải thiện này dần dần biến mất cho b lớn hơn, cho đến khi speculative decoding với k=5 trở nên chậm hơn đáng kể so với baseline.

May mắn thay, chúng tôi có thể giảm thiểu hành vi này bằng cách cố định k=1, đánh đổi độ trễ cao hơn một chút cho b<4 để có hành vi tốt hơn nhiều tại b>4. Trong trường hợp cực đoan, chúng tôi cũng có thể tắt speculative decoding một cách động cho b trên một ngưỡng nhất định, chẳng hạn như 32. Bằng cách này, chúng tôi có thể đạt được tăng tốc >2x trong trường hợp tốt nhất, và tái tạo hoàn hảo hành vi baseline trong trường hợp xấu nhất. Những kết quả này cho thấy rằng trong bối cảnh sản xuất quy mô lớn, một speculator được huấn luyện tốt có thể hoạt động tốt nhất với k=1, và không có xử lý đặc biệt multi-candidate, vì cải thiện biên trong recall không đáng để chịu chi phí đánh giá các ứng viên bổ sung.

Chuyển sang một domain dữ liệu khác, chúng tôi huấn luyện một speculator cho Codellama-13B-instruct. Bởi vì code được formalized hơn, lặp lại hơn, và do đó dự đoán được hơn so với ngôn ngữ tự nhiên, chúng tôi tăng số lượng stage/head trong speculator của chúng tôi từ 3 lên 7 (tăng độ dài đầu vào từ 4 lên 8) để tận dụng tính dự đoán được này. Benchmark hiệu suất được đưa ra trong Bảng 2 cho độ dài prompt 64 và 512.

Ở đây chúng tôi quan sát rằng các head tăng lên phù hợp với khối lượng công việc code. Bằng cách gấp đôi độ dài n-gram tối đa có thể chấp nhận, chúng tôi gần như gấp đôi độ dài n-gram được chấp nhận trung bình τ. Tăng tốc logic >5x này tạo ra tăng tốc wall-clock >3x, cho phép chúng tôi vận hành mô hình 13B parameter ở độ chính xác fp16 với throughput 181.5 token per second (5.51ms/token tại b=2, k=2, p=64).

Ngoài hiệu suất được cải thiện đáng chú ý, kết quả trong Bảng 2 phần lớn phản ánh những kết quả trong Bảng 1. Cải thiện giảm dần khi kích thước batch và độ dài prompt tăng, thu nhỏ k tối ưu, vẫn nhỏ nói chung. Một sự khác biệt là thực tế rằng độ chính xác speculator, như được chỉ ra bởi τ, thực tế tăng nhẹ với độ dài prompt, trái ngược với việc giảm nhẹ trong Bảng 1.

Cuối cùng, chúng tôi huấn luyện một speculator cho một mô hình code nội bộ lớn hơn, Granite-20B. Do hạn chế bộ nhớ tại thời điểm huấn luyện, chúng tôi giảm số lượng speculator head trong code speculator của chúng tôi từ 7 xuống 5, và huấn luyện tương tự như đối với Codellama-13B. Tương tự như speculator Llama2-13B của chúng tôi, sau đó chúng tôi triển khai mô hình này đến server nội bộ của chúng tôi và benchmark độ trễ và throughput qua các kích thước batch, với kết quả được đưa ra trong Hình 4.

Hành vi phản ánh kết quả trước đây, mặc dù kích thước mô hình lớn hơn. Phù hợp với Codellama-13B, chúng tôi thấy tăng tốc khoảng 3x so với baseline non-speculative trong trường hợp tốt nhất. Tương tự như Llama2-13B, chúng tôi thấy rằng cố định k=1 mua cho chúng tôi hành vi mở rộng tốt hơn nhiều, với chi phí là giảm độ trễ nhẹ cho kích thước batch rất nhỏ. Khối lượng công việc heterogeneous mở rộng tốt hơn so với homogeneous, mặc dù với throughput thấp hơn, do kích thước batch nhỏ hơn trong thực tế (người dùng mô phỏng không còn đồng bộ hóa nhân tạo các cuộc gọi của họ). Và bằng cách tắt speculative decoding cho kích thước batch lớn, chúng tôi có thể đạt được điều tốt nhất của cả hai thế giới.

Mặc dù đạt được tăng tốc 2-3x trong môi trường sản xuất là đáng kể, phương pháp được thảo luận cho đến nay đại diện cho một nỗ lực đầu tiên phiên bản 1. Một số con đường có sẵn để khám phá để cải thiện thêm hiệu quả của các mô hình speculator của chúng tôi.

Đầu tiên, các speculator ngôn ngữ tự nhiên của chúng tôi bị hạn chế bởi số lượng head hạn chế. Bởi vì chúng tôi chỉ speculate 3 token mỗi lần, chúng tôi bị giới hạn ở 4 token per step (1 dự đoán ground truth từ mô hình cơ sở, cộng với tối đa 3 token speculated được chấp nhận). Tăng số lượng head lên 4, như trong [1], hoặc 5, như trong [7] và [10], là một cách đơn giản và dễ dàng để cải thiện hiệu suất của các mô hình của chúng tôi. Cùng một lập luận cũng có thể được áp dụng cho mô hình Granite-20B 5-head code của chúng tôi, mặc dù thực tế chúng tôi đạt được cùng cải thiện độ trễ wall-clock như đối với speculator Codellama-13B 7-head của chúng tôi, gợi ý rằng head 6 và 7 có thể dư thừa. Cần nghiên cứu thêm.

Thứ hai, chúng tôi đã từ chối tie bất kỳ trọng số nào trong các speculator của mình, hoặc giữa các head/stage, hoặc giữa speculator và mô hình cơ sở. Điều này là do làm như vậy không cung cấp tăng tốc wall-clock. Tuy nhiên, giảm số lượng tham số thông qua weight tying có thể hữu ích về mặt thu nhỏ chi phí bộ nhớ GPU, đặc biệt khi chúng tôi tăng số lượng head/stage trong các speculator của mình. Nó cũng có thể dẫn đến hội tụ được cải thiện, ví dụ bằng cách khởi tạo các embedding layer hoặc shared layer thành các giá trị giàu ngữ nghĩa của embedding layer của mô hình cơ sở. Làm việc trong cùng không gian latent với mô hình cơ sở cũng mở ra cơ hội để giới thiệu các auxiliary loss hữu ích, như trong [7].

Tại thời điểm suy luận, k tối ưu phụ thuộc vào b, và vì vậy chúng tôi muốn hoán đổi động sang giá trị tối ưu cho mỗi yêu cầu suy luận đến, step to step, thay vì chỉ đơn giản bật/tắt giữa các giá trị mặc định k=(0,1,5). Tuy nhiên, kế hoạch mở rộng tối ưu sẽ cụ thể theo mô hình và phần cứng, và do đó, mặc dù mong muốn cho một triển khai speculative decoding hiệu quả, không có khả năng mang lại nhiều insight rộng hơn.

Trong báo cáo kỹ thuật này, chúng tôi đã giới thiệu một cải tiến mới về kiến trúc speculative decoding Medusa, cải thiện các chuỗi token được dự đoán bằng cách điều kiện hóa trên các token được lấy mẫu trước đó, ngoài vector trạng thái hiện tại của mô hình cơ sở. Sử dụng một quy trình huấn luyện hai giai đoạn, chúng tôi tạo ra các accelerator được huấn luyện cho Llama2-7B, Llama2-13B, Codellama-13B, và một mô hình Granite-20B nội bộ, và thảo luận tác động hiệu suất trên baseline cấp sản xuất được tối ưu hóa cao. Đặc biệt, speculative decoding mang lại tăng tốc hứa hẹn của nó trong các tình huống tối ưu, và chúng tôi có thể chạy cực kỳ nhanh ở độ chính xác fp16. Tuy nhiên, cải thiện suy giảm đối với khối lượng công việc lớn hơn, khi cửa sổ có sẵn để xử lý tăng tính song song trên GPU thu hẹp. Do đó, việc cung cấp tăng tốc đáng tin cậy thông qua speculative decoding trong môi trường sản xuất đòi hỏi thiết kế và nỗ lực chu đáo - và cẩn thận.

Chúng tôi cảm ơn Supriyo Chakraborty, cho các cuộc thảo luận thiết kế sâu sắc trong giai đoạn đầu của dự án, và Antoni Viros i Martin, Linsong Chu, và Brian Vaughan, cho các tư vấn có giá trị về code.
