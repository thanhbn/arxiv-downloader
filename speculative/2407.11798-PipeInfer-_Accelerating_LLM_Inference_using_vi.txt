# 2407.11798.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/speculative/2407.11798.pdf
# Kích thước tệp: 746315 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
PipeInfer: Tăng tốc Suy luận LLM sử dụng
Phỏng đoán Pipelined Bất đồng bộ
1Branden Butler,1Sixing Yu,2Arya Mazaheri,1Ali Jannesari
Đại học Bang Iowa1
Đại học Kỹ thuật Darmstadt2
{butler1, yusx, jannesar }@iastate.edu, arya.mazaheri@tu-darmstadt.de
Tóm tắt —Suy luận của các Mô hình Ngôn ngữ Lớn (LLM) trên
các cụm máy tính đã trở thành một điểm tập trung nghiên cứu trong
thời gian gần đây, với nhiều kỹ thuật tăng tốc lấy cảm hứng từ
thực thi phỏng đoán của CPU. Những kỹ thuật này giảm thiểu các
nút thắt liên quan đến băng thông bộ nhớ, nhưng cũng tăng độ trễ
end-to-end cho mỗi lần chạy suy luận, đòi hỏi tỷ lệ chấp nhận
phỏng đoán cao để cải thiện hiệu suất. Kết hợp với tỷ lệ chấp
nhận biến đổi giữa các tác vụ, các kỹ thuật suy luận phỏng đoán
có thể dẫn đến hiệu suất giảm. Ngoài ra, các thiết kế pipeline-
parallel đòi hỏi nhiều yêu cầu người dùng để duy trì sử dụng tối
đa. Như một biện pháp khắc phục, chúng tôi đề xuất PipeInfer,
một kỹ thuật tăng tốc phỏng đoán pipelined để giảm độ trễ inter-
token và cải thiện việc sử dụng hệ thống cho các tình huống
yêu cầu đơn lẻ đồng thời cũng cải thiện khả năng chịu đựng
với tỷ lệ chấp nhận phỏng đoán thấp và các kết nối băng thông
thấp. PipeInfer thể hiện sự cải thiện lên đến 2.15 × trong tốc
độ sinh so với suy luận phỏng đoán tiêu chuẩn. PipeInfer đạt
được sự cải thiện thông qua Phỏng đoán Bất đồng bộ Liên tục
và Hủy bỏ Suy luận Sớm, cái trước cải thiện độ trễ và tốc độ
sinh bằng cách chạy suy luận token đơn lẻ đồng thời với một
số lần chạy phỏng đoán, trong khi cái sau cải thiện tốc độ và
độ trễ bằng cách bỏ qua tính toán của các lần chạy bị vô hiệu
hóa, ngay cả trong quá trình suy luận.
Từ khóa chỉ mục —mô hình ngôn ngữ lớn, suy luận, phỏng đoán,
tăng tốc, phân tán, song song

I. GIỚI THIỆU
Các Mô hình Ngôn ngữ Lớn (LLM) đã trở nên cực kỳ phổ
biến trong thời gian gần đây, đặc biệt là do tính linh hoạt của
chúng trong các tác vụ như hiểu và sinh ngôn ngữ. Trong số
các kiến trúc khác nhau, các mô hình Transformer chỉ có
bộ giải mã như loạt GPT của OpenAI [1] và họ Llama của
Meta [2] đã trở nên nổi bật. Những mô hình này bao gồm
một loạt các lớp giải mã, mỗi lớp có kiến trúc giống hệt
nhau, được đặt giữa một lớp embedding đầu vào và một lớp
đầu ra [2], [3]. Không giống như các mô hình encoder-decoder,
các mô hình chỉ có decoder tập trung hoàn toàn vào việc sinh
các chuỗi đầu ra dựa trên đầu vào mà chúng nhận được.
Trong quá trình suy luận, mỗi lớp đưa đầu ra của nó trực
tiếp vào đầu vào của lớp kế tiếp. Bản chất tự hồi quy của
chúng có nghĩa là để sinh mỗi token đầu ra, tất cả các lớp
cần được đánh giá một cách lặp lại. Thiết kế này tạo ra một
thách thức đáng kể, vì kích thước mô hình vượt quá kích
thước cache của bộ xử lý đích, và một nút thắt băng thông
bộ nhớ trở nên rõ ràng [4], [5]. Nút thắt này ảnh hưởng đến
khả năng mở rộng và tốc độ xử lý của mô hình, tạo ra thách
thức trong các ứng dụng thời gian thực.

Bất chấp những thách thức này, phương pháp tự hồi quy được
ưa chuộng cho một số ứng dụng nhất định do khả năng duy trì
mức độ chính xác cao và hiểu biết ngữ cảnh trong việc sinh
chuỗi. Nghiên cứu hiện tại đang tích cực khám phá các giải
pháp để giảm thiểu những nút thắt này, bao gồm các tiến bộ
trong thiết kế phần cứng [6], các kiến trúc hiệu quả hơn [7],
và các kỹ thuật xử lý song song mới để khai thác suy luận
theo lô [8]. Suy luận theo lô ít bị ảnh hưởng bởi các hạn chế
băng thông do tăng tính địa phương thời gian và không gian,
cải thiện tỷ lệ hit cache của bộ xử lý với chi phí là tăng độ trễ.

Gần đây, các kỹ thuật sáng tạo nhằm giảm thiểu nút thắt
băng thông bộ nhớ trong LLM đã xuất hiện, lấy cảm hứng
từ khái niệm thực thi phỏng đoán của CPU, bao gồm
SpecInfer [8] và Staged Speculative Decoding [9]. Những
kỹ thuật này sử dụng một mô hình phụ nhỏ hơn để sinh một
cây các chuỗi phỏng đoán, sau đó được gộp lại và chạy qua
mô hình đích. Chìa khóa của quá trình này là việc suy luận
trên một cây phỏng đoán tạo ra các phân phối xác suất cho
tất cả các token trong cây, cho phép suy luận nhảy vọt nhiều
token cùng một lúc. Việc tăng tốc được thể hiện bởi những
kỹ thuật này là do hiệu quả lớn hơn mà xử lý theo lô mang
lại. Dựa trên khái niệm này, các kỹ thuật khác như Medusa [10]
và Lookahead Decoding [11] đã xuất hiện, điều chỉnh phương
pháp giải mã phỏng đoán bằng cách sửa đổi việc sinh và xác
minh các phỏng đoán.

Một thách thức đáng kể với những kỹ thuật phỏng đoán này
là sự gia tăng độ trễ, đặc biệt rõ ràng trong các tình huống
có độ chính xác phỏng đoán thấp. Cốt lõi của vấn đề nằm ở
sự cân bằng giữa thời gian tính toán và hiệu quả. Mặc dù
mỗi lần chạy suy luận có thể mất thời gian lâu hơn, hiệu quả
bị ảnh hưởng đáng kể nếu độ chính xác phỏng đoán thấp, dẫn
đến ít token đúng và được xác minh hơn. Sự mất cân bằng
này khiến các chi phí tính toán và thời gian vượt quá lợi ích
thu được từ các phỏng đoán đúng. Ngoài ra, trong các hệ
thống không đồng nhất như điện thoại thông minh với CPU,
GPU và NPU, chi phí đồng bộ hóa quá cao để tận dụng đầy
đủ tất cả các phần tử xử lý. Các kỹ thuật phỏng đoán có thể
giải quyết một phần điều này bằng cách chạy phỏng đoán trên
một phần tử xử lý và xác minh trên phần tử khác, nhưng các
phương pháp hiện tại chỉ chạy một giai đoạn tại một thời
điểm và do đó vẫn cho thấy việc sử dụng hạn chế.

Trong bài báo này, chúng tôi khắc phục những vấn đề này
bằng cách sửa đổi thuật toán suy luận phỏng đoán để chạy
nhiều lần xác minh đồng thời, khai thác kiến trúc pipeline-
parallel để đạt được việc sử dụng hệ thống cao đồng thời duy
trì chi phí giao tiếp thấp. Thiết kế của chúng tôi đạt được khả
năng phục hồi đáng kể đối với độ trễ kết nối và tính toán, cho
phép suy luận tốc độ cao trên các cụm phần cứng thông thường
khác nhau chi phí thấp. Chúng tôi tin rằng công việc tương lai
có thể xây dựng trên thiết kế của chúng tôi để đạt được việc
sử dụng cao hơn trong các hệ thống không đồng nhất bất chấp
độ trễ kết nối và sự khác biệt lớn về thông lượng. Cuối cùng,
thiết kế của chúng tôi không bị ràng buộc với các thuật toán
suy luận phỏng đoán. Chúng tôi tin rằng phương pháp này có
thể được mở rộng cho các kỹ thuật tăng tốc khác, như Lookahead
Decoding hoặc Medusa speculation heads.

So với suy luận phỏng đoán pipeline-parallel, chúng tôi quan
sát được sự cải thiện khoảng 1.5-2.15 × trong tốc độ sinh
tổng thể trong các trường hợp thử nghiệm của chúng tôi đồng
thời đạt được gần như không có sự chậm lại cho độ chính xác
phỏng đoán kém. Thử nghiệm với Gigabit Ethernet làm kết
nối cho thấy khả năng chịu đựng với các hạn chế về độ trễ
và thông lượng, tăng cải thiện so với suy luận phỏng đoán
trong những tình huống như vậy. Đối với các mô hình được
căn chỉnh tốt, chúng tôi quan sát được tốc độ sinh nhanh
hơn lên đến 1.7 × so với phỏng đoán pipeline-parallel, và
đối với các mô hình được căn chỉnh kém, chúng tôi quan sát
được sự cải thiện lên đến 2.15 ×.

Chúng tôi giới thiệu một số đóng góp chính cho lĩnh vực
giải mã phỏng đoán trong các mô hình ngôn ngữ lớn:

•Phỏng đoán và Suy luận Bất đồng bộ: Chúng tôi tăng
cường suy luận phỏng đoán bằng cách tích hợp các pipeline
tính toán riêng biệt về mặt vật lý để chạy suy luận token
đơn lẻ hoặc xác minh cây đồng thời với việc sinh cây phỏng
đoán. Sửa đổi này cho phép xử lý đồng thời, cải thiện đáng
kể hiệu quả tính toán và giảm độ trễ. Độ trễ time-to-first-
token đạt gần như ngang bằng với suy luận lặp không phỏng
đoán, trong khi việc sử dụng hệ thống tăng gấp đôi.

•Phỏng đoán Liên tục: Bằng cách tận dụng phỏng đoán
bất đồng bộ, chúng tôi đã thiết kế một phương pháp sinh
phỏng đoán liên tục trong các micro-batch nhỏ thay vì
các batch đơn lẻ lớn, cải thiện độ trễ end-to-end và giảm
hình phạt cho tỷ lệ chấp nhận phỏng đoán thấp. Với phỏng
đoán liên tục, chúng tôi quan sát thấy việc giảm độ trễ tỷ
lệ thuận với việc giảm kích thước batch. Chúng tôi cũng
quan sát thấy phỏng đoán liên tục cho phép PipeInfer thích
ứng với các tình huống băng thông thấp. Phỏng đoán liên
tục cải thiện tốc độ sinh của PipeInfer lên đến 1.5 ×.

•Pipelined KV Cache Multibuffering: Để bảo tồn mối
quan hệ nhân quả của các token được sinh, chúng tôi phân
đoạn các chuỗi KV cache thành các phần riêng tư cho mỗi
lần chạy phỏng đoán. Các thao tác cache được pipelined
để duy trì tính nhất quán trong quá trình suy luận, cho
phép các lần chạy phỏng đoán tránh tính toán các token
được chia sẻ bởi các lần chạy trước đó, ngay cả trước khi
chúng được hoàn thành. Bằng cách khai thác khả năng này,
chúng tôi cải thiện thông lượng tính toán theo một hệ số
tỷ lệ thuận với sự căn chỉnh của mô hình phỏng đoán.

•Hủy bỏ Suy luận Sớm: Chúng tôi đã thiết kế một phương
pháp xóa các lần chạy bị vô hiệu hóa khỏi pipeline bằng
cách truyền ngược một tín hiệu hủy bỏ bất đồng bộ, giảm
tác động hiệu suất của phỏng đoán liên tục với các mô hình
phỏng đoán được căn chỉnh kém. Một cách phản trực giác,
chúng tôi quan sát được các cải thiện tốc độ lớn hơn lên
đến 2.15× cho các mô hình được căn chỉnh kém nhờ vào
việc hủy bỏ suy luận sớm.

II. BỐI CẢNH VÀ ĐỘNG LỰC

Các Mô hình Ngôn ngữ Lớn dựa trên kiến trúc Transformer
decoder, bao gồm Llama 2 [2] và GPT [1], được xây dựng
từ một loạt các lớp decoder. Mỗi lớp decoder chứa một mô-
đun attention và một perceptron đa lớp [3]. Mô-đun attention
tính toán điểm số cho tất cả các token trong chuỗi, nhưng
các vector key và value thường được cache để ngăn ngừa
việc tính toán lại không cần thiết.

Trong quá trình suy luận, mỗi lớp decoder được đánh giá
theo trình tự, đòi hỏi tải các trọng số cho mô-đun attention
và MLP, cũng như các mục liên quan từ KV cache. Kích
thước khổng lồ của hầu hết các mô hình hiện đại đòi hỏi
nhiều gigabyte chuyển đổi bộ nhớ trong một lần chạy suy
luận duy nhất, dẫn đến giới hạn băng thông bộ nhớ cho
kích thước batch nhỏ [9]. Hạn chế băng thông này phản
ánh các ràng buộc tương tự trong pipeline CPU, gây ra
các stall đáng kể khi các phần tử xử lý chờ đợi các giá
trị từ bộ nhớ [4]. Lấy cảm hứng từ thực thi phỏng đoán
của CPU, nhiều thiết kế tương tự cho suy luận phỏng đoán
đã được tạo ra để giảm thiểu nút thắt này [8]–[10].

A. Suy luận Phỏng đoán

Suy luận phỏng đoán hoạt động thông qua hai thành phần chính:
giai đoạn phỏng đoán và giai đoạn xác minh.

1) Phỏng đoán: Giai đoạn phỏng đoán bao gồm một tập hợp
các mô hình phụ được ghép nối với mô hình đích chính. Các
mô hình phụ được chọn nhỏ hơn và nhanh hơn để chạy so
với mô hình đích chính. Các mô hình phỏng đoán được chạy
đầu tiên trên chuỗi đầu vào, sinh ra nhiều chuỗi đầu ra một
cách lặp lại. Chúng tiếp tục quá trình này cho đến khi xác
suất đầu ra cao nhất giảm xuống dưới một ngưỡng được chỉ
định biểu thị mức độ tin cậy thấp nhất được phép cho một
token phỏng đoán. Khi đạt được ngưỡng cắt này, cây toàn
diện của các chuỗi phỏng đoán, bao gồm tất cả các đầu ra
tiềm năng được suy ra cho đến nay, được chuẩn bị cho giai
đoạn tiếp theo.

2) Xác minh: Giai đoạn xác minh lấy toàn bộ cây phỏng
đoán và sinh ra một attention mask đặc biệt để đảm bảo rằng
các chuỗi trong cây vẫn loại trừ lẫn nhau về mặt khả năng
nhìn thấy token của chúng. Kỹ thuật masking này bảo tồn
các mối quan hệ nhân quả vốn có trong các chuỗi, đồng thời
ngăn ngừa bất kỳ sự can thiệp chéo nào giữa chúng.

Sau đó, cây phỏng đoán và attention mask tương ứng của
nó được tích hợp vào pipeline suy luận của mô hình đích.
Khi nhiều token được đưa vào pipeline suy luận, đầu ra là
một tập hợp các vector logit cho mỗi token trong cây đầu
vào. Giai đoạn xác minh sau đó sử dụng những vector này
để so sánh lặp lại các token phỏng đoán với phân phối xác
suất của mô hình đích tại vị trí của token đó. Nếu token
phỏng đoán khớp với một token có thể được lấy mẫu từ
phân phối, giai đoạn xác minh chấp nhận token phỏng đoán
và tiếp tục xác minh các token khác trong chuỗi đó. Ngược
lại, sự không khớp dẫn đến việc lấy mẫu một token mới từ
phân phối xác suất và sau đó kết thúc việc xác minh.

Chạy một lần xác minh hiệu quả hơn việc chạy từng token
qua toàn bộ mô hình. Phương pháp xử lý theo lô cho phép
tái sử dụng trọng số lớp, giảm tần suất loại bỏ cache CPU.
Ngoài ra, cơ chế suy luận phỏng đoán bao gồm một tập hợp
các xác suất dự đoán cho mỗi nút lá trong cây phỏng đoán.
Những xác suất này được tận dụng để dự đoán token tiếp
theo cho các chuỗi hoàn toàn đúng, đảm bảo rằng suy luận
mô hình đích luôn có năng suất, tránh các tình huống mà
một lần chạy trở nên hoàn toàn vô nghĩa.

B. KV Cache

Khi hoàn thành giai đoạn xác minh, hệ thống phải sửa đổi
KV cache cho cả mô hình đích và mô hình phỏng đoán. KV
cache là một phương pháp cải thiện tốc độ sinh bằng cách
cache các vector attention cho các token trước đó [3]. Các
mục tương ứng với các token bị từ chối phải được loại bỏ
hoặc bị che trong các lần chạy xác minh tiếp theo. Các triển
khai phổ biến, như llama.cpp [12], gắn metadata vào mỗi
ô KV cache, xác định vị trí của mục và nó thuộc về chuỗi
nào. Các triển khai như vậy thực hiện các sửa đổi cache
cần thiết bằng cách sửa đổi metadata thay thế. Metadata
sau đó được sử dụng để xây dựng attention mask.

C. Thách thức của Suy luận Phỏng đoán

Trong suy luận phỏng đoán tiêu chuẩn, một nút thắt đáng chú
ý phát sinh từ yêu cầu rằng suy luận của mô hình đích phải
chờ đợi cho đến khi hoàn thành các chuỗi phỏng đoán được
sinh bởi các mô hình phỏng đoán. Thời gian chờ đợi này gây
ra một độ trễ đáng kể đối với độ trễ time-to-first-token, một
thước đo quan trọng về khả năng phản hồi của hệ thống. Ngoài
ra, độ phức tạp và kích thước của các mô hình phỏng đoán bị
hạn chế. Bất kỳ sự gia tăng nào trong độ trễ suy luận của
những mô hình phỏng đoán này đều có tác động trực tiếp và
tỷ lệ thuận đến độ trễ của toàn bộ hệ thống.

Công việc hiện có [8] cũng giả định rằng các mô hình phỏng
đoán được chạy trên cùng hệ thống với mô hình đích, đòi hỏi
hoặc VRAM GPU đáng kể hoặc số lượng node đáng kể để
chia trọng số mô hình đích giữa chúng. Tại bất kỳ thời điểm
nào, chỉ có một tập hợp các trọng số này được sử dụng tích
cực, cho thấy tính khả thi lý thuyết của việc tạm thời page
chúng ra CPU RAM hoặc không gian đĩa khi cần thiết. Tuy
nhiên, phương pháp này tạo ra các thách thức và không hiệu
quả riêng của nó, đặc biệt là về mặt quản lý tài nguyên và
thời gian truy cập. Sự đánh đổi ở đây nằm ở việc cân bằng
yêu cầu truy cập nhanh đến những trọng số này so với các
hạn chế do tài nguyên phần cứng áp đặt, đặc biệt trong các
tình huống mà VRAM hoặc các node tính toán đang khan
hiếm. Chúng tôi giải quyết vấn đề này bằng cách chuyển
các mô hình phỏng đoán sang một pipeline chuyên dụng,
cho phép các tối ưu hóa thêm như phỏng đoán bất đồng bộ.

III. CÔNG VIỆC LIÊN QUAN

Giải mã phỏng đoán đã được khám phá trước đó thông qua
SpecInfer [8] và Staged Speculative Decoding [9]. SpecInfer
sử dụng nhiều mô hình phỏng đoán được thực thi song song
với nhau, nhưng không chạy các mô hình phỏng đoán và đích
song song, dẫn đến tăng độ trễ end-to-end. Staged Speculative
Decoding có cách tiếp cận hơi khác, trong đó bản thân các
mô hình phỏng đoán được suy luận phỏng đoán, cải thiện
tốc độ tổng thể nhưng phải chịu hình phạt độ trễ thậm chí
lớn hơn, vì các phỏng đoán theo giai đoạn cũng không được
chạy song song với các mục tiêu tương ứng của chúng.

Medusa [10] có cách tiếp cận hơi khác với giải mã phỏng
đoán, thêm các sampling head mới vào mô hình đích để tạo
ra các phỏng đoán mà không cần mô hình phụ. Medusa không
gây ra chi phí độ trễ đáng kể nhưng đòi hỏi huấn luyện các
sampling head mới cho mô hình đích.

Lookahead Decoding [11] có cách tiếp cận hoàn toàn khác,
lựa chọn sử dụng lặp Jacobi để sinh nhiều token đồng thời
bằng cách sinh N-gram dựa trên quỹ đạo của các token hiện
tại. Các n-gram được sinh được cache và sau đó được xác
minh trong một giai đoạn riêng biệt. Lookahead Decoding
thể hiện việc sử dụng cao và độ trễ thấp trên các hệ thống
node đơn, nhưng không tính đến các hệ thống đa node hoặc
các hệ thống có kết nối chậm.

Một công việc gần đây có tên SPEED [13] sử dụng các kỹ
thuật phỏng đoán tương tự để tăng hiệu quả giải mã, nhưng
thay vào đó nhắm mục tiêu suy luận trên một GPU duy nhất.
Ngoài ra, SPEED sinh các phỏng đoán từ các trạng thái ẩn
của các lớp trước đó, loại bỏ nhu cầu về một mô hình phỏng
đoán riêng biệt. Cách tiếp cận này tương phản với PipeInfer,
sử dụng một mô hình phụ để sinh phỏng đoán. Mặc dù sự
khác biệt này ảnh hưởng đến chi phí tính toán, nó cũng ảnh
hưởng đến tính linh hoạt và khả năng thích ứng của hệ thống
với các tình huống mô hình hóa khác nhau. Hơn nữa, việc
vô hiệu hóa các phỏng đoán dẫn đến tạm dừng cho toàn bộ
hệ thống trong SPEED, trong khi PipeInfer tiếp tục suy luận
tất cả các lần chạy hợp lệ khác cùng lúc với việc thực hiện
các hoạt động vô hiệu hóa và xóa. Cuối cùng, SPEED nhắm
mục tiêu các mô hình chuyên biệt triển khai chia sẻ tham số,
trong khi PipeInfer hoạt động trên nhiều mô hình off-the-shelf.

Khai thác pipeline parallelism để cải thiện độ trễ suy luận
đồng thời chạy phỏng đoán đã được khám phá trước đó [14].
Tuy nhiên, công việc này thực hiện phỏng đoán thông qua
các phương pháp tương tự như SPEED, dự đoán một token
tiếp theo duy nhất từ các trạng thái ẩn của các lớp trung
gian, đòi hỏi các sampling head bổ sung được huấn luyện.
Ngoài ra, phương pháp này chỉ phỏng đoán một token trước
tại một thời điểm, hạn chế tốc độ tăng có thể so với các
phương pháp khác sử dụng các mô hình phỏng đoán hoặc
head riêng biệt.

Công việc cũng đã được thực hiện sử dụng tensor parallelism,
phân phối các hoạt động sub-layer trên các node thay vì
toàn bộ các lớp [15]. Tuy nhiên, các phương pháp như vậy
đã được phát hiện bị ảnh hưởng bởi các nút thắt băng thông
kết nối cực kỳ nghiêm trọng, ngay cả đối với các node tính
toán cực kỳ chậm như Raspberry Pi 4. Thậm chí chỉ tám
node đã phát sinh thời gian đồng bộ hóa cao hơn thời gian
dành để chạy bản thân suy luận.

Các kỹ thuật không phỏng đoán cũng đã được khám phá
trong lĩnh vực tăng tốc suy luận. Một kỹ thuật nổi bật dựa
trên suy luận early exit, trong đó một số lớp mô hình sau
được bỏ qua nếu đầu ra từ một lớp trước đó có thể được
sử dụng thay thế. CALM [16], ví dụ, huấn luyện một mô
hình phân loại để phát hiện khi một lớp trước đó đủ tin
cậy vào đầu ra của nó. Tương tự, Depth-Adaptive Transformers
[17] sử dụng các bộ phân loại để dự đoán độ sâu mà tại
đó suy luận có thể dừng lại, hoặc theo token hoặc theo chuỗi.
Một chiến lược early-exit khác có tên LITE [18] tăng tốc
suy luận đồng thời duy trì độ chính xác đầu ra, một vấn đề
làm khổ các hệ thống tương tự khác. Giải mã early exit
cho thấy triển vọng nhưng đòi hỏi huấn luyện các bộ phân
loại cho mô hình đích.

Các kỹ thuật tăng tốc không phỏng đoán khác tập trung
vào việc sửa đổi độ chính xác mà tại đó suy luận được
thực hiện. Những kỹ thuật như vậy bao gồm các chiến lược
quantization như AWQ [19] và QuIP [20] hoặc các chiến
lược pruning như SparseGPT [21] và SparseML [22]. Các
kỹ thuật khác như SqueezeLLM [23] kết hợp sparsity và
quantization. Các phương pháp quantization và pruning
thường đòi hỏi một bước chuyển đổi offline để nén các
trọng số mô hình gốc nhưng có thể ít tốn kém hơn đáng
kể so với một lần chạy pre-training đầy đủ.

PipeInfer không đòi hỏi bất kỳ bước pre-training hoặc
chuyển đổi nào, nhưng cũng không xung đột với các kỹ
thuật quantization hoặc pruning.

IV. PHƯƠNG PHÁP PIPEINFER

Phương pháp được đề xuất của chúng tôi, PipeInfer, tăng
cường suy luận phỏng đoán và bao gồm bốn thành phần chính:
Phỏng đoán Bất đồng bộ, Phỏng đoán Liên tục, Pipelined
KV Cache Multibuffering, và Hủy bỏ Suy luận Sớm. Triển
khai tham chiếu của chúng tôi được xây dựng trên llama.cpp [12],
với giao tiếp inter-node được thực hiện bằng MPI [24]. Sơ đồ
hệ thống tổng thể được hiển thị trong Hình 1.

A. Phỏng đoán Bất đồng bộ

Yêu cầu pipeline suy luận mô hình đích chờ đợi các chuỗi
phỏng đoán được sinh bởi các mô hình phỏng đoán tăng cả
độ trễ time-to-first-token (TTFT) và độ trễ inter-token.
PipeInfer giảm những độ trễ này thông qua Phỏng đoán Bất
đồng bộ, trong đó pipeline đích chạy song song với phỏng
đoán. Để thực hiện điều này, PipeInfer sử dụng hai pipeline
tính toán liên kết lỏng lẻo, một cho mô hình đích và một
cho mô hình phỏng đoán. Sau khi xử lý prompt ban đầu,
cả hai pipeline đều được cung cấp token được sinh đầu tiên.
Pipeline đích chạy suy luận trên token đơn này, trong khi
pipeline phỏng đoán sinh một cây các chuỗi phỏng đoán.
Khi cây phỏng đoán được hoàn thành, nó được đưa vào
pipeline đích, thực hiện xác minh cây. Khi hoàn thành lần
chạy suy luận đầu tiên, các logit được chuyển đến head node,
thực hiện sampling, và quá trình lặp lại.

Để độ chính xác phỏng đoán lớn hơn, các mô hình phỏng
đoán lớn hơn có thể được sử dụng mà không tăng đáng kể
độ trễ suy luận của hệ thống do thiết kế bất đồng bộ: một
mô hình phỏng đoán lớn hơn đòi hỏi nhiều thời gian hơn
để sinh cây phỏng đoán, nhưng pipeline đích đồng thời
chạy suy luận. Nhiều mô hình phỏng đoán cũng có thể được
sử dụng, với các kích thước khác nhau sao cho mô hình
phỏng đoán nhỏ nhất và ít chính xác nhất nhanh chóng sinh
một cây phỏng đoán để giữ cho pipeline đích đầy, trong
khi mô hình phỏng đoán lớn nhất sinh một cây chính xác hơn.

1) Theo dõi trạng thái: Mỗi lần chạy của pipeline đích
được theo dõi trong một cấu trúc dữ liệu chứa đồ thị tính
toán, batch được sử dụng để bắt đầu lần chạy, và một mảng
các chỉ số ánh xạ mỗi token trong batch đến tập hợp logit
tương ứng. Cấu trúc dữ liệu được tạo ngay trước khi lần
chạy bắt đầu và được đặt trong một hàng đợi FIFO. Khi
lần chạy bắt đầu, head node gửi dữ liệu cấu hình xuống
pipeline, chi tiết thông tin như kích thước batch và mảng
các chuỗi cho mỗi token. Head node sau đó đánh giá một
vài lớp đầu tiên theo phân chia được cấp phát. Khi hoàn
thành, các tensor activation được gửi đến node tiếp theo.
Tất cả các hoạt động gửi được hoàn thành bằng triển khai
có buffer, cho phép node gửi tiếp tục trước khi node nhận
sẵn sàng.

2) Giao dịch hoạt động pipeline: Hầu hết các hoạt động
pipeline, như gửi dữ liệu cấu hình hoặc tensor activation,
được sắp xếp nghiêm ngặt để tensor activation không thể
được nhận trước khi dữ liệu cấu hình cần thiết được nhận
và xử lý. PipeInfer thực hiện điều này bằng cách sử dụng
MPI tag: một thông điệp bắt đầu chỉ ra sự khởi đầu của
một giao dịch, một cấu trúc được định nghĩa bởi PipeInfer
để chỉ ra một hoạt động nguyên tử duy nhất phải được thực
thi theo cùng thứ tự như nhận được. Thông điệp bắt đầu
giao dịch chứa tag xác định loại giao dịch, và một handler
trên mỗi worker node gọi hàm tương ứng với loại đó. Tất
cả các cuộc gọi MPI send và receive trong hàm đó sử dụng
tag được gán cho loại giao dịch. Các giao tiếp point-to-point
MPI không vượt qua cho các thông điệp có cùng người gửi,
người nhận và tag [24], vì vậy bằng cách này, chúng tôi
đảm bảo thứ tự xác định của các giao dịch pipeline. Quá
trình này được hiển thị trong Hình 2.

B. Phỏng đoán Liên tục

Phỏng đoán bất đồng bộ cải thiện độ trễ end-to-end, nhưng
vẫn có sự sử dụng chưa đầy đủ đáng kể trong tình huống
yêu cầu đơn lẻ. Sau khi cây phỏng đoán được sinh, phần
lớn hệ thống vẫn nhàn rỗi cho đến khi pipeline đích hoàn
thành lần chạy không phỏng đoán ban đầu. Đây không phải
là vấn đề đối với các pipeline ngắn vì việc sử dụng hệ thống
tỷ lệ thuận cao hơn, nhưng các pipeline dài hơn bị ảnh hưởng
bởi các bong bóng không hoạt động lớn.

Phỏng đoán Liên tục giảm kích thước của những bong bóng
này bằng cách sinh cây phỏng đoán bất cứ khi nào head node
sẽ nhàn rỗi. Trạng thái nhàn rỗi được xác định bằng cách
thăm dò một giao dịch chuyển logit đến. Nếu một giao dịch
đang chờ được xử lý, head node gọi quy trình sampling và
xác minh. Nếu không, node sinh một cây phỏng đoán khác.
Bằng cách sinh phỏng đoán một cách cơ hội, việc sử dụng
hệ thống cải thiện tỷ lệ thuận với độ sâu pipeline. Dòng thời
gian của phỏng đoán liên tục được hiển thị trong Hình 3.

Các cây phỏng đoán được sinh theo cách này xây dựng
trên các cây trước đó, vì vậy nếu một phỏng đoán trước
đó bị từ chối, nhiều lần chạy phỏng đoán bị vô hiệu hóa.

1) Microbatching: Một vấn đề ngay lập tức với phỏng
đoán liên tục liên quan đến kích thước của các cây phỏng
đoán. Trong suy luận phỏng đoán tiêu chuẩn và phỏng đoán
bất đồng bộ, các cây lớn hơn có tiềm năng cải thiện tốc độ
sinh so với các cây nhỏ hơn, vì các batch lớn hơn gây ra
ít cache miss CPU hơn và ít giao tiếp inter-node hơn so
với nhiều batch nhỏ hơn. Tuy nhiên, kích thước của cây
phỏng đoán tăng tỷ lệ thuận với độ trễ suy luận. Ngoài ra,
khi độ sâu của cây tăng, xác suất một chuỗi hoàn chỉnh
được chấp nhận giảm do sự phân kỳ giữa đầu ra của mô
hình phỏng đoán và mô hình đích. Do đó, các batch lớn hơn
có thể gây ra độ trễ cao hơn mà không cải thiện số lượng
token được chấp nhận.

Nguyên tắc tương tự áp dụng cho phỏng đoán liên tục nhưng
được phóng đại bởi số lượng lớn hơn của các lần chạy phỏng
đoán. Như một biện pháp cân bằng, Pipeinfer sinh các micro-
batch phỏng đoán, có kích thước từ 1 đến 4 token. Kích
thước batch nhỏ hơn cải thiện độ trễ suy luận với chi phí
là tăng áp lực băng thông bộ nhớ. Lợi ích của micro-batch
là ba mặt: (1) sự mất cân bằng giữa các lần chạy phỏng
đoán và không phỏng đoán được giảm, giảm kích thước của
các bong bóng không hoạt động liên quan đến sự mất cân
bằng này; (2) chia một batch phỏng đoán lớn thành nhiều
micro-batch cho phép hệ thống cập nhật các token đã biết
đúng của mô hình phỏng đoán sau khi xác minh chỉ một
micro-batch duy nhất, cải thiện tỷ lệ chấp nhận tổng thể;
(3) micro-batch cho phép granularity tinh hơn trong bối
cảnh Hủy bỏ Suy luận Sớm, một thành phần khác của
PipeInfer. Microbatching cải thiện độ trễ inter-token tổng
thể, jitter độ trễ run-to-run, tỷ lệ chấp nhận token, và việc
sử dụng hệ thống.

2) Phỏng đoán phản ứng: Khi số lượng cây phỏng đoán
tăng, xác suất tất cả các token được chấp nhận giảm, vì
các chuỗi cuối cùng phân kỳ khỏi mô hình đích. Như một
nỗ lực ngăn ngừa tính toán lãng phí, chúng tôi đã thêm một
tham số khác vào phỏng đoán liên tục được gọi là hệ số
phục hồi ngưỡng tin cậy. Hệ số phục hồi này là một giá
trị floating point được thêm vào ngưỡng tin cậy phỏng đoán
gốc cho mỗi lần lặp thành công của phỏng đoán liên tục,
được reset khi chấp nhận một lần chạy hoàn thành. Hiệu
ứng là một gradient tăng dần của độ tin cậy cần thiết để
tiếp tục phỏng đoán, giảm xác suất tính toán lãng phí.

Chúng tôi cũng thêm nghịch đảo, hệ số suy giảm ngưỡng
tin cậy, được trừ khỏi ngưỡng cutoff khi phỏng đoán thất
bại và không có logit nào đang chờ được lấy mẫu. Hệ số
suy giảm này được thiết kế để tăng việc sử dụng khi chờ
đợi phần còn lại của pipeline hoàn thành.

Với hai hệ số này, phỏng đoán liên tục của PipeInfer trở
nên thích ứng với điều kiện hệ thống trong thời gian thực,
phản ứng với các chậm trễ bất ngờ một cách duyên dáng
và mở rộng cả lên các cụm cấp đại học hiệu suất cao và
xuống các cụm Beowulf cấp người tiêu dùng. Những hệ số
này cũng cho phép PipeInfer được điều chỉnh hướng tới
hiệu suất cao hơn hoặc hiệu quả năng lượng lớn hơn.

C. Pipelined KV Cache Multibuffering

Chạy nhiều phỏng đoán và một suy luận không phỏng đoán
đồng thời đòi hỏi quản lý cẩn thận KV cache, cơ chế mà
các vector liên quan đến attention được cache để ngăn ngừa
tính toán lại không cần thiết. PipeInfer sử dụng triển khai
KV cache có trong llama.cpp [12]. Metadata cache cho phép
chậm trễ gần như bằng không từ việc sao chép số lượng lớn
các ô cache từ chuỗi này sang chuỗi khác. PipeInfer sử
dụng thiết kế này để tạo nhiều phân vùng của các dải chuỗi,
mỗi dải được cấp phát động theo chính sách FIFO cho một
lần chạy suy luận cụ thể. Một hàng đợi lưu trữ các định
danh chuỗi hiện tại miễn phí, chỉ định sự bắt đầu của một
dải như vậy.

1) Phân vùng chuỗi: PipeInfer chạy các suy luận không
phỏng đoán sử dụng một định danh chuỗi được xác định
trước là zero, được gọi là chuỗi chính thức, trong khi các
suy luận phỏng đoán được cấp phát một định danh chuỗi
từ hàng đợi FIFO nói trên. Kết hợp với causal attention
mask, mỗi lần chạy suy luận được đảm bảo không tương
tác với các mục cache từ các lần chạy suy luận khác.

Các phân vùng hoạt động tương tự như back và front buffer
trong các sơ đồ double-buffering thông thường: trong khi
một lần chạy phỏng đoán đang tiến hành, phân vùng hoạt
động như back buffer, chỉ có thể đọc bởi lần chạy suy luận
được gán. Khi một lần chạy phỏng đoán được hoàn thành,
một "buffer swap" được thực hiện, trong đó các mục tương
ứng với các token được chấp nhận được sao chép đến "front
buffer", và phân vùng được đánh dấu là miễn phí để sử
dụng bởi các lần chạy khác. Front buffer trong ví dụ này
là chuỗi chính thức.

2) Chấp nhận và truyền bá chuỗi: Khi một chuỗi phỏng
đoán được chấp nhận trong quá trình xác minh, các mục
cache cho chuỗi đó được sao chép thêm đến tất cả các
chuỗi khác, đảm bảo rằng các lần chạy mới có các mục
đúng. Chỉ các mục cho đến vị trí của token được chấp
nhận cuối cùng được sao chép, đảm bảo rằng các mục
cache của các lần chạy đang tiến hành không bị thay đổi
vượt quá những gì đã được chấp nhận. Ghi đè các mục
hiện có trong các phân vùng được cấp phát sẽ gây ra các
vấn đề về tính đúng đắn nếu các mục được sửa đổi trước
khi các giá trị attention được tính toán, có thể gây ra các
điều kiện đua và đầu ra không chính xác. PipeInfer quản
lý cẩn thận thứ tự hoạt động để ngăn ngừa các điều kiện
như vậy xảy ra.

3) Giao dịch và chia sẻ mục cache sớm: Các lệnh hoạt
động cache không được phát sóng đến tất cả các node
đồng thời mà được pipelined như các tensor activation,
sử dụng cùng cơ chế giao dịch. Làm như vậy đảm bảo tính
toàn vẹn của các lần chạy đang tiến hành và cho phép
PipeInfer ngay lập tức gửi lệnh sao chép cache sau khi
bắt đầu suy luận của một lần chạy không phỏng đoán,
dẫn đến các mục cache được đảm bảo đúng được sao chép
đến tất cả các chuỗi ngay sau khi một node hoàn thành
việc đánh giá tập hợp các lớp của nó. Sao chép những
mục này cho phép các lần chạy phỏng đoán bỏ qua việc
đánh giá token đầu tiên trong một chuỗi và thay vào đó
tái sử dụng mục cache cho token đó.

D. Hủy bỏ Suy luận Sớm

Khi chạy nhiều suy luận đồng thời, có khả năng bằng cách
chấp nhận nhiều token trong xác minh token-tree, một số
lần chạy trong pipeline trở nên thừa thãi hoặc không hợp
lệ. Vô hiệu hóa xảy ra khi một lần chạy phỏng đoán trong
pipeline có các token bắt đầu không khớp với những gì đã
được chấp nhận, có nghĩa là tất cả các token trong lần chạy
được đảm bảo bị từ chối. Các lần chạy thừa thãi xảy ra khi
tất cả các token trong lần chạy đã được chấp nhận; ví dụ,
một lần chạy không phỏng đoán có thể trở nên thừa thãi
nếu một lần chạy phỏng đoán trước đó cũng đã sinh cùng
token đó.

1) Phát hiện vô hiệu hóa: PipeInfer phát hiện những tình
huống này thông qua hai phương pháp: so sánh vị trí token
bắt đầu và kết thúc tối đa của một lần chạy với vị trí kết
thúc của các token được chấp nhận hiện tại, và so sánh
chuỗi token của mỗi lần chạy với các token được chấp nhận
hiện tại. Cả hai phương pháp đều sử dụng một cấu trúc dữ
liệu chứa các token phỏng đoán và vị trí token tối đa và
tối thiểu của lần chạy. Cấu trúc dữ liệu này được tạo khi
một lần chạy được bắt đầu và được đặt trong một hàng đợi
FIFO. Trong phương pháp trước, nếu vị trí kết thúc tối đa
của một lần chạy nhỏ hơn vị trí kết thúc của các token
được chấp nhận hiện tại, thì lần chạy được đánh dấu là
thừa thãi. Trong phương pháp sau, head node lặp qua hàng
đợi FIFO sau mỗi giai đoạn sampling và so sánh các token
phỏng đoán với các token được chấp nhận hiện tại. Nếu
phần đầu của chuỗi phỏng đoán không khớp với phần cuối
của các token được chấp nhận, thì lần chạy được đánh dấu
là vô hiệu hóa.

2) Truyền ngược hủy bỏ: Khi phát hiện các lần chạy thừa
thãi hoặc vô hiệu hóa, PipeInfer truyền ngược một tín hiệu
hủy bỏ đặc biệt qua pipeline. Tín hiệu chứa chỉ một định
danh được gán duy nhất tương ứng với lần chạy nên được
hủy bỏ. Các node chưa đánh giá lần chạy bị hủy sẽ bỏ qua
hoàn toàn việc đánh giá, cải thiện hiệu suất khi pipeline
bão hòa hoặc nếu có một node chậm hơn có thể trở thành
nút thắt. Các node hiện đang đánh giá một lần chạy thăm
dó tín hiệu hủy bỏ tại các điểm đồng bộ hóa thread, cho
phép một node bỏ qua tính toán ngay cả khi nó hiện đang
xử lý lần chạy bị hủy.

Khi hoàn thành một lần chạy, cấu trúc dữ liệu chứa các
token phỏng đoán và vị trí của chúng được lấy ra khỏi
FIFO. Để duy trì tính nhất quán, các lần chạy bị hủy vẫn
chuyển các tensor activation rỗng xuống pipeline, vì vậy
thứ tự của các thông điệp được duy trì, và trạng thái nội
bộ của mỗi node được giữ nguyên. Do đó, các lần chạy
bị hủy vẫn gây ra một lượng nhỏ giao tiếp.

3) Cân nhắc hiệu suất và xung đột: Hủy bỏ Suy luận Sớm
chỉ cải thiện hiệu suất khi hủy bỏ các lần chạy phỏng đoán;
các lần chạy không phỏng đoán được đánh dấu là bị hủy
nhưng vẫn được đánh giá toàn bộ, và chỉ việc sampling
cuối cùng bị bỏ qua. Lý do đằng sau sự khác biệt trong
hành vi này là thực tế rằng Pipelined KV Cache Multibuffering
dựa vào việc các lần chạy không phỏng đoán luôn được
đánh giá toàn bộ để bỏ qua việc đánh giá token đầu tiên
trong một lần chạy phỏng đoán. Nếu các lần chạy không
phỏng đoán thực sự bị hủy giữa chừng, các lệnh sao chép
cache tiếp theo sẽ sao chép các mục không hợp lệ vào
phân vùng chuỗi của lần chạy phỏng đoán.

E. Độ chính xác Mô hình

PipeInfer tăng tốc quy trình suy luận mà không mất độ chính
xác mô hình. Tại giai đoạn sampling và xác minh token, mỗi
token phỏng đoán chỉ được chấp nhận nếu có thể xác minh
rằng việc lấy mẫu từ phân phối đầu ra của mô hình đích sẽ
tạo ra token đó. Chúng tôi sử dụng thuật toán xác minh token
từ SpecInfer [8] cho giai đoạn này.

Để thuật toán xác minh token đảm bảo cùng đầu ra mô hình
như trong chế độ suy luận ngây thơ không phỏng đoán, phân
phối xác suất đầu ra từ mô hình cũng phải giống nhau. Việc
quản lý cẩn thận KV cache của PipeInfer đảm bảo rằng mỗi
lần chạy suy luận phỏng đoán đồng thời hoàn toàn độc lập,
và việc sử dụng giao dịch đảm bảo thứ tự đúng của các hoạt
động.

Hủy bỏ suy luận sớm chỉ hủy bỏ các lần chạy được đảm
bảo không được chấp nhận, và tất cả các lần chạy khác được
phép hoàn thành. Các lần chạy không phỏng đoán luôn được
phép chạy đến hoàn thành, đảm bảo KV cache được giữ ở
trạng thái nhất quán và hợp lệ.

V. THÍ NGHIỆM

A. Thiết lập Thí nghiệm

Testbed. Chúng tôi đánh giá hiệu suất của PipeInfer bằng
cách chạy các LLM khác nhau trên các node tính toán khác
nhau với các cấu hình khác nhau. Các cấu hình cụm được
hiển thị trong Bảng II. Trên các hệ thống đa socket, NUMA
awareness được kích hoạt, và các trọng số mô hình được
phân phối giữa các node NUMA để tận dụng các kênh bộ
nhớ độc lập. 13 node không đồng nhất bao gồm năm Dell
Optiplex cũ kết hợp với 8 node Intel Xeon. Cấu hình này
được sử dụng để kiểm tra khả năng phục hồi của PipeInfer
đối với các pipeline không đồng nhất, nơi các node chậm
hơn có thể làm chậm pipeline do các nút thắt tính toán hoặc
băng thông bộ nhớ lớn hơn. Các Optiplex được cấu hình
với bộ xử lý Intel Core i5 và i7 thế hệ thứ hai và thứ tư,
tất cả năm đều sử dụng bộ nhớ DDR3 dual-channel.

Trên cụm C, các thí nghiệm được thực thi thông qua job
manager và được cấp quyền truy cập độc quyền vào tất cả
các node. Trên các cụm A và B, không có job manager, và
chỉ có các dịch vụ hệ thống thiết yếu đang chạy.

Prompt được Kiểm tra. Các prompt chúng tôi kiểm tra dài
128 token, được định dạng theo định dạng prompt dự kiến
của các mô hình. Chúng tôi sử dụng nhiều prompt để kiểm
tra các tình huống sử dụng khác nhau và để phù hợp với
trường hợp sử dụng dự kiến của mỗi mô hình:

•Prompt đầu tiên yêu cầu mô hình tạo một chương trình
Python thể hiện các tính năng nâng cao, yêu cầu nó giữ
lại bất kỳ giải thích nào, vì vậy mô hình chỉ tạo ra mã.

•Prompt thứ hai yêu cầu mô hình viết một câu chuyện
hư cấu về một chiến binh tên Goliath.

•Prompt thứ ba không sử dụng định dạng đặc biệt và là
một đoạn trích ngẫu nhiên từ bộ dữ liệu Wikitext-2 [31].

Tất cả các mô hình đều tạo 512 token đầu ra sử dụng greedy
sampling. Chúng tôi lựa chọn sử dụng greedy sampling để
duy trì các generation chính xác trên tất cả ba chiến lược
suy luận.

Cặp Mô hình. Các cặp mô hình LLM tham gia vào các thí
nghiệm của chúng tôi được hiển thị trong Bảng I, bao gồm
các họ mô hình Llama [2] và Falcon [30], cũng như một
Llama merge phổ biến có tên Goliath [28]. Goliath là một
mô hình độc đáo được tạo bằng cách ghép hai mô hình
Llama 2-70B lại với nhau, tạo ra kiến trúc mô hình cao và
mỏng so với Falcon, rộng hơn. Chúng tôi thêm Goliath vào
các trường hợp thử nghiệm để xác định liệu các kiến trúc
dài có ưu tiên một chiến lược suy luận không tỷ lệ so với
các chiến lược khác.

Baseline. Chúng tôi so sánh PipeInfer với suy luận lặp tiêu
chuẩn trong tình huống pipeline-parallel cũng như suy luận
phỏng đoán pipeline-parallel, là một triển khai của SpecInfer [8]
sử dụng một mô hình phỏng đoán duy nhất.

Metric đánh giá. Chúng tôi ghi lại bốn metric chính trong
các thí nghiệm:

1) Tốc độ sinh trung bình được đo bằng cách ghi lại tổng
thời gian wall clock giữa sự bắt đầu và hoàn thành của suy
luận, không tính đến xử lý prompt ban đầu và prefilling.

2) Độ trễ Time-to-first-token (TTFT) được đo như thời gian
CPU tiêu thụ bởi thread chính giữa việc hoàn thành giai
đoạn xử lý prompt và việc chấp nhận token đầu tiên, không
bao gồm token được lấy mẫu ở cuối giai đoạn xử lý prompt.
Chúng tôi không xem token được lấy mẫu từ giai đoạn xử
lý prompt là token đầu tiên vì không phải tất cả các tình
huống suy luận đều yêu cầu giai đoạn xử lý prompt, như
khi prompt được cache, và vì cả suy luận phỏng đoán và
PipeInfer đều không tham gia trong quá trình xử lý prompt.

3) Độ trễ Inter-token (ITL) đo thời gian trung bình giữa
mỗi token được chấp nhận. Các phép đo ITL được thực hiện
bằng cách ghi lại thời gian CPU tiêu thụ bởi thread chính
giữa mỗi token được chấp nhận và tính trung bình các ghi
lại. Cần lưu ý rằng ITL đo thời gian giữa mỗi token được
chấp nhận chứ không phải thời gian giữa bắt đầu và hoàn
thành một lần chạy.

4) Tiêu thụ bộ nhớ trên mỗi node được ghi lại thông qua
pmap [32]. Các tệp mô hình được memory-mapped, và chỉ
các trang gây ra page fault trên node được bao gồm trong
các tính toán sử dụng bộ nhớ. Trước mỗi thí nghiệm, chúng
tôi xóa file cache để đảm bảo rằng các trang được fault
vào bộ nhớ gắn với cùng socket yêu cầu dữ liệu, đảm bảo
môi trường thí nghiệm nhất quán.

Chúng tôi cũng quan sát đầu ra trực tiếp và so sánh nó với
đầu ra baseline suy luận node đơn để đánh giá tính đúng
đắn. Điều này có thể thực hiện được vì quyết định sử dụng
greedy sampling đảm bảo đầu ra xác định không có biến
đổi run-to-run trong đầu ra được tạo cho một prompt đầu
vào nhất định.

B. Kết quả Thí nghiệm

Để đánh giá, chúng tôi triển khai PipeInfer trên framework
Llama.cpp phổ biến [12]. Framework bao gồm cả triển khai
tham chiếu cho suy luận phỏng đoán và cho suy luận node
đơn tiêu chuẩn. Framework cũng bao gồm triển khai suy
luận pipeline-parallel sử dụng MPI.

Để đo sự cải thiện của PipeInfer, chúng tôi chạy suy luận
của một số mô hình trong bốn tình huống: suy luận node
đơn bình thường, suy luận pipeline parallel ngây thơ, suy
luận phỏng đoán pipeline-parallel, và suy luận PipeInfer.
Chúng tôi chạy mỗi thí nghiệm 10 lần và tính trung bình
kết quả. Trừ khi được chỉ định khác, tất cả các thí nghiệm
được chạy trên các node tính toán Intel Xeon Gold.

Phân tích tốc độ sinh. Cặp Dolphin và TinyLlama thể hiện
tỷ lệ chấp nhận khoảng 79% với kích thước cây phỏng đoán
được giới hạn ở bốn token; tốc độ sinh kết quả được hiển
thị trong Hình 4a. Chúng tôi quan sát thấy PipeInfer cải
thiện tốc độ sinh so với suy luận phỏng đoán và lặp trong
tất cả các trường hợp thử nghiệm được ghi lại. Chúng tôi
cũng quan sát thấy tốc độ sinh sử dụng suy luận phỏng
đoán và lặp về cơ bản không đổi khi số lượng node tăng.
Chúng tôi tin rằng điều này là do sự kết hợp của kết nối
cực kỳ nhanh và các tensor activation của mô hình đích
tương đối nhỏ.

Chuyển TinyLlama sang Orca 2 7B một cách kỳ lạ làm giảm
tỷ lệ chấp nhận tổng thể xuống 66%, làm giảm hiệu suất
cho suy luận lặp và phỏng đoán, như được hiển thị trong
Hình 4a. PipeInfer thay vào đó cho thấy hiệu suất tương
tự với các thử nghiệm TinyLlama, với cải thiện nhẹ trong
các trường hợp 8 và 32 node. Chúng tôi quan sát thấy trường
hợp suy luận phỏng đoán không còn thể hiện tốc độ sinh
không đổi khi số lượng node tăng; chúng tôi lý thuyết rằng
độ tin cậy của Orca2 vào các phỏng đoán của nó đủ cao
để tạo ra các cây phỏng đoán lớn hơn cho mỗi lần chạy,
trong khi không căn chỉnh đủ tốt với mô hình đích để giảm
đáng kể số lượng lần chạy cần thiết, dẫn đến tăng áp lực
băng thông kết nối.

So với các cặp Dolphin, cặp Goliath và XWin-7B tạo ra
tỷ lệ chấp nhận cực kỳ thấp là 52%, gây ra sự suy giảm
hiệu suất của suy luận phỏng đoán khi số lượng node tăng.
Hình 4b vẽ biểu đồ sự suy giảm này. Khả năng phục hồi
của PipeInfer đối với căn chỉnh thấp lại được thể hiện,
đạt được tốc độ sinh cao hơn đáng kể so với hai chiến lược
suy luận khác. Tốc độ sinh cao nhất được đạt tại tám node,
sau đó là sự suy giảm chậm về hiệu suất khi số lượng node
tăng.

Để kiểm tra liệu tỷ lệ chấp nhận có ảnh hưởng đến số lượng
node tối ưu, chúng tôi thay thế XWin-7B bằng XWin-13B,
cải thiện tỷ lệ chấp nhận lên 61%. Hình 4b tiết lộ rằng mặc
dù căn chỉnh tăng cường đã tăng tốc độ sinh, nó không thay
đổi số lượng node tối ưu, vẫn ở mức 8. Chúng tôi quan sát
thấy cấu hình 32-node đạt ngang bằng với cấu hình 8-node,
trong khi cấu hình 15-node chỉ cải thiện nhẹ so với thử
nghiệm XWin-7B, cho thấy tính phi tuyến trong việc mở
rộng của PipeInfer.

Falcon-180B, được ghép nối với Falcon-7B, có tỷ lệ chấp
nhận cao tương đối so với sự chênh lệch kích thước của
các mô hình: 68.675%. Hình 4c tiết lộ rằng, với tỷ lệ chấp
nhận đủ cao và kích thước mô hình phỏng đoán đủ thấp,
suy luận phỏng đoán tiếp cận hiệu suất của PipeInfer cho
số lượng node thấp. Tuy nhiên, khi số lượng node tăng,
hiệu suất của PipeInfer tăng vọt trong khi tốc độ sinh của
suy luận phỏng đoán giảm.

Tăng tỷ lệ chấp nhận lên 69.47% bằng cách chuyển Falcon-7B
thành Falcon-40B đảo ngược xu hướng: sự khác biệt giữa
hai chiến lược là lớn nhất ở số lượng node thấp hơn do yêu
cầu tính toán cực kỳ của mô hình phỏng đoán. Hình 4c cho
thấy xu hướng này.

Phân tích độ trễ time-to-first-token. Kiểm tra các độ trễ
time-to-first-token của các thử nghiệm trước đó tiết lộ rằng
PipeInfer đạt được gần như ngang bằng với suy luận lặp
và độ trễ thấp hơn đáng kể so với suy luận phỏng đoán.
Điều này được hiển thị trong Hình 5a, 5b và 5c. Chúng
tôi quan sát thấy việc tăng kích thước mô hình phỏng đoán
không ảnh hưởng đáng chú ý đến độ trễ TTFT. Do đó,
PipeInfer trở thành một lựa chọn xuất sắc cho các tình
huống thời gian thực hoặc hội thoại. Suy luận phỏng đoán
bị ảnh hưởng nặng nề bởi các độ trễ cao hơn do hậu quả
của việc chờ đợi các cây phỏng đoán.

Phân tích độ trễ inter-token. Hình 6a, 6b và 6c cho thấy
các độ trễ inter-token được ghi lại trong các thí nghiệm
của chúng tôi. Chúng tôi quan sát thấy các phép đo ITL
tuân theo các xu hướng được hiển thị bởi tốc độ sinh được
đo, xác minh tính đúng đắn của kết quả.

Phân tích hiệu quả bộ nhớ. Việc sử dụng bộ nhớ được ghi
lại trong mỗi thí nghiệm. Chúng tôi quan sát thấy việc tiêu
thụ bộ nhớ của PipeInfer bằng với suy luận phỏng đoán.
Việc sử dụng bộ nhớ trên mỗi node được giảm cho tất cả
các chiến lược suy luận khi số lượng node tăng. Suy luận
lặp duy trì yêu cầu bộ nhớ thấp hơn do thiếu mô hình phỏng
đoán.

So sánh việc sử dụng bộ nhớ trên mỗi node với tốc độ sinh
trong Hình 7a tiết lộ rằng, trong số ba chiến lược suy luận,
PipeInfer đạt được tỷ lệ tốc độ-với-tiêu-thụ-bộ-nhớ cao
nhất, cho thấy PipeInfer mở rộng xuống các cấu hình cụm
cấp thấp rất tốt.

Phân tích hiệu suất phần cứng bị ràng buộc. Để đo ảnh
hưởng của các ràng buộc tính toán và băng thông, chúng
tôi chạy một số thí nghiệm suy luận trên hai cụm khác,
mỗi cụm sử dụng phần cứng chậm hơn đáng kể và kết nối
Gigabit Ethernet, kết quả được hiển thị trong Hình 7c.
PipeInfer thể hiện các cải thiện tốc độ lớn nhất so với suy
luận phỏng đoán trong tình huống này, có thể do chi phí
phỏng đoán tăng và khả năng thích ứng được cung cấp bởi
hủy bỏ suy luận sớm.

Chúng tôi cũng quan sát thấy việc tăng số lượng node cải
thiện hiệu suất trong tất cả trừ hai trường hợp, ngay cả khi
các node mới chậm hơn so với các node ban đầu. Đối với
cặp Dolphin, việc thêm các node bổ sung vượt quá 8 node
Xeon E5 chỉ làm giảm hiệu suất một cách nhẹ nhàng, trong
khi trong trường hợp Falcon, hiệu suất bị giảm đáng kể.
Chúng tôi tin rằng sự suy giảm mạnh cho trường hợp Falcon
là kết quả của kích thước mô hình lớn, làm trầm trọng thêm
nút thắt tính toán trên các node chậm nhất. Thử nghiệm
Goliath cho thấy hiệu suất cải thiện với các node chậm hơn,
và chúng tôi tin rằng điều này là do tỷ lệ chấp nhận thấp
kết hợp với hủy bỏ suy luận sớm và kiến trúc cao và mỏng
độc đáo.

Một quan sát quan trọng chúng tôi đưa ra là PipeInfer có
vẻ ít bị ảnh hưởng bởi sự suy giảm hiệu suất do căn chỉnh
thấp: cải thiện của PipeInfer so với suy luận phỏng đoán
tăng cho Goliath so với Dolphin và Falcon. Ngược lại, cải
thiện của PipeInfer là nhẹ nhàng khi được triển khai trong
các pipeline nông.

Kiểm tra trên các cụm cấp thấp hơn này cũng cho thấy sự
cải thiện độ trễ cực kỳ mà phỏng đoán bất đồng bộ cung
cấp. Các độ trễ time-to-first-token được hiển thị trong Hình
7b. Trong một số trường hợp, PipeInfer đạt được độ trễ
TTFT thấp hơn so với suy luận lặp; điều này được quy cho
thực tế rằng một trong các node chỉ dành riêng cho phỏng
đoán dưới PipeInfer, làm cho pipeline đích ngắn hơn một
node so với trong trường hợp suy luận lặp.

Đầu ra và độ chính xác mô hình. Chúng tôi xác minh rằng
đầu ra của PipeInfer nhất quán với đầu ra từ suy luận phỏng
đoán tiêu chuẩn, suy luận lặp pipeline-parallel, và suy luận
node đơn. Quyết định của chúng tôi sử dụng greedy sampling
dẫn đến đầu ra xác định cho tất cả các trường hợp, và chúng
tôi xác minh rằng có sự lệch bằng không giữa đầu ra cuối
cùng của PipeInfer và đầu ra của các phương pháp khác.

Nghiên cứu Ablation. Chúng tôi thực hiện một số nghiên
cứu ablation với ba cặp mô hình khác nhau trên cấu hình
8-node của Cluster C. Kết quả của các nghiên cứu được
hiển thị trong Hình 8. Baseline của PipeInfer với tất cả
các tính năng được kích hoạt được bao gồm cho mục đích
so sánh. Loại bỏ hủy bỏ suy luận sớm dẫn đến giảm tốc
độ sinh và tăng độ trễ inter-token phù hợp với các giả
thuyết của chúng tôi. Loại bỏ phỏng đoán liên tục và tăng
kích thước batch phỏng đoán như một biện pháp cân bằng
gây ra sự suy giảm hiệu suất nghiêm trọng cho các mô hình
Dolphin và Goliath và suy giảm hiệu suất vừa phải cho
Falcon. Chúng tôi giả thuyết rằng khả năng chống chịu lớn
hơn của Falcon đối với ablation này có thể được quy cho
việc hủy bỏ suy luận sớm hủy bỏ một tỷ lệ đáng kể các
lần chạy phỏng đoán liên tục. Loại bỏ KV cache multibuffering
dẫn đến đầu ra không nhất quán, và loại bỏ phỏng đoán
bất đồng bộ tuần tự hóa tất cả các hoạt động khác, gây ra
hỏng KV cache và đầu ra không nhất quán. Do đó, chúng
tôi không bao gồm số liệu hiệu suất cho hai ablation này,
vì chúng gặp lỗi tính đúng đắn.

VI. THÍ NGHIỆM GPU

Chúng tôi đã tiến hành thí nghiệm với PipeInfer sử dụng
tính toán kết hợp GPU và CPU. Tuy nhiên, triển khai GPU
của chúng tôi dựa trên commit llama.cpp sau này, sau khi
tái cấu trúc backend đáng kể, và vì vậy những kết quả này
không thể so sánh trực tiếp với kết quả chỉ CPU trước đó
của chúng tôi. Ngoài ra, triển khai MPI GPU chưa được
tối ưu hóa đầy đủ và chúng tôi mong đợi cải thiện liên tục
trên toàn bộ trong tương lai.

A. Thiết lập Thí nghiệm

Testbed. Đánh giá GPU của chúng tôi sử dụng nhiều loại
phần cứng để kiểm tra hiệu quả của phương pháp trên nhiều
nhà cung cấp GPU và API. Cấu hình testbench được hiển
thị trong Bảng IV.

Chúng tôi cũng kiểm tra với nhiều loại họ mô hình, được
hiển thị trong Bảng III.

B. Kết quả Thí nghiệm

Như được hiển thị trong Hình 9, chúng tôi quan sát các
mẫu tương tự trong kết quả GPU của chúng tôi với các
thí nghiệm trước đó. Tốc độ sinh tổng thể của PipeInfer
lớn hơn suy luận phỏng đoán tiêu chuẩn trong tất cả trừ
một trường hợp. Tuy nhiên, chúng tôi quan sát một số outlier
đáng kể, đặc biệt trong thí nghiệm liên quan đến cặp mô
hình Dolphin 2.9 70B và 8B. Cặp này là cặp duy nhất dựa
trên Llama 3 mà chúng tôi kiểm tra, do đó chúng tôi không
thể kết luận liệu outlier này cụ thể cho Dolphin 2.9 hay
liệu nó vốn có trong các mô hình Llama 3 nói chung.

Chúng tôi cũng bao gồm một tập hợp thí nghiệm bổ sung
tập trung vào biến động prompt-to-prompt, được hiển thị
trong Hình 10. Trong những thí nghiệm này, chúng tôi quan
sát thấy tốc độ sinh tổng thể của PipeInfer vẫn tương đối
nhất quán trên phạm vi các prompt được kiểm tra, trong
khi suy luận phỏng đoán thấy các thay đổi tốc độ bất thường
hơn.

VII. KẾT LUẬN VÀ TRIỂN VỌNG

PipeInfer tăng cường hiệu quả và tốc độ xử lý của LLM
bằng cách sửa đổi thuật toán suy luận phỏng đoán để hỗ
trợ nhiều lần chạy xác minh trong kiến trúc pipeline-parallel.
Chúng tôi đạt được không chỉ việc sử dụng hệ thống được
tối ưu hóa và chi phí giao tiếp tối thiểu mà còn thể hiện
khả năng phục hồi đặc biệt đối với độ trễ và chậm trễ tính
toán, đặc biệt trong môi trường phần cứng không đồng nhất
hiệu quả về chi phí. Kết quả cho thấy sự cải thiện đáng
kể 2.15 × trong tốc độ sinh, duy trì hiệu suất cao ngay cả
với độ chính xác phỏng đoán thấp. Ngoài ra, cách tiếp cận
của chúng tôi thể hiện tính mạnh mẽ trong các môi trường
bị ràng buộc về độ trễ và thông lượng, đạt được việc sử
dụng CPU và GPU cao.

PipeInfer thể hiện hiệu suất cải thiện đáng kể trong nhiều
tình huống, và công việc tương lai có thể mở rộng nó cho
các chiến lược tăng tốc suy luận khác, cải thiện thêm hiệu
suất của nó. Chúng tôi tin rằng Lookahead decoding [11]
và Medusa [10] sẽ được hưởng lợi rất nhiều từ sự tăng
cường PipeInfer. Chúng tôi cũng tin rằng các kỹ thuật tự
phỏng đoán như SPEED [13] hoặc PPD [14] có thể bổ sung
cho cách tiếp cận mô hình phỏng đoán bên ngoài của PipeInfer.

PipeInfer cũng có thể được mở rộng để hỗ trợ song song
hóa lai thông qua các node đa GPU, áp dụng tensor parallelism
ở cấp node địa phương và duy trì pipeline parallelism trên
toàn cụm. Thay vào đó, các nút thắt băng thông do từ bus
PCIe có thể được giảm thiểu thông qua việc áp dụng PipeInfer
trên một node duy nhất.

Các nút thắt trong pipeline cũng có thể được giảm thiểu
bằng cách thêm các node mới song song với các node chậm
nhất, hoạt động như load-balancer. Khi node chính bận,
các node phụ có thể đảm nhận việc tính toán của các lớp
được chỉ định. Việc sử dụng nhiều lần chạy suy luận độc
lập và đồng thời của PipeInfer cho phép cân bằng tải như
vậy mà không ảnh hưởng đến kết quả cuối cùng, miễn là
thứ tự vẫn nhất quán.

TÀI LIỆU THAM KHẢO

[1] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-
V oss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler,
J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,
B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,
and D. Amodei, "Language models are few-shot learners," 2020.

[2] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
T. Lacroix, B. Rozi `ere, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez,
A. Joulin, E. Grave, and G. Lample, "Llama: Open and efficient
foundation language models," 2023.

[3] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. Kaiser, and I. Polosukhin, "Attention is all you need," 2023.

[4] D. A. Patterson, "Latency lags bandwith," Commun. ACM , vol. 47,
p. 71–75, oct 2004.

[5] N. Shazeer, "Fast transformer decoding: One write-head is all you need,"
2019.

[6] S. Markidis, S. W. D. Chien, E. Laure, I. B. Peng, and J. S. Vetter,
"Nvidia tensor core programmability, performance & precision," in 2018
IEEE International Parallel and Distributed Processing Symposium
Workshops (IPDPSW) , IEEE, May 2018.

[7] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bam-
ford, D. S. Chaplot, D. de las Casas, E. B. Hanna, F. Bressand,
G. Lengyel, G. Bour, G. Lample, L. R. Lavaud, L. Saulnier, M.-A.
Lachaux, P. Stock, S. Subramanian, S. Yang, S. Antoniak, T. L. Scao,
T. Gervet, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed, "Mixtral of
experts," 2024.

[8] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, Z. Wang, R. Y . Y . Wong,
A. Zhu, L. Yang, X. Shi, C. Shi, Z. Chen, D. Arfeen, R. Abhyankar,
and Z. Jia, "Specinfer: Accelerating generative large language model
serving with speculative inference and token tree verification," 2023.

[9] B. Spector and C. Re, "Accelerating llm inference with staged specula-
tive decoding," 2023.

[10] T. Cai, Y . Li, Z. Geng, H. Peng, and T. Dao, "Medusa: Simple framework
for accelerating llm generation with multiple decoding heads." https:
//github.com/FasterDecoding/Medusa, 2023.

[11] Y . Fu, P. Bailis, I. Stoica, and H. Zhang, "Breaking the sequential
dependency of llm inference using lookahead decoding," November
2023.

[12] G. Gerganov, "ggerganov/llama.cpp: Port of facebook's llama model in
c/c++." https://github.com/ggerganov/llama.cpp, 2023.

[13] C. Hooper, S. Kim, H. Mohammadzadeh, H. Genc, K. Keutzer, A. Gho-
lami, and S. Shao, "Speed: Speculative pipelined execution for efficient
decoding," 2024.

[14] S. Yang, G. Lee, J. Cho, D. Papailiopoulos, and K. Lee, "Predictive
pipelined decoding: A compute-latency trade-off for exact llm decod-
ing," 2023.

[15] B. Tadych, "Distributed llama - distributed inference of large language
models with slow synchronization over ethernet." https://github.com/
b4rtaz/distributed-llama/blob/main/report/report.pdf, 2024.

[16] T. Schuster, A. Fisch, J. Gupta, M. Dehghani, D. Bahri, V . Q. Tran,
Y . Tay, and D. Metzler, "Confident adaptive language modeling," 2022.

[17] M. Elbayad, J. Gu, E. Grave, and M. Auli, "Depth-adaptive transformer,"
2020.

[18] N. Varshney, A. Chatterjee, M. Parmar, and C. Baral, "Accelerating llama
inference by enabling intermediate layer decoding via instruction tuning
with lite," 2023.

[19] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, C. Gan, and S. Han,
"Awq: Activation-aware weight quantization for llm compression and
acceleration," 2023.

[20] J. Chee, Y . Cai, V . Kuleshov, and C. M. De Sa, "Quip: 2-bit quantization
of large language models with guarantees," in Advances in Neural
Information Processing Systems (A. Oh, T. Neumann, A. Globerson,
K. Saenko, M. Hardt, and S. Levine, eds.), vol. 36, pp. 4396–4429,
Curran Associates, Inc., 2023.

[21] E. Frantar and D. Alistarh, "Sparsegpt: Massive language models can
be accurately pruned in one-shot," 2023.

[22] M. Kurtz, J. Kopinsky, R. Gelashvili, A. Matveev, J. Carr, M. Goin,
W. Leiserson, S. Moore, B. Nell, N. Shavit, and D. Alistarh, "Inducing
and exploiting activation sparsity for fast inference on deep neural
networks," in Proceedings of the 37th International Conference on
Machine Learning (H. D. III and A. Singh, eds.), vol. 119 of Proceedings
of Machine Learning Research , (Virtual), pp. 5533–5543, PMLR, 13–18
Jul 2020.

[23] S. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li, S. Shen, M. W.
Mahoney, and K. Keutzer, "Squeezellm: Dense-and-sparse quantization,"
2024.

[24] Message Passing Interface Forum, MPI: A Message-Passing Interface
Standard Version 4.1 , Nov. 2023.

[25] E. Hartford, "dolphin-2.1-70b." https://huggingface.co/
cognitivecomputations/dolphin-2.1-70b, 2023.

[26] J. Zhao, "Tinyllama-1.1b-1t-openorca." https://huggingface.co/
jeff31415/TinyLlama-1.1B-1T-OpenOrca, 2023.

[27] A. Mitra, L. D. Corro, S. Mahajan, A. Codas, C. Simoes, S. Agar-
wal, X. Chen, A. Razdaibiedina, E. Jones, K. Aggarwal, H. Palangi,
G. Zheng, C. Rosset, H. Khanpour, and A. Awadallah, "Orca 2: Teaching
small language models how to reason," 2023.

[28] A. Dale, "Goliath 120b." https://huggingface.co/alpindale/goliath-120b,
2023.

[29] X.-L. Team, "Xwin-lm," 9 2023.

[30] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru,
M. Debbah, ´Etienne Goffinet, D. Hesslow, J. Launay, Q. Malartic,
D. Mazzotta, B. Noune, B. Pannier, and G. Penedo, "The falcon series
of open language models," 2023.

[31] S. Merity, C. Xiong, J. Bradbury, and R. Socher, "Pointer sentinel
mixture models," 2016.

[32] A. Cahalan, pmap(1) Linux User's Manual , 2002.

[33] S. Research, "Senku-70b." https://huggingface.co/ShinojiResearch/
Senku-70B, 2024.

[34] J. Bai, S. Bai, Y . Chu, Z. Cui, K. Dang, X. Deng, Y . Fan, W. Ge,
Y . Han, F. Huang, B. Hui, L. Ji, M. Li, J. Lin, R. Lin, D. Liu, G. Liu,
C. Lu, K. Lu, J. Ma, R. Men, X. Ren, X. Ren, C. Tan, S. Tan, J. Tu,
P. Wang, S. Wang, W. Wang, S. Wu, B. Xu, J. Xu, A. Yang, H. Yang,
J. Yang, S. Yang, Y . Yao, B. Yu, H. Yuan, Z. Yuan, J. Zhang, X. Zhang,
Y . Zhang, Z. Zhang, C. Zhou, J. Zhou, X. Zhou, and T. Zhu, "Qwen
technical report," 2023.

[35] . AI, :, A. Young, B. Chen, C. Li, C. Huang, G. Zhang, G. Zhang,
H. Li, J. Zhu, J. Chen, J. Chang, K. Yu, P. Liu, Q. Liu, S. Yue, S. Yang,
S. Yang, T. Yu, W. Xie, W. Huang, X. Hu, X. Ren, X. Niu, P. Nie,
Y . Xu, Y . Liu, Y . Wang, Y . Cai, Z. Gu, Z. Liu, and Z. Dai, "Yi: Open
foundation models by 01.ai," 2024.
