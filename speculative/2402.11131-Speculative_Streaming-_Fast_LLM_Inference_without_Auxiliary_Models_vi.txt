# Speculative Streaming: Suy đoán LLM nhanh chóng mà không cần mô hình phụ trợ

Nikhil Bhendawade1Irina Belousova1Qichen Fu1Henry Mason1Mohammad Rastegari1Mahyar Najibi1

## Tóm tắt

Giải mã suy đoán là một kỹ thuật nổi bật để tăng tốc suy luận của một mô hình ngôn ngữ mục tiêu lớn dựa trên dự đoán của một mô hình bản thảo phụ trợ. Mặc dù hiệu quả, trong các môi trường ứng dụng cụ thể, nó thường liên quan đến việc tinh chỉnh cả mô hình bản thảo và mô hình mục tiêu để đạt được tỷ lệ chấp nhận cao. Khi số lượng tác vụ hạ nguồn tăng lên, những mô hình bản thảo này thêm độ phức tạp đáng kể vào hệ thống suy luận. Chúng tôi đề xuất Speculative Streaming, một phương pháp giải mã suy đoán mô hình đơn lẻ kết hợp việc soạn thảo vào mô hình mục tiêu bằng cách thay đổi mục tiêu tinh chỉnh từ dự đoán token tiếp theo sang dự đoán n-gram tương lai. Speculative Streaming tăng tốc giải mã từ 1.8 - 3.1X trong một tập hợp đa dạng các tác vụ, như Tóm tắt, Truy vấn có cấu trúc và Biểu diễn ý nghĩa, mà không hy sinh chất lượng sinh ra. Ngoài ra, Speculative Streaming có hiệu quả tham số. Nó đạt được tốc độ tăng tốc ngang bằng/cao hơn so với kiến trúc kiểu Medusa trong khi sử dụng ít hơn khoảng 10000X tham số bổ sung, làm cho nó phù hợp với các thiết bị hạn chế tài nguyên.

## 1. Giới thiệu

Các transformer lớn là công cụ hàng đầu ngày nay cho mô hình hóa ngôn ngữ. Chất lượng của những mô hình này cải thiện khi chúng mở rộng quy mô (Kaplan et al., 2020), dẫn đến việc giới thiệu các mô hình hàng tỷ tham số tiên tiến (Brown et al., 2020; Thoppilan et al., 2022; Chowdhery et al., 2023; Touvron et al., 2023). Mặc dù những mô hình này rất hiệu quả cho việc sinh token, chúng phát sinh chi phí suy luận cao vì mô hình và các trạng thái tạm thời của nó cần được tải vào bộ nhớ tính toán cho mỗi token được sinh ra tiếp theo. Hơn nữa, việc mở rộng quy mô các mô hình này, bên cạnh việc làm cho mỗi lần gọi tốn nhiều tính toán hơn, cũng làm cho việc sinh tự hồi quy của chúng bị giới hạn bởi bộ nhớ (Pope et al., 2023), ngăn chúng sử dụng hiệu quả khả năng tính toán có sẵn.

Điều này đặt ra thách thức đáng kể cho việc triển khai các transformer tự hồi quy lớn, đặc biệt là cho các ứng dụng hướng người dùng với yêu cầu độ trễ nghiêm ngặt.

Với bản chất bị giới hạn bộ nhớ của suy luận mô hình ngôn ngữ lớn (LLM), công trình gần đây (Leviathan et al., 2023; Chen et al., 2023) đã đề xuất Giải mã suy đoán như một kỹ thuật hiệu quả để tăng tốc giải mã dựa trên các khái niệm được mượn từ tính toán suy đoán (Burton, 1985) để khai thác khả năng tính toán bổ sung có sẵn. Cốt lõi của giải mã suy đoán là ý tưởng suy đoán nhiều token ứng viên tương lai trước, sau đó xác minh tất cả chúng song song. Để đạt được điều này, như được hiển thị trong Hình 1(a), một phương pháp mô hình hai giai đoạn được sử dụng: một mô hình "bản thảo" phụ trợ nhỏ để suy đoán ứng viên và một mô hình "mục tiêu" lớn để xác minh (Leviathan et al., 2023; Chen et al., 2023). Mặc dù hiệu quả trong việc tăng tốc LLM, giải mã suy đoán làm phức tạp việc triển khai. Đào tạo cũng trở nên đòi hỏi và phức tạp hơn, vì một mô hình bản thảo riêng biệt cần được đào tạo và điều chỉnh với mô hình mục tiêu. Nó cũng không hiệu quả về tài nguyên, yêu cầu lưu trữ hai mô hình trong bộ nhớ trong quá trình dự đoán token. Dung lượng tăng này đặc biệt không thỏa mãn cho các thiết bị hạn chế tài nguyên.

Trong bài báo này, chúng tôi đề xuất Speculative Streaming, một

phương pháp giải mã suy đoán mô hình đơn lẻ thống nhất suy đoán và xác minh, loại bỏ nhu cầu cho một mô hình bản thảo riêng biệt như được hiển thị trong Hình 1(b). Điều này được thực hiện bằng cách kết hợp chú ý đa luồng vào mô hình mục tiêu để thực hiện dự đoán n-gram phục vụ như suy đoán ứng viên tương lai. Kết quả là, một lần chạy mô hình chuyển tiếp có thể xác minh các token được sinh ra trước đó trong khi đồng thời suy đoán về các token tương lai. Hơn nữa, so với các phương pháp trước đó, Speculative Streaming được đào tạo đầu cuối, tự nhiên điều chỉnh các giai đoạn suy đoán và xác minh.

Trong khi làm cho hệ thống đơn giản hơn đáng kể và hiệu quả tài nguyên, Speculative Streaming đạt được tốc độ tăng tốc tương đương với giải mã suy đoán hai giai đoạn (Leviathan et al., 2023) mà không làm giảm các chỉ số chất lượng trên một tập hợp đa dạng các tác vụ hạ nguồn. Nó cũng dẫn đến tốc độ tăng tốc ngang bằng hoặc tốt hơn so với mô hình giải mã theo khối gần đây hơn, Medusa (Cai et al., 2023), mà giới thiệu nhiều đầu dự đoán chiều cao bổ sung. Hơn nữa, Speculative Streaming yêu cầu ít hơn 10000X tham số bổ sung so với Medusa (Cai et al., 2023), điều này làm cho nó trở thành phương pháp lý tưởng cho các thiết bị hạn chế tài nguyên.

Tóm lại, các ưu điểm của Speculative Streaming như sau:

– Đạt được tốc độ tăng tốc đáng kể thông qua tinh chỉnh được sắp xếp hợp lý và loại bỏ nhu cầu cho một mô hình bản thảo riêng biệt.

– Thể hiện hiệu quả tài nguyên với ít hơn 10000X tham số bổ sung so với (Cai et al., 2023) trong khi đạt được tốc độ tăng tốc tốt hơn, tất cả mà không làm giảm chất lượng trên một loạt các tác vụ đa dạng.

– Đơn giản hóa quy trình triển khai bằng cách loại bỏ nhu cầu quản lý, điều chỉnh và chuyển đổi giữa hai mô hình trong quá trình thực thi, như được quan sát trong (Leviathan et al., 2023).

## 2. Các công trình liên quan

Suy luận của các mô hình ngôn ngữ lớn thường bị giới hạn bởi bản chất tuần tự của giải mã tự hồi quy, nơi mỗi việc sinh token yêu cầu một lần chuyển tiếp mạng hoàn chỉnh. Một số phương pháp đã được đề xuất để giải quyết độ trễ suy luận cao của các mô hình ngôn ngữ lớn bằng cách trực tiếp giảm dung lượng bộ nhớ của LLM. Lượng tử hóa mô hình (Frantar et al., 2022; Yao et al., 2022; Dettmers et al., 2023), chưng cất kiến thức thành một mô hình nhỏ hơn (Gu et al., 2023; Agarwal et al., 2023), và cắt tỉa (Frantar & Alistarh, 2023; Sun et al., 2023a) là trong số những kỹ thuật này.

Gần đây, giải mã suy đoán (SD) đã nổi lên như một kỹ thuật quan trọng để tăng tốc giải mã tự hồi quy.

Phương pháp giải mã suy đoán ban đầu (Chen et al., 2023; Leviathan et al., 2023) sử dụng một LLM nhỏ hơn (a.k.a. mô hình bản thảo), để sinh ra một chuỗi ứng viên các token để được xác minh bởi mô hình mục tiêu. Với một mô hình bản thảo được điều chỉnh tốt, kỹ thuật này có thể đạt được tốc độ tăng tốc suy luận 2-3x. Các biến thể SD gần đây đề xuất tính toán song song dọc theo trục lô (Sun et al., 2023b), và các lô có cấu trúc cây (Miao et al., 2023; Spector & Re, 2023) để cải thiện tỷ lệ chấp nhận của các token được đoán bởi mô hình mục tiêu và để tăng cường hiệu suất hơn nữa. Tuy nhiên, những phương pháp này gặp phải một hạn chế chung: sự cần thiết phải phát triển một mô hình bản thảo chính xác và độc lập. Đầu tiên, việc đào tạo một mô hình bản thảo như vậy được điều chỉnh với mô hình chính không phải là đơn giản (Zhou et al., 2023). Thứ hai, việc lưu trữ hai mô hình khác nhau làm tăng độ phức tạp hệ thống, và tốn kém hơn về mặt tính toán và vận hành để duy trì.

Rất gần đây, suy đoán mô hình đơn lẻ cũng đã được xem xét. Đặc biệt, lấy cảm hứng từ (Qi et al., 2020; Stern et al., 2018), Medusa (Cai et al., 2023) mở rộng mô hình chính để dự đoán các token tương lai song song bằng cách đào tạo nhiều đầu đầu ra. Mặc dù nó không yêu cầu một mô hình bản thảo, mỗi đầu Medusa có kích thước (kích thước ẩn × kích thước từ vựng) yêu cầu các tham số bổ sung không thể thương lượng. Vì việc sinh tự hồi quy thường theo một mẫu tính toán bị giới hạn bộ nhớ, số lượng đáng kể các tham số bổ sung gây ra thách thức triển khai trên các thiết bị hạn chế tài nguyên. Thay vào đó, giải mã nhìn trước (Fu et al., 2023) đề xuất một phương pháp giải mã song song mà không học các tham số mới. Nó sử dụng Giải mã Jacobi và bộ nhớ đệm quỹ đạo lịch sử n-gram để sinh ra và xác minh dự đoán n-gram tương lai. Khác biệt, Speculative Streaming đạt được việc sinh n-gram bằng cách học một tập hợp các nhúng token và tăng tốc giải mã bằng cách chạy suy đoán và xác minh đồng thời. Đáng chú ý, phương pháp của chúng tôi có thể đạt được tốc độ tăng tốc tốt hơn với số lượng tham số bổ sung ít hơn đáng kể so với các phương pháp giải mã suy đoán hiện có (Chen et al., 2023; Leviathan et al., 2023; Cai et al., 2023), trong khi đơn giản hóa việc triển khai trên thiết bị.

## 3. Phương pháp

### 3.1. Động lực

Hầu hết các phương pháp giải mã suy đoán thể hiện sự phân tách rõ ràng trong các quy trình đào tạo của mô hình bản thảo và mô hình mục tiêu. Tuy nhiên, việc trực tiếp sử dụng một mô hình bản thảo có sẵn thường dẫn đến hiệu suất dưới tối ưu trong nhiều ứng dụng hạ nguồn. Các token được suy đoán thường xuyên thất bại trong việc xác minh của mô hình mục tiêu khi các mô hình bản thảo và mục tiêu không được điều chỉnh. Để đạt được tốc độ tăng tốc cải thiện, việc tinh chỉnh cả mô hình bản thảo và mô hình mục tiêu trên các ứng dụng hạ nguồn trở nên cần thiết. Mục tiêu của chúng tôi là thiết kế một khung mô hình đơn lẻ có thể đào tạo đầu cuối có khả năng đồng thời dự đoán token tiếp theo và suy đoán các token tương lai. Điều này loại bỏ nhu cầu cho một mô hình bản thảo phụ trợ trong khi đạt được tốc độ tăng tốc tương tự như những gì được báo cáo trong (Leviathan et al., 2023). Chúng tôi nhằm đạt được tốc độ tăng tốc này bằng cách tăng cường độ số học của các lần gọi transformer tự hồi quy mà không làm giảm chất lượng sinh ra.

### 3.2. Speculative Streaming

Chúng tôi đề xuất Speculative Streaming để cho phép tinh chỉnh và suy luận suy đoán hiệu quả tham số của các mô hình chỉ giải mã trên các ứng dụng hạ nguồn. So với giải mã suy đoán bản thảo-mục tiêu tiêu chuẩn (Leviathan et al., 2023) và giải mã theo khối được đề xuất gần đây hơn (Cai et al., 2023), Speculative Streaming giới thiệu các sửa đổi sau. (a) Thiết kế và khởi tạo luồng suy đoán như được mô tả trong Phần 3.2.1 (c) Suy đoán và xác minh song song như được mô tả trong Phần 3.2.2 (d) Cắt tỉa bản thảo cây song song, được mô tả trong Phần 3.2.3 và cuối cùng (e) Mục tiêu đào tạo như được mô tả trong Phần 3.2.4.

#### 3.2.1. THIẾT KẾ VÀ KHỞI TẠO LUỒNG

Tinh chỉnh hiệu quả tham số (Hu et al., 2022) của các mô hình ngôn ngữ tiền đào tạo chỉ giải mã liên quan đến việc đào tạo các bộ điều chỉnh thứ hạng thấp để dự đoán token mục tiêu tiếp theo yt cho các token ngữ cảnh (x1....xm) và các token mục tiêu trước đó (y1..y<t) trên các ứng dụng hạ nguồn. Để nhúng một khái niệm về quy hoạch token tương lai một cách vốn có, chúng tôi sửa đổi mục tiêu đào tạo của mô hình mục tiêu từ dự đoán token tiếp theo sang dự đoán n-gram bằng cách sử dụng khái niệm chú ý đa luồng (Qi et al., 2020; Yang et al., 2019). Mục tiêu này cho phép mô hình lập kế hoạch cho các token tương lai và ngăn chặn việc khớp quá mức trên các tương quan cục bộ. Ngoài ra, mỗi luồng trong số γ luồng sinh ra các token suy đoán với chi phí độ trễ không đáng kể khi mô hình bị giới hạn bộ nhớ. Cụ thể, mỗi luồng được thêm dự đoán p(yt+j|y<t, x), trong đó 1<=j<=γ, trong khi luồng chính dự đoán p(yt|y<t, x). Chúng tôi gọi cơ chế chú ý đa đầu được mô tả trong (Vaswani et al., 2017) là tự chú ý luồng chính và giới thiệu γ luồng tự chú ý bổ sung để suy đoán các token tương lai.

Cơ chế chú ý của luồng chính giống như chú ý đa đầu tiêu chuẩn (Vaswani et al., 2017).

Mk+1t = MHA(Mkt, Mk≤t, Mk≤t)                    (1)

Trong đó Mkt biểu thị các trạng thái ẩn của luồng chính tại lớp k và bước thời gian t và MHA(H, H, H) biểu thị chú ý giữa truy vấn HWQ, khóa HWK và giá trị HWV như được mô tả trong (Vaswani et al., 2017). Mặt khác, mỗi luồng suy đoán j tại bước thời gian t chú ý đến các trạng thái ẩn luồng chính trước đó cũng như các trạng thái ẩn luồng suy đoán như:

Sk+1tj = MHA(Sktj, Mk≤t⊕Skt(≤j), Mk≤t⊕Skt(≤j))     (2)

trong đó Mk+1t và Sk+1t tham chiếu đến các luồng chính và suy đoán tại bước thời gian t và lớp k. Trạng thái ẩn của lớp transformer cuối N, MNt được sử dụng để dự đoán yt, trong khi mỗi luồng suy đoán tại lớp cuối, SNtj dự đoán yt+j. Chúng tôi gọi các lớp kết hợp cơ chế chú ý trong Phương trình (1) là các lớp MHA trong khi các lớp kết hợp chú ý luồng suy đoán Phương trình (2) được gọi là các lớp MSA.

Các phép chiếu khóa/giá trị của các trạng thái ẩn luồng chính được lưu trữ trong bộ nhớ đệm trong quá trình suy luận để tránh tính toán lại, trong khi chú ý luồng suy đoán được thiết kế đặc biệt để tránh lưu trữ các phép chiếu khóa/giá trị bổ sung liên quan đến các luồng riêng lẻ. Điều này là vì các luồng suy đoán được đào tạo để học các đặc trưng ngữ cảnh từ ngữ cảnh khóa/giá trị luồng chính cho phép chúng tôi không giới thiệu chi phí lưu trữ bổ sung và hoạt động trong giới hạn bộ nhớ của các thiết bị hạn chế tài nguyên trong quá trình suy luận. Chúng tôi khởi tạo các trạng thái ẩn của các luồng suy đoán tại lớp N-Ns thay vì khởi tạo chúng từ lớp nhúng, trong đó Ns < N. Cụ thể, luồng j tại thời gian t được khởi tạo tại lớp N-Ns như:

SN-Nstj = fη(MN-Nst) + PN-Nsj                    (3)

trong đó Pj là một nhúng định danh luồng nhúng một cảm giác về vị trí tương đối vào các luồng và phân biệt tính toán từ luồng chính. fη là một phép biến đổi tuyến tính của thứ hạng η để biến đổi các trạng thái ẩn luồng chính thành các trạng thái ẩn luồng suy đoán. Việc khởi tạo này giúp giảm tính toán cho mỗi lần chuyển tiếp, vì chỉ luồng chính cần được truyền qua N-Ns lớp, trong khi các luồng suy đoán được truyền qua Ns lớp cuối, giảm đóng góp FLOP suy đoán bằng (N-Ns)/N và do đó giúp với mức tiêu thụ điện năng cao nhất trên thiết bị. Về độ trễ lần chuyển tiếp, FLOP không đóng góp đáng kể khi mô hình bị giới hạn bộ nhớ, tuy nhiên, như chúng tôi mô tả trong Phần 3.2.2, chúng tôi lấy mẫu các token bổ sung để làm cho mô hình bị giới hạn tính toán, do đó việc giảm FLOP trở nên quan trọng. Việc khởi tạo với trạng thái ẩn của các lớp transformer trung bình-trên cũng có thể giúp với dự đoán token tương lai vì M(N-Ns) chính nó chứa các đặc trưng ngữ cảnh cấp cao để hỗ trợ dự đoán các n-gram tương lai (Pal et al., 2023). Chúng tôi cũng thử nghiệm với thiết kế luồng dựa trên xoay giá trị không yêu cầu nhúng định danh và không phát sinh chi phí tham số như được mô tả trong B.

#### 3.2.2. SUY ĐOÁN VÀ XÁC MINH SONG SONG

Trong giải mã suy đoán bản thảo-mục tiêu tiêu chuẩn (Leviathan et al., 2023), các quy trình suy đoán và xác minh xảy ra tuần tự. Mô hình bản thảo chờ mô hình mục tiêu đưa ra một sự điều chỉnh trước khi sinh ra bản thảo tiếp theo. Mô hình mục tiêu cũng cần chờ bản thảo suy đoán được sinh ra. Speculative Streaming làm cho quy trình này hiệu quả hơn bằng cách song song hóa suy đoán và xác minh. Trong mỗi lần chuyển tiếp, bản thảo được sinh ra trong bước trước được xác minh và một bản thảo mới được sinh ra như được hiển thị trong Hình 2. Ví dụ, trong bước s, nếu các token bản thảo (ỹ1..ỹδ) được chấp nhận trong đó 0 < δ ≤ γ, luồng chính Mδ được sử dụng để đưa ra một token điều chỉnh và logit từ các luồng suy đoán Sδ(1...γ) được sử dụng để sinh bản thảo cho bước s+1.

Thay vì sử dụng một chuỗi tuyến tính các token được suy đoán để xác minh, chúng tôi lấy mẫu một cây các token từ các luồng chính và suy đoán, sao cho mỗi đường dẫn trong cây là một ứng viên xác minh có thể. Bản thảo cây cho phép chấp nhận chuỗi ứng viên khớp dài nhất và nhiều token hơn có thể được tiến lên trong mỗi lần chuyển tiếp. Để tạo ra một bản thảo cây, thay vì lấy mẫu 1 token từ logit của các luồng suy đoán, (z1...zγ), chúng tôi lấy mẫu k token hàng đầu và tạo thành một cây các token được lấy mẫu như được hiển thị trong Hình 2, sao cho các token được lấy mẫu từ luồng n là tiền thân của các token được lấy mẫu từ luồng n+1. Chúng tôi xử lý một bản thảo cây các token suy đoán trong một lần chuyển tiếp bằng cách tạo ra một mặt nạ chú ý cộng (Vaswani et al., 2017) sao cho mỗi nút trong cây chú ý đến tiền thân của nó. Mặt nạ chú ý giữa token thứ k được lấy mẫu từ logit của luồng j, ỹjk và token thứ m được lấy mẫu từ logit của luồng n, ỹnm là

aỹjkỹnm = {0 nếu j = n+1, -∞ ngược lại}         (4)

Vui lòng tham khảo Hình 8 để biết thêm chi tiết. Điều đáng chú ý là đối với γ và k cố định, mặt nạ chú ý vẫn không đổi trong mỗi lần chuyển tiếp và cho phép ghép lô hiệu quả.

#### 3.2.3. CẮT TỈA CÂY SONG SONG

Một trong những vấn đề với việc tạo ra bản thảo cây suy đoán ngây thơ là mọi hoán vị giữa k token được lấy mẫu từ mỗi luồng cần được xem xét như một ứng viên suy đoán khả thi cho lần xác minh tiếp theo. Ví dụ, lấy mẫu k token từ mỗi luồng trong số γ luồng dẫn đến bản thảo cây có kích thước 1 + ∑γg=1 kg. Hơn nữa, mỗi token bản thảo được ghép lô với γ luồng suy đoán trong các lớp MSA để đảm bảo rằng việc sinh ra bản thảo tiếp theo xảy ra trong cùng một lần chuyển tiếp, dẫn đến kích thước lô là (1+γ)*(1+∑γg=1 kg). Khi kích thước lô tăng, suy luận mô hình mục tiêu trở nên bị giới hạn tính toán, loại bỏ lợi ích độ trễ của việc lấy mẫu nhiều token hơn. Chúng tôi giảm thiểu vấn đề này bằng cách giới thiệu một lớp cắt tỉa bản thảo cây song song, cắt tỉa một số token từ bản thảo cây đầu vào dựa trên xác suất chuyển đổi giữa các token cha và con trực tiếp. Để có được xác suất chuyển đổi mà không sử dụng các mô hình proxy, chúng tôi sử dụng một kỹ thuật dựa trên thoát sớm. Cụ thể, các trạng thái ẩn của luồng chính tại lớp l, Ml được truyền qua một phép biến đổi tuyến tính thứ hạng thấp oθ, trong đó thứ hạng θ thường được đặt thành một giá trị nhỏ như 8 để giữ chi phí tham số nhỏ. Chúng tôi sử dụng đầu mô hình hóa ngôn ngữ ban đầu, H để có được logit thoát sớm, z̃ = H(oθ(Ml). z̃pc được sử dụng để xấp xỉ xác suất chuyển đổi giữa token cha p và token con c. Lớp cắt tỉa có thể được chèn vào bất kỳ điểm nào trong mạng, được hướng dẫn bởi sự đánh đổi giữa độ trễ lần chuyển tiếp và độ chính xác cắt tỉa. Việc chèn sớm giảm độ trễ nhưng có nguy cơ cắt tỉa các token có giá trị tiềm năng. Ngược lại, việc chèn muộn giữ lại nhiều token "tốt" hơn nhưng với chi phí là độ trễ lần chuyển tiếp tăng. Trong tất cả các thí nghiệm được mô tả trong Phần 4.1, chúng tôi chèn lớp cắt tỉa ngay trước khi chèn luồng suy đoán một cách thực nghiệm. Thêm chi tiết có thể được tìm thấy trong Hình 7.

#### 3.2.4. MỤC TIÊU ĐÀO TẠO

Để sinh ra các n-gram tương lai một cách hiệu quả, chúng tôi tinh chỉnh mô hình cơ sở một cách chung trên tổn thất dự đoán của token tiếp theo cũng như γ token tương lai như

Lss = -α0(∑T[t=1] log pθ(yt|y<t, x))                 (5)
    - ∑γ[j=1] αj(∑T-j[t=1] log pθ(yt+j|y<t, x))

trong đó α0 và αj được đặt một cách thực nghiệm để chuẩn hóa tổn thất của dự đoán token tiếp theo và token suy đoán. Bộ điều chỉnh cắt tỉa cây được mô tả trong Phần 3.2.3 có thể được đào tạo trên tổn thất dự đoán token tiếp theo, hoặc cùng với các luồng chính và suy đoán hoặc sau đào tạo các luồng. Thời gian đào tạo thay đổi dựa trên số lượng lớp MSA nhưng có thể so sánh với phương pháp kiểu (Cai et al., 2023) cho Ns = 4. Đối với các thí nghiệm được mô tả trong 4, công thức của chúng tôi liên quan đến việc đào tạo các bộ điều chỉnh LoRA trong 5 epoch trên các tập dữ liệu hạ nguồn trong BFloat16, sử dụng bộ tối ưu hóa AdamQ, tốc độ học 5e-4 và một bộ lập lịch tuyến tính. Đối với cắt tỉa cây (xem Phần 3.2.3), chúng tôi sử dụng một phép biến đổi tuyến tính thứ hạng thấp có thứ hạng 8 để giữ chi phí tham số tối thiểu.

## 4. Thí nghiệm

Chúng tôi đánh giá các phương pháp của mình trên các mô hình tiền đào tạo có quy mô khác nhau và một tập hợp đa dạng các ứng dụng hạ nguồn.

**Tập dữ liệu.** Chúng tôi kiểm tra các phương pháp của mình trên một tập hợp đa dạng các ứng dụng quan trọng cho các trợ lý AI trên thiết bị, cụ thể là Truy vấn có cấu trúc, Tóm tắt văn bản và Biểu diễn ý nghĩa. Chúng tôi đặc biệt chọn thiết lập tinh chỉnh vì nó đã trở thành một chuẩn mực để chia sẻ một mô hình cơ sở và sử dụng các bộ điều chỉnh cụ thể cho ứng dụng cho các ứng dụng hướng người dùng. Chúng tôi sử dụng tập dữ liệu Dialogsum (Chen et al., 2021) cho Tóm tắt văn bản, tập dữ liệu sql-create-context được xây dựng từ WikiSQL (Zhong et al., 2017) và SPIDER (Yu et al., 2018) cho Truy vấn có cấu trúc, và tập dữ liệu e2e-nlg (Dušek et al., 2020) cho Biểu diễn ý nghĩa.

**Cấu hình mô hình.** Chúng tôi đã thử nghiệm bốn mô hình nguồn mở khác nhau có quy mô khác nhau, Phi(1.3B)(Li et al., 2023), Openllama(7B)(Touvron et al., 2023), và OPT(1.3B, 6.7B) (Zhang et al., 2022). Chúng tôi so sánh phương pháp của mình với giải mã suy đoán bản thảo-mục tiêu tiêu chuẩn ((Leviathan et al., 2023)) và khung giải mã suy đoán mô hình đơn lẻ, Medusa (Cai et al., 2023). Đối với phương pháp bản thảo-mục tiêu tiêu chuẩn, chúng tôi sử dụng OPT-125m, cấu hình nhỏ nhất của các mô hình OPT nguồn mở có sẵn làm mô hình bản thảo.

**Đường cơ sở** Để so sánh với phương pháp kiểu Medusa (Cai et al., 2023), chúng tôi sử dụng các mô hình cơ sở tiền đào tạo với các bộ điều chỉnh LoRA (Hu et al., 2022) có thứ hạng 32 và các đầu Medusa làm đường cơ sở, và Speculative Streaming với cùng các mô hình cơ sở, nhúng định danh luồng và các bộ điều chỉnh LoRA làm mục tiêu. Các đầu Medusa được đào tạo theo công thức được mô tả trong (Cai et al., 2023). Cả các đầu Medusa và số lượng luồng tối đa đều được cố định ở 4 và các khối dư mỗi đầu được sử dụng trong Medusa được đặt thành 1. Để so sánh với giải mã suy đoán bản thảo-mục tiêu tiêu chuẩn (Leviathan et al., 2023), chúng tôi sử dụng các mô hình OPT vì chúng đi kèm với các cấu hình và kích thước khác nhau. OPT-125m được triển khai như một mô hình bản thảo trong khi OPT-1.3b và OPT-6.7b được sử dụng như các mô hình mục tiêu vì tỷ lệ 10-100X thường được coi là tối ưu. Lưu ý rằng, tương tự như mô hình mục tiêu, chỉ các bộ điều chỉnh LoRA của mô hình bản thảo được tinh chỉnh trên các ứng dụng hạ nguồn vì việc tinh chỉnh toàn bộ mô hình bản thảo trên mỗi ứng dụng hạ nguồn không thực tế trong các thiết lập trên thiết bị. Ngoài ra, tinh chỉnh LoRA có xu hướng đạt hiệu suất ngang bằng với tinh chỉnh mô hình đầy đủ (Hu et al., 2022).

### 4.1. Kết quả

#### 4.1.1. TỔNG QUAN

Chúng tôi báo cáo tốc độ tăng tốc thời gian thực và các chỉ số chất lượng sinh ra trên phân chia kiểm tra sử dụng kích thước lô 1 trên một GPU Nvidia A100-80G duy nhất. Suy luận được thực hiện trong float16 sử dụng lấy mẫu tham lam và T=0. Vui lòng tham khảo Phụ lục A.2 để biết thêm chi tiết thí nghiệm và Phụ lục B cho các nghiên cứu cắt bỏ về lấy mẫu top-k và T=1. Chúng tôi sử dụng chỉ số độ chính xác Khớp chính xác (EM) cho tác vụ truy vấn có cấu trúc và các chỉ số Rouge1/RougeLSum cho các tác vụ Tóm tắt đối thoại và Biểu diễn ý nghĩa. Chúng tôi sử dụng Ns/N là 1/6 cho tác vụ truy vấn có cấu trúc và 1/2 cho tác vụ tóm tắt và biểu diễn ý nghĩa. Ns được chọn để đảm bảo chỉ số sinh ra ngang bằng với đường cơ sở. Chi tiết về tác động của Ns lên chỉ số sinh ra được tìm thấy trong Phần 4.2.

Bảng 1 trình bày so sánh giữa đường cơ sở giải mã tự hồi quy tiêu chuẩn, Medusa và phương pháp của chúng tôi về tốc độ tăng tốc, tỷ lệ giảm gọi và số lượng tham số bổ sung. Chúng tôi thấy rằng trên nhiều tác vụ hạ nguồn khác nhau, tốc độ tăng tốc thời gian thực và tỷ lệ giảm gọi của Speculative Streaming luôn ngang bằng/cao hơn so với Medusa trong khi phát sinh chi phí tham số ít hơn đáng kể. Hơn nữa, như được tóm tắt trong Bảng 2, phương pháp của chúng tôi đạt được độ trễ thời gian thực tốt hơn so với giải mã suy đoán bản thảo-mục tiêu tiêu chuẩn vì sự khác biệt về số lượng lần gọi mục tiêu giữa cả hai phương pháp không đủ lớn để bù đắp chi phí soạn thảo tự hồi quy. Tất cả độ trễ thời gian thực được báo cáo sử dụng các phiên bản nguồn mở của các mô hình có sẵn trên (Wolf et al., 2019) và có thể việc tối ưu hóa thêm các mô hình bản thảo và mục tiêu bằng các kỹ thuật suy luận hiệu quả (Nvidia, 2024) hoặc lượng tử hóa (int4/8) có thể dẫn đến độ trễ thấp hơn. Cuối cùng, điều đáng chú ý là các chỉ số sinh ra của phương pháp chúng tôi luôn có thể so sánh với các mô hình cơ sở được tinh chỉnh LoRA làm cho nó trở thành một sự thay thế tuyệt vời cho việc tinh chỉnh dựa trên dự đoán token tiếp theo.

#### 4.1.2. PHÂN TÍCH VÀ HIỂU BIẾT

**Không có mô hình phụ trợ** Các đầu Medusa sinh ra mỗi token một cách độc lập từ trạng thái ẩn được chia sẻ của lớp cuối, và sự phụ thuộc giữa các token suy đoán được dự đoán bởi các đầu medusa, y(t+1..t+γ) và token tiếp theo yt được dự đoán bởi mô hình cơ sở tại bước thời gian t có thể không được nắm bắt tốt vì không có cơ chế chú ý nào liên quan. Mặt khác, các luồng suy đoán chú ý đến luồng chính và nhau, nắm bắt sự phụ thuộc token, dẫn đến tỷ lệ giảm gọi tốt hơn so với Medusa. Về tham số, mỗi đầu Medusa thêm khoảng h² + hv tham số, trong đó h là kích thước ẩn và v là kích thước từ vựng. Số lượng đầu Medusa cũng mở rộng tuyến tính w.r.t. γ, độ dài của cửa sổ suy đoán, điều này đến lượt làm tăng chi phí tham số tuyến tính với γ. Mặt khác, Speculative Streaming sử dụng các bộ điều chỉnh suy đoán không mở rộng với γ. Mặc dù, các nhúng định danh luồng mở rộng với γ, chi phí tham số liên quan đến mỗi nhúng là tuyến tính với h. Hơn nữa, trong các thiết lập tinh chỉnh, các tham số "bộ điều chỉnh suy đoán" được chia sẻ với các bộ điều chỉnh mô hình cơ sở, do đó, chi phí tham số liên quan đến phương pháp của chúng tôi chỉ là γh.

**Với mô hình phụ trợ** Speculative Streaming luôn đạt được độ trễ thời gian thực thấp hơn so với giải mã suy đoán bản thảo-mục tiêu tiêu chuẩn như được mô tả trong Bảng 2. Điều đáng chú ý là, các lần gọi mô hình mục tiêu của giải mã suy đoán bản thảo-mục tiêu thấp hơn so với Speculative Streaming, tuy nhiên, nó có chi phí là chạy mô hình bản thảo γ lần một cách tự hồi quy để sinh ra bản thảo suy đoán. Mặt khác, việc sinh bản thảo với Speculative Streaming gần như không phát sinh chi phí độ trễ bổ sung, vì giải mã mô hình mục tiêu có xu hướng bị giới hạn bộ nhớ ngay cả với kích thước bản thảo cây tăng. Điều này dịch sang tăng sử dụng kernel và cường độ số học như được hiển thị trong Hình 3. Phương pháp dựa trên bản thảo mặt khác có sử dụng kernel thấp vì bản chất bị giới hạn bộ nhớ của soạn thảo tự hồi quy.

Một lập luận có thể được đưa ra rằng một mô hình bản thảo nhỏ hơn có thể hoạt động tốt hơn vì soạn thảo sẽ tốn ít chi phí hơn, nhưng tỷ lệ chấp nhận cũng có thể giảm khi kích thước mô hình bản thảo giảm. Để chính thức hóa so sánh với giải mã suy đoán bản thảo-mục tiêu tiêu chuẩn, chúng tôi thực hiện phân tích sau, giả sử, Cdraft là chi phí độ trễ liên quan đến lần chuyển tiếp qua mô hình bản thảo, Ctarget là chi phí liên quan đến lần chuyển tiếp qua mô hình mục tiêu, trong khi Css là chi phí liên quan đến lần chuyển tiếp speculative streaming. ζ là số lượng token giải mã được tiến lên trong bước xác minh cho phương pháp bản thảo-mục tiêu trong khi β là số lượng token được tiến lên trong Speculative Streaming. Chúng tôi cân bằng chi phí độ trễ liên quan đến việc tiến lên token đơn lẻ để so sánh cả hai phương pháp.

(γ*Cdraft + Ctarget)/ζ = Css/β                    (6)
(γ + Ctarget/Cdraft)/ζ = (Css/Cdraft)/β

Giả định γ = 4, Ctarget/Cdraft = 10, và Css ≈ Ctarget, ζ = 1.4β, có nghĩa là việc tiến lên mỗi bước xác minh trong phương pháp bản thảo-mục tiêu tiêu chuẩn phải là 1.4X của Speculative Streaming để đạt được độ ngang bằng độ trễ thời gian thực. Lưu ý rằng, phân tích này bỏ qua chi phí điều chỉnh bộ nhớ đệm và chi phí xử lý prompt, nhưng cung cấp trực giác có giá trị để hướng dẫn lựa chọn giữa các phương pháp bản thảo-mục tiêu vs Speculative Streaming. Chúng tôi cũng phân tích dưới những thiết lập nào speculative streaming có khả năng mang lại nhiều lợi ích hơn so với phương pháp bản thảo-mục tiêu tiêu chuẩn. Hình 4 hiển thị tốc độ tăng tốc lý thuyết của Speculative Streaming so với phương pháp dựa trên bản thảo-mục tiêu cho các tỷ lệ độ trễ Mục tiêu đến bản thảo khác nhau. Khi tỷ lệ độ trễ tăng, phương pháp bản thảo-mục tiêu có khả năng mang lại nhiều lợi ích tốc độ tăng tốc hơn khi ζ/β > 1, có nghĩa là khi mô hình bản thảo đủ chính xác để đạt được nhiều tiến bộ token hơn mỗi bước xác minh mô hình mục tiêu so với Speculative Streaming và cũng đủ nhỏ để mang lại tỷ lệ độ trễ cao hơn, nó có khả năng mang lại lợi ích hơn. Việc tìm/tạo ra một mô hình như vậy thường yêu cầu những nỗ lực kỹ thuật đáng kể. Trong các thiết lập ứng dụng hạ nguồn, việc tìm các mô hình bản thảo lý tưởng trở nên thậm chí thách thức hơn vì ζ có xu hướng thay đổi dựa trên ứng dụng. Nếu các ứng dụng chia sẻ mô hình bản thảo và chỉ đào tạo các bộ điều chỉnh, mô hình bản thảo có thể không còn đủ nhỏ để đáp ứng tỷ lệ độ trễ mục tiêu-đến-bản thảo, làm cho việc đạt được nhiều tốc độ tăng tốc hơn so với Speculative Streaming trở nên thách thức.

### 4.2. Nghiên cứu cắt bỏ

**Kích thước bản thảo suy đoán.** Để cải thiện tỷ lệ chấp nhận của bản thảo cây, chúng tôi thử các thiết lập khác nhau của γ, số lượng vị trí suy đoán, và k, số lượng token được lấy mẫu mỗi vị trí suy đoán. Hình 5 hiển thị tốc độ tăng tốc thời gian thực cho γ = 3. Khi chúng tôi lấy mẫu nhiều token hơn từ mỗi vị trí suy đoán, việc tiến lên mỗi lần chuyển tiếp, β tăng vì có nhiều ứng viên hơn cho việc xác minh, dẫn đến tốc độ tăng tốc nhiều hơn. Tuy nhiên, khi chúng tôi tiếp tục tăng k, chi phí độ trễ lần chuyển tiếp trở nên nổi bật hơn khi mô hình chuyển sang giai đoạn bị giới hạn tính toán và tốc độ tăng tốc đảo chiều hướng. Điều này là vì việc tạo thành bản thảo cây một cách ngây thơ dẫn đến tăng theo cấp số nhân trong kích thước lô với k như được mô tả trong 3.2.3. Chúng tôi chèn một lớp cắt tỉa cây để loại bỏ các đường dẫn ít có khả năng hơn và giảm kích thước của bản thảo cây. Việc cắt tỉa bản thảo cây giảm độ trễ lần chuyển tiếp, và một ngưỡng được hiệu chuẩn tốt đảm bảo rằng chỉ các đường dẫn nhiễu trong cây bị cắt tỉa. Cắt tỉa cây có xu hướng giúp với tốc độ tăng tốc thời gian thực khi k tiếp tục tăng như được hiển thị trong Hình 5.

**Số lượng lớp MSA** Có những đánh đổi liên quan đến việc quyết định số lượng lớp MSA để kết hợp về chỉ số sinh ra hạ nguồn, thời gian đào tạo và tăng FLOP. Khi chúng tôi tăng số lượng lớp MSA, chỉ số sinh ra cải thiện và xu hướng này vẫn giống nhau trên các tác vụ hạ nguồn khác nhau. Thông thường việc kết hợp MSA trong 2 - 8 lớp hàng đầu mang lại một sự đánh đổi tốt giữa chỉ số, tăng FLOP và thời gian đào tạo. Hình 6 hiển thị hiệu suất sinh ra của mô hình OPT-1.3b trên các tác vụ Truy vấn có cấu trúc và Tóm tắt.

## 5. Kết luận

Trong bài báo này, chúng tôi đã đề xuất Speculative Streaming, một phương pháp để tăng tốc giải mã của các mô hình ngôn ngữ lớn. So với các phương pháp giải mã suy đoán tiêu chuẩn, Speculative Streaming loại bỏ nhu cầu cho một mô hình "bản thảo" phụ trợ. Thay vào đó, nó thống nhất suy đoán và xác minh bằng cách kết hợp hiệu quả nhiều luồng suy đoán vào một mô hình "mục tiêu" duy nhất. Speculative Streaming đơn giản hóa quy trình tinh chỉnh và đạt được tốc độ tăng tốc và chất lượng ngang bằng hoặc tốt hơn so với các phương pháp trước đó. Nó cũng hiệu quả về tham số và loại bỏ nhu cầu tải hai mô hình vào bộ nhớ, làm cho nó trở thành một phương pháp phù hợp cho các tình huống hạn chế tài nguyên.

## Lời cảm ơn

Chúng tôi muốn cảm ơn Sachin Mehta, Moin Nabi, Antonie Lin, Minsik Cho, Arsalan Farooq, và Jason Williams vì những phản hồi và thảo luận có giá trị của họ.
