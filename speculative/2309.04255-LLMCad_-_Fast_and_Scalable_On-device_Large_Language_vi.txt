# 2309.04255.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/speculative/2309.04255.pdf
# Kích thước tệp: 1667824 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
LLMCad : Suy luận Mô hình Ngôn ngữ Lớn trên Thiết bị Nhanh và Có thể Mở rộng
Daliang Xu♦, Wangsong Yin♦, Xin Jin♦, Ying Zhang♦, Shiyun Wei⋆, Mengwei Xu^,
Xuanzhe Liu♦
♦Phòng thí nghiệm Chính về Công nghệ Phần mềm Tin cậy Cao (Đại học Bắc Kinh), Bắc Kinh, Trung Quốc
⋆Phòng thí nghiệm Zhongguancun, Bắc Kinh, Trung Quốc.
^Phòng thí nghiệm Chính của Nhà nước về Công nghệ Mạng và Chuyển mạch (BUPT), Bắc Kinh, Trung Quốc
{xudaliang,hg,xinjinpku,zhang.ying,weishiyun,liuxuanzhe}@pku.edu.cn
yws@stu.pku.edu.cn
mwx@bupt.edu.cn
TÓM TẮT
Các tác vụ sinh, chẳng hạn như tạo văn bản và trả lời câu hỏi, giữ một vị trí quan trọng trong lĩnh vực ứng dụng di động. Do tính nhạy cảm của chúng đối với mối quan tâm về quyền riêng tư, có nhu cầu ngày càng tăng để thực thi chúng trực tiếp trên thiết bị di động. Hiện tại, việc thực thi các tác vụ sinh này phụ thuộc rất nhiều vào Mô hình Ngôn ngữ Lớn (LLM). Tuy nhiên, khả năng bộ nhớ hạn chế của các thiết bị này đặt ra một thách thức to lớn đối với khả năng mở rộng của các mô hình như vậy.

Trong nghiên cứu của chúng tôi, chúng tôi giới thiệu LLMCad , một công cụ suy luận trên thiết bị sáng tạo được thiết kế đặc biệt cho các tác vụ Xử lý Ngôn ngữ Tự nhiên (NLP) sinh hiệu quả. Ý tưởng cốt lõi đằng sau LLMCad xoay quanh sự hợp tác mô hình: một LLM nhỏ gọn, nằm trong bộ nhớ, đảm nhận việc tạo ra các token đơn giản nhất, trong khi một LLM độ chính xác cao can thiệp để xác thực các token này và sửa chữa bất kỳ lỗi nào được xác định. LLMCad kết hợp ba kỹ thuật mới: (1) Thay vì tạo các token ứng cử viên một cách tuần tự, LLMCad sử dụng LLM nhỏ hơn để xây dựng một cây token, bao gồm một phạm vi rộng hơn các đường dẫn token hợp lý. Sau đó, LLM lớn hơn có thể xác thực tất cả các đường dẫn này một cách đồng thời một cách hiệu quả. (2) Nó sử dụng một chiến lược dự phòng tự điều chỉnh, khởi động nhanh chóng quá trình xác minh bất cứ khi nào LLM nhỏ hơn tạo ra một token lỗi. (3) Để đảm bảo luồng tạo token liên tục, LLMCad tạo token một cách suy đoán trong quá trình xác minh bằng cách triển khai một đường ống tính toán-IO.

Thông qua một loạt thí nghiệm mở rộng, LLMCad thể hiện tốc độ tạo token ấn tượng, đạt được tốc độ nhanh hơn tới 9,3× so với các công cụ suy luận hiện có.

1 GIỚI THIỆU
Các tác vụ sinh như tạo văn bản, trả lời câu hỏi và dịch thuật đóng vai trò quan trọng trên thiết bị di động, vì nhiều ứng dụng dựa vào chúng để cung cấp các chức năng chính. Ví dụ, ứng dụng phương thức nhập như Google

100M 1B 10B 100B 1T
Kích thước tham số02040Độ chính xác (%)
Jetson TX2
(IoT)Xiaomi10
(Smartphone)Jetson Orin
(Autonomous)

LLaMA-Math
GPT3-NLU
GPT3-Mode
GPT3-GM
Rào cản bộ nhớ(a) Khả năng xuất hiện qua nhiều LLM khác nhau.

100M 1B 10B 100B 1T
Kích thước tham số02040Độ trễ (s)
Jetson TX2
(IoT)Xiaomi10
(Smartphone)Jetson Orin
(Autonomous)

T5-TX2
LLaMA-Xiaomi 10
LLaMA-Orin
Rào cản bộ nhớ
(b) Độ trễ tạo của LLM qua nhiều thiết bị khác nhau.

Hình 1: Rào cản bộ nhớ cản trở "quy luật tỷ lệ" của LLM trên thiết bị di động. *-Math, *-NLU, *-Mode, và *-GM biểu thị khả năng xuất hiện của LLM: suy luận toán học, hiểu đa nhiệm vụ, số học chế độ, và học biểu diễn có ý nghĩa.

GBoard tận dụng rất nhiều khả năng tạo văn bản của nó, trong khi trợ lý riêng tư như Apple Siri sử dụng nó để trả lời câu hỏi. Các tác vụ như vậy thường nhạy cảm về quyền riêng tư và phụ thuộc rất nhiều vào dữ liệu riêng của người dùng, do đó đòi hỏi suy luận cục bộ trên thiết bị.

Các mô hình ngôn ngữ lớn (LLM), đặc biệt là những mô hình được xây dựng trên bộ giải mã transformer [ 65] như GPT-3 [ 17] và LLaMA [ 64], đã trở thành phương pháp tiêu chuẩn để giải quyết các tác vụ sinh NLP. Nghiên cứu gần đây trong cộng đồng học máy đã chứng minh rằng việc mở rộng quy mô kích thước tham số của các LLM như vậy mang lại cải thiện độ chính xác và khả năng xuất hiện [ 17,64,71,71,72], như được thể hiện trong Hình 1(a). Nói chung, một LLM cần có hơn 1B tham số để học các biểu diễn có ý nghĩa [ 51], hơn 10B tham số để thể hiện khả năng suy luận số học nhất định [ 22], và hơn 30B

1arXiv:2309.04255v1  [cs.NI]  8 Sep 2023

--- TRANG 2 ---
Conference'17, July 2017, Washington, DC, USA Daliang Xu♦, Wangsong Yin♦, Xin Jin♦, Ying Zhang♦, Shiyun Wei⋆, Mengwei Xu^, Xuanzhe Liu♦

tham số để đạt được khả năng hiểu đa nhiệm vụ [ 30]. Hiện tượng này được cộng đồng học máy công nhận rộng rãi là quy luật tỷ lệ [11, 12, 21, 34].

Thách thức chính: rào cản bộ nhớ. Tuy nhiên, các thí nghiệm sơ bộ của chúng tôi trong Hình 1(b) cho thấy rằng khả năng tỷ lệ bị thách thức trên thiết bị di động. Cụ thể, khi các LLM quá lớn để vừa với bộ nhớ thiết bị, các công cụ DNN di động như MNN [ 4] và llama.cpp [ 45] cần phải liên tục giải phóng và tải trọng số mô hình. Điều này dẫn đến độ trễ suy luận kéo dài 59–224 ×. Rào cản bộ nhớ như vậy cản trở nghiêm trọng quy luật tỷ lệ. Người dùng phải lựa chọn giữa tạo theo thời gian thực và khả năng xuất hiện. Ví dụ, 10B tham số đại diện cho kích thước tối thiểu cần thiết để LLaMA có khả năng suy luận số học, nhưng nó cũng đại diện cho kích thước tham số tối đa để đạt được suy luận thời gian thực trên điện thoại thông minh (ví dụ, Xiaomi 10).

LLMCad : phá vỡ rào cản bộ nhớ thông qua hợp tác mô hình. Trong bài báo này, chúng tôi đề xuất LLMCad , công cụ suy luận hiệu quả đầu tiên cho các tác vụ NLP sinh trên thiết bị. LLMCad mang khả năng tỷ lệ của LLM đến thiết bị di động với tốc độ tạo có thể chấp nhận được thông qua hợp tác mô hình . Ý tưởng chính là ủy thác hầu hết các token cho một LLM thời gian thực nhỏ hơn có thể được lưu trữ hoàn toàn trong bộ nhớ thiết bị (được gọi là LLM thường trú bộ nhớ). Thiết kế này dựa trên một quan sát chính rằng, trong khi một LLM nhỏ hơn không đủ để cung cấp các câu từ đầu cuối đến cuối thỏa mãn, chúng có thể tạo ra đúng hầu hết các token dễ (ví dụ, từ hạn định, đại từ và dấu câu). Hơn nữa, các LLM thường được đào tạo với một loạt các biến thể mô hình, ví dụ T5-Small/Base/Large [ 55] và LLaMa-7B/13B/33B [ 64], và đối tác nhỏ hơn của nó (ví dụ, LLaMa-7B và T5-small, được gọi là mô hình thường trú bộ nhớ trong bài báo này) thường có thể được lưu trữ trong bộ nhớ một cách dễ dàng [17, 55, 64, 75].

LLMCad sử dụng một hình thức hợp tác mô hình độc đáo, được gọi là "tạo-rồi-xác minh" [ 20,41]. Trong phương pháp này, LLM thường trú bộ nhớ đóng vai trò là bộ tạo token, trong khi LLM mục tiêu hoạt động như một bộ xác minh, sử dụng đầu ra của nó làm chuẩn mực để kiểm tra và sửa chữa bất kỳ lỗi nào được giới thiệu trong quá trình tạo token. Phương pháp này cung cấp hai lợi thế đáng kể: (1) Không làm giảm độ chính xác. Mỗi token được xác minh bởi mô hình mục tiêu, do đó độ chính xác của nó được đảm bảo. Điều này rất quan trọng vì một token sai có thể lan truyền lỗi của nó đến các token tiếp theo do bản chất tự hồi quy. (2) Xác minh nhanh. Như sẽ được chi tiết trong §2.3, việc xác minh 𝑁 token có thể được thực hiện trong một lần suy luận của mô hình mục tiêu, do đó nhanh hơn nhiều so với việc sử dụng nó để tạo 𝑁 token tuần tự.

Mặc dù có những lợi thế này, việc áp dụng hợp tác mô hình cho LLM trên thiết bị giới thiệu ba thách thức đặc biệt:

•Token chính xác bị bỏ qua với độ tin cậy không tối ưu. Thông thường, các công cụ và nghiên cứu LLM tiên tiến luôn sử dụng token có xác suất cao nhất làm đầu ra. Tuy nhiên, quan sát của chúng tôi đã tiết lộ rằng một số lỗi tạo bởi LLM thường trú bộ nhớ có thể được sửa chữa bởi các token không tối ưu. Hình 4 đưa ra một ví dụ thực tế về hiện tượng như vậy. Cho rằng chi phí hiệu suất đáng kể liên quan đến xác minh trên thiết bị, LLMCad phải tận dụng những token thường bị bỏ qua này để giảm tần suất xác minh.

•Thời điểm xác minh. Một khía cạnh quan trọng khác là xác định khi nào bắt đầu quá trình xác minh. Xác minh trên thiết bị tốn thời gian, ví dụ, mất 7,1s trên Jetson TX2. Xác minh quá sớm hoặc quá muộn chỉ lãng phí tài nguyên khan hiếm của thiết bị di động bằng xác minh không hợp lệ (tức là không phát hiện lỗi) hoặc token vô dụng. Các công trình trước đây thường dựa vào một token đơn hoặc độ dài chuỗi token, có thể không xác định chính xác thời điểm xác minh tối ưu.

•Bất đối xứng IO so với tính toán. Với một tầng LLM, việc thực thi LLM lớn chặn suy luận mô hình nhỏ do sự phụ thuộc token chéo, và bộ xử lý được sử dụng dưới mức do I/O trở thành nút thắt cổ chai trong quá trình tải trọng số. Tình huống như vậy cản trở nghiêm trọng tốc độ suy luận vì mô hình mục tiêu cần được gọi không thể tránh khỏi để đảm bảo tạo token chính xác.

Để đáp ứng, LLMCad thiết kế ba kỹ thuật mới:

(1) Tạo và xác minh cây token ( §3.2). Thay vì tạo và xác minh một chuỗi token tuyến tính, LLMCad sử dụng một phương pháp khác bằng cách xây dựng và xác thực một "cây token." Cây token này cho phép mỗi token có nhiều token kế tiếp tiềm năng. Để thực hiện điều này một cách hiệu quả, LLMCad sử dụng ba mô-đun mới: (1) Bộ điều tốc nhánh dựa trên độ tin cậy điều chỉnh tiến trình của các nhánh khác nhau để ngăn việc phân bổ lãng phí tài nguyên tính toán cho nhánh sai; (2) Bộ giải mã cây tạo token từ các nhánh khác nhau mà không phát sinh chi phí chuyển đổi ngữ cảnh giữa chúng; (3) Bộ xác minh cây token không tự hồi quy kiểm tra và sửa chữa tất cả lỗi trong một cây token theo cách hàng loạt, với chi phí của một lần lặp duy nhất.

(2) Chiến lược dự phòng tự thích ứng ( §3.3). Chiến lược này được thiết kế để khởi động quá trình xác minh kịp thời khi LLM thường trú bộ nhớ tạo ra một token không chính xác. Nó được lấy cảm hứng từ hai quan sát chính: (1) Thông thường, mỗi token được tạo bởi LLM thường trú bộ nhớ giới thiệu một số "bất định" (điểm tin cậy không hoàn hảo). LLMCad sử dụng một chỉ số chính xác hơn được gọi là bất định tích lũy trong cây token so với. So với các công trình trước, chỉ số này phản ánh tốt hơn xác suất lỗi liên quan đến việc tạo LLM thường trú bộ nhớ, đặc biệt xem xét bản chất tích lũy của các mô hình tự hồi quy. (2) Dữ liệu lịch sử liên quan đến độ chính xác của các token đã xác minh

2

--- TRANG 3 ---
LLMCad : Suy luận Mô hình Ngôn ngữ Lớn trên Thiết bị Nhanh và Có thể Mở rộng Conference'17, July 2017, Washington, DC, USA

được khai thác để đánh giá khả năng tạo của LLM thường trú bộ nhớ. Khả năng tạo mạnh hơn đòi hỏi tần suất xác minh thấp hơn.

(3) Đường ống tạo suy đoán ( §3.4). Để phá vỡ sự phụ thuộc token chéo và tăng cường tính song song, chúng tôi đề xuất tạo suy đoán , tức là tiếp tục tạo token thông qua LLM thường trú bộ nhớ trong quá trình xác minh, Điều này được dựa trên hiểu biết rằng đôi khi quá trình xác minh có thể không phát hiện lỗi, làm cho các token được tạo suy đoán có thể sử dụng được. Tuy nhiên, việc tạo suy đoán đồng thời với xác minh trực tiếp có thể dẫn đến tranh chấp bộ xử lý và bộ nhớ. Để giải quyết thêm vấn đề này, LLMCad kết hợp một đường ống tinh vi, đảm bảo rằng tạo suy đoán chỉ chạy khi tải tham số LLM mục tiêu dưới giới hạn trên bộ nhớ để tránh can thiệp vào quá trình xác minh thông thường.

Triển khai và đánh giá. Chúng tôi đã triển khai đầy đủ LLMCad trên hai công cụ LLM SOTA: PyTorch [ 5] và llama.cpp [ 45]. Đánh giá mở rộng của hệ thống được tiến hành trên bốn nền tảng: hai thiết bị IoT (Jetson TX2 và Jetson Orin NX) và hai điện thoại thông minh (Xiaomi 10 và Xiaomi 11). Đánh giá này bao gồm sáu LLM được sử dụng rộng rãi (GPT2 [ 54], T5 [ 55], mT5 [ 75], Bart [ 42], Vicuna, và LLaMa2 [ 64]) và bảy bộ dữ liệu (CNN/Daily [ 61], Wikitext [ 47], iwlt2017 [ 19], wmt14/22 [ 15], SQuAD [ 56], parrot, và TruthfulQA [ 44]). Chúng tôi cũng so sánh LLMCad với năm đường cơ sở cạnh tranh tiên tiến [ 5,27,37,41,45], bao gồm hai khung đường ống tính toán-tải và hai khung hợp tác LLM "bộ tạo và bộ xác minh". Kết quả của chúng tôi chứng minh rõ ràng hiệu suất vượt trội của LLMCad . Khi so sánh với các công cụ LLM tiên tiến, LLMCad có thể giảm thời gian tạo trung bình mỗi token 2,9–9,3 × và 3,5–4,7× trên thiết bị IoT và điện thoại thông minh, tương ứng, mà không làm giảm độ chính xác. Đối với các LLM có kích thước >10B như LLaMA2-13B trước đây không thể chịu đựng được trên điện thoại thông minh, LLMCad tạo ra hơn một token mỗi giây. Hơn nữa, so với các đường cơ sở cạnh tranh, LLMCad có thể đạt được tăng tốc lên đến 5,6 × và độ chính xác cao hơn đáng chú ý.

Các đóng góp chính của công trình này như sau:
•Chúng tôi khám phá kỹ lưỡng các cơ hội và thách thức của việc suy luận LLM trên thiết bị.
•Chúng tôi đề xuất LLMCad , công cụ suy luận hiệu quả đầu tiên cho các tác vụ NLP sinh trên thiết bị. Để tăng tốc quy trình tạo, LLMCad sử dụng hợp tác LLM "bộ tạo và bộ xác minh" và kết hợp ba kỹ thuật mới: tạo và xác minh cây, chiến lược dự phòng tự thích ứng, và đường ống tạo suy đoán. Những tiến bộ này cho phép LLMCad giảm thiểu hiệu quả vấn đề rào cản bộ nhớ.

Masked Multi Self Attention 
LayerNorm
Feed Forward
LayerNormN      Lớp Decoder  Đầu vào
Embedding văn bảnEmbedding vị trí
Đầu ra(a) Kiến trúc GPT3

Lớp Decoder  
Lớp Decoder  
Lớp Decoder  Youshould
wearwear
shoesshoes
<EOS>iter0 iter1 iter2
...Mô hình ngôn ngữ lớn
Lớp Decoder  (b) Suy luận tự hồi quy

Hình 2: Kiến trúc của mô hình ngôn ngữ chỉ giải mã (GPT3) và tổng quan về mẫu suy luận LLM: tự hồi quy.

•Chúng tôi tạo nguyên mẫu LLMCad và đánh giá nó với các LLM đại diện và thiết bị di động thương mại. Kết quả chứng minh hiệu suất vượt trội của nó so với các phương pháp hiện có.

2 BỐI CẢNH VÀ ĐỘNG LỰC

2.1 LLM Sinh dựa trên Decoder

Các tác vụ ML sinh trên thiết bị. Các tác vụ sinh là những tác vụ liên quan đến việc tạo ra hoặc tổng hợp tự động nội dung mới như chuỗi văn bản và pixel hình ảnh [ 14,58,59]. Các tác vụ sinh điển hình trong lĩnh vực NLP (trọng tâm của công trình này) bao gồm mô hình hóa ngôn ngữ, dịch máy, tóm tắt và trả lời câu hỏi. Trọng tâm chính là cung cấp đầu ra nội dung mới, có ý nghĩa và mạch lạc. So với các tác vụ phân loại truyền thống như phân loại chủ đề và phân loại hình ảnh, các tác vụ sinh thường thách thức hơn nhưng cũng sâu sắc hơn đối với cuộc sống con người [63].

Các tác vụ ML sinh đã được phục vụ rộng rãi cho người dùng di động, chẳng hạn như mô hình hóa ngôn ngữ cho Google Gboard [ 1], trả lời câu hỏi cho Siri [ 6], và các dịch vụ dịch thuật như iTranslate [ 3] và Google Translate [ 2], v.v. Để đảm bảo quyền riêng tư dữ liệu người dùng (ví dụ, kho văn bản) và tính khả dụng dịch vụ, các mô hình tốt hơn là được triển khai trực tiếp trên thiết bị để suy luận cục bộ.

Kiến trúc LLM dựa trên Decoder. Mô hình ngôn ngữ lớn (LLM) dựa trên decoder, bao gồm cả kiến trúc chỉ decoder và encoder-decoder, là phương pháp tiêu chuẩn cho các tác vụ sinh, như GPT-3 [ 17], LLaMa [ 64], và GLM-130B [ 23,79]. Như được thể hiện trong Hình 2(a), một LLM dựa trên decoder điển hình bao gồm một embedding văn bản, một embedding vị trí, và nhiều lớp decoder được xếp chồng tuần tự, trong đó mỗi lớp decoder bao gồm self-attention có mặt nạ, LayerNorm, và các phép toán Tuyến tính. Đối với những LLM encoder-decoder như T5 [ 55] và mT5 [ 75], các lớp encoder được kết hợp trước decoder để tăng cường khả năng hiểu ngữ nghĩa.

3

--- TRANG 4 ---
Conference'17, July 2017, Washington, DC, USA Daliang Xu♦, Wangsong Yin♦, Xin Jin♦, Ying Zhang♦, Shiyun Wei⋆, Mengwei Xu^, Xuanzhe Liu♦

Suy luận tự hồi quy. Các LLM sinh sử dụng một quy trình suy luận tự hồi quy tạo ra một token tại một thời điểm và lấy token đó làm đầu vào để tạo ra token tiếp theo. Ví dụ, Hình 2(b) minh họa một quy trình suy luận ba lần lặp tự hồi quy. Trong lần lặp thứ 1, mô hình lấy tất cả các token hiện có ("You should") làm đầu vào và tạo ra đầu ra "wear." Trong lần lặp tiếp theo, "wear" mới được tạo sẽ được đưa vào mô hình, sau đó dự đoán "shoes." Quá trình này tiếp tục cho đến khi mô hình tạo ra token kết thúc chuỗi ( <𝐸𝑂𝑆>), chỉ ra sự kết thúc của quy trình tạo. Bản chất của suy luận tự hồi quy giới thiệu các thách thức độc đáo để tối ưu hóa LLM trên thiết bị như sẽ được mô tả sau.

2.2 LLM trên thiết bị bị Giới hạn bởi Bộ nhớ

Trong phần này, chúng tôi thực hiện các thí nghiệm thí điểm để tiết lộ vấn đề hiệu suất của suy luận LLM trên thiết bị. Các thí nghiệm được thực hiện trên các LLM điển hình (GPT2, T5, và LLaMa), bộ dữ liệu (SQuAD và TriviaQA), và thiết bị di động (Jetson TX2 và Xiaomi 10) sử dụng các công cụ DL tiên tiến (PyTorch [ 5] và llama.cpp [ 45]. Chúng tôi tóm tắt những phát hiện chính dưới đây.

Mở rộng kích thước tham số mang lại cải thiện độ chính xác. Kiến trúc LLM dựa trên Transformer có tính linh hoạt và khả năng mở rộng cao bằng cách đơn giản điều chỉnh các lớp encoder/decoder, độ dài chuỗi, và các siêu tham số khác. Do đó, LLM phổ biến thường được phát triển với một loạt các biến thể mô hình, như T5-Small/Base/Large [ 55] và LLaMa-7B/13B/33B/65B [ 64]. Với kích thước tham số mở rộng, mô hình thể hiện khả năng mạnh hơn. Như được thể hiện trong Bảng 1, T5-Large vượt trội hơn T5-Small với biên độ đáng kể, đạt được cải thiện 7,6% về độ chính xác trên bộ dữ liệu SQuAD. Tương tự, LLaMa-13B chứng minh độ chính xác QA cao hơn 6,6% trên TriviaQA so với LLaMa-7B. Thật vậy, hiện tượng như vậy được biết đến rộng rãi trong cộng đồng ML là quy luật tỷ lệ [11, 12, 21, 34].

Khả năng mở rộng LLM trên thiết bị cản trở trên rào cản bộ nhớ. Tuy nhiên, như được thể hiện trong Bảng 1, tốc độ suy luận giảm nhanh chóng khi mức tiêu thụ bộ nhớ vượt quá ngân sách bộ nhớ. Ví dụ, trên thiết bị TX2, độ trễ suy luận tăng 189–224 × với chỉ tăng 5,8–12,2× về kích thước mô hình. Để hiểu sâu hơn về các yếu tố ảnh hưởng đến tốc độ suy luận, chúng tôi đã tiến hành phân tích phân tách, như được thể hiện trong Hình 3. Nó cho thấy rõ ràng rằng, khi suy luận mô hình đòi hỏi kích thước bộ nhớ không thể chi trả trên thiết bị edge, việc tải tham số từ đĩa vào bộ nhớ (tức là disk I/O) sớm trở thành nút thắt cổ chai của thời gian suy luận (95,9–98,8%). Tình huống này là do các công cụ LLM phía thiết bị tiên tiến, như

(a) Trên TriviaQA với Xiaomi10
# Tham số (B) Độ chính xác PeakMem. (GB) Thời gian Suy luận (ms)
LLaMa-7B (4bits) 7 50.0 4.1 275 (1x)
LLaMa-13B (4bits) 13 56.6 9.8 10118 (37x)
LLaMa-33B (4bits) 33 65.1 20.8 22017 (87x)

(b) Trên SQuAD với TX2
# Tham số (B) Độ chính xác PeakMem. (GB) Thời gian Suy luận (ms)
GPT2 0.14 49.8 0.5 37.64 (1x)
GPT2-Large 0.8 53.7 3.1 8065 (214x)
mT5-Small 0.3 76.4 0.7 31 (1x)
mT5-Base 0.58 83.8 1.5 2134 (69x)
mT5-Large 1.2 87.0 3.9 7214 (230x)
T5-Small 0.06 79.1 0.2 27 (1x)
T5-Base 0.22 85.4 0.8 37 (1.4x)
T5-Large 0.73 86.7 2.8 7098 (263x)

Bảng 1: Kích thước tham số, độ chính xác, sử dụng bộ nhớ đỉnh, và độ trễ suy luận của các biến thể LLM trong một lần lặp tự hồi quy. (a) thí nghiệm được thực hiện trên PyTorch trong khi (b) thí nghiệm được thực hiện trên llama.cpp.

100101102
Tỷ lệ phần trăm (%)SmallBaseLargeT5
100.0%100.0%1.2% 98.8%tính toán tải

100101102
Tỷ lệ phần trăm (%)SmallBaseLargemT5100.0%2.7% 97.3%2.3% 97.7%

100101102
Tỷ lệ phần trăm (%)BaseLargeGPT2 100.0%1.7% 98.3%

100101102
Tỷ lệ phần trăm (%)7B13BLLaMa100.0%4.1% 95.9%

Hình 3: Phân tách độ trễ suy luận của các biến thể LLM khác nhau trong một lần lặp tự hồi quy.

MNN [ 4] và llama.cpp [ 45], sử dụng kỹ thuật hoán đổi tự động giải phóng bộ nhớ trọng số đã suy luận và tải trọng số cần suy luận từ đĩa khi ràng buộc bộ nhớ bị vượt quá.

Bản chất tự hồi quy làm cho các tối ưu hóa bộ nhớ truyền thống hầu như không hiệu quả đối với LLM sinh. Đáng chú ý rằng tối ưu hóa bộ nhớ cho suy luận mô hình đã là một chủ đề được nghiên cứu kỹ lưỡng trong những năm gần đây [ 27,32, 46,78]. Nhiều phương pháp cấp độ hệ thống đã được khám phá, như xử lý hàng loạt [ 78], đường ống tính toán-I/O [ 27], và hoán đổi thông minh [ 32,46]. Tuy nhiên, những công trình này khó có thể áp dụng cho LLM trên thiết bị, bởi vì: (1) Tính song song/xử lý hàng loạt không khả dụng vì suy luận tự hồi quy yêu cầu tạo token tuần tự; (2) Chồng chéo có thể đạt được lợi ích hạn chế vì thời gian I/O lớn hơn hàng trăm lần so với tính toán. Ngoài ra, các phương pháp thuật toán như lượng tử hóa [ 25,31,66,76] có thể mang lại giảm bộ nhớ vài lần (ví dụ, FP16− →INT4 [ 25]), hiệu quả của chúng bị hạn chế vì độ chính xác bit thấp (ví dụ, 2 bit) đã được chứng minh là không đủ để giữ lại khả năng mô hình [ 25,36,76]. Lưu ý rằng

4

--- TRANG 5 ---
LLMCad : Suy luận Mô hình Ngôn ngữ Lớn trên Thiết bị Nhanh và Có thể Mở rộng Conference'17, July 2017, Washington, DC, USA

đề xuất của công trình này ở cấp độ hệ thống và tương thích với lượng tử hóa.

2.3 Cơ hội và Thách thức của Hợp tác LLM

Công trình này tập trung vào phương pháp hợp tác mô hình [ 40,60, 70], tận dụng nhiều mô hình với sự đánh đổi độ chính xác-chi phí để tăng tốc suy luận. Trong trường hợp các tác vụ NLP sinh, chúng tôi ủy thác hầu hết các tính toán (tức là token) cho một mô hình nhỏ hơn hoàn toàn phù hợp với ngân sách bộ nhớ. Lý do chính là các mô hình nhỏ hơn có thể thể hiện hiệu suất gần với mô hình lớn, đặc biệt đối với các điểm dữ liệu dễ hơn [ 24,29]. Các thí nghiệm thực nghiệm của chúng tôi đã xác nhận giả định như vậy: trong bộ dữ liệu dịch iwslt2017 de-en [ 19], mT5-Small tạo ra chính xác hơn 80% token như mT5-Large làm. Hình 4 đưa ra một ví dụ cụ thể về dịch một câu bởi mô hình nhỏ hơn và LLM lớn hơn, và chuẩn mực tương ứng. Nó cho thấy rằng hầu hết các token (màu xanh lá) được tạo bởi mô hình nhỏ là chính xác.

Tuy nhiên, việc sử dụng hợp tác mô hình cho LLM đối mặt với một thách thức quan trọng: Ủy thác token sai có thể là chết người. Tầng mô hình truyền thống dựa vào kiến thức nội bộ [ 60] hoặc bên ngoài [ 40,70] để chọn một phần dữ liệu (thường là những cái dễ hơn) được xử lý bởi mô hình nhỏ hơn. Trong hoàn cảnh như vậy, độ chính xác không thể được đảm bảo. Đối với các tác vụ NLP sinh, tuy nhiên, một token được tạo sai bởi mô hình nhỏ có thể lan truyền lỗi đến những token tiếp theo và cuối cùng gây ra kết quả thảm khốc do bản chất tự hồi quy của nó [ 53,55,65]. Ví dụ, trong Hình 4, "ice" thứ hai màu đỏ là không chính xác, dẫn đến hai lần tạo "ice" bổ sung và thông tin dịch sai trong các token tiếp theo. Lưu ý rằng các tác vụ NLP sinh thường nhạy cảm về độ chính xác, ví dụ, dịch thuật và Q/A, vì một kết quả được tạo sai có thể làm sai lệch người dùng và dẫn đến hành vi bất ngờ.

Để giải quyết vấn đề này, LLMCad sử dụng một hình thức hợp tác mô hình độc đáo, được gọi là "tạo-rồi-xác minh" [ 20,41]. Trong phương pháp này, LLM thường trú bộ nhớ đóng vai trò là bộ tạo token, trong khi LLM mục tiêu hoạt động như bộ xác minh, sử dụng đầu ra của nó làm chuẩn mực để kiểm tra và sửa chữa bất kỳ lỗi nào được giới thiệu trong quá trình tạo token. Bằng cách làm như vậy, LLMCad có thể ngăn chặn lan truyền lỗi của nó đến các token tiếp theo do bản chất tự hồi quy và đảm bảo không làm giảm độ chính xác.

3 THIẾT KẾ

3.1 Tổng quan

LLMCad được xây dựng trên hai LLM: một mô hình mục tiêu chính xác nhưng nặng (không thể vừa với bộ nhớ thiết bị) như mT5-large; và một mô hình thường trú bộ nhớ ít chính xác hơn nhưng nhẹ như mT5-small. Mục tiêu thiết kế của LLMCad là tạo văn bản với tốc độ của mô hình thường trú bộ nhớ mà không làm giảm độ chính xác của mô hình mục tiêu (lớn hơn).

Luồng công việc đơn giản hóa và một ví dụ minh họa Hình 5 minh họa luồng công việc của LLMCad . Hình 6 cũng cung cấp một ví dụ minh họa dựa trên trường hợp của Hình 4 để minh họa luồng công việc. Về cơ bản, LLMCad là một khung tạo và xác minh sử dụng LLM thường trú bộ nhớ làm bộ tạo, và LLM mục tiêu làm bộ xác minh.

Đầu tiên, LLMCad đưa văn bản đầu vào vào mô hình thường trú bộ nhớ và tạo ra một cây token . Một cây token là kết quả trung gian được tạo bởi mô hình thường trú bộ nhớ (chi tiết trong tạo cây §3.2). Không giống như một chuỗi token trong đó mỗi token chỉ có một token kế tiếp duy nhất, một token trong cây token có thể có nhiều token kế tiếp ứng cử viên, như được thể hiện trong Hình 6 1○. Mỗi token ứng cử viên đại diện cho một chuỗi token ứng cử viên (được gọi là một nhánh ). Điều này dựa trên quan sát rằng đôi khi các token "không tối ưu" được tạo bởi LLM thường trú bộ nhớ là đầu ra thực của LLM mục tiêu, ví dụ, token thay thế "cap". Trong thực tế, bất kỳ token ứng cử viên nào có độ tin cậy cao hơn ngưỡng (ví dụ, 30%) đều tạo ra một nhánh.

Mỗi token được tạo bởi LLM thường trú bộ nhớ giới thiệu một số "bất định" (điểm tin cậy không hoàn hảo). Một khi bất định như vậy tích lũy đến một mức trong câu đầu ra, LLM mục tiêu được sử dụng để xác minh tất cả các nhánh được tạo kể từ lần xác minh cuối cùng, như được thể hiện trong Hình 6 2○. Đáng chú ý, việc xác minh 𝑁 token có thể được thực hiện trong một lần suy luận với LLM mục tiêu, do đó nhanh hơn nhiều so với việc sử dụng nó để tạo một token bằng 𝑁 lần. quá trình xác minh như vậy do đó được gọi là "không tự hồi quy". Một khi phát hiện lỗi, LLMCad sẽ quay lại cây token và sửa chữa nó. Chi tiết về chiến lược xác minh và quay lại được thảo luận trong §3.2.

Quá trình xác minh bao gồm suy luận LLM mục tiêu, do đó bị giới hạn I/O như đã thể hiện trước đó trong §2.3. LLMCad đề xuất thêm Tạo suy đoán để khai thác các tài nguyên phần cứng chưa được sử dụng bằng cách tạo token một cách suy đoán (§3.4), tức là tiếp tục tạo token thông qua mô hình thường trú bộ nhớ trong quá trình xác minh, như được thể hiện trong Hình 6 3○ các hộp đứt nét ở phía bên trái của đường đứt nét đỏ. Phương pháp này dựa trên hiểu biết rằng đôi khi việc xác minh không phát hiện lỗi nên các token được tạo suy đoán có thể được sử dụng sau đó. Do đó, nó có thể ẩn hiệu quả độ trễ thực thi dưới I/O, ví dụ, nhánh thứ hai trong Hình 6 3○.

LLMCad lặp lại quá trình tạo và xác minh trên cho đến khi gặp "<EOS>", token kết thúc.

5

--- TRANG 6 ---
Conference'17, July 2017, Washington, DC, USA Daliang Xu♦, Wangsong Yin♦, Xin Jin♦, Ying Zhang♦, Shiyun Wei⋆, Mengwei Xu^, Xuanzhe Liu♦

Last year , I showed these two slides to illustrate that the Arctic  ice ice ice ice,  which has shrinked for nearly three million years, is 40 percent
Last year , I showed these two slides to illustrate that the Arctic ice cap, which for about three million years has shrunk by 40 percent.
Last year I showed these two slides to illustrate that the arctic ice cap, which for most of the last three million years  has shrunk by 40 percent.Nhỏ hơn:
Lớn hơn:
Nhãn:... that the Arctic  ice cap, which has shrinked for ...Token tin cậy cao thứ 2

Hình 4: Một ví dụ tạo dịch từ các mô hình mt5-small và mt5-large cũng như nhãn chuẩn mực của nó. Xanh lá: các phần chính xác của việc tạo mô hình nhỏ; Đỏ: các phần lan truyền lỗi của việc tạo mô hình nhỏ; Xanh dương: token không tối ưu là câu trả lời chính xác. Đáng chú ý, trên bộ dữ liệu dịch iwslt2017 de-en [ 19], mô hình mT5-small tạo ra chính xác gần 69,3% token, trong khi con số cho mô hình mT5-large là 73,1%.

Cây Token Suy đoánXác minh
không tự hồi quyKết quả Xác minh
Tạo câyDự phòng
Chuỗi đã xác minh
Bộ giải mã cây
Bộ điều khiển tạo nhánh
dựa trên độ tin cậy
Tạo câyToken
cây Kế hoạch thực thi
suy đoán
Hit/Miss?Quay lại?
Đầu ra<EOS>?
Translation the following de text to en.
Jetzt muss ich meine Schuhe ausziehen,
um überhaupt an Bord zu kommen!Đầu vào
Mô hình nhỏMô hình lớn
Tự hồi quy

Hình 5: Luồng công việc của LLMCad .

3.2 Tạo và xác minh cây token

Phần này chủ yếu thảo luận về cách cây token được tạo và xác minh trong LLMCad .

Tạo cây token Để tạo ra các cây token hữu ích, LLMCad cần trả lời hai câu hỏi quan trọng:
•Các nhánh cạnh tranh tài nguyên tính toán (ví dụ, GPU) để tạo ra các token tiếp theo bằng cách chạy mô hình thường trú bộ nhớ. Tại một thời điểm, nhánh nào sẽ nhận tài nguyên để kéo dài chuỗi token của nó? Quyết định này rất quan trọng vì việc tạo token cho một nhánh sai (như được xác minh bởi mô hình mục tiêu sau) lãng phí tài nguyên tính toán và trì hoãn việc tạo token đúng.

•Tạo token từ các nhánh khác nhau yêu cầu chuyển đổi giữa các ngữ cảnh nhánh. Làm thế nào để tạo token từ các nhánh khác nhau một cách hiệu quả? Thiết kế này rất quan trọng vì LLMCad cần thường xuyên chuyển đổi giữa hàng chục nhánh.

Để đáp ứng, LLMCad kết hợp hai kỹ thuật mới:
•Bộ điều tốc nhánh dựa trên độ tin cậy. Để điều chỉnh đúng tiến trình của các nhánh khác nhau, LLMCad dựa vào thực tế rằng nhánh có xác suất cao hơn có nhiều khả năng là kết quả chính xác và nên có độ dài chuỗi dài hơn. Ở đây, LLMCad mô hình hóa xác suất với điểm tin cậy tích lũy được đưa ra bởi thường trú bộ nhớ cho mỗi token được tạo. Để kiểm soát độ dài nhánh một cách động, LLMCad sử dụng công bằng max-min [28].

Giả sử có 𝑁 nhánh và 𝑀 token, và nhánh thứ i bao gồm 𝑇𝐵
𝑖 token, độ tin cậy tích lũy nhánh thứ i 𝐶𝑖 là tích của độ tin cậy của mỗi token. Do đó, vấn đề độ dài nhánh có thể được thực hiện bằng cách giải quyết vấn đề sau.

𝑓(𝑥)=𝑀∗𝐶𝑥Í𝑁
𝑖=0𝐶𝑖−𝑇𝐵
𝑥 (1)
𝑂𝑏𝑗=𝑚𝑖𝑛𝑁
𝑥=0𝑓(𝑥) (2)

Dưới công bằng max-min, LLMCad cố gắng phân bổ nhiều tài nguyên phần cứng hơn cho nhánh có nhiều khả năng là chuẩn mực.

•Bộ giải mã cây. Chúng tôi bắt đầu nghiên cứu bằng cách tiến hành phân tích toàn diện về các lý do cơ bản đằng sau chi phí hiệu suất đáng kể phát sinh do chuyển đổi ngữ cảnh nhánh (ví dụ, 25% chi phí cho các mô hình mT5 trên Jetson TX2). Trong Hình 7(a), chúng tôi cung cấp một minh họa về việc triển khai chuyển đổi ngữ cảnh nhánh trong các công cụ LLM tiên tiến, như PyTorch, sử dụng kịch bản được mô tả trong Hình 6 1○ làm nghiên cứu trường hợp. Trong minh họa này, các lần lặp 1–4 lấy token đầu ra trước đó làm đầu vào mới. Tuy nhiên, việc tạo token "7" trong lần lặp 5 đòi hỏi chuyển đổi nhánh từ b1 sang b2, liên quan đến việc loại bỏ token 𝑇4, bỏ qua token mới 𝑇6, và sử dụng đầu ra không tối ưu 𝑇5 từ lần lặp 3 làm đầu vào. Do đó, LLMCad phải lệch khỏi quy tắc tự hồi quy và sửa đổi mỗi đầu vào lần lặp với một lượng lớn siêu dữ liệu (ví dụ, bộ nhớ cache Key-Value [ 7,36,78] và id vị trí [65]) duy trì các hoạt động và tương tác CPU-GPU.

Để giải quyết vấn đề này, LLMCad kết hợp kỹ thuật che giấu [ 65]. Kỹ thuật này được sử dụng để đảm bảo rằng các dự đoán cho vị trí 𝑖 chỉ phụ thuộc vào các đầu ra đã biết ở các vị trí nhỏ hơn 𝑖 trong các LLM dựa trên decoder. Kỹ thuật che giấu dựa vào một bảng trong đó tất cả các vị trí có giá trị một sẽ được tính đến cho tính toán.

Quan trọng, bộ giải mã cây giữ lại quy trình tự hồi quy trong khi chỉ sửa đổi bảng che giấu của nó để hỗ trợ việc cô lập các hiệu ứng từ các nhánh khác nhau, như được thể hiện trong Hình 7(b). Trong mỗi lần lặp, LLMCad coi token mới được tạo là đầu vào, giống như trong việc tạo thông thường. Tuy nhiên, nó chỉ gán giá trị một cho các vị trí trước đó trên cùng một nhánh. Ví dụ, khi tạo token "7" cho nhánh b2 trong lần lặp 5, đầu vào vẫn là 𝑇6, đầu ra của lần lặp 4, nhưng chỉ các vị trí "1, 2, 3, và 5" được đặt thành một; tất cả các vị trí khác được đặt thành không. Phương pháp này đảm bảo

6

--- TRANG 7 ---
LLMCad : Suy luận Mô hình Ngôn ngữ Lớn trên Thiết bị Nhanh và Có thể Mở rộng Conference'17, July 2017, Washington, DC, USA

① Cây Token
T1: the T2: ArcticT4: ice
T5: capT3: iceice
capice
, which has
cap , which for shrinked② Xác minh Không tự hồi quy
Cần quay lạiwhich forabout three millon years

three millon years

Xác minh
for threeReuse③ Tạo
Suy đoánT6: ice
T7: ,iceb1
b2

Chuẩn mực:   the Arctic ice cap, which for  aboutĐiểm tin cậy = 0.6
Điểm tin cậy = 0.3

Đầu ra gốcĐầu ra đã xác minh
cap ,which has iceỞt lặp
Tất cả đầu ra xác minhLLM Mục tiêu

iceĐầu vào cây token
Đầu ra đã xác minh
 dự phòngLLM Thường trú Bộ nhớ
LLM
Mục tiêu

Hình 6: Một ví dụ minh họa của LLMCad .

Token đầu vào Đầu ra
T 1 T 2
T1T2 T3
T1T2T3T4T5
T1T2T3T4T6
T1T2T3T5T7Iter1:
Iter2:
Iter3:
Iter4:
Iter5:tự hồi quy
chuyển đổi ngữ cảnh
nhánh

(a) Quy trình tạo cây hiện có

Token đầu vào
T 1 T 2
T1T2 T3
T1T2T3T 4 T 5
T1T2T3T4T6
T1T2T3T5T7Iter1:
Iter2:
Iter3:
Iter4:
Iter5:T5
T6T4tự hồi quy
tự hồi quy
... ...T 1
T1
T1T2
T1T2T3
T1T2T3T5T2
T3
T41 0 0 0 0 0
1 1 0 0 0 0
1 1 1 0 0 0
1 1 1 1 0 0
1 1 1 0 1 0Bảng che giấu Token trong tính toán Đầu ra (b) Quy trình bộ giải mã cây

Hình 7: Ví dụ về quy trình tạo cây trong các công cụ LLM tiên tiến và quy trình bộ giải mã cây của LLMCad dựa trên trường hợp của Hình 6 1○.

T1T2
T5
T3T4
T6Cây Token
T8T7
T1 T2 T5
Chuỗi đã xác minhT'
7T1T2
T3T4
T6T8T7 T1T2T5
T1Chuỗi nhánh
T 1 T 2
T 3T 5
T6T 1 T 2
T 1T 1 T 2
T 5 T 1 T 2
T'7T1T2T5
T1T2
T3T 8 T1T6Kết quả
Nhánh1
Kết quả
Nhánh2
Kết quả
Nhánh3LLM
Mục tiêuhàng loạt nhỏ
T1T5
T2 T5 T'7T2T 2
T 4
T7T 2
T 5
T 3
T6
T 8Đầu ra Đã xác minh Đầu ra Gốc
Cây token chính xácT2① Xác minh không tự hồi quy theo hàng loạt
② Tìm kiếm chuỗi đã xác minh theo chiều sâu trước

kích thước
hàng loạt=3T1T5T2
T1T2T'7T1T2T5Chuỗi chính xác Nhánh1
Chuỗi chính xác Nhánh2
Chuỗi chính xác Nhánh3

Hình 8: Minh họa xác minh cây token.

rằng các token "4 và 6" không ảnh hưởng đến tính toán, cho phép LLMCad tạo token "7" mà không bị ảnh hưởng.

Xác minh và sửa chữa cây token Để đạt được mục tiêu không hy sinh độ chính xác, LLMCad phải xác minh mỗi token được tạo bởi LLM thường trú bộ nhớ. Một phương pháp trực quan là sử dụng LLM mục tiêu để chạy xác minh sau mỗi lần tạo token bởi LLM thường trú bộ nhớ. Tuy nhiên, phương pháp như vậy (được gọi là xác minh tự hồi quy (AV) ) thậm chí còn chậm hơn việc tạo mỗi token bởi mô hình mục tiêu trực tiếp, vì AV không giảm số lần suy luận LLM mục tiêu mà thậm chí còn sử dụng LLM thường trú bộ nhớ.

Để giải quyết vấn đề này, LLMCad dựa trên hai cơ hội: (1) LLM mục tiêu có thể kiểm tra một chuỗi token

2.5 5.0 7.5 10.0
mT5-Large trên TX2255075Độ trễ (s)
9.9xxác minh không tự hồi quy xác minh tự hồi quy

2.5 5.0 7.5 10.0
LLaMa-13B trên Xiaomi 1050100
8.5x

Hình 9: Độ trễ xác minh với chuỗi đầu vào tăng.

song song bằng cách truy cập tham số của nó chỉ một lần (được gọi là xác minh không tự hồi quy (NAV) ) và kết quả xác minh giống như kiểm tra chúng tuần tự [ 20,41]. (2) Quá trình NAV nhanh hơn AV rất nhiều. Các thí nghiệm thí điểm của chúng tôi trong Hình 9 trên các mô hình LLaMa-13B và mT5-Large sử dụng Xiaomi10 và TX2 cho thấy NAV vượt trội hơn AV trong

7

--- TRANG 8 ---
Conference'17, July 2017, Washington, DC, USA Daliang Xu♦, Wangsong Yin♦, Xin Jin♦, Ying Zhang♦, Shiyun Wei⋆, Mengwei Xu^, Xuanzhe Liu♦

thời gian kiểm tra qua 2–10 token đầu vào, với lợi ích của nó nổi bật hơn khi số lượng token tăng. NAV giảm đáng kể thời gian xác minh cho các mô hình mT5-Large và LLaMa-13B 8,5–9,9 × ở 10 token, do NAV chỉ hoán đổi trọng số một lần so với nhiều lần hoán đổi của AV mỗi token, giảm chi phí I/O.

Tóm lại, NAV có thể xác minh song song nhiều token một cách chính xác với chi phí chỉ một lần suy luận LLM mục tiêu. LLMCad kết hợp NAV và mở rộng nó để hỗ trợ xác minh cây token, như được thể hiện trong Hình 8, bao gồm hai bước quan trọng:

•Xác minh không tự hồi quy theo hàng loạt. Như được thể hiện trong Hình 8 1○, để hỗ trợ xác minh cây token, LLMCad đầu tiên chia cây token thành nhiều chuỗi nhánh và kết hợp chúng thành một hàng loạt nhỏ làm đầu vào của LLM mục tiêu. Sau NAV, LLMCad có thể thu được kết quả chính xác của mỗi vị trí trong mỗi chuỗi nhánh. So với các chuỗi nhánh gốc, LLMCad có thể phát hiện tất cả lỗi của một cây token, ví dụ, 𝑇′
7 trong kết quả nhánh2. Một chuỗi chính xác nhánh là chuỗi con dẫn đến vị trí lỗi đầu tiên, cộng với token được sửa chữa, để tránh lan truyền lỗi. Ví dụ, chuỗi chính xác nhánh1 dừng ở 𝑇2, cộng với 𝑇5, tức là các token "1, 2 và 5". Cần lưu ý, cho rằng NAV bị giới hạn I/O, việc tăng kích thước hàng loạt (ví dụ, <10) có tác động không đáng kể đến thời gian xác minh.

•Tìm kiếm chuỗi đã xác minh theo chiều sâu trước. Dựa trên các chuỗi chính xác, LLMCad có thể xây dựng một cây token chính xác, như được thể hiện trong Hình 8 2○. Nút lá của nó là token được sửa chữa đầu tiên hoặc token cuối cùng của chuỗi nhánh gốc. LLMCad tận dụng thuật toán tìm kiếm theo chiều sâu trước để tìm đường dẫn chính xác dài nhất trong cây token chính xác làm chuỗi đã xác minh .

Nếu chuỗi đã xác minh có một token được sửa chữa, ví dụ, 𝑇′
7, LLMCad sẽ quay lại cây token đến vị trí lỗi, sửa lỗi, và sử dụng nó làm đầu vào mới cho việc tạo tương lai.

3.3 Chiến lược dự phòng tự thích ứng

Chiến lược này được thiết kế để khởi động quá trình xác minh kịp thời khi LLM thường trú bộ nhớ tạo ra một token không chính xác. Để đạt được mục tiêu này, LLMCad cần trả lời hai câu hỏi quan trọng:

•Lựa chọn Chỉ số Quyết định. Chỉ số quyết định nên đánh giá hiệu quả xác suất lỗi trong cây token.

•Giá trị Ngưỡng cho Các Tác vụ Khác nhau. Nhận ra rằng một ngưỡng phổ quát có thể không phù hợp cho tất cả các tác vụ, LLMCad phải thiết lập các giá trị ngưỡng phù hợp được điều chỉnh theo các tác vụ cụ thể.

Để giải quyết những vấn đề này, LLMCad giới thiệu hai kỹ thuật sáng tạo:

•Độ tin cậy tích lũy cây ( 𝑇𝑐). Chúng tôi đề xuất sử dụng độ tin cậy tích lũy cây làm biến quyết định để khởi động dự phòng. Không giống như các nghiên cứu trước [ 37,41] dựa vào độ tin cậy token đơn hoặc độ dài chuỗi token, 𝑇𝑐 cung cấp đánh giá toàn diện về bất định toàn cục. Nó nắm bắt lỗi một cách chính xác hơn do bản chất tự hồi quy của việc tạo token.

Công thức độ tin cậy tích lũy cây là 𝑇𝑐=
𝑚𝑎𝑥𝑁𝑐
𝑖=1𝐶𝑖, trong đó 𝑁𝑐 đại diện cho số lượng nhánh trong một cây token, và 𝐶𝑖 biểu thị độ tin cậy tích lũy của nhánh thứ 𝑖. Chúng tôi chọn độ tin cậy tích lũy tối đa so với độ tin cậy tối thiểu/trung bình vì nhánh có độ tin cậy nhất có nhiều khả năng mang lại kết quả chính xác sau xác minh, và quá trình xác minh chỉ có thể xác định lỗi khi nhánh có độ tin cậy nhất sai.

•ngưỡng tự thích ứng ( 𝛼) được sử dụng để xác định khi nào LLM mục tiêu sẽ xác minh. Nó hoạt động trên nguyên tắc rằng LLM thường trú bộ nhớ, tạo ra đầu ra gần giống với LLM mục tiêu, nên được tin tưởng hơn, tức là tần suất xác minh thấp hơn bằng cách đặt ngưỡng thấp hơn. Để đánh giá sự tương tự đầu ra, LLMCad dựa vào dữ liệu lịch sử về độ chính xác của các token đã xác minh.

Người dùng có thể chọn giá trị 𝛼 ban đầu hoặc sử dụng giá trị mặc định (0,01) được cung cấp bởi hệ thống. Sau khi xác minh, LLMCad cập nhật ngưỡng tự thích ứng ( 𝛼) sử dụng quy tắc sau:

𝛼𝑖+1=(𝛼𝑖∗0.5��𝑓 𝑁𝑐𝑜𝑟𝑟𝑒𝑐𝑡 ==𝑁𝑎𝑙𝑙
𝛼𝑖/𝑇𝑁𝑎𝑙𝑙−𝑁𝑐𝑜𝑟𝑟𝑒𝑐𝑡
𝑁𝑎𝑙𝑙𝑐 𝑖𝑓 𝑁𝑐𝑜𝑟𝑟𝑒𝑐𝑡 <𝑁𝑎𝑙𝑙(3)

trong đó 𝑁𝑐𝑜𝑟𝑟𝑒𝑐𝑡 và 𝑁𝑎𝑙𝑙 là số lượng tổng token và token chính xác trong nhánh khớp nhất trong một lần xác minh. Cụ thể, khi quá trình xác minh không phát hiện lỗi, LLMCad giảm 𝛼 bằng cách nhân giá trị hiện tại với 0,5, độ tin cậy tích lũy của 3-5 token trong các quan sát thực nghiệm. Ngược lại, nếu xác minh xác định lỗi, ngưỡng được tăng bằng cách chia 𝛼 cho độ tin cậy tích lũy trung bình của tất cả các token tiếp theo sau token được tạo không chính xác. Lý do đằng sau việc sử dụng hàm mũ là độ tin cậy tích lũy cây là tích của độ tin cậy của mỗi token, tích lũy theo cấp số nhân.

Tóm lại, sau mỗi lần tạo token bởi LLM thường trú bộ nhớ, LLMCad tính toán 𝑇𝑐. Nếu 𝑇𝑐 rơi xuống dưới 𝛼, một dự phòng xảy ra, và mô hình mục tiêu bắt đầu xác minh. Sau khi xác minh, 𝛼 được cập nhật dựa trên lịch sử độ chính xác tạo mới nhất.

3.4 Đường ống Tạo Suy đoán

Như được tự tại trong §2.3, việc sử dụng GPU trải qua các đợt tăng theo chu kỳ do thực tế là các công cụ LLM SOTA sử dụng kỹ thuật hoán đổi. Để thu hoạch các chu kỳ miễn phí, LLMCad đề xuất kỹ thuật tạo suy đoán bằng cách cho phép LLM thường trú bộ nhớ tiếp tục tạo token trong quá trình xác minh. Phương pháp này dựa trên

8

--- TRANG 9 ---
LLMCad : Suy luận Mô hình Ngôn ngữ Lớn trên Thiết bị Nhanh và Có thể Mở rộng Conference'17, July 2017, Washington, DC, USA

mT5-Large GPT2-Large
Tải024Độ trễ (s)Một mình
Song song

mT5-Large GPT2-Large
Tính toán0.00.1Một mình
Song song

Hình 10: Thời gian tải và tính toán của việc thực thi mô hình mục tiêu với song song hóa mô hình thường trú bộ nhớ.

LLM Mục tiêu
LLM Thường trú
Bộ nhớC C C CC L UL
SC SC ...SCC
Dự phòngGiới hạn trên
bộ nhớ 
Suy đoán Kết thúc xác minh L ...
SC SC SCC
C ...LGiới hạn trên
bộ nhớ 
Suy đoánGiới hạn trên
bộ nhớ 
UL UL
... ...
UL L C SC Tính toánTính toán
suy đoánTải tham số dưới
giới hạn trên bộ nhớTải tham số trên
giới hạn trên bộ nhớ

Hình 11: Tạo suy đoán của LLMCad .

hiểu biết rằng đôi khi quá trình xác minh không phát hiện lỗi nên các token được tạo suy đoán có thể được sử dụng sau đó.

Tác động của tạo suy đoán đến LLM mục tiêu. Các thí nghiệm sơ bộ của chúng tôi trên TX2 sử dụng các mô hình mT5-Large và GPT2-Large cho thấy việc song song hóa việc thực thi LLM thường trú bộ nhớ và mục tiêu tăng thời gian tính toán và tải LLM mục tiêu 2,2–2,3 × và 1,05–1,09×, tương ứng. Độ trễ tính toán được gây ra bởi tranh chấp lõi GPU, trong khi độ trễ tải là bất ngờ vì tranh chấp bộ nhớ. Cụ thể, LLM thường trú bộ nhớ liên tục phân bổ các vùng bộ nhớ cho các token được tạo suy đoán, trong khi LLM mục tiêu phân bổ động các vùng bộ nhớ để tải tham số. Thông thường, tác động không đáng kể được tác động lên nhau trừ khi việc sử dụng bộ nhớ vượt quá 90%. Tuy nhiên, việc sử dụng bộ nhớ vượt quá 90% hoặc thậm chí đạt 95% là phổ biến trong kịch bản tạo suy đoán, do thực tế là các công cụ LLM tiên tiến hiện tại được thiết kế để tải càng nhiều tham số càng tốt từ đĩa vào bộ nhớ để giảm thời gian suy luận.

Đường ống tính toán-tải. Để giải quyết vấn đề trên, LLMCad lập kế hoạch thực thi song song một cách tinh tế, như được thể hiện trong Hình 11. Nguyên tắc chính là quá trình xác minh bình thường không thể bị ảnh hưởng bởi việc thực thi suy đoán. Do đó, có một giới hạn trên bộ nhớ bằng cách lược tả hoặc người dùng xác định để tránh tranh chấp bộ nhớ hai LLM, và hai LLM tính toán không thể được thực thi song song.

Sau khi đưa vào chuỗi đầu vào, các tham số của cả LLM thường trú bộ nhớ và mục tiêu được tải vào bộ nhớ. Một khi việc tải LLM thường trú bộ nhớ kết thúc, nó bắt đầu tạo token, và việc tải tham số LLM mục tiêu (màu vàng) sẽ dừng trước khi giới hạn trên bộ nhớ bị vượt quá để tránh ảnh hưởng đến việc tạo LLM thường trú bộ nhớ bình thường. Khi điều kiện dự phòng được đáp ứng,

Nền tảng Bộ xử lý Phần mềm Bộ nhớ
Jetson TX24x Cortex-A57
Maxwell 128 lõi CUDATorch-1.10
Ubuntu 18.048G
Jetson
Orin NXAmpere 1024 lõi CUDA
+ 32 lõi TensorTorch-2.0
Ubuntu 18.048G
Xiaomi 101x 2.84GHz A77+3x 2.4GHz Cortex A77
+4x 1.8GHz Cortex A55Android 10
llama.cpp8G
Xiaomi
111x 3.0 GHz X2+ 3x 2.5 GHz Cortex A710
+ 1.8GHz 4x Cortex-A510Android 10
llama.cpp8G

Bảng 2: Các nền tảng được sử dụng trong thí nghiệm.

Thiết bị Tác vụLLM Thường trú
Bộ nhớLLM
Mục tiêuKhoảng cách
Tốc độBộ dữ liệu
Jetson TX2
Jetson
Orin NXTmT5-small (0.3B) mT5-Large (1.2B) 230x IWLST17-de-en [19]
Bart-base Bart-Large WMT14-de-en [15]
QAmT5-small (0.3B) mT5-Large (1.2B) 230x SQuAD_v2 [56]
T5-small (0.06B) T5-large (0.73B) 263x SQuAD_v2
LM GPT2 (0.14B) GPT2-Large (0.8) 214x Wikitext [47]
S T5-small (0.06B) T5-large (0.73B) 263x CNN/Daily [61]
Xiaomi 10
Xiaomi
ProTVicuna-7B
(INT4)Vicuna-13B
(INT4)59xParrot
WMT22-de-en
WMT22-zh-en
QA LLaMa2-Chat-
7B (INT4)LLaMa2-Chat-
13B (INT4)59xSQuAD
TruthfulQA [44]
S CNN/Daily
T, S, QA, và LM đại diện cho các tác vụ sinh của dịch thuật, tóm tắt,
trả lời câu hỏi, và mô hình hóa ngôn ngữ.

Bảng 3: Các tác vụ, mô hình, bộ dữ liệu, và thiết bị được thử nghiệm tương ứng được sử dụng trong thí nghiệm.

phần còn lại của tham số cho mô hình mục tiêu (màu cam) sẽ được tải vào bộ nhớ, và sau đó việc tính toán của LLM mục tiêu bắt đầu. Việc thực thi suy đoán (màu xanh) sẽ không chạy trừ khi quá trình xác minh đang tải tham số dưới ngân sách bộ nhớ (màu vàng), tránh tranh chấp bộ xử lý và bộ nhớ.

4 ĐÁNH GIÁ

4.1 Triển khai và Thiết lập

Chúng tôi đã triển khai đầy đủ LLMCad với 4,5k SLoC (Python: 3.500 và C/C++: 1.000). Nguyên mẫu là một khung độc lập hỗ trợ LLM được xuất từ TensorFlow [ 8] và PyTorch [ 5]. LLMCad tận dụng llama.cpp [ 45] (một trong những công cụ LLM trên thiết bị nhẹ nhất) làm backend điện thoại thông minh và PyTorch [5] làm backend thiết bị IoT.

Thiết lập phần cứng. Chúng tôi thử nghiệm hiệu suất của LLMCad trên bốn thiết bị: 2 điện thoại thông minh (Xiaomi 10 và Xiaomi 12) và 2 thiết bị IoT (Jetson TX2, và Jetson Orin), như được tóm tắt trong Bảng 2. Chúng tôi chạy LLM trên GPU Jetson và CPU điện thoại thông minh, vì các công cụ LLM hiện tại [refs] có hỗ trợ chưa trưởng thành cho GPU/NPU điện thoại thông minh. Tuy nhiên, thiết kế của LLMCad trực giao với các loại phần cứng.

Mô hình và bộ dữ liệu. Chúng tôi thử nghiệm với một loạt các mô hình LLM điển hình với nhiều bộ dữ liệu tác vụ sinh khác nhau trên các thiết bị khác nhau, như được tóm tắt trong Bảng 3. Trên các thiết bị IoT, chúng tôi đánh giá LLMCad trên hai tác vụ dịch thuật, hai tác vụ trả lời câu hỏi, một tác vụ mô hình hóa ngôn ngữ, và một tác vụ tóm tắt với các mô hình mT5, T5, GPT2, và Bart. Tất cả các mô hình được chúng tôi tinh chỉnh như [ 37] làm. Đối với thiết bị điện thoại thông minh, chúng tôi sử dụng các mô hình Vicuna-1.5 và LLaMA2

9

--- TRANG 10 ---
Conference'17, July 2017, Washington, DC, USA Daliang Xu♦, Wangsong Yin♦, Xin Jin♦, Ying Zhang♦, Shiyun Wei⋆, Mengwei Xu^, Xuanzhe Liu♦

với ba tác vụ dịch thuật, một tác vụ trả lời câu hỏi và, và một tóm tắt. Tất cả các mô hình được tải xuống từ kho hugging face [ 9] và đã được lượng tử hóa bởi AutoGPTQ [ 10] thành định dạng 4-bit để tiết kiệm bộ nhớ và cải thiện tốc độ suy luận.

Đường cơ sở. Chúng tôi chủ yếu so sánh LLMCad với 5 đường cơ sở tiên tiến có thể được chia thành hai loại:
•3x đường cơ sở LLM đơn. (1) Tiêu chuẩn (Std) luôn sử dụng LLM mục tiêu để tạo đầu ra, với PyTorch cho IoT và llama.cpp cho điện thoại thông minh. (2) Đường ống tiêu chuẩn (StdPL) : Nó thực thi một đường ống theo lớp, chồng chéo I/O và tính toán, như được sử dụng bởi các công cụ suy luận LLM SOTA hiện tại. (3) Suy luận Transformer Nhanh (STI) [27] : Một khung suy luận NLP edge với mảnh tham số lượng tử hóa và đường ống tính toán-tải tinh vi.
•2x đường cơ sở hợp tác LLM. (1) Giải mã Suy đoán (SP) [37]: Một khung tiên tiến cũng sử dụng hợp tác LLM "bộ tạo và bộ xác minh". (2) Bộ Giải mã Transformer Lớn Nhỏ (BLD) [37]: Một thuật toán xác định thời điểm xác minh và cơ chế quay lại cho hợp tác LLM "bộ tạo và bộ xác minh".

Chỉ số và cấu hình. Chúng tôi chủ yếu báo cáo độ chính xác tạo và thời gian tạo mỗi token. Để rõ ràng, mục tiêu của LLMCad là căn chỉnh đầu ra LLM thường trú bộ nhớ với LLM mục tiêu. Do đó, chúng tôi coi văn bản được tạo bởi LLM mục tiêu là chuẩn mực và tính điểm Rouge-L [ 43], một sự tương tự giữa hai chuỗi dựa trên chuỗi con chung dài nhất, làm độ chính xác tạo .

4.2 Tốc độ Tạo

Hiệu suất tổng thể. Chúng tôi đầu tiên điều tra toàn diện hiệu suất tạo của LLMCad trên bốn thiết bị được thử nghiệm. Kết quả độ chính xác tạo và thời gian tạo mỗi token được minh họa trong Bảng 4, Hình 12 và Hình 13, tương ứng. Quan sát chính của chúng tôi là LLMCad nhất quán và đáng kể vượt trội hơn các đường cơ sở khác về thời gian tạo mỗi token mà không làm giảm độ chính xác trên tất cả các thiết bị được thử nghiệm.

•Thời gian tạo của LLMCad so với đường cơ sở LLM đơn. So với Std, LLMCad đạt được tăng tốc 2,9-9,3× và 3,47–4,67× trong thời gian tạo trung bình mỗi token trên thiết bị IoT và điện thoại thông minh, tương ứng, mà không làm giảm độ chính xác. Cụ thể, LLMCad có thể tạo kết quả trả lời câu hỏi trên Xiaomi 11 với tốc độ nhanh nhất là 0,86 s/token. Thành tựu này cho phép tạo token thời gian thực với LLM hơn 10B trên thiết bị COTS lần đầu tiên. Điều này được gây ra bởi thực tế là LLMCad có thể ủy thác hầu hết việc tạo token cho LLM thường trú bộ nhớ và đảm bảo tính chính xác bằng xác minh không tự hồi quy.

Khi so sánh với các đường cơ sở cạnh tranh hơn như StdPL và STI, LLMCad giảm thời gian tạo trung bình mỗi token 2,9-9,3× và 1,83–2,45×, tương ứng. Những lợi ích đó được gây ra bởi thực tế việc sử dụng LLM thường trú bộ nhớ để tạo văn bản nhất quán vượt trội hơn bất kỳ phương pháp đường ống hoặc lượng tử hóa nào của LLM mục tiêu trên thiết bị di động, trong đó LLM thường trú bộ nhớ có thể mang lại cải thiện tốc độ hơn hàng trăm lần so với LLM mục tiêu. Bên cạnh đó, nó cũng có thể cải thiện độ chính xác tạo 11,1–19,0 điểm phần trăm, so với STI. Điều này có lợi từ xác minh cây không tự hồi quy của chúng tôi có thể kiểm tra và sửa chữa tất cả lỗi bởi LLM thường trú bộ nhớ một cách hiệu quả.

•Thời gian tạo của LLMCad so với đường cơ sở hợp tác LLM. So với BLD, LLMCad có thể đạt được cải thiện độ chính xác tạo 4,5–94,5 và 9,8–96,7 điểm phần trăm với tăng tốc 1,1-1,4 × và 1,1–1,3× trong thời gian tạo trung bình mỗi token trên thiết bị IoT và điện thoại thông minh, tương ứng. Đó là bởi vì, không giống như BLD tăng tốc quá trình tạo bằng cách giảm số lần sửa chữa (hy sinh độ chính xác), chiến lược dự phòng tự thích ứng của chúng tôi nhằm giảm thiểu số lần xác minh trong khi đảm bảo xác minh cho mỗi token. Phương pháp như vậy tăng cường tốc độ tạo mà không hy sinh độ chính xác. Hơn nữa, thực thi suy đoán cho phép LLM thường trú bộ nhớ tạo văn bản sớm hơn mà không chờ kết quả xác minh khi không có lỗi được phát hiện bởi quá trình xác minh, giảm thêm độ trễ tạo.

Tương tự, LLMCad có thể giảm thời gian tạo trung bình mỗi token 1,93–2,00 × và 1,34–1,77× trên thiết bị IoT và điện thoại thông minh, tương ứng. Đó là bởi vì, không giống như SP sử dụng độ dài chuỗi token, chiến lược dự phòng tự thích ứng của chúng tôi có thể tìm chính xác khi LLM thường trú bộ nhớ tạo lỗi và có thể giảm tần suất xác minh.

4.3 Phân tích Độ nhạy Bộ nhớ

Phần này để điều tra tác động của các ngân sách bộ nhớ khác nhau đối với phương pháp của chúng tôi. Chúng tôi tiến hành thêm thí nghiệm trên các mô hình mT5 và T5 trên TX2 và LLaMa2 trên Xiaomi 10 tương ứng dưới các ngân sách bộ nhớ khác nhau (ví dụ, từ 4GB đến 8GB trên Xiaomi 10). Tăng tốc của các đường cơ sở khác nhau được thể hiện trong Hình 14. LLMCad nhất quán thể hiện tăng tốc cao nhất trong tất cả các đường cơ sở từ 8GB đến 4GB và lợi ích của nó nổi bật hơn với ngân sách bộ nhớ giảm.

LLMCad giảm thời gian tạo dưới ngân sách bộ nhớ 6GB trên Jetson TX2 5,54 ×, 1,76× và 1,12× trung bình cho StdPL , SP và BLD, tương ứng; trong khi tăng tốc cho ngân sách bộ nhớ 4GB là 6,12 ×, 1,91× và 1,25×, tương ứng, lớn hơn 1,29 ×, 1,08× và 2,1× so với dưới 6GB trung bình. Tương tự, LLMCad đạt được tăng tốc thời gian tạo dưới ngân sách bộ nhớ 4GB trên xiaomi 10 4,72 × và 1,34× trung bình cho StdPL và BLD, tương ứng. Đó là bởi vì khi ngân sách bộ nhớ nghiêm ngặt hơn, khoảng cách tốc độ suy luận

10

--- TRANG 11 ---
LLMCad : Suy luận Mô hình Ngôn ngữ Lớn trên Thiết bị Nhanh và Có thể Mở rộng Conference'17, July 2017, Washington, DC, USA

Mô hình Bộ dữ liệu StdPL SP BLD Của chúng tôi
mT5-LargeT: IWLST17-de-en 100 100 96.1 100
QA: SQuAD 100 100 52.9 100
T5-LargeS: CNN/Daily 100 100 5.5 100
QA: SQuAD 100 100 51.4 100
Bart-Large T: WMT14-de-en 100 100 96.5 100
GPT2-Large LM: Wikitext 100 100 12.9 100

(a) Thiết bị IoTMô hình Bộ dữ liệu StdPL STI SP BLD Của chúng tôi
Vicuna-13B
(INT4)T: Parrot 100 86.7 100 89.7 100
T: WMT22-de-en 100 87.2 100 90.2 100
T: WMT22-zh-en 100 88.1 100 80.4 100
LLaMa2-Chat
(INT4)S: CNN/Daily 100 81.0 100 7.96 100
QA: SQuAD 100 83.2 100 3.4 100
QA: Truthful_QA 100 85.4 100 4.7 100

(b) Điện thoại thông minh

Bảng 4: Tóm tắt độ chính xác tạo của LLMCad và đường cơ sở trên các thiết bị được thử nghiệm. T:*, S:*, QA:*, và LM:* đại diện cho các tác vụ sinh của dịch thuật, tóm tắt, trả lời câu hỏi, và mô hình hóa ngôn ngữ.

mt5_iwlst17_de_en  0  2  4  6Độ trễ trên Orin (s)7.20 7.06
2.31
1.70
1.26Vanilla StdPL SP BLD Của chúng tôi

mt5_squad_v2  0  2  4  67.20 7.06
1.80
1.050.78

t5_squad_v2  0  2  4  67.10 7.03
1.82
1.050.83

t5_CNN_Daily  0  2  4  67.10 7.03
2.74
2.01
1.38

bart_wmt14_de_en  0  2  4  66.10 6.10
2.031.82
1.34

gpt2_wikitext  0  2  4  6  88.007.69
5.335.06
2.70

mt5_iwlst17_de_en  0  2  4  6Độ trễ trên TX2(s)7.20 7.06
2.34
1.70
1.26

mt5_squad_v2  0  2  4  67.20 7.06
1.84
1.130.90

t5_squad_v2  0  2  4  67.10 7.03
1.85
1.100.95

t5_CNN_Daily  0  2  4  67.10 7.03
2.77
2.05
1.49

bart_wmt14_de_en  0  2  4  66.10 6.10
2.081.85
1.41

gpt2_wikitext  0  2  4  6  88.007.69
5.375.13
2.78

Hình 12: Độ trễ tạo trung bình mỗi token của LLMCad và các đường cơ sở dưới các tác vụ khác nhau trên thiết bị IoT.

Vicuna_Parrot  0  5 10 15Độ trễ trên Xiaomi 11 (s)15.214.9
8.3
6.0
3.73.3Vanilla StdPL STI SP BLD Của chúng tôi

Vicuna_wmt22_de_en  0  5 10 1515.214.9
8.3
6.2
3.83.6

Vicuna_wmt22_zh_en  0  5 10 1515.215.0
8.3
6.4
4.5
3.4

LLaMa2_squad  0  5 10 1515.214.9
10.6
5.5
4.54.1

LLaMa2_CNN_daily  0  5 10 1515.214.9
7.8
5.04.33.6

LLaMa2_TruthfulQA  0  5 10 1515.214.9
10.6
6.1
4.3
3.3

Vicuna_Parrot  0  5 10 15Độ trễ trên Xiaomi 10 (s)16.215.9
8.7
6.4
3.93.6

Vicuna_wmt22_de_en  0  5 10 1516.215.9
8.7
6.7
4.24.0

Vicuna_wmt22_zh_en  0  5 10 1516.216.0
8.7
6.8
4.9
3.7

LLaMa2_squad  0  5 10 1516.215.9
11.3
5.9
4.84.4

LLaMa2_CNN_daily  0  5 10 1516.215.9
8.1
5.54.73.8

LLaMa2_TruthfulQA  0  5 10 1516.215.9
11.3
6.6
4.7
3.8

Hình 13: Độ trễ suy luận trung bình mỗi token của LLMCad và các đường cơ sở dưới các tác vụ khác nhau trên điện thoại thông minh.

Mô hình-tác vụ-bộ dữ liệu Vanilla StdPL SP BLD Của chúng tôi
mT5-translation
IWLST17-DE->EN36.9 36.2 12.0 7.7 7.7 (4.8×)
T5-summary
CNN/Daily36.4 36.0 7.6 10.3 8.4 (4.3×)
T5-QA
SQuAD36.9 36.5 15.4 9.9 4.6 (8.0×)

(a) Jetson Orin NXMô hình-tác vụ-bộ dữ liệu Vanilla StdPL STI SP BLD Của chúng tôi
LLaMa2-summarization
CNN/Daily mail56.2 55.1 27.9 21.5 18.6 17.3 (3.2×)
LLaMa2-QA
TruthfulQA56.2 55.1 28.1 23.9 18.3 14.3 (3.9×)
Vicuna-translation
WMT22-DE-EN56.2 55.1 20.7 20.4 20.3 15.5 (3.6×)

(b) Xiaomi 11

Bảng 5: Tóm tắt tiêu thụ năng lượng (J) của các mô hình khác nhau trên các thiết bị khác nhau.

11

--- TRANG 12 ---
Conference'17, July 2017, Washington, DC, USA Daliang Xu♦, Wangsong Yin♦, Xin Jin♦, Ying Zhang♦, Shiyun Wei⋆, Mengwei Xu^, Xuanzhe Liu♦

4.85.05.25.45.6
t5_CNN_Daily12345Tăng tốc trên TX2 (x)
4.04.3 4.3 4.44.5
3.3 3.4 3.4 3.43.4
1.0 1.0 1.0 1.01.02.5 2.5 2.5 2.52.5Của chúng tôi BLD StdPL SP

4.85.05.25.45.6
t5_squad_v202468
7.17.8 7.8 7.98.0
6.46.6 6.6 6.76.7
1.0 1.0 1.0 1.01.03.7 3.8 3.8 3.83.8

4 5 6 7 8
LLaMa2_CNN_daily246Tăng tốc trên Xiaomi10 (x)
4.8 4.9 4.95.05.5
3.6 3.6 3.6 3.7 3.8
3.1 3.1 3.1 3.2 3.3
2.1 2.2
1.1 1.0 1.1Của chúng tôi BLD StdPL SP STI

4 5 6 7 8
LLaMa2_TruthfulQA24
4.7 4.7 4.7 4.85.1
3.7 3.7 3.7 3.7 3.8
2.8 2.8 2.8 2.9 3.0
1.8 1.8
0.9 1.0 1.1

Hình 14: Tăng tốc của các đường cơ sở khác nhau dưới các ngân sách bộ nhớ khác nhau.

mT5_iwlst17_de_en  0  2  4  6Độ trễ (s)7.20
1.67 1.561.29Std Std+TGV Std+TGV+SPP Std+TGV+SPP+SF

T5_CNN_Daily  0  2  4  67.10
1.871.69 1.65

LLaMA2-SQuAD  02.5  57.5 1010.20
3.92
3.132.74

Hình 15: Nghiên cứu loại bỏ của LLMCad .

giữa LLM thường trú bộ nhớ và mục tiêu có ý nghĩa hơn, và việc ủy thác token cho LLM thường trú bộ nhớ có thể đạt được nhiều lợi ích hơn.

4.4 Phân tích Tiêu thụ Năng lượng

Sau đó chúng tôi đánh giá tiêu thụ năng lượng của LLMCad với các mô hình mT5 và T5 trên thiết bị IoT và mô hình Vicuana và LLaMa2 trên điện thoại thông minh. Như được thể hiện trong Bảng 5, so với Std, StdPL, SP và BLD, LLMCad giảm tiêu thụ năng lượng mỗi token 4,35–7,96, 4,34–7,92, 1,56–3,33, và 1,05–2,15× trên Jetson Orin NX, LLMCad đạt được giảm tiêu thụ năng lượng 3,22–3,59, 3,18–3,56, 1,24–1,66, 1,07–1,31 và 2,01–2,56, × tương ứng trên Xiaomi 11, cộng với STI. Điều này là bởi vì hai kỹ thuật của LLMCad có thể ủy thác càng nhiều token càng tốt cho LLM thường trú bộ nhớ mà không hy sinh độ chính xác.

So với tăng tốc độ trễ, tiêu thụ năng lượng của LLMCad tương đối thấp hơn. Tình huống này phát sinh bởi vì tạo suy đoán của chúng tôi song song hóa việc thực thi LLM thường trú bộ nhớ và mục tiêu cùng nhau, dẫn đến tiêu thụ năng lượng nhiều hơn.

4.5 Nghiên cứu Loại bỏ

Kỹ thuật tổng thể. Chúng tôi tiến hành thêm phân tích phân tách về lợi ích mang lại bởi mỗi kỹ thuật của LLMCad . Các thí nghiệm được thực hiện với các mô hình mT5 và T5 trên TX2 và LLaMa2 trên Xiaomi 10. Kết quả được minh họa trong Hình 15. Thanh ngoài cùng bên trái giống như đường cơ sở Vanilla , trong khi thanh ngoài cùng bên trái là LLMCad . Ba kỹ thuật quan trọng tạo và xác minh cây token trong §3.2, chiến lược dự phòng tự thích ứng §3.3 và tạo suy đoán trong §3.4 được đại diện bởi TGV, SF và SPP tương ứng.

Chúng tôi quan sát rằng tất cả các kỹ thuật đều đóng góp không tầm thường vào cải thiện. Đầu tiên, tạo và xác minh cây không tự hồi quy có thể ủy thác hầu hết việc tạo token cho LLM thường trú bộ nhớ, dẫn đến tăng tốc 2,6–4,3 cho các mô hình mT5, T5 và LLaMa2, tương ứng. Lợi ích nhiều hơn cho mô hình mT5 trên bộ dữ liệu dịch IWLST là bởi vì mô hình mT5-Small có thể tạo ra nhiều token chính xác hơn so với hai mô hình khác nên nhiều token hơn có thể được ủy thác cho LLM thường trú bộ nhớ. Bên cạnh đó, thực thi suy đoán có thể giảm thời gian tạo mỗi token lên đến 1,51×. Đó là bởi vì LLMCad có thể sử dụng trực tiếp kết quả suy đoán khi xác minh không phát hiện lỗi, đặc biệt đúng cho mô hình LLaMA2. Cuối cùng, Chiến lược dự phòng tự thích ứng đạt được tăng tốc 1,08–1,20 ×. Điều này đạt được bằng cách tận dụng độ tin cậy tích lũy cây để đánh giá xác suất lỗi và điều chỉnh động thời điểm xác minh để đáp ứng với sự thay đổi trong độ phức tạp tác vụ.

5 CÔNG TRÌNH LIÊN QUAN

Hợp tác mô hình là một kỹ thuật tối ưu hóa phổ biến được sử dụng để giảm độ trễ suy luận [ 40,70]. Ý tưởng chính của nó là ủy thác hầu hết khối lượng công việc cho các mô hình nhẹ để giảm độ trễ suy luận trong khi duy trì độ chính xác tương đối cao. Tabi [ 70] là một công cụ suy luận đa cấp phục vụ truy vấn sử dụng nhiều mô hình nhỏ khác nhau tùy theo độ khó của truy vấn. MobiSR [40] và NestDNN [24] sử dụng ý tưởng tương tự nhưng phụ thuộc vào độ phân giải hoặc tài nguyên có sẵn. Các công trình "thoát sớm" khác [ 60,62,73,81], đề xuất thời điểm thoát thích ứng dựa vào độ khó dữ liệu đầu vào cũng có thể được coi là một sự hợp tác. Tuy nhiên, chúng tập trung vào kiến trúc mô hình dựa trên CNN/encoder hoặc phải sửa đổi và đào tạo lại mô hình, khó phù hợp với suy luận LLM trên thiết bị. Các công trình liên quan gần nhất là giải mã suy đoán [20,37,41,48], cũng sử dụng LLM nhỏ hơn để tạo văn bản và LLM lớn hơn để xác minh văn bản. LLMCad được thúc đẩy bởi những công trình này và là công cụ suy luận đầu tiên cho các tác vụ NLP sinh trên thiết bị, xem xét các thách thức độc đáo của thiết bị di động như tình huống giới hạn bộ nhớ.

Tối ưu hóa ML di động. Các phương pháp tối ưu hóa học máy, như nén mô hình [25,26,33,50,52,68,

12

--- TRANG 13 ---
LLMCad : Suy luận Mô hình Ngôn ngữ Lớn trên Thiết bị Nhanh và Có thể Mở rộng Conference'17, July 2017, Washington, DC, USA

76,77], giảm kích thước mô hình bằng lượng tử hóa và chưng cất kiến thức, bộ nhớ đệm [69,74,80], giảm tính toán bằng cách tái sử dụng kết quả hiện có, và cắt tỉa token [16,18,38, 57,66], giảm tính toán bằng cách cắt tỉa các token vô dụng, đã được nghiên cứu rộng rãi để giảm độ trễ tạo. LLMCad trực giao với và tương thích với những tối ưu hóa cấp độ thuật toán đó.

Bên cạnh đó, một số nhà nghiên cứu tập trung vào tạo văn bản theo cách không tự hồi quy [ 35,39]. Tuy nhiên, những công trình này chỉ có thể áp dụng cho các mô hình <1B và có vấn đề suy giảm độ chính xác, không phải hướng nghiên cứu chính.

Tối ưu hóa đường ống cho ML. Tối ưu hóa đường ống đã được sử dụng rộng rãi để tăng tốc ML [ 13,27,49,67,82]. Hầu hết trong số chúng, như PipeDream [ 49], được sử dụng để mở rộng ML ra nhiều máy bằng cách đường ống tính toán tiến/lùi với đồng bộ hóa kích hoạt/gradient để giảm thiểu bong bóng I/O và giao tiếp mạng. Tuy nhiên, có một số nghiên cứu tập trung vào tối ưu hóa máy/tác vụ đơn. Ví dụ, PipeSwitch [ 13] giới thiệu đường ống truyền mô hình qua PCIe và thực thi tác vụ trong GPU để giảm chi phí hiệu suất chuyển đổi ngữ cảnh; trong khi STI [27] đường ống tải mảnh mô hình với tính toán của nó để giảm độ trễ suy luận. LLMCad được lấy cảm hứng từ những nỗ lực này và đề xuất một đường ống tạo suy đoán hiệu quả để giải quyết thách thức chặn I/O và tính song song hạn chế.

6 KẾT LUẬN

Công trình này đã đề xuất LLMCad , công cụ suy luận hiệu quả đầu tiên cho các tác vụ NLP sinh trên thiết bị. Nó phá vỡ rào cản bộ nhớ và mang khả năng tỷ lệ của LLM đến thiết bị di động Nó kết hợp ba kỹ thuật mới, bao gồm: tạo và xác minh cây token, Chiến lược dự phòng tự thích ứng và đường ống tạo suy đoán có thể khai thác các tài nguyên phần cứng lãng phí trong quá trình xác minh. Các thí nghiệm của chúng tôi đã chứng minh rằng khi so sánh với các công cụ LLM tiên tiến, LLMCad có thể giảm thời gian tạo trung bình mỗi token 2,9–9,3 × và 3,5–4,7× trên thiết bị IoT và điện thoại thông minh, mà không làm giảm độ chính xác.

TÀI LIỆU THAM KHẢO

[1]Gboard - the Google Keyboard - Apps on Google Play — play.google.com. https://play.google.com/store/apps/details?id=com. google.android.inputmethod.latin&hl=en. [Accessed 22-Jul-2023].
[2]Google Translate - Apps on Google Play — play.google.com. https://play.google.com/store/apps/details?id=com.google.android. apps.translate&hl=en_US. [Accessed 22-Jul-2023].
[3]iTranslate — itranslate.com. https://itranslate.com/. [Accessed 22-Jul- 2023].
[4] MNN. https://github.com/alibaba/MNN. Accessed: [2023.7].
[5] PyTorch. https://pytorch.org/. Accessed: [2023.7].
[6]Siri — apple.com. https://www.apple.com/siri/. [Accessed 22-Jul-2023].
[7] TensorFlow. https://vllm.ai/. Accessed: [2023.7].
[8] TensorFlow. https://www.tensorflow.org/. Accessed: [2023.7].
[9] TensorFlow. https://huggingface.co/TheBloke. Accessed: [2023.7].
[10] TensorFlow. https://github.com/PanQiWei/AutoGPTQ. Accessed: [2023.7].
[11] Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang, Stephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer. Scaling laws for generative mixed-modal language models. arXiv preprint arXiv:2301.03728, 2023.
[12] Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling laws in language and vision. Advances in Neural Information Processing Systems, 35:22300–22312, 2022.
[13] Zhihao Bai, Zhen Zhang, Yibo Zhu, and Xin Jin. {PipeSwitch}: Fast pipelined context switching for deep learning applications. In14th USENIX Symposium onOperating Systems Design and Implementation (OSDI 20), pages 499–514, 2020.
[14] David Bau, Hendrik Strobelt, William Peebles, Jonas Wulff, Bolei Zhou, Jun-Yan Zhu, and Antonio Torralba. Semantic photo manipulation with a generative image prior. arXiv preprint arXiv:2005.07727, 2020.
[15] Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ale s Tam- chyna. Findings of the 2014 workshop on statistical machine trans- lation. In Proceedings oftheNinth Workshop onStatistical Machine Translation , pages 12–58, Baltimore, Maryland, USA, June 2014. Asso- ciation for Computational Linguistics.
[16] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022.
[17] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learn- ers. Advances inneural information processing systems , 33:1877– 1901, 2020.
[18] Han Cai, Ji Lin, Yujun Lin, Zhijian Liu, Haotian Tang, Hanrui Wang, Ligeng Zhu, and Song Han. Enable deep learning on mobile devices: Methods, systems, and applications. ACM Transactions onDesign Automation ofElectronic Systems (TODAES), 27(3):1–50, 2022.
[19] Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian Stüker, Katsuhito Sudoh, Koichiro Yoshino, and Chris- tian Federmann. Overview of the IWSLT 2017 evaluation cam- paign. In Proceedings ofthe14th International Conference onSpoken Language Translation , pages 2–14, Tokyo, Japan, December 14-15 2017. International Workshop on Spoken Language Translation.
[20] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large lan- guage model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023.
[21] Aidan Clark, Diego De Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed language models. In International Conference onMachine Learning , pages 4057–4086. PMLR, 2022.
[22] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Hee- woo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
[23] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings ofthe60th Annual Meeting oftheAssociation forComputational Linguistics (Volume 1: Long Papers), pages 320–335, 2022.

13

--- TRANG 14 ---
Conference'17, July 2017, Washington, DC, USA Daliang Xu♦, Wangsong Yin♦, Xin Jin♦, Ying Zhang♦, Shiyun Wei⋆, Mengwei Xu^, Xuanzhe Liu♦

[24] Biyi Fang, Xiao Zeng, and Mi Zhang. Nestdnn: Resource-aware multi- tenant on-device deep learning for continuous mobile vision. In Proceedings ofthe24th Annual International Conference onMobile Computing andNetworking, pages 115–127, 2018.
[25] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained trans- formers. arXiv preprint arXiv:2210.17323, 2022.
[26] Hui Guan, Shaoshan Liu, Xiaolong Ma, Wei Niu, Bin Ren, Xipeng Shen, Yanzhi Wang, and Pu Zhao. Cocopie: Enabling real-time ai on off-the-shelf mobile devices via compression-compilation co-design. Communications oftheACM, 64(6):62–68, 2021.
[27] Liwei Guo, Wonkyo Choe, and Felix Xiaozhu Lin. Sti: Turbocharge nlp inference at the edge via elastic pipelining. In Proceedings of the28th ACM International Conference onArchitectural Support for Programming Languages and Operating Systems, Volume 2, pages 791–803, 2023.
[28] Ellen L. Hahne. Round-robin scheduling for max-min fairness in data networks. IEEE Journal onSelected Areas incommunications , 9(7):1024–1039, 1991.
[29] Rui Han, Qinglong Zhang, Chi Harold Liu, Guoren Wang, Jian Tang, and Lydia Y Chen. Legodnn: block-grained scaling of deep neu- ral networks for mobile vision. In Proceedings ofthe27th Annual International Conference onMobile Computing and Networking , pages 406–419, 2021.
[30] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multi- task language understanding. arXiv preprint arXiv:2009.03300, 2020.
[31] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adapta- tion of large language models. arXiv preprint arXiv:2106.09685 , 2021.
[32] Chien-Chin Huang, Gu Jin, and Jinyang Li. Swapadvisor: Push- ing deep learning beyond the gpu memory limit via smart swap- ping. In Proceedings oftheTwenty-Fifth International Conference onArchitectural Support forProgramming Languages andOperating Systems, pages 1341–1355, 2020.
[33] Loc N Huynh, Youngki Lee, and Rajesh Krishna Balan. Deepmon: Mo- bile gpu-based deep learning framework for continuous vision appli- cations. In Proceedings ofthe15th Annual International Conference onMobile Systems, Applications, andServices, pages 82–95, 2017.
[34] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Ben- jamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.
[35] Jungo Kasai, Nikolaos Pappas, Hao Peng, James Cross, and Noah A Smith. Deep encoder, shallow decoder: Reevaluating non- autoregressive machine translation. arXiv preprint arXiv:2006.10369 , 2020.
[36] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W Mahoney, et al. Full stack optimization of transformer inference: a survey. arXiv preprint arXiv:2302.14017, 2023.
[37] Sehoon Kim, Karttikeya Mangalam, Jitendra Malik, Michael W Ma- honey, Amir Gholami, and Kurt Keutzer. Big little transformer decoder. arXiv preprint arXiv:2302.07863, 2023.
[38] Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph Hassoun, and Kurt Keutzer. Learned token pruning for transformers. In Proceedings ofthe28th ACM SIGKDD Conference onKnowledge Discovery andData Mining, pages 784–794, 2022.
[39] Xiang Kong, Adithya Renduchintala, James Cross, Yuqing Tang, Jiatao Gu, and Xian Li. Multilingual neural machine translation with deep en- coder and multiple shallow decoders. arXiv preprint arXiv:2206.02079 , 2022.
[40] Royson Lee, Stylianos I Venieris, Lukasz Dudziak, Sourav Bhattacharya, and Nicholas D Lane. Mobisr: Efficient on-device super-resolution through heterogeneous mobile processors. In The 25th annual international conference onmobile computing andnetworking , pages 1–16, 2019.
[41] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference onMachine Learning, pages 19274–19286. PMLR, 2023.
[42] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Ab- delrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettle- moyer. BART: denoising sequence-to-sequence pre-training for nat- ural language generation, translation, and comprehension. CoRR , abs/1910.13461, 2019.
[43] Chin-Yew Lin. Rouge: A package for automatic evaluation of sum- maries. In Text summarization branches out, pages 74–81, 2004.
[44] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods, 2021.
[45] llama.cpp. Port of Facebook's LLaMA model in C/C++ Resources. https://github.com/ggerganov/llama.cpp, Year of publication. Ac- cessed: [2023.7].
[46] Chen Meng, Minmin Sun, Jun Yang, Minghui Qiu, and Yang Gu. Train- ing deeper models by gpu memory optimization on tensorflow. In Proc. ofMLSystems Workshop inNIPS, volume 7, 2017.
[47] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016.
[48] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative llm serving with speculative inference and token tree verification. arXiv preprint arXiv:2305.09781, 2023.
[49] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. InProceedings ofthe27th ACM Symposium onOperating Systems Principles, pages 1–15, 2019.
[50] Wei Niu, Xiaolong Ma, Sheng Lin, Shihao Wang, Xuehai Qian, Xue Lin, Yanzhi Wang, and Bin Ren. Patdnn: Achieving real-time dnn execution on mobile devices with pattern-based weight prun- ing. In Proceedings oftheTwenty-Fifth International Conference onArchitectural Support forProgramming Languages andOperating Systems, pages 907–922, 2020.
[51] Roma Patel and Ellie Pavlick. Mapping language models to grounded conceptual spaces. In International Conference onLearning Representations, 2021.
[52] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. Proceedings of Machine Learning andSystems, 5, 2023.
[53] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.
[54] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[55] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. TheJournal ofMachine Learning Research, 21(1):5485–5551, 2020.
[56] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ Questions for Machine Comprehension of Text. arXiv e-prints, page arXiv:1606.05250, 2016.
[57] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient vision transformers with dynamic

14

--- TRANG 15 ---
LLMCad : Suy luận Mô hình Ngôn ngữ Lớn trên Thiết bị Nhanh và Có thể Mở rộng Conference'17, July 2017, Washington, DC, USA

token sparsification. Advances inneural information processing systems, 34:13937–13949, 2021.
[58] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthe- sis. In International conference onmachine learning , pages 1060–1069. PMLR, 2016.
[59] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffu- sion models. In Proceedings oftheIEEE/CVF conference oncomputer vision andpattern recognition, pages 10684–10695, 2022.
[60] Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi Tay, and Donald Metzler. Confident adaptive lan- guage modeling. Advances inNeural Information Processing Systems , 35:17456–17472, 2022.
[61] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointer-generator networks. In Proceedings ofthe55th Annual Meeting oftheAssociation forComputational Linguistics (Volume 1:Long Papers) , pages 1073–1083, Vancouver, Canada, July 2017. Association for Computational Linguistics.
[62] Thierry Tambe, Coleman Hooper, Lillian Pentecost, Tianyu Jia, En-Yu Yang, Marco Donato, Victor Sanh, Paul Whatmough, Alexander M Rush, David Brooks, et al. Edgebert: Sentence-level energy optimiza- tions for latency-aware multi-task nlp inference. In MICRO-54: 54th Annual IEEE/ACM International Symposium onMicroarchitecture , pages 830–844, 2021.
[63] FP Team. Generative AI: Advantages, Disadvantages, Limitations, and Challenges — fact.technology. https://fact.technology/learn/ generative-ai-advantages-limitations-and-challenges/. [Accessed 22- Jul-2023].
[64] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie- Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[65] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances inneural information processing systems , 30, 2017.
[66] Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse attention architecture with cascade token and head pruning. In 2021 IEEE International Symposium onHigh-Performance Computer Architecture (HPCA), pages 97–110. IEEE, 2021.
[67] Shibo Wang, Jinliang Wei, Amit Sabne, Andy Davis, Berkin Ilbeyi, Blake Hechtman, Dehao Chen, Karthik Srinivasa Murthy, Marcello Maggioni, Qiao Zhang, et al. Overlap communication with depen- dent computation via decomposition in large deep learning mod- els. In Proceedings ofthe28th ACM International Conference on Architectural Support forProgramming Languages and Operating Systems, Volume 1, pages 93–106, 2022.
[68] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-attention distillation for task-agnostic com- pression of pre-trained transformers. Advances inNeural Information Processing Systems, 33:5776–5788, 2020.
[69] Xiaofei Wang, Yiwen Han, Victor CM Leung, Dusit Niyato, Xueqiang Yan, and Xu Chen. Convergence of edge computing and deep learning: A comprehensive survey. IEEE Communications Surveys &Tutorials , 22(2):869–904, 2020.
[70] Yiding Wang, Kai Chen, Haisheng Tan, and Kun Guo. Tabi: An efficient multi-level inference system for large language models. In Proceedings oftheEighteenth European Conference onComputer Systems , pages 233–248, 2023.
[71] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebas- tian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald

Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.
[72] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompt- ing elicits reasoning in large language models. Advances inNeural Information Processing Systems, 35:24824–24837, 2022.
[73] Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. Deebert: Dynamic early exiting for accelerating bert inference. arXiv preprint arXiv:2004.12993, 2020.
[74] Mengwei Xu, Mengze Zhu, Yunxin Liu, Felix Xiaozhu Lin, and Xu- anzhe Liu. Deepcache: Principled cache for mobile deep vision. In Proceedings ofthe24th annual international conference onmobile computing andnetworking, pages 129–144, 2018.
[75] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al- Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: A mas- sively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934, 2020.
[76] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. Advances in Neural Information Processing Systems, 35:27168–27183, 2022.
[77] Zhao You, Shulin Feng, Dan Su, and Dong Yu. Speechmoe2: Mixture- of-experts model with improved routing. In ICASSP 2022-2022 IEEE International Conference onAcoustics, Speech andSignal Processing (ICASSP), pages 7217–7221. IEEE, 2022.
[78] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: A distributed serving system for {Transformer-Based }generative models. In 16th USENIX Symposium onOperating Systems Design andImplementation (OSDI 22), pages 521–538, 2022.
[79] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 , 2022.
[80] Wuyang Zhang, Zhezhi He, Luyang Liu, Zhenhua Jia, Yunxin Liu, Marco Gruteser, Dipankar Raychaudhuri, and Yanyong Zhang. Elf: accelerate high-resolution mobile deep vision with content-aware parallel offloading. In Proceedings ofthe27th Annual International Conference onMobile Computing andNetworking , pages 201–214, 2021.
[81] Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian McAuley, Ke Xu, and Furu Wei. Bert loses patience: Fast and robust inference with early exit. Advances inNeural Information Processing Systems , 33:18330–18341, 2020.
[82] Zhe Zhou, Xuechao Wei, Jiejing Zhang, and Guangyu Sun. {PetS}: A unified framework for {Parameter-Efficient }transformers serving. In 2022 USENIX Annual Technical Conference (USENIX ATC 22), pages 489–504, 2022.

15
