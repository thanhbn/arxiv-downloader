# 2309.04255.pdf
# Chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/speculative/2309.04255.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 1667824 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================


--- TRANG 1 ---
LLMCad : Suy luáº­n MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n trÃªn Thiáº¿t bá»‹ Nhanh vÃ  CÃ³ thá»ƒ Má»Ÿ rá»™ng
Daliang Xuâ™¦, Wangsong Yinâ™¦, Xin Jinâ™¦, Ying Zhangâ™¦, Shiyun Weiâ‹†, Mengwei Xu^,
Xuanzhe Liuâ™¦
â™¦PhÃ²ng thÃ­ nghiá»‡m ChÃ­nh vá» CÃ´ng nghá»‡ Pháº§n má»m Tin cáº­y Cao (Äáº¡i há»c Báº¯c Kinh), Báº¯c Kinh, Trung Quá»‘c
â‹†PhÃ²ng thÃ­ nghiá»‡m Zhongguancun, Báº¯c Kinh, Trung Quá»‘c.
^PhÃ²ng thÃ­ nghiá»‡m ChÃ­nh cá»§a NhÃ  nÆ°á»›c vá» CÃ´ng nghá»‡ Máº¡ng vÃ  Chuyá»ƒn máº¡ch (BUPT), Báº¯c Kinh, Trung Quá»‘c
{xudaliang,hg,xinjinpku,zhang.ying,weishiyun,liuxuanzhe}@pku.edu.cn
yws@stu.pku.edu.cn
mwx@bupt.edu.cn
TÃ“M Táº®T
CÃ¡c tÃ¡c vá»¥ sinh, cháº³ng háº¡n nhÆ° táº¡o vÄƒn báº£n vÃ  tráº£ lá»i cÃ¢u há»i, giá»¯ má»™t vá»‹ trÃ­ quan trá»ng trong lÄ©nh vá»±c á»©ng dá»¥ng di Ä‘á»™ng. Do tÃ­nh nháº¡y cáº£m cá»§a chÃºng Ä‘á»‘i vá»›i má»‘i quan tÃ¢m vá» quyá»n riÃªng tÆ°, cÃ³ nhu cáº§u ngÃ y cÃ ng tÄƒng Ä‘á»ƒ thá»±c thi chÃºng trá»±c tiáº¿p trÃªn thiáº¿t bá»‹ di Ä‘á»™ng. Hiá»‡n táº¡i, viá»‡c thá»±c thi cÃ¡c tÃ¡c vá»¥ sinh nÃ y phá»¥ thuá»™c ráº¥t nhiá»u vÃ o MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLM). Tuy nhiÃªn, kháº£ nÄƒng bá»™ nhá»› háº¡n cháº¿ cá»§a cÃ¡c thiáº¿t bá»‹ nÃ y Ä‘áº·t ra má»™t thÃ¡ch thá»©c to lá»›n Ä‘á»‘i vá»›i kháº£ nÄƒng má»Ÿ rá»™ng cá»§a cÃ¡c mÃ´ hÃ¬nh nhÆ° váº­y.

Trong nghiÃªn cá»©u cá»§a chÃºng tÃ´i, chÃºng tÃ´i giá»›i thiá»‡u LLMCad , má»™t cÃ´ng cá»¥ suy luáº­n trÃªn thiáº¿t bá»‹ sÃ¡ng táº¡o Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t cho cÃ¡c tÃ¡c vá»¥ Xá»­ lÃ½ NgÃ´n ngá»¯ Tá»± nhiÃªn (NLP) sinh hiá»‡u quáº£. Ã tÆ°á»Ÿng cá»‘t lÃµi Ä‘áº±ng sau LLMCad xoay quanh sá»± há»£p tÃ¡c mÃ´ hÃ¬nh: má»™t LLM nhá» gá»n, náº±m trong bá»™ nhá»›, Ä‘áº£m nháº­n viá»‡c táº¡o ra cÃ¡c token Ä‘Æ¡n giáº£n nháº¥t, trong khi má»™t LLM Ä‘á»™ chÃ­nh xÃ¡c cao can thiá»‡p Ä‘á»ƒ xÃ¡c thá»±c cÃ¡c token nÃ y vÃ  sá»­a chá»¯a báº¥t ká»³ lá»—i nÃ o Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh. LLMCad káº¿t há»£p ba ká»¹ thuáº­t má»›i: (1) Thay vÃ¬ táº¡o cÃ¡c token á»©ng cá»­ viÃªn má»™t cÃ¡ch tuáº§n tá»±, LLMCad sá»­ dá»¥ng LLM nhá» hÆ¡n Ä‘á»ƒ xÃ¢y dá»±ng má»™t cÃ¢y token, bao gá»“m má»™t pháº¡m vi rá»™ng hÆ¡n cÃ¡c Ä‘Æ°á»ng dáº«n token há»£p lÃ½. Sau Ä‘Ã³, LLM lá»›n hÆ¡n cÃ³ thá»ƒ xÃ¡c thá»±c táº¥t cáº£ cÃ¡c Ä‘Æ°á»ng dáº«n nÃ y má»™t cÃ¡ch Ä‘á»“ng thá»i má»™t cÃ¡ch hiá»‡u quáº£. (2) NÃ³ sá»­ dá»¥ng má»™t chiáº¿n lÆ°á»£c dá»± phÃ²ng tá»± Ä‘iá»u chá»‰nh, khá»Ÿi Ä‘á»™ng nhanh chÃ³ng quÃ¡ trÃ¬nh xÃ¡c minh báº¥t cá»© khi nÃ o LLM nhá» hÆ¡n táº¡o ra má»™t token lá»—i. (3) Äá»ƒ Ä‘áº£m báº£o luá»“ng táº¡o token liÃªn tá»¥c, LLMCad táº¡o token má»™t cÃ¡ch suy Ä‘oÃ¡n trong quÃ¡ trÃ¬nh xÃ¡c minh báº±ng cÃ¡ch triá»ƒn khai má»™t Ä‘Æ°á»ng á»‘ng tÃ­nh toÃ¡n-IO.

ThÃ´ng qua má»™t loáº¡t thÃ­ nghiá»‡m má»Ÿ rá»™ng, LLMCad thá»ƒ hiá»‡n tá»‘c Ä‘á»™ táº¡o token áº¥n tÆ°á»£ng, Ä‘áº¡t Ä‘Æ°á»£c tá»‘c Ä‘á»™ nhanh hÆ¡n tá»›i 9,3Ã— so vá»›i cÃ¡c cÃ´ng cá»¥ suy luáº­n hiá»‡n cÃ³.

1 GIá»šI THIá»†U
CÃ¡c tÃ¡c vá»¥ sinh nhÆ° táº¡o vÄƒn báº£n, tráº£ lá»i cÃ¢u há»i vÃ  dá»‹ch thuáº­t Ä‘Ã³ng vai trÃ² quan trá»ng trÃªn thiáº¿t bá»‹ di Ä‘á»™ng, vÃ¬ nhiá»u á»©ng dá»¥ng dá»±a vÃ o chÃºng Ä‘á»ƒ cung cáº¥p cÃ¡c chá»©c nÄƒng chÃ­nh. VÃ­ dá»¥, á»©ng dá»¥ng phÆ°Æ¡ng thá»©c nháº­p nhÆ° Google

100M 1B 10B 100B 1T
KÃ­ch thÆ°á»›c tham sá»‘02040Äá»™ chÃ­nh xÃ¡c (%)
Jetson TX2
(IoT)Xiaomi10
(Smartphone)Jetson Orin
(Autonomous)

LLaMA-Math
GPT3-NLU
GPT3-Mode
GPT3-GM
RÃ o cáº£n bá»™ nhá»›(a) Kháº£ nÄƒng xuáº¥t hiá»‡n qua nhiá»u LLM khÃ¡c nhau.

100M 1B 10B 100B 1T
KÃ­ch thÆ°á»›c tham sá»‘02040Äá»™ trá»… (s)
Jetson TX2
(IoT)Xiaomi10
(Smartphone)Jetson Orin
(Autonomous)

T5-TX2
LLaMA-Xiaomi 10
LLaMA-Orin
RÃ o cáº£n bá»™ nhá»›
(b) Äá»™ trá»… táº¡o cá»§a LLM qua nhiá»u thiáº¿t bá»‹ khÃ¡c nhau.

HÃ¬nh 1: RÃ o cáº£n bá»™ nhá»› cáº£n trá»Ÿ "quy luáº­t tá»· lá»‡" cá»§a LLM trÃªn thiáº¿t bá»‹ di Ä‘á»™ng. *-Math, *-NLU, *-Mode, vÃ  *-GM biá»ƒu thá»‹ kháº£ nÄƒng xuáº¥t hiá»‡n cá»§a LLM: suy luáº­n toÃ¡n há»c, hiá»ƒu Ä‘a nhiá»‡m vá»¥, sá»‘ há»c cháº¿ Ä‘á»™, vÃ  há»c biá»ƒu diá»…n cÃ³ Ã½ nghÄ©a.

GBoard táº­n dá»¥ng ráº¥t nhiá»u kháº£ nÄƒng táº¡o vÄƒn báº£n cá»§a nÃ³, trong khi trá»£ lÃ½ riÃªng tÆ° nhÆ° Apple Siri sá»­ dá»¥ng nÃ³ Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i. CÃ¡c tÃ¡c vá»¥ nhÆ° váº­y thÆ°á»ng nháº¡y cáº£m vá» quyá»n riÃªng tÆ° vÃ  phá»¥ thuá»™c ráº¥t nhiá»u vÃ o dá»¯ liá»‡u riÃªng cá»§a ngÆ°á»i dÃ¹ng, do Ä‘Ã³ Ä‘Ã²i há»i suy luáº­n cá»¥c bá»™ trÃªn thiáº¿t bá»‹.

CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM), Ä‘áº·c biá»‡t lÃ  nhá»¯ng mÃ´ hÃ¬nh Ä‘Æ°á»£c xÃ¢y dá»±ng trÃªn bá»™ giáº£i mÃ£ transformer [ 65] nhÆ° GPT-3 [ 17] vÃ  LLaMA [ 64], Ä‘Ã£ trá»Ÿ thÃ nh phÆ°Æ¡ng phÃ¡p tiÃªu chuáº©n Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c tÃ¡c vá»¥ sinh NLP. NghiÃªn cá»©u gáº§n Ä‘Ã¢y trong cá»™ng Ä‘á»“ng há»c mÃ¡y Ä‘Ã£ chá»©ng minh ráº±ng viá»‡c má»Ÿ rá»™ng quy mÃ´ kÃ­ch thÆ°á»›c tham sá»‘ cá»§a cÃ¡c LLM nhÆ° váº­y mang láº¡i cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c vÃ  kháº£ nÄƒng xuáº¥t hiá»‡n [ 17,64,71,71,72], nhÆ° Ä‘Æ°á»£c thá»ƒ hiá»‡n trong HÃ¬nh 1(a). NÃ³i chung, má»™t LLM cáº§n cÃ³ hÆ¡n 1B tham sá»‘ Ä‘á»ƒ há»c cÃ¡c biá»ƒu diá»…n cÃ³ Ã½ nghÄ©a [ 51], hÆ¡n 10B tham sá»‘ Ä‘á»ƒ thá»ƒ hiá»‡n kháº£ nÄƒng suy luáº­n sá»‘ há»c nháº¥t Ä‘á»‹nh [ 22], vÃ  hÆ¡n 30B

1arXiv:2309.04255v1  [cs.NI]  8 Sep 2023

--- TRANG 2 ---
Conference'17, July 2017, Washington, DC, USA Daliang Xuâ™¦, Wangsong Yinâ™¦, Xin Jinâ™¦, Ying Zhangâ™¦, Shiyun Weiâ‹†, Mengwei Xu^, Xuanzhe Liuâ™¦

tham sá»‘ Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c kháº£ nÄƒng hiá»ƒu Ä‘a nhiá»‡m vá»¥ [ 30]. Hiá»‡n tÆ°á»£ng nÃ y Ä‘Æ°á»£c cá»™ng Ä‘á»“ng há»c mÃ¡y cÃ´ng nháº­n rá»™ng rÃ£i lÃ  quy luáº­t tá»· lá»‡ [11, 12, 21, 34].

ThÃ¡ch thá»©c chÃ­nh: rÃ o cáº£n bá»™ nhá»›. Tuy nhiÃªn, cÃ¡c thÃ­ nghiá»‡m sÆ¡ bá»™ cá»§a chÃºng tÃ´i trong HÃ¬nh 1(b) cho tháº¥y ráº±ng kháº£ nÄƒng tá»· lá»‡ bá»‹ thÃ¡ch thá»©c trÃªn thiáº¿t bá»‹ di Ä‘á»™ng. Cá»¥ thá»ƒ, khi cÃ¡c LLM quÃ¡ lá»›n Ä‘á»ƒ vá»«a vá»›i bá»™ nhá»› thiáº¿t bá»‹, cÃ¡c cÃ´ng cá»¥ DNN di Ä‘á»™ng nhÆ° MNN [ 4] vÃ  llama.cpp [ 45] cáº§n pháº£i liÃªn tá»¥c giáº£i phÃ³ng vÃ  táº£i trá»ng sá»‘ mÃ´ hÃ¬nh. Äiá»u nÃ y dáº«n Ä‘áº¿n Ä‘á»™ trá»… suy luáº­n kÃ©o dÃ i 59â€“224 Ã—. RÃ o cáº£n bá»™ nhá»› nhÆ° váº­y cáº£n trá»Ÿ nghiÃªm trá»ng quy luáº­t tá»· lá»‡. NgÆ°á»i dÃ¹ng pháº£i lá»±a chá»n giá»¯a táº¡o theo thá»i gian thá»±c vÃ  kháº£ nÄƒng xuáº¥t hiá»‡n. VÃ­ dá»¥, 10B tham sá»‘ Ä‘áº¡i diá»‡n cho kÃ­ch thÆ°á»›c tá»‘i thiá»ƒu cáº§n thiáº¿t Ä‘á»ƒ LLaMA cÃ³ kháº£ nÄƒng suy luáº­n sá»‘ há»c, nhÆ°ng nÃ³ cÅ©ng Ä‘áº¡i diá»‡n cho kÃ­ch thÆ°á»›c tham sá»‘ tá»‘i Ä‘a Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c suy luáº­n thá»i gian thá»±c trÃªn Ä‘iá»‡n thoáº¡i thÃ´ng minh (vÃ­ dá»¥, Xiaomi 10).

LLMCad : phÃ¡ vá»¡ rÃ o cáº£n bá»™ nhá»› thÃ´ng qua há»£p tÃ¡c mÃ´ hÃ¬nh. Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t LLMCad , cÃ´ng cá»¥ suy luáº­n hiá»‡u quáº£ Ä‘áº§u tiÃªn cho cÃ¡c tÃ¡c vá»¥ NLP sinh trÃªn thiáº¿t bá»‹. LLMCad mang kháº£ nÄƒng tá»· lá»‡ cá»§a LLM Ä‘áº¿n thiáº¿t bá»‹ di Ä‘á»™ng vá»›i tá»‘c Ä‘á»™ táº¡o cÃ³ thá»ƒ cháº¥p nháº­n Ä‘Æ°á»£c thÃ´ng qua há»£p tÃ¡c mÃ´ hÃ¬nh . Ã tÆ°á»Ÿng chÃ­nh lÃ  á»§y thÃ¡c háº§u háº¿t cÃ¡c token cho má»™t LLM thá»i gian thá»±c nhá» hÆ¡n cÃ³ thá»ƒ Ä‘Æ°á»£c lÆ°u trá»¯ hoÃ n toÃ n trong bá»™ nhá»› thiáº¿t bá»‹ (Ä‘Æ°á»£c gá»i lÃ  LLM thÆ°á»ng trÃº bá»™ nhá»›). Thiáº¿t káº¿ nÃ y dá»±a trÃªn má»™t quan sÃ¡t chÃ­nh ráº±ng, trong khi má»™t LLM nhá» hÆ¡n khÃ´ng Ä‘á»§ Ä‘á»ƒ cung cáº¥p cÃ¡c cÃ¢u tá»« Ä‘áº§u cuá»‘i Ä‘áº¿n cuá»‘i thá»a mÃ£n, chÃºng cÃ³ thá»ƒ táº¡o ra Ä‘Ãºng háº§u háº¿t cÃ¡c token dá»… (vÃ­ dá»¥, tá»« háº¡n Ä‘á»‹nh, Ä‘áº¡i tá»« vÃ  dáº¥u cÃ¢u). HÆ¡n ná»¯a, cÃ¡c LLM thÆ°á»ng Ä‘Æ°á»£c Ä‘Ã o táº¡o vá»›i má»™t loáº¡t cÃ¡c biáº¿n thá»ƒ mÃ´ hÃ¬nh, vÃ­ dá»¥ T5-Small/Base/Large [ 55] vÃ  LLaMa-7B/13B/33B [ 64], vÃ  Ä‘á»‘i tÃ¡c nhá» hÆ¡n cá»§a nÃ³ (vÃ­ dá»¥, LLaMa-7B vÃ  T5-small, Ä‘Æ°á»£c gá»i lÃ  mÃ´ hÃ¬nh thÆ°á»ng trÃº bá»™ nhá»› trong bÃ i bÃ¡o nÃ y) thÆ°á»ng cÃ³ thá»ƒ Ä‘Æ°á»£c lÆ°u trá»¯ trong bá»™ nhá»› má»™t cÃ¡ch dá»… dÃ ng [17, 55, 64, 75].

LLMCad sá»­ dá»¥ng má»™t hÃ¬nh thá»©c há»£p tÃ¡c mÃ´ hÃ¬nh Ä‘á»™c Ä‘Ã¡o, Ä‘Æ°á»£c gá»i lÃ  "táº¡o-rá»“i-xÃ¡c minh" [ 20,41]. Trong phÆ°Æ¡ng phÃ¡p nÃ y, LLM thÆ°á»ng trÃº bá»™ nhá»› Ä‘Ã³ng vai trÃ² lÃ  bá»™ táº¡o token, trong khi LLM má»¥c tiÃªu hoáº¡t Ä‘á»™ng nhÆ° má»™t bá»™ xÃ¡c minh, sá»­ dá»¥ng Ä‘áº§u ra cá»§a nÃ³ lÃ m chuáº©n má»±c Ä‘á»ƒ kiá»ƒm tra vÃ  sá»­a chá»¯a báº¥t ká»³ lá»—i nÃ o Ä‘Æ°á»£c giá»›i thiá»‡u trong quÃ¡ trÃ¬nh táº¡o token. PhÆ°Æ¡ng phÃ¡p nÃ y cung cáº¥p hai lá»£i tháº¿ Ä‘Ã¡ng ká»ƒ: (1) KhÃ´ng lÃ m giáº£m Ä‘á»™ chÃ­nh xÃ¡c. Má»—i token Ä‘Æ°á»£c xÃ¡c minh bá»Ÿi mÃ´ hÃ¬nh má»¥c tiÃªu, do Ä‘Ã³ Ä‘á»™ chÃ­nh xÃ¡c cá»§a nÃ³ Ä‘Æ°á»£c Ä‘áº£m báº£o. Äiá»u nÃ y ráº¥t quan trá»ng vÃ¬ má»™t token sai cÃ³ thá»ƒ lan truyá»n lá»—i cá»§a nÃ³ Ä‘áº¿n cÃ¡c token tiáº¿p theo do báº£n cháº¥t tá»± há»“i quy. (2) XÃ¡c minh nhanh. NhÆ° sáº½ Ä‘Æ°á»£c chi tiáº¿t trong Â§2.3, viá»‡c xÃ¡c minh ğ‘ token cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n trong má»™t láº§n suy luáº­n cá»§a mÃ´ hÃ¬nh má»¥c tiÃªu, do Ä‘Ã³ nhanh hÆ¡n nhiá»u so vá»›i viá»‡c sá»­ dá»¥ng nÃ³ Ä‘á»ƒ táº¡o ğ‘ token tuáº§n tá»±.

Máº·c dÃ¹ cÃ³ nhá»¯ng lá»£i tháº¿ nÃ y, viá»‡c Ã¡p dá»¥ng há»£p tÃ¡c mÃ´ hÃ¬nh cho LLM trÃªn thiáº¿t bá»‹ giá»›i thiá»‡u ba thÃ¡ch thá»©c Ä‘áº·c biá»‡t:

â€¢Token chÃ­nh xÃ¡c bá»‹ bá» qua vá»›i Ä‘á»™ tin cáº­y khÃ´ng tá»‘i Æ°u. ThÃ´ng thÆ°á»ng, cÃ¡c cÃ´ng cá»¥ vÃ  nghiÃªn cá»©u LLM tiÃªn tiáº¿n luÃ´n sá»­ dá»¥ng token cÃ³ xÃ¡c suáº¥t cao nháº¥t lÃ m Ä‘áº§u ra. Tuy nhiÃªn, quan sÃ¡t cá»§a chÃºng tÃ´i Ä‘Ã£ tiáº¿t lá»™ ráº±ng má»™t sá»‘ lá»—i táº¡o bá»Ÿi LLM thÆ°á»ng trÃº bá»™ nhá»› cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­a chá»¯a bá»Ÿi cÃ¡c token khÃ´ng tá»‘i Æ°u. HÃ¬nh 4 Ä‘Æ°a ra má»™t vÃ­ dá»¥ thá»±c táº¿ vá» hiá»‡n tÆ°á»£ng nhÆ° váº­y. Cho ráº±ng chi phÃ­ hiá»‡u suáº¥t Ä‘Ã¡ng ká»ƒ liÃªn quan Ä‘áº¿n xÃ¡c minh trÃªn thiáº¿t bá»‹, LLMCad pháº£i táº­n dá»¥ng nhá»¯ng token thÆ°á»ng bá»‹ bá» qua nÃ y Ä‘á»ƒ giáº£m táº§n suáº¥t xÃ¡c minh.

â€¢Thá»i Ä‘iá»ƒm xÃ¡c minh. Má»™t khÃ­a cáº¡nh quan trá»ng khÃ¡c lÃ  xÃ¡c Ä‘á»‹nh khi nÃ o báº¯t Ä‘áº§u quÃ¡ trÃ¬nh xÃ¡c minh. XÃ¡c minh trÃªn thiáº¿t bá»‹ tá»‘n thá»i gian, vÃ­ dá»¥, máº¥t 7,1s trÃªn Jetson TX2. XÃ¡c minh quÃ¡ sá»›m hoáº·c quÃ¡ muá»™n chá»‰ lÃ£ng phÃ­ tÃ i nguyÃªn khan hiáº¿m cá»§a thiáº¿t bá»‹ di Ä‘á»™ng báº±ng xÃ¡c minh khÃ´ng há»£p lá»‡ (tá»©c lÃ  khÃ´ng phÃ¡t hiá»‡n lá»—i) hoáº·c token vÃ´ dá»¥ng. CÃ¡c cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y thÆ°á»ng dá»±a vÃ o má»™t token Ä‘Æ¡n hoáº·c Ä‘á»™ dÃ i chuá»—i token, cÃ³ thá»ƒ khÃ´ng xÃ¡c Ä‘á»‹nh chÃ­nh xÃ¡c thá»i Ä‘iá»ƒm xÃ¡c minh tá»‘i Æ°u.

â€¢Báº¥t Ä‘á»‘i xá»©ng IO so vá»›i tÃ­nh toÃ¡n. Vá»›i má»™t táº§ng LLM, viá»‡c thá»±c thi LLM lá»›n cháº·n suy luáº­n mÃ´ hÃ¬nh nhá» do sá»± phá»¥ thuá»™c token chÃ©o, vÃ  bá»™ xá»­ lÃ½ Ä‘Æ°á»£c sá»­ dá»¥ng dÆ°á»›i má»©c do I/O trá»Ÿ thÃ nh nÃºt tháº¯t cá»• chai trong quÃ¡ trÃ¬nh táº£i trá»ng sá»‘. TÃ¬nh huá»‘ng nhÆ° váº­y cáº£n trá»Ÿ nghiÃªm trá»ng tá»‘c Ä‘á»™ suy luáº­n vÃ¬ mÃ´ hÃ¬nh má»¥c tiÃªu cáº§n Ä‘Æ°á»£c gá»i khÃ´ng thá»ƒ trÃ¡nh khá»i Ä‘á»ƒ Ä‘áº£m báº£o táº¡o token chÃ­nh xÃ¡c.

Äá»ƒ Ä‘Ã¡p á»©ng, LLMCad thiáº¿t káº¿ ba ká»¹ thuáº­t má»›i:

(1) Táº¡o vÃ  xÃ¡c minh cÃ¢y token ( Â§3.2). Thay vÃ¬ táº¡o vÃ  xÃ¡c minh má»™t chuá»—i token tuyáº¿n tÃ­nh, LLMCad sá»­ dá»¥ng má»™t phÆ°Æ¡ng phÃ¡p khÃ¡c báº±ng cÃ¡ch xÃ¢y dá»±ng vÃ  xÃ¡c thá»±c má»™t "cÃ¢y token." CÃ¢y token nÃ y cho phÃ©p má»—i token cÃ³ nhiá»u token káº¿ tiáº¿p tiá»m nÄƒng. Äá»ƒ thá»±c hiá»‡n Ä‘iá»u nÃ y má»™t cÃ¡ch hiá»‡u quáº£, LLMCad sá»­ dá»¥ng ba mÃ´-Ä‘un má»›i: (1) Bá»™ Ä‘iá»u tá»‘c nhÃ¡nh dá»±a trÃªn Ä‘á»™ tin cáº­y Ä‘iá»u chá»‰nh tiáº¿n trÃ¬nh cá»§a cÃ¡c nhÃ¡nh khÃ¡c nhau Ä‘á»ƒ ngÄƒn viá»‡c phÃ¢n bá»• lÃ£ng phÃ­ tÃ i nguyÃªn tÃ­nh toÃ¡n cho nhÃ¡nh sai; (2) Bá»™ giáº£i mÃ£ cÃ¢y táº¡o token tá»« cÃ¡c nhÃ¡nh khÃ¡c nhau mÃ  khÃ´ng phÃ¡t sinh chi phÃ­ chuyá»ƒn Ä‘á»•i ngá»¯ cáº£nh giá»¯a chÃºng; (3) Bá»™ xÃ¡c minh cÃ¢y token khÃ´ng tá»± há»“i quy kiá»ƒm tra vÃ  sá»­a chá»¯a táº¥t cáº£ lá»—i trong má»™t cÃ¢y token theo cÃ¡ch hÃ ng loáº¡t, vá»›i chi phÃ­ cá»§a má»™t láº§n láº·p duy nháº¥t.

(2) Chiáº¿n lÆ°á»£c dá»± phÃ²ng tá»± thÃ­ch á»©ng ( Â§3.3). Chiáº¿n lÆ°á»£c nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ khá»Ÿi Ä‘á»™ng quÃ¡ trÃ¬nh xÃ¡c minh ká»‹p thá»i khi LLM thÆ°á»ng trÃº bá»™ nhá»› táº¡o ra má»™t token khÃ´ng chÃ­nh xÃ¡c. NÃ³ Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« hai quan sÃ¡t chÃ­nh: (1) ThÃ´ng thÆ°á»ng, má»—i token Ä‘Æ°á»£c táº¡o bá»Ÿi LLM thÆ°á»ng trÃº bá»™ nhá»› giá»›i thiá»‡u má»™t sá»‘ "báº¥t Ä‘á»‹nh" (Ä‘iá»ƒm tin cáº­y khÃ´ng hoÃ n háº£o). LLMCad sá»­ dá»¥ng má»™t chá»‰ sá»‘ chÃ­nh xÃ¡c hÆ¡n Ä‘Æ°á»£c gá»i lÃ  báº¥t Ä‘á»‹nh tÃ­ch lÅ©y trong cÃ¢y token so vá»›i. So vá»›i cÃ¡c cÃ´ng trÃ¬nh trÆ°á»›c, chá»‰ sá»‘ nÃ y pháº£n Ã¡nh tá»‘t hÆ¡n xÃ¡c suáº¥t lá»—i liÃªn quan Ä‘áº¿n viá»‡c táº¡o LLM thÆ°á»ng trÃº bá»™ nhá»›, Ä‘áº·c biá»‡t xem xÃ©t báº£n cháº¥t tÃ­ch lÅ©y cá»§a cÃ¡c mÃ´ hÃ¬nh tá»± há»“i quy. (2) Dá»¯ liá»‡u lá»‹ch sá»­ liÃªn quan Ä‘áº¿n Ä‘á»™ chÃ­nh xÃ¡c cá»§a cÃ¡c token Ä‘Ã£ xÃ¡c minh

2

--- TRANG 3 ---
LLMCad : Suy luáº­n MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n trÃªn Thiáº¿t bá»‹ Nhanh vÃ  CÃ³ thá»ƒ Má»Ÿ rá»™ng Conference'17, July 2017, Washington, DC, USA

Ä‘Æ°á»£c khai thÃ¡c Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng táº¡o cá»§a LLM thÆ°á»ng trÃº bá»™ nhá»›. Kháº£ nÄƒng táº¡o máº¡nh hÆ¡n Ä‘Ã²i há»i táº§n suáº¥t xÃ¡c minh tháº¥p hÆ¡n.

(3) ÄÆ°á»ng á»‘ng táº¡o suy Ä‘oÃ¡n ( Â§3.4). Äá»ƒ phÃ¡ vá»¡ sá»± phá»¥ thuá»™c token chÃ©o vÃ  tÄƒng cÆ°á»ng tÃ­nh song song, chÃºng tÃ´i Ä‘á» xuáº¥t táº¡o suy Ä‘oÃ¡n , tá»©c lÃ  tiáº¿p tá»¥c táº¡o token thÃ´ng qua LLM thÆ°á»ng trÃº bá»™ nhá»› trong quÃ¡ trÃ¬nh xÃ¡c minh, Äiá»u nÃ y Ä‘Æ°á»£c dá»±a trÃªn hiá»ƒu biáº¿t ráº±ng Ä‘Ã´i khi quÃ¡ trÃ¬nh xÃ¡c minh cÃ³ thá»ƒ khÃ´ng phÃ¡t hiá»‡n lá»—i, lÃ m cho cÃ¡c token Ä‘Æ°á»£c táº¡o suy Ä‘oÃ¡n cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘Æ°á»£c. Tuy nhiÃªn, viá»‡c táº¡o suy Ä‘oÃ¡n Ä‘á»“ng thá»i vá»›i xÃ¡c minh trá»±c tiáº¿p cÃ³ thá»ƒ dáº«n Ä‘áº¿n tranh cháº¥p bá»™ xá»­ lÃ½ vÃ  bá»™ nhá»›. Äá»ƒ giáº£i quyáº¿t thÃªm váº¥n Ä‘á» nÃ y, LLMCad káº¿t há»£p má»™t Ä‘Æ°á»ng á»‘ng tinh vi, Ä‘áº£m báº£o ráº±ng táº¡o suy Ä‘oÃ¡n chá»‰ cháº¡y khi táº£i tham sá»‘ LLM má»¥c tiÃªu dÆ°á»›i giá»›i háº¡n trÃªn bá»™ nhá»› Ä‘á»ƒ trÃ¡nh can thiá»‡p vÃ o quÃ¡ trÃ¬nh xÃ¡c minh thÃ´ng thÆ°á»ng.

Triá»ƒn khai vÃ  Ä‘Ã¡nh giÃ¡. ChÃºng tÃ´i Ä‘Ã£ triá»ƒn khai Ä‘áº§y Ä‘á»§ LLMCad trÃªn hai cÃ´ng cá»¥ LLM SOTA: PyTorch [ 5] vÃ  llama.cpp [ 45]. ÄÃ¡nh giÃ¡ má»Ÿ rá»™ng cá»§a há»‡ thá»‘ng Ä‘Æ°á»£c tiáº¿n hÃ nh trÃªn bá»‘n ná»n táº£ng: hai thiáº¿t bá»‹ IoT (Jetson TX2 vÃ  Jetson Orin NX) vÃ  hai Ä‘iá»‡n thoáº¡i thÃ´ng minh (Xiaomi 10 vÃ  Xiaomi 11). ÄÃ¡nh giÃ¡ nÃ y bao gá»“m sÃ¡u LLM Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i (GPT2 [ 54], T5 [ 55], mT5 [ 75], Bart [ 42], Vicuna, vÃ  LLaMa2 [ 64]) vÃ  báº£y bá»™ dá»¯ liá»‡u (CNN/Daily [ 61], Wikitext [ 47], iwlt2017 [ 19], wmt14/22 [ 15], SQuAD [ 56], parrot, vÃ  TruthfulQA [ 44]). ChÃºng tÃ´i cÅ©ng so sÃ¡nh LLMCad vá»›i nÄƒm Ä‘Æ°á»ng cÆ¡ sá»Ÿ cáº¡nh tranh tiÃªn tiáº¿n [ 5,27,37,41,45], bao gá»“m hai khung Ä‘Æ°á»ng á»‘ng tÃ­nh toÃ¡n-táº£i vÃ  hai khung há»£p tÃ¡c LLM "bá»™ táº¡o vÃ  bá»™ xÃ¡c minh". Káº¿t quáº£ cá»§a chÃºng tÃ´i chá»©ng minh rÃµ rÃ ng hiá»‡u suáº¥t vÆ°á»£t trá»™i cá»§a LLMCad . Khi so sÃ¡nh vá»›i cÃ¡c cÃ´ng cá»¥ LLM tiÃªn tiáº¿n, LLMCad cÃ³ thá»ƒ giáº£m thá»i gian táº¡o trung bÃ¬nh má»—i token 2,9â€“9,3 Ã— vÃ  3,5â€“4,7Ã— trÃªn thiáº¿t bá»‹ IoT vÃ  Ä‘iá»‡n thoáº¡i thÃ´ng minh, tÆ°Æ¡ng á»©ng, mÃ  khÃ´ng lÃ m giáº£m Ä‘á»™ chÃ­nh xÃ¡c. Äá»‘i vá»›i cÃ¡c LLM cÃ³ kÃ­ch thÆ°á»›c >10B nhÆ° LLaMA2-13B trÆ°á»›c Ä‘Ã¢y khÃ´ng thá»ƒ chá»‹u Ä‘á»±ng Ä‘Æ°á»£c trÃªn Ä‘iá»‡n thoáº¡i thÃ´ng minh, LLMCad táº¡o ra hÆ¡n má»™t token má»—i giÃ¢y. HÆ¡n ná»¯a, so vá»›i cÃ¡c Ä‘Æ°á»ng cÆ¡ sá»Ÿ cáº¡nh tranh, LLMCad cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c tÄƒng tá»‘c lÃªn Ä‘áº¿n 5,6 Ã— vÃ  Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n Ä‘Ã¡ng chÃº Ã½.

CÃ¡c Ä‘Ã³ng gÃ³p chÃ­nh cá»§a cÃ´ng trÃ¬nh nÃ y nhÆ° sau:
â€¢ChÃºng tÃ´i khÃ¡m phÃ¡ ká»¹ lÆ°á»¡ng cÃ¡c cÆ¡ há»™i vÃ  thÃ¡ch thá»©c cá»§a viá»‡c suy luáº­n LLM trÃªn thiáº¿t bá»‹.
â€¢ChÃºng tÃ´i Ä‘á» xuáº¥t LLMCad , cÃ´ng cá»¥ suy luáº­n hiá»‡u quáº£ Ä‘áº§u tiÃªn cho cÃ¡c tÃ¡c vá»¥ NLP sinh trÃªn thiáº¿t bá»‹. Äá»ƒ tÄƒng tá»‘c quy trÃ¬nh táº¡o, LLMCad sá»­ dá»¥ng há»£p tÃ¡c LLM "bá»™ táº¡o vÃ  bá»™ xÃ¡c minh" vÃ  káº¿t há»£p ba ká»¹ thuáº­t má»›i: táº¡o vÃ  xÃ¡c minh cÃ¢y, chiáº¿n lÆ°á»£c dá»± phÃ²ng tá»± thÃ­ch á»©ng, vÃ  Ä‘Æ°á»ng á»‘ng táº¡o suy Ä‘oÃ¡n. Nhá»¯ng tiáº¿n bá»™ nÃ y cho phÃ©p LLMCad giáº£m thiá»ƒu hiá»‡u quáº£ váº¥n Ä‘á» rÃ o cáº£n bá»™ nhá»›.

Masked Multi Self Attention 
LayerNorm
Feed Forward
LayerNormN      Lá»›p Decoder  Äáº§u vÃ o
Embedding vÄƒn báº£nEmbedding vá»‹ trÃ­
Äáº§u ra(a) Kiáº¿n trÃºc GPT3

Lá»›p Decoder  
Lá»›p Decoder  
Lá»›p Decoder  Youshould
wearwear
shoesshoes
<EOS>iter0 iter1 iter2
...MÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n
Lá»›p Decoder  (b) Suy luáº­n tá»± há»“i quy

HÃ¬nh 2: Kiáº¿n trÃºc cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯ chá»‰ giáº£i mÃ£ (GPT3) vÃ  tá»•ng quan vá» máº«u suy luáº­n LLM: tá»± há»“i quy.

â€¢ChÃºng tÃ´i táº¡o nguyÃªn máº«u LLMCad vÃ  Ä‘Ã¡nh giÃ¡ nÃ³ vá»›i cÃ¡c LLM Ä‘áº¡i diá»‡n vÃ  thiáº¿t bá»‹ di Ä‘á»™ng thÆ°Æ¡ng máº¡i. Káº¿t quáº£ chá»©ng minh hiá»‡u suáº¥t vÆ°á»£t trá»™i cá»§a nÃ³ so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n cÃ³.

2 Bá»I Cáº¢NH VÃ€ Äá»˜NG Lá»°C

2.1 LLM Sinh dá»±a trÃªn Decoder

CÃ¡c tÃ¡c vá»¥ ML sinh trÃªn thiáº¿t bá»‹. CÃ¡c tÃ¡c vá»¥ sinh lÃ  nhá»¯ng tÃ¡c vá»¥ liÃªn quan Ä‘áº¿n viá»‡c táº¡o ra hoáº·c tá»•ng há»£p tá»± Ä‘á»™ng ná»™i dung má»›i nhÆ° chuá»—i vÄƒn báº£n vÃ  pixel hÃ¬nh áº£nh [ 14,58,59]. CÃ¡c tÃ¡c vá»¥ sinh Ä‘iá»ƒn hÃ¬nh trong lÄ©nh vá»±c NLP (trá»ng tÃ¢m cá»§a cÃ´ng trÃ¬nh nÃ y) bao gá»“m mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯, dá»‹ch mÃ¡y, tÃ³m táº¯t vÃ  tráº£ lá»i cÃ¢u há»i. Trá»ng tÃ¢m chÃ­nh lÃ  cung cáº¥p Ä‘áº§u ra ná»™i dung má»›i, cÃ³ Ã½ nghÄ©a vÃ  máº¡ch láº¡c. So vá»›i cÃ¡c tÃ¡c vá»¥ phÃ¢n loáº¡i truyá»n thá»‘ng nhÆ° phÃ¢n loáº¡i chá»§ Ä‘á» vÃ  phÃ¢n loáº¡i hÃ¬nh áº£nh, cÃ¡c tÃ¡c vá»¥ sinh thÆ°á»ng thÃ¡ch thá»©c hÆ¡n nhÆ°ng cÅ©ng sÃ¢u sáº¯c hÆ¡n Ä‘á»‘i vá»›i cuá»™c sá»‘ng con ngÆ°á»i [63].

CÃ¡c tÃ¡c vá»¥ ML sinh Ä‘Ã£ Ä‘Æ°á»£c phá»¥c vá»¥ rá»™ng rÃ£i cho ngÆ°á»i dÃ¹ng di Ä‘á»™ng, cháº³ng háº¡n nhÆ° mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯ cho Google Gboard [ 1], tráº£ lá»i cÃ¢u há»i cho Siri [ 6], vÃ  cÃ¡c dá»‹ch vá»¥ dá»‹ch thuáº­t nhÆ° iTranslate [ 3] vÃ  Google Translate [ 2], v.v. Äá»ƒ Ä‘áº£m báº£o quyá»n riÃªng tÆ° dá»¯ liá»‡u ngÆ°á»i dÃ¹ng (vÃ­ dá»¥, kho vÄƒn báº£n) vÃ  tÃ­nh kháº£ dá»¥ng dá»‹ch vá»¥, cÃ¡c mÃ´ hÃ¬nh tá»‘t hÆ¡n lÃ  Ä‘Æ°á»£c triá»ƒn khai trá»±c tiáº¿p trÃªn thiáº¿t bá»‹ Ä‘á»ƒ suy luáº­n cá»¥c bá»™.

Kiáº¿n trÃºc LLM dá»±a trÃªn Decoder. MÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM) dá»±a trÃªn decoder, bao gá»“m cáº£ kiáº¿n trÃºc chá»‰ decoder vÃ  encoder-decoder, lÃ  phÆ°Æ¡ng phÃ¡p tiÃªu chuáº©n cho cÃ¡c tÃ¡c vá»¥ sinh, nhÆ° GPT-3 [ 17], LLaMa [ 64], vÃ  GLM-130B [ 23,79]. NhÆ° Ä‘Æ°á»£c thá»ƒ hiá»‡n trong HÃ¬nh 2(a), má»™t LLM dá»±a trÃªn decoder Ä‘iá»ƒn hÃ¬nh bao gá»“m má»™t embedding vÄƒn báº£n, má»™t embedding vá»‹ trÃ­, vÃ  nhiá»u lá»›p decoder Ä‘Æ°á»£c xáº¿p chá»“ng tuáº§n tá»±, trong Ä‘Ã³ má»—i lá»›p decoder bao gá»“m self-attention cÃ³ máº·t náº¡, LayerNorm, vÃ  cÃ¡c phÃ©p toÃ¡n Tuyáº¿n tÃ­nh. Äá»‘i vá»›i nhá»¯ng LLM encoder-decoder nhÆ° T5 [ 55] vÃ  mT5 [ 75], cÃ¡c lá»›p encoder Ä‘Æ°á»£c káº¿t há»£p trÆ°á»›c decoder Ä‘á»ƒ tÄƒng cÆ°á»ng kháº£ nÄƒng hiá»ƒu ngá»¯ nghÄ©a.

3

--- TRANG 4 ---
Conference'17, July 2017, Washington, DC, USA Daliang Xuâ™¦, Wangsong Yinâ™¦, Xin Jinâ™¦, Ying Zhangâ™¦, Shiyun Weiâ‹†, Mengwei Xu^, Xuanzhe Liuâ™¦

Suy luáº­n tá»± há»“i quy. CÃ¡c LLM sinh sá»­ dá»¥ng má»™t quy trÃ¬nh suy luáº­n tá»± há»“i quy táº¡o ra má»™t token táº¡i má»™t thá»i Ä‘iá»ƒm vÃ  láº¥y token Ä‘Ã³ lÃ m Ä‘áº§u vÃ o Ä‘á»ƒ táº¡o ra token tiáº¿p theo. VÃ­ dá»¥, HÃ¬nh 2(b) minh há»a má»™t quy trÃ¬nh suy luáº­n ba láº§n láº·p tá»± há»“i quy. Trong láº§n láº·p thá»© 1, mÃ´ hÃ¬nh láº¥y táº¥t cáº£ cÃ¡c token hiá»‡n cÃ³ ("You should") lÃ m Ä‘áº§u vÃ o vÃ  táº¡o ra Ä‘áº§u ra "wear." Trong láº§n láº·p tiáº¿p theo, "wear" má»›i Ä‘Æ°á»£c táº¡o sáº½ Ä‘Æ°á»£c Ä‘Æ°a vÃ o mÃ´ hÃ¬nh, sau Ä‘Ã³ dá»± Ä‘oÃ¡n "shoes." QuÃ¡ trÃ¬nh nÃ y tiáº¿p tá»¥c cho Ä‘áº¿n khi mÃ´ hÃ¬nh táº¡o ra token káº¿t thÃºc chuá»—i ( <ğ¸ğ‘‚ğ‘†>), chá»‰ ra sá»± káº¿t thÃºc cá»§a quy trÃ¬nh táº¡o. Báº£n cháº¥t cá»§a suy luáº­n tá»± há»“i quy giá»›i thiá»‡u cÃ¡c thÃ¡ch thá»©c Ä‘á»™c Ä‘Ã¡o Ä‘á»ƒ tá»‘i Æ°u hÃ³a LLM trÃªn thiáº¿t bá»‹ nhÆ° sáº½ Ä‘Æ°á»£c mÃ´ táº£ sau.

2.2 LLM trÃªn thiáº¿t bá»‹ bá»‹ Giá»›i háº¡n bá»Ÿi Bá»™ nhá»›

Trong pháº§n nÃ y, chÃºng tÃ´i thá»±c hiá»‡n cÃ¡c thÃ­ nghiá»‡m thÃ­ Ä‘iá»ƒm Ä‘á»ƒ tiáº¿t lá»™ váº¥n Ä‘á» hiá»‡u suáº¥t cá»§a suy luáº­n LLM trÃªn thiáº¿t bá»‹. CÃ¡c thÃ­ nghiá»‡m Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn cÃ¡c LLM Ä‘iá»ƒn hÃ¬nh (GPT2, T5, vÃ  LLaMa), bá»™ dá»¯ liá»‡u (SQuAD vÃ  TriviaQA), vÃ  thiáº¿t bá»‹ di Ä‘á»™ng (Jetson TX2 vÃ  Xiaomi 10) sá»­ dá»¥ng cÃ¡c cÃ´ng cá»¥ DL tiÃªn tiáº¿n (PyTorch [ 5] vÃ  llama.cpp [ 45]. ChÃºng tÃ´i tÃ³m táº¯t nhá»¯ng phÃ¡t hiá»‡n chÃ­nh dÆ°á»›i Ä‘Ã¢y.

Má»Ÿ rá»™ng kÃ­ch thÆ°á»›c tham sá»‘ mang láº¡i cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c. Kiáº¿n trÃºc LLM dá»±a trÃªn Transformer cÃ³ tÃ­nh linh hoáº¡t vÃ  kháº£ nÄƒng má»Ÿ rá»™ng cao báº±ng cÃ¡ch Ä‘Æ¡n giáº£n Ä‘iá»u chá»‰nh cÃ¡c lá»›p encoder/decoder, Ä‘á»™ dÃ i chuá»—i, vÃ  cÃ¡c siÃªu tham sá»‘ khÃ¡c. Do Ä‘Ã³, LLM phá»• biáº¿n thÆ°á»ng Ä‘Æ°á»£c phÃ¡t triá»ƒn vá»›i má»™t loáº¡t cÃ¡c biáº¿n thá»ƒ mÃ´ hÃ¬nh, nhÆ° T5-Small/Base/Large [ 55] vÃ  LLaMa-7B/13B/33B/65B [ 64]. Vá»›i kÃ­ch thÆ°á»›c tham sá»‘ má»Ÿ rá»™ng, mÃ´ hÃ¬nh thá»ƒ hiá»‡n kháº£ nÄƒng máº¡nh hÆ¡n. NhÆ° Ä‘Æ°á»£c thá»ƒ hiá»‡n trong Báº£ng 1, T5-Large vÆ°á»£t trá»™i hÆ¡n T5-Small vá»›i biÃªn Ä‘á»™ Ä‘Ã¡ng ká»ƒ, Ä‘áº¡t Ä‘Æ°á»£c cáº£i thiá»‡n 7,6% vá» Ä‘á»™ chÃ­nh xÃ¡c trÃªn bá»™ dá»¯ liá»‡u SQuAD. TÆ°Æ¡ng tá»±, LLaMa-13B chá»©ng minh Ä‘á»™ chÃ­nh xÃ¡c QA cao hÆ¡n 6,6% trÃªn TriviaQA so vá»›i LLaMa-7B. Tháº­t váº­y, hiá»‡n tÆ°á»£ng nhÆ° váº­y Ä‘Æ°á»£c biáº¿t Ä‘áº¿n rá»™ng rÃ£i trong cá»™ng Ä‘á»“ng ML lÃ  quy luáº­t tá»· lá»‡ [11, 12, 21, 34].

Kháº£ nÄƒng má»Ÿ rá»™ng LLM trÃªn thiáº¿t bá»‹ cáº£n trá»Ÿ trÃªn rÃ o cáº£n bá»™ nhá»›. Tuy nhiÃªn, nhÆ° Ä‘Æ°á»£c thá»ƒ hiá»‡n trong Báº£ng 1, tá»‘c Ä‘á»™ suy luáº­n giáº£m nhanh chÃ³ng khi má»©c tiÃªu thá»¥ bá»™ nhá»› vÆ°á»£t quÃ¡ ngÃ¢n sÃ¡ch bá»™ nhá»›. VÃ­ dá»¥, trÃªn thiáº¿t bá»‹ TX2, Ä‘á»™ trá»… suy luáº­n tÄƒng 189â€“224 Ã— vá»›i chá»‰ tÄƒng 5,8â€“12,2Ã— vá» kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh. Äá»ƒ hiá»ƒu sÃ¢u hÆ¡n vá» cÃ¡c yáº¿u tá»‘ áº£nh hÆ°á»Ÿng Ä‘áº¿n tá»‘c Ä‘á»™ suy luáº­n, chÃºng tÃ´i Ä‘Ã£ tiáº¿n hÃ nh phÃ¢n tÃ­ch phÃ¢n tÃ¡ch, nhÆ° Ä‘Æ°á»£c thá»ƒ hiá»‡n trong HÃ¬nh 3. NÃ³ cho tháº¥y rÃµ rÃ ng ráº±ng, khi suy luáº­n mÃ´ hÃ¬nh Ä‘Ã²i há»i kÃ­ch thÆ°á»›c bá»™ nhá»› khÃ´ng thá»ƒ chi tráº£ trÃªn thiáº¿t bá»‹ edge, viá»‡c táº£i tham sá»‘ tá»« Ä‘Ä©a vÃ o bá»™ nhá»› (tá»©c lÃ  disk I/O) sá»›m trá»Ÿ thÃ nh nÃºt tháº¯t cá»• chai cá»§a thá»i gian suy luáº­n (95,9â€“98,8%). TÃ¬nh huá»‘ng nÃ y lÃ  do cÃ¡c cÃ´ng cá»¥ LLM phÃ­a thiáº¿t bá»‹ tiÃªn tiáº¿n, nhÆ°

(a) TrÃªn TriviaQA vá»›i Xiaomi10
# Tham sá»‘ (B) Äá»™ chÃ­nh xÃ¡c PeakMem. (GB) Thá»i gian Suy luáº­n (ms)
LLaMa-7B (4bits) 7 50.0 4.1 275 (1x)
LLaMa-13B (4bits) 13 56.6 9.8 10118 (37x)
LLaMa-33B (4bits) 33 65.1 20.8 22017 (87x)

(b) TrÃªn SQuAD vá»›i TX2
# Tham sá»‘ (B) Äá»™ chÃ­nh xÃ¡c PeakMem. (GB) Thá»i gian Suy luáº­n (ms)
GPT2 0.14 49.8 0.5 37.64 (1x)
GPT2-Large 0.8 53.7 3.1 8065 (214x)
mT5-Small 0.3 76.4 0.7 31 (1x)
mT5-Base 0.58 83.8 1.5 2134 (69x)
mT5-Large 1.2 87.0 3.9 7214 (230x)
T5-Small 0.06 79.1 0.2 27 (1x)
T5-Base 0.22 85.4 0.8 37 (1.4x)
T5-Large 0.73 86.7 2.8 7098 (263x)

Báº£ng 1: KÃ­ch thÆ°á»›c tham sá»‘, Ä‘á»™ chÃ­nh xÃ¡c, sá»­ dá»¥ng bá»™ nhá»› Ä‘á»‰nh, vÃ  Ä‘á»™ trá»… suy luáº­n cá»§a cÃ¡c biáº¿n thá»ƒ LLM trong má»™t láº§n láº·p tá»± há»“i quy. (a) thÃ­ nghiá»‡m Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn PyTorch trong khi (b) thÃ­ nghiá»‡m Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn llama.cpp.

100101102
Tá»· lá»‡ pháº§n trÄƒm (%)SmallBaseLargeT5
100.0%100.0%1.2% 98.8%tÃ­nh toÃ¡n táº£i

100101102
Tá»· lá»‡ pháº§n trÄƒm (%)SmallBaseLargemT5100.0%2.7% 97.3%2.3% 97.7%

100101102
Tá»· lá»‡ pháº§n trÄƒm (%)BaseLargeGPT2 100.0%1.7% 98.3%

100101102
Tá»· lá»‡ pháº§n trÄƒm (%)7B13BLLaMa100.0%4.1% 95.9%

HÃ¬nh 3: PhÃ¢n tÃ¡ch Ä‘á»™ trá»… suy luáº­n cá»§a cÃ¡c biáº¿n thá»ƒ LLM khÃ¡c nhau trong má»™t láº§n láº·p tá»± há»“i quy.

MNN [ 4] vÃ  llama.cpp [ 45], sá»­ dá»¥ng ká»¹ thuáº­t hoÃ¡n Ä‘á»•i tá»± Ä‘á»™ng giáº£i phÃ³ng bá»™ nhá»› trá»ng sá»‘ Ä‘Ã£ suy luáº­n vÃ  táº£i trá»ng sá»‘ cáº§n suy luáº­n tá»« Ä‘Ä©a khi rÃ ng buá»™c bá»™ nhá»› bá»‹ vÆ°á»£t quÃ¡.

Báº£n cháº¥t tá»± há»“i quy lÃ m cho cÃ¡c tá»‘i Æ°u hÃ³a bá»™ nhá»› truyá»n thá»‘ng háº§u nhÆ° khÃ´ng hiá»‡u quáº£ Ä‘á»‘i vá»›i LLM sinh. ÄÃ¡ng chÃº Ã½ ráº±ng tá»‘i Æ°u hÃ³a bá»™ nhá»› cho suy luáº­n mÃ´ hÃ¬nh Ä‘Ã£ lÃ  má»™t chá»§ Ä‘á» Ä‘Æ°á»£c nghiÃªn cá»©u ká»¹ lÆ°á»¡ng trong nhá»¯ng nÄƒm gáº§n Ä‘Ã¢y [ 27,32, 46,78]. Nhiá»u phÆ°Æ¡ng phÃ¡p cáº¥p Ä‘á»™ há»‡ thá»‘ng Ä‘Ã£ Ä‘Æ°á»£c khÃ¡m phÃ¡, nhÆ° xá»­ lÃ½ hÃ ng loáº¡t [ 78], Ä‘Æ°á»ng á»‘ng tÃ­nh toÃ¡n-I/O [ 27], vÃ  hoÃ¡n Ä‘á»•i thÃ´ng minh [ 32,46]. Tuy nhiÃªn, nhá»¯ng cÃ´ng trÃ¬nh nÃ y khÃ³ cÃ³ thá»ƒ Ã¡p dá»¥ng cho LLM trÃªn thiáº¿t bá»‹, bá»Ÿi vÃ¬: (1) TÃ­nh song song/xá»­ lÃ½ hÃ ng loáº¡t khÃ´ng kháº£ dá»¥ng vÃ¬ suy luáº­n tá»± há»“i quy yÃªu cáº§u táº¡o token tuáº§n tá»±; (2) Chá»“ng chÃ©o cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c lá»£i Ã­ch háº¡n cháº¿ vÃ¬ thá»i gian I/O lá»›n hÆ¡n hÃ ng trÄƒm láº§n so vá»›i tÃ­nh toÃ¡n. NgoÃ i ra, cÃ¡c phÆ°Æ¡ng phÃ¡p thuáº­t toÃ¡n nhÆ° lÆ°á»£ng tá»­ hÃ³a [ 25,31,66,76] cÃ³ thá»ƒ mang láº¡i giáº£m bá»™ nhá»› vÃ i láº§n (vÃ­ dá»¥, FP16âˆ’ â†’INT4 [ 25]), hiá»‡u quáº£ cá»§a chÃºng bá»‹ háº¡n cháº¿ vÃ¬ Ä‘á»™ chÃ­nh xÃ¡c bit tháº¥p (vÃ­ dá»¥, 2 bit) Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh lÃ  khÃ´ng Ä‘á»§ Ä‘á»ƒ giá»¯ láº¡i kháº£ nÄƒng mÃ´ hÃ¬nh [ 25,36,76]. LÆ°u Ã½ ráº±ng

4

--- TRANG 5 ---
LLMCad : Suy luáº­n MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n trÃªn Thiáº¿t bá»‹ Nhanh vÃ  CÃ³ thá»ƒ Má»Ÿ rá»™ng Conference'17, July 2017, Washington, DC, USA

Ä‘á» xuáº¥t cá»§a cÃ´ng trÃ¬nh nÃ y á»Ÿ cáº¥p Ä‘á»™ há»‡ thá»‘ng vÃ  tÆ°Æ¡ng thÃ­ch vá»›i lÆ°á»£ng tá»­ hÃ³a.

2.3 CÆ¡ há»™i vÃ  ThÃ¡ch thá»©c cá»§a Há»£p tÃ¡c LLM

CÃ´ng trÃ¬nh nÃ y táº­p trung vÃ o phÆ°Æ¡ng phÃ¡p há»£p tÃ¡c mÃ´ hÃ¬nh [ 40,60, 70], táº­n dá»¥ng nhiá»u mÃ´ hÃ¬nh vá»›i sá»± Ä‘Ã¡nh Ä‘á»•i Ä‘á»™ chÃ­nh xÃ¡c-chi phÃ­ Ä‘á»ƒ tÄƒng tá»‘c suy luáº­n. Trong trÆ°á»ng há»£p cÃ¡c tÃ¡c vá»¥ NLP sinh, chÃºng tÃ´i á»§y thÃ¡c háº§u háº¿t cÃ¡c tÃ­nh toÃ¡n (tá»©c lÃ  token) cho má»™t mÃ´ hÃ¬nh nhá» hÆ¡n hoÃ n toÃ n phÃ¹ há»£p vá»›i ngÃ¢n sÃ¡ch bá»™ nhá»›. LÃ½ do chÃ­nh lÃ  cÃ¡c mÃ´ hÃ¬nh nhá» hÆ¡n cÃ³ thá»ƒ thá»ƒ hiá»‡n hiá»‡u suáº¥t gáº§n vá»›i mÃ´ hÃ¬nh lá»›n, Ä‘áº·c biá»‡t Ä‘á»‘i vá»›i cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u dá»… hÆ¡n [ 24,29]. CÃ¡c thÃ­ nghiá»‡m thá»±c nghiá»‡m cá»§a chÃºng tÃ´i Ä‘Ã£ xÃ¡c nháº­n giáº£ Ä‘á»‹nh nhÆ° váº­y: trong bá»™ dá»¯ liá»‡u dá»‹ch iwslt2017 de-en [ 19], mT5-Small táº¡o ra chÃ­nh xÃ¡c hÆ¡n 80% token nhÆ° mT5-Large lÃ m. HÃ¬nh 4 Ä‘Æ°a ra má»™t vÃ­ dá»¥ cá»¥ thá»ƒ vá» dá»‹ch má»™t cÃ¢u bá»Ÿi mÃ´ hÃ¬nh nhá» hÆ¡n vÃ  LLM lá»›n hÆ¡n, vÃ  chuáº©n má»±c tÆ°Æ¡ng á»©ng. NÃ³ cho tháº¥y ráº±ng háº§u háº¿t cÃ¡c token (mÃ u xanh lÃ¡) Ä‘Æ°á»£c táº¡o bá»Ÿi mÃ´ hÃ¬nh nhá» lÃ  chÃ­nh xÃ¡c.

Tuy nhiÃªn, viá»‡c sá»­ dá»¥ng há»£p tÃ¡c mÃ´ hÃ¬nh cho LLM Ä‘á»‘i máº·t vá»›i má»™t thÃ¡ch thá»©c quan trá»ng: á»¦y thÃ¡c token sai cÃ³ thá»ƒ lÃ  cháº¿t ngÆ°á»i. Táº§ng mÃ´ hÃ¬nh truyá»n thá»‘ng dá»±a vÃ o kiáº¿n thá»©c ná»™i bá»™ [ 60] hoáº·c bÃªn ngoÃ i [ 40,70] Ä‘á»ƒ chá»n má»™t pháº§n dá»¯ liá»‡u (thÆ°á»ng lÃ  nhá»¯ng cÃ¡i dá»… hÆ¡n) Ä‘Æ°á»£c xá»­ lÃ½ bá»Ÿi mÃ´ hÃ¬nh nhá» hÆ¡n. Trong hoÃ n cáº£nh nhÆ° váº­y, Ä‘á»™ chÃ­nh xÃ¡c khÃ´ng thá»ƒ Ä‘Æ°á»£c Ä‘áº£m báº£o. Äá»‘i vá»›i cÃ¡c tÃ¡c vá»¥ NLP sinh, tuy nhiÃªn, má»™t token Ä‘Æ°á»£c táº¡o sai bá»Ÿi mÃ´ hÃ¬nh nhá» cÃ³ thá»ƒ lan truyá»n lá»—i Ä‘áº¿n nhá»¯ng token tiáº¿p theo vÃ  cuá»‘i cÃ¹ng gÃ¢y ra káº¿t quáº£ tháº£m khá»‘c do báº£n cháº¥t tá»± há»“i quy cá»§a nÃ³ [ 53,55,65]. VÃ­ dá»¥, trong HÃ¬nh 4, "ice" thá»© hai mÃ u Ä‘á» lÃ  khÃ´ng chÃ­nh xÃ¡c, dáº«n Ä‘áº¿n hai láº§n táº¡o "ice" bá»• sung vÃ  thÃ´ng tin dá»‹ch sai trong cÃ¡c token tiáº¿p theo. LÆ°u Ã½ ráº±ng cÃ¡c tÃ¡c vá»¥ NLP sinh thÆ°á»ng nháº¡y cáº£m vá» Ä‘á»™ chÃ­nh xÃ¡c, vÃ­ dá»¥, dá»‹ch thuáº­t vÃ  Q/A, vÃ¬ má»™t káº¿t quáº£ Ä‘Æ°á»£c táº¡o sai cÃ³ thá»ƒ lÃ m sai lá»‡ch ngÆ°á»i dÃ¹ng vÃ  dáº«n Ä‘áº¿n hÃ nh vi báº¥t ngá».

Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, LLMCad sá»­ dá»¥ng má»™t hÃ¬nh thá»©c há»£p tÃ¡c mÃ´ hÃ¬nh Ä‘á»™c Ä‘Ã¡o, Ä‘Æ°á»£c gá»i lÃ  "táº¡o-rá»“i-xÃ¡c minh" [ 20,41]. Trong phÆ°Æ¡ng phÃ¡p nÃ y, LLM thÆ°á»ng trÃº bá»™ nhá»› Ä‘Ã³ng vai trÃ² lÃ  bá»™ táº¡o token, trong khi LLM má»¥c tiÃªu hoáº¡t Ä‘á»™ng nhÆ° bá»™ xÃ¡c minh, sá»­ dá»¥ng Ä‘áº§u ra cá»§a nÃ³ lÃ m chuáº©n má»±c Ä‘á»ƒ kiá»ƒm tra vÃ  sá»­a chá»¯a báº¥t ká»³ lá»—i nÃ o Ä‘Æ°á»£c giá»›i thiá»‡u trong quÃ¡ trÃ¬nh táº¡o token. Báº±ng cÃ¡ch lÃ m nhÆ° váº­y, LLMCad cÃ³ thá»ƒ ngÄƒn cháº·n lan truyá»n lá»—i cá»§a nÃ³ Ä‘áº¿n cÃ¡c token tiáº¿p theo do báº£n cháº¥t tá»± há»“i quy vÃ  Ä‘áº£m báº£o khÃ´ng lÃ m giáº£m Ä‘á»™ chÃ­nh xÃ¡c.

3 THIáº¾T Káº¾

3.1 Tá»•ng quan

LLMCad Ä‘Æ°á»£c xÃ¢y dá»±ng trÃªn hai LLM: má»™t mÃ´ hÃ¬nh má»¥c tiÃªu chÃ­nh xÃ¡c nhÆ°ng náº·ng (khÃ´ng thá»ƒ vá»«a vá»›i bá»™ nhá»› thiáº¿t bá»‹) nhÆ° mT5-large; vÃ  má»™t mÃ´ hÃ¬nh thÆ°á»ng trÃº bá»™ nhá»› Ã­t chÃ­nh xÃ¡c hÆ¡n nhÆ°ng nháº¹ nhÆ° mT5-small. Má»¥c tiÃªu thiáº¿t káº¿ cá»§a LLMCad lÃ  táº¡o vÄƒn báº£n vá»›i tá»‘c Ä‘á»™ cá»§a mÃ´ hÃ¬nh thÆ°á»ng trÃº bá»™ nhá»› mÃ  khÃ´ng lÃ m giáº£m Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh má»¥c tiÃªu (lá»›n hÆ¡n).

Luá»“ng cÃ´ng viá»‡c Ä‘Æ¡n giáº£n hÃ³a vÃ  má»™t vÃ­ dá»¥ minh há»a HÃ¬nh 5 minh há»a luá»“ng cÃ´ng viá»‡c cá»§a LLMCad . HÃ¬nh 6 cÅ©ng cung cáº¥p má»™t vÃ­ dá»¥ minh há»a dá»±a trÃªn trÆ°á»ng há»£p cá»§a HÃ¬nh 4 Ä‘á»ƒ minh há»a luá»“ng cÃ´ng viá»‡c. Vá» cÆ¡ báº£n, LLMCad lÃ  má»™t khung táº¡o vÃ  xÃ¡c minh sá»­ dá»¥ng LLM thÆ°á»ng trÃº bá»™ nhá»› lÃ m bá»™ táº¡o, vÃ  LLM má»¥c tiÃªu lÃ m bá»™ xÃ¡c minh.

Äáº§u tiÃªn, LLMCad Ä‘Æ°a vÄƒn báº£n Ä‘áº§u vÃ o vÃ o mÃ´ hÃ¬nh thÆ°á»ng trÃº bá»™ nhá»› vÃ  táº¡o ra má»™t cÃ¢y token . Má»™t cÃ¢y token lÃ  káº¿t quáº£ trung gian Ä‘Æ°á»£c táº¡o bá»Ÿi mÃ´ hÃ¬nh thÆ°á»ng trÃº bá»™ nhá»› (chi tiáº¿t trong táº¡o cÃ¢y Â§3.2). KhÃ´ng giá»‘ng nhÆ° má»™t chuá»—i token trong Ä‘Ã³ má»—i token chá»‰ cÃ³ má»™t token káº¿ tiáº¿p duy nháº¥t, má»™t token trong cÃ¢y token cÃ³ thá»ƒ cÃ³ nhiá»u token káº¿ tiáº¿p á»©ng cá»­ viÃªn, nhÆ° Ä‘Æ°á»£c thá»ƒ hiá»‡n trong HÃ¬nh 6 1â—‹. Má»—i token á»©ng cá»­ viÃªn Ä‘áº¡i diá»‡n cho má»™t chuá»—i token á»©ng cá»­ viÃªn (Ä‘Æ°á»£c gá»i lÃ  má»™t nhÃ¡nh ). Äiá»u nÃ y dá»±a trÃªn quan sÃ¡t ráº±ng Ä‘Ã´i khi cÃ¡c token "khÃ´ng tá»‘i Æ°u" Ä‘Æ°á»£c táº¡o bá»Ÿi LLM thÆ°á»ng trÃº bá»™ nhá»› lÃ  Ä‘áº§u ra thá»±c cá»§a LLM má»¥c tiÃªu, vÃ­ dá»¥, token thay tháº¿ "cap". Trong thá»±c táº¿, báº¥t ká»³ token á»©ng cá»­ viÃªn nÃ o cÃ³ Ä‘á»™ tin cáº­y cao hÆ¡n ngÆ°á»¡ng (vÃ­ dá»¥, 30%) Ä‘á»u táº¡o ra má»™t nhÃ¡nh.

Má»—i token Ä‘Æ°á»£c táº¡o bá»Ÿi LLM thÆ°á»ng trÃº bá»™ nhá»› giá»›i thiá»‡u má»™t sá»‘ "báº¥t Ä‘á»‹nh" (Ä‘iá»ƒm tin cáº­y khÃ´ng hoÃ n háº£o). Má»™t khi báº¥t Ä‘á»‹nh nhÆ° váº­y tÃ­ch lÅ©y Ä‘áº¿n má»™t má»©c trong cÃ¢u Ä‘áº§u ra, LLM má»¥c tiÃªu Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xÃ¡c minh táº¥t cáº£ cÃ¡c nhÃ¡nh Ä‘Æ°á»£c táº¡o ká»ƒ tá»« láº§n xÃ¡c minh cuá»‘i cÃ¹ng, nhÆ° Ä‘Æ°á»£c thá»ƒ hiá»‡n trong HÃ¬nh 6 2â—‹. ÄÃ¡ng chÃº Ã½, viá»‡c xÃ¡c minh ğ‘ token cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n trong má»™t láº§n suy luáº­n vá»›i LLM má»¥c tiÃªu, do Ä‘Ã³ nhanh hÆ¡n nhiá»u so vá»›i viá»‡c sá»­ dá»¥ng nÃ³ Ä‘á»ƒ táº¡o má»™t token báº±ng ğ‘ láº§n. quÃ¡ trÃ¬nh xÃ¡c minh nhÆ° váº­y do Ä‘Ã³ Ä‘Æ°á»£c gá»i lÃ  "khÃ´ng tá»± há»“i quy". Má»™t khi phÃ¡t hiá»‡n lá»—i, LLMCad sáº½ quay láº¡i cÃ¢y token vÃ  sá»­a chá»¯a nÃ³. Chi tiáº¿t vá» chiáº¿n lÆ°á»£c xÃ¡c minh vÃ  quay láº¡i Ä‘Æ°á»£c tháº£o luáº­n trong Â§3.2.

QuÃ¡ trÃ¬nh xÃ¡c minh bao gá»“m suy luáº­n LLM má»¥c tiÃªu, do Ä‘Ã³ bá»‹ giá»›i háº¡n I/O nhÆ° Ä‘Ã£ thá»ƒ hiá»‡n trÆ°á»›c Ä‘Ã³ trong Â§2.3. LLMCad Ä‘á» xuáº¥t thÃªm Táº¡o suy Ä‘oÃ¡n Ä‘á»ƒ khai thÃ¡c cÃ¡c tÃ i nguyÃªn pháº§n cá»©ng chÆ°a Ä‘Æ°á»£c sá»­ dá»¥ng báº±ng cÃ¡ch táº¡o token má»™t cÃ¡ch suy Ä‘oÃ¡n (Â§3.4), tá»©c lÃ  tiáº¿p tá»¥c táº¡o token thÃ´ng qua mÃ´ hÃ¬nh thÆ°á»ng trÃº bá»™ nhá»› trong quÃ¡ trÃ¬nh xÃ¡c minh, nhÆ° Ä‘Æ°á»£c thá»ƒ hiá»‡n trong HÃ¬nh 6 3â—‹ cÃ¡c há»™p Ä‘á»©t nÃ©t á»Ÿ phÃ­a bÃªn trÃ¡i cá»§a Ä‘Æ°á»ng Ä‘á»©t nÃ©t Ä‘á». PhÆ°Æ¡ng phÃ¡p nÃ y dá»±a trÃªn hiá»ƒu biáº¿t ráº±ng Ä‘Ã´i khi viá»‡c xÃ¡c minh khÃ´ng phÃ¡t hiá»‡n lá»—i nÃªn cÃ¡c token Ä‘Æ°á»£c táº¡o suy Ä‘oÃ¡n cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng sau Ä‘Ã³. Do Ä‘Ã³, nÃ³ cÃ³ thá»ƒ áº©n hiá»‡u quáº£ Ä‘á»™ trá»… thá»±c thi dÆ°á»›i I/O, vÃ­ dá»¥, nhÃ¡nh thá»© hai trong HÃ¬nh 6 3â—‹.

LLMCad láº·p láº¡i quÃ¡ trÃ¬nh táº¡o vÃ  xÃ¡c minh trÃªn cho Ä‘áº¿n khi gáº·p "<EOS>", token káº¿t thÃºc.

5

--- TRANG 6 ---
Conference'17, July 2017, Washington, DC, USA Daliang Xuâ™¦, Wangsong Yinâ™¦, Xin Jinâ™¦, Ying Zhangâ™¦, Shiyun Weiâ‹†, Mengwei Xu^, Xuanzhe Liuâ™¦

Last year , I showed these two slides to illustrate that the Arctic  ice ice ice ice,  which has shrinked for nearly three million years, is 40 percent
Last year , I showed these two slides to illustrate that the Arctic ice cap, which for about three million years has shrunk by 40 percent.
Last year I showed these two slides to illustrate that the arctic ice cap, which for most of the last three million years  has shrunk by 40 percent.Nhá» hÆ¡n:
Lá»›n hÆ¡n:
NhÃ£n:... that the Arctic  ice cap, which has shrinked for ...Token tin cáº­y cao thá»© 2

HÃ¬nh 4: Má»™t vÃ­ dá»¥ táº¡o dá»‹ch tá»« cÃ¡c mÃ´ hÃ¬nh mt5-small vÃ  mt5-large cÅ©ng nhÆ° nhÃ£n chuáº©n má»±c cá»§a nÃ³. Xanh lÃ¡: cÃ¡c pháº§n chÃ­nh xÃ¡c cá»§a viá»‡c táº¡o mÃ´ hÃ¬nh nhá»; Äá»: cÃ¡c pháº§n lan truyá»n lá»—i cá»§a viá»‡c táº¡o mÃ´ hÃ¬nh nhá»; Xanh dÆ°Æ¡ng: token khÃ´ng tá»‘i Æ°u lÃ  cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c. ÄÃ¡ng chÃº Ã½, trÃªn bá»™ dá»¯ liá»‡u dá»‹ch iwslt2017 de-en [ 19], mÃ´ hÃ¬nh mT5-small táº¡o ra chÃ­nh xÃ¡c gáº§n 69,3% token, trong khi con sá»‘ cho mÃ´ hÃ¬nh mT5-large lÃ  73,1%.

CÃ¢y Token Suy Ä‘oÃ¡nXÃ¡c minh
khÃ´ng tá»± há»“i quyKáº¿t quáº£ XÃ¡c minh
Táº¡o cÃ¢yDá»± phÃ²ng
Chuá»—i Ä‘Ã£ xÃ¡c minh
Bá»™ giáº£i mÃ£ cÃ¢y
Bá»™ Ä‘iá»u khiá»ƒn táº¡o nhÃ¡nh
dá»±a trÃªn Ä‘á»™ tin cáº­y
Táº¡o cÃ¢yToken
cÃ¢y Káº¿ hoáº¡ch thá»±c thi
suy Ä‘oÃ¡n
Hit/Miss?Quay láº¡i?
Äáº§u ra<EOS>?
Translation the following de text to en.
Jetzt muss ich meine Schuhe ausziehen,
um Ã¼berhaupt an Bord zu kommen!Äáº§u vÃ o
MÃ´ hÃ¬nh nhá»MÃ´ hÃ¬nh lá»›n
Tá»± há»“i quy

HÃ¬nh 5: Luá»“ng cÃ´ng viá»‡c cá»§a LLMCad .

3.2 Táº¡o vÃ  xÃ¡c minh cÃ¢y token

Pháº§n nÃ y chá»§ yáº¿u tháº£o luáº­n vá» cÃ¡ch cÃ¢y token Ä‘Æ°á»£c táº¡o vÃ  xÃ¡c minh trong LLMCad .

Táº¡o cÃ¢y token Äá»ƒ táº¡o ra cÃ¡c cÃ¢y token há»¯u Ã­ch, LLMCad cáº§n tráº£ lá»i hai cÃ¢u há»i quan trá»ng:
â€¢CÃ¡c nhÃ¡nh cáº¡nh tranh tÃ i nguyÃªn tÃ­nh toÃ¡n (vÃ­ dá»¥, GPU) Ä‘á»ƒ táº¡o ra cÃ¡c token tiáº¿p theo báº±ng cÃ¡ch cháº¡y mÃ´ hÃ¬nh thÆ°á»ng trÃº bá»™ nhá»›. Táº¡i má»™t thá»i Ä‘iá»ƒm, nhÃ¡nh nÃ o sáº½ nháº­n tÃ i nguyÃªn Ä‘á»ƒ kÃ©o dÃ i chuá»—i token cá»§a nÃ³? Quyáº¿t Ä‘á»‹nh nÃ y ráº¥t quan trá»ng vÃ¬ viá»‡c táº¡o token cho má»™t nhÃ¡nh sai (nhÆ° Ä‘Æ°á»£c xÃ¡c minh bá»Ÿi mÃ´ hÃ¬nh má»¥c tiÃªu sau) lÃ£ng phÃ­ tÃ i nguyÃªn tÃ­nh toÃ¡n vÃ  trÃ¬ hoÃ£n viá»‡c táº¡o token Ä‘Ãºng.

â€¢Táº¡o token tá»« cÃ¡c nhÃ¡nh khÃ¡c nhau yÃªu cáº§u chuyá»ƒn Ä‘á»•i giá»¯a cÃ¡c ngá»¯ cáº£nh nhÃ¡nh. LÃ m tháº¿ nÃ o Ä‘á»ƒ táº¡o token tá»« cÃ¡c nhÃ¡nh khÃ¡c nhau má»™t cÃ¡ch hiá»‡u quáº£? Thiáº¿t káº¿ nÃ y ráº¥t quan trá»ng vÃ¬ LLMCad cáº§n thÆ°á»ng xuyÃªn chuyá»ƒn Ä‘á»•i giá»¯a hÃ ng chá»¥c nhÃ¡nh.

Äá»ƒ Ä‘Ã¡p á»©ng, LLMCad káº¿t há»£p hai ká»¹ thuáº­t má»›i:
â€¢Bá»™ Ä‘iá»u tá»‘c nhÃ¡nh dá»±a trÃªn Ä‘á»™ tin cáº­y. Äá»ƒ Ä‘iá»u chá»‰nh Ä‘Ãºng tiáº¿n trÃ¬nh cá»§a cÃ¡c nhÃ¡nh khÃ¡c nhau, LLMCad dá»±a vÃ o thá»±c táº¿ ráº±ng nhÃ¡nh cÃ³ xÃ¡c suáº¥t cao hÆ¡n cÃ³ nhiá»u kháº£ nÄƒng lÃ  káº¿t quáº£ chÃ­nh xÃ¡c vÃ  nÃªn cÃ³ Ä‘á»™ dÃ i chuá»—i dÃ i hÆ¡n. á» Ä‘Ã¢y, LLMCad mÃ´ hÃ¬nh hÃ³a xÃ¡c suáº¥t vá»›i Ä‘iá»ƒm tin cáº­y tÃ­ch lÅ©y Ä‘Æ°á»£c Ä‘Æ°a ra bá»Ÿi thÆ°á»ng trÃº bá»™ nhá»› cho má»—i token Ä‘Æ°á»£c táº¡o. Äá»ƒ kiá»ƒm soÃ¡t Ä‘á»™ dÃ i nhÃ¡nh má»™t cÃ¡ch Ä‘á»™ng, LLMCad sá»­ dá»¥ng cÃ´ng báº±ng max-min [28].

Giáº£ sá»­ cÃ³ ğ‘ nhÃ¡nh vÃ  ğ‘€ token, vÃ  nhÃ¡nh thá»© i bao gá»“m ğ‘‡ğµ
ğ‘– token, Ä‘á»™ tin cáº­y tÃ­ch lÅ©y nhÃ¡nh thá»© i ğ¶ğ‘– lÃ  tÃ­ch cá»§a Ä‘á»™ tin cáº­y cá»§a má»—i token. Do Ä‘Ã³, váº¥n Ä‘á» Ä‘á»™ dÃ i nhÃ¡nh cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n báº±ng cÃ¡ch giáº£i quyáº¿t váº¥n Ä‘á» sau.

ğ‘“(ğ‘¥)=ğ‘€âˆ—ğ¶ğ‘¥Ãğ‘
ğ‘–=0ğ¶ğ‘–âˆ’ğ‘‡ğµ
ğ‘¥ (1)
ğ‘‚ğ‘ğ‘—=ğ‘šğ‘–ğ‘›ğ‘
ğ‘¥=0ğ‘“(ğ‘¥) (2)

DÆ°á»›i cÃ´ng báº±ng max-min, LLMCad cá»‘ gáº¯ng phÃ¢n bá»• nhiá»u tÃ i nguyÃªn pháº§n cá»©ng hÆ¡n cho nhÃ¡nh cÃ³ nhiá»u kháº£ nÄƒng lÃ  chuáº©n má»±c.

â€¢Bá»™ giáº£i mÃ£ cÃ¢y. ChÃºng tÃ´i báº¯t Ä‘áº§u nghiÃªn cá»©u báº±ng cÃ¡ch tiáº¿n hÃ nh phÃ¢n tÃ­ch toÃ n diá»‡n vá» cÃ¡c lÃ½ do cÆ¡ báº£n Ä‘áº±ng sau chi phÃ­ hiá»‡u suáº¥t Ä‘Ã¡ng ká»ƒ phÃ¡t sinh do chuyá»ƒn Ä‘á»•i ngá»¯ cáº£nh nhÃ¡nh (vÃ­ dá»¥, 25% chi phÃ­ cho cÃ¡c mÃ´ hÃ¬nh mT5 trÃªn Jetson TX2). Trong HÃ¬nh 7(a), chÃºng tÃ´i cung cáº¥p má»™t minh há»a vá» viá»‡c triá»ƒn khai chuyá»ƒn Ä‘á»•i ngá»¯ cáº£nh nhÃ¡nh trong cÃ¡c cÃ´ng cá»¥ LLM tiÃªn tiáº¿n, nhÆ° PyTorch, sá»­ dá»¥ng ká»‹ch báº£n Ä‘Æ°á»£c mÃ´ táº£ trong HÃ¬nh 6 1â—‹ lÃ m nghiÃªn cá»©u trÆ°á»ng há»£p. Trong minh há»a nÃ y, cÃ¡c láº§n láº·p 1â€“4 láº¥y token Ä‘áº§u ra trÆ°á»›c Ä‘Ã³ lÃ m Ä‘áº§u vÃ o má»›i. Tuy nhiÃªn, viá»‡c táº¡o token "7" trong láº§n láº·p 5 Ä‘Ã²i há»i chuyá»ƒn Ä‘á»•i nhÃ¡nh tá»« b1 sang b2, liÃªn quan Ä‘áº¿n viá»‡c loáº¡i bá» token ğ‘‡4, bá» qua token má»›i ğ‘‡6, vÃ  sá»­ dá»¥ng Ä‘áº§u ra khÃ´ng tá»‘i Æ°u ğ‘‡5 tá»« láº§n láº·p 3 lÃ m Ä‘áº§u vÃ o. Do Ä‘Ã³, LLMCad pháº£i lá»‡ch khá»i quy táº¯c tá»± há»“i quy vÃ  sá»­a Ä‘á»•i má»—i Ä‘áº§u vÃ o láº§n láº·p vá»›i má»™t lÆ°á»£ng lá»›n siÃªu dá»¯ liá»‡u (vÃ­ dá»¥, bá»™ nhá»› cache Key-Value [ 7,36,78] vÃ  id vá»‹ trÃ­ [65]) duy trÃ¬ cÃ¡c hoáº¡t Ä‘á»™ng vÃ  tÆ°Æ¡ng tÃ¡c CPU-GPU.

Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, LLMCad káº¿t há»£p ká»¹ thuáº­t che giáº¥u [ 65]. Ká»¹ thuáº­t nÃ y Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng cÃ¡c dá»± Ä‘oÃ¡n cho vá»‹ trÃ­ ğ‘– chá»‰ phá»¥ thuá»™c vÃ o cÃ¡c Ä‘áº§u ra Ä‘Ã£ biáº¿t á»Ÿ cÃ¡c vá»‹ trÃ­ nhá» hÆ¡n ğ‘– trong cÃ¡c LLM dá»±a trÃªn decoder. Ká»¹ thuáº­t che giáº¥u dá»±a vÃ o má»™t báº£ng trong Ä‘Ã³ táº¥t cáº£ cÃ¡c vá»‹ trÃ­ cÃ³ giÃ¡ trá»‹ má»™t sáº½ Ä‘Æ°á»£c tÃ­nh Ä‘áº¿n cho tÃ­nh toÃ¡n.

Quan trá»ng, bá»™ giáº£i mÃ£ cÃ¢y giá»¯ láº¡i quy trÃ¬nh tá»± há»“i quy trong khi chá»‰ sá»­a Ä‘á»•i báº£ng che giáº¥u cá»§a nÃ³ Ä‘á»ƒ há»— trá»£ viá»‡c cÃ´ láº­p cÃ¡c hiá»‡u á»©ng tá»« cÃ¡c nhÃ¡nh khÃ¡c nhau, nhÆ° Ä‘Æ°á»£c thá»ƒ hiá»‡n trong HÃ¬nh 7(b). Trong má»—i láº§n láº·p, LLMCad coi token má»›i Ä‘Æ°á»£c táº¡o lÃ  Ä‘áº§u vÃ o, giá»‘ng nhÆ° trong viá»‡c táº¡o thÃ´ng thÆ°á»ng. Tuy nhiÃªn, nÃ³ chá»‰ gÃ¡n giÃ¡ trá»‹ má»™t cho cÃ¡c vá»‹ trÃ­ trÆ°á»›c Ä‘Ã³ trÃªn cÃ¹ng má»™t nhÃ¡nh. VÃ­ dá»¥, khi táº¡o token "7" cho nhÃ¡nh b2 trong láº§n láº·p 5, Ä‘áº§u vÃ o váº«n lÃ  ğ‘‡6, Ä‘áº§u ra cá»§a láº§n láº·p 4, nhÆ°ng chá»‰ cÃ¡c vá»‹ trÃ­ "1, 2, 3, vÃ  5" Ä‘Æ°á»£c Ä‘áº·t thÃ nh má»™t; táº¥t cáº£ cÃ¡c vá»‹ trÃ­ khÃ¡c Ä‘Æ°á»£c Ä‘áº·t thÃ nh khÃ´ng. PhÆ°Æ¡ng phÃ¡p nÃ y Ä‘áº£m báº£o

6

--- TRANG 7 ---
LLMCad : Suy luáº­n MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n trÃªn Thiáº¿t bá»‹ Nhanh vÃ  CÃ³ thá»ƒ Má»Ÿ rá»™ng Conference'17, July 2017, Washington, DC, USA

â‘  CÃ¢y Token
T1: the T2: ArcticT4: ice
T5: capT3: iceice
capice
, which has
cap , which for shrinkedâ‘¡ XÃ¡c minh KhÃ´ng tá»± há»“i quy
Cáº§n quay láº¡iwhich forabout three millon years

three millon years

XÃ¡c minh
for threeReuseâ‘¢ Táº¡o
Suy Ä‘oÃ¡nT6: ice
T7: ,iceb1
b2

Chuáº©n má»±c:   the Arctic ice cap, which for  aboutÄiá»ƒm tin cáº­y = 0.6
Äiá»ƒm tin cáº­y = 0.3

Äáº§u ra gá»‘cÄáº§u ra Ä‘Ã£ xÃ¡c minh
cap ,which has iceá»t láº·p
Táº¥t cáº£ Ä‘áº§u ra xÃ¡c minhLLM Má»¥c tiÃªu

iceÄáº§u vÃ o cÃ¢y token
Äáº§u ra Ä‘Ã£ xÃ¡c minh
 dá»± phÃ²ngLLM ThÆ°á»ng trÃº Bá»™ nhá»›
LLM
Má»¥c tiÃªu

HÃ¬nh 6: Má»™t vÃ­ dá»¥ minh há»a cá»§a LLMCad .

Token Ä‘áº§u vÃ o Äáº§u ra
T 1 T 2
T1T2 T3
T1T2T3T4T5
T1T2T3T4T6
T1T2T3T5T7Iter1:
Iter2:
Iter3:
Iter4:
Iter5:tá»± há»“i quy
chuyá»ƒn Ä‘á»•i ngá»¯ cáº£nh
nhÃ¡nh

(a) Quy trÃ¬nh táº¡o cÃ¢y hiá»‡n cÃ³

Token Ä‘áº§u vÃ o
T 1 T 2
T1T2 T3
T1T2T3T 4 T 5
T1T2T3T4T6
T1T2T3T5T7Iter1:
Iter2:
Iter3:
Iter4:
Iter5:T5
T6T4tá»± há»“i quy
tá»± há»“i quy
... ...T 1
T1
T1T2
T1T2T3
T1T2T3T5T2
T3
T41 0 0 0 0 0
1 1 0 0 0 0
1 1 1 0 0 0
1 1 1 1 0 0
1 1 1 0 1 0Báº£ng che giáº¥u Token trong tÃ­nh toÃ¡n Äáº§u ra (b) Quy trÃ¬nh bá»™ giáº£i mÃ£ cÃ¢y

HÃ¬nh 7: VÃ­ dá»¥ vá» quy trÃ¬nh táº¡o cÃ¢y trong cÃ¡c cÃ´ng cá»¥ LLM tiÃªn tiáº¿n vÃ  quy trÃ¬nh bá»™ giáº£i mÃ£ cÃ¢y cá»§a LLMCad dá»±a trÃªn trÆ°á»ng há»£p cá»§a HÃ¬nh 6 1â—‹.

T1T2
T5
T3T4
T6CÃ¢y Token
T8T7
T1 T2 T5
Chuá»—i Ä‘Ã£ xÃ¡c minhT'
7T1T2
T3T4
T6T8T7 T1T2T5
T1Chuá»—i nhÃ¡nh
T 1 T 2
T 3T 5
T6T 1 T 2
T 1T 1 T 2
T 5 T 1 T 2
T'7T1T2T5
T1T2
T3T 8 T1T6Káº¿t quáº£
NhÃ¡nh1
Káº¿t quáº£
NhÃ¡nh2
Káº¿t quáº£
NhÃ¡nh3LLM
Má»¥c tiÃªuhÃ ng loáº¡t nhá»
T1T5
T2 T5 T'7T2T 2
T 4
T7T 2
T 5
T 3
T6
T 8Äáº§u ra ÄÃ£ xÃ¡c minh Äáº§u ra Gá»‘c
CÃ¢y token chÃ­nh xÃ¡cT2â‘  XÃ¡c minh khÃ´ng tá»± há»“i quy theo hÃ ng loáº¡t
â‘¡ TÃ¬m kiáº¿m chuá»—i Ä‘Ã£ xÃ¡c minh theo chiá»u sÃ¢u trÆ°á»›c

kÃ­ch thÆ°á»›c
hÃ ng loáº¡t=3T1T5T2
T1T2T'7T1T2T5Chuá»—i chÃ­nh xÃ¡c NhÃ¡nh1
Chuá»—i chÃ­nh xÃ¡c NhÃ¡nh2
Chuá»—i chÃ­nh xÃ¡c NhÃ¡nh3

HÃ¬nh 8: Minh há»a xÃ¡c minh cÃ¢y token.

ráº±ng cÃ¡c token "4 vÃ  6" khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n tÃ­nh toÃ¡n, cho phÃ©p LLMCad táº¡o token "7" mÃ  khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng.

XÃ¡c minh vÃ  sá»­a chá»¯a cÃ¢y token Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c má»¥c tiÃªu khÃ´ng hy sinh Ä‘á»™ chÃ­nh xÃ¡c, LLMCad pháº£i xÃ¡c minh má»—i token Ä‘Æ°á»£c táº¡o bá»Ÿi LLM thÆ°á»ng trÃº bá»™ nhá»›. Má»™t phÆ°Æ¡ng phÃ¡p trá»±c quan lÃ  sá»­ dá»¥ng LLM má»¥c tiÃªu Ä‘á»ƒ cháº¡y xÃ¡c minh sau má»—i láº§n táº¡o token bá»Ÿi LLM thÆ°á»ng trÃº bá»™ nhá»›. Tuy nhiÃªn, phÆ°Æ¡ng phÃ¡p nhÆ° váº­y (Ä‘Æ°á»£c gá»i lÃ  xÃ¡c minh tá»± há»“i quy (AV) ) tháº­m chÃ­ cÃ²n cháº­m hÆ¡n viá»‡c táº¡o má»—i token bá»Ÿi mÃ´ hÃ¬nh má»¥c tiÃªu trá»±c tiáº¿p, vÃ¬ AV khÃ´ng giáº£m sá»‘ láº§n suy luáº­n LLM má»¥c tiÃªu mÃ  tháº­m chÃ­ cÃ²n sá»­ dá»¥ng LLM thÆ°á»ng trÃº bá»™ nhá»›.

Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, LLMCad dá»±a trÃªn hai cÆ¡ há»™i: (1) LLM má»¥c tiÃªu cÃ³ thá»ƒ kiá»ƒm tra má»™t chuá»—i token

2.5 5.0 7.5 10.0
mT5-Large trÃªn TX2255075Äá»™ trá»… (s)
9.9xxÃ¡c minh khÃ´ng tá»± há»“i quy xÃ¡c minh tá»± há»“i quy

2.5 5.0 7.5 10.0
LLaMa-13B trÃªn Xiaomi 1050100
8.5x

HÃ¬nh 9: Äá»™ trá»… xÃ¡c minh vá»›i chuá»—i Ä‘áº§u vÃ o tÄƒng.

song song báº±ng cÃ¡ch truy cáº­p tham sá»‘ cá»§a nÃ³ chá»‰ má»™t láº§n (Ä‘Æ°á»£c gá»i lÃ  xÃ¡c minh khÃ´ng tá»± há»“i quy (NAV) ) vÃ  káº¿t quáº£ xÃ¡c minh giá»‘ng nhÆ° kiá»ƒm tra chÃºng tuáº§n tá»± [ 20,41]. (2) QuÃ¡ trÃ¬nh NAV nhanh hÆ¡n AV ráº¥t nhiá»u. CÃ¡c thÃ­ nghiá»‡m thÃ­ Ä‘iá»ƒm cá»§a chÃºng tÃ´i trong HÃ¬nh 9 trÃªn cÃ¡c mÃ´ hÃ¬nh LLaMa-13B vÃ  mT5-Large sá»­ dá»¥ng Xiaomi10 vÃ  TX2 cho tháº¥y NAV vÆ°á»£t trá»™i hÆ¡n AV trong

7

--- TRANG 8 ---
Conference'17, July 2017, Washington, DC, USA Daliang Xuâ™¦, Wangsong Yinâ™¦, Xin Jinâ™¦, Ying Zhangâ™¦, Shiyun Weiâ‹†, Mengwei Xu^, Xuanzhe Liuâ™¦

thá»i gian kiá»ƒm tra qua 2â€“10 token Ä‘áº§u vÃ o, vá»›i lá»£i Ã­ch cá»§a nÃ³ ná»•i báº­t hÆ¡n khi sá»‘ lÆ°á»£ng token tÄƒng. NAV giáº£m Ä‘Ã¡ng ká»ƒ thá»i gian xÃ¡c minh cho cÃ¡c mÃ´ hÃ¬nh mT5-Large vÃ  LLaMa-13B 8,5â€“9,9 Ã— á»Ÿ 10 token, do NAV chá»‰ hoÃ¡n Ä‘á»•i trá»ng sá»‘ má»™t láº§n so vá»›i nhiá»u láº§n hoÃ¡n Ä‘á»•i cá»§a AV má»—i token, giáº£m chi phÃ­ I/O.

TÃ³m láº¡i, NAV cÃ³ thá»ƒ xÃ¡c minh song song nhiá»u token má»™t cÃ¡ch chÃ­nh xÃ¡c vá»›i chi phÃ­ chá»‰ má»™t láº§n suy luáº­n LLM má»¥c tiÃªu. LLMCad káº¿t há»£p NAV vÃ  má»Ÿ rá»™ng nÃ³ Ä‘á»ƒ há»— trá»£ xÃ¡c minh cÃ¢y token, nhÆ° Ä‘Æ°á»£c thá»ƒ hiá»‡n trong HÃ¬nh 8, bao gá»“m hai bÆ°á»›c quan trá»ng:

â€¢XÃ¡c minh khÃ´ng tá»± há»“i quy theo hÃ ng loáº¡t. NhÆ° Ä‘Æ°á»£c thá»ƒ hiá»‡n trong HÃ¬nh 8 1â—‹, Ä‘á»ƒ há»— trá»£ xÃ¡c minh cÃ¢y token, LLMCad Ä‘áº§u tiÃªn chia cÃ¢y token thÃ nh nhiá»u chuá»—i nhÃ¡nh vÃ  káº¿t há»£p chÃºng thÃ nh má»™t hÃ ng loáº¡t nhá» lÃ m Ä‘áº§u vÃ o cá»§a LLM má»¥c tiÃªu. Sau NAV, LLMCad cÃ³ thá»ƒ thu Ä‘Æ°á»£c káº¿t quáº£ chÃ­nh xÃ¡c cá»§a má»—i vá»‹ trÃ­ trong má»—i chuá»—i nhÃ¡nh. So vá»›i cÃ¡c chuá»—i nhÃ¡nh gá»‘c, LLMCad cÃ³ thá»ƒ phÃ¡t hiá»‡n táº¥t cáº£ lá»—i cá»§a má»™t cÃ¢y token, vÃ­ dá»¥, ğ‘‡â€²
7 trong káº¿t quáº£ nhÃ¡nh2. Má»™t chuá»—i chÃ­nh xÃ¡c nhÃ¡nh lÃ  chuá»—i con dáº«n Ä‘áº¿n vá»‹ trÃ­ lá»—i Ä‘áº§u tiÃªn, cá»™ng vá»›i token Ä‘Æ°á»£c sá»­a chá»¯a, Ä‘á»ƒ trÃ¡nh lan truyá»n lá»—i. VÃ­ dá»¥, chuá»—i chÃ­nh xÃ¡c nhÃ¡nh1 dá»«ng á»Ÿ ğ‘‡2, cá»™ng vá»›i ğ‘‡5, tá»©c lÃ  cÃ¡c token "1, 2 vÃ  5". Cáº§n lÆ°u Ã½, cho ráº±ng NAV bá»‹ giá»›i háº¡n I/O, viá»‡c tÄƒng kÃ­ch thÆ°á»›c hÃ ng loáº¡t (vÃ­ dá»¥, <10) cÃ³ tÃ¡c Ä‘á»™ng khÃ´ng Ä‘Ã¡ng ká»ƒ Ä‘áº¿n thá»i gian xÃ¡c minh.

â€¢TÃ¬m kiáº¿m chuá»—i Ä‘Ã£ xÃ¡c minh theo chiá»u sÃ¢u trÆ°á»›c. Dá»±a trÃªn cÃ¡c chuá»—i chÃ­nh xÃ¡c, LLMCad cÃ³ thá»ƒ xÃ¢y dá»±ng má»™t cÃ¢y token chÃ­nh xÃ¡c, nhÆ° Ä‘Æ°á»£c thá»ƒ hiá»‡n trong HÃ¬nh 8 2â—‹. NÃºt lÃ¡ cá»§a nÃ³ lÃ  token Ä‘Æ°á»£c sá»­a chá»¯a Ä‘áº§u tiÃªn hoáº·c token cuá»‘i cÃ¹ng cá»§a chuá»—i nhÃ¡nh gá»‘c. LLMCad táº­n dá»¥ng thuáº­t toÃ¡n tÃ¬m kiáº¿m theo chiá»u sÃ¢u trÆ°á»›c Ä‘á»ƒ tÃ¬m Ä‘Æ°á»ng dáº«n chÃ­nh xÃ¡c dÃ i nháº¥t trong cÃ¢y token chÃ­nh xÃ¡c lÃ m chuá»—i Ä‘Ã£ xÃ¡c minh .

Náº¿u chuá»—i Ä‘Ã£ xÃ¡c minh cÃ³ má»™t token Ä‘Æ°á»£c sá»­a chá»¯a, vÃ­ dá»¥, ğ‘‡â€²
7, LLMCad sáº½ quay láº¡i cÃ¢y token Ä‘áº¿n vá»‹ trÃ­ lá»—i, sá»­a lá»—i, vÃ  sá»­ dá»¥ng nÃ³ lÃ m Ä‘áº§u vÃ o má»›i cho viá»‡c táº¡o tÆ°Æ¡ng lai.

3.3 Chiáº¿n lÆ°á»£c dá»± phÃ²ng tá»± thÃ­ch á»©ng

Chiáº¿n lÆ°á»£c nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ khá»Ÿi Ä‘á»™ng quÃ¡ trÃ¬nh xÃ¡c minh ká»‹p thá»i khi LLM thÆ°á»ng trÃº bá»™ nhá»› táº¡o ra má»™t token khÃ´ng chÃ­nh xÃ¡c. Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c má»¥c tiÃªu nÃ y, LLMCad cáº§n tráº£ lá»i hai cÃ¢u há»i quan trá»ng:

â€¢Lá»±a chá»n Chá»‰ sá»‘ Quyáº¿t Ä‘á»‹nh. Chá»‰ sá»‘ quyáº¿t Ä‘á»‹nh nÃªn Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ xÃ¡c suáº¥t lá»—i trong cÃ¢y token.

â€¢GiÃ¡ trá»‹ NgÆ°á»¡ng cho CÃ¡c TÃ¡c vá»¥ KhÃ¡c nhau. Nháº­n ra ráº±ng má»™t ngÆ°á»¡ng phá»• quÃ¡t cÃ³ thá»ƒ khÃ´ng phÃ¹ há»£p cho táº¥t cáº£ cÃ¡c tÃ¡c vá»¥, LLMCad pháº£i thiáº¿t láº­p cÃ¡c giÃ¡ trá»‹ ngÆ°á»¡ng phÃ¹ há»£p Ä‘Æ°á»£c Ä‘iá»u chá»‰nh theo cÃ¡c tÃ¡c vá»¥ cá»¥ thá»ƒ.

Äá»ƒ giáº£i quyáº¿t nhá»¯ng váº¥n Ä‘á» nÃ y, LLMCad giá»›i thiá»‡u hai ká»¹ thuáº­t sÃ¡ng táº¡o:

â€¢Äá»™ tin cáº­y tÃ­ch lÅ©y cÃ¢y ( ğ‘‡ğ‘). ChÃºng tÃ´i Ä‘á» xuáº¥t sá»­ dá»¥ng Ä‘á»™ tin cáº­y tÃ­ch lÅ©y cÃ¢y lÃ m biáº¿n quyáº¿t Ä‘á»‹nh Ä‘á»ƒ khá»Ÿi Ä‘á»™ng dá»± phÃ²ng. KhÃ´ng giá»‘ng nhÆ° cÃ¡c nghiÃªn cá»©u trÆ°á»›c [ 37,41] dá»±a vÃ o Ä‘á»™ tin cáº­y token Ä‘Æ¡n hoáº·c Ä‘á»™ dÃ i chuá»—i token, ğ‘‡ğ‘ cung cáº¥p Ä‘Ã¡nh giÃ¡ toÃ n diá»‡n vá» báº¥t Ä‘á»‹nh toÃ n cá»¥c. NÃ³ náº¯m báº¯t lá»—i má»™t cÃ¡ch chÃ­nh xÃ¡c hÆ¡n do báº£n cháº¥t tá»± há»“i quy cá»§a viá»‡c táº¡o token.

CÃ´ng thá»©c Ä‘á»™ tin cáº­y tÃ­ch lÅ©y cÃ¢y lÃ  ğ‘‡ğ‘=
ğ‘šğ‘ğ‘¥ğ‘ğ‘
ğ‘–=1ğ¶ğ‘–, trong Ä‘Ã³ ğ‘ğ‘ Ä‘áº¡i diá»‡n cho sá»‘ lÆ°á»£ng nhÃ¡nh trong má»™t cÃ¢y token, vÃ  ğ¶ğ‘– biá»ƒu thá»‹ Ä‘á»™ tin cáº­y tÃ­ch lÅ©y cá»§a nhÃ¡nh thá»© ğ‘–. ChÃºng tÃ´i chá»n Ä‘á»™ tin cáº­y tÃ­ch lÅ©y tá»‘i Ä‘a so vá»›i Ä‘á»™ tin cáº­y tá»‘i thiá»ƒu/trung bÃ¬nh vÃ¬ nhÃ¡nh cÃ³ Ä‘á»™ tin cáº­y nháº¥t cÃ³ nhiá»u kháº£ nÄƒng mang láº¡i káº¿t quáº£ chÃ­nh xÃ¡c sau xÃ¡c minh, vÃ  quÃ¡ trÃ¬nh xÃ¡c minh chá»‰ cÃ³ thá»ƒ xÃ¡c Ä‘á»‹nh lá»—i khi nhÃ¡nh cÃ³ Ä‘á»™ tin cáº­y nháº¥t sai.

â€¢ngÆ°á»¡ng tá»± thÃ­ch á»©ng ( ğ›¼) Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xÃ¡c Ä‘á»‹nh khi nÃ o LLM má»¥c tiÃªu sáº½ xÃ¡c minh. NÃ³ hoáº¡t Ä‘á»™ng trÃªn nguyÃªn táº¯c ráº±ng LLM thÆ°á»ng trÃº bá»™ nhá»›, táº¡o ra Ä‘áº§u ra gáº§n giá»‘ng vá»›i LLM má»¥c tiÃªu, nÃªn Ä‘Æ°á»£c tin tÆ°á»Ÿng hÆ¡n, tá»©c lÃ  táº§n suáº¥t xÃ¡c minh tháº¥p hÆ¡n báº±ng cÃ¡ch Ä‘áº·t ngÆ°á»¡ng tháº¥p hÆ¡n. Äá»ƒ Ä‘Ã¡nh giÃ¡ sá»± tÆ°Æ¡ng tá»± Ä‘áº§u ra, LLMCad dá»±a vÃ o dá»¯ liá»‡u lá»‹ch sá»­ vá» Ä‘á»™ chÃ­nh xÃ¡c cá»§a cÃ¡c token Ä‘Ã£ xÃ¡c minh.

NgÆ°á»i dÃ¹ng cÃ³ thá»ƒ chá»n giÃ¡ trá»‹ ğ›¼ ban Ä‘áº§u hoáº·c sá»­ dá»¥ng giÃ¡ trá»‹ máº·c Ä‘á»‹nh (0,01) Ä‘Æ°á»£c cung cáº¥p bá»Ÿi há»‡ thá»‘ng. Sau khi xÃ¡c minh, LLMCad cáº­p nháº­t ngÆ°á»¡ng tá»± thÃ­ch á»©ng ( ğ›¼) sá»­ dá»¥ng quy táº¯c sau:

ğ›¼ğ‘–+1=(ğ›¼ğ‘–âˆ—0.5ï¿½ï¿½ğ‘“ ğ‘ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡ ==ğ‘ğ‘ğ‘™ğ‘™
ğ›¼ğ‘–/ğ‘‡ğ‘ğ‘ğ‘™ğ‘™âˆ’ğ‘ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡
ğ‘ğ‘ğ‘™ğ‘™ğ‘ ğ‘–ğ‘“ ğ‘ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡ <ğ‘ğ‘ğ‘™ğ‘™(3)

trong Ä‘Ã³ ğ‘ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡ vÃ  ğ‘ğ‘ğ‘™ğ‘™ lÃ  sá»‘ lÆ°á»£ng tá»•ng token vÃ  token chÃ­nh xÃ¡c trong nhÃ¡nh khá»›p nháº¥t trong má»™t láº§n xÃ¡c minh. Cá»¥ thá»ƒ, khi quÃ¡ trÃ¬nh xÃ¡c minh khÃ´ng phÃ¡t hiá»‡n lá»—i, LLMCad giáº£m ğ›¼ báº±ng cÃ¡ch nhÃ¢n giÃ¡ trá»‹ hiá»‡n táº¡i vá»›i 0,5, Ä‘á»™ tin cáº­y tÃ­ch lÅ©y cá»§a 3-5 token trong cÃ¡c quan sÃ¡t thá»±c nghiá»‡m. NgÆ°á»£c láº¡i, náº¿u xÃ¡c minh xÃ¡c Ä‘á»‹nh lá»—i, ngÆ°á»¡ng Ä‘Æ°á»£c tÄƒng báº±ng cÃ¡ch chia ğ›¼ cho Ä‘á»™ tin cáº­y tÃ­ch lÅ©y trung bÃ¬nh cá»§a táº¥t cáº£ cÃ¡c token tiáº¿p theo sau token Ä‘Æ°á»£c táº¡o khÃ´ng chÃ­nh xÃ¡c. LÃ½ do Ä‘áº±ng sau viá»‡c sá»­ dá»¥ng hÃ m mÅ© lÃ  Ä‘á»™ tin cáº­y tÃ­ch lÅ©y cÃ¢y lÃ  tÃ­ch cá»§a Ä‘á»™ tin cáº­y cá»§a má»—i token, tÃ­ch lÅ©y theo cáº¥p sá»‘ nhÃ¢n.

TÃ³m láº¡i, sau má»—i láº§n táº¡o token bá»Ÿi LLM thÆ°á»ng trÃº bá»™ nhá»›, LLMCad tÃ­nh toÃ¡n ğ‘‡ğ‘. Náº¿u ğ‘‡ğ‘ rÆ¡i xuá»‘ng dÆ°á»›i ğ›¼, má»™t dá»± phÃ²ng xáº£y ra, vÃ  mÃ´ hÃ¬nh má»¥c tiÃªu báº¯t Ä‘áº§u xÃ¡c minh. Sau khi xÃ¡c minh, ğ›¼ Ä‘Æ°á»£c cáº­p nháº­t dá»±a trÃªn lá»‹ch sá»­ Ä‘á»™ chÃ­nh xÃ¡c táº¡o má»›i nháº¥t.

3.4 ÄÆ°á»ng á»‘ng Táº¡o Suy Ä‘oÃ¡n

NhÆ° Ä‘Æ°á»£c tá»± táº¡i trong Â§2.3, viá»‡c sá»­ dá»¥ng GPU tráº£i qua cÃ¡c Ä‘á»£t tÄƒng theo chu ká»³ do thá»±c táº¿ lÃ  cÃ¡c cÃ´ng cá»¥ LLM SOTA sá»­ dá»¥ng ká»¹ thuáº­t hoÃ¡n Ä‘á»•i. Äá»ƒ thu hoáº¡ch cÃ¡c chu ká»³ miá»…n phÃ­, LLMCad Ä‘á» xuáº¥t ká»¹ thuáº­t táº¡o suy Ä‘oÃ¡n báº±ng cÃ¡ch cho phÃ©p LLM thÆ°á»ng trÃº bá»™ nhá»› tiáº¿p tá»¥c táº¡o token trong quÃ¡ trÃ¬nh xÃ¡c minh. PhÆ°Æ¡ng phÃ¡p nÃ y dá»±a trÃªn

8

--- TRANG 9 ---
LLMCad : Suy luáº­n MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n trÃªn Thiáº¿t bá»‹ Nhanh vÃ  CÃ³ thá»ƒ Má»Ÿ rá»™ng Conference'17, July 2017, Washington, DC, USA

mT5-Large GPT2-Large
Táº£i024Äá»™ trá»… (s)Má»™t mÃ¬nh
Song song

mT5-Large GPT2-Large
TÃ­nh toÃ¡n0.00.1Má»™t mÃ¬nh
Song song

HÃ¬nh 10: Thá»i gian táº£i vÃ  tÃ­nh toÃ¡n cá»§a viá»‡c thá»±c thi mÃ´ hÃ¬nh má»¥c tiÃªu vá»›i song song hÃ³a mÃ´ hÃ¬nh thÆ°á»ng trÃº bá»™ nhá»›.

LLM Má»¥c tiÃªu
LLM ThÆ°á»ng trÃº
Bá»™ nhá»›C C C CC L UL
SC SC ...SCC
Dá»± phÃ²ngGiá»›i háº¡n trÃªn
bá»™ nhá»› 
Suy Ä‘oÃ¡n Káº¿t thÃºc xÃ¡c minh L ...
SC SC SCC
C ...LGiá»›i háº¡n trÃªn
bá»™ nhá»› 
Suy Ä‘oÃ¡nGiá»›i háº¡n trÃªn
bá»™ nhá»› 
UL UL
... ...
UL L C SC TÃ­nh toÃ¡nTÃ­nh toÃ¡n
suy Ä‘oÃ¡nTáº£i tham sá»‘ dÆ°á»›i
giá»›i háº¡n trÃªn bá»™ nhá»›Táº£i tham sá»‘ trÃªn
giá»›i háº¡n trÃªn bá»™ nhá»›

HÃ¬nh 11: Táº¡o suy Ä‘oÃ¡n cá»§a LLMCad .

hiá»ƒu biáº¿t ráº±ng Ä‘Ã´i khi quÃ¡ trÃ¬nh xÃ¡c minh khÃ´ng phÃ¡t hiá»‡n lá»—i nÃªn cÃ¡c token Ä‘Æ°á»£c táº¡o suy Ä‘oÃ¡n cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng sau Ä‘Ã³.

TÃ¡c Ä‘á»™ng cá»§a táº¡o suy Ä‘oÃ¡n Ä‘áº¿n LLM má»¥c tiÃªu. CÃ¡c thÃ­ nghiá»‡m sÆ¡ bá»™ cá»§a chÃºng tÃ´i trÃªn TX2 sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh mT5-Large vÃ  GPT2-Large cho tháº¥y viá»‡c song song hÃ³a viá»‡c thá»±c thi LLM thÆ°á»ng trÃº bá»™ nhá»› vÃ  má»¥c tiÃªu tÄƒng thá»i gian tÃ­nh toÃ¡n vÃ  táº£i LLM má»¥c tiÃªu 2,2â€“2,3 Ã— vÃ  1,05â€“1,09Ã—, tÆ°Æ¡ng á»©ng. Äá»™ trá»… tÃ­nh toÃ¡n Ä‘Æ°á»£c gÃ¢y ra bá»Ÿi tranh cháº¥p lÃµi GPU, trong khi Ä‘á»™ trá»… táº£i lÃ  báº¥t ngá» vÃ¬ tranh cháº¥p bá»™ nhá»›. Cá»¥ thá»ƒ, LLM thÆ°á»ng trÃº bá»™ nhá»› liÃªn tá»¥c phÃ¢n bá»• cÃ¡c vÃ¹ng bá»™ nhá»› cho cÃ¡c token Ä‘Æ°á»£c táº¡o suy Ä‘oÃ¡n, trong khi LLM má»¥c tiÃªu phÃ¢n bá»• Ä‘á»™ng cÃ¡c vÃ¹ng bá»™ nhá»› Ä‘á»ƒ táº£i tham sá»‘. ThÃ´ng thÆ°á»ng, tÃ¡c Ä‘á»™ng khÃ´ng Ä‘Ã¡ng ká»ƒ Ä‘Æ°á»£c tÃ¡c Ä‘á»™ng lÃªn nhau trá»« khi viá»‡c sá»­ dá»¥ng bá»™ nhá»› vÆ°á»£t quÃ¡ 90%. Tuy nhiÃªn, viá»‡c sá»­ dá»¥ng bá»™ nhá»› vÆ°á»£t quÃ¡ 90% hoáº·c tháº­m chÃ­ Ä‘áº¡t 95% lÃ  phá»• biáº¿n trong ká»‹ch báº£n táº¡o suy Ä‘oÃ¡n, do thá»±c táº¿ lÃ  cÃ¡c cÃ´ng cá»¥ LLM tiÃªn tiáº¿n hiá»‡n táº¡i Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ táº£i cÃ ng nhiá»u tham sá»‘ cÃ ng tá»‘t tá»« Ä‘Ä©a vÃ o bá»™ nhá»› Ä‘á»ƒ giáº£m thá»i gian suy luáº­n.

ÄÆ°á»ng á»‘ng tÃ­nh toÃ¡n-táº£i. Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» trÃªn, LLMCad láº­p káº¿ hoáº¡ch thá»±c thi song song má»™t cÃ¡ch tinh táº¿, nhÆ° Ä‘Æ°á»£c thá»ƒ hiá»‡n trong HÃ¬nh 11. NguyÃªn táº¯c chÃ­nh lÃ  quÃ¡ trÃ¬nh xÃ¡c minh bÃ¬nh thÆ°á»ng khÃ´ng thá»ƒ bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi viá»‡c thá»±c thi suy Ä‘oÃ¡n. Do Ä‘Ã³, cÃ³ má»™t giá»›i háº¡n trÃªn bá»™ nhá»› báº±ng cÃ¡ch lÆ°á»£c táº£ hoáº·c ngÆ°á»i dÃ¹ng xÃ¡c Ä‘á»‹nh Ä‘á»ƒ trÃ¡nh tranh cháº¥p bá»™ nhá»› hai LLM, vÃ  hai LLM tÃ­nh toÃ¡n khÃ´ng thá»ƒ Ä‘Æ°á»£c thá»±c thi song song.

Sau khi Ä‘Æ°a vÃ o chuá»—i Ä‘áº§u vÃ o, cÃ¡c tham sá»‘ cá»§a cáº£ LLM thÆ°á»ng trÃº bá»™ nhá»› vÃ  má»¥c tiÃªu Ä‘Æ°á»£c táº£i vÃ o bá»™ nhá»›. Má»™t khi viá»‡c táº£i LLM thÆ°á»ng trÃº bá»™ nhá»› káº¿t thÃºc, nÃ³ báº¯t Ä‘áº§u táº¡o token, vÃ  viá»‡c táº£i tham sá»‘ LLM má»¥c tiÃªu (mÃ u vÃ ng) sáº½ dá»«ng trÆ°á»›c khi giá»›i háº¡n trÃªn bá»™ nhá»› bá»‹ vÆ°á»£t quÃ¡ Ä‘á»ƒ trÃ¡nh áº£nh hÆ°á»Ÿng Ä‘áº¿n viá»‡c táº¡o LLM thÆ°á»ng trÃº bá»™ nhá»› bÃ¬nh thÆ°á»ng. Khi Ä‘iá»u kiá»‡n dá»± phÃ²ng Ä‘Æ°á»£c Ä‘Ã¡p á»©ng,

Ná»n táº£ng Bá»™ xá»­ lÃ½ Pháº§n má»m Bá»™ nhá»›
Jetson TX24x Cortex-A57
Maxwell 128 lÃµi CUDATorch-1.10
Ubuntu 18.048G
Jetson
Orin NXAmpere 1024 lÃµi CUDA
+ 32 lÃµi TensorTorch-2.0
Ubuntu 18.048G
Xiaomi 101x 2.84GHz A77+3x 2.4GHz Cortex A77
+4x 1.8GHz Cortex A55Android 10
llama.cpp8G
Xiaomi
111x 3.0 GHz X2+ 3x 2.5 GHz Cortex A710
+ 1.8GHz 4x Cortex-A510Android 10
llama.cpp8G

Báº£ng 2: CÃ¡c ná»n táº£ng Ä‘Æ°á»£c sá»­ dá»¥ng trong thÃ­ nghiá»‡m.

Thiáº¿t bá»‹ TÃ¡c vá»¥LLM ThÆ°á»ng trÃº
Bá»™ nhá»›LLM
Má»¥c tiÃªuKhoáº£ng cÃ¡ch
Tá»‘c Ä‘á»™Bá»™ dá»¯ liá»‡u
Jetson TX2
Jetson
Orin NXTmT5-small (0.3B) mT5-Large (1.2B) 230x IWLST17-de-en [19]
Bart-base Bart-Large WMT14-de-en [15]
QAmT5-small (0.3B) mT5-Large (1.2B) 230x SQuAD_v2 [56]
T5-small (0.06B) T5-large (0.73B) 263x SQuAD_v2
LM GPT2 (0.14B) GPT2-Large (0.8) 214x Wikitext [47]
S T5-small (0.06B) T5-large (0.73B) 263x CNN/Daily [61]
Xiaomi 10
Xiaomi
ProTVicuna-7B
(INT4)Vicuna-13B
(INT4)59xParrot
WMT22-de-en
WMT22-zh-en
QA LLaMa2-Chat-
7B (INT4)LLaMa2-Chat-
13B (INT4)59xSQuAD
TruthfulQA [44]
S CNN/Daily
T, S, QA, vÃ  LM Ä‘áº¡i diá»‡n cho cÃ¡c tÃ¡c vá»¥ sinh cá»§a dá»‹ch thuáº­t, tÃ³m táº¯t,
tráº£ lá»i cÃ¢u há»i, vÃ  mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯.

Báº£ng 3: CÃ¡c tÃ¡c vá»¥, mÃ´ hÃ¬nh, bá»™ dá»¯ liá»‡u, vÃ  thiáº¿t bá»‹ Ä‘Æ°á»£c thá»­ nghiá»‡m tÆ°Æ¡ng á»©ng Ä‘Æ°á»£c sá»­ dá»¥ng trong thÃ­ nghiá»‡m.

pháº§n cÃ²n láº¡i cá»§a tham sá»‘ cho mÃ´ hÃ¬nh má»¥c tiÃªu (mÃ u cam) sáº½ Ä‘Æ°á»£c táº£i vÃ o bá»™ nhá»›, vÃ  sau Ä‘Ã³ viá»‡c tÃ­nh toÃ¡n cá»§a LLM má»¥c tiÃªu báº¯t Ä‘áº§u. Viá»‡c thá»±c thi suy Ä‘oÃ¡n (mÃ u xanh) sáº½ khÃ´ng cháº¡y trá»« khi quÃ¡ trÃ¬nh xÃ¡c minh Ä‘ang táº£i tham sá»‘ dÆ°á»›i ngÃ¢n sÃ¡ch bá»™ nhá»› (mÃ u vÃ ng), trÃ¡nh tranh cháº¥p bá»™ xá»­ lÃ½ vÃ  bá»™ nhá»›.

4 ÄÃNH GIÃ

4.1 Triá»ƒn khai vÃ  Thiáº¿t láº­p

ChÃºng tÃ´i Ä‘Ã£ triá»ƒn khai Ä‘áº§y Ä‘á»§ LLMCad vá»›i 4,5k SLoC (Python: 3.500 vÃ  C/C++: 1.000). NguyÃªn máº«u lÃ  má»™t khung Ä‘á»™c láº­p há»— trá»£ LLM Ä‘Æ°á»£c xuáº¥t tá»« TensorFlow [ 8] vÃ  PyTorch [ 5]. LLMCad táº­n dá»¥ng llama.cpp [ 45] (má»™t trong nhá»¯ng cÃ´ng cá»¥ LLM trÃªn thiáº¿t bá»‹ nháº¹ nháº¥t) lÃ m backend Ä‘iá»‡n thoáº¡i thÃ´ng minh vÃ  PyTorch [5] lÃ m backend thiáº¿t bá»‹ IoT.

Thiáº¿t láº­p pháº§n cá»©ng. ChÃºng tÃ´i thá»­ nghiá»‡m hiá»‡u suáº¥t cá»§a LLMCad trÃªn bá»‘n thiáº¿t bá»‹: 2 Ä‘iá»‡n thoáº¡i thÃ´ng minh (Xiaomi 10 vÃ  Xiaomi 12) vÃ  2 thiáº¿t bá»‹ IoT (Jetson TX2, vÃ  Jetson Orin), nhÆ° Ä‘Æ°á»£c tÃ³m táº¯t trong Báº£ng 2. ChÃºng tÃ´i cháº¡y LLM trÃªn GPU Jetson vÃ  CPU Ä‘iá»‡n thoáº¡i thÃ´ng minh, vÃ¬ cÃ¡c cÃ´ng cá»¥ LLM hiá»‡n táº¡i [refs] cÃ³ há»— trá»£ chÆ°a trÆ°á»Ÿng thÃ nh cho GPU/NPU Ä‘iá»‡n thoáº¡i thÃ´ng minh. Tuy nhiÃªn, thiáº¿t káº¿ cá»§a LLMCad trá»±c giao vá»›i cÃ¡c loáº¡i pháº§n cá»©ng.

MÃ´ hÃ¬nh vÃ  bá»™ dá»¯ liá»‡u. ChÃºng tÃ´i thá»­ nghiá»‡m vá»›i má»™t loáº¡t cÃ¡c mÃ´ hÃ¬nh LLM Ä‘iá»ƒn hÃ¬nh vá»›i nhiá»u bá»™ dá»¯ liá»‡u tÃ¡c vá»¥ sinh khÃ¡c nhau trÃªn cÃ¡c thiáº¿t bá»‹ khÃ¡c nhau, nhÆ° Ä‘Æ°á»£c tÃ³m táº¯t trong Báº£ng 3. TrÃªn cÃ¡c thiáº¿t bá»‹ IoT, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ LLMCad trÃªn hai tÃ¡c vá»¥ dá»‹ch thuáº­t, hai tÃ¡c vá»¥ tráº£ lá»i cÃ¢u há»i, má»™t tÃ¡c vá»¥ mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯, vÃ  má»™t tÃ¡c vá»¥ tÃ³m táº¯t vá»›i cÃ¡c mÃ´ hÃ¬nh mT5, T5, GPT2, vÃ  Bart. Táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c chÃºng tÃ´i tinh chá»‰nh nhÆ° [ 37] lÃ m. Äá»‘i vá»›i thiáº¿t bá»‹ Ä‘iá»‡n thoáº¡i thÃ´ng minh, chÃºng tÃ´i sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh Vicuna-1.5 vÃ  LLaMA2

9

--- TRANG 10 ---
Conference'17, July 2017, Washington, DC, USA Daliang Xuâ™¦, Wangsong Yinâ™¦, Xin Jinâ™¦, Ying Zhangâ™¦, Shiyun Weiâ‹†, Mengwei Xu^, Xuanzhe Liuâ™¦

vá»›i ba tÃ¡c vá»¥ dá»‹ch thuáº­t, má»™t tÃ¡c vá»¥ tráº£ lá»i cÃ¢u há»i vÃ , vÃ  má»™t tÃ³m táº¯t. Táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c táº£i xuá»‘ng tá»« kho hugging face [ 9] vÃ  Ä‘Ã£ Ä‘Æ°á»£c lÆ°á»£ng tá»­ hÃ³a bá»Ÿi AutoGPTQ [ 10] thÃ nh Ä‘á»‹nh dáº¡ng 4-bit Ä‘á»ƒ tiáº¿t kiá»‡m bá»™ nhá»› vÃ  cáº£i thiá»‡n tá»‘c Ä‘á»™ suy luáº­n.

ÄÆ°á»ng cÆ¡ sá»Ÿ. ChÃºng tÃ´i chá»§ yáº¿u so sÃ¡nh LLMCad vá»›i 5 Ä‘Æ°á»ng cÆ¡ sá»Ÿ tiÃªn tiáº¿n cÃ³ thá»ƒ Ä‘Æ°á»£c chia thÃ nh hai loáº¡i:
â€¢3x Ä‘Æ°á»ng cÆ¡ sá»Ÿ LLM Ä‘Æ¡n. (1) TiÃªu chuáº©n (Std) luÃ´n sá»­ dá»¥ng LLM má»¥c tiÃªu Ä‘á»ƒ táº¡o Ä‘áº§u ra, vá»›i PyTorch cho IoT vÃ  llama.cpp cho Ä‘iá»‡n thoáº¡i thÃ´ng minh. (2) ÄÆ°á»ng á»‘ng tiÃªu chuáº©n (StdPL) : NÃ³ thá»±c thi má»™t Ä‘Æ°á»ng á»‘ng theo lá»›p, chá»“ng chÃ©o I/O vÃ  tÃ­nh toÃ¡n, nhÆ° Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi cÃ¡c cÃ´ng cá»¥ suy luáº­n LLM SOTA hiá»‡n táº¡i. (3) Suy luáº­n Transformer Nhanh (STI) [27] : Má»™t khung suy luáº­n NLP edge vá»›i máº£nh tham sá»‘ lÆ°á»£ng tá»­ hÃ³a vÃ  Ä‘Æ°á»ng á»‘ng tÃ­nh toÃ¡n-táº£i tinh vi.
â€¢2x Ä‘Æ°á»ng cÆ¡ sá»Ÿ há»£p tÃ¡c LLM. (1) Giáº£i mÃ£ Suy Ä‘oÃ¡n (SP) [37]: Má»™t khung tiÃªn tiáº¿n cÅ©ng sá»­ dá»¥ng há»£p tÃ¡c LLM "bá»™ táº¡o vÃ  bá»™ xÃ¡c minh". (2) Bá»™ Giáº£i mÃ£ Transformer Lá»›n Nhá» (BLD) [37]: Má»™t thuáº­t toÃ¡n xÃ¡c Ä‘á»‹nh thá»i Ä‘iá»ƒm xÃ¡c minh vÃ  cÆ¡ cháº¿ quay láº¡i cho há»£p tÃ¡c LLM "bá»™ táº¡o vÃ  bá»™ xÃ¡c minh".

Chá»‰ sá»‘ vÃ  cáº¥u hÃ¬nh. ChÃºng tÃ´i chá»§ yáº¿u bÃ¡o cÃ¡o Ä‘á»™ chÃ­nh xÃ¡c táº¡o vÃ  thá»i gian táº¡o má»—i token. Äá»ƒ rÃµ rÃ ng, má»¥c tiÃªu cá»§a LLMCad lÃ  cÄƒn chá»‰nh Ä‘áº§u ra LLM thÆ°á»ng trÃº bá»™ nhá»› vá»›i LLM má»¥c tiÃªu. Do Ä‘Ã³, chÃºng tÃ´i coi vÄƒn báº£n Ä‘Æ°á»£c táº¡o bá»Ÿi LLM má»¥c tiÃªu lÃ  chuáº©n má»±c vÃ  tÃ­nh Ä‘iá»ƒm Rouge-L [ 43], má»™t sá»± tÆ°Æ¡ng tá»± giá»¯a hai chuá»—i dá»±a trÃªn chuá»—i con chung dÃ i nháº¥t, lÃ m Ä‘á»™ chÃ­nh xÃ¡c táº¡o .

4.2 Tá»‘c Ä‘á»™ Táº¡o

Hiá»‡u suáº¥t tá»•ng thá»ƒ. ChÃºng tÃ´i Ä‘áº§u tiÃªn Ä‘iá»u tra toÃ n diá»‡n hiá»‡u suáº¥t táº¡o cá»§a LLMCad trÃªn bá»‘n thiáº¿t bá»‹ Ä‘Æ°á»£c thá»­ nghiá»‡m. Káº¿t quáº£ Ä‘á»™ chÃ­nh xÃ¡c táº¡o vÃ  thá»i gian táº¡o má»—i token Ä‘Æ°á»£c minh há»a trong Báº£ng 4, HÃ¬nh 12 vÃ  HÃ¬nh 13, tÆ°Æ¡ng á»©ng. Quan sÃ¡t chÃ­nh cá»§a chÃºng tÃ´i lÃ  LLMCad nháº¥t quÃ¡n vÃ  Ä‘Ã¡ng ká»ƒ vÆ°á»£t trá»™i hÆ¡n cÃ¡c Ä‘Æ°á»ng cÆ¡ sá»Ÿ khÃ¡c vá» thá»i gian táº¡o má»—i token mÃ  khÃ´ng lÃ m giáº£m Ä‘á»™ chÃ­nh xÃ¡c trÃªn táº¥t cáº£ cÃ¡c thiáº¿t bá»‹ Ä‘Æ°á»£c thá»­ nghiá»‡m.

â€¢Thá»i gian táº¡o cá»§a LLMCad so vá»›i Ä‘Æ°á»ng cÆ¡ sá»Ÿ LLM Ä‘Æ¡n. So vá»›i Std, LLMCad Ä‘áº¡t Ä‘Æ°á»£c tÄƒng tá»‘c 2,9-9,3Ã— vÃ  3,47â€“4,67Ã— trong thá»i gian táº¡o trung bÃ¬nh má»—i token trÃªn thiáº¿t bá»‹ IoT vÃ  Ä‘iá»‡n thoáº¡i thÃ´ng minh, tÆ°Æ¡ng á»©ng, mÃ  khÃ´ng lÃ m giáº£m Ä‘á»™ chÃ­nh xÃ¡c. Cá»¥ thá»ƒ, LLMCad cÃ³ thá»ƒ táº¡o káº¿t quáº£ tráº£ lá»i cÃ¢u há»i trÃªn Xiaomi 11 vá»›i tá»‘c Ä‘á»™ nhanh nháº¥t lÃ  0,86 s/token. ThÃ nh tá»±u nÃ y cho phÃ©p táº¡o token thá»i gian thá»±c vá»›i LLM hÆ¡n 10B trÃªn thiáº¿t bá»‹ COTS láº§n Ä‘áº§u tiÃªn. Äiá»u nÃ y Ä‘Æ°á»£c gÃ¢y ra bá»Ÿi thá»±c táº¿ lÃ  LLMCad cÃ³ thá»ƒ á»§y thÃ¡c háº§u háº¿t viá»‡c táº¡o token cho LLM thÆ°á»ng trÃº bá»™ nhá»› vÃ  Ä‘áº£m báº£o tÃ­nh chÃ­nh xÃ¡c báº±ng xÃ¡c minh khÃ´ng tá»± há»“i quy.

Khi so sÃ¡nh vá»›i cÃ¡c Ä‘Æ°á»ng cÆ¡ sá»Ÿ cáº¡nh tranh hÆ¡n nhÆ° StdPL vÃ  STI, LLMCad giáº£m thá»i gian táº¡o trung bÃ¬nh má»—i token 2,9-9,3Ã— vÃ  1,83â€“2,45Ã—, tÆ°Æ¡ng á»©ng. Nhá»¯ng lá»£i Ã­ch Ä‘Ã³ Ä‘Æ°á»£c gÃ¢y ra bá»Ÿi thá»±c táº¿ viá»‡c sá»­ dá»¥ng LLM thÆ°á»ng trÃº bá»™ nhá»› Ä‘á»ƒ táº¡o vÄƒn báº£n nháº¥t quÃ¡n vÆ°á»£t trá»™i hÆ¡n báº¥t ká»³ phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»ng á»‘ng hoáº·c lÆ°á»£ng tá»­ hÃ³a nÃ o cá»§a LLM má»¥c tiÃªu trÃªn thiáº¿t bá»‹ di Ä‘á»™ng, trong Ä‘Ã³ LLM thÆ°á»ng trÃº bá»™ nhá»› cÃ³ thá»ƒ mang láº¡i cáº£i thiá»‡n tá»‘c Ä‘á»™ hÆ¡n hÃ ng trÄƒm láº§n so vá»›i LLM má»¥c tiÃªu. BÃªn cáº¡nh Ä‘Ã³, nÃ³ cÅ©ng cÃ³ thá»ƒ cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c táº¡o 11,1â€“19,0 Ä‘iá»ƒm pháº§n trÄƒm, so vá»›i STI. Äiá»u nÃ y cÃ³ lá»£i tá»« xÃ¡c minh cÃ¢y khÃ´ng tá»± há»“i quy cá»§a chÃºng tÃ´i cÃ³ thá»ƒ kiá»ƒm tra vÃ  sá»­a chá»¯a táº¥t cáº£ lá»—i bá»Ÿi LLM thÆ°á»ng trÃº bá»™ nhá»› má»™t cÃ¡ch hiá»‡u quáº£.

â€¢Thá»i gian táº¡o cá»§a LLMCad so vá»›i Ä‘Æ°á»ng cÆ¡ sá»Ÿ há»£p tÃ¡c LLM. So vá»›i BLD, LLMCad cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c táº¡o 4,5â€“94,5 vÃ  9,8â€“96,7 Ä‘iá»ƒm pháº§n trÄƒm vá»›i tÄƒng tá»‘c 1,1-1,4 Ã— vÃ  1,1â€“1,3Ã— trong thá»i gian táº¡o trung bÃ¬nh má»—i token trÃªn thiáº¿t bá»‹ IoT vÃ  Ä‘iá»‡n thoáº¡i thÃ´ng minh, tÆ°Æ¡ng á»©ng. ÄÃ³ lÃ  bá»Ÿi vÃ¬, khÃ´ng giá»‘ng nhÆ° BLD tÄƒng tá»‘c quÃ¡ trÃ¬nh táº¡o báº±ng cÃ¡ch giáº£m sá»‘ láº§n sá»­a chá»¯a (hy sinh Ä‘á»™ chÃ­nh xÃ¡c), chiáº¿n lÆ°á»£c dá»± phÃ²ng tá»± thÃ­ch á»©ng cá»§a chÃºng tÃ´i nháº±m giáº£m thiá»ƒu sá»‘ láº§n xÃ¡c minh trong khi Ä‘áº£m báº£o xÃ¡c minh cho má»—i token. PhÆ°Æ¡ng phÃ¡p nhÆ° váº­y tÄƒng cÆ°á»ng tá»‘c Ä‘á»™ táº¡o mÃ  khÃ´ng hy sinh Ä‘á»™ chÃ­nh xÃ¡c. HÆ¡n ná»¯a, thá»±c thi suy Ä‘oÃ¡n cho phÃ©p LLM thÆ°á»ng trÃº bá»™ nhá»› táº¡o vÄƒn báº£n sá»›m hÆ¡n mÃ  khÃ´ng chá» káº¿t quáº£ xÃ¡c minh khi khÃ´ng cÃ³ lá»—i Ä‘Æ°á»£c phÃ¡t hiá»‡n bá»Ÿi quÃ¡ trÃ¬nh xÃ¡c minh, giáº£m thÃªm Ä‘á»™ trá»… táº¡o.

TÆ°Æ¡ng tá»±, LLMCad cÃ³ thá»ƒ giáº£m thá»i gian táº¡o trung bÃ¬nh má»—i token 1,93â€“2,00 Ã— vÃ  1,34â€“1,77Ã— trÃªn thiáº¿t bá»‹ IoT vÃ  Ä‘iá»‡n thoáº¡i thÃ´ng minh, tÆ°Æ¡ng á»©ng. ÄÃ³ lÃ  bá»Ÿi vÃ¬, khÃ´ng giá»‘ng nhÆ° SP sá»­ dá»¥ng Ä‘á»™ dÃ i chuá»—i token, chiáº¿n lÆ°á»£c dá»± phÃ²ng tá»± thÃ­ch á»©ng cá»§a chÃºng tÃ´i cÃ³ thá»ƒ tÃ¬m chÃ­nh xÃ¡c khi LLM thÆ°á»ng trÃº bá»™ nhá»› táº¡o lá»—i vÃ  cÃ³ thá»ƒ giáº£m táº§n suáº¥t xÃ¡c minh.

4.3 PhÃ¢n tÃ­ch Äá»™ nháº¡y Bá»™ nhá»›

Pháº§n nÃ y Ä‘á»ƒ Ä‘iá»u tra tÃ¡c Ä‘á»™ng cá»§a cÃ¡c ngÃ¢n sÃ¡ch bá»™ nhá»› khÃ¡c nhau Ä‘á»‘i vá»›i phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i. ChÃºng tÃ´i tiáº¿n hÃ nh thÃªm thÃ­ nghiá»‡m trÃªn cÃ¡c mÃ´ hÃ¬nh mT5 vÃ  T5 trÃªn TX2 vÃ  LLaMa2 trÃªn Xiaomi 10 tÆ°Æ¡ng á»©ng dÆ°á»›i cÃ¡c ngÃ¢n sÃ¡ch bá»™ nhá»› khÃ¡c nhau (vÃ­ dá»¥, tá»« 4GB Ä‘áº¿n 8GB trÃªn Xiaomi 10). TÄƒng tá»‘c cá»§a cÃ¡c Ä‘Æ°á»ng cÆ¡ sá»Ÿ khÃ¡c nhau Ä‘Æ°á»£c thá»ƒ hiá»‡n trong HÃ¬nh 14. LLMCad nháº¥t quÃ¡n thá»ƒ hiá»‡n tÄƒng tá»‘c cao nháº¥t trong táº¥t cáº£ cÃ¡c Ä‘Æ°á»ng cÆ¡ sá»Ÿ tá»« 8GB Ä‘áº¿n 4GB vÃ  lá»£i Ã­ch cá»§a nÃ³ ná»•i báº­t hÆ¡n vá»›i ngÃ¢n sÃ¡ch bá»™ nhá»› giáº£m.

LLMCad giáº£m thá»i gian táº¡o dÆ°á»›i ngÃ¢n sÃ¡ch bá»™ nhá»› 6GB trÃªn Jetson TX2 5,54 Ã—, 1,76Ã— vÃ  1,12Ã— trung bÃ¬nh cho StdPL , SP vÃ  BLD, tÆ°Æ¡ng á»©ng; trong khi tÄƒng tá»‘c cho ngÃ¢n sÃ¡ch bá»™ nhá»› 4GB lÃ  6,12 Ã—, 1,91Ã— vÃ  1,25Ã—, tÆ°Æ¡ng á»©ng, lá»›n hÆ¡n 1,29 Ã—, 1,08Ã— vÃ  2,1Ã— so vá»›i dÆ°á»›i 6GB trung bÃ¬nh. TÆ°Æ¡ng tá»±, LLMCad Ä‘áº¡t Ä‘Æ°á»£c tÄƒng tá»‘c thá»i gian táº¡o dÆ°á»›i ngÃ¢n sÃ¡ch bá»™ nhá»› 4GB trÃªn xiaomi 10 4,72 Ã— vÃ  1,34Ã— trung bÃ¬nh cho StdPL vÃ  BLD, tÆ°Æ¡ng á»©ng. ÄÃ³ lÃ  bá»Ÿi vÃ¬ khi ngÃ¢n sÃ¡ch bá»™ nhá»› nghiÃªm ngáº·t hÆ¡n, khoáº£ng cÃ¡ch tá»‘c Ä‘á»™ suy luáº­n

10

--- TRANG 11 ---
LLMCad : Suy luáº­n MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n trÃªn Thiáº¿t bá»‹ Nhanh vÃ  CÃ³ thá»ƒ Má»Ÿ rá»™ng Conference'17, July 2017, Washington, DC, USA

MÃ´ hÃ¬nh Bá»™ dá»¯ liá»‡u StdPL SP BLD Cá»§a chÃºng tÃ´i
mT5-LargeT: IWLST17-de-en 100 100 96.1 100
QA: SQuAD 100 100 52.9 100
T5-LargeS: CNN/Daily 100 100 5.5 100
QA: SQuAD 100 100 51.4 100
Bart-Large T: WMT14-de-en 100 100 96.5 100
GPT2-Large LM: Wikitext 100 100 12.9 100

(a) Thiáº¿t bá»‹ IoTMÃ´ hÃ¬nh Bá»™ dá»¯ liá»‡u StdPL STI SP BLD Cá»§a chÃºng tÃ´i
Vicuna-13B
(INT4)T: Parrot 100 86.7 100 89.7 100
T: WMT22-de-en 100 87.2 100 90.2 100
T: WMT22-zh-en 100 88.1 100 80.4 100
LLaMa2-Chat
(INT4)S: CNN/Daily 100 81.0 100 7.96 100
QA: SQuAD 100 83.2 100 3.4 100
QA: Truthful_QA 100 85.4 100 4.7 100

(b) Äiá»‡n thoáº¡i thÃ´ng minh

Báº£ng 4: TÃ³m táº¯t Ä‘á»™ chÃ­nh xÃ¡c táº¡o cá»§a LLMCad vÃ  Ä‘Æ°á»ng cÆ¡ sá»Ÿ trÃªn cÃ¡c thiáº¿t bá»‹ Ä‘Æ°á»£c thá»­ nghiá»‡m. T:*, S:*, QA:*, vÃ  LM:* Ä‘áº¡i diá»‡n cho cÃ¡c tÃ¡c vá»¥ sinh cá»§a dá»‹ch thuáº­t, tÃ³m táº¯t, tráº£ lá»i cÃ¢u há»i, vÃ  mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯.

mt5_iwlst17_de_en  0  2  4  6Äá»™ trá»… trÃªn Orin (s)7.20 7.06
2.31
1.70
1.26Vanilla StdPL SP BLD Cá»§a chÃºng tÃ´i

mt5_squad_v2  0  2  4  67.20 7.06
1.80
1.050.78

t5_squad_v2  0  2  4  67.10 7.03
1.82
1.050.83

t5_CNN_Daily  0  2  4  67.10 7.03
2.74
2.01
1.38

bart_wmt14_de_en  0  2  4  66.10 6.10
2.031.82
1.34

gpt2_wikitext  0  2  4  6  88.007.69
5.335.06
2.70

mt5_iwlst17_de_en  0  2  4  6Äá»™ trá»… trÃªn TX2(s)7.20 7.06
2.34
1.70
1.26

mt5_squad_v2  0  2  4  67.20 7.06
1.84
1.130.90

t5_squad_v2  0  2  4  67.10 7.03
1.85
1.100.95

t5_CNN_Daily  0  2  4  67.10 7.03
2.77
2.05
1.49

bart_wmt14_de_en  0  2  4  66.10 6.10
2.081.85
1.41

gpt2_wikitext  0  2  4  6  88.007.69
5.375.13
2.78

HÃ¬nh 12: Äá»™ trá»… táº¡o trung bÃ¬nh má»—i token cá»§a LLMCad vÃ  cÃ¡c Ä‘Æ°á»ng cÆ¡ sá»Ÿ dÆ°á»›i cÃ¡c tÃ¡c vá»¥ khÃ¡c nhau trÃªn thiáº¿t bá»‹ IoT.

Vicuna_Parrot  0  5 10 15Äá»™ trá»… trÃªn Xiaomi 11 (s)15.214.9
8.3
6.0
3.73.3Vanilla StdPL STI SP BLD Cá»§a chÃºng tÃ´i

Vicuna_wmt22_de_en  0  5 10 1515.214.9
8.3
6.2
3.83.6

Vicuna_wmt22_zh_en  0  5 10 1515.215.0
8.3
6.4
4.5
3.4

LLaMa2_squad  0  5 10 1515.214.9
10.6
5.5
4.54.1

LLaMa2_CNN_daily  0  5 10 1515.214.9
7.8
5.04.33.6

LLaMa2_TruthfulQA  0  5 10 1515.214.9
10.6
6.1
4.3
3.3

Vicuna_Parrot  0  5 10 15Äá»™ trá»… trÃªn Xiaomi 10 (s)16.215.9
8.7
6.4
3.93.6

Vicuna_wmt22_de_en  0  5 10 1516.215.9
8.7
6.7
4.24.0

Vicuna_wmt22_zh_en  0  5 10 1516.216.0
8.7
6.8
4.9
3.7

LLaMa2_squad  0  5 10 1516.215.9
11.3
5.9
4.84.4

LLaMa2_CNN_daily  0  5 10 1516.215.9
8.1
5.54.73.8

LLaMa2_TruthfulQA  0  5 10 1516.215.9
11.3
6.6
4.7
3.8

HÃ¬nh 13: Äá»™ trá»… suy luáº­n trung bÃ¬nh má»—i token cá»§a LLMCad vÃ  cÃ¡c Ä‘Æ°á»ng cÆ¡ sá»Ÿ dÆ°á»›i cÃ¡c tÃ¡c vá»¥ khÃ¡c nhau trÃªn Ä‘iá»‡n thoáº¡i thÃ´ng minh.

MÃ´ hÃ¬nh-tÃ¡c vá»¥-bá»™ dá»¯ liá»‡u Vanilla StdPL SP BLD Cá»§a chÃºng tÃ´i
mT5-translation
IWLST17-DE->EN36.9 36.2 12.0 7.7 7.7 (4.8Ã—)
T5-summary
CNN/Daily36.4 36.0 7.6 10.3 8.4 (4.3Ã—)
T5-QA
SQuAD36.9 36.5 15.4 9.9 4.6 (8.0Ã—)

(a) Jetson Orin NXMÃ´ hÃ¬nh-tÃ¡c vá»¥-bá»™ dá»¯ liá»‡u Vanilla StdPL STI SP BLD Cá»§a chÃºng tÃ´i
LLaMa2-summarization
CNN/Daily mail56.2 55.1 27.9 21.5 18.6 17.3 (3.2Ã—)
LLaMa2-QA
TruthfulQA56.2 55.1 28.1 23.9 18.3 14.3 (3.9Ã—)
Vicuna-translation
WMT22-DE-EN56.2 55.1 20.7 20.4 20.3 15.5 (3.6Ã—)

(b) Xiaomi 11

Báº£ng 5: TÃ³m táº¯t tiÃªu thá»¥ nÄƒng lÆ°á»£ng (J) cá»§a cÃ¡c mÃ´ hÃ¬nh khÃ¡c nhau trÃªn cÃ¡c thiáº¿t bá»‹ khÃ¡c nhau.

11

--- TRANG 12 ---
Conference'17, July 2017, Washington, DC, USA Daliang Xuâ™¦, Wangsong Yinâ™¦, Xin Jinâ™¦, Ying Zhangâ™¦, Shiyun Weiâ‹†, Mengwei Xu^, Xuanzhe Liuâ™¦

4.85.05.25.45.6
t5_CNN_Daily12345TÄƒng tá»‘c trÃªn TX2 (x)
4.04.3 4.3 4.44.5
3.3 3.4 3.4 3.43.4
1.0 1.0 1.0 1.01.02.5 2.5 2.5 2.52.5Cá»§a chÃºng tÃ´i BLD StdPL SP

4.85.05.25.45.6
t5_squad_v202468
7.17.8 7.8 7.98.0
6.46.6 6.6 6.76.7
1.0 1.0 1.0 1.01.03.7 3.8 3.8 3.83.8

4 5 6 7 8
LLaMa2_CNN_daily246TÄƒng tá»‘c trÃªn Xiaomi10 (x)
4.8 4.9 4.95.05.5
3.6 3.6 3.6 3.7 3.8
3.1 3.1 3.1 3.2 3.3
2.1 2.2
1.1 1.0 1.1Cá»§a chÃºng tÃ´i BLD StdPL SP STI

4 5 6 7 8
LLaMa2_TruthfulQA24
4.7 4.7 4.7 4.85.1
3.7 3.7 3.7 3.7 3.8
2.8 2.8 2.8 2.9 3.0
1.8 1.8
0.9 1.0 1.1

HÃ¬nh 14: TÄƒng tá»‘c cá»§a cÃ¡c Ä‘Æ°á»ng cÆ¡ sá»Ÿ khÃ¡c nhau dÆ°á»›i cÃ¡c ngÃ¢n sÃ¡ch bá»™ nhá»› khÃ¡c nhau.

mT5_iwlst17_de_en  0  2  4  6Äá»™ trá»… (s)7.20
1.67 1.561.29Std Std+TGV Std+TGV+SPP Std+TGV+SPP+SF

T5_CNN_Daily  0  2  4  67.10
1.871.69 1.65

LLaMA2-SQuAD  02.5  57.5 1010.20
3.92
3.132.74

HÃ¬nh 15: NghiÃªn cá»©u loáº¡i bá» cá»§a LLMCad .

giá»¯a LLM thÆ°á»ng trÃº bá»™ nhá»› vÃ  má»¥c tiÃªu cÃ³ Ã½ nghÄ©a hÆ¡n, vÃ  viá»‡c á»§y thÃ¡c token cho LLM thÆ°á»ng trÃº bá»™ nhá»› cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c nhiá»u lá»£i Ã­ch hÆ¡n.

4.4 PhÃ¢n tÃ­ch TiÃªu thá»¥ NÄƒng lÆ°á»£ng

Sau Ä‘Ã³ chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ tiÃªu thá»¥ nÄƒng lÆ°á»£ng cá»§a LLMCad vá»›i cÃ¡c mÃ´ hÃ¬nh mT5 vÃ  T5 trÃªn thiáº¿t bá»‹ IoT vÃ  mÃ´ hÃ¬nh Vicuana vÃ  LLaMa2 trÃªn Ä‘iá»‡n thoáº¡i thÃ´ng minh. NhÆ° Ä‘Æ°á»£c thá»ƒ hiá»‡n trong Báº£ng 5, so vá»›i Std, StdPL, SP vÃ  BLD, LLMCad giáº£m tiÃªu thá»¥ nÄƒng lÆ°á»£ng má»—i token 4,35â€“7,96, 4,34â€“7,92, 1,56â€“3,33, vÃ  1,05â€“2,15Ã— trÃªn Jetson Orin NX, LLMCad Ä‘áº¡t Ä‘Æ°á»£c giáº£m tiÃªu thá»¥ nÄƒng lÆ°á»£ng 3,22â€“3,59, 3,18â€“3,56, 1,24â€“1,66, 1,07â€“1,31 vÃ  2,01â€“2,56, Ã— tÆ°Æ¡ng á»©ng trÃªn Xiaomi 11, cá»™ng vá»›i STI. Äiá»u nÃ y lÃ  bá»Ÿi vÃ¬ hai ká»¹ thuáº­t cá»§a LLMCad cÃ³ thá»ƒ á»§y thÃ¡c cÃ ng nhiá»u token cÃ ng tá»‘t cho LLM thÆ°á»ng trÃº bá»™ nhá»› mÃ  khÃ´ng hy sinh Ä‘á»™ chÃ­nh xÃ¡c.

So vá»›i tÄƒng tá»‘c Ä‘á»™ trá»…, tiÃªu thá»¥ nÄƒng lÆ°á»£ng cá»§a LLMCad tÆ°Æ¡ng Ä‘á»‘i tháº¥p hÆ¡n. TÃ¬nh huá»‘ng nÃ y phÃ¡t sinh bá»Ÿi vÃ¬ táº¡o suy Ä‘oÃ¡n cá»§a chÃºng tÃ´i song song hÃ³a viá»‡c thá»±c thi LLM thÆ°á»ng trÃº bá»™ nhá»› vÃ  má»¥c tiÃªu cÃ¹ng nhau, dáº«n Ä‘áº¿n tiÃªu thá»¥ nÄƒng lÆ°á»£ng nhiá»u hÆ¡n.

4.5 NghiÃªn cá»©u Loáº¡i bá»

Ká»¹ thuáº­t tá»•ng thá»ƒ. ChÃºng tÃ´i tiáº¿n hÃ nh thÃªm phÃ¢n tÃ­ch phÃ¢n tÃ¡ch vá» lá»£i Ã­ch mang láº¡i bá»Ÿi má»—i ká»¹ thuáº­t cá»§a LLMCad . CÃ¡c thÃ­ nghiá»‡m Ä‘Æ°á»£c thá»±c hiá»‡n vá»›i cÃ¡c mÃ´ hÃ¬nh mT5 vÃ  T5 trÃªn TX2 vÃ  LLaMa2 trÃªn Xiaomi 10. Káº¿t quáº£ Ä‘Æ°á»£c minh há»a trong HÃ¬nh 15. Thanh ngoÃ i cÃ¹ng bÃªn trÃ¡i giá»‘ng nhÆ° Ä‘Æ°á»ng cÆ¡ sá»Ÿ Vanilla , trong khi thanh ngoÃ i cÃ¹ng bÃªn trÃ¡i lÃ  LLMCad . Ba ká»¹ thuáº­t quan trá»ng táº¡o vÃ  xÃ¡c minh cÃ¢y token trong Â§3.2, chiáº¿n lÆ°á»£c dá»± phÃ²ng tá»± thÃ­ch á»©ng Â§3.3 vÃ  táº¡o suy Ä‘oÃ¡n trong Â§3.4 Ä‘Æ°á»£c Ä‘áº¡i diá»‡n bá»Ÿi TGV, SF vÃ  SPP tÆ°Æ¡ng á»©ng.

ChÃºng tÃ´i quan sÃ¡t ráº±ng táº¥t cáº£ cÃ¡c ká»¹ thuáº­t Ä‘á»u Ä‘Ã³ng gÃ³p khÃ´ng táº§m thÆ°á»ng vÃ o cáº£i thiá»‡n. Äáº§u tiÃªn, táº¡o vÃ  xÃ¡c minh cÃ¢y khÃ´ng tá»± há»“i quy cÃ³ thá»ƒ á»§y thÃ¡c háº§u háº¿t viá»‡c táº¡o token cho LLM thÆ°á»ng trÃº bá»™ nhá»›, dáº«n Ä‘áº¿n tÄƒng tá»‘c 2,6â€“4,3 cho cÃ¡c mÃ´ hÃ¬nh mT5, T5 vÃ  LLaMa2, tÆ°Æ¡ng á»©ng. Lá»£i Ã­ch nhiá»u hÆ¡n cho mÃ´ hÃ¬nh mT5 trÃªn bá»™ dá»¯ liá»‡u dá»‹ch IWLST lÃ  bá»Ÿi vÃ¬ mÃ´ hÃ¬nh mT5-Small cÃ³ thá»ƒ táº¡o ra nhiá»u token chÃ­nh xÃ¡c hÆ¡n so vá»›i hai mÃ´ hÃ¬nh khÃ¡c nÃªn nhiá»u token hÆ¡n cÃ³ thá»ƒ Ä‘Æ°á»£c á»§y thÃ¡c cho LLM thÆ°á»ng trÃº bá»™ nhá»›. BÃªn cáº¡nh Ä‘Ã³, thá»±c thi suy Ä‘oÃ¡n cÃ³ thá»ƒ giáº£m thá»i gian táº¡o má»—i token lÃªn Ä‘áº¿n 1,51Ã—. ÄÃ³ lÃ  bá»Ÿi vÃ¬ LLMCad cÃ³ thá»ƒ sá»­ dá»¥ng trá»±c tiáº¿p káº¿t quáº£ suy Ä‘oÃ¡n khi xÃ¡c minh khÃ´ng phÃ¡t hiá»‡n lá»—i, Ä‘áº·c biá»‡t Ä‘Ãºng cho mÃ´ hÃ¬nh LLaMA2. Cuá»‘i cÃ¹ng, Chiáº¿n lÆ°á»£c dá»± phÃ²ng tá»± thÃ­ch á»©ng Ä‘áº¡t Ä‘Æ°á»£c tÄƒng tá»‘c 1,08â€“1,20 Ã—. Äiá»u nÃ y Ä‘áº¡t Ä‘Æ°á»£c báº±ng cÃ¡ch táº­n dá»¥ng Ä‘á»™ tin cáº­y tÃ­ch lÅ©y cÃ¢y Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ xÃ¡c suáº¥t lá»—i vÃ  Ä‘iá»u chá»‰nh Ä‘á»™ng thá»i Ä‘iá»ƒm xÃ¡c minh Ä‘á»ƒ Ä‘Ã¡p á»©ng vá»›i sá»± thay Ä‘á»•i trong Ä‘á»™ phá»©c táº¡p tÃ¡c vá»¥.

5 CÃ”NG TRÃŒNH LIÃŠN QUAN

Há»£p tÃ¡c mÃ´ hÃ¬nh lÃ  má»™t ká»¹ thuáº­t tá»‘i Æ°u hÃ³a phá»• biáº¿n Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ giáº£m Ä‘á»™ trá»… suy luáº­n [ 40,70]. Ã tÆ°á»Ÿng chÃ­nh cá»§a nÃ³ lÃ  á»§y thÃ¡c háº§u háº¿t khá»‘i lÆ°á»£ng cÃ´ng viá»‡c cho cÃ¡c mÃ´ hÃ¬nh nháº¹ Ä‘á»ƒ giáº£m Ä‘á»™ trá»… suy luáº­n trong khi duy trÃ¬ Ä‘á»™ chÃ­nh xÃ¡c tÆ°Æ¡ng Ä‘á»‘i cao. Tabi [ 70] lÃ  má»™t cÃ´ng cá»¥ suy luáº­n Ä‘a cáº¥p phá»¥c vá»¥ truy váº¥n sá»­ dá»¥ng nhiá»u mÃ´ hÃ¬nh nhá» khÃ¡c nhau tÃ¹y theo Ä‘á»™ khÃ³ cá»§a truy váº¥n. MobiSR [40] vÃ  NestDNN [24] sá»­ dá»¥ng Ã½ tÆ°á»Ÿng tÆ°Æ¡ng tá»± nhÆ°ng phá»¥ thuá»™c vÃ o Ä‘á»™ phÃ¢n giáº£i hoáº·c tÃ i nguyÃªn cÃ³ sáºµn. CÃ¡c cÃ´ng trÃ¬nh "thoÃ¡t sá»›m" khÃ¡c [ 60,62,73,81], Ä‘á» xuáº¥t thá»i Ä‘iá»ƒm thoÃ¡t thÃ­ch á»©ng dá»±a vÃ o Ä‘á»™ khÃ³ dá»¯ liá»‡u Ä‘áº§u vÃ o cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c coi lÃ  má»™t sá»± há»£p tÃ¡c. Tuy nhiÃªn, chÃºng táº­p trung vÃ o kiáº¿n trÃºc mÃ´ hÃ¬nh dá»±a trÃªn CNN/encoder hoáº·c pháº£i sá»­a Ä‘á»•i vÃ  Ä‘Ã o táº¡o láº¡i mÃ´ hÃ¬nh, khÃ³ phÃ¹ há»£p vá»›i suy luáº­n LLM trÃªn thiáº¿t bá»‹. CÃ¡c cÃ´ng trÃ¬nh liÃªn quan gáº§n nháº¥t lÃ  giáº£i mÃ£ suy Ä‘oÃ¡n [20,37,41,48], cÅ©ng sá»­ dá»¥ng LLM nhá» hÆ¡n Ä‘á»ƒ táº¡o vÄƒn báº£n vÃ  LLM lá»›n hÆ¡n Ä‘á»ƒ xÃ¡c minh vÄƒn báº£n. LLMCad Ä‘Æ°á»£c thÃºc Ä‘áº©y bá»Ÿi nhá»¯ng cÃ´ng trÃ¬nh nÃ y vÃ  lÃ  cÃ´ng cá»¥ suy luáº­n Ä‘áº§u tiÃªn cho cÃ¡c tÃ¡c vá»¥ NLP sinh trÃªn thiáº¿t bá»‹, xem xÃ©t cÃ¡c thÃ¡ch thá»©c Ä‘á»™c Ä‘Ã¡o cá»§a thiáº¿t bá»‹ di Ä‘á»™ng nhÆ° tÃ¬nh huá»‘ng giá»›i háº¡n bá»™ nhá»›.

Tá»‘i Æ°u hÃ³a ML di Ä‘á»™ng. CÃ¡c phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u hÃ³a há»c mÃ¡y, nhÆ° nÃ©n mÃ´ hÃ¬nh [25,26,33,50,52,68,

12

--- TRANG 13 ---
LLMCad : Suy luáº­n MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n trÃªn Thiáº¿t bá»‹ Nhanh vÃ  CÃ³ thá»ƒ Má»Ÿ rá»™ng Conference'17, July 2017, Washington, DC, USA

76,77], giáº£m kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh báº±ng lÆ°á»£ng tá»­ hÃ³a vÃ  chÆ°ng cáº¥t kiáº¿n thá»©c, bá»™ nhá»› Ä‘á»‡m [69,74,80], giáº£m tÃ­nh toÃ¡n báº±ng cÃ¡ch tÃ¡i sá»­ dá»¥ng káº¿t quáº£ hiá»‡n cÃ³, vÃ  cáº¯t tá»‰a token [16,18,38, 57,66], giáº£m tÃ­nh toÃ¡n báº±ng cÃ¡ch cáº¯t tá»‰a cÃ¡c token vÃ´ dá»¥ng, Ä‘Ã£ Ä‘Æ°á»£c nghiÃªn cá»©u rá»™ng rÃ£i Ä‘á»ƒ giáº£m Ä‘á»™ trá»… táº¡o. LLMCad trá»±c giao vá»›i vÃ  tÆ°Æ¡ng thÃ­ch vá»›i nhá»¯ng tá»‘i Æ°u hÃ³a cáº¥p Ä‘á»™ thuáº­t toÃ¡n Ä‘Ã³.

BÃªn cáº¡nh Ä‘Ã³, má»™t sá»‘ nhÃ  nghiÃªn cá»©u táº­p trung vÃ o táº¡o vÄƒn báº£n theo cÃ¡ch khÃ´ng tá»± há»“i quy [ 35,39]. Tuy nhiÃªn, nhá»¯ng cÃ´ng trÃ¬nh nÃ y chá»‰ cÃ³ thá»ƒ Ã¡p dá»¥ng cho cÃ¡c mÃ´ hÃ¬nh <1B vÃ  cÃ³ váº¥n Ä‘á» suy giáº£m Ä‘á»™ chÃ­nh xÃ¡c, khÃ´ng pháº£i hÆ°á»›ng nghiÃªn cá»©u chÃ­nh.

Tá»‘i Æ°u hÃ³a Ä‘Æ°á»ng á»‘ng cho ML. Tá»‘i Æ°u hÃ³a Ä‘Æ°á»ng á»‘ng Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i Ä‘á»ƒ tÄƒng tá»‘c ML [ 13,27,49,67,82]. Háº§u háº¿t trong sá»‘ chÃºng, nhÆ° PipeDream [ 49], Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ má»Ÿ rá»™ng ML ra nhiá»u mÃ¡y báº±ng cÃ¡ch Ä‘Æ°á»ng á»‘ng tÃ­nh toÃ¡n tiáº¿n/lÃ¹i vá»›i Ä‘á»“ng bá»™ hÃ³a kÃ­ch hoáº¡t/gradient Ä‘á»ƒ giáº£m thiá»ƒu bong bÃ³ng I/O vÃ  giao tiáº¿p máº¡ng. Tuy nhiÃªn, cÃ³ má»™t sá»‘ nghiÃªn cá»©u táº­p trung vÃ o tá»‘i Æ°u hÃ³a mÃ¡y/tÃ¡c vá»¥ Ä‘Æ¡n. VÃ­ dá»¥, PipeSwitch [ 13] giá»›i thiá»‡u Ä‘Æ°á»ng á»‘ng truyá»n mÃ´ hÃ¬nh qua PCIe vÃ  thá»±c thi tÃ¡c vá»¥ trong GPU Ä‘á»ƒ giáº£m chi phÃ­ hiá»‡u suáº¥t chuyá»ƒn Ä‘á»•i ngá»¯ cáº£nh; trong khi STI [27] Ä‘Æ°á»ng á»‘ng táº£i máº£nh mÃ´ hÃ¬nh vá»›i tÃ­nh toÃ¡n cá»§a nÃ³ Ä‘á»ƒ giáº£m Ä‘á»™ trá»… suy luáº­n. LLMCad Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« nhá»¯ng ná»— lá»±c nÃ y vÃ  Ä‘á» xuáº¥t má»™t Ä‘Æ°á»ng á»‘ng táº¡o suy Ä‘oÃ¡n hiá»‡u quáº£ Ä‘á»ƒ giáº£i quyáº¿t thÃ¡ch thá»©c cháº·n I/O vÃ  tÃ­nh song song háº¡n cháº¿.

6 Káº¾T LUáº¬N

CÃ´ng trÃ¬nh nÃ y Ä‘Ã£ Ä‘á» xuáº¥t LLMCad , cÃ´ng cá»¥ suy luáº­n hiá»‡u quáº£ Ä‘áº§u tiÃªn cho cÃ¡c tÃ¡c vá»¥ NLP sinh trÃªn thiáº¿t bá»‹. NÃ³ phÃ¡ vá»¡ rÃ o cáº£n bá»™ nhá»› vÃ  mang kháº£ nÄƒng tá»· lá»‡ cá»§a LLM Ä‘áº¿n thiáº¿t bá»‹ di Ä‘á»™ng NÃ³ káº¿t há»£p ba ká»¹ thuáº­t má»›i, bao gá»“m: táº¡o vÃ  xÃ¡c minh cÃ¢y token, Chiáº¿n lÆ°á»£c dá»± phÃ²ng tá»± thÃ­ch á»©ng vÃ  Ä‘Æ°á»ng á»‘ng táº¡o suy Ä‘oÃ¡n cÃ³ thá»ƒ khai thÃ¡c cÃ¡c tÃ i nguyÃªn pháº§n cá»©ng lÃ£ng phÃ­ trong quÃ¡ trÃ¬nh xÃ¡c minh. CÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i Ä‘Ã£ chá»©ng minh ráº±ng khi so sÃ¡nh vá»›i cÃ¡c cÃ´ng cá»¥ LLM tiÃªn tiáº¿n, LLMCad cÃ³ thá»ƒ giáº£m thá»i gian táº¡o trung bÃ¬nh má»—i token 2,9â€“9,3 Ã— vÃ  3,5â€“4,7Ã— trÃªn thiáº¿t bá»‹ IoT vÃ  Ä‘iá»‡n thoáº¡i thÃ´ng minh, mÃ  khÃ´ng lÃ m giáº£m Ä‘á»™ chÃ­nh xÃ¡c.

TÃ€I LIá»†U THAM KHáº¢O

[1]Gboard - the Google Keyboard - Apps on Google Play â€” play.google.com. https://play.google.com/store/apps/details?id=com. google.android.inputmethod.latin&hl=en. [Accessed 22-Jul-2023].
[2]Google Translate - Apps on Google Play â€” play.google.com. https://play.google.com/store/apps/details?id=com.google.android. apps.translate&hl=en_US. [Accessed 22-Jul-2023].
[3]iTranslate â€” itranslate.com. https://itranslate.com/. [Accessed 22-Jul- 2023].
[4] MNN. https://github.com/alibaba/MNN. Accessed: [2023.7].
[5] PyTorch. https://pytorch.org/. Accessed: [2023.7].
[6]Siri â€” apple.com. https://www.apple.com/siri/. [Accessed 22-Jul-2023].
[7] TensorFlow. https://vllm.ai/. Accessed: [2023.7].
[8] TensorFlow. https://www.tensorflow.org/. Accessed: [2023.7].
[9] TensorFlow. https://huggingface.co/TheBloke. Accessed: [2023.7].
[10] TensorFlow. https://github.com/PanQiWei/AutoGPTQ. Accessed: [2023.7].
[11] Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang, Stephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer. Scaling laws for generative mixed-modal language models. arXiv preprint arXiv:2301.03728, 2023.
[12] Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling laws in language and vision. Advances in Neural Information Processing Systems, 35:22300â€“22312, 2022.
[13] Zhihao Bai, Zhen Zhang, Yibo Zhu, and Xin Jin. {PipeSwitch}: Fast pipelined context switching for deep learning applications. In14th USENIX Symposium onOperating Systems Design and Implementation (OSDI 20), pages 499â€“514, 2020.
[14] David Bau, Hendrik Strobelt, William Peebles, Jonas Wulff, Bolei Zhou, Jun-Yan Zhu, and Antonio Torralba. Semantic photo manipulation with a generative image prior. arXiv preprint arXiv:2005.07727, 2020.
[15] Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ale s Tam- chyna. Findings of the 2014 workshop on statistical machine trans- lation. In Proceedings oftheNinth Workshop onStatistical Machine Translation , pages 12â€“58, Baltimore, Maryland, USA, June 2014. Asso- ciation for Computational Linguistics.
[16] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022.
[17] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learn- ers. Advances inneural information processing systems , 33:1877â€“ 1901, 2020.
[18] Han Cai, Ji Lin, Yujun Lin, Zhijian Liu, Haotian Tang, Hanrui Wang, Ligeng Zhu, and Song Han. Enable deep learning on mobile devices: Methods, systems, and applications. ACM Transactions onDesign Automation ofElectronic Systems (TODAES), 27(3):1â€“50, 2022.
[19] Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian StÃ¼ker, Katsuhito Sudoh, Koichiro Yoshino, and Chris- tian Federmann. Overview of the IWSLT 2017 evaluation cam- paign. In Proceedings ofthe14th International Conference onSpoken Language Translation , pages 2â€“14, Tokyo, Japan, December 14-15 2017. International Workshop on Spoken Language Translation.
[20] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large lan- guage model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023.
[21] Aidan Clark, Diego De Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed language models. In International Conference onMachine Learning , pages 4057â€“4086. PMLR, 2022.
[22] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Hee- woo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
[23] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings ofthe60th Annual Meeting oftheAssociation forComputational Linguistics (Volume 1: Long Papers), pages 320â€“335, 2022.

13

--- TRANG 14 ---
Conference'17, July 2017, Washington, DC, USA Daliang Xuâ™¦, Wangsong Yinâ™¦, Xin Jinâ™¦, Ying Zhangâ™¦, Shiyun Weiâ‹†, Mengwei Xu^, Xuanzhe Liuâ™¦

[24] Biyi Fang, Xiao Zeng, and Mi Zhang. Nestdnn: Resource-aware multi- tenant on-device deep learning for continuous mobile vision. In Proceedings ofthe24th Annual International Conference onMobile Computing andNetworking, pages 115â€“127, 2018.
[25] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained trans- formers. arXiv preprint arXiv:2210.17323, 2022.
[26] Hui Guan, Shaoshan Liu, Xiaolong Ma, Wei Niu, Bin Ren, Xipeng Shen, Yanzhi Wang, and Pu Zhao. Cocopie: Enabling real-time ai on off-the-shelf mobile devices via compression-compilation co-design. Communications oftheACM, 64(6):62â€“68, 2021.
[27] Liwei Guo, Wonkyo Choe, and Felix Xiaozhu Lin. Sti: Turbocharge nlp inference at the edge via elastic pipelining. In Proceedings of the28th ACM International Conference onArchitectural Support for Programming Languages and Operating Systems, Volume 2, pages 791â€“803, 2023.
[28] Ellen L. Hahne. Round-robin scheduling for max-min fairness in data networks. IEEE Journal onSelected Areas incommunications , 9(7):1024â€“1039, 1991.
[29] Rui Han, Qinglong Zhang, Chi Harold Liu, Guoren Wang, Jian Tang, and Lydia Y Chen. Legodnn: block-grained scaling of deep neu- ral networks for mobile vision. In Proceedings ofthe27th Annual International Conference onMobile Computing and Networking , pages 406â€“419, 2021.
[30] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multi- task language understanding. arXiv preprint arXiv:2009.03300, 2020.
[31] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adapta- tion of large language models. arXiv preprint arXiv:2106.09685 , 2021.
[32] Chien-Chin Huang, Gu Jin, and Jinyang Li. Swapadvisor: Push- ing deep learning beyond the gpu memory limit via smart swap- ping. In Proceedings oftheTwenty-Fifth International Conference onArchitectural Support forProgramming Languages andOperating Systems, pages 1341â€“1355, 2020.
[33] Loc N Huynh, Youngki Lee, and Rajesh Krishna Balan. Deepmon: Mo- bile gpu-based deep learning framework for continuous vision appli- cations. In Proceedings ofthe15th Annual International Conference onMobile Systems, Applications, andServices, pages 82â€“95, 2017.
[34] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Ben- jamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.
[35] Jungo Kasai, Nikolaos Pappas, Hao Peng, James Cross, and Noah A Smith. Deep encoder, shallow decoder: Reevaluating non- autoregressive machine translation. arXiv preprint arXiv:2006.10369 , 2020.
[36] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W Mahoney, et al. Full stack optimization of transformer inference: a survey. arXiv preprint arXiv:2302.14017, 2023.
[37] Sehoon Kim, Karttikeya Mangalam, Jitendra Malik, Michael W Ma- honey, Amir Gholami, and Kurt Keutzer. Big little transformer decoder. arXiv preprint arXiv:2302.07863, 2023.
[38] Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph Hassoun, and Kurt Keutzer. Learned token pruning for transformers. In Proceedings ofthe28th ACM SIGKDD Conference onKnowledge Discovery andData Mining, pages 784â€“794, 2022.
[39] Xiang Kong, Adithya Renduchintala, James Cross, Yuqing Tang, Jiatao Gu, and Xian Li. Multilingual neural machine translation with deep en- coder and multiple shallow decoders. arXiv preprint arXiv:2206.02079 , 2022.
[40] Royson Lee, Stylianos I Venieris, Lukasz Dudziak, Sourav Bhattacharya, and Nicholas D Lane. Mobisr: Efficient on-device super-resolution through heterogeneous mobile processors. In The 25th annual international conference onmobile computing andnetworking , pages 1â€“16, 2019.
[41] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference onMachine Learning, pages 19274â€“19286. PMLR, 2023.
[42] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Ab- delrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettle- moyer. BART: denoising sequence-to-sequence pre-training for nat- ural language generation, translation, and comprehension. CoRR , abs/1910.13461, 2019.
[43] Chin-Yew Lin. Rouge: A package for automatic evaluation of sum- maries. In Text summarization branches out, pages 74â€“81, 2004.
[44] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods, 2021.
[45] llama.cpp. Port of Facebook's LLaMA model in C/C++ Resources. https://github.com/ggerganov/llama.cpp, Year of publication. Ac- cessed: [2023.7].
[46] Chen Meng, Minmin Sun, Jun Yang, Minghui Qiu, and Yang Gu. Train- ing deeper models by gpu memory optimization on tensorflow. In Proc. ofMLSystems Workshop inNIPS, volume 7, 2017.
[47] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016.
[48] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative llm serving with speculative inference and token tree verification. arXiv preprint arXiv:2305.09781, 2023.
[49] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. InProceedings ofthe27th ACM Symposium onOperating Systems Principles, pages 1â€“15, 2019.
[50] Wei Niu, Xiaolong Ma, Sheng Lin, Shihao Wang, Xuehai Qian, Xue Lin, Yanzhi Wang, and Bin Ren. Patdnn: Achieving real-time dnn execution on mobile devices with pattern-based weight prun- ing. In Proceedings oftheTwenty-Fifth International Conference onArchitectural Support forProgramming Languages andOperating Systems, pages 907â€“922, 2020.
[51] Roma Patel and Ellie Pavlick. Mapping language models to grounded conceptual spaces. In International Conference onLearning Representations, 2021.
[52] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. Proceedings of Machine Learning andSystems, 5, 2023.
[53] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.
[54] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[55] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. TheJournal ofMachine Learning Research, 21(1):5485â€“5551, 2020.
[56] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ Questions for Machine Comprehension of Text. arXiv e-prints, page arXiv:1606.05250, 2016.
[57] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient vision transformers with dynamic

14

--- TRANG 15 ---
LLMCad : Suy luáº­n MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n trÃªn Thiáº¿t bá»‹ Nhanh vÃ  CÃ³ thá»ƒ Má»Ÿ rá»™ng Conference'17, July 2017, Washington, DC, USA

token sparsification. Advances inneural information processing systems, 34:13937â€“13949, 2021.
[58] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthe- sis. In International conference onmachine learning , pages 1060â€“1069. PMLR, 2016.
[59] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. High-resolution image synthesis with latent diffu- sion models. In Proceedings oftheIEEE/CVF conference oncomputer vision andpattern recognition, pages 10684â€“10695, 2022.
[60] Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi Tay, and Donald Metzler. Confident adaptive lan- guage modeling. Advances inNeural Information Processing Systems , 35:17456â€“17472, 2022.
[61] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointer-generator networks. In Proceedings ofthe55th Annual Meeting oftheAssociation forComputational Linguistics (Volume 1:Long Papers) , pages 1073â€“1083, Vancouver, Canada, July 2017. Association for Computational Linguistics.
[62] Thierry Tambe, Coleman Hooper, Lillian Pentecost, Tianyu Jia, En-Yu Yang, Marco Donato, Victor Sanh, Paul Whatmough, Alexander M Rush, David Brooks, et al. Edgebert: Sentence-level energy optimiza- tions for latency-aware multi-task nlp inference. In MICRO-54: 54th Annual IEEE/ACM International Symposium onMicroarchitecture , pages 830â€“844, 2021.
[63] FP Team. Generative AI: Advantages, Disadvantages, Limitations, and Challenges â€” fact.technology. https://fact.technology/learn/ generative-ai-advantages-limitations-and-challenges/. [Accessed 22- Jul-2023].
[64] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie- Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[65] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances inneural information processing systems , 30, 2017.
[66] Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse attention architecture with cascade token and head pruning. In 2021 IEEE International Symposium onHigh-Performance Computer Architecture (HPCA), pages 97â€“110. IEEE, 2021.
[67] Shibo Wang, Jinliang Wei, Amit Sabne, Andy Davis, Berkin Ilbeyi, Blake Hechtman, Dehao Chen, Karthik Srinivasa Murthy, Marcello Maggioni, Qiao Zhang, et al. Overlap communication with depen- dent computation via decomposition in large deep learning mod- els. In Proceedings ofthe28th ACM International Conference on Architectural Support forProgramming Languages and Operating Systems, Volume 1, pages 93â€“106, 2022.
[68] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-attention distillation for task-agnostic com- pression of pre-trained transformers. Advances inNeural Information Processing Systems, 33:5776â€“5788, 2020.
[69] Xiaofei Wang, Yiwen Han, Victor CM Leung, Dusit Niyato, Xueqiang Yan, and Xu Chen. Convergence of edge computing and deep learning: A comprehensive survey. IEEE Communications Surveys &Tutorials , 22(2):869â€“904, 2020.
[70] Yiding Wang, Kai Chen, Haisheng Tan, and Kun Guo. Tabi: An efficient multi-level inference system for large language models. In Proceedings oftheEighteenth European Conference onComputer Systems , pages 233â€“248, 2023.
[71] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebas- tian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald

Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.
[72] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompt- ing elicits reasoning in large language models. Advances inNeural Information Processing Systems, 35:24824â€“24837, 2022.
[73] Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. Deebert: Dynamic early exiting for accelerating bert inference. arXiv preprint arXiv:2004.12993, 2020.
[74] Mengwei Xu, Mengze Zhu, Yunxin Liu, Felix Xiaozhu Lin, and Xu- anzhe Liu. Deepcache: Principled cache for mobile deep vision. In Proceedings ofthe24th annual international conference onmobile computing andnetworking, pages 129â€“144, 2018.
[75] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al- Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: A mas- sively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934, 2020.
[76] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. Advances in Neural Information Processing Systems, 35:27168â€“27183, 2022.
[77] Zhao You, Shulin Feng, Dan Su, and Dong Yu. Speechmoe2: Mixture- of-experts model with improved routing. In ICASSP 2022-2022 IEEE International Conference onAcoustics, Speech andSignal Processing (ICASSP), pages 7217â€“7221. IEEE, 2022.
[78] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: A distributed serving system for {Transformer-Based }generative models. In 16th USENIX Symposium onOperating Systems Design andImplementation (OSDI 22), pages 521â€“538, 2022.
[79] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 , 2022.
[80] Wuyang Zhang, Zhezhi He, Luyang Liu, Zhenhua Jia, Yunxin Liu, Marco Gruteser, Dipankar Raychaudhuri, and Yanyong Zhang. Elf: accelerate high-resolution mobile deep vision with content-aware parallel offloading. In Proceedings ofthe27th Annual International Conference onMobile Computing andNetworking , pages 201â€“214, 2021.
[81] Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian McAuley, Ke Xu, and Furu Wei. Bert loses patience: Fast and robust inference with early exit. Advances inNeural Information Processing Systems , 33:18330â€“18341, 2020.
[82] Zhe Zhou, Xuechao Wei, Jiejing Zhang, and Guangyu Sun. {PetS}: A unified framework for {Parameter-Efficient }transformers serving. In 2022 USENIX Annual Technical Conference (USENIX ATC 22), pages 489â€“504, 2022.

15
