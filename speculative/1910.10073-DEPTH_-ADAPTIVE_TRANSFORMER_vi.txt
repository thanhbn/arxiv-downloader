# DEPTH-ADAPTIVE TRANSFORMER
Maha Elbayad
Univ. Grenoble AlpesJiatao Gu, Edouard Grave, Michael Auli
Facebook AI Research

TÓM TẮT
Các mô hình sequence-to-sequence tiên tiến cho các tác vụ quy mô lớn thực hiện một số lượng tính toán cố định cho mỗi chuỗi đầu vào bất kể nó dễ hay khó xử lý. Trong bài báo này, chúng tôi huấn luyện các mô hình Transformer có thể đưa ra dự đoán đầu ra ở các giai đoạn khác nhau của mạng và chúng tôi nghiên cứu các cách khác nhau để dự đoán lượng tính toán cần thiết cho một chuỗi cụ thể. Khác với tính toán động trong Universal Transformers, áp dụng cùng một tập hợp các lớp một cách lặp lại, chúng tôi áp dụng các lớp khác nhau ở mỗi bước để điều chỉnh cả lượng tính toán cũng như khả năng của mô hình. Trên bài toán dịch IWSLT German-English, phương pháp của chúng tôi đạt độ chính xác tương đương với mô hình Transformer baseline được điều chỉnh tốt trong khi sử dụng ít hơn một phần tư số lớp decoder.

1 GIỚI THIỆU
Kích thước của các mô hình chuỗi thần kinh hiện đại (Gehring et al., 2017; Vaswani et al., 2017; Devlin et al., 2019) có thể lên tới hàng tỷ tham số (Radford et al., 2019). Ví dụ, đội thắng cuộc trong nhiệm vụ dịch máy tin tức WMT'19 tiếng Anh-Đức đã sử dụng một ensemble tổng cộng hai tỷ tham số (Ng et al., 2019). Trong khi các mô hình lớn được yêu cầu để làm tốt hơn trên các ví dụ khó, các mô hình nhỏ có khả năng hoạt động tương tự trên các ví dụ dễ, ví dụ, ensemble nói trên có lẽ không cần thiết để dịch một cụm từ ngắn như "Thank you". Tuy nhiên, các mô hình hiện tại áp dụng cùng một lượng tính toán bất kể đầu vào là dễ hay khó.

Trong bài báo này, chúng tôi đề xuất các Transformer thích ứng số lượng lớp theo từng đầu vào để đạt được sự cân bằng tốt giữa tốc độ và độ chính xác tại thời gian suy luận. Chúng tôi mở rộng Graves (2016; ACT) người đã giới thiệu tính toán động cho mạng thần kinh hồi quy theo nhiều cách: chúng tôi áp dụng các lớp khác nhau ở mỗi giai đoạn, chúng tôi nghiên cứu một loạt các thiết kế và mục tiêu huấn luyện cho mô-đun dừng và chúng tôi giám sát một cách rõ ràng thông qua các oracle đơn giản để đạt hiệu suất tốt trên các tác vụ quy mô lớn.

Universal Transformers (UT) dựa vào ACT cho tính toán động và áp dụng lặp lại cùng một lớp (Dehghani et al., 2018). Công trình của chúng tôi xem xét nhiều cơ chế khác nhau để ước tính độ sâu mạng và áp dụng một lớp khác nhau ở mỗi bước. Hơn nữa, Dehghani et al. (2018) cố định số bước cho dịch máy quy mô lớn trong khi chúng tôi thay đổi số bước để chứng minh những cải thiện đáng kể về tốc độ mà không mất độ chính xác. UT sử dụng một lớp chứa nhiều trọng số như toàn bộ Transformer tiêu chuẩn và lớp này được áp dụng nhiều lần ảnh hưởng đến tốc độ. Phương pháp của chúng tôi không tăng kích thước của các lớp riêng lẻ. Chúng tôi cũng mở rộng công trình phân loại đối tượng hiệu quả tài nguyên của Huang et al. (2017) và Bolukbasi et al. (2017) cho dự đoán có cấu trúc nơi các quyết định tính toán động ảnh hưởng đến tính toán tương lai. Các công trình liên quan từ thị giác máy tính bao gồm Teerapittayanon et al. (2016); Figurnov et al. (2017) và Wang et al. (2018) những người đã khám phá ý tưởng định tuyến động bằng cách thoát sớm hoặc bỏ qua các lớp.

Chúng tôi mã hóa chuỗi đầu vào sử dụng encoder Transformer tiêu chuẩn để tạo ra chuỗi đầu ra với lượng tính toán thay đổi trong mạng decoder. Tính toán động đặt ra thách thức cho self-attention vì các lớp bị bỏ qua ở các bước thời gian trước có thể được yêu cầu trong tương lai. Chúng tôi thử nghiệm hai phương pháp để giải quyết vấn đề này và chỉ ra rằng một phương pháp đơn giản hoạt động tốt (§2). Tiếp theo, chúng tôi nghiên cứu các cơ chế khác nhau để kiểm soát lượng tính toán trong mạng decoder, hoặc cho toàn bộ chuỗi hoặc theo từng token. Điều này bao gồm các bộ phân loại đa thức và nhị thức được giám sát bởi likelihood của mô hình hoặc liệu argmax đã đúng chưa cũng như đơn giản là ngưỡng hóa điểm số mô hình (§3). Các thí nghiệm trên IWSLT14 German-English

Công việc được thực hiện trong thời gian thực tập tại Facebook AI Research.

dịch máy (Cettolo et al., 2014) cũng như dịch WMT'14 English-French cho thấy chúng tôi có thể đạt hiệu suất của các mô hình baseline được điều chỉnh tốt với ít hơn 76% tính toán (§4).

2 DỰ ĐOÁN CÓ CẤU TRÚC BẤT KỲ LÚC NÀO

Trước tiên chúng tôi trình bày một mô hình có thể đưa ra dự đoán ở các lớp khác nhau. Điều này được biết đến như dự đoán bất kỳ lúc nào cho các mô hình thị giác máy tính (Huang et al., 2017) và chúng tôi mở rộng nó cho dự đoán có cấu trúc.

2.1 TRANSFORMER VỚI NHIỀU BỘ PHÂN LOẠI ĐẦU RA

Chúng tôi dựa phương pháp của mình trên mô hình Transformer sequence-to-sequence (Vaswani et al., 2017). Cả mạng encoder và decoder đều chứa N khối xếp chồng trong đó mỗi khối có nhiều khối con được bao quanh bởi các kết nối skip tàn dư. Khối con đầu tiên là multi-head dot-product self-attention và khối thứ hai là mạng feed-forward fully connected theo vị trí. Đối với decoder, có một khối con bổ sung sau self-attention để thêm ngữ cảnh nguồn thông qua một multi-head attention khác.

Cho một cặp chuỗi nguồn-đích (x;y), x được xử lý với encoder để đưa ra các biểu diễn s = (s₁;...;s|x|). Tiếp theo, decoder tạo ra y từng bước một. Đối với mỗi token mới yₜ được nhập vào decoder tại thời điểm t, N khối decoder xử lý nó để tạo ra các trạng thái ẩn (hₜⁿ)₁≤n≤N:

h₀ₜ = embed(yₜ); hₜⁿ = block n(hₜⁿ⁻¹, s); (1)

trong đó block n là ánh xạ liên kết với khối thứ n và embed là bảng tra cứu.

Phân phối đầu ra để dự đoán token tiếp theo được tính bằng cách đưa các kích hoạt của lớp decoder cuối cùng hₜᴺ vào một bộ phân loại đầu ra được chuẩn hóa softmax W:

p(yₜ₊₁|hₜᴺ) = softmax(Whₜᴺ) (2)

Các Transformer tiêu chuẩn có một bộ phân loại đầu ra duy nhất được gắn vào đỉnh của mạng decoder. Tuy nhiên, để tính toán động chúng ta cần có khả năng đưa ra dự đoán ở các giai đoạn khác nhau của mạng. Để đạt được điều này, chúng tôi gắn các bộ phân loại đầu ra Cₙ được tham số hóa bởi Wₙ vào đầu ra hₜⁿ của mỗi khối decoder trong số N khối:

∀n, p(yₜ₊₁|hₜⁿ) = softmax(Wₙhₜⁿ) (3)

Các bộ phân loại có thể được tham số hóa độc lập hoặc chúng ta có thể chia sẻ trọng số qua N khối.

2.2 HUẤN LUYỆN NHIỀU BỘ PHÂN LOẠI ĐẦU RA

Tính toán động cho phép mô hình sử dụng bất kỳ bộ phân loại thoát nào trong số N thay vì chỉ bộ cuối cùng. Một số mô hình của chúng tôi có thể chọn một bộ phân loại đầu ra khác nhau ở mỗi bước thời gian dẫn đến số lượng tổ hợp bộ phân loại đầu ra có thể có theo cấp số nhân trong độ dài chuỗi.

Chúng tôi xem xét hai cách có thể để huấn luyện mạng decoder (Hình 1). Huấn luyện căn chỉnh tối ưu hóa tất cả các bộ phân loại đồng thời và giả định tất cả các trạng thái ẩn trước đó được yêu cầu bởi self-attention đều có sẵn. Tuy nhiên, tại thời gian kiểm tra điều này thường không xảy ra khi chúng ta chọn một lối thoát khác nhau cho mỗi token dẫn đến các trạng thái không căn chỉnh. Thay vào đó, huấn luyện hỗn hợp lấy mẫu nhiều chuỗi lối thoát cho một câu cho trước và expose mô hình với các trạng thái ẩn từ các lớp khác nhau.

Nói chung, đối với một chuỗi đầu ra y cho trước, chúng ta có một chuỗi các lối thoát được chọn (n₁,...,n|y|) và chúng ta ký hiệu khối mà chúng ta thoát tại thời điểm t là nₜ.

2.2.1 HUẤN LUYỆN CĂN CHỈNH

Huấn luyện căn chỉnh giả định tất cả các trạng thái ẩn h₁ⁿ¹,...,hₜⁿ¹ đều có sẵn để tính self-attention và nó tối ưu hóa N hạng mục loss, một cho mỗi lối thoát (Hình 1a):

LLₜⁿ = -log p(yₜ|hₜ₋₁ⁿ); LLⁿ = Σₜ₌₁|y| LLₜⁿ; Ldec(x,y) = 1/Σₙωₙ Σₙ₌₁ᴺ ωₙLLⁿ: (4)

Loss tổng hợp Ldec(x,y) là trung bình có trọng số của N hạng mục w.r.t. đến (ω₁,...ωₙ). Chúng tôi thấy rằng trọng số đồng đều đạt BLEU tốt hơn so với các sơ đồ cân bằng khác (xem Phụ lục A).

Tại thời gian suy luận, không phải tất cả các bước thời gian sẽ có trạng thái ẩn cho lớp hiện tại vì mô hình đã thoát sớm. Trong trường hợp này, chúng tôi đơn giản sao chép trạng thái được tính cuối cùng lên tất cả các lớp trên, tương tự như huấn luyện hỗn hợp (§2.2.2). Tuy nhiên, chúng tôi áp dụng các phép chiếu key và value cụ thể theo lớp cho trạng thái được sao chép.

2.2.2 HUẤN LUYỆN HỖN HỢP

Huấn luyện căn chỉnh giả định rằng tất cả các trạng thái ẩn của các bước thời gian trước đó đều có sẵn nhưng giả định này không thực tế vì một lối thoát sớm có thể đã được chọn trước đó. Điều này tạo ra sự không khớp giữa huấn luyện và kiểm tra. Huấn luyện hỗn hợp giảm sự không khớp bằng cách huấn luyện mô hình sử dụng các trạng thái ẩn từ các khối khác nhau của các bước thời gian trước đó cho self-attention. Chúng tôi lấy mẫu M chuỗi thoát khác nhau (n₁⁽ᵐ⁾,...n|y|⁽ᵐ⁾)₁≤m≤M và đánh giá loss sau:

LL(n₁,...,n|y|) = Σₜ₌₁|y| -log p(yₜ|h_{t-1}^{nₜ}); Ldec(x,y) = 1/M Σₘ₌₁ᴹ LL(n₁⁽ᵐ⁾,...,n|y|⁽ᵐ⁾): (5)

Khi nₜ < N, chúng tôi sao chép trạng thái ẩn được đánh giá cuối cùng hₜⁿ lên các lớp tiếp theo để self-attention của các bước thời gian tương lai có thể hoạt động như bình thường (xem Hình 1b).

3 ƯỚC TÍNH ĐỘ SÂU THÍCH ỨNG

Chúng tôi trình bày nhiều cơ chế khác nhau để dự đoán khối decoder mà mô hình sẽ dừng và xuất ra token tiếp theo, hoặc khi nào nó nên thoát để đạt được sự cân bằng tốt giữa tốc độ và độ chính xác. Chúng tôi xem xét hai phương pháp: độ sâu cụ thể theo chuỗi giải mã tất cả các token đầu ra sử dụng cùng một khối (§3.1) trong khi độ sâu cụ thể theo token xác định một lối thoát riêng biệt cho từng token riêng lẻ (§3.2).

Chúng tôi mô hình hóa phân phối thoát tại bước thời gian t với phân phối tham số qₜ trong đó qₜ(n) là xác suất tính toán khối 1;...;khối n và sau đó phát ra dự đoán với Cₙ. Các tham số của qₜ được tối ưu hóa để khớp với phân phối oracle q*ₜ với cross-entropy:

Lexit(x,y) = Σₜ H(q*ₜ(x,y), qₜ(x)) (6)

Loss thoát (Lexit) được lan truyền ngược đến các tham số encoder-decoder. Chúng tôi đồng thời tối ưu hóa loss giải mã (Eq. (4)) và loss thoát (Eq. (6)) được cân bằng bởi một siêu tham số α để đảm bảo rằng mô hình duy trì độ chính xác tạo tốt. Loss cuối cùng có dạng:

L(x,y) = Ldec(x,y) + αLexit(x,y); (7)

Trong phần sau, chúng tôi mô tả cho mỗi phương pháp cách phân phối thoát qₜ được mô hình hóa (được minh họa trong Hình 2) và cách phân phối oracle q*ₜ được suy ra.

3.1 ĐỘ SÂU CỤ THỂ THEO CHUỖI:

Đối với độ sâu cụ thể theo chuỗi, phân phối thoát q và phân phối oracle q* độc lập với bước thời gian nên chúng tôi bỏ chỉ số dưới t. Chúng tôi điều kiện lối thoát trên chuỗi nguồn bằng cách đưa trung bình s̄ của các đầu ra encoder vào một bộ phân loại đa thức:

s̄ = 1/|x| Σₜ sₜ; q(n|x) = softmax(Wₕs̄ + bₕ) ∈ ℝᴺ; (8)

trong đó Wₕ và bₕ là trọng số và bias của cơ chế dừng. Chúng tôi xem xét hai oracle để xác định khối nào trong số N khối nên được chọn. Oracle đầu tiên dựa trên likelihood chuỗi và oracle thứ hai nhìn vào tổng hợp của các token được dự đoán đúng tại mỗi khối.

Dựa trên likelihood: Oracle này dựa trên likelihood của toàn bộ chuỗi sau mỗi khối và chúng tôi tối ưu hóa nó với Dirac delta tập trung xung quanh lối thoát có likelihood chuỗi cao nhất.

q*(x,y) = δ(arg max_n LLⁿ):

Chúng tôi thêm một hạng mục chính quy hóa để khuyến khích các lối thoát thấp hơn đạt likelihood tốt:

q*(x,y) = δ(arg max_n LLⁿ - αn): (9)

Dựa trên tính đúng đắn: Likelihood bỏ qua liệu mô hình đã gán điểm số cao nhất cho dự đoán đúng chưa. Thay vào đó, oracle này chọn khối thấp nhất gán điểm số lớn nhất cho dự đoán đúng. Đối với mỗi khối, chúng tôi đếm số lượng token được dự đoán đúng trong chuỗi và chọn khối có số token đúng nhiều nhất. Một hạng mục chính quy hóa kiểm soát sự cân bằng giữa tốc độ và độ chính xác.

Cⁿ = #{t|yₜ = arg max_y p(y|h_{t-1}^n)}; q*(x,y) = δ(arg max_n Cⁿ - αn): (10)

Các oracle dựa trên các metric kiểm tra như BLEU là khả thi nhưng tốn kém để tính toán vì chúng ta sẽ cần giải mã mỗi câu huấn luyện N lần. Chúng tôi để lại điều này cho công việc tương lai.

3.2 ĐỘ SÂU CỤ THỂ THEO TOKEN:

Phương pháp cụ thể theo token có thể chọn một lối thoát khác nhau ở mỗi bước thời gian. Chúng tôi xem xét hai tùy chọn cho phân phối thoát qₜ tại bước thời gian t: một đa thức với bộ phân loại được điều kiện trên trạng thái ẩn decoder đầu tiên h¹ₜ và một geometric-like trong đó xác suất thoát σⁿₜ được ước tính sau mỗi khối dựa trên các kích hoạt của khối hiện tại hⁿₜ.

Đa thức qₜ:
qₜ(n|x,y<t) = softmax(Wₕh¹ₜ + bₕ); (11)

Lối thoát có xác suất cao nhất arg max qₜ(n|x,y<t) được chọn tại suy luận.

Geometric-like qₜ:
∀n ∈ [1::N-1]; σⁿₜ = sigmoid(w^T_h h^n_t + bₕ); (12)

qₜ(n|x,y<t) = {
  σⁿₜ ∏_{n'<n} (1-σⁿ'ₜ),  nếu n < N
  ∏_{n'<N} (1-σⁿ'ₜ),     nếu không
} (13)

trong đó, d là chiều của các trạng thái decoder, Wₕ ∈ ℝ^{N×d} và wₕ ∈ ℝ^d là trọng số của các cơ chế dừng, và bₕ là bias của chúng. Trong quá trình suy luận decoder thoát khi tín hiệu dừng σⁿₜ vượt quá ngưỡng τₙ mà chúng tôi điều chỉnh trên tập valid để đạt được sự cân bằng tốt hơn giữa độ chính xác và tốc độ. Nếu các ngưỡng (τₙ)₁≤n<N chưa được vượt quá, thì chúng tôi mặc định thoát tại khối N.

Hai bộ phân loại được huấn luyện để tối thiểu hóa cross-entropy đối với một trong các phân phối oracle sau:

Dựa trên likelihood: Tại mỗi bước thời gian t, chúng tôi chọn khối có bộ phân loại thoát có likelihood cao nhất cộng với một hạng mục chính quy hóa được cân bằng bởi α để khuyến khích các lối thoát thấp hơn.

q*ₜ(x,y) = δ(arg max_n LLⁿₜ - αn) (14)

Oracle này bỏ qua tác động của quyết định hiện tại đến các bước thời gian tương lai và do đó chúng tôi xem xét làm mượt các likelihood với một kernel RBF.

κ(t,t') = e^{-|t-t'|²/σ²}; g̃LLⁿₜ = ∑_{t'=1}^{|y|} κ(t,t') LLⁿₜ'; q*ₜ(x,y) = δ(arg max_n g̃LLⁿₜ - αn); (15)

trong đó chúng ta kiểm soát kích thước của ngữ cảnh xung quanh với độ rộng kernel σ. Chúng tôi gọi oracle này là LL(α,σ) bao gồm trường hợp chúng ta chỉ nhìn vào likelihood của token hiện tại với σ → 0.

Dựa trên tính đúng đắn: Tương tự như oracle dựa trên likelihood, chúng ta có thể nhìn vào tính đúng đắn của dự đoán tại bước thời gian t cũng như các vị trí xung quanh. Chúng tôi định nghĩa mục tiêu q*ₜ như sau:

Cⁿₜ = 1[yₜ = arg max_y p(y|h^n_{t-1})]; C̃ⁿₜ = ∑_{t'=1}^{|y|} κ(t,t')Cⁿₜ; (16)

q*ₜ(x,y) = δ(arg max_n C̃ⁿₜ - αn): (17)

Ngưỡng hóa tin cậy Cuối cùng, chúng tôi xem xét ngưỡng hóa các dự đoán mô hình (§2), tức là thoát khi điểm số tối đa của bộ phân loại đầu ra hiện tại p(yₜ₊₁|hⁿₜ) vượt quá ngưỡng siêu tham số τₙ. Điều này không yêu cầu huấn luyện và các ngưỡng τ = (τ₁,...,τₙ₋₁) được điều chỉnh đơn giản trên tập valid để tối đa hóa BLEU. Cụ thể, trong 10k vòng lặp, chúng tôi lấy mẫu một chuỗi ngưỡng τ ~ U(0,1)^{N-1}, giải mã tập valid với các ngưỡng được lấy mẫu và sau đó đánh giá điểm BLEU và chi phí tính toán đạt được với lựa chọn τ này. Sau 10k đánh giá, chúng tôi chọn các ngưỡng hoạt động tốt nhất, tức là với BLEU cao nhất trong mỗi phân đoạn chi phí.

4 THÍ NGHIỆM

4.1 THIẾT LẬP THÍ NGHIỆM

Chúng tôi đánh giá trên nhiều benchmark và đo BLEU được token hóa (Papineni et al., 2002):

IWSLT'14 German to English (De-En). Chúng tôi sử dụng thiết lập của Edunov et al. (2018) và huấn luyện trên 160K cặp câu. Chúng tôi sử dụng N = 6 khối, một mạng feed-forward (ffn) có chiều trung gian 1024, 4 head, dropout 0.3, chiều embedding denc = 512 cho encoder và ddec = 256 cho decoder. Embeddings không được tie với 6 bộ phân loại đầu ra khác nhau. Chúng tôi đánh giá với một checkpoint duy nhất và beam có độ rộng 5.

WMT'14 English to French (En-Fr). Chúng tôi cũng thử nghiệm trên tác vụ WMT'14 English-French lớn hơn nhiều bao gồm 35.5m cặp câu huấn luyện. Chúng tôi phát triển trên 26k cặp held out và kiểm tra trên newstest14. Từ vựng bao gồm 44k loại BPE chung (Sennrich et al., 2016). Chúng tôi sử dụng kiến trúc Transformer big và tie các embedding của encoder, decoder và các bộ phân loại đầu ra ((Wn)₁≤n≤6; §2.1). Chúng tôi tính trung bình mười checkpoint cuối cùng và sử dụng beam có độ rộng 4.

Các mô hình được triển khai trong fairseq (Ott et al., 2019) và được huấn luyện với Adam (Kingma & Ba, 2015). Chúng tôi huấn luyện trong 50k cập nhật trên 128 GPU với kích thước batch 460k token cho WMT'14 En-Fr và trên 2 GPU với 8k token mỗi batch cho IWSLT'14 De-En. Để ổn định hóa huấn luyện, chúng tôi tái chuẩn hóa các gradient nếu norm vượt quá gclip = 3.

Đối với các mô hình có lối thoát thích ứng, đầu tiên chúng tôi huấn luyện không có dự đoán thoát (α = 0 trong Eq. (7)) sử dụng chế độ căn chỉnh (xem §2.2.1) trong 50k cập nhật và sau đó tiếp tục huấn luyện với α ≠ 0 cho đến khi hội tụ. Các bộ phân loại dự đoán thoát được tham số hóa bởi một lớp tuyến tính duy nhất (Eq. (8)) với cùng chiều đầu vào như chiều embedding, ví dụ 1024 cho Transformer big; chiều đầu ra là N cho bộ phân loại đa thức hoặc một cho geometric-like. Chúng tôi thoát khi σt,n > 0.5 cho các bộ phân loại geometric-like.

4.2 HUẤN LUYỆN NHIỀU BỘ PHÂN LOẠI ĐẦU RA

Trước tiên chúng tôi so sánh hai chế độ huấn luyện cho mô hình của chúng tôi (§2.2). Huấn luyện căn chỉnh thực hiện self-attention trên các trạng thái căn chỉnh (§2.2.1) và huấn luyện hỗn hợp expose self-attention với các trạng thái ẩn từ các khối khác nhau (§2.2.2).

Chúng tôi so sánh hai chế độ huấn luyện khi chọn hoặc một lối thoát được lấy mẫu đồng đều hoặc một lối thoát cố định n = 1,...,6 tại thời gian suy luận cho mỗi bước thời gian. Thí nghiệm lối thoát được lấy mẫu kiểm tra độ mạnh mẽ đối với các trạng thái ẩn hỗn hợp và thiết lập lối thoát cố định mô phỏng một cài đặt lý tưởng nơi tất cả các trạng thái trước đó đều có sẵn. Làm baseline chúng tôi cho thấy sáu Transformer tiêu chuẩn riêng biệt với N ∈ [1::6] khối decoder. Tất cả các mô hình được huấn luyện với số lượng cập nhật bằng nhau và huấn luyện hỗn hợp với M=6 đường dẫn có thể so sánh nhất với huấn luyện căn chỉnh vì số lượng loss trên mỗi mẫu là giống hệt nhau.

Bảng 1 cho thấy huấn luyện căn chỉnh vượt trội hơn huấn luyện hỗn hợp cả cho lối thoát cố định cũng như cho lối thoát được lấy mẫu ngẫu nhiên. Điều sau này là đáng ngạc nhiên vì huấn luyện căn chỉnh không bao giờ expose cơ chế self-attention với các trạng thái ẩn từ các khối khác. Chúng tôi nghi ngờ rằng điều này là do các kết nối tàn dư sao chép các đặc trưng từ các khối thấp hơn lên các lớp tiếp theo và những kết nối này phổ biến trong các mô hình Transformer (§2). Huấn luyện căn chỉnh cũng hoạt động rất cạnh tranh với các mô hình baseline riêng lẻ.

Huấn luyện căn chỉnh về mặt khái niệm đơn giản và nhanh. Chúng ta có thể xử lý một ví dụ huấn luyện với N lối thoát trong một lần forward/backward pass duy nhất trong khi cần M pass cho huấn luyện hỗn hợp. Trong phần còn lại của bài báo, chúng tôi sử dụng chế độ căn chỉnh để huấn luyện các mô hình của chúng tôi. Phụ lục A báo cáo các thí nghiệm với cân bằng các bộ phân loại đầu ra khác nhau một cách khác nhau nhưng chúng tôi thấy rằng sơ đồ cân bằng đồng đều hoạt động tốt. Trên thiết lập lớn nhất của chúng tôi, WMT'14 English-French, thời gian huấn luyện của một mô hình căn chỉnh với sáu bộ phân loại đầu ra chỉ tăng một cách biên tế khoảng 1% so với baseline với một bộ phân loại đầu ra duy nhất giữ mọi thứ khác bằng nhau.

4.3 ƯỚC TÍNH ĐỘ SÂU THÍCH ỨNG

Tiếp theo, chúng tôi huấn luyện các mô hình với các trạng thái căn chỉnh và so sánh các bộ phân loại độ sâu thích ứng về BLEU cũng như nỗ lực tính toán. Chúng tôi đo cái sau như lối thoát trung bình mỗi token đầu ra (AE). Làm baseline chúng tôi sử dụng lại sáu Transformer tiêu chuẩn riêng biệt với N ∈ [1::6] với một bộ phân loại đầu ra duy nhất. Chúng tôi cũng đo hiệu suất của mô hình chế độ căn chỉnh được huấn luyện cho các lối thoát cố định n ∈ [1::6]. Đối với các mô hình độ sâu thích ứng cụ thể theo token (Tok), chúng tôi huấn luyện bốn tổ hợp: oracle dựa trên likelihood (LL) + geometric-like, oracle dựa trên likelihood (LL) + đa thức, oracle dựa trên tính đúng đắn (C) + geometric-like và oracle dựa trên tính đúng đắn (C) + đa thức. Các mô hình cụ thể theo chuỗi (Seq) được huấn luyện với oracle tính đúng đắn (C) và oracle likelihood (LL) với các giá trị khác nhau cho trọng số chính quy hóa α. Tất cả các tham số được điều chỉnh trên tập valid và chúng tôi báo cáo kết quả trên tập test cho một loạt các lối thoát trung bình.

Hình 3 cho thấy mô hình căn chỉnh (đường màu xanh) có thể khớp với độ chính xác của Transformer 6-khối tiêu chuẩn (đường màu đen) ở một nửa số lớp (n = 3) bằng cách luôn thoát tại khối thứ ba. Mô hình căn chỉnh vượt trội hơn baseline cho n = 2,...,6.

Đối với các cơ chế dừng cụ thể theo token (Hình 3a), các bộ phân loại geometric-like đạt được sự cân bằng tốt hơn giữa tốc độ và độ chính xác so với các bộ phân loại đa thức (tam giác tô màu vs. tam giác rỗng). Đối với các bộ phân loại geometric-like, oracle tính đúng đắn vượt trội hơn oracle likelihood (Tok-C geometric-like vs. Tok-LL geometric-like) nhưng xu hướng ít rõ ràng hơn đối với các bộ phân loại đa thức. Ở cấp độ chuỗi, likelihood là oracle tốt hơn (Hình 3b).

Điểm Tok-C geometric-like ngoài cùng bên phải (α = 0, σ = 0.1) đạt 34.73 BLEU tại AE = 1.42 tương ứng với độ chính xác tương tự như baseline N = 6 với ít hơn 76% số khối giải mã.

Độ chính xác tốt nhất của mô hình căn chỉnh là 34.95 BLEU tại lối thoát 5 và cấu hình Tok-C geometric-like có thể so sánh tốt nhất đạt 34.99 BLEU tại AE = 1.97, hoặc ít hơn 61% số khối giải mã.

Khi cố định ngân sách thành hai khối decoder, Tok-C geometric-like với AE = 1.97 đạt BLEU 35, cải thiện 0.64 BLEU so với baseline (N = 2) và căn chỉnh cả hai đều đạt BLEU 34.35.

Ngưỡng hóa tin cậy (Hình 3c) hoạt động rất tốt nhưng không thể vượt trội hơn Tok-C geometric-like.

Ablation của các siêu tham số Trong phần này, chúng tôi nhìn vào hiệu ứng của hai siêu tham số chính trên IWSLT'14 De-En: quy mô chính quy hóa α (xem Eq. (9)), và độ rộng kernel RBF σ được sử dụng để làm mượt các điểm số (xem Eq. (15)). Chúng tôi huấn luyện các mô hình Tok-LL Geometric-like và đánh giá chúng với các ngưỡng mặc định của chúng (thoát nếu σⁿₜ > 0.5). Hình 4a cho thấy các giá trị cao hơn của α dẫn đến các lối thoát thấp hơn. Hình 4b cho thấy hiệu ứng của σ cho hai giá trị của α. Trong cả hai đường cong, chúng ta thấy rằng các kernel rộng hơn ưu tiên các lối thoát cao hơn.

4.4 MỞ RỘNG QUY MÔ CÁC MÔ HÌNH ĐỘ SÂU THÍCH ỨNG

Cuối cùng, chúng tôi lấy các mô hình hoạt động tốt nhất từ benchmark IWSLT và kiểm tra chúng trên benchmark WMT'14 English-French quy mô lớn. Kết quả trên tập test (Hình 5a) cho thấy độ sâu thích ứng vẫn cho thấy những cải thiện nhưng chúng bị giảm trong thiết lập quy mô rất lớn này. Ngưỡng hóa tin cậy hoạt động rất tốt và các phương pháp độ sâu cụ thể theo chuỗi chỉ cải thiện một cách biên tế so với baseline. Tok-LL geometric-like có thể khớp với kết quả baseline tốt nhất BLEU 43.4 (N = 6) bằng cách chỉ sử dụng AE = 2.40 tương ứng với 40% số khối decoder; kết quả căn chỉnh tốt nhất BLEU 43.6 có thể được khớp với AE = 3.25. Trong thiết lập này, Tok-LL geometric-like hơi vượt trội hơn đối tác Tok-C.

Ngưỡng hóa tin cậy khớp với độ chính xác của baseline N=6 với AE≈2.5 hoặc ít hơn 59% số khối giải mã. Tuy nhiên, ngưỡng hóa tin cậy yêu cầu tính toán bộ phân loại đầu ra tại mỗi khối để xác định có dừng hay tiếp tục. Đây là một overhead lớn vì các bộ phân loại đầu ra dự đoán 44k loại cho benchmark này (§4.1). Để tính toán tốt hơn cho điều này, chúng tôi đo số FLOP trung bình mỗi token đầu ra (chi tiết trong Phụ lục B). Hình 5b cho thấy phương pháp Tok-LL geometric-like cung cấp sự cân bằng tốt hơn khi overhead của các bộ phân loại đầu ra được xem xét.

4.5 KẾT QUẢ ĐỊNH TÍNH

Phân phối thoát cho một mẫu cho trước có thể cung cấp cái nhìn sâu sắc về những gì mà một decoder Depth-Adaptive Transformer coi là một nhiệm vụ khó khăn. Trong phần này, đối với mỗi giả thuyết ŷ, chúng tôi sẽ nhìn vào chuỗi các lối thoát được chọn (n₁,...,n|ŷ|) và các điểm số xác suất (p₁,...p|ŷ|) với pₜ = p(ŷₜ|h^{nₜ}_{t-1}) tức là độ tin cậy của mô hình trong token được lấy mẫu tại lối thoát được chọn.

Hình 6 và 7 cho thấy các giả thuyết từ các tập test WMT'14 En-Fr và IWSLT'14 De-En tương ứng. Đối với mỗi giả thuyết chúng tôi nêu các lối thoát và các điểm số xác suất. Trong Hình 6a, việc dự đoán 'présent' (có nghĩa là 'present') là khó. Một bản dịch đơn giản là 'était là' nhưng mô hình chọn 'present' cũng phù hợp. Trong Hình 6b, mô hình sử dụng nhiều tính toán hơn để dự đoán mạo từ xác định 'les' vì nguồn đã bỏ qua mạo từ cho 'passengers'.

Một xu hướng rõ ràng trong cả hai benchmark là mô hình yêu cầu ít tính toán hơn gần cuối giải mã để tạo ra marker kết thúc chuỗi <=s> và dấu chấm phía trước khi có liên quan. Trong Hình 8, chúng tôi cho thấy phân phối của các lối thoát ở đầu và gần cuối các giả thuyết tập test. Chúng tôi coi đầu của một chuỗi là 10% token đầu tiên và cuối là 10% token cuối cùng. Các phân phối thoát được hiển thị cho ba mô hình trên WMT'14 En-Fr: Mô hình 1 có lối thoát trung bình AE = 2.53, Mô hình 2 thoát tại AE = 3.79 trung bình và Mô hình 3 với AE = 4.68.

Trong cùng các mô hình, các lối thoát sâu muộn được sử dụng ở đầu chuỗi và các lối thoát sớm được chọn gần cuối. Đối với các mô hình được chính quy hóa mạnh như Mô hình 1 với AE = 2.53, sự chênh lệch giữa đầu và cuối ít nghiêm trọng hơn vì mô hình thoát sớm hầu hết thời gian. Mô hình 2 và Mô hình 3 ít được chính quy hóa hơn (AE cao hơn) và có xu hướng sử dụng các lối thoát muộn ở đầu chuỗi và các lối thoát sớm gần cuối. Mặt khác, Mô hình 1 được chính quy hóa nhiều hơn với AE = 2.53 thoát sớm hầu hết thời gian. Cũng có một mối tương quan giữa xác suất mô hình và lượng tính toán, đặc biệt trong các mô hình có AE thấp. Hình 9 cho thấy biểu đồ chung của các điểm số và lối thoát được chọn. Đối với cả Mô hình 1 và Mô hình 2, các lối thoát thấp (n≤2) được sử dụng trong dải tin cậy cao [0.8,1] và các lối thoát cao (n≥4) được sử dụng trong dải tin cậy thấp [0,0.5].

Mô hình 3 có lối thoát trung bình cao (AE = 4.68) nên hầu hết các token thoát muộn, tuy nhiên, trong các dải tin cậy thấp mô hình không thoát sớm hơn n = 5.

5 KẾT LUẬN

Chúng tôi đã mở rộng dự đoán bất kỳ lúc nào cho cài đặt dự đoán có cấu trúc và giới thiệu các phương pháp đơn giản nhưng hiệu quả để trang bị cho các mô hình chuỗi khả năng đưa ra dự đoán tại các điểm khác nhau trong mạng. Chúng tôi đã so sánh một số cơ chế khác nhau để dự đoán độ sâu mạng cần thiết và thấy rằng một bộ phân loại geometric-like đơn giản dựa trên tính đúng đắn đạt được sự cân bằng tốt nhất giữa tốc độ và độ chính xác. Kết quả cho thấy số lượng lớp decoder có thể được giảm hơn ba phần tư mà không mất độ chính xác so với baseline Transformer được điều chỉnh tốt.

LỜI CẢM ƠN
Chúng tôi cảm ơn Laurens van der Maaten vì những nhận xét và đề xuất hữu ích.

TÀI LIỆU THAM KHẢO
[Danh sách tài liệu tham khảo giữ nguyên như bản gốc]

PHỤ LỤC A MỞ RỘNG LOSS

Trong phần này chúng tôi thử nghiệm với các trọng số khác nhau để mở rộng các loss bộ phân loại đầu ra. Thay vì cân bằng đồng đều, chúng tôi thiên về các bộ phân loại đầu ra cụ thể bằng cách gán trọng số cao hơn cho các loss của chúng. Bảng 2 cho thấy cân bằng các bộ phân loại bằng nhau cung cấp kết quả tốt.

Mở rộng gradient Việc thêm giám sát trung gian ở các cấp độ khác nhau của decoder dẫn đến các gradient phong phú hơn cho các khối thấp hơn so với các khối trên. Điều này là do các lớp trước ảnh hưởng đến nhiều hạng mục loss hơn trong loss tổng hợp của Eq. (4). Để cân bằng các gradient của mỗi khối trong decoder, chúng tôi mở rộng các gradient của mỗi hạng mục loss (LLⁿ) khi nó đang cập nhật các tham số của khối liên kết của nó (khối n với tham số θₙ) và hoàn nguyên nó về quy mô bình thường trước khi lan truyền ngược lên các khối trước đó. Hình 10 và Thuật toán 1 minh họa quy trình mở rộng gradient này. Các θₙ được cập nhật với gradient được khuếch đại γₙ từ giám sát của khối và (N-n) gradient từ các khối tiếp theo. Chúng tôi chọn γₙ = γ/(N-n) để kiểm soát tỷ lệ γ:1 như tỷ lệ của giám sát khối với giám sát các khối tiếp theo.

Bảng 3 cho thấy mở rộng gradient có thể có lợi cho lớp thấp nhất với chi phí của các lớp cao hơn. Tuy nhiên, không mở rộng nhìn chung hoạt động rất tốt.

PHỤ LỤC B ƯỚC TÍNH FLOP

Phần này chi tiết việc tính toán FLOP chúng tôi báo cáo. FLOP mỗi token chỉ dành cho mạng decoder vì chúng tôi sử dụng encoder có cùng kích thước cho tất cả các mô hình. Chúng tôi phân tích FLOP của mọi hoạt động trong Thuật toán 2 (mặt trước màu xanh của câu lệnh thuật toán). Chúng tôi bỏ qua các phi tuyến tính, chuẩn hóa và kết nối tàn dư. Các hoạt động chính chúng tôi tính đến là tích vô hướng và theo đó là tích ma trận-vector vì chúng đại diện cho phần lớn FLOP (chúng tôi giả định kích thước batch một để đơn giản hóa tính toán).

Với sự phân tích này, tổng chi phí tính toán tại bước thời gian t của một khối decoder mà chúng ta thực sự đi qua, được ký hiệu với FC, là:

FC(x,t) = 12d²d + 4dfddd + 4tdd + 4|x|dd + 4[[FirstCall]]|x|ddde;

trong đó chi phí ánh xạ key và value của nguồn được tính vào lần đầu tiên khối được gọi (được đánh dấu với FirstCall). Điều này xảy ra tại t = 1 cho mô hình baseline nhưng nó phụ thuộc vào đầu vào với ước tính độ sâu thích ứng và có thể không bao giờ xảy ra nếu tất cả các token thoát sớm.

Nếu bị bỏ qua, một khối vẫn phải tính toán key và value của khối self-attention để self-attention của các bước thời gian tương lai có thể hoạt động. Chúng tôi sẽ ký hiệu chi phí này với FS và chúng ta có FS = 4d²d.

Tùy thuộc vào cơ chế dừng, một chi phí dự đoán thoát, được ký hiệu với FP, được thêm vào:

Độ sâu cụ thể theo chuỗi: FP(t,q(t)) = 2[[t = 1]]Ndd
Đa thức cụ thể theo token: FP(t,q(t)) = 2Ndd  
Geometric-like cụ thể theo token: FP(t,q(t)) = 2ddq(t)
Ngưỡng hóa tin cậy: FP(t,q(t)) = 2q(t)Vdd

Đối với một tập hợp các chuỗi nguồn {x⁽ⁱ⁾}ᵢ∈I và các giả thuyết được tạo ra {y⁽ⁱ⁾}ᵢ∈I, FLOP trung bình mỗi token là:

Baseline (N khối): [1/Σᵢ|y⁽ⁱ⁾|] Σᵢ Σₜ₌₁|y⁽ⁱ⁾| [NFC(x⁽ⁱ⁾,t) + 2Vdd]

Độ sâu thích ứng: [1/Σᵢ|y⁽ⁱ⁾|] Σᵢ Σₜ₌₁|y⁽ⁱ⁾| [q(t)FC(x⁽ⁱ⁾,t) + (N-q(t))FS + FP(t,q(t)) + 2Vdd]

Trong trường hợp ngưỡng hóa tin cậy, chi phí dự đoán đầu ra cuối cùng (2Vdd) đã được tính trong chi phí dự đoán thoát FP.
