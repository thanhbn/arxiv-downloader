# 2404.08698.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/speculative/2404.08698.pdf
# File size: 4139791 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Lossless Acceleration of Large Language Model via Adaptive N-gram
Parallel Decoding
Jie Ou, Yueming Chen, Wenhong Tian∗
University of Electronic Science and Technology of China, Chengdu, China
oujieww6@gmail.com, yuemingchen121@gmail.com
tian_wenhong@uestc.edu.cn
Abstract
While Large Language Models (LLMs) have
shown remarkable abilities, they are hindered
by significant resource consumption and con-
siderable latency due to autoregressive process-
ing. In this study, we introduce Adaptive N-
gram Parallel Decoding (ANPD), an innova-
tive and lossless approach that accelerates infer-
ence by allowing the simultaneous generation
of multiple tokens. ANPD incorporates a two-
stage approach: it begins with a rapid drafting
phase that employs an N-gram module, which
adapts based on the current interactive context,
followed by a verification phase, during which
the original LLM assesses and confirms the
proposed tokens. Consequently, ANPD pre-
serves the integrity of the LLM’s original out-
put while enhancing processing speed. We fur-
ther leverage a multi-level architecture for the
N-gram module to enhance the precision of the
initial draft, consequently reducing inference
latency. ANPD eliminates the need for retrain-
ing or extra GPU memory, making it an effi-
cient and plug-and-play enhancement. In our
experiments, models such as LLaMA and its
fine-tuned variants have shown speed improve-
ments up to 3.67×, validating the effectiveness
of our proposed ANPD.
1 Introduction
The advent of Large Language Models (LLMs)
such as GPT-4 (OpenAI, 2023), ChatGPT (Brown
et al., 2020), LLaMA (Touvron et al., 2023a), and
PaLM (Chowdhery et al., 2023), has revolution-
ized the landscape of natural language processing.
However, the majority of LLMs (Touvron et al.,
2023a; Anil et al., 2023; Bai et al., 2023) rely on the
decoder-only Transformers architecture (Alec et al.,
2018), which is intrinsically autoregressive and
consequently leads to increased generation time
during inference. This characteristic has made the
improvement of LLM inference efficiency a sig-
∗Corresponding authornificant research area within the natural language
processing community.
Model compression techniques such as quantiza-
tion (Han et al., 2015), pruning (Molchanov et al.,
2016), and distillation (Hinton et al., 2015) have
been employed to alleviate the computational costs
associated with LLMs. Recently, innovative meth-
ods such as early exit strategies (Yang et al., 2023b;
Bae et al., 2023; Kong et al., 2022; Schuster et al.,
2022; Varshney et al., 2023) and speculative decod-
ing (Kim et al., 2023; Xia et al., 2022; Leviathan
et al., 2023; Spector and Re, 2023; Zhang et al.,
2023a) have been proposed to speed up the in-
ference process. While these methods are effec-
tive, they typically necessitate modifications to the
model architecture and re-training, which can in-
cur substantial costs. Additionally, they may alter
the model’s output and require extra GPU mem-
ory needs. A method avoiding draft models using
retrieval is presented in (He et al., 2023), but it
requires a large database.
For certain LLMs, such as LLaMA, the tokeniza-
tion process can dissect a single word into multiple
tokens, thereby exacerbating inference latency. As
illustrated in Figure 1, the token count exceeds the
word count, resulting in an increased number of
autoregressive generation steps. In such scenar-
ios, given the constraints imposed by contextual
information, the search space for predicting the
next token that forms part of a word based on the
current token is significantly narrowed. Moreover,
contextual information can often be leveraged to
identify patterns and correlations between words.
This is especially evident for simple phrases and
paragraphs, where the context can provide clear
indicators that reduce the dependency on LLM de-
coding.
Based on the above motivation, this paper
presents a novel approach, the Adaptive N-gram
Parallel Decoding ( ANPD ), designed to enhance
inference efficiency without necessitating retrain-arXiv:2404.08698v2  [cs.CL]  10 Jul 2024

--- PAGE 2 ---
Figure 1: The comparative analysis of the number of
words and tokens after tokenizer processing for the CN-
N/Daily Mail and XSUM datasets.
ing or the integration of an auxiliary small language
model. ANPD dynamically generates draft outputs
via an adaptive N-gram module using real-time
statistics, after which the drafts are verified by the
LLM. This characteristic is exactly the difference
between ANPD and the previous speculative de-
coding methods. The primary contributions of this
work can be summarized as follows:
•We propose ANPD, a novel and lossless algo-
rithm that offers a plug-and-play module for
acceleration of LLM inference.
•We propose an adaptive N-gram modeling
strategy that is specifically adapted for LLMs,
markedly diminishing the complexity of lan-
guage modeling and reducing the dependency
on large-scale textual datasets.
•We propose a Multi-Level N-gram (MLN) al-
gorithm aimed at increasing the precision of
draft outputs, thereby enhancing the efficiency
of the acceleration process.
•We conduct extensive experiments on various
models and datasets, demonstrating the robust
acceleration capabilities of ANPD, with a no-
table increase of 1.95 ×-3.67×on LLaMA and
its fine-tuned derivatives.
2 Related Work
Inference systems. The development of special-
ized inference systems for Large Language Mod-
els (LLMs), such as NVIDIA’s TensorRT-LLM
(NVIDIA, 2023), Orca (Yu et al., 2022), Flex-
Gen (Sheng et al., 2023), and DeepSpeed Inference
(Aminabadi et al., 2022), represents a notable ad-
vancement in the field. Despite progress, there isstill a gap in the careful co-design of algorithms
and systems, which is necessary to fully harness
the potential of the hardware.
Compression. Efficient LLM inference is facili-
tated by techniques such as quantization (Han et al.,
2015; Frantar et al., 2022; Dettmers et al., 2022;
Xiao et al., 2023), pruning (Bansal et al., 2023;
Frantar and Alistarh, 2023; Liu et al., 2023), distil-
lation (Tang et al., 2019; Touvron et al., 2021), and
exit early strategies (Schuster et al., 2022; Kong
et al., 2022; Yang et al., 2023b; Bae et al., 2023;
Del Corro et al., 2023) suggest that some tokens
can be accurately generated using only a fraction
of the model layers. Token Prunings (Hou et al.,
2022; Yao et al., 2022; Zhang et al., 2023b) reduce
memory and computational demand to accelerate
the inference process by prioritizing crucial tokens.
These methods enhance efficiency but may neces-
sitate model alterations, re-training, and potentially
reduce accuracy.
Speculative Execution. Speculative execution
(Burton, 1985), adapted as speculative decoding in
LLMs (Chen et al., 2023; Leviathan et al., 2023),
has improved inference speeds by preempting com-
putations. SpecInfer (Miao et al., 2023) leverages
existing distilled, quantized, and pruned variants of
an LLM, to build a small speculative model pool
to guide speculation. However, these approaches
require a high-quality draft model, and increase
the memory footprint. Leviathan et al. (2023) also
mentioned that unigram and bigram can be used as
draft models, but they did not propose a method on
how to build a bigram model for the actual running
LLMs. Yang et al. (2023a) presented a method of
copying reference tokens to the decoder, though
its utility is limited by a dependency on repeated
text. These techniques increase resource use and
compel specialized training, such as distillation,
for the draft model to ensure compatibility with the
primary model.
3 Method
Figure 2 illustrates the framework and workflow of
proposed ANPD. We explain the original autore-
gressive decoding in the Appendix A.1.
3.1 Adaptive N-gram Parallel Decoding
Figure 2 illustrates the pipeline of our ANPD. The
process begins with tokenizing the input text into
tokens. The N-gram module’s Memory actually
stores token ids to streamline processing, Figure 2

--- PAGE 3 ---
Figure 2: The pipeline of the ANPD. The tokenizer first processes the text to obtain a list of tokens. These tokens
are used to initialize the N-gram module. Simultaneously, these tokens are fed into the LLM for processing via
autoregression. The predicted token at time t0in the figure is "_Very". This word is used as a query into the N-gram
module, yielding the token "_Re", which along with the "_Very" are sent to the LLM for inference at time t1. A
green checkmark signifies acceptance of the predicted token, while a red cross indicates rejection. Each accepted
token, is combined with the first N−1tokens to form a tuple, and the update method is called to refresh the
N-gram module.
shows tokens as the basis for modeling to make it
easier for readers to understand and improve read-
ability. Next, the LLM engages in autoregressive
inference, divided into two parts: 1. Prefill, where
the full prompt is input to generate the first token;
2. Decoding, ANPD feeds multiple tokens from
the N-gram module into the LLM, and the LLM
uses kv-cache for efficient computations to validate
tokens for parallel output generation. Tokens that
fail validation are discarded along with subsequent
tokens. Simultaneously, we use an adaptive strat-
egy to update the N-gram module throughout LLM
generation, avoiding reliance on static Memory.
Token Level N-gram Module. Contextual in-
formation is vital for content extraction, summa-
rization, and code generation, as it helps refine
the search space during each LLM decoding step.
This includes strong correlations among tokens
within words and between words in phrases and
contexts. We constructed a token-level N-gram
module to uniformly model the above correlations.
The N-gram module1is a probabilistic language
model, that predicts the next item in a sequence
using an (N−1)-th order Markov model, where
Nis the subsequence length. For a token sequence
x1, x2, ..., x t−1, the model estimates the probabil-
ity of xtbased on the preceding N−1tokens, as
1https://web.stanford.edu/~jurafsky/slp3/3.pdfP(xt|x1, ..., x t−1)≈P(xt|xt−N+1, ..., x t−1). In
a bigram model ( N= 2), the sentence probability
is:
P(x1, x2, ..., x n)≈nY
i=2P(xi|xi−1), (1)
probabilities P(xi|xi−1)derive from frequency
counts in the corpus. We have architected the N-
gram module to encapsulate three principal func-
tions essential for its operation:
•Initialize: using a tokenizer converts each
prompt into a sequence of token ids. It then
performs probabilistic statistics on these ids
and records the probability for each token tu-
ple.
•Update: during the decoding, each new to-
ken is paired with the previous N−1tokens
to form a tuple, used to update the module’s
probability Memory.
•Query: the query operation utilizes the to-
ken ids tuple, constructed through the subse-
quence from t−N+ 1tot−1, to predict
the next token xt, effectively leveraging the
statistical results established by the preceding
functions.

--- PAGE 4 ---
These functions collectively enable the N-gram
module to dynamically adapt to the evolving text
generation process, ensuring that each token gener-
ated is contextually relevant and statistically coher-
ent.
Parallel Decoding. The parallel decoding in
our ANPD is similar to the speculative decoding
approach and occurs in two distinct stages:
1.Drafting: the N-gram module is harnessed
to generate a sequence of subsequent tokens.
By iterating through Ksteps, the module
constructs a preliminary draft tokens with
length K. Specifically, the draft module
generates a series of Ktemporary tokens
xi+1, ..., x i+K, succeeding a given prompt se-
quence x1, ..., x i.
2.Verification: the original Large Language
Model (LLM) verifies the proposed draft
tokens, through a singular forward pass as
P(x′
i+K+1|(k, v)1, ...,(k, v)i, xi+1, ..., x i+K),
within which the LLM computes the prob-
ability distributions for each draft token,
then to ascertain their congruence with the
proposed draft tokens xi+1, ..., x i+K. If a
draft token xjdoes not pass this validation, it
is replaced by the LLM’s prediction x′
j, and a
new drafting begins from this token.
The ANPD enhances efficiency by eliminating
the need for a smaller draft deep learning model,
leveraging the much lower computational cost N-
gram module to accelerate LLM inference. For
LLMs, conducting parallel inference of Ktokens
introduces a negligible increase in computational
latency compared to single token autoregressive
inference, as shown in Figure 7 in Appendix A.2.
Meanwhile, our technique is intrinsically capable
of yielding at least jtokens ( 1≤j≤K+ 1) for
each decoding step, this intrinsic capability fun-
damentally assures, in principle, an acceleration
of the decoding processes within the Large Lan-
guage Model (LLM), thereby enhancing the over-
all computational throughput and reducing latency.
The implementation of the two-stage process con-
fers upon the ANPD the ability to iteratively refine
draft outputs. Furthermore, this guarantees that
our ANPD method is lossless, maintaining consis-
tency with the original LLM’s generated content.
The detailed procedure of ANPD is presented in
Algorithm 1, with a comprehensive explanation
available in Appendix ??.Algorithm 1 Adaptive N-gram Parallel Decoding
1:Input: prompt ,K,M
2:Output: O
3:token _ids←TOKENIZER (prompt )
4:Memory ←INITIALIZE (token _ids)
5:O←[ ],drafts ←[ ]
6:pred←LLM (prompt )
7:drafts.append (pred[−1])
8:while length (O)<Mdo
9: token _ids.append (drafts [1])
10: O.append (token _ids[−1]),UPDATE (O[−1])
11: tmp_token _ids←token _ids[−N+ 1 :]
12: fork←1toKdo
13: tmp←tmp_token _ids[−N+k:]
14: drafts.append (QUERY (tmp))
15: tmp_token _ids.append (drafts [−1])
16: end for
17: predicts ←LLM (drafts )
18: forj←2toLENGTH (drafts )do
19: ifdrafts [j] == predicts [j−1]then
20: O.append (drafts [j])
21: UPDATE (drafts [j])
22: token _ids.append (drafts [j])
23: else
24: break
25: end if
26: end for
27: ifj== LENGTH (drafts )then
28: drafts ←[predicts [j]]
29: else
30: drafts ←[predicts [j−1]]
31: end if
32:end while
3.2 Multi-Level N-gram
The predictive accuracy of the N-gram module is
known to correlate with N, larger Nvalues gen-
erally result in more accurate content predictions.
This effect is especially noticeable in settings with
the longer context of Language Model (LM) tasks,
where increasing Ncan markedly decrease the fre-
quency of prediction errors.
While a larger Ntends to improve the predictive
accuracy of the N-gram module, it may not always
result in a successful match during the Query oper-
ation. To address this, we propose the Multi-Level
N-gram (MLN) approach, which is based on opti-
mal prefix matching. The MLN design initializes
N−1separate modules, each corresponding to an
n-gram module ( n∈[2, N]). During prediction,

--- PAGE 5 ---
Algorithm 2 Multi-Level N-gram
1:Input: tmp,N,token _ids
2:Output: result
3:Memory ←INITIALIZE (token _ids)
4:result ←NULL
5:n←N
6:while n≥2do
7: pred←QUERY (query, n )
8: ifpred̸=NULL then
9: result ←pred
10: break
11: end if
12: n←n−1
13:end while
14:return result
the query starts with the largest Nand proceeds to
lower nlevels, stopping when a successful match
is found as shown in Algorithm 2.
4 Experiments
4.1 Implementation Details
We selected a diverse range of models, varying in
scale, architectural design, and training approaches,
to ensure a thorough evaluation, including LLaMA-
7B (Touvron et al., 2023a), LLaMA-2-7B (Touvron
et al., 2023b), ChatGLM3-6B (Du et al., 2022),
LLaMA-2-13B, CodeLLaMA-7B (Roziere et al.,
2023), CodeLLaMA-13B, and instruction-tuned
variants such as Alpaca-7B and Alpaca-CNN/DM-
7B, fine-tuning details are provided in the Ap-
pendix A.4. We use one RTX-3090 GPU for all 7B
models, while the larger 13B models necessitate
four RTX-3090 GPUs and the accelerate2library.
4.2 Datasets & Metrics
To validate the effectiveness of our method in accel-
erating text generation for LLMs, we concentrated
on two tasks: text summarization and code gener-
ation, utilizing datasets such as CNN/Daily Mail
(CNN/DM) (Hermann et al., 2015), Extreme Sum-
marization (XSum) (Narayan et al., 2018), and the
HumanEval (Chen et al., 2021). For additional de-
tails on the evaluation settings, please see Appendix
A.5. We employ the speed-up ratio as the evalu-
ation metric, which is calculated by dividing the
inference time of the autoregressive process by the
inference time of the ANPD process, under identi-
cal conditions across all samples (For summariza-
2https://github.com/huggingface/acceleratetion tasks, we use a sample size of 1000 to ensure
statistical significance, as recommended by (Zhang
et al., 2023a)). This metric intuitively demonstrates
the performance improvement in speed when using
the ANPD algorithm.
4.3 Main Results
In Table 1, we present a comparative analysis that
outlines the acceleration benefits for various mod-
els and datasets. We have selected (Zhang et al.,
2023a) for comparison. Not only are their experi-
mental datasets and models aligned with ours, but
their methodologies are also open-sourced to fa-
cilitate easy replication. The prompts used with
these models are comprehensively documented in
Appendix A.5 to facilitate further examination and
ensure the reproducibility of the results reported in
this paper.
As illustrated in Table 1, the ANPD algorithm
consistently accelerates inference across various
models, including the base LLM, the instruction-
fine-tuned Alpaca, and the model fine-tuned with
dataset-specific instructions, indicating its robust-
ness and efficiency in accelerating text generation.
Remarkably, for the LLaMA-7B model, ANPD can
speed up the inference speed over 2.0 ×, which is
still valid on LLaMA2. Our method achieves a
twofold (2.9088 ×vs. 1.3293 ×) increase in accel-
eration compared to (Zhang et al., 2023a) on the
LLaMA-2-13B. Despite the ChatGLM3 model hav-
ing a significantly larger vocabulary (nearly twice
that of LLaMA, the token/word ratio will be closer
to 1), our ANPD algorithm still achieves a speed-
up of 1.7046 ×and 1.6647 ×for CNN/DM and
XSum, respectively. In ChatGLM3, ANPD’s pre-
dictive mechanism primarily leverages the asso-
ciative relationships between phrases and individ-
ual words, rather than engaging in token-level pre-
dictions within the words themselves. So, ANPD
maintains robustness and consistently enhances in-
ference speeds across varied LLMs. Owing to the
presence of a high occurrence of correlated patterns
in code writing tasks, which significantly enhanced
the prediction accuracy of the ANPD algorithm.
The ANPD algorithm was able to achieve a sub-
stantial speed-up of 3.6665 ×on the HumanEval,
but (Zhang et al., 2023a) only has a speed-up of
1.6758 ×for CodeLLaMA-13B.
4.4 Ablation Study
We conduct an analysis of hyperparameters on CN-
N/DM dataset, focusing primarily on KandN. In

--- PAGE 6 ---
Model shot CNN/DM XSum
LLaMA-7B 1 2.7455x 3.1195x
Alpaca-7B 0 2.5566x 2.3022x
Alpaca-CNN/DM-7B 0 1.9481x 2.0561x
LLaMA-2-13b (Zhang et al., 2023a) 1 1.3293x 1.2801x
LLaMA-2-7B 1 2.8604x 2.7973x
LLaMA-2-13B 1 2.9088x 2.6063x
ChatGLM3-6B 0 1.7046x 1.6647x
Model shot HumanEval
CodeLLaMA-13B (Zhang et al., 2023a) 0 1.6758x
CodeLLaMA-7B 0 3.5985x
CodeLLaMA-13B 0 3.6665x
Table 1: The comparison of acceleration effects on dif-
ferent models and datasets.
Figure 3, we set Nto 2, and perform a comparative
analysis of the parameter K. Our findings indicate
that increasing Kcontributes to a greater accelera-
tion effect, however, the acceleration gains plateau
when Klies within the range of 6 to 8.
Figure 3: Speed up ratio of LLM for different K.
Based on the experiment in Figure 3, we selected
6, 7, and 8 for Kto conduct further hyperparameter
combination experiments, as illustrated in Figures 4
and 5. The experimental results indicate that the
Multi-Level N-gram (MLN) approach enhances in-
ferential speed as the parameter Nincreases. How-
ever, beyond N= 5, further increments in Nyield
no significant additional gains. Additionally, the ef-
fect of the parameter Kon acceleration is relatively
stable; as shown in Figure 3, the acceleration effect
reaches a plateau within the range of 6 to 8 for
K. These findings are consistent across different
models with different N.
Based on the empirical evidence presented in
Figure 4 and Figure 5, a pragmatic choice for N
andKcan be posited at N= 5andK= 7respec-
tively. The analogous experiments pertaining to
the HumanEval dataset have been relegated to Ap-
pendix A.6 for reference, similar conclusions can
also be observed in this dataset. While employing
the Multi-Level N-gram (MLN) has improved the
accuracy of draft predictions, we have also carried
out distinct experiments (Figure 10, Appendix A.6)using N-gram modules without MLN, to demon-
strate that simply enlarging the value of Nis not
effective.
Figure 4: Decoding speed up ratio of LLaMA-7B for
different KandN.
Figure 5: Decoding speed up ratio of Alpaca-CNN/DM-
7B for different KandN.
4.5 Case Study
Figure 6 showcases a detailed example of the
ANPD inference process, utilizing the Alpaca-7B
model on a sample from the CNN/DM test set. The
Alpaca-7B model, which has been fine-tuned with
instructions, was chosen due to its broad applica-
bility in practical scenarios. In this example, the
ANPD algorithm is configured with N= 5 and
K= 7, achieving a 2.19 ×decoding speed-up com-
pared to the original autoregressive process, with
a draft text pass rate (Draft hit ratio, α) of 20.59%
in the LLM verification phase. Based on the hit
ratio, we can derive the theoretical upper bound of
acceleration as (α×K) + 1 , we can calculate that
the theoretical speed-up is 2.44, as the loss caused
by implementation problems will be slightly higher
than the actual acceleration rate. The Figure 6
uses red underlines to represent a decoding step,
including drafting and verification, with the yellow
background indicating the beginning of one step.
Light blue and green backgrounds mark the draft

--- PAGE 7 ---
Figure 6: Visualizing the step-by-step inference process of ANPD: An example from CNN/DM.
content that has passed verification. This example
demonstrates that inference acceleration primar-
ily benefits from the combination of names (e.g.,
_Athlet, ic, _Bil, ba, o), partial words(e.g., _har, sh,
ly), and phrases (e.g., _reduced, _to), aligning with
the motivation behind the ANPD algorithm. The
ANPD can quickly capture the association between
tokens and words based on this information, and
establish the prediction model, thus accelerating
the end-to-end decoding process.
4.6 User Friendly
As ANPD does not involve additional deep learn-
ing models or plug-in databases, it does not re-
quire complex initialization processes and envi-
ronment configuration installations. Consequently,
users can employ it directly and with great conve-
nience, as illustrated in Listing 1. We plan to re-
lease the associated open-source software packages
on GitHub3, making them accessible for everyone
to utilize and contribute to.
Listing 1: Python example
from anpd import anpd_llm
# import other libraries as usual
model = AutoModel.from_pretrain()
model = anpd_llm(model, n=5, k=7)
prompt = "Hello,World!"
result = model.gen(prompt)
3https://github.com/oujieww/ANPD5 Conclusion
In this paper, we presented the ANPD algorithm, a
novel and lossless approach to accelerate the Large
Language Models (LLMs) inference. This algo-
rithm implements an adaptive N-gram modeling
strategy, reducing the necessity for large corpora
and eliminating the requirement to build an addi-
tional deep-learning draft language model. The
Multi-Level N-gram (MLN) strategy not only en-
hances draft output accuracy but also further boosts
efficiency. Our empirical studies across various
models and datasets validate the ANPD algorithm’s
effectiveness, with a remarkable peak acceleration
of up to 3.67 ×achieved. The ANPD algorithm
has demonstrated its potency as a powerful tool
for enhancing the efficiency of LLMs. As a plug-
and-play module, it enables more extensive and
pragmatic use of LLMs in various real-world con-
texts.
Future Works. We believe that ANPD can be
further enhanced in two key aspects:
1.Incorporating the specific characteristics of in-
dividual LLMs (e.g., LLaMA, ChatGLM) by
creating features tailored to different LLMs to
further accelerate the inference performance.
2.Exploring the possibility of generating multi-
ple tokens in parallel during the LLMs verifi-
cation process to further accelerate the infer-
ence performance.

--- PAGE 8 ---
6 Acknowledgements
This research is supported by the National Key Re-
search and Development Program of China with
Grant ID 2018AAA0103203 and the Chengdu Sci-
ence and Technology Project with Grant ID 2022-
YF05-02014-SN. This research is also supported by
Huawei MindSpore Team for providing technical
assistance and experience sharing.
References
Radford Alec, Narasimhan Karthik, Salimans Tim, and
S Ilya. 2018. Improving language understanding
with unsupervised learning. Citado , 17:1–12.
Reza Yazdani Aminabadi, Samyam Rajbhandari, Am-
mar Ahmad Awan, Cheng Li, Du Li, Elton Zheng,
Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff
Rasley, et al. 2022. Deepspeed-inference: enabling
efficient inference of transformer models at unprece-
dented scale. In SC22: International Conference for
High Performance Computing, Networking, Storage
and Analysis , pages 1–15. IEEE.
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-
son, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
Chen, et al. 2023. Palm 2 technical report. arXiv
preprint arXiv:2305.10403 .
Sangmin Bae, Jongwoo Ko, Hwanjun Song, and Se-
Young Yun. 2023. Fast and robust early-exiting
framework for autoregressive language models with
synchronized parallel decoding. In Proceedings of
the 2023 Conference on Empirical Methods in Natu-
ral Language Processing , pages 5910–5924.
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al. 2023. Qwen technical report. arXiv
preprint arXiv:2309.16609 .
Hritik Bansal, Karthik Gopalakrishnan, Saket Dingliwal,
Sravan Bodapati, Katrin Kirchhoff, and Dan Roth.
2023. Rethinking the role of scale for in-context
learning: An interpretability-based case study at 66
billion scale.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
F Warren Burton. 1985. Speculative computation, par-
allelism, and functional programming. IEEE Trans-
actions on Computers , 100(12):1190–1193.
Charlie Chen, Sebastian Borgeaud, Geoffrey Irving,
Jean-Baptiste Lespiau, Laurent Sifre, and John
Jumper. 2023. Accelerating large language modeldecoding with speculative sampling. arXiv preprint
arXiv:2302.01318 .
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, Gretchen
Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-
try, Pamela Mishkin, Brooke Chan, Scott Gray,
Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz
Kaiser, Mohammad Bavarian, Clemens Winter,
Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza-
beth Barnes, Ariel Herbert-V oss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N.
Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welinder,
Bob McGrew, Dario Amodei, Sam McCandlish, Ilya
Sutskever, and Wojciech Zaremba. 2021. Evaluating
large language models trained on code.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebas-
tian Gehrmann, et al. 2023. Palm: Scaling language
modeling with pathways. Journal of Machine Learn-
ing Research , 24(240):1–113.
Luciano Del Corro, Allie Del Giorno, Sahaj Agarwal,
Bin Yu, Ahmed Awadallah, and Subhabrata Mukher-
jee. 2023. Skipdecode: Autoregressive skip decoding
with batching and caching for efficient llm inference.
arXiv preprint arXiv:2307.02628 .
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke
Zettlemoyer. 2022. Gpt3. int8 (): 8-bit matrix mul-
tiplication for transformers at scale. Advances in
Neural Information Processing Systems , 35:30318–
30332.
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:
General language model pretraining with autoregres-
sive blank infilling. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 320–335.
Elias Frantar and Dan Alistarh. 2023. Massive language
models can be accurately pruned in one-shot. arXiv
preprint arXiv:2301.00774 .
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and
Dan Alistarh. 2022. Gptq: Accurate post-training
quantization for generative pre-trained transformers.
arXiv preprint arXiv:2210.17323 .
Song Han, Huizi Mao, and William J Dally. 2015. Deep
compression: Compressing deep neural networks
with pruning, trained quantization and huffman cod-
ing. arXiv preprint arXiv:1510.00149 .

--- PAGE 9 ---
Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee,
and Di He. 2023. Rest: Retrieval-based speculative
decoding. arXiv preprint arXiv:2311.08252 .
Karl Moritz Hermann, Tomás Kociský, Edward Grefen-
stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,
and Phil Blunsom. 2015. Teaching machines to read
and comprehend. In NIPS , pages 1693–1701.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531 .
Le Hou, Richard Yuanzhe Pang, Tianyi Zhou, Yuexin
Wu, Xinying Song, Xiaodan Song, and Denny Zhou.
2022. Token dropping for efficient bert pretraining.
InProceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 3774–3784.
Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Ji-
tendra Malik, Michael W Mahoney, Amir Gholami,
and Kurt Keutzer. 2023. Speculative decoding with
big little decoder. In Thirty-seventh Conference on
Neural Information Processing Systems .
Jun Kong, Jin Wang, Liang-Chih Yu, and Xuejie Zhang.
2022. Accelerating inference for pretrained language
models by unified multi-perspective early exiting. In
Proceedings of the 29th International Conference on
Computational Linguistics , pages 4677–4686.
Yaniv Leviathan, Matan Kalman, and Yossi Matias.
2023. Fast inference from transformers via spec-
ulative decoding. In International Conference on
Machine Learning , pages 19274–19286. PMLR.
Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang
Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang,
Yuandong Tian, Christopher Re, et al. 2023. Deja
vu: Contextual sparsity for efficient llms at infer-
ence time. In International Conference on Machine
Learning , pages 22137–22176. PMLR.
Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao
Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuom-
ing Chen, Daiyaan Arfeen, Reyna Abhyankar, and
Zhihao Jia. 2023. Specinfer: Accelerating generative
llm serving with speculative inference and token tree
verification. arXiv preprint arXiv:2305.09781 .
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo
Aila, and Jan Kautz. 2016. Pruning convolutional
neural networks for resource efficient inference. In
International Conference on Learning Representa-
tions .
Shashi Narayan, Shay B. Cohen, and Mirella Lapata.
2018. Don’t give me the details, just the summary!
topic-aware convolutional neural networks for ex-
treme summarization. ArXiv , abs/1808.08745.
NVIDIA. 2023. Tensorrt-llm: NVIDIA tensorrt for
large language models.
OpenAI. 2023. Gpt-4 technical report.Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,
Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.
Code llama: Open foundation models for code. arXiv
preprint arXiv:2308.12950 .
Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani,
Dara Bahri, Vinh Tran, Yi Tay, and Donald Metzler.
2022. Confident adaptive language modeling. Ad-
vances in Neural Information Processing Systems ,
35:17456–17472.
Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuo-
han Li, Max Ryabinin, Beidi Chen, Percy Liang,
Christopher Ré, Ion Stoica, and Ce Zhang. 2023.
Flexgen: High-throughput generative inference of
large language models with a single gpu. In Inter-
national Conference on Machine Learning , pages
31094–31116. PMLR.
Benjamin Frederick Spector and Christopher Re. 2023.
Accelerating llm inference with staged speculative
decoding. In Workshop on Efficient Systems for Foun-
dation Models@ ICML2023 .
Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga
Vechtomova, and Jimmy Lin. 2019. Distilling task-
specific knowledge from bert into simple neural net-
works. arXiv preprint arXiv:1903.12136 .
Hugo Touvron, Matthieu Cord, Matthijs Douze, Fran-
cisco Massa, Alexandre Sablayrolles, and Hervé Jé-
gou. 2021. Training data-efficient image transform-
ers & distillation through attention. In International
conference on machine learning , pages 10347–10357.
PMLR.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, et al. 2023a. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023b. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Neeraj Varshney, Agneet Chatterjee, Mihir Parmar, and
Chitta Baral. 2023. Accelerating llama inference by
enabling intermediate layer decoding via instruction
tuning with lite.
Heming Xia, Tao Ge, Si-Qing Chen, Furu Wei, and
Zhifang Sui. 2022. Speculative decoding: Lossless
speedup of autoregressive translation.
Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu,
Julien Demouth, and Song Han. 2023. Smoothquant:
Accurate and efficient post-training quantization for
large language models. In International Conference
on Machine Learning , pages 38087–38099. PMLR.

--- PAGE 10 ---
Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin
Jiang, Linjun Yang, Rangan Majumder, and Furu
Wei. 2023a. Inference with reference: Lossless ac-
celeration of large language models. arXiv preprint
arXiv:2304.04487 .
Seongjun Yang, Gibbeum Lee, Jaewoong Cho, Dimitris
Papailiopoulos, and Kangwook Lee. 2023b. Predic-
tive pipelined decoding: A compute-latency trade-off
for exact llm decoding. In Workshop on Efficient
Systems for Foundation Models@ ICML2023 .
Zhewei Yao, Xiaoxia Wu, Conglong Li, Connor Holmes,
Minjia Zhang, Cheng Li, and Yuxiong He. 2022.
Random-ltd: Random and layerwise token dropping
brings efficient training for large-scale transformers.
arXiv preprint arXiv:2211.11586 .
Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soo-
jeong Kim, and Byung-Gon Chun. 2022. Orca: A
distributed serving system for {Transformer-Based }
generative models. In 16th USENIX Symposium
on Operating Systems Design and Implementation
(OSDI 22) , pages 521–538.
Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen,
Gang Chen, and Sharad Mehrotra. 2023a. Draft
& verify: Lossless large language model accelera-
tion via self-speculative decoding. arXiv preprint
arXiv:2309.08168 .
Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong
Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuan-
dong Tian, Christopher Ré, Clark Barrett, et al. 2023b.
H_2o: Heavy-hitter oracle for efficient generative
inference of large language models. arXiv preprint
arXiv:2306.14048 .
