# 2407.12021.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/speculative/2407.12021.pdf
# File size: 737590 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Adaptive Draft-Verification for Efficient Large Language Model Decoding
Xukun Liu1, Bowen Lei2, Ruqi Zhang3Dongkuan Xu4
1Northwestern University
2Texas A&M University
3Purdue University
4North Carolina State University
xukunliu2025@u.northwestern.edu, bowenlei@stat.tamu.edu, ruqiz@purdue.edu, dxu27@ncsu.edu
Abstract
Large language model (LLM) decoding involves generating
a sequence of tokens based on a given context, where each
token is predicted one at a time using the model’s learned
probabilities. The typical autoregressive decoding method re-
quires a separate forward pass through the model for each to-
ken generated, which is computationally inefficient and poses
challenges for deploying LLMs in latency-sensitive scenar-
ios. The main limitations of current decoding methods stem
from their inefficiencies and resource demands. Existing ap-
proaches either necessitate fine-tuning smaller models, which
is resource-intensive, or relying on fixed retrieval schemes to
construct drafts for the next tokens, which lack adaptability
and fail to generalize across different models and contexts.
To address these issues, we introduce a novel methodology
called ADED1, which accelerates LLM decoding without re-
quiring fine-tuning. Our approach involves an adaptive draft-
verification process that evolves over time to improve effi-
ciency. We utilize a tri-gram matrix-based LLM representa-
tion to dynamically approximate the output distribution of
the LLM, allowing the model to adjust to changing token
probabilities during the decoding process. Additionally, we
implement a draft construction mechanism that effectively
balances exploration and exploitation, ensuring that the drafts
generated are both diverse and close to the true output dis-
tribution of the LLM. The importance of this design lies in
its ability to optimize the draft distribution adaptively, lead-
ing to faster and more accurate decoding. Through extensive
experiments on various benchmark datasets and LLM archi-
tectures, we demonstrate that ADED accelerates the decoding
process while maintaining high accuracy, making it suitable
for deployment in a wide range of practical applications.
Introduction
Large language model (LLM) decoding involves generating
a sequence of tokens based on a given context, where each
token is predicted one at a time using the model’s learned
probabilities (Brown et al. 2020; Zhang et al. 2022; Touvron
et al. 2023a,b). The core mechanism is autoregressive, where
each new token is generated conditioned on the previously
generated tokens and the given context. This process is cru-
cial for applications like text generation (Li et al. 2024a; Peng
et al. 2023; Chang et al. 2023), machine translation (Zhang,
Haddow, and Birch 2023; Moslem et al. 2023; Hendy et al.
1Project repo: https://anonymous.4open.science/r/ADED-C7D5
Small 
LLM
Corpus 
Trie
Large 
LLM
Tri-gram 
Matrix
The 
LLMs
LLMs 
is
is 
the
is 
a
the 
best
the 
biggest
the 
greatest
technique
Last 
2 
tokens 
of 
input
MCTS 
Simulation
REST
Lookahead
Speculative 
Decoding
Ours
Input 
Tokens
Draft 
Tokens 
Generated
Intermediate 
Results
MCTS
The 
LLMs 
is 
the 
best
The 
LLMs 
is  
a 
technique
...
Output
LLMs
Original 
LLMs 
DecodingFigure 1: Comparison of different LLM decoding strategies.
InSpeculative Decoding , a small LLM generates predictions
(red blocks) from inputs (blue blocks). Yellow blocks indi-
cating intermediate results obtained from language model.
Lookahead uses a large LLM for forward-looking predictions.
REST employs a corpus trie for rapid token lookups. ADED in-
tegrates Monte Carlo Tree Search with tri-gram statistics and
recent token history to simulate potential outputs, refining
its recommendations over time. ADED ’s adaptive approach
offers advantages in terms of speed and accuracy by continu-
ously evolving its draft constructions.
2023), and conversational AI (Shanahan 2024; Wu et al. 2023;
Saka et al. 2023). However, each decoding step involves a for-
ward pass through the model, making the process inherently
sequential and computationally expensive. The inefficiencies
arise due to the need to reload the model for each token
prediction, leading to high computational costs and memory
bandwidth usage. This serial nature of decoding is a signif-
icant bottleneck, especially for real-time applications (Liu
et al. 2023a; Mandvikar 2023; Antoniol et al. 1994) where
latency is critical. Thus, optimizing the decoding speed of
LLMs is essential for practical deployment.
Recent research has explored various strategies to miti-
gate the inefficiencies of LLM decoding. Speculative De-
coding (Leviathan, Kalman, and Matias 2023; Spector and
Re 2023; Chen et al. 2023) introduces an approach where aarXiv:2407.12021v2  [cs.CL]  19 Aug 2024

--- PAGE 2 ---
smaller, more efficient model generates several token predic-
tions in parallel, which are then verified by the larger target
model. This method leverages the efficiency of smaller mod-
els to reduce the number of serial forward passes required,
achieving substantial speedups without altering the output
distribution. Lookahead Decoding (Fu et al. 2024a) uses
the full context to predict multiple future tokens, creating a
buffer that reduces the dependency on sequential processing.
REST (He et al. 2024) employs a retrieval-based approach
where relevant tokens are fetched from a pre-constructed
datastore using the current context, forming drafts that are
verified by the LLM. These methods can be summarized
within the draft-verification pipeline, as shown in Figure 1.
Speculative Decoding andLookahead Decoding both gen-
erate draft tokens through predictive models, while REST
constructs drafts from retrieved tokens based on the context.
In each case, the drafts are then verified by the main LLM,
ensuring that the final output adheres to the model’s learned
probabilities. Despite their advancements, these approaches
face notable limitations. They often require additional train-
ing or fine-tuning, which can be resource-intensive. Fixed
retrieval schemes lack adaptability, making it challenging to
adjust the draft distribution in real-time based on the evolving
LLM output. Additionally, these methods may not general-
ize well across different models and contexts, limiting their
effectiveness in dynamic environments.
In this work, our focus is on fine-tuning-free draft-
verification to address these limitations. The draft-
verification pipeline can be viewed as a rejection sampling
procedure where the similarity between the proposal distribu-
tion ( draft ) and the target distribution ( LLM output) is crucial
for the acceptance rate and convergence speed. Higher simi-
larity results in a higher acceptance rate and faster decoding
speed. Very few fine-tuning-free approaches, e.g.,REST (He
et al. 2024), typically use fixed retrieval-based schemes to
construct drafts. These schemes lack the adaptability to adjust
the draft distribution based on the evolving LLM output dis-
tribution, resulting in a persistent gap between the draft and
the actual LLM output. This gap reduces the draft acceptance
rate and limits the potential for improving decoding speed.
To address this issue, we raise the following question:
Research Question: How to design an adaptive draft con-
struction process that can evolve itself and accurately ap-
proximate LLM outputs during decoding?
To introduce adaptability and find drafts that are increas-
ingly close to the LLM output distribution during decod-
ing, we not only need to have an adaptive draft construction
pipeline but also need to maintain a balance between explo-
ration and exploitation. This balance ensures that speedups
can be achieved by leveraging existing knowledge of draft
construction while continuously exploring better draft con-
struction capabilities. To achieve this, we propose a novel
methodology called ADED (Adaptive Draft-Verification for
Efficient LLM Decoding). ADED incorporates a tri-gram-
matrix-based adaptive LLM representative to control the
conditional probability distribution of the next token, which
can be updated during the decoding process to adjust the
draft construction accordingly. To balance exploration and
exploitation, we design a draft maker inspired by MonteCarlo Tree Search ( MCTS )(Coulom 2007; Browne et al.
2012; James, Konidaris, and Rosman 2017; ´Swiechowski
et al. 2023). This draft maker uses a token preference score
to maintain the balance during the search process. The score
consists of two parts: the first part is based on the approxi-
mate conditional probability distribution of the next token
obtained from the LLM representative, reflecting the draft
maker ’s current knowledge of the LLM output; the second
part encourages the draft maker to explore unexplored or
less-explored draft spaces. Theoretically, we show that our
method can be viewed as a constrained optimization problem
to encourage the draft distribution to converge to the LLM
output distribution. Using the token preference score, the
draft maker can effectively search the draft space and gener-
ate candidate tokens. After the draft construction and verifi-
cation are completed, the information is fed back to the LLM
representative to update its approximation of the LLM output.
This feedback loop enriches the draft maker ’s knowledge in
subsequent rounds of draft-verification, enabling adaptability
and self-improvement in the draft construction process.
In summary, our contributions are concluded as follows:
•We design a tri-gram matrix-based representation that
dynamically approximates the LLM output distribution,
enhancing adaptability without the need for fine-tuning.
It addresses the limitation of fixed retrieval schemes by
continuously evolving with the model’s predictions.
•We develop a draft maker that effectively balances ex-
ploration and exploitation to generate high-quality drafts.
This mechanism improves decoding speed and accuracy
by ensuring that the drafts are closely aligned with the
LLM’s output distribution. Our experiments show a 2.5X
improvement in decoding speed compared to baselines.
•Through extensive experiments on various benchmark
datasets and LLM architectures, we demonstrate that
ADED successfully accelerates the decoding process while
maintaining high accuracy. Specifically, we achieve up to
a2.5X speedup in latency and an average acceptance rate
improvement of 20% over existing methods.
•Our method’s ability to adapt to evolving LLM outputs
and continuously refine draft construction sets it apart
from existing ones, addressing the need for more flexible
and dynamic decoding solutions.
Methodology
We propose a new fast fine-tuning-free draft-verification
LLM decoding method by introducing adaptability into the
decoding and learning from LLM, which is illustrated in Fig-
ure 2. Existing accelerated decoding algorithms either require
additional fine-tuning or lack adaptability to LLM’s output
distributions, resulting in additional cost or insufficient ac-
celeration. To address these issues, we design an adaptive
LLM representation based on a tri-gram matrix to adaptively
approximate the output distribution of the LLM, develop a
draft maker that balances exploration and exploitation for
self-improvement towards high-quality drafts, and verify the
drafts using tree attention.

--- PAGE 3 ---
Corpus
Tri-gram 
Matrix
A1
A2
A3
MCTS 
Draft 
Maker
A4
A5
A6
A7
A3 
A4
A5
A1 
A2
A3
A2 
A3
A4
A6 
A7
B
1
1 
B
1
2 
B
1
3
A6 
A7
B
2
1 
B
2
2 
B
2
3
A6 
A7
B
3
1 
B
3
2 
B
3
3
A1 
... 
A7
B
1
1 
B
1
2 
B
1
3
B
2
1 
B
2
2 
B
2
3
B
3
1 
B
3
2 
B
3
3
LLM
2. 
Update 
Tri-gram 
Matrix
...
3. 
Retrieval
4. 
Select 
Top-k
Position:     
1   
...   
7         
8    
9   
10       
8     
9    
10     
8     
9    
10
3. 
Retrieval
1. 
Generate 
Tri-gram
5. 
Construct 
LLM 
Input
ResultFigure 2: This figure illustrates the data processing workflow
ofADED . Initially, the input tokens undergo preprocessing to
calculate their tri-grams, which serve to update the tri-gram
matrix. Subsequently, the updated matrix, in conjunction with
the last two tokens of the input, is used to retrieve potential
token sequences. These sequences are ranked, and the top-k
sequences are selected, and then appended to the original
input. Finally, these extended sequences are inputted into the
Large Language Model for prediction.
Preliminary
Speculative decoding is a method to accelerate language
model inference by using a smaller auxiliary model to gener-
ate a draft sequence, reducing the computational load on
the larger model (Leviathan, Kalman, and Matias 2023).
Retrieval-based speculative decoding extends this by in-
corporating a retrieval system instead of the smaller model,
leveraging pre-stored corpus segments for relevant text gen-
eration. Monte Carlo Tree Search (MCTS ) (Coulom 2007;
Browne et al. 2012; James, Konidaris, and Rosman 2017;
´Swiechowski et al. 2023) is an algorithm that optimizes
decision-making by balancing exploration and exploitation
of future states. It selects nodes for further exploration us-
ing a combination of node visit counts and estimated values,
aiming to maximize overall outcomes. For a comprehensive
discussion of these methods, please refer to Appendix E .
Adaptive LLM Representative
To approximate the output token distribution of the LLM
without fine-tuning the small model, we distill linguistic
knowledge from a small corpus and construct a tri-gram
matrix as an initial representation of the LLM, which allows
us to leverage the statistical regularities of language at a gran-
ular level. Specifically, we summarize and count each set
of three tokens that appear in the corpus and compute the
probability of the third token appearing conditional on the
first two tokens. The formula is defined in Eq. (1):
P(wi|wi−2, wi−1) =C(wi−2, wi−1, wi)
C(wi−2, wi−1), (1)where P(wi|wi−2, wi−1)is the conditional probability of
a word wigiven the two preceding words wi−2andwi−1,
C(wi−2, wi−1, wi)is the count of the tri-gram occurrence in
the corpus, and C(wi−2, wi−1)is the count of the preceding
bi-gram (Mori, Nishimura, and Itoh 1998).
In this way, we can obtain a good initial LLM represen-
tative at a much lower cost, which can generate an approx-
imate distribution of the next token based on the previous
tokens. This LLM representative will collaborate with our
draft maker to generate drafts and get feedback to update the
tri-gram matrix for adaptability and self-improvement. Please
seeSection 2.3 for more details.
Draft Maker and Self-Improvement
With the help of the LLM representative, we further pro-
pose a draft maker that balances exploration and exploitation
while searching for candidate drafts that are closer to the
LLM output. On the one hand, the draft maker leverages the
conditional probabilities from the LLM representative, which
include current knowledge of the LLM output. On the other
hand, the draft maker is encouraged to search more in the
unexplored or less explored draft space to find better draft
candidates. Then, with the feedback from the LLM output,
the LLM representative can update its understanding of the
LLM output, improve the draft maker’s search, and achieve
self-improvement. Details are provided below.
Draft Search Score : Given the initial tokens, we exploit
Monte Carlo Tree Search ( MCTS ) (Coulom 2007) to guide
the search process of the drafts of the next tokens, where we
prioritize candidate tokens according to the conditional prob-
ability from the tri-gram matrix-based LLM representative
and the node visitation counts during the tree search. The
score plays a key role in balancing exploration and utilization
during the Monte Carlo tree search and is defined as Eq. (2).
PUCT (s, a) =Q(s, a) +E·P(s, a)·pP
bN(s, b)
1 +N(s, a).(2)
The score design is motivated by PUCT Score (Rosin 2011;
Silver et al. 2017). In particular, Q(s, a)assesses the quality
of taking action ain state s, while P(s, a)represents the
prior probability of selecting action ain state s. The term
N(s, a)denotes the number of times the action ahas been
taken from state s, andP
bN(s, b)sums the counts for all
actions from state s. Eq. (3) plays a critical role in determin-
ing the balance between exploration and exploitation within
theMCTS framework.
E=C1+ logP
bN(s, b) +C2+ 1
C2
, (3)
The constant C1acts as a base level adjustment, while C2
modulates the logarithmic term to scale the exploration factor
dynamically based on the total visitation counts. This formula
ensures that our draft choices are contextually appropriate and
optimizes the robustness and coherence of text generation.
Self-Improvement Strategy Transfer : Based on the final
search score obtained during the search, we can construct
draft candidates and verify them to get the final decoding
output (please see Section 2.4 ) and feed it back for self-
improvement. This final output decoding represents LLM’s

--- PAGE 4 ---
output distribution, which would be a good learning material
for the LLM representative. Therefore, we feed this knowl-
edge into the LLM representative in order to obtain updated
conditional probability distributions, thus providing the draft
maker with more accurate and exploitable knowledge, which
is illustrated in Figure 2. Specifically, this technique oper-
ates by first extracting tri-grams from recent outputs of the
LLM. Each tri-gram’s frequency is then used to update its
probability as potential outputs. These adjusted probabilities
are fed into the MCTS as part of the policy network, influ-
encing the selection phase of the tree search. The updated
tri-gram probabilities essentially serve as a dynamic policy
guide, enhancing the model’s ability to generate contextually
relevant and coherent sequences. By incorporating learned
tri-gram probabilities into the tree search algorithm, we effec-
tively create a feedback loop where the search strategy itself
evolves over time. This strategy adjustment is executed by
recalibrating the exploration-exploitation balance based on
the empirical data derived from the model’s own outputs.
Draft Construction and Verification
It is important to note that candidate drafts generated by
the draft maker often have common starting segments that
can cause redundant recalculations in the Transformer layers
if not managed correctly. To address the issue, a pseudo-
sequence that guarantees that each draft is a sub-sequence
and that any common prefix appears only once is created (He
et al. 2024). Motivated by this observation, we use a specific
attention mask for each attention layer, called tree atten-
tion(Miao et al. 2023; Cai et al. 2024). This mask aligns the
computations for each token with its dependencies according
to the original draft sequence, preserving the draft’s contex-
tual integrity and preventing unnecessary computations. The
approval of drafts relies on a comparison with the conditional
distribution from the LLM. At each position, new tokens are
sampled and compared to the draft tokens. If a sampled token
corresponds to the draft token, it is approved; otherwise, the
draft is discarded from that point. This selective approval
ensures that the output sequence aligns with what would be
produced by a typical autoregressive process, thus upholding
the authenticity of the generated text.
Theoretical Insight: Why ADED usesMCTS
In this section, we provide a theoretical justification for the
design of the ADED method. We show that the draft search
inADED using MCTS can be viewed as a form of policy
optimization, while the inference mechanism of LLMs can
be viewed as a similar form of penalty optimization.
MCTS inADED : The token selection procedure in ADED
decoding can be viewed as an action selection process. The
MCTS algorithm optimizes its policy by iteratively build-
ing a search tree and updating visit counts for each node
(state-action pair) based on the search paths. The visit count
distribution ˆπ(a|x)is defined as:
ˆπ(a|x)≜1 +n(x, a)
|A|+P
bn(x, b), (4)
where n(x, a)represents the visit count for action ain state
x, and|A|represents the total number of possible actions atstatex. Then, the action selection in MCTS can be written as
selecting the action a∗:
a∗(x)≜arg max
a[Q(x, a) +λN·πθ(a|x)
ˆπ(a|x)] (5)
Following (Grill et al. 2020), we use q∈ R|A|to denote the
vector of Q-function Q(x, a). With proper choice of hyper-
parameters, the MCTS algorithm can be viewed as search-
ing for the optimum solution to a policy optimization prob-
lem (Grill et al. 2020) as below:
¯π≜arg max
y∈S
q⊤y−λNKL[πθ, y]
, (6)
where Sis the|A|-dimensional simplex, λNis a regulariza-
tion parameter that depends on hyperparameters and balances
exploration and exploitation, and KL is the KL-divergence.
LLM Inference Mechanism : Large language models, par-
ticularly those based on the Transformer architecture, gener-
ate text by predicting the probability distribution of the next
token given the previous tokens. During training, the model
maximizes the log-likelihood of the observed data, which is
equivalent to minimizing the cross-entropy loss:
L(θ) =−TX
t=1logP(wt|w1:t−1;θ) +λ
2∥θ∥2
2,(7)
where Pdenotes the conditional probability of LLM, wde-
notes the tokens, and θdenotes the model parameters.
Comparative Analysis : As shown in Eq (6) and Eq. (7),
bothMCTS and LLMs can be viewed as regularized optimiza-
tion problems for selecting the distribution of the next tokens.
On the one hand, the Q-function in MCTS forADED can be
viewed as an approximation to the log-likelihood of LLMs:
Q(x, a) =−TX
t=2logˆP(wt|wt−1, wt−2;θ)
≈logP(w0, w1,···, wT;θ)
=−TX
t=2logP(wt|w1:t−1;θ), (8)
where ˆPandPare the conditional probability distribution
from tri-gram-matrix-based LLM representative and LLMs,
respectively. On the other hand, both MCTS and LLMs em-
ploy regularization to improve the optimization procedure.
As a result, we verify the similarities between MCTS and
LLM Inference in terms of optimization and regularization.
Experiments
Experimental Setup
Models and Datasets. We conduct a series of experiments
with five distinct models on three datasets to evaluate the
efficacy of ADED . In particular, We use three Vicuna mod-
els (Chiang et al. 2023) (7B, 13B, 33B) and two LLaMA2-
chat models (Touvron et al. 2023b) (7B, 13B) to evaluate
the acceleration capabilities across different model sizes and
types. Our assessment incorporates the HumanEval (Chen

--- PAGE 5 ---
Table 1: Latency and Average Accept Length Comparison between ADED and Baselines. In most test cases, ADED has the lowest
latency, longer accept length, and higher efficiency.
Latency Average Accept Length
Benchmark Model REST REST Single Lookahead Autoregressive ADED REST REST Single Lookahead ADED
MT-BenchVicuna-7B 16.31 17.36 18.93 24.77 12.95 1.97 1.98 1.89 2.42
Vicuna-13B 25.43 25.99 32.73 44.07 22.94 1.98 1.99 1.85 2.39
Vicuna-33B 28.63 28.62 40.53 52.97 24.96 1.95 1.96 1.83 2.29
Llama2-7B 16.08 17.67 18.84 25.58 13.85 1.96 1.95 1.96 2.30
Llama2-13B 27.13 29.80 31.24 44.76 25.13 1.95 1.95 1.96 2.32
AlpacaVicuna-7B 14.24 14.58 18.73 24.49 12.81 2.22 2.22 1.89 2.33
Vicuna-13B 22.94 23.01 32.60 43.60 24.06 2.21 2.21 1.86 2.26
Vicuna-33B 26.03 25.89 40.58 52.52 24.62 2.11 2.12 1.82 2.21
Llama2-7B 14.13 14.87 19.28 25.38 12.90 2.21 2.20 1.97 2.37
Llama2-13B 23.66 24.07 31.18 44.04 23.57 2.15 2.13 1.96 2.32
Human EvalVicuna-7B 14.90 15.56 18.99 25.49 11.24 2.21 2.23 2.10 2.67
Vicuna-13B 20.17 20.61 27.43 45.13 19.96 2.50 2.50 2.23 2.81
Vicuna-33B 24.91 25.06 31.34 52.32 21.19 2.29 2.30 2.02 2.62
Llama2-7B 14.37 15.57 15.28 25.91 11.68 2.19 2.19 2.27 2.63
Llama2-13B 25.46 25.85 26.72 45.25 21.82 2.01 2.01 2.17 2.60
Vicuna-7B Vicuna-13B Vicuna-33B Llama-2-7b Llama-2-13b020406080T okens per Secondx1.91
x1.92x2.12x1.85
x1.78Auto Regressive
ADED
(a) Throughput and Speedup on MT-Bench
Vicuna-7B Vicuna-13B Vicuna-33B Llama-2-7b Llama-2-13b020406080T okens per Secondx1.91
x1.81 x2.13x1.97
x1.87Auto Regressive
ADED (b) Throughput and Speedup on Alpaca
Vicuna-7B Vicuna-13B Vicuna-33B Llama-2-7b Llama-2-13b020406080T okens per Secondx2.27
x2.26x2.47x2.22
x2.07Auto Regressive
ADED (c) Throughput and Speedup on Human-Eval
Figure 3: Comparison of ADED ’s throughput for different models on (a) MT-Bench, (b) Alpaca, and (c) Human-Eval. The
performance of ADED shows stable and significant improvements across different models and benchmarks.
et al. 2021), MT-Bench (Zheng et al. 2023), and Alpaca (Taori
et al. 2023) datasets to ascertain general natural language un-
derstanding and generation competencies. These datasets are
meticulously chosen to guarantee a comprehensive analysis
of the acceleration techniques across various tasks.
Corpus. We construct two corpora. The first one is built
using a portion of the Python pre-training code from The
Stack (Kocetkov et al. 2022), comprising about 2.7M Python
code samples with a resulting size of 1007MB. The second
is constructed using data derived from UltraChat (Ding et al.
2023), consisting of around 774K ChatGPT conversations,
producing a corpus with a size of 574MB. The experiments
on the MT-Bench and Alpaca are conducted using the Ultra-
Chat corpus, while the Human-Eval benchmark utilize the
corpus from The Stack.
Metrics. To assess the acceleration performance on large
language models, we use two main metrics: speedup ratio
and average acceptance length. Speedup ratio, calculated as
the ratio of the time required by the baseline models to com-
plete inference tasks without acceleration to the time required
by our ADED , measures the efficiency gains introduced by
the algorithm. The second metric, average acceptance length,
measures the average number of tokens accepted per forward
pass by the target large language models, excluding any over-
head of retrieving and constructing draft tokens, indicating
the maximum possible acceleration.
Baselines. We compare various foundational approaches
to improve the decoding speed of large language models. Weexamine Lookahead Decoding (Fu et al. 2024a), a precise
and parallel decoding algorithm that cuts down latency with-
out relying on draft models. We compare REST (He et al.
2024) (Retrieval-Based Speculative Decoding), which adopts
a retrieval-based strategy to create draft tokens, in contrast
to conventional speculative decoding methods that rely on
a draft model.For fairness in comparison, we include REST
Single , a single-threaded version of REST, to evaluate per-
formance under constrained processing conditions. We also
include the traditional Autoregressive method, which repre-
sents the standard decoding approach, serving as a baseline
to highlight the improvements offered by the other methods.
All experiments are conducted on NVIDIA A6000 GPUs,
except for the 33B model, which utilizes an NVIDIA H100.
The experiments default to Greedy sampling.
Configuration of ADED .To ensure reproducibility, we
provide detailed hyperparameters, the corpus used, and the
runtime environment for each set of experiments in Appendix
F. This allows interested researchers to replicate our findings
accurately and compare them against their setups. Due to
space constraints, these details are not included in the main
text but are comprehensively documented in the appendix.
Main Results
In the experiments, we compare the efficacy of different
baselines applied to various models, utilizing three datasets:
MT-Bench, Human-Eval, and Alpaca. We focus on metrics
of Accept Length, Latency, and Speedup Ratio. Table 1 sum-

--- PAGE 6 ---
0 2000 4000 6000 8000 10000 12000 14000
T okens1.01.52.02.53.03.54.04.5Average Accept LengthWith Adaptive
Without Adaptive(a) Effect of the adaptive strategy
WritingRoleplayReasoning
Math
Coding
Extraction
STEMHumanities2.642.242.21
2.41
2.69
2.17
2.332.402.64(b) Stability amount differ-
ent tasks
Figure 4: (4a) Adaptive Strategy comparison on MTBench:
Performance of Vicuna-7B model with and without the adap-
tive strategy on the MT-Bench dataset, showing the advantage
of using the adaptive approach. (4b) Average Accept Length
for different tasks on MT-Bench, demonstrating that ADED
consistently performs well across tasks.
marizes the latency and average accept length on the three
datasets. ADED consistently demonstrates lower latency, par-
ticularly for the vicuna-7B and llama2-13B models. For in-
stance, on MT-Bench, ADED achieves a latency of 12.95
ms for vicuna-7B, which is lower than REST (16.31 ms),
REST Single Thread (17.36 ms), and Lookahead (18.93
ms). Notably, the memory required for ADED (574MB)
is only 5.6% of that required for REST (12GB). Accord-
ing to Table 2, even when ADED uses a smaller corpus
(253MB, which is just 2.5% of REST’s requirements), it
still achieves lower latency than REST . This trend is also
observed on Alpaca, where ADED achieves a latency of 12.81
ms for vicuna-7B, compared to 14.24 ms for REST, 14.58
ms for REST Single Thread, and 18.73 ms for Lookahead .
The accept length results in Table 1 indicate the quality
of the generated outputs, with longer accept lengths sug-
gesting more coherent and contextually relevant text. Our
method, ADED , outperforms other methods across different
models on both MT-Bench and Alpaca datasets. For example,
on MT-Bench, ADED achieves the highest accept length for
vicuna-33B and llama2-13B models, showcasing its superior
language generation capabilities.
Speedup ratio are used to evaluate the efficiency. ADED
consistently shows a significant improvement in speed up
across all datasets in Figure 3. This efficiency is noticeable
on the MT-Bench, Alpaca and Human-Eval datasets, where
ADED not only reduces latency but also enhances the overall
processing speed. For instance, ADED achieves a speedup
of 1.92x on MT-Bench with the vicuna-13B model, outper-
forming REST ,REST Single Thread, and Lookahead . On
the HumanEval dataset, the vicuna-33B model, for example,
demonstrates a speedup of nearly 2.5x when using ADED .
Stability of ADED
In this section, we analyze the stability of ADED across dif-
ferent categories of tasks. The categories include writing,
roleplay, reasoning, math, coding, extraction, STEM, and
humanities. The experimental results shown in Figure 4b
indicate that ADED maintains consistent performance across
categories. The average accept length remains stable, demon-
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
T op-P2.222.242.262.282.302.32Average Accept Lengths
Average Accept Lengths(a) Top-p
0.00.10.20.30.40.50.60.70.80.9
T emperature2.222.242.262.282.302.32Average Accept Lengths
Average Accept Lengths (b) Temperature
Figure 5: Sensitivity analysis of ADED on (5a) top-p and (5b)
temperature parameters.
strating that ADED can effectively handle a diverse range of
tasks without significant variations in performance.
To further evaluate the robustness of ADED , we examine
the effects of varying the top-p and temperature parameters
on the performance. Figures 5a and 5b summarize the impact
of these parameters on the average accept length. In particular,
Figure 5a shows that changes of top-p do not affect the per-
formance of ADED significantly. The average accept length
remains relatively stable across different values of top-p, in-
dicating that ADED is not overly sensitive to this parameter.
Figure 5b demonstrates that variations in the temperature pa-
rameter have negligible impact on the performance of ADED .
The consistency in the average accept length across different
temperature values further supports the robustness.
Ablation Study
To gain insight into our method, we conduct a series of abla-
tion studies. Full studies are summarized in Appendix D .
Effect of the adaptive strategy. Figure 4a illustrates the
performance of our adaptive strategy on Vicuna-7B, with
the analysis of average accept lengths over varying token
counts. We find that the adaptive strategy maintains a higher
average accept length over the entire range compared to the
non-adaptive strategy. The adaptive strategy’s success is at-
tributed to its dynamic adjustment of the model’s probability
distributions based on the tri-gram frequencies from prior out-
puts. This allows the model to better manage longer contexts
and maintain relevance, enhancing stability and coherence in
longer interactions.
Effect of the corpus size. Table 2 shows the impact of the
increase in corpus size from 121k to 774k on various perfor-
mance metrics. With the expansion of the corpus, there is a
gradual improvement in the Accept Length from 2.30 to 2.42.
This increase suggests that larger datasets provide a broader
array of language patterns, which enhances the model’s abil-
ity to generate more coherent and contextually relevant out-
puts. Despite the increase in data size from 253 MB to 574
MB, the system maintains efficient data processing capability.
The small differences in latency affirm ADED ’s consistent
performance, even with smaller corpus sizes, which further
extends its potential for use on resource-constrained devices.
The modest rise in retrieval time underscores the efficiency
of the retrieval algorithms, which can manage larger datasets
without significantly compromising response speed. In sum-
mary, the results show that larger corpus sizes can improve
the quality of the model’s output while maintaining good
system performance.

--- PAGE 7 ---
Table 2: Effect of Corpus Size.
Corpus Size Corpus Size Latency Accept Length Speed up
121k 253 MB 13.09 ms 2.30 1.89
774k 574 MB 12.95 ms 2.42 1.93
Effect of MCTS .Figure 6 presents the results for Vicuna-
7B and Vicuna-13B models on the MT-Bench dataset, show-
ing the impact of different MCTS search counts on perfor-
mance. We find that for both models, increasing the number
of searches improves performance, while the optimal number
varies by model size. The average exact length and latency
are plotted against the number of searches, illustrating the
trade-off between performance and computational cost. As
shown in the figure, there is a notable improvement in the av-
erage exact length as the number of searches increases, along
with an increase in latency, indicating a balance between
search depth and generation time. We further compare greedy
search with MCTS (full traversal is not feasible due to the
huge search space) while keeping the number of 150 search
iterations constant. The results show that the average accept
length of the greedy search is only 1.493, significantly lower
than that obtained by MCTS, demonstrating the superiority
of MCTS in efficiently managing the vast decision spaces.
50 100 150 200 250 300 350
MCTS Search Counts2.2252.2502.2752.3002.3252.3502.375Average Accept Length
13.013.113.213.313.413.5
Latency (ms)
Average Accept Length Latency
(a) Vicuna-7B
50 100 150 200 250 300 350
MCTS Search Counts2.2002.2252.2502.2752.3002.325Average Accept Length
24.024.525.025.5
Latency (ms)
Average Accept Length Latency (b) Vicuna-13B
Figure 6: Comparison of Search Counts and Performance on
MT-Bench.
Effect of N-gram Model Choice. Our studies extensively
evaluate the impact of different n-gram configurations on
decoding performance. In tests conducted on the MT-Bench
dataset using the Vicuna-7B model, bi-grams and 4-grams
result in accept lengths of 1.80 and 1.82, respectively. These
results are significantly lower compared to the 2.30 accept
length achieved with tri-grams. Bi-gram models demonstrate
limited capability in effectively utilizing contextual informa-
tion, often leading to outputs that appear more random and
less coherent. Conversely, 4-grams exhibit overly determinis-
tic behavior, constraining the diversity of the generated text
due to their restrictive nature in capturing extensive prior con-
text. Tri-grams strike an optimal balance, providing enough
contextual depth to enhance the coherence and relevance
of outputs while still allowing for sufficient variability and
diversity in text generation. This balance makes tri-grams
particularly effective in large language model decoding, as
they encapsulate the ideal compromise between randomness
and contextual awareness.Related Work
A number of research efforts on decoding strategies for large
language models have used draft models to improve de-
coding efficiency. Techniques such as Speculative Decod-
ing (Leviathan, Kalman, and Matias 2023; Spector and Re
2023; Chen et al. 2023; Stern, Shazeer, and Uszkoreit 2018),
Madusa (Cai et al. 2024), Eagle (Li et al. 2024b), various
other approaches requiring draft models (Zhang et al. 2024;
Liu et al. 2023b; Kim et al. 2024; Fu et al. 2024b) fall into
this category, utilizing models to generate drafts. Specifically,
Speculative Decoding uses an advanced sampling technique
where the auxiliary model generates a set of potential token
sequences, and the primary model selects the most sequences,
resulting in a good balance between speed and accuracy. Al-
though these methods primarily aim to enhance the accuracy
of generated texts and significantly accelerate the response
time during initial text generation, their adoption comes with
drawbacks. The primary issue is the necessity for additional
training specific to the draft models, which could be resource-
intensive. Moreover, these techniques generally depend on
GPU resources (Kwon et al. 2023; Sheng et al. 2023; Park
et al. 2024) for inference, potentially limiting their applica-
tion in environments where such hardware is unavailable or
when operating under strict resource constraints.
A significant portion of recent advances has focused on im-
proving efficiency without relying on draft models (Fu et al.
2024a; He et al. 2024). Two notable approaches in this realm
areLookahead decoding (Fu et al. 2024a) and Retrieval-
Based Speculative Decoding (REST) (He et al. 2024). Looka-
head decoding is an approach that enhances the efficiency of
the decoding process through the prediction of subsequent
tokens via Jacobi Iteration (Sleijpen and Van der V orst 2000).
It employs a heuristic to estimate the future cost of a se-
quence without the need to explicitly create a draft. REST
introduces a retrieval-enhanced generation model that specu-
latively decodes sequences without the need for producing
preliminary drafts. It instead searches and prioritizes possible
continuations from an already established sequence database.
However, these methods exhibit lower accuracy and greater
resource use compared to our approach. They demand more
memory and GPU processing power, posing challenges in
resource-scarce settings.
Conclusion
ADED improves the LLM decoding process by introducing
adaptability and efficiency, significantly reducing latency and
computational demands. This method achieves up to a 2.5X
speedup in decoding and a 20% improvement in acceptance
rates, outperforming traditional techniques. Unlike existing
approaches, ADED dynamically adjusts the draft distribution
using a tri-gram matrix and enhances draft quality through
MCTS , eliminating the need for fine-tuning. The continuous
feedback loop ensures ongoing improvements in draft gener-
ation. While ADED demonstrates robust performance across
various benchmarks, future work will focus on exploring its
application in more diverse real-world scenarios. Addition-
ally, addressing potential limitations in extremely large-scale
deployments will be a priority.

--- PAGE 8 ---
References
Antoniol, G.; Brugnara, F.; Cettolo, M.; and Federico, M.
1994. Language model estimations and representations for
real-time continuous speech recognition. In ICSLP , 859–862.
Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,
A.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,
T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,
C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;
Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.;
Sutskever, I.; and Amodei, D. 2020. Language Models are
Few-Shot Learners. arXiv:2005.14165.
Browne, C. B.; Powley, E.; Whitehouse, D.; Lucas, S. M.;
Cowling, P. I.; Rohlfshagen, P.; Tavener, S.; Perez, D.;
Samothrakis, S.; and Colton, S. 2012. A survey of monte
carlo tree search methods. IEEE Transactions on Computa-
tional Intelligence and AI in games , 4(1): 1–43.
Cai, T.; Li, Y .; Geng, Z.; Peng, H.; Lee, J. D.; Chen, D.; and
Dao, T. 2024. Medusa: Simple LLM Inference Acceleration
Framework with Multiple Decoding Heads. arXiv preprint
arXiv: 2401.10774 .
Chang, J. D.; Brantley, K.; Ramamurthy, R.; Misra, D.; and
Sun, W. 2023. Learning to generate better than your llm.
arXiv preprint arXiv:2306.11816 .
Chen, C.; Borgeaud, S.; Irving, G.; Lespiau, J.-B.; Sifre, L.;
and Jumper, J. 2023. Accelerating Large Language Model
Decoding with Speculative Sampling. arXiv:2302.01318.
Chen, M.; Tworek, J.; Jun, H.; Yuan, Q.; de Oliveira Pinto,
H. P.; Kaplan, J.; Edwards, H.; Burda, Y .; Joseph, N.; Brock-
man, G.; Ray, A.; Puri, R.; Krueger, G.; Petrov, M.; Khlaaf,
H.; Sastry, G.; Mishkin, P.; Chan, B.; Gray, S.; Ryder, N.;
Pavlov, M.; Power, A.; Kaiser, L.; Bavarian, M.; Winter, C.;
Tillet, P.; Such, F. P.; Cummings, D.; Plappert, M.; Chantzis,
F.; Barnes, E.; Herbert-V oss, A.; Guss, W. H.; Nichol, A.;
Paino, A.; Tezak, N.; Tang, J.; Babuschkin, I.; Balaji, S.;
Jain, S.; Saunders, W.; Hesse, C.; Carr, A. N.; Leike, J.;
Achiam, J.; Misra, V .; Morikawa, E.; Radford, A.; Knight,
M.; Brundage, M.; Murati, M.; Mayer, K.; Welinder, P.; Mc-
Grew, B.; Amodei, D.; McCandlish, S.; Sutskever, I.; and
Zaremba, W. 2021. Evaluating Large Language Models
Trained on Code.
Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang, H.;
Zheng, L.; Zhuang, S.; Zhuang, Y .; Gonzalez, J. E.; Stoica,
I.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot
Impressing GPT-4 with 90%* ChatGPT Quality.
Coulom, R. 2007. Efficient Selectivity and Backup Opera-
tors in Monte-Carlo Tree Search. In van den Herik, H. J.;
Ciancarini, P.; and Donkers, H. H. L. M. J., eds., Comput-
ers and Games , 72–83. Berlin, Heidelberg: Springer Berlin
Heidelberg. ISBN 978-3-540-75538-8.
Ding, N.; Chen, Y .; Xu, B.; Qin, Y .; Zheng, Z.; Hu, S.; Liu,
Z.; Sun, M.; and Zhou, B. 2023. Enhancing Chat Language
Models by Scaling High-quality Instructional Conversations.
arXiv preprint arXiv:2305.14233 .
Fu, Y .; Bailis, P.; Stoica, I.; and Zhang, H. 2024a. Break the
Sequential Dependency of LLM Inference Using Lookahead
Decoding. arXiv:2402.02057.Fu, Y .; Bailis, P.; Stoica, I.; and Zhang, H. 2024b. Break
the sequential dependency of llm inference using lookahead
decoding. arXiv preprint arXiv:2402.02057 .
Grill, J.; Altché, F.; Tang, Y .; Hubert, T.; Valko, M.;
Antonoglou, I.; and Munos, R. 2020. Monte-Carlo
Tree Search as Regularized Policy Optimization. CoRR ,
abs/2007.12509.
He, Z.; Zhong, Z.; Cai, T.; Lee, J.; and He, D. 2024. REST:
Retrieval-Based Speculative Decoding. In Duh, K.; Gomez,
H.; and Bethard, S., eds., Proceedings of the 2024 Confer-
ence of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies
(Volume 1: Long Papers) , 1582–1595. Mexico City, Mexico:
Association for Computational Linguistics.
Hendy, A.; Abdelrehim, M.; Sharaf, A.; Raunak, V .; Gabr,
M.; Matsushita, H.; Kim, Y . J.; Afify, M.; and Awadalla, H. H.
2023. How Good Are GPT Models at Machine Translation?
A Comprehensive Evaluation. arXiv:2302.09210.
James, S.; Konidaris, G.; and Rosman, B. 2017. An analysis
of monte carlo tree search. In Proceedings of the AAAI
Conference on Artificial Intelligence , volume 31.
Kim, S.; Mangalam, K.; Moon, S.; Malik, J.; Mahoney,
M. W.; Gholami, A.; and Keutzer, K. 2024. Speculative
decoding with big little decoder. Advances in Neural Infor-
mation Processing Systems , 36.
Kocetkov, D.; Li, R.; Allal, L. B.; Li, J.; Mou, C.; Ferrandis,
C. M.; Jernite, Y .; Mitchell, M.; Hughes, S.; Wolf, T.; Bah-
danau, D.; von Werra, L.; and de Vries, H. 2022. The Stack: 3
TB of permissively licensed source code. arXiv:2211.15533.
Kwon, W.; Li, Z.; Zhuang, S.; Sheng, Y .; Zheng, L.; Yu,
C. H.; Gonzalez, J.; Zhang, H.; and Stoica, I. 2023. Efficient
memory management for large language model serving with
pagedattention. In Proceedings of the 29th Symposium on
Operating Systems Principles , 611–626.
Leviathan, Y .; Kalman, M.; and Matias, Y . 2023. Fast
Inference from Transformers via Speculative Decoding.
arXiv:2211.17192.
Li, J.; Tang, T.; Zhao, W. X.; Nie, J.-Y .; and Wen, J.-R. 2024a.
Pre-trained Language Models for Text Generation: A Survey.
ACM Computing Surveys , 56(9): 1–39.
Li, Y .; Wei, F.; Zhang, C.; and Zhang, H. 2024b. EAGLE:
Speculative Sampling Requires Rethinking Feature Uncer-
tainty. arXiv preprint arXiv:2401.15077 .
Liu, J.; Yu, C.; Gao, J.; Xie, Y .; Liao, Q.; Wu, Y .; and
Wang, Y . 2023a. Llm-powered hierarchical language
agent for real-time human-ai coordination. arXiv preprint
arXiv:2312.15224 .
Liu, X.; Hu, L.; Bailis, P.; Stoica, I.; Deng, Z.; Cheung,
A.; and Zhang, H. 2023b. Online Speculative Decoding.
arXiv:2310.07177.
Mandvikar, S. 2023. Factors to Consider When Selecting
a Large Language Model: A Comparative Analysis. Inter-
national Journal of Intelligent Automation and Computing ,
6(3): 37–40.
Miao, X.; Oliaro, G.; Zhang, Z.; Cheng, X.; Wang, Z.; Wong,
R. Y . Y .; Chen, Z.; Arfeen, D.; Abhyankar, R.; and Jia,

--- PAGE 9 ---
Z. 2023. Specinfer: Accelerating generative llm serving
with speculative inference and token tree verification. arXiv
preprint arXiv:2305.09781 , 1(2): 4.
Mori, S.; Nishimura, M.; and Itoh, N. 1998. Word clustering
for a word bi-gram model. In ICSLP . Citeseer.
Moslem, Y .; Haque, R.; Kelleher, J. D.; and Way, A. 2023.
Adaptive Machine Translation with Large Language Models.
arXiv:2301.13294.
Park, Y .; Hyun, J.; Cho, S.; Sim, B.; and Lee, J. W. 2024.
Any-Precision LLM: Low-Cost Deployment of Multiple,
Different-Sized LLMs. arXiv preprint arXiv:2402.10517 .
Peng, C.; Yang, X.; Chen, A.; Smith, K. E.; PourNejatian, N.;
Costa, A. B.; Martin, C.; Flores, M. G.; Zhang, Y .; Magoc,
T.; et al. 2023. A study of generative large language model
for medical research and healthcare. NPJ Digital Medicine ,
6(1): 210.
Rosin, C. D. 2011. Multi-armed bandits with episode context.
Annals of Mathematics and Artificial Intelligence , 61: 203–
230.
Saka, A. B.; Oyedele, L. O.; Akanbi, L. A.; Ganiyu, S. A.;
Chan, D. W.; and Bello, S. A. 2023. Conversational artifi-
cial intelligence in the AEC industry: A review of present
status, challenges and opportunities. Advanced Engineering
Informatics , 55: 101869.
Shanahan, M. 2024. Talking about large language models.
Communications of the ACM , 67(2): 68–79.
Sheng, Y .; Zheng, L.; Yuan, B.; Li, Z.; Ryabinin, M.; Chen,
B.; Liang, P.; Ré, C.; Stoica, I.; and Zhang, C. 2023. Flex-
gen: High-throughput generative inference of large language
models with a single gpu. In International Conference on
Machine Learning , 31094–31116. PMLR.
Silver, D.; Schrittwieser, J.; Simonyan, K.; Antonoglou, I.;
Huang, A.; Guez, A.; Hubert, T.; Baker, L.; Lai, M.; Bolton,
A.; et al. 2017. Mastering the game of go without human
knowledge. nature , 550(7676): 354–359.
Sleijpen, G. L.; and Van der V orst, H. A. 2000. A Jacobi–
Davidson iteration method for linear eigenvalue problems.
SIAM review , 42(2): 267–293.
Spector, B.; and Re, C. 2023. Accelerating LLM Inference
with Staged Speculative Decoding. arXiv:2308.04623.
Stern, M.; Shazeer, N.; and Uszkoreit, J. 2018. Block-
wise Parallel Decoding for Deep Autoregressive Models.
arXiv:1811.03115.
´Swiechowski, M.; Godlewski, K.; Sawicki, B.; and Ma ´ndz-
iuk, J. 2023. Monte Carlo tree search: A review of recent
modifications and applications. Artificial Intelligence Review ,
56(3): 2497–2562.
Taori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y .; Li, X.;
Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stan-
ford Alpaca: An Instruction-following LLaMA model. https:
//github.com/tatsu-lab/stanford_alpaca.
Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,
M.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.;
Azhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample,
G. 2023a. LLaMA: Open and Efficient Foundation Language
Models. arXiv:2302.13971.Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;
Babaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,
S.; Bikel, D.; Blecher, L.; Ferrer, C. C.; Chen, M.; Cucurull,
G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller, B.; Gao,
C.; Goswami, V .; Goyal, N.; Hartshorn, A.; Hosseini, S.; Hou,
R.; Inan, H.; Kardas, M.; Kerkez, V .; Khabsa, M.; Kloumann,
I.; Korenev, A.; Koura, P. S.; Lachaux, M.-A.; Lavril, T.; Lee,
J.; Liskovich, D.; Lu, Y .; Mao, Y .; Martinet, X.; Mihaylov,
T.; Mishra, P.; Molybog, I.; Nie, Y .; Poulton, A.; Reizenstein,
J.; Rungta, R.; Saladi, K.; Schelten, A.; Silva, R.; Smith,
E. M.; Subramanian, R.; Tan, X. E.; Tang, B.; Taylor, R.;
Williams, A.; Kuan, J. X.; Xu, P.; Yan, Z.; Zarov, I.; Zhang, Y .;
Fan, A.; Kambadur, M.; Narang, S.; Rodriguez, A.; Stojnic,
R.; Edunov, S.; and Scialom, T. 2023b. Llama 2: Open
Foundation and Fine-Tuned Chat Models. arXiv:2307.09288.
Wu, Q.; Bansal, G.; Zhang, J.; Wu, Y .; Li, B.; Zhu, E.; Jiang,
L.; Zhang, X.; Zhang, S.; Liu, J.; Awadallah, A. H.; White,
R. W.; Burger, D.; and Wang, C. 2023. AutoGen: Enabling
Next-Gen LLM Applications via Multi-Agent Conversation.
arXiv:2308.08155.
Zhang, A.; Wang, C.; Wang, Y .; Zhang, X.; and Cheng, Y .
2024. Recurrent drafter for fast speculative decoding in large
language models. arXiv preprint arXiv:2403.09919 .
Zhang, B.; Haddow, B.; and Birch, A. 2023. Prompting
large language model for machine translation: A case study.
InInternational Conference on Machine Learning , 41092–
41110. PMLR.
Zhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.; Chen,
S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V .; Mihaylov, T.; Ott,
M.; Shleifer, S.; Shuster, K.; Simig, D.; Koura, P. S.; Sridhar,
A.; Wang, T.; and Zettlemoyer, L. 2022. OPT: Open Pre-
trained Transformer Language Models. arXiv:2205.01068.
Zheng, L.; Chiang, W.-L.; Sheng, Y .; Zhuang, S.; Wu, Z.;
Zhuang, Y .; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.; Zhang, H.;
Gonzalez, J. E.; and Stoica, I. 2023. Judging LLM-as-a-Judge
with MT-Bench and Chatbot Arena. arXiv:2306.05685.

--- PAGE 10 ---
Appendix
Advantages on Computation Efficiency
Our technique provides substantial benefits when imple-
mented on edge devices like laptops and smartphones, which
often face limitations in GPU capabilities and memory. In
contrast to traditional decoding methods that depend heavily
on GPU power or large memory sizes, our strategy is crafted
for high efficiency with low resource demands.
Reduced GPU Requirements Our approach, which does
not require fine-tuning and utilizes a lightweight probabilis-
tic model, primarily operates on the CPU, eliminating the
need for substantial GPU resources. This feature is especially
advantageous for edge devices with limited GPU access. By
minimizing GPU dependency, our technique can be applied
more widely, enhancing LLM decoding across a broader
array of devices.
Low Memory Usage Our method avoids the need for
bulky initial models or intricate neural network architectures,
considerably lowering the memory usage typically needed
for LLM decoding. This aspect is particularly suitable for
devices with limited memory, such as budget laptops and
mobile phones. The decrease in memory usage not only leads
to quicker processing times but also reduces power consump-
tion, which is vital for devices running on batteries. Com-
pared to REST, which also requires a corpus, our method
significantly reduces memory usage; for instance, both using
the Stack dataset, our method requires only less than 1GB
while REST needs 27GB.
Ultimately, our decoding method is exceptionally apt for
practical use in edge systems, where there is often a scarcity
of computational resources. It offers a viable and effective op-
tion for improving LLM decoding without sacrificing speed
or precision, thus bringing sophisticated language processing
to less powerful devices.
Broader Impacts
The advancements presented in this paper, specifically the
accelerated LLM decoding via Monte Carlo Tree Search
(MCTS ) and self-evolving speculation, have several broader
impacts worth discussing. These impacts span multiple do-
mains including technology, society, and ethics.
Technological Impact
Our method significantly enhances the efficiency and speed
of autoregressive LLM decoding. This improvement can ben-
efit numerous applications that rely on real-time language
processing, such as interactive chatbots, automated customer
service, and real-time translation systems. By reducing the
computational load and memory requirements, our approach
also makes it feasible to deploy advanced LLMs on edge
devices like smartphones and IoT devices, broadening their
accessibility and usability.
Societal Impact
The ability to perform faster and more efficient language
model decoding can have a profound impact on society. For
instance, it can improve the responsiveness and accuracy ofassistive technologies for individuals with disabilities, such
as voice-controlled assistants and text-to-speech systems. Ad-
ditionally, educational tools that rely on real-time feedback
and interactive learning can benefit from quicker and more
reliable LLM responses, enhancing the learning experience
for students.
Ethical Considerations
While our advancements offer significant benefits, they also
raise important ethical considerations. The increased effi-
ciency of LLMs could lead to more widespread use of auto-
mated systems, which might replace human jobs in certain
sectors. It is crucial to address the potential displacement of
workers by fostering skills development and creating new job
opportunities that leverage human-LLM collaboration.
Moreover, the deployment of more powerful LLMs on a
wider scale necessitates robust measures to mitigate misuse.
Enhanced LLM capabilities could be exploited for malicious
purposes, such as generating misleading information or deep-
fake content. Therefore, it is essential to implement strong
ethical guidelines and monitoring mechanisms to prevent
abuse and ensure that the technology is used responsibly.
Environmental Impact
Improving the efficiency of LLM decoding can also con-
tribute to environmental sustainability. By reducing the
computational resources required for LLM operations, our
method decreases the energy consumption associated with
running these models. This reduction is particularly impor-
tant given the growing concerns about the environmental
footprint of large-scale AI systems. Our approach aligns with
the broader goal of developing greener AI technologies that
minimize their impact on the planet.
In summary, the proposed method for accelerating LLM
decoding has far-reaching implications across various do-
mains. While it offers substantial benefits, it is essential to
address the accompanying ethical, societal, and environmen-
tal challenges to ensure that the technology is developed and
deployed in a responsible and beneficial manner.
Limitations
TheADED methodology, while effective in accelerating LLM
decoding, presents certain limitations primarily concerning
accuracy and efficiency. Firstly, the retrieval-based approach
integral to ADED , although less resource-intensive, does not
yet match the accuracy levels of traditional draft model meth-
ods. This discrepancy highlights a potential trade-off between
resource efficiency and optimal accuracy in language model
decoding. Secondly, the use of Monte Carlo Tree Search
(MCTS ) in our adaptive draft-verification process, though
instrumental in improving decoding speed, still incurs addi-
tional computation. As a result, while ADED achieves notable
speed enhancements, the acceleration ratio remains subopti-
mal when compared to the theoretical maximum acceptance
length. Future improvements will aim to refine these aspects
to better balance speed and accuracy, potentially through
more advanced optimization of the MCTS algorithm or alter-
native adaptive strategies.

--- PAGE 11 ---
Table 3: Comparison of Different Methods.
Method Requires GPU Computation Memory Overhead
Lookahead ✓ ↑ ×
Eagle ✓ ↑ ×
Medusa ✓ ↑ ×
REST × ↓ ↑
Speculative Decoding ✓ ↑ ×
ADED × ↓ Very Low
Ablation Study
Effect of Draft Search Score
In this section, we present additional experiments to elucidate
our choice and the implications of the hyperparameters c1
andc2in the Draft Search Score formula.
Our analysis is grounded in Equation 16, which demon-
strates that the exploration-exploitation balance is modulated
byc1andc2. Specifically, c1positively correlates with the
exploration constant E, while c2inversely affects it.
0 100 200 300 400 500
C12.232.242.252.262.272.282.29Average Accept Length
Average Accept Length when C2 = 8.0
(a) Impact of c1on Acceptance
Length
0 2 4 6 8 10
1 / C22.2502.2552.2602.265Average Accept Length
Average Accept Length when C1 = 2.414(b) Impact of c2on Acceptance
Length
Figure 7: Relationship between Average Acceptance Length
and Exploration Parameters.
Figure 7 illustrates the relationship between the average
acceptance length and both c1and the reciprocal of c2(1/c2).
We observe an initial increase in performance with moderate
levels of exploration, followed by a decline as the param-
eters extend to either extreme. This pattern suggests that
both insufficient and excessive exploration can detrimentally
affect model performance. This pattern indicates that both
insufficient and excessive exploration levels impact the algo-
rithm’s performance, highlighting the importance of careful
adjustment of c1andc2.
Summary
In Experiment section, we conduct ablation studies to explore
the impact of various configurations and optimizations in
ADED .
The adaptive strategy shows significant benefits across dif-
ferent models, notably improving accept lengths by dynam-
ically adjusting probability distributions based on tri-gram
frequencies.
Our examination of the effects of corpus size indicates that
larger datasets contribute to improved accuracy, enhancing
the coherence and quality of the output. However, even with
smaller corpus sizes, our system demonstrates a notable in-
crease in processing speed, underscoring its efficiency andcapability to deliver accelerated performance without com-
promising on resource constraints.
The effectiveness of the Monte Carlo Tree Search ( MCTS )
is analyzed through varying search counts, demonstrating that
increased iterations enhance performance while balancing
computational costs.
Additionally, comparisons between different n-gram mod-
els indicate that tri-grams achieve an optimal balance of ran-
domness and contextual relevance, significantly outperform-
ing other configurations in terms of decoding performance.
These studies collectively affirm the critical design choices
made in ADED , validating its robustness and efficiency in
language model decoding.
Preliminary
Retrieval-Based Speculative Decoding: Decoding in large
language models describes the procedure of text generation
by sequentially predicting tokens. Given a context sequence
s= (x1, ..., x t−1, xt), conventional autoregressive decoding
techniques produce the subsequent token at position t+ 1
using conditional probability:
xt+1∼p(x|x1, ..., x t;θlarge), (9)
where pdenotes the conditional probability distribution cal-
culated by the LLM with parameters θlarge. To reduce these
computational burdens during inference, speculative decod-
ing is proposed (Leviathan, Kalman, and Matias 2023). It
reduces the frequency of forward passes using θlargeby incor-
porating an auxiliary language model with fewer parameters
θsmall.
The speculative decoding process is implemented itera-
tively as follows: Using the smaller model θsmall, a draft
sequence of the next mtokens is generated autoregressively:
˜xt+i∼p(x|s,˜xt+1, ...,˜xt+i−1;θsmall), (10)
where i= 1, ..., m . Despite the sequential nature of this
generation, the reduced model complexity of θsmallleads to a
lower computational overhead compared to θlarge.Retrieval-
Based Speculative Decoding extends the basic speculative
decoding framework by replacing the smaller language model
with a retrieval system. This method uses:
˜xt+i∼p(x|s,˜xt+1, ...,˜xt+i−1;θCorpus), (11)
where retrieve (x1, ..., x t)fetches contextually relevant text
segments from a pre-stored corpus, reducing reliance on
frequent recalculations with θlarge.

--- PAGE 12 ---
Monte Carlo Tree Search: Monte Carlo Tree Search
(MCTS ) (Coulom 2007) is a popular algorithmic in artificial
intelligence, which explores potential future states from a
current decision point by building a tree of possibilities, bal-
ancing exploration of new paths and exploitation of known
beneficial paths.
In the context of MCTS , each node in the tree represents
a possible state, and these nodes are expanded based on the
results of simulated plays. A key aspect of MCTS is how it
selects nodes for further exploration, which is based on the
number of visits to each node, denoted as N(s). Specifically,
the selection process in MCTS can be interpreted as an ap-
proximate solution to a regular optimization problem(Grill
et al. 2020). The node visit count N(s)acts as a regularizer,
guiding the algorithm towards a balance between exploring
less visited, uncertain nodes and exploiting nodes that are
known to yield higher rewards. The optimization problem
can be formally expressed as
max
s∈Children (s′) 
Q(s) +cs
logN(s′)
N(s)!
, (12)
where s′is the current node, srepresents a child node, Q(s)
is the estimated value of node s,N(s′)is the number of visits
to the parent node, and cis a constant that controls the trade-
off between exploration and exploitation. This formulation
highlights how MCTS inherently balances the dual objectives
of accuracy (through Q(s)) and robustness (through the regu-
larization term involving N(s)).
PUCT Score: The Probabilistic Upper Confidence Bound
applied to Trees (PUCT) score is an adaptation of the Upper
Confidence Bound (UCB) used in Monte Carlo Tree Search
to optimize the exploration-exploitation trade-off in decision-
making environments. The UCB formula is given by:
UCB =Q(s, a) +s
2 logN
N(s, a), (13)
where Q(s, a)represents the average reward of taking action
ain state s,Nis the total number of trials, and N(s, a)is
the number of trials where action awas taken from state s.
To enhance the decision-making in tree-based strategies,
the UCB was extended to Upper Confidence Bound applied
to Trees (UCT), which adjusts the exploration parameter
dynamically:
UCT =Q(s, a) +C·s
logN
N(s, a), (14)
where Cis a variable exploration coefficient, adjusting dy-
namically based on the tree’s exploration needs.
The PUCT score further incorporates probabilistic assess-
ments into the UCT framework, allowing for decisions that
consider the variance in outcome distributions:
PUCT =Q(s, a) +c·P(s, a)·s
logN(s)
1 +N(s, a),(15)
where cis a constant scaling the exploration term, and P(s, a)
represents the prior probability of selecting action ain states, enhancing the traditional exploration term by accounting
for underlying probability distributions.
In our implementation, we use a variant of the original
PUCT formula that allows for finer granularity in adjust-
ments:
E=C1+ logP
bN(s, b) +C2+ 1
C2
, (16)
PUCT (s, a) =Q(s, a) +E·P(s, a)·pP
bN(s, b)
1 +N(s, a).(17)
This adaptation allows our model to dynamically calibrate
the exploration-exploitation balance more effectively, particu-
larly in environments with large and complex decision spaces
such as those involved in LLM decoding. The parameters c1
andc2are introduced to further refine the balance between
exploration and exploitation, where c1scales the exploration
bonus and c2stabilizes the logarithmic term for numerical sta-
bility. This nuanced approach enhances the adaptability and
efficiency of our MCTS framework, providing a substantial
improvement over standard UCB and UCT methods.
Detailed Experimental Settings
Configuration of ADED
For our experiments, we use the following hyperparameters
to optimize the performance of ADED : We set t, the threshold
for the elimination of tri-gram probability, at 12 to focus only
on the most prevalent tri-grams. The number of iterations
for Monte Carlo Tree Search ( MCTS ), denoted s, is fixed at
150. The parameters in the MCTS PUCT Score function, c1
andc2, are 32.0 and 8.0, respectively, to effectively balance
exploration and exploitation. The search depth and the length
of each retrieved continuation candidate, represented by l, is
4; and the number of continuation candidates, n, is set at 24.
Experimental Settings
•For all experiments listed in Table 1, ADED uses UltraChat
as the corpus, with the same configuration as mentioned
in the previous section. All data presented in the table are
the medians of three repeated runs.
•For Figure 4a, ADED uses ShareGPT as the corpus with
c1= 2.414.
• For Figure 4b, ADED employs ShareGPT as the corpus.
•For Figure 5, ADED utilizes ShareGPT as the corpus with
c1= 2.414, and experiments are repeated seven times.
•For Table 2, ADED is tested using two different corpora:
ShareGPT (corpus size 121k) and UltraChat (corpus size
774k), with results reported as the median of three repeti-
tions.
•For Figure 5, ADED employs ShareGPT as the corpus,
setting c1= 2.414.
Configuration of Baselines
REST The REST baseline uses the default settings with the
following specific configurations:
• Number of threads: 6

--- PAGE 13 ---
• Draft choice: 64
• Datasets: UltraChat and The Stack
•Token spans: [16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3,
2]
REST Single Thread The REST Single Thread baseline
uses the same settings as REST except the Number of threads:
• Number of threads: 1
• Draft choice: 64
• Datasets: UltraChat and The Stack
•Token spans: [16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3,
2]
Lookahead The Lookahead baseline uses the default set-
tings with the following specific configurations:
• LEVEL: 5
• WIN: 15
• GUESS: 15
• FLASH: 0
Implementation Details
In this section, we describe the implementation details of
our proposed Monte Carlo Tree Search ( MCTS ) algorithm
for text generation and the dynamic adjustment of tri-gram
probabilities.Monte Carlo Tree Search for Text Generation
Algorithm 1: Monte Carlo Tree Search for Text Generation
Input :iterations — Number of iterations for MCTS ,self
— The object containing the tree structure and parameters
Output :sentences — Generated sentences ranked by their
scores
1:rng←initialize random number generator
2:foriter←1toiterations do
3:node←self.root
4:state←[node.word ]
5:end←False {Selection}
6: while notnode.untried _words is empty and
node.children exist and
notenddo
7: selected _child←node.select _child ()
8: state.append (selected _child.word )
9: node←selected _child
10: iflength of state == self.sentence _length
then
11: end←True
12: end if
13: end while {Expansion}
14: if notnode.untried _words is empty and not end
then
15: untried _words ←node.untried _words
16: p_word←node.word. 1
17: forword inuntried _words do
18: tmp_untried _words ←
get potential words from tri-gram matrix
19: child _node ←
new Node (p_word, word, tmp _untried _words )
20: node.children.append (child _node)
21: end for
22: node←node.children.last ()
23: state.append (node.word )
24: end if {Simulation}
25: while length of state < self.sentence _length do
26: last_word←state.last ()
27: next _words ←
get from tri-gram matrix using last_word
28: ifnext _words is not empty then
29: select next word based on probabilities from
next _words
30: state.append (selected word )
31: else
32: break
33: end if
34: end while {Backpropagation}
35: score←1.0
36: fori←1tolength of state−1do
37: score←score +get score from tri-gram matrix
38: end for
39: current _node←node
40: while current _node is not None do
41: current _node.visits ←current _node.visits +1
42: current _node.score ←current _node.score +
score
43: current _node←current _node.parent
44: end while
45:end for {Extract and sort top sentences}
46:sentences ←extract sentences from root
47:sortsentences by score
48:return sentences

--- PAGE 14 ---
Algorithm 1 illustrates the Monte Carlo Tree Search
(MCTS ) procedure for text generation. The procedure, RunM-
CTS, takes the number of iterations as input. The algorithm
proceeds through the following phases:
Selection: Starting from the root node, the algorithm se-
lects child nodes based on the selection criteria until a node
with untried words or no children is reached, or the end of
the sentence is detected.
Expansion: If the node has untried words and the end of
the sentence is not reached, child nodes are created for each
untried word using potential words from the tri-gram matrix.
Simulation: A sequence of words is generated starting
from the current state until the sentence reaches the desired
length or no more words can be selected from the tri-gram
matrix.
Backpropagation: The score for the generated sequence
is calculated, and this score is propagated back through the
selected nodes, updating their visit counts and scores.
Finally, the top sentences are extracted from the root node
and sorted by score.
Dynamic Adjustment of tri-gram Probabilities
Algorithm 2 details the procedure for dynamically adjusting
tri-gram probabilities. The procedure, Adjust3Gram , takes
a sequence of tokens, the maximum length of n-grams to
consider, an increment value, and the maximum probability.
For each tri-gram in the recent portion of the token se-
quence, the probability is increased by the increment value,
up to the specified maximum probability. If the tri-gram is not
already in the tri-gram matrix, it is added with the increment
value as its initial probability.
Algorithm 2: Dynamic Adjustment of tri-gram Probabilities
Input :tokens — Sequence of tokens, maxLength — Max-
imum length of tokens to consider
Parameter :increment — Value by which the probability
should be increased, maxProb — Maximum allowable prob-
ability
Output : Updated tri-gram matrix with adjusted probabili-
ties
1:n←length of tokens
2:startIndex ←max(0 , n−maxLength )
3:subTokens ←tokens [startIndex :n]
4:fori←2to length of subTokens do
5:triGram ←(subTokens [i−2], subTokens [i−
1], subTokens [i])
6: iftriGram in tri-gram matrix then
7: currentProb ←tri-gram matrix [triGram ]
8: newProb ← min(currentProb +
increment, maxProb )
9: tri-gram matrix [triGram ]←newProb
10: else
11: tri-gram matrix [triGram ]←increment
12: end if
13:end for
14:return Updated tri-gram matrixDetailed Explanation of Core Concepts and
Interactions
This section provides a detailed explanation of the relation-
ships and interactions among the core components of our
methodology: the Large Language Model (LLM), the LLM
representative tri-gram matrix, Monte Carlo Tree Search
(MCTS ), and the final output decoding. These components
are integral to our Adaptive Draft-Verification for Efficient
LLM Decoding ( ADED ) framework.
LLM Representative Tri-gram Matrix
The LLM representative tri-gram matrix is a crucial compo-
nent of our ADED framework. It serves as a condensed repre-
sentation of the LLM’s knowledge, capturing the conditional
probabilities of token sequences based on their occurrence
within the training corpus. This matrix is dynamically up-
dated during the decoding process to reflect the changing
context and to better predict subsequent tokens.
Monte Carlo Tree Search ( MCTS )
MCTS is employed to explore potential draft outputs based on
the probabilities indicated by the tri-gram matrix. It balances
exploration of new, potentially high-reward token sequences
with the exploitation of known, high-probability paths. This
ensures a diverse yet accurate set of drafts that are likely to
align closely with the true output distribution of the LLM.
Interaction Between the Tri-gram Matrix and MCTS
The tri-gram matrix provides the initial probability estimates
that guide the MCTS in selecting the most promising token
sequences during the draft generation phase. As MCTS ex-
plores different paths, it accumulates data about the success
of various sequences, which in turn feedbacks into the tri-
gram matrix. This feedback loop allows the tri-gram matrix
to evolve and adapt, improving its accuracy and the efficiency
of the draft verification process.
Final Output Decoding
Once the drafts have been generated and evaluated using
MCTS , the final output decoding phase begins. This process
involves comparing the drafts against the output distribution
of the LLM using tree attention to select the sequence that
best matches the expected outcomes. The selected draft then
becomes the final output of the model for the given input
context.
Summary
To aid in understanding, below is a conceptual diagram illus-
trating the interactions among these components. The follow-
ing Figure 8 shows the flow of data and decisions from the
initial input through the tri-gram matrix, MCTS , and finally
to the output.
Additionally, Figure 9 complements this explanation, de-
picting the validation process used to assess the accuracy of
retrieval results using tree attention.
Code — https://anonymous.4open.science/r/ADED-C7D5/

--- PAGE 15 ---
Corpus
Tri-gram 
Matrix
A1
A2
A3
MCTS 
Draft 
Maker
A4
A5
A6
A7
A3 
A4
A5
A1 
A2
A3
A2 
A3
A4
A6 
A7
B
1
1 
B
1
2 
B
1
3
A6 
A7
B
2
1 
B
2
2 
B
2
3
A6 
A7
B
3
1 
B
3
2 
B
3
3
A1 
... 
A7
B
1
1 
B
1
2 
B
1
3
B
2
1 
B
2
2 
B
2
3
B
3
1 
B
3
2 
B
3
3
Large 
Language 
Model
A8
B
1
1' 
B
1
2' 
B
1
3'
B
2
1' 
B
2
2' 
B
2
3'
B
3
1' 
B
3
2' 
B
3
3'
A8
B
1
1' 
B
1
2' 
B
1
3'
B
2
1' 
B
2
2' 
B
2
3'
B
3
1' 
B
3
2' 
B
3
3'
A1...A7 
A8 
B
3
2 
B
3
3 
B
3
3'
A1...A7 
A8 
Position:     
8       
9    
10   
11     
9   
10   
11     
9   
10   
11
Position:     
8       
9    
10   
11     
9   
10   
11     
9   
10   
11
2. 
Generate 
Tri-gram
3. 
Update 
Tri-gram
Input
1
...
4. 
Retrieval
5. 
Select 
Top-k
Position:       
1   
...   
7           
8    
9   
10      
8    
9   
10      
8     
9   
10
6
7
If 
None 
Retrieval 
Result 
Matches
If 
Retrieval 
Result 
Matches
Accept 
More 
Than 
One 
Tokens
Accept 
One 
Token
8. 
Continue 
Generation 
Until 
Finished
4. 
RetrievalFigure 8: Data Flow and Interaction Among Core Algorithm Components. This figure details the sequential data processing
steps within our ADED . Initially, the input tokens are processed to compute their tri-grams, which are then utilized to update
the tri-gram matrix. Subsequently, the updated tri-gram matrix, in conjunction with the last two tokens of the input, guides the
Monte Carlo Tree Search ( MCTS ) in retrieving potential token sequences. The retrieval results are then ranked, and the top-k
sequences are selected and appended to the original input. These extended sequences are fed into the Large Language Model
(LLM) for further processing.
A1
A2
A3
B1
B2
B3
Large 
Language 
Model
A3'
B1'
B2'
B3'
A1'
A2'
A3'
B1
A3'
A3'
B1
A3'
B1'
A3'
B1
A3'
B1'
B1'
B2
B2'
A3'
B1
A3'
B1'
B1'
B2
B2'
B2'
B3
B3'
?
=
=
=
=
=
=
Newly 
Generated 
Token(s)
Condition
Figure 9: Validation of Retrieval Results in Large Language Model.
