# Tăng Tốc Không Mất Mát cho Mô Hình Ngôn Ngữ Lớn với Đề Xuất Phân Cấp dựa trên Tính Cục Bộ Thời Gian trong Giải Mã Suy Đoán

Sukmin Cho1 Sangjin Choi1 Taeho Hwang2 Jeongyeon Seo2 Soyeong Jeong3
Huije Lee2 Hoyun Song2 Jong C. Park2 Youngjin Kwon1∗
Trường Tin học1,2 Trường Sau đại học AI3
Viện Công nghệ Tiên tiến Hàn Quốc
{smcho,sjchoi,yjkwon}@casys.kaist.ac.kr1{doubleyyh,yena.seo,starsuzi,huijelee,hysong,jongpark}@kaist.ac.kr2,3

## Tóm tắt

Tăng tốc suy luận trong Mô hình Ngôn ngữ Lớn (LLM) là quan trọng cho các tương tác thời gian thực, vì LLM đã được tích hợp rộng rãi vào các dịch vụ thực tế. Giải mã suy đoán, một giải pháp hoàn toàn thuật toán, đã thu hút sự chú ý trong việc cải thiện tốc độ suy luận bằng cách đề xuất và xác minh token, từ đó tạo ra nhiều token trong một lần chuyển tiếp duy nhất. Tuy nhiên, các chiến lược đề xuất hiện tại thường yêu cầu tinh chỉnh đáng kể hoặc có hiệu suất không nhất quán trên các nhiệm vụ. Để giải quyết những thách thức này, chúng tôi đề xuất Hierarchy Drafting (HD)1, một phương pháp đề xuất không mất mát mới tổ chức các nguồn token khác nhau thành nhiều cơ sở dữ liệu trong một khung phân cấp dựa trên tính cục bộ thời gian. Trong bước đề xuất, HD truy cập tuần tự nhiều cơ sở dữ liệu để lấy token đề xuất từ tính cục bộ cao nhất đến thấp nhất, đảm bảo tăng tốc nhất quán trên các nhiệm vụ đa dạng và giảm thiểu độ trễ đề xuất. Các thí nghiệm của chúng tôi trên Spec-Bench sử dụng LLM với 7B và 13B tham số cho thấy HD vượt trội so với các phương pháp đề xuất không mất mát hiện có, đạt được tăng tốc suy luận mạnh mẽ trên các kích thước mô hình, nhiệm vụ và nhiệt độ.

## 1 Giới thiệu

Với nhu cầu ngày càng tăng về việc tăng tốc suy luận Mô hình Ngôn ngữ Lớn (LLM) để cho phép các tương tác người-LLM thời gian thực hiệu quả, Giải mã Suy đoán (Stern et al., 2018; Leviathan et al., 2023; Chen et al., 2023a) đã thu hút sự chú ý vì cung cấp một giải pháp hoàn toàn thuật toán với những nhược điểm tối thiểu. Trong khi giải mã tự hồi quy tạo ra từng token một, bước giải mã trong phương pháp này được chia thành hai bước phụ: đề xuất, nơi các token có khả năng được lấy mẫu từ bên ngoài từ một mô hình ít phức tạp hơn, và xác minh, nơi các token được lấy mẫu được chấp nhận hoặc từ chối bằng cách so sánh với đầu ra thực tế của LLM. Bằng cách cho phép LLM tạo ra nhiều token được chấp nhận trong giai đoạn xác minh, giải mã suy đoán cải thiện cả thông lượng và độ trễ của suy luận LLM. Quan trọng là, hiệu quả của phương pháp này phụ thuộc vào cách các token đề xuất được tạo ra, vì lợi ích hiệu suất phụ thuộc vào tỷ lệ chấp nhận của các token này (Chen et al., 2023a). Do đó, các phương pháp tiếp theo đối với giải mã suy đoán đã tập trung vào việc phát triển các chiến lược đề xuất lấy mẫu các token phù hợp chặt chẽ với mô hình đích.

Các nỗ lực gần đây trong giải mã suy đoán đã tập trung vào việc phát triển các phương pháp đề xuất hiệu quả, sử dụng các phương pháp dựa trên LM, chẳng hạn như sử dụng các mô hình nhỏ hơn LLM (Zhou et al., 2024; Miao et al., 2024) hoặc kết hợp các nhánh chuyên biệt trong kiến trúc LLM (Cai et al., 2024b; Li et al., 2024a). Tuy nhiên, khả năng áp dụng của chúng trong các tình huống thực tế bị hạn chế bởi chi phí đáng kể liên quan đến tinh chỉnh để tối ưu hóa. Đầu tiên, các mô hình nhỏ hơn để đề xuất phải được tinh chỉnh, chẳng hạn như bằng chưng cất, để tạo ra các token tương tự như LLM để đạt hiệu suất tối ưu bất kể các nhiệm vụ được đưa ra (Zhou et al., 2024; Yi et al., 2024). Ngoài ra, các họ LLM hiện tại (Touvron et al., 2023; Zheng et al., 2023) không cung cấp các mô hình có kích thước phù hợp để đề xuất, thường đòi hỏi phải huấn luyện từ đầu. Trong đề xuất dựa trên nhánh, cải tiến kiến trúc LLM gốc, chi phí tính toán để huấn luyện các nhánh như vậy trong LLM là đáng kể do tính toán gradient trên toàn bộ mô hình, mặc dù hầu hết các tham số vẫn bị đông lạnh (Cai et al., 2024b; Li et al., 2024a,b). Ví dụ, EAGLE (Li et al., 2024b), một trong những phương pháp hàng đầu, cần 1-2 ngày huấn luyện trên 2-4 tỷ token sử dụng 4 GPU A100 để huấn luyện mô hình 70B.

Để giải quyết những hạn chế này, bài báo này khám phá một chiến lược đề xuất nhẹ, không mất mát: Đề xuất Cơ sở dữ liệu, loại bỏ nhu cầu cập nhật tham số (Saxena, 2023; Fu et al., 2024; He et al., 2024). Đề xuất cơ sở dữ liệu xây dựng các cơ sở dữ liệu từ các nguồn token khác nhau và lấy các token đề xuất từ cơ sở dữ liệu sử dụng các token trước đó. Tuy nhiên, vì công việc trước đây dựa vào một cơ sở dữ liệu duy nhất từ một nguồn duy nhất, phạm vi bao phủ của các token đề xuất bị hạn chế, dẫn đến tăng tốc không nhất quán trên các nhiệm vụ khác nhau, như được mô tả ở bên trái của Hình 1. Ví dụ, PLD (Saxena, 2023), sử dụng các token trước đó làm nguồn, cho thấy điểm mạnh trong tóm tắt, lặp lại cao các token trong các văn bản trước đó, nhưng nó chỉ đạt được tăng tốc cận biên trong QA, nơi ít token hứa hẹn hơn được bao gồm trong văn bản trước. Một giải pháp đơn giản để cải thiện phạm vi bao phủ là kết hợp các nguồn đa dạng vào một cơ sở dữ liệu duy nhất. Tuy nhiên, việc tăng quy mô cơ sở dữ liệu dẫn đến độ trễ đề xuất cao hơn, gây ra chi phí bổ sung. Như được thể hiện ở bên phải của Hình 1, REST (He et al., 2024), sử dụng cơ sở dữ liệu lớn nhất, dự đoán chính xác các token tương lai nhưng gặp phải độ trễ đáng kể, phủ nhận lợi ích tỷ lệ chấp nhận cao của nó. Do đó, bài báo này đề xuất một giải pháp cho những hạn chế này: Sử dụng các nguồn token đa dạng đồng thời để có hiệu suất mạnh mẽ và chi phí tối thiểu.

Với mục tiêu này trong tâm trí, chúng tôi đề xuất một giải pháp đơn giản nhưng hiệu quả: Hierarchy Drafting (HD), tích hợp các nguồn token đa dạng vào một khung phân cấp. Phương pháp đề xuất của chúng tôi được lấy cảm hứng từ hệ thống phân cấp bộ nhớ, ưu tiên dữ liệu có tính cục bộ thời gian cao trong truy cập bộ nhớ để tối ưu hóa hiệu suất (Aggarwal et al., 1987). Do đó, HD nhóm các token đề xuất từ các nguồn đa dạng dựa trên tính cục bộ thời gian của chúng—xu hướng một số token xuất hiện lại trong hoặc qua các quá trình tạo. Ví dụ, khi một LLM giải một bài toán như, 'Các đỉnh của tam giác ở các điểm (0, 0), (-1, 1), và (3, 3). Diện tích của tam giác là bao nhiêu?', các tọa độ thường xuyên lặp lại chỉ trong một quá trình tạo cho một truy vấn cụ thể nhưng không qua các quá trình tạo khác. Theo nghĩa liên quan, các cụm từ thường được tạo bởi LLM, chẳng hạn như 'với tư cách là một trợ lý AI', hoặc các mẫu ngữ pháp thường xuyên thể hiện tính cục bộ tương đối vừa phải, thường xuất hiện qua các quá trình tạo khác nhau.

Dựa trên tính cục bộ thời gian của chúng, nhiều cơ sở dữ liệu của HD tổ chức chúng thành cơ sở dữ liệu phụ thuộc ngữ cảnh, lưu trữ các token có tính cục bộ thời gian cao cho một ngữ cảnh cụ thể; cơ sở dữ liệu phụ thuộc mô hình, nắm bắt các cụm từ thường xuyên lặp lại bởi LLM qua các thế hệ; và cơ sở dữ liệu phụ thuộc thống kê, chứa các cụm từ phổ biến về mặt thống kê với tính cục bộ thấp hơn một chút qua các quá trình so với những cụm trong cơ sở dữ liệu phụ thuộc mô hình. Trong quá trình suy luận, HD truy cập các cơ sở dữ liệu theo thứ tự tính cục bộ thời gian, ưu tiên các token có tính cục bộ cao bằng cách bắt đầu với phụ thuộc ngữ cảnh, sau đó phụ thuộc mô hình, và cuối cùng các cơ sở dữ liệu phụ thuộc thống kê cho đến khi có đủ số lượng token đề xuất để chuyển cho LLM để xác minh.

Chiến lược này có hai lợi ích: thứ nhất, tăng độ chính xác đề xuất bằng cách tận dụng tính cục bộ thời gian và thứ hai, giảm chi phí từ độ trễ đề xuất, vì quy mô của các cơ sở dữ liệu tương quan nghịch với mức độ cục bộ—các token có tính cục bộ cao hiếm hơn. Do đó, bắt đầu với cơ sở dữ liệu phụ thuộc ngữ cảnh nhỏ hơn để đề xuất token chính xác và nhanh hơn so với việc chỉ sử dụng cơ sở dữ liệu phụ thuộc thống kê lớn hơn. Ngoài ra, khung phân cấp của chúng tôi có thể bao gồm các phương pháp đề xuất cơ sở dữ liệu khác nhờ bản chất plug-and-play, giúp dễ dàng tích hợp các nguồn đề xuất đa dạng dựa trên tính cục bộ thời gian của chúng.

Chúng tôi đánh giá HD và các phương pháp đề xuất cơ sở dữ liệu khác sử dụng các LLM được áp dụng rộng rãi, Llama-2 (Touvron et al., 2023) và Vicuna (Zheng et al., 2023), trên Spec-Bench (Xia et al., 2024), một điểm chuẩn được thiết kế để đánh giá hiệu quả trên các nhiệm vụ đa dạng. Phương pháp đề xuất của chúng tôi, HD, vượt trội so với các phương pháp khác trong thí nghiệm và nhất quán đạt được tăng tốc suy luận đáng kể trên các cài đặt khác nhau, bao gồm kích thước mô hình, nhiệt độ và nhiệm vụ. Chúng tôi cũng phân tích cách khung phân cấp thích ứng chọn cơ sở dữ liệu phù hợp cho mỗi nhiệm vụ trong khi giảm thiểu độ trễ đề xuất, phù hợp với mục tiêu thiết kế của chúng tôi.

Đóng góp của chúng tôi trong bài báo này có ba khía cạnh:
• Chúng tôi xác định những hạn chế của các phương pháp giải mã suy đoán hiện có, đòi hỏi tinh chỉnh bổ sung hoặc mang lại lợi ích tăng tốc không nhất quán.
• Chúng tôi giới thiệu một phương pháp đề xuất cơ sở dữ liệu mới, Hierarchy Drafting (HD), kết hợp các nguồn token đa dạng vào khung phân cấp để có hiệu suất mạnh mẽ với việc giảm thiểu chi phí.
• Chúng tôi chứng minh rằng HD nhất quán đạt được lợi ích tăng tốc đáng kể trên các tình huống khác nhau so với các phương pháp không mất mát khác.

## 2 Công việc Liên quan

Bây giờ chúng tôi giới thiệu giải mã suy đoán và các chiến lược đề xuất không mất mát dựa trên cơ sở dữ liệu.

**Giải mã Suy đoán** Giải mã suy đoán là một phương pháp mới tăng tốc suy luận LLM bằng cách giảm thiểu số lần chuyển tiếp cần thiết, từ đó giảm tổng độ trễ (Stern et al., 2018; Leviathan et al., 2023; Chen et al., 2023a). Khái niệm cốt lõi là các token, chẳng hạn như các cụm từ thường xuyên, có thể được dự đoán với độ tin cậy cao sử dụng các mô hình đơn giản hơn, cho phép tạo nhiều token cùng lúc. Stern et al. (2018) đã giới thiệu mô hình Draft-then-Verify, chia mỗi bước giải mã thành hai bước phụ: đề xuất nhiều token từ các mô hình đề xuất và xác minh chúng so với đầu ra LLM song song. Khái niệm này đã được mở rộng để dự đoán chính xác các token tương lai cùng với chiến lược lấy mẫu hỗ trợ (Xia et al., 2023; Leviathan et al., 2023; Chen et al., 2023a).

**Các Loại Phương pháp Đề xuất** Phương pháp đơn giản cho chiến lược đề xuất của giải mã suy đoán bao gồm việc sử dụng một mô hình ngôn ngữ bổ sung (LM) chuyên biệt cho đề xuất (Leviathan et al., 2023; Chen et al., 2023a; Miao et al., 2024; Zhou et al., 2024). Để đảm bảo đề xuất hiệu quả, các LM như vậy phải theo mẫu tạo của mô hình đích và nhỏ hơn để giảm thiểu chi phí độ trễ bổ sung. Các LM có kích thước tham số dưới một tỷ thường được ưa thích cho đề xuất, nhưng hiện tại, các họ LLM được sử dụng rộng rãi thường không có các mô hình phù hợp. Ví dụ, mô hình Llama-2 nhỏ nhất có sẵn chính thức (Touvron et al., 2023), với 7 tỷ tham số, quá lớn và không hiệu quả cho mục đích đề xuất. Do đó, các phương pháp như vậy thường yêu cầu chi phí huấn luyện để có được LM phù hợp cho LLM được nhắm mục tiêu, chẳng hạn như các mô hình chưng cất từ các mô hình đích (Miao et al., 2024) hoặc các mô hình nhẹ được huấn luyện cho các thiết bị di động (Zhang et al., 2024).

Thay vì sử dụng một mô hình ngôn ngữ riêng biệt để đề xuất, một số phương pháp tăng cường khả năng đề xuất của chính mô hình đích (Cai et al., 2024b; Li et al., 2024a,b; Ankner et al., 2024). Trong dòng công việc này, lớp hoặc nhánh bổ sung trong mô hình đích được tích hợp vào mô hình đích để dự đoán một số token tiếp theo nhiều hơn token tiếp theo ngay lập tức dựa trên các trạng thái ẩn cuối cùng của các đầu vào cụ thể. Theo Stern et al. (2018), khai thác nhiều đầu cho giải mã song song, Medusa (Cai et al., 2024b) đầu tiên tích hợp các đầu giải mã bổ sung vào mô hình đích. Sau đó, các phương pháp đề xuất dựa trên nhánh (Li et al., 2024a,b; Ankner et al., 2024) cho thấy hiệu quả đáng chú ý trong việc lấy mẫu các token tương lai phù hợp với việc đạt được kết quả hiện đại. Tuy nhiên, việc tích hợp các lớp hoặc nhánh này vẫn yêu cầu chi phí huấn luyện đáng kể. Tóm lại, các phương pháp đề xuất dựa trên nhánh đạt được lợi ích tăng tốc đáng chú ý nhưng yêu cầu chi phí tính toán bổ sung, không hề nhỏ và là một loại chi phí mới để thực hiện giải mã suy đoán.

**Đề xuất Cơ sở dữ liệu** Đề xuất cơ sở dữ liệu loại bỏ chi phí huấn luyện bằng cách truy xuất các token đề xuất cho các đầu vào trước đó từ một cơ sở dữ liệu thay vì dựa vào các LM nhỏ hơn hoặc các nhánh kiến trúc bổ sung. Cơ sở dữ liệu lưu trữ các cặp token, với các token tiền tố làm khóa và các token tiếp theo làm giá trị. Các nguồn của các cơ sở dữ liệu này khác nhau giữa các phương pháp khác nhau, với mỗi phương pháp dựa vào nguồn cơ sở dữ liệu độc đáo riêng của nó. Một số phương pháp sử dụng các token gợi ý đầu vào làm nguồn đề xuất, đặc biệt hiệu quả cho các nhiệm vụ như tóm tắt hoặc tạo tăng cường truy xuất, nơi các token đầu vào thường xuyên lặp lại trong quá trình tạo (Saxena, 2023; Yang et al., 2023). Phương pháp khác truy xuất các token đề xuất từ các kho văn bản lớn bằng cách tận dụng các mẫu ngôn ngữ (He et al., 2024). Mặc dù truy xuất từ các kho lớn giới thiệu một số chi phí độ trễ, tăng tốc thu được từ đề xuất chính xác thường vượt trội điều này, dẫn đến suy luận nhanh hơn tổng thể. Ngoài ra, LLM có thể phục vụ như các nguồn cho đề xuất cơ sở dữ liệu bằng cách tạo ra các token được lưu trữ trong cơ sở dữ liệu, thông qua giải mã song song (Santilli et al., 2023; Fu et al., 2024) hoặc tái chế token (Luo et al., 2024), nơi các token liên quan đến quá trình tạo hiện tại. Cuối cùng, các văn bản được tạo trước đây bởi LLM có thể được phục vụ như các nguồn token đề xuất vì LLM thường xuyên tái sử dụng các cụm từ hoặc từ cụ thể (Spector và Ré, 2023). Mỗi nguồn cung cấp điểm mạnh riêng biệt trong việc dự đoán các token tương lai trong một số tình huống nhất định, nhưng những điểm mạnh này có thể trở thành điểm yếu trong những tình huống khác. Do đó, điều đáng chú ý là việc dựa vào một nguồn duy nhất có thể dẫn đến những hạn chế.

Bảng 1 cho thấy kết quả thí nghiệm của các phương pháp đề xuất cơ sở dữ liệu hiện tại, xây dựng cơ sở dữ liệu của chúng từ một nguồn duy nhất. Cụ thể, PLD (Saxena, 2023) thể hiện tăng tốc cao nhất so với các phương pháp khác nhưng cũng cho thấy độ lệch chuẩn đáng kể trong lợi ích tăng tốc. Sự biến đổi này được quy cho kích thước cơ sở dữ liệu hạn chế và không đều, dẫn đến tăng tốc không nhất quán trong quá trình tạo. Ngược lại, LADE (Fu et al., 2024) đạt được độ trễ đề xuất ấn tượng thấp—dưới 0.01 ms. Tuy nhiên, giá trị đáng chú ý này không chuyển thành tăng tốc đáng kể do kích thước cơ sở dữ liệu nhỏ, tương tự như PLD. Tuy nhiên, việc chỉ tăng kích thước cơ sở dữ liệu, như được chứng minh bởi REST (He et al., 2024), không cung cấp một giải pháp khả thi để cải thiện hiệu quả của đề xuất cơ sở dữ liệu. Trong khi quy mô cơ sở dữ liệu lớn hơn có thể cải thiện độ chính xác của bước đề xuất, nó cũng dẫn đến độ trễ cao hơn vì việc truy xuất token từ cơ sở dữ liệu lớn hơn giới thiệu chi phí xử lý bổ sung.

Do đó, để giải quyết những hạn chế của các phương pháp đề xuất không mất mát hiện tại dựa vào một nguồn duy nhất, chúng tôi đề xuất tích hợp các nguồn đa dạng vào một khung phân cấp, nhằm khai thác điểm mạnh của mỗi nguồn một cách hiệu quả hơn với chi phí tối thiểu.

## 3 Phương pháp

Chúng tôi bắt đầu bằng cách định nghĩa chính thức giải mã suy đoán và đề xuất cơ sở dữ liệu và trình bày phương pháp đề xuất của chúng tôi, Hierarchy Drafting (HD), giải quyết những hạn chế của các phương pháp đề xuất cơ sở dữ liệu.

### 3.1 Kiến thức cơ bản

**Giải mã Suy đoán** Tại mỗi bước của giải mã suy đoán, nhiều token ˜x1:m (tức là, chuỗi token đề xuất) được đề xuất từ một mô hình xấp xỉ Mq để dự đoán các token tương lai của LLM Mp (tức là, mô hình đích) cho các token văn bản trước đó x≤t:

˜x1:m ∼ mMq(x≤t). (1)

Tất cả chuỗi token đề xuất ˜x1:m được xác minh so với đầu ra thực tế của Mp. Ví dụ, trong giải mã tham lam, các token x′t+1:t+m được lấy cho một ˜x1:m và x≤t cụ thể bằng cách giải các phương trình sau song song:

x′t+1 = arg max PMp(x|x≤t),
x′t+2 = arg max PMp(x|˜x1,x≤t),
...
x′t+m = arg max PMp(x|˜x1:m,x≤t). (2)

Mỗi token x′t+i được xác minh so với token đề xuất tương ứng ˜xt+i, bắt đầu từ i = 0 cho đến khi xác minh thất bại hoặc i = m được đạt. Để tăng khả năng chấp nhận, nhiều chuỗi token đề xuất ˜X = {˜xi}Ni=1 (tức là, tập đề xuất) được xác minh song song. Mặt nạ chú ý chuyên biệt thực hiện xác minh song song của tập đề xuất, không phải mặt nạ chú ý nhân quả (Fu et al., 2024; Miao et al., 2024). Trong chiến lược lấy mẫu, lấy mẫu suy đoán (Chen et al., 2023a) thường được sử dụng để chấp nhận nhiều token hơn trong khi duy trì các phân phối đầu ra giống hệt của mô hình đích. Tóm lại, bước tạo được chia thành hai bước phụ với một lần chuyển tiếp duy nhất của mô hình đích. Nhiều token được chấp nhận được tạo đồng thời, nén quá trình giải mã tổng thể.

**Đề xuất Cơ sở dữ liệu** Như được thể hiện ở bên trái của Hình 2, các phương pháp được bao gồm trong đề xuất cơ sở dữ liệu khai thác cơ sở dữ liệu D, có các token tiền tố làm khóa và các token tiếp theo làm giá trị. Cho mỗi bước của quá trình tạo, chuỗi token đề xuất ˜x1:m được truy xuất từ cơ sở dữ liệu D cho các token trước đó xt−l:t cụ thể:

˜x1:m ∈ ˜X = Ret(xt−l:t; D), (3)

trong đó l và m là độ dài của các token trước đó và chuỗi token đề xuất. Sau đó, bước xác minh giống như các phương pháp khác.

### 3.2 Hierarchy Drafting

Chúng tôi giới thiệu Hierarchy Drafting (HD), tổ chức các token từ các nguồn đa dạng thành ba cơ sở dữ liệu dựa trên tính cục bộ thời gian và truy cập chúng theo thứ tự từ quy mô nhỏ nhất đến lớn nhất. Tổng quan và quá trình giải mã được mô tả ở bên phải của Hình 2.

**Quan sát: Tính Cục bộ Thời gian** Ý tưởng chính đằng sau đề xuất cơ sở dữ liệu là một số token dễ truy xuất từ cơ sở dữ liệu vì chúng thể hiện tính cục bộ thời gian—có nghĩa là chúng có xu hướng được lặp lại trong hoặc qua các quá trình tạo. Tuy nhiên, lưu ý rằng không phải tất cả các chuỗi token đề xuất đều có cùng mức độ tính cục bộ thời gian trong quá trình tạo. Chúng tôi phân tích mẫu của các 4-gram độc đáo trong 100 lần tạo văn bản trên Spec-Bench (Xia et al., 2024), như được thể hiện trong Hình 3. Kết quả tiết lộ rằng một số 4-gram thường xuyên lặp lại và thể hiện các mức độ cục bộ khác nhau. Cụ thể, các chấm xanh và biểu đồ nhỏ bên phải trong Hình 3 minh họa tính dư thừa cục bộ, nơi cùng một 4-gram xuất hiện nhiều lần trong một bước tạo duy nhất. Điều này phản ánh tính cục bộ thời gian cao trong một lần tạo duy nhất thay vì qua nhiều lần tạo. Ngược lại, các chấm đỏ trong Hình 3 làm nổi bật một mẫu nơi mô hình lặp đi lặp lại tạo ra cùng một 4-gram ở các giai đoạn khác nhau của quá trình tạo, minh họa xu hướng tái sử dụng các chuỗi quen thuộc theo thời gian. Ngoài ra, biểu đồ dưới của Hình 3 trình bày nghiên cứu tần suất của các chấm đỏ và xanh được lấy mẫu, chứng minh rằng một số token thể hiện tính cục bộ thời gian cao trong một ngữ cảnh cụ thể, trong khi những token khác duy trì tính cục bộ nhất quán qua các quá trình tạo. Do đó, với tính cục bộ thời gian khác nhau của các token trong suốt quá trình tạo, các bước đề xuất nên ưu tiên các token có tính cục bộ thời gian cao hơn những token khác.

**Thiết kế Cơ sở dữ liệu** Dựa trên tính cục bộ thời gian của các ứng viên token đề xuất, chúng tôi thiết kế ba loại cơ sở dữ liệu để phân loại chúng. 1) Cơ sở dữ liệu phụ thuộc ngữ cảnh (Dc) chứa các token có liên quan cao đến ngữ cảnh cụ thể của quá trình tạo, chẳng hạn như các chấm xanh trong Hình 3. Điều này bao gồm các token từ gợi ý đầu vào, các token được tạo thông qua giải mã song song, các token bị loại bỏ trong quá trình tạo, và những token khác có liên quan cao đến một ngữ cảnh cụ thể. Dc là bảng tra cứu với các token tiền tố, x1:l, làm khóa và các token tiếp theo, xl:l+m, làm giá trị. Ngoài ra, Dc được cập nhật liên tục trong mỗi bước chuyển tiếp và được khởi tạo khi quá trình tạo tiếp theo được bắt đầu. Cơ sở dữ liệu tuân theo chính sách Least Recently Used (LRU) để cập nhật chuỗi đề xuất. 2) Cơ sở dữ liệu phụ thuộc mô hình (Dm) lưu trữ các token thường xuyên được tạo bởi LLM bất kể ngữ cảnh, như được đại diện bởi các chấm đỏ trong Hình 3. Các chuỗi token được tạo thường xuyên nhất top-k, x1:l+m, được lấy mẫu từ các văn bản được tạo bởi mô hình, với x1:l làm khóa và xl+1:l+m làm giá trị. Đối với Dc và Dm, kích thước tối đa của các giá trị cho một khóa duy nhất giống như kích thước tập đề xuất tối đa N. 3) Cơ sở dữ liệu phụ thuộc thống kê (Ds) rút các token của nó từ các kho văn bản lớn để nắm bắt các cụm từ phổ quát thường được sử dụng trong ngôn ngữ. Mặc dù các token này thường xuyên, chúng xảy ra ít nhất quán hơn qua các quá trình so với những token trong Dm. Để truy xuất hiệu quả chuỗi từ một kho lớn, chúng tôi sử dụng một mảng hậu tố (Manber và Myers, 1993) theo việc thực hiện của He et al. (2024). Chi tiết thực hiện có trong §4.

Thiết kế cơ sở dữ liệu của chúng tôi mang lại ba lợi thế riêng biệt. Đầu tiên, nó tích hợp các nguồn đa dạng vào nhiều cơ sở dữ liệu, cho phép chúng tôi tận dụng điểm mạnh của mỗi nguồn để tăng tốc mạnh mẽ trên các nhiệm vụ khác nhau. Sau đó, kích thước của mỗi cơ sở dữ liệu giảm khi tính cục bộ thời gian của các token tăng vì các token có tính cục bộ cao hơn hiếm hơn, cung cấp cơ hội để tối ưu hóa độ trễ đề xuất. Cuối cùng, thiết kế là plug-and-play, dễ dàng tích hợp các nguồn token bổ sung bằng cách gán chúng vào cơ sở dữ liệu phù hợp dựa trên tính cục bộ thời gian của chúng.

**Truy cập Phân cấp** Sử dụng ba cơ sở dữ liệu được thiết kế với tính cục bộ thời gian trong tâm trí, chúng tôi truy xuất chuỗi token đề xuất ˜x1:m cho đầu vào trước đó xt−l:t cụ thể. Thứ tự truy cập cơ sở dữ liệu dựa trên mức độ tính cục bộ thời gian trong quá trình tạo hiện tại; do đó, truy cập bắt đầu với Dc. Truy cập sau đó tiến đến Dm, có tính cục bộ cao qua các thế hệ, và cuối cùng Ds, với tính cục bộ vừa phải qua các thế hệ, cho đến khi tập đề xuất ˜X tích lũy đủ số lượng ứng viên như tham số siêu định nghĩa N. Những truy cập này tận dụng tính cục bộ của chuỗi token đề xuất để tăng cường độ chính xác đề xuất và giảm thiểu chi phí độ trễ, bảo toàn lợi ích của đề xuất.

**Quá trình Giải mã** Chúng tôi giới thiệu quá trình suy luận của giải mã suy đoán với phương pháp đề xuất của chúng tôi, HD. Đầu tiên, cho một đầu vào trước đó xt−l,t cụ thể, chúng tôi có được tập token đề xuất ˜X từ ba cơ sở dữ liệu với truy cập phân cấp. Sau đó, LLM đích Mp xác minh các chuỗi token đề xuất trong khi đồng thời tạo ra các token bổ sung ˆx. Những token này được sử dụng để cập nhật cơ sở dữ liệu phụ thuộc ngữ cảnh thông qua giải mã song song (Santilli et al., 2023; Fu et al., 2024) hoặc bằng cách tái chế các token bị lãng phí (Luo et al., 2024). Những quá trình này được lặp lại một cách lặp đi lặp lại cho đến khi token [EOS] được tạo hoặc chuỗi đạt độ dài tối đa T được xác định trước. Chi tiết của giải mã được mô tả trong Thuật toán 1.

## 4 Thiết lập Thí nghiệm

Chúng tôi giới thiệu chi tiết của các thiết lập thí nghiệm được thực hiện để đánh giá hiệu quả của HD.

**Tập dữ liệu** Chúng tôi khai thác Spec-Bench (Xia et al., 2024), một điểm chuẩn toàn diện để đánh giá giải mã suy đoán trên các nhiệm vụ khác nhau. Cụ thể, các tập dữ liệu được thu thập là MT-bench (Zheng et al., 2023) cho Hội thoại Đa lượt, WMT14 DE-EN (Bojar et al., 2014) cho Dịch thuật, CNN/Daily Mail (Nallapati et al., 2016) cho Tóm tắt, Natural Question (Kwiatkowski et al., 2019) cho Hỏi đáp, GSM8K (Cobbe et al., 2021) cho Lý luận Toán học, DPR (Karpukhin et al., 2020) cho RAG. Mỗi nhiệm vụ có 80 trường hợp, tạo tổng cộng 480 lần tạo.

**Mô hình** Chúng tôi sử dụng hai họ LLM: Vicuna-v1.3-{7,13,33}B (Zheng et al., 2023) và Llama-2-chat-{7,13}B (Touvron et al., 2023) để chứng minh hiệu quả của phương pháp đề xuất.

**Phương pháp Cơ sở** Chúng tôi so sánh phương pháp đề xuất của chúng tôi, HD, với giải mã tự hồi quy và các phương pháp đề xuất cơ sở dữ liệu khác nhau để xác thực hiệu quả của nó. Cụ thể, 1) Giải mã tự hồi quy (AR) phục vụ như một chỉ số để đo lợi ích tăng tốc. Chúng tôi cũng bao gồm 2) PLD2 (Saxena, 2023), sử dụng các gợi ý đầu vào trước đó làm cơ sở dữ liệu, 3) LADE (Fu et al., 2024), sử dụng giải mã song song thông qua phương pháp lặp Jacobian, và 4) REST (He et al., 2024), truy xuất các token đề xuất từ một kho văn bản lớn.

**Thước đo Đánh giá** Chúng tôi sử dụng nhiều thước đo khác nhau để đánh giá chi phí đề xuất, độ chính xác đề xuất và lợi ích tăng tốc. Để đo chi phí đề xuất, chúng tôi sử dụng 1) Độ trễ Đề xuất, đề cập đến thời gian cần thiết để lấy các token đề xuất. Theo Zhou et al. (2024), độ chính xác đề xuất được đánh giá sử dụng 2) Tỷ lệ Chấp nhận (α) và 3) Token Được chấp nhận Trung bình (τ). Tỷ lệ chấp nhận (α) đại diện cho tỷ lệ các token được chấp nhận so với tổng số token, trong khi token được chấp nhận trung bình (τ) biểu thị số lượng token được chấp nhận dự kiến mỗi bước. Cuối cùng, lợi ích tăng tốc được đo bằng 4) Tỷ lệ Tăng tốc, so sánh #token/giây của mỗi phương pháp từ giải mã tự hồi quy.

**Chi tiết Thực hiện** Phương pháp đề xuất của chúng tôi, HD, được cấu hình với các siêu tham số l, m, N và T được đặt lần lượt là 2, 4, 7 và 1024. Cụ thể, l biểu thị độ dài của các token trước đó được sử dụng làm khóa cơ sở dữ liệu, và m đại diện cho độ dài của chuỗi đề xuất được sử dụng làm giá trị cơ sở dữ liệu. Cuối cùng, N chỉ định kích thước của tập đề xuất được chuyển cho LLM để xác minh. Để áp dụng chiến lược lấy mẫu, chúng tôi khai thác lấy mẫu suy đoán (Chen et al., 2023a) bằng cách đặt xác suất đề xuất là 1.0. Đối với cơ sở dữ liệu phụ thuộc ngữ cảnh (Dc), các token đầu vào trước đó và các token được tạo thông qua giải mã song song được bao gồm. Đối với giải mã song song, chúng tôi tuân theo việc thực hiện được đề xuất bởi LADE (Fu et al., 2024), cho phép xử lý đồng thời các nhánh giải mã song song và xác minh. Do đó, theo việc thực hiện của LADE, bước xác minh dựa trên n-gram. Đối với cơ sở dữ liệu phụ thuộc mô hình (Dm), chúng tôi thu thập các văn bản được tạo bởi LLM từ phần tiếng Anh của tập huấn luyện OASST (Köpf et al., 2023), sử dụng mô hình 7B từ họ LLM được nhắm mục tiêu. Tổng cộng 39.283 văn bản được tạo, từ đó chúng tôi lấy mẫu 100k chuỗi token thường xuyên nhất. Cuối cùng, đối với cơ sở dữ liệu phụ thuộc thống kê (Ds), chúng tôi áp dụng thiết lập của REST (He et al., 2024), sử dụng dữ liệu có nguồn gốc từ UltraChat (Ding et al., 2023), với kích thước cơ sở dữ liệu khoảng 12GB. Chi tiết hơn có trong Phụ lục B.

**Thiết lập Thí nghiệm** Tất cả các thí nghiệm được thực hiện trên một máy được trang bị một GPU A100-40GB-PCIe duy nhất cho các mô hình 7B và 13B và GPU A100-80GB-PCIe cho mô hình 33B, sử dụng độ chính xác float16 cho các mô hình. Để đảm bảo so sánh công bằng, chúng tôi tuân theo các triển khai của các phương pháp đề xuất cơ sở dữ liệu khác và các tập lệnh đánh giá được cung cấp bởi Xia et al. (2024)3. Kết quả thí nghiệm của chúng tôi dựa trên một lần chạy duy nhất, mặc dù chúng tôi quan sát chỉ có sự khác biệt cận biên giữa các lần chạy.

## 5 Kết quả

Bây giờ chúng tôi trình bày kết quả thí nghiệm trên Spec-Bench, cùng với phân tích sâu về HD.

### 5.1 Kết quả Chính

Bảng 2 trình bày kết quả chính của chúng tôi, trung bình trên tất cả các trường hợp của Spec-Bench sử dụng ba mô hình, ở cả nhiệt độ thấp (T = 0.0) và nhiệt độ cao (T = 1.0). Đầu tiên, phương pháp đề xuất của chúng tôi, HD, đạt được lợi ích tăng tốc vượt trội trên tất cả các tình huống. Chi tiết, khi nhiệt độ là 0.0, HD đạt được tốc độ suy luận nhanh hơn 1.5x so với giải mã tự hồi quy, trong khi các phương pháp khác không vượt quá tăng tốc 1.4x. Ngoài ra, trong khi lợi ích tăng tốc ở T = 1.0 thấp hơn một chút so với T = 0.0, HD vẫn đạt được tốc độ suy luận nhanh nhất so với tất cả các phương pháp khác trên tất cả các mô hình. Những kết quả này chứng minh rằng khung phân cấp của chúng tôi hiệu quả tăng cường tốc độ suy luận bằng cách kết hợp các nguồn token đa dạng vào ba cơ sở dữ liệu được tổ chức theo tính cục bộ thời gian.

Ngoài lợi ích tăng tốc, chúng tôi phân tích độ trễ bổ sung gây ra bởi quá trình đề xuất, thêm chi phí vắng mặt trong giải mã tự hồi quy, và cũng đánh giá cách chính xác bước đề xuất truy xuất các token phù hợp với đầu ra của LLM. Về độ trễ đề xuất, LADE yêu cầu thời gian cực kỳ ngắn—dưới 0.01 ms mỗi đề xuất—trong khi REST mất thời gian đáng kể hơn, với độ trễ gần 3.00 ms. Tuy nhiên, độ chính xác đề xuất cho thấy xu hướng ngược lại: LADE thể hiện các giá trị thấp hơn cho cả tỷ lệ chấp nhận (α) và token được chấp nhận trung bình (τ), trong khi REST đạt được các giá trị cao hơn cho cả hai. Đáng chú ý, phương pháp đề xuất của chúng tôi, HD, đề xuất nhanh hơn một chút so với REST, mặc dù truy cập cùng một cơ sở dữ liệu rộng lớn, và dự đoán chính xác 70% token được tạo, đạt được độ chính xác cao nhất trong số tất cả các phương pháp khác. Những kết quả này chỉ ra rằng HD thành công cân bằng tăng độ chính xác với giảm độ trễ đề xuất thông qua truy cập cơ sở dữ liệu phân cấp, dẫn đến lợi ích tăng tốc đáng kể.

**So sánh với Các Phương pháp Không phải Cơ sở dữ liệu** Chúng tôi so sánh các phương pháp đề xuất cơ sở dữ liệu đa dạng với hai phương pháp đề xuất không phải cơ sở dữ liệu, SpS (Chen et al., 2023b) và MEDUSA (Cai et al., 2024b), để xác nhận liệu hiệu suất có cạnh tranh mà không cần huấn luyện bổ sung. Như được thể hiện trong Hình 4, trong khi các phương pháp đề xuất cơ sở dữ liệu khác hoạt động kém đáng kể so với các phương pháp đề xuất không phải cơ sở dữ liệu, phương pháp đề xuất của chúng tôi, HD, vượt trội SpS và thu hẹp đáng kể khoảng cách hiệu suất với MEDUSA. Điều này chứng minh rằng phương pháp đề xuất của chúng tôi cho thấy tiềm năng đạt được lợi ích tăng tốc đáng kể hơn mà không cần huấn luyện lại các mô hình bằng cách khai thác các tài nguyên dữ liệu phổ biến trong các tình huống phục vụ thực tế.

**Tính mạnh mẽ trên Các Nhiệm vụ** Chúng tôi đánh giá tính mạnh mẽ của các phương pháp đề xuất cơ sở dữ liệu trên các nhiệm vụ tạo khác nhau, như được minh họa ở bên phải của Hình 4. Việc dựa vào một nguồn duy nhất dẫn đến sự biến đổi trong lợi ích tăng tốc, khiến hầu hết các phương pháp, ngoại trừ HD, cho thấy hiệu suất không nhất quán với các vùng lõm trong các nhiệm vụ cụ thể. Cụ thể, PLD đạt được tăng tốc đáng kể trong các nhiệm vụ như Tóm tắt và RAG nhưng cung cấp cải thiện tối thiểu trong Dịch thuật và QA. Ngoài ra, các phương pháp khác thể hiện lợi ích tăng tốc khác nhau tùy thuộc vào mô hình được sử dụng—REST, ví dụ, hoạt động tốt với Llama-7b trong tóm tắt nhưng cho thấy kết quả yếu hơn với Vicuna-7b, gần như khớp với tốc độ giải mã tự hồi quy. Ngược lại, phương pháp đề xuất của chúng tôi nhất quán đạt được tăng tốc cao nhất trên tất cả các nhiệm vụ và mô hình, chiếm diện tích lớn nhất trong mỗi biểu đồ. Điều này chứng minh rằng việc kết hợp các nguồn đa dạng tăng cường tính mạnh mẽ, làm cho các phương pháp đề xuất cơ sở dữ liệu phù hợp hơn cho các tình huống thực tế.

### 5.2 Phân tích

Trong phần này, chúng tôi cung cấp phân tích sâu về HD để điều tra hiệu quả của nó.

**Phân tích Ba Cơ sở dữ liệu** Bên trái của Hình 5 mô tả tỷ lệ thành công xác minh và độ trễ đề xuất của mỗi cơ sở dữ liệu. Tỷ lệ thành công xác minh đo tỷ lệ các trường hợp được chấp nhận so với tổng số truy cập cơ sở dữ liệu trong bước xác minh. Dc đạt được tỷ lệ thành công xác minh cao nhất hơn 30% với độ trễ đề xuất thấp nhất, chứng minh hiệu quả của nó trong việc lấy các token tương lai liên quan đến ngữ cảnh. Tuy nhiên, Dm cho thấy tỷ lệ thành công xác minh thấp hơn, 15.5%, với độ trễ hơi cao hơn, chỉ ra rằng mặc dù nó hoạt động khá, nó ít phù hợp với các ngữ cảnh cụ thể. Ds thể hiện tỷ lệ thành công xác minh thấp nhất dưới 10% và độ trễ đề xuất cao nhất hơn 10ms do quy mô lớn hơn và tính cục bộ thấp hơn. Những điều này làm nổi bật rằng các token đề xuất có tính cục bộ cao hơn được chấp nhận thường xuyên hơn, chỉ ra sự phù hợp với mục tiêu thiết kế của chúng tôi.

**Mẫu Truy cập trên Các Nhiệm vụ** Chúng tôi phân tích cách phương pháp đề xuất của chúng tôi, HD, đạt được lợi ích tăng tốc nhất quán trên các nhiệm vụ với tỷ lệ thành công xác minh của các cơ sở dữ liệu cho mỗi nhiệm vụ. Như được thể hiện ở bên phải của Hình 5, Dc xuất sắc trong các nhiệm vụ như Hội thoại Đa lượt hoặc Tóm tắt, nơi các token cụ thể cho ngữ cảnh được lặp lại cao, dẫn đến thành công xác minh cao. Tuy nhiên, đối với các nhiệm vụ như dịch thuật và QA, cung cấp ít manh mối rõ ràng hơn từ các đầu vào hoặc ngữ cảnh trước đó, Dc đạt được thành công xác minh thấp hơn. Trong những trường hợp này, Dm và Ds bù đắp cho điểm yếu của Dc bằng cách cho thấy thành công xác minh cao hơn so với các nhiệm vụ khác nơi Dc vượt trội. Những kết quả này làm nổi bật cách HD của chúng tôi hiệu quả truy cập cơ sở dữ liệu phù hợp cho mỗi nhiệm vụ, hiệu quả tận dụng điểm mạnh riêng biệt của các nguồn đa dạng.

**Thứ tự Truy cập Cơ sở dữ liệu** Chúng tôi phân tích tác động của thứ tự truy cập trong khung phân cấp, như được thể hiện trong Hình 6. Như mong đợi, thứ tự truy cập gốc của chúng tôi (cms), ưu tiên các cơ sở dữ liệu từ tính cục bộ thời gian cao nhất đến thấp nhất, đạt được tỷ lệ chấp nhận cao nhất và độ trễ đề xuất thấp nhất. Trong khi các thứ tự khác duy trì tỷ lệ chấp nhận trên 50%, đủ cho một số lợi ích tăng tốc, tăng tốc thực tế của chúng thấp hơn đáng kể do độ trễ đề xuất bổ sung, đạt đến 12ms cho các thứ tự như scm hoặc smc. Những kết quả này chứng minh rằng truy cập phân cấp tận dụng đầy đủ tiềm năng của nhiều cơ sở dữ liệu với độ trễ đề xuất tối thiểu so với các thứ tự khác, nhấn mạnh tầm quan trọng của tính cục bộ thời gian trong thứ tự đề xuất.

Chúng tôi cung cấp phân tích bổ sung trong Phụ lục C.

## 6 Thảo luận

Mặc dù phương pháp đề xuất của chúng tôi đạt được lợi ích hiệu suất đáng kể so với các phương pháp đề xuất cơ sở dữ liệu khác, các phương pháp gần đây dựa trên huấn luyện lại mô hình (Cai et al., 2024b; Li et al., 2024a; Ankner et al., 2024) đã chứng minh tăng tốc cao hơn đáng kể. Tuy nhiên, điều cần thiết phải lưu ý rằng chi phí huấn luyện liên quan đến các phương pháp này không hề nhỏ, đặc biệt trong các cài đặt động hoặc tốn tài nguyên. Ví dụ, các phương pháp dựa trên huấn luyện lại đòi hỏi các bước huấn luyện bổ sung, đặt ra những thách thức thực tế trong các ứng dụng thực tế như phục vụ đa mô hình (Sheng et al., 2024; Ramírez et al., 2024) hoặc môi trường hạn chế tài nguyên (Cai et al., 2024a). Cụ thể, triển khai nhiều LLM cho các nhiệm vụ cụ thể cho lĩnh vực đa dạng sử dụng nhiều bộ điều hợp LoRA (Sheng et al., 2024) hoặc sử dụng các chiến lược định tuyến mô hình để phục vụ hiệu quả (Ramírez et al., 2024) có thể tăng đáng kể chi phí tính toán khi các phương pháp như vậy phải được áp dụng cho tất cả LLM. Kết quả là, yêu cầu huấn luyện lại có thể phức tạp hóa triển khai, đặc biệt trong các tình huống phục vụ thực tế.

Với những ràng buộc này, chúng tôi định vị các phương pháp đề xuất cơ sở dữ liệu như một giải pháp thay thế thực tế cho huấn luyện lại mô hình bằng cách tận dụng các tài nguyên dữ liệu sẵn có trong các tình huống phục vụ thay vì khẳng định hiệu suất tốt nhất. Các phương pháp đề xuất cơ sở dữ liệu có thể hiệu quả giải quyết các thách thức phục vụ trong các ứng dụng thực tế bằng cách đạt được giải mã suy đoán hoàn toàn không mất mát mà không yêu cầu cập nhật tham số. Trong số các phương pháp đề xuất cơ sở dữ liệu, phương pháp đề xuất của chúng tôi, HD, tiếp tục tăng cường tính thực tế của đề xuất cơ sở dữ liệu bằng cách kết hợp các tài nguyên dữ liệu đa dạng vào một khung phân cấp để đề xuất chính xác và hiệu quả các token tương lai của LLM. Do đó, HD thu hẹp khoảng cách hiệu suất với các phương pháp giải mã suy đoán hiện đại, chứng minh tiềm năng của đề xuất cơ sở dữ liệu để tăng tốc suy luận đáng kể mà không cần tinh chỉnh mô hình.

## 7 Kết luận

Trong công việc này, chúng tôi khám phá các phương pháp đề xuất cơ sở dữ liệu trong giải mã suy đoán, không yêu cầu huấn luyện hoặc tinh chỉnh bổ sung. Các phương pháp hiện có dựa vào một cơ sở dữ liệu duy nhất từ một nguồn duy nhất, dẫn đến lợi ích tăng tốc không nhất quán và không tối ưu. Để giải quyết điều này, chúng tôi đề xuất Hierarchical Drafting (HD), sử dụng tối ưu các nguồn đa dạng bằng cách xây dựng nhiều cơ sở dữ liệu dựa trên tính cục bộ thời gian. Phương pháp của chúng tôi truy cập phân cấp các cơ sở dữ liệu này, ưu tiên những cơ sở có tính cục bộ cao nhất để tăng tốc tối ưu. Kết quả thí nghiệm cho thấy HD nhất quán và hiệu quả tăng tốc suy luận LLM trên các tình huống khác nhau, vượt trội so với các phương pháp đề xuất cơ sở dữ liệu khác. Những phát hiện này chứng minh rằng khung phân cấp của chúng tôi tối đa hóa điểm mạnh của mỗi cơ sở dữ liệu với chi phí tối thiểu, mở rộng các hướng khai thác nhiều cơ sở dữ liệu để tăng tốc không mất mát trong giải mã suy đoán.

## Hạn chế

Một hạn chế của bài báo này là việc sử dụng hạn chế các LLM với hơn 13B tham số. Trong khi đánh giá của chúng tôi tập trung vào các mô hình như Llama-2 và Vicuna với tối đa 13B tham số, hiệu suất của HD trên các mô hình lớn hơn vẫn chưa được khám phá. Tuy nhiên, chúng tôi mong đợi rằng các mô hình lớn hơn sẽ phù hợp hơn nhiều cho phương pháp của chúng tôi, xem xét tỷ lệ chấp nhận cao của phương pháp đề xuất của chúng tôi, HD, trên các tình huống đa dạng và độ nhạy cảm giảm với độ trễ đề xuất khi độ trễ tạo tăng. Ngoài ra, chúng tôi dự định mở rộng các thí nghiệm của mình sang các mô hình lớn hơn trong công việc tương lai.

Trong khi bài báo này tận dụng nhiều cơ sở dữ liệu để tối đa hóa điểm mạnh của chúng với chi phí tối thiểu, các nguồn của các cơ sở dữ liệu này không hoàn toàn mới. Thay vì tập trung vào tính mới của mỗi nguồn, chúng tôi nhấn mạnh rằng phương pháp của chúng tôi là plug-and-play, giúp dễ dàng tích hợp các phương pháp tương lai bằng cách đơn giản thêm các token từ các nguồn mới vào cơ sở dữ liệu phù hợp. Ví dụ, mặc dù chúng tôi bỏ qua tái chế token (Luo et al., 2024) trong các thí nghiệm của mình, các token được tái chế có thể được thêm vào cơ sở dữ liệu phụ thuộc ngữ cảnh, với tính cục bộ thời gian của chúng.

## Tuyên bố Đạo đức

Công việc này đề xuất một chiến lược đề xuất không mất mát trong giải mã suy đoán để có lợi ích tăng tốc tối ưu và tổng quát. Tuy nhiên, phương pháp của chúng tôi có thể tạo ra các phản hồi bạo lực hoặc thiên vị, điều này nằm ngoài phạm vi của bài báo này. Chúng tôi tin tưởng mạnh mẽ rằng nghiên cứu tương lai về các mô hình ngôn ngữ lớn sẽ giải quyết những vấn đề này và giúp giảm thiểu những lo ngại như vậy.

## Lời cảm ơn

Công việc này được hỗ trợ bởi tài trợ của Quỹ Nghiên cứu Quốc gia Hàn Quốc (NRF) được tài trợ bởi chính phủ Hàn Quốc (MSIT) (RS-2024-00359979). Ngoài ra, công việc này được hỗ trợ bởi Viện Lập kế hoạch và Đánh giá Công nghệ Thông tin & Truyền thông (IITP) được tài trợ bởi chính phủ Hàn Quốc (MSIT) (RS-2023-00215700 và RS-2024-00395134).
