# EMS-SD: Giải Mã Suy Đoán Đa Mẫu Hiệu Quả
cho Tăng Tốc Mô Hình Ngôn Ngữ Lớn

Yunsheng Ni Chuanjian Liu Yehui Tang Kai Han* Yunhe Wang*
Phòng thí nghiệm Noah's Ark của Huawei
{niyunsheng,kai.han,yunhe.wang}@huawei.com

## Tóm tắt

Giải mã suy đoán nổi lên như một kỹ thuật then chốt để tăng cường tốc độ suy luận của Mô hình Ngôn ngữ Lớn (LLM). Mặc dù nghiên cứu gần đây nhằm cải thiện hiệu quả dự đoán, giải mã suy đoán đa mẫu đã bị bỏ qua do số lượng token được chấp nhận khác nhau trong một batch trong giai đoạn xác minh. Phương pháp vanilla thêm token đệm để đảm bảo số lượng token mới giữ nhất quán giữa các mẫu. Tuy nhiên, điều này làm tăng chi phí tính toán và truy cập bộ nhớ, từ đó giảm tỷ lệ tăng tốc. Chúng tôi đề xuất một phương pháp mới có thể giải quyết vấn đề token không nhất quán được chấp nhận bởi các mẫu khác nhau mà không cần tăng chi phí bộ nhớ hoặc tính toán. Hơn nữa, phương pháp đề xuất của chúng tôi có thể xử lý tình huống mà token dự đoán của các mẫu khác nhau không nhất quán mà không cần thêm token đệm. Các thí nghiệm đầy đủ chứng minh hiệu quả của phương pháp chúng tôi. Mã nguồn của chúng tôi có sẵn tại https://github.com/niyunsheng/EMS-SD.

## 1 Giới thiệu

Mô hình Ngôn ngữ Lớn (LLM) (Radford et al., 2019; Achiam et al., 2023; Touvron et al., 2023; Wang et al., 2023) đã thể hiện khả năng đáng kể, đặc biệt trong lĩnh vực xử lý ngôn ngữ tự nhiên. Mô hình Ngôn ngữ Lớn tự hồi quy tạo ra một token trong một lần đi qua, trong khi giải mã suy đoán cho phép các mô hình lớn tạo ra nhiều token trong một lần đi qua, từ đó cải thiện đáng kể tốc độ suy luận. Điều quan trọng cần nhấn mạnh là thời gian suy luận của LLM trên một token và nhiều token là gần tương đương. Do đó, giảm số bước suy luận có thể giảm đáng kể thời gian suy luận.

Rất nhiều phương pháp giải mã suy đoán hiệu quả đã được đề xuất gần đây. Tuy nhiên, không phương pháp nào trong số này cung cấp một nghiên cứu toàn diện về giải mã suy đoán trong các tình huống đa mẫu. Theo hiểu biết của chúng tôi, chỉ có EAGLE (Li et al., 2024) trình bày kết quả cho kích thước batch ≤4 nhưng không thảo luận về kích thước batch lớn hơn.

Thách thức chính trong giải mã suy đoán đa mẫu là sự không nhất quán trong số lượng token được chấp nhận giữa các mẫu sau một lần suy luận. Giải pháp vanilla là thêm token đệm để đạt được tính đồng nhất. Cách tiếp cận này cũng được EAGLE sử dụng. Tuy nhiên, những token đệm này làm tăng chi phí tính toán và truy cập bộ nhớ, điều này trở nên đáng kể khi kích thước batch tăng, từ đó giảm tỷ lệ tăng tốc.

**Liệu chúng ta có thể thực hiện giải mã suy đoán đa mẫu mà không tăng chi phí tính toán và truy cập bộ nhớ không?**

Chúng tôi đề xuất một phương pháp mới và hiệu quả để giải quyết vấn đề này. Cụ thể, chúng tôi đề xuất bộ nhớ đệm Key-Value (KV) không đệm trong giai đoạn xác minh, xác định vị trí bắt đầu của bộ nhớ đệm KV cho các mẫu khác nhau, do đó loại bỏ nhu cầu token đệm. Hơn nữa, dự đoán khả năng bất đồng trong số lượng token được dự đoán giữa các mẫu khác nhau, chúng tôi đề xuất phương pháp token đầu vào không đệm như một giải pháp trong giai đoạn dự đoán. Phương pháp này nối tất cả token đầu vào trước khi suy luận và mở rộng các token này trong quá trình tính toán attention.

Những đóng góp chính như sau:

1. Chúng tôi đề xuất một phương pháp Giải mã Suy đoán Đa mẫu Hiệu quả (EMS-SD), tính đến đầy đủ tính không đồng nhất giữa các mẫu khác nhau. Ngay cả khi số lượng token mới được tạo của các mẫu khác nhau thay đổi, bộ nhớ đệm KV vẫn liên tục mà không thêm token đệm. Tương tự, khi số lượng token dự đoán của các mẫu khác nhau thay đổi, tất cả token đầu vào được nối mà không thêm token đệm.

2. Các thí nghiệm đầy đủ đã chứng minh rằng phương pháp đề xuất của chúng tôi đạt được tốc độ tăng tốc cao hơn nhiều so với phương pháp vanilla trong giải mã suy đoán đa mẫu.

3. Chúng tôi là những người đầu tiên nghiên cứu giải mã suy đoán trong bối cảnh tình huống đa mẫu, và chúng tôi đã đề xuất một phương pháp hiệu quả để giải quyết vấn đề này. Phương pháp của chúng tôi có thể dễ dàng tích hợp vào hầu như tất cả các phương pháp giải mã suy đoán cơ bản.

## 2 Nghiên cứu liên quan

**Mô hình Ngôn ngữ Lớn.** Kể từ sự ra đời của dòng mô hình GPT (Radford et al., 2019), đặc biệt là sau khi ChatGPT (Achiam et al., 2023) xuất hiện, đã có sự phát triển mạnh mẽ của các mô hình ngôn ngữ lớn, bao gồm Llama (Touvron et al., 2023), Vicuna (Chiang et al., 2023), ChatGLM (Zeng et al., 2022), QWen (Bai et al., 2023), Baichuan (Yang et al., 2023a), Gemini (Team et al., 2023), Pangu-π (Wang et al., 2023), Mistral (Jiang et al., 2023, 2024), v.v.

**Giải mã suy đoán.** Giải mã suy đoán có thể được chia thành hai giai đoạn nói chung: dự đoán và xác minh. Một số nghiên cứu đã đề xuất các phương pháp dự đoán hiệu quả. Những phương pháp dự đoán này có thể được phân loại rộng rãi thành hai loại: những phương pháp cần đào tạo và những phương pháp không cần đào tạo. Ví dụ, các phương pháp không cần đào tạo bao gồm LLMA (Yang et al., 2023b), REST (He et al., 2023), Lookahead (Fu et al., 2023), PLD (Saxena, 2023), v.v. Ngược lại, các phương pháp cần đào tạo bao gồm dự đoán mô hình nháp (Leviathan et al., 2023), Medusa (Cai et al., 2024), Hydra (Ankner et al., 2024), kangaroo (Liu et al., 2024), EAGLE (Li et al., 2024), v.v.

**Giải mã cây động.** SpecInfer (Miao et al., 2023) giới thiệu cơ chế giải mã cây, dự đoán nhiều token ở cùng một vị trí để cải thiện tỷ lệ chấp nhận. Cấu trúc cây được thiết kế thủ công, và Medusa, EAGLE, v.v cũng vậy. Một số nghiên cứu gần đây đã tập trung vào vấn đề giải mã cây động. Sequoia (Chen et al., 2024) giới thiệu một trình tối ưu hóa cây nhận biết phần cứng. RSD (Jeon et al., 2024) thay đổi động cấu trúc cây trong ngân sách tính toán cố định. Và EAGLE2 (Li et al., 2024) tạo ra cây dự đoán một cách động dựa trên điểm tin cậy từ mô hình nháp.

## 3 Phương pháp tiếp cận

### 3.1 Suy nghĩ lại về Giải mã Suy đoán Đa mẫu Vanilla

**Hạn chế về truy cập bộ nhớ.** Cần lưu ý rằng các framework AI chính như PyTorch (Paszke et al., 2019) chỉ hỗ trợ truy cập bộ nhớ đệm key-value được căn chỉnh. Do đó, hai yêu cầu chính phải được đáp ứng cho suy luận LLM: (1) số lượng token giữa các mẫu khác nhau trong một batch phải bằng nhau trước khi suy luận, và (2) số lượng token đầu vào phải giữ nhất quán cho tất cả các mẫu trong quá trình suy luận. Để đảm bảo tính đồng nhất, token đệm được thêm vào các mẫu có độ dài token khác nhau. Ngoài ra, mask attention được sử dụng để ngăn chặn việc tính toán token đệm.

**Thêm token đệm để căn chỉnh độ dài đầu ra của các mẫu khác nhau.** Vấn đề chính là số lượng token được chấp nhận trong giai đoạn xác minh khác nhau đáng kể giữa các mẫu trong một batch. Để minh họa, nếu k token được dự đoán trong giai đoạn dự đoán, thì số lượng token được chấp nhận có thể thay đổi từ 1 đến k+1. Phương pháp vanilla thêm token đệm để đảm bảo số lượng token mới giống nhau cho mỗi mẫu trong một batch. Tuy nhiên, cách tiếp cận này dẫn đến sự gia tăng đáng kể trong chi phí tính toán và truy cập bộ nhớ, điều này dẫn đến sự giảm đáng kể trong tốc độ tăng tốc. Trong Phụ lục B, chúng tôi trình bày phân tích lý thuyết về tác động của token đệm đối với tốc độ tăng tốc.

**Thêm token đệm để căn chỉnh độ dài đầu vào của các mẫu khác nhau.** Một vấn đề khác là số lượng token được dự đoán cho các mẫu khác nhau trong giai đoạn dự đoán có thể thay đổi. Trong trường hợp này, token đệm cũng cần được thêm vào để căn chỉnh độ dài đầu vào. Vấn đề này không phát sinh trong mọi trường hợp, và thường được quan sát thấy nhất trong các tình huống dự đoán dựa trên truy xuất, bao gồm LLMA (Yang et al., 2023b) và REST (He et al., 2023). Điều này là do phương pháp dự đoán dựa trên truy xuất sử dụng quy trình khớp văn bản, trong đó các mẫu khác nhau có thể không khớp với văn bản được dự đoán cùng lúc. Trong các phương pháp tổng quát hơn, chẳng hạn như dự đoán mô hình nháp (Leviathan et al., 2023), tạo ra cùng số lượng token dự đoán cho các mẫu khác nhau. Một số nghiên cứu gần đây đã tập trung vào vấn đề giải mã cây động (Chen et al., 2024; Jeon et al., 2024). Có thể trong tương lai, có thể có các cây dự đoán tối ưu khác nhau hoặc số lượng token tối ưu khác nhau cho các mẫu khác nhau.

**Phân tích trường hợp.** Như minh họa trong Hình 2 và Bảng 1, chúng tôi xây dựng hai mẫu trong một batch làm ví dụ. Trong bước giải mã 1, mẫu 1 phải thêm 3 token đệm để đảm bảo độ dài đầu vào giống với mẫu 0. Sau đó, sau giai đoạn xác minh, mẫu 1 phải thêm 3 token đệm trong bộ nhớ đệm KV để đảm bảo độ dài đầu ra giống với mẫu 0. Trong bước giải mã 2, mẫu 0 phải thêm 3 token đệm trong giai đoạn dự đoán và 4 token đệm trong bộ nhớ đệm KV sau giai đoạn xác minh.

### 3.2 Giải mã Suy đoán Đa mẫu Hiệu quả

Phương pháp Vanilla có xu hướng dẫn đến chi phí tính toán và truy cập bộ nhớ cao. Ngược lại, cách tiếp cận của chúng tôi không gặp phải những nhược điểm như vậy, từ đó mang lại tỷ lệ tăng tốc cao hơn. Trong phần này, chúng tôi trước tiên chỉ ra rằng truy cập bộ nhớ đệm KV được căn chỉnh không phải là bất biến, và sau đó trình bày hai thành phần chính của cách tiếp cận: bộ nhớ đệm KV không đệm và token đầu vào không đệm.

**Truy cập bộ nhớ đệm KV được căn chỉnh không bắt buộc.** Trong các mô hình tự hồi quy, mỗi token chỉ phụ thuộc vào các token trước đó trong quá trình tính toán attention. Về mặt lý thuyết, với vị trí của token đầu vào và quyền truy cập vào bộ nhớ đệm KV, chúng ta có thể tính toán đầu ra attention. Những thao tác này có thể được đóng gói trong các kernel CUDA, như được chứng minh bằng các triển khai trong các framework như FasterTransformer (NVIDIA, 2021), FlashAttention (Dao et al., 2022), và PyTorch (Paszke et al., 2019). Khi gọi các kernel này, chúng ta có thể tính toán đầu ra attention cho các token khác nhau, ngay cả khi những token khác nhau này ở trong các mẫu khác nhau và dựa vào số lượng token trước đó khác nhau.

**Bộ nhớ đệm KV không đệm.** Trước tiên, chúng tôi giới thiệu thành phần chính đầu tiên: bộ nhớ đệm KV không đệm. Điều này loại bỏ nhu cầu thêm token đệm khi các mẫu khác nhau chấp nhận độ dài khác nhau trong giai đoạn xác minh. Cụ thể, chúng tôi xác định vị trí bắt đầu của bộ nhớ đệm KV cho từng mẫu riêng lẻ, thay vì căn chỉnh ghi theo cách tương tự như Pytorch. Cần lưu ý rằng các vị trí bắt đầu khác nhau của các mẫu dẫn đến sự khác biệt nhỏ trong khối lượng tính toán cho các kernel CUDA attention. Tuy nhiên, vì tất cả các token ở các vị trí và mẫu khác nhau tính toán đầu ra attention song song, tốc độ tổng thể được quyết định bởi token cần khối lượng tính toán lớn nhất, thường là token có số lượng token trước đó cao nhất. Như minh họa trong phần dưới của Hình 2, vị trí bắt đầu của bộ nhớ đệm KV của hai mẫu là khác biệt. Đối với mỗi token đầu vào, chúng tôi ban đầu tính toán bộ nhớ đệm KV của nó và sau đó ghi nó vào bộ nhớ dựa trên vị trí được chỉ định cho mỗi mẫu. Sau đó, đầu ra attention cho tất cả các token, trên các mẫu và vị trí khác nhau, được tính toán song song.

Bằng cách sử dụng vị trí bắt đầu bộ nhớ đệm KV duy nhất cho mỗi mẫu, chúng ta có thể xác định độc lập vị trí bắt đầu tiếp theo trong quá trình xác minh, bất kể độ dài chấp nhận khác nhau giữa các mẫu. Do đó, cách tiếp cận này loại bỏ nhu cầu token đệm bổ sung, từ đó ngăn chặn lãng phí bộ nhớ và chi phí tính toán. Như thể hiện trong Hình 2, mẫu 0 chấp nhận 4 token, làm tiến vị trí bắt đầu bộ nhớ đệm KV lên 4. Trong khi mẫu 1 chấp nhận 1 token, làm tiến nó lên 1.

**Token đầu vào không đệm.** Thứ hai, để giải quyết vấn đề số lượng token đầu vào khác nhau giữa các mẫu khác nhau, chúng tôi đề xuất phương pháp "token đầu vào không đệm" như một giải pháp. Nói chung, trước khi đưa vào mạng Transformer, tất cả token đầu vào được nối với nhau, và số lượng token đầu vào cho mỗi mẫu được ghi lại. Ngoài ra, trong quá trình tính toán kết quả attention, kernel CUDA tái tạo các chỉ số batch và vị trí chuỗi gốc cho mỗi token. Việc tái tạo này cho phép chúng ta xác định bộ nhớ đệm KV cụ thể mà mỗi token cần dựa vào. Hình 3 cho thấy luồng xử lý chung. Tham khảo Phụ lục D để biết các quy trình xử lý cụ thể.

## 4 Thí nghiệm

### 4.1 Chi tiết triển khai

**Phương pháp Giải mã Suy đoán Cơ bản.** Hiệu quả của phương pháp chúng tôi được đánh giá thông qua hai phương pháp giải mã suy đoán cơ bản. Bao gồm LLMA (Yang et al., 2023b), một phương pháp dựa trên truy xuất, và phương pháp dự đoán mô hình nháp (Leviathan et al., 2023), sử dụng mô hình nháp để dự đoán. Trong phương pháp LLMA, độ dài khớp được đặt thành 2 và độ dài sao chép thành 7. Trong phương pháp dự đoán mô hình nháp, mô hình nháp được sử dụng để dự đoán 4 token.

**Mô hình và Tập dữ liệu.** Chúng tôi áp dụng các mô hình dòng Opt (Zhang et al., 2022), bao gồm Opt-2.7b, Opt-6.7b, và Opt-13b. Đối với phương pháp dự đoán mô hình nháp, chúng tôi sử dụng Opt-125m làm mô hình nháp. Tập dữ liệu thử nghiệm bao gồm tổng cộng 480 mẩu dữ liệu được chọn từ tập con Test CNN/Daily Mail (See et al., 2017). Trong thí nghiệm của chúng tôi, chúng tôi sử dụng một GPU A100 cho các mô hình Opt-2.7b và 6.7b, trong khi sử dụng hai GPU cho mô hình Opt-13b. Tất cả kết quả thí nghiệm đều được thực hiện ba lần độc lập và tính giá trị trung bình. Chúng tôi cũng đã tiến hành thí nghiệm trên các tập dữ liệu GSM8K (Cobbe et al., 2021) và MT-bench (Zheng et al., 2023), chi tiết có thể tìm thấy trong Phụ lục C.

**Chỉ số.** Để xác định tốc độ của một phương pháp nhất định, chúng tôi sử dụng token mỗi giây làm chỉ số. Hơn nữa, tỷ lệ tăng tốc đại diện cho bội số giữa việc sử dụng phương pháp giải mã suy đoán và không sử dụng. Cho rằng độ dài tạo ra của Tập dữ liệu CNN/Daily Mail tương đối ngắn (ít hơn 128), chúng tôi giới hạn xem xét của mình đối với quá trình giải mã tăng dần. Trong giải mã suy đoán, độ dài chấp nhận trung bình là một chỉ số quan trọng, với độ dài chấp nhận trung bình lớn hơn thường chỉ ra tỷ lệ tăng tốc cao hơn. Vì một số token đệm cần được thêm vào trong phương pháp giải mã suy đoán đa mẫu vanilla, tỷ lệ đệm trung bình cũng là một chỉ số quan trọng.

**Triển khai mã cụ thể.** Phương pháp đề xuất của chúng tôi đòi hỏi việc thay đổi kernel CUDA. Và chúng tôi đã triển khai phương pháp của mình trên framework FasterTransformer (NVIDIA, 2021), một thư viện tăng tốc C++ được sử dụng rộng rãi tạo điều kiện cho việc triển khai phương pháp của chúng tôi. Các phương pháp đề xuất được triển khai bằng cách sửa đổi giao diện gọi Python và các kernel CUDA. Chi tiết thêm có sẵn trong kho lưu trữ mã nguồn mở.

### 4.2 Thí nghiệm sử dụng LLMA

Trong phần này, LLMA được áp dụng làm phương pháp cơ bản của giải mã suy đoán.

**Nghiên cứu loại bỏ về hai thành phần chính: bộ nhớ đệm KV không đệm và token đầu vào không đệm.** Như minh họa trong Bảng 2, mô hình opt-6.7b được sử dụng để tiến hành thí nghiệm loại bỏ về hai phương pháp chính. Trước tiên, có thể quan sát thấy rằng dưới các kích thước batch khác nhau, phương pháp của chúng tôi thể hiện tốc độ tăng tốc vượt trội so với phương pháp vanilla. Khi kích thước batch được đặt thành 8, phương pháp của chúng tôi đạt được tốc độ tăng tốc 2.17 lần, trong khi phương pháp vanilla chỉ đạt được tốc độ tăng tốc 1.37 lần. Thứ hai, cả hai phương pháp con đều có ý nghĩa, với "bộ nhớ đệm KV không đệm" đóng vai trò đặc biệt quan trọng.

**Thí nghiệm trên các kích thước mô hình khác nhau.** Chúng tôi đã tiến hành thí nghiệm trên hai mô hình có kích thước khác, cụ thể là opt-2.7b và opt-13b. Như minh họa trong Bảng 3, hai mô hình nhỏ hơn thể hiện tỷ lệ tăng tốc cao hơn khi sử dụng phương pháp của chúng tôi so với phương pháp vanilla, bất kể kích thước batch khác nhau. Với kích thước batch là 12, mô hình opt-13b đạt được tốc độ tăng tốc 1.62 lần, trong khi phương pháp vanilla không thể hiện tăng tốc và bị phương pháp giải mã greedy vượt qua. Như Hình 4 cho thấy, tỷ lệ đệm trung bình ở đây vượt quá 115%, làm nổi bật lý do chính cho sự kém hiệu quả của phương pháp vanilla.

### 4.3 Thí nghiệm sử dụng Dự đoán Mô hình Nháp

Trong phần này, phương pháp dự đoán mô hình nháp được áp dụng làm phương pháp cơ bản của giải mã suy đoán. Điều quan trọng cần lưu ý là khi sử dụng cách tiếp cận dự đoán mô hình nháp, số lượng dự đoán cho mỗi mẫu là giống nhau. Do đó, chỉ có "bộ nhớ đệm KV không đệm" được sử dụng trong phần này.

Như minh họa trong Bảng 4, chúng tôi sử dụng mô hình opt-125m làm mô hình nháp, và thử nghiệm ba mô hình có kích thước khác nhau. Phương pháp của chúng tôi thể hiện tốc độ tăng tốc vượt trội so với phương pháp vanilla trên các mô hình đa dạng và kích thước batch khác nhau. Như minh họa trong Hình 4, mô hình opt-6.7b, với kích thước batch được đặt thành 8, thể hiện sự gia tăng đáng kể trong số lượng token đệm, vượt quá 60% khi sử dụng LLMA và vượt quá 20% khi sử dụng dự đoán mô hình nháp. Điều này giải thích tại sao phương pháp vanilla có sự suy giảm tốc độ tăng tốc nghiêm trọng trong các trường hợp đa mẫu.

### 4.4 Phân tích về Sự Giảm Tốc độ Tăng tốc với Đa mẫu

Như minh họa trong Bảng 2, rõ ràng tỷ lệ tăng tốc thể hiện sự suy giảm trong bối cảnh nhiều mẫu. Khi kích thước batch được đặt thành 4, tỷ lệ tăng tốc là 2.83, trong khi khi kích thước batch được đặt thành 16, tỷ lệ tăng tốc là 1.63. Kết luận tương tự có thể được rút ra từ Bảng 4.

Chúng tôi đã xác định được hai yếu tố góp phần vào việc giảm tỷ lệ tăng tốc. Trước tiên, khi kích thước batch đủ lớn và nhiều token được xử lý đồng thời, thời gian suy luận riêng lẻ cho LLM tăng đáng kể. Như minh họa trong Hình 5, thời gian suy luận cho mô hình opt-6.7b với kích thước batch 16 là 22.6 mili giây để xử lý năm token mỗi mẫu, trong khi đối với một token duy nhất, nó là 16.6 mili giây, chậm hơn 1.36 lần.

Thứ hai, lý do chính cho sự suy giảm hiệu suất này là sự khác biệt đáng kể trong tốc độ tăng tốc giữa các mẫu khác nhau. Độ dài chấp nhận trung bình có tương quan tích cực với tỷ lệ tăng tốc. Như minh họa trong Hình 6(a), sự khác biệt độ dài chấp nhận trung bình của opt-2.7b/13b trên các mẫu khác nhau lớn hơn so với opt-6.7b khi phương pháp LLMA được sử dụng. Hình 6(c) cho thấy sự khác biệt trong độ dài chấp nhận trung bình giữa các mô hình tương đối khiêm tốn khi sử dụng mô hình nháp để dự đoán.

Khi kích thước batch tăng, độ dài chấp nhận trung bình tối thiểu trong batch giảm. Như minh họa trong Hình 6, so sánh các mô hình opt-6.7b và opt-2.7b cho thấy tỷ lệ tăng tốc của mô hình sau không đều hơn trên các mẫu thử nghiệm. Khi kích thước batch tăng, độ dài chấp nhận trung bình tối thiểu trong batch giảm với tốc độ nhanh hơn, mặc dù tỷ lệ tăng tốc của chúng tương tự khi kích thước batch bằng một.

Để duy trì tỷ lệ tăng tốc trong các trường hợp nhiều mẫu, phương pháp đơn giản nhất là đảm bảo tỷ lệ tăng tốc của các mẫu khác nhau tương tự dưới phương pháp giải mã suy đoán cơ bản. Tuy nhiên, giải pháp tối ưu cho vấn đề này là batching động (Yu et al., 2022), bao gồm việc thay thế một mẫu đã hoàn thành trong batch bằng một mẫu mới sau khi nó đã hoàn thành, thay vì chờ tất cả các mẫu trong batch hoàn thành trước khi tiến hành suy luận tiếp theo. Việc triển khai batching động dự kiến sẽ tăng cường hiệu quả xử lý đa mẫu, với tiềm năng đạt được tốc độ tăng tốc có thể so sánh với trường hợp một mẫu.

## 5 Kết luận

Trong bài báo này, chúng tôi trình bày nghiên cứu đầu tiên về giải mã suy đoán đa mẫu. Chúng tôi giới thiệu một phương pháp hiệu quả, được gọi là EMS-SD. EMS-SD là một giải pháp hiệu quả cho vấn đề không nhất quán của các mẫu khác nhau trong giai đoạn dự đoán và xác minh, mà không cần token đệm. Phương pháp đề xuất được tích hợp linh hoạt với hầu như bất kỳ phương pháp giải mã suy đoán cơ bản nào. Các so sánh rộng rãi cho thấy EMS-SD thể hiện hiệu suất vượt trội so với phương pháp vanilla trong giải mã suy đoán đa mẫu.

## Hạn chế

Công trình này có bốn hạn chế: 1) Bằng chứng lý thuyết chỉ ra rằng batching động có thể giúp giảm thiểu sự suy giảm hiệu suất xảy ra trong giải mã suy đoán đa mẫu. Tuy nhiên, điều này chưa được xác nhận thực nghiệm. Các thí nghiệm tiếp theo sẽ đánh giá hiệu quả của giải mã suy đoán đa mẫu kết hợp với batching động. 2) Tác động tiêu cực tiềm ẩn của truy cập bộ nhớ không liền kề đối với hiệu suất không được xem xét. Trong giải mã greedy theo batch, truy cập bộ nhớ giữa các mẫu khác nhau là liên tục. Tuy nhiên, trong phương pháp đề xuất, do độ dài khác nhau của các mẫu khác nhau, truy cập bộ nhớ không liên tục. Điều này có thể có tác động tiêu cực đến tăng tốc. 3) Mặc dù phương pháp của chúng tôi độc lập với framework suy luận, chúng tôi chưa triển khai phương pháp của mình trên các framework như PyTorch (Paszke et al., 2019) hoặc vLLM (Kwon et al., 2023). Điều này chắc chắn hạn chế tính dễ sử dụng của phương pháp chúng tôi. Trong công việc tiếp theo, chúng tôi sẽ xem xét triển khai phương pháp của mình trong những framework này. 4) Giải mã cây sẽ tăng tốc thêm giải mã suy đoán, điều này đã được xác minh rộng rãi trong giải mã suy đoán một mẫu (Miao et al., 2023; Cai et al., 2024; Liu et al., 2024; Li et al., 2024). Tuy nhiên, hiệu quả của việc tích hợp giải mã cây với lý luận suy đoán đa mẫu vẫn chưa được xác nhận. Các thí nghiệm trong tương lai sẽ đánh giá hiệu quả của giải mã suy đoán đa mẫu khi được tích hợp với giải mã cây.
