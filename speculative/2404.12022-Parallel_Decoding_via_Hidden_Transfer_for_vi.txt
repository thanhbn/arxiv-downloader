# Giải mã song song thông qua truyền ẩn cho việc tăng tốc mô hình ngôn ngữ lớn không mất mát

Pengfei Wu1,2∗, Jiahao Liu3∗, Zhuocheng Gong1, Qifan Wang4, Jinpeng Li1
Jingang Wang3, Xunliang Cai3, Dongyan Zhao1,2,5,6 †
1Viện Công nghệ Máy tính Wangxuan, Đại học Bắc Kinh
2Trung tâm Khoa học Dữ liệu, AAIS, Đại học Bắc Kinh;3Meituan;4Meta AI
5Phòng thí nghiệm Quốc gia về Trí tuệ Nhân tạo Tổng quát;6BIGAI, Bắc Kinh, Trung Quốc
{pengfeiwu1999,lijinpeng}@stu.pku.edu.cn
{liujiahao12,wangjingang02,caixunliang}@meituan.com
wqfcr@fb.com ,{zhaody,gzhch}@pku.edu.cn

## Tóm tắt

Các mô hình ngôn ngữ lớn (LLM) gần đây đã cho thấy hiệu suất đáng chú ý trên nhiều tác vụ khác nhau. Tuy nhiên, số lượng tham số đáng kể trong LLM góp phần vào độ trễ lớn trong quá trình suy luận mô hình. Điều này đặc biệt rõ ràng khi sử dụng các phương pháp giải mã tự hồi quy, tạo ra một token trong một quá trình tiến duy nhất, do đó không tận dụng hoàn toàn khả năng tính toán song song của GPU. Trong bài báo này, chúng tôi đề xuất một phương pháp giải mã song song mới, cụ thể là truyền ẩn, giải mã nhiều token liên tiếp đồng thời trong một lần truyền tiến duy nhất. Ý tưởng là truyền các trạng thái ẩn trung gian của ngữ cảnh trước đó đến các trạng thái ẩn giả của các token tương lai sẽ được tạo, và sau đó các trạng thái ẩn giả sẽ truyền qua các lớp transformer tiếp theo từ đó thu nhận thêm thông tin ngữ nghĩa và đạt được độ chính xác dự đoán vượt trội của các token tương lai.

Ngoài ra, chúng tôi sử dụng cơ chế chú ý cây mới để đồng thời tạo và xác minh nhiều ứng viên của chuỗi đầu ra, điều này đảm bảo việc tạo không mất mát và cải thiện thêm hiệu quả tạo của phương pháp chúng tôi. Các thí nghiệm chứng minh hiệu quả của phương pháp chúng tôi. Chúng tôi tiến hành nhiều thí nghiệm phân tích để chứng minh động lực của chúng tôi. Về các chỉ số tăng tốc, chúng tôi vượt trội hơn tất cả các kỹ thuật tăng tốc mô hình đơn, bao gồm Medusa và Self-Speculative decoding.

## 1 Giới thiệu

Những phát triển gần đây trong các mô hình ngôn ngữ lớn dựa trên Transformer (Vaswani et al., 2017; Radford et al., 2018, 2019; Brown et al., 2020; Zeng et al., 2022; Zhang et al., 2022; Touvron et al., 2023a,b; Roziere et al., 2023) đã chứng minh hiệu suất đáng chú ý trên một phổ rộng các tác vụ. Tuy nhiên, những mô hình này phải vật lộn với độ trễ suy luận quá mức do quá trình tạo một token cho mỗi lần truyền tiến vốn có tính nối tiếp, quá trình suy luận thường bị giới hạn bởi băng thông bộ nhớ, có nghĩa là hầu hết thời gian suy luận mô hình được dành để tải hàng tỷ tham số từ bộ nhớ thay vì tính toán, dẫn đến lãng phí sức mạnh tính toán song song của GPU (Shazeer, 2019). Trong các thí nghiệm của chúng tôi, chúng tôi thấy rằng do tính song song của tính toán GPU, thời gian để nhiều token truyền song song gần như bằng thời gian để một token truyền (như được hiển thị trong Bảng 1). Do đó, hiệu quả thấp của giai đoạn suy luận trở thành nút thắt cổ chai lớn nhất để mở rộng các tình huống ứng dụng của LLM.

Để giải quyết nút thắt cổ chai này, các công trình đương đại đề xuất giải mã suy đoán (Leviathan et al., 2023; Chen et al., 2023a; Zhang et al., 2023), sử dụng một mô hình ngôn ngữ nhỏ để phác thảo một vài token trước. sau đó LLM xác minh các token được phác thảo và chấp nhận những token đúng. Mặc dù kỹ thuật tăng tốc này đạt được hiệu suất hứa hẹn, nó có những hạn chế: giải mã suy đoán yêu cầu một mô hình khác để thực hiện việc phác thảo. Việc hợp tác với một mô hình bổ sung trong một số tình huống là bất tiện vì nó đòi hỏi việc lập lịch phức tạp hơn và mô hình phác thảo có thể tiêu thụ thêm tài nguyên GPU và bộ nhớ. Do đó, một số nhà nghiên cứu đã tìm hiểu tăng tốc mô hình đơn. Đó là, tăng tốc suy luận của LLM mà không có mô hình phụ trợ. Self-speculative decoding (Zhang et al., 2023) và Medusa (Cai et al., 2023) là các phương pháp điển hình trong dòng nghiên cứu này. Medusa dự đoán không chỉ token tiếp theo trong một lần truyền tiến duy nhất mà còn một vài token trước token tiếp theo. Những token bổ sung này được dự đoán dựa trên các trạng thái ẩn cuối cùng của các token đầu vào thông qua các đầu Medusa có thể huấn luyện.

Phương pháp của chúng tôi phù hợp với dòng tăng tốc mô hình đơn. Chúng tôi thiết kế một phương pháp mới gọi là Truyền ẩn, để dự đoán các trạng thái ẩn giả của các token tương lai trong các lớp trung gian. Chúng tôi sử dụng một phép chiếu tuyến tính có thể huấn luyện để truyền các trạng thái ẩn của các token đầu vào đến các trạng thái ẩn giả của các token tương lai trong một lớp trung gian nhất định, và các trạng thái ẩn giả được tổng hợp truyền qua các lớp tiếp theo và tương tác với các trạng thái ẩn của toàn bộ chuỗi như bình thường, trong lớp cuối cùng, chúng tôi sử dụng lm-head gốc và giải mã các token phác thảo của các vị trí tương lai. Bằng cách này, chúng tôi có thể dự đoán nhiều hơn chỉ token tiếp theo mà còn một vài token trước trong một lần truyền tiến duy nhất. Trong giai đoạn huấn luyện, chúng tôi sử dụng KL-divergence làm tín hiệu giám sát, giảm thiểu phân phối giữa các token được dự đoán bởi các trạng thái ẩn giả và các token thực. Ngoài thiết kế mới của Truyền ẩn, chúng tôi cũng sử dụng cơ chế chú ý cây (Cai et al., 2023; Miao et al., 2023; Spector and Re, 2023) để đồng thời thực hiện dự đoán token và xác minh token để đảm bảo việc tạo không mất mát của phương pháp chúng tôi.

Động lực cho truyền ẩn là các trạng thái ẩn giả được tổng hợp sẽ tương tác với chính chúng và các trạng thái ẩn trước đó của ngữ cảnh trong quá trình truyền tiến trong đó thu được thêm thông tin ngữ nghĩa để tăng cường tỷ lệ thành công dự đoán các token tương lai. Các thí nghiệm của chúng tôi cho thấy rằng động lực này được thực hiện, và phương pháp của chúng tôi có thể đạt được độ chính xác dự đoán token phác thảo tốt nhất và tỷ lệ tăng tốc suy luận so với các phương pháp khác trong thiết lập mô hình đơn.

Những đóng góp chính của chúng tôi là: (1) Theo hiểu biết tốt nhất của chúng tôi, chúng tôi là những người đầu tiên nghiên cứu việc dự đoán các trạng thái ẩn giả của các token tương lai trong LLM, các thí nghiệm của chúng tôi chứng minh rằng các trạng thái ẩn trung gian có thể được dự đoán trực tiếp và tinh chỉnh trong quá trình truyền tiến. (2) Chúng tôi đề xuất Truyền ẩn, một phương pháp tăng tốc không mất mát mô hình đơn mới để cải thiện hiệu quả suy luận của LLM. Phương pháp của chúng tôi dự đoán nhiều token phác thảo với các trạng thái ẩn giả tổng hợp. (3) Chúng tôi tiến hành các thí nghiệm khác nhau để chứng minh hiệu quả của phương pháp chúng tôi, bao gồm một số thí nghiệm phân tích để chứng minh động lực của chúng tôi.

## 2 Công trình liên quan

Để giải quyết vấn đề độ trễ suy luận trong LLM, các công trình hiện có có thể được chia thành hai loại sau: chúng tôi gọi loại đầu tiên là Nén mô hình, bao gồm chưng cất mô hình (Sanh et al., 2019), cắt tỉa mô hình (Frantar and Alistarh, 2023; Wang et al., 2021) và lượng tử hóa mô hình (Liu et al., 2023a), nhằm thay thế mô hình ngôn ngữ lớn gốc bằng một mô hình nhỏ có đầu ra tương tự nhưng không giống hệt trong lĩnh vực nhất định. Chúng tôi gọi loại phương pháp thứ hai là Giải mã suy đoán, ý tưởng chính là giảm số lượng truyền tiến của LLM dưới điều kiện kết quả được tạo ra không thay đổi.

### 2.1 Nén mô hình

Có nhiều công trình tập trung vào việc làm nhẹ mô hình, bao gồm lượng tử hóa mô hình (Han et al., 2015; Zhao et al., 2019; Jacob et al., 2018; Yao et al., 2022; Frantar et al., 2022; Liu et al., 2023a), chưng cất kiến thức (Hinton et al., 2015; Cho and Hariharan, 2019; Hsieh et al., 2023), cắt tỉa mô hình (Xia et al., 2023; Guo et al., 2023; Chen et al., 2023b) và làm thưa mô hình (Hoefler et al., 2021; Liu et al., 2023b). Lượng tử hóa mô hình là chuyển đổi các tham số mô hình thành các số dấu phẩy động có độ chính xác thấp hơn hoặc số nguyên; Cắt tỉa và làm thưa mô hình loại bỏ các thành phần dư thừa trong LLM; Chưng cất kiến thức hoạt động bằng cách chuyển giao kiến thức từ mô hình giáo viên đến mô hình học sinh (mô hình nhỏ). Các phương pháp nén mô hình này có phạm vi ứng dụng rộng, nhưng chúng không đảm bảo rằng đầu ra hoàn toàn nhất quán với LLM gốc.

### 2.2 Giải mã suy đoán

Chuỗi phương pháp này nhằm nhanh chóng tạo ra một số token phác thảo và sử dụng LLM để xác minh chúng song song để duy trì việc tạo không mất mát về mặt lý thuyết. Chúng ta có thể chia loại phương pháp này thành hai loại dựa trên số lượng mô hình được triển khai, mô hình đơn và mô hình đa. Phương pháp mô hình đơn được đại diện bởi Medusa (Cai et al., 2023), và phương pháp Self-speculative decoding (Zhang et al., 2023), Medusa và huấn luyện thêm nhiều đầu để dự đoán các token tiếp theo dựa trên trạng thái ẩn cuối cùng của các token đầu vào sau một lần truyền tiến duy nhất. Các phương pháp Self-speculative sử dụng một tập con các lớp trung gian của toàn bộ LLM làm mô hình phác thảo để tạo các token phác thảo. Phương pháp mô hình đa được đại diện bởi giải mã suy đoán truyền thống (Leviathan et al., 2023; Chen et al., 2023a), sử dụng một mô hình ngôn ngữ nhỏ (SLM) làm mô hình phác thảo để tạo các token phác thảo, LLM xác minh các token song song.

## 3 Phương pháp luận

Trong phần này, đầu tiên chúng tôi định nghĩa công thức bài toán, bao gồm tổng quan về thuật toán giải mã tự hồi quy truyền thống và thuật toán giải mã song song, sau đó chúng tôi cung cấp mô tả chi tiết về quá trình huấn luyện và kiểm tra của phương pháp truyền ẩn của chúng tôi.

### 3.1 Công thức bài toán

Các mô hình ngôn ngữ tự hồi quy dựa trên Transformer nhằm xây dựng phân phối của token thứ (n+1) cho trước n token tiền tố, ký hiệu là P(xn+1|x1, x2, ..., xn). Những mô hình này có khả năng xử lý toàn bộ chuỗi song song trong giai đoạn huấn luyện. Tuy nhiên, trong giai đoạn suy luận, quá trình tạo trở nên nối tiếp một cách tự nhiên. Điều này là do yêu cầu thông tin ngữ nghĩa của n token trước đó để dự đoán phân phối xác suất của token thứ (n+1). Các kỹ thuật tạo tự hồi quy truyền thống, theo đó một token duy nhất được tạo ra cho mỗi quá trình tiến của mô hình, không tận dụng được khả năng xử lý song song của GPU, do đó ảnh hưởng đến hiệu quả phản hồi của các hệ thống AI khác nhau. Bản chất của các thuật toán giải mã song song, được minh họa bởi giải mã suy đoán, nằm ở tham vọng của chúng là tăng số lượng token dự kiến được tạo ra trong một lần truyền tiến duy nhất của LLM trong khi duy trì tính nhất quán của việc tạo. Do đó, mục tiêu tối ưu hóa của loại phương pháp này có thể được viết để tìm một số nguyên dương tối đa k thỏa mãn các điều kiện sau:

˜P(Xn+k+1...Xn+1|X≤n) = P(Xn+k+1...Xn+1|X≤n)

Trong đó P và ˜P đại diện cho phân phối token được đưa ra bởi mô hình ngôn ngữ gốc và thuật toán giải mã song song tương ứng. Đối với hầu hết các thuật toán giải mã song song, ˜P được thu được bởi một quá trình phác thảo và xác minh, chú ý cây được áp dụng rộng rãi để xác minh nhiều chuỗi ứng viên đồng thời, vì vậy nhiều công trình tập trung vào cách tạo ra các ứng viên tốt hơn trong giai đoạn phác thảo, quá trình có thể được mô tả như công thức sau:

Xn+k+1, ..., Xn+1 = M(X≤n)

Trong một số thuật toán giải mã suy đoán, M đại diện cho các mô hình ngôn ngữ nhỏ, hoặc một phần của LLM gốc (Zhang et al., 2023), trong chuỗi phương pháp block-wise (được đại diện bởi Medusa (Cai et al., 2023)), M có thể được xem như các đầu bổ sung trong lớp cuối cùng của LLM. Trong phương pháp của chúng tôi, M có thể được xem như phép chiếu tuyến tính truyền ẩn trong một số lớp trung gian, trong phần tiếp theo, chúng tôi giới thiệu ngắn gọn phương pháp của chúng tôi, bao gồm các giai đoạn huấn luyện và suy luận.

### 3.2 Truyền ẩn

Ý tưởng cốt lõi của truyền ẩn của chúng tôi là dự đoán k+1 token tương lai (bao gồm k token phác thảo và token tiếp theo được tạo chính xác bởi mô hình ngôn ngữ) bằng một bước truyền tiến LLM mà không triển khai SLM, các công trình hiện có trong thiết lập này hoặc sử dụng phương pháp thoát sớm để dự đoán trực tiếp phân phối token (Bae et al., 2023) hoặc huấn luyện thêm k lm-head để dự đoán k token phác thảo trong lớp cuối cùng, chúng tôi tin rằng phương pháp đầu tiên sẽ mất nhiều thông tin ở các lớp cao hơn, giả sử chúng ta sử dụng Xn để đại diện cho token thứ n của chuỗi đầu vào, và hjn đại diện cho trạng thái ẩn lớp thứ j của token thứ n, phương pháp đầu tiên huấn luyện một lm-head sớm để dự đoán Xn+1 trước, nhưng một khi Xn+1 được dự đoán, nó phải được sử dụng làm đầu vào trong vòng truyền tiến tiếp theo, quá trình truyền tiến trước đó sẽ dừng ở lớp giữa, kết quả là trạng thái ẩn lớp cao hơn của Xn sẽ bị mất (tức là hj+1n, hj+2n...), mặc dù có một số công trình tuyên bố rằng họ có thể đơn giản sử dụng cơ chế sao chép để mô phỏng trạng thái ẩn của các lớp cao hơn (Elbayad et al., 2019), nhưng các thí nghiệm khác vẫn cho thấy rằng cơ chế sao chép hoạt động kém trong một số trường hợp (Bae et al., 2023). Phương pháp thứ hai sử dụng các lm-head bổ sung để dự đoán k token phác thảo tương lai rất đơn giản và hiệu quả, nhưng chúng tôi tin rằng phương pháp này thiếu sự tương tác của k token phác thảo và các token trước đó vì các token phác thảo không tương tác với các token trước đó thông qua cơ chế chú ý, ví dụ nó chỉ sử dụng các trạng thái ẩn cuối cùng của Xn để dự đoán Xn+2 mà không sử dụng thông tin của Xn+1, nhưng đôi khi Xn+2 phụ thuộc vào Xn+1, vì vậy phương pháp của chúng tôi chọn huấn luyện nhiều hàm truyền (các phép chiếu tuyến tính đơn giản) để dự đoán các trạng thái ẩn của k token phác thảo tương lai trong một số lớp trung gian bằng cách ánh xạ hjn đến hjn+1...hjn+k, vì vậy chúng ta có thể tiếp tục truyền tiến với n+k trạng thái ẩn trung gian. Trong các lớp transformer tiếp theo, k trạng thái ẩn cuối cùng sẽ truyền qua lm-head gốc để dự đoán phân phối token bình thường, toàn bộ quá trình huấn luyện và suy luận so sánh với Truyền ẩn và Medusa được hiển thị trong Hình 2.

### 3.3 Giai đoạn huấn luyện

Ở giai đoạn huấn luyện, cần phải huấn luyện nhiều phép chiếu tuyến tính trong nhiều lớp cố định, trong đó vị trí của các lớp truyền tương ứng và số bước truyền được coi là siêu tham số. Số bước truyền bằng số trạng thái ẩn giả được dự đoán trong một quá trình tiến duy nhất cho một token; do đó, chúng tôi ký hiệu số này là k. Bởi vì chúng tôi huấn luyện k phép chiếu tuyến tính riêng biệt nên chúng tôi cần tiến hành quá trình huấn luyện k lần (chúng tôi huấn luyện một phép chiếu tuyến tính cho một bước truyền trong một lớp nhất định). Để đơn giản, chúng tôi thảo luận về quá trình huấn luyện của bước truyền thứ i, chúng tôi ký hiệu chỉ số của lớp truyền cho bước i là ti. vì vậy chúng ta có thể sử dụng Witi ∈ Rd×d (1≤i≤k) để ký hiệu phép chiếu tuyến tính có thể huấn luyện cho bước thứ i (d đại diện cho chiều ẩn của LLM). Giả sử chúng ta có các chuỗi token gốc X1, X2, ..., Xn, đầu tiên chúng ta thực hiện quá trình tiến đến lớp ti để nhận các trạng thái ẩn tương ứng hti1, hti2, ..., htin, sau đó chúng ta truyền tất cả n trạng thái ẩn thành các trạng thái ẩn giả tương ứng, có thể được công thức hóa như dưới đây:

êhtin, êhtin-1, ..., êhti1 = Witi · (htin, htin-1, ..., hti1)

Sau khi truyền htij thành êhtij (1≤j≤n), chúng ta nối các trạng thái ẩn gốc và các trạng thái ẩn giả thành một chuỗi mới (tức là hti1, ..., htin-1, htin, êhti1, ..., êhtin-1, êhtin). Dễ dàng thấy rằng êhtij là trạng thái ẩn giả của hij+1, vì vậy trong các lớp tự chú ý nó chỉ có thể xem các trạng thái ẩn từ hti1 đến htij+i-1 và chính nó trong chuỗi, chúng tôi thiết kế mask chú ý để đạt được mục tiêu. Để đảm bảo tính nhất quán giữa giai đoạn huấn luyện và giai đoạn suy luận, chúng tôi đặt position id j+i cho êhtij để xây dựng embedding vị trí.

Sau đó chúng ta tiếp tục truyền tiến chuỗi mới, và nhận được các biểu diễn cuối cùng của lớp cuối cùng (tức là hl1, ..., hln-1, hln, êhl1, ..., êhln-1, êhln, trong đó l ký hiệu số lớp của LLM). Sau đó chúng ta truyền chuỗi trạng thái ẩn qua lm-head gốc và cuối cùng nhận được phân phối token của mỗi vị trí (Pl1, ..., Pln-1, Pln, êPl1, ..., êPln-1, êPln). Chúng tôi sử dụng KL-divergence giữa các phân phối token được đưa ra bởi các trạng thái ẩn giả và các trạng thái ẩn gốc làm tín hiệu giám sát. Mất mát có thể được công thức hóa như dưới đây:

Lossdistll = Σq=1n-i-1 KL-divergence (êPln+q, Plq+i)

### 3.4 Giai đoạn suy luận

Trong giai đoạn suy luận, quá trình là đầu tiên tạo ra một số ứng viên chuỗi và sau đó xác minh chúng, mục đích của giai đoạn xác minh là đảm bảo tính nhất quán token với giải mã tự hồi quy. Chúng tôi xây dựng nhiều ứng viên chuỗi thành cấu trúc cây bằng cách hợp nhất các tổ tiên chung của chúng, sau đó chúng tôi làm phẳng cây thành chuỗi và xây dựng mask chú ý chính xác để duy trì thứ tự của chúng, cuối cùng chúng tôi gửi toàn bộ chuỗi vào LLM để xác minh các ứng viên và tạo ra các ứng viên mới cùng lúc. Hình 2 hiển thị giai đoạn suy luận của cả Truyền ẩn và Medusa.

#### 3.4.1 Chú ý cây

Khi dự đoán các token phác thảo cho một số bước tiếp theo, rõ ràng là các token phác thảo thuộc về các bước khác nhau tạo thành cấu trúc cây theo thứ tự của chúng. Cơ chế chú ý cây biến đổi cây phân cấp này thành chuỗi tuyến tính trong khi bảo tồn các chỉ số vị trí gốc của mỗi token. Hơn nữa, nó sử dụng mask chú ý chuyên biệt để đảm bảo rằng một token chỉ chú ý đến các tổ tiên của nó trong cấu trúc cây, từ đó duy trì các thuộc tính của mô hình ngôn ngữ nhân quả. Trong quá trình suy luận, cấu trúc cây được định nghĩa trước và parser tạo điều kiện cho việc biến đổi nhanh chóng các ứng viên token giữa các biểu diễn cây và chuỗi, loại bỏ nhu cầu tính toán bổ sung.

## 4 Thí nghiệm

### 4.1 Thiết lập

Chúng tôi đánh giá phương pháp của mình trên hai chuỗi mô hình khác nhau với kích thước khác nhau, bao gồm LLaMA-2-CHAT-13B (Touvron et al., 2023b), LLaMA-2-CHAT-7B (Touvron et al., 2023b) và Vicuna-13B (Chiang et al., 2023), Vicuna-7B (Chiang et al., 2023). Chúng tôi sử dụng chiến lược lấy mẫu tham lam cho LLM và các token được tạo bằng phương pháp của chúng tôi giống hệt với những token được tạo bởi giải mã tự hồi quy tiêu chuẩn về mặt lý thuyết. Chúng tôi chia các thí nghiệm thành phần thí nghiệm chính, nghiên cứu phân tích và loại bỏ. Các thí nghiệm chính được tiến hành trên tất cả các mô hình và nghiên cứu phân tích và loại bỏ chỉ được tiến hành trên mô hình 7B để đơn giản.

Trong phần thí nghiệm chính, chúng tôi so sánh tỷ lệ tăng tốc thời gian end-to-end của chúng tôi với Medusa (Cai et al., 2023) và Self-speculative decoding (Zhang et al., 2023) để thể hiện hiệu quả của phương pháp chúng tôi (chi tiết hơn trong Phụ lục). Bởi vì phương pháp self-speculative decoding (Zhang et al., 2023) cần cẩn thận lựa chọn các lớp transformer bị bỏ qua cho mỗi mô hình, và nó không cung cấp các lớp bỏ qua ban đầu của mô hình 7B trong mã nguồn mở của họ cho thuật toán tối ưu hóa của họ, vì vậy chúng tôi chỉ so sánh với họ trên LLaMA-2-Chat-13B và vicuna-13B, chúng tôi sử dụng tỷ lệ tăng tốc được báo cáo trong bài báo của họ về LLaMA-2-Chat-13B cho dataset Xsum, và chạy mã nguồn mở của họ để đánh giá hiệu quả của họ trên dataset Gsm8K và vicuna-13B. chúng tôi sử dụng cùng cấu trúc Cây và dự đoán ba token phác thảo tiếp theo (loại trừ token tiếp theo được tạo bởi lm-head gốc) cho Medusa và phương pháp của chúng tôi, và chúng tôi thực hiện truyền ẩn cho phương pháp của chúng tôi trên các lớp 25, 30, 35 tương ứng.

Phần thí nghiệm phân tích và nghiên cứu loại bỏ nhằm xác minh động lực của chúng tôi. Vì vậy, đầu tiên chúng tôi so sánh độ chính xác dự đoán token phác thảo giữa phương pháp của chúng tôi và hai baseline (ví dụ: đầu Medusa và Early existing trên các lớp trung gian khác nhau), kết quả cho thấy chúng tôi có độ chính xác dự đoán tốt nhất cho các token phác thảo tương lai trong một lần truyền tiến duy nhất; Để xác minh động lực của chúng tôi, chúng tôi cũng phân tích cách độ tương tự trạng thái ẩn giữa các trạng thái ẩn giả được dự đoán và các trạng thái ẩn gốc thay đổi cùng với quá trình truyền tiến và chứng minh việc tinh chỉnh của các lớp transformer, cuối cùng chúng tôi huấn luyện nhiều phép chiếu truyền trên các lớp khác nhau cho các bước truyền khác nhau để khám phá cách lựa chọn các lớp truyền. Cuối cùng, chúng tôi cũng so sánh độ chính xác dự đoán truyền của bước truyền thứ hai giữa thiết lập các trạng thái ẩn giả đầu tiên bị che hoặc không trong nghiên cứu loại bỏ.

Tất cả các thí nghiệm được tiến hành trên một GPU NVIDIA A100-80GB duy nhất và tất cả các triển khai đều dựa trên PyTorch sử dụng kiến trúc của HuggingFace (Wolf et al., 2020; Lhoest et al., 2021).

### 4.2 Bộ dữ liệu

Chúng tôi sử dụng bộ dữ liệu ShareGPT làm bộ dữ liệu huấn luyện của chúng tôi cho tất cả các mô hình, và chúng tôi sử dụng tập kiểm tra của Extreme Summarization (XSum) (Narayan et al., 2018), Gsm8k (Cobbe et al., 2021) làm bộ dữ liệu kiểm tra của chúng tôi. ShareGPT là bộ dữ liệu cuộc trò chuyện đa vòng bao gồm gần 70.000 mẫu, chúng tôi huấn luyện một epoch cho tất cả các mô hình. XSum (Narayan et al., 2018) là bộ dữ liệu để đánh giá các hệ thống tóm tắt tài liệu đơn trừu tượng, tập kiểm tra của nó có 11.334 mẫu. chúng tôi chỉ lấy mẫu 1000 câu theo (Zhang et al., 2023). Bộ dữ liệu Gsm8k (Cobbe et al., 2021) bao gồm bộ sưu tập 8.500 bài toán từ toán học đa dạng về ngôn ngữ, chất lượng cao cho học sinh tiểu học, tất cả đều được tạo ra một cách tỉ mỉ bởi các tác giả con người, chúng tôi sử dụng toàn bộ tập kiểm tra của nó với 1000 mẫu. Cả hai bộ dữ liệu đều được đánh giá dưới thiết lập 1-shot theo (Zhang et al., 2023).

### 4.3 Kết quả chính

Bảng 1 cho thấy tỷ lệ tăng tốc của phương pháp chúng tôi tốt hơn đáng kể so với các baseline khác về thời gian end-to-end cho tất cả bộ dữ liệu kiểm tra (chi tiết hơn trong phụ lục), nó có tỷ lệ tăng tốc tối đa 1.28x so với Medusa, tương tự như phương pháp của chúng tôi. Sử dụng truyền ẩn để dự đoán các trạng thái ẩn giả trong lớp trung gian mang lại nhiều lợi ích hơn trong hiệu suất tổng thể so với việc sử dụng đầu Medusa để dự đoán phân phối token trực tiếp, cải thiện này rõ ràng hơn trong các mô hình 7B, và chúng tôi thấy rằng tỷ lệ tăng tốc trên Gsm8k cao hơn vì câu trả lời trong Gsm8k có tính logic và có thể dự đoán được hơn với nhiều ký hiệu toán học hơn.

### 4.4 Nghiên cứu phân tích

Trong phần này, chúng tôi tiến hành một số thí nghiệm phân tích để xác minh thêm hiệu quả và động lực của phương pháp chúng tôi, ý tưởng chính của phương pháp chúng tôi là dự đoán các token phác thảo chính xác hơn trong một lần truyền tiến duy nhất bằng cách dự đoán các trạng thái ẩn giả trong các lớp trung gian. Vì vậy, đầu tiên chúng tôi so sánh phương pháp của chúng tôi với Medusa và early exiting để cho thấy chúng tôi có độ chính xác dự đoán token phác thảo tốt hơn; sau đó chúng tôi phân tích cách các trạng thái ẩn thay đổi trong quá trình truyền tiến để xác minh việc tinh chỉnh mà chúng tôi đề xuất. Chúng tôi cũng xác minh sự phụ thuộc dự đoán token phác thảo và cho thấy cách lựa chọn các lớp truyền.

**Độ chính xác dự đoán token phác thảo** Trong một lần truyền tiến duy nhất (cho chuỗi token X1, ...Xn và mô hình cần dự đoán K token phác thảo tương lai êXn+2, ... êXn+k+1), vì vậy đầu tiên chúng tôi so sánh độ chính xác dự đoán của các token phác thảo trên ba phương pháp khác nhau: early exiting trong các lớp trung gian, sử dụng đầu medusa và phương pháp truyền ẩn của chúng tôi. Phương pháp Early exiting huấn luyện các lm-head độc lập trong một số lớp transformer trung gian để dự đoán trực tiếp các token phác thảo (ví dụ huấn luyện một lm-head và sử dụng nó để ánh xạ hjn đến êXn+2). Chúng tôi sử dụng LLaMA-2-Chat-13B làm mô hình cơ sở cho thí nghiệm này, ba phương pháp khác nhau được huấn luyện trên bộ dữ liệu ShareGPT trong một epoch và chúng tôi đặt K là 3. Chúng tôi lấy mẫu ngẫu nhiên 100 chuỗi từ tập kiểm tra của bộ dữ liệu XSum và Gsm8K tương ứng, đối với mỗi chuỗi, chúng tôi chia ngẫu nhiên 50 điểm ở phần đầu ra của nó, và đối với mỗi điểm chia, chúng tôi sử dụng chuỗi token trước đó làm đầu vào prompt và dự đoán K token phác thảo bằng các phương pháp khác nhau và so sánh với các token được tạo tham lam bởi mô hình gốc trong K bước tiếp theo. (Chúng tôi chọn năm hạt giống ngẫu nhiên và lấy trung bình kết quả để loại bỏ tốt hơn các lỗi ngẫu nhiên) Hình 3 cho thấy phương pháp truyền ẩn của chúng tôi đạt được độ chính xác dự đoán tốt nhất trong số chúng.

**Tinh chỉnh trạng thái ẩn giả** chúng tôi tiến hành một thí nghiệm khác để chứng minh rằng quá trình truyền tiến trong các lớp transformer có thể tinh chỉnh các trạng thái ẩn được dự đoán bằng cách cung cấp thêm thông tin ngữ nghĩa sử dụng cơ chế tự chú ý. Chúng tôi so sánh độ tương tự cosine của các trạng thái ẩn giả được dự đoán với các trạng thái ẩn gốc, và theo dõi cách độ tương tự cosine thay đổi với quá trình tiến (ví dụ, chúng tôi ký hiệu chuỗi prompt là X1, ...Xn và chúng tôi tính độ tương tự cosine giữa trạng thái ẩn giả êhtn+1 và htn+1 thực, htn+1 là trạng thái ẩn của Xn+1 trên lớp thứ t và Xn+1 là token đầu ra giải mã tham lam cho ngữ cảnh tiền tố n). Chúng tôi lấy mẫu ngẫu nhiên 100 chuỗi cho hai bộ dữ liệu và chia ngẫu nhiên 50 lần cho mỗi chuỗi. Hình 4 cho thấy độ tương tự cosine trở nên gần nhau hơn cùng với quá trình tiến, chứng minh rằng với quá trình tiến, các trạng thái ẩn có thể được tinh chỉnh bởi các lớp transformer.

**Cách chọn lớp truyền** Trong giai đoạn suy luận của phương pháp chúng tôi, chúng ta cần chọn lớp nào để truyền và tạo ra các trạng thái ẩn giả, có sự đánh đổi giữa độ chính xác và hiệu quả: nếu chúng ta truyền trên các lớp thấp hơn, các trạng thái ẩn giả sẽ truyền qua nhiều lớp tiếp theo hơn và tốn nhiều tài nguyên tính toán hơn nhưng thu được thêm thông tin ngữ nghĩa bằng cách tương tác với các trạng thái ẩn của ngữ cảnh; nếu chúng ta truyền trên các lớp cao hơn, các trạng thái ẩn giả sẽ tốn ít tài nguyên tính toán hơn với ít thông tin ngữ nghĩa hơn. Vì vậy, chúng tôi huấn luyện bước truyền thứ nhất và thứ hai trên các lớp khác nhau của LLM cố định để nghiên cứu tác động của việc lựa chọn lớp truyền. Chúng tôi chọn mô hình Vicuna-7B và LlaMa-2-7b-chat. Đối với bước truyền đầu tiên, chúng tôi huấn luyện cấu trúc truyền khác nhau trên các lớp thứ 5, 10, 15, 20, 25 riêng biệt, và báo cáo độ chính xác dự đoán token trong hình 5, chúng tôi thấy rằng từ lớp thấp hơn đến lớp giữa, độ chính xác dự đoán về cơ bản không thay đổi, và từ lớp giữa đến lớp cao, độ chính xác dự đoán giảm nhanh chóng, chứng minh rằng lớp giữa để thực hiện truyền là lựa chọn tối ưu cho cả độ chính xác và hiệu quả tính toán, vì vậy chúng tôi chọn lớp thứ 15 để thực hiện bước truyền đầu tiên. Sau khi cố định lớp truyền đầu tiên, chúng tôi cũng huấn luyện các cấu trúc truyền khác nhau trên các lớp cao hơn nó làm lớp truyền thứ hai của chúng tôi. Hình 6 cho thấy bước truyền thứ hai có cùng quy tắc, bắt đầu từ lớp thứ 15, tỷ lệ chính xác không thay đổi trong một phạm vi, và sau đó giảm nhanh chóng, vì vậy chúng tôi chọn lớp thứ 20 để thực hiện bước truyền thứ hai.

### 4.5 Nghiên cứu loại bỏ

Rõ ràng là Medusa (Cai et al., 2023) dự đoán các token phác thảo song song có nghĩa là việc tạo giữa các token phác thảo khác nhau là độc lập. Động lực của chúng tôi là việc tạo nối tiếp các token phác thảo sẽ mang lại hiệu suất tốt hơn và chúng tôi sử dụng thí nghiệm để chứng minh điều đó. Chúng tôi so sánh độ chính xác dự đoán của bước truyền thứ hai dưới hai thiết lập: Masked và No masked, Masked có nghĩa là trạng thái ẩn giả thứ hai không thể nhìn thấy trạng thái ẩn giả đầu tiên trong quá trình tiến, và No masked là cơ chế tự chú ý bình thường. Hình 4 cho thấy trong tất cả các mô hình và bộ dữ liệu, Masked hoạt động kém hơn chứng minh rằng thông tin ngữ nghĩa của token phác thảo đầu tiên quan trọng đối với việc tạo token phác thảo thứ hai.

## 5 Kết luận

Trong bài báo này, chúng tôi giới thiệu một phương pháp giải mã song song mới, được gọi là truyền ẩn, được thiết kế để tăng tốc suy luận trong các mô hình ngôn ngữ lớn. Bằng cách huấn luyện một phép chiếu biến đổi tuyến tính trong các lớp trung gian, mô hình của chúng tôi có khả năng dự đoán các trạng thái ẩn giả của nhiều token tiếp theo trong một lần truyền tiến duy nhất. Những trạng thái ẩn được dự đoán này thu được thông tin ngữ nghĩa bổ sung thông qua các lớp transformer tiếp theo, dẫn đến độ chính xác dự đoán được cải thiện. Thông qua các thí nghiệm phân tích, chúng tôi đã chứng minh rằng các trạng thái ẩn được dự đoán bởi các lớp trung gian được tinh chỉnh dần dần, thu được thông tin ngữ nghĩa tăng dần trong các lớp transformer tiếp theo bằng cách tương tác với ngữ cảnh. Các thí nghiệm của chúng tôi chứng minh rằng phương pháp của chúng tôi vượt trội hơn các phương pháp hiện có về độ chính xác dự đoán trong một lần lặp tiến duy nhất và cũng đạt được những cải thiện đáng kể về tốc độ tạo.

## Hạn chế

Trong giai đoạn xác minh của phương pháp chúng tôi, chúng tôi chỉ đơn giản sử dụng chú ý cây vanilla mà không có tối ưu hóa cụ thể. Tuy nhiên, các lựa chọn cấu trúc của chú ý cây ảnh hưởng đáng kể đến tốc độ tạo. Do đó, công việc tương lai sẽ tập trung vào việc tối ưu hóa nó. Hơn nữa, chúng tôi nhằm tiến hành một bộ thí nghiệm phân tích rộng rãi hơn để làm sáng tỏ tốt hơn các cơ chế cơ bản của truyền ẩn và thiết kế các phương pháp huấn luyện được cải thiện để nâng cao chất lượng tạo token phác thảo. Phương pháp của chúng tôi đòi hỏi việc mở rộng chuỗi đầu vào trong cả quá trình huấn luyện và suy luận, điều này có thể dẫn đến sự gia tăng yêu cầu tài nguyên tính toán. Vấn đề này sẽ được giải quyết trong nghiên cứu tương lai.
