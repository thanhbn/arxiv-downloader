# 2402.15758.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/speculative/2402.15758.pdf
# Kích thước tệp: 1079123 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
KHAI THÁC VÀ PHÂN TÍCH DỮ LIỆU LỚN
ISSN2 22096-0654 ll0?/??llpp???–???
Tập 1, Số 1, Tháng 1 năm 2018
Tập 1, Số 1, Tháng 9 năm 2018
Chimera: Một Phương Pháp Giải Mã Không Mất Mát để Tăng Tốc Suy Luận Mô Hình Ngôn Ngữ Lớn bằng Cách Hợp Nhất Tất Cả Các Token

Ziqian Zeng†, Jiahong Yu†, Qianshi Pang, Zihao Wang, Huiping Zhuang, Hongen Shao, Xiaofeng Zou*

Tóm tắt: Các mô hình ngôn ngữ lớn (LLM) đã thể hiện khả năng đáng chú ý trên nhiều nhiệm vụ khác nhau. Tuy nhiên, việc ứng dụng rộng rãi của chúng bị cản trở bởi quá trình giải mã tiêu tốn nhiều tài nguyên. Để giải quyết thách thức này, các phương pháp hiện tại đã tích hợp thêm các đầu giải mã để cho phép dự đoán song song nhiều token tiếp theo, từ đó đạt được tăng tốc suy luận. Tuy nhiên, độ chính xác của các đầu giải mã này vẫn thấp hơn phương pháp giải mã tự hồi quy. Nhằm khắc phục những hạn chế này, chúng tôi đề xuất Chimera, một khung làm việc mới được thiết kế đặc biệt cho lấy mẫu suy đoán. Trong khung làm việc này, chúng tôi giới thiệu một mô hình dự thảo nhẹ sử dụng hiệu quả các token được tạo trước đó để dự đoán các từ tiếp theo. Để đảm bảo cả độ chính xác và hiệu quả, chúng tôi trình bày hai chiến lược trong mô hình dự thảo nhẹ. Thứ nhất, chúng tôi tập trung vào việc nắm bắt các phụ thuộc tầm ngắn ở lớp dưới cùng. Thứ hai, chúng tôi tận dụng các biểu diễn sẵn có từ LLM gốc. Thông qua đánh giá thực nghiệm trên dòng Vicuna và LlaMA-2-chat, Chimera cho thấy kết quả ấn tượng, đạt được tỷ lệ tăng tốc độ trễ trung bình 2.7x so với phương pháp giải mã tự hồi quy thông thường. Điều này làm nổi bật tiềm năng của khung làm việc được đề xuất trong việc cải thiện đáng kể hiệu quả của các mô hình ngôn ngữ lớn trong quá trình giải mã.

Từ khóa: giải mã không mất mát; tăng tốc suy luận; mô hình ngôn ngữ lớn

1 Giới thiệu

Sự hội tụ của thế giới mạng, vật lý và xã hội đã tạo ra khái niệm Trí tuệ Vật lý Xã hội Mạng (CPSI), cho phép phát triển các hệ thống thông minh và kết nối trên nhiều lĩnh vực khác nhau. Một thành phần chính của CPSI là Dữ liệu Lớn khổng lồ và phức tạp xuất hiện từ những môi trường tích hợp này. Khai thác giá trị của Dữ liệu Lớn CPSI này đòi hỏi việc sử dụng các kỹ thuật tính toán Dữ liệu Lớn tiên tiến.

Một thách thức quan trọng trong tính toán Dữ liệu Lớn CPSI là nhu cầu suy luận hiệu quả và hiệu suất cao của các mô hình ngôn ngữ lớn (LLM)[1–6], đóng vai trò then chốt trong việc trích xuất thông tin hữu ích và

•Ziqian Zeng, Jiahong Yu, Qianshi Pang, Huiping Zhuang, Hongen Shao, Xiaofeng Zou thuộc Đại học Công nghệ Nam Trung Quốc, Quảng Châu, 510641, Trung Quốc.
E-mail: zqzeng@scut.edu.cn; jiahongyugz@gmail.com; qianshipang@gmail.com; hpzhuang@scut.edu.cn; ftshaohongen@mail.scut.edu.cn; zouxiaofeng@scut.edu.cn
•Zihao Wang thuộc Đại học Khoa học và Công nghệ Hồng Kông, E-mail:zwanggc@cse.ust.hk
∗Tác giả liên hệ
†Ziqian Zeng và Jiahong Yu đóng góp bằng nhau cho công trình này.
Bản thảo nhận: 2024-04-20 thúc đẩy việc ra quyết định thông minh trong các hệ thống CPSI. LLM, với kiến thức rộng lớn và kiến trúc phức tạp, thường gặp phải độ trễ suy luận cao trong quá trình suy luận, tạo ra nút thắt trong phản hồi thời gian thực hoặc gần thời gian thực mà các ứng dụng CPSI yêu cầu.

Độ trễ này một phần phát sinh từ giải mã tự hồi quy, trong đó token được tạo ra ở mỗi bước phụ thuộc vào tất cả các token được tạo trước đó. Để giảm bớt vấn đề độ trễ vốn có của giải mã tự hồi quy, một phương pháp giải mã mới được gọi là giải mã suy đoán[7, 8] đã xuất hiện.

Quá trình giải mã suy đoán[7, 8] có thể được tóm tắt là "Dự thảo-rồi-Xác minh." Nó bao gồm hai mô hình: LLM gốc và mô hình dự thảo. Mô hình dự thảo đầu tiên tạo hiệu quả nhiều suy đoán bao gồm nhiều token tiếp theo. Sau đó, LLM gốc xác minh tất cả những suy đoán này để chấp nhận cái đáp ứng tiêu chí xác minh của LLM. Quá trình dự thảo và xác minh được lặp lại cho đến khi đáp ứng điều kiện kết thúc. Cho rằng mô hình dự thảo[7, 9] thường nhỏ hơn LLM gốc, nó có thể tạo nhiều token nhanh chóng, do đó đạt được tăng tốc đáng kể.arXiv:2402.15758v2 [cs.CL] 18 Apr 2024

--- TRANG 2 ---
2 Khai Thác và Phân Tích Dữ Liệu Lớn, Tháng 4 năm 2024, 1(1): 000-000

Tuy nhiên, việc thu thập và duy trì một mô hình dự thảo riêng biệt là thách thức trong các triển khai thực tế. Để giải quyết vấn đề này, Giải mã Song song Theo khối[8] và Medusa[10] đã thêm các đầu giải mã bổ sung trên đỉnh khối Transformer cuối cùng để dự đoán nhiều token tiếp theo song song. Mặc dù các phương pháp này đơn giản và hiệu quả, độ chính xác của các đầu giải mã thấp hơn đáng kể so với đầu giải mã tự hồi quy, được hỗ trợ bởi các phát hiện trình bày trong Bảng 5. Sự chênh lệch về độ chính xác này phát sinh từ việc LLM được huấn luyện để dự đoán từ tiếp theo dựa trên tất cả các từ được tạo trước đó, trong khi các đầu giải mã dự đoán từ tiếp theo dựa trên chuỗi tiền tố thiếu quyền truy cập vào các token được tạo bởi các đầu giải mã trước đó. Cụ thể, giả sử mô hình dự thảo sẽ tạo chuỗi suy đoán {i+1,···, i+k} cho chuỗi tiền tố {1,···, i}, đầu giải mã song song dự đoán từ thứ (i+k) dựa trên chuỗi tiền tố, không có quyền truy cập vào các token {i+ 1,···, i+k−1}. Quyền truy cập hạn chế vào toàn bộ ngữ cảnh góp phần vào độ chính xác thấp hơn quan sát được trong giải mã song song.

Hiệu ứng tăng tốc của giải mã suy đoán chủ yếu phụ thuộc vào số token dự thảo được chấp nhận mỗi lần xác minh. Cải thiện độ chính xác và tốc độ của mô hình dự thảo là một trong những yếu tố chính trong việc nâng cao tỷ lệ chấp nhận[11]. Để tăng cường độ chính xác của mô hình dự thảo mà không làm giảm đáng kể tốc độ, chúng tôi đề xuất một mô hình dự thảo nhẹ tận dụng hiệu quả tất cả các token được tạo trước đó để dự đoán từ tiếp theo.

Việc phát triển mô hình dự thảo nhẹ với năng lực tương đương LLM gốc là thách thức. Chúng tôi đề xuất hai chiến lược để đối phó với thách thức này.

Chiến lược đầu tiên là nắm bắt các phụ thuộc tầm ngắn thay vì tầm dài ở lớp dưới cùng của mô hình dự thảo nhẹ. Như được chỉ ra trong các nghiên cứu trước[12, 13], các lớp thấp hơn của mô hình ngôn ngữ được tiền huấn luyện chủ yếu tập trung vào các phụ thuộc tầm ngắn, trong khi các lớp sâu hơn có khả năng nắm bắt các phụ thuộc tầm dài. Chỉ nắm bắt các phụ thuộc tầm ngắn có thể giảm thời gian suy luận có liên quan cao đến số token trong chuỗi đầu vào. Do đó, chúng tôi đề xuất một bộ mã hóa trigram để mã hóa các phụ thuộc tầm ngắn, tức là các trigram trong chuỗi đầu vào. Một ưu điểm bổ sung của bộ mã hóa trigram là chúng ta có thể truy xuất các trạng thái ẩn của trigram thông qua bảng tra cứu được tính toán trước, thay vì tính toán chúng ngay lập tức, tiết kiệm thêm thời gian suy luận.

Chiến lược thứ hai là tận dụng biểu diễn sẵn có của chuỗi tiền tố từ LLM gốc. Nắm bắt các phụ thuộc tầm dài trên toàn bộ ngữ cảnh thường yêu cầu nhiều khối Transformer, nhưng mô hình dự thảo nhẹ không thể đáp ứng điều đó. May mắn thay, LLM gốc đã mã hóa đúng cách các phụ thuộc giữa các chuỗi tiền tố. Do đó, chúng tôi tận dụng đầy đủ các trạng thái ẩn sẵn có từ LLM gốc. Chúng tôi đề xuất một bộ mã hóa ngữ cảnh đầy đủ được triển khai thông qua khối Transformer để nắm bắt các phụ thuộc tầm dài. Đầu vào của bộ mã hóa ngữ cảnh đầy đủ là sự nối tiếp của các trạng thái ẩn trưởng thành từ LLM gốc và các trạng thái ẩn xanh từ bộ mã hóa trigram. Cuối cùng, chúng tôi sử dụng nhiều đầu giải mã dư để tạo các từ tiếp theo ở các vị trí offset khác nhau. Do năng lực hạn chế của mô hình dự thảo nhẹ, các đầu giải mã dư không chỉ lấy trạng thái ẩn của token cuối cùng từ bộ mã hóa ngữ cảnh đầy đủ làm đầu vào mà còn xem xét trạng thái ẩn của token cuối cùng từ LLM gốc.

Những đóng góp của phương pháp chúng tôi được tóm tắt như sau:

•Để cải thiện độ chính xác của mô hình dự thảo, chúng tôi đề xuất một mô hình dự thảo nhẹ có thể xem xét tất cả các token trước đó khi tạo từ tiếp theo.

•Chúng tôi đề xuất hai chiến lược trong mô hình dự thảo nhẹ để đảm bảo độ chính xác và hiệu quả, tức là nắm bắt các phụ thuộc tầm ngắn ở lớp dưới cùng và tận dụng biểu diễn sẵn có từ LLM gốc.

•Các kết quả thực nghiệm cho thấy phương pháp của chúng tôi có thể tăng tốc vicuna-7b và vicuna-13b lên 2.7x.

Mã Chimera có sẵn tại: https://github.com/kafkayu/Chimera

2 Nghiên cứu liên quan

Dự thảo rất quan trọng trong pipeline "Dự thảo-rồi-Xác minh" của giải mã suy đoán. Theo cách thức thực hiện quá trình dự thảo, các phương pháp dự thảo có thể được phân loại thô thành dự thảo độc lập và tự dự thảo.

Dự thảo Độc lập. SpecDec[14] đề xuất sử dụng một mô hình độc lập cho quá trình dự thảo. Hơn nữa, nghiên cứu gần đây đã nhấn mạnh các phương pháp chưng cất kiến thức đa dạng để tinh chỉnh LM nhỏ thành những người dự thảo thành thạo, tăng cường sự liên kết hành vi[15–17]. Tuy nhiên, phương pháp này yêu cầu một loạt rộng

--- TRANG 3 ---
Ziqian Zeng et al.: Chimera: Một Phương Pháp Giải Mã Không Mất Mát để Tăng Tốc Suy Luận Mô Hình Ngôn Ngữ Lớn bằng Cách Hợp Nhất Tất Cả Các Token 3

tài nguyên tính toán để điều chỉnh mô hình dự thảo. Để giảm bớt chi phí tính toán, một phương pháp trực tiếp và hiệu quả hơn là sử dụng cùng dòng LM nhỏ làm người dự thảo để tăng tốc suy luận của các đối tác lớn hơn của chúng[7, 18–20]. Tuy nhiên, vẫn tồn tại khoảng cách hành vi đáng kể giữa LM nhỏ và LLM đích, dẫn đến độ chính xác suy luận không tối ưu.

Tự Dự thảo. Để giải quyết những thách thức nêu trên, một số phương pháp đã được đề xuất để tận dụng chính LLM đích cho việc dự thảo hiệu quả[8, 21, 22]. Cụ thể, các phương pháp như Giải mã Theo khối[8] và Medusa[10] đã giới thiệu các đầu FFN bổ sung trên đỉnh bộ giải mã transformer, cho phép tạo nhiều token đồng thời mỗi bước. Một hướng nghiên cứu khác đã tận dụng các kỹ thuật thoát sớm hiện có hoặc bỏ qua lớp trong chính LLM đích để xử lý nhiệm vụ dự thảo[23–25]. Ví dụ[23], đã giới thiệu các quy trình con bổ sung sớm trong bước giải mã hiện tại để bắt đầu dự thảo các token tương lai trước.

3 Nền tảng

3.1 Giải mã Tự hồi quy và Giải mã Suy đoán

Lấy mẫu tự hồi quy là một phương pháp phổ biến để tạo dữ liệu chuỗi một cách tuần tự. Trong lấy mẫu tự hồi quy, mô hình tạo các phần tử chuỗi từ trái sang phải, một vị trí một lần. Sau khi tạo một phần tử, nó đóng vai trò là điều kiện ngữ cảnh để tạo phần tử tiếp theo, và quá trình này được lặp lại đệ quy cho đến khi chuỗi hoàn chỉnh được tạo ra. Thuật toán cho lấy mẫu tự hồi quy có thể được mô tả bằng công thức sau:

xt+1∼pt+1=Mp(x|x<t+1), (1)

trong đó xt+1 là từ thứ (t+ 1) được dự đoán, pt+1 là phân phối xác suất dự đoán của từ thứ (t+ 1), M(·) là LLM.

Giải mã suy đoán là một mô hình giải mã trong đó, ở mỗi bước giải mã, nhiều token tương lai được dự thảo hiệu quả trước và sau đó được xác minh song song bằng LLM gốc. Cụ thể, trong giải mã suy đoán, quá trình được chia thành hai bước: dự thảo và xác minh.

Dự thảo. Trong bước dự thảo, nhiều token tương lai được tạo nhanh chóng song song mà không cần chờ xác minh. Quá trình dự thảo này thường hiệu quả hơn vì nó có thể tận dụng tính song song và khai thác sức mạnh tính toán của phần cứng hiện đại.

Xác minh. Khi bước dự thảo hoàn tất, tất cả các token dự thảo được xác minh bằng Mô hình Ngôn ngữ đích đồng thời. LLM đích đánh giá xác suất hoặc khả năng của mỗi token dự thảo, cho phép xác minh hiệu quả và nhanh chóng.

Bằng cách kết hợp dự thảo và xác minh song song, giải mã suy đoán có thể tăng tốc đáng kể suy luận, từ đó tăng tốc suy luận của các mô hình tạo sinh.

4 Phương pháp

Trong phần này, chúng tôi sẽ giới thiệu kiến thức nền tảng về giải mã tự hồi quy và giải mã suy đoán trong 3.1, mô hình dự thảo nhẹ trong 4.1, và chi tiết huấn luyện trong 4.2. Kiến trúc mô hình tổng quát được thể hiện trong Hình 1.

4.1 Mô hình Dự thảo Nhẹ

Mô hình dự thảo nhẹ bao gồm ba mô-đun: (1) bộ mã hóa trigram, (2) bộ mã hóa ngữ cảnh đầy đủ, và (3) các đầu giải mã dư.

4.1.1 Bộ mã hóa Trigram

Chúng tôi áp dụng bộ mã hóa trigram làm lớp dưới cùng của mô hình dự thảo nhẹ để tiết kiệm thời gian suy luận mà không làm giảm quá nhiều hiệu suất. Như được chỉ ra trong các nghiên cứu trước[12, 13], các lớp thấp hơn của mô hình ngôn ngữ được tiền huấn luyện chủ yếu tập trung vào các phụ thuộc tầm ngắn. Cắt toàn bộ đầu vào thành các đoạn có độ dài ngắn có thể giảm thời gian suy luận vì độ phức tạp tính toán liên quan đến độ dài đầu vào. Ví dụ, độ phức tạp thời gian và không gian của self-attention là bậc hai theo độ dài đầu vào[26].

Các embedding của chuỗi tiền tố được ký hiệu là X={x1,···, xn}, trong đó n là số token trong chuỗi tiền tố đầu vào, xi∈ Rd, d là số chiều của embedding. Biểu diễn của trigram ti∈ R3d là sự nối tiếp của các embedding và được ký hiệu là [xi−2;xi−1;xi]. Đầu vào của bộ mã hóa trigram được ký hiệu là T={t1,···, tm}. Lưu ý hai trigram đầu tiên được gộp thành unigram và bigram, tức là t1= [x1] và t1= [x1;x2]. Khi dự đoán token tiếp theo ở các vị trí khác nhau, số trigram trong đầu vào khác nhau. Chúng tôi sử dụng m để ký hiệu nó. Mối quan hệ giữa số token và số trigram được mô tả như sau: Một chuỗi gồm m token có thể được chia thành m trigram.

--- TRANG 4 ---
4 Khai Thác và Phân Tích Dữ Liệu Lớn, Tháng 4 năm 2024, 1(1): 000-000

[Hình 1 có chứa các thành phần kiến trúc mô hình Chimera với các embedding, lớp Transformer, đầu LM và các bộ mã hóa khác nhau]

Hình 1 Kiến trúc của mô hình Chimera. Các embedding của chuỗi tiền tố được ký hiệu là x1, x2, x3, x4, x5. Các trạng thái ẩn cuối cùng được biểu thị là hLLM1, hLLM2. Các trigram được tính bằng công thức 2 được đại diện bởi htrigram1, htrigram2 và các trạng thái ẩn ngữ cảnh đầy đủ được tính bằng công thức 3 được biểu thị là hfull2, hfull3, với chỉ số dưới cho biết vị trí của chúng trong chuỗi.

Như được thể hiện trong Hình 2, bộ mã hóa trigram bao gồm một MLP với 2 lớp tuyến tính, được tham số hóa bởi W1∈ R3d×d và W2∈ Rd×d tương ứng. Cho đầu vào là ti, các trạng thái ẩn tương ứng htrigrami được tạo bởi bộ mã hóa trigram được định nghĩa như sau:

htrigrami = MLP(SiLU(MLP(ti))), (2)

trong đó SiLU là hàm kích hoạt. Chuỗi đầu ra của bộ mã hóa trigram được ký hiệu là Htrigram={htrigram1,···, htrigramm}.

[Hình 2 thể hiện kiến trúc bộ mã hóa trigram với các lớp Linear và hàm kích hoạt SiLU]

Hình 2 Kiến trúc của bộ mã hóa trigram. xi−3, xi−2...xi+1 đại diện cho các embedding của token ở mỗi vị trí tương ứng. htrigrami−1, htrigrami, htrigrami+1 chỉ ra các đầu ra của bộ mã hóa trigram.

Một ưu điểm bổ sung của bộ mã hóa trigram là chúng ta có thể truy xuất các trạng thái ẩn của trigram thông qua bảng tra cứu được tính toán trước, thay vì tính toán chúng ngay lập tức, tiết kiệm thêm thời gian suy luận. Chúng tôi sử dụng từ điển trigram kích thước cố định làm bộ nhớ đệm, được tải vào bộ nhớ. Bất cứ khi nào gặp trigram mới không có trong từ điển, trigram ít được sử dụng gần đây nhất từ bộ nhớ đệm được chuyển sang lưu trữ thứ cấp, mục tương ứng bị xóa khỏi bộ nhớ đệm, và trigram mới được tải vào bộ nhớ.

4.1.2 Bộ mã hóa Ngữ cảnh Đầy đủ

Chúng tôi đề xuất bộ mã hóa ngữ cảnh đầy đủ được triển khai thông qua khối Transformer để nắm bắt các phụ thuộc tầm dài. Nắm bắt các phụ thuộc tầm dài trên toàn bộ ngữ cảnh thường yêu cầu nhiều khối Transformer, nhưng mô hình dự thảo nhẹ không thể đáp ứng điều đó. May mắn thay, LLM gốc đã mã hóa đúng cách các phụ thuộc giữa các chuỗi tiền tố. Do đó, chúng tôi tận dụng đầy đủ các trạng thái ẩn sẵn có từ LLM gốc.

Như được thể hiện trong Hình 4, đầu vào của bộ mã hóa ngữ cảnh đầy đủ, được ký hiệu là [HLLM;Htrigram], là sự nối tiếp của các trạng thái ẩn trưởng thành từ LLM gốc HLLM= [hLLM1,···, hLLMn] và các trạng thái ẩn xanh từ bộ mã hóa trigram Htrigram={htrigram1,···, htrigramm}. Điều này dẫn đến độ dài đầu vào dài hơn, tức là m+n. Để quản lý độ dài tăng này, chúng tôi chỉ giữ lại m trạng thái ẩn cuối cùng làm đầu ra của bộ mã hóa ngữ cảnh đầy đủ. Bộ mã hóa ngữ cảnh đầy đủ được triển khai bằng khối Transformer. Đầu ra của bộ mã hóa ngữ cảnh đầy đủ được định nghĩa như sau:

[Hdiscard;Hfull] = Transformer([HLLM;Htrigram]). (3)

Trong quá trình huấn luyện, mặt nạ attention được thiết kế tỉ mỉ. Chỉ cần có token tương lai trong trigram, thì trigram này bị che. Một ví dụ minh họa cơ chế mặt nạ attention trong quá trình huấn luyện được trình bày trong Hình 3.

[Hình 3 thể hiện mặt nạ attention với các token có thể nhìn thấy và các token bị che]

Hình 3 Minh họa được cung cấp mô tả các mặt nạ attention được sử dụng trong Bộ mã hóa Ngữ cảnh Đầy đủ trong giai đoạn huấn luyện. Các hình chữ nhật màu xám đại diện cho các token nên bị che trong quá trình tính toán attention. Đầu vào của bộ mã hóa ngữ cảnh đầy đủ là sự nối tiếp của các trạng thái ẩn cuối cùng từ mô hình ngôn ngữ gốc và các đầu ra của bộ mã hóa trigram.

4.1.3 Các Đầu Giải mã Dư

Chúng tôi đề xuất nhiều đầu giải mã dư để tạo các từ tiếp theo ở các vị trí offset khác nhau. Đầu giải mã ở offset k∈0,···, K−1 chịu trách nhiệm dự đoán từ thứ (n+ 2 + k), trong đó K là số đầu giải mã trong mô hình dự thảo và n là số token trong chuỗi tiền tố. Ví dụ, cho chuỗi tiền tố có độ dài n, LLM gốc dự đoán từ thứ (n+ 1) bằng đầu LLM gốc của nó. Đầu giải mã thứ 0 dự đoán từ thứ (n+ 2). Do năng lực hạn chế của mô hình dự thảo nhẹ, các đầu giải mã dư không chỉ lấy trạng thái ẩn của token cuối cùng từ bộ mã hóa ngữ cảnh đầy đủ làm đầu vào mà còn xem xét trạng thái ẩn của token cuối cùng từ LLM gốc. Đầu vào của đầu giải mã dư dự đoán từ tiếp theo ở offset k là sự nối tiếp của cả hai trạng thái ẩn cuối cùng hLLMn và hfulln+1+k.

Đầu ra của đầu giải mã dư được định nghĩa như sau:

pkn= Softmax(MLP[hLLMn;hfulln+1+k]), (4)

Trong đó MLP được tham số hóa bởi Θk∈ Rd×|V|, và |V| là kích thước từ vựng. Lưu ý rằng các đầu giải mã với offset khác nhau có các tham số MLP khác nhau.

4.2 Huấn luyện Mô hình

Hàm mục tiêu huấn luyện bao gồm hai phần: (1) huấn luyện mô hình dự thảo để dự đoán từ tiếp theo cho tất cả các từ trước đó và (2) điều chỉnh năng lực của bộ mã hóa ngữ cảnh đầy đủ với LLM gốc.

[Hình 4 thể hiện kiến trúc của bộ mã hóa ngữ cảnh đầy đủ và các đầu giải mã dư]

Hình 4 Kiến trúc của bộ mã hóa ngữ cảnh đầy đủ và các đầu giải mã dư. hLLM1 và hLLM2 đại diện cho các trạng thái ẩn cuối cùng của LLM gốc. hfull2 và hfull3 chỉ ra các đầu ra của bộ mã hóa ngữ cảnh đầy đủ.

Để huấn luyện mô hình dự thảo dự đoán từ tiếp theo chính xác, chúng tôi áp dụng hàm mất mát cross-entropy làm hàm mục tiêu:

Lnext_word=−|T|∑t=1 l∑k=0 ykt log(pkt), (5)

trong đó l là số đầu giải mã trong mô hình dự thảo, ykt∈ R|V| là nhãn dưới dạng vector one-hot trong đó phần tử tương ứng với từ thứ (t+ 2 + k) được đặt thành một, |T| là tổng số token trong tập huấn luyện.

Để tăng cường năng lực của bộ mã hóa ngữ cảnh đầy đủ, chúng tôi thực hiện chưng cất trên trạng thái ẩn của token cuối cùng của bộ mã hóa ngữ cảnh đầy đủ. Chúng được huấn luyện với tín hiệu giám sát từ các trạng thái ẩn của lớp cuối cùng của LLM gốc. Hàm mất mát chưng cất được định nghĩa là sai số bình phương trung bình (MSE) của các trạng thái ẩn giữa LLM gốc và bộ mã hóa ngữ cảnh đầy đủ:

Ldistill =|T|∑t=1 MSE(hfullt, hLLMt). (6)

Hàm mục tiêu cuối cùng được định nghĩa là

L=Lnext_word+Ldistill. (7)

4.3 Xác minh

Dựa trên phương pháp Medusa[10], chúng tôi sử dụng cơ chế attention có cấu trúc cây trong quá trình xác minh. Cụ thể, cho dự thảo có cấu trúc cây, mô hình ngôn ngữ lớn gốc tính toán xác suất của mỗi token ứng viên thông qua một lần forward pass duy nhất. Sau đó chúng tôi tận dụng cả giải mã tham lam[16] và phương pháp

--- TRANG 5 ---
Ziqian Zeng et al.: Chimera: Một Phương Pháp Giải Mã Không Mất Mát để Tăng Tốc Suy Luận Mô Hình Ngôn Ngữ Lớn bằng Cách Hợp Nhất Tất Cả Các Token 5

[Nội dung tiếp tục với mô tả quá trình xác minh và suy luận, bao gồm các hình vẽ minh họa về quá trình inference và tree decoding]

giải mã điển hình để lấy mẫu các chuỗi token có thể chấp nhận được từ các token dự thảo. Phương pháp giải mã điển hình lựa chọn các chuỗi ứng viên dựa trên xác suất của chúng dưới mô hình ngôn ngữ gốc, thay vì sử dụng lấy mẫu từ chối, do đó cải thiện hiệu quả mà không làm giảm tính đa dạng.

4.4 Suy luận

Quá trình suy luận được mô tả trong hình 5. Trong bước dự thảo, chúng ta có thể lấy mẫu top-k token từ ba đầu chimera và tạo ra nhiều chuỗi ứng viên tận dụng các token này. Sau đó chúng ta tiến hành xác minh các chuỗi ứng viên này song song với mô hình ngôn ngữ lớn gốc, thu được xác suất có điều kiện của mỗi token trong các chuỗi. Ví dụ, chúng ta có thể chấp nhận chuỗi ứng viên có xác suất tổng thể cao nhất thông qua phương pháp tìm kiếm tham lam.

5 Thực nghiệm

5.1 Thiết lập Thực nghiệm

Tất cả các thực nghiệm được thực hiện trên máy chủ với CPU 32 nhân, bộ nhớ máy chủ 64 GiB và GPU A800(80G).

LLM Xương sống. Chúng tôi đánh giá hiệu suất của phương pháp trên nhiều LLM xương sống khác nhau, bao gồm Vicuna-7B, Vicuna-13B, Vicuna-33B[27], LlaMA-2-chat-7B và LlaMA-2-chat-13B[28]. Thông tin chi tiết về các LLM xương sống có thể tìm thấy trong Bảng 1.

Bảng 1 Tóm tắt Các Mô hình
Mô hình | Params | Lớp | Kích thước ẩn | Đầu attention
Vicuna-7B | 7B | 32 | 4096 | 32
Vicuna-13B | 13B | 40 | 5120 | 40
Vicuna-33B | 33B | 60 | 6656 | 52
LlaMA-2-chat-7B | 7B | 32 | 4096 | 32
LlaMA-2-chat-13B | 13B | 40 | 5120 | 40

Bộ dữ liệu. Chimera (với các LLM xương sống cụ thể) được huấn luyện trên bộ dữ liệu ShareGPT[29], bao gồm 70.000 cuộc đối thoại được tạo thủ công cho các cuộc trò chuyện nhiều lượt. Hiệu suất của tất cả các phương pháp được đánh giá trên hai điểm chuẩn: MT-bench[30] và Vicuna-bench[27] để đánh giá các khả năng đối thoại nhiều lượt và tổng quát, tương ứng.

Chỉ số. Giống như các phương pháp dựa trên lấy mẫu suy đoán khác, Chimera chủ yếu tập trung vào độ trễ hơn là thông lượng. Chúng tôi đánh giá hiệu ứng tăng tốc bằng các chỉ số sau:

•Tăng tốc thời gian thực[8]: Tỷ lệ tăng tốc thời gian thực hiện thực tế so với giải mã tự hồi quy.

•Độ dài chấp nhận trung bình: Số token trung bình được chấp nhận mỗi lần forward pass của LLM đích.

•Độ chính xác top-k: Xác suất token thứ i được dự đoán bởi LLM gốc, ký hiệu là tLLMi, có trong top-k token có xác suất cao nhất được dự đoán bởi mô hình Chimera, ký hiệu là tchimerаi1, tchimerаi2, ..., tchimerаik.

Chính thức, độ chính xác top-k của token thứ i có thể được biểu diễn là:

Accik=P{tLLMi∈ {tchimerаi1, tchimerаi2, ..., tchimerаik}} (8)

Chiến lược giải mã. Hai chiến lược giải mã được kiểm tra để xác minh: giải mã tham lam và giải mã điển hình[10]. Đối với giải mã điển hình, nhiệt độ được đặt là T= 1.

5.2 Đường cơ sở

Phương pháp của chúng tôi được so sánh với ba phương pháp, bao gồm giải mã tự hồi quy mặc định và hai phương pháp gần đây để tăng tốc LLM.

Lookahead[31]. Lookahead Decoding tạo song song nhiều n-gram rời rạc để phù hợp với các phần tương lai của chuỗi. Nó tái công thức hóa giải mã tự hồi quy như nghiệm của hệ phương trình phi tuyến. Sau đó, lặp Jacobi được sử dụng cho giải mã song song, nắm bắt, xác thực và tích hợp các n-gram được tạo vào chuỗi.

Medusa-1[10]. Medusa-1 là phương pháp huấn luyện hiệu quả tham số với nhiều đầu giải mã được huấn luyện trên cùng mô hình. Bằng cách nới lỏng yêu cầu khớp phân phối của mô hình gốc, Medusa cho phép tạo sinh nhanh hơn và không tham lam và giải quyết các thách thức của giải mã suy đoán.

Medusa-2 yêu cầu điều chỉnh tinh của các LLM xương sống, khác với phương pháp đề xuất trong bài báo này. Trong các phần tiếp theo, chúng tôi sẽ nhất quán gọi Medusa-1 là "Medusa" cho ngắn gọn.

5.3 Kết quả chính

5.3.1 Tăng tốc suy luận LLM

Để chứng minh hiệu quả của các phương pháp, chúng tôi trình bày tỷ lệ tăng tốc, là tỷ lệ giữa thời gian suy luận gốc và thời gian suy luận tăng tốc, cũng như độ dài chấp nhận trung bình. Độ dài chấp nhận trung bình tương ứng với số token trung bình được chấp nhận trong mỗi lần forward pass của các LLM xương sống.

Hình 6 chứng minh rằng phương pháp của chúng tôi đạt được tăng tốc tốt hơn trên tất cả năm LLM xương sống trên MT-bench với giải mã tham lam. Tỷ lệ tăng tốc tối đa là 2.77x, vượt Medusa 0.79x và Lookahead 1.41x. Tỷ lệ tăng tốc trung bình là 2.77x.

Nghiên cứu trong các phương pháp giải mã khác nhau. Bảng 2 cũng kiểm tra thiết lập giải mã điển hình (T=1) trên MT-bench, nơi phương pháp của chúng tôi thể hiện tăng tốc thậm chí tốt hơn trên giải mã tham lam. Cụ thể, tỷ lệ tăng tốc tốt nhất được quan sát trên Vicuna-33B, đạt tăng tốc tối đa 2.91x. Tăng tốc tối thiểu là 2.62x, với tăng tốc trung bình 2.79x, vượt tìm kiếm tham lam 0.14x về tốc độ. Độ dài trung bình tối đa là 3.38, tối thiểu là 3.30, và tổng thể trung bình là 3.32.

Nghiên cứu trong các bộ dữ liệu khác nhau. Bảng 3 xác nhận tất cả các phát hiện trong MT-bench với Vicuna-bench, chứng minh rằng phương pháp của chúng tôi có thể được áp dụng cho nhiều lĩnh vực khác nhau. Phương pháp của chúng tôi thể hiện hiệu suất tăng tốc tốt nhất trên Vicuna33B, đạt tăng tốc tối đa 2.77x, dưới phương pháp giải mã tham lam. Tăng tốc tối thiểu trên tất cả năm mô hình là 2.61x, với tăng tốc trung bình 2.72x. Độ dài chấp nhận trung bình tối đa là 3.30, cho thấy trung bình, mô hình có thể dự đoán 3.3 token trong một lần forward pass. Độ dài chấp nhận trung bình tối thiểu là 3.21, với tổng thể trung bình 3.26.

Bảng 2 Tăng tốc thời gian thực và độ dài chấp nhận trung bình trên MT-bench, V đại diện cho Vicuna, và L2 viết tắt của LlaMA-2-chat.

Chiến lược giải mã | Mô hình | Tăng tốc | Độ dài chấp nhận trung bình
Tham lam | V-7B | 2.73x | 3.30
| V-13B | 2.75x | 3.26
| V-33B | 2.77x | 3.23
| L2-7B | 2.61x | 3.28
| L2-13B | 2.74x | 3.21
Điển hình | V-7B | 2.81x | 3.38
| V-13B | 2.85x | 3.35
| V-33B | 2.91x | 3.31
| L2-7B | 2.62x | 3.31
| L2-13B | 2.74x | 3.30

Nghiên cứu trường hợp trong các lĩnh vực khác nhau. Chúng tôi đánh giá hiệu suất của phương pháp bằng kỹ thuật giải mã tham lam trên 13 trường hợp khác nhau dựa trên mô hình Vicuna-7B. Kết quả được trình bày trong Hình 7 và Bảng 4. Mô hình của chúng tôi hoạt động tốt trong các trường hợp coding và roleplay, đạt tỷ lệ tăng tốc 3.03x và 2.91x, tương ứng. Những tỷ lệ tăng tốc này vượt qua

--- TRANG 6 ---
6 Khai Thác và Phân Tích Dữ Liệu Lớn, Tháng 4 năm 2024, 1(1): 000-000

[Hình 6 thể hiện biểu đồ so sánh tăng tốc giữa các phương pháp khác nhau]

tăng tốc trung bình của Vicuna-7B. Tỷ lệ tăng tốc thấp nhất quan sát được là 2.51x, vẫn vượt trội các phương pháp khác, và độ lệch so với tăng tốc trung bình chỉ là 12%. Điều này cho thấy phương pháp của chúng tôi thể hiện khả năng tổng quát mạnh và không bị suy giảm hiệu suất đáng kể trên các trường hợp khác nhau.

5.3.2 Các đầu giải mã chính xác

Độ chính xác của mỗi đầu giải mã trong Bảng 5. Phương pháp của chúng tôi vượt trội Medusa với biên độ trung bình 0.13 trên Medusa head 0 và đạt được cải thiện tối đa 27% trên top1 so với Medusa head 0. Trên Medusa head 1, Medusa head 2, và Medusa head 3, Chimera vượt Medusa lần lượt 79%, 217%, và 267% trên chỉ số top1. Những kết quả này chứng minh rằng mô hình của chúng tôi thể hiện độ chính xác cao hơn trong việc dự đoán các chuỗi dài hơn so với Medusa. Điều này cũng cho thấy rằng mô hình của chúng tôi có khả năng trích xuất thông tin cần thiết để dự đoán các chuỗi dài hiệu quả hơn.

Bảng 3 Tăng tốc thời gian thực và độ dài chấp nhận trung bình trên Vicuna-Bench, V đại diện cho Vicuna, và L2 viết tắt của LlaMA-2-chat.

Chiến lược giải mã | Mô hình | Tăng tốc | Độ dài chấp nhận trung bình
Tham lam | V-7B | 2.83x | 3.47
| V-13B | 2.87x | 3.42
| V-33B | 2.89x | 3.54
| L2-7B | 2.75x | 3.44
| L2-13B | 2.77x | 3.41
Điển hình | V-7B | 2.85x | 3.49
| V-13B | 2.88x | 3.45
| V-33B | 2.90x | 3.45
| L2-7B | 2.85x | 3.37
| L2-13B | 2.80x | 3.34

[Hình 7 thể hiện tăng tốc thời gian thực của các trường hợp khác nhau]

5.4 Nghiên cứu Loại bỏ

5.4.1 Bộ mã hóa Trigram

Bộ nhớ đệm trigram. Để giảm thời gian tính toán, có thể sử dụng bộ nhớ đệm trigram được xây dựng trước, lưu trữ các đầu ra của N-gram thường gặp sau khi đi qua bộ mã hóa trigram. Trên Vicuna-7B, phương pháp này có thể dẫn đến tăng tốc tối đa 1.45x so với không sử dụng bộ nhớ đệm trigram. Cụ thể, tăng tốc của mô hình cải thiện từ 2.69x lên 2.81x dưới phương pháp giải mã tham lam.

Bộ mã hóa trigram. Việc lựa chọn và nghiên cứu loại bỏ của bộ mã hóa trigram được tóm tắt trong bảng 6: Về độ chính xác, hiệu suất của các đầu khác nhau trong 1-gram, 2-gram, 3-gram, 4-gram, và 5-gram nhất quán vượt trội các đầu 1-gram và 2-gram dưới 1%. Đáng chú ý, đầu 3-gram thể hiện tính toán tương đối nhanh hơn và phù hợp hơn cho chiến lược bộ nhớ đệm n-gram. Do đó, Chimera cuối cùng chọn phương pháp 3-gram. Ngoài ra, quan sát thấy việc sử dụng transformer đông lạnh cho kết quả kém hơn so với sử dụng phương pháp n-gram. Mặt khác, hiệu suất của transformer (finetune) có thể so sánh với 3-gram, nhưng transformer yêu cầu thời gian tính toán nhiều hơn đáng kể và không tương thích với chiến lược bộ nhớ đệm. Trong thực tế, hiệu ứng tăng tốc vẫn kém hơn so với Bộ mã hóa N-gram.

5.4.2 Kết nối dư của các đầu giải mã

Hiệu ứng của việc loại bỏ kết nối dư được thể hiện trong Hình 8. Có thể quan sát thấy rằng hiệu suất không có dư tương tự như Medusa, nhưng việc bao gồm kết nối dư cải thiện đáng kể độ chính xác của mỗi đầu. Cụ thể, so với mô hình không có kết nối dư, head0 cho thấy cải thiện 0.12, trong khi head3 vượt trội Medusa 0.26. Điều này cho thấy rằng

Bảng 4 Kiểm tra Trường hợp trong Vicuna-7B. Chỉ số là độ dài chấp nhận trung bình.

Trường hợp | Chiến lược giải mã
| Tham lam | Điển hình
Viết | 3.31 | 3.45
Nhập vai | 3.51 | 3.57
Lý luận | 3.55 | 3.61
Toán | 3.41 | 3.66
Lập trình | 3.57 | 3.82
Trích xuất | 3.27 | 3.57
STEM | 3.29 | 3.45
Nhân văn | 3.28 | 3.39
Tổng quát | 3.46 | 3.71
Kiến thức | 3.43 | 3.51
Thường thức | 3.27 | 3.49
Fermi | 3.51 | 3.62
Phản thực tế | 3.57 | 3.76

Bảng 5 Độ chính xác Đầu của Medusa và Chimera trong Vicuna-7B. Top-k đại diện cho độ chính xác top-k của mô hình.

Đầu | Mô hình | Top-1 | Top-2 | Top-3 | Top-4 | Top-5
0 | Chimera | 0.66 | 0.78 | 0.83 | 0.87 | 0.90
| Medusa | 0.52 | 0.65 | 0.71 | 0.74 | 0.77
1 | Chimera | 0.49 | 0.55 | 0.60 | 0.68 | 0.75
| Medusa | 0.29 | 0.39 | 0.45 | 0.50 | 0.53
2 | Chimera | 0.38 | 0.44 | 0.49 | 0.55 | 0.62
| Medusa | 0.12 | 0.25 | 0.31 | 0.34 | 0.38
3 | Chimera | 0.33 | 0.38 | 0.43 | 0.46 | 0.54
| Medusa | 0.09 | 0.18 | 0.23 | 0.27 | 0.30

đầu dư hiệu quả hơn trong việc dự đoán token cho các chuỗi dài hơn trong ngữ cảnh LLM.

Bảng 6 Tác động của bộ mã hóa k-gram lên Chimera. Chimera (k-gram) chỉ ra mô hình Chimera với bộ mã hóa k-gram. Transformer (Frozen) đại diện cho việc chúng tôi sử dụng lớp transformer đầu tiên đông lạnh của LLM gốc, và Transformer (Finetune) là một lớp transformer được điều chỉnh tinh, chỉ số là độ chính xác dự đoán của top-5 token.

Mô hình | Đầu
| 0 | 1 | 2 | 3
Chimera (1-gram) | 0.83 | 0.65 | 0.50 | 0.48
Chimera (2-gram) | 0.87 | 0.71 | 0.58 | 0.51
Chimera (3-gram) | 0.90 | 0.75 | 0.62 | 0.54
Chimera (4-gram) | 0.91 | 0.75 | 0.63 | 0.54
Chimera (5-gram) | 0.91 | 0.76 | 0.63 | 0.54
Transformer (Frozen) | 0.85 | 0.60 | 0.49 | 0.41
Transformer (Finetune) | 0.89 | 0.74 | 0.56 | 0.52

[Hình 8 thể hiện độ chính xác dự đoán top-5 token trong các biến thể khác nhau của đầu giải mã dư]

6 Kết luận

Bài báo này trình bày Chimera, một khung làm việc mới cho lấy mẫu suy đoán. Chúng tôi đề xuất một mô hình dự thảo nhẹ tận dụng các token được tạo trước đó để dự đoán các từ tiếp theo. Để đảm bảo độ chính xác và hiệu quả, chúng tôi giới thiệu hai chiến lược trong mô hình dự thảo nhẹ: nắm bắt các phụ thuộc tầm ngắn ở lớp dưới cùng và tận dụng các biểu diễn sẵn có từ LLM gốc. Trong các thực nghiệm được thực hiện trên dòng Vicuna và LlaMA-2-chat, Chimera đạt được tỷ lệ tăng tốc độ trễ trung bình 2.7x so với giải mã tự hồi quy.

--- TRANG 7 ---
[Tiếp tục với phần Acknowledgement và References như trong bản gốc]

Lời cảm ơn

Nghiên cứu này được hỗ trợ bởi Quỹ Nghiên cứu Cơ bản và Ứng dụng Cơ bản Quảng Châu (Số tài trợ 2023A04J1687), Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số tài trợ 6230070401), Quỹ Đổi mới Công nghệ Đại học Công nghệ Nam Trung Quốc-TCL, Chương trình Nghiên cứu sinh sau tiến sĩ của CPSF (Số tài trợ GZC20230841).

Tài liệu tham khảo

[1] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru, M. Debbah, É. Goffinet, D. Hesslow, J. Launay, Q. Malartic et al., "The falcon series of open language models," arXiv preprint arXiv:2311.16867, 2023.

[2] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., "Llama 2: Open foundation and fine-tuned chat models," arXiv preprint arXiv:2307.09288, 2023.

[3] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., "Gpt-4 technical report," arXiv preprint arXiv:2303.08774, 2023.

[4] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen et al., "Palm 2 technical report," arXiv preprint arXiv:2305.10403, 2023.

[5] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang et al., "Qwen technical report," arXiv preprint arXiv:2309.16609, 2023.

[6] A. Yang, B. Xiao, B. Wang, B. Zhang, C. Bian, C. Yin, C. Lv, D. Pan, D. Wang, D. Yan et al., "Baichuan 2: Open large-scale language models," arXiv preprint arXiv:2309.10305, 2023.

[7] Y. Leviathan, M. Kalman, and Y. Matias, "Fast inference from transformers via speculative decoding," in ICML, vol. 202, 2023, pp. 19 274–19 286.

[8] M. Stern, N. Shazeer, and J. Uszkoreit, "Blockwise parallel decoding for deep autoregressive models," in NeurIPS, 2018, pp. 10 107–10 116.

[9] C. Chen, S. Borgeaud, G. Irving, J.-B. Lespiau, L. Sifre, and J. Jumper, "Accelerating large language model decoding with speculative sampling," 2023.

[10] T. Cai, Y. Li, Z. Geng, H. Peng, J. D. Lee, D. Chen, and T. Dao, "Medusa: Simple llm inference acceleration framework with multiple decoding heads," arXiv preprint arXiv: 2401.10774, 2024.

[11] H. Xia, Z. Yang, Q. Dong, P. Wang, Y. Li, T. Ge, T. Liu, W. Li, and Z. Sui, "Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding," arXiv preprint arXiv:2401.07851, 2024.

[12] G. Jawahar, B. Sagot, and D. Seddah, "What does BERT learn about the structure of language?" in ACL, 2019, pp. 3651–3657.

[13] A. Rogers, O. Kovaleva, and A. Rumshisky, "A primer in bertology: What we know about how BERT works," TACL, vol. 8, pp. 842–866, 2020.

[14] H. Xia, T. Ge, P. Wang, S. Chen, F. Wei, and Z. Sui, "Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation," in EMNLP, 2023, pp. 3909–3925.

[15] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, Z. Wang, Z. Zhang, R. Y. Y. Wong, A. Zhu, L. Yang, X. Shi, C. Shi, Z. Chen, D. Arfeen, R. Abhyankar, and Z. Jia, "Specinfer: Accelerating generative large language model serving with tree-based speculative inference and verification," 2024.

[16] S. Kim, K. Mangalam, S. Moon, J. Malik, M. W. Mahoney, A. Gholami, and K. Keutzer, "Speculative decoding with big little decoder," in NeurIPS, 2023.

[17] Y. Zhou, K. Lyu, A. S. Rawat, A. K. Menon, A. Rostamizadeh, S. Kumar, J.-F. Kagy, and R. Agarwal, "Distillspec: Improving speculative decoding via knowledge distillation," 2023.

[18] B. Spector and C. Re, "Accelerating llm inference with staged speculative decoding," 2023.

[19] Z. Sun, A. T. Suresh, J. H. Ro, A. Beirami, H. Jain, and F. X. Yu, "Spectr: Fast speculative decoding via optimal transport," in NeurIPS, 2023.

[20] Z. Chen, X. Yang, J. Lin, C. Sun, J. Huang, and K. C.-C. Chang, "Cascade speculative drafting for even faster llm inference," 2023.

[21] A. Santilli, S. Severino, E. Postolache, V. Maiorca, M. Mancusi, R. Marin, and E. Rodolà, "Accelerating transformer inference for translation via parallel decoding," in ACL, 2023, pp. 12 336–12 355.

[22] C. Hooper, S. Kim, H. Mohammadzadeh, H. Genc, K. Keutzer, A. Gholami, and S. Shao, "Speed: Speculative pipelined execution for efficient decoding," 2024.

[23] S. Yang, G. Lee, J. Cho, D. Papailiopoulos, and K. Lee, "Predictive pipelined decoding: A compute-latency trade-off for exact llm decoding," 2023.

[24] J. Zhang, J. Wang, H. Li, L. Shou, K. Chen, G. Chen, and S. Mehrotra, "Draft &verify: Lossless large language model acceleration via self-speculative decoding," 2023.

[25] G. Monea, A. Joulin, and E. Grave, "Pass: Parallel speculative sampling," 2023.

[26] F. D. Keles, P. M. Wijewardena, and C. Hegde, "On the computational complexity of self-attention," in ALT, vol. 201, 2023, pp. 597–619.

[27] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing, "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality," March 2023. [Online]. Available: https://lmsys.org/blog/2023-03-30-vicuna/

[28] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith,

--- TRANG 8 đến TRANG 11 ---
[Nội dung tiếp tục với phần còn lại của tài liệu tham khảo và thông tin xuất bản]

R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom, "Llama 2: Open foundation and fine-tuned chat models," 2023.

[29] Aeala, "Sharegpt_vicuna_unfiltered," https://huggingface.co/datasets/Aeala/ShareGPT_Vicuna_unfiltered, 2022.

[30] Q. Zheng, "Wku_nlp at semeval-2023 task 9: Translation augmented multilingual tweet intimacy analysis," in ACL, 2023, pp. 1525–1530.

[31] Y. Fu, P. Bailis, I. Stoica, and H. Zhang, "Break the sequential dependency of llm inference using lookahead decoding," 2024.

[32] X. Liu, L. Hu, P. Bailis, I. Stoica, Z. Deng, A. Cheung, and H. Zhang, "Online speculative decoding," 2023.

ISSNll1007-0214ll0?/??llpp???-???
Tập 22, Số 1, Tháng 2 năm 2017
