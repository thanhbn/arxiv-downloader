# 2311.08252.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/speculative/2311.08252.pdf
# Kích thước tệp: 666306 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
REST: Giải Mã Suy Đoán Dựa Trên Truy Xuất
Zhenyu He1∗, Zexuan Zhong2∗,Tianle Cai2∗, Jason D. Lee2, Di He1†
1Phòng thí nghiệm Quốc gia về AI Tổng quát, Trường Trí tuệ Nhân tạo, Đại học Bắc Kinh
2Đại học Princeton
hezhenyu@stu.pku.edu.cn ,zzhong@cs.princeton.edu ,
{tianle.cai, jasonlee}@princeton.edu ,dihe@pku.edu.cn
Tóm tắt
Chúng tôi giới thiệu Giải Mã Suy Đoán Dựa Trên Truy Xuất (REST), một thuật toán mới được thiết kế để tăng tốc quá trình sinh văn bản của mô hình ngôn ngữ. Ý tưởng chính thúc đẩy phát triển REST là quan sát rằng quá trình sinh văn bản thường bao gồm các giai đoạn và mẫu phổ biến nhất định. Khác với các phương pháp trước đây dựa vào mô hình ngôn ngữ nháp cho giải mã suy đoán, REST khai thác sức mạnh của truy xuất để sinh ra các token nháp. Phương pháp này rút ra từ nguồn kiến thức hiện có, truy xuất và sử dụng các token liên quan dựa trên ngữ cảnh hiện tại. Tính chất cắm-và-chạy của nó cho phép tích hợp liền mạch và tăng tốc bất kỳ mô hình ngôn ngữ nào, tất cả mà không cần đào tạo thêm. Khi được đánh giá trên các mô hình ngôn ngữ 7B và 13B trong thiết lập batch đơn, REST đạt được tăng tốc đáng kể từ 1.62× đến 2.36× trong sinh mã hoặc văn bản. Mã nguồn của REST có sẵn tại https://github.com/FasterDecoding/REST.

1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLM) dựa trên Transformer đã nổi lên như một mô hình nền tảng trong xử lý ngôn ngữ tự nhiên (Vaswani et al., 2017; Devlin et al., 2019; Brown et al., 2020; Zhang et al., 2022; Scao et al., 2022; Chowdhery et al., 2022; Zeng et al., 2022; Touvron et al., 2023). Mặc dù chúng đạt được hiệu suất ấn tượng trên nhiều nhiệm vụ khác nhau, chi phí suy luận là rất lớn trong các tình huống thực tế. Trong quá trình suy luận, mô hình tự hồi quy sử dụng ngữ cảnh trước đó để sinh ra token tiếp theo. Mỗi lần lặp đòi hỏi phải tải lại LLM hàng tỷ tham số từ Bộ nhớ Băng thông Cao (HBM) vào bộ đệm trên chip của các bộ tăng tốc hiện đại như GPU, làm cho toàn bộ quá trình sinh ra không hiệu quả và tốn thời gian.

*Ba tác giả này đóng góp ngang nhau cho dự án này.
†Liên hệ: Di He < dihe@pku.edu.cn >.

Một hướng gần đây trong việc tăng tốc sinh LLM là giảm số lượng quá trình chuyển tiếp với LLM trong khi đảm bảo chất lượng của chuỗi đầu ra đồng thời. Giải mã suy đoán (Leviathan et al., 2023; Chen et al., 2023; Miao et al., 2023; Spector and Re, 2023) là một trong những phương pháp điển hình theo hướng này. Theo trực giác, các phương pháp giải mã suy đoán tận dụng một LM nhỏ để sinh ra các token với chi phí tính toán ít hơn. Trong quá trình suy luận, phương pháp đầu tiên sử dụng LM nhỏ để tạo ra một chuỗi token nháp và sau đó sử dụng LLM để xác minh. Nếu dự đoán từ cả hai mô hình nhất quán, chúng ta có thể chấp nhận bản nháp và trả về cho người dùng. Ở đây, việc sinh token thực tế được thực hiện bằng LM nhỏ, và LM lớn chỉ được sử dụng để xác nhận bản nháp, có thể được thực hiện song song và chỉ yêu cầu tải lại bộ nhớ một lần. Do đó, toàn bộ khung của giải mã suy đoán giảm chi phí suy luận tổng thể.

Tuy nhiên, việc có được một mô hình nháp chất lượng cao vẫn là thách thức: Nó phải cân bằng giữa kích thước nhỏ và sức mạnh dự đoán mạnh mẽ trong khi khớp với từ vựng của mô hình cơ sở; ngoài ra, nó cũng nên tích hợp tốt vào hệ thống phân tán để phục vụ. Do đó, mọi người thường cần đào tạo một mô hình nháp cụ thể cho mô hình và trường hợp sử dụng của họ (Chen et al., 2023; Miao et al., 2023; Cai et al., 2023).

Trong nghiên cứu này, thay vì dựa vào một LM nhỏ bổ sung, chúng tôi điều tra việc sử dụng một kho dữ liệu trực tiếp để xây dựng chuỗi token nháp trong giải mã suy đoán. Chúng tôi phát triển một phương pháp dựa trên truy xuất, được gọi là Giải Mã Suy Đoán Dựa Trên Truy Xuất (REST) (Hình 1). So với các phương pháp trước đây, hệ thống dựa trên truy xuất của chúng tôi thay thế mô hình nháp tham số bằng một kho dữ liệu truy xuất không tham số, có thể dễ dàng chuyển đổi sang bất kỳ LLM nào và tăng tốc suy luận của nó.

Để sử dụng REST, bước đầu tiên là xây dựng kho dữ liệu. Trong bài báo này, chúng tôi tận dụng kho dữ liệu tiền đào tạo hoặc kho dữ liệu điều chỉnh hướng dẫn arXiv:2311.08252v2 [cs.CL] 4 Apr 2024

--- TRANG 2 ---
để xây dựng kho dữ liệu của chúng tôi, phục vụ như nguồn cho chuỗi token nháp. Trong mỗi bước suy luận, đầu tiên chúng tôi sử dụng các token trước đó (token được sinh trước hoặc token gợi ý) làm truy vấn để xác định khớp chính xác trong kho dữ liệu. Các token tiếp theo từ những khớp chính xác này được coi là các ứng viên sinh. Một Trie được xây dựng bằng cách sử dụng các ứng viên này. Các nút có tần suất cao nhất được chọn làm token nháp. Chuỗi này sau đó trải qua xác minh bởi LLM thông qua một lần chuyển tiếp duy nhất, được hỗ trợ bởi một mặt nạ chú ý được thiết kế tỉ mỉ được gọi là chú ý cây (Cai et al., 2023; Miao et al., 2023; Spector and Re, 2023). Vì nhiều chuỗi con trong quá trình sinh có thể xuất hiện trong kho dữ liệu, REST có thể thường xuyên sinh ra nhiều token đúng mỗi bước.

Chúng tôi tiến hành các thí nghiệm rộng rãi để kiểm tra hiệu quả và hiệu suất của REST trong các tình huống khác nhau. Đối với lĩnh vực mã, chúng tôi sử dụng một phần mã tiền đào tạo Python (2.7M mẫu) từ The Stack (Kocetkov et al., 2022) làm kho dữ liệu và tăng tốc CodeLlama (Rozière et al., 2023) 7B và 13B tương ứng. Kết quả cho thấy trên HumanEval (Chen et al., 2021) REST đạt được tăng tốc 2.12× đến 2.36×. Đối với lĩnh vực tổng quát, chúng tôi xây dựng một kho dữ liệu sử dụng UltraChat (Ding et al., 2023), chứa khoảng 774K cuộc hội thoại. Kết quả cho thấy trên MT-Bench (Zheng et al., 2023) REST tăng tốc 7B và 13B Vicuna (Chiang et al., 2023) lần lượt 1.62× đến 1.77×.

2 Nghiên cứu liên quan
Cải thiện hiệu quả suy luận LLM đã trở thành một hướng nghiên cứu mới nổi trong những năm gần đây. Nói chung, các nỗ lực trước đây có thể được chia thành hai loại: tăng tốc không mất mát và tăng tốc có mất mát. Các phương pháp tăng tốc có mất mát nhằm học các mô hình hiệu quả có thể thực thi nhanh hơn và hoạt động tương tự như LLM mục tiêu. Những phương pháp này bao gồm cắt tỉa (Wang et al., 2021; Hubara et al., 2021; Ma et al., 2023; Frantar and Alistarh, 2023), lượng tử hóa (Yao et al., 2022; Park et al., 2022; Dettmers et al., 2022; Frantar et al., 2022; Xiao et al., 2023; Liu et al., 2023) và chưng cất kiến thức (Sanh et al., 2019). Các chiến lược tăng tốc không mất mát tập trung vào việc tăng tốc trực tiếp LLM mục tiêu từ các góc độ khác nhau, chẳng hạn như tối ưu hóa bộ nhớ và IO (Dao et al., 2022; Dao, 2023; Kwon et al., 2023; Sheng et al., 2023), và các cách để giảm lệnh gọi hàm của LLM trong quá trình giải mã, ví dụ: giải mã suy đoán (Stern et al., 2018; Leviathan et al., 2023; Chen et al., 2023; Miao et al., 2023; Spector and Re, 2023; Cai et al., 2023). Công trình này thuộc nhánh thứ hai. Giải mã suy đoán (Leviathan et al., 2023; Chen et al., 2023; Miao et al., 2023; Spector and Re, 2023) tận dụng một mô hình nhỏ hơn để sinh ra bản nháp và sử dụng LLM để xác minh các token nháp với một lần chuyển tiếp duy nhất. Trong khung này, giải mã song song theo khối (Stern et al., 2018) và Medusa (Cai et al., 2023) đào tạo nhiều đầu dựa trên LLM để sinh token nháp.

Phương pháp của chúng tôi khác biệt với những phương pháp này bằng cách truy xuất token nháp từ kho dữ liệu, trình bày một con đường mới cho cải thiện hiệu quả trong sinh mô hình ngôn ngữ lớn. Mặc dù có một nghiên cứu tương tự, LLMA (Yang et al., 2023), sử dụng truy xuất để tăng tốc sinh, công trình của chúng tôi phân biệt mình theo hai cách chính: (1) Phương pháp LLMA được thiết kế cho các tình huống mà ngữ cảnh được tham chiếu (như trong Sinh Tăng cường Truy xuất và Sinh Hỗ trợ Bộ đệm) được cung cấp trong quá trình sinh, và nó chỉ truy xuất từ những ngữ cảnh được tham chiếu này. Ngược lại, phương pháp của chúng tôi truy xuất token nháp từ một kho dữ liệu toàn diện, do đó không bị giới hạn trong một ngữ cảnh nhỏ. (2) Trong khung LLMA, thực thể được truy xuất thường được giới hạn trong một hoặc một số ít. Tuy nhiên, phương pháp của chúng tôi được thiết kế để xử lý một số lượng thực thể được truy xuất lớn hơn nhiều. Sự khác biệt trong phương pháp này cho phép chúng tôi tận dụng cơ sở thông tin rộng hơn trong quá trình sinh.

3 Giải Mã Suy Đoán Dựa Trên Truy Xuất
Trong phần này, đầu tiên chúng tôi cung cấp ký hiệu và tổng quan nền tảng về giải mã suy đoán và sau đó giới thiệu khung REST được đề xuất của chúng tôi.

3.1 Nền tảng: Giải Mã Suy Đoán
Chúng tôi sử dụng x∈ V để biểu thị một token trong đó V là từ vựng. Tại mỗi bước thời gian t, cho ngữ cảnh trước đó s= (x1, ..., xt−1, xt), phương pháp giải mã tự hồi quy sinh ra token tại vị trí t+ 1 theo:
xt+1∼p(x|x1, . . . , xt;θlarge),
trong đó p(·) là phân phối xác suất có điều kiện được tính bởi LLM với tham số θlarge. Trong quá trình này, một lần chạy chuyển tiếp của LLM được yêu cầu tại mỗi bước sinh. Điều này tốn thời gian đáng kể do băng thông bộ nhớ

--- TRANG 3 ---
f = lambda num: [i for i ⬅ 📝Input Ngữ cảnh được truy xuất
numbers = […] \n for i
dictionary = {…} \n for i
import math \n for i
numbers = […] \n for i
file = open(…) \n for i
def sorted_c(…)\n for i Phần tiếp theo
in range(
, item in
in range(
in sorted(
in range(
in sorted Gốc 3
2 in Trie của
Phần tiếp theo
1,1
1 sorted 3 range (
(
_
1 item
in 5
1 Bước 1: Truy xuất tài liệu Bước 2: Xây dựng Trie
✔
✔✔
✔✔
✔✔✔ in
range
sorted
( Chú ý Cây Bước 3: Xác minh ứng viên
Giải Mã Suy Đoán Dựa Trên Truy Xuất (REST) 📜 Ứng viên
in
in range ✅
✅✅
❌ in range(
in sorted

Hình 1: Tổng quan về REST. Trong quá trình suy luận, ngữ cảnh đầu vào được sử dụng làm truy vấn để truy xuất tài liệu từ kho dữ liệu khớp với hậu tố dài nhất của đầu vào. Một Trie được xây dựng bằng cách sử dụng các phần tiếp theo từ các tài liệu được truy xuất. Chúng tôi cắt tỉa các nhánh có tần suất thấp (trọng số) và cây con còn lại được sử dụng làm ứng viên. Các ứng viên sẽ được đưa vào LLM với mặt nạ chú ý cây để xác minh. Tất cả token đúng từ đầu sẽ được chấp nhận, và các token nháp sau lỗi đầu tiên sẽ bị từ chối.

và không thể khai thác đầy đủ sức mạnh tính toán của phần cứng GPU hiện đại (Shazeer, 2019).

Giải mã suy đoán nhằm giảm chi phí tính toán trong quá trình suy luận bằng cách giảm số lần thực thi với θlarge. Ngoài LLM θlarge, giải mã suy đoán tận dụng một mô hình ngôn ngữ khác có kích thước nhỏ hơn nhiều với tham số θsmall. Tại bước t, phương pháp hoạt động bằng cách thực hiện lặp đi lặp lại các bước sau.

Xây dựng bản nháp Cho s= (x1, . . . , xt), LM nhỏ θsmall được sử dụng để sinh ra m token tiếp theo ˜xt+1, . . . , ˜xt+m theo cách tự hồi quy:
˜xt+i∼p(x|s,˜xt+1, . . . , ˜xt+i−1;θsmall),
trong đó i= 1, . . . , m.

Mặc dù các token vẫn được sinh từng cái một, chi phí tính toán của quá trình này được giảm vì nó sử dụng θsmall thay vì θlarge.

Xác minh bản nháp Sau khi các token nháp ˜xt+1, . . . , ˜xt+m được sinh ra, chúng được đưa vào LLM cùng với ngữ cảnh s. LLM θlarge sau đó tính các xác suất có điều kiện với một lần chuyển tiếp duy nhất:
p(x|s;θlarge),
p(x|s,˜xt+1;θlarge),
. . .
p(x|s,˜xt+1, . . . , ˜xt+m−1;θlarge).

Chấp nhận bản nháp Bắt đầu từ token được sinh đầu tiên trong bản nháp, xác suất có điều kiện rút ra từ θsmall được so sánh với của θlarge. Chúng ta có thể sử dụng lấy mẫu từ chối đã sửa đổi để khớp phân phối được sinh với LLM (Leviathan et al., 2023; Chen et al., 2023). Đối với vị trí t+i, đầu tiên chúng ta lấy mẫu một giá trị r từ phân phối đều U(0,1). Nếu r <min 1,p(x|s,˜xt+1,...,˜xt+i−1;θlarge) p(x|s,˜xt+1,...,˜xt+i−1;θsmall), chúng ta chấp nhận token nháp ˜xt+i và tiếp tục xác nhận token tiếp theo ˜xt+i+1. Ngược lại, chúng ta dừng quá trình chấp nhận, lấy mẫu lại xt+i, từ chối tất cả token nháp sau xt+i, và chuyển sang quá trình giải mã suy đoán mới tại vị trí t+i+ 1.

3.2 Phương pháp của chúng tôi: REST
Trong khi trong giải mã suy đoán cổ điển, một LM nhỏ hơn được sử dụng làm mô hình nháp, việc tìm một mô hình nháp chất lượng cao thường là thách thức vì một số lý do: (1) Để hiệu quả, mô hình nháp cần đủ nhẹ để không gây ra nhiều chi phí. (2) Để chất lượng, nó cần dự đoán đầu ra LLM một cách chính xác. (3) Để tích hợp hệ thống, nó cần cùng bộ từ vựng với LLM, và một kiến trúc phân phối dễ dàng với cấu hình tương tự như LLM (Chen et al., 2023). Những thách thức này đòi hỏi việc lựa chọn cẩn thận hoặc thậm chí đào tạo các mô hình nháp tùy chỉnh cho mỗi LLM mới.

Trong bài báo này, chúng tôi giải quyết các thách thức theo cách khác. Chúng tôi phát triển một phương pháp không cần đào tạo cho giải mã suy đoán

--- TRANG 4 ---
mà có thể dễ dàng tích hợp với bất kỳ mô hình mới nào để tăng tốc suy luận. Thay vì dựa vào mô hình nháp tham số, phương pháp Giải Mã Suy Đoán Dựa Trên Truy Xuất (REST) của chúng tôi đề xuất sử dụng truy xuất để xây dựng bản nháp. Tổng quan về REST được hiển thị trong Hình 1. Trong phần sau, đầu tiên chúng tôi mô tả việc xây dựng kho dữ liệu và các thao tác trên đó, sau đó trình bày việc sử dụng nó để xây dựng và xác minh bản nháp. Cùng nhau, REST cung cấp một giải pháp hiệu quả, chất lượng cao và dễ tích hợp để tăng tốc suy luận của LLM.

Xây dựng kho dữ liệu REST hoạt động dựa trên kho dữ liệu được xây dựng sẵn D={(ci, ti)}, trong đó ci đại diện cho một ngữ cảnh và ti đại diện cho phần tiếp theo tương ứng của ngữ cảnh ci. Cho một kho dữ liệu văn bản/mã, chúng tôi xây dựng kho dữ liệu D sử dụng ngữ cảnh tiền tố và phần tiếp theo tương ứng tại mỗi vị trí.

Truy xuất từ kho dữ liệu Trong suy luận, cho một ngữ cảnh s= (x1, . . . , xt), mục tiêu của chúng tôi là xây dựng các token nháp có khả năng là phần tiếp theo của s. Khác với giải mã suy đoán vanilla sử dụng LM nhỏ để xây dựng bản nháp, chúng tôi tận dụng kho dữ liệu được xây dựng D và trực tiếp truy xuất token nháp từ kho dữ liệu. Đầu tiên chúng tôi sử dụng ngữ cảnh s để truy xuất các cặp ngữ cảnh-phần tiếp theo từ kho dữ liệu D và xây dựng một tập hợp các ứng viên tiếp theo S:
S={ti|(ci, ti)∈Retrieve (D, s)},
trong đó Retrieve (D, s) thực hiện một quá trình truy xuất trong kho dữ liệu D trả về một tập hợp các cặp ngữ cảnh-phần tiếp theo {(ci, ti)} bằng cách sử dụng s làm truy vấn.

Việc sử dụng các mô hình truy xuất dày đặc gần đây (Khandelwal et al., 2020; Karpukhin et al., 2020) để tìm ngữ cảnh ci tương tự với s là đơn giản. Tuy nhiên, việc sử dụng bộ truy xuất dày đặc thêm chi phí bổ sung trong quá trình suy luận. Chúng tôi thay vào đó sử dụng phương pháp khớp chính xác nhanh để truy xuất các ứng viên tiếp theo. Quá trình truy xuất của chúng tôi được hiển thị trong Thuật toán 1. Chúng tôi nhằm tìm các ngữ cảnh trong D khớp với hậu tố dài nhất của s. Chúng tôi sử dụng chiến lược tham lam và bắt đầu từ giới hạn trên độ dài khớp được định nghĩa trước nmax. Đối với mỗi độ dài hậu tố n, chúng tôi lấy hậu tố của ngữ cảnh s với n token q (dòng 5), và lấy tất cả ngữ cảnh ci khớp với q như một hậu tố (dòng 6). Nếu ít nhất một ngữ cảnh trong D khớp với q hiện tại (tức là S̸=∅), chúng tôi trả về các cặp ngữ cảnh-phần tiếp theo tương ứng làm kết quả truy xuất; ngược lại chúng tôi giảm độ dài khớp n đi một và cố gắng khớp hậu tố ngắn hơn (dòng 7). Chúng tôi sử dụng mảng hậu tố (Manber and Myers, 1993) để thực hiện khớp chính xác hiệu quả trong kho dữ liệu D cho một q cho trước. Quá trình truy xuất dẫn đến chi phí không đáng kể (<6%) trong các thí nghiệm của chúng tôi (xem chi tiết trong Phần 5).

Thuật toán 1 Thuật toán truy xuất dựa trên khớp chính xác
Retrieve (D, s). Chúng tôi trả về các cặp ngữ cảnh-phần tiếp theo trong D khớp với hậu tố dài nhất của s.
1:Đầu vào: Ngữ cảnh s, kho dữ liệu D, độ dài hậu tố tối đa nmax
2:Khởi tạo n←nmax
3:Khởi tạo S← ∅
4:while S=∅do
5: q←suffix (s, n)
6: S← {(ci, ti)|q=suffix (ci, n)} ⊆D
7: n←n−1
8:end while
9:return S

Xây dựng bản nháp từ kết quả truy xuất Kết quả truy xuất S bao gồm các phần tiếp theo có thể có của ngữ cảnh s. Đối với mỗi ti∈S, bất kỳ tiền tố nào của ti có thể phục vụ như token nháp của s trong giải mã suy đoán và được xác minh thêm bởi LLM. Lưu ý rằng tập hợp các ứng viên tiếp theo được truy xuất S có thể lớn. Việc sử dụng tất cả ứng viên làm token nháp và đưa chúng vào LLM để xác minh là không khả thi. Ở đây chúng tôi trình bày cách chọn token nháp chất lượng cao từ tập hợp được truy xuất S. Một chiến lược ngây thơ là lấy mẫu một tập con chuỗi trong S làm token nháp. Tuy nhiên, điều này không tối ưu vì việc lấy mẫu chứa tính ngẫu nhiên khi S lớn.

Chúng tôi chọn token nháp từ kết quả truy xuất S sử dụng Trie. Trong Trie, đường dẫn duy nhất từ một nút đến nút gốc tương ứng với một tiền tố của ti∈S. Đối với mỗi nút, chúng tôi gán một trọng số phản ánh số lượng (tần suất) của tiền tố tương ứng xuất hiện trong các ứng viên được truy xuất. Như được hiển thị trong Thuật toán 2, đầu tiên chúng tôi xây dựng một Trie sử dụng tất cả chuỗi trong S, và trọng số nút được cập nhật khi một ứng viên ti được chèn vào Trie (dòng 2-7). Cấu trúc dữ liệu Trie cho phép chúng tôi ưu tiên token sử dụng trọng số và chọn các tiền tố tần suất cao (dòng 8-15). Trong việc thực hiện thực tế, chúng tôi chọn một cây con chứa c nút hàng đầu với trọng số cao nhất,

Chúng tôi đặt nmax là 16 trong các thí nghiệm, vì chỉ có vài trường hợp dẫn đến khớp tối đa với hơn 16 token.

--- TRANG 5 ---
tương đương với việc chọn c tiền tố tần suất cao hàng đầu làm chuỗi nháp.

Thuật toán 2 Lựa chọn chuỗi nháp sử dụng Trie.
1:Đầu vào: Ứng viên Tiếp theo S, siêu tham số c
2:Khởi tạo Trie T
3:foreach ti∈S do
4: foreach tiền tố của ti do
5: Chèn tiền tố vào T và cập nhật trọng số nút
6: end for
7:end for
8:Khởi tạo hàng đợi ưu tiên trống Q (Max Heap dựa trên trọng số nút)
9:foreach nút trong T do
10: Thêm (node.prefix, node.weight) vào Q
11:end for
12:while Q.size > c do
13: Lấy ra tiền tố có trọng số nhỏ nhất từ Q
14:end while
15:return Q

Xác minh bản nháp của REST Trong REST, nhiều chuỗi nháp có thể được truy xuất từ kho dữ liệu. Mặc dù ban đầu có thể tiếp cận các bản nháp một cách độc lập và đưa chúng vào LLM như các chuỗi riêng biệt trong một batch, quan sát thực tế cho thấy nhiều bản nháp chia sẻ tiền tố chung. Điều này dẫn đến tính toán dư thừa của các lớp Transformer trên những tiền tố chia sẻ này qua các chuỗi khác nhau, dẫn đến lãng phí sức mạnh tính toán. Để tối ưu hóa hiệu quả, chúng tôi xây dựng một chuỗi giả từ cây con sử dụng tìm kiếm theo chiều rộng đầu tiên. Theo định nghĩa, có thể ngay lập tức thu được rằng mỗi bản nháp tạo thành một chuỗi con của chuỗi giả này, và bất kỳ tiền tố chia sẻ nào chỉ xuất hiện một lần. Để thực hiện chính xác LLM trên chuỗi giả này, chúng tôi thực hiện một mặt nạ chú ý được thiết kế cẩn thận trong mỗi lớp chú ý, đảm bảo rằng việc tính toán của mỗi token phản ánh chính xác các phụ thuộc của nó trong chuỗi nháp gốc. Chiến lược chú ý này cũng được gọi là chú ý cây (Cai et al., 2023; Miao et al., 2023; Spector and Re, 2023).

Chấp nhận bản nháp của REST Chúng tôi áp dụng chiến lược chấp nhận đơn giản hơn so với giải mã suy đoán gốc. Bằng cách đưa các bản nháp vào LLM, chúng tôi thu được phân phối có điều kiện tại mỗi vị trí được đưa ra bởi θlarge, nơi chúng tôi lấy mẫu token mới. Sau đó chúng tôi đánh giá liệu token mới được lấy mẫu có trùng với token nháp tại mỗi vị trí hay không. Tất cả token nháp đúng từ đầu sẽ được chấp nhận, và các token nháp sau lỗi đầu tiên sẽ bị từ chối. Bằng cách này, các chuỗi được tạo ra bằng REST giống hệt với những chuỗi được sinh bởi sinh tự hồi quy tiêu chuẩn.

So sánh với các phương pháp hiện tại Mặc dù REST tuân theo lược đồ tương tự như của giải mã suy đoán, nó cung cấp những lợi thế đáng kể so với các phương pháp hiện tại. Các phương pháp giải mã suy đoán hiện tại dựa vào một mô hình nhỏ chất lượng cao để sinh token nháp (Leviathan et al., 2023; Chen et al., 2023). Những phương pháp như vậy phải cân bằng giữa kích thước nhỏ và sức mạnh dự đoán mạnh mẽ, đồng thời cũng khớp với từ vựng của mô hình cơ sở. Hơn nữa, chúng yêu cầu bộ nhớ GPU bổ sung và gây ra độ phức tạp trong quá trình suy luận. Ngược lại, REST trực tiếp truy xuất token nháp từ kho dữ liệu, có thể dễ dàng tích hợp với các mô hình ngôn ngữ có bất kỳ kích thước, từ vựng hoặc kiến trúc nào. Khác với Stern et al. (2018) và Cai et al. (2023) đào tạo các mô-đun chuyên biệt để tạo ra mô hình nháp, REST loại bỏ nhu cầu về bất kỳ bước đào tạo bổ sung nào và có thể phục vụ như một giải pháp cắm-và-chạy để giải mã hiệu quả qua các mô hình khác nhau. Hơn nữa, hiệu quả của REST bị ảnh hưởng bởi chất lượng kết quả truy xuất. Điều này mở ra cơ hội để nâng cao hơn nữa REST bằng cách sử dụng kho dữ liệu tốt hơn/lớn hơn hoặc mô hình truy xuất tiên tiến. Chúng tôi cũng lưu ý rằng ngoài việc sử dụng REST trực tiếp, có thể kết hợp REST với giải mã suy đoán vanilla. Sự kết hợp này có thể nâng cao tốc độ sinh của LM nhỏ. Chúng tôi để dành điều này cho công việc tương lai.

4 Thí nghiệm
4.1 Thiết lập Thí nghiệm
Chiến lược lấy mẫu Chúng tôi thực hiện hai cơ chế lấy mẫu: lấy mẫu tham lam và lấy mẫu nhân (Holtzman et al., 2019) cho LLM. Lấy mẫu tham lam chọn token có xác suất cao nhất tại mỗi bước. Lấy mẫu nhân, còn được gọi là lấy mẫu top-p, sinh token bằng cách lấy mẫu từ các token có khả năng nhất trong phân phối dự đoán của mô hình cho đến khi xác suất tích lũy của chúng đạt ngưỡng p. Đáng chú ý rằng theo phương pháp của chúng tôi, chúng tôi chỉ chấp nhận token nháp nếu chúng khớp với token được lấy mẫu từ

--- TRANG 6 ---
LLM. Do đó, các chuỗi được tạo ra bằng REST giống hệt với những chuỗi được sinh bởi sinh tự hồi quy tiêu chuẩn.

Tập dữ liệu và mô hình Chúng tôi tiến hành thí nghiệm trên hai tập dữ liệu: HumanEval (Chen et al., 2021) và MT-Bench (Zheng et al., 2023). HumanEval là một tập dữ liệu bao gồm 164 bài toán lập trình Python được viết bởi con người. Mục tiêu cho các mô hình là sinh ra giải pháp mã sử dụng docstring được cung cấp làm gợi ý. Mặt khác, MT-Bench chứa 80 câu hỏi nhiều lượt được thiết kế để mô phỏng các cuộc đối thoại nhiều lượt thế giới thực. Chúng tôi so sánh tốc độ sinh của sinh tự hồi quy tiêu chuẩn với REST, tập trung vào cả tập dữ liệu HumanEval và MT-Bench. Đối với HumanEval, chúng tôi thực hiện đánh giá 1-shot cho lấy mẫu tham lam và đánh giá 10-shot cho lấy mẫu nhân và sử dụng CodeLlama (Rozière et al., 2023). Trong khi đối với MT-Bench, chúng tôi thực hiện đánh giá 1-shot cho cả lấy mẫu tham lam và lấy mẫu nhân và sử dụng Vicuna (Chiang et al., 2023). Chúng tôi kiểm tra cả cấu hình 7B và 13B của CodeLlama và Vicuna, với giới hạn sinh tối đa là 512 token và 1024 token tương ứng. Tất cả thí nghiệm được tiến hành trên một GPU NVIDIA A6000 và 96 lõi CPU. Tất cả kết quả được lấy trung bình qua ba lần chạy khác nhau.

Siêu tham số Khi thực hiện khớp chính xác trong kho dữ liệu, độ dài hậu tố ngữ cảnh bắt đầu, nmax, được đặt là 16, và được giảm dần đi một cho đến khi chúng tôi tìm thấy ngữ cảnh khớp trong kho dữ liệu. Độ dài của mỗi ứng viên tiếp theo được truy xuất được ký hiệu là m, được cắt ngắn thành 10. Kết quả thực nghiệm từ Medusa (Cai et al., 2023) đề xuất 64 token nháp là cấu hình tính toán tối ưu. Do đó, chúng tôi giới hạn số lượng token nháp được chọn tối đa trong Trie được xây dựng thành 64, được ký hiệu là c.

Thang đo Thang đo đầu tiên chúng tôi sử dụng là Thời gian Token Trung bình, là thời gian sinh trung bình của một token cho LLM. Thang đo khác, Độ dài Sinh Trung bình, được tính như tỷ lệ giữa độ dài của các token được sinh và số bước chuyển tiếp được thực hiện bởi LLM gốc. Chính thức, nếu L biểu thị độ dài của các token được sinh và F đại diện cho số bước chuyển tiếp, Độ dài Sinh Trung bình, M, được đưa ra bởi:
M=L/F.

Lưu ý rằng Độ dài Sinh Trung bình (M) hoạt động như giới hạn trên của tăng tốc mà REST có thể đạt được, bỏ qua chi phí cho việc truy xuất và xây dựng token nháp.

Kho dữ liệu Đối với CodeLlama, chúng tôi xây dựng kho dữ liệu sử dụng một phần mã tiền đào tạo Python từ The Stack (Kocetkov et al., 2022). Tập dữ liệu này bao gồm khoảng 2.7M mẫu mã Python và dẫn đến kho dữ liệu có kích thước 27GB. Mặt khác, đối với Vicuna, chúng tôi xây dựng kho dữ liệu sử dụng dữ liệu rút ra từ UltraChat (Ding et al., 2023). Tập dữ liệu này bao gồm khoảng 774K cuộc hội thoại từ ChatGPT, tạo ra kho dữ liệu có kích thước 12GB.

Baseline Chúng tôi thực hiện giải mã suy đoán (Leviathan et al., 2023; Chen et al., 2023) làm baseline để so sánh. Đối với các LM nháp nhỏ, chúng tôi kiểm tra nhiều kích thước mô hình khác nhau, bao gồm Llama 68M và Llama 160M được đào tạo bởi Miao et al. (2023), TinyLlama 1.1B và TinyLlama-Chat 1.1B được đào tạo bởi Zhang et al. (2023). Chúng tôi cũng kiểm tra số lượng token nháp khác nhau từ 1 đến 15 (hiệu suất giảm khi lớn hơn 15).

4.2 Kết quả Chính
Bảng 1 so sánh tốc độ sinh của REST với tốc độ của giải mã tự hồi quy tiêu chuẩn và giải mã suy đoán.

Về tốc độ sinh, REST thể hiện sự nâng cao tốc độ đáng kể so với giải mã tự hồi quy tiêu chuẩn và giải mã suy đoán, đạt được tăng 2.16× đến 2.36× cho CodeLlama trong benchmark HumanEval. Benchmark MT-Bench cũng cho thấy tăng tốc cho Vicuna khi sử dụng phương pháp của chúng tôi, với hệ số từ 1.62× đến 1.77×. Những kết quả thực nghiệm này làm tăng trọng lượng cho hiệu quả của phương pháp chúng tôi trong việc tăng tốc quá trình sinh của LLM. Lưu ý rằng tăng tốc của lấy mẫu nhân không tốt bằng của lấy mẫu tham lam. Chúng tôi suy đoán rằng sự giảm hiệu suất này được gây ra bởi tính ngẫu nhiên được giới thiệu bởi lấy mẫu nhân. Vì giải mã suy đoán có thể đạt kết quả tốt hơn với LM nháp mạnh hơn phù hợp với LLM, chúng tôi không tuyên bố rằng REST có thể vượt trội hơn giải mã suy đoán trong mọi hoàn cảnh. Tuy nhiên, REST chắc chắn cung cấp một phương pháp mạnh mẽ và đơn giản cho suy luận nhanh hơn của LLM.

Một quan sát thú vị khác nổi lên từ những kết quả này là bản chất phụ thuộc vào lĩnh vực của các cải thiện tốc độ. Đặc điểm này cũng đã được ghi nhận trong các phương pháp khác như giải mã suy đoán (Chen et al., 2023) và Medusa (Cai et al., 2023). Cụ thể, tăng tốc đạt được với REST lớn hơn đáng kể trong benchmark HumanEval so với benchmark MT-Bench, cho thấy rằng hiệu quả của REST có thể thay đổi tùy thuộc vào lĩnh vực cụ thể.

Ngoài ra, điều quan trọng cần lưu ý là thời gian trung bình (chia cho tổng số token) cần thiết cho truy xuất (bao gồm thời gian để xây dựng Trie) là dưới 1 ms. Thời gian này rất nhỏ và có thể, cho tất cả mục đích thực tế, được coi là không đáng kể. Thời gian truy xuất không đáng kể này càng nhấn mạnh hiệu quả của REST.

5 Nghiên cứu Khử bỏ
Để hiểu sâu hơn về phương pháp của chúng tôi, chúng tôi tiến hành một loạt nghiên cứu khử bỏ và phân tích tập trung vào từng thành phần riêng lẻ. Nhiều nghiên cứu khử bỏ khác có thể được tìm thấy trong Phụ lục B.

https://chat.openai.com/

--- TRANG 7 ---
[Bảng 1: Tốc độ trên HumanEval và MT-Bench với giải mã tự hồi quy tiêu chuẩn, giải mã suy đoán và REST. Nhiệt độ được đặt là 0.8 và top-p là 0.95 cho lấy mẫu nhân trong HumanEval. Đối với MT-Bench, các thiết lập là 0.7 cho nhiệt độ và 0.8 cho top-p. Đối với giải mã suy đoán, chúng tôi tiến hành thí nghiệm sử dụng số lượng token nháp khác nhau và LM nhỏ khác nhau và ghi lại kết quả tốt nhất (kết quả chi tiết có thể được tìm thấy trong Phụ lục A). Tất cả thí nghiệm được tiến hành trên một GPU NVIDIA A6000 và 96 lõi CPU với kích thước batch là 1.]

Ảnh hưởng của kích thước kho dữ liệu Tăng kích thước kho dữ liệu là một chiến lược hiệu quả để nâng cao độ chính xác của token nháp được truy xuất trong Trie, điều này có thể tăng đáng kể tốc độ sinh. Trong Bảng 2, chúng tôi cho thấy khi kích thước kho dữ liệu tăng, cả Độ dài Sinh Trung bình và Thời gian Token Trung bình đều cải thiện tương ứng. Tuy nhiên, điều quan trọng cần lưu ý là tăng trưởng tăng tốc không rõ ràng như của Độ dài Sinh Trung bình. Sự khác biệt này có thể được quy cho chi phí của việc lấy token nháp. Chúng tôi giả định rằng trong các ứng dụng công nghiệp, sẽ có đủ lưu trữ đĩa để xây dựng kho dữ liệu lớn và đủ lõi CPU cho truy xuất nhanh. Chúng tôi cũng trực quan hóa xu hướng mở rộng kích thước kho dữ liệu truy xuất trong Hình 2. Từ đó, chúng tôi có thể suy ra rằng vẫn còn tiềm năng để đạt được tốc độ thậm chí nhanh hơn với kho dữ liệu lớn hơn.

Ảnh hưởng của chiến lược lựa chọn token nháp Chúng tôi so sánh việc lựa chọn token nháp trong Trie với việc lấy mẫu ngẫu nhiên các ứng viên tiếp theo được truy xuất làm token nháp. Để so sánh công bằng, chúng tôi sử dụng kỹ thuật lấy mẫu ngẫu nhiên để lấy mẫu nhiều nhất tám chuỗi từ tất cả ứng viên được truy xuất. Hơn nữa, mỗi chuỗi được cắt ngắn đến độ dài tối đa là 8. Điều này dẫn đến số lượng token nháp tối đa là 64, tương ứng với số lượng token nháp được chọn tối đa từ Trie. Dữ liệu được trình bày trong Bảng 3 cho thấy việc lựa chọn token nháp từ Trie, trái ngược với việc sử dụng phương pháp lấy mẫu ngẫu nhiên, nâng cao hiệu suất.

Ảnh hưởng của lựa chọn độ dài hậu tố tối đa Chúng tôi thay đổi giá trị của nmax để kiểm tra tốc độ sinh của REST. Kết quả của nghiên cứu này được mô tả trong Hình 3. Một quan sát thú vị là khi giá trị của nmax được đặt dưới 6, có sự tăng đáng kể trong thời gian sinh. Ngược lại, khi nmax vượt quá 6, tốc độ sinh vẫn luôn cao và dường như phần lớn không bị ảnh hưởng bởi những thay đổi thêm vào giá trị nmax. Do đó, trong thực tế, không có nhu cầu đáng kể để dành quá nhiều nỗ lực trong việc lựa chọn giá trị tối ưu chính xác của nmax.

6 Kết luận
Trong công trình này, chúng tôi đề xuất REST: giải mã suy đoán dựa trên truy xuất. Thay vì yêu cầu một LM nhỏ, REST sử dụng kho dữ liệu để truy xuất và sử dụng token nháp. Chúng tôi xây dựng Trie để chọn các token nháp có khả năng nhất. REST không chỉ đơn giản để thực hiện mà còn dễ dàng tích hợp vào quá trình sinh của bất kỳ mô hình ngôn ngữ hiện tại nào mà không cần đào tạo thêm. Chúng tôi muốn khám phá truy xuất quy mô lớn trong bước tiếp theo. Đối với các tình huống mà lưu trữ đĩa bị hạn chế, chúng tôi cũng sẽ khám phá các phương pháp giảm thiểu kích thước kho dữ liệu mà không ảnh hưởng đến hiệu suất.

Hạn chế
Những hạn chế của công trình chúng tôi như sau:
• Mặc dù tính chất cắm-và-chạy của REST, điều quan trọng cần thừa nhận rằng hiệu suất của REST bị ảnh hưởng trực tiếp bởi độ chính xác và tính đầy đủ của kho dữ liệu. Để cải thiện sự phù hợp với LLM, có thể có lợi khi cân nhắc xây dựng kho dữ liệu từ nội dung được sinh bởi chính LLM.

• Thiếu khả năng trong ngữ cảnh. Ví dụ, thách thức của việc truy xuất tên biến được cá nhân hóa trong sinh mã—một nhiệm vụ vốn đòi hỏi hiểu biết ngữ cảnh—đặt ra một câu hỏi thú vị: Làm thế nào chúng ta có thể trao quyền cho các phương pháp truy xuất để xử lý hiệu quả những phức tạp như vậy?

Lời cảm ơn
JDL ghi nhận sự hỗ trợ của ARO dưới MURI Award W911NF-11-1-0304, Học bổng Nghiên cứu Sloan, NSF CCF 2002272, NSF IIS 2107304, NSF CIF 2212262, Giải thưởng Nhà điều tra Trẻ ONR, và NSF CAREER Award 2144994. Chúng tôi cảm ơn tất cả các nhà đánh giá ẩn danh vì những đánh giá rất cẩn thận và chi tiết cũng như những đề xuất có giá trị. Sự giúp đỡ của họ đã nâng cao hơn nữa công trình của chúng tôi.

[Danh sách tài liệu tham khảo và các phụ lục tiếp tục...]
