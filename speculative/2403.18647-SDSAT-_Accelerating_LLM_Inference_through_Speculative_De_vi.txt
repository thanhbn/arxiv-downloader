# 2403.18647.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/speculative/2403.18647.pdf
# Kích thước tệp: 3892879 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
SDSAT: Tăng tốc Suy luận LLM thông qua Giải mã Suy đoán với Các Token Thích ứng Ngữ nghĩa
Chengbo Liu∗Yong Zhu†
Tóm tắt
Chúng tôi đề xuất một phương án tăng tốc cho các mô hình ngôn ngữ lớn (LLMs) thông qua Giải mã Suy đoán với Các Token Thích ứng Ngữ nghĩa (SDSAT). Mục tiêu chính của thiết kế này là nâng cao khả năng của mô hình LLM trong việc tạo ra các token bản thảo chính xác hơn mà không làm giảm độ chính xác của mô hình. Các chiến lược cốt lõi bao gồm: 1) Tinh chỉnh mô hình bằng cách kết hợp các token thích ứng ngữ nghĩa có khả năng giải mã linh hoạt mà không thay đổi cấu trúc của nó, cho phép chúng tạo ra các token bản thảo chất lượng cao. 2) Bằng cách sử dụng phương pháp huấn luyện không ảnh hưởng đến các token tiêu chuẩn, mô hình có thể có được khả năng giải mã song song trên khung gốc của nó với chi phí huấn luyện tối thiểu. 3) Chúng tôi đã thiết kế các chiến lược tạo ra "hai-bước-bản-thảo-sau-đó-xác-minh" sử dụng cả tìm kiếm tham lam và lấy mẫu nhân. Các thử nghiệm được tiến hành trên các mô hình CodeLlama-13B và 7B đã cho kết quả tăng tốc hơn 3.5X và 3.0X, tương ứng.
Vui lòng tham khảo https://github.com/ainergy-ml/SDSAT
1 Giới thiệu
Các LLM dựa trên Transformer, mặc dù được huấn luyện hiệu quả song song trên TPU và GPU, gặp phải những hạn chế trong việc lấy mẫu tự hồi quy do nhu cầu băng thông bộ nhớ cao (Stern et al., 2018), dẫn đến độ trễ khi tạo ra nhiều token từng cái một. Gần đây, các thuật toán dựa trên ý tưởng giải mã suy đoán đã được phát triển đặc biệt để tối ưu hóa tốc độ. Các phương pháp này trước tiên thu được các token bản thảo với chi phí thấp hơn, sau đó sử dụng mô hình đích để xác minh. Chúng có thể được phân loại như sau:
Loại đầu tiên dựa vào các mô hình nhỏ hơn. Chen et al. (2023), và Leviathan et al. (2023) đều sử dụng một mô hình bản thảo nhỏ hơn nhanh hơn nhưng ít mạnh mẽ hơn. Chất lượng của bản thảo, được tạo ra bởi mô hình này, có thể so sánh với việc lấy mẫu một token duy nhất từ mô hình đích lớn hơn. Leviathan et al. (2023) đã giới thiệu phương pháp lấy mẫu suy đoán và chứng minh rằng phân phối của văn bản được tạo ra vẫn không thay đổi cho cả cài đặt tham lam và không tham lam, đạt được sự tăng tốc 2X-3X so với việc triển khai T5X tiêu chuẩn. Zhou et al. (2023) tận dụng Chưng cất Kiến thức (KD) để nâng cao giải mã suy đoán bằng cách căn chỉnh một mô hình học sinh nhỏ hơn (bản thảo) với một mô hình giáo viên (đích) để có tỷ lệ chấp nhận tốt hơn.
Loại khác là sử dụng chính mô hình đó, suy luận một số lượng lớn hơn các token cùng một lúc để tăng tốc độ. Santilli et al. (2023) triển khai quy trình giải mã tham lam song song với một khối token, mà không tiến hành bất kỳ huấn luyện nào. Medusa (Cai et al., 2023) tận dụng một loạt các MLP để dự đoán token, dựa vào các đặc trưng từ lớp thứ hai từ trên xuống của Mô hình Ngôn ngữ Lớn gốc. Phương pháp này giảm đáng kể thời gian cần thiết để tạo ra các bản thảo. Xia et al. (2023) cho thấy rằng cách tiếp cận của họ có thể đạt được khoảng 5 × tăng tốc trong các nhiệm vụ seq2seq khác nhau bằng cách che token, bao gồm dịch máy và tóm tắt trừu tượng. Tuy nhiên, độ dài của việc tạo token được xử lý tương đối ngắn. Eagle (Li et al., 2024) xây dựng trên mô hình hiện có bằng cách chỉ thêm một plugin nhẹ (một lớp giải mã transformer duy nhất) vào LLM. Plugin này được huấn luyện để đưa mô hình nhỏ càng gần càng tốt với khả năng của mô hình đích. Quá trình suy luận của mô hình đích sử dụng tree attention (Cai et al., 2023) (Miao et al., 2024) (Spector & Re, 2023) để thu được tất cả các đầu ra cùng một lúc, và quá trình xác minh sử dụng lấy mẫu suy đoán (Leviathan et al., 2023) để đạt được kết quả cuối cùng từ mô hình đích.
Loại thứ ba là tạo token bản thảo dựa trên dữ liệu, được đại diện bởi He et al. (2023), sử dụng dữ liệu được tạo sẵn để giúp nhanh chóng thu được các câu trả lời thay thế cho các nhiệm vụ mới, sử dụng phương pháp tree-attention. Điều này cho phép thu được kết quả suy luận của mô hình đích tất cả cùng một lúc.
Giải pháp được đề xuất của chúng tôi mang lại những cải tiến đáng kể mà không cần đưa ra các mô hình nhỏ mới hoặc thực hiện các sửa đổi đối với mô hình hiện có. Điều này loại bỏ nhu cầu về các quy trình thích ứng phức tạp và khó khăn. Ngoài ra, không có yêu cầu về cơ sở dữ liệu bên ngoài. Bằng cách chịu một chi phí huấn luyện bổ sung tối thiểu, mô hình có thể nâng cao khả năng tạo ra các token bản thảo có độ chính xác cao, điều này trực tiếp góp phần vào việc tăng tốc hiệu suất của mô hình.
Những đóng góp chính của bài báo này như sau:
•Chúng tôi đã xác minh rằng các mô hình ngôn ngữ lớn (LLM) có thể tạo ra các token bản thảo chất lượng cao mà không yêu cầu bất kỳ sửa đổi nào đối với cấu trúc của chúng, thông qua việc giới thiệu các token thích ứng ngữ nghĩa.
•Chúng tôi đã phát triển một phương pháp luận huấn luyện sáng tạo cho phép các LLM tạo ra các token bản thảo chính xác mà không làm giảm độ chính xác và hiệu suất tổng thể của mô hình.
•Hơn nữa, chúng tôi đề xuất một phương pháp tạo ra "hai-bước-bản-thảo-sau-đó-xác-minh" hiệu quả cho cả tìm kiếm tham lam và lấy mẫu nhân (Holtzman et al., 2019), dẫn đến hiệu quả giải mã cao.
Sử dụng các mô hình CodeLlama-13B và 7B (Roziere et al., 2023), và chỉ huấn luyện trên 2-8B token, mô hình có thể duy trì độ chính xác gần như không thay đổi trong khi nâng cao đáng kể tốc độ.

--- TRANG 2 ---
Hình 1: Sơ đồ của các cơ chế suy luận, trong khi hình bên trái mô tả quá trình tìm kiếm tham lam, đây là một mẫu đặc biệt của quá trình lấy mẫu nhân được hiển thị ở hình bên phải. Dấu kiểm màu xanh lá cây cho biết một token được chấp nhận, và dấu x cho biết một token bị từ chối.

2 Phương pháp luận
Như đã thảo luận bởi Xia et al. (2023), cơ chế attention chia sẻ được sử dụng trong Medusa (Cai et al., 2023) hạn chế đáng kể hiệu quả soạn thảo, dẫn đến tỷ lệ loại bỏ cao của các token được soạn thảo. Ở đây, chúng tôi giới thiệu một phương pháp soạn thảo nối thêm các token thích ứng ngữ nghĩa vào chuỗi để sử dụng các truy vấn attention riêng biệt để dự đoán từng token được soạn thảo, cung cấp một cách tiếp cận linh hoạt áp dụng được cho các mô hình khác nhau mà không yêu cầu sửa đổi cấu trúc. Tiếp tục, chúng tôi đã thiết kế phương pháp tạo ra sử dụng cả tìm kiếm tham lam và lấy mẫu nhân, các quá trình được minh họa trong Hình 1.

2.1 Suy luận
2.1.1 Tìm kiếm tham lam

Hình 2: Một ví dụ về quá trình "hai-bước-bản-thảo-sau-đó-xác-minh" sử dụng chiến lược tạo tìm kiếm tham lam, với [32011], [32012], [32013] là các token thích ứng được chọn cho mô hình CodeLlama. Mỗi vòng lặp bao gồm ba bước: sau hai bước soạn thảo, bước thứ ba là xác minh. Trong sơ đồ, các token "by" và "fostering" trong vòng lặp1 không khớp, do đó token được chấp nhận đã xác minh cho vòng lặp1 kết thúc tại "by". Vòng lặp thứ hai vượt qua tất cả các xác minh, do đó kết quả được tạo ra bởi tất cả các token thích ứng được chấp nhận.

Để đơn giản, trước tiên chúng tôi minh họa nó bằng cách sử dụng phương pháp tạo tìm kiếm tham lam như được hiển thị ở nửa bên trái của Hình 1. Bước soạn thảo 1: Gắn k token thích ứng vào chuỗi và tạo ra k token bản thảo song song. Bước soạn thảo 2: Tận dụng các đầu ra từ bước 1 để tạo ra k token bản thảo thứ hai song song, và sau đó thực hiện xác minh của bước 3. Hình 2 cho thấy một ví dụ để minh họa quá trình lý luận rõ ràng hơn. Quá trình suy luận có thể được mô hình hóa như sau.

Bước 1: Bước soạn thảo 1
yn+1=argmaxP θ(yn+1|y1:n,X)
y′n+2=argmaxP θ(y′n+2|y1:n,a1,X)
· · ·
y′n+k+1=argmaxP θ(y′n+k+1|y1:n,a1:k,X)(1)

Bước 2: Bước soạn thảo 2
yn+2=argmaxP θ(yn+2|y1:n+1,X)
y′′n+3=argmaxP θ(y′′n+3|y1:n+1,y′n+2,X)
· · ·
y′′n+k+2=argmaxP θ(y′′n+k+2|y1:n+1,y′n+2:n+k+1,X)(2)

Bước 3: Chiến lược xác minh
yi={
yi, nếu n+1≤i≤n+2
y′′i, nếu n+2<i≤n+2+k và y′′i−1=y′i−1
dừng vòng lặp hiện tại, trường hợp khác (3)

Cho X đại diện cho đầu vào của mô hình, là một chuỗi có độ dài tùy ý, y đại diện cho đầu ra của mô hình và a đại diện cho các token thích ứng ngữ nghĩa. Cho rằng y′n+1 và y′′n+2 là các token đã được chấp nhận dứt khoát, chúng có thể được biểu thị trực tiếp là yn+1,yn+2. y′n+2,y′′n+3, ...,y′n+k+1,y′′n+k+2 đại diện cho các token bản thảo được tạo ra, cần xác định thêm liệu chúng có thể được chấp nhận hay không. Bằng cách so sánh các token được soạn thảo này, thông qua quá trình Xác minh Bước 3, chúng ta có thể thu được các token được chấp nhận. Khi điều kiện dừng của vòng lặp hiện tại được kích hoạt, sẽ có tối đa 2 + k token được chấp nhận. Có nghĩa là, nếu mọi token được soạn thảo đều được chấp nhận, điều này sẽ mang lại tối đa 2 + k token mỗi vòng lặp chỉ với 2 lần lặp, vượt qua việc triển khai đơn giản chỉ sẽ nhận được 2 token được chấp nhận. Bước này khác với quá trình được mô tả bởi Xia et al. (2023), nơi giai đoạn soạn thảo chỉ bao gồm một bước trước khi ngay lập tức chuyển sang giai đoạn xác minh. Phương pháp của họ chủ yếu dựa vào các đầu ra có xác suất cao hơn của mô hình làm tiêu chí để chấp nhận các token được soạn thảo. Tuy nhiên, có thể mô hình tự tin tạo ra các câu trả lời không chính xác với xác suất cao. Do đó, chỉ dựa vào xác suất token để xác minh có thể làm giảm độ chính xác trong quá trình suy luận mô hình. Cách tiếp cận xác minh của chúng tôi đảm bảo rằng dưới tìm kiếm tham lam, kết quả giải mã hoàn toàn bằng nhau với những kết quả từ quá trình suy luận gốc.

2.1.2 Lấy mẫu nhân

Như được hiển thị ở nửa bên phải của Hình 1, lấy mẫu nhân cũng có thể được chia thành ba bước.

Bước Soạn thảo 1: Bước này về cơ bản tương tự như phương pháp đã thảo luận trước đó, nơi một số token thích ứng được nối liền ở cuối chuỗi đầu vào và sau đó được gửi cùng nhau vào mô hình đích. Sự khác biệt nằm ở đầu ra cho mỗi vị trí; do việc sử dụng lấy mẫu nhân, thực际上có nhiều token bản thảo có thể.

Bước Soạn thảo 2: Các token bản thảo được tạo ra trong bước đầu tiên được tổ chức thành cấu trúc cây, nơi mỗi nhánh đại diện cho một chuỗi thay thế. Các chuỗi thay thế này sau đó được trình bày cho mô hình để thu được đầu ra trong một lần. Để xử lý chính xác một chuỗi ứng viên cho trước với mô hình chỉ giải mã, chúng tôi giới thiệu một cấu trúc cây được thiết kế thủ công, cùng với một mặt nạ attention tương ứng trong mỗi lớp attention. Cách tiếp cận này đảm bảo rằng việc tính toán cho mỗi token chính xác phản ánh các mối quan hệ của nó trong bước soạn thảo 1. Phương pháp này, được gọi là tree attention, đã được chi tiết trong nghiên cứu của He et al. (2023), Cai et al. (2023), và Li et al. (2024).

Xác minh: Trong giai đoạn xác minh, chúng tôi áp dụng một phương pháp đánh giá đơn giản tương tự như He et al. (2023), giống như chiến lược xác minh và chiến lược dừng được đề cập trong bối cảnh tìm kiếm tham lam. Chúng tôi chọn nhánh dài nhất của cây đã được xác minh và chấp nhận làm kết quả cho vòng lặp hiện tại.

2.2 Huấn luyện

Cách tiếp cận huấn luyện này khác với mô hình dựa trên Bart được huấn luyện bởi Xia et al. (2023), sử dụng token [Mask] nhất quán để che ngẫu nhiên trong các nhiệm vụ dịch thuật. Cho độ dài chuỗi ngắn điển hình của các nhiệm vụ dịch thuật, phương pháp này có thể hiệu quả. Tuy nhiên, đối với các chuỗi dài hơn, mà không có bất kỳ tối ưu hóa nào trong quá trình huấn luyện, hiệu quả học tập của các token tiêu chuẩn có thể bị ảnh hưởng xấu. Tốt hơn là áp dụng một cách tiếp cận huấn luyện mới, giảm thiểu tác động của các token thích ứng ngữ nghĩa đối với chính mô hình. Xem chi tiết mất mát trong Phụ lục A.

Cách tiếp cận cơ bản. Trong quá trình huấn luyện, các chuỗi đầu vào, ban đầu hoàn toàn bao gồm các token tiêu chuẩn, được thay thế có chọn lọc bằng các token thích ứng ngữ nghĩa, với số lượng thay thế xấp xỉ phân phối Poisson. Hoạt động này cũng có thể được coi là một hoạt động che và phân phối của số lượng token thích ứng có thể được coi như sự kết hợp của hai quá trình ngẫu nhiên độc lập. Ban đầu, 10% các vị trí trong một chuỗi có độ dài n được chọn trong một quá trình lựa chọn ngẫu nhiên phân phối đồng đều. Sau đó, kích thước cửa sổ che tối đa được định nghĩa là L, và kích thước cửa sổ che được xác định bởi một quá trình ngẫu nhiên phân phối đồng đều, chọn một giá trị từ 1 đến L. Sự kết hợp của hai quá trình này tạo ra một phân phối xác suất cụ thể. Đối với toàn bộ chuỗi, khi n lớn, phân phối của số lượng token được thay thế xấp xỉ phân phối Poisson. Cho A đại diện cho các token thích ứng ngữ nghĩa, chuỗi hỗn hợp này có thể được biểu thị là

M1:n=H(Y1:n,A1:L) (4)

Sau khi kết hợp các token thích ứng ngữ nghĩa, đối với một giá trị dự đoán cho trước, huấn luyện độc quyền với các token tiêu chuẩn dẫn đến một giá trị dự đoán cho tất cả các đầu vào là

ŷi=Pθ(ŷi|Y1:i−1) (5)

Bằng cách chèn trực tiếp tất cả các token thích ứng vào chuỗi để thu được chuỗi M, giá trị dự đoán cho đầu vào là

ŷi=Pθ(ŷi|M1:i−1) (6)

Cho hàm mất mát được ký hiệu bởi f. Đối với một chuỗi Y, hàm mất mát cho Mô hình Ngôn ngữ Hồi quy Tự động (LLM) có thể được biểu thị là

f(Ŷ,Y) =−1/n ∑i=1^n log(ŷi) (7)

Do đó, đối với hai chuỗi khác nhau, một là chuỗi đích YM và một khác là giá trị dự đoán M̂, mất mát của cách tiếp cận cơ bản có thể được biểu thị là

loss =f(M̂,YM) (8)

Trong quá trình huấn luyện của LLM, là huấn luyện song song, mất mát thường được tính toán như entropy chéo cho tất cả các token một cách bình đẳng. Cách tiếp cận cơ bản này để tính toán mất mát ảnh hưởng đáng kể đến việc huấn luyện các token tiêu chuẩn, dẫn đến mất mát cao hơn như Hình 5 cho thấy.

Cách tiếp cận cải tiến. Một cách tiếp cận cải tiến là cô lập tác động của các token thích ứng đối với các token tiêu chuẩn. Trong khi, trong quá trình suy luận mô hình, chỉ cần nối thêm một số token thích ứng ở cuối chuỗi, thay vì chèn chúng vào giữa. Tuy nhiên, để huấn luyện hiệu quả hơn, nhiều token thích ứng được chèn ngẫu nhiên vào giữa chuỗi trong quá trình huấn luyện. Điều này có thể ảnh hưởng đến việc huấn luyện các token tiêu chuẩn, vì sự hiện diện của các token thích ứng trước chúng có thể dẫn đến mất mát tăng lên cho các token tiêu chuẩn này so với hoàn cảnh bình thường. Để giải quyết vấn đề này, hai loại chuỗi đầu vào có thể được sử dụng trong quá trình huấn luyện thực tế. Loại đầu tiên hoàn toàn bao gồm các token tiêu chuẩn, dẫn đến một đầu ra được đại diện bởi Y. Loại thứ hai trộn các token tiêu chuẩn với các token thích ứng, với dự đoán được ký hiệu là M̂ và các nhãn tương ứng được ký hiệu là YM. Mmask đại diện cho một ma trận có cùng hình dạng với M, tương ứng với các vị trí trong chuỗi M được thay thế bởi các token thích ứng được đặt thành 1, và tất cả các vị trí khác được đặt thành 0. Mất mát sau đó được tính toán là:

loss =1/2(f(Ŷ,Y) +w f(M̂,YM)⊙Mmask) (9)

Trong cách tiếp cận huấn luyện này, hai loại chuỗi được đặt đơn giản để mỗi loại chiếm một nửa dữ liệu huấn luyện, với trọng số w được đặt thành 1.

--- TRANG 3 ---
3 Thí nghiệm
3.1 Tập dữ liệu

Dữ liệu huấn luyện của chúng tôi được chọn từ StarCoder (Li et al., 2023), được chọn hoàn toàn ngẫu nhiên theo tỷ lệ phân phối của các ngôn ngữ lập trình. Ngoài ra, chúng tôi đã tiến hành một quy trình khử nhiễm trên dữ liệu huấn luyện để đảm bảo rằng tập dữ liệu kiểm tra sẽ không xuất hiện trong tập dữ liệu huấn luyện. Theo cách tiếp cận được sử dụng bởi CodeLlama (Roziere et al., 2023), chúng tôi áp dụng một mục tiêu điền vào cho việc huấn luyện các mô hình của chúng tôi. Để trình bày chi tiết, chúng tôi phân tách các văn bản huấn luyện thành ba thành phần: một tiền tố, một phần giữa và một hậu tố, bằng cách chọn các điểm phân tách từ một phân phối đồng đều trên độ dài của tài liệu ở cấp độ ký tự. Sau đó chúng tôi sắp xếp lại các phân đoạn này thành hai cấu hình: tiền tố-hậu tố-giữa (PSM) cho một nửa số phân tách và hậu tố-tiền tố-giữa (SPM) cho phần còn lại, đảm bảo tương thích. Sự sắp xếp lại này được thực hiện với xác suất 50%, có nghĩa là mỗi cấu hình được áp dụng với xác suất 25%.

3.2 Chi tiết huấn luyện

Trong nghiên cứu của chúng tôi, chúng tôi cấu hình các tham số tinh chỉnh bằng cách tham khảo những tham số được nêu trong bài báo CodeLlama. Trình tối ưu là AdamW (Loshchilov & Hutter, 2017), với các giá trị β1 và β2 được đặt lần lượt là 0.9 và 0.95. Cách tiếp cận của chúng tôi bao gồm một giai đoạn tiền huấn luyện liên tục tập trung giới hạn chỉ 2000 bước, sử dụng lịch trình cosine với 400 bước khởi động. Chúng tôi đặt tốc độ học ban đầu là 5e-5, và tốc độ học cuối cùng là 1e-5. Quá trình huấn luyện tận dụng kích thước batch 4M token, được tổ chức thành các chuỗi của 16,384 token mỗi chuỗi. Trong thí nghiệm tiếp theo, chúng tôi huấn luyện cả mô hình CodeLlama-7B và 13B, thay đổi kích thước cửa sổ che tối đa (L) cho mỗi mô hình để thu được nhiều mô hình được huấn luyện, mà chúng tôi gọi là các mô hình SDSAT. Các token thích ứng ngữ nghĩa được sử dụng ở đây đều có cùng giá trị. Các mô hình được huấn luyện với các token đa dạng đã được nghiên cứu trong Phụ lục B

3.3 Kết quả

Chúng tôi đã tiến hành kiểm tra độ chính xác cho các mô hình CodeLlama-7B và 13B gốc. Đáng chú ý là khi kiểm tra chúng trên các tập dữ liệu sau đây, có nhiều chi tiết có thể ảnh hưởng đến độ chính xác, chẳng hạn như "stop words" và "max new tokens" của việc tạo ra, có nên thực hiện "strip operation" trên prompt hay không, v.v. Nếu những chi tiết này không nhất quán, kết quả cũng sẽ thay đổi tương ứng. Vì CodeLlama chưa tiết lộ các đặc tả kiểm tra của nó, việc đánh giá lại các mô hình đã phát hành của họ dẫn đến sự khác biệt nhỏ về độ chính xác so với các con số được báo cáo bởi họ. Chúng tôi tạm kết luận rằng những khác biệt nhỏ này do các sắc thái trong quá trình kiểm tra không làm giảm các kết luận tổng thể của công việc chúng tôi.

3.3.1 Tạo mã Python

Chúng tôi tham khảo bài báo CodeLlama để chọn các tập dữ liệu kiểm tra của chúng tôi. Tương tự, chúng tôi bắt đầu bằng cách báo cáo kết quả cho việc tạo mã Python sử dụng các benchmark HumanEval (Chen et al., 2021) và MBPP (Austin et al., 2021). Để đánh giá chính xác hơn, chúng tôi sử dụng phần chia kiểm tra của MBPP (sanitized), chứa 257 bài toán được xác minh thủ công. Ngoài ra, chúng tôi cũng tiến hành đánh giá lại trên phiên bản gốc của CodeLlama. Trong phần này, chúng tôi chỉ trình bày kết quả zero-shot. Bảng 1 chỉ ra rằng độ chính xác của mô hình 7B gần như giống hệt với mô hình CodeLlama gốc. Ngoài ra, mô hình 13B thể hiện độ chính xác tổng thể cao hơn một chút so với mô hình gốc.

3.3.2 Đánh giá đa ngôn ngữ

Tiếp theo, chúng tôi cũng đánh giá các mô hình được tinh chỉnh của chúng tôi trên một tập hợp đa dạng hơn các ngôn ngữ lập trình sử dụng benchmark MultiPL-E (Cassano et al., 2022). Giống như CodeLlama, chúng tôi báo cáo kết quả cho Python, C++, Java, PHP, TypeScript, C#, và Bash trong Bảng 2.

Sau khi huấn luyện, mô hình 7B trải qua một sự giảm nhẹ về độ chính xác trung bình, giảm từ 26.3% xuống 24.5%, với sự sụt giảm đáng kể nhất được quan sát trong các ngôn ngữ lập trình Java, C# và Bash. Ngược lại, mô hình 13B thể hiện sự tăng lên về độ chính xác trung bình, đặc biệt trong ngôn ngữ TypeScript (TS). Đáng chú ý là điều này có thể do những biến động vốn có trong quá trình huấn luyện gốc của CodeLlama cho TS, nơi độ chính xác của mô hình 7B trong TS thậm chí còn vượt qua mô hình 13B.

Trong việc đánh giá các mô hình sử dụng HumanEval (Chen et al., 2021) và MultiPL-E (Cassano et al., 2022), chúng tôi quan sát những biến động trong độ chính xác của các mô hình được lưu tại các giai đoạn khác nhau của quá trình huấn luyện. Những biến động này có thể được quy cho kích thước hạn chế của dữ liệu kiểm tra có sẵn cho mỗi ngôn ngữ. Ngoài ra, quan trọng là phải thừa nhận rằng những thiên lệch tiềm ẩn trong các tập dữ liệu được chọn ngẫu nhiên của chúng tôi có thể góp phần vào hiệu suất biến động trên nhiều ngôn ngữ mã hóa.

3.3.3 Đánh giá điền vào

Allal et al. (2023) đã điều chỉnh benchmark điền vào mã HumanEval cho các ngôn ngữ lập trình khác nhau thông qua việc sử dụng MultiPL-E (Cassano et al., 2022). Trong quá trình này, các dòng mã riêng lẻ được che, và các dự đoán được ghi điểm với một số liệu khớp chính xác so với giải pháp nền. Từ Bảng 3, chúng ta có thể thấy rằng các mô hình SDSAT được tinh chỉnh của chúng tôi và các mô hình CodeLlama gốc thể hiện hiệu suất tương tự trên Python, Java, và JavaScript. Đáng chú ý, trong Java, các mô hình được tinh chỉnh thể hiện độ chính xác cao hơn. Cho kích thước lớn của tập dữ liệu kiểm tra, kết quả tương đối ổn định.

3.4 Cải thiện thời gian thực

Để kiểm tra hiệu suất suy luận của các mô hình 7B và 13B, chúng tôi sử dụng một phần của HumanEval (Chen et al., 2021) và các tập dữ liệu điền vào MultiPL-E (Cassano et al., 2022) như đại diện cho các nhiệm vụ tạo ra và hoàn thành tương ứng, và phân tích các số liệu hiệu suất khác nhau của chúng, được tóm tắt trong Bảng 4. Chúng tôi đo lường cải thiện thời gian thực với kích thước batch 1 trên một NVIDIA A100 duy nhất cho cả tìm kiếm tham lam (temp=0) và lấy mẫu nhân (temp >0).

3.4.1 Tìm kiếm tham lam

Trong quá trình suy luận, bằng cách kiểm soát việc sử dụng số lượng khác nhau của các token thích ứng, được ký hiệu ở đây là k, chúng ta có thể quan sát xu hướng thay đổi trong các chỉ số hiệu suất khác nhau. Khi k tăng, thời gian vòng lặp không tăng tuyến tính mà vẫn gần như bằng phẳng, đây là một lý do quan trọng tại sao thuật toán của chúng tôi có thể cải thiện hiệu quả tốc độ suy luận. Theo Hình 3, số liệu "Token mỗi Giây" rõ ràng cho thấy rằng khi k tăng dần, tốc độ tiếp tục tăng, Cụ thể, tại k=13, có sự tăng khoảng 3.1X về tốc độ trên tập dữ liệu HumanEval (Chen et al., 2021). Ngoài ra, "Tỷ lệ Chấp nhận" phù hợp với hiểu biết trực quan của chúng ta rằng đối với các token thích ứng xa hơn, khi chúng xa hơn khỏi ngữ cảnh hiệu quả, xác suất dự đoán chính xác giảm.

Đối với mô hình 13B, như được thấy từ Hình 4, do khả năng học mạnh hơn, "Tỷ lệ Chấp nhận" của nó cao hơn so với mô hình 7B, điều này trực tiếp ảnh hưởng đến tỷ lệ cải thiện tốc độ của nó. Cụ thể, tại k=13, có sự tăng hơn 3.5X về tốc độ trên tập dữ liệu HumanEval.

3.4.2 Lấy mẫu nhân

Chúng tôi đặt các tham số thành top k=10, top p=0.95, và tiến hành thí nghiệm với hai cài đặt nhiệt độ: 0.2 và 1. Đối với cách tiếp cận tạo ra này, chúng tôi thiết kế số lượng token thích ứng (độ sâu cây) từ 5 đến 13 và so sánh nó với phương pháp lấy mẫu nhân truyền thống.

Từ dữ liệu được trình bày trong Hình 3 và 4, có thể quan sát được rằng dưới phương pháp lấy mẫu nhân, mô hình 13B thể hiện tốc độ suy luận nhanh hơn so với mô hình 7B. Ngoài ra, cài đặt nhiệt độ thấp hơn dẫn đến việc tăng tốc thêm tốc độ suy luận. Hiện tượng này có thể được quy cho hai yếu tố chính: thứ nhất, việc sử dụng tree attention yêu cầu cung cấp cho mô hình các đầu vào dài hơn trong bước soạn thảo 2; quan trọng hơn, việc sử dụng thuật toán lấy mẫu với nhiệt độ cao hơn tạo ra sự không chắc chắn lớn hơn, điều này lần lượt làm giảm tỷ lệ chấp nhận.

Một số kết luận có thể được rút ra từ phân tích tổng thể: 1) Kích thước mô hình càng lớn, kết quả học tập của các token thích ứng ngữ nghĩa càng hiệu quả, điều này được phản ánh trong cả tỷ lệ chấp nhận cao hơn và cải thiện tốc độ. 2) Khi số lượng token thích ứng tăng, lợi ích tốc độ trở nên rõ rệt hơn. 3) Nói chung, cài đặt nhiệt độ thấp hơn dẫn đến tăng tốc độ lớn hơn.

4 Kết luận

Bài báo này sử dụng giải mã suy đoán bằng cách giới thiệu các token thích ứng ngữ nghĩa, cho phép các LLM hiện có tạo ra các token bản thảo chính xác với chi phí tối thiểu và không có bất kỳ sửa đổi cấu trúc nào đối với mô hình. Cách tiếp cận này nâng cao đáng kể tốc độ tạo ra của các mô hình trong khi duy trì độ chính xác gần như không thay đổi thông qua huấn luyện được tối ưu hóa. Ngoài ra, chúng tôi đã thiết kế các lược đồ tìm kiếm tham lam và lấy mẫu nhân phổ quát có thể dễ dàng chuyển giao sang các LLM khác.

Tài liệu tham khảo

[References section with various academic papers and their citations - maintaining original formatting]

A Mất mát huấn luyện

Hình 5 trình bày các đường cong mất mát cho hai cách tiếp cận. Mất mát của cách tiếp cận huấn luyện cơ bản liên tục vượt quá của cách tiếp cận cải tiến, điều này sẽ ảnh hưởng đến benchmark của mô hình.

B Tác động của token thích ứng

Chúng tôi cũng đã tiến hành thí nghiệm để điều tra các hiệu ứng của việc huấn luyện các mô hình với các token thích ứng ngữ nghĩa đa dạng. Thí nghiệm của chúng tôi chỉ ra rằng việc sử dụng các token đa dạng không ảnh hưởng đáng kể đến độ chính xác so với việc sử dụng các token giống hệt nhau. Tuy nhiên, huấn luyện với các token đa dạng hạn chế chúng ta không thể mở rộng tùy ý độ dài của k; chúng ta chỉ có thể sử dụng các token có độ dài nhỏ hơn L. Hạn chế này có thể ràng buộc tính linh hoạt trong quá trình suy luận. Do đó, chúng tôi khuyến nghị huấn luyện các mô hình với các token giống hệt nhau.

Hình 6 và 7 minh họa kết quả thu được bằng cách sử dụng phương pháp này, với các mô hình được huấn luyện sử dụng các token thích ứng đa dạng được đại diện bởi SDSAT-D.

[Tables and figures follow with performance metrics and comparisons between different model configurations]
