# 2404.19737.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/speculative/2404.19737.pdf
# Kích thước file: 1725307 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Các Mô hình Ngôn ngữ Lớn Tốt hơn & Nhanh hơn thông qua Dự đoán Đa token
Fabian Gloeckle* 1 2Badr Youbi Idrissi* 1 3Baptiste Rozière1David Lopez-Paz+ 1Gabriel Synnaeve+ 1

Tóm tắt
Các mô hình ngôn ngữ lớn như GPT và Llama được huấn luyện với hàm mất mát dự đoán token tiếp theo. Trong nghiên cứu này, chúng tôi đề xuất rằng việc huấn luyện các mô hình ngôn ngữ để dự đoán nhiều token tương lai cùng một lúc sẽ mang lại hiệu quả mẫu cao hơn. Cụ thể hơn, tại mỗi vị trí trong kho ngữ liệu huấn luyện, chúng tôi yêu cầu mô hình dự đoán n token tiếp theo sử dụng n đầu ra độc lập, hoạt động trên một thân mô hình chung. Xem dự đoán đa token như một nhiệm vụ huấn luyện phụ trợ, chúng tôi đo được khả năng cải thiện downstream mà không có chi phí phụ trong thời gian huấn luyện cho cả mô hình code và ngôn ngữ tự nhiên. Phương pháp này càng hữu ích hơn với các kích thước mô hình lớn hơn, và vẫn giữ được sức hấp dẫn khi huấn luyện qua nhiều epoch. Các cải thiện đặc biệt rõ rệt trên các benchmark tạo sinh như lập trình, nơi các mô hình của chúng tôi luôn vượt trội các baseline mạnh bằng vài điểm phần trăm. Các mô hình 13B tham số của chúng tôi giải được nhiều hơn 12% bài toán trên HumanEval và 17% hơn trên MBPP so với các mô hình next-token tương đương. Các thí nghiệm trên các nhiệm vụ thuật toán nhỏ chứng minh rằng dự đoán đa token có lợi cho việc phát triển induction heads và khả năng lý luận thuật toán. Như một lợi ích bổ sung, các mô hình được huấn luyện với dự đoán 4-token nhanh gấp 3× trong suy luận, ngay cả với kích thước batch lớn.

1. Giới thiệu
Nhân loại đã cô đọng những nỗ lực tài tình nhất, những phát hiện bất ngờ và những sản phẩm tuyệt đẹp của mình thành văn bản. Các Mô hình Ngôn ngữ Lớn (LLM) được huấn luyện trên tất cả những kho ngữ liệu này có thể trích xuất lượng kiến thức thế giới ấn tượng, cũng như khả năng lý luận cơ bản bằng cách thực hiện một nhiệm vụ học không giám sát đơn giản nhưng mạnh mẽ: dự đoán token tiếp theo. Bất chấp làn sóng thành tựu ấn tượng gần đây (OpenAI, 2023), dự đoán token tiếp theo vẫn là một cách không hiệu quả để thu nhận ngôn ngữ, kiến thức thế giới và khả năng lý luận. Cụ thể hơn, teacher forcing với dự đoán token tiếp theo bám vào các mẫu cục bộ và bỏ qua các quyết định "khó". Do đó, vẫn là sự thật rằng các bộ dự đoán token tiếp theo hiện đại nhất cần lượng dữ liệu nhiều hơn hàng bậc so với trẻ em con người để đạt đến cùng mức độ trôi chảy (Frank, 2023).

Hình 1: Tổng quan về dự đoán đa token. (Trên) Trong quá trình huấn luyện, mô hình dự đoán 4 token tương lai cùng một lúc, thông qua một thân chung và 4 đầu ra chuyên dụng. Trong quá trình suy luận, chúng tôi chỉ sử dụng đầu ra dự đoán token tiếp theo. Tùy chọn, ba đầu còn lại có thể được sử dụng để tăng tốc thời gian suy luận. (Dưới) Dự đoán đa token cải thiện pass@1 trên nhiệm vụ code MBPP, đáng kể khi kích thước mô hình tăng. Thanh lỗi là khoảng tin cậy 90% được tính bằng bootstrapping trên các mẫu dataset.

Trong nghiên cứu này, chúng tôi lập luận rằng việc huấn luyện LLM để dự đoán nhiều token cùng một lúc sẽ thúc đẩy các mô hình này hướng tới hiệu quả mẫu tốt hơn. Như dự đoán trong Hình 1, dự đoán đa token hướng dẫn LLM dự đoán n token tương lai từ mỗi vị trí trong kho ngữ liệu huấn luyện, tất cả cùng một lúc và song song (Qi et al., 2020).

Đóng góp Trong khi dự đoán đa token đã được nghiên cứu trong văn liệu trước đây (Qi et al., 2020), công trình hiện tại đưa ra những đóng góp sau:

1. Chúng tôi đề xuất một kiến trúc dự đoán đa token đơn giản không có chi phí phụ về thời gian huấn luyện hoặc bộ nhớ (Mục 2).
2. Chúng tôi cung cấp bằng chứng thực nghiệm rằng mô hình huấn luyện này có lợi ở quy mô lớn, với các mô hình lên đến 13B tham số giải được khoảng 15% bài toán code nhiều hơn trung bình (Mục 3).
3. Dự đoán đa token cho phép self-speculative decoding, làm cho các mô hình nhanh gấp 3 lần trong thời gian suy luận trên một phạm vi rộng các kích thước batch (Mục 3.2).

Trong khi không tốn chi phí và đơn giản, dự đoán đa token là một sự thay đổi hiệu quả để huấn luyện các mô hình transformer mạnh hơn và nhanh hơn. Chúng tôi hy vọng rằng công trình của chúng tôi sẽ thúc đẩy sự quan tâm đến các hàm mất mát phụ trợ mới cho LLM vượt xa dự đoán token tiếp theo, để cải thiện hiệu suất, tính nhất quán và khả năng lý luận của những mô hình hấp dẫn này.

2. Phương pháp
Mô hình hóa ngôn ngữ tiêu chuẩn học về một kho ngữ liệu văn bản lớn x1, . . . xT bằng cách thực hiện một nhiệm vụ dự đoán token tiếp theo. Chính thức, mục tiêu học tập là tối thiểu hóa hàm mất mát cross-entropy

L1=−∑t log Pθ(xt+1|xt:1), (1)

trong đó Pθ là mô hình ngôn ngữ lớn của chúng tôi đang được huấn luyện, để tối đa hóa xác suất của xt+1 như token tương lai tiếp theo, cho trước lịch sử các token trước đây xt:1=xt, . . . , x1.

Trong công trình này, chúng tôi tổng quát hóa điều trên bằng cách thực hiện một nhiệm vụ dự đoán đa token, trong đó tại mỗi vị trí của kho ngữ liệu huấn luyện, mô hình được hướng dẫn dự đoán n token tương lai cùng một lúc. Điều này chuyển thành hàm mất mát cross-entropy

Ln=−∑t log Pθ(xt+n:t+1|xt:1). (2)

Để làm cho vấn đề dễ xử lý, chúng tôi giả định rằng mô hình ngôn ngữ lớn Pθ sử dụng một thân chung để tạo ra một biểu diễn tiềm ẩn zt:1 của ngữ cảnh quan sát được xt:1, sau đó được đưa vào n đầu độc lập để dự đoán song song từng token trong n token tương lai (xem Hình 1). Điều này dẫn đến phân tích nhân tử sau của hàm mất mát cross-entropy dự đoán đa token:

Ln=−∑t log Pθ(xt+n:t+1|zt:1)·Pθ(zt:1|xt:1)
=−∑t ∑i=1^n log Pθ(xt+i|zt:1)·Pθ(zt:1|xt:1).

Trong thực tế, kiến trúc của chúng tôi bao gồm một thân transformer chung fs tạo ra biểu diễn ẩn zt:1 từ ngữ cảnh quan sát được xt:1, n đầu ra độc lập được thực hiện dưới dạng các lớp transformer fhi, và một ma trận unembedding chung fu. Do đó, để dự đoán n token tương lai, chúng tôi tính:

Pθ(xt+i|xt:1) = softmax (fu(fhi(fs(xt:1)))),

cho i = 1, . . . n, trong đó, đặc biệt, Pθ(xt+1|xt:1) là đầu dự đoán token tiếp theo của chúng tôi. Xem Phụ lục B cho các biến thể khác của kiến trúc dự đoán đa token.

Triển khai tiết kiệm bộ nhớ Một thách thức lớn trong việc huấn luyện các bộ dự đoán đa token là giảm việc sử dụng bộ nhớ GPU của chúng. Để hiểu tại sao đây là trường hợp, hãy nhớ rằng trong các LLM hiện tại, kích thước từ vựng V lớn hơn nhiều so với chiều d của biểu diễn tiềm ẩn - do đó, các vector logit trở thành nút thắt cổ chai sử dụng bộ nhớ GPU. Các triển khai ngây thơ của các bộ dự đoán đa token cụ thể hóa tất cả logit và gradient của chúng, cả hai có hình dạng (n, V), hạn chế nghiêm trọng kích thước batch cho phép và việc sử dụng bộ nhớ GPU trung bình. Vì những lý do này, trong kiến trúc của chúng tôi, chúng tôi đề xuất điều chỉnh cẩn thận chuỗi các hoạt động forward và backward, như được minh họa trong Hình 2. Cụ thể, sau khi pass forward qua thân chung fs, chúng tôi tuần tự tính toán pass forward và backward của từng đầu ra độc lập fi, tích lũy gradient tại thân. Trong khi điều này tạo ra logit (và gradient của chúng) cho đầu ra fi, những thứ này được giải phóng trước khi tiếp tục đến đầu ra tiếp theo fi+1, chỉ yêu cầu lưu trữ dài hạn gradient thân có chiều d là ∂Ln/∂fs. Tóm lại, chúng tôi đã giảm việc sử dụng bộ nhớ GPU đỉnh từ O(nV+d) xuống O(V+d), mà không tốn kém runtime (Bảng S5).

Suy luận Trong thời gian suy luận, cách sử dụng cơ bản nhất của kiến trúc được đề xuất là dự đoán autoregressive token tiếp theo vanilla bằng cách sử dụng đầu dự đoán token tiếp theo Pθ(xt+1|xt:1), trong khi loại bỏ tất cả các đầu khác. Tuy nhiên, các đầu ra bổ sung có thể được tận dụng để tăng tốc giải mã từ đầu dự đoán token tiếp theo với các phương pháp self-speculative decoding như blockwise parallel decoding (Stern et al., 2018) - một biến thể của speculative decoding (Leviathan et al., 2023) mà không cần mô hình draft bổ sung - và speculative decoding với tree attention giống Medusa (Cai et al., 2024).

--- TRANG 2 ---
Các Mô hình Ngôn ngữ Lớn Tốt hơn & Nhanh hơn thông qua Dự đoán Đa token

Hình 2: Thứ tự forward/backward trong mô hình dự đoán n-token với n = 2 đầu. Bằng cách thực hiện forward/backward trên các đầu theo thứ tự tuần tự, chúng tôi tránh cụ thể hóa tất cả gradient lớp unembedding trong bộ nhớ đồng thời và giảm việc sử dụng bộ nhớ GPU đỉnh.

3. Thí nghiệm trên dữ liệu thực
Chúng tôi chứng minh hiệu quả của các hàm mất mát dự đoán đa token bằng bảy thí nghiệm quy mô lớn. Mục 3.1 cho thấy cách dự đoán đa token ngày càng hữu ích khi tăng kích thước mô hình. Mục 3.2 cho thấy cách các đầu dự đoán bổ sung có thể tăng tốc suy luận với hệ số 3× bằng cách sử dụng speculative decoding. Mục 3.3 chứng minh cách dự đoán đa token thúc đẩy học các mẫu dài hạn hơn, một sự thật rõ ràng nhất trong trường hợp cực đoan của tokenization cấp byte. Mục 3.4 cho thấy rằng bộ dự đoán 4-token dẫn đến những cải thiện mạnh mẽ với tokenizer có kích thước 32k. Mục 3.5 minh họa rằng lợi ích của dự đoán đa token vẫn tồn tại cho các lần chạy huấn luyện với nhiều epoch. Mục 3.6 giới thiệu các biểu diễn phong phú được thúc đẩy bởi pretraining với các hàm mất mát dự đoán đa token bằng cách finetune trên dataset CodeContests (Li et al., 2022). Mục 3.7 cho thấy rằng lợi ích của dự đoán đa token mang lại cho các mô hình ngôn ngữ tự nhiên, cải thiện các phép đánh giá tạo sinh như tóm tắt, trong khi không giảm đáng kể trên các benchmark tiêu chuẩn dựa trên câu hỏi trắc nghiệm và negative log-likelihood.

Để cho phép so sánh công bằng giữa các bộ dự đoán token tiếp theo và bộ dự đoán n-token, các thí nghiệm sau đây luôn so sánh các mô hình có số lượng tham số bằng nhau. Nghĩa là, khi chúng tôi thêm n−1 lớp trong các đầu dự đoán tương lai, chúng tôi loại bỏ n−1 lớp từ thân mô hình chung. Vui lòng tham khảo Bảng S14 cho các kiến trúc mô hình và Bảng S13 cho tổng quan về các siêu tham số chúng tôi sử dụng trong các thí nghiệm.

3.1. Lợi ích tăng theo kích thước mô hình
Để nghiên cứu hiện tượng này, chúng tôi huấn luyện các mô hình có sáu kích thước trong phạm vi 300M đến 13B tham số từ đầu trên ít nhất 91B token của code. Kết quả đánh giá trong Hình 3 cho MBPP (Austin et al., 2021) và HumanEval (Chen et al., 2021) cho thấy rằng có thể, với cùng ngân sách tính toán chính xác, vắt ra hiệu suất nhiều hơn từ các mô hình ngôn ngữ lớn cho trước một dataset cố định bằng cách sử dụng dự đoán đa token.

Chúng tôi tin rằng tính hữu ích này chỉ ở quy mô lớn là lý do có thể tại sao dự đoán đa token cho đến nay đã bị bỏ qua phần lớn như một hàm mất mát huấn luyện đầy hứa hẹn cho việc huấn luyện mô hình ngôn ngữ lớn.

3.2. Suy luận nhanh hơn
Chúng tôi triển khai greedy self-speculative decoding (Stern et al., 2018) với các kích thước batch không đồng nhất bằng cách sử dụng xFormers (Lefaudeux et al., 2022) và đo tốc độ giải mã của mô hình dự đoán 4-token tốt nhất với 7B tham số trên việc hoàn thành các prompt được lấy từ một dataset thử nghiệm gồm code và ngôn ngữ tự nhiên (Bảng S2) không được thấy trong quá trình huấn luyện. Chúng tôi quan sát tăng tốc 3.0× trên code với trung bình 2.5 token được chấp nhận trong 3 gợi ý trên code, và 2.7× trên văn bản. Trên mô hình dự đoán 8-byte, tăng tốc suy luận là 6.4× (Bảng S3). Pretraining với dự đoán đa token cho phép các đầu bổ sung chính xác hơn nhiều so với việc finetune đơn giản của mô hình dự đoán token tiếp theo, do đó cho phép các mô hình của chúng tôi khai thác toàn bộ tiềm năng của self-speculative decoding.

3.3. Học các mẫu toàn cục với dự đoán đa byte
Để cho thấy rằng nhiệm vụ dự đoán token tiếp theo bám vào các mẫu cục bộ, chúng tôi đã đi đến trường hợp cực đoan của tokenization cấp byte bằng cách huấn luyện một transformer cấp byte 7B tham số trên 314B byte, tương đương với khoảng 116B token. Mô hình dự đoán 8-byte đạt được những cải thiện đáng kinh ngạc so với dự đoán next-byte, giải được nhiều hơn 67% bài toán trên MBPP pass@1 và 20% bài toán nhiều hơn trên HumanEval pass@1.

Do đó, dự đoán đa byte là một con đường rất hứa hẹn để mở khóa việc huấn luyện hiệu quả các mô hình cấp byte. Self-speculative decoding có thể đạt tăng tốc 6 lần cho mô hình dự đoán 8-byte, điều này sẽ cho phép bù đắp hoàn toàn chi phí của các chuỗi cấp byte dài hơn tại thời gian suy luận và thậm chí nhanh hơn mô hình dự đoán token tiếp theo gần hai lần. Mô hình dự đoán 8-byte là một mô hình dựa trên byte mạnh mẽ, tiếp cận hiệu suất của các mô hình dựa trên token mặc dù đã được huấn luyện trên ít dữ liệu hơn 1.7×.

3.4. Tìm kiếm n tối ưu
Để hiểu rõ hơn về ảnh hưởng của số lượng token được dự đoán, chúng tôi đã thực hiện các ablation toàn diện trên các mô hình quy mô 7B được huấn luyện trên 200B token của code. Chúng tôi thử n = 1, 2, 4, 6 và 8 trong cài đặt này. Kết quả trong bảng 1 cho thấy rằng huấn luyện với 4 token tương lai vượt trội tất cả các mô hình khác một cách nhất quán trong suốt HumanEval và MBPP cho các chỉ số pass tại 1, 10 và 100: +3.8%, +2.1% và +3.2% cho MBPP và +1.2%, +3.7% và +4.1% cho HumanEval. Thú vị là, cho APPS/Intro, n = 6 dẫn đầu với +0.7%, +3.0% và +5.3%. Rất có thể kích thước cửa sổ tối ưu phụ thuộc vào phân phối dữ liệu đầu vào. Đối với các mô hình cấp byte, kích thước cửa sổ tối ưu nhất quán hơn (8 byte) trên các benchmark này.

3.5. Huấn luyện cho nhiều epoch
Huấn luyện đa token vẫn duy trì lợi thế so với dự đoán token tiếp theo khi được huấn luyện trên nhiều epoch của cùng một dữ liệu. Các cải thiện giảm đi nhưng chúng tôi vẫn có tăng +2.4% trên pass@1 trên MBPP và tăng +3.2% trên pass@100 trên HumanEval, trong khi có hiệu suất tương tự cho phần còn lại. Đối với APPS/Intro, kích thước cửa sổ 4 đã không tối ưu với 200B token huấn luyện.

3.6. Finetune các bộ dự đoán đa token
Các mô hình được pretrain với hàm mất mát dự đoán đa token cũng vượt trội các mô hình next-token để sử dụng trong finetuning. Chúng tôi đánh giá điều này bằng cách finetune các mô hình 7B tham số từ Mục 3.3 trên dataset CodeContests (Li et al., 2022). Chúng tôi so sánh mô hình dự đoán 4-token với baseline dự đoán token tiếp theo, và bao gồm một cài đặt trong đó mô hình dự đoán 4-token được tách bỏ các đầu dự đoán bổ sung và được finetune bằng cách sử dụng mục tiêu dự đoán token tiếp theo cổ điển.

Theo kết quả trong Hình 4, cả hai cách finetune mô hình dự đoán 4-token đều vượt trội mô hình dự đoán token tiếp theo trên pass@k với k. Điều này có nghĩa là các mô hình tốt hơn cả trong việc hiểu và giải quyết nhiệm vụ và tạo ra các câu trả lời đa dạng. Lưu ý rằng CodeContests là benchmark lập trình thách thức nhất mà chúng tôi đánh giá trong nghiên cứu này. Finetune dự đoán token tiếp theo trên pretraining dự đoán 4-token dường như là phương pháp tốt nhất tổng thể, phù hợp với mô hình cổ điển của pretraining với các nhiệm vụ phụ trợ theo sau bởi finetune cụ thể cho nhiệm vụ. Vui lòng tham khảo Phụ lục F cho chi tiết.

Hình 4: So sánh hiệu suất finetune trên CodeContests. Chúng tôi finetune một mô hình dự đoán 4-token trên CodeContests (Li et al., 2022) (split train) sử dụng dự đoán n′-token như hàm mất mát huấn luyện với n′ = 4 hoặc n′ = 1, và so sánh với finetune của mô hình baseline dự đoán token tiếp theo (n = n′ = 1). Để đánh giá, chúng tôi tạo 1000 mẫu trên mỗi bài toán test cho mỗi nhiệt độ T trong {0.5, 0.6, 0.7, 0.8, 0.9}, và tính pass@k cho mỗi giá trị của k và T. Được hiển thị là k → max T pass_at(k, T), tức là chúng tôi cấp quyền truy cập vào một oracle nhiệt độ. Chúng tôi quan sát rằng cả hai cách finetune mô hình dự đoán 4-token đều vượt trội baseline dự đoán token tiếp theo. Thú vị là, sử dụng finetune dự đoán token tiếp theo trên mô hình dự đoán 4-token dường như là phương pháp tốt nhất tổng thể.

3.7. Dự đoán đa token trên ngôn ngữ tự nhiên
Để đánh giá huấn luyện dự đoán đa token trên ngôn ngữ tự nhiên, chúng tôi huấn luyện các mô hình kích thước 7B tham số trên 200B token của ngôn ngữ tự nhiên với hàm mất mát dự đoán 4-token, 2-token và token tiếp theo, tương ứng. Trong Hình 5, chúng tôi đánh giá các checkpoint kết quả trên 6 benchmark NLP tiêu chuẩn. Trên các benchmark này, mô hình dự đoán 2 token tương lai có hiệu suất ngang với baseline dự đoán token tiếp theo trong suốt quá trình huấn luyện. Mô hình dự đoán 4 token tương lai gặp sự suy giảm hiệu suất. Số liệu chi tiết được báo cáo trong Phụ lục G.

Tuy nhiên, chúng tôi không tin rằng các benchmark trắc nghiệm và dựa trên likelihood phù hợp để phân biệt hiệu quả khả năng tạo sinh của các mô hình ngôn ngữ. Để tránh nhu cầu chú thích con người về chất lượng tạo sinh hoặc các judge mô hình ngôn ngữ - điều này đi kèm với những cạm bẫy riêng, như được chỉ ra bởi Koo et al. (2023) - chúng tôi tiến hành đánh giá trên các benchmark tóm tắt và toán học ngôn ngữ tự nhiên và so sánh các mô hình được pretrain với kích thước tập huấn luyện 200B và 500B token và với các hàm mất mát dự đoán token tiếp theo và đa token, tương ứng.

Đối với tóm tắt, chúng tôi sử dụng tám benchmark trong đó các chỉ số ROUGE (Lin, 2004) đối với một bản tóm tắt ground-truth cho phép đánh giá tự động các văn bản được tạo. Chúng tôi finetune mỗi mô hình được pretrain trên dataset huấn luyện của từng benchmark trong ba epoch và chọn checkpoint với điểm số ROUGE-L F1 cao nhất trên dataset validation. Hình 6 cho thấy rằng các mô hình dự đoán đa token với cả n = 2 và n = 4 cải thiện so với baseline next-token trong điểm số ROUGE-L F1 cho cả hai kích thước dataset huấn luyện, với khoảng cách hiệu suất thu hẹp với kích thước dataset lớn hơn. Tất cả các chỉ số có thể được tìm thấy trong Phụ lục H.

Đối với toán học ngôn ngữ tự nhiên, chúng tôi đánh giá các mô hình được pretrain trong chế độ 8-shot trên benchmark GSM8K (Cobbe et al., 2021) và đo độ chính xác của câu trả lời cuối cùng được tạo ra sau một chuỗi suy nghĩ được gợi ra bởi các ví dụ few-shot. Chúng tôi đánh giá các chỉ số pass@k để định lượng tính đa dạng và tính đúng đắn của câu trả lời như trong các đánh giá code và sử dụng nhiệt độ sampling giữa 0.2 và 1.4. Kết quả được mô tả trong Hình S13 trong Phụ lục I. Đối với 200B token huấn luyện, mô hình n = 2 rõ ràng vượt trội baseline dự đoán token tiếp theo, trong khi mô hình đảo ngược sau 500B token và n = 4 tệ hơn trong suốt.

Hình 5: Huấn luyện đa token với các mô hình 7B không cải thiện hiệu suất trên các nhiệm vụ lựa chọn. Hình này cho thấy sự tiến hóa của độ chính xác trung bình của 6 benchmark NLP tiêu chuẩn. Kết quả chi tiết trong Phụ lục G cho các mô hình 7B được huấn luyện trên 200B token của dữ liệu ngôn ngữ. Mô hình 2 token tương lai có cùng hiệu suất với baseline và mô hình 4 token tương lai thoái hóa một chút. Các kích thước mô hình lớn hơn có thể cần thiết để thấy cải thiện trên các nhiệm vụ này.

Hình 6: Hiệu suất trên tóm tắt văn bản trừu tượng. Điểm số ROUGE-L (longest common subsequence overlap) F1 trung bình cho các mô hình 7B được huấn luyện trên 200B và 500B token của ngôn ngữ tự nhiên trên tám benchmark tóm tắt. Chúng tôi finetune các mô hình tương ứng trên dữ liệu huấn luyện của từng nhiệm vụ riêng biệt trong ba epoch và chọn các checkpoint với điểm số ROUGE-L F1 validation cao nhất. Cả hai mô hình dự đoán đa token n = 2 và n = 4 đều có lợi thế so với các mô hình dự đoán token tiếp theo. Điểm số cá nhân trên mỗi dataset và chi tiết thêm có thể được tìm thấy trong Phụ lục H.

4. Ablation trên dữ liệu tổng hợp
Điều gì thúc đẩy các cải thiện trong hiệu suất downstream của các mô hình dự đoán đa token trên tất cả các nhiệm vụ mà chúng tôi đã xem xét? Bằng cách tiến hành các thí nghiệm đồ chơi trên các dataset huấn luyện được kiểm soát và các nhiệm vụ đánh giá, chúng tôi chứng minh rằng dự đoán đa token dẫn đến những thay đổi định tính trong khả năng mô hình và hành vi tổng quát hóa. Cụ thể, Mục 4.1 cho thấy rằng đối với các kích thước mô hình nhỏ, khả năng induction - như được thảo luận bởi Olsson et al. (2022) - hoặc chỉ hình thành khi sử dụng dự đoán đa token như hàm mất mát huấn luyện, hoặc nó được cải thiện rất nhiều bởi nó. Hơn nữa, Mục 4.2 cho thấy rằng dự đoán đa token cải thiện tổng quát hóa trên một nhiệm vụ số học, thậm chí nhiều hơn so với việc tăng gấp ba kích thước mô hình.

4.1. Khả năng induction
Induction mô tả một mẫu lý luận đơn giản hoàn thành các mẫu một phần bằng sự tiếp tục gần đây nhất của chúng (Olsson et al., 2022). Nói cách khác, nếu một câu chứa "AB" và sau đó đề cập đến "A", induction là dự đoán rằng sự tiếp tục là "B". Chúng tôi thiết kế một thiết lập để đo khả năng induction một cách có kiểm soát. Huấn luyện các mô hình nhỏ có kích thước 1M đến 1B tham số nonembedding trên một dataset truyện trẻ em, chúng tôi đo khả năng induction bằng một tập test được điều chỉnh: trong 100 truyện từ split test gốc, chúng tôi thay thế tên nhân vật bằng các tên được tạo ngẫu nhiên bao gồm hai token với tokenizer mà chúng tôi sử dụng. Dự đoán token đầu tiên của các tên hai token này được liên kết với ngữ nghĩa của văn bản trước đó, trong khi dự đoán token thứ hai của mỗi lần xuất hiện tên sau khi nó đã được đề cập ít nhất một lần có thể được xem như một nhiệm vụ induction thuần túy. Trong các thí nghiệm của chúng tôi, chúng tôi huấn luyện tối đa 90 epoch và thực hiện early stopping đối với chỉ số test (tức là chúng tôi cho phép một oracle epoch). Hình 7 báo cáo khả năng induction được đo bằng độ chính xác trên các token thứ hai của tên liên quan đến kích thước mô hình cho hai lần chạy với các seed khác nhau.

Chúng tôi thấy rằng hàm mất mát dự đoán 2-token dẫn đến sự hình thành khả năng induction được cải thiện rất nhiều cho các mô hình có kích thước 30M tham số nonembedding trở xuống, với lợi thế của chúng biến mất cho các kích thước 100M tham số nonembedding trở lên. Chúng tôi giải thích phát hiện này như sau: các hàm mất mát dự đoán đa token giúp các mô hình học chuyển thông tin qua các vị trí chuỗi, điều này hỗ trợ việc hình thành induction head và các cơ chế học trong ngữ cảnh khác. Tuy nhiên, một khi khả năng induction đã được hình thành, các tính năng đã học này chuyển đổi induction thành một nhiệm vụ có thể được giải quyết cục bộ tại token hiện tại và được học chỉ với dự đoán token tiếp theo. Từ điểm này trở đi, dự đoán đa token thực sự gây tổn hại trên benchmark hạn chế này - nhưng chúng tôi suy đoán rằng có những hình thức lý luận trong ngữ cảnh cao hơn mà nó tiếp tục đóng góp, như được chứng minh bởi kết quả trong Mục 3.1. Trong Hình S14, chúng tôi cung cấp bằng chứng cho lời giải thích này: bằng cách thay thế dataset truyện trẻ em bằng hỗn hợp chất lượng cao hơn 9:1 của dataset sách với truyện trẻ em, chúng tôi thúc ép việc hình thành khả năng induction sớm trong huấn luyện bằng chính dataset. Do đó, ngoại trừ hai kích thước mô hình nhỏ nhất, lợi thế của dự đoán đa token trên nhiệm vụ biến mất: việc học tính năng của các tính năng induction đã chuyển đổi nhiệm vụ thành một nhiệm vụ dự đoán token tiếp theo thuần túy.

Hình 7: Khả năng induction của các mô hình dự đoán n-token. Được hiển thị là độ chính xác trên token thứ hai của các tên hai token đã được đề cập trước đó. Được hiển thị là các số cho các mô hình được huấn luyện với hàm mất mát dự đoán token tiếp theo và dự đoán 2-token, tương ứng, với hai lần chạy độc lập mỗi loại. Các đường biểu thị trung bình mỗi hàm mất mát. Đối với các kích thước mô hình nhỏ, các mô hình dự đoán token tiếp theo học được thực tế không có hoặc khả năng induction tệ hơn đáng kể so với các mô hình dự đoán 2-token, với nhược điểm của chúng biến mất ở kích thước 100M tham số nonembedding.

4.2. Lý luận thuật toán
Các nhiệm vụ lý luận thuật toán cho phép đo các hình thức lý luận trong ngữ cảnh phức tạp hơn so với chỉ induction. Chúng tôi huấn luyện và đánh giá các mô hình trên một nhiệm vụ về số học đa thức trong vành F7[X]/(X5) với phủ định unary, cộng, nhân và hợp thành đa thức như các phép toán. Các hệ số của toán hạng và các toán tử được lấy mẫu đều. Nhiệm vụ là trả về các hệ số của đa thức tương ứng với các biểu thức kết quả. Số m phép toán chứa trong các biểu thức được chọn đều từ phạm vi từ 1 đến 5 tại thời gian huấn luyện, và có thể được sử dụng để điều chỉnh độ khó của cả đánh giá tổng quát hóa trong domain (m ≤ 5) và ngoài domain (m > 5). Các đánh giá được tiến hành với greedy sampling trên một tập test cố định gồm 2000 mẫu trên mỗi số phép toán.

Chúng tôi huấn luyện các mô hình có hai kích thước nhỏ với 30M và 100M tham số nonembedding, tương ứng. Điều này mô phỏng các điều kiện của các mô hình ngôn ngữ lớn được huấn luyện trên các kho ngữ liệu văn bản khổng lồ mà tương tự bị thiếu tham số và không thể ghi nhớ toàn bộ dataset huấn luyện của chúng.

Dự đoán đa token cải thiện khả năng lý luận thuật toán được đo bởi nhiệm vụ này trên các độ khó nhiệm vụ (Hình 8). Cụ thể, nó dẫn đến những cải thiện ấn tượng trong tổng quát hóa ngoài phân phối, mặc dù các số tuyệt đối thấp. Tăng kích thước mô hình từ 30M lên 100M tham số, mặt khác, không cải thiện độ chính xác đánh giá nhiều như việc thay thế dự đoán token tiếp theo bằng dự đoán đa token (Hình S16). Trong Phụ lục K, chúng tôi hơn nữa cho thấy rằng các mô hình dự đoán đa token duy trì lợi thế của chúng so với các mô hình dự đoán token tiếp theo trên nhiệm vụ này khi được huấn luyện và đánh giá với pause token (Goyal et al., 2023).

Hình 8: Độ chính xác trên một nhiệm vụ số học đa thức với số lượng phép toán thay đổi trên mỗi biểu thức. Huấn luyện với các hàm mất mát dự đoán đa token tăng độ chính xác trên các độ khó nhiệm vụ. Cụ thể, nó cũng cải thiện đáng kể hiệu suất tổng quát hóa ngoài domain, mặc dù ở mức tuyệt đối thấp. Tăng gấp ba kích thước mô hình, mặt khác, có ảnh hưởng nhỏ hơn đáng kể so với việc thay thế dự đoán token tiếp theo bằng hàm mất mát dự đoán đa token (Hình S16). Được hiển thị là hai lần chạy độc lập trên mỗi cấu hình với các mô hình 100M tham số.

5. Tại sao nó hoạt động? Một số suy đoán
Tại sao dự đoán đa token mang lại hiệu suất vượt trội trên các benchmark đánh giá lập trình, và trên các nhiệm vụ lý luận thuật toán nhỏ? Trực giác của chúng tôi, được phát triển trong mục này, là dự đoán đa token giảm thiểu sự khác biệt phân phối giữa teacher forcing thời gian huấn luyện và tạo sinh autoregressive thời gian suy luận. Chúng tôi hỗ trợ quan điểm này với một lập luận minh họa về các trọng số ngầm mà dự đoán đa token gán cho các token tùy thuộc vào mức độ liên quan của chúng đối với sự tiếp tục của văn bản, cũng như với một phân tích thông tin lý thuyết của hàm mất mát dự đoán đa token.

5.1. Lookahead tăng cường các điểm lựa chọn
Không phải tất cả các quyết định token đều quan trọng như nhau để tạo ra các văn bản hữu ích từ các mô hình ngôn ngữ (Bachmann và Nagarajan, 2024; Lin et al., 2024). Trong khi một số token cho phép các biến thể phong cách không hạn chế phần còn lại của văn bản, những token khác đại diện cho các điểm lựa chọn được liên kết với các thuộc tính ngữ nghĩa cấp cao hơn của văn bản và có thể quyết định liệu một câu trả lời có được coi là hữu ích hay đi lệch hướng.

Dự đoán đa token ngầm gán trọng số cho các token huấn luyện tùy thuộc vào mức độ chúng tương quan chặt chẽ với các token kế tiếp của chúng. Như một ví dụ minh họa, hãy xem xét chuỗi được mô tả trong Hình 9 trong đó một chuyển tiếp là một điểm lựa chọn khó dự đoán trong khi các chuyển tiếp khác được coi là "không quan trọng". Các chuyển tiếp không quan trọng theo sau một điểm lựa chọn cũng khó dự đoán trước. Bằng cách đánh dấu và đếm các số hạng mất mát, chúng tôi thấy rằng dự đoán n-token liên kết một trọng số n(n+1)/2 với các điểm lựa chọn thông qua các tương quan của chúng, và một trọng số nhỏ hơn là n với các điểm không quan trọng. Vui lòng tham khảo Phụ lục L.3 cho chi tiết thêm. Nói chung, chúng tôi tin rằng chất lượng của các tạo sinh văn bản phụ thuộc vào việc chọn đúng quyết định tại các điểm lựa chọn, và các hàm mất mát dự đoán n-token thúc đẩy những điều đó.

5.2. Lập luận thông tin lý thuyết
Các mô hình ngôn ngữ thường được huấn luyện bằng teacher-forcing, trong đó mô hình nhận được ground truth cho mỗi token tương lai trong quá trình huấn luyện. Tuy nhiên, trong thời gian tạo sinh test, việc tạo sinh là không được hướng dẫn và autoregressive, do đó lỗi tích lũy. Teacher-forcing, chúng tôi lập luận, khuyến khích các mô hình tập trung vào dự đoán tốt trong ngắn hạn, với chi phí tiềm năng là bỏ qua các phụ thuộc dài hạn hơn trong cấu trúc tổng thể của chuỗi được tạo.

Để minh họa tác động của dự đoán đa token, hãy xem xét lập luận thông tin lý thuyết sau. Ở đây, X biểu thị token tương lai tiếp theo, và Y là token tương lai thứ hai tiếp theo. Việc tạo ra cả hai token này được điều kiện hóa trên một số ngữ cảnh quan sát được, đầu vào C, mà chúng tôi bỏ qua khỏi các phương trình của chúng tôi để đơn giản. Khi được đặt trước token X, dự đoán token tiếp theo vanilla liên quan đến lượng H(X), trong khi dự đoán đa token với n = 2 nhằm vào H(X) + H(Y). Chúng tôi phân tích hai lượng này như:

H(X) = H(X|Y) + I(X;Y),
H(X) + H(Y) = H(X|Y) + 2I(X;Y) + H(Y|X).

Bằng cách loại bỏ số hạng H(Y|X) - xuất hiện lại khi dự đoán ở vị trí tiếp theo - chúng tôi quan sát rằng dự đoán 2-token tăng tầm quan trọng của I(X;Y) với hệ số 2. Vậy, các bộ dự đoán đa token chính xác hơn trong việc dự đoán các token X có liên quan đến phần còn lại của văn bản sắp tới. Trong Phụ lục L.2, chúng tôi đưa ra một phiên bản tương đối của các phương trình trên cho thấy trọng số tăng của thông tin tương hỗ tương đối trong một phân tích hàm mất mát dự đoán 2-token.

6. Công trình liên quan
Các hàm mất mát mô hình hóa ngôn ngữ Dong et al. (2019) và Tay et al. (2022) huấn luyện trên một hỗn hợp các nhiệm vụ denoising với các mặt nạ attention khác nhau (full, causal và prefix attention) để thu hẹp khoảng cách hiệu suất với pretraining token tiếp theo trên các nhiệm vụ tạo sinh. Tay et al. (2022) sử dụng mục tiêu span corruption, thay thế các khoảng token bằng các token đặc biệt cho encoder và decoder sau đó dự đoán nội dung của những khoảng đó. Không giống như UniLM, điều này cho phép huấn luyện causal đầy đủ với teacher forcing. Tương tự, Yang et al. (2019) huấn luyện trên các chuỗi được hoán vị, trong khi bảo tồn các embedding vị trí gốc, hiệu quả huấn luyện mô hình để dự đoán các phần khác nhau của chuỗi cho trước một hỗn hợp thông tin quá khứ và tương lai. Mô hình hóa ngôn ngữ hoán vị này gần nhất với chúng tôi vì nó cho phép dự đoán vượt ra ngoài token tiếp theo. Tuy nhiên tất cả các nhiệm vụ mô hình hóa ngôn ngữ này huấn luyện chỉ trên một tỷ lệ nhỏ của văn bản đầu vào: trung bình chỉ 15% token được backward through. Đối với Dong et al. (2019), nơi việc masking được thực hiện theo kiểu BERT, khó có thể mask nhiều hơn 15% vì nó phá hủy quá nhiều thông tin. Đối với Tay et al. (2022), về mặt kỹ thuật có thể có tỷ lệ lớn hơn nhưng trong thực tế, các cài đặt được sử dụng có từ 15% đến 25% token bị mask. (Yang et al., 2019) cũng làm cho việc huấn luyện trên toàn bộ chuỗi trở nên khả thi vì nó chỉ được hoán vị, và không có thông tin nào bị mất. Tuy nhiên, trong thực tế, vì việc hoán vị hoàn toàn ngẫu nhiên rất khó tái tạo, chỉ 15% được dự đoán vì lý do ổn định huấn luyện.

Dự đoán đa token trong mô hình hóa ngôn ngữ Qi et al. (2020) lập luận rằng dự đoán đa token khuyến khích lập kế hoạch, cải thiện biểu diễn và ngăn chặn overfitting trên các mẫu cục bộ có thể dẫn đến từ huấn luyện teacher-forced. Tuy nhiên, phương pháp kỹ thuật của họ nhân bản residual stream n lần trong khi của chúng tôi cho phép so sánh matched-compute và làm cho các biểu diễn residual tham gia trực tiếp hơn vào các số hạng hàm mất mát phụ trợ. Stern et al. (2018) và Cai et al. (2024) đề xuất finetune mô hình với dự đoán đa token để suy luận nhanh hơn nhưng không nghiên cứu ảnh hưởng của hàm mất mát như vậy trong quá trình pretraining. Pal et al. (2023) sử dụng các phương pháp probing để cho thấy rằng các mô hình dự đoán token tiếp theo có thể dự đoán các token liên tiếp bổ sung ở một mức độ nhất định, nhưng ít hơn so với các mô hình của chúng tôi được huấn luyện cụ thể cho nhiệm vụ này. Jianyu Zhang (2024) quan sát các cải thiện trong các nhiệm vụ mô hình hóa ngôn ngữ với phân loại nhị phân đa nhãn về sự xuất hiện của các từ vựng trong tương lai như một nhiệm vụ học phụ trợ.

Self-speculative decoding Stern et al. (2018) là, theo hiểu biết tốt nhất của chúng tôi, những người đầu tiên đề xuất một scheme speculative decoding để suy luận nhanh hơn. Kiến trúc của chúng tôi thay thế các đầu dự đoán tuyến tính của họ bằng các lớp transformer, nhưng ngoài ra tương tự. Bằng cách tổ chức lại thứ tự forward/backward, chúng tôi có thể sử dụng tất cả các số hạng mất mát thay vì chọn ngẫu nhiên một đầu để tính toán mất mát. Cai et al. (2024) trình bày một scheme self-speculative decoding phức tạp hơn sử dụng các dự đoán top-k của mỗi đầu thay vì chỉ dự đoán tốt nhất. Nó có thể được sử dụng với các mô hình dự đoán đa token mà chúng tôi huấn luyện.

Dự đoán đa mục tiêu Học đa nhiệm vụ là mô hình của việc huấn luyện mạng neural cùng lúc trên nhiều nhiệm vụ để cải thiện hiệu suất trên các nhiệm vụ quan tâm (Caruana, 1997). Học với các nhiệm vụ phụ trợ như vậy cho phép các mô hình khai thác các phụ thuộc giữa các biến mục tiêu và thậm chí có thể được ưa thích trong trường hợp các mục tiêu độc lập (Waegeman et al., 2019). Trong khi các kiến trúc được thiết kế cụ thể hơn cho dự đoán đa mục tiêu có thể hình dung được (Spyromitros-Xioufis et al., 2016; Read et al., 2021), các phương pháp deep learning hiện đại thường dựa vào các thân mô hình chung lớn với các đầu dự đoán riêng biệt cho các nhiệm vụ tương ứng (Caruana, 1997; Silver et al., 2016; Lample et al., 2022) như chúng tôi làm. Dự đoán đa mục tiêu đã được chứng minh là một chiến lược thành công trong nhiều lĩnh vực khác nhau, ví dụ để học dự đoán chuỗi thời gian với các bước thời gian xa hơn trong tương lai như các mục tiêu phụ trợ (Vapnik và Vashist, 2009) hoặc để học từ video với một số khung hình tương lai (Mathieu et al., 2016; Srivastava et al., 2016) hoặc biểu diễn của các khung hình tương lai (Vondrick et al., 2016) như các mục tiêu phụ trợ.

7. Kết luận
Chúng tôi đã đề xuất dự đoán đa token như một cải thiện so với dự đoán token tiếp theo trong việc huấn luyện các mô hình ngôn ngữ cho các nhiệm vụ tạo sinh hoặc lý luận. Các thí nghiệm của chúng tôi (lên đến 7B tham số và 1T token) cho thấy rằng điều này ngày càng hữu ích cho các mô hình lớn hơn và đặc biệt cho thấy những cải thiện mạnh mẽ cho các nhiệm vụ code. Chúng tôi cho rằng phương pháp của chúng tôi giảm sự không khớp phân phối giữa huấn luyện teacher-forced và tạo sinh autoregressive. Khi được sử dụng với speculative decoding, suy luận chính xác trở nên nhanh gấp 3 lần.

Trong công trình tương lai, chúng tôi muốn hiểu rõ hơn cách tự động chọn n trong các hàm mất mát dự đoán đa token. Một khả năng để làm điều đó là sử dụng loss scale và loss balancing (Défossez et al., 2022). Ngoài ra, các kích thước từ vựng tối ưu cho dự đoán đa token có thể khác với những kích thước cho dự đoán token tiếp theo, và việc điều chỉnh chúng có thể dẫn đến kết quả tốt hơn, cũng như cải thiện sự đánh đổi giữa độ dài chuỗi nén và chi phí compute-per-byte. Cuối cùng, chúng tôi muốn phát triển các hàm mất mát dự đoán phụ trợ được cải thiện hoạt động trong không gian embedding (LeCun, 2022).

Tuyên bố tác động
Mục tiêu của bài báo này là làm cho các mô hình ngôn ngữ hiệu quả hơn về mặt tính toán và dữ liệu. Trong khi điều này về nguyên tắc có thể giảm tác động sinh thái của việc huấn luyện LLM, chúng ta cần cẩn thận về các hiệu ứng phản hồi. Tất cả các lợi thế xã hội, cũng như rủi ro, của LLM nên được xem xét khi sử dụng công trình này.

Tác động môi trường
Tổng cộng, việc huấn luyện tất cả các mô hình được báo cáo trong bài báo yêu cầu khoảng 500K giờ GPU tính toán trên phần cứng loại A100-80GB và H100. Tổng lượng khí thải ước tính là khoảng 50 tCO2eq, 100% trong số đó được bù đắp bởi chương trình bền vững của Meta.

Lời cảm ơn
Chúng tôi cảm ơn Jianyu Zhang, Léon Bottou, Emmanuel Dupoux, Pierre-Emmanuel Mazaré, Yann LeCun, Quentin Garrido, Megi Dervishi, Mathurin Videau và Timothée Darcet và các sinh viên PhD FAIR khác và các thành viên nhóm CodeGen cho những thảo luận hữu ích. Chúng tôi cảm ơn Jonas Gehring cho chuyên môn kỹ thuật của anh ấy và nhóm Llama gốc và nhóm xFormers cho phép loại nghiên cứu này.

--- TRANG 10 ---
Các Mô hình Ngôn ngữ Lớn Tốt hơn & Nhanh hơn thông qua Dự đoán Đa token

Tài liệu tham khảo
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.

Gregor Bachmann and Vaishnavh Nagarajan. The pitfalls of next-token prediction, 2024.

Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks, 2015.

Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language, 2019.

Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads, 2024.

Rich Caruana. Multitask learning. Machine learning, 28: 41-75, 1997.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

Nakhun Chumpolsathien. Using knowledge distillation from keyword extraction to improve the informativeness of neural cross-lingual summarization. Master's thesis, Beijing Institute of Technology, 2020.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Unified language model pre-training for natural language understanding and generation. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pages 13063-13075, 2019.

Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression. arXiv preprint arXiv:2210.13438, 2022.

Moussa Kamal Eddine, Antoine J. P. Tixier, and Michalis Vazirgiannis. Barthez: a skilled pretrained french sequence-to-sequence model, 2021.

Alexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir R. Radev. Multi-news: a large-scale multi-document summarization dataset and abstractive hierarchical model, 2019.

Mehrdad Farahani. Summarization using bert2bert model on wikisummary dataset. https://github.com/m3hrdadfi/wikisummary, 2020.

Mehrdad Farahani, Mohammad Gharachorloo, and Mohammad Manthouri. Leveraging parsbert and pretrained mt5 for persian abstractive text summarization. In 2021 26th International Computer Conference, Computer Society of Iran (CSICC). IEEE, March 2021. doi: 10.1109/csicc52343.2021.9420563. URL http://dx.doi.org/10.1109/CSICC52343.2021.9420563.

Michael C Frank. Bridging the data gap between children and large language models. Trends in Cognitive Sciences, 2023.

Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: A human-annotated dialogue dataset for abstractive summarization. In Proceedings of the 2nd Workshop on New Frontiers in Summarization. Association for Computational Linguistics, 2019. doi: 10.18653/v1/d19-5409. URL http://dx.doi.org/10.18653/v1/D19-5409.

Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens, 2023.

Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021.

Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration, 2020.

Jianyu Zhang Leon Bottou. Multi-label classification as an auxiliary loss for language modelling. personal communication, 2024.

Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettelmoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension, 2017.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015.

Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, and Dongyeop Kang. Benchmarking cognitive biases in large language models as evaluators. arXiv preprint arXiv:2309.17012, 2023.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019.

Guillaume Lample, Marie-Anne Lachaux, Thibaut Lavril, Xavier Martinet, Amaury Hayat, Gabriel Ebner, Aurélien Rodriguez, and Timothée Lacroix. Hypertree proof search for neural theorem proving, 2022.

Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1), 2022.

Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza. xformers: A modular and hackable transformer modelling library. https://github.com/facebookresearch/xformers, 2022.

Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding, 2023.

Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092-1097, 2022.

Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013.

Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, and Weizhu Chen. Rho-1: Not all tokens are what you need, 2024.

Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts, 2017.

Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.

Michael Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond mean square error, 2016.

Ramesh Nallapati, Bowen Zhou, Cicero Nogueira dos santos, Caglar Gulcehre, and Bing Xiang. Abstractive text summarization using sequence-to-sequence rnns and beyond, 2016.

Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization, 2018.

Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.

OpenAI. Gpt-4 technical report, 2023.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022.

Koyena Pal, Jiuding Sun, Andrew Yuan, Byron C. Wallace, and David Bau. Future lens: Anticipating subsequent tokens from a single hidden state, 2023.

Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang, and Ming Zhou. Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training, 2020.

Jesse Read, Bernhard Pfahringer, Geoffrey Holmes, and Eibe Frank. Classifier chains: A review and perspectives. Journal of Artificial Intelligence Research, 70:683-718, 2021.

Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011.

Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions, 2019.

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587): 484-489, 2016.

Aaditya K Singh, Stephanie CY Chan, Ted Moskovitz, Erin Grant, Andrew M Saxe, and Felix Hill. The transient nature of emergent in-context learning in transformers. arXiv preprint arXiv:2311.08360, 2023.

Eleftherios Spyromitros-Xioufis, Grigorios Tsoumakas, William Groves, and Ioannis Vlahavas. Multi-target regression via input space expansion: treating targets as inputs. Machine Learning, 104:55-98, 2016.

Nitish Srivastava, Elman Mansimov, and Ruslan Salakhutdinov. Unsupervised learning of video representations using lstms, 2016.

Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autoregressive models, 2018.

Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Siamak Shakeri, Dara Bahri, Tal Schuster, et al. Ul2: Unifying language learning paradigms. arXiv preprint arXiv:2205.05131, 2022.

Vladimir Vapnik and Akshay Vashist. A new learning paradigm: Learning using privileged information. Neural networks, 22(5-6):544-557, 2009.

Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Anticipating visual representations from unlabeled video, 2016.

Willem Waegeman, Krzysztof Dembczyński, and Eyke Hüllermeier. Multi-target prediction: a unifying view on problems and methods. Data Mining and Knowledge Discovery, 33:293-324, 2019.

Vikas Yadav, Steven Bethard, and Mihai Surdeanu. Quick and (not so) dirty: Unsupervised selection of justification sentences for multi-hop question answering. arXiv preprint arXiv:1911.07176, 2019.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in neural information processing systems, pages 5753-5763, 2019.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence?, 2019.

[Tiếp tục với phần dịch từ trang 13 trở đi...]

A. Kết quả bổ sung về self-speculative decoding

[Tất cả nội dung còn lại của tài liệu được dịch tiếp theo cấu trúc và định dạng tương tự...]
