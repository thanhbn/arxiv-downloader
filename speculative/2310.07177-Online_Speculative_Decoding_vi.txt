# 2310.07177.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/speculative/2310.07177.pdf
# Kích thước tệp: 2380181 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Giải Mã Suy Đoán Trực Tuyến
Xiaoxuan Liu1Lanxiang Hu2Peter Bailis3Alvin Cheung1Zhijie Deng4Ion Stoica1Hao Zhang2
Tóm tắt
Giải mã suy đoán là một kỹ thuật quan trọng để tăng tốc suy luận của các mô hình ngôn ngữ lớn (LLM) bằng cách sử dụng một mô hình nháp nhỏ hơn để dự đoán đầu ra của mô hình đích. Tuy nhiên, hiệu quả của nó có thể bị hạn chế do độ chính xác dự đoán thấp của mô hình nháp, đặc biệt khi đối mặt với các đầu vào văn bản đa dạng và khoảng cách khả năng lớn giữa mô hình nháp và mô hình đích. Chúng tôi giới thiệu giải mã suy đoán trực tuyến để giải quyết thử thách này. Ý tưởng chính là liên tục cập nhật (nhiều) mô hình nháp dựa trên dữ liệu truy vấn người dùng quan sát được. Việc thích ứng với phân phối truy vấn giúp giảm thiểu sự chênh lệch giữa phân phối huấn luyện của mô hình nháp và phân phối truy vấn, cho phép mô hình nháp dự đoán chính xác hơn đầu ra của mô hình đích. Chúng tôi phát triển một nguyên mẫu của giải mã suy đoán trực tuyến dựa trên chưng cất kiến thức và đánh giá nó bằng cả dữ liệu truy vấn tổng hợp và thực tế. Kết quả cho thấy sự gia tăng đáng kể trong tỷ lệ chấp nhận token từ 0,1 đến 0,65, mang lại giảm độ trễ từ 1,42× đến 2,17×. Mã nguồn của chúng tôi có sẵn tại https://github.com/LiuXiaoxuanPKU/OSD.

1. Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) như GPT-4 (OpenAI, 2023) và LLaMA (Touvron et al., 2023a;b) đang nhanh chóng cách mạng hóa các ứng dụng ngày nay. Nhiều công ty đang chạy đua triển khai LLM trong các dịch vụ trực tuyến, như tìm kiếm, chatbot và trợ lý ảo. Vì hầu hết các dịch vụ này yêu cầu độ trễ thấp, việc tối ưu hóa độ trễ phục vụ LLM trực tiếp dẫn đến chất lượng dịch vụ tốt hơn và giảm chi phí.

Độ trễ của dịch vụ LLM ngày nay thật không may là rất cao, chủ yếu vì việc phục vụ một truy vấn người dùng đòi hỏi nhiều đánh giá nối tiếp của LLM, mỗi lần chỉ tạo ra một token của phản hồi. Một giải pháp mới nổi để giảm độ trễ là giải mã suy đoán (Leviathan et al., 2023)—nó sử dụng một mô hình nháp nhỏ để suy đoán nhiều token đầu ra của mô hình (lớn) đích, và sau đó để LLM đích xác minh các suy đoán này song song. Nếu việc xác minh một token thất bại, mô hình lớn phải tính toán lại từ điểm đó. Do đó, hiệu suất của giải mã suy đoán phụ thuộc chủ yếu vào độ chính xác suy đoán của mô hình nháp (còn được gọi là tỷ lệ chấp nhận token). Khi có mặt các đầu vào văn bản đa dạng, độ chính xác của các suy đoán thường không cao lắm, do khoảng cách khả năng giữa mô hình nháp và mô hình đích. Tuy nhiên, việc sử dụng một mô hình nháp lớn hơn, chính xác hơn lại đi ngược lại mục đích của giải mã suy đoán vì nó sẽ tăng độ trễ.

Để giải quyết thử thách này, chúng tôi giới thiệu một phương pháp mới, giải mã suy đoán trực tuyến (OSD), để tinh chỉnh định kỳ mô hình nháp dựa trên các chỉnh sửa của mô hình đích. OSD nhằm mục đích giảm độ trễ truy vấn trong khi bảo toàn kích thước nhỏ gọn của mô hình nháp.

Đầu tiên, OSD sử dụng chưng cất kiến thức trong giải mã suy đoán để tăng cường sự liên kết giữa mô hình nháp và mô hình đích. Giải mã suy đoán bao gồm việc mô hình nháp đề xuất các token tiềm năng với các phân phối xác suất tương ứng. Mô hình đích sau đó đánh giá các đề xuất này, chỉnh sửa sự khác biệt để đảm bảo rằng các đầu ra vẫn nhất quán với những gì được tạo ra mà không có mô hình nháp. Cơ chế chỉnh sửa này đóng vai trò là một cách hiệu quả cho mô hình nháp để tiếp thu và học hỏi từ thông tin phong phú này. So với tinh chỉnh nhãn thông thường, chưng cất kiến thức mang lại lợi thế đáng kể

--- TRANG 2 ---
Giải Mã Suy Đoán Trực Tuyến

bằng cách cung cấp phân phối xác suất cho mỗi token. Bằng cách tận dụng những hiểu biết từ mô hình giáo viên (Gu et al., 2023), phương pháp này hiệu quả trong việc điều chỉnh mô hình nháp và mô hình đích.

Hơn nữa, thay vì dựa vào một mô hình nháp tĩnh, chúng tôi định kỳ cập nhật mô hình nháp. Điều này là do các truy vấn người dùng đến một dịch vụ LLM cụ thể thường thể hiện các phân phối cụ thể theo lĩnh vực (Zheng et al., 2023a), phản ánh các mẫu sử dụng chung. Trong khi việc suy đoán chính xác đầu ra của mô hình lớn hơn trên bất kỳ đầu vào đa dạng nào là thách thức, việc tăng cường độ chính xác dự đoán của mô hình nháp chỉ cho các đầu vào tương tự được đăng lên dịch vụ, được đặc trưng bởi phân phối truy vấn là khả thi. Các cập nhật có thể được thực hiện thông qua nhiều phương pháp. Một cách tiếp cận là tinh chỉnh mô hình nháp trong nền và sau đó áp dụng các cập nhật này theo thời gian thực sau một khoảng thời gian xác định trước. Một cách khác, có thể tận dụng khả năng tính toán dư thừa của hệ thống phục vụ trong khi nó đang chạy, như được chi tiết trong §4.2.2. Quan trọng là, việc điều chỉnh thời gian thực của các mô hình nháp cho phép chúng liên tục thích ứng dựa trên dữ liệu truy vấn đến. Cách tiếp cận động này rất cần thiết để duy trì tỷ lệ chấp nhận token cao, đảm bảo mô hình vẫn hiệu quả và cập nhật với dữ liệu và xu hướng phát triển.

Cuối cùng, để cải thiện thêm tỷ lệ chấp nhận token, OSD không chỉ thu hẹp phân phối truy vấn mà còn định tuyến mỗi truy vấn đến mô hình nháp phù hợp nhất cho phân phối cụ thể đó. Điều này được thực hiện bằng cách phát triển các mô hình nháp được tinh chỉnh để phục vụ các lĩnh vực riêng biệt. Việc tập trung vào phân phối truy vấn hẹp hơn đã được chứng minh là hiệu quả hơn cho việc học. Do đó, OSD hiệu quả định hướng các truy vấn đến mô hình nháp tương ứng chuyên về lĩnh vực của chúng. Như được chứng minh trong §5.2 của đánh giá của chúng tôi, chúng tôi đã khéo léo huấn luyện nhiều mô hình nháp, mỗi mô hình được thiết kế riêng cho các ngôn ngữ hoặc chủ đề khác nhau. Phương pháp này nổi bật tiềm năng đáng kể để cải thiện hiệu quả và độ chính xác khi xử lý một loạt các truy vấn đa dạng.

Tóm lại, bài báo này đóng góp những điều sau:
• Chúng tôi khám phá các phương pháp chưng cất kiến thức tổng quát (GKD) khác nhau để xây dựng mô hình nháp và xác định các biến thể hiệu quả nhất (Phần 4.1).
• Chúng tôi giới thiệu giải mã suy đoán trực tuyến để giảm độ trễ phục vụ LLM bằng cách thích ứng mô hình nháp một cách linh hoạt (§4.2).
• Chúng tôi nghiên cứu tùy chỉnh mô hình nháp cho giải mã suy đoán, trong đó mỗi truy vấn được định hướng đến mô hình nháp tương ứng với lĩnh vực của truy vấn (§4.3).
• OSD thể hiện sự cải thiện đáng kể trong tỷ lệ chấp nhận token, lên đến 10-65% trên các bộ dữ liệu đa dạng, dẫn đến giảm độ trễ 1,4-2,1×. OSD có thể được kết hợp với các phương pháp hiện có xây dựng mô hình nháp tĩnh và đạt được độ chính xác như thể tất cả dữ liệu truy vấn đều có sẵn từ trước (§5).

2. Công trình liên quan
Giải mã suy đoán. Giải mã suy đoán (Leviathan et al., 2023; Chen et al., 2023a) tăng tốc giải mã LLM bằng cách sử dụng một mô hình nháp (nhỏ) để dự đoán đầu ra của mô hình đích lớn hơn, sau đó mô hình đích xác minh. Giả sử mô hình nháp có thể dự đoán chính xác hơn một token cho mỗi bước xác minh, I/O bộ nhớ để truy cập trọng số và KV cache của mô hình đích (lớn) tại thời điểm suy luận được phân bổ trên nhiều token đầu ra, do đó giảm độ trễ, đặc biệt vì suy luận LLM thường bị ràng buộc bởi băng thông HBM GPU. Hiệu quả của giải mã suy đoán phụ thuộc vào khả năng của mô hình nháp trong việc dự đoán chính xác đầu ra của mô hình đích. Công trình hiện tại cải thiện độ chính xác suy đoán bằng cách sử dụng các mô hình nháp theo giai đoạn (Spector & Re, 2023), kiểu RAG (He et al., 2023), đa mô hình (Miao et al., 2023; Chen et al., 2023b) và lấy mẫu nhiều ứng viên từ mô hình nháp (Yang et al., 2024; Cai et al., 2024). Ngoài ra, tồn tại một hướng nghiên cứu loại bỏ nhu cầu về mô hình nháp riêng biệt bằng cách tận dụng các mô-đun phụ trợ trong chính mô hình đích (Cai et al., 2023; Stern et al., 2018; Cai et al., 2024; Lin et al., 2024; Zhang et al., 2023). Các phương pháp này chủ yếu giả định một mô hình nháp tĩnh sau khi triển khai. Ngược lại, công trình của chúng tôi giới thiệu một khung làm việc tích cực thích ứng mô hình nháp với phân phối truy vấn người dùng đang phát triển một cách linh hoạt, bất kể cấu trúc của mô hình nháp. OSD trực giao với các phương pháp nêu trên, cho phép tích hợp với chúng để cải thiện hiệu quả tổng thể trong các tình huống triển khai trực tuyến.

Chưng cất cho các mô hình tự hồi quy. Chưng cất kiến thức (KD) là một khung làm việc để tạo ra các mô hình nhỏ hơn mô phỏng hiệu suất của các mô hình lớn hơn. Tuy nhiên, KD ở dạng thông thường đã được quan sát là ít hiệu quả hơn đối với LLM. Gu et al. (2023) mở rộng KD cho LLM tự hồi quy bằng cách giải mã từ mô hình học sinh và tối ưu hóa phân kỳ KL ngược giữa học sinh và giáo viên. Agarwal et al. (2023) giới thiệu chưng cất kiến thức tổng quát (GKD) để tối ưu hóa tổ hợp tuyến tính của KL thuận và KL ngược giữa giáo viên và học sinh, sử dụng hỗn hợp dữ liệu được lấy mẫu từ giáo viên và học sinh. Lấy cảm hứng từ cả hai công trình, OSD áp dụng KD cho giải mã suy đoán cho LLM và mở rộng nó để điều chỉnh động các mô hình nháp (Phần 4.1). Chúng tôi thừa nhận sự xuất hiện đồng thời của một công trình liên quan, DistillSpec (Zhou et al., 2023), cũng sử dụng KD cho giải mã suy đoán. Tuy nhiên, công trình của chúng tôi và DistillSpec được phát triển đồng thời. Hơn nữa, DistillSpec đại diện cho một khía cạnh cụ thể của khung làm việc rộng lớn hơn OSD của chúng tôi. OSD không chỉ khám phá KD cho giải mã suy đoán mà còn giải quyết các thách thức trong môi trường trực tuyến và định tuyến truy vấn qua các phân phối khác nhau.

--- TRANG 3 ---
Giải Mã Suy Đoán Trực Tuyến

3. Kiến thức nền
Chúng tôi trước tiên xem xét ngắn gọn giải mã suy đoán (Leviathan et al., 2023), một kỹ thuật quan trọng tăng tốc suy luận của một LLM đích lớn p(·|x) với các đề xuất token từ một mô hình nháp nhỏ qθ(·|x). x biểu thị nối liên các prompt đầu vào và các token đã được tạo ra. Cả hai phân phối đều được học theo cách tự hồi quy. Chúng tôi nhấn mạnh các tham số θ của mô hình nháp vì chúng ta thường cần điều chỉnh chúng theo LLM đích để tăng tốc đáng kể hơn.

Giải mã suy đoán sử dụng một mô hình nháp (nhỏ) để đề xuất k token y ≜ {yi}k i=1 ∼ qθ(·|x), và để LLM đích ước tính k + 1 xác suất token {p(y|x,y<i)}k+1 i=1 song song. Chúng tôi chi tiết quá trình lấy mẫu trong Phụ lục A.1. Công trình trước đây đã chỉ ra rằng các mẫu kết quả ỹ ≜ {y1, . . . , ya+1} tuân thủ nghiêm ngặt phân phối của LLM đích p(·|x) (Leviathan et al., 2023). Nó nối ỹ với x và lặp lại quá trình trên cho đến khi gặp 〈EOS〉.

Tỷ lệ chấp nhận và tăng tốc kỳ vọng. Tỷ lệ chấp nhận, ký hiệu là α, đóng vai trò là thước đo mức độ gần gũi của mô hình nháp so với mô hình đích. Nó được định nghĩa là xác suất kỳ vọng rằng giải mã suy đoán sẽ chấp nhận một token đề xuất cho prompt yi ∼ qθ(yi|x,y<i). Tỷ lệ này trực tiếp ảnh hưởng đến độ dài kỳ vọng (E(|ỹ|)) của ỹ cho mỗi lần chạy LLM đích và tăng tốc của giải mã suy đoán như được chi tiết trong Hình 8.

Giả sử rằng k + 1 đánh giá đồng thời của LLM đích p mất khoảng thời gian bằng nhau như tạo ra một token song song, gọi c là tỷ số thời gian cho một lần chạy đơn giữa qθ và p. Độ dài tạo ra kỳ vọng của một lần chạy LLM đích đơn và tăng tốc trong tổng thời gian thực do giải mã suy đoán được biểu diễn như (Leviathan et al., 2023):

E(|ỹ|) = (1−αk+1)/(1−α), E(tăng tốc) = (1−αk+1)/((1−α)(kc+1)). (1)

Quan sát. Quá trình giải mã suy đoán vốn xác định những không chính xác của LLM nháp và cung cấp các giải pháp đúng cho những không chính xác này. Do đó, chúng ta nhận được những hiểu biết có giá trị về các lĩnh vực và chiến lược để tinh chỉnh mô hình nháp mà không cần chi phí bổ sung. Hơn nữa, do kích thước giảm của mô hình nháp (ví dụ, nhỏ hơn 20× so với mô hình đích), việc điều chỉnh nó không chỉ hiệu quả mà còn khả thi cho các điều chỉnh trực tuyến thời gian thực. Công trình trước đây (Leviathan et al., 2023; Miao et al., 2023) chủ yếu tiếp cận giải mã suy đoán theo cách ngoại tuyến, nghĩa là mô hình nháp vẫn tĩnh trong khi triển khai trực tuyến. Chúng tôi tiếp theo phát triển giải mã suy đoán trực tuyến để thu hẹp khoảng cách này.

4. Giải Mã Suy Đoán Trực Tuyến
Chúng tôi đề xuất phương pháp giải mã suy đoán trực tuyến để cập nhật mô hình nháp một cách động. Chúng tôi đóng khung bài toán học như chưng cất kiến thức trực tuyến dựa trên thông tin phụ trợ nêu trên, trong đó mô hình giáo viên và học sinh tương ứng với LLM đích và nháp trong giải mã suy đoán.

4.1. Chưng Cất Kiến Thức cho Giải Mã Suy Đoán
Chưng cất kiến thức là một khung làm việc tổng quát để điều chỉnh phân phối dự đoán của một mô hình nhỏ (tức là mô hình học sinh) với phân phối của một mô hình lớn hơn (tức là mô hình giáo viên). Nghiên cứu trước đây đã sử dụng chưng cất kiến thức để nén mạng nơ-ron, dẫn đến giảm chi phí suy luận và yêu cầu bộ nhớ. Chúng tôi cho rằng chưng cất kiến thức rất hiệu quả cho giải mã suy đoán. Trong cách tiếp cận này, mô hình nháp đóng vai trò học sinh và mô hình đích đóng vai trò giáo viên. Trong quá trình giải mã suy đoán, chúng ta có thông tin đầy đủ về cả xác suất được đề xuất và được xác minh của mỗi token. Thông tin này giúp xây dựng mục tiêu để chưng cất mô hình nháp, điều chỉnh phân phối đầu ra của nó với phân phối của mô hình đích và do đó cải thiện tỷ lệ chấp nhận token của mô hình nháp.

Mất mát chưng cất thường có dạng:
ℓ(θ) = 1/(nB) ∑x(i)∈B ℓ(x(i),θ), ℓ(x,θ) = D(p(·|x)∥qθ(·|x)), (2)

trong đó B = {x(i)}nB i=1 biểu thị một batch đầu vào và D biểu thị một số thước đo khoảng cách.

Thước đo khoảng cách. Trong trường hợp các mô hình tự hồi quy, phân phối dự đoán là phân loại tại mỗi token. Thường, chúng ta có thể tăng cường các logit được dự đoán với nhiệt độ có thể điều chỉnh τ cho biến đổi softmax. Sau đó chúng ta sử dụng KL thuận và KL ngược (RKL) phổ biến, cũng như hỗn hợp của chúng (tức là phân kỳ JSD) để khởi tạo D (Agarwal et al., 2023; Gu et al., 2023):

ℓKL(x,θ) = DKL(p(·|x)∥qθ(·|x)),
ℓRKL(x,θ) = DKL(qθ(·|x)∥p(·|x)),
ℓJSD[β](x,θ) = βDKL(p(·|x)∥pβ θ(·|x)) + (1−β)DKL(qθ(·|x)∥pβ θ(·|x)), (3)

trong đó pβ θ(·|x) ≜ βp(·|x) + (1−β)qθ(·|x). Các mục tiêu này khác biệt với các mục tiêu tinh chỉnh dựa trên nhãn thường được sử dụng trong giải mã suy đoán, như được nổi bật trong (Miao et al., 2023; Leviathan et al., 2023). Như được hiển thị trong Phần 5.1, các mục tiêu dựa trên phân kỳ KL được chứng minh là hiệu quả hơn. Điều này là do phân phối truyền tải thông tin phong phú hơn so với chỉ các nhãn, do đó tăng cường khả năng của chúng để hướng dẫn mô hình học sinh (Hinton et al.,

--- TRANG 4 ---
Giải Mã Suy Đoán Trực Tuyến

2015). Ngoài ra, các mục tiêu này tăng cường tốc độ hội tụ (He et al., 2022) và củng cố hiệu chuẩn. Trong nghiên cứu của chúng tôi, phù hợp với nghiên cứu trước đây (Agarwal et al., 2023), chúng tôi xác định thực nghiệm rằng thước đo khoảng cách tối ưu có thể thay đổi tùy thuộc vào nhiệm vụ và khả năng tương đối của mô hình giáo viên và học sinh (xem §5.1).

Lấy mẫu và ước tính gradient. Việc ước tính các mục tiêu trên bao gồm kỳ vọng qua qθ(·|x) hoặc p(·|x), điều này nên được mở rộng đệ quy. Khi độ sâu đệ quy vượt quá 1, chúng ta không thể tính toán DKL một cách phân tích mà phải dựa vào xấp xỉ Monte Carlo. Khi lấy mẫu từ qθ(·|x), chúng ta nên vi phân qua quá trình lấy mẫu để ước tính gradient không thiên vị. Tuy nhiên, điều này dẫn đến các ước tính theo kiểu gradient chính sách và nên dựa vào các chính sách phức tạp như reward hacking và điều chuẩn một bước để giảm phương sai gradient và ổn định huấn luyện (Gu et al., 2023).

Ngược lại, một cách tiếp cận đơn giản hơn là bỏ qua việc vi phân qua quá trình lấy mẫu (Agarwal et al., 2023), trong đó mẫu y được cắm trực tiếp vào mục tiêu:

ℓ(x,θ) ≈ ∑|y|+1 j=1 D(p(y|x,y<j)∥qθ(y|x,y<j)). (4)

Bằng cách này, các thước đo khoảng cách khác nhau có thể được áp dụng dễ dàng. Ngoài ra, việc lấy mẫu trở nên tách rời khỏi thước đo khoảng cách. tức là, chúng ta lấy mẫu y từ một hỗn hợp tùy ý của p(·|x) và qθ(·|x) nhưng sử dụng KL, RKL hoặc JSD để ước tính sự không liên kết phân phối.

Trực quan, các mẫu từ mô hình giáo viên thường nhất quán, điều này có thể gây khó khăn trong việc khớp mô hình học sinh nhỏ, trong khi các mẫu từ mô hình học sinh có thể kém cấu trúc hơn hoặc thậm chí vô nghĩa. Một chiến lược khắc phục là cân bằng giữa chúng thông qua lấy mẫu hỗn hợp (Gu et al., 2023), tức là yj ∼ βp(·|x,y<j) + (1−β)qθ(·|x,y<j).

4.2. Thích Ứng Trực Tuyến
Phần này mở rộng việc áp dụng chưng cất kiến thức cho giải mã suy đoán trong môi trường trực tuyến. Cách tiếp cận cho phép cải thiện hiệu suất của mô hình nháp bằng cách sử dụng kết quả từ giải mã suy đoán, do đó thích ứng động với phân phối truy vấn và cải thiện tỷ lệ chấp nhận token. Chúng tôi cũng thảo luận về sự cân bằng của phương pháp khi tích hợp các hệ thống phục vụ LLM.

4.2.1. THUẬT TOÁN
Chúng tôi mô tả thuật toán giải mã suy đoán trực tuyến (OSD) của chúng tôi trong Thuật toán 1. OSD bắt đầu bằng việc huấn luyện mô hình nháp sử dụng bộ dữ liệu khởi động (Dòng 2). Hệ thống phục vụ sau đó liên tục xử lý các yêu cầu đến (như được mô tả trong Dòng 6 đến 23). Đối với mỗi yêu cầu, nó sử dụng giải mã suy đoán tiêu chuẩn (Dòng 10-11) để tạo phản hồi cho đến token 〈EOS〉. Đồng thời, OSD theo dõi chỉ số token (chỉ số lỗi) và logit đích nơi mô hình nháp đề xuất các token sai (Dòng 15). Tận dụng thông tin được theo dõi, OSD cập nhật mô hình nháp mỗi I lần lặp, với I là một tham số có thể điều chỉnh động. OSD cập nhật mô hình nháp với các hàm mất mát khác nhau (Dòng 20) như được mô tả trong Phần 4.1. Việc lựa chọn hàm mất mát phụ thuộc vào cặp mô hình (nháp, đích) cụ thể và dữ liệu đầu vào tương ứng.

Thuật toán 1 Giải Mã Suy Đoán Trực Tuyến.
1: Đầu vào: LLM đích p(·|x), LLM nháp qθ(·|x), bộ dữ liệu khởi động D, luồng dữ liệu trực tuyến S, số lượng dự đoán k, bộ đệm tạm thời R, bộ đệm phát lại Q, khoảng cập nhật cho mô hình nháp I.
2: Huấn luyện trước qθ để xấp xỉ p với dữ liệu từ D bằng cách tối thiểu hóa ℓ(x,θ) sử dụng Phương trình (4);
3: i ← 0;
4: Q ← [];
5: curlen = |x| // Tổng độ dài chuỗi, bao gồm độ dài prompt và token đã tạo ra
6: while True do
7: R ← [] // Danh sách các cặp (chỉ số lỗi, logit đích tại chỉ số lỗi) cho một yêu cầu đơn
8: x ∼ S, i ← i + 1;
9: while 〈EOS〉 not in x do
10: y = {y1, ..., yk} ∼ qθ(·|x);
11: Ước tính {p(y|x,y<i)}k+1 i=1 song song;
12: Xác định số token được chấp nhận a và lấy mẫu thêm một token, tạo ra y = {y1, . . . , ya+1};
13: curlen ← curlen + a + 1;
14: chỉ số lỗi ← curlen;
15: Thêm (chỉ số lỗi, p(y|x,y<a+1)) vào R;
16: x ← [x, y<a+2];
17: end while
18: Thêm (x, R) vào Q;
19: if i mod I = 0 then
20: Cập nhật qθ trên Q để tối thiểu hóa ℓ(x,θ) một cách phân tích;
21: Q ← [];
22: end if
23: end while

Thảo luận. OSD sử dụng bộ đệm phát lại, Q, để nắm bắt tất cả thông tin phù hợp để cập nhật mô hình nháp. Các chính sách đuổi khác nhau có thể được sử dụng để duy trì kích thước nhỏ gọn cho Q. Ví dụ, có thể chọn chỉ giữ lại các cặp thông tin nhất hoặc các mục gần đây nhất. Tương tự, người dùng có tùy chọn giữ lại dữ liệu trong Q ngay cả sau khi sử dụng nó để cập nhật mô hình nhiều lần. Việc xác định chiến lược đuổi/giữ lại tối ưu là chủ đề cho khám phá tương lai. Trong nghiên cứu hiện tại, chúng tôi tránh đuổi bất kỳ cặp nào và giải phóng Q sau mỗi lần cập nhật mô hình. Hơn nữa, I là một tham số động. Tùy thuộc vào tải hệ thống và tốc độ mà phân phối truy vấn thay đổi, người dùng có thể điều chỉnh I tương ứng. Ví dụ, chúng ta có thể thực hiện cập nhật gradient một cách cơ hội chỉ khi lưu lượng dịch vụ không ở đỉnh (tức là có flops dự phòng).

--- TRANG 5 ---
Giải Mã Suy Đoán Trực Tuyến

Trong việc triển khai hệ thống, hai pipeline riêng biệt có thể được thiết lập: một cho huấn luyện và một cho suy luận. Cách tiếp cận này cho phép sử dụng cơ sở hạ tầng hiện có. Việc cập nhật định kỳ trọng số mô hình nháp là cần thiết để đảm bảo thích ứng liên tục. Ngoài ra, một pipeline thống nhất có thể được phát triển để phục vụ cả huấn luyện và suy luận. Hệ thống tích hợp này liên tục tăng cường mô hình nháp, duy trì tỷ lệ chấp nhận token cao một cách nhất quán. Nhìn chung, OSD liên tục cải thiện sự xấp xỉ của mô hình nháp (được chỉ ra bởi tỷ lệ chấp nhận token α tăng) bằng cách học từ mô hình đích trong giai đoạn phục vụ. Chúng tôi tiếp theo chứng minh cách tỷ lệ chấp nhận được tăng cường trực tiếp góp phần vào việc giảm độ trễ yêu cầu.

4.2.2. PHÂN TÍCH ĐỘ TRỄ & FLOPS
Độ trễ. Như được chi tiết trong Phụ lục A.3, so với giải mã suy đoán tiêu chuẩn, tăng tốc kỳ vọng cho giải mã suy đoán trực tuyến là (1+α2+α2²+...+αk²)/(1+α1+α1²+...+αk₁). Dựa trên dữ liệu từ thí nghiệm của chúng tôi (tham khảo Bảng 1), khi so sánh với giải mã suy đoán tiêu chuẩn, chúng tôi mong đợi cải thiện tăng tốc cho Vicuna-7B (LLaMA-160M làm mô hình nháp) theo các hệ số 2,42×, 1,43×, 1,64× và 1,22×. Tương tự, đối với Flan-T5-XL 3B (T5-small 80M làm mô hình nháp), các cải thiện tăng tốc là 3,06×, 1,76×, 2,72× và 1,55× trên bốn bộ dữ liệu được đánh giá.

FLOPs. (1) FLOPs cần thiết để cập nhật mô hình nháp ít hơn nhiều so với FLOPs cần thiết cho suy luận trên mô hình lớn. Như được chi tiết trong Phụ lục A.4, đối với hai cặp được đánh giá, tỷ lệ FLOPs giữa mô hình đích và mô hình nháp là 18,75 cho cặp (LLaMA-160M, Vicuna7B), và 12,6 cho cặp (T5-small 80M, Flan-T5-XL 3B). (2) Trong các hệ thống thực tế, FLOPs cần thiết cho suy luận thấp hơn đáng kể so với khả năng của máy. Phụ lục A.4 cung cấp phân tích các trace chatbot Arena nơi việc sử dụng tính toán của cụm dưới 1 phần trăm. Với hai quan sát trên, rõ ràng là FLOPs dành cho suy luận và cập nhật mô hình nháp tương đối không đáng kể so với FLOPs tiêu thụ cho suy luận mô hình đích và tổng FLOPs của cụm.

Băng thông bộ nhớ. Như được chi tiết trong Bảng 5 và Phụ lục A.5, việc cập nhật mô hình nháp không tốn nhiều bộ nhớ vì nó nhỏ. Phần lớn các hoạt động bộ nhớ vẫn bị chi phối bởi việc tải mô hình đích. OSD có thể giảm đáng kể băng thông bộ nhớ thông qua tỷ lệ chấp nhận token cao hơn, do đó giảm tần suất gọi mô hình lớn hơn để xác minh.

4.3. Tùy Chỉnh và Định Tuyến Mô Hình Nháp
Các truy vấn có thể được phân loại theo nhiều đặc điểm khác nhau. Hình 2 minh họa hai cách phân loại truy vấn từ bộ dữ liệu Arena (được chi tiết trong Phụ lục A.7). Chúng tôi tận dụng quan sát này để thu hẹp thêm lĩnh vực truy vấn để cải thiện thích ứng mô hình nháp. Như được nổi bật trong Phần 5.3, chúng ta có thể cải thiện độ chính xác của mô hình nháp nếu chúng ta ràng buộc nó vào một lĩnh vực cụ thể. Điều này phản ánh quan sát rằng việc sử dụng một mô hình nháp duy nhất cho tất cả truy vấn dẫn đến tỷ lệ chấp nhận token thấp hơn so với việc điều chỉnh mô hình nháp cho các lĩnh vực truy vấn cụ thể. Hơn nữa, chúng tôi quan sát sự thay đổi theo thời gian trong phân phối, được chi tiết trong Phụ lục A.7, tỷ lệ của các chủ đề và ngôn ngữ khác nhau cũng thay đổi qua các timestamp khác nhau.

[Hình 2: Biểu đồ bên trái mô tả tỷ lệ truy vấn bằng các ngôn ngữ khác tiếng Anh trong bộ dữ liệu Arena. Biểu đồ bên phải minh họa phân phối truy vấn qua các chủ đề khác nhau trong bộ dữ liệu.]

Trong nghiên cứu của chúng tôi, chúng tôi triển khai hai cách tiếp cận cho việc phân loại truy vấn. Cách tiếp cận đầu tiên bao gồm việc sắp xếp truy vấn dựa trên ngôn ngữ, sử dụng các thẻ ngôn ngữ có sẵn trong metadata truy vấn. Chiến lược thứ hai nhóm các truy vấn theo chủ đề của chúng, sử dụng một mô hình BERT nhỏ (67M) (Luqmani, 2023). Để giảm thiểu chi phí định tuyến, chúng tôi thực hiện định tuyến ở cấp độ truy vấn thay vì ở cấp độ token. Quá trình định tuyến rất hiệu quả vì nó chỉ dựa vào việc sử dụng thẻ truy vấn (để truy cập metadata) hoặc triển khai mô hình phân loại đơn giản. Bài báo này nổi bật việc tăng cường tỷ lệ chấp nhận token đạt được thông qua việc tùy chỉnh mô hình nháp. Chúng tôi để lại cho công việc tương lai việc xác định định tuyến truy vấn tối ưu và số lượng hoặc kích thước của các mô hình nháp tùy chỉnh này.

5. Thí nghiệm
Để đánh giá hiệu quả của OSD, chúng tôi đánh giá khả năng cải thiện tỷ lệ chấp nhận token (α) trong bối cảnh ngoại tuyến. Sau đó, chúng tôi kiểm tra tác động của phương pháp trong môi trường trực tuyến, phát hiện rằng tỷ lệ chấp nhận cải thiện ngay cả với lượng dữ liệu vừa phải trong khi duy trì mức độ chính xác tương đương với tình huống ngoại tuyến. Cuối cùng, chúng tôi nghiên cứu độ trễ truy vấn và thực hiện phân tích định lượng chuyên sâu để hiểu rõ về các token được học bởi mô hình nháp.

Trong suốt các thí nghiệm của chúng tôi, chúng tôi sử dụng hai mô hình đích (Mp): Vicuna-7B (Chiang et al., 2023) và FLAN-T5-XL (3B) (Chung et al., 2022). Cụ thể đối với Vicuna-7B, chúng tôi sử dụng LLaMA-160M (Miao et al., 2023) làm mô hình nháp (Mq). Đối với FLAN-T5-XL, chúng tôi sử dụng T5-Small (Raffel et al., 2020) làm mô hình nháp. Chúng tôi đánh giá hiệu suất trên

--- TRANG 6 ---
Giải Mã Suy Đoán Trực Tuyến

Bảng 1. Tỷ lệ chấp nhận token (α) sau hai epoch. FT: Tinh chỉnh trên nhãn do giáo viên tạo ra. TF, SF, MixF: Lấy mẫu token từ giáo viên, học sinh và hỗn hợp tương ứng, tất cả với KL thuận. Đối với nhiệm vụ, SP: Spider. GS: Gsm8k. CP: Code-serarch-Python. AL: Alpaca-finance.

[THIS IS TABLE: Shows token acceptance rates across different models and tasks with Original, FT, TF, SF, MixF columns for Vicuna-7B and FLAN T5-XL models]

bốn bộ dữ liệu đa dạng: Text-to-SQL (Spider) (Yu et al., 2018), toán học trường đại học (Gsm8k) (Cobbe et al., 2021), tạo mã Python (Code-search-Python) (Husain et al., 2019), và trả lời câu hỏi tài chính (Alpaca-finance) (Bharti, 2023). Trong tất cả thí nghiệm, chúng tôi đặt số token đề xuất là 5 cho giải mã suy đoán. Đối với tất cả thí nghiệm trực tuyến, chúng tôi cố định khoảng cập nhật I ở 8.

5.1. Đánh Giá Ngoại Tuyến
Trong phần này, chúng tôi đánh giá hiệu quả của việc sử dụng chưng cất kiến thức để huấn luyện một mô hình nhỏ cụ thể cho suy đoán trong môi trường ngoại tuyến. Trong môi trường như vậy, mô hình suy đoán Mq có quyền truy cập không hạn chế vào bộ dữ liệu, và phân phối truy vấn vẫn ổn định. Để mô phỏng các điều kiện ngoại tuyến này, chúng tôi chưng cất Mq sử dụng bộ dữ liệu huấn luyện trong hai epoch và sau đó đánh giá hiệu suất của nó bằng cách đo tỷ lệ chấp nhận token trung bình (α) trên tập kiểm tra. Như được chi tiết trong Phần 4.1, chúng tôi đánh giá các phương pháp lấy mẫu khác nhau, cụ thể là lấy mẫu giáo viên, lấy mẫu học sinh và lấy mẫu hỗn hợp ở cấp độ token. Bảng 1 hiển thị tỷ lệ chấp nhận token của mô hình nháp cho mỗi phương pháp, sử dụng KL thuận làm thước đo khoảng cách trên bộ dữ liệu kiểm tra. Để so sánh, chúng tôi cũng cung cấp tỷ lệ chấp nhận cho tinh chỉnh nhãn do giáo viên tạo ra và mô hình gốc.

Đối với cả mô hình Vicuna-7B và FLAN-T5-XL, phương pháp lấy mẫu giáo viên vượt trội hơn các phương pháp khác bằng cách đạt được tỷ lệ chấp nhận cao nhất. Hơn nữa, chưng cất kiến thức đã chứng minh hiệu quả trong việc tăng cường sự xấp xỉ của mô hình nháp, dẫn đến tỷ lệ chấp nhận token cao. Cuối cùng, chúng tôi thí nghiệm với các thước đo khoảng cách khác nhau như KL ngược và JSD. Tuy nhiên, các thước đo này hoặc tương đương hoặc kém hiệu quả khi so sánh với KL thuận. Thước đo khoảng cách hoặc phương pháp lấy mẫu tối ưu thay đổi tùy thuộc vào nhiệm vụ và mô hình, và chúng tôi để lại cho công việc tương lai việc tìm ra sự kết hợp tốt nhất.

5.2. Đánh Giá Trực Tuyến
Học trực tuyến. Đầu tiên, chúng tôi đánh giá hiệu quả của OSD bằng cách giải quyết hai câu hỏi: (1) Thuật toán trực tuyến có tăng tỷ lệ chấp nhận token không? Và sự tăng cường này có tương đương với tỷ lệ đạt được trong môi trường ngoại tuyến, vốn đóng vai trò là giới hạn trên do có quyền truy cập đầy đủ vào dữ liệu không? (2) Thuật toán trực tuyến tăng tỷ lệ chấp nhận token nhanh như thế nào, do đó cho thấy rằng mô hình nhỏ gọn đã nắm bắt được phân phối cơ bản?

Trong cách tiếp cận của chúng tôi, chúng tôi mô phỏng quá trình phục vụ trực tuyến bằng cách lặp qua các bộ dữ liệu, trích xuất prompt và streaming các yêu cầu tạo ra. Hệ thống sử dụng giải mã suy đoán cho mỗi yêu cầu này. Trong suốt giai đoạn phục vụ này, chúng tôi liên tục tinh chỉnh các mô hình suy đoán, như được chi tiết trong Thuật toán 1. Đối với baseline của chúng tôi, chúng tôi hình dung một tình huống nơi hệ thống phục vụ có khả năng thu thập dữ liệu ngoại tuyến để chưng cất một mô hình nháp ban đầu. Mô hình này sau đó được triển khai trực tuyến để phục vụ các yêu cầu tương lai. Quá trình này được mô phỏng bằng cách sử dụng 10% của bộ dữ liệu để chưng cất mô hình nháp, mô hình này vẫn tĩnh trong quá trình phục vụ trực tuyến. Đối với các thước đo đánh giá, chúng tôi tính toán tỷ lệ chấp nhận token trung bình trên 50 yêu cầu gần đây nhất. Điều này chứng minh hiệu quả của Mq trên dữ liệu gần đây nhất.

Như được mô tả trong Hình 2, cả đối với Vicuna-7B và FLAN-T5, ban đầu, OSD mang lại tỷ lệ chấp nhận token thấp hơn so với mô hình được chưng cất ngoại tuyến. Tuy nhiên, những tỷ lệ chấp nhận này tăng nhanh chóng khi mô hình nháp được tiếp xúc với nhiều dữ liệu hơn. Chúng tôi cũng ghi chú tỷ lệ chấp nhận token từ môi trường ngoại tuyến để nổi bật hiệu suất đỉnh tiềm năng mà hệ thống phục vụ trực tuyến có thể đạt được. Trong tất cả các trường hợp, bối cảnh trực tuyến có thể đạt được kết quả tương đương. Trong một số tình huống, OSD thậm chí vượt qua tỷ lệ chấp nhận token của các alpha kiểm tra ngoại tuyến. Sự khác biệt này có thể được quy cho thực tế rằng các alpha kiểm tra ngoại tuyến được đánh giá trên toàn bộ bộ dữ liệu kiểm tra, trong khi các alpha trực tuyến đại diện cho trung bình di động của 50 yêu cầu gần đây nhất. Có thể OSD hoạt động tối ưu trên các tập con dữ liệu cụ thể, đặc biệt nếu những tập con đó có phân phối hẹp hơn so với bộ dữ liệu hoàn chỉnh.

Thay đổi phân phối. Chúng tôi đánh giá khả năng thích ứng của OSD với các thay đổi phân phối. Chúng tôi sử dụng một LLaMA-160M duy nhất làm mô hình nháp ban đầu và Vicuna-7B làm mô hình đích. Để mô phỏng thay đổi phân phối, chúng tôi tích hợp dữ liệu từ các bộ dữ liệu đa dạng. Cụ thể, chúng tôi chọn 2k prompt từ mỗi bộ dữ liệu. Dữ liệu từ bốn bộ dữ liệu được kết hợp bằng cách nối trực tiếp, sao cho các bản ghi từ i×2k đến (i+1)×2k chỉ thuộc về bộ dữ liệu i.

Như được minh họa trong Hình 4, giá trị alpha của OSD giảm đáng kể tại các ranh giới phân phối, đặc biệt xung quanh 2K, 4K và 6K bản ghi. Điều này được mong đợi vì mô hình nháp ban đầu gặp khó khăn khi đối mặt với phân phối mới. Tuy nhiên, giá trị alpha phục hồi nhanh chóng khi OSD xử lý nhiều dữ liệu hơn, nổi bật khả năng thích ứng của nó với các phân phối truy vấn thay đổi.

--- TRANG 7 ---
Giải Mã Suy Đoán Trực Tuyến

[Các biểu đồ hiển thị tỷ lệ chấp nhận token trực tuyến (α) qua các bộ dữ liệu khác nhau]

Hình 3. Tỷ lệ chấp nhận trực tuyến (α) qua các bộ dữ liệu khác nhau. Trục x đại diện cho số bản ghi mà OSD đã xử lý. α được tính trung bình trên 50 bản ghi gần đây nhất.

[Biểu đồ thay đổi phân phối]

Hình 4. Thay đổi phân phối

Chúng tôi cũng so sánh kết quả của chúng tôi với những kết quả từ môi trường tĩnh. Để đảm bảo mô hình nháp không chỉ ghi nhớ dữ liệu, chúng tôi chọn các mẫu khác biệt với dữ liệu đánh giá trực tuyến. Các mẫu này tương ứng với 30%, 50%, 70% và 100% khối lượng đánh giá trực tuyến của mỗi bộ dữ liệu, với số lượng 0,6K, 1K, 1,4K và 2K tương ứng. Như được mô tả trong Hình 4, khi có thay đổi ban đầu trong phân phối truy vấn, hiệu suất của OSD phù hợp hoặc hơi tụt hậu so với chưng cất với 30% dữ liệu. Tuy nhiên, nó nhanh chóng bắt kịp, phù hợp hoặc thậm chí vượt qua hiệu suất được thấy với quyền truy cập dữ liệu 70% đến 100%. Điều này nổi bật khả năng của OSD trong việc cạnh tranh với các mô hình được tiếp xúc đầy đủ với phân phối truy vấn, ngay cả khi không có hiểu biết sâu sắc về động lực truy vấn cơ bản.

5.3. Khối Lượng Công Việc Thực Tế & Mô Hình Nháp Tùy Chỉnh
Chúng tôi đánh giá OSD trên các cuộc trò chuyện LMSYS-chat thực tế (Phụ lục A.7) kéo dài 4 tháng. Chúng tôi đề xuất rằng việc sử dụng các mô hình nháp riêng biệt cho các truy vấn về các tiểu mục khác nhau có thể tăng cường tỷ lệ chấp nhận token. Để thực hiện thí nghiệm, chúng tôi so sánh hai chế độ: Mô hình Nháp Đơn và Mô hình Nháp Riêng Biệt. Trong trường hợp Mô hình Nháp Đơn, mô hình đích được ghép nối với một mô hình nháp duy nhất mà qua đó tất cả dữ liệu được xử lý đồng nhất. Ngược lại, trong trường hợp Mô hình Nháp Riêng Biệt, trong khi mô hình đích vẫn không thay đổi, nó được hỗ trợ bởi nhiều mô hình nháp. Ở đây, mỗi mô hình nháp được chuyên môn hóa để xử lý một chủ đề cụ thể. Khi nhận được yêu cầu mới, truy vấn trước tiên được phân loại bằng một số mô hình phân loại để xác định chủ đề liên quan. Sau đó, truy vấn được định hướng đến mô hình nháp phù hợp, được thiết kế cho chủ đề đó, sau đó suy đoán và chuyển tiếp yêu cầu đến mô hình đích.

Không như cách tiếp cận sử dụng một mô hình nháp duy nhất cho tất cả chủ đề, việc gán các mô hình nháp cụ thể cho từng chủ đề riêng lẻ thu hẹp phạm vi phân phối truy vấn mà mỗi mô hình phải thích ứng. Cách tiếp cận tập trung này đơn giản hóa quá trình học cho mỗi mô hình nháp, vì chúng xử lý một tập hợp truy vấn hạn chế hơn.

Đầu tiên, chúng tôi phân loại các cuộc trò chuyện dựa trên ngôn ngữ và chúng tôi tập trung vào các cuộc trò chuyện trong năm ngôn ngữ hàng đầu, loại trừ tiếng Anh. Đối với mỗi ngôn ngữ được chọn, chúng tôi sử dụng một LLaMA-160M độc lập để làm mô hình nháp. Tất cả mô hình nháp chia sẻ cùng một Vicuna-7B làm mô hình đích. Tỷ lệ chấp nhận token, được tính trung bình trên 100 yêu cầu gần đây nhất, được hiển thị trong Hình 5, cho thấy rằng OSD tăng cường tỷ lệ từ 0,1 đến 0,2, ngay cả với dưới 2K điểm dữ liệu. Đáng chú ý, tiếng Nhật là dễ nhất trong khi tiếng Bồ Đào Nha là khó nhất.

Tiếp theo, chúng tôi thử một cách khác để nhóm các cuộc trò chuyện bằng cách định tuyến cuộc trò chuyện đến các mô hình nháp được chỉ định cho các chủ đề khác nhau. Chúng tôi lấy chủ đề của các truy vấn bằng cách sử dụng mô hình Bert được chưng cất và tinh chỉnh (Luqmani, 2023), tập trung vào năm chủ đề hàng đầu. Đối với các chủ đề có hơn 5K cuộc trò chuyện, chúng tôi lấy mẫu đều để giữ trong phạm vi 5K. Hình 5 cho thấy tỷ lệ chấp nhận trên 0,6 qua các chủ đề, với các cuộc thảo luận Xã hội và Máy tính đạt đỉnh gần 0,9.

Để so sánh thêm giữa trường hợp Mô hình Nháp Đơn và Mô hình Nháp Riêng Biệt, chúng tôi phân tích tỷ lệ chấp nhận token và tiêu thụ bộ nhớ dưới đây.

Tỷ lệ chấp nhận token: chúng tôi đo và vẽ biểu đồ tỷ lệ chấp nhận token sử dụng - một mô hình nháp phổ quát duy nhất so với nhiều mô hình nháp cụ thể theo chủ đề - trong Hình 6, để nổi bật ý tưởng tùy chỉnh mô hình nháp cho các loại truy vấn khác nhau. Như được thấy từ biểu đồ, qua tất cả chủ đề, việc sử dụng nhiều mô hình nháp dẫn đến sự gia tăng tỷ lệ chấp nhận token từ 0,1 đến 0,2. Điều này phù hợp với kỳ vọng của chúng tôi rằng các mô hình nháp được hưởng lợi từ phân phối truy vấn hẹp hơn, làm cho việc học và thích ứng dễ dàng hơn.

Tiêu thụ bộ nhớ: Trong phân tích dưới đây, chúng tôi giả định rằng nhiều mô hình nháp được tải trước vào bộ nhớ. Khi

--- TRANG 8 ---
Giải Mã Suy Đoán Trực Tuyến

Bảng 2. Độ trễ token đo được (ms) và tăng tốc qua các α đo được trên một A100-80G đơn với kích thước batch 1. Original là suy luận không có giải mã suy đoán.

[THIS IS TABLE: Shows timing and speedup data across different configurations]

xem xét năm mô hình nháp, kích thước tích lũy đạt 800M, khoảng 10% kích thước mô hình đích (7B), cả về trọng số mô hình và kích thước key-value (kv) cache. So với việc sử dụng một mô hình nháp duy nhất, định tuyến tăng chi phí bộ nhớ từ 2% lên 10%. Tuy nhiên, tương đối với kích thước của mô hình đích, yêu cầu bộ nhớ bổ sung này nên có thể quản lý được. Chúng tôi nghĩ có sự cân bằng giữa số lượng mô hình nháp và tiêu thụ bộ nhớ và để lại cho nghiên cứu tương lai việc quyết định số lượng mô hình nháp tối ưu và chiến lược phân loại tốt nhất.

5.4. Đo Độ Trễ
Trong phần này, chúng tôi đo độ trễ OSD ở các tỷ lệ chấp nhận khác nhau. Sau đó, chúng tôi đánh giá cải thiện hiệu suất được cung cấp bởi các mô hình được chưng cất qua bốn bộ dữ liệu được đánh giá. Trong suốt các thí nghiệm này, chúng tôi nghiên cứu hai cấu hình: (LLaMA-160M nháp, Vicuna-7B đích) và (TinyLLaMA-1.1B nháp, Vicuna-33B đích). Chúng tôi thực hiện các thí nghiệm với llamacpp (Gerganov, 2023) trên một A100-80G đơn.

Như được minh họa trong Bảng 2, đối với tỷ lệ chấp nhận token giống nhau, sự kết hợp của TinyLLaMA-1.1B và Vicuna-33B vượt trội hơn cặp LLaMA-160M và Vicuna-7B về tăng tốc. Hiệu suất được tăng cường này có thể quy cho sự khác biệt độ trễ lớn hơn giữa mô hình nháp và đích trong cặp đầu tiên (ký hiệu là c = 0.08 cho TinyLLaMA-1.1B và Vicuna-33B, so với c = 0.13 cho LLaMA-160M và Vicuna-7B). Nói đơn giản hơn, mô hình nháp tốn chi phí tương đối thấp hơn, dẫn đến tăng tốc rõ rệt hơn với độ chính xác suy đoán tương đương. Nhìn chung, OSD có thể đạt tăng tốc tối đa 2.63× khi tỷ lệ chấp nhận token vượt quá 0.9. Cuối cùng, chúng tôi đánh giá tăng tốc đạt được bởi mô hình được chưng cất của OSD trên bốn bộ dữ liệu được đánh giá. Đối với mô hình nháp, như đã thảo luận trong Phần 4.1, chúng tôi sử dụng lấy mẫu giáo viên với KD thuận làm phương pháp chưng cất, như được chi tiết trong Bảng 3. Kết quả cho thấy rằng OSD của chúng tôi có thể mang lại tăng tốc từ 1.42× đến 2.17× qua bốn bộ dữ liệu được đánh giá.

5.5. Phân Tích Định Tính
Trong phần này, chúng tôi phân tích cách OSD tăng cường tỷ lệ chấp nhận token, và những token nào mô hình nháp thu được qua các phân phối truy vấn thay đổi.

Bảng 3. Độ trễ token đo được (ms) và tăng tốc trên bốn bộ dữ liệu.

[THIS IS TABLE: Shows timing and speedup data across different datasets]

Độ chính xác và recall token tần suất cao. Trong thí nghiệm trên bộ dữ liệu Spider, (LLaMA-160M nháp, Vicuna-7B đích). Chúng tôi xác định 100 token hàng đầu được tạo ra thường xuyên nhất bởi mô hình đích, chiếm 72.2% tất cả xuất hiện, theo phân phối luật công suất. Hình 7 cho thấy sự cải thiện đáng kể trong cả độ chính xác và recall của những token này sau chưng cất.

Token được học qua các bộ dữ liệu khác nhau. Chúng tôi phân tích 10 token hàng đầu với những cải thiện độ chính xác và recall rõ rệt nhất qua các bộ dữ liệu khác nhau, tập trung vào 100 token thường xuyên nhất để hiểu xu hướng học của mô hình nháp. Như được chi tiết trong Bảng 6, các token được cải thiện phù hợp với phân phối dữ liệu cơ bản. Ví dụ, trong bộ dữ liệu Spider SQL, các token như SELECT và WHERE có tỷ lệ chấp nhận cao hơn đáng kể sau chưng cất. Những mẫu này nổi bật khả năng của mô hình nháp trong việc thích ứng và dự đoán các token nhất quán với phân phối dữ liệu.

5.6. OSD và Medusa
Tiếp theo, chúng tôi chứng minh việc áp dụng OSD cho Medusa. Bằng cách thường xuyên cập nhật trọng số của lớp tuyến tính trong giờ không cao điểm, OSD có thể cải thiện thêm tỷ lệ chấp nhận token trên bộ dữ liệu cho trước và đạt được tăng tốc thêm.

Medusa (Cai et al., 2024) khác với giải mã suy đoán tiêu chuẩn theo hai cách chính: (1) Giải mã suy đoán tiêu chuẩn đề xuất một token duy nhất cho mỗi vị trí, trong khi Medusa đề xuất nhiều ứng viên token cho mỗi vị trí. (2) Giải mã suy đoán tiêu chuẩn sử dụng một mô hình nháp nhỏ, độc lập để đề xuất token, trong khi Medusa sử dụng các đầu bổ sung—các lớp tuyến tính lấy các trạng thái ẩn cuối cùng làm đầu vào và xuất phân phối xác suất token cho mỗi vị trí.

Trong Bảng 4, chúng tôi so sánh hiệu suất của Medusa-v1 kết hợp với OSD. Bằng cách tinh chỉnh các đầu bổ sung trong Medusa trên bộ dữ liệu Spider, sự kết hợp này đạt được tăng tốc 2.01×, so với cải thiện 1.34× quan sát được với Medusa đơn thuần. Đáng chú ý, khi giải mã suy đoán tiêu chuẩn với cập nhật trực tuyến được áp dụng cho bộ dữ liệu Spider, nó mang lại kết quả tốt hơn thậm chí nhiều hơn, đạt tăng tốc 2.17× so với cải thiện 2.01× được cung cấp bởi Medusa với OSD. Trên bộ dữ liệu Arena, Medusa-v1 thường hoạt động tốt trên bộ dữ liệu chat vì các đầu phụ được huấn luyện ban đầu trên bộ dữ liệu ShareGPT (sha, 2023), thể hiện tăng tốc 2.03× mà không cần cập nhật nào. Ngược lại, sự kết hợp của giải mã suy đoán tiêu chuẩn với OSD mang lại kết quả tệ hơn so với Medusa-v1 đơn thuần, chỉ với tăng tốc 1.51×. Tuy nhiên, OSD vẫn quản lý để tăng cường hiệu suất của Medusa-v1 thêm 0.35×. Điều này nhấn mạnh tiềm năng của OSD trong việc tối ưu hóa thêm phương pháp giải mã suy đoán kiểu cây.

Bảng 4. OSD và Medusa

[THIS IS TABLE: Shows comparison data for OSD and Medusa across different datasets]

6. Kết luận
Hiệu quả của giải mã suy đoán phụ thuộc vào sự xấp xỉ của mô hình nháp đối với mô hình đích. Chúng tôi giới thiệu một phương pháp suy đoán trực tuyến liên tục tăng cường mô hình nháp dựa trên các phân phối dữ liệu thay đổi. Các thí nghiệm trên cả dữ liệu tổng hợp và thực tế chứng minh rằng giải mã suy đoán trực tuyến nhanh chóng thích ứng với các phân phối dữ liệu mới, tăng cường đáng kể tỷ lệ chấp nhận token.

Tuyên bố Tác động
Bài báo này trình bày công trình có mục tiêu thúc đẩy lĩnh vực Học Máy. Có nhiều hậu quả xã hội tiềm năng của công trình của chúng tôi, không có gì trong số đó chúng tôi cảm thấy cần được nổi bật cụ thể ở đây.

Lời cảm ơn
Z.J. Deng được hỗ trợ bởi NSF của Trung Quốc (Số 62306176), Quỹ Khoa học Tự nhiên của Thượng Hải (Số 23ZR1428700), Chương trình R&D Chủ chốt của tỉnh Sơn Đông, Trung Quốc (2023CXGC010112), và Quỹ Mô hình Nền tảng CCF-Baichuan-Ebtech. Công trình này được hỗ trợ một phần bởi Quỹ Khoa học Quốc gia thông qua các grant CCF-2238346, IIS-1955488, IIS-2027575, ARO W911NF2110339, ONR N00014-21-1-2724, và các giải thưởng DOE DE-SC0016260, DE-SC0021982.

[Phần tham khảo và phụ lục tiếp tục với danh sách đầy đủ các tài liệu tham khảo và các phần phụ lục chi tiết...]
