# 2302.01318.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/speculative/2302.01318.pdf
# Kích thước tệp: 349916 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
2023-2-3
Tăng tốc Giải mã Mô hình Ngôn ngữ Lớn
với Lấy mẫu Suy đoán
Charlie Chen1, Sebastian Borgeaud1, Geoﬀrey Irving1, Jean-Baptiste Lespiau1, Laurent Sifre1và John
Jumper1
1Tất cả tác giả từ DeepMind
Chúng tôi trình bày lấy mẫu suy đoán, một thuật toán để tăng tốc giải mã transformer bằng cách cho phép
tạo ra nhiều token từ mỗi lần gọi transformer. Thuật toán của chúng tôi dựa trên quan sát rằng
độ trễ của việc chấm điểm song song các phần tiếp tục ngắn, được tạo ra bởi một mô hình nháp
nhanh hơn nhưng ít mạnh mẽ hơn, có thể so sánh được với việc lấy mẫu một token duy nhất từ mô hình
đích lớn hơn. Điều này được kết hợp với một sơ đồ lấy mẫu từ chối được sửa đổi mới bảo toàn phân phối
của mô hình đích trong phạm vi số học phần cứng. Chúng tôi đánh giá lấy mẫu suy đoán với Chinchilla,
một mô hình ngôn ngữ 70 tỷ tham số, đạt được tăng tốc giải mã 2–25 lần trong thiết lập phân tán, mà không
làm tổn hại chất lượng mẫu hoặc thực hiện các sửa đổi đối với chính mô hình.

Giới thiệu
Việc mở rộng quy mô các mô hình transformer lên 500B+ tham số đã dẫn đến những cải thiện hiệu suất lớn trên
nhiều tác vụ xử lý ngôn ngữ tự nhiên, thị giác máy tính và học tăng cường (Arnab et al., 2021; Brown
et al., 2020; Chowdhery et al., 2022; Dosovitskiy et al., 2020; Hoﬀmann et al., 2022; Rae et al., 2021).
Tuy nhiên, việc giải mã transformer vẫn là một quá trình rất tốn kém và không hiệu quả trong chế độ này.
Lấy mẫu transformer thường bị giới hạn bởi băng thông bộ nhớ (Shazeer, 2019), vì vậy đối với một tập hợp
phần cứng nhất định, thời gian tạo ra một token duy nhất trong các mô hình transformer tỷ lệ với xấp xỉ
bậc nhất với kích thước tham số và kích thước bộ nhớ transformer. Kích thước của các mô hình ngôn ngữ
cũng đòi hỏi phục vụ với song song hóa mô hình – thêm chi phí truyền thông (Pope
et al., 2022) và nhân lên yêu cầu tài nguyên. Vì mỗi token mới phụ thuộc vào quá khứ,
nhiều lần gọi transformer như vậy được yêu cầu để lấy mẫu một chuỗi mới.

Chúng tôi trình bày một thuật toán để tăng tốc lấy mẫu transformer cho các ứng dụng quan trọng về độ trễ,
mà chúng tôi gọi là lấy mẫu suy đoán (SpS). Điều này được đạt được bằng:

1. Tạo ra một bản nháp ngắn có độ dài 𝐾. Điều này có thể đạt được bằng một mô hình song song (Stern
et al., 2018) hoặc bằng cách gọi một mô hình tự hồi quy nhanh hơn 𝐾 lần. Chúng tôi sẽ gọi mô hình này
là mô hình nháp, và tập trung vào trường hợp nó là tự hồi quy.

2. Chấm điểm bản nháp bằng mô hình lớn hơn, mạnh mẽ hơn mà chúng ta muốn lấy mẫu từ đó. Chúng tôi sẽ
gọi mô hình này là mô hình đích.

3. Sử dụng một sơ đồ lấy mẫu từ chối được sửa đổi, chấp nhận một tập con của 𝐾 token nháp từ trái
sang phải, khôi phục phân phối của mô hình đích trong quá trình này.

Trực quan, thường có các chuỗi mà token tiếp theo có thể "hiển nhiên". Do đó, nếu có
sự đồng thuận mạnh mẽ giữa phân phối của mô hình nháp và mô hình đích trên một token nhất định hoặc chuỗi con
của các token, thiết lập này cho phép tạo ra nhiều token mỗi khi mô hình đích được gọi.

Chúng tôi chỉ ra rằng tỷ lệ chấp nhận dự kiến của các token nháp đủ để bù đắp chi phí của

Tác giả liên hệ: ccharlie@deepmind.com
©2023 DeepMind. Tất cả quyền được bảo lưu arXiv:2302.01318v1 [cs.CL] 2 Feb 2023

--- TRANG 2 ---
Tăng tốc Giải mã Mô hình Ngôn ngữ Lớn với Lấy mẫu Suy đoán

quá trình soạn thảo cho các mô hình ngôn ngữ lớn, dẫn đến một phương pháp hiệu quả và thực tế để giảm
độ trễ lấy mẫu mà không cần sửa đổi mô hình đích hoặc làm thiên lệch phân phối mẫu.

Tùy thuộc vào miền đánh giá, SpS dẫn đến tăng tốc 2–25 lần khi lấy mẫu từ Chinchilla
(Hoﬀmann et al., 2022). Đáng chú ý, số token trung bình mỗi giây với SpS thường vượt quá giới hạn
lý tưởng hóa về tốc độ lấy mẫu tự hồi quy do băng thông bộ nhớ áp đặt.

Công trình Liên quan
Đã có một lượng lớn công trình tập trung vào việc cải thiện độ trễ lấy mẫu của các transformer lớn
và các mô hình tự hồi quy khác.

Vì hiệu suất lấy mẫu được liên kết chặt chẽ với kích thước mô hình trong bộ nhớ, lượng tử hóa thành
int8 hoặc thậm chí int4 (Dettmers et al., 2022; Yao et al., 2022) và chưng cất (Jiao et al., 2020; Sanh
et al., 2019) của các transformer là những kỹ thuật hiệu quả để giảm độ trễ lấy mẫu với ít hoặc không có
hình phạt hiệu suất. Quan sát rằng kích thước mô hình đóng góp ít hơn dự kiến vào hiệu suất cuối cùng (Hoﬀmann et al., 2022) cũng đã khuyến khích các mô hình ngôn ngữ nhỏ hơn nói chung.

Trong quá trình lấy mẫu, một bộ nhớ đệm của các khóa và giá trị được duy trì cho mọi lớp attention, và có thể
trở thành nút thắt cổ chai băng thông bộ nhớ khi kích thước batch tăng. Các phương pháp như multi-query
attention (Shazeer, 2019) nhằm cải thiện hiệu suất lấy mẫu bằng cách thu nhỏ bộ nhớ đệm này. Tuy nhiên
những kỹ thuật này hiệu quả nhất trong việc tối đa hóa thông lượng (ở kích thước batch lớn hơn) thay vì độ trễ,
đặc biệt đối với các mô hình lớn hơn nơi phần lớn ngân sách băng thông bộ nhớ được tiêu thụ bởi các
tham số.

Sử dụng kết hợp các kỹ thuật trên, thêm vào một số tối ưu hóa cấp thấp cho
TPU, Pope et al. (2022) đã cải thiện đáng kể độ trễ và hiệu quả phục vụ của PaLM 540B.

Có một lượng công trình hiện tại tương tự khai thác hiệu quả của các transformer và các mô hình
chuỗi hoạt động song song. Điều này bao gồm lấy mẫu song song khối (Stern et al., 2018), giải mã
tích cực (Ge et al., 2022), thêm vào một số công trình trong việc song song hóa các mô hình tự hồi quy trong
miền hình ảnh (Song et al., 2021; Wiggers và Hoogeboom, 2020). Những phương pháp này chưa được
thích ứng với các trường hợp sử dụng mô hình ngôn ngữ điển hình vì chúng hoặc chỉ hoạt động với lấy mẫu tham lam, làm thiên lệch
kết quả hoặc tập trung vào các phương thức khác. Hơn nữa, theo hiểu biết của chúng tôi, không có kỹ thuật nào trong số này
đã được mở rộng quy mô cho thiết lập phân tán, điều cần thiết cho các bộ giải mã đắt nhất với
hàng chục hoặc hàng trăm tỷ tham số.

Trùng hợp, công trình trong bản thảo này được thực hiện đồng thời và độc lập với
công trình về giải mã suy đoán từ Leviathan et al. (2022). Chúng tôi tập trung nhiều hơn vào thiết lập phục vụ phân tán cho các mô hình lớn và đưa ra một số tối ưu hóa gia tăng, nhưng ngoài ra ý tưởng cốt lõi
cơ bản là giống nhau.

Lấy mẫu Tự hồi quy
Trong khi các transformer có thể được huấn luyện hiệu quả và song song trên TPU và GPU, các mẫu thường
được rút ra tự hồi quy (Xem thuật toán 1). Đối với hầu hết các ứng dụng, lấy mẫu tự hồi quy (ArS) là
bị giới hạn cao bởi băng thông bộ nhớ và do đó không thể sử dụng hiệu quả phần cứng gia tốc hiện đại
(Shazeer, 2019). Một lần gọi mô hình bị giới hạn bộ nhớ chỉ tạo ra một token duy nhất cho mỗi chuỗi trong
batch, do đó việc tạo ra nhiều token giới thiệu một lượng lớn độ trễ trong bất kỳ hệ thống nào
sử dụng nó.

--- TRANG 3 ---
Tăng tốc Giải mã Mô hình Ngôn ngữ Lớn với Lấy mẫu Suy đoán

Điều này đặc biệt có vấn đề khi số lượng tham số trong mô hình tăng. Vì tất cả các tham số
mô hình cần đi qua ít nhất một chip gia tốc, kích thước mô hình chia cho tổng
băng thông bộ nhớ trên tất cả các chip cho chúng ta một giới hạn cứng về tốc độ lấy mẫu tự hồi quy
tối đa. Các mô hình lớn hơn cũng yêu cầu phục vụ trên nhiều gia tốc, giới thiệu thêm một nguồn
độ trễ do chi phí truyền thông giữa các thiết bị.

Thuật toán 1 Tự hồi quy (ArS) với Mô hình Tự hồi quy
Cho mô hình đích tự hồi quy 𝑞¹jº và chuỗi prompt ban đầu 𝑥1𝑥𝑡 và độ dài chuỗi
đích 𝑇.
Khởi tạo 𝑛 𝑡.
while 𝑛 𝑇 do
    Lấy mẫu 𝑥𝑛¸1𝑞¹𝑥j𝑥1𝑥𝑛º
    𝑛 𝑛¸1
end while

Thuật toán 2 Lấy mẫu Suy đoán (SpS) với Mô hình Đích và Nháp Tự hồi quy
Cho lookahead 𝐾 và độ dài chuỗi đích tối thiểu 𝑇.
Cho mô hình đích tự hồi quy 𝑞¹jº, và mô hình nháp tự hồi quy 𝑝¹jº, chuỗi prompt
ban đầu 𝑥0𝑥𝑡.
Khởi tạo 𝑛 𝑡.
while 𝑛 𝑇 do
    for 𝑡=1 : 𝐾 do
        Lấy mẫu nháp tự hồi quy ˜𝑥𝑡𝑝¹𝑥j𝑥1𝑥𝑛˜𝑥1 ˜𝑥𝑡1º
    end for
    Song song, tính 𝐾¸1 tập logit từ các bản nháp ˜𝑥1 ˜𝑥𝐾:
    𝑞¹𝑥j𝑥1𝑥𝑛º 𝑞¹𝑥j𝑥1𝑥𝑛˜𝑥1º  𝑞¹𝑥j𝑥1𝑥𝑛˜𝑥1 ˜𝑥𝐾º
    for 𝑡=1 : 𝐾 do
        Lấy mẫu 𝑟𝑈»01¼ từ phân phối đều.
        if 𝑟 min(1, 𝑞¹𝑥j𝑥1𝑥𝑛¸𝑡1º/𝑝¹𝑥j𝑥1𝑥𝑛¸𝑡1º) then
            Đặt 𝑥𝑛¸𝑡 ˜𝑥𝑡 và 𝑛 𝑛¸1.
        else
            lấy mẫu 𝑥𝑛¸𝑡¹𝑞¹𝑥j𝑥1𝑥𝑛¸𝑡1º𝑝¹𝑥j𝑥1𝑥𝑛¸𝑡1ºº¸ và thoát khỏi vòng lặp for.
        end if
    end for
    Nếu tất cả token 𝑥𝑛¸1𝑥𝑛¸𝐾 được chấp nhận, lấy mẫu token thêm 𝑥𝑛¸𝐾¸1𝑞¹𝑥j𝑥1𝑥𝑛𝑥𝑛¸𝐾º và
    đặt 𝑛 𝑛¸1.
end while

Lấy mẫu Suy đoán

Chấm điểm Có điều kiện
Đối với lấy mẫu suy đoán (Xem thuật toán 2), trước tiên chúng tôi đưa ra quan sát rằng việc tính toán logit
của một phần tiếp tục ngắn 𝐾 token song song có độ trễ rất tương tự như việc lấy mẫu một

--- TRANG 4 ---
Tăng tốc Giải mã Mô hình Ngôn ngữ Lớn với Lấy mẫu Suy đoán

token duy nhất. Chúng tôi tập trung chú ý vào các transformer lớn, được phân mảnh theo kiểu Megatron (Shoeybi et al.,
2019). Đối với những mô hình này, phần lớn thời gian lấy mẫu có thể được quy cho ba thành phần:

1. Các Lớp Tuyến tính: Đối với kích thước batch nhỏ, mỗi lớp tuyến tính chỉ xử lý một số lượng nhỏ
embedding. Điều này khiến các phép nhân ma trận dày đặc trong các lớp feed-forward, truy vấn, khóa,
tính toán giá trị và phép chiếu attention cuối cùng trở nên bị giới hạn bộ nhớ. Đối với 𝐾 nhỏ,
điều này sẽ tiếp tục bị giới hạn bộ nhớ và do đó mất một lượng thời gian tương tự.

2. Cơ chế Attention: Cơ chế attention cũng bị giới hạn bộ nhớ. Trong quá trình lấy mẫu,
chúng ta duy trì một bộ nhớ đệm của tất cả các khóa và giá trị của các token trước đó trong chuỗi để tránh
tính toán lại. Những KV-cache này lớn, và chiếm phần lớn việc sử dụng băng thông bộ nhớ
cho cơ chế attention. Tuy nhiên, vì kích thước KV-cache không
thay đổi khi chúng ta tăng 𝐾, có rất ít hoặc không có sự chênh lệch trong thành phần này.

3. All-reduce: Khi các mô hình tăng kích thước, các tham số của chúng cần được chia trên nhiều gia tốc,
dẫn đến chi phí truyền thông. Với Megatron, điều này thể hiện dưới dạng all-reduce
sau mỗi lớp feed-forward và attention. Vì chỉ các activation cho một số lượng nhỏ
token được truyền, thao tác này thường bị giới hạn độ trễ thay vì giới hạn thông lượng
cho cả lấy mẫu và chấm điểm (đối với 𝐾 nhỏ). Một lần nữa, điều này dẫn đến một lượng thời gian
tương tự được dành trong hai trường hợp.

Các nguồn chi phí khác có thể tồn tại, tùy thuộc vào việc triển khai transformer cụ thể. Do đó
vẫn có thể lựa chọn mã hóa vị trí, phương pháp giải mã (ví dụ: có thể yêu cầu sắp xếp
cho nucleus sampling), hạn chế phần cứng, v.v. có thể giới thiệu một số sự khác biệt giữa chấm điểm
và lấy mẫu. Tuy nhiên, nếu các điều kiện được đáp ứng sao cho các thành phần trên chiếm ưu thế thì
chấm điểm không nên chậm hơn đáng kể đối với 𝐾 nhỏ.

Lấy mẫu Từ chối Được sửa đổi
Chúng tôi yêu cầu một phương pháp để khôi phục phân phối của mô hình đích từ các mẫu từ mô hình
nháp, và logit của các token đó từ cả hai mô hình.

Để đạt được điều này, chúng tôi giới thiệu sơ đồ lấy mẫu từ chối sau đây của các token được soạn thảo. Cho một
chuỗi token 𝑥1𝑥𝑛, và 𝐾 token nháp ˜𝑥𝑛¸1 ˜𝑥𝑛¸𝐾 được tạo ra từ 𝑝¹jº, chúng ta chấp nhận ˜𝑥𝑛¸1
với xác suất:

min(1, 𝑞¹˜𝑥𝑛¸1j𝑥1𝑥𝑛º/𝑝¹˜𝑥𝑛¸1j𝑥1𝑥𝑛º)

Trong đó 𝑞¹˜𝑥𝑛¸1j𝑥1𝑥𝑛º và 𝑝¹˜𝑥𝑛¸1j𝑥1𝑥𝑛º là xác suất của ˜𝑥𝑛¸1 theo mô hình đích và
nháp tương ứng, có điều kiện trên ngữ cảnh cho đến nay.

Nếu token được chấp nhận, chúng ta đặt 𝑥𝑛¸1 ˜𝑥𝑛¸1 và lặp lại quá trình cho ˜𝑥𝑛¸2 cho đến khi một token
bị từ chối hoặc tất cả các token đã được chấp nhận.

Nếu ˜𝑥𝑛¸1 bị từ chối, chúng ta lấy mẫu lại 𝑥𝑛¸1 từ phân phối sau:

𝑥𝑛¸1¹𝑞¹𝑥j𝑥1𝑥𝑛º𝑝¹𝑥j𝑥1𝑥𝑛ºº¸

Trong đó ¹º¸ biểu thị:

¹𝑓¹𝑥ºº¸=max¹0 𝑓¹𝑥ººÍ𝑥max¹0 𝑓¹𝑥ºº

Bằng cách áp dụng điều này một cách tuần tự, chúng ta khôi phục phân phối của mô hình đích cho các token được chấp nhận
(xem chứng minh trong Định lý 1) trong phạm vi số học phần cứng. Lưu ý rằng:

--- TRANG 5 ---
Tăng tốc Giải mã Mô hình Ngôn ngữ Lớn với Lấy mẫu Suy đoán

• Ít nhất một token sẽ luôn được tạo ra từ một vòng lặp draft-accept – nếu token đầu tiên bị
từ chối, một token hợp lệ được lấy mẫu lại.

• Vì token cuối cùng của bản nháp cho chúng ta logit cho token tiếp theo, nếu mọi token được soạn thảo đều
được chấp nhận, chúng ta có thể lấy mẫu từ nó bình thường. Điều này cho chúng ta tối đa 𝐾¸1 token mỗi vòng lặp,
so với việc triển khai ngây thơ chỉ trả về 𝐾 token.

Với các phương pháp lấy mẫu tiêu chuẩn như nucleus, top-k sampling và điều chỉnh nhiệt độ, chúng ta
có thể sửa đổi xác suất tương ứng trước khi áp dụng sơ đồ lấy mẫu từ chối này. Chúng tôi đã
quan sát thấy rằng tỷ lệ chấp nhận tổng thể mạnh mẽ đối với các tham số cụ thể được sử dụng.

Vì chúng ta không tương tác với phần thân của chính transformer, phương pháp này có thể được sử dụng
kết hợp với nhiều kỹ thuật khác để tăng tốc hoặc tối ưu hóa việc sử dụng bộ nhớ của lấy mẫu, như
lượng tử hóa và multi-query attention.

Lựa chọn Mô hình Nháp
Vì tiêu chí chấp nhận đảm bảo phân phối của mô hình đích trong các mẫu của chúng ta, chúng ta
tự do lựa chọn phương pháp để soạn thảo một phần tiếp tục miễn là nó hiển thị logit, và có
tỷ lệ chấp nhận đủ cao và/hoặc độ trễ đủ thấp để hòa vốn. Có một số cách tiếp cận
ở đây:

• Kết hợp việc tạo bản nháp vào mô hình đích, và huấn luyện mô hình từ đầu. Đây
là chiến lược được sử dụng bởi Stern et al. (2018), thêm nhiều đầu vào transformer để
tạo ra nhiều token.

• Sử dụng chưng cất cấp độ chuỗi (Kim và Rush, 2016) để tạo ra một mô hình thứ hai
dự đoán 𝐾 token song song. Chiến lược này được sử dụng bởi Ge et al. (2022).

• Đặt một phần của các activation của mô hình đích làm đầu vào cho mô hình nháp, và huấn luyện
mô hình nháp với đầu vào này.

Mặc dù những phương pháp này có thể sẽ tạo ra các bản nháp mạnh mẽ, chúng đòi hỏi một số lượng lớn dữ liệu được tạo
ra từ mô hình đích hoặc thay đổi đối với mô hình đích. Chưng cất cấp độ chuỗi đặc biệt
sẽ yêu cầu một ngân sách tính toán lớn. Điều này làm cho chúng ít thực tế hơn cho các ứng dụng quy mô lớn.

Trong khi các mô hình ngôn ngữ lớn tạo ra các mẫu tốt hơn, trực quan có những token "dễ hơn" để dự đoán
mà các mô hình nhỏ hơn có thể đủ. Do đó chúng ta có thể đơn giản sử dụng một phiên bản nhỏ hơn của
mô hình ngôn ngữ đích làm bản nháp và đạt được tỷ lệ chấp nhận cao. Điều này cũng sẽ thuận tiện
từ góc độ kỹ thuật và quy trình làm việc, vì công cụ mạnh mẽ cho những mô hình như vậy đã có sẵn
để huấn luyện mô hình đích ngay từ đầu.

Kết quả
Chúng tôi huấn luyện một mô hình nháp 4 tỷ tham số được tối ưu hóa cho độ trễ lấy mẫu trên 16 TPU v4s – cùng
phần cứng thường được sử dụng để phục vụ Chinchilla cho mục đích nghiên cứu. Mô hình này được huấn luyện
với cùng tokenizer và bộ dữ liệu như Chinchilla, với chiều rộng nhỏ hơn một chút và chỉ có 8
lớp. Số lượng lớp tương đối ít cho phép nó đạt được tốc độ lấy mẫu 1.8ms/token
so với 14.1ms/token của Chinchilla. Để biết chi tiết, vui lòng tham khảo các siêu tham số trong Bảng 2.

Đối với thiết lập phân tán, việc chọn ngây thơ một mô hình nhỏ làm bản nháp là không đủ, vì các
mô hình khác nhau có thiết lập suy luận tối ưu khác nhau. Ví dụ, thông thường phục vụ Chinchilla 70B

--- TRANG 6 ---
Tăng tốc Giải mã Mô hình Ngôn ngữ Lớn với Lấy mẫu Suy đoán

Bảng 1 | Hiệu suất và tốc độ của Chinchilla trên XSum và HumanEval với lấy mẫu ngây thơ và suy đoán
ở kích thước batch 1 và 𝐾=4. XSum được thực hiện với tham số nucleus 𝑝=0.8, và
HumanEval với 𝑝=0.95 và nhiệt độ 0.8.

Phương pháp Lấy mẫu | Benchmark | Kết quả | Thời gian Token Trung bình | Tăng tốc
ArS (Nucleus) | XSum (ROUGE-2) | 0.112 | 14.1ms/Token | 1×
SpS (Nucleus) | | 0.114 | 7.52ms/Token | 1.92×
ArS (Greedy) | XSum (ROUGE-2) | 0.157 | 14.1ms/Token | 1×
SpS (Greedy) | | 0.156 | 7.00ms/Token | 2.01×
ArS (Nucleus) | HumanEval (100 Shot) | 45.1% | 14.1ms/Token | 1×
SpS (Nucleus) | | 47.0% | 5.73ms/Token | 2.46×

trên 16 TPU v4s (nơi nó đạt được 14.1ms/token đã nêu), trong khi chinchilla-optimal
7B đạt được độ trễ lấy mẫu thấp nhất trên 4 TPU v4s (nơi nó đạt được 5ms/token). Đối với các
mô hình nhỏ hơn, băng thông bộ nhớ và flops bổ sung không đủ để bù đắp chi phí truyền thông
bổ sung giữa nhiều thiết bị hơn – phục vụ 7B trên 16 TPU thực sự tăng độ trễ. Điều này
có nghĩa là 7B sẽ chỉ cung cấp tăng tốc khiêm tốn nếu được sử dụng làm bản nháp với topo tối ưu của nó, và
chúng ta sẽ không tận dụng đầy đủ phần cứng trong quá trình soạn thảo.

Chúng ta có thể tránh vấn đề này bằng cách huấn luyện một mô hình rộng hơn với số lượng lớp tương đối ít
để giảm thiểu chi phí truyền thông. Đã được quan sát thấy rằng hiệu suất của các mô hình ngôn ngữ
tương đối mạnh mẽ đối với những thay đổi trong tỷ lệ khung hình mô hình (Levine et al., 2020), vì vậy điều này cho phép chúng ta
phục vụ một mô hình nháp mạnh mẽ có thể được lấy mẫu nhanh chóng trên cùng phần cứng với mô hình đích.

Đánh giá trên XSum và HumanEval
Chúng tôi đánh giá lấy mẫu suy đoán với Chinchilla trên hai tác vụ và tóm tắt kết quả trong Bảng 1:

• Benchmark XSum (Narayan et al., 2018). Đây là một tác vụ tóm tắt ngôn ngữ tự nhiên
sử dụng prompt 1-shot nơi chúng tôi lấy mẫu tổng cộng 11.305 chuỗi với độ dài chuỗi tối đa 128.

• Tác vụ HumanEval 100-shot (Chen et al., 2021). Đây là một tác vụ tạo mã liên quan đến việc
tạo ra 16.400 mẫu với độ dài chuỗi tối đa 512.

Ngay cả với lấy mẫu tham lam, một token duy nhất lệch do số học có thể dẫn đến hai chuỗi
phân kỳ mạnh mẽ. Vì các hạt giống giả ngẫu nhiên được xử lý khác nhau giữa ArS và SpS, và
vì các đồ thị tính toán khác nhau dẫn đến số học khác nhau, chúng ta không thể mong đợi đầu ra
giống hệt nhau. Tuy nhiên, chúng ta mong đợi các mẫu đến từ cùng một phân phối trong phạm vi số học và
chúng tôi xác minh điều này theo kinh nghiệm bằng cách đánh giá những benchmark này.

Chúng tôi chạy các tác vụ ở kích thước batch 1 với SpS và ArS. Thời gian thực hiện mỗi vòng lặp SpS/ArS có
phương sai thấp, và chúng ta có thể đo trực tiếp từ hồ sơ TPU. Để có được tăng tốc trung bình, độ lệch
chuẩn và các chỉ số khác, chúng tôi ghi lại số lượng token được tạo ra cho mỗi vòng lặp suy đoán. Trong
Bảng 1 chúng tôi hiển thị hiệu suất trên benchmark XSum và HumanEval cho lấy mẫu ngây thơ và suy đoán
với Chinchilla.

--- TRANG 7 ---
Tăng tốc Giải mã Mô hình Ngôn ngữ Lớn với Lấy mẫu Suy đoán

Chúng tôi đạt được tăng tốc đáng kể trong cả hai tác vụ, với HumanEval đạt tăng tốc gần 2.5×.
Tuy nhiên, chúng tôi có sự ngang bằng trong các chỉ số benchmark – phân phối mẫu cơ bản có thể chứng minh là
giống nhau đến mức số học, và điều này xác minh rằng mô hình nháp không làm thiên lệch kết quả theo kinh nghiệm. Trong
trường hợp của HumanEval và XSum tham lam, tăng tốc này vượt quá giới hạn băng thông bộ nhớ lý thuyết
của phần cứng cho lấy mẫu tự hồi quy (kích thước mô hình chia cho tổng băng thông bộ nhớ).

Tỷ lệ chấp nhận thay đổi theo miền
Rõ ràng rằng tỷ lệ chấp nhận phụ thuộc vào ứng dụng và phương pháp giải mã.
HumanEval đạt được tăng tốc lớn hơn đáng kể—Chúng tôi đưa ra giả thuyết rằng điều này là do sự kết hợp
của mã chứa nhiều chuỗi con phổ biến (ví dụ: for i in range(len(arr)): sẽ
tương đối dễ cho một mô hình nháp đoán), thường được phân tách thành một tập hợp nhỏ hơn các token ngắn hơn
và giá trị nhiệt độ làm sắc nét cả logit nháp và đích.

[Hình 1: Ba biểu đồ hiển thị mối quan hệ giữa số lượng token nháp (K) và thời gian lấy mẫu trung bình, tỷ lệ chấp nhận, và thời gian vòng lặp tổng]

Hình 1 | Trái: Thời gian trung bình để tạo 128 token, với độ lệch chuẩn. Lưu ý rằng khi 𝐾
tăng, tăng tốc tổng thể đạt đỉnh hoặc thậm chí thoái lui, với XSum tối ưu ở 𝐾=3. Phương sai
tăng nhất quán với 𝐾. Giữa: Số lượng token được chấp nhận trung bình chia cho
𝐾¸1– điều này phục vụ như một thước đo hiệu quả tổng thể của sơ đồ từ chối được sửa đổi, giảm
với lookahead. Phải: Thời gian trung bình mỗi vòng lặp tăng xấp xỉ tuyến tính với 𝐾
do số lượng lời gọi mô hình tăng. Lưu ý rằng gradient hơi cao hơn tốc độ lấy mẫu
của mô hình nháp, do chi phí bổ sung trong nucleus decoding.

Đánh đổi giữa bản nháp dài hơn và chấm điểm thường xuyên hơn
Chúng tôi hình dung sự đánh đổi của việc tăng 𝐾, số lượng token được lấy mẫu bởi mô hình nháp trong Hình 1.
Khi 𝐾 tăng, chúng ta cần ít lời gọi chấm điểm hơn từ các mô hình lớn để tạo ra cùng độ dài
chuỗi, có khả năng cho chúng ta tăng tốc lớn hơn. Tuy nhiên, tổng thời gian vòng lặp tăng xấp xỉ
tuyến tính với số lượng lời gọi mô hình nháp lớn hơn và tăng nhỏ trong thời gian chấm điểm. Hiệu quả
tổng thể của tỷ lệ token được chấp nhận giảm khi 𝐾 tăng, vì các token sau phụ thuộc
vào việc chấp nhận các token trước đó. Điều này dẫn đến tăng tốc trung bình đạt đỉnh hoặc
thậm chí thoái hóa với 𝐾 lớn hơn (ví dụ, độ trễ nucleus của XSum được giảm thiểu ở 𝐾=3),
tùy thuộc vào miền.

--- TRANG 8 ---
Tăng tốc Giải mã Mô hình Ngôn ngữ Lớn với Lấy mẫu Suy đoán

Hơn nữa, mặc dù các giá trị 𝐾 lớn hơn có thể mang lại tăng tốc trung bình lớn hơn một chút trong một số
trường hợp, nó cũng tăng phương sai của thời gian để tạo ra một chuỗi đầy đủ. Điều này có thể
có vấn đề đối với các thiết lập nơi độ trễ P90, P99 quan trọng.

Kết luận
Trong công trình này, chúng tôi trình bày một thuật toán và quy trình làm việc mới để tăng tốc giải mã của các mô hình ngôn ngữ.
Lấy mẫu suy đoán không yêu cầu thực hiện bất kỳ sửa đổi nào đối với các tham số hoặc kiến trúc của mô hình ngôn ngữ
đích, không mất mát có thể chứng minh được trong phạm vi số học, mở rộng quy mô tốt với mô hình
nháp phù hợp và bổ sung cho nhiều kỹ thuật hiện có để giảm độ trễ trong thiết lập kích thước batch nhỏ.

Chúng tôi tối ưu hóa và mở rộng quy mô kỹ thuật cho Chinchilla 70B sử dụng một mô hình nháp dễ
huấn luyện với cơ sở hạ tầng hiện có, chứng minh rằng nó mang lại tăng tốc lớn trên các tác vụ benchmark
và các phương pháp giải mã phổ biến trong quá trình này. Chúng tôi xác minh rằng nó thực sự không mất mát theo kinh nghiệm trong
các tác vụ downstream của nó.

Tài liệu Tham khảo
A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lucic, và C. Schmid. Vivit: Một vision transformer video.
Trong 2021 IEEE/CVF International Conference on Computer Vision (ICCV), trang 6816–6826. IEEE
Computer Society, 2021.

T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, et al. Các mô hình ngôn ngữ là những người học few-shot. Advances in neural information
processing systems, 33:1877–1901, 2020.

M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph,
G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray,
N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings,
M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang,
I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra,
E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew,
D. Amodei, S. McCandlish, I. Sutskever, và W. Zaremba. Đánh giá các mô hình ngôn ngữ lớn được huấn luyện
trên mã. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.

A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung,
C. Sutton, S. Gehrmann, et al. Palm: Mở rộng quy mô mô hình hóa ngôn ngữ với pathways. arXiv preprint
arXiv:2204.02311, 2022.

T. Dettmers, M. Lewis, Y. Belkada, và L. Zettlemoyer. Llm.int8(): Nhân ma trận 8-bit cho
transformers ở quy mô lớn. arXiv preprint arXiv:2208.07339, 2022.

A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. Một hình ảnh đáng giá 16x16 từ: Transformers cho nhận dạng hình ảnh
ở quy mô lớn. arXiv preprint arXiv:2010.11929, 2020.

T. Ge, H. Xia, X. Sun, S. Chen, và F. Wei. Tăng tốc không mất mát cho tạo seq2seq với giải mã
tích cực. ArXiv, abs/2205.10350, 2022.

--- TRANG 9 ---
Tăng tốc Giải mã Mô hình Ngôn ngữ Lớn với Lấy mẫu Suy đoán

J. Hoﬀmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A.
Hendricks, J. Welbl, A. Clark, et al. Huấn luyện các mô hình ngôn ngữ lớn tối ưu tính toán. arXiv preprint
arXiv:2203.15556, 2022.

X. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, và Q. Liu. TinyBERT: Chưng cất BERT
cho hiểu biết ngôn ngữ tự nhiên. Trong Findings of the Association for Computational Linguistics: EMNLP 2020, trang 4163–4174, Online, Nov. 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.ﬁndings-emnlp.372. URL https://aclanthology.org/2020.findings-emnlp.372.

Y. Kim và A. M. Rush. Chưng cất kiến thức cấp độ chuỗi. CoRR, abs/1606.07947, 2016. URL
http://arxiv.org/abs/1606.07947.

Y. Leviathan, M. Kalman, và Y. Matias. Suy luận nhanh từ transformers qua giải mã suy đoán.
ArXiv, abs/2211.17192, 2022.

Y. Levine, N. Wies, O. Sharir, H. Bata, và A. Shashua. Sự tương tác độ sâu-chiều rộng trong self-attention.
arXiv preprint arXiv:2006.12467, 2020.

S. Narayan, S. B. Cohen, và M. Lapata. Đừng cho tôi chi tiết, chỉ cần tóm tắt! mạng nơ-ron tích chập
nhận biết chủ đề cho tóm tắt cực đoan. Trong Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, trang 1797–1807, Brussels, Belgium, Oct.-Nov. 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1206. URL https://aclanthology.org/D18-1206.

R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek, K. Xiao, S. Agrawal,
và J. Dean. Mở rộng quy mô suy luận transformer hiệu quả. arXiv preprint arXiv:2211.05102, 2022.

J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoﬀmann, F. Song, J. Aslanides, S. Henderson, R. Ring,
S. Young, et al. Mở rộng quy mô các mô hình ngôn ngữ: Phương pháp, phân tích & hiểu biết từ việc huấn luyện gopher. arXiv
preprint arXiv:2112.11446, 2021.

V. Sanh, L. Debut, J. Chaumond, và T. Wolf. Distilbert, một phiên bản chưng cất của bert: nhỏ hơn, nhanh hơn,
rẻ hơn và nhẹ hơn. arXiv preprint arXiv:1910.01108, 2019.

N. Shazeer. Giải mã transformer nhanh: Một write-head là tất cả những gì bạn cần. CoRR, abs/1911.02150, 2019.
URL http://arxiv.org/abs/1911.02150.

M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, và B. Catanzaro. Megatron-lm: Huấn luyện
các mô hình ngôn ngữ multi-billion tham số sử dụng song song hóa mô hình. arXiv preprint arXiv:1909.08053,
2019.

Y. Song, C. Meng, R. Liao, và S. Ermon. Tăng tốc tính toán feedforward qua giải
phương trình phi tuyến song song. Trong M. Meila và T. Zhang, editors, Proceedings of the 38th International Conference
on Machine Learning, volume 139 of Proceedings of Machine Learning Research, trang 9791–9800.
PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/song21a.html.

M. Stern, N. Shazeer, và J. Uszkoreit. Giải mã song song theo khối cho các mô hình tự hồi quy sâu.
CoRR, abs/1811.03115, 2018. URL http://arxiv.org/abs/1811.03115.

A. Wiggers và E. Hoogeboom. Lấy mẫu dự đoán với các mô hình tự hồi quy dự báo. Trong H. D.
III và A. Singh, editors, Proceedings of the 37th International Conference on Machine Learning,
volume 119 of Proceedings of Machine Learning Research, trang 10260–10269. PMLR, 13–18 Jul
2020. URL https://proceedings.mlr.press/v119/wiggers20a.html.

--- TRANG 10 ---
Tăng tốc Giải mã Mô hình Ngôn ngữ Lớn với Lấy mẫu Suy đoán

Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, và Y. He. Zeroquant: Lượng tử hóa sau huấn luyện
hiệu quả và có thể chi trả cho các transformer quy mô lớn. arXiv preprint arXiv:2206.01861, 2022.

Tài liệu Bổ sung

Đóng góp của Tác giả
• Đề xuất ban đầu: Charlie Chen, John Jumper và Geoﬀrey Irving
• Triển khai, Tối ưu hóa và Mở rộng quy mô ban đầu: Charlie Chen
• Sơ đồ Lấy mẫu Từ chối Được sửa đổi: John Jumper
• Cải tiến Kỹ thuật: Jean-Baptiste Lespiau và Charlie Chen
• Thí nghiệm: Charlie Chen, Sebastian Borgeaud và Laurent Sifre
• Bản thảo Bản thảo: Charlie Chen và Sebastian Borgeaud
• Phản hồi Bản thảo: Laurent Sifre, Geoﬀrey Irving và John Jumper

Lời cảm ơn
Chúng tôi muốn cảm ơn Oriol Vinyals và Koray Kavukcuoglu cho lời khuyên tử tế và khả năng lãnh đạo của các bạn. Chúng tôi cũng
muốn cảm ơn Evan Senter cho phản hồi bổ sung của bạn về bản thảo và Amelia Glaese cho
sự hỗ trợ của bạn trong việc điều hướng quá trình xuất bản. Cuối cùng, chúng tôi muốn cảm ơn Blake Hechtman, Berkin
Ilbeyi cho lời khuyên quý giá của các bạn về XLA và Nikolai Grigoriev cho các cuộc thảo luận của chúng tôi về các thủ thuật khác nhau
có thể được áp dụng cho kiến trúc transformer.

Siêu tham số

Bảng 2 | Siêu tham số cho mô hình nháp
Mô hình | 𝑑model | Heads | Layers | Params
Đích (Chinchilla) | 8192 | 64 | 80 | 70B
Nháp | 6144 | 48 | 8 | 4B

Chứng minh

Định lý 1 (Lấy mẫu Từ chối Được sửa đổi khôi phục phân phối đích). Cho các phân phối rời rạc
𝑞, 𝑝 và một mẫu nháp duy nhất ˜𝑥𝑝, gọi 𝑋 là mẫu cuối cùng. Để 𝑋=𝑥 đúng, chúng ta phải
hoặc lấy mẫu ˜𝑥=𝑥 và sau đó chấp nhận nó, hoặc lấy mẫu lại nó sau khi ˜𝑥 (của bất kỳ giá trị nào) bị từ chối. Do đó:

ℙ¹𝑋=𝑥º
=ℙ¹˜𝑥=𝑥ºℙ¹˜𝑥 được chấp nhận|˜𝑥=𝑥º+ℙ¹˜𝑥 bị từ chốiºℙ¹𝑋=𝑥|˜𝑥 bị từ chối]

Cho hạng đầu tiên, chúng ta áp dụng quy tắc chấp nhận:
ℙ¹˜𝑥=𝑥ºℙ¹˜𝑥 được chấp nhận|˜𝑥=𝑥º
=𝑝¹𝑥ºmin(1, 𝑞¹𝑥º/𝑝¹𝑥º)

--- TRANG 11 ---
Tăng tốc Giải mã Mô hình Ngôn ngữ Lớn với Lấy mẫu Suy đoán

=min¹𝑝¹𝑥º𝑞¹𝑥ºº

Cho hạng có điều kiện thứ hai, chúng ta áp dụng quy tắc lấy mẫu lại:
ℙ¹𝑋=𝑥|˜𝑥 bị từ chối]=¹𝑞¹𝑥º𝑝¹𝑥ºº¸

Trong đó ¹º¸ biểu thị:
¹𝑓¹𝑥ºº¸=max¹0 𝑓¹𝑥ººÍ𝑥max¹0 𝑓¹𝑥ºº

Cuối cùng, chúng ta tính xác suất từ chối:
ℙ¹˜𝑥 bị từ chối]=1ℙ¹˜𝑥 được chấp nhận]
=1∑︁𝑥0ℙ¹𝑋=𝑥0|˜𝑥 được chấp nhận]
=1∑︁𝑥0min¹𝑝¹𝑥0º𝑞¹𝑥0ºº
=∑︁𝑥0max¹0𝑞¹𝑥0º𝑝¹𝑥0ºº
=∑︁𝑥0𝑞¹𝑥0ºmin¹𝑝¹𝑥0º𝑞¹𝑥0ºº
=∑︁𝑥0max¹0𝑞¹𝑥0º𝑝¹𝑥0ºº

Điều này bằng mẫu số của ¹𝑞¹𝑥º𝑝¹𝑥ºº¸, vì vậy:
ℙ¹˜𝑥 bị từ chối]ℙ¹𝑋=𝑥|˜𝑥 bị từ chối]=max¹0𝑞¹𝑥º𝑝¹𝑥ºº

Do đó:
ℙ¹𝑋=𝑥º
=min¹𝑝¹𝑥º𝑞¹𝑥ºº+max¹0𝑞¹𝑥º��¹𝑥ºº
=𝑞¹𝑥º

và chúng ta đã khôi phục đích mong muốn.
