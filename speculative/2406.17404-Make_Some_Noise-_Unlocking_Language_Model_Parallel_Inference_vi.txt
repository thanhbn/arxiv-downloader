Làm Một Số Tiếng Ồn: Mở Khóa Khả Năng Suy Luận Song Song của Mô Hình Ngôn Ngữ thông qua Huấn Luyện Có Nhiễu

Yixuan Wang1,†, Xianzhen Luo1,†, Fuxuan Wei1, Yijun Liu1, Qingfu Zhu1,*,
Xuanyu Zhang2, Qing Yang2, Dongliang Xu2, Wanxiang Che1

1Viện Công nghệ Harbin, Harbin, Trung Quốc
2Công ty TNHH Khoa học Công nghệ Du Xiaoman (Bắc Kinh)

Tóm tắt

Các phương pháp giải mã suy đoán hiện tại thường yêu cầu cấu trúc mô hình bổ sung và các quy trình huấn luyện để hỗ trợ mô hình tạo ra token dự thảo. Điều này làm cho việc di chuyển các phương pháp tăng tốc sang mô hình mới tốn kém hơn và đòi hỏi nhiều bộ nhớ thiết bị hơn. Để giải quyết vấn đề này, chúng tôi đề xuất khung huấn luyện Make Some Noise (MSN) như một sự thay thế cho giai đoạn tinh chỉnh có giám sát của mô hình ngôn ngữ lớn. Phương pháp huấn luyện này đơn giản chỉ giới thiệu một số nhiễu ở đầu vào để mô hình học tác vụ khử nhiễu. Nó tăng cường đáng kể khả năng giải mã song song của mô hình mà không ảnh hưởng đến khả năng tác vụ ban đầu. Ngoài ra, chúng tôi đề xuất chiến lược giải mã Jacobi tăng cường truy xuất dựa trên cây (TR-Jacobi) để cải thiện thêm tốc độ suy luận của các mô hình MSN. Các thí nghiệm trong cả lĩnh vực tổng quát và mã đã cho thấy MSN có thể cải thiện tốc độ suy luận 2.3-2.7 lần mà không làm giảm hiệu suất mô hình. Mô hình MSN cũng đạt được tỷ lệ tăng tốc tương đương với mô hình SOTA có cấu trúc mô hình bổ sung trên Spec-Bench.

1 Giới thiệu

Các mô hình ngôn ngữ lớn (LLM) được đại diện bởi GPT-4 (OpenAI et al., 2024) và LLaMA (Touvron et al., 2023) đã tạo ra những đột phá lớn cho trí tuệ nhân tạo (Koco´n et al., 2023). Tuy nhiên, LLM gặp phải độ trễ suy luận cao do mô hình giải mã tự hồi quy (AR), điều này ràng buộc mô hình chỉ tạo ra một token mỗi bước giải mã. Điều này hạn chế đáng kể các ứng dụng của LLM khi cần phản hồi dài.

Để giải quyết nút thắt do AR gây ra, giải mã suy đoán (Leviathan et al., 2023; Chen et al., 2023) được đề xuất để có được nhiều hơn một token trong một bước giải mã. Nó đầu tiên đoán các token dự thảo nhiều bước và sau đó xác minh chúng đồng thời trong một lần forward của mô hình. Khi bất kỳ token dự thảo nào được chấp nhận, nó có thể tăng tốc hiệu quả quá trình suy luận. Chen et al. (2023) sử dụng một LLM tương đối nhỏ để tạo ra các token dự thảo nhiều bước và xác minh chúng song song trên LLM mục tiêu. Medusa (Cai et al., 2024) mở rộng và huấn luyện nhiều đầu mô hình ngôn ngữ cho các mô hình hiện có để dự đoán các token dự thảo sau này. Nó đạt được tăng tốc suy luận đáng kể thông qua xác minh hiệu quả sử dụng chú ý cây. BiTA (Lin et al., 2024a) tận dụng đầy đủ khả năng của chính LLM thông qua thiết kế hiệu quả tham số cho phép mô hình tạo ra token dự thảo dựa trên các token đặc biệt có thể huấn luyện. Kou et al. (2024) đề xuất một phương pháp hậu huấn luyện dựa trên các quỹ đạo Jacobi được xây dựng có thể tăng tốc khả năng giải mã Jacobi của chính mô hình.

Mặc dù các phương pháp trên cải thiện hiệu quả suy luận của mô hình ở một mức độ nhất định, vẫn còn một số vấn đề cần giải quyết như được hiển thị trong Hình 1. (1) Cấu trúc Bổ sung. Hầu hết các phương pháp giải mã suy đoán hiện tại phụ thuộc nhiều vào các cấu trúc mô hình bổ sung để thực hiện dự đoán token dự thảo (ví dụ: mô hình riêng biệt, đầu mô hình ngôn ngữ, lời nhắc có thể huấn luyện, v.v.). Trong trường hợp của Medusa, ví dụ, nó thêm 1.6B tham số (5 đầu medusa bổ sung) vào mô hình mục tiêu 7B, điều này chắc chắn sẽ tăng yêu cầu bộ nhớ để suy luận mô hình. (2) Hậu Huấn luyện Riêng biệt. Các phương pháp giải mã suy đoán dựa trên mô hình hiện tại được huấn luyện sau giai đoạn tinh chỉnh có giám sát (SFT) của LLM để có được khả năng tăng tốc. Quá trình này thường yêu cầu thiết lập mô hình phức tạp hoặc xây dựng dữ liệu tốn thời gian, và một số phương pháp thậm chí mất một phần khả năng tác vụ ban đầu của mô hình. Việc huấn luyện riêng biệt khả năng tác vụ và tăng tốc dẫn đến một cách tiếp cận quá phức tạp mà không dễ triển khai.

Để giải quyết vấn đề trên, chúng tôi đề xuất một khung huấn luyện có nhiễu Make Some Noise (MSN) như một sự thay thế cho SFT, cho phép mô hình có được cả khả năng liên quan đến tác vụ cũng như khả năng tăng tốc ở cùng một giai đoạn mà không cần các cấu trúc và giai đoạn huấn luyện bổ sung. Cụ thể, chúng tôi coi quá trình giải mã Jacobi (Santilli et al., 2023) như một quá trình khử nhiễu, và cải thiện khả năng khử nhiễu của mô hình bằng cách bao gồm một tác vụ khử nhiễu mô hình ngôn ngữ nhân quả trong giai đoạn SFT. Vì giai đoạn SFT hầu như là một khía cạnh cần thiết của các ứng dụng LLM, cách tiếp cận đề xuất của chúng tôi có thể được hiểu như một bữa trưa miễn phí cho khả năng suy luận song song của LLM.

Trong giai đoạn suy luận, chúng tôi sử dụng giải mã Jacobi để đạt được tăng tốc suy luận thông qua các lần lặp lại của các token nhiễu ngẫu nhiên cũng như xác minh. Bên cạnh đó, để giảm nhẹ vấn đề khởi động lạnh của giải mã Jacobi và giảm thiểu tác động của nhiễu ban đầu ngẫu nhiên, chúng tôi cũng đề xuất phương pháp giải mã Jacobi tăng cường truy xuất dựa trên cây (TR-Jacobi), có thể cải thiện hiệu quả tỷ lệ tăng tốc.

Chúng tôi đã tiến hành các thí nghiệm chi tiết trong lĩnh vực tổng quát và mã. Kết quả cho thấy khung huấn luyện MSN có thể cải thiện đáng kể khả năng khử nhiễu của mô hình mà không ảnh hưởng đến hiệu suất của mô hình SFT ban đầu, từ đó đạt được hiệu ứng tăng tốc suy luận 2.3-2.7 lần. Ngoài ra, chúng tôi thực hiện đánh giá chi tiết trên Specbench, được thiết kế đặc biệt cho giải mã suy đoán. Như một phương pháp giải mã suy đoán không có cấu trúc bổ sung và huấn luyện, tỷ lệ tăng tốc của mô hình MSN dưới chiến lược giải mã TR-Jacobi vượt trội đáng kể so với các phương pháp không có cấu trúc bổ sung khác và có tỷ lệ tăng tốc tương đương với mô hình SOTA có cấu trúc mô hình và huấn luyện bổ sung.

Các đóng góp chính của chúng tôi có thể được tóm tắt như sau:

• Chúng tôi đề xuất một khung huấn luyện mới Make Some Noise (MSN) như một sự thay thế cho SFT, có thể mở khóa khả năng giải mã song song của mô hình thông qua tác vụ khử nhiễu.

• Chúng tôi đề xuất một phương pháp giải mã tăng cường truy xuất dựa trên cây giúp cải thiện hiệu quả tốc độ suy luận của các mô hình MSN dưới các nút thắt bộ nhớ.

• Các thí nghiệm cho thấy huấn luyện MSN cho phép mô hình có tỷ lệ tăng tốc tương đương với phương pháp SOTA mà không mất đáng kể hiệu suất tác vụ.

2 Công trình Liên quan

2.1 Giải mã Jacobi

Giải mã Jacobi (Santilli et al., 2023) coi việc giải mã tham lam của các tác vụ tạo sinh như giải phương trình:

y1 = arg max P_theta(y1|x)
y2 = arg max P_theta(y2|y1, x)
...
ym = arg max P_theta(ym|y1:m−1, x) (1)

Giải mã tự hồi quy giải các phương trình từ đầu đến cuối dựa trên đầu vào x cho trước, thay thế dần dần các biến đã giải. Ngược lại, giải mã Jacobi dựa vào các phương pháp lặp điểm cố định Jacobi và Gauss-Seidel (GS) (Ortega và Rheinboldt, 2000) để giải Phương trình 1 song song. Cụ thể, nó truyền một dãy khởi tạo có độ dài m vào mô hình để tạo sinh lặp lại cho đến khi dãy hội tụ đến một điểm cố định. Giải mã Jacobi mong đợi giải phương trình trong ít hơn m lần lặp, nhưng thực tế các mô hình hiện có hoạt động kém dưới chiến lược giải mã này do thiếu khả năng khử nhiễu. Kou et al. (2024) cải thiện đáng kể hiệu quả của giải mã Jacobi bằng cách xây dựng dữ liệu quỹ đạo trong quá trình giải mã Jacobi và thực hiện huấn luyện nhất quán.

2.2 Giải mã Suy đoán

Giải mã suy đoán có thể tăng hiệu quả tốc độ giải mã mà không thay đổi chất lượng đầu ra bằng cách đoán và xác minh đầu ra của mô hình ngôn ngữ tự hồi quy song song. Công việc chính hiện tại đã tập trung vào việc điều tra cách hoàn thành việc tạo token dự thảo một cách hiệu quả. Stern et al. (2018) hoàn thành việc dự đoán token dự thảo với các cấu trúc mô hình bổ sung. Chen et al. (2023) tạo ra các token dự thảo đáng tin cậy bằng một mô hình nhỏ bên ngoài. Cai et al. (2024) huấn luyện nhiều đầu cho mô hình LLM để dự đoán token dự thảo dựa trên công việc trước đó. Li et al. (2024) tận dụng đầy đủ thông tin trong lớp ẩn để hoàn thành dự đoán chất lượng cao của các mô hình dự thảo với một lớp giải mã riêng biệt. Lin et al. (2024a) cho phép mô hình dự đoán token dự thảo bằng cách huấn luyện các token tiền tố.

Ngoài ra, có một số phương pháp giải mã suy đoán không yêu cầu huấn luyện. LLMA (Yang et al., 2023) đạt được tăng tốc 2x ∼3x trên các tác vụ như đối thoại bằng cách truy xuất các đoạn văn bản từ văn bản tham khảo. Fu et al. (2024) thực hiện xác minh hiệu quả hơn bằng cách thu thập các đoạn n-gram được tạo ra trong quá trình giải mã Jacobi như token dự thảo. Saxena (2023) đạt được tăng tốc trong các lĩnh vực cụ thể đơn giản bằng cách truy xuất token dự thảo từ lời nhắc phía trước. REST (He et al., 2023) cho phép tạo token dự thảo dạng plug-in bằng cách truy xuất cơ sở dữ liệu kiến thức được xây dựng. Zhao et al. (2024) đề xuất Ouroboros kết hợp ưu điểm của cả hai phương pháp truy xuất và mô hình dự thảo. Nó sử dụng phương pháp truy xuất để tăng cường thêm độ dài tạo sinh của mô hình dự thảo, đạt được tỷ lệ tăng tốc đáng kể.

Để cải thiện thêm hiệu quả xác minh của token dự thảo, Miao et al. (2023) đề xuất xác minh nhiều đường dẫn như một cây token cùng một lúc bằng cách thiết kế ma trận mặt nạ chú ý. Ngày nay, xác minh cây token đã trở thành một kỹ thuật được sử dụng rộng rãi để cải thiện hiệu quả xác minh của giải mã suy đoán.

3 Phương pháp

3.1 Tổng quan

Ý tưởng cốt lõi của chúng tôi là coi giải mã song song như một loại tạo sinh văn bản dưới nhiễu, tương tự như giải mã Jacobi. Điều này yêu cầu mô hình có khả năng tạo ra token chính xác tương ứng bất chấp token nhiễu, điều này không thể thực hiện được với việc huấn luyện teacher-forcing hiện tại (Bachmann và Nagarajan, 2024).

Được truyền cảm hứng từ các công trình liên quan giải quyết thiên lệch tiếp xúc (Bengio et al., 2015; Zhang et al., 2019), chúng tôi chọn tăng cường khả năng khử nhiễu của mô hình bằng cách thêm một số nhiễu ở mức token vào dãy đầu vào trong giai đoạn SFT của LLM. Như được hiển thị trong Hình 2, chúng tôi kết hợp một tác vụ khử nhiễu mô hình ngôn ngữ nhân quả trong giai đoạn huấn luyện để đảm bảo rằng mô hình có khả năng tạo sinh mạnh mẽ. Trong giai đoạn suy luận, chúng tôi sử dụng nhiễu ngẫu nhiên được nối ở cuối dãy, và tiếp tục tạo sinh và xác minh token dự thảo bằng khử nhiễu lặp lại, phù hợp với quá trình giải mã Jacobi.

Để đảm bảo rằng khả năng khử nhiễu được cải thiện mà không ảnh hưởng đến việc thu được khả năng tác vụ, chúng tôi xây dựng phương pháp theo nội dung và vị trí của đoạn nhiễu (Phần 3.2). Ngoài ra, để tăng cường thêm hiệu quả xác thực của mô hình, chúng tôi đề xuất chiến lược giải mã Jacobi tăng cường truy xuất dựa trên cây (TR-Jacobi) (Phần 3.3).

3.2 Khung Huấn luyện Có nhiễu

Teacher-forcing đã được áp dụng rộng rãi như một phương pháp huấn luyện hiệu quả bởi các mô hình tạo sinh chủ đạo. Nó huấn luyện mô hình với nhãn tại thời điểm t như đầu vào tại thời điểm t+1, có thể tăng tốc sự hội tụ của mô hình. Đối với dãy X=x0x1...xn, hàm mất mát của một mô hình tự hồi quy truyền thống có thể được công thức hóa như sau:

Loss_AR = ∑(i=0 to n) −log P(Xi|X<i; theta) (2)

trong đó theta là tập hợp các tham số của mô hình ngôn ngữ và X<i đại diện cho dãy con x0x1...xi−1. Mô hình được huấn luyện để tạo ra kết quả dựa trên các nhãn chính xác, do đó mỗi bước tạo sinh yêu cầu kết quả được tạo ra ở bước trước đó.

Để trang bị cho mô hình khả năng khử nhiễu, chúng tôi giới thiệu token nhiễu nhân quả trong giai đoạn huấn luyện. Như được hiển thị trong Hình 2, chúng tôi chèn một số token nhiễu vào đầu vào để phá vỡ hạn chế rằng teacher-forcing luôn lấy nhãn vàng làm đầu vào. Để giảm thiểu tác động của nhiễu đến huấn luyện, chúng tôi chỉ thay thế một đoạn ngắn bằng token nhiễu trong mỗi mẫu. Mẫu nhiễu có thể được biểu diễn như X̂=x0x1...x̂i...x̂j...xn, trong đó x̂i...x̂j đại diện cho đoạn nhiễu. Hàm mất mát của phương pháp huấn luyện có nhiễu có thể được công thức hóa như sau:

Loss_MSN = ∑(i=0 to n) −log P(Xi|X̂<i; theta) (3)

trong đó Xi đại diện cho token của nhãn vàng và X̂<i đại diện cho dãy con với token nhiễu. Cần lưu ý rằng mặc dù đầu vào chứa một phần token nhiễu, mục tiêu của mô hình để học vẫn là các nhãn chính xác. Việc huấn luyện với nhiễu như vậy có thể mở khóa khả năng giải mã song song của mô hình ở một mức độ nhất định. Để giảm thêm tác động của nhiễu đến tác vụ SFT, chúng tôi điều tra nội dung của nhiễu và vị trí của nhiễu.

Nội dung của Đoạn Nhiễu. Động lực chính cho việc huấn luyện có nhiễu là trang bị cho mô hình khả năng tạo ra token chính xác bất chấp đầu vào có nhiễu, điều này được đạt được thông qua mất mát của các đoạn nhiễu. Tuy nhiên, mặt nạ chú ý nhân quả của LLM dẫn đến khả năng các token nhiễu có thể có tác động đến các mục tiêu huấn luyện tự hồi quy sau này. Để giảm thiểu tác động, chúng tôi chọn nhiễu phía trước làm nội dung chính của các đoạn. Cụ thể, chúng tôi lấy mẫu ngẫu nhiên các token phía trước làm token nhiễu hiện tại, có thể được công thức hóa như sau:

x̂i = random_sample(X<i) (4)

trong đó X<i đại diện cho dãy con phía trước của xi. So với nhiễu ngẫu nhiên, nhiễu phía trước có ít tác động hơn đến các token tiếp theo. Ngoài ra, việc khử nhiễu các token nhiễu phía trước thách thức hơn vì chúng liên quan nhiều hơn đến ngữ cảnh.

Vị trí của Đoạn Nhiễu. Được truyền cảm hứng từ Lin et al. (2024b), chúng tôi đã thử hai phương pháp chọn vị trí nhiễu, chọn ngẫu nhiên và chọn dựa trên PPL. Các thí nghiệm (xem Phụ lục A để biết chi tiết) đã phát hiện rằng cả hai phương pháp đều không có tác động đáng kể đến hiệu suất tác vụ mô hình và tỷ lệ tăng tốc tương tự nhau. Chúng tôi suy đoán rằng các đoạn nhiễu của chúng tôi (ít hơn 10) có thể tương đối ngắn trên các bộ dữ liệu SFT với độ dài trung bình 600 hoặc hơn, và không có tác động đến việc huấn luyện bản thân mô hình. Do đó chúng tôi chọn phương pháp nhiễu thay thế ngẫu nhiên đơn giản hơn.

Trong thực tế, ở mỗi bước huấn luyện, chúng tôi chỉ thay thế một đoạn ngẫu nhiên có độ dài cố định bằng nhiễu phía trước cho phản hồi của mỗi mẫu.

3.3 Giải mã TR-Jacobi

Giải mã Jacobi Dựa trên Cây. Như đã thảo luận trong Phần 2.2, việc sử dụng xác minh cây token đã trở thành một phương pháp xác minh phổ biến trong giải mã suy đoán. Trong bài báo này, chúng tôi cũng muốn cải thiện hiệu quả của giải mã Jacobi bằng cách xây dựng nhiều dãy ứng viên. Giống như Medusa (Cai et al., 2024), chúng tôi đã chọn theo phương pháp heuristic một cây thưa thớt làm mẫu chú ý cây của chúng tôi (xem Phụ lục B để biết chi tiết). Ở đầu của việc tạo sinh, chúng tôi khởi tạo tất cả các nút của cây bằng nhiễu phía trước để bắt đầu giải mã Jacobi dựa trên cây. Như được hiển thị trong Hình 3, đối với mỗi quá trình forward, mỗi đường dẫn thực hiện một quá trình giải mã Jacobi thông thường qua chú ý cây. Sau đó chúng tôi chọn đường dẫn có độ dài chấp nhận dài nhất và tiếp tục điền các nút cây xác thực cho vòng tiếp theo dựa trên các dự đoán tiếp theo của đường dẫn. Quan trọng là chúng tôi sử dụng các token nhiễu phía trước để điền các vị trí còn lại trong cây xác thực, giống như giải mã Jacobi thông thường.

Giải mã Jacob Tăng cường Truy xuất. Ngoài ra, đối với các phương pháp thiết kế dự đoán token dự thảo ở phía đầu vào của mô hình (ví dụ: Jacobi, BiTA, v.v.), khởi động lạnh cũng là một vấn đề quan trọng cần được giải quyết. Khi tất cả token dự thảo của đầu vào này được chấp nhận, mô hình sẽ không có cách nào để có được token dự thảo mới trong vòng này. Các phương pháp hiện tại giảm nhẹ vấn đề này bằng cách nối thêm nhiều token sau đó, nhưng phát sinh chi phí suy luận bổ sung.

Để tránh bắt đầu xác thực từ nhiễu hoàn toàn ngẫu nhiên trong trường hợp này, chúng tôi xem xét kết hợp token dự thảo dựa trên truy xuất và tạo sinh token dự thảo dựa trên mô hình.

Cụ thể, chúng tôi đặt một đường dẫn truy xuất trong cây token để giữ các token ứng viên thu được bằng cách truy xuất các token trước đó. Để truy xuất, chúng tôi sử dụng một phương pháp đơn giản và hiệu quả gọi là giải mã prompt lookahead (Saxena, 2023) để có được token dự thảo với cùng một khởi đầu trực tiếp từ các token phía trước hiện tại để xác minh, điều này tăng tốc đáng kể suy luận trên các tác vụ như tóm tắt. Các thí nghiệm phân tích trong Phần 5.3 chứng minh rằng việc kết hợp thông tin được truy xuất có hiệu quả trong việc cải thiện tỷ lệ tăng tốc của mô hình trong các lĩnh vực cụ thể. Ngoài ra, giải mã Jacobi có thể giảm nhẹ các vấn đề cố hữu của các phương pháp truy xuất trong các lĩnh vực như dịch thuật.

4 Thí nghiệm

4.1 Thiết lập Thí nghiệm

Bộ dữ liệu. Để xác minh rằng việc huấn luyện SFT Make Some Noise (MSN) đề xuất có thể mang lại tăng tốc suy luận mà không làm giảm hiệu suất mô hình, chúng tôi đã xây dựng các bộ dữ liệu SFT trong lĩnh vực tổng quát và mã, tương ứng.

Đối với lĩnh vực tổng quát, chúng tôi tuân theo thiết lập của Lin et al. (2024a) để xây dựng một bộ dữ liệu huấn luyện chứa 190k mẫu từ LIMA (Zhou et al., 2024), Alpaca-GPT4 (Peng et al., 2023), CodeAlpaca (Chaudhary, 2023), OpenPlatypus (Lee et al., 2023) và CIP (Palla, 2023). Lưu ý rằng chúng tôi chỉ sử dụng 100k mẫu từ CIP. Đối với lĩnh vực mã, chúng tôi áp dụng tổng cộng 185k mẫu từ Magicoder-OSS (Wei et al., 2023) và Evol-CodeAlpaca (Luo et al., 2023) làm bộ dữ liệu huấn luyện, được sử dụng rộng rãi trong tác vụ tổng hợp chương trình.

Cài đặt Huấn luyện. Để đánh giá phương pháp đề xuất một cách toàn diện, chúng tôi chọn LLama3-8B-Base (Touvron et al., 2023) và DeepseekCoder-6.7b-Base (Guo et al., 2024) làm mô hình nền tảng cho lĩnh vực tổng quát và mã, tương ứng. Các cài đặt huấn luyện cho MSN được căn chỉnh với baseline (SFT), duy trì độ dài dãy 2048 token, kích thước batch 512, và epoch huấn luyện 4. Tinh chỉnh tham số đầy đủ được thực hiện trên hai máy chủ, mỗi máy được trang bị 8 GPU A100-80GB, sử dụng độ chính xác bf16. Chúng tôi xác định rằng độ dài đoạn nhiễu 4 là tối ưu cho việc thay thế nhiễu động cho mỗi mẫu.

Cài đặt Đánh giá. Trong bài báo này, chúng tôi tiến hành thí nghiệm về hiệu suất tác vụ và hiệu suất tăng tốc của MSN, tương ứng. Đối với hiệu suất tác vụ, chúng tôi sử dụng MT-bench (Zheng et al., 2024) và MMLU (Hendrycks et al., 2020) trong lĩnh vực tổng quát, các benchmark HumanEval (Chen et al., 2021) và MBPP (Austin et al., 2021) trong lĩnh vực mã để đánh giá. Evalplus (Liu et al., 2023), cung cấp các test case bổ sung cho các vấn đề trong HumanEval và MBPP, cũng được bao gồm. Đối với hiệu suất tăng tốc, chúng tôi thực hiện đánh giá tăng tốc của các phương pháp giải mã song song đề xuất trên Spec-Bench (Xia et al., 2024). Benchmark này chứa dữ liệu từ nhiều lĩnh vực và cung cấp so sánh công bằng với các phương pháp tăng tốc hiện có. Theo công việc trước đó, tất cả các thí nghiệm liên quan đến tốc độ được thực hiện trên một thiết bị A100-80G duy nhất với kích thước batch là 1. Đối với mô hình MSN của chúng tôi, độ dài token dự thảo trong quá trình suy luận phù hợp với độ dài đoạn nhiễu trong quá trình huấn luyện, là 4.

4.2 So sánh với SFT

Baseline. Đầu tiên chúng tôi xác thực tác động của khung huấn luyện MSN đề xuất đối với hiệu suất của các tác vụ mô hình trong lĩnh vực tổng quát và mã. Tinh chỉnh có giám sát tiêu chuẩn (SFT) được chọn làm phương pháp baseline để so sánh. Cụ thể, chúng tôi thực hiện SFT cụ thể theo lĩnh vực và huấn luyện có nhiễu dựa trên cùng mô hình cơ sở và so sánh hiệu suất của cả hai trên các tác vụ downstream.

Kết quả. Metric trong Bảng 1 đại diện cho hiệu suất tác vụ của mỗi mô hình. Không có mất mát hiệu suất đáng kể của mô hình được huấn luyện bởi MSN trên tác vụ downstream so với SFT. Hơn nữa, mô hình MSN thậm chí mang lại một sự cải thiện hiệu suất nhẹ trong cả hai lĩnh vực. Sự tăng cường trong lĩnh vực mã đặc biệt đáng chú ý, cho rằng việc đánh giá các chương trình được tạo ra nghiêm ngặt hơn việc đánh giá cuộc trò chuyện. Các chương trình phải được định dạng chính xác và vượt qua tất cả các test case để được coi là thành công. Điều này cho thấy MSN không làm tổn hại mô hình để có được khả năng trong giai đoạn SFT. Phân tích của chúng tôi cho thấy sự tăng này đến từ thực tế rằng nhiễu giảm nhẹ các tác động tiêu cực của việc huấn luyện teacher forcing trên mô hình ở một mức độ nhất định. Tác vụ khử nhiễu nhân quả buộc mô hình tập trung vào các token xa hơn khi dự đoán token vị trí hiện tại vì đầu vào hiện tại có nhiễu. Chúng tôi cũng tiến hành một số thí nghiệm so sánh SFT và MSN với các mô hình cơ sở khác nhau trong lĩnh vực mã, có thể được tìm thấy trong Phụ lục C.

Ngoài điều này, chúng tôi kiểm tra ngắn gọn hiệu ứng tăng tốc của phương pháp MSN trên chiến lược giải mã giống Jacobi. Chúng ta có thể thấy rằng việc huấn luyện có mục tiêu về khả năng khử nhiễu của mô hình cải thiện đáng kể tỷ lệ tăng tốc của giải mã Jacobi trong các lĩnh vực khác nhau. TR-Jacobi đề xuất của chúng tôi cải thiện thêm tỷ lệ tăng tốc bằng cách xác minh nhiều đường dẫn đồng thời.

4.3 So sánh với Các Phương pháp Giải mã Suy đoán Khác

Baseline. Để so sánh thêm MSN với các phương pháp giải mã suy đoán hiện có, chúng tôi tiến hành đánh giá trên Spec-Bench (Xia et al., 2024). Chúng tôi chọn cả các phương pháp suy đoán không bao gồm cấu trúc bổ sung (Jacobi, LookAhead, PLD) và những phương pháp yêu cầu cấu trúc bổ sung (Medusa2, EAGLE) để so sánh. EAGLE và Medusa2 được hậu huấn luyện trên Vicuna-7b-v1.3 (Chiang et al., 2023), đã là một mô hình hậu SFT. Vì MSN đề xuất được thực hiện trong giai đoạn SFT, chúng tôi cần thực hiện MSN SFT trên một mô hình cơ sở. Do đó, chúng tôi tiến hành MSN trên LLaMA3-8B-Base và thực hiện đánh giá tăng tốc trên hai mô hình nền tảng khác nhau để so sánh thô tỷ lệ tăng tốc dựa trên các throughput tự hồi quy (AR) khác nhau.

Kết quả. Kết quả thí nghiệm tăng tốc tổng thể được hiển thị trong Bảng 2. Sau việc huấn luyện cụ thể về khả năng khử nhiễu, mô hình MSN cải thiện tỷ lệ tăng tốc trên tất cả các chiến lược giải mã giống Jacobi. Đối với LookAhead, khả năng khử nhiễu có thể tạo ra các n-gram không mạch lạc, có thể dẫn đến cải thiện tương đối thấp. Đối với cả tỷ lệ tăng tốc giải mã Jacobi và TR-Jacobi, huấn luyện có nhiễu mang lại những cải thiện đáng kể.

Tỷ lệ tăng tốc của mô hình MSN dưới giải mã TR-Jacobi có tính cạnh tranh với các phương pháp khác. Như một phương pháp không có giai đoạn huấn luyện bổ sung và không có cấu trúc mô hình bổ sung, phương pháp tăng tốc đề xuất cũng có thể so sánh với các mô hình có cấu trúc bổ sung. Có thể nói một cách công bằng rằng MSN là một cách nhẹ và hiệu quả để đạt được tăng tốc suy luận có thể so sánh với các mô hình SOTA hiện có trong khi cải thiện độ mạnh mẽ của mô hình.

5 Thảo luận

5.1 Tác động của Độ dài Đoạn Nhiễu

Độ dài span bao gồm độ dài của đoạn nhiễu trong quá trình huấn luyện và độ dài của dãy dự thảo được thêm vào trong quá trình suy luận. Độ dài span huấn luyện ảnh hưởng đến độ khó của việc học mô hình từ các mẫu, trong khi độ dài span trong quá trình suy luận tác động đến cả độ dài hit và độ trễ hoạt động suy đoán.

Độ dài Đoạn Nhiễu Huấn luyện. Độ dài đoạn nhiễu huấn luyện đề cập đến số lượng token nhiễu. Nếu độ dài quá ngắn, khả năng khử nhiễu của mô hình có thể bị giảm, dẫn đến tăng tốc hạn chế trong quá trình suy luận. Ngược lại, nếu độ dài quá dài, nó tăng đáng kể độ khó của việc khử nhiễu, ảnh hưởng đến hiểu biết của mô hình về mẫu và do đó làm tổn hại hiệu suất tác vụ của nó. Để quan sát tác động của các độ dài span huấn luyện khác nhau, chúng tôi thí nghiệm với độ dài span 1, 4, và 8 trên Deepseek Coder và hiệu suất tác vụ và tăng tốc được hiển thị trong Bảng 3. Nó chứng minh rằng độ dài 1 mang lại hiệu suất tác vụ cao nhưng cung cấp tăng tốc tối thiểu. Độ dài 8 cung cấp tăng tốc đáng kể nhưng với chi phí là suy giảm hiệu suất tác vụ đáng kể. Độ dài 4 đạt được tăng tốc cao nhất với tác động thấp hơn đến hiệu suất.

Độ dài Đoạn Nhiễu Suy luận. Độ dài đoạn nhiễu suy luận đại diện cho số lượng token dự thảo cho lần lặp Jacobi, đây cũng là số lần tối đa mà token có thể được khử nhiễu lặp lại. Chúng tôi thực hiện thí nghiệm suy luận song song với độ dài đoạn nhiễu suy luận khác nhau cho các mô hình được huấn luyện với độ dài đoạn nhiễu huấn luyện khác nhau (mô tả ở trên). Chúng tôi thấy rằng mô hình có thể tổng quát hóa từ kích thước đoạn nhiễu huấn luyện nhỏ hơn sang kích thước đoạn nhiễu suy luận lớn hơn. Điều này cho thấy rằng mặc dù chúng tôi chỉ huấn luyện một bước để đi trực tiếp từ token nhiễu sang token vàng, mô hình có thể tổng quát hóa để có được khả năng khử nhiễu lặp lại. Ngoài ra, độ dài nhiễu huấn luyện 8 không vượt trội so với độ dài nhiễu huấn luyện 4, cho thấy độ dài 4 đã đạt đến nút thắt của khả năng khử nhiễu của mô hình trong giai đoạn SFT.

5.2 Tác động của Quy mô Mô hình

Để đánh giá khả năng tổng quát hóa của MSN, các thí nghiệm được tiến hành trên các kích thước khác nhau của Starcoder2 (Lozhkov et al., 2024), cụ thể là 3B, 7B, và 15B tham số. Dữ liệu huấn luyện vẫn phù hợp với Phần 4.1, và HumanEval với giải mã Jacobi được sử dụng để đánh giá tăng tốc. Kết quả của thí nghiệm được hiển thị trong Hình 6. Nhìn chung, MSN thể hiện tăng tốc đáng kể trên tất cả các kích thước mô hình, cho thấy khả năng ứng dụng rộng rãi của nó.

Cụ thể, khi tăng kích thước mô hình từ 3B lên 7B, Mean Accepted Tokens (#MAT) chỉ tăng 0.03, và tỷ lệ tăng tốc giảm nhẹ. Điều này cho thấy rằng một mô hình 3B đủ để học khả năng khử nhiễu và hiệu quả của việc khử nhiễu không thay đổi đáng kể với sự tăng lên của tham số từ 3B lên 7B. Sự tăng gia tăng trong MAT cho mô hình 7B không đủ để bù đắp chi phí tính toán bổ sung của token dự thảo trong quá trình suy luận, dẫn đến giảm tỷ lệ tăng tốc. Tuy nhiên, khi kích thước mô hình đạt 15B, khả năng khử nhiễu tăng mạnh. #MAT tăng gần 1, và chi phí tính toán bổ sung của token dự thảo được giảm nhẹ bởi sự cải thiện đáng kể trong tỷ lệ hit, dẫn đến tăng 0.6 trong tỷ lệ tăng tốc. Kết quả về quy mô mô hình minh họa thêm khả năng ứng dụng rộng rãi của phương pháp chúng tôi và chứng minh rằng các mô hình lớn hơn có tiềm năng lớn hơn.

5.3 Tác động của Đường dẫn Truy xuất

Để phân tích thêm sự tăng cường hiệu suất mà các đường dẫn truy xuất mang lại cho giải mã TR-Jacobi, chúng tôi thực hiện thí nghiệm ablation với Llama3 trên Mt-Bench. Chúng tôi so sánh #MAE cho phương pháp truy xuất thuần túy PLD, phương pháp Jacobi thuần túy TR-Jacobi w/o R, và TR-Jacobi trên mỗi lĩnh vực. Kết quả của thí nghiệm được hiển thị trong Hình 5. TR-Jacobi đề xuất của chúng tôi tích hợp và vượt trội so với các giải pháp Jacobi thuần túy và truy xuất thuần túy về hiệu suất tăng tốc trong các lĩnh vực khác nhau. Đường dẫn truy xuất giảm nhẹ sự khởi động lạnh và bất ổn do nhiễu ngẫu nhiên của phương pháp Jacobi. Phương pháp Jacobi có thể tiếp tục lặp lại trên đường dẫn truy xuất và cũng có thể xử lý các tác vụ với ngữ cảnh ngắn hơn (ví dụ: dịch thuật).

6 Kết luận

Trong bài báo này, chúng tôi đề xuất một khung huấn luyện hiệu quả Make Some Noise (MSN) để được sử dụng như một sự thay thế cho giai đoạn SFT. Nó tăng cường khả năng khử nhiễu của mô hình mà không ảnh hưởng đến hiệu suất huấn luyện SFT. Kết hợp với chiến lược giải mã TR-Jacobi đề xuất của chúng tôi, mô hình MSN có thể đạt được tăng tốc 2.3-2.7x trong lĩnh vực tổng quát và mã mà không có cấu trúc và huấn luyện bổ sung.

Hạn chế

Khử nhiễu nhân quả, như một tác vụ tổng quát hơn, chỉ được sử dụng cho các thí nghiệm trong giai đoạn SFT trong bài báo này do tài nguyên tính toán hạn chế. Việc hợp nhất tác vụ khử nhiễu với tác vụ dự đoán token tiếp theo vào tác vụ tiền huấn luyện là một khám phá đáng giá. Ngoài điều này, độ dài đoạn nhiễu tối ưu có thể liên quan đến nội dung của bộ huấn luyện SFT (dự đoán song song của văn bản mã ít khó khăn hơn, văn bản ngôn ngữ tự nhiên khó khăn hơn). Đối với một bộ dữ liệu SFT mới, việc xác nhận các đoạn nhiễu tối ưu có thể yêu cầu một số thí nghiệm trước để tìm kiếm, điều này áp đặt một gánh nặng nhất định cho việc huấn luyện MSN.

Tuyên bố Đạo đức

Dữ liệu nguồn cho các phương pháp đề xuất đến hoàn toàn từ các tài nguyên dự án có sẵn công khai trên các trang web hợp pháp và không liên quan đến bất kỳ thông tin nhạy cảm nào. Ngoài ra, tất cả các baseline và bộ dữ liệu được sử dụng trong các thí nghiệm của chúng tôi cũng có sẵn công khai, và chúng tôi đã thừa nhận các tác giả tương ứng bằng cách trích dẫn công trình của họ.

Lời cảm ơn

Chúng tôi biết ơn sự hỗ trợ của Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (NSFC) thông qua các grant 62236004, 62206078, 62441603 và 62476073 và sự hỗ trợ của Công ty TNHH Khoa học Công nghệ Du Xiaoman (Bắc Kinh).
