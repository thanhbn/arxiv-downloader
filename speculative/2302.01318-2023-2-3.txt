# 2302.01318.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/speculative/2302.01318.pdf
# File size: 349916 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
2023-2-3
Accelerating Large Language Model Decoding
with Speculative Sampling
Charlie Chen1, Sebastian Borgeaud1, Geoï¬€rey Irving1, Jean-Baptiste Lespiau1, Laurent Sifre1and John
Jumper1
1All authors from DeepMind
We present speculative sampling, an algorithm for accelerating transformer decoding by enabling the
generation of multiple tokens from each transformer call. Our algorithm relies on the observation that
the latency of parallel scoring of short continuations, generated by a faster but less powerful draft
model, is comparable to that of sampling a single token from the larger target model. This is combined
with a novel modiï¬ed rejection sampling scheme which preserves the distribution of the target model
withinhardwarenumerics. WebenchmarkspeculativesamplingwithChinchilla,a70billionparameter
language model, achieving a 2â€“2Â“5decoding speedup in a distributed setup, without compromising
the sample quality or making modiï¬cations to the model itself.
Introduction
Scaling transformer models to 500B+ parameters has led to large performance improvements on
many natural language, computer vision and reinforcement learning tasks (Arnab et al., 2021; Brown
etal.,2020;Chowdheryetal.,2022;Dosovitskiyetal.,2020;Hoï¬€mannetal.,2022;Raeetal.,2021).
However, transformer decoding remains a highly costly and ineï¬ƒcient process in this regime.
Transformer sampling is typically memory bandwidth bound (Shazeer, 2019), so for a given set of
hardware, the time to generate a single token in transformer models is proportional to a ï¬rst order
approximationtothesizeofparametersandthesizeofthetransformermemory. Thesizeoflanguage
models also necessitates serving with model parallelism â€“ adding communication overheads (Pope
et al., 2022) and multiplying resource requirements. Since each new token depends on the past,
many such transformer calls are required to sample a new sequence.
We present an algorithm to accelerate transformer sampling for latency critical applications, which
we call speculative sampling (SpS). This is achieved by:
1.Generating a short draft of length ğ¾. This can be attained with either a parallel model (Stern
et al., 2018) or by calling a faster, auto-regressive model ğ¾times. We shall refer to this model
as thedraft model , and focus on the case where it is auto-regressive.
2.Scoring the draft using the larger, more powerful model from we wish to sample from. We shall
refer to this model as the target model .
3.Using a modiï¬ed rejection sampling scheme, accept a subset of the ğ¾draft tokens from left to
right, recovering the distribution of the target model in the process.
Intuitively, there are often sequences where the next token might be â€œobviousâ€. Therefore, if there is
strongagreementbetweenthedraftandtargetmodelâ€™sdistributionsonagiventokenorsub-sequence
of tokens, this setup permits the generation of multiple tokens each time the target model is called .
We show that the expected acceptance rate of draft tokens is suï¬ƒcient to oï¬€set the overhead of the
Corresponding author(s): ccharlie@deepmind.com
Â©2023 DeepMind. All rights reservedarXiv:2302.01318v1  [cs.CL]  2 Feb 2023

--- PAGE 2 ---
Accelerating Large Language Model Decoding with Speculative Sampling
draftingprocessforlargelanguagemodels,resultinginaneï¬€ectiveandpracticalmethodforreducing
sampling latency without the need for modifying the target model or biasing the sample distribution.
Depending on the evaluation domain, SpS leads to a 2â€“2Â“5speedup when sampling from Chinchilla
(Hoï¬€mann et al., 2022). Notably, the mean tokens per second with SpS often exceeds the idealised
ceiling on auto-regressive sampling speed imposed by the memory bandwidth.
Related Work
There has been a substantial body of work focused on improving sampling latency of large transform-
ers and other auto-regressive models.
Since sampling performance is heavily coupled with the model size in memory, quantisation to
int8or even int4(Dettmers et al., 2022; Yao et al., 2022) and distillation (Jiao et al., 2020; Sanh
et al., 2019) of transformers are eï¬€ective techniques for reducing sampling latency with little to no
performance penalty. The observation that model size contributes less to the ï¬nal performance than
expected (Hoï¬€mann et al., 2022) has also encouraged smaller language models in general.
During sampling, a cache of the keys and values is maintained for every attention layer, and could
become a memory bandwidth bottleneck as the batch size increases. Methods such as multi-query
attention (Shazeer, 2019) aims to improve sampling performance by shrinking this cache. However
thesetechniquesaremosteï¬€ectiveatmaximisingthroughout(atlargerbatchsizes)insteadoflatency,
especially for larger models where the majority of the memory bandwidth budget is consumed by the
parameters.
Using a combination of the above techniques, in addition to a number of low-level optimisations to
TPUs, Pope et al. (2022) have greatly improved the serving latency and eï¬ƒciency of PaLM 540B.
There is an existing body of similar work exploiting the eï¬ƒciency of transformers and sequence
models operating in parallel. This includes block parallel sampling (Stern et al., 2018), aggressive
decoding (Ge et al., 2022), in addition to some work in parallelizing autoregressive models in the
image domain (Song et al., 2021; Wiggers and Hoogeboom, 2020). These methods have yet to be
adapted to typical language model use-cases since they either only work with greedy sampling, bias
the results or are focused on other modalities. Further, to our knowledge none of these techniques
have been scaled to distributed setups, which is necessary for the most expensive decoders with the
tens or hundreds of billions of parameters.
Coincidentally, the work in this manuscript was undertaken concurrently and independently of
the work on speculative decoding from Leviathan et al. (2022). We focus more heavily the distributed
serving setting for large models and oï¬€er some incremental optimisations, but otherwise the core
underlying idea is the same.
Auto-regressive Sampling
Whilst transformers can be trained eï¬ƒciently and in parallel on TPUs and GPUs, samples are typically
drawn auto-regressively (See algorithm 1). For most applications, auto-regressive sampling (ArS) is
highlymemorybandwidthboundandthuscannotmakeeï¬€ectiveuseofmodernacceleratorhardware
(Shazeer, 2019). A memory bound model call only generates a single token for every sequence in the
batch, hence generating multiple tokens introduces a large amount of latency in any system which
makes use of it.
2

--- PAGE 3 ---
Accelerating Large Language Model Decoding with Speculative Sampling
Thisisespeciallyproblematicasthenumberofparameters inthemodelincreases. Sinceallthemodel
parameters need to pass through at least one accelerator chip, the model size divided by the total
memory bandwidth across all chips gives us a hard ceiling on the maximum auto-regressive sampling
speed. Larger models also require serving on multiple accelerators, introducing a further source of
latency due to inter-device communication overheads.
Algorithm 1 Auto-regressive (ArS) with Auto-Regressive Models
Givenauto-regressivetargetmodel ğ‘Â¹Â“jÂ“Âºandinitialpromptsequence ğ‘¥1Â”Â“Â“Â“Â”ğ‘¥ğ‘¡andtargetsequence
lengthğ‘‡.
Initialiseğ‘› ğ‘¡.
whileğ‘› Âğ‘‡do
Sampleğ‘¥ğ‘›Â¸1ğ‘Â¹ğ‘¥jğ‘¥1Â”Â“Â“Â“Â”ğ‘¥ğ‘›Âº
ğ‘› ğ‘›Â¸1
end while
Algorithm 2 Speculative Sampling (SpS) with Auto-Regressive Target and Draft Models
Given lookahead ğ¾and minimum target sequence length ğ‘‡.
Given auto-regressive target model ğ‘Â¹Â“jÂ“Âº, and auto-regressive draft model ğ‘Â¹Â“jÂ“Âº, initial prompt
sequenceğ‘¥0Â”Â“Â“Â“Â”ğ‘¥ğ‘¡.
Initialiseğ‘› ğ‘¡.
whileğ‘› Âğ‘‡do
forğ‘¡=1 :ğ¾do
Sample draft auto-regressively Ëœğ‘¥ğ‘¡ğ‘Â¹ğ‘¥jÂ”ğ‘¥1Â”Â“Â“Â“Â”ğ‘¥ğ‘›Â”Ëœğ‘¥1Â”Â“Â“Â“Â” Ëœğ‘¥ğ‘¡ 1Âº
end for
In parallel, compute ğ¾Â¸1sets of logits from drafts Ëœğ‘¥1Â”Â“Â“Â“Â” Ëœğ‘¥ğ¾:
ğ‘Â¹ğ‘¥jÂ”ğ‘¥1Â”Â“Â“Â“Â”ğ‘¥ğ‘›ÂºÂ” ğ‘Â¹ğ‘¥jÂ”ğ‘¥1Â”Â“Â“Â“Â”ğ‘¥ğ‘›Â”Ëœğ‘¥1ÂºÂ” Â“Â“Â“Â” ğ‘Â¹ğ‘¥jÂ”ğ‘¥1Â”Â“Â“Â“Â”ğ‘¥ğ‘›Â”Ëœğ‘¥1Â”Â“Â“Â“Â” Ëœğ‘¥ğ¾Âº
forğ‘¡=1 :ğ¾do
Sampleğ‘Ÿğ‘ˆÂ»0Â”1Â¼from a uniform distribution.
ifğ‘Ÿ Âmin
1Â”ğ‘Â¹ğ‘¥jğ‘¥1Â”Â“Â“Â“Â”ğ‘¥ğ‘›Â¸ğ‘¡ 1Âº
ğ‘Â¹ğ‘¥jğ‘¥1Â”Â“Â“Â“Â”ğ‘¥ğ‘›Â¸ğ‘¡ 1Âº
,then
Setğ‘¥ğ‘›Â¸ğ‘¡ Ëœğ‘¥ğ‘¡andğ‘› ğ‘›Â¸1.
else
sampleğ‘¥ğ‘›Â¸ğ‘¡Â¹ğ‘Â¹ğ‘¥jğ‘¥1Â”Â“Â“Â“Â”ğ‘¥ğ‘›Â¸ğ‘¡ 1Âº ğ‘Â¹ğ‘¥jğ‘¥1Â”Â“Â“Â“Â”ğ‘¥ğ‘›Â¸ğ‘¡ 1ÂºÂºÂ¸and exit for loop.
end if
end for
If all tokens ğ‘¥ğ‘›Â¸1Â”Â“Â“Â“Â”ğ‘¥ğ‘›Â¸ğ¾are accepted, sample extra token ğ‘¥ğ‘›Â¸ğ¾Â¸1ğ‘Â¹ğ‘¥jÂ”ğ‘¥1Â”Â“Â“Â“Â”ğ‘¥ğ‘›Â”ğ‘¥ğ‘›Â¸ğ¾Âºand
setğ‘› ğ‘›Â¸1.
end while
Speculative Sampling
Conditional Scoring
For speculative sampling (See algorithm 2), we ï¬rst make the observation that computing the logits
of a short continuation of ğ¾tokens in parallel has a very similar latency to that of sampling a single
3

--- PAGE 4 ---
Accelerating Large Language Model Decoding with Speculative Sampling
token. We focus our attention on large transformers, sharded in the Megatron style (Shoeybi et al.,
2019). For these models the majority of sampling time can be attributed to three components:
1.Linear Layers: For small batch sizes, each linear layer only processes a small number of
embeddings. This causes the dense matrix multiplies in the feed-forward layers, queries, keys,
values computations and the ï¬nal attention projection to become memory bound. For small ğ¾,
this will continue to be memory bound and therefore take a similar amount of time.
2.The Attention Mechanism: Theattentionmechanismisalsomemorybound. Duringsampling,
we maintain a cache of all the keys and values of the previous tokens in the sequence to avoid
re-computation. These KV-caches are large, and accounts for the majority of the memory
bandwidth utilisation for the attention mechanism. However, since the KV-cache size does not
change as we increase ğ¾, there is little to no delta in this component.
3.All-reduces: Asmodelsgrowinsize, itsparametersneedtobedividedacrossmultipleaccelera-
tors, leading to communication overheads. With Megatron, this manifests itself as an all-reduce
after every feed-forward and attention layer. Since only the activations for a small number of
tokens are transmitted, this operation is typically latency bound instead of throughput bound
for both sampling and scoring (for small ğ¾). Again, this results in a similar amount of time
spent in the two cases.
Other sources of overhead may exist, depending on the exact transformer implementation. Therefore
it is still possible that the choice of positioning encoding, decoding method (e.g. a sort might be
required for nucleus sampling), hardware limitations etc. can introduce some deltas between scoring
and sampling. However, if the conditions are met such that the above components dominate then
scoring should not be signiï¬cantly slower for small ğ¾.
Modiï¬ed Rejection Sampling
We require a method to recover the distribution of the target model from samples from the draft
model, and logits of said tokens from both models.
To achieve this, we introduce the following rejection sampling scheme of the drafted tokens. Given a
sequence of tokens ğ‘¥1Â”Â“Â“Â“Â”ğ‘¥ğ‘›, andğ¾draft tokens Ëœğ‘¥ğ‘›Â¸1Â”Â“Â“Â“Â” Ëœğ‘¥ğ‘›Â¸ğ¾generated from ğ‘Â¹Â“jÂ“Âº, we accept Ëœğ‘¥ğ‘›Â¸1
with probability:
min
1Â”ğ‘Â¹Ëœğ‘¥ğ‘›Â¸1jğ‘¥1Â”Â“Â“Â“Â”ğ‘¥ğ‘›Âº
ğ‘Â¹Ëœğ‘¥ğ‘›Â¸1jğ‘¥1Â”Â“Â“Â“Â”ğ‘¥ğ‘›Âº
Whereğ‘Â¹Ëœğ‘¥ğ‘›Â¸1jğ‘¥1Â”Â“Â“Â“Â”ğ‘¥ğ‘›Âºandğ‘Â¹Ëœğ‘¥ğ‘›Â¸1jğ‘¥1Â”Â“Â“Â“Â”ğ‘¥ğ‘›Âºare the probability of Ëœğ‘¥ğ‘›Â¸1according to the target and
draft models respectively, conditioned on the context so far.
If the token is accepted, we set ğ‘¥ğ‘›Â¸1 Ëœğ‘¥ğ‘›Â¸1and repeat the process for Ëœğ‘¥ğ‘›Â¸2until either a token
is rejected or all tokens have been accepted.
IfËœğ‘¥ğ‘›Â¸1is rejected, we resample ğ‘¥ğ‘›Â¸1from the following distribution:
ğ‘¥ğ‘›Â¸1Â¹ğ‘Â¹ğ‘¥jğ‘¥1Â”Â“Â“Â“Â”ğ‘¥ğ‘›Âº ğ‘Â¹ğ‘¥jğ‘¥1Â”Â“Â“Â“Â”ğ‘¥ğ‘›ÂºÂºÂ¸
WhereÂ¹Â“ÂºÂ¸denotes:
Â¹ğ‘“Â¹ğ‘¥ÂºÂºÂ¸=maxÂ¹0Â” ğ‘“Â¹ğ‘¥ÂºÂºÃ
ğ‘¥maxÂ¹0Â” ğ‘“Â¹ğ‘¥ÂºÂº
By applying this sequentially, we recover the distribution of the target model for the accepted tokens
(see proof in Theorem 1) within hardware numerics. Note that:
4

--- PAGE 5 ---
Accelerating Large Language Model Decoding with Speculative Sampling
â€¢At least one token will always be generated from a draft-accept loop â€“ if the ï¬rst token is
rejected, a valid token is resampled.
â€¢Since the ï¬nal token of the draft gives us the logits for the next token, if every drafted token is
accepted, we can sample from it normally. This gives us a maximum of ğ¾Â¸1tokens per loop,
over the naive implementation which would only return ğ¾tokens.
With standard sampling methods such as nucleus, top-k sampling and adjusting temperature, we
can modify the probabilities accordingly before applying this rejection sampling scheme. We have
observed that the overall acceptance rate is robust to the exact parameters used.
Because we do not interact with the body of the transformer itself, this method can be used in
conjunction many other techniques for accelerating or optimising the memory use of sampling, such
as quantisation and multi-query attention.
Choice of Draft Models
Since the acceptance criterion guarantees the distribution of the target model in our samples, we are
free to choose the method for drafting a continuation as long as it exposes logits, and there is a high
enough acceptance rate and/or low enough latency to break-even. There exist several approaches
here:
â€¢Incorporating draft generation into the target model, and train the model from the start. This
is the strategy used by Stern et al. (2018), which adds multiple heads into the transformer to
generate multiple tokens.
â€¢Using sequence level distillation (Kim and Rush, 2016) to generate a second model which
predictsğ¾tokens in parallel. This strategy was employed by Ge et al. (2022).
â€¢Set a portion of the activations of the target model as an input to the draft model, and train the
draft model with this input.
Although these methods will likely yield powerful drafts, they require a large number of data gener-
ated from the target model or changes to the target model. Sequence level distillation in particular
would require a large compute budget. This makes them less practical for large scale applications.
Whilst large language models produce better samples, intuitively there are "easier" tokens to predict
for which smaller models may be suï¬ƒcient. Therefore we may simply use a smaller version of the
target language model as the draft and obtain high acceptance rates. This would also be convenient
from an engineering and workï¬‚ow perspective, since robust tooling for such models should already
exist to train the target model in the ï¬rst place.
Results
We train a 4 billion parameter draft model optimised for sampling latency on 16 TPU v4s â€“ the same
hardware that is typically used to serve Chinchilla for research purposes. This model was trained
with the same tokeniser and dataset as Chinchilla, with a slightly smaller width and with only 8
layers. The relatively few number of layers allows it to achieve a sampling speed of 1.8ms/token
comparedto 14.1ms/token forChinchilla. Fordetails,pleaserefertothehyperparametersinTable2.
For distributed setups it is insuï¬ƒcient to naively choose a small model as the draft, since diï¬€er-
ent models have diï¬€erent optimal inference setups. For example, it is typical to serve Chinchilla 70B
5

--- PAGE 6 ---
Accelerating Large Language Model Decoding with Speculative Sampling
Table1jChinchillaperformanceandspeedonXSumandHumanEvalwithnaiveandspeculative
sampling at batch size 1 and ğ¾=4. XSum was executed with nucleus parameter ğ‘=0Â“8, and
HumanEval with ğ‘=0Â“95and temperature 0Â“8.
Sampling Method Benchmark Result Mean Token Time Speed Up
ArS (Nucleus)XSum (ROUGE-2)0.112 14.1ms/Token 1
SpS (Nucleus) 0.114 7.52ms/Token 1Â“92
ArS (Greedy)XSum (ROUGE-2)0.157 14.1ms/Token 1
SpS (Greedy) 0.156 7.00ms/Token 2Â“01
ArS (Nucleus)HumanEval (100 Shot)45.1% 14.1ms/Token 1
SpS (Nucleus) 47.0% 5.73ms/Token 2Â“46
on 16 TPU v4s (where it achieves the aforementioned 14.1ms/token ), whereas a chinchilla-optimal
7B achieves its lowest sampling latency on 4 TPU v4s (where it achieves 5ms/token ). For smaller
models, the additional memory bandwidth and ï¬‚ops are insuï¬ƒcient to oï¬€set the additional communi-
cation overhead between more devices â€“ serving a 7B on 16 TPUs actually increases the latency. This
means the 7B would provide only a modest speedup if used as a draft with its optimal topology, and
we will not make full utilisation of the hardware during drafting.
We can sidestep this issue by training a wider model with a relatively few number of layers in
order to minimise communication overhead. It has been observed that the performance of language
models is relatively robust to changes in model aspect ratio (Levine et al., 2020), so this allows us
to serve a powerful draft model which can be sampled rapidly on the same hardware as the target
model.
Evaluation on XSum and HumanEval
We evaluate speculative sampling with Chinchilla on two tasks and summarize the results in Table 1:
â€¢The XSum (Narayan et al., 2018) benchmark. This is a natural language summarisation task
using a 1-shot prompt where we sample a total of 11,305 sequences with a maximum sequence
length 128.
â€¢The 100-shot HumanEval task (Chen et al., 2021). This is a code generation task involves the
generation of 16,400 samples with a maximum sequence length of 512.
Even with greedy sampling, a single token deviating due to numerics could result in two sequences
diverging wildly. Since pseudo-random seeds are processed diï¬€erently between ArS and SpS, and
because the diï¬€erent computation graphs lead to diï¬€erent numerics, we cannot not expect identical
outputs. However, we expect the samples to come from the same distribution within numerics and
we empirically verify this by evaluating these benchmarks.
We run the tasks at batch size 1 with SpS and ArS. The time taken per SpS/ArS loop has low
variance, and we can measure it directly from TPU proï¬les. To obtain the average speedup, standard
deviations and other metrics, we log the amount of tokens generated for each speculative loop. In
Table 1 we show the performance on the XSum and HumanEval benchmarks for naive and speculative
sampling with Chinchilla.
6

--- PAGE 7 ---
Accelerating Large Language Model Decoding with Speculative Sampling
We obtain a substantial speedup in both tasks, with HumanEval reaching speedups of almost 2Â“5.
Yet, we have parity in the benchmark metrics â€“ the underlying samples distribution is provably the
sameuptonumerics, andthisveriï¬esthatthedraftmodelisnotbiasingtheresultsempirically. Inthe
case of HumanEval and greedy XSum, this speedup exceeded the theoretical memory bandwidth limit
of the hardware for autoregressive sampling (model size divided by the total memory bandwidth).
Acceptance rate changes per domain
It is apparent that the acceptance rate is dependent on the application and the decoding method.
HumanEvalachievesasigniï¬cantlylargerspeedupâ€”Wehypothesizethatthisisduetoacombination
of code containing a lot of common sub-sequences (e.g. for i in range(len(arr)): would be
relatively easy for a draft model to guess), is often decomposed into a smaller set of shorter tokens
and the temperature value sharpening both the draft and target logits.
01234567
Number of draft tokens (K)40060080010001200140016001800ms
Mean Sampling Time (128 tokens)
Human Eval
XSum
01234567
Number of draft tokens (K)0.50.60.70.80.91.0
Acceptance rate
Human Eval
XSum
01234567
Number of draft tokens (K)1416182022242628ms
Total loop time
Figure 1jLeft:The average time to generate 128 tokens, with standard deviation. Note that as ğ¾
increases, the overall speedup plateaus or even regresses, with XSum being optimal at ğ¾=3. The
variance consistently increases with ğ¾.Middle:The average number of tokens accepted divided by
ğ¾Â¸1â€“ this serves as a measure of the overall eï¬ƒciency of the modiï¬ed rejection scheme, which
decreases with the lookahead. Right:Average time per loop increases approximately linearly with ğ¾
duetotheincreasednumberofmodelcalls. Notethatthegradientisslightlyhigherthanthesampling
speed of the draft model, due to additional overheads in nucleus decoding.
Trade oï¬€ between longer drafts and more frequent scoring
Wevisualisethetrade-oï¬€ofincreasing ğ¾,thenumberoftokenssampledbythedraftmodelinFigure1.
Asğ¾increases, we need fewer scoring calls from the large models to generate the same sequence
length, potentially giving us a larger speedup. However, the total loop time increases approximately
linearly with the larger number of draft model calls and small increases in the scoring time. The
overall eï¬ƒciency of the proportion of accepted tokens decreases as ğ¾increases, since later tokens
depend on the acceptance of previous tokens. This results in the average speedup plateauing or
even degrading with a larger ğ¾(for example, XSum with nucleusâ€™s latency is minimised at ğ¾=3),
depending on the domain.
7

--- PAGE 8 ---
Accelerating Large Language Model Decoding with Speculative Sampling
Further, even though larger values of ğ¾may yield marginally greater mean speedups in certain
circumstances, it also increases variance of the time to generate a full sequence. This could be
problematic for settings where the P90, P99 latencies of concern.
Conclusion
Inthiswork,wedemonstrateanewalgorithmandworkï¬‚owforacceleratingthedecodingoflanguage
models. Speculative sampling does not require making any modiï¬cations to the target language
modelâ€™s parameters or architecture, is provably lossless within numerics, scales well with the appro-
priate draft model and complements many existing techniques for reducing latency in the small batch
size setting.
We optimise and scale the technique to Chinchilla 70B using a draft model which was easy to
trainwithexistinginfrastructure, demonstratingthatityieldsalargespeedupacrossbenchmarktasks
and common decoding methods in the process. We verify that it is indeed lossless empirically in its
downstream tasks.
References
A.Arnab,M.Dehghani,G.Heigold,C.Sun,M.Lucic,andC.Schmid. Vivit: Avideovisiontransformer.
In2021 IEEE/CVF International Conference on Computer Vision (ICCV) , pages 6816â€“6826. IEEE
Computer Society, 2021.
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information
processing systems , 33:1877â€“1901, 2020.
M.Chen,J.Tworek,H.Jun,Q.Yuan,H.P.deOliveiraPinto,J.Kaplan,H.Edwards,Y.Burda,N.Joseph,
G.Brockman,A.Ray,R.Puri,G.Krueger,M.Petrov,H.Khlaaf,G.Sastry,P.Mishkin,B.Chan,S.Gray,
N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings,
M.Plappert,F.Chantzis,E.Barnes,A.Herbert-Voss,W.H.Guss,A.Nichol,A.Paino,N.Tezak,J.Tang,
I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra,
E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew,
D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained
on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374 .
A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung,
C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint
arXiv:2204.02311 , 2022.
T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for
transformers at scale. arXiv preprint arXiv:2208.07339 , 2022.
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-
derer,G.Heigold,S.Gelly,etal. Animageisworth16x16words: Transformersforimagerecognition
at scale.arXiv preprint arXiv:2010.11929 , 2020.
T.Ge,H.Xia,X.Sun,S.Chen,andF.Wei. Losslessaccelerationforseq2seqgenerationwithaggressive
decoding. ArXiv, abs/2205.10350, 2022.
8

--- PAGE 9 ---
Accelerating Large Language Model Decoding with Speculative Sampling
J. Hoï¬€mann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A.
Hendricks,J.Welbl,A.Clark,etal. Trainingcompute-optimallargelanguagemodels. arXivpreprint
arXiv:2203.15556 , 2022.
X. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and Q. Liu. TinyBERT: Distilling BERT
for natural language understanding. In Findings of the Association for Computational Linguis-
tics: EMNLP 2020 , pages 4163â€“4174, Online, Nov. 2020. Association for Computational Linguis-
tics. doi: 10.18653/v1/2020.ï¬ndings-emnlp.372. URL https://aclanthology.org/2020.
findings-emnlp.372 .
Y. Kim and A. M. Rush. Sequence-level knowledge distillation. CoRR, abs/1606.07947, 2016. URL
http://arxiv.org/abs/1606.07947 .
Y. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers via speculative decoding.
ArXiv, abs/2211.17192, 2022.
Y. Levine, N. Wies, O. Sharir, H. Bata, and A. Shashua. The depth-to-width interplay in self-attention.
arXiv preprint arXiv:2006.12467 , 2020.
S. Narayan, S. B. Cohen, and M. Lapata. Donâ€™t give me the details, just the summary! topic-
aware convolutional neural networks for extreme summarization. In Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing , pages 1797â€“1807, Brussels,
Belgium, Oct.-Nov. 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1206.
URLhttps://aclanthology.org/D18-1206 .
R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek, K. Xiao, S. Agrawal,
and J. Dean. Eï¬ƒciently scaling transformer inference. arXiv preprint arXiv:2211.05102 , 2022.
J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoï¬€mann, F. Song, J. Aslanides, S. Henderson, R. Ring,
S. Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv
preprint arXiv:2112.11446 , 2021.
V. Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of bert: smaller, faster,
cheaper and lighter. arXiv preprint arXiv:1910.01108 , 2019.
N. Shazeer. Fast transformer decoding: One write-head is all you need. CoRR, abs/1911.02150, 2019.
URLhttp://arxiv.org/abs/1911.02150 .
M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm: Training
multi-billionparameterlanguagemodelsusingmodelparallelism. arXivpreprintarXiv:1909.08053 ,
2019.
Y.Song, C.Meng, R.Liao, andS.Ermon. Acceleratingfeedforwardcomputationviaparallelnonlinear
equationsolving. InM.MeilaandT.Zhang,editors, Proceedingsofthe38thInternationalConference
on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pages 9791â€“9800.
PMLR, 18â€“24 Jul 2021. URL https://proceedings.mlr.press/v139/song21a.html .
M. Stern, N. Shazeer, and J. Uszkoreit. Blockwise parallel decoding for deep autoregressive models.
CoRR, abs/1811.03115, 2018. URL http://arxiv.org/abs/1811.03115 .
A. Wiggers and E. Hoogeboom. Predictive sampling with forecasting autoregressive models. In H. D.
III and A. Singh, editors, Proceedings of the 37th International Conference on Machine Learning ,
volume 119 of Proceedings of Machine Learning Research , pages 10260â€“10269. PMLR, 13â€“18 Jul
2020. URL https://proceedings.mlr.press/v119/wiggers20a.html .
9

--- PAGE 10 ---
Accelerating Large Language Model Decoding with Speculative Sampling
Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He. Zeroquant: Eï¬ƒcient and aï¬€ordable
post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861 , 2022.
Supplementary Materials
Author Contributions
â€¢Initial proposal: Charlie Chen, John Jumper and Geoï¬€rey Irving
â€¢Initial Implementation, Optimisation and Scaling: Charlie Chen
â€¢Modiï¬ed Rejection Sampling Scheme: John Jumper
â€¢Engineering Improvements: Jean-Baptiste Lespiau and Charlie Chen
â€¢Experiments: Charlie Chen, Sebastian Borgeaud and Laurent Sifre
â€¢Draft of Manuscript: Charlie Chen and Sebastian Borgeaud
â€¢Manuscript Feedback: Laurent Sifre, Geoï¬€rey Irving and John Jumper
Acknowledgements
Weâ€™d like to thank Oriol Vinyals and Koray Kavukcuoglu for your kind advice and leadership. Weâ€™d
also like to thank Evan Senter for your additional feedback on the manuscript and Amelia Glaese for
your support in navigating the publishing process. Finally, weâ€™d like to thank Blake Hechtman, Berkin
Ilbeyi for your valuable advice on XLA and Nikolai Grigoriev for our discussions on the various tricks
that can be applied to the transformer architecture.
Hyperparams
Table 2jHyperparameters for the draft model
Model ğ‘‘modelHeads Layers Params
Target (Chinchilla) 8192 64 80 70B
Draft 6144 48 8 4B
Proofs
Theorem1 (Modiï¬edRejectionSamplingrecoversthetargetdistribution) .Givendiscretedistributions
ğ‘,ğ‘and a single draft sample Ëœğ‘¥ğ‘, letğ‘‹be the ï¬nal resulting sample. For ğ‘‹=ğ‘¥to be true, we must
either sample Ëœğ‘¥=ğ‘¥and then accept it, or resample it after Ëœğ‘¥(of any value) is rejected. Hence:
â„™Â¹ğ‘‹=ğ‘¥Âº
=â„™Â¹Ëœğ‘¥=ğ‘¥Âºâ„™Â¹Ëœğ‘¥acceptedjËœğ‘¥=ğ‘¥ÂºÂ¸â„™Â¹Ëœğ‘¥rejectedÂºâ„™Â¹ğ‘‹=ğ‘¥jËœğ‘¥rejectedÂº
For the ï¬rst term, we apply the acceptance rule:
â„™Â¹Ëœğ‘¥=ğ‘¥Âºâ„™Â¹Ëœğ‘¥acceptedjËœğ‘¥=ğ‘¥Âº
=ğ‘Â¹ğ‘¥Âºmin
1Â”ğ‘Â¹ğ‘¥Âº
ğ‘Â¹ğ‘¥Âº
10

--- PAGE 11 ---
Accelerating Large Language Model Decoding with Speculative Sampling
=minÂ¹ğ‘Â¹ğ‘¥ÂºÂ”ğ‘Â¹ğ‘¥ÂºÂº
For the second conditional term, we apply the resampling rule:
â„™Â¹ğ‘‹=ğ‘¥jËœğ‘¥rejectedÂº=Â¹ğ‘Â¹ğ‘¥Âº ğ‘Â¹ğ‘¥ÂºÂºÂ¸
WhereÂ¹Â“ÂºÂ¸denotes:
Â¹ğ‘“Â¹ğ‘¥ÂºÂºÂ¸=maxÂ¹0Â” ğ‘“Â¹ğ‘¥ÂºÂºÃ
ğ‘¥maxÂ¹0Â” ğ‘“Â¹ğ‘¥ÂºÂº
Finally, we calculate the probability of rejection:
â„™Â¹Ëœğ‘¥rejectedÂº=1 â„™Â¹Ëœğ‘¥acceptedÂº
=1 âˆ‘ï¸
ğ‘¥0â„™Â¹ğ‘‹=ğ‘¥0Â”Ëœğ‘¥acceptedÂº
=1 âˆ‘ï¸
ğ‘¥0minÂ¹ğ‘Â¹ğ‘¥0ÂºÂ”ğ‘Â¹ğ‘¥0ÂºÂº
=âˆ‘ï¸
ğ‘¥0maxÂ¹0Â”ğ‘Â¹ğ‘¥0Âº ğ‘Â¹ğ‘¥0ÂºÂº
=âˆ‘ï¸
ğ‘¥0ğ‘Â¹ğ‘¥0Âº minÂ¹ğ‘Â¹ğ‘¥0ÂºÂ”ğ‘Â¹ğ‘¥0ÂºÂº
=âˆ‘ï¸
ğ‘¥0maxÂ¹0Â”ğ‘Â¹ğ‘¥0Âº ğ‘Â¹ğ‘¥0ÂºÂº
This is equal to the denominator of Â¹ğ‘Â¹ğ‘¥Âº ğ‘Â¹ğ‘¥ÂºÂºÂ¸, so:
â„™Â¹Ëœğ‘¥rejectedÂºâ„™Â¹ğ‘‹=ğ‘¥jËœğ‘¥rejectedÂº=maxÂ¹0Â”ğ‘Â¹ğ‘¥Âº ğ‘Â¹ğ‘¥ÂºÂº
Hence:
â„™Â¹ğ‘‹=ğ‘¥Âº
=minÂ¹ğ‘Â¹ğ‘¥ÂºÂ”ğ‘Â¹ğ‘¥ÂºÂºÂ¸maxÂ¹0Â”ğ‘Â¹ğ‘¥Âº ğ‘Â¹ğ‘¥ÂºÂº
=ğ‘Â¹ğ‘¥Âº
and we have recovered the desired target.
11
