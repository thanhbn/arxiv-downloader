# 2403.18647.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/speculative/2403.18647.pdf
# File size: 3892879 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SDSAT: Accelerating LLM Inference through Speculative De-
coding with Semantic Adaptive Tokens
Chengbo Liu∗Yong Zhu†
Abstract
We propose an acceleration scheme for large language models (LLMs)
through Speculative Decoding with Semantic Adaptive Tokens (SDSAT).
The primary objective of this design is to enhance the LLM model’s ability
to generate draft tokens more accurately without compromising the model’s
accuracy. The core strategies involve: 1) Fine-tune the model by incorpo-
rating semantic adaptive tokens that possess flexible decoding capabilities
without changing its structure, allowing them to generate high-quality draft
tokens. 2) By employing a training method that does not affect the standard
tokens, the model can acquire parallel decoding abilities atop its original
framework with minimal training overhead. 3) We have designed the ”two-
step-draft-then-verify” generation strategies using both greedy search and
nucleus sampling. Experiments conducted on the CodeLlama-13B and 7B
models have yielded speed increases of over 3.5X and 3.0X, respectively.
Please refer to https://github.com/ainergy-ml/SDSAT
1 Introduction
Transformer-based LLMs, though efficiently trained in parallel on TPUs and GPUs, face
limitations in auto-regressive sampling due to high memory bandwidth demands (Stern
et al., 2018), resulting in latency when generating multiple tokens one by one. Recently,
algorithms based on the idea of speculative decoding have been developed specifically to
optimize speed. These methods first obtain draft tokens at a lower cost, and then use the
target model for verification. They can be categorized as follows:
The first category relies on smaller models. Chen et al. (2023), and Leviathan et al. (2023)
both utilize a smaller draft model that is faster but less powerful. The quality of the draft,
generated by this model, is comparable to that of sampling a single token from the larger
target model. Leviathan et al. (2023) introduced the speculative sampling method and
proved that the distribution of the generated text remains unchanged for both the greedy
and non-greedy settings, achieving a 2X-3X acceleration compared to the standard T5X
implementation. Zhou et al. (2023) leveraging Knowledge Distillation (KD) to enhance
speculative decoding by aligning a smaller student (draft) model with a teacher (target)
model for better acceptance rates.
Another category is to use the model itself, inferring a larger number of tokens at once to
increase speed. Santilli et al. (2023) implements the greedy decoding procedure in parallel
with a block of tokens, without conducting any training. Medusa (Cai et al., 2023) leverages
a series of MLPs for token prediction, drawing on features from the second-to-top layer of
the original Large Language Model. This method significantly reduces the time required to
generate drafts. Xia et al. (2023) shows that their approach can achieve around a 5 ×speedup
in various seq2seq tasks by masking tokens, including machine translation and abstractive
summarization. However, the length of token generation processed is relatively short. Eagle
(Li et al., 2024) builds on the existing model by adding only a lightweight plug-in (a single
transformer decoder layer) to the LLM. This plug-in is trained to bring the small model as
close as possible to the capability of the target model. The target model’s inference process
∗Thundersoft, liucb0320@thundersoft.com
†Thundersoft, zhuyong@thundersoft.com , Corresponding Author
1arXiv:2403.18647v2  [cs.CL]  1 Apr 2024

--- PAGE 2 ---
uses tree attention (Cai et al., 2023) (Miao et al., 2024) (Spector & Re, 2023) to obtain all
outputs at once, and the verification process uses speculative sampling (Leviathan et al.,
2023) to achieve the final results from the target model.
The third category is data-driven draft token generation, represented by He et al. (2023),
which uses pre-made data to help quickly obtain alternative answers for new tasks, using
the tree-attention method. This allows for the inference results of the target model to be
obtained all at once.
Our proposed solution offers significant improvements without the need for introducing
new smaller models or making modifications to the existing model. This eliminates the need
for complex and difficult adaptation processes. Additionally, there is no requirement for an
external database. By incurring a minimal additional training cost, the model can enhance
its capability to generate highly accurate draft tokens, which in turn directly contributes to
the acceleration of the model’s performance.
The main contributions of this paper are as follows:
•We have verified that large language models (LLMs) can produce high-quality draft
tokens without requiring any modifications to their structure, through the introduction of
semantic adaptive tokens.
•We have developed an innovative training methodology that enables LLMs to produce
accurate draft tokens without compromising the model’s overall accuracy and performance.
•Furthermore, we propose an efficient ”two-step-draft-then-verify” generation method
for both greedy search and nucleus sampling (Holtzman et al., 2019), which leads to high
decoding efficiency.
Utilizing the CodeLlama-13B and 7B models (Roziere et al., 2023), and training only on 2-8B
tokens, the model can maintain nearly unchanged accuracy while significantly enhancing
speed.
Figure 1: Diagram of the inference mechanisms, while the left figure depicts the greedy
search process, which is a special pattern of the nucleus sampling process shown in the right
figure. A green checkmark indicates a token is accepted, and a cross indicates a token is
rejected.
2 Methodology
As discussed by Xia et al. (2023), the shared attention mechanism used in Medusa (Cai
et al., 2023) significantly limits drafting efficiency, leading to a high discard rate of drafted
tokens. Here, we introduces a drafting method that appends semantic adaptive tokens to
sequences so that uses distinct attention queries for predicting each drafted token, offering
a flexible approach applicable to various models without requiring structural modifications.
Continuously, we designed the generation method using both greedy search and nucleus
sampling, the processes are illustrated in Figure 1.
2

--- PAGE 3 ---
2.1 Inference
2.1.1 Greedy search
Figure 2: An example of the ”two-step-draft-then-verifiy” process using the greedy search
generation strategy, with [32011], [32012], [32013] as the adaptive tokens selected for the
CodeLlama model. Each loop consists of three steps: after two drafting steps, the third
step is verification. In the diagram, the tokens ”by” and ”fostering” in loop1 do not match,
therefore the verified accepted token for loop1 ends at ”by”. The second loop passes all
verifications, hence the results generated by all adaptive tokens are accepted.
For simplicity, we firstly illustrate it using a greedy search generation method as shown in
left half of Figure 1. Draft step 1: Attach kadaptive tokens to the sequence and generate
kdraft tokens in parallel. Draft step 2: Leverage the outputs from step 1 to generate the
second kdraft tokens in parallel, and then perform verification of step 3. Figure 2 shows
an example to illustrate the reasoning process more clearly. The inference process can be
modeled as follows.
Step 1: Draft step 1


yn+1=argmaxP θ(yn+1|y1:n,X)
y′
n+2=argmaxP θ(y′
n+2|y1:n,a1,X)
· · ·
y′
n+k+1=argmaxP θ(y′
n+k+1|y1:n,a1:k,X)(1)
Step 2: Draft step 2


yn+2=argmaxP θ(yn+2|y1:n+1,X)
y′′
n+3=argmaxP θ(y′′
n+3|y1:n+1,y′
n+2,X)
· · ·
y′′
n+k+2=argmaxP θ(y′′
n+k+2|y1:n+1,y′
n+2:n+k+1,X)(2)
Step 3: The verification strategy
yi=

yi, if n+1≤i≤n+2
y′′
i, if n+2<i≤n+2+kandy′′
i−1=y′
i−1
stop cur loop, otherwise(3)
LetXrepresent the input of the model, which is a sequence of arbitrary length, yrepresent
the model’s output and arepresent the semantic adaptive tokens. Given that y′
n+1and
3

--- PAGE 4 ---
y′′
n+2are tokens that have been definitively accepted, they can be directly denoted as
yn+1,yn+2.y′
n+2,y′′
n+3, ...,y′
n+k+1,y′′
n+k+2represent generated draft tokens, which need
further determination on whether they can be accepted. By comparing these drafted tokens,
through the Step 3 Verification process, we can obtain accepted tokens. When the stop
condition of the current loop is triggered, there will be a maximum of 2 +kaccepted tokens.
That is to say, if every drafted token is accepted, this will yield a maximum of 2 +ktokens
per loop with only 2 iterations, surpassing the naive implementation which would get only 2
accepted tokens. This step diverges from the process described by Xia et al. (2023), where the
draft phase consists of only one step before immediately moving on to the verification phase.
Their method primarily relies on the higher probability outputs of the model as the criterion
for accepting drafted tokens. However, it is possible for the model to confidently produce
incorrect answers with high probability. Thus, relying solely on tokens probability for
verification could decrease the accuracy during model inference. Our verification approach
ensures that under greedy search, the decoding results are entirely equal to those from the
original inference process.
2.1.2 Nucleus sampling
As shown in the right half of Figure 1, the nucleus sampling can also be divided into three
steps.
Draft Step 1: This step is essentially similar to the method discussed previously, where
several adaptive tokens are concatenated at the end of the input sequence and then sent
together into the target model. The difference lies in the output for each position; due to the
use of nucleus sampling, there are actually many possible draft tokens.
Draft Step 2: The draft tokens generated in the first step are organized into a tree structure,
where each branch represents an alternative sequence. These alternative sequences are then
presented to the model to obtain output in one go. To accurately process a given candidate
sequence with a decoder only model, we introduce a manually designed tree structure,
along with a corresponding attention mask within every attention layer. This approach
ensures that the calculation for each token accurately reflects its relationships in draft step 1.
This method, known as tree attention, was detailed in the research by He et al. (2023), Cai
et al. (2023), and Li et al. (2024).
Verification: During the verification phase, we adopt a simple judgment method similar to
He et al. (2023), akin to the verification strategy and stop strategy mentioned in the context
of greedy search. We select the longest branch of the tree that has been verified and accepted
as the result for the current loop.
2.2 Training
This training approach differs from the Bart-based model trained by Xia et al. (2023), which
employs a consistent [Mask] token for random masking in translation tasks. Given the
short sequence lengths typical of translation tasks, this method can be effective. However,
for longer sequences, without any optimization during the training process, the learning
effectiveness of standard tokens could be adversely affected. It is preferable to adopt a new
training approach, which minimizes the impact of semantic adaptive tokens on the model
itself. See loss detail in Appendix A.
Basic approach . During training, the input sequences, originally composed entirely of
standard tokens, are selectively replaced with semantic adaptive tokens, with the quantity
of replacement approximated a Poisson distribution. This operation can also be considered
as a mask operation and distribution of the number of adaptive tokens can be regarded
as the combination of two independent random processes. Initially, 10% of the positions
in a sequence of length nare selected in a uniform distribution random selection process.
Subsequently, the maximum mask window size is defined as L, and the mask window
size is determined by a uniform distribution random process, selecting a value between 1
andL. The combination of these two processes produces a specific probability distribution.
For the entire sequence, when nis large, the distribution of the number of replaced tokens
4

--- PAGE 5 ---
approximates a Poisson distribution. Let Arepresent the semantic adaptive tokens, this
mixed sequence can be expressed as
M1:n=H(Y1:n,A1:L) (4)
After incorporating semantic adaptive tokens, for a given prediction value, training exclu-
sively with standard tokens results in a prediction value for all inputs as
ˆyi=Pθ(ˆyi|Y1:i−1) (5)
By directly inserting all adaptive tokens into the sequence to obtain sequence M, the
prediction value for the input is
ˆyi=Pθ(ˆyi|M1:i−1) (6)
Let the loss function be denoted by f. For a sequence Y, the loss function for Auto Regression
Language Models (LLMs) can be expressed as
f(ˆY,Y) =−1
nn
∑
i=1log(ˆyi) (7)
Therefore, for two different sequences, one being target sequence YMand the other being
the prediction value ˆM, the loss of basic approach can be expressed as
loss =f(ˆM,YM) (8)
During the training process of LLMs, which is parallel training, the loss is generally calcu-
lated as the cross-entropy for all tokens indiscriminately. This basic approach to calculating
loss significantly affects the training of standard tokens, leading to a higher loss as Figure 5
shows.
Improved approach . An improved approach is to isolate the impact of adaptive tokens
on standard tokens. While, during model inference, it is only necessary to append several
adaptive tokens at the end of the sequence, rather than inserting them in the middle.
However, for more efficient training, multiple adaptive tokens are randomly inserted in the
middle of the sequence during the training process. This can affect the training of standard
tokens, as the presence of adaptive tokens ahead of them may lead to increased loss for
these standard tokens compared to normal circumstances. To address this issue, two types
of input sequences can be used in the actual training process. The first type consists entirely
of standard tokens, leading to an output represented by Y. The second type mixes standard
tokens with adaptive tokens, with the prediction denoted by ˆMand corresponding labels
denoted as YM.Mmaskrepresents a matrix with the same shape as M, which corresponds
to positions in the Msequence that are replaced by adaptive tokens set to 1, and all other
positions set to 0. The loss is then calculated as:
loss =1
2(f(ˆY,Y) +w f(ˆM,YM)⊙Mmask) (9)
In this training approach, the two types of sequences are simply set to each constitute half
of the training data, with the weight wset to 1.
3 Experiments
3.1 Dataset
Our training data is selected from StarCoder (Li et al., 2023), chosen entirely at random in
proportion to the distribution of programming languages. Additionally, we have conducted
a decontamination process on training data to ensure that the test dataset will not appear
in the training dataset. Following the approach used by CodeLlama (Roziere et al., 2023),
we adopt an infilling objective for training our models. To elaborate, we dissect training
texts into three components: a prefix, a middle section, and a suffix, by choosing split points
from a uniform distribution across the document’s length at the character level. We then
rearrange these segments into two configurations: prefix-suffix-middle (PSM) for half of
the splits and suffix-prefix-middle (SPM) for the remainder, ensuring compatibility. This
reorganization is executed with a 50% likelihood, meaning each configuration is applied
with a 25% probability.
5

--- PAGE 6 ---
Model Size HumanEval MBPP
pass@1 pass@1
CodeLlama 7B 33.5% 49.8%
SDSAT (L=3) 7B 33.5% 48.6%
SDSAT (L=5) 7B 31.1% 49.8%
CodeLlama 13B 36.0% 51.0%
SDSAT (L=7) 13B 38.4% 51.4%
Table 1: HumanEval Pass@1 scores and MBPP-sanitized Pass@1 scores with greedy search
decoding.
3.2 Training details
In our study, we configured the finetuning parameters by referencing those outlined in
the CodeLlama paper. The optimizer is AdamW (Loshchilov & Hutter, 2017), with β1and
β2values set at 0.9 and 0.95, respectively. Our approach includes a focused continuous
pretraining phase limited to just 2000 steps, utilizing a cosine schedule with 400 warm-up
steps. We set an initial learning rate of 5e-5, and a final learning rate of 1e-5. The training
process leverages a batch size of 4M tokens, which are organized into sequences of 16,384
tokens each. In the subsequent experiment, we trained both CodeLlama-7B and 13B models,
varying the max mask window size (L) for each to obtain multiple trained models, which
we refer to as SDSAT models. The semantic adaptive tokens used here are all the same
value. Models trained with diverse tokens were studied in Appendix B
3.3 Results
We conducted accuracy test for original CodeLlama-7B and 13B models. It is worth men-
tioning that when testing them on the following datasets, there are many details that can
affect the accuracy, such as ”stop words” and ”max new tokens” of the generation, whether
to do ”strip operation” on prompt, etc. If these details are inconsistent, the results will also
change accordingly. Since CodeLlama has not disclosed its testing specifics, re-evaluating
their released models results in slight discrepancies in accuracy compared to the figures
reported by them. We tentatively conclude that these minor differences due to the nuances
in the testing process do not detract from the overall conclusions of our work.
3.3.1 Python code generation
We reference the CodeLlama paper to select our test datasets. Similarly, we begin by
reporting results for Python code generation using the HumanEval (Chen et al., 2021) and
MBPP (Austin et al., 2021) benchmarks. For more precise evaluations, we use the test split
of MBPP (santitized), which contains 257 problems manually verified. Additionally, we
also conduct re-evaluations on the original version of CodeLlama. In this section, we only
present zero-shot results. Table 1 indicates that the accuracy of the 7B model is nearly
identical to the native CodeLlama model. Additionally, the 13B model exhibits slightly
higher overall accuracy compared to the native model.
3.3.2 Multilingual evaluation
Next, we also evaluate our fine-tuned models on a more diverse set of programming
languages using the MultiPL-E benchmark (Cassano et al., 2022). Just like CodeLlama, we
report results for Python, C++, Java, PHP , TypeScript, C#, and Bash in Table 2.
After training, the 7B model experienced a slight decrease in average accuracy, dropping
from 26.3% to 24.5%, with the most significant decline observed in the programming
language Java, C# and Bash. In contrast, the 13B model exhibited an increase in average
accuracy, particularly in the TypeScript (TS) language. It’s worth noting that this could be
6

--- PAGE 7 ---
due to inherent fluctuations in the original CodeLlama’s training process for TS, where the
7B model’s accuracy in TS even surpassed that of the 13B model.
In evaluating models using the HumanEval (Chen et al., 2021) and MultiPL-E (Cassano
et al., 2022), we observe fluctuations in the accuracy of models saved at different stages
of the training process. These fluctuations may be attributed to the limited size of the test
data available for each language. Additionally, it’s important to acknowledge that potential
biases in our randomly selected datasets could contribute to the fluctuating performance
across multiple coding languages.
Model Size C++ Java PHP TS C# Bash Average
CodeLlama 7B 28.6% 34.2% 24.2% 33.3% 25.3% 12.0% 26.3%
SDSAT (L=3) 7B 36.7% 29.7% 25.5% 30.8% 21.5% 8.9% 25.5%
SDSAT (L=5) 7B 32.9% 27.2% 24.2% 33.3% 20.3% 8.9% 24.5%
CodeLlama 13B 39.1% 38.0% 34.2% 29.6% 27.3% 15.2% 30.6%
SDSAT (L=7) 13B 39.1% 37.3% 33.5% 37.1% 27.9% 13.9% 31.5%
Table 2: MultiPL-E Pass@1 scores with greedy search decoding across various programming
languages.
3.3.3 Infilling evaluation
Allal et al. (2023) adapted the HumanEval code infilling benchmark for various program-
ming languages through the use of MultiPL-E (Cassano et al., 2022). In this process, indi-
vidual lines of code are masked, and the predictions are scored with an exact match metric
against the ground truth solution. From Table 3, we can see that our fine-tuned SDSAT
models and the original CodeLlama models exhibit similar performance across Python,
Java, and JavaScript. Notably, in Java, the fine-tuned models demonstrate a higher accuracy.
Given the large size of the test dataset, the results are relatively stable.
Model Size Python Java JS
CodeLlama 7B 72.70% 77.60% 82.60%
SDSAT (L=3) 7B 72.67% 78.37% 82.53%
SDSAT (L=5) 7B 72.29% 78.99% 82.12%
CodeLlama 13B 74.50% 80.00% 85.00%
SDSAT (L=7) 13B 73.54% 84.94% 84.36%
Table 3: Multilingual HumanEval single line infilling with MultiPL-E. Exact match rates
on the line infilling benchmark from Allal et al. (2023) with greedy decoding. Evaluated in
suffix-prefix-middle (SPM) format only.
3.4 Walltime improvement
To test the inference performance of the 7B and 13B models, we use a portion of HumanEval
(Chen et al., 2021) and MultiPL-E (Cassano et al., 2022) infilling datasets as representatives for
generation tasks and completion tasks respectively, and analyze their various performance
metrics, summarized in Table 4. We measure walltime improvements with a batch size of 1
on a single NVIDIA A100 for both greedy search (temp=0) and nucleus sampling (temp >0).
3.4.1 Greedy search
In the process of inference, by controlling the use of different numbers of adaptive tokens,
denoted here as k, we can observe the trend of changes in various performance indicators.
Askincreases, the loop time does not increase linearly but remains almost flat, which is a
significant reason why our algorithm can effectively improve inference speed. According to
7

--- PAGE 8 ---
Size L Temp Data Tokens/s Speed
CodeLlama SDSAT
7B 50HumanEval 37.09 114.92 >3.0X
MultiPL-E infilling 35.93 110.1 >3.0X
0.2HumanEval 36.53 107.59 >2.9X
MultiPL-E infilling 34.24 87.11 >2.5X
1HumanEval 36.66 81.45 >2.2X
MultiPL-E infilling 34.16 81.45 >2.3X
13B 70HumanEval 29.54 105.1 >3.5X
MultiPL-E infilling 27.35 87.53 >3.2X
0.2HumanEval 28.63 99.05 >3.4X
MultiPL-E infilling 26.12 66.64 >2.5X
1HumanEval 28.53 69.84 >2.4X
MultiPL-E infilling 25.95 63.42 >2.4X
Table 4: Comparison of speed. Notably, SDSAT’s selection for ”Tokens per Second” (token/s)
showcases the highest speed across all kvalues. Within the nucleus sampling method, the k
value also corresponds to the tree-depth. It is important to note that the kvalue is exclusive to
the SDSAT model. For the CodeLlama model, the kvalue is inherently set to 0. Additionally,
a temperature setting of 0 indicates the utilization of greedy search.
Figure 3: Performance of SDSAT-7B (L=5). Left: Accept rate, which means the average
number of tokens accepted divided by the number of adaptive tokens. Right: Tokens per
second of the generated new tokens.
Figure 3, the ”Tokens per Second” metric clearly shows that as k gradually increases, the
speed continues to rise, Specifically, at k=13, there is an approximate 3.1X increase in speed
on the HumanEval dataset (Chen et al., 2021). Additionally, the ”Accept Rate” aligns with
our intuitive understanding that for more distant adaptive tokens, as they are further from
the effective context, the probability of accurate prediction decreases.
For the 13B model, as seen from Figure 4, due to its stronger learning capabilities, its ”Accept
Rate” is higher compared to the 7B model, which directly affects its speed improvement
ratio. Specifically, at k=13, there is a more than 3.5X increase in speed on the HumanEval
dataset.
3.4.2 Nucleus sampling
We set the parameters to top k=10, top p=0.95, and conduct experiments with two tempera-
ture settings: 0.2 and 1. For this generation approach, we design the number of adaptive
tokens (tree depth) from 5 to 13 and compare it against the traditional nucleus sampling
method.
8

--- PAGE 9 ---
Figure 4: Performance of SDSAT-13B (L=7). Left: Accept rate. Right: Tokens per second of
the generated new tokens
From the data presented in Figure 3 and 4, it’s observable that under the nucleus sampling
method, the 13B model exhibits faster inference speeds compared to the 7B model. Addi-
tionally, a lower temperature setting results in further acceleration of inference speed. This
phenomenon can be attributed to two main factors: firstly, the use of tree attention requires
providing the model with longer inputs during draft step 2; more critically, employing a
sampling algorithm with a higher temperature introduces greater uncertainty, which in turn
lowers the acceptance rate.
Several conclusions can be drawn from the overall analysis: 1) The larger the model size,
the more effective the learning outcome of semantic adaptive tokens, which is reflected in
both higher acceptance rates and speed improvements. 2) As the number of adaptive tokens
increases, the speed benefit becomes more pronounced. 3) Generally, a lower temperature
setting results in greater speed gains.
4 Conclusion
This paper utilizes speculative decoding by introducing semantic adaptive tokens, enabling
existing LLMs to generate accurate draft tokens with minimal cost and without any struc-
tural modifications to the model. This approach significantly enhances the generation speed
of the models while maintaining nearly unchanged accuracy through optimized training.
Additionally, we have designed universal greedy search and nucleus sampling schemes
that can be easily transferred to other LLMs.
References
Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Car-
los Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al.
Santacoder: don’t reach for the stars! arXiv preprint arXiv:2301.03988 , 2023.
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David
Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with
large language models. arXiv preprint arXiv:2108.07732 , 2021.
Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, and Tri Dao. Medusa: Simple
framework for accelerating llm generation with multiple decoding heads, 2023.
Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin,
Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feld-
man, et al. Multipl-e: A scalable and extensible approach to benchmarking neural code
generation. arXiv preprint arXiv:2208.08227 , 2022.
9

--- PAGE 10 ---
Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre,
and John Jumper. Accelerating large language model decoding with speculative sampling.
arXiv preprint arXiv:2302.01318 , 2023.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,
Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evalu-
ating large language models trained on code. arXiv preprint arXiv:2107.03374 , 2021.
Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee, and Di He. Rest: Retrieval-based
speculative decoding. arXiv preprint arXiv:2311.08252 , 2023.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural
text degeneration. arXiv preprint arXiv:1904.09751 , 2019.
Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via
speculative decoding. In International Conference on Machine Learning , pp. 19274–19286.
PMLR, 2023.
Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Cheng-
hao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the
source be with you! arXiv preprint arXiv:2305.06161 , 2023.
Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling
requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077 , 2024.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. Learning,Learning ,
Nov 2017.
Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang,
Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming Chen,
Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative
large language model serving with tree-based speculative inference and verification, 2024.
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,
Yossi Adi, Jingyu Liu, Tal Remez, J ´er´emy Rapin, et al. Code llama: Open foundation
models for code. arXiv preprint arXiv:2308.12950 , 2023.
Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi,
Riccardo Marin, and Emanuele Rodol `a. Accelerating transformer inference for translation
via parallel decoding. arXiv preprint arXiv:2305.10427 , 2023.
Benjamin Spector and Chris Re. Accelerating llm inference with staged speculative decoding.
arXiv preprint arXiv:2308.04623 , 2023.
Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep
autoregressive models, 2018.
Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu Wei, and Zhifang Sui. Speculative de-
coding: Exploiting speculative execution for accelerating seq2seq generation. In Findings
of the Association for Computational Linguistics: EMNLP 2023 , pp. 3909–3925, 2023.
Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Ros-
tamizadeh, Sanjiv Kumar, Jean-Fran c ¸ois Kagy, and Rishabh Agarwal. Distillspec: Im-
proving speculative decoding via knowledge distillation. arXiv preprint arXiv:2310.08461 ,
2023.
A Training loss
Figure 5 presents the loss curves for the two approaches. The loss of the basic training ap-
proach consistently exceeds that of the improved approach, which will affect the benchmark
of the model.
10

--- PAGE 11 ---
Figure 5: The loss curves of standard tokens corresponding to two different training methods.
The loss of standard tokens is calculated by excluding the loss associated with adaptive
tokens and computing the average loss across all standard tokens.
B Adaptive token impact
We also conducted experiments to investigate the effects of training models with diverse
semantic adaptive tokens. Our experiments indicate that using varied tokens does not
significantly impact accuracy compared to using identical tokens. However, training with
diverse tokens restricts us from arbitrarily extending the length of k; we can only use
tokens with a length less than L. This limitation might constrain flexibility during inference.
Therefore, we recommend training models with identical tokens.
Figures 6 and 7 illustrate the results obtained by employing this method, with models
trained using diverse adaptive tokens represented by SDSAT-D.
Figure 6: Performance of SDSAT-D-7B (L=5) trained with diverse adaptive tokens ([32011]-
[32012] are used for CodeLlama). Left: Accept rate. Right: Tokens per second of the
generated new tokens.
11

--- PAGE 12 ---
Figure 7: Performance of SDSAT-D-13B (L=7) trained with diverse adaptive tokens ([32011]-
[32012], [31864] and [31857] are used for CodeLlama). Left: Accept rate. Right: Tokens per
second of the generated new tokens.
Model Size HumanEval MBPP
pass@1 pass@1
CodeLlama 7B 33.5% 49.8%
SDSAT-D (L=5) 7B 32.3% 48.3%
CodeLlama 13B 36.0% 51.0%
SDSAT-D (L=7) 13B 38.4% 51.0%
Table 5: HumanEval Pass@1 scores and MBPP-sanitized Pass@1 scores.
Model Size C++ Java PHP TS C# Bash Average
CodeLlama 7B 28.6% 34.2% 24.2% 33.3% 25.3% 12.0% 26.3%
SDSAT-D (L=5) 7B 31.7% 26.6% 27.3% 33.3% 19.6% 10.1% 24.8%
CodeLlama 13B 39.1% 38.0% 34.2% 29.6% 27.3% 15.2% 30.6%
SDSAT-D (L=7) 13B 39.1% 36.1% 32.9% 39.0% 26.0% 14.6% 31.3%
Table 6: MultiPL-E Pass@1 scores. Using greedy decoding in different programming
languages.
Model Size Python Java JS
CodeLlama 7B 72.70% 77.60% 82.60%
SDSAT-D (L=5) 7B 71.42% 78.31% 82.63%
CodeLlama 13B 74.50% 80.00% 85.00%
SDSAT-D (L=7) 13B 73.53% 85.24% 84.76%
Table 7: Multilingual HumanEval single line infilling with MultiPL-E.
12
