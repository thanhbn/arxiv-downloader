# 2402.13720.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/speculative/2402.13720.pdf
# Kích thước tệp: 803689 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Ouroboros: Tạo ra Bản Th草 Dài Hơn Theo Từng Cụm Từ
để Giải Mã Suy Đoán Nhanh Hơn
Weilin Zhao1*, Yuxiang Huang1∗, Xu Han1,2†, Wang Xu1, Chaojun Xiao1,
Xinrong Zhang1,Yewei Fang3,Kaihuo Zhang3,Zhiyuan Liu1†,Maosong Sun1
1Khoa Khoa học Máy tính và Công nghệ, Viện Trí tuệ Nhân tạo,
Trung tâm Nghiên cứu Quốc gia Bắc Kinh về Khoa học và Công nghệ Thông tin,
Đại học Thanh Hoa, Bắc Kinh, Trung Quốc.
2Phòng thí nghiệm Trí tuệ Nhân tạo Thượng Hải, Thượng Hải, Trung Quốc.3ModelBest Inc.
{zwl23,huang-yx21}@mails.tsinghua.edu.cn, {hanxu2022,liuzy}@tsinghua.edu.cn
Tóm tắt
Giải mã suy đoán là một phương pháp được sử dụng rộng rãi
nhằm tăng tốc quá trình tạo ra của các mô hình ngôn ngữ lớn (LLM) mà không có sự
thỏa hiệp nào trong hiệu suất mô hình. Nó đạt được
mục tiêu này bằng cách sử dụng một mô hình nhỏ hơn hiện có
để soạn thảo và sau đó sử dụng LLM đích để xác minh bản thảo theo cách song song
chi phí thấp. Dưới khung soạn thảo-xác minh
như vậy, hiệu quả soạn thảo đã trở thành một
nút thắt cổ chai trong tốc độ cuối cùng của giải
mã suy đoán. Do đó, tạo ra các bản thảo dài hơn
với chi phí ít hơn có thể dẫn đến tốc độ giải mã
tốt hơn. Để đạt được điều này, chúng tôi giới thiệu
Ouroboros, có thể tạo ra các cụm từ thảo
để song song hóa quá trình soạn thảo và đồng thời
làm dài các bản thảo theo cách không cần đào tạo.
Kết quả thực nghiệm trên các nhiệm vụ tạo văn bản
điển hình khác nhau cho thấy Ouroboros có thể
đạt tốc độ tăng lên đến 2.8× so với giải
mã suy đoán và 3.9× so với giải mã vanilla,
mà không cần tinh chỉnh các mô hình thảo và đích.
Mã nguồn của Ouroboros có sẵn tại
https://github.com/thunlp/Ouroboros.
1 Giới thiệu
Hưởng lợi từ những tiến bộ gần đây trong các thiết bị
tính toán song song và thuật toán đào tạo phân tán
(Shoeybi et al., 2019; Rasley et al., 2020),
thời gian đào tạo của LLM đã được rút ngắn đáng kể.
Tuy nhiên, vẫn còn thách thức để
đạt được song song hóa trong suy luận mô hình vì
hầu hết các LLM dựa trên cơ chế tạo
tự hồi quy thể hiện sự phụ thuộc tuần tự
giữa các token được tạo ra. Đối với những
LLM tự hồi quy này, chúng phải tạo token từng cái một.
Để tăng tốc suy luận của LLM, các phương pháp
nén mô hình điển hình như lượng tử hóa
(Frantar et al., 2023; Lin et al., 2023; Kim
et al., 2023) và cắt tỉa (Han et al., 2015; Wang
*cho biết đóng góp bằng nhau.
†cho biết tác giả tương ứng.
0.068 1.1 7 13
Kích thước Mô hình Thảo (B)15.015.516.016.517.0Tốc độ Giải mã (token/s)
4 6 8 10
Độ dài Thảo (#token)
 của Mô hình Thảo 7B16.517.017.518.0Tốc độ Giải mã (token/s)1234
#Token Thảo Chính xác (Trung bình)3.03.54.04.5
#Token Thảo Chính xác (Trung bình)Hình 1: Sự đánh đổi giữa hiệu quả và hiệu suất soạn thảo.
Hình minh họa tốc độ giải mã suy đoán tối ưu của
Llama-2-chat-70B trên MT-Bench (Zheng et al., 2024) và
số lượng token thảo chính xác trung bình tương ứng tại mỗi vòng lặp.
et al., 2020; Xia et al., 2023b) có thể gây suy giảm
hiệu suất mô hình và đôi khi thậm chí yêu cầu
chi phí đào tạo bổ sung không đáng kể. Để
tăng tốc suy luận của LLM mà không mất mát,
giải mã suy đoán (Xia et al., 2023a; Leviathan et al.,
2023; Chen et al., 2023a) đã được đề xuất. Với
một LLM đích, giải mã suy đoán chọn một
LLM nhỏ hơn nhiều làm mô hình thảo để tạo ra
nhiều token như một bản thảo. LLM đích sau đó xem xét
bản thảo một cách song song, chấp nhận tiền tố chính xác dài nhất
trong khi loại bỏ các token thảo còn lại.
Dưới khung soạn thảo-xác minh như vậy,
cách tạo ra các bản thảo dài và chính xác một cách hiệu quả
đã trở thành một yếu tố quan trọng trong việc tăng tốc
suy luận LLM. Như trong Hình 1, chúng tôi kiểm tra sự đánh đổi
giữa kích thước mô hình thảo và tốc độ giải mã suy đoán.
Chúng tôi quan sát thấy các mô hình thảo lớn hơn có xu hướng
đạt độ chính xác thảo cao hơn, nhưng chi phí
tạo ra bản thảo cũng tăng lên, do đó các mô hình
cỡ trung bình mang lại tốc độ giải mã tốt nhất. Bên cạnh đó,
chúng tôi điều tra sự đánh đổi giữa độ dài thảo
và tốc độ giải mã suy đoán. Những phát hiện của chúng tôi
chỉ ra rằng mô hình đích có thể chấp nhận nhiều token hơn
trên mỗi vòng lặp trung bình khi bản thảo dài hơn,
nhưng các bản thảo dài hơn tạo ra nhiều lượt chuyển tiến
cần thiết cho mô hình thảo, và do đó, độ dài thảo tối ưu
trong giải mã suy đoán không phải là lớn nhất.arXiv:2402.13720v3  [cs.CL]  15 Tháng 10 2024

--- TRANG 2 ---
mal draft length in speculative decoding is not the
largest one.
Dựa trên phân tích sâu hơn về các kết quả thực nghiệm thí điểm
ở trên, không khó để quan sát những
hạn chế của giải mã suy đoán về hiệu quả soạn thảo:
(1) Bản Thảo Không Đủ. Soạn thảo quá ngắn bỏ lỡ
khả năng tăng tốc tiềm năng. Tuy nhiên,
vì chi phí thời gian liên quan đến chính việc soạn thảo,
tạo ra các bản thảo dài có thể dẫn đến chi phí
cao khi những bản thảo này thất bại. Nếu có thể tạo ra
các bản thảo dài hơn một cách hiệu quả hơn, hiệu ứng tăng tốc
sẽ được tăng cường đáng kể. (2) Bản Thảo Chưa Được
Sử Dụng Hết. Đối với giải mã suy đoán hiện tại, những token
không được mô hình đích chấp nhận bị loại bỏ hoàn toàn,
dẫn đến chi phí thất bại cao khi tạo ra các bản thảo dài.
Thực tế, một số token bị loại bỏ có thể chứa thông tin
hữu ích có thể được sử dụng trong các vòng lặp soạn thảo
tương lai.
Để vượt qua những hạn chế trên, bài báo này
giới thiệu một khung giải mã hiệu quả hơn
có tên Ouroboros, với những cải tiến so với giải
mã suy đoán như sau:
(1) Tăng tốc soạn thảo qua cụm từ. Vì
việc tạo ra mô hình bị ràng buộc bởi bộ nhớ hơn là
tính toán (Leviathan et al., 2023), soạn thảo ở cấp độ
cụm từ thay vì cấp độ token có thể làm cho giai đoạn
soạn thảo hiệu quả hơn trong việc tạo ra các bản thảo dài hơn.
Được truyền cảm hứng từ những nỗ lực trước đây
như giải mã nhìn trước (Fu et al., 2023) đã
thành công sử dụng cụm từ để tăng tốc mô hình
đích, Ouroboros điều chỉnh phương pháp này để
nâng cao hiệu quả soạn thảo. Trong các phần tiếp theo,
chúng tôi sẽ chứng minh rằng tăng tốc mô hình
thảo với cụm từ trước và sau đó sử dụng
bản thảo để tăng tốc mô hình đích sẽ hiệu quả hơn
so với việc trực tiếp tăng tốc mô hình đích với cụm từ.
(2) Làm dài bản thảo qua cụm từ. Việc
nối các cụm từ có thể mở rộng bản thảo thậm chí dài hơn
với chi phí thấp. Theo token cuối cùng của bản thảo,
các cụm từ bắt đầu bằng token cuối có thể được sử dụng để
mở rộng bản thảo. Bằng cách nối bản thảo và
k cụm từ khác nhau, Ouroboros tạo ra k bản thảo
dài hơn để mô hình đích xác minh, tạo ra
chi phí bổ sung gần như bằng không.
(3) Tạo ra cụm từ từ xác minh. Trong
giai đoạn xác minh, Ouroboros lọc ra
các cụm từ chất lượng cao từ những token bị loại bỏ
trong giải mã suy đoán. Những cụm từ bị loại bỏ này
có thể được sử dụng để tăng tốc soạn thảo trong các
vòng lặp tiếp theo.
(4) Tái sử dụng cụm từ từ ngữ cảnh lịch sử. Chúng tôi
phát hiện rằng các cụm từ được tạo ra từ các nhiệm vụ tương tự có thể
được tái sử dụng để tăng tốc soạn thảo với nhau.
Ouroboros do đó tái sử dụng các cụm từ trong ngữ cảnh lịch sử
để tăng tốc soạn thảo trong các vòng lặp tiếp theo.
Đáng chú ý, Ouroboros không yêu cầu bất kỳ
đào tạo bổ sung nào, và có thể được áp dụng trong tất cả
các ứng dụng với giải mã suy đoán. Chúng tôi
triển khai Ouroboros và tiến hành đủ thực nghiệm
trên các nhiệm vụ tạo văn bản khác nhau như
tạo mã (Chen et al., 2021; Austin et al.,
2021), tóm tắt văn bản (See et al., 2017; Hermann et al., 2015) và dịch máy (Bojar et al., 2016). Kết quả thực nghiệm cho thấy
Ouroboros hoàn toàn không mất mát về hiệu suất nhiệm vụ
và có thể đạt được tăng tốc suy luận đáng kể mà không cần
tinh chỉnh mô hình bổ sung. So với các phương pháp giải mã
cạnh tranh gần đây, Ouroboros đạt tốc độ tăng lên đến
1.9× so với giải mã nhìn trước, lên đến 2.8× so với
giải mã suy đoán, và lên đến 3.9× so với giải mã
tự hồi quy naïve.
2 Cơ sở lý thuyết
Đối với một mô hình đích T để tăng tốc suy luận, giải mã
suy đoán đầu tiên tìm một mô hình thảo phù hợp S.
Với một tiền tố đầu vào x1···n, giải mã suy đoán
sử dụng mô hình thảo S để tạo ra một bản thảo d1···γ
gồm γ token theo cách tự hồi quy,
d1= arg max
d1PS(d1|x1···n),
di= arg max
diPS(di|x1···n, d1···i−1), i= 2···γ.(1)
Sau đó, mô hình đích T xác minh bản thảo d1···γ,
bằng cách nhập d1···γ và thực hiện nhiệm vụ
dự đoán token tiếp theo bằng một lượt chuyển tiến duy nhất.
Mỗi kết quả xác minh vi được lấy mẫu từ phân phối
dự đoán token tiếp theo của mô hình đích,
v1∼PT(· |x1···n),
vi∼PT(· |x1···n, d1···i−1), i= 2···γ.(2)
Phải tồn tại một A∈[0, γ] thỏa mãn
v1···A=d1···A,
vA+1̸=dA+1hoặc A=γ.(3)
Ở đây, v1···A+1 sẽ giống như kết quả khi
mô hình đích tự hồi quy tạo ra A+ 1
token từ x, vì tại thời điểm này,
v1∼PT(· |x1···n),
vi∼PT(· |x1···n, v1···i−1), i= 2···A+ 1.(4)
Chúng tôi gọi A là số lượng token chính xác trong
bản thảo, A+ 1 token d1···A, vA+1 được chấp nhận như

--- TRANG 3 ---
kết quả tạo ra của mô hình đích trong khi
các token thảo còn lại dA+1···γ bị loại bỏ.
Xin lưu ý rằng việc suy luận ở trên khác một chút
so với giải mã suy đoán ban đầu (Chen et al.,
2023a): chúng tôi đặt nhiệt độ cho việc lấy mẫu ngẫu nhiên
trong mô hình thảo thành 0 để giải thích rõ ràng hơn,
nhưng điều này sẽ không ảnh hưởng đến tính đúng đắn của
mô hình đích.
Chúng tôi định nghĩa A(γ) là số lượng token chính xác
trung bình khi mô hình thảo là S và độ dài
thảo là γ. Tốc độ tăng của giải mã suy đoán
được tính như sau (Leviathan et al., 2023),
trong đó tS và tT là thời gian chuyển tiến của mô hình
thảo và mô hình đích, tương ứng,
[A(γ) + 1]·tT
γ·tS+tT, (5)
trong đó tử số là thời gian cho việc tạo ra vanilla
của mô hình đích, và mẫu số
là thời gian của giải mã suy đoán.
3 Phương pháp
Trong Ouroboros, chúng tôi đầu tiên tăng tốc soạn thảo qua
cụm từ, để chúng tôi có thể tạo ra γ token với
ít lượt chuyển tiến hơn của mô hình thảo, và chúng tôi
ký hiệu tỷ lệ giảm này là c. Tiếp theo, do
thực tế rằng hàm A(γ) là đơn điệu
không giảm, chúng tôi làm dài bản thảo qua cụm từ, cho phép
tạo ra nhiều token hơn mà không cần
lượt chuyển tiến bổ sung. Chúng tôi ký hiệu
độ dài mở rộng không chi phí này là β. Theo hai
hướng tối ưu hóa này, Công thức (5) trở thành
[A(γ+β) + 1]·tT
1
cγ·tS+tT. (6)
Chúng tôi tiếp tục tăng c bằng cách tạo ra
cụm từ một cách heuristic từ kết quả xác minh và tái sử dụng
cụm từ từ ngữ cảnh được tạo ra trong lịch sử, có thể
dẫn đến hiệu quả soạn thảo tốt hơn. Tiếp theo, chúng tôi sẽ
đi sâu vào chi tiết cho từng phần của Ouroboros.
3.1 Tăng tốc Soạn thảo qua Cụm từ
Chúng tôi lấy cảm hứng từ giải mã nhìn trước (Fu
et al., 2023), sử dụng cụm từ để trực tiếp tăng tốc
mô hình đích T. Tuy nhiên, mỗi vòng soạn thảo
cụm từ yêu cầu một lượt chuyển tiến của mô hình
đích T để xác minh bản thảo, hạn chế toàn bộ
hiệu ứng tăng tốc của giải mã nhìn trước. Khác
với giải mã nhìn trước, chúng tôi sử dụng cụm từ để
gián tiếp tăng tốc mô hình đích T thông qua
một mô hình thảo S, cho phép mỗi lượt chuyển tiến của
mô hình đích đồng thời xác minh nhiều
vòng cụm từ, đạt được tăng tốc tốt hơn.
Như được hiển thị trong Hình 2, trong Ouroboros, quá trình
soạn thảo của mô hình thảo được thực hiện từng cụm từ
thay vì từng token. Nhiều cụm từ mới
được tạo ra song song trong mỗi lượt chuyển tiến
của mô hình thảo. Chúng tôi sẽ không đi vào chi tiết
cách tạo ra các cụm từ thảo mới song song do
hạn chế về không gian, và có thể tham khảo Fu et al. (2023).
3.2 Làm dài Bản thảo qua Cụm từ
Vì việc tạo ra mô hình bị ràng buộc bởi bộ nhớ, thời gian
mô hình đích mất để xác minh hàng chục token bằng
một lượt chuyển tiến duy nhất không khác nhiều so với thời gian
dành cho việc xác minh một token duy nhất (Leviathan et al., 2023).
Do đó, chúng tôi đề xuất sử dụng cụm từ để mở rộng
bản thảo một cách heuristic vì việc nối cụm từ gần như
không tốn chi phí.

--- TRANG 4 ---
𝑑!𝑑"⋯𝑑#𝑝"!𝑝$!𝑝""𝑝$"𝑥!⋯𝑥%𝑑!𝑑"⋯𝑑#𝑝""𝑝$"𝑝"!𝑝$!Tiền tốBản thảoCụm từ 1Cụm từ 2𝑥!⋯𝑥%𝑑!𝑑"⋯𝑑#𝑝"!𝑝$!𝑝""𝑝$"QKCụm từHình 3: Cơ chế che attention tùy chỉnh
để xác minh các bản thảo được làm dài.
Thử nhiều cụm từ để có được nhiều bản thảo dài hơn
có thể tăng xác suất một trong những cụm từ này
vượt qua xác minh của mô hình đích.
Nhưng chi phí xác minh những bản thảo dài hơn này từng
cái một cũng không thể chịu được. Được truyền cảm hứng
từ Cai et al. (2024), chúng tôi xây dựng một cơ chế
che attention Transformer tinh vi để hoàn thành việc xác minh
tất cả các bản thảo dài hơn chỉ với một lượt chuyển tiến
của mô hình đích T, như được hiển thị trong Hình 3.
Cụ thể, với một bản thảo d1···γ, chúng tôi chọn ra K
cụm từ p1
1···β,···, pK
1···β bắt đầu bằng token dγ,
tức là p1
1=p2
1=···=pK
1=dγ. K bản thảo
được làm dài là

d1···γ, p1
2···β,
d1···γ, p2
2···β,
...
d1···γ, pK
2···β.(7)
Mô hình đích sau đó tính toán vj
i để xác minh pj
i cho
tất cả j= 2···K, i= 2···β+ 1 trong Công thức (8). Tất cả vj
i
được tính toán song song bằng một lượt chuyển tiến duy nhất
sử dụng cơ chế che attention trong Hình 3.
vj
i∼PT(· |x1···n, d1···γ, pj
2···i−1) (8)
Tương tự như Công thức (3), chúng tôi định nghĩa ˆAj thỏa mãn
(vj
2···ˆAj=pj
2···ˆAj,
vj
ˆAj+1̸=pj
ˆAj+1hoặc ˆAj=β.(9)
Một khi bản thảo d1···γ được mô hình đích chấp nhận
hoàn toàn, chúng tôi sử dụng cụm từ pj với ˆAj lớn nhất
để tăng số lượng token được chấp nhận. Cụ thể hơn,
d1···γ, pj
2···ˆAj, vj
ˆAj+1 sẽ được chấp nhận.

Yi-base 34B/6B MBPP CNN/DM WMT-16
A 12.7 8.4 5.3
#Match 18.9 17.8 17.0
Bảng 1: Token khớp trung bình so với
số lượng token được chấp nhận. Thực nghiệm trên Yi-
base 34B/6B (Young et al., 2024) trên ba bộ dữ liệu,
MBPP (Austin et al., 2021), CNN/DM (See et al., 2017;
Hermann et al., 2015) và WMT16 (Bojar et al., 2016)
sử dụng Giải mã Suy đoán, với độ dài thảo γ= 20.
3.3 Tạo ra Cụm từ từ Xác minh
Trong giải mã suy đoán, chi phí khi một bản thảo thất bại
tương đối cao. Nếu bản thảo d1···γ không được chấp nhận
hoàn toàn, xác minh truyền thống chỉ chấp nhận tiền tố
thảo đúng d1···A và loại bỏ dA+1···γ, như trong
Phần 2. Để đánh giá thông tin trong những
token thảo bị loại bỏ này, chúng tôi định nghĩa #Match (d1···γ, v1···γ)
là số lượng token khớp của bản thảo và
kết quả xác minh,
#Match (d1···γ, v1···γ) =γX
i=1[di=vi], (10)
trong đó [di=vi] = 1 khi di=vi và 0 ngược lại.
Như trong Bảng 1, chúng tôi thấy rằng #Match lớn hơn
nhiều so với độ dài của tiền tố chính xác A. Chúng tôi
xem xét một số trường hợp cụ thể và thấy rằng đôi khi
sự xuất hiện của tình huống này là do sự
đặt sai chỗ của việc tạo ra, như được hiển thị trong Hình
4. Do đó, chúng tôi đề xuất lọc ra những
phân đoạn con của bản thảo bị loại bỏ khớp với
kết quả xác minh của mô hình đích, như được hiển thị
trong Hình 4. Sau đó chúng tôi chèn những phân đoạn đó vào
pool cụm từ để tăng tốc các bản thảo tương lai.
Như trong Phần 3.2, nếu bản thảo d1···γ được chấp nhận
hoàn toàn, chỉ có một hậu tố cụm từ pj được sử dụng,
để lại tất cả các cụm từ được thử khác không được sử dụng.
Chúng tôi giả định kết quả xác minh vo
2···β sửa một số
lỗi trong cụm từ po
2···β cho tất cả o̸=j: chúng tôi sử dụng
po
1, vo
2···β để thay thế cụm từ po
1···β trong pool cụm từ.
3.4 Tái sử dụng Cụm từ từ Ngữ cảnh Lịch sử
Trong các tình huống thực tế, những cuộc hội thoại liền kề
từ người dùng có thể thể hiện sự tương tự. Tận dụng
những điểm tương đồng này, chúng tôi có thể tiếp tục nâng cao
tốc độ suy luận của mô hình. Khác với giải mã nhìn trước
làm sạch các cụm từ từ ngữ cảnh lịch sử khi tiến hành
việc tạo ra các truy vấn đầu vào tiếp theo, chúng tôi đề xuất
tái sử dụng các cụm từ được tạo ra từ các lần tạo ra
trước đó để tiếp tục tăng tốc quá trình soạn thảo.

--- TRANG 5 ---
[P1,2,4,6,8Xác minh Song song(1 Lượt chuyển tiến)[2,424,6,,1.Thảo×112.Xác minh×1
[P2,4,6,8],8
3.Thảo×74,6,8]
P[2,46,8],4.Xác minh×1Thảo Lặp lại!5.Kết quả đầu raGiải mã Suy đoánOuroboros[P1,2,4,6,8Xác minh Song song(1 Lượt chuyển tiến)[2,424,6,,1.Thảo×112.Xác minh×1
[P2,4,6,8],8
4,6,8]
P[2,46,8],4.Xác minh×15.Kết quả đầu ra,6,8]
Thảo Song song(1 Lượt chuyển tiến)3.Thảo×3Lượt chuyển tiến của mô hình thảoLượt chuyển tiến của mô hình đích
,
Xác minh Song song(1 Lượt chuyển tiến)
Hầu hết Đúng nhưng Đặt sai chỗ
,
Xác minh Song song(1 Lượt chuyển tiến)
,6,8,[x*2 for x in [1,2,3,4]]=P=
Lượt chuyển tiến ít hơn[x*2 for x in [1,2,3,4]]=P=Cụm từHình 4: Minh họa việc tạo ra cụm từ từ kết quả xác minh của mô hình đích.
3.5 Những Ưu điểm của Khung Không cần Đào tạo
Ouroboros hoàn toàn không cần đào tạo. Chúng tôi không
sử dụng các phương pháp như chưng cất mô hình để
tăng hàm A(γ) hoặc nén mô hình
để giảm tS trong Công thức (5). Về mặt tạo ra
cụm từ, tất cả các chiến lược tạo ra cụm từ của chúng tôi
đều là heuristic. Tất cả các cụm từ được tích lũy dần dần
trong quá trình tạo ra của mô hình, mà không cần
chuẩn bị trước trên kho ngữ liệu quy mô lớn. Tất cả
những điều này có nghĩa là, với một mô hình thảo trong
bất kỳ phương pháp giải mã suy đoán nào, chúng tôi có thể
sử dụng Ouroboros để giúp những phương pháp này đạt được
tăng tốc hơn nữa mà không tạo ra chi phí bổ sung.
4 Thực nghiệm
Phần này tập trung vào đánh giá Ouroboros trên
các nhiệm vụ tạo văn bản khác nhau để chứng minh
hiệu quả và hiệu suất của Ouroboros.
4.1 Cài đặt Thực nghiệm
Để đánh giá tăng tốc tổng thể do
Ouroboros gây ra, chúng tôi đánh giá Ouroboros trên các
nhiệm vụ tạo văn bản điển hình khác nhau, bao gồm tạo mã,
lý luận số học, tóm tắt tài liệu,
và dịch máy.
Bộ dữ liệu. Đối với tạo mã, chúng tôi đánh giá
Ouroboros trên HumanEval (Chen et al., 2021) và
tập validation của MBPP (Austin et al., 2021).
Có 164 mục trong HumanEval và chúng được
tạo thành từ một lời nhắc văn bản và một tiền tố của hàm Python.
Tập validation của MBPP có 90 mục,
trong đó toàn bộ hàm được mong đợi để
dự đoán với một lời nhắc văn bản và các trường hợp thử nghiệm đã cho.
Độ dài tạo ra tối đa trên HumanEval
và MBPP được đặt thành 512. Đối với lý luận số học,
tóm tắt tài liệu và dịch máy, chúng tôi đánh giá
phương pháp của chúng tôi trên GSM8K (Cobbe
et al., 2021), CNN/DM (See et al., 2017; Hermann
et al., 2015) và WMT16 (Bojar et al., 2016),
tương ứng. Chúng tôi lấy mẫu ngẫu nhiên 100 mục từ
GSM8K và CNN/DM, và lấy mẫu 100 mục
từ tập con dịch tiếng Đức sang tiếng Anh của
WMT16. Độ dài tạo ra tối đa trên
GSM8k, CNN/DM và WMT16 tương ứng
được đặt thành 256, 128, và 64.
Mô hình. Đối với HumanEval và MBPP, chúng tôi sử dụng
Yi-base-34B/6B (Young et al., 2024), DeepSeek-
coder-instruct-33b/6.7B (Bi et al., 2024) và
CodeLlama-instruct-34B/7B (Roziere et al., 2023)
làm mô hình đích/thảo cho các thực nghiệm của chúng tôi. Đối với
GSM8K, CNN/DM và WMT16, chúng tôi sử dụng Yi-base-
34B/6B và Llama-2-chat-70B/7B (Touvron et al.,
2023). Chúng tôi sử dụng mô hình lớn hơn làm mô hình đích
và mô hình nhỏ hơn để soạn thảo. Tất cả những mô hình này
đều là LLM đại diện và phổ biến gần đây.
Phương pháp Đánh giá. Các baseline của
thực nghiệm của chúng tôi là giải mã tự hồi quy vanilla,
giải mã suy đoán (Leviathan et al., 2023; Chen
et al., 2023a), và giải mã nhìn trước (Fu et al.,
2023). Chúng tôi báo cáo tốc độ giải mã (token/s) và

--- TRANG 6 ---
tỷ lệ tăng tốc so với giải mã tự hồi quy vanilla.
Nhiều siêu tham số khác của thực nghiệm của chúng tôi
được bao gồm trong Phụ lục B.
Phần cứng và Triển khai. Tất cả thực nghiệm
được thực hiện trên 2 ×NVIDIA 80GB A800
GPU với NVLINK ×8 được kết nối. CPU
được sử dụng là Intel(R) Xeon(R) Platinum 8350C. Chúng tôi
sử dụng gói transformers của Huggingface để
tiến hành song song hóa mô hình tự động cho các phương pháp
của chúng tôi và tất cả các baseline.
4.2 Kết quả Tổng thể
Chúng tôi tiến hành thực nghiệm trong cả tình huống
giải mã tham lam và lấy mẫu ngẫu nhiên. Kết quả
thực nghiệm được liệt kê như sau.
Giải mã tham lam. Như được hiển thị trong Hình 5, trong
tình huống tạo ra tham lam, Ouroboros vượt trội hơn
giải mã tham lam vanilla, giải mã nhìn trước,
và giải mã suy đoán dưới tất cả cấu hình
mô hình nền và bộ dữ liệu. Ouroboros có thể
đạt được tốc độ tạo ra lên đến 61.2 token/s với
độ dài tạo ra tối đa là 512, đạt được
tốc độ tăng 3.9× so với giải mã tham lam,
2.8× so với giải mã suy đoán, và 1.9×
so với giải mã nhìn trước. Bảng 2 cho thấy
Ouroboros cũng có thể có được tăng tốc đáng kể trên
các nhiệm vụ ngôn ngữ tự nhiên điển hình, nơi giải mã nhìn trước
và giải mã suy đoán chỉ có thể đạt được
tăng tốc hạn chế.
Lấy mẫu ngẫu nhiên. Chúng tôi thử nghiệm Ouroboros trong
tình huống lấy mẫu ngẫu nhiên sử dụng Llama-2-chat-
70B/7B, xem xét rằng những mô hình sau SFT
phù hợp hơn để hiểu tốt ngữ nghĩa phong phú
được phân phối trong ngôn ngữ tự nhiên của con người và
tạo ra kết quả đa dạng. Đối với các siêu tham số
lấy mẫu, nhiệt độ tạo ra được đặt
giữa 0.5 và 1.0, và top-p được đặt thành 0.8. Bảng
3 cho thấy Ouroboros cũng có thể được áp dụng cho
lấy mẫu ngẫu nhiên, và tốc độ tăng so với các phương pháp
baseline không khác nhiều so với các quan sát
trong tình huống giải mã tham lam.

Yi
34B/6BDeepSeek-coder
34B/6.7BCodeLlama
34B/7B0204060token/s
15.621.532.361.2
15.421.525.141.0
18.323.427.239.2HumanEval
Vanilla
Suy đoánNhìn trước
Ouroboros
Yi
34B/6BDeepSeek-coder
34B/6.7BCodeLlama
34B/7B010203040token/s
15.521.827.744.0
15.018.019.031.8
16.520.122.734.5MBPP
Vanilla
Suy đoánNhìn trước
OuroborosHình 5: Tốc độ giải mã tham lam (token/s) trên HumanEval và MBPP.
Nhiệm vụ Thuật toánYi 34B/6B Llama-2 70B/7B
token/s tăng tốc token/s tăng tốc
GSM8kVanilla 15.33 1.00 × 8.96 1.00 ×
Suy đoán 16.99 1.11 × 16.86 1.88 ×
Nhìn trước 25.14 1.64 × 13.77 1.54 ×
Ouroboros 28.23 1.84 × 24.03 2.68 ×
CNN/DMVanilla 14.62 1.00 8.12 1.00
Suy đoán 17.82 1.22 × 12.77 1.57 ×
Nhìn trước 18.77 1.28 × 9.47 1.17 ×
Ouroboros 22.65 1.55 × 14.67 1.81 ×
WMT16Vanilla 14.78 1.00 × 9.52 1.00 ×
Suy đoán 17.48 1.18 × 14.72 1.55 ×
Nhìn trước 17.98 1.22 × 14.65 1.54 ×
Ouroboros 19.94 1.35 × 19.27 2.02 ×
Bảng 2: Tốc độ giải mã tham lam (token/s) và
tỷ lệ tăng tốc trên GSM8K, CNN/DM và WMT16.
Nhiệm vụ Thuật toánTham lam Ngẫu nhiên
(temp=0.0) temp=0.5 temp=1.0
GSM8kVanilla 8.96 8.97 8.96
Suy đoán 16.86 14.99 14.11
Nhìn trước 13.77 13.56 13.46
Ouroboros 24.03 22.04 19.27
CNN/DMVanilla 8.12 8.83 8.30
Suy đoán 12.77 13.43 13.75
Nhìn trước 9.47 9.31 9.47
Ouroboros 14.67 14.97 14.23
WMT16Vanilla 9.52 9.75 9.75
Suy đoán 14.72 13.91 14.28
Nhìn trước 14.65 12.05 11.91
Ouroboros 19.27 19.95 19.33
Bảng 3: Tốc độ lấy mẫu ngẫu nhiên (token/s)
so với tốc độ giải mã tham lam (token/s), được thử nghiệm
trên Llama-2-chat 70B/7B. "temp" có nghĩa là nhiệt độ
được sử dụng trong tình huống lấy mẫu ngẫu nhiên.

--- TRANG 7 ---
Phương pháp token/s
Baseline 21.46
+ Tăng tốc soạn thảo qua cụm từ 49.90
+ Làm dài bản thảo qua cụm từ 55.92
+ Tạo ra cụm từ từ xác minh 58.18
+ Tái sử dụng cụm từ từ ngữ cảnh lịch sử 61.20
Bảng 4: Nghiên cứu loại bỏ của từng thành phần trong
Ouroboros trên HumanEval sử dụng Yi 34B/6B.
0 1 2 3 4 5 6 7 8 9 20
K5455565758token/s
Hình 6: Hiệu ứng của việc chọn K cụm từ cho Công thức (7),
được thử nghiệm trên HumanEval sử dụng Yi 34B/6B mà không tái sử dụng
cụm từ từ ngữ cảnh lịch sử.
4.3 Nghiên cứu Loại bỏ và Phân tích
Trong phần này, để có cái nhìn sâu hơn về cách
Ouroboros đạt được tốc độ tạo ra cao hơn, chúng tôi
tiến hành nghiên cứu loại bỏ và phân tích để trả lời
các câu hỏi sau.
Hiệu ứng của từng thành phần là gì? Để
chứng minh tăng tốc cụ thể được tạo ra bởi
từng cơ chế, kết quả loại bỏ được trong Bảng 4.
Trong bảng, mỗi thành phần có thể mang lại một mức tăng
tốc độ, trong khi tăng tốc soạn thảo và làm dài
bản thảo là những cái hiệu quả nhất.
Cần bao nhiêu cụm từ để làm dài
bản thảo? Xác minh quá nhiều cụm từ có thể
làm chậm xác minh trong các tình huống suy luận
mô hình thực tế. Ở đây xuất hiện một sự đánh đổi giữa
nhiều cụm từ hơn cho khả năng tăng tốc và xác minh
chậm hơn. Trong Hình 6, tồn tại một giá trị K tốt nhất,
ít hơn hoặc nhiều cụm từ hơn gây ra tốc độ giải mã chậm hơn.
Các thực nghiệm loại bỏ khác. Kết quả của
Ouroboros trên các cài đặt siêu tham số và độ dài
khác nhau được hiển thị trong Phụ lục B và Phụ lục E,
tương ứng. Việc loại bỏ tái sử dụng cụm từ trên
tính địa phương ngữ cảnh khác nhau được trong Phụ lục F.
4.4 So sánh với Các Phương pháp Dựa trên Đào tạo
Chúng tôi so sánh Ouroboros không cần đào tạo của chúng tôi với
phương pháp dựa trên đào tạo hiện đại nhất Eagle (Li
et al., 2024) trên Spec-Bench (Xia et al., 2024).
Ý tưởng chính của Eagle là đào tạo đặc biệt một
Spec-BenchOuroboros Eagle
token/s #accept token/s #accept
MT-Bench 24.23 5.16 28.20 3.52
Translation 18.91 3.92 24.23 3.16
Summarization 19.91 4.93 22.03 3.16
QA 21.63 4.67 24.83 3.23
Math Reasoning 25.39 4.95 29.90 3.81
RAG 19.00 5.43 20.56 3.54
Average 21.51 4.96 24.96 3.48
Bảng 5: Tốc độ (token/s) so sánh với các phương pháp
dựa trên đào tạo, được thử nghiệm trên Llama-2-chat-70B. "#accept"
có nghĩa là số lượng token thảo được mô hình đích chấp nhận
trong mỗi vòng lặp (trung bình). Ouroboros sử dụng
Llama-2-chat-7B làm mô hình thảo trong khi Eagle sử dụng
mô hình 1B được đào tạo của nó để soạn thảo.
mô hình thảo nhỏ dưới kiến trúc mô hình
được thiết kế mới của họ và tạo ra các bản thảo kiểu cây
cho LLM đích. Để tăng tốc Llama-2-chat-
70B, nó đào tạo một mô hình thảo 1B bằng cách chưng cất từ
Llama-2-chat-70B. Kết quả trong Bảng 5 cho thấy rằng,
mặc dù Eagle đào tạo một mô hình thảo nhỏ hơn nhiều với
1
7 quy mô của chúng tôi để soạn thảo, nó chỉ có thể
đạt được tốc độ nhanh hơn một chút so với phương pháp
không cần đào tạo của chúng tôi. Điều này là do mô hình
nhỏ hơn nhiều làm giảm đáng kể số lượng token thảo
được mô hình đích chấp nhận trong mỗi vòng lặp,
tức là mất hiệu quả thảo. Chúng tôi nhìn lại
các phương pháp dựa trên đào tạo bao gồm Eagle, họ
theo đuổi tốc độ soạn thảo tối ưu với cái giá phải trả
là mất độ chính xác thảo. Ngược lại, chúng tôi
tối ưu hóa tốc độ soạn thảo trong khi giữ nguyên độ chính xác.
Hiện tại, Eagle không thể được tích hợp
vào Ouroboros vì kiến trúc mô hình đặc biệt
của mô hình thảo của họ chỉ hỗ trợ soạn thảo
cấp token tự hồi quy. Chúng tôi tin rằng phương pháp của chúng tôi
có thể tìm được sự cân bằng tốt hơn giữa tốc độ soạn thảo
và độ chính xác khi kết hợp với đào tạo, đó sẽ
là công việc tương lai của chúng tôi.
4.5 So sánh với Các Phương pháp Dựa trên Cụm từ
Chúng tôi so sánh Ouroboros với phương pháp dựa trên cụm từ
khác PLD (Saxena, 2023) và REST (He et al.,
2024) trên Spec-Bench (Xia et al., 2024). Những
phương pháp này truy xuất cụm từ từ lời nhắc hoặc tài liệu
làm bản thảo cho mô hình đích xác minh.
Ouroboros vượt trội hơn những baseline này với biên độ
lớn trong Bảng 6, cho thấy hiệu quả
của việc sử dụng mô hình thảo như một trung gian để lọc
bỏ các cụm từ chất lượng thấp trước khi cung cấp chúng cho
mô hình đích.

--- TRANG 8 ---
Spec-BenchOuroboros PLD REST
token/s #accept token/s #accept token/s #accept
MT-Bench 24.23 5.16 13.95 1.45 14.93 1.94
Translation 18.91 3.92 12.99 1.41 13.02 1.57
Summarization 19.91 4.93 16.98 1.97 12.60 1.69
QA 21.63 4.67 11.33 1.26 16.10 1.96
Math Reasoning 25.39 4.95 15.86 1.75 13.07 1.60
RAG 19.00 5.43 15.51 1.64 13.28 1.91
Average 21.51 4.96 14.44 1.51 13.83 1.83
Bảng 6: Tốc độ (token/s) so sánh với các phương pháp
dựa trên cụm từ, được thử nghiệm trên Llama-2-chat-70B. "#accept"
có nghĩa là số lượng token thảo được mô hình đích chấp nhận
trong mỗi vòng lặp (trung bình).
5 Công trình Liên quan
Phần này giới thiệu các nỗ lực hiện có cho
suy luận LLM hiệu quả, bao gồm giải mã hiệu quả, triển khai
hiệu quả và nén mô hình.
5.1 Giải mã Hiệu quả
Để giảm bớt các vấn đề hiệu quả do giải mã
tự hồi quy gây ra, các phương pháp giải mã
không tự hồi quy đã được đề xuất. Thay vì tạo ra
token từng cái một, các phương pháp không tự hồi quy
tạo ra nhiều token song song tại một thời điểm (Wei
et al., 2019; Guo et al., 2020; Ghazvininejad et al.,
2019). Những phương pháp không tự hồi quy này mang lại
một cải thiện về hiệu quả suy luận và cũng
làm giảm đáng kể hiệu suất mô hình. Vì vậy,
các phương pháp soạn thảo-rồi-xác minh đã được
đề xuất (Stern et al., 2018; Xia et al., 2023a;
Leviathan et al., 2023; Chen et al., 2023a). Các
phương pháp soạn thảo-rồi-xác minh tránh LLM khỏi
việc tạo ra tuần tự và thay vào đó sử dụng chúng để xác
minh bản thảo theo cách song song không tự hồi quy,
không làm giảm hiệu suất mô hình và
tăng tốc đáng kể suy luận.
Chìa khóa cho các phương pháp soạn thảo-rồi-xác minh là
tạo ra bản thảo nhanh chóng và tốt.
(1)Các phương pháp như giải mã suy đoán (Xia
et al., 2023a; Leviathan et al., 2023; Chen et al.,
2023a) sử dụng một mô hình nhỏ hơn mô hình đích
để tạo ra bản thảo. Để tiếp tục căn chỉnh mô hình thảo
với mô hình đích, các kỹ thuật chưng cất được
áp dụng (Miao et al., 2023; Zhou et al., 2023). Thậm chí
các mô hình nhỏ hơn nhiều có thể được sử dụng làm bản thảo để tăng tốc
mô hình thảo, tạo thành giải mã suy đoán
đa giai đoạn (Spector and Re, 2023; Chen et al.,
2023b). Ngoài việc sử dụng các mô hình khác nhau, Self-
Speculative (Zhang et al., 2024a) và PPD (Yang
et al., 2023b) chọn một phần của các lớp mô hình làm
mô hình thảo, trong khi những phương pháp này yêu cầu đào tạo
hoặc tiền xử lý bổ sung.
(2)Một số nỗ lực khác khám phá việc sử dụng chính mô hình đích
để tạo ra bản thảo một cách hiệu quả. Block-
wise (Stern et al., 2018) và Medusa (Cai et al.,
2024) đào tạo nhiều đầu ra bổ sung trên đỉnh
LLM và tinh chỉnh các đầu ra để tạo ra nhiều
token thảo song song. Parallel Decoding (Santilli
et al., 2023) và PaSS (Monea et al., 2023) thêm
các hậu tố đầu vào phụ trợ như token đệm hoặc
token đệm có thể học để tạo ra các hậu tố đầu ra thảo.
Lookahead Decoding (Fu et al., 2023)
tạo ra các pool n-gram sử dụng lặp Jacobi và
sử dụng những n-gram bắt đầu bằng token được tạo ra cuối cùng
làm bản thảo.
(3)Các phương pháp khác khám phá việc truy xuất cụm từ
từ các lời nhắc trước đó hoặc tài liệu hiện có, như
PLD (Saxena, 2023), LLMA (Yang et al., 2023a),
và REST (He et al., 2024). Những phương pháp này sử dụng
mô hình đích để trực tiếp xác minh cụm từ, phát sinh
chi phí thất bại cao trên mỗi lần thử bản thảo. Tuy nhiên, chúng tôi
là những người đầu tiên sử dụng mô hình thảo như một trung gian
để lọc bỏ các cụm từ chất lượng thấp trước khi cung cấp
chúng cho mô hình đích.
Cả hai phương pháp trên đều không đủ hiệu quả,
vì vậy chúng tôi đầu tiên giới thiệu một mô hình nhỏ để soạn thảo
và sau đó tiếp tục tăng tốc quá trình thảo
của mô hình nhỏ bằng cách sử dụng cụm từ. Chúng tôi liệt kê các
khả năng khác nhau cho nguồn của cụm từ (từ
mô hình thảo, từ xác minh, và từ ngữ cảnh
lịch sử) trong phương pháp của chúng tôi và tiến hành thực nghiệm loại bỏ
cho mỗi nguồn. Điều này chỉ ra khả năng
rằng các nguồn cụm từ khác (Saxena, 2023;
Yang et al., 2023a; He et al., 2024) có thể được tích hợp
vào khung của chúng tôi để đạt được tăng tốc hơn nữa.
Ngoài việc tạo ra bản thảo tốt hơn, che attention
tam giác truyền thống chỉ có thể xác minh một bản thảo
câu sử dụng một lượt chuyển tiến hoàn chỉnh. Xác minh
kiểu cây (Miao et al., 2023; Cai et al., 2024; Fu
et al., 2023) thiết kế che attention cụ thể để
xác minh nhiều bản thảo có thể tại một thời điểm.
5.2 Triển khai Hiệu quả
Giải pháp trực tiếp nhất để đạt được suy luận
LLM hiệu quả là triển khai LLM một cách hiệu quả để tận dụng
các thiết bị phần cứng (như GPU).
FlashDecoding (Dao et al., 2023) tăng tốc tính toán
attention Transformer trong LLM bằng cách phân chia
chuỗi giải mã thành nhiều khối nhỏ và thực hiện
tính toán theo khối trong SRAM GPU nhanh
thay vì GPU HBM. PageAttention (Kwon et al., 2023) sử dụng quản lý bộ nhớ ảo
được phân trang để tổ chức bộ nhớ đệm giải mã
trong quá trình giải mã, do đó sử dụng hiệu quả
băng thông bộ nhớ GPU để giảm chi phí
truy cập bộ nhớ của suy luận. Tensor Parallelism (Shoeybi et al., 2019) tăng tốc suy luận
bằng cách chia sẻ ma trận vào các GPU phân tán và thực hiện
phép nhân ma trận theo cách phân tán. Một số nỗ lực triển khai LLM bằng cách tối ưu hóa
các toán tử cơ bản (Nvidia, a; Wang et al., 2021;
Nvidia, b) và đạt được kết quả hứa hẹn. Lưu ý
rằng những triển khai hiệu quả của LLM này và
Ouroboros là trực giao. Kết hợp các phương pháp triển khai hiệu quả
và Ouroboros có thể đạt được
tăng tốc suy luận đáng kể hơn.
5.3 Nén Mô hình
Các phương pháp nén mô hình được đề xuất để giảm
số lượng phép toán cần thiết cho việc thực thi mô hình.
Cắt tỉa có cấu trúc (Fan et al., 2020; Wang
et al., 2020; Zhang et al., 2021; Xia et al., 2022) và
cắt tỉa không có cấu trúc (Han et al., 2015; Chen et al.,
2020; Xu et al., 2021) loại bỏ các tham số
không cần thiết. Các phương pháp lượng tử hóa (Zafrir et al., 2019; Frantar
et al., 2023; Lin et al., 2023; Kim et al., 2023; Stock
et al., 2021) lượng tử hóa các tham số thành các biểu diễn
bit thấp. Các phương pháp chưng cất (Hinton et al., 2015;
Sun et al., 2019; Jiao et al., 2020; Liu et al., 2022;
Park et al., 2021) được sử dụng để giúp căn chỉnh
những mô hình được nén đó với các phiên bản gốc của chúng
để duy trì hiệu suất mô hình. Early-exiting (Elbayad et al., 2019; Bae et al., 2023) kết thúc
quá trình suy luận khi kết quả đầu ra trong
các lớp nông đạt ngưỡng độ tin cậy.
MoEfication (Zhang et al., 2022; Song et al., 2023)
biến một mô hình dày đặc thành một mô hình kích hoạt
thưa thớt. Nén mô hình và Ouroboros cũng trực giao
và có thể được tích hợp để tăng tốc hơn nữa.
6 Kết luận
Trong bài báo này, chúng tôi đề xuất một thuật toán thực tế
Ouroboros để cải thiện hiệu quả soạn thảo cho giải
mã suy đoán. Chúng tôi tạo ra cụm từ theo cách
heuristic và sử dụng cụm từ để giúp các mô hình thảo
tăng tốc soạn thảo và làm dài bản thảo. Thực nghiệm của chúng tôi
xác minh rằng, so với các baseline điển hình
giải mã suy đoán vanilla và giải mã nhìn trước,
Ouroboros tương ứng đạt được tốc độ tăng
2.8× và 1.9× và không ảnh hưởng đến chất lượng
tạo ra chút nào. Phương pháp của chúng tôi hoàn toàn không cần đào tạo,
để làm cho nó dễ dàng áp dụng hơn cho người dùng
giải mã suy đoán. Và phiên bản dựa trên đào tạo
của Ouroboros hướng tới tăng tốc cực đại
sẽ là công việc tiếp theo của chúng tôi.
Hạn chế
Chúng tôi chỉ tập trung vào cấu trúc mô hình chỉ giải mã
trong thực nghiệm. Tuy nhiên, Ouroboros là một
khung có thể mở rộng, có thể được áp dụng cho
các cấu trúc mô hình khác nhau. Phương pháp của chúng tôi hiện tại
tập trung vào tối ưu hóa trong các tình huống không cần đào tạo. Nếu
tính nhất quán của các mô hình lớn và nhỏ được cải thiện
thông qua đào tạo trong tương lai, phương pháp của chúng tôi
có thể đạt được tăng tốc cao hơn. Cả triển khai hiệu quả
và nén mô hình đều trực giao
với phương pháp của chúng tôi, và có thể được kết hợp để
tăng tốc hơn nữa.
Chúng tôi chỉ tập trung vào tình huống truy vấn đơn. Việc
áp dụng lấy mẫu suy đoán trong tình huống suy luận
theo lô không nằm trong phạm vi của
bài báo này và có thể tham khảo (Liu et al., 2024; Qian et al.,
2024; Chen et al., 2024).
Cân nhắc Đạo đức
Phương pháp của chúng tôi không có rủi ro tiềm ẩn vì chúng tôi
không cần đào tạo và không ảnh hưởng đến kết quả tạo ra
chút nào.
Lời cảm ơn
Công trình này được hỗ trợ bởi Chương trình R&D Chủ chốt Quốc gia
của Trung Quốc (2022ZD0160501), Viện Guo
Qiang tại Đại học Thanh Hoa. Yuxiang Huang được
hỗ trợ bởi Chương trình Nghiên cứu Khoa học Sáng kiến Đại học Thanh Hoa (Chương trình Thúc đẩy Nghiên cứu Học thuật Sinh viên). Chúng tôi ghi nhận
các cuộc thảo luận có giá trị với Yuhui Li, tác giả của
EAGLE.
Tài liệu tham khảo
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten
Bosma, Henryk Michalewski, David Dohan, Ellen
Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.
Program synthesis with large language models. arXiv
preprint arXiv:2108.07732 .
Sangmin Bae, Jongwoo Ko, Hwanjun Song, and Se-
Young Yun. 2023. Fast and robust early-exiting
framework for autoregressive language models with
synchronized parallel decoding. In Proceedings of
EMNLP .
Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen,
Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong,
Qiushi Du, Zhe Fu, et al. 2024. Deepseek llm: Scal-
ing open-source language models with longtermism.
arXiv preprint arXiv:2401.02954 .

--- TRANG 10 ---
Ondřej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Matthias Huck, An-
tonio Jimeno Yepes, Philipp Koehn, Varvara Lo-
gacheva, Christof Monz, Matteo Negri, Aurelie
Neveol, Mariana Neves, Martin Popel, Matt Post,
Raphael Rubino, Carolina Scarton, Lucia Specia,
Marco Turchi, Karin Verspoor, and Marcos Zampieri.
2016. Findings of the 2016 conference on machine
translation. In Proceedings of the First Conference
on Machine Translation , pages 131–198.
Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng,
Jason D. Lee, Deming Chen, and Tri Dao. 2024.
Medusa: Simple llm inference acceleration frame-
work with multiple decoding heads. arXiv preprint
arXiv: 2401.10774 .
Charlie Chen, Sebastian Borgeaud, Geoffrey Irving,
Jean-Baptiste Lespiau, Laurent Sifre, and John
Jumper. 2023a. Accelerating large language model
decoding with speculative sampling. arXiv preprint
arXiv:2302.01318 .
Jian Chen, Vashisth Tiwari, Ranajoy Sadhukhan,
Zhuoming Chen, Jinyuan Shi, Ian En-Hsu Yen,
and Beidi Chen. 2024. Magicdec: Breaking the
latency-throughput tradeoff for long context gen-
eration with speculative decoding. arXiv preprint
arXiv:2408.11049 .
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, et al. 2021. Evaluating large
language models trained on code. arXiv preprint
arXiv:2107.03374 .
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia
Liu, Yang Zhang, Zhangyang Wang, and Michael
Carbin. 2020. The lottery ticket hypothesis for pre-
trained bert networks. In Proceedings of NeurIPS ,
pages 15834–15846.
Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun,
Jie Huang, and Kevin Chen-Chuan Chang. 2023b.
Cascade speculative drafting for even faster llm infer-
ence. arXiv preprint arXiv:2312.11462 .
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
2021. Training verifiers to solve math word prob-
lems. arXiv preprint arXiv:2110.14168 .
Tri Dao, Daniel Haziza, Francisco Massa, and Grig-
ory Sizov. 2023. Flash-decoding for long-context
inference.
Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael
Auli. 2019. Depth-adaptive transformer. In Proceed-
ings of ICLR .
Angela Fan, Edouard Grave, and Armand Joulin. 2020.
Reducing transformer depth on demand with struc-
tured dropout. In Proceedings of ICLR .
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and
Dan Alistarh. 2023. Gptq: Accurate quantization for
generative pre-trained transformers. In Proceedings
of ICLR .
Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang.
2023. Breaking the sequential dependency of llm
inference using lookahead decoding.
Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and
Luke Zettlemoyer. 2019. Mask-predict: Parallel
decoding of conditional masked language models.
arXiv preprint arXiv:1904.09324 .
Junliang Guo, Linli Xu, and Enhong Chen. 2020.
Jointly masked sequence-to-sequence model for non-
autoregressive neural machine translation. In Pro-
ceedings of ACL , pages 376–385.
Song Han, Jeff Pool, John Tran, and William J. Dally.
2015. Learning both weights and connections for
efficient neural network. In Proceedings of NeurIPS ,
pages 1135–1143.
Zhenyu He, Zexuan Zhong, Tianle Cai, Jason Lee, and
Di He. 2024. REST: Retrieval-based speculative
decoding. In Proceedings of NAACL , pages 1582–
1595.
Karl Moritz Hermann, Tomás Kociský, Edward Grefen-
stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,
and Phil Blunsom. 2015. Teaching machines to read
and comprehend. In Proceedings of NeurIPS , pages
1693–1701.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531 .
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao
Chen, Linlin Li, Fang Wang, and Qun Liu. 2020.
TinyBERT: Distilling BERT for natural language
understanding. In Findings of EMNLP , pages 4163–
4174.
Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen
Dong, Xiuyu Li, Sheng Shen, Michael W Ma-
honey, and Kurt Keutzer. 2023. Squeezellm:
Dense-and-sparse quantization. arXiv preprint
arXiv:2306.07629 .
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying
Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gon-
zalez, Hao Zhang, and Ion Stoica. 2023. Efficient
memory management for large language model serv-
ing with pagedattention. In Proceedings of SOSP .
Yaniv Leviathan, Matan Kalman, and Yossi Matias.
2023. Fast inference from transformers via spec-
ulative decoding. In Proceedings of ICML , pages
19274–19286.
Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang
Zhang. 2024. Eagle: Speculative sampling requires
rethinking feature uncertainty. In Proceedings of
ICML .

--- TRANG 11 ---
Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang,
Xingyu Dang, and Song Han. 2023. Awq: Activation-
aware weight quantization for llm compression and
acceleration. arXiv preprint arXiv:2306.00978 .
Chang Liu, Chongyang Tao, Jiazhan Feng, and Dongyan
Zhao. 2022. Multi-granularity structural knowledge
distillation for language model compression. In Pro-
ceedings of ACL , pages 1001–1011.
Xiaoxuan Liu, Cade Daniel, Langxiang Hu, Woosuk
Kwon, Zhuohan Li, Xiangxi Mo, Alvin Cheung,
Zhijie Deng, Ion Stoica, and Hao Zhang. 2024.
Optimizing speculative decoding for serving large
language models using goodput. arXiv preprint
arXiv:2406.14066 .
Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao
Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuom-
ing Chen, Daiyaan Arfeen, Reyna Abhyankar, and
Zhihao Jia. 2023. Specinfer: Accelerating generative
llm serving with speculative inference and token tree
verification. arXiv preprint arXiv:2305.09781 .
Giovanni Monea, Armand Joulin, and Edouard Grave.
2023. Pass: Parallel speculative sampling. arXiv
preprint arXiv:2311.13581 .
Nvidia. a. Fastertransformer.
Nvidia. b. Tensorrt-llm.
Geondo Park, Gyeongman Kim, and Eunho Yang. 2021.
Distilling linguistic context for language model com-
pression. In Proceedings of EMNLP , pages 364–378.
Haifeng Qian, Sujan Kumar Gonugondla, Sungsoo Ha,
Mingyue Shang, Sanjay Krishna Gouda, Ramesh Nal-
lapati, Sudipta Sengupta, Xiaofei Ma, and Anoop De-
oras. 2024. Bass: Batched attention-optimized spec-
ulative sampling. arXiv preprint arXiv:2404.15778 .
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and
Yuxiong He. 2020. Deepspeed: System optimiza-
tions enable training deep learning models with over
100 billion parameters. In Proceedings of SIGKDD ,
pages 3505–3506.
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,
Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.
Code llama: Open foundation models for code. arXiv
preprint arXiv:2308.12950 .
Andrea Santilli, Silvio Severino, Emilian Postolache,
Valentino Maiorca, Michele Mancusi, Riccardo
Marin, and Emanuele Rodolà. 2023. Accelerating
transformer inference for translation via parallel de-
coding. arXiv preprint arXiv:2305.10427 .
Apoorv Saxena. 2023. Prompt lookup decoding.
Abigail See, Peter J. Liu, and Christopher D. Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of ACL , pages
1073–1083.
Mohammad Shoeybi, Mostofa Patwary, Raul Puri,
Patrick LeGresley, Jared Casper, and Bryan Catan-
zaro. 2019. Megatron-lm: Training multi-billion
parameter language models using model parallelism.
arXiv preprint arXiv:1909.08053 .
Yixin Song, Zeyu Mi, Haotong Xie, and Haibo Chen.
2023. Powerinfer: Fast large language model serv-
ing with a consumer-grade gpu. arXiv preprint
arXiv:2312.12456 .
Benjamin Spector and Chris Re. 2023. Accelerating llm
inference with staged speculative decoding. arXiv
preprint arXiv:2308.04623 .
Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit.
2018. Blockwise parallel decoding for deep autore-
gressive models. volume 31.
Pierre Stock, Angela Fan, Benjamin Graham, Edouard
Grave, Rémi Gribonval, Herve Jegou, and Armand
Joulin. 2021. Training with quantization noise for ex-
treme model compression. In Proceedings of ICLR .
Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.
Patient knowledge distillation for BERT model com-
pression. In Proceedings EMNLP-IJCNLP , pages
4323–4332.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Xiaohui Wang, Ying Xiong, Yang Wei, Mingxuan Wang,
and Lei Li. 2021. LightSeq: A high performance
inference library for transformers. In Proceedings of
NAACL , pages 113–120.
Ziheng Wang, Jeremy Wohlwend, and Tao Lei. 2020.
Structured pruning of large language models. In
Proceedings of EMNLP , pages 6151–6162.
Bingzhen Wei, Mingxuan Wang, Hao Zhou, Junyang
Lin, Jun Xie, and Xu Sun. 2019. Imitation learning
for non-autoregressive neural machine translation.
arXiv preprint arXiv:1906.02041 .
Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu
Wei, and Zhifang Sui. 2023a. Speculative decod-
ing: Exploiting speculative execution for accelerat-
ing seq2seq generation. In Proceedings of EMNLP ,
pages 3909–3925.
Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang,
Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and
Zhifang Sui. 2024. Unlocking efficiency in large
language model inference: A comprehensive sur-
vey of speculative decoding. arXiv preprint
arXiv:2401.07851 .
Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi
Chen. 2023b. Sheared llama: Accelerating language
model pre-training via structured pruning. arXiv
preprint arXiv:2310.06694 .

--- TRANG 12 ---
Mengzhou Xia, Zexuan Zhong, and Danqi Chen. 2022.
Structured pruning learns compact and accurate mod-
els. In Proceedings of ACL , pages 1513–1528.
Dongkuan Xu, Ian En-Hsu Yen, Jinxi Zhao, and Zhibin
Xiao. 2021. Rethinking network pruning – under the
pre-train and fine-tune paradigm. In Proceedings of
NAACL , pages 2376–2382.
Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin
Jiang, Linjun Yang, Rangan Majumder, and Furu
Wei. 2023a. Inference with reference: Lossless ac-
celeration of large language models. arXiv preprint
arXiv:2304.04487 .
Seongjun Yang, Gibbeum Lee, Jaewoong Cho, Dim-
itris Papailiopoulos, and Kangwook Lee. 2023b.
Predictive pipelined decoding: A compute-latency
trade-off for exact llm decoding. arXiv preprint
arXiv:2307.05908 .
Alex Young, Bei Chen, Chao Li, Chengen Huang,
Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng
Zhu, Jianqun Chen, Jing Chang, et al. 2024. Yi:
Open foundation models by 01. ai. arXiv preprint
arXiv:2403.04652 .
Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe
Wasserblat. 2019. Q8bert: Quantized 8bit bert. In
Proceedings of EMC2-NIPS , pages 36–39.
Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen,
Gang Chen, and Sharad Mehrotra. 2024a. Draft&
verify: Lossless large language model acceleration
via self-speculative decoding. In Proceedings of ACL ,
pages 11263–11282.
Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and
Wei Lu. 2024b. Tinyllama: An open-source small
language model. arXiv preprint arXiv:2401.02385 .
Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li,
Maosong Sun, and Jie Zhou. 2022. MoEfication:
Transformer feed-forward layers are mixtures of ex-
perts. In Findings of ACL , pages 877–890.
Zhengyan Zhang, Fanchao Qi, Zhiyuan Liu, Qun Liu,
and Maosong Sun. 2021. Know what you don't need:
Single-shot meta-pruning for attention heads. vol-
ume 2, pages 36–42.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024.
Judging llm-as-a-judge with mt-bench and chatbot
arena. Proceedings of NeurIPS , 36.
Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat,
Aditya Krishna Menon, Afshin Rostamizadeh, San-
jiv Kumar, Jean-François Kagy, and Rishabh Agar-
wal. 2023. Distillspec: Improving speculative de-
coding via knowledge distillation. arXiv preprint
arXiv:2310.08461 .

--- TRANG 13 ---
A Ouroboros không gây ra bất kỳ
suy giảm hiệu suất nào
Mô hìnhYi
34B/6BDeepSeek
33b/6.7BCodeLlama
34B/7B
HumanEvalTham lam 18.3 64.6 36.0
Suy đoán 18.9 64.6 36.0
Nhìn trước 18.9 64.6 36.0
Ouroboros 18.9 64.6 36.0
MBPPTham lam 37.8 70.0 46.6
Suy đoán 38.9 71.1 46.6
Nhìn trước 38.9 71.1 46.6
Ouroboros 38.9 71.1 46.6
Bảng 7: Hiệu suất nhiệm vụ của HumanEval và MBPP
của Tham lam, Suy đoán, Nhìn trước và Ouroboros sử
dụng tạo ra tham lam. Chúng tôi báo cáo Độ chính xác(%) của Hu-
manEval và MBPP.
Mô hìnhYi
34B/6BLlama
70B/7B
GSM8KTham lam 61.0 66.0
Suy đoán 61.0 66.0
Nhìn trước 61.0 66.0
Ouroboros 61.0 65.0
CNN/DMTham lam 35.2 37.1
Suy đoán 35.1 37.2
Nhìn trước 35.3 37.1
Ouroboros 35.3 37.1
WMT16Tham lam 21.7 31.1
Suy đoán 21.9 31.1
Nhìn trước 22.0 31.1
Ouroboros 21.1 31.1
Bảng 8: Hiệu suất nhiệm vụ của GSM8K, CNN/DM
và WMT16 của Tham lam, Suy đoán, Nhìn trước và
Ouroboros sử dụng tạo ra tham lam. Chúng tôi báo cáo Độ chính
xác của GSM8K(%), điểm Rouge1(%) của CNN/DM và
điểm BLEU(%) của WMT16.
Bảng 7, 8 và 9 cho thấy hiệu suất nhiệm vụ của
các thực nghiệm được hiển thị trong Hình 5 và Bảng 2.
Về mặt lý thuyết, Giải mã Suy đoán, Giải mã Nhìn trước
và Ouroboros tạo ra cùng một chuỗi token
so với Giải mã vanilla.
Chúng tôi quan sát thấy không có suy giảm hiệu suất tổng thể
nào của Ouroboros từ Bảng 7, 8 và 9. Sự khác biệt nhỏ
trong hiệu suất nhiệm vụ giữa vanilla, Suy
đoán, Nhìn trước và Ouroboros là do
lỗi dấu phẩy động, vì các phép tính không
hoàn toàn giống nhau giữa các phương pháp (đặc biệt là
thứ tự tính toán của attention). Kết quả thực nghiệm
cho thấy rằng lỗi tính toán sẽ không gây ra
suy giảm hiệu suất đáng chú ý.
Mô hìnhYi
34B/6B
HumanEvalLấy mẫu Ngẫu nhiên 15.9
Suy đoán 18.9
Nhìn trước 18.9
Ouroboros 17.7
MBPPLấy mẫu Ngẫu nhiên 36.7
Suy đoán 33.3
Nhìn trước 35.6
Ouroboros 36.7
GSM8KLấy mẫu Ngẫu nhiên 67.0
Suy đoán 70.0
Nhìn trước 69.0
Ouroboros 69.0
CNN/DMLấy mẫu Ngẫu nhiên 34.6
Suy đoán 34.0
Nhìn trước 34.6
Ouroboros 35.6
WMT16Lấy mẫu Ngẫu nhiên 20.6
Suy đoán 21.2
Nhìn trước 22.7
Ouroboros 25.0
Bảng 9: Hiệu suất nhiệm vụ sử dụng lấy mẫu ngẫu nhiên. Chúng tôi
báo cáo độ chính xác cho HumanEval, MBPP và GSM8K,
điểm Rouge 1 cho CNN/DM và điểm BLEU cho
WMT16.
B Siêu tham số
W là một siêu tham số được sử dụng trong Giải mã Nhìn trước
để tạo ra cụm từ. W lớn hơn chỉ ra nhiều
cụm từ hơn được tạo ra trong mỗi lượt chuyển tiến của mô hình thảo
của chúng tôi trong Ouroboros, nhưng cũng có thể tốn thời gian hơn.
Để điều tra độ nhạy cảm của siêu tham số, chúng tôi
tiến hành tìm kiếm lưới trên γ, W và β trên GSM8K
với Yi-34B/6B. Chúng tôi báo cáo token/s của mỗi cài đặt.
W= 14 W= 15 W= 16
β= 5 β= 6 β= 7 β= 5 β= 6 β= 7 β= 5 β= 6 β= 7
γ= 3 28.17 28.07 27.30 27.97 28.23 27.38 28.34 27.64 27.21
γ= 4 28.52 28.42 27.91 29.06 28.77 28.23 29.00 28.68 27.64
γ= 5 28.13 29.32 28.20 28.31 28.96 27.99 27.94 29.03 27.90
độ lệch chuẩn token/s: 0.57 độ lệch chuẩn tăng tốc: 0.04
Bảng 10: Kết quả của tìm kiếm lưới với γ∈
[3,5], W∈[14,16], β∈[5,7]. "std" đại diện cho
độ lệch chuẩn.
Bảng 10 cho thấy Ouroboros ổn định với các
siêu tham số. Điều này tạo cơ hội để cung
cấp một công thức chung hoặc tiến hành tìm kiếm heuristic
thay vì tìm kiếm lưới để tìm một kết hợp tốt
của các siêu tham số trong thời gian ngắn.
B.1 Công thức Siêu tham số
Ở đây, chúng tôi cung cấp một công thức chung cho việc điều chỉnh
siêu tham số trong Bảng 11. Chúng tôi phân biệt các nhiệm vụ
downstream thành hai loại: (1) HH: Tính đồng nhất cao
giữa mô hình thảo và đích, ví dụ: tạo mã

--- TRANG 14 ---
tion; (2) LH: Tính đồng nhất thấp giữa mô hình thảo và đích, ví dụ: dịch máy.
Nhiệm vụ γ W β K
HH 7∼14 15 ∼20 5 ∼7 3∼5
LH 2∼6 15 ∼20 5 ∼7 3∼5
Bảng 11: Một công thức thực nghiệm cho việc điều chỉnh siêu tham số.
B.2 Điều chỉnh Siêu tham số
Theo tính ổn định của các siêu tham số, chúng tôi
tiến hành tìm kiếm heuristic để tìm một kết hợp tốt.
Thuật toán của tìm kiếm heuristic được
mô tả trong Thuật toán 1.
Thuật toán 1: Tìm kiếm Heuristic của Siêu tham số
Đầu vào: Loại nhiệm vụ T, Bộ dữ liệu D, Hàm mục tiêu f.
Đầu ra: Siêu tham số γ, W, β, K
K0:= 3//K luôn luôn là 3 trong các thực nghiệm của chúng tôi
ˆW∼[15,20]
ˆβ∼[5,7]
nếu T=HH thì
ˆγ∼[7,14]
ngược lại
ˆγ∼[2,6]
kết thúc nếu
γ0:= arg min γThời gian Đồng hồ (f(D; ˆγ,ˆW,ˆβ, K0))
W0:= arg min WThời gian Đồng hồ (f(D;γ0,ˆW,ˆβ, K0))
β0:= arg min βThời gian Đồng hồ (f(D;γ0, W0,ˆβ, K0))
trả về γ0, W0, β0, K0
B.3 Siêu tham số Được Sử dụng trong Thực nghiệm của Chúng tôi
Các siêu tham số của thực nghiệm của chúng tôi được hiển thị
trong Bảng 12. Trong Bảng 4, chúng tôi sử dụng γ= 12, W=
20, β= 8 và K= 3. Trong Hình 6, chúng tôi sử dụng γ=
12, W= 20, β= 8. Trong Bảng 17, chúng tôi sử dụng γ=
6, W= 18, β= 6 và K= 3. Trong Bảng 13, chúng tôi
theo các siêu tham số trong Bảng 12.
C Hiệu quả Khối
Để cho thấy tiềm năng của việc tối ưu hóa hệ thống
hơn nữa, chúng tôi theo Miao et al. (2023) để kiểm tra
Hiệu quả Khối, với giả định rằng thời gian
soạn thảo có thể được bỏ qua hoàn toàn. Hiệu quả khối
là giới hạn trên lý thuyết của tỷ lệ tăng tốc, mà không
xem xét bất kỳ ràng buộc tài nguyên nào.
Định nghĩa C.1 (Hiệu quả Khối). Định nghĩa Hiệu quả
Khối η như
η=# Token được tạo ra
# Lần gọi mô hình đích
Nhiệm vụ MôinhSpeculative Lookahead Ouroboros
γ W β γ W β K
HumanEvalYi-34B/6B 12 16 7 12 20 8 3
DeepSeek-33b/6.7B 3 16 7 11 20 8 3
CodeLlama-34B/7B 5 17 5 10 20 7 3
MBPPYi-34B/6B 6 16 6 6 18 6 3
DeepSeek-33b/6.7B 5 15 7 7 15 6 3
CodeLlama-34B/7B 4 16 6 8 16 7 3
GSM8KYi-34B/6B 7 15 6 4 15 7 3
Llama-70B/7B 7 15 6 6 17 7 3
CNN/DMYi-34B/6B 5 15 6 10 15 6 3
Llama-70B/7B 4 16 5 4 13 6 3
WMT16Yi-34B/6B 2 15 6 5 16 6 3
Llama-70B/7B 5 18 7 4 13 6 3
Bảng 12: Siêu tham số của các thực nghiệm trong Hình 5
và Bảng 2.
Chúng tôi đo Hiệu quả Khối với Yi-34B/6B
trên tất cả 5 bộ dữ liệu theo các cài đặt của Hình 5
và Bảng 2. Kết quả trong Bảng 13.
Speculative Lookahead Ouroboros
HumanEval 11.16 3.08 13.12
MBPP 6.14 2.71 7.43
GSM8K 5.23 2.30 4.65
CNN/DM 4.71 1.70 7.46
WMT16 2.41 1.37 4.05
Bảng 13: Hiệu quả Khối η của Yi-34B/6B trên
HumanEval, MBPP, GSM8K, CNN/DM và WMT16.
Phương pháp với η lớn nhất được làm nổi bật bằng chữ đậm,
và kết quả lớn thứ hai được gạch chân.
Từ Bảng 13, Ouroboros có Hiệu quả Khối tương đối cao
trong hầu hết các tình huống, cho thấy rằng
nó có tỷ lệ tăng tốc lý tưởng lớn hơn có thể
được đạt được bởi các thuật toán hoặc triển khai hệ thống
trong tương lai.
D So sánh với các phương pháp
gia tốc thảo khác
Cascade Speculative Decoding (Chen et al., 2023b)
là một phương pháp điển hình tăng tốc mô hình
thảo trong giải mã suy đoán bằng cách tiếp tục sử dụng
một mô hình thậm chí nhỏ hơn để soạn thảo cho mô hình thảo.
So sánh giữa Ouroboros và Cascade
được mô tả trong Bảng 14. Kết quả cho thấy rằng
Ouroboros vượt trội hơn Cascade, cho thấy rằng
cách chúng tôi tăng tốc bản thảo hiệu quả hơn.
Cascade hoạt động chậm hơn so với giải mã suy đoán
ban đầu. Một lý do có thể là các mô hình
nhỏ được sử dụng để tăng tốc mô hình Llama-
2-7B, tức là Llama-160M (Miao et al., 2023)
và TinyLlama-1.1B (Zhang et al., 2024b), không được
đào tạo chính thức bởi Meta. Điều này đã dẫn đến sự
không khớp trong đầu ra của mô hình, do đó làm chậm
quá trình soạn thảo.

--- TRANG 15 ---
Phương pháp Dòng Mô hình token/s
Speculative 7B→70B 16.86
Cascade 160M→7B→70B 7.90
Cascade 1.1B→7B→70B 9.22
Ouroboros 7B→70B 24.03
Bảng 14: So sánh với các phiên bản cascade khác nhau của
giải mã suy đoán, được thử nghiệm trên GSM8K.
E Ouroboros dưới các độ dài tạo ra
khác nhau
Để điều tra tỷ lệ tăng tốc dưới các giới hạn độ dài
tạo ra khác nhau, chúng tôi tiến hành thực nghiệm sau
với các giới hạn độ dài tạo ra 64, 128,
256, và 512. Thực nghiệm này được thực hiện trên
GSM8K với Yi-34B/6B, với các siêu tham số khác
được căn chỉnh với Bảng 12.
Tham lam Speculative Lookahead Ouroboros
l= 64token/s 14.31 20.18 20.20 30.99
tăng tốc 1.00 1.41 1.41 2.17
l= 128token/s 14.57 21.90 22.68 35.32
tăng tốc 1.00 1.50 1.56 2.42
l= 256token/s 14.41 22.12 24.58 38.36
tăng tốc 1.00 1.54 1.71 2.66
l= 512token/s 15.35 21.45 25.14 40.94
tăng tốc 1.00 1.40 1.64 2.67
Bảng 15: Tăng tốc với nhiều độ dài tạo ra. "l"
đại diện cho giới hạn độ dài tạo ra.
Tham lam Speculative Lookahead Ouroboros
l= 64 44.51 44.51 44.51 44.51
l= 128 60.98 60.98 60.98 60.98
l= 256 64.02 64.02 63.41 63.41
l= 512 64.63 64.63 64.02 64.63
Bảng 16: Hiệu suất nhiệm vụ (độ chính xác) với nhiều
độ dài tạo ra. "l" đại diện cho giới hạn độ dài tạo ra.
Từ Bảng 15, Ouroboros vượt trội hơn giải mã
tự hồi quy, Giải mã Suy đoán, và
Giải mã Nhìn trước dưới các độ dài khác nhau. Trong Bảng
16, giới hạn độ dài tạo ra ngắn (l= 64,128)
dẫn đến việc cắt cụt câu trả lời, dẫn đến
hiệu suất nhiệm vụ thấp hơn đáng kể. Hiệu suất nhiệm vụ
gần nhau và tỷ lệ tăng tốc được đạt được trong l=
256,512, cho thấy rằng tất cả các thuật toán dừng lại
ngay lập tức sau khi tạo ra "<EOS>".
Hai kết luận có thể được rút ra. Thứ nhất, các
giới hạn độ dài được sử dụng trong thực nghiệm của chúng tôi là hợp lý,
vì các giới hạn độ dài ngắn hơn dẫn đến việc tạo ra
Nhiệm vụ1Nhiệm vụ2Nhiệm vụ3Nhiệm vụ44444333322221111434343432121212143214321432143214434433322122111CN=1CN=2CN=4xáo trộnThứ tự Đánh giáHình 7: Nghiên cứu loại bỏ về tính địa phương ngữ cảnh. Trong
thực nghiệm của chúng tôi, nhiệm vụ 1,2,3,4 tương ứng là MBPP, GSM8K,
CNN/DM, và WMT16.
câu trả lời kém. Thứ hai, việc lặp lại cụm từ trong việc tạo ra
không phải là lý do tại sao Ouroboros đạt được
tăng tốc cao như vậy. Nếu vậy, chúng ta sẽ có thể quan sát
hiệu suất nhiệm vụ thấp hơn và tăng tốc cao hơn nhiều
từ l= 256 đến l= 512.
F Tính Địa phương Ngữ cảnh
Chúng tôi tiếp tục nghiên cứu cách tính địa phương ngữ cảnh tăng tốc
việc tạo ra thông qua các giai đoạn được tái sử dụng từ lịch sử.
Tính địa phương ngữ cảnh vẫn rất cao khi liên tục
tạo ra trong một nhiệm vụ, nhưng có thể thay đổi
qua các bộ dữ liệu khác nhau. Ví dụ, pool cụm từ
nên chứa các đoạn mã khi chạy trên
các bộ dữ liệu tạo mã như HumanEval và
MBPP, nhưng nó có thể chứa các đoạn ngôn ngữ tự nhiên
khi tạo ra văn bản. Vẫn còn một câu hỏi rằng
liệu một loại đoạn ngôn ngữ có thể tăng tốc
việc tạo ra loại ngôn ngữ khác hay không. Chúng tôi
tiến hành các thực nghiệm sau để điều tra thêm
đến mức độ nào và cách tính địa phương ngữ cảnh có thể
tăng tốc việc tạo ra thông qua việc tái sử dụng cụm từ từ
lịch sử.
Chúng tôi chọn bốn bộ dữ liệu: MBPP, GSM8K,
CNN/DM, và WMT16, tương ứng với tạo mã,
lý luận số học, tóm tắt tài liệu,
và dịch máy, và lấy mẫu
20 mục từ mỗi bộ dữ liệu để tổ chức một bộ dữ liệu mới
chứa các lĩnh vực khác nhau. Chúng tôi đánh giá
Ouroboros với nhiều thứ tự đánh giá, từ
đó chúng tôi có thể thay đổi tính địa phương ngữ cảnh. Chúng tôi định nghĩa
số liên tiếp CN là bao nhiêu mục
từ cùng một bộ dữ liệu được kiểm tra liên tiếp, như
được hiển thị trong Hình 7. Trong cấu hình "xáo trộn",
chúng tôi làm ngẫu nhiên thứ tự của các mục dữ liệu. CN cao hơn
chỉ ra tính địa phương tốt hơn, và cấu hình "xáo trộn"
nên có tính địa phương tệ nhất. Sau đó chúng tôi
đo tốc độ tạo ra trong các cấu hình CN khác nhau
hoặc xáo trộn ngẫu nhiên.
Bảng 17 cho thấy rằng tính địa phương ngữ cảnh thực sự ảnh hưởng
đến hiệu quả của việc tái sử dụng cụm từ. Với
việc tái sử dụng cụm từ, CN=20 dẫn đến giải mã nhanh hơn
Cài đặt xáo trộn CN=20 xáo trộn CN=20 CN=10 CN=4 CN = 1
Tái sử dụng Cụm từ tắt tắt bật bật bật bật bật
token/s 32.68 32.53 35.39 36.00 35.32 34.83 35.36
Bảng 17: Kết quả của nghiên cứu loại bỏ về tính địa phương
ngữ cảnh. "CN" đại diện cho số liên tiếp.
so với cài đặt xáo trộn. Tuy nhiên, hiệu ứng
gây ra bởi tính địa phương ngữ cảnh nhỏ hơn so với việc
có bật tái sử dụng cụm từ hay không. Tốc độ của CN thấp hơn
tương tự như cài đặt xáo trộn khi áp dụng
tái sử dụng cụm từ, nhưng cả hai đều nhanh hơn khoảng
3 token/s so với cài đặt khởi động lạnh,
chứng minh rằng pool cụm từ vẫn hiệu quả
qua nhiều nhiệm vụ.

--- TRANG 16 ---
