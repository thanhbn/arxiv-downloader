# 2408.11850.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/speculative/2408.11850.pdf
# File size: 649973 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Published as a conference paper at ICLR 2025
PEARL: P ARALLEL SPECULATIVE DECODING WITH
ADAPTIVE DRAFT LENGTH
Tianyu Liu1,3‚àóYun Li2‚Ä†Qitan Lv1Kai Liu2Jianchen Zhu2Winston Hu2Xiao Sun3‚Ä†
1University of Science and Technology of China
2Tencent
3OpenGVLab, Shanghai AI Laboratory
{tianyu liu, qitanlv }@mail.ustc.edu.cn
yunli.charles@gmail.com
{raccoonliu, dickzhu, winstony }@tencent.com
sunxiao@pjlab.org.cn
ABSTRACT
Speculative decoding (SD), where an extra draft model is employed to provide
multiple draft tokens first, and then the original target model verifies these tokens
in parallel, has shown great power for LLM inference acceleration. However, ex-
isting SD methods suffer from the mutual waiting problem, i.e., the target model
gets stuck when the draft model is guessing tokens, and vice versa. This prob-
lem is directly incurred by the asynchronous execution of the draft model and the
target model and is exacerbated due to the fixed draft length in speculative decod-
ing. To address these challenges, we propose a conceptually simple, flexible, and
general framework to boost speculative decoding, namely Parallel sp Eculative de-
coding with Adaptive d RaftLength (PEARL). Specifically, PEARL proposes pre-
verify to verify the first draft token in advance during the drafting phase, and post-
verify to generate more draft tokens during the verification phase. PEARL paral-
lels the drafting phase and the verification phase via applying the two strategies,
and achieves adaptive draft length for different scenarios, which effectively allevi-
ates the mutual waiting problem. Experiments on various text generation bench-
marks demonstrate the effectiveness of our PEARL, leading to a superior speed
up performance up to 4.43√óand1.50√ó, compared to auto-regressive decoding
and vanilla speculative decoding, respectively. Our code is available at https:
//github.com/smart-lty/ParallelSpeculativeDecoding .
1 I NTRODUCTION
Large language models (LLMs) such as GPT-4, LlaMA, and DeepSeek (Achiam et al., 2023;
DeepSeek-AI, 2024; Bommasani et al., 2021; Touvron et al., 2023) have dominated natural language
understanding and generation (Khurana et al., 2023) over a wide range of applications. However,
the substantial inference latency of these LLMs has emerged as a significant obstacle bounding their
broader application in scenarios with restricted computational resources. This latency primarily
originates from the auto-regressive token-by-token decoding process wherein decoding Ktokens
requires Kserial runs of LLMs, incurring exacerbated latency with both the length of generated
tokens and the model scale.
To address this challenge, extensive research efforts have been devoted to accelerating LLM in-
ference. Given that inference from large models is often constrained more by memory bandwidth
and communication than by arithmetic operations Leviathan et al. (2023), one innovative inference
paradigm, Speculative Decoding (SD), has emerged as a new trend and shown superior performance
by effectively enabling better GPU utilization. As shown in the upper part of Figure. 1, the key idea
of the SD algorithm is to employ an extra small model (referred as the draft model ) to first generate Œ≥
‚àóThis work is done when Tianyu Liu works as an intern in Tencent.
‚Ä†The Corresponding Authors.
1arXiv:2408.11850v3  [cs.CL]  17 Feb 2025

--- PAGE 2 ---
Published as a conference paper at ICLR 2025
Prefix: In the domain of speculative  decoding, PEARL is a
Hurry Up! I am waiting!
 Step 1 D: model free algorithm
Hurry Up! I am waiting!
 Step 2 V: model free algorithm parallel
Hurry Up! I am waiting!
 Step 3 D: inference acceleration method
Hurry Up! I am waiting!
 Step 4 V: inference acceleration method that
Step 1 Pre: parallel
 Step 1 Pre: model free algorithm
Step 2 Pre: inference acceleration method
 Step 2 Pre: Inference
Step 3 Post: Inference acceleration method
 Step 3 Post: that can achieve
Step 4 Post: adaptive draft length
 Step 4 Post: that can achievePEARL is a model free algorithm parallel inference acceleration method that
PEARL is a model free algorithm parallel inference acceleration method that can achieve adaptive draft length
Figure 1: An overview of speculative decoding (the upper part) and our PEARL (the lower part).
SD employs a draft model to provide multiple drafts and then the target model verifies the drafts
in parallel. However, SD suffers from the mutual waiting problem, i.e., the target model gets stuck
when the draft model is guessing tokens, and vice versa (the dashed dialogue box). PEARL parallels
the drafting and verification process to alleviate the mutual waiting problem. Moreover, PEARL can
leverage adaptive draft length to generate more tokens within the same amount of time to further
mitigate the mutual waiting problem. Specifically, PEARL generates fewer draft tokens if they will
be rejected (step 1 in the lower part), and more draft tokens if they can be accepted (steps 3 and 4).
draft tokens for the original large model (referred as the target model ), and then the target model ver-
ifies these draft tokens in parallel within a single forward. Here, Œ≥is a fixed hyperparameter window
size.Draft length is the number of tokens generated by the draft model in a continuous execution.
Therefore, the draft length is set to Œ≥in SD. Following-up works effectively extend this framework
by either removing the necessity of the draft model (Cai et al., 2024; Fu et al., 2024; Zhang et al.,
2023) or identifying a compact draft model with high distribution alignment (Zhou et al., 2023; Zhao
et al., 2024; Miao et al., 2023). Extensive experiments demonstrate that this draft-then-verify frame-
work effectively enhances the concurrency of the target model, thereby significantly accelerating the
inference process.
Albeit with multiple benefits of this draft-then-verify framework, it confronts one significant chal-
lenge that may hinder its performance and deployment‚Äîthe mutual waiting problem. That is, the
target model will be idle when the draft model is generating the draft tokens and the draft model
will be idle when the target model is verifying the previously drafted tokens. This mutual waiting
problem primarily stems from two limitations inherent in speculative decoding: (i)the asynchronous
execution of the draft and verify phases, which directly results in the mutual waiting problem; and
(ii)the fixed draft length, which cannot adapt to most decoding steps and thus exacerbate the mutual
waiting problem.
Therefore, in this paper, we seek to answer the question: Can we draft and verify in parallel and
adaptively adjust draft length? With this consideration, we propose a conceptually simple, flexible,
and general framework to boost speculative decoding, namely Parallel sp Eculative decoding with
Adaptive d RaftLength (PEARL). Specifically, PEARL consists of two strategies pre-verify and
post-verify :(i)pre-verify uses the target model to verify the first draft token during drafting phase,
which allows the draft model to generate less draft tokens in difficult scenarios; (ii)post-verify
uses the draft model to continue generating draft tokens during verification phase, which provides
more draft tokens in simple situations. As shown in the lower part of Figure.1, PEARL effectively
alleviates the mutual waiting problem with parallelism andadaptive draft length via these two
strategies. We conduct extensive experiments on various text generation benchmarks, leading to a
superior speed up performance up to 4.43√óand1.50√ó, compared to auto-regressive decoding and
speculative decoding, respectively.
2

--- PAGE 3 ---
Published as a conference paper at ICLR 2025
ùëÄùëû
ùëÄùëù
ùëÄùëû
ùëÄùëù
ùëÄùëûdecoder ùëÄùëùdecoder Model Stuck√ó√ó√ó√ó√ó√ó√ó
‚àö‚àö‚àö‚àö‚àö‚àö‚àö‚àö‚àö‚àö‚àö‚àö‚àö‚àö√ó‚àö‚àö‚àö‚àö‚àö‚àö
Draft Lengthspeculative decoding
PEARL
(a)
deepseek 1.3&33 deepseek 7&33 codellama 7&34 codellama 7&70llama2 7&700.000.020.040.060.080.100.120.14Time (s)0.080.08 0.080.14 0.14
0.05 0.05
0.040.07 0.07
(a)Average Draft and T arget Times
Avg Draft Time
Avg T arget Time
0 20 40 60 80 100
Iteration0102050100200
(b)Influence of Draft Length
static optimal draft length
iteration optimal draft length (b)
Figure 2: Motivated observations. (a) The time of both the drafting phase and verification phase
is non-negligible, therefore the asynchronous execution of the draft model and the target model
directly incurs the mutual waiting problem. (b) We observe that the optimal draft length changes
significantly in different iterations, which exacerbates the mutual waiting problem.
2 B ACKGROUND
Notations. In this paper, we use Mqto denote the draft model and Mpto denote the target model.
Mq(¬∑), Mp(¬∑)denotes the logits of the next token of a single forward of Mq, Mprespectively. Œ≥is
a hyperparameter to control the window size during speculative decoding. We denote the running
speed between MqandMpasc, which is defined as the ratio between the time for a single forward
ofMpand the time for a single forward of Mq, i.e., c=T(Mp(¬∑))/T(Mq(¬∑)).
Speculative decoding. Given an input sequence xas a prefix, a speculative decoding step consists of
a drafting phase and a verification phase. During the drafting phase, the draft model Mqis employed
to give Œ≥draft tokens x1, x2, ..., x Œ≥by running Œ≥times model forward and sample. Here, we denote
Mq(x+ [x1, ..., x i‚àí1])asqi, then each draft token is given by xi‚àºqi, i= 1, ..., Œ≥ . During the
verification phase, the prefix xtogether with Œ≥draft tokens are sent to Mpfor verification. The
target model Mpinputs x+[x1, ..., x Œ≥]and outputs the logits p1, p2, ..., p Œ≥+1. Then SD sequentially
verifies xivia speculative sampling, where the acceptance rate is given by:
Œ±i=Ô£±
Ô£≤
Ô£≥1 pi[xi]‚â•qi[xi],
pi[xi]
qi[xi]pi[xi]< qi[xi],(1)
If SD rejects xi, it will resample a token from norm (max(0 , pi‚àíqi)), otherwise, SD accepts all the
draft tokens and samples an additional token from pŒ≥+1. In this way, each SD step generates tokens
with a number of at least 1 and at most Œ≥+ 1, leading to efficiency acceleration.
Window size and draft length. We emphasize that the window size is a hyperparameter that con-
trols the drafting behavior. Draft length is the number of tokens generated by the draft model in a
continuous execution, which is fixed and the same as the window size in SD, while draft length is
adaptive and may be not equal to window size in PEARL.
3 M ETHODOLOGY
3.1 M OTIVATED OBSERVATION
As illustrated in Figure. 2(a), the mutual waiting problem is directly incurred by the asynchronous
execution of the draft model and the target model. In our experiments, we observe that the time
consumed during the drafting phase and the verification phase is usually non-negligible. Take the
instance of Codellama 7B & 34B, at each decoding step, although the running speed of Codellama
7B is almost 3 times faster than Codellama 34B, the total time consumption for generating 6 draft
3

--- PAGE 4 ---
Published as a conference paper at ICLR 2025
prefix
2=32=45Draft ModelTarget ModelParallel Speculative Decoding With Adaptive Draft Length&!?&"?&#?&(?&)?&*?
2=652=785&+?&,?&-?&!.?&!!?&!"?
2=95Pre-Verify: 
‚ùåPre-Verify: 
‚úÖPost-Verify: 
‚úÖPost-Verify: 
‚ùå"%&&(
‚úÖ"'!&"("&"(#&"($&&)
‚úÖ&*
‚úÖ&+
‚úÖ"(%&"(&&"('&&,
‚úÖ&-
‚ùå("&!
‚ùå(!
Figure 3: Illustration of our PEARL. At T= 0,Mqgenerates x1, x2, x3andMprejects x1with
thepre-verify strategy. At T= 3t,Mpaccepts x4and switches to the post-verify strategy. At
T= 6t,Mpaccepts all draft tokens x4, x5, x6in the last decoding step, while Mqcontinues drafting
x7, x8, x9. AtT= 9t,Mprejects x9, drops x10, x11, x12and switches to the pre-verify strategy.
The final output is [y1, x4, x5, x6, x7, x8, y2].
tokens is even 2 times than the time consumption for one verification step. Therefore, the mutual
waiting problem exists at any timestamp , and severely affects the acceleration effectiveness of SD.
The asynchronous execution of the draft model and the target model is the direct cause of the mutual
waiting problem, which is determined by two requirements of speculative decoding: (1) the drafting
phase requires the input prefix to be verified; (2) the verification phase requires the draft model to
complete generating draft tokens. This implies the great potential for alleviating the mutual waiting
problem through parallelism: if we can remove the two requirements and parallel the drafting phase
and the verification phase, a substantial acceleration can be possible.
Another limitation that aggravates the mutual waiting problem is the fixed draft length in SD, which
is not appropriate for all the decoding steps. As shown in Figure 2(b), the optimal draft length
changes significantly in different iterations. On the one hand, when the optimal draft length is less
than the fixed draft length, the draft model will generate meaningless draft tokens that block the
target model. On another hand, when the optimal draft length is more than the fixed draft length, the
draft model could have generated more draft tokens that can be accepted by the target model with
a single forward. However, a fixed draft length will interrupt the longer drafting phase and take an
additional verification phase, which strengthens the mutual waiting problem as well. This motivates
our PEARL to further alleviate the mutual waiting problem with adaptive draft length.
Together with the two motivations, we propose two simple and effective strategies, pre-verify and
post-verify . The pre-verify removes requirement 2 and allows the target model to verify the first draft
token in advance. The post-verify removes requirement 1 and allows the draft model to continue
generating draft tokens during the verification phase. The two strategies enable parallelism and
achieve adaptive draft length to effectively alleviate the mutual waiting problem.
3.2 P RE-VERIFY :VERIFY THE FIRST DRAFT TOKEN IN ADVANCE .
The pre-verify strategy aims at removing the requirement that the verification phase requires the
draft model to complete generating draft tokens. Therefore, we seek to verify some draft tokens in
advance during the drafting phase. We delve explicitly into the drafting stage. During the drafting
phase, the draft model tries to give Œ≥draft tokens by running Œ≥times model forward. We find that
the input of the draft model in Œ≥times forward is x,x+ [x1], ...,x+ [x1, x2, ..., x Œ≥‚àí1], respectively.
4

--- PAGE 5 ---
Published as a conference paper at ICLR 2025
Only the origin prefix xcan be acquired by the target model for parallel verification. Therefore, we
propose to run the target model to output the logits Mp(x)in parallel. In this way, we can verify
the first token x1before the verification phase. We implement the same lossless verification method
following (Leviathan et al., 2023) as illustrated in Section 2.
By applying such a pre-verify strategy, we can verify the first draft token before the verification
phase. If the first token is rejected, all of the following draft tokens are meaningless and should be
dropped. Hence we could skip the verification phase and directly conduct the next drafting phase
with the prefix x+ [y1]. If the first token is accepted, all the draft tokens will be sent to the target
model in the verification phase. In Figure. 3, at the timestamp of T= 0, the draft model generates
x1, x2, x3while the target model outputs pt
0, rejects the first token x1and sample another token y1.
At the timestamp of T= 3t, the draft model generates x4, x5, x6while the target model accepts the
first token x4. Then x4, x5, x6is sent to the target model in the next verification phase.
3.3 P OST-VERIFY :CONTINUE DRAFTING DURING VERIFICATION .
Thepost-verify strategy aims at removing the requirement that the drafting phase requires the input
prefix to be verified. However, this assumption brings the limitation that the draft model should be
stuck until the target model finishes verification.
Therefore, we discard this assumption and make another assumption: we directly assume that all
the draft tokens can be accepted. In this way, We find that when all the Œ≥draft tokens are accepted,
sampling a new token from Mp(x+ [x1, ..., x Œ≥])is not necessary, as the draft model could have
generated more draft tokens that can be accepted. Hence we can use the draft model to continue
drafting xŒ≥+1, ..., x 2Œ≥during the verification phase.
Algorithm 1 Parallel Speculative Decoding with Adaptive Draft Length.
Require: the draft model Mq, the target model Mp, the input prefix x, the max generate tokens L,
the window size Œ≥.
Initialization: mode ‚Üê‚Äùpre-verify‚Äù
while len(x)< L do
ifmode = ‚Äùpre-verify‚Äù then
x, mode ‚ÜêPre-verify( Mq, Mp,x, Œ≥)
else
x, mode ‚ÜêPost-verify( Mq, Mp,x, Œ≥)
end if
end while
If all the Œ≥draft tokens are accepted, we can skip the next drafting phase as we already get the
draft tokens in the next drafting phase. The last logit Mp(x+ [x1, ..., x Œ≥])can be used to verify
xŒ≥+1, which is a ‚Äùpre-verify‚Äù process as well. In Figure. 3, at the timestamp of T= 6t, the target
model takes in x4, x5, x6and outputs pt
x4, pt
x5, pt
x6, while the draft model continues to guess next
draft tokens x7, x8, x9. Fortunately, all the draft tokens are accepted, and we can directly conduct
the next verification phase with prefix x+ [y1, x4, x5, x6, x7, x8, x9]. At the timestamp of T= 9t,
the target model takes in x7, x8, x9and outputs pt
x7, pt
x8, pt
x9, while the draft model continues to
guess the next draft tokens x10, x11, x12. Unfortunately, only x8is accepted, and the draft tokens
x10, x11, x12will be dropped. Finally, the prefix x+ [y1, x4, x5, x6, x7, x8, y2]is input to the next
drafting phase.
3.4 PEARL: PARALLEL SPECULATIVE DECODING WITH ADAPTIVE DRAFT LENGTH
Taking together the two strategies, our PEARL framework consists of a draft model, a target model,
and two strategies to decode tokens. The two strategies are switched according to the verification
results in the previous decoding step. Algorithm 1 provides a summary of our PEARL. We also
provide more details in Algorithm 2. Note that pre-verify and post-verify strategies are not executed
only once in the process of generating a sentence and will be repeatedly switched according to the
token acceptance situation during the whole process of generating. We provide a simple step-by-step
5

--- PAGE 6 ---
Published as a conference paper at ICLR 2025
profiling example in Appendix B for better understanding. Then we show how our PEARL achieves
parallelism and adaptive draft length to alleviate the mutual waiting problem.
Parallelism. With the two strategies pre-verify andpost-verify , At any timestamp, the draft model
and the target model are running in parallel, which directly breaks the asynchronous execution of
the draft model and the target model.
Adaptive draft length. In our PEARL, the drafting process can be seen as a segmented drafting
process. If the draft model cannot generate any ‚Äùright‚Äù tokens, the pre-verify strategy will avoid the
additional drafting process. If the draft model could have generated more ‚Äùright‚Äù tokens, the target
model would not interrupt the drafting phase, where the draft model can generate more draft tokens
with post-verify strategy. Therefore, PEARL can utilize the two simple yet effective strategies to
implement adaptive draft length to alleviate the mutual waiting problem.
4 E XPERIMENTS
4.1 E XPERIMENTAL SETUP
Tasks and Datasets. We conduct experiments on various text generation tasks to evaluate the effec-
tiveness of our PEARL, including HumanEval (code generation task) (Chen et al., 2021), GSM8K &
MGSM (multilingual arithmetic reasoning task, MGSM is the multilingual translation of GSM8K)
(Cobbe et al., 2021; Shi et al.), and MT-bench (multi-round dialogue task) (Zheng et al., 2024).
These tasks and datasets are representative benchmarks for evaluation. More details can be found in
Appendix C.1.
Evaluation Details. We evaluate the effectiveness of our PEARL with some state-of-the-art LLM
families, including CodeLlama (Roziere et al., 2023), Deepseek-Coder (Guo et al., 2024), Llama 2
(Touvron et al., 2023) and Llama 3.1 (Dubey et al., 2024). In our experiments, the models with size
less than 7B are used as the draft models and the models with size greater than 33B are used as the
target models. We report the walltime speedup ratio as the metric. Additional evaluation details are
provided in Appendix C.2 and C.3.
Baseline Methods. We implement four training-free inference acceleration methods as our base-
lines. (i) Speculate decoding: standalone SD methods (Leviathan et al., 2023; Chen et al., 2023)
resort to a draft model to draft future tokens and then verify them in parallel. (ii) Ouroboros:
ouroboros (Zhao et al., 2024) proposes phrase candidate pool from the verification process to gen-
erate more precise and longer drafts. (iii) Lookahead Decoding: look ahead decoding (Fu et al.,
2024) caches the generation trajectory (n-grams) as drafts to reduce the number of total decoding
steps. (iv) Assisted generation: assisted generation (Joao Gante, 2023) employs a heuristic ap-
proach to determine the number of draft tokens in the next iteration, based on the verification results
of tokens generated by the draft model in the previous round.
Table 1: Experiment results on the code generation task. Part of the results of Lookahead Decoding
and Ouroboros are taken from (Zhao et al., 2024). We bold the best results for each model
combination.‚àóSome results of ouroboros and lookahead decoding are reproduced in their official
implementation with default parameters. Other results are reproduced in our implementation. The
symbol ‚Äô-‚Äô denote that the methods do not support current model configuration.2
MethodCodeLlama CodeLlama Llama2 Llama3.1 DeepSeek DeepSeek
7&34B 7&70B 7&70B 8&70B 1.3&33B 6.7&33B
Auto Regressive 1.00 √ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó
Speculative Decoding (Leviathan et al., 2023) 1.76 √ó 3.03√ó 2.35√ó 2.60√ó 2.32√ó 1.94√ó
Ouroboros (Zhao et al., 2024) 2.14 √ó‚àó3.28√ó‚àó2.10√ó -‚àó3.25√ó‚àó2.66√ó
Lookahead Decoding (Fu et al., 2024) 1.72 √ó‚àó1.57√ó‚àó1.80√ó -‚àó1.82√ó‚àó1.82√ó
Assisted Generation (Joao Gante, 2023) 1.37 √ó 2.49√ó 2.27√ó 2.72√ó 1.88√ó 1.52√ó
PEARL (ours) 2.48 √ó 4.43√ó 3.29√ó 3.87√ó 3.48√ó 2.79√ó
2Their implementation requires transformers version of 4.36.2, while Llama 3.1 requires transformers ‚â•
4.43.0
6

--- PAGE 7 ---
Published as a conference paper at ICLR 2025
Table 2: Experiment results on the multilingual arithmetic reasoning task with Llama 2 7&70B.
Webold the best results for each category. Results of ouroboros and lookahead decoding are
reproduced in their official implementation with default parameters. Other results are reproduced in
our implementation.
Method English (GSM8K) Bengali German Spanish French Japanese Russian Swahili Tegulu Thai Chinese Avg.
Auto Regressive 1.00 √ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó
Speculative Decoding 2.48 √ó 2.69√ó 2.77√ó 2.64√ó 2.71√ó 2.71√ó 2.72√ó 2.81√ó 2.65√ó 2.71√ó 2.78√ó 2.70√ó
Ouroboros 1.60 √ó 1.75√ó 1.88√ó 1.69√ó 1.80√ó 1.95√ó 1.65√ó 1.68√ó 2.45√ó 1.92√ó 1.81√ó 1.84√ó
Lookahead Decoding 1.23 √ó 1.34√ó 1.51√ó 1.50√ó 1.48√ó 1.29√ó 1.43√ó 1.60√ó 1.28√ó 1.23√ó 1.48√ó 1.39√ó
Assisted Generation 1.96 √ó 1.69√ó 1.75√ó 1.70√ó 1.67√ó 2.02√ó 1.68√ó 1.58√ó 3.07√ó 2.17√ó 1.97√ó 1.93√ó
PEARL (ours) 3.82 √ó 3.94√ó 4.00√ó 3.81√ó 3.76√ó 3.94√ó 3.85√ó 4.18√ó 4.10√ó 3.93√ó 4.06√ó 3.95√ó
Table 3: Experiment results on the multi-round dialogue task with Llama 2 7&70B. We bold the
best results for each category. Results of ouroboros and lookahead decoding are reproduced in their
official implementation with default parameters. Other results are reproduced in our implementation.
Method Writing Roleplay Reasoning Math Coding Extraction Stem Humanities Avg.
Auto Regressive 1.00 √ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó
Speculative Decoding 1.70 √ó 1.73√ó 1.96√ó 2.00√ó 1.93√ó 2.14√ó 1.87√ó 1.81√ó 1.89√ó
Ouroboros 1.42 √ó 1.35√ó 1.40√ó 1.61√ó 1.35√ó 1.67√ó 1.44√ó 1.36√ó 1.45√ó
Lookahead Decoding 1.31 √ó 1.24√ó 1.50√ó 1.51√ó 1.38√ó 1.40√ó 1.29√ó 1.27√ó 1.36√ó
Assisted Generation 1.41 √ó 1.40√ó 1.39√ó 1.64√ó 1.74√ó 1.92√ó 1.57√ó 1.47√ó 1.55√ó
PEARL (ours) 2.40 √ó 2.45√ó 2.85√ó 2.79√ó 2.67√ó 2.92√ó 2.58√ó 2.50√ó 2.64√ó
4.2 M AIN RESULTS .
We conduct extensive experiments on the aforementioned benchmarks. As shown in Table 1,
PEARL significantly outperforms vanilla speculative decoding, Ouroboros, Lookahead decoding
and assisted generation in all backbone model configurations on the HumanEval dataset, which
encompass different scales of model configurations including 1.3&33B,6.7&33B (7&34B) and
7&70B. Specifically, PEARL can achieve up to 4.43√óand1.50√óspeed up compared with vanilla
auto-regressive methods and vanilla speculative decoding, respectively. These results indicate the
universal existence of the mutual waiting problem, and demonstrate that PEARL effectively ad-
dresses the mutual waiting problem, thereby achieving significant inference acceleration results
compared to methods based on the traditional draft-then-verify framework. Moreover, as shown
in Table 2, 3, PEARL can also achieve significant inference acceleration on 12 multilingual arith-
metic tasks and 8 multi-round dialogue tasks, whereas PEARL can achieve 1.36‚àº1.55√óspeedup
compared with vanilla speculative decoding. These results demonstrate the superior potential of the
parallel speculative decoding framework to exploit the computation resources more adequately. We
provide more evaluation results on MGSM and MT-bench with more advanced Llama 3.1 8&70B
in Appendix D.
4.3 A BLATION STUDIES
To provide more insights into the two proposed strategies, we conduct the ablation study. We denote
PEARL without pre-verify as PEARL w/o pre-verify and PEARL without post-verify as PEARL w/o
post-verify and present the main results of ablation studies.
As shown in Table 4, the absence of any strategy of PEARL results in a performance degradation of
the entire framework. The absence of the post-verify strategy exhibits a more pronounced impact on
the performance of PEARL than the pre-verify strategy. We explain the reason for this phenomenon
as follows. Intuitively, the pre-verify strategy makes more contributions when the acceptance rate
is relatively low. The pre-verify strategy can save a target model forward when the first draft token
is rejected by the target model. Denote the acceptance rate as Œ±, and the pre-verify strategy will
take effect with probability 1‚àíŒ±. Therefore, better alignment between the draft model and the
target model will make pre-verify strategy less effective. However, the post-verify strategy makes
more contributions when the two models are aligned, i.e., there are more situations in which all
draft tokens are accepted by the target model. Therefore, the two strategies are complementary and
accelerate inference together.
7

--- PAGE 8 ---
Published as a conference paper at ICLR 2025
Table 4: Ablation results of PEARL on HumanEval and GSM8K datasets.
HumanEval GSM8K
Methods CodeLlama 7B&34B CodeLlama 7B&70B DeepSeek 1.3B&33B Llama 2 7B&70B
PEARL w/o pre-verify 2.21√ó(‚Üì0.14) 3.53√ó(‚Üì0.26) 3.19√ó(‚Üì0.29) 2.51√ó(‚Üì0.26)
PEARL w/o post-verify 1.64√ó(‚Üì0.71) 2.57√ó(‚Üì1.22) 2.37√ó(‚Üì1.11) 2.15√ó(‚Üì0.72)
PEARL 2.35√ó 3.79√ó 3.48√ó 2.87√ó
In our experiments, all the model combinations show great alignment, which leads to the superiority
of the post-verify strategy. As the language models evolve and more speculative decoding methods,
the alignment between the draft model and the target model will be better, which further highlights
the importance of the post-verify strategy. Meanwhile, we can further improve the pre-verify strat-
egy by pre-verifying multiple draft tokens (similar to the cache pool in Ouroboros and Lookahead
Decoding) for more acceleration. We leave these as future works.
4.4 C ASE STUDIES
In this subsection, we present two important case studies to discuss the sensitive analysis of Œ≥and
the mean accepted tokens of PEARL. We provide more experiment results in Appendix D.
4.4.1 S ENSITIVE ANALYSIS OF THE WINDOW SIZE Œ≥
Intuitively, the optimal value of the window size Œ≥should be the speed ratio cbetween the draft
model and the target model, where the draft model and the target model can achieve best parallelism
and fully alleviate the mutual waiting problem. However, it is often the case that cmay not be an
integer. Therefore, we propose to select the round integer of cas the window size Œ≥. We conduct
some case studies to see the effect of different Œ≥values in different model configurations and differ-
ent tasks. As shown in Table 5 (we mark the round integer below each model configuration), directly
choosing the round integer of cas the window size can achieve the maximal inference acceleration,
which is robust to the model configurations. Meanwhile, as shown in Table 6 (the round integer of c
of Llama 2 7&70B is 5), setting the window size as 5 can achieve the maximal inference acceleration
as well, which is robust to the tasks. These results suggest great convenience of PEARL framework
which alleviates the burden of tuning Œ≥for different model configurations and different tasks.
Table 5: Optimal Œ≥of different model
combinations on HumanEval. (unit: tok/sec)
Œ≥CodeLlama 7&34 CodeLlama 7&70 DeepSeek 6.7&33
(c=3) (c=5) (c=3)
2 33.25 16.28 30.82
3 46.06 23.14 48.46
4 44.12 29.65 47.22
5 44.93 40.72 46.91
6 41.83 35.39 44.36Table 6: Optimal Œ≥for different tasks of Llama 2
7B&70B. (c=5)
Œ≥HumanEval GSM8K MT-Bench
3 20.39 18.23 17.67
4 24.58 21.81 20.69
5 30.34 26.47 24.25
6 28.02 24.59 22.71
7 28.09 24.23 22.54
4.4.2 M EAN ACCEPTED TOKENS
In Section 3.4, we claim that PEARL can achieve adaptive draft length for acceleration. To further
illustrate the real mean accepted tokens in PEARL under real-world complex conditions, we conduct
experiments on the HumanEval, GSM8K, and MT-Bench datasets. As shown in Table 7, we still
empirically observe that that PEARL obtains more accepted tokens compared to vanilla SD meth-
ods, which further demonstrates the effectiveness of the PEARL framework. Specifically, PEARL
achieves the max number of mean accepted tokens to 39.9, which significantly outperforms vanilla
SD methods by a large margin. Note that the mean accepted tokens ( MAT ) and the speed ratio c be-
tween the draft model and the target model both influence the final speed-up results. For example, in
the case of Deepseek 6.7&33B, the draft model runs approximately three times faster than the target
model. Even if the MAT approaches infinity, where all tokens are generated by the 6.7B model, the
theoretical maximum speed-up would be capped at 3 √ó. Consequently, with a MAT of 39.9, PEARL
8

--- PAGE 9 ---
Published as a conference paper at ICLR 2025
achieves a 2.75 √óspeed-up, which is close to this theoretical optimum. These results demonstrate
that our PEARL can fully exploit the inference ability of the draft model for further acceleration.
Table 7: Comparison of mean accepted tokens of vanilla SD methods and PEARL.
Methods CodeLlama 7B&34B CodeLlama 7B&70B DeepSeek 1.3B&33B DeepSeek 6.7B&33B
Speculative Decoding 5.27 8.32 7.23 5.69
Ours 27.95 26.53 29.65 39.90
4.5 M ORE DISCUSSIONS ON PEARL
Clarification of the application scenarios. First, we would like to clarify the application scenar-
ios of our PEARL. The key idea of speculative decoding is to exploit the computation redundancy
for acceleration Leviathan et al. (2023). Based on this idea, we observe the mutual waiting problem,
which hinders speculative decoding to fully utilize the redundant computational resources. There-
fore, the main application scenarios of PEARL focus on the scenarios with adequate computational
resources, where speculative decoding cannot sufficiently use these resources.
PEARL in resource-adequate scenarios. In such scenarios, the draft model and the target model
can be deployed separately. Simultaneously running the draft model and the target model would not
bring additional latency in either both drafting or verification stages. Our main experiments along
with all baseline methods are conducted under these scenarios, where we deploy the draft model and
the target model on different devices. Besides, it is feasible to integrate tensor parallelism (TP) with
PEARL in these situations. We provide the solution in Appendix F.
PEARL in resource-constrained scenarios. However, we acknowledge that in many situations,
the GPU resources are limited, and the draft model and the target model are deployed on the same
devices. We refer to this as a ‚Äúco-locate‚Äù setting or resource competitions (RC). The key problem
lies in the nature of GPU hardware design‚Äîtwo running processes on the same GPU will compete
for GPU resources, which may lead to slowdowns. We provide a solution to address this issue.
Generally, in real-world LLM applications, the large-scale target model is usually placed with more
than 1 GPU to handle more requests and long context inference, while the small-scale draft model
only needs 1 GPU for inference. In this case, we can apply pipeline parallelism (PP) to serve the
target model with multiple GPUs. Inspired by this observation, we propose an improved version of
PEARL to effectively utilize GPU computation resources with PP without resource competitions.
The key idea is to transfer the computation of the draft model to another GPU when the target model
is running on a specific GPU. Specifically, we transfer the first ‚åàŒ≥
2‚åâdraft token generation to the last
device, while the last ‚åäŒ≥
2‚åãdraft tokens are generated with the first device. As the computation of the
target model is conducted sequentially with multiple GPUs, this could effectively utilize the GPU
resources to avoid RC. We conduct some experiments in Table 8 and find that this strategy allows
PEARL to retain 89%‚àº99% of its original performance, demonstrating the effectiveness of our
PEARL in such conditions. We provide detailed implementation and additional experiment results
of this strategy in Appendix G.
Table 8: Comparisons of Llama models for PEARL on MT-bench with and without RC scenario.
Models Writing Roleplay Reasoning Math Coding Extraction Stem Humanities Avg.
Llama 2 7b&70b 22.10 22.47 26.16 25.74 24.63 26.11 23.84 23.09 24.28
Llama 2 7b&70b (RC) 19.88 20.24 25.06 24.47 23.47 25.79 21.55 22.03 22.83
performance retain 89.95% 90.08% 95.80% 95.07% 95.29% 98.77% 90.39% 95.41% 94.03%
Llama 3.1 8b&70b 31.23 30.08 35.09 36.59 31.95 34.60 30.06 27.51 32.14
Llama 3.1 8b&70b (RC) 29.65 27.54 35.01 36.31 29.85 33.99 26.77 26.10 30.78
performance retain 94.94% 91.56% 99.77% 99.23% 93.43% 98.24% 89.06% 94.87% 95.77%
5 R ELATED WORK
Transformer inference acceleration. Inference acceleration is a field that has been extensively
studied over a long period of time (Liu et al., 2024). There exists extensive works for transformer
9

--- PAGE 10 ---
Published as a conference paper at ICLR 2025
inference acceleration with the rising of LLMs (Xia et al., 2024; Lv et al., 2024; Chen et al., 2024).
This includes efforts of model compression (Zhu et al., 2023), efficient architecture design (Chitty-
Venkata & Somani, 2022), and hardware optimization and implementation (Dao et al., 2022). Model
compression methods such as quantization (Choi et al., 2018), knowledge distillation (Hinton et al.,
2015), and structure pruning (Han et al., 2015) aim at reducing the number of computational op-
erations. Efficient architecture design is proposed to develop lightweight transformer architectures.
Hardware optimization and implementation is proposed for efficient execution to fully exploit the
hardware devices. These methods have achieved great success, while they are orthogonal to specu-
lative decoding algorithms, which can be integrated for further speedup.
Draft-then-verify framework. While SD exhibits great acceleration effectiveness and lossless gen-
eralization quality, it remains a challenge to find a compact draft model with high distribution align-
ment. Some works focus on removing the necessity of the draft model. Self-speculative decoding
(Zhang et al., 2023) proposes to skip some intermediate layers of the target model for drafting.
Medusa (Cai et al., 2024) adds extra decoding heads at the top of the target model to generate drafts.
Lookahead decoding(Fu et al., 2024) caches the generation trajectory (n-grams) as the drafts. Eagle
(Li et al., 2024) employs an additional transformer decoder layer to generate drafts at the feature
level. Glide (Du et al., 2024) reuses the kv cache from the target model to decode more accurate
draft tokens. DistillSpec (Zhou et al., 2023) utilizes distillation method to identify a compact draft
model. Ouroboros (Zhao et al., 2024) combines the standard SD and lookahead decoding to gener-
ate more precise and longer drafts. Besides these works, SpecInfer (Miao et al., 2023) proposes tree
attention, which is widely used to verify more drafts and increase the acceptance rate. However, all
of them do not address the parallelism issue. From this perspective, our PEARL is orthogonal to
these methods and can be integrated with these methods, which is left as a future work.
6 C ONCLUSION AND FUTURE WORK
Limitations and broader impact. As our PEARL is a parallel acceleration framework, it remains
a challenge to schedule the GPU resources to avoid resource competitions, which may potentially
increase power consumption. We affirm our commitment to contributing positively to society, avoid-
ing harm, and upholding honesty and trustworthiness. We appropriately cite the previous methods
and datasets we use, and ensure that all data involved is fully public, with no private data being uti-
lized. Furthermore, we are committed to correctly maintaining the inference acceleration techniques
we have developed, without incurring any form of discrimination.
Conclusion. In this paper, we propose a novel inference acceleration framework, called PEARL,
which significantly improves LLM inference efficiency. PEARL consists of two simple and effective
strategies, i.e., pre-verify andpost-verify , which effectively alleviates the mutual waiting problem
with parallelism and adaptive draft length. Extensive experiments demonstrate that our proposed
PEARL outperforms existing state-of-the-art methods on various text generation benchmarks.
Future work. For future research, we aim to integrate PEARL with existing accelerated inference
methods to explore more efficient and resource-friendly acceleration approaches for LLM inference.
Hopefully, PEARL will facilitate the future development of LLM inference acceleration.
ACKNOWLEDGEMENTS
The authors would like to thank all the anonymous reviewers for their insightful comments.
REFERENCES
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical
report. arXiv preprint arXiv:2303.08774 , 2023.
Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,
Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportu-
nities and risks of foundation models. arXiv preprint arXiv:2108.07258 , 2021.
10

--- PAGE 11 ---
Published as a conference paper at ICLR 2025
Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee, Deming Chen, and Tri
Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv
preprint arXiv:2401.10774 , 2024.
Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John
Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint
arXiv:2302.01318 , 2023.
Hanzhu Chen, Xu Shen, Qitan Lv, Jie Wang, Xiaoqi Ni, and Jieping Ye. Sac-kg: Exploiting large
language models as skilled automatic constructors for domain knowledge graph. In Proceedings
of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers) , pp. 4345‚Äì4360, 2024.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374 , 2021.
Krishna Teja Chitty-Venkata and Arun K Somani. Neural architecture search survey: A hardware
perspective. ACM Computing Surveys , 55(4):1‚Äì36, 2022.
Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srini-
vasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural
networks. arXiv preprint arXiv:1805.06085 , 2018.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to
solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R ¬¥e. Flashattention: Fast and memory-
efficient exact attention with io-awareness. Advances in Neural Information Processing Systems ,
35:16344‚Äì16359, 2022.
DeepSeek-AI. Deepseek-v3 technical report, 2024. URL https://arxiv.org/abs/2412.
19437 .
Cunxiao Du, Jing Jiang, Xu Yuanchen, Jiawei Wu, Sicheng Yu, Yongqi Li, Shenggui Li, Kai Xu,
Liqiang Nie, Zhaopeng Tu, et al. Glide with a cape: A low-hassle method to accelerate speculative
decoding. arXiv preprint arXiv:2402.02082 , 2024.
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha
Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.
arXiv preprint arXiv:2407.21783 , 2024.
Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Break the sequential dependency of llm infer-
ence using lookahead decoding. arXiv preprint arXiv:2402.02057 , 2024.
Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao
Bi, Y Wu, YK Li, et al. Deepseek-coder: When the large language model meets programming‚Äìthe
rise of code intelligence. arXiv preprint arXiv:2401.14196 , 2024.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149 , 2015.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531 , 2015.
Joao Gante. Assisted generation: a new direction toward low-latency text generation, 2023. URL
https://huggingface.co/blog/assisted-generation .
Diksha Khurana, Aditya Koli, Kiran Khatter, and Sukhdev Singh. Natural language processing:
State of the art, current trends and challenges. Multimedia tools and applications , 82(3):3713‚Äì
3744, 2023.
11

--- PAGE 12 ---
Published as a conference paper at ICLR 2025
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.
Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model
serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating
Systems Principles , 2023.
Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative
decoding. In International Conference on Machine Learning , pp. 19274‚Äì19286. PMLR, 2023.
Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling requires
rethinking feature uncertainty. arXiv preprint arXiv:2401.15077 , 2024.
Tianyu Liu, Qitan Lv, Jie Wang, Shuling Yang, and Hanzhu Chen. Learning rule-induced subgraph
representations for inductive relation prediction. Advances in Neural Information Processing
Systems , 36, 2024.
Qitan Lv, Jie Wang, Hanzhu Chen, Bin Li, Yongdong Zhang, and Feng Wu. Coarse-to-fine high-
lighting: Reducing knowledge hallucination in large language models. In Forty-first International
Conference on Machine Learning , 2024.
Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong,
Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating
generative llm serving with speculative inference and token tree verification. arXiv preprint
arXiv:2305.09781 , 2023.
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi
Adi, Jingyu Liu, Tal Remez, J ¬¥er¬¥emy Rapin, et al. Code llama: Open foundation models for code.
arXiv preprint arXiv:2308.12950 , 2023.
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush V osoughi,
Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. Language models are multi-
lingual chain-of-thought reasoners, 2022. URL https://arxiv. org/abs/2210.03057 .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ¬¥ee
Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.
Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and
Zhifang Sui. Unlocking efficiency in large language model inference: A comprehensive survey
of speculative decoding. arXiv preprint arXiv:2401.07851 , 2024.
Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, and Sharad Mehrotra. Draft &
verify: Lossless large language model acceleration via self-speculative decoding. arXiv preprint
arXiv:2309.08168 , 2023.
Weilin Zhao, Yuxiang Huang, Xu Han, Chaojun Xiao, Zhiyuan Liu, and Maosong Sun. Ouroboros:
Speculative decoding with large model enhanced drafting. arXiv preprint arXiv:2402.13720 ,
2024.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and
chatbot arena. Advances in Neural Information Processing Systems , 36, 2024.
Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao
Zhang. Distserve: Disaggregating prefill and decoding for goodput-optimized large language
model serving. arXiv preprint arXiv:2401.09670 , 2024.
Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh,
Sanjiv Kumar, Jean-Franc ¬∏ois Kagy, and Rishabh Agarwal. Distillspec: Improving speculative
decoding via knowledge distillation. arXiv preprint arXiv:2310.08461 , 2023.
Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. A survey on model compression for
large language models. arXiv preprint arXiv:2308.07633 , 2023.
12

--- PAGE 13 ---
Published as a conference paper at ICLR 2025
A A LGORITHM OF PEARL
Here, we give the whole algorithm of our PEARL in detail in Algorithm. 2.
Algorithm 2 Parallel Speculative Decoding with Adaptive Draft Length.
Input: the draft model Mq, the target model Mp, the input prefix x, the max generate tokens L, the
window size Œ≥.
‚ñ∑Thepre-verify strategy is used first.
1:Initialization: mode ‚Üê‚Äùpre-verify‚Äù
2:while len(x)< L do
3: ifmode = ‚Äùpre-verify‚Äù then
4: ‚ñ∑Pre-verify strategy
5: fori= 1toŒ≥do
6: qi‚ÜêMq(x+ [x1, ..., x i‚àí1])
7: xi‚àºqi
8: end for
9: ‚ñ∑running the target model in parallel to verify the first draft token in advance.
10: p‚ÜêMp(x)
11: ifr‚àºU(0,1)‚â§p[x1]
q1[x1]then
12: ‚úìaccept the first token
13: x‚Üêx+ [x1, ..., x Œ≥]
14: mode‚Üê‚Äùpost-verify‚Äù
15: else
16: √óreject the first token
17: y‚àºnorm (max(0, p‚àíq1))
18: x‚Üêx+ [y]
19: mode‚Üê‚Äùpre-verify‚Äù
20: end if
21: else
22: ‚ñ∑Post-verify strategy
23: x,[x1, x2, ..., x Œ≥]‚Üêx ‚ñ∑split the prefix to get the last Œ≥draft tokens
24: fori=Œ≥+ 1to2Œ≥do
25: ‚ñ∑running the draft model in parallel to continue drafting.
26: qi‚ÜêMq(x+ [x1, ..., x i‚àí1])
27: xi‚àºqi
28: end for
29: p1, p2, ..., p Œ≥‚ÜêMp(x+ [x1]), Mp(x+ [x1, x2]), ..., M p(x+ [x1, ..., x Œ≥])
30: retrival q1, q2, ..., q Œ≥from the cache
31: r1‚àºU(0,1), ..., r Œ≥‚àºU(0,1)
32: n‚Üêmin({i‚àí1|1‚â§i‚â§Œ≥, ri>pi[xi]
qi[xi]} ‚à™ {Œ≥})
33: ifn=Œ≥then
34: ‚úìaccept all draft tokens
35: x‚Üêx+ [x1, ..., x 2Œ≥]
36: mode‚Üê‚Äùpost-verify‚Äù
37: else
38: √óreject someone
39: y‚àºnorm (max(0, pn+1‚àíqn+1))
40: x‚Üêx+ [x1, ..., x n, y]
41: mode‚Üê‚Äùpre-verify‚Äù
42: end if
43: end if
44:end while
13

--- PAGE 14 ---
Published as a conference paper at ICLR 2025
B A S IMPLE STEP-BY-STEP PROFILING EXAMPLE
We provide a simple step-by-step profiling of PEARL with a real data prompt ‚Äúx+y = 4z, x*y =
4zÀÜ2, express x-y in z‚Äù in Table 9.
Table 9: Simple step-by-step profiling of PEARL with prompt ‚Äúx+y = 4z, x*y = 4zÀÜ2, express x-y
in z‚Äù. We only report the first 7 steps for simplicity. The prompt is selected from MT-bench, and we
use Llama 2 7&70b as our base model pair.
steps input prefix current
modedraft
model
outputtarget
model
outputjudging rea-
sonoutput prefix
0 x+y = 4z, x*y = 4zÀÜ2,
express x-y in zpre-
verifyGreat, I‚Äôm I great is not I,
hence great is
rejected, turn
to pre-verifyI
1 I pre-
verify‚Äôm glad
you‚Äô ‚Äô is accepted,
turn to post-
verifyI‚Äô
2 I‚Äôm glad you post-
verifyare inter-
ested in
exploringm m is accepted,
but glad is re-
jected, turn to
pre-verifyI‚Äôm happy
3 I‚Äôm happy pre-
verifyto help
you with
thisto to is accepted,
turn to post-
verifyI‚Äôm happy to
4 I‚Äôm happy to help
you with thispost-
verifyequation!
However,
Ihelp you
withthe first 3
tokens are
accepted,
but this is
rejected, turn
to pre-verifyI‚Äôm happy to
help you with
your
5 I‚Äôm happy to help
you with yourpre-
verifyquestion!
However,
Iquestion question is ac-
cepted, turn to
post-verifyI‚Äôm happy to
help you with
your question
6 I‚Äôm happy to help
you with your ques-
tion! However, Ipost-
verifynotice that
the equa-
tion you! How-
ever, I no-
ticeall previous
draft tokens
are accepted,
keep post-
verifyI‚Äôm happy
to help you
with your
question!
However, I
notice
7 I‚Äôm happy to help
you with your ques-
tion! However, I no-
tice that the equation
youpost-
verifyprovided
is not
correct.that the equation is re-
jected. turn to
pre-verifyI‚Äôm happy
to help you
with your
question!
However, I
notice that the
equations
we give some explanations about the whole process.
1) At step 0, the prompt ‚Äùx+y = 4z, x*y = 4zÀÜ2, express x-y in z‚Äù is input to the draft model and
the target model simultaneously with strategy pre-verify. Within the left of this explanation, we
omit this original prompt for simplicity. The draft model outputs [Great, I‚Äôm], while the target
model outputs [I]. Then, we will use [I] to verify the first token [Great] in the draft tokens. As it
14

--- PAGE 15 ---
Published as a conference paper at ICLR 2025
is not the same, we reject the draft tokens, save a verification stage of the other draft tokens for
acceleration, and the output prefix is [I]. As there exists a rejected token, the next strategy is still
pre-verify.
2) At step 1, the prefix [I] is input, and the draft model outputs [‚Äôm glad you] and the target model
outputs [‚Äô]. This time, [‚Äô] is accepted by the target model, and the next strategy is tuned to
post-verify.
3) At step 2, the prefix together with the other draft tokens [I‚Äôm glad you] is input. The draft model
outputs [are interested in exploring] while the target model outputs [m], i.e., the first draft token
[m] is accepted, but the second draft token [glad] is rejected. The target model additionally
appends [happy] to the prefix. As there exists a rejected token, the next strategy is pre-verify.
4) At step 3, the prefix [I‚Äôm happy] is input, and the draft model outputs [to help you with this]
and the target model outputs [to]. This time, [to] is accepted by the target model, and the next
strategy is tuned to post-verify.
5) At step 4, the prefix together with the other draft tokens [I‚Äôm happy to help you with this] is
input. The draft model outputs [equation! However, I] while the target model outputs [help you
with], i.e., the first three draft tokens [help you with] is accepted, but the fourth draft token [you]
is rejected. The target model additionally appends [your] to the prefix. As there exists a rejected
token, the next strategy is pre-verify.
6) At step 5, the prefix [I‚Äôm happy to help you with your] is input, and the draft model outputs
[question! However, I] and the target model outputs [question]. This time, [question] is accepted
by the target model, and the next strategy is tuned to post-verify.
7) At step 6, the prefix together with the other draft tokens [I‚Äôm happy to help you with your ques-
tion! However, I] is input. The draft model outputs [notice that the equation you] while the target
model outputs [! However, I notice]. All the draft tokens are accepted, and the next strategy is
still post-verify. Therefore, we save the times of the draft model forward for acceleration.
8) At step 7, the prefix together with the other draft tokens [I‚Äôm happy to help you with your ques-
tion! However, I notice that the equation you] is input. The draft model outputs [provided is
not correct] while the target model outputs [that the], i.e., the first two draft tokens [that the] are
accepted, but the third draft token [equation] is rejected. The target model additionally appends
[equations] to the prefix. As there exists a rejected token, the next strategy is pre-verify.
C E VALUATION DETAILS
C.1 D ATASET CONFIGURATIONS
In our experiments, we evaluate the effectiveness of our PEARL on 4 categories of text generation
tasks, including code generation, arithmetic reasoning, multilingual inference, and multi-round di-
alogue. For the code generation task, we employ HumanEval (Chen et al., 2021), a famous code
generation benchmark which is composed of 164 entries. For arithmetic reasoning and multilingual
inference, we employ GSM8K and MGSM (Cobbe et al., 2021; Shi et al.) as the evaluation bench-
mark. As the GSM8K is the English version of MGSM, we report their results in the same table. For
GSM8K, we sample the first 100 entries for evaluation. For the other 10 categories in MGSM, we
select 10 entries for each language. For multi-round dialogue, we employ MT-bench(Zheng et al.,
2024) as the benchmark. The maximum generation lengths of these tasks are respectively set to
1024, 256, 256, and 256.
C.2 M ODEL CONFIGURATIONS
We select some representative models for evaluation, including Llama 2 Touvron et al. (2023),
Codellama Roziere et al. (2023), and Deepseek-Coder Guo et al. (2024). We summarize the model
configuration in Table 10. In our experiments, all models are loaded in the precision of bfloat-16.
Our PEARL does not introduce any additional training, and directly uses these models to evaluate
our algorithm. The running speed is measured on the code generation tasks.
15

--- PAGE 16 ---
Published as a conference paper at ICLR 2025
Table 10: Detailed model configurations.
Models Layers dim FFN dim speed (tok/s)
Codellama-7B 32 4096 11008 49.34
Codellama-34B 48 8192 22016 18.58
Codellama-70B 80 8192 28672 9.20
Deepseek-1.3B 24 2048 5504 63.20
Deepseek-6.7B 32 4096 11008 50.05
Deepseek-33B 62 7168 19200 17.37
Llama-2-7B 32 4096 11008 49.94
Llama-2-70B 80 8192 28672 9.22
Llama-3.1-8B 32 4096 14336 44.37
Llama-3.1-70B 80 8192 28672 9.00
C.3 E VALUATION DETAILS
All of our experiments including latency measurement, ablation studies, and case studies are con-
ducted on NVIDIA A100-SXM4-80G GPUs. For models with sizes of 1.3B and 7B, we put them
on a single A100, while 34B models are deployed on 2 A100, and 70B models are deployed on 3
A100. For inference, we use batch size 1, which is commonly used in other speculative decoding
works. For the compared baselines, including Lookahead decoding and Ouroboros, we reproduce
the results of them on the code generation tasks with the default parameters as described in their
paper or code. When evaluating these methods, the model configuration and GPU usage are the
same as our PEARL.
Table 11: The number of model runs of the draft model and the target model with different model
configurations on HumanEval
Draft Model (SD) Target Model (SD) Draft Model (PEARL) Target Model (PEARL)
Deepseek 1.3B&33B 140500 35125 181864 (1.29 √ó) 45466 (1.29 √ó)
Deepseek 6.7B&33B 128973 42991 174855 (1.35 √ó) 58285 (1.36 √ó)
Codellama 7B&34B 132054 44018 181020 (1.37 √ó) 60340 (1.37 √ó)
Codellama 7B&70B 151960 30392 198370 (1.30 √ó) 39674 (1.30 √ó)
Llama2 7B&70B 175460 35092 248720 (1.41 √ó) 49744 (1.42 √ó)
As our PEARL is a parallel inference acceleration framework, we implement the parallel algorithm
in accelerate, which can be further optimized with other parallel techniques. We leave this as a
potential future work to acquire more acceleration.
D M ORE EXPERIMENT RESULTS
D.1 E VALUATION RESULTS OF LLAMA 3.1 ONMT- BENCH AND MGSM
As illustrated in Section 4.2, we provide more evaluation results of PEARL in Table 12 and 13 with
both Llama 2 7&70B and Llama 3.1 8&70B. Notably, Llama 3.1 is a more advanced LLM series
which requires the transformers version to be greater than 4.43.0. Therefore, we cannot reproduce
the results of baseline Ouroboros and Lookahead Decoding.
Table 12: Multi-language experiment results using Llama 3.1 8B&70B on GSM8K and MGSM
(Cobbe et al., 2021; Shi et al.). We bold the best results for each language.
Method English (GSM8K) Bengali German Spanish French Japanese Russian Swahili Tegulu Thai Chinese Avg.
Auto Regressive 1.00 √ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó
Speculative Decoding 2.48 √ó 2.69√ó 2.77√ó 2.64√ó 2.71√ó 2.71√ó 2.72√ó 2.81√ó 2.65√ó 2.71√ó 2.78√ó 2.70√ó
Ours 3.82 √ó 3.94√ó 4.00√ó 3.81√ó 3.76√ó 3.94√ó 3.85√ó 4.18√ó 4.10√ó 3.93√ó 4.06√ó 3.95√ó
16

--- PAGE 17 ---
Published as a conference paper at ICLR 2025
Table 13: Experiment results using Llama2 7B&70B and Llama 3.1 8B&70B on MT-bench (Zheng
et al., 2024). We bold the best results for each subtask.
Model Configuration Method Writing Roleplay Reasoning Math Coding Extraction Stem Humanities Avg.
Llama 2 7B&70BAuto Regressive 1.00 √ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó
Speculative Decoding 1.70 √ó 1.73√ó 1.96√ó 2.00√ó 1.93√ó 2.14√ó 1.87√ó 1.81√ó 1.89√ó
Ours 2.40 √ó 2.45√ó 2.85√ó 2.79√ó 2.67√ó 2.92√ó 2.58√ó 2.50√ó 2.64√ó
Llama 3.1 8B&70BAuto Regressive 1.00 √ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó
Speculative Decoding 2.29 √ó 2.24√ó 2.66√ó 2.81√ó 2.35√ó 2.64√ó 2.22√ó 2.12√ó 2.42√ó
Ours 3.49 √ó 3.35√ó 3.92√ó 4.06√ó 3.55√ó 3.95√ó 3.34√ó 3.05√ó 3.59√ó
D.2 O PTIMAL Œ≥OFSPECULATIVE DECODING
In recent speculative decoding papers, the compared results of vanilla speculative decoding are
commonly based on a fixed window size Œ≥= 5. We find that vanilla speculative decoding can
achieve better results with appropriate Œ≥. Therefore, all the results of speculative decoding in our
paper are based on their optimal Œ≥. We present some searching results of Œ≥for speculative decoding
in Table 14.
Table 14: Optimal Œ≥values of speculative decoding for each model pair. (Unit: tokens / second)
codellama 7&34B codellama 7&70B deepseek 1.3&33B deepseek 6.7&33B
30.95 ( Œ≥= 4) 25.92 ( Œ≥= 8) 37.22 ( Œ≥= 6) 30.92 ( Œ≥= 4)
31.85 ( Œ≥= 5) 26.02 ( Œ≥= 9) 38.53 ( Œ≥= 7) 31.74 ( Œ≥= 5)
33.57 ( Œ≥= 6) 27.60 ( Œ≥= 10 ) 39.52 ( Œ≥= 8) 33.77 ( Œ≥= 6)
33.52 ( Œ≥= 7) 27.23 ( Œ≥= 11 ) 39.38 ( Œ≥= 9) 33.55 ( Œ≥= 7)
32.79 ( Œ≥= 8) 26.65 ( Œ≥= 12 ) 38.69 ( Œ≥= 10 ) 32.97 ( Œ≥= 8)
D.3 T IMECONSUMPTION OF EACH COMPONENT IN ONEPEARL S TEP
To investigate the influence and potential additional latency of the parallel inference, we measure
the time cost of each component in one PEARL step with different sizes of model pairs in Table 15.
From these results, we can find that the communication cost is negligible. The draft time and the
target time are very close, indicating that PEARL effectively addresses the mutual waiting problem.
Table 15: The time cost of each component in one PEARL step. The experiments are conducted on
HumanEval.
llama 2 7&70B codellama 7&34B deepseek 1.3&33B
communication 0.2 ms 0.3 ms 0.2 ms
verify 1.7 ms 1.6 ms 1.7 ms
draft 105.1 ms 68.9 ms 65.1 ms
target 108.0 ms 71.1 ms 66.1 ms
E F ORWARD TIMES COMPARISON OF PEARL AND SD METHODS
Considering that PEARL is a parallel framework, both the draft model and the target model are
running simultaneously at all times. Therefore, we measure the number of model runs for PEARL
compared to the traditional SD method to provide a more comprehensive perspective in Table 11.
The results show that our PEARL exhibits relatively more forward times of both the draft model and
the target model compared to traditional SD. As our PEARL is a parallel inference framework, which
executes the draft model and the target model in parallel at any timestamp, it naturally increases the
17

--- PAGE 18 ---
Published as a conference paper at ICLR 2025
forward times of the target model and leads to more power consumption. However, the additional
inference time occurs at another process, which will not affect the multi-user throughput.
F I NTEGRATING TP WITH PEARL
In resource-adequate scenarios, it is possible to integrate tensor parallelism (TP) with PEARL. The
key to integrating TP is to deploy the draft model and the target model on separate devices. The
most direct way is to deploy the small-scale draft model on 1 GPU and the large-scale target model
on the rest GPUs. Take the example of 8 GPUs, we can place the draft model on GPU 0, while
the target model is set on GPUs 1-7. In this way, the draft model and the target model can conduct
parallel inference and achieve the best inference speedup. Meanwhile, it is possible to deploy the
draft model on CPU / edge computing. The separation idea is similar to PD dis-aggregation (Zhong
et al., 2024), which dis-aggregates the prefilling and decoding process on different devices to fully
exploit the computation resources. PEARL shares the same idea to disaggregate the decoding
process of the draft model and the target model on different devices.
However, we notice that in the modern TP framework, layer width should be divisible by TP size
(which would not work with TP=7 as shown in the example. We propose to use padding techniques
to address the division issue directly. For example, given a weight matrix W‚àà R4096√ó4096and
TP=7, we can append an extra zero matrix Wpad‚àà R4096√ó6toW, and form a padded weight
matrix ÀÜW‚àà R4096√ó4102. In this way, the dimension issue can be effectively addressed. For other
Wand TP size pairs, we can get the padded matrix similarly. We provide some explanation to show
this padding technique is lossless to the performance.
1. For MLP layers, padding a zero matrix Wpad‚àà Rd√órto the weight matrix does not affect
the final results. Directly remove the final rcolumns of the output can get the original
output.
2. For attention layers, padding a zero matrix Wpad‚àà Rd√órto the weight matrix is equivalent
to padding a zero matrix to Q, K , where Qpad=Q[WQ;WQ
pad], Kpad=K[WK;WK
pad].
The attention weight is computed as QpadKT
pad =Q[WQ;WQ
pad][WK;WK
pad]TKT.
AsWQ
padandWK
padare zero matrices, [WQ;WQ
pad][WK;WK
pad]T=WQ(WK)T, i.e.,
QpadKT
pad=QKT. Therefore, padding does not affect the attention weight matrix.
3. For norm layers, padding a zero matrix may change the scaling factor, e.g., variance in
RMSNorm. When computing these scaling factors, ignoring the additional zeros can keep
the original results.
Due to the complexity of implementing TP with an existing framework (vLLM (Kwon et al., 2023)),
we leave this as a promising future work to integrate TP with PEARL.
G E XPERIMENT RESULTS UNDER LIMITED GPU R ESOURCES
Although our PEARL parallels the draft model and the target model at the algorithmic level, it
still remains a challenge for deployment at the hardware level in the GPU-constrained scenarios,
which we refer to as ‚Äùco-locate‚Äù setting or resource competitions (RC). The key problem lies in the
nature of GPU hardware design ‚Äî- two running processes on the same GPU will compete for GPU
resources, which leads to significant slowdowns.
However, in real-world LLM applications, the large-scale target model is usually placed with more
than 1 GPU to handle more requests and long context inference, while the small-scale draft model
only needs 1 GPU for inference. In this case, pipeline parallelism (PP) is the most common solution
to serve the target model with multiple GPUs, which distributes the parameters to different GPUs
and conducts computations sequentially with these GPUs.
Inspired by this observation, we propose an improved version of PEARL to effectively utilize GPU
computation resources with PP without resource competitions. The key idea is to transfer the com-
putation of the draft model to another GPU when the target model is running on a specific GPU.
Specifically, we transfer the first ‚åàŒ≥
2‚åâdraft token generation to the last device, while the last ‚åäŒ≥
2‚åã
18

--- PAGE 19 ---
Published as a conference paper at ICLR 2025
draft tokens are generated with the first device. As the computation of the target model is conducted
sequentially with multiple GPUs, this method could effectively utilize the GPU resources to avoid
resource competition.
Take an instance of c= 5, the target model is placed with g= 4 GPUs, we denote the time for a
target model forward as t, and the time that the target model runs at GPU 0 ist
4. To analyze the
GPU utilization in detail, we split tintogc= 20 steps, where each step Œ∑=t
20. During one target
model forward, the occupied GPU number in 20 steps is given by:
Mp:0, 0, 0, 0, 0; 1, 1, 1, 1, 1; 2, 2, 2, 2, 2; 3, 3, 3, 3, 3; (2)
Then we can further analyze the occupied GPU number of the draft model with proposed methods.
First, as the draft model can generate ctokens in 20 steps, it only needs 4 steps to generate 1 draft
token. Taking ‚åàŒ≥
2‚åâ= 3,‚åäŒ≥
2‚åã= 2, the first 3√ó4 = 12 steps of the draft model will occupy the GPU
3, while the last 2√ó4 = 8 steps of the draft model will occupy the GPU 0. Therefore, the occupied
GPU number of the draft model in 20 steps is given by:
Mq:3, 3, 3, 3; 3, 3, 3, 3; 3, 3, 3, 3; 0, 0, 0, 0; 0, 0, 0, 0; (3)
In this way, the draft model and the target model will occupy different devices at each step, which
effectively avoids resource competition. However, in real-world settings, moving the draft model
from the last device to the first device is non-trivial and costly. As a compromise, We propose to
load the draft model both at the first device and the last device. During the inference process, we
only move the intermediate KV Cache from the last device to the first device. Many KV Cache
compression methods can help further reduce the cost. As the draft model is relatively small, the
KV cache itself does not incur significant memory overhead. For example, with Llama 3.1 8B, batch
size=1 and input length=1024, the size of the KV cache is 2√ó2√ó32√ó1√ó8√ó1024√ó128‚âà0.13GB.
With NVlink, the theoretical time cost for transporting the KV cache is 0.13/300‚âà0.43ms, which
is significantly lower than the computational time cost. We provide empirical results of the KV
cache transport time cost in Table 16.
Table 16: The empirical time cost of transporting KV cache with different input length. The
experiments are conducted with Llama 3.1 8B on HumanEval.
input length 128 256 512 1024
empirical time cost 1.3 ms 1.4 ms 1.5 ms 1.6 ms
To further evaluate the effectiveness of this method, we conduct some experiments in Table 8, 17
and 18. We found that this strategy allows PEARL to retain 89%‚àº99% of its original performance,
demonstrating the effectiveness of our PEARL in such conditions.
Table 17: Performance comparison of Llama 2 and Llama 3.1 models for PEARL on 4 benchmarks
with and without RC (resource competitions). Using proposed strategy in the appendix, PEARL
can work well in RC settings with only a slight performance decrease ( <5%).
Humaneval GSMBK MT-bench MGSM
llama 2 32.53 30.33 24.28 30.13
llama 2 (RC) 31.52 (0.97√ó) 29.05 (0.96√ó) 22.83 (0.94√ó) 28.56 (0.95√ó)
llama 3.1 33.56 32.97 32.14 35.02
llama 3.1 (RC) 31.83 (0.95√ó) 31.51 (0.96√ó) 30.78 (0.96√ó) 33.65 (0.96√ó)
19

--- PAGE 20 ---
Published as a conference paper at ICLR 2025
Table 18: Performance comparison of Llama 2 and Llama 3.1 models for PEARL on MGSM with
and without RC. Using proposed strategy in the appendix, PEARL can work well in RC settings
with only a slight performance decrease ( <10%).
english (GSMBK) bengali german spanish french japanese russian swahili tegulu thai chinese average
llama 2 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó
llama 2 (RC) 0.96√ó 0.95√ó 0.95√ó 0.94√ó 0.95√ó 0.96√ó 0.94√ó 0.95√ó 0.96√ó 0.93√ó 0.92√ó 0.95√ó
llama 3.1 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó 1.00√ó
llama 3.1 (RC) 0.98√ó 0.94√ó 0.95√ó 0.94√ó 0.94√ó 0.96√ó 0.96√ó 0.95√ó 0.98√ó 0.98√ó 1.00√ó 0.96√ó
20
