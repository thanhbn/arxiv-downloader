# 2309.04255.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/speculative/2309.04255.pdf
# File size: 1667824 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
LLMCad : Fast and Scalable On-device Large Language
Model Inference
Daliang Xu‚ô¶, Wangsong Yin‚ô¶, Xin Jin‚ô¶, Ying Zhang‚ô¶, Shiyun Wei‚ãÜ, Mengwei Xu^,
Xuanzhe Liu‚ô¶
‚ô¶Key Lab of High Confidence Software Technologies (Peking University), Beijing, China
‚ãÜZhongguancun Laboratory, Beijing, China.
^State Key Laboratory of Networking and Switching Technology (BUPT), Beijing, China
{xudaliang,hg,xinjinpku,zhang.ying,weishiyun,liuxuanzhe}@pku.edu.cn
yws@stu.pku.edu.cn
mwx@bupt.edu.cn
ABSTRACT
Generative tasks, such as text generation and question an-
swering, hold a crucial position in the realm of mobile appli-
cations. Due to their sensitivity to privacy concerns, there
is a growing demand for their execution directly on mo-
bile devices. Currently, the execution of these generative
tasks heavily depends on Large Language Models (LLMs).
Nevertheless, the limited memory capacity of these devices
presents a formidable challenge to the scalability of such
models.
In our research, we introduce LLMCad , an innovative on-
device inference engine specifically designed for efficient
generative Natural Language Processing (NLP) tasks. The
core idea behind LLMCad revolves around model collabora-
tion: a compact LLM, residing in memory, takes charge of
generating the most straightforward tokens, while a high-
precision LLM steps in to validate these tokens and rectify
any identified errors. LLMCad incorporates three novel tech-
niques: (1) Instead of generating candidate tokens in a se-
quential manner, LLMCad employs the smaller LLM to con-
struct a token tree, encompassing a wider range of plausible
token pathways. Subsequently, the larger LLM can efficiently
validate all of these pathways simultaneously. (2) It employs
a self-adjusting fallback strategy, swiftly initiating the verifi-
cation process whenever the smaller LLM generates an erro-
neous token. (3) To ensure a continuous flow of token gen-
eration, LLMCad speculatively generates tokens during the
verification process by implementing a compute-IO pipeline.
Through an extensive series of experiments, LLMCad show-
cases an impressive token generation speed, achieving rates
up to 9.3√ófaster than existing inference engines.
1 INTRODUCTION
Generative tasks like text generation, question answering,
and translation play a crucial role on mobile devices, as
numerous applications rely on them to deliver key function-
alities. For instance, input method application like Google
100M 1B 10B 100B 1T
Parameter size02040Acccuracy (%)
Jetson TX2
(IoT)Xiaomi10
(Smartphone)Jetson Orin
(Autonomous)
LLaMA-Math
GPT3-NLU
GPT3-Mode
GPT3-GM
Memory wall(a) Emergent abilities across various LLMs.
100M 1B 10B 100B 1T
Parameter size02040Latency (s)
Jetson TX2
(IoT)Xiaomi10
(Smartphone)Jetson Orin
(Autonomous)
T5-TX2
LLaMA-Xiaomi 10
LLaMA-Orin
Memory wall
(b) Generation latency of LLMs across various devices.
Figure 1: The memory wall hinders LLM‚Äôs ‚Äúscaling law‚Äù
on mobile devices. *-Math, *-NLU, *-Mode, and *-GM de-
note LLMs‚Äô emergent abilities: math reasoning, multi-
task comprehension, mode arithmetic, and learning
meaningful representations.
GBoard heavily leverages its text generation capabilities,
while private assistant like Apple Siri uses it for question an-
swering. Such tasks are often privacy-sensitive and heavily
rely on users‚Äô private data, thereby necessitating on-device
local inference.
Large language models (LLMs), especially those built atop
transformer decoder [ 65] such as GPT-3 [ 17] and LLaMA [ 64],
have become the de-facto approach to solve NLP genera-
tive tasks. Recent research in the machine learning com-
munity has demonstrated that scaling up such LLMs pa-
rameter size brings accuracy improvement and emergent
ability [ 17,64,71,71,72], as shown in Figure 1(a). In general,
an LLM necessitates more than 1B parameters to learn mean-
ingful representations [ 51], over 10B parameters to exhibit
certain arithmetic reasoning abilities [ 22], and more than 30B
1arXiv:2309.04255v1  [cs.NI]  8 Sep 2023

--- PAGE 2 ---
Conference‚Äô17, July 2017, Washington, DC, USA Daliang Xu‚ô¶, Wangsong Yin‚ô¶, Xin Jin‚ô¶, Ying Zhang‚ô¶, Shiyun Wei‚ãÜ, Mengwei Xu^, Xuanzhe Liu‚ô¶
parameters to achieve multi-task comprehension capabili-
ties [ 30]. This phenomenon is well-recognized in the machine
learning community as the scaling law [11, 12, 21, 34].
Key challenge: memory wall. However, our preliminary
experiments in Figure 1(b) reveal that the scaling ability is
challenged on mobile devices. Specifically, when LLMs are
too large to be fit into device memory, mobile DNN engines
like MNN [ 4]a nd llama.cpp [ 45] need to repetitively release
and load model weights. It results in 59‚Äì224 √ólengthened
inference latency. Such memory wall severely hinders the
scaling law. Users have to choose between real-time gen-
eration and emergent ability. For instance, 10B parameters
represent the minimum size required for LLaMA to possess
arithmetic reasoning capabilities, yet it also represents the
maximum parameter size for achieving real-time inference
on smartphones (e.g., Xiaomi 10).
LLMCad : breaking memory wall through model collab-
oration. In this paper, we propose LLMCad , the first efficient
inference engine for on-device generative NLP tasks. LLMCad
delivers LLM‚Äôs scaling ability to mobile devices with a tol-
erable generation speed through model collaboration . The
main idea is to delegate most tokens to a smaller real-time
LLM that can be totally hosted in device memory (namely
memory-resident LLM). The design is based on a key obser-
vation that, while a smaller LLM is inadequate to deliver sat-
isfactory end-to-end sentences, they can correctly generate
most easy tokens (e.g., determiners, pronouns, and punctu-
ations). Furthermore, LLMs are often trained with a series
of model variants, e.g. T5-Small/Base/Large [ 55] and LLaMa-
7B/13B/33B [ 64], and its smaller counterpart (e.g., LLaMa-7B
and T5-small, dubbed memory-resident model in this paper)
can often be hosted in memory easily [17, 55, 64, 75].
LLMCad employs a unique form of model collaboration,
namely ‚Äúgenerate-then-verify‚Äù [ 20,41]. In this approach, the
memory-resident LLM serves as a token generator, while a
target LLM acts as a verifier, using its output as the ground
truth to inspect and rectify any errors introduced during
the token generation process. This approach provides two
significant advantages: (1) No compromising accuracy. Each
token is verified by the target model, therefore its accuracy is
guaranteed. This is crucial as a wrong token could propagate
its error to the subquent tokens due to the autoregressive
nature. (2) Fast verification. As will be detailed in ¬ß2.3, the
verification of ùëÅtokens can be accomplished within one-
shot inference o f the target model, therefore much faster
than using it to generate ùëÅtokens sequentially.
Despite these advantages, applying model collaboration
for on-device LLM introduces three distinctive challenges:
‚Ä¢Overlooked correct tokens with sub-optimal confidence. Typ-
ically, state-of-the-art LLM engines and studies always
use the token with the highest probability as the output.Nevertheless, our observation has revealed that some of
generation errors by the memory-resident LLM can be
rectified by the sub-optimal tokens. Figure 4 gives a real-
world example of such phenomenon. Given the significant
performance overhead associated with on-device verifi-
cation, LLMCad must capitalize on these often-overlooked
tokens to reduce the frequency of verification.
‚Ä¢Verification timing. Another crucial aspect is determining
when to initiate the verification process. On-device verifi-
cation is time-consuming, e.g., taking 7.1s on Jetson TX2.
Too early or too late verification just wastes computing
mobile devices scarce resources by invalid verification (i.e.,
no errors detected) or useless tokens. Prior works have
typically relied either a single token or token sequence
length, which may not accurately pinpoint the optimal
verification timing.
‚Ä¢IO vs. compute asymmetry. With a LLM cascade, the large
LLM execution blocks the small model inference due to
the cross-token dependency, and the processor is under-
utilized as the I/O bottlenecks during weights loading.
Such a situation severely hampers the inference speed
as the target model needs to be invoked unavoidably to
guarantee correct tokens generation.
In response, LLMCad desgins three novel techniques:
(1) Token tree generation and verification ( ¬ß3.2). Instead
of generating and verifying a linear token sequence, LLMCad
employs a different approach by constructing and validat-
ing a ‚Äútoken tree.‚Äù This token tree permits each token to
have multiple potential succeeding tokens. To accomplish
this efficiently, LLMCad employs three novel modules: (1)
Confidence-based branch pacer paces the progress of
different branches to prevent the wasteful allocation of com-
puting resources to the wrong branch; (2) Tree decoder
generates tokens from various branches without incurring
the overhead of context switching between them; (3) Non
-autoregressive token tree verifier examines and rec-
tifies all errors within a token tree in a batch manner, at the
cost of a single iteration.
(2) Self-adaptive fallback strategy ( ¬ß3.3). This strategy is
devised to initiate the verification process promptly when
the memory-resident LLM generates an incorrect token. It is
inspired by two key observations: (1) Typically, each token
generated by the memory-resident LLM introduces some
‚Äúuncertainty‚Äù (imperfect confidence score). LLMCad uses a
more accurate metric referred to as cumulative uncertainty
within the token tree compared. Compared to prior works,
this metric better reflects the error probability associated
with memory-resident LLM generation, especially consid-
ering the accumulative nature of autoregressive models. (2)
Historical data pertaining to the accuracy of verified tokens
2

--- PAGE 3 ---
LLMCad : Fast and Scalable On-device Large Language Model Inference Conference‚Äô17, July 2017, Washington, DC, USA
is harnessed to assess the memory-resident LLM‚Äôs genera-
tion capability. A stronger generation ability necessitates a
lower frequency of verification.
(3) Speculative generation pipeline ( ¬ß3.4). To break the
cross-token dependency and enhance parallelism, we pro-
pose speculative generation , i.e., continuing generating tokens
through the memory-resident LLM during the verification
process, This is founded on the insight that sometimes the
verification process may not detect errors, rendering the
speculatively generated tokens usable. However, simultane-
ous speculative generation with verification directly can lead
to processor and memory contentions. To further tackle this
issue, LLMCad incorporates a fine-grained pipeline, ensuring
that the speculative generation only runs when loading tar-
get LLM parameters below the memory upper bound to void
interfering with the regular verification process.
Implementation and evaluation. We have fully im-
plemented LLMCad on top of two SOTA LLM engines: Py-
Torch [ 5] and llama.cpp [ 45]. Extensive evaluation of the
system was conducted across four platforms: two IoT de-
vices (Jetson TX2 and Jetson Orin NX) and two smartphones
(Xiaomi 10 and Xiaomi 11). This evaluation encompassed
six widely utilized LLMs (GPT2 [ 54], T5 [ 55], mT5 [ 75],
Bart [ 42], Vicuna, and LLaMa2 [ 64]) and seven datasets (CN-
N/Daily [ 61], Wikitext [ 47], iwlt2017 [ 19], wmt14/22 [ 15],
SQuAD [ 56], parrot, and TruthfulQA [ 44]). We also com-
pared LLMCad with five state-of-the-art competitive base-
lines [ 5,27,37,41,45], encompassing two computing-loading
pipeline frameworks and two ‚Äúgenerator and verifier‚Äù LLM
collaboration frameworks. Our results unequivocally demon-
strate LLMCad ‚Äôs superior performance. When compared to
the state-of-the-art LLM engines, LLMCad can reduce average
per-token generation time by 2.9‚Äì9.3 √óand 3.5‚Äì4.7√óon IoT
devices and smartphones, respectively, without comprising
accuracy. For >10B-sized LLMs like LLaMA2-13B that are
previously unbearable on smartphones, LLMCad generates
more than one token per second. Furthermore, compared
with competitive baselines, LLMCad can achieve up to 5.6 √ó
speedup and noticeable higher accuracy.
The major contributions of this work are as follows:
‚Ä¢We thoroughly explore the opportunities and challenges
of inferring LLMs on the device.
‚Ä¢We propose LLMCad , the first efficient inference engine for
on-device generative NLP tasks. To speedup the genera-
tion procedure, LLMCad uses ‚Äúgenerator and verifier‚Äù LLM
collaboration and incorporates three novel techniques:
tree generation and verification, self-adaptive fallback
strategy, and speculative generation pipeline. These ad-
vancements enable LLMCad to effectively mitigate the mem-
ory wall problem.
Masked Multi Self Attention 
LayerNorm
Feed Forward
LayerNormN      Decoder  LayersInput
Text embeddingPosition embedding
Output(a) GPT3 architecture
Decoder  layer
Decoder  layer
Decoder  layerYoushould
wearwear
shoesshoes
<EOS>iter0 iter1 iter2
...Large language model
Decoder  layer (b) Autoregressive inference
Figure 2: The architecture of decoder-only language
model (GPT3) and the overview of LLM inference pat-
tern: autoregressive.
‚Ä¢We prototype LLMCad and evaluate it with representative
LLMs and commodity mobile devices. The results demon-
strate its superior performance over existing methods.
2 BACKGROUND AND MOTIVATION
2.1 Decoder-based Generative LLMs
On-device generative ML tasks. Generative tasks are those
involving automatic generation or synthesis of new contents
like text sequences and image pixels [ 14,58,59]. Typical
generative tasks in the NLP domain (focus of this work)
include language modeling, machine translation, summa-
rization, and question answering. The key focus is to pro-
vide novel, meaningful, and coherent content output. As
compared to traditional classification tasks such as topic
classification and image classification, generative tasks are
often more challenging but also more profound to human
lives [63].
Generative ML tasks have been widely served for mobile
users, such as language modeling for Google Gboard [ 1],
question answering for Siri [ 6], and translation services like
iTranslate [ 3] and Google Translate [ 2], etc. To guarantee
user data privacy (e.g., text corpus) and service availability,
the models are better to be deployed on devices directly for
local inference.
Decoder-based LLM architecture. Decoder-based large
language model (LLM), including both decoder-only and
encoder-decoder architectures, is the de-facto approach for
generative tasks, such as GPT-3 [ 17], LLaMa [ 64], and GLM-
130B [ 23,79]. As shown in Figure 2(a), a typical decoder-
based LLM consists of a text embedding, a position embed-
ding, and many sequentially stacked decoder layers, where
each decoder layer includes masked self-attention, Layer-
Norm, and Linear operations. For those encoder-decoder
LLMs such as T5 [ 55] and mT5 [ 75], encoder layers are in-
corporated before the decoder to enhance semantic under-
standing capabilities.
3

--- PAGE 4 ---
Conference‚Äô17, July 2017, Washington, DC, USA Daliang Xu‚ô¶, Wangsong Yin‚ô¶, Xin Jin‚ô¶, Ying Zhang‚ô¶, Shiyun Wei‚ãÜ, Mengwei Xu^, Xuanzhe Liu‚ô¶
Autoregressive inference. Generative LLMs employ an au-
toregressive inference procedure that generates one token at
a time and takes that token as input to generate the next one.
For instance, Figure 2(b) illustrates a three-autoregressive-
iteration inference procedure. In the 1st iteration, the model
takes all existing tokens ("You should") as input and gen-
erates the output "wear." In the next iteration, the newly
generated "wear" will be fed into the model, which then
predicts "shoes." This process continues until the model gen-
erates the end-of-sequence token ( <ùê∏ùëÇùëÜ>), indicating the
end of the generation procedure. The nature of autoregres-
sive inference introduces unique challenges for optimizing
on-device LLM as will be described later.
2.2 On-device LLM is Memory-bounded
In this section, we perform pilot experiments to reveal the
performance issue of on-device LLM inference. The experi-
ments are performed on typical LLMs (GPT2, T5, and LLaMa),
datasets (SQuAD and TriviaQA), and mobile devices (Jetson
TX2 and Xiaomi 10) using state-of-the-art DL engines (Py-
Torch [ 5] and llama.cpp [ 45]. We summarize our key findings
below.
Scaling up parameter size brings accuracy improve-
ment. Transformer-based LLM architecture is highly flex-
ible and scalable by simply adjusting the encoder/decoder
layers, sequence length, and other hyper-parameters. Con-
sequently, popular LLM is often developed with a series
of model variants, such as T5-Small/Base/Large [ 55] and
LLaMa-7B/13B/33B/65B [ 64]. With the parameter size scal-
ing up, the model exhibits stronger abilities. As shown in
Table 1, T5-Large outperformed T5-Small by a significant
margin, achieving a 7.6% improvement in accuracy on the
SQuAD dataset. Similarly, LLaMa-13B demonstrated a 6.6%
higher QA accuracy on TriviaQA than LLaMa-7B. Indeed,
such a phenomenon is well known in ML community as
scaling law [11, 12, 21, 34].
On-device LLM scalability hinders on the memory
wall. However, as shown in Table 1, the inference speed
declines rapidly when the memory consumption exceeds
the memory budget. For instance, on the TX2 device, the in-
ference latency increases by 189‚Äì224 √ówith only 5.8‚Äì12.2√ó
increase in model size. To gain a deeper understanding of the
factors influencing inference speed, we conducted a break-
down analysis, as shown in Figure 3. It clearly shows that,
when the model inference demands a memory size unaf-
fordable on edge devices, loading parameters from disk to
memory (i.e., disk I/O) soon becomes the bottleneck of the
inference time (95.9‚Äì98.8%). This situation attributes to the
fact that the state-of-the-art device-side LLM engines, such(a) On TriviaQA with Xiaomi10
# of Params (B) Accuracy PeakMem. (GB) Infer. Time (ms)
LLaMa-7B (4bits) 7 50.0 4.1 275 (1x)
LLaMa-13B (4bits) 13 56.6 9.8 10118 (37x)
LLaMa-33B (4bits) 33 65.1 20.8 22017 (87x)
(b) On SQuAD with TX2
# of Params (B) Accuracy PeakMem. (GB) Infer. Time (ms)
GPT2 0.14 49.8 0.5 37.64 (1x)
GPT2-Large 0.8 53.7 3.1 8065 (214x)
mT5-Small 0.3 76.4 0.7 31 (1x)
mT5-Base 0.58 83.8 1.5 2134 (69x)
mT5-Large 1.2 87.0 3.9 7214 (230x)
T5-Small 0.06 79.1 0.2 27 (1x)
T5-Base 0.22 85.4 0.8 37 (1.4x)
T5-Large 0.73 86.7 2.8 7098 (263x)
Table 1: The parameter size, accuracy, peak memory
usage, and inference latency of LLM variants in one au-
toregressive iteration. (a) experiments are performed
on PyTorch while (b) experiments are performed on
llama.cpp.
100101102
Percentage (%)SmallBaseLargeT5
100.0%100.0%1.2% 98.8%computing load
100101102
Percentage (%)SmallBaseLargemT5100.0%2.7% 97.3%2.3% 97.7%
100101102
Percentage (%)BaseLargeGPT2 100.0%1.7% 98.3%
100101102
Percentage (%)7B13BLLaMa100.0%4.1% 95.9%
Figure 3: Inference delay breakdown of different LLM
variants in one autoregressive iteration.
as MNN [ 4] and llama.cpp [ 45], resort to the swapping tech-
nique which dynamically releases the inferred weights mem-
ory and loads weights to be inferred from the disk when
memory constraints are exceeded.
The autoregressive nature makes traditional mem-
ory optimizations barely effective for generative LLM.
It is worth noting that memory optimization for model infer-
ence has been a well-researched topic in recent years [ 27,32,
46,78]. Various system-level methods have been explored,
such as batching [ 78], compute-I/O pipeline [ 27], and smart
swapping [ 32,46]. However, these works can hardly apply
to on-device LLM, because: (1) Parallelization/batching is
not available as autoregressive inference requires generat-
ing tokens sequentially; (2) Overlapping can gain limited
benefits since I/O time is over a hundredfold larger than
computing. Additionally, algorithmic approaches like quan-
tization [ 25,31,66,76] can bring a few times memory re-
duction (e.g., FP16‚àí ‚ÜíINT4 [ 25]), their efficiency is limited as
low-bit precision (e.g., 2 bits) has been demonstrated to be
insufficient to retain model capability [ 25,36,76]. Note that
4

--- PAGE 5 ---
LLMCad : Fast and Scalable On-device Large Language Model Inference Conference‚Äô17, July 2017, Washington, DC, USA
the proposal of this work is at system level and is compatible
with quantization.
2.3 Opportunities and Challenges of LLM
collaboration
This work focuses on model collaboration approach [ 40,60,
70], which leverages multiple models with accuracy-cost
tradeoff to speed up inference. In the case of generative
NLP tasks, we delegate most computations (i.e., tokens) to
a smaller model that entirely fits into the memory bud-
get. The key rationale is that smaller models can exhibit
close performance to the large one, especially for the easier
data points [ 24,29]. Our empirical experiments have con-
firmed such an assumption: in the iwslt2017 de-en translation
dataset [ 19], mT5-Small correctly generates more than 80%
tokens as mT5-Large does. Figure 4 gives one concrete ex-
ample of translating a sentence by the smaller model and
larger LLM, and the corresponding ground truth. It shows
that most of the tokens (in green) generated by the small
model are correct.
However, employing model collaboration for LLMs faces
one critical challenge: Wrong token delegation could be fatal.
Traditional model cascade either relies on internal [ 60] or
external knowledge [ 40,70] to select a portion of data (often
the easier ones) to be processed by the smaller model. In
such circumstance, the accuracy cannot be guaranteed. For
generative NLP tasks, however, a wrongly generated token
by the small model could propagate the error to the subse-
quent ones and finally cause catastrophic results due to its
autoregressive nature [ 53,55,65]. For example, in Figure 4,
the second "ice" in red is incorrect, resulting in additional
two "ice" generations and wrong translation information
in the subsequent tokens. Note that generative NLP tasks
are often accuracy-sensitive, e.g., translation and Q/A, as a
wrongly generated result could misinform users and result
in unexpected behaviors.
To tackle this issue, LLMCad employs a unique form of
model collaboration, namely ‚Äúgenerate-then-verify‚Äù [ 20,41].
In this approach, the memory-resident LLM serves as a token
generator, while a target LLM acts as a verifier, using its
output as the ground truth to inspect and rectify any errors
introduced during the token generation process. By doing
that, LLMCad can prevent from propagating its error to the
subquent tokens due to the autoregressive nature.and ensure
no compromising accuracy.
3 DESIGN
3.1 Overview
LLMCad is built on two LLMs: a target model that is accurate
but heavy (cannot fit to device memory) like mT5-large; and a
memory-resident model that is less accurate but lightweightlike mT5-small. The design goal of LLMCad is to generate
texts with the speed of memory-resident model without com-
promising the accuracy of target (larger) model.
Simplified workflow and an illustrative example Fig-
ure 5 illustrates the workflow of LLMCad . Figure 6 also pro-
vides an illustrative example based on the case of Figure 4 to
exemplify the workflow. Essentially, LLMCad is a generation
and verification framework using the memory-resident LLM
as a generator, and the target LLM as a verifier.
First, LLMCad feeds the input text to the memory-resident
model and generates a token tree . Atoken tree is the interme-
diate result generated by the memory-resident model (details
intree generation¬ß3.2). Unlike a token sequence where each
token has only a single succeeding token, a token in a to-
ken tree can have multiple candidate succeeding tokens, as
shown in Figure 6 1‚óã. Each of the candidate tokens represents
a candidate token sequence (referred to as a branch ). This is
based on the observation that sometimes the ‚Äúsub-optimal‚Äù
tokens generated by the memory-resident LLM is the true
output of the target LLM, e.g., the alternative token ‚Äúcap‚Äù. In
practice, any candidate token with a confidence higher than
a threshold (e.g., 30%) generates a branch.
Each token generated by the memory-resident LLM in-
troduces some ‚Äúuncertainy‚Äù (imperfect confidence score).
Once such uncertainty accumulates to a level in the output
sentence, the target LLM is used to verify all the generated
branches since last verification, as shown in Figure 6 2‚óã. No-
tably, the verification of ùëÅtokens can be done within one-
shot inference with target LLM, therefore much faster than
using it to generate one token by ùëÅtimes. such verification
process is therefore termed ‚Äúnon-autoregressive‚Äù. Once an
error is detected, LLMCad will rollback the token tree and rec-
tify the it. The details of the verification and rollback strategy
is discussed in¬ß3.2.
The verification process involves target LLM inference,
therefore is I/O-bound as previously shown in ¬ß2.3.LLMCad
further proposes Speculative generation to exploit the under-
utilized hardware resources by generating tokens specu-
latively (¬ß3.4), i.e., continuing generating tokens through
the memory-resident model during verification process, as
shown in Figure 6 3‚óãdashed boxes on the left side of the red
dash line. This approach is based on the insight that some-
times the verification detects no error so the speculatively
generated tokens could be used afterwards. Therefore, it can
effectively hide execution latency under the I/O, e.g., the
second branch in Figure 6 3‚óã.
LLMCad repeat the above generation and verification pro-
cess until it encounters ‚Äú<EOS>‚Äù, the end token.
5

--- PAGE 6 ---
Conference‚Äô17, July 2017, Washington, DC, USA Daliang Xu‚ô¶, Wangsong Yin‚ô¶, Xin Jin‚ô¶, Ying Zhang‚ô¶, Shiyun Wei‚ãÜ, Mengwei Xu^, Xuanzhe Liu‚ô¶
Last year , I showed these two slides to illustrate that the Arctic  ice ice ice ice,  which has shrinked for nearly three million years, is 40 percent
Last year , I showed these two slides to illustrate that the Arctic ice cap, which for about three million years has shrunk by 40 percent.
Last year I showed these two slides to illustrate¬†that the arctic ice cap, which for most of the last three million years¬† has shrunk by 40 percent.Smaller:
Larger:
Label:... that the Arctic  ice cap, which has shrinked for ...2-nd highest confidence token
Figure 4: A translation generation example from mt5-small and mt5-large models as well as its ground truth label.
Green: correct parts of small model generation; Red: error propagation parts of small model generation; Blue: the
sub-optimal token which is the correct answer. Noticeably, on the iwslt2017 de-en translation dataset [ 19], the
mT5-small model correctly generates nearly 69.3% of tokens, while the number for the mT5-large model is 73.1%.
Speculative T oken T reeNon-autoregressive
veriÔ¨ÅcationVeriÔ¨Åcation Results
Tree generationFallback
VeriÔ¨Åed sequence
Tree decoder
ConÔ¨Ådence-based branch
generation controller
Tree generationToken
tree Speculative
execution planning
Hit/Miss?Rollback?
Output<EOS>?
Translation the following de text to en.
Jetzt muss ich meine Schuhe ausziehen,
um √ºberhaupt an Bord zu kommen!Input
Small modelLarge model
Autoregressive
Figure 5: The workflow of LLMCad .
3.2 Token tree generation and verification
This subsection is mainly to discuss how the token tree is
generated and verified in LLMCad .
Token tree generation To generate useful token trees,
LLMCad needs to answer two crucial questions:
‚Ä¢Branches compete for computing resource (e.g., GPU) to
generate subsequent tokens by running memory-resident
model. At a timestamp, which branch shall receive the re-
source to lengthen its token sequence? The decision is crucial
as generating tokens to a wrong branch (as verified by the
target model later) wastes computing resource and delays
the generation of true tokens.
‚Ä¢Generating tokens from different branches requires switch-
ing between branch contexts. How to generate token from
different branches efficiently? The design is crucial as LLMCad
needs to frequently switch between up to tens of branches.
In response, LLMCad incorporates two novel techniques:
‚Ä¢Confidence-based branch pacer. To properly pace the progress
of different branches, LLMCad relies on the fact that branch
with a higher probability is more likely to be the correct re-
sult and should have a longer sequence length. Here, LLMCad
models the probability with the cumulative confidence scores
given by the memory-resident for each token generated. To
control the branch length dynamically, LLMCad uses max-min
fairness [28].
Assuming that there are ùëÅbranches and ùëÄtokens, and
the i-th branch includes ùëáùêµ
ùëñtokens, i-th branch cumulative
confidenceùê∂ùëñis the product of every token‚Äôs confidence.
Thus, the branch length problem can be done by solving thefollowing problem.
ùëì(ùë•)=ùëÄ‚àóùê∂ùë•√çùëÅ
ùëñ=0ùê∂ùëñ‚àíùëáùêµ
ùë• (1)
ùëÇùëèùëó=ùëöùëñùëõùëÅ
ùë•=0ùëì(ùë•) (2)
Under the max-min fairness, LLMCad tries to allocate more
hardware resources to the branch which is more likely to be
the ground truth.
‚Ä¢Tree decoder. We commence our study by conducting an ex-
haustive analysis of the fundamental reasons behind the sub-
stantial performance overhead incurred by branch context
switching (e.g., 25% overhead for mT5 models on Jetson TX2).
In Figure 7(a), we provide an illustration of the implemen-
tation of branch context switching within state-of-the-art
LLM engines, such as PyTorch, using the scenario depicted
in Figure 6 1‚óãas a case study. In this illustration, iterations
1‚Äì4 take the previous output token as the new input. How-
ever, generating token ‚Äú7‚Äù in iteration 5 necessitates branch
switch from b1 to b2, which involves the removal of token
ùëá4, the omission of the new token ùëá6, and the utilization of
the sub-optimal output ùëá5from iteration 3 as input. Conse-
quently, LLMCad must deviate from the autoregressive rule
and modify each iteration input with a substantial amount
of metadata (e.g., Key-Value cache [ 7,36,78] and position
ids [65]) maintaining operations and CPU-GPU interactions.
To tackle this issue, LLMCad incorporates masking tech-
nique [ 65]. This technique is employed to ensure that the
predictions for position ùëñdepends only on the known outputs
at positions less than ùëñin decoder-based LLMs. The masking
technique relies on a table where all the positions with value
of one will be taken into account for calculation.
Crucially, the tree decoder retains the autoregressive pro-
cedure while only modifying its masking table to support the
isolation of effects from different branches, as demonstrated
in Figure 7(b). During each iteration, LLMCad treats the new
generated token as input, just as in regular generation. How-
ever, it assigns a value of one only to the previous positions
on the same branch. For example, when generating token ‚Äú7‚Äù
for branch b2 in iteration 5, the input remains ùëá6, the output
of iteration 4, but only the positions ‚Äú1, 2, 3, and 5‚Äù are set to
one; all other positions are set to zero. This approach ensures
6

--- PAGE 7 ---
LLMCad : Fast and Scalable On-device Large Language Model Inference Conference‚Äô17, July 2017, Washington, DC, USA
‚ë† Token Tree
T1: the T2: ArcticT4: ice
T5: capT3: iceice
capice
, which has
cap , which for shrinked‚ë° Non-autor egressiveV erification
Need r ollbackwhich forabout three millon years
three millon years
Verification
for threeReuse‚ë¢ Speculative
generationT6: ice
T7: ,iceb1
b2
Ground T ruth:   the Arctic ice cap, which for  aboutConfidence scor e = 0.6
Confidence scor e = 0.3
Origin outputsVerified outputs
cap ,which has iceOne iteration
All verification outputsTarget LLM
iceInput token tr ee
Verified
outputs fallbackMemory-resident LLM
Target
LLM
Figure 6: An illustrative example of LLMCad .
Input tokens Outputs
T 1 T 2
T1T2 T3
T1T2T3T4T5
T1T2T3T4T6
T1T2T3T5T7Iter1:
Iter2:
Iter3:
Iter4:
Iter5:autoregressive
branch context
switching
(a) Existing tree generation procedure
Input tokens
T 1 T 2
T1T2 T3
T1T2T3T 4 T 5
T1T2T3T4T6
T1T2T3T5T7Iter1:
Iter2:
Iter3:
Iter4:
Iter5:T5
T6T4autoregressive
autoregressive
... ...T 1
T1
T1T2
T1T2T3
T1T2T3T5T2
T3
T41 0 0 0 0 0
1 1 0 0 0 0
1 1 1 0 0 0
1 1 1 1 0 0
1 1 1 0 1 0Masking table Tokens in calculation Outputs (b) Tree decoder procedure
Figure 7: Examples of tree generation procedure in state-of-the-art LLM engines and the tree decoder procedure of
LLMCad based on the case of Figure 6 1‚óã.
T1T2
T5
T3T4
T6Token Tree
T8T7
T1 T2 T5
VeriÔ¨Åed sequenceT'
7T1T2
T3T4
T6T8T7 T1T2T5
T1Branch sequences
T 1 T 2
T 3T 5
T6T 1 T 2
T 1T 1 T 2
T 5 T 1 T 2
T'7T1T2T5
T1T2
T3T 8 T1T6Branch1
results
Branch2
results
Branch3
resultsT ar get
LLMmini-batch
T1T5
T2 T5 T'7T2T 2
T 4
T7T 2
T 5
T 3
T6
T 8VeriÔ¨Åed Outputs Origin Outputs
Correct token treeT2‚ë† Batched non-autoreguressive veriÔ¨Åcation
‚ë° Depth-Ô¨Årst veriÔ¨Åed sequence searchingbatch
size=3T1T5T2
T1T2T'7T1T2T5Branch1 correct sequence
Branch2 correct sequence
Branch3 correct sequence
Figure 8: The illustration of token tree verification.
that tokens ‚Äú4 and 6‚Äù do not affect the calculation, enabling
LLMCad to generate token ‚Äú7‚Äù without being influenced.
Token tree verification and rectification To achieve the
goal of not sacrificing accuracy, LLMCad has to verify every
token generated by the memory-resident LLM. An intuitive
approach is to use the target LLM to run verification after
each token generation by the memory-resident LLM. How-
ever, such an approach (called autoregressive verification (AV) )
is even slower than generating every token by the target
model directly, since AV does not reduce the target LLM
inference times but even uses memory-resident LLM.
To tackle this issue, LLMCad is based on two opportuni-
ties: (1) The target LLM can examine a sequence of tokens
2.5 5.0 7.5 10.0
mT5-Large on TX2255075Latency (s)
9.9xnon-autoregressive verification autoregressive verification
2.5 5.0 7.5 10.0
LLaMa-13B on Xiaomi 1050100
8.5xFigure 9: The latency of verification with input se-
quencing increasing.
in parallel by visiting its parameters only once (called non-
autoregressive verification (NAV) ) and the verification results
are the same as examining them sequentially [ 20,41]. (2)
The NAV process is way faster than AV. Our pilot exper-
iments in Figure 9 on LLaMa-13B and mT5-Large models
using Xiaomi10 and TX2 shows that NAV outperforms AV in
7

--- PAGE 8 ---
Conference‚Äô17, July 2017, Washington, DC, USA Daliang Xu‚ô¶, Wangsong Yin‚ô¶, Xin Jin‚ô¶, Ying Zhang‚ô¶, Shiyun Wei‚ãÜ, Mengwei Xu^, Xuanzhe Liu‚ô¶
examining time across 2‚Äì10 input tokens, with its benefits
more pronounced as token count increases. NAV signifi-
cantly reduces verification time for mT5-Large and LLaMa-
13B models by 8.5‚Äì9.9 √óat 10 tokens, attributed to NAV‚Äôs
single weight swapping versus AV‚Äôs multiple swappings per
token, reducing I/O overhead.
To sum up, NAV can parallel verifying multiple tokens cor-
rectly at the cost of only one target LLM inference. LLMCad
incorporates NAV and extends it to support token tree veri-
fication, as shown in Figure 8, including two crucial steps:
‚Ä¢Batched non-autoregressive verification. As shown in Fig-
ure 8 1‚óã, to support token tree verification, LLMCad first
divides the token tree into several branch sequences and
combines them to a mini-batch as the input of the target
LLM. After NAV, LLMCad can obtain the correct results of
every position in each branch sequence. Compared with
the origin branch sequences, LLMCad can detect all errors
of a token tree, e.g., ùëá‚Ä≤
7in branch2 results. A branch correct
sequence is the sub-sequence leading up to the first error
position, plus the rectified token, to avoid error propaga-
tion. For example, branch1 correct sequence stops at the
ùëá2, plus theùëá5, i.e., tokens ‚Äú1, 2 and 5‚Äù. To be noted, given
that the NAV is I/O-bounded, increasing batch size (e.g.,
<10) has a negligible effects on verification time.
‚Ä¢Depth-first verified sequence searching. Based on the correct
sequences, LLMCad can build a correct token tree, as shown
in Figure 8 2‚óã. Its leaf node is either first rectified token or
the origin branch sequence last token. LLMCad leverages
depth-first searching algorithm to find the longest correct
path in the correct token tree as the verified sequence .
If the verified sequence has a rectified token, e.g., ùëá‚Ä≤
7,
LLMCad will rollback the token tree to the error position, fix
the error, and use it as the new input for future generation.
3.3 Self-adaptive fallback strategy
This strategy is devised to initiate the verification process
promptly when the memory-resident LLM generates an in-
correct token. To achieve this goal, LLMCad needs to answer
two crucial questions:
‚Ä¢Selection of Decision Metric. The decision metric should
effectively evaluate the probability of errors within the
token tree.
‚Ä¢Threshold Values for Different Tasks. Recognizing
that a universal threshold may not be suitable for all tasks,
LLMCad must establish appropriate threshold values tai-
lored to specific tasks.
To tackle these issues, LLMCad introduces two innovative
techniques:
‚Ä¢Tree-cumulative confidence ( ùëáùëê).We propose using tree-
cumulative confidence as the decision variable for initiating
fallback. Unlike prior studies [ 37,41] that rely on a singletoken confidence or token sequence length, ùëáùëêprovides a
comprehensive assessment of global uncertainty. It captures
errors more accurately due to the autoregressive nature of
token generation.
The formulation tree-cumulative confidence is asùëáùëê=
ùëöùëéùë•ùëÅùëê
ùëñ=1ùê∂ùëñ, whereùëÅùëêrepresents the number of branches in a
token tree, and ùê∂ùëñdenotes the cumulative confidence of the
ùëñ-th branch. We select the maximum cumulative confidence
over minimum/average confidence because the most confi-
dent branch is more likely to yield the correct result after
verification, and the verification process can only identity
errors when the most confident branch is wrong.
‚Ä¢self-adaptive threshold ( ùõº)is utilized to determine when
target LLM shall verify. It operates on the principle that the
memory-resident LLM, which generates outputs closely re-
sembling those of the target LLM, should be trusted more, i.e.,
a lower verification frequency by setting a lower threshold.
To assess the outputs similarity, LLMCad relies on historical
data regarding the accuracy of verified tokens.
Users can either select an initial ùõºvalue or utilize the
default value (0.01) provided by the system. After verifica-
tion, LLMCad updates the self-adaptive threshold ( ùõº)using the
following rule:
ùõºùëñ+1=(ùõºùëñ‚àó0.5ùëñùëì ùëÅùëêùëúùëüùëüùëíùëêùë° ==ùëÅùëéùëôùëô
ùõºùëñ/ùëáùëÅùëéùëôùëô‚àíùëÅùëêùëúùëüùëüùëíùëêùë°
ùëÅùëéùëôùëôùëê ùëñùëì ùëÅùëêùëúùëüùëüùëíùëêùë° <ùëÅùëéùëôùëô(3)
whereùëÅùëêùëúùëüùëüùëíùëêùë° andùëÅùëéùëôùëôare the number of total tokens and
correct tokens in the most matching branch during one veri-
fication. Specifically, when the verification process detects
no error, LLMCad lowersùõºby multiplying the current value
by 0.5, the cumulative confidence of 3-5 tokens in empirical
observations. In contrast, if verification identifies errors, the
threshold is increased by dividing ùõºby the average cumula-
tive confidence of all tokens subsequent to the incorrectly
generated one. The rationale behind the use of an exponential
function is that the tree-cumulative confidence is the product
of every token‚Äôs confidence, accumulating exponentially.
In summary, after each token generation by the memory-
resident LLM, LLMCad calculatesùëáùëê. Ifùëáùëêfalls below ùõº, a
fallback occurs, and the target model begins verification.
After verification, ùõºis updated based on the latest generation
accuracy history.
3.4 Speculative Generation Pipeline
As elaborated in¬ß2.3, the GPU utilization undergoes cycli-
cal upswings attributed to the fact that SOTA LLM engines
resort to the swapping technique. To harvest the free cycles,
LLMCad proposes speculative generation technique by allow-
ing the memory-resident LLM to continue generating tokens
during verification process. This approach is based on the
8

--- PAGE 9 ---
LLMCad : Fast and Scalable On-device Large Language Model Inference Conference‚Äô17, July 2017, Washington, DC, USA
mT5-Large GPT2-Large
Loading024Latency (s)Alone
Parallel
mT5-Large GPT2-Large
Computing0.00.1Alone
Parallel
Figure 10: The loading and computing time of the tar-
get model execution with the memory-resident model
parallelization.
Targe LLM
Memory-
resident LLMC C C CC L UL
SC SC ...SCC
FallbackMemory upper
bound¬†
Speculative VeriÔ¨Åcation ends¬†L ...
SC SC SCC
C ...LMemory upper
bound¬†
SpeculativeMemory upper
bound¬†
UL UL
... ...
UL L C SC ComputingSpeculative
computingParameters loading below
the¬† memory upper boundParameters loading over
the memory upper bound
Figure 11: LLMCad ‚Äôs speculative generation.
insight that sometimes the verification detects no error so
the speculatively generated tokens could be used afterwards.
The impacts of speculative generation on target LLM.
Our preliminary experiments on TX2 using mT5-Large and
GPT2-Large models shows that paralleling the memory-
resident and target LLM execution increases the the tar-
get LLM computing and loading time by 2.2‚Äì2.3 √óand 1.05‚Äì
1.09√ó, respectively. The computing delay is attributed to the
GPU cores contention, while the loading delay is unexpected
which is because of the memory contention. Specifically, the
memory-resident LLM continually allocates memory regions
for speculatively generated tokens, while the target LLM dy-
namically allocates memory regions for loading parameters.
Typically, negligible effects are exerted on each other unless
the memory usage is over 90%. However, it is common for
memory usage to exceed 90% or even reach 95% in the specu-
lative generation scenario, attributed to the fact that existing
state-of-the-art LLMs engines are designed for loading as
many parameters as possible from disk to the memory to
reduce inference time.
Computing-loading pipeline. To tackle the above issue,
LLMCad delicately plans the parallel execution, as shown in
Figure 11. The main principle is that normal verification
process cannot be influenced by the speculative execution.
Thus, there is a memory upper bound by profiling or user
defining to avoid two LLMs memory contention, and the two
LLMs computing cannot be executed in parallel.
After feeding the input sequence, the parameters of both
the memory-resident and the target LLMs are loaded to the
memory. Once the memory-resident LLM loading finishes,
it begins to generate tokens, and the target LLM parameters
loading (in yellow) will stop before the memory upper bound
is exceeded to avoid influencing normal memory-resident
LLM generation. When the fallback condition is met, thePlatform Processor Software Mem.
Jetson TX24x Cortex-A57
Maxwell 128 CUDA coresTorch-1.10
Ubuntu 18.048G
Jetson
Orin NXAmpere 1024 CUDA cores
+ 32 Tensor CoresTorch-2.0
Ubuntu 18.048G
Xiaomi 101x 2.84GHz A77+3x 2.4GHz Cortex A77
+4x 1.8GHz Cortex A55Android 10
llama.cpp8G
Xiaomi
111x 3.0 GHz X2+ 3x 2.5 GHz Cortex A710
+ 1.8GHz 4x Cortex-A510Android 10
llama.cpp8G
Table 2: Platforms used in the experiments.
Devices TasksMemory-
resident LLMsTarget
LLMsSpeed
GapDatasets
Jetson TX2
Jetson
Orin NXTmT5-small (0.3B) mT5-Large (1.2B) 230x IWLST17-de-en [19]
Bart-base Bart-Large WMT14-de-en [15]
QAmT5-small (0.3B) mT5-Large (1.2B) 230x SQuAD_v2 [56]
T5-small (0.06B) T5-large (0.73B) 263x SQuAD_v2
LM GPT2 (0.14B) GPT2-Large (0.8) 214x Wikitext [47]
S T5-small (0.06B) T5-large (0.73B) 263x CNN/Daily [61]
Xiaomi 10
Xiaomi
ProTVicuna-7B
(INT4)Vicuna-13B
(INT4)59xParrot
WMT22-de-en
WMT22-zh-en
QA LLaMa2-Chat-
7B (INT4)LLaMa2-Chat-
13B (INT4)59xSQuAD
TruthfulQA [44]
S CNN/Daily
T, S, QA, and LM represent the generative tasks of translation, summary,
question-answering, and language modeling.
Table 3: Tasks, models, datasets, and their correspond-
ing tested devices used in the experiments.
rest of parameters for the target model (in orange) will be
loaded into the memory, and then the computing of the
target LLM begins. The speculative execution (in blue) will
not run unless the verification process is loading parameters
below the memory budget (in yellow), avoiding processors
and memory contention.
4 EVALUATION
4.1 Implementation and Setups
We have fully implemented LLMCad with 4.5k SLoC (Python:
3,500 and C/C++: 1,000). The prototype is a standalone frame-
work supporting LLMs exported from TensorFlow [ 8] and
PyTorch [ 5].LLMCad leverages llama.cpp [ 45] (one of the
most lightweight on-device LLM engine) as the smartphone
backend and PyTorch [5] as the IoT device backend.
Hardware setup. We test the performance of LLMCad on
four devices: 2 smartphones (Xiaomi 10 and Xiaomi 12) and
2 IoT devices (Jetson TX2, and Jetson Orin), as summarized
in Table 2. We run LLMs on Jetson GPUs and smartphone
CPUs, since the existing LLM engines [refs] have unmature
support for smartphone GPU/NPU. Nevertheless, LLMCad ‚Äôs
design is orthogonal to hardware types.
Models and datasets. We test with a range of typical LLM
models with various generative tasks datasets across differ-
ent devices, as summarized in Table 3. On the IoT devices,
we evalaute LLMCad on two translation tasks, two question
answering tasks, one language modeling task, and one sum-
marization tasks with mT5, T5, GPT2, and Bart models. All
the models are fine-tuned by ourselves as [ 37] does. For
smartphone devices, we use Vicuna-1.5 and LLaMA2 models
9

--- PAGE 10 ---
Conference‚Äô17, July 2017, Washington, DC, USA Daliang Xu‚ô¶, Wangsong Yin‚ô¶, Xin Jin‚ô¶, Ying Zhang‚ô¶, Shiyun Wei‚ãÜ, Mengwei Xu^, Xuanzhe Liu‚ô¶
with three translation tasks, one question answering task
and , and one summarization. All the models are downloaded
from hugging face repository [ 9] and have been quantized
by AutoGPTQ [ 10] into 4-bit format for saving memory and
improving inference speed.
Baselines. We mainly compare LLMCad with 5 state-of-the-
art baselines which can be divided into two categories:
‚Ä¢3xSingle-LLM baselines. (1)Standard (Std) always uti-
lizes the target LLM to generate outputs, with PyTorch for
IoT and llama.cpp for smartphones. (2) Standard pipeline
(StdPL) : It executes a layer-wise pipeline, overlapping I/O
and computation, as used by existing SOTA LLM inference
engines. (3) Speedy Transformer Inference (STI) [27] :
An edge NLP inference framework with quantization param-
eter shards and fine-grained computing-loading pipeline.
‚Ä¢2xLLM collaboration baselines. (1)Speculative Decoding
(SP) [37]: A state-of-the-art framework also uses ‚Äúgenerator
and verifier‚Äù LLM collaboration. (2) Big Little Transformer
Decoder (BLD) [37]: An algorithm of determining verifi-
cation timing and rollback mechanism for ‚Äúgenerator and
verifier‚Äù LLM collaboration.
Metrics and configurations. We mainly report genera-
tion accuracy and the per-token generation time. For clarity,
LLMCad ‚Äôs goal is to align the memory-resident LLM outputs
to the target LLM. Thus, we regard the text produced by the
target LLM as the ground truth and calculate the Rouge-L
score [ 43], a similarity between two sequences based on the
longest common subsequence, as the generation accuracy .
4.2 Generation Speed
Overall performance. We first comprehensively investi-
gate the generation performance of LLMCad on four tested
devices. The generation accuracy and per-token generation
time results are illustrated in Table 4, Figure 12 and Figure 13,
respectively. Our key observation is that LLMCad consis-
tently and remarkably outperforms other baselines on
per-token generation time without comprising accu-
racy across all tested devices.
‚Ä¢Generation time of LLMCad v.s. Single-LLM baselines.
Compared with Std, LLMCad achieves a 2.9-9.3√óand 3.47‚Äì
4.67√óspeedup in per-token average generation time on IoT
and smartphone devices, respectively, without compromis-
ing accuracy. Specifically, LLMCad can generating question-
answering outcomes on Xiaomi 11 at a fastest speed of 0.86
s/token. This achievement enables real-time token genera-
tion with over-10B LLM on COTS device for the first time.
This is attributed to the fact that LLMCad can delegate most of
token generations to the memory-resident LLM and ensure
correctness by non-autoregressive verification.
When compared with more competitive baselines like
StdPL and STI, LLMCad reduce per-token average generationtime 2.9-9.3√óand 1.83‚Äì2.45√ó, respectively. Those benefits are
attributed to the fact employing memory-resident LLMs for
text generation consistently outpaces any pipeline or quan-
tization approaches of target LLMs on the mobile device,
where memory-resident LLM can yield a over hundredfold
speed improvement compared to the target LLM. Besides,
it can also improve generation accuracy by 11.1‚Äì19.0 per-
centage point, compared to STI. This benefits from our tree
non-autoregressive verification which can examine and cor-
rect all errors by the memory-resident LLM efficiently.
‚Ä¢Generation time of LLMCad v.s. LLM collaboration base-
lines. Compared with BLD, LLMCad can achieves a 4.5‚Äì94.5
and 9.8‚Äì96.7 percentage point generation accuracy improve-
ment with a 1.1-1.4 √óand 1.1‚Äì1.3√óspeedup in per-token
average generation time on IoT and smartphone devices,
respectively. That is because, unlike BLD which speeds up
the generation process by reducing the number of correc-
tion (sacrificing accuracy), our self-adaptive fallback strategy
aims to minimize verification times while ensuring verifica-
tion for each token. Such an approach enhances generation
speed without sacrificing accuracy. Furthermore, specula-
tive execution enables memory-resident LLM to generate
texts earlier without waiting the verification results when
no errors are detected by the verification process, further
reducing generation latency.
Similarly, LLMCad can reduce per-token average genera-
tion time by 1.93‚Äì2.00 √óand 1.34‚Äì1.77√óon IoT and smart-
phone devices, respectively. That is because, unlike SP uses
token sequence length, our self-adaptive fallback strategy
can accurate finds when the memory-resident LLM generate
errors and can reduce verification frequency.
4.3 Memory Sensitivity Analysis
This subsection is to investigate the impact of different mem-
ory budgets on our approach. We further conduct experi-
ments on mT5 and T5 models on TX2 and LLaMa2 on Xiaomi
10 respectively under different memory budgets (e.g., from
4GB to 8GB on Xiaomi 10). The speedup of different baselines
are shown in Figure 14. LLMCad consistently exhibits the
highest speedup among all baselines from 8GB to 4GB
and its benefits are more prominent with the decreas-
ing memory budget.
LLMCad reduces generation time under 6GB memory bud-
get on Jetson TX2 by 5.54 √ó, 1.76√óand 1.12√óon average
forStdPL ,SPandBLD, respectively; while the speedups for
4GB memory budget are 6.12 √ó, 1.91√óand 1.25√ó, correspond-
ingly, which are 1.29 √ó, 1.08√óand 2.1√ólarger than that under
6GB on average. Similarly, LLMCad achieve a generation time
speedup under 4GB memory budget on xiaomi 10 by 4.72 √ó
and 1.34√óon average for StdPL andBLD, respectively. That
is because when the memory budget is stricter, the inference
10

--- PAGE 11 ---
LLMCad : Fast and Scalable On-device Large Language Model Inference Conference‚Äô17, July 2017, Washington, DC, USA
Models Datasets StdPL SP BLD Ours
mT5-LargeT: IWLST17-de-en 100 100 96.1 100
QA: SQuAD 100 100 52.9 100
T5-LargeS: CNN/Daily 100 100 5.5 100
QA: SQuAD 100 100 51.4 100
Bart-Large T: WMT14-de-en 100 100 96.5 100
GPT2-Large LM: Wikitext 100 100 12.9 100
(a) IoT devicesModels Datasets StdPL STI SP BLD Ours
Vicuna-13B
(INT4)T: Parrot 100 86.7 100 89.7 100
T: WMT22-de-en 100 87.2 100 90.2 100
T: WMT22-zh-en 100 88.1 100 80.4 100
LLaMa2-Chat
(INT4)S: CNN/Daily 100 81.0 100 7.96 100
QA: SQuAD 100 83.2 100 3.4 100
QA: Truthful_QA 100 85.4 100 4.7 100
(b) Smartphones
Table 4: The summary of the generation accuracy of LLMCad and the baseline on tested devices. T:*, S:*, QA:*, and
LM:* represent the generative tasks of translation, summary, question-answering, and language modeling.
mt5_iwlst17_de_en  0  2  4  6Latency on Orin (s)7.20 7.06
2.31
1.70
1.26Vanilla StdPL SP BLD Ours
mt5_squad_v2  0  2  4  67.20 7.06
1.80
1.050.78
t5_squad_v2  0  2  4  67.10 7.03
1.82
1.050.83
t5_CNN_Daily  0  2  4  67.10 7.03
2.74
2.01
1.38
bart_wmt14_de_en  0  2  4  66.10 6.10
2.031.82
1.34
gpt2_wikitext  0  2  4  6  88.007.69
5.335.06
2.70
mt5_iwlst17_de_en  0  2  4  6Latency on TX2(s)7.20 7.06
2.34
1.70
1.26
mt5_squad_v2  0  2  4  67.20 7.06
1.84
1.130.90
t5_squad_v2  0  2  4  67.10 7.03
1.85
1.100.95
t5_CNN_Daily  0  2  4  67.10 7.03
2.77
2.05
1.49
bart_wmt14_de_en  0  2  4  66.10 6.10
2.081.85
1.41
gpt2_wikitext  0  2  4  6  88.007.69
5.375.13
2.78
Figure 12: Average per-token generation latency of LLMCad and the baselines under different tasks on IoT devices.
Vicuna_Parrot  0  5 10 15Latency on Xiaomi 11 (s)15.214.9
8.3
6.0
3.73.3Vanilla StdPL STI SP BLD Ours
Vicuna_wmt22_de_en  0  5 10 1515.214.9
8.3
6.2
3.83.6
Vicuna_wmt22_zh_en  0  5 10 1515.215.0
8.3
6.4
4.5
3.4
LLaMa2_squad  0  5 10 1515.214.9
10.6
5.5
4.54.1
LLaMa2_CNN_daily  0  5 10 1515.214.9
7.8
5.04.33.6
LLaMa2_TruthfulQA  0  5 10 1515.214.9
10.6
6.1
4.3
3.3
Vicuna_Parrot  0  5 10 15Latency on Xiaomi 10 (s)16.215.9
8.7
6.4
3.93.6
Vicuna_wmt22_de_en  0  5 10 1516.215.9
8.7
6.7
4.24.0
Vicuna_wmt22_zh_en  0  5 10 1516.216.0
8.7
6.8
4.9
3.7
LLaMa2_squad  0  5 10 1516.215.9
11.3
5.9
4.84.4
LLaMa2_CNN_daily  0  5 10 1516.215.9
8.1
5.54.73.8
LLaMa2_TruthfulQA  0  5 10 1516.215.9
11.3
6.6
4.7
3.8
Figure 13: Average per-token inference latency of LLMCad and the baselines under different tasks on smartphones.
Models-tasks-datasets Vanilla StdPL SP BLD Ours
mT5-translation
IWLST17-DE->EN36.9 36.2 12.0 7.7 7.7 (4.8√ó)
T5-summary
CNN/Daily36.4 36.0 7.6 10.3 8.4 (4.3√ó)
T5-QA
SQuAD36.9 36.5 15.4 9.9 4.6 (8.0√ó)
(a) Jetson Orin NXModels-tasks-datasets Vanilla StdPL STI SP BLD Ours
LLaMa2-summarization
CNN/Daily mail56.2 55.1 27.9 21.5 18.6 17.3 (3.2√ó)
LLaMa2-QA
TruthfulQA56.2 55.1 28.1 23.9 18.3 14.3 (3.9√ó)
Vicuna-translation
WMT22-DE-EN56.2 55.1 20.7 20.4 20.3 15.5 (3.6√ó)
(b) Xiaomi 11
Table 5: The summary of the energy consumption (J) of different models across different devices.
11

--- PAGE 12 ---
Conference‚Äô17, July 2017, Washington, DC, USA Daliang Xu‚ô¶, Wangsong Yin‚ô¶, Xin Jin‚ô¶, Ying Zhang‚ô¶, Shiyun Wei‚ãÜ, Mengwei Xu^, Xuanzhe Liu‚ô¶
4.85.05.25.45.6
t5_CNN_Daily12345Speedup on TX2 (x)
4.04.3 4.3 4.44.5
3.3 3.4 3.4 3.43.4
1.0 1.0 1.0 1.01.02.5 2.5 2.5 2.52.5Ours BLD StdPL SP
4.85.05.25.45.6
t5_squad_v202468
7.17.8 7.8 7.98.0
6.46.6 6.6 6.76.7
1.0 1.0 1.0 1.01.03.7 3.8 3.8 3.83.8
4 5 6 7 8
LLaMa2_CNN_daily246Speedup on Xiaomi10 (x)
4.8 4.9 4.95.05.5
3.6 3.6 3.6 3.7 3.8
3.1 3.1 3.1 3.2 3.3
2.1 2.2
1.1 1.0 1.1Ours BLD StdPL SP STI
4 5 6 7 8
LLaMa2_TruthfulQA24
4.7 4.7 4.7 4.85.1
3.7 3.7 3.7 3.7 3.8
2.8 2.8 2.8 2.9 3.0
1.8 1.8
0.9 1.0 1.1
Figure 14: The speedup of different baselines under
different memory budgets.
mT5_iwlst17_de_en  0  2  4  6Latency (s)7.20
1.67 1.561.29Std Std+TGV Std+TGV+SPP Std+TGV+SPP+SF
T5_CNN_Daily  0  2  4  67.10
1.871.69 1.65
LLaMA2-SQuAD  02.5  57.5 1010.20
3.92
3.132.74
Figure 15: Ablation study of LLMCad .
speed gap between the memory-resident and target LLM
is more significant, and delegating tokens to the memory-
resident LLM can gain more benefits.
4.4 Energy Consumption Analysis
We then evaluate the energy consumption of LLMCad with
mT5 and T5 models on IoT devices and Vicuana and LLaMa2
model on smartphones. As shown in Table 5, compared with
Std, StdPL, SP and BLD, LLMCad reduces per-token energy
consumption by 4.35‚Äì7.96, 4.34‚Äì7.92, 1.56‚Äì3.33, and 1.05‚Äì
2.15√óon Jetson Orin NX, LLMCad achieves a energy con-
sumption reduction by 3.22‚Äì3.59, 3.18‚Äì3.56, 1.24‚Äì1.66, 1.07‚Äì
1.31 and 2.01‚Äì2.56, √ócorrespondingly on Xiaomi 11, plus
STI. This is because LLMCad ‚Äôs two techniques can delegate
as many tokens as possible to memory-resident LLM while
not sacrificing accuracy.
Compared with the latency speedup, LLMCad ‚Äôs energy con-
sumption is relatively lower. This situation arises because
our speculative generation parallels the memory-resident
and target LLM execution together, resulting in more energy
consumption.4.5 Ablation Study
Overall techniques. We further conduct a breakdown anal-
ysis of the benefit brought by each of LLMCad ‚Äôs techniques.
The experiments are performed with the mT5 and T5 models
on TX2 and LLaMa2 on Xiaomi 10. The results are illus-
trated in Figure 15. The leftmost bar is the same as baseline
Vanilla , while the leftmost one is LLMCad . The three crucial
technique token tree generation and verification in¬ß3.2,self-
adaptive fallback strategy ¬ß3.3 and the speculate generation in
¬ß3.4 are represented by TGV, SF and SPP correspondingly.
We observe that all techniques make a non-trivial con-
tribution to the improvement. First, tree non-autoregressive
generationa and verification can delegete most token gener-
ations to the memory-resident LLM, leading to an 2.6‚Äì4.3
speedup for mT5, T5 and LLaMa2 models, respectively. The
more benefits for mT5 model on IWLST translation dataset
are because the mT5-Small model can generate more correct
tokens than other two models so that more tokens can be
delegated to the memory-resident LLM. Besides, speculative
execution can reduce per-token generation time by up to
1.51√ó. That is because LLMCad can directly use the specula-
tive results when the verification detects no error, especially
true for LLaMA2 model. Lastly, Self-adaptive fallback strategy
achieves a 1.08‚Äì1.20 √óspeedup. This is achieved by leverag-
ing tree-cumulative confidence to assess error probabilities
and dynamically adjust verification timings in response to
variations in task complexity.
5 RELATED WORK
Model collaboration is a common optimization technique
utilized to reduce inference latency [ 40,70]. Its key idea is
to delegate most of the workloads to lightweight models to
reduce inference latency while maintaining relatively high
accuracy. Tabi [ 70] is a multi-level inference engine that
serves queries using various small models according to the
queries difficulties. MobiSR [40] and NestDNN [24] employ
the similar idea but depending on either resolutions or avail-
able resources. Other ‚Äúearly exit‚Äù works [ 60,62,73,81], which
propose adaptive exit timing relying on input data difficulties
can also be regarded as a collaboration. However, they either
focus on CNN/encoder-based model architectures or must
modify and retrain the model, hardly fitting into on-device
LLM inference. The most closely related works are specu-
lative decoding [20,37,41,48], which also employs smaller
LLMs for text generation and larger LLMs for text verifica-
tion. LLMCad is motivated by these works and is the first infer-
ence engine for on-device generative NLP tasks, considering
mobile devices unique challenges like the memory-bound
situation.
Mobile ML optimization. Machine learning optimization
approaches, such as model compression [25,26,33,50,52,68,
12

--- PAGE 13 ---
LLMCad : Fast and Scalable On-device Large Language Model Inference Conference‚Äô17, July 2017, Washington, DC, USA
76,77], reducing the model size by quantization and knowl-
edge distillation, caching [69,74,80], reducing computation
by reusing existing results, and token pruning [16,18,38,
57,66], reducing computation by pruning useless tokens,
have been extensively researched to reduce the generation
latency. LLMCad is orthogonal to and compatible with those
algorithm-level optimizations.
Besides, some of researchers focus on generate text in a
non-autoregressive manner [ 35,39]. However, these works
can only apply to <1B models and have accuracy degradation
problems, not the mainstream research direction.
Pipeline optimization for ML. Pipeline optimization has
been extensively used to accelerate ML [ 13,27,49,67,82].
Most of them, such as PipeDream [ 49], are utilized to scale
out ML to multiple machines by pipelining forward/back-
ward computation with activation/gradients synchronization
to minimize bubbles of I/O and network communication. Still,
there are some studies focusing on single machine/task opti-
mization. For instance, PipeSwitch [ 13] introduce pipelining
model transmission over the PCIe and task execution in the
GPU to reduce context switching performance overhead;
while STI [27] pipelines model shards loading with its com-
putation to reduce inference latency. LLMCad is inspired by
these efforts and propose an efficient speculative generation
pipeline to address the challenge of I/O blocking and limited
parallelism.
6 CONCLUSIONS
This work has proposed LLMCad , the first efficient inference
engine for on-device generative NLP tasks. It breaks the
memory wall and deliver LLM‚Äôs scaling ability to mobile
devices It incorporates three novel techniques, including:
token tree generation and verification, Self-adaptive fallback
strategy and speculative generation pipeline that can exploit
the waste hardware resources during the verification process.
Our experiments have demonstrated that when compared to
the state-of-the-art LLM engines, LLMCad can reduce average
per-token generation time by 2.9‚Äì9.3 √óand 3.5‚Äì4.7√óon IoT
devices and smartphones, without comprising accuracy.
REFERENCES
[1]Gboard - the Google Keyboard - Apps on Google Play ‚Äî
play.google.com. https://play.google.com/store/apps/details?id=com.
google.android.inputmethod.latin&hl=en. [Accessed 22-Jul-2023].
[2]Google Translate - Apps on Google Play ‚Äî play.google.com.
https://play.google.com/store/apps/details?id=com.google.android.
apps.translate&hl=en_US. [Accessed 22-Jul-2023].
[3]iTranslate ‚Äî itranslate.com. https://itranslate.com/. [Accessed 22-Jul-
2023].
[4] MNN. https://github.com/alibaba/MNN. Accessed: [2023.7].
[5] PyTorch. https://pytorch.org/. Accessed: [2023.7].
[6]Siri ‚Äî apple.com. https://www.apple.com/siri/. [Accessed 22-Jul-2023].
[7] TensorFlow. https://vllm.ai/. Accessed: [2023.7].[8] TensorFlow. https://www.tensorflow.org/. Accessed: [2023.7].
[9] TensorFlow. https://huggingface.co/TheBloke. Accessed: [2023.7].
[10] TensorFlow. https://github.com/PanQiWei/AutoGPTQ. Accessed:
[2023.7].
[11] Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen
Hambardzumyan, Susan Zhang, Stephen Roller, Naman Goyal, Omer
Levy, and Luke Zettlemoyer. Scaling laws for generative mixed-modal
language models. arXiv preprint arXiv:2301.03728, 2023.
[12] Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai.
Revisiting neural scaling laws in language and vision. Advances in
Neural Information Processing Systems, 35:22300‚Äì22312, 2022.
[13] Zhihao Bai, Zhen Zhang, Yibo Zhu, and Xin Jin. {PipeSwitch}:
Fast pipelined context switching for deep learning applications.
In14th USENIX Symposium onOperating Systems Design and
Implementation (OSDI 20), pages 499‚Äì514, 2020.
[14] David Bau, Hendrik Strobelt, William Peebles, Jonas Wulff, Bolei Zhou,
Jun-Yan Zhu, and Antonio Torralba. Semantic photo manipulation
with a generative image prior. arXiv preprint arXiv:2005.07727, 2020.
[15] Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow,
Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt
Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ale s Tam-
chyna. Findings of the 2014 workshop on statistical machine trans-
lation. In Proceedings oftheNinth Workshop onStatistical Machine
Translation , pages 12‚Äì58, Baltimore, Maryland, USA, June 2014. Asso-
ciation for Computational Linguistics.
[16] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph
Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster.
arXiv preprint arXiv:2210.09461, 2022.
[17] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish
Sastry, Amanda Askell, et al. Language models are few-shot learn-
ers. Advances inneural information processing systems , 33:1877‚Äì
1901, 2020.
[18] Han Cai, Ji Lin, Yujun Lin, Zhijian Liu, Haotian Tang, Hanrui Wang,
Ligeng Zhu, and Song Han. Enable deep learning on mobile devices:
Methods, systems, and applications. ACM Transactions onDesign
Automation ofElectronic Systems (TODAES), 27(3):1‚Äì50, 2022.
[19] Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues,
Sebastian St√ºker, Katsuhito Sudoh, Koichiro Yoshino, and Chris-
tian Federmann. Overview of the IWSLT 2017 evaluation cam-
paign. In Proceedings ofthe14th International Conference onSpoken
Language Translation , pages 2‚Äì14, Tokyo, Japan, December 14-15 2017.
International Workshop on Spoken Language Translation.
[20] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste
Lespiau, Laurent Sifre, and John Jumper. Accelerating large lan-
guage model decoding with speculative sampling. arXiv preprint
arXiv:2302.01318, 2023.
[21] Aidan Clark, Diego De Las Casas, Aurelia Guy, Arthur Mensch, Michela
Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor
Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed language
models. In International Conference onMachine Learning , pages
4057‚Äì4086. PMLR, 2022.
[22] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Hee-
woo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton,
Reiichiro Nakano, et al. Training verifiers to solve math word problems.
arXiv preprint arXiv:2110.14168, 2021.
[23] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin
Yang, and Jie Tang. Glm: General language model pretraining with
autoregressive blank infilling. In Proceedings ofthe60th Annual
Meeting oftheAssociation forComputational Linguistics (Volume 1:
Long Papers), pages 320‚Äì335, 2022.
13

--- PAGE 14 ---
Conference‚Äô17, July 2017, Washington, DC, USA Daliang Xu‚ô¶, Wangsong Yin‚ô¶, Xin Jin‚ô¶, Ying Zhang‚ô¶, Shiyun Wei‚ãÜ, Mengwei Xu^, Xuanzhe Liu‚ô¶
[24] Biyi Fang, Xiao Zeng, and Mi Zhang. Nestdnn: Resource-aware multi-
tenant on-device deep learning for continuous mobile vision. In
Proceedings ofthe24th Annual International Conference onMobile
Computing andNetworking, pages 115‚Äì127, 2018.
[25] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq:
Accurate post-training quantization for generative pre-trained trans-
formers. arXiv preprint arXiv:2210.17323, 2022.
[26] Hui Guan, Shaoshan Liu, Xiaolong Ma, Wei Niu, Bin Ren, Xipeng
Shen, Yanzhi Wang, and Pu Zhao. Cocopie: Enabling real-time ai on
off-the-shelf mobile devices via compression-compilation co-design.
Communications oftheACM, 64(6):62‚Äì68, 2021.
[27] Liwei Guo, Wonkyo Choe, and Felix Xiaozhu Lin. Sti: Turbocharge
nlp inference at the edge via elastic pipelining. In Proceedings of
the28th ACM International Conference onArchitectural Support for
Programming Languages and Operating Systems, Volume 2, pages
791‚Äì803, 2023.
[28] Ellen L. Hahne. Round-robin scheduling for max-min fairness in
data networks. IEEE Journal onSelected Areas incommunications ,
9(7):1024‚Äì1039, 1991.
[29] Rui Han, Qinglong Zhang, Chi Harold Liu, Guoren Wang, Jian Tang,
and Lydia Y Chen. Legodnn: block-grained scaling of deep neu-
ral networks for mobile vision. In Proceedings ofthe27th Annual
International Conference onMobile Computing and Networking ,
pages 406‚Äì419, 2021.
[30] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas
Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multi-
task language understanding. arXiv preprint arXiv:2009.03300, 2020.
[31] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adapta-
tion of large language models. arXiv preprint arXiv:2106.09685 , 2021.
[32] Chien-Chin Huang, Gu Jin, and Jinyang Li. Swapadvisor: Push-
ing deep learning beyond the gpu memory limit via smart swap-
ping. In Proceedings oftheTwenty-Fifth International Conference
onArchitectural Support forProgramming Languages andOperating
Systems, pages 1341‚Äì1355, 2020.
[33] Loc N Huynh, Youngki Lee, and Rajesh Krishna Balan. Deepmon: Mo-
bile gpu-based deep learning framework for continuous vision appli-
cations. In Proceedings ofthe15th Annual International Conference
onMobile Systems, Applications, andServices, pages 82‚Äì95, 2017.
[34] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Ben-
jamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and
Dario Amodei. Scaling laws for neural language models. arXiv preprint
arXiv:2001.08361, 2020.
[35] Jungo Kasai, Nikolaos Pappas, Hao Peng, James Cross, and
Noah A Smith. Deep encoder, shallow decoder: Reevaluating non-
autoregressive machine translation. arXiv preprint arXiv:2006.10369 ,
2020.
[36] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang,
Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer,
Michael W Mahoney, et al. Full stack optimization of transformer
inference: a survey. arXiv preprint arXiv:2302.14017, 2023.
[37] Sehoon Kim, Karttikeya Mangalam, Jitendra Malik, Michael W Ma-
honey, Amir Gholami, and Kurt Keutzer. Big little transformer decoder.
arXiv preprint arXiv:2302.07863, 2023.
[38] Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk
Kwon, Joseph Hassoun, and Kurt Keutzer. Learned token pruning for
transformers. In Proceedings ofthe28th ACM SIGKDD Conference
onKnowledge Discovery andData Mining, pages 784‚Äì794, 2022.
[39] Xiang Kong, Adithya Renduchintala, James Cross, Yuqing Tang, Jiatao
Gu, and Xian Li. Multilingual neural machine translation with deep en-
coder and multiple shallow decoders. arXiv preprint arXiv:2206.02079 ,
2022.[40] Royson Lee, Stylianos I Venieris, Lukasz Dudziak, Sourav Bhattacharya,
and Nicholas D Lane. Mobisr: Efficient on-device super-resolution
through heterogeneous mobile processors. In The 25th annual
international conference onmobile computing andnetworking , pages
1‚Äì16, 2019.
[41] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from
transformers via speculative decoding. In International Conference
onMachine Learning, pages 19274‚Äì19286. PMLR, 2023.
[42] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Ab-
delrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettle-
moyer. BART: denoising sequence-to-sequence pre-training for nat-
ural language generation, translation, and comprehension. CoRR ,
abs/1910.13461, 2019.
[43] Chin-Yew Lin. Rouge: A package for automatic evaluation of sum-
maries. In Text summarization branches out, pages 74‚Äì81, 2004.
[44] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring
how models mimic human falsehoods, 2021.
[45] llama.cpp. Port of Facebook‚Äôs LLaMA model in C/C++ Resources.
https://github.com/ggerganov/llama.cpp, Year of publication. Ac-
cessed: [2023.7].
[46] Chen Meng, Minmin Sun, Jun Yang, Minghui Qiu, and Yang Gu. Train-
ing deeper models by gpu memory optimization on tensorflow. In
Proc. ofMLSystems Workshop inNIPS, volume 7, 2017.
[47] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
Pointer sentinel mixture models, 2016.
[48] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu
Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna
Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative llm
serving with speculative inference and token tree verification. arXiv
preprint arXiv:2305.09781, 2023.
[49] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri,
Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei
Zaharia. Pipedream: Generalized pipeline parallelism for dnn training.
InProceedings ofthe27th ACM Symposium onOperating Systems
Principles, pages 1‚Äì15, 2019.
[50] Wei Niu, Xiaolong Ma, Sheng Lin, Shihao Wang, Xuehai Qian,
Xue Lin, Yanzhi Wang, and Bin Ren. Patdnn: Achieving real-time
dnn execution on mobile devices with pattern-based weight prun-
ing. In Proceedings oftheTwenty-Fifth International Conference
onArchitectural Support forProgramming Languages andOperating
Systems, pages 907‚Äì922, 2020.
[51] Roma Patel and Ellie Pavlick. Mapping language models to
grounded conceptual spaces. In International Conference onLearning
Representations, 2021.
[52] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin,
James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and
Jeff Dean. Efficiently scaling transformer inference. Proceedings of
Machine Learning andSystems, 5, 2023.
[53] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.
Improving language understanding by generative pre-training. 2018.
[54] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,
Ilya Sutskever, et al. Language models are unsupervised multitask
learners. OpenAI blog, 1(8):9, 2019.
[55] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
the limits of transfer learning with a unified text-to-text transformer.
TheJournal ofMachine Learning Research, 21(1):5485‚Äì5551, 2020.
[56] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
SQuAD: 100,000+ Questions for Machine Comprehension of Text.
arXiv e-prints, page arXiv:1606.05250, 2016.
[57] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and
Cho-Jui Hsieh. Dynamicvit: Efficient vision transformers with dynamic
14

--- PAGE 15 ---
LLMCad : Fast and Scalable On-device Large Language Model Inference Conference‚Äô17, July 2017, Washington, DC, USA
token sparsification. Advances inneural information processing
systems, 34:13937‚Äì13949, 2021.
[58] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt
Schiele, and Honglak Lee. Generative adversarial text to image synthe-
sis. In International conference onmachine learning , pages 1060‚Äì1069.
PMLR, 2016.
[59] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser,
and Bj√∂rn Ommer. High-resolution image synthesis with latent diffu-
sion models. In Proceedings oftheIEEE/CVF conference oncomputer
vision andpattern recognition, pages 10684‚Äì10695, 2022.
[60] Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri,
Vinh Tran, Yi Tay, and Donald Metzler. Confident adaptive lan-
guage modeling. Advances inNeural Information Processing Systems ,
35:17456‚Äì17472, 2022.
[61] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point:
Summarization with pointer-generator networks. In Proceedings
ofthe55th Annual Meeting oftheAssociation forComputational
Linguistics (Volume 1:Long Papers) , pages 1073‚Äì1083, Vancouver,
Canada, July 2017. Association for Computational Linguistics.
[62] Thierry Tambe, Coleman Hooper, Lillian Pentecost, Tianyu Jia, En-Yu
Yang, Marco Donato, Victor Sanh, Paul Whatmough, Alexander M
Rush, David Brooks, et al. Edgebert: Sentence-level energy optimiza-
tions for latency-aware multi-task nlp inference. In MICRO-54: 54th
Annual IEEE/ACM International Symposium onMicroarchitecture ,
pages 830‚Äì844, 2021.
[63] FP Team. Generative AI: Advantages, Disadvantages, Limitations,
and Challenges ‚Äî fact.technology. https://fact.technology/learn/
generative-ai-advantages-limitations-and-challenges/. [Accessed 22-
Jul-2023].
[64] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-
Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric
Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation
language models. arXiv preprint arXiv:2302.13971, 2023.
[65] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention
is all you need. Advances inneural information processing systems ,
30, 2017.
[66] Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse
attention architecture with cascade token and head pruning. In
2021 IEEE International Symposium onHigh-Performance Computer
Architecture (HPCA), pages 97‚Äì110. IEEE, 2021.
[67] Shibo Wang, Jinliang Wei, Amit Sabne, Andy Davis, Berkin Ilbeyi,
Blake Hechtman, Dehao Chen, Karthik Srinivasa Murthy, Marcello
Maggioni, Qiao Zhang, et al. Overlap communication with depen-
dent computation via decomposition in large deep learning mod-
els. In Proceedings ofthe28th ACM International Conference on
Architectural Support forProgramming Languages and Operating
Systems, Volume 1, pages 93‚Äì106, 2022.
[68] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming
Zhou. Minilm: Deep self-attention distillation for task-agnostic com-
pression of pre-trained transformers. Advances inNeural Information
Processing Systems, 33:5776‚Äì5788, 2020.
[69] Xiaofei Wang, Yiwen Han, Victor CM Leung, Dusit Niyato, Xueqiang
Yan, and Xu Chen. Convergence of edge computing and deep learning:
A comprehensive survey. IEEE Communications Surveys &Tutorials ,
22(2):869‚Äì904, 2020.
[70] Yiding Wang, Kai Chen, Haisheng Tan, and Kun Guo. Tabi: An efficient
multi-level inference system for large language models. In Proceedings
oftheEighteenth European Conference onComputer Systems , pages
233‚Äì248, 2023.
[71] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebas-
tian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, DonaldMetzler, et al. Emergent abilities of large language models. arXiv
preprint arXiv:2206.07682, 2022.
[72] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia,
Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompt-
ing elicits reasoning in large language models. Advances inNeural
Information Processing Systems, 35:24824‚Äì24837, 2022.
[73] Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. Deebert:
Dynamic early exiting for accelerating bert inference. arXiv preprint
arXiv:2004.12993, 2020.
[74] Mengwei Xu, Mengze Zhu, Yunxin Liu, Felix Xiaozhu Lin, and Xu-
anzhe Liu. Deepcache: Principled cache for mobile deep vision. In
Proceedings ofthe24th annual international conference onmobile
computing andnetworking, pages 129‚Äì144, 2018.
[75] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-
Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: A mas-
sively multilingual pre-trained text-to-text transformer. arXiv preprint
arXiv:2010.11934, 2020.
[76] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu,
Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable
post-training quantization for large-scale transformers. Advances in
Neural Information Processing Systems, 35:27168‚Äì27183, 2022.
[77] Zhao You, Shulin Feng, Dan Su, and Dong Yu. Speechmoe2: Mixture-
of-experts model with improved routing. In ICASSP 2022-2022 IEEE
International Conference onAcoustics, Speech andSignal Processing
(ICASSP), pages 7217‚Äì7221. IEEE, 2022.
[78] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim,
and Byung-Gon Chun. Orca: A distributed serving system for
{Transformer-Based }generative models. In 16th USENIX Symposium
onOperating Systems Design andImplementation (OSDI 22), pages
521‚Äì538, 2022.
[79] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming
Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b:
An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 ,
2022.
[80] Wuyang Zhang, Zhezhi He, Luyang Liu, Zhenhua Jia, Yunxin Liu,
Marco Gruteser, Dipankar Raychaudhuri, and Yanyong Zhang. Elf:
accelerate high-resolution mobile deep vision with content-aware
parallel offloading. In Proceedings ofthe27th Annual International
Conference onMobile Computing andNetworking , pages 201‚Äì214,
2021.
[81] Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian McAuley, Ke Xu, and
Furu Wei. Bert loses patience: Fast and robust inference with early exit.
Advances inNeural Information Processing Systems , 33:18330‚Äì18341,
2020.
[82] Zhe Zhou, Xuechao Wei, Jiejing Zhang, and Guangyu Sun. {PetS}: A
unified framework for {Parameter-Efficient }transformers serving. In
2022 USENIX Annual Technical Conference (USENIX ATC 22), pages
489‚Äì504, 2022.
15
