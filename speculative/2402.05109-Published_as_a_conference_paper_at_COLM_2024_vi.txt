Đã xuất bản như một bài báo hội nghị tại COLM 2024

Hydra: Các Đầu Dự Thảo Phụ Thuộc Tuần Tự cho Giải Mã Medusa

Zachary Ankner1,2,∗Rishab Parthasarathy1,∗Aniruddha Nrusimha1
Christopher Rinard1Jonathan Ragan-Kelley1William Brandon1
1MIT2MosaicML

Tóm tắt

Để chống lại bản chất bị ràng buộc băng thông bộ nhớ của suy luận LLM tự hồi quy, nghiên cứu trước đây đã đề xuất khung giải mã suy đoán. Để thực hiện giải mã suy đoán, một mô hình dự thảo nhỏ đề xuất các tiếp tục ứng cử viên của chuỗi đầu vào sau đó được xác minh song song bởi mô hình cơ sở. Một cách để chỉ định mô hình dự thảo, như được sử dụng trong khung giải mã Medusa gần đây, là như một tập hợp các đầu nhẹ, được gọi là đầu dự thảo, hoạt động trên các trạng thái ẩn của mô hình cơ sở. Cho đến nay, tất cả các đầu dự thảo hiện tại đều độc lập tuần tự, có nghĩa là chúng suy đoán các token trong tiếp tục ứng cử viên độc lập với bất kỳ token nào trước đó trong tiếp tục ứng cử viên. Trong công trình này, chúng tôi đề xuất các đầu Hydra: một thay thế phụ thuộc tuần tự có thể thay thế trực tiếp cho các đầu dự thảo tiêu chuẩn nhằm cải thiện đáng kể độ chính xác của suy đoán đầu dự thảo. Chúng tôi tiếp tục khám phá không gian thiết kế của các mục tiêu huấn luyện và kiến trúc đầu Hydra, và đề xuất một công thức đầu Hydra được điều chỉnh cẩn thận, mà chúng tôi gọi là Hydra++, cải thiện thông lượng giải mã lên đến 1.31 × và 2.70 × so với giải mã Medusa và giải mã tự hồi quy tương ứng. Nhìn chung, các đầu Hydra là một can thiệp đơn giản và có động lực rõ ràng đối với các đầu dự thảo tiêu chuẩn nhằm cải thiện đáng kể tốc độ end-to-end của giải mã suy đoán dựa trên đầu dự thảo. Chúng tôi công khai mã nguồn tại https://github.com/zankner/Hydra.

1 Giới thiệu

Khi các mô hình ngôn ngữ lớn (LLM) dựa trên transformer đã được triển khai rộng rãi, nghiên cứu về cải thiện hiệu quả suy luận của các mô hình này đã trở nên ngày càng quan trọng. Trong khi việc tiền huấn luyện LLM đạt được việc sử dụng phần cứng cao bằng cách hoạt động trên toàn bộ chuỗi đầu vào song song, hiệu quả của suy luận LLM truyền thống đã bị ràng buộc bởi nhu cầu tạo ra các token từng cái một theo trình tự. Trên phần cứng GPU hiện tại, bản chất tuần tự của giải mã LLM làm cho nó trở thành vấn đề bị ràng buộc băng thông bộ nhớ, với thông lượng bị giới hạn bởi việc di chuyển các ma trận trọng số lớn từ bộ nhớ chính GPU đến các thanh ghi cục bộ. Vì mỗi bước tạo sinh yêu cầu truy cập toàn bộ trọng số của mô hình, nhưng chỉ liên quan đến một số lượng tương đối nhỏ các FLOP (chỉ xử lý một token cho mỗi chuỗi trong batch), giải mã LLM có xu hướng không sử dụng hết khả năng phong phú của GPU cho tính toán dấu phẩy động.

Để giảm thiểu nút thắt cổ chai băng thông bộ nhớ trong giải mã LLM tuần tự, nghiên cứu gần đây đã điều tra việc tăng tốc suy luận LLM thông qua giải mã suy đoán. Giải mã suy đoán sử dụng một mô hình dự thảo nhỏ hơn để đề xuất một tiếp tục ứng cử viên nhiều token của chuỗi hiện tại ở mỗi bước tạo sinh. LLM gốc sau đó xác minh tất cả các token trong tiếp tục ứng cử viên song song, thêm một số tập con của chúng vào chuỗi và loại bỏ phần còn lại. Bởi vì mỗi bước xác minh chỉ yêu cầu một lần truyền thuận duy nhất qua LLM gốc, nhưng có thể dẫn đến việc thêm nhiều hơn một token vào chuỗi, giải mã suy đoán có thể tăng tốc giải mã bằng cách giảm lượng di chuyển dữ liệu trọng số cần thiết cho mỗi token được tạo.

∗Đóng góp ngang nhau. Liên hệ với ankner@mit.edu.

Một thành phần quan trọng trong bất kỳ ứng dụng nào của giải mã suy đoán là lựa chọn mô hình dự thảo, phải đủ rẻ sao cho chi phí truy vấn nó không làm triệt tiêu lợi ích hiệu quả từ việc truy vấn mô hình cơ sở song song, nhưng đủ chính xác sao cho tỷ lệ chấp nhận trong bước xác minh vẫn cao. Trong khi các mô hình dự thảo được sử dụng trong giải mã suy đoán truyền thống là các mô hình ngôn ngữ độc lập, được huấn luyện riêng biệt, Stern et al. (2018) và Cai et al. (2024) thay vào đó điều tra việc cấu trúc mô hình dự thảo như một tập hợp các đầu nhẹ hoạt động trên các trạng thái ẩn giàu ngữ nghĩa của mô hình cơ sở. Chúng tôi gọi các đầu nhẹ hoạt động trên các trạng thái ẩn gốc của LLM là đầu dự thảo. Trong mô hình đầu dự thảo, mỗi đầu dự thảo chịu trách nhiệm dự đoán danh tính của một token cách một số bước cố định trong tương lai.

Tất cả các đầu dự thảo cho đến nay đều đưa ra dự đoán chỉ như một hàm của các trạng thái ẩn của mô hình cơ sở từ các token đã được xác minh trước đó, làm cho chúng không nhận thức được các token trước đó trong tiếp tục ứng cử viên hiện tại. Do các phụ thuộc thống kê mạnh giữa các token lân cận trong ngôn ngữ, sự độc lập tuần tự này giới hạn độ chính xác dự đoán của các kiến trúc đầu dự thảo hiện tại. Trong công trình này, chúng tôi đề xuất các đầu Hydra: một lựa chọn thay thế phụ thuộc tuần tự có thể thay thế trực tiếp cho các đầu dự thảo tiêu chuẩn nhằm cải thiện độ chính xác dự đoán token và do đó cải thiện thông lượng giải mã end-to-end. Để xây dựng các đầu dự thảo phụ thuộc tuần tự, chúng tôi đặt đầu ra của mỗi đầu là một hàm của tiếp tục ứng cử viên cho đến nay. Thay đổi thiết kế đơn giản này dẫn đến chất lượng suy đoán tốt hơn đáng kể so với các đầu dự thảo tiêu chuẩn, tăng độ dài chấp nhận tiếp tục ứng cử viên trung bình lên đến 0.46 token.

Cải thiện chất lượng suy đoán này tương ứng với cải thiện đáng kể về tốc độ giải mã, với giải mã dựa trên đầu Hydra đạt được thông lượng tốt hơn lên đến 1.1 × so với giải mã Medusa.

Ngoài việc đề xuất các đầu Hydra, chúng tôi tiếp tục khám phá không gian thiết kế của mục tiêu huấn luyện và kiến trúc của chúng. Chúng tôi thấy rằng việc mở rộng độ sâu của các MLP đầu dự thảo, sử dụng mục tiêu chưng cất giáo viên, và thêm một lớp decoder transformer bổ sung để mã hóa tốt hơn chuỗi đã được xác minh đạt được thông lượng cao hơn lên đến 1.31 × và 2.70 × so với giải mã Medusa tiêu chuẩn và giải mã tự hồi quy thông thường tương ứng.

Cuối cùng, chúng tôi điều tra giải mã Hydra và Hydra++ trong các cài đặt suy luận thay thế. Cài đặt đầu tiên mà chúng tôi xem xét là suy luận theo batch. Cài đặt tiếp theo chúng tôi kiểm tra là giải mã không tham lam. Chúng tôi chỉ ra rằng bằng cách sử dụng lấy mẫu chấp nhận điển hình (Cai et al., 2024), một tiêu chí xác minh không bảo toàn phân phối, Hydra++ có thể đạt được chất lượng tạo sinh giống như lấy mẫu không tham lam của mô hình cơ sở trong khi không làm giảm độ dài chấp nhận.

Đóng góp Trong công trình này, chúng tôi trình bày các đóng góp sau:
• Chúng tôi phân tích công thức tiêu chuẩn của các đầu dự thảo và quan sát rằng chúng độc lập tuần tự trong quá trình giải mã. Chúng tôi đề xuất các đầu Hydra như một lựa chọn thay thế phụ thuộc tuần tự và chỉ ra rằng việc giới thiệu phụ thuộc tuần tự tăng thông lượng giải mã end-to-end lên đến 1.10 × so với giải mã Medusa (Phần 6.1).
• Chúng tôi khám phá không gian thiết kế của các đầu Hydra để tạo ra công thức đầu dự thảo Hydra++ nhằm tăng thêm thông lượng giải mã lên đến 1.31 × và 2.70 × so với giải mã Medusa và giải mã tự hồi quy tiêu chuẩn tương ứng (Phần 3.1, Phần 6.1).
• Chúng tôi phân tích hiệu suất của giải mã Hydra trong cài đặt suy luận theo batch, chứng minh rằng nó đạt được thông lượng tốt hơn Medusa ở tất cả các kích thước batch được đánh giá (Phần 6.2).
• Chúng tôi chứng minh rằng lấy mẫu sử dụng tiêu chí chấp nhận điển hình cho phép Hydra++ đạt được chất lượng giống như lấy mẫu từ mô hình cơ sở trong khi bảo toàn lợi ích thông lượng của suy đoán (Phần 6.3).

2 Kiến thức nền tảng

Giải mã suy đoán. Giải mã suy đoán (Stern et al., 2018; Leviathan et al., 2023; Chen et al., 2023) cung cấp một khung tổng quát cho giải mã LLM hiệu quả. Giải mã suy đoán tạo văn bản bằng cách kết hợp một mô hình cơ sở đắt đỏ, chất lượng cao với một mô hình dự thảo rẻ hơn, chất lượng thấp hơn. Đối với mỗi bước giải mã, mô hình dự thảo tạo một hoặc nhiều tiếp tục ứng cử viên, mỗi cái mở rộng nhiều token vào tương lai. Sau đó chúng tôi sử dụng một lần truyền thuận duy nhất qua mô hình cơ sở để xác minh các tiếp tục ứng cử viên này song song dựa trên một tiêu chí xác minh nào đó. Quá trình xác minh xác định các token ứng cử viên nào sẽ được thêm vào chuỗi và các token nào sẽ bị loại bỏ.

Trong dạng đơn giản nhất của giải mã suy đoán, mô hình dự thảo chỉ tạo một tiếp tục ứng cử viên duy nhất ở mỗi bước tạo sinh. Gọi x≤t là chuỗi đã được tạo cho đến nay và cố định một độ dài suy đoán K nào đó, chúng tôi truy vấn phân phối liên hợp của mô hình dự thảo pdraft(xt+1,...,xt+K|x≤t) để tạo một tiếp tục ứng cử viên ˆxt+1,...,ˆxt+K. Sau đó chúng tôi gọi mô hình cơ sở trên tiếp tục ứng cử viên để tính các xác suất có điều kiện: pbase(ˆxt+1|x≤t),...,pbase(ˆxt+K|x≤t,ˆxt+1,...,ˆxt+K−1); việc truy vấn mô hình cơ sở được thực hiện trong một lần truyền thuận duy nhất. Các xác suất mô hình cơ sở này trở thành đầu vào cho tiêu chí xác minh, chọn một tiền tố ˆxt+1,...,ˆxt+naccept của tiếp tục ứng cử viên để chấp nhận, loại bỏ phần còn lại.

Các tiêu chí xác minh phổ biến để sử dụng với giải mã suy đoán bao gồm lấy mẫu lại từ chối (Leviathan et al., 2023; Chen et al., 2023), đảm bảo rằng phân phối đầu ra khớp với phân phối của mô hình cơ sở, và chấp nhận tham lam (Stern et al., 2018), trong đó các token ứng cử viên được chấp nhận nếu chúng khớp với dự đoán có khả năng nhất của LLM cơ sở. Đối với tất cả các tiêu chí xác minh đang được sử dụng phổ biến, việc sử dụng mô hình dự thảo có dự đoán khớp với mô hình cơ sở một cách chính xác hơn sẽ dẫn đến độ dài chấp nhận trung bình tăng, và do đó thông lượng giải mã lớn hơn.

Giải mã cây. Giải mã suy đoán có thể được tổng quát hóa cho các cài đặt trong đó mô hình dự thảo đề xuất một cây các tiếp tục ứng cử viên, thay vì một tiếp tục ứng cử viên duy nhất (Miao et al., 2023; Spector & Re, 2023; Cai et al., 2024). Các nút của cây ứng cử viên này tương ứng với các token ứng cử viên, và các con của một nút đại diện cho các token khác nhau có thể có thể theo sau nó trong tiếp tục. Do đó, mỗi đường dẫn dọc theo cây đại diện cho một tiếp tục ứng cử viên khác nhau. Để điền vào một nút với m con, chúng tôi truy vấn mô hình dự thảo cho m token có khả năng nhất có thể theo sau nó, có điều kiện trên chuỗi được tạo cho đến nay và tiếp tục ứng cử viên được định nghĩa bởi đường dẫn đến nút từ gốc của cây. Các con ở mỗi nút được sắp xếp theo thứ tự giảm dần của xác suất có điều kiện. Thông thường, các cây tĩnh được sử dụng trong đó cấu trúc của cây được cố định tại thời điểm thiết kế, có nghĩa là số con m ở mỗi vị trí trong cây không phụ thuộc vào bất kỳ dữ liệu thời gian chạy nào.

Sau khi điền cây tiếp tục ứng cử viên bằng mô hình dự thảo, chúng tôi tính các xác suất có điều kiện của tất cả các nút trong cây bằng một lần truyền thuận duy nhất qua mô hình cơ sở. Chúng tôi truy vấn mô hình cơ sở cho các xác suất có điều kiện này trong một lần truyền thuận duy nhất bằng cách đóng gói tất cả các token của cây vào một chuỗi đầu vào duy nhất, và thao tác mặt nạ attention để đảm bảo rằng mỗi token chỉ có thể attend đến các cha mẹ của nó trong cây. Các xác suất có điều kiện thu được từ việc truy vấn mô hình cơ sở sau đó có thể được sử dụng làm đầu vào cho các tiêu chí xác minh giống như được sử dụng trong cài đặt ứng cử viên đơn.

Các đầu nhẹ như một mô hình dự thảo. Trong khi thông thường mô hình dự thảo được sử dụng trong giải mã suy đoán là một mô hình ngôn ngữ được huấn luyện độc lập, Stern et al. (2018) định nghĩa mô hình dự thảo như một tập hợp các đầu nhẹ, mà chúng tôi gọi là đầu dự thảo, nhận đầu vào là trạng thái ẩn của mô hình cơ sở. Với K là độ dài suy đoán tối đa, mô hình dự thảo được sử dụng bởi Stern et al. được định nghĩa bởi một tập hợp các MLP nhỏ fdraft,1, ..., fdraft,K chịu trách nhiệm dự đoán các token 1, ..., K bước vào tương lai. Các dự đoán của các đầu này độc lập thống kê với nhau; gọi x≤t là chuỗi được tạo cho đến nay, và gọi ht−1 là trạng thái ẩn lớp cuối của token được xử lý gần đây nhất bởi mô hình cơ sở, Stern et al. tính các dự đoán dự thảo của họ ở mỗi bước tạo sinh như pdraft(xt+i|x≤t+i−1) = fdraft,i(ht−1)

Hình 1 cung cấp một hình ảnh hóa việc tạo tiếp tục ứng cử viên bằng các đầu dự thảo.

Giải mã Medusa. Giải mã Medusa (Cai et al., 2024) là một cấu hình cụ thể của các kỹ thuật được liệt kê ở trên. Cụ thể, đó là giải mã suy đoán với một cây ứng cử viên, trong đó mô hình dự thảo là một tập hợp các đầu dự thảo.

Trong khi giải mã Medusa không phụ thuộc vào kiến trúc được sử dụng cho mỗi đầu dự thảo fdraft,i, Cai et al. (2024) chọn sử dụng một MLP một lớp với kết nối tàn dư.

3 Các đầu Hydra

Quan sát chính đằng sau các đầu Hydra là không có phụ thuộc tuần tự trong các đầu dự thảo tiêu chuẩn, tức là mỗi đầu dự thảo đưa ra dự đoán độc lập. Một mô hình dự thảo được định nghĩa bởi một tập hợp các đầu dự thảo dự đoán danh tính của token thứ i trong tương lai như fdraft,i(ht−1). Tuy nhiên, ht−1 chỉ là một hàm của chuỗi đã được tạo x≤t−1. Do đó, khi sử dụng các đầu dự thảo:
pdraft(ˆxt+i|x≤t,ˆxt+1, ..., ˆxt+i−1) = pdraft(ˆxt+i|x≤t−1)

Trực quan, điều này có nghĩa là không có phụ thuộc tuần tự giữa các đầu dự thảo: khi chúng ta sử dụng một đầu dự thảo để suy đoán token thứ i trong một tiếp tục ứng cử viên, nó không nhận thức được các token thứ 1, 2, ..., (i−1) trong tiếp tục ứng cử viên.

Chúng tôi đề xuất các đầu Hydra, là các đầu dự thảo phụ thuộc tuần tự. Các đầu Hydra phụ thuộc tuần tự vì chúng là một hàm của cả trạng thái ẩn của mô hình cơ sở đến thời điểm t cũng như các embedding đầu vào của các token được lấy mẫu bởi các đầu Hydra trước đó. Cụ thể, mô hình dự thảo bây giờ là một tập hợp các đầu Hydra {fHydra,1, ..., fHydra,K} và phân phối của token thứ i trong tương lai được tham số hóa bởi tập hợp các đầu Hydra này như:
pdraft(ˆxt+i|x≤t,ˆxt+1, ..., ˆxt+i−1) = fHydra,i(ht−1,xt,ˆxt+1, ...,ˆxt+i−1)

trong đó ht−1 lại là trạng thái ẩn của mô hình cơ sở của token cuối cùng trong chuỗi đã được giải mã. Phụ thuộc tuần tự của các đầu Hydra so với các đầu dự thảo tiêu chuẩn được hình ảnh hóa trong Hình 1. Chúng tôi sử dụng thuật ngữ Giải mã Hydra để chỉ giải mã suy đoán với các ứng cử viên cây và các đầu Hydra.

Như với Medusa, khung của giải mã Hydra tương thích với bất kỳ lựa chọn kiến trúc mô hình nào được sử dụng để thực hiện fHydra,i. Kiến trúc đầu Hydra cơ bản nhất mà chúng tôi kiểm tra chỉ đơn giản là một MLP một lớp ẩn có đầu vào là trạng thái ẩn ht−1 được nối với các embedding đầu vào của các token trước đó trong tiếp tục ứng cử viên Ext,Eˆxt+1, ...,Eˆxt+i−1, trong đó việc nối được thực hiện dọc theo chiều đặc trưng.

3.1 Hydra++

Với mục tiêu cải thiện thêm các đầu dự thảo, chúng tôi điều tra các cải tiến mục tiêu huấn luyện và kiến trúc đầu dự thảo trực giao với việc giới thiệu phụ thuộc tuần tự. Chúng tôi chi tiết việc tìm kiếm công thức đầu dự thảo tối ưu này trong Phụ lục A. Cuối cùng, chúng tôi thấy rằng ba thay đổi là có lợi:

1. Mở rộng quy mô: Chúng tôi mở rộng MLP của mỗi đầu thành 4 lớp. Trong các thí nghiệm của chúng tôi, chúng tôi thấy rằng mở rộng quy mô lên 5 lớp trở lên không mang lại lợi ích bổ sung.

2. Chưng cất: Theo Zhou et al. (2024), chúng tôi huấn luyện trên mục tiêu tự chưng cất trong đó các đầu dự thảo được huấn luyện để dự đoán phân phối của mô hình cơ sở cho một token nhất định thay vì token thực.

3. Attention tiền tố: Để cải thiện khả năng của mô hình dự thảo trong việc điều kiện hóa thông tin từ khắp toàn bộ ngữ cảnh, thay vì chỉ token được xác minh gần đây nhất, chúng tôi mở rộng mô hình cơ sở với một lớp decoder self-attention bổ sung có vai trò duy nhất là tạo ra các trạng thái ẩn nhiều thông tin hơn để sử dụng làm đầu vào cho mô hình dự thảo. Lớp được thêm này chỉ được truy vấn một lần cho mỗi bước giải mã.

Chúng tôi gọi phiên bản Hydra tận dụng tất cả các thay đổi trên là Hydra++.

4 Khám phá các cây giải mã hiệu suất cao

Tương tự như Medusa, chúng tôi luôn thực hiện giải mã suy đoán dựa trên cây với topology cây tĩnh được tính toán offline. Tính toán topology cây hiệu suất cao cho một cài đặt suy luận nhất định là không tầm thường, bởi vì các cài đặt khác nhau yêu cầu các topology cây khác nhau; lựa chọn mô hình cơ sở, mô hình dự thảo, kích thước batch (được thảo luận thêm trong Phần 6.2), và phần cứng đều có thể ảnh hưởng đến hiệu suất tương đối của các cây khác nhau.

Chúng tôi rút ra các cây giải mã theo cách định hướng dữ liệu bằng thuật toán hai giai đoạn: đầu tiên, chúng tôi tìm một chuỗi các cây "đề xuất" T1,...,TN với kích thước 1,...,N sao cho mỗi cây đề xuất xấp xỉ tối đa hóa độ dài chấp nhận mong đợi cho kích thước của nó; sau đó, chúng tôi chọn kích thước cây tối ưu cho một thiết lập nhất định bằng cách đo thực nghiệm thông lượng end-to-end đạt được khi sử dụng mỗi Ti, và chọn cây tối đa hóa thông lượng.

Để xác định chuỗi các cây đề xuất T1,...,TN, chúng tôi theo một quy trình tham lam lặp đơn giản. Chúng tôi đầu tiên khởi tạo T1 thành cây một nút tầm thường. Sau đó, ở mỗi bước i, chúng tôi mô phỏng giải mã suy đoán bằng Ti−1 trên một kho ngữ liệu văn bản mẫu, và xác định con của một nút hiện tại sẽ mang lại cải thiện lớn nhất về độ dài chấp nhận mong đợi nếu được thêm vào cây. Sau đó chúng tôi thêm nút này vào Ti−1 để tạo thành cây Ti, và lặp lại.

Sau khi tính toán T1,...,TN, chúng tôi đo thông lượng của giải mã suy đoán bằng mỗi Ti trong cấu hình suy luận mong muốn của chúng tôi (tức là kích thước batch, phần cứng, v.v.), và chọn cây tối đa hóa thông lượng giải mã thực nghiệm.

Trong thực tế, chúng tôi đặt kích thước cây tối đa là N=100, và sử dụng một tập con 100 câu hỏi của bộ dữ liệu Alpaca (Taori et al., 2023) để thu thập thống kê độ dài chấp nhận mô phỏng và thông lượng của chúng tôi. Chúng tôi cung cấp thêm chi tiết về các cây được khám phá cho mỗi chiến lược giải mã và kích thước batch trong Phụ lục B.

5 Chi tiết huấn luyện và đánh giá chung

Trong phần này, chúng tôi chi tiết các yếu tố của quy trình huấn luyện và đánh giá chung trong tất cả các thí nghiệm của chúng tôi.

Các mô hình. Làm mô hình cơ sở, chúng tôi xây dựng trên họ mô hình Vicuna (Chiang et al., 2023), là các mô hình LLaMa được fine-tune cho đối thoại (Touvron et al., 2023). Chúng tôi xem xét các mô hình Vicuna 7B, 13B, và 33B tham số.

Huấn luyện. Trong khi các đầu dự thảo có thể được huấn luyện cùng với mô hình cơ sở, trong công trình này chúng tôi chỉ nghiên cứu các mô hình cơ sở với trọng số đóng băng. Tất cả các mô hình được huấn luyện trên bộ dữ liệu ShareGPT (ShareGPT, 2023), một tập hợp các cuộc hội thoại nhiều lượt. Huấn luyện được thực hiện trên 8 × NVIDIA A100-80GB GPU và được tiến hành bằng HuggingFace Trainer (HuggingFace). Chúng tôi sử dụng lịch trình tốc độ học cosine với warmup (Loshchilov & Hutter, 2017) và tốc độ học đỉnh 1e-3, và chúng tôi sử dụng trình tối ưu hóa AdamW (Loshchilov & Hutter, 2019) với các tham số β1=0.9, β2=0.999. Tất cả các đầu Hydra và Medusa được huấn luyện trong một epoch, vì chúng tôi quan sát thấy rằng hiệu suất cho các mô hình đó bão hòa ở một epoch và không cải thiện với việc huấn luyện thêm. Tất cả các đầu Hydra++ được huấn luyện trong mười epoch.

Đánh giá. Tất cả các đánh giá được thực hiện trên MT-Bench (Zheng et al., 2023), một benchmark hội thoại nhiều lượt. Trừ khi được chỉ định khác, các thí nghiệm được tiến hành bằng giải mã suy đoán với tiêu chí xác minh tham lam; vì không có tính ngẫu nhiên trong quy trình lấy mẫu tham lam, chúng tôi không báo cáo chất lượng của các thế hệ vì chúng giống hệt với mô hình cơ sở. Thay vào đó, chúng tôi báo cáo thông lượng trung bình, là số token được tạo ra mỗi giây, và độ dài chấp nhận trung bình, là số token được tạo ra mỗi bước giải mã, để đánh giá tốc độ và chất lượng của giải mã Hydra. Chúng tôi benchmark tất cả các thí nghiệm tham số 7B và 13B trên một GPU A100-40GB duy nhất và tất cả các thí nghiệm tham số 33B trên một GPU A100-80GB duy nhất.

6 Kết quả

Trong phần này chúng tôi điều tra các đặc tính hiệu suất của giải mã Hydra. Chúng tôi kiểm tra tác động của kiến trúc mô hình dự thảo đối với thông lượng và độ trễ trên một loạt kích thước batch, và cũng điều tra tác động của kiến trúc mô hình dự thảo đối với chất lượng tạo sinh khi giải mã với các tiêu chí xác minh không bảo toàn phân phối.

6.1 Thí nghiệm thông lượng giải mã kích thước batch-1

Để đánh giá tác động của các can thiệp của chúng tôi đối với độ chính xác dự đoán mô hình dự thảo (và do đó thông lượng giải mã), chúng tôi so sánh thông lượng giải mã kích thước batch-1 có thể đạt được khi sử dụng các đầu dự thảo Medusa, các đầu dự thảo Hydra cơ bản của chúng tôi, và các đầu dự thảo Hydra++ nâng cao của chúng tôi. Chúng tôi cũng bao gồm thông lượng của giải mã tự hồi quy không suy đoán làm đường cơ sở. Chúng tôi tóm tắt kết quả của các thí nghiệm thông lượng giải mã này trong Hình 2.

Chúng tôi thấy rằng trên tất cả các kích thước mô hình cơ sở được đánh giá, Hydra đạt được độ dài chấp nhận trung bình cao hơn Medusa, và Hydra++ đạt được độ dài chấp nhận cao hơn Hydra, dẫn đến cải thiện đáng kể về thông lượng giải mã trong cả hai trường hợp. Cụ thể, các đầu Hydra đạt được cải thiện thông lượng 2.36 ×, 2.17×, và 2.15 × so với giải mã tự hồi quy cho các mô hình cơ sở 7B, 13B, và 33B tham số tương ứng. Điều này chuyển thành cải thiện thông lượng so với giải mã Medusa là 1.11 ×, 1.10×, và 1.11× cho các mô hình cơ sở 7B, 13B, và 33B tham số tương ứng. Hơn nữa, Hydra++ thậm chí còn hiệu suất hơn và đạt được cải thiện thông lượng so với giải mã tự hồi quy là 2.70 ×, 2.50×, và 2.53 × chuyển thành cải thiện thông lượng so với Medusa là 1.27 ×, 1.27×, và 1.31 × cho các mô hình cơ sở 7B, 13B, và 33B tham số tương ứng. Các kết quả này chứng minh rằng việc làm cho các đầu dự thảo phụ thuộc tuần tự cải thiện đáng kể độ chính xác dự đoán của chúng, và do đó tốc độ giải mã của chúng. Hơn nữa, các kết quả này cho thấy rằng bất kỳ overhead nào được giới thiệu bằng cách chuyển từ Hydra sang kiến trúc Hydra++ biểu cảm hơn đều được bù đắp hơn bằng sự gia tăng độ chính xác mà những thay đổi đó cho phép. Chúng tôi điều tra thêm overhead đầu dự thảo trong Phụ lục D.

6.2 Giải mã suy đoán cho suy luận theo batch

Các kỹ thuật giải mã suy đoán thường được đánh giá trong cài đặt kích thước batch-1. Khi chỉ có một chuỗi duy nhất trong batch, giải mã bị ràng buộc băng thông bộ nhớ cực kỳ, và số lượng lớn FLOP có thể được tiêu thụ trong bước xác minh của giải mã suy đoán "miễn phí" mà không làm tăng đáng kể độ trễ mỗi bước giải mã. Tuy nhiên, ở kích thước batch lớn hơn, việc xác minh dễ trở thành bị ràng buộc tính toán hơn, và số token được xác minh mỗi chuỗi mỗi bước phải được kiểm soát chặt chẽ hơn để tránh làm bão hòa khả năng tính toán của GPU và đi vào chế độ mà giải mã suy đoán trở nên không có lợi.

Để đánh giá liệu các lợi ích hiệu suất từ Hydra và Hydra++ quan sát thấy ở kích thước batch 1 có tiếp tục giữ trong chế độ suy luận theo batch hay không, chúng tôi đánh giá thông lượng và độ trễ của Medusa, Hydra, và Hydra++ ở kích thước batch {1,2,4,8} bằng xác minh tham lam và mô hình cơ sở 7B. Chúng tôi rút ra cây giải mã được sử dụng cho mỗi cấu hình mô hình dự thảo và kích thước batch bằng thuật toán được mô tả trong Phần 4.

Kết quả Chúng tôi vẽ mối quan hệ giữa kích thước batch, thông lượng, và độ trễ bằng mô hình cơ sở 7B trong Hình 3. Trong khi chúng tôi thấy rằng tất cả các kỹ thuật giải mã suy đoán vượt trội so với giải mã tự hồi quy tiêu chuẩn cho tất cả các kích thước batch được kiểm tra, cải thiện tương đối của giải mã suy đoán giảm khi kích thước batch tăng. Cụ thể, đối với kích thước batch 1 Hydra++ có cải thiện thông lượng 2.70 × so với giải mã tiêu chuẩn, nhưng lợi ích này giảm xuống 1.63 × ở kích thước batch 8. Các kết quả này cho thấy rằng trong khi lợi ích từ giải mã Hydra ít đáng kể hơn ở kích thước batch lớn hơn, giải mã Hydra vẫn là một cải thiện so với cả Medusa và giải mã tiêu chuẩn ở kích thước batch lớn hơn.

6.3 Lấy mẫu chấp nhận điển hình

Cho đến nay chúng tôi đã sử dụng tiêu chí chấp nhận tham lam, trong đó các token ứng cử viên chỉ được chấp nhận nếu chúng khớp với dự đoán token tiếp theo tham lam của LLM cơ sở. Bây giờ chúng tôi đánh giá tác động của giải mã Hydra đối với thông lượng và chất lượng bằng tiêu chí xác minh chấp nhận điển hình không tham lam, không bảo toàn phân phối được giới thiệu bởi Cai et al. (2024).

Tiêu chí chấp nhận điển hình. Mục đích của tiêu chí xác minh chấp nhận điển hình là lấy mẫu các chuỗi đa dạng và sáng tạo hơn so với chấp nhận tham lam, trong khi bảo toàn lợi ích hiệu quả của giải mã suy đoán bằng cách tránh sự suy giảm tỷ lệ chấp nhận quan sát thấy khi sử dụng lấy mẫu lại từ chối (Gante, 2023; Spector & Re, 2023). Tiêu chí chấp nhận điển hình chỉ định rằng một token suy đoán ˆxt+i được chấp nhận nếu:
pbase(ˆxt+i|x≤t,ˆxt+1, ..., ˆxt+i−1;τ)>min(ϵ,αexp(−H(pbase(·|x≤t,ˆxt+1, ..., ˆxt+i−1;τ))))

trong đó ϵ được gọi là ngưỡng posterior, α được gọi là alpha posterior, τ là nhiệt độ lấy mẫu, và H(·) là entropy. Cả ϵ và α đều là các siêu tham số cần được điều chỉnh. Để phân tích thêm về chấp nhận điển hình, chúng tôi giới thiệu độc giả đến Cai et al. (2024).

Thiết lập. Chúng tôi đánh giá cách các cài đặt khác nhau của ϵ và α ảnh hưởng đến cả độ dài chấp nhận và chất lượng tạo sinh. Theo Cai et al. (2024), chúng tôi đánh giá chấp nhận điển hình trên các danh mục "Viết" và "Đóng vai" của MT-Bench, và báo cáo điểm trung bình LLM-as-a-judge để định lượng chất lượng tạo sinh (Zheng et al., 2023). Chúng tôi cố định nhiệt độ lấy mẫu τ=0.7, thay đổi ngưỡng posterior ϵ∈ {0.05, 0.1, 0.15, 0.2, 0.25}, và đặt alpha posterior là α=√ϵ.

Kết quả. Chúng tôi vẽ cách thay đổi ngưỡng posterior ảnh hưởng đến cả độ dài suy đoán trung bình và chất lượng của các thế hệ kết quả trong Hình 4. Đối với Medusa, Hydra, và Hydra++, việc tăng ngưỡng posterior làm giảm nhẹ độ dài suy đoán trung bình, nhưng đối với tất cả các ngưỡng posterior được kiểm tra, Hydra và Hydra++ có độ dài chấp nhận trung bình cao hơn đáng kể so với Medusa. Trong khi cả Medusa và Hydra đều không thể đạt được chất lượng giống như lấy mẫu ngẫu nhiên từ mô hình cơ sở cho bất kỳ ngưỡng posterior nào được xem xét, đối với ϵ=0.15 Hydra++ đạt được chất lượng tạo sinh giống như lấy mẫu trực tiếp từ mô hình cơ sở. Các kết quả này chứng minh rằng chất lượng đầu được cải thiện đạt được từ Hydra++ là cần thiết để khớp với chất lượng tạo sinh của đường cơ sở cho suy luận không tham lam trong khi vẫn duy trì độ dài chấp nhận trung bình cao.

7 Công trình liên quan

Tăng tốc suy luận LLM là một lĩnh vực nghiên cứu tích cực. Kỹ thuật mà công trình của chúng tôi dựa trên, giải mã suy đoán, lần đầu được đề xuất bởi Leviathan et al. (2023) và Chen et al. (2023), và được dự đoán trong một dạng hạn chế bởi Stern et al. (2018). Công trình gần đây đã khám phá các lựa chọn thay thế cho các mô hình dự thảo tiêu chuẩn như sử dụng cơ chế truy xuất để đề xuất tiếp tục (He et al., 2023), và tái công thức hóa lấy mẫu mô hình ngôn ngữ theo phép lặp Jacobi (Fu et al., 2023). Một hướng khác của nghiên cứu giải mã suy đoán đã điều tra việc xác minh một cây các tiếp tục ứng cử viên thay vì một tiếp tục đơn (Miao et al., 2023; Spector & Re, 2023; Cai et al., 2024). Ngoài giải mã cây, Spector & Re (2023) cũng đề xuất mở rộng khung giải mã suy đoán cơ bản bằng cách xây dựng một hệ thống phân cấp các mô hình dự thảo, với mỗi cái hỗ trợ giải mã suy đoán cho cái tiếp theo. Các hướng nghiên cứu đương đại khác về giải mã suy đoán bao gồm huấn luyện trực tuyến mô hình dự thảo dựa trên truy vấn người dùng (Liu et al., 2023a) và căn chỉnh dựa trên chưng cất tri thức của mô hình dự thảo (Zhou et al., 2024). Chúng tôi cũng muốn thừa nhận công trình đồng thời EAGLE (Li et al., 2024) là công trình tương tự nhất với chúng tôi. Chúng tôi thảo luận EAGLE trong Phụ lục C. Chúng tôi cũng muốn thừa nhận Zhang et al. (2024); Wertheimer et al. (2024) người đã đồng thời điều tra các đầu dự thảo phụ thuộc tuần tự.

Một hướng khác để tăng tốc suy luận LLM là giảm thiểu tác động bộ nhớ của LLM. Một kỹ thuật phổ biến là nén LLM bằng cách lượng tử hóa trọng số hoặc cắt tỉa các đặc trưng của mô hình (Dettmers et al., 2022; Xiao et al., 2023; Frantar et al., 2023; Frantar & Alistarh, 2023; Liu et al., 2023b; Alizadeh et al., 2024; Sheng et al., 2023). Để giảm dung lượng bộ nhớ của KV-cache, Shazeer (2019) và Ainslie et al. (2023) giới thiệu attention đa truy vấn và attention truy vấn nhóm tương ứng. Các công trình này giảm kích thước của KV-cache bằng cách sử dụng ít đầu key và value hơn so với số đầu query trong attention. Một phương pháp khác để giảm dung lượng bộ nhớ của LLM là chưng cất tri thức, trong đó một mạng sinh viên nhỏ hơn được huấn luyện để chính xác như mô hình lớn hơn ban đầu (Sanh et al., 2020). Các kỹ thuật giảm bộ nhớ và tăng tốc suy luận này trực giao với, và có khả năng bổ sung cho, giải mã suy đoán.

Tăng kích thước batch mà tại đó suy luận được thực hiện là một kỹ thuật khác để cải thiện thông lượng suy luận LLM. Nhiều công trình điều tra lập lịch tốt hơn cho suy luận theo batch và quản lý tài nguyên chia sẻ được cải thiện trong suy luận theo batch (Yu et al., 2022; Kwon et al., 2023).

8 Kết luận

Trong công trình này, chúng tôi kiểm tra có hệ thống giải mã suy đoán dựa trên đầu dự thảo và đề xuất các phương pháp để cải thiện chất lượng suy đoán của các đầu dự thảo. Chúng tôi đưa ra quan sát rằng các đầu dự thảo được đề xuất trước đây độc lập tuần tự, dẫn đến chất lượng dự đoán kém. Để khắc phục vấn đề này, chúng tôi đề xuất các đầu Hydra: một sự thay thế phụ thuộc tuần tự có thể thay thế trực tiếp cho các đầu dự thảo tiêu chuẩn. Các đầu Hydra được tạo phụ thuộc tuần tự bằng cách nhận đầu vào là các embedding đầu vào của mô hình cơ sở của các token trong tiếp tục ứng cử viên. Thay đổi đơn giản này dẫn đến cải thiện đáng kể về tốc độ giải mã: giải mã Hydra đạt được cải thiện lên đến 1.11 × về thông lượng end-to-end so với giải mã Medusa. Chúng tôi cũng điều tra các mục tiêu huấn luyện và kiến trúc khác nhau cho các đầu Hydra, cuối cùng đề xuất một công thức đầu Hydra mà chúng tôi gọi là Hydra++ tăng thông lượng giải mã lên đến 1.31 × và 2.70 × so với Medusa và giải mã tự hồi quy tương ứng. Cuối cùng, chúng tôi chứng minh rằng Hydra++ tiếp tục mang lại lợi ích khi thực hiện suy luận theo batch, và bằng cách sử dụng lấy mẫu chấp nhận điển hình Hydra++ có thể đạt được chất lượng giống như lấy mẫu không tham lam của mô hình cơ sở mà không làm giảm độ dài tiếp tục được chấp nhận. Giải mã suy đoán dựa trên đầu dự thảo là một lựa chọn thay thế hiệu quả và đơn giản cho mô hình giải mã suy đoán tiêu chuẩn, và công trình của chúng tôi đi một bước quan trọng hướng tới tối đa hóa hiệu suất của giải mã với các đầu dự thảo thông qua việc xây dựng các đầu dự thảo phụ thuộc tuần tự.
