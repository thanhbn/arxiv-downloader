# TRIFORCE: Tăng tốc không mất mát trong sinh chuỗi dài với giải mã suy đoán phân cấp

Hanshi Sun1, Zhuoming Chen1, Xinyu Yang1, Yuandong Tian2, Beidi Chen1,2
1Carnegie Mellon University
2Meta AI (FAIR)
{hanshis, zhuominc, xinyuya2, beidic}@andrew.cmu.edu
yuandong@meta.com

## Tóm tắt

Với các mô hình ngôn ngữ lớn (LLM) được triển khai rộng rãi trong việc sinh nội dung dài gần đây, đã xuất hiện nhu cầu ngày càng tăng về hỗ trợ suy luận chuỗi dài hiệu quả. Tuy nhiên, cache key-value (KV), được lưu trữ để tránh tính toán lại, đã trở thành một nút thắt cổ chai quan trọng bằng cách tăng tuyến tính về kích thước theo độ dài chuỗi. Do tính chất tự hồi quy của LLM, toàn bộ cache KV sẽ được tải cho mỗi token được sinh ra, dẫn đến việc sử dụng thấp các lõi tính toán và độ trễ cao. Trong khi các phương pháp nén khác nhau cho cache KV đã được đề xuất để giảm thiểu vấn đề này, chúng gặp phải sự suy giảm chất lượng sinh. Chúng tôi giới thiệu TRIFORCE, một hệ thống giải mã suy đoán phân cấp có thể mở rộng cho việc sinh chuỗi dài. Phương pháp này tận dụng trọng số mô hình gốc và cache KV thưa thớt động qua truy xuất như một mô hình nháp, đóng vai trò như một lớp trung gian trong hệ thống phân cấp và được suy đoán thêm bởi một mô hình nhỏ hơn để giảm độ trễ soạn thảo của nó. TRIFORCE không chỉ tạo ra tăng tốc ấn tượng cho Llama2-7B-128K, đạt tới 2.31× trên GPU A100 mà còn thể hiện khả năng mở rộng trong việc xử lý các ngữ cảnh thậm chí dài hơn. Đối với cài đặt offloading trên hai GPU RTX 4090, TRIFORCE đạt 0.108s/token—chỉ chậm bằng một nửa so với baseline tự hồi quy trên A100, đạt 7.78× trên hệ thống offloading được tối ưu hóa của chúng tôi. Ngoài ra, TRIFORCE thực hiện nhanh gấp 4.86× so với DeepSpeed-Zero-Inference trên một GPU RTX 4090. Tính mạnh mẽ của TRIFORCE được nhấn mạnh bởi hiệu suất xuất sắc nhất quán của nó qua các mức nhiệt độ khác nhau. Mã nguồn có sẵn tại https://github.com/Infini-AI-Lab/TriForce.

## 1 Giới thiệu

Các mô hình ngôn ngữ lớn (LLM) với khả năng ngữ cảnh dài, như GPT-4 (Achiam et al., 2023), Gemini (Team et al., 2023), và LWM (Liu et al., 2024a) tiếp tục xuất hiện và có ứng dụng thành thạo trong các tình huống bao gồm chatbot, sinh hình ảnh và phân tích tài chính (Touvron et al., 2023; Chowdhery et al., 2023; Zhao et al., 2023; Reddy et al., 2024). Tuy nhiên, việc phục vụ các LLM này một cách hiệu quả mà không mất mát là thách thức. Do tính chất tự hồi quy của LLM, toàn bộ cache key-value (KV), lưu trữ các trạng thái key-value trung gian từ ngữ cảnh trước để tránh tính toán lại, cùng với tham số mô hình sẽ được tải vào SRAM GPU cho mỗi token được sinh ra, dẫn đến việc sử dụng thấp các lõi tính toán. Ngoài khối lượng lớn tham số mô hình, dung lượng bộ nhớ của cache KV, tăng tuyến tính theo độ dài chuỗi (Pope et al., 2023), đang trở thành nút thắt cổ chai mới cho việc sinh chuỗi dài.

Các phương pháp gần đây đã đề xuất các chiến lược loại bỏ cache KV (Xiao et al., 2023b; Zhang et al., 2024b; Liu et al., 2024c; Jiang et al., 2023; Ge et al., 2023) để giảm thiểu dung lượng bộ nhớ đáng kể của cache KV, có chọn lọc loại bỏ các cặp KV khỏi cache dựa trên chính sách loại bỏ được thiết kế, cho phép các mô hình sinh văn bản với ngân sách cache KV hạn chế. Tuy nhiên, xét rằng các cặp KV bị loại bỏ không thể khôi phục và khó khăn trong việc dự đoán chính xác cặp KV nào sẽ quan trọng cho việc sinh văn bản tương lai, chúng gặp khó khăn với việc mất thông tin tiềm tàng, bao gồm ảo giác và sự không nhất quán về ngữ cảnh (Yang et al., 2024), đặc biệt trong ngữ cảnh dài. Những thách thức như vậy ngăn cản các phương pháp này tăng tốc độ mà không hy sinh hiệu suất của mô hình, như được minh họa trong Hình 1.

Đồng thời, giải mã suy đoán, tận dụng mô hình nháp nhẹ để dự đoán tuần tự vài token tiếp theo và để mô hình đích xác minh các token được dự đoán song song, được giới thiệu để tăng tốc suy luận LLM trong khi bảo toàn chính xác đầu ra mô hình một cách có thể chứng minh (Leviathan et al., 2023; Chen et al., 2023a; Xia et al., 2024). Tuy nhiên, triển khai nó cho việc sinh chuỗi dài đối mặt với một số thách thức. Thứ nhất, việc đào tạo mô hình nháp để phù hợp với độ dài ngữ cảnh của LLM đích đòi hỏi tính toán khổng lồ và vẫn còn nghi vấn liệu các mô hình nhỏ này có thể đạt độ chính xác tương tự với độ dài ngữ cảnh khoảng 1M (Beltagy et al., 2020; Peng et al., 2023; Yan et al., 2024). Thứ hai, chúng tôi phát hiện rằng mô hình nháp với các phương pháp không cần đào tạo hiện có (ví dụ: chiến lược loại bỏ cache KV) có thể dẫn đến hiệu suất suy đoán kém. Một sự phân kỳ tăng liên tục (Leviathan et al., 2023) được chứng kiến khi độ dài chuỗi tăng, như được thể hiện trong Hình 2a.

Trong việc theo đuổi tăng tốc không mất mát, chúng tôi sử dụng tính năng không mất mát của giải mã suy đoán làm nền tảng của hệ thống. Một thuật toán giải mã suy đoán lý tưởng nên (i) không cần đào tạo, (ii) duy trì tỷ lệ chấp nhận cao (Leviathan et al., 2023) với ngữ cảnh dài, và (iii) có chi phí soạn thảo thấp. Tuy nhiên, hai thách thức kỹ thuật cần được giải quyết để đạt được mục tiêu. Thứ nhất, không rõ ràng ngay lập tức chúng ta có thể sử dụng gì cho việc soạn thảo độ trễ thấp mà không đào tạo mô hình nháp nhỏ hơn để phù hợp với độ dài ngữ cảnh dài. Thứ hai, các yếu tố chính để đạt tỷ lệ chấp nhận cao với ngữ cảnh dài vẫn chưa rõ ràng.

May mắn thay, dựa trên khám phá sơ bộ của chúng tôi, ba quan sát quan trọng mở đường cho việc thiết kế hệ thống có thể áp dụng để phục vụ LLM với ngữ cảnh dài.

Suy đoán phân cấp cho nút thắt cổ chai bộ nhớ kép: Như được minh họa trong Hình 2b và 2c, chúng tôi nhận ra hai nút thắt cổ chai bộ nhớ: trọng số mô hình và cache KV, và cái sau dần trở thành nút thắt cổ chai chủ đạo khi độ dài ngữ cảnh tăng. Điều này truyền cảm hứng cho chúng tôi áp dụng suy đoán phân cấp để giải quyết hai nút thắt cổ chai tuần tự bằng các mô hình nháp khác nhau.

Tận dụng sự thưa thớt attention cho giải mã suy đoán: Chúng tôi xác định sự dư thừa đáng kể trong cache KV, phát hiện rằng một phần tương đối nhỏ của nó đủ để đạt tỷ lệ chấp nhận cao bằng cách sử dụng cache KV một phần như cache nháp cho tự suy đoán.

Khai thác tính địa phương ngữ cảnh cho hiệu quả soạn thảo: Chúng tôi khám phá rằng thông tin từ token ngữ cảnh dài cần thiết bởi các token liền kề có xu hướng tương tự. Quan sát này gợi ý rằng một phân đoạn cụ thể của cache có thể được tái sử dụng hiệu quả qua nhiều bước giải mã, phân bổ chi phí xây dựng cache nháp và nâng cao hiệu quả soạn thảo.

Dựa trên những hiểu biết này, chúng tôi giới thiệu phương pháp suy đoán phân cấp. Đối với mô hình đích ngữ cảnh dài (ví dụ: Llama2-7B-128K (Peng et al., 2023)), chúng tôi tận dụng trọng số mô hình gốc nhưng chỉ với một tỷ lệ nhỏ (ví dụ: 3%) cache KV như một bản nháp để giải quyết nút thắt cổ chai của cache KV. Phân cấp, mô hình nháp được suy đoán thêm bởi một mô hình nhẹ (ví dụ: Llama-68M) với cache StreamingLLM để giải quyết nút thắt cổ chai của trọng số mô hình. Chúng tôi trình bày TRIFORCE, được miêu tả trong Hình 1, một hệ thống giải mã suy đoán có thể mở rộng và mạnh mẽ tích hợp soạn thảo dựa trên truy xuất và suy đoán phân cấp, được tối ưu hóa cho cả tình huống trên chip và offloading. Cụ thể,

• Trong Phần 4.1, bằng cách duy trì cache đầy đủ, chúng tôi có được sự linh hoạt để chọn cặp KV, cho phép chúng tôi thiết kế phương pháp chọn lựa ưu việt, được gọi là soạn thảo dựa trên truy xuất. Chiến lược này truy xuất thông tin ngữ cảnh cần thiết cho nhu cầu tương lai, được đặc trưng là không mất mát, đặc biệt so với các phương pháp dựa trên loại bỏ như StreamingLLM và H2O. Chúng tôi tiếp tục chứng minh tính hiệu quả và mạnh mẽ của nó trên các tập dữ liệu khác nhau.

• Trong Phần 4.2, chúng tôi đề xuất một hệ thống phân cấp để giải quyết nút thắt cổ chai bộ nhớ kép. Sử dụng mô hình nhẹ được ghép với cache StreamingLLM cho suy đoán ban đầu, chúng tôi có thể giảm độ trễ soạn thảo cho giai đoạn suy đoán tiếp theo, do đó tăng tốc suy luận đầu cuối.

Theo kinh nghiệm, trong Phần 5, chúng tôi thực hiện các thí nghiệm và nghiên cứu khử bỏ mở rộng để chứng minh hiệu quả của TRIFORCE. Chúng tôi cho thấy TRIFORCE đạt tới 2.31× tăng tốc cho Llama2-7B-128K trên một GPU A100 duy nhất trên Hugging Face (Wolf et al., 2019) với đồ thị CUDA (NVIDIA & Fitzek, 2020). Đối với cài đặt offloading, TRIFORCE đạt 7.78× ấn tượng trên hai GPU RTX 4090, đạt 0.108 s/token—chỉ chậm bằng một nửa so với baseline tự hồi quy trên A100. TRIFORCE có thể phục vụ hiệu quả Llama2-13B với ngữ cảnh 128K với 0.226s/token, nhanh gấp 7.94× trên hệ thống offloading được tối ưu hóa của chúng tôi. Trên một GPU RTX 4090 duy nhất, TRIFORCE nhanh gấp 4.86× so với DeepSpeed-Zero-Inference (Aminabadi et al., 2022). Hơn nữa, chúng tôi cho thấy: (i) TRIFORCE có giới hạn trên lý thuyết 13.1×, thể hiện khả năng mở rộng đặc biệt khi xử lý ngữ cảnh dài; (ii) TRIFORCE mạnh mẽ qua các cài đặt nhiệt độ khác nhau, duy trì tỷ lệ chấp nhận trên 0.9 ngay cả khi nhiệt độ được đặt là 1.0; và (iii) khả năng của TRIFORCE xử lý hiệu quả các batch lớn, đạt 1.9× tăng tốc cho kích thước batch sáu với ngữ cảnh 19K mỗi mẫu.

## 2 Nền tảng

### 2.1 Giải mã suy đoán

Giải mã suy đoán (Stern et al., 2018; Leviathan et al., 2023; Chen et al., 2023a; Kim et al., 2024; Zhang et al., 2023; Santilli et al., 2023; Hooper et al., 2023) được đặc trưng bởi việc tăng tốc giải mã LLM trong khi duy trì chính xác phân phối đầu ra của mô hình. Vì tốc độ của quá trình giải mã tự hồi quy chủ yếu bị giới hạn bởi thời gian tải trọng số mô hình và cache KV vào SRAM GPU, giải mã suy đoán tận dụng quan sát rằng việc sinh một token mất thời gian tương tự như xử lý hàng chục token song song. Các phương pháp suy đoán dựa trên cây được đề xuất để tận dụng đầy đủ ngân sách suy đoán (Fu et al., 2024; Li et al., 2024). Thay vì đưa ra một dự đoán cho token tiếp theo, các phương pháp dựa trên cây tận dụng nhiều ứng viên để tăng tỷ lệ chấp nhận để nhiều token hơn có thể được chấp nhận (Miao et al., 2023; Sun et al., 2024; Chen et al., 2024). Các kỹ thuật suy đoán theo giai đoạn (Spector & Re, 2023; Chen et al., 2023b) đã được đề xuất để tăng tốc suy luận bằng cách sử dụng tầng thác mô hình nháp, và suy đoán phân cấp có điểm tương đồng với những phương pháp này. Tuy nhiên, phương pháp của chúng tôi tập trung vào nhiệm vụ sinh chuỗi dài, đặt ra những thách thức độc đáo. Chúng tôi sử dụng các phương pháp soạn thảo khác nhau cho mỗi giai đoạn suy đoán để giải quyết hai nút thắt cổ chai riêng biệt trong tình huống ngữ cảnh dài, thay vì tập trung vào trọng số mô hình ở mọi giai đoạn để tăng tốc. Trong khi đó, các phương pháp tự suy đoán như Medusa (Cai et al., 2024; Ankner et al., 2024), trực giao với phương pháp của chúng tôi, đòi hỏi nỗ lực đào tạo và có thể được tích hợp vào mô hình nháp trung gian của chúng tôi.

### 2.2 Chiến lược loại bỏ cache KV

StreamingLLM (Xiao et al., 2023b) giải quyết những hạn chế của window attention và sliding window với tính toán lại bằng cách trình bày một phương pháp đơn giản nhưng hiệu quả cho phép LLM xử lý chuỗi văn bản dài vô hạn mà không cần fine-tuning. StreamingLLM ổn định hiệu suất bằng cách giữ lại các token sink attention quan trọng cùng với KV gần đây để tính toán attention. Bằng cách ưu tiên token sink, StreamingLLM đảm bảo phân phối điểm attention ổn định, thúc đẩy mô hình ngôn ngữ nhất quán cho văn bản dài.

H2O (Zhang et al., 2024b) giới thiệu phương pháp tham lam nhưng chi phí thấp để xử lý luồng đầu vào độ dài vô hạn, được lấy cảm hứng từ phiên bản đơn giản hóa của chính sách loại bỏ heavy-hitters (H2). Phương pháp này cập nhật động cache KV dựa trên điểm attention tích lũy, loại bỏ có hệ thống KV ít quan trọng nhất để duy trì kích thước cache cố định. Bằng cách tận dụng thuật toán tham lam dựa trên thống kê địa phương, H2O hiệu quả chọn cặp KV nào để bảo toàn trong cache, đảm bảo suy luận hiệu quả mà không làm giảm chất lượng.

Tuy nhiên, quan trọng là nhận ra rằng những kỹ thuật này không tăng kích thước cửa sổ ngữ cảnh (Zhang et al., 2024a; Jin et al., 2024; Ge et al., 2023; Jiang et al., 2023). Chúng tập trung vào việc chỉ giữ lại các token gần đây nhất cùng với attention sink hoặc heavy-hitter, trong khi loại bỏ các token khác. Những phương pháp này giới hạn mô hình xử lý dựa trên chính sách loại bỏ được thiết kế và token gần đây. Do đó, chúng có thể không áp dụng trực tiếp cho các nhiệm vụ đòi hỏi hiểu biết ngữ cảnh dài toàn diện.

### 2.3 Lượng tử hóa cache KV

Một số phương pháp lượng tử hóa cache KV đã được giới thiệu để nâng cao hiệu quả suy luận cho việc sinh chuỗi dài, nhằm duy trì chất lượng sinh trong khi giảm tiêu thụ bộ nhớ (Xiao et al., 2023a; Hooper et al., 2024; Sheng et al., 2023; Liu et al., 2023a; Zirui Liu et al., 2023; Yue et al., 2024). Các phương pháp lượng tử hóa tập trung vào nén độ rộng bit của kích hoạt cache KV, trực giao với phương pháp của chúng tôi.

## 3 Quan sát

Thiết kế TRIFORCE của chúng tôi được truyền cảm hứng bởi hai quan sát thực nghiệm quan trọng về LLM khi xử lý ngữ cảnh dài, được chi tiết như sau.

### 3.1 Tận dụng sự thưa thớt attention cho giải mã suy đoán

**Quan sát**: Hiện tượng thưa thớt attention trong LLM được huấn luyện trước đã được khám phá bởi nhiều nghiên cứu (Zhang et al., 2024b; Xiao et al., 2023b; Liu et al., 2023b; 2024c). Trong nghiên cứu của chúng tôi, chúng tôi thực hiện suy luận zero-shot trên tập test PG-19 (Rae et al., 2019) với mô hình Llama2-7B-128K. Bằng cách trực quan hóa sự thưa thớt qua các attention head khác nhau, được thể hiện trong Hình 3a, chúng tôi quan sát rằng với độ dài ngữ cảnh 120K, có thể khôi phục hơn 96% điểm attention chỉ với 4K token qua hầu hết các layer.

**Phân tích**: Sự hiện diện của thưa thớt trong các block attention gợi ý rằng một phần của cache KV có thể đóng vai trò cache nháp để đạt tỷ lệ chấp nhận cao trong giải mã tự suy đoán. Vì cache KV là nút thắt cổ chai trong cài đặt này, chúng ta có thể tải toàn bộ trọng số mô hình với cache KV một phần như mô hình nháp. Hình 3b chứng minh rằng chỉ sử dụng 1K token có thể lý thuyết đạt 97.6% tỷ lệ chấp nhận với phương pháp chọn Top-K. Trong khi tình huống này đại diện cho giới hạn trên lý thuyết tối ưu, các triển khai thực tế như H2O và StreamingLLM thể hiện kết quả đầy hứa hẹn, đạt hơn 90.5% tỷ lệ chấp nhận với ngân sách cache KV 1K. Cần lưu ý rằng chúng tôi duy trì cache đầy đủ cho hai layer đầu tiên để minh họa, trong khi không layer nào bị bỏ qua trong triển khai hệ thống thực tế để hiệu quả.

### 3.2 Khai thác tính địa phương ngữ cảnh cho hiệu quả soạn thảo

**Quan sát**: Khám phá của chúng tôi tiết lộ rằng thông tin từ token ngữ cảnh dài cần thiết bởi các token liền kề có xu hướng tương tự. Trong thí nghiệm, với độ dài ngữ cảnh được thiết lập ở 120K, chúng tôi hướng dẫn mô hình sinh 256 token. Bằng cách chọn các chỉ số top-4K theo điểm attention của token được prefill cuối cùng, chúng tôi sử dụng các chỉ số này để thu thập điểm attention cho các token được sinh tiếp theo và đánh giá tỷ lệ khôi phục điểm cho 120K token được prefill ban đầu. Như được thể hiện trong Hình 3c, nó dẫn đến việc khôi phục cao qua hầu hết các layer và xu hướng giảm chậm khi số lượng token tăng.

**Hiểu biết**: Quan sát này cho phép xây dựng cache một lần để đủ cho nhiều bước giải mã, do đó phân bổ độ trễ xây dựng cache nháp và tăng hiệu quả. Khi cache KV mới được giới thiệu, được hướng dẫn bởi hiểu biết rằng từ gần đây có mối tương quan mạnh hơn với token hiện đang được giải mã, các mục này sẽ thay thế những cái ít quan trọng hơn. Các hoạt động xây dựng lại cache có thể được lên lịch định kỳ hoặc thích ứng để đáp ứng với sự giảm tỷ lệ chấp nhận, đảm bảo cache vẫn được căn chỉnh động với ngữ cảnh đang phát triển. Đáng chú ý, cả StreamingLLM và H2O đều kết hợp nguyên tắc này một cách ngầm định. H2O liên tục giữ lại token với điểm cao, và StreamingLLM tái sử dụng thông tin địa phương rộng rãi và token sink, cả hai đều giảm nhu cầu xây dựng lại cache hoàn toàn.

## 4 TRIFORCE

Phần này nhằm giới thiệu TRIFORCE, tận dụng chính sách chọn cache KV dựa trên truy xuất và hệ thống suy đoán phân cấp. Chúng tôi đầu tiên lập luận rằng phương pháp soạn thảo dựa trên truy xuất của chúng tôi trực quan và không mất mát so với các chiến lược hiện có như StreamingLLM và H2O. Tiếp theo, chúng tôi giới thiệu hệ thống phân cấp được thiết kế để hiệu quả giải quyết nút thắt cổ chai kép trong giải mã suy đoán, tạo điều kiện cải thiện đáng kể tốc độ tổng thể. Cuối cùng, TRIFORCE được trình bày chi tiết trong Phần 4.3.

### 4.1 Soạn thảo dựa trên truy xuất

Trong các tình huống đòi hỏi phụ thuộc ngữ cảnh dài hạn, các phương pháp như StreamingLLM và H2O hoạt động kém do chiến lược cập nhật cache của chúng, không hiệu quả trong việc truy xuất chính xác thông tin ngữ cảnh chi tiết vì chúng không thể tránh và không thể khôi phục việc loại bỏ cặp KV. Trong thí nghiệm của chúng tôi, chúng tôi thách thức StreamingLLM và H2O với nhiệm vụ truy xuất needle (Liu et al., 2024b; Peng et al., 2023; Liu et al., 2024a). Như được chi tiết trong Bảng 1, có sự giảm đáng chú ý trong tỷ lệ chấp nhận của chúng so với hiệu suất trên tập dữ liệu PG-19, làm nổi bật những hạn chế của chúng. Về cơ bản, StreamingLLM và H2O hoạt động theo nguyên tắc có mất mát, vì token bị loại bỏ bị loại bỏ vĩnh viễn, khiến chúng không phù hợp cho cài đặt đòi hỏi bảo toàn cache KV đầy đủ cho mô hình đích.

Nhu cầu giữ toàn bộ cache KV trong cài đặt của chúng tôi cho phép chúng tôi chọn cache KV tự do hơn (Singhal et al., 2001). Hiểu biết này dẫn chúng tôi phát triển chính sách chọn hiệu quả hơn cho xấp xỉ không mất mát. Trong phương pháp của chúng tôi, được thể hiện trong Hình 4, cache KV được phân đoạn thành các chunk nhỏ. Trong giai đoạn truy xuất, chúng tôi tính toán attention giữa truy vấn đã cho và cache key trung bình trong mỗi chunk. Phương pháp này hiệu quả làm nổi bật các chunk liên quan nhất, cho phép chúng tôi thu thập cache KV với ngân sách cố định dựa trên điểm. Như được minh họa trong Bảng 1, phương pháp dựa trên truy xuất xuất sắc bằng cách chủ động xác định thông tin quan trọng nhất cho nhiệm vụ thay vì dựa vào các phương pháp quản lý cache thụ động và dựa trên thời gian. Bằng cách tập trung vào tính liên quan hơn là tính gần đây, chính sách dựa trên truy xuất chứng minh tiềm năng xử lý tập dữ liệu dày đặc ngữ cảnh.

### 4.2 Suy đoán phân cấp

Trong khi giải quyết nút thắt cổ chai cache KV nâng cao hiệu quả, yêu cầu tải toàn bộ trọng số mô hình để soạn thảo lại giới thiệu độ trễ, chuyển nút thắt cổ chai trở lại trọng số mô hình. Để giải quyết thách thức này, chúng tôi triển khai hệ thống phân cấp, như được minh họa trong Hình 1. Hệ thống này sử dụng mô hình nhẹ thứ hai với cache StreamingLLM để thực hiện suy đoán ban đầu cho mô hình đích với cache dựa trên truy xuất (đóng vai trò mô hình nháp cho mô hình đích với cache KV đầy đủ). Bằng cách thiết lập hệ thống phân cấp suy đoán tuần tự này, chúng tôi hiệu quả giảm độ trễ soạn thảo và tăng tốc suy luận tổng thể.

**Tính đúng đắn**: Phân phối đầu ra gốc được bảo toàn trong giai đoạn suy đoán cuối cùng, giống hệt với thuật toán giải mã suy đoán tiêu chuẩn (Leviathan et al., 2023; Chen et al., 2023a), và chứng minh là tầm thường.

### 4.3 Thuật toán

TRIFORCE được thiết kế để khai thác các nút thắt cổ chai liên quan đến cả trọng số mô hình và cache KV để nâng cao tốc độ suy luận của LLM cho việc sinh chuỗi dài. Chúng tôi trình bày mã giả cho TRIFORCE trong Thuật toán 1. Nó bắt đầu bằng cách prefill mô hình đích Mp với cache đầy đủ Cp và mô hình nháp Mq với cache StreamingLLM Cq sử dụng tiền tố đầu vào đã cho, và sau đó xây dựng cache truy xuất Cr. Cơ chế khởi tạo và cập nhật cho cache truy xuất Cr được hướng dẫn bởi hiểu biết về tính địa phương ngữ cảnh được thảo luận trong Phần 3.2. Chúng tôi đầu tiên xây dựng Cr sử dụng token cuối cùng của tiền tố, sắp xếp token theo thứ tự giảm dần của tầm quan trọng. Trong các suy luận tiếp theo, chúng tôi ghi đè token với tầm quan trọng ít nhất, duy trì tính liên quan và tiện ích của cache. Việc xây dựng lại Cr được kích hoạt khi tỷ lệ chấp nhận trung bình rolling giảm dưới ngưỡng hoặc ở stride được thiết kế.

Suy luận tiến triển lặp đi lặp lại cho đến khi đạt độ dài chuỗi đích T. Sau mỗi lần lặp, cache Cr và Cq được cập nhật để chuẩn bị cho giai đoạn suy đoán tiếp theo.

Mỗi lần lặp bao gồm hai suy đoán: ban đầu, Mq sử dụng Cq để dự đoán Mp với Cr cho γ1 bước cho đến khi n ≥ γ2. Tiếp theo, n token này được tự xác minh (Zhang et al., 2023) bởi Mp với Cp. Quá trình này xây dựng hệ thống phân cấp: lớp đầu tiên của hệ thống phân cấp sử dụng mô hình nhỏ hơn, nhanh hơn Mq với ngữ cảnh địa phương Cq để suy đoán mô hình lớn Mp với ngữ cảnh toàn cục một phần nhưng chất lượng cao Cr, giải quyết nút thắt cổ chai trọng số mô hình. Lớp thứ hai sử dụng mô hình Mp với cache truy xuất cho tự suy đoán, vượt qua nút thắt cổ chai do cache KV gây ra. Thuật toán suy đoán phân cấp này tăng hiệu quả bằng cách hiệu quả giải quyết cả hai nút thắt cổ chai. Triển khai hệ thống được chi tiết trong Phụ lục A.

## 5 Đánh giá thực nghiệm

Trong phần này, mục tiêu của chúng tôi là thể hiện khả năng của TRIFORCE, một thuật toán giải mã suy đoán có thể mở rộng và mạnh mẽ được thiết kế để đẩy nhanh suy luận của LLM cho việc sinh chuỗi dài, giảm đáng kể thời gian wall-clock. Chúng tôi đầu tiên trình bày hệ thống đầu cuối của mình, nổi bật tổng thể tăng tốc đạt được, bao gồm cả cài đặt trên chip và offloading, sau đó so sánh với các phương pháp khác và thí nghiệm khử bỏ.

### 5.1 Kết quả đầu cuối

Chúng tôi chứng minh rằng TRIFORCE tăng tốc sinh chuỗi dài, tới 2.31× trên A100 trong cài đặt trên chip và 7.78× trên hai RTX 4090 với offloading cho Llama2-7B-128K.

**Thiết lập**: Thí nghiệm của chúng tôi dựa trên mô hình Llama2 và LWM với kích thước cửa sổ ngữ cảnh 128K (Touvron et al., 2023; Liu et al., 2024a; Peng et al., 2023), đóng vai trò mô hình đích. Trong thiết lập này, chúng tôi sử dụng cache truy xuất 4K như cache nháp trung gian trong hệ thống phân cấp, trong khi tận dụng mô hình JackFram/Llama68M (JF68M) (Miao et al., 2023) như mô hình nháp ban đầu. Đối với thí nghiệm liên quan đến offloading, chúng tôi nhằm tối đa hóa sử dụng bộ nhớ bằng cách lấp đầy nó càng nhiều càng tốt và offload cache KV còn lại sang CPU (AMD EPYC 9754 @ 2.25 GHz), trong khi giữ trọng số mô hình trên GPU.

Đánh giá của chúng tôi được thực hiện trên tập dữ liệu PG-19 (Rae et al., 2019) và NarrativeQA (Kočiský et al., 2018), mỗi tập kiểm tra 100 ví dụ, được cấu hình với độ dài prompt 122K cho cài đặt trên chip và 127K cho cài đặt offloading, và nhằm sinh 256 token. Hiệu suất của TRIFORCE được phân tích qua các cấu hình phần cứng khác nhau, bao gồm thí nghiệm trên chip trên A100, và thí nghiệm offloading trên GPU RTX 4090.

**Chính sách ngây thơ**: Vì khó đào tạo mô hình nháp với ngữ cảnh dài, chúng tôi xem xét JF68M với cache StreamingLLM như phương pháp chính sách ngây thơ, và ngân sách của nó được đặt là 1K. Ngoài ra, chúng tôi thí nghiệm với các nhiệt độ khác nhau để kiểm tra tính mạnh mẽ của nó.

**Kết quả chính**: Chúng tôi đánh giá TRIFORCE sử dụng nhiệt độ khác nhau, như được mô tả trong Bảng 2. Chúng tôi quan sát rằng TRIFORCE đạt tới 2.31× tăng tốc cho cài đặt trên chip với ngân sách cache KV tối thiểu 4K cho Llama2-7B-128K. Đối với cài đặt offloading, chúng tôi cung cấp kết quả đầu cuối trên GPU tiêu dùng cho nhiều mô hình hơn, bao gồm Llama2-7B-128K, Llama2-13B-128K, và LWM-Text-Chat-128K. Đáng chú ý, trong Bảng 3 chúng tôi chứng minh rằng TRIFORCE có thể phục vụ hiệu quả Llama2-13B với ngữ cảnh 128K trên hai RTX 4090, đạt thời gian trung bình giữa các token thấp tới 0.226 giây, nhanh gấp 7.94× so với hệ thống offloading được tối ưu hóa cao. Hơn nữa, với TRIFORCE, Llama2-7B-128K có thể được phục vụ với 0.108s/token—chỉ chậm bằng một nửa so với baseline tự hồi quy trên A100. Chúng tôi cũng minh họa cách TRIFORCE tăng hiệu quả suy luận batch, cài đặt được sử dụng thường xuyên hơn trong phục vụ mô hình thực tế. TRIFORCE đạt 1.9× cho kích thước batch sáu, với mỗi mẫu trong batch có ngữ cảnh 19K, được thể hiện trong Bảng 4.

**Phân tích**: (1) Hiệu quả: Tích hợp hệ thống phân cấp của TRIFORCE nâng cao đáng kể tăng tốc, với TRIFORCE thể hiện cải thiện rõ rệt so với cả phương pháp StreamingLLM với suy đoán phân cấp và phương pháp truy xuất không có hệ thống phân cấp. (2) Khả năng mở rộng: Như được mô tả trong Hình 5, TRIFORCE thể hiện khả năng mở rộng xuất sắc với độ dài ngữ cảnh dài hơn. Khả năng mở rộng này được quy cho tỷ lệ chấp nhận cao và khoảng cách ngày càng tăng giữa độ trễ của mô hình nháp và mô hình đích. Lý thuyết, TRIFORCE có thể đạt tăng tốc tới 13.1×, cao gấp 7 lần so với chính sách ngây thơ, nhấn mạnh tiềm năng mở rộng đáng kể của nó. (3) Tính mạnh mẽ: Không giống các phương pháp giải mã suy đoán vanilla, TRIFORCE duy trì hiệu suất tương đối nhất quán qua các cài đặt nhiệt độ khác nhau. Nó thể hiện ít nhạy cảm nhiệt độ hơn, duy trì tỷ lệ chấp nhận trên 0.9 ngay cả khi nhiệt độ được đặt là 1.0, làm nổi bật tính ổn định và độ tin cậy của nó.

### 5.2 So sánh với các phương pháp khác

Chúng tôi cung cấp so sánh với REST (He et al., 2023) và Skipping Layers (Zhang et al., 2023). Bảng 5 so sánh TRIFORCE, REST, và Skipping Layers với Llama2-7B-128K trên A100 sử dụng tập dữ liệu PG-19, cho thấy TRIFORCE đạt tăng tốc tốt nhất cho sinh chuỗi dài. TRIFORCE truy xuất thông tin từ cache KV, cho phép thích ứng động với ngữ cảnh, trong khi REST sử dụng datastore được định nghĩa trước bên ngoài. Trong Bảng 5, Skipping Layers sử dụng 68% cache KV, trong khi TRIFORCE hiệu quả chỉ sử dụng 3%, giải quyết nút thắt cổ chai trong tình huống ngữ cảnh dài tốt hơn.

### 5.3 Kết quả khử bỏ

Chúng tôi trình bày các nghiên cứu khử bỏ mở rộng của TRIFORCE, tập trung vào ba điểm chính: (1) ảnh hưởng của các ngân sách cache KV khác nhau, (2) tác động của việc chọn kích thước chunk, và (3) tính tương thích của TRIFORCE với giải mã suy đoán dựa trên cây.

#### 5.3.1 Ngân sách cache KV

Như được minh họa trong Hình 6a, đối với Llama2-7B-128K, tỷ lệ chấp nhận tăng theo ngân sách cache lên đến 4K, sau đó ổn định về 1.0. Điều này gợi ý rằng việc tăng kích thước cache vượt quá 4K mang lại lợi ích giảm dần do độ trễ soạn thảo. Do đó, ngân sách cache KV 4K là tối ưu cho TRIFORCE, cân bằng tỷ lệ chấp nhận cao và chi phí soạn thảo tối thiểu.

#### 5.3.2 Kích thước chunk cache KV

Vì chúng tôi sử dụng tính địa phương ngữ cảnh để tái sử dụng cache truy xuất, chúng tôi cần kiểm tra tác động của kích thước chunk cache KV lên hiệu suất. Hình 6b cho thấy chunk nhỏ hơn có thể overfit với token đơn lẻ, hạn chế khái quát, trong khi chunk lớn hơn có thể pha loãng token điểm cao với token điểm thấp, dẫn đến giảm phân biệt giữa các chunk. Chunk lớn cũng giảm tính linh hoạt chọn lựa, hạn chế đa dạng trong ngân sách cache cố định.

#### 5.3.3 Tính tương thích với giải mã suy đoán dựa trên cây

Chúng tôi khám phá khả năng tích hợp TRIFORCE với giải mã suy đoán dựa trên cây. Cụ thể, đối với Llama2-7B-128K trên A100, chúng tôi ước tính số lượng token được sinh lý thuyết khi TRIFORCE được kết hợp với cấu trúc cây, bao gồm Sequoia (Chen et al., 2024) và Independent Sequences. Như được mô tả trong Hình 6c, tích hợp này có thể cải thiện tăng tốc đầu cuối bằng cách sử dụng ngân sách suy đoán bổ sung.

## 6 Kết luận

Trong công trình này, chúng tôi đã giới thiệu TRIFORCE, một hệ thống giải mã suy đoán phân cấp nhằm nâng cao đáng kể hiệu quả phục vụ LLM với ngữ cảnh dài. Tận dụng hiểu biết từ thưa thớt attention và tính địa phương ngữ cảnh, TRIFORCE giảm thiểu nút thắt cổ chai kép liên quan đến cache KV và trọng số mô hình. Các thí nghiệm thực nghiệm của chúng tôi chứng minh hiệu suất đáng chú ý của TRIFORCE, bao gồm tăng tốc đáng kể tới 2.31× trên A100 và 7.78× trên hai RTX 4090 với offloading, đạt 0.108s/token—chỉ chậm bằng một nửa so với baseline tự hồi quy trên A100. Những thành tựu này minh họa tiềm năng của TRIFORCE cách mạng hóa việc phục vụ mô hình ngữ cảnh dài cho sinh chuỗi dài.
