# 2312.11462.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/speculative/2312.11462.pdf
# Kích thước tệp: 490267 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Cascade Speculative Drafting cho Suy luận LLM Nhanh hơn nữa
Ziyi Chen1Xiaocong Yang1Jiacheng Lin1Chenkai Sun1Kevin Chen-Chuan Chang1Jie Huang1
Tóm tắt
Được giới thiệu để nâng cao hiệu quả của suy luận mô hình ngôn ngữ lớn (LLM), giải mã suy đoán hoạt động bằng cách có một mô hình nhỏ hơn tạo ra một bản thảo. Một mô hình đích lớn hơn sau đó xem xét bản thảo này để căn chỉnh với đầu ra của nó, và bất kỳ sự chấp nhận nào bởi mô hình đích dẫn đến việc giảm số lần chạy mô hình đích, cuối cùng cải thiện hiệu quả. Tuy nhiên, quá trình soạn thảo trong giải mã suy đoán bao gồm việc tạo tự hồi quy chậm và phân bổ thời gian bằng nhau để tạo token, bất kể tầm quan trọng của chúng. Những thiếu hiệu quả này cùng nhau góp phần vào hiệu suất dưới mức tối ưu của giải mã suy đoán.
Để cải thiện thêm suy luận LLM, chúng tôi giới thiệu Cascade Speculative Drafting (CS Drafting), một thuật toán thực thi suy đoán kết hợp hai loại tầng. Tầng Dọc loại bỏ việc tạo tự hồi quy từ các mô hình thần kinh, trong khi Tầng Ngang tối ưu hóa phân bổ thời gian trong soạn thảo để cải thiện hiệu quả. Kết hợp cả hai tầng, CS Drafting đạt được tối đa 81 phần trăm tăng tốc bổ sung so với giải mã suy đoán trong các thí nghiệm của chúng tôi, trong khi duy trì cùng phân phối đầu ra như mô hình đích. Mã của chúng tôi có sẵn công khai tại https://github.com/lfsszd/CS-Drafting.
1. Giới thiệu
Sự ra đời của các Mô hình Ngôn ngữ Lớn (LLM), như GPT-4 (OpenAI, 2023), đã đánh dấu một cột mốc quan trọng trong lĩnh vực xử lý ngôn ngữ tự nhiên (NLP). Những mô hình này không chỉ xuất sắc trong các nhiệm vụ NLP khác nhau mà còn tìm thấy các ứng dụng rộng rãi trong các cài đặt tương tác người dùng, chẳng hạn như chatbot và trợ lý ảo. Tuy nhiên, những ứng dụng này liên quan đến số lượng người dùng cực kỳ cao, lên đến hàng trăm triệu mỗi ngày. Để phục vụ theo thời gian thực ở quy mô này,
1Khoa Khoa học Máy tính, Đại học Illinois tại Urbana-Champaign. Liên hệ: Ziyi Chen <ziyic2@illinois.edu >, Kevin Chen-Chuan Chang <kc-chang@illinois.edu >, Jie Huang <jeffhj@illinois.edu >.
Preprint.một hệ thống độ trễ thấp không chỉ tiết kiệm chi phí mà còn quan trọng để duy trì dịch vụ hoạt động. Ngoài ra, quy mô khổng lồ của dịch vụ có nghĩa là ngay cả một cải thiện nhỏ trong độ trễ của LLM cũng có thể đóng góp rất lớn cho cả nhà cung cấp dịch vụ và cộng đồng. Do đó, tối ưu hóa độ trễ của LLM đã trở thành một lĩnh vực nghiên cứu quan trọng.
Thật không may, kích thước ngày càng tăng của LLM làm tăng đáng kể độ trễ, đặc biệt trong việc tạo dạng dài, vì LLM tự hồi quy tạo token từng cái một. Một giải pháp đang nổi lên, được gọi là giải mã suy đoán (Leviathan et al., 2023; Chen et al., 2023; Xia et al., 2023), cho thấy tiềm năng giảm thiểu vấn đề này. Trong giải mã suy đoán, một mô hình thảo (nhỏ hơn và nhanh hơn) tạo ra k token trong mỗi bước (với k là một siêu tham số) một cách tự hồi quy, và những token này sau đó được xem xét bởi một mô hình đích (lớn hơn và chậm hơn) song song. Trong một lần chạy duy nhất, mô hình đích sẽ chấp nhận bất kỳ token nào phù hợp với đầu ra của nó và tạo thêm một token. Quá trình soạn thảo trong giải mã suy đoán cho phép mô hình đích tạo ra nhiều token trong một lần chạy duy nhất trong khi duy trì phân phối đầu ra không thay đổi. Với một mô hình thảo có kích thước phù hợp, giải mã suy đoán đạt được tăng tốc từ 2 đến 3 lần, làm cho nó trở thành một phương pháp tiềm năng để giải quyết các vấn đề độ trễ cao.
Tuy nhiên, vì các mô hình thảo thường được yêu cầu tạo ra nhiều token trong nhiều bước, trong đó mỗi quá trình tạo vẫn liên quan đến giải mã tự hồi quy không hiệu quả, hiệu suất của giải mã suy đoán có thể bị hạn chế bởi độ trễ soạn thảo. Sự thiếu hiệu quả này cũng được chỉ ra bởi các thí nghiệm của Leviathan et al. (2023), nơi quan sát thấy rằng các mô hình rất nhỏ (ví dụ, nhỏ hơn khoảng hai bậc độ lớn so với mô hình đích) thường là lựa chọn tốt nhất cho soạn thảo vì chi phí suy luận của chúng thấp hơn so với mô hình thảo lớn hơn, mặc dù thực tế là các mô hình thảo lớn hơn thường có chất lượng tạo ra cao hơn. Điều này nhấn mạnh rằng cải thiện hiệu quả soạn thảo là quan trọng để nâng cao hơn nữa hiệu suất của giải mã suy đoán.
Xét đến điều này, một chiến lược chính để giải quyết nút thắt cổ chai này là tránh việc tạo tự hồi quy không hiệu quả của các mô hình thảo thần kinh. Dựa trên cân nhắc này, cần lưu ý rằng các mô hình ngôn ngữ thống kê, chẳng hạn như các mô hình ngôn ngữ bigram, gây ra độ trễ và chi phí tài nguyên tính toán không đáng kể so với các mô hình ngôn ngữ thần kinh, do cấu trúc đơn giản của chúng. Tuy nhiên, vì các token được tạo ra bởi
1arXiv:2312.11462v4  [cs.LG]  27 Feb 2024

--- TRANG 2 ---
Cascade Speculative Drafting cho Suy luận LLM Nhanh hơn nữa
Hình 1. Hình này cho thấy một ví dụ về Cascade Speculative Drafting với mô hình đích Mt và các mô hình thảo Md1, Md2, và Md3.
Tầng ngang bao gồm việc sử dụng các mô hình thảo lớn hơn để tạo ra các token trước và các mô hình nhỏ hơn để tạo ra các token sau. Tầng dọc yêu cầu mỗi mô hình xem xét bản thảo từ các mô hình nhỏ hơn trừ mô hình nhỏ nhất là một mô hình ngôn ngữ thống kê. Vì tầng ngang và tầng dọc trực giao với nhau, CS Drafting kết hợp cả hai phương pháp để đạt hiệu quả tối ưu.
0 5 10 15 20 25 30
Vị trí token0.10.20.30.40.50.60.70.8Tỷ lệ chấp nhận
FLAN-T5-small
FLAN-T5-base
FLAN-T5-large������������������������������������������������
Hình 2. Xác suất chấp nhận các token thảo liên quan đến vị trí của chúng trong một bước duy nhất của giải mã suy đoán, được đánh giá trên các mô hình FLAN-T5- SMALL, BASE, và LARGE trên GSM8K. Mô hình thảo tạo ra 30 token trong mỗi bước. Kết quả trên MMLU được hiển thị trong Hình 3 của Phụ lục.
các mô hình ngôn ngữ thống kê thường không có xác suất cao được chấp nhận bởi mô hình đích, giải mã suy đoán chỉ với các mô hình ngôn ngữ thống kê có thể không mang lại kết quả tối ưu so với việc sử dụng một mô hình ngôn ngữ thần kinh có kích thước phù hợp từ cùng họ với mô hình thảo.
Tuy nhiên, chúng tôi nhận thấy rằng không nhất thiết phải chỉ sử dụng một mô hình thảo trong giải mã suy đoán—các mô hình ngôn ngữ thống kê có thể phục vụ như "mô hình thảo" cho mô hình thảo thần kinh, do đó loại bỏ việc tạo tự hồi quy từ mô hình thảo thần kinh.Hơn nữa, phân tích của chúng tôi trong Hình 2 tiết lộ một mô hình trong bước soạn thảo: các token được tạo ra sau trong chuỗi bởi mô hình thảo cho thấy xác suất được chấp nhận bởi mô hình đích giảm dần. Điều này là do xác suất của một token được chấp nhận được điều kiện hóa trên việc chấp nhận các token trước đó. Nó chỉ ra rằng các token sau từ các mô hình thảo dễ bị từ chối hơn, đóng góp ít hơn vào số lượng token được chấp nhận dự kiến mỗi bước thảo, nhưng vẫn gây ra cùng độ trễ.
Được truyền cảm hứng từ các quan sát trên, chúng tôi đề xuất Cascade Speculative Drafting (CS Drafting), một thuật toán thực thi suy đoán bao gồm nhiều mô hình thảo, bắt đầu với mô hình nhỏ nhất là một mô hình ngôn ngữ thống kê. Mỗi mô hình thảo thần kinh xem xét các thế hệ từ một mô hình nhỏ hơn và sau đó đề xuất nội dung đã xem xét của nó cho một mô hình thảo lớn hơn hoặc mô hình đích. Trong thiết kế này, việc soạn thảo của mỗi mô hình thần kinh sẽ được tăng tốc bởi việc soạn thảo từ một mô hình nhỏ hơn, tránh sự thiếu hiệu quả của việc tạo tự hồi quy từ các mô hình thần kinh. Chúng tôi gọi phương pháp giải mã suy đoán phân cấp này là Tầng Dọc. Ngoài ra, chúng tôi đề xuất việc sử dụng các mô hình thảo nhỏ hơn, nhanh hơn để tạo ra các token có độ từ chối cao ở cuối trong việc tạo soạn thảo, tạo thành Tầng Ngang. Cùng với Tầng Dọc nói trên, những chiến lược này tạo thành phương pháp CS Drafting hoàn chỉnh của chúng tôi, như được minh họa trong Hình 1.
Thông qua phân tích lý thuyết và các nghiên cứu thực nghiệm, chúng tôi chứng minh rằng thuật toán CS Drafting vượt trội hơn giải mã suy đoán về độ trễ trong các nhiệm vụ khác nhau
2

--- TRANG 3 ---
Cascade Speculative Drafting cho Suy luận LLM Nhanh hơn nữa
và các cài đặt, đạt được tăng tốc bổ sung lên đến 81% so với giải mã suy đoán. Những phát hiện này nổi bật các lợi thế thực tế và cải tiến hiệu quả được cung cấp bởi cả tầng dọc và tầng ngang.
Các đóng góp chính được tóm tắt như sau:
•Chúng tôi giới thiệu Cascade Speculative Drafting (CS Drafting), một thuật toán dựa trên thực thi suy đoán cải thiện tốc độ suy luận mô hình ngôn ngữ mà không hy sinh chất lượng tạo ra.
•Chúng tôi cung cấp các phân tích lý thuyết hỗ trợ hiệu quả của phương pháp CS Drafting đề xuất.
•Chúng tôi tiến hành các thí nghiệm thực nghiệm cho thấy CS Drafting đạt được tăng tốc thêm so với giải mã suy đoán trong các nhiệm vụ và cài đặt khác nhau.
2. Cơ sở
Khái niệm cốt lõi của giải mã suy đoán (Leviathan et al., 2023) liên quan đến việc sử dụng một mô hình thảo nhỏ để tạo token tuần tự với xác thực bởi một mô hình đích lớn hơn dẫn đến giảm độ trễ. Thiết kế này tăng tốc việc lấy mẫu từ các mô hình tự hồi quy mà không thay đổi phân phối đầu ra. Ở trung tâm của nó, có hai quan sát chính: 1) một số thế hệ trong mô hình hóa ngôn ngữ đơn giản hơn những cái khác và có thể được dự đoán bởi các mô hình hiệu quả hơn một cách chính xác, và 2) việc sử dụng thực thi suy đoán cùng với một phương pháp lấy mẫu mới cho phép giải mã nhanh hơn, chính xác từ các mô hình lớn.
Cụ thể, để x là các token đầu vào tại một lần chạy và Mt và Md lần lượt là mô hình đích và mô hình thảo, k là số token thảo được tạo ra mỗi bước, và Mt(x)[i] và Md(x)[i] là đầu ra xác suất của chúng tại token thứ i khi đầu vào là x. Chúng tôi diễn giải lấy mẫu suy đoán như một hoạt động hai giai đoạn. Trong giai đoạn đề xuất, chúng tôi lấy mẫu {xt+1, ..., xt+k} từ mô hình thảo Md một cách tự hồi quy và nối chúng vào x. Trong giai đoạn xem xét, để xi ∈ {xt+1, ..., xt+k} đại diện cho token tại vị trí hiện tại, và chúng tôi chấp nhận nó nếu Md(x)[i-1] ≤ Mt(x)[i-1]; trong trường hợp Md(x)[i-1] > Mt(x)[i-1], chúng tôi từ chối xi với xác suất 1-Mt(x)[i-1]/Md(x)[i-1] và tiến hành lấy mẫu lại xi từ một phân phối tái hiệu chỉnh norm(max(0, Mt(x)[i-1]-Md(x)[i-1])) và từ chối bất kỳ token nào theo sau xi. Cuối cùng, mô hình đích sẽ tạo ra một token bổ sung theo sau các token được chấp nhận. Thiết kế như vậy đảm bảo đầu ra giống như lấy mẫu tự hồi quy chỉ sử dụng mô hình đích, như được chứng minh trong Leviathan et al. (2023).
Giải mã suy đoán đã được xác thực thực nghiệm trên các nhiệm vụ và kích thước mô hình khác nhau, chứng minh một sự tăng tốc đáng kể trong thời gian suy luận (nhanh hơn 2x-3x) so với các triển khai tiêu chuẩn, mà không ảnh hưởng đến đầu ra. Quan trọng là, nó không yêu cầu đào tạo cụ thể cho nhiệm vụ, thay đổi kiến trúc mô hình, hoặc thay đổi quy trình đào tạo, làm cho nó trở thành một giải pháp thực tế để giảm độ trễ của suy luận LLM.
3. Cascade Speculative Drafting
Trong phần này, chúng tôi giới thiệu phương pháp đề xuất của chúng tôi, Cascade Speculative Drafting (CS Drafting), một thuật toán thực thi suy đoán kết hợp hai loại tầng: tầng dọc và tầng ngang.
3.1. Tầng Dọc
Một sự thiếu hiệu quả đáng chú ý của thuật toán giải mã suy đoán là sự phụ thuộc vào việc tạo tự hồi quy của một mô hình thảo nhỏ hơn. Vì mô hình thảo phải chạy k lần cho mỗi lần chạy mô hình đích, chi phí vẫn có thể đáng kể mặc dù kích thước nhỏ hơn của nó. Xét đến điều này, chúng tôi giảm sự thiếu hiệu quả soạn thảo bằng cách sử dụng một mô hình thậm chí nhỏ hơn để hỗ trợ trong soạn thảo và sử dụng mô hình thảo ban đầu để xem xét việc tạo ra của mô hình nhỏ hơn này. Ngoài ra, vì quá trình này có thể được thực hiện lại trên mô hình thảo tạo thảo cho mô hình ban đầu, chúng tôi thực hiện đệ quy quá trình này cho đến khi nó đạt đến một mô hình thảo thống kê liên quan đến chi phí không đáng kể, chẳng hạn như một mô hình ngôn ngữ bigram. Trong phương pháp này, chúng tôi mong đợi mỗi bước đệ quy sẽ giảm độ trễ soạn thảo mà không thay đổi phân phối đầu ra. Chúng tôi gọi phương pháp suy đoán đệ quy này là Tầng Dọc.
Ngoài ra, chúng tôi kết hợp độ khoan dung, một siêu tham số làm lỏng quá trình xem xét bởi mô hình đích, cho phép tốc độ nhanh hơn với sự đánh đổi của kết quả có thể khác với mô hình đích (Leviathan et al., 2023). Độ khoan dung có thể được áp dụng trong quá trình lấy mẫu hoặc giải mã tham lam với giải mã suy đoán. Để độ khoan dung l ∈ [1,∞). Khi lấy mẫu, điều kiện chấp nhận cho token xi được chuyển đổi thành Md(x)[i] ≤ l × Mt(x)[i]. Nếu điều kiện chấp nhận không được thỏa mãn, với xác suất 1-l×Mt(x)/Md(x), chúng tôi từ chối xi và bất kỳ token nào theo sau.1 Khi thực hiện giải mã tham lam, điều kiện chấp nhận trở thành xác định và đơn giản là argmax Md(x)[i] = argmax Mt(x)[i] hoặc Md(x)[i] ≤ l × Mt(x)[i].
Đối với thuật toán giải mã suy đoán, chất lượng giảm được giới thiệu bởi độ khoan dung thường không mong muốn. Tuy nhiên, đối với phương pháp tầng dọc, độ khoan dung chỉ ảnh hưởng đến đầu ra cuối cùng nếu nó được áp dụng khi mô hình đích xem xét. Do đó, chúng tôi có thể hạn chế việc áp dụng độ khoan dung trong tầng dọc chỉ khi các mô hình thảo xem xét và không áp dụng độ khoan dung khi mô hình đích xem xét. Điều này có thể đảm bảo đầu ra cuối cùng không bị thay đổi trong khi giảm thêm độ trễ.
1Để đơn giản, chúng tôi giả định các đầu ra xác suất không được chuẩn hóa. Chúng tôi giới thiệu độc giả đến Leviathan et al. (2023) để thảo luận về lấy mẫu chuẩn hóa.
3

--- TRANG 4 ---
Cascade Speculative Drafting cho Suy luận LLM Nhanh hơn nữa
Thuật toán 1 CascadeSpeculativeDraftingStep
Yêu cầu: các mô hình thảo {Md1, ...,Mdn}, mô hình đích Mt,prefix, cờ isFirstCall, siêu tham số Knn,l
draftList ←[Md1, ...,Mdn]
▷Khởi tạo curGen và curProb.
curGen ←prefix, curProbs ←một danh sách các số một với cùng độ dài như prefix
▷Giải nén một danh sách k cho lần gọi hàm hiện tại.
[k1, ..., kn−1]←hàng đầu tiên của Knn
▷Tạo ra sử dụng MaG cho trường hợp cơ sở của lời gọi đệ quy.
if draftList is empty then
M ← phần tử đầu tiên của draftList
res← M (curGen )
return res.generation, res.logits
end if
▷Thực hiện tầng ngang với vòng lặp for.
for i←1 to n do
▷Chuẩn bị các đối số cho lời gọi đệ quy tiếp theo.
curTarget ←mục thứ i của draftList
curDraftList ←danh sách con của draftList bắt đầu từ chỉ mục i+ 1
curK←ma trận con của Knn từ góc trên trái tại (i+ 1, i+ 1) mở rộng đến góc dưới phải
curPrefix ←curGen
while curGen.length - curPrefix.length is less than ki do
curPrefix ←curGen
▷Thực hiện tầng dọc với lời gọi đệ quy.
[x1, .., xu],[p1, p2, ..., pv]←CascadeSpeculativeDraftingStep (curDraftList, curTarget, curPrefix, False, curK, l)
curGen ←[x1, .., xu]
s←curProbs.length + 1
Add elements of [p1, p2, ..., pv] to curProbs
end while
end for
▷Đặt độ khoan dung thành 1 khi mô hình đích ban đầu xem xét.
if isFirstCall then
l←1
end if
▷Sử dụng Mt để xem xét việc tạo thảo.
[x1, ..., xout],[p′1, p′2, ..., p′out]= review( Mt, curGen, curProbs, l)
return [x1, ..., xout],[p′1, p′2, ..., p′out]
3.2. Tầng Ngang
Một quan sát chính khác là trong các bước soạn thảo của giải mã suy đoán, không phải tất cả các token soạn thảo đều được tạo ra bình đẳng, như được minh họa trong Hình 2. Token thảo đầu tiên có nhiều khả năng được chấp nhận hơn vì nó chỉ phụ thuộc vào chính nó; token cuối cùng hiếm khi được chấp nhận, vì nó có cơ hội được xem xét chỉ khi tất cả các token trước đó được chấp nhận. Từ quan điểm lý thuyết, giả định sự kiện chấp nhận mỗi token là một phân phối Bernoulli với xác suất p, xác suất token thứ n được chấp nhận là pn, ngụ ý một sự giảm theo cấp số nhân của giá trị cho các token được tạo ra sau trong chuỗi.
Được truyền cảm hứng từ quan sát này, chúng tôi thiết kế Tầng Ngang, một phương pháp cải thiện phân bổ thời gian bằng phân bổ token thảo. Tầng Ngang gán mô hình thảo lớn nhất để thực hiện việc tạo ra token thảo đầu tiên do có sự căn chỉnh đầu ra cao nhất với mô hình đích, và nó dần dần sử dụng một mô hình nhỏ hơn khi token thảo mới được tạo ra ít có khả năng được chấp nhận hơn. Quá trình này dừng lại sau mô hình nhỏ nhất, tức là, một mô hình ngôn ngữ thống kê hoàn thành. Thiết kế này giảm chi phí thời gian của việc tạo ra các token thảo không quan trọng với một mô hình thảo tốn kém, dẫn đến giảm độ trễ tổng thể.
3.3. Max-Gram cho Soạn thảo Thống kê Tốt hơn
Vì cả Tầng Dọc và Tầng Ngang đều nhận xét tầng hướng tới các mô hình thảo nhanh hơn, một mô hình ngôn ngữ thống kê, là cơ sở của tầng, trở nên thiết yếu cho hiệu quả của cả hai phương pháp. Trong việc theo đuổi một
4

--- TRANG 5 ---
Cascade Speculative Drafting cho Suy luận LLM Nhanh hơn nữa
mô hình ngôn ngữ thống kê hiệu quả hơn, chúng tôi nhận thấy một mô hình chung: trong việc tạo ra mô hình ngôn ngữ, một số từ và cụm từ từ truy vấn đầu vào thường xuyên xuất hiện lại trong nội dung được tạo ra. Dựa trên quan sát này, chúng tôi thiết kế thuật toán Max-Gram (MaG). Nó tham lam xác định các khớp tối đa giữa đầu vào ban đầu (hoặc thế hệ hiện có) và các token từ cuối thế hệ. Trong những trường hợp không có khớp, chúng tôi sử dụng mô hình bigram dựa trên phân phối xác suất của Wikipedia (được chọn để duy trì tính tổng quát). Chúng tôi bao gồm một phiên bản thân thiện GPU của thuật toán Max-Gram trong Phụ lục A.
3.4. Thuật toán
Kết hợp tầng ngang và tầng dọc, thuật toán của cascade speculative decoding được trình bày trong Thuật toán 1. Ở trung tâm của nó, tầng ngang được thực hiện bởi vòng lặp for, trong khi tầng dọc được thực hiện thông qua các lời gọi đệ quy. Đáng chú ý, mô hình MaG được kết hợp như mô hình thảo nhỏ nhất để tránh việc tạo tự hồi quy từ một mô hình thần kinh. Một ví dụ về CS Drafting được hiển thị trong Hình 1.
Thuật toán yêu cầu một siêu tham số tam giác trên, Knn, với mỗi hàng phục vụ như tiêu chí dừng cho một lớp lời gọi đệ quy. Để đơn giản, chúng tôi giả định độ khoan dung l là phổ quát cho thuật toán, trừ khi mô hình đích đang được xem xét; do đó, thuật toán có thể hưởng lợi từ tăng tốc của độ khoan dung mà không thay đổi phân phối đầu ra.
4. Phân tích
Chúng tôi bắt đầu với một số khái niệm. Để Mt là mô hình đích, Md là mô hình thảo, và k là số token thảo được tạo ra mỗi bước.
Tỷ lệ chấp nhận dự kiến α(Mt,Md) là xác suất của việc tạo ra thảo bởi Md được chấp nhận bởi mô hình đích Mt.
Hệ số chi phí c(Mt,Md) là tỷ lệ thời gian cho một lần chạy duy nhất của mô hình thảo Md trên mô hình đích Mt.
Hệ số cải thiện thời gian tường dự kiến (EWIF) là cải thiện thời gian dự kiến đạt được bởi một thuật toán dưới giả định i.i.d. của việc chấp nhận token.
Mặc dù cài đặt đơn giản của EWIF, Leviathan et al. (2023) đã chứng minh rằng nó phù hợp với kết quả thí nghiệm trong hầu hết các trường hợp. Do đó, phân tích của chúng tôi sẽ tập trung vào EWIF.
4.1. Tầng Dọc
Chúng tôi phân tích EWIF của tầng dọc bằng cách sử dụng các hàm tạo, một chủ đề được nghiên cứu kỹ lưỡng trong toán học tổ hợp (West, 2021). Các tính chất của các hàm tạo hữu ích trong quá trình đệ quy và đánh giá làm cho biểu thức cuối cùng của chúng tôi đơn giản.
Chúng tôi bắt đầu với việc suy ra hàm tạo xác suất cho giải mã suy đoán.
Định lý 4.1. Đối với giải mã suy đoán giữa Mt và Md, để pi là xác suất tạo ra i token. Hàm tạo xác suất của pi thỏa mãn phương trình sau:
ϕ(α,k)(x) = 1 + (x−1)1−αk+1xk+1/(1−αx), (1)
trong đó α=α(Mt,Md).
Chứng minh trong Phụ lục C.1.
Hệ quả 4.2. EWIF của giải mã suy đoán là
ϕ′(α,k)(1)/(ck+1)=1−αk+1/(1−α)(ck+1).
Chúng tôi sử dụng hàm tạo để suy ra EWIF của một tầng dọc và phân tích trường hợp liên quan đến hai mô hình thảo, Md1 và Md2.
Định lý 4.3. Giả định k là tham số giải mã suy đoán giữa Md1 và Md2, và n là số bước Mt xem xét. EWIF bởi hệ thống này là
1−αϕn(α)/(1−α)(1 + ncd1+nkcd2), (2)
trong đó ϕ(x) =ϕ(α(Md1,Md2),k)(x),α=α(Mt,Md), và cd1, cd2 lần lượt là c(Mt,Md1), c(Mt,Md2).
Hệ quả 4.4. ϕα′,k(α)< α cho bất kỳ 1> α > 0,1> α′> 0, k > 0, vì vậy nếu cd2<<1, EWIF của Md1 và Md2 cao hơn EWIF của chỉ Md1.
Chứng minh trong Phụ lục C.2.
Do đó, với mô hình thống kê có chi phí không đáng kể (tức là, cd2<<1), nó hầu như luôn có thể cải thiện hiệu quả của một hệ thống SD.
4.2. Tầng Ngang
Chúng tôi cũng trình bày một phân tích về cải thiện thời gian tường được cung cấp bởi tầng ngang.
Để hỗ trợ phân tích, chúng tôi thiết lập các khái niệm. Để Mt là mô hình đích, {Mi} là các mô hình thảo hỗ trợ việc tạo ra với Mi là mô hình thảo tạo ra token thứ i. Trong trường hợp đơn giản hơn của giải mã suy đoán, Mi=Md cho bất kỳ i nào. Để x là đầu vào cho mô hình tại một lần chạy duy nhất, Mt(x) và Mi(x) sau đó là phân phối xác suất đầu ra với đầu vào x. Để đơn giản hóa ký hiệu, để αi=α(Mt(x),Mi(x)) và ci=c(Mt(x),Mi(x)).
Định lý 4.5. Hệ số cải thiện thời gian tường dự kiến (EWIF) của tầng ngang là
T(k, α1, ..., αk, c1, ..., ck) =∑ki=0∏ij=1αj/(1+∑ki=1ci).
5

--- TRANG 6 ---
Cascade Speculative Drafting cho Suy luận LLM Nhanh hơn nữa
Dataset Md1Md2 k1k2EWIF
CNNDM SMALL - 9 - 2.65
CNNDM BASE - 8 - 2.96
CNNDM BASE SMALL 5 3 3.03
ENDE SMALL - 12 - 3.61
ENDE BASE - 11 - 3.75
ENDE BASE SMALL 5 8 3.93
Bảng 1. EWIF mô phỏng dưới giả định rằng phân phối chấp nhận là một phân phối Bernoulli. BASE và SMALL đề cập đến FLAN-T5- BASE và FLAN-T5- SMALL. Trong mô phỏng, lấy mẫu suy đoán với tầng ngang vượt trội hơn hiệu suất của giải mã suy đoán vanilla trên cả tập dữ liệu CNN Dailymail (Nallapati et al., 2016) và WMT EnDe (Bojar et al., 2018).
Hơn nữa, định lý 4.5 có thể được sử dụng để phân tích tầm quan trọng của các token trong bước soạn thảo.
Hệ quả 4.6. Xác suất token thứ i được chấp nhận là ∏ij=1αj. Đạo hàm của EWIF đối với αl là dT(k,α1,...,αk,c1,...,ck)/dαl=∑ki=l∏ij=1,j̸=lαj/(1+∑ki=1ci).
Cụ thể, dT(k,α1,...,αk,c1,...,ck)/dα1=∑ki=1∏ij=2αj/(1+∑ki=1ci) và dT(k,α1,...,αk,c1,...,ck)/dαk=∏k−1j=1αj/(1+∑ki=1ci).
Sử dụng thông tin được cung cấp bởi Leviathan et al. (2023), chúng tôi tính toán một EWIF mô phỏng dưới giả định rằng sự kiện chấp nhận bởi mô hình đích là một thử nghiệm Bernoulli. Kết quả, được hiển thị trong Bảng 1, chỉ ra rằng lấy mẫu suy đoán với một tầng ngang đạt được EWIF tốt hơn so với lấy mẫu suy đoán vanilla dưới giả định này.
5. Thí nghiệm
5.1. Số liệu Đánh giá
Các công trình trước đây về giải mã suy đoán và các phương pháp liên quan dựa vào thời gian tường như một phương pháp đánh giá. Tuy nhiên, có những lo ngại về chuẩn hóa và tính hợp pháp liên quan đến thời gian tường. Leviathan et al. (2023) đề cập rằng hệ số chi phí, tỷ lệ giữa thời gian cho một lần chạy duy nhất của mô hình thảo và mô hình đích sẽ thay đổi từ hệ thống này sang hệ thống khác có thể dẫn đến sự biến đổi đáng kể trong cải thiện thời gian tường trên các hệ thống khác nhau, làm cho việc so sánh trên các công trình nghiên cứu khác nhau trở nên khó khăn.
Ngoài ra, một phân tích gần đây cho thấy tốc độ GPU có thể thay đổi cho cùng một mô hình GPU với một GPU nhanh hơn 1.5x so với GPU khác của cùng mô hình (Sinha et al., 2022); do đó, cải thiện thời gian tường lý thuyết lên đến 1.5x có thể bị làm suy yếu bởi sự biến đổi GPU, và các phương pháp không có cải thiện nào có thể được đo với một cải thiện bằng cách chạy trên một GPU khác. Sự thiếu chuẩn hóa và ổn định làm cho thời gian tường không phù hợp để đo lường cải thiện thuật toán. Để giải quyết vấn đề này, chúng tôi thiết kế một số liệu mới để đo lường giải mã suy đoán và các thuật toán liên quan của nó có thể giảm tác động của hiệu suất GPU thay đổi/không ổn định.
Phương pháp đề xuất của chúng tôi, cải thiện thời gian tường chuẩn hóa (SWI), tính toán thời gian GPU của các mô hình, giả định rằng mỗi lần chạy của một mô hình ngôn ngữ tốn cùng một lượng thời gian, một giả định được đưa ra khi phát minh ra thuật toán lấy mẫu suy đoán (Chen et al., 2023). Cho một hệ thống giải mã suy đoán S chứa k mô hình thảo Md1, ...,Mdk, mô hình đích Mt giải quyết một nhiệm vụ T, và để count(S, m, T) là số lần chạy của mô hình m trong quá trình thực thi nhiệm vụ T với hệ thống S, để ci là hệ số chi phí dự kiến là tỷ lệ thời gian giữa một lần chạy duy nhất của Mi và Mt, và để n là tổng số token được tạo ra, cải thiện thời gian tường chuẩn hóa được định nghĩa là
SWI (Md1, ...,Mdk,Mt, T) = n/(count(S,Mt, T) +∑ni=1count(S,Mdi, T)ci),
đó chỉ đơn giản là tổng thời gian GPU bởi tất cả các mô hình khi ci được đo trong một thí nghiệm địa phương. Lợi ích của đánh giá SWI là count(S,Mt, T) và count(S,Mdi, T) sẽ nhất quán trên các hệ thống khác nhau, vì vậy bằng cách sử dụng cùng ci trên các thí nghiệm khác nhau, sự biến đổi hiệu suất GPU sẽ được loại bỏ; do đó, nó cung cấp một phép đo chuẩn hóa, có thể tái tạo.
Trong thí nghiệm của chúng tôi, chúng tôi sẽ sử dụng hai bộ {ci}, {ci} hiện có được báo cáo bởi Leviathan et al. (2023) và một {ci} heuristic là tỷ lệ số tham số có thể điều chỉnh giữa Mdi và Mt.
5.2. Thiết lập
Để đảm bảo tính tổng quát của các phát hiện của chúng tôi, chúng tôi thực hiện thí nghiệm trên cả mô hình encoder-decoder và mô hình chỉ decoder. Đối với các mô hình encoder-decoder, chúng tôi chọn mô hình đích và mô hình thảo của chúng tôi từ họ FLAN-T5 (Chung et al., 2022) cho thí nghiệm của chúng tôi, vì có sự biến đổi lớn về kích thước mô hình trong họ FLAN-T5 (từ 77 triệu đến 11 tỷ tham số). Chúng tôi sử dụng FLAN-T5- XXL làm mô hình đích của chúng tôi, FLAN-T5- BASE và FLAN-T5- SMALL làm các mô hình thảo xem xét của chúng tôi. Đối với các mô hình chỉ decoder, chúng tôi chọn LLAMA-2-chat-7B (Touvron et al., 2023) làm mô hình đích. Vì không có phát hành chính thức của một mô hình nhỏ hơn trong họ LLAMA, chúng tôi sử dụng một mô hình được đào tạo trước 160M với cùng tokenizer như mô hình thảo xem xét (Miao et al., 2024). Trong cả hai trường hợp, thuật toán Max-Gram được sử dụng như mô hình thảo tạo ra. Để căn chỉnh thí nghiệm của chúng tôi với việc sử dụng chung hiện tại, chúng tôi không thực hiện fine-tuning, và việc tạo ra được tiến hành theo cách zero-shot.
Vì chúng tôi không quan sát thấy bất kỳ sự khác biệt đáng kể nào giữa lấy mẫu với nhiệt độ 1 và giải mã tham lam trong các thí nghiệm giải mã suy đoán trước đây (Leviathan et al.,
6

--- TRANG 7 ---
Cascade Speculative Drafting cho Suy luận LLM Nhanh hơn nữa
Dataset Algorithm {Mdi} Speedup (MS) k11k12k22l
GSM8K Autoregressive - 1 - - - -
GSM8K S Decoding BASE 3.38 10 - - -
GSM8K S Decoding SMALL 3.06 11 - - -
GSM8K CS Drafting BASE ,MAG 3.70 10 - - -
GSM8K CS Drafting SMALL ,MAG 3.19 11 - - -
GSM8K CS Drafting BASE ,SMALL ,MAG 3.88 8 13 1 3
MMLU Autoregressive - 1 - - - -
MMLU S Decoding BASE 3.97 13 - - -
MMLU S Decoding SMALL 4.12 19 - - -
MMLU CS Drafting BASE ,MAG 4.56 13 - - -
MMLU CS Drafting SMALL ,MAG 4.39 14 - - -
MMLU CS Drafting BASE ,SMALL ,MAG 4.88 5 19 1 5
Dataset Algorithm {Mdi} Speedup (PW) k11k12k22l
GSM8K Autoregressive - 1 - - - -
GSM8K S Decoding BASE 2.99 8 - - -
GSM8K S Decoding SMALL 2.76 8 - - -
GSM8K CS Drafting BASE ,MAG 3.27 9 - - -
GSM8K CS Drafting SMALL ,MAG 2.82 11 - - -
GSM8K CS Drafting BASE ,SMALL ,MAG 3.43 5 9 1 3
MMLU Autoregressive - 1 - - - -
MMLU S Decoding BASE 3.42 10 - - -
MMLU S Decoding SMALL 3.51 11 - - -
MMLU CS Drafting BASE ,MAG 4.21 6 - - -
MMLU CS Drafting SMALL ,MAG 3.99 13 - - -
MMLU CS Drafting BASE ,SMALL ,MAG 4.32 5 8 1 5
Bảng 2. Kết quả thí nghiệm trên FLAN-T5. Speedup (MS) là cải thiện thời gian tường chuẩn hóa với giả định rằng độ trễ của mỗi lần chạy của một mô hình là số tham số của nó (kích thước mô hình). Speedup (PW) là SWI với giả định rằng độ trễ của mỗi lần chạy của một mô hình là dữ liệu chi phí thời gian được báo cáo từ công trình trước đây (Leviathan et al., 2023). k11,k12,k22,l là các siêu tham số. k11 và k12 đại diện cho hạn chế bước mô hình đích và các mô hình thảo, k22 là hạn chế bước giữa mô hình thảo thứ nhất và thứ hai, và l là độ khoan dung như được hiển thị trong thuật toán 1. Đối với giải mã suy đoán, k11 đơn giản là k.
2023), và để đảm bảo các thí nghiệm của chúng tôi hoàn toàn có thể tái tạo, chúng tôi thực hiện lấy mẫu ở nhiệt độ 0, tức là, sử dụng giải mã tham lam theo mặc định.
Suy luận Cascade Speculative Drafting Để giảm số lượng siêu tham số cần điều chỉnh, chúng tôi sử dụng MaG để tạo ra 10 token cùng một lúc, vì hiếm khi có hơn 10 token được chấp nhận. Chúng tôi không sử dụng độ khoan dung khi người xem xét là Mt để đảm bảo phân phối đầu ra không thay đổi. Chúng tôi cũng tránh độ khoan dung giữa MaG và người xem xét của nó, vì vẫn có một khoảng cách hiệu suất đáng kể giữa MaG và một mô hình thần kinh.
Với những ràng buộc này, chúng tôi còn lại tối đa bốn siêu tham số: k11,k12,k22, và l. Đối với bước CS Drafting nơi mô hình đích là người xem xét, k11 và k12 được sử dụng. k21 và l được sử dụng trong bước nơi Md1 là người xem xét.
Baseline Chúng tôi xem xét cả giải mã suy đoán và việc tạo tự hồi quy vanilla như baseline của chúng tôi. Khi thực hiện giải mã suy đoán với mô hình đích là FLAN-T5- XXL, chúng tôi chọn FLAN-T5- SMALL và FLAN-T5-BASE làm mô hình thảo, vì chúng hoạt động tốt hơn FLAN-T5- LARGE, theo Leviathan et al. (2023) và các thí nghiệm sơ bộ của chúng tôi. Khi LLAMA-2-chat-7B là mô hình đích, chúng tôi sử dụng mô hình 160M làm mô hình thảo. Chúng tôi kiểm tra một phạm vi rộng các giá trị k, từ 2 đến 30, để đảm bảo rằng chúng tôi tìm thấy kết quả tốt nhất cho mỗi mô hình phục vụ như mô hình thảo của chúng tôi để thiết lập baseline của chúng tôi.
Dataset Chúng tôi đã chọn hai tập dữ liệu thường được sử dụng cho các thí nghiệm của chúng tôi. Đối với cả hai tập dữ liệu, chúng tôi đã tiến hành thí nghiệm trong thiết lập chuỗi suy nghĩ zero-shot (Kojima et al., 2022; Wei et al., 2023):
•GSM8K (Cobbe et al., 2021) là một tập dữ liệu bao gồm 8,500 bài toán từ chất lượng cao, đa dạng về ngôn ngữ, cấp tiểu học. Nó tập trung vào lý luận nhiều bước với các vấn đề thường có thể giải quyết bằng cách sử dụng số học cơ bản
7

--- TRANG 8 ---
Cascade Speculative Drafting cho Suy luận LLM Nhanh hơn nữa
Dataset Algorithm {Mdi} Speedup k11
GSM8K Auto. - 1 -
GSM8K S Decoding 160 M 2.48 12
GSM8K CS Drafting 160 M,MAG 2.86 15
MMLU Auto. - 1 -
MMLU S Decoding 160 M 2.12 10
MMLU CS Drafting 160 M,MAG 2.64 13
Bảng 3. Kết quả thí nghiệm trên LLAMA.
trong 2-8 bước.
•MMLU (Hendrycks et al., 2021), hoặc Massive Multitask Language Understanding, là một benchmark để kiểm tra mức độ hiểu biết kiến thức của các mô hình ngôn ngữ lớn. Nó bao gồm 57 chủ đề đa dạng, từ khoa học cơ bản đến luật nâng cao.
5.3. Kết quả Thí nghiệm
So sánh với Baseline Bảng 2 trình bày kết quả thí nghiệm. Trong hai cài đặt của SWI, Cascade Speculative Decoding đã vượt trội hơn thuật toán giải mã suy đoán. Đối với GSM8K, Cascade Speculative Decoding đạt được tăng tốc bổ sung tối đa 44% so với thuật toán suy đoán nhanh nhất; đối với MMLU, cải thiện tăng tốc bổ sung tối đa so với giải mã suy đoán là 81%. Nhìn chung, CS Decoding đạt được tăng tốc 3-4x so với việc tạo tự hồi quy, thiết lập chính nó như một thuật toán nhanh hơn so với các baseline.
Hiệu quả của MaG Khi so sánh CS Drafting với một mô hình thần kinh và MaG với thiết lập giải mã suy đoán nhanh nhất, chúng tôi thấy rằng CS Drafting với một mô hình thần kinh đạt được tăng tốc lên đến 70% trên MMLU và tăng tốc 32% trên GSM8K. Đáng chú ý, thuật toán MaG chỉ liên quan đến một mô hình bigram với nhiều tham số bằng kích thước của tokenizer, làm cho chi phí bộ nhớ của nó không đáng kể.
Đáng chú ý, tăng tốc đạt được bằng cách sử dụng CS Drafting với một mô hình thần kinh không liên quan đến overhead triển khai bổ sung trong khi giảm cả độ trễ và chi phí tính toán, làm cho nó trở thành lựa chọn vượt trội so với giải mã suy đoán.
Kích thước Mô hình Thảo Mặc dù FLAN-T5- SMALL chủ yếu vượt trội hơn FLAN-T5- BASE như một mô hình thảo cho giải mã suy đoán, trong CS Drafting với sự hỗ trợ của MaG, FLAN-T5- BASE luôn vượt trội hơn FLAN-T5- SMALL. Điều này ngụ ý rằng với sự hạn chế của một mô hình thảo duy nhất, kích thước lý tưởng của mô hình thảo có thể tăng với sự hỗ trợ của mô hình MaG.
Kết quả của Mô hình Chỉ Decoder Như được hiển thị trong Bảng 3, kết quả cho các mô hình chỉ decoder thể hiện các đặc điểm tương tự như những gì quan sát được trong các thí nghiệm trên các mô hình encoder-decoder. Do đó, đối với các thuật toán dựa trên giải mã suy đoán, chúng tôi tin rằng không có sự khác biệt rõ ràng giữa một mô hình encoder-decoder và một mô hình chỉ decoder, phù hợp với kết quả trong Leviathan et al. (2023).
6. Công trình Liên quan
6.1. Phương pháp Hiệu quả cho Suy luận Mô hình Ngôn ngữ
Trong kỷ nguyên của các mô hình ngôn ngữ lớn, hiệu quả trong quá trình suy luận trở thành chìa khóa cho dịch vụ mô hình. Để giảm chi phí suy luận mô hình và tăng tốc, một số phương pháp hiệu quả đã được đề xuất, bao gồm tỉa, chưng cất kiến thức và lượng tử hóa (Treviso et al., 2023). Tỉa mô hình thực hiện các phương pháp có cấu trúc (Xia et al., 2022; Voita et al., 2019) hoặc không có cấu trúc (Guo et al., 2021; Chen et al., 2020) để loại bỏ các tham số mô hình dư thừa để giảm bộ nhớ lưu trữ và tăng tốc độ suy luận. Chưng cất kiến thức thực hiện phương pháp chuyển giao kiến thức từ một mô hình giáo viên ưu việt đến một mô hình học sinh nhỏ hơn (Hinton et al., 2015; Gou et al., 2021). Lượng tử hóa ánh xạ các biểu diễn dữ liệu độ chính xác cao (ví dụ 32 bit) thành những cái độ chính xác thấp (ví dụ 8 bit) để giảm tiêu thụ bộ nhớ (Bhandare et al., 2019; Schaefer et al., 2023).
6.2. Giải mã Suy đoán
Với thành công của Giải mã Suy đoán (Chen et al., 2023; Leviathan et al., 2023) trong việc giảm độ trễ suy luận mô hình ngôn ngữ lớn, một số công trình gần đây đã cố gắng cải thiện Giải mã Suy đoán bằng cách giảm tỷ lệ từ chối. Zhou et al. (2023) đề xuất sử dụng chưng cất kiến thức tổng quát và đạt được tỷ lệ từ chối thấp hơn so với các phương pháp chưng cất kiến thức khác. Tránh một mô hình thảo bổ sung, tự soạn thảo là một phương pháp cho giải mã suy đoán bằng cách tái sử dụng một phần của mô hình đích cùng với trọng số được thêm vào để thực hiện soạn thảo (Zhang et al., 2023; Hooper et al., 2024). Attention cây liên quan đến việc tạo ra nhiều ứng viên trong quá trình soạn thảo để tăng cơ hội chấp nhận (Spector & Re, 2023; Miao et al., 2024). Bên cạnh việc giảm tỷ lệ từ chối, cải thiện hiệu quả soạn thảo cũng có thể giảm độ trễ. Spector & Re (2023) đề xuất sử dụng giải mã suy đoán cho soạn thảo, cho thấy sự tương tự với tầng dọc; tuy nhiên, phương pháp của họ chỉ có hai lớp giải mã suy đoán và không quan sát bản chất đệ quy của tầng dọc cũng như độ khoan dung giữa các mô hình thảo, hai khía cạnh quan trọng cho hiệu suất của tầng dọc.
7. Kết luận
Trong công trình này, chúng tôi đề xuất một thuật toán mới, CS Drafting, bao gồm hai tầng: tầng dọc và tầng ngang. Tầng dọc loại bỏ sự cần thiết của việc tạo tự hồi quy từ một mô hình ngôn ngữ thần kinh, trong khi tầng ngang hiệu quả phân bổ chi phí của các token soạn thảo ở các vị trí khác nhau. Các thí nghiệm của chúng tôi cho thấy CS Drafting đạt được tăng tốc bổ sung lên đến 81 phần trăm so với giải mã suy đoán, trong khi duy trì cùng phân phối đầu ra như mô hình đích. Công trình của chúng tôi chứng minh rằng suy luận LLM có thể được tăng tốc thêm bởi các tầng mà không hy sinh chất lượng tạo ra hoặc yêu cầu đào tạo cụ thể nhiệm vụ bổ sung.
8

--- TRANG 9 ---
Cascade Speculative Drafting cho Suy luận LLM Nhanh hơn nữa
Tuyên bố Tác động
Bài báo này trình bày công trình có mục tiêu thúc đẩy lĩnh vực Học máy. Có nhiều hậu quả xã hội tiềm năng của công trình của chúng tôi, không có gì chúng tôi cảm thấy phải được làm nổi bật cụ thể ở đây.
Tài liệu tham khảo
Bhandare, A., Sripathi, V., Karkada, D., Menon, V., Choi, S., Datta, K., and Saletore, V. Efficient 8-bit quantization of transformer neural machine language translation model, 2019.
Bojar, O., Federmann, C., Fishel, M., Graham, Y., Haddow, B., Huck, M., Koehn, P., and Monz, C. Findings of the 2018 conference on machine translation (WMT18). In Bojar, O., Chatterjee, R., Federmann, C., Fishel, M., Graham, Y., Haddow, B., Huck, M., Yepes, A. J., Koehn, P., Monz, C., Negri, M., Névéol, A., Neves, M., Post, M., Specia, L., Turchi, M., and Verspoor, K. (eds.), Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pp. 272–303, Belgium, Brussels, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6401.
Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre, L., and Jumper, J. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023.
Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Wang, Z., and Carbin, M. The lottery ticket hypothesis for pre-trained bert networks, 2020.
Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.
Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems, 2021.
Gou, J., Yu, B., Maybank, S. J., and Tao, D. Knowledge distillation: A survey. International Journal of Computer Vision, 129(6):1789–1819, March 2021. ISSN 1573-1405. doi: 10.1007/s11263-021-01453-z.
Guo, D., Rush, A. M., and Kim, Y. Parameter-efficient transfer learning with diff pruning, 2021.
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding, 2021.
Hinton, G., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network, 2015.
Hooper, C., Kim, S., Mohammadzadeh, H., Genc, H., Keutzer, K., Gholami, A., and Shao, S. Speed: Speculative pipelined execution for efficient decoding, 2024.
Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 22199–22213, 2022.
Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 1207–1216, Stanford, CA, 2000. Morgan Kaufmann.
Leviathan, Y., Kalman, M., and Matias, Y. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pp. 19274–19286. PMLR, 2023.
Miao, X., Oliaro, G., Zhang, Z., Cheng, X., Wang, Z., Zhang, Z., Wong, R. Y. Y., Zhu, A., Yang, L., Shi, X., Shi, C., Chen, Z., Arfeen, D., Abhyankar, R., and Jia, Z. Specinfer: Accelerating generative large language model serving with tree-based speculative inference and verification, 2024.
Nallapati, R., Zhou, B., dos santos, C. N., Gulcehre, C., and Xiang, B. Abstractive text summarization using sequence-to-sequence rnns and beyond, 2016.
OpenAI. Gpt-4 technical report, 2023.
Schaefer, C. J., Guo, E., Stanton, C., Zhang, X., Jablin, T., Lambert-Shirzad, N., Li, J., Chou, C., Joshi, S., and Wang, Y. E. Mixed precision post training quantization of neural networks with sensitivity guided search, 2023.
Sinha, P., Guliani, A., Jain, R., Tran, B., Sinclair, M. D., and Venkataraman, S. Not all gpus are created equal: characterizing variability in large-scale, accelerator-rich systems. In SC22: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 01–15. IEEE, 2022.
Spector, B. and Re, C. Accelerating llm inference with staged speculative decoding. arXiv preprint arXiv:2308.04623, 2023.
9

--- TRANG 10 ---
Cascade Speculative Drafting cho Suy luận LLM Nhanh hơn nữa
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned chat models, 2023.
Treviso, M., Lee, J.-U., Ji, T., van Aken, B., Cao, Q., Ciosici, M. R., Hassid, M., Heafield, K., Hooker, S., Raffel, C., Martins, P. H., Martins, A. F. T., Forde, J. Z., Milder, P., Simpson, E., Slonim, N., Dodge, J., Strubell, E., Balasubramanian, N., Derczynski, L., Gurevych, I., and Schwartz, R. Efficient methods for natural language processing: A survey, 2023.
Voita, E., Talbot, D., Moiseev, F., Sennrich, R., and Titov, I. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned, 2019.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thought prompting elicits reasoning in large language models, 2023.
West, D. Combinatorial Mathematics. Cambridge University Press, 2021. ISBN 9781107058583.
Xia, H., Ge, T., Wang, P., Chen, S.-Q., Wei, F., and Sui, Z. Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 3909–3925, 2023.
Xia, M., Zhong, Z., and Chen, D. Structured pruning learns compact and accurate models, 2022.
Zhang, J., Wang, J., Li, H., Shou, L., Chen, K., Chen, G., and Mehrotra, S. Draft & verify: Lossless large language model acceleration via self-speculative decoding, 2023.
Zhou, Y., Lyu, K., Rawat, A. S., Menon, A. K., Rostamizadeh, A., Kumar, S., Kagy, J.-F., and Agarwal, R. Distillspec: Improving speculative decoding via knowledge distillation, 2023.
10

--- TRANG 11 ---
Cascade Speculative Drafting cho Suy luận LLM Nhanh hơn nữa
A. Triển khai Max-Gram
Listing 1. Thuật toán Max-Gram
1def torch_index(t, value):
2 return (t == value).nonzero(as_tuple=True)[0][0]
3
4
5def max_gram(input_ids, encoder_ids, n=1):
6 matches = (encoder_ids[0] == input_ids[0, -1]).int()
7 if matches.sum() < 1:
8 return None
9 for i in range(2, input_ids.shape[-1] + 1):
10 new_matches = (encoder_ids[0, :(-1 *(i - 1))] == input_ids[0, -1 *i]).int()
11 combined_matches = (2 - new_matches == matches[1:]).int()
12 if combined_matches.sum() < 1:
13 index = torch_index(torch.cat(
14 (
15 torch.tensor([0] *(i - 1), device=torch.device(encoder_ids.device)),
16 matches
17 ),
18 dim=-1
19 ), 1)
20 return encoder_ids[:, index:index + n]
21 else:
22 matches = combined_matches
23 index = torch_index(torch.cat((
24 torch.tensor([0] *(encoder_ids.shape[-1] - matches.shape[-1])), matches), dim=-1
25 ), 1)
26 return encoder_ids[:, index+1:index + n+1]
B. Kết quả Bổ sung
0 5 10 15 20 25 30
Vị trí token0.10.20.30.40.50.60.7Tỷ lệ chấp nhậnTỷ lệ chấp nhận các token thảo trên tập dữ liệu MMLU
FLAN-T5-small
FLAN-T5-base
FLAN-T5-large
Hình 3. Xác suất chấp nhận các token thảo liên quan đến vị trí của chúng trong một bước duy nhất của giải mã suy đoán, được đánh giá trên các mô hình FLAN-T5- SMALL, BASE, và LARGE trên MMLU.
11

--- TRANG 12 ---
Cascade Speculative Drafting cho Suy luận LLM Nhanh hơn nữa
C. Chứng minh
C.1. Chứng minh cho Định lý 4.1
Chứng minh. Xác suất chấp nhận i token là αi−αi+1, với ngoại lệ của token thứ k+ 1, có xác suất αk được chấp nhận. Điều này là do nó yêu cầu tất cả i token đầu tiên được chấp nhận và token thứ i+ 1 bị từ chối để điều này xảy ra. Do đó,
ϕ(α,k)(x) =αkxk+1+∑k−1i=0(αi−αi+1)xi+1. (3)
Bằng cách sắp xếp lại các số hạng, chúng ta có thể đạt được một biểu thức dễ làm việc hơn nhiều
ϕ(α,k)(x) =x+∑ki=1αi(xi+1−xi) (4)
=x+ (x−1)∑ki=1αi(xi) (5)
=x+ (x−1)1−αi+1xi+1/(1−αx). (6)
C.2. Chứng minh cho Định lý 4.3
Chứng minh. Để α′=α(Md1,Md2). Chúng ta đầu tiên tính toán số lượng token dự kiến được tạo ra trong một bước của tầng dọc với Md1,Md2. Với tính chất của hàm tạo, hệ số của số hạng xj của ϕn(x) là xác suất của tổng độ dài chấp nhận của n bước suy đoán là j. Do đó, ϕn(x) đại diện cho hàm tạo xác suất ngay trước khi Mt thực hiện việc tạo ra.
Để đạt được số lượng chấp nhận token dự kiến của một hàm tạo xác suất, chúng ta tìm kiếm một toán tử có thể ánh xạ hàm tạo xác suất thành kỳ vọng mong muốn.
Để đạt được toán tử, chúng ta bắt đầu với một số hạng đa thức duy nhất của xj. May mắn thay, cho kết quả cuối cùng của 1−αj+1/(1−α) (Leviathan et al., 2023), toán tử Tα(f(x)) =1−αf(α)/(1−α) sẽ chuyển đổi xj thành 1−αj+1/(1−α). Ngoài ra, do tính tuyến tính của toán tử, điều này có thể được mở rộng cho bất kỳ đa thức nào. Do đó, chúng ta đạt được toán tử mong muốn để ánh xạ một hàm tạo xác suất thành kỳ vọng mong muốn.
Áp dụng toán tử Tα cho ϕn(x), chúng ta đạt được kết quả của 1−αϕn(α)/(1−α) cho số lượng token được chấp nhận dự kiến. Hơn nữa, vì số lần gọi Md1 là n, và Md2 được gọi k lần cho mỗi lần gọi Md1 cho tổng cộng nk lần gọi Md2. Chi phí thời gian là 1 +ncd1+nkcd2 điều này ngụ ý EWIF của hệ thống là 1−αϕn(α)/(1−α)(1+ncd1+nkcd2).
Đối với Hệ quả 4.4, vì cả 0< α < 1 và 0< α′<1, chúng ta có αi+1α′i+1< αα′, có nghĩa là 1−αk+1α′k+1/(1−αα′)<1. Cùng với α−1<0, chúng ta có ϕα′,k(α)<1 + (α−1) = α. Nếu chúng ta cũng để nkcd2= 0, chúng ta có 1−αϕn(α)/(1−α)(1+ncd1+nkcd2)> 1−ααn/(1−α)(1+ncd1), đó là EWIF cho giải mã suy đoán với kích thước bước n.
12
