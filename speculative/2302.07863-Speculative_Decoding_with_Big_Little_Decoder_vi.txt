# 2302.07863.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/speculative/2302.07863.pdf
# Kích thước tệp: 1185917 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Giải Mã Suy Đoán với Bộ Giải Mã Lớn Nhỏ
Sehoon Kim Karttikeya Mangalam Suhong Moon
Jitendra Malik1Michael W. Mahoney123Amir Gholami12Kurt Keutzer1
1University of California, Berkeley2ICSI3LBNL
{sehoonkim, mangalam, suhong.moon, malik, mahoneymw, amirgh, keutzer}@berkeley.edu

Tóm tắt
Sự xuất hiện gần đây của các Mô hình Ngôn ngữ Lớn dựa trên kiến trúc Transformer đã tạo ra những tiến bộ ấn tượng trong lĩnh vực Xử lý Ngôn ngữ Tự nhiên. Tuy nhiên, các mô hình này có độ trễ suy luận dài, điều này hạn chế việc triển khai và khiến chúng trở nên cực kỳ tốn kém cho các ứng dụng thời gian thực khác nhau. Độ trễ suy luận còn bị làm trầm trọng thêm bởi các tác vụ sinh tự hồi quy, vì các mô hình cần chạy lặp đi lặp lại để sinh token tuần tự mà không tận dụng song song hóa ở mức token. Để giải quyết vấn đề này, chúng tôi đề xuất Big Little Decoder (BiLD), một framework có thể cải thiện hiệu quả suy luận và độ trễ cho nhiều ứng dụng sinh văn bản. Framework BiLD chứa hai mô hình với kích thước khác nhau cộng tác để sinh văn bản. Mô hình nhỏ chạy tự hồi quy để sinh văn bản với chi phí suy luận thấp, và mô hình lớn chỉ được kích hoạt thỉnh thoảng để tinh chỉnh các dự đoán không chính xác của mô hình nhỏ theo cách không tự hồi quy. Để điều phối mô hình nhỏ và lớn, BiLD giới thiệu hai chính sách đơn giản nhưng hiệu quả: (1) chính sách dự phòng xác định khi nào chuyển quyền điều khiển cho mô hình lớn; và (2) chính sách hoàn trả xác định khi nào mô hình lớn cần sửa chữa các dự đoán không chính xác của mô hình nhỏ. Để đánh giá framework của chúng tôi trên các tác vụ và mô hình khác nhau, chúng tôi áp dụng BiLD cho các tình huống sinh văn bản khác nhau bao gồm dịch máy trên IWSLT 2017 De-En và WMT 2014 De-En, và tóm tắt trên XSUM và CNN/DailyMail. Trên GPU NVIDIA T4, framework của chúng tôi đạt được tốc độ tăng tốc lên đến 2.12× với sự suy giảm chất lượng sinh tối thiểu. Hơn nữa, framework của chúng tôi hoàn toàn plug-and-play và có thể được áp dụng mà không cần bất kỳ sửa đổi nào trong quá trình huấn luyện hoặc kiến trúc mô hình. Mã nguồn của chúng tôi được công khai¹.

1 Giới thiệu
Trong những năm gần đây, Transformer [63] đã trở thành kiến trúc mô hình tiêu chuẩn cho một loạt các tác vụ Xử lý Ngôn ngữ Tự nhiên. Tiềm năng của kiến trúc Transformer đã được tăng cường hơn nữa bởi sự xuất hiện của các Mô hình Ngôn ngữ Lớn (LLM) với hàng trăm tỷ tham số được huấn luyện trên các kho dữ liệu văn bản khổng lồ [2,47,50,10,23,7,55,83,62]. Mặc dù có hiệu suất tốt, việc chạy hiệu quả các mô hình này để suy luận là một thử thách do kích thước mô hình lớn và độ phức tạp thời gian chạy. Điều này hạn chế việc sử dụng chúng trong nhiều ứng dụng yêu cầu phản hồi thời gian thực.

Những bất cập về mặt tính toán này đặc biệt rõ rệt trong các tác vụ sinh tự hồi quy như dịch máy [3,1], tóm tắt [21], và mô hình hóa ngôn ngữ [41]. Đối với các tác vụ này, mô hình cần chạy lặp đi lặp lại để sinh token tuần tự, vì mỗi token phụ thuộc vào các token đã được sinh trước đó. Điều này yêu cầu các mô hình tải ma trận trọng số, cũng như các khóa và giá trị được cache của các token đã sinh trước đó [46], cho mỗi lần sinh token, do đó ngăn cản

¹https://github.com/kssteven418/BigLittleDecoder
Hội nghị lần thứ 37 về Hệ thống Xử lý Thông tin Neural (NeurIPS 2023).arXiv:2302.07863v4 [cs.CL] 12 Oct 2023

--- TRANG 2 ---
Mô hình Lớn
(#tham số ~10N)
<s>①Nhiều
Nhiều②bài
bài③hát
hát④nổi
nổi⑤tiếng
Mô hình Nhỏ
(#tham số N)
<s>①Nhiều
Nhiều②bài
bài③hát
hát④nổi
Mô hình Lớn
(#tham số ~10N)
<s> Nhiều bài hát nổi⑤' tiếng
Tự hồi quy (tuần tự) Tự hồi quy (tuần tự) Không tự hồi quy (song song)
Độ trễ: ①+ ②+ ③+ ④ + ⑤ Độ trễ: ①+ ②+ ③+ ④+ ⑤' (①+ ②+ ③+ ④+ ⑤ >>⑤')

Hình 1: Minh họa (Trái) quy trình giải mã tự hồi quy bình thường của một mô hình lớn và (Phải) BiLD bao gồm một mô hình nhỏ và một mô hình lớn. Trong BiLD, mô hình nhỏ sinh token tự hồi quy (tức là tuần tự) cho đến khi nó chuyển quyền điều khiển cho mô hình lớn. Mô hình lớn sau đó nhận đầu vào là các token được sinh bởi mô hình nhỏ song song, cho phép thực thi không tự hồi quy (tức là song song) để sinh token tiếp theo. Điều này cải thiện độ trễ end-to-end bằng cách cho phép sử dụng hiệu quả hơn phần cứng bên dưới.

song song hóa các giá trị được tải trên nhiều token. Điều này làm cho việc sinh văn bản tự hồi quy bị ràng buộc bởi băng thông bộ nhớ trong quá trình suy luận [8]. Kết quả là, các tác vụ sinh tự hồi quy bị hiệu quả sử dụng phần cứng thấp cũng như độ trễ suy luận cao [32]. Ngược lại, các tác vụ không tự hồi quy, như phân loại văn bản [65], có thể xử lý toàn bộ chuỗi đầu vào bằng một lần tải trọng số duy nhất, sau đó được chia sẻ trên tất cả các token đầu vào song song. Với sự phổ biến ngày càng tăng của các tác vụ sinh văn bản, theo sau những tiến bộ trong LLM, việc cải thiện độ trễ suy luận và hiệu quả thời gian chạy của các quy trình giải mã tự hồi quy là rất quan trọng mặc dù có thể phải hy sinh chất lượng sinh.

Để khắc phục điều này, giải mã không tự hồi quy [16,69,39,59,70,52,37,15,19] đã được khám phá để tối đa hóa song song hóa ở mức token và giảm độ trễ suy luận của các tác vụ sinh bằng cách sinh nhiều token đồng thời. Cách tiếp cận này có thể hiệu quả hơn về mặt tính toán so với quy trình tự hồi quy thông thường. Tuy nhiên, giải mã không tự hồi quy gặp phải các vấn đề về chất lượng sinh văn bản do giả định về tính độc lập có điều kiện giữa các token đầu ra [29]. Để đạt được hiệu suất tương đương với các quy trình tự hồi quy, nó thường yêu cầu các chiến lược huấn luyện phức tạp và thường phụ thuộc vào tác vụ, thông tin gợi ý bổ sung hướng dẫn quy trình giải mã [69, 39, 59, 70, 52], và chưng cất kiến thức [84].

Trong bài báo này, chúng tôi giới thiệu một framework mới có tên Big Little Decoder (BiLD) có thể được áp dụng cho các tình huống sinh văn bản khác nhau để giảm độ trễ suy luận mà không cần thêm vòng lặp huấn luyện hoặc sửa đổi pipeline huấn luyện hiện tại hoặc kiến trúc mô hình. Như được minh họa trong Hình 1 (Phải), framework BiLD bao gồm hai mô hình decoder, một mô hình lớn và mô hình nhỏ, cùng nhau sinh chuỗi văn bản. Cụ thể, chỉ mô hình nhỏ được thực thi tự hồi quy để sinh phần lớn văn bản, tận dụng chi phí thời gian chạy nhỏ của nó. Mô hình lớn chỉ tham gia thỉnh thoảng để tinh chỉnh các dự đoán không chính xác của mô hình nhỏ, do đó cho phép thực thi không tự hồi quy hiệu quả. Sơ đồ tự hồi quy nhỏ, không tự hồi quy lớn này dẫn đến cải thiện đáng kể lên đến ~2× trong độ trễ suy luận end-to-end, so với thực thi tự hồi quy thông thường, trong khi duy trì chất lượng sinh tương tự hoặc tốt hơn. Tính hiệu quả của framework chúng tôi cũng được hỗ trợ bởi quan sát của chúng tôi rằng các dự đoán được thực hiện bởi mô hình nhỏ và lớn chỉ hơi khác nhau, và do đó mô hình nhỏ có thể đạt được hiệu suất của mô hình lớn với việc tinh chỉnh tối thiểu các dự đoán của chính nó (Hình 2, Phần 3.1).

Tóm lại, những đóng góp chính của chúng tôi như sau:
• Chúng tôi giới thiệu BiLD, một framework tổng quát cho phép suy luận nhanh hơn của các ứng dụng sinh văn bản khác nhau. Framework của chúng tôi được thiết kế để điều phối một mô hình lớn và một mô hình nhỏ sao cho mô hình lớn chỉ được thực thi không thường xuyên và hiệu quả theo cách không tự hồi quy để tinh chỉnh các dự đoán không chính xác của mô hình nhỏ.
• Chúng tôi đề xuất hai chính sách cho BiLD: chính sách dự phòng cho phép mô hình nhỏ chuyển quyền điều khiển cho mô hình lớn nếu nó không đủ tự tin (Phần 3.3), và chính sách hoàn trả cho phép mô hình lớn xem xét và sửa chữa các dự đoán không chính xác của mô hình nhỏ (Phần 3.4).
• Chúng tôi giới thiệu một kỹ thuật đơn giản nhưng hiệu quả để căn chỉnh các dự đoán của mô hình nhỏ với những dự đoán của mô hình lớn. Bằng cách kết hợp kỹ thuật căn chỉnh dự đoán này vào framework BiLD, chúng tôi có thể tăng cường hiệu suất của nó hơn nữa với nỗ lực bổ sung tối thiểu (Phần 3.5.1).

--- TRANG 3 ---
• Chúng tôi áp dụng BiLD cho 4 tình huống sinh văn bản khác nhau bao gồm IWSLT 2017 De-En [3] và WMT 2014 De-En [1] cho dịch máy, XSUM [43] và CNN/DailyMail [21] cho tóm tắt. So với thực thi tự hồi quy hoàn toàn, BiLD đạt được tốc độ tăng tốc lên đến 1.85× mà không suy giảm chất lượng sinh và 2.12× cho phép suy giảm ~1 điểm trên GPU NVIDIA T4 (Phần 4.2).

2 Công trình liên quan
2.1 Suy luận Giải mã Transformer Hiệu quả
Nhiều cách tiếp cận đã được đề xuất để tăng tốc độ và giảm chi phí suy luận tổng thể của Transformer. Các cách tiếp cận nổi tiếng bao gồm thiết kế kiến trúc hiệu quả [26,33,36,60,67,75], lượng tử hóa [31,54,81,82,79,74,9], cắt tỉa [14,49,34,42,64,12,35], và tìm kiếm kiến trúc mạng nơ-ron [5,56,57,66,76,80]. Trong khi các phương pháp này thường phù hợp cho các tác vụ dựa trên Transformer, một số công trình đã tập trung vào các cơ chế giải mã hiệu quả để giảm chi phí của các tác vụ tự hồi quy.

Một hướng nghiên cứu phổ biến có sự tương đồng với công trình của chúng tôi là giải mã không tự hồi quy. Giải mã không tự hồi quy, còn được gọi là giải mã song song, lần đầu tiên được giới thiệu trong [16] như một phương pháp để giảm độ trễ suy luận bằng cách tạo ra nhiều token đầu ra song song, do đó tránh việc sinh văn bản tuần tự. Các công trình tiếp theo đã cải thiện hơn nữa hiệu suất của các mô hình không tự hồi quy bằng cách kết hợp thông tin phụ trợ hoặc gợi ý [69,39,59,70,52] để đảm bảo giải mã song song chính xác hơn, hoặc bằng cách cho phép nhiều vòng lặp bổ sung để tinh chỉnh bất kỳ dự đoán không chính xác nào [37,15,19]. Một sơ đồ giải mã đa vòng lặp như vậy cũng đã được đề xuất trong [71,17,58], sinh văn bản với ít bước hơn so với sơ đồ tự hồi quy bằng cách chèn hoặc xóa nhiều token mỗi vòng lặp. Tuy nhiên, các công trình này yêu cầu các chiến lược huấn luyện phức tạp và thường phụ thuộc vào tác vụ và/hoặc thông tin phụ trợ để đạt được hiệu suất tương đương với các mô hình tự hồi quy. Ngược lại, phương pháp của chúng tôi hướng đến một giải pháp plug-and-play không yêu cầu bất kỳ pipeline huấn luyện phức tạp nào.

Công trình của chúng tôi cũng liên quan đến các cách tiếp cận giảm chi phí giải mã bằng cách làm cho các decoder trở nên nông. [29] chứng minh rằng việc tăng độ sâu của encoder và giảm độ sâu của decoder có thể giảm độ trễ giải mã trong khi vẫn bảo tồn hiệu suất. CALM [51] gần đây giới thiệu thoát sớm, điều chỉnh động độ sâu của decoder cho mỗi lần sinh token bằng cách kết thúc suy luận ở lớp giữa, thay vì thực thi đến lớp cuối. Trong khi phương pháp của chúng tôi có cùng mục tiêu tăng tốc giải mã, chúng tôi sử dụng cách tiếp cận khác bằng cách cải thiện song song hóa giải mã thay vì bỏ qua tính toán không cần thiết. Ngoài ra, framework của chúng tôi cung cấp một số lợi thế so với CALM: (1) phương pháp của chúng tôi là cách tiếp cận hộp đen hoàn toàn không liên quan đến bất kỳ sửa đổi nào đối với cấu trúc mô hình, trong khi CALM yêu cầu các sửa đổi như truyền trạng thái cho các lớp bị bỏ qua; (2) cách tiếp cận của chúng tôi không yêu cầu thay đổi pipeline huấn luyện, trong khi CALM yêu cầu loss trung bình trên tất cả các lớp để đảm bảo tính nhất quán của lớp; (3) cách tiếp cận của chúng tôi cũng có thể được áp dụng mà không cần bất kỳ huấn luyện nào, điều này rất quan trọng trong các trường hợp sử dụng LLM khác nhau nơi huấn luyện là không khả thi hoặc cực kỳ tốn kém. Trong Phần 4.4, chúng tôi cũng cho thấy rằng chiến lược thoát sớm có thể được triển khai trong framework của chúng tôi để mang lại chất lượng sinh tốt hơn đáng kể, chứng minh thêm tính tổng quát của phương pháp chúng tôi đối với một loạt vấn đề rộng hơn.

2.2 Sử dụng Nhiều Mô hình
Việc điều phối sử dụng nhiều mô hình cũng đã được khám phá trong chưng cất kiến thức và học tập tổ hợp. Chưng cất kiến thức là một phương pháp được chấp nhận rộng rãi để tăng cường hiệu suất của các mô hình nhỏ hơn bằng cách huấn luyện chúng để sao chép hành vi của các mô hình lớn hơn, phức tạp hơn [22]. Khi được áp dụng cho kiến trúc Transformer, cách tiếp cận này liên quan đến việc chưng cất các logit cuối cùng [48,61] và/hoặc trạng thái ẩn của một mô hình lớn hơn, như bản đồ attention [60,28,68]. Ngược lại với chưng cất kiến thức, sử dụng kiến thức của mô hình lớn chỉ trong thời gian huấn luyện để cải thiện việc huấn luyện mô hình nhỏ hơn, phương pháp của chúng tôi là một giải pháp thời gian chạy được áp dụng trong quá trình giải mã. Do đó, cách tiếp cận của chúng tôi có thể thích ứng hơn với hành vi thời gian chạy và không thêm độ phức tạp vào việc huấn luyện.

Học tập tổ hợp là một cách tiếp cận khác để điều phối nhiều mô hình, trong đó nhiều mô hình được huấn luyện độc lập và các dự đoán của chúng được kết hợp để cải thiện hiệu suất tổng thể. Học tập tổ hợp đã được tìm thấy mang lại kết quả đầy hứa hẹn cho suy luận Transformer [44,25,77,40,24],

--- TRANG 4 ---
đặc biệt khi các mô hình được tổng hợp được pre-train trên các bộ dữ liệu khác nhau và sử dụng các kỹ thuật khác nhau. Tuy nhiên, học tập tổ hợp thường yêu cầu chạy nhiều mô hình và kết hợp các dự đoán của chúng tại thời gian chạy, điều này có thể tốn kém về mặt tính toán và không được tối ưu hóa cho độ trễ. Nghiên cứu của chúng tôi nhằm tối ưu hóa cả hiệu suất mô hình và độ trễ thời gian chạy.

Đồng thời và độc lập với công trình của chúng tôi, [38,4] cũng đề xuất một thuật toán thú vị để tăng tốc suy luận sinh bằng cách sử dụng một mô hình mạnh hơn để chấm điểm và lấy mẫu suy đoán các dự đoán từ một mô hình ít mạnh hơn. Trong khi [38,4] cung cấp các ước lượng không thiên vị khớp với phân phối xác suất của mô hình mạnh hơn, đánh giá thực nghiệm rộng rãi của chúng tôi cho thấy rằng cách tiếp cận của chúng tôi có thể mang lại sự đánh đổi hiệu suất-độ trễ vượt trội, do chính sách hoàn trả không ngẫu nhiên (tức là từ chối) cũng như kích thước cửa sổ dự phòng động. Xem Phần 4.2 và Phụ lục 7.3 để có so sánh chuyên sâu.

3 Phương pháp
3.1 Ví dụ Động lực

[Hình 2: Chất lượng sinh văn bản cho các tỷ lệ khác nhau của sự tham gia của mô hình lớn vào dự đoán của mô hình nhỏ, được đánh giá trên các bộ dữ liệu validation của (Trái) dịch WMT 2014 De-En [1]; và (Phải) tóm tắt CNN/DailyMail [21]. Chúng ta thấy rằng các mô hình nhỏ có thể đạt được chất lượng sinh tương đương hoặc tốt hơn so với các mô hình lớn nếu ~20% các dự đoán không chính xác của chúng được thay thế.]

Mặc dù các mô hình lớn có xu hướng tạo ra văn bản chất lượng cao hơn, chúng cũng dẫn đến độ trễ end-to-end dài hơn, điều này có thể bị làm trầm trọng thêm bởi quá trình hồi quy dự đoán một token tại một thời điểm. Tuy nhiên, trong nhiều tình huống sinh văn bản, chúng tôi chứng minh rằng một mô hình nhỏ hơn một bậc độ lớn so với mô hình lớn hơn có thể đạt được chất lượng sinh tương đương với mô hình lớn hơn, với điều kiện là một số dự đoán sai lầm được sửa chữa. Điều này ngụ ý rằng chỉ một phần nhỏ các dự đoán của mô hình nhỏ lệch khỏi những dự đoán của mô hình lớn hơn. Để xác nhận khẳng định này, chúng tôi đánh giá hai tình huống sinh khác nhau, dịch máy với mT5 [78] trên WMT 2014 De-En [1] và tóm tắt với T5 [47] trên CNN/DailyMail [21] bằng cách chạy mô hình lớn cùng với mô hình nhỏ cho mỗi vòng lặp giải mã. Xem Phần 4.1 để biết thêm chi tiết về các mô hình này. Sau đó, chúng tôi đo khả năng của mô hình lớn dự đoán cùng token mà mô hình nhỏ sinh ra. Nếu khả năng này dưới một ngưỡng nhất định, chúng tôi giả định rằng dự đoán của mô hình nhỏ không đủ chính xác, và chúng tôi thay thế nó bằng dự đoán của mô hình lớn. Bằng cách kiểm soát ngưỡng, chúng tôi có thể điều chỉnh tỷ lệ tham gia của mô hình lớn.

Hình 2 vẽ chất lượng sinh văn bản trên bộ dữ liệu validation của mỗi benchmark cho các tỷ lệ khác nhau của sự tham gia của mô hình lớn. Kết quả thể hiện một xu hướng rõ ràng trên các tác vụ nơi các mô hình nhỏ với kích thước nhỏ hơn ~10× có thể giữ được chất lượng sinh của mô hình lớn chỉ khi khoảng 20% các dự đoán không chính xác của chúng được thay thế bởi mô hình lớn. Trong khi thí nghiệm này giả định một trường hợp lý tưởng nơi các dự đoán của mô hình lớn có sẵn như ground truth trong mỗi vòng lặp, nó vẫn chứng minh tính khả thi của việc đạt được chất lượng sinh văn bản của mô hình lớn trong khi duy trì độ trễ suy luận thấp của mô hình nhỏ.

3.2 Công thức Vấn đề
Tại vòng lặp giải mã thứ n, mô hình nhỏ và mô hình lớn mỗi cái nhận đầu vào là một văn bản đầu ra được sinh một phần y₁:n₋₁ = (y₁,···, yn₋₁), và sau đó sinh một phân phối xác suất trên toàn bộ từ vựng pS(y|y₁:n₋₁) và pL(y|y₁:n₋₁), tương ứng. Sau đó, token tiếp theo yn,S và yn,L được lấy mẫu từ các phân phối xác suất,

yn,S ∼ pS(y|y₁:n₋₁) và yn,L ∼ pL(y|y₁:n₋₁). (1)

Tùy thuộc vào việc có sử dụng mô hình nhỏ hay mô hình lớn cho bước giải mã thứ n, token thứ n yn có thể là yn,S hoặc yn,L. Khi quyết định sử dụng mô hình nào, việc chạy mô hình lớn cùng với mô hình nhỏ cho mỗi bước giải mã để xác minh các dự đoán của mô hình nhỏ là không khả thi, như trong các thí nghiệm ở Phần 3.1. Do đó, cần thiết phải chuyển quyền điều khiển cho

--- TRANG 5 ---
Mô hình Nhỏ
(#tham số N)
<s>Nhiều
Nhiềunguời
ngườinổi
nổitiếng
Mô hình Lớn
(#tham số ~10N)
<s> Nhiều người nổi tiếngsoạn
làđang hát0.8 0.75 0.70.9
0.55Ngưỡng Dự phòng αFB=0.6
Dự phòng
Nhiều bài hát nổi tiếng?

Mô hình Nhỏ
(#tham số N)
<s>Nhiều
Nhiềunguời
ngườinổi
nổitiếng
Mô hình Lớn
(#tham số ~10N)
<s> Nhiều người nổi tiếngsoạn
làđang hátHoàn trả
Nhiều bài hát nổi tiếngkhoảng cách: d=3Ngưỡng Hoàn trả αRB=2
Văn bản được sinh: Nhiều bài hát nổi tiếng

Hình 3: (Trên) Chính sách dự phòng. Khi mô hình nhỏ sinh token tự hồi quy, nếu xác suất dự đoán của một token cụ thể dưới giá trị ngưỡng dự phòng được định trước αFB, dự đoán được coi là không đủ tự tin, và quyền điều khiển sau đó được chuyển cho mô hình lớn hơn để tạo ra token tương ứng. (Dưới) Chính sách hoàn trả. Nếu mô hình lớn tiếp quản quyền điều khiển, nó tạo ra các dự đoán riêng cho tất cả các token trước đó, cũng như token hiện tại. Nếu xác suất dự đoán từ mô hình lớn cho một token đã được sinh trước đó lệch khỏi dự đoán của mô hình nhỏ bằng một metric khoảng cách d vượt quá ngưỡng hoàn trả được định trước αRB, dự đoán của mô hình nhỏ được coi là không chính xác. Trong trường hợp như vậy, chúng ta hoàn trả tất cả các dự đoán được thực hiện bởi mô hình nhỏ theo sau token tương ứng.

mô hình lớn chỉ khi mô hình nhỏ có khả năng thực hiện dự đoán không chính xác dựa trên chính sách π(y₁:n₋₁) trả về giá trị boolean {0, 1} cho biết có sử dụng mô hình lớn hay không:

yn = yn,S nếu π(y₁:n₋₁) = 0
     yn,L nếu π(y₁:n₋₁) = 1 . (2)

Do đó, mục tiêu là thiết kế một chính sách π nhẹ dẫn đến chất lượng sinh văn bản cao với độ trễ end-to-end tối thiểu bằng cách gọi mô hình lớn chỉ khi cần thiết. Để minh họa cơ chế mà độ trễ được giảm, hãy xem xét một trường hợp đơn giản nơi mô hình nhỏ đã sinh các token y₁ đến yn tự hồi quy. Nếu mô hình lớn tiếp quản quyền điều khiển và dự đoán token tiếp theo, yn+1, bây giờ nó có thể nhận nhiều token đầu vào (y₁ đến yn) song song, do đó cho phép suy luận không tự hồi quy. Đáng chú ý rằng cách tiếp cận không tự hồi quy này sẽ yêu cầu cùng một lượng FLOP như cách tiếp cận hồi quy dự đoán y₁ đến yn+1 tuần tự; tuy nhiên, nó nhanh hơn nhiều trên phần cứng do song song hóa ở mức token và tăng cường độ số học [72]. Nói cách khác, xử lý nhiều token trong một thao tác bộ nhớ duy nhất hiệu quả hơn so với xử lý riêng lẻ các token bằng các thao tác bộ nhớ riêng biệt, vì các truy cập bộ nhớ có thể tốn kém hơn các thao tác số học trong các tác vụ giải mã [32]. Nếu việc tiết kiệm độ trễ từ việc chạy mô hình lớn không tự hồi quy vượt quá chi phí bổ sung của việc chạy mô hình nhỏ, có sự giảm độ trễ ròng. Do đó, mục tiêu của cách tiếp cận này không phải là giảm số lượng FLOP, mà là cải thiện việc sử dụng phần cứng và cường độ số học của quá trình giải mã. Phân tích chi tiết hơn về điều này có thể được tìm thấy trong Phụ lục 7.5.1. Hình 1 cung cấp một cái nhìn tổng quan cao về cách mô hình nhỏ và mô hình lớn trong BiLD phối hợp để sinh văn bản.

Bây giờ chúng tôi tập trung vào việc xây dựng một chính sách π phù hợp cho framework của chúng tôi. Ở đây, chúng tôi giới thiệu hai chính sách đơn giản, chính sách dự phòng và hoàn trả, mà (mặc dù đơn giản) dẫn đến hiệu suất cao với việc giảm độ trễ đáng kể. Chúng tôi thảo luận chi tiết trong các tiểu mục sau.

3.3 Chính sách Dự phòng: Mô hình Nhỏ Biết Khi Nào Dừng Dự đoán
Nguyên tắc đầu tiên của chính sách là mô hình nhỏ nên có thể xác định khi nào chuyển quyền điều khiển cho mô hình lớn. Bất cứ khi nào mô hình nhỏ thiếu tự tin trong dự đoán của mình, tốt hơn là cho phép mô hình lớn tiếp quản. Lượng hóa độ tin cậy (hoặc không chắc chắn, theo chiều ngược lại) đã là một lĩnh vực nghiên cứu tích cực [13,30], và bất kỳ metric độ tin cậy nhẹ nào cũng có thể phục vụ như một ứng viên tiềm năng. Ở đây, chúng tôi thấy đủ với một chính sách đơn giản dựa trên xác suất dự đoán tối đa, tức là max y pS(y|y₁:n₋₁), tương tự như các quan sát được thực hiện trong [20]. Nếu xác suất dự đoán tối đa thấp hơn một ngưỡng nhất định αFB, thì dự đoán của mô hình nhỏ được coi là không đủ tự tin, và chúng ta dự phòng sang mô hình lớn để sinh token tiếp theo. Lưu ý rằng điều này không gây ra chi phí thời gian chạy. Hình 3 (Trên) minh họa chính sách dự phòng.

Chính sách Dự phòng: Nếu max y pS(y|y₁:n₋₁) < αFB, thì dự phòng sang mô hình lớn và đặt yn = yn,L.

3.4 Chính sách Hoàn trả: Mô hình Lớn Biết Khi Nào Hoàn nguyên Dự đoán
Trong khi chính sách dự phòng cho phép mô hình lớn tiếp quản khi mô hình nhỏ không đủ tự tin, vẫn có thể mô hình nhỏ quá tự tin trong các dự đoán không chính xác của mình [18]. Hơn nữa, một dự đoán không chính xác duy nhất ở vòng lặp giải mã sớm có thể dẫn đến hiệu ứng thảm khốc [51], vì nó sẽ ảnh hưởng đến tất cả các dự đoán token tiếp theo. Để tránh các trường hợp như vậy, mong muốn có mô hình lớn xem xét các dự đoán của mô hình nhỏ và đảm bảo tính hợp lệ của mỗi dự đoán.

Trong framework của chúng tôi, điều này không có bất kỳ chi phí bổ sung nào. Khi mô hình lớn được cung cấp các token được sinh bởi mô hình nhỏ cho dự đoán không tự hồi quy của token tiếp theo, nó cũng tạo ra các dự đoán riêng cho tất cả các bước giải mã trước đó. Điều đó có nghĩa là, cho văn bản được sinh một phần y₁:n, nó sinh pL(y|y₁:m) cho tất cả các bước giải mã trước đó và hiện tại m = 1,···, n, có thể được sử dụng để xác nhận các dự đoán trước đó của mô hình nhỏ.

Do đó, đối với một số metric khoảng cách d(·,·) so sánh hai phân phối xác suất, chúng ta tìm bước giải mã nhỏ nhất m sao cho

d(pS(y|y₁:m), pL(y|y₁:m)) > αRB (3)

cho một ngưỡng được định trước αRB. Nếu m như vậy tồn tại, chúng ta coi dự đoán trước đó ym của mô hình nhỏ là không chính xác, và chúng ta hoàn trả tất cả các dự đoán theo sau, tức là ym đến yn, vì tất cả chúng đều phụ thuộc vào dự đoán sai. Sau đó chúng ta thay thế ym bằng ym,L của mô hình lớn. Chúng tôi sẽ thảo luận trong Phần 4.2 rằng cross-entropy loss giữa nhãn cứng của mô hình nhỏ và nhãn mềm của mô hình lớn (đo khả năng thu được dự đoán của mô hình nhỏ từ đầu ra của mô hình lớn) là một lựa chọn tốt cho metric d. Hoàn trả có thể gây ra độ trễ bổ sung do cần tính toán trùng lặp cho các token bị hoàn nguyên. Tuy nhiên, chúng tôi chứng minh trong Phần 4.3 lợi thế ròng của hoàn trả vì chất lượng sinh văn bản được cải thiện vượt trội hơn độ trễ bổ sung. Xem Hình 3 (Dưới) để có minh họa chi tiết về chính sách hoàn trả.

Chính sách Hoàn trả: Nếu tồn tại một m tối thiểu ∈ [1, n-1] sao cho d(pS(y|y₁:m), pL(y|y₁:m)) > αRB, thì hoàn trả các dự đoán (ym,···, yn) và đặt ym = ym,L.

3.5 Big Little Decoder

[Thuật toán 1: Big Little Decoder
1: y ← [<s>]
2: while y[-1] ≠ <eos>
3:   pS ← SmallModel(y)
4:   if max(pS[-1]) > αFB
5:     # Sử dụng dự đoán của mô hình nhỏ
6:     y ← y + [sample(pS[-1])]
7:   else
8:     # Dự phòng sang mô hình lớn
9:     pL ← LargeModel(y)
10:    m ← min. index sao cho d(pL[m], pS[m]) > αRB
11:    if m tồn tại
12:      # Hoàn trả: sử dụng dự đoán của mô hình lớn
13:      y ← y[:m] + [sample(pL[m])]
14:    else
15:      # Không hoàn trả: sử dụng dự đoán của mô hình lớn
16:      y ← y + [sample(pL[-1])]
17: return y]

Kết hợp lại, framework Big Little Decoder (BiLD) bao gồm một mô hình nhỏ, một mô hình lớn, và một chính sách xác định sử dụng mô hình nào cho mỗi vòng lặp giải mã. Chính sách bao gồm hai thành phần: chính sách dự phòng để dự phòng sang mô hình lớn khi dự đoán của mô hình nhỏ không đủ tự tin; và chính sách hoàn trả để hoàn trả các dự đoán của mô hình nhỏ nếu chúng lệch khỏi các dự đoán của mô hình lớn. Thuật toán 1 cung cấp một tóm tắt về thuật toán end-to-end.

3.5.1 Căn chỉnh Dự đoán Mô hình
BiLD là một framework tổng quát không áp đặt hạn chế nào về việc lựa chọn mô hình nhỏ và lớn miễn là chúng sử dụng cùng từ vựng. Do đó, như sẽ được chứng minh trong Phần 4.2, hai

--- TRANG 6 ---
mô hình được huấn luyện độc lập có thể tạo thành BiLD để đạt được cải thiện độ trễ đáng kể. Tuy nhiên, khi hai mô hình được huấn luyện riêng biệt, chúng có thể sinh chuỗi với ý nghĩa ngữ nghĩa tương tự hoặc giống hệt nhau nhưng sử dụng từ vựng khác nhau. Ví dụ, một mô hình có thể tạo ra cụm từ "viết rất khó" trong khi mô hình khác có thể sinh "viết rất khó khăn". Bởi vì chính sách BiLD dựa vào mức độ đồng ý giữa các mô hình lớn và nhỏ, sự khác biệt ở mức từ vựng như vậy có thể dẫn đến những bất đồng không cần thiết làm hoàn trả dự đoán của mô hình nhỏ mà không có bất kỳ cải thiện nào về chất lượng sinh.

Để giải quyết vấn đề này và tối ưu hóa hiệu suất BiLD hơn nữa, chúng tôi trình bày một cách tiếp cận đơn giản gọi là căn chỉnh dự đoán mô hình giúp căn chỉnh các dự đoán được tạo ra bởi các mô hình nhỏ và lớn. Để đạt được điều này, chúng tôi tận dụng một bộ dữ liệu hiệu chuẩn Xcal = {x(i)} đại diện tốt cho phân phối câu đầu vào. Sau đó chúng tôi sinh chuỗi đầu ra tương ứng cho mỗi chuỗi đầu vào bằng mô hình lớn, dẫn đến Ycal = {y(i)} trong đó y(i) = arg max pL(y|x(i)). Tiếp theo, chúng tôi fine-tune mô hình nhỏ sử dụng các ví dụ hiệu chuẩn (xcal, ycal) ∈ (Xcal, Ycal).

Lý do cơ bản của cách tiếp cận này là tăng khả năng mô hình nhỏ sinh các chuỗi mà mô hình lớn sẽ sinh ra. Điều này có thể giảm thiểu khoảng cách giữa các dự đoán của mô hình nhỏ và mô hình lớn cho mỗi token, tức là d(pS(y|x, y1:m), pL(y|x, y1:m)), trong suốt quá trình giải mã, do đó tránh những lần hoàn trả không cần thiết. Mặc dù đơn giản, các thí nghiệm của chúng tôi trong Phần 4.2 chứng minh rằng cách tiếp cận này có thể được kết hợp vào framework BiLD với nỗ lực tối thiểu để tăng cường đáng kể hiệu suất. Chúng tôi nhấn mạnh thêm rằng phương pháp này không đưa thêm bất kỳ độ phức tạp hoặc siêu tham số nào vào pipeline huấn luyện bình thường. Điều này có thể so sánh với chưng cất kiến thức [22], một phương pháp thay thế để căn chỉnh các dự đoán mô hình, yêu cầu sửa đổi pipeline huấn luyện, truy cập vào các trạng thái ẩn bên trong như logit, và điều chỉnh siêu tham số bổ sung.

4 Đánh giá
4.1 Thiết lập Thí nghiệm
Mô hình và Bộ dữ liệu. Để truy cập tính tổng quát và hiệu lực của BiLD trong các thiết lập sinh văn bản khác nhau, chúng tôi đã chọn IWSLT 2017 De-En [3] và WMT 2014 De-En [1] cho các benchmark dịch máy và XSUM [43] và CNN/DailyMail [21] cho các benchmark tóm tắt. Chúng tôi sử dụng mT5-large và small [78] cho dịch máy và T5-large và small [47] cho tóm tắt làm các mô hình mục tiêu của chúng tôi, trong đó kích thước của các mô hình khác nhau khoảng 20 lần. Framework của chúng tôi được xây dựng trên PyTorch [45] và thư viện HuggingFace Transformers [73] cùng với các checkpoint pre-train của chúng.

Huấn luyện. Chúng tôi fine-tune các mô hình pre-train trên các benchmark mục tiêu trong 500k bước để có được các mô hình nhỏ và lớn cơ sở. Để huấn luyện các mô hình nhỏ được căn chỉnh thông qua phương pháp căn chỉnh dự đoán (Phần 3.5.1), chúng tôi sinh các chuỗi đầu ra từ các chuỗi đầu vào của các bộ dữ liệu huấn luyện sử dụng các mô hình lớn được huấn luyện hoàn toàn để tạo các bộ dữ liệu hiệu chuẩn. Sau đó chúng tôi fine-tune các mô hình nhỏ pre-train trên bộ dữ liệu hiệu chuẩn sử dụng cùng công thức huấn luyện và số bước như các mô hình nhỏ cơ sở. Thêm chi tiết huấn luyện có thể được tìm thấy trong Phụ lục 7.1.1. Trong suốt bài báo, chúng tôi gọi BiLD với các mô hình nhỏ cơ sở và được căn chỉnh là BiLD không căn chỉnh và căn chỉnh, tương ứng.

Suy luận. Tất cả các đánh giá suy luận được thực hiện trên một GPU NVIDIA T4 duy nhất của một instance GCP n1-standard-4, sử dụng batch size 1, đây là trường hợp sử dụng phổ biến cho phục vụ trực tuyến [51]. Đối với metric khoảng cách d trong Phương trình 3 cho chính sách hoàn trả, chúng tôi sử dụng cross-entropy loss giữa nhãn cứng của mô hình nhỏ và nhãn mềm của mô hình lớn. Đối với suy luận BiLD, chúng tôi quét qua các ngưỡng dự phòng và hoàn trả khác nhau để khám phá các sự đánh đổi khác nhau giữa chất lượng sinh và độ trễ. Thêm chi tiết đánh giá có thể được tìm thấy trong Phụ lục 7.1.2.

4.2 Kết quả Chính
Các kết quả chính được minh họa trong Hình 4, cho thấy sự đánh đổi giữa chất lượng sinh văn bản và độ trễ end-to-end trung bình trên mỗi ví dụ, được chuẩn hóa bởi độ trễ suy luận vanilla của các mô hình lớn cơ sở thuần túy. Các sự đánh đổi được thu được bằng cách kiểm soát các ngưỡng dự phòng và hoàn trả. Bảng 1 tóm tắt kết quả, với hàng thứ hai và thứ ba tương ứng với BiLD không căn chỉnh

--- TRANG 7 ---
Bảng 1: Tóm tắt Hình 4 so sánh chất lượng sinh và tăng tốc độ trễ của BiLD với suy luận vanilla với các mô hình lớn cơ sở. Hàng đầu tiên báo cáo suy luận vanilla, và hàng thứ hai và thứ ba báo cáo BiLD không căn chỉnh. Hàng thứ tư và thứ năm báo cáo BiLD căn chỉnh. Trong cả hai trường hợp BiLD không căn chỉnh và căn chỉnh, chúng tôi báo cáo tăng tốc với suy giảm điểm BLEU/ROUGE-L tối thiểu (hàng thứ hai và thứ tư), và trong khoảng suy giảm ~1 điểm (hàng thứ ba và thứ năm).

[THIS IS TABLE: Shows comparison of different tasks (Machine Translation with mT5, Summarization with T5) across datasets (IWSLT, WMT, XSUM, CNN/DailyMail) with BLEU/ROUGE-L scores and speedup values for different BiLD configurations]

BiLD. Khi kết hợp với các mô hình nhỏ cơ sở được fine-tune bình thường, BiLD đạt được tăng tốc trung bình 1.50× trên tất cả các benchmark, với tăng tốc lên đến 1.71× trên CNN/DailyMail mà không có bất kỳ suy giảm nào về chất lượng sinh văn bản (hàng thứ 2). Bằng cách cho phép suy giảm ~1 điểm, BiLD đạt được tăng tốc trung bình 1.70×, với tăng tốc lên đến 2.05× (hàng thứ 3). Lưu ý rằng BiLD không căn chỉnh là một giải pháp plug-and-play thuần túy không yêu cầu nỗ lực hoặc chi phí huấn luyện bổ sung ngoài việc chuẩn bị các mô hình nhỏ và lớn độc lập.

Ngoài ra, Hình 4 cho thấy hiệu quả của phương pháp căn chỉnh dự đoán, dẫn đến cải thiện nhất quán của BiLD căn chỉnh so với BiLD không căn chỉnh. Như được tóm tắt trong hàng thứ tư và thứ năm của Bảng 1, BiLD căn chỉnh kết hợp các mô hình nhỏ được căn chỉnh mang lại tăng tốc trung bình 1.61×, với tăng tốc lên đến 1.85× (hàng thứ 4). Trong khoảng suy giảm ~1 điểm, nó đạt được tăng tốc trung bình 1.85×, với tăng tốc lên đến 2.12× (hàng thứ 5). Kết quả cũng chứng minh rằng cả BiLD không căn chỉnh và căn chỉnh đều vượt trội hơn điểm BLEU/ROUGE-L cơ sở trong chế độ độ trễ cao, có thể được gán cho hiệu ứng tổ hợp của việc sử dụng hai mô hình khác nhau, như cũng được nghiên cứu trong công trình trước [40]. Trong Phụ lục 7.5.2, chúng tôi cung cấp các ví dụ về chuỗi văn bản được sinh bởi BiLD, chứng minh rằng sự tham gia của mô hình lớn trong giải mã BiLD không chỉ cải thiện độ chính xác dự đoán mà còn ngăn chặn các dự đoán không chính xác khỏi việc tác động đến những dự đoán tương lai.

Chúng tôi đã thực hiện thêm so sánh hiệu suất của phương pháp chúng tôi với phương pháp lấy mẫu suy đoán được đề xuất trong [4] trên các benchmark IWSLT 2017 De-En và XSUM. Chúng tôi triển khai và đánh giá nó trong cùng môi trường với các thí nghiệm BiLD chính của chúng tôi sử dụng cùng các mô hình lớn và nhỏ cơ sở. Chúng tôi áp dụng kích thước cửa sổ cố định [3, 10]. Trên benchmark IWSLT, lấy mẫu suy đoán đạt điểm BLEU 39.93 với tăng tốc 1.28×, trong khi BiLD (không căn chỉnh) đạt điểm BLEU cao hơn 0.61 với tăng tốc tương tự, hoặc tăng độ trễ thêm 0.21× với điểm BLEU tương tự. Trên benchmark XSUM, lấy mẫu suy đoán đạt điểm ROUGE-L 35.00 với tăng tốc 1.25×. Ngược lại, BiLD đạt được tăng điểm ROUGE-L lên đến 0.30 với độ trễ nhanh hơn, hoặc tăng độ trễ lên đến 0.22× với điểm ROUGE-L tốt hơn. Chúng tôi cung cấp so sánh chi tiết hơn trong Phụ lục 7.3.

--- TRANG 8 ---
[Hình 4: Chất lượng sinh và độ trễ end-to-end trung bình của việc xử lý một ví dụ duy nhất trên 4 benchmark khác nhau. Chúng tôi báo cáo BLEU cho dịch máy và ROUGE-L cho tóm tắt làm các metric hiệu suất. Các đường màu xanh lá cây và xanh dương là BiLD không căn chỉnh và căn chỉnh, tương ứng. Các dấu X là suy luận vanilla với các mô hình lớn cơ sở. Để so sánh, hai đường ngang được vẽ để chỉ ra điểm BLEU/ROUGE-L của suy luận vanilla và suy giảm 1 điểm từ đó. Độ trễ trên trục x được chuẩn hóa bởi độ trễ cơ sở.]

--- TRANG 9 ---
[Hình 5: Kết quả nghiên cứu ablation cho BiLD trên (Trái) tác vụ dịch IWSLT 2017 De-En và (Phải) tác vụ tóm tắt XSUM mà không có chính sách hoàn trả hoặc dự phòng. Các mô hình nhỏ được căn chỉnh được sử dụng trong tất cả các trường hợp. Kết quả chứng minh rằng BiLD trải qua suy giảm hiệu suất đáng kể mà không có chính sách nào trong cả hai tác vụ. Các đường ngang chỉ ra điểm suy luận vanilla và suy giảm 1 điểm từ đó.]

[Hình 6: Ứng dụng framework BiLD cho vấn đề thoát sớm sử dụng mô hình mT5-small làm mô hình lớn và lớp đầu tiên của nó làm mô hình nhỏ, được đánh giá trên (Trái) benchmark IWSLT 2017 De-En và (Phải) WMT 2014 De-En. Các dấu × chỉ ra độ trễ và điểm BLEU của các mô hình mT5-small. Các đường ngang chỉ ra điểm suy luận vanilla và suy giảm 1 điểm từ đó.]

4.3 Nghiên cứu Ablation
Chúng tôi đã thực hiện thêm hai nghiên cứu ablation để xác nhận các thành phần riêng lẻ của BiLD bằng cách (1) loại bỏ chính sách hoàn trả, và (2) loại bỏ chính sách dự phòng. Khi loại bỏ chính sách hoàn trả, chúng tôi sử dụng cùng ngưỡng dự phòng như các thí nghiệm chính để kiểm soát sự đánh đổi chất lượng sinh và độ trễ. Khi loại bỏ chính sách dự phòng, chúng tôi sử dụng cùng ngưỡng hoàn trả như các thí nghiệm chính. Ngoài ra, chúng tôi áp dụng dự phòng sau một số lượng cố định các lần thực thi mô hình nhỏ (quét qua [3, 10]), tương tự như [4].

Hình 5 minh họa kết quả của các nghiên cứu ablation này trên IWSLT 2017 De-En cho dịch máy và XSUM cho tóm tắt với BiLD căn chỉnh. Kết quả cho thấy rằng chính sách hoàn trả nhất quán tạo ra chất lượng sinh tốt hơn trên tất cả các chế độ độ trễ, đặc biệt trong chế độ BLEU/ROUGE cao nơi sự tham gia của mô hình lớn thông qua hoàn trả là rất quan trọng trong việc sửa chữa các dự đoán sai của mô hình nhỏ. Điều này chứng minh rằng, mặc dù chi phí độ trễ bổ sung từ tính toán trùng lặp của các token bị hoàn nguyên, việc cải thiện chất lượng sinh văn bản vượt trội hơn chi phí này. Tương tự, việc loại bỏ chính sách dự phòng và định kỳ chuyển quyền điều khiển cho mô hình lớn sau một số lượng cố định các lần sinh token dẫn đến suy giảm hiệu suất đáng kể. Kết hợp lại, những kết quả này nhấn mạnh rằng cả hai chính sách đều là các thành phần quan trọng của BiLD.

4.4 Chiến lược Thoát Sớm trong Framework BiLD
Cho đến nay, chúng tôi đã chứng minh cách BiLD có thể được sử dụng như một framework tổng quát để tăng tốc quá trình sinh văn bản bằng cách kết hợp một mô hình nhỏ và một mô hình lớn. Tuy nhiên, việc có hai mô hình riêng biệt không phải là một hạn chế vì chúng có thể được kết hợp thành một mô hình duy nhất bằng cách sử dụng một tập con của mô hình lớn hơn, chẳng hạn như một số lớp đầu của nó, làm mô hình nhỏ hơn. Cách tiếp cận này giống với chiến lược thoát sớm, là một phương pháp phổ biến để tăng tốc quá trình giải mã [51]. Phần này chứng minh cách chiến lược thoát sớm có thể được tái khung hóa trong framework BiLD.

Để chứng minh khả năng áp dụng của việc sử dụng chiến lược thoát sớm trong framework BiLD, chúng tôi sử dụng mô hình mT5-small làm mô hình lớn và lớp đầu tiên (trong số 8) làm mô hình nhỏ, và đánh giá nó trên hai benchmark dịch máy: IWSLT 2017 De-En và WMT 2014 De-En. Để đảm bảo tính nhất quán giữa dự đoán được thực hiện sau lớp đầu tiên và dự đoán được thực hiện sau lớp cuối cùng, chúng tôi huấn luyện mô hình với loss trung bình của hai lớp này, tương tự như [11,51]. Đầu dự đoán được chia sẻ cho hai lớp này. Thêm chi tiết huấn luyện và đánh giá có thể được tìm thấy trong Phụ lục 7.2.1.

Hình 6 minh họa kết quả, nơi cho mỗi benchmark, BiLD đạt được tăng tốc lên đến 1.60× và 1.74× trong phạm vi sụt giảm điểm BLEU ít hơn một điểm, tương ứng. Điều này chứng minh tính mở rộng của framework BiLD đối với các vấn đề thoát sớm. Trong Phụ lục 7.2.2, chúng tôi cung cấp thêm so sánh chi tiết kết quả của chúng tôi với CALM [51], một framework khác kết hợp thoát sớm cho giải mã Transformer nhanh. So với CALM, BiLD cung cấp hai lợi thế góp phần vào chất lượng sinh tốt hơn: (1) trong BiLD, ngay cả khi dự đoán thoát sớm (tức là dự đoán được thực hiện bởi mô hình nhỏ hơn) không chính xác, nó có thể được sửa chữa và thay thế bằng chính sách hoàn trả; (2) các cache khóa và giá trị cho các lớp bị bỏ qua được điền bằng các giá trị thực tế thay vì được tính toán từ các trạng thái ẩn của lớp thoát, dẫn đến giảm lan truyền lỗi và cải thiện tính ổn định giải mã. Kết quả là, khi được thử nghiệm trên IWSLT 2017 De-En và WMT 2014 De-En sử dụng mT5-small, BiLD đạt được cải thiện điểm BLEU lên đến ~2 điểm so với CALM trong cả hai bộ dữ liệu (Hình 7).

--- TRANG 10 ---
5 Kết luận
Trong công trình này, chúng tôi đã giới thiệu Big Little Decoder (BiLD), một framework giảm độ trễ suy luận end-to-end cho nhiều tác vụ sinh văn bản mà không cần huấn luyện hoặc sửa đổi các mô hình hiện tại. Về bản chất, framework của chúng tôi kết hợp một mô hình decoder lớn và nhỏ cùng nhau để sinh văn bản hiệu quả hơn. Cụ thể, chúng tôi bắt đầu suy luận với một mô hình nhỏ chạy tự hồi quy trong phần lớn thời gian để sinh văn bản với chi phí suy luận thấp, trong khi mô hình lớn được thực thi không tự hồi quy để tinh chỉnh các dự đoán không chính xác của mô hình nhỏ. BiLD kết hợp hai chính sách, chính sách dự phòng, chuyển quyền điều khiển cho mô hình lớn khi mô hình nhỏ không chắc chắn, và chính sách hoàn trả, cho phép mô hình lớn hoàn nguyên các dự đoán không chính xác của mô hình nhỏ. Framework của chúng tôi được đánh giá trên các tình huống sinh văn bản khác nhau, bao gồm dịch máy, tóm tắt, và mô hình hóa ngôn ngữ. Chạy trên GPU NVIDIA Titan Xp, mà không có sụt giảm hiệu suất BiLD đạt được tăng tốc trung bình 1.52×, với cải thiện lên đến 2.18× trên một số tác vụ. Hơn nữa, khi cho phép suy giảm 1 điểm hiệu suất, BiLD đạt được tăng tốc trung bình 1.76× với tăng tốc lên đến 2.38× trên một số tác vụ.

6 Lời cảm ơn
Chúng tôi cảm ơn sự hỗ trợ hào phóng từ Google Cloud, đội ngũ Google TRC, và đặc biệt Jonathan Caton, Giáo sư David Patterson, và Tiến sĩ Ed Chi. Phòng thí nghiệm của Giáo sư Keutzer được tài trợ bởi tập đoàn Intel, đội ngũ Intel VLAB, trung tâm xuất sắc Intel One-API, cũng như nguồn tài trợ thông qua BDD và BAIR. Sehoon Kim và Suhong Moon muốn cảm ơn sự hỗ trợ từ Korea Foundation for Advanced Studies (KFAS). Amir Gholami được hỗ trợ thông qua tài trợ từ Samsung SAIT. Michael W. Mahoney cũng muốn cảm ơn Giải thưởng Nghiên cứu Khoa học J. P. Morgan Chase cũng như DOE, NSF, và ONR. Kết luận của chúng tôi không nhất thiết phản ánh vị trí hoặc chính sách của các nhà tài trợ, và không nên suy ra bất kỳ sự chứng thực chính thức nào.

Tài liệu tham khảo
[1]Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ale s Tamchyna. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12–58, Baltimore, Maryland, USA, June 2014. Association for Computational Linguistics.

[2]Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.

[3]Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian Stüker, Katsuhito Sudoh, Koichiro Yoshino, and Christian Federmann. Overview of the IWSLT 2017 evaluation campaign. In Proceedings of the 14th International Conference on Spoken Language Translation, pages 2–14, Tokyo, Japan, December 14-15 2017. International Workshop on Spoken Language Translation.

[4] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023.

[5]Daoyuan Chen, Yaliang Li, Minghui Qiu, Zhen Wang, Bofang Li, Bolin Ding, Hongbo Deng, Jun Huang, Wei Lin, and Jingren Zhou. Adabert: Task-adaptive bert compression with differentiable neural architecture search. arXiv preprint arXiv:2001.04246, 2020.

[6]Patrick H Chen and Cho-jui Hsieh. A comparison of second-order methods for deep convolutional neural networks. openreview under ICLR 2018, 2018.

--- TRANG 11 ---
[7]Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

[8]Michiel de Jong, Yury Zemlyanskiy, Joshua Ainslie, Nicholas FitzGerald, Sumit Sanghai, Fei Sha, and William Cohen. Fido: Fusion-in-decoder optimized for stronger performance and faster inference. arXiv preprint arXiv:2212.08153, 2022.

[9]Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8(): 8-bit matrix multiplication for transformers at scale. In Advances in Neural Information Processing Systems, 2022.

[10] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pages 5547–5569. PMLR, 2022.

[11] Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli. Depth-adaptive transformer. arXiv preprint arXiv:1910.10073, 2019.

[12] Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with structured dropout. arXiv preprint arXiv:1909.11556, 2019.

[13] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pages 1050–1059. PMLR, 2016.

[14] Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv preprint arXiv:1902.09574, 2019.

[15] Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel decoding of conditional masked language models. arXiv preprint arXiv:1904.09324, 2019.

[16] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-autoregressive neural machine translation. arXiv preprint arXiv:1711.02281, 2017.

[17] Jiatao Gu, Changhan Wang, and Junbo Zhao. Levenshtein transformer. Advances in Neural Information Processing Systems, 32, 2019.

[18] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International conference on machine learning, pages 1321–1330. PMLR, 2017.

[19] Junliang Guo, Linli Xu, and Enhong Chen. Jointly masked sequence-to-sequence model for non-autoregressive neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 376–385, 2020.

[20] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016.

[21] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Advances in neural information processing systems, pages 1693–1701, 2015.

[22] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. Workshop paper in NIPS, 2014.

[23] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.

[24] Chris Hokamp, Demian Gholipour Ghalandari, Nghia The Pham, and John Glover. Dyne: Dynamic ensemble decoding for multi-document summarization. arXiv preprint arXiv:2006.08748, 2020.

[25] Ngo Quang Huy, Tu Minh Phuong, and Ngo Xuan Bach. Autoencoding language model based ensemble learning for commonsense validation and explanation. arXiv preprint arXiv:2204.03324, 2022.

[26] Forrest N Iandola, Albert E Shaw, Ravi Krishna, and Kurt W Keutzer. Squeezebert: What can computer vision teach nlp about efficient neural networks? arXiv preprint arXiv:2006.11316, 2020.

--- TRANG 12 ---
[27] Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. arXiv preprint arXiv:2306.02561, 2023.

[28] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351, 2019.

[29] Jungo Kasai, Nikolaos Pappas, Hao Peng, James Cross, and Noah A Smith. Deep encoder, shallow decoder: Reevaluating non-autoregressive machine translation. arXiv preprint arXiv:2006.10369, 2020.

[30] Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? Advances in neural information processing systems, 30, 2017.

[31] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. I-bert: Integer-only bert quantization. arXiv preprint arXiv:2101.01321, 2021.

[32] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W Mahoney, et al. Full stack optimization of transformer inference: a survey. arXiv preprint arXiv:2302.14017, 2023.

[33] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2019.

[34] Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar, Mark Kurtz, Benjamin Fineran, Michael Goin, and Dan Alistarh. The optimal bert surgeon: Scalable and accurate second-order pruning for large language models. arXiv preprint arXiv:2203.07259, 2022.

[35] Woosuk Kwon, Sehoon Kim, Michael W Mahoney, Joseph Hassoun, Kurt Keutzer, and Amir Gholami. A fast post-training pruning framework for transformers. arXiv preprint arXiv:2204.09656, 2022.

[36] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.

[37] Jason Lee, Elman Mansimov, and Kyunghyun Cho. Deterministic non-autoregressive neural sequence modeling by iterative refinement. arXiv preprint arXiv:1802.06901, 2018.

[38] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 19274–19286. PMLR, 2023.

[39] Zhuohan Li, Zi Lin, Di He, Fei Tian, Tao Qin, Liwei Wang, and Tie-Yan Liu. Hint-based training for non-autoregressive machine translation. arXiv preprint arXiv:1909.06708, 2019.

[40] Yoshitomo Matsubara, Luca Soldaini, Eric Lind, and Alessandro Moschitti. Ensemble transformer for efficient and accurate ranking tasks: an application to question answering systems. arXiv preprint arXiv:2201.05767, 2022.

[41] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016.

[42] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? arXiv preprint arXiv:1905.10650, 2019.

[43] Shashi Narayan, Shay B Cohen, and Mirella Lapata. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. arXiv preprint arXiv:1808.08745, 2018.

[44] Liu Pai. Qiaoning at semeval-2020 task 4: Commonsense validation and explanation system based on ensemble of language model. In Proceedings of the Fourteenth Workshop on Semantic Evaluation, pages 415–421, 2020.

[45] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.

[46] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. arXiv preprint arXiv:2211.05102, 2022.

--- TRANG 13 ---
[47] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020.

[48] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.

[49] Victor Sanh, Thomas Wolf, and Alexander Rush. Movement pruning: Adaptive sparsity by fine-tuning. Advances in Neural Information Processing Systems, 33:20378–20389, 2020.

[50] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.

[51] Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Q Tran, Yi Tay, and Donald Metzler. Confident adaptive language modeling. arXiv preprint arXiv:2207.07061, 2022.

[52] Chenze Shao, Jinchao Zhang, Yang Feng, Fandong Meng, and Jie Zhou. Minimizing the bag-of-ngrams difference for non-autoregressive neural machine translation. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 198–205, 2020.

[53] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596–4604, 2018.

[54] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-BERT: Hessian based ultra low precision quantization of bert. In AAAI, pages 8815–8821, 2020.

[55] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022.

[56] David So, Quoc Le, and Chen Liang. The evolved transformer. In International Conference on Machine Learning, pages 5877–5886. PMLR, 2019.

[57] David R So, Wojciech Mańke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc V Le. Primer: Searching for efficient transformers for language modeling. arXiv preprint arXiv:2109.08668, 2021.

[58] Mitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit. Insertion transformer: Flexible sequence generation via insertion operations. In International Conference on Machine Learning, pages 5976–5985. PMLR, 2019.

[59] Zhiqing Sun, Zhuohan Li, Haoqing Wang, Di He, Zi Lin, and Zhihong Deng. Fast structured decoding for sequence models. Advances in Neural Information Processing Systems, 32, 2019.

[60] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. Mobilebert: a compact task-agnostic bert for resource-limited devices. arXiv preprint arXiv:2004.02984, 2020.

[61] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-specific knowledge from bert into simple neural networks. arXiv preprint arXiv:1903.12136, 2019.

[62] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.

[63] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008, 2017.

[64] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint arXiv:1905.09418, 2019.

[65] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.

--- TRANG 14 ---
[66] Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, and Song Han. Hat: Hardware-aware transformers for efficient natural language processing. arXiv preprint arXiv:2005.14187, 2020.

[67] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.

[68] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. arXiv preprint arXiv:2002.10957, 2020.

[69] Yiren Wang, Fei Tian, Di He, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu. Non-autoregressive machine translation with auxiliary regularization. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 5377–5384, 2019.

[70] Bingzhen Wei, Mingxuan Wang, Hao Zhou, Junyang Lin, Jun Xie, and Xu Sun. Imitation learning for non-autoregressive neural machine translation. arXiv preprint arXiv:1906.02041, 2019.

[71] Sean Welleck, Kianté Brantley, Hal Daumé Iii, and Kyunghyun Cho. Non-monotonic sequential text generation. In International Conference on Machine Learning, pages 6716–6726. PMLR, 2019.

[72] Samuel Williams, Andrew Waterman, and David Patterson. Roofline: an insightful visual performance model for multicore architectures. Communications of the ACM, 52(4):65–76, 2009.

[73] Thomas Wolf, Julien Chaumond, Lysandre Debut, Victor Sanh, Clement Delangue, Anthony Moi, Pierric Cistac, Morgan Funtowicz, Joe Davison, Sam Shleifer, et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, 2020.

[74] Xiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, and Yuxiong He. Extreme compression for pre-trained transformers made simple and efficient. arXiv preprint arXiv:2206.01859, 2022.

[75] Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han. Lite transformer with long-short range attention. arXiv preprint arXiv:2004.11886, 2020.

[76] Jin Xu, Xu Tan, Renqian Luo, Kaitao Song, Jian Li, Tao Qin, and Tie-Yan Liu. Nas-bert: task-agnostic and adaptive-size bert compression with neural architecture search. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 1933–1943, 2021.

[77] Yige Xu, Xipeng Qiu, Ligao Zhou, and Xuanjing Huang. Improving bert fine-tuning via self-ensemble and self-distillation. arXiv preprint arXiv:2002.10345, 2020.

[78] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934, 2020.

[79] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.

[80] Yichun Yin, Cheng Chen, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. Autotinybert: Automatic hyper-parameter optimization for efficient pre-trained language models. arXiv preprint arXiv:2107.13686, 2021.

[81] Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, and Andreas Moshovos. Gobo: Quantizing attention-based nlp models for low latency and energy efficient inference. In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pages 811–824. IEEE, 2020.

[82] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8BERT: Quantized 8bit bert. arXiv preprint arXiv:1910.06188, 2019.

[83] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.

[84] Chunting Zhou, Graham Neubig, and Jiatao Gu. Understanding knowledge distillation in non-autoregressive machine translation. arXiv preprint arXiv:1911.02727, 2019.

--- TRANG 15 ---
7 Tài liệu Bổ sung
7.1 Chi tiết Thí nghiệm

Bảng 2: Cấu hình mô hình của các mô hình lớn và nhỏ cho mỗi tác vụ đánh giá. Để so sánh, số lớp, chiều ẩn, chiều FFN, và số tham số decoder (không bao gồm embedding) cho mỗi mô hình được cung cấp.

[THIS IS TABLE:
Tác vụ | Mô hình | # Lớp | chiều | chiều FFN | # Tham số
Dịch máy | mT5-large [78] | 24 | 1024 | 2816 | 409M
         | mT5-small [78] | 8 | 512 | 1024 | 25M
Tóm tắt  | T5-large [47] | 24 | 1024 | 4096 | 402M
         | T5-small [47] | 6 | 512 | 2048 | 25M]

7.1.1 Chi tiết Huấn luyện
Đối với dịch máy, chúng tôi sử dụng IWSLT 2017 German-English [3] và WMT 2014 German-English [1] làm các benchmark mục tiêu, và mT5 [78] làm mô hình mục tiêu. Chúng tôi sử dụng mT5-small 8 lớp và mT5-large 24 lớp làm các mô hình nhỏ và lớn. Đối với tóm tắt, chúng tôi sử dụng XSUM [43] và CNN/DailyMail [21] làm các benchmark mục tiêu, và T5 [47] làm mô hình mục tiêu. Chúng tôi sử dụng T5-small và T5-large với 6 và 24 lớp, tương ứng, cho các mô hình nhỏ và lớn. Bảng 2 tóm tắt kích thước và cấu hình của mỗi mô hình. Tất cả các mô hình được fine-tune từ các checkpoint pre-train của thư viện HuggingFace [73] trong 500k bước sử dụng batch size 16. Chúng tôi sử dụng trình tối ưu Adafactor [53] với tốc độ học cố định {0.5, 1, 2, 5}e−4 cho các mô hình nhỏ và {0.5, 1}e−4 cho các mô hình lớn. Chúng tôi gọi các mô hình được fine-tune bình thường trên các bộ dữ liệu validation là các mô hình nhỏ và lớn cơ sở.

Khi huấn luyện các mô hình nhỏ được căn chỉnh thông qua phương pháp căn chỉnh dự đoán được mô tả trong Phần 3.5.1, trước tiên chúng tôi sinh các bộ dữ liệu hiệu chuẩn sử dụng các chuỗi đầu vào từ các bộ dữ liệu huấn luyện của mỗi benchmark. Sau đó chúng tôi sử dụng mô hình lớn được huấn luyện hoàn toàn để sinh các chuỗi đầu ra thông qua lấy mẫu tham lam với kích thước beam là 1. Để đảm bảo so sánh công bằng, chúng tôi fine-tune các mô hình nhỏ pre-train (thay vì các mô hình nhỏ cơ sở đã được fine-tune trên các bộ dữ liệu huấn luyện) trên các bộ dữ liệu hiệu chuẩn sử dụng cùng công thức huấn luyện và số bước huấn luyện như được mô tả ở trên. Quyết định này dựa trên quan sát của chúng tôi rằng việc fine-tune một mô hình cơ sở sử dụng bộ dữ liệu hiệu chuẩn có xu hướng cải thiện chất lượng sinh, có thể do số lượng ví dụ huấn luyện tăng và hiệu ứng tăng cường dữ liệu, điều này khiến việc so sánh công bằng giữa BiLD không căn chỉnh và BiLD căn chỉnh trở nên khó khăn. Tuy nhiên, trong thực tế, người ta có thể có được các mô hình được căn chỉnh bằng cách áp dụng phương pháp căn chỉnh dự đoán trực tiếp lên các mô hình nhỏ cơ sở đã được fine-tune để đạt được hiệu suất tốt nhất.

7.1.2 Chi tiết Đánh giá
Tất cả các đánh giá suy luận bao gồm đo độ trễ được thực hiện trên một GPU NVIDIA T4 duy nhất của một instance GCP n1-standard-4 với 4 vCPU và 15GB bộ nhớ. Để suy luận, chúng tôi sử dụng batch size 1, đây là trường hợp sử dụng phổ biến cho phục vụ trực tuyến [51]. Đối với metric khoảng cách d trong Phương trình 3 cho chính sách hoàn trả, chúng tôi sử dụng cross-entropy loss giữa nhãn cứng của mô hình nhỏ và nhãn mềm của mô hình lớn. Điều này đo (âm log) khả năng thu được dự đoán của mô hình nhỏ từ đầu ra của mô hình lớn. Đối với suy luận BiLD, chúng tôi quét qua các ngưỡng dự phòng và hoàn trả khác nhau để khám phá các sự đánh đổi khác nhau giữa chất lượng sinh và độ trễ. Đối với các tác vụ dịch máy, chúng tôi sử dụng ngưỡng dự phòng trong [0.5, 0.9] và ngưỡng hoàn trả trong [1, 10]. Đối với các tác vụ tóm tắt, ngưỡng dự phòng trong [0.2, 0.6] và ngưỡng hoàn trả trong [2, 6]. Chúng tôi giữ độ dài sinh tối đa của mô hình nhỏ là 10 để tránh chi phí hoàn trả cao. Trong Phụ lục 7.5.3, chúng tôi cung cấp phân tích chi tiết về cách thay đổi các ngưỡng dự phòng và hoàn trả ảnh hưởng đến sự đánh đổi giữa chất lượng sinh và độ trễ trong framework BiLD.

--- TRANG 16 ---
[Hình 7: Các đường cong đánh đổi giữa độ trễ suy luận và điểm BLEU cho BiLD và CALM trong thiết lập thoát sớm cho (Trái) IWSLT 2017 De-En và (Phải) WMT 2014 De-En. Các dấu × chỉ ra độ trễ suy luận vanilla và điểm BLEU của các mô hình mT5-small. Các đường ngang chỉ ra điểm suy luận vanilla và suy giảm 1 điểm từ đó. BiLD vượt trội hơn CALM trên tất cả các chế độ tăng tốc lên đến 2~2.5 điểm BLEU tốt hơn, chứng minh tính hiệu quả của cách tiếp cận của chúng tôi cho chiến lược thoát sớm.]

7.2 Chi tiết về Chiến lược Thoát Sớm trong Framework BiLD
7.2.1 Chi tiết Huấn luyện và Đánh giá
Chi tiết huấn luyện và đánh giá cho BiLD cũng như cho CALM như sau.

BiLD. Chúng tôi sử dụng mô hình mT5-small làm mô hình lớn và lớp đầu tiên (trong số 8) làm mô hình nhỏ, và đánh giá nó trên hai benchmark dịch máy: IWSLT 2017 De-En và WMT 2014 De-En. Để đảm bảo tính nhất quán giữa dự đoán được thực hiện sau lớp đầu tiên và dự đoán được thực hiện sau lớp cuối cùng, chúng tôi fine-tune mô hình mT5 pre-train sử dụng loss trung bình của lớp đầu tiên và lớp cuối cùng, tương tự như [11,51]. Nghĩa là, L = 1/2(L₁ + L₋₁) trong đó L₁ và L₋₁ là negative log-likelihood loss sau lớp đầu tiên và lớp cuối cùng. Đầu dự đoán được chia sẻ cho hai lớp này. Chúng tôi fine-tune mô hình mT5-small pre-train trên mỗi benchmark trong 500k bước sử dụng batch size 16. Tương tự như các thí nghiệm chính, chúng tôi sử dụng trình tối ưu Adafactor [53] với tốc độ học cố định {0.5, 1, 2, 5}e−4. Để đánh giá, chúng tôi sử dụng ngưỡng dự phòng trong [0.2, 0.8] và ngưỡng hoàn trả trong [0.5, 1.5].

CALM. Để tái tạo CALM [51] trong thiết lập thí nghiệm của chúng tôi, chúng tôi đã fine-tune mô hình mT5-small pre-train trên các bộ dữ liệu IWSLT 2017 De-En và WMT 2014 De-En. Chúng tôi sử dụng loss trung bình trên tất cả các lớp, tức là L = ∑ᴸᵢ₌₁ wᵢLᵢ, trong đó wᵢ = i/∑ᴸⱼ₌₁ j, được giới thiệu trong bài báo để đảm bảo tính nhất quán của lớp. Chúng tôi sử dụng trình tối ưu Adafactor [53] với tốc độ học cố định {0.5, 1, 2, 5}e−4 cho 500k bước huấn luyện. Để so sánh công bằng, chúng tôi khớp điểm BLEU của mô hình đã được fine-tune với điểm của các mô hình BiLD. Trong số hai biện pháp tin cậy không cần huấn luyện được giới thiệu trong bài báo CALM, biện pháp dựa trên softmax và dựa trên độ bão hòa trạng thái ẩn, chúng tôi đã chọn sử dụng cách tiếp cận thứ hai làm tiêu chí thoát sớm. Nghĩa là, nếu độ tương tự cosine giữa trạng thái ẩn của lớp hiện tại và trạng thái ẩn của lớp trước vượt quá một ngưỡng nhất định, chúng tôi thực hiện thoát sớm. Chúng tôi đã phát hiện rằng thay thế dựa trên softmax không áp dụng được trong tình huống đánh giá của chúng tôi do từ vựng đầu ra lớn (hơn 200k cho mT5, lớn hơn ~10× so với T5), điều này tăng đáng kể chi phí độ trễ. Như được mô tả trong bài báo, khi thoát sớm xảy ra, các trạng thái ẩn của lớp thoát được truyền xuống các lớp còn lại để tính toán các cache khóa và giá trị. Để đạt được các sự đánh đổi khác nhau giữa độ trễ và chất lượng sinh, chúng tôi quét qua λ trong [0.7, 0.98] và t trong {0, 1, 2, 4, 8} trong hàm ngưỡng suy giảm.

7.2.2 So sánh Hiệu suất giữa BiLD và CALM
Hình 7 minh họa các đường cong điểm BLEU và độ trễ của BiLD so với CALM trong thiết lập thoát sớm. Trong cả hai tác vụ, phương pháp của chúng tôi đạt được điểm BLEU tốt hơn đáng kể với cùng tăng tốc độ trễ, mang lại cải thiện lên đến khoảng 2 điểm BLEU trong chế độ tăng tốc ~1.5×. Điều này có thể được gán cho hai yếu tố. Thứ nhất, trong BiLD, ngay cả khi dự đoán thoát sớm (tức là dự đoán được thực hiện bởi mô hình nhỏ hơn) không chính xác, nó có thể được sửa chữa và thay thế bằng chính sách hoàn trả. Do đó, một lỗi trong lớp thoát sớm được lan truyền ít thảm khốc hơn đến dự đoán tương lai. Thứ hai, các cache khóa và giá trị cho các lớp bị bỏ qua được điền bằng các giá trị thực tế thay vì được tính toán từ các trạng thái ẩn của lớp thoát. Điều này cũng dẫn đến giảm lan truyền lỗi và cải thiện tính ổn định giải mã.

7.3 So sánh với Các Framework Giải mã Suy đoán Khác
Đồng thời và độc lập với công trình của chúng tôi, [38,4] cũng đề xuất một thuật toán để tăng tốc suy luận sinh bằng cách sử dụng một mô hình mạnh hơn để chấm điểm và lấy mẫu suy đoán các dự đoán từ một mô hình ít mạnh hơn. Trong khi cách tiếp cận dựa trên rejection sampling trong [38,4] cung cấp các ước lượng không thiên vị khớp với phân phối xác suất của mô hình mạnh hơn, đánh giá thực nghiệm rộng rãi của chúng tôi cho thấy rằng cách tiếp cận của chúng tôi có thể mang lại sự đánh đổi hiệu suất-độ trễ vượt trội, do chính sách hoàn trả không ngẫu nhiên (tức là từ chối) cũng như kích thước cửa sổ dự phòng động. Dưới đây, chúng tôi cung cấp các khác biệt trong phương pháp chi tiết và so sánh định lượng, cũng như những hiểu biết của chúng tôi về độ trễ và hiệu suất tốt hơn của cách tiếp cận chúng tôi.

7.3.1 Khác biệt trong phương pháp
Trong khi ý tưởng sử dụng hai mô hình với kích thước khác nhau có thể được coi là tương tự với các framework giải mã suy đoán trong [38, 4], chúng tôi có những khác biệt rõ ràng trong phương pháp chi tiết.

(1) Cách tiếp cận Hoàn trả Dự đoán Không Ngẫu nhiên: Khác biệt chính nằm ở cách chúng tôi quyết định hoàn trả (ví dụ, từ chối) các dự đoán từ mô hình nhỏ. Trong chính sách hoàn trả của chúng tôi, chúng tôi đề xuất đưa ra quyết định từ chối dựa trên khoảng cách giữa các dự đoán của mô hình nhỏ và lớn, khác với chính sách rejection sampling được nêu trong [38,4]. Trong khi [38,4] đề xuất một ước lượng không thiên vị về dự đoán của mô hình lớn, Hình 2 chứng minh rằng việc kết hợp các dự đoán từ cả hai mô hình thông qua cách tiếp cận từ chối dựa trên khoảng cách của chúng tôi có thể vượt qua việc sử dụng độc quyền xác suất dự đoán của mô hình lớn. BiLD tìm cách tìm và sử dụng điểm hiệu suất tối ưu này mà không gây ra nhiều chi phí thời gian chạy. Chúng tôi có thảo luận thêm dưới đây về cách chính sách từ chối của chúng tôi có lợi cho hiệu suất sinh văn bản.

(2) Kích thước Cửa sổ Dự phòng Động: Ngoài ra, chúng tôi giới thiệu kích thước cửa sổ dự phòng động trong chính sách dự phòng của chúng tôi. Trong [38,4], kích thước cửa sổ vẫn là một siêu tham số cố định; tuy nhiên, cũng được nhấn mạnh trong [4] rằng kích thước cửa sổ có thể có tác động đáng kể đến độ trễ end-to-end. Cách tiếp cận của chúng tôi cung cấp một giải pháp hiệu quả và mạnh mẽ: điều chỉnh kích thước cửa sổ tại thời gian chạy dựa trên mức độ tin cậy của mô hình nhỏ trong thời gian chạy. Nghiên cứu ablation của chúng tôi (Hình 5) chứng minh rằng việc bỏ qua chính sách dự phòng và định kỳ chuyển quyền điều khiển cho mô hình lớn, như được đề xuất trong [38,4], có thể dẫn đến suy giảm độ trễ đáng kể.

(3) Tăng cường Căn chỉnh Mô hình: Ngoài framework cốt lõi, chúng tôi giới thiệu một phương pháp căn chỉnh mô hình để căn chỉnh các dự đoán của mô hình nhỏ với những dự đoán của mô hình lớn. Điều này tăng cường framework bằng cách giảm những lần từ chối không cần thiết và có thể được kết hợp với những điều chỉnh tối thiểu đối với pipeline huấn luyện.

7.3.2 So sánh Định lượng
Trong Bảng 3, chúng tôi cung cấp so sánh định lượng toàn diện giữa phương pháp của chúng tôi và [38,4] trên hai bộ dữ liệu khác nhau: IWSLT cho dịch máy và XSum cho tóm tắt. Để đảm bảo so sánh công bằng tách biệt tác động của chính các framework, chúng tôi sử dụng mô hình nhỏ cơ sở (không được căn chỉnh) cho tất cả các thí nghiệm. Chúng tôi duy trì cùng thiết lập đánh giá và không gian siêu tham số được nêu trong Phụ lục 7.1.2.

Bảng 3 bao gồm hai cấu hình BiLD: một cấu hình khớp độ trễ và cấu hình khác khớp điểm BLEU/ROUGE-L so với các phương pháp dựa trên rejection sampling. Trên tất cả các thí nghiệm, BiLD nhất quán vượt trội hơn giải mã suy đoán. Nó đạt được hoặc (1) điểm BLEU/ROUGE-L được cải thiện đáng kể với tăng độ trễ tương đương, hoặc (2) tăng độ trễ vượt trội trong khi giữ nguyên điểm BLEU/ROUGE-L.

--- TRANG 17 ---
Bảng 3: So sánh BiLD với các phương pháp lấy mẫu suy đoán dựa trên rejection sampling khác được đề xuất trong [38,4] trên IWSLT và XSUM. Đối với BiLD, chúng tôi bao gồm hai cấu hình BiLD: một cấu hình khớp độ trễ và cấu hình khác khớp điểm BLEU/ROUGE-L so với các phương pháp dựa trên rejection sampling. Lưu ý rằng BiLD nhất quán vượt trội hơn các phương pháp khác bằng cách đạt được hoặc (1) điểm BLEU/ROUGE-L được cải thiện với tăng độ trễ tương đương, hoặc (2) tăng độ trễ được cải thiện trong khi giữ nguyên điểm hiệu suất.

[THIS IS TABLE:
Bộ dữ liệu | IWSLT | XSUM
BLEU | Tăng tốc | ROUGE-L | Tăng tốc
Suy luận Vanilla | 40.32 | - | 35.08 | -
Dựa trên Rejection Sampling [38, 4] | 39.93 | 1.28× | 35.00 | 1.25×
BiLD (Khớp Độ trễ) | 40.54 | 1.23× | 35.30 | 1.42×
BiLD (Khớp BLEU/ROUGE-L) | 39.87 | 1.49× | 34.96 | 1.50×]

Bảng 4: So sánh tỷ lệ phần trăm xuất hiện dự phòng và hoàn trả (từ chối) của BiLD và các phương pháp lấy mẫu suy đoán dựa trên rejection sampling khác [38,4]. Trong khi đạt được điểm BLEU/ROUGE-L tốt hơn trong IWSLT và XSUM, BiLD liên quan đến số lượng dự phòng và hoàn trả ít hơn đáng kể, dẫn đến tăng tốc độ trễ tốt hơn đáng kể.

[THIS IS TABLE:
Tác vụ | Phương pháp | BLEU/ROUGE-L | Tăng tốc | % Dự phòng | % Hoàn trả (từ chối)
IWSLT | Dựa trên Rejection Sampling [38, 6] | 39.93 | 1.28× | 23.24% | 9.81%
       | BiLD (BLEU Tốt hơn) | 40.33 | 1.43× | 21.09% | 1.56%
XSUM  | Dựa trên Rejection Sampling [38, 6] | 35.00 | 1.25× | 36.84% | 24.24%
       | BiLD (ROUGE-L Tốt hơn) | 35.12 | 1.48× | 32.33% | 6.41%]

7.3.3 Hiểu biết về Độ trễ và Hiệu suất Tốt hơn
Phân tích định lượng cho thấy một xu hướng nhất quán nơi phương pháp của chúng tôi, khi so sánh với các framework giải mã suy đoán khác, hiệu quả tăng cường cả chất lượng sinh văn bản và độ trễ. Chúng tôi cung cấp hiểu biết và giải thích tại sao cách tiếp cận của chúng tôi vượt qua các framework giải mã suy đoán.

(1) Chất lượng sinh văn bản tốt hơn
Hiệu ứng tổ hợp: Sức mạnh của việc pha trộn đầu ra từ nhiều mô hình đã được khám phá tốt trong các lĩnh vực khác nhau. Điều này cũng đúng trong các mô hình LLM mã nguồn mở thể hiện những điểm mạnh và điểm yếu đa dạng do sự khác biệt trong dữ liệu, kiến trúc, và siêu tham số, làm cho các mô hình khác nhau bổ sung cho nhau [27]. Thực tế, chúng tôi cho thấy những hiệu ứng như vậy của việc pha trộn nhiều đầu ra mô hình trong Hình 2, nơi sự kết hợp của 20% dự đoán của mô hình lớn với dự đoán của mô hình nhỏ vượt trội hơn việc bắt chước chính xác hành vi của mô hình lớn. Cách tiếp cận của chúng tôi cung cấp các chính sách dự phòng và hoàn trả hiệu quả khai thác điểm tổ hợp tối ưu, tạo ra chất lượng đầu ra vượt trội so với ước lượng không thiên vị của mô hình lớn trong [38, 4].

Chính sách hoàn trả dẫn đến hiệu suất cao hơn: Phương pháp của chúng tôi áp dụng chính sách từ chối hoàn toàn loại bỏ dự đoán của mô hình nhỏ nếu nó lệch đáng kể khỏi đối tác của mô hình lớn, dựa trên metric khoảng cách dựa trên cross entropy. Điều này trái ngược với giải mã suy đoán, nơi quyết định liên quan đến quá trình rejection sampling ngẫu nhiên. Chúng tôi quan sát thực nghiệm rằng chính sách từ chối cứng của BiLD cho phép điểm BLEU/ROUGE-L tốt hơn với số lượng hoàn trả (từ chối) ít hơn đáng kể so với chính sách từ chối ngẫu nhiên của giải mã suy đoán như được mô tả trong Bảng 4. Chúng tôi giả thuyết rằng sự thúc đẩy hiệu suất dự đoán này xuất phát từ chính sách hoàn trả cứng của chúng tôi, ngăn chặn các dự đoán có khả năng sai lầm bằng cách loại trừ tính ngẫu nhiên. Chúng tôi giả thuyết thêm rằng chiến lược như vậy có thể giải quyết bias tiếp xúc, giảm thiểu tác động của một dự đoán sai lầm giai đoạn đầu duy nhất đối với các dự đoán tiếp theo.

(2) Độ trễ end-to-end thấp hơn Hơn nữa, chính sách dự phòng của chúng tôi giới thiệu kích thước cửa sổ dự phòng động (tức là số vòng lặp liên tiếp của mô hình nhỏ) được xác định dựa trên độ tin cậy dự đoán thời gian chạy của mô hình nhỏ. Điều này trái ngược với giải mã suy đoán áp dụng kích thước cửa sổ tĩnh. Những lợi thế của kích thước cửa sổ động có hai mặt:

--- TRANG 18 ---
Ít dự phòng hơn: Kích thước cửa sổ động cho phép mô hình nhỏ tiếp tục đưa ra dự đoán khi nó tự tin, do đó giảm thiểu sự tham gia không cần thiết của mô hình lớn. Điều này được hỗ trợ bởi Bảng 4 nơi BiLD liên quan đến số lượng dự phòng ít hơn (23.24% → 21.09% và 36.84% → 32.33%) so với [38, 4] trong khi đạt được hiệu suất tốt hơn.

Ít hoàn trả/từ chối hơn: Kích thước cửa sổ động hơn nữa cho phép ngăn chặn mô hình nhỏ khi nó không chắc chắn, điều này tránh hoàn trả các dự đoán sai của mô hình nhỏ. Điều này cũng được hỗ trợ bởi Bảng 4 nơi BiLD liên quan đến số lượng hoàn trả ít hơn đáng kể (9.81% → 1.56% và 24.24% → 6.41%) so với [38, 4] trong khi đạt được hiệu suất tốt hơn.

Việc giảm thiểu cả dự phòng và hoàn trả/từ chối giảm tính toán không cần thiết, trực tiếp chuyển thành cải thiện độ trễ end-to-end.

7.4 BiLD với Lấy mẫu
Cách tiếp cận của chúng tôi không bị hạn chế chỉ với giải mã tham lam, mà nó có thể mở rộng một cách liền mạch sang các phương pháp lấy mẫu. Sửa đổi duy nhất là thực hiện lấy mẫu ngẫu nhiên thay vì lấy mẫu tham lam khi rút một token từ cả mô hình nhỏ và mô hình lớn trong khi sử dụng cùng chính sách dự phòng và hoàn trả. Điều này là bởi vì cả chính sách dự phòng và hoàn trả, dựa trên xác suất dự đoán tối đa, đều phục vụ như một chỉ báo hiệu quả về sự không chắc chắn của mô hình nhỏ trong dự đoán và các bất chính xác tiềm năng, bất kể phương pháp lấy mẫu. Bảng sau minh họa sự đánh đổi độ trễ với hiệu suất của cách tiếp cận dựa trên lấy mẫu, cụ thể sử dụng nucleus sampling với p=0.8, tương tự như [6]. Đánh giá này tuân theo cùng môi trường với các thí nghiệm khác được nêu trong bài báo, và cả hai trường hợp đều liên quan đến các mô hình nhỏ được căn chỉnh.

Bảng 5: BiLD với nucleus sampling (p=0.8) trên IWSLT và XSUM. Tương tự như trường hợp giải mã tham lam, phương pháp của chúng tôi đạt được tăng tốc ~1.5× mà không làm tổn hại hiệu suất và tăng tốc ~1.8× với sự giảm điểm BLEU/ROUGE khiêm tốn 1 điểm với lấy mẫu.

[THIS IS TABLE:
Bộ dữ liệu | IWSLT | XSUM
BLEU | Tăng tốc | ROUGE-L | Tăng tốc
Suy luận Vanilla | 39.24 | - | 34.00 | -
BiLD | 39.72 (+0.48) | 1.51× | 34.34 (+0.34) | 1.22×
     | 39.26 (+0.02) | 1.63× | 34.04 (+0.04) | 1.45×
     | 38.27 (-0.97) | 1.80× | 33.10 (-0.90) | 1.85×]

Bảng 5 thể hiện điểm BLEU/ROUGE-L của BiLD trên các benchmark IWSLT và XSUM cũng như tăng tốc tương đối của chúng. Như có thể thấy trong bảng, phương pháp của chúng tôi thể hiện xu hướng tương tự như trường hợp giải mã tham lam. Nó đạt được tăng tốc ~1.5× mà không làm tổn hại hiệu suất và tăng tốc ~1.8× với sự giảm điểm BLEU/ROUGE khiêm tốn 1 điểm.

7.5 Phân tích Bổ sung
7.5.1 Phân tích Mô hình của BiLD: FLOP, MOP, và Cường độ Số học
Hình 8 so sánh FLOP trung bình, MOP (thao tác bộ nhớ), cường độ số học, và tăng tốc độ trễ của suy luận vanilla và BiLD trên các benchmark CNN/DailyMail. Đối với BiLD, chúng tôi sử dụng mô hình với điểm ROUGE-L gần như tương tự suy luận vanilla, và tất cả các số được chuẩn hóa bởi các số của suy luận vanilla. Hình minh họa rằng BiLD thể hiện FLOP hơi cao hơn so với suy luận vanilla. Điều này là do các thực thi tự hồi quy và không tự hồi quy có cùng lượng FLOP, và BiLD liên quan đến chi phí bổ sung của việc chạy mô hình nhỏ song song. Tuy nhiên, trong trường hợp MOP, BiLD chứng minh sự giảm đáng kể ~5× các thao tác bộ nhớ. Điều này có thể được gán cho khả năng của BiLD xử lý nhiều token với một lần tải trọng số duy nhất, do đó tăng cường song song hóa ở mức token và tối đa hóa việc tái sử dụng dữ liệu. Ngược lại, điều này không phải là trường hợp trong suy luận vanilla nơi một lần tải trọng số chỉ có thể xử lý một token duy nhất. Do đó, BiLD đạt được cường độ số học cao hơn đáng kể, khoảng 5 lần lớn hơn so với suy luận vanilla. Cường độ số học [72] đo số lượng thao tác số học có thể được thực hiện trên mỗi thao tác bộ nhớ. Cho rằng các thao tác bộ nhớ có thể đóng góp nhiều hơn vào độ trễ suy luận tổng thể so với các thao tác số học trong nhiều tình huống giải mã Transformer [32], việc giảm các thao tác bộ nhớ

--- TRANG 19 ---
[Hình 8: So sánh FLOP, MOP (thao tác bộ nhớ), cường độ số học, và tăng tốc độ trễ của suy luận vanilla và BiLD trên benchmark CNN/DailyMail. Cách tiếp cận BiLD dẫn đến sự giảm đáng kể trong MOP do song song hóa ở mức token được cải thiện, dẫn đến cường độ số học cao hơn đáng kể.]

và tăng cường độ số học có thể hiệu quả giảm thiểu nút thắt cổ chai suy luận. Điều này dẫn đến tăng tốc độ trễ tổng thể 1.85× trên phần cứng thực tế.

7.5.2 Ví dụ về Chuỗi Được Sinh
Hình 9 cung cấp các ví dụ về chuỗi văn bản được sinh bởi BiLD trên bộ validation của IWSLT 2017 De-En, cùng với ground truth (tức là nhãn) và đầu ra của các mô hình lớn và nhỏ cơ sở thuần túy. Các token được sinh từ mô hình lớn của BiLD được tô sáng bằng màu xanh, trong khi tất cả các token khác được sinh bởi mô hình nhỏ. Kết quả minh họa rằng mô hình nhỏ thường tạo ra văn bản chất lượng thấp, bằng cách dự đoán các token không chính xác có thể thay đổi ý nghĩa của toàn bộ câu. Để tương phản, có thể quan sát từ các ví dụ rằng BiLD có thể cải thiện chất lượng sinh văn bản bằng cách để mô hình lớn can thiệp khi mô hình nhỏ sinh các token không chính xác. Đặc biệt, trong các ví dụ được cung cấp, BiLD có xu hướng mạnh như mô hình lớn trong việc dự đoán thuật ngữ. Tổng thể, sự tham gia của mô hình lớn trong giải mã BiLD không chỉ cải thiện độ chính xác dự đoán mà còn ngăn chặn các dự đoán không chính xác khỏi việc tác động đến những dự đoán tương lai.

[Hình 9: Ví dụ về chuỗi văn bản mà BiLD sinh với bộ validation của IWSLT 2017 De-En, so với ground truth và đầu ra của các baseline lớn và nhỏ. Đối với BiLD, các token được sinh bởi mô hình lớn được tô sáng bằng màu đỏ, trong khi tất cả các token khác được sinh bởi mô hình nhỏ. Điều này minh họa rằng với sự tham gia nhỏ của mô hình lớn, BiLD có thể sửa chữa không chỉ từ vựng không chính xác mà còn ngữ nghĩa sai của văn bản mà mô hình nhỏ sẽ sinh ra.]

--- TRANG 20 ---
[Hình 10: Sự đánh đổi giữa độ trễ và chất lượng sinh (ROUGE-L) cho mô hình BiLD được căn chỉnh trên hai tác vụ tóm tắt: (Trái) XSUM và (Phải) CNN/DailyMail. Mỗi đường cong đại diện cho một ngưỡng hoàn trả khác nhau, với các ngưỡng nhỏ hơn chỉ ra nhiều hoàn trả hơn. Sự đánh đổi có thể được thu được thêm trong mỗi đường cong với các ngưỡng dự phòng khác nhau, nơi kích thước scatter lớn hơn chỉ ra ngưỡng dự phòng lớn hơn. Ngưỡng dự phòng lớn hơn ngụ ý nhiều dự phòng hơn.]

7.5.3 Tác động của Dự phòng và Hoàn trả đối với Hiệu suất
Chúng tôi đã khám phá cách framework BiLD có thể đạt được các sự đánh đổi khác nhau giữa độ trễ và chất lượng sinh bằng cách điều chỉnh các ngưỡng dự phòng và hoàn trả. Trong phần này, chúng tôi trình bày phân tích chi tiết về cách các ngưỡng này ảnh hưởng đến hiệu suất sử dụng mô hình BiLD được căn chỉnh trên hai tác vụ tóm tắt khác nhau, XSUM và CNN/DailyMail, như được minh họa trong Hình 10. Các đường cong khác nhau trong biểu đồ đại diện cho các ngưỡng hoàn trả khác nhau, và mỗi điểm scatter trong đường cong đại diện cho các ngưỡng dự phòng khác nhau. Lưu ý rằng ngưỡng hoàn trả nhỏ ngụ ý nhiều hoàn trả hơn, trong khi ngưỡng dự phòng lớn hơn ngụ ý nhiều dự phòng hơn.

Chúng tôi quan sát một xu hướng chung nơi các ngưỡng hoàn trả nhỏ hơn (tức là nhiều hoàn trả hơn) dẫn đến chất lượng sinh tốt hơn nhưng độ trễ dài hơn. Xu hướng này được mong đợi bởi vì, với nhiều hoàn trả hơn, chúng ta ngăn chặn nhiều dự đoán của mô hình nhỏ có thể không chính xác bằng cách hy sinh độ trễ. Tương tự, cũng có xu hướng chung rằng các ngưỡng dự phòng nhỏ hơn (tức là ít dự phòng hơn) dẫn đến độ trễ nhanh hơn nhưng chất lượng sinh tệ hơn. Tuy nhiên, chúng tôi quan sát rằng việc hạ thấp tỷ lệ dự phòng quá một điểm nhất định thực sự có thể làm tổn hại cả độ trễ và chất lượng sinh. Điều này là bởi vì các dự đoán không chính xác mà mô hình nhỏ nên dự phòng sau đó bị hoàn trả, gây ra chi phí 'xóa' bổ sung cho các token theo sau.
