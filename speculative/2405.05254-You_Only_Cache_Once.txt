# 2405.05254.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/speculative/2405.05254.pdf
# File size: 949808 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
You Only Cache Once:
Decoder-Decoder Architectures for Language Models
Yutao Sun‚àó‚Ä†‚Ä°Li Dong‚àó‚Ä†Yi Zhu‚Ä†Shaohan Huang‚Ä†
Wenhui Wang‚Ä†Shuming Ma‚Ä†Quanlu Zhang‚Ä†Jianyong Wang‚Ä°Furu Wei‚Ä†‚ãÑ
‚Ä†Microsoft Research‚Ä°Tsinghua University
https://aka.ms/GeneralAI
Abstract
We introduce a decoder-decoder architecture, YOCO, for large language models,
which only caches key-value pairs once. It consists of two components, i.e., a cross-
decoder stacked upon a self-decoder . The self-decoder efficiently encodes global
key-value (KV) caches that are reused by the cross-decoder via cross-attention.
The overall model behaves like a decoder-only Transformer, although YOCO
only caches once. The design substantially reduces GPU memory demands, yet
retains global attention capability. Additionally, the computation flow enables
prefilling to early exit without changing the final output, thereby significantly
speeding up the prefill stage. Experimental results demonstrate that YOCO achieves
favorable performance compared to Transformer in various settings of scaling
up model size and number of training tokens. We also extend YOCO to 1M
context length with near-perfect needle retrieval accuracy. The profiling results
show that YOCO improves inference memory, prefill latency, and throughput by
orders of magnitude across context lengths and model sizes. Code is available at
https://aka.ms/YOCO .
0204060
02040
060120180
GPU Memory ‚Üì
(GB)Throughput ‚Üë
(wps)Prefilling Latency ‚Üì
(s)6.4X30.3X9.6XInference Cost (@512k) Decoder -Decoder LLM
YOCO TransformerCross -Decoder
<s>    You    Only   CacheYou   Only   Cache  Once
Self-DecoderKV Cache
Figure 1: We propose a decoder-decoder architecture, YOCO, for large language model, which only
caches key/value once. YOCO markedly reduces the KV cache memory and the prefilling time, while
being scalable in terms of training tokens, model size, and context length. The inference cost is
reported to be 512K as the context length, and Figures 7‚Äì10 present more results for different lengths.
‚àóEqual contribution. ‚ãÑCorresponding author.arXiv:2405.05254v2  [cs.CL]  9 May 2024

--- PAGE 2 ---
1 Introduction
The decoder-only Transformer [ VSP+17] has become the de facto architecture for language models.
Numerous efforts have continued to develop suitable architectures for language modeling. There have
been main strands of explorations. First, encoder-only language models, such as BERT [ DCLT19 ],
bidirectionally encode the input sequence. Second, encoder-decoder models, such as T5 [ RSR+20],
use a bidirectional encoder to encode input and a unidirectional decoder to generate output. Both
of the above layouts struggle with autoregressive generation due to bidirectionality. Specifically,
encoders have to encode the whole input and output tokens again for the next generation step.
Although encoder-decoder can use only decoder to generate, the output tokens do not fully leverage
the parameters of encoder, especially for multi-turn conversation. Third, decoder-only language
models, such as GPT [ BMR+20], generate tokens autoregressively. By caching the previously
computed key/value vectors, the model can reuse them for the current generation step. The key-value
(KV) cache avoids encoding the history again for each token, greatly improving the inference speed.
This compelling feature establishes the decoder-only language model as the standard option.
However, as the number of serving tokens increases, the KV caches occupy a lot of GPU memory,
rendering the inference of large language models memory-bounded [ PDC+22]. For the example
of a 65B-size language model (augmented with grouped-query attention [ ALTdJ+23] and 8-bit
KV quantization), 512K tokens occupy about 86GB GPU memory, which is even larger than the
capacity of one H100-80GB GPU. In addition, the prefilling latency of long-sequence input is
extremely high. For instance, using four H100 GPUs, the 7B language model (augmented with
Flash-Decoding [ DHMS23 ] and kernel fusion) requires about 110 seconds to prefill 450K tokens, and
380 seconds for 1M length. The above bottlenecks make it difficult to deploy long-context language
models in practice.
In this work, we propose a decoder-decoder architecture, YOCO, for large language models, which
only caches KV pairs once. Specifically, we stack cross-decoder upon self-decoder. Given an input
sequence, the self-decoder utilizes efficient self-attention to obtain KV caches. Then the cross-decoder
layers employ cross-attention to reuse the shared KV caches. The decoder-decoder architecture is
conceptually similar to encoder-decoder, but the whole model behaves more like a decoder-only
model from the external view. So, it naturally fits into autoregressive generation tasks, such as
language modeling. First, because YOCO only caches once2, the GPU memory consumption of KV
caches is significantly reduced. Second, the computation flow of the decoder-decoder architecture
enables prefilling to early exit before entering the self-decoder. The nice property speeds up the prefill
stage dramatically, improving user experience for long-context language models. Third, YOCO
allows for more efficient system design for distributed long-sequence training. In addition, we
propose gated retention for self-decoder, which augments retention [ SDH+23] with a data-controlled
gating mechanism.
We conduct extensive experiments to show that YOCO achieves favorable language modeling perfor-
mance and has many advantages in terms of inference efficiency. Experimental results demonstrate
that YOCO can be scaled up with more training tokens, larger model size, and longer context length.
Specifically, we scale up the 3B YOCO model to trillions of training tokens, attaining results on par
with prominent Transformer language models, such as StableLM [ TBMR ]. Moreover, the scaling
curves ranging from 160M to 13B show that YOCO are competitive compared to Transformer. We
also extend the context length of YOCO to 1M tokens, achieving near perfect needle retrieval accuracy.
In the multi-needle test, YOCO obtains competitive results even compared to larger Transformers.
In addition to good performance on various tasks, the profiling results show that YOCO improves the
GPU memory footprint, prefill latency, throughput, and serving capacity. In particular, the memory of
KV caches can be reduced by about 80√ófor 65B models. Even for a 3B model, the overall inference
memory consumption can be reduced by two times for 32K tokens and by more than nine times for
1M tokens. The prefill stage is speeded up by 71.8√ófor the 1M context and 2.87√ófor the 32K input.
For example, for a 512K context, YOCO reduces the Transformer prefilling latency from 180 seconds
to less than six seconds. The results position YOCO as a strong candidate model architecture for
future large language models with native long-sequence support.
2The word ‚Äúonce‚Äù refers to global KV cache. Strictly, self-decoder also needs to store a certain number of
caches. As the self-decoder utilizes an efficient attention module, the cache size is bounded to a constant, which
can be ignored compared to global caches when the sequence length is large.
2

--- PAGE 3 ---
Figure 2: Overview of the decoder-decoder architecture. Self-decoder generates the global KV cache.
Then cross-decoder employs cross-attention to reuse the shared KV caches. Both self-decoder and
cross-decoder use causal masking. The overall architecture behaves like a decoder-only Transformer,
autoregressively generating tokens.
2 You Only Cache Once (YOCO)
The proposed architecture, named YOCO, is designed for autoregressive modeling, such as large
language models (LLMs). As shown in Figure 2, the decoder-decoder architecture has two parts, i.e.,
self-decoder and cross-decoder. Specifically, YOCO is stacked with Lblocks, where the firstL
2layers
are self-decoder while the rest modules are cross-decoder. Given an input sequence x=x1¬∑¬∑¬∑x|x|,
the input embeddings are packed into X0= [x1,¬∑¬∑¬∑,x|x|]‚ààR|x|√ódmodel, where dmodel is hidden
dimension. We first obtain contextualized vector representations Xl= Self-Decoder( Xl‚àí1), l‚àà
[1,L
2], where XL/2is used to produce KV caches ÀÜK,ÀÜVfor cross-decoder. Then we compute
Xl= Cross-Decoder( Xl‚àí1,ÀÜK,ÀÜV), l‚àà[L
2+ 1, L]to get the output vectors XL.
Both self- and cross-decoder follow a similar block layout (i.e., interleaved attention and feed-forward
network) as in Transformer [ VSP+17]. We also include pre-RMSNorm [ ZS19 ], SwiGLU [ Sha20 ],
and grouped-query attention [ ALTdJ+23] as improvements. The difference between the two parts
lies in attention modules. Self-decoder (Section 2.1) uses efficient self-attention (e.g., sliding-window
attention). In comparison, cross-decoder (Section 2.2) uses global cross-attention to attend to the
shared KV caches produced by the output of the self-decoder.
2.1 Self-Decoder
Self-decoder takes token embeddings X0as input and compute intermediate vector representation
M=XL/2:
Yl= ESA(LN( Xl)) +Xl
Xl+1= SwiGLU(LN( Yl)) +Yl(1)
where ESA(¬∑)represents efficient self-attention, SwiGLU( X) = (swish( XW G)‚äôXW 1)W2, and
RMSNorm [ZS19] is used for LN(¬∑). Causal masking is used for efficient self-attention.
3

--- PAGE 4 ---
Cross -Decoder
(Skipped)
Self-DecoderKV CacheCross -DecoderPrefilling Generation
Pre-    filling  context  and   then  generatethen  generate  newFigure 3: YOCO Inference. Prefill : encode input to-
kens in parallel. Generation : decode output tokens
one by one. The computation flow enables prefill-
ing to early exit without changing the final output,
thereby significantly speeding up the prefill stage.KV Cache Memory
Transformer O(LND )
YOCO O((N+L)D)
Table 1: Inference memory complexity of KV
caches. N, L, D are the sequence length, num-
ber of layers, and hidden dimension.
Prefilling Time
Transformer O(LN2D)
YOCO O(LND )
Table 2: Prefilling time complexity of attention
modules. N, L, D are the same as above.
The key property of the efficient self-attention module is O(1)inference memory, i.e., constant
number of KV caches. For example, the cache size of sliding-window attention [CGRS19] depends
on the window size instead of the input length. More design choices (e.g., gated retention) of the
efficient self-attention module are detailed in Section 3.
2.2 Cross-Decoder
First, the output of the self-decoder XL/2generates global KV caches ÀÜK,ÀÜVfor cross-decoder:
ÀÜK= LN( XL/2)WK,ÀÜV= LN( XL/2)WV (2)
where WK, WV‚ààRd√ódare learnable weights. Then, cross-decoder layers are stacked after the
self-decoder to obtain the final output vectors XL. The KV caches ÀÜK,ÀÜVare reused by all theL
2cross-decoder modules:
ÀÜQl= LN( Xl)Wl
Q
Yl= Attention( ÀÜQl,ÀÜK,ÀÜV) +Xl
Xl+1= SwiGLU(LN( Yl)) +Yl(3)
where Attention( ¬∑)is standard multi-head attention [ VSP+17], and Wl
Q‚ààRd√ódis a learnable
matrix. Causal masking is also used for cross-attention. Because cross-attention is compatible with
group query attention [ ALTdJ+23], we can further save the memory consumption of KV caches.
After obtaining XL, asoftmax classifier performs next-token prediction.
2.3 Inference Advantages
In addition to competitive language modeling results, YOCO significantly reduces serving costs and
improves inference performance. We report detailed inference comparisons in Section 4.4.
Saving GPU Memory and Serving More Tokens. Table 1 compares the memory complexity
between Transformers and YOCO. Specifically, because global KV caches are reused and efficient
self-attention needs constant caches, the number of caches is O(N+CL), where Nis the input
length, Cis a constant (e.g., sliding window size), and Lis the number of layers. For long sequences,
CLis much smaller than N, so about O(N)caches are required, i.e., you only cache once.
In comparison, Transformer decoders have to store N√óLkeys and values during inference. So
YOCO roughly saves Ltimes GPU memory for caches compared to Transformer decoders. Because
the inference capacity bottleneck becomes KV caches (Figure 7b), our method enables us to serve
4

--- PAGE 5 ---
many more tokens without being out of GPU memory. The increased batch size is also beneficial to
inference throughput.
Reducing Prefilling Time and Improving Throughput. As shown in Figure 3, because the
cross-decoder reuses the outputs of self-decoder, we can exit early before entering the cross-decoder
during the prefill stage. The intriguing property of computation dependency greatly accelerates the
prefilling speed.
First, only half the layers are needed for forward computation, i.e., at least half prefilling latency
reduction. Second, the efficient attention modules of the self-decoder are usually fast. For the example
of 512K context length, we can decrease the prefilling latency from 180 seconds (Transformer with
optimized inference, such as Flash-Decoding and kernel fusion) to less than 6 seconds (Figure 9).
Even for 32K length, YOCO has about three times speedup in terms of prefilling time. Table 2
compares prefilling time complexity of attention modules between Transformer and YOCO.
3 Design Choices of Self-Decoder
We can choose various efficient self-attention methods for self-decoder. As long as the module only
requires constant inference memory, the cache memory complexity of the self-decoder depends on
the number of layers. Moreover, a good module choice improves both training and deployment costs.
In this work, we use gated retention (Section 3.1) or sliding-window attention (Section 3.2).
3.1 Gated Retention
Gated retention (gRet, aka gRetNet or RetNet-3) augments retention [ SDH+23] with a data-dependent
gating mechanism, which achieves training parallelism, good performance, and low inference cost
simultaneously for sequence modeling. We use gRet as the default efficient self-attention module
in the experiments. The method unifies the parallel, recurrent, and chunkwise recurrent computa-
tion paradigms. These three representations are equivalent and can obtain the same computation
results. The training process usually uses the parallel or chunkwise recurrent paradigms, while the
inference stage can employ the recurrent paradigm for constant KV memory. We describe the three
representations as follows:
The Parallel Representation The gated retention is defined as:
Q= (XW Q)‚äôŒò, K = (XW K)‚äôŒò, V =XW V,Œòn=einŒ∏
Œ≥= sigmoid( XW Œ≥)1/œÑ, D nm=Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥nY
i=m+1Œ≥i, n‚â•m
0, n < m
gRet( X) = (QK‚ä∫‚äôD)V(4)
where WQ, WK, WV‚ààRd√ódandWŒ≥‚ààRd√ó1are learnable weights, and the temperature term œÑen-
courages Œ≥to 1 for better memorization [ YWS+23]. The data-controlled decay is head-wise [ Kat23 ]
rather than element-wise so that the computation can fully utilize NVIDIA tensor cores. Refer to
[SDH+23] for more details about the other designs.
The Recurrent Representation Being equivalent to Equation (4), the output of gated retention can
be computed recurrently. For the n-th timestep, the output is obtained via:
Sn=Œ≥nSn‚àí1+K‚ä∫
nVn
gRet( Xn) =QnSn, n = 1,¬∑¬∑¬∑,|x| (5)
where Q, K, V, Œ≥ are the same as in Equation (4). During auto-regressive inference, the self-decoder
maintains Snas the intermediate state for an efficient generation.
The Chunkwise Recurrent Representation The chunk-wise representation is a unified formulation
of recurrent and parallel representations. Given chunk size B, the outputs are computed chunk by
chunk. The computation is divided into inner-chunk and cross-chunk parts. Denote [i]as the i-th
5

--- PAGE 6 ---
chunk, i.e., x[i]=x(i‚àí1)B+1,¬∑¬∑¬∑, xiB, we compute the i-th chunk as:
Œ≤(i‚àí1)B+j=(i‚àí1)B+jY
k=(i‚àí1)B+1Œ≥k, D [i](j, k) =Œ≤(i‚àí1)B+k
Œ≤(i‚àí1)B+jifj‚â§kelse 0
Ri=K‚ä∫
[i](V[i]‚äôŒ≤iB
Œ≤[i]) +Œ≤iBRi‚àí1, Œ≤[i](j, k) =Œ≤(i‚àí1)B+j
gRet( X) = (Q[i]K‚ä∫
[i]‚äôD[i])V[i]| {z }
Inner-Chunk+ (Q[i]Ri‚àí1)‚äôŒ≤[i]|{z }
Cross-Chunk(6)
where Riis the intermediate state of the i-th chunk, and Œ≤summarizes the data-controlled decay Œ≥.
The proof in Appendix B shows the equivalence between the computation paradigms. The chunkwise
paradigm combines the best of parallelism and recurrence, i.e., saving FLOPs compared with fully
parallel computation and reducing the iterations compared to recurrent computation. During the
training and prefill stages, the chunk-wise representation increases throughput and reduces GPU
memory consumption.
Multi-Head Gated Retention Similar to multi-head attention [ VSP+17] and multi-scale reten-
tion [SDH+23], we apply gated retention to each head and combine the outputs together:
head i= gRet( X)
Y= GroupNormh(Concat(head 1,¬∑¬∑¬∑,head n))
MHGR( X) = (swish( XW G)‚äôY)WO(7)
where WG, WO‚ààRd√ódare learnable matrices, and GroupNorm [WH18 ] normalizes each
head [WMH+23]. We also apply swish gate to increase non-linearity [SDH+23].
3.2 Sliding-Window Attention
Sliding-window attention [ CGRS19 ] restricts the attention range into a fixed window size C. In
contrast, vanilla Transformer decoders attend to all previous tokens. During inference, the KV cache
memory complexity can be reduced from O(N)toO(C), i.e., the memory usage is constant rather
than increasing with sequence length. Similar to multi-head self-attention [ VSP+17], we compute
the output of sliding-window attention via:
Q=XW Q, K =XW K, V =XW V
head i= softmax( Q[i]K‚ä∫
[i]+B)V
Bij=0, i ‚àíC < j ‚â§i
‚àí ‚àû, otherwise
Y= Concat(head 1,¬∑¬∑¬∑,head h)
SWA( X) =Y WO(8)
where WQ, WK, WV, WO‚ààRd√ódare learnable matrices, and the window causal mask Bcontrols
each query only attends to the previous keys whose distances are less than C. The pre-normalization
and residual connection are also applied to the module.
4 Experiments
We evaluate YOCO for large language models from the following perspectives. First, we follow the
setting of StableLM-3B-4E1T [ TBMR ] to scale up training tokens (Section 4.1). Second, we present
the scaling curves of the proposed architectures (Section 4.2). Third, we scale up the YOCO model
to 1M context length and evaluate its long-sequence modeling capability (Section 4.3). Fourth, we
analyze the deployment advantages, including GPU memory footprint, serving capacity, prefilling
time, and throughput (Section 4.4). Experimental results show that YOCO achieves competitive
performance across various evaluation metrics. More importantly, the proposed method significantly
reduces the inference cost.
6

--- PAGE 7 ---
Model ARC-C ARC-E BoolQ Hellaswag OBQA PIQA Winogrande SciQ Avg
Training with 1T tokens
OpenLLaMA-3B-v2 0.339 0.676 0.657 0.700 0.260 0.767 0.629 0.924 0.619
StableLM-base-alpha-3B-v2 0.324 0.673 0.646 0.686 0.264 0.760 0.621 0.921 0.612
StableLM-3B-4E1T ‚Äî 0.666 ‚Äî ‚Äî ‚Äî 0.768 0.632 0.914 ‚Äî
YOCO-3B 0.379 0.731 0.645 0.689 0.298 0.763 0.639 0.924 0.634
Training with 1.6T tokens
StableLM-3B-4E1T ‚Äî 0.688 ‚Äî ‚Äî ‚Äî 0.762 0.627 0.913 ‚Äî
YOCO-3B 0.396 0.733 0.644 0.698 0.300 0.764 0.631 0.921 0.636
Extending context length to 1M tokens
YOCO-3B-1M 0.413 0.747 0.638 0.705 0.300 0.773 0.651 0.932 0.645
Table 3: Eval Harness [ GTA+23] results compared with previous well-trained Transformer language
models [ TBMR ,Tow,GL23 ]. We scale the 3B model to 1.6 trillion training tokens. The 1T and
1.6T results of StableLM-3B-4E1T are taken from its technical report [ TBMR ]. YOCO-3B-1M is
extended to the context length of 1M tokens.
4.1 Language Modeling Evaluation
We train a 3B-size YOCO language models by scaling up the number of training tokens. Then we
compare the checkpoints with strong Transformer-based language models.
Setup We use a similar training recipe as in StableLM-3B-4E1T [ TBMR ]. We adjust the head
dimension to 128 instead of 80 as in StableLM for better kernel support. In order to keep the model
size unchanged, we set the hidden size to 3072 and the number of layers to 26. Grouped-query
attention [ ALTdJ+23] is used, where the number of query heads is 24, and the number of key-value
heads is 8. We train YOCO with gated retention (Section 3.1). The non-embedding parameter
count is 2.8B. In comparison, StableLM-3B-4E1T is 2.7B and OpenLLaMA-v2-3B [ GL23 ] is 3.2B.
The training sequence length is 4096. The batch size is 4M tokens. We use the AdamW [ LH19 ]
optimizer with Œ≤= 0.9,0.95. The maximal learning rate is 3.2e-4 with 1000 warmup steps and linear
decay to 1.28e-5. The total schedule is set to 5T tokens. We train the model with 400k steps (i.e.,
1.6T tokens) given the resource budget. The curated training corpus is similar to [ TBMR ]. We use
tiktoken-cl100k_base as the tokenizer. Detailed hyperparameters are described in Appendix C.
Results Table 3 compares the YOCO checkpoints with OpenLLaMA-v2-3B [ GL23 ], StableLM-
base-alpha-3B-v2 [ Tow], and StableLM-3B-4E1T [ TBMR ]. We use LM Eval Harness [ GTA+23] to
evaluate the zero-shot performance on various downstream tasks. OpenLLaMA-v2-3B and StableLM-
base-alpha-3B-v2 are trained with 1T tokens. The intermediate numbers of StableLM-3B-4E1T
are taken from its technical report [ TBMR ]. Experimental results across end tasks indicate that
YOCO achieves comparable results with previous well-tuned Transformer language models. Both the
checkpoints trained with 1T tokens and 1.6T tokens obtain consistent trend. Moreover, the results
show that YOCO is scalable in terms of training tokens.
4.2 Scalability Compared with Transformers
100101
#Parameters (B)2.93.03.13.23.33.43.53.6LossTransformer
YOCOSWA
YOCOgRet
Figure 4: LM loss decreases along with scaling up
the model size (ranging from 160M to 13B).We compare the scaling curves between
Llama Transformer [ VSP+17,TLI+23],
YOCO with gated retention (YOCO gRet; Sec-
tion 3.1), and YOCO with sliding-window
attention (YOCO SWA; Section 3.2). We train
language models of various sizes (i.e., 160M,
400M, 830M, 1.4B, 2.7B, 6.8B, and 13B) us-
ing the same training data and settings. The
validation loss is used as the evaluation met-
ric. The scaling law [ KMH+20] is supposed
to extrapolate larger-size performance.
Setup We augment the Transformer archi-
tecture with Llama [ TLI+23] improvements,
such as RMSNorm [ ZS19 ], SwiGLU [ Sha20 ],
and removing bias. The sliding window size
7

--- PAGE 8 ---
128K 256K 384K 512K 640K 768K 896K1M
Context Length0
9
18
27
36
45
54
63
72
81
90
100Depth (%)Needle Retrieval Across 1M Context ("Needle In A HayStack")
0.00.20.40.60.81.0
ScoreFigure 5: Needle-in-a-haystack results in 1M length.
Model Size N= 1 N= 2 N= 4 N= 8
YaRN-Mistral-128K [PQFS23] 7B 0.02 0.12 0.08 0.20
LWM-1M-text [LYZA24] 7B 1.00 0.90 0.76 0.62
MiniCPM-128K [HTH+24] 2.4B 1.00 1.00 0.54 0.56
ChatGLM3-128K [ZLD+22] 6B 0.94 0.72 0.52 0.44
YOCO-3B-1M 3B 0.98 0.98 0.84 0.56
Table 4: Multi-needle retrieval accuracy. Nindicates the number of needles. N= 1is single-needle
retrieval used as a reference, and N > 1indicates the multi-needle test. The evaluation is conducted
in 128K length, because most previous long-context models are tuned with this length.
of YOCO SWAis 1,024. We align the number of parameters by adjusting the FFN intermediate
dimension. The training batch size is 0.25M tokens with a 2k sequence length. We train the models
with 40k steps, i.e., 10B tokens. In practice, we find that the setting is effective for loss convergence,
and the scaling laws can be well-fitted. More hyperparameters are detailed in Appendix D.
Results Figure 4 reports the validation loss with various parameter counts. We also fit the scaling
curves as in [ KMH+20]. YOCO obtains comparable performance from 160M to 13B compared to the
Llama-optimized transformer architecture. The findings demonstrate that YOCO scales effectively
with respect to model size. Moreover, YOCO gRetoutperforms Transformer and YOCO SWA. The
gains come from hybrid architectures of attention and retention, whose inductive biases tend to be
complementary to each other. We observed similar gains by interleaving the attention and retention
modules (1:3). Recent hybrid architectures [LLB+24] also confirm similar findings.
4.3 Long-Context Evaluation
We extend the context length of YOCO-3B (Section 4.1) to 1M tokens. We evaluate long-context
models on needle retrieval and language modeling tasks.
We continue the model training with longer lengths progressively. The length schedule is 64K, 256K,
and 1M tokens. The batch size is kept the same as before. The learning rate and RoPE [ SLP+21]
Œ∏are set as in Table 7. Training data is up-sampled according to sequence length [ FPN+24]. For a
fair comparison, we do not use long-instruction tuning data. More training details are described in
Appendix E. A chunk parallelism algorithm for YOCO is proposed in Appendix A, which reduces
communication overhead and GPU memory fragmentation in our experiments of 1M length.
Needle In A Haystack The pressure test evaluates whether models can retrieve ‚Äúneedles‚Äù from a long
document [ Kam23 ]. We follow the evaluation setting of Gemini 1.5 [ RST+24] and LWM [ LYZA24 ].
The needles are constructed as a city with a magic number. We run 10 times at the same depth
and length. The averaged accuracy is reported. Figure 5 shows that YOCO-3B-1M passes the
Needle-In-A-Haystack test with near perfect accuracy. The results indicate that YOCO has strong
long-context modeling capability.
8

--- PAGE 9 ---
(a) Book data.
 (b) Repository-level code data.
Figure 6: Cumulative average negative log-likelihood on book and repository-level code. We filter
the validation examples that are longer than 1M tokens. YOCO achieves improved performance with
longer context, i.e., utilizing long-distance information for language modeling.
Multi-Needle Retrieval Besides the above single-needle retrieval, we conduct a multi-needle
evaluation. We compare YOCO-3B-1M with previous long-context language models, including
MiniCPM-128K [ HTH+24], ChatGLM3-128K [ ZLD+22], YaRN-Mistral-128K [ PQFS23 ], and
LWM-1M-text [ LYZA24 ]. The evaluation is conducted in 128K sequence length, because most
previous models are tuned with this length.
Table 4 reports the accuracy with Nneedles. Among these models, LWM-1M-text and YOCO-3B-1M
are trained with a 1M context length, while the others are in 128K length. Although LWM-1M-text
continues training of Llama-2-7B, YOCO-3B-1M can still achieve comparable performance with
half the model size. Moreover, the 7B-size YaRN-Mistral-128K [ PQFS23 ] obtained by postion
interpolation lags behind the other models. Compared to MiniCPM-128K and ChatGLM3-128K,
YOCO-3B-1M also outperforms these well-trained language models.
Perplexity over Long Sequences Figure 6 shows the cumulative average negative log-likelihood
(NLL) as a function of context length. We evaluate both book and repository-level code data. We
follow the setting of [ RST+24] and filter validation data that are longer than 1M tokens. NLL
decreases consistently with longer sequence length. The results indicate that YOCO can effectively
utilize long-distance dependency for language modeling. We also observe that the NLL-length curves
tend to fit the power law, where the gaps are affected by the noise within the validation examples.
4.4 Inference Advantages
We analyze inference efficiency from various perspectives, such as GPU memory footprint, prefilling
latency, throughput, and serving capacity. We demonstrate that YOCO reduces the deployment
cost by orders of magnitude, especially for long-sequence inference. More importantly, the user
experience (such as latency) is improved while maintaining good performance and reducing expenses.
We compare YOCO gRetwith Transformer. The default model configuration follows Section 4.1.
Notice that Transformer uses grouped-query attention [ ALTdJ+23], Flash-Decoding [ DHMS23 ], and
kernel fusion for a fair comparison. As described in Section 3.1, gated retention uses the chunk-
recurrent representation in the prefill stage, and the recurrent representation in the generation stage.
The chunk size is set to 256. We implement a Triton [ TC19 ] kernel for gated retention. The evaluation
sequence length is ranging from 32K to 1M. The last 1,024 tokens are supposed to be generated,
while the previous tokens are given input context. The experiments are conducted with H100-80GB
GPU cards.
GPU Memory The inference memory consumption is made up of three parts, namely model weights,
intermediate activation, and KV cache. Figure 7b presents the breakdown memory profiling results.
Along with an increase in context length, the main memory bottleneck becomes KV caches, while
model weights consume constant memory. The results show that YOCO gRetalleviates the activation
cost and KV cache memory footprint.
9

--- PAGE 10 ---
32K 256K 512K 1M
Length20406080100120GPU Memory (GB)
4.16x6.39x9.38x
 32K 64K 128K01530
1.95x2.32x3.01xTransformer
YOCO(a) Inference memory of Transformer and YOCO across various lengths.
Transformer YOCO20406080100120GPU Memory (GB)9.38xKV Cache
Weight
Other(b) Breakdown memory con-
sumption in 1M context length.
Figure 7: GPU memory consumption during inference.
1.2B 6.4B 13B 30B 65B
Model Size0100200300400500600KV Cache Memory Consumption 
 (KB / T oken)
24x32x40x64x80xTransformer
YOCO
Figure 8: GPU memory consumption of KV cache for each token with different model size. YOCO
can save more for larger model size.
As shown in Figure 7a, the memory cost is significantly reduced using YOCO. Moreover, the memory
consumption of YOCO increases slowly along the sequence length. For example of 1M length, the
overall inference memory usage is only 12.4GB, while Transformers occupy 9.4√óGPU memory.
YOCO makes it feasible to deploy long-sequence modeling on customer-level GPUs. Even with
a 32K sequence length, YOCO requires about 2√óless memory than Transformer. Although we
compare 3B-size models here, the reduction ratio becomes larger as the number of layers increases.
Figure 8 reports the GPU memory consumption of KV cache for each token. As YOCO only caches
one layer of global key-value pairs, it needs roughly Ltimes fewer memory compared to Transformer.
For example, YOCO can serve 128K tokens with 1GB GPU memory, while Transformer with
GQA [ALTdJ+23] can only support 1.6K tokens at 65B model size.
Prefilling Latency In the prefill stage, the model encodes input tokens in parallel. As shown in
Figure 9, the prefilling latency is a pain point of user experience for long-context models. For 512K-
and 1M-length input sequences, Transformer needs about 180 seconds and 300 seconds, respectively.
The computational complexity of Transformer is O(N2), which requires a large number of FLOPs
for long context. In contrast, YOCO‚Äôs prefilling time is O(N), growing linearly (Section 2.3) along
the sequence length.
Figure 9 shows that YOCO reduces the Transformer prefilling time from 180 seconds to less than 6
seconds for 512K context. As described in Section 2.3, the prefill stage can early exit before entering
cross-decoder. So, there is at least two times speedup of prefilling latency even for short context. For
example, YOCO is 2.87√ófaster than Transformer for 32K length.
10

--- PAGE 11 ---
32K 256K 512K 1M
Length050100150200250300Prefilling Time (s)
15.55x30.3x71.82x
 32K 64K 128K02040
2.87x5.05x8.36xTransformer
YOCOFigure 9: Prefilling latency for different length, i.e., the encoding time of given input prompt before
generating the first token. Transformer‚Äôs time grows quadratically while YOCO‚Äôs grows linearly.
Even for a short input length, such as 32K, YOCO can still accelerate 2.87√ó.
32K 64K 128K 256K 512K
Context Length0100200300400500600Throughput (token/s)2.72x
2.57x
2.77x
4.37x9.56xTransformer
YOCO
Figure 10: Inference throughput of Transformer and YOCO varying the context length.
Throughput The throughput indicates how many tokens the model can process per second, involving
both pre-filling and generation time. Figure 10 shows that YOCO achieves higher throughput across
context lengths compared to Transformer. For the example of 512K queries, Transformer‚Äôs throughput
is 4.5 token/s while YOCO reaches 43.1 token/s, i.e, achieving 9.6√óspeedup. The throughput is
improved for the following reasons. First, YOCO decreases the time required for prefilling as
previously demonstrated. Second, as the memory consumption is reduced, we can use larger batch
size for inference, which also contributes to the throughput improvement.
5 Conclusion
In this work, we propose a decoder-decoder architecture (YOCO) for large language modeling.
YOCO achieves significantly better inference efficiency and competitive performance compared with
Transformers. Experimental results demonstrate that YOCO achieves favorable results for large
language models under various settings, i.e., scaling up number of training tokens, scaling up model
size, and scaling up context length to 1M tokens. Profiling results also show that YOCO improves
inference efficiency by orders of magnitude, especially for long-sequence modeling.
The work can be advanced from the following perspectives:
‚Ä¢YOCO + BitNet + Groq. Groq achieves very high throughput by putting all things within SRAM.
However, the memory capacity bottleneck limits the model size and input token count. Now,
hundreds of chips are connected to host just one model. As a solution, YOCO reduces KV cache
11

--- PAGE 12 ---
memory, and BitNet reduces model weight memory. The LLM deployment cost is expected to be
reduced by orders of magnitude using the above combination.
‚Ä¢YOCO for Multimodal Large Language Models. The YOCO layout is general to the use of
multiple self-decoders. The cross-attention layers are natural for multimodal fusion [ BWD+22,
WBD+22]. The causal dependency of self-decoders also perfectly fits in streaming video. The
async multimodal large language models can avoid different data steams block each other, which is
critical for real-time applications, such as robotics.
‚Ä¢Optimized Mechanism for KV Cache Module. Figure 2 explicitly highlights KV cache, which
opens up new opportunities to develop native memory mechanisms. First, we can integrate
a cache compression mechanism to obtain more compact memory. Second, we can build an
index [ WDC+23] for efficient key-value retrieval. As YOCO reuses caches, it enables us to
maintain only one index rather than creating an index for each layer. Third, the disentangled
modeling supports pre-caching context, which is potentially useful for native RAG and LLM-native
search engines.
Acknowledgement
We would like to acknowledge Ben Huntley for maintaining the GPU cluster. The long-sequence
training utilizes CUBE , which is an internal version of [ LML+23]. We implement the Triton kernel of
gated retention based on FLA[YZ24].
References
[AET+23]Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James
Zou, Atri Rudra, and Christopher R√©. Zoology: Measuring and improving recall in
efficient language models. arXiv preprint arXiv:2312.04927 , 2023.
[ALTdJ+23]Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico
Lebr√≥n, and Sumit Sanghai. Training generalized multi-query transformer models from
multi-head checkpoints. arXiv preprint arXiv:2305.13245 , 2023.
[BMR+20]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-
hini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher
Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language
models are few-shot learners. In Advances in Neural Information Processing Systems ,
volume 33, pages 1877‚Äì1901. Curran Associates, Inc., 2020.
[BWD+22]Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed, Kriti Aggar-
wal, Subhojit Som, Songhao Piao, and Furu Wei. VLMo: Unified vision-language pre-
training with mixture-of-modality-experts. In Alice H. Oh, Alekh Agarwal, Danielle
Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing
Systems , 2022.
[CGRS19] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences
with sparse Transformers. URL https://openai.com/blog/sparse-transformers , 2019.
[DCLT19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-
training of deep bidirectional transformers for language understanding. In Proceedings
of the 2019 Conference of the North American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) ,
pages 4171‚Äì4186, Minneapolis, Minnesota, June 2019. Association for Computational
Linguistics.
[DFS+22]Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher
R√©. Hungry hungry hippos: Towards language modeling with state space models. arXiv
preprint arXiv:2212.14052 , 2022.
12

--- PAGE 13 ---
[DHMS23] Tri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov. Flash-Decoding for long-
context inference. https://crfm.stanford.edu/2023/10/12/flashdecoding.
html , 2023.
[DMD+23]Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang,
Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens.
arXiv preprint arXiv:2307.02486 , 2023.
[FPN+24]Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hanna Hajishirzi, Yoon Kim, and
Hao Peng. Data engineering for scaling language models to 128k context. ArXiv ,
abs/2402.10171, 2024.
[GD23] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state
spaces. arXiv preprint arXiv:2312.00752 , 2023.
[GL23] Xinyang Geng and Hao Liu. OpenLLaMA: An open reproduction of LLaMA. https:
//github.com/openlm-research/open_llama , 2023.
[GTA+23]Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi,
Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac‚Äôh, Haonan Li, Kyle
McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey
Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang,
Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12
2023.
[HTH+24]Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng,
Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the po-
tential of small language models with scalable training strategies. arXiv preprint
arXiv:2404.06395 , 2024.
[Kam23] Greg Kamradt. Needle in a Haystack - pressure testing llms. https://github.com/
gkamradt/LLMTest_NeedleInAHaystack/tree/main , 2023.
[Kat23] Tobias Katsch. Gateloop: Fully data-controlled linear recurrence for sequence modeling.
arXiv preprint arXiv:2311.01927 , 2023.
[KMH+20]Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess,
Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws
for neural language models. CoRR , abs/2001.08361, 2020.
[LH19] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Interna-
tional Conference on Learning Representations , 2019.
[LLB+24]Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedi-
gos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri
Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman,
Avashalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, Mor Zusman, and
Yoav Shoham. Jamba: A hybrid Transformer-Mamba language model. CoRR ,
abs/2403.19887, 2024.
[LML+23]Zhiqi Lin, Youshan Miao, Guodong Liu, Xiaoxiang Shi, Quanlu Zhang, Fan Yang,
Saeed Maleki, Yi Zhu, Xu Cao, Cheng Li, Mao Yang, Lintao Zhang, and Lidong Zhou.
SuperScaler: Supporting flexible DNN parallelization via a unified abstraction, 2023.
[LXLY21] Shenggui Li, Fuzhao Xue, Yongbin Li, and Yang You. Sequence parallelism: Making
4d parallelism possible. arXiv preprint arXiv:2105.13120 , 2021.
[LYZA24] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length
video and language with ringattention. arXiv preprint arXiv:2402.08268 , 2024.
[LZA23] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers
for near-infinite context. arXiv preprint arXiv:2310.01889 , 2023.
13

--- PAGE 14 ---
[PDC+22]Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Brad-
bury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean.
Efficiently scaling Transformer inference. ArXiv , abs/2211.05102, 2022.
[PQFS23] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient
context window extension of large language models. arXiv preprint arXiv:2309.00071 ,
2023.
[RSR+20]Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learn-
ing with a unified text-to-text transformer. Journal of Machine Learning Research ,
21(140):1‚Äì67, 2020.
[RST+24]Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap,
Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrit-
twieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of
tokens of context. arXiv preprint arXiv:2403.05530 , 2024.
[SDH+23]Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong
Wang, and Furu Wei. Retentive network: A successor to transformer for large language
models. arXiv preprint arXiv:2307.08621 , 2023.
[Sha20] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202 ,
2020.
[SIE+23]Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. Zeroscrolls: A
zero-shot benchmark for long text understanding. arXiv preprint arXiv:2305.14196 ,
2023.
[SLP+21]Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced
transformer with rotary position embedding. arXiv preprint arXiv:2104.09864 , 2021.
[TBMR] Jonathan Tow, Marco Bellagente, Dakota Mahan, and Carlos Riquelme. StableLM 3B
4E1T. https://aka.ms/StableLM-3B-4E1T .
[TC19] Philippe Tillet and David Cox. Triton: an intermediate language and compiler for tiled
neural network computations. In Proceedings of the 3rd ACM SIGPLAN International
Workshop on Machine Learning and Programming Languages , pages 10‚Äì19, 2019.
[TLI+23]Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,
Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar,
et al. Llama: Open and efficient foundation language models. arXiv preprint
arXiv:2302.13971 , 2023.
[Tow] Jonathan Tow. StableLM Alpha v2 models. https://huggingface.co/
stabilityai/stablelm-base-alpha-3b-v2 .
[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.
Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in
Neural Information Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA , pages 6000‚Äì
6010, 2017.
[WBD+22]Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti
Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as
a foreign language: BEiT pretraining for all vision and vision-language tasks. arXiv
preprint arXiv:2208.10442 , 2022.
[WDC+23]Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and
Furu Wei. Augmenting language models with long-term memory. In Thirty-seventh
Conference on Neural Information Processing Systems , 2023.
[WH18] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European
conference on computer vision (ECCV) , pages 3‚Äì19, 2018.
14

--- PAGE 15 ---
[WMH+23]Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang
Peng, Yu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, Barun Patra, Zhun Liu,
Vishrav Chaudhary, Xia Song, and Furu Wei. Magneto: A foundation Transformer. In
Proceedings of the 40th International Conference on Machine Learning , volume 202,
pages 36077‚Äì36092, 2023.
[XLM+23]Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui
Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al.
Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039 ,
2023.
[YWS+23]Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim.
Gated linear attention transformers with hardware-efficient training. arXiv preprint
arXiv:2312.06635 , 2023.
[YZ24] Songlin Yang and Yu Zhang. FLA: A Triton-based library for hardware-efficient imple-
mentations of linear attention mechanism. https://github.com/sustcsonglin/
flash-linear-attention , 2024.
[ZLD+22]Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi
Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. GLM-130B: An open bilingual pre-
trained model. arXiv preprint arXiv:2210.02414 , 2022.
[ZS19] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in
Neural Information Processing Systems , 32, 2019.
15

--- PAGE 16 ---
A Chunk Parallelism for Long-Sequence Training of YOCO
We introduce chunk parallelism for YOCO to reduce the communication frequency, accelerating
long-sequence training. Dividing long sequences into different devices is essential when the training
length is extremely long [ LXLY21 ,DMD+23]. However, the overall throughput tends to be bounded
by GPU communication [ LZA23 ]. Cross-decoder disentangles self-attention dependency while
preserving modeling capability, bringing intriguing advantages to distributed long-sequence training.
ùëãùëã1 ùëã2
SplitSelf-Decoderùêæ ùëâùëÇ1 ùëÇ2
Cross -Decoder
ùëÄ1 ùëÄ2ProjectùëÑ1 ùêæ1 ùëâ1 ùêæ2 ùëâ2 ùëÑ2All-Gather OnceGPU 1                                      GPU 2
Figure 11: Chunk parallelism of YOCO training on two GPU devices. The training strategy is to
partition the sequence into different chunks. Mdenotes the intermediate representation XL/2, i.e.,
the output of self-decoder. The keys and values in the cross-decoder are only gathered once.
In self-decoder, the dependency only exists in the adjacent devices. For example, gated retention
only requires the hidden state Snin Equation (5), and sliding-window attention attends to tokens
within the context window. Therefore, the communication amount of self-decoder is relatively small.
In the cross-decoder, the all-gather operation is only triggered once for the KV cache, rather than
communicating in each layer. The hardware-friendly architecture gives more flexibility to distributed
long-sequence training.
B Chunk-wise Representation of Gated Retention
We illustrate the equivalence between recurrent representation and chunkwise recurrent representation
of gated retention. For the output On,ncan be split as n=kB+rwhere Bis the chunk size:
On=nX
m=1nY
i=m+1Œ≥iQnK‚ä∫
mVm=nX
m=kB+1nY
i=m+1Œ≥iQnK‚ä∫
mVm+kBX
m=1nY
i=m+1Œ≥iQnK‚ä∫
mVm
nX
m=kB+1nY
i=m+1Œ≥iQnK‚ä∫
mVm= (QnK‚ä∫
kB+1:n‚äôŒìkB+1:n)VkB+1:n
kBX
m=1nY
i=m+1Œ≥iQnK‚ä∫
mVm= (QnnY
i=kB+1Œ≥i)k‚àí1X
c=0BX
m=1(K‚ä∫
m+cBVm+cB(c+1)BY
i=m+cB+1Œ≥i)kBY
i=(c+1)B+1Œ≥i
= (Qnn‚àí1Y
i=kB+1Œ≥i)kX
c=1(K‚ä∫
[c](V[c]‚äôŒ∂[c]))kY
i=c+1Œ±i
= (Qnn‚àí1Y
i=kB+1Œ≥i)Ri‚àí1
(9)
16

--- PAGE 17 ---
where Œìi=Qn
k=i+1Œ≥i,Œ∂[c](j, k) =QcB
i=(c‚àí1)B+j+1Œ≥i,Œ±i=QiB
j=(i‚àí1)B+1Œ≥j,[i]indicates the i-th
chunk, i.e., x[i]= [x(i‚àí1)B+1,¬∑¬∑¬∑, xiB].Rnis written as a recurrent function:
Ri=K‚ä∫
[i](V[i]‚äôŒ∂[i]) +Œ±iRi‚àí1 (10)
Denote [i]as the i-th chunk, i.e., x[i]= [x(i‚àí1)B+1,¬∑¬∑¬∑, xiB],Œ≤(i‚àí1)B+j=Q(i‚àí1)B+j
k=(i‚àí1)B+1,
Œ≤[i](j, k) =Œ≤(i‚àí1)B+j, We concatenate the output in a block together:
O[n]=[n]X
m=kB+1Œ≤[n]Q[n]K‚ä∫
mVm+kBX
m=1Œ≤[n]Q[n]nY
i=m+1Œ≥iK‚ä∫
mVm
[n]X
m=kB+1Œ≤[n]Q[n]K‚ä∫
mVm= (Q[n]K‚ä∫
[n]‚äôD[n])V[n], D [n](j, k) =Œ≤(n‚àí1)B+k
Œ≤(n‚àí1)B+jifj‚â§kelse 0
kBX
m=1Œ≤[n]Q[n]nY
i=m+1Œ≥iK‚ä∫
mVm=Œ≤[n]Q[n]Ri‚àí1, R i=K‚ä∫
[i](V[i]‚äôŒ≤iB
Œ≤[i]) +Œ≤iBRi‚àí1,
O[n]= (Q[n]K‚ä∫
[n]‚äôD[n])V[n]| {z }
Inner-Chunk+ (Q[n]Rn‚àí1)‚äôŒ≤[n]| {z }
Cross-Chunk
(11)
Finally, we show that the chunkwise recurrent representation of gated retention is equivalent to the
other two representations.
C Hyperparameters for YOCO-3B
We describe the hyperparameters used for Section 4.1. The hidden dimension is set to 3072. The
number of layers is 26. The number of query heads is 24, and the number of key/value heads is 8 with
grouped-query attention [ ALTdJ+23]. The total number of parameters without embedding is 2.83B.
The training batch size is 4M tokens. We use 4096 training length. The optimizer is AdamW [ LH19 ]
withŒ≤= (0.9,0.95). The learning rate is 3.2√ó10‚àí4with 1000 warmup steps. We set a 5T-token
learning rate schedule with linear decay to 1.28√ó10‚àí5.
Params Values
Layers 26
Hidden size 3072
FFN size 8192
V ocab size 100,288
Heads 24
Key-value heads 8
Adam Œ≤ (0.9, 0.95)
LR 3.2√ó10‚àí4
Batch size 4M
Warmup steps 1000
Weight decay 0.1
Dropout 0.0
Table 5: Hyperparamters used for the YOCO-3B model in Section 4.1.
D Hyperparameters for Scaling Curves
We describe the hyperparameters used for Section 4.2. Table 6 reports the hidden dimension, number
of layers, and number of heads used for different model sizes. The head dimension of gated retention
is set to 256. To align the number of parameters, the FFN size for Transformer is8
3dwhile the FFN
17

--- PAGE 18 ---
size for YOCO is 3d. The training length is set to 2048. The batch size is set to 0.25M tokens. We
use the AdamW [ LH19 ] optimizer with Œ≤1= 0.9, Œ≤2= 0.98. The learning rate is 1.5√ó10‚àí4for
160M to 1.4B sizes and 7.5√ó10‚àí5for 2.7B to 13B sizes. The warmup step is 375 with linear rate
decay. The weight decay is set to 0.05. We train the models with 40k steps, i.e., 10B tokens.
Size Hidden Dim. #Layers #Heads
160M 768 12 12
400M 1024 24 16
830M 1536 24 12
1.4B 2048 24 16
2.7B 2560 32 20
6.8B 4096 32 32
13B 5120 40 40
Table 6: Model size and hyper-parameters used for scaling curves in Section 4.2.
E Hyperparameters for Length Extension
We progressively extend the context length to 1M tokens in Section 4.3. The length schedule is 64K,
256K, and 1M. We up-sample the documents that are longer than the training length. Table 7 shows
that we use different RoPE Œ∏and learning rate for each stage.
Training Length 65,536 262,144 1,048,576
Learning Rate 8√ó10‚àí54√ó10‚àí52√ó10‚àí5
RoPE Œ∏ 640K 5M 80M
Training Tokens 6B 4B 1.5B
Table 7: Hyperparamters used for length extension in Section 4.3.
F Pseudo Code of Gated Retention
We present pseudocode for the three computation paradigms of gated retention (Section 3.1). Parallel
implementation enables training parallelism to fully utilize GPUs. The recurrent paradigm enables
low-cost inference. Chunkwise retention combines the above advantages (i.e., parallel within each
chunk and recurrent across chunks), which has linear memory complexity for long sequences.
def ParallelRetention(
q, # bsz ‚àónum_head ‚àólen ‚àódim
k, # bsz ‚àónum_head ‚àólen ‚àódim
v, # bsz ‚àónum_head ‚àólen ‚àódim
gt): # bsz ‚àónum_head ‚àólen
retention = q @ k.transpose( ‚àí1,‚àí2)
causal_mask = torch.full([q.shape[ ‚àí2], q.shape[ ‚àí2]], float(" ‚àíinf"), device=q.device).
triu(1).type_as(q)
gt = F.logsigmoid(gt).cumsum( ‚àí1) / gate_logit_normalizer
mask = (g[..., None] ‚àíg[..., None, :] + causal_mask).exp()
retention = retention ‚àómask
output = retention @ v
output = group_norm(output)
return output
18

--- PAGE 19 ---
def RecurrentRetention(
q, k, v, # bsz ‚àónum_head ‚àódim
past_kv, # bsz ‚àónum_head ‚àódim ‚àódim
gt # bsz ‚àónum_head ‚àó1‚àó1
):
gt = F.logsigmoid(gt) / gate_logit_normalizer
current_kv = gt.exp() ‚àópast_kv + k.unsqueeze( ‚àí1)‚àóv.unsqueeze( ‚àí2)
output = torch.sum(q.unsqueeze( ‚àí1)‚àócurrent_kv, dim= ‚àí2)
output = group_norm(output)
return output, current_kv
def ChunkwiseRetention(
q, k, v, # bsz ‚àónum_head ‚àóchunk_size ‚àódim
past_kv, # bsz ‚àónum_head ‚àódim ‚àódim
gt): # bsz ‚àónum_head ‚àóchunk_size
gt = F.logsigmoid(gt).cumsum( ‚àí1) / gate_logit_normalizer
cross_retention = (q @ past_kv) ‚àógt[..., None].exp()
inner_retention = ParallelRetention(q, k, v, gt)
retention = inner_retention + cross_retention
output = group_norm(retention)
value_decay = ( ‚àígt + gt[:, :, :, ‚àí1, None]).exp()[..., None]
chunk_decay = gt[..., ‚àí1].exp()
current_kv = chunk_decay ‚àópast_kv + k.transpose( ‚àí1,‚àí2) @ (v ‚àóvalue_decay)
return output, current_kv
G Comparisons with Transformer Variants
We compare YOCO gRetand YOCO SWAwith Transformer and other variants, including H3 [ DFS+22],
RetNet [ SDH+23], Mamba [ GD23 ], and gRetNet (Section 3.1). All models have 160M parameters
with 12 layers and a hidden dimension of 768. The weights of word embedding and softmax
projection are shared. For Mamba, we follow all the details in the paper [ GD23 ], where double-SSM
layers are implemented instead of ‚ÄúSSM + SwiGLU‚Äù. For H3, the experiment uses a hybrid version
following the original paper [ DFS+22], where attention layers are inserted into the second layer and
theL
2+ 1layer. For RetNet and gRetNet, the value dimension is dinstead of 2d, and the intermediate
dimension of SwiGLU is7
3dto match the number of parameters.
G.1 Fine-Grained LM Perplexity Results
Table 8 reports the validation perplexity for language modeling. Following Zoology [ AET+23],
we divide the perplexity into Ar-Hit , where the predicted token is a bigram previously seen in the
previous context, and First-Occur , where the predicted token cannot be recalled from the context.
Valid. Set AR-Hit First-Occur
Mamba [GD23] 3.645 1.555 4.126
RetNet [SDH+23] 3.633 1.466 4.131
Hybrid H3 [DFS+22] 3.591 1.251 4.130
gRetNet 3.600 1.354 4.116
Transformer 3.564 1.219 4.104
YOCO SWA 3.553 1.202 4.094
YOCO gRet 3.530 1.199 4.067
Table 8: Fine-grained perplexity results on language modeling. We report perplexity on both the
overall validation set and the fine-grained diagnosis sets [ AET+23], i.e., ‚ÄúAR-Hit‚Äù evaluates the
associative recall capability, and ‚ÄúFirst-Occur‚Äù indicates the regular language modeling performance.
19

--- PAGE 20 ---
G.2 Long-Context Evaluation
We evaluate the long-context modeling for the above architectures on four tasks of the Zero-
SCROLLS [ SIE+23] benchmark. We continue training the 160M models in Table 8 as long-context
models. Specifically, we further train the models with 2B tokens in 16,384 length. The rotation base
scaling [ XLM+23] is also used for length extension. For sparse Transformer, we keep the 2,048
context window and do not change the rotation base (i.e., RoPE Œ∏).
4096 8192 12288 16384
Length2.53.03.54.0PPL
Qasper
4096 8192 12288 16384
Length2.62.83.0PPL
GovReport
4096 8192 12288 16384
Length3.73.83.94.0PPL
QMSum
4096 8192 12288 16384
Length4.55.05.56.0PPL
NarrativeQA
Mamba Sparse TRM Hybrid H3 Transformer YOCOgRet
Figure 12: Long sequence task perplexity decreases along with the increasing input length.
Figure 12 reports the perplexity of the answers with different input lengths. Among all these
architectures, YOCO and Transformer consistently perform better than others across tasks and
lengths.
20
