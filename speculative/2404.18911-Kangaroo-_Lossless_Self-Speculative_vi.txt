# Kangaroo: Giải mã Đầu cơ Tự thân Không mất mát thông qua Thoát ra Sớm Kép

Fangcheng Liu†Yehui Tang†Zhenhua Liu†
Yunsheng Ni†Kai Han⋆,†Yunhe Wang⋆,†
†Huawei Noah's Ark Lab⋆Tác giả Liên hệ
{liufangcheng3,kai.han,yunhe.wang}@huawei.com

## Tóm tắt
Giải mã đầu cơ đã chứng minh hiệu quả trong việc tăng tốc suy luận của các mô hình ngôn ngữ lớn trong khi duy trì phân phối lấy mẫu nhất quán. Tuy nhiên, phương pháp thông thường là huấn luyện một mô hình nháp riêng biệt để đạt được tỷ lệ chấp nhận token thỏa mãn có thể tốn kém. Lấy cảm hứng từ thoát ra sớm, chúng tôi đề xuất một khung giải mã đầu cơ tự thân mới Kangaroo, sử dụng một mạng con nông cố định như một mô hình tự nháp, với các lớp còn lại phục vụ như mô hình đích lớn hơn. Chúng tôi huấn luyện một mô-đun adapter nhẹ và hiệu quả trên đỉnh của mạng con để kết nối khoảng cách giữa khả năng biểu diễn của mạng con và mô hình đầy đủ. Đáng chú ý là độ trễ suy luận của mô hình tự nháp có thể không còn nhỏ so với mô hình lớn, đòi hỏi các chiến lược để tăng tỷ lệ chấp nhận token trong khi giảm thiểu các bước nháp của mô hình nhỏ. Để giải quyết thách thức này, chúng tôi giới thiệu một cơ chế thoát ra sớm bổ sung để tạo ra các token nháp. Cụ thể, chúng tôi dừng dự đoán tiếp theo của mô hình nhỏ trong giai đoạn nháp một khi mức độ tin cậy cho token hiện tại giảm xuống dưới một ngưỡng nhất định. Các thí nghiệm rộng rãi trên Spec-Bench chứng minh hiệu quả của Kangaroo. Dưới xác minh chuỗi đơn, Kangaroo đạt được tăng tốc lên đến 1.68× trên Spec-Bench, vượt trội hơn Medusa-1 với 88.7% ít tham số bổ sung hơn (67M so với 591M). Mã nguồn cho Kangaroo có sẵn tại https://github.com/Equationliu/Kangaroo.

## 1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLMs) đã không thể phủ nhận việc thể hiện hiệu suất đáng chú ý trên vô số nhiệm vụ ngôn ngữ tự nhiên. Tuy nhiên, bị ràng buộc bởi nút thắt cổ chai băng thông bộ nhớ, độ trễ chính cho giải mã tự hồi quy của LLMs xuất phát từ các thao tác đọc/ghi bộ nhớ của trọng số mô hình thay vì tính toán số học. Ví dụ, giải mã với Vicuna-33B trên bốn GPU NVIDIA V100 chỉ cho thông lượng bảy token mới mỗi giây. Để giải quyết thách thức này, các kỹ thuật Giải mã Đầu cơ (SD) đã được phát triển, nhằm tăng tốc giải mã tự hồi quy bằng cách xác minh nhiều token được tạo ra bởi một mô hình nháp song song. Với γ token nháp, SD có thể tạo ra 1 đến γ + 1 token mới trong mỗi lần truyền tiến của LLM lớn. Hiệu quả của SD dựa vào hai yếu tố chính: 1) khoảng cách giữa mô hình nháp và LLM đích. Các nhà nghiên cứu thường huấn luyện một mô hình nháp nhỏ từ đầu trên một kho dữ liệu lớn để tăng tốc các LLM lớn từ cùng series, ví dụ, LLaMA-68M cho LLaMA-7B. Tuy nhiên, việc huấn luyện các mô hình chuyên biệt cho nhiệm vụ như vậy có thể tốn kém, hạn chế ứng dụng trong các tình huống thực tế; 2) độ trễ suy luận của mô hình nháp. Nếu chi phí suy luận của mô hình nhỏ có thể bỏ qua so với LLM đích lớn, tỷ lệ tăng tốc end-to-end tỷ lệ thuận trực tiếp với tỷ lệ chấp nhận token nhất quán như được định nghĩa trong Eq (2).

Để giải quyết các vấn đề nêu trên, một số nghiên cứu đã đề xuất các phương pháp tự nháp không phụ thuộc vào các mô hình nháp bên ngoài. LLMA và REST tạo ra các token nháp bằng cách chọn các đoạn văn bản từ tham chiếu hoặc truy xuất các token liên quan từ cơ sở dữ liệu. Đáng chú ý, Medusa huấn luyện nhiều đầu FFN độc lập thời gian trên đỉnh của lớp decoder cuối cùng. Tuy nhiên, các phương pháp này vẫn đưa ra một số thách thức. Trong khi Medusa có thể tạo ra nhiều token nháp một cách hiệu quả tại các vị trí liền kề, tỷ lệ chấp nhận token của nó chưa thỏa mãn (xem Hình 1(a)). Thêm vào đó, tập trung độc quyền vào tỷ lệ chấp nhận token mà không xem xét độ trễ của việc tạo ra token nháp có thể dẫn đến tăng tốc end-to-end dưới tối ưu. Ví dụ, Lookahead đạt được tỷ lệ chấp nhận token tương đương với Kangaroo trong nhiệm vụ con lý luận toán học, vượt trội đáng kể so với Medusa. Tuy nhiên, do hiệu quả thấp hơn trong việc tạo ra token nháp so với Medusa, tỷ lệ tăng tốc end-to-end của nó thấp hơn một chút so với Medusa (xem Hình 1).

Để đối phó với những thách thức này, chúng tôi thiết kế một mô hình tự nháp tự hồi quy bằng cách huấn luyện một mô-đun adapter nhẹ và hiệu quả trên đỉnh của một mạng con nông cố định của LLM lớn gốc. Như được hiển thị trong Hình 2, kiến trúc mạng adapter chỉ bao gồm một multi-head attention và hai lớp chuẩn hóa. Đáng ngạc nhiên, chúng tôi thấy thiết kế đơn giản này hiệu quả nhưng mạnh mẽ, chỉ với 11.3% tham số của các đầu Medusa. Để giảm thêm độ trễ suy luận của mô hình tự nháp, chúng tôi giới thiệu một cơ chế thoát ra sớm bổ sung để tạo ra token nháp, nhằm tránh chi phí không cần thiết trên các token khó hơn.

Tóm lại, các đóng góp chính của chúng tôi là:
• Chúng tôi đề xuất một khung giải mã đầu cơ tự thân mới dựa trên cơ chế thoát ra sớm kép, được đặt tên là Kangaroo. Đầu tiên, mô hình nhỏ tự nháp tương đương thoát ra sớm từ các lớp nông cố định của LLM lớn và kết nối với một mạng adapter để tạo ra token nháp. Thứ hai, trong giai đoạn nháp, Kangaroo sử dụng thoát ra sớm tại các điểm phù hợp để tránh chi phí tính toán không cần thiết trên các token thách thức hơn.

• Kangaroo cung cấp một phương pháp chi phí thấp để huấn luyện một mô hình nhỏ nhẹ. Vì mô hình nháp đầu cơ tự thân và LLM lớn chia sẻ một số KV cache và tính toán, yêu cầu triển khai bổ sung duy nhất trong thực tế là một mạng adapter nhỏ.

• Các thí nghiệm trên Spec-Bench xác nhận hiệu quả của Kangaroo. Dưới xác minh chuỗi đơn, Kangaroo đạt được tăng tốc lên đến 1.7× trên Spec-Bench, vượt trội hơn Medusa-1 với 88.7% ít tham số bổ sung hơn, tức là 67M so với 591M.

Bài báo này được cấu trúc như sau: Phần 2 xem xét các công trình liên quan, và Phần 3 giới thiệu khung của chúng tôi, Kangaroo. Phần thí nghiệm, Phần 4, cung cấp phân tích và so sánh với các phương pháp tự nháp khác nhau, cùng với các nghiên cứu ablation để xác định các thành phần chính của Kangaroo. Kết luận được trình bày trong Phần 5.

## 2 Công trình liên quan

**Tăng tốc Suy luận của Mô hình Ngôn ngữ Lớn** Với sự phát triển nhanh chóng của các mô hình ngôn ngữ lớn, nỗ lực nghiên cứu đáng kể đã được dành cho việc tăng tốc độ suy luận của chúng. Các kỹ thuật như chưng cất kiến thức, nén mô hình và lượng tử hóa cũng đã được áp dụng rộng rãi trong lĩnh vực này. Tuy nhiên, các phương pháp này thường yêu cầu huấn luyện bổ sung backbone hoặc sửa đổi đáng kể kiến trúc mô hình. Các nỗ lực gần đây đã khám phá thoát ra sớm trên các mô hình như series T5 và kiến trúc chỉ decoder. Tuy nhiên, vì thoát ra sớm tăng tốc suy luận bằng cách tiết kiệm các tính toán tiếp theo, nó không thể tránh khỏi vấn đề suy giảm hiệu suất.

**Giải mã Đầu cơ** Giải mã Đầu cơ (SD) đã nhận được sự chú ý đáng kể do khả năng tăng tốc suy luận của LLMs trong khi duy trì cùng phân phối lấy mẫu. Nói chung, SD liên quan đến việc tìm hoặc huấn luyện một mô hình nháp nhỏ được căn chỉnh chặt chẽ với LLM đích. Do đó, nghiên cứu gần đây đã tập trung vào các phương pháp tự nháp thuận tiện hơn. Ví dụ, các phương pháp như giải mã song song theo khối và Medusa tăng tốc việc tạo ra token nháp bằng cách huấn luyện nhiều Mạng Neural Feedforward (FFNs) độc lập thời gian tại lớp thứ hai từ trên xuống. Một số kỹ thuật tăng tốc tự nháp được lấy cảm hứng từ thoát ra sớm. Draft & Verify, ví dụ, tạo ra token nháp bằng cách bỏ qua các lớp trung gian dư thừa của LLM đích. Trong khi phương pháp này có thể đạt được tỷ lệ chấp nhận token cao, độ trễ suy luận của "mô hình nhỏ" là cực kỳ cao, có thể cản trở hiệu quả tăng tốc end-to-end. SPEED điều chỉnh thoát ra sớm cho thực thi đầu cơ pipeline cho các decoder transformer sử dụng chia sẻ tham số. Đồng thời, chúng tôi đã biết rằng cũng có một số công trình cải thiện Medusa bằng cách giới thiệu phụ thuộc thời gian giữa các token nháp. Để tóm tắt chi tiết hơn, chúng tôi giới thiệu độc giả đến một khảo sát gần đây về giải mã đầu cơ.

## 3 Kangaroo

Trong phần này, đầu tiên chúng tôi đi sâu vào phân tích chuyên sâu về tỷ lệ chấp nhận token, tỷ lệ nén và tỷ lệ tăng tốc cho một số thuật toán tự nháp. Tiếp theo, chúng tôi giới thiệu khung của chúng tôi, Kangaroo, sử dụng giải mã đầu cơ tự thân bằng cách chia sẻ một mạng con nông cố định của LLM lớn. Để giảm thêm độ trễ suy luận của mô hình tự nháp, chúng tôi giới thiệu một cơ chế thoát ra sớm bổ sung khi tạo ra token nháp.

**Ký hiệu.** Chúng tôi sử dụng xt để biểu thị chuỗi token rời rạc (x1,···, xt) và xi:j để đại diện cho chuỗi (xi,···, xj). Cho V là không gian rời rạc trên tất cả các token có thể trong từ vựng của LLM, chúng tôi mô hình hóa quá trình tự hồi quy của một mô hình ngôn ngữ M bằng các phân phối có điều kiện M(· |xt)∈R|V| trong đó |V| là kích thước từ vựng. Chúng tôi sử dụng chỉ số dưới Mn(· |xt) để biểu thị mục thứ n của phân phối xác suất. Chúng tôi ký hiệu mô hình ngôn ngữ đích lớn và mô hình nhỏ đầu cơ lần lượt là Mb và Ms.

**Tỷ lệ Chấp nhận Token Giảm dần theo Hướng Đầu cơ** Giải mã đầu cơ thường được đánh giá bằng hai chỉ số chính: tỷ lệ tăng tốc walltime và tỷ lệ nén. Cho một thuật toán giải mã đầu cơ, chúng tôi thực thi nó để tạo ra N token mới và ghi lại các token được chấp nhận mỗi lần truyền tiến của mô hình lớn như một danh sách S = [s1, s2,···, s|S|] trong đó Σksk = N. Tỷ lệ nén (CR) được định nghĩa là
CR = 1/|S| Σksk. (1)

Lưu ý rằng trong quá trình xác minh lấy mẫu đầu cơ, một khi token nháp bị từ chối bởi mô hình lớn Mb, tất cả các token tiếp theo sẽ bị loại bỏ bất kể chất lượng của chúng. Tỷ lệ nén không phản ánh chính xác mức độ chấp nhận của thuật toán nháp cho các token ở các khoảng cách khác nhau. Do đó, chúng tôi đề xuất một chỉ số đánh giá mới gọi là tỷ lệ chấp nhận token nhất quán:

**Định nghĩa 1.** Tỷ lệ chấp nhận token nhất quán CTAR(w), cho một tiền tố và một cửa sổ tiếp theo với kích thước w, là xác suất mà w token đoán từ mô hình nháp Ms đều được chấp nhận bởi mô hình đích Mb.

Đối với cài đặt giải mã tham lam, CTAR(xt, w) là 0 nếu có ít nhất một dự đoán top-1 không nhất quán giữa Ms và Mb trong cửa sổ, ngược lại là 1. Tương tự như tỷ lệ nén, tỷ lệ chấp nhận token nhất quán có thể được tính như:
CTAR(w) = 1/|S| ΣkI(sk−w > 0), (2)
là một hàm giảm w.r.t. kích thước cửa sổ w. Chúng tôi vẽ CTARs thực nghiệm (cho w = 1,2,···,6) của một số thuật toán giải mã đầu cơ tự nháp trên nhiệm vụ con lý luận toán học của Spec-Bench trong Hình 1(a). Có thể thấy rằng ngoài tỷ lệ chấp nhận token, tốc độ tạo ra token nháp cũng có tác động đáng kể đến tỷ lệ tăng tốc end-to-end cuối cùng.

### 3.1 Thoát ra Sớm như Mô hình Tự Nháp

Huấn luyện một mô hình nhỏ bổ sung từ đầu thường tốn kém, do đó đáng xem xét việc chia sẻ một phần tham số với LLM đích. Lấy cảm hứng từ khái niệm thoát ra sớm, chúng tôi trực tiếp trích xuất các trạng thái ẩn từ một mạng con nông cố định của LLM đích và học một ánh xạ từ lớp nông đến lớp cuối cùng. Cụ thể, chúng tôi huấn luyện một adapter nhẹ và hiệu quả A để kết nối khoảng cách giữa mô hình tự nháp Ms = A ◦ Mb[:l] và mô hình đích Mb, trong đó lớp thoát ra sớm l ∈ {1,2,···, L} và A biểu thị mạng adapter. Như được hiển thị trong Hình 2, kiến trúc của adapter A chỉ bao gồm một multi-head attention và hai lớp chuẩn hóa.

**Hàm Mất mát Huấn luyện** Một phương pháp tầm thường để huấn luyện mạng adapter là tối đa hóa tỷ lệ chấp nhận token qua mỗi vị trí, trong khi chúng tôi thấy rằng hàm mất mát cross-entropy thể hiện tỷ lệ hội tụ nhanh hơn, tức là,
A* = arg minA ΣtΣn−Mbn(xt) log Msn(xt). (3)

### 3.2 Các Bước Nháp Động với Thoát ra Sớm

Giải mã đầu cơ thường sử dụng một bước nháp cố định trong giai đoạn nháp, nhưng điều này thường dẫn đến tối ưu cục bộ. Một mặt, độ khó của việc dự đoán token tiếp theo khác nhau qua các tình huống ngữ cảnh khác nhau. Do đó, rất có thể lãng phí thời gian trên các mẫu thách thức hơn hoặc bỏ lỡ cơ hội để suy đoán trên các token đơn giản hơn xa hơn. Mặt khác, suy luận của mô hình nhỏ được sử dụng trong phương pháp này vẫn phát sinh một chi phí nhất định, và việc chấm dứt kịp thời có thể tiết kiệm một lượng đáng kể độ trễ. Do đó, chúng tôi dừng nháp một khi độ tin cậy top-1 trên mô hình tự nháp thấp hơn một ngưỡng được định trước η, tức là,
max n Msn(x) ≤ η. (4)

## 4 Thí nghiệm

### 4.1 Chi tiết Triển khai

Chúng tôi tiến hành thí nghiệm trên các mô hình Vicuna với kích thước 7B và 13B. Chúng tôi chọn ba phương pháp giải mã đầu cơ tự nháp để so sánh, tức là Lookahead, Medusa và REST. Chúng tôi sử dụng chỉ số tỷ lệ nén và tỷ lệ tăng tốc walltime. Để so sánh công bằng, chúng tôi đánh giá hiệu suất của các phương pháp tự nháp được chọn với Spec-Bench được đề xuất gần đây. Tất cả các mô hình được đánh giá trên GPU NVIDIA V100. Đối với Kangaroo, chúng tôi huấn luyện mạng adapter trong 10 epoch với bộ tối ưu AdamW trên bộ dữ liệu ShareGPT theo Medusa.

### 4.2 Nghiên cứu Ablation

**Độ sâu của Mạng con Nông.** Khả năng của mô hình tự nháp Ms phụ thuộc cao vào độ sâu của mạng con nông được chia sẻ. Tuy nhiên, việc chọn các lớp thoát ra sớm sâu hơn, chẳng hạn như một nửa lớp của Mb, sẽ dẫn đến độ trễ suy luận quá cao. Do đó, lớp thoát ra sớm l kiểm soát sự cân bằng giữa tỷ lệ chấp nhận token và hiệu quả nháp. Như được hiển thị trong Hình 3(a), chúng tôi đặt ℓ = 2 cho Vicuna-7B và ℓ = 3 cho Vicuna-13B.

**Kiến trúc của Mô-đun Adapter.** Trong một khối transformer, thành phần FFN chiếm 67% tổng số tham số. Như được hiển thị trong Bảng 2, chúng tôi thấy rằng việc loại bỏ thành phần FFN và chia sẻ LM Head của LLM đích là cực kỳ hiệu quả.

**Thoát ra Động so với Nháp Bước Cố định.** Để xác nhận hiệu quả của các bước nháp động với ngưỡng cố định, chúng tôi vẽ so sánh cho các η khác nhau trong Hình 3(b). Chiến lược bước cố định (η = 0) đạt được tỷ lệ nén tối đa, tuy nhiên, dẫn đến tăng tốc walltime end-to-end dưới tối ưu. Nhìn chung, ngưỡng tối ưu η nhất quán qua các bước tối đa khác nhau. Đối với Kangaroo, chúng tôi đặt γ = 6 và η = 0.6.

## 5 Kết luận

Trong bài báo này, chúng tôi giới thiệu Kangaroo, một khung giải mã đầu cơ tự thân mới được thiết kế riêng để tăng tốc suy luận của các mô hình ngôn ngữ lớn. Kangaroo sử dụng một mạng con nông cố định để hình thành mô hình tự nháp, với các lớp còn lại phục vụ như mô hình đích lớn hơn. Để giảm độ trễ suy luận của mô hình tự nháp, chúng tôi giới thiệu một cơ chế thoát ra sớm bổ sung để tạo ra token nháp, nhằm tránh chi phí không cần thiết trên các token khó hơn. Dưới xác minh chuỗi đơn, Kangaroo đạt được tăng tốc lên đến 1.7× trên Spec-Bench, vượt trội hơn Medusa-1 với 88.7% ít tham số bổ sung hơn.
