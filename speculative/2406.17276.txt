# 2406.17276.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/speculative/2406.17276.pdf
# File size: 1741171 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure
Jikai Wang1*, Yi Su1*, Juntao Li1†,
Qingrong Xia2,Zi Ye2,Xinyu Duan2,Zhefeng Wang2,Min Zhang1,
1Institute of Computer Science and Technology, Soochow University, China
2Huawei Cloud
risus254@gmail.com, yisunlp@outlook.com
ljt@suda.edu.cn
Abstract
Autoregressive language models demonstrate
excellent performance in various scenarios.
However, the inference efficiency is limited by
its one-step-one-word generation mode, which
has become a pressing problem recently as the
models become increasingly larger. Specula-
tive decoding employs a "draft and then ver-
ify" mechanism to allow multiple tokens to be
generated in one step, realizing lossless accel-
eration. Existing methods mainly adopt fixed
heuristic draft structures, which fail to adapt
to different situations to maximize the accep-
tance length during verification. To alleviate
this dilemma, we proposed OPT-Tree, an algo-
rithm to construct adaptive and scalable draft
trees. It searches the optimal tree structure that
maximizes the mathematical expectation of the
acceptance length in each decoding step. Ex-
perimental results reveal that OPT-Tree outper-
forms the existing draft structures and achieves
a speed-up ratio of up to 3.2 compared with
autoregressive decoding. If the draft model
is powerful enough and the node budget is
sufficient, it can generate more than ten to-
kens in a single step. Our code is available
at https://github.com/Jikai0Wang/OPT-Tree.
1 Introduction
Large language models (LLMs) (Black et al., 2022;
Touvron et al., 2023; Achiam et al., 2023; Zheng
et al., 2024) have achieved remarkable performance
in various NLP scenarios. As models grow in size
and complexity, the computational demands for
inference increase significantly. Therefore, it is
becoming increasingly important to accelerate de-
coding to save computing overhead.
Autoregressive models (Black et al., 2022;
Zhang et al., 2022; Touvron et al., 2023) usually
generate one token in one decoding step, leading to
*Equal contribution.
†Corresponding author.limited decoding efficiency. In recent work, spec-
ulative decoding (Leviathan et al., 2023; He et al.,
2023; Fu et al., 2024; Cai et al., 2024; Li et al.,
2024) has shown great potential for lossless ac-
celerated decoding. It applies a "draft and then
verify" mechanism to maintain the original output
distribution of the target model to be accelerated.
Drafting is performed by a less-overhead drafting
model. The generated draft is verified in parallel by
the target model to generate multiple tokens in one
decoding step, bringing promising acceleration.
Existing work like EAGLE (Li et al., 2024) has
proposed methods for training small but effective
draft models. To the best of our knowledge, pre-
vious work mainly adopts drafts with structures
of Sequences or fixed trees. However, we argue
that neither of them is the optimal draft structure
under a limited node budget. Sequence-structured
drafts (Stern et al., 2018; Leviathan et al., 2023;
Xia et al., 2023; Yang et al., 2023; Zhang et al.,
2023; Fu et al., 2024) contain redundant nodes. For
example, "A-B-C-D-E" and "A-B-C-F-G" have the
same prefix "A-B-C", which is calculated twice
during verification. Therefore, there are only 7
valid tokens among the 10 nodes of these two se-
quences. Drafts with tree structure (He et al., 2023;
Cai et al., 2024; Li et al., 2024; Jeon et al., 2024;
Chen et al., 2024) solved this problem. The same
token can only appear once in the same tree layer.
A corresponding tree attention mask is designed
for parallel verification. The specific structure of
the tree is usually heuristic and remains constant.
However, given a node budget, the best structure
that maximizes the acceptance length during verifi-
cation would change according to different inputs
in each decoding step.
This paper proposes an adaptive and scalable
tree structure called OPT-Tree. It can be applied
to any autoregressive draft model. As is shown in
Figure 1, the tree structure adaptively changes in
each decoding step to maximize the mathematical
1arXiv:2406.17276v4  [cs.CL]  24 Apr 2025

--- PAGE 2 ---
Figure 1: Draft structures used in speculative decoding. Nodes in the same layer share the same position index.
OPT-Tree various in each decoding step to achieve a larger acceptance length.
expectation of the acceptance length. We apply
a greedy algorithm to construct an OPT-Tree in
each step. Details are elaborated in Section 3. We
conduct comprehensive experiments in Section 4
to evaluate the effectiveness of OPT-Tree. Experi-
mental results demonstrate that OPT-Tree outper-
forms the baselines and can be up to 3.2 times
faster than vanilla autoregressive decoding. The
mathematical expectation of the acceptance length
is generally positively correlated with the actual
acceptance length in practice. Moreover, OPT-Tree
performs well when the tree size scales up. Using
LLaMA-2-7B as the draft model, LLaMA-2-7B
can generate 10 tokens in a single decoding step
with OPT-Tree when the number of nodes is over
500, which indicates its great potential for adapting
to more powerful computation resources and more
effective draft models in the future.
2 Preliminaries
We provide the necessary definitions in this section.
Inference. After inputting x= (x1, x2, ..., x l),
where lis the current sequence length, the target
model Mand the drafting model Mdreturn the
next word distribution p(yl+1|x1, x2, ..., x l)and
pd(ˆyl+1|x1, x2, ..., x l)respectively, where yl+1
andˆyl+1are the sampled next words.
Speculative Decoding. In speculative decoding
with tree-structured draft, Mdfirst infers dsteps
to generate a draft tree Tof depth dand then M
verify the draft. The verification depends on the
sampling method. For greedy sampling, the ground
truth is the sequence of tokens with the highest
probability for each position output by M. For all
branches in the tree that contain the root node, the
longest branch with the same prefix as the ground
truth is accepted. Therefore, multiple tokens can
be generated in one decoding step while ensuringthat the generated sequences are consistent with
the original ones.
3 OPT-Tree
This section introduces OPT-Tree, an algorithm for
constructing our defined optimal draft tree structure
for any input sequence in speculative decoding with
autoregressive draft models.
Draft tree Tis defined as follows:
T= (V,E)
V=l+d[
i=l+1ni[
j=1
(ˆyi
j,ˆpi
j)	
,(1)
where VandEis the set of all nodes and edges. ni
represents the number of sampled tokens in the ith
layer of T.ˆpi
jis calculated by:
ˆpi
j=Y
ˆy∈P(ˆyi
j)pd(ˆy), (2)
where P(ˆyi
j)is the set of all parent nodes of ˆyi
j(in-
cluding itself). ˆpi
jof the root node is regarded as
positive infinity. For each node in T, if it has kchil-
dren, they are ktokens greedily sampled according
topdfrom its subsequent token distribution. The
purpose of calculating ˆpis to simplify subsequent
operations.
Theorem 3.1. For any two nodes viandvjin the
tree, if viis a node in the subtree of vj, then ˆpofvi
is less than ˆpofvj.
Considering a certain step in speculative decod-
ing whose input is x, the draft model Mdgenerates
a draft tree based on xand the given tree structure
T. Then, the target model inputs the draft tree and
the corresponding tree attention mask and returns
the next tokens of each token in T. We get the
2

--- PAGE 3 ---
Figure 2: An example of a draft tree containing ˆpin
each node. The value of E(A)is 2.07.
longest accepted candidate with length Aby com-
paring the next tokens and the draft tree. Given M,
Mdandn, for input x, an optimal tree structure
Toptshould maximize the mathematical expecta-
tion of the acceptance length E(A). Note that Topt
changes as the input changes. Since the optimiza-
tion goal of the draft model is to make its output
distribution close to the target model distribution,
for each node, ˆpwill be positively related to its
probability of being accepted during verification
when using an effective draft model for speculative
decoding. Therefore, E(A)can be approximately
calculated by ˆp:
E(A) =X
(ˆyi
j,ˆpi
j)∈TY
ˆy∈P(ˆyi
j)pd(ˆy)
=X
(ˆyi
j,ˆpi
j)∈Tˆpi
j.(3)
Figure 2 shows a simple example of calculating ˆp
andE(A).E(A)should positively correlate with
the acceptance length. We discuss their correlation
in Section 4.2.
We use Esub(T, n)to represent the maximum
value of E(A)for all subtrees of Tthat contain
the root node and have n nodes. Note that the root
node is not considered when calculating node trees
and mathematical expectations.
Then, we propose Algorithm 1 to construct Topt
during the drafting phase for each decoding step.
We initialize Twith a root node. At each drafting
step, we greedily sample n tokens with the largest
ˆpin the next token distributions of nodes in the last
layer of Tto construct the next layer. Thasd∗n
nodes at this time. Finally, we select the nnodes in
Twith the largest p. It is easy to prove that these n
nodes are a subtree of T, which contains the root
node:
Proof. (1) If these nodes can not form a tree with
the root, there is at least one node viwhose parentnode vjis not among these nodes. (2) Accord-
ing to Theorem 3.1, ˆpofvjis larger than ˆpofvi.
Therefore, vjis also selected. (1) and (2) are con-
tradictory, so these nodes must be able to form a
subtree of Tcontaining the root node.
Algorithm 1 Construct an OPT-Tree Topt
Input: Input sequence x= (x1, x2, ..., x l),
draft model Md, number of nodes n, threshold
δ.
Output: A draft tree Topt.
Initialize a tree Twith root node xl
E←0
Output distribution Pd(T)←Md(T)
T←topk(Pd(T), n)
while Depth of tree D(T)< nandEsub(T, n)−
E > δ do
//Drafting step
E←Esub(T, n)
Output distribution Pd(T)←Md(T)
T←topk(Pd(T), n)
end while
Topt←Select the nnodes with the largest ˆp
from T
Theorem 3.2. As the drafting step increases,
Esub(T, n)is monotonic non-decreasing.
Algorithm 2 Speculative Decoding with Adaptive
Draft Tree Structure
Input: Input sequence x= (x1, x2, ..., x l), tar-
get model M, draft model Md, number of nodes
n, threshold δ.
Output: New input sequence x′= (x1, x2, ...,
xl+A)
Topt←Construct the draft tree with nnodes
mask ←Compute the corresponding tree atten-
tion mask
P←M(Topt, mask )
(yl+1, yl+2, ..., yl+A)←V erify (Topt, P)
//Find the longest accepted candidate. If a
sequence of length A−1successfully hits, its
next word will also be accepted. So, the total
acceptance length is A.
x′←Concat (x,(yl+1, yl+2, ..., yl+A))
According to Theorem 3.2, we can get the de-
sired Toptin theory by stopping drafting when
E(T)no longer increases. However, the draft
model brings additional overhead to the practice.
3

--- PAGE 4 ---
M Md Tree MAL Tokens/s Speedup M Md Tree MAL Tokens/s Speedup
LLaMA-2
-7BNone - 1.00 51.89 1.00
LLaMA-2
-13BNone - 1.00 26.79 1.00
L-68MBinary 2.12 68.58 1.32
L-68MBinary 2.05 40.24 1.50
EAGLE 2.47 77.06 1.49 EAGLE 2.42 46.82 1.75
OPT-Tree 2.58 87.57 1.69 OPT-Tree 2.58 48.10 1.80
L-1BBinary 3.95 46.10 0.89
L-1BBinary 3.95 37.37 1.39
EAGLE 4.23 47.74 0.92 EAGLE 4.25 40.12 1.50
OPT-Tree 4.88 52.48 1.01 OPT-Tree 5.20 43.40 1.62
EAGLEBinary 3.40 107.91 2.08
EAGLEBinary 3.54 66.24 2.47
EAGLE 3.73 130.50 2.51 EAGLE 3.80 73.97 2.76
OPT-Tree 4.36 132.75 2.56 OPT-Tree 4.35 76.61 2.86
LLaMA-2
-70BNone - 1.00 6.29 1.00
Vicuna
-33BNone - 1.00 11.25 1.00
L-7BBinary 4.84 11.05 1.76
V-7BBinary 4.41 12.49 1.11
EAGLE 4.97 11.35 1.80 EAGLE 4.64 12.99 1.15
OPT-Tree 7.74 11.65 1.85 OPT-Tree 6.51 13.74 1.22
EAGLEBinary 3.39 17.02 2.71
EAGLEBinary 2.35 21.13 1.88
EAGLE 3.67 18.81 2.99 EAGLE 2.69 24.92 2.21
OPT-Tree 4.06 19.21 3.05 OPT-Tree 3.06 25.17 2.24
Table 1: Experimental results on MT-Bench. Mdbeing None represents vanilla autoregressive decoding. "L" and
"V" in Mdcolumn represent "LLaMA-2" and "Vicuna". "MAL" indicates "Mean Acceptance Length".
For autoregressive draft models, the drafting over-
head is proportional to the depth of the draft tree.
Taking this into consideration, we introduce a
threshold δwhen setting the conditions for termi-
nating drafting. The value of δshould be controlled
between µand 1, where µis the time of one draft-
ing step divided by the time of one decoding step.
A complete decoding step of Mis shown in
Algorithm 2. In practice, both MandMduse key
and value cache to calculate attention. Thus, the
actual input length of each drafting step is n, which
avoids computational bottlenecks in the inference
of draft model under larger budgets of tree size.
4 Experiments
4.1 Main Results
Setup. We adopt LLaMA-2-7B, LLaMA-2-13B,
LLaMA-2-70B (Touvron et al., 2023) and Vicuna-
33B (Zheng et al., 2024) as target models to verify
the effectiveness of OPT-Tree. We use a single
GeForce RTX 4090 GPU for LLaMA-2-7B, a sin-
gle L20 GPU for LLaMA-2-13B and 4 A100-PCIE-
40GB GPUs for LLaMA-2-70B and Vicuna-33B.
We choose one or two smaller models in the same
version as the draft model for each target model.
Moreover, we adopt a corresponding EAGLE draft
model for each target model. The temperature is
set to zero. EAGLE (Li et al., 2024) is an effective
speculation decoding method that trains additional
Figure 3: The relationship between input length and the
wall time for inference for models of different sizes on
various GPUs.
autoregressive heads as draft models. It uses a
well-designed heuristic draft tree structure with 25
nodes. In our experiments, we regard it as the EA-
GLE draft tree. EAGLE is certified by Xia et al.
(2024) as the fastest speculative method in their ex-
periments. For each target and draft model group,
we perform speculative decoding with greedy sam-
pling and compare OPT-Tree with the Binary tree
and EAGLE tree.
We compare the average acceptance length and
number of tokens generated per second decoding
with different tree structures. The speedup ratio is
calculated according to generation speed. The node
4

--- PAGE 5 ---
M Md Tree MAL Tokens/s Speedup M Md Tree MAL Tokens/s Speedup
LLaMA-2
-7BNone - 1.00 52.76 1.00
LLaMA-2
-13BNone - 1.00 27.10 1.00
L-68MBinary 2.20 73.49 1.39
L-68MBinary 2.21 45.18 1.67
EAGLE 2.63 85.62 1.62 EAGLE 2.60 52.83 1.95
OPT-Tree 2.78 96.43 1.83 OPT-Tree 2.81 53.54 1.98
L-1BBinary 3.55 40.69 0.77
L-1BBinary 3.76 36.54 1.35
EAGLE 3.87 44.42 0.84 EAGLE 4.10 37.29 1.38
OPT-Tree 4.46 50.83 0.96 OPT-Tree 5.10 42.97 1.59
EAGLEBinary 3.52 118.15 2.24
EAGLEBinary 3.80 73.30 2.70
EAGLE 3.83 137.41 2.60 EAGLE 4.06 80.47 2.97
OPT-Tree 4.68 140.55 2.66 OPT-Tree 5.03 80.94 2.99
LLaMA-2
-70BNone - 1.00 6.38 1.00
Vicuna
-33BNone - 1.00 10.74 1.00
L-7BBinary 4.85 11.20 1.76
V-7BBinary 4.95 13.15 1.22
EAGLE 4.98 11.51 1.80 EAGLE 4.81 13.38 1.25
OPT-Tree 7.62 12.10 1.90 OPT-Tree 6.35 13.98 1.30
EAGLEBinary 3.62 18.63 2.92
EAGLEBinary 2.82 25.20 2.35
EAGLE 3.91 20.42 3.20 EAGLE 3.15 28.37 2.64
OPT-Tree 4.55 20.50 3.21 OPT-Tree 3.47 28.76 2.68
Table 2: Experimental results on GSM8K. Mdbeing None represents vanilla autoregressive decoding. "L" and "V"
inMdcolumn represent "LLaMA-2" and "Vicuna". "MAL" indicates "Mean Acceptance Length".
budget is determined by the target model and com-
putational resource since the inference time gener-
ally remains the same within a certain input length.
Figure 3 displays the inference time for input with
various lengths for the 4 target models used in the
experiments. The number of nodes needs to be
controlled within a certain range to avoid excessive
time consumption in the verification phase. It is
treated as a hyperparameter chosen in [25,50,60]
to maximize the speedup ratio according to differ-
ent target models and GPU resources except for
the EAGLE tree. We conduct evaluation on MT-
Bench (Zheng et al., 2024) and GSM8K (Cobbe
et al., 2021).
Results. Experimental results are shown in Table 1
and Table 2. Note that using LLaMA-2-1B as the
draft model can hardly speed up decoding when
the target model is LLaMA-2-7B because the dif-
ference in inference time between the two models
is too small. EAGLE draft models achieve strong
performance with fewer parameters, thus provid-
ing better acceleration than the small models in
the same series with the target models. OPT-Tree
outperforms other tree structures in terms of mean
acceptance length in each group of experiments, es-
pecially when the performance of the draft model
is close to the target model (e.g., LLaMA-2-70B
combined with L-7B and Vicuna-33B combined
with Vicuna-7B), indicating its high upper limit.
Since OPT-Trees are usually deeper than binarytrees and EAGLE trees, they incur more overhead
when drafting. Therefore, from the perspective of
tokens per second, the improvement is not as sig-
nificant as that from the mean acceptance length.
Tokens per second are affected by different hard-
ware resources and random errors. In addition,
some method-independent techniques can also be
used to reduce computation time. For example, the
unchanged part of the attention mask in the draft-
ing phase can be initialized only once and called
multiple times, thus saving the time of multiple ini-
tializations. In order to make a fairer comparison
in our experiments, we avoid these tricks to be con-
sistent with EAGLE’s practice. Overall, OPT-Tree
outperforms the baselines. It can be up to about 3.2
times faster than vanilla autoregressive decoding.
The similar performance on both datasets verifies
the robustness of the proposed method.
4.2 Correlation between E(A)andA
The theory of OPT-Tree is based on the premise
thatE(A)is positively correlated with actual A.
We record the values of E(A)andAof OPT-Tree
in about 8000 decoding steps for 4 groups of Mand
Md. Figure 4 shows the results. The value of E(A)
is rounded. The darker areas in the four images
are basically distributed along the main diagonal
line. When E(A)of the tree is larger, it also tends
to get a more considerable acceptance length after
verification. A stronger draft model shifts the distri-
5

--- PAGE 6 ---
Figure 4: Correlation between E(A)andA. The hori-
zontal axis represents E(A), and the vertical axis rep-
resents A. Each square shows the number of times the
corresponding situation occurs. The darker the color,
the more times it indicates.
bution to the lower right corner. These phenomena
corroborate our theoretical analysis. In addition, in
the LLaMA-2-70B+LLaMA-2-7B group, high val-
ues of E(A)andA(e.g., E(A) = 14 , A= 15 ) are
generally found, which demonstrates the potential
of OPT-Tree to adapt to stronger draft models and
larger draft tree sizes.
4.3 Scaling the Draft Tree Size
We conduct experiments to explore the changes
in mean acceptance length with larger tree sizes.
We compare OPT-Tree with Sequoia (Chen et al.,
2024) using LLaMA-2-7B and LLaMA-2-70B as
target models. Sequoia is a scalable draft tree that
uses dynamic programming to solve for the tree
structure. It requires the target and draft models
to be used in advance to infer some samples to
determine the best structure. The tree structure is
fixed when doing speculative decoding. We use
200 samples in C4 (Raffel et al., 2020) to construct
the Sequoia trees. Temperature is set to 0 in the
experiments.
The experimental results are shown in Figure
5. OPT-Tree outperforms Sequoia under various
tree sizes. For LLaMA-2-7B+LLaMA-2-68M, the
mean acceptance length with both OPT-Tree and
Sequoia proliferates when the number of nodes is
smaller than 130. When the number of nodes ex-
ceeds 140, the mean acceptance length increases
slowly. For LLaMA-2-70B+LLaMA-2-7B, the
growth of mean acceptance length with Sequoia
Figure 5: Mean acceptance length under different tree
sizes under two sets of experiments.
Figure 6: The two figures on the left and right are the
mean acceptance length and tokens/s under different
thresholds on MT-Bench. The target model is LLaMA-
2-7B. The blue and orange dashed lines in the right
figure represent the values of µwith LLaMA-2-68M
and EAGLE as the draft model, respectively.
tends to be flat when the number of nodes exceeds
150. However, OPT-Tree can continue to improve
the mean acceptance length even if the number of
nodes exceeds 500. Since LLaMA-2-7B is a strong
draft model for LLaMA-2-70B, the mean accep-
tance length can achieve 10 with an OPT-Tree of
500 nodes. A tree with 500 nodes costs a large
amount of computation time for LLaMA-2-70B
with A100-PCIE-40GB GPUs, thus being unable
to speed up decoding in our practice. However, this
cost may be acceptable if more powerful computa-
tional resources are equipped in the future.
4.4 Impact of the Threshold
Considering the overhead of the draft model is pro-
portional to the depth of the tree, the tree that max-
imizes the acceptance length does not necessarily
have the highest speed-up ratio. Therefore, we ex-
6

--- PAGE 7 ---
M M d MAL Tokens/s Speedup
LLaMA-2-7BL-68M 2.72 88.90 1.71
L-1B 5.25 49.76 0.96
EAGLE 4.07 125.79 2.42
LLaMA-2-13BL-68M 2.26 43.45 1.62
L-1B 4.23 37.84 1.41
EAGLE 4.13 69.27 2.21
LLaMA-2-70BL-7B 7.17 11.87 1.89
EAGLE 4.09 18.92 3.01
Vicuna-33BV-7B 4.91 13.48 1.20
EAGLE 2.89 25.31 2.25
Table 3: Performance of OPT-Tree on MT-Bench with
the temperature set to 1. "L" and "V" in Mdcolumn
represents "LLaMA-2" and "Vicuna". "MAL" indicates
"Mean Acceptance Length".
Figure 7: The two figures on the left and right are the
mean acceptance length and tokens/s with OPT-Tree
with different temperatures on MT-Bench. The target
model is LLaMA-2-7B.
periment to study the mean acceptance length and
tokens/s under different thresholds.
Figure 6 shows the experimental results on
LLaMA-2-7B. The mean acceptance length drops
as the threshold grows when using LLaMA-2-68M
as the draft model. However, there is a slight fluctu-
ation for the EAGLE draft model. This is because
E(A)andAare not completely equivalent. We
calculate µfor each group of models, which is the
time of one drafting step divided by the time of one
decoding step. A threshold that is too large will
reduce the tree’s depth, thus reducing the value of
A. On the other hand, a threshold that is too small
may make the tree too deep and increase the cost
of drafting. When the depth of the tree increases by
one but the increment of the E(A)does not exceed
µ, it is not worth increasing the depth. So, we set a
threshold between µand 1 in practice. LLaMA-2-
68M and EAGLE achieve the highest acceleration
when δ= 0.2andδ= 0.8, respectively.4.5 Performance on Non-greedy Settings
In the decoding setting of non-greedy sampling
(random sampling), we only modify the acceptable
tokens during the verification phase. We conduct
experiments to evaluate OPT-Tree on these non-
greedy settings, where the temperature exceeds 0.
We perform speculative decoding with OPT-Tree
on the MT-Bench dataset for all groups of models
in 4.1 with the temperature set to 1. Table 3 dis-
plays the experimental results. The mean accep-
tance length and the speedup ratio of speculative
decoding with OPT-Tree are slightly lower when
the temperature is set to 1 than when the tempera-
ture is set to 0. Since the draft tree greedily samples
tokens with higher probability, the positive corre-
lation between E(A) and A will be weakened in
the decoding of random sampling. Therefore, it is
typical for the acceleration of speculative decoding
to drop when the temperature is greater than 0. Fig-
ure 7 shows specific changes in mean acceptance
length and tokens/s with different temperature val-
ues. Both metrics drop as the temperature rises in
general. But even when the temperature is set to 1,
opt-tree can still provide high speedup compared
to vanilla autoregressive decoding.
4.6 Case Study
We show an example of speculative decoding with
an OPT-Tree of 50 nodes on LLaMA-2-70B with
LLaMA-2-7B as the draft model in Figure 8. The
threshold is 0.7, and the temperature is 0. The
mean acceptance length is 9.34, and the generation
speed is 12.07 tokens per second. Most words
(blue text) are generated by the draft model and
then verified by the target model. Each couple of
red words and the continuous blue text in front
of it is generated in a single decoding step of the
target model. The appearance of red words is either
because the depth of the draft tree is limited or
because none of the candidates for this position
hits the target. Prepositions (e.g., in,forandwith),
conjunctions (e.g., andandor), articles (e.g., aand
the), punctuation and other words which have no
apparent practical meanings are prone to miss in the
draft. In addition, the beginning of new sentences
in drafts tends to be rejected because it has no solid
sequential association with the previous word.
5 Related Work
Speculative decoding (Stern et al., 2018; Xia et al.,
2023; Leviathan et al., 2023; Chen et al., 2023a) ac-
7

--- PAGE 8 ---
Figure 8: An example of speculative decoding with OPT-Tree on LLaMA-2-70B. Text on a blue background is the
input prompt. Blue text represents drafts generated by LLaMA-2-7B and accepted by LLaMA-2-70B. Red text
represents the next token for each accepted draft, which is generated by LLaMA-2-70B during the verification.
celerates autoregressive decoding by drafting and
then verifying while ensuring consistent output.
Drafting methods are mainly divided into indepen-
dent drafting and self-drafting. Independent draft-
ing leverages an external low-cost model. SpecDec
(Xia et al., 2023) trains a non-autoregressive model
for drafting while others (Leviathan et al., 2023;
Chen et al., 2023a; Spector and Re, 2023; Chen
et al., 2023b, 2024) directly utilize a smaller ver-
sion of the target model. In addition, REST (He
et al., 2023) proposed a retrieval-based drafting
method. Self-drafting uses the original information
of the target model to draft. Yang et al. (2023)
adopt an early-exiting mechanism for drafting.
Similarly, Zhang et al. (2023) performs adaptive
layer skipping in the drafting phase. Lookahead
Decoding (Fu et al., 2024) designed an algorithm
for parallel drafting and verification. MEDUSA
(Cai et al., 2024) trains multiple decoding heads
to obtain candidates for multiple steps from origi-
nal features in parallel. Considering that different
sampling results at each step in drafting will af-
fect the distribution of subsequent outputs, EAGLE
(Li et al., 2024) designed an autoregressive head,
which introduced the embedding of each word in
the drafting stage.
The verification method has evolved from
sequence-structured verification to tree-structured
verification. Early work (Stern et al., 2018;Leviathan et al., 2023; Xia et al., 2023; Yang et al.,
2023; Zhang et al., 2023; Fu et al., 2024) verifies
drafts in the form of one or several sequences. How-
ever, as the number of verification tokens increases,
there are a large number of prefix duplications be-
tween sequences, resulting in redundant calcula-
tions. To alleviate this problem, recent work (He
et al., 2023; Cai et al., 2024; Li et al., 2024; Jeon
et al., 2024) uses heuristic tree-structured drafts and
designs corresponding attention masks for parallel
verification. Chen et al. (2024) proposed Sequoia,
an algorithm for constructing draft trees, which
performs well as the tree size scales up.
6 Conclusion
In this paper, we propose a novel and effective
method called OPT-Tree to construct adaptive draft
tree structures for speculative decoding. OPT-Tree
maximizes the mathematical expectation of the ac-
ceptance length under any limited draft tree size.
Experimental results with ten groups of target mod-
els and draft models on two datasets show that
opt-tree outperforms existing draft structures. It
achieves a lossless acceleration of up to 3.2 times
compared to vanilla autoregressive decoding and
shows robustness on different datasets and with
different temperatures. Additionally, if equipped
with a strong draft model, the mean acceptance
length with OPT-Tree continues to grow even if
8

--- PAGE 9 ---
the number of nodes is over 500, demonstrating its
great potential for adapting to scenarios with more
powerful computational resources.
Limitations
Different hardware resources and environments
will affect the throughput speed reported in the
experiments in this article. The experiments in
this paper adopt the same decoding framework as
EAGLE (Li et al., 2024) for fair comparison. In
practice, the decoding algorithm can be optimized
from other perspectives to further improve the de-
coding speed, which is not explored in this paper.
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774 .
Sidney Black, Stella Biderman, Eric Hallahan, Quentin
Anthony, Leo Gao, Laurence Golding, Horace
He, Connor Leahy, Kyle McDonell, Jason Phang,
Michael Pieler, Usvsn Sai Prashanth, Shivanshu Puro-
hit, Laria Reynolds, Jonathan Tow, Ben Wang, and
Samuel Weinbach. 2022. GPT-NeoX-20B: An open-
source autoregressive language model. In Proceed-
ings of BigScience Episode #5 – Workshop on Chal-
lenges & Perspectives in Creating Large Language
Models , pages 95–136, virtual+Dublin. Association
for Computational Linguistics.
Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng,
Jason D Lee, Deming Chen, and Tri Dao. 2024.
Medusa: Simple llm inference acceleration frame-
work with multiple decoding heads. arXiv preprint
arXiv:2401.10774 .
Charlie Chen, Sebastian Borgeaud, Geoffrey Irving,
Jean-Baptiste Lespiau, Laurent Sifre, and John
Jumper. 2023a. Accelerating large language model
decoding with speculative sampling. arXiv preprint
arXiv:2302.01318 .
Zhuoming Chen, Avner May, Ruslan Svirschevski,
Yuhsun Huang, Max Ryabinin, Zhihao Jia, and
Beidi Chen. 2024. Sequoia: Scalable, robust, and
hardware-aware speculative decoding. arXiv preprint
arXiv:2402.12374 .
Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun,
Jie Huang, and Kevin Chen-Chuan Chang. 2023b.
Cascade speculative drafting for even faster llm infer-
ence. arXiv preprint arXiv:2312.11462 .
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.2021. Training verifiers to solve math word prob-
lems. arXiv preprint arXiv:2110.14168 .
Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang.
2024. Break the sequential dependency of llm in-
ference using lookahead decoding. arXiv preprint
arXiv:2402.02057 .
Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee,
and Di He. 2023. Rest: Retrieval-based speculative
decoding. arXiv preprint arXiv:2311.08252 .
Wonseok Jeon, Mukul Gagrani, Raghavv Goel, Juny-
oung Park, Mingu Lee, and Christopher Lott. 2024.
Recursive speculative decoding: Accelerating LLM
inference via sampling without replacement. In ICLR
2024 Workshop on Large Language Model (LLM)
Agents .
Yaniv Leviathan, Matan Kalman, and Yossi Matias.
2023. Fast inference from transformers via spec-
ulative decoding. In International Conference on
Machine Learning , pages 19274–19286. PMLR.
Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang
Zhang. 2024. Eagle: Speculative sampling re-
quires rethinking feature uncertainty. arXiv preprint
arXiv:2401.15077 .
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the lim-
its of transfer learning with a unified text-to-text
transformer. Journal of machine learning research ,
21(140):1–67.
Benjamin Spector and Chris Re. 2023. Accelerating llm
inference with staged speculative decoding. arXiv
preprint arXiv:2308.04623 .
Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit.
2018. Blockwise parallel decoding for deep autore-
gressive models. Advances in Neural Information
Processing Systems , 31.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu
Wei, and Zhifang Sui. 2023. Speculative decod-
ing: Exploiting speculative execution for accelerating
seq2seq generation. In Findings of the Association
for Computational Linguistics: EMNLP 2023 , pages
3909–3925.
Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang,
Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and
Zhifang Sui. 2024. Unlocking efficiency in large
language model inference: A comprehensive sur-
vey of speculative decoding. arXiv preprint
arXiv:2401.07851 .
9

--- PAGE 10 ---
Seongjun Yang, Gibbeum Lee, Jaewoong Cho, Dim-
itris Papailiopoulos, and Kangwook Lee. 2023. Pre-
dictive pipelined decoding: A compute-latency
trade-off for exact llm decoding. arXiv preprint
arXiv:2307.05908 .
Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen,
Gang Chen, and Sharad Mehrotra. 2023. Draft
& verify: Lossless large language model accelera-
tion via self-speculative decoding. arXiv preprint
arXiv:2309.08168 .
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.
Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068 .
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024.
Judging llm-as-a-judge with mt-bench and chatbot
arena. Advances in Neural Information Processing
Systems , 36.
10
