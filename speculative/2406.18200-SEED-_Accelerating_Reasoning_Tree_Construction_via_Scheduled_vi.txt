# 2406.18200.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/speculative/2406.18200.pdf
# Kích thước tệp: 7152011 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
SEED: Tăng tốc Xây dựng Cây Lý luận thông qua Giải mã Suy đoán có Lập lịch
Zhenglin Wang*, Jialong Wu∗, Yilong Lai, Congzhi Zhang, Deyu Zhou†
Trường Khoa học Máy tính và Kỹ thuật, Phòng thí nghiệm Trọng điểm về Mạng Máy tính
và Tích hợp Thông tin, Bộ Giáo dục, Đại học Đông Nam, Trung Quốc
{zhenglin, jialongwu, yilong.lai, zhangcongzhi, d.zhou}@seu.edu.cn
Tóm tắt
Các Mô hình Ngôn ngữ Lớn (LLMs) thể hiện
khả năng nổi bật đáng chú ý trên nhiều
nhiệm vụ khác nhau, nhưng vẫn còn thiếu sót
trong các nhiệm vụ lý luận và lập kế hoạch phức tạp.
Các phương pháp lý luận dựa trên tìm kiếm cây
giải quyết vấn đề này bằng cách khuyến khích
khám phá các bước trung gian, vượt trội hơn
khả năng của việc gợi ý chuỗi suy nghĩ.
Tuy nhiên, độ trễ suy luận đáng kể được
đưa vào do việc khám phá và đánh giá
có hệ thống nhiều đường dẫn suy nghĩ. Bài báo
này giới thiệu SEED, một khung suy luận
mới và hiệu quả để cải thiện đồng thời tốc độ
thời gian chạy và quản lý bộ nhớ GPU.
Dựa trên việc thực thi suy đoán có lập lịch,
SEED xử lý hiệu quả nhiều lần lặp
cho việc tạo suy nghĩ và đánh giá trạng thái,
tận dụng chiến lược lập lịch theo vòng
để quản lý việc điều phối mô hình nháp. Các đánh giá
thực nghiệm rộng rãi trên ba bộ dữ liệu lý luận
chứng minh hiệu suất tăng tốc vượt trội
của SEED1.
1 Giới thiệu
Mặc dù các Mô hình Ngôn ngữ Lớn (LLMs) đã
thể hiện khả năng nổi bật đáng chú ý trên nhiều
loại nhiệm vụ (Ouyang et al., 2022; OpenAI, 2022;
Touvron et al., 2023a,b; Achiam et al., 2023),
hiệu suất của chúng trên các nhiệm vụ lý luận và
lập kế hoạch phức tạp vẫn chưa tối ưu (Zhang et al.,
2024b). Các kỹ thuật gợi ý truyền thống hoặc đơn giản
(Wei et al., 2022; Kojima et al., 2022),
được sử dụng rộng rãi, không đủ
cho các nhiệm vụ đòi hỏi hành động khám phá hoặc
nhìn trước chiến lược (Liao et al., 2024).
Các phương pháp lý luận Dựa trên Tìm kiếm Cây
(TSB) hiệu quả khai thác khả năng lập kế hoạch và lý luận
của LLMs bằng cách phân tách các vấn đề
và sau đó điều phối một kế hoạch
có cấu trúc (Hui et al., 2024). Những phương pháp này không chỉ
tận dụng điểm mạnh vốn có của LLMs trong việc xử lý
các tập dữ liệu lớn mà còn giải quyết những hạn chế của chúng
trong các tình huống giải quyết vấn đề động (Hao et al.,
2023; Guan et al., 2023). Ví dụ, Yao et al.
(2024) đã giới thiệu việc gợi ý Cây-các-Suy nghĩ (ToT),
tổng quát hóa vượt ra ngoài việc gợi ý Chuỗi-các-Suy nghĩ
(CoT) bằng cách thúc đẩy việc khám phá
các suy nghĩ trung gian phục vụ như các bước quan trọng trong
việc giải quyết vấn đề tổng quát với LLMs. Theo
cách này, các công trình tiếp theo, như Lý luận qua
Lập kế hoạch (RAP) (Hao et al., 2023) và Phản ánh
trên Cây Tìm kiếm (RoT) được đề xuất (Hui et al.,
2024). Những cách tiếp cận này tận dụng khả năng
của LLMs để tạo ra và đánh giá các
suy nghĩ trung gian và sau đó tích hợp chúng với các thuật toán tìm kiếm
để cải thiện hiệu quả giải quyết vấn đề.
1arXiv:2406.18200v2 [cs.CL] 17 Dec 2024

--- TRANG 2 ---
Tuy nhiên, những phương pháp như vậy đưa ra một vấn đề nghiêm trọng
về độ trễ suy luận do yêu cầu khám phá
có hệ thống các suy nghĩ với việc nhìn trước và
quay lại. Các phương pháp lý luận TSB chủ yếu
bao gồm hai phần chính, xây dựng cây và
thuật toán tìm kiếm. Các nghiên cứu gần đây đã tăng cường
hiệu quả của các thuật toán tìm kiếm bằng cách kết hợp
phần thưởng đa dạng hoặc kỹ thuật cắt tỉa (Yan
et al., 2024; Hui et al., 2024). Theo hiểu biết của chúng tôi,
không có công trình nào trước đây khám phá việc tăng tốc
xây dựng cây, đây là trọng tâm của
bài báo này.
Việc thực thi Tuần tự truyền thống của LLMs cần
việc thực thi lặp đi lặp lại, dẫn đến thời gian thực thi
dài, như được thể hiện trong Hình 1 (a). Ví dụ,
khi áp dụng việc gợi ý ToT để thực thi một
mẫu duy nhất trong bộ dữ liệu GSM8K, tổng thời gian chạy
trung bình là khoảng 100 giây sử dụng xử lý tuần tự
với một mô hình 7B trên các GPU tiêu dùng.
Nếu việc thực thi của LLMs chuyển từ tuần tự
sang xử lý song song, nó có thể đặt ra thách thức
cho người dùng cuối hoặc các nhà nghiên cứu chỉ có các GPU tiêu dùng,
như được minh họa trong Hình 1 (d). Tình trạng như vậy
thường làm trầm trọng thêm các vấn đề liên quan đến
hạn chế phần cứng, đòi hỏi các chiến lược cho
quản lý tài nguyên và tối ưu hóa hiệu quả.
Giải mã suy đoán hiện được sử dụng rộng rãi để tăng tốc
suy luận (Xia et al., 2024), bao gồm việc
sử dụng một mô hình nháp nhỏ với một mô hình đích
lớn hơn, như được mô tả trong Hình 1 (b). Một cách trực quan, những
mô hình nháp này đạt được tốc độ suy luận nhanh nhờ
kích thước nhỏ của chúng. Nếu chúng được thực thi song song,
mối quan tâm về các ràng buộc bộ nhớ GPU
trở nên không đáng kể, cho phép hiệu suất tốc độ
tương đương với các tình huống được minh họa trong
Hình 1 (d). Hơn nữa, giải mã suy đoán sử dụng
mô hình hai giai đoạn nháp-rồi-xác minh, và
mô hình đích không được sử dụng đầy đủ khi tỷ lệ
chấp nhận của các token được soạn thảo tương đối cao.
Bằng cách tăng số lượng mô hình nháp, tiềm năng
của một mô hình đích duy nhất có thể được khai thác hiệu quả,
đảm bảo khả năng của nó được sử dụng tối ưu.

Do đó, chúng tôi đề xuất một khung suy luận
mới và hiệu quả, SEED, để giải quyết đồng thời tốc độ
thời gian chạy và quản lý tài nguyên bộ nhớ GPU
trong xây dựng cây lý luận. SEED
hiệu quả xử lý hai tình huống: (1) thực thi
nhiều lần lặp với cùng một gợi ý; (2) đánh giá
nhiều lần lặp với các gợi ý khác nhau.
Chúng tôi sử dụng giải mã suy đoán có lập lịch để quản lý
việc lập lịch của các mô hình nháp song song. Như
được mô tả trong Hình 1 (c), cho rằng chỉ có
một mô hình đích chung, không thể đồng thời
xác minh nhiều mô hình nháp, chúng tôi giải quyết
hạn chế này bằng cách lấy cảm hứng từ lập lịch
quy trình trong quản lý hệ điều hành (Zhao và Stankovic, 1989; Siahaan, 2016). Để đạt được điều này,
chiến lược Lập lịch theo Vòng sử dụng hàng đợi
Đến trước-Phục vụ trước (FCFS) được sử dụng
để kiểm soát và duy trì luồng thực thi tổng thể.
SEED đạt được hiệu suất tốc độ xuất sắc trên
ba bộ dữ liệu lý luận và lập kế hoạch: GSM8K,
Viết Sáng tạo và Blocksworld. Nó cũng cung cấp
một con đường khả thi để tiến hành suy luận theo lô
trong giải mã suy đoán không cần huấn luyện trong khi bảo tồn
phân phối ban đầu, đảm bảo kết quả không mất mát.

Đóng góp của chúng tôi có thể được tóm tắt như sau:
• Một khung suy luận hiệu quả, SEED, được
đề xuất để tăng tốc cả Bộ tạo Suy nghĩ
và Bộ đánh giá Trạng thái trong xây dựng cây lý luận.
• Thực thi Suy đoán có Lập lịch tích hợp
việc soạn thảo song song với giải mã suy đoán
được đề xuất, sử dụng chiến lược Lập lịch theo Vòng
hiệu quả để quản lý việc soạn thảo song song
không có xung đột xác minh.
• Về mặt thực nghiệm, các nghiên cứu thí nghiệm và phân tích
rộng rãi được thực hiện để chứng minh
hiệu quả của SEED. SEED đạt được
tăng tốc 1.1−1.5×, tạo ra tới 20 token
bổ sung mỗi giây trên ba bộ dữ liệu
lý luận.

2 Công trình Liên quan
2.1 Lý luận Dựa trên Tìm kiếm Cây
Gần đây, các phương pháp lý luận TSB đã được
tận dụng rộng rãi để tăng cường khả năng lý luận
của LLMs như RAP (Hao et al., 2023),
ToT (Yao et al., 2024), RoT (Hui et al., 2024).
Những phương pháp này tạo ra một cây lý luận cho phép
xem xét nhiều đường dẫn lý luận và tự đánh giá
các lựa chọn để xác định hướng hành động tiếp theo. Tại
mỗi bước lý luận, các thuật toán tìm kiếm cây
phổ biến như Tìm kiếm Ưu tiên Chiều rộng (BFS) (Bundy
và Wallen, 1984) và Tìm kiếm Cây Monte-Carlo
(MCTS) (Kocsis và Szepesvári, 2006) được
tích hợp để khám phá cây tìm kiếm một trạng thái
tối ưu. Ngoài ra, việc xây dựng hoặc tìm kiếm cây
đòi hỏi nhiều lần lặp hơn các phương pháp
lấy mẫu đơn (ví dụ, gợi ý Đầu vào-đầu ra và CoT (Wei
et al., 2022)), dẫn đến độ trễ suy luận cao hơn.

--- TRANG 3 ---
Để giải quyết vấn đề này, một số nghiên cứu giới thiệu phần thưởng
đa dạng (Yan et al., 2024) hoặc kỹ thuật cắt tỉa (Hui
et al., 2024) để giảm thiểu việc tìm kiếm không hiệu quả trong
các lần lặp, cải thiện hiệu quả tìm kiếm. Tuy nhiên,
những phương pháp này vẫn bỏ qua độ trễ suy luận
gây ra bởi quá trình lặp của việc xây dựng cây.
Thay vào đó, chúng tôi tập trung vào xây dựng cây, tận dụng
giải mã suy đoán có lập lịch để tăng tốc
quá trình và giảm độ trễ suy luận.

2.2 Giải mã Song song
Độ trễ suy luận của LLMs đã nổi lên như một
rào cản đáng kể, hạn chế khả năng lý luận
đáng chú ý của chúng trong các nhiệm vụ hạ lưu (Xia
et al., 2024). Một yếu tố chính góp phần vào
độ trễ suy luận cao là chiến lược giải mã
tuần tự cho việc tạo token được áp dụng bởi
hầu hết tất cả LLMs (Lu et al., 2024b). Có
nhiều nghiên cứu đã khám phá thách thức này
thông qua các chiến lược giải mã song song, như Giải mã
Suy đoán (SD) (Zhou et al., 2023; Cai et al.,
2024), Thoát sớm (EE) (Del Corro et al., 2023;
Elhoushi et al., 2024), và Không-Tự-Hồi quy
(NAR) (Ghazvininejad et al., 2019; Lu et al.,
2024a). Trong bài báo này, chúng tôi tập trung vào nghiên cứu
Giải mã Suy đoán. Trong SD, một hướng công việc
thuộc về danh mục không cần huấn luyện (Sun et al.,
2024b; Liu et al., 2023). Cách tiếp cận cắm-và-chạy
này tích hợp liền mạch với các phương pháp suy luận
mô-đun khác (ví dụ, CoT, TSB), cho phép
tăng tốc suy luận trực tiếp đáng kể và giảm
độ trễ suy luận trên các mô hình mã nguồn mở. Theo như
chúng tôi biết, chúng tôi là những người đầu tiên khám phá việc thực thi
SD có lập lịch để tích hợp với khung TSB,
mà không sửa đổi kiến trúc LLM hay đòi hỏi
huấn luyện bổ sung và duy trì đầu ra không mất mát.

3 Kiến thức Cơ bản
3.1 Giải mã Suy đoán
Kỹ thuật cốt lõi của giải mã suy đoán bao gồm
việc sử dụng một mô hình nháp nhỏ để tạo token
tuần tự, với một mô hình đích lớn hơn xác thực
những token này (Leviathan et al., 2023). Cụ thể,
cho c là các token đầu vào, Md và Mt là
mô hình nháp và mô hình đích tương ứng, và
k là số lượng token nháp được tạo ra mỗi
bước. Giải mã suy đoán là một mô hình giải mã
hai giai đoạn Nháp-rồi-Xác minh.2 Trong giai đoạn
nháp, Md lấy mẫu một chuỗi nháp các token
tự hồi quy, được ký hiệu là ˆx1, . . . , ˆxk, trong đó
ˆxi∼pd(x|ˆx1, . . . , ˆxi−1, c) cho i = 1, . . . , k. Trong
giai đoạn xác minh, chuỗi nháp các token
cùng với c, được chuyển đến Mt để có được
phân phối đầu ra của chúng pt(x|ˆx1, . . . , ˆxi−1, c) song song,
và sau đó được xác minh từ ˆx1 đến ˆxk. Token
nháp ˆxi được chấp nhận với xác suất
min(1, pt(x|ˆx1,...,ˆxi−1,c)/pd(x|ˆx1,...,ˆxi−1,c)). Một khi một token bị từ chối,
việc xác minh kết thúc và một giai đoạn lấy mẫu lại theo sau
để trả về một token mới bởi Mt. Token mới này
sau đó được sử dụng như điểm kết thúc được tạo theo sau
các token được chấp nhận. Như đã được chứng minh trong Leviathan
et al. (2023), phương pháp này tương đương với việc lấy mẫu
trực tiếp từ LLM đích. SEED áp dụng
phương pháp này, đảm bảo rằng phân phối của văn bản
được tạo ra không thay đổi cho cả cài đặt tham lam
và không tham lam.

2Trong bài báo sau, chúng tôi định nghĩa "Xác minh" là
"Xác minh" được đề cập ở đây, bao gồm cả giai đoạn xác minh và
lấy mẫu lại.

3.2 Chú ý Cây
Các nghiên cứu giải mã suy đoán hiện tại đã chứng minh
rằng khi mô hình nháp lấy mẫu nhiều ứng viên
mỗi vị trí trong chuỗi nháp, độ dài chấp nhận
kỳ vọng mỗi bước có thể được tăng cường
trong giai đoạn xác minh (Chen et al.,
2023a). Ngoài ra, kỹ thuật chú ý cây
cho phép nhiều chuỗi nháp ứng viên chia sẻ
bộ nhớ đệm của các token được tạo ra, cải thiện thêm
hiệu quả của giai đoạn xác minh (Cai et al.,
2024). Bằng cách sử dụng chú ý cây, việc chấp nhận xác minh
của giải mã suy đoán được tăng lên. Chúng tôi minh họa
chiến lược mặt nạ chú ý cây chi tiết trong Phụ lục B.
SEED được đề xuất của chúng tôi có thể tận dụng
cách tiếp cận này để đạt được tăng tốc thêm.

3.3 Công thức Nhiệm vụ TSB
Cho một câu hỏi đầu vào ban đầu I, một cây lý luận được
xây dựng với thuật toán tìm kiếm tương đối phổ biến BFS
theo Yao et al. (2024), như được thể hiện trong
Hình 2. Trong cây lý luận được xây dựng, mỗi
nút đại diện cho một trạng thái riêng biệt Si, bao gồm
một giải pháp từng phần với đầu vào c và các
đề xuất suy nghĩ được phát triển dần dần z1,···, zn.
Trong quá trình mở rộng của mỗi nút, Bộ tạo
Suy nghĩ G(·) tạo ra nhiều đường dẫn lý luận
để phân tách quá trình trung gian từ trạng thái
hiện tại. Một khi những suy nghĩ này được tạo ra,
Bộ đánh giá Trạng thái E(·) đánh giá đóng góp
của mỗi đường dẫn hướng tới việc giải quyết vấn đề, phục vụ như
một phương pháp heuristic để hướng dẫn thuật toán tìm kiếm. Đánh giá
này hỗ trợ trong việc xác định những trạng thái nào cần tiếp tục
khám phá và thiết lập thứ tự khám phá.

--- TRANG 4 ---
Đầu vào Ban đầu
*n
n
Bộ tạo Suy nghĩ
Bộ đánh giá Trạng thái S0
S1
S2 Hình 2: Hai thành phần chính trong xây dựng cây lý luận,
đó là Bộ tạo Suy nghĩ và Bộ đánh giá Trạng thái,
tương ứng.

ploration. Lấy nút gốc S0 làm ví dụ trong
Hình 2, trước tiên nó tạo ra n đường dẫn lý luận dựa trên
cùng một đầu vào c, đó là gợi ý ban đầu I
và sau đó chọn đường dẫn giữa bởi
Bộ đánh giá Trạng thái cho những n đường dẫn này.

4 Phương pháp
SEED được đề xuất của chúng tôi là một khung suy luận hiệu quả
được thiết kế để tăng tốc việc xây dựng
cây lý luận. Các việc thực thi tạo ra khác nhau trong
Bộ tạo Suy nghĩ hoặc Bộ đánh giá Trạng thái được
tiến hành trong các nhánh riêng biệt, đảm bảo rằng chúng
không can thiệp lẫn nhau. Do đó, Thực thi
Suy đoán có Lập lịch được triển khai
trong cả Bộ tạo Suy nghĩ và Bộ đánh giá Trạng thái,
cho phép xử lý song song để tăng tốc
toàn bộ việc xây dựng cây lý luận, như được chi tiết trong
Thuật toán 2.

Chúng tôi trước tiên giới thiệu hai giai đoạn trong Thực thi
Suy đoán có Lập lịch trong §4.1. Tiếp theo, chúng tôi
mô tả Chiến lược Lập lịch theo Vòng được thiết kế để
quản lý hiệu quả việc soạn thảo song song mà không có xung đột
trong §4.2. Nguyên lý kỹ thuật của SEED được lấy cảm hứng
từ lập lịch hệ điều hành. Sự tương tự chi tiết
giữa lập lịch hệ điều hành với SEED
được trình bày trong §4.3. Cuối cùng, thuật toán
kết hợp được elaborated trong §4.4.

4.1 Thực thi Suy đoán có Lập lịch
Chúng tôi chi tiết thêm thuật toán thực thi suy đoán có lập lịch
trong SEED. Để tăng cường độ rõ ràng,
chúng tôi chia thuật toán thành hai giai đoạn: giai đoạn
soạn thảo song song và giai đoạn xác minh tuần tự.

Giai đoạn Soạn thảo Song song Kích thước mô hình ảnh hưởng
đáng kể đến việc sử dụng bộ nhớ và thời gian suy luận.
Với điều này, cho rằng kích thước nhỏ và tốc độ suy luận
nhanh của các mô hình nháp, chúng ta có thể trực tiếp
khởi tạo nhiều mô hình nháp tương ứng với số lượng suy nghĩ, cho phép các quá trình song song.
Cụ thể, nếu số lượng suy nghĩ Nt được đặt
thành n, các mô hình nháp Md1, Md2,···, Mdn lấy
c1, c2,···, cn làm token đầu vào tương ứng trong
giai đoạn soạn thảo. Lưu ý rằng, trong Tạo
Suy nghĩ, các hướng dẫn đầu vào giống nhau, tức là
c1=c2=···=cn; trong Đánh giá Trạng thái,
chúng có thể khác nhau, được ký hiệu là c1̸=c2̸=··· ̸=cn.
Như được minh họa trong Hình 3 (a), ba mô hình nháp
bắt đầu lấy mẫu đồng thời khi hàng đợi
Q ban đầu trống. Trong giai đoạn tiếp theo, các
mô hình nháp vào hàng đợi theo thứ tự
hoàn thành việc tạo ra trước. Trong Hình 3 (a), Mô hình Nháp

trước tiên hoàn thành quá trình soạn thảo và
là mô hình đầu tiên vào hàng đợi Q, theo sau bởi Mô hình Nháp
 và Mô hình Nháp
. Mỗi mô hình nháp
đang tạo ra các token của riêng mình trong khi mô hình đích
Mt đang xác minh các token của các mô hình nháp khác.
Bằng cách này, chúng ta có thể tận dụng tiềm năng của các
mô hình nháp nhỏ để hoàn thành quá trình soạn thảo của chúng
đồng thời, trong khi mô hình đích lớn hơn chỉ cần
xác minh chúng tuần tự.

Giai đoạn Xác minh Tuần tự Chỉ có một mô hình đích
duy nhất được sử dụng cho việc xác minh tuần tự
nhiều chuỗi nháp trong SEED. Mô hình đích trước tiên xác minh
các token được tạo ra bởi mô hình nháp ở đầu hàng đợi.
Trong giai đoạn xác minh, hai tình huống có thể xảy ra:
chấp nhận và từ chối. Nếu các token được tạo ra
bởi mô hình nháp được chấp nhận bởi mô hình đích, chúng
được giữ lại, như được minh họa bởi Mô hình Nháp
trong Hình 3 (a). Nếu bị từ chối, một token mới được
lấy mẫu lại bởi mô hình đích, như được chứng minh
bởi Mô hình Nháp
 và Mô hình Nháp
. Lấy
Mô hình Nháp
 làm ví dụ, nó soạn thảo hai
token, "many" và "duch", bị từ chối bởi
mô hình đích. Mô hình Đích
 sau đó lấy mẫu lại
một token mới "much". Hơn nữa, khi được chấp nhận,
mô hình đích chỉ yêu cầu thời gian thực thi
, khi bị từ chối, nó phát sinh thời gian bổ sung để lấy mẫu lại
.

4.2 Chiến lược Lập lịch theo Vòng
Với việc tích hợp soạn thảo song song và xác minh
tuần tự, điều quan trọng là tối ưu hóa
lập lịch để đảm bảo tính đúng đắn của việc thực thi
suy đoán trong khi sử dụng hiệu quả mô hình đích
và giảm độ trễ thực thi tổng thể.
Lấy cảm hứng từ lập lịch quy trình trong quản lý hệ điều hành,
sử dụng chính sách lập lịch Đến trước-Phục vụ trước
(FCFS) cho tất cả các yêu cầu, đảm bảo công bằng và ngăn chặn
sự đói (Zhao và Stankovic, 1989; Siahaan, 2016),
chúng tôi tận dụng Chiến lược Lập lịch theo Vòng
tích hợp với chính sách lập lịch FCFS để quản lý
quá trình xác minh một cách hiệu quả. Khi một mô hình nháp
hoàn thành giai đoạn soạn thảo và sẵn sàng để
xác minh, các chuỗi nháp cùng với c được
đặt vào một hàng đợi.

Như được mô tả trong Hình 3 (a), khi hàng đợi Q không
trống, một chuỗi token nháp được dequeue
theo cách FCFS. Mô hình Đích
 trước tiên xác minh
các token được tạo ra bởi Mô hình Nháp
, theo sau
tuần tự bởi các token được tạo ra bởi Mô hình Nháp
và Mô hình Nháp
, tuân thủ FCFS. Khi
hoàn thành việc xác minh một chuỗi nháp
liên kết với một mô hình nháp, mô hình nháp tiến hành
quá trình soạn thảo trong lần lặp tiếp theo.
Sơ đồ lập lịch tổng thể được thể hiện trong Hình 3 (b),
mỗi mô hình nháp hiển thị một loạt các lần lặp
để hoàn thành tiến trình soạn thảo tổng thể cho
Bộ tạo Suy nghĩ hoặc Bộ đánh giá Trạng thái. Mô hình đích
luôn hoạt động trên toàn bộ
dòng thời gian lập lịch. Hoạt động liên tục này
đảm bảo rằng mô hình đích được sử dụng hiệu quả,
giải quyết các vấn đề liên quan đến thời gian nhàn rỗi khi tỷ lệ
chấp nhận tương đối cao. Một khi tất cả các quá trình soạn thảo
và xác minh được hoàn thành, toàn bộ
việc thực thi kết thúc, dẫn đến việc tạo ra
n chuỗi.

4.3 Nguyên lý Kỹ thuật
Các nghiên cứu trước đây đã thích ứng nguyên lý của
bộ lập lịch hệ điều hành (OS) để quản lý quy trình hiệu quả

--- TRANG 5 ---
v1 v1
c3 + How many duck
muchc1 + How man y eggsc1 + How man y eggsDraft Model 1 Draft Model 2 Draft Model 3c1
Queue
（FCFS）
Target Model Target Model Target Model
c2
c2 + What is total c3 + How man y duckc3
c1 + How man y eggs c2 + What is total c3 + How man y duck
c2 + What is  total
the
Draft
Model 2Draft
Model 3
Target
ModelDraft
Model 1
it0it0
v1it1 it2it0
it1
v3it1
v2 v1it2
v3it2
it3
v2 v2 v3it3
v3v2it3it4
v3
Time
(a) (b) 
Hình 3: (a) Tình huống mà mô hình đích quản lý việc xác minh các mô hình đích ở đầu; (b)
Sơ đồ lập lịch tổng thể cho một mô hình đích và ba mô hình nháp.
 ,
 ,
 đại diện cho Mô hình Nháp 1,
Mô hình Nháp 2, Mô hình Nháp 3, tương ứng.
 ,
,
biểu thị thời gian thực thi soạn thảo cho mỗi
mô hình nháp tương ứng.
 đề cập đến Mô hình Đích.
 đại diện cho thời gian thực thi của giai đoạn xác minh, trong khi
 chỉ định
thời gian lấy mẫu lại trong trường hợp từ chối.

cess management (Kwon et al., 2023). Như được thể hiện trong
Hình 4, mỗi thành phần trong SEED có thể được ánh xạ
đến một thành phần tương ứng trong bộ lập lịch
hệ điều hành. Chúng tôi elaborate từng thành phần
riêng lẻ như dưới đây:

• Việc thực thi lập lịch theo vòng trong SEED tương ứng
với lập lịch quy trình trong OS.
Cả hai đều sử dụng hàng đợi FCFS để kiểm soát và duy trì
luồng thực thi tổng thể. Một sự khác biệt chính
tồn tại: trong SEED, sau khi các token soạn thảo
được xử lý bởi giai đoạn xác minh,
mô hình nháp được trả về hàng đợi, tức là
"vòng". Ngược lại, trong lập lịch OS, một
quy trình đã được xử lý bởi CPU được
đánh dấu là hoàn thành.

• Việc xác minh các token nháp ˆX phản ánh một
quy trình hoạt động trong lập lịch OS.

• Mô hình đích phục vụ Mt tương tự như
CPU.

• Tổng thời gian xác minh của Mt giống như
thời gian CPU trong lập lịch quy trình OS.

OS
Mt
Rounds-Scheduled
 ExecutionProcess Scheduling
 CPU
Target 
ModelProcesses
DraftsCPU Time
Verification
TimeSEEDHình 4: Sự tương tự giữa bộ lập lịch Hệ điều hành
với SEED được đề xuất của chúng tôi.

4.4 Thuật toán
Các cơ chế tăng tốc cốt lõi của SEED, kết hợp
thực thi suy đoán có lập lịch với chiến lược
lập lịch theo vòng, được trình bày trong Thuật toán 1.

Về bản chất, việc soạn thảo song song được thực hiện
bởi nhiều quy trình song song D(n), trong khi việc xác minh
tuần tự được thực hiện bởi một quy trình xác minh
V xoay vòng xác minh từ hàng đợi xác minh
Q. Quy trình xác minh có hai giai đoạn,
đó là giai đoạn xác minh E và giai đoạn
lấy mẫu lại R. Để duy trì tính bất đồng bộ của
vòng lặp sự kiện nháp-rồi-xác minh, tận dụng bản đồ
nhãn nháp γ đảm bảo mỗi quy trình nháp chờ đợi
xác minh trước khi tiến hành với các nháp mới. Tại
giai đoạn ban đầu, mỗi phần tử trong bản đồ nhãn nháp
γ được đặt thành 1, cho thấy tất cả các mô hình nháp có thể
thực hiện soạn thảo. Sau khi hoàn thành việc xác minh
một mô hình nháp, nhãn tương ứng trong γ thay đổi
thành 0, chờ đợi để soạn thảo lại. Đáng chú ý, D(n) và
V được đồng bộ hóa. Điều kiện kết thúc cho
cả quy trình D(n) và quy trình V là tất cả token
được xác minh hiện tại Li, i∈[1, n] bằng độ dài mới
tối đa l. Khi tất cả các quy trình được hoàn thành, chúng ta
có thể có được một danh sách chứa n phản hồi.

--- TRANG 6 ---
Thuật toán 1 Thực thi Suy đoán có Lập lịch
với Chiến lược Lập lịch theo Vòng
1:Input: Mô hình nháp {Md1,···, Mdn}, tiền tố
{c1,···, cn}, mô hình đích Mt, độ dài mới tối đa l, độ dài nháp
k, soạn thảo tự hồi quy pdi và độ dài token
được xác minh hiện tại Li của mô hình nháp thứ i Mdi,i∈[1, n];
2:Initialize: Prefill{Md1,···, Mdn} với tiền tố; Tạo
hàng đợi xác minh Q và bản đồ nhãn nháp γ[i] có độ dài
n, với mỗi phần tử được đặt thành 1, i∈[1, n];Li←1,
i∈[1, n]; Định nghĩa ˆXi[1 :k] đại diện cho ˆx1, . . . , ˆxk
chuỗi các token nháp được tạo ra từ pdi,i∈[1, n];
Bắt đầu n quy trình nháp D(n) và 1 quy trình xác minh V
Đồng bộ;
3:Processes D(n): ▷Soạn thảo Song song
4:while∃i∈[1, n] :Li< ldo
5: ifγ(i)then
6: ˆXi[1 :k]←pdi(Mdi, ci,ˆXi[1 :Li], k)
7: Q←ˆXi[1 :k]▷Thêm token nháp vào hàng đợi
8: γ[i]←0 ▷Quy trình Nháp D(i) chờ
9: end if
10:end while
11:Process V: ▷Xác minh Tuần tự
12:while∃i∈[1, n] :Li< ldo
13: ifQ không trống then
14: ˆXi[1 :k]←queue (Q) ▷FCFS
15: t1,···, tk← E(Mt, ci,ˆXi[1 :k])
16: forj= 1tokdo
17: iftj là chấp nhận then
18: ˆXi[Li+ 1]←ˆxj
19: Li← L i+ 1
20: else
21: ˆX[Li+ 1]← R (Mt, ci,ˆXi[1 :Li])
22: Li← L i+ 1
23: Break
24: end if
25: end for
26: γ[i]←1 ▷Quy trình Nháp D(i) tiếp tục
27: end if
28:end while
29: Chờ tất cả D(n) và V hoàn thành
30:return [phản hồi 1, . . . , phản hồi n]

5 Thí nghiệm
5.1 Bộ dữ liệu
Ba bộ dữ liệu lý luận và lập kế hoạch được sử dụng rộng rãi
được chọn cho các thí nghiệm của chúng tôi. Để đánh giá
hiệu quả của các nhiệm vụ sáng tạo và lập kế hoạch, chúng tôi
tận dụng bộ dữ liệu Viết Sáng tạo (CW) (Yao
et al., 2024), trong đó đầu vào là bốn câu ngẫu nhiên
và đầu ra nên là một đoạn văn mạch lạc
với bốn đoạn kết thúc trong bốn câu đầu vào
tương ứng, với độ sâu cây ToT T là
2. Đối với lý luận toán học, GSM8K (Cobbe
et al., 2021) là một bộ dữ liệu bao gồm các bài toán
từ toán học cấp tiểu học chất lượng cao đòi hỏi
lý luận đa bước, với độ sâu cây T là 4.
Nhiệm vụ này mở rộng và khám phá, đặt ra
những thách thức đáng kể cho tư duy sáng tạo và
lập kế hoạch cấp cao. Để chứng minh tốt hơn hiệu suất
tăng tốc trong việc giải quyết các vấn đề lập kế hoạch
phức tạp hơn, chúng tôi chọn bộ dữ liệu
Blocksworld (BW) (Valmeekam et al., 2023). Chúng tôi đặt
độ sâu cây T thành 7 cho nhiệm vụ này để cho phép
nhiều lần lặp hơn. Cụ thể, chúng tôi sử dụng 1319 mẫu
từ tập kiểm tra GSM8K, 100 mẫu ngẫu nhiên
từ bộ dữ liệu CW theo (Yao et al., 2024),
và 145 mẫu từ bộ dữ liệu BW bước-6.

5.2 Đường cơ sở
Nghiên cứu này tập trung vào việc tăng tốc quá trình
xây dựng cây lý luận thay vì thuật toán tìm kiếm
hoặc các phương pháp gợi ý tiên tiến. Việc
lựa chọn đường cơ sở sẽ được thảo luận trong Phụ lục A.1. Chúng tôi xem xét các mô hình giải mã
sau đây làm đường cơ sở của chúng tôi: (1) AR biểu thị
ToT gốc (Yao et al., 2024) sử dụng việc tạo
tự hồi quy tiêu chuẩn như được thể hiện trong Hình 1
(a); (2) SD trình bày việc áp dụng lấy mẫu suy đoán
được chi tiết trong 3.2 trên cơ sở của
ToT như được thể hiện trong Hình 1 (b); (3) MCSD sử dụng
lấy mẫu đa ứng viên và sử dụng thuật toán xác minh
tiên tiến để cải thiện tỷ lệ chấp nhận
và tăng cường tốc độ của SD (Yang et al., 2024).
Tương tự như SD, nó tuân thủ chỉ một quy trình thực thi
nối tiếp một mẫu duy nhất. Đáng chú ý, cả SD và

--- TRANG 7 ---
Temp. kconfig PhươngPhápCW(T= 2) GSM8K( T= 4) BW( T= 7)
Tokens/s Tăng tốc Tokens/s Tăng tốc Tokens/s Tăng tốc
0.2- AR 38.42 1.000 × 42.31 1.000 × 34.19 1.000 ×
(1,1,1)SD 39.96 1.040 × 51.11 1.208 × 36.28 1.061 ×
w. S EED 41.53 1.081× 53.14 1.256× 36.93 1.080×
MCSD 40.19 1.046 × 52.42 1.239 × 36.04 1.054 ×
w. S EED 41.46 1.079× 53.78 1.271× 36.96 1.081×
(2,2,1)SD 46.22 1.203 × 60.63 1.433 × 40.04 1.171 ×
w. S EED 48.60 1.265× 65.24 1.542× 44.24 1.294×
MCSD 46.80 1.218 × 60.88 1.439 × 40.79 1.193 ×
w. S EED 48.79 1.270× 65.58 1.550× 44.75 1.309×
1.0- AR 39.47 1.000 × 47.81 1.000 × 34.62 1.000 ×
(1,1,1)SD 45.90 1.163 × 55.32 1.157 × 35.14 1.015 ×
w. S EED 46.77 1.185× 61.01 1.276× 38.94 1.125×
MCSD 45.63 1.156 × 58.47 1.223 × 38.05 1.099 ×
w. S EED 46.54 1.179× 65.50 1.370× 40.02 1.156×
(2,2,1)SD 57.39 1.454 × 66.74 1.396 × 45.98 1.328 ×
w. S EED 58.89 1.492× 72.62 1.519× 47.22 1.364×
MCSD 56.24 1.425 × 67.36 1.409 × 46.18 1.334 ×
w. S EED 59.76 1.514× 74.44 1.557× 47.71 1.378×
Bảng 1: Hiệu suất tăng tốc của SEED được đề xuất của chúng tôi và các đường cơ sở, với cài đặt SEED cho Md và Mt
lần lượt là LLaMA-68M và LLaMA2-7B. Minh họa của kconfig=(2,2,1) được trình bày trong Phụ lục B.
Tất cả tăng tốc so với AR vanilla. Kết quả tốt nhất trong tất cả các phương pháp được in đậm.

MCSD trực giao với SEED được đề xuất của chúng tôi. Chúng tôi
áp dụng khung của chúng tôi trong hai cách tiếp cận giải mã này
để xác minh hiệu quả của SEED trên các
tỷ lệ chấp nhận khác nhau.

5.3 Thiết lập
Bộ Mô hình Đánh giá của chúng tôi dựa trên
bộ LLaMA Chat có sẵn công khai (Touvron et al.,
2023b), đã thể hiện hiệu suất mạnh trong việc thực thi
hướng dẫn và trong các tình huống TSB. Chúng tôi sử dụng
(Md,Mt) theo công trình trước (Chen et al.,
2023b; Yang et al., 2024): (LLaMA-68M-Chat3,
LLaMA-2-Chat-7B4) và (LLaMA-160M-Chat5,
LLaMA-2-Chat-13B6). Để xác minh tính mở rộng
của khung của chúng tôi, chúng tôi cũng tiến hành thí nghiệm
sử dụng bộ QWen (Bai et al., 2023). Thông tin chi tiết
và kết quả cho cả cặp LLaMA khác và bộ QWen
có thể được tìm thấy trong Phụ lục A.2.

Siêu tham số Chúng tôi thực hiện thuật toán BFS
làm chiến lược tìm kiếm. Nhiệt độ được đặt thành 0.2

3https://huggingface.co/Felladrin/Llama-68M-Chat-v1
4https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
5https://huggingface.co/Felladrin/Llama-160M-Chat-v1
6https://huggingface.co/meta-llama/Llama-2-13b-chat-hf

và 1.0 để đánh giá dưới các điều kiện khác nhau.7 Các
gợi ý chi tiết cho Bộ tạo Suy nghĩ và
Bộ đánh giá Trạng thái, cùng với thiết lập ToT cho mỗi
nhiệm vụ được cung cấp trong Phụ lục D.

Môi trường Các thí nghiệm được tiến hành trên
một NVIDIA RTX A100-80G đơn hoặc một nút đơn
được trang bị bốn GPU NVIDIA RTX 3090-
24GB. Sự khác biệt nhỏ trong hiệu suất phần cứng
giữa những nền tảng này được thảo luận trong
Phụ lục A.4.

6 Kết quả và Phân tích
6.1 Kết quả Chính
Bảng 1 trình bày phân tích toàn diện về
SEED được đề xuất của chúng tôi và các đường cơ sở được áp dụng cho ba
bộ dữ liệu lý luận. Nếu mỗi phần tử trong kconfig là 1, chúng tôi
sử dụng lấy mẫu đơn truyền thống tại mỗi vị trí
của chuỗi nháp. Nếu không, chúng tôi sử dụng chú ý cây,
đại diện cho việc lấy mẫu nhiều token ứng viên
tại mỗi vị trí và xác minh song song
(chi tiết trong Phần 3.2). Một số lớn hơn tại mỗi
vị trí trong kconfig có nghĩa là nhiều ứng viên hơn,

7Chúng tôi tránh nhiệt độ 0 vì giải mã tham lam không
có ý nghĩa trong Bộ tạo Suy nghĩ.

--- TRANG 8 ---
0.275 0.30 0.325 0.35 0.375 0.40
Tỷ lệ Chấp nhận 
1.01.21.41.61.82.0Tăng tốc
CW, [SD] vs [SD w/ SEED]
SD
SD w/ SEED
0.30 0.325 0.35 0.375 0.40 0.425
Tỷ lệ Chấp nhận 
0.81.01.21.41.61.82.0Tăng tốc
GSM8K, [SD] vs [SD w/ SEED]
SD
SD w/ SEED
0.225 0.25 0.275 0.30 0.325 0.35 0.375 0.40
Tỷ lệ Chấp nhận 
1.01.11.21.31.41.51.6Tăng tốc
BW, [SD] vs [SD w/ SEED]
SD
SD w/ SEED
0.275 0.30 0.325 0.35 0.375 0.40
Tỷ lệ Chấp nhận 
1.01.21.41.61.82.0Tăng tốc
CW, [MCSD] vs [MCSD w/ SEED]
MCSD
MCSD w/ SEED
0.30 0.325 0.35 0.375 0.40 0.425
Tỷ lệ Chấp nhận 
0.81.01.21.41.61.82.0Tăng tốc
GSM8K, [MCSD] vs [MCSD w/ SEED]
MCSD
MCSD w/ SEED
0.225 0.25 0.275 0.30 0.325 0.35 0.375 0.40
Tỷ lệ Chấp nhận 
1.01.11.21.31.41.51.6Tăng tốc
BW, [MCSD] vs [MCSD w/ SEED]
MCSD
MCSD w/ SEEDHình 5: Sự thay đổi hiệu suất tăng tốc trên ba bộ dữ liệu ở các tỷ lệ chấp nhận α khác nhau.

BW GSM8k CW0.200.250.300.350.400.45Tỷ lệ Chấp nhận 
Bộ tạo Suy nghĩ
Bộ đánh giá Trạng thái
BW GSM8k CW1.01.21.41.61.82.0Tăng tốcBộ tạo Suy nghĩ
Bộ đánh giá Trạng thái
Hình 6: Tỷ lệ chấp nhận α và hiệu suất tăng tốc của
Bộ tạo Suy nghĩ và Bộ đánh giá Trạng thái.

nói chung mang lại tăng tốc cao hơn. MCSD đạt được
tăng tốc tốt hơn SD bằng cách sử dụng thuật toán xác minh
tiên tiến dẫn đến tỷ lệ chấp nhận cao hơn. Với SEED của chúng tôi, hiệu suất của hai
đường cơ sở này được cải thiện thêm, chứng minh
hiệu quả của nó trên các tỷ lệ chấp nhận khác nhau.
Trên tất cả các bộ dữ liệu trên các độ sâu lý luận khác nhau
T, khung của chúng tôi, luôn vượt trội hơn các
đường cơ sở trên các cài đặt và cấu hình khác nhau,
bao gồm nhiệt độ và kconfig, về mặt
tăng tốc, đạt được tăng tốc thêm. Cụ thể,
trên bộ dữ liệu GSM8K, sử dụng chú ý cây,
MCSD trong khung SEED được đề xuất của chúng tôi đạt được
tăng tốc lên đến 1.5× so với AR, tạo ra
gần 30 token bổ sung mỗi giây.

6.2 Phân tích
Chúng tôi sử dụng SEED(với MCSD) để tiến hành
thí nghiệm phân tích sau để trả lời
câu hỏi nghiên cứu (RQ) sau sử dụng dưới điều kiện
kconfig = (2,2,1) và nhiệt độ = 1.0.

RQ1: SEED hoạt động như thế nào ở các tỷ lệ chấp nhận
khác nhau? Chúng tôi lấy mẫu các điểm dữ liệu từ ba bộ dữ liệu trong các phạm vi tỷ lệ chấp nhận khác nhau, chúng tôi
báo cáo riêng biệt tăng tốc đạt được bởi SEED
và đường cơ sở cho những mẫu này trong Hình 5. Rõ ràng
rằng dưới cùng tỷ lệ chấp nhận, SEED
vượt trội hơn đường cơ sở về mặt tăng tốc. Cải thiện
này được quy cho khung của chúng tôi, đạt được
tăng tốc không phải bằng cách tăng tỷ lệ chấp nhận
mà bằng cách lập lịch các mô hình nháp. Ngoài ra,
khi tỷ lệ chấp nhận tăng, cả SEED và
đường cơ sở đều thể hiện xu hướng tăng đáng chú ý trong
tăng tốc, đây là đặc tính vốn có của
phương pháp giải mã suy đoán.

RQ2: SEED có thể hiện hiệu ứng tăng tốc khác nhau
trên các thành phần khác nhau của ToT không? SEED
tăng tốc hai thành phần trong xây dựng cây lý luận,
đó là TG và SE. Hình 6
trình bày tỷ lệ chấp nhận α và hiệu suất tăng tốc
của hai thành phần chính của phương pháp SEED
trên bộ dữ liệu GSM8K, xác nhận rằng
câu trả lời cho RQ2 là Có. Thành phần TG thực thi
nhiều lần lặp với cùng gợi ý trong khi
SE đề cập đến đánh giá nhiều lần lặp với
các gợi ý khác nhau. Thành phần TG luôn
vượt trội hơn thành phần SE về cả
α và tăng tốc, có thể do SE
tương đối khó hơn so với TG. Sự thành thạo
giữa mô hình đích và mô hình nháp có thể
phù hợp hơn trong việc đề xuất các suy nghĩ,
so với khả năng ra quyết định.

RQ3: Tăng tốc và việc sử dụng GPU tăng như thế nào
với số lượng suy nghĩ? Trong giải mã
suy đoán, cả mô hình đích và nháp pa-

--- TRANG 9 ---
TimeGPU-Util(%)(a)
SD
TimeGPU-Util(%)SEED
2 3 4 5 6
Số lượng Suy nghĩ0.81.01.21.4Tăng tốc(b)
5060708090100
GPU-AUC(%)
AR (Tăng tốc & GPU-AUC)
SEED (Tăng tốc)
SEED (GPU-AUC)Hình 7: (a) So sánh trực quan về việc sử dụng GPU
giữa SD và SEED trong 120 giây
dưới n= 3. (b) Sự thay đổi của tăng tốc và tỷ lệ
chấp nhận α với số lượng đường dẫn lý luận n.

rameters được tải vào bộ nhớ GPU. Chúng tôi ghi lại
việc sử dụng GPU trong cùng thời lượng cho
SD và SEED trên một thể hiện GSM8K để trực quan
hiệu quả của việc soạn thảo song song trong Hình 7 (a).
Phần trên minh họa việc sử dụng GPU của
SD dao động từng đợt, chủ yếu do
mô hình đích nhàn rỗi trong quá trình soạn thảo, trong khi
phần dưới cho thấy SEED thể hiện việc sử dụng ổn định,
được quy cho sự tham gia tích cực của mô hình đích
trong giai đoạn xác minh. Khi số lượng
suy nghĩ n tăng trong một phạm vi nhất định, thời gian
nhàn rỗi của mô hình đích giảm, dẫn đến
việc sử dụng GPU cao hơn và tăng tốc, như được thể hiện
trong Hình 7 (b). Tuy nhiên, khi số lượng
suy nghĩ trở nên quá lớn (ví dụ, n=6), khả năng xác minh
cố định của mô hình đích dẫn đến sự bão hòa tăng tốc
của SEED. Điều này biểu hiện như nhiều mô hình nháp hơn
được đặt trong trạng thái chờ, giảm
tính song song của nháp và gây ra nghẽn cổ chai làm giảm
việc sử dụng và tăng tốc.

7 Kết luận và Thảo luận
Trong bài báo này, chúng tôi giới thiệu SEED, một khung suy luận
mới được thiết kế để tối ưu tốc độ thời gian chạy
và quản lý việc sử dụng bộ nhớ GPU hiệu quả trong
việc xây dựng cây lý luận cho các nhiệm vụ lý luận
và lập kế hoạch phức tạp. SEED sử dụng
thực thi suy đoán có lập lịch để tăng cường
hiệu suất của LLMs bằng cách tích hợp việc quản lý
nhiều mô hình nháp và một mô hình đích duy nhất,
dựa trên các nguyên lý tương tự như lập lịch quy trình
hệ điều hành. Chiến lược này không chỉ
giảm thiểu độ trễ suy luận vốn có trong các phương pháp lý luận
dựa trên tìm kiếm cây mà còn sử dụng hiệu quả
các tài nguyên tính toán có sẵn. Đánh giá thí nghiệm
rộng rãi của chúng tôi trên ba lý luận chứng minh rằng SEED đạt được
những cải thiện đáng kể trong tốc độ suy luận, tạo ra
tới 20 token bổ sung mỗi giây.

Công trình của chúng tôi tăng tốc sự phát triển của ToT,
cung cấp tiềm năng mở rộng liền mạch để
tiến xa hơn việc mở rộng thời gian kiểm tra của LLMs (Zhang
et al., 2024a; Xie et al., 2024; Zhang et al., 2024c;
Snell et al., 2024; Wu et al., 2024). Khung
và tầm nhìn này đại diện cho một hướng đầy hứa hẹn
để cải thiện hiệu quả lý luận LLM trong
các ứng dụng thế giới thực.

Hạn chế
Mặc dù SEED đã đạt được hiệu suất tăng tốc
đặc biệt trong các thí nghiệm, công trình của chúng tôi
cũng có những hạn chế sau.

• Khung của chúng tôi giới thiệu việc soạn thảo song song,
bao gồm n−1 mô hình soạn thảo bổ sung,
vốn dĩ đòi hỏi việc bổ sung
một số lượng tương đương KV-Cache. Cho rằng
sự gia tăng được quy cho các mô hình nháp nhỏ
(68M/160M) tương đối tối thiểu, chúng tôi không
tối ưu hóa việc quản lý KV-Cache trong
công trình này.

• Nghiên cứu này chỉ tập trung vào việc tối ưu
tốc độ suy luận của việc xây dựng cây cho
nhiệm vụ lý luận TSB và không tối ưu
tốc độ tìm kiếm cho những nhiệm vụ này. Khung
của chúng tôi sử dụng thuật toán tìm kiếm tương đối đơn giản BFS.
Thực tế, SEED có thể tích hợp liền mạch
các thuật toán tìm kiếm tiên tiến hơn, như
A∗(Hart et al., 1968) và MCTS (Kocsis
và Szepesvári, 2006), v.v., mà chúng tôi để lại
cho nghiên cứu tương lai.

• Chúng tôi sử dụng thuật toán lập lịch được sử dụng
rộng rãi nhất, FCFS. Công trình tương lai có thể khám phá
việc tích hợp các thuật toán lập lịch tiên tiến hơn,
như những thuật toán được sử dụng trong hệ thống thời gian thực,
để tăng cường thêm tính phản hồi
và hiệu quả của SEED.

Trong tương lai, SEED có thể tương thích với
vLLM (Kwon et al., 2023) và FlashAttention-
2 (Dao, 2024), cho phép suy luận tiết kiệm bộ nhớ hơn
trên các chuỗi dài hơn. Ngoài ra,
KV-Cache bổ sung có thể được giảm bằng cách lưu trữ
tiền tố chung trong quá trình xây dựng cây lý luận,
điều này sẽ giảm chi phí song song trong các
lần lặp sau.

Hơn nữa, phương pháp của chúng tôi cung cấp một triển khai
tiềm năng của giải mã suy đoán theo lô từ
quan điểm lập lịch thực thi, có thể
được tích hợp với các phương pháp giải mã suy đoán theo lô
dựa trên KV-Cache khác (Ni et al., 2024), như
được thảo luận thêm trong Phụ lục A.5.

--- TRANG 10 ---
Lời cảm ơn
Các tác giả muốn cảm ơn những người đánh giá ẩn danh
vì những nhận xét sâu sắc của họ. Công trình này
được tài trợ bởi Quỹ Khoa học Tự nhiên Quốc gia
của Trung Quốc (Số Grant 62176053). Công trình này được
hỗ trợ bởi Trung tâm Tính toán Dữ liệu Lớn của
Đại học Đông Nam.

Tài liệu tham khảo
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774.

Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al. 2023. Qwen technical report. arXiv
e-prints, pages arXiv–2309.

Nikhil Bhendawade, Irina Belousova, Qichen Fu, Henry
Mason, Mohammad Rastegari, và Mahyar Najibi.
2024. Speculative streaming: Fast llm inference with-
out auxiliary models. arXiv e-prints, pages arXiv–
2402.

Ondˇrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve Saint-
Amand, Radu Soricut, Lucia Specia, và Aleš Tam-
chyna. 2014. Findings of the 2014 workshop on
statistical machine translation. In Proceedings of the
Ninth Workshop on Statistical Machine Translation,
pages 12–58, Baltimore, Maryland, USA. Associa-
tion for Computational Linguistics.

Alan Bundy và Lincoln Wallen. 1984. Breadth-first
search. Catalogue of artificial intelligence tools,
pages 13–13.

Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng,
Jason D. Lee, Deming Chen, và Tri Dao. 2024.
Medusa: Simple LLM inference acceleration frame-
work with multiple decoding heads. In Forty-first
International Conference on Machine Learning.

Charlie Chen, Sebastian Borgeaud, Geoffrey Irving,
Jean-Baptiste Lespiau, Laurent Sifre, và John
Jumper. 2023a. Accelerating large language model
decoding with speculative sampling. arXiv e-prints,
pages arXiv–2302.

Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun,
Jie Huang, và Kevin Chen-Chuan Chang. 2023b.
Cascade speculative drafting for even faster llm infer-
ence. arXiv preprint arXiv:2312.11462.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, et al. 2021. Training verifiers to solve math
word problems. arXiv preprint arXiv:2110.14168.

--- TRANG 11 ---
Tri Dao. 2024. Flashattention-2: Faster attention with
better parallelism and work partitioning. In The
Twelfth International Conference on Learning Repre-
sentations.

Luciano Del Corro, Allison Del Giorno, Sahaj Agarwal,
Bin Yu, Ahmed Hassan Awadallah, và Subhabrata
Mukherjee. 2023. Skipdecode: Autoregressive skip
decoding with batching and caching for efficient llm
inference.

Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich,
Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas
Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed
Roman, Ahmed Aly, Beidi Chen, và Carole-Jean
Wu. 2024. LayerSkip: Enabling early exit inference
and self-speculative decoding. In Proceedings of the
62nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
12622–12642, Bangkok, Thailand. Association for
Computational Linguistics.

Marjan Ghazvininejad, Omer Levy, Yinhan Liu, và
Luke Zettlemoyer. 2019. Mask-predict: Parallel de-
coding of conditional masked language models. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP), pages 6112–6121.

Lin Guan, Karthik Valmeekam, Sarath Sreedharan,
và Subbarao Kambhampati. 2023. Leveraging pre-
trained large language models to construct and utilize
world models for model-based task planning. Ad-
vances in Neural Information Processing Systems,
36:79081–79094.

Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen
Wang, Daisy Wang, và Zhiting Hu. 2023. Rea-
soning with language model is planning with world
model. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing,
pages 8154–8173.

Peter E Hart, Nils J Nilsson, và Bertram Raphael. 1968.
A formal basis for the heuristic determination of min-
imum cost paths. IEEE transactions on Systems Sci-
ence and Cybernetics, 4(2):100–107.

Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee,
và Di He. 2023. Rest: Retrieval-based speculative
decoding. arXiv e-prints, pages arXiv–2311.

Wenyang Hui, Yan Wang, Kewei Tu, và Chengyue
Jiang. 2024. Rot: Enhancing large language mod-
els with reflection on search trees. arXiv preprint
arXiv:2404.05449.

Levente Kocsis và Csaba Szepesvári. 2006. Bandit
based monte-carlo planning. In European conference
on machine learning, pages 282–293. Springer.

Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, và Yusuke Iwasawa. 2022. Large lan-
guage models are zero-shot reasoners. Advances in
neural information processing systems, 35:22199–
22213.

--- TRANG 12 ---
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying
Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gon-
zalez, Hao Zhang, và Ion Stoica. 2023. Efficient
memory management for large language model serv-
ing with pagedattention. In Proceedings of the 29th
Symposium on Operating Systems Principles, pages
611–626.

Yaniv Leviathan, Matan Kalman, và Yossi Matias.
2023. Fast inference from transformers via spec-
ulative decoding. In International Conference on
Machine Learning, pages 19274–19286. PMLR.

Yuhui Li, Fangyun Wei, Chao Zhang, và Hongyang
Zhang. 2024. EAGLE: Speculative sampling requires
rethinking feature uncertainty. In Forty-first Interna-
tional Conference on Machine Learning.

Haoran Liao, Jidong Tian, Shaohua Hu, Hao He, và
Yaohui Jin. 2024. Look before you leap: Problem
elaboration prompting improves mathematical rea-
soning in large language models. arXiv e-prints,
pages arXiv–2402.

Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica,
Zhijie Deng, Alvin Cheung, và Hao Zhang. 2023.
Online speculative decoding.

Bo-Ru Lu, Nikita Haduong, Chien-Yu Lin, Hao Cheng,
Noah A Smith, và Mari Ostendorf. 2024a. Encode
once and decode in parallel: Efficient transformer
decoding. arXiv e-prints, pages arXiv–2403.

Jinghui Lu, Ziwei Yang, Yanjie Wang, Xuejing Liu, và
Can Huang. 2024b. Padellm-ner: Parallel decoding
in large language models for named entity recogni-
tion. arXiv e-prints, pages arXiv–2402.

Yunsheng Ni, Chuanjian Liu, Yehui Tang, Kai Han, và
Yunhe Wang. 2024. Ems-sd: Efficient multi-sample
speculative decoding for accelerating large language
models. arXiv e-prints, pages arXiv–2405.

OpenAI. 2022. Introducing ChatGPT.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in neural in-
formation processing systems, 35:27730–27744.

Andysah Putera Utama Siahaan. 2016. Comparison
analysis of cpu scheduling: Fcfs, sjf and round robin.
International Journal of Engineering Development
and Research, 4(3):124–132.

Charlie Snell, Jaehoon Lee, Kelvin Xu, và Aviral Ku-
mar. 2024. Scaling llm test-time compute optimally
can be more effective than scaling model parameters.
arXiv preprint arXiv:2408.03314.

Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong
Tian, và Beidi Chen. 2024a. Triforce: Lossless
acceleration of long sequence generation with hierar-
chical speculative decoding. In First Conference on
Language Modeling.

--- TRANG 13 ---
Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ah-
mad Beirami, Himanshu Jain, và Felix Yu. 2024b.
Spectr: Fast speculative decoding via optimal trans-
port. Advances in Neural Information Processing
Systems, 36.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, et al. 2023a. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023b. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288.

Karthik Valmeekam, Matthew Marquez, Sarath Sreed-
haran, và Subbarao Kambhampati. 2023. On the
planning abilities of large language models-a criti-
cal investigation. Advances in Neural Information
Processing Systems, 36:75993–76005.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,
Ed H. Chi, Sharan Narang, Aakanksha Chowdhery,
và Denny Zhou. 2023. Self-consistency improves
chain of thought reasoning in language models. In
The Eleventh International Conference on Learning
Representations.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
et al. 2022. Chain-of-thought prompting elicits rea-
soning in large language models. Advances in neural
information processing systems, 35:24824–24837.

Siwei Wu, Zhongyuan Peng, Xinrun Du, Tuney Zheng,
Minghao Liu, Jialong Wu, Jiachen Ma, Yizhi Li, Jian
Yang, Wangchunshu Zhou, et al. 2024. A compara-
tive study on reasoning patterns of openai's o1 model.
arXiv preprint arXiv:2410.13639.

Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang,
Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, và Zhi-
fang Sui. 2024. Unlocking efficiency in large lan-
guage model inference: A comprehensive survey of
speculative decoding. In Findings of the Associa-
tion for Computational Linguistics ACL 2024, pages
7655–7671, Bangkok, Thailand and virtual meeting.
Association for Computational Linguistics.

Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen
Kan, Timothy P Lillicrap, Kenji Kawaguchi, và
Michael Shieh. 2024. Monte carlo tree search boosts
reasoning via iterative preference learning. arXiv
preprint arXiv:2405.00451.

Hanqi Yan, Qinglin Zhu, Xinyu Wang, Lin Gui, và
Yulan He. 2024. Mirror: A multiple-perspective
self-reflection method for knowledge-rich reasoning.
arXiv preprint arXiv:2402.14963.

--- TRANG 14 ---
Sen Yang, Shujian Huang, Xinyu Dai, và Jiajun Chen.
2024. Multi-candidate speculative decoding. arXiv
e-prints, pages arXiv–2401.

Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
Tom Griffiths, Yuan Cao, và Karthik Narasimhan.
2024. Tree of thoughts: Deliberate problem solving
with large language models. Advances in Neural
Information Processing Systems, 36.

Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong
Li, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco
Pavone, Yuqiang Li, et al. 2024a. Llama-berry: Pair-
wise optimization for o1-like olympiad-level mathe-
matical reasoning. arXiv preprint arXiv:2410.02884.

Linhai Zhang, Jialong Wu, Deyu Zhou, và Guoqiang
Xu. 2024b. STAR: Constraint LoRA with dynamic
active learning for data-efficient fine-tuning of large
language models. In Findings of the Association for
Computational Linguistics ACL 2024, pages 3519–
3532, Bangkok, Thailand and virtual meeting. Asso-
ciation for Computational Linguistics.

Xuan Zhang, Chao Du, Tianyu Pang, Qian Liu, Wei
Gao, và Min Lin. 2024c. Chain of preference opti-
mization: Improving chain-of-thought reasoning in
LLMs. In The Thirty-eighth Annual Conference on
Neural Information Processing Systems.

Wei Zhao và John A Stankovic. 1989. Performance
analysis of fcfs and improved fcfs scheduling algo-
rithms for dynamic real-time computer systems. In
1989 Real-Time Systems Symposium, pages 156–157.
IEEE Computer Society.

Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat,
Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv
Kumar, Jean-François Kagy, và Rishabh Agarwal.
2023. Distillspec: Improving speculative decoding
via knowledge distillation. In The Twelfth Interna-
tional Conference on Learning Representations.

--- TRANG 15 ---
A Thảo luận
A.1 Lựa chọn Đường cơ sở
Xem Phần 5.2, nơi chúng tôi liệt kê tất cả các đường cơ sở
được sử dụng để so sánh với SEED được đề xuất của chúng tôi trong
nghiên cứu này. Tuy nhiên, một số chiến lược giải mã suy đoán
khác chưa được khám phá làm đường cơ sở.
Chúng tôi không kết luận những chiến lược này dựa trên
các cân nhắc sau như được thể hiện trong Bảng 4:

(1) Không cần huấn luyện cho biết phương pháp có
yêu cầu huấn luyện hay không.
∗Medusa (Cai et al., 2024) thêm các đầu FFN bổ sung
lên trên bộ giải mã Transformer, cho phép
tạo token song song tại mỗi bước;
∗Eagle (Li et al., 2024) thực hiện quá trình soạn thảo
tự hồi quy ở mức có cấu trúc hơn, cụ thể
là lớp thứ hai từ trên cùng của các đặc trưng;
∗SS(Bhendawade et al., 2024) tích hợp giai đoạn
soạn thảo vào mô hình đích bằng cách sửa đổi
mục tiêu tinh chỉnh từ token tiếp theo
thành dự đoán n-gram tương lai.

Những phương pháp này đều yêu cầu huấn luyện và không phải
cắm-và-chạy, vì chúng huấn luyện LLM để phục vụ
như cả mô hình đích và mô hình nháp, điều này
phân loại chúng như tự soạn thảo ■ theo Xia
et al. (2024); ngược lại, phương pháp của chúng tôi sử dụng
soạn thảo độc lập ▲(nháp-và-đích), đặt nó trong
một loại SD khác. Do đó, chúng tôi không coi
chúng là đường cơ sở.

(2) Không cần kiến thức bổ sung cho biết quá trình
SD có sử dụng các mô-đun kiến thức bổ sung hay không.
∗CS-drafting (Chen et al., 2023b) dựa vào
một mô hình bigram dựa trên phân phối xác suất
của Wikipedia làm mô hình nháp ở mức
cơ bản hơn.
∗REST (He et al., 2023) truy xuất từ các kho
dữ liệu mã và hội thoại rộng lớn để tạo
token nháp.

Hai cách tiếp cận này giới thiệu các mô-đun kiến thức bên ngoài,
làm cho nó phụ thuộc đáng kể vào
hiệu quả của các mô-đun kiến thức bên ngoài
và không công bằng khi so sánh chúng tôi với các mô hình
nháp-và-đích.

(3) Không mất mát cho biết phương pháp có
tạo ra cùng phân phối đầu ra như giải mã AR
trong mô hình nền hay không.
SS(Bhendawade et al., 2024) và Medusa (Cai
et al., 2024), vốn dĩ không không mất mát,

Temp. kconfig Phương pháp Tokens/s Tăng tốc
0.2- AR 31.22 1.000×
(1,1,1,1)SD 32.91 1.054×
w. S EED 34.62 1.109×
0.6- AR 37.93 1.000×
(1,1,1,1)SD 39.22 1.034×
w. S EED 41.91 1.105×
1- AR 33.86 1.000×
(1,1,1,1)SD 34.91 1.031×
w. S EED 39.35 1.162×
Bảng 2: Hiệu suất tăng tốc trên bộ dữ liệu Viết Sáng tạo
của SEED trong việc sử dụng QWen1.5-0.5B-Chat làm
Md và QWen1.5-7B-Chat làm Mt. Từ vựng của
hai mô hình này giống hệt nhau, cho phép lấy mẫu suy đoán.

không phù hợp để so sánh với SEED được đề xuất
của chúng tôi, duy trì tính không mất mát nhất quán
với SD trong một nháp-rồi-xác minh đơn.

A.2 Khả năng Mở rộng và Khả năng Mở rộng
Bộ LLaMA Bảng 3 cho thấy hiệu suất
của mỗi phương pháp khi sử dụng LLaMA-160M-Chat8
làm mô hình nháp Md và LLaMA-2-Chat-13B9 làm
mô hình đích Mt.

Bộ QWen Khung của chúng tôi dựa trên giải mã
suy đoán, vì vậy thiết lập mô hình của mô hình nháp
và mô hình đích có thể nhất quán với
nó. Do đó, bất kỳ bộ LLM nào cũng có thể được tích hợp
vào khung của chúng tôi. Chúng tôi cũng tiến hành thí nghiệm
sử dụng bộ QWen1.5.10 Cụ thể, chúng tôi
sử dụng QWen1.5-0.5B-Chat11 làm mô hình nháp Md
và sử dụng QWen1.5-7B-Chat12 làm mô hình đích
Mt. Kết quả được trình bày trong Bảng 2. Kết quả
phù hợp với các phát hiện được trình bày trong
Phần 6.1, chứng minh hiệu suất vượt trội
của khung của chúng tôi. Nó cũng làm nổi bật khả năng mở rộng
của khung của chúng tôi đối với bộ LLM (Bai et al.,
2023).

A.3 Hiệu suất Nhiệm vụ
Độ chính xác Leviathan et al. (2023) đã chứng minh
các đầu ra của AR và SD giống nhau. Chúng tôi riêng biệt
đánh giá hiệu suất của bộ dữ liệu GSM8K sử dụng AR với QWen1.5-7B và SEED với

8https://huggingface.co/Felladrin/Llama-160M-Chat-v1
9https://huggingface.co/meta-llama/Llama-2-13b-chat-hf
10https://qwenlm.github.io/zh/blog/qwen1.5/
11https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat
12https://huggingface.co/Qwen/Qwen1.5-7B-Chat

--- TRANG 16 ---
Temp. kconfig PhươngphápCW(T= 2) GSM8K( T= 4) BW( T= 7)
Tokens/s Tăng tốc Tokens/s Tăng tốc Tokens/s Tăng tốc
0.2- AR 32.33 1.000 × 32.08 1.000 × 32.91 1.000 ×
(2,1,1)SD 33.14 1.025 × 34.97 1.090 × 33.17 1.008 ×
w. S EED 33.82 1.046× 36.80 1.147× 33.54 1.019×
MCSD 33.27 1.029 × 35.71 1.113 × 33.37 1.014 ×
w. S EED 36.18 1.119× 36.28 1.131× 34.36 1.044×
(4,2,1)SD 34.23 1.059 × 38.95 1.214 × 36.04 1.095 ×
w. S EED 38.57 1.193× 41.06 1.280× 36.76 1.117×
MCSD 35.56 1.100 × 41.09 1.281 × 37.58 1.142 ×
w. S EED 40.28 1.246× 44.11 1.375× 38.70 1.176×
1.0- AR 39.57 1.000 × 31.54 1.000 × 32.87 1.000 ×
(2,1,1)SD 40.28 1.018 × 35.23 1.117 × 34.32 1.044 ×
w. S EED 42.74 1.080× 36.71 1.164× 35.37 1.076×
MCSD 40.68 1.028 × 35.26 1.118 × 35.01 1.065 ×
w. S EED 43.37 1.096× 37.15 1.178× 35.86 1.091×
(4,2,1)SD 43.69 1.104 × 36.87 1.169 × 37.83 1.151 ×
w. S EED 47.25 1.194× 40.66 1.289× 38.56 1.173×
MCSD 45.19 1.142 × 36.90 1.170 × 39.28 1.195 ×
w. S EED 49.74 1.257× 41.54 1.317× 40.43 1.230×
Bảng 3: Hiệu suất tăng tốc của SEED được đề xuất của chúng tôi và các đường cơ sở, với cài đặt SEED cho Md và Mt
lần lượt là LLaMA-160M và LLaMA2-13B. Tất cả tăng tốc so với AR vanilla. Kết quả tốt nhất trong
tất cả các phương pháp được in đậm.

bộ QWen1.5 nói trên sử dụng QWen1.5-
0.5B và QWen1.5-7B, và thấy rằng sự khác biệt hiệu suất
nằm trong ±1.5%, điều này có thể chấp nhận được và chứng minh rằng hiệu suất thực sự
không mất mát.

Hiệu suất trên Các Nhiệm vụ Không phải Lý luận SEED
là một phương pháp đa năng có thể được áp dụng không chỉ
trong các nhiệm vụ lý luận liên quan đến TSB mà còn trong các
nhiệm vụ không phải lý luận. Khả năng áp dụng tổng quát của nó làm cho nó
trở thành một giải pháp mạnh mẽ cho nhiều tình huống khác nhau. Chúng tôi cụ thể
áp dụng SEED cho TSB trong các nhiệm vụ lý luận
dựa trên một số cân nhắc chính:

• Tính Thực tiễn của TSB: Phương pháp TSB cho phép
tạo ra nhiều chuỗi đồng thời
trong cả tình huống đầu vào giống hệt nhau và khác nhau. Điều này làm cho nó
trở thành một lựa chọn thực tế cho xử lý hiệu quả.

• Hiệu quả trên GPU Cấp Tiêu dùng: Thông thường, TSB bao gồm việc tạo ra 2-6 đường dẫn lý luận
đồng thời, có thể được xử lý bởi
GPU cấp tiêu dùng. Ngược lại, các phương pháp
gợi ý như Tự-Nhất quán (Wang
et al., 2023) thường yêu cầu tạo ra 10-20
chuỗi, song song đặt gánh nặng lớn hơn
lên tài nguyên phần cứng.

• Liên quan đến Độ khó Nhiệm vụ: Các nhiệm vụ lý luận là các chuẩn mực thách thức để đánh giá
LLMs. Nếu khung của chúng tôi đạt được
tăng tốc hiệu quả dưới sự chấp nhận trong những
nhiệm vụ này, nó có khả năng hoạt động tốt trên các nhiệm vụ
đơn giản hơn, như dịch thuật, nơi sự phù hợp
giữa mô hình đích và nháp tốt hơn. Trong các thí nghiệm
khám phá sớm, SEED đạt được tăng tốc 1.31x so với AR trên
bộ dữ liệu WMT (Bojar et al., 2014), chứng minh
hiệu quả của nó.

A.4 Phụ thuộc Phần cứng
Thí nghiệm được tiến hành trên một máy chủ 4 ×3090
trong giai đoạn khám phá sớm. Từ các
thí nghiệm trên phần cứng khác nhau được thể hiện trong Hình 5, phương pháp
của chúng tôi vẫn hiệu quả so với SD với
cùng cài đặt. Hiệu suất tăng tốc trên
4×3090 thấp hơn so với 1 ×A100, có thể do
thời gian giao tiếp tăng giữa nhiều
GPU (Sun et al., 2024a). Điều này cũng rõ ràng từ
kết quả bộ Qwen, nơi SD hoạt động tệ hơn
AR trên 4 ×3090.

A.5 Suy luận Theo lô
Suy luận theo lô xử lý nhiều chuỗi có
độ dài khác nhau. Trong SD, mỗi chuỗi trong cùng
lô yêu cầu padding bổ sung do các tỷ lệ chấp nhận khác nhau

--- TRANG 17 ---
Phương pháp Không cần huấn luyện Không mất mát Loại SD Không cần kiến thức bổ sung Tăng tốc
Vanilla AR ✓ ✓ - ✓ ✗
SD (Leviathan et al., 2023) ✓ ✓ ▲ ✓ ✓
CS-Drafting (Chen et al., 2023b) ✓ ✓ ▲ ✗ ✓
REST (He et al., 2023) ✓ ✓ ▲ ✗ ✓
Medusa (Cai et al., 2024) ✗ ✗ ■ ✓ ✓
Eagle (Li et al., 2024) ✗ ✓ ■ ✓ ✓
SS (Bhendawade et al., 2024) ✗ ✗ ■ ✓ ✓
MCSD (Yang et al., 2024) ✓ ✓ ▲ ✓ ✓
SEED (Của chúng tôi) ✓ ✓ ▲ ✓ ✓
Bảng 4: So sánh toàn diện của các phương pháp được liệt kê và SEED. ■ đại diện cho phương pháp SD nháp-và-đích,
trong khi ▲ đại diện cho phương pháp SD tự-nháp.

Bộ LLM GPU Phương pháp Tokens/s Tăng tốc
LLaMA2
160M/13B4×RTX 3090sAR 38.77 1.000×
SD 42.18 1.088×
w. S EED 44.93 1.159×
1×RTX A100AR 39.57 1.000×
SD 43.69 1.104×
w. S EED 47.25 1.194×
Qwen1.5
0.5B/7B4×RTX 3090sAR 27.51 1.000×
SD 27.43 0.997×
w. S EED 29.57 1.075×
1×RTX A100AR 33.86 1.000×
SD 34.91 1.031×
w. S EED 39.35 1.162×
Bảng 5: Hiệu suất tốc độ của bộ LLaMA2 trên bộ dữ liệu Viết
Sáng tạo dưới các môi trường phần cứng khác nhau
với nhiệt độ = 1.0 và kconfig = (1,1,1,1),
cũng như hiệu suất của bộ Qwen1.5 trên
bộ dữ liệu GSM8K trên các môi trường phần cứng khác nhau
với nhiệt độ = 1.0 và kconfig = (4,2,1).

tance rates và độ dài chuỗi, có thể dẫn đến
lưu trữ và tính toán quá mức (Ni et al.,
2024). Điều này có thể dẫn đến KV-Cache quá dài,
do đó làm chậm hiệu ứng tăng tốc do
độ dài chấp nhận không nhất quán. SEED của chúng tôi duy trì
độ dài ban đầu của KV-Cache mà không cần
padding dựa trên các tỷ lệ chấp nhận khác nhau.
Mỗi chuỗi nháp được xác minh tương ứng trực tiếp
với một chuỗi trong lô (số lượng mô hình nháp
n = kích thước lô). Cách tiếp cận soạn thảo song song của chúng tôi
đảm bảo triển khai lô hiệu quả trong khi bảo tồn
lợi ích tăng tốc của SD.

B Chi tiết về Chú ý Cây
Đặt kconfig thành (2,2,1) cho thấy rằng mỗi giai đoạn nháp
tạo ra một nhóm k= 3 token, với
hai vị trí đầu mỗi vị trí lấy mẫu 2 ứng viên,
và vị trí thứ ba lấy mẫu 1. Hình 8 minh họa

x22x11(2, 2, 1)
x21 x31 Rootk_config( k=3)
x11x21x22x31
x11
x21
x22
x31
 x24x12x23x32
x33
x34Hình 8: Chú ý cây được sử dụng trong SEED, nhiều
token trong chuỗi đơn được xử lý đồng thời.
Root biểu thị các token trước đó. ✓ cho thấy nơi có
chú ý, trong khi phần còn lại được che. Để đơn giản,
chúng tôi chỉ trực quan hóa mặt nạ chú ý cây của các token có
màu vàng.

một trường hợp chú ý cây với cấu hình
kconfig = (2,2,1).

C Xây dựng Cây Lý luận
Quá trình xây dựng cây lý luận sử dụng
Thuật toán BFS được nêu trong Thuật toán 2.

Thuật toán 2 SEED(x, pθ,G, n,E, s, b)
1:Input: Gợi ý ban đầu I, thực thi suy đoán có lập lịch
với chiến lược lập lịch theo vòng pθ, bộ tạo suy nghĩ
G(·) với số lượng suy nghĩ n, bộ đánh giá trạng thái E(·),
giới hạn bước T, giới hạn chiều rộng b.
2:Initialize: Trạng thái S;S0← {I}
3:fori= 1,···,Tdo
4: S′
i← { [c, zi]|c←Si−1,zi∈G(pθ, c, n)}
▷Tạo suy nghĩ Song song
5: Ei←E(pθ, S′
i) ▷Đánh giá trạng thái Song song
6: Si←arg max S⊂S′
i,|S|=bP
s∈SEi(s)
7:end for
8:return G(pθ,arg max s∈STET(s),1)

--- TRANG 18 ---
D Thiết lập Chi tiết và Gợi ý
Chúng tôi triển khai một ToT-BFS đơn giản và tổng quát
theo Yao et al. (2024). Trong Bộ tạo
Suy nghĩ, chúng tôi tận dụng chiến lược lấy mẫu để tạo
suy nghĩ cho bước suy nghĩ tiếp theo. Trong
Bộ đánh giá Trạng thái, chúng tôi tận dụng chiến lược giá trị
để đánh giá các suy nghĩ được tạo ra và đưa ra một giá trị vô hướng
(ví dụ, "1-10") hoặc một phân loại (ví dụ,
"tốt/xấu") có thể được chuyển đổi heuristic
thành một giá trị. Để giới thiệu tính đa dạng trong việc tạo
suy nghĩ trên tất cả các nhiệm vụ, chúng tôi đặt nhiệt độ tạo
là 0.2/1( >0) cho các mô hình bộ LLaMA
và 0.2/0.6/1( >0) cho các mô hình bộ QWen. Độ sâu cây
T gợi ý rằng các hoạt động với
mức độ phức tạp hoặc lần lặp khác nhau, với các cây sâu hơn
có thể đại diện cho các tính toán hoặc quá trình ra quyết định
phức tạp hơn. Thiết lập ToT của ba nhiệm vụ mà SEED sử dụng như sau:

• Viết Sáng tạo: Chúng tôi xây dựng một cây lý luận
với độ sâu T là 2 (với 1 bước suy nghĩ
trung gian) tạo ra 3 kế hoạch và
đoạn văn. Bộ đánh giá Trạng thái đánh giá các kế hoạch
và đưa ra điểm mạch lạc với mỗi kế hoạch
và đoạn văn.

• GSM8K: Chúng tôi xây dựng một cây lý luận với
độ sâu T là 4 (với 3 bước suy nghĩ
trung gian) tạo ra 3 câu hỏi phụ và
câu trả lời phụ tương ứng. Thiết lập này phù hợp
với các phát hiện từ Hao et al. (2023),
cho thấy rằng ba bước thường đủ
để đạt được mức độ chính xác có thể chấp nhận được. Bộ đánh giá Trạng thái đánh giá chúng và
đưa ra một con số đại diện cho sự hữu ích
để trả lời câu hỏi. Chúng tôi chọn cái
có giá trị cao nhất và thêm nó vào
câu hỏi phụ và câu trả lời phụ trước đó.

• Blocksworld 6-bước: Chúng tôi xây dựng một cây lý luận
với độ sâu T là 7 (với 6 bước suy nghĩ
trung gian) tạo ra 3 suy nghĩ,
bao gồm kế hoạch hành động và hành động hiện tại.
Do tính phức tạp của nhiệm vụ này, các minh họa
được cung cấp trong gợi ý, được gắn nhãn
là "tốt/xấu", để hỗ trợ Bộ đánh giá Trạng thái trong
đánh giá của nó.

Các gợi ý cho các nhiệm vụ được mô tả ở trên được
trình bày dưới đây. Các phần màu cam trong gợi ý được
yêu cầu để LLM hoàn thành. Trong quá trình đánh giá, chúng tôi yêu cầu LLM tạo ra cả điểm số
và giải thích (một ngữ cảnh với 128 token mới),
thay vì chỉ một điểm số. Cách tiếp cận này thúc đẩy
tăng tốc trong việc tạo ra và làm cho đánh giá
ToT hợp lý hơn.

Gợi ý cho GSM8K
Bộ tạo Suy nghĩ
Cho một câu hỏi: {initial_prompt}, câu hỏi phụ
và câu trả lời phụ trước đó là:
{state_text}
Vui lòng đưa ra câu hỏi phụ tiếp theo để lý luận
thêm về câu hỏi.
Câu hỏi phụ là: {sub-question}
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
Cho một câu hỏi: {initial_prompt}, câu hỏi phụ
là: {sub_question}
Vui lòng trả lời câu hỏi phụ dựa trên
câu hỏi.
Câu trả lời phụ là: {sub_answer}

Bộ đánh giá Trạng thái
Cho một câu hỏi: {initial_prompt}, câu hỏi phụ
là: {sub_question}, câu trả lời phụ là:
{sub_answer}
Vui lòng đưa ra một số từ 1 đến 10 để
đánh giá câu trả lời. Số càng cao, càng có nhiều
sự giúp đỡ trong việc trả lời câu hỏi.
Số là: {value}

Gợi ý cho Viết Sáng tạo
Bộ tạo Suy nghĩ
Viết một đoạn văn mạch lạc gồm 4 đoạn ngắn. Câu
kết thúc của mỗi đoạn phải là:
{initial_prompt}
Lập kế hoạch rồi viết. Đầu ra của bạn nên có
định dạng sau:
Kế hoạch:
Kế hoạch của bạn ở đây.
Đoạn văn:
Đoạn văn của bạn ở đây.
Đầu ra là:
{Plan}
{Passage}

Bộ đánh giá Trạng thái
Phân tích đoạn văn: {Passage}, sau đó ở dòng cuối
kết luận "Do đó điểm mạch lạc là [s]", trong đó [
s] là một số nguyên từ 1 đến 10.
Điểm mạch lạc là: {value}

Gợi ý cho Blocksworld
Bộ tạo Suy nghĩ
Tôi đang chơi với một bộ khối mà tôi cần
sắp xếp các khối thành các chồng. Đây là những hành động
tôi có thể làm:
Nhặt một khối
Bỏ một khối từ trên đỉnh khối khác
Đặt xuống một khối
Xếp một khối lên trên đỉnh khối khác

Tôi có những hạn chế sau đối với hành động của mình:
##Hạn chế về Hành động##
<—Bỏ qua minh họa—>
[TUYÊN BỐ]
{initial_prompt}
Kế hoạch của tôi như sau:
{state_text}
Hành động hiện tại là:
{action}

Bộ đánh giá Trạng thái
Tôi đang chơi với một bộ khối mà tôi cần
sắp xếp các khối thành các chồng. Đây là những hành động
tôi có thể làm:
Nhặt một khối
Bỏ một khối từ trên đỉnh khối khác
Đặt xuống một khối
Xếp một khối lên trên đỉnh khối khác

Tôi có những hạn chế sau đối với hành động của mình:
##Hạn chế về Hành động##
<—Bỏ qua minh họa—>
Vui lòng đánh giá xem hành động đã cho có phải là một
hành động tốt dưới những điều kiện nhất định hay không.
[TUYÊN BỐ]
{initial_prompt}
[HÀNH ĐỘNG]
{state_text}
[ĐÁNH GIÁ]
Đánh giá là:
{evaluation}

Hạn chế về Hành động cho Blocksworld
Tôi có những hạn chế sau đối với hành động của mình:
Tôi chỉ có thể nhặt hoặc bỏ một khối tại một thời điểm.
Tôi chỉ có thể nhặt hoặc bỏ một khối nếu tay tôi trống.
Tôi chỉ có thể nhặt một khối nếu khối đó ở trên bàn
và khối đó trong. Một khối trong nếu
khối đó không có khối nào khác ở trên đỉnh và nếu khối
đó không được nhặt lên.
Tôi chỉ có thể bỏ một khối từ trên đỉnh khối khác
nếu khối tôi đang bỏ thực sự ở trên đỉnh
khối kia.
Tôi chỉ có thể bỏ một khối từ trên đỉnh khối khác
nếu khối tôi đang bỏ trong.
Một khi tôi nhặt hoặc bỏ một khối, tôi đang giữ
khối đó.
Tôi chỉ có thể đặt xuống một khối mà tôi đang giữ.
Tôi chỉ có thể xếp một khối lên trên đỉnh khối khác nếu tôi
đang giữ khối được xếp.
Tôi chỉ có thể xếp một khối lên trên đỉnh khối khác nếu
khối mà tôi đang xếp khối lên trên đó trong.
Một khi tôi đặt xuống hoặc xếp một khối, tay tôi
trở nên trống.
