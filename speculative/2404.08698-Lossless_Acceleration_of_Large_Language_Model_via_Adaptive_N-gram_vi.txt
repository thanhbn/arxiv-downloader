# Gia tốc không mất mát cho Mô hình Ngôn ngữ Lớn thông qua Giải mã Song song N-gram Thích ứng

Jie Ou, Yueming Chen, Wenhong Tian∗
Đại học Khoa học và Công nghệ Điện tử Trung Quốc, Thành Đô, Trung Quốc
oujieww6@gmail.com, yuemingchen121@gmail.com
tian_wenhong@uestc.edu.cn

## Tóm tắt
Trong khi các Mô hình Ngôn ngữ Lớn (LLM) đã thể hiện khả năng đáng chú ý, chúng bị cản trở bởi việc tiêu thụ tài nguyên đáng kể và độ trễ đáng kể do quá trình tự hồi quy. Trong nghiên cứu này, chúng tôi giới thiệu Giải mã Song song N-gram Thích ứng (ANPD), một phương pháp sáng tạo và không mất mát giúp tăng tốc suy luận bằng cách cho phép tạo ra nhiều token đồng thời. ANPD kết hợp phương pháp hai giai đoạn: bắt đầu với giai đoạn nháp nhanh sử dụng mô-đun N-gram, thích ứng dựa trên bối cảnh tương tác hiện tại, sau đó là giai đoạn xác minh, trong đó LLM gốc đánh giá và xác nhận các token được đề xuất. Do đó, ANPD bảo toàn tính toàn vẹn của đầu ra gốc của LLM trong khi nâng cao tốc độ xử lý. Chúng tôi tiếp tục tận dụng kiến trúc đa cấp cho mô-đun N-gram để nâng cao độ chính xác của bản nháp ban đầu, từ đó giảm độ trễ suy luận. ANPD loại bỏ nhu cầu huấn luyện lại hoặc bộ nhớ GPU bổ sung, làm cho nó trở thành một cải tiến hiệu quả và có thể cắm và chạy. Trong các thí nghiệm của chúng tôi, các mô hình như LLaMA và các biến thể được tinh chỉnh của nó đã cho thấy cải thiện tốc độ lên đến 3,67×, xác thực hiệu quả của ANPD được đề xuất.

## 1 Giới thiệu
Sự ra đời của các Mô hình Ngôn ngữ Lớn (LLM) như GPT-4 (OpenAI, 2023), ChatGPT (Brown et al., 2020), LLaMA (Touvron et al., 2023a), và PaLM (Chowdhery et al., 2023), đã cách mạng hóa bối cảnh xử lý ngôn ngữ tự nhiên. Tuy nhiên, phần lớn các LLM (Touvron et al., 2023a; Anil et al., 2023; Bai et al., 2023) dựa vào kiến trúc Transformers chỉ có bộ giải mã (Alec et al., 2018), vốn có tính tự hồi quy và do đó dẫn đến thời gian tạo ra tăng lên trong quá trình suy luận. Đặc điểm này đã làm cho việc cải thiện hiệu quả suy luận LLM trở thành một lĩnh vực nghiên cứu quan trọng trong cộng đồng xử lý ngôn ngữ tự nhiên.

Các kỹ thuật nén mô hình như lượng tử hóa (Han et al., 2015), tỉa (Molchanov et al., 2016), và chưng cất (Hinton et al., 2015) đã được sử dụng để giảm bớt chi phí tính toán liên quan đến LLM. Gần đây, các phương pháp sáng tạo như chiến lược thoát sớm (Yang et al., 2023b; Bae et al., 2023; Kong et al., 2022; Schuster et al., 2022; Varshney et al., 2023) và giải mã suy đoán (Kim et al., 2023; Xia et al., 2022; Leviathan et al., 2023; Spector and Re, 2023; Zhang et al., 2023a) đã được đề xuất để tăng tốc quá trình suy luận. Trong khi các phương pháp này hiệu quả, chúng thường đòi hỏi sự thay đổi kiến trúc mô hình và huấn luyện lại, điều này có thể tốn chi phí đáng kể. Ngoài ra, chúng có thể thay đổi đầu ra của mô hình và yêu cầu nhu cầu bộ nhớ GPU bổ sung. Một phương pháp tránh mô hình nháp sử dụng truy xuất được trình bày trong (He et al., 2023), nhưng nó đòi hỏi một cơ sở dữ liệu lớn.

Đối với một số LLM nhất định, như LLaMA, quá trình token hóa có thể chia nhỏ một từ đơn thành nhiều token, từ đó làm trầm trọng thêm độ trễ suy luận. Như được minh họa trong Hình 1, số lượng token vượt quá số lượng từ, dẫn đến việc tăng số bước tạo ra tự hồi quy. Trong những tình huống như vậy, với các ràng buộc được áp đặt bởi thông tin bối cảnh, không gian tìm kiếm để dự đoán token tiếp theo tạo thành một phần của từ dựa trên token hiện tại bị thu hẹp đáng kể. Hơn nữa, thông tin bối cảnh thường có thể được tận dụng để xác định các mẫu và tương quan giữa các từ. Điều này đặc biệt rõ ràng đối với các cụm từ và đoạn văn đơn giản, nơi bối cảnh có thể cung cấp các chỉ báo rõ ràng giảm sự phụ thuộc vào việc giải mã LLM.

Dựa trên động lực trên, bài báo này trình bày một phương pháp mới, Giải mã Song song N-gram Thích ứng (ANPD), được thiết kế để nâng cao hiệu quả suy luận mà không cần thiết huấn luyện lại hoặc tích hợp mô hình ngôn ngữ nhỏ phụ trợ. ANPD tạo ra đầu ra nháp một cách động thông qua mô-đun N-gram thích ứng sử dụng thống kê thời gian thực, sau đó các bản nháp được xác minh bởi LLM. Đặc điểm này chính xác là sự khác biệt giữa ANPD và các phương pháp giải mã suy đoán trước đây. Các đóng góp chính của công trình này có thể được tóm tắt như sau:

• Chúng tôi đề xuất ANPD, một thuật toán mới và không mất mát cung cấp mô-đun cắm và chạy để tăng tốc suy luận LLM.

• Chúng tôi đề xuất chiến lược mô hình hóa N-gram thích ứng được điều chỉnh đặc biệt cho LLM, giảm đáng kể độ phức tạp của mô hình hóa ngôn ngữ và giảm sự phụ thuộc vào các tập dữ liệu văn bản quy mô lớn.

• Chúng tôi đề xuất thuật toán N-gram Đa cấp (MLN) nhằm tăng độ chính xác của đầu ra nháp, từ đó nâng cao hiệu quả của quá trình tăng tốc.

• Chúng tôi tiến hành các thí nghiệm rộng rãi trên nhiều mô hình và tập dữ liệu khác nhau, chứng minh khả năng tăng tốc mạnh mẽ của ANPD, với sự gia tăng đáng chú ý từ 1,95×-3,67× trên LLaMA và các phái sinh được tinh chỉnh của nó.

## 2 Công trình liên quan

**Hệ thống suy luận.** Việc phát triển các hệ thống suy luận chuyên biệt cho Mô hình Ngôn ngữ Lớn (LLM), như TensorRT-LLM của NVIDIA (NVIDIA, 2023), Orca (Yu et al., 2022), FlexGen (Sheng et al., 2023), và DeepSpeed Inference (Aminabadi et al., 2022), đại diện cho một tiến bộ đáng chú ý trong lĩnh vực này. Mặc dù có tiến bộ, vẫn còn khoảng cách trong việc đồng thiết kế cẩn thận các thuật toán và hệ thống, điều này cần thiết để khai thác đầy đủ tiềm năng của phần cứng.

**Nén.** Suy luận LLM hiệu quả được tạo điều kiện bởi các kỹ thuật như lượng tử hóa (Han et al., 2015; Frantar et al., 2022; Dettmers et al., 2022; Xiao et al., 2023), tỉa (Bansal et al., 2023; Frantar and Alistarh, 2023; Liu et al., 2023), chưng cất (Tang et al., 2019; Touvron et al., 2021), và các chiến lược thoát sớm (Schuster et al., 2022; Kong et al., 2022; Yang et al., 2023b; Bae et al., 2023; Del Corro et al., 2023) đề xuất rằng một số token có thể được tạo ra chính xác chỉ sử dụng một phần của các lớp mô hình. Tỉa Token (Hou et al., 2022; Yao et al., 2022; Zhang et al., 2023b) giảm nhu cầu bộ nhớ và tính toán để tăng tốc quá trình suy luận bằng cách ưu tiên các token quan trọng. Các phương pháp này nâng cao hiệu quả nhưng có thể đòi hỏi thay đổi mô hình, huấn luyện lại, và có khả năng giảm độ chính xác.

**Thực thi suy đoán.** Thực thi suy đoán (Burton, 1985), được điều chỉnh như giải mã suy đoán trong LLM (Chen et al., 2023; Leviathan et al., 2023), đã cải thiện tốc độ suy luận bằng cách ngăn chặn các tính toán. SpecInfer (Miao et al., 2023) tận dụng các biến thể đã chưng cất, lượng tử hóa và đã tỉa hiện có của LLM, để xây dựng một bể mô hình suy đoán nhỏ để hướng dẫn suy đoán. Tuy nhiên, các phương pháp này đòi hỏi mô hình nháp chất lượng cao, và tăng dấu chân bộ nhớ. Leviathan et al. (2023) cũng đề cập rằng unigram và bigram có thể được sử dụng như mô hình nháp, nhưng họ không đề xuất phương pháp về cách xây dựng mô hình bigram cho các LLM đang chạy thực tế. Yang et al. (2023a) đã trình bày một phương pháp sao chép token tham chiếu vào bộ giải mã, mặc dù tiện ích của nó bị hạn chế bởi sự phụ thuộc vào văn bản lặp lại. Các kỹ thuật này tăng việc sử dụng tài nguyên và buộc phải huấn luyện chuyên biệt, như chưng cất, cho mô hình nháp để đảm bảo tính tương thích với mô hình chính.

## 3 Phương pháp

Hình 2 minh họa khuôn khổ và quy trình làm việc của ANPD được đề xuất. Chúng tôi giải thích việc giải mã tự hồi quy gốc trong Phụ lục A.1.

### 3.1 Giải mã Song song N-gram Thích ứng

Hình 2 minh họa quy trình của ANPD. Quá trình bắt đầu với việc token hóa văn bản đầu vào thành các token. Bộ nhớ của mô-đun N-gram thực sự lưu trữ id token để hợp lý hóa quá trình xử lý, Hình 2 hiển thị các token làm cơ sở cho mô hình hóa để làm cho người đọc dễ hiểu hơn và cải thiện khả năng đọc. Tiếp theo, LLM tham gia vào suy luận tự hồi quy, được chia thành hai phần: 1. Prefill, nơi toàn bộ prompt được nhập để tạo ra token đầu tiên; 2. Giải mã, ANPD đưa nhiều token từ mô-đun N-gram vào LLM, và LLM sử dụng kv-cache cho các tính toán hiệu quả để xác thực token cho việc tạo ra đầu ra song song. Các token không vượt qua xác thực bị loại bỏ cùng với các token tiếp theo. Đồng thời, chúng tôi sử dụng chiến lược thích ứng để cập nhật mô-đun N-gram trong suốt quá trình tạo ra LLM, tránh sự phụ thuộc vào Bộ nhớ tĩnh.

**Mô-đun N-gram Cấp Token.** Thông tin bối cảnh rất quan trọng cho việc trích xuất nội dung, tóm tắt và tạo mã, vì nó giúp tinh chỉnh không gian tìm kiếm trong mỗi bước giải mã LLM. Điều này bao gồm các tương quan mạnh giữa các token trong từ và giữa các từ trong cụm từ và bối cảnh. Chúng tôi đã xây dựng mô-đun N-gram cấp token để mô hình hóa một cách thống nhất các tương quan trên.

Mô-đun N-gram là một mô hình ngôn ngữ xác suất, dự đoán mục tiếp theo trong một chuỗi sử dụng mô hình Markov bậc (N−1), trong đó N là độ dài chuỗi con. Đối với một chuỗi token x₁, x₂, ..., x_{t−1}, mô hình ước tính xác suất của x_t dựa trên N−1 token trước đó, như P(x_t|x₁, ..., x_{t−1}) ≈ P(x_t|x_{t−N+1}, ..., x_{t−1}). Trong mô hình bigram (N = 2), xác suất câu là:

P(x₁, x₂, ..., x_n) ≈ ∏_{i=2}^n P(x_i|x_{i−1}), (1)

xác suất P(x_i|x_{i−1}) được dẫn xuất từ số lần đếm tần suất trong kho ngữ liệu. Chúng tôi đã thiết kế mô-đun N-gram để bao gồm ba chức năng chính thiết yếu cho hoạt động của nó:

• **Initialize**: sử dụng tokenizer chuyển đổi mỗi prompt thành một chuỗi id token. Sau đó nó thực hiện thống kê xác suất trên các id này và ghi lại xác suất cho mỗi tuple token.

• **Update**: trong quá trình giải mã, mỗi token mới được ghép nối với N−1 token trước đó để tạo thành một tuple, được sử dụng để cập nhật Bộ nhớ xác suất của mô-đun.

• **Query**: hoạt động truy vấn sử dụng tuple id token, được xây dựng thông qua chuỗi con từ t−N+1 đến t−1, để dự đoán token tiếp theo x_t, hiệu quả tận dụng các kết quả thống kê được thiết lập bởi các chức năng trước đó.

Các chức năng này cùng nhau cho phép mô-đun N-gram thích ứng động với quá trình tạo văn bản đang phát triển, đảm bảo rằng mỗi token được tạo ra có liên quan về mặt bối cảnh và mạch lạc về mặt thống kê.

**Giải mã Song song.** Việc giải mã song song trong ANPD của chúng tôi tương tự như phương pháp giải mã suy đoán và xảy ra trong hai giai đoạn riêng biệt:

1. **Nháp**: mô-đun N-gram được tận dụng để tạo ra một chuỗi các token tiếp theo. Bằng cách lặp lại qua K bước, mô-đun xây dựng các token nháp sơ bộ với độ dài K. Cụ thể, mô-đun nháp tạo ra một chuỗi K token tạm thời x_{i+1}, ..., x_{i+K}, kế tiếp chuỗi prompt đã cho x₁, ..., x_i.

2. **Xác minh**: Mô hình Ngôn ngữ Lớn (LLM) gốc xác minh các token nháp được đề xuất, thông qua một lượt forward đơn lẻ như P(x'_{i+K+1}|(k, v)₁, ...,(k, v)_i, x_{i+1}, ..., x_{i+K}), trong đó LLM tính toán các phân phối xác suất cho mỗi token nháp, sau đó xác định sự phù hợp của chúng với các token nháp được đề xuất x_{i+1}, ..., x_{i+K}. Nếu một token nháp x_j không vượt qua xác thực này, nó được thay thế bằng dự đoán của LLM x'_j, và việc nháp mới bắt đầu từ token này.

ANPD nâng cao hiệu quả bằng cách loại bỏ nhu cầu về mô hình học sâu nháp nhỏ hơn, tận dụng mô-đun N-gram với chi phí tính toán thấp hơn nhiều để tăng tốc suy luận LLM. Đối với LLM, việc tiến hành suy luận song song của K token đưa ra mức tăng không đáng kể trong độ trễ tính toán so với suy luận tự hồi quy token đơn, như được hiển thị trong Hình 7 trong Phụ lục A.2. Trong khi đó, kỹ thuật của chúng tôi có khả năng nội tại tạo ra ít nhất j token (1 ≤ j ≤ K + 1) cho mỗi bước giải mã, khả năng nội tại này về cơ bản đảm bảo, về nguyên tắc, một sự tăng tốc của các quá trình giải mã trong Mô hình Ngôn ngữ Lớn (LLM), từ đó nâng cao thông lượng tính toán tổng thể và giảm độ trễ. Việc thực hiện quá trình hai giai đoạn trao cho ANPD khả năng tinh chỉnh lặp lại đầu ra nháp. Hơn nữa, điều này đảm bảo rằng phương pháp ANPD của chúng tôi là không mất mát, duy trì tính nhất quán với nội dung được tạo ra bởi LLM gốc. Quy trình chi tiết của ANPD được trình bày trong Thuật toán 1, với giải thích toàn diện có sẵn trong Phụ lục ??.

### 3.2 N-gram Đa cấp

Độ chính xác dự đoán của mô-đun N-gram được biết là có tương quan với N, các giá trị N lớn hơn thường dẫn đến dự đoán nội dung chính xác hơn. Hiệu ứng này đặc biệt đáng chú ý trong các thiết lập với bối cảnh dài hơn của các tác vụ Mô hình Ngôn ngữ (LM), nơi việc tăng N có thể giảm đáng kể tần suất lỗi dự đoán.

Trong khi N lớn hơn có xu hướng cải thiện độ chính xác dự đoán của mô-đun N-gram, nó có thể không phải lúc nào cũng dẫn đến một kết quả khớp thành công trong hoạt động Query. Để giải quyết điều này, chúng tôi đề xuất phương pháp N-gram Đa cấp (MLN), dựa trên khớp tiền tố tối ưu. Thiết kế MLN khởi tạo N−1 mô-đun riêng biệt, mỗi mô-đun tương ứng với một mô-đun n-gram (n ∈ [2, N]). Trong quá trình dự đoán, truy vấn bắt đầu với N lớn nhất và tiến tới các cấp n thấp hơn, dừng lại khi tìm thấy kết quả khớp thành công như được hiển thị trong Thuật toán 2.

## 4 Thí nghiệm

### 4.1 Chi tiết Triển khai

Chúng tôi đã chọn một loạt mô hình đa dạng, khác nhau về quy mô, thiết kế kiến trúc và phương pháp huấn luyện, để đảm bảo đánh giá toàn diện, bao gồm LLaMA-7B (Touvron et al., 2023a), LLaMA-2-7B (Touvron et al., 2023b), ChatGLM3-6B (Du et al., 2022), LLaMA-2-13B, CodeLLaMA-7B (Roziere et al., 2023), CodeLLaMA-13B, và các biến thể được tinh chỉnh hướng dẫn như Alpaca-7B và Alpaca-CNN/DM-7B, chi tiết tinh chỉnh được cung cấp trong Phụ lục A.4. Chúng tôi sử dụng một GPU RTX-3090 cho tất cả các mô hình 7B, trong khi các mô hình 13B lớn hơn cần thiết bốn GPU RTX-3090 và thư viện accelerate.

### 4.2 Tập dữ liệu & Chỉ số

Để xác thực hiệu quả của phương pháp của chúng tôi trong việc tăng tốc tạo văn bản cho LLM, chúng tôi tập trung vào hai tác vụ: tóm tắt văn bản và tạo mã, sử dụng các tập dữ liệu như CNN/Daily Mail (CNN/DM) (Hermann et al., 2015), Extreme Summarization (XSum) (Narayan et al., 2018), và HumanEval (Chen et al., 2021). Để biết thêm chi tiết về cài đặt đánh giá, vui lòng xem Phụ lục A.5. Chúng tôi sử dụng tỷ lệ tăng tốc làm chỉ số đánh giá, được tính bằng cách chia thời gian suy luận của quá trình tự hồi quy cho thời gian suy luận của quá trình ANPD, dưới điều kiện giống hệt nhau trên tất cả các mẫu (Đối với các tác vụ tóm tắt, chúng tôi sử dụng kích thước mẫu 1000 để đảm bảo ý nghĩa thống kê, như được khuyến nghị bởi (Zhang et al., 2023a)). Chỉ số này một cách trực quan chứng minh cải thiện hiệu suất về tốc độ khi sử dụng thuật toán ANPD.

### 4.3 Kết quả Chính

Trong Bảng 1, chúng tôi trình bày phân tích so sánh nêu rõ lợi ích tăng tốc cho các mô hình và tập dữ liệu khác nhau. Chúng tôi đã chọn (Zhang et al., 2023a) để so sánh. Không chỉ các tập dữ liệu và mô hình thí nghiệm của họ phù hợp với của chúng tôi, mà phương pháp luận của họ cũng được mở nguồn để tạo điều kiện dễ dàng sao chép. Các prompt được sử dụng với những mô hình này được ghi chép toàn diện trong Phụ lục A.5 để tạo điều kiện kiểm tra thêm và đảm bảo khả năng tái tạo của các kết quả được báo cáo trong bài báo này.

Như được minh họa trong Bảng 1, thuật toán ANPD liên tục tăng tốc suy luận trên các mô hình khác nhau, bao gồm LLM cơ sở, Alpaca được tinh chỉnh hướng dẫn, và mô hình được tinh chỉnh với hướng dẫn cụ thể tập dữ liệu, cho thấy tính mạnh mẽ và hiệu quả của nó trong việc tăng tốc tạo văn bản. Đáng chú ý, đối với mô hình LLaMA-7B, ANPD có thể tăng tốc độ suy luận hơn 2,0×, điều này vẫn có hiệu lực trên LLaMA2. Phương pháp của chúng tôi đạt được mức tăng gấp đôi (2.9088× so với 1.3293×) về tăng tốc so với (Zhang et al., 2023a) trên LLaMA-2-13B. Mặc dù mô hình ChatGLM3 có từ vựng lớn hơn đáng kể (gần gấp đôi so với LLaMA, tỷ lệ token/từ sẽ gần hơn với 1), thuật toán ANPD của chúng tôi vẫn đạt được tăng tốc 1.7046× và 1.6647× cho CNN/DM và XSum, tương ứng. Trong ChatGLM3, cơ chế dự đoán của ANPD chủ yếu tận dụng các mối quan hệ liên kết giữa các cụm từ và từ riêng lẻ, thay vì tham gia vào dự đoán cấp token trong chính các từ. Vì vậy, ANPD duy trì tính mạnh mẽ và liên tục nâng cao tốc độ suy luận trên các LLM đa dạng. Do sự hiện diện của tần suất cao các mẫu tương quan trong các tác vụ viết mã, điều này đã nâng cao đáng kể độ chính xác dự đoán của thuật toán ANPD. Thuật toán ANPD đã có thể đạt được tăng tốc đáng kể 3.6665× trên HumanEval, nhưng (Zhang et al., 2023a) chỉ có tăng tốc 1.6758× cho CodeLLaMA-13B.

### 4.4 Nghiên cứu Loại bỏ

Chúng tôi tiến hành phân tích các siêu tham số trên tập dữ liệu CNN/DM, tập trung chủ yếu vào K và N. Trong Hình 3, chúng tôi đặt N là 2, và thực hiện phân tích so sánh tham số K. Các phát hiện của chúng tôi chỉ ra rằng việc tăng K góp phần vào hiệu ứng tăng tốc lớn hơn, tuy nhiên, lợi ích tăng tốc đạt đến mức ổn định khi K nằm trong khoảng từ 6 đến 8.

Dựa trên thí nghiệm trong Hình 3, chúng tôi đã chọn 6, 7 và 8 cho K để tiến hành các thí nghiệm kết hợp siêu tham số thêm, như được minh họa trong Hình 4 và 5. Kết quả thí nghiệm chỉ ra rằng phương pháp N-gram Đa cấp (MLN) nâng cao tốc độ suy luận khi tham số N tăng. Tuy nhiên, vượt quá N = 5, việc tăng thêm N không mang lại lợi ích đáng kể nào. Ngoài ra, hiệu ứng của tham số K trên tăng tốc tương đối ổn định; như được hiển thị trong Hình 3, hiệu ứng tăng tốc đạt đến mức ổn định trong khoảng từ 6 đến 8 cho K. Những phát hiện này nhất quán trên các mô hình khác nhau với N khác nhau.

Dựa trên bằng chứng thực nghiệm được trình bày trong Hình 4 và Hình 5, một lựa chọn thực dụng cho N và K có thể được đặt ra ở N = 5 và K = 7 tương ứng. Các thí nghiệm tương tự liên quan đến tập dữ liệu HumanEval đã được đưa vào Phụ lục A.6 để tham khảo, những kết luận tương tự cũng có thể được quan sát trong tập dữ liệu này. Trong khi việc sử dụng N-gram Đa cấp (MLN) đã cải thiện độ chính xác của dự đoán nháp, chúng tôi cũng đã thực hiện các thí nghiệm riêng biệt (Hình 10, Phụ lục A.6) sử dụng các mô-đun N-gram không có MLN, để chứng minh rằng đơn giản việc mở rộng giá trị của N là không hiệu quả.

### 4.5 Nghiên cứu Trường hợp

Hình 6 trình bày một ví dụ chi tiết về quy trình suy luận ANPD, sử dụng mô hình Alpaca-7B trên một mẫu từ tập kiểm tra CNN/DM. Mô hình Alpaca-7B, đã được tinh chỉnh với hướng dẫn, được chọn do tính ứng dụng rộng rãi trong các tình huống thực tế. Trong ví dụ này, thuật toán ANPD được cấu hình với N = 5 và K = 7, đạt được tăng tốc giải mã 2.19× so với quá trình tự hồi quy gốc, với tỷ lệ vượt qua văn bản nháp (Tỷ lệ trúng nháp, α) là 20.59% trong giai đoạn xác minh LLM. Dựa trên tỷ lệ trúng, chúng ta có thể dẫn xuất giới hạn trên lý thuyết của tăng tốc là (α×K) + 1, chúng ta có thể tính toán rằng tăng tốc lý thuyết là 2.44, vì tổn thất do các vấn đề triển khai sẽ cao hơn một chút so với tỷ lệ tăng tốc thực tế. Hình 6 sử dụng gạch chân màu đỏ để đại diện cho một bước giải mã, bao gồm nháp và xác minh, với nền màu vàng chỉ ra sự bắt đầu của một bước. Nền màu xanh nhạt và xanh lá cây đánh dấu nội dung nháp đã vượt qua xác minh. Ví dụ này chứng minh rằng tăng tốc suy luận chủ yếu được hưởng lợi từ sự kết hợp của tên (ví dụ: _Athlet, ic, _Bil, ba, o), từ một phần (ví dụ: _har, sh, ly), và cụm từ (ví dụ: _reduced, _to), phù hợp với động lực đằng sau thuật toán ANPD. ANPD có thể nhanh chóng nắm bắt sự liên kết giữa token và từ dựa trên thông tin này, và thiết lập mô hình dự đoán, từ đó tăng tốc quá trình giải mã từ đầu đến cuối.

### 4.6 Thân thiện với Người dùng

Vì ANPD không liên quan đến các mô hình học sâu bổ sung hoặc cơ sở dữ liệu cắm thêm, nó không đòi hỏi các quy trình khởi tạo phức tạp và cài đặt cấu hình môi trường. Do đó, người dùng có thể sử dụng nó trực tiếp và với sự tiện lợi lớn, như được minh họa trong Listing 1. Chúng tôi dự định phát hành các gói phần mềm mã nguồn mở liên quan trên GitHub, làm cho chúng có thể truy cập được cho mọi người sử dụng và đóng góp.

## 5 Kết luận

Trong bài báo này, chúng tôi đã trình bày thuật toán ANPD, một phương pháp mới và không mất mát để tăng tốc suy luận Mô hình Ngôn ngữ Lớn (LLM). Thuật toán này triển khai chiến lược mô hình hóa N-gram thích ứng, giảm nhu cầu về kho ngữ liệu lớn và loại bỏ yêu cầu xây dựng mô hình ngôn ngữ nháp học sâu bổ sung. Chiến lược N-gram Đa cấp (MLN) không chỉ nâng cao độ chính xác đầu ra nháp mà còn tiếp tục tăng hiệu quả. Các nghiên cứu thực nghiệm của chúng tôi trên nhiều mô hình và tập dữ liệu khác nhau xác thực hiệu quả của thuật toán ANPD, với mức tăng tốc đỉnh đáng chú ý lên đến 3.67× đạt được. Thuật toán ANPD đã chứng minh sức mạnh của nó như một công cụ mạnh mẽ để nâng cao hiệu quả của LLM. Như một mô-đun cắm và chạy, nó cho phép sử dụng rộng rãi và thực dụng hơn của LLM trong nhiều bối cảnh thế giới thực khác nhau.

**Công việc Tương lai.** Chúng tôi tin rằng ANPD có thể được nâng cao thêm trong hai khía cạnh chính:

1. Kết hợp các đặc điểm cụ thể của từng LLM riêng lẻ (ví dụ: LLaMA, ChatGLM) bằng cách tạo ra các tính năng được điều chỉnh cho các LLM khác nhau để tiếp tục tăng tốc hiệu suất suy luận.

2. Khám phá khả năng tạo ra nhiều token song song trong quá trình xác minh LLM để tiếp tục tăng tốc hiệu suất suy luận.

## 6 Lời cảm ơn

Nghiên cứu này được hỗ trợ bởi Chương trình Nghiên cứu và Phát triển Trọng điểm Quốc gia Trung Quốc với Mã Tài trợ 2018AAA0103203 và Dự án Khoa học và Công nghệ Thành Đô với Mã Tài trợ 2022-YF05-02014-SN. Nghiên cứu này cũng được hỗ trợ bởi Đội MindSpore Huawei để cung cấp hỗ trợ kỹ thuật và chia sẻ kinh nghiệm.
