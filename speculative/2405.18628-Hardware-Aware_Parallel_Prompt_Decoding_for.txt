# 2405.18628.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/speculative/2405.18628.pdf
# File size: 3431242 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Hardware-Aware Parallel Prompt Decoding for
Memory-Efficient Acceleration of LLM Inference
Hao (Mark) Chen1Wayne Luk1Ka Fai Cedric Yiu2
Rui Li3Konstantin Mishchenko3Stylianos I. Venieris3Hongxiang Fan1,3
1Imperial College London, UK
2Hong Kong Polytechnic University, Hong Kong
3Samsung AI Center, Cambridge, UK
{hc1620,w.luk}@ic.ac.uk {rui.li,s.venieris}@samsung.com
konsta.mish@gmail.com cedric.yiu@polyu.edu.hk hongxiangfan@ieee.org
Abstract
The auto-regressive decoding of Large Language Models (LLMs) results in signifi-
cant overheads in their hardware performance. While recent research has investi-
gated various speculative decoding techniques for multi-token generation, these
efforts have primarily focused on improving processing speed such as throughput.
Crucially, they often neglect other metrics essential for real-life deployments, such
as memory consumption and training cost. To overcome these limitations, we
propose a novel parallel prompt decoding that requires only 0.0002 % trainable pa-
rameters, enabling efficient training on a single A100-40GB GPU in just 16 hours.
Inspired by the human natural language generation process, PPD approximates out-
puts generated at future timesteps in parallel by using multiple prompt tokens. This
approach partially recovers the missing conditional dependency information neces-
sary for multi-token generation, resulting in up to a 28% higher acceptance rate for
long-range predictions. Furthermore, we present a hardware-aware dynamic sparse
tree technique that adaptively optimizes this decoding scheme to fully leverage
the computational capacities on different GPUs. Through extensive experiments
across LLMs ranging from MobileLlama to Vicuna-13B on a wide range of bench-
marks, our approach demonstrates up to 2.49 ×speedup and maintains a minimal
runtime memory overhead of just 0.0004% . More importantly, our parallel prompt
decoding can serve as an orthogonal optimization for synergistic integration with
existing speculative decoding, showing up to 1.22×further speed improvement.
Our code is available at https://github.com/hmarkc/parallel-prompt-decoding.
1 Introduction
0.0004% memory overhead 16-hour GPU training
2.24X speedup 
Figure 1: Comparison of memory, speedup,
and training cost on MT-Bench with Vicuna-
7B. Circle diameter shows training GPU hours.The recent advances in large language models
(LLMs) are increasingly gaining influence across
various AI applications. However, autoregressive
generation, the de facto approach employed in LLM
inference, suffers from inadequate hardware per-
formance due to its inherent sequential nature [ 23].
Speculative decoding [ 13,2,11], an emerging ac-
celeration technique, employs a guess-and-verify
framework for LLM inference, where a smaller draft
model first predicts multiple tokens sequentially and
then the original LLM verifies them in parallel. De-
spite its potential, the effectiveness of speculative
Preprint. Under review.arXiv:2405.18628v2  [cs.LG]  2 Jun 2024

--- PAGE 2 ---
Embedding Layer
Transformer Blocks
Prompt Tokens
Train
FrozenLLMEfficient Training:0.0002% parameters16 GPU hours
Hardware-Aware Dynamic ApproachLLMABCDXGuess PhaseS1S2
Guess (Candidate) tokenPrompt tokenAccepted tokenLLMCVerify Phase (+ Next Guess)S1S2CS1S2CS1S2DDXDEFEYFZVG
Candidate 3Candidate 2Candidate 1
Y != X, rejectedGuess tokens match with generated token acceptTrainingInference SchemeDeploymentP=0.7P=0.1P=0.2P=0.3P=0.6P=0.1Prompt TokenBudget: 4
Prompt TokenBudget: 2P=0.3P=0.5P=0.2Prompt TokenBudget: 3
Prompt Tokens Parallel In
Predicts Parallel Out
Merge 
NeurIPS is one of the best conferences with respect to advancements in AI.Human
Inspire(Parallel Generation) (Common Expr.) Figure 2: Overview of PPD . The left section shows the location of trainable parameters and the
middle section displays the combined guess-and-verify process during inference. The "prompt
token" denotes the special token with separately trained embeddings to perform parallel prediction.
decoding is limited by the complexity and cost of training a draft model capable of consistently
achieving high acceptance rates across diverse base models and datasets. Additionally, the extra run-
time memory overhead for executing draft models poses a significant barrier to the broader adoption
of speculative decoding, particularly in edge and mobile environments where memory capacity is
limited. Considering the growing need for user privacy and personalization, deploying LLMs on
devices urges a more memory- and cost-efficient solution for accelerating LLM inference. Recent
efforts have explored the possibility of generating multiple tokens in parallel without relying on a
separate transformer draft model [ 20]. Approaches such as inserting additional decoding heads [ 1]
and retrieving frequently used tokens [ 9] are employed to enhance performance. However, these
methods either aggressively assume conditional independence among the tokens generated in a single
step [ 1,9], or use placeholder tokens ( e.g., [PAD] token) that do not convey enough contextual
information [ 20]. Therefore, they often suffer from low acceptance rates or degradation in output
quality due to the lack of sufficient conditional information during inference.
To alleviate the complexity and overhead associated with the use of draft models while maintaining a
high acceptance rate, we propose Parallel Prompt Decoding (PPD ), a novel architecture-agnostic
and memory-efficient framework that adopts prompt tuning for non-autoregressive LLM inference.
Inspired by the human natural language generation process where continuous words like common
expressions and phrases are produced simultaneously, PPD introduces the use of prompt tokens,
the meticulously trained embeddings, for multi-token prediction. Specifically, these trained prompt
tokens are appended to the original input sequence in parallel, enabling the concurrent generation
of multiple output tokens in a single forward pass. The key intuition of PPD lies in the observation
that if trained properly, prompt tokens appended to the input can approximate tokens generated
at future timesteps, thereby partially recovering the missing conditional dependency information
for multi-token generation. By strategically positioning trained prompt tokens, PPD achieves up
to a 28% higher acceptance rate when predicting long-range tokens. To further increase the token
acceptance rate, we generate multiple candidate continuations with each prompt token and use them
in combination with a customized tree attention mask to minimize the computation and memory
overhead. The capability of PPD to use low-cost prompt tokens for accurate multi-token prediction
forms the foundation for accelerating LLM inference. As shown in Figure 1, PPD achieves a
comparable speedup to the state-of-the-art speculative decoding approaches with negligible memory
overhead and reduced training cost. Moreover, to facilitate the optimized implementation of PPD
across different hardware platforms, we propose a hardware-aware dynamic sparse tree technique
that adaptively refines the prompt structure during runtime based on the computational resources
available on the specific hardware.
To demonstrate the effectiveness of our approach, we evaluate PPD on MobileLLaMA [ 6], Vicuna-7b
and Vicuna-13b [ 5]. Running on a single GPU using the A100-40GB and RTX 4090, our method
achieves a speedup ratio for inference from 2.12×to2.49×across a diverse range of popular
datasets including MT-Bench, HumanEval, and GSM8K. Our experiments demonstrate that PPD
not only achieves comparable throughput to the state-of-the-art speculative decoding method, but it
also manages this with significantly fewer trainable parameters—specifically, 0.0002% of trainable
parameters—and incurs only a minimal memory overhead ( 0.0004% ), showcasing that PPD is
remarkably cost- and memory-efficient. The training of prompt tokens can be completed in 16 hours
2

--- PAGE 3 ---
using one A100 GPU, 8 hours using four GeForce RTX 3090 GPUs, compared to the 1-2 days on
four A100 GPUs required for Eagle [ 16]. Furthermore, since PPD does not require the modification
of the original LLM or the addition of extra networks, it is highly adaptable and orthogonal to other
decoding techniques. For instance, it can be effectively combined with a draft model to further reduce
inference latency.
Our contributions are summarized as follows:
•A novel Parallel Prompt Decoding (PPD ) that adopts cost-effective prompt tokens for
non-autoregressive LLM inference, achieving a high acceptance rate for long-distance token
prediction with preserved output quality.
•A hardware-aware dynamic sparse tree technique that adaptively optimizes the prompt struc-
ture of PPD at runtime based on the available compute and memory resources, facilitating
its efficient deployment on various hardware platforms.
•An open-source implementation of PPD , accompanied by comprehensive evaluations on
various models and benchmarks. Our experiments demonstrate that PPD achieves significant
speed improvements with negligible memory overhead and reduced training cost.
2 Background and Related Work
To enhance the inference speed of LLM, various approaches adopt an iterative guess-and-verify
strategy to enable multi-token generation. In the guessing phase, potential future tokens are proposed
at a faster speed than in traditional autoregressive implementations. Subsequently, a parallelized
verification process assesses which guessed tokens should be accepted. Depending on how tokens
are generated during the guess stage, these approaches can generally be categorized as i)speculative
decoding and ii)parallel decoding.
2.1 Speculative Decoding
The guessing phase of speculative decoding adopts a lightweight draft model to generate multiple
tokens at an increased speed [ 11]. During the verification stage, the original LLM subsequently
determines the acceptance of the guessed tokens. It is worth noting that both draft and original models
still follow the auto-regressive inference scheme. The speedup comes from two factors: i)the draft
model runs much faster than the original model and more tokens can be generated within the same
time unit; and ii)token verification is executed concurrently, either by batching or by incorporating
multiple candidates into a single input using customized sparse attention masks [ 18]. Therefore, the
overall speedup depends on the acceptance rate and the inference latency of draft models.
Building on the speculative decoding scheme, various studies have been conducted to further optimize
its inference speed. To improve the accuracy of the draft model and its token acceptance rate,
Eagle [16] incorporates the hidden features into the draft model’s forward pass. SpecInfer [18] adopts
a tree-based speculative inference and verification scheme, improving the diversity of speculation
candidates. Sequoia [4] optimizes the sparse tree structure by considering the capability of the
underlying hardware platforms. However, most of these methods require the storage and maintenance
of a separate draft model. Moreover, there is extra complexity in designing an efficient draft model.
2.2 Parallel Decoding
To overcome the inherent limitations of autoregressive inference and the memory overhead associated
with using a separate draft model, several attempts have been made to integrate both guessing and
verification using one unified model. Medusa1[1] introduces language model (LM) heads at the final
layer of the original LLM, facilitating the generation of multiple tokens in a single forward pass. It
also utilizes tree attention masks in its verification process to increase speed even further. To enhance
token drafting with retrieval-augmented generation [ 10],Rest [9] introduce retrieval-based decoding
tailored for specific scenarios. Inspired by Jacobi decoding [ 20] that adopts multiple special tokens to
accelerate machine translation, Lookahead Decoding [8] improves upon this method by generating
parallel n-grams and employing a caching memory pool. To capture more information while using
1We categorize Medusa as parallel decoding because it only adopts LM heads instead of separate models.
3

--- PAGE 4 ---
multiple special tokens at distinct positions, PaSS [19] trains additional tokens with embedding layers
for parallel decoding. Hierarchical parallel decoding [ 17] introduces the use of [Fork ]and[Join]
tokens, enabling parallel execution of multiple structural subroutines.
Our approach can be categorized as parallel decoding, with three novel features to distinguish it from
other approaches: 1) PPD trains the embeddings of parameterized ensemble prompt tokens, 2)it
utilizes a dynamic sparse tree, adapting its structure at every inference step, and 3)we propose a
hardware-aware algorithm for designing a dynamic sparse tree tailored to each hardware platform.
3 Parallel Prompt Decoding ( PPD )
PPD trains embeddings for prompt tokens rather than developing a separate model. Our method
integrates three substeps into a single decoding step, following the guess-and-verify strategy: (1) can-
didate generation , where multiple candidate continuations2are predicted by strategically inserting
the prompt tokens into the input sequence. Tree attention [ 18] merges the processing of multiple
candidates into a single forward pass; (2) candidate verification , where two verification schemes,
exact matching [ 8] and typical acceptance [ 1], are implemented; (3) candidate acceptance , where
validated candidates are integrated into the input and KV cache is updated accordingly. The inference
scheme in Figure 2 illustrates the generation and verification combined in a single forward pass.
3.1 Prompt Tokens
The prompt tokens are the key component of PPD to realize multi-token generation. Initially
introduced in [ 12] to adapt LLMs for specific tasks, prompt tokens are typically prepended to the
input, with outputs generated in an autoregressive manner. In this work, we propose a novel approach
of utilizing prompt tokens by strategically positioning them at locations where tokens are anticipated
to be generated in parallel. For conventional parallel decoding techniques [ 23,1] that presume
complete conditional independence among tokens decoded in a single step, the exact conditional
probability p(yi+k+1|x, y 1:i+k)is approximated by pθ(yi+k+1|x, y 1:i), where xis the input, y1:i+k
represents the generated outputs so far, k >0indicates the token distance.3However, we observe
that as kincreases, the gap between the actual probability and its approximation expands, primarily
due to the absence of relevant conditional dependencies. We argue that prompt tokens can bridge this
gap by more accurately modeling the conditional probability as pθ(yi+k+1|x, y 1:i, ti+1:i+k)where
tiis the prompt token with token distance i. Through this forward pass in the decoder layers, these
causally linked prompt tokens facilitate the flow of information along the sequence of speculative
tokens, thus restoring the conditional probability.
3.2 Ensemble Prompt Tokens
Inspired by prompt ensembling [ 12], which uses multiple prompts to generate diverse responses
and aggregates these to derive a single answer, we introduce the concept of ensemble prompt
token (EPT). This additional abstraction allows us to decouple each prompt token from the fixed
embedding dimension. For every prompt token, there exist multiple corresponding EPTs, each with
its distinct embedding. We modify the attention mask to ensure that each nthEPT only depends on
the corresponding nthEPTs from preceding prompt tokens. This selective visibility is maintained for
both training and inference, where the guess token for each prompt token is determined by averaging
the logits of its EPTs. The use of EPTs not only enables direct and flexible control over the trainable
parameters, but also leads to an increase in prediction accuracy. The probability is approximated as
1
nPn
j=1pθ(yi+k+1|x, y 1:i, vj
i+1:i+k), where vj
i+mdenotes the jthEPT at a token distance of m.4
3.3 Training
During training, only the embeddings of prompt tokens are changed, with the parameters of the
original LLM remaining frozen. We adopt the following two training techniques:
2A candidate token, also referred to as a "guess token", is a draft token generated from a prompt token.
3The token distance is the number of tokens between the last accepted token and the predicted token.
4Further details about EPTs can be found in Appendix B.
4

--- PAGE 5 ---
GenerateMaskDynamic Sparse TreeAttention Mask for max depth=2
A
B
b
C
D
cFull Sparse Tree
A
0
B
b
1
C
3
2
D
c
4
7
5
6
8max depth=2
A
0
B
b
1
3
2
4
7max depth=1
Accepted TokenGuess (Candidate) TokenPrompt tokenPrune
Figure 3: Dynamic sparse tree.
Random Insertion of Prompt Tokens: Randomly inserting prompt tokens throughout the input
sequence reduces contextual bias from appending them only at the end. This approach broadens the
predictive capacity of prompt tokens beyond a limited vocabulary such as <eos> and punctuation.
Knowledge Distillation: To align the predictive behavior of prompt tokens with the original LLM,
we employ knowledge distillation. Instead of using hard labels, prompt tokens are trained against the
logits produced by the original LLM. Following Medusa [1], The loss function is formulated as:
LPD=1
NNX
i=1DKL(Pi∥Qi)·αi−1, (1)
where DKLis the KL divergence, Piis the predicted distribution of the ithprompt token, Qiis the
corresponding distribution from the original LLM, and αis the decay ratio.
4 Dynamic Sparse Tree
4.1 Motivation
To achieve higher speedup, PPD utilizes a specialized tree attention [ 1,18] to process multiple
candidates within a single decoding step without expanding the batch size. Notably, PPD employs
a sparse tree [ 1,4], designed to prioritize candidates with higher prediction accuracy. One key
distinction from the sparse tree used in previous works is the appending of a sequence of prompt
tokens to each tree node as shown in Figure 3. To optimize the amortized acceptance length across
decoding steps, it is crucial to carefully balance the number of candidate tokens and prompt tokens.
Instead of appending a uniform number of prompt tokens to every candidate token, we allocate them
based on each candidate’s probability, causing the tree’s maximum depth and structure to vary at each
decoding step.
4.2 Construction Algorithm
We aim to construct a dynamic sparse tree that maximizes the amortized number of tokens generated
with limited candidate tokens and prompt tokens. We first define the tree construction algorithm as a
constrained optimization problem.
Definition 4.1. Letmbe the maximal number of prompt tokens per tree node. The dynamic sparse
treeTcan exist in mstates, each represented by Tkcorresponding to state sk, where 1≤k≤m.
LetC(Tk)denote the subtree of Tkcomposed solely of candidate tokens. The maximum depth of
C(Tk)isk.
Proposition 4.1. For a dynamic sparse tree state Tk, where each candidate token vfollows a path
Path(v)from the root, and the acceptance probability pkat each path position k, the expected
number of tokens f(Tk)generated is given by f(Tk) =P
v∈C(Tk)Q
i∈Path(v)pi, whereQ
i∈Path(v)pi
represents the contribution of a token vto the expected number of tokens.
We then propose an approximation of the amortized number of tokens generated, by considering the
tokens generated at the current and the next decoding step.
5

--- PAGE 6 ---
Proposition 4.2. The expected total number of tokens F(Tk)generated for the dy-
namic sparse tree state F(Tk)at the current and the next decoding step is given by
F(Tk) =f(Tk) +Pm
i=1p(si|sk)f(Ti), where p(si|sk)represents the state transition probability
from state skto state si.
We are now ready to introduce Proposition 4.3, which we use in the pruning algorithm.
Proposition 4.3. For a dynamic sparse tree state Tkwith candidate subtree ck=C(Tk), the change
in expected total tokens F(Tk)due to the removal of a prompt token at candidate token cis given by
∆F=p(c)·(f(Ti)−f(Ti−1)), where p(c)is the acceptance probability of candidate c,idenotes
the number of prompt tokens prior to removal. We assume that i >1.
To construct an approximately optimal dynamic sparse tree with specified numbers of candidate
and prompt tokens, the process includes: (1) Optimal Candidate Trees: Constructing trees using
only candidate tokens at varying depths, employing the algorithm from Medusa [ 1] and Sequoia [ 4]
to maximize f(Tk)as stated in Proposition 4.1; (2) Appending Prompt Tokens: Attaching the
maximum allowable prompt tokens to each candidate token from the first step; (3) Greedy Prompt
Token Removal: Removing prompt tokens greedily to minimize ∆F(Proposition 4.3), continuing
until the desired prompt token count is reached.
We now introduce the formulation of the real amortized number of tokens generated.
Proposition 4.4. The amortized number of tokens R(Tk)generated for the dynamic sparse tree state
F(Tk)is given by R(T) =Pm
i=1p(si)f(Ti), where p(si)is the steady-state probability of state si,
andfis the function defined in Proposition 4.1.
Hardware-awareness. All the probabilities used above can be approximated on a validation dataset.
The dynamic sparse tree construction algorithm can now be formulated as finding the dynamic sparse
treeTwithnccandidate tokens and npprompt tokens to maximize R(T):
c(nc, np) = max
T,|C(T)|=nc,|T|=nc+npR(T).
For a fixed tree size n, we explore all combinations of ncandnpwhere n=nc+np, to identify the
dynamic sparse tree that maximizes R(Tk). To determine the optimal tree size n, we define two key
functions: 1. Acceptance length τ(n)(hardware-independent) and 2. Forward pass latency Lfp(n)
(hardware-dependent). The speedup ratio, Speedup (n) =τ(n)
Lfp(n), is estimated using a validation
dataset, with τ(n)evaluated once and Lfp(n)tested on different hardware types. We aim to find the
value of nthat maximizes Speedup (n).
5 Experiments
Models and testbeds . We conducted all the experiments using MobileLLaMA-1.4B [ 6], Vicuna-
7B and Vicuna-13B [ 5]. We used 3 prompt tokens and 1 EPT per prompt token for all inference
experiments. The inference throughputs of the models are evaluated on a single NVIDIA A100 GPU
with 40GB of memory and a GeForce RTX 4090 using a batch size of 1 and FP16 precision. Further
details about the experimental setup can be found in Appendix C.
Training . We froze all trainable parameters of the original LLM. Prompt token embeddings were
trained using distillation logits generated from the ShareGPT dataset [ 22], with a maximum context
length of 1024, a cosine learning rate scheduler starting at 0.01, and no warmup. Prompt token
embeddings are initialized with normal text token embeddings.
Datasets . We assess the throughput performance of PPD across various tasks and datasets. Specifi-
cally, we evaluated PPD using the MT-Bench dataset [ 25], which contains multi-turn questions with
a range of topics, in both non-greedy (temperature follows the default configuration) and greedy
settings (temperature=0). We used the GSM8K [ 7] and HumanEval [ 3] datasets only in the greedy
setting. The GSM8K dataset consists of grade school math problems and we used the first 500
questions of the test split for our evaluations. HumanEval includes coding tasks, for which we set
a maximum new token limit of 512 to control the length of the generated sequences. We used the
Alpaca [ 15] dataset as the validation dataset to produce the latencies and acceptance lengths used for
dynamic sparse tree construction.
6

--- PAGE 7 ---
Model Method T τ L fp(s) Quality Ptr(%) Str Sinput
MVanilla 50.2 1.00 0.020 - NA NA 1
PPD 108.7 2.43 0.022 Same 4.50e−4(10,84,89) (40,285,285)
V-7BVanilla 39.2 1.00 0.026 5.99 NA NA 1
Medusa 82.0 2.51 0.0307 5.98 8.07 63 63
PPD 88.0 2.54 0.029 5.93 1.82e−4(10,33,34) (40,105,105)
V-13BVanilla 30.4 1.00 0.0330 6.38 NA NA 1
Medusa 63.4 2.59 0.0408 - 5.52 63 63
PPD 66.1 2.44 0.0379 6.32 7.87e−5(10,20,20) (40,60,60)
Table 1: Comparative performance metrics of MobileLLaMA (M) for greedy setting, Vicuna-7B
(V-7B) and Vicuna-13B (V-13B) for non-greedy setting using different decoding methods. The table
details throughput ( Tin tokens/s), average accept lengths ( τin tokens), forward pass latency ( Lfpin
seconds), quality scores on MT-benchmark, percentages of additional trainable parameters ( Ptr) and
input lengths ( Sinput) after the prefilling phase. PPD employs a dynamic sparse tree with variable tree
sizes ( Str), represented as tuples. Same means the output matches with that of the original LLM.
5.1 Speedup Comparison with Parallel Decoding Methods
V-7B V-13B2030405060708090Throughput (tokens/s)1.02.24
2.09
1.41.59
1.48
1.02.18
2.07
1.31.421.46Speedup on different model sizes
Vanilla
PPD
Medusa
LookAhead
REST
PLD
Figure 4: Comparative evaluation of latency
speedup between PPD and other parallel de-
coding methods. The experiments were con-
ducted using the MT-Bench dataset, with the
temperature set to MT-Bench’s default config-
uration for Medusa and PPD .We compare the speedup ratios of PPD with state-
of-the-art parallel decoding methods on MT-Bench
in non-greedy settings in Figure 4. PPD achieves
speedups up to 13.8% higher than Medusa and be-
tween 2 times and 3 times higher than other parallel
decoding methods. We examine the factors con-
tributing to the enhanced speedup ratios and other
performance metrics, as presented in Table 1. The
reasons for the increase in speedup ratios are two-
fold. Firstly, PPD produces candidate tokens with
a higher acceptance rate than Medusa when utiliz-
ing a sparse tree of the same size. Notably, PPD
continues to achieve a comparable or slightly bet-
ter acceptance rate even when employing a much
smaller sparse tree – ranging from one-third to half
the size. Secondly, PPD benefits from lower forward
pass latency due to its ability to use smaller sparse
tree sizes and hence shorter input lengths. PPD also
eliminates the computational overhead associated
with separate decoding heads. PPD maintains the
same output quality, achieving about the same score on MT-Bench while using significantly fewer
trainable parameters.
Figure 5 displays the throughput of PPD on MT-Bench, HumanEval, and GSM8K with temperature
equal to 0. PPD achieves consistent walltime speedup ratios from 2.12 ×to 2.49 ×on different
GPUs. In general, PPD performs better in coding and math reasoning tasks, achieving speedups
between 2.21 ×and 2.49 ×. This can be attributed to the fact that both code and math equations often
contain fixed patterns and repetitive symbols, which narrows the range of plausible candidates and
simplifies the prediction. We also found that with typical acceptance, the speedup increases with
temperature. Another notable trend is that smaller models, such as Vicuna-7B, generally achieve
more significant speedup ratios as compared to larger models, like Vicuna-13B. PPD aims to generate
more tokens per step, which comes with increased computational demands. For larger models that
already require substantial computational resources, it is necessary to limit the size of the sparse tree
to avoid exceeding the GPU’s utilization cap and causing increased latency. As a result, the number
of tokens accepted per step is reduced, leading to lower speedups. However, this can be amortized
when using more powerful GPUs than the NVIDIA A100 and the RTX 4090, such as NVIDIA H100.
5.2 Long-range Token Prediction
For a specific sparse tree, the accumulative accuracy provides a theoretical upper bound for the
number of generated tokens per step and the maximum possible speedup ratio. Hence, maximizing
7

--- PAGE 8 ---
MobileLLaMA V-7B V-13B20406080100Throughput (tokens/s)1.02.16
1.02.19
1.02.12Multi-turn Dialogue - MT-Bench, A100
Vanilla
PPD
MobileLLaMA V-7B V-13B20406080100Throughput (tokens/s)1.02.25
1.02.28
1.02.21Coding - HumanEval, A100
Vanilla
PPD
MobileLLaMA V-7B V-13B20406080100120Throughput (tokens/s)1.02.47
1.02.33
1.02.27Math - GSM8K, A100
Vanilla
PPD
MobileLLaMA V-7B20406080100120140Throughput (tokens/s)1.02.2
1.02.14Multi-turn Dialogue - MT-Bench, 4090
Vanilla
PPD
MobileLLaMA V-7B20406080100120140Throughput (tokens/s)1.02.26
1.02.33Coding - HumanEval, 4090
Vanilla
PPD
MobileLLaMA V-7B20406080100120140160Throughput (tokens/s)1.02.49
1.02.29Math - GSM8K, 4090
Vanilla
PPDFigure 5: Throughput of PPD and vanilla models across different tasks. The temperature for
experiments are set to 0 and the generated output of PPD exactly matches that of the original LLM.
We do not show results of Vicuna-13B on RTX 4090 as it does not fit into the GPU memory.
accumulative accuracy is crucial for the effectiveness of PPD . Figure 6 demonstrates the accumulative
accuracy of the tokens predicted at various positions. We summarize the following three key insights
from the results.
PPD excels at predicting more distant tokens. As depicted in Figure 6a, PPD consistently
outperforms Medusa in accuracy across all token positions. The accuracy gap between PPD and
Medusa widens with the increased token distance ( e.g., the top-10 accuracy difference is 0.03 for the
‘next next’ word versus 0.12 for the ‘next next next next’ word). This improvement can be attributed
toPPD ’s ability to partially recover conditional dependency information through causally connected
prompt tokens.
PPD performs well at generating a broader array of plausible token candidates. For example, in
predicting the token at a token distance of 3, the top-10 candidates exhibit an accuracy improvement of
0.1 over Medusa, compared to only 0.02 for the top-1 candidate. This demonstrates the value of using
tree attention and the largest viable tree size during inference, as multiple candidate continuations
further boost accuracy improvement.
Multiple EPTs per prompt token and larger model sizes yield modest improvements in predic-
tion accuracy . Figure 6b shows that using 100 EPTs per prompt token leads to accuracy improvement,
ranging from 0.018 to 0.045. Figure 6c displays that PPD with Vicuna-13B outperforms Vicuna-7B
with an accuracy gain of 0.011 ∼0.038. This increase is due to Vicuna-13B’s greater embedding
dimensions and deeper layers, which enhance the expressive power of prompt tokens. However, these
gains are modest and can be offset by the increased computational burden of larger models.
2 4 6 8 10
top-k0.20.30.40.50.60.70.80.9Cumulative Accuracy
PPD, V7, 100 EPT, @ 1
PPD, V7, 100 EPT, @ 2
PPD, V7, 100 EPT, @ 3Medusa, V7, @ 1
Medusa, V7, @ 2
Medusa, V7, @ 3
(a) PD vs. Medusa
2 4 6 8 10
top-k0.20.30.40.50.60.70.80.9Cumulative Accuracy
PPD, V7, 100 EPT, @ 1
PPD, V7, 100 EPT, @ 2
PPD, V7, 100 EPT, @ 3PPD, V7, 1 EPT, @ 1
PPD, V7, 1 EPT, @ 2
PPD, V7, 1 EPT, @ 3 (b) 100 EPT vs. 1 EPT
2 4 6 8 10
top-k0.20.30.40.50.60.70.80.9Cumulative Accuracy
PPD, V13, 1 EPT, @ 1
PPD, V13, 1 EPT, @ 2
PPD, V13, 1 EPT, @ 3PPD, V7, 1 EPT, @ 1
PPD, V7, 1 EPT, @ 2
PPD, V7, 1 EPT, @ 3 (c) 13b vs. 7b
Figure 6: Accumulative accuracy comparisons across different model configurations and prediction
distances. ‘V7’ for Vicuna-7B, and ‘V13’ for Vicuna-13B. The notation ‘@ i’ refers to a token
distance of i. ‘100 EPT’ represents 100 EPTs per prompt token. Accumulative accuracy is defined as
top-k accuracy ( e.g., a prediction is correct if the top-k candidates contain the ground truth). These
measurements were obtained from the Alpaca Eval dataset with a maximum of 20 steps.
8

--- PAGE 9 ---
5.3 Memory Efficiency and Synergistic Integrations with Speculative Decoding
V-7B12.0012.2512.5012.7513.0013.2513.5013.7514.00
V-13B24.0024.2524.5024.7525.0025.2525.5025.7526.00Model Memory Usage (GB)
Vanilla
PPD
Medusa
Eagle
Figure 7: Model memory usage.Memory efficiency. As shown in Figure 7, we com-
pare the memory overhead of PPD with the leading
parallel decoding (Medusa) and speculative decod-
ing approaches (Eagle). The memory overhead of
PPD is just 0.004% of Medusa’s and 0.007% of Ea-
gle’s. This efficiency stems from the efficient use of
embeddings in PPD , which are significantly smaller
than decoding heads and draft models, both of which
scale with vocabulary size.
PPD + Speculative Decoding. As an orthogonal
optimization in accelerating LLMs, PPD can be
easily integrated with speculative decoding [ 11].
To demonstrate this, we applied PPD to Vicuna-
68M [ 24] and used it as the draft model for Vicuna-
7B. This combination resulted in a speedup of up to
1.22×for speculative decoding on Vicuna-7B compared to using speculative decoding alone.
5.4 Ablation Study
Dynamic Sparse Tree. Figure 8a shows that dynamic sparse trees consistently achieve longer
acceptance lengths compared to static and random ones across varying sizes. The acceptance length
for dynamic sparse trees shows a steady increase as the tree size extends, suggesting its good
scalability. The convergence of dynamic and static sparse trees at larger sizes suggests a structural
similarity emerging from constraints in tree depth and tree node count.
Hardware-aware Tree Size. Figure 8b presents the theoretical speedup across different GPUs.
Figure 8c validates that the optimal sparse tree size, derived from theoretical speedup models, indeed
results in the greatest actual speedup observed.
5 10 20 35 60 120 200 500
Prompt Length1.61.82.02.22.42.6Acceptance LengthAcceptance Lengths for Different Sparse Trees
Static Sparse Tree
Dynamic Sparse Tree
Random Sparse Tree
(a)
5 10 20 35 60 120 200 500
Prompt Length1.01.52.02.53.03.5Latency Overhead
0.81.01.21.41.61.82.0
Theoretical Speedup**Latency Overhead and Theoretical Speedup
Latency A100
Latency 4090
Theoretical Speedup A100
Theoretical Speedup 4090
Optimal Tree A100
Optimal Tree 4090 (b)
5 10 20 35 60 120 200 500
Prompt Length1.01.21.41.61.82.0Actual Speedup**Actual Speedup for Dynamic Sparse Tree
Actual Throughput A100
Actual Throughput 4090
Optimal Tree A100
Optimal Tree 4090
Peak Speedup A100
Peak Speedup 4090 (c)
Figure 8: Evaluation of Dynamic Sparse Tree Performance. The static sparse trees in (a) always use
the largest possible prompt tokens for each candidate. The theoretical speedup in (b) is calculated as
the ratio of acceptance lengths (hardware-independent) to latency overhead (hardware-dependent).
The optimal tree size is obtained from the peak value of the theoretical speedup. The latencies in (b)
are obtained from inference on the same prompt for 512 forward passes. (c) shows the actual speedup
obtained by running inference on different GPUs with different tree lengths on Alpaca Eval dataset.
6 Conclusion
We introduced PPD , a memory-efficient, cost-effective, and powerful parallel decoding method that
incorporates a hardware-aware dynamic sparse tree. Utilizing specially trained prompt tokens to
predict long-range tokens accurately, PPD achieves a speedup of up to 2.49 ×in inference while
employing only 0.0002% additional trainable parameters and without incorporating new models
or architectural components. We believe that PPD offers a novel perspective on the capabilities of
parallel decoding. In future work, it could be synergized with other speculative or parallel decoding
techniques to expedite inference even further.
9

--- PAGE 10 ---
References
[1]Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, and Tri
Dao. Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads.
arXiv preprint arXiv:2401.10774 , 2024.
[2] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and
John Jumper. Accelerating Large Language Model Decoding with Speculative Sampling. arXiv
preprint arXiv:2302.01318 , 2023.
[3]Mark Chen et al. Evaluating Large Language Models Trained on Code. arXiv preprint
arXiv:2107.03374 , 2021.
[4] Zhuoming Chen, Avner May, Ruslan Svirschevski, Yuhsun Huang, Max Ryabinin, Zhihao Jia,
and Beidi Chen. Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding. arXiv
preprint arXiv:2402.12374 , 2024.
[5]Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:
An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality, March 2023. URL
https://lmsys.org/blog/2023-03-30-vicuna/ .
[6]Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei,
Xinyu Zhang, Bo Zhang, Xiaolin Wei, and Chunhua Shen. MobileVLM : A Fast, Strong and
Open Vision Language Assistant for Mobile Devices. arXiv preprint arXiv:2312.16886 , 2023.
[7]Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John
Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 ,
2021.
[8]Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Breaking the Sequential Dependency of
LLM Inference Using Lookahead Decoding, November 2023. URL https://lmsys.org/
blog/2023-11-21-lookahead-decoding/ .
[9]Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D. Lee, and Di He. Rest: Retrieval-based
Speculative Decoding. arXiv preprint arXiv:2311.08252 , 2023.
[10] Vladimir Karpukhin, Barlas O ˘guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov,
Danqi Chen, and Wen-tau Yih. Dense Passage Retrieval for Open-Domain Question Answering.
InProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
(EMNLP) , pages 6769–6781, 2020.
[11] Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W. Mahoney, Amir
Gholami, and Kurt Keutzer. Speculative Decoding with Big Little Decoder. Advances in Neural
Information Processing Systems (NeurIPS) , 36, 2024.
[12] Brian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-Efficient
Prompt Tuning. In Conference on Empirical Methods in Natural Language Processing
(EMNLP) , 2021.
[13] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast Inference from Transformers via
Speculative Decoding. In International Conference on Machine Learning (ICML) , 2023.
[14] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation.
InProceedings of the 59th Annual Meeting of the Association for Computational Linguistics
and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long
Papers) , pages 4582–4597. Association for Computational Linguistics, 2021.
[15] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin,
Percy Liang, and Tatsunori B. Hashimoto. AlpacaEval: An Automatic Evaluator of Instruction-
following Models. https://github.com/tatsu-lab/alpaca_eval , 2023.
10

--- PAGE 11 ---
[16] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. EAGLE: Speculative Sampling
Requires Rethinking Feature Uncertainty. arXiv preprint arXiv:2401.15077 , 2024.
[17] Mingdao Liu, Aohan Zeng, Bowen Wang, Peng Zhang, Jie Tang, and Yuxiao Dong. APAR:
LLMs can do auto-parallel auto-regressive decoding. arXiv preprint arXiv:2401.06761 , 2024.
[18] Xupeng Miao et al. SpecInfer: Accelerating Large Language Model Serving with Tree-based
Speculative Inference and Verification. In ACM International Conference on Architectural
Support for Programming Languages and Operating Systems (ASPLOS) , 2024.
[19] Giovanni Monea, Armand Joulin, and Edouard Grave. PaSS: Parallel Speculative Sampling.
arXiv preprint arXiv:2311.13581 , 2023.
[20] Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi,
Riccardo Marin, and Emanuele Rodolà. Accelerating Transformer Inference for Translation via
Parallel Decoding. In Annual Meeting of the Association for Computational Linguistics (ACL) ,
2023.
[21] Apoorv Saxena. Prompt Lookup Decoding, November 2023. URL https://github.com/
apoorvumang/prompt-lookup-decoding/ .
[22] ShareGPT. ShareGPT, 2023. URL https://huggingface.co/datasets/Aeala/
ShareGPT_Vicuna_unfiltered .
[23] Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise Parallel Decoding for Deep
Autoregressive Models. In Advances in Neural Information Processing Systems (NeurIPS) ,
2018.
[24] Sen Yang, Shujian Huang, Xinyu Dai, and Jiajun Chen. Multi-Candidate Speculative Decoding.
arXiv preprint arXiv:2401.06706 , 2024.
[25] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion
Stoica. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. In Advances in Neural
Information Processing Systems (NeurIPS) , 2023.
11

--- PAGE 12 ---
Supplementary Material
Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
Acceleration of LLM Inference
Table of Contents
A Training Loss 13
B Extended Ablation Study 13
B.1 Effect of EPTs on Prediction Accuracy . . . . . . . . . . . . . . . . . . . . . . 13
B.2 Impact of Knowledge Distillation (KD), Epochs, and Batch Size on Prediction
Accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
B.3 Prefix Tuning + Prompt Token . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
B.4 Custom Decoding Heads + Prompt Token . . . . . . . . . . . . . . . . . . . . . 15
B.5 Attention Masking for EPTs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
B.6 Aggregation Method for EPTs . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
B.7 Multi-exit Ensemble . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
C Experiment Details 18
D Limitations 19
E Societal Impact 19
12

--- PAGE 13 ---
A Training Loss
0 2 4 6 8 10 12
Epoch1.52.02.53.03.54.04.55.05.5LossTraining Loss vs Epoch
(a) 3 prompt tokens, 1 EPTs
0 2 4 6 8 10 12
Epoch2.02.53.03.5LossTraining Loss vs Epoch (b) 3 prompt tokens, 100 EPTs
Figure 9: Training Loss
We study the training loss of PPD with different EPTs. Figure 9a shows that, with 3 prompt tokens
and 1 EPT, the initial loss is quite high, starting above 5. There is a sharp decrease in loss within
the first epoch, dropping below 2. After this initial drop, the loss stabilizes and oscillates around a
value slightly below 2 for the remainder of the training epochs (up to epoch 12). The loss oscillations
remain within a narrow range, indicating consistent performance. The fluctuation can be attributed
to the insertion of prompt tokens at random positions. On the other hand, Figure 9b, with 3 prompt
tokens and 100 EPTs, shows the initial loss starting below 3, significantly lower than PPD with 1
EPT. Similarly, there is a sharp decrease within the first epoch, with the loss dropping to around 2.5.
However, unlike PPD with 1 EPT, the loss continues to decrease gradually over the epochs, showing
a downward trend. This suggests that increasing the number of EPTs improves the model’s learning
capacity and reduce training loss more effectively over time.
B Extended Ablation Study
B.1 Effect of EPTs on Prediction Accuracy
EPT @1 Top-1 @1 Top-5 @2 Top-1 @2 Top-5
100 0.506 0.794 0.276 0.602
50 0.502 0.791 0.281 0.604
20 0.501 0.791 0.276 0.607
10 0.494 0.786 0.273 0.600
5 0.499 0.787 0.265 0.596
2 0.486 0.777 0.259 0.583
1 0.472 0.771 0.248 0.576
Table 2: Prediction Accuracy of PPD with different EPTs. ’@i’ denotes a token distance of i. ’Top-k’
denotes the top-k prediction accuracy. The results are obtained on Alpaca dataset with 20 steps.
Table 2 presents the prediction accuracy of PPD using different EPTs. The results indicate that
increasing the number of EPTs generally enhances the prediction accuracy of PPD, particularly for
long-range token predictions. Higher EPT numbers (e.g., 100 and 50) consistently produce better
prediction accuracy compared to lower EPT numbers.
B.2 Impact of Knowledge Distillation (KD), Epochs, and Batch Size on Prediction Accuracy
Table 3 summarizes our results with different settings. We analyze the effect of each factor on the
prediction accuracy in the following discussion.
13

--- PAGE 14 ---
EPT KD Epoch Batch @1 Top-1 @1 Top-5 @2 Top-1 @2 Top-5
100 Yes 1 4 0.504 0.793 0.273 0.598
100 Yes 2 4 0.512 0.797 0.288 0.611
100 Yes 6 4 0.520 0.802 0.302 0.620
100 Yes 8 4 0.524 0.804 0.307 0.619
100 Yes 10 4 0.523 0.804 0.305 0.623
100 Yes 12 4 0.525 0.805 0.308 0.625
100 No 12 4 0.506 0.794 0.276 0.602
100 Yes 12 1 0.530 0.809 0.309 0.626
1 Yes 12 1 0.484 0.775 0.259 0.581
1 Yes 2 4 0.474 0.773 0.247 0.574
1 Yes 6 4 0.480 0.773 0.250 0.580
1 Yes 8 4 0.484 0.778 0.257 0.583
1 Yes 10 4 0.482 0.777 0.257 0.584
1 Yes 12 4 0.485 0.779 0.261 0.586
1 No 12 4 0.472 0.771 0.248 0.576
Table 3: Prediction Accuracy for PPD with and without knowledge distillation (KD) for different
EPTs, epochs, and batch sizes.
B.2.1 Training Epochs
We first investigate the effect of the number of training epochs on prediction accuracy. For models
using 100 EPTs with KD enabled and a batch size of 4, we observe a steady improvement in prediction
accuracy as the number of epochs increases. Specifically, the Top-1 accuracy at a 1-token distance
increases from 0.504 at 1 epoch to 0.525 at 12 epochs, while the Top-5 accuracy at a 1-token distance
improves from 0.793 to 0.805. Similarly, Top-1 accuracy at a 2-token distance increases from 0.273
to 0.308, and Top-5 accuracy at a 2-token distance improves from 0.598 to 0.625 over the same range
of epochs. This trend demonstrates the positive impact of prolonged training on the performance of
PPD when KD is applied.
B.2.2 Knowledge Distillation
When KD is not applied, as shown for 100 EPTs at 12 epochs with a batch size of 4, the performance
metrics are generally lower. The improvement in prediction accuracy with KD is up to 12%. This
suggests that KD contributes significantly to prediction accuracy for PPD .
B.2.3 Effect of Batch Size
We also examine the impact of batch size on the prediction accuracy. For the model trained with 100
EPTs, KD enabled, and 12 epochs, reducing the batch size from 4 to 1 results in a slight improvement
in prediction accuracy up to 1%.
B.3 Prefix Tuning + Prompt Token
Prefix tuning [ 14], similar to prompt tuning, provides a parameter-efficient approach to fine-tune
a pre-trained model. Unlike prompt tuning, it modifies the KV cache of every attention layer by
prepending trained vectors. We hypothesize that the combination of prefix tuning and prompt tokens
can lead to greater learning capacity and higher prediction accuracy. This hypothesis is based on
the intuition that prompt tokens should see a different context than the input tokens when predicting
long-range tokens. For example, if the input sequence is "Once upon a time", then enhancing the
input with a prompt template might provide more suitable semantic context for long-range prediction.
An enhanced input like "Predict the next-next token. Once upon a time" might empower the prompt
token to predict the correct next-next token. Prefix tuning serves as the prompt template to enhance
the hidden states visible to the prompt tokens.
To retain the original model’s distribution, we modify the attention mask so that prefix tokens are
only visible to prompt tokens. This ensures that we can generate outputs that preserve the original
14

--- PAGE 15 ---
Figure 10: ’P1’ is the prefix token for the prompt token ’S1’ and ’P2’ for ’S2’. ’C’ is the input
token. The green tick means visibility during attention calculation. For instance, ’S1’ can see ’P1’
but cannot see ’P2’. ’C’ does not see any prefix tokens so the generated output corresponding to ’C’
is not altered by the use of prefix tuning.
model’s distribution. We posit that prompt tokens at different positions should see different contexts
so we allow a prompt token at a specific position to see a distinct set of prefix tokens, as shown in
Figure 10.
Prefix Tuning @1 Top-1 @1 Top-5 @2 Top-1 @2 Top-5
No 0.485 0.779 0.261 0.586
Yes 0.412 0.738 0.204 0.541
Table 4: Prediction Accuracy of PPD with and without prefix tuning. 1 EPT is used for all models
and 1 prefix token is used for prefix tuning.
Table 4 compares the prediction accuracy of PPD with and without the use of prefix tuning. The
results show that the models without prefix tuning outperform those with prefix tuning up to 28%,
which suggests that, in this setup, prefix tuning does not enhance the prediction accuracy of PPD.
Instead, it appears to degrade performance, potentially due to the complexity introduced by modifying
the KV cache of attention layers with the prefix token. Unlike prompt tokens, prefix tokens do
not interact with input tokens, meaning they do not change dynamically through the transformer
layers based on the input context. This lack of interaction and dynamic adjustment could be a factor
contributing to the decreased prediction accuracy observed with prefix tuning.
B.4 Custom Decoding Heads + Prompt Token
It has been demonstrated that a fine-tuned decoding head alone can effectively predict long-range
tokens [ 23,1]. Thus, we hypothesize that combining a separately fine-tuned decoding head with
prompt tokens might further enhance the potential of PPD . As shown in Figure 11, we trained a
separate decoding head to transform only the hidden states of prompt tokens into logits. A key
distinction from Medusa is that this decoding head is responsible for generating tokens at multiple
positions, rather than just one.
We propose two training methods. In the first method, the custom decoding head and prompt tokens
are trained together from scratch in a single stage. In the second method, the prompt tokens are
initially trained for 2 epochs, followed by training both the prompt tokens and the decoding head
with a smaller learning rate in a two-stage process.
Table 5 presents the prediction accuracy of PPD with and without a custom decoding head. When
trained using the single-stage method, PPD with the custom decoding head shows a 12%-21%
decrease in prediction accuracy compared to the baseline PPD without the custom decoding head.
This suggests that the single-stage approach does not result in stable or effective training.
15

--- PAGE 16 ---
Figure 11: Custom decoding head with PPD . The feature extractor refers to the LLMs without the
decoding heads. ’H1’ is the generated hidden state for the input token ’C’. ’H2’ is the hidden state
for the prompt token ’S1’ and ’H3’ for ’S2’. ’LM1’ is the original LLM’s decoding head and it takes
in the hidden states of input tokens. ’LM2’ is the custom decoding heads for PPD and only takes in
the hidden states of prompt tokens.
Method Name @1 Top-1 @1 Top-5 @2 Top-1 @2 Top-5
PPD without custom decoding head 0.485 0.779 0.261 0.586
PPD with custom decoding head (1-stage) 0.385 0.614 0.229 0.482
PPD with custom decoding head (2-stage) 0.506 0.795 0.276 0.602
Table 5: Prediction Accuracy of PPD with and without custom decoding head. 1 EPT is used for all
models. 1-stage and 2-stage refer to the training strategies of custom decoding head.
In contrast, the two-stage training method results in a limited improvement of 2.1%-4.3% in prediction
accuracy compared to the baseline. This suggests that adding a custom decoding head may not be
necessary, given the additional trainable parameters and the limited improvement in prediction
accuracy.
B.5 Attention Masking for EPTs
In this paper, we proposed a specialized attention mask for EPTs to achieve the effect of prompt
ensemble. However, there are alternative masking strategies available. Here, we describe and compare
three types of attention masks that we implemented and experimented with.
(a) Ensemble Attention Mask
 (b) Decoder-like Attention Mask
 (c) Encoder-like Attention Mask
Figure 12: Different Mask Strategies for EPTs. ’C’ is an input token. ’V1’ and ’V2’ are the EPTs for
prompt tokens ’S1’ and ’V3’ and ’V4’ for ’S2’.
B.5.1 Ensemble Attention Masking
The ensemble attention masking is the masking strategy we previously described. In this approach,
EPTs are divided into ndisjoint groups, where nis the number of EPTs per prompt token. All kth
EPTs across prompt tokens are placed in the same group. An EPT vin group ican only attend to
16

--- PAGE 17 ---
EPTs that meet the following two criteria: 1) they must belong to group i, and 2) their position indices
must be smaller than the position index of v. Since this masking strategy effectively averages the
results of disjoint groups of EPTs, we refer to it as the "ensemble attention masking". Figure 12a
provides an example of the ensemble attention masking.
B.5.2 Decoder-like Attention Masking
Decoder-like attention masking is a simple strategy where EPTs can only attend to EPTs with smaller
position indices. This results in a triangular-shaped attention mask, similar to the one used in decoder
layers, hence the name "decoder-like attention masking". Figure 12b provides an example of this
masking strategy.
B.5.3 Encoder-like Attention Masking
In encoder-like attention masking, an EPT corresponding to a prompt token Pcan attend to all EPTs
with smaller position indices as well as all EPTs associated with P. This allows EPTs to see both
preceding and succeeding EPTs, similar to the token visibility in an encoder layer, hence the name
"encoder-like attention masking". Figure 12c illustrates this masking strategy.
B.5.4 Results
Method Name @1 Top-1 @1 Top-5 @2 Top-1 @2 Top-5
PPD with ensemble attention mask 0.506 0.794 0.276 0.602
PPD with decoder attention mask 0.465 0.755 0.262 0.572
PPD with encoder attention mask 0.473 0.765 0.256 0.573
Table 6: Prediction Accuracy of PPD with different attention masking strategies for EPTs. 100 EPT
is used for all models.
The results in Table 6 indicate that the ensemble attention mask outperforms the other masking
strategies. In comparison, the PPD with decoder attention mask shows 4.9%-8.0% lower prediction
accuracy. The PPD with encoder attention mask also underperforms in prediction accuracy relative
to the ensemble attention mask by 3.7%-7.2%.
These results suggest that the ensemble attention mask is the most effective strategy among the three,
likely due to its ability to effectively average the votes of disjoint groups of EPTs, thereby improving
prediction accuracy. The decoder-like and encoder-like attention masks, while simpler, do not provide
the same level of performance, indicating that the structure and specificity of the ensemble attention
mask better facilitate accurate long-range token prediction. Additionally, ensemble attention masking
is more sparse, which offers greater potential for optimization.
B.6 Aggregation Method for EPTs
In addition to simply averaging the logits from EPTs, we explored more advanced aggregation
methods. For instance, we applied learned weights to aggregate the logits. The final logit pcan be
expressed as:
p=nX
i=1wi·pi,
where nis the number of EPTs and wiis the learned scalar weight for the ithEPT.
Aggregation Method @1 Top-1 @1 Top-5 @2 Top-1 @2 Top-5
Average 0.506 0.794 0.276 0.602
Learned Weight 0.503 0.779 0.250 0.576
Table 7: Prediction Accuracy of PPD with different aggregation methods for EPTs. 100 EPT is used
for all models.
17

--- PAGE 18 ---
The results in Table 7 show the prediction accuracy of PPD with two different aggregation methods
for EPTs: simple averaging and learned weights. When using learned weights to aggregate logits, the
model shows a slight decrease of 0.6%-9.4% in prediction accuracy.
These results suggest that while learned weights provide a more flexible aggregation method, they do
not necessarily lead to improved prediction accuracy in this context. The simplicity and stability of
the averaging method appear to offer better performance, possibly due to the additional complexity
and potential overfitting introduced by learning the weights.
B.7 Multi-exit Ensemble
While using EPTs for prompt ensemble improves prediction accuracy, it also increases input length,
resulting in higher computational overhead and forward pass latency. To address this, we propose the
use of a multi-exit ensemble method. In multi-exit ensemble, the hidden states of a prompt token
from the last kdecoder layers are extracted and averaged to produce the final hidden state, which
is then decoded by the decoding head into a guess token, as illustrated in Figure 13. This approach
achieves prompt ensemble without the associated computational costs.
Figure 13: Mult-exit ensemble. ’D1’, ’D10’, ’D11’, and ’D12’ are the decoder layers in order. ’S1’
is a prompt token and ’H1’, ’H2’, ’H3’ are the corresponding hidden states from the last 3 decoder
layers. ’H4’ is obtained from averaging these 3 hidden states. The decoding head ’LM’ translates
’H4’ into a token ’E’.
The hypothesis is that taking the hidden states from the last few decoder layers for ensemble
might work because these layers capture increasingly abstract and high-level representations of the
input sequence. By averaging the hidden states from multiple layers, we can combine diverse but
complementary information, leading to a more robust and accurate final hidden state. Additionally,
since the final layers are closest to the output, they are more likely to contain refined and contextually
relevant information, making the ensemble more effective.
Method Name @1 Top-1 @1 Top-5 @2 Top-1 @2 Top-5
PPD without multi-exit 0.485 0.779 0.261 0.586
PPD with 3 exits 0.422 0.723 0.214 0.517
PPD with 2 exits 0.420 0.723 0.213 0.518
Table 8: Prediction Accuracy of PPD with and without multi-exit ensemble. 1 EPT is used for all
models. kexits refer to the number of exits used.
Table 8 shows the comparison of prediction accuracy of PPD with and without mult-exit ensemble.
The results indicate that the introduction of multi-exit ensemble with both 2 and 3 exits results in a
7%-18% decrease in prediction accuracy compared to the baseline model without multi-exit.
These findings suggest that the multi-exit ensemble approach, as implemented, does not enhance
prediction accuracy and instead leads to a notable decrease in performance. This may be due to the
averaging of hidden states from multiple layers introducing noise or reducing the specificity of the
representations needed for accurate prediction. Further refinement of the multi-exit ensemble may be
necessary to achieve the desired improvements in accuracy.
C Experiment Details
For the throughput experiments, each result is obtained by averaging three separate runs. The standard
deviations of these runs are reported as error bars in the bar charts. To ensure a fair comparison in our
comparative experiments, we maintained consistent hardware settings and software versions.
18

--- PAGE 19 ---
We selected 3 prompt tokens because adding more would not further increase the expected acceptance
length due to the tree size limit. The number of EPTs per prompt token was optimized to maximize
throughput.
In Fig. 2, the temperature settings for PPD , Eagle [ 16], and Medusa [ 1] follow the default con-
figuration, while the other models use a greedy setting (temperature=0). This choice is based on
findings that retrieval-based methods perform significantly worse in non-greedy settings. Similarly,
LOOKAHEAD DECODING [8], REST [ 9], and PLD [ 21] in Fig. 4 also use a temperature setting of 0 for
the same reasons.
D Limitations
Despite its efficiency, we have identified the following limitations of PPD :
1.Low prediction accuracy for very small models. We found that for very small models like
Vicuna-68M [ 24], which only has 2 decoder layers and an embedding dimension of less than
1000, PPD suffers from low prediction accuracy. This is because the embedding dimension
determines the expressive power of a prompt token, and the transformer architecture’s depth
is crucial for efficient information flow to the prompt tokens.
2.GPU compute resource constraint. Since PPD trades additional compute resources for
increased throughput, its effectiveness depends on the availability of idle GPU compute
resources. On a GPU with limited compute resources, the speedup ratios achieved by PPD
are expected to decrease.
3.Extended input length. The improvement in acceptance length with PPD is not as signif-
icant as the gain in prediction accuracy compared to Medusa. This is because PPD must
reserve a substantial portion of the input for prompt tokens, which limits the size of the
sparse tree that can be used.
E Societal Impact
In this paper, we proposed PPD to accelerate LLMs easily and cheaply. Since PPD reduces the time
required for handling a single inference request, it could bring down the cost of deploying LLMs
for both the companies and the public. This might lead to increased accessibility of LLM services.
Moreover, latency-sensitive applications like chatbots will benefit greatly from the usage of PPD as it
reduces the inference latency greatly, thereby enhancing the user experience.
While PPD aims to make AI more accessible, there may still be a digital divide where certain
communities lack the necessary infrastructure, such as stable internet connections or modern hardware,
to fully benefit from these advancements. This could further widen the gap between technology-
privileged and underserved populations. On the other hand, PPD might be misused by malicious
parties to manipulate the output of the original LLM, resulting in the generation of unreliable
information and fake data.
19
