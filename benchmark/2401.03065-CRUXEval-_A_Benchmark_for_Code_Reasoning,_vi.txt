# 2401.03065.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/benchmark/2401.03065.pdf
# Kích thước tệp: 2012215 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
CRUXEval: Một Benchmark để Lý Luận, Hiểu và Thực Thi Mã
Alex Gu⋆gua@mit.edu
MIT CSAIL
Baptiste Rozière broz@meta.com
Meta AI
Hugh Leather hleather@meta.com
Meta AI
Armando Solar-Lezama asolar@csail.mit.edu
MIT CSAIL
Gabriel Synnaeve gab@meta.com
Meta AI
Sida I. Wang sida@meta.com
Meta AI

Tóm tắt
Chúng tôi trình bày CRUXEval (Code Reasoning, Understanding, and eXecution Evaluation), một benchmark bao gồm 800 hàm Python (3-13 dòng). Mỗi hàm đi kèm với một cặp đầu vào-đầu ra, dẫn đến hai nhiệm vụ tự nhiên: dự đoán đầu vào và dự đoán đầu ra. Đầu tiên, chúng tôi đề xuất một công thức chung để tạo ra benchmark thực thi của chúng tôi có thể được sử dụng để tạo ra các biến thể tương lai của benchmark. Thứ hai, chúng tôi đánh giá hai mươi mô hình mã trên benchmark của chúng tôi và phát hiện ra rằng nhiều mô hình gần đây có điểm số cao trên HumanEval không cho thấy cùng mức cải thiện trên benchmark của chúng tôi. Thứ ba, chúng tôi chỉ ra rằng các sơ đồ CoT đơn giản và fine-tuning có thể cải thiện hiệu suất trên benchmark của chúng tôi nhưng vẫn còn xa mới giải quyết được nó. Thiết lập tốt nhất, GPT-4 với chuỗi suy nghĩ (CoT), đạt được pass@1 là 75% và 81% cho dự đoán đầu vào và đầu ra tương ứng. Ngược lại, Code Llama 34B đạt được pass@1 là 50% và 46% cho dự đoán đầu vào và đầu ra, làm nổi bật khoảng cách giữa các mô hình mã nguồn mở và đóng. Vì không có mô hình nào gần với việc làm chủ CRUXEval, chúng tôi cung cấp các ví dụ về những thất bại nhất quán của GPT-4 trên các chương trình đơn giản như một lăng kính để nhìn vào khả năng lý luận mã của nó và các lĩnh vực cần cải thiện.

1 Giới thiệu
Trong những tháng gần đây, kỹ thuật phần mềm và lập trình đã trở thành các lĩnh vực ngày càng phổ biến đối với các mô hình ngôn ngữ (LMs) khi chúng cố gắng chinh phục một loạt các nhiệm vụ bao gồm
⋆Công việc chủ yếu được thực hiện trong thời gian thực tập tại Meta AI
1arXiv:2401.03065v1 [cs.SE] 5 Jan 2024

--- TRANG 2 ---
hoàn thành mã, sửa chữa chương trình, gỡ lỗi, tạo test case và tối ưu hóa mã (xem Zan et al. (2023) và Fan et al. (2023) để có khảo sát). Các mô hình gần đây bao gồm Code Llama (Roziere et al., 2023), GPT-3.5 (Brown et al., 2020; Ouyang et al., 2022), và GPT-4 (OpenAI, 2023) đã cho thấy tiềm năng trong các nhiệm vụ liên quan đến mã và đang được sử dụng để phát triển các công cụ giúp lập trình viên viết mã hiệu quả hơn.

Cách chính mà cộng đồng đã đánh giá các LMs mã là thông qua các benchmark như HumanEval (Chen et al., 2021) và MBPP (Austin et al., 2021), kiểm tra khả năng tạo ra các đoạn mã ngắn từ các đặc tả ngôn ngữ tự nhiên. Trong khi HumanEval và MBPP nắm bắt khả năng tạo mã trên các nhiệm vụ đơn giản và cơ bản, có sự thiếu vắng các benchmark nắm bắt các khía cạnh cơ bản khác của LMs mã như hiểu mã và thực thi.

Được thúc đẩy bởi điều này, chúng tôi đóng góp một benchmark mới, CRUXEval (Code Reasoning, Understanding, and eXecution Evaluation) với hai nhiệm vụ: 1) dự đoán đầu ra, CRUXEval-O để đo khả năng theo dõi thực thi mã và 2) dự đoán đầu vào, CRUXEval-I để đo khả năng lý luận và hiểu mã. Một ví dụ về mẫu trong CRUXEval được hiển thị trong Listings 1 và 2 (được chỉnh sửa để dễ đọc). CRUXEval kiểm tra khả năng của LMs mã để lý luận về hành vi thực thi của các chương trình Python đơn giản. Trong khi LMs không nên được mong đợi thay thế một trình thông dịch trên các vấn đề phức tạp tùy ý, chúng tôi đảm bảo các mẫu trong benchmark của chúng tôi đơn giản (tối đa 13 dòng, không có phép tính phức tạp) và có thể giải quyết được bởi một sinh viên tốt nghiệp CS cấp đại học mà không cần thêm bộ nhớ (theo ý kiến của chúng tôi). CRUXEval cung cấp một thăm dò hữu ích và quan trọng để hiểu rõ hơn khả năng của LMs mã, vì việc theo dõi một vài bước thực thi mã đơn giản nên là một yêu cầu cơ bản đối với các mô hình này. Khả năng lý luận về hành vi thực thi của mã cũng mở đường cho việc giải quyết các nhiệm vụ khó khăn hơn như sửa chữa mã với phản hồi thực thi và tóm tắt mã.

Listing 1: Vấn đề mẫu
def f(string):
    string_x = string.rstrip("a")
    string = string_x.rstrip("e")
    return string
# dự đoán đầu ra, CRUXEval-O
assert f("xxxxaaee") == ??
## GPT4: "xxxx", không chính xác
# dự đoán đầu vào, CRUXEval-I
assert f(??) == "xxxxaa"
## GPT4: "xxxxaae", chính xác

Listing 2: Vấn đề mẫu
def f(nums):
    count = len(nums)
    for i in range(-count+1, 0):
        nums.append(nums[i])
    return nums
# dự đoán đầu ra, CRUXEval-O
assert f([2, 6, 1, 3, 1]) == ??
# GPT4: [2, 6, 1, 3, 1, 6, 1, 3, 1], không chính xác
# dự đoán đầu vào, CRUXEval-I
assert f(??) == [2, 6, 1, 3, 1, 6, 3, 6, 6]
# GPT4: [2, 6, 1], không chính xác

Ở mức độ cao, benchmark của chúng tôi được xây dựng như sau. Đầu tiên, chúng tôi sử dụng Code Llama 34B để tạo ra một tập hợp lớn các hàm và đầu vào. Các đầu ra được tạo ra bằng cách thực thi các hàm trên các đầu vào. Thứ hai, chúng tôi lọc tập hợp để benchmark của chúng tôi chỉ bao gồm các vấn đề ngắn với yêu cầu tính toán và bộ nhớ thấp, các vấn đề mà một lập trình viên con người giỏi nên có thể làm mà không cần thêm bộ nhớ trong khoảng một phút. Thứ ba, chúng tôi chọn ngẫu nhiên 800 mẫu vượt qua bộ lọc, đảm bảo benchmark vừa đủ nhỏ để dễ dàng chạy nhưng đủ lớn để thấy được sự khác biệt về hiệu suất một cách đáng tin cậy giữa các mô hình khác nhau. Chúng tôi sử dụng phương pháp này vì trong khi khó có thể tự tạo ra ví dụ mà các mô hình mạnh nhất như GPT-4 thất bại hoàn toàn, chúng tôi quan sát thấy rằng chúng thất bại khá thường xuyên trên các chương trình ngẫu nhiên nhưng hợp lý. Chúng tôi cũng nhấn mạnh rằng khi các mô hình cải thiện, phương pháp tạo-và-lọc này có thể được sử dụng để tạo ra các benchmark tương lai khó khăn hơn và kiểm tra các khía cạnh khác nhau của việc thực thi chương trình.

2

--- TRANG 3 ---
Mô hình tốt nhất, GPT-4, đạt được pass@1 là 67% trên CRUXEval-I và 63% trên CRUXEval-O. Ngược lại, các mô hình mã nguồn mở tốt nhất chỉ đạt được 47% trên CRUXEval-I và 44% trên CRUXEval-O, thất bại hơn một nửa thời gian trong việc dự đoán thực thi đơn giản và lý luận mã mặc dù được đào tạo trên 100G mã Python và 1T dữ liệu mã. Chúng tôi cũng quan sát thấy rằng đối với các mô hình cơ sở, hiệu suất HumanEval mạnh hơn có mối tương quan với hiệu suất CRUXEval mạnh hơn. Tuy nhiên, xu hướng này phá vỡ đối với các mô hình được chưng cất trên dữ liệu giống GPT-4 như WizardCoder, Phind và Phi, có điểm số HumanEval ấn tượng cao nhưng không tốt hơn CodeLlama trên CRUXEval.

Chúng tôi cũng quan sát thấy rằng CoT và fine-tuning trên các assertion đầu vào-đầu ra là những kỹ thuật hiệu quả để cải thiện hiệu suất trên CRUXEval, nhưng còn xa mới đủ để làm chủ nó. Nhìn chung, benchmark của chúng tôi tiết lộ rằng khoảng cách giữa GPT-4 và các mô hình mã nguồn mở phản ánh khả năng mạnh hơn của GPT-4 trong việc lý luận về hành vi của mã. Vì các benchmark hiện tại như HumanEval và MBPP không đủ để đo khả năng hiểu và thực thi mã, việc nắm bắt nó thông qua benchmark của chúng tôi là quan trọng để tạo ra tiến bộ hướng tới việc thu hẹp khoảng cách giữa các mô hình mở và GPT-4. Cuối cùng, chúng tôi phát hiện ra rằng mặc dù có khả năng ấn tượng, GPT-4 vẫn thất bại nhất quán trong việc hiểu hành vi thực thi của một số chương trình Python đơn giản đáng ngạc nhiên.

2 Công trình liên quan
LMs cho Tạo mã: Đã có nhiều nỗ lực đào tạo LMs để tạo mã. Các mô hình cơ sở bao gồm Codex (Chen et al., 2021), CodeGeeX (Zheng et al., 2023), SantaCoder (Allal et al., 2023), PolyCoder (Xu et al., 2022), InCoder (Fried et al., 2022), CodeGen (Nijkamp et al., 2022), StarCoder (Li et al., 2023a), DeepSeek-Coder (AI, 2023), và Code Llama (Roziere et al., 2023). Sau đó, một số mô hình này được fine-tuned trên dữ liệu giống hướng dẫn được chưng cất từ GPT-3.5 và GPT-4, tạo ra các mô hình như Phind (Royzen et al., 2023), WizardCoder (Luo et al., 2023), và Phi-1/Phi-1.5 (Li et al., 2023b; Gunasekar et al., 2023). Chúng tôi đánh giá hiệu suất của một lựa chọn các mô hình này trên CRUXEval của chúng tôi.

Benchmarks để Đánh giá LMs Mã: Có nhiều benchmark phục vụ để đánh giá các khía cạnh khác nhau của các LMs mã này. Chúng tôi khảo sát một số ở đây và giới thiệu độc giả đến khảo sát (Zhang et al., 2023h) để biết thêm. HumanEval (Chen et al., 2021) và MBPP (Austin et al., 2021) đánh giá việc tạo mã Python trên các hàm tương đối đơn giản. HumanEval+ (Liu et al., 2023c) bổ sung HumanEval với các test case tốt hơn sau khi phát hiện nhiều giải pháp vượt qua là không chính xác. ReCode (Wang et al., 2022a) là một biến thể của HumanEval với tên hàm và docstring bị nhiễu. HumanEval-X (Zheng et al., 2023), MultiPLe (Cassano et al., 2022), và MBXP (Athiwaratkun et al., 2022) là các mở rộng của HumanEval và MBPP với trọng tâm bao gồm các ngôn ngữ lập trình ngoài Python. APPS (Hendrycks et al., 2021), CodeContests (Li et al., 2022), và LeetCode-Hard (Shinn et al., 2023) đánh giá việc tạo mã trên các vấn đề khó khăn hơn, kiểu phỏng vấn hoặc thi đấu.

Cũng có các benchmark để đánh giá việc tạo mã trong các ứng dụng khoa học dữ liệu, như DS-1000 (Lai et al., 2023), ARCADE (Yin et al., 2022), NumpyEval (Zhang et al., 2023b), và PandasEval (Jain et al., 2022). Đi xa hơn một bước, một số benchmark cũng đo khả năng sử dụng API hoặc thực hiện các nhiệm vụ kỹ thuật phần mềm tổng quát hơn, như JuICe (Agashe et al., 2019), APIBench (Patil et al., 2023), RepoBench (Liu et al., 2023e), ODEX (Wang et al., 2022b), SWE-Bench (Jimenez et al., 2023), GoogleCodeRepo (Shrivastava et al., 2023), RepoEval (Zhang et al., 2023a), và Cocomic-Data (Ding et al., 2022).

3

--- TRANG 4 ---
Cuối cùng, có nhiều benchmark cho các nhiệm vụ khác, như dịch mã (Roziere et al., 2020; Zhu et al., 2022; Ahmad et al., 2021), tạo test case (Tufano et al., 2022; Watson et al., 2020), tìm kiếm mã (Husain et al., 2019), dự đoán kiểu (Mir et al., 2022; Wei et al., 2023; Malik et al., 2019), tạo thông điệp commit (Liu et al., 2020), tóm tắt mã (LeClair et al., 2019; Iyer et al., 2016; Barone & Sennrich, 2017; Hasan et al., 2021; Alon et al., 2018), bảo mật mã (Liguori et al., 2022; Pearce et al., 2022; Tony et al., 2023), sửa chữa chương trình (Jiang et al., 2023b; Xia et al., 2022; Tufano et al., 2019; Haque et al., 2022; Jin et al., 2023; Gupta et al., 2017; Berabi et al., 2021), tối ưu hóa hiệu suất (Garg et al., 2022; Madaan et al., 2023a), và nhiều hơn nữa.

Theo hiểu biết của chúng tôi, CRUXEval của chúng tôi là benchmark công khai đầu tiên để đo khả năng thực thi của LMs mã. Trong khi một số công trình trước đây đã đo khả năng dự đoán đầu ra của LMs mã, chúng tôi tận dụng CRUXEval-O của chúng tôi để thực hiện một cuộc điều tra kỹ lưỡng hơn về những khả năng này. CRUXEval-I của chúng tôi là đầu tiên để đo khả năng của LMs mã thực hiện dự đoán đầu vào.

Tận dụng Test Cases và Thực thi Mã: Một hướng công việc khác sử dụng test cases và thông tin thực thi mã để cải thiện việc tạo mã. Một số ví dụ bao gồm Speculyzer (Key et al., 2022), CodeT (Chen et al., 2022), CodeGen-Test (Zhong et al., 2022), Coder-Reviewer reranking (Zhang et al., 2023g), MBR-EXEC (Shi et al., 2022) TCoT (Tian & Chen, 2023), Algo (Zhang et al., 2023d), Pangu-Coder2 (Shen et al., 2023), LEVER Ni et al. (2023), và Self-Play (Haluptzok et al., 2022). Ý tưởng của những công trình này là tạo ra nhiều chương trình và nhiều test cases và chọn chương trình và test cases nào có vẻ chính xác dựa trên kết quả thực thi. Các công trình khác sử dụng phản hồi thực thi kiểu RL để cải thiện việc tạo mã, bao gồm CodeRL (Le et al., 2022), Reflexion (Shinn et al., 2023), và PG-TD (Zhang et al., 2023e). (Chen et al., 2023; Olausson et al., 2023b; Madaan et al., 2023b; Peng et al., 2023; Zhang et al., 2023c) điều tra self-repair, sử dụng thông điệp lỗi làm phản hồi cho các mô hình để cải thiện.

Liên quan nhất đến công việc của chúng tôi, một số công trình kiểm tra và cải thiện khả năng thực thi của LMs mã. Austin et al. (2021), Scratchpad (Nye et al., 2021), và CodeExecutor (Liu et al., 2023a) đào tạo LMs mã trên thông tin thực thi. Được truyền cảm hứng từ những công trình này, chúng tôi đề cập ngắn gọn đến hai cách nguyên thủy để cải thiện hiệu suất trên benchmark của chúng tôi, chuỗi suy nghĩ và fine-tuning. Tiến về phía trước, chúng tôi tin rằng CRUXEval của chúng tôi có thể phục vụ như một điểm tham chiếu hữu ích khi nhiều kỹ thuật hơn được thiết kế để cải thiện khả năng thực thi mã.

Các chế độ thất bại của Lý luận LM: Một ước mơ khác của cộng đồng là hiểu rõ hơn các chế độ thất bại của LMs trên các nhiệm vụ lý luận. Bubeck et al. (2023); Liu et al. (2023b); Arkoudas (2023); Zhang et al. (2022); Dziri et al. (2023); Olausson et al. (2023a); Lee et al. (2023); Zhang et al. (2023f) tất cả đều điều tra và chỉ ra các chế độ thất bại khác nhau của LMs trên nhiều nhiệm vụ lý luận đa dạng. Các ví dụ khác về thất bại lý luận bao gồm 1) hiểu phủ định (Hosseini et al., 2021), 2) bỏ qua ngữ cảnh không liên quan (Shi et al., 2023), 3) hoạt động dưới các tình huống phản thực tế như Python 1-indexed hoặc phép cộng cơ số-9 (Wu et al., 2023), và 4) tạo mã Python sau khi hoán đổi định danh như print, len = len, print (Miceli-Barone et al., 2023). Lấy quan điểm lý thuyết hơn, Dziri et al. (2023); Zhou et al. (2023); Merrill & Sabharwal (2023); Giannou et al. (2023) đặc trưng hóa các loại nhiệm vụ lý luận mà transformers có thể và không thể được mong đợi thực hiện. Merrill et al. (2021) lập luận rằng không thể học ý nghĩa từ hình thức không được căn cứ với sự phụ thuộc ngữ cảnh và giả định rằng cú pháp độc lập với ngữ nghĩa. Trong công trình này, chúng tôi sử dụng CRUXEval để kiểm tra thực nghiệm các thất bại trong thực thi mã / lý luận.

4

--- TRANG 5 ---
3 Xây dựng Benchmark
CRUXEval bao gồm 800 hàm riêng biệt, mỗi hàm có một cặp đầu vào-đầu ra sao cho việc thực thi hàm trên đầu vào tạo ra đầu ra một cách xác định. Sử dụng những hàm và cặp đầu vào-đầu ra này, chúng tôi tạo ra hai nhiệm vụ benchmark. Trong nhiệm vụ dự đoán đầu ra, mục tiêu là dự đoán đầu ra của việc thực thi hàm trên đầu vào liên quan của nó. Trong nhiệm vụ dự đoán đầu vào, mục tiêu là tìm bất kỳ đầu vào nào sao cho việc thực thi hàm trên đầu vào đó tạo ra đầu ra. Đối với cả hai nhiệm vụ, chúng tôi sử dụng một metric chính xác dựa trên thực thi. Đối với dự đoán đầu vào, một đầu vào được tạo ra vượt qua nếu assert f(đầu vào được tạo) == đầu ra vượt qua, và đối với dự đoán đầu ra, một đầu ra được tạo ra vượt qua nếu assert f(đầu vào) == đầu ra được tạo vượt qua. Một số thống kê về các mẫu của CRUXEval có thể được tìm thấy trong Phụ lục A.3.

3.1 Tạo ra Ứng viên
Chúng tôi sử dụng Code Llama 34B để tạo ra tất cả các hàm ứng viên và đầu vào của CRUXEval. Để làm điều này, chúng tôi nhắc nó với tên của một hàm trong thư viện chuẩn Python như str.zfill và yêu cầu nó tạo ra một hàm Python sử dụng hàm thư viện cùng với 5 đầu vào kiểm tra. Chúng tôi cung cấp hai ví dụ few-shot khác nhau trong prompt của chúng tôi để cải thiện tính đa dạng của các thế hệ (xem Phụ lục A.2 để biết thêm chi tiết). Một prompt mẫu được hiển thị trong Listing 11.

Chúng tôi sử dụng tổng cộng 69 hàm khác nhau từ thư viện chuẩn: 47 từ str, 11 từ dict, và 11 từ list (xem Phụ lục A.1 để có danh sách đầy đủ các hàm). Nhìn chung, chúng tôi tạo ra tổng cộng 102000 hàm (46% str, 27% dict, 27% list) và 489306 cặp đầu vào-đầu ra.

3.2 Lọc Ứng viên
Tiếp theo, chúng tôi lọc các ứng viên được tạo ra để đảm bảo rằng các mẫu trong dataset là hợp lý và chất lượng cao. Để tránh buộc mô hình thực hiện các nhiệm vụ như tính toán số học, chúng tôi thiết kế các bộ lọc để benchmark chỉ bao gồm các mẫu có thể giải quyết được bởi con người mà không cần thêm bộ nhớ.

Cụ thể, chúng tôi lọc dựa trên các tiêu chí sau.
• Thời gian biên dịch: tất cả các đối số của hàm phải được sử dụng trong hàm, độ dài mã từ 75 đến 300 ký tự, không có lỗi cú pháp, assertion đúng assert f(đầu vào) == đầu ra.
• Thời gian chạy: không có phép toán số thực, phép chia thật, exp, các phép toán số nguyên khác phải có ít nhất một đối số ≤3, các phép toán chuỗi và danh sách phải có ít nhất một đối số có độ dài ≤3, hoàn thành chạy trong 2 giây, không có ngoại lệ không được bắt.
• Cố gắng tốt nhất để loại bỏ mã không mong muốn khác: hàm không thể có bất kỳ import nào (như os, random), phải là xác định (random, thứ tự set), và không thể có tác dụng phụ như input, builtins.

5

--- TRANG 6 ---
3.3 Kích thước Dữ liệu và đo Nhiễu
Thành công của HumanEval (164 ví dụ) cho thấy rằng các benchmark đánh giá có thể nhỏ nơi đánh giá nhanh hơn và rẻ hơn là một lợi thế bị bỏ qua. Vì các ví dụ bổ sung dễ dàng tạo ra, chúng tôi đầu tiên tạo ra quá nhiều và sau đó đo liệu nhiễu có đủ nhỏ trên một dataset nhỏ hơn không.

Trong tất cả các mẫu, Code Llama 34B vượt trội hơn Code Llama 13B như mong đợi và chúng tôi muốn giữ lại tính chất này với độ tin cậy cao trong một dataset nhỏ hơn. Để làm điều này, chúng tôi lấy các mẫu bootstrap có kích thước N từ ~1700 mẫu để đo xác suất rằng hiệu suất sẽ bị đảo ngược, được hiển thị trong Hình 1. 800 ví dụ đủ để kiểm tra rằng Code Llama 34B > Code Llama 13B, Code Llama cot > Code Llama cũng như giữa Deepseek 33B > Code Llama 34B (đầu ra).

Chúng tôi đo hai nguồn nhiễu: 1) lấy mẫu điểm dữ liệu nào để bao gồm trong benchmark, và 2) lấy mẫu ứng viên từ các mô hình cho mỗi điểm dữ liệu (nhiệt độ > 0). Trong số này, 1) chiếm ưu thế 2). Đối với 1) vì mô hình A không phải lúc nào cũng vượt trội hơn mô hình B trên tất cả các điểm dữ liệu ngay cả khi A > B trong tổng thể, hiệu suất được đo phụ thuộc vào điểm dữ liệu nào được bao gồm. Chúng tôi có thể đo cả nhiễu trên mỗi mô hình riêng lẻ, và cũng đo nhiễu loại 1) trên các cặp mô hình sử dụng bootstrap. May mắn thay, chúng tôi không thấy sự khác biệt lớn giữa các mô hình và yếu tố quan trọng nhất chỉ là kích thước của dataset. Nhiễu loại 1) thường khoảng 1.5% cho mỗi mô hình trong khi loại 2) khoảng 0.2% tại N=800. Nhiễu loại 1) thường trở nên nhỏ hơn trên các cặp mô hình do tương quan, tạo ra kết quả có ý nghĩa thống kê ở mức α=0.05 cho nhiều cặp mô hình.

[Có biểu đồ hiển thị sự khác biệt giữa các cặp mô hình trên các mẫu bootstrap với các kích thước khác nhau]

4 Đánh giá
Chúng tôi đánh giá một lựa chọn các mô hình trên CRUXEval: StarCoder (Base 7B, 15.5B) (Li et al., 2023a), Mistral (7B) (Jiang et al., 2023a), WizardCoder (13B, 34B) (Luo et al., 2023), Phi-1 Gunasekar et al. (2023) và Phi-1.5 (Li et al., 2023b) (1.3B), Phind v2 (Royzen et al., 2023) (34B), Code Llama (Roziere et al., 2023) (Base và Python 7B, 13B, 34B), DeepSeek Coder (Base và Instruct 6.7B, 33B), GPT-3.5

6

--- TRANG 7 ---
(Brown et al., 2020; Ouyang et al., 2022), và GPT-4 (OpenAI, 2023). Để tạo điều kiện tái tạo, các checkpoint HuggingFace của các mô hình không phải GPT có trong Phụ lục B và tất cả các prompt có trong Phụ lục D.2.

Chúng tôi sử dụng N=100 mẫu cho tất cả các mô hình không phải GPT và N=10 mẫu cho các mô hình GPT. Chúng tôi báo cáo cả điểm số pass@1 (T=0.2) và điểm số pass@5 (T=0.8). Kết quả được hiển thị trong Hình 2, và điểm số thô được cung cấp trong Phụ lục trong Bảng 2. Trong Hình 2, chúng tôi hiển thị các khoảng được tạo ra bởi 10000 mẫu bootstrap từ dataset, nơi các whiskers không chồng lấp sẽ có ý nghĩa ở mức 2.5%. Để có thêm sức mạnh thống kê, chúng tôi so sánh các cặp mô hình trên mỗi mẫu được bootstrap. Chúng tôi hiển thị cách mỗi mô hình so sánh với Code Llama 34B trong Hình 16. Các khoảng thường giảm do tương quan. Trên tất cả các mô hình so với Code Llama 34B, nếu thanh trung vị vượt qua whisker trong Hình 2, thì sự khác biệt thực sự tồn tại với >97.5% xác suất dưới paired bootstrap. Ví dụ, Code Llama 34B tốt hơn wizard 34B trên đầu vào và Code Llama 34B tệ hơn deepseek 33B trên dự đoán đầu ra với >97.5% xác suất.

[Có hai biểu đồ hiển thị hiệu suất CRUXEval-I và CRUXEval-O với pass@1 và pass@5]

5 Phân tích Định lượng
Tương quan giữa điểm số trên HumanEval và CRUXEval: Sau khi phát hành mô hình Code Llama và API của GPT-3.5 và GPT-4, đã có nhiều nỗ lực sáng tạo để lấy dữ liệu được chưng cất từ các mô hình GPT và sử dụng chúng để đào tạo các mô hình mã mạnh hơn như WizardCoder (Luo et al., 2023), Phi-1 (Gunasekar et al., 2023), Phi-1.5 (Gunasekar et al., 2023), và Phind (Royzen et al., 2023). Ví dụ, WizardCoder 34B bắt đầu với mô hình cơ sở Code Llama 34B và cải thiện điểm số pass@1 HumanEval từ 53.7% lên 73.2%, một thành tựu đáng kể và ấn tượng. Vẫn còn sự tò mò về việc liệu các mô hình này có cho thấy cải thiện tổng quát hơn trong các khía cạnh khác của lập trình hoặc hiểu mã không (Gudibande et al., 2023). Chúng tôi đo điều này thông qua CRUXEval.

7

--- TRANG 8 ---
Trong Hình 3, chúng tôi vẽ biểu đồ điểm số HumanEval được báo cáo (chúng tôi không tự tái tạo chúng) so với điểm số trên CRUXEval. Thật vậy, chúng tôi phát hiện ra một số outliers thú vị: khi so sánh các mô hình được chưng cất WizardCoder 34B và Phind 34B với Code Llama 34B, chúng tôi thấy rằng các mô hình được chưng cất ghi điểm hơn 20% so với Code Llama trên HumanEval nhưng không cho thấy cải thiện mạnh mẽ này khi được đánh giá trên cả dự đoán đầu vào và đầu ra. Ngoài ra, mô hình Phi-1 vượt trội hơn hầu hết các mô hình lớn hơn trên HumanEval, nhưng thực hiện trong số tệ nhất của tất cả các mô hình được đánh giá của chúng tôi trên CRUXEval. Nhìn chung, điều này cho thấy rằng các mô hình được tối ưu hóa cho nhiệm vụ HumanEval bằng cách chưng cất dữ liệu từ GPT-3.5 và GPT-4 (WizardCoder, Phind, Phi) có thể không học được các khả năng lý luận mã khác trong quá trình này. Mặt khác, đối với các mô hình như StarCoder, Mistral, CodeLlama và DeepSeek-Base, chúng tôi vẫn thấy xu hướng tích cực giữa điểm số HumanEval và điểm số CRUXEval, cho thấy rằng khả năng tạo mã và thực thi/hiểu có tương quan.

[Có hai biểu đồ tương quan giữa HumanEval và CRUXEval cho cả đầu vào và đầu ra]

Mối quan hệ giữa dự đoán đầu vào và dự đoán đầu ra: Trong Hình 4a, chúng tôi so sánh điểm số pass@1 của dự đoán đầu vào và dự đoán đầu ra với nhau. Về mặt khái niệm, hai nhiệm vụ có vẻ tương đối khác nhau: dự đoán đầu ra trực tiếp kiểm tra khả năng thực thi mã, trong khi dự đoán đầu vào yêu cầu hiểu biết cấp cao hơn về chức năng của mã. Tuy nhiên, chúng tôi phát hiện ra rằng có mối tương quan mạnh giữa hiệu suất của chúng. Điều này gợi ý giả thuyết rằng hiệu suất trên các nhiệm vụ liên quan đến mã tương đối khác biệt có thể có tương quan chặt chẽ. Ngoài ra, chúng tôi thấy tác động tương đối rõ ràng của việc mở rộng quy mô mô hình trên hai nhiệm vụ của chúng tôi.

8

--- TRANG 9 ---
[Có hai biểu đồ tương quan giữa dự đoán đầu vào và đầu ra, một không có CoT và một có CoT]

Ma trận nhầm lẫn/tương quan lỗi cho các mô hình khác nhau. Hình 5 hiển thị tương quan theo cặp của điểm số pass@1 cho mỗi cặp mô hình. Tương quan được chọn dựa trên tín hiệu cao nhất giữa khoảng cách cosine, Spearman và Kendall. Phần giữa của các mô hình "mở" (StarCoder, Code Llama, DeepSeek, v.v.) có tương quan mạnh với nhau. Tương quan mạnh được thấy giữa các kích thước của cùng mô hình, giữa các mô hình cùng kích thước, và giữa instruct và base (Phind 34B, Wizard 34B vs. Code Llama 34B). Kết quả CoT cũng có xu hướng có tương quan mạnh với các kết quả CoT khác, thậm chí GPT-4 vs Llama 13B. Đối với nhiệm vụ đầu ra, Deepseek tạo thành một cụm con nhỏ của các liên kết đặc biệt mạnh.

9

--- TRANG 10 ---
[Có ma trận tương quan hiển thị mối quan hệ giữa các dự đoán của mô hình cho cả đầu vào và đầu ra]

5.1 Chain of Thought Prompting
Tiếp theo, chúng tôi đánh giá cách phương pháp chain-of-thought (CoT) prompting phổ biến (Wei et al., 2022) ảnh hưởng đến hiệu suất của các mô hình Code Llama, GPT-3.5 và GPT-4 trên CRUXEval. Các prompt đầy đủ có thể được tìm thấy trong Phụ lục D.3. Tất cả kết quả được báo cáo sử dụng N=10 mẫu ngoại trừ CodeLlama 13B và 34B không có CoT, được báo cáo với N=100 mẫu. Như trước, pass@1 được báo cáo với T=0.2 và pass@5 với T=0.8. Kết quả bổ sung có thể được tìm thấy trong Phụ lục C.2.

Tác động của CoT: Chúng tôi bắt đầu bằng cách tập trung sự chú ý vào điểm số pass@1 của các mô hình có và không có CoT. Trong Hình 4b, chúng tôi vẽ điểm số dự đoán đầu vào và đầu ra của mỗi mô hình có và không có CoT. Đầu tiên, GPT-4 hưởng lợi đáng kể hơn các mô hình khác. Thứ hai, các cải thiện dự đoán đầu ra thường lớn hơn dự đoán đầu vào. Thực tế, CoT dường như không cải thiện hiệu suất Code Llama 13B và GPT-3.5 trên dự đoán đầu vào. Điều này trực quan, vì dự đoán đầu vào liên quan đến một nhiệm vụ lý luận khó khăn hơn, trong khi dự đoán đầu ra chỉ yêu cầu thực thi chương trình từng bước. Chúng tôi hoãn các con số thô đến Phụ lục trong Bảng 3.

CoT giúp Code Llama 34B và GPT-4 trên cả dự đoán đầu vào và đầu ra, GPT-3.5 chỉ trên dự đoán đầu ra, và Code Llama 13B trên cả hai nhiệm vụ đều không. CoT cũng dẫn đến cải thiện lớn hơn trên dự đoán đầu ra hơn dự đoán đầu vào. GPT-4 hưởng lợi đáng kể hơn từ CoT so với các mô hình khác, đạt được pass@1 cao nhất là 74.8% trên dự đoán đầu vào và 81.9% trên dự đoán đầu ra nhưng vẫn còn xa mới làm chủ benchmark.

10

--- TRANG 11 ---
CoT mở rộng khoảng cách giữa điểm số pass@5 và pass@1: Trong Hình 6, chúng tôi vẽ điểm số pass@5 so với điểm số pass@1 cho tất cả các mô hình. Đối với các mô hình không có CoT (hiển thị màu xanh), có tương quan tích cực giữa điểm số pass@1 và pass@5. Đối với các mô hình có CoT (hiển thị màu cam), chúng tôi thấy sự gia tăng khoảng cách giữa điểm số pass@5 và pass@1. Chúng tôi tin rằng hiện tượng này có thể do tính đa dạng bổ sung được tạo ra bởi CoT, mà chúng tôi phân tích chi tiết trong Phụ lục C.3.

Vì CoT tăng tính đa dạng của các đầu vào và đầu ra được tạo ra, các mô hình có CoT thấy khoảng cách lớn hơn giữa điểm số pass@1 và pass@5 so với các mô hình không có.

[Có hai biểu đồ hiển thị pass@5 vs pass@1 cho dự đoán đầu vào và đầu ra]

Dự đoán của CoT vs. Mô hình Cơ sở: Trong Hình 7, chúng tôi hiển thị ma trận nhầm lẫn trên các mẫu để hiểu rõ hơn tương quan giữa dự đoán đầu ra trực tiếp và dự đoán CoT. Đối với CodeLlama 13B, 34B và GPT-3.5, chúng tôi quan sát một số lượng lớn các mẫu nơi dự đoán trực tiếp thành công nhưng CoT thất bại. Tuy nhiên, với GPT-4, chúng tôi quan sát rằng có tương đối ít mẫu nơi điều này xảy ra.

11

--- TRANG 12 ---
[Có bốn ma trận nhầm lẫn cho Code Llama 13B, 34B, GPT-3.5 và GPT-4, mỗi cái hiển thị cả dự đoán đầu vào và đầu ra]

5.2 Thí nghiệm Fine-tuning
Tiếp theo, chúng tôi thực hiện phân tích sơ bộ để hiểu tác động của các sơ đồ fine-tuning đơn giản trên hiệu suất CRUXEval. Chúng tôi fine-tuned Code Llama 34B trên gần 140K mẫu của các hàm Python được chưng cất với quy trình được nêu trong Mục 3, không có lọc. Chúng tôi thực hiện khử nhiễm yếu, chỉ loại bỏ các mẫu nơi cả hàm và cặp đầu vào-đầu ra trùng khớp với các mẫu trong benchmark.

Cụ thể, chúng tôi finetune trên hỗn hợp 50% mẫu nơi hàm không có trong benchmark và 50% mẫu nơi hàm có trong benchmark nhưng cặp đầu vào-đầu ra không có, một thiết lập rất tự do. Độ chính xác đào tạo và kiểm tra theo thời gian được hiển thị trong Hình 8. Mặc dù finetuning trên các chương trình rất giống với benchmark, chúng tôi vẫn quan sát hiệu ứng đạt bằng trong độ chính xác kiểm tra, cho thấy rằng các nhiệm vụ thực thi của chúng tôi có thể quá khó để học từ sơ đồ fine-tuning đơn giản này. Chúng tôi hoãn một vài hiểu biết khác từ fine-tuning đến Phụ lục C.7 và đề xuất một vài ý tưởng fine-tuning để cải thiện benchmark của chúng tôi trong Mục 7.

12

--- TRANG 13 ---
[Có hai biểu đồ hiển thị cải thiện và giới hạn của hiệu suất CRUXEval sau Fine-Tuning cho cả dự đoán đầu vào và đầu ra]

6 Phân tích Định tính
Tất cả các mô hình ngoại trừ GPT4 có tỷ lệ thất bại trên 50%, cho thấy chúng không thể thực hiện các thực thi đơn giản. Trong phần này, chúng tôi tập trung vào GPT4 với CoT và xác minh rằng 20% thất bại còn lại là do mô hình, là nhất quán và thực sự trên các chương trình đơn giản. Chúng tôi giới thiệu độc giả đến Phụ lục E để có thêm ví dụ về các thất bại được nổi bật trong phần này và các thành công ấn tượng.

Thất bại của GPT-4 CoT. GPT-4 Cot ghi điểm 0/10 trên 54 nhiệm vụ dự đoán đầu ra và 65 nhiệm vụ dự đoán đầu vào. Trên 22 vấn đề, nó ghi điểm 0/10 trên cả nhiệm vụ dự đoán đầu vào và đầu ra. Chúng tôi kiểm tra thủ công 22 vấn đề nếu chúng vượt qua tiêu chí của chúng tôi về việc là các vấn đề đơn giản. Hầu hết thực sự đơn giản (Listings 3, 4). Có 2 vấn đề yêu cầu đếm khoảng 30 (Listing 5) và 2 vấn đề (Listing 6) yêu cầu mô phỏng một vài bước thực tế, có thể khó khăn cho việc tạo trực tiếp nhưng trong phạm vi cho CoT.

[Các listing code với ví dụ thất bại của GPT-4]

7 Hạn chế và Công việc Tương lai
Tương quan giữa các nhiệm vụ mã khác nhau: Trong khi benchmark của chúng tôi phục vụ như một lăng kính thú vị để phân tích LMs mã, người ta có thể phản đối rằng dự đoán đầu ra có thể đơn giản được thực hiện với một trình thông dịch Python và khả năng dự đoán đầu vào có thể được tăng cường đáng kể bằng cách trang bị LM với một trình thông dịch, như trong chế độ GPT-4 Code Interpreter. Trong khi điều này đúng, chúng tôi tin rằng một LM mã tốt vẫn nên có khả năng hiểu và thực thi mã tốt, tương tự như một lập trình viên mạnh. Chúng tôi thấy rằng các mô hình cơ sở có tương quan khá mạnh giữa điểm số HumanEval, dự đoán đầu vào và dự đoán đầu ra. Một hướng tương lai thú vị là điều tra sâu hơn tương quan giữa hiệu suất trên các nhiệm vụ liên quan đến mã khác nhau như hoàn thành mã, thực thi, tìm lỗi và tóm tắt mã.

Chưng cất các benchmark thực thi tương lai: Benchmark của chúng tôi chỉ đo độ chính xác dự đoán đầu vào và đầu ra của các hàm Python tương đối đơn giản và tự chứa được chưng cất từ một mô hình duy nhất (Code Llama 34B). Cũng sẽ thú vị khi đo các khả năng này trên các đoạn mã dài hơn và khó khăn hơn, các mẫu mã miền mở, hoặc mã trong các ngôn ngữ lập trình khác. Vì kỹ thuật chưng cất của chúng tôi tương đối tổng quát, chúng tôi hoan nghênh những người khác tạo ra các benchmark riêng của họ để đo việc thực thi các đoạn mã từ các phân phối khác.

Biến thể do prompt và nhiệt độ: Độ chính xác của một mô hình trên benchmark của chúng tôi có thể rất nhạy cảm với prompt và định dạng nhiệm vụ (Mizrahi et al., 2023). Chúng tôi cố gắng hết sức để giải quyết điều này bằng cách sử dụng các prompt tương tự nhất có thể trên các mô hình (xem Phụ lục D.2 và D.3) nhưng hiểu rằng một số prompt có thể cải thiện hiệu suất của các mô hình nhất định trong khi giảm hiệu suất của những mô hình khác. Cũng có vô số kỹ thuật prompting (xem (Liu et al., 2023d) để có khảo sát toàn diện) có thể được thử để cải thiện hiệu suất. Chúng tôi cũng chạy tất cả các thí nghiệm của chúng tôi với T=0.2 và T=0.8 do hạn chế ngân sách, nhưng các nhiệt độ khác nhau sẽ dẫn đến hiệu suất khác nhau cho tất cả các mô hình. Người ta phải luôn thận trọng và phê phán khi sử dụng benchmarks để so sánh các mô hình. Ví dụ, đối với dự đoán đầu vào, trong khi pass@1 47.9% của Phind v2 có vẻ đánh bại 46.5% của CodeLlama, độ lệch chuẩn của cả hai mô hình đối với 800 mẫu được chọn hóa ra khoảng 1.5%, vì vậy kết luận này không thể được đưa ra.

Mất thông tin do pass@1: Trong khi metric pass@k trung bình phổ biến trong văn học tạo mã, nó nén một lượng lớn thông tin thành một số. Trong khi chúng tôi đề xuất báo cáo pass@1 và pass@5 cho benchmark của chúng tôi, chúng tôi bình luận rằng pass@k chỉ là một góc nhìn để đo khả năng thực thi. Chúng tôi cố gắng đưa thêm ánh sáng vào hành vi bằng cách bao gồm thêm một chút phân tích trong suốt công trình này, nhưng khuyến khích việc phát triển các kỹ thuật đánh giá và phân tích khác nhau.

15

--- TRANG 16 ---
Fine-tuning: Trong thí nghiệm fine-tuning đầu tiên của chúng tôi, chúng tôi chỉ kiểm tra khớp chuỗi chính xác khi khử nhiễu tập fine-tuning, vì vậy vẫn có thể có sự trùng lặp ngữ nghĩa hoặc các chương trình tương tự với các sửa đổi nhỏ, có thể dẫn đến hiệu suất cao hơn so với nếu những ví dụ đó được loại bỏ. Trong công trình này, chúng tôi chỉ xem xét sơ đồ fine-tuning trực tiếp và đơn giản nhất. Chúng tôi tin rằng có chỗ để cải thiện thông qua các kỹ thuật tinh vi hơn, như sử dụng process supervision (Uesato et al., 2022), fine-tuning trên các thế hệ CoT chính xác, hoặc fine-tuning trên các đoạn mã trong khi bao gồm trạng thái chương trình sau mỗi bước. Thấy rằng các mô hình như Phi, WizardCoder và Phind vượt trội hơn Code Llama trên HumanEval nhưng không trên CRUXEval truyền cảm hứng cho nhu cầu điều tra sâu hơn về tiện ích của finetuning trên dữ liệu được chưng cất từ một mô hình mạnh hơn. Cuối cùng, vẫn còn là một sự tò mò liệu fine-tuning trên thông tin thực thi có thể giúp khả năng tạo mã hay không.

Cải thiện đồng thời việc tạo mã và thực thi mã: Như chúng tôi đã phát hiện, các mô hình được chưng cất như Phi, Phind và WizardCoder được fine-tuned trên việc tạo mã không cải thiện đáng kể trên CRUXEval so với các mô hình cơ sở của chúng. Không biết liệu điều ngược lại có đúng không: liệu fine-tuning cải thiện trên thực thi mã có dẫn đến khả năng tạo mã tốt hơn không? Cũng sẽ thú vị khi khám phá các kỹ thuật có thể dẫn đến hiệu suất cải thiện trên cả việc tạo mã và thực thi mã đồng thời.

Hiểu lý luận từ lăng kính mã: Như công việc tương lai, chúng tôi tin rằng benchmark của chúng tôi phục vụ như một điểm khởi đầu tốt hướng tới việc hiểu khả năng lý luận mã của LM. Nhiều đánh giá thực thi hơn nữa có thể có thể, như kiểm tra thực thi của các hàm đệ quy, thực thi từ mô tả ngôn ngữ tự nhiên và một đầu vào, hoặc thực thi của một thành phần của hai hàm. Chúng tôi thấy rằng dự đoán đầu ra phục vụ như một testbed tốt để hiểu các thất bại CoT, vì mỗi bước rõ ràng tương ứng với một phép toán với một sự thật cơ bản, vì vậy các thất bại lý luận có thể được xác định chính xác. Chúng tôi quan sát nhiều ví dụ về thất bại CoT do các sai lầm đơn giản mà mô hình dường như có kiến thức về (xem Phụ lục E.3.2 để có ví dụ), và nó nên có thể phân tích và đặc trưng hóa hành vi này một cách có hệ thống hơn.

Self-repair: Gần đây, self-repair đã được sử dụng để cải thiện khả năng lý luận và lập trình của LLMs (Chen et al., 2023; Olausson et al., 2023b; Madaan et al., 2023b; Peng et al., 2023; Zhang et al., 2023c; Tyen et al., 2023). Từ phân tích định tính của chúng tôi, chúng tôi thấy rằng khi sử dụng CoT, nhiều thất bại dự đoán đầu ra là lỗi tụng niệm thông tin mà mô hình có thể đã hiểu. Do đó, chúng tôi tin rằng những sai lầm này có thể dễ sửa chữa hơn so với khi đường lý luận chính xác không được tìm thấy ngay từ đầu, và rằng CRUXEval có thể là một nhiệm vụ đơn giản hơn để hiểu rõ hơn khả năng sửa chữa mô hình.

8 Kết luận
Chúng tôi đề xuất CRUXEval, một benchmark mới bao gồm các hàm Python đơn giản để đánh giá khả năng dự đoán đầu vào và đầu ra của LMs mã. Đầu tiên, chúng tôi đề xuất một công thức ba phần để chưng cất benchmark của chúng tôi bao gồm chưng cất quy mô lớn, lọc và lựa chọn kích thước dữ liệu thông qua phân tích nhiễu thống kê (Mục 3). Thứ hai, chúng tôi tiến hành phân tích định tính bằng cách đánh giá 20 mô hình trên benchmark của chúng tôi (Mục 4). Phân tích của chúng tôi dẫn đến những hiểu biết về tương quan giữa HumanEval và benchmark của chúng tôi, tương quan giữa dự đoán đầu vào và đầu ra, sự khác biệt giữa các LMs mã khác nhau, và tính đa dạng của các mô hình khác nhau. Thứ ba, chúng tôi khám phá tiềm năng của CoT (Mục 5.1) và fine-tuning (Mục 5.2) để cải thiện hiệu suất. Thứ tư, chúng tôi cung cấp phân tích định tính thể hiện thành công và thất bại của GPT-4 trên benchmark của chúng tôi (Mục 6 và Phụ lục E). Nhìn chung, chúng tôi tin rằng CRUXEval cung cấp một góc nhìn bổ sung cho đánh giá LM mã cổ điển như HumanEval và MBPP và khuyến khích những người tạo ra LMs mã tương lai thử benchmark của chúng tôi!

9 Lời cảm ơn
Theo thứ tự bảng chữ cái theo tên, chúng tôi cảm ơn Chris Cummings, Dylan Zhang, Jonas Gehring, Kunhao Zheng, Luyu Gao, Naman Jain, Nicolas Usunier, Ofir Press, Robin Jia, Scott Yih, Theo Olausson, và Wen-Ding Li vì những gợi ý và hỗ trợ rất hữu ích đã ảnh hưởng đến quỹ đạo của công việc này.

A. Gu được hỗ trợ bởi National Science Foundation (NSF) Graduate Research Fellowship dưới Grant No. 2141064. A. Solar-Lezama được hỗ trợ bởi National Science Foundation (NSF) và Intel Corporation thông qua NSF Grant CCF:2217064.

[Phần còn lại của tài liệu tiếp tục với các tham chiếu và phụ lục chi tiết...]
