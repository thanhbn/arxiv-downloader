# TOOLTALK: ĐÁNH GIÁ VIỆC SỬ DỤNG CÔNG CỤ TRONG BỐI CẢNH ĐỐI THOẠI

Nicholas Farn và Richard Shin
Microsoft Corporation
{nifarn,eush}@microsoft.com

TÓM TẮT

Các mô hình ngôn ngữ lớn (LLM) đã cho thấy những cải tiến đáng kể trong kỹ năng lý luận và ra quyết định và có thể tiến hành các cuộc đối thoại tự nhiên với người dùng. Nhiều nghiên cứu gần đây tìm cách tăng cường các trợ lý dựa trên LLM với các công cụ bên ngoài để chúng có thể truy cập thông tin riêng tư hoặc cập nhật và thực hiện các hành động thay mặt người dùng. Để đo lường hiệu suất của các trợ lý này tốt hơn, bài báo này giới thiệu ToolTalk, một bộ đánh giá bao gồm các ý định phức tạp của người dùng đòi hỏi việc sử dụng công cụ nhiều bước được chỉ định thông qua đối thoại. ToolTalk chứa 28 công cụ được nhóm thành 7 plugin, và bao gồm một triển khai mô phỏng hoàn chỉnh của mỗi công cụ, cho phép đánh giá hoàn toàn tự động các trợ lý dựa vào phản hồi thực thi. ToolTalk cũng nhấn mạnh các công cụ có tác động bên ngoài đến thế giới thay vì chỉ các công cụ để tham khảo hoặc tìm kiếm thông tin. Chúng tôi đánh giá GPT-3.5 và GPT-4 trên ToolTalk với tỷ lệ thành công lần lượt là 26% và 50%. Phân tích của chúng tôi về các lỗi tiết lộ ba danh mục chính và đề xuất một số hướng cải tiến trong tương lai. Chúng tôi phát hành ToolTalk tại https://github.com/microsoft/ToolTalk.

1 GIỚI THIỆU

Các mô hình ngôn ngữ lớn (LLM) có thể thực hiện những thành tích ấn tượng trong hiểu ngôn ngữ tự nhiên, tạo sinh và các tác vụ khác liên quan đến thao tác văn bản. Với những điều chỉnh phù hợp sau huấn luyện trước, chúng có thể tiến hành các cuộc đối thoại trôi chảy và tự nhiên với người dùng. Tuy nhiên, phạm vi của những cuộc đối thoại như vậy vẫn bị hạn chế do LLM thiếu khả năng truy cập kiến thức bên ngoài dữ liệu huấn luyện của chúng, thể hiện khả năng lý luận toán học và tính toán hạn chế, và không thể tương tác với thế giới bên ngoài.

Để vượt qua những hạn chế này, nhiều nghiên cứu trước đây đã đề xuất tích hợp các chatbot được hỗ trợ bởi LLM với khả năng sử dụng các công cụ như công cụ tìm kiếm (Nakano et al., 2022), máy tính, hoặc API web (Mialon et al., 2023). Việc đạt được tiến bộ có ý nghĩa trong việc sử dụng công cụ đòi hỏi các bộ đánh giá có liên quan và các tập dữ liệu đánh giá có thể hoàn toàn thử thách các hệ thống này với các cuộc đối thoại thực tế và đầy thách thức. Trong bài báo này, chúng tôi giới thiệu ToolTalk như một bước hướng tới mục tiêu này. ToolTalk bao gồm 78 cuộc đối thoại với tổng cộng 178 lượt, sử dụng 28 công cụ độc đáo được nhóm thành 7 danh mục, cùng với một phương pháp đánh giá được điều chỉnh để đo lường việc sử dụng công cụ chính xác.

Một số cân nhắc đã định hướng thiết kế ToolTalk của chúng tôi để mô phỏng tốt nhất các cuộc đối thoại điển hình mà người dùng có thể muốn có với một trợ lý dựa trên LLM. Đầu tiên, chúng tôi muốn đảm bảo rằng ToolTalk mang tính đối thoại và cho phép nhiều vòng đối thoại giữa người dùng và trợ lý cho một ý định duy nhất; phản ánh cách người dùng có thể không phải lúc nào cũng muốn công thức hóa toàn bộ yêu cầu của họ trong một phát ngôn và có thể thêm các bổ sung hoặc đưa ra sửa lỗi sau khi nhận được một số phản hồi từ trợ lý. Điều này cho phép chúng tôi bao gồm các ý định của người dùng đòi hỏi một chuỗi phức tạp các lời gọi công cụ mà không cần có những phát ngôn dài một cách không tự nhiên. Thứ hai, chúng tôi bao gồm một tập hợp sự thật cơ bản về các lời gọi công cụ mà nên được thực hiện cho mỗi phát ngôn của người dùng, phù hợp để sử dụng trong một đánh giá tự động so sánh với các lời gọi công cụ được dự đoán bởi một trợ lý. Thứ ba, ToolTalk bao gồm các triển khai có thể thực thi của mọi công cụ được bao gồm trong tập dữ liệu, để hỗ trợ việc đánh giá các trợ lý có thể xem xét kết quả từ các lời gọi công cụ trước đó để quyết định những cái nào sẽ thực hiện tiếp theo. Thứ tư, ToolTalk bao gồm các công cụ được dự định có tác dụng phụ (như gửi email, hoặc thêm/xóa các sự kiện lịch), mà chúng tôi gọi là "công cụ hành động", thay vì chỉ thực hiện các truy vấn cơ sở dữ liệu (như tìm kiếm email chứa một từ khóa cụ thể). Các công cụ hành động như vậy là cần thiết nếu trợ lý muốn tự động hóa các tác vụ của người dùng.

Chúng tôi điều chỉnh phương pháp đánh giá của mình hướng tới các đặc điểm cụ thể của thiết kế tập dữ liệu, vượt ra ngoài các số liệu phổ biến như độ chính xác khớp chính xác. Cụ thể, chúng tôi xem xét riêng biệt các lời gọi của công cụ hành động và không hành động, xem xét rằng các lời gọi không chính xác đến công cụ hành động, như gửi tin nhắn đến người sai, có thể có tác động tiêu cực đặc biệt cho người dùng. Mặt khác, nếu trợ lý thực hiện cả các lời gọi công cụ không hành động đúng và một số lời gọi không chính xác không liên quan, những cái không liên quan vẫn có thể cung cấp thông tin hữu ích cho người dùng (ngay cả khi đó không phải là thứ người dùng yêu cầu trực tiếp). Do đó, chúng tôi sử dụng độ nhớ lại lời gọi công cụ và tỷ lệ hành động không chính xác làm các số liệu chính trong một lượt đối thoại duy nhất, và định nghĩa một khái niệm thành công ở cấp độ đối thoại.

Chúng tôi áp dụng ToolTalk trên hai trợ lý được triển khai bằng cách sử dụng hỗ trợ gọi hàm của API Chat completions của OpenAI với các mô hình GPT-3.5 và GPT-4. Chúng tôi phát hiện rằng gpt-3.5-turbo-0613 và gpt-4-0613 đạt được tỷ lệ thành công ở cấp độ đối thoại lần lượt là 26% và 50%, chứng minh rằng việc sử dụng công cụ trong bối cảnh đối thoại vẫn là một tác vụ khó khăn ngay cả đối với một số mô hình tiên tiến nhất. Sau đó chúng tôi tiến hành các phân tích sâu hơn để xác định lý do tại sao GPT-3.5 và GPT-4 thất bại trong các cuộc đối thoại. Chúng tôi phát hiện rằng cả GPT-3.5 và GPT-4 đều có thể ảo giác các đối số, không hiểu tài liệu, và thậm chí thẳng thắn tuyên bố đã hoàn thành một tác vụ mà không gọi bất kỳ công cụ nào.

Bài báo của chúng tôi đóng góp những điều sau:
• Chúng tôi giới thiệu một tập dữ liệu đối thoại cho các trợ lý được hỗ trợ bởi LLM sử dụng công cụ, chứa một loạt các công cụ và các cuộc đối thoại ví dụ với các chú thích sự thật cơ bản cho các lời gọi công cụ cho phép đánh giá tự động.
• Chúng tôi đảm bảo rằng tập dữ liệu chứa các cuộc đối thoại nhiều lượt đòi hỏi sử dụng nhiều công cụ, bao gồm các công cụ có tác dụng phụ, để mô phỏng tốt hơn cách người dùng có thể tương tác với một trợ lý sử dụng công cụ.
• Chúng tôi phát triển một phương pháp đánh giá phản ánh sự khác biệt giữa các công cụ có tác dụng phụ và các công cụ không có.
• Chúng tôi đánh giá các trợ lý được xây dựng bằng GPT-3.5 và GPT-4 sử dụng tập dữ liệu của chúng tôi và phân tích các lỗi của họ, tìm thấy các vấn đề như các đối số ảo giác và tài liệu hiểu sai.

2 THIẾT KẾ TẬP DỮ LIỆU

2.1 PLUGIN VÀ CÔNG CỤ

ToolTalk được thiết kế cho một mô hình mà người dùng cá nhân sẽ có thể tùy chỉnh một trợ lý cá nhân với một số plugin có sẵn thông qua các cửa hàng trực tuyến khác nhau. Điều này có thể được xem tương tự như cách một người dùng có thể tùy chỉnh điện thoại của họ với các ứng dụng có chức năng khác nhau. Mỗi plugin chứa một tập hợp các công cụ được thiết kế xung quanh một mục đích duy nhất như quản lý lịch, mua vé xem phim, hoặc nghe nhạc. Chúng tôi định nghĩa một công cụ như một hàm duy nhất cần thiết để hoàn thành mục đích đó như tạo sự kiện, tìm kiếm phim, hoặc phát một bài hát. Chúng tôi giả định rằng hầu hết các plugin sẽ cần chứa nhiều công cụ. Ví dụ, một plugin "Lịch" lý thuyết không chỉ nên có khả năng tạo sự kiện, mà còn để tìm kiếm, sửa đổi và xóa các sự kiện này.

Đối với tập dữ liệu của chúng tôi, chúng tôi đã định nghĩa 7 plugin chứa tổng cộng 28 công cụ (xem Phụ lục A để có danh sách đầy đủ). Sử dụng các lĩnh vực tương tự như những lĩnh vực trong Li et al. (2023), chúng tôi đã tạo ra các plugin sau:
• AccountTools: chứa các công cụ để quản lý tài khoản như đăng nhập và đăng xuất, cập nhật thông tin tài khoản, hoặc tra cứu người dùng khác.
• Alarm: thêm, xóa và tìm báo thức.
• Calendar: tạo, sửa đổi, xóa và tìm kiếm sự kiện và cuộc họp
• Email: tìm kiếm hộp thư và gửi email
• Message: gửi và đọc tin nhắn từ người dùng khác
• Reminder: thiết lập, hoàn thành và xóa lời nhắc trên danh sách việc cần làm
• Weather: truy vấn thời tiết hiện tại, dự báo thời tiết và dữ liệu thời tiết lịch sử dựa trên vị trí

Để dạy LLM về cách sử dụng các công cụ, mỗi công cụ chứa một mô tả cấp cao, tài liệu chi tiết về từng tham số của nó, và một mô tả về giá trị trả về của nó. Để tạo điều kiện thuận lợi cho việc đánh giá, mỗi công cụ có một triển khai mô phỏng trong Python, cùng với một phương thức để đánh giá xem hai lời gọi của cùng một công cụ với các tham số khác nhau có nên được coi là tương đương hay không. Chúng tôi cũng lưu ý cho mỗi công cụ xem nó có được coi là một hành động (có tác dụng phụ) hay không. Chúng tôi cũng bao gồm các cơ sở dữ liệu đi kèm với thông tin giả định về người dùng hư cấu hiện có, email, lời nhắc, và vv, để các triển khai công cụ mô phỏng sử dụng.

2.2 TẠO CÁC CUỘC ĐỐI THOẠI

Để giúp tạo ra các cuộc đối thoại thực tế thử thách các công cụ và plugin của chúng tôi, chúng tôi đã sử dụng GPT-4. Đối với mỗi tập con của 3 plugin từ 7 plugin mà chúng tôi đã định nghĩa, chúng tôi tạo ra các lời nhắc liệt kê tài liệu cho tất cả các công cụ trong 3 plugin này, và hướng dẫn GPT-4 tạo ra 3 kịch bản thực tế liên quan đến người dùng cố gắng hoàn thành một tác vụ sử dụng ít nhất 5 lời gọi công cụ từ tập con ngẫu nhiên các plugin. Chúng tôi tạo ra nhiều lời nhắc bằng số lượng công cụ tồn tại trong tập con 3 plugin hiện đang được xem xét, sao cho mỗi lời nhắc hướng dẫn GPT-4 cụ thể sử dụng một trong các công cụ trong tập con 3 plugin. Chúng tôi cung cấp mẫu lời nhắc được sử dụng trong Phụ lục B.

Quy trình trên dẫn đến việc tạo ra ~400 kịch bản. Sau đó chúng tôi lặp lại việc lấy mẫu một kịch bản đều từ tất cả các công cụ, loại bỏ các kịch bản được lấy mẫu không liên quan đến công cụ yêu cầu, ảo giác các công cụ không tồn tại, hoặc có vẻ không hợp lý. Sử dụng một kịch bản được lấy mẫu làm hướng dẫn chung, chúng tôi tạo thủ công một cuộc đối thoại, viết ra tất cả các phần của nó bằng tay.

Mỗi cuộc đối thoại bao gồm một phát ngôn của người dùng, các lời gọi công cụ mà trợ lý nên thực hiện với phát ngôn đó, các giá trị trả về mong đợi cho những lời gọi đó, và các phản hồi ngôn ngữ tự nhiên của trợ lý với các phát ngôn của người dùng cộng với các lời gọi công cụ và kết quả của chúng, lặp lại theo thứ tự đó cho đến khi cuộc đối thoại kết thúc. Như siêu dữ liệu cho cuộc đối thoại, chúng tôi cũng chỉ định một dấu thời gian cho cuộc đối thoại, và vị trí và tên người dùng của người dùng. Chúng tôi đảm bảo rằng mỗi cuộc đối thoại chứa ít nhất 3 lời gọi công cụ. Chúng tôi lặp lại việc lấy mẫu các kịch bản trên cho đến khi chúng tôi đã viết 50 cuộc đối thoại.

Ngoài ra, chúng tôi tạo ra 28 cuộc đối thoại "dễ" hoàn toàn bằng tay, một cho mỗi công cụ. Phiên bản dễ này của ToolTalk bao gồm một vài lượt đối thoại người dùng-trợ lý theo sau bởi một lời gọi công cụ duy nhất. Kết hợp với 50 ví dụ "khó" trước đó, chúng tôi tạo ra tổng cộng 78 cuộc đối thoại tạo thành ToolTalk.

Sau khi xây dựng các cuộc đối thoại, chúng tôi đảm bảo rằng các cơ sở dữ liệu được sử dụng bởi các triển khai công cụ mô phỏng của chúng tôi chứa nội dung cần thiết để khi chúng tôi thực thi các lời gọi công cụ sự thật cơ bản như được liệt kê trong các cuộc đối thoại chúng tôi đã tạo, chúng trả về cùng các giá trị sự thật cơ bản.

3 PHƯƠNG PHÁP ĐÁNH GIÁ

Việc đánh giá một trợ lý sử dụng công cụ với ToolTalk bao gồm hai giai đoạn. Trong giai đoạn đầu tiên, đối với mỗi cuộc đối thoại, chúng tôi lấy tất cả các tiền tố kết thúc bằng một phát ngôn của người dùng (có thể đã được đi trước bởi các phát ngôn của người dùng trước đó, các lời gọi công cụ được thực hiện cho những phát ngôn đó, kết quả của những lời gọi đó, và phản hồi của trợ lý xem xét tất cả những điều trên). Chúng tôi chạy trợ lý với tiền tố này, nơi nó có thể dự đoán một lời gọi công cụ hoặc tạo ra một phản hồi với các lời gọi đã được thực hiện và kết quả của chúng; nếu trợ lý dự đoán một lời gọi công cụ, chúng tôi thực thi nó bằng cách sử dụng các triển khai công cụ mô phỏng của chúng tôi và sau đó cung cấp cho trợ lý kết quả. Trong giai đoạn thứ hai, đối với mỗi tiền tố cuộc đối thoại, chúng tôi so sánh các lời gọi công cụ được dự đoán cho tiền tố đó với sự thật cơ bản tương ứng của nó, tính toán độ nhớ lại lời gọi công cụ và tỷ lệ hành động không chính xác như được mô tả dưới đây.

3.1 TÍNH ĐÚNG ĐẮN CỦA LỜI GỌI CÔNG CỤ

Như được mô tả trong Phần 2.1, đối với mỗi công cụ hành động, chúng tôi đã định nghĩa một hàm để so sánh một lời gọi được dự đoán và một lời gọi sự thật cơ bản của công cụ đó (xem xét các đối số trong các lời gọi), để giúp chúng tôi xác định xem một lời gọi công cụ được dự đoán có nên được coi là tương đương với một lời gọi trong sự thật cơ bản hay không. Ví dụ, nếu một email được yêu cầu gửi đến nhiều người, chúng tôi chỉ kiểm tra rằng tập hợp các email là giống nhau thay vì yêu cầu cùng thứ tự chính xác.

Đối với các trường đối số chấp nhận đầu vào ngôn ngữ tự nhiên tự do, như nội dung tin nhắn và mô tả sự kiện, chúng tôi tính toán các embedding của chúng với DistilBERT sử dụng sent2vec và kiểm tra xem độ tương tự cosine của chúng có trên 0.9 hay không.

Đối với các đối số tùy chọn, nếu lời gọi sự thật cơ bản có một giá trị cho một cái, thì chúng tôi so sánh giá trị của nó với cái trong lời gọi được dự đoán; nếu lời gọi sự thật cơ bản thiếu một giá trị cho một đối số tùy chọn, thì nó hoàn toàn bị bỏ qua và lời gọi được dự đoán có thể có bất kỳ giá trị nào cho đối số đó (hoặc không có gì cả) trong khi vẫn được coi là đúng. Ví dụ, mô tả của một sự kiện lịch là một đối số tùy chọn, và nếu nó không được đề cập rõ ràng trong cuộc đối thoại, thì không có khả năng tác động đến tính đúng đắn của một lời gọi được dự đoán dù nó có được điền hay không.

Đối với các công cụ không hành động (thường là các công cụ để tìm kiếm qua cơ sở dữ liệu), chúng tôi không so sánh các đối số trong các lời gọi công cụ, mà so sánh kết quả thực thi của các lời gọi công cụ được dự đoán và sự thật cơ bản. Chúng được coi là tương đương nếu kết quả giống hệt nhau.

3.2 MÔ PHỎNG CUỘC ĐỐI THOẠI

Thuật toán 1 cho thấy mã giả chung cho mô phỏng cuộc đối thoại. Để mô phỏng một cuộc đối thoại, đầu tiên chúng tôi đặt lại trạng thái của thế giới (ví dụ: cơ sở dữ liệu được đặt lại về trạng thái ban đầu của chúng). Đối với mỗi lượt trong sự thật cơ bản (bao gồm phát ngôn của người dùng, lời gọi công cụ cho phát ngôn đó, và trả lời của trợ lý), chúng tôi cung cấp thông tin từ tất cả các lượt trước đó, theo sau bởi phát ngôn của người dùng trong lượt hiện tại, cho mô hình. Sau đó chúng tôi để mô hình dự đoán bao nhiêu lời gọi công cụ mà nó muốn, thực thi chúng từng cái một cho đến khi mô hình dự đoán tạo ra một phản hồi cho người dùng thay vì một lời gọi công cụ.

3.3 CÁC HÀNH ĐỘNG KHÔNG CHÍNH XÁC

Mỗi công cụ được gắn nhãn là hành động hoặc không. Chúng tôi coi một công cụ là hành động nếu việc thực thi của nó có khả năng ảnh hưởng đến thế giới bên ngoài như gửi tin nhắn hoặc xóa sự kiện lịch. Ngược lại, các công cụ không hành động chỉ thụ động tham khảo kiến thức từ thế giới bên ngoài như tra cứu thời tiết hoặc gọi máy tính. Chúng tôi tạo ra sự phân biệt này giữa công cụ hành động và không hành động vì các lời gọi không chính xác đến công cụ hành động có hậu quả nghiêm trọng hơn nhiều. Ví dụ, một lời gọi không chính xác đến công cụ DeleteAlarm có thể dẫn đến người dùng ngủ quên. Mặc dù một trợ lý về mặt lý thuyết có thể nhận ra rằng nó đã thực hiện một lời gọi công cụ hành động không chính xác và thực hiện một cái khác để đảo ngược tác động của nó, không phải tất cả các hành động đều có thể đảo ngược.

Do đó, trong quá trình đánh giá, chúng tôi cũng theo dõi các hành động "không chính xác". Chúng tôi coi một hành động là "không chính xác" nếu công cụ được gọi được gắn nhãn là hành động, nó không khớp với bất kỳ lời gọi nào trong sự thật cơ bản, và nếu lời gọi công cụ được thực thi mà không có bất kỳ lỗi nào (bao gồm bằng cách có số lượng đối số đúng và truyền các loại đúng).

3.4 CÁC SỐ LIỆU

∀g∈G;g∈M⇐⇒ ∃ p∈P where ftool(p, g) (1)
success = (M==G)∧(I==∅) (2)

Chúng tôi sử dụng hàm tính đúng đắn lời gọi công cụ, ftool, để so sánh mỗi dự đoán với tất cả các lời gọi công cụ trong sự thật cơ bản; như được mô tả trong Thuật toán 2, mỗi lời gọi công cụ sự thật cơ bản chỉ có thể khớp một lần với một lời gọi công cụ được dự đoán. Cho một tập hợp các dự đoán M khớp với sự thật cơ bản (được định nghĩa trong phương trình 1), tập hợp tất cả các dự đoán P, và tập hợp tất cả các lời gọi công cụ sự thật cơ bản G, chúng tôi tính toán precision và recall là |M|/|P| và |M|/|G| tương ứng. Ngoài ra, chúng tôi định nghĩa A là tập hợp tất cả các hành động được dự đoán và I là tập hợp các hành động không chính xác và tính toán tỷ lệ hành động không chính xác là |I|/|A|.

Ngoài ra, chúng tôi tính toán thành công như một giá trị boolean cho mỗi cuộc đối thoại, theo Phương trình 2. Trợ lý thành công trong một cuộc đối thoại nếu và chỉ nếu nó có recall hoàn hảo và không có hành động không chính xác. Chúng tôi lấy tỷ lệ thành công trên tất cả các cuộc đối thoại làm số liệu chính của chúng tôi. Vì tỷ lệ thành công là một hợp thành của hai điểm số, chúng tôi giữ recall và tỷ lệ hành động không chính xác làm các số liệu bổ sung để cung cấp thêm chi tiết. Chúng tôi cũng bao gồm precision như một thước đo hiệu quả trong dự đoán công cụ; một precision cao hơn cho thấy rằng có ít lời gọi công cụ được dự đoán không cần thiết theo sự thật cơ bản.

4 THỰC NGHIỆM VÀ PHÂN TÍCH

4.1 THỰC NGHIỆM

Chúng tôi đánh giá GPT-3.5 (gpt-3.5-turbo-0613) và GPT-4 (gpt-4-0613) trên ToolTalk sử dụng chức năng functions như một phần của API Chat completions của OpenAI (OpenAI). API này nhận đầu vào là một tin nhắn hệ thống tùy chọn, một lịch sử tin nhắn giữa người dùng và trợ lý, tài liệu công cụ, và bất kỳ lời gọi công cụ trước đó và phản hồi của chúng, và tạo ra đầu ra là một lời gọi công cụ hoặc một tin nhắn trợ lý.

Trong tin nhắn hệ thống, chúng tôi bao gồm vị trí, dấu thời gian và (nếu có) tên người dùng của cuộc đối thoại. Chúng tôi cung cấp tài liệu cho tất cả 28 công cụ cùng một lúc để mô phỏng một người dùng với tất cả 7 plugin được kích hoạt. Sau đó chúng tôi mô phỏng và đánh giá tất cả các cuộc đối thoại trong các tập con dễ và khó của ToolTalk, theo Thuật toán 1 và 2.

Bảng 1 cho thấy kết quả. Chúng tôi có tỷ lệ thành công 85.7% và 92.8% cho GPT-3.5 và GPT-4 trên phiên bản dễ của ToolTalk, và tỷ lệ thành công 26.0% và 50.0% trên phiên bản khó. GPT-4 vượt trội hơn GPT-3.5, nhưng vẫn đạt được tỷ lệ hành động không chính xác tương tự. Từ precision, chúng ta có thể thấy rằng GPT-4 cũng hiệu quả hơn GPT-3.5. Tuy nhiên, hiệu suất cho cả hai mô hình đều thấp, cho thấy độ khó của việc sử dụng công cụ trong bối cảnh cuộc đối thoại.

4.2 PHÂN TÍCH

Chúng tôi phân tích các cuộc đối thoại mà hoặc GPT-4 hoặc GPT-3.5 thất bại. Chúng tôi nhận thấy rằng đối với cả hai LLM, có ba lý do chính mà chúng có thể thất bại. Đầu tiên, mô hình có thể dự đoán một lời gọi công cụ sớm trong một lượt trước khi người dùng cung cấp thông tin cần thiết. Thứ hai, mô hình có thể thể hiện kế hoạch kém, dẫn đến việc bỏ sót hoặc sử dụng sai công cụ. Thứ ba, nó có thể đã chọn công cụ đúng để sử dụng, nhưng gọi nó với các đối số không chính xác hoặc thiếu, không tuân theo chữ ký hàm của công cụ được mô tả trong tài liệu. GPT-3.5 dễ bị những lỗi này hơn, nhưng chúng cũng biểu hiện đối với GPT-4.

Lời gọi công cụ sớm. Điều này thường xảy ra khi người dùng có một ý định rõ ràng, ví dụ: "Tôi muốn tạo một sự kiện", nhưng chưa cung cấp thông tin cần thiết để cung cấp làm đối số. Sau đó nó biểu hiện như việc ảo giác các giá trị hợp lý để cung cấp làm đối số. Điều này vô hại khi dự đoán các công cụ tham khảo nhưng là một đóng góp trực tiếp vào thất bại khi dự đoán các công cụ hành động. Đáng lo ngại, ngay cả khi các đối số ảo giác sẽ dẫn đến lỗi thực thi, mô hình sẽ tiếp tục ảo giác thêm đối số. Bất chấp những vấn đề này, cả GPT-3.5 và GPT-4 thường sẽ chọn các công cụ đúng để hoàn thành ý định.

Lý luận có lỗi. Cuối cùng, các lời gọi công cụ sớm có thể được giải thích chủ yếu bởi lý luận có lỗi, nơi LLM không phản ánh rằng nó không có tất cả thông tin cần thiết để hoàn thành một tác vụ và cần yêu cầu người dùng cung cấp thêm làm rõ. Tương tự, việc bỏ sót hoặc sử dụng sai công cụ cũng có thể được giải thích bởi kỹ năng lý luận có lỗi; thay vì phản ánh và nhận ra nó cần yêu cầu người dùng cung cấp thêm làm rõ, LLM không nhận ra rằng nó cần gọi các công cụ bổ sung để hoàn thành một tác vụ.

Ví dụ, công cụ SendEmail yêu cầu một địa chỉ email người nhận, có thể được lấy từ một tên người dùng với công cụ QueryUser. Tuy nhiên, thay vì sử dụng QueryUser và sau đó chuyển kết quả của nó cho SendEmail, mô hình có thể thay vào đó ảo giác một địa chỉ email hợp lý thuộc về người dùng. Trong các trường hợp khác, mô hình sẽ quên các chi tiết cụ thể của tác vụ và không gọi các công cụ tương ứng. Ví dụ, nếu người dùng muốn cả gửi tin nhắn và thay đổi lịch của họ, mô hình sẽ chỉ thay đổi lịch và không gửi tin nhắn. Trong các trường hợp nghiêm trọng, cả hai LLM có thể ảo giác công cụ hoặc không dự đoán bất kỳ việc sử dụng công cụ nào cả và tự tin tuyên bố rằng nó đã hoàn thành tác vụ.

Lời gọi không chính xác của công cụ đúng. Ngay cả khi mô hình chọn công cụ đúng, nó có thể gọi công cụ với các đối số không chính xác, bằng cách thiếu giá trị hoặc cung cấp giá trị sai. Điều này có thể xảy ra từ việc không hiểu tài liệu, không hiểu đầu ra của các lời gọi công cụ trước đó, hoặc kỹ năng toán học yếu. Các ví dụ bao gồm cung cấp 2 PM như "2:00" thay vì "14:00"; tính toán một sự kiện 10 giờ kết thúc lúc 6 PM là 6 PM đến 12 AM; cung cấp không chính xác một lời nhắc mà nó vừa tạo cho công cụ DeleteReminder.

Kết quả định lượng. Bảng 2 cho thấy số lượng lượt mà các loại lỗi trên xảy ra, trong đánh giá của chúng tôi về GPT-4 và GPT-3.5. Chúng tôi xác định các loại lỗi tự động bằng cách so sánh dự đoán cho một lượt duy nhất với sự thật cơ bản cho cùng lượt đó và xem những dự đoán và lời gọi công cụ sự thật cơ bản nào không tìm được khớp. GPT-4 tổng thể tạo ra ít lỗi hơn cho mỗi danh mục so với GPT-3.5. Tuy nhiên, GPT-4 thường thất bại vì những lý do tương tự như GPT-3.5 trong các trường hợp mà cả hai đều thất bại trong cùng cuộc đối thoại. GPT-4 thực sự thể hiện một cải tiến rõ ràng trong kế hoạch so với GPT-3.5 vì GPT-4 thường sẽ có thể xác định tất cả các công cụ cần thiết để hoàn thành một tác vụ.

Bài học. Kết quả và phân tích của chúng tôi đề xuất một số cách để cải thiện việc sử dụng và thiết kế công cụ cho LLM. Một số hình thức tự phản ánh hoặc căn cứ cho các giá trị đối số có vẻ là chìa khóa để giảm việc gọi công cụ sớm. Điều này cũng có thể giúp LLM xác định xem nó có tất cả các công cụ cần thiết để hoàn thành một tác vụ hay không. Đối với GPT-3.5 nói riêng, việc giảm thiểu số lượng đối số trong các công cụ có vẻ có khả năng dẫn đến những cải tiến tốt. Điều này là bởi vì không giống như GPT-4, GPT-3.5 gặp khó khăn hơn trong việc khôi phục từ lỗi, thường bỏ cuộc.

4.3 THỰC NGHIỆM LOẠI BỎ TÀI LIỆU

Chúng tôi thực hiện một nghiên cứu ablation để đo lường tác động của tài liệu công cụ bằng cách loại bỏ tất cả các mô tả công cụ và tham số chỉ giữ lại tên và loại tham số. Chúng tôi đánh giá lại GPT-3.5 và GPT-4 trên ToolTalk tạo ra Bảng 3. Chúng tôi cũng chạy lại phân tích của chúng tôi về các loại lỗi tạo ra Bảng 4.

Hiệu suất trên ToolTalk giảm đáng kể trên toàn bộ ngoại trừ tỷ lệ hành động không chính xác. Sự giảm trong tỷ lệ hành động không chính xác có thể do các công cụ khó sử dụng hơn, dẫn đến ít việc thực thi công cụ thành công tổng thể, dù nó có khớp với sự thật cơ bản hay không.

Từ Bảng 4 chúng ta có thể thấy rằng kế hoạch có lỗi chiếm phần lớn các lỗi được tạo ra bởi GPT-3.5 và GPT-4. Chúng tôi thực hiện phân tích định tính và phát hiện cả hai mô hình có xu hướng gọi các công cụ với các đối số được định dạng không chính xác, nhận lỗi trong phản hồi thực thi, sau đó kiên trì trong cùng định dạng không chính xác. Điều này dẫn đến cả hai mô hình cuối cùng bỏ cuộc và dự đoán một phản hồi trợ lý do đó thiếu tất cả các lời gọi công cụ khác trong sự thật cơ bản.

5 CÔNG TRÌNH LIÊN QUAN

Trong Phần 1, chúng tôi đã mô tả các tiêu chí mong muốn của chúng tôi để đánh giá các trợ lý dựa trên LLM sử dụng công cụ: sử dụng đối thoại để chỉ định các ý định đòi hỏi các lời gọi công cụ nhiều bước, và các hành động thay vì chỉ truy xuất thông tin, cho một đánh giá hoàn toàn tự động không yêu cầu đánh giá của con người về đầu ra của hệ thống đang được thử nghiệm. Bảng 5 tóm tắt cách các công trình khác về đánh giá LLM sử dụng công cụ so sánh theo những yếu tố này. Chúng tôi mô tả công trình liên quan chi tiết hơn dưới đây.

LLM được tăng cường công cụ cũng được biết đến như học tập tăng cường công cụ, LLM công cụ, học công cụ, mô hình ngôn ngữ tăng cường (ALM), hoặc thao tác công cụ với LLM (Xu et al., 2023; Mialon et al., 2023; Qin et al., 2023a). Phát triển trong lĩnh vực này bao gồm cải thiện hiệu suất LLM trong các tác vụ truyền thống bằng cách cho chúng truy cập vào các công cụ như máy tính hoặc công cụ tìm kiếm (Lu et al., 2023a; Yao et al., 2022b; Paranjape et al., 2023; Hao et al., 2023). Nó cũng có thể bao gồm việc áp dụng LLM vào các tác vụ tự động hóa truyền thống như robot embodied hoặc duyệt web (Liu et al., 2023b; Deng et al., 2023; Yao et al., 2022a; Liang et al., 2023), được gọi là "LLM-as-agent" bởi AgentBench (Liu et al., 2023a).

Các tác vụ truyền thống mà LLM được tăng cường công cụ đã được áp dụng bao gồm trả lời câu hỏi như ScienceQA (Saikh et al., 2022) hoặc HotPotQA (Yang et al., 2018), lý luận toán học (Cobbe et al., 2021; Lu et al., 2023b; Qiao et al., 2023), dịch đa ngôn ngữ và QA (Lewis et al., 2020; Scarton et al., 2019), QA miền mở (Zhu et al., 2021), và QA thường thức (Talmor et al., 2019) để đặt tên một vài. Những tác vụ này hữu ích để chứng minh lợi ích của việc tăng cường LLM với việc sử dụng công cụ, nhưng không phân biệt đầy đủ LLM dựa vào kiến thức nội bộ so với việc sử dụng công cụ tốt như thế nào (Zhuang et al., 2023). Chúng cũng không kết hợp việc sử dụng các công cụ ảnh hưởng đến thế giới bên ngoài vì chúng không cần thiết cho những tác vụ đó.

Các benchmark agent phổ biến đã được áp dụng cho LLM được tăng cường công cụ bao gồm WebShop (Yao et al., 2022a), Tabletop (Liang et al., 2023), Mind2Web (Deng et al., 2023), và ALFWorld (Shridhar et al., 2020). Ngoài ra, AgentBench biên dịch Mind2Web, WebShop, và ALFWorld thành một benchmark thống nhất trong khi thêm các môi trường agent bổ sung như tương tác với terminal bash, tạo lệnh SQL để truy vấn cơ sở dữ liệu, tương tác với đồ thị kiến thức, mô phỏng trò chơi bài kỹ thuật số, và câu đố tư duy ngang (Liu et al., 2023a). ToolBench làm điều gì đó tương tự bằng cách biên dịch Tabletop và Webshop trong khi giới thiệu nhiều tác vụ khác bao gồm dự đoán một lời gọi API duy nhất. Những benchmark này hữu ích để đánh giá hiệu quả của LLM được tăng cường công cụ trong nhiều tình huống tự trị. Tuy nhiên, không ai trong số chúng thử nghiệm LLM được tăng cường công cụ trong bối cảnh đối thoại. Hơn nữa, các tác vụ trong những benchmark này bao gồm việc đưa ra một phát ngôn duy nhất mà một agent sau đó cố gắng hoàn thành mà không có bất kỳ tương tác con người nào nữa. Điều này trái ngược với ToolTalk, nơi một cuộc đối thoại sẽ bao gồm nhiều phát ngôn với nhiều tác vụ trung gian.

Các công trình trước đây cũng đã tạo ra các tập dữ liệu để đánh giá các trợ lý dựa trên LLM được tăng cường công cụ. Các ví dụ bao gồm ToolLLM (Qin et al., 2023b), API-Bank (Li et al., 2023), TPTU (Ruan et al., 2023), Gorilla (Patil et al., 2023), RestGPT (Song et al., 2023), GPT4Tools (Yang et al., 2023), và ToolAlpaca (Tang et al., 2023) trong số những cái khác. Thật không may, nhiều tập dữ liệu này yêu cầu kiểm tra thủ công đầu ra của trợ lý đang được thử nghiệm để thực hiện đánh giá hoàn chỉnh. Nhiều trong số chúng cũng có các truy vấn không thực tế, và không phản ánh các câu hỏi hoặc ý định mà con người có khả năng nói trong đời thực. Nhiều trong số chúng cũng đơn giản, nơi giải pháp yêu cầu một hoặc hai lời gọi công cụ (Li et al., 2023; Ruan et al., 2023; Yang et al., 2023; Tang et al., 2023). Ngoại trừ Li et al. (2023), những cái này xem xét các phát ngôn của người dùng một cách riêng lẻ thay vì như một phần của cuộc đối thoại hoặc đối thoại.

Cũng tồn tại một tập hợp công trình về các hệ thống đối thoại hướng tác vụ. Lĩnh vực nghiên cứu này tập trung vào việc thu thập đối thoại thực tế, hướng tác vụ cho các tác vụ phân loại ý định và điền slot (Larson & Leach, 2022). Một số tập dữ liệu đối thoại hướng tác vụ phổ biến bao gồm MultiWoz (Budzianowski et al., 2018), Taskmaster và TicketTalk (Byrne et al., 2019; 2020), và STAR và STARv2 (Mosig et al., 2020; Zhao et al., 2022). Các mục tiêu tạo ra đối thoại thực tế và đánh giá về phân loại ý định và điền slot có một số điểm chồng lấp với ToolTalk. Tuy nhiên, các tập dữ liệu đối thoại hướng tác vụ thường chỉ dự đoán một ý định duy nhất cho mỗi phát ngôn của người dùng, không mô phỏng plugin hoặc công cụ, và không cung cấp phản hồi thực thi cho các lời gọi công cụ được dự đoán. TicketTalk (Byrne et al., 2020) đáng chú ý ở chỗ nó cung cấp mô phỏng API đặt vé xem phim, tuy nhiên API này không cung cấp phản hồi thực thi và không được định nghĩa chặt chẽ cho phép các đối số lỏng lẻo như "ở đây" hoặc "bây giờ".

6 KẾT LUẬN

Chúng tôi trình bày ToolTalk, một benchmark mới để đánh giá LLM được tăng cường công cụ trong bối cảnh đối thoại. Benchmark của chúng tôi nhấn mạnh việc điều phối phức tạp nhiều công cụ trong bối cảnh đối thoại. Chúng tôi cung cấp các triển khai mô phỏng của tất cả các công cụ, cho phép đánh giá hoàn toàn tự động nơi LLM có thể quyết định công cụ nào để gọi thêm dựa trên kết quả của các lời gọi công cụ trước đó. Cuối cùng, chúng tôi cũng giới thiệu một hình thức đánh giá tính đúng đắn độc đáo có tính đến các khía cạnh riêng của từng công cụ và liệu một hệ thống sử dụng công cụ có tạo ra các hành động không chính xác hay không. Chúng tôi đánh giá GPT-3.5 và GPT-4 sử dụng tập dữ liệu và phương pháp của chúng tôi và phân tích các lỗi của chúng, tìm thấy ba danh mục chính: lời gọi công cụ sớm, lý luận có lỗi, và lời gọi không chính xác của công cụ đúng. Trong tương lai, chúng tôi hy vọng mở rộng phạm vi của tập dữ liệu này đến nhiều cuộc đối thoại hơn và mô phỏng thậm chí nhiều plugin đa dạng hơn. Chúng tôi cũng hy vọng thấy nghiên cứu tương lai xem xét cách thiết kế lại tốt hơn các giao diện API hiện có cho LLM.

7 KHẢ NĂNG TÁI TẠO

Chúng tôi làm cho ToolTalk có sẵn rộng rãi hơn bằng cách phát hành nó trên github. Chúng tôi bao gồm các phiên bản chính xác của GPT-3.5 (gpt-3.5-turbo-0613) và GPT-4 (gpt-4-0613) có sẵn thông qua API OpenAI để có thể tái tạo kết quả của chúng tôi sau khi phát hành. Chúng tôi bao gồm lời nhắc được sử dụng để tạo ra các kịch bản của chúng tôi trong Phụ lục B. Chúng tôi bao gồm thông tin về lời nhắc hệ thống và ứng dụng API Chat completions của OpenAI trong Phần 4.1.
