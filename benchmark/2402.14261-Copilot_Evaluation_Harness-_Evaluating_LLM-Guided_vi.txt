# 2402.14261.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/benchmark/2402.14261.pdf
# Kích thước file: 1549639 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Copilot Evaluation Harness: Đánh giá Lập trình Phần mềm được Hướng dẫn bởi LLM
Anisha Agarwal
Microsoft
Redmond, USA Aaron Chan
Microsoft
Redmond, USA Shubham Chandel
Microsoft
Redmond, USA
Jinu Jang
Microsoft
Redmond, USA Shaun Miller
Microsoft
Redmond, USA Roshanak Zilouchian
Moghaddam
Microsoft
Redmond, USA
Yevhen Mohylevskyy
Microsoft
Redmond, USA Neel Sundaresan
Microsoft
Redmond, USA Michele Tufano
Microsoft
Redmond, USA

Tóm tắt
Việc tích hợp các Mô hình Ngôn ngữ Lớn (LLM) vào Môi trường Phát triển (IDE) đã trở thành điểm tâm trong phát triển phần mềm hiện đại. Các LLM như OpenAI GPT-3.5/4 và Code Llama mang lại tiềm năng tăng cường đáng kể năng suất của nhà phát triển bằng cách đóng vai trò là trợ lý lập trình thông minh, được điều khiển bằng chat. Tuy nhiên, việc sử dụng LLM ngay từ đầu khó có thể tối ưu cho bất kỳ tình huống cụ thể nào. Thay vào đó, mỗi hệ thống yêu cầu LLM phải được tinh chỉnh theo bộ heuristic của nó để đảm bảo hiệu suất tốt nhất. Trong bài báo này, chúng tôi giới thiệu harness đánh giá Copilot: một bộ dữ liệu và công cụ để đánh giá các tương tác IDE được hướng dẫn bởi LLM, bao gồm các tình huống và ngôn ngữ lập trình khác nhau. Chúng tôi đề xuất các metric của mình như một đánh giá mạnh mẽ và dày đặc thông tin hơn so với các hệ thống đánh giá tiên tiến trước đây. Chúng tôi thiết kế và tính toán cả metric thành công tĩnh và dựa trên thực thi cho các tình huống bao gồm một loạt rộng các nhiệm vụ nhà phát triển, bao gồm tạo code từ ngôn ngữ tự nhiên (generate), tạo tài liệu từ code (doc), tạo test case (test), sửa lỗi (fix), và hiểu workspace và giải quyết truy vấn (workspace). Các metric thành công này được thiết kế để đánh giá hiệu suất của LLM trong một IDE cụ thể và không gian tham số tương ứng của nó. Những học hỏi của chúng tôi từ việc đánh giá ba LLM phổ biến sử dụng các metric này có thể cung cấp thông tin cho việc phát triển và xác thực các tình huống tương lai trong IDE được hướng dẫn bởi LLM.

Từ khóa: Mô hình Ngôn ngữ Lớn, VSCode, Copilot, Đánh giá Tạo Code

1 Giới thiệu
Sự phát triển liên tục của các thực tiễn phát triển phần mềm đã dẫn đến sự quan tâm ngày càng tăng trong việc tích hợp công nghệ tiên tiến để nâng cao năng suất của nhà phát triển [10]. Một công nghệ như vậy đã thu hút sự chú ý đáng kể là việc sử dụng các Mô hình Ngôn ngữ Lớn (LLM) trong Môi trường Phát triển Tích hợp (IDE) [9,25]. Các LLM, được minh họa bởi các mô hình như GPT-3.5 [26] và GPT-4 [27] của OpenAI, cũng như các mô hình nguồn mở mạnh như Code Llama [35], đưa ra lời hứa đóng vai trò là trợ lý lập trình thông minh. Trong bài báo này, chúng tôi giới thiệu harness đánh giá Copilot để khám phá toàn diện các khả năng và ứng dụng tiềm năng của lập trình được hướng dẫn bởi LLM trong bối cảnh IDE, với trọng tâm đặc biệt vào khả năng thích ứng của chúng qua các tình huống và ngôn ngữ lập trình đa dạng.

Hình 1. Một nhà phát triển đã nhập mô tả của một hàm, trong trường hợp này nên tạo số fibonnaci. LLM đã tạo code cho hàm này được highlighted ở định dạng diff.

Trọng tâm của cuộc điều tra của chúng tôi nằm ở việc đánh giá năm tình huống phát triển phần mềm chính. Những tình huống này bao gồm một phổ các nhiệm vụ nhà phát triển, mỗi tình huống giải quyết các thách thức và cơ hội cụ thể:

• Tạo Tài liệu từ Code (doc): LLM hỗ trợ trong việc tự động hóa nhiệm vụ tạo tài liệu từ code.arXiv:2402.14261v1 [cs.SE] 22 Feb 2024

--- TRANG 2 ---
Agarwal và Chan, et al.

Hình 2. Một nhà phát triển sử dụng /doc để tạo tài liệu cho một hàm tạo số Fibonacci. LLM tạo tài liệu cho hàm này được highlighted ở định dạng diff.

Hình 3. Một nhà phát triển yêu cầu mô hình sửa lỗi trong code fibonacci của họ, và mô hình trình bày bản sửa lỗi (chính tả từ "yield" đúng) ở định dạng diff.

• Sửa Lỗi (fix): LLM đóng vai trò quan trọng trong việc xác định và khắc phục các cảnh báo và lỗi được đưa ra bởi các công cụ phân tích tĩnh.
• Tạo Code từ Ngôn ngữ Tự nhiên (generate): LLM tạo các đoạn code từ mô tả ngôn ngữ tự nhiên.
• Tạo Test Case cho Code (test): LLM được sử dụng để tự động tạo test case cho code, nhằm nâng cao thực tiễn đảm bảo chất lượng phần mềm.
• Hiểu Workspace và Giải quyết Truy vấn (workspace): LLM giúp các nhà phát triển hiểu dự án hiện tại bằng cách phản hồi các truy vấn của nhà phát triển theo sự hiểu biết về codebase trong workspace cục bộ.

Các metric trong harness đánh giá của chúng tôi được thiết kế để đánh giá tính hiệu quả, độ chính xác và hiệu suất của các tương tác lập trình được hướng dẫn bởi LLM qua các tình huống phát triển thực tế. Framework của chúng tôi cho phép bất kỳ IDE nào được kết nối và đánh giá sử dụng các metric của chúng tôi. Do đó, chúng tôi cung cấp một hệ thống để tinh chỉnh không gian tham số IDE để đạt được kết quả tích hợp LLM vượt trội.

Trong khi các công trình trước đây đã cung cấp một harness đánh giá cho tạo code [10], một harness đánh giá toàn diện hơn là cần thiết với các Mô hình Ngôn ngữ Lớn mới hỗ trợ nhiều tình huống kỹ thuật phần mềm trong một IDE. Có một không gian tham số rộng để tinh chỉnh và tối ưu hóa khi tích hợp một LLM với một IDE: các prompt cho mô hình được diễn đạt như thế nào [38,43]? Thông tin nên được cung cấp theo thứ tự nào [23]? Các phản hồi của mô hình được phân tích và chèn lại vào code gốc như thế nào? Bối cảnh nào nên được cung cấp cho mô hình, ngoài truy vấn [30,33]? Tất cả những yếu tố này và hơn thế nữa đóng một vai trò trong việc mô hình có thể hoạt động tốt như thế nào trong IDE.

Các harness đánh giá trước đây để lại khoảng trống trong không gian rộng này mà chúng tôi tìm cách bao phủ với harness Đánh giá Copilot. Trong bộ dữ liệu HumanEval [10], ví dụ, các mô hình được đánh giá về khả năng tạo hàm từ docstring. Điều này tương tự như metric đánh giá tạo phương thức của chúng tôi. Tuy nhiên, trong HumanEval, các test case đơn giản, trực tiếp, là câu hỏi thuật toán kiểu phỏng vấn coding. Trong các test case của chúng tôi, mô hình phải tạo hàm từ code thế giới thực, nhiều trong số đó tương tác với hàng chục phương thức và file khác để hoàn thành một nhiệm vụ nhất định. Mức độ phức tạp này là cần thiết để đánh giá đúng khả năng tạo code của các LLM tiên tiến trong bối cảnh thế giới thực.

Các công trình khác sử dụng chính LLM để đánh giá đầu ra [11,15,44,45]. Mặc dù điều này có thể hiệu quả, không có gì đảm bảo về logic hoặc lý luận của mô hình, và tính ngẫu nhiên tự nhiên của LLM khiến việc tính toán điểm "đúng" cho bất kỳ test case cụ thể nào trở nên khó khăn (tức là mô hình có thể thay đổi câu trả lời từ lần chạy này sang lần chạy khác). Lỗi và khoảng trống logic có thể lan truyền từ dữ liệu test vào kết quả đánh giá.

Với framework đánh giá của chúng tôi, chúng tôi trình bày một tiêu chuẩn đánh giá mới cho code được tạo bởi mô hình. Harness đánh giá của chúng tôi cho phép hiểu biết tự động về cách thay đổi prompt và tham số tác động đến hiệu suất, qua hàng trăm test case trải rộng một loạt các tình huống lập trình với code thế giới thực. Trong phiên bản này của công trình, chúng tôi thảo luận kết quả chi tiết từ hai trong năm metric được nêu ở trên: tạo tài liệu và sửa lỗi.

Chúng tôi áp dụng framework đánh giá của mình để đánh giá tính hiệu quả của Visual Studio Code, một IDE được sử dụng bởi 15 triệu lập trình viên trên toàn thế giới. Đánh giá của chúng tôi trải rộng một phổ các mô hình LLM, từ các mô hình độc quyền như GPT-3.5 và GPT-4 của OpenAI đến các lựa chọn thay thế có sẵn công khai như Code Llama. Chúng tôi tin rằng một bộ mô hình đa dạng là cần thiết để cung cấp góc nhìn toàn diện về khả năng và hạn chế của lập trình được hướng dẫn bởi LLM, phục vụ nhu cầu và sở thích của một đối tượng nhà phát triển rộng rãi.

--- TRANG 3 ---
Copilot Evaluation Harness: Đánh giá Lập trình Phần mềm được Hướng dẫn bởi LLM

2 Công trình Liên quan
Dưới đây, chúng tôi giải thích cách công trình của chúng tôi xây dựng dựa trên và mở rộng công trình liên quan về LLM, Đánh giá LLM, và Đánh giá LLM cho các nhiệm vụ kỹ thuật phần mềm.

2.1 LLM
Các Mô hình Ngôn ngữ Lớn (LLM) [13,16,20] là các mô hình ngôn ngữ tiên tiến với kích thước tham số khổng lồ có thể hiểu và tạo ra ngôn ngữ con người. Nhiều LLM nổi tiếng như GPT-3 [14], InstructGPT [28], và GPT-4 [27] tận dụng kiến trúc Transformer [40]. So với các mô hình machine learning truyền thống, LLM yêu cầu một lượng lớn dữ liệu và yêu cầu phần cứng rất cao để training. Đổi lại LLM cung cấp hiệu suất cao hơn nhiều so với các mô hình machine learning truyền thống khi so sánh chất lượng phản hồi của chúng trên các nhiệm vụ tương tự. Tuy nhiên, kết quả từ LLM ít có thể giải thích được hơn so với các mô hình truyền thống.

Dựa trên thành công của LLM, các nhà nghiên cứu đã bắt đầu khám phá lợi thế của việc mở rộng quy mô LLM. Ví dụ, Gropher [32] có 280 tỷ tham số, Megatron-turing NLG [37] có 530 tỷ tham số và PaLM [12] có 540 tỷ tham số vượt trội hơn con người trung bình trên benchmark BIGbench [39]. Tương tự, các nhà nghiên cứu cũng khám phá fine-tuning LLM cho các nhiệm vụ cụ thể và/hoặc với phản hồi của con người [28].

Trong nghiên cứu của chúng tôi, chúng tôi kiểm tra hiệu suất của ba LLM nổi bật: GPT-3.5, GPT-4 của OpenAI, và CodeLlama trên năm tình huống kỹ thuật phần mềm khác nhau. Chúng tôi đã chọn LLM của OpenAI như đại diện của các mô hình ngôn ngữ đa mục đích áp dụng cho các nhiệm vụ Kỹ thuật Phần mềm, với quy mô lớn và tính chất độc quyền của chúng. Ngược lại, chúng tôi đã bao gồm CodeLlama như một minh họa của một mô hình nguồn mở, nhỏ hơn và được tối ưu hóa được fine-tune đặc biệt cho các ứng dụng liên quan đến code.

2.2 Đánh giá LLM
Các công trình trước đây đã đánh giá tính hiệu quả của LLM từ nhiều góc độ khác nhau bao gồm hiệu suất trong các nhiệm vụ ngôn ngữ tự nhiên, lý luận, tính mạnh mẽ, an toàn, v.v. [8]. Ví dụ, khi nói đến phân tích cảm xúc, [22] và [31] cho thấy LLM hoạt động tốt hơn nhiều so với các mô hình phân tích cảm xúc truyền thống. Tương tự, [21] đánh giá hiệu suất của ChatGPT trên một loạt các nhiệm vụ bao gồm trả lời câu hỏi, tóm tắt văn bản, tạo code, lý luận, và giải quyết các vấn đề đạo đức.

Không giống như các mô hình machine learning truyền thống nơi k-fold cross validation là một quy trình đánh giá phổ biến, LLM thường được đánh giá sử dụng các tập dữ liệu tĩnh. Tập dữ liệu phổ biến để đánh giá LLM bao gồm: GLUE [42], SuperGLUE [41], BIGBench [39], Massive Multitask Language Understanding (MMLU) [18], Ethics Benchmark [17], và những cái khác.

Trong bài báo này, chúng tôi tách khỏi các metric dựa trên ngôn ngữ thông thường, như BLEU, thường được sử dụng trong các nghiên cứu trước đây. Thay vào đó, chúng tôi thiết kế các metric được điều chỉnh đặc biệt cho Kỹ thuật Phần mềm và các nhiệm vụ đang được xem xét.

2.3 Đánh giá LLM cho Các nhiệm vụ Kỹ thuật Phần mềm
LLM đã được sử dụng rộng rãi trong các nhiệm vụ kỹ thuật phần mềm khác nhau, như tạo code, tóm tắt code, hoàn thành code, tìm kiếm code, tài liệu code, review code, phát hiện lỗi, và test phần mềm. Tuy nhiên, đánh giá tính hiệu quả và hiệu suất của LLM cho các nhiệm vụ SE không phải là một vấn đề tầm thường, vì có nhiều yếu tố và thách thức liên quan. Trong phần này, chúng tôi xem xét một số công trình hiện có đã đề xuất hoặc áp dụng các phương pháp và metric đánh giá khác nhau cho LLM cho các nhiệm vụ SE.

Một trong những công trình toàn diện nhất là bài báo [19], cung cấp một đánh giá tài liệu hệ thống về giao điểm của LLM và SE, bao gồm các khía cạnh khác nhau như thu thập dữ liệu, tiền xử lý, ứng dụng, tối ưu hóa, đánh giá, và kỹ thuật prompt. Bài báo cũng phân loại và so sánh các LLM khác nhau đã được sử dụng trong các nhiệm vụ SE, như GPT-3, CodeBERT, và GraphCodeBERT, và phân tích điểm mạnh và điểm yếu của chúng. Bài báo cũng xác định các thách thức hiện tại và hướng tương lai cho LLM cho SE.

CodeXGLUE [24], là một nền tảng đánh giá toàn diện cho LLM trong các nhiệm vụ Kỹ thuật Phần mềm. CodeXGLUE bao gồm một tập dữ liệu benchmark với 14 nhiệm vụ bao gồm các tình huống thông minh code và cung cấp các mô hình baseline như CodeBERT và CodeGPT. Nó nhằm kích thích nghiên cứu và phát triển trong LLM cho SE, cung cấp một tập dữ liệu đa dạng cho các ngôn ngữ lập trình và nhiệm vụ khác nhau. Các metric đánh giá của CodeXGLUE, cả tự động và dựa trên con người, cùng với bảng xếp hạng và nền tảng trực tuyến, tạo điều kiện so sánh công bằng giữa các mô hình.

Một trong những công trình đầu tiên đánh giá LLM cho code xem xét thực thi code và test case là bài báo [10], giới thiệu HumanEval, một tập dữ liệu benchmark và một thách thức để đo tính đúng đắn chức năng của LLM được training trên code. HumanEval bao gồm 164 bài toán lập trình được viết tay bằng Python, mỗi bài có signature hàm, docstring, body, và một số unit test. Các bài toán bao gồm nhiều chủ đề khác nhau, như hiểu ngôn ngữ, thuật toán, và toán học đơn giản, và một số trong số chúng có thể so sánh với các câu hỏi phỏng vấn phần mềm đơn giản. Mục tiêu của HumanEval là đo khả năng của LLM trong việc tổng hợp các chương trình từ docstring mà pass các test case đã cho.

Trong nghiên cứu của chúng tôi, chúng tôi xây dựng trên nền tảng được đặt bởi các công trình trước đây trong tài liệu, tìm cách nâng cao những đóng góp của họ. Giống như HumanEval, chúng tôi kết hợp xem xét thực thi code và test case, nhưng chúng tôi mở rộng cả về độ rộng của các nhiệm vụ SE được giải quyết và việc tinh chỉnh các metric đánh giá. Trái ngược với HumanEval, đánh giá của chúng tôi bao gồm các codebase lớn và thế giới thực. Hơn nữa,

--- TRANG 4 ---
Agarwal và Chan, et al.

trọng tâm của chúng tôi là phát triển một framework đánh giá toàn diện cho lập trình được hướng dẫn bởi LLM trong các tương tác IDE, với trọng tâm đặc biệt vào tính thực tiễn của chúng qua các ngôn ngữ và tình huống lập trình đa dạng.

3 Đánh giá Lập trình Phần mềm được Hướng dẫn bởi LLM

Ngoài HumanEval [10], các metric dựa trên khớp như BLEU [29] hoặc Code-BLEU [34] thường được áp dụng để benchmark hiệu suất của LLM trong các nhiệm vụ kỹ thuật phần mềm. Khi LLM trở nên phổ biến và mạnh mẽ hơn, nhiều nghiên cứu sử dụng chính các mô hình LLM để đánh giá đầu ra LLM [11,15,44,45]. Tuy nhiên, công trình trước đây gợi ý các metric thay thế như tính đúng đắn chức năng phản ánh tốt hơn thành công của các mô hình tạo sinh trong tạo code [10], dịch code [36], và các nhiệm vụ khác. Xây dựng trên công trình trước đây trong lĩnh vực này, chúng tôi mở rộng harness HumanEval và đánh giá năng lực mô hình tích hợp IDE trong năm nhiệm vụ kỹ thuật phần mềm được liệt kê ở trên.

3.1 Tạo Tài liệu từ Code (doc)

Nhiệm vụ này liên quan đến việc tạo tài liệu cho một phương thức. Hình 2 hiển thị một ví dụ trong VS Code IDE. Trong trường hợp này nhà phát triển yêu cầu LLM tạo tài liệu cho một hàm Fibonacci sử dụng /doc.

3.1.1 Metrics. Trong tình huống này, chúng tôi xem xét việc tạo docstring thành công nếu vị trí, định dạng, và phạm vi bao phủ của văn bản được tạo là đúng. Chúng tôi báo cáo các metric sau cho tình huống này:

• Tính Đúng Cú pháp: Chúng tôi kiểm tra rằng docstring đã được chèn vào code theo cách không làm gián đoạn cú pháp của file khi thêm vào.
• Tính Đúng Định dạng: Nếu comment tài liệu được đặt theo cách có thể chấp nhận về mặt cú pháp cho ngôn ngữ đã cho, chúng tôi kiểm tra thêm tính đúng đắn của việc tài liệu hóa câu lệnh return, các tham số hàm với kiểu của chúng, tên hàm, và liệu có mô tả hàm được viết hay không.

3.1.2 Quy trình Đánh giá. Chúng tôi bắt đầu với một tập các phương thức. Đối với mỗi phương thức, chúng tôi cung cấp signature và body của phương thức cho LLM như context. Sau đó chúng tôi prompt LLM với yêu cầu tạo tài liệu cho phương thức, và trả về hàm input với docstring được tạo chèn vào vị trí đúng trong hàm.

Sau khi LLM tạo tài liệu và docstring được tạo được chèn vào file code, chúng tôi đánh giá tính đúng cú pháp của file với docstring được tạo, cũng như tính đúng đắn của chính docstring.

3.2 Sửa Lỗi (fix)

Nhiệm vụ này liên quan đến việc sử dụng LLM để sửa lỗi được xác định bởi các công cụ phân tích tĩnh, với kỳ vọng rằng code được sửa lỗi sẽ có ít lỗi tổng thể hơn so với code gốc. Chúng tôi sử dụng các bộ phân tích tĩnh sau:

• javascript: eslint [2];
• ts: eslint [2], tsc (typescript compiler);
• python: pylint [4], pyright [5];
• java: spotbugs [7];
• c#: roslyn [6];
• cpp: clang [1].

Nếu lỗi gốc được sửa nhưng một lỗi khác được đưa vào thay thế, test case sẽ fail.

Hình 4 hiển thị một ví dụ trong VS Code IDE. Một lập trình viên có lỗi do chính tả sai từ "yield", và mô hình sửa lỗi này.

3.2.1 Metrics. Trong tình huống này, chúng tôi xem xét việc sửa lỗi thành công nếu code kết quả đúng về mặt cú pháp và cảnh báo hoặc lỗi phân tích tĩnh tương ứng đã biến mất.

• Tính Đúng Cú pháp: chúng tôi xác nhận rằng file code với bản sửa lỗi vẫn đúng về mặt cú pháp.
• Tỷ lệ Sửa: chúng tôi kiểm tra rằng một cảnh báo hoặc lỗi phân tích tĩnh hiện có trong code đã được giải quyết thành công bởi các thay đổi được đề xuất, mà không đưa vào bất kỳ lỗi nào khác.

3.2.2 Quy trình Đánh giá. Cho một tập các lỗi được tìm thấy bởi các công cụ phân tích tĩnh, chúng tôi cung cấp nội dung file và thông tin chẩn đoán cho LLM để tạo một bản sửa. Chúng tôi đánh giá liệu mô hình có sửa lỗi gốc, liệu nó có tạo ra lỗi mới nào, và liệu code được mô hình sửa đổi có vẫn đúng về mặt cú pháp sau khi bản sửa được chèn vào.

3.3 Tạo Code từ Ngôn ngữ Tự nhiên (generate)

Nhiệm vụ này liên quan đến việc tạo một đoạn code từ mô tả ngôn ngữ tự nhiên. Hình 1 hiển thị một ví dụ về nhiệm vụ như vậy trong VS Code IDE. Trong trường hợp này, nhà phát triển yêu cầu LLM viết một hàm tạo ra n giá trị đầu tiên của dãy Fibonacci, và editor hiển thị hàm được tạo trong view diff.

3.3.1 Metrics. Tương tự như các đánh giá trước đây về tạo code [10], chúng tôi xem xét một đoạn code được tạo thành công nếu code được tạo đúng về mặt cú pháp và tất cả test case bao phủ code được tạo đều pass. Do đó, chúng tôi báo cáo các metric sau cho tình huống này:

• Tính Đúng Cú pháp: Chúng tôi tính toán và báo cáo phần trăm code được tạo đúng về mặt cú pháp. Đối với metric này, chúng tôi kiểm tra tính đúng cú pháp của code được tạo sử dụng một parser đặc thù ngôn ngữ (ví dụ, tree-sitter cho mỗi ngôn ngữ).
• Tỷ lệ Pass Test: Chúng tôi kiểm tra số lượng test pass và fail và tính tỷ lệ test pass. Để tính số này, chúng tôi thực thi toàn bộ test suite

--- TRANG 5 ---
Copilot Evaluation Harness: Đánh giá Lập trình Phần mềm được Hướng dẫn bởi LLM

Hình 4. Một nhà phát triển yêu cầu mô hình sửa lỗi trong code fibonacci của họ, và mô hình trình bày bản sửa lỗi (chính tả từ "yield" đúng) ở định dạng diff.

của dự án người dùng và theo dõi test nào fail mà trước đây pass trước khi mô hình inject code.

3.3.2 Quy trình Đánh giá. Chúng tôi bắt đầu với một tập các repository có test case. Từ mỗi repository, chúng tôi chọn các phương thức: 1) được bao phủ bởi các test case trong test suite của repository đã cho, và 2) có docstring. Đối với mỗi phương thức, chúng tôi yêu cầu một LLM tạo body của phương thức cho signature và docstring của phương thức. Chúng tôi cung cấp nội dung của file phương thức như context cho LLM, thay thế body phương thức gốc bằng một dòng comment đọc "Your Code Here."

Sau khi LLM tạo body phương thức, chúng tôi đặt code được tạo lại vào vị trí của body phương thức gốc và đánh giá code bằng cách chạy test suite của repository với body phương thức mới. Sau đó chúng tôi tính toán và báo cáo tính đúng cú pháp và tỷ lệ pass test, như được giải thích ở trên.

3.4 Tạo Test Case cho Code (test)

Nhiệm vụ này liên quan đến việc sử dụng LLM để tạo test case cho code. Các nhà phát triển thường rút ngắn khi nói đến việc viết unit test. Tự động hóa việc tạo test có thể động viên nhiều nhà phát triển hơn bao gồm unit test. Hình 5 hiển thị một ví dụ về nhà phát triển yêu cầu test trong VS Code IDE. Trong ví dụ này, nhà phát triển yêu cầu LLM tạo một test cho hàm Fibonacci sử dụng lệnh chat scenario /test.

3.4.1 Metrics. Trong tình huống này, chúng tôi xem xét một test được tạo thành công nếu nó đúng về mặt cú pháp và có thể pass khi thực thi. Lưu ý rằng, đối với đánh giá này, điều này có nghĩa là chúng tôi giả sử code mà test được viết cho là đúng.

• Tính Đúng Cú pháp: Chúng tôi tính phần trăm test được tạo đúng về mặt cú pháp. Chúng tôi kiểm tra tính đúng cú pháp của test được tạo sử dụng một parser đặc thù ngôn ngữ.

• Tỷ lệ Pass Test được Tạo: Chúng tôi tính tỷ lệ pass của test được tạo. Chúng tôi giả sử phương thức gốc là đúng, và thực thi test được tạo trên phương thức focal của nó.

3.4.2 Quy trình Đánh giá. Cho một tập các phương thức, chúng tôi cung cấp signature phương thức, docstring, và body như context cho LLM để tạo một test cho mỗi phương thức focal.

Một khi LLM tạo một test cho phương thức, chúng tôi thêm test vào repository chứa phương thức, và cố gắng thực thi test.

Đối với Javascript và Typescript, chúng tôi tạo test sử dụng thư viện Jest hoặc Mocha. Test suite gốc của repository không cần phải được viết với một trong hai thư viện, nhưng file gốc của mỗi phương thức phải có thể pass mà không có lỗi khi một test case tầm thường (về cơ bản chỉ assert true) được append vào file. Khi đánh giá test được tạo, chúng tôi tạm thời append chúng vào file phương thức focal để giảm thiểu lỗi import, và chạy toàn bộ file. Nếu chạy file với một test case tầm thường được append (ví dụ một test luôn luôn đúng) trả về false hoặc lỗi, chúng tôi biết kết quả từ test được tạo trên file đó không đáng tin cậy.

3.5 Hiểu Workspace và Giải quyết Truy vấn (workspace)

Trong nhiệm vụ Workspace, chúng tôi đưa cho mô hình truy vấn ngôn ngữ tự nhiên của người dùng, và yêu cầu nó xác định các đoạn liên quan của codebase có thể hỗ trợ trong việc trả lời câu hỏi của người dùng. Điều này test khả năng của mô hình để hiểu cả yêu cầu ngôn ngữ tự nhiên từ người dùng và lượng lớn code.

3.5.1 Metrics. Chúng tôi đánh giá chất lượng các đoạn được retrieve của LLM theo hai cách:

• Mean Reciprocal Rank (MRR): Cho một danh sách được xếp hạng các đoạn được retrieve của mô hình, chúng tôi tính 1/r, trong đó r là thứ hạng của đoạn đúng trong danh sách của mô hình. Vậy, nếu mô hình xếp hạng đoạn đúng thứ hai, chúng tôi

--- TRANG 6 ---
Agarwal và Chan, et al.

Hình 5. Một nhà phát triển sử dụng /test để tạo một test cho hàm tạo số Fibonacci. LLM tạo hàm test_fibonacci cho hàm này trong một file test.

sẽ xem xét điểm của mô hình cho test case đó là 1/2. MRR là trung bình của tất cả điểm test case.

• Phát hiện Keyword End to End: Chúng tôi bắt đầu với một tập dữ liệu được tạo thủ công của các truy vấn người dùng và keyword liên quan đến câu trả lời đúng cho truy vấn. Chúng tôi lấy danh sách được xếp hạng các đoạn được retrieve của mô hình và chuyển nó cho mô hình cùng với mỗi truy vấn người dùng. Sau đó, chúng tôi phát hiện liệu keyword liên quan có xuất hiện trong phản hồi của mô hình hay không, cho cả truy vấn và kết quả được retrieve.

3.5.2 Quy trình Đánh giá. Đối với mỗi datapoint, chúng tôi cung cấp cho LLM truy vấn người dùng và toàn bộ context của codebase liên quan đến truy vấn đã cho. Chúng tôi yêu cầu LLM retrieve một danh sách được xếp hạng các đoạn code liên quan từ codebase. Chúng tôi trực tiếp đánh giá chất lượng kết quả được retrieve của mô hình sử dụng MRR, một metric đánh giá mô hình có thể tìm thấy các đoạn code liên quan nhất trong quá trình retrieval tốt như thế nào.

Chúng tôi cũng đánh giá chất lượng của tất cả các đoạn code được retrieve bằng cách yêu cầu mô hình trả lời truy vấn người dùng gốc, cung cấp truy vấn và các đoạn như context. Chúng tôi tìm kiếm phản hồi cuối cùng của mô hình cho một tập keyword liên quan đến truy vấn đã cho để xác định liệu mô hình có thể tìm thấy thông tin cần thiết để trả lời đầy đủ câu hỏi hay không.

Với metric này, chúng tôi đánh giá khả năng retrieval của mô hình trên quy mô end to end, và xác định kỹ năng của mô hình trong việc tìm các đoạn code thực sự giúp nó trả lời câu hỏi trong tầm tay.

4 Copilot Evaluation Harness

Chúng tôi giới thiệu harness Đánh giá Copilot end to end để tính toán các metric đánh giá như mô tả ở trên. Đầu tiên, chúng tôi chia sẻ chi tiết về việc thu thập dữ liệu cần thiết cho mỗi đánh giá. Sau đó, chúng tôi giải thích quy trình tạo một môi trường test cho mỗi ngôn ngữ và nhu cầu build và chạy test. Cuối cùng, chúng tôi đưa ra chi tiết triển khai cụ thể bổ sung về quy trình đánh giá cho mỗi metric.

4.1 Thu thập Dữ liệu

Tập dữ liệu của chúng tôi được tạo thành từ các phương thức từ hàng trăm repository GitHub công khai qua 6 ngôn ngữ: JavaScript, Typescript, Python, Java, C/C++, và C#. Một số đánh giá của chúng tôi yêu cầu khả năng build và chạy test cho các repository liên quan đến test case. Để đáp ứng yêu cầu này, chúng tôi đã phát triển một build agent như một phần của harness đánh giá của chúng tôi cố gắng các chiến lược build và test khác nhau trên bất kỳ repository tùy ý nào. Ngoài ra, chúng tôi có khả năng chạy các công cụ phân tích tĩnh trên các repository mà chúng tôi có thể build và test. Build agent này là cần thiết trong việc thu thập các tập dữ liệu test và thực hiện đánh giá.

Đối với mỗi ngôn ngữ, chúng tôi lấy mẫu từ các repository công khai Github mà code của chúng tôi có thể build và test suite của chúng tôi có thể chạy sử dụng build agent của chúng tôi. Build agent hỗ trợ Node 18+, Python 3.8+, Java JDK 1.8 (yêu cầu Maven), .NET 6.0, 7.0 và 8.0, và một tập được curation thủ công các repository C++. Chúng tôi phải thu thập thủ công các repository C++ do sự biến đổi rộng của các bước build C++. Chúng tôi bỏ qua các repository nhỏ hơn 1 MB và lớn hơn 100 MB. Chúng tôi bỏ qua các repository mất hơn 10 phút để build và chạy test. Cuối cùng, chúng tôi bỏ qua các repository không chứa bất kỳ phương thức nào.

4.1.1 Javascript và Typescript. Trong Javascript và Typescript, chúng tôi sub-select trên các repo chứa file package.json ở thư mục root. File package.json hoạt động kết hợp với npm (Node Package Manager) để xử lý các nhiệm vụ khác nhau trong repo, như chỉ định dependencies để cài đặt và chạy test suite. Chúng tôi dựa vào npm cho đánh giá Javascript và Typescript code, vì vậy chúng tôi chỉ xem xét các repo có infrastructure được xây dựng để được quản lý với npm.

4.1.2 Java. Trong Java, chúng tôi xem xét các repository tận dụng Maven cho quy trình build của chúng. Ngoài ra, tại thời điểm viết, chúng tôi chỉ xem xét các dự án sử dụng JDK 1.8.

4.1.3 Python. Trong Python, chúng tôi chỉ xem xét các repository mà chúng tôi có thể cài đặt thành công tất cả dependencies trong một virtual environment.

4.1.4 C/C++. Trong C/C++, chúng tôi tận dụng clang để build dự án. Vì sự đa dạng tuyệt đối của các cách mà các repository C/C++ có thể được build, chúng tôi trình bày một tập các repository được curation thủ công mà chúng tôi đã xác minh sẽ build và test trong một docker image.

4.2 Thu thập Test Case

Sau khi xác định các repository phù hợp cho mỗi ngôn ngữ, chúng tôi tạo test case cho mỗi metric đánh giá dựa trên code trong các repository. Hầu hết các đánh giá yêu cầu xác định các phương thức đáp ứng các điều kiện nhất định, như được bao phủ bởi test hiện có hoặc chứa cảnh báo từ công cụ phân tích tĩnh. Tiêu chí để tạo test case đánh giá khác nhau từ metric này sang metric khác, và được giải thích cho mỗi metric dưới đây.

4.2.1 Tạo Tài liệu từ Code (doc). Chúng tôi tạo test case bằng cách xác định các phương thức trong repository dài hơn ba dòng và không phải kết quả của minification hoặc obfuscation. Chúng tôi cung cấp phương thức và yêu cầu coding assistant đang được đánh giá tạo một docstring cho phương thức. Chúng tôi xem xét việc tạo docstring thành công nếu vị trí, định dạng, và phạm vi bao phủ của văn bản được tạo là đúng.

4.2.2 Sửa Lỗi (fix). Chúng tôi tạo test case dựa trên cảnh báo và lỗi công cụ phân tích tĩnh được flag trên một repository đã cho. Chúng tôi chỉ xem xét các cảnh báo phân tích tĩnh không liên quan đến import hoặc configuration vì các vấn đề như vậy khó sửa với chỉ một file duy nhất như context. Chúng tôi xem xét việc tạo bản sửa thành công nếu nó đúng về mặt cú pháp và giảm nghiêm ngặt số lượng cảnh báo phân tích tĩnh khi thực thi. Chúng tôi phải xem xét giảm nghiêm ngặt thay vì sự hiện diện của cảnh báo hoặc lỗi gốc, vì có thể coding assistant sửa vấn đề gốc trong khi đưa vào một vấn đề mới, mà nhà phát triển sẽ không xem như một bản sửa hoàn chỉnh.

4.2.3 Tạo Code từ Ngôn ngữ Tự nhiên (generate). Chúng tôi tạo test case bằng cách xác định các phương thức trong một repository đã cho được bao phủ bởi một số test hiện có đang pass. Test case đưa cho coding assistant khả năng nhìn thấy toàn bộ file lên đến và bao gồm signature phương thức. Coding assistant sau đó được yêu cầu tạo body phương thức liên quan đến signature phương thức. Chúng tôi xem xét một đoạn code được tạo thành công nếu code được tạo đúng về mặt cú pháp và tất cả test case bao phủ code được tạo đều pass.

4.2.4 Tạo Test từ Code (test). Chúng tôi tạo test case bằng cách xác định các phương thức trong một repository đã cho. Chúng tôi yêu cầu coding assistant cung cấp một test hoạt động cho phương thức đã cho. Chúng tôi xem xét test được tạo thành công nếu nó gọi phương thức đã cho và pass thực thi.

4.2.5 Hiểu Workspace và Giải quyết Truy vấn (workspace). Chúng tôi thu thập câu hỏi từ các nhà phát triển về các khía cạnh nhất định của workspace dự án của họ, như cách thành ngữ để build một tính năng nhất định. Context fetching diễn ra như một phần của lệnh workspace sẽ trả về một số đoạn code liên quan. Chúng tôi đánh giá chất lượng các đoạn được retrieve của LLM sử dụng MRR như được giải thích ở trên.

5 Thí nghiệm

Sử dụng các metric harness đánh giá Copilot và test case được chi tiết ở trên, chúng tôi tính toán thành công của hai mô hình OpenAI: GPT-3.5 và GPT-4, cũng như CodeLlama trên các tình huống tạo tài liệu và sửa lỗi sử dụng một extension chat được hỗ trợ bởi LLM trong VSCode IDE với hơn 700K người dùng hoạt động như code assistant.

Thí nghiệm của chúng tôi nhằm trả lời các câu hỏi nghiên cứu sau:

• RQ1. So sánh Mô hình: Các LLM khác nhau so sánh với nhau như thế nào khi được tích hợp với một coding assistant?
• RQ2. Cải thiện Tích hợp: Harness Đánh giá Copilot có thể cung cấp những insight gì cho các kỹ sư để cải thiện việc tích hợp LLM trong một coding assistant?
• RQ3. Tính Hợp lệ Dữ liệu: Các test case đánh giá của chúng tôi so sánh như thế nào với việc sử dụng thực tế của một coding assistant được hỗ trợ bởi LLM? Các test case trong harness của chúng tôi có phản ánh cách người dùng thế giới thực tương tác với một coding assistant được hỗ trợ bởi LLM không?

--- TRANG 7 ---
Copilot Evaluation Harness: Đánh giá Lập trình Phần mềm được Hướng dẫn bởi LLM

chúng tôi có thể chạy sử dụng build agent của chúng tôi. Build agent hỗ trợ Node 18+, Python 3.8+, Java JDK 1.8 (yêu cầu Maven), .NET 6.0, 7.0 và 8.0, và một tập được curation thủ công các repository C++. Chúng tôi phải thu thập thủ công các repository C++ do sự biến đổi rộng của các bước build C++.
Chúng tôi bỏ qua các repository nhỏ hơn 1 MB và lớn hơn 100 MB. Chúng tôi bỏ qua các repository mất hơn 10 phút để build và chạy test. Cuối cùng, chúng tôi bỏ qua các repository không chứa bất kỳ phương thức nào.

4.1.1 Javascript và Typescript. Trong Javascript và Typescript, chúng tôi sub-select trên các repo chứa file package.json ở thư mục root. File package.json hoạt động kết hợp với npm (Node Package Manager) để xử lý các nhiệm vụ khác nhau trong repo, như chỉ định dependencies để cài đặt và chạy test suite. Chúng tôi dựa vào npm cho đánh giá Javascript và Typescript code, vì vậy chúng tôi chỉ xem xét các repo có infrastructure được xây dựng để được quản lý với npm.

4.1.2 Java. Trong Java, chúng tôi xem xét các repository tận dụng Maven cho quy trình build của chúng. Ngoài ra, tại thời điểm viết, chúng tôi chỉ xem xét các dự án sử dụng JDK 1.8.

4.1.3 Python. Trong Python, chúng tôi chỉ xem xét các repository mà chúng tôi có thể cài đặt thành công tất cả dependencies trong một virtual environment.

4.1.4 C/C++. Trong C/C++, chúng tôi tận dụng clang để build dự án. Vì sự đa dạng tuyệt đối của các cách mà các repository C/C++ có thể được build, chúng tôi trình bày một tập các repository được curation thủ công mà chúng tôi đã xác minh sẽ build và test trong một docker image.

4.2 Thu thập Test Case

Sau khi xác định các repository phù hợp cho mỗi ngôn ngữ, chúng tôi tạo test case cho mỗi metric đánh giá dựa trên code trong các repository. Hầu hết các đánh giá yêu cầu xác định các phương thức đáp ứng các điều kiện nhất định, như được bao phủ bởi test hiện có hoặc chứa cảnh báo từ công cụ phân tích tĩnh. Tiêu chí để tạo test case đánh giá khác nhau từ metric này sang metric khác, và được giải thích cho mỗi metric dưới đây.

4.2.1 Tạo Tài liệu từ Code (doc). Chúng tôi tạo test case bằng cách xác định các phương thức trong repository dài hơn ba dòng và không phải kết quả của minification hoặc obfuscation. Chúng tôi cung cấp phương thức và yêu cầu coding assistant đang được đánh giá tạo một docstring cho phương thức. Chúng tôi xem xét việc tạo docstring thành công nếu vị trí, định dạng, và phạm vi bao phủ của văn bản được tạo là đúng.

4.2.2 Sửa Lỗi (fix). Chúng tôi tạo test case dựa trên cảnh báo và lỗi công cụ phân tích tĩnh được flag trên một repository đã cho. Chúng tôi chỉ xem xét các cảnh báo phân tích tĩnh không liên quan đến import hoặc configuration vì các vấn đề như vậy khó sửa với chỉ một file duy nhất như context. Chúng tôi xem xét việc tạo bản sửa thành công nếu nó đúng về mặt cú pháp và giảm nghiêm ngặt số lượng cảnh báo phân tích tĩnh khi thực thi. Chúng tôi phải xem xét giảm nghiêm ngặt thay vì sự hiện diện của cảnh báo hoặc lỗi gốc, vì có thể coding assistant sửa vấn đề gốc trong khi đưa vào một vấn đề mới, mà nhà phát triển sẽ không xem như một bản sửa hoàn chỉnh.

4.2.3 Tạo Code từ Ngôn ngữ Tự nhiên (generate). Chúng tôi tạo test case bằng cách xác định các phương thức trong một repository đã cho được bao phủ bởi một số test hiện có đang pass. Test case đưa cho coding assistant khả năng nhìn thấy toàn bộ file lên đến và bao gồm signature phương thức. Coding assistant sau đó được yêu cầu tạo body phương thức liên quan đến signature phương thức. Chúng tôi xem xét một đoạn code được tạo thành công nếu code được tạo đúng về mặt cú pháp và tất cả test case bao phủ code được tạo đều pass.

4.2.4 Tạo Test từ Code (test). Chúng tôi tạo test case bằng cách xác định các phương thức trong một repository đã cho. Chúng tôi yêu cầu coding assistant cung cấp một test hoạt động cho phương thức đã cho. Chúng tôi xem xét test được tạo thành công nếu nó gọi phương thức đã cho và pass thực thi.

4.2.5 Hiểu Workspace và Giải quyết Truy vấn (workspace). Chúng tôi thu thập câu hỏi từ các nhà phát triển về các khía cạnh nhất định của workspace dự án của họ, như cách thành ngữ để build một tính năng nhất định. Context fetching diễn ra như một phần của lệnh workspace sẽ trả về một số đoạn code liên quan. Chúng tôi đánh giá chất lượng các đoạn được retrieve của LLM sử dụng MRR như được giải thích ở trên.

5 Thí nghiệm

Sử dụng các metric harness đánh giá Copilot và test case được chi tiết ở trên, chúng tôi tính toán thành công của hai mô hình OpenAI: GPT-3.5 và GPT-4, cũng như CodeLlama trên các tình huống tạo tài liệu và sửa lỗi sử dụng một extension chat được hỗ trợ bởi LLM trong VSCode IDE với hơn 700K người dùng hoạt động như code assistant.

Thí nghiệm của chúng tôi nhằm trả lời các câu hỏi nghiên cứu sau:

• RQ1. So sánh Mô hình: Các LLM khác nhau so sánh với nhau như thế nào khi được tích hợp với một coding assistant?
• RQ2. Cải thiện Tích hợp: Harness Đánh giá Copilot có thể cung cấp những insight gì cho các kỹ sư để cải thiện việc tích hợp LLM trong một coding assistant?
• RQ3. Tính Hợp lệ Dữ liệu: Các test case đánh giá của chúng tôi so sánh như thế nào với việc sử dụng thực tế của một coding assistant được hỗ trợ bởi LLM? Các test case trong harness của chúng tôi có phản ánh cách người dùng thế giới thực tương tác với một coding assistant được hỗ trợ bởi LLM không?

--- TRANG 8 ---
Agarwal và Chan, et al.

Trong phần này, chúng tôi thảo luận các phát hiện liên quan đến những câu hỏi nghiên cứu này.

[THIS IS TABLE: Doc performance table showing Syntax Correctness and Format Correctness for different languages (Python, Javascript, Typescript, Java, C#, C/C++) across three models (GPT-4, GPT-3.5, CodeLlama)]

[THIS IS TABLE: Fix performance table showing Syntax Correctness and Bugs Fixed for different languages (Python, Javascript, Typescript, C#) across three models (GPT-4, GPT-3.5, CodeLlama)]

5.1 RQ1. So sánh Mô hình

Dưới đây chúng tôi thảo luận những học hỏi của mình khi so sánh ba LLM tiên tiến khi được sử dụng để hỗ trợ extension chat mục tiêu của chúng tôi trong VSCode.

5.1.1 Tạo Tài liệu từ Code (doc). Bảng 1 cho thấy rằng, đối với việc tạo docstring, GPT-4 nói chung vượt trội hơn GPT-3.5 và Code Llama. GPT-3.5 và GPT-4 rất tương tự về hiệu suất với nhau, với Code Llama hơi kém hơn. Các ngoại lệ chính ở đây là Python, nơi Code Llama hoạt động ở mức hơi cao hơn GPT-4, và C/C++, nơi Code Llama hoạt động kém đáng kể. Một giải thích có thể là GPT-3.5 và GPT-4 được train trên một corpus khổng lồ, bao gồm phần lớn code nguồn mở trên internet. Do đó, hiệu suất của các mô hình GPT có thể được thổi phồng bởi thực tế là nó đã thấy nhiều pattern code khác nhau. Code Llama, một mô hình tương đối nhỏ, ít có khả năng đã thấy một đoạn code nhất định, có thể cản trở hiệu suất của nó so với các mô hình GPT.

[Passing Fix Request example and GPT-4 response are shown]

Hình 6. Ví dụ prompt và phản hồi cho một test case sửa lỗi pass.

--- TRANG 9 ---
Copilot Evaluation Harness: Đánh giá Lập trình Phần mềm được Hướng dẫn bởi LLM

[Failed Fix Request example and GPT-4 response are shown]

Hình 7. Ví dụ prompt và phản hồi cho tình huống chat sửa lỗi thất bại. Ở đây, mô hình xác định vấn đề có thể là gì và cố gắng sửa nó. Tuy nhiên, bản sửa không đúng, và cùng lỗi cú pháp vẫn tồn tại, vì nó cố gắng thực hiện phép toán > trước khi kiểm tra liệu age có phải None không.

5.1.2 Sửa Lỗi (fix). Bảng 2 cho thấy kết quả cho việc sửa lỗi: tương tự như đánh giá tạo docstring, GPT-4 có xu hướng vượt trội hơn GPT-3.5 một chút, với Code Llama kém hơn nữa. Đối với việc sửa lỗi, ngoại lệ dường như là C#, mà tất cả ba mô hình dường như gặp khó khăn, với GPT-3.5 cuối cùng vượt trội hơn cả GPT-4 và Code Llama. Hình 6 và 7 cho thấy một ví dụ pass và fail với GPT-4. Trong Hình 6, mô hình thêm một câu lệnh if để kiểm tra liệu self.writers có phải None không trước khi cố gắng sử dụng nó như một iterable, điều này giải quyết lỗi. Trong Hình 7, mô hình tương tự thêm một kiểm tra liệu Hero.age có phải None không. Tuy nhiên, vì nó thêm kiểm tra sau phép toán gây ra lỗi, lỗi tiếp tục xảy ra. Mặc dù mô hình có thể xác định một bản sửa tiềm năng, nó chèn kiểm tra ở vị trí sai và không thể sửa lỗi.

[GPT-3.5 vs. GPT-4 Fix Request Response example shown]

Hình 8. Ví dụ prompt và phản hồi cho cùng test case, nơi cả GPT-3.5 và GPT-4 đều nên fail, nhưng chỉ GPT-4 fail vì nó cố gắng một cách tiếp cận tinh tế hơn để sửa lỗi so với GPT-3.5.

Một nguyên nhân phổ biến của kết quả khác nhau giữa các LLM GPT-3.5 và GPT-4 xảy ra khi các mô hình cố gắng giải quyết lỗi "has an 'any' type", như có thể thấy trong Hình 8. Khi

--- TRANG 10 ---
Agarwal và Chan, et al.

mô hình GPT-4 cố gắng chỉ định kiểu của một biến tên res, nó dự đoán kiểu là Electron.MessageBoxReturnValue và cast biến thành kiểu đó. Tuy nhiên, kiểu đó không phải là kiểu return hợp lệ cho code. GPT-3.5, mặt khác, cast biến thành kiểu any, do đó tránh các vấn đề phức tạp hơn, nhưng để lại một code smell (vì không phải thực tiễn tốt để cast biến thành kiểu any). Trong trường hợp này, GPT-3.5 pass đánh giá của chúng tôi, trong khi GPT-4 fail, mặc dù bản sửa được GPT-4 cố gắng tinh tế và tiên tiến hơn. Khi kiểm tra kỹ hơn các trường hợp GPT-3.5 thành công và GPT-4 fail, chúng tôi thấy hiện tượng này thường xuyên: GPT-4 fail với một cách tiếp cận phức tạp hơn, trong khi GPT-3.5 về mặt kỹ thuật pass, nhưng với một giải pháp sơ khai và dưới tối ưu.

5.2 RQ2. Cải thiện Tích hợp

Dưới đây chúng tôi thảo luận cách harness đánh giá của chúng tôi có thể được sử dụng để học insights về cách tích hợp LLM với IDE tốt hơn.

5.2.1 Tạo Tài liệu từ Code (doc). Kiểm tra thêm kết quả của chúng tôi trong bảng 1 tiết lộ bốn lớp lỗi gây ra việc đánh giá tạo docstring fail:

1. Thay đổi Logic Code: Mô hình thay đổi logic cơ bản của code khi nó viết lại hàm focal vào file cùng với docstring được tạo.
2. Thay đổi Cú pháp: Mô hình thay đổi cú pháp của code focal khi viết vào file. Điều này bao gồm các thay đổi như thêm dấu chấm phẩy vào cuối dòng, hoặc thêm type decorator vào signature hàm.
3. Docstring Không hoàn chỉnh: Mô hình tạo mô tả cho hàm đúng, nhưng không mô tả object được return và mọi tham số của hàm.
4. Docstring Không liên quan: Mô hình trả về một docstring không liên quan đến code block chúng tôi yêu cầu nó tài liệu hóa.

Khi kiểm tra kỹ hơn, trong các trường hợp chỉ một trong GPT-3.5 hoặc GPT-4 pass, chúng tôi nhận thấy rằng mô hình GPT-4 có xu hướng thực hiện thay đổi đối với code focal làm cho code sạch hơn so với GPT-3.5. Ví dụ, trong Hình 9, mô hình GPT-4 thêm type decorator vào input của hàm, và chỉ định kiểu return. Mặc dù docstring của mô hình GPT-4 đúng (và chi tiết hơn so với mô hình GPT-3.5), mô hình GPT-4 fail test case này, vì chúng tôi kỳ vọng mô hình không thực hiện thay đổi nào đối với code focal. Tuy nhiên, một lỗi như vậy cho thấy cách các nỗ lực của mô hình GPT-4 trong việc cải thiện liên quan hơn có thể giảm điểm số của nó mà không cho thấy hiệu suất tệ hơn.

Dựa trên phát hiện này, chúng tôi chèn một hướng dẫn bổ sung trong prompt tạo docstring của coding assistant nói với mô hình cụ thể không thay đổi bất kỳ code focal nào. Điều này dẫn đến một cải thiện đáng kể trong kết quả đánh giá cho tất cả các ngôn ngữ, từ 5% trong C++ đến 11% trong Java.

[Doc: GPT-4 Syntactical Change example shown]

Hình 9. Ví dụ prompt và phản hồi cho đánh giá doc. GPT-3.5 pass, và GPT-4 fail vì nó trả về hàm gốc với kiểu tham số và return. Điều này fail đánh giá của chúng tôi vì chúng tôi yêu cầu mô hình để code focal không thay đổi.

Chúng tôi cũng thấy rằng GPT-4 tốt hơn trong việc tuân theo hướng dẫn cụ thể. Điều này được làm nổi bật bởi ví dụ trong Hình 10, nơi GPT-4 tạo tài liệu cho lớp Vec thay vì hàm get_colour_at. Lúc đầu, điều này xuất hiện là một lỗi của GPT-4. Tuy nhiên, một đánh giá của prompt cho thấy việc parsing không chính xác tên hàm cho C++ một cách sai lầm đặt Vec trong prompt thay vì get_colour_at. Mặc dù GPT-3.5 nhận được cùng hướng dẫn, nó viết tài liệu cho get_colour_at. Từ các ví dụ như thế này, chúng tôi thấy rằng mô hình GPT-4 nhạy cảm hơn với hướng dẫn, và chúng tôi phải thiết kế prompt của mình tương ứng.

[Doc: Incorrect Focal Function example shown]

Hình 10. Ví dụ prompt và phản hồi cho đánh giá doc. GPT-3.5 pass, và GPT-4 fail vì nó thêm tài liệu cho lớp Vec, thay vì hàm get colour. Điều này gây ra bởi một lỗi chúng tôi có thể làm nổi bật trong parsing và prompting của pipeline VSCode: mô hình được yêu cầu một cách sai lầm tài liệu hóa Vec, không get colour. GPT-4 fail vì nó tốt hơn trong việc tuân theo hướng dẫn.

Với các khuyến nghị dựa trên những kết quả này từ harness đánh giá, chúng tôi có thể giảm đáng kể số lượng lỗi cú pháp và logic code trong việc tạo tài liệu trong extension chat được hỗ trợ bởi LLM cho VSCode. Chúng tôi cũng có thể sửa lỗi trong parsing các hàm trong Java và C++ dẫn đến mô hình nhận keyword không chính xác như tên hàm. Việc phát hiện các lỗi như vậy, cũng như các cải thiện tương ứng, chỉ có thể với một hệ thống đánh giá mạnh mẽ và toàn diện.

5.2.2 Sửa Lỗi (fix). Để hiểu hiệu suất của các LLM trên đánh giá sửa lỗi, chúng tôi đi sâu hơn vào các loại lỗi mà các bộ phân tích tĩnh của chúng tôi đưa ra. Bảng 3 cho thấy một phân tích của một số lỗi phân tích tĩnh phổ biến được đưa cho các mô hình để giải quyết cho Typescript, cũng như hiệu suất của các mô hình GPT trên chúng. Chúng tôi thấy rằng cả hai mô hình thường có thể tìm thấy namespace của objects và sửa các vấn đề kiểu. Tuy nhiên, như được tham chiếu trước đây, vẫn có nhiều trường hợp lỗi "has an 'any' type" không được giải quyết đúng. Hình 8 cho thấy một ví dụ như vậy. Và, trong khi Hình 8 cho thấy một ví dụ cho GPT-4, Hình 11 cho thấy một hiện tượng tương tự cho GPT-3.5. Mô hình GPT-3.5 hallucinate một kiểu Token cho biến token. Để giải quyết các vấn đề này trong một extension chat được hỗ trợ bởi LLM, chúng tôi cần cung cấp cho các mô hình context bổ sung như kiểu biến target và namespace để LLM có thể sửa vấn đề này đúng, thay vì sai sử dụng các kiểu hiện có hoặc hallucinate hoàn toàn các kiểu mới.

[Fix Errors in Typescript table shown]

[Fix Hallucination - GPT-3.5 example shown]

Hình 11. Ví dụ prompt và phản hồi thất bại từ GPT-3.5. Mô hình hallucinate một kiểu (Token) khớp với tên của biến.

5.3 RQ3. Xác thực Dữ liệu

Mặc dù tập dữ liệu của chúng tôi được lấy từ các repository git thế giới thực, điều này không đảm bảo rằng test case của chúng tôi phản ánh chính xác cách người dùng tương tác với LLM thông qua IDE. Để xác nhận tính hợp lệ của tập dữ liệu của chúng tôi, chúng tôi thu thập dữ liệu sử dụng minh họa cách hàng trăm nhà phát triển Microsoft sử dụng các chức năng tạo docstring và sửa lỗi của extension chat được hỗ trợ bởi LLM mục tiêu của chúng tôi trong VS Code. Sau đó chúng tôi so sánh những trường hợp này với test case của chúng tôi.

--- TRANG 12 ---
Agarwal và Chan, et al.

Đối với việc tạo tài liệu, chúng tôi sử dụng mô hình embedding ada của OpenAI [3] để embed các đoạn code được tài liệu hóa và so sánh các đoạn từ tập dữ liệu của chúng tôi với những cái được thu thập từ dữ liệu sử dụng của nhà phát triển Microsoft. Tương tự, đối với telemetry sửa lỗi, chúng tôi embed các đoạn code chứa lỗi. Chúng tôi sử dụng giảm chiều PCA để vẽ dữ liệu trong hai chiều. Giảm chiều PCA được tối ưu hóa để tìm một mặt phẳng tối đa hóa khoảng cách giữa các điểm và outlier. Hình 12 và 13 cho thấy kết quả của so sánh này. Chúng tôi thấy rằng mỗi ngôn ngữ tạo thành một cluster, và việc sử dụng thực và dữ liệu của chúng tôi tồn tại trong một không gian tương tự cho mỗi cluster ngôn ngữ, cho cả việc tạo tài liệu và sửa lỗi.

Hình 12. So sánh tập dữ liệu của chúng tôi cho đánh giá Tạo tài liệu với việc sử dụng thế giới thực qua các ngôn ngữ.

Hình 13. So sánh tập dữ liệu của chúng tôi cho đánh giá Sửa lỗi với việc sử dụng thế giới thực qua các ngôn ngữ.

Chúng tôi không nhằm khớp các test case trong tập dữ liệu của mình với điểm sử dụng thực từng điểm một. Thay vào đó, chúng tôi đang xác định

--- TRANG 13 ---
Copilot Evaluation Harness: Đánh giá Lập trình Phần mềm được Hướng dẫn bởi LLM

liệu test case của chúng tôi có phải outlier trong không gian của việc sử dụng thực hay không. Nếu chúng không phải outlier, chúng tôi có thể suy ra rằng tập dữ liệu của chúng tôi phù hợp với việc sử dụng thế giới thực của extension chat. Phân tích này gợi ý rằng tập dữ liệu của chúng tôi cho cả đánh giá tạo tài liệu và sửa lỗi phù hợp với việc sử dụng thế giới thực.

6 Kết luận và Công trình Tương lai

Với việc sử dụng ngày càng tăng của LLM để hỗ trợ các nhà phát triển trong các nhiệm vụ kỹ thuật phức tạp đến nhu cầu về các đánh giá mạnh mẽ hơn của code được tạo bởi LLM. Đặc biệt khi nhiều công ty và sản phẩm tìm cách tích hợp LLM vào workflow của họ, các metric đánh giá hiện có không đủ để xác nhận chất lượng và tính đúng đắn của code được tạo bởi máy. Trong bài báo này, chúng tôi đề xuất một giải pháp cho vấn đề này thông qua harness Đánh giá Copilot. Chúng tôi định nghĩa năm metric đánh giá chính cho không gian vấn đề tạo code: tạo phương thức, tạo test, tạo docstring, sửa lỗi và hiểu workspace. Chúng tôi chi tiết phương pháp cần thiết để thu thập test case và kết quả đánh giá cho mỗi trong năm metric đó. Chúng tôi cũng cung cấp kết quả sơ bộ cho hai trong năm metric qua vô số ngôn ngữ lập trình.

Mục tiêu của chúng tôi trong việc tạo harness đánh giá là xác thực chất lượng của code được tạo bởi LLM. Mặc dù chúng tôi đã thấy những tiến bộ khổng lồ trong không gian ML tạo code, chúng tôi tìm cách làm nổi bật bao nhiêu giám sát và nỗ lực kỹ thuật được yêu cầu để tích hợp đáng tin cậy và tối ưu LLM vào một workflow code. Chúng tôi nhằm cung cấp cho các nhà phát triển một bộ đánh giá toàn diện, với đó họ có thể tối ưu hóa việc tích hợp LLM vào workflow coding của họ. Với harness Đánh giá Copilot, các lập trình viên có thể đánh giá một cách có hệ thống và mạnh mẽ hơn tác động của các tham số như cách diễn đạt prompt, thay đổi trong thứ tự thông tin được cung cấp, thay đổi trong context được cung cấp cho mô hình, và hơn thế nữa.

Hơn nữa, harness Đánh giá Copilot có thể được sử dụng cho tối ưu hóa chi phí bằng cách tiết lộ rằng một mô hình LLM thân thiện ngân sách hơn (ví dụ CodeLLama) có thể thể hiện hiệu suất thỏa đáng trong các nhiệm vụ như tài liệu hóa. Insight này cho phép các nhà phát triển cân bằng thông minh tài nguyên bằng cách phân bổ nhiệm vụ cho LLM hiệu quả chi phí khi hiệu suất của nó được coi là đủ. Đồng thời, các nhiệm vụ phức tạp hơn có thể được chuyển sang LLM mạnh mẽ hơn để đảm bảo kết quả tối ưu.

Chúng tôi xuất bản bài báo này như một tài liệu sống về tiến trình của chúng tôi. Công trình tương lai về dự án này liên quan đến báo cáo kết quả cho ba metric đánh giá còn lại và mở source dữ liệu và code đánh giá của chúng tôi.

Tài liệu tham khảo
[1] Clang. https://clang.llvm.org/ .
[2] Eslint. https://eslint.org .
[3] Openai ada. https://openai.com/blog/new-and-improved-embedding-model .
[4] Pylint. https://pylint.pycqa.org/en/latest/user_guide/usage/run.html .
[5] Pyright. https://github.com/microsoft/pyright .
[6] Roslyn. https://github.com/dotnet/roslyn-analyzers .
[7] Spotbugs. https://spotbugs.github.io/ .
[8] Chang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H., Yi, X., Wang, C., Wang, Y., Ye, W., Zhang, Y., Chang, Y., Yu, P. S., Yang, Q., và Xie, X. A survey on evaluation of large language models, 2023.
[9] Chen, B., Mustakin, N., Hoang, A., Fuad, S., và Wong, D. Vscuda: Llm based cuda extension for visual studio code. In Proceedings of the SC '23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (New York, NY, USA, 2023), SC-W '23, Association for Computing Machinery, p. 11–17.
[10] Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., et al. Evaluating large language models trained on code.
[11] Chen, Y., Wang, R., Jiang, H., Shi, S., và Xu, R. Exploring the use of large language models for reference-free text quality evaluation: An empirical study, 2023.
[12] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., và Others. Palm: Scaling language modeling with pathways, 2022.
[13] Devlin, J., Chang, M.-W., Lee, K., và Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019.
[14] Floridi, L., và Chiriatti, M. Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines 30 (2020), 681–694.
[15] Fu, J., Ng, S.-K., Jiang, Z., và Liu, P. Gptscore: Evaluate as you desire, 2023.
[16] Gao, T., Fisch, A., và Chen, D. Making pre-trained language models better few-shot learners, 2021.
[17] Hendrycks, D., Burns, C., Basart, S., Critch, A., Li, J., Song, D., và Steinhardt, J. Aligning ai with shared human values, 2023.
[18] Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., và Steinhardt, J. Measuring massive multitask language understanding, 2021.
[19] Hou, X., Zhao, Y., Liu, Y., Yang, Z., Wang, K., Li, L., Luo, X., Lo, D., Grundy, J., và Wang, H. Large language models for software engineering: A systematic literature review. arXiv preprint arXiv:2308.10620 (2023).
[20] Kombrink, S., Mikolov, T., Karafiát, M., và Burget, L. Recurrent neural network based language modeling in meeting recognition. In Interspeech (2011), vol. 11, pp. 2877–2880.
[21] Laskar, M. T. R., Bari, M. S., Rahman, M., Bhuiyan, M. A. H., Joty, S., và Huang, J. X. A systematic study and comprehensive evaluation of chatgpt on benchmark datasets, 2023.
[22] Liang, P., Bommasani, R., Lee, T., Tsipras, D., và Others. Holistic evaluation of language models, 2023.
[23] Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqa, M., Petroni, F., và Liang, P. Lost in the middle: How language models use long contexts, 2023.
[24] Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., Clement, C., Drain, D., Jiang, D., Tang, D., Li, G., Zhou, L., Shou, L., Zhou, L., Tufano, M., Gong, M., Zhou, M., Duan, N., Sundaresan, N., Deng, S. K., Fu, S., và Liu, S. Codexglue: A machine learning benchmark dataset for code understanding and generation, 2021.
[25] Nam, D., Macvean, A., Hellendoorn, V., Vasilescu, B., và Myers, B. In-ide generation-based information support with a large language model, 2023.
[26] OpenAI. Gpt 3.5 models, 2023.
[27] OpenAI. Gpt-4 technical report, 2023.
[28] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems 35 (2022), 27730–27744.

--- TRANG 14 ---
Agarwal và Chan, et al.

[29] Papineni, K., Roukos, S., Ward, T., và Zhu, W.-J. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics (2002), pp. 311–318.
[30] Petroni, F., Lewis, P., Piktus, A., Rocktäschel, T., Wu, Y., Miller, A. H., và Riedel, S. How context affects language models' factual predictions, 2020.
[31] Qin, C., Zhang, A., Zhang, Z., Chen, J., Yasunaga, M., và Yang, D. Is chatgpt a general-purpose natural language processing task solver?, 2023.
[32] Rae, J. W., Borgeaud, S., Cai, T., Millican, K., và Others. Scaling language models: Methods, analysis & insights from training gopher, 2022.
[33] Ram, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K., và Shoham, Y. In-context retrieval-augmented language models, 2023.
[34] Ren, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sundaresan, N., Zhou, M., Blanco, A., và Ma, S. Codebleu: a method for automatic evaluation of code synthesis. arXiv preprint arXiv:2009.10297 (2020).
[35] Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950 (2023).
[36] Roziere, B., Lachaux, M.-A., Chanussot, L., và Lample, G. Unsupervised translation of programming languages. In Proceedings of the 34th International Conference on Neural Information Processing Systems (Red Hook, NY, USA, 2020), NIPS'20, Curran Associates Inc.
[37] Smith, S., Patwary, M., Norick, B., LeGresley, P., Rajbhandari, S., Casper, J., Liu, Z., Prabhumoye, S., Zerveas, G., Korthikanti, V., Zhang, E., Child, R., Aminabadi, R. Y., Bernauer, J., Song, X., Shoeybi, M., He, Y., Houston, M., Tiwary, S., và Catanzaro, B. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model, 2022.
[38] Sridhar, A., Lo, R., Xu, F. F., Zhu, H., và Zhou, S. Hierarchical prompting assists large language model on web navigation, 2023.
[39] Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2023.
[40] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., và Polosukhin, I. Attention is all you need. Advances in neural information processing systems 30 (2017).
[41] Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., và Bowman, S. R. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. Curran Associates Inc., Red Hook, NY, USA, 2019.
[42] Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., và Bowman, S. R. Glue: A multi-task benchmark and analysis platform for natural language understanding, 2019.
[43] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., và Zhou, D. Chain-of-thought prompting elicits reasoning in large language models, 2023.
[44] Zhang, X., Yu, B., Yu, H., Lv, Y., Liu, T., Huang, F., Xu, H., và Li, Y. Wider and deeper llm networks are fairer llm evaluators, 2023.
[45] Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., Zhang, S., Ghosh, G., Lewis, M., Zettlemoyer, L., và Levy, O. Lima: Less is more for alignment, 2023.
