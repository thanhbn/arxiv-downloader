# 2205.13452.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/benchmark/2205.13452.pdf
# Kích thước tệp: 3845384 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Được công bố như một bài báo hội nghị tại ICLR 2023
ĐÁNH GIÁ LIÊN TỤC CHO HỌC TẬP SUỐT ĐỜI:
XÁC ĐỊNH KHOẢNG CÁCH ỔN ĐỊNH
Matthias De Lange, Gido M. van de Ven & Tinne Tuytelaars
KU Leuven
TÓM TẮT
Các phân phối tạo dữ liệu phụ thuộc thời gian đã được chứng minh là khó khăn cho việc huấn luyện mạng nơ-ron dựa trên gradient, vì các cập nhật tham lam dẫn đến việc quên thảm khốc kiến thức đã học trước đó. Mặc dù có tiến bộ trong lĩnh vực học tập liên tục để khắc phục việc quên này, chúng tôi cho thấy một tập hợp các phương pháp tiên tiến phổ biến vẫn gặp phải việc quên đáng kể khi bắt đầu học các nhiệm vụ mới, ngoại trừ việc quên này là tạm thời và được theo sau bởi một giai đoạn phục hồi hiệu suất. Chúng tôi gọi hiện tượng hấp dẫn nhưng có thể có vấn đề này là khoảng cách ổn định. Khoảng cách ổn định có thể đã vẫn ở dưới radar do thực hành tiêu chuẩn trong lĩnh vực đánh giá các mô hình học tập liên tục chỉ sau mỗi nhiệm vụ. Thay vào đó, chúng tôi thiết lập một khung để đánh giá liên tục sử dụng đánh giá theo từng lần lặp và chúng tôi định nghĩa một tập hợp các chỉ số mới để định lượng hiệu suất trường hợp xấu nhất. Thực nghiệm cho thấy experience replay, constraint-based replay, knowledge-distillation, và các phương pháp regularization tham số đều dễ bị khoảng cách ổn định; và khoảng cách ổn định có thể được quan sát trong các benchmark học tập tăng dần theo lớp, nhiệm vụ và miền. Ngoài ra, một thí nghiệm có kiểm soát cho thấy khoảng cách ổn định tăng lên khi các nhiệm vụ khác biệt hơn. Cuối cùng, bằng cách tách biệt gradient thành các thành phần tính dẻo và ổn định, chúng tôi đề xuất một giải thích khái niệm cho khoảng cách ổn định.

1 GIỚI THIỆU
Sự hội tụ nhanh trong tối ưu hóa dựa trên gradient đã dẫn đến nhiều thành công với các mạng nơ-ron có tham số hóa cao (Krizhevsky et al., 2012; Mnih et al., 2013; Devlin et al., 2018). Trong mô hình huấn luyện tiêu chuẩn, những kết quả này có điều kiện là có một phân phối tạo dữ liệu tĩnh. Tuy nhiên, khi tính không ổn định được giới thiệu bằng phân phối tạo dữ liệu thay đổi theo thời gian, các cập nhật dựa trên gradient tham lam ghi đè các tham số của giải pháp trước đó. Điều này dẫn đến việc quên thảm khốc (French, 1999) và là một trong những trở ngại chính trong học tập liên tục hoặc học tập suốt đời.

Học tập liên tục thường được trình bày như khao khát học theo cách con người học, tích lũy thay vì thay thế kiến thức. Để đạt được mục đích này, nhiều công trình đã tập trung vào việc giảm thiểu việc quên thảm khốc với kết quả đầy hứa hẹn, cho thấy hành vi học tập như vậy có thể thực hiện được đối với mạng nơ-ron nhân tạo (De Lange et al., 2021; Parisi et al., 2019). Ngược lại, công trình này một cách đáng ngạc nhiên xác định việc quên đáng kể vẫn hiện diện trong quá trình chuyển đổi nhiệm vụ đối với các phương pháp tiên tiến tiêu chuẩn dựa trên experience replay, constraint-based replay, knowledge distillation, và parameter regularization, mặc dù việc quên được quan sát là tạm thời và được theo sau bởi giai đoạn phục hồi. Chúng tôi gọi hiện tượng này là khoảng cách ổn định.

Các đóng góp trong công trình này theo ba hướng chính, với mã nguồn có sẵn công khai.¹ Đầu tiên, chúng tôi định nghĩa một khung cho đánh giá liên tục đánh giá người học sau mỗi cập nhật. Khung này được thiết kế để cho phép giám sát hiệu suất trường hợp xấu nhất của người học liên tục từ góc độ của các tác nhân thu thập kiến thức trong suốt cuộc đời của họ. Để làm điều này, chúng tôi đề xuất các chỉ số có nguyên tắc mới như độ chính xác tối thiểu và trường hợp xấu nhất (min-ACC và WC-ACC).

Thứ hai, chúng tôi tiến hành một nghiên cứu thực nghiệm với khung đánh giá liên tục, dẫn đến việc xác định khoảng cách ổn định, như được minh họa trong Hình 1, trong nhiều phương pháp và cài đặt khác nhau. Một nghiên cứu ablation về tần suất đánh giá cho thấy đánh giá liên tục là phương tiện cần thiết để phát hiện khoảng cách ổn định, giải thích tại sao hiện tượng này vẫn chưa được xác định cho đến nay. Ngoài ra, chúng tôi thấy rằng khoảng cách ổn định bị ảnh hưởng đáng kể bởi mức độ tương tự của các nhiệm vụ liên tiếp trong luồng dữ liệu.

Thứ ba, chúng tôi đề xuất một phân tích khái niệm để giúp giải thích khoảng cách ổn định, bằng cách tách biệt các gradient dựa trên tính dẻo và ổn định. Chúng tôi làm điều này cho một số phương pháp: Experience Replay (Chaudhry et al., 2019b), GEM (Lopez-Paz & Ranzato, 2017), EWC (Kirkpatrick et al., 2017), SI (Zenke et al., 2017), và LwF (Li & Hoiem, 2017). Các thí nghiệm bổ sung với phân tích gradient cung cấp bằng chứng hỗ trợ cho giả thuyết.

Ý nghĩa của khoảng cách ổn định. (i) Đánh giá liên tục là quan trọng, đặc biệt là cho các ứng dụng quan trọng về an toàn, vì các phương pháp học tập liên tục đại diện gặp khó khăn trong việc duy trì hiệu suất mạnh mẽ trong quá trình học. (ii) Có nguy cơ những thay đổi phân phối đột ngột có thể bị khai thác bởi kẻ thù có thể kiểm soát luồng dữ liệu để giảm hiệu suất một cách tạm thời nhưng đáng kể. (iii) Bên cạnh những ý nghĩa thực tế này, bản thân khoảng cách ổn định là một hiện tượng khoa học hấp dẫn truyền cảm hứng cho nghiên cứu thêm. Ví dụ, khoảng cách ổn định gợi ý các phương pháp học tập liên tục hiện tại có thể thể hiện động lực học khác biệt cơ bản so với bộ não con người.

Hình 1: Khoảng cách ổn định: việc quên đáng kể được theo sau bởi phục hồi khi học các nhiệm vụ mới trong các phương pháp học tập liên tục tiên tiến. Đánh giá liên tục tại mỗi lần lặp (đường cong màu cam) tiết lộ khoảng cách ổn định, vẫn chưa được xác định với đánh giá hướng nhiệm vụ tiêu chuẩn (kim cương đỏ). Được hiển thị là độ chính xác trên nhiệm vụ đầu tiên, khi một mạng sử dụng Experience Replay học tuần tự năm nhiệm vụ đầu tiên của class-incremental Split-MiniImagenet. Chi tiết thêm trong Hình 2.

[Biểu đồ hiển thị độ chính xác theo số lần lặp, với đường cong cho ρeval=1 và ρeval=Ti]

2 KIẾN THỨC CƠ BẢN VỀ HỌC TẬP LIÊN TỤC

Mục tiêu phân loại học tập liên tục hoặc suốt đời là học một hàm f:X→Y với các tham số, ánh xạ không gian đầu vào X đến không gian đầu ra Y, từ một luồng dữ liệu không ổn định S={(x,y)0,(x,y)1,...,(x,y)n}, trong đó tuple dữ liệu (x∈X,y∈Y)t được lấy mẫu từ một phân phối tạo dữ liệu D phụ thuộc vào thời gian t. Trong khi học máy tiêu chuẩn giả định một phân phối tạo dữ liệu tĩnh, học tập liên tục giới thiệu sự phụ thuộc vào biến thời gian t. Sự phụ thuộc thời gian này giới thiệu một sự đánh đổi giữa thích ứng với phân phối tạo dữ liệu hiện tại và giữ lại kiến thức thu được từ những phân phối trước đó, còn được gọi là sự đánh đổi ổn định-tính dẻo (Grossberg, 1982).

Nhiệm vụ. Luồng dữ liệu thường được giả định được chia thành các phân phối ổn định cục bộ, được gọi là nhiệm vụ. Chúng tôi giới thiệu định danh nhiệm vụ rời rạc k để chỉ nhiệm vụ thứ k Tk với phân phối tạo dữ liệu ổn định cục bộ Dk. Ngoài ra, biến thời gian t được giả định rời rạc, chỉ số lần lặp tổng thể trong luồng, và t|Tk| chỉ số lần lặp tổng thể ở cuối nhiệm vụ Tk.

Học tập liên tục. Trong giai đoạn huấn luyện, một người học liên tục cập nhật f dựa trên các tuple mới (x,y)t từ luồng dữ liệu (De Lange & Tuytelaars, 2021). Tối ưu hóa theo sau minimization nguy cơ thực nghiệm trên các tập huấn luyện quan sát được D̃k, vì người học không có quyền truy cập trực tiếp vào các phân phối tạo dữ liệu Dk. Mục tiêu negative log-likelihood mà chúng ta lý tưởng muốn tối ưu hóa trong khi học nhiệm vụ Tk là:

min_θ Lk = Σ(n=1 to k) E_(x,y)~D̃n [−yT log f(x;θ)]     (1)

Một thách thức chính cho học tập liên tục là ước tính mục tiêu này chỉ có dữ liệu huấn luyện của nhiệm vụ hiện tại D̃k có sẵn, trong khi sử dụng các tài nguyên tính toán và bộ nhớ bổ sung hạn chế.

--- TRANG 2 ---
Được công bố như một bài báo hội nghị tại ICLR 2023

3 MỘT KHUNG CHO ĐÁNH GIÁ LIÊN TỤC

Người học liên tục cập nhật f có một đối tác, đánh giá viên liên tục, theo dõi hiệu suất của f theo thời gian. Không chỉ dữ liệu được quan sát bởi người học trong luồng dữ liệu S có thể thay đổi theo thời gian, mà cả phân phối mà người học được đánh giá cũng vậy. Điều này được phản ánh trong sự phụ thuộc thời gian trên luồng dữ liệu của đánh giá viên SE (De Lange & Tuytelaars, 2021). Đánh giá viên được cấu hình bởi ba yếu tố chính: i) tần suất đánh giá ρeval xác định tần suất f được đánh giá; ii) quá trình xây dựng SE; iii) tập hợp các chỉ số đánh giá.

Tần suất đánh giá. Cách tiếp cận điển hình để đánh giá trong học tập liên tục là đánh giá dựa trên nhiệm vụ: hiệu suất của mô hình f được đánh giá chỉ sau khi hoàn thành việc huấn luyện trên một nhiệm vụ mới. Như một sự thay thế, ở đây chúng tôi định nghĩa đánh giá liên tục là đánh giá hiệu suất của f mỗi ρeval lần lặp huấn luyện. Để đánh giá chi tiết nhất ρeval được đặt thành 1, cài đặt được sử dụng trong phần sau trừ khi có đề cập khác. Hiệu ứng của việc tăng ρeval có thể được quan sát trong Hình 2 và được nghiên cứu chi tiết hơn trong Phụ lục B.

Định nghĩa luồng đánh giá. Hành vi mong muốn của mô hình có thể được tách biệt trong hiệu suất trên một tập hợp các nhiệm vụ đánh giá. Chúng tôi định nghĩa một tập hợp N nhiệm vụ đánh giá TE={E0,...,EN}, trong đó mỗi nhiệm vụ đánh giá Ei có một tập dữ liệu D̃E,i được lấy mẫu từ phân phối tạo dữ liệu DE,i. Tập hợp các nhiệm vụ đánh giá có thể được mở rộng tại bất kỳ thời điểm nào: TE ← TE ∪ {EN+1}. Theo tài liệu (van de Ven et al., 2020), chúng tôi giả định một nhiệm vụ đánh giá mới Ek được thêm vào khi gặp mỗi nhiệm vụ huấn luyện Tk trong S. Thông thường, các nhiệm vụ đánh giá được chọn để phù hợp với các nhiệm vụ trong luồng huấn luyện S, trong đó dữ liệu đánh giá D̃E,k có thể bao gồm một tập con của dữ liệu huấn luyện nhiệm vụ quan sát được D̃k, hoặc phổ biến hơn, bao gồm một tập đánh giá giữ lại để kiểm tra hiệu suất tổng quát hóa với D̃k ∩ D̃E,k = ∅. Khi tập hợp các nhiệm vụ đánh giá mở rộng qua luồng huấn luyện, chúng tôi cung cấp các nới lỏng về tính khả thi tính toán trong Phụ lục B.

Chỉ số đánh giá liên tục Yếu tố thứ ba của đánh giá viên là tập hợp các chỉ số đánh giá được sử dụng để đánh giá hiệu suất của người học. Cho đến nay, các chỉ số đã được định nghĩa chủ yếu giả định đánh giá trên quá trình chuyển đổi nhiệm vụ và tập trung vào hiệu suất cuối cùng của người học. Chúng tôi ủng hộ rằng hiệu suất trường hợp xấu nhất liên tục là quan trọng đối với người học liên tục trong thế giới thực. Do đó, chúng tôi đề xuất các chỉ số mới để định lượng hiệu suất trường hợp xấu nhất (WC-ACC, min-ACC, WFw) và các chỉ số cũng áp dụng cho luồng dữ liệu bất khả tri nhiệm vụ (WFw, WPw). Chúng tôi tập trung vào các chỉ số phân loại, với độ chính xác (tỷ lệ phần trăm các trường hợp được phân loại đúng) được coi là chỉ số hiệu suất chính. Sử dụng ft để chỉ phiên bản của mô hình sau lần lặp huấn luyện tổng thể thứ t, độ chính xác của nhiệm vụ đánh giá Ek tại lần lặp này được ký hiệu là A(Ek, ft). Chúng tôi cung cấp một phân loại các chỉ số dựa trên tính dẻo và ổn định trong phần sau.

3.1 CHỈ SỐ DựA TRÊN ỔN ĐỊNH

Để đo lường tính ổn định của người học, chúng tôi nhằm định lượng mức độ kiến thức từ các nhiệm vụ quan sát trước đó T<k được bảo tồn trong khi học nhiệm vụ mới Tk.

Việc quên trung bình (FORG) (Chaudhry et al., 2018) tính trung bình sự khác biệt độ chính xác cho mô hình gần đây nhất ft|Tk| so với ft|Ti| ngay sau khi học Ti với nhiệm vụ đánh giá Ei, và được định nghĩa là 1/(k-1) Σ(i=1 to k-1)[A(Ei, ft|Ti|) - A(Ei, ft|Tk|)]. Việc quên lớn chỉ ra hiện tượng quên thảm khốc, trong khi việc quên âm chỉ ra việc chuyển giao kiến thức từ nhiệm vụ mới đến các nhiệm vụ trước đó.

Đối với hiệu suất trường hợp xấu nhất, việc có một thước đo tuyệt đối về hiệu suất nhiệm vụ trước đó là mong muốn. Chúng tôi định nghĩa độ chính xác tối thiểu trung bình (min-ACC) tại nhiệm vụ huấn luyện hiện tại Tk là độ chính xác tối thiểu tuyệt đối trung bình trên các nhiệm vụ đánh giá trước đó Ei sau khi chúng đã được học:

min-ACC_Tk = 1/(k-1) Σ(i=1 to k-1) min_n {A(Ei, fn); ∀t|Ti| < n ≤ t}     (2)

trong đó số lần lặp n dao động từ sau khi nhiệm vụ được học đến lần lặp hiện tại t. Điều này đưa ra một thước đo trường hợp xấu nhất về mức độ kiến thức được bảo tồn trong các nhiệm vụ quan sát trước đó T<k. Trong phần sau, chúng tôi báo cáo min-ACC cho nhiệm vụ cuối cùng, bỏ qua sự phụ thuộc vào Tk để ngắn gọn.

Hơn nữa, chúng tôi giới thiệu một chỉ số ổn định tổng quát hơn không giả định luồng dữ liệu dựa trên nhiệm vụ, và do đó cũng áp dụng cho học tập tăng dần dữ liệu. Chúng tôi định nghĩa Windowed-Forgetting (WFw) dựa trên một cửa sổ w đánh giá độ chính xác liên tiếp được tính trung bình trên tập đánh giá TE. Đối với một nhiệm vụ đánh giá duy nhất Ei, việc giảm độ chính xác tối đa trong cửa sổ Δw,−t,Ei và Windowed-Forgetting cụ thể theo nhiệm vụ WFw t,Ei được định nghĩa tại lần lặp hiện tại t là

Δw,−t,Ei = max_{m<n} (A(Ei, fm) - A(Ei, fn)); ∀m,n ∈ [t-w+1, t]     (3)

WFw t,Ei = max_n Δw,−n,Ei; ∀n ≤ t     (4)

Tính trung bình chỉ số trên tất cả N nhiệm vụ đánh giá dẫn đến một chỉ số duy nhất WFw t = N^(-1) Σ(i=1 to N) WFw t,Ei. Vì nó xác định việc giảm hiệu suất tối đa quan sát được trong cửa sổ, nó được coi là một chỉ số trường hợp xấu nhất. Các đánh giá viên có thể đủ khả năng độ phức tạp không gian tuyến tính có thể xem xét toàn bộ lịch sử tại lần lặp t với WFt. Để tính đến cả việc quên nhanh và việc quên ở quy mô lớn hơn với bộ nhớ không đổi, chúng tôi khởi tạo WF10 và WF100.

3.2 CHỈ SỐ DựA TRÊN TÍNH DẺO

Tính dẻo đề cập trong công trình này đến khả năng của người học để thu thập kiến thức mới từ phân phối tạo dữ liệu hiện tại Dk. Độ chính xác nhiệm vụ hiện tại đo lường tính dẻo là A(Ek, ft) với t ∈ [t|Tk-1|, t|Tk|]. Các chỉ số khác được đề xuất trong tài liệu là thước đo few-shot Learning Curve Area (Chaudhry et al., 2019a) và zero-shot Forward Transfer (Lopez-Paz & Ranzato, 2017).

Các chỉ số trước đó phụ thuộc vào nhiệm vụ học tập hiện tại Tk và do đó không trực tiếp áp dụng cho luồng dữ liệu bất khả tri nhiệm vụ. Như đối tác cho WFw, chúng tôi giới thiệu Windowed Plasticity (WPw) áp dụng tổng quát hơn, được định nghĩa trên tất cả N nhiệm vụ đánh giá bằng việc tăng độ chính xác tối đa Δw,+t,Ei trong một cửa sổ kích thước w. Phụ lục định nghĩa Δw,+t,Ei (Eq. 10) như Δw,−t,Ei nhưng với ràng buộc m > n.

WPw t = 1/N Σ(i=1 to N) max_n Δw,+n,Ei; ∀n ≤ t     (5)

3.3 CHỈ SỐ DựA TRÊN SỰ ĐÁNH ĐỔI ỔN ĐỊNH-TÍNH DẺO

Mục tiêu chính trong học tập liên tục là tìm ra sự cân bằng giữa học các nhiệm vụ mới Tk và giữ lại kiến thức từ các nhiệm vụ trước đó T<k, thường được gọi là sự đánh đổi ổn định-tính dẻo (Grossberg, 1982). Các chỉ số sự đánh đổi ổn định-tính dẻo cung cấp một chỉ số duy nhất để định lượng sự cân bằng này. Chỉ số tiêu chuẩn cho học tập liên tục là Độ chính xác Trung bình (ACC) mà sau khi học nhiệm vụ Tk tính trung bình hiệu suất trên tất cả các nhiệm vụ đánh giá: ACC_Tk = 1/k Σ(i=1 to k) A(Ei, ft|Tk|).

Nó cung cấp một thước đo sự đánh đổi bằng cách bao gồm độ chính xác của cả nhiệm vụ đánh giá hiện tại Ek (tính dẻo) và tất cả các nhiệm vụ đánh giá trước đó E<k (ổn định). ACC được đo chỉ tại mô hình cuối cùng ft|Tk| và bỏ qua hiệu suất giữa các quá trình chuyển đổi nhiệm vụ. Do đó, chúng tôi đề xuất Độ chính xác Trường hợp Xấu nhất (WC-ACC) như sự đánh đổi giữa độ chính xác trên lần lặp t của nhiệm vụ hiện tại Tk và chỉ số trường hợp xấu nhất min-ACC_Tk (xem Eq. 2) cho các nhiệm vụ trước đó:

WC-ACC_t = 1/k A(Ek, ft) + (1 - 1/k) min-ACC_Tk     (6)

Chỉ số này định lượng theo từng lần lặp độ chính xác tối thiểu mà các nhiệm vụ trước đó giữ lại sau khi được học, và độ chính xác của nhiệm vụ hiện tại. WC-ACC đưa ra một đảm bảo giới hạn dưới trên ACC được thiết lập qua các lần lặp. Đánh giá sau khi học nhiệm vụ Tk, chúng tôi kết luận giới hạn dưới sau:

WC-ACC_t|Tk| ≤ ACC_Tk     (7)

4 XÁC ĐỊNH KHOẢNG CÁCH ỔN ĐỊNH VỚI ĐÁNH GIÁ LIÊN TỤC

Chúng tôi bắt đầu bằng việc tiến hành một nghiên cứu thực nghiệm giới hạn trong Experience Replay (ER) (Chaudhry et al., 2019b), vì nó đã được chứng minh là thắng thế so với các phương pháp dựa trên regularization, đặc biệt là cho học tập tăng dần theo lớp và miền (van de Ven et al., 2022). ER lưu trữ một tập con nhỏ các mẫu trong một bộ đệm bộ nhớ, được xem xét lại sau đó bằng cách lấy mẫu mini-batch một phần là dữ liệu mới, một phần là dữ liệu bộ nhớ. Do tính đơn giản và hiệu quả chống lại việc quên thảm khốc, nó đã được áp dụng rộng rãi trong tài liệu (Rebuffi et al., 2017; Chaudhry et al., 2019b; De Lange & Tuytelaars, 2021; Chaudhry et al., 2018).

--- TRANG 3 ---
Được công bố như một bài báo hội nghị tại ICLR 2023

Hình 2: Độ chính xác trung bình (mean ± SD trên 5 seed) trên nhiệm vụ đầu tiên khi sử dụng ER. Các thí nghiệm là class-incremental (a-c) hoặc domain-incremental (d). Tần suất đánh giá dao động từ đánh giá tiêu chuẩn trên quá trình chuyển đổi nhiệm vụ (kim cương đỏ) đến đánh giá liên tục theo từng lần lặp (ρeval = 1). Đánh giá theo từng lần lặp tiết lộ những giảm sụt sắc nét, tạm thời trong hiệu suất khi học các nhiệm vụ mới: khoảng cách ổn định. Các đường thẳng đứng chỉ ra quá trình chuyển đổi nhiệm vụ; các đường ngang là min-ACC được tính trung bình trên các seed.

[Bốn biểu đồ hiển thị độ chính xác theo số lần lặp cho Split-MNIST, Split-CIFAR10, Split-MiniImagenet, và Mini-DomainNet]

Tập dữ liệu. Cho các thí nghiệm về học tập tăng dần theo lớp chúng tôi sử dụng ba tập dữ liệu tiêu chuẩn:
MNIST (LeCun & Cortes, 2010) bao gồm các chữ số viết tay màu xám, CIFAR10 (Krizhevsky et al., 2009) chứa hình ảnh từ một loạt các phương tiện và động vật, và MiniImagenet (Vinyals et al., 2016) là một tập con của Imagenet (Russakovsky et al., 2015). Split-MNIST, Split-CIFAR10, và Split-MiniImagenet được định nghĩa bằng cách chia dữ liệu thành 5, 5, và 20 nhiệm vụ dựa trên 10, 10, và 100 lớp tương ứng. Cho học tập tăng dần theo miền chúng tôi xem xét những thay đổi miền mạnh mẽ trong Mini-DomainNet (Zhou et al., 2021), một tập con thu nhỏ của 126 lớp của DomainNet (Peng et al., 2019) với hơn 90k hình ảnh, xem xét các miền: clipart, painting, real, sketch.

Thiết lập. Chúng tôi sử dụng đánh giá liên tục với tần suất đánh giá trong phạm vi ρeval ∈ {1, 10, 10², 10³} và kích thước tập con 1k mỗi nhiệm vụ đánh giá, dựa trên phân tích tính khả thi của chúng tôi trong Phụ lục B. Để tham khảo với tài liệu, các chỉ số dựa trên chuyển đổi nhiệm vụ ACC và FORG theo đánh giá tiêu chuẩn với toàn bộ tập kiểm tra. Split-MNIST sử dụng MLP với 2 lớp ẩn gồm 400 đơn vị. Split-CIFAR10, Split-MiniImagenet và Mini-DomainNet sử dụng phiên bản mỏng của Resnet18 (Lopez-Paz & Ranzato, 2017). Tối ưu hóa SGD được sử dụng với momentum 0.9. Để đảm bảo phân tích trường hợp xấu nhất của chúng tôi áp dụng cho cấu hình trường hợp tốt nhất cho ER, chúng tôi chạy gridsearch trên các siêu tham số khác nhau và chọn mục có chỉ số sự đánh đổi ổn định-tính dẻo ACC cao nhất trên dữ liệu đánh giá giữ lại (Lopez-Paz & Ranzato, 2017). Chi tiết cho tất cả các thí nghiệm có thể được tìm thấy trong Phụ lục C và mã có sẵn tại đây: https://github.com/mattdl/ContinualEvaluation.

Phân tích định tính với đánh giá liên tục. Hình 2 minh họa các đường cong độ chính xác nhiệm vụ đầu tiên cho ER trên 4 benchmark. Các dấu hiệu đỏ trên quá trình chuyển đổi nhiệm vụ chỉ ra sơ đồ đánh giá tiêu chuẩn trong học tập liên tục. Chúng tôi thấy rằng đánh giá liên tục (ρeval = 1) tiết lộ việc quên tạm thời đáng kể giữa các quá trình chuyển đổi nhiệm vụ, cả cho các benchmark tăng dần theo lớp và miền. Sau những giảm sụt hiệu suất đáng kể, việc phục hồi một phần theo sau, làm cho các chỉ số dựa trên chuyển đổi nhiệm vụ như ACC và FORG trở thành những ước tính hiệu suất trường hợp xấu nhất kém. Chúng tôi gọi hiện tượng quên tạm thời, đáng kể này khi học các nhiệm vụ mới là khoảng cách ổn định.

Định lượng hiệu suất trường hợp xấu nhất. Phân tích định tính cho phép quan sát khoảng cách ổn định từ các đường cong độ chính xác theo thời gian. Tuy nhiên, phân tích hiệu suất cũng yêu cầu một thước đo định lượng cho hiệu suất trường hợp xấu nhất. Cho class-incremental Split-MiniImagenet, chúng tôi báo cáo kết quả cho các chỉ số đánh giá liên tục mới của chúng tôi trong Bảng 1, từ đó có thể rút ra hai quan sát quan trọng. Đầu tiên, chúng tôi xác nhận rằng chỉ số tiêu chuẩn ACC được đo trên quá trình chuyển đổi nhiệm vụ bỏ qua khoảng cách ổn định, trong khi các chỉ số hiệu suất trường hợp xấu nhất của chúng tôi min-ACC và WC-ACC rõ ràng chỉ ra sự mất mát hiệu suất khủng khiếp. Thứ hai, các chỉ số của chúng tôi chỉ ra tương tự như Hình 2 rằng tần suất đánh giá chi tiết là quan trọng để xác định khoảng cách ổn định. Bảng 1 cho thấy những giảm sụt hiệu suất sắc nét được loại bỏ dần khi min-ACC và WC-ACC đều tăng với tần suất đánh giá ρeval, và việc quên quy mô lớn WF100 giảm. Bảng 3 trong Phụ lục báo cáo kết quả tương tự trên các benchmark khác. Ngoài ra, Phụ lục D báo cáo kết quả cho khoảng cách ổn định cho các kích thước bộ đệm ER khác nhau, trong một thí nghiệm phương thức giọng nói, và cho học tập liên tục trực tuyến.

Bảng 1: Các chỉ số đánh giá liên tục mới được đề xuất của chúng tôi trên class-incremental Split-MiniImagenet cho một loạt tần suất đánh giá ρeval. Các chỉ số học tập liên tục tiêu chuẩn là 32.9±0.8 (ACC) và -32.3±1.0 (FORG). Kết quả trên 5 seed được báo cáo là mean ± SD.

[Bảng hiển thị các chỉ số Trade-off, Stability, và Plasticity cho các giá trị ρeval khác nhau]

4.1 ẢNH HƯỞNG CỦA SỰ TƯƠNG TỰ NHIỆM VỤ LÊN KHOẢNG CÁCH ỔN ĐỊNH

Khi sự dịch chuyển miền giữa các nhiệm vụ tiếp theo trong luồng tăng lên, sự can thiệp của các mục tiêu được dự kiến dẫn đến việc quên cao hơn (FORG) và do đó độ chính xác trung bình thấp hơn (ACC) trên tất cả các nhiệm vụ. Tuy nhiên, hiệu ứng lên khoảng cách ổn định vẫn chưa rõ ràng.

Thiết lập thí nghiệm. Chúng tôi thiết lập một thí nghiệm học tập tăng dần theo miền có kiểm soát với Rotated-MNIST, trong đó mỗi nhiệm vụ cấu thành toàn bộ tập dữ liệu MNIST với một phép biến đổi xoay tăng dần nhưng cố định φ độ. Việc xoay cụ thể theo nhiệm vụ tăng dần dẫn đến những dịch chuyển miền có thể kiểm soát qua các nhiệm vụ trong không gian đầu vào. Để tránh sự mơ hồ giữa các chữ số 6 và 9, chúng tôi hạn chế tổng xoay không vượt quá 180°. Chúng tôi xem xét một xoay cố định tích lũy φ độ, dẫn đến 0°, (0°+φ), và (0°+2φ) cho ba nhiệm vụ, với 0° được đặt thành 90° cho xoay nhiệm vụ ban đầu. Để tăng sự khác biệt nhiệm vụ, chúng tôi xem xét một loạt xoay tương đối tăng dần φ = [10°, 30°, 60°, 80°]. Chúng tôi nghiên cứu ER với dung lượng bộ đệm 1k mẫu.

Kết quả. Bảng 2 xác nhận sự giảm trong ACC khi sự dịch chuyển phân phối giữa các nhiệm vụ tăng lên. Giữa chuỗi dễ nhất (φ = 10°) và khó nhất (φ = 80°), ACC giảm 6.6%. Tuy nhiên, hiệu ứng lên khoảng cách ổn định lớn hơn đáng kể khi min-ACC giảm từ 94.3% xuống 69.2%, giảm 25.1%. Tương tự, FORG giữa φ = 60° và φ = 80° chỉ ra chỉ 1.1% quên nhiều hơn, trong khi WF10 cho thấy sự giảm 5.0%. Điều này chỉ ra cả i) các chỉ số tiêu chuẩn thất bại trong việc nắm bắt hiệu ứng của khoảng cách ổn định, và ii) khoảng cách ổn định tăng đáng kể cho những dịch chuyển phân phối lớn hơn. Như xác nhận bổ sung, các đường cong độ chính xác cho nhiệm vụ đầu tiên trong Hình 3 cho thấy một cách định tính rằng khoảng cách ổn định tăng đáng kể với việc tăng xoay.

Bảng 2: Kết quả tương tự nhiệm vụ cho ER trong Rotated-MNIST. Ba nhiệm vụ tiếp theo được xây dựng bằng cách xoay hình ảnh MNIST φ độ mỗi nhiệm vụ. Chúng tôi giảm tương tự nhiệm vụ bằng cách tăng xoay từ φ = 10° đến φ = 80°. Kết quả trên 5 seed được báo cáo là mean ± SD.

[Bảng hiển thị các chỉ số cho các giá trị φ khác nhau]

--- TRANG 4 ---
Được công bố như một bài báo hội nghị tại ICLR 2023

Hình 3: Thí nghiệm tương tự nhiệm vụ có kiểm soát với góc xoay nhiệm vụ trong Rotated-MNIST. Các đường cong độ chính xác cho ER trong nhiệm vụ đầu tiên được báo cáo là mean ± SD trên 5 seed. Hai đường thẳng đứng chỉ ra quá trình chuyển đổi nhiệm vụ. Các đường ngang chỉ ra min-ACC được tính trung bình trên các seed.

[Biểu đồ hiển thị độ chính xác theo số lần lặp cho các góc φ khác nhau]

5 PHÂN TÍCH KHÁI NIỆM CỦA KHOẢNG CÁCH ỔN ĐỊNH

Để thiết lập một nền tảng khái niệm cho hiện tượng khoảng cách ổn định, chúng tôi tách biệt các gradient học tập liên tục của mục tiêu L thành các gradient tính dẻo và ổn định có trọng số α:

∇L = α∇L_plasticity + (1-α)∇L_stability     (8)

Trong tối ưu hóa dựa trên gradient của mô hình f, ∇L_plasticity nhằm cải thiện hiệu suất trên nhiệm vụ hiện tại và ∇L_stability duy trì hiệu suất trên các nhiệm vụ quá khứ. Trong quan điểm của sự đánh đổi ổn định-tính dẻo (Grossberg, 1982), các phương pháp học tập liên tục nhằm cân bằng cả hai số hạng gradient. Tuy nhiên, do đánh giá hướng nhiệm vụ và thiếu đánh giá liên tục, tiến bộ trong sự cân bằng này chỉ được xác minh trên các quá trình chuyển đổi nhiệm vụ.

Finetuning tối ưu hóa chỉ cho nhiệm vụ hiện tại Tk và không quan tâm đến tính ổn định, với ||∇L_stability|| = 0. Việc thiếu số hạng ổn định dẫn đến các cập nhật tham lam cho dữ liệu quan sát hiện tại, dẫn đến việc quên trên các dịch chuyển phân phối. Khung đánh giá liên tục của chúng tôi chỉ ra rằng việc quên nghiêm trọng các nhiệm vụ quá khứ xảy ra ngay trong vài lần lặp huấn luyện đầu tiên trên một nhiệm vụ mới (xem Phụ lục D.3).

Experience replay (ER) học đồng thời từ dữ liệu của nhiệm vụ mới và một tập con dữ liệu nhiệm vụ trước đó được lấy mẫu từ bộ đệm kinh nghiệm M có kích thước M. Số hạng mất mát L_stability được thu được bằng cách xem lại các mẫu nhiệm vụ trước đó trong M. Ngược lại với finetuning, điều này dẫn đến một gradient ổn định ∇L_stability được thiết kế để ngăn chặn việc quên thảm khốc.

Việc quên. Phân tích thêm của chúng tôi về động lực gradient ∇L_stability chỉ ra một chuẩn gradient thấp trong giai đoạn đầu của việc huấn luyện trên một nhiệm vụ mới. Đầu tiên chúng tôi xem xét một luồng dữ liệu với hai nhiệm vụ tiếp theo. Khi việc huấn luyện bắt đầu trên T2, f có thể đã hội tụ cho nhiệm vụ đầu tiên T1, dẫn đến ||∇L_T1|| ≈ 0. Do đó, ngay sau quá trình chuyển đổi nhiệm vụ, chúng ta thực sự có ||∇L_stability|| ≈ 0 vì các mẫu được phát lại độc quyền từ T1. Tổng quát hóa điều này thành chuỗi nhiệm vụ dài hơn yêu cầu không chỉ gradient gần như zero cho nhiệm vụ trước đó, mà cho tất cả dữ liệu nhiệm vụ trước đó trong toàn bộ bộ đệm phát lại M. Điều này đã được xác nhận thực nghiệm bởi Verwimp et al. (2021) chỉ ra ER hội tụ nhất quán đến các gradient gần như zero cho M. Chúng tôi chứng minh những phát hiện tương tự cho Split-MNIST trong Hình 4(e-h). Do sự mất cân bằng của các gradient ổn định và tính dẻo, gradient tính dẻo sẽ thống trị và có thể dẫn đến việc quên. Những phát hiện thực nghiệm của chúng tôi thực sự xác nhận cho ER rằng những giảm sụt đáng kể trong độ chính xác xảy ra trực tiếp sau quá trình chuyển đổi nhiệm vụ, như được thể hiện trong Hình 2.

Phục hồi. Khi các bước ban đầu của việc học một nhiệm vụ mới với các cập nhật ∇L_plasticity tham lam, các tham số thay đổi với hầu như không có tín hiệu gradient từ bộ đệm phát lại M. Những cập nhật này lần lượt dẫn đến việc tăng chuẩn gradient của ∇L_stability, cho phép các mẫu trong M được học lại.

Ngược lại với niềm tin trước đây rằng ∇L_stability duy trì kiến thức trước đó, chúng tôi thấy rằng tính ổn định được bảo tồn ít nhất một phần bằng phương tiện học lại, dẫn đến việc phục hồi kiến thức trước đó. Điều này được xác nhận bởi các thí nghiệm của chúng tôi trong Phần 4 trên năm benchmark tăng dần theo lớp và miền.

--- TRANG 5 ---
Được công bố như một bài báo hội nghị tại ICLR 2023

5.1 TÌM KIẾM KHOẢNG CÁCH ỔN ĐỊNH TRONG CÁC PHƯƠNG PHÁP HỌC TẬP LIÊN TỤC KHÁC

Phân tích của chúng tôi về việc tách biệt các gradient dựa trên tính dẻo và ổn định cũng áp dụng cho các phương pháp đại diện khác trong học tập liên tục. Ngoài ra, chúng tôi cung cấp bằng chứng thực nghiệm cho khoảng cách ổn định trong các phương pháp này, tập trung vào phương pháp constraint-based replay GEM (Lopez-Paz & Ranzato, 2017), phương pháp knowledge-distillation LwF (Li & Hoiem, 2017), và các phương pháp parameter regularization EWC (Kirkpatrick et al., 2017) và SI (Zenke et al., 2017). Chi tiết của các thí nghiệm có trong Phụ lục C.

Hình 4: Các đường cong độ chính xác GEM và ER (a-d) trên class-incremental Split-MNIST cho bốn nhiệm vụ đầu tiên, và các chuẩn L2 theo từng lần lặp của ∇L_stability hiện tại (e-h). Kết quả được báo cáo là mean ± SD trên 5 seed, với các đường ngang đại diện cho min-ACC trung bình. Các đường thẳng đứng chỉ ra sự bắt đầu của một nhiệm vụ mới. Lưu ý rằng tỷ lệ trục x thay đổi qua (a-d) và (e-h) là zoom của 50 lần lặp đầu tiên.

[Tám biểu đồ con hiển thị độ chính xác và gradient norm cho các nhiệm vụ T1-T4]

Gradient-constrained replay. Gradient Episodic memory (GEM) (Lopez-Paz & Ranzato, 2017) khai thác một bộ đệm bộ nhớ M tương tự như ER, được chia cho K bộ đệm nhiệm vụ có kích thước bằng nhau Mk. Tuy nhiên, thay vì tối ưu hóa trực tiếp mục tiêu cho các mẫu trong M, các gradient cụ thể theo nhiệm vụ gk = ∇L(Mk) của chúng được sử dụng để tạo thành một tập hợp các ràng buộc. Các ràng buộc ⟨gt, gn⟩ ≥ 0, ∀n < k cố gắng ngăn chặn sự tăng mất mát trên k-1 nhiệm vụ trước đó, với gt = ∇L_plasticity là gradient của mẫu quan sát hiện tại (x,y)t trong nhiệm vụ Tk. Gradient hiện tại gt được chiếu lên gradient gần nhất g̃ thỏa mãn tập ràng buộc, thu được bằng Quadratic Programming. Chúng tôi tái công thức hóa phép chiếu để thu được gradient ổn định có điều kiện:

∇L_stability = {0̃, nếu ⟨gt, gn⟩ ≥ 0, ∀n < k
                g̃ - gt, ngược lại                    (9)

Vì các ràng buộc GEM cố gắng rõ ràng ngăn chặn sự tăng trong các mất mát nhiệm vụ trước đó, ||∇L_stability|| chỉ bằng zero nếu cập nhật gradient hiện tại gt nằm trong vùng khả thi của tập ràng buộc hoặc gần zero với chỉ các ràng buộc bị vi phạm nhẹ ||g̃ - gt|| ≈ 0. Vì trong các ràng buộc tích vô hướng dấu hiệu được xác định chỉ dựa trên các góc gradient, việc thỏa mãn chúng độc lập với chuẩn của ||∇L_stability|| trên các quá trình chuyển đổi nhiệm vụ. Điều này gợi ý rằng GEM có thể cho phép tránh hoặc giảm thiểu khoảng cách ổn định.

Tuy nhiên, thực nghiệm chúng tôi thấy rằng GEM cũng dễ bị khoảng cách ổn định (Hình 4). So với ER, khoảng cách ổn định của GEM lớn hơn đáng kể, được chỉ ra bởi sự khác biệt lớn trong các đường ngang đại diện cho min-ACC trung bình. Trên các quá trình chuyển đổi nhiệm vụ cho Split-MNIST, Hình 4(e-h) cho thấy rằng GEM có ||∇L_stability|| đáng kể so với ER (được xác định theo Eq. 9), chỉ ra vi phạm lớn các ràng buộc. Tuy nhiên, đặc biệt cho các quá trình chuyển đổi nhiệm vụ T3 và T4 chúng tôi quan sát các gradient giảm xuống độ lớn gần zero, dẫn đến một vài cập nhật chủ yếu dựa trên ∇L_plasticity.

Các phương pháp dựa trên distillation (Li & Hoiem, 2017; Rannen et al., 2017) ngăn chặn việc quên một nhiệm vụ trước đó Tk-1 bằng cách phạt các thay đổi trong phân phối đầu ra cho các mẫu trong nhiệm vụ hiện tại x ∈ D̃k. Điều này dẫn đến một mục tiêu regularization L_stability = KL(ft|Tk-1|(xt)||ft(xt)) distill kiến thức qua KL-divergence (Hinton et al., 2015) từ mô hình nhiệm vụ trước đó tại t|Tk-1| đến mô hình hiện tại. Trước cập nhật đầu tiên trên một nhiệm vụ mới, các mô hình nhiệm vụ trước đó và hiện tại giống hệt nhau, dẫn đến ||∇L_stability|| = 0 do các phân phối khớp hoàn hảo, có thể dẫn đến khoảng cách ổn định.

Các phương pháp parameter regularization (Kirkpatrick et al., 2017; Zenke et al., 2017; Aljundi et al., 2018; Kao et al., 2021), còn được gọi là các phương pháp dựa trên model-prior, phạt các thay đổi trong các tham số quan trọng cho các nhiệm vụ trước đó với mục tiêu regularization L_stability = (θ - θ*)^T Ω (θ - θ*), trong đó tầm quan trọng tham số được định nghĩa bởi ma trận phạt Ω ∈ R^|θ|×|θ| và được cân đối bởi sự thay đổi của tham số mô hình w.r.t. giải pháp nhiệm vụ trước đó θ* = θt|Tk-1|. Cho các cập nhật đầu tiên trên một nhiệm vụ mới, các tham số vẫn gần với giải pháp nhiệm vụ trước đó dẫn đến ||∇L_stability|| ≈ 0, và thậm chí giống hệt nhau cho cập nhật đầu tiên, dẫn đến ||∇L_stability|| = 0.

Hình 5 xác nhận thực nghiệm khoảng cách ổn định cho các phương pháp đại diện dựa trên distillation (LwF) và parameter regularization (EWC, SI) trong hai cài đặt: đầu tiên, task-incremental Split-MNIST, trong đó mỗi nhiệm vụ được phân bổ một bộ phân loại riêng biệt (Hình 5(a-d)); thứ hai, Rotated-MNIST từ Phần 4.1 với tương tự nhiệm vụ thấp nhất (φ = 80°) giữa 3 nhiệm vụ. Chi tiết thiết lập được báo cáo trong Phụ lục C.

Hình 5: Các đường cong độ chính xác của các phương pháp dựa trên distillation (LwF) và parameter regularization (EWC, SI) trên bốn nhiệm vụ đầu tiên (a-d) của task-incremental Split-MNIST, với một đầu phân loại riêng biệt cho mỗi nhiệm vụ. Các phương pháp tương tự được xem xét cho domain-incremental Rotated-MNIST (φ = 80°) cho hai nhiệm vụ đầu tiên (e-f) trong số ba. Kết quả được báo cáo là mean ± SD trên 5 seed, với các đường ngang đại diện cho min-ACC trung bình. Các đường thẳng đứng chỉ ra sự bắt đầu của một nhiệm vụ mới.

[Sáu biểu đồ con hiển thị độ chính xác cho các phương pháp khác nhau]

6 KẾT LUẬN

Công trình này đề xuất một khung mới cho đánh giá liên tục với các chỉ số mới cho phép đo lường hiệu suất trường hợp xấu nhất và áp dụng cho luồng dữ liệu bất khả tri nhiệm vụ. Khung đánh giá của chúng tôi đã xác định những thiếu sót của giao thức đánh giá hướng nhiệm vụ tiêu chuẩn cho học tập liên tục, vì chúng tôi đã xác định một hiện tượng nổi bật và có thể có vấn đề: khoảng cách ổn định. Trong nghiên cứu của chúng tôi trên bảy benchmark học tập liên tục, chúng tôi đã chỉ ra rằng khi bắt đầu học một nhiệm vụ mới, các phương pháp học tập liên tục tiên tiến khác nhau gặp phải mất mát đáng kể trong hiệu suất trên các nhiệm vụ đã học trước đó mà, một cách hấp dẫn, thường được phục hồi sau đó. Chúng tôi thấy khoảng cách ổn định tăng đáng kể khi các nhiệm vụ tiếp theo khác biệt hơn. Để cung cấp cái nhìn sâu sắc về những gì có thể gây ra khoảng cách ổn định, chúng tôi đã chính thức hóa một phân tích khái niệm của các gradient, được tách biệt thành các số hạng tính dẻo và ổn định. Điều này dẫn đến những phát hiện nhất quán cho experience replay, knowledge-distillation, và các phương pháp parameter regularization, nhưng để lại một số câu hỏi mở cho constraint-based replay. Các hướng thú vị cho công việc tương lai bao gồm các cơ chế để khắc phục khoảng cách ổn định và kết nối với mạng nơ-ron sinh học: khi học điều gì đó mới, liệu bộ não có gặp phải việc quên tạm thời không?

--- TRANG 6 ---
Được công bố như một bài báo hội nghị tại ICLR 2023

LỜI CẢM ơN

Dự án này đã nhận được tài trợ từ dự án ERC KeepOnLearning (số tham chiếu 101021347), dự án KU Leuven C1 Macchina, và chương trình nghiên cứu và đổi mới Horizon 2020 của Liên minh Châu Âu theo thỏa thuận tài trợ Marie Skłodowska-Curie số 101067759.

TÀI LIỆU THAM KHẢO

Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C. Fowlkes, Stefano Soatto, và Pietro Perona. Task2vec: Task embedding for meta-learning. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.

Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, và Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. Trong ECCV, pp. 139–154, 2018.

Rahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Min Lin, Laurent Charlin, và Tinne Tuytelaars. Online continual learning with maximally interfered retrieval. Proceedings NeurIPS 2019, 32, 2019a.

Rahaf Aljundi, Min Lin, Baptiste Goujaud, và Yoshua Bengio. Gradient based sample selection for online continual learning. NeurIPS, pp. 11816–11825, 2019b.

Johannes Buchner. Synthetic speech commands: a public dataset for single-word speech recognition. Kaggle Dataset, 2017. URL https://www.kaggle.com/datasets/jbuchner/synthetic-speech-commands-dataset.

Lucas Caccia, Rahaf Aljundi, Nader Asadi, Tinne Tuytelaars, Joelle Pineau, và Eugene Belilovsky. New insights on reducing abrupt representation change in online continual learning. Trong International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=N8MaByOzUfb.

Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, và Philip HS Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. Trong ECCV, pp. 532–547, 2018.

Arslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, và Mohamed Elhoseiny. Efficient lifelong learning with a-GEM. Trong International Conference on Learning Representations, 2019a. URL https://openreview.net/forum?id=Hkf2_sC5FX.

Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, và Marc'Aurelio Ranzato. Continual learning with tiny episodic memories. arXiv preprint arXiv:1902.10486, 2019b.

Aristotelis Chrysakis và Marie-Francine Moens. Online continual learning from imbalanced data. Proceedings of Machine Learning Research, 2020.

Andrea Cossu, Antonio Carta, Vincenzo Lomonaco, và Davide Bacciu. Continual learning for recurrent neural networks: An empirical evaluation. Neural Networks, 143:607–627, 2021.

Matthias De Lange và Tinne Tuytelaars. Continual prototype evolution: Learning online from non-stationary data streams. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8250–8259, 2021.

Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Greg Slabaugh, và Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1–1, 2021. doi: 10.1109/TPAMI.2021.3057446.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

--- TRANG 7 ---
Được công bố như một bài báo hội nghị tại ICLR 2023

Enrico Fini, Stéphane Lathuiliere, Enver Sangineto, Moin Nabi, và Elisa Ricci. Online continual learning under extreme memory constraints. Trong European Conference on Computer Vision, pp. 720–735. Springer, 2020.

Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3(4):128–135, 1999.

Stephen Grossberg. Studies of mind and brain : neural principles of learning, perception, development, cognition, and motor control. Boston studies in the philosophy of science 70. Reidel, Dordrecht, 1982. ISBN 9027713596.

Geoffrey Hinton, Oriol Vinyals, và Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.

Ta-Chu Kao, Kristopher Jensen, Gido van de Ven, Alberto Bernacchia, và Guillaume Hennequin. Natural continual learning: success is a journey, not (just) a destination. Trong M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, và J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 28067–28079. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/ec5aa0b7846082a2415f0902f0da88f2-Paper.pdf.

James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. PNAS, pp. 201611835, 2017.

Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.

Alex Krizhevsky, Ilya Sutskever, và Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Trong F. Pereira, C.J. Burges, L. Bottou, và K.Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 25. Curran Associates, Inc., 2012. URL https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf.

Yann LeCun và Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.lecun.com/exdb/mnist/.

Zhizhong Li và Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017.

Vincenzo Lomonaco, Lorenzo Pellegrini, Andrea Cossu, Antonio Carta, Gabriele Graffeti, Tyler L Hayes, Matthias De Lange, Marc Masana, Jary Pomponi, Gido M van de Ven, et al. Avalanche: an end-to-end library for continual learning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 3600–3610, 2021.

David Lopez-Paz và Marc'Aurelio Ranzato. Gradient episodic memory for continual learning. Trong Advances in neural information processing systems, pp. 6467–6476, 2017.

Arun Mallya và Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. Trong Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 7765–7773, 2018.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, và Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.

German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, và Stefan Wermter. Continual lifelong learning with neural networks: A review. Neural Networks, 2019.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, và Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. Trong H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, và R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.

Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, và Bo Wang. Moment matching for multi-source domain adaptation. Trong Proceedings of the IEEE International Conference on Computer Vision, pp. 1406–1415, 2019.

Amal Rannen, Rahaf Aljundi, Matthew B Blaschko, và Tinne Tuytelaars. Encoder based lifelong learning. Trong ICCV, pp. 1320–1328, 2017.

Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, và Christoph H Lampert. icarl: Incremental classifier and representation learning. Trong CVPR, pp. 2001–2010, 2017.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 115(3):211–252, 2015.

Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, và Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.

Joan Serra, Didac Suris, Marius Miron, và Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. Trong International Conference on Machine Learning, pp. 4548–4557. PMLR, 2018.

Dongsub Shim, Zheda Mai, Jihwan Jeong, Scott Sanner, Hyunwoo Kim, và Jongseong Jang. Online class-incremental continual learning with adversarial shapley value. Trong Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 9630–9638, 2021.

Gido M van de Ven, Hava T Siegelmann, và Andreas S Tolias. Brain-inspired replay for continual learning with artificial neural networks. Nature Communications, 11:4069, 2020.

Gido M van de Ven, Tinne Tuytelaars, và Andreas S Tolias. Three types of incremental learning. Nature Machine Intelligence, 4:1185–1197, 2022.

Vinay Kumar Verma, Kevin J Liang, Nikhil Mehta, Piyush Rai, và Lawrence Carin. Efficient feature transformations for discriminative and generative continual learning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13865–13875, 2021.

Eli Verwimp, Matthias De Lange, và Tinne Tuytelaars. Rehearsal revealed: The limits and merits of revisiting samples in continual learning. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 9385–9394, October 2021.

Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, và Daan Wierstra. Matching networks for one shot learning. Trong Proceedings of the 30th International Conference on Neural Information Processing Systems, pp. 3637–3645, 2016.

Friedemann Zenke, Ben Poole, và Surya Ganguli. Continual learning through synaptic intelligence. Trong International Conference on Machine Learning, pp. 3987–3995. PMLR, 2017.

Kaiyang Zhou, Yongxin Yang, Yu Qiao, và Tao Xiang. Domain adaptive ensemble learning. IEEE Transactions on Image Processing, 30:8008–8018, 2021.

--- TRANG 8 ---
Được công bố như một bài báo hội nghị tại ICLR 2023

TÀI LIỆU BỔ SUNG

Tài liệu bổ sung bao gồm một thảo luận về các hạn chế và tác động xã hội của công trình này (Phụ lục A), một nghiên cứu thực nghiệm và thảo luận về tính khả thi tính toán cho đánh giá liên tục (Phụ lục B), chi tiết tái tạo chi tiết cho tất cả các thí nghiệm (Phụ lục C), và bằng chứng thực nghiệm bổ sung trong Phụ lục D.

A HẠN CHẾ VÀ TÁC ĐỘNG XÃ HỘI

Độ phức tạp tính toán của đánh giá liên tục. Cả độ phức tạp thời gian và không gian đều là các yếu tố quan trọng cho học tập liên tục với tài nguyên hạn chế. Độ phức tạp không gian cho đánh giá liên tục giữ lại sự tăng tuyến tính với số lượng nhiệm vụ k như đối với đánh giá tiêu chuẩn. Tuy nhiên, độ phức tạp thời gian thay đổi từ tần suất dựa trên nhiệm vụ sang tần suất theo từng lần lặp. Tỷ lệ tăng tính toán cho một nhiệm vụ duy nhất Tk có thể được định nghĩa là (t|Tk| - t|Tk-1|)/ρeval trong đó t|Tk| - t|Tk-1| là số lần lặp cho nhiệm vụ đó. Mặc dù sự tăng về độ phức tạp thời gian, do những phát hiện thực nghiệm của chúng tôi, chúng tôi ủng hộ việc sử dụng đánh giá liên tục khi có thể để phân tích hành vi học tập và đặc biệt cho các ứng dụng quan trọng về an toàn. Chúng tôi tham khảo Phụ lục B để cho phép đánh giá liên tục dễ quản lý.

Các phương pháp cô lập tham số (De Lange et al., 2021) không được xem xét trong nghiên cứu thực nghiệm của chúng tôi về khoảng cách ổn định, vì thông thường các phần cố định của mô hình được phân bổ cho các nhiệm vụ cụ thể (Rusu et al., 2016; Verma et al., 2021; Serra et al., 2018; Mallya & Lazebnik, 2018). Định danh nhiệm vụ thường được giả định có sẵn cho suy luận, cho phép kích hoạt chỉ mạng con cho nhiệm vụ đó. Điều này cho phép ngăn chặn bất kỳ thay đổi nào và do đó việc quên cho các nhiệm vụ quan sát được, mặc dù cản trở việc chuyển giao ngược. Do đó, đối với các phương pháp mà không cho phép thay đổi hiệu suất của các nhiệm vụ đã học, khoảng cách ổn định cũng không thể hiện diện.

Tác động xã hội. Một tác nhân học tập liên tục hoạt động trong thế giới thực có thể dẫn đến kết quả không thể dự đoán vì tác nhân có thể quan sát các điểm dữ liệu độc hại, học từ dữ liệu có thiên lệch cao, hoặc học hành vi không mong muốn khác biệt với các mục tiêu ban đầu của nó. Sự tiến hóa như vậy có thể vẫn không được phát hiện với đánh giá thưa thớt hoặc thậm chí không đánh giá người học. Ngoài ra, những thay đổi phân phối đột ngột có thể bị khai thác bởi kẻ thù để giảm hiệu suất một cách tạm thời nhưng đáng kể. Do đó, chúng tôi coi đánh giá liên tục quan trọng cho việc giám sát chi tiết hành vi học của tác nhân.

B ĐÁNH GIÁ LIÊN TỤC QUẢN LÝ ĐƯỢC

Thách thức trong đánh giá liên tục là hai mặt. Đầu tiên, tần suất đánh giá cao là đòi hỏi tính toán. Thứ hai, tập hợp các phân phối kiểm tra ổn định cục bộ của đánh giá viên có thể mở rộng với số lượng nhiệm vụ. Hơn nữa, chúng tôi thảo luận những cân nhắc thực tế cho các ứng dụng thế giới thực bằng cách sử dụng dữ liệu huấn luyện cho đánh giá và song song.

Tần suất đánh giá. Một sự nới lỏng đầu tiên cho đánh giá liên tục khả thi là tăng tần suất đánh giá theo từng cập nhật ρeval = 1 đến các giá trị lớn hơn. Chúng tôi thực hiện một phân tích cho ρeval ∈ {1, 10, 10², 10³} trên 4 benchmark cho Experience Replay (ER). Chúng tôi theo thiết lập đầy đủ và tập dữ liệu được thảo luận trong Phần 4 và Phụ lục C. Quan trọng cho phân tích này là những phát hiện của chúng tôi trong Phần 4 cho thấy đánh giá liên tục tiết lộ những giảm sụt thảm khốc trong hiệu suất sau các quá trình chuyển đổi nhiệm vụ. Bảng 1 cho thấy cho Split-MiniImagenet rằng những giảm sụt sắc nét này được loại bỏ dần khi min-ACC và WC-ACC đều tăng với tần suất đánh giá ρeval, và việc quên quy mô lớn WF100 giảm. Hiện tượng này được minh họa trong Hình 2, trong đó việc tăng thêm tần suất lên đến đánh giá tiêu chuẩn trên các quá trình chuyển đổi nhiệm vụ trở nên hoàn toàn bỏ qua những giảm sụt hiệu suất này. Do đó, đánh giá liên tục của các thí nghiệm của chúng tôi trong bài báo chính áp dụng ρeval = 1 trừ khi có đề cập khác.

Bổ sung cho kết quả Split-MiniImagenet ER, chúng tôi cũng báo cáo kết quả cho Split-MNIST, Split-CIFAR10, và Mini-DomainNet trong Bảng 3. Chúng tôi thấy kết luận nhất quán như cho Split-MiniImagenet, tức là khoảng cách ổn định không được quan sát cho tần suất đánh giá lớn hơn, đặc biệt đáng chú ý cho ρeval ∈ {100, 1000} được chỉ ra bằng chữ đậm. Kích thước mẫu con của các tập đánh giá là 1000 như cho các thí nghiệm khác trong công trình này.

Chúng tôi nhận thấy rằng kết quả trong Split-CIFAR10 có phương sai lớn trong min-ACC (và do đó WC-ACC) cho ρeval = 100. Vì độ chính xác Split-CIFAR10 ồn ào qua các lần lặp như được chỉ ra trong Hình 2, chúng tôi thấy trong một số lần chạy cho ρeval = 100 hoàn toàn bỏ lỡ việc giảm độ chính xác xuống zero của T4 cho 2 trong số 5 seed. Sự phụ thuộc vào seed khởi tạo này nhấn mạnh tầm quan trọng của ρeval nhỏ hơn để giảm sự phụ thuộc vào điểm đánh giá chính xác.

Bảng 3: Kết quả đầy đủ trên Split-MNIST, Split-CIFAR10, Split-MiniImagenet và Mini-DomainNet trong các chỉ số đánh giá liên tục cho một loạt tần suất đánh giá ρeval. Kết quả trên 5 seed được báo cáo là mean ± SD. Tập con Split-MiniImagenet của kết quả đầy đủ được báo cáo trong bài báo chính trong Bảng 1.

[Bảng lớn hiển thị các chỉ số cho nhiều tập dữ liệu và giá trị ρeval]

Lấy mẫu con các tập đánh giá. Một sự nới lỏng thứ hai cho đánh giá liên tục khả thi là kiểm soát số lượng mẫu đánh giá. Tập hợp các nhiệm vụ đánh giá TE thường tăng tuyến tính theo thời gian. Điều này không thể tránh khỏi nếu chúng ta muốn cung cấp đảm bảo hiệu suất dựa trên dữ liệu cho mỗi nhiệm vụ đã học. Tuy nhiên, chúng ta có thể giảm thời gian tính toán tuyệt đối bằng cách hạn chế số lượng mẫu mỗi tập đánh giá. Với ρeval = 1 và lấy mẫu con đồng đều mỗi tập dữ liệu nhiệm vụ đánh giá D̃E,i, Bảng 4 chỉ ra cho Split-MNIST, Split-CIFAR10, và Split-MiniImagenet rằng kích thước mẫu 1k cung cấp một xấp xỉ tốt cho việc sử dụng toàn bộ tập kiểm tra ('All'). Kích thước mẫu nhỏ hơn 100 gây ra nhiều nhiễu hơn trong ước tính hiệu suất, dẫn đến độ lệch lớn hơn so với toàn bộ tập dữ liệu, đặc biệt đáng chú ý cho WF100 và WP100.

Thay vì lấy mẫu đồng đều, một sơ đồ lấy mẫu thay thế nhưng phức tạp hơn cũng sẽ có thể để hạn chế số lượng mẫu đánh giá. Ví dụ bằng cách đo tương tự nhiệm vụ (ví dụ qua task2vec (Achille et al., 2019)) và hợp nhất dữ liệu kiểm tra của các nhiệm vụ tương tự trong các bin dung lượng cố định. Mặc dù điều này sẽ áp dụng cho người học thế giới thực, trong phân tích của chúng tôi chúng tôi chọn lấy mẫu con đơn giản để tránh các yếu tố gây nhiễu bắt nguồn từ quy trình tương tự nhiệm vụ.

Khai thác dữ liệu huấn luyện. Một bất lợi của việc trích xuất dữ liệu đánh giá giữ lại từ luồng dữ liệu là dữ liệu này không thể được sử dụng bởi người học. Tập trung vào Split-MiniImagenet, Bảng 4 cho thấy đánh giá liên tục của ER với các tập con huấn luyện so với dữ liệu đánh giá thực tế, với ACC tương tự 31.5±0.5 cho tập huấn luyện và thậm chí cao hơn một chút 32.7±1.3 cho tập kiểm tra. Chúng tôi thấy rằng chỉ số ổn định trường hợp xấu nhất min-ACC rất tương tự trên cả hai tập, và việc quên thảm khốc trong WF100 cao hơn đáng kể cho dữ liệu huấn luyện. Điều này gợi ý nó có thể phục vụ như một ước tính tốt (quá) để giám sát tính ổn định của người học.

Song song. Trong đánh giá liên tục thế giới thực, song song có thể được khai thác bằng cách tách người học và đánh giá viên trong các đơn vị xử lý khác nhau. Đánh giá viên nắm bắt một ảnh chụp từ mô hình

--- TRANG 9 ---
Được công bố như một bài báo hội nghị tại ICLR 2023

Bảng 4: Các chỉ số đánh giá liên tục cho các kích thước mẫu khác nhau của các tập đánh giá và so sánh với hiệu suất tập huấn luyện trên 3 benchmark với ρeval = 1. 'All' chỉ ra các tập kiểm tra nhiệm vụ đầy đủ. Kết quả trên 5 seed được báo cáo là mean ± SD. 10³ in đậm chỉ ra cài đặt chúng tôi áp dụng cho các thí nghiệm khác trong công trình này.

[Bảng hiển thị các chỉ số cho các kích thước mẫu khác nhau]

để đánh giá, trong khi người học tiếp tục quá trình tối ưu hóa liên tục. Trong kịch bản này, tần suất ρeval phụ thuộc vào thời gian xử lý của đánh giá viên.

C CHI TIẾT TÁI TẠO

Tập dữ liệu và biến đổi. Cho học tập tăng dần theo lớp, Split-MNIST, Split-CIFAR10, và Split-MiniImagenet chia dữ liệu của chúng thành 5, 5, và 20 nhiệm vụ dựa trên 10, 10, và 100 lớp tương ứng. Mỗi tập hợp dữ liệu liên quan đến một tập hợp lớp sau đó tạo thành một nhiệm vụ. Sự chia này được thực hiện cho cả tập huấn luyện và kiểm tra, để tạo ra một luồng huấn luyện và đánh giá của các nhiệm vụ tuần tự. Thứ tự của các nhóm lớp để tạo nhiệm vụ là tuần tự, ví dụ trong Split-MNIST chúng tôi sử dụng các chữ số {0,1} cho T1, {2,3} cho T2, lên đến {8,9} cho T5. MNIST (LeCun & Cortes, 2010) bao gồm các chữ số viết tay màu xám với đầu vào (28×28). Tập dữ liệu chứa khoảng 70k hình ảnh từ đó 60k huấn luyện và 10k hình ảnh kiểm tra. CIFAR10 (Krizhevsky et al., 2009) chứa 50k hình ảnh huấn luyện và 10k hình ảnh kiểm tra từ một loạt phương tiện và động vật với đầu vào màu (32×32). Mini-Imagenet (Vinyals et al., 2016) là một tập con 100 lớp của Imagenet (Russakovsky et al., 2015), với đầu vào màu được thay đổi kích thước thành (84×84). Mỗi lớp chứa 600 hình ảnh, từ đó 500 được sử dụng cho huấn luyện và 100 cho kiểm tra.

Cho học tập tăng dần theo miền chúng tôi xem xét những thay đổi miền mạnh mẽ trong Mini-DomainNet (Zhou et al., 2021), một tập con thu nhỏ của 126 lớp của DomainNet (Peng et al., 2019) với hơn 90k hình ảnh, xem xét các miền: clipart, painting, real, sketch. Đầu vào từ tất cả các tập dữ liệu được chuẩn hóa và Mini-DomainNet thay đổi kích thước hình ảnh DomainNet gốc với kích thước nhỏ hơn của chúng thành 96 sử dụng nội suy song tuyến tính, theo sau bởi một cắt trung tâm thành đầu vào cố định (96×96×3). Cho tất cả các tập dữ liệu, tất cả các biến đổi được sử dụng đều xác định.

Quy trình gridsearch. Tất cả các thí nghiệm cho Split-MNIST và Split-CIFAR10 thực hiện gridsearch trên các siêu tham số, với mỗi lần chạy được tính trung bình trên 5 seed khởi tạo. Kết quả với ACC cao nhất được chọn làm mục tốt nhất, theo (Lopez-Paz & Ranzato, 2017). Cho tính khả thi tính toán trong các benchmark lớn hơn, Split-MiniImagenet và Mini-DomainNet, gridsearch được thực hiện trên một seed duy nhất và mục hàng đầu được tính trung bình trên 5 seed khởi tạo. Cho tất cả các thí nghiệm, tốc độ học cho các cập nhật dựa trên gradient được xem xét như siêu tham số trong tập hợp {0.1, 0.01, 0.001, 0.0001}. Kích thước batch cố định được sử dụng cho tất cả các benchmark, với 128 cho Split-MiniImagenet và Mini-DomainNet quy mô lớn hơn, và 256 cho Split-MNIST và Split-CIFAR10 nhỏ hơn. Các siêu tham số cụ thể theo phương pháp và học tập liên tục khác được thảo luận dưới đây.

Thiết lập đánh giá liên tục. Chúng tôi sử dụng đánh giá liên tục với ρeval = 1 và kích thước tập con 1k mỗi nhiệm vụ đánh giá, dựa trên phân tích tính khả thi của chúng tôi trong Phụ lục B. Để tham khảo với tài liệu, các chỉ số dựa trên chuyển đổi nhiệm vụ ACC và FORG theo đánh giá tiêu chuẩn với toàn bộ tập kiểm tra.

Kiến trúc và tối ưu hóa. Split-MNIST sử dụng MLP với 2 lớp ẩn gồm 400 đơn vị. Split-CIFAR10, Split-MiniImagenet và Mini-DomainNet sử dụng phiên bản mỏng của Resnet18 (Lopez-Paz & Ranzato, 2017; De Lange & Tuytelaars, 2021). Tối ưu hóa SGD được sử dụng với momentum 0.9. Split-MNIST, Split-CIFAR10 và Split-MiniImagenet được cấu hình cho 10 epoch mỗi nhiệm vụ, tương đương khoảng 500, 400, và 200 lần lặp mỗi nhiệm vụ với kích thước batch đã định nghĩa. Mini-DomainNet có 300 lần lặp huấn luyện mỗi nhiệm vụ để cân bằng tính toán đều qua các nhiệm vụ.

Kết quả cho việc xác định khoảng cách ổn định trong Phần 4 (giới thiệu) được thu được sử dụng Experience Replay (ER) trong đó chúng tôi xem xét siêu tham số cân đối mất mát α ∈ {0.1, 0.3, 0.5, 0.7, 0.9} như trong Eq. 8. Kích thước batch đã định nghĩa ở trên chỉ ra số lượng mẫu mới trong mini-batch. Ngoài ra, một batch khác cùng kích thước được lấy mẫu từ bộ nhớ phát lại, được nối với dữ liệu mới, theo (Chaudhry et al., 2019b; Verwimp et al., 2021). Lấy mẫu reservoir dựa trên lớp được sử dụng như trong (De Lange & Tuytelaars, 2021) để xác định mẫu nào được chọn để lưu trữ trong bộ đệm M với tổng dung lượng cố định |M| mẫu. Chúng tôi chỉ ra các siêu tham số đã chọn (lr, α, |M|) mỗi tập dữ liệu ở đây: Split-MNIST (0.01, 0.3, 2×10³), Split-CIFAR10 (0.1, 0.7, 10³), Split-MiniImagenet (0.1, 0.5, 10⁴), Mini-DomainNet (0.1, 0.3, 10³).

Kết quả trong Phần 5.1 sử dụng kết quả ER thu được từ Phần 4 (giới thiệu). GEM (Lopez-Paz & Ranzato, 2017) sử dụng tham số bias tiêu chuẩn của tác giả là 0.5, với tốc độ học tốt nhất lr = 0.01 và lr = 0.001 cho Split-MNIST và Split-CIFAR10 tương ứng. Thiết lập task-incremental Split-MNIST dựa trên (van de Ven et al., 2022) với tối ưu hóa Adam, kích thước batch 128 và lr = 0.001, nhưng với 200 lần lặp mỗi nhiệm vụ. Vì nó là task-incremental, mỗi nhiệm vụ được phân bổ một đầu phân loại riêng biệt và tại suy luận, bộ phân loại tương ứng được sử dụng cho một mẫu đã cho. Các chi tiết khác vẫn giống như thiết lập class-incremental Split-MNIST của chúng tôi. Độ mạnh regularization tốt nhất được chọn dựa trên ACC cao nhất, cho SI ∈ {0.1, 1, 10, 100, 1000}, cho EWC ∈ {10⁴, 10⁵, 10⁶, 10⁷}, cho LwF ∈ {1, 2, 5} và nhiệt độ distillation {0.5, 1, 2}.

Kết quả trong Phụ lục B dựa trên kết quả ACC tốt nhất cho ER được báo cáo trong Phần 4, xem ở trên để biết chi tiết. Trong thí nghiệm cho các kích thước mẫu con khác nhau, các tập con đánh giá và huấn luyện được thu được từ toàn bộ tập thông qua lấy mẫu đồng đều mỗi đánh giá.

Codebase và thiết bị đã sử dụng. Các thí nghiệm dựa trên khung Avalanche (Lomonaco et al., 2021) trong Pytorch (Paszke et al., 2019). Tất cả kết quả được thực hiện trên một cụm tính toán với một loạt GPU NVIDIA.

Định nghĩa của maximal delta increase Δw,+t,Ei. Chúng tôi định nghĩa trong bài báo chính maximal delta decrease Δw,−t,Ei, và do hạn chế không gian báo cáo định nghĩa tương tự của maximal delta decrease Δw,+t,Ei ở đây trong Phụ lục là:

Δw,+t,Ei = max_{m>n} (A(Ei, fm) - A(Ei, fn)); ∀m,n ∈ [t-w+1, t]     (10)

Lưu ý rằng phương trình giống hệt với Δw,−t,Ei (Eq. 3), nhưng thay thế ràng buộc m < n bằng m > n.

D KẾT QUẢ BỔ SUNG

D.1 KHOẢNG CÁCH ỔN ĐỊNH TRONG HỌC TẬP LIÊN TỤC TRỰC TUYẾN VỚI ER

Kết quả chính trong bài báo dựa trên 10 epoch mỗi nhiệm vụ cho các benchmark tăng dần theo lớp. Trong học tập liên tục trực tuyến, hoặc học tập liên tục streaming, mỗi mẫu trong luồng chỉ được quan sát một lần (Shim et al., 2021; Fini et al., 2020; Chrysakis & Moens, 2020; De Lange & Tuytelaars, 2021; Caccia et al., 2022). Chúng tôi cho thấy cho Split-MNIST và Split-CIFAR10 trong Hình 6 rằng khoảng cách ổn định tồn tại cho ER trong thiết lập học tập liên tục trực tuyến.

Ngoài ra, Bảng 5 cho thấy các chỉ số được đề xuất của chúng tôi thành công xác định khoảng cách ổn định trong các luồng học tập liên tục trực tuyến. Ví dụ trong Split-MNIST, WF10 chỉ ra những giảm sụt hiệu suất khủng khiếp gần 40% trong khi FORG dựa trên nhiệm vụ tiêu chuẩn chỉ chỉ ra 5% quên. Hơn nữa, trong Split-CIFAR10 một ACC được duy trì ở 41.8%, trong khi min-ACC cho thấy tất cả các nhiệm vụ giảm xuống zero ít nhất một lần trong luồng.

Hình 6: Học Tập Liên Tục Trực tuyến chỉ lấy mẫu mỗi thể hiện từ luồng một lần, giống như một thiết lập streaming. Báo cáo các đường cong độ chính xác ER cho bốn nhiệm vụ đầu tiên trên Split-MNIST (a-d) và Split-CIFAR10 (e-h), là mean ± SD trên 5 seed, với các đường ngang đại diện cho min-ACC trung bình. Các đường thẳng đứng chỉ ra sự bắt đầu của một nhiệm vụ mới.

[Tám biểu đồ con hiển thị độ chính xác cho các nhiệm vụ khác nhau]

Bảng 5: Kết quả Học Tập Liên Tục Trực tuyến cho ER. Kết quả được báo cáo là mean ± SD trên 5 seed.

[Bảng hiển thị các chỉ số cho Split-MNIST và Split-CIFAR10]

D.2 KHOẢNG CÁCH ỔN ĐỊNH CHO CÁC KÍCH THƯỚC BỘ NHỚ ER KHÁC NHAU

Trong bài báo chính, ER được xem xét cho một kích thước bộ nhớ cố định duy nhất M mẫu. Hình 7 cho thấy kết quả class-incremental Split-MNIST cho một loạt kích thước bộ nhớ M ∈ {500, 1000, 2000, |S|} trong đó |S| lưu trữ tất cả các mẫu đã thấy trong luồng. Thiết lập benchmark và siêu tham số ER giống hệt với Phần 4. Ba quan sát chính có thể được đưa ra. Đầu tiên, khoảng cách ổn định tồn tại cho tất cả kích thước bộ nhớ, thậm chí khi lưu trữ tất cả mẫu với M = |S| xấp xỉ học tập chung trên các nhiệm vụ quan sát được. Thứ hai, với việc giảm kích thước bộ nhớ M, giai đoạn phục hồi xấu đi sau việc giảm sụt hiệu suất khủng khiếp trong khoảng cách ổn định. Điều này được mong đợi vì kích thước bộ nhớ lớn hơn cho phép xấp xỉ tốt hơn phân phối chung. Thứ ba, mặc dù lưu trữ tất cả mẫu dẫn đến ACC tổng thể cao hơn, Hình 7 chỉ ra một giảm sụt hiệu suất nhỏ hơn trong khoảng cách ổn định. Điều này đặc biệt đáng chú ý cho T2 đến T4 ngay sau khi học nhiệm vụ khi tất cả kích thước bộ nhớ có hiệu suất tương tự. Overfit trên kích thước bộ nhớ lớn hơn thách thức hơn và có thể mất nhiều lần lặp hơn mỗi nhiệm vụ. Do đó, với tham chiếu đến phân tích khái niệm của chúng tôi, các chuẩn gradient trên các quá trình chuyển đổi nhiệm vụ có thể cao hơn cho M lớn hơn và do đó giảm việc giảm trong khoảng cách ổn định.

Hình 7: Các đường cong độ chính xác ER trên Split-MNIST cho bốn nhiệm vụ đầu tiên, cho kích thước bộ nhớ M ∈ {500, 1000, 2000, |S|} trong đó |S| lưu trữ tất cả các mẫu đã thấy trong luồng. Kết quả được báo cáo là mean ± SD trên 5 seed, với các đường ngang đại diện cho min-ACC trung bình. Các đường thẳng đứng chỉ ra sự bắt đầu của một nhiệm vụ mới.

[Bốn biểu đồ con hiển thị độ chính xác cho các kích thước bộ nhớ khác nhau]

D.3 PHÂN TÍCH HIỆU SUẤT TRƯỜNG HỢP XẤU NHẤT CỦA FINETUNING

Chúng tôi kiểm tra phương pháp finetuning huấn luyện một cách ngây thơ trên dữ liệu nhiệm vụ mới với stochastic gradient descent (SGD). Finetuning đã được chỉ ra lặp đi lặp lại là dễ bị quên thảm khốc (De Lange et al., 2021; van de Ven et al., 2022). Chúng tôi chỉ ra với đánh giá liên tục và các chỉ số được đề xuất của chúng tôi rằng việc quên lớn xảy ra trong một cửa sổ chỉ vài lần lặp. Chúng tôi sử dụng cùng thiết lập thí nghiệm với momentum 0.9 như được mô tả cho bốn benchmark trong Phần 4, nhưng với SGD thuần thay vì ER.

Bảng 6 chỉ ra việc quên thảm khốc và nhanh chóng trong một cửa sổ 10 cập nhật với các giá trị lớn của WF10. Ngoài ra, hiệu suất trường hợp xấu nhất min-ACC cho thấy cho các benchmark học tập tăng dần theo lớp rằng độ chính xác giảm xuống zero cho các nhiệm vụ đã học trước đó, và trung bình xuống 4.9% cho domain-incremental Mini-DomainNet.

Bảng 6: Kết quả Finetuning với đánh giá liên tục cho bốn benchmark, được báo cáo là mean ± SD trên 5 seed.

[Bảng hiển thị các chỉ số cho các benchmark khác nhau]

D.4 KHOẢNG CÁCH ỔN ĐỊNH VỚI PHÂN LOẠI CÁC TỪ NÓI

Hình 8: Các đường cong độ chính xác theo từng nhiệm vụ (cho nhiệm vụ 1 đến 4) cho ER trong khi học năm nhiệm vụ đầu tiên của thí nghiệm nhận dạng giọng nói, được thực hiện theo cách tăng dần theo lớp. Được báo cáo là mean ± SD trên 5 seed. Các quá trình chuyển đổi nhiệm vụ được chỉ ra bằng các đường thẳng đứng màu xám đứt nét.

Để kiểm tra liệu khoảng cách ổn định cũng có thể được quan sát trong các miền khác ngoài phân loại hình ảnh, chúng tôi thực hiện một thí nghiệm học tập liên tục dựa trên tập dữ liệu Synthetic Speech Commands (Buchner, 2017), chứa hơn 30.000 clip âm thanh của 30 từ nói khác nhau. Để biến tập dữ liệu nhận dạng giọng nói này thành một thí nghiệm học tập liên tục, chúng tôi theo sát Cossu et al. (2021). Tập dữ liệu được chia thành các nhiệm vụ khác nhau sao cho mỗi nhiệm vụ chứa tất cả các mẫu huấn luyện của hai từ, và những nhiệm vụ này được trình bày cho thuật toán lần lượt. Chúng tôi sử dụng cùng chuỗi nhiệm vụ như Cossu et al. (2021). Thí nghiệm được thực hiện theo kịch bản học tập tăng dần theo lớp (tức là, định danh nhiệm vụ không được cung cấp tại thời điểm kiểm tra), vì vậy thuật toán phải học phân biệt giữa tất cả các từ gặp phải cho đến nay.

Theo Cossu et al. (2021), chúng tôi sử dụng MLP với một lớp ẩn duy nhất chứa 1024 đơn vị với kích hoạt ReLU, theo sau bởi một lớp đầu ra softmax. Huấn luyện được thực hiện sử dụng tối ưu hóa Adam với tốc độ học 0.0001. Mỗi nhiệm vụ được huấn luyện cho 200 lần lặp với kích thước mini-batch 256. Chúng tôi sử dụng cùng các bước tiền xử lý như Cossu et al. (2021): cho mỗi clip âm thanh 40 hệ số Mel được trích xuất sử dụng cửa sổ trượt 25 ms với bước 10 ms, dẫn đến chuỗi có độ dài cố định với 101 bước thời gian. Những chuỗi này được tổng hợp theo thời gian, sao cho mỗi mẫu được đưa vào mạng bao gồm 40×101 = 4040 đặc trưng đầu vào.

Trên thí nghiệm học tập tăng dần theo lớp này chúng tôi kiểm tra phương pháp ER, sử dụng ngân sách bộ nhớ 100 ví dụ mỗi lớp. Hình 8 hiển thị các đường cong độ chính xác theo từng nhiệm vụ cho năm nhiệm vụ đầu tiên, hiển thị khoảng cách ổn định rõ ràng sau mỗi quá trình chuyển đổi nhiệm vụ, do đó xác nhận rằng hiện tượng khoảng cách ổn định không cụ thể cho miền phân loại hình ảnh.

D.5 TƯƠNG QUAN VIỆC QUÊN THẢM KHỐC VÀ CÁC CHỈ SỐ KHOẢNG CÁCH ỔN ĐỊNH

Hình 9: Phân tích tương quan giữa việc quên dựa trên nhiệm vụ (thước đo quên thảm khốc) và windowed-forgetting (thước đo khoảng cách ổn định). Sau mỗi nhiệm vụ mới được học, FORG và WF10 được bao gồm cho tất cả các nhiệm vụ đánh giá. Hệ số tương quan Pearson cho thấy tương quan mạnh cho benchmark dễ hơn Split-MNIST, nhưng gần với zero cho các benchmark thách thức hơn. Tương quan được xác định trên các trung bình trên 5 seed (mục in đậm), các mục nhạt minh họa kết quả cho mỗi seed.

Hai trong số các chỉ số được đề xuất của chúng tôi trực tiếp đo lường các thuộc tính của khoảng cách ổn định. WF10 đo lường mức độ dốc của việc giảm tương đối ban đầu sau khi học một nhiệm vụ và min-ACC đo lường hiệu suất tối thiểu tuyệt đối. Với một phân tích tương quan chúng tôi hy vọng thu được thêm thông tin sâu sắc w.r.t. thước đo tiêu chuẩn cho việc quên thảm khốc (FORG).

Trên mỗi quá trình chuyển đổi nhiệm vụ trong luồng học, chúng tôi so sánh FORG với WF10 cho mỗi nhiệm vụ đánh giá riêng biệt. Cách tiếp cận này có thể so sánh cho mỗi nhiệm vụ đánh giá mức độ dốc của khoảng cách ổn định, và hiệu ứng là gì cho việc quên thảm khốc. Thí nghiệm xem xét dữ liệu đánh giá cho ER từ phân tích trong Hình 2 trong bài báo chính. Chúng tôi cũng xem xét min-ACC như thước đo cho khoảng cách ổn định, nhưng vì nó thường giảm xuống độ chính xác zero sau nhiệm vụ đầu tiên, nó không cung cấp thêm thông tin sâu sắc.

Hình 9 chỉ ra kết quả cho nghiên cứu tương quan của chúng tôi trên Split-MNIST, Split-CIFAR10, và Split-MiniImagenet. Split-MNIST dễ hơn cho thấy hệ số tương quan Pearson cao ρ = 0.86. Tuy nhiên, mối quan hệ tuyến tính ít nổi bật hơn cho Split-CIFAR10 và Split-MiniImagenet thách thức hơn với ρ = 0.37 và ρ = 0.61. Đối với hai cái sau, chúng tôi cụ thể quan sát sự hình thành của các cột. Vì WF10 đo lường một giảm sụt tối đa cho nhiệm vụ đánh giá do khoảng cách ổn định, độ chính xác vẫn có thể phục hồi với ER khi hội tụ cho nhiệm vụ mới. Tuy nhiên, theo thời gian khi học thêm nhiều nhiệm vụ trong luồng, độ chính xác tại việc phục hồi của nhiệm vụ đánh giá giảm. Điều này dẫn đến các mục với WF10 không đổi, trong khi FORG giảm. Mối quan hệ giữa hai chỉ số này cũng có thể được quan sát định tính trong các đường cong độ chính xác của Hình 2 trong bài báo chính.

--- TRANG 10 ---
Được công bố như một bài báo hội nghị tại ICLR 2023

D.6 KẾT QUẢ GEM BỔ SUNG CHO SPLIT-MNIST VÀ SPLIT-CIFAR10

Bài báo chính cho thấy kết quả cho các đường cong độ chính xác Split-MNIST và zoom của các chuẩn gradient ổn định trên các quá trình chuyển đổi nhiệm vụ. Trong Hình 10 chúng tôi cung cấp kết quả chuẩn ∇L_stability đầy đủ trong quá trình học. Đối với phân tích khoảng cách ổn định, zoom trên các quá trình chuyển đổi nhiệm vụ thông tin nhất, trong khi kết quả đầy đủ đưa ra cái nhìn tổng quan về độ lớn gradient hướng tới sự hội tụ của các nhiệm vụ.

GEM và ER được báo cáo cho ACC tốt nhất, cả hai với tốc độ học 0.01 (xem chi tiết trong Phụ lục ở trên). So với GEM, ER thể hiện các chuẩn gradient ổn định lớn hơn trên gần như toàn bộ quỹ đạo học. Điều này chỉ ra rằng hướng tới sự hội tụ của nhiệm vụ, các gradient mini-batch chủ yếu thỏa mãn các ràng buộc GEM với các góc gradient ≥ 90°, do đó dẫn đến các vector chiếu với độ lớn nhỏ. Mặc dù sử dụng các gradient của các mẫu phát lại trực tiếp cho ER hội tụ đến các chuẩn gradient thấp, có một sự khác biệt đáng chú ý cho các giá trị gần zero cho GEM trong đó những gradient này được sử dụng cho các ràng buộc dựa trên góc.

Bổ sung cho kết quả trên Split-MNIST, Hình 11 cung cấp các đường cong độ chính xác cho bốn nhiệm vụ đầu tiên trên Split-CIFAR10. Những kết quả này tương ứng với tài liệu (De Lange & Tuytelaars, 2021; Aljundi et al., 2019b;a), trong đó GEM không hiệu quả cho benchmark class-incremental Split-CIFAR10 khó khăn hơn. Đáng chú ý là việc giảm sụt sắc nét xuống độ chính xác gần zero trên các quá trình chuyển đổi nhiệm vụ (khoảng cách ổn định), sau đó đạt đỉnh và sau đó giảm nhanh chóng.

Hình 10: Các chuẩn L2 theo từng lần lặp của GEM và ER của ∇L_stability hiện tại trên Split-MNIST. Kết quả được báo cáo là mean ± SD trên 5 seed. Các đường thẳng đứng chỉ ra sự bắt đầu của một nhiệm vụ mới.

Hình 11: Các đường cong độ chính xác GEM và ER cho bốn nhiệm vụ đầu tiên của Split-CIFAR10, được báo cáo là mean ± SD trên 5 seed. Min-ACC được tính trung bình trên các seed là zero cho cả hai phương pháp.

D.7 KẾT QUẢ LWF BỔ SUNG CHO MINI-DOMAIN NET

Bên cạnh phân tích thực nghiệm trên Split-MNIST và Rotated-MNIST, chúng tôi chứng minh khoảng cách ổn định cho Learning without Forgetting (LwF) (Li & Hoiem, 2017) trong domain-incremental Mini-DomainNet (Hình 12). Các khoảng cách ổn định đặc biệt đáng chú ý cho T1 và T2 trong Hình 12(a,b). Hình 12(e) báo cáo các chuẩn gradient ổn định, với trọng tâm vào các quá trình chuyển đổi nhiệm vụ trong Hình 12(f,g). Kết quả xác nhận các chuẩn gradient zero trên các quá trình chuyển đổi nhiệm vụ, theo sau bởi phục hồi với các chuẩn gradient tăng lên.

Hình 12: Các đường cong độ chính xác LwF và ER (a-d) trên Mini-DomainNet cho bốn miền, và các chuẩn L2 theo từng lần lặp của ∇L_stability hiện tại (e-g). Kết quả được báo cáo là mean ± SD trên 5 seed, với các đường ngang đại diện cho min-ACC trung bình. Các đường thẳng đứng chỉ ra sự bắt đầu của một nhiệm vụ mới. Lưu ý rằng tỷ lệ trục x thay đổi qua (a-d), (e) tổng quan toàn bộ quỹ đạo học, và (f,g) là zoom vào vài lần lặp đầu tiên cho hai nhiệm vụ cuối.
