README:
Như một bổ sung cho dự án, một codebase phát hiện đối tượng, YOLO. Air gần đây đã được mở. Nó tích hợp các cơ chế attention khác nhau trong thuật toán phát hiện đối tượng. Mã đơn giản và dễ đọc. Chào mừng bạn chơi và star!

Đối với người mới bắt đầu (như tôi): Gần đây, tôi thấy một vấn đề khi đọc báo. Đôi khi ý tưởng cốt lõi của bài báo rất đơn giản, và mã cốt lõi có thể chỉ là một chục dòng. Tuy nhiên, khi tôi mở mã nguồn được tác giả phát hành, tôi thấy rằng module được đề xuất được nhúng trong khung tác vụ như phân loại, phát hiện và phân đoạn, dẫn đến mã dư thừa. Đối với tôi, người không quen thuộc với khung tác vụ cụ thể, việc tìm mã cốt lõi rất khó khăn, dẫn đến một số khó khăn trong việc hiểu bài báo và ý tưởng mạng.

Đối với nâng cao (như bạn): Nếu các đơn vị cơ bản conv, FC và RNN được coi như các khối Lego nhỏ, và các cấu trúc transformer và RESNET được coi như các lâu đài LEGO đã được xây dựng, các module được cung cấp bởi dự án này là các thành phần LEGO với thông tin ngữ nghĩa hoàn chỉnh. Để tránh xây dựng bánh xe lặp đi lặp lại, các nhà nghiên cứu khoa học chỉ nên suy nghĩ về cách sử dụng những "thành phần LEGO" này để xây dựng các tác phẩm đầy màu sắc hơn.

Đối với thành thạo (có thể như bạn): Khả năng hạn chế, không thích phun nhẹ!!!

Đối với Tất cả: Dự án này nhằm mục đích thực hiện một cơ sở mã có thể làm cho người mới bắt đầu học sâu hiểu và phục vụ cộng đồng nghiên cứu khoa học và công nghiệp. Như tài khoản WeChat chính thức fightingcv. Mục đích của dự án này là đạt được Hãy để không có bài báo khó đọc nào trên thế giới. (đồng thời, chúng tôi cũng hoan nghênh tất cả các nhà nghiên cứu khoa học sắp xếp mã cốt lõi của công việc của họ vào dự án này, thúc đẩy sự phát triển của cộng đồng nghiên cứu khoa học, và chỉ ra tác giả của mã trong README)

...

Oracle Segment:
..
from model.attention.ViP import WeightedPermuteMLP
import torch
from torch import nn
from torch.nn import functional as F
...

Hướng dẫn:
Tôi đang dự định sử dụng mô hình fighting-cv để hoàn thành các lớp attention cho ViP Attention Usage. Bạn có thể cung cấp cho tôi một số hướng dẫn về việc hoàn thành nhiệm vụ này không?

Hướng dẫn:
package_1: ViP
sub_package: WeightedPermuteMLP
package_2: torch

Đầu ra Sự thật Chuẩn:
from model.attention.ViP import WeightedPermuteMLP
import torch
from torch import nn
from torch.nn import functional as F
input=torch.randn(64,8,8,512)
seg_dim=8
vip=WeightedPermuteMLP(512,seg_dim)
out=vip(input)
print(out.shape)

Bảng 18: Ví dụ về đầu vào-đầu ra cho External-Attention-pytorch GitHub về tác vụ lớp attention trên miền Attention Usage. URL README là https://github.com/xmu-xiaoma666/External-Attention-pytorch/blob/master/README_EN.md . URL GitHub là https://github.com/xmu-xiaoma666/External-Attention-pytorch .

[Tiếp theo sẽ là các bảng tương tự cho 17 repository còn lại, mỗi bảng chứa README, Oracle Segment, Hướng dẫn, Yêu cầu Đối số, và Đầu ra Sự thật Chuẩn cho các tác vụ cụ thể của từng repository]

C.4 BM25

Truy xuất: Trong cài đặt này, chúng tôi áp dụng bộ truy xuất BM25 để lấy đoạn liên quan đến hướng dẫn trong các tệp README. Chúng tôi đặt khoảng truy xuất của bộ truy xuất BM25 ở 10 câu bằng cách sử dụng tokenizer câu NLTK vì độ dài trung bình của một Oracle là 9,5.

C.5 Kết quả Trên Các Repository GitHub Khác nhau

Bảng 36: Bảng này hiển thị điểm Pass@5 của GPT-4 và Claude trên bộ 1⁄4 (*) và GPT 3.5 và Claude trên bộ đầy đủ trên các repository GitHub khác nhau. Oracle, Code và BM25 đại diện cho các cài đặt Oracle, Code và Retrieval.

D Đóng góp

Nhóm được dẫn dắt bởi Xiangru Tang, Yuliang Liu, và Zefan Cai, những người không chỉ chạy các thí nghiệm và thiết kế các mô hình mà còn đóng vai trò trong việc chuẩn bị bản thảo. Yanjun cũng đóng góp rất nhiều vào việc viết bài báo. Yanjun Shao, Junjie Lu, và Yichi Zhang hỗ trợ thực hiện công việc thí nghiệm. Việc xây dựng bộ dữ liệu, bao gồm các tác vụ như chú thích dữ liệu và làm sạch, được thực hiện bởi Zexuan Deng, Helan Hu, Kaikai An, Ruijun Huang, Shuzheng Si, Sheng Chen, Haozhe Zhao, và Liang Chen. Các cố vấn, ví dụ như Yan Wang, Tianyu Liu, Zhiwei Jiang, Baobao Chang, Yin Fang, Yujia Qin, Wangchunshu Zhou, Yilun Zhao, Arman Cohan, và Mark Gerstein đóng góp đáng kể thông qua các cuộc thảo luận.
