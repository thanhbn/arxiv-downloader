# ClassEval : Một Benchmark Được Tạo Thủ Công
để Đánh Giá LLMs trên Việc Sinh Mã Cấp Độ Lớp
Xueying Du Mingwei Liu Kaixin Wang Hanlin Wang Junwei Liu
Yixuan Chen Jiayi Feng Chaofeng Sha Xin Peng Yiling Lou
Đại học Fudan
Thượng Hải, Trung Quốc
{xueyingdu21,kxwang23,wanghanlin23}@m.fudan.edu.cn
{jwliu22,23212010005,23210240148}@m.fudan.edu.cn
{liumingwei,cfsha,pengxin,yilinglou}@fudan.edu.cn
TÓM TẮT
Gần đây, nhiều mô hình ngôn ngữ lớn (LLMs) đã được đề xuất,
thể hiện khả năng tiên tiến trong việc sinh mã. Đồng thời,
nhiều nỗ lực đã được dành để đánh giá LLMs trên các benchmark sinh mã
như HumanEval. Mặc dù rất hữu ích
để so sánh các LLMs khác nhau, việc đánh giá hiện tại tập trung vào một kịch bản sinh mã đơn giản (tức là sinh mã cấp độ hàm hoặc cấp độ câu lệnh), chủ yếu yêu cầu LLMs sinh ra một đơn vị mã duy nhất (ví dụ, một hàm hoặc một câu lệnh) cho mô tả ngôn ngữ tự nhiên đã cho. Việc đánh giá như vậy tập trung vào việc sinh các đơn vị mã độc lập và thường có quy mô nhỏ, do đó để lại câu hỏi không rõ ràng về việc LLMs hoạt động như thế nào trong việc sinh mã phức tạp hơn.

Để lấp đầy khoảng trống kiến thức này, chúng tôi thực hiện nỗ lực đầu tiên để đánh giá
LLMs trong một kịch bản sinh mã thách thức hơn, tức là sinh mã cấp độ lớp. Chúng tôi đầu tiên xây dựng thủ công benchmark sinh mã cấp độ lớp đầu tiên ClassEval gồm 100 tác vụ sinh mã Python cấp độ lớp với khoảng 500 giờ công người. Dựa trên benchmark mới ClassEval, sau đó chúng tôi thực hiện nghiên cứu đầu tiên về 11 LLMs tiên tiến nhất trên việc sinh mã cấp độ lớp. Dựa trên kết quả của chúng tôi, chúng tôi có những phát hiện chính sau đây. Đầu tiên, chúng tôi thấy rằng tất cả LLMs hiện tại đều thể hiện hiệu suất tệ hơn nhiều trên việc sinh mã cấp độ lớp so với trên các benchmark sinh mã cấp độ phương thức độc lập như HumanEval; và khả năng mã hóa cấp độ phương thức không thể phản ánh tương đương khả năng mã hóa cấp độ lớp giữa các LLMs. Thứ hai, chúng tôi thấy rằng GPT-4 và GPT-3.5 vẫn thể hiện sự vượt trội áp đảo so với các LLMs khác trên việc sinh mã cấp độ lớp, và các mô hình tầng thứ hai bao gồm Instruct-StarCoder, Instruct-CodeGen, và WizardCoder với hiệu suất rất tương tự. Thứ ba, chúng tôi thấy rằng việc sinh toàn bộ lớp cùng một lúc (tức là chiến lược sinh toàn diện) là chiến lược sinh tốt nhất chỉ đối với GPT-4 và GPT-3.5, trong khi việc sinh từng phương thức một (tức là sinh tăng dần và tổng hợp) là các chiến lược tốt hơn cho các mô hình khác với khả năng hạn chế trong việc hiểu các hướng dẫn dài và sử dụng thông tin ở giữa. Cuối cùng, chúng tôi thấy khả năng hạn chế của mô hình trong việc sinh mã phụ thuộc phương thức và thảo luận về các loại lỗi thường gặp trong các lớp được sinh ra. Benchmark của chúng tôi có sẵn tại
https://github.com/FudanSELab/ClassEval

TỪ KHÓA
Sinh Mã Cấp Độ Lớp, Mô Hình Ngôn Ngữ Lớn, Benchmark

1 GIỚI THIỆU
Các kỹ thuật sinh mã tự động sinh các đoạn mã cho mô tả ngôn ngữ tự nhiên đã cho, có thể được tận dụng để cải thiện năng suất phát triển và đã được nghiên cứu rộng rãi trong tài liệu [1–3]. Sự tiến bộ gần đây trong các mô hình ngôn ngữ lớn (LLMs) đã mang lại những tiến bộ đáng kể trong lĩnh vực sinh mã. Đến nay, các nhà nghiên cứu đã đề xuất nhiều LLMs khác nhau [4–14] (như GPT-4 [4], WizardCoder [8], và Instruct-CodeGen [10]) bằng cách huấn luyện các mô hình lớn với hơn hàng tỷ tham số trên các kho ngữ liệu chung hoặc đặc thù mã và hướng dẫn.

Để hiểu đầy đủ khả năng sinh mã của các LLMs mới nổi, nhiều nỗ lực đã được dành để đánh giá LLMs trên các benchmark sinh mã được xây dựng tự động hoặc thủ công. Đến nay, nhiều benchmark sinh mã đã được đề xuất, như HumanEval [15] và MBPP [16]. Mặc dù rất hữu ích cho mọi người để hiểu và so sánh hiệu suất của các LLMs khác nhau, việc đánh giá hiện tại thực sự tập trung vào một kịch bản sinh mã khá đơn giản, tức là sinh mã cấp độ hàm hoặc cấp độ câu lệnh. Chúng chủ yếu yêu cầu LLMs sinh ra một đơn vị mã duy nhất (ví dụ, một hàm hoặc một câu lệnh) cho các mô tả ngôn ngữ tự nhiên đã cho theo cách độc lập, điều này vốn có hai hạn chế trong việc đánh giá LLMs trong sinh mã. Đầu tiên, việc đánh giá như vậy có xu hướng tập trung vào việc sinh mã có độ dài ngắn, ví dụ, mỗi tác vụ trong benchmark được sử dụng rộng rãi nhất HumanEval chỉ bao gồm việc sinh mã trung bình 11.5 dòng và 24.4 token. Số lượng token được sinh như vậy nằm trong giới hạn tối đa số token trong các LLMs gần đây (ví dụ, 2,048 cho WizardCoder [8]). Do đó, vẫn không rõ về tiềm năng thêm của LLMs trong việc sinh các đoạn mã dài. Thứ hai, việc đánh giá như vậy chủ yếu tập trung vào việc sinh một đơn vị mã duy nhất, ví dụ, một hàm hoặc một câu lệnh. Tuy nhiên, như được thể hiện trong công trình trước [17], chỉ 30% phương thức là độc lập với các ngữ cảnh mã khác trong các dự án mã nguồn mở. Do đó, vẫn không rõ LLMs hoạt động như thế nào trong việc sinh một đơn vị mã ghép của nhiều phương thức¹ phụ thuộc lẫn nhau (ví dụ, gọi lẫn nhau hoặc truy cập cùng một biến).

Benchmark ClassEval. Để lấp đầy khoảng trống kiến thức này, công trình này thực hiện nỗ lực đầu tiên để đánh giá LLMs trong một kịch bản sinh mã thách thức hơn, tức là sinh mã cấp độ lớp. Cụ thể, chúng tôi đánh giá khả năng của mô hình trong việc sinh một lớp của nhiều phương thức phụ thuộc lẫn nhau cho mô tả ngôn ngữ tự nhiên đã cho. Vì không có benchmark hiện tại nào bao gồm các tác vụ sinh mã cấp độ lớp¹, chúng tôi xây dựng thủ công benchmark sinh mã cấp độ lớp đầu tiên ClassEval theo cách nghiêm ngặt và tốn thời gian, mất khoảng 500 giờ công người để xây dựng 100 tác vụ sinh mã Python cấp độ lớp. Nhìn chung, ClassEval bao gồm một loạt các chủ đề trong phát triển phần mềm thực tế (ví dụ, hệ thống quản lý và phát triển game). Mỗi tác vụ được xây dựng với một bộ test có độ đủ kiểm tra cao (ví dụ, 98.2% và 99.7% độ bao phủ cấp độ nhánh hoặc cấp độ câu lệnh) để tạo điều kiện kiểm tra tính đúng đắn đáng tin cậy của mã được sinh ra; hơn nữa, mỗi tác vụ được thiết kế để sinh một lớp của nhiều phương thức với các phụ thuộc đa dạng (ví dụ, phụ thuộc trường, phương thức và thư viện).

Nghiên cứu thực nghiệm. Dựa trên benchmark mới ClassEval, sau đó chúng tôi thực hiện nghiên cứu đầu tiên để đánh giá LLMs trên việc sinh mã cấp độ lớp. Cụ thể, các thí nghiệm của chúng tôi bao gồm 11 LLMs tiên tiến nhất, đa dạng về kích thước mô hình, mô hình nền tảng, nguồn gốc hoặc lĩnh vực. Đối với mỗi LLM được nghiên cứu, chúng tôi khám phá hiệu suất của nó trong việc sinh mã cấp độ lớp với ba chiến lược sinh khác nhau, tức là sinh toàn diện (sinh toàn bộ lớp cùng một lúc), sinh tăng dần và sinh tổng hợp (sinh lớp từng phương thức một). Đối với mỗi đoạn mã được sinh ra, chúng tôi đo tính đúng đắn của nó với thước đo được sử dụng rộng rãi Pass@k [18]. Ngoài ra, chúng tôi cũng điều tra khả năng của mô hình trong việc sinh mã phụ thuộc và phân tích các trường hợp xấu của các lớp không đúng.

Phát hiện và ý nghĩa chính. Dựa trên kết quả của chúng tôi, chúng tôi có những phát hiện chính sau đây. Đầu tiên, chúng tôi thấy rằng tất cả LLMs hiện tại đều thể hiện hiệu suất tệ hơn nhiều trên việc sinh mã cấp độ lớp so với trên các benchmark sinh mã cấp độ phương thức độc lập như HumanEval; và khả năng mã hóa cấp độ phương thức không thể phản ánh tương đương khả năng mã hóa cấp độ lớp giữa các LLMs. Thứ hai, chúng tôi thấy rằng GPT-4 và GPT-3.5 vẫn thể hiện sự vượt trội áp đảo so với các LLMs khác trên việc sinh mã cấp độ lớp, và các mô hình tầng thứ hai bao gồm Instruct-StarCoder, Instruct-CodeGen, và WizardCoder với hiệu suất rất tương tự. Thứ ba, chúng tôi thấy rằng việc sinh toàn bộ lớp cùng một lúc (tức là chiến lược sinh toàn diện) là chiến lược sinh tốt nhất chỉ đối với GPT-4 và GPT-3.5, trong khi việc sinh từng bước (tức là sinh tăng dần và tổng hợp) là các chiến lược tốt hơn cho các mô hình khác với khả năng hạn chế trong việc hiểu các hướng dẫn dài và sử dụng thông tin ở giữa. Cuối cùng, chúng tôi thấy khả năng hạn chế của mô hình trong việc sinh mã phụ thuộc phương thức và thảo luận về các loại lỗi thường gặp trong các lớp được sinh ra.

Tóm lại, bài báo này đưa ra những đóng góp sau đây:
• Benchmark đầu tiên ClassEval cho việc sinh mã cấp độ lớp, được xây dựng thủ công với 500 giờ công người và có sẵn công khai tại [19];
• Nghiên cứu đầu tiên để đánh giá 11 LLMs đại diện trên việc sinh mã cấp độ lớp với ba chiến lược sinh khác nhau;
• Các phát hiện và ý nghĩa trong việc phân tích khả năng của mô hình và hướng phát triển tương lai cho LLMs trên việc sinh mã cấp độ lớp.

2 KIẾN THỨC NỀN
Chúng tôi đầu tiên giới thiệu các LLMs gần đây cho việc sinh mã trong Phần 2.1 và sau đó động cơ cho nghiên cứu của chúng tôi bằng cách xem xét lại các benchmark sinh mã hiện tại trong Phần 2.2.

2.1 Mô Hình Ngôn Ngữ Lớn cho Sinh Mã
Sinh mã là một tác vụ tập trung vào việc sinh các đoạn mã cho mô tả ngôn ngữ tự nhiên đã cho, đã được nghiên cứu rộng rãi trong tài liệu gần đây [1–3]. LLMs chung (ví dụ, GPT-4 [4] và ChatGLM [7]), là các mô hình lớn với hơn hàng tỷ tham số được huấn luyện trên các kho ngữ liệu văn bản/mã chung và hướng dẫn, thể hiện khả năng đáng chú ý không chỉ trong các tác vụ NLP chung [20] mà còn hiệu suất đầy hứa hẹn trong sinh mã. Ví dụ, GPT-4 đạt tỷ lệ vượt qua cao nhất trên benchmark HumanEval [8]. Do đó, gần đây đã có xu hướng tăng lên để đánh giá khả năng sinh mã ngay cả đối với LLMs chung [15,21]. LLMs mã, là các mô hình lớn chủ yếu được huấn luyện với các kho ngữ liệu và hướng dẫn đặc thù mã lớn, thường có khả năng tốt hơn LLMs chung trong các tác vụ sinh mã [8,22,23]. Các LLMs mã hiện tại được thiết kế với các mục tiêu huấn luyện khác nhau. Ví dụ, một số sử dụng dự đoán token tiếp theo, trong khi một số LLMs mã (ví dụ, InCoder [13] và StarCoder [9]) được huấn luyện với khả năng "điền vào giữa" (FIM), tức là điền phần thiếu dựa trên ngữ cảnh. Đến nay, một số lượng lớn LLMs mã đã được đề xuất, như WizardCoder [8], Instruct-StarCoder [24], và Instruct-CodeGen [10].

2.2 Các Benchmark Hiện Tại cho Sinh Mã
Các benchmark sinh mã thường bao gồm nhiều tác vụ mã hóa khác nhau nơi một mô tả ngôn ngữ tự nhiên phục vụ như đầu vào, và mã tương ứng phục vụ như đầu ra thực tế. Các thước đo đánh giá như tỷ lệ vượt qua (Pass@k [18]) thường được sử dụng để đánh giá tính đúng đắn của mã được sinh ra.

Đến nay, nhiều benchmark sinh mã đã được xây dựng thông qua các cách thức tự động hoặc thủ công. Trong nghiên cứu này, chúng tôi xem xét lại các benchmark sinh mã được sử dụng rộng rãi từ ba nguồn sau đây: (i) Top-10 bộ dữ liệu phổ biến với lượng tải xuống cao nhất từ các bộ dữ liệu sinh mã Huggingface [32], (ii) benchmark liên quan đến các bài báo LLM gần đây (phát hành từ tháng 6/2021 đến tháng 6/2023), và (iii) benchmark nâng cao như HumanEval+ [31] và Multi-HumanEval [28]. Bảng 1 cung cấp tổng quan về 13 benchmark riêng biệt được thu thập từ ba nguồn, bao gồm thời gian phát hành, phương pháp xây dựng (tức là viết thủ công hoặc thu thập tự động từ kho mã công khai hoặc cuộc thi), kích thước benchmark (#Tác vụ), độ chi tiết mã đích, ngôn ngữ mã đích, quy mô mã (#LOC: số dòng mã trung bình, #Token: số lượng token trung bình), số lượng trường hợp kiểm tra trung bình mỗi tác vụ (#Tests), và thông tin đầu vào chi tiết. Chúng tôi cũng trình bày benchmark được xây dựng ClassEval trong hàng cuối để so sánh.

Dựa trên Bảng 1, chúng tôi thấy rằng các benchmark hiện tại thực sự định hình một kịch bản sinh mã khá đơn giản, chủ yếu đánh giá khả năng của LLMs trong việc sinh một đơn vị mã duy nhất (một hàm hoặc một câu lệnh) theo cách khá độc lập. Cụ thể, các benchmark hiện tại thường tập trung vào các tác vụ sinh mã cấp độ hàm hoặc cấp độ câu lệnh (Cột "Độ chi tiết") và hiếm khi bao gồm các ngữ cảnh mã bổ sung trong đầu vào (Cột "Thông tin đầu vào"), điều này giả định rằng mã sẽ được sinh ra là một đơn vị độc lập và do đó dẫn đến hai hạn chế trong việc đánh giá LLMs.

Đầu tiên, các benchmark hiện tại chủ yếu tập trung vào các tác vụ sinh mã ngắn, như sinh một hàm hoặc một câu lệnh. Những tác vụ này thường bao gồm số lượng dòng hạn chế (ví dụ, 1 đến 30) và token (ví dụ, 4.6 đến 108.2), có thể không khám phá đầy đủ khả năng của các LLMs gần đây có thể xử lý các chuỗi dài hơn nhiều, như WizardCoder với 2,048 token. Do đó, tiềm năng của LLMs trong việc sinh các đoạn mã dài hơn vẫn không rõ ràng. Thứ hai, các benchmark hiện tại chủ yếu tập trung vào việc sinh các đơn vị mã độc lập mà không xem xét các ngữ cảnh mã khác. Ví dụ, như được thể hiện trong Hình 1, các benchmark như MBPP và HumanEval chỉ cung cấp thông tin hạn chế làm đầu vào, như mô tả ngôn ngữ tự nhiên hoặc chữ ký hàm với các đầu vào và đầu ra mẫu. Tuy nhiên, trong các kịch bản thực tế, các phương thức thường phụ thuộc lẫn nhau hoặc chia sẻ biến. Công trình trước [17] chỉ ra rằng chỉ 30% phương thức trong các dự án mã nguồn mở độc lập với các ngữ cảnh mã khác. Do đó, vẫn không rõ LLMs hoạt động như thế nào trong việc sinh một đơn vị mã ghép của nhiều phương thức phụ thuộc lẫn nhau (ví dụ, gọi lẫn nhau hoặc truy cập cùng một biến).

Động cơ của chúng tôi. Các benchmark hiện tại không thể tạo điều kiện thuận lợi cho việc đánh giá mô hình trên các tác vụ sinh mã phức tạp hơn, như sinh các đơn vị mã dài hơn và ghép của nhiều phương thức phụ thuộc lẫn nhau. Để giải quyết khoảng trống này, chúng tôi xây dựng thủ công benchmark sinh mã cấp độ lớp đầu tiên ClassEval và thực hiện nghiên cứu đầu tiên để đánh giá LLMs trên các tác vụ sinh mã cấp độ lớp, yêu cầu LLMs sinh một lớp của nhiều phương thức phụ thuộc lẫn nhau dựa trên mô tả ngôn ngữ tự nhiên đã cho.

3 BENCHMARK MỚI CLASSEVAL
Trong phần này, chúng tôi giới thiệu benchmark mới ClassEval. Chúng tôi trình bày định dạng benchmark (Phần 3.1), quy trình xây dựng (Phần 3.2), và đặc điểm benchmark (Phần 3.3).

3.1 Định Dạng Benchmark
Mỗi tác vụ mã hóa trong ClassEval bao gồm một mô tả đầu vào cho lớp đích (tức là lớp sẽ được sinh ra), một bộ kiểm tra để xác minh tính đúng đắn của mã được sinh ra, và một giải pháp tiêu chuẩn hoạt động như một triển khai tham chiếu của lớp đích.

Thông thường, LLMs sinh các đoạn mã dựa trên mô tả đầu vào và tính đúng đắn được xác minh với bộ kiểm tra được cung cấp. Mã được sinh ra phải tuân thủ một giao diện nhất quán (ví dụ, các loại tham số đầu vào và giá trị trả về) được chỉ định trong bộ kiểm tra để thực thi hợp lệ. Ví dụ, benchmark HumanEval chỉ định chữ ký của hàm đích (Hình 1) để đảm bảo rằng các thân hàm được sinh ra được kiểm tra hợp lệ bởi bộ kiểm tra đã cho.

Để đạt được điều này, chúng tôi định nghĩa một định dạng khung lớp cho các mô tả đầu vào trong các tác vụ mã hóa của chúng tôi. Khung lớp phục vụ như một bản thiết kế có cấu trúc cho lớp đích, chứa cả thông tin cấp độ lớp (câu lệnh import, tên lớp, mô tả lớp và constructor lớp) và thông tin cấp độ phương thức (chữ ký phương thức, mô tả chức năng, mô tả tham số/trả về, và đầu vào/đầu ra mẫu). Các định nghĩa chi tiết của các yếu tố trong khung lớp ở Bảng 2. Cột "Bắt buộc" chỉ ra liệu yếu tố có bắt buộc trong khung lớp hay không. Các yếu tố cấp độ phương thức đều được áp dụng từ các benchmark hiện tại như HumanEval. Hình 2 minh họa thêm một ví dụ về khung lớp, với các thành phần khác nhau được làm nổi bật bằng nhiều màu sắc khác nhau. Khung lớp, được lấy cảm hứng từ lập trình theo hợp đồng [33], phục vụ như các đặc tả chính thức và chính xác cho việc sinh mã bằng cách phác thảo các hành vi mong đợi, điều kiện tiên quyết và điều kiện hậu quả. LLMs sinh mã cấp độ lớp phù hợp với bộ kiểm tra đã cho dựa trên khung lớp.

3.2 Quy Trình Xây Dựng Benchmark
Hình 3 minh họa quy trình xây dựng ClassEval. Chúng tôi tuân theo bốn bước để tạo ClassEval: (i) chọn các tác vụ mã hóa phù hợp bằng các chiến lược khác nhau (Phần 3.2.1); (ii) xây dựng khung lớp dựa trên các nguyên tắc của lập trình theo hợp đồng [33] và phát triển theo hướng kiểm tra [34] (Phần 3.2.2); (iii) tạo bộ kiểm tra cho mỗi khung lớp (Phần 3.2.3); và (iv) viết giải pháp tiêu chuẩn cho mỗi tác vụ mã hóa (Phần 3.2.4). Các khung lớp được xây dựng, bộ kiểm tra và giải pháp tiêu chuẩn tạo thành benchmark sinh mã cấp độ lớp ClassEval của chúng tôi.

Để tránh việc các tác vụ mã hóa bị nhìn thấy bởi LLMs trong quá trình huấn luyện của chúng, benchmark của chúng tôi được xây dựng hoàn toàn thủ công, nhằm giảm thiểu rò rỉ dữ liệu tiềm năng từ các nguồn mã hiện tại. Việc xây dựng thủ công của chúng tôi bao gồm một quá trình tốn thời gian với khoảng 500 giờ công người để xây dựng 100 tác vụ mã hóa cấp độ lớp. Do những nỗ lực thủ công đáng kể cần thiết, hiện tại chúng tôi dừng quy mô benchmark ở kích thước này. Hơn nữa, theo xu hướng của hầu hết các benchmark hiện tại [15,16], benchmark của chúng tôi chủ yếu tập trung vào Python do tính phổ biến của nó [35].

3.2.1 Chọn Lọc Tác Vụ. Trong bước này, chúng tôi thiết kế các tác vụ mã hóa cấp độ lớp (tức là một mô tả lớp duy nhất cho mỗi tác vụ như được định nghĩa trong Bảng 2) cho benchmark của chúng tôi.

Nguồn Bao Gồm. Chúng tôi thiết kế các tác vụ mã hóa của chúng tôi để bao gồm các chủ đề phát triển đa dạng và thực tế, dựa trên ba nguồn sau đây. (i) Xem xét lại Các Benchmark Hiện Tại. Chúng tôi tham khảo các benchmark đã được thiết lập như HumanEval và MBPP (Bảng 1) để bao gồm các chủ đề phổ biến và thông thường. (ii) Khám phá Các Chủ đề PyPI. Chúng tôi khám phá thủ công Chỉ mục Gói Python (PyPI) [36], lưu trữ một kho lớn các gói phần mềm Python và cung cấp một loạt chủ đề tác vụ tiềm năng đa dạng. (iii) Động não. Tất cả các tác giả (với 2-8 năm kinh nghiệm phát triển Python) tích cực tham gia động não để tạo ra các tác vụ mã hóa tiềm năng ngoài những tác vụ được thu thập ở trên.

Tiêu Chí Loại Trừ. Benchmark của chúng tôi tập trung vào các tác vụ mã hóa có thể được triển khai trong một lớp duy nhất. Do đó, chúng tôi loại trừ các tác vụ có phụ thuộc phức tạp vào môi trường thực thi, bao gồm những tác vụ liên quan đến (i) Lập trình Mạng, (ii) Thiết kế Giao diện Người dùng Đồ họa (GUI), (iii) Trực quan hóa Dữ liệu, (iv) Lập trình Hệ thống, và (v) Lập trình Đồng thời. Những tác vụ này thường yêu cầu tương tác với các lớp khác hoặc không thể dễ dàng được xác minh bằng các câu lệnh khẳng định trong kiểm tra đơn vị.

Bằng cách này, chúng tôi có được một danh sách 100 tác vụ mã hóa cấp độ lớp đa dạng, bao gồm một phổ rộng các chủ đề, như Phát triển Game, Xử lý Tệp và Hệ thống Quản lý. Bảng 3 trình bày phân phối chủ đề của các tác vụ của chúng tôi.

3.2.2 Xây Dựng Khung Lớp. Trong bước này, chúng tôi xây dựng thủ công khung lớp cho mỗi tác vụ mã hóa, có sự tham gia của 5 người với trung bình 3 năm kinh nghiệm phát triển Python. Mỗi khung ban đầu được giao cho hai người tham gia, một người chịu trách nhiệm viết khung lớp và người kia kiểm tra kép. Trong trường hợp bất đồng, người thứ ba tạo điều kiện thảo luận để đạt được sự đồng thuận về khung lớp. Toàn bộ quá trình tuân thủ các nguyên tắc thiết kế sau đây.

Nguyên tắc 1 (phụ thuộc): Mỗi khung lớp nên chứa các phương thức với các phụ thuộc đa dạng, tức là các phương thức phụ thuộc vào các ngữ cảnh mã khác trong lớp. Công trình trước [17] đã chỉ ra rằng phần lớn các phương thức (hơn 70%) phụ thuộc vào các ngữ cảnh mã khác trong dự án. Không giống như các benchmark trước (ví dụ, HumanEval và MBPP được thể hiện trong Hình 1) tập trung vào việc sinh mã cấp độ hàm độc lập, benchmark cấp độ lớp của chúng tôi nhằm nắm bắt kịch bản thực tế nơi các phương thức thường có phụ thuộc với các ngữ cảnh mã khác. Để phân biệt benchmark của chúng tôi với các benchmark cấp độ hàm, chúng tôi cố ý tránh các tác vụ sinh một lớp với các phương thức độc lập, điều này về cơ bản sẽ là một tập hợp các tác vụ mã hóa cấp độ phương thức riêng lẻ. Thay vào đó, các khung lớp trong benchmark của chúng tôi bao gồm các phương thức với các phụ thuộc đa dạng, bao gồm (i) Phụ thuộc Thư viện, nơi các phương thức phụ thuộc vào các thư viện bên ngoài; (ii) Phụ thuộc Trường, nơi các phương thức phụ thuộc vào các biến thể hiện lớp (trường); (iii) Phụ thuộc Phương thức, nơi các phương thức phụ thuộc vào các phương thức khác trong cùng lớp; và (iv) Độc lập, nơi các phương thức hoạt động độc lập mà không phụ thuộc vào trường, phương thức hoặc thư viện bên ngoài.

Nguyên tắc 2 (constructor lớp): Constructor lớp (nếu có) trong mỗi khung lớp nên định nghĩa các trường lớp và giá trị mặc định của chúng. Constructor cũng bao gồm mô tả ngôn ngữ tự nhiên về các trường lớp để cung cấp hiểu biết rõ ràng về ý nghĩa của chúng. Quan trọng, constructor không gọi đến các phương thức khác trong lớp để bảo tồn tính độc lập và tự chứa của quá trình khởi tạo lớp.

Nguyên tắc 3 (chức năng phương thức): Chúng tôi tránh bao gồm các chức năng phức tạp như đóng kết nối cơ sở dữ liệu, không dễ kiểm tra và xác minh. Ngoài ra, chúng tôi nâng cao khả năng tái sử dụng và bảo trì mã bằng cách chia nhỏ các chức năng chung và lặp lại thành các phương thức riêng biệt. Nguyên tắc này nuôi dưỡng các phụ thuộc tiềm năng giữa các phương thức, mô phỏng một kịch bản mã hóa kết nối và thực tế hơn.

Nguyên tắc 4 (tham số phương thức): Các tham số phương thức được giới hạn ở các kiểu dữ liệu nguyên thủy, tránh các tham số cấp độ đối tượng hoặc các đối số được định nghĩa lỏng lẻo như **kwargs. Nguyên tắc này không chỉ nâng cao tính rõ ràng trong việc gọi phương thức mà còn tạo điều kiện thuận lợi cho việc kiểm tra, làm cho việc tạo kiểm tra đơn vị và xác minh chức năng của các phương thức riêng lẻ một cách riêng biệt dễ dàng hơn.

Nguyên tắc 5 (giá trị trả về phương thức): Các phương thức nên bao gồm giá trị trả về bất cứ khi nào có thể để kiểm tra. Để chỉ ra thành công hoặc thất bại, chúng sử dụng kiểu trả về Boolean để chuẩn hóa thay vì chuỗi tùy chỉnh. Ngoài ra, thiết kế phương thức có thể bao gồm các điều kiện đánh giá cho các tham số đầu vào và bao gồm các cơ chế xử lý ngoại lệ. Các đặc tả chi tiết về các loại ngoại lệ, nội dung thông báo và các tình huống kích hoạt được cung cấp để đảm bảo việc kiểm tra và xác thực toàn diện cho việc xử lý ngoại lệ.

Mỗi khung lớp được xây dựng sẽ chứa các yếu tố bắt buộc (tức là mô tả lớp, tên lớp, chữ ký phương thức và mô tả chức năng) và các yếu tố tùy chọn (tức là câu lệnh import, constructor lớp, mô tả tham số/trả về và đầu vào/đầu ra mẫu).

3.2.3 Xây Dựng Kiểm Tra. Trong bước này, chúng tôi xây dựng thủ công một bộ kiểm tra cho mỗi tác vụ mã hóa dựa trên khung lớp của nó. Các người tham gia chịu trách nhiệm tạo khung lớp bây giờ đảm nhận nhiệm vụ viết bộ kiểm tra tương ứng. Tương tự, một người tham gia tập trung vào việc viết các trường hợp kiểm tra đơn vị, trong khi người khác đảm bảo chất lượng và tính đúng đắn của các trường hợp kiểm tra.

Các phương thức trong mỗi khung lớp được thiết kế để có nhiều mối quan hệ phụ thuộc, như đã đề cập trong Nguyên tắc 1 trong Phần 3.2.2. Do đó, các người tham gia được yêu cầu xây dựng các trường hợp kiểm tra ở hai cấp độ: kiểm tra cấp độ phương thức và kiểm tra cấp độ lớp, để kiểm tra đầy đủ tính đúng đắn của các phương thức được triển khai khi chúng được gọi riêng lẻ hoặc cùng nhau. Kiểm tra cấp độ phương thức chủ yếu kiểm tra tính đúng đắn của mỗi phương thức đang được kiểm tra bằng cách gọi nó một cách độc lập mà không gọi bất kỳ phương thức nào khác trong lớp. Mặt khác, kiểm tra cấp độ lớp chủ yếu kiểm tra tính đúng đắn của nhiều phương thức đang được kiểm tra bằng cách gọi chúng tuần tự cùng nhau. Kiểm tra cấp độ phương thức đảm bảo rằng tính đúng đắn của mỗi phương thức đang được kiểm tra được kiểm tra riêng lẻ mà không bị ảnh hưởng bởi việc triển khai không đúng của các phương thức khác, trong khi kiểm tra cấp độ lớp đánh giá tính đúng đắn tổng thể của lớp bằng cách xem xét các tương tác của nó. Hình 4 cung cấp hai ví dụ về cả trường hợp kiểm tra cấp độ phương thức và cấp độ lớp được xây dựng cho khung lớp trong Hình 2. Ngoài ra, chúng tôi bao gồm các ví dụ về trường hợp kiểm tra từ các benchmark hiện tại HumanEval và MBPP để làm nổi bật sự khác biệt. Kiểm tra cấp độ hàm trong các benchmark hiện tại có thể so sánh với kiểm tra cấp độ phương thức trong benchmark của chúng tôi, nhưng sự khác biệt chính là kiểm tra cấp độ hàm trong các benchmark hiện tại chỉ kiểm tra giá trị trả về của hàm đang được kiểm tra trong khi kiểm tra cấp độ phương thức của chúng tôi kiểm tra thêm các trường của lớp. Như được thể hiện trong Hình 4, khi kiểm tra phương thức purchase_item, kiểm tra cấp độ phương thức trong ClassEval không chỉ xác minh giá trị trả về mà còn đánh giá các hoạt động được thực hiện trên trường inventory. Hơn nữa, các benchmark hiện tại thiếu kiểm tra cấp độ lớp vì chúng chủ yếu tập trung vào việc sinh hàm đơn.

Sau đó chúng tôi giới thiệu các nguyên tắc chính của việc xây dựng kiểm tra cấp độ phương thức và kiểm tra cấp độ lớp, tương ứng. Đối với kiểm tra cấp độ phương thức, các người tham gia được yêu cầu tạo ít nhất năm trường hợp kiểm tra để bao gồm các kịch bản đa dạng của mỗi phương thức đang được kiểm tra. Đối với kiểm tra cấp độ lớp, các người tham gia được yêu cầu xây dựng các trường hợp kiểm tra với các kết hợp khác nhau của các phương thức đang được kiểm tra, đảm bảo rằng mỗi phương thức được gọi ít nhất một lần trong kiểm tra cấp độ lớp. Để đơn giản hóa việc xây dựng kiểm tra, các người tham gia được yêu cầu sử dụng framework unittest hiện tại [37], cung cấp các API khẳng định đa dạng và một tập hợp Test Fixtures (ví dụ, các phương thức setUp và tearDown) cho các tác vụ chuẩn bị và dọn dẹp trước và sau khi thực thi kiểm tra. Ngoài ra, tất cả các trường hợp kiểm tra được xây dựng được giới hạn trong thời gian chạy năm giây để ngăn chặn các vòng lặp vô hạn tiềm năng trong mã được sinh ra.

3.2.4 Xây Dựng Giải Pháp Tiêu Chuẩn. Trong bước này, chúng tôi viết thủ công giải pháp tiêu chuẩn cho mỗi tác vụ mã hóa dựa trên khung lớp và trường hợp kiểm tra được xây dựng của nó. Bốn người tham gia (mỗi người có 2-4 năm kinh nghiệm phát triển Python) không tham gia vào việc xây dựng khung lớp và trường hợp kiểm tra được tham gia vào bước này. Mỗi tác vụ mã hóa được giao cho hai người tham gia, với một người chịu trách nhiệm viết giải pháp tiêu chuẩn và người khác kiểm tra kép. Các người tham gia được yêu cầu thực thi các giải pháp với trường hợp kiểm tra để xác định và sửa chữa bất kỳ lỗi nào.

3.3 Đặc Điểm Benchmark
Bằng cách này, chúng tôi xây dựng thủ công một benchmark mới ClassEval gồm 100 tác vụ mã hóa cấp độ lớp. Các đặc điểm chi tiết như sau.

Quy mô. ClassEval bao gồm 100 lớp và 412 phương thức. Để tạo điều kiện so sánh trực tiếp với các benchmark sinh mã khác, chúng tôi bao gồm dữ liệu thống kê của ClassEval trong Bảng 1. Kết quả tiết lộ sự khác biệt lớn về số dòng mã cho ClassEval (45.7) so với hai benchmark viết tay được sử dụng rộng rãi nhất, HumanEval và MBPP, với hệ số nhân lần lượt là 4.0 và 6.7. Ngoài ra, chúng tôi thực hiện thống kê bổ sung về số lượng token trung bình trong toàn bộ thông tin docstring (khung lớp) trong ClassEval (259.3), vượt qua HumanEval (67.7) và MBPP (14.5) với hệ số lần lượt là 3.8 và 17.9. Những kết quả này chứng minh rằng tác vụ sinh mã cấp độ lớp trong ClassEval trình bày độ phức tạp cao hơn, bao gồm việc sinh mã dài hơn, cũng như thông tin docstring chi tiết và tinh vi hơn.

Đầy Đủ Kiểm Tra. Bảng 4 cung cấp thống kê độ bao phủ cho các trường hợp kiểm tra trong benchmark của chúng tôi so với HumanEval và MBPP. Chúng tôi thu thập độ bao phủ cấp độ câu lệnh và cấp độ nhánh của các trường hợp kiểm tra trên mã giải pháp tiêu chuẩn bằng bộ công cụ Python coverage [38]. Ngoài ra, chúng tôi cung cấp số lượng kiểm tra cấp độ phương thức trung bình (#Tests/M) và kiểm tra cấp độ lớp trung bình (#Tests/C). Như được thể hiện trong Bảng 4, các trường hợp kiểm tra trong ClassEval đạt độ bao phủ cấp độ câu lệnh và cấp độ nhánh cao hơn đáng kể (cả hai đều trên 98%) so với HumanEval và MBPP. Điều này chỉ ra việc kiểm tra mã rộng rãi hơn cho các giải pháp được sinh ra trong benchmark của chúng tôi, được hỗ trợ bởi thực tế là ClassEval cũng bao gồm số lượng lớn hơn kiểm tra cấp độ phương thức và cấp độ lớp trung bình.

Phụ Thuộc. ClassEval tập trung vào các tác vụ sinh mã cấp độ lớp, phân biệt nó với các benchmark trước. Bảng 5 thể hiện phân phối các cấp độ phụ thuộc trong các phương thức trên ClassEval và các benchmark trước, như đã giải thích trong Phần 3.1. Đáng chú ý, các phụ thuộc Thư viện, Trường và Phương thức không loại trừ lẫn nhau, và một số phương thức có thể có sự kết hợp của phụ thuộc Trường và Phương thức. Chúng tôi phân loại các phương thức có phụ thuộc Trường hoặc Phương thức là các phương thức phụ thuộc cấp độ lớp, tổng cộng 314 (76.2%) trong ClassEval. Việc bao gồm này làm cho ClassEval trở thành một benchmark toàn diện, phù hợp để đánh giá LLMs phải tính đến các tương tác cấp độ lớp phức tạp và phụ thuộc ngữ cảnh.

Nhìn chung, so với các benchmark sinh mã được tạo thủ công trước đây, ClassEval chứa các tác vụ mã hóa cấp độ lớp phức tạp bao gồm các đoạn mã quy mô lớn hơn, phụ thuộc đa dạng, trường hợp kiểm tra đầy đủ và một loạt chủ đề rộng hơn từ phát triển phần mềm thực tế.

4 NGHIÊN CỨU THỰC NGHIỆM
Sử dụng ClassEval, chúng tôi thực hiện nghiên cứu đầu tiên để đánh giá các LLMs hiện tại trên việc sinh mã cấp độ lớp bằng cách trả lời các câu hỏi nghiên cứu sau đây.

• RQ1 (Tính Đúng Đắn Tổng Thể): LLMs hoạt động như thế nào trên việc sinh mã cấp độ lớp?
• RQ2 (Chiến Lược Sinh): các chiến lược sinh khác nhau hoạt động như thế nào đối với LLMs trên việc sinh mã cấp độ lớp?
• RQ3 (Sinh Phụ Thuộc): LLMs hoạt động như thế nào trong việc sinh mã phụ thuộc vào các ngữ cảnh khác trong quá trình sinh mã cấp độ lớp?
• RQ4 (Phân Tích Trường Hợp Xấu): các lỗi phổ biến trong quá trình sinh mã cấp độ lớp là gì?

4.1 LLMs Được Nghiên Cứu
Chúng tôi chọn các LLMs tiên tiến nhất đã được nghiên cứu rộng rãi trong công việc sinh mã gần đây [8,31]. Cụ thể, chúng tôi tập trung vào các mô hình gần đây được phát hành từ năm 2022, và chúng tôi loại trừ các mô hình nhỏ (với ít hơn 1B tham số) do hiệu quả hạn chế của chúng hoặc các mô hình lớn (với hơn 20B tham số) do giới hạn tài nguyên của chúng tôi. Bảng 6 trình bày 11 LLMs được nghiên cứu trong các thí nghiệm của chúng tôi với thời gian phát hành (Cột "Thời gian"), kích thước mô hình (Cột "Kích thước") và mô hình cơ sở. Ngoài ra, chúng tôi cũng tóm tắt các đặc điểm huấn luyện của các mô hình được nghiên cứu, bao gồm liệu mô hình có được huấn luyện để có khả năng "điền vào giữa" (FIM) và liệu nó có khả năng tuân theo hướng dẫn (IF) thông qua tinh chỉnh hướng dẫn. Cả khả năng FIM và IF đều cần thiết cho các tác vụ sinh mã cấp độ lớp. Như được thể hiện trong Bảng 6, nghiên cứu của chúng tôi bao gồm một phạm vi rộng LLMs đa dạng theo nhiều chiều, như (i) vừa mã nguồn đóng và mã nguồn mở, (ii) sử dụng các mô hình cơ sở khác nhau, (iii) bao gồm một loạt kích thước mô hình từ 1B đến 16B, (iv) được huấn luyện bởi cả hướng dẫn chung hoặc đặc thù mã, và (v) thể hiện các khả năng FIM và IF khác nhau.

4.2 Chiến Lược Sinh Được Nghiên Cứu
Với một tác vụ sinh mã cấp độ lớp, chúng tôi nghiên cứu hiệu suất của mỗi mô hình với ba chiến lược sinh khác nhau như sau:

• Sinh Toàn Diện: mô hình được yêu cầu sinh toàn bộ lớp cùng một lúc với khung lớp làm đầu vào.
• Sinh Tăng Dần: mô hình được yêu cầu sinh lớp theo cách từng phương thức một. Mỗi lần lặp dựa trên các thân phương thức đã được sinh ra trong các lần lặp trước. Quá trình lặp lại tiếp tục cho đến khi tất cả các phương thức trong lớp được sinh ra.
• Sinh Tổng Hợp: mô hình được yêu cầu sinh lớp theo cách từng phương thức một. Mỗi lần lặp độc lập, không xem xét các phương thức được sinh ra khác. Tất cả các phương thức được sinh ra được lắp ráp để tạo thành lớp cuối cùng.

Chiến lược sinh toàn diện đánh giá khả năng của mô hình trong việc xử lý các tác vụ mã hóa dài và phức tạp cùng một lúc, trong khi các chiến lược sinh tăng dần và tổng hợp tập trung vào việc hoàn thành lớp từng bước. Chiến lược tăng dần mô phỏng phát triển phần mềm tiến bộ, nơi các nhà phát triển triển khai tăng dần các phương thức hiện tại dựa trên các phương thức hiện có. Ngược lại, chiến lược tổng hợp mô phỏng các kịch bản lập trình thực tế, nơi các nhà phát triển triển khai các phương thức hiện tại dựa trên các chữ ký phương thức có sẵn khác. Chiến lược sinh tổng hợp không bị ảnh hưởng bởi các gợi ý (nếu các phương thức được triển khai đúng) hoặc thông tin gây hiểu lầm (nếu các phương thức được triển khai không đúng) vì nó không sử dụng triển khai phương thức khác làm đầu vào. Đáng chú ý, cả chiến lược sinh tăng dần và tổng hợp đều khác với các tác vụ sinh mã cấp độ hàm độc lập trong các benchmark hiện tại như HumanEval, vì đầu vào của chúng tôi bao gồm ngữ cảnh cấp độ lớp như constructor lớp và các chữ ký phương thức khác trong khung lớp.

4.3 Thiết Kế Prompt
Sau đó chúng tôi mô tả cách chúng tôi nhắc LLMs giải quyết mỗi tác vụ sinh mã cấp độ lớp trong ClassEval với mỗi chiến lược sinh.

LLMs với khả năng IF. Theo thực hành phổ biến của việc nhắc LLMs với khả năng IF như WizardCoder [8], chúng tôi đặt prompt của chúng gồm hai phần: (i) một prompt hệ thống như câu khởi đầu để khởi tạo mô hình, và theo sau là (ii) một hướng dẫn tác vụ để mô tả mục tiêu của tác vụ. Mỗi chiến lược sinh được đặt với hướng dẫn tác vụ cụ thể của nó, tức là Instruction-H cho sinh toàn diện, Instruction-I cho sinh tăng dần, và Instruction-C cho sinh tổng hợp. Mẫu prompt như sau, và mỗi yếu tố đã được định nghĩa trước đó trong Bảng 2.

Prompt Hệ thống: Được cung cấp dưới đây là một hướng dẫn chi tiết một tác vụ. Soạn một phản hồi phù hợp với yêu cầu.
Instruction-H: Vui lòng hoàn thành lớp ${Tên Lớp} trong mã tiếp theo. ${Khung Lớp}
Instruction-I: Vui lòng hoàn thành phương thức ${Tên Phương Thức} trong lớp ${Tên Lớp} sau đây. ${Thông tin Cấp Lớp} ${Phương Thức Được Sinh với Thiết Kế Hợp Đồng} ${Thiết Kế Hợp Đồng Phương Thức Đích}
Instruction-C: Vui lòng hoàn thành phương thức ${Tên Phương Thức} trong lớp ${Tên Lớp} sau đây. ${Thông tin Cấp Lớp} ${Chữ Ký Phương Thức Khác} ${Thiết Kế Hợp Đồng Phương Thức Đích}

LLMs không có khả năng IF. Prompt của các mô hình này là ngữ cảnh mã không có bất kỳ hướng dẫn nào: (i) đối với sinh toàn diện, prompt chỉ là khung lớp; (ii) đối với sinh tăng dần, prompt trong mỗi lần lặp bao gồm thông tin cấp độ lớp, các phương thức được sinh ra và thiết kế hợp đồng phương thức đích; (iii) đối với sinh tổng hợp, prompt cho mỗi phương thức bao gồm thông tin cấp độ lớp, các chữ ký phương thức khác và thiết kế hợp đồng phương thức đích.

4.4 Thước Đo
Để đánh giá tính đúng đắn, chúng tôi sử dụng thước đo Pass@k được sử dụng rộng rãi [18], tính tỷ lệ phần trăm các vấn đề được giải quyết dựa trên 𝑘 mẫu mã được tạo ra cho mỗi tác vụ:

Pass@k =E Problems [1−(n−c k)/(n k)] (1)

Trong Phương trình 1, 𝑛 đại diện cho tổng số mẫu, 𝑐 biểu thị số mẫu đúng, và 𝑘 đại diện cho 𝑘 trong 𝑝𝑎𝑠𝑠@𝑘. Cụ thể, chúng tôi tính cả Pass@k cấp độ lớp và Pass@k cấp độ phương thức trong các tác vụ sinh mã cấp độ lớp: Pass@k cấp độ lớp xem xét các mẫu mã ở độ chi tiết lớp và Pass@k cấp độ phương thức xem xét các mẫu mã ở độ chi tiết phương thức. Một mẫu mã cấp độ lớp được coi là đúng nếu nó vượt qua tất cả các trường hợp kiểm tra cấp độ phương thức và cấp độ lớp; và một mẫu cấp độ phương thức được coi là đúng nếu nó vượt qua tất cả các trường hợp kiểm tra cấp độ phương thức. Để duy trì chi phí và thời gian phản hồi chấp nhận được trong các thiết lập thực tế, chúng tôi đặt 𝑛 thành năm. Để giải quyết thách thức của phương sai lấy mẫu cao, chúng tôi sử dụng một ước lượng không thiên vị phù hợp với công việc trước [15].

Ngoài tính đúng đắn của mã, chúng tôi đo thêm khả năng của mô hình trong việc sinh mã phụ thuộc vào các ngữ cảnh (tức là gọi các phương thức khác được khai báo trong lớp hoặc đánh giá các trường trong lớp). Khả năng như vậy là cần thiết trong sinh mã cấp độ lớp. Để đạt được điều này, chúng tôi thiết kế thước đo DEP, tính tỷ lệ phần trăm phụ thuộc được sinh ra mỗi phương thức so với số lượng phụ thuộc thực tế trong phương thức giải pháp tiêu chuẩn. Cụ thể, chúng tôi xem xét phụ thuộc phương thức DEP(M) và phụ thuộc trường DEP(F):

DEP(𝑀)=Σni=1Gi(𝑀)/Σni=1Si(𝑀) (2) DEP(𝐹)=Σni=1Gi(𝐹)/Σni=1Si(𝐹) (3)

𝐺𝑖(𝑀/𝐹) là số lượng phụ thuộc phương thức/trường được sinh ra trong phương thức thứ 𝑖, và 𝑆𝑖(𝑀/𝐹) là số lượng phụ thuộc phương thức/trường thực tế trong phương thức thứ 𝑖 của giải pháp tiêu chuẩn.

Đối với mỗi chiến lược sinh, chúng tôi sử dụng lấy mẫu nucleus để tạo ra 5 mẫu và tính các thước đo Pass@k với 𝑘={1,3,5}. Ngoài ra, chúng tôi cũng sử dụng chiến lược lấy mẫu tham lam để tạo ra một mẫu tham lam duy nhất và tính Pass@1 và thước đo DEP. Chi tiết lấy mẫu thêm ở Phần 4.5.

4.5 Chi Tiết Triển Khai
Chúng tôi sử dụng giao diện API OpenAI, cụ thể là giao diện mô hình "gpt-4" và "gpt-3.5-turbo" [44], vào tháng 7/2023. Đối với các LLMs mã nguồn mở, chúng tôi trực tiếp lấy và chạy các phiên bản đã phát hành từ các kho lưu trữ chính thức của chúng dựa trên tài liệu. Độ dài cửa sổ tối đa được đặt thành 2,048 token cho tất cả LLMs, được xác định bởi độ dài cửa sổ tối đa nhỏ nhất trong số các LLMs được nghiên cứu.

Phù hợp với công việc gần đây [17], chúng tôi xem xét hai phương pháp lấy mẫu cho sinh mã: (i) lấy mẫu nucleus [45], nơi năm mẫu mã giải pháp được tạo ngẫu nhiên cho mỗi tác vụ với nhiệt độ 0.2 [15] và top_p mặc định, và (ii) lấy mẫu tham lam [46], nơi chỉ một mẫu mã giải pháp duy nhất được tạo ra cho mỗi tác vụ sử dụng giải mã tham lam, tức là đặt siêu tham số "do_sample" thành false (nhiệt độ 0). Trong mỗi lần lặp trong sinh tăng dần và tổng hợp, chúng tôi có được kết quả Top-1 được tạo ra cho mỗi phương thức. Các thí nghiệm của chúng tôi được chạy trên một cơ sở hạ tầng tính toán bao gồm tám GPU A800-80G.

5 KẾT QUẢ

5.1 RQ1: Tính Đúng Đắn Tổng Thể
Hình 5 thể hiện Pass@1 cấp độ lớp và cấp độ phương thức với lấy mẫu tham lam của các LLMs được nghiên cứu trên ClassEval và HumanEval. Do giới hạn không gian, chúng tôi chỉ trình bày Pass@1 cấp độ lớp tốt nhất (và Pass@1 cấp độ phương thức tương ứng) cho mỗi mô hình trong ba chiến lược sinh. Một so sánh chi tiết giữa ba chiến lược sinh được thảo luận trong Phần 5.3. Kết quả Pass@1 cấp độ phương thức trên HumanEval được áp dụng trực tiếp từ công việc mới nhất [8], và kết quả ChatGLM trên HumanEval không có trong đánh giá hiện tại. Bảng 7 trình bày Pass@k cấp độ lớp và cấp độ phương thức với lấy mẫu nucleus trên ClassEval. Tương tự, do giới hạn không gian, chúng tôi chỉ trình bày kết quả cho chiến lược sinh với Pass@1 cấp độ lớp cao nhất. Dựa trên Hình 5 và Bảng 7, chúng tôi có những quan sát sau đây.

Sinh mã cấp độ lớp so với Sinh mã cấp độ phương thức. Dựa trên Hình 5, chúng tôi quan sát một sự giảm đáng kể về tính đúng đắn cho tất cả các mô hình được nghiên cứu trên benchmark cấp độ lớp ClassEval của chúng tôi so với benchmark cấp độ phương thức hiện tại HumanEval. Cụ thể, các mô hình hoạt động tốt nhất GPT-4 và GPT-3.5 đạt 85.4%/68.9% tính đúng đắn trên các tác vụ cấp độ phương thức trong HumanEval, nhưng chỉ 37.0%/27.0% tính đúng đắn trên các tác vụ cấp độ lớp trong ClassEval. Xu hướng tương tự có thể được quan sát trên các mô hình khác, ví dụ, WizardCoder sinh đúng 59.8% phương thức trên HumanEval, nhưng chỉ 11.0% lớp đúng trong benchmark của chúng tôi. Mặc dù có những thách thức vốn có của việc sinh một lớp với nhiều phương thức, sự giảm tính đúng đắn quan sát được trên benchmark ClassEval của chúng tôi không chỉ do số lượng phương thức lớn hơn cần sinh ra. Mã được sinh ra bởi tất cả các mô hình cũng thể hiện tính đúng đắn cấp độ phương thức thấp hơn trên ClassEval so với HumanEval. Ví dụ, Pass@1 cấp độ phương thức của GPT-4 và GPT-3.5 giảm từ 85.4%/68.9% (trên HumanEval) xuống 62.5%/52.5% (trên ClassEval). Sự giảm này có thể được quy cho độ phức tạp của việc sinh mã phụ thuộc vào ngữ cảnh khác, được biết là thách thức hơn việc sinh mã độc lập. Phát hiện này phù hợp với công việc gần đây [17]. Tóm lại, kết quả của chúng tôi cho thấy rằng các LLMs hiện tại vẫn có hiệu suất hạn chế trong việc giải quyết các tác vụ mã hóa phức tạp, như sinh mã cấp độ lớp.

Ngoài ra, chúng tôi quan sát rằng hiệu suất của mô hình trong các tác vụ sinh mã cấp độ phương thức độc lập không nhất thiết phản ánh khả năng sinh mã cấp độ lớp của chúng. Ví dụ, trong khi WizardCoder và Instruct-StarCoder thể hiện Pass@1 cấp độ phương thức cao hơn nhiều (59.8% và 34.1%) so với SantaCoder (14.6%) trên HumanEval, cả ba mô hình đều thể hiện hiệu suất tương tự trên các tác vụ sinh mã cấp độ lớp trong ClassEval (khoảng 10% - 11% Pass@1). Điều này chỉ ra rằng khả năng mã hóa cấp độ phương thức không thể đại diện tương đương cho khả năng mã hóa cấp độ lớp giữa các LLMs, xác nhận thêm sự cần thiết của việc xây dựng benchmark sinh mã cấp độ lớp.

Phát hiện 1: Các LLMs hiện tại thể hiện hiệu suất thấp hơn đáng kể trên các tác vụ sinh mã cấp độ lớp so với các tác vụ sinh mã cấp độ phương thức độc lập. Ngoài ra, khả năng mã hóa cấp độ phương thức không thể đại diện tương đương cho khả năng mã hóa cấp độ lớp giữa các LLMs. Những phát hiện này xác nhận mạnh mẽ động cơ và sự cần thiết của việc xây dựng các benchmark sinh mã cấp độ lớp.

So sánh giữa các mô hình. Như được thể hiện trong Hình 5 và Bảng 7, chuỗi GPT (GPT-4 và GPT-3.5) vượt trội đáng kể so với tất cả các mô hình khác trong việc giải quyết các tác vụ mã hóa cấp độ lớp với cả lấy mẫu tham lam và lấy mẫu nucleus. Ví dụ, trong Bảng 7, chúng vượt trội hơn mô hình xếp thứ ba WizardCoder 25.4% và 17.4% trong Pass@1 cấp độ lớp với lấy mẫu nucleus. Kết quả như vậy chỉ ra sự thống trị tương đối ổn định của các mô hình GPT khi được khái quát hóa để giải quyết các tác vụ mã hóa cấp độ lớp thách thức hơn.

Tầng xếp hạng thứ hai bao gồm các mô hình mã lớn hơn như Instruct-StarCoder, Instruct-CodeGen và WizardCoder, đạt Pass@1 tương tự với lấy mẫu tham lam trong khoảng từ 10.0% - 11.1%. Đáng chú ý, trong khi các mô hình này thể hiện sự khác biệt hiệu suất đáng kể trên các tác vụ mã hóa cấp độ phương thức trong HumanEval (WizardCoder vượt trội hơn Instruct-CodeGen 27.5% trên HumanEval), chúng hoạt động tương tự trên các tác vụ mã hóa cấp độ lớp. Các mô hình nhỏ hơn (ví dụ, PolyCoder) hoặc mô hình chung (ví dụ, ChatGLM) thường thể hiện hiệu suất tệ hơn, như mong đợi do tầm quan trọng của kích thước mô hình và bộ dữ liệu hướng dẫn cho khái quát hóa. Ngoại lệ duy nhất là SantaCoder, đạt hiệu suất có thể so sánh với các mô hình mã lớn hơn (Instruct-StarCoder, WizardCoder và Instruct-CodeGen) với kích thước mô hình nhỏ hơn nhiều.

Phát hiện 2: Trên sinh mã cấp độ lớp, GPT-4/GPT-3.5 vẫn thể hiện sự vượt trội áp đảo so với các LLMs khác; Instruct-StarCoder, Instruct-CodeGen và WizardCoder hoạt động tương tự như tầng thứ hai; các mô hình nhỏ hoặc chung thường hoạt động tệ nhất, ngoại trừ SantaCoder, đạt hiệu suất có thể so sánh với các mô hình lớn hơn nhưng với ít tham số hơn nhiều.

5.2 RQ2: Chiến Lược Sinh
Hình 6 so sánh Pass@5 cấp độ lớp và Pass@5 cấp độ phương thức của ba chiến lược sinh khác nhau (tức là sinh toàn diện, tăng dần và tổng hợp). Dựa trên hình, nhìn chung, chúng tôi thấy rằng chiến lược sinh tốt nhất khác nhau giữa các LLMs khác nhau.

Chiến lược toàn diện so với các chiến lược khác. Một mặt, sinh toàn diện là chiến lược sinh tốt nhất chỉ đối với hai mô hình GPT-4 và GPT-3.5, đạt Pass@5 cấp độ lớp cao hơn nhiều so với hai chiến lược khác (tức là cải thiện từ 6% đến 9% cho GPT-4 và 4% đến 14% cho GPT-3.5). Ngoài ra, ngay cả đối với tính đúng đắn cấp độ phương thức, sinh toàn diện vẫn vượt trội hơn việc sinh phương thức theo cách tăng dần hoặc tổng hợp (tức là cải thiện 1.4% - 9.0% trong Pass@5 cấp độ phương thức). Mặt khác, xu hướng khác nhau đối với các mô hình khác, thực sự hoạt động tốt hơn nhiều khi sinh lớp từng phương thức một, cụ thể với các chiến lược tăng dần hoặc tổng hợp. Ví dụ, về tính đúng đắn cấp độ lớp, CodeGeeX và SantaCoder sinh ra nhiều lớp đúng hơn 9% và 7% với chiến lược tăng dần so với chiến lược sinh toàn diện. Lý do chính là các mô hình này có thể sinh ra nhiều phương thức đúng hơn (tức là cao hơn 27.9% và 19.2% Pass@5 cấp độ phương thức) khi sinh mỗi phương thức trong các lần lặp riêng biệt so với việc sinh tất cả phương thức cùng một lúc. Do đó, các mô hình này có cơ hội cao hơn để sinh ra nhiều lớp đúng hơn nếu chúng có thể sinh ra nhiều phương thức đúng hơn với chiến lược tăng dần hoặc tổng hợp.

Một lý do tiềm năng cho quan sát trên có thể là hầu hết các mô hình (ngoại trừ GPT), thể hiện khả năng khá hạn chế trong việc sử dụng các ngữ cảnh đầu vào dài, do đó thấy thách thức hơn trong việc hiểu đầy đủ các tác vụ sinh mã được đưa ra toàn bộ khung lớp. Như được tiết lộ bởi công việc gần đây [47], LLMs thường trở nên kém hiệu quả đáng kể với việc tăng độ dài đầu vào; và cụ thể chúng có xu hướng sử dụng tốt hơn thông tin nằm ở đầu hoặc cuối đầu vào hơn là ở giữa đầu vào. Do đó, hầu hết các LLMs hiện tại hoạt động tốt hơn trong việc sinh một lớp từng phương thức một, vì các đầu vào tác vụ có trọng tâm nguyên tử hơn trong kịch bản sinh tăng dần hoặc tổng hợp như vậy; đối với các mô hình như GPT-3.5 và GPT-4 với khả năng hiểu tốt hơn về các hướng dẫn dài, việc cung cấp ngữ cảnh cấp độ lớp cùng một lúc thực sự có lợi cho chúng để nắm bắt và sử dụng đầy đủ các ràng buộc giữa mỗi phương thức, dẫn đến tính đúng đắn mã cấp độ lớp tốt hơn.

Chiến lược tăng dần so với chiến lược tổng hợp. Đối với hai chiến lược từng phương thức một (tức là chiến lược tăng dần và tổng hợp), chúng tôi thấy các mô hình được nghiên cứu thực sự có sở thích khác nhau về chúng. Cụ thể, so với cách sinh tổng hợp, các đầu vào bổ sung (thân phương thức được sinh ra trong các lần lặp trước) trong chiến lược tăng dần hữu ích cho một số mô hình như Instruct-CodeGen, InCoder, CodeGeeX và SantaCoder. Ngược lại, các thân phương thức được sinh ra trước đó có thể ảnh hưởng tiêu cực đến hiệu suất của các mô hình như Instruct-StarCoder và WizardCoder, dẫn đến tính đúng đắn cấp độ lớp thấp hơn trong sinh tăng dần. Ngoài khả năng hạn chế trong việc xử lý các đầu vào dài đã đề cập ở trên, một lý do tiềm năng khác cho sở thích của mô hình về cách sinh cá nhân hơn có thể là sinh tổng hợp phù hợp tốt hơn với các hướng dẫn tác vụ đơn giản và nguyên tử trong quá trình tinh chỉnh hướng dẫn.

Phát hiện 3: Sinh toàn bộ lớp cùng một lúc (tức là chiến lược toàn diện) là chiến lược sinh tốt nhất chỉ đối với GPT-4 và GPT-3.5. Đối với các mô hình khác, sinh từng phương thức một (tức là tăng dần và tổng hợp) hoạt động tốt hơn. Sự khác biệt như vậy có thể xuất phát từ khả năng hạn chế của chúng trong việc hiểu các hướng dẫn dài và sử dụng thông tin ở giữa.

5.3 RQ3: Sinh Phụ Thuộc
Phụ thuộc phương thức so với Phụ thuộc trường. Hình 7 trình bày các phụ thuộc trường DEP(F) trung bình và các phụ thuộc phương thức DEP(M) của mỗi mô hình với lấy mẫu nucleus. Do giới hạn không gian, chúng tôi chỉ trình bày kết quả tốt nhất trong ba chiến lược sinh. Dựa trên Hình 7, chúng tôi có thể thấy rằng tất cả các mô hình đều thể hiện tỷ lệ thành công cao hơn nhiều trong việc sinh mã phụ thuộc vào trường hơn việc sinh mã phụ thuộc vào các phương thức khác (tức là DEP(F) cao hơn DEP(M) trên tất cả các mô hình). Nói cách khác, có thể dễ dàng hơn nhiều đối với các mô hình để sinh mã truy cập trường hơn mã gọi phương thức. Ngoài ra, trong tất cả các mô hình, các mô hình GPT vẫn thể hiện sự vượt trội nhất quán trong việc sinh mã phụ thuộc, ví dụ, GPT-4 vượt trội đáng kể so với các LLMs khác ít nhất 12.6%/6.3% cải thiện trong DEP(F)/DEP(M).

Với quan sát của chúng tôi ở trên rằng việc sinh phụ thuộc phương thức thách thức hơn, chúng tôi điều tra thêm cách mỗi mô hình hoạt động trong việc sinh đúng mã gọi số lượng khác nhau của các phương thức khác. Hình 8 là một biểu đồ cột xếp chồng thể hiện tỷ lệ các phương thức được sinh đúng so với tất cả phương thức với số lượng đã cho (tức là 0, 1, 2) phụ thuộc phương thức (dựa trên giải pháp tiêu chuẩn). Dựa trên hình, chúng tôi có thể thấy rằng tất cả các mô hình hoạt động tốt nhất khi sinh các phương thức không gọi bất kỳ phương thức nào khác được khai báo trong lớp (thanh màu xanh trong hình). Ngoài ra, chúng tôi thấy rằng không có sự khác biệt rõ ràng khi hầu hết các mô hình sinh mã gọi một phương thức khác (thanh màu xanh lá) hoặc gọi hai phương thức khác (thanh màu vàng). Cụ thể, đối với tất cả các mô hình, tỷ lệ trung bình của mã được sinh đúng gọi một hoặc hai phương thức tương ứng là 27.7% và 27.6%.

Phát hiện 4: Dễ dàng hơn cho tất cả các mô hình để sinh mã truy cập trường hơn mã gọi phương thức. Ngoài ra, chúng tốt hơn trong việc sinh các phương thức độc lập không gọi bất kỳ phương thức nào khác.

5.4 RQ4: Phân Tích Trường Hợp Xấu
Chúng tôi phân tích thêm các lớp được sinh không đúng. Để đạt được điều này, chúng tôi tự động phân tích các nhật ký lỗi được tạo ra trong quá trình diễn giải và thực thi, và trình bày phân phối lỗi của tất cả các mô hình trong Hình 9. Cụ thể, chúng tôi thấy rằng hầu hết mã không đúng gặp phải AttributeError và TypeError, chỉ ra khả năng hạn chế của mô hình trong việc hiểu và thỏa mãn các ràng buộc cú pháp hoặc ngữ nghĩa trong ngữ cảnh mã. Ngoài ra, một số ít trường hợp gặp phải KeyError do các hoạt động sai lầm trên biến từ điển. Hình 10 thể hiện một ví dụ như vậy từ GPT-3.5, kết quả từ sự hiểu sai về phụ thuộc trường. Cụ thể, mô hình truy cập sai phần tử đầu tiên của danh sách trường BMI_std, là một từ điển với khóa "male". Cố gắng truy cập khóa self.sex như "female" trong từ điển này kích hoạt KeyError. Trường hợp này chỉ ra một trong những thách thức mà LLMs có thể gặp phải trong việc xử lý các phụ thuộc cấp độ lớp vốn có.

Phát hiện 5: Các lớp được sinh ra bởi LLMs gặp phải AttributeError và TypeError thường xuyên nhất. Ngoài ra, các mô hình có thể gặp khó khăn trong việc hiểu các ngữ cảnh phụ thuộc trong lớp.

6 ĐE DỌA ĐỐI VỚI TÍNH HỢP LỆ
Đe dọa trong xây dựng benchmark. Một đe dọa tiềm năng là rò rỉ dữ liệu giữa benchmark của chúng tôi và dữ liệu huấn luyện mô hình, do đó chúng tôi xây dựng thủ công benchmark ClassEval. Chúng tôi cũng có sự tham gia của nhiều người tham gia để giảm thiểu tính chủ quan và sai lầm trong sự tham gia thủ công. Một đe dọa khác nằm ở kích thước hạn chế và ngôn ngữ lập trình trong benchmark hiện tại của chúng tôi, không thể đảm bảo tính khái quát hóa của các phát hiện của chúng tôi, và chúng tôi dự định tiếp tục mở rộng benchmark của chúng tôi trong tương lai. Đe dọa trong nghiên cứu thực nghiệm. Để tránh triển khai mô hình có lỗi, chúng tôi áp dụng các phiên bản công khai theo hướng dẫn chính thức của mỗi mô hình. Một đe dọa khác nằm ở các prompt được sử dụng trong thí nghiệm của chúng tôi, có thể ảnh hưởng đến các phát hiện của chúng tôi. Để tránh đánh giá thấp các mô hình được nghiên cứu, chúng tôi thực hiện một nghiên cứu thí điểm trên một tập nhỏ các ứng viên prompt và chọn một với hiệu suất tốt nhất trên ba tác vụ mã hóa cấp độ lớp riêng biệt. Chúng tôi cũng báo cáo kết quả với giải mã tham lam, có tính xác định, để giảm thiểu tính ngẫu nhiên trong phản hồi mô hình.

7 CÔNG VIỆC LIÊN QUAN
Vì chúng tôi đã thảo luận hầu hết công việc liên quan về LLMs và các benchmark sinh mã hiện tại trong Phần 2, chúng tôi chủ yếu giới thiệu công việc liên quan về đánh giá LLM trong phần này. Đánh giá đa khía cạnh cho LLMs là rất quan trọng để hiểu khả năng của mô hình với bản chất hộp đen của LLMs. Đến nay, việc đánh giá cho LLMs đã bao gồm một loạt rộng [20], bao gồm không chỉ các tác vụ NLP truyền thống (ví dụ, phân tích cảm xúc [48], trả lời câu hỏi [49], và lý luận [5]) mà còn một số lĩnh vực hạ nguồn cụ thể (ví dụ, y học [50], agent [51], và hệ thống khuyến nghị [52]). Cụ thể trong lĩnh vực kỹ thuật phần mềm, đánh giá hiện tại tập trung chủ yếu vào các tác vụ sinh mã [15,16,31,53]. Nhiều LLMs mã (ví dụ, Codex [15] và PanGu-Coder2 [21]) được phát hành cùng với đánh giá nghiêm ngặt trên HumanEval để chứng minh khả năng của chúng trong sinh mã. Trong khi những nỗ lực trước đây này không đưa các kịch bản ngoài sinh mã cấp độ hàm vào tài khoản, công việc của chúng tôi lấp đầy khoảng trống này bằng cách xây dựng thủ công benchmark sinh mã cấp độ lớp đầu tiên để đánh giá LLM trên các tác vụ phát triển phần mềm phức tạp và thực tế hơn.

8 KẾT LUẬN
Công việc này thực hiện nỗ lực đầu tiên để đánh giá LLMs trên sinh mã cấp độ lớp. Chúng tôi đầu tiên xây dựng thủ công benchmark sinh mã cấp độ lớp đầu tiên ClassEval và thực hiện nghiên cứu đầu tiên về 11 LLMs tiên tiến nhất trên sinh mã cấp độ lớp. Chúng tôi thấy rằng tất cả LLMs hoạt động tệ hơn nhiều trên sinh mã cấp độ lớp so với cấp độ phương thức. Trong khi các mô hình GPT vẫn thống trị các LLMs khác trên sinh mã cấp độ lớp, thứ hạng hiệu suất mô hình trên sinh mã cấp độ phương thức không còn giữ nguyên trong sinh mã cấp độ lớp. Bên cạnh đó, hầu hết các mô hình (ngoại trừ mô hình GPT) hoạt động tốt hơn khi sinh lớp từng phương thức một; và chúng có khả năng hạn chế trong việc sinh mã phụ thuộc.

TÀI LIỆU THAM KHẢO
[1] V. Vikram, C. Lemieux, and R. Padhye, "Can large language models write good property-based tests?" 2023.
[2] S. Kang, J. Yoon, and S. Yoo, "Large language models are few-shot testers: Exploring llm-based general bug reproduction," trong 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), 2023, tr. 2312–2323.
[3] S. Kang, B. Chen, S. Yoo, and J.-G. Lou, "Explainable automated debugging via large language model-driven scientific debugging," 2023.
[4] OpenAI, "GPT-4 technical report," CoRR, tập abs/2303.08774, 2023. [Trực tuyến]. Có sẵn: https://doi.org/10.48550/arXiv.2303.08774
[5] N. Bian, X. Han, L. Sun, H. Lin, Y. Lu, and B. He, "Chatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models," CoRR, tập abs/2303.16421, 2023. [Trực tuyến]. Có sẵn: https://doi.org/10.48550/arXiv.2303.16421
[6] L. Zheng, W. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica, "Judging llm-as-a-judge with mt-bench and chatbot arena," CoRR, tập abs/2306.05685, 2023. [Trực tuyến]. Có sẵn: https://doi.org/10.48550/arXiv.2306.05685
[7] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang, "GLM: general language model pretraining with autoregressive blank infilling," trong Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, S. Muresan, P. Nakov, and A. Villavicencio, Eds. Association for Computational Linguistics, 2022, tr. 320–335. [Trực tuyến]. Có sẵn: https://doi.org/10.18653/v1/2022.acl-long.26
[8] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin, and D. Jiang, "Wizardcoder: Empowering code large language models with evol-instruct," CoRR, tập abs/2306.08568, 2023. [Trực tuyến]. Có sẵn: https://doi.org/10.48550/arXiv.2306.08568
[9] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, Q. Liu, E. Zheltonozhskii, T. Y. Zhuo, T. Wang, O. Dehaene, M. Davaadorj, J. Lamy-Poirier, J. Monteiro, O. Shliazhko, N. Gontier, N. Meade, A. Zebaze, M. Yee, L. K. Umapathi, J. Zhu, B. Lipkin, M. Oblokulov, Z. Wang, R. M. V, J. Stillerman, S. S. Patel, D. Abulkhanov, M. Zocca, M. Dey, Z. Zhang, N. Moustafa-Fahmy, U. Bhattacharyya, W. Yu, S. Singh, S. Luccioni, P. Villegas, M. Kunakov, F. Zhdanov, M. Romero, T. Lee, N. Timor, J. Ding, C. Schlesinger, H. Schoelkopf, J. Ebert, T. Dao, M. Mishra, A. Gu, J. Robinson, C. J. Anderson, B. Dolan-Gavitt, D. Contractor, S. Reddy, D. Fried, D. Bahdanau, Y. Jernite, C. M. Ferrandis, S. Hughes, T. Wolf, A. Guha, L. von Werra, and H. de Vries, "Starcoder: may the source be with you!" CoRR, tập abs/2305.06161, 2023. [Trực tuyến]. Có sẵn: https://doi.org/10.48550/arXiv.2305.06161
[10] (2023) Instruct-codegen. [Trực tuyến]. Có sẵn: https://huggingface.co/sahil2801/instruct-codegen-16B
[11] Q. Zheng, X. Xia, X. Zou, Y. Dong, S. Wang, Y. Xue, Z. Wang, L. Shen, A. Wang, Y. Li, T. Su, Z. Yang, and J. Tang, "Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x," CoRR, tập abs/2303.17568, 2023. [Trực tuyến]. Có sẵn: https://doi.org/10.48550/arXiv.2303.17568
[12] F. F. Xu, U. Alon, G. Neubig, and V. J. Hellendoorn, "A systematic evaluation of large language models of code," trong MAPS@PLDI 2022: 6th ACM SIGPLAN International Symposium on Machine Programming, San Diego, CA, USA, 13 June 2022, S. Chaudhuri and C. Sutton, Eds. ACM, 2022, tr. 1–10. [Trực tuyến]. Có sẵn: https://doi.org/10.1145/3520312.3534862
[13] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, S. Yih, L. Zettlemoyer, and M. Lewis, "Incoder: A generative model for code infilling and synthesis," trong The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [Trực tuyến]. Có sẵn: https://openreview.net/pdf?id=hQwb-lbM6EL
[14] L. B. Allal, R. Li, D. Kocetkov, C. Mou, C. Akiki, C. M. Ferrandis, N. Muennighoff, M. Mishra, A. Gu, M. Dey, L. K. Umapathi, C. J. Anderson, Y. Zi, J. Lamy-Poirier, H. Schoelkopf, S. Troshin, D. Abulkhanov, M. Romero, M. Lappert, F. D. Toni, B. G. del Río, Q. Liu, S. Bose, U. Bhattacharyya, T. Y. Zhuo, I. Yu, P. Villegas, M. Zocca, S. Mangrulkar, D. Lansky, H. Nguyen, D. Contractor, L. Villa, J. Li, D. Bahdanau, Y. Jernite, S. Hughes, D. Fried, A. Guha, H. de Vries, and L. von Werra, "Santacoder: don't reach for the stars!" CoRR, tập abs/2301.03988, 2023. [Trực tuyến]. Có sẵn: https://doi.org/10.48550/arXiv.2301.03988
[15] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba, "Evaluating large language models trained on code," CoRR, tập abs/2107.03374, 2021. [Trực tuyến]. Có sẵn: https://arxiv.org/abs/2107.03374
[16] J. Austin, A. Odena, M. I. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. J. Cai, M. Terry, Q. V. Le, and C. Sutton, "Program synthesis with large language models," CoRR, tập abs/2108.07732, 2021. [Trực tuyến]. Có sẵn: https://arxiv.org/abs/2108.07732
[17] H. Yu, B. Shen, D. Ran, J. Zhang, Q. Zhang, Y. Ma, G. Liang, Y. Li, T. Xie, and Q. Wang, "Codereval: A benchmark of pragmatic code generation with generative pre-trained models," CoRR, tập abs/2302.00288, 2023. [Trực tuyến]. Có sẵn: https://doi.org/10.48550/arXiv.2302.00288
[18] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba, "Evaluating large language models trained on code," CoRR, tập abs/2107.03374, 2021. [Trực tuyến]. Có sẵn: https://arxiv.org/abs/2107.03374
[19] Classeval github. [Trực tuyến]. Có sẵn: https://github.com/FudanSELab/ClassEval
[20] Y. Chang, X. Wang, J. Wang, Y. Wu, K. Zhu, H. Chen, L. Yang, X. Yi, C. Wang, Y. Wang, W. Ye, Y. Zhang, Y. Chang, P. S. Yu, Q. Yang, and X. Xie, "A survey on evaluation of large language models," CoRR, tập abs/2307.03109, 2023. [Trực tuyến]. Có sẵn: https://doi.org/10.48550/arXiv.2307.03109
[21] B. Shen, J. Zhang, T. Chen, D. Zan, B. Geng, A. Fu, M. Zeng, A. Yu, J. Ji, J. Zhao, Y. Guo, and Q. Wang, "Pangu-coder2: Boosting large language models for code with ranking feedback," 2023.
[22] F. Christopoulou, G. Lampouras, M. Gritta, G. Zhang, Y. Guo, Z. Li, Q. Zhang, M. Xiao, B. Shen, L. Li, H. Yu, L. Yan, P. Zhou, X. Wang, Y. Ma, I. Iacobacci, Y. Wang, G. Liang, J. Wei, X. Jiang, Q. Wang, and Q. Liu, "Pangu-coder: Program synthesis with function-level language modeling," CoRR, tập abs/2207.11280, 2022. [Trực tuyến]. Có sẵn: https://doi.org/10.48550/arXiv.2207.11280
[23] D. Zan, B. Chen, F. Zhang, D. Lu, B. Wu, B. Guan, W. Yongji, and J.-G. Lou, "Large language models meet NL2Code: A survey," trong Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Toronto, Canada: Association for Computational Linguistics, Jul. 2023, tr. 7443–7464. [Trực tuyến]. Có sẵn: https://aclanthology.org/2023.acl-long.411
[24] (2023) Instruct-starcoder. [Trực tuyến]. Có sẵn: https://huggingface.co/GeorgiaTechResearchInstitute/starcoder-gpteacher-code-instruct
[25] S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer, "Mapping language to code in programmatic context," trong Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, Eds. Association for Computational Linguistics, 2018, tr. 1643–1652. [Trực tuyến]. Có sẵn: https://doi.org/10.18653/v1/d18-1192
[26] P. Yin, B. Deng, E. Chen, B. Vasilescu, and G. Neubig, "Learning to mine aligned code and natural language pairs from stack overflow," trong Proceedings of the 15th International Conference on Mining Software Repositories, MSR 2018, Gothenburg, Sweden, May 28-29, 2018, A. Zaidman, Y. Kamei, and E. Hill, Eds. ACM, 2018, tr. 476–486. [Trực tuyến]. Có sẵn: https://doi.org/10.1145/3196398.3196408
[27] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He, D. Song, and J. Steinhardt, "Measuring coding challenge competence with APPS," trong Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, J. Vanschoren and S. Yeung, Eds., 2021. [Trực tuyến]. Có sẵn: https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/c24cd76e1ce41366a4bbe8a49b02a028-Abstract-round2.html
[28] B. Athiwaratkun, S. K. Gouda, Z. Wang, X. Li, Y. Tian, M. Tan, W. U. Ahmad, S. Wang, Q. Sun, M. Shang, S. K. Gonugondla, H. Ding, V. Kumar, N. Fulton, A. Farahani, S. Jain, R. Giaquinto, H. Qian, M. K. Ramanathan, and R. Nallapati, "Multi-lingual evaluation of code generation models," trong The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [Trực tuyến]. Có sẵn: https://openreview.net/pdf?id=Bo7eeXm6An8
[29] Y. Li, D. H. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. D. Lago, T. Hubert, P. Choy, C. de Masson d'Autume, I. Babuschkin, X. Chen, P. Huang, J. Welbl, S. Gowal, A. Cherepanov, J. Molloy, D. J. Mankowitz, E. S. Robson, P. Kohli, N. de Freitas, K. Kavukcuoglu, and O. Vinyals, "Competition-level code generation with alphacode," CoRR, tập abs/2203.07814, 2022. [Trực tuyến]. Có sẵn: https://doi.org/10.48550/arXiv.2203.07814
[30] Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, S. W. Yih, D. Fried, S. I. Wang, and T. Yu, "DS-1000: A natural and reliable benchmark for data science code generation," CoRR, tập abs/2211.11501, 2022. [Trực tuyến]. Có sẵn: https://doi.org/10.48550/arXiv.2211.11501
[31] J. Liu, C. S. Xia, Y. Wang, and L. Zhang, "Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation," CoRR, tập abs/2305.01210, 2023. [Trực tuyến]. Có sẵn: https://doi.org/10.48550/arXiv.2305.01210
[32] Code generation datasets in huggingface. [Trực tuyến]. Có sẵn: https://hf.co/datasets?other=code-generation
[33] B. Meyer, "Applying "design by contract"," Computer, tập 25, số 10, tr. 40–51, 1992. [Trực tuyến]. Có sẵn: https://doi.org/10.1109/2.161279
[34] T. Bhat and N. Nagappan, "Evaluating the efficacy of test-driven development: industrial case studies," trong 2006 International Symposium on Empirical Software Engineering (ISESE 2006), September 21-22, 2006, Rio de Janeiro, Brazil, G. H. Travassos, J. C. Maldonado, and C. Wohlin, Eds. ACM, 2006, tr. 356–363. [Trực tuyến]. Có sẵn: https://doi.org/10.1145/1159733.1159787
[35] K. Srinath, "Python–the fastest growing programming language," International Research Journal of Engineering and Technology, tập 4, số 12, tr. 354–357, 2017.
[36] Pypi. [Trực tuyến]. Có sẵn: https://pypi.org/search
[37] Unittest framework. [Trực tuyến]. Có sẵn: https://pypi.org/project/unitest
[38] Coverage library. [Trực tuyến]. Có sẵn: https://pypi.org/project/coverage
[39] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong, "A conversational paradigm for program synthesis," CoRR, tập abs/2203.13474, 2022. [Trực tuyến]. Có sẵn: https://doi.org/10.48550/arXiv.2203.13474
[40] (2021) Dense-6.7b. [Trực tuyến]. Có sẵn: https://huggingface.co/KoboldAI/fairseq-dense-6.7B-Shinen
[41] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., "Language models are unsupervised multitask learners," OpenAI blog, tập 1, số 8, tr. 9, 2019.
[42] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample, "Llama: Open and efficient foundation language models," CoRR, tập abs/2302.13971, 2023. [Trực tuyến]. Có sẵn: https://doi.org/10.48550/arXiv.2302.13971
[43] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia, W. L. Tam, Z. Ma, Y. Xue, J. Zhai, W. Chen, Z. Liu, P. Zhang, Y. Dong, and J. Tang, "GLM-130B: an open bilingual pre-trained model," trong The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [Trực tuyến]. Có sẵn: https://openreview.net/pdf?id=-Aw0rrrPUF
[44] Openai api interface. [Trực tuyến]. Có sẵn: https://platform.openai.com/docs/api-reference
[45] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi, "The curious case of neural text degeneration," trong 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. [Trực tuyến]. Có sẵn: https://openreview.net/forum?id=rygGQyrFvH
[46] S. Chen, R. Varma, A. Sandryhaila, and J. Kovacevic, "Discrete signal processing on graphs: Sampling theory," IEEE Trans. Signal Process., tập 63, số 24, tr. 6510–6523, 2015. [Trực tuyến]. Có sẵn: https://doi.org/10.1109/TSP.2015.2469645
[47] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang, "Lost in the middle: How language models use long contexts," CoRR, tập abs/2307.03172, 2023. [Trực tuyến]. Có sẵn: https://doi.org/10.48550/arXiv.2307.03172
[48] Y. Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su, B. Wilie, H. Lovenia, Z. Ji, T. Yu, W. Chung, Q. V. Do, Y. Xu, and P. Fung, "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity," CoRR, tập abs/2302.04023, 2023. [Trực tuyến]. Có sẵn: https://doi.org/10.48550/arXiv.2302.04023
[49] Y. Bai, J. Ying, Y. Cao, X. Lv, Y. He, X. Wang, J. Yu, K. Zeng, Y. Xiao, H. Lyu, J. Zhang, J. Li, and L. Hou, "Benchmarking foundation models with language-model-as-an-examiner," CoRR, tập abs/2306.04181, 2023. [Trực tuyến]. Có sẵn: https://doi.org/10.48550/arXiv.2306.04181
[50] J. Chervenak, H. Lieman, M. Blanco-Breindel, and S. Jindal, "The promise and peril of using a large language model to obtain clinical information: Chatgpt performs strongly as a fertility counseling tool with limitations," Fertility and Sterility, 2023.
[51] S. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv, L. Cui, O. K. Mohammed, B. Patra, Q. Liu, K. Aggarwal, Z. Chi, J. Bjorck, V. Chaudhary, S. Som, X. Song, and F. Wei, "Language is not all you need: Aligning perception with language models," CoRR, tập abs/2302.14045, 2023. [Trực tuyến]. Có sẵn: https://doi.org/10.48550/arXiv.2302.14045
[52] W. Fan, Z. Zhao, J. Li, Y. Liu, X. Mei, Y. Wang, J. Tang, and Q. Li, "Recommender systems in the era of large language models (llms)," CoRR, tập abs/2307.02046, 2023. [Trực tuyến]. Có sẵn: https://doi.org/10.48550/arXiv.2307.02046
[53] J. Li, G. Li, Y. Li, and Z. Jin, "Enabling programming thinking in large language models toward code generation," CoRR, tập abs/2305.06599, 2023. [Trực tuyến]. Có sẵn: https://doi.org/10.48550/arXiv.2305.06599
