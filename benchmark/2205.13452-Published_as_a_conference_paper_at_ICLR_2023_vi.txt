# 2205.13452.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/benchmark/2205.13452.pdf
# Kích thước tệp: 3845384 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023
ĐÁNH GIÁ LIÊN TỤC CHO HỌC TẬP SUỐT ĐỜI:
XÁC ĐỊNH KHOẢNG CÁCH ỔN ĐỊNH
Matthias De Lange, Gido M. van de Ven & Tinne Tuytelaars
KU Leuven
TÓM TẮT
Các phân phối tạo dữ liệu phụ thuộc thời gian đã được chứng minh là khó khăn cho việc huấn luyện dựa trên gradient của mạng nơ-ron, vì các cập nhật tham lam dẫn đến việc quên thảm khốc kiến thức đã học trước đó. Mặc dù có tiến bộ trong lĩnh vực học tập liên tục để khắc phục việc quên này, chúng tôi cho thấy một tập hợp các phương pháp tiên tiến phổ biến vẫn gặp phải việc quên đáng kể khi bắt đầu học các nhiệm vụ mới, ngoại trừ việc quên này là tạm thời và được theo sau bởi một giai đoạn phục hồi hiệu suất. Chúng tôi gọi hiện tượng hấp dẫn nhưng có thể có vấn đề này là khoảng cách ổn định. Khoảng cách ổn định có thể đã không được chú ý do thực tiễn tiêu chuẩn trong lĩnh vực đánh giá các mô hình học tập liên tục chỉ sau mỗi nhiệm vụ. Thay vào đó, chúng tôi thiết lập một khung cho đánh giá liên tục sử dụng đánh giá mỗi lần lặp và chúng tôi định nghĩa một tập hợp các chỉ số mới để định lượng hiệu suất trường hợp xấu nhất. Thực nghiệm, chúng tôi cho thấy rằng các phương pháp phát lại kinh nghiệm, phát lại dựa trên ràng buộc, chưng cất kiến thức và chính quy hóa tham số đều dễ bị khoảng cách ổn định; và khoảng cách ổn định có thể được quan sát trong các điểm chuẩn học tập gia tăng theo lớp, nhiệm vụ và miền. Ngoài ra, một thí nghiệm được kiểm soát cho thấy khoảng cách ổn định tăng khi các nhiệm vụ khác biệt hơn. Cuối cùng, bằng cách tách rời các gradient thành các thành phần tính dẻo và ổn định, chúng tôi đề xuất một giải thích khái niệm cho khoảng cách ổn định.

1 GIỚI THIỆU
Sự hội tụ nhanh trong tối ưu hóa dựa trên gradient đã mang lại nhiều thành công với các mạng nơ-ron có tham số hóa cao (Krizhevsky et al., 2012; Mnih et al., 2013; Devlin et al., 2018). Trong mô hình huấn luyện tiêu chuẩn, các kết quả này có điều kiện là có một phân phối tạo dữ liệu tĩnh. Tuy nhiên, khi tính không ổn định được giới thiệu bằng một phân phối tạo dữ liệu biến thiên theo thời gian, các cập nhật dựa trên gradient một cách tham lam ghi đè các tham số của giải pháp trước đó. Điều này dẫn đến việc quên thảm khốc (French, 1999) và là một trong những trở ngại chính trong học tập liên tục hoặc suốt đời.

Học tập liên tục thường được trình bày như mong muốn học theo cách con người học, tích lũy thay vì thay thế kiến thức. Để đạt được điều này, nhiều công trình đã tập trung vào việc giảm thiểu việc quên thảm khốc với những kết quả hứa hẹn, cho thấy hành vi học tập như vậy có thể khả thi cho các mạng nơ-ron nhân tạo (De Lange et al., 2021; Parisi et al., 2019). Ngược lại, công trình này một cách đáng ngạc nhiên xác định việc quên đáng kể vẫn còn tồn tại trong quá trình chuyển đổi nhiệm vụ đối với các phương pháp tiên tiến tiêu chuẩn dựa trên phát lại kinh nghiệm, phát lại dựa trên ràng buộc, chưng cất kiến thức và chính quy hóa tham số, mặc dù việc quên được quan sát là tạm thời và được theo sau bởi một giai đoạn phục hồi. Chúng tôi gọi hiện tượng này là khoảng cách ổn định.

Các đóng góp trong công trình này dọc theo ba dòng chính, với mã nguồn có sẵn công khai.¹ Đầu tiên, chúng tôi định nghĩa một khung cho đánh giá liên tục để đánh giá người học sau mỗi lần cập nhật. Khung này được thiết kế để cho phép giám sát hiệu suất trường hợp xấu nhất của người học liên tục từ góc độ của các tác nhân thu thập kiến thức trong suốt vòng đời của họ. Để làm điều này, chúng tôi đề xuất các chỉ số có nguyên tắc mới như độ chính xác tối thiểu và trường hợp xấu nhất (min-ACC và WC-ACC).

Thứ hai, chúng tôi tiến hành một nghiên cứu thực nghiệm với khung đánh giá liên tục, dẫn đến việc xác định khoảng cách ổn định, như được minh họa trong Hình 1, trong nhiều phương pháp và thiết lập khác nhau. Một nghiên cứu loại bỏ về tần suất đánh giá chỉ ra rằng đánh giá liên tục là một phương tiện cần thiết để phát hiện khoảng cách ổn định, giải thích tại sao hiện tượng này vẫn chưa được xác định cho đến nay. Ngoài ra, chúng tôi thấy rằng khoảng cách ổn định bị ảnh hưởng đáng kể bởi mức độ tương tự của các nhiệm vụ liên tiếp trong luồng dữ liệu.

Thứ ba, chúng tôi đề xuất một phân tích khái niệm để giúp giải thích khoảng cách ổn định, bằng cách tách rời các gradient dựa trên tính dẻo và ổn định. Chúng tôi thực hiện điều này cho một số phương pháp: Experience Replay (Chaudhry et al., 2019b), GEM (Lopez-Paz & Ranzato, 2017), EWC (Kirkpatrick et al., 2017), SI (Zenke et al., 2017), và LwF (Li & Hoiem, 2017). Các thí nghiệm bổ sung với phân tích gradient cung cấp bằng chứng hỗ trợ cho giả thuyết.

Các tác động của khoảng cách ổn định. (i) Đánh giá liên tục là quan trọng, đặc biệt đối với các ứng dụng quan trọng về an toàn, vì các phương pháp học tập liên tục đại diện gặp khó khăn trong việc duy trì hiệu suất mạnh mẽ trong quá trình học. (ii) Có nguy cơ rằng các thay đổi phân phối đột ngột có thể bị khai thác bởi những kẻ tấn công có thể kiểm soát luồng dữ liệu để giảm hiệu suất một cách tạm thời nhưng đáng kể. (iii) Bên cạnh những tác động thực tế này, bản thân khoảng cách ổn định là một hiện tượng hấp dẫn về mặt khoa học mà truyền cảm hứng cho nghiên cứu tiếp theo. Ví dụ, khoảng cách ổn định gợi ý rằng các phương pháp học tập liên tục hiện tại có thể thể hiện động lực học khác biệt một cách cơ bản so với não bộ con người.

Hình 1: Khoảng cách ổn định: việc quên đáng kể theo sau bởi phục hồi khi học các nhiệm vụ mới trong các phương pháp học tập liên tục tiên tiến. Đánh giá liên tục tại mỗi lần lặp (đường cong màu cam) tiết lộ khoảng cách ổn định, vẫn không được xác định với đánh giá hướng nhiệm vụ tiêu chuẩn (kim cương đỏ). Được hiển thị là độ chính xác trên nhiệm vụ đầu tiên, khi một mạng sử dụng Experience Replay học tuần tự năm nhiệm vụ đầu tiên của Split-MiniImagenet gia tăng theo lớp. Chi tiết hơn trong Hình 2.

[Biểu đồ hiển thị độ chính xác theo số lần lặp với đánh giá liên tục và đánh giá dựa trên nhiệm vụ]

2 CƠ SỞ VỀ HỌC TẬP LIÊN TỤC

Mục tiêu phân loại học tập liên tục hoặc suốt đời là học một hàm f: X → Y với các tham số θ, ánh xạ không gian đầu vào X đến không gian đầu ra Y, từ một luồng dữ liệu không ổn định S = {(x,y)₀, (x,y)₁, ..., (x,y)ₙ}, trong đó bộ dữ liệu (x ∈ X, y ∈ Y)ₜ được lấy mẫu từ một phân phối tạo dữ liệu D_t phụ thuộc vào thời gian t. Trong khi học máy tiêu chuẩn giả định một phân phối tạo dữ liệu tĩnh, học tập liên tục giới thiệu sự phụ thuộc vào biến thời gian t. Sự phụ thuộc thời gian này giới thiệu một sự đánh đổi giữa thích ứng với phân phối tạo dữ liệu hiện tại và duy trì kiến thức thu được từ những phân phối trước đó, còn được gọi là sự đánh đổi ổn định-tính dẻo (Grossberg, 1982).

Nhiệm vụ. Luồng dữ liệu thường được giả định là được chia thành các phân phối cục bộ ổn định, được gọi là nhiệm vụ. Chúng tôi giới thiệu định danh nhiệm vụ rời rạc k để chỉ nhiệm vụ thứ k T_k với phân phối tạo dữ liệu cục bộ ổn định D_k. Ngoài ra, biến thời gian t được giả định là rời rạc, chỉ số lần lặp tổng thể trong luồng, và t|T_k| chỉ số lần lặp tổng thể tại cuối nhiệm vụ T_k.

Học tập liên tục. Trong giai đoạn huấn luyện, người học liên tục cập nhật f dựa trên các bộ dữ liệu mới (x,y)_t từ luồng dữ liệu (De Lange & Tuytelaars, 2021). Tối ưu hóa tuân theo tối thiểu hóa rủi ro thực nghiệm trên các tập huấn luyện đã quan sát Ẽ_D_k, vì người học không có quyền truy cập trực tiếp vào các phân phối tạo dữ liệu D_k. Mục tiêu log-likelihood âm mà chúng ta lý tưởng muốn tối ưu hóa khi học nhiệm vụ T_k là:

min_θ L_k = Σ_{n=1}^k E_{(x,y)∼Ẽ_D_n} [-y^T log f(x; θ)]     (1)

Một thách thức chính cho học tập liên tục là ước tính mục tiêu này chỉ có dữ liệu huấn luyện của nhiệm vụ hiện tại Ẽ_D_k có sẵn, trong khi sử dụng tài nguyên tính toán và bộ nhớ bổ sung hạn chế.

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

3 MỘT KHUNG CHO ĐÁNH GIÁ LIÊN TỤC

Người học liên tục cập nhật f có một đối tác, người đánh giá liên tục, theo dõi hiệu suất của f theo thời gian. Không chỉ dữ liệu được quan sát bởi người học trong luồng dữ liệu S có thể thay đổi theo thời gian, mà còn cả phân phối mà người học được đánh giá. Điều này được phản ánh trong sự phụ thuộc thời gian của luồng dữ liệu của người đánh giá S_E (De Lange & Tuytelaars, 2021). Người đánh giá được cấu hình bởi ba yếu tố chính: i) chu kỳ đánh giá ρ_eval xác định tần suất f được đánh giá; ii) quy trình xây dựng S_E; iii) tập hợp các chỉ số đánh giá.

Chu kỳ đánh giá. Cách tiếp cận điển hình để đánh giá trong học tập liên tục là đánh giá dựa trên nhiệm vụ: hiệu suất của mô hình f chỉ được đánh giá sau khi hoàn thành việc huấn luyện trên một nhiệm vụ mới. Như một giải pháp thay thế, ở đây chúng tôi định nghĩa đánh giá liên tục là đánh giá hiệu suất của f mỗi ρ_eval lần lặp huấn luyện. Để đánh giá chi tiết nhất, ρ_eval được đặt bằng 1, thiết lập được sử dụng trong phần sau trừ khi được đề cập khác. Hiệu ứng của việc tăng ρ_eval có thể được quan sát trong Hình 2 và được nghiên cứu chi tiết hơn trong Phụ lục B.

Định nghĩa luồng đánh giá. Hành vi mong muốn của mô hình có thể được tách rời trong hiệu suất trên một tập hợp các nhiệm vụ đánh giá. Chúng tôi định nghĩa một tập hợp N nhiệm vụ đánh giá T_E = {E₀, ..., E_N}, trong đó mỗi nhiệm vụ đánh giá E_i có một tập dữ liệu Ẽ_D_E,i được lấy mẫu từ phân phối tạo dữ liệu D_E,i. Tập hợp các nhiệm vụ đánh giá có thể được mở rộng tại bất kỳ thời điểm nào: T_E ← T_E ∪ {E_{N+1}}. Theo tài liệu (van de Ven et al., 2020), chúng tôi giả định một nhiệm vụ đánh giá mới E_k được thêm vào mỗi nhiệm vụ huấn luyện T_k được gặp phải trong S. Thông thường, các nhiệm vụ đánh giá được chọn để phù hợp với các nhiệm vụ trong luồng huấn luyện S, trong đó dữ liệu đánh giá Ẽ_D_E,k có thể bao gồm một tập con của dữ liệu huấn luyện của nhiệm vụ đã quan sát Ẽ_D_k, hoặc phổ biến hơn, bao gồm một tập đánh giá riêng biệt để kiểm tra hiệu suất tổng quát hóa với Ẽ_D_k ∩ Ẽ_D_E,k = ∅. Khi tập hợp các nhiệm vụ đánh giá mở rộng theo luồng huấn luyện, chúng tôi cung cấp các giảm nhẹ liên quan đến tính khả thi tính toán trong Phụ lục B.

Các chỉ số đánh giá liên tục. Yếu tố thứ ba của người đánh giá là tập hợp các chỉ số đánh giá được sử dụng để đánh giá hiệu suất của người học. Cho đến nay, các chỉ số đã được định nghĩa chủ yếu giả định đánh giá tại các chuyển đổi nhiệm vụ và tập trung vào hiệu suất cuối cùng của người học. Chúng tôi ủng hộ rằng hiệu suất trường hợp xấu nhất liên tục là quan trọng đối với người học liên tục trong thế giới thực. Do đó, chúng tôi đề xuất các chỉ số mới để định lượng hiệu suất trường hợp xấu nhất (WC-ACC, min-ACC, WF_w) và các chỉ số cũng có thể áp dụng cho các luồng dữ liệu không biết nhiệm vụ (WF_w, WP_w). Chúng tôi tập trung vào các chỉ số phân loại, với độ chính xác (tỷ lệ phần trăm các thể hiện được phân loại đúng) được coi là chỉ số hiệu suất chính. Sử dụng f_t để chỉ phiên bản của mô hình sau lần lặp huấn luyện tổng thể thứ t, độ chính xác của nhiệm vụ đánh giá E_k tại lần lặp này được ký hiệu là A(E_k, f_t). Chúng tôi cung cấp một phân loại các chỉ số dựa trên tính dẻo và ổn định như sau.

3.1 CÁC CHỈ SỐ DỰA TRÊN ỔN ĐỊNH

Để đo lường tính ổn định của người học, chúng tôi nhằm định lượng mức độ bảo tồn kiến thức từ các nhiệm vụ đã quan sát trước đó T_{<k} trong khi học nhiệm vụ mới T_k. Quên trung bình (FORG) (Chaudhry et al., 2018) tính trung bình sự khác biệt độ chính xác cho mô hình gần đây nhất f_{t|T_k|} so với f_{t|T_i|} ngay sau khi học T_i với nhiệm vụ đánh giá E_i, và được định nghĩa là 1/(k-1) ∑_{i}^{k-1} [A(E_i, f_{t|T_i|}) - A(E_i, f_{t|T_k|})]. Việc quên lớn chỉ ra hiện tượng quên thảm khốc, trong khi việc quên âm chỉ ra việc chuyển giao kiến thức từ nhiệm vụ mới đến các nhiệm vụ trước đó.

Đối với hiệu suất trường hợp xấu nhất, việc có một biện pháp tuyệt đối về hiệu suất nhiệm vụ trước đó là mong muốn. Chúng tôi định nghĩa độ chính xác tối thiểu trung bình (min-ACC) tại nhiệm vụ huấn luyện hiện tại T_k là độ chính xác tối thiểu tuyệt đối trung bình trên các nhiệm vụ đánh giá trước đó E_i sau khi chúng đã được học:

min-ACC_{T_k} = 1/(k-1) ∑_{i}^{k-1} min_n A(E_i, f_n), ∀t|T_i| < n ≤ t     (2)

trong đó số lần lặp n dao động từ sau khi nhiệm vụ được học cho đến lần lặp hiện tại t. Điều này đưa ra một biện pháp trường hợp xấu nhất về mức độ bảo tồn kiến thức trong các nhiệm vụ đã quan sát trước đó T_{<k}. Trong phần sau, chúng tôi báo cáo min-ACC cho nhiệm vụ cuối cùng, bỏ qua sự phụ thuộc vào T_k để ngắn gọn.

Hơn nữa, chúng tôi giới thiệu một chỉ số ổn định tổng quát hơn không giả định một luồng dữ liệu dựa trên nhiệm vụ, và do đó cũng có thể áp dụng cho học tập gia tăng dữ liệu. Chúng tôi định nghĩa Quên Cửa sổ (WF_w) dựa trên một cửa sổ w đánh giá độ chính xác liên tiếp trung bình trên tập đánh giá T_E. Đối với một nhiệm vụ đánh giá đơn lẻ E_i, sự giảm độ chính xác tối đa trong cửa sổ Δ_{w,t,E_i}^- và Quên Cửa sổ cụ thể cho nhiệm vụ WF_{w,t,E_i}^- được định nghĩa tại lần lặp hiện tại t là:

Δ_{w,t,E_i}^- = max_{m<n} (A(E_i, f_m) - A(E_i, f_n)), ∀m,n ∈ [t-w+1, t]     (3)

WF_{w,t,E_i}^- = max_n Δ_{w,n,E_i}^-, ∀n ≤ t     (4)

Việc tính trung bình chỉ số trên tất cả N nhiệm vụ đánh giá dẫn đến một chỉ số duy nhất WF_w^t = N^{-1} ∑_i^N WF_{w,t,E_i}^-. Vì nó xác định sự giảm hiệu suất tối đa được quan sát trong cửa sổ, nó được coi là một chỉ số trường hợp xấu nhất. Các người đánh giá có thể chi trả độ phức tạp không gian tuyến tính có thể xem xét toàn bộ lịch sử tại lần lặp t với WF_∞^t. Để tính đến cả việc quên nhanh và việc quên ở quy mô lớn hơn với bộ nhớ không đổi, chúng tôi tạo ra WF_10 và WF_100.

3.2 CÁC CHỈ SỐ DỰA TRÊN TÍNH DẺO

Tính dẻo trong công trình này đề cập đến khả năng của người học để thu thập kiến thức mới từ phân phối tạo dữ liệu hiện tại D_k. Độ chính xác nhiệm vụ hiện tại đo lường tính dẻo là A(E_k, f_t) với t ∈ [t|T_{k-1}|, t|T_k|]. Các chỉ số khác được đề xuất trong tài liệu là biện pháp few-shot Diện tích Đường cong Học tập (Chaudhry et al., 2019a) và Chuyển giao Tiến zero-shot (Lopez-Paz & Ranzato, 2017). Các chỉ số trước đó phụ thuộc vào nhiệm vụ học tập hiện tại T_k và do đó không áp dụng trực tiếp cho các luồng dữ liệu không biết nhiệm vụ. Như đối tác cho WF_w, chúng tôi giới thiệu Tính dẻo Cửa sổ (WP_w) có thể áp dụng tổng quát hơn, được định nghĩa trên tất cả N nhiệm vụ đánh giá bằng sự gia tăng độ chính xác tối đa Δ_{w,t,E_i}^+ trong một cửa sổ có kích thước w. Phụ lục định nghĩa Δ_{w,t,E_i}^+ (Phương trình 10) như Δ_{w,t,E_i}^- nhưng với ràng buộc m > n.

WP_w^t = 1/N ∑_i^N max_n Δ_{w,n,E_i}^+, ∀n ≤ t     (5)

3.3 CÁC CHỈ SỐ DỰA TRÊN SỰ ĐÁNH ĐỔI ỔN ĐỊNH-TÍNH DẺO

Mục tiêu chính trong học tập liên tục là tìm một sự cân bằng giữa việc học các nhiệm vụ mới T_k và duy trì kiến thức từ các nhiệm vụ trước đó T_{<k}, thường được gọi là sự đánh đổi ổn định-tính dẻo (Grossberg, 1982). Các chỉ số đánh đổi ổn định-tính dẻo cung cấp một chỉ số duy nhất để định lượng sự cân bằng này. Chỉ số tiêu chuẩn cho học tập liên tục là Độ chính xác Trung bình (ACC) mà sau khi học nhiệm vụ T_k tính trung bình hiệu suất trên tất cả các nhiệm vụ đánh giá: ACC_{T_k} = 1/k ∑_i^k A(E_i, f_{t|T_k|}). Nó cung cấp một biện pháp đánh đổi bằng cách bao gồm độ chính xác của cả nhiệm vụ đánh giá hiện tại E_k (tính dẻo) và tất cả các nhiệm vụ đánh giá trước đó E_{<k} (ổn định). ACC được đo chỉ tại mô hình cuối cùng f_{t|T_k|} và bỏ qua hiệu suất giữa các chuyển đổi nhiệm vụ. Do đó, chúng tôi đề xuất Độ chính xác Trường hợp Xấu nhất (WC-ACC) như sự đánh đổi giữa độ chính xác tại lần lặp t của nhiệm vụ hiện tại T_k và chỉ số trường hợp xấu nhất min-ACC_{T_k} (xem Phương trình 2) cho các nhiệm vụ trước đó:

WC-ACC_t = 1/k A(E_k, f_t) + (1 - 1/k) min-ACC_{T_k}     (6)

Chỉ số này định lượng mỗi lần lặp độ chính xác tối thiểu mà các nhiệm vụ trước đó duy trì sau khi được học, và độ chính xác của nhiệm vụ hiện tại. WC-ACC đưa ra một đảm bảo cận dưới trên ACC được thiết lập qua các lần lặp. Đánh giá sau khi học nhiệm vụ T_k, chúng ta kết luận cận dưới sau:

WC-ACC_{t|T_k|} ≤ ACC_{T_k}     (7)

4 XÁC ĐỊNH KHOẢNG CÁCH ỔN ĐỊNH VỚI ĐÁNH GIÁ LIÊN TỤC

Chúng tôi bắt đầu bằng cách tiến hành một nghiên cứu thực nghiệm giới hạn trong Experience Replay (ER) (Chaudhry et al., 2019b), vì nó đã được chỉ ra là vượt trội hơn các phương pháp dựa trên chính quy hóa, đặc biệt đối với học tập gia tăng theo lớp và miền (van de Ven et al., 2022). ER lưu trữ một tập con nhỏ các mẫu trong một bộ đệm bộ nhớ, được xem lại sau này bằng cách lấy mẫu các mini-batch một phần là dữ liệu mới, một phần là dữ liệu bộ nhớ. Do tính đơn giản và hiệu quả chống lại việc quên thảm khốc, nó đã được áp dụng rộng rãi trong tài liệu (Rebuffi et al., 2017; Chaudhry et al., 2019b; De Lange & Tuytelaars, 2021; Chaudhry et al., 2018).

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

Hình 2: Độ chính xác trung bình (trung bình ± SD trên 5 seeds) trên nhiệm vụ đầu tiên khi sử dụng ER. Các thí nghiệm là gia tăng theo lớp (a-c) hoặc gia tăng theo miền (d). Chu kỳ đánh giá dao động từ đánh giá tiêu chuẩn tại các chuyển đổi nhiệm vụ (kim cương đỏ) đến đánh giá liên tục mỗi lần lặp (ρ_eval = 1). Đánh giá mỗi lần lặp tiết lộ những sụt giảm sắc nét, tạm thời trong hiệu suất khi học các nhiệm vụ mới: khoảng cách ổn định. Các đường thẳng đứng chỉ ra các chuyển đổi nhiệm vụ; các đường ngang là min-ACC trung bình trên các seeds.

[Hình 2 hiển thị 5 biểu đồ (a-e) cho các dataset khác nhau với độ chính xác theo số lần lặp]

Bộ dữ liệu. Đối với các thí nghiệm về học tập gia tăng theo lớp, chúng tôi sử dụng ba bộ dữ liệu tiêu chuẩn: MNIST (LeCun & Cortes, 2010) bao gồm các chữ số viết tay màu xám, CIFAR10 (Krizhevsky et al., 2009) chứa hình ảnh từ một loạt các phương tiện và động vật, và MiniImagenet (Vinyals et al., 2016) là một tập con của Imagenet (Russakovsky et al., 2015). Split-MNIST, Split-CIFAR10, và Split-MiniImagenet được định nghĩa bằng cách chia dữ liệu thành 5, 5, và 20 nhiệm vụ dựa trên 10, 10, và 100 lớp. Đối với học tập gia tăng theo miền, chúng tôi xem xét những thay đổi miền mạnh mẽ trong Mini-DomainNet (Zhou et al., 2021), một tập con thu nhỏ của 126 lớp của DomainNet (Peng et al., 2019) với hơn 90k hình ảnh, xem xét các miền: clipart, painting, real, sketch.

Thiết lập. Chúng tôi sử dụng đánh giá liên tục với chu kỳ đánh giá trong phạm vi ρ_eval ∈ {1, 10, 10², 10³} và kích thước tập con 1k mỗi nhiệm vụ đánh giá, dựa trên phân tích tính khả thi của chúng tôi trong Phụ lục B. Để tham chiếu với tài liệu, các chỉ số dựa trên chuyển đổi nhiệm vụ ACC và FORG tuân theo đánh giá tiêu chuẩn với toàn bộ tập kiểm tra. Split-MNIST sử dụng MLP với 2 lớp ẩn của 400 đơn vị. Split-CIFAR10, Split-MiniImagenet và Mini-DomainNet sử dụng phiên bản mỏng của Resnet18 (Lopez-Paz & Ranzato, 2017). Tối ưu hóa SGD được sử dụng với momentum 0.9. Để đảm bảo phân tích trường hợp xấu nhất của chúng tôi áp dụng cho cấu hình trường hợp tốt nhất cho ER, chúng tôi chạy gridsearch trên các siêu tham số khác nhau và chọn mục có chỉ số đánh đổi ổn định-tính dẻo ACC cao nhất trên dữ liệu đánh giá được giữ lại (Lopez-Paz & Ranzato, 2017). Chi tiết cho tất cả các thí nghiệm có thể được tìm thấy trong Phụ lục C và mã có sẵn tại đây: https://github.com/mattdl/ContinualEvaluation.

Phân tích định tính với đánh giá liên tục. Hình 2 minh họa các đường cong độ chính xác nhiệm vụ đầu tiên cho ER trên 4 điểm chuẩn. Các điểm đánh dấu đỏ tại các chuyển đổi nhiệm vụ chỉ ra sơ đồ đánh giá tiêu chuẩn trong học tập liên tục. Chúng tôi thấy rằng đánh giá liên tục (ρ_eval = 1) tiết lộ việc quên tạm thời đáng kể giữa các chuyển đổi nhiệm vụ, cả cho các điểm chuẩn gia tăng theo lớp và theo miền. Sau những sụt giảm hiệu suất đáng kể, phục hồi một phần theo sau, làm cho các chỉ số dựa trên chuyển đổi nhiệm vụ như ACC và FORG trở thành những ước tính hiệu suất trường hợp xấu nhất kém. Chúng tôi gọi hiện tượng quên tạm thời, đáng kể này khi học các nhiệm vụ mới là khoảng cách ổn định.

Định lượng hiệu suất trường hợp xấu nhất. Phân tích định tính cho phép quan sát khoảng cách ổn định từ các đường cong độ chính xác theo thời gian. Tuy nhiên, phân tích hiệu suất cũng yêu cầu một biện pháp định lượng cho hiệu suất trường hợp xấu nhất. Đối với Split-MiniImagenet gia tăng theo lớp, chúng tôi báo cáo kết quả cho các chỉ số đánh giá liên tục mới của chúng tôi trong Bảng 1, từ đó có thể đưa ra hai quan sát quan trọng. Đầu tiên, chúng tôi xác nhận rằng chỉ số tiêu chuẩn ACC được đo tại các chuyển đổi nhiệm vụ bỏ qua khoảng cách ổn định, trong khi các chỉ số hiệu suất trường hợp xấu nhất min-ACC và WC-ACC của chúng tôi rõ ràng chỉ ra sự mất hiệu suất nghiêm trọng. Thứ hai, các chỉ số của chúng tôi chỉ ra tương tự như Hình 2 rằng chu kỳ đánh giá chi tiết là quan trọng để xác định khoảng cách ổn định. Bảng 1 cho thấy rằng những sụt giảm hiệu suất sắc nét được loại bỏ dần khi min-ACC và WC-ACC đều tăng với chu kỳ đánh giá ρ_eval, và việc quên quy mô lớn WF_100 giảm. Bảng 3 trong Phụ lục báo cáo kết quả tương tự trên các điểm chuẩn khác. Ngoài ra, Phụ lục D báo cáo kết quả cho khoảng cách ổn định cho các kích thước bộ đệm ER khác nhau, trong một thí nghiệm phương thức giọng nói, và cho học tập liên tục trực tuyến.

Bảng 1: Các chỉ số đánh giá liên tục mới được đề xuất của chúng tôi trên Split-MiniImagenet gia tăng theo lớp cho một phạm vi chu kỳ đánh giá ρ_eval. Các chỉ số học tập liên tục tiêu chuẩn là 32.9±0.8 (ACC) và 32.3±1.0 (FORG). Kết quả trên 5 seeds được báo cáo là trung bình (± SD).

[Bảng 1 hiển thị kết quả với các cột Trade-off, Stability, và Plasticity]

4.1 ẢNH HƯỞNG CỦA TÍNH TƯƠNG TỰ NHIỆM VỤ ĐỐI VỚI KHOẢNG CÁCH ỔN ĐỊNH

Khi sự thay đổi miền giữa các nhiệm vụ liên tiếp trong luồng tăng lên, sự can thiệp của các mục tiêu được dự kiến dẫn đến việc quên cao hơn (FORG) và do đó độ chính xác trung bình thấp hơn (ACC) trên tất cả các nhiệm vụ. Tuy nhiên, hiệu ứng đối với khoảng cách ổn định vẫn chưa rõ ràng.

Thiết lập thí nghiệm. Chúng tôi thiết lập một thí nghiệm học tập gia tăng theo miền được kiểm soát với Rotated-MNIST, trong đó mỗi nhiệm vụ tạo thành toàn bộ bộ dữ liệu MNIST với một phép biến đổi xoay tăng dần nhưng cố định φ độ. Việc xoay cụ thể cho nhiệm vụ tăng dần dẫn đến các thay đổi miền có thể kiểm soát được qua các nhiệm vụ trong không gian đầu vào. Để tránh sự mơ hồ giữa các chữ số 6 và 9, chúng tôi hạn chế tổng số xoay không vượt quá 180°. Chúng tôi xem xét một phép xoay cố định tích lũy φ độ, dẫn đến φ₀, (φ₀ + φ), và (φ₀ + 2φ) cho ba nhiệm vụ, với φ₀ được đặt thành 90° cho phép xoay của nhiệm vụ ban đầu. Để tăng tính khác biệt của nhiệm vụ, chúng tôi xem xét một phạm vi các phép xoay tương đối tăng dần φ = [10°, 30°, 60°, 80°]. Chúng tôi nghiên cứu ER với dung lượng bộ đệm 1k mẫu.

Kết quả. Bảng 2 xác nhận sự giảm trong ACC khi sự thay đổi phân phối giữa các nhiệm vụ tăng lên. Giữa chuỗi dễ nhất (φ = 10°) và khó nhất (φ = 80°), ACC giảm 6.6%. Tuy nhiên, hiệu ứng đối với khoảng cách ổn định lớn hơn đáng kể khi min-ACC giảm từ 94.3% xuống 69.2%, giảm 25.1%. Tương tự, FORG giữa φ = 60° và φ = 80° chỉ ra chỉ 1.1% quên nhiều hơn, trong khi WF_10 cho thấy sự giảm 5.0%. Điều này chỉ ra cả hai rằng i) các chỉ số tiêu chuẩn không thể nắm bắt các hiệu ứng của khoảng cách ổn định, và ii) khoảng cách ổn định tăng đáng kể đối với các thay đổi phân phối lớn hơn. Như xác nhận bổ sung, các đường cong độ chính xác cho nhiệm vụ đầu tiên trong Hình 3 cho thấy định tính rằng khoảng cách ổn định tăng đáng kể với việc tăng các phép xoay.

Bảng 2: Kết quả tính tương tự nhiệm vụ cho ER trong Rotated-MNIST. Ba nhiệm vụ liên tiếp được xây dựng bằng cách xoay hình ảnh MNIST φ độ mỗi nhiệm vụ. Chúng tôi giảm tính tương tự nhiệm vụ bằng cách tăng phép xoay từ φ = 10° đến φ = 80°. Kết quả trên 5 seeds được báo cáo là trung bình (± SD).

[Bảng 2 hiển thị kết quả với các góc xoay khác nhau]

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

Hình 3: Thí nghiệm tính tương tự nhiệm vụ được kiểm soát với góc xoay nhiệm vụ trong Rotated-MNIST. Các đường cong độ chính xác cho ER trong nhiệm vụ đầu tiên được báo cáo là trung bình (± SD) trên 5 seeds. Hai đường thẳng đứng chỉ ra các chuyển đổi nhiệm vụ. Các đường ngang chỉ ra min-ACC trung bình trên các seeds.

[Hình 3 hiển thị hai biểu đồ với các góc xoay khác nhau]

5 PHÂN TÍCH KHÁI NIỆM CỦA KHOẢNG CÁCH ỔN ĐỊNH

Để thiết lập một nền tảng khái niệm cho hiện tượng khoảng cách ổn định, chúng tôi tách rời các gradient học tập liên tục của mục tiêu L thành các gradient tính dẻo và ổn định có trọng số λ:

∇L = λ∇L_plasticity + (1-λ)∇L_stability                    (8)

Trong tối ưu hóa dựa trên gradient của mô hình f, ∇L_plasticity nhằm cải thiện hiệu suất trên nhiệm vụ hiện tại và ∇L_stability duy trì hiệu suất trên các nhiệm vụ trong quá khứ. Trong quan điểm của sự đánh đổi ổn định-tính dẻo (Grossberg, 1982), các phương pháp học tập liên tục nhằm cân bằng cả hai số hạng gradient. Tuy nhiên, do đánh giá hướng nhiệm vụ và thiếu đánh giá liên tục, tiến bộ trong sự cân bằng này chỉ được xác minh tại các chuyển đổi nhiệm vụ.

Finetuning tối ưu hóa hoàn toàn cho nhiệm vụ hiện tại T_k và không quan tâm đến tính ổn định, với ||∇L_stability|| = 0. Việc thiếu một số hạng ổn định dẫn đến các cập nhật tham lam cho dữ liệu được quan sát hiện tại, dẫn đến việc quên trên các thay đổi phân phối. Khung đánh giá liên tục của chúng tôi chỉ ra rằng việc quên nghiêm trọng các nhiệm vụ trong quá khứ xảy ra ngay trong vài lần lặp huấn luyện đầu tiên trên một nhiệm vụ mới (xem Phụ lục D.3).

Experience replay (ER) học đồng thời từ dữ liệu của nhiệm vụ mới và một tập con dữ liệu nhiệm vụ trước đó được lấy mẫu từ bộ đệm kinh nghiệm M có kích thước |M|. Số hạng mất mát L_stability được thu được bằng cách xem lại các mẫu nhiệm vụ trước đó trong M. Ngược lại với finetuning, điều này dẫn đến một gradient ổn định ∇L_stability được thiết kế để ngăn chặn việc quên thảm khốc.

Quên. Phân tích thêm của chúng tôi về động lực gradient ∇L_stability chỉ ra một norm gradient thấp trong giai đoạn đầu của việc huấn luyện trên một nhiệm vụ mới. Chúng tôi đầu tiên xem xét một luồng dữ liệu với hai nhiệm vụ liên tiếp. Khi việc huấn luyện bắt đầu trên T₂, f có thể đã hội tụ cho nhiệm vụ đầu tiên T₁, dẫn đến ||∇L_T₁|| ≈ 0. Do đó, ngay sau chuyển đổi nhiệm vụ, chúng ta thực sự có ||∇L_stability|| ≈ 0 vì các mẫu được phát lại hoàn toàn từ T₁. Việc tổng quát hóa điều này cho các chuỗi nhiệm vụ dài hơn yêu cầu không chỉ các gradient gần bằng không cho nhiệm vụ trước đó, mà cho tất cả dữ liệu nhiệm vụ trước đó trong toàn bộ bộ đệm phát lại M. Điều này đã được xác nhận thực nghiệm bởi Verwimp et al. (2021) chỉ ra ER liên tục hội tụ đến các gradient gần bằng không cho M. Chúng tôi chứng minh những phát hiện tương tự cho Split-MNIST trong Hình 4(e-h). Do sự mất cân bằng của các gradient ổn định và tính dẻo, gradient tính dẻo sẽ chiếm ưu thế và có thể dẫn đến việc quên. Những phát hiện thực nghiệm của chúng tôi thực sự xác nhận cho ER rằng những sụt giảm đáng kể trong độ chính xác xảy ra ngay sau chuyển đổi nhiệm vụ, như được hiển thị trong Hình 2.

Phục hồi. Sau các bước ban đầu của việc học một nhiệm vụ mới với các cập nhật ∇L_plasticity tham lam, các tham số thay đổi với gần như không có tín hiệu gradient từ bộ đệm phát lại M. Những cập nhật này lần lượt dẫn đến việc tăng norm gradient của ∇L_stability, cho phép các mẫu trong M được học lại.

Ngược lại với niềm tin trước đó về ∇L_stability duy trì kiến thức trước đó, chúng tôi thấy rằng tính ổn định được bảo tồn ít nhất một phần bằng cách học lại, dẫn đến sự phục hồi kiến thức trước đó. Điều này được xác nhận bởi các thí nghiệm của chúng tôi trong Phần 4 trên năm điểm chuẩn gia tăng theo lớp và miền.

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

5.1 TÌM KIẾM KHOẢNG CÁCH ỔN ĐỊNH TRONG CÁC PHƯƠNG PHÁP HỌC TẬP LIÊN TỤC KHÁC

Phân tích của chúng tôi về việc tách rời các gradient dựa trên tính dẻo và ổn định cũng áp dụng cho các phương pháp đại diện khác trong học tập liên tục. Ngoài ra, chúng tôi cung cấp bằng chứng thực nghiệm cho khoảng cách ổn định trong các phương pháp này, tập trung vào phương pháp phát lại dựa trên ràng buộc GEM (Lopez-Paz & Ranzato, 2017), phương pháp chưng cất kiến thức LwF (Li & Hoiem, 2017), và các phương pháp chính quy hóa tham số EWC (Kirkpatrick et al., 2017) và SI (Zenke et al., 2017). Chi tiết của các thí nghiệm có trong Phụ lục C.

Hình 4: Các đường cong độ chính xác GEM và ER (a-d) trên Split-MNIST gia tăng theo lớp cho bốn nhiệm vụ đầu tiên, và các L2-norm mỗi lần lặp của ∇L_stability hiện tại (e-h). Kết quả được báo cáo là trung bình (± SD) trên 5 seeds, với các đường ngang đại diện cho min-ACC trung bình. Các đường thẳng đứng chỉ ra bắt đầu của một nhiệm vụ mới. Lưu ý rằng thang đo trục x khác nhau qua (a-d) và (e-h) là phóng to của 50 lần lặp đầu tiên.

[Hình 4 hiển thị 8 biểu đồ con (a-h) với các đường cong độ chính xác và gradient norms]

Phát lại ràng buộc gradient. Gradient Episodic memory (GEM) (Lopez-Paz & Ranzato, 2017) khai thác một bộ đệm bộ nhớ M tương tự như ER, được chia thành K bộ đệm nhiệm vụ có kích thước bằng nhau M_k. Tuy nhiên, thay vì trực tiếp tối ưu hóa mục tiêu cho các mẫu trong M, các gradient cụ thể cho nhiệm vụ g_k = ∇L(M_k) của chúng được sử dụng để tạo thành một tập hợp các ràng buộc. Các ràng buộc ⟨g_t, g_n⟩ ≥ 0, ∀n < k cố gắng ngăn chặn việc tăng mất mát trên k-1 nhiệm vụ trước đó, với g_t = ∇L_plasticity là gradient của mẫu quan sát hiện tại (x,y)_t trong nhiệm vụ T_k. Gradient hiện tại g_t được chiếu đến gradient gần nhất g̃ thỏa mãn tập hợp ràng buộc, thu được bằng Lập trình Bậc hai. Chúng tôi tái công thức hóa phép chiếu để thu được gradient ổn định có điều kiện:

∇L_stability = {
  0̃, nếu ⟨g_t, g_n⟩ ≥ 0, ∀n < k
  g̃ - g_t, ngược lại
}                                    (9)

Vì các ràng buộc GEM một cách rõ ràng cố gắng ngăn chặn việc tăng mất mát nhiệm vụ trước đó, ||∇L_stability|| chỉ bằng không nếu cập nhật gradient hiện tại g_t nằm trong vùng khả thi của tập hợp ràng buộc hoặc gần bằng không với chỉ các ràng buộc bị vi phạm nhẹ ||g̃ - g_t|| ≈ 0. Vì trong các ràng buộc tích vô hướng, dấu hiệu được xác định hoàn toàn dựa trên các góc gradient, việc thỏa mãn chúng độc lập với norm của ||∇L_stability|| tại các chuyển đổi nhiệm vụ. Điều này gợi ý rằng GEM có thể cho phép tránh hoặc giảm thiểu khoảng cách ổn định.

Tuy nhiên, thực nghiệm chúng tôi thấy rằng GEM cũng dễ bị khoảng cách ổn định (Hình 4). So với ER, khoảng cách ổn định của GEM lớn hơn đáng kể, được chỉ ra bởi sự chênh lệch lớn trong các đường ngang đại diện cho min-ACC trung bình. Tại các chuyển đổi nhiệm vụ cho Split-MNIST, Hình 4(e-h) cho thấy GEM có ||∇L_stability|| đáng kể so với ER (được xác định theo Phương trình 9), chỉ ra các vi phạm lớn của các ràng buộc. Tuy nhiên, đặc biệt đối với các chuyển đổi nhiệm vụ T₃ và T₄, chúng ta quan sát các gradient giảm xuống độ lớn gần bằng không, dẫn đến một vài cập nhật chủ yếu dựa trên ∇L_plasticity.

Các phương pháp dựa trên chưng cất (Li & Hoiem, 2017; Rannen et al., 2017) ngăn chặn việc quên nhiệm vụ trước đó T_{k-1} bằng cách phạt các thay đổi trong phân phối đầu ra cho các mẫu trong nhiệm vụ hiện tại x ∈ D̃_k.

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

Điều này dẫn đến một mục tiêu chính quy hóa L_stability = KL(f_{t|T_{k-1}|}(x_t)||f_t(x_t)) chưng cất kiến thức thông qua KL-divergence (Hinton et al., 2015) từ mô hình nhiệm vụ trước đó tại t|T_{k-1}| đến mô hình hiện tại. Trước khi cập nhật đầu tiên trên một nhiệm vụ mới, các mô hình nhiệm vụ trước đó và hiện tại là giống hệt nhau, dẫn đến ||∇L_stability|| = 0 gây ra bởi các phân phối khớp hoàn hảo, có thể dẫn đến một khoảng cách ổn định.

Các phương pháp chính quy hóa tham số (Kirkpatrick et al., 2017; Zenke et al., 2017; Aljundi et al., 2018; Kao et al., 2021), còn được gọi là các phương pháp dựa trên prior mô hình, phạt các thay đổi trong các tham số quan trọng cho các nhiệm vụ trước đó với mục tiêu chính quy hóa L_stability = (θ - θ*)^T Ω (θ - θ*), trong đó tầm quan trọng tham số được định nghĩa bởi ma trận phạt Ω ∈ R^{|θ|×|θ|} và được cân nhắc bởi sự thay đổi của các tham số mô hình w.r.t. giải pháp nhiệm vụ trước đó θ* = θ_{t|T_{k-1}|}. Đối với các cập nhật đầu tiên trên một nhiệm vụ mới, các tham số vẫn gần với giải pháp nhiệm vụ trước đó dẫn đến ||∇L_stability|| ≈ 0, và thậm chí giống hệt nhau cho cập nhật đầu tiên, dẫn đến ||∇L_stability|| = 0.

Hình 5 xác nhận thực nghiệm khoảng cách ổn định cho các phương pháp dựa trên chưng cất đại diện (LwF) và chính quy hóa tham số (EWC, SI) trong hai thiết lập: đầu tiên, Split-MNIST gia tăng theo nhiệm vụ, trong đó mỗi nhiệm vụ được phân bổ một bộ phân loại riêng biệt (Hình 5(a-d)); thứ hai, Rotated-MNIST từ Phần 4.1 với tính tương tự nhiệm vụ thấp nhất (φ = 80°) giữa 3 nhiệm vụ. Chi tiết thiết lập được báo cáo trong Phụ lục C.

Hình 5: Các đường cong độ chính xác của phương pháp dựa trên chưng cất (LwF) và chính quy hóa tham số (EWC, SI) trên bốn nhiệm vụ đầu tiên (a-d) của Split-MNIST gia tăng theo nhiệm vụ, với một đầu phân loại riêng biệt cho mỗi nhiệm vụ. Các phương pháp tương tự được xem xét cho Rotated-MNIST gia tăng theo miền (φ = 80°) cho hai nhiệm vụ đầu tiên (e-f) trong số ba. Kết quả được báo cáo là trung bình (± SD) trên 5 seeds, với các đường ngang đại diện cho min-ACC trung bình. Các đường thẳng đứng chỉ ra bắt đầu của một nhiệm vụ mới.

[Hình 5 hiển thị 6 biểu đồ con (a-f) với các đường cong độ chính xác cho các phương pháp khác nhau]

6 KẾT LUẬN

Công trình này đề xuất một khung mới cho đánh giá liên tục với các chỉ số mới cho phép đo lường hiệu suất trường hợp xấu nhất và có thể áp dụng cho các luồng dữ liệu không biết nhiệm vụ. Khung đánh giá của chúng tôi đã xác định những thiếu sót của giao thức đánh giá hướng nhiệm vụ tiêu chuẩn cho học tập liên tục, vì chúng tôi đã xác định một hiện tượng nổi bật và có thể có vấn đề: khoảng cách ổn định. Trong nghiên cứu của chúng tôi trên bảy điểm chuẩn học tập liên tục, chúng tôi đã chỉ ra rằng khi bắt đầu học một nhiệm vụ mới, các phương pháp học tập liên tục tiên tiến khác nhau gặp phải một sự mất hiệu suất đáng kể trên các nhiệm vụ đã học trước đó, thú vị là thường được phục hồi sau đó. Chúng tôi thấy khoảng cách ổn định tăng đáng kể khi các nhiệm vụ liên tiếp khác biệt hơn. Để cung cấp cái nhìn sâu sắc về điều gì có thể gây ra khoảng cách ổn định, chúng tôi đã chính thức hóa một phân tích khái niệm của các gradient, được tách rời thành các số hạng tính dẻo và ổn định. Điều này dẫn đến những phát hiện nhất quán cho phát lại kinh nghiệm, chưng cất kiến thức và các phương pháp chính quy hóa tham số, nhưng để lại một số câu hỏi mở cho phát lại dựa trên ràng buộc. Các hướng thú vị cho công việc tương lai bao gồm các cơ chế để khắc phục khoảng cách ổn định và kết nối với các mạng nơ-ron sinh học: khi học điều gì đó mới, liệu não bộ có gặp phải việc quên tạm thời như vậy không?

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

LỜI CẢM ơN

Dự án này đã nhận được tài trợ từ dự án ERC KeepOnLearning (số tham chiếu 101021347), dự án KU Leuven C1 Macchina, và chương trình nghiên cứu và đổi mới Horizon 2020 của Liên minh Châu Âu dưới thỏa thuận tài trợ Marie Skłodowska-Curie số 101067759.

TÀI LIỆU THAM KHẢO

Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C. Fowlkes, Stefano Soatto, và Pietro Perona. Task2vec: Task embedding for meta-learning. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), tháng 10 năm 2019.

[Tiếp tục với danh sách tài liệu tham khảo đầy đủ...]

--- TRANG 8-21 ---
[Tiếp tục dịch toàn bộ nội dung còn lại của tài liệu, bao gồm tất cả các phần phụ lục, bảng, hình, và tài liệu tham khảo, giữ nguyên cấu trúc và định dạng]
