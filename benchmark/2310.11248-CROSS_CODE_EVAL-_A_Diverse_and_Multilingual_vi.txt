# 2310.11248.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/benchmark/2310.11248.pdf
# Kích thước tệp: 4086750 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
CROSS CODE EVAL: Một Benchmark Đa Dạng và Đa Ngôn Ngữ
cho Việc Hoàn Thiện Mã Nguồn Đa Tệp
Yangruibo Ding1∗Zijian Wang2,∗Wasi Uddin Ahmad2,∗
Hantian Ding2Ming Tan2Nihal Jain2Murali Krishna Ramanathan2
Ramesh Nallapati2Parminder Bhatia2Dan Roth2Bing Xiang2
1Đại học Columbia2AWS AI Labs
yrbding@cs.columbia.edu {zijwan,wuahmad}@amazon.com
https://crosscodeeval.github.io
Tóm tắt
Các mô hình hoàn thiện mã nguồn đã có những tiến bộ đáng kể trong những năm gần đây, tuy nhiên các bộ dữ liệu đánh giá phổ biến hiện tại như HumanEval và MBPP chủ yếu tập trung vào các tác vụ hoàn thiện mã nguồn trong một tệp duy nhất. Thiết lập đơn giản hóa quá mức này không thể đại diện cho tình huống phát triển phần mềm thực tế, nơi các kho mã nguồn trải rộng trên nhiều tệp với nhiều phụ thuộc đa tệp, và việc truy cập và hiểu ngữ cảnh đa tệp thường được yêu cầu để hoàn thiện mã nguồn một cách chính xác. Để lấp đầy khoảng trống này, chúng tôi đề xuất CROSS CODEEVAL, một benchmark hoàn thiện mã nguồn đa dạng và đa ngôn ngữ đòi hỏi hiểu biết sâu sắc về ngữ cảnh đa tệp để hoàn thiện mã nguồn một cách chính xác. CROSS CODEEVAL được xây dựng trên một tập hợp đa dạng các kho mã nguồn thực tế, mở và có giấy phép cho phép trong bốn ngôn ngữ lập trình phổ biến: Python, Java, TypeScript và C#. Để tạo ra các ví dụ đòi hỏi ngữ cảnh đa tệp một cách nghiêm ngặt để hoàn thiện chính xác, chúng tôi đề xuất một phương pháp đơn giản nhưng hiệu quả dựa trên phân tích tĩnh để xác định việc sử dụng ngữ cảnh đa tệp trong tệp hiện tại.

Các thí nghiệm rộng rãi trên các mô hình ngôn ngữ mã nguồn tiên tiến như CodeGen và StarCoder cho thấy CROSS CODEEVAL cực kỳ khó khăn khi thiếu ngữ cảnh đa tệp liên quan, và chúng ta thấy sự cải thiện rõ ràng khi thêm các ngữ cảnh này vào prompt. Tuy nhiên, mặc dù có những cải thiện như vậy, đỉnh cao hiệu suất vẫn chưa đạt được ngay cả với mô hình có hiệu suất cao nhất, cho thấy CROSS CODEEVAL cũng có khả năng đánh giá khả năng của mô hình trong việc tận dụng ngữ cảnh rộng lớn để hoàn thiện mã nguồn tốt hơn. Cuối cùng, chúng tôi đã đánh giá các phương pháp khác nhau trong việc truy xuất ngữ cảnh đa tệp và cho thấy CROSS CODEEVAL cũng có thể được sử dụng để đo lường khả năng của các bộ truy xuất mã nguồn.

1 Giới thiệu
Các mô hình ngôn ngữ cho mã nguồn (code LMs), như Codex (Chen et al., 2021), CodeGen (Nijkamp et al., 2023b,a), và StarCoder (Li et al., 2023), đã chứng minh sức mạnh của chúng trong việc nâng cao năng suất của nhà phát triển thông qua những kết quả đầy hứa hẹn trong các tác vụ hoàn thiện mã nguồn. Để đánh giá các mô hình này, các nhà nghiên cứu đề xuất nhiều benchmark đánh giá hoàn thiện mã nguồn (ví dụ: Chen et al., 2021; Lu et al., 2021; Athiwaratkun et al., 2023; Austin et al., 2021), nơi mô hình được yêu cầu hoàn thiện mã nguồn dựa trên ngữ cảnh trong tệp hiện tại. Tuy nhiên, thiết lập đánh giá như vậy quá đơn giản hóa và không thể phản ánh chính xác khả năng của mô hình trong hoàn thiện mã nguồn. Cụ thể, trong lĩnh vực phát triển phần mềm hiện đại, các kho mã nguồn bao gồm nhiều tệp, mỗi tệp đều liên kết với nhau thông qua các phụ thuộc đa tệp rộng rãi, tức là thông tin ngữ cảnh từ các tệp mã nguồn khác trong cùng kho mã nguồn. Một nhược điểm đáng kể của hầu hết các benchmark hiện có là xu hướng bỏ qua những phụ thuộc phức tạp này. Kết quả là, chúng không thể cung cấp một đánh giá toàn diện về các mô hình hoàn thiện mã nguồn trong các tình huống thực tế.

Hình 1 minh họa hạn chế của các bộ đánh giá hoàn thiện mã nguồn thông thường với một ví dụ thực tế. Nhà phát triển đang viết một test case cho một lớp, CaseConverter, được triển khai trong kho mã nguồn hiện tại. CodeGen-2B-mono (Nijkamp et al., 2023b), một mô hình ngôn ngữ mã nguồn Python lớn, không thể hoàn thiện lời gọi API nếu chỉ có ngữ cảnh tệp hiện tại.

Được thúc đẩy bởi những ví dụ như vậy và để lấp đầy nhu cầu đánh giá hoàn thiện mã nguồn trong phát triển phần mềm thực tế với nhiều phụ thuộc ngữ cảnh đa tệp can thiệp, chúng tôi đề xuất CROSS CODEEVAL, một benchmark đa dạng và đa ngôn ngữ để đánh giá khả năng của các mô hình ngôn ngữ mã nguồn trong việc sử dụng ngữ cảnh đa tệp cho hoàn thiện mã nguồn. Bộ dữ liệu mới này bao gồm 10k ví dụ từ 1k kho mã nguồn trong 4 ngôn ngữ. Không giống như các bộ dữ liệu hiện có nơi câu trả lời đúng có thể được dự đoán chỉ với ngữ cảnh từ tệp hiện tại, CROSS CODEEVAL đòi hỏi nghiêm ngặt ngữ cảnh đa tệp để hoàn thiện chính xác mã nguồn bị thiếu (§2.2). Các ví dụ của CROSS CODEEVAL được tuyển chọn cẩn thận từ các kho mã nguồn mở hiện có với một loạt bộ lọc chất lượng, và chúng tôi đảm bảo CROSS CODEEVAL có sự chồng chéo tối thiểu với dữ liệu huấn luyện từ các code LM hiện có, loại bỏ yếu tố gây nhiễu của rò rỉ dữ liệu và ghi nhớ trong việc diễn giải kết quả (§2.1 & 2.3).

Chúng tôi đã tiến hành đánh giá toàn diện các code LM công khai và độc quyền phổ biến: CodeGen (Nijkamp et al., 2023b,a) và StarCoder (Li et al., 2023) với nhiều kích thước khác nhau từ 350M đến 16B tham số, và GPT-3.5-Turbo của OpenAI trong §3. Kết quả thực nghiệm cho thấy khi chỉ được cung cấp ngữ cảnh tệp hiện tại, các mô hình này cho kết quả không tối ưu. Đáng chú ý, việc kết hợp ngữ cảnh đa tệp vào prompt làm tăng đáng kể hiệu suất của các code LM này, ngay cả trong thiết lập zero-shot. Điều này nhấn mạnh rằng CROSS CODEEVAL phục vụ hiệu quả mục tiêu của nó như một benchmark nhằm đánh giá hoàn thiện mã nguồn đa tệp. Hơn nữa, ngay cả khi cung cấp ngữ cảnh đa tệp trong prompt, hiệu suất của các mô hình mạnh nhất vẫn còn khá khiếm khuyết, làm nổi bật rằng CROSS CODEEVAL cũng có công dụng trong việc đánh giá khả năng của mô hình để tận dụng ngữ cảnh rộng lớn trong hoàn thiện mã nguồn. Cuối cùng, chúng tôi đã đánh giá các phương pháp truy xuất khác nhau từ thưa thớt đến dày đặc, chứng minh rằng CROSS CODEEVAL có thể phục vụ thêm như một benchmark cho truy xuất mã nguồn.

2 CROSS CODE EVAL: Một Benchmark cho Hoàn thiện Mã nguồn Đa tệp
CROSS CODEEVAL là một bộ dữ liệu hoàn thiện phạm vi đa dạng và đa ngôn ngữ trong bốn ngôn ngữ phổ biến: Python, Java, TypeScript và C# nơi các ví dụ bao gồm các prompt mã nguồn kết thúc ở vị trí con trỏ tưởng tượng và tham chiếu bao gồm các chuỗi token mã nguồn từ vị trí con trỏ đến cuối câu lệnh. Các ví dụ CROSS CODEEVAL có một thuộc tính chính - câu lệnh cần được hoàn thiện phải có ít nhất một việc sử dụng API cục bộ (lớp, biến và phương thức được định nghĩa trong kho phần mềm). Tiếp theo, chúng tôi mô tả ngắn gọn cách chúng tôi thu thập các kho phần mềm (§2.1), chọn một tập con của chúng để xây dựng CROSS CODEEVAL (§2.2), quá trình hậu xử lý và kiểm soát chất lượng (§2.3), và thống kê CROSS CODEEVAL cùng phạm vi trong tương lai để mở rộng bộ dữ liệu (§2.4).

2.1 Thu thập Bộ dữ liệu
Chúng tôi thu thập các kho mã nguồn có giấy phép cho phép từ GitHub. Để giảm thiểu các vấn đề rò rỉ dữ liệu tiềm ẩn, chúng tôi tập trung vào các repo được tạo gần đây và không phải là fork. Cụ thể, chúng tôi thu thập các repo được tạo từ 2023-03-05 đến 2023-06-15 vào ngày 2023-09-01. Khoảng thời gian này đảm bảo đủ dữ liệu được thu thập mà không chồng chéo với dữ liệu huấn luyện của nhiều code LM hiện có được phát hành trước giữa năm 2023, bất kể dữ liệu có được công khai hay không. Chúng tôi giới hạn các repo chứa bốn ngôn ngữ mà chúng tôi nghiên cứu và chỉ giữ các repo có kích thước tệp nén <1MB và số sao >= 3. Sau đó chúng tôi lọc bỏ các repo có ít hơn 10 hoặc nhiều hơn 50 tệp mã nguồn. Cuối cùng, chúng tôi loại bỏ các repo có ít nhất một tệp mã nguồn khớp chính xác với một trong các tệp mã nguồn trong bộ dữ liệu Stack (Kocetkov et al., 2022) thường được sử dụng. Kết quả là, chúng tôi có được tương ứng 471, 239, 193 và 99 repo.

2.2 Tạo Bộ dữ liệu
Chúng tôi đề xuất một phương pháp dựa trên phân tích tĩnh để tự động xác định các đoạn mã nguồn cần ngữ cảnh đa tệp. Phương pháp của chúng tôi được minh họa trong Hình 2. Đầu tiên, chúng tôi tìm tất cả các import nội bộ dự án trong tệp gốc. Tiếp theo, một lớp trống được tạo cho mỗi tên được import để thay thế câu lệnh import. Vì tên được import hiện tham chiếu đến một lớp trống, bất kỳ lời gọi tiếp theo đến hàm thành viên hoặc thuộc tính của nó sẽ gây ra lỗi tên không xác định. Chúng tôi tận dụng phân tích tĩnh để bắt những lỗi như vậy trong tệp đã sửa đổi, chính xác tương ứng với các tên trong tệp gốc chỉ có thể được giải quyết bằng ngữ cảnh đa tệp. Chúng tôi ánh xạ vị trí của các tên không xác định trở lại tệp gốc để xác định điểm chia của prompt và tham chiếu.

Để tăng tính đa dạng trong bộ dữ liệu, chúng tôi ngẫu nhiên chọn một token tree-sitter trong cùng dòng trước thực thể đa tệp làm vị trí con trỏ, chia mã nguồn thành prompt và tham chiếu. Thường xảy ra trường hợp cùng một API đa tệp được gọi nhiều lần trong một tệp, và có khả năng các mô hình suy luận tên API từ các lời gọi trước đó ngay cả khi không có ngữ cảnh đa tệp. Do đó, nếu cùng một tên không xác định được báo cáo ở nhiều nơi trong một tệp, chúng tôi chỉ giữ lại lần xuất hiện đầu tiên. Trong công việc này, chúng tôi tập trung vào việc triển khai phương pháp của chúng tôi trong bốn ngôn ngữ lập trình phổ biến, trong khi ý tưởng có thể được tổng quát hóa cho các ngôn ngữ khác về nguyên tắc. Cụ thể, đối với Python, chúng tôi sử dụng Pylint để phát hiện các tên không xác định; đối với Java, chúng tôi sử dụng trình biên dịch javac; đối với TypeScript, chúng tôi sử dụng trình biên dịch tsc; đối với C#, chúng tôi sử dụng trình biên dịch csc từ image mono. Chúng tôi sử dụng tree-sitter để xác định các câu lệnh đầy đủ để xây dựng các completion tham chiếu trong Python. Đối với Java, TypeScript và C#, chúng tôi xem xét các câu lệnh kết thúc bằng ";", "{" hoặc "}". Xem Phụ lục A để biết thêm chi tiết.

2.3 Hậu xử lý và Kiểm soát Chất lượng
Chúng tôi thiết kế một loạt bộ lọc hậu xử lý dựa trên quy tắc và mô hình để đảm bảo chất lượng của bộ dữ liệu. Chúng tôi lọc các ví dụ nếu (1) ít hơn N dòng mã nguồn (các dòng không bao gồm câu lệnh import, với N= 10,20,30,5 tương ứng cho Python, Java, TypeScript và C#) trong prompt, (2) tham chiếu quá ngắn (<3 token) hoặc quá dài (>30 token). Chúng tôi loại trừ các ví dụ nếu tham chiếu được tìm thấy nguyên văn trong bất kỳ tệp mã nguồn nào khác trong kho mã nguồn (tức là đa tệp). Chúng tôi cũng loại bỏ các ví dụ có tham chiếu trùng lặp. Các bước lọc cùng nhau loại bỏ 15%-20% các ví dụ.

Hơn nữa, để đảm bảo rằng tham chiếu không thể được suy luận một cách dự đoán được chỉ từ tệp hiện tại (có thể do các gợi ý mạnh trong tên hàm và bình luận), chúng tôi đưa các ví dụ (prompt đầu vào) vào mô hình starcoderbase-1B (Li et al., 2023) để hoàn thiện câu lệnh và loại bỏ các kết quả khớp chính xác. Bước này dẫn đến việc loại bỏ <10% các ví dụ được tạo ra. Như một lợi ích phụ, điều này bảo vệ thêm rằng các ví dụ không được nhìn thấy bởi các code LM có sẵn công khai trong khi CROSS CODEEVAL được xây dựng dựa trên các kho mã nguồn không chồng chéo với Stack và có thể là các bộ dữ liệu pre-training riêng tư khác được tạo ra trước năm 2023. Cuối cùng, chúng tôi thực hiện chú thích của con người trên một mẫu phụ của CROSS CODEEVAL kết quả và thấy rằng bộ dữ liệu có chất lượng thỏa đáng để phục vụ mục tiêu hoàn thiện mã nguồn đa tệp. Xem Phụ lục B để biết thêm chi tiết.

2.4 Thống kê Bộ dữ liệu, Phạm vi và Mở rộng Tương lai

[THIS IS TABLE: Thống kê CROSS CODEEVAL với các cột cho Python, Java, TypeScript, C# và các hàng cho số lượng repositories, files, examples, v.v.]

Tính năng Python Java TypeScript C#
# Repositories 471 239 193 99
# Files 1368 745 779 642
# Examples 2665 2139 3356 1768
Avg. # lines trong prompt 90.6 106.7 116.5 71.1
Avg. # tokens trong prompt 938.9 995.3 944.9 584.1
Avg. # lines trong reference 1.0 1.1 1.7 1.7
Avg. # tokens trong reference 13.2 14.5 17.4 12.5

Bảng 1: Thống kê CROSS CODEEVAL. 

Thống kê Chúng tôi trình bày thống kê của CROSS CODEEVAL trong Bảng 1. Chúng tôi sử dụng tokenizer StarCoder (Li et al., 2023) để tính số lượng token.

Phạm vi Ngoài prompt và tham chiếu, chúng tôi bao gồm các dòng mã nguồn theo sau tham chiếu từ các tệp mã nguồn gốc trong các ví dụ CROSS CODEEVAL. Với các dòng mã nguồn ở bên trái (prompt hoặc prefix) và bên phải (suffix) của tham chiếu, CROSS CODEEVAL có thể được sử dụng để đánh giá các code LM cho khả năng fill-in-the-middle (FIM) (Bavarian et al., 2022).

Mở rộng Tương lai CROSS CODEEVAL hiện hỗ trợ bốn ngôn ngữ phổ biến. Vì phương pháp của chúng tôi có thể tổng quát hóa, CROSS CODEEVAL có thể được mở rộng cho các ngôn ngữ khác. Ngoài ra, chúng tôi khuyến nghị các bộ dữ liệu pre-training code LM trong tương lai nên loại trừ rõ ràng CROSS CODEEVAL để giảm thiểu tác động của việc ghi nhớ.

3 Thí nghiệm
3.1 Mô hình
Chúng tôi đánh giá CROSS CODEEVAL với các mô hình ngôn ngữ lớn công khai và độc quyền phổ biến.

CodeGen (Nijkamp et al., 2023b,a) là một loạt các code LM tạo sinh. CodeGen hỗ trợ ngữ cảnh chỉ bên trái. CodeGen2.5 đáng chú ý hỗ trợ fill-in-the-middle và cải thiện thêm hiệu suất thông qua huấn luyện đa epoch. Chúng tôi đánh giá các mô hình CodeGen với nhiều kích thước khác nhau từ 350M đến 16B.

StarCoder (Li et al., 2023) là một code LM tạo sinh dựa trên multi-query với 15.5B tham số mô hình được huấn luyện trên bộ dữ liệu The Stack (Kocetkov et al., 2022). Nó hỗ trợ lên đến 8k token. Chúng tôi cũng đánh giá phiên bản base của nó với các kích thước khác nhau: 1B, 3B và 7B.

GPT-3.5-turbo (Ouyang et al., 2022) là một trong những mô hình mạnh nhất được phát triển bởi OpenAI. Nó được huấn luyện với dữ liệu văn bản và mã nguồn toàn diện và hỗ trợ lên đến 4k độ dài chuỗi tối đa. Trọng số mô hình của nó vẫn là độc quyền và chỉ có thể truy cập thông qua API.

3.2 Metrics Đánh giá
Trong việc đánh giá hiệu suất của các mô hình ngôn ngữ mã nguồn, chúng tôi báo cáo hiệu suất trong hai danh mục chính: khớp mã nguồn và khớp định danh (Ding et al., 2022).

Khớp Mã nguồn Metric khớp mã nguồn trực tiếp so sánh mã nguồn được tạo với tham chiếu và được đo bằng khớp chính xác (EM) và độ tương tự chỉnh sửa (ES). Những metric này giúp đánh giá độ chính xác tổng thể của quá trình hoàn thiện mã nguồn, tính đến các yếu tố như định danh, từ khóa, toán tử, dấu phân cách và literals.

Khớp Định danh Metric này đánh giá khả năng của mô hình trong việc dự đoán các giao diện lập trình ứng dụng (API) đúng. Để thực hiện đánh giá này, đầu tiên chúng tôi phân tích mã nguồn và trích xuất các định danh từ dự đoán mô hình và tham chiếu, tạo ra hai danh sách định danh có thứ tự. Sau đó chúng tôi so sánh các định danh được dự đoán với tham chiếu và báo cáo kết quả trong EM và điểm F1.

3.3 Thiết lập Thí nghiệm
Framework đánh giá của chúng tôi dựa trên thư viện Transformers (Wolf et al., 2020). Tất cả các thí nghiệm được tiến hành với thiết lập zero-shot và không có huấn luyện nào được tham gia. Chúng tôi sử dụng cùng một bộ siêu tham số cho tạo mã nguồn trên tất cả các mô hình. Chúng tôi đặt độ dài chuỗi tối đa là 2,048 cho họ CodeGen, 4096 cho GPT-3.5-turbo và 8,192 cho họ StarCoder. Chúng tôi sử dụng độ dài tạo tối đa là 50 và phần còn lại làm prompt.

Chúng tôi khám phá tìm kiếm tham lam và nucleus sampling (Holtzman et al., 2020) với reranking (Hossain et al., 2020). Chúng tôi thấy không có sự khác biệt đáng kể giữa hai cách, và chúng tôi trình bày kết quả tìm kiếm tham lam trong bài báo chính và tham khảo độc giả đến Phụ lục D.2 cho nucleus sampling.

Chúng tôi hậu xử lý các dự đoán mô hình để trích xuất câu lệnh. Đối với Python, chúng tôi lặp lại phân tích nối của prompt và n token completion (ví dụ: n= 1,2, . . . , 50) cho đến khi chuỗi trở nên có thể phân tích được (không có lỗi cú pháp) và token completion thứ (n+ 1) là ký tự xuống dòng. Đối với Java, TypeScript và C#, chúng tôi xem xét các câu lệnh kết thúc bằng ";", "{" và "}", thay vì xuống dòng.

Chỉ Ngữ cảnh Trong-Tệp Trong thực tiễn tiêu chuẩn, các mô hình ngôn ngữ pre-trained được sử dụng để thực hiện hoàn thiện mã nguồn theo cách zero-shot bằng cách tính đến ngữ cảnh mã nguồn được cung cấp. Theo thực tiễn này, chúng tôi tiến hành thí nghiệm sử dụng các code LM (3.1), nơi chúng được cung cấp ngữ cảnh mã nguồn từ tệp hiện tại. Như được hiển thị trong Hình 3, prompt baseline chỉ bao gồm ngữ cảnh trong-tệp.

Ngữ cảnh Đa-tệp Được Truy xuất Được truyền cảm hứng bởi hiệu quả của framework retrieve-and-generate (RG) được đề xuất gần đây cho hoàn thiện mã nguồn cấp repository (Zhang et al., 2023), chúng tôi áp dụng nó cho truy xuất ngữ cảnh đa tệp. Trong framework RG, cơ sở dữ liệu truy xuất được xây dựng bằng cách quét lặp các tệp trong repository và trích xuất các đoạn mã nguồn M dòng liền kề không chồng chéo (trong tất cả thí nghiệm của chúng tôi, M= 10), đây là các ứng viên cho truy xuất ngữ cảnh đa tệp. Truy vấn cho truy xuất được xây dựng sử dụng N dòng cuối (chúng tôi đặt N= 10) của ngữ cảnh trong-tệp. Chúng tôi sử dụng BM25 (Robertson et al., 2009) để tính độ tương tự giữa truy vấn và các ứng viên (chunks ngữ cảnh đa tệp), và sử dụng top-5 đoạn mã nguồn tương tự nhất làm ngữ cảnh đa tệp, xem "Retrieval Context" trong Hình 3. Chúng tôi xem xét tối đa 512 BPE token cho ngữ cảnh như vậy, và phần còn lại của các token sẽ bị cắt bỏ. Hình 3 minh họa ngữ cảnh được truy xuất và prompt tương ứng cho mô hình để hoàn thiện. Với ngữ cảnh trong-tệp làm truy vấn, framework RG thành công truy xuất định nghĩa lớp của CaseConverter nằm trong tệp khác cho tiện ích. Chúng tôi tiếp tục bọc định nghĩa lớp vào một template dưới dạng bình luận mã nguồn và sử dụng nó làm ngữ cảnh đa tệp. Để xây dựng prompt truy xuất, chúng tôi đặt trước ngữ cảnh được truy xuất vào ngữ cảnh trong-tệp.

Truy xuất với Tham chiếu Để định lượng tác động giới hạn trên của ngữ cảnh đa tệp được truy xuất bởi framework RG, chúng tôi thiết kế "truy xuất với tham chiếu" để so sánh. Trong thiết lập này, chúng tôi không chỉ sử dụng ngữ cảnh trong-tệp (như trong thiết lập truy xuất tiêu chuẩn) mà còn sử dụng tham chiếu để truy xuất ngữ cảnh đa tệp. Cụ thể, truy vấn được xây dựng bằng cách sử dụng N dòng cuối của phép nối ngữ cảnh trong-tệp và completion tham chiếu, thay vì chỉ ngữ cảnh trong-tệp trong thiết lập truy xuất tiêu chuẩn. Chúng tôi đặt trước ngữ cảnh được truy xuất (tức là "Retrieval w/ Ref. Context") vào ngữ cảnh trong-tệp để xây dựng prompt cho thiết lập này.

Lưu ý rằng Retrieval w/ Ref. context không thể được áp dụng cho hoàn thiện mã nguồn thực tế, vì completion tham chiếu là không biết. Chúng tôi sử dụng nó như một ước tính giới hạn trên hiệu suất mô hình với framework RG. Ngoài ra, hiệu suất của mô hình trong thiết lập này không tối ưu, vì nó vẫn có thể bị hạn chế bởi truy xuất không hoàn hảo và khả năng của mô hình trong việc sử dụng mã nguồn được truy xuất, và chúng tôi thực hiện đánh giá và phân tích bổ sung về các phương pháp truy xuất sau này trong §3.5.

3.4 Kết quả
Chúng tôi trình bày kết quả trong Bảng 2 và kết quả bổ sung trong Bảng 7. Chúng ta thấy rằng tất cả các mô hình hoạt động kém khi prompt chỉ bao gồm ngữ cảnh trong-tệp. Ví dụ, mô hình StarCoder có hiệu suất tốt nhất với kích thước 15.5B chỉ báo cáo 8.82% khớp chính xác mã nguồn trong Python. Ngay cả một code LM lớn cũng khó đạt được hiệu suất hứa hẹn trong việc hoàn thiện các mẫu CROSS CODEEVAL chỉ với ngữ cảnh trong-tệp vì nó không thể cung cấp đủ gợi ý cho hoàn thiện mã nguồn. Điều này cho thấy thiết kế của CROSS CODEEVAL rằng ngữ cảnh đa tệp là cần thiết để hoàn thiện mã nguồn một cách chính xác.

Hiệu suất cải thiện đáng kể khi ngữ cảnh đa tệp được thêm vào prompt trên tất cả các mô hình và kích thước. Hình 4 cho thấy những cải thiện đáng kể từ việc bao gồm ngữ cảnh đa tệp trong các mô hình CodeGen và StarCoder. Nhìn vào Bảng 2, chúng ta thấy rằng mô hình StarCoder báo cáo lên đến 3.0× và 4.5× khớp mã nguồn chính xác tốt hơn khi bao gồm ngữ cảnh được truy xuất và được truy xuất với tham chiếu tương ứng. Kết quả nhấn mạnh hạn chế của các bộ dữ liệu hiện có chỉ xem xét ngữ cảnh trong-tệp để đánh giá code LM, làm cho các bộ dữ liệu này không đủ để phản ánh khả năng tốt nhất của mô hình trong các tình huống thực tế. Ngược lại, CROSS CODEEVAL duy trì các ngữ cảnh đa tệp cho các mẫu hoàn thiện mã nguồn, cung cấp tài nguyên để cả xác định khả năng tốt nhất của mô hình và phân tích hành vi của mô hình khi nhìn thấy ngữ cảnh toàn diện hơn.

[THIS IS TABLE: Kết quả hiệu suất của các code LM khác nhau trên CROSS CODEEVAL, hiển thị Code Match và Identifier Match cho Python, Java, TypeScript, C#]

Bảng 2: Hiệu suất của các code LM khác nhau trên CROSS CODEEVAL. "Retrieval" và "Retrieval w/ Ref." có nghĩa là chúng tôi xây dựng prompt bằng cách đặt trước ngữ cảnh đa tệp được truy xuất với prompt và prompt + tham chiếu (xem §3.3 để biết chi tiết). Hiệu suất không có ngữ cảnh đa tệp (hàng đầu tiên trong mỗi phần) thường kém. Khi prompt được tăng cường với ngữ cảnh đa tệp (hàng giữa trong mỗi phần), hiệu suất tăng đáng kể. Việc sử dụng completion tham chiếu trong việc xây dựng truy vấn cho truy xuất ngữ cảnh đa tệp (hàng cuối trong mỗi phần) cho thấy giới hạn trên của phương pháp retrieve-and-generate (RG). Kết quả của các mô hình khác có trong Bảng 7.

3.5 Phân tích và Thảo luận
Cải thiện so với Hoàn thiện Mã nguồn Bị xuống cấp Bảng 3 trình bày sự thay đổi số lượng hoàn thiện đúng (dựa trên khớp chính xác với tham chiếu) trên các thiết lập prompt khác nhau. Kết quả cho thấy tất cả các mô hình theo xu hướng hiệu suất cải thiện với ngữ cảnh đa tệp tốt hơn (In-file →Retrieval →Retrieval w/ Ref.). Tuy nhiên, sự biến đổi của tạo sinh đúng/sai là đáng kể; ví dụ, khi thay đổi từ Retrieval sang Retrieval w/ Ref. với StarCoder trong CROSS CODEEVAL Python, chúng ta thấy 327 tạo sinh đúng thay đổi thành sai, và 468 tạo sinh thay đổi theo hướng khác. Qua kiểm tra thủ công, chúng ta thấy rằng việc truy xuất ngữ cảnh đa tệp đúng đóng vai trò rất lớn, vì chất lượng truy xuất tương quan trực tiếp với việc mô hình có thể tạo ra chính xác hay không. Hiệu ứng này được tăng cường thêm bởi thực tế rằng truy xuất xảy ra trong các dòng mã nguồn cố định không thường theo cấu trúc mã nguồn, làm cho việc mô hình tiêu hóa trở nên khó khăn, đặc biệt ở thiết lập zero-shot, phản hồi kết quả từ Zhang et al. (2023). Điều này nhấn mạnh rằng các mô hình tốt nhất hiện tại vẫn không hoàn hảo trong việc tận dụng ngữ cảnh rộng lớn để hoàn thiện mã nguồn tốt hơn. Hơn nữa, nó kêu gọi các nghiên cứu bổ sung trong việc tối ưu hóa các phương pháp truy xuất cho mã nguồn: chúng tôi cho thấy đánh giá với các phương pháp truy xuất khác nhau sau trong phần này.

[Tiếp tục với phần còn lại của văn bản...]

--- TRANG 8 ---
[THIS IS TABLE: Bảng 3 hiển thị số lượng hoàn thiện mã nguồn đúng sử dụng các mô hình tạo mã nguồn khác nhau trên benchmark CROSS CODEEVAL]

[Hình 4: Hiệu suất của các mô hình với nhiều kích thước khác nhau]

Khả năng Mở rộng Hiệu suất Mô hình Hình 4 trực quan hóa cách hiệu suất của CodeGen và StarCoder mở rộng theo kích thước mô hình. Chúng ta thấy hiệu suất tăng theo luật lũy thừa trong tất cả các thiết lập như mong đợi (Kaplan et al., 2020). Tuy nhiên, một lần nữa hiệu suất còn xa mới hoàn hảo ngay cả với mô hình có hiệu suất tốt nhất với ngữ cảnh được truy xuất tốt nhất.

Vị trí của Ngữ cảnh Đa tệp Được Truy xuất Để xác định ngữ cảnh đa tệp liên quan, chúng tôi truy xuất các đoạn mã nguồn từ các tệp khác của repository. Để hiểu tệp nào đóng góp vào ngữ cảnh đa tệp, chúng tôi tiến hành thêm một nghiên cứu về các đoạn mã nguồn được truy xuất. Để phân tích các đoạn mã nguồn được truy xuất cho mỗi prompt, chúng tôi kiểm tra các tệp để xác định xem chúng có đáp ứng các tiêu chí sau hay không: (1) chúng được import bởi tệp đích, (2) chúng nằm trong cùng thư mục với tệp đích, (3) chúng có tên tương tự với tệp đích (với tên tệp chia sẻ ít nhất một token, giả định kiểu tên tệp snake-case hoặc CamelCase), và (4) chúng bao gồm ít nhất một API import trong dự án, tương tự như tệp đích.

Phân tích của chúng tôi cho thấy hầu hết các đoạn mã nguồn đều có nguồn gốc từ các tệp hoặc từ cùng thư mục với tệp đích (Python: 49.0%, Java: 37.8%, TypeScript: 51.3%, C#: 51.7%), hoặc có tên tương tự (Python: 33.4%, Java: 44.5%, TypeScript: 24.9%, C#: 39%). Chúng tôi cũng quan sát thấy rằng các tệp đích và các tệp đa tệp thường chia sẻ ít nhất một câu lệnh import API nội bộ dự án. Kết quả này phù hợp với những phát hiện của Zhang et al. (2023).

Chồng chéo Định danh với Ngữ cảnh Đa tệp Được Truy xuất Định danh là một phần quan trọng của các cấu trúc ngôn ngữ lập trình bao gồm các đề cập API trong mã nguồn. Do đó, chúng tôi kiểm tra phân phối của các ngữ cảnh đa tệp được truy xuất cho các ví dụ trong CROSS CODEEVAL bao gồm các đề cập định danh cũng có mặt trong các tham chiếu. Trong Hình 5, chúng tôi hiển thị phân phối và điểm khớp chính xác định danh đạt được bởi code LM có hiệu suất tốt nhất, StarCoder. Nói chung, rõ ràng rằng tỷ lệ chồng chéo định danh tăng dẫn đến hiệu suất cao hơn, cho thấy mối tương quan tích cực. Điều này kêu gọi một cuộc điều tra về các kỹ thuật truy xuất, với sự nhấn mạnh đặc biệt vào các thuật ngữ quan trọng như định danh cho truy xuất ngữ cảnh đa tệp.

CROSS CODEEVAL như Benchmark Truy xuất Mã nguồn Các quan sát ở trên (ví dụ: hiệu suất giới hạn trên không hoàn hảo và chồng chéo định danh) nhấn mạnh vai trò quan trọng của phương pháp truy xuất mã nguồn. Với sự phụ thuộc mạnh mẽ rằng dự đoán đúng đòi hỏi truy xuất chính xác ngữ cảnh đa tệp liên quan, chúng tôi đề xuất sử dụng CROSS CODEEVAL như một benchmark truy xuất mã nguồn. Chúng tôi thực hiện thí nghiệm với các retriever khác nhau từ thưa thớt (BM25 như chúng tôi đã sử dụng trong phần còn lại của thí nghiệm) đến neural (UniXCoder (Guo et al., 2022) và OpenAI embedding). Đối với UniXCoder, chúng tôi sử dụng độ dài chuỗi tối đa 256 cho mỗi chunk 10 dòng và đối với OpenAI embedding chúng tôi sử dụng 8,000. Chúng tôi sử dụng độ tương tự cosine của embedding của prompt và các chunk để truy xuất top 5 chunk.

Bảng 4 hiển thị kết quả với các phương pháp truy xuất này. Một mặt, chúng ta thấy BM25 cung cấp một baseline mạnh và, trong hầu hết các trường hợp, có thể vượt trội retriever dựa trên UniXCoder. Mặt khác, truy xuất với embedding ada của OpenAI thường tốt hơn cả BM25 và UniXCoder, đặc biệt đối với Java và C#. Tuy nhiên, hiệu suất với retriever có hiệu suất tốt nhất vẫn còn dưới mức tối ưu (<20 EM trong tất cả ngôn ngữ), kêu gọi phát triển trong tương lai của retriever mã nguồn tốt hơn.

[Bảng 4: Kết quả đánh giá của các phương pháp thưa thớt và neural khác nhau trong việc truy xuất ngữ cảnh đa tệp]

4 Công trình Liên quan
Sự xuất hiện của các mô hình ngôn ngữ mã nguồn (LMs) (Feng et al., 2020; Ahmad et al., 2021; Wang et al., 2021; Guo et al., 2022) đã thúc đẩy việc tự động hóa các ứng dụng kỹ thuật phần mềm. Trong số đó, hoàn thiện mã nguồn đã nhận được sự chú ý nhiều nhất, và kết quả là, AI tạo sinh được hỗ trợ bởi các mô hình ngôn ngữ lớn cho mã nguồn (Chen et al., 2021; Xu et al., 2022; Wang và Komatsuzaki, 2021; Black et al., 2021, 2022; Nijkamp et al., 2023b; Fried et al., 2023; Li et al., 2022; CodeGeeX, 2022; Allal et al., 2023; Li et al., 2023; Nijkamp et al., 2023a) đã trở thành hiện thực. Các bộ dữ liệu benchmark đã đóng vai trò then chốt trong việc thúc đẩy lĩnh vực AI tạo sinh cho mã nguồn. Một nhóm lớn các công trình gần đây (Chen et al., 2021; CodeGeeX, 2022; Austin et al., 2021; Athiwaratkun et al., 2023; Cassano et al., 2023; Hendrycks et al., 2021; Raychev et al., 2016; Lu et al., 2021; Allamanis và Sutton, 2013; Puri et al., 2021; Husain et al., 2019; Clement et al., 2021; Ding et al., 2023; Wang et al., 2023; Lu et al., 2022) đã phát triển các benchmark để tạo điều kiện cho việc đánh giá các code LM. Các benchmark này thường đánh giá khả năng hoàn thiện mã nguồn dựa trên ngữ cảnh trong-tệp – các prompt mã nguồn chứa các đoạn mã nguồn từ các tệp hiện tại (nơi người dùng đang viết mã nguồn). Do đó, khả năng của các code LM này để tạo ra mã nguồn đòi hỏi ngữ cảnh cấp kho phần mềm đã được bỏ qua cho đến gần đây.

Một số công trình gần đây đề xuất các framework và benchmark tạo mã nguồn cấp repository (Shrivastava et al., 2023; Ding et al., 2022; Pei et al., 2023; Zhang et al., 2023). Trong khi các công trình này chia sẻ những hiểu biết cấp cao với CROSS CODEEVAL, nhấn mạnh tầm quan trọng của ngữ cảnh đa tệp, trọng tâm của chúng chủ yếu là đề xuất một phương pháp mới để kết hợp những ngữ cảnh như vậy, và các bộ dữ liệu được thu thập để đánh giá các phương pháp riêng của chúng thay vì được chế tác cẩn thận như một benchmark để đánh giá các code LM nói chung. Ví dụ, Shrivastava et al. (2023) và Ding et al. (2022) chỉ thu thập dữ liệu cho một ngôn ngữ lập trình duy nhất, và Pei et al. (2023) thu hẹp phạm vi hoàn thiện chỉ đến các đối số hàm. Như một so sánh, CROSS CODEEVAL bao gồm toàn diện bốn ngôn ngữ lập trình khác nhau (Python, Java, Typescript và C#) và nhắm đến việc đánh giá khả năng hoàn thiện mã nguồn tổng quát của các code LM thay vì một loại ứng dụng cụ thể. REPOEVAL (Zhang et al., 2023) là một công trình đồng thời xây dựng benchmark hoàn thiện mã nguồn cấp repository trong Python, được xây dựng từ 16 kho GitHub. Các kho này bị giới hạn trong một lĩnh vực (chủ yếu là công việc học thuật/nghiên cứu), một số trong số chúng chồng chéo với các bộ dữ liệu pre-training mã nguồn phổ biến (như The Stack (Kocetkov et al., 2022)), và một số có giấy phép không cho phép. Ngược lại, CROSS CODEEVAL được dẫn xuất từ một nhóm đa dạng các kho GitHub có giấy phép cho phép trong 4 ngôn ngữ phổ biến (§2.4). Hơn nữa, CROSS CODEEVAL không chồng chéo với The Stack để tránh rò rỉ dữ liệu, giảm thiểu các vấn đề ghi nhớ tiềm ẩn trong quá trình đánh giá.

5 Kết luận
Chúng tôi giới thiệu CROSS CODEEVAL, một benchmark đa dạng và đa ngôn ngữ cho hoàn thiện mã nguồn đa tệp. CROSS CODEEVAL đòi hỏi hiểu biết ngữ cảnh đa tệp để hoàn thiện mã nguồn một cách chính xác. Chúng tôi sử dụng một phương pháp dựa trên phân tích tĩnh để xác định việc sử dụng ngữ cảnh đa tệp trong mã nguồn, và thực hiện các bước để đảm bảo bộ dữ liệu có chất lượng cao và có sự rò rỉ dữ liệu tối thiểu với bộ dữ liệu pre-training của các code LM phổ biến. Chúng tôi thí nghiệm với các mô hình ngôn ngữ mã nguồn phổ biến và kết quả cho thấy việc bao gồm ngữ cảnh đa tệp cải thiện đáng kể độ chính xác của chúng trong hoàn thiện mã nguồn, chứng minh rằng CROSS CODEEVAL là một benchmark hiệu quả đánh giá khả năng hoàn thiện mã nguồn đa tệp. Hơn nữa, ngay cả mô hình có hiệu suất cao nhất với phương pháp truy xuất tốt nhất vẫn cho thấy dư địa lớn để cải thiện, nhấn mạnh nhu cầu về những tiến bộ thêm trong việc tận dụng ngữ cảnh rộng lớn cho hoàn thiện mã nguồn và retriever mã nguồn tốt hơn. Trong cả hai hướng, CROSS CODEEVAL đứng như một benchmark then chốt. Chúng tôi hình dung CROSS CODEEVAL có thể lấp đầy khoảng trống trong việc đánh giá hoàn thiện mã nguồn đòi hỏi ngữ cảnh đa tệp và thúc đẩy nghiên cứu tương lai trong tất cả các khía cạnh theo hướng này.

--- TRANG 11 ---
Lời cảm ơn
Chúng tôi muốn cảm ơn Ramana Keerthi và Akhilesh Bontala vì sự giúp đỡ trong việc thu thập dữ liệu, và Bryan McWhorter vì các tư vấn pháp lý khác nhau.

Tài liệu tham khảo
Ahmad, W., Chakraborty, S., Ray, B., Chang, K.W., 2021. Unified pre-training for program understanding and generation, trong: Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, Online. pp. 2655–2668. URL: https://aclanthology.org/2021.naacl-main.211 , doi: 10.18653/v1/2021.naacl-main.211 .

[Tiếp tục danh sách tài liệu tham khảo...]

--- TRANG 15 ---
CROSS CODE EVAL: Một Benchmark Đa Dạng và Đa Ngôn Ngữ cho
Hoàn thiện Mã nguồn Đa tệp
Phụ lục

A Chi tiết Thêm về Tạo CROSS CODE EVAL
A.1 Python
Chúng tôi đã trình bày một ví dụ Python trong Hình 2, Phần 2.2 trong bài báo chính. Ở đây chúng tôi mô tả chi tiết phần phân tích tĩnh. Nhớ lại rằng mục đích của phân tích tĩnh là phát hiện các thành viên không xác định trong tệp đã sửa đổi có các import được thay thế bằng các lớp trống. Do đó, không cần bao gồm thông tin cấp dự án ở giai đoạn này, và chúng tôi chỉ chạy Pylint trên tệp độc lập. Chúng tôi hạn chế các loại đầu ra chỉ là lỗi và bỏ qua các loại khác như cảnh báo và quy ước mã hóa. Trong ví dụ ở Hình 2, Phần 2.2, đầu ra Pylint cho tệp đã sửa đổi là "test_utils.py:10:8: E1101: Class 'CaseConverter' has no 'camel_to_snake' member (no-member)". Chúng tôi thu thập tất cả các lỗi loại E1101 và sử dụng chúng để xác định vị trí sử dụng ngữ cảnh đa tệp.

A.2 Java
Thay vì sử dụng công cụ phân tích tĩnh, chúng tôi trực tiếp tận dụng trình biên dịch Java mặc định để trích xuất các hàm thành viên không xác định trong tệp đã sửa đổi, có lớp sở hữu, ban đầu được import từ tệp dự án khác, được thay thế bằng một lớp trống. Hình 6 cho thấy quy trình làm việc của việc trích xuất phương thức đa tệp. Cụ thể, chúng tôi có thể phân tích các lỗi mới với dấu ∧ trong mã đã sửa đổi để xác định vị trí các lời gọi của hàm đa tệp. Một minh họa được cung cấp trong Hình 7.

A.3 TypeScript
Chúng tôi thực hiện các bước sau để tạo ví dụ từ các kho TypeScript.

1. Trích xuất các câu lệnh import từ các tệp nguồn (các tệp có phần mở rộng ".ts" hoặc ".tsx") và xác định các phụ thuộc. Ví dụ, một tệp fileA.ts có câu lệnh import import module_name from '../fileB.ts'; phụ thuộc vào ../fileB.ts. (../fileB.ts là một tệp đa-tệp đối với fileA.ts)

2. Xác định các dòng đa tệp bằng cách sửa đổi mã nguồn, sau đó biên dịch dự án bằng tsc. Chúng tôi thực hiện hai loại sửa đổi sau.
• Loại bỏ câu lệnh import và kiểm tra thông báo đầu ra biên dịch cho "error TS2304: Cannot find name 'X'."
• Loại bỏ câu lệnh import và thêm một lớp giả "class module_name {}" và kiểm tra thông báo đầu ra biên dịch cho "error TS2339: Property 'X' does not exist on type 'Y'."

3. Từ thông báo lỗi trình biên dịch, chúng tôi sử dụng số dòng để tạo tham chiếu cho các ví dụ.

A.4 C#
Đối với các kho C#, quy trình tạo ví dụ theo hai bước. Trong bước đầu tiên, chúng tôi chọn một tệp mã nguồn từ kho C# và thay thế lớp gốc bằng một lớp giả. Sau đó, chúng tôi chạy trình biên dịch C# mono trên tất cả các tệp C# và thu thập các lỗi biên dịch. Trong bước tiếp theo, chúng tôi so sánh các thông báo lỗi trình biên dịch trước và sau việc thay thế trong bước-1 để xác định vị trí các tham chiếu đa tệp dựa trên các lỗi gia tăng, và trích xuất số dòng cho các tham chiếu trong các ví dụ CROSS CODEEVAL. Chúng tôi lặp lại quy trình cho mỗi tệp mã nguồn trong một kho.

[Tiếp tục với phần còn lại của phụ lục...]

B Chú thích Con người cho Kiểm soát Chất lượng
Để đánh giá chất lượng và cải thiện thêm CROSS CODEEVAL, chúng tôi đã tiến hành chú thích con người. Chúng tôi ngẫu nhiên lấy mẫu 100 và 50 ví dụ từ bộ CROSS CODEEVAL Python và Java tương ứng. Sáu tác giả chú thích mỗi người 50 ví dụ, và mỗi ví dụ được chú thích bởi hai người chú thích có kinh nghiệm đáng kể trong ngôn ngữ đích. Ba câu hỏi được hỏi trong việc chú thích:

1. Tham chiếu có chứa bất kỳ tên nào đòi hỏi nhìn vào tệp đa-tệp liên quan không? [Chú ý: khi trả lời, hãy suy nghĩ xem việc nhìn vào tệp đa-tệp liên quan có giúp bạn biết thêm thông tin về một tên được đề cập trong tham chiếu không.]
Chọn từ A: Có. / B: Không, tôi đã biết mọi thứ về tất cả các tên được đề cập trong tham chiếu. / C: Tham chiếu không chứa tên như vậy.

2. Bạn có thể dự đoán tham chiếu chỉ với ngữ cảnh tệp hiện tại không? [Chú ý: khi trả lời, hãy suy nghĩ xem bạn có thể đoán tham chiếu với ngữ cảnh tệp hiện tại không.]
Chọn từ A: Không, tôi không thể dự đoán. / B: Tôi có thể dự đoán nhưng nó có thể không khớp chính xác với tham chiếu. / C: Tôi có thể dự đoán tham chiếu.

3. Bạn có muốn loại bỏ ví dụ khỏi bộ dữ liệu không? [Chú ý: Ví dụ có đủ tốt để được bao gồm trong bộ dữ liệu không? Sử dụng phán đoán tốt nhất của bạn để đưa ra quyết định.]
Chọn từ A: Không. / B: Có thể. / C: Có.

[Bảng 5 và 6 với điểm thỏa thuận và phân phối câu trả lời chú thích]

Chúng tôi tính điểm thỏa thuận và tóm tắt phân phối của các chú thích trong Bảng 5 & 6. Nhìn chung, chúng ta thấy điểm thỏa thuận tuyệt vời trong hầu hết các câu hỏi ở cả hai ngôn ngữ, cho thấy các người chú thích có sự đồng thuận trong những câu hỏi này. Nhìn vào phân phối của các câu trả lời chú thích, chúng ta thấy rằng trong gần như 100% trường hợp, các tham chiếu chứa các tên đòi hỏi thông tin đa tệp (Q1), và chỉ ở 2% rằng tham chiếu có thể được dự đoán chỉ với ngữ cảnh tệp hiện tại (Q2). Cả hai cùng nhau cho thấy rằng CROSS CODEEVAL phục vụ mục đích của nó là một benchmark hoàn thiện mã nguồn đa tệp chuyên dụng phản ánh chính xác khả năng của mô hình trong hiểu biết ngữ cảnh đa tệp. Bên cạnh đó, chúng ta thấy 2.5% và 7% ví dụ mà các người chú thích nghĩ nên được loại bỏ khỏi bộ dữ liệu (Q3). Một cái nhìn gần hơn cho thấy nhiều ví dụ như vậy chứa các chuỗi dài trong tham chiếu, ví dụ: Chat.sendClientSystemMessage("Available scripts:");, không thể được dự đoán dễ dàng và cũng dẫn đến bất đồng giữa các người chú thích. Chúng tôi dự định cải thiện bộ dữ liệu bằng cách lọc ra các ví dụ có chuỗi dài trong tham chiếu trong bản sửa đổi dữ liệu tiếp theo.

C Chi tiết Mô hình Retrieve-and-Generate
[Hình 8 và mô tả chi tiết về framework RG]

D Kết quả Đánh giá Bổ sung và Ablations
D.1 Đánh giá với Các Mô hình Bổ sung
[Bảng 7 với kết quả bổ sung]

D.2 Nucleus Sampling với Re-ranking
[Bảng 8 với kết quả nucleus sampling]

D.3 Kết quả Bổ sung của Truy xuất Mã nguồn
[Bảng 9 với kết quả identifier match]

D.4 Phân tích Định tính
[Hình 9 và 10 với các ví dụ định tính]

E Hạn chế
Đánh giá Zero-shot Việc đánh giá của chúng tôi được thực hiện theo cách zero-shot. Chúng tôi không thực hiện nghiên cứu few-shot vì độ dài chuỗi tối đa của hầu hết các mô hình được đánh giá khá hạn chế để đặt trước các ví dụ bổ sung vào prompt. Do đó, hiệu suất sẽ bị hạn chế vì định dạng của ngữ cảnh đa tệp không bao giờ được mô hình nhìn thấy trong cả quá trình huấn luyện và prompting. Chúng tôi hy vọng CROSS CODEEVAL khuyến khích nghiên cứu tương lai điều tra các phương pháp để truy xuất và kết hợp ngữ cảnh đa tệp vào mô hình một cách hiệu quả.

Chất lượng Truy xuất Ngữ cảnh Đa tệp Việc đặt trước ngữ cảnh đa tệp vào prompt đã cho thấy cải thiện đáng kể đối với hiệu suất của code LM. Tuy nhiên, như chúng tôi đã phân tích và xác định trong Phần 3.5, framework truy xuất RG không hoàn hảo. Do cửa sổ ngữ cảnh có độ dài cố định và tính toán độ tương tự dựa trên token, RG đôi khi truy xuất thông tin vô dụng và không giúp code LM tạo ra tốt hơn. Đối với công việc tương lai, chúng tôi mong đợi một phương pháp truy xuất tiên tiến hơn để thay thế RG cho các ngữ cảnh đa tệp chính xác hơn.

Ghi nhớ Các code LM được huấn luyện trên một lượng lớn mã nguồn không được gán nhãn. Không có cách nào chúng tôi có thể đảm bảo rằng tất cả các mô hình đều không thấy dữ liệu đánh giá trong quá khứ. Chúng tôi cố gắng hết sức bằng cách loại trừ các gói phổ biến khỏi chú thích (xem Phần 2.1). Mặc dù vậy, chúng tôi đề xuất các nhà nghiên cứu và người thực hành cần cẩn thận trong việc diễn giải kết quả với ghi nhớ tiềm ẩn trong tâm trí. Nghiên cứu tương lai trong việc kết hợp ngữ cảnh đa tệp cũng có thể xem xét việc khử trùng lặp dữ liệu huấn luyện với CROSS CODEEVAL, ví dụ thông qua các phương pháp trong Lee et al. (2022).
