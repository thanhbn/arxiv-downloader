# Ra khỏi BLEU: Chúng ta nên đánh giá chất lượng của các mô hình sinh mã như thế nào?

Mikhail Evtikhiev
JetBrains Research
Cộng hòa Cyprus
mikhail.evtikhiev@jetbrains.com

Egor Bogomolov
JetBrains Research
Cộng hòa Cyprus
egor.bogomolov@jetbrains.com

Yaroslav Sokolov
JetBrains
Đức
yaroslav.sokolov@jetbrains.com

Timofey Bryksin
JetBrains Research
Cộng hòa Cyprus
timofey.bryksin@jetbrains.com

TÓM TẮT

Trong những năm gần đây, các nhà nghiên cứu đã tạo ra và giới thiệu một số lượng đáng kể các mô hình sinh mã khác nhau. Vì việc đánh giá con người cho mỗi phiên bản mô hình mới là không khả thi, cộng đồng đã áp dụng các chỉ số đánh giá tự động như BLEU để xấp xỉ kết quả của phán đoán con người. Những chỉ số này có nguồn gốc từ lĩnh vực dịch máy và chưa rõ liệu chúng có áp dụng được cho các tác vụ sinh mã và chúng đồng ý với đánh giá con người ở mức độ nào trong tác vụ này. Cũng có các chỉ số khác, CodeBLEU và RUBY, được phát triển để ước lượng sự tương tự của mã, có tính đến các tính chất của mã nguồn. Tuy nhiên, đối với những chỉ số này hầu như không có nghiên cứu nào về sự đồng ý của chúng với đánh giá con người. Bất chấp tất cả điều đó, những khác biệt tối thiểu trong điểm số chỉ số đã được sử dụng trong các bài báo gần đây để tuyên bố sự vượt trội của một số mô hình sinh mã so với các mô hình khác.

Trong bài báo này, chúng tôi trình bày một nghiên cứu về khả năng áp dụng của sáu chỉ số—BLEU, ROUGE-L, METEOR, ChrF, CodeBLEU, và RUBY—để đánh giá các mô hình sinh mã. Chúng tôi tiến hành một nghiên cứu trên hai bộ dữ liệu sinh mã khác nhau và sử dụng các người chú thích con người để đánh giá chất lượng của tất cả các mô hình chạy trên những bộ dữ liệu này. Kết quả cho thấy rằng đối với bộ dữ liệu CoNaLa của các dòng Python đơn, không có chỉ số nào có thể mô phỏng chính xác phán đoán con người về mô hình nào tốt hơn với độ tin cậy >95% nếu sự khác biệt trong điểm số mô hình nhỏ hơn 5 điểm. Đối với bộ dữ liệu HearthStone, bao gồm các lớp có cấu trúc đặc biệt, sự khác biệt trong điểm số mô hình ít nhất 2 điểm là đủ để tuyên bố sự vượt trội của một mô hình so với mô hình khác. Các phát hiện của chúng tôi cho thấy rằng chỉ số ChrF phù hợp hơn để đánh giá các mô hình sinh mã so với BLEU và CodeBLEU thường được sử dụng. Tuy nhiên, việc tìm ra một chỉ số cho sinh mã gần gũi với con người đòi hỏi công việc bổ sung.

Quyền được cấp để tạo bản sao kỹ thuật số hoặc bản cứng của toàn bộ hoặc một phần tác phẩm này cho mục đích sử dụng cá nhân hoặc trong lớp học mà không tính phí với điều kiện các bản sao không được tạo hoặc phân phối để kiếm lợi nhuận hoặc lợi thế thương mại và các bản sao phải ghi rõ thông báo này và trích dẫn đầy đủ trên trang đầu tiên. Bản quyền cho các thành phần của tác phẩm này thuộc sở hữu của các bên khác ACM phải được tôn trọng. Việc trích dẫn có ghi nguồn được cho phép. Để sao chép bằng cách khác, hoặc tái xuất bản, để đăng trên máy chủ hoặc phân phối lại danh sách, yêu cầu quyền cụ thể trước và/hoặc phí. Yêu cầu quyền từ permissions@acm.org.

Conference'17, July 2017, Washington, DC, USA
©2023 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

Định dạng tham chiếu ACM:
Mikhail Evtikhiev, Egor Bogomolov, Yaroslav Sokolov, và Timofey Bryksin.
2023. Ra khỏi BLEU: Chúng ta nên đánh giá chất lượng của các mô hình sinh mã như thế nào?. Trong Proceedings of ACM Conference (Conference'17). ACM, New York, NY, USA, 17 trang. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 GIỚI THIỆU

Các hệ thống sinh mã là một cách để làm cho quá trình viết mã nguồn dễ dàng và dễ tiếp cận hơn. Trong một công thức phổ biến, các hệ thống như vậy lấy một ý định—mô tả bằng ngôn ngữ tự nhiên—làm đầu vào và tạo ra một đoạn mã thực hiện ý định đó. Sinh mã thích hợp là một vấn đề lâu dài [1] mà, nếu được thực hiện tốt, sẽ hỗ trợ trong giáo dục, đơn giản hóa việc soạn thảo triển khai chương trình cho những người không phải là lập trình viên, và thu hút những lập trình viên mới có thể có kinh nghiệm lập trình hạn chế trong một ngôn ngữ nhất định [2]. Do đó, có một mô hình sinh mã mạnh có thể rất có lợi cho ngành công nghiệp phát triển phần mềm.

Hiện tại, có nhiều mô hình sinh mã khác nhau [2–6] và một số bộ dữ liệu [7–13] mà các mô hình này được đánh giá. Các mô hình sinh mã thường được đánh giá bằng độ chính xác, chỉ số BLEU [14], hoặc chỉ số CodeBLEU [15]. Ban đầu, BLEU được tạo ra để đánh giá chất lượng dịch máy cho xử lý ngôn ngữ tự nhiên, và nó đã được xác thực thực nghiệm có tương quan với phán đoán con người về chất lượng dịch cho văn bản ngôn ngữ tự nhiên. Tuy nhiên, không có xác thực như vậy tồn tại cho tác vụ sinh mã. Hơn nữa, đối với vấn đề di chuyển mã liên quan gần, Tran et al. [16] đã chỉ ra rằng kết quả BLEU chỉ tương quan yếu với phán đoán con người. Đối với vấn đề tóm tắt mã liên quan, Roy et al. [17] đã chỉ ra rằng chỉ số BLEU là chỉ báo kém tin cậy hơn về phán đoán con người so với các chỉ số khác, như METEOR hoặc ChrF.

Chúng tôi xác định ba vấn đề có thể với việc áp dụng chỉ số BLEU cho tác vụ sinh mã, mà theo hiểu biết tốt nhất của chúng tôi, hầu như không được giải quyết [15, 16]:

• Không rõ liệu các chỉ số hiện tại có phù hợp cho việc đánh giá các mô hình sinh mã hay không.
• Không rõ mức độ ý nghĩa của điểm số chỉ số và sự khác biệt trong điểm số phải lớn như thế nào để tuyên bố sự vượt trội của một mô hình so với mô hình khác.
• Không rõ các chỉ số tương quan tốt như thế nào với phán đoán con người đối với các bộ dữ liệu sinh mã hiện tại.

Trong nghiên cứu của chúng tôi, chúng tôi xem xét hai bộ dữ liệu khác nhau. Bộ dữ liệu CoNaLa [12] là một bộ dữ liệu các câu hỏi được đăng trên Stack Overflow1 với các giải pháp bằng Python. Các giải pháp ngắn và thường dài một dòng. Card2code Hearthstone [10] là một bộ dữ liệu dành riêng cho việc sinh các lớp là mô tả của các thẻ được sử dụng trong trò chơi Hearthstone. Các lớp cứng nhắc và hầu hết các cấu trúc lớp giống hệt nhau cho mỗi đoạn mã. Đối với mỗi bộ dữ liệu, chúng tôi xem xét một số mô hình học máy cho sinh mã. Đối với bộ dữ liệu CoNaLa, chúng tôi so sánh kết quả của năm mô hình khác nhau: 1) đường cơ sở CoNaLa [12], 2) Codex [2], 3) TranX không có tiền huấn luyện [5], 4) TranX với tiền huấn luyện, và 5) TranX với tiền huấn luyện và sắp xếp lại [6]. Trong khi có sẵn công khai, các mô hình được chọn rất khác nhau về chất lượng và độ phức tạp, cho phép phán đoán về mối quan hệ giữa chất lượng mô hình, giá trị chỉ số, và đánh giá con người. Đối với bộ dữ liệu Hearthstone, chúng tôi so sánh kết quả của hai mô hình đã được đánh giá trước đó trên bộ dữ liệu này: NL2Code [3] và GCNN [4].

Để giải quyết vấn đề về khả năng áp dụng của các chỉ số tự động, chúng tôi thực hiện lấy mẫu bootstrap theo cặp [18]. Chúng tôi xem xét điểm số chỉ số BLEU, METEOR, ROUGE-L, ChrF, CodeBLEU, và RUBY [14–16,19–21] của các mô hình.

Để giải quyết vấn đề tương quan giữa đánh giá con người và điểm số chỉ số máy tính, chúng tôi thực hiện đánh giá con người các đoạn mã được sinh ra. Các nhà phát triển phần mềm đánh giá xem các đoạn mã được đề xuất có hữu ích trong việc giải quyết vấn đề được đặt ra trên thang điểm từ 0 đến 4. Đối với bộ dữ liệu CoNaLa, 12 nhà phát triển tham gia đánh giá và chúng tôi nhận được trung bình 4,5 điểm từ các nhà phát triển khác nhau cho mỗi đoạn mã. Đối với bộ dữ liệu Hearthstone, có bốn người chấm điểm, và mỗi người chấm điểm đánh giá toàn bộ bộ dữ liệu.

Số lượng điểm mà chúng tôi thu thập cho mỗi đoạn mã không đủ để phân tích hiệu suất chỉ số ở cấp độ đoạn mã, vì Mathur et al. [22] lập luận rằng cần có 15 điểm cho mỗi đoạn mã để cung cấp điểm số ổn định. Do đó, chúng tôi tập trung vào việc so sánh các mô hình ở cấp độ corpus. Tập hợp các mô hình ML có sẵn không đủ lớn để nghiên cứu ý nghĩa của sự khác biệt trong điểm số chỉ số: ví dụ, đối với bộ dữ liệu CoNaLa chỉ có năm mô hình gốc, và do đó chỉ có mười cặp mô hình khác nhau để so sánh. Để cung cấp phân tích thống kê về sự khác biệt điểm số cấp độ corpus, chúng tôi mở rộng tập hợp mô hình gốc với một tập hợp các mô hình tổng hợp. Trong đó, chúng tôi thay thế một phần của một số dự đoán mô hình bằng các dự đoán có điểm đánh giá con người cao hơn hoặc thấp hơn, theo Roy et al. [17].

Các phát hiện và đóng góp của chúng tôi như sau:
• Chúng tôi thấy rằng các chỉ số hiện tại không phù hợp để đánh giá sinh mã, vì đối với mỗi bộ dữ liệu và mỗi chỉ số, các chỉ số không đồng ý với phán đoán con người trong hơn 5% các trường hợp.
• Chúng tôi thấy rằng sự khác biệt trong điểm số chỉ số của hai mô hình nhỏ hơn hai điểm trên thang điểm 0–100 là không có ý nghĩa thống kê trong hơn 5% các trường hợp. Phát hiện này không phụ thuộc vào đánh giá con người và cho thấy cần thiết phải kiểm tra ý nghĩa thống kê khi báo cáo tăng điểm số chỉ số nhỏ hơn hai điểm.
• Chúng tôi thấy rằng, khi tính đến đánh giá con người, tất cả các chỉ số đều không đáng tin cậy trên bộ dữ liệu CoNaLa nếu sự khác biệt điểm số nhỏ hơn năm điểm, và không đáng tin cậy trên bộ dữ liệu HearthStone nếu sự khác biệt điểm số nhỏ hơn hai điểm. Trong tất cả các chỉ số chúng tôi xem xét, ChrF và ROUGE-L là các chỉ số hoạt động tốt nhất cho tác vụ sinh mã.

Bài báo này được cấu trúc như sau. Trong Phần 2, chúng tôi mô tả vấn đề sinh mã, mô tả ngắn gọn các chỉ số chúng tôi sử dụng để đánh giá mã được sinh ra, và mô tả một nghiên cứu tương tự của Roy et al. [17] nhắm đến vấn đề tóm tắt mã. Trong Phần 3, chúng tôi so sánh việc sử dụng chỉ số tự động với đánh giá dựa trên kiểm tra, và phác thảo các vấn đề có thể với việc sử dụng hiện tại của các chỉ số tự động. Trong Phần 4, chúng tôi mô tả phương pháp nghiên cứu của chúng tôi: phác thảo quy trình nghiên cứu, giải thích lựa chọn bộ dữ liệu và mô hình của chúng tôi, các câu hỏi nghiên cứu, cách tiếp cận của chúng tôi để trả lời chúng. Trong Phần 5, chúng tôi trình bày kết quả và trả lời các RQ được trình bày trong phần trước. Trong Phần 6, chúng tôi tóm tắt các phát hiện để cung cấp hướng dẫn cho các nhà thực hành muốn sử dụng chỉ số tự động để đánh giá các mô hình sinh mã, và trình bày các hướng cho công việc tương lai. Trong Phần 7, chúng tôi giải quyết các mối đe dọa đến tính hợp lệ của nghiên cứu. Trong Phần 8, chúng tôi tóm tắt bài báo của chúng tôi. Trong Phụ lục A, chúng tôi mô tả chi tiết hơn các chỉ số chúng tôi nghiên cứu. Cuối cùng, gói tái tạo của chúng tôi có thể được tìm thấy tại https://github.com/JetBrains-Research/codegen-metrics.

2 BỐI CẢNH

2.1 Sinh mã

Sinh mã là một vấn đề lâu dài [1], và một mô hình sinh mã tốt có thể giảm rào cản cho việc viết mã, tự động hóa một số tác vụ thường xuyên mà các kỹ sư có, và giúp những người không phải lập trình viên tạo ra các giải pháp lập trình cho các vấn đề của họ. Vấn đề này cũng liên quan đến các ứng dụng khác của học máy đối với mã. Trong bối cảnh lớn hơn của các tác vụ liên quan đến mã, sinh mã là một tác vụ bổ sung cho tóm tắt mã và liên quan chặt chẽ đến di chuyển mã và hoàn thành mã.

Sự phát triển của học sâu đã cho phép ứng dụng thành công các mô hình neural khác nhau vào vấn đề sinh mã. Đặc biệt, Ling et al. [10] đề xuất một mô hình sequence-to-sequence để sinh mã từ các mô tả ngôn ngữ tự nhiên. Yin et al. [3] và Rabinovich et al. [23] đã sửa đổi bộ giải mã tiêu chuẩn sinh ra một chuỗi token để thực thi các quy tắc ngữ pháp bằng cách đầu tiên sinh ra một cây cú pháp trừu tượng và sau đó chuyển đổi nó thành mã. Sun et al. [4] đề xuất thay thế mạng neural hồi quy bằng mạng neural tích chập cấu trúc dựa trên ngữ pháp. Không giống như mạng neural hồi quy, mạng neural tích chập có thể theo dõi ngữ cảnh ngay cả giữa các vùng xa nhau của dữ liệu được phân tích. Ngược lại, mạng neural hồi quy không có khả năng theo dõi ngữ cảnh khi các phần thông tin liên quan cách xa nhau, còn được gọi là vấn đề phụ thuộc dài hạn [24,25]. Wei et al. [26] đề xuất huấn luyện kép các mô hình sinh mã và tóm tắt mã để nâng cao chất lượng của cả hai mô hình.

Trái ngược với mạng neural hồi quy, các mô hình dựa trên kiến trúc Transformer [27] xử lý toàn bộ chuỗi đồng thời, hiệu quả hơn cả về tốc độ tính toán và nắm bắt các phụ thuộc giữa các token xa nhau. Ngày nay, chúng ta quan sát tiến bộ nhanh chóng trong chất lượng của các mô hình sinh mã nhờ các mô hình Transformer dựa trên gigantic như Codex [2], AlphaCode [28], và CodeParrot.2

Bảng 1 tóm tắt các loại mạng neural và chỉ số được sử dụng bởi các nhà nghiên cứu trong các bài báo đã thảo luận ở trên.

2.2 Đánh giá các mô hình sinh mã

Để có thể theo dõi cải tiến của một mô hình, cần thiết phải đánh giá hiệu suất của nó. Đánh giá con người là tiêu chuẩn vàng cho hầu hết các vấn đề dịch máy hoặc sinh máy. Tuy nhiên, đánh giá thủ công cũng rất đắt đỏ và chậm, và không thực tế để thực hiện đánh giá con người cho từng mẫu được sinh ra trong quá trình phát triển mô hình. Do đó, quan trọng là phải có một chỉ số dễ tính toán để đánh giá đầu ra của mô hình.

Tác vụ sinh mã cũng không khác. Các cách tiếp cận đánh giá cho sinh mã có thể được chia thành ba loại:
(1) Chỉ số từ lĩnh vực dịch máy;
(2) Chỉ số được phát triển để so sánh các đoạn mã;
(3) Chạy và kiểm tra mã được sinh ra.

Tiếp theo, chúng tôi thảo luận chi tiết cả ba.

2.2.1 Chỉ số từ dịch máy. Như Bảng 1 cho thấy, chất lượng của các mô hình sinh mã thường được đánh giá bằng điểm số chỉ số BLEU [14] hoặc độ chính xác. Chỉ số BLEU (BiLingual Evaluation Understudy) là một chỉ số ban đầu được phát triển để đánh giá chất lượng tự động của các văn bản được dịch máy. Chỉ số BLEU là một chỉ số cấp độ corpus dựa trên thước đo chính xác n-gram đã sửa đổi với một hình phạt độ dài cho các câu ứng viên ngắn hơn so với các câu tham chiếu.

Các nhà nghiên cứu cũng xem xét các chỉ số dịch máy khác:
• ROUGE-L [19] là một chỉ số hướng recall tìm kiếm chuỗi con chung dài nhất giữa tham chiếu và ứng viên.
• METEOR [29] là một chỉ số recall-precision hỗn hợp cũng phạt các ứng viên vì không có các unigram kề nhau mà kề nhau trong ví dụ tham chiếu.
• ChrF [21] là một chỉ số F-score n-gram ký tự, trong đó precision và recall trong tính toán F-score được lấy trung bình trên 1- đến 6-gram của ký tự.

Ngoài các chỉ số đã đề cập, các nhà nghiên cứu thường báo cáo Độ chính xác như một chỉ số bổ sung. Trong khi nó hỗ trợ thực tế rằng một mô hình vượt trội hơn mô hình khác, nó hiếm khi được sử dụng làm chỉ số chính trong các tác vụ sinh do quá nghiêm ngặt và ít mạnh mẽ hơn. Do đó, chúng tôi không phân tích Độ chính xác trong nghiên cứu của chúng tôi, vì chúng tôi tập trung vào các chỉ số được sử dụng để so sánh mô hình trực tiếp.

2.2.2 Chỉ số được thiết kế cho mã.

RUBY. Chỉ số RUBY được đề xuất bởi Tran et al. [16] như một thay thế cho các chỉ số ngôn ngữ tự nhiên. Thực vậy, mặc dù BLEU (như METEOR và ROUGE-L) ban đầu được tạo ra để đánh giá các mô hình dịch máy cho ngôn ngữ tự nhiên, nó được sử dụng rộng rãi để đánh giá các mô hình sinh mã, di chuyển mã, và tóm tắt mã. Tran et al. [16] đã tiến hành một nghiên cứu thực nghiệm về BLEU để kiểm tra tính phù hợp của nó trong bối cảnh tác vụ di chuyển mã. Trong bài báo của họ, họ chỉ ra rằng chỉ số BLEU có tương quan khá yếu là 0,583 với đánh giá con người. Các tác giả cũng xây dựng một bộ dữ liệu tổng hợp để minh họa rằng BLEU có thể mang lại kết quả tương tự cho các mô hình có chất lượng khác nhau từ góc độ của người chấm điểm con người. Để giải quyết vấn đề này, các tác giả đã phát minh ra một chỉ số mới RUBY, tính đến cấu trúc mã. Chỉ số so sánh đồ thị phụ thuộc chương trình (PDG) của tham chiếu và ứng viên; nếu không thể xây dựng PDG, nó sẽ chuyển về so sánh cây cú pháp trừu tượng (AST), và nếu AST cũng không thể xây dựng được, chỉ số so sánh khoảng cách chỉnh sửa chuỗi có trọng số giữa các chuỗi tham chiếu R và ứng viên (được token hóa).

CodeBLEU. Ren et al. [15] đề xuất một chỉ số mới gọi là CodeBLEU để đánh giá chất lượng mã được sinh ra cho các tác vụ sinh mã, dịch mã, và tinh chỉnh mã. CodeBLEU là một chỉ số tổng hợp với điểm số là trung bình có trọng số của 4 chỉ số phụ khác nhau xử lý mã khác nhau: như một đồ thị luồng dữ liệu, như một cây cú pháp trừu tượng, và như văn bản. Đối với văn bản, CodeBLEU cung cấp hai chỉ số phụ khác nhau, một trong số đó xử lý tất cả các token như có tầm quan trọng bằng nhau, và một chỉ số khác ưu tiên trọng số cao hơn cho các từ khóa.

2.2.3 Đánh giá dựa trên kiểm tra. Hiệu suất ấn tượng của các mô hình quy mô lớn gần đây [2,28] cho phép sử dụng các kỹ thuật đánh giá gần hơn với các ứng dụng thực tế: thực sự chạy mã được sinh ra trên các bài kiểm tra đơn vị được viết trước và kiểm tra xem nó có giải quyết được vấn đề được đặt ra hay không. Ví dụ, các tác giả của Codex [2] cũng trình bày một bộ dữ liệu gọi là HumanEval bao gồm các tác vụ lập trình và các bài kiểm tra xác thực tính đúng đắn của mã được sinh ra.

Trong khi cách tiếp cận này hợp lý, chúng tôi lập luận rằng hiện tại nó sẽ không thay thế hoàn toàn các kỹ thuật đánh giá hiện có dựa trên việc sử dụng các chỉ số tự động. Để áp dụng đánh giá dựa trên kiểm tra, các nhà nghiên cứu cần các bộ dữ liệu được tạo cẩn thận cho từng thiết lập sinh mã cụ thể. Ngoài ra, các mô hình được nghiên cứu nên vượt qua đủ số lượng bài kiểm tra để phân biệt mạnh mẽ giữa chúng.

2.3 Nghiên cứu về chỉ số cho tóm tắt mã

Các chỉ số tự động được sử dụng cho nhiều tác vụ sinh liên quan đến mã khác như dịch mã, tóm tắt mã, hoặc tinh chỉnh mã [13]. Gần đây, Roy et al. [17] đã nghiên cứu khả năng áp dụng của các chỉ số tự động cho tác vụ tóm tắt mã, liên quan chặt chẽ đến sinh mã. Đối với tác vụ này, các chỉ số như BLEU được sử dụng rộng rãi như các proxy của đánh giá con người. Các tác giả chỉ ra rằng không có sự khác biệt có ý nghĩa thống kê giữa các mô hình với điểm số corpus khác nhau ít hơn 1,5 điểm theo bất kỳ chỉ số nào được xem xét. Hơn nữa, tất cả các chỉ số mà các tác giả xem xét đều không phải là proxy đáng tin cậy của đánh giá con người nếu sự khác biệt trong điểm số corpus nhỏ hơn hai điểm theo các chỉ số. Trong tất cả các chỉ số được Roy et al. xem xét, METEOR, ChrF, và BERTScore cho thấy sự đồng ý tốt nhất với phán đoán con người ở cấp độ corpus. Vì Roy et al. thực hiện một nghiên cứu mở rộng về hiệu suất chỉ số cho một tác vụ liên quan chặt chẽ đến sinh mã, chúng tôi áp dụng nhiều phương pháp mà họ sử dụng trong nghiên cứu của chúng tôi.

2.3.1 Bộ dữ liệu và gắn nhãn. Roy et al. sử dụng bộ dữ liệu tóm tắt mã Java của LeClair et al. [30]. Họ lấy mẫu ngẫu nhiên 383 đoạn mã từ đó và sinh ra năm bản tóm tắt với các mô hình khác nhau. Các người chú thích con người sau đó đánh giá năm bản tóm tắt được sinh ra và bản tóm tắt tham chiếu trên thang điểm Likert năm điểm để đánh giá tính súc tích, tính trôi chảy, và sự phù hợp nội dung của mỗi bản tóm tắt. Họ cũng gán một điểm Đánh giá Trực tiếp (DA) trên thang điểm 0–100 phản ánh ý kiến của họ về chất lượng tổng thể của bản tóm tắt. Chỉ điểm Đánh giá Trực tiếp được sử dụng để phân tích hiệu suất chỉ số tương đối.

2.3.2 Đánh giá chỉ số cấp độ corpus. Đánh giá cấp độ corpus về khả năng áp dụng chỉ số của Roy et al. theo đuổi hai mục tiêu hơi khác nhau. Đầu tiên, các tác giả quan tâm đến việc liệu các chỉ số có khả năng phân biệt chất lượng của các mô hình hiện có hay không. Để làm điều đó, họ thực hiện kiểm tra ý nghĩa ngẫu nhiên trên bộ dữ liệu 383 đoạn mã để tìm hiểu rằng trong năm mô hình được xem xét trong nghiên cứu, sự khác biệt trong điểm số của năm mô hình tốt nhất không có ý nghĩa thống kê. Quan trọng là nhấn mạnh rằng sự thiếu khác biệt thống kê này chỉ được tìm thấy từ điểm số chỉ số và không dựa vào gắn nhãn con người.

Mục tiêu thứ hai cho đánh giá chỉ số cấp độ corpus là tìm hiểu xem các chỉ số cấp độ corpus thường được sử dụng có phản ánh đánh giá chất lượng con người của các bản tóm tắt được sinh ra hay không. Có sự thiếu hụt tương đối của các mô hình học máy có sẵn (Roy et al. đã sử dụng năm mô hình tóm tắt mã trong nghiên cứu của họ). Do đó, không thể nghiên cứu trực tiếp sự khác biệt nào trong điểm số chỉ số cần thiết để tuyên bố rằng một mô hình tốt hơn mô hình khác theo con người – không có đủ cặp mô hình để có đủ dữ liệu về sự khác biệt trong điểm số mô hình. Tuy nhiên, nếu có nhiều mô hình độc lập hơn, các nhà nghiên cứu sẽ phải gắn nhãn nhiều đầu ra mô hình hơn, tăng chi phí và sự vất vả của nghiên cứu. Để có được sự đa dạng hơn trong điểm số chỉ số mà không tăng số lượng bản tóm tắt cần gắn nhãn, Roy et al. sử dụng các mô hình tổng hợp.

Một mô hình tổng hợp là một mô hình mang lại một tập hợp các bản tóm tắt dựa trên một trong năm mô hình gốc, với một tỷ lệ khác nhau của các bản tóm tắt được thay thế bằng các dự đoán của các mô hình khác. Đặc biệt, để tạo ra một mô hình tổng hợp cải thiện mô hình A gốc 1%, các tác giả thay thế 1% các bản tóm tắt được dự đoán bởi mô hình bằng các dự đoán tốt hơn của các mô hình khác. Chất lượng của dự đoán được đánh giá theo điểm số DA con người.

Roy et al. tạo ra một tập hợp các mô hình tổng hợp và sau đó chọn 100 trong số chúng. Sau đó, họ thêm chúng vào năm mô hình gốc và thực hiện so sánh theo cặp thành một số nhóm khác nhau dựa trên ý nghĩa thống kê của sự khác biệt điểm số chỉ số cũng như của độ lớn khác biệt. Nhóm có thể được định nghĩa, ví dụ, cho sự khác biệt chỉ số có ý nghĩa thống kê giữa hai và năm. Đối với mỗi cặp này, Roy et al. cũng tính toán ý nghĩa của sự khác biệt trong điểm số đánh giá DA con người tương ứng của họ. Hiệu quả của một chỉ số đánh giá tự động cấp độ corpus sau đó có thể được xác định bằng cách nhìn vào sự đồng ý giữa điểm số chỉ số và điểm số đánh giá con người. Đối với một chỉ số đánh giá tự động đáng tin cậy, người ta mong đợi tìm thấy sự tương ứng một-một giữa sự khác biệt có ý nghĩa trong điểm số chỉ số và điểm số đánh giá con người.

Sử dụng cách tiếp cận so sánh theo cặp, Roy et al. có thể phân tích những điều sau:
• Họ tìm hiểu cho bao nhiêu cặp trong một nhóm nhất định mà hai mô hình trong cặp khác biệt có ý nghĩa, theo từng chỉ số. Điều này cho phép suy ra sự khác biệt nào trong điểm số chỉ số của hai đầu ra mô hình cần thiết để mong đợi rằng hai mô hình cũng sẽ khác biệt có ý nghĩa từ quan điểm của chỉ số.
• Đối với mỗi nhóm và mỗi chỉ số, họ xem xét nhóm các cặp, trong đó một mô hình tốt hơn một cách có ý nghĩa so với mô hình khác theo chỉ số. Sau đó, đối với mỗi cặp, họ kiểm tra xem hai mô hình trong đó cũng có khác biệt có ý nghĩa hay không, theo đánh giá con người. Điều này cho phép họ nghiên cứu lỗi Type-I của mỗi chỉ số và kiểm tra nó thay đổi như thế nào từ nhóm này sang nhóm khác.
• Đối với mỗi nhóm và mỗi chỉ số, họ xem xét nhóm các cặp, trong đó hai mô hình không khác biệt có ý nghĩa, theo chỉ số. Sau đó, đối với mỗi cặp từ nhóm này, họ kiểm tra xem hai mô hình trong đó có khác biệt có ý nghĩa hay không, theo đánh giá con người. Điều này cho phép họ nghiên cứu lỗi Type-II của mỗi chỉ số và kiểm tra nó thay đổi như thế nào từ nhóm này sang nhóm khác.

Từ phân tích này, Roy et al. thấy rằng các chỉ số đánh giá tự động không thể nắm bắt chính xác sự khác biệt trong chất lượng tóm tắt giữa hai cách tiếp cận khi sự khác biệt chỉ số nhỏ hơn hai điểm. METEOR, BERTScore, và ChrF hoạt động tốt nhất về tỷ lệ lỗi Type-I và Type-II. BLEU có tỷ lệ lỗi Type-I cao nhất bất kể độ lớn của sự khác biệt.

2.3.3 Phân tích cấp độ đoạn mã. Roy et al. cũng xem xét hiệu suất chỉ số cho cấp độ đoạn mã. Về nguyên tắc, phân tích kết quả chỉ số cấp độ đoạn mã có thể cung cấp lợi thế so với phân tích cấp độ corpus bằng cách theo dõi hiệu suất tinh tế của các mô hình. Để thực hiện phân tích cấp độ đoạn mã, Roy et al. sử dụng kỹ thuật Xếp hạng Tương đối Đánh giá Trực tiếp, so sánh điểm số tương đối theo cặp của hai đoạn mã [31]. Kỹ thuật này dựa trên điểm số Đánh giá Trực tiếp và không thể áp dụng cho các chú thích trên thang điểm năm điểm.

3 ĐỘNG LỰC

Các chỉ số được sử dụng trong giai đoạn xác thực của một quy trình học máy và để so sánh các mô hình khác nhau. Tuy nhiên, nếu đánh giá con người là tiêu chuẩn vàng, chỉ số được sử dụng nên phù hợp với phán đoán con người càng gần càng tốt. Ví dụ, trong dịch máy, có một cuộc thi hàng năm giữa các chỉ số khác nhau, với chỉ số tốt nhất là chỉ số mô phỏng phán đoán con người tốt nhất [31, 32].

Ngay cả khi một số chỉ số (như BLEU) đã được sử dụng trong quá khứ để mô phỏng phán đoán con người, có thể có lợi khi xem xét các chỉ số khác có thể có tương quan tốt hơn với đánh giá con người. Một tình huống tương tự đã xuất hiện trong sinh ngôn ngữ tự nhiên: mặc dù BLEU ban đầu được áp dụng cho lĩnh vực này, sau đó đã được chỉ ra rằng các chỉ số dựa trên chồng chéo từ (như BLEU) có tương quan rất thấp với phán đoán con người trong một số tác vụ sinh ngôn ngữ tự nhiên như sinh phản hồi đối thoại [33].

Trong phần còn lại của phần này, chúng tôi thảo luận chi tiết tại sao việc nghiên cứu các chỉ số tự động cho sinh mã là quan trọng và những câu hỏi nào đáng được trả lời trong vấn đề này.

3.1 Chỉ số và đánh giá dựa trên kiểm tra

Với việc giới thiệu gần đây HumanEval [2], một bộ dữ liệu cho phép chạy và kiểm tra mã Python được sinh ra trong một thiết lập gần-thực tế, có vẻ như việc sử dụng các chỉ số tự động sẽ sớm trở nên lỗi thời. Tuy nhiên, chúng tôi nghĩ rằng sẽ không phải như vậy trong tương lai gần.

Thứ nhất, việc thu thập các bộ dữ liệu đánh giá dựa trên kiểm tra đòi hỏi nỗ lực con người đáng kể để phát triển một tập hợp các tác vụ cũng như bao phủ chúng với các bài kiểm tra. Cho rằng tác vụ sinh mã có thể được công thức hóa khác nhau và áp dụng cho các ngôn ngữ và lĩnh vực khác nhau, mỗi trường hợp cụ thể đòi hỏi một hệ thống đánh giá được tạo thủ công riêng biệt. Do đó, việc sử dụng các chỉ số tự động hữu ích khi áp dụng sinh mã trong các lĩnh vực mới.

Thứ hai, việc huấn luyện và suy luận các mô hình rất lớn như Codex vừa tốn kém vừa thách thức về mặt kỹ thuật [2]. Vì lý do này, một hướng nghiên cứu quan trọng là phát triển các mô hình sinh mã nhỏ hơn chưa thể đạt được chất lượng có thể so sánh với các đối tác dựa trên Transformer lớn. Đối với các mô hình nhỏ hơn, các khung đánh giá như HumanEval sẽ dẫn đến điểm số chỉ số kém, và tính mạnh mẽ của chúng để so sánh mô hình trong trường hợp này vẫn là một câu hỏi mở.

Cuối cùng, ngay cả khi hai mô hình sinh mã không vượt qua bất kỳ bài kiểm tra nào, vẫn có thể nói đoạn mã nào gần hơn với giải pháp đúng. Ví dụ, đối với một vấn đề "Loại bỏ các giá trị None trong từ điển d" và hai đoạn mã được trình bày dưới đây, đoạn đầu tiên gần hơn nhiều với giải pháp đúng, mặc dù nó vẫn không vượt qua các bài kiểm tra.

1. print(dict((k,v) for k,v in d.items() if v)))
2. list(d.values())

Do đó, quan trọng là có thể đánh giá chất lượng của các đoạn mã được sinh ra ngay cả khi chúng không vượt qua các bài kiểm tra, vì các nhà phát triển có thể thấy một số đoạn mã được sinh ra dễ sửa và tích hợp vào mã của họ hơn.

3.2 Các chỉ số hiện tại có phù hợp cho sinh mã không?

Các chỉ số dịch máy được phát triển cho ngôn ngữ tự nhiên và không tính đến các tính chất của ngôn ngữ lập trình. Việc sử dụng các chỉ số như vậy có thể không tối ưu cho đánh giá sinh mã do một số yếu tố.

3.2.1 Sự khác biệt giữa ngôn ngữ lập trình và ngôn ngữ tự nhiên. Ngôn ngữ lập trình có cấu trúc cú pháp nghiêm ngặt, trong khi cấu trúc ngôn ngữ tự nhiên thoải mái hơn. Ví dụ, trong khi việc hoán đổi hai nhóm token trong một câu ngôn ngữ tự nhiên thường không ảnh hưởng mạnh đến ý nghĩa của nó, việc chuyển đổi như vậy thường sẽ làm cho một đoạn mã không hợp lệ. Thứ hai, các chỉ số dịch máy (MT) đo lường độ chính xác từ vựng của đầu ra mô hình, trong khi đối với mã được sinh ra chúng ta muốn đánh giá chức năng của nó. Có thể làm cho các chỉ số MT phù hợp hơn với mã, ví dụ, có thể đổi tên tất cả các biến trong ứng viên và các tham chiếu theo thứ tự xuất hiện của chúng, loại bỏ sự không khớp giả do các quy ước đặt tên khác nhau. Tuy nhiên, một số vấn đề không thể được giải quyết rõ ràng mà không tính đến cấu trúc mã. Do đó, có thể rằng một chỉ số sẽ tính đến cấu trúc và cú pháp của các đoạn mã sẽ là một proxy tốt hơn của đánh giá con người.

3.2.2 BLEU đã bị vượt qua trong các tác vụ khác. Đánh giá con người là lựa chọn tốt nhất để đánh giá chất lượng của mô hình sinh mã và được coi là sự thật cơ bản trong đánh giá chỉ số trong nhiều tác vụ khác nhau, xem ví dụ [34]. Tuy nhiên, vì đánh giá con người rất đắt đỏ, rõ ràng là không thể có mỗi đầu ra mới của mô hình được đánh giá bởi một nhóm lập trình viên. A priori, không rõ liệu BLEU hoặc bất kỳ điểm số chỉ số nào khác có tương quan tốt với đánh giá con người đối với tác vụ sinh mã hay không. Các bài báo gốc cho các chỉ số dịch máy [14,19–21] bao gồm các nghiên cứu cho thấy tương quan cao giữa điểm số chỉ số và phán đoán con người đối với tác vụ dịch máy. Tuy nhiên, một đánh giá của Reiter [34] cho thấy rằng tương quan BLEU–con người kém đối với các tác vụ sinh ngôn ngữ tự nhiên và BLEU chỉ nên được sử dụng để đánh giá các hệ thống NLP dịch máy.

Đối với vấn đề di chuyển mã liên quan gần, đã được chỉ ra rằng tương quan giữa điểm số BLEU và điểm của con người là 0,583, khá yếu [16]. Cũng có một nghiên cứu về tương quan chỉ số-con người cho các chỉ số BLEU, độ chính xác, và CodeBLEU [15], đã chỉ ra rằng chỉ số CodeBLEU tương quan tốt hơn với ý kiến con người so với độ chính xác hoặc BLEU. Tuy nhiên, nghiên cứu này không xem xét các chỉ số khác. Cuối cùng, Roy et al. [17] đã thực hiện một nghiên cứu mở rộng về khả năng áp dụng của các chỉ số tự động đối với vấn đề tóm tắt mã, để tìm ra rằng chỉ số BLEU de-facto tiêu chuẩn là một trong những chỉ số tồi tệ nhất để đánh giá các mô hình tóm tắt mã trong số sáu chỉ số mà họ xem xét.

Tất cả những quan sát này nhấn mạnh rằng khả năng áp dụng của một chỉ số cụ thể phụ thuộc mạnh vào vấn đề. Do đó, việc sử dụng một chỉ số thành công hoạt động cho một vấn đề cho một vấn đề khác có thể là không có căn cứ.

3.2.3 Dịch từ chỉ số sang đánh giá con người. Không rõ rằng sự tăng trong điểm số chỉ số có liên quan tuyến tính đến sự tăng của chất lượng "thực" của đoạn mã. Để minh họa, hãy xem xét một trong những tác vụ trong bộ dữ liệu CoNaLa:

Tác vụ: nối một danh sách các chuỗi ['a','b','c']
giải pháp mô hình cơ sở: set(['a','b','b'])
giải pháp best-tranx-rerank: ''.join(['a','b','c'])

Mặc dù đoạn mã cơ sở không giải quyết được câu hỏi tác vụ (và thậm chí không thể tái tạo danh sách các chuỗi cần được nối), nó có điểm BLEU tương đối cao là 48,09. Đoạn mã thứ hai thành công giải quyết vấn đề và có điểm BLEU là 100.

Bây giờ, hãy xem xét các đầu ra giả định của hai mô hình A và B khác nhau. Cả hai đầu ra đều có BLEU 50, nhưng đối với mô hình A mỗi ứng viên có BLEU 50 và có chất lượng tương tự như trên, trong khi đối với mô hình B, một nửa ứng viên có BLEU không và nửa còn lại có BLEU 100. Trong trường hợp này, có thể lập luận rằng mô hình B tốt hơn mô hình A, ngay cả khi chúng có điểm BLEU cấp độ corpus gần nhau: cho ví dụ trên, mô hình A có thể sinh mã hầu như không liên quan mọi lúc, trong khi mô hình B sinh mã hoàn hảo trong một nửa các trường hợp.

Nếu sự phụ thuộc giữa đánh giá con người và giá trị chỉ số không tuyến tính, chúng ta không thể đơn giản lấy trung bình các giá trị chỉ số trên tất cả các đoạn mã để phản ánh đánh giá con người của mô hình. Ngoài ra, có thể có những lý do khác tại sao điểm BLEU và điểm con người có thể không tương quan tốt, và cần thiết phải nghiên cứu tương quan giữa hai để có thể suy ra kiến thức về cách diễn giải điểm BLEU và đánh giá chất lượng mô hình từ chúng.

3.3 Chúng ta có sử dụng chỉ số tự động đúng cách không?

Cách phổ biến sử dụng chỉ số tự động để đánh giá mô hình là báo cáo một số cấp độ corpus duy nhất [3,5,13,23]. Trong khi cách tiếp cận này đơn giản và có thể rất thực tế trong quá trình huấn luyện, không rõ làm thế nào sự khác biệt thô trong điểm số chỉ số có thể được dịch thành các tuyên bố về ý nghĩa thống kê của sự khác biệt.

Trong lĩnh vực sinh mã, việc so sánh các mô hình khác nhau thường được thực hiện bằng cách đơn giản so sánh điểm BLEU hoặc CodeBLEU của chúng, được lấy trung bình trên toàn bộ bộ dữ liệu kiểm tra (xem ví dụ, [3,5,13,23]). Tuy nhiên, khi một cải tiến từ ví dụ, điểm BLEU từ 29 đến điểm BLEU 30 được tuyên bố, nó hiếm khi được hỗ trợ bởi dữ liệu về ý nghĩa thống kê của cải tiến. Như Roy et al. [17] đã chỉ ra, đối với tác vụ tóm tắt mã liên quan gần, sự khác biệt nhỏ trong điểm số chỉ số không có ý nghĩa thống kê, có thể rằng hiện tượng tương tự tồn tại cho sinh mã.

Do đó, quan trọng là nghiên cứu sự khác biệt giữa điểm số chỉ số của hai mô hình cho một bộ dữ liệu cụ thể phải lớn như thế nào để tuyên bố rằng một trong các mô hình tốt hơn mô hình khác với độ tin cậy mong muốn.

4 PHƯƠNG PHÁP

Các vấn đề chúng tôi liệt kê trong Phần 3 đã thúc đẩy chúng tôi đặt ra các câu hỏi nghiên cứu sau:

RQ1 Hiệu suất của các mô hình được xem xét có khác nhau đáng kể ở cấp độ corpus không?

RQ2 Kết quả của các chỉ số tự động có ý nghĩa như thế nào và sự khác biệt trong điểm số chỉ số cấp độ corpus của hai mô hình phải lớn như thế nào để tuyên bố rằng một mô hình tốt hơn (theo chỉ số nhất định) so với mô hình khác với ý nghĩa được xác định trước?

RQ3 Điểm số chỉ số cấp độ corpus phản ánh đánh giá con người về mã được sinh ra tốt như thế nào?

Lấy cảm hứng từ công việc của Roy et al. [17] được mô tả chi tiết trong Phần 2.3, quy trình cách tiếp cận của chúng tôi như sau:

1. Chúng tôi thu thập đầu ra của các mô hình trên các bộ dữ liệu chúng tôi xem xét.
2. Chúng tôi đánh giá các chỉ số tự động trên các đoạn mã được sinh ra, nhận được điểm số chỉ số cho mỗi đoạn mã được sinh ra.
3. Chúng tôi thực hiện đánh giá con người của các đoạn mã được sinh ra (được mô tả dưới đây chi tiết hơn), thu thập một tập hợp điểm con người cho mỗi đoạn mã được sinh ra.
4. Sử dụng tập hợp điểm con người thu được, chúng tôi nhận được điểm con người "sự thật cơ bản" bằng cách tổng hợp các điểm lại với nhau bằng thuật toán M-MSR [35], nhận được một điểm duy nhất cho mỗi đoạn mã được đánh giá bởi các chuyên gia. Chúng tôi sử dụng việc thực hiện thuật toán M-MSR của Ustalov et al. [36].
5. Sử dụng đầu ra của các mô hình, chúng tôi tạo ra các mô hình tổng hợp bằng cách thay thế một số dự đoán bằng các dự đoán nhận được điểm đánh giá con người cao hơn hoặc thấp hơn. Ví dụ, để có được một mô hình tranx-annot tổng hợp với 1% dự đoán được cải thiện, chúng tôi xem xét đầu ra của nó và thay thế 1% dự đoán tồi tệ nhất của nó bằng các dự đoán tốt nhất có sẵn từ các mô hình khác. Chất lượng của một dự đoán được dẫn xuất từ điểm đánh giá con người.
6. Đối với mỗi cặp mô hình tổng hợp và không tổng hợp được đánh giá trên cùng một bộ dữ liệu, chúng tôi thực hiện lấy mẫu bootstrap theo cặp. Chúng tôi làm điều đó để tìm ý nghĩa thống kê của tuyên bố rằng một trong các mô hình tốt hơn mô hình khác theo điểm số chỉ số. Chúng tôi sử dụng ngưỡng 95% để tuyên bố sự khác biệt có ý nghĩa thống kê giữa các mô hình.
7. Đối với mỗi bộ dữ liệu được đánh giá bởi con người và đối với mỗi cặp mô hình được đánh giá trên đó, chúng tôi thực hiện lấy mẫu bootstrap theo cặp trên các điểm sự thật cơ bản. Chúng tôi sử dụng kết quả kiểm tra thống kê để kiểm tra với ý nghĩa thống kê nào chúng ta có thể suy ra rằng một trong các mô hình tốt hơn mô hình khác theo ý kiến con người.
8. Theo Mathur et al. [22] và Roy et al. [17], chúng tôi thực hiện so sánh mô hình theo cặp của đánh giá con người và chỉ số cấp độ corpus cho các bộ dữ liệu CoNaLa và Hearthstone. Chúng tôi bắt đầu bằng cách tính toán sự khác biệt trong điểm số chỉ số cấp độ corpus cho tất cả các cặp mô hình được đánh giá trên bộ dữ liệu nhất định. Sau đó chúng tôi chia những cặp mô hình này thành một số nhóm theo sự khác biệt trong điểm số chỉ số; chúng tôi cũng có một nhóm bổ sung cho các cặp mà chỉ số không thể phân biệt. Đối với mỗi cặp trong mỗi nhóm, chúng tôi kiểm tra xem đánh giá con người có đồng ý với đánh giá chỉ số hay không, tức là liệu con người có phân biệt cặp mô hình hay không.

Sẽ thú vị khi thực hiện phân tích so sánh các chỉ số ở cấp độ đoạn mã. Tuy nhiên, Mathur et al. [22] lập luận rằng cần thiết phải thu thập ít nhất 15 đánh giá con người cho mỗi đoạn mã để cung cấp điểm số ổn định và phân tích hiệu suất chỉ số ở cấp độ đoạn mã. Vì chúng tôi có thể thu thập bốn điểm cho mỗi đoạn mã đối với bộ dữ liệu Hearthstone và 4,5 điểm cho mỗi đoạn mã đối với bộ dữ liệu CoNaLa, chúng tôi chọn không phân tích hiệu suất chỉ số ở cấp độ đoạn mã.

4.1 Bộ dữ liệu và mô hình

Trong nghiên cứu của chúng tôi, chúng tôi xem xét hai bộ dữ liệu khác nhau: CoNaLa [12] và Card2code Hearthstone [10]. Chúng tôi tập trung vào các bộ dữ liệu chứa mã Python tổng quát, để các bộ dữ liệu không phải Python như Spider (chứa SQL) [7] và JuICe (chứa Jupyter Notebooks) [37] ngoài phạm vi. Chúng tôi cũng để bộ dữ liệu CodeXGLUE [13] ngoài phạm vi, vì các vấn đề code to text trong bộ dữ liệu CodeXGLUE đến từ bộ dữ liệu Concode [38], tập trung vào mã Java. Cho sự phổ biến cao mà bộ dữ liệu CodeXGLUE đã đạt được gần đây, sẽ thú vị khi mở rộng nghiên cứu của chúng tôi sau này cho bộ dữ liệu này.

Đối với cả hai bộ dữ liệu, chúng tôi sử dụng các mô hình được đề xuất bởi các tác giả trong các công trình trước đó. Chúng tôi sử dụng các triển khai gốc, siêu tham số, và—nếu có thể—các trọng số được huấn luyện gốc hoặc mã được sinh ra bởi mô hình như được cung cấp bởi các tác giả.

4.1.1 CoNaLa. Bộ dữ liệu CoNaLa được thu thập bởi Yin et al. [39] và bao gồm 2.879 ví dụ (chia thành 2.379 ví dụ huấn luyện và 500 ví dụ kiểm tra), được thu thập từ Stack Overflow và sau đó được sắp xếp thủ công bởi các người chú thích con người. Ngoài bộ dữ liệu chính, Yin et al. cũng cung cấp một bộ dữ liệu lớn được khai thác tự động bao gồm các câu hỏi "how to" trên Stack Overflow làm ý định huấn luyện và các dòng liền kề từ các khối mã trong câu trả lời làm triển khai ứng viên cho ý định. Bộ dữ liệu này có hơn một trăm nghìn ví dụ. Một số mô hình mà chúng tôi xem xét sử dụng nó để huấn luyện.

Bộ dữ liệu CoNaLa có các tính năng sau:
• Bộ dữ liệu CoNaLa có sự đa dạng ý định hợp lý bao phủ nhiều phương pháp được sử dụng trong Python (so với, ví dụ, bộ dữ liệu Card2Code [10], dành riêng cho việc sinh các lớp với cấu trúc rất cứng nhắc).
• Ý định trong bộ dữ liệu CoNaLa được chi tiết và viết bằng ngôn ngữ tự nhiên, phân biệt nó với, ví dụ, bộ dữ liệu Docstrings [11], trong đó các ý định khá ngắn và trong nhiều trường hợp một lập trình viên con người sẽ gặp vấn đề khi viết mã đúng chỉ với ý định.
• Có sự lựa chọn tương đối phong phú của các mô hình có sẵn công khai đã được đánh giá trên bộ dữ liệu này (so với các bộ dữ liệu khác), cho phép chúng tôi có nhiều so sánh hơn.
• Các mô hình hoạt động tốt nhất được đánh giá trên bộ dữ liệu CoNaLa có điểm BLEU khoảng 30, cho phép có các đoạn mã kiểm tra được sinh ra có chất lượng cả cao và thấp. Ví dụ, mô hình tốt nhất được đánh giá trên bộ dữ liệu Docstrings có BLEU 12,1, tương ứng với đa số các đoạn mã có chất lượng thấp, khiến việc phân biệt đáng tin cậy giữa chúng khó khăn hơn cho những người chấm điểm con người.
• Các đoạn mã CoNaLa thường rất ngắn, với đa số tuyệt đối của chúng là một dòng mã duy nhất. Điều này hạn chế khả năng sử dụng có thể của các chỉ số CodeBLEU và RUBY tính đến cấu trúc mã.

Chúng tôi đánh giá năm mô hình trên bộ dữ liệu CoNaLa. Một trong các mô hình chúng tôi xem xét là mô hình CoNaLa cơ sở [12], một mô hình khác là Codex [2], và ba mô hình khác là các mô hình tranX dựa trên Transformer. Mô hình tranx-annot được huấn luyện trên bộ dữ liệu CoNaLa chính; best-tranx cũng được tiền huấn luyện trên phiên bản được khai thác tự động lớn hơn của CoNaLa trước khi được huấn luyện trên bộ dữ liệu CoNaLa chính; best-tranx-rerank là phiên bản nâng cao của mô hình thứ hai sử dụng xử lý hậu kỳ sắp xếp lại (tức là, sắp xếp lại các dự đoán n-tốt nhất để tăng chất lượng đầu ra). Đối với mỗi mô hình này, chúng tôi sử dụng thiết lập tiêu chuẩn như được cung cấp trong gói tái tạo. Cuối cùng, chúng tôi chạy Codex [2], cụ thể là phiên bản davinci của nó, trong chế độ Q&A. Theo khuyến nghị của các tác giả, chúng tôi không tinh chỉnh Codex trên phần huấn luyện CoNaLa mà cung cấp cho nó ba đoạn mã làm ví dụ. Nghĩa là, mỗi đoạn mã được sinh ra thông qua OpenAI Q&A API để sinh mã Python, và ba cặp ý định-đoạn mã được cung cấp làm ví dụ. Quan trọng là nhấn mạnh rằng thiết lập chính xác của các mô hình (như lựa chọn siêu tham số hoặc cấu hình) không quan trọng đối với nghiên cứu của chúng tôi, vì chúng tôi không cố gắng ước lượng mô hình nào tốt hơn một cách khách quan, mà tập trung vào việc nghiên cứu đánh giá chỉ số của các đầu ra của các mô hình sinh mã. Do đó, yêu cầu không tầm thường duy nhất mà các đầu ra nên thỏa mãn là các mô hình khác nhau nên tạo ra các đoạn mã có chất lượng khác nhau cho cùng một công thức vấn đề (để có thể tạo ra các mô hình tổng hợp có chất lượng khác biệt đáng kể).

4.1.2 Card2Code Hearthstone. Card2Code là một cặp bộ dữ liệu được dẫn xuất từ các trò chơi thẻ sưu tập Magic the Gathering và Hearthstone; trong nghiên cứu của chúng tôi, chúng tôi tập trung vào bộ dữ liệu Hearthstone vì nó phổ biến hơn trong các nhà nghiên cứu. Bộ dữ liệu Hearthstone chứa 665 cặp mô tả thẻ Hearthstone và các đoạn mã Python tương ứng. Mỗi đoạn mã là một triển khai lớp có thể được sử dụng trong trình mô phỏng Hearthbreaker Hearthstone [40] để mô tả logic của thẻ. Bộ dữ liệu được chia thành 533 cặp huấn luyện, 66 cặp xác thực, và 66 cặp kiểm tra. Bộ dữ liệu Hearthstone có các tính năng sau:

• Vì các ý định là mô tả của các thẻ Hearthstone phải tuân thủ ký hiệu Hearthbreaker, mã được sinh ra có cấu trúc tương đối cứng nhắc.
• Vấn đề sinh mã rất đặc biệt: mỗi tác vụ yêu cầu mô hình sinh ra một lớp. Các đoạn mã có đề cương rất tương tự, và sự khác biệt giữa các đoạn mã khác nhau bị hạn chế: mỗi đoạn mã là một lớp kế thừa từ một trong ba lớp cha (MinionCard, SpellCard và WeaponCard). Hầu như mọi đoạn mã có chính xác hai phương thức: một constructor và một phương thức với tên phụ thuộc vào lớp cha (use cho SpellCard, create_weapon cho WeaponCard). Do đó, tính tổng quát của các kết luận chúng ta có thể suy ra từ kết quả bị hạn chế.
• Mã được sinh ra tương đối dài và phức tạp, cho phép áp dụng các chỉ số CodeBLEU và RUBY tính đến cấu trúc mã cơ bản.

Chỉ có hai mô hình có sẵn công khai được đánh giá trên bộ dữ liệu Hearthstone. Một trong các mô hình là mô hình neural cú pháp NL2code [3], và một mô hình khác là mạng neural tích chập cấu trúc dựa trên ngữ pháp GCNN [4]. Đối với NL2code, chúng tôi sử dụng đầu ra của mô hình được cung cấp bởi các tác giả, và đối với GCNN chúng tôi sử dụng thiết lập tiêu chuẩn như được cung cấp trong gói tái tạo, nhưng giới hạn huấn luyện đến 30 epoch vì thiết lập tiêu chuẩn của 1000 epoch (như được viết trong gói tái tạo) không khả thi với tài nguyên tính toán của chúng tôi. Mô hình Codex được huấn luyện trước rõ ràng quen thuộc với bộ dữ liệu vì nó cung cấp các đoạn mã tham chiếu làm đầu ra, vì vậy chúng tôi không xem xét nó. Đặc biệt, không có giới hạn chặt chẽ về số lượng token được sinh ra, Codex thành công sinh ra một số lớp từ bộ dữ liệu kiểm tra trong một lần chạy duy nhất. Điều này cho thấy rằng Codex có khả năng tái tạo toàn bộ tệp mà nó đã thấy trong quá trình huấn luyện, bao gồm những tệp từ bộ dữ liệu Hearthstone.

Để kiểm tra ý nghĩa của sự khác biệt trong điểm số chỉ số, chúng tôi sử dụng lấy mẫu bootstrap theo cặp [18] cho điểm số chỉ số của các mô hình được đánh giá trên phần kiểm tra của bộ dữ liệu.

4.2 RQ1: Hiệu suất mô hình cấp độ corpus

Để giải quyết RQ1, chúng tôi so sánh ý nghĩa của sự khác biệt điểm số chỉ số ở cấp độ corpus. Đối với các chỉ số định nghĩa điểm số cấp độ corpus như một tổng hợp của điểm số cấp độ đoạn mã, có thể sử dụng các kỹ thuật như kiểm tra dấu hạng Wilcoxon [41] để so sánh các mô hình. Tuy nhiên, có các chỉ số như BLEU là cấp độ corpus theo thiết kế, sao cho việc lấy trung bình đơn giản của điểm số từng đoạn mã trên corpus không cho điểm số chỉ số cấp độ corpus (xem phụ lục A.1 để biết thêm chi tiết). Do đó, kiểm tra Wilcoxon không áp dụng được trong trường hợp này. Điều này hạn chế chúng tôi sử dụng kiểm tra ý nghĩa ngẫu nhiên để so sánh điểm số cấp độ corpus, là một thực hành phổ biến trong cộng đồng dịch máy [42]. Theo Graham et al. [42], có ít khác biệt thực tế giữa việc sử dụng bootstrap, bootstrap theo cặp và ngẫu nhiên hóa xấp xỉ để kiểm tra ý nghĩa. Chúng tôi chọn lấy mẫu bootstrap theo cặp để kiểm tra ý nghĩa. Để kiểm tra ý nghĩa thống kê, chúng tôi lấy 1000 mẫu bootstrap.

4.3 RQ2: Ý nghĩa của điểm số chỉ số tự động

Để giải quyết RQ2, chúng tôi xem xét ý nghĩa của sự khác biệt trong điểm số chỉ số cho các cặp mô hình khác nhau. Chúng tôi mong đợi rằng ý nghĩa của sự khác biệt trong điểm số chỉ số sẽ thay đổi với sự khác biệt trong điểm số (sao cho đối với một cặp mô hình với điểm BLEU 20 và 80, có khả năng hơn là một trong các mô hình sẽ tốt hơn mô hình khác, so với cặp mô hình với điểm BLEU 29,5 và 30). Do đó, chúng tôi theo Roy et al. [17] và chia các cặp mô hình thành các nhóm theo sự khác biệt trong điểm số. Thành phần nhóm ([0,1), [1,2) v.v.) hơi khác nhau cho bộ dữ liệu Hearthstone và CoNaLa. Nó được xác định thực nghiệm để có số lượng cặp tương tự trong mỗi nhóm. Chúng tôi cố gắng có số lượng cặp có thể so sánh trong mỗi nhóm để có số lượng đáng kể cặp trong mỗi nhóm, sao cho có thể rút ra kết luận mạnh mẽ về mặt thống kê.

Chúng tôi mở rộng tập hợp các mô hình ML gốc với các mô hình tổng hợp được xây dựng theo cách tiếp cận của Roy et al. [17]. Chúng tôi xây dựng đầu ra của các mô hình tổng hợp từ đầu ra của các mô hình thực. Có một số lý do tại sao chúng tôi sử dụng các mô hình tổng hợp:

(1) Có sự khan hiếm tương đối của các mô hình có sẵn. Trong trường hợp tốt nhất của bộ dữ liệu CoNaLa, chúng tôi chỉ có năm mô hình có chất lượng khác nhau, có thể không cung cấp đủ dữ liệu để đánh giá khả năng áp dụng chỉ số. Việc sử dụng các mô hình tổng hợp cho phép chúng tôi bao phủ một phạm vi giá trị chỉ số đa dạng hơn nhiều mà không cần huấn luyện nhiều mô hình mới.

(2) Ngay cả khi có sự đa dạng lớn của các mô hình sao cho sẽ có đủ điểm dữ liệu để so sánh chỉ số phù hợp, nó sẽ đòi hỏi đầu tư khổng lồ trong việc gắn nhãn dữ liệu. Ví dụ, trong nghiên cứu này, chúng tôi nghiên cứu đầu ra của 85 mô hình tổng cộng (bao gồm cả mô hình gốc và tổng hợp) chỉ cho bộ dữ liệu CoNaLa, với mỗi đầu ra bao gồm 472 đoạn mã. Nếu tất cả 85 mô hình đều độc lập, nó sẽ đòi hỏi những người có kinh nghiệm về Python gắn nhãn hơn 40.000 đoạn mã. Vì chúng tôi coi cần thiết phải thu thập ít nhất ba điểm cho mỗi đoạn mã, quy trình như vậy sẽ cực kỳ khó khăn hoặc đắt đỏ.

(3) Cải thiện hoặc làm xấu đi điểm số mô hình dẫn đến một tập hợp các mô hình tổng hợp với điểm số chỉ số và con người tương đối gần nhau. Điều này cho phép chúng tôi so sánh nhiều mô hình với điểm số tương đối gần và kiểm tra ý nghĩa của sự khác biệt tương đối nhỏ trong chúng. Điều này có liên quan đến các nhà nghiên cứu và thực hành, vì các cải tiến so với các mô hình tiên tiến thường đến trong các bước tăng nhỏ.

4.3.1 Xây dựng mô hình tổng hợp. Chúng tôi tạo ra một mô hình tổng hợp bằng cách bắt đầu với đầu ra của một số mô hình gốc và thay thế X% các đoạn mã được đánh giá tồi tệ nhất của nó bằng đoạn mã được đánh giá tốt nhất cho vấn đề. Chất lượng của đoạn mã được đánh giá theo điểm đánh giá con người. Nếu đoạn mã được chọn đã là đoạn mã được đánh giá tốt nhất, nó được bỏ qua. Quy trình ngược được áp dụng cho các mô hình tổng hợp được làm xấu đi. Chúng tôi tiếp tục quy trình thay thế cho đến khi X% đoạn mã được thay đổi hoặc không còn đoạn mã nào để thay đổi.

Theo Roy et al. [17], chúng tôi xem xét tám tỷ lệ khác nhau cho các thay thế: thay thế 1%, 3%, 5%, 10%, 15%, 20%, 25%, và 30% các đoạn mã được sinh ra. Tỷ lệ thay thế của chúng tôi giống hệt với Roy et al. với một biến thể nhỏ: chúng tôi thay thế 3% bộ dữ liệu thay vì 2% được thay thế bởi Roy et al. Quy trình này mang lại 5×8×2=80 mô hình tổng hợp cho CoNaLa và 2×8×2=32 mô hình tổng hợp cho bộ dữ liệu Hearthstone. Sau đó, chúng tôi thêm các mô hình gốc và loại bỏ trùng lặp bằng cách loại bỏ các mô hình có đầu ra hoàn toàn giống hệt nhau. Điều này để lại cho chúng tôi 81 mô hình cho CoNaLa và 29 mô hình cho Hearthstone mà chúng tôi sử dụng cho phân tích của chúng tôi trong RQ2. Chúng tôi xem xét tất cả các kết hợp theo cặp của các mô hình (cả tổng hợp và gốc) và thực hiện kiểm tra khác biệt theo cặp cho mỗi chỉ số.

4.4 RQ3: Sự đồng ý giữa chỉ số và đánh giá con người

Để giải quyết RQ3, chúng tôi đánh giá mức độ đồng ý giữa đánh giá con người và điểm số chỉ số ở cấp độ corpus. Để làm như vậy, chúng tôi thực hiện các kiểm tra ý nghĩa cấp độ corpus để kiểm tra xem chỉ số và dự đoán con người có đồng ý cho mỗi cặp mô hình hay không. Tương tự như câu hỏi nghiên cứu trước đó, chúng tôi sử dụng cả mô hình gốc và tổng hợp mà chúng tôi đã sử dụng trong RQ2.

4.4.1 Nhóm cho đánh giá cấp độ corpus. Có một số lựa chọn cho sự bất đồng giữa người đánh giá con người và chỉ số cho một cặp mô hình A và B nhất định:
• Khi A tốt hơn B theo chỉ số, nhưng các mô hình tương đương theo người đánh giá con người (lỗi Type-I).
• Khi các mô hình A và B tương đương theo chỉ số, nhưng một trong các mô hình tốt hơn theo người đánh giá con người (lỗi Type-II).
• Khi mô hình A tốt hơn mô hình B theo chỉ số, nhưng theo người đánh giá con người, mô hình B tốt hơn mô hình A (lỗi Type-I).

Chúng tôi xem xét tất cả các kết hợp theo cặp của các mô hình (cả tổng hợp và gốc) và thực hiện kiểm tra khác biệt theo cặp cho đánh giá con người và chỉ số. Sử dụng điểm con người tổng hợp làm sự thật cơ bản, chúng tôi định lượng lỗi Type-I và Type-II của chỉ số. Vì chúng tôi mong đợi rằng xác suất một chỉ số mắc lỗi cho một cặp mô hình phụ thuộc vào sự khác biệt của điểm số mô hình, chúng tôi chia dữ liệu về lỗi chỉ số thành một số nhóm. Nhóm NS tương ứng với các trường hợp mà sự khác biệt trong điểm số mô hình không có ý nghĩa, theo chỉ số nhất định. Tất cả lỗi trong nhóm này là lỗi Type-I. Các nhóm khác tương ứng với các trường hợp mà sự khác biệt trong điểm số mô hình có ý nghĩa, theo chỉ số. Thành phần nhóm cho RQ3 giống hệt với cái chúng tôi chọn cho RQ2.

4.4.2 Đánh giá con người. Để có đánh giá con người về các mô hình được xem xét, chúng tôi tạo ra một cuộc khảo sát, trong đó chúng tôi yêu cầu các lập trình viên đánh giá các đoạn mã. Các đoạn mã được trình bày từng cái một và được chọn ngẫu nhiên từ nhóm kết hợp các đoạn mã được sinh ra bởi các mô hình và các đoạn mã tham chiếu. Những người chấm điểm không biết nguồn gốc của mỗi đoạn mã. Những người chấm điểm đánh giá các đoạn mã trên thang điểm từ 0 đến 4, với các mô tả điểm sau:

0: Đoạn mã hoàn toàn không hữu ích, nó không liên quan đến vấn đề.
1: Đoạn mã hơi hữu ích, nó chứa thông tin liên quan đến vấn đề, nhưng dễ viết giải pháp từ đầu hơn.
2: Đoạn mã có phần hữu ích, nó đòi hỏi thay đổi đáng kể (so với kích thước của đoạn mã), nhưng vẫn hữu ích.
3: Đoạn mã hữu ích nhưng cần được thay đổi một chút để giải quyết vấn đề.
4: Đoạn mã rất hữu ích, nó giải quyết vấn đề.

Những người chấm điểm không phải đánh giá tất cả các đoạn mã trong bộ dữ liệu và có thể dừng lại bất cứ lúc nào.

4.4.3 Bộ dữ liệu CoNaLa. Đối với bộ dữ liệu CoNaLa, có 2.860 đoạn mã cần đánh giá: 5×472 đoạn mã được sinh ra bởi các mô hình cộng với 500 đoạn mã tham chiếu (đối với một số ý định, bộ dữ liệu chứa nhiều hơn một đoạn mã tham chiếu). 16 người tham gia đã tham gia cuộc khảo sát của chúng tôi, và trung bình, chúng tôi nhận được 4,49 điểm cho mỗi đoạn mã được sinh ra bởi mô hình. Hình 2 cho thấy phân phối số điểm. Ba trong số những người chấm điểm có ít hơn hai năm kinh nghiệm với Python, sáu có hai đến ba năm kinh nghiệm, và bảy đang lập trình bằng Python trong bốn năm trở lên. Chúng tôi đã tuyển dụng những người chấm điểm từ hàng ngũ đồng nghiệp của chúng tôi và thông qua các bài đăng trong tài khoản Twitter khoa học của chúng tôi. Vào thời điểm chấm điểm, tất cả những người chấm điểm đều đang nghiên cứu trong lĩnh vực kỹ thuật phần mềm khoa học máy tính.

4.4.4 Bộ dữ liệu Hearthstone. Tương tự như bộ dữ liệu CoNaLa, chúng tôi cũng đã chạy một cuộc khảo sát trong đó các lập trình viên đánh giá các đoạn mã. Các đoạn mã được trình bày từng cái một cùng với hình ảnh thẻ Hearthstone, và những người chấm điểm đánh giá xem đoạn mã có đại diện chính xác cho thẻ hay không. Hình 3 cho thấy một ví dụ về hình ảnh thẻ cùng với đoạn mã tương ứng.

Có 198 đoạn mã cần đánh giá: 2×66 đoạn mã được sinh ra bởi các mô hình cộng với 66 đoạn mã tham chiếu. Bốn người tham gia đã tham gia cuộc khảo sát, mỗi người tham gia đã chấm điểm tất cả các đoạn mã. Hai trong số những người tham gia có ba năm trở lên kinh nghiệm chơi Hearthstone, và hai người tham gia khác đã học các quy tắc thông qua video và sách hướng dẫn. Một trong những người chấm điểm có 1,5 năm kinh nghiệm với Python, hai có hai năm kinh nghiệm, và một đang lập trình bằng Python trong bốn năm.

5 KẾT QUẢ

5.1 RQ1: Hiệu suất mô hình cấp độ corpus

5.1.1 Bộ dữ liệu CoNaLa. Phần kiểm tra của bộ dữ liệu CoNaLa bao gồm 500 đoạn mã tham chiếu, nhưng một số ý định xuất hiện nhiều hơn một lần, vì vậy tổng cộng, có 472 ý định duy nhất. Các tham chiếu khác nhau tương ứng với cùng một ý định, được tính như các phần của corpus tham chiếu. Chúng tôi xem xét năm mô hình khác nhau được huấn luyện trên bộ dữ liệu CoNaLa: CoNaLa cơ sở (baseline), tranX được huấn luyện trên bộ dữ liệu chính (tranx-annot), phiên bản tốt nhất của tranX của Yin et al. [5] với tiền huấn luyện trên phiên bản không được làm sạch của CoNaLa và không có sắp xếp lại (best-tranx), phiên bản tốt nhất của tranX với cùng tiền huấn luyện và sắp xếp lại (best-tranx-rerank) [6], và Codex [2]. Chúng tôi tính toán điểm BLEU, ROUGE-L, METEOR, ChrF, CodeBLEU, và RUBY cho đầu ra của các mô hình này (nhận điểm cho mỗi đoạn mã kiểm tra). Bảng 2 cho thấy giá trị chỉ số cho tất cả các mô hình trên bộ dữ liệu CoNaLa.

Cùng với các chỉ số tự động, chúng tôi báo cáo điểm người đánh giá tổng hợp (xem hàng Human trong Bảng 2). Chúng tôi chuyển đổi tất cả các chỉ số về thang điểm 0–100 bằng cách nhân với một hệ số thích hợp: chúng tôi nhân điểm người đánh giá với 25 và nhân điểm chỉ số tự động với 100, nếu điểm chỉ số nằm trong khoảng [0,1]. Cùng với điểm số, chúng tôi báo cáo khoảng tin cậy cho mỗi chỉ số. Khoảng tin cậy được tính toán với sự hỗ trợ của bootstrap trên 1.000 lần lấy mẫu lại; X+Y−Z nên được đọc là "95% các mô hình được lấy mẫu lại mang lại điểm trong khoảng [X−Z,X+Y]".

Chỉ số BLEU không nhận ra sự khác biệt về chất lượng giữa Codex và best-tranx-rerank, và giữa Codex và best-tranx. Chỉ số RUBY không nhận ra sự khác biệt về chất lượng giữa bất kỳ ba mô hình nào sau đây: baseline, tranx-annot, và best-tranx. Chỉ số CodeBLEU không nhận ra sự khác biệt về chất lượng giữa bất kỳ hai mô hình nào từ những mô hình sau: tranx-annot, best-tranx, và best-tranx-rerank. Đối với năm mô hình gốc được đánh giá trên bộ dữ liệu CoNaLa, sự khác biệt trong điểm BLEU, RUBY, hoặc CodeBLEU không phải lúc nào cũng có ý nghĩa thống kê. Điều này quan trọng, vì hiện tại các mô hình sinh mã được đánh giá bằng CodeBLEU hoặc BLEU, và điểm mô hình thường được cung cấp mà không có bất kỳ dữ liệu nào về ý nghĩa thống kê.

5.1.2 Bộ dữ liệu Hearthstone. Đối với bộ dữ liệu Hearthstone, chúng tôi chỉ đánh giá hai mô hình khác nhau có sẵn: một mô hình neural cú pháp NL2Code [3] và một mạng neural tích chập cấu trúc dựa trên ngữ pháp (GCNN) [4]. Chúng tôi tính toán điểm BLEU, ROUGE-L, METEOR, ChrF, CodeBLEU, và RUBY cho đầu ra của các mô hình này, nhận điểm cho mỗi đoạn mã kiểm tra. Định dạng mà chúng tôi báo cáo điểm giống với định dạng mà chúng tôi trình bày điểm CoNaLa. Chúng tôi huấn luyện mô hình GCNN trong 30 epoch, vì không có số epoch được khuyến nghị trong bài báo gốc [4], và giá trị mặc định của 1.000 epoch không khả thi. Điều này có thể là lý do tại sao mô hình GCNN chúng tôi huấn luyện hoạt động tương đối tệ hơn NL2Code trái ngược với kết quả của bài báo gốc [4].

Theo các chỉ số ROUGE-L, METEOR, và BLEU, mô hình NL2Code tốt hơn GCNN với độ tin cậy >95%, xem Bảng 5.

Chúng tôi thấy rằng ngay cả đối với các mô hình không tổng hợp chúng tôi xem xét trên bộ dữ liệu CoNaLa và Hearthstone, cải tiến trong điểm chỉ số có thể là bề ngoài và không có ý nghĩa thống kê. Điều này nhấn mạnh sự cần thiết phải kiểm tra ý nghĩa của cải tiến trong chất lượng mô hình đối với tác vụ sinh mã.

5.2 RQ2: Ý nghĩa của điểm số chỉ số tự động

5.2.1 Bộ dữ liệu CoNaLa. Trong Bảng 3, chúng tôi trình bày dữ liệu về ý nghĩa của sự khác biệt trong điểm mô hình. Đối với mỗi cặp mô hình, chúng tôi tính toán sự khác biệt trong điểm của họ, theo từng chỉ số chúng tôi xem xét và kiểm tra xem sự khác biệt có có ý nghĩa hay không, theo quy trình lấy mẫu bootstrap theo cặp. Mỗi ô bảng chứa số cặp mô hình tương ứng với chỉ số và sự khác biệt được đề cập trong hàng và cột, tương ứng. Ví dụ, có 192 cặp mô hình với sự khác biệt trong điểm BLEU trong khoảng [0,2), mà sự khác biệt này có ý nghĩa. Chúng tôi chia các điểm có thể thành bốn nhóm khác nhau—[0,2), [2,5), [5,10), [10,100)—và đặt mỗi cặp mô hình vào nhóm tương ứng. Kết quả cho thấy rằng ngoại trừ chỉ số BLEU, nếu sự khác biệt trong điểm chỉ số của hai mô hình lớn hơn hai điểm, thì có thể tuyên bố với ít nhất 95% độ tin cậy rằng sự khác biệt có ý nghĩa. Dữ liệu về độ tin cậy của ý nghĩa sự khác biệt có thể được thu được trực tiếp từ bảng: đối với mỗi cặp nhóm-chỉ số, độ tin cậy được cho bởi S/(S+NS). Ở đây S là số cặp mô hình, mà sự khác biệt trong điểm có ý nghĩa, và NS là số cặp mô hình, mà sự khác biệt trong điểm không có ý nghĩa thống kê. Kết quả cũng cho thấy rằng nếu sự khác biệt trong điểm của hai mô hình nhỏ hơn hai điểm, không thể tuyên bố rằng một trong các mô hình tốt hơn mà không thực hiện các kiểm tra thống kê bổ sung. Hơn nữa, nếu sự khác biệt trong điểm BLEU nhỏ hơn năm điểm, các kiểm tra thống kê bổ sung cần thiết để tuyên bố rằng sự khác biệt có ý nghĩa.

5.2.2 Bộ dữ liệu Hearthstone. Bảng 6 trình bày sự phụ thuộc giữa sự khác biệt trong điểm mô hình theo các chỉ số và khả năng của chúng xác định mô hình nào tốt hơn với ít nhất 95% độ tin cậy. Kết quả cho thấy rằng đối với bộ dữ liệu Hearthstone, sự khác biệt trong điểm nhỏ hơn hai điểm theo bất kỳ chỉ số nào làm cho không thể tuyên bố rằng một trong các mô hình tốt hơn một cách có ý nghĩa mà không có các kiểm tra thống kê bổ sung. Đối với các chỉ số BLEU và CodeBLEU được cộng đồng áp dụng—và chỉ đối với chúng—không thể tuyên bố rằng một trong các mô hình tốt hơn một cách có ý nghĩa nếu sự khác biệt trong điểm mô hình nhỏ hơn bốn điểm. Tương tự như kết quả của chúng tôi trên bộ dữ liệu CoNaLa, phát hiện này nhấn mạnh rằng sự khác biệt nhỏ trong điểm chỉ số nên được báo cáo cùng với các kiểm tra thống kê chứng minh ý nghĩa của sự khác biệt.

Các phát hiện của chúng tôi về ý nghĩa của cải tiến điểm chỉ số mở rộng các quan sát chúng tôi đã đưa ra trong phần trước. Chúng tôi thấy rằng đối với không có chỉ số nào chúng tôi xem xét, cải tiến điểm nhỏ hơn hai điểm đủ để tuyên bố cải tiến có ý nghĩa thống kê mà không có các kiểm tra bổ sung. Hơn nữa, cải tiến điểm đủ để tuyên bố cải tiến có ý nghĩa thống kê đối với các chỉ số BLEU và CodeBLEU được cộng đồng áp dụng thậm chí còn cao hơn.

5.3 RQ3: Sự đồng ý giữa chỉ số và đánh giá con người

5.3.1 Bộ dữ liệu CoNaLa. Chúng tôi cũng thực hiện đánh giá con người của bộ dữ liệu CoNaLa và so sánh nó với kết quả của các chỉ số tự động. Chúng tôi tính toán điểm con người "sự thật cơ bản" theo thuật toán M-MSR được đề xuất bởi Ma et al. [35].

Đối với các mô hình không tổng hợp, các chỉ số khác nhau cho thấy kết quả khác nhau trong việc nhận ra ý nghĩa của sự khác biệt trong chất lượng đầu ra của các mô hình. Sự thật cơ bản con người chúng tôi thu được từ các điểm được thu thập cho thấy rằng tất cả sự khác biệt trong điểm mô hình đều có ý nghĩa. Xếp hạng của các mô hình như sau: Codex > best-tranx-rerank > best-tranx > tranx-annot > baseline.

Kết quả của các chỉ số ChrF, ROUGE-L, và METEOR đồng ý với phán đoán con người (xem Bảng 2), trong khi BLEU, RUBY, và CodeBLEU không đồng ý với người đánh giá cho ít nhất một cặp mô hình.

Chúng tôi trình bày so sánh đánh giá con người với các chỉ số tự động trên các mô hình tổng hợp trong Bảng 4. Mỗi cột chứa dữ liệu về các cặp mô hình với sự khác biệt có ý nghĩa thống kê trong điểm chỉ số trong khoảng nhất định. Cột Delta: NS chứa tất cả các cặp mô hình, mà sự khác biệt trong điểm không có ý nghĩa thống kê. Mỗi ô bảng chứa số cặp mô hình tương ứng với chỉ số được đề cập trong hàng tương ứng và sự khác biệt được đề cập trong cột. Ví dụ, có 187 cặp mô hình với sự khác biệt trong điểm BLEU trong khoảng [0,2), mà sự khác biệt này có ý nghĩa. Cột "Mismatches" liệt kê số cặp mô hình mà đánh giá chỉ số không đồng ý với đánh giá con người. Ví dụ, trong số 187 cặp mô hình với sự khác biệt trong điểm BLEU trong khoảng [0,2) mà sự khác biệt có ý nghĩa, đối với 2,7% đánh giá chỉ số không đồng ý với đánh giá con người.

Đối với tỷ lệ bất đồng của các chỉ số cấp độ corpus với điểm con người tổng hợp, chúng ta có thể thấy những điều sau:

1. Các chỉ số không đáng tin cậy trong việc xác định rằng sự khác biệt giữa các mô hình không có ý nghĩa với tỷ lệ lỗi trên 60% cho mỗi chỉ số chúng tôi xem xét, xem cột Delta: NS.
2. Khi sự khác biệt trong điểm chỉ số nhỏ hơn 5 điểm, không có chỉ số nào đủ đáng tin cậy để mô phỏng phán đoán con người với ít nhất 95% độ chính xác.
3. Đối với nhóm [5,10) của sự khác biệt điểm chỉ số, chỉ có các chỉ số RUBY, ChrF và ROUGE-L có thể mô phỏng phán đoán con người với ít nhất 95% độ chính xác, xem cột Delta: [5, 10).
4. Có thể lập luận rằng trong các chỉ số chúng tôi xem xét, BLEU là tồi tệ nhất trong việc mô phỏng phán đoán con người: mặc dù nó có tỷ lệ bất khớp tổng thể cao thứ hai, nó là chỉ số hoạt động tồi tệ nhất đối với các mô hình với sự khác biệt điểm hơn năm điểm, xem hàng BLEU. Nó cũng là chỉ số duy nhất đôi khi không đồng ý với phán đoán con người đối với cặp mô hình có sự khác biệt điểm hơn 10 điểm.
5. Các chỉ số RUBY và CodeBLEU, được phát triển để đánh giá mã, không hoạt động tốt hơn đáng kể so với các chỉ số có nguồn gốc từ lĩnh vực dịch máy. Hơn nữa, chúng nằm trong số những chỉ số kém tin cậy nhất về tỷ lệ bất khớp tổng thể, xem cột Total mismatch.
6. Tất cả các chỉ số có tỷ lệ lỗi Type-I cao nhất đối với nhóm [2,5), sau đó giảm với sự tăng trong sự khác biệt điểm, xem cột Delta: [2, 5). Điều này có thể được giải thích bằng tỷ lệ bất khớp cao trong nhóm NS, bao gồm các cặp mô hình với sự khác biệt điểm nhỏ nói chung. Nếu chúng ta không xem xét nhóm NS riêng biệt và tổng hợp kết quả theo sự khác biệt trong điểm cặp mô hình, tỷ lệ lỗi cao nhất là đối với nhóm [0,2), tương tự như kết quả của Roy et al. [17].

Khuyến nghị chung cho các nhà thực hành, dựa trên kết quả nghiên cứu của chúng tôi, là sự khác biệt điểm chỉ số ít nhất năm điểm cần thiết để tuyên bố với ít nhất 95% chắc chắn rằng một mô hình tốt hơn mô hình khác trên bộ dữ liệu CoNaLa, nếu phán đoán con người được coi là sự thật vàng. ChrF và ROUGE-L là các chỉ số hoạt động tốt nhất để đánh giá các mô hình sinh mã trong số các chỉ số chúng tôi xem xét.

5.3.2 Bộ dữ liệu Hearthstone. Chúng tôi cũng tiến hành đánh giá con người của bộ dữ liệu Hearthstone. Tương tự như bộ dữ liệu CoNaLa, chúng tôi tính toán điểm con người "sự thật cơ bản" theo thuật toán M-MSR. Đối với các mô hình không tổng hợp, người chấm điểm con người không thể quyết định với độ tin cậy >95% rằng NL2Code tốt hơn GCNN, và điều tương tự cũng đúng đối với các chỉ số CodeBLEU, ChrF, và RUBY.

Đối với tỷ lệ bất đồng của các chỉ số cấp độ corpus với điểm con người tổng hợp trên các mô hình tổng hợp, chúng ta có thể thấy những điều sau:

1. Các chỉ số không đáng tin cậy trong việc xác định rằng sự khác biệt giữa các mô hình không có ý nghĩa. Tuy nhiên, tỷ lệ lỗi tương đối tốt hơn một chút so với quan sát trên bộ dữ liệu CoNaLa: ChrF và ROUGE-L thể hiện tỷ lệ lỗi dưới 60%, xem cột Delta: NS.
2. Tỷ lệ bất khớp tổng thể đối với bộ dữ liệu Hearthstone tệ hơn so với quan sát trên bộ dữ liệu CoNaLa, xem cột Total mismatch. Lý do có thể là chúng tôi chỉ có hai mô hình có sẵn cho bộ dữ liệu, và điểm chỉ số của chúng tương đối gần nhau. Vì tất cả các mô hình tổng hợp được sinh ra từ hai mô hình này, không ngạc nhiên khi điểm của các mô hình tổng hợp cũng khá gần nhau và khó cho các chỉ số phân biệt giữa các mô hình.
3. Không có chỉ số nào đủ đáng tin cậy để phân biệt giữa các mô hình với sự khác biệt điểm nhỏ hơn hai điểm với độ chính xác >95%, xem cột Delta: [1, 2).
4. Một lần nữa, chỉ số BLEU hoạt động kém: tỷ lệ bất khớp tổng thể của nó nằm trong số tồi tệ nhất, và, cùng với METEOR, đây là hai chỉ số duy nhất thất bại trong việc phân biệt tốt giữa các mô hình với sự khác biệt điểm hơn hai điểm, xem hàng BLEU.
5. RUBY và CodeBLEU, các chỉ số được phát triển để đánh giá mã, không hoạt động tốt hơn đáng kể so với các chỉ số có nguồn gốc từ lĩnh vực dịch máy. Hơn nữa, chúng nằm trong số các chỉ số tồi tệ nhất về tỷ lệ bất khớp tổng thể.
6. Không có xu hướng rõ ràng cho tỷ lệ lỗi Type-I trên tất cả các chỉ số, không giống như đối với bộ dữ liệu CoNaLa. Điều này có thể được giải thích bằng việc lựa chọn nhóm khác với cái được chọn cho bộ dữ liệu CoNaLa. Thật không may, việc lựa chọn nhóm tương tự như cái được thực hiện cho bộ dữ liệu CoNaLa sẽ còn ít thông tin hơn: đối với hầu hết các chỉ số, các nhóm [5,10) và [10,100) sẽ gần như trống vì hai mô hình không tổng hợp có sẵn cho bộ dữ liệu này có chất lượng tương đối gần nhau.

Khuyến nghị chung cho các nhà thực hành dựa trên kết quả được thu thập là sự khác biệt điểm chỉ số ít nhất hai điểm cần thiết để tuyên bố với ít nhất 95% chắc chắn rằng một mô hình tốt hơn mô hình khác trên bộ dữ liệu Hearthstone, nếu phán đoán con người được coi là sự thật vàng. Chỉ số ROUGE-L là chỉ số hoạt động tốt nhất để đánh giá các mô hình sinh mã trên bộ dữ liệu này, với ChrF là tốt thứ hai.

6 HÀM Ý NGHIÊN CỨU

Trong công việc này, chúng tôi nghiên cứu khả năng áp dụng của các chỉ số tự động khác nhau — BLEU, ROUGE-L, METEOR, ChrF, RUBY, và CodeBLEU — để đánh giá các mô hình sinh mã.

Dựa trên kết quả, chúng tôi suy ra các khuyến nghị sau cho các nhà thực hành. Thứ nhất, điểm chỉ số nên được báo cáo cùng với dữ liệu về ý nghĩa của sự khác biệt trong điểm. Thứ hai, sự khác biệt trong điểm chỉ số nhỏ hơn hai điểm không đủ để tuyên bố rằng một mô hình tốt hơn mô hình khác, ngay cả khi sự khác biệt có ý nghĩa thống kê. Thứ ba, bất chấp BLEU và CodeBLEU là các chỉ số phổ biến nhất để đánh giá các mô hình sinh mã, chúng tôi khuyến nghị sử dụng ChrF làm chỉ số tiêu chuẩn cho các tác vụ sinh mã. Chúng tôi cũng tin rằng cộng đồng sẽ hưởng lợi từ một chỉ số mới được thiết kế riêng để đánh giá tác vụ sinh mã. Để hỗ trợ việc phát triển chỉ số như vậy, chúng tôi làm cho điểm đánh giá con người được thu thập mở nguồn cho cả hai bộ dữ liệu được nghiên cứu và khuyến khích các nhà nghiên cứu khác sử dụng chúng trong công việc của họ. Cuối cùng, chúng tôi mạnh mẽ khuyến khích các nhà thực hành phát triển mô hình sinh mã xuất bản đầu ra của mô hình của họ, vì gần như không thể quan sát các cải tiến nhỏ nhưng có ý nghĩa trong sinh mã mà không có khả năng thực hiện các kiểm tra thống kê trên cả đầu ra mô hình cũ và mới.

6.1 Công việc tương lai

Sử dụng các quan sát chúng tôi đã đưa ra ở trên, chúng tôi thấy các hướng công việc tương lai sau:

Thứ nhất là mở rộng nghiên cứu này với các ngôn ngữ lập trình, bộ dữ liệu, và mô hình sinh mã khác. Dữ liệu thu được sau đó nên được sử dụng để đánh giá khả năng áp dụng của các chỉ số khác nhau cho bộ dữ liệu cụ thể và hoặc ngôn ngữ lập trình. Trong khi đánh giá như vậy tốn kém và dài, nó sẽ cho phép so sánh các mô hình sinh mã với chắc chắn hơn. Hướng hành động tốt nhất cũng sẽ bao gồm đánh giá con người đầy đủ của các mô hình sinh mã như được thực hiện cho ví dụ, dịch máy [43]. Một thách thức cụ thể cho bộ dữ liệu gắn nhãn sẽ là thu thập hơn 15 đánh giá cho mỗi đoạn mã. Theo các phát hiện của Mathur et al. [22], điều này sẽ cho phép đánh giá chất lượng của các mô hình khác nhau ở cấp độ đoạn mã và theo dõi chi tiết của các cải tiến.

Thứ hai là tạo ra các chỉ số mới để đánh giá chất lượng của các mô hình sinh mã. Xem xét thành công tương đối của các chỉ số được nâng cao bằng ML cho các tác vụ xử lý ngôn ngữ tự nhiên [31,32] và cho tóm tắt mã [17], chúng tôi suy đoán rằng một hướng đầy hứa hẹn cho một chỉ số mới sẽ là một tương tự của BERTScore [44], sẽ sử dụng các embedding từ một mô hình ngôn ngữ lớn để tính toán điểm tương tự giữa tham chiếu và các đoạn mã ứng viên.

7 MỐI ĐE DỌA ĐỐI VỚI TÍNH HỢP LỆ

7.1 Mối đe dọa bên ngoài

Trong bài báo này, chúng tôi xử lý các mối đe dọa tính hợp lệ bên ngoài như những thiếu sót có thể ảnh hưởng đến khả năng tổng quát hóa nghiên cứu của chúng tôi cho các tình huống khác. Trước hết, nghiên cứu của chúng tôi dựa trên hai bộ dữ liệu Python: một bộ dữ liệu các dòng Python đơn và một bộ dữ liệu Card2Code [10] đặc biệt, mà các mô hình được cho là sinh ra các lớp với cấu trúc rất cụ thể. Sẽ thú vị khi khám phá các bộ dữ liệu khác; thật không may, có sự lựa chọn hạn chế của các bộ dữ liệu hiện có, và rất ít mô hình có thể chạy trên một bộ dữ liệu cụ thể thường có sẵn công khai. Bộ dữ liệu thú vị nhất bị để ngoài phạm vi của bài báo này là Docstrings [11]. Thật không may, các mô hình hiện có được huấn luyện trên nó hoạt động khá kém. Đặc biệt, mô hình tốt nhất có sẵn có điểm BLEU là 12,1 [16], có nghĩa là điểm con người dự kiến cho đầu ra của nó sẽ khá kém.

Mối đe dọa lựa chọn bộ dữ liệu liên quan chặt chẽ đến mối đe dọa lựa chọn mô hình. Đối với mỗi bộ dữ liệu chúng tôi xem xét, ngoại trừ CoNaLa, có sự thiếu hụt tương đối của các mô hình có sẵn; đặc biệt, chúng tôi chạy tất cả các mô hình có sẵn công khai cho bộ dữ liệu Hearthstone. Chúng tôi đã liên hệ với các tác giả của các mô hình không được mở nguồn, nhưng thật không may không nhận được phản hồi. Có thể rằng lựa chọn mô hình khác nhau sẽ mang lại kết quả khác nhau.

Tất cả các bộ dữ liệu chúng tôi sử dụng có các đoạn mã được viết bằng Python. Trong khi hầu hết các bộ dữ liệu công khai hiện có cho sinh mã thực sự có mã bằng Python, sinh mã trong các ngôn ngữ khác là một tác vụ quan trọng và sự lựa chọn ngôn ngữ có thể ảnh hưởng đến kết quả của một nghiên cứu như chúng tôi.

Tất cả các mối đe dọa tính hợp lệ bên ngoài liên quan đến vấn đề thiên lệch lấy mẫu. Trong khi chúng tôi không thể biết chắc chắn, nếu kết quả nghiên cứu của chúng tôi có hiệu lực đối với các ngôn ngữ lập trình khác và các bộ dữ liệu Python khác, thực tế rằng các chỉ số phổ biến tương quan yếu với phán đoán con người đối với các bộ dữ liệu được nghiên cứu cho thấy rằng nó cũng có thể là trường hợp đối với những bộ dữ liệu khác. Do đó, chúng ta cần một đánh giá mở rộng về các chỉ số sinh mã để so sánh các mô hình một cách mạnh mẽ.

7.2 Mối đe dọa nội bộ

Trong bài báo này, chúng tôi xem xét các mối đe dọa tính hợp lệ nội bộ là những thiếu sót ảnh hưởng đến độ tin cậy của mối quan hệ nhân quả được kiểm tra. Các mối đe dọa nội bộ đối với tính hợp lệ của nghiên cứu của chúng tôi liên quan đến việc lựa chọn người chấm điểm con người. Một trong những mối đe dọa có thể là số lượng điểm trung bình nhỏ có sẵn cho mỗi đoạn mã. Có thể do số lượng hạn chế các nhà phát triển đã tham gia đánh giá, điểm con người chúng tôi dẫn xuất khác với điểm con người "thực" cho các đoạn mã được phân tích. Vấn đề này, thật không may, phổ biến đối với nhiều nghiên cứu sử dụng đánh giá con người. Số lượng điểm con người chúng tôi thu thập cho mỗi đoạn mã không ít hơn trong các nghiên cứu khác sử dụng đánh giá con người cho mã [16,17], và kết quả của chúng tôi phù hợp với các phát hiện của Roy et al. [17] nghiên cứu các chỉ số cho tóm tắt mã.

Một vấn đề liên quan là người chấm điểm thiên lệch. Một người chấm điểm có thể có sở thích riêng của họ trong phong cách mã hóa hoặc sử dụng các công nghệ cụ thể có thể ảnh hưởng đến điểm họ gán cho các đoạn mã. Để giảm thiểu vấn đề này và tuân theo các thực hành khảo sát tiêu chuẩn [17], chúng tôi xáo trộn các đoạn mã được trình bày, và thêm các đoạn mã đúng, sao cho người chấm điểm không biết đoạn mã nào đúng hay không, để làm mờ hiệu ứng học tập có thể trên các đầu ra của các mô hình khác nhau.

Chúng tôi tin rằng trong khi tất cả các mối đe dọa đối với tính hợp lệ được liệt kê ở trên là hữu hình, chúng tôi đã thực hiện tất cả các biện pháp cần thiết để giảm thiểu chúng, và kết quả của chúng tôi hợp lệ và có thể sử dụng được cho cộng đồng.

8 KẾT LUẬN

Trong nghiên cứu này, chúng tôi kiểm tra thực hành hiện tại của việc đánh giá chất lượng của các mô hình sinh mã với một điểm cấp độ corpus duy nhất dựa trên các chỉ số tự động. Đặc biệt, chúng tôi kiểm tra xem đánh giá như vậy có mang lại kết quả có ý nghĩa thống kê và tương quan tốt với phán đoán con người hay không. Chúng tôi xem xét sáu chỉ số—BLEU, ROUGE-L, METEOR, ChrF, CodeBLEU, và RUBY—cho các mô hình sinh mã được đánh giá trên hai bộ dữ liệu Python khác nhau: CoNaLa [12] và Hearthstone [10].

Chúng tôi thấy rằng ngay cả khi không tính đến kết quả đánh giá con người, việc cải thiện điểm chỉ số cấp độ corpus nhỏ hơn 2 điểm có thể không đủ để đảm bảo cải thiện có ý nghĩa thống kê về chất lượng mà không có các kiểm tra thống kê bổ sung. Khi chúng tôi cũng xem xét kết quả đánh giá con người, chúng tôi thấy rằng đối với một số bộ dữ liệu, ngay cả cải thiện điểm nhỏ hơn 5 điểm có thể không tương ứng với cải thiện có ý nghĩa thống kê theo phán đoán con người. Trong các chỉ số chúng tôi nghiên cứu, ChrF hóa ra gần nhất với đánh giá con người. Tuy nhiên, nó không thể được coi là chỉ số "hoàn hảo" cho sinh mã và việc tìm ra chỉ số như vậy đòi hỏi công việc bổ sung.

Trong công việc tương lai, chúng tôi hướng đến việc mở rộng nghiên cứu của chúng tôi cho các bộ dữ liệu và mô hình sinh mã khác, và thực hiện đánh giá con người mở rộng hơn sẽ cho phép kiểm tra cải tiến mô hình ở cấp độ đoạn mã thay vì ở cấp độ corpus.

TÀI LIỆU THAM KHẢO

[1] R. Balzer, Một góc nhìn 15 năm về lập trình tự động, IEEE Transactions on Software Engineering (11) (1985) 1257–1268.
[2] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al., Đánh giá các mô hình ngôn ngữ lớn được huấn luyện trên mã, arXiv preprint arXiv:2107.03374 (2021).
[3] P. Yin, G. Neubig, Một mô hình neural cú pháp cho sinh mã đa mục đích, arXiv preprint arXiv:1704.01696 (2017).
[4] Z. Sun, Q. Zhu, L. Mou, Y. Xiong, G. Li, L. Zhang, Một bộ giải mã CNN cấu trúc dựa trên ngữ pháp cho sinh mã, trong: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33, 2019, pp. 7055–7062.
[5] P. Yin, G. Neubig, Tranx: Một trình phân tích cú pháp trừu tượng neural dựa trên chuyển tiếp cho phân tích ngữ nghĩa và sinh mã, arXiv preprint arXiv:1810.02720 (2018).
[6] P. Yin, G. Neubig, Sắp xếp lại cho phân tích ngữ nghĩa neural, trong: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019, pp. 4553–4559.
[7] T. Yu, R. Zhang, K. Yang, M. Yasunaga, D. Wang, Z. Li, J. Ma, I. Li, Q. Yao, S. Roman, et al., Spider: Một bộ dữ liệu quy mô lớn được gắn nhãn bởi con người cho phân tích ngữ nghĩa phức tạp và xuyên lĩnh vực và tác vụ text-to-sql, arXiv preprint arXiv:1809.08887 (2018).
[8] Y. Oda, H. Fudaba, G. Neubig, H. Hata, S. Sakti, T. Toda, S. Nakamura, Học sinh mã giả từ mã nguồn sử dụng dịch máy thống kê (t), trong: 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE), IEEE, 2015, pp. 574–584.
[9] R. Agashe, S. Iyer, L. Zettlemoyer, Juice: Một bộ dữ liệu giám sát từ xa quy mô lớn cho sinh mã dựa trên ngữ cảnh miền mở, arXiv preprint arXiv:1910.02216 (2019).
[10] W. Ling, E. Grefenstette, K. M. Hermann, T. Kočisk`y, A. Senior, F. Wang, P. Blunsom, Mạng dự đoán tiềm ẩn cho sinh mã, arXiv preprint arXiv:1603.06744 (2016).
[11] A. V. M. Barone, R. Sennrich, Một corpus song song của các hàm python và chuỗi tài liệu cho tự động hóa tài liệu mã và sinh mã, arXiv preprint arXiv:1707.02275 (2017).
[12] P. Yin, B. Deng, E. Chen, B. Vasilescu, G. Neubig, Học khai thác các cặp mã và ngôn ngữ tự nhiên được căn chỉnh từ stack overflow, trong: International Conference on Mining Software Repositories, MSR, ACM, 2018, pp. 476–486. doi:https://doi.org/10.1145/3196398.3196408.
[13] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement, D. Drain, D. Jiang, D. Tang, et al., Codexglue: Một bộ dữ liệu điểm chuẩn học máy cho hiểu và sinh mã, arXiv preprint arXiv:2102.04664 (2021).
[14] K. Papineni, S. Roukos, T. Ward, W.-J. Zhu, Bleu: một phương pháp để đánh giá tự động dịch máy, trong: Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, pp. 311–318.
[15] S. Ren, D. Guo, S. Lu, L. Zhou, S. Liu, D. Tang, M. Zhou, A. Blanco, S. Ma, Codebleu: một phương pháp để đánh giá tự động tổng hợp mã, arXiv preprint arXiv:2009.10297 (2020).
[16] N. Tran, H. Tran, S. Nguyen, H. Nguyen, T. Nguyen, Điểm bleu có hoạt động cho di chuyển mã không?, trong: 2019 IEEE/ACM 27th International Conference on Program Comprehension (ICPC), IEEE, 2019, pp. 165–176.
[17] D. Roy, S. Fakhoury, V. Arnaoudova, Đánh giá lại các chỉ số đánh giá tự động cho các tác vụ tóm tắt mã, trong: Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2021, pp. 1105–1116.
[18] B. Efron, Ước lượng tỷ lệ lỗi của một quy tắc dự đoán: cải thiện về xác thực chéo, Journal of the American statistical association 78 (382) (1983) 316–331.
[19] C.-Y. Lin, Rouge: Một gói để đánh giá tự động các bản tóm tắt, trong: Text summarization branches out, 2004, pp. 74–81.
[20] S. Banerjee, A. Lavie, Meteor: Một chỉ số tự động cho đánh giá mt với tương quan được cải thiện với phán đoán con người, trong: Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, 2005, pp. 65–72.
[21] M. Popović, chrf: f-score n-gram ký tự cho đánh giá mt tự động, trong: Proceedings of the Tenth Workshop on Statistical Machine Translation, 2015, pp. 392–395.
[22] N. Mathur, T. Baldwin, T. Cohn, Rối rắm trong bleu: Đánh giá lại việc đánh giá các chỉ số đánh giá dịch máy tự động, arXiv preprint arXiv:2006.06264 (2020).
[23] M. Rabinovich, M. Stern, D. Klein, Mạng cú pháp trừu tượng cho sinh mã và phân tích ngữ nghĩa, arXiv preprint arXiv:1704.07535 (2017).
[24] S. Hochreiter, J. Schmidhuber, Bộ nhớ ngắn hạn dài, Neural computation 9 (8) (1997) 1735–1780.
[25] Y. Bengio, P. Simard, P. Frasconi, Học các phụ thuộc dài hạn với gradient descent khó khăn, IEEE transactions on neural networks 5 (2) (1994) 157–166.
[26] B. Wei, G. Li, X. Xia, Z. Fu, Z. Jin, Sinh mã như một tác vụ kép của tóm tắt mã, trong: Advances in Neural Information Processing Systems, 2019, pp. 6563–6573.
[27] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, I. Polosukhin, Attention is all you need, trong: I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, R. Garnett (Eds.), Advances in Neural Information Processing Systems, Vol. 30, Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
[28] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. D. Lago, T. Hubert, P. Choy, C. d. M. d'Autume, I. Babuschkin, X. Chen, P.-S. Huang, J. Welbl, S. Gowal, A. Cherepanov, J. Molloy, D. J. Mankowitz, E. S. Robson, P. Kohli, N. de Freitas, K. Kavukcuoglu, O. Vinyals, Sinh mã cấp độ cạnh tranh với alphacode (2022). doi:10.48550/ARXIV.2203.07814. URL https://arxiv.org/abs/2203.07814
[29] M. Denkowski, A. Lavie, Meteor universal: Đánh giá dịch cụ thể ngôn ngữ cho bất kỳ ngôn ngữ đích nào, trong: Proceedings of the ninth workshop on statistical machine translation, 2014, pp. 376–380.
[30] A. LeClair, C. McMillan, Khuyến nghị cho bộ dữ liệu để tóm tắt mã nguồn, arXiv preprint arXiv:1904.02660 (2019).
[31] Q. Ma, J. Wei, O. Bojar, Y. Graham, Kết quả của tác vụ chia sẻ chỉ số wmt19: Các hệ thống mt mạnh cấp độ đoạn và đặt ra thách thức lớn, trong: Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), 2019, pp. 62–90.
[32] Q. Ma, O. Bojar, Y. Graham, Kết quả của tác vụ chia sẻ chỉ số wmt18: Cả ký tự và embedding đều đạt được hiệu suất tốt, trong: Proceedings of the third conference on machine translation: shared task papers, 2018, pp. 671–688.
[33] C.-W. Liu, R. Lowe, I. V. Serban, M. Noseworthy, L. Charlin, J. Pineau, Cách không đánh giá hệ thống đối thoại của bạn: Một nghiên cứu thực nghiệm về các chỉ số đánh giá không giám sát cho sinh phản hồi đối thoại, arXiv preprint arXiv:1603.08023 (2016).
[34] E. Reiter, Một đánh giá có cấu trúc về tính hợp lệ của bleu, Computational Linguistics 44 (3) (2018) 393–401.
[35] Q. Ma, A. Olshevsky, Crowdsourcing đối thủ thông qua hoàn thành ma trận hạng một mạnh mẽ, arXiv preprint arXiv:2010.12181 (2020).
[36] D. Ustalov, N. Pavlichenko, V. Losev, I. Giliazev, E. Tulin, Một bộ công cụ kiểm soát chất lượng tính toán crowdsourcing đa mục đích cho Python, trong: The Ninth AAAI Conference on Human Computation and Crowdsourcing: Works-in-Progress and Demonstration Track, HCOMP 2021, 2021. arXiv:2109.08584. URL https://www.humancomputation.com/assets/wips_demos/HCOMP_2021_paper_85.pdf
[37] R. Agashe, S. Iyer, L. Zettlemoyer, JuICe: Một bộ dữ liệu giám sát từ xa quy mô lớn cho sinh mã dựa trên ngữ cảnh miền mở, trong: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics, Hong Kong, China, 2019, pp. 5436–5446. doi:10.18653/v1/D19-1546. URL https://aclanthology.org/D19-1546
[38] S. Iyer, I. Konstas, A. Cheung, L. Zettlemoyer, Ánh xạ ngôn ngữ thành mã trong bối cảnh lập trình, arXiv preprint arXiv:1808.09588 (2018).
[39] P. Yin, B. Deng, E. Chen, B. Vasilescu, G. Neubig, Học khai thác các cặp mã và ngôn ngữ tự nhiên được căn chỉnh từ stack overflow, trong: 2018 IEEE/ACM 15th international conference on mining software repositories (MSR), IEEE, 2018, pp. 476–486.
[40] trình mô phỏng hearthstone mã nguồn mở, https://github.com/danielyule/hearthbreaker, truy cập: 2022-06-06.
[41] F. Wilcoxon, So sánh cá nhân bằng phương pháp xếp hạng, trong: Breakthroughs in statistics, Springer, 1992, pp. 196–202.
[42] Y. Graham, N. Mathur, T. Baldwin, Kiểm tra ý nghĩa ngẫu nhiên trong dịch máy, trong: Proceedings of the Ninth Workshop on Statistical Machine Translation, 2014, pp. 266–274.
[43] L. Barrault, O. Bojar, M. R. Costa-Jussa, C. Federmann, M. Fishel, Y. Graham, Các phát hiện của hội nghị dịch máy 2019 (wmt19), Association for Computational Linguistics (ACL), 2019.
[44] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, Y. Artzi, Bertscore: Đánh giá sinh văn bản với bert, arXiv preprint arXiv:1904.09675 (2019).
[45] B. Chen, C. Cherry, Một so sánh có hệ thống của các kỹ thuật làm mịn cho bleu cấp độ câu, trong: Proceedings of the ninth workshop on statistical machine translation, 2014, pp. 362–367.
[46] M. Post, Một lời kêu gọi sự rõ ràng trong báo cáo điểm bleu, arXiv preprint arXiv:1804.08771 (2018).
[47] Script perl Rouge 1.5.5, https://github.com/andersjo/pyrouge/tree/master/tools/ROUGE-1.5.5, truy cập: 2022-05-19.
[48] M. Popović, chrf được giải cấu trúc: tham số beta và trọng số n-gram, trong: Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016, pp. 499–504.

A TÍNH TOÁN CHỈ SỐ

A.1 BLEU

Chỉ số BLEU dựa trên thước đo chính xác n-gram đã sửa đổi với hình phạt độ dài cho các câu ứng viên ngắn hơn so với các câu tham chiếu. Điểm BLEU được xác định bởi công thức sau:

BLEU = BP·exp(∑(n=1 to N) wn log pn); BP = min(1,e^(1-r/c)), (1)

trong đó BP là hình phạt ngắn gọn với r là độ dài của tham chiếu và c là độ dài dịch ứng viên. pn tương ứng với sự chồng chéo có trọng số giữa bag của n-gram (các thuật ngữ được lặp lại được cho phép lên đến số lần lặp lại tối đa trên các tham chiếu). Nếu S_n^ref và S_n^can là bag của n-gram cho tham chiếu và ứng viên tương ứng, thì

pn = (S_n^ref ∩ S_n^can) / S_n^ref (2)

Cuối cùng, wn là trọng số cho các đóng góp n-gram khác nhau; trọng số tiêu chuẩn là w1=...=w4=1/4, wn>4=0. Triển khai BLEU gốc [14] là một chỉ số cấp độ corpus, vì nó tính đến độ chính xác trung bình vi mô. Nghĩa là, để tính độ chính xác, người ta phải tổng các tử số và mẫu số cho mỗi cặp giả thuyết-tham chiếu trước khi chia. Có thể định nghĩa chỉ số sentenceBLEU để chấm điểm giả thuyết riêng lẻ (như được thực hiện bởi, ví dụ, Roy et al. [17]) bằng cách xem xét mỗi giả thuyết và tham chiếu như một corpus độc lập. Tuy nhiên, người ta phải nhớ rằng trung bình của sentenceBLEU trên toàn bộ bộ dữ liệu không nhất thiết bằng BLEU được đánh giá trên bộ dữ liệu.

Giá trị BLEU từ 0 đến 1, với điểm số cao hơn tương ứng với độ chính xác n-gram tốt hơn. Tuy nhiên, các nhà thực hành thường nhân điểm BLEU với hệ số 100 trong báo cáo chất lượng mô hình của họ. Triển khai mặc định của chỉ số BLEU cho điểm số không đối với các ứng viên có sự chồng chéo bằng không trong 4-gram với tham chiếu. Hạn chế này có thể phạt các câu ứng viên có chất lượng tầm thường quá nặng (ví dụ đối với một tham chiếu bảy token, một ứng viên đoán đúng 6 token nhưng bỏ lỡ token #4 sẽ nhận được điểm số không). Một số thuật toán làm mịn đã được đề xuất để tránh những tình huống này, một so sánh có hệ thống của các kỹ thuật làm mịn cho BLEU cấp độ câu cho tác vụ dịch máy có thể được tìm thấy trong bài báo của Chen et al. [45]

Trong nghiên cứu của chúng tôi, chúng tôi sử dụng triển khai BLEU tham chiếu từ gói sacrebleu [46].

A.2 ROUGE-L

ROUGE-L là một chỉ số từ họ chỉ số ROUGE được đề xuất lần đầu bởi Lin [19]. Ban đầu nó được đề xuất để đánh giá chất lượng của các bản tóm tắt văn bản ngắn, nhưng sau đó được áp dụng cho các tác vụ khác. Khái niệm cơ bản cho tính toán ROUGE-L là chuỗi con chung dài nhất (của giả thuyết và tham chiếu). Chuỗi con chung giữa hai chuỗi X=[xi], Y=[yj] là một chuỗi [zl] là chuỗi con của cả X,Y. Chuỗi con chung dài nhất sau đó đơn giản là một chuỗi con chung có độ dài tối đa. Điều này cho phép chúng ta định nghĩa precision, recall và chỉ số ROUGE-L cho giả thuyết H và tham chiếu R như

R_lcs(H,R) = LCS(H,R)/len(R)
P_lcs(H,R) = LCS(H,R)/len(H)
ROUGE_L(H,R) = (1+β²)P_lcs R_lcs/(R_lcs + β²P_lcs)

β là tham số xác định trọng số recall, trong đánh giá của chúng tôi chúng tôi sử dụng β=1 (trọng số bằng nhau của precision và recall). Các giá trị có thể từ 0 đến 1, nhưng tương tự như BLEU và các chỉ số khác, điểm cấp độ corpus thường được nhân với 100 để đơn giản hóa nhận thức. ROUGE-L thường được sử dụng như một chỉ số cấp độ đoạn mã [17]. Điều này có nghĩa là để có được điểm ROUGE-L cấp độ corpus, người ta phải lấy trung bình điểm cấp độ đoạn mã. Để có một ví dụ đơn giản, hãy xem xét một tham chiếu và hai giả thuyết:

R: police killed the gunman
H1: police kill the gunman
H2: the gunman killed police

Chuỗi con chung dài nhất giữa R,H1 dài 3 token (token đầu tiên, thứ ba và thứ tư), và chuỗi con chung dài nhất giữa R,H2 dài 2 token (hoặc token đầu tiên và thứ hai, hoặc token thứ ba và thứ tư). Do đó ROUGE_L(H1,R)=0.75, ROUGE_L(H2,R)=0.5.

Chúng tôi sử dụng triển khai ROUGE-L từ gói rouge-score, mang lại kết quả giống hệt với script Perl gốc [47].

A.3 ChrF

ChrF là một chỉ số F-measure dựa trên ký tự được đề xuất lần đầu bởi Popovic [21]. Ban đầu nó được đề xuất để đánh giá tự động đầu ra dịch máy. Như một chỉ số dựa trên ký tự, ChrF không phụ thuộc vào quy tắc token hóa. Nó tính đến mọi ký tự, ngoại trừ khoảng trắng. Để tính ChrF trong định nghĩa tiêu chuẩn của nó, trước tiên người ta phải tính precision và recall cấp độ ký tự chrPk, chrRk cho k-gram ký tự, trong đó 1≤k≤6. Tổng precision và recall n-gram ChrP, ChrR là trung bình số học của chrPk, chrRk tương ứng. Cuối cùng, ChrF được tính như

ChrF_β = (1+β²)ChrP ChrR/(ChrR + β²ChrP) (3)

Định nghĩa ChrF tiêu chuẩn mà chúng tôi sử dụng đặt β=2, vì lựa chọn β này mang lại kết quả tốt nhất trong các tác vụ dịch máy [48].

Chúng tôi sử dụng triển khai ChrF tham chiếu từ gói sacrebleu.

A.4 METEOR

METEOR được tạo ra như một chỉ số để đánh giá dịch máy [20]. Có một số phiên bản của chỉ số có quy tắc tính toán hơi khác nhau. Trong tính toán của chúng tôi, chúng tôi đã sử dụng phiên bản mới nhất của chỉ số – METEOR 1.5 [29]. Tính toán của nó bao gồm các bước sau:

• Tạo căn chỉnh giữa các chuỗi giả thuyết và tham chiếu. Căn chỉnh giữa các chuỗi giả thuyết và tham chiếu là một ánh xạ giữa các unigram của những chuỗi này, sao cho mỗi unigram trong mỗi chuỗi ánh xạ đến không hoặc một unigram trong chuỗi khác. Căn chỉnh được tạo trong một số giai đoạn với các quy tắc khác nhau cho việc khớp unigram trong mỗi giai đoạn. Trong giai đoạn đầu tiên, hai từ được khớp khi và chỉ khi chúng giống hệt nhau. Trong giai đoạn thứ hai, chúng được khớp nếu chúng giống hệt nhau sau Porter stemming. Trong giai đoạn thứ ba, hai từ được khớp nếu chúng là từ đồng nghĩa theo cơ sở dữ liệu WordNet. Cuối cùng, hai cụm từ được khớp nếu chúng được liệt kê như paraphrase trong bảng ngôn ngữ tương ứng. Các ánh xạ được áp dụng lặp đi lặp lại, và căn chỉnh cuối cùng là tập con lớn nhất của tất cả các khớp được xây dựng bằng tìm kiếm beam. Để xác định căn chỉnh cuối cùng, các tiêu chí sau theo thứ tự quan trọng được áp dụng:
  – Số từ được bao phủ trên cả hai câu nên được tối đa hóa.
  – Số chunk nên được tối thiểu hóa. Một chunk là một chuỗi liền kề các khớp có thứ tự giống hệt nhau trong cả hai câu.
  – Tổng khoảng cách tuyệt đối giữa các chỉ số bắt đầu khớp trong hai câu nên được tối thiểu hóa. Điều này để phá vỡ hòa bằng cách ưu tiên căn chỉnh các cụm từ xảy ra ở vị trí tương tự trong cả hai câu.

• Sau khi căn chỉnh đã được xây dựng, các từ trong giả thuyết và tham chiếu được chia thành từ nội dung và từ chức năng theo danh sách từ chức năng đặc biệt. Đối với mỗi matcher được áp dụng, người ta nên đếm số từ nội dung và từ chức năng được bao phủ bởi các khớp loại này. Sau đó người ta tính precision và recall có trọng số P,R sử dụng trọng số matcher và trọng số từ nội dung-chức năng. Từ P,R người ta sau đó tính trung bình điều hòa có trọng số F_mean. Cuối cùng, để phạt khoảng trống và sự khác biệt trong thứ tự từ, người ta tính hình phạt phân mảnh sử dụng tổng số từ được khớp và số chunk. Điểm METEOR cuối cùng được tính từ F_mean và hình phạt phân mảnh.

Chúng tôi sử dụng triển khai METEOR từ gói sacrerouge, sử dụng script gốc và cung cấp wrapper Python cho nó.

A.5 RUBY

Chỉ số được định nghĩa như

RUBY(R,C) = {
  GRS(R,C) nếu PDG áp dụng được,
  TRS(R,C) nếu AST áp dụng được,
  STS(R,C) nếu không
} (4)

Ở đây PDG là viết tắt của program dependence graph và AST là viết tắt của abstract syntax tree, R tương ứng với tham chiếu và C tương ứng với ứng viên. GRS(R,C) đo lường sự tương tự giữa hai program dependence graph cho R,C như

GRS(R,C) = 1 - GED(PDG_R,PDG_C)/(size(PDG_R) + size(PDG_C)), (5)

trong đó GED(PDG_R,PDG_C) là khoảng cách chỉnh sửa giữa PDG của mã tham chiếu và PDG của mã ứng viên. size(g) là tổng số đỉnh và cạnh của đồ thị g. GED(a,b) được tính như số tối thiểu các thao tác chỉnh sửa đồ thị để biến đổi một đồ thị thành đồ thị khác với các thao tác chỉnh sửa đồ thị được phép trên đỉnh và cạnh là chèn, xóa, và thay thế.

Trong trường hợp PDG không có sẵn cho đoạn mã ứng viên, lựa chọn dự phòng tiếp theo là TRS(R,C), đo lường sự tương tự giữa các AST cho đoạn mã tham chiếu và ứng viên như

TRS(R,C) = 1 - TED(AST_R,AST_C)/(size(AST_R) + size(AST_C)), (6)

trong đó size(T) là số nút trong AST, và TED(a,b) là khoảng cách chỉnh sửa giữa các AST của mã tham chiếu AST_R và mã ứng viên AST_C. TED được cho bởi số tối thiểu các thao tác chỉnh sửa trên các nút AST (bao gồm thêm, xóa, thay thế và di chuyển) làm cho AST_R và AST_C giống hệt nhau.

Cuối cùng, lựa chọn dự phòng cuối cùng cho RUBY, luôn có thể được tính toán, là hàm tương tự chuỗi STS(R,C) được định nghĩa như

STS(R,C) = 1 - SED(S_R,S_C)/max(length(S_R),length(S_C)), (7)

trong đó SED(S_R,S_C) là khoảng cách chỉnh sửa chuỗi giữa chuỗi tham chiếu S_R và chuỗi ứng viên S_C. Nó đo lường số hành động xóa/thêm token mà người dùng phải thực hiện để biến đổi mã ứng viên thành mã tham chiếu; length(t) là độ dài của chuỗi t. Tran et al. tạo động lực cho lựa chọn chỉ số này bằng quan sát rằng các chỉ số trừu tượng hơn có tương quan tốt hơn với phán đoán con người. Vì Tran et al. không cung cấp triển khai tham chiếu của RUBY, trong nghiên cứu của chúng tôi chúng tôi sử dụng triển khai RUBY của riêng chúng tôi.

A.6 CodeBLEU

Chỉ số CodeBLEU như được đề xuất bởi Ren et al. [15] được cho bởi

CodeBLEU = 0.1·BLEU + 0.1·BLEU_w + (8)
+ 0.4·Match_ast + 0.4·Match_df, (9)

trong đó:

• BLEU là chỉ số BLEU thông thường.
• BLEU_w là chỉ số BLEU được tính chỉ trên unigram với từ khóa được cho trọng số cao hơn 5 lần. Nói cách khác, BLEU_w là precision cho unigram với hình phạt ngắn gọn BLEU. Ví dụ, đối với tham chiếu Python for x in lst và giả thuyết for x của BLEU_w = e^(-1/3)/6^(1/2).
• Match_ast là khớp AST cú pháp. Để tính chỉ số phụ này, trước tiên người ta phải xây dựng AST cho cả tham chiếu và giả thuyết, và trích xuất tất cả các cây con từ cả hai AST. Để theo dõi cấu trúc cú pháp, các tác giả không quan tâm đến các giá trị trong các nút lá. Match_ast sau đó được cho bởi Match_ast = Count_clip(T_hyp)/Count(T_ref), trong đó Count(T_ref) là tổng số cây con trong AST tham chiếu và Count_clip(T_cand) là số cây con trong AST giả thuyết được khớp bởi các cây con trong tham chiếu.
• Match_df là khớp luồng dữ liệu ngữ nghĩa xem xét sự tương tự ngữ nghĩa giữa giả thuyết và tham chiếu bằng cách so sánh các đồ thị luồng dữ liệu của tham chiếu và giả thuyết. Chỉ số phụ được tính trong một số bước như sau:
  1. Xây dựng đồ thị luồng dữ liệu cho tham chiếu và giả thuyết. Để làm điều đó, trước tiên người ta phải có được chuỗi biến V={v0,v1,...,vm} từ AST. Mỗi biến sau đó trở thành một nút của đồ thị luồng dữ liệu, và các cạnh có hướng ε=⟨vi,vj⟩ biểu thị rằng giá trị của biến thứ j-th đến từ biến thứ i-th. Đồ thị G=(V,E) là đồ thị luồng dữ liệu.
  2. Chuẩn hóa các mục luồng dữ liệu. Để làm điều đó, người ta phải thu thập tất cả các biến trong các mục luồng dữ liệu và đổi tên chúng var_i, trong đó i là thứ tự xuất hiện biến trong tất cả các mục luồng dữ liệu.
  3. Tính điểm khớp luồng dữ liệu ngữ nghĩa như Match_df = Count_clip(DF_hyp)/Count(DF_ref), trong đó Count(DF_ref) là tổng số luồng dữ liệu tham chiếu và Count_clip(DF_cand) là số luồng dữ liệu ứng viên được khớp.

Ren et al. so sánh CodeBLEU với BLEU và độ chính xác. Vì CodeBLEU không được so sánh với các chỉ số tự động khác ngoài BLEU và độ chính xác, chúng ta cần thực hiện đánh giá thêm. Trong nghiên cứu của chúng tôi, chúng tôi sử dụng triển khai CodeBLEU của riêng chúng tôi.
