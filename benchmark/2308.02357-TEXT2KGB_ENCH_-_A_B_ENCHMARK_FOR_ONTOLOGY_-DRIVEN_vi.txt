# TEXT2KGBENCH: MỘT BỘ ĐÁNH GIÁ CHO VIỆC TẠO ĐỒ THỊ KIẾN THỨC DỰA TRÊN ONTOLOGY TỪ VĂN BẢN
BẢN THẢO TRƯỚC KHI XUẤT BẢN

Nandana Mihindukulasooriya∗
IBM Research Europe
Ireland
nandana@ibm.com

Sanju Tiwari
Universidad Autonoma de Tamaulipas, Mexico
tiwarisanju18@ieee.org

Carlos F. Enguix
Universidad Autonoma de Tamaulipas, Mexico
carlos.f.enguix@gmail.com

Kusum Lata
Sharda University, India
kusumlata.1@sharda.ac.in

7 tháng 8, 2023

TÓM TẮT

Những tiến bộ gần đây trong các mô hình ngôn ngữ lớn (LLM) và các mô hình nền tảng với khả năng nổi lên đã được chứng minh là cải thiện hiệu suất của nhiều tác vụ NLP. LLM và Đồ thị Kiến thức (KG) có thể bổ sung cho nhau sao cho LLM có thể được sử dụng để xây dựng hoặc hoàn thiện KG trong khi các KG hiện có có thể được sử dụng cho các tác vụ khác nhau như làm cho đầu ra LLM có thể giải thích được hoặc kiểm tra sự thật theo cách Neuro-Symbolic. Trong bài báo này, chúng tôi trình bày Text2KGBench, một bộ đánh giá để đánh giá khả năng của các mô hình ngôn ngữ trong việc tạo KG từ văn bản ngôn ngữ tự nhiên được hướng dẫn bởi một ontology. Với một ontology đầu vào và một tập hợp các câu, nhiệm vụ là trích xuất các sự kiện từ văn bản trong khi tuân thủ ontology đã cho (khái niệm, quan hệ, ràng buộc miền/phạm vi) và trung thành với các câu đầu vào. Chúng tôi cung cấp hai bộ dữ liệu (i) Wikidata-TekGen với 10 ontology và 13.474 câu và (ii) DBpedia-WebNLG với 19 ontology và 4.860 câu. Chúng tôi định nghĩa bảy thước đo đánh giá để đo lường hiệu suất trích xuất sự kiện, sự tuân thủ ontology, và ảo giác của LLM. Hơn nữa, chúng tôi cung cấp kết quả cho hai mô hình cơ sở, Vicuna-13B và Alpaca-LoRA-13B sử dụng tạo prompt tự động từ các trường hợp thử nghiệm. Kết quả cơ sở cho thấy vẫn còn chỗ để cải thiện bằng cả kỹ thuật Semantic Web và Xử lý Ngôn ngữ Tự nhiên.

Loại Tài nguyên: Bộ đánh giá Đánh giá
Kho Nguồn: https://github.com/cenguix/Text2KGBench
DOI: https://doi.org/10.5281/zenodo.7916716
Giấy phép: Creative Commons Attribution (CC BY 4.0)

Từ khóa: Bộ đánh giá · Trích xuất Quan hệ · Đồ thị Kiến thức · Tạo Đồ thị Kiến thức · Mô hình Ngôn ngữ Lớn

1 Giới thiệu

Đồ thị Kiến thức (KG) đang trở nên phổ biến trong cả công nghiệp và học thuật do các ứng dụng hữu ích của chúng trong một phạm vi rộng các tác vụ như trả lời câu hỏi, khuyến nghị, tìm kiếm ngữ nghĩa, và phân tích nâng cao với khả năng giải thích [Hogan et al., 2021]. Một KG có thể được tạo ra bằng cách sử dụng các ánh xạ như RDB2RDF [Sahoo et al., 2009] nếu nguồn là dữ liệu quan hệ hoặc bán cấu trúc sử dụng RML [Dimou et al., 2014]. Crowdsourcing có thể được sử dụng để xây dựng chúng theo cách thủ công như trong Wikidata [Vrandečić và Krötzsch, 2014]. Tuy nhiên, có những trường hợp dữ liệu ở định dạng không có cấu trúc trong các tài liệu văn bản và crowdsourcing không phải là một lựa chọn (ví dụ, các tài liệu nội bộ). Một giải pháp trong những trường hợp như vậy là xây dựng đồ thị kiến thức bằng cách sử dụng các kỹ thuật Xử lý Ngôn ngữ Tự nhiên (NLP) như Nhận dạng Thực thể Có tên (NER), Trích xuất Quan hệ, Trích xuất Thông tin Mở, Liên kết Thực thể, và Liên kết Quan hệ. Có một sự quan tâm ngày càng tăng trong cộng đồng Semantic Web để khám phá các phương pháp như vậy như đã thấy từ các hội thảo như Text2KG [Tiwari et al., 2022, 2023] và NLP4KGC [Vakaj et al., 2023].

Những tiến bộ gần đây trong các mô hình ngôn ngữ lớn (LLM) và các mô hình nền tảng với khả năng nổi lên đã được chứng minh là cải thiện hiệu suất trong nhiều tác vụ NLP [Brown et al., 2020]. KG và LLM có thể bổ sung cho nhau theo cả hai hướng; một mặt, LLM có thể hữu ích trong việc xây dựng KG và mặt khác KG có thể được sử dụng để xác thực đầu ra LLM hoặc làm cho chúng có thể giải thích được. Các phương pháp như Neuro-Symbolic AI [Hitzler, 2022] sẽ cho phép sử dụng KG và LLM cùng nhau. Để thúc đẩy nghiên cứu theo hướng này, việc thiết lập các bộ đánh giá là cần thiết. Trong bối cảnh này, Text2KGBench là một bộ đánh giá để đo lường khả năng của LLM trong việc tạo KG từ văn bản tuân thủ một ontology đã cho. Trong phiên bản này, chúng tôi không đánh giá khả năng xử lý hoặc tạo ra các biểu diễn RDF/OWL mà là khả năng trích xuất sự kiện sử dụng các quan hệ chính xác.

Có nhiều cách LLM có thể được điều chỉnh cho tác vụ này, bao gồm tinh chỉnh [Howard và Ruder, 2018a] (cũng được gọi là điều chỉnh mô hình), cập nhật tất cả các tham số mô hình, Prompt tuning [Lester et al., 2021a] hoặc Prefix-Tuning [Li và Liang, 2021] bằng cách giữ các tham số mô hình cố định và chỉ thêm tiền tố một số token có thể điều chỉnh vào văn bản đầu vào và thiết kế prompt nơi mô hình được sử dụng như nó vốn có, nhưng prompt hoặc đầu vào cho mô hình được thiết kế để cung cấp một vài ví dụ về tác vụ [Brown et al., 2020]. Mỗi phương pháp này có ưu và nhược điểm riêng về hiệu suất, tài nguyên tính toán, thời gian huấn luyện, thích ứng miền và dữ liệu huấn luyện yêu cầu. Bộ đánh giá của chúng tôi cung cấp dữ liệu huấn luyện có thể được sử dụng trong bất kỳ phương pháp nào trong số những phương pháp đó.

Học trong ngữ cảnh [Min et al., 2022, Xie et al., 2021] với thiết kế prompt là về việc dạy một mô hình thực hiện một tác vụ mới chỉ bằng cách cung cấp một vài minh họa về các cặp đầu vào-đầu ra tại thời gian suy luận. Tinh chỉnh hướng dẫn sử dụng các phương pháp như InstructGPT [Ouyang et al., 2022], Reinforcement Learning from Human Feedback (RLHF) [Christiano et al., 2017, Stiennon et al., 2020] cải thiện đáng kể khả năng của các mô hình để tuân theo một phạm vi rộng các hướng dẫn bằng văn bản.

Một số lượng lớn LLM đã được phát hành trong những tháng gần đây [Yang et al., 2023], đặc biệt trong gia đình các mô hình GPT như GPT-3 [Brown et al., 2020], ChatGPT, LLaMA [Touvron et al., 2023], BLOOM [Scao et al., 2022], PaLM [Chowdhery et al., 2022], và Bard. Những mô hình như vậy có thể dễ dàng được điều chỉnh cho việc tạo KG từ văn bản với một thiết kế prompt chứa hướng dẫn và thông tin ngữ cảnh.

Những đóng góp chính của bài báo này là:

• Chúng tôi đề xuất một bộ đánh giá mới Text2KGBench bằng cách mở rộng việc trích xuất quan hệ bằng cách hướng dẫn nó với ontology và hướng dẫn. Chúng tôi cung cấp hai bộ dữ liệu, (a) Wikidata-TekGen với 10 ontology và 13.474 câu được căn chỉnh với bộ ba và (b) DBpedia-WebNLG với 19 ontology và 4.860 câu được căn chỉnh với bộ ba bằng cách tái sử dụng các corpus TekGen [Agarwal et al., 2021] và WebNLG [Gardent et al., 2017]. Chúng tôi định nghĩa bảy thước đo để đo lường độ chính xác của việc trích xuất sự kiện, sự tuân thủ ontology và phát hiện ảo giác và cung cấp các script đánh giá.

• Chúng tôi cung cấp kết quả cho hai baseline sử dụng LLM mã nguồn mở, bao gồm Vicuna-13B [Chiang et al., 2023] và Alpaca-LoRA-13B [Taori et al., 2023, Hu et al., 2022] với học trong ngữ cảnh. Chúng tôi cũng cung cấp một baseline tự động tạo prompt từ ontology và phương pháp tìm ví dụ minh họa tốt nhất với độ tương tự câu sử dụng mô hình SBERT T5-XXL [Reimers và Gurevych, 2019, Ni et al., 2022]. Chúng tôi cung cấp tất cả các prompt đã tạo, độ tương tự, và phản hồi LLM để phân tích thêm.

Phần còn lại của bài báo được tổ chức như sau. Phần 2 giới thiệu nhiệm vụ của bộ đánh giá, Phần 3 mô tả cách bộ đánh giá được tạo ra, Phần 4 định nghĩa các thước đo đánh giá và Phần 5 trình bày các baseline và kết quả đánh giá. Sau công trình liên quan trong Phần 6, bài báo kết thúc với một số nhận xét cuối cùng và công việc tương lai trong Phần 7.

2 Mô tả Nhiệm vụ

Phần này giới thiệu nhiệm vụ của Text2KGBench. Với những tiến bộ gần đây của LLM, chúng tôi hình dung rằng LLM có thể được sử dụng để tạo KG được hướng dẫn bởi các ontology như được minh họa trong Hình 1. Với một ontology và corpus văn bản, mục tiêu là xây dựng các prompt để hướng dẫn mô hình trích xuất các sự kiện liên quan đến ontology. Những sự kiện được trích xuất như vậy có thể được xác thực thêm và xử lý hậu kỳ để tạo ra một đồ thị kiến thức.

Trong bối cảnh của Text2KGBench, chúng tôi định nghĩa nhiệm vụ như một tác vụ trích xuất sự kiện được hướng dẫn bởi một ontology. Nhiệm vụ được đề xuất có liên quan chặt chẽ với các tác vụ trích xuất quan hệ và phân loại quan hệ trong tài liệu nhưng với một định nghĩa ontology rõ ràng được đưa ra như đầu vào. Có ba đầu vào chính cho nhiệm vụ:

Ontology: Ontology định nghĩa các khái niệm quan tâm, một tập hợp các quan hệ được định nghĩa với tên chính tắc của chúng, ràng buộc miền và phạm vi cho các quan hệ. Điều này có thể được mở rộng thêm với các tiên đề ontology khác để hướng dẫn các mô hình.

Corpus Văn bản: Corpus văn bản chứa tập hợp các câu ngôn ngữ tự nhiên có chứa các sự kiện có thể được biểu đạt bằng ontology nói trên.

Ví dụ: Các ví dụ minh họa hoặc dữ liệu huấn luyện chứa các cặp câu và các sự kiện được trích xuất từ chúng tuân thủ ontology.

Với những đầu vào này, một hệ thống nên có thể tạo ra các sự kiện tuân thủ một tập hợp các kỳ vọng. Đầu tiên, hệ thống nên sử dụng ontology và các ví dụ minh họa như hướng dẫn về những sự kiện nào cần trích xuất và những quan hệ nào được sử dụng trong đầu ra. Nó nên tuân theo các tên quan hệ chính tắc và định dạng đầu ra ví dụ. Trong đánh giá, chúng tôi đo lường khía cạnh này bằng các thước đo tuân thủ ontology. Thứ hai, hệ thống nên trung thành với câu đầu vào. Điều này có nghĩa là hệ thống chỉ nên xem xét các sự kiện được đề cập trong câu như sự thật (bất kể kiến thức nó có thể có từ việc huấn luyện trước). Nó không nên bao gồm thông tin bổ sung không được trực tiếp hoặc gián tiếp nêu ra hoặc ngụ ý bởi câu. Khía cạnh này được đo lường bằng các thước đo độ chính xác trích xuất sự kiện. Cuối cùng, hệ thống không nên ảo giác tức là nó không nên đưa ra các thực thể/quan hệ mới hoặc giả mạo không được đề cập trong câu và ontology. Khía cạnh này được đo lường bằng các thước đo ảo giác. Phần 4 cung cấp chi tiết về các thước đo đánh giá.

Trong phiên bản này của Text2KGBench, chúng tôi không đánh giá khả năng của hệ thống để xử lý cú pháp RDF/OWL hoặc hiểu biết sâu về ngữ nghĩa OWL. Do đó, chúng tôi đang sử dụng các cách diễn đạt và định dạng bộ ba hướng ngôn ngữ đơn giản hơn để trình bày thông tin cho một LLM. Hình 2 minh họa một ví dụ về việc thực hiện nhiệm vụ bằng cách sử dụng học trong ngữ cảnh của LLM với một prompt.

Có một số thành phần hoặc hướng nghiên cứu có thể ảnh hưởng đến kết quả của một hệ thống được thử nghiệm dưới bộ đánh giá này. Một trong những khía cạnh quan trọng nhất là mô hình (LLM) đang được sử dụng. Tùy thuộc vào các đặc điểm như kiến trúc, dữ liệu huấn luyện được sử dụng, số lượng tham số, và những hướng dẫn nào đã được sử dụng để tinh chỉnh, mỗi mô hình ngôn ngữ có thể có những khả năng khác nhau, và nó có tác động trực tiếp đến kết quả thu được từ mô hình.

Kỹ thuật prompt hoặc tạo prompt tự động cũng đóng vai trò quan trọng trong tác vụ này. Gần đây, có một hướng nghiên cứu tập trung vào cách xây dựng các prompt hiệu quả để có được đầu ra mong đợi từ LLM. Trong bộ đánh giá này, những người tham gia có thể thiết kế các prompt khác nhau được hướng dẫn bởi một ontology và các kỹ thuật lý luận có thể được sử dụng để phát triển các prompt hiệu quả nhất. Liên quan đến việc tạo prompt, một khía cạnh quan trọng khác là cách tìm ví dụ minh họa liên quan nhất hoặc hữu ích nhất từ dữ liệu huấn luyện cho một trường hợp thử nghiệm. Điều này có thể được thực hiện bằng cách sử dụng các thước đo độ tương tự câu hoặc sử dụng các manh mối ngữ nghĩa nâng cao hơn từ ontology.

Xử lý hậu kỳ và xác thực cũng quan trọng để trích xuất các bộ ba chính xác và làm sạch chúng bằng cách loại bỏ các bộ ba không hợp lý. Việc trích xuất ban đầu có thể được thực hiện bằng các kỹ thuật khớp mẫu như sử dụng regex. Xác thực các bộ ba được tạo là một lĩnh vực nghiên cứu mở khác có thể sử dụng các phương pháp ngôn ngữ học để phát hiện ảo giác và các phương pháp dựa trên lý luận để xác thực rằng các bộ ba được tạo ra nhất quán với ontology.

3 Tạo Bộ đánh giá

Text2KGBench bao gồm hai bộ dữ liệu: wikidata-tekgen và dbpedia-webnlg. Như đã thảo luận ở trên, mỗi bộ có một tập hợp các ontology và corpus văn bản nơi các câu được căn chỉnh với các bộ ba theo ontology đã cho.

3.1 Bộ dữ liệu Wikidata-TekGen

Bộ dữ liệu này được tạo ra bằng cách sử dụng việc căn chỉnh câu được cung cấp bởi corpus TekGen.

Lựa chọn Ontology Như bước đầu tiên của việc xây dựng bộ dữ liệu, chúng tôi đã tạo ra 10 ontology nhỏ bằng cách tái sử dụng các khái niệm và quan hệ được mô tả trong Wikidata. Chúng tôi chọn một miền, chẳng hạn như phim hoặc thể thao và khám phá các khái niệm và quan hệ liên quan đến miền đã cho trong Wikidata. Với điều đó, một tập hợp các khái niệm cho miền được xác định, và một mẫu các trường hợp của chúng được kiểm tra cho các quan hệ thường gặp nhất. Một khi một quan hệ được xác định, trang thuộc tính của nó được sử dụng để hiểu việc sử dụng, và các ràng buộc miền phạm vi. Ví dụ, trang thuộc tính cho quan hệ "director (P57)" mô tả các ràng buộc chủ thể và loại giá trị. Một cách lặp đi lặp lại, nhiều khái niệm được thêm vào ontology dựa trên các ràng buộc miền/phạm vi của các quan hệ đã chọn. Quá trình này được thực hiện thủ công và mỗi ontology được hình thành bởi một tác giả có chuyên môn Semantic Web và được xem xét bởi hai chuyên gia khác. Bảng 1 cho thấy thống kê khái niệm và quan hệ cho mỗi trong số 10 ontology mà chúng tôi đã tạo ra.

Một ví dụ ontology cho miền âm nhạc được hiển thị trong Hình 3. Tất cả 10 ontology có sẵn như các ontology OWL được nối tiếp trong Turtle và ở định dạng json nhỏ gọn trong repo.

Tạo bộ ba và căn chỉnh với câu Với một ontology từ bước trước, một truy vấn SPARQL được tham số hóa được sử dụng để tạo ra một tập hợp K bộ ba cho mỗi quan hệ. Truy vấn SPARQL đảm bảo rằng các bộ ba xác nhận các hạn chế miền và phạm vi của mỗi ontology. Ví dụ, cho quan hệ "director", chúng tôi sẽ nhận được các bộ ba như director("Lion King","Roger Allers").

Trong bộ dữ liệu này, chúng tôi tái sử dụng corpus TekGen Agarwal et al. [2021] cung cấp các bộ ba Wikidata được căn chỉnh với các câu tương ứng từ Wikipedia. Corpus TekGen được tạo ra bằng cách sử dụng giám sát từ xa và nó có 16 triệu căn chỉnh bộ ba-câu bao phủ 663 quan hệ Wikidata. Cho mỗi bộ ba chúng tôi nhận được từ bước trước, chúng tôi phân tích corpus TekGen để có được một câu được căn chỉnh khi có sẵn. Ví dụ, bộ ba trong câu trước sẽ được căn chỉnh với một câu như "The Lion King is an animated musical drama film directed by Roger Allers and Rob Minkoff, produced by Don Hahn.". Một khi một câu được tìm thấy, chúng tôi kiểm tra tất cả các quan hệ khác liên quan đến câu trong corpus TekGen và bao gồm chúng cũng nếu chúng là một phần của ontology của chúng tôi. Ví dụ, trong câu này, director ("Lion King", "Rob Minkoff") và producer("Lion King", "Don Hahn") cũng sẽ được bao gồm trong bộ dữ liệu.

Một khi chúng tôi hoàn thành quá trình này cho tất cả 10 ontology, chúng tôi tạo ra 13.474 căn chỉnh câu - bộ ba và chúng được chia thành các tập huấn luyện, xác thực và thử nghiệm.

Xác thực thủ công và làm sạch Vì corpus TekGen được tạo ra bằng cách sử dụng giám sát từ xa, nó có thể có nhiễu và một số căn chỉnh không chính xác. Để đánh giá các mô hình với một tập hợp các trường hợp thử nghiệm chính xác hơn, chúng tôi đã phân tích thủ công các câu thử nghiệm và chọn một tập con nhỏ hơn các câu được căn chỉnh chính xác hơn cho mỗi ontology. Cho bài tập này, các người chú thích nhìn vào bộ ba và câu được căn chỉnh trong tiêu chuẩn vàng và chọn các câu mà một con người có thể dễ dàng trích xuất bộ ba sao cho sự kiện được đề cập rõ ràng trong văn bản. Ví dụ, "The film was also nominated for Academy Award for Best Picture." là một câu nhiễu để trích xuất bộ ba "nominated for(Working Girl, Academy Award for Best Picture) vì không thể cho một mô hình giải quyết coreference để hiểu thuật ngữ "the film" đang đề cập đến gì, chỉ với câu này như đầu vào. Một ví dụ khác, câu "Welcome to Eltingville was written by Dorkin and Chuck Sheetz" được căn chỉnh sai với bộ ba director("Welcome to Eltingville", "Chuck Sheetz") vì các thực thể cùng xuất hiện trong câu và Chuck Sheetz vừa là đạo diễn vừa là nhà văn. Cho một mẫu dữ liệu thử nghiệm, các tác giả đã loại bỏ những căn chỉnh như vậy và tạo ra một tập thử nghiệm khác với 939 căn chỉnh câu-bộ ba đã được xác minh. Các hệ thống có thể sử dụng cả tập thử nghiệm lớn hơn và tập thử nghiệm nhỏ hơn chất lượng cao này cho các đánh giá của họ.

Tạo câu chưa thấy Một trong những cạm bẫy của bộ đánh giá này là các mô hình ngôn ngữ đang được thử nghiệm có thể đã thấy những câu này hoặc thậm chí các căn chỉnh dưới một số hình thức. Sau đó có thể tranh luận rằng họ có thể đã ghi nhớ một số quan hệ này. Một khía cạnh quan trọng để đánh giá là liệu hiệu suất mô hình có bị ảnh hưởng nếu chúng tôi thử nghiệm mô hình với các câu chưa thấy không phải là một phần của Wikipedia và không được thấy trong quá trình huấn luyện trước. Để làm điều đó, chúng tôi phát minh ra các câu mới với các sự kiện mà các người chú thích nghĩ ra. Ví dụ, một câu như "John Doe starred in the movie The Fake Movie released in 2025". Với bài tập này, các tác giả tạo ra 174 câu chưa thấy khoảng hai câu cho mỗi quan hệ trong mỗi ontology. Hơn nữa, tập câu chưa thấy này có thể được sử dụng để kiểm tra mức độ trung thành của mô hình với câu đã cho khi tạo ra các bộ ba.

3.2 Bộ dữ liệu DBpedia-WebNLG

Bộ dữ liệu DBpedia-WebNLG được tạo ra bằng cách tái sử dụng các căn chỉnh trong corpus WebNLG.

Lựa chọn Ontology Tương tự như bộ dữ liệu trước, bước đầu tiên là tạo ra một tập hợp các ontology. WebNLG bao gồm 19 danh mục và chúng tôi tạo ra một ontology cho mỗi danh mục. Đầu tiên, chúng tôi phân tích các bộ ba trong mỗi danh mục để trích xuất các quan hệ trong mỗi danh mục và định nghĩa các khái niệm dựa trên các ràng buộc miền và phạm vi của những quan hệ đó. Thống kê cho 19 ontology kết quả được hiển thị trong Bảng 1.

Tạo bộ ba và căn chỉnh với câu Chúng tôi đã phân tích bộ dữ liệu WebNLG 3.0 tiếng Anh và thu thập các câu trong một trong các phần chia (WebNLG 3triples). Khi tạo các tập huấn luyện và thử nghiệm, chúng tôi đảm bảo rằng cùng một sự kiện sẽ không xuất hiện trong cả tập huấn luyện và thử nghiệm. Vì các căn chỉnh (verbalizations) được xác minh bởi crowdsourcing trong WebNLG, không cần thiết cho chúng tôi tạo ra một tập được xác thực thủ công. Chúng tôi tạo ra 4.860 căn chỉnh câu - bộ ba sử dụng dữ liệu WebNLG và chia thành các phần chia huấn luyện và thử nghiệm.

Các phần chia train/val/test cho cả hai bộ đánh giá được thực hiện như các fold ngẫu nhiên phân tầng nhằm mục đích bảo toàn phân phối quan hệ càng nhiều càng tốt bằng cách sử dụng scikit-learn. Lý do cho các phần chia là cung cấp dữ liệu huấn luyện (ví dụ cho học trong ngữ cảnh hoặc tinh chỉnh mô hình) cho các hệ thống tương lai sẽ sử dụng bộ đánh giá và dữ liệu xác thực (để tối ưu hóa siêu tham số).

4 Thước đo Đánh giá

Trong phần này, chúng tôi trình bày tập hợp các thước đo đánh giá mà chúng tôi sử dụng trong Text2KGBench để đo lường hiệu suất của các hệ thống trong việc tạo ra sự kiện từ văn bản. Các thước đo đánh giá nhằm xác thực ba khía cạnh: (i) các sự kiện được trích xuất chính xác theo ontology đã cho, (ii) các sự kiện được trích xuất tuân thủ ontology đã cho, và (iii) đầu ra không bao gồm bất kỳ ảo giác nào.

Với một prompt tương tự như Hình 2, LLM sẽ tạo ra một đầu ra văn bản có thể được phân tích thành một tập hợp các bộ ba, mà chúng tôi gọi là bộ ba đầu ra LLM. Đầu ra mong đợi cho mỗi câu thử nghiệm nằm trong các tệp ground truth.

Độ chính xác Trích xuất Sự kiện: Điều này được đo lường bằng các điểm số Precision (P), Recall (R), và F1 bằng cách so sánh các bộ ba đầu ra LLM với các bộ ba ground truth. P được tính bằng cách chia số lượng bộ ba LLM chính xác (là một phần của ground truth) cho số lượng bộ ba LLM. R được tính bằng cách chia số lượng bộ ba LLM chính xác cho số lượng bộ ba ground truth. F1 được tính như trung bình hài hòa của P và R. Nếu đầu ra LLM trống, P, R, và F1 được đặt thành 0. Vì tập hợp các bộ ba không đầy đủ cho một câu đã cho, để tránh âm tính giả, chúng tôi tuân theo một phương pháp đóng cục bộ bằng cách chỉ xem xét các quan hệ là một phần của ground truth. Với P, R, F1, số cao hơn đại diện cho hiệu suất tốt hơn.

Tuân thủ Ontology: Điều này được đo lường bằng thước đo Ontology Conformance (OC) được tính như tỷ lệ phần trăm của các bộ ba đầu ra LLM tuân thủ ontology đầu vào, tức là, các bộ ba đầu ra LLM tuân thủ ontology chia cho tổng số bộ ba đầu ra LLM. Trong phiên bản này, một bộ ba được coi là tuân thủ ontology nếu quan hệ là một trong các quan hệ chính tắc được liệt kê trong ontology. Điều này có thể được mở rộng thêm để xác thực các hạn chế khác như miền, phạm vi hoặc các tiên đề ontology khác.

Ảo giác: Ảo giác được định nghĩa như nội dung được tạo ra vô nghĩa hoặc không trung thành với nội dung nguồn được cung cấp [Ji et al., 2023]. Chúng tôi tính ba thước đo ảo giác, ảo giác chủ thể (SH), ảo giác quan hệ (RH), và ảo giác đối tượng (OH). Những thước đo này được tính bằng cách so sánh bộ ba được tạo ra với câu thử nghiệm và ontology. Cho mỗi bộ ba, SH và OH kiểm tra xem chủ thể và đối tượng có mặt trong câu hoặc các khái niệm ontology, và RH kiểm tra xem quan hệ có mặt trong các quan hệ ontology. Với SH và OH, chúng tôi sử dụng stemming để tính đến các dạng uốn cong với các biến thể hình thái như "America", "American", "Americans", v.v. Mỗi thuật ngữ trong chủ thể hoặc đối tượng và câu thử nghiệm được stemmed trước khi kiểm tra xem chủ thể hoặc đối tượng có mặt như một chuỗi con trong câu thử nghiệm và/hoặc các khái niệm ontology. Với RH, các quan hệ được khớp bằng cách sử dụng khớp chính xác. Trong phiên bản này, RH và OC có liên quan nghịch đảo tức là 1 - OC bằng RH.

5 Baseline và Kết quả Đánh giá

Trong phần này, chúng tôi trình bày các mô hình LLM baseline, baseline cho việc tạo prompt tự động và kết quả đánh giá cho các mô hình baseline cho hai bộ dữ liệu của Text2KGBench mà chúng tôi đã mô tả trong Phần 3.

5.1 Mô hình LLM Baseline

5.1.1 Vicuna-13B

Vicuna-13B [Chiang et al., 2023] là một LLM mã nguồn mở tinh chỉnh mô hình LLaMA cơ sở với 70K cuộc trò chuyện do người dùng chia sẻ từ ShareGPT. Chúng tôi có được mô hình LlaMA 13B LLM, checkpoint, và tokenizer, thông qua kho Pyllama Github và áp dụng trọng số Vicuna từ FastChat như trọng số delta. Vicuna-13B tuyên bố có 90% hiệu suất của OpenAI ChatGPT và Google Bard [Zheng et al., 2023] nơi các tác giả đã sử dụng một thước đo "Relative Response Quality" sử dụng LLM mạnh (GPT4) như thẩm phán để đánh giá mô hình trên các câu hỏi mở.

5.1.2 Alpaca-LoRA-13B

Alpaca-LoRA là một mô hình tinh chỉnh mô hình LLaMA cơ sở với cùng 52K hướng dẫn của mô hình Alpaca được tạo ra bằng cách sử dụng self-instruct [Wang et al., 2022] với mô hình text-davinci-003 của OpenAI. Alpaca-LoRA được tinh chỉnh bằng cách sử dụng Low-Rank Adaptation [Hu et al., 2022] cho phép giảm số lượng tham số có thể huấn luyện bằng một bậc đáng kể bằng cách đóng băng trọng số mô hình được huấn luyện trước và tiêm các ma trận phân rã rank có thể huấn luyện vào mỗi lớp transformer.

5.2 Tạo Prompt Tự động

Cả hai mô hình LLM của chúng tôi đều là các mô hình chỉ giải mã kiểu GPT được tinh chỉnh hướng dẫn. Chúng có thể được sử dụng cho các tác vụ downstream bằng cách cung cấp một prompt với một hướng dẫn. Trong phần này, chúng tôi trình bày các bước liên quan đến việc tự động tạo prompt cho mỗi câu thử nghiệm.

Prompt baseline của chúng tôi bao gồm các phần chính của chúng tôi: (a) Hướng dẫn, (b) Mô tả ontology, (c) Ví dụ minh họa, và (d) Câu thử nghiệm như được minh họa trong Hình 2.

Hướng dẫn Đây là một hướng dẫn cố định mà chúng tôi sử dụng cho tất cả các trường hợp thử nghiệm qua các ontology. Chúng tôi sử dụng cụm từ sau "Given the following ontology and sentences, please extract the triples from the sentence according to the relations in the ontology. In the output, only include the triples in the given output format." như hướng dẫn. Chúng tôi mô tả nhiệm vụ cũng như yêu cầu mô hình ít dài dòng hơn và chỉ xuất ra các bộ ba trong định dạng đã cho.

Mô tả ontology Phần này của prompt cung cấp một mô tả về ontology cho mô hình như ngữ cảnh. Mỗi trường hợp thử nghiệm trong bộ đánh giá của chúng tôi được liên kết với một ontology. Phần này của prompt diễn đạt ontology bằng cách liệt kê tập hợp các khái niệm, và một tập hợp các quan hệ với các ràng buộc miền và phạm vi của chúng được đưa ra bởi ontology. Ví dụ, cho trường hợp thử nghiệm trong ontology phim, các khái niệm sẽ là một danh sách như một bộ phim, thể loại phim, thể loại, công ty sản xuất phim, giải thưởng phim, con người v.v. và các quan hệ sẽ là một danh sách như director(film, human), cast_member(film, human), award_received(film, award), genre(film, genre), production_company(film, film production company), v.v. Trong suốt prompt, chúng tôi sử dụng ký hiệu relation(subject, object) để biểu diễn các quan hệ và mong đợi mô hình tuân theo ký hiệu trong đầu ra.

Ví dụ Minh họa Phần này của prompt được sử dụng để cung cấp cho LLM một ví dụ để hiển thị một câu đầu vào và đầu ra mong đợi. LLM có khả năng Học trong Ngữ cảnh nơi chúng học nhiệm vụ và định dạng đầu ra từ các ví dụ được cung cấp trong prompt. Các ví dụ được lấy từ dữ liệu huấn luyện cho mỗi bộ dữ liệu dựa trên độ tương tự của chúng với câu thử nghiệm. Chúng tôi đã sử dụng độ tương tự câu bằng cách sử dụng Sentence Transformers (SBERT) [Reimers và Gurevych, 2019] với mô hình T5-XXL [Ni et al., 2022]. Ví dụ, với một câu thử nghiệm như "Super Capers, a film written by Ray Griggs, is 98 minutes long.", nó có thể tìm câu tương tự nhất trong dữ liệu huấn luyện, "English Without Tears, written by Terence Rattigan, runs 89 minutes." với các bộ ba được căn chỉnh của nó. Đầu ra ví dụ tuân theo cùng ký hiệu quan hệ.

Câu Thử nghiệm Cuối cùng, prompt chứa câu thử nghiệm từ đó chúng tôi muốn trích xuất các sự kiện tuân thủ ontology. Tương tự như ví dụ, prompt kết thúc với một "Test Output:" nơi mô hình được mong đợi tạo ra các sự kiện trong câu tuân theo cùng định dạng như trong câu ví dụ.

5.3 Kết quả Đánh giá

Chúng tôi chạy suy luận cho các prompt được tạo tự động cho cả corpus Wikidata-TekGen và DBpedia-WebNLG và tính toán các thước đo được thảo luận trong Phần 4: Precision (P), Recall (R), F1, Ontology Conformance (OC), Subject/Relation/ Object Hallucinations (SH/RH/OH). Bảng 2 minh họa các giá trị trung bình qua tất cả các ontology trong một bộ dữ liệu đã cho. Như đã thảo luận trong Phần 3, ba cài đặt khác nhau trong bộ dữ liệu Wikidata-TekGen: tất cả các trường hợp thử nghiệm (All), tập con được xác thực và làm sạch thủ công (selected), và các câu chưa thấy (Unseen) được chú thích trong cột "Variant".

Mỗi hàng trong Bảng 2 là một tổng hợp kết quả từ các trường hợp thử nghiệm qua nhiều ontology (10 cho Wikidata-TekGen và 19 cho DBpedia-WebNLG) và Bảng 3 hiển thị kết quả ở mức ontology cá nhân cho hàng đầu tiên của Bảng 2, tức là, Wikidata-TekGen - Vicuna - All. Để ngắn gọn, kết quả mức ontology cho các hàng khác được bao gồm trong Wiki dự án.

Từ kết quả, một số quan sát ban đầu từ Bảng 2 về các bộ dữ liệu khác nhau và mô hình LLM:

• Precision, Recall và điểm F1 có giá trị trung gian thấp
• Ontology Conformance khá cao trong hầu hết tất cả các mục
• Subject, Relation, Object Hallucination tương đối thấp

Những kết quả này nên được phân tích thêm để hiểu các khả năng và hạn chế khác nhau của LLM trong việc tạo KG từ văn bản. Một phân tích sâu về kết quả nằm ngoài phạm vi của bài báo này do hạn chế về không gian và chúng tôi mong đợi các bài báo hệ thống sử dụng bộ đánh giá sẽ cung cấp những hiểu biết và kết luận về khía cạnh này. Vì chúng tôi đã sử dụng các mô hình LLM như chúng vốn có mà không có bất kỳ tinh chỉnh, điều chỉnh prompt hoặc xác thực ngữ nghĩa nào, chúng tôi tin rằng có một chỗ cải thiện lớn.

5.4 Phân tích Lỗi

Chúng tôi đã thực hiện một phân tích lỗi ban đầu để hiểu các lỗi do các mô hình tạo ra và Bảng 4 hiển thị một số ví dụ cho các loại lỗi khác nhau. Ngoài ra, chúng tôi nhận thấy rằng có một số dương tính giả trong ảo giác do LLM mở rộng từ viết tắt, ví dụ, câu có thể có "NATO" nơi mô hình tạo ra "North Atlantic Treaty Organization" như một chủ thể. Chúng tôi có kế hoạch xem xét các từ viết tắt, bí danh, v.v. trong tính toán ảo giác trong tương lai.

6 Công trình Liên quan

Mục tiêu chính của nhiệm vụ tạo đồ thị kiến thức là trích xuất thông tin có cấu trúc từ các nguồn không đồng nhất. Phần này sẽ khám phá các Bộ đánh giá Trích xuất Quan hệ, Mô hình Nền tảng cho Việc Tạo Đồ thị Kiến thức và Hoàn thiện Đồ thị Kiến thức (KBC)-KG-triple generation Bán tự động/Tự động. Trích xuất quan hệ đã sử dụng đáng kể các bộ dữ liệu sau như bộ dữ liệu New York Times (NYT)/ NYT-FB [Mintz et al., 2009] [Riedel et al., 2010] [Marcheggiani và Titov, 2016], TAC Relation Extraction Dataset (TACRED) [Zhang et al., 2017], Large-Scale Document-Level Relation Extraction Dataset(DocRED) [Yao et al., 2019], Bộ dữ liệu WEB-NLG [Gardent et al., 2017], bộ dữ liệu FewRel [Han et al., 2018], FewRel 2.0 [Gao et al., 2019]. Các bộ đánh giá trích xuất quan hệ tồn tại cho lĩnh vực khoa học là bộ dữ liệu SciERC [Luan et al., 2018] và SCIREX [Jain et al., 2020]. Bộ dữ liệu SCIREX được dự định để phát hiện cả quan hệ nhị phân và n-ary giữa các thực thể và khái niệm, trong khi bộ dữ liệu SciERC được dự định để xác định quan hệ nhị phân giữa các thực thể trong các bài báo khoa học. Có một số bộ dữ liệu bao phủ nhiều ngôn ngữ, như bộ dữ liệu Multilingual LAMA (Language Model Analysis) [Kassner et al., 2021], bao phủ 53 ngôn ngữ, MiLER SMiLER(Samsung Multi-Lingual Entity and Relation Extraction dataset [Seganti et al., 2021] bao phủ 14 ngôn ngữ, DiS-ReX [Bhartiya et al., 2022] bao phủ 4 ngôn ngữ. Thông qua liên kết thực thể, Knowledge-Enhanced Relation Extraction Dataset (KERED) [Lin et al., 2022] cung cấp ngữ cảnh kiến thức cho các thực thể và chú thích mỗi câu với một sự kiện quan hệ. Bộ dữ liệu này bao gồm NYT10m, Wikidata [Vrandečić và Krötzsch, 2014] (Wiki80 và Wiki20m).

Các bộ dữ liệu đánh giá trích xuất quan hệ có thể được sử dụng để đánh giá hiệu suất của các mô hình nền tảng. Một bài báo khảo sát [Yang et al., 2023] đã khám phá lịch sử của những mô hình nền tảng này và tóm tắt các tác vụ khác nhau. Các mô hình nền tảng thường được phân loại thành hai danh mục: Chỉ Encoder hoặc Encoder-Decoder (kiểu BERT) và Chỉ Decoder (kiểu GPT) [Yang et al., 2023]. Các mô hình kiểu BERT vẫn đang thách thức vì chúng đang được phát triển và chủ yếu có sẵn như mã nguồn mở. Chúng được coi là Mô hình Ngôn ngữ Mask bao gồm RoBERTa [Liu et al., 2019], BERT [Devlin et al., 2018], và T5 [Raffel et al., 2020]. Các mô hình chỉ Decoder (kiểu GPT) (GPT-3 [Brown et al., 2020], PaLM [Chowdhery et al., 2022], OPT [Zhang et al., 2022] và BLOOM [Scao et al., 2022]) thường cần tinh chỉnh trên các bộ dữ liệu của tác vụ downstream cụ thể. Brown et. al. [Brown et al., 2020] đã huấn luyện GPT-3 (một mô hình ngôn ngữ tự hồi quy) với 175 tỷ tham số và cũng thử nghiệm hiệu suất của nó với cài đặt few-shot. Jeremy và Sebastian [Howard và Ruder, 2018b] đã đề xuất một phương pháp học chuyển giao hiệu quả, Universal Language Model Fine-tuning (ULMFiT), cho bất kỳ tác vụ NLP nào. Brian et. al. [Lester et al., 2021b] khám phá điều chỉnh prompt để học các prompt mềm để điều chỉnh các mô hình ngôn ngữ. Các prompt mềm được học bằng lan truyền ngược, trong khi GPT-3 sử dụng các prompt văn bản rời rạc [Wang et al., 2019].

Vicuna [Chiang et al., 2023] là một chatbot mã nguồn mở và nó được huấn luyện bằng cách tinh chỉnh LLaMA. Nó được hiển thị bằng đánh giá rằng Vicuna đã thực hiện hơn 90% chất lượng của Google Bard và OpenAI ChatGPT so với các mô hình khác như LLaMA và Alpaca. Alpaca [Taori et al., 2023] đã được giới thiệu như một mô hình tuân theo hướng dẫn mạnh mẽ, có thể tái tạo. Nó được tinh chỉnh từ mô hình LLaMA 7B trên 52K minh họa tuân theo hướng dẫn.

KBC và KG-triple generation đã trở thành một lĩnh vực nghiên cứu nóng với sự phối hợp/tích hợp với LLM. Các khả năng là vô hạn về việc tự động tạo ra các bộ ba mới thông qua việc sử dụng LLM và "gót chân Achilles" duy nhất bao gồm tài nguyên máy tính cần thiết để tích hợp cả hai hệ thống. Trong [Bi et al., 2023] được trình bày một hệ thống tự động tạo ra các bộ ba từ ngôn ngữ tự nhiên và các tác vụ hoàn thiện mã. Trong trường hợp này, nó được trình bày như các đoạn mã đầu vào biểu thị định nghĩa lớp và hàm. Họ coi việc sử dụng mạng nơ-ron có mặt trong LLM được huấn luyện trước như "hộp đen". Trong [Alivanistos et al., 2022] được trình bày một hệ thống sử dụng GPT3 LLM với mục tiêu xây dựng một cơ sở kiến thức bán tự động thông qua một quá trình nhiều bước kết hợp các kỹ thuật prompting tùy chỉnh để dự đoán các đối tượng bị thiếu trong các bộ ba nơi các chủ thể và quan hệ được đưa ra. Trong [Khorashadizadeh et al., 2023], các tác giả thực hiện một nghiên cứu định tính về các mô hình ngôn ngữ lớn sử dụng ChatGPT cho các tác vụ khác nhau bao gồm điền đầy KG, hoàn thiện KG, xác minh bộ ba hoặc sự kiện và xác định một số thách thức như ảo giác, công bằng và thiên vị, và chi phí tính toán cao. Và cuối cùng, chúng tôi bao gồm tham chiếu [Veseli et al., 2023] trong phần này nơi nó được trình bày một bộ dữ liệu đánh giá để đánh giá tiềm năng Knowledge Base Completion (KBC) cho các mô hình ngôn ngữ (LM).

Về các thước đo ảo giác, dữ liệu được tạo ra bởi hệ thống dựa trên Deep Learning nhạy cảm với việc ảo giác văn bản "không trung thành" làm giảm hiệu suất hệ thống, đặc biệt trong các hệ thống quan trọng như dữ liệu bệnh nhân nơi dữ liệu không đúng sự thật có thể gây ra rủi ro nghiêm trọng cho bệnh nhân. Trong Ji et al. [2023] được trình bày một khảo sát mô tả các thước đo ảo giác và các tác vụ Tạo Ngôn ngữ Tự nhiên (NLG) downstream chung nhạy cảm với hiện tượng ảo giác, như Trả lời Câu hỏi Tạo sinh (GQA), Tạo Đối thoại, Tạo Dữ liệu-sang-văn bản, và vân vân. Các tác giả chứng minh khó khăn mà việc phát hiện văn bản vô nghĩa/không trung thành với nội dung nguồn trong các hệ thống dựa trên NLG/Transformer gây ra. Các tác giả phân biệt giữa ảo giác "nội tại" và "ngoại tại", cái trước là đầu ra generator không "trung thành" với nguồn và cái sau là đầu ra không thể được xác nhận, cũng không bị phủ nhận từ nội dung nguồn tham chiếu đến ví dụ như các nguồn sự kiện bên ngoài thế giới thực. Trong trường hợp của chúng tôi, chúng tôi tập trung vào ảo giác nội tại và bao gồm trong phần công việc tương lai về phát hiện ảo giác ngoại tại. Chúng tôi không phân biệt như được cung cấp trong khảo sát, giữa "trung thành" và "thực tế". Chúng tôi sử dụng đầu vào nguồn/ground truth như sự kiện (tức là ví dụ như sự tuân thủ ontology) chỉ ra mức độ "trung thành" của dữ liệu như vậy. Về phương pháp cho các phép đo thước đo ảo giác, hệ thống của chúng tôi dựa trên các thước đo dựa trên mô hình như dựa trên Trích xuất Thông tin đại diện kiến thức như các bộ ba tuple Semantic Web (tức là, ví dụ: relation(subject, object) triples được hỗ trợ bởi tiền xử lý stemming). Chúng tôi cũng dự kiến các thước đo trung thành/ảo giác dựa trên fuzzy hơn bằng cách áp dụng embeddings kiến thức dựa trên vector và truy xuất thông qua K-Nearest-Neighbors (KNN) các bộ ba liên quan nhất với một ngưỡng đã cho.

7 Kết luận và Công việc Tương lai

Trong bài báo này, chúng tôi đã trình bày Text2KGBench, một bộ đánh giá để đánh giá khả năng của LLM trong việc trích xuất sự kiện từ một corpus văn bản được hướng dẫn bởi một ontology.

Hạn chế Trong phiên bản này, chúng tôi chỉ xem xét các ontology có kích thước nhỏ hơn theo thiết kế để phục vụ cho các hạn chế kích thước token của LLM. Tuy nhiên, trong thực tế, có những ontology khá lớn hơn trong các lĩnh vực như y học. Trong các phiên bản tương lai, chúng tôi có kế hoạch bao gồm các trường hợp với các ontology lớn hơn nhiều sẽ yêu cầu các hệ thống tự động chọn phần của ontology hoặc tập hợp các tiên đề liên quan đến văn bản đầu vào đã cho. Ngoài ra, có nghiên cứu về việc mở rộng khả năng của LLM để xử lý các ngữ cảnh dài hơn như Unlimiformer[Bertsch et al., 2023]. Hơn nữa, trong phiên bản này, chúng tôi đã tách các biểu diễn OWL/RDF của KG bằng cách diễn đạt các ontology và bộ ba. Trong các phiên bản tương lai, chúng tôi sẽ thử nghiệm LLM về việc xử lý những biểu diễn này trực tiếp mà không có tiền/hậu xử lý.

Công việc Tương lai Một khía cạnh quan trọng khi nói đến các mô hình nền tảng là thiên vị và công bằng. Trong công việc tương lai, chúng tôi muốn mở rộng thêm bộ đánh giá của chúng tôi xem xét các biến thiên vị khác nhau như giới tính, chủng tộc/dân tộc, vị trí địa lý, v.v. và tạo ra các trường hợp thử nghiệm đối chiếu để xác minh tính công bằng của LLM khi tạo Đồ thị Kiến thức từ văn bản. Nói cách khác, chúng tôi muốn đánh giá một cách có hệ thống liệu quá trình này có thực hiện tốt hơn cho một nhóm con nhất định dựa trên giới tính, nhân khẩu học, hoặc tình trạng kinh tế xã hội của họ. Hơn nữa, chúng tôi sẽ có kế hoạch đo lường nhiều khả năng lý luận hơn khi thực hiện trích xuất sự kiện và tạo KG. Chúng tôi có kế hoạch mở rộng bộ đánh giá với một bộ dữ liệu yêu cầu lý luận ngữ nghĩa nhiều hơn để thực hiện nhiệm vụ. Trong bộ đánh giá Text2KGBench, chúng tôi hiện tại đã tập trung vào các mô hình LLM mã nguồn mở có sẵn. Ngoài ra, chúng tôi có kế hoạch so sánh cả hai baseline LLM, Vicuna-13B và Alpaca-Lora-13B, và bất kỳ LLM mã nguồn mở mới nổi nào với ChatGPT và GPT-4 LLM thương mại của OpenAI.

Tác động: Với sự phổ biến của các LLM giống GPT, có một sự nhiệt tình lớn cho việc sử dụng những mô hình như vậy cùng với KG và để xây dựng KG. Các tác giả tin chắc rằng việc xây dựng KG dựa trên ontology từ văn bản tận dụng LLM sẽ được cộng đồng Semantic Web quan tâm. Theo hiểu biết tốt nhất của chúng tôi, Text2KG là bộ đánh giá đầu tiên cho nhiệm vụ này. Chúng tôi cung cấp tất cả các tài nguyên cần thiết để sử dụng và mở rộng thêm bộ đánh giá với các cải thiện. Các tác giả dự đoán rằng điều này sẽ truyền cảm hứng cho nghiên cứu theo hướng này bằng cách cung cấp một cách để đo lường và so sánh hiệu suất của các phương pháp khác nhau.

Khả năng Tái sử dụng và Tính Bền vững: Có hai hội thảo đang diễn ra liên quan đến việc tạo KG từ văn bản, Text2KG tại ESWC và NLP4KGC tại Web Conference. Hơn nữa, có một số đặc biệt được đề xuất về chủ đề này tại tạp chí Semantic Web. Đây sẽ là một tài nguyên hữu ích để đánh giá các phương pháp được trình bày tại những địa điểm đó. Vì các tác giả cũng là đồng tổ chức của những sự kiện này, họ có kế hoạch duy trì và cung cấp các phiên bản tương lai được cải thiện của dữ liệu phối hợp với những hội thảo đó. Cũng quan trọng cần lưu ý rằng các tác giả và tổ chức của các hội thảo nói trên không phải từ một tổ chức duy nhất mà được phân phối qua nhiều tổ chức và làm cho tài nguyên được đề xuất không phụ thuộc vào một tổ chức duy nhất. Mã được sử dụng để tạo ra tài nguyên có sẵn làm cho bất kỳ ai có thể tái tạo, cải thiện hoặc tạo ra công trình phái sinh từ nó.

Tuyên bố Tính Sẵn có Tài nguyên: Bộ dữ liệu Text2KGBench có sẵn từ zenodo, và mã được sử dụng để tạo ra bộ đánh giá, script đánh giá, baseline, đầu ra LLM, kết quả đánh giá có sẵn từ Github. Các bộ dữ liệu thô chúng tôi sử dụng là corpus TekGen và corpus WebNLG. Các mô hình LLM chúng tôi sử dụng là LLaMA để tạo ra Vicuna-13B và Alpaca-LoRA-13B. Cho độ tương tự câu chúng tôi sử dụng SBERT với mô hình T5-XXL.
