# 2310.06770.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/benchmark/2310.06770.pdf
# Kích thước tệp: 4533722 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
SWE-BENCH: CÁC MÔ HÌNH NGÔN NGỮ CÓ THỂ GIẢI QUYẾT
CÁC VẤN ĐỀ GITHUB THỰC TẾ KHÔNG?
Carlos E. Jimenez* 1,2 John Yang* 1,2 Alexander Wettig1,2
Shunyu Yao1,2 Kexin Pei3 Ofir Press1,2 Karthik Narasimhan1,2
1Đại học Princeton 2Princeton Language and Intelligence 3Đại học Chicago
TÓM TẮT
Các mô hình ngôn ngữ đã vượt qua khả năng đánh giá hiệu quả của chúng ta, nhưng để
phát triển tương lai của chúng, việc nghiên cứu biên giới khả năng của chúng là thiết yếu.
Chúng tôi nhận thấy kỹ thuật phần mềm thực tế là một testbed phong phú, bền vững và
đầy thử thách để đánh giá thế hệ tiếp theo của các mô hình ngôn ngữ. Để đạt được mục
đích này, chúng tôi giới thiệu SWE-bench, một khung đánh giá bao gồm 2.294 vấn đề
kỹ thuật phần mềm được rút ra từ các vấn đề GitHub thực tế và các pull request tương
ứng trên 12 kho lưu trữ Python phổ biến. Cho một codebase cùng với mô tả của một vấn
đề cần được giải quyết, một mô hình ngôn ngữ được giao nhiệm vụ chỉnh sửa codebase
để giải quyết vấn đề. Việc giải quyết các vấn đề trong SWE-bench thường yêu cầu hiểu
và phối hợp các thay đổi trên nhiều hàm, lớp và thậm chí các tệp đồng thời, đòi hỏi các
mô hình tương tác với môi trường thực thi, xử lý các ngữ cảnh cực kỳ dài và thực hiện
lý luận phức tạp vượt xa các nhiệm vụ tạo mã truyền thống. Các đánh giá của chúng tôi
cho thấy cả các mô hình độc quyền tiên tiến nhất và mô hình tinh chỉnh SWE-Llama của
chúng tôi chỉ có thể giải quyết những vấn đề đơn giản nhất. Mô hình hoạt động tốt nhất,
Claude 2, chỉ có thể giải quyết 1.96% các vấn đề. Những tiến bộ trên SWE-bench đại
diện cho các bước tiến hướng tới các LM thực tế hơn, thông minh hơn và tự động hơn.

1 GIỚI THIỆU
Các mô hình ngôn ngữ (LM) đang được triển khai nhanh chóng trong các sản phẩm thương mại như
chatbot và trợ lý lập trình. Đồng thời, các benchmark hiện tại đã trở nên bão hòa (Kiela et al., 2021;
Ott et al., 2022) và không thể nắm bắt được biên giới của những gì các LM tiên tiến có thể và không
thể làm. Cần có những benchmark đầy thử thách phản ánh chính xác hơn các ứng dụng thực tế của
LM để giúp định hình phát triển và sử dụng tương lai của chúng (Srivastava et al., 2023).

euclidean_diffma trix_transf ormds tack_s truc t_colv s tack_s truc t_coljoin_s truc t_colPr e PRP os t PRT es tsUnit T es ts
data leak in GBD T due t o w arm
s tart (This is about the non-
his t o gram-bas ed v ersion of ...IssueCodebasesklearn/e x amples/se tup.cf gse tup.p yREADME.rs tr eqs.txtLanguage ModelGenera t ed PRsklearngradient_boosting.pyutilshelper.py+20  -12

Hình 1: SWE-bench lấy các instance nhiệm vụ từ các kho lưu trữ Python thực tế bằng cách kết nối
các vấn đề GitHub với các giải pháp pull request đã được merge giải quyết các test liên quan. Được
cung cấp với văn bản vấn đề và một snapshot codebase, các mô hình tạo ra một patch được đánh
giá dựa trên các test thực tế.

Xây dựng một benchmark tốt là khó khăn vì các nhiệm vụ phải đủ thử thách để làm khó các mô hình
hiện tại, nhưng các dự đoán của mô hình cũng phải dễ dàng xác minh (Martínez-Plumed et al.,
2021). Các nhiệm vụ lập trình hấp dẫn vì chúng đặt ra những vấn đề đầy thử thách cho LM nhưng
các giải pháp được tạo ra có thể dễ dàng xác minh bằng cách chạy unit test. Tuy nhiên, các benchmark
lập trình hiện tại, như HumanEval (Chen et al., 2021), chủ yếu liên quan đến các vấn đề khép kín có
thể được giải quyết trong vài dòng mã.

*Đóng góp ngang nhau. Liên hệ với carlosej@princeton.edu, johnby@stanford.edu.
Dữ liệu, mã và bảng xếp hạng tại swebench.com
1arXiv:2310.06770v3 [cs.CL] 11 Nov 2024

--- TRANG 2 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Trong thế giới thực, kỹ thuật phần mềm không đơn giản như vậy. Sửa một lỗi có thể liên quan đến việc điều hướng một kho lưu trữ lớn, hiểu sự tương tác giữa các hàm trong các tệp khác nhau, hoặc phát hiện một lỗi nhỏ trong mã phức tạp. Được truyền cảm hứng từ điều này, chúng tôi giới thiệu SWE-bench, một benchmark đánh giá các LM trong một môi trường kỹ thuật phần mềm thực tế. Như được hiển thị trong Hình 1, các mô hình được giao nhiệm vụ giải quyết các vấn đề (thường là báo cáo lỗi hoặc yêu cầu tính năng) được gửi đến các kho lưu trữ GitHub phổ biến. Mỗi nhiệm vụ yêu cầu tạo ra một patch mô tả các thay đổi cần áp dụng cho codebase hiện có. Codebase được sửa đổi sau đó được đánh giá bằng framework kiểm thử của kho lưu trữ.

SWE-bench cung cấp một số lợi thế so với các benchmark lập trình LM hiện có. Bao gồm, một môi trường thực tế sử dụng các vấn đề và giải pháp do người dùng gửi, các đầu vào đa dạng có các vấn đề mã độc đáo từ 12 kho lưu trữ, một framework mạnh mẽ cho đánh giá dựa trên thực thi, và khả năng cập nhật liên tục benchmark với các instance mới, yêu cầu can thiệp tối thiểu từ con người.

Chúng tôi đánh giá nhiều LM tiên tiến trên SWE-bench và nhận thấy chúng không thể giải quyết tất cả trừ những vấn đề đơn giản nhất. Sử dụng một bộ truy xuất BM25, Claude 2 chỉ có thể giải quyết 1.96% các vấn đề.

Ngoài SWE-bench, các đóng góp của chúng tôi bao gồm việc phát hành một dataset huấn luyện, SWE-bench-train, điều này là thiết yếu để thúc đẩy phát triển mô hình mở trong lĩnh vực đầy thử thách này. Dataset này bao gồm một bộ sưu tập 19.000 instance nhiệm vụ không phải kiểm thử được rút ra từ 37 kho lưu trữ.

Sử dụng SWE-bench-train, chúng tôi phát hành hai mô hình tinh chỉnh, SWE-Llama 7b và 13b, dựa trên mô hình CodeLlama (Rozière et al., 2023). Chúng tôi nhận thấy trong một số cài đặt SWE-Llama 13b có khả năng cạnh tranh với Claude 2 và có thể xử lý các ngữ cảnh vượt quá 100.000 token.

2 SWE-BENCH

SWE-bench là một benchmark có các vấn đề GitHub từ các kho lưu trữ phổ biến báo cáo lỗi hoặc yêu cầu tính năng mới, và các pull request thực hiện thay đổi vào kho lưu trữ để giải quyết những vấn đề này. Nhiệm vụ là tạo ra một pull request giải quyết một vấn đề được cho và vượt qua các test liên quan đến vấn đề.

2.1 XÂY DỰNG BENCHMARK

GitHub là một nguồn dữ liệu phong phú cho phát triển phần mềm, nhưng các kho lưu trữ, vấn đề và pull request có thể ồn ào, tùy ý hoặc được tài liệu hóa và duy trì kém. Để tìm các instance nhiệm vụ chất lượng cao ở quy mô lớn, chúng tôi sử dụng một pipeline 3 giai đoạn như sau.

R esolv es an issue
Contribut es t es ts✓ ✓ A ttribut e Filt er21Scrape PR s12 popular r eposit ories>90% Python Code3Ins talls successfully
PR passes all t es ts✓✓Ex ecution Filt er

Hình 2: Các instance nhiệm vụ SWE-bench được tạo từ các pull request đã merge giải quyết một vấn đề, đóng góp test và cài đặt thành công.

Giai đoạn I: Lựa chọn repo và thu thập dữ liệu. Chúng tôi bắt đầu bằng việc thu thập các pull request (PR) từ 12 kho lưu trữ Python mã nguồn mở phổ biến trên GitHub, tạo ra khoảng ~90.000 PR tổng cộng. Chúng tôi tập trung vào các kho lưu trữ phổ biến vì chúng có xu hướng được duy trì tốt hơn, có hướng dẫn đóng góp rõ ràng và có coverage test tốt hơn. Mỗi PR có một codebase liên kết được chỉ định bởi base commit của nó.

Giai đoạn II: Lọc dựa trên thuộc tính. Chúng tôi tạo các nhiệm vụ ứng viên bằng cách lựa chọn các PR đã merge mà (1) giải quyết một vấn đề GitHub và (2) thực hiện thay đổi vào các tệp test của kho lưu trữ, điều này cho thấy người dùng có khả năng đã đóng góp test để kiểm tra xem vấn đề đã được giải quyết hay chưa.

Giai đoạn III: Lọc dựa trên thực thi. Đối với mỗi nhiệm vụ ứng viên, chúng tôi áp dụng nội dung test của PR và ghi lại các kết quả test liên quan trước và sau khi nội dung khác của PR được áp dụng. Chúng tôi lọc bỏ các instance nhiệm vụ không có ít nhất một test mà trạng thái của nó thay đổi từ fail sang pass (sau đây được gọi là fail-to-pass test). Chúng tôi cũng lọc bỏ các instance dẫn đến lỗi cài đặt hoặc runtime.

--- TRANG 3 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Thông qua các giai đoạn lọc này, 90.000 PR ban đầu được lọc xuống còn 2.294 instance nhiệm vụ tạo thành SWE-bench. Một phân tích cuối cùng của các instance nhiệm vụ này qua các kho lưu trữ được trình bày trong Hình 3, và Bảng 1 làm nổi bật các tính năng chính của các instance nhiệm vụ SWE-bench. Chúng tôi nhấn mạnh rằng các codebase lớn với hàng nghìn tệp, và các pull request tham chiếu thường thực hiện thay đổi cho nhiều tệp cùng một lúc. Các chi tiết kỹ thuật về pipeline xây dựng SWE-bench được thảo luận trong Phụ lục A. Thống kê dataset bổ sung được đưa vào Phụ lục A.5.

2.2 CÔNG THỨC NHIỆM VỤ

Đầu vào mô hình. Một mô hình được cung cấp mô tả văn bản vấn đề và một codebase hoàn chỉnh. Mô hình sau đó được giao nhiệm vụ thực hiện chỉnh sửa vào codebase để giải quyết vấn đề. Trong thực tế, chúng tôi biểu diễn các chỉnh sửa như các tệp patch, chỉ định những dòng nào trong codebase cần sửa đổi để giải quyết vấn đề.

Phương pháp đánh giá. Để đánh giá một giải pháp được đề xuất, chúng tôi áp dụng patch được tạo ra, sử dụng chương trình patch của unix, vào codebase và sau đó thực thi các unit và system test liên quan đến instance nhiệm vụ. Nếu patch áp dụng thành công và tất cả các test này pass, chúng tôi coi giải pháp được đề xuất đã giải quyết thành công vấn đề. Phương pháp cho benchmark của chúng tôi là tỷ lệ phần trăm các instance nhiệm vụ được giải quyết. Các chi tiết kỹ thuật bổ sung trong Phụ lục A.4.

2.3 ĐẶC ĐIỂM CỦA SWE-BENCH

Các benchmark truyền thống trong NLP thường chỉ liên quan đến các chuỗi đầu vào và đầu ra ngắn và xem xét các vấn đề khá "nhân tạo" được tạo ra đặc biệt cho benchmark. Ngược lại, môi trường xây dựng thực tế của SWE-bench tạo ra cho dataset những tính chất độc đáo, mà chúng tôi thảo luận dưới đây.

Các nhiệm vụ kỹ thuật phần mềm thực tế. Vì mỗi instance nhiệm vụ trong SWE-bench bao gồm một codebase lớn và phức tạp cùng với mô tả của một vấn đề liên quan, việc giải quyết SWE-bench yêu cầu thể hiện các kỹ năng và kiến thức tinh vi mà các kỹ sư phần mềm có kinh nghiệm sở hữu nhưng không được đánh giá thường xuyên trong các benchmark tạo mã truyền thống.

Có thể cập nhật liên tục. Quy trình thu thập của chúng tôi có thể dễ dàng áp dụng cho bất kỳ kho lưu trữ Python nào trên GitHub và yêu cầu can thiệp tối thiểu từ con người. Do đó, chúng tôi có thể mở rộng SWE-bench với một nguồn cung cấp liên tục các instance nhiệm vụ mới và đánh giá LM trên các vấn đề được tạo sau ngày huấn luyện của chúng, điều này đảm bảo rằng giải pháp không được bao gồm trong corpus huấn luyện của chúng.

Các đầu vào dài đa dạng. Các mô tả vấn đề thường dài và chi tiết (trung bình 195 từ), và các codebase thường xuyên chứa hàng nghìn tệp. Việc giải quyết SWE-bench yêu cầu xác định số lượng tương đối nhỏ các dòng cần được chỉnh sửa để giải quyết một vấn đề giữa một biển ngữ cảnh.

Đánh giá mạnh mẽ. Đối với mỗi instance nhiệm vụ, có ít nhất một fail-to-pass test được sử dụng để kiểm tra giải pháp tham chiếu, và 40% instance có ít nhất hai fail-to-pass test. Các test này đánh giá xem mô hình có giải quyết vấn đề trong issue hay không. Ngoài ra, trung vị của 51 test bổ sung chạy để kiểm tra xem chức năng trước đó có được duy trì đúng cách hay không.

Chỉnh sửa mã xuyên ngữ cảnh. Không giống như các thiết lập trước đây có thể hạn chế phạm vi chỉnh sửa cho một hàm hoặc lớp riêng lẻ (ví dụ: Chen et al., 2021; Cassano et al., 2022) hoặc cung cấp các điền vào chỗ trống kiểu cloze (ví dụ: Lu et al., 2021; Fried et al., 2023), SWE-bench không cung cấp hướng dẫn rõ ràng như vậy. Thay vì chỉ phải tạo ra một đoạn mã ngắn, benchmark của chúng tôi thử thách các mô hình tạo ra các sửa đổi ở nhiều vị trí của một codebase lớn. Các giải pháp tham chiếu của SWE-bench trung bình chỉnh sửa 1.7 tệp, 3.0 hàm và 32.8 dòng (được thêm hoặc xóa).

Phạm vi rộng cho các giải pháp có thể. Nhiệm vụ chỉnh sửa mã quy mô kho lưu trữ có thể phục vụ như một sân chơi bình đẳng để so sánh các phương pháp từ truy xuất và các mô hình ngữ cảnh dài đến các agent ra quyết định, có thể lý luận và hành động trong mã. SWE-bench cũng cho phép tự do sáng tạo, vì các mô hình có thể tạo ra các giải pháp mới có thể khác với PR tham chiếu.

2.4 SWE-BENCH LITE

Việc đánh giá LM trên SWE-bench có thể tốn thời gian và, tùy thuộc vào mô hình, yêu cầu một lượng tính toán hoặc tín dụng API tốn kém. Do kết quả hiệu suất ban đầu được trình bày trong Phần 5 khá thấp, độ khó của SWE-bench làm cho nó hữu ích để đánh giá tiến bộ LM trong dài hạn, nhưng có thể gây sợ hãi cho các hệ thống ban đầu cố gắng đạt tiến bộ trong ngắn hạn.

--- TRANG 4 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

astropy (95)django (850) flask (11)
matplotlib (184)
pylint (57)
pytest (119)
requests (44)
scikit-learn (229)
seaborn (22)
sphinx (187)sympy (386)xarray (110)

Hình 3: Phân phối các nhiệm vụ SWE-bench (trong ngoặc) qua 12 kho lưu trữ GitHub mã nguồn mở, mỗi kho chứa mã nguồn cho một gói PyPI phổ biến, được tải xuống rộng rãi.

Bảng 1: Số trung bình và tối đa đặc trưng cho các thuộc tính khác nhau của một instance nhiệm vụ SWE-bench. Thống kê là micro-average được tính toán mà không nhóm theo kho lưu trữ.

Mean Max
Độ dài văn bản vấn đề (Từ) 195.1 4477
Codebase# Tệp (không phải test) 3,010 5,890
# Dòng (không phải test) 438K 886K
Gold Patch# Dòng đã chỉnh sửa 32.8 5888
# Tệp đã chỉnh sửa 1.7 31
# Hàm đã chỉnh sửa 3 36
Tests# Fail to Pass 9.1 1633
# Tổng cộng 120.8 9459

Để khuyến khích việc áp dụng SWE-bench, chúng tôi tạo ra một tập con Lite gồm 300 instance từ SWE-bench đã được lấy mẫu để khép kín hơn, tập trung vào việc đánh giá các bản sửa lỗi chức năng. Tiêu chí lọc đầy đủ và thông tin dataset được bao gồm trong SWE-bench Lite bao phủ 11 trong số 12 kho lưu trữ ban đầu, với sự đa dạng và phân phối tương tự của các instance nhiệm vụ qua các kho lưu trữ như bản gốc. Chi tiết đầy đủ về việc phân chia Lite và chi tiết lọc được bao gồm trong Phụ lục A.7.

3 SWE-LLAMA: TINH CHỈNH CODELLAMA CHO SWE-BENCH

Điều quan trọng là đánh giá hiệu suất của các mô hình mở trên SWE-bench cùng với các mô hình độc quyền. Tại thời điểm viết, chỉ có các mô hình CodeLlama (Rozière et al., 2023) có thể xử lý các ngữ cảnh rất dài cần thiết. Tuy nhiên, chúng tôi quan sát thấy các biến thể CodeLlama sẵn có không có khả năng tuân theo các hướng dẫn chi tiết để tạo ra các chỉnh sửa mã toàn kho lưu trữ, và thường xuất ra các phản hồi placeholder hoặc mã không liên quan. Để đánh giá tốt hơn khả năng của các mô hình này, chúng tôi thực hiện tinh chỉnh có giám sát trên các mô hình CodeLlama-Python 7 tỷ và 13 tỷ tham số. Các mô hình kết quả là các trình chỉnh sửa kho lưu trữ chuyên biệt có thể chạy trên phần cứng tiêu dùng và giải quyết các vấn đề GitHub.

Dữ liệu huấn luyện. Chúng tôi tuân theo quy trình thu thập dữ liệu của mình và thu thập 19.000 cặp issue-PR từ thêm 37 kho lưu trữ gói Python phổ biến. Trái ngược với Phần 2.1, chúng tôi không yêu cầu các pull request đóng góp thay đổi test. Điều này cho phép chúng tôi tạo ra một tập huấn luyện lớn hơn nhiều để sử dụng cho tinh chỉnh có giám sát. Để loại bỏ rủi ro nhiễm dữ liệu, tập hợp các kho lưu trữ trong dữ liệu huấn luyện rời rạc với những kho được bao gồm trong benchmark đánh giá.

Chi tiết huấn luyện. Cho các hướng dẫn, một văn bản vấn đề từ GitHub và các tệp mã liên quan làm prompt, chúng tôi tinh chỉnh SWE-Llama để tạo ra patch giải quyết vấn đề đã cho ("gold patch"). Để tiết kiệm bộ nhớ, chúng tôi chỉ tinh chỉnh các trọng số của sublayer attention sử dụng LoRA Hu et al. (2022), và loại trừ các chuỗi huấn luyện có hơn 30.000 token, giảm kích thước hiệu quả của corpus huấn luyện xuống 10.000 instance. Thêm chi tiết được cung cấp trong Phụ lục B.

4 THIẾT LẬP THỰC NGHIỆM

Trong phần này chúng tôi giải thích cách các đầu vào được xây dựng để chạy đánh giá SWE-bench. Ngoài ra, chúng tôi xem xét các mô hình mà chúng tôi đánh giá trong công việc này.

4.1 PHƯƠNG PHÁP DỰA TRÊN TRUY XUẤT

Các instance SWE-bench cung cấp mô tả vấn đề và một codebase làm đầu vào cho mô hình. Trong khi các mô tả vấn đề thường ngắn (trung bình 195 từ như được hiển thị trong Bảng 1), các codebase bao gồm nhiều token hơn (trung bình 438K dòng) so với có thể thường được đưa vào cửa sổ ngữ cảnh của LM. Sau đó câu hỏi đặt ra là chính xác làm thế nào để chọn ngữ cảnh liên quan để cung cấp cho mô hình?

--- TRANG 5 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Để giải quyết vấn đề này cho các baseline của chúng tôi, chúng tôi đơn giản sử dụng một hệ thống truy xuất chung để lựa chọn các tệp để chèn làm ngữ cảnh. Cụ thể, chúng tôi đánh giá các mô hình dưới hai thiết lập ngữ cảnh liên quan: 1) truy xuất thưa thớt và 2) truy xuất oracle.

Truy xuất thưa thớt. Các phương pháp truy xuất dày đặc không phù hợp với thiết lập của chúng tôi do độ dài key và query rất dài, và đặc biệt là thiết lập bất thường của việc truy xuất tài liệu mã với các truy vấn ngôn ngữ tự nhiên. Do đó, chúng tôi chọn sử dụng truy xuất BM25 (Robertson et al., 2009) để truy xuất các tệp liên quan để cung cấp làm ngữ cảnh cho mỗi instance nhiệm vụ. Chúng tôi thực nghiệm với ba giới hạn ngữ cảnh tối đa khác nhau, và đơn giản truy xuất nhiều tệp nhất có thể vừa trong giới hạn được chỉ định. Chúng tôi đánh giá mỗi mô hình trên tất cả các giới hạn phù hợp với cửa sổ ngữ cảnh của nó và báo cáo hiệu suất tốt nhất. Từ quan sát, các mô hình hoạt động tốt nhất trên cửa sổ ngữ cảnh ngắn nhất, như được hiển thị trong Bảng 2.

Truy xuất "Oracle". Để phân tích, chúng tôi cũng xem xét một thiết lập nơi chúng tôi "truy xuất" các tệp được chỉnh sửa bởi patch tham chiếu đã giải quyết vấn đề trên GitHub. Thiết lập "oracle" này ít thực tế hơn, vì một kỹ sư làm việc để giải quyết một vấn đề có thể không biết trước những tệp nào cần được sửa đổi. Ngoài ra, thiết lập này cũng không nhất thiết toàn diện vì chỉ riêng các tệp đã chỉnh sửa có thể không bao gồm tất cả ngữ cảnh cần thiết để hiểu chính xác cách phần mềm sẽ hoạt động khi tương tác với các phần không nhìn thấy của mã.

Chúng tôi so sánh kết quả truy xuất BM25 với những kết quả của thiết lập truy xuất "oracle", như được hiển thị trong Bảng 3. Chúng tôi quan sát thấy trong khoảng 40% instance, BM25 truy xuất một superset của các tệp oracle cho giới hạn ngữ cảnh 27.000 token. Tuy nhiên, trong gần một nửa số instance với giới hạn 27.000 token, nó không truy xuất được tệp nào từ ngữ cảnh "oracle".

4.2 ĐỊNH DẠNG ĐẦU VÀO

Khi các tệp được truy xuất đã được lựa chọn bằng một trong hai phương pháp trên, chúng tôi xây dựng đầu vào cho mô hình bao gồm hướng dẫn nhiệm vụ, văn bản vấn đề, các tệp được truy xuất và tài liệu, và cuối cùng là một tệp patch mẫu và prompt để tạo ra tệp patch. Các ví dụ về instance và chi tiết thêm về công thức này được cung cấp trong Phụ lục D.

4.3 CÁC MÔ HÌNH

Do nhu cầu xử lý độ dài chuỗi dài, hiện tại chỉ có một số ít mô hình phù hợp với SWE-bench. Do đó, chúng tôi đánh giá ChatGPT-3.5 (gpt-3.5-turbo-16k-0613), GPT-4 (gpt-4-32k-0613), Claude 2, và SWE-Llama với các giới hạn ngữ cảnh được hiển thị trong Bảng 4.

Bảng 2: Tỷ lệ giải quyết mô hình với truy xuất BM25, với các độ dài ngữ cảnh tối đa khác nhau.

Max. Content
Model 13k27k50k
Claude 2 1.96 1.87 1.22
SWE-Llama 7b 0.70 0.31 0.00
SWE-Llama 13b 0.70 0.48 0.00

Bảng 3: BM25 recall đối với các tệp oracle cho các độ dài ngữ cảnh tối đa khác nhau.

BM25 Recall
13k 27k 50k
Avg. 29.58 44.41 51.06
All 26.09 39.83 45.90
Any 34.77 51.27 58.38

Bảng 4: Chúng tôi so sánh các độ dài ngữ cảnh khác nhau và tỷ lệ của thiết lập truy xuất "oracle" được bao phủ. Các mô hình có độ dài ngữ cảnh ngắn hơn do đó bị bất lợi về mặt bẩm sinh. Lưu ý rằng mô tả độ dài token là một thước đo tương đối không chuẩn (ví dụ: các chuỗi tokenized Llama dài hơn 42% trung bình so với chuỗi tương đương được tokenized cho GPT-4).

ChatGPT-3.5 GPT-4 Claude 2 SWE-Llama
Max. Tokens 16,385 32,768 100,000 ≥100,000
% of Instances 58.1% 84.1% 96.4% ≥94.8%

--- TRANG 6 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 5: Chúng tôi so sánh các mô hình với nhau sử dụng bộ truy xuất BM25 như được mô tả trong Phần 4.

SWE-bench SWE-bench Lite
Model % Resolved % Apply % Resolved % Apply
Claude 3 Opus 3.79 46.56 4.33 51.67
Claude 2 1.97 43.07 3.00 33.00
ChatGPT-3.5 0.17 26.33 0.33 10.00
GPT-4-turbo 1.31 26.90 2.67 29.67
SWE-Llama 7b 0.70 51.74 1.33 38.00
SWE-Llama 13b 0.70 53.62 1.00 38.00

astropy djangomatplotlibseabornflaskrequestsxarray pylint pytest
scikit-learnsphinx sympy051015% ResolvedChatGPT-3.5
Claude 2
SWE-Llama 13b

Hình 4: Tỷ lệ giải quyết cho ba mô hình qua 12 kho lưu trữ được đại diện trong SWE-bench trong thiết lập truy xuất "Oracle".

5 KẾT QUẢ

Chúng tôi báo cáo kết quả cho các mô hình sử dụng các cơ chế truy xuất và phong cách prompting khác nhau, sau đó cung cấp một số phân tích và hiểu biết về hiệu suất mô hình và độ khó. Chúng tôi tóm tắt hiệu suất của các mô hình sử dụng truy xuất BM25 trong Bảng 5. Nhìn chung, các mô hình gặp khó khăn đáng kể trong việc giải quyết các vấn đề. Mô hình hoạt động tốt nhất, Claude 2, chỉ có thể giải quyết 1.96% các vấn đề.

Để phân tích tầm quan trọng của bộ truy xuất đối với kết quả hệ thống tổng thể, chúng tôi trình bày kết quả truy xuất "oracle" trong Bảng 18 của Phụ lục. Ở đó, Claude 2 có thể giải quyết 4.8% các vấn đề sử dụng bộ truy xuất "oracle". Chúng tôi phân tích thêm tầm quan trọng của ngữ cảnh trong thảo luận dưới đây.

Độ khó khác nhau qua các kho lưu trữ. Khi phân tích hiệu suất theo kho lưu trữ, tất cả các mô hình có xu hướng tương tự qua các kho lưu trữ khác nhau như được hiển thị trong Hình 4. Bất chấp điều này, các vấn đề được giải quyết bởi mỗi mô hình không nhất thiết trùng lặp rộng rãi. Ví dụ, trong thiết lập "oracle" Claude 2 và SWE-Llama 13b hoạt động tương đương, với mỗi mô hình giải quyết 110 và 91 instance tương ứng. Tuy nhiên, trong số các instance này, Claude 2 chỉ giải quyết 42% các instance được giải quyết bởi SWE-Llama.

Điều này cũng có thể liên quan đến sự hiện diện của hình ảnh trong các vấn đề, có thể được mã hóa vào markdown vấn đề với các liên kết hình ảnh nhúng (tức là ![image][https://...]). Một số kho lưu trữ tự nhiên có nhiều instance với hình ảnh hơn; ví dụ 32% instance matplotlib và 10% instance seaborn chứa hình ảnh nhúng trong văn bản vấn đề của chúng so với chỉ 2% tất cả instance. Việc giải quyết các instance này có thể yêu cầu các LM đa phương thức hoặc một số loại sử dụng công cụ bên ngoài để xử lý hình ảnh.

Độ khó tương quan với độ dài ngữ cảnh. Các mô hình chat có thể được pre-train trên các chuỗi mã dài nhưng thường được yêu cầu tạo ra các đoạn mã ngắn hơn với ngữ cảnh hạn chế được cung cấp để đóng khung câu hỏi. Như được hiển thị trong Hình 5, chúng ta thấy rằng khi tổng độ dài ngữ cảnh tăng lên, hiệu suất của Claude 2 giảm đáng kể; hành vi này cũng được quan sát thấy trong các mô hình khác. Trong các thiết lập đánh giá của chúng tôi, các mô hình thấy rất nhiều mã có thể không liên quan trực tiếp đến việc giải quyết vấn đề hiện tại, và chúng dường như thường gặp khó khăn với việc định vị mã có vấn đề cần được cập nhật. Kết quả này củng cố các nghiên cứu khác cho thấy các mô hình bị phân tâm bởi ngữ cảnh bổ sung và có thể nhạy cảm với vị trí tương đối của các chuỗi mục tiêu (Liu et al., 2023b). Ngay cả khi tăng kích thước ngữ cảnh tối đa cho BM25 sẽ tăng recall đối với các tệp oracle, hiệu suất giảm, như được hiển thị trong Bảng 2, vì các mô hình đơn giản không hiệu quả trong việc định vị mã có vấn đề.

Điều tra thêm về điều này, chúng tôi cung cấp một ablation đầu vào trên ngữ cảnh truy xuất "oracle", "oracle"-collapsed, trong đó các tệp được truy xuất bị thu gọn hoàn toàn, ngoại trừ các dòng thực sự được chỉnh sửa bởi

--- TRANG 7 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

<20k20k-50k 50k-100k>100k
# of Input Tokens01020304050% of Tasks
<500500-1k 1k-2k>2k
# of Issue TokensStatus
Resolved
Applied

Hình 5: Chúng tôi so sánh hiệu suất của Claude 2 trên các nhiệm vụ được phân chia theo tổng độ dài đầu vào và chỉ theo độ dài vấn đề.

Bảng 6: Chúng tôi hiển thị kết quả cho thiết lập truy xuất "Oracle"-collapsed, sử dụng các tệp oracle nhưng thu gọn mã không được sửa đổi trực tiếp bởi PR ±15 dòng.

Model"Oracle"-collapsed
Resolved Applied
Claude 3 Opus 9.39 48.00
Claude 2 5.93 68.18
GPT-4 3.40 48.65
ChatGPT-3.5 1.09 40.93

pull request thực (với ±15 dòng buffer) được hiển thị trong Bảng 6. Trong thiết lập này, chúng ta thấy sự gia tăng hiệu suất, với GPT-4 tăng từ 1.3% lên 3.4% và Claude 2 từ 4.8% lên 5.9%.

Độ khó không tương quan với ngày giải quyết vấn đề. Trong Bảng 7 chúng tôi hiển thị kết quả mô hình trong thiết lập truy xuất "oracle", được phân chia theo ngày, cho các PR được tạo trước hoặc sau 2023. Chúng tôi thấy rằng đối với hầu hết các mô hình có ít khác biệt về hiệu suất trước và sau ngày này, ngoại trừ GPT-4. Chúng tôi coi kết quả này là phần lớn hứa hẹn vì nó cho thấy rằng mặc dù các mô hình đã được tiếp xúc với một số phiên bản của codebase kho lưu trữ, chúng không có khả năng "gian lận" để giải quyết các vấn đề chỉ bằng cách tạo ra một phiên bản gần đây hơn của kho lưu trữ.

Bảng 7: Chúng tôi so sánh hiệu suất trên các instance nhiệm vụ từ trước và sau 2023 trong thiết lập truy xuất "Oracle". Hầu hết các mô hình hiển thị ít khác biệt về hiệu suất.∗Do hạn chế ngân sách, GPT-4 được đánh giá trên 25% tập con ngẫu nhiên của các nhiệm vụ SWE-bench, có thể ảnh hưởng đến hiệu suất.

Claude 2 ChatGPT-3.5 GPT-4∗SWE-Llama 7b SWE-Llama 13b
Trước 2023 4.87 0.49 1.96 2.95 3.98
Sau 2023 4.23 0.77 0.0 3.46 3.85

Các mô hình tinh chỉnh nhạy cảm với sự thay đổi phân phối ngữ cảnh. Các mô hình tinh chỉnh SWE-Llama 7b và 13b hoạt động kém đáng ngạc nhiên với ngữ cảnh được truy xuất bởi BM25. Vì các mô hình này được tinh chỉnh sử dụng truy xuất "oracle" làm ngữ cảnh, chúng tôi nghi ngờ sự thay đổi ngữ cảnh này làm cho mô hình khó hoạt động đáng tin cậy. Ví dụ, SWE-Llama được huấn luyện để chỉnh sửa mọi tệp được bao gồm làm ngữ cảnh trong khi trong thiết lập BM25 nhiều tệp được cung cấp trong ngữ cảnh không được dự kiến sẽ thay đổi.

Tạo patch dễ hơn việc tạo toàn bộ tệp. Các mô hình thường được huấn luyện sử dụng các tệp mã tiêu chuẩn và có khả năng hiếm khi thấy các tệp patch. Chúng tôi thường công thức nhiệm vụ của mình để các mô hình tạo ra các tệp patch thay vì tái tạo toàn bộ tệp với thay đổi được đề xuất của chúng, vì các tệp patch thường sẽ là một biểu diễn hiệu quả hơn nhiều của một thay đổi tệp. Như được hiển thị trong Bảng 5, chúng tôi quan sát thấy các mô hình vẫn gặp khó khăn với việc tạo ra các tệp patch được định dạng tốt. Vì vậy chúng tôi thử nghiệm với việc yêu cầu các mô hình thay vào đó tái tạo toàn bộ tệp với các thay đổi được đề xuất của chúng để giải quyết vấn đề. Trong thiết lập này, chúng tôi thấy rằng các mô hình thường hoạt động tệ hơn trong nhiệm vụ này so với khi tạo ra các tệp patch; ví dụ, Claude 2 đạt 2.2% so với 4.8% trong bảng chính cho truy xuất "oracle". Ngay cả khi kiểm soát độ dài instance, tạo ra trên nửa ngắn hơn của các instance nhiệm vụ theo token đầu vào cho 3.9% so với 7.8% cho việc tạo patch với Claude 2.

Các mô hình ngôn ngữ có xu hướng tạo ra các chỉnh sửa ngắn hơn, đơn giản hơn. Các tệp patch được tạo ra bởi mô hình có xu hướng thêm và xóa ít dòng hơn so với patch vàng tương ứng của chúng. Như được hiển thị trong Bảng 8, so với một patch vàng trung bình, các tệp patch được tạo ra bởi mô hình mà áp dụng đúng cách có ít hơn một nửa tổng độ dài (74.5 so với 30.1 dòng) của các tệp patch chỉnh sửa vàng, và hiếm khi chỉnh sửa hơn một tệp duy nhất.

5.1 PHÂN TÍCH ĐỊNH TÍNH CÁC GENERATION CỦA SWE-LLAMA

Chúng tôi chọn 11 generation từ SWE-Llama và Claude 2 để hiểu rõ hơn về chất lượng của nhiệm vụ và các patch được tạo ra dưới thiết lập truy xuất "oracle". Ở đây chúng tôi thảo luận một ví dụ từ SWE-Llama và các phát hiện tổng thể của chúng tôi, với các phân tích sâu cho các ví dụ khác được hiển thị trong Phụ lục F.

--- TRANG 8 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 8: Trung bình các chỉnh sửa của các patch được tạo ra bởi mô hình trong thiết lập truy xuất "oracle" qua các patch được áp dụng thành công. Đối với các instance nhiệm vụ cụ thể cho mỗi mô hình, chúng tôi tính toán cùng các thống kê qua các patch vàng. Avg Gold hiển thị thống kê macro-averaged qua các patch vàng tương ứng của mỗi mô hình. All Gold hiển thị thống kê cho tất cả các patch vàng không điều kiện trên hiệu suất mô hình.

Model Total Lines Added Removed Functions Files
Claude 2 19.6 4.2 1.9 1.1 1.0
Gold 44.1 12.0 5.8 2.1 1.2
ChatGPT-3.5 30.1 3.8 2.7 1.6 1.0
Gold 39.6 9.5 6.1 1.9 1.2
GPT-4 20.9 4.4 1.5 1.0 1.0
Gold 33.6 8.4 3.8 1.9 1.1
SWE-Llama 13b 17.6 1.6 1.2 1.2 1.1
Gold 37.8 10.0 4.4 1.9 1.1
SWE-Llama 7b 16.7 1.3 1.2 1.2 1.1
Gold 40.2 11.3 4.9 1.9 1.1
Avg Gold 39.1 10.2 5.0 1.9 1.1
All Gold 74.5 22.3 10.5 3.0 1.7

Gold P a t ch      (self, section: str) -> List[str]: 
-         self. (_( ), self._consume_fields()) 
+         self._config.napoleon_use_param: 
+            
+            fields = self. (multiple=True) 
+             self. (fields) 
+        : 
+            fields = self. () 
+            return self. (_( ), fields)def
return
if
return
else_parse_other_parameters_section
_format_fields
_consume_fields
_format_docutils_params
_consume_fields
_format_fields'Other Parameters'
'Other Parameters'# Allow to declare multiple parameters at once (ex: x, y: int) sphinx/ e xt/ napoleon/ docs tring.p y

Genera t ed P a t ch      (self, section: str) -> List[str]: 
         self. (_( ), self._consume_fields()) 
        return self._format_docutils_params(self._consume_fields())def
return_parse_other_parameters_section
_format_fields-'Other Parameters'+sphinx/ e xt/ napoleon/ docs tring.p yGenera t ed P a t ch T es t R esultsPASSED
PASSED
PASSED
PASSED
PASSED
FAILED
FAILEDNumpyDocstringTest (test_yield_types) 
TestNumpyDocstring (test_escape_args_and_kwargs 1) 
TestNumpyDocstring (test_escape_args_and_kwargs 2)
TestNumpyDocstring (test_escape_args_and_kwargs 3) 
TestNumpyDocstring (test_pep526_annotations) 
NumpyDocstringTest (test_parameters_with_class_reference) 
TestNumpyDocstring (test_token_type_invalid)===== 2 failed, 45 passed, 8 warnings in 5.16s =====M odel I n p utY o u w ill  b e  pr o v ided  w ith  a  partial  code  b ase  and  an  iss ue
s ta t e m ent  e xplaining  a  pr o b le m t o  r esol v e.napoleon _u se _ para m sho u ld  also  a ff ec t  " o ther  
para m e t ers " sec tion  Subj ec t : napoleon _u se _ para m 
sho u ld  also  a ff ec t  " o ther  para m e t ers " sec tiondef self
return
def self
if ( , se.. .
     self. (_(
 ( , section) :
    
    fields = self. ( )
     self._config.napoleon_use_param: ..._parse_other_parameters_section
_format_fields
_parse_parameters_section
_consume_fields    # type: (unicode) -> List[unicode ]
# type: (unicode) -> List[unicode ]
'Other Para.. .

### P r o b le m
Cu rr ently , napoleon  al w a y s  r enders  the  O ther  para m e t ers  
sec tion  as  i f napoleon _u se _ para m w as  F alse , see  so u r ce

Hình 6: Chúng tôi hiển thị một ví dụ về một instance nhiệm vụ được định dạng, một dự đoán mô hình và các log framework kiểm thử. Trong các patch, màu đỏ nổi bật là xóa. Màu xanh lá nổi bật là thêm.

Chúng tôi sẽ xem xét instance nhiệm vụ sphinx-doc sphinx-8713 từ trình tạo tài liệu Sphinx, được hiển thị trong Hình 6. Vấn đề nêu rằng phần mở rộng napoleon của Sphinx không định dạng đúng cách từ khóa tài liệu "Other Parameters" khi cài đặt config napoleon.use_param được đặt thành True. Văn bản vấn đề cung cấp thêm một đoạn mã chi tiết về nơi nghi ngờ có mã nguồn có vấn đề, cũng như một số ví dụ mã để tái tạo lỗi và thông tin bổ sung liên quan đến các phiên bản gói. Đối với instance cụ thể này, mô hình đã không giải quyết được nhiệm vụ, không vượt qua một số test được giải quyết bởi giải pháp vàng.

Trong thiết lập truy xuất "oracle", đầu vào mô hình cung cấp văn bản vấn đề này cùng với một số hướng dẫn, nội dung đầy đủ của các tệp được chỉnh sửa bởi patch vàng, và một ví dụ về định dạng diff mà chúng tôi mong đợi câu trả lời ở trong đó. Tổng đầu vào mô hình bao gồm 1.558 dòng ngữ cảnh hoặc 20.882 token. Khi so sánh patch vàng và patch của mô hình, chúng tôi thấy một lỗi rõ ràng. Trong khi mô hình chỉnh sửa hàm đúng, _parse_other_parameters_section tại dòng 684 trong sphinx/ext/napoleon/docstring.py, nó thay đổi hàm để hoạt động như thể napoleon. use_param luôn True thay vì kiểm tra cài đặt config trước và sao chép những gì _parse_parameters_section làm, như patch vàng. Trong các test, test_parameters

--- TRANG 9 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

_with_class_reference trực tiếp so sánh tài liệu được tạo ra sử dụng một config nơi napoleon_use_param được đặt thành False, điều này bắt lỗi của mô hình ngay lập tức.

So sánh kết quả qua tất cả các ví dụ mà chúng tôi xem xét, chúng tôi nhận thấy một số xu hướng nổi bật trong hành vi. Các mô hình có xu hướng viết mã Python nguyên thủy và không tận dụng các thư viện bên thứ ba hiện có hoặc phần còn lại của codebase cho các giải pháp của chúng. Các generation của mô hình cũng phản ánh một phương pháp "tham lam" của việc giải quyết vấn đề một cách chính xác, với ít quan tâm đến phong cách mã hoặc các ràng buộc logic có thể được phản ánh bởi codebase (tức là sử dụng import tương đối thay vì tuyệt đối). Ngược lại, chúng tôi quan sát thấy nhiều patch vàng sẽ thực hiện các cải tiến cấu trúc bao phủ một phạm vi lớn hơn nhiều của codebase; các chỉnh sửa này không chỉ giải quyết vấn đề, mà còn dự đoán và giải quyết các vấn đề tương lai tiềm năng.

6 CÔNG TRÌNH LIÊN QUAN

Đánh giá LM. Một số công trình gần đây để đánh giá LM đã đề xuất một tập hợp các nhiệm vụ khác biệt lẫn nhau trải rộng qua nhiều lĩnh vực (Hendrycks et al., 2021; Liang et al., 2022; Srivastava et al., 2023) hoặc chuyển sang web như một thiết lập tương tác có các nhiệm vụ yêu cầu nhiều bước để giải quyết (Yao et al., 2022; Zhou et al., 2023; Deng et al., 2023; Liu et al., 2023d). Có một số nhược điểm với thiết lập kiểu "potpourri" như vậy. Đầu tiên, mỗi nhiệm vụ có xu hướng tập trung hẹp vào một hoặc một vài kỹ năng, dẫn đến những thử thách thường quá đơn giản, nhốt mô hình vào một vai trò thu hẹp, và không cung cấp cho các mô hình băng thông để thể hiện tính linh hoạt của chúng hoặc có khả năng thể hiện các khả năng mới (Srivastava et al., 2023). Do đó, hiệu suất của một mô hình trên các tập hợp nhiệm vụ như vậy có thể không mang lại những hiểu biết có thể hành động, sâu sắc về khả năng của nó và cách cải thiện chúng (Schlangen, 2019; Martínez-Plumed et al., 2021; Bowman & Dahl, 2021). SWE-bench giải quyết những thiếu sót này, vì công việc của chúng tôi chứng minh rằng nó đầy thử thách đáng kể, trình bày một loạt rộng các khả năng để cải thiện LM để giải quyết nhiệm vụ này, và dễ dàng làm mới theo thời gian với các instance nhiệm vụ mới, mỗi cái giới thiệu những thử thách mới, tinh tế và thực tế.

Các benchmark tạo mã. HumanEval (Chen et al., 2021) là tiêu chuẩn hiện tại trong một cuộc theo đuổi lâu dài về việc tổng hợp mã từ các mô tả ngôn ngữ tự nhiên (Yu et al., 2018; Austin et al., 2021; Hendrycks et al., 2021; Li et al., 2022a; Zan et al., 2023). Trong năm qua, các benchmark tiếp theo đã tìm cách bổ sung HumanEval với các phần mở rộng cho các ngôn ngữ khác nhau (Cassano et al., 2022; Athiwaratkun et al., 2023; Orlanski et al., 2023), các biến thể trong phạm vi chỉnh sửa (Yu et al., 2023; Du et al., 2023), các nhiệm vụ hoàn thành mã tương tự nhưng mới (Muennighoff et al., 2023), và nhiều kiểm thử hơn (Liu et al., 2023a). Đồng thời, các công việc riêng biệt đã tìm cách giới thiệu các mô hình lập trình mới (Yin et al., 2022; Yang et al., 2023) hoặc thiết kế các vấn đề cụ thể cho thư viện (Lai et al., 2022; Zan et al., 2022). Thay vì phân chia các vấn đề thành các dataset riêng biệt và cắt giảm chúng vì sự đơn giản, quy trình thu thập của SWE-bench biến đổi mã nguồn với xử lý tối thiểu sau đó, bảo tồn một tập hợp rộng hơn nhiều các thử thách dựa trên kỹ thuật phần mềm thế giới thực ngoài việc hoàn thành dạng đóng, như tạo patch, lý luận qua các ngữ cảnh dài, điều hướng một thư mục codebase, và nắm bắt các mối quan hệ dựa trên dependency qua các module.

ML cho Kỹ thuật Phần mềm. Để vượt qua các kỹ thuật phân tích chương trình truyền thống có thể không mở rộng hoặc kết hợp ngôn ngữ tự nhiên, một hướng của nghiên cứu kỹ thuật phần mềm hiện tại là sử dụng mạng thần kinh, bao gồm LM, để tự động hóa các quy trình phát triển phần mềm thế giới thực (Maniatis et al., 2023; Zheng et al., 2023; Hou et al., 2023). Các trường hợp sử dụng bao gồm tự động hóa tạo commit (Jung, 2021; Liu et al., 2023c), review PR (Yang et al., 2016; Li et al., 2022b; Tufano et al., 2021), định vị lỗi (Kim et al., 2019; Chakraborty et al., 2018), kiểm thử (Kang et al., 2023; Xia et al., 2023; Wang et al., 2023), và sửa chương trình (Gupta et al., 2017; Allamanis et al., 2017; Monperrus, 2018; Jiang et al., 2018; Goues et al., 2019; Gao et al., 2022; Dinh et al., 2023; Motwani & Brun, 2023). Liên quan nhất đến SWE-bench là các công việc đã tìm cách áp dụng LM hướng tới sửa chương trình tự động (Xia & Zhang, 2022; 2023; Fan et al., 2023; Sobania et al., 2023), hướng dẫn chỉnh sửa mã với commit (Chakraborty & Ray, 2021; Zhang et al., 2022; Fakhoury et al., 2023). Tuy nhiên, không có dataset hiện có nào (Just et al., 2014; Karampatsis & Sutton, 2019) trình bày ngữ cảnh mã ở quy mô của SWE-bench. Hơn nữa, SWE-bench có thể dễ dàng mở rộng cho các ngôn ngữ lập trình và kho lưu trữ mới, và nó cung cấp một sân chơi thực tế và đầy thử thách hơn đáng kể để thực hiện các thí nghiệm hướng tới việc bổ sung LM với các công cụ và thực hành kỹ thuật phần mềm.

--- TRANG 10 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

7 THẢO LUẬN

Giới hạn và hướng tương lai. Các instance nhiệm vụ SWE-bench đều bằng Python; chúng tôi hy vọng áp dụng quy trình thu thập instance nhiệm vụ của SWE-bench để mở rộng coverage của nó sang nhiều ngôn ngữ lập trình và lĩnh vực hơn. Thứ hai, các thí nghiệm của chúng tôi nhằm thiết lập một baseline của các phương pháp đơn giản và trực tiếp nhất cho nhiệm vụ này; chúng tôi không có ý định hạn chế các phương pháp tương lai vào cùng loại phương pháp và khuyến khích công việc tương lai điều tra các phương pháp khác nhau (ví dụ: các phương pháp dựa trên agent, LM được bổ sung công cụ).

Cuối cùng, trong khi công việc này đánh giá các mô hình sử dụng kiểm thử mã dựa trên thực thi, việc chỉ dựa vào phương pháp này là không đủ để đảm bảo hiệu suất đáng tin cậy của các generation mô hình, vì chúng tôi thấy các generation mã tự động từ LM có thể thường ít toàn diện, hiệu quả hoặc dễ đọc hơn so với các giải pháp viết bởi con người.

Kết luận. Sự phức tạp của các quy trình phát triển phần mềm thế giới thực mở rộng xa hơn việc chỉ hoàn thành mã. Bằng cách dựa vào pipeline hợp tác mã nguồn mở, SWE-bench tạo ra một gương phản ánh trung thực của môi trường lập trình thế giới thực. Môi trường thực tế hơn này khuyến khích các giải pháp sáng tạo có thể có khả năng ứng dụng ngay lập tức trong phát triển phần mềm mã nguồn mở. Chúng tôi hy vọng rằng benchmark này và các đóng góp khác của chúng tôi có thể phục vụ như những tài sản quý giá trong phát triển tương lai của các LM thực tế hơn, thông minh hơn và tự động hơn.

--- TRANG 11 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

8 TUYÊN BỐ ĐẠỌC ĐỨC

SWE-bench được thu thập hoàn toàn từ các kho lưu trữ công cộng với các giấy phép cho phép sử dụng phần mềm mà các đóng góp của chúng tôi phù hợp với. Chi tiết của các giấy phép được bao gồm trong Bảng 12.

Trong quá trình thu thập hoặc đánh giá, chúng tôi không thu thập thông tin về người dùng GitHub, và các instance nhiệm vụ SWE-bench không sử dụng dữ liệu GitHub ngoài những gì được cung cấp qua API công khai và website. Các đóng góp của chúng tôi không liên quan đến bất kỳ sự tham gia của chủ thể con người nào; chúng tôi không thực hiện crowdsourcing hoặc tuyển dụng công nhân nhiệm vụ con người cho bất kỳ phần nào của SWE-bench, bao gồm các quy trình thu thập và đánh giá cùng với các thí nghiệm. Tiêu chí lọc của SWE-bench cho các kho lưu trữ GitHub dựa trên độ phổ biến không ngầm hoặc rõ ràng dựa vào bất kỳ heuristic phân biệt hoặc thiên vị nào cho việc lựa chọn kho lưu trữ. Để phát hành dataset, chúng tôi dự định mở nguồn các instance nhiệm vụ SWE-bench, cơ sở hạ tầng thu thập và đánh giá, kết quả thí nghiệm, dữ liệu huấn luyện được sử dụng để tinh chỉnh các mô hình SWE-Llama, và trọng số mô hình SWE-Llama. Theo các tiền lệ thực hành tốt nhất, chúng tôi cũng sẽ đưa ra tài liệu phong phú để mô tả từng thành phần và việc sử dụng của nó, và chúng tôi cũng sẽ thiết lập các kênh liên lạc thuận tiện để thu thập phản hồi để cải thiện SWE-bench. SWE-bench không đưa ra bất kỳ hiểu biết có hại ngay lập tức nào. Chúng tôi thảo luận ngắn gọn về tác động tiềm năng của việc sử dụng SWE-bench trong Phần E.

9 TUYÊN BỐ KHẢ NĂNG TÁI TẠO

Cho bài nộp của chúng tôi, chúng tôi đã tải lên toàn bộ mã nguồn như một tệp nén đã được ẩn danh đúng cách. Chúng tôi đã tổ chức codebase sao cho các thư mục riêng biệt tương ứng với các đóng góp khác nhau trong bài báo chính (tức là thu thập dataset, đánh giá, suy luận mô hình mã nguồn mở, huấn luyện SWE-Llama, v.v.). Mã nguồn chứa tài liệu inline chi tiết mục đích và cách sử dụng của các phần khác nhau của codebase. Ngoài ra, chúng tôi cũng bao gồm tập hợp đầy đủ 2294 instance nhiệm vụ SWE-bench chứa tất cả các thành phần được thảo luận trong bài báo chính. Ngoài tài liệu trong mã nguồn, chúng tôi bao gồm các chi tiết kỹ thuật đầy đủ cho pipeline thu thập và quy trình đánh giá trong Phần A.2 và Phần A.4 bổ sung cho các chi tiết ban đầu trong Phần 2 của bài báo chính. Các phần này bao phủ đầy đủ logic được trình bày trong mã và có thể hữu ích để hiểu nó. Tiến về phía trước, như được thảo luận trong tuyên bố đạo đức, chúng tôi dự định chính thức phát hành SWE-bench cho công chúng như một kho lưu trữ mã nguồn mở với các chi tiết đầy đủ mô tả benchmark, phác thảo mã và chi tiết việc sử dụng của nó. Một thành phần chính của SWE-bench là framework thu thập, sẽ là một phần của mã mã nguồn mở. Vì thiết kế dễ duy trì của nó, như được thảo luận trong bài báo chính, hy vọng và niềm tin của chúng tôi là SWE-bench nên có tính tái tạo cao.

10 LỜI CẢM ƠN

Chúng tôi cảm ơn Danqi Chen, Tri Dao, Zexuan Zhong, Tianyu Gao, Will Merrill, Mengzhou Xia, Dan Friedman, Adithya Bhaskar, Austin Watkins, Aatmik Gupta, và Richard Zhu vì phản hồi và lời khuyên quý giá của họ. Chúng tôi ghi nhận sự hỗ trợ từ Quỹ Khoa học Quốc gia dưới Grant No. 2239363 và một giải thưởng nghiên cứu hợp tác Oracle. Bất kỳ ý kiến, phát hiện, kết luận hoặc khuyến nghị nào được thể hiện trong tài liệu này là của (các) tác giả và không nhất thiết phản ánh quan điểm của Quỹ Khoa học Quốc gia.

--- TRANG 12 ---
[Phần này chứa các tài liệu tham khảo và phụ lục mà tôi sẽ không dịch do độ dài và tính chất kỹ thuật chi tiết]
