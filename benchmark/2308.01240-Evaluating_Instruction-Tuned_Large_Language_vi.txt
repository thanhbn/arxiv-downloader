# 2308.01240.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/benchmark/2308.01240.pdf
# Kích thước tệp: 1259509 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Đánh giá các Mô hình Ngôn ngữ Lớn được Điều chỉnh Hướng dẫn
trên Hiểu và Sinh mã
Zhiqiang Yuan, Junwei Liu, Qiancheng Zi, Mingwei Liu, Xin Peng, Yiling Lou
Khoa Khoa học Máy tính, Đại học Fudan, Trung Quốc
{zhiqiangyuan23, jwliu22, qczi22 }@m.fudan.edu.cn
{liumingwei, pengxin, yilinglou }@fudan.edu.cn
Tóm tắt —Điều chỉnh hướng dẫn đã được đề xuất để tăng cường
khả năng tổng quát hóa của các mô hình ngôn ngữ lớn (LLM) trên
các tác vụ mới. Cho đến nay, nhiều nỗ lực đã được dành cho việc
đánh giá các LLM được hướng dẫn, bao gồm không chỉ các tác vụ
NLP tổng quát mà còn các lĩnh vực cụ thể. Tuy nhiên, ít có đánh giá
nào về các LLM được hướng dẫn đi sâu vào lĩnh vực kỹ thuật phần
mềm, ngoại trừ tác vụ NL-to-Code (tạo một hàm cho mô tả ngôn
ngữ tự nhiên đã cho), chỉ là một trong những tác vụ liên quan đến
mã trong phát triển và bảo trì phần mềm. Mặc dù một số công trình
gần đây khám phá khả năng của các mô hình được hướng dẫn như
ChatGPT trên các tác vụ SE, những mô hình thương mại này là
closed-source, do đó thiếu tính minh bạch và khả năng tái tạo.
Nhìn chung, vẫn chưa rõ các LLM được hướng dẫn mã nguồn mở
gần đây hoạt động như thế nào trên các tác vụ hiểu và sinh mã đa
dạng.

Để lấp đầy khoảng trống kiến thức này, chúng tôi đánh giá 10
LLM được hướng dẫn mã nguồn mở trên bốn tác vụ hiểu và sinh
mã đại diện (tức là phát hiện lỗi, phát hiện bản sao, sinh assertion,
và tóm tắt mã). Chúng tôi có những phát hiện chính sau đây. Thứ
nhất, trong thiết lập zero-shot, các LLM được hướng dẫn rất cạnh
tranh trên các tác vụ hiểu và sinh mã và đôi khi thậm chí còn tốt
hơn các mô hình SOTA nhỏ được tinh chỉnh cụ thể trên từng tác vụ.
Chúng tôi cũng thấy rằng trên các tác vụ liên quan đến mã, các
LLM được hướng dẫn bởi lĩnh vực mã không nhất thiết vượt trội
hơn các LLM được hướng dẫn bởi lĩnh vực tổng quát, và các LLM
được hướng dẫn lớn hơn không phải lúc nào cũng tốt hơn. Thứ hai,
trong thiết lập few-shot, chúng tôi thấy rằng việc thêm các ví dụ
minh họa giúp các LLM được hướng dẫn hoạt động tốt hơn đáng kể
trên hầu hết các tác vụ hiểu và sinh mã; tuy nhiên, các ví dụ đôi khi
có thể gây ra hiệu suất không ổn định hoặc thậm chí tệ hơn. Ngoài
ra, chúng tôi quan sát thấy hiệu suất giảm khi độ dài đầu vào tăng
và khả năng tuân theo hướng dẫn tăng trong thiết lập few-shot.
Hơn nữa, chúng tôi thấy chiến lược lựa chọn shot dựa trên BM25
được sử dụng rộng rãi vượt trội đáng kể so với lựa chọn ngẫu nhiên
cơ bản hoặc lựa chọn cố định chỉ trên các vấn đề sinh (ví dụ: sinh
assertion và tóm tắt mã), trong khi không có sự khác biệt đáng kể
so với các chiến lược cơ bản trên các vấn đề phân loại (ví dụ: phát
hiện lỗi hoặc phát hiện bản sao). Thứ ba, trong thiết lập fine-tuning,
chúng tôi thấy rằng fine-tuning có thể cải thiện thêm hiệu suất mô
hình trên các tác vụ hiểu và sinh mã so với hiệu suất zero-shot/one-
shot. Ngoài ra, sau khi được fine-tune trên cùng một tập dữ liệu tác
vụ, các LLM được hướng dẫn vượt trội hơn cả các mô hình SOTA
nhỏ và các LLM tương tự về quy mô mà không có điều chỉnh
hướng dẫn. Dựa trên các phát hiện của chúng tôi, chúng tôi tiếp tục
trình bày các ý nghĩa thực tiễn về khuyến nghị mô hình và sử dụng,
đánh đổi hiệu suất và chi phí, và hướng tương lai.

I. GIỚI THIỆU
Các Mô hình Ngôn ngữ Lớn (LLM) đã đạt được tiến bộ lớn
và đã được áp dụng trong nhiều lĩnh vực khác nhau [1], [2],
[3], [4]. Gần đây hơn, điều chỉnh hướng dẫn đã được đề xuất để tăng cường khả năng tổng quát hóa của LLM trên các
tác vụ mới [5], [6], [7], [8], [9]. Cụ thể, điều chỉnh hướng dẫn
fine-tune các LLM được pre-train (còn được gọi là mô hình
nền tảng) với các hướng dẫn lớn từ nhiều tác vụ, và các LLM
sau điều chỉnh hướng dẫn (được ký hiệu là LLM được điều
chỉnh hướng dẫn hoặc LLM được hướng dẫn tóm tắt) có thể
giải quyết các tác vụ chưa thấy khác nhau trong kịch bản zero-
shot mà không cần fine-tuning thêm hoặc ví dụ minh họa [10],
[9], [8]. Ví dụ, ChatGPT [11], một LLM được hướng dẫn phổ
biến, là closed-source và được hỗ trợ bởi OpenAI. Ngoài ra,
đã có một xu hướng cuồng nhiệt trong việc phát triển các LLM
được hướng dẫn trong cộng đồng mã nguồn mở, ví dụ: các
nhà nghiên cứu đã đề xuất nhiều LLM được hướng dẫn mã
nguồn mở (ví dụ: Vicuna [5], Alpaca [6], và CodeAlpaca [7])
bằng cách fine-tune mô hình nền tảng mã nguồn mở LLaMA
[2] với các tập dữ liệu hướng dẫn từ các lĩnh vực tổng quát
hoặc cụ thể về mã.

Giữa sự phát triển nhanh chóng và sự phổ biến ngày càng
tăng của các LLM được điều chỉnh hướng dẫn, việc nắm bắt
khả năng của những mô hình mới này trở nên cấp thiết. Cho
đến nay, nhiều nỗ lực nghiên cứu đã được đưa vào việc đánh
giá các LLM được hướng dẫn [12], [13], [14], [15], [16], bao
gồm không chỉ các tác vụ NLP tổng quát [17], [18] (ví dụ:
phân tích tình cảm, phân loại văn bản, và hiểu ngữ nghĩa) mà
còn các lĩnh vực cụ thể [19] (ví dụ: y tế, giáo dục, và ứng dụng
agent). Tuy nhiên, ít có đánh giá nào về các LLM được hướng
dẫn đi sâu vào lĩnh vực kỹ thuật phần mềm. Hầu hết đánh giá
chỉ tập trung vào khả năng lập trình của các LLM được hướng
dẫn [20], [21], [2], [22] bằng cách đánh giá khả năng mô hình
tạo ra một đoạn mã cho một mô tả ngôn ngữ tự nhiên đã cho,
chỉ là một trong những tác vụ liên quan đến mã đa dạng trong
phát triển và bảo trì phần mềm. Mặc dù một số công trình gần
đây khám phá khả năng của các mô hình được hướng dẫn như
ChatGPT và Codex [23], [24], [25], [26] trên các tác vụ kỹ
thuật phần mềm (ví dụ: kiểm thử và gỡ lỗi), những mô hình
thương mại này là closed-source, do đó thiếu tính minh bạch
và khả năng tái tạo. Nhìn chung, vẫn chưa rõ các LLM được
hướng dẫn mã nguồn mở gần đây hoạt động như thế nào trên
các tác vụ hiểu và sinh mã đa dạng trong kỹ thuật phần mềm.

Để lấp đầy khoảng trống kiến thức này, chúng tôi thực hiện
nỗ lực đầu tiên để đánh giá các LLM được điều chỉnh hướng
dẫn mã nguồn mở trên các tác vụ hiểu và sinh mã. Trong khi
luôn có sự nhiệt tình nghiên cứu trong việc tận dụng các kỹ
thuật học sâu tiên tiến để giải quyết các tác vụ liên quan đến
mã trong lĩnh vực kỹ thuật phần mềm [27], [28], [29], [30],
công trình hiện có tập trung vào arXiv:2308.01240v1  [cs.CL]  2 Aug 2023

--- TRANG 2 ---
các mô hình pre-train tương đối nhỏ mà không có điều chỉnh
hướng dẫn (ví dụ: CodeT5 [31] và CodeBERT [32]). Khác với
công trình hiện có, chúng tôi tập trung vào các mô hình được
điều chỉnh hướng dẫn gần đây, làm sáng tỏ khả năng của điều
chỉnh hướng dẫn trong cả hiểu và sinh mã.

Trong công trình này, chúng tôi thực hiện một nghiên cứu
toàn diện cho 10 LLM được điều chỉnh hướng dẫn tiên tiến
trên 4 tác vụ hiểu và sinh mã đại diện, tức là phát hiện lỗi,
phát hiện bản sao, sinh assertion, và tóm tắt mã. Cụ thể, các
LLM được hướng dẫn mà chúng tôi nghiên cứu bao gồm những
mô hình mã nguồn mở mới được phát hành trong bốn tháng
qua (tức là từ ngày 1 tháng 3 đến ngày 1 tháng 7 năm 2023),
bao gồm một loạt rộng các quy mô mô hình (tức là từ 6B đến
16B tham số) dựa trên các mô hình nền tảng khác nhau (ví dụ:
LLaMA [2], Pythia [33], và GLM [34]) được điều chỉnh bởi
các hướng dẫn từ cả lĩnh vực tổng quát và mã. Ngoài ra, chúng
tôi tiếp tục bao gồm bốn mô hình pre-train nhỏ (tức là CodeGPT-
adapted [35], CoText [36], PLBART [37], và CodeT5 [31])
đã được chứng minh đạt hiệu suất tốt nhất sau khi được fine-
tune trên từng tác vụ được nghiên cứu [29] và một mô hình lớn
bổ sung mà không có điều chỉnh hướng dẫn (tức là CodeGen-
6B [38]) làm baseline, để so sánh hiệu suất của các mô hình
được hướng dẫn với các mô hình SOTA. Đối với tất cả các
LLM được điều chỉnh hướng dẫn được nghiên cứu, chúng tôi
đánh giá khả năng của chúng trong ba thiết lập khác nhau bằng
cách trả lời các câu hỏi nghiên cứu sau.

• RQ1: Các LLM được điều chỉnh hướng dẫn hoạt động
như thế nào trên các tác vụ hiểu và sinh mã trong thiết
lập zero-shot? RQ này nhằm điều tra khả năng tổng quát
hóa zero-shot của các LLM được hướng dẫn trên các tác
vụ liên quan đến mã.

• RQ2: Các LLM được điều chỉnh hướng dẫn hoạt động
như thế nào trên các tác vụ hiểu và sinh mã trong thiết
lập few-shot? RQ này nhằm điều tra khả năng học trong
ngữ cảnh của các LLM được điều chỉnh hướng dẫn trên
các tác vụ liên quan đến mã khi các ví dụ minh họa bổ
sung được đưa ra trong prompt. Ngoài ra, chúng tôi cũng
nghiên cứu tác động của ba chiến lược lựa chọn shot khác
nhau.

• RQ3: Các LLM được điều chỉnh hướng dẫn hoạt động
như thế nào trên các tác vụ hiểu và sinh mã với fine-
tuning thêm? RQ này nhằm khám phá hiệu suất của các
LLM được hướng dẫn sau khi chúng được fine-tune thêm
trên từng tác vụ liên quan đến mã cụ thể.

Ngoài ra, chúng tôi cũng điều tra chi phí bộ nhớ và chi phí
thời gian của việc sử dụng các mô hình được hướng dẫn này
để cộng đồng tham khảo thông qua câu hỏi nghiên cứu sau.

• RQ4: Chi phí của các LLM được điều chỉnh hướng dẫn
trong quá trình fine-tuning và inference là như thế nào?

Các phát hiện và ý nghĩa chính. Dựa trên kết quả của chúng
tôi, chúng tôi có những phát hiện chính sau đây. Thứ nhất,
trong thiết lập zero-shot, chúng tôi thấy rằng các LLM được
hướng dẫn rất cạnh tranh trên các tác vụ hiểu và sinh mã và
đôi khi thậm chí còn tốt hơn các mô hình SOTA nhỏ được
fine-tune cụ thể trên từng tác vụ. Chúng tôi cũng có những phát hiện thú vị rằng trên các tác vụ liên quan đến mã, các
LLM được hướng dẫn bởi lĩnh vực mã không nhất thiết vượt
trội hơn các LLM được hướng dẫn bởi lĩnh vực tổng quát, và
các LLM được hướng dẫn lớn hơn không phải lúc nào cũng
tốt hơn. Thứ hai, trong thiết lập few-shot, chúng tôi thấy rằng
việc thêm các ví dụ minh họa giúp các LLM được hướng dẫn
hoạt động tốt hơn đáng kể trên hầu hết các tác vụ hiểu và sinh
mã; tuy nhiên, các ví dụ đôi khi có thể gây ra hiệu suất không
ổn định hoặc thậm chí tệ hơn. Ngoài ra, chúng tôi quan sát
thấy hiệu suất giảm khi độ dài đầu vào tăng và khả năng tuân
theo hướng dẫn tăng trong thiết lập few-shot. Hơn nữa, chúng
tôi thấy chiến lược lựa chọn shot dựa trên BM25 được sử dụng
rộng rãi vượt trội đáng kể so với lựa chọn ngẫu nhiên cơ bản
hoặc lựa chọn cố định chỉ trên các vấn đề sinh (ví dụ: sinh
assertion và tóm tắt mã), trong khi không có sự khác biệt đáng
kể so với các chiến lược cơ bản trên các vấn đề phân loại (ví
dụ: phát hiện lỗi hoặc phát hiện bản sao). Thứ ba, trong thiết
lập fine-tuning, chúng tôi thấy rằng fine-tuning có thể cải thiện
thêm hiệu suất mô hình trên các tác vụ hiểu và sinh mã so với
hiệu suất zero-shot/one-shot. Ngoài ra, sau khi được fine-tune
trên cùng một tập dữ liệu tác vụ, các LLM được hướng dẫn
vượt trội hơn cả các mô hình SOTA nhỏ và các LLM tương tự
về quy mô mà không có điều chỉnh hướng dẫn, cho thấy lợi
ích lớn của điều chỉnh hướng dẫn. Cuối cùng, chúng tôi thấy
rằng các LLM được điều chỉnh hướng dẫn có quy mô tương
tự khác nhau về chi phí bộ nhớ và chi phí thời gian; trong khi
chúng không nhất thiết tốn nhiều tài nguyên bộ nhớ hơn các
mô hình SOTA nhỏ, nhưng tốn nhiều chi phí thời gian hơn
các mô hình SOTA nhỏ trong cả fine-tuning và inference. Dựa
trên các phát hiện của chúng tôi, chúng tôi tiếp tục trình bày
các ý nghĩa thực tiễn về khuyến nghị mô hình và sử dụng, đánh
đổi hiệu suất và chi phí, và các hướng tương lai.

Tóm lại, công trình này đóng góp những điều sau:

• Theo hiểu biết của chúng tôi, bài báo này phục vụ như
nghiên cứu đầu tiên về đánh giá các LLM được điều chỉnh
hướng dẫn mã nguồn mở trên các tác vụ hiểu và sinh mã.
Cụ thể, chúng tôi thực hiện một thí nghiệm quy mô lớn
gồm 10 LLM được hướng dẫn với năm mô hình baseline
trên bốn tác vụ liên quan đến mã đại diện trong các thiết
lập zero-shot, few-shot, và fine-tuning, tốn khoảng 1000
giờ GPU trên một GPU NVIDIA A800-80GB.

• Nghiên cứu của chúng tôi cho thấy nhiều phát hiện và ý
nghĩa thực tiễn về các LLM được hướng dẫn cho hiểu và
sinh mã, chẳng hạn như so sánh giữa các LLM được hướng
dẫn và các mô hình SOTA trong các thiết lập khác nhau,
khuyến nghị cho các LLM được hướng dẫn và chiến lược
lựa chọn shot, và đánh đổi giữa hiệu suất mô hình và chi
phí.

II. BỐI CẢNH
Hình 1. Điều chỉnh hướng dẫn

--- TRANG 3 ---
Điều chỉnh hướng dẫn. Các mô hình ngôn ngữ lớn (LLM)
là các mô hình có hàng trăm tỷ (hoặc nhiều hơn) tham số,
được pre-train trên các tập dữ liệu văn bản lớn [39], [40],
[41], [42]. Để tăng cường khả năng tổng quát hóa của chúng
đối với các tác vụ chưa thấy, LLM phục vụ như các mô hình
nền tảng để điều chỉnh thêm với tập dữ liệu hướng dẫn, một
quá trình được gọi là điều chỉnh hướng dẫn [10], [9], [8], như
được mô tả trong Hình 1. Điều chỉnh hướng dẫn là một phương
pháp có giám sát để dạy các mô hình nền tảng tuân theo hướng
dẫn để giải quyết các tác vụ mới, tức là tạo ra đầu ra dựa trên
đầu vào hướng dẫn. Sau điều chỉnh hướng dẫn, LLM có được
khả năng tuân theo hướng dẫn tác vụ cho các tác vụ mới mà
không cần bất kỳ ví dụ minh họa nào, do đó tăng cường khả
năng tổng quát hóa của chúng.

Học trong ngữ cảnh: zero-shot và few-shot. Học trong ngữ
cảnh đề cập đến quá trình yêu cầu LLM đưa ra suy luận dựa
trên các ví dụ minh họa hoặc mô tả tác vụ đã cho [1]. Nó có
thể được phân loại thành hai loại: học zero-shot và học few-
shot [1]. Trong học zero-shot, LLM tạo ra phản hồi chỉ dựa
trên mô tả hướng dẫn ngôn ngữ tự nhiên, mà không cần cập
nhật tham số nào. Mặt khác, các tác vụ học few-shot bao gồm
LLM đưa ra phản hồi chỉ với một vài ví dụ minh họa, một lần
nữa mà không cập nhật tham số mô hình.

Fine-tuning hiệu quả tham số. Fine-tuning mô hình trên các
tác vụ cụ thể có hiệu quả trong việc tăng cường hiệu suất cụ
thể cho tác vụ. Tuy nhiên, số lượng lớn tham số trong mô hình
làm cho việc cập nhật tất cả chúng trong quá trình fine-tuning
trở nên không thực tế. Để giải quyết vấn đề này, một phương
pháp Fine-tuning Hiệu quả Tham số (PEFT) [43] được đề
xuất, duy trì hiệu suất tốt nhất của LLM trong khi fine-tune
chỉ một số lượng nhỏ tham số. Một số kỹ thuật thuộc về PEFT,
bao gồm adapter tuning [43], prefix tuning [44], prompt tuning
[45], và low-rank adaptation (LoRA). Trong số các kỹ thuật
này, LoRA nổi bật với khả năng giảm đáng kể số lượng tham
số được cập nhật trong quá trình fine-tuning bằng cách áp đặt
ràng buộc low-rank trên ma trận được cập nhật của mỗi lớp
dense. Các nghiên cứu [46] đã cho thấy hiệu suất vượt trội của
LoRA so với các phương pháp PEFT khác trong khi sử dụng
ít tham số có thể training hơn, làm cho nó trở thành lựa chọn
phổ biến cho fine-tuning hiệu quả trong các triển khai LLM
mã nguồn mở như LLaMA [2] và BLOOM [3].

III. THIẾT LẬP THÍ NGHIỆM
Công trình này thực hiện nỗ lực đầu tiên để hiểu một cách
toàn diện hiệu suất của các LLM được điều chỉnh hướng dẫn
trên hiểu và sinh mã bằng cách trả lời các câu hỏi nghiên cứu
sau.

• RQ1 (Zero Shot): Các LLM được hướng dẫn hoạt động
như thế nào trên các tác vụ hiểu và sinh mã trong thiết
lập zero-shot?

• RQ2 (Few Shot): Các LLM được hướng dẫn hoạt động
như thế nào trên các tác vụ hiểu và sinh mã trong thiết
lập few-shot?

• RQ3 (Fine Tuning): Các LLM được hướng dẫn hoạt động
như thế nào trên các tác vụ hiểu và sinh mã với fine-
tuning thêm?

• RQ4 (Chi phí): Chi phí của các LLM được hướng dẫn
là như thế nào?

RQ1 điều tra khả năng tổng quát hóa zero-shot của các LLM
được điều chỉnh hướng dẫn trên các tác vụ liên quan đến mã
khác nhau; RQ2 điều tra khả năng học trong ngữ cảnh của
các LLM được điều chỉnh hướng dẫn trong thiết lập few-shot
cũng như tác động của các chiến lược lựa chọn shot khác nhau;
RQ3 điều tra hiệu suất của các LLM được điều chỉnh hướng
dẫn sau khi được fine-tune thêm trên tác vụ cụ thể; và RQ4
so sánh chi phí inference và fine-tuning của các LLM được
hướng dẫn về chi phí bộ nhớ và thời gian.

A. Các Tác vụ & Tập dữ liệu & Thước đo được Nghiên cứu

1) Tác vụ: Trong công trình này, chúng tôi tập trung vào bốn
tác vụ hiểu và sinh mã đại diện đã được sử dụng rộng rãi trong
các nghiên cứu thực nghiệm gần đây về trí tuệ mã [29], [30],
[28], đó là Phát hiện Bản sao (CD), Phát hiện Lỗi (DD), Sinh
Assertion (AG), và Tóm tắt Mã (CS).

• Phát hiện Lỗi (DD). Tác vụ này phát hiện xem các đoạn
mã có chứa lỗ hổng liên quan đến bảo mật như Denial of
Service và injection hay không. Tác vụ này có thể phản
ánh khả năng mô hình hiểu tính đúng đắn của mã.

• Phát hiện Bản sao (CD). Tác vụ này xác định xem các
đoạn mã có tương tự cao hay không. Tác vụ này có thể
phản ánh khả năng mô hình nắm bắt sự tương tự của mã.

• Sinh Assertion (AG). Tác vụ này sinh ra câu lệnh assertion
cho phương thức focal đã cho (ví dụ: mã đang được kiểm
thử) và tiền tố unit test (ví dụ: mã test trước assertion).
Tác vụ này có thể phản ánh khả năng mô hình hiểu ý định
và sinh ra oracle cho mã đang được kiểm thử.

• Tóm tắt Mã (CS). Tác vụ này sinh ra mô tả ngôn ngữ tự
nhiên cho chức năng của đoạn mã đã cho. Tác vụ này có
thể phản ánh khả năng mô hình hiểu ý định mã và sinh
ra tóm tắt bằng ngôn ngữ tự nhiên.

2) Thước đo: Đối với các tác vụ phát hiện lỗi, phát hiện bản
sao, và sinh assertion, chúng tôi tuân theo công trình trước
[47], [29], [48] bằng cách sử dụng cùng các thước đo. Cụ thể,
chúng tôi áp dụng Accuracy (Acc) cho phát hiện lỗi, tính tỷ
lệ các lỗi được xác định bởi các mô hình; cho phát hiện bản
sao, chúng tôi áp dụng F1 [49], tính trung bình điều hòa của
precision và recall; cho sinh assertion, chúng tôi áp dụng exact
match (EM), tính tỷ lệ các assertion được sinh ra giống hệt
với ground truth.

Đối với tóm tắt mã, chúng tôi không tuân theo công trình
trước [29] để sử dụng thước đo BLEU, đo sự tương tự giữa
chuỗi token được sinh ra và ground truth dựa trên khớp n-
gram. Như đã được chỉ ra bởi công trình trước về tóm tắt mã
[50], BLEU không thể nắm bắt chính xác sự tương tự ngữ
nghĩa giữa các chuỗi token, do đó dẫn đến đánh giá tự động
không nhất quán với đánh giá thủ công. Ngoài ra, một bias
như vậy thậm chí sẽ được phóng đại trong việc đánh giá các
LLM được điều chỉnh hướng dẫn, vì những mô hình này đã
được điều chỉnh trên các hướng dẫn giống con người và có
xu hướng sinh ra tóm tắt mã có định dạng khác với ground
truth. Do đó, BLEU có xu hướng đánh giá thấp sự tương tự
giữa tóm tắt mã được sinh ra và

--- TRANG 4 ---
ground truth. Để khắc phục vấn đề này, chúng tôi tuân theo
xu hướng gần đây trong việc đánh giá các LLM được điều
chỉnh hướng dẫn bằng cách kết hợp LLM mạnh mẽ ChatGPT
[11] làm người phán xét [51], [52], [53], [54], [55], [56], [57],
[58]. Cụ thể, chúng tôi áp dụng mẫu prompt phổ biến được
sử dụng trong công trình trước [51] để truy vấn ChatGPT so
sánh chất lượng của tóm tắt mã được sinh ra và ground truth.
Hình 2 trình bày một ví dụ cho prompt. Ngoài ra, để giảm
thiểu bias vị trí tiềm ẩn, chúng tôi tiếp tục kết hợp chiến lược
few-shot được đề xuất trong công trình trước [51]. Cụ thể,
chúng tôi bao gồm hai ví dụ được tạo thủ công, được gắn
nhãn là "SUMMARY1 good" và "SUMMARY2 good" vào
prompt để hướng dẫn quá trình đánh giá. Sau đó chúng tôi
tính tỷ lệ ưa thích PR (tức là tần suất các trường hợp mà tóm
tắt được sinh ra được ChatGPT ưa thích) để đo khả năng mô
hình trên tác vụ tóm tắt mã.

Hình 2. Prompt được sử dụng để phán xét

3) Tập dữ liệu: Tuân theo công trình trước [29], chúng tôi
áp dụng các tập dữ liệu được sử dụng rộng rãi cho từng tác
vụ được nghiên cứu. Xem xét chi phí không tầm thường của
việc fine-tune các LLM được hướng dẫn và quy mô rộng lớn
của các thí nghiệm, chúng tôi không sử dụng tất cả các mục
dữ liệu trong các tập dữ liệu gốc, mà lấy mẫu một phần các
mục dữ liệu làm tập dữ liệu training, validation, và testing của
chúng tôi. Bảng I liệt kê kích thước chi tiết của các tập dữ
liệu được sử dụng trong từng tác vụ. Cụ thể, chúng tôi lấy
mẫu 2.000 mục làm tập dữ liệu testing cho từng tác vụ ngoại
trừ tóm tắt mã, do chi phí thời gian và tài chính lớn trong việc
truy vấn API ChatGPT để tính thước đo PR.

Bảng I
TÓM TẮT CÁC TÁC VỤ, TẬP DỮ LIỆU VÀ THƯỚC ĐO ĐƯỢC NGHIÊN CỨU

Tác vụ Tập dữ liệu Kích thước Train/Val/Test Thước đo
Phát hiện Lỗi (DD) Devign [59] 20k/2k/2k Acc
Phát hiện Bản sao (CD) BigCloneBench [60] 100k/2k/2k F1
Sinh Assertion (AG) ATLAS [61] 120k/2k/2k EM
Tóm tắt Mã (CS) CodeSearchNet [62] 100k/2k/0.1k PR

B. Các Mô hình được Nghiên cứu

Các LLM được Điều chỉnh Hướng dẫn được Nghiên cứu.
Vì các LLM được điều chỉnh hướng dẫn là một lĩnh vực mới
nổi và phát triển nhanh chóng gần đây (đặc biệt sau khi phát
hành ChatGPT), nghiên cứu của chúng tôi chủ yếu tập trung
vào các LLM được điều chỉnh hướng dẫn gần đây đã được
phát hành trong bốn tháng qua (tức là từ ngày 1 tháng 3 đến
ngày 1 tháng 7 năm 2023). Cụ thể, chúng tôi loại trừ các LLM
được điều chỉnh hướng dẫn (i) là closed-source (ví dụ: ChatGPT)
do thiếu tính minh bạch và khả năng tái tạo, hoặc (ii) có hơn
20B tham số (ví dụ: Falcon 40B [77]) do hạn chế tài nguyên
của chúng tôi. Kết quả là, các thí nghiệm của chúng tôi bao
gồm tổng cộng 10 LLM được điều chỉnh hướng dẫn gần đây.
Bảng II trình bày chi tiết về các LLM được điều chỉnh hướng
dẫn mà chúng tôi nghiên cứu. Từ bảng, các thí nghiệm của
chúng tôi bao gồm một phổ rộng các LLM được điều chỉnh
hướng dẫn đa dạng theo nhiều chiều, chẳng hạn như kích thước
mô hình, mô hình nền tảng, và lĩnh vực dữ liệu hướng dẫn.
Ví dụ, kích thước của các LLM được điều chỉnh hướng dẫn
mà chúng tôi nghiên cứu dao động từ 6B (ví dụ: ChatGLM-
6B) đến 16B (ví dụ: Instruct-CodeGen-16B), và các LLM
được điều chỉnh hướng dẫn mà chúng tôi nghiên cứu được
xây dựng trên các mô hình nền tảng khác nhau (ví dụ: LLaMA,
Pythia, và GLM). Hơn nữa, chúng tôi bao gồm cả LLM được
(i) điều chỉnh trên hướng dẫn tổng quát hoặc (ii) điều chỉnh
trên hướng dẫn cụ thể về mã (tức là hướng dẫn là các tác vụ
liên quan đến mã). Ví dụ, Alpaca được xây dựng bằng cách
điều chỉnh mô hình nền tảng LLaMA trên tập dữ liệu hướng
dẫn tổng quát [65], trong khi CodeAlpaca được xây dựng bằng
cách điều chỉnh mô hình nền tảng LLaMA trên tập dữ liệu
hướng dẫn mã [72].

Các Baseline Khác. Ngoài 10 LLM được điều chỉnh hướng
dẫn được đề cập ở trên, chúng tôi tiếp tục bao gồm năm baseline
như sau. (i) Các mô hình SOTA nhỏ: chúng tôi bao gồm bốn
mô hình pre-train nhỏ (với ít hơn 1B tham số rất nhiều) đã
đạt hiệu suất tốt nhất trên các tác vụ nghiên cứu của chúng
tôi sau khi được fine-tune trên từng tác vụ như được thể hiện
trong công trình mới nhất [29], đó là CodeGPT-adapted [35]
cho phát hiện bản sao, CoText [36] cho phát hiện lỗi, PLBART
[37] cho sinh assertion, và CodeT5 [31] cho tóm tắt mã. Đáng
chú ý là chúng tôi sử dụng các mô hình xếp hạng thứ hai cho
các tác vụ phát hiện lỗi và phát hiện bản sao vì mô hình Top-1
SynCoBERT [78] là closed-source. Chúng tôi bao gồm những
mô hình này để cho phép so sánh giữa các LLM được điều
chỉnh hướng dẫn và các mô hình pre-train SOTA nhỏ này.
(ii) Các mô hình lớn mà không có điều chỉnh hướng dẫn:
chúng tôi cũng bao gồm một mô hình ngôn ngữ lớn trong
phạm vi quy mô tương tự như các LLM được điều chỉnh hướng
dẫn mà chúng tôi nghiên cứu nhưng không có điều chỉnh hướng
dẫn, tức là CodeGen-6B [38], được pre-train trên tập dữ liệu
của nhiều ngôn ngữ lập trình theo cách autoregressive. Chúng
tôi bao gồm mô hình này để so sánh các LLM có quy mô tương
tự có/không có điều chỉnh hướng dẫn.

C. Thiết kế Prompt

Thông thường, prompt để truy vấn các LLM được điều chỉnh
hướng dẫn bao gồm ba phần: system prompt, hướng dẫn tác
vụ, và từ khóa để phân tách các phần khác nhau. Bảng III
trình bày cấu trúc prompt của từng mô hình. Sau đó chúng tôi
giải thích chi tiết từng phần.

System prompt là câu khởi đầu trong prompt, thường được
định nghĩa bởi chính từng LLM được điều chỉnh hướng dẫn.
Ví dụ, Alpaca chính thức gợi ý sử dụng câu "Below is an
instruction that describes a task, paired with an input that
provides further context. Write a response that appropriately
completes the request." làm system prompt của nó. Lưu ý
rằng system prompt là tùy chọn vì không phải tất cả các LLM
được điều chỉnh hướng dẫn đều kết hợp system prompt (ví
dụ: ChatGLM và CodeAlpaca). Do đó, các thí nghiệm của
chúng tôi chỉ bao gồm system prompt cho các LLM được điều
chỉnh hướng dẫn mà tài liệu chính thức hoặc kho mã đã cung
cấp

--- TRANG 5 ---
Bảng II
TÓM TẮT CÁC LLM ĐƯỢC ĐIỀU CHỈNH HƯỚNG DẪN ĐƯỢC NGHIÊN CỨU

LLM được Điều chỉnh Hướng dẫn Mô hình Nền tảng Tập dữ liệu Điều chỉnh Hướng dẫn
Lĩnh vực Nguồn Tập dữ liệu Kích thước Thời gian Phát hành

ChatGLM-6B [34] GLM [63] Tổng quát Corpus tiếng Trung và tiếng Anh [64] Không rõ Không rõ 2023/03
Alpaca-7B [6] LLaMA [2] Tổng quát Self-instruct [65] GPT-3 52k 2023/03
Vicuna-7B [5] LLaMA [2] Tổng quát ShareGPT [66] ChatGPT 70k 2023/03
Dolly-v2-7B [67] Pythia [33] Tổng quát databricks-dolly-15k [68] Thủ công 15k 2023/04
StableLM-7B [69] StableLM-Base [70] Tổng quát Năm tập dữ liệu đối thoại [71] ChatGPT+GPT-3+Thủ công Không rõ 2023/04
CodeAlpaca-7B [7] LLaMA [2] Mã Code Alpaca [72] GPT-3 20K 2023/03
Dolly-v2-12B [67] Pythia [33] Tổng quát databricks-dolly-15k [68] Thủ công 15k 2023/04
Vicuna-13B [5] LLaMA [2] Tổng quát ShareGPT [66] ChatGPT 70k 2023/03
WizardCoder-15B [22] StarCoder [73] Mã CodeAlpaca với Evol-Instruct [74] GPT-3 78k 2023/06
Instruct-CodeGen-16B [75] CodeGen-multi [38] Mã Hướng dẫn mã [76] Không rõ 250k 2023/05

Bảng III
CẤU TRÚC PROMPT

LLM Prompt
Vicuna ${System prompt}.User: ${Task instruction}.Assistant:
StableLM ${System prompt}.<|USER|>:${Task instruction}.<|ASSISTANT|>:
Alpaca
${System prompt}.Instruction: ${Task instruction}.Response:Dolly
Instruct-CodeGen
WizardCoder
ChatGLMInstruction: ${Task instruction}.Response:CodeAlpaca

system prompt của nó. Chi tiết thêm về system prompt được
sử dụng trong từng mô hình có thể tìm thấy trên trang web
của chúng tôi [79].

Hướng dẫn tác vụ mô tả tác vụ chi tiết cần được thực hiện
bởi LLM. Tuân theo nguyên tắc thiết kế prompt rằng mục tiêu
tác vụ nên được làm rõ [14] và tham khảo mô tả tác vụ trong
các nghiên cứu trước [48], [47], chúng tôi thiết kế hướng dẫn
tác vụ cho từng tác vụ như sau. Đối với phát hiện lỗi, hướng
dẫn tác vụ là "Is there a defect in ${code}, and respond to
YES or NO", trong đó ${code} là đoạn mã đã cho để phát
hiện lỗi. Đối với phát hiện bản sao, hướng dẫn tác vụ là "Is
there a clone relation between the ${code1} and ${code2},
and respond to YES or NO", trong đó ${code1} và ${code2}
là cặp đoạn mã đã cho để phát hiện bản sao. Đối với sinh
assertion, hướng dẫn tác vụ là "Generate an assertion code
at the <AssertPlaceHolder> in the following ${code} using
Junit API", trong đó ${code} là phương thức focal đã cho
(phương thức đang được kiểm thử) và tiền tố test và <AssertPlaceHolder>
là một placeholder trong ${code}. Đối với tóm tắt mã, hướng
dẫn tác vụ là "Generate the method-level comment for the
following ${code}", trong đó ${code} là đoạn mã đã cho để
tóm tắt.

Từ khóa là các token được bảo lưu được định nghĩa bởi
chính các LLM được điều chỉnh hướng dẫn để chỉ ra các phần
khác nhau trong prompt. Ví dụ, Vicuna sử dụng từ khóa "User"
để chỉ ra hướng dẫn tác vụ theo sau trong khi StableLM sử
dụng "<|USER|>". Các từ khóa chi tiết được định nghĩa bởi
từng mô hình được đánh dấu in đậm trong Bảng III.

D. Quy trình Thí nghiệm

Trong RQ1, chúng tôi điều tra khả năng tổng quát hóa zero-
shot của các LLM được điều chỉnh hướng dẫn trên từng tác
vụ hiểu và sinh mã. Cụ thể, đối với tất cả 10 LLM được điều
chỉnh hướng dẫn được nghiên cứu cũng như baseline chưa
được hướng dẫn CodeGen-6B, chúng tôi trực tiếp truy vấn
chúng thông qua prompt (được định nghĩa trong Phần III-C).
Đối với bốn mô hình SOTA nhỏ, trước tiên chúng tôi fine-tune
chúng trên từng tác vụ và so sánh hiệu suất fine-tuned của
chúng trên tập dữ liệu testing, vì những mô hình nhỏ và chưa
được hướng dẫn như vậy không áp dụng được trong thiết lập
zero-shot.

Trong RQ2, chúng tôi điều tra hiệu suất few-shot cũng như
tác động của các chiến lược lựa chọn shot khác nhau. Hiện
tại chúng tôi tập trung vào one shot, vì hai hoặc nhiều shot
đôi khi vượt quá số token tối đa mà mô hình có thể xử lý trên
các tác vụ của chúng tôi. Cụ thể, chúng tôi bao gồm một ví
dụ minh họa ở đầu prompt trong thiết lập one-shot. Chúng
tôi xem xét ba chiến lược lựa chọn shot phổ biến đã được sử
dụng rộng rãi trong công trình trước [80], bao gồm (i) chiến
lược Fix One (FO) sử dụng nhất quán cùng một ví dụ được
thiết kế thủ công cho tất cả các mục dữ liệu testing trong từng
tác vụ; (ii) Randomly-selected One (RO) lựa chọn ngẫu nhiên
một ví dụ minh họa từ tập dữ liệu training cho từng mục dữ
liệu testing trong từng tác vụ; và (iii) BM25-based One (BO),
truy xuất ví dụ minh họa có độ tương tự BM25 [81] cao nhất
với mục dữ liệu testing từ tập dữ liệu training.

Trong RQ3, chúng tôi fine-tune tất cả các mô hình được
nghiên cứu trên cùng tập dữ liệu training cho từng tác vụ.
Cụ thể, đối với các mô hình lớn như 10 LLM được điều chỉnh
hướng dẫn và CodeGen-6B, chúng tôi fine-tune chúng với
chiến lược điều chỉnh hiệu quả tham số Low-Rank Adaptation
(LoRA), gia tốc quá trình training các mô hình lớn với chi
phí bộ nhớ ít hơn bằng cách bao gồm một số lượng nhỏ tham
số có thể training (tức là adapter) và đóng băng các tham số
gốc cho fine-tuning LLM. Đối với các mô hình SOTA nhỏ,
chúng tôi tuân theo công trình trước [29] để điều chỉnh tất cả
các tham số của chúng.

Trong RQ4, chúng tôi ghi lại chi phí thời gian trung bình
và chi phí bộ nhớ của từng mô hình trong giai đoạn training
và inference. Tổng cộng, công trình này thực hiện các thí
nghiệm quy mô lớn cho 15 mô hình trên 4 tác vụ liên quan
đến mã trong các thiết lập đa dạng (tức là zero-shot, few-shot
với ba chiến lược lựa chọn shot khác nhau, và fine-tuning),
tốn khoảng 1000 giờ GPU trên một GPU NVIDIA A800-80GB.

E. Chi tiết Triển khai

Triển khai mô hình. Đối với tất cả các mô hình lớn được
nghiên cứu, chúng tôi trực tiếp tải xuống chúng từ các kho
mã chính thức, vì chúng là mã nguồn mở. Ngoài ra, chúng
tôi trực tiếp sử dụng các script fine-tuning và inference của
những mô hình này nếu chúng được cung cấp trong các kho.
Đối với các mô hình SOTA nhỏ được so sánh, vì kết quả được
báo cáo trong công trình trước [29] dựa trên tập dữ liệu training
và tập dữ liệu testing khác với các thí nghiệm của chúng tôi,
để so sánh công bằng, chúng tôi tự fine-tune những mô hình
này trên cùng tập dữ liệu training

--- TRANG 6 ---
được sử dụng để fine-tune các mô hình được điều chỉnh hướng
dẫn. Cụ thể, chúng tôi nghiêm túc tuân theo cùng các siêu
tham số training những mô hình SOTA này như công trình
trước [29], bao gồm batch size, số epoch training, và learning
rate.

Siêu tham số. Đối với tất cả các LLM được nghiên cứu,
chúng tôi đặt "max length" thành 2.048 token. Để giảm thiểu
tính ngẫu nhiên trong các phản hồi được sinh ra, chúng tôi
đặt tham số "do sample" thành False để đảm bảo kết quả xác
định. Các siêu tham số khác của các LLM được điều chỉnh
hướng dẫn được đặt là các giá trị mặc định được cung cấp
trong các kho mã của chúng.

Môi trường. Thí nghiệm của chúng tôi được thực hiện trên
một GPU NVIDIA A800-80G duy nhất. Hệ điều hành là Ubuntu
20.04.6 LTS.

IV. KẾT QUẢ THÍ NGHIỆM

A. Hiệu suất trong Zero Shot (RQ1)

Bảng IV trình bày hiệu suất của các LLM được điều chỉnh
hướng dẫn được nghiên cứu trên từng tác vụ liên quan đến
mã trong thiết lập zero-shot. Chúng tôi cũng trình bày kết
quả của các mô hình pre-train tương đối nhỏ hơn đã được
fine-tune trên từng tác vụ (hàng "SOTA Model") và kết quả
của mô hình ngôn ngữ lớn về mã mà không có điều chỉnh
hướng dẫn (hàng "CodeGen-6B") để so sánh. Lưu ý rằng hiệu
suất của các mô hình SOTA được báo cáo trong công trình
của chúng tôi thấp hơn so với công trình trước [29] vì chúng
được training và đánh giá trên tập dữ liệu training nhỏ hơn
trong công trình của chúng tôi. Hiệu suất tốt nhất trong từng
tác vụ được đánh dấu. Dựa trên bảng, chúng tôi có các quan
sát sau.

Bảng IV
HIỆU SUẤT TRONG ZERO-SHOT

Mô hình DD (%) CD (%) AG (%) CS (%)
SOTA Model 58.7 7.4 25.7 24.0
CodeGen-6B 0.3 1.4 0.0 0.0
ChatGLM-6B 7.1 17.5 1.7 45.0
Vicuna-7B 54.0 13.2 10.1 48.0
Alpaca-7B 45.8 22.1 5.3 32.0
Dolly-7B 33.1 21.3 1.9 12.0
StableLM-7B 44.3 24.3 1.1 30.0
CodeAlpaca-7B 51.9 1.4 4.4 9.0
Dolly-12B 33.8 23.5 1.0 5.0
Vicuna-13B 49.8 14.1 12.0 63.0
WizardCoder-15B 54.4 23.8 19.4 71.0
Instruct-CodeGen-16B 47.8 14.2 8.4 9.0

Thứ nhất, ngay cả trong thiết lập zero-shot, các LLM được
điều chỉnh hướng dẫn đạt hiệu suất tương đương một cách
đáng ngạc nhiên, đôi khi thậm chí tốt hơn so với các tác vụ
hiểu và sinh mã so với các mô hình SOTA nhỏ đã được fine-
tune cụ thể trên từng tác vụ. Ví dụ, trong phát hiện bản sao
và tóm tắt mã, hầu hết các LLM được điều chỉnh hướng dẫn
vượt trội hơn các mô hình SOTA fine-tuned. Ngoài ra, trong
phát hiện lỗi và sinh assertion, mặc dù các mô hình SOTA
fine-tuned giữ hiệu suất Top-1 trên hai tác vụ này, một số
LLM được điều chỉnh hướng dẫn (ví dụ: WizardCoder-15B)
có thể đạt hiệu suất tương đương (tức là 54.4% v.s. 58.7%
trên phát hiện lỗi). Đáng chú ý là CodeGen-6B (một LLM
có số lượng tham số tương đương nhưng không có điều chỉnh
hướng dẫn) hoạt động cực kỳ kém trong thiết lập zero-shot
như vậy vì nó luôn có xu hướng tạo ra các đoạn mã không
liên quan đến tác vụ. Điều này xác nhận rằng điều chỉnh hướng
dẫn thực sự cải thiện khả năng tổng quát hóa zero-shot của
LLM trên các tác vụ liên quan đến mã chưa thấy.

Thứ hai, thú vị là chúng tôi thấy rằng các LLM được điều
chỉnh bởi hướng dẫn cụ thể về mã không nhất thiết vượt trội
hơn các LLM được điều chỉnh bởi hướng dẫn tổng quát. Ví
dụ, Alpaca-7B vượt trội hơn CodeAlpaca-7B trên tất cả các
tác vụ liên quan đến mã ngoại trừ phát hiện lỗi, trong khi cơ
bản chúng điều chỉnh cùng mô hình nền tảng LLaMA trên
hướng dẫn tổng quát hoặc hướng dẫn cụ thể về mã tương ứng.
Ngoài ra, chúng tôi quan sát thấy rằng các LLM được điều
chỉnh hướng dẫn tổng quát nhỏ hơn (ví dụ: Vicuna-7B) có
cơ hội vượt trội hơn các LLM được điều chỉnh hướng dẫn cụ
thể về mã lớn hơn (ví dụ: Instruct-CodeGen-16B) trên một
số tác vụ. Một lý do tiềm ẩn có thể là các hướng dẫn tổng
quát (ví dụ: ShareGPT [66]) cũng chứa hướng dẫn cụ thể về
mã và các hướng dẫn khác không liên quan đến mã có thể
hữu ích thêm để cải thiện khả năng hiểu hướng dẫn của LLM.
Tóm lại, các LLM được điều chỉnh hướng dẫn tổng quát có
thể đạt hiệu suất cạnh tranh trên các tác vụ hiểu và sinh mã.

Thứ ba, chúng tôi quan sát thấy rằng việc tăng quy mô mô
hình đôi khi mang lại cải thiện nhỏ hoặc đôi khi thậm chí
giảm trong thiết lập zero-shot. Ví dụ, Dolly hoạt động tệ hơn
nhiều trên sinh assertion và tóm tắt mã khi quy mô mô hình
tăng từ 7B lên 12B. Ngoài ra, đối với Vicuna, mặc dù việc
tăng quy mô mô hình từ 7B lên 13B cải thiện hiệu suất trên
các tác vụ sinh như sinh assertion và tóm tắt mã, tác động trên
các tác vụ phân loại như phát hiện lỗi hoặc phát hiện bản sao
khá nhỏ. Thực tế, các nghiên cứu trước [82], [83] cũng tiết
lộ những quan sát tương tự rằng Dolly-12B hoạt động tệ hơn
Dolly-6B trong các tác vụ sinh, và ALMoST-7B [83] vượt
trội hơn Alpaca-13B, Dolly-12B, và OpenAssistant-12B [84].

Phát hiện 1: Trong thiết lập zero-shot, các LLM được
điều chỉnh hướng dẫn có tính cạnh tranh và đôi khi thậm
chí tốt hơn trên các tác vụ hiểu và sinh mã so với các mô
hình SOTA nhỏ được fine-tune cụ thể trên từng tác vụ.
Ngoài ra, các LLM được điều chỉnh hướng dẫn mã không
nhất thiết vượt trội hơn các LLM được điều chỉnh hướng
dẫn tổng quát trên các tác vụ hiểu và sinh mã, và sự cải
thiện từ quy mô mô hình lớn hơn đôi khi có thể bị hạn
chế hoặc thậm chí âm.

B. Hiệu suất trong Few Shot (RQ2)

Trong RQ2, chúng tôi điều tra rộng rãi các LLM được điều
chỉnh hướng dẫn trong thiết lập few-shot (one-shot trong các
thí nghiệm của chúng tôi), bao gồm so sánh hiệu suất và khả
năng tuân theo hướng dẫn với thiết lập zero-shot (trong Phần
IV-B1 và Phần IV-B2) và tác động của các chiến lược lựa
chọn shot khác nhau (Phần IV-B3).

1) So sánh với hiệu suất zero-shot: Bảng V trình bày hiệu
suất của các LLM được điều chỉnh hướng dẫn trong thiết lập
zero-shot (tức là trong cột "ZO") và trong thiết lập one-shot
(tức là trong cột "BO"). Để tiết kiệm không gian, ở đây chúng
tôi chỉ trình bày

--- TRANG 7 ---
Bảng V
HIỆU SUẤT LLM DƯỚI ZERO-SHOT VÀ ONE-SHOT

LLM DD (%) CD (%) AG (%) CS (%)
ZO BO ZO BO ZO BO ZO BO

CodeGen-6B 0.3 43.6 1.4 23.4 0.0 56.2 0.0 13.0
ChatGLM-6B 7.1 54.2 17.5 12.8 1.7 46.2 45.0 54.0
Vicuna-7B 54.0 54.1 13.2 - 10.1 31.2 48.0 37.0
Alpaca-7B 45.8 55.4 22.1 - 5.3 41.4 32.0 6.0
Dolly-7B 33.1 49.9 21.3 23.5 1.9 51.0 12.0 14.0
StableLM-7B 44.3 43.4 24.3 - 1.1 44.4 30.0 19.0
CodeAlpaca-7B 51.9 50.3 1.4 10.3 4.4 35.1 9.0 34.0
Dolly-12B 33.8 52.7 23.5 22.6 1.0 51.7 5.0 8.0
Vicuna-13B 49.8 53.0 14.1 6.5 12.0 44.0 63.0 24.0
WizardCoder-15B 54.4 53.8 23.8 7.3 19.4 63.3 71.0 50.0
Instruct-CodeGen-16B 47.8 54.6 14.2 20.7 8.4 55.0 9.0 41.0

kết quả của chiến lược lựa chọn one-shot tốt nhất (tức là chiến
lược dựa trên BM25) và so sánh giữa các chiến lược khác
nhau có thể tìm thấy trong Phần IV-B3. Dựa trên bảng, chúng
tôi có các quan sát sau.

Nhìn chung, việc bao gồm một ví dụ minh họa trong đầu
vào cải thiện hiệu suất của tất cả các LLM được điều chỉnh
hướng dẫn được nghiên cứu trên hầu hết các tác vụ, tức là
25 trên 40 trường hợp (10 LLM được điều chỉnh hướng dẫn
× 4 tác vụ). Ví dụ, trên sinh assertion, việc bao gồm một ví
dụ cải thiện đáng kể hiệu suất của tất cả các LLM được điều
chỉnh hướng dẫn. Cụ thể, đối với những trường hợp mà các
LLM được điều chỉnh hướng dẫn hoạt động cực kỳ kém (tức
là với hiệu suất dưới 10%), việc thêm one shot luôn có thể
cải thiện hiệu suất bằng một biên độ lớn. Ví dụ, hiệu suất của
ChatGLM-6B tăng từ 7.1% lên 54.2% trên phát hiện lỗi, và
hiệu suất của Instruct-CodeGen-16B tăng từ 9.0% lên 41.0%
trên tóm tắt mã. Những cải thiện được đề cập ở trên cho thấy
khả năng học trong ngữ cảnh mạnh mẽ của các LLM được
điều chỉnh hướng dẫn. Quan sát như vậy phù hợp với đánh
giá trước đây về các LLM được điều chỉnh hướng dẫn trên
các tác vụ khác như giải quyết coreference, QA sách đóng và
trích xuất quan hệ [85], [10], [19].

Phát hiện 2: Việc bao gồm các ví dụ minh họa cải thiện
đáng kể hiệu suất của các LLM được điều chỉnh hướng
dẫn trên hầu hết các tác vụ hiểu và sinh mã; và đối với
những trường hợp mà các LLM được điều chỉnh hướng
dẫn hoạt động cực kỳ kém trong thiết lập zero-shot, việc
bao gồm one shot liên tục cải thiện hiệu suất của chúng
với biên độ đặc biệt lớn.

Mặt khác, có một vài trường hợp mà việc thêm ví dụ gây
ra hiệu suất không ổn định và thậm chí tệ hơn của các LLM
được điều chỉnh hướng dẫn. Ví dụ, trên phát hiện bản sao,
nhiều LLM được điều chỉnh hướng dẫn (tức là Vicuna-7B,
Alpaca-7B, và StableLM-7B) thể hiện phản hồi thiên lệch,
tức là tất cả dự đoán phân loại nhị phân của chúng đều giống
nhau (tất cả yes hoặc tất cả no) trên toàn bộ tập dữ liệu testing,
dẫn đến tính toán không hợp lệ của thước đo F1 (được đánh
dấu là "-" trong bảng). Mặc dù chúng tôi đã điều chỉnh prompt
với nhiều nỗ lực, cùng quan sát vẫn tồn tại. Thực tế, chúng
tôi không phải là công trình đầu tiên tìm thấy một số hành vi
mô hình không trực quan trong thiết lập few-shot. Ví dụ, công
trình trước [86], [87] cho thấy rằng hiệu suất của LLM bị ảnh
hưởng bởi thứ tự của các ví dụ trong đầu vào trong khi đôi
khi thậm chí các ví dụ không chính xác cũng hữu ích cho
LLM trong thiết lập few-shot. Ngoài tính không ổn định vốn
có của học few-shot, công trình gần đây [88] cho thấy rằng
trong lĩnh vực tổng quát, LLM đôi khi hoạt động tệ hơn nhiều
khi ngữ cảnh đầu vào trở nên dài hơn. So với thiết lập zero-
shot, độ dài đầu vào dài hơn nhiều trong thiết lập one-shot
do việc bao gồm các ví dụ bổ sung, do đó đôi khi làm phân
tâm mô hình và giảm hiệu suất. Để xác thực giả định này,
chúng tôi tiếp tục trình bày hiệu suất mô hình với đầu vào có
độ dài khác nhau trong Hình 3. Để tiết kiệm không gian,
chúng tôi chủ yếu trình bày kết quả trên các tác vụ phát hiện
lỗi và sinh assertion. Cụ thể, trục x trình bày số lượng token
(mỗi LLM sử dụng tokenizer liên kết của riêng nó) trong khi
trục y trình bày hiệu suất tương ứng. Nhìn chung, đối với
hầu hết các mô hình, chúng tôi quan sát thấy xu hướng giảm
trong hiệu suất mô hình với độ dài đầu vào tăng, ngụ ý rằng
các mô hình gặp khó khăn trong việc sử dụng thông tin hữu
ích trong đầu vào dài. Kết quả của chúng tôi mở rộng phát
hiện trước đây trên lĩnh vực tổng quát [88] rằng hiệu suất của
các LLM được hướng dẫn cũng giảm với đầu vào dài hơn
trên các tác vụ cụ thể về mã.

Phát hiện 3: Trong thiết lập one-shot, việc thêm ví dụ
đôi khi gây ra hiệu suất không ổn định và thậm chí tệ
hơn của các LLM được điều chỉnh hướng dẫn. Một lý do
tiềm ẩn có thể là khả năng hạn chế của các LLM được
điều chỉnh hướng dẫn hiện có trong việc sử dụng ngữ
cảnh đầu vào dài hơn.

Hình 3. Hiệu suất mô hình với độ dài đầu vào khác nhau

2) Khả năng Tuân theo Hướng dẫn: Chúng tôi tiếp tục so
sánh khả năng mô hình tuân theo hướng dẫn trong thiết lập
zero-shot và one-shot. Mô hình được coi là tuân theo hướng
dẫn đã cho, khi phản hồi của nó tuân thủ yêu cầu trong hướng
dẫn. Ví dụ, trong các tác vụ phân loại (ví dụ: phát hiện bản
sao và phát hiện lỗi), một phản hồi tuân theo hướng dẫn được
cho là trả lời "YES" hoặc "NO"; đối với các tác vụ sinh (ví
dụ: sinh assertion), một phản hồi tuân theo hướng dẫn được
cho là trả về một câu lệnh assertion. Trọng tâm chính là về
định dạng chứ không phải tính đúng đắn của phản hồi mô
hình. Khả năng tuân theo hướng dẫn là một chỉ số thiết yếu
trong tổng quát hóa tác vụ và đã được đánh giá rộng rãi trong
công trình trước [89].

Bảng VII cho thấy khả năng tuân theo hướng dẫn của từng
mô hình trong cả thiết lập zero-shot và one-shot. Thứ nhất,
trong thiết lập zero-shot, chúng tôi thấy rằng hầu hết các LLM
được điều chỉnh hướng dẫn

--- TRANG 8 ---
Bảng VI
HIỆU SUẤT VỚI CÁC CHIẾN LƯỢC LỰA CHỌN ONE-SHOT KHÁC NHAU

LLM DD (%) CD (%) AG (%) CS (%)
FO RO BO FO RO BO FO RO BO FO RO BO

CodeGen-6B 42.3 40.6 43.6 24.3 23.8 23.4 5.5 7.4 56.2 10.0 6.0 13.0
ChatGLM-6B 45.6 50.9 54.2 6.7 11.3 12.8 2.6 3.9 46.2 44.0 35.0 54.0
Vicuna-7B 53.4 53.9 54.1 - - - 8.4 9.5 31.2 65.0 26.0 37.0
Alpaca-7B 46.3 50.1 55.4 - - - 0.0 1.0 41.4 2.0 2.0 6.0
Dolly-7B 49.7 46.0 49.9 16.2 - 23.5 4.7 5.2 51.0 15.0 13.0 14.0
StableLM-7B 44.9 47.4 43.4 - 0.7 - 0.9 1.2 44.4 16.0 8.0 19.0
CodeAlpaca-7B 50.5 51.5 50.3 6.4 18.2 10.3 4.6 3.1 35.1 27.0 26.0 34.0
Dolly-12B 41.3 44.3 52.7 23.6 24.3 22.6 8.8 7.0 51.7 6.0 5.0 8.0
Vicuna-13B 53.9 52.4 53.0 1.4 9.7 6.5 14.1 11.6 44.0 42.0 23.0 24.0
WizardCoder-15B 53.6 53.8 53.8 - - 7.3 25.5 23.1 63.3 72.0 53.0 50.0
Instruct-CodeGen-16B 50.9 50.4 54.6 24.3 23.2 20.7 9.2 9.0 55.0 28.0 29.0 41.0

Bảng VII
KHẢ NĂNG TUÂN THEO HƯỚNG DẪN

LLM DD (%) CD (%) AG (%)
ZO BO ZO BO ZO BO

CodeGen-6B 0.4 77.8 0.3 94.2 0.0 96.6
ChatGLM-6B 16.2 98.2 32.4 95.3 98.7 99.7
Vicuna-7B 100.0 100.0 100.0 100.0 82.8 99.8
Alpaca-7B 99.3 100.0 100.0 100.0 97.5 100.0
Dolly-7B 65.6 95.7 61.0 97.8 13.6 91.3
StableLM-7B 92.5 96.6 90.6 98.2 27.9 99.6
CodeAlpaca-7B 97.4 96.3 100.0 100.0 6.4 94.0
Dolly-12B 65.2 93.8 98.9 100.0 25.0 98.6
Vicuna-13B 92.0 99.2 61.2 100.0 99.4 99.2
WizardCoder-15B 98.7 98.2 100.0 100.0 95.4 99.5
Instruct-CodeGen-16B 97.7 97.6 99.7 97.7 56.0 99.8

thể hiện khả năng tuân theo hướng dẫn cao trên các tác vụ
hiểu và sinh mã; và mô hình không có điều chỉnh hướng dẫn
(ví dụ: CodeGen-6B) thể hiện khả năng tuân theo hướng dẫn
cực kỳ kém. Kết quả xác nhận rằng điều chỉnh hướng dẫn thực
sự cải thiện khả năng tuân theo hướng dẫn của các mô hình.
Thứ hai, trong thiết lập one-shot, chúng tôi thấy rằng việc
thêm một ví dụ minh họa vào đầu vào cải thiện đáng kể khả
năng tuân theo hướng dẫn. Ví dụ, trên phát hiện lỗi, khả năng
tuân theo hướng dẫn của ChatGLM-6B cải thiện từ 16.2%
lên 98.2%. Khả năng tuân theo hướng dẫn tăng từ thiết lập
zero-shot sang thiết lập one-shot cũng có thể là lý do cho sự
cải thiện hiệu suất trong thiết lập one-shot được quan sát
trong Phần IV-B1.

Phát hiện 4: Các LLM được hướng dẫn thể hiện khả
năng tuân theo hướng dẫn hợp lý trên các tác vụ hiểu và
sinh mã trong thiết lập zero-shot, và việc kết hợp một ví
dụ minh họa tiếp tục cải thiện hiệu quả khả năng tuân
theo hướng dẫn của chúng, có thể giúp cải thiện hiệu
suất trên các tác vụ.

3) Hiệu suất LLM với các chiến lược lựa chọn one-shot
khác nhau: Bảng VI cho thấy hiệu suất của các LLM được
điều chỉnh hướng dẫn với các chiến lược lựa chọn one-shot
khác nhau. Như đã đề cập trong Phần III-D, chúng tôi xem
xét ba chiến lược lựa chọn, bao gồm fix one shot (cột "FO"),
randomly-selected one shot (cột "RO"), và BM25-similarity-
based one shot (cột "BO"). Tương tự, "-" chỉ ra rằng LLM
đưa ra dự đoán thiên lệch trên toàn bộ tập testing và do đó
dẫn đến tính toán không hợp lệ của thước đo F1 trên tác vụ
phát hiện bản sao. Ngoài ra, chúng tôi tiếp tục thực hiện Paired
T-Test [90] để kiểm tra xem có sự khác biệt có ý nghĩa thống
kê giữa từng chiến lược ở mức ý nghĩa 0.05 hay không. Bảng
VIII trình bày các p-value giữa từng hai chiến lược, và các
trường hợp có sự khác biệt đáng kể (p-value<0.05) được
đánh dấu.

Bảng VIII
P-VALUE TRONG PAIRED T-TEST Ở MỨC Ý NGHĨA 0.05

Nhóm Kiểm soát DD CD AG CS
FO/BO 0.0571 0.0857 2.5089e-08 0.5661
RO/BO 0.0643 0.5702 1.4141e-08 0.0053
FO/RO 0.3374 0.7275 0.6697 0.0314

Dựa trên Bảng VI và Bảng VIII, chúng tôi có hai quan sát
sau. Thứ nhất, trên các tác vụ sinh (tức là sinh assertion và
tóm tắt mã), chiến lược dựa trên BM25 là chiến lược lựa chọn
shot tốt nhất, tốt hơn đáng kể so với lựa chọn ngẫu nhiên và
lựa chọn cố định. Quan sát như vậy phù hợp với công trình
trước về chiến lược lựa chọn few-shot cho LLM được điều
chỉnh hướng dẫn closed-source Codex [91], cũng thấy rằng
lựa chọn dựa trên BM25 tốt hơn lựa chọn ngẫu nhiên cho
Codex trên các tác vụ sinh assertion và sửa chữa chương trình.
Kết quả của chúng tôi tiếp tục xác nhận sự vượt trội của chiến
lược lựa chọn dựa trên BM25 cho một phổ rộng các LLM
được điều chỉnh hướng dẫn mã nguồn mở trên sinh assertion
và tóm tắt mã. Thứ hai, thú vị là trên các vấn đề phân loại
(tức là phát hiện lỗi và phát hiện bản sao), chúng tôi thấy
rằng chiến lược dựa trên BM25 được sử dụng rộng rãi không
có sự khác biệt đáng kể so với hai chiến lược cơ bản khác.

Phát hiện 5: Trong thiết lập one-shot, chiến lược lựa
chọn dựa trên BM25 thường là tốt nhất trên các vấn đề
sinh như sinh assertion và tóm tắt mã, nhưng không có
sự khác biệt đáng kể so với các chiến lược cơ bản (ngẫu
nhiên hoặc cố định) trên các vấn đề phân loại như phát
hiện lỗi và phát hiện bản sao.

--- TRANG 9 ---
Bảng IX
SO SÁNH HIỆU SUẤT LLM ĐƯỢC FINE-TUNED VỚI ZERO-SHOT VÀ ONE-SHOT (GIÁ TRỊ +/- SO VỚI FT)

LLM DD (%) CD (%) AG (%) CS (%)
FT ZO BO FT ZO BO FT ZO BO FT ZO BO

SOTA Model 58.7 / / 7.4 / / 25.7 / / 24.0 / /
CodeGen-6B 46.3 -46.0 -2.7 70.4 -69.0 -47.0 43.2 -43.2 +13.0 19.0 -19.0 -6.0
ChatGLM-6B 61.0 -53.9 -6.8 96.9 -79.4 -84.1 55.7 -54.0 -9.5 44.0 +1.0 +10.0
Vicuna-7B 54.6 -0.6 -0.5 77.0 -63.8 -77.0 39.8 -29.7 -8.6 45.0 +3.0 -8.0
Alpaca-7B 50.2 -4.4 +5.2 96.6 -74.5 -96.6 54.5 -49.2 -13.1 67.0 -35.0 -6.0
Dolly-7B 53.1 -20.0 -3.2 95.7 -74.4 -72.2 52.0 -50.1 -1.0 57.0 -45.0 -43.0
StableLM-7B 47.2 -2.9 -3.8 69.9 -45.6 -69.9 5.6 -4.5 +38.8 2.0 +28.0 +17.0
CodeAlpaca-7B 55.1 -3.2 -4.8 95.6 -94.2 -85.3 57.8 -53.4 -22.7 58.0 -49.0 -24.0

C. Hiệu suất Fine-tuned (RQ3)

Trong RQ3, chúng tôi điều tra hiệu suất của các LLM được
điều chỉnh hướng dẫn sau khi được fine-tune trên từng tác
vụ. Bảng IX trình bày hiệu suất fine-tuned của các LLM được
hướng dẫn trên từng tác vụ, trong đó cột "FT" trình bày hiệu
suất fine-tuned trong khi các cột "ZO" và "BO" trình bày
hiệu suất one-shot và hiệu suất one-shot so với hiệu suất fine-
tuned. Chúng tôi cũng bao gồm hiệu suất fine-tuning các mô
hình SOTA nhỏ trên từng tác vụ (hàng "SOTA model").

Fine-tuning v.s. Zero/One-shot. Fine-tuning các LLM được
hướng dẫn trên các tác vụ cụ thể tiếp tục cải thiện hiệu suất
mô hình so với thiết lập zero-shot/one-shot. Như được thể
hiện trong bảng, hiệu suất tốt nhất trong từng tác vụ được
đạt bằng cách fine-tune LLM được hướng dẫn trên tác vụ cụ
thể. Công trình trước [92], [93], [8] cũng tiết lộ những quan
sát tương tự rằng fine-tuning hiệu quả tham số có thể vượt
trội hơn học trong ngữ cảnh (tức là học few-shot) trên các
mô hình như GPT-3, phù hợp với kết quả của chúng tôi; nhưng
đánh giá của chúng tôi tập trung vào các LLM được điều chỉnh
hướng dẫn gần đây hơn trong lĩnh vực hiểu và sinh mã.

Bảng X
THAM SỐ CÓ THỂ TRAINING CỦA LLM

Nhóm LLM Tham số Training
LLM được điều chỉnh
hướng dẫn với LoRA

ChatGLM-6B 3M
Vicuna-7B 8M
Alpaca-7B 8M
Dolly-7B 8M
StableLM-7B 6M
CodeAplaca-7B 8M

Mô hình SOTA

CoTexT-220M 220M
CodeGPT-adapted-120M 120M
CodeT5-220M 220M
PLBART-140M 140M

LLM được Hướng dẫn v.s. Mô hình SOTA. Sau khi được
fine-tune trên cùng tập dữ liệu training của từng tác vụ, hầu
hết các LLM được điều chỉnh hướng dẫn vượt trội hơn các
mô hình SOTA nhỏ. Như đã đề cập trong Phần III-D, do hạn
chế tài nguyên, chúng tôi fine-tune từng LLM được điều chỉnh
hướng dẫn với chiến lược điều chỉnh hiệu quả tham số LoRA,
do đó chỉ một số lượng nhỏ tham số sẽ được cập nhật trong
quá trình fine-tuning; đối với các mô hình SOTA nhỏ, phù
hợp với công trình trước [29], tất cả các tham số của chúng
sẽ được cập nhật trong quá trình fine-tuning. Bảng X trình
bày số lượng tham số được cập nhật trong fine-tuning cho
từng mô hình. Thú vị là số lượng tham số được cập nhật trong
các LLM được hướng dẫn thực sự nhỏ hơn nhiều so với các
mô hình SOTA nhỏ (ví dụ: <10M v.s. >100M); tuy nhiên,
các LLM được điều chỉnh hướng dẫn thể hiện hiệu suất cao
hơn nhiều so với các mô hình SOTA sau fine-tuning. Lý do
có thể là LLM được hướng dẫn gốc đã khá mạnh mẽ (như
được xác nhận bởi RQ1 của chúng tôi) và fine-tuning một
số lượng nhỏ tham số là đủ để cải thiện thêm hiệu suất mô
hình trên tác vụ. Ngoài các mô hình SOTA nhỏ, mô hình lớn
mà không có điều chỉnh hướng dẫn (tức là CodeGen-6B) vẫn
hoạt động tệ hơn các mô hình được điều chỉnh hướng dẫn sau
khi cả hai được fine-tune, cho thấy rằng lợi ích từ điều chỉnh
hướng dẫn không thể dễ dàng được giảm thiểu trong giai đoạn
fine-tuning sau đó.

Phát hiện 6: Fine-tuning các LLM được hướng dẫn có
thể cải thiện thêm hiệu suất mô hình trên các tác vụ hiểu
và sinh mã so với hiệu suất zero-shot/one-shot. Bên cạnh
đó, sau khi fine-tune trên cùng tập dữ liệu tác vụ, các
LLM được hướng dẫn vượt trội hơn cả các mô hình SOTA
nhỏ và các LLM có quy mô tương tự mà không có điều
chỉnh hướng dẫn, cho thấy lợi ích lớn của điều chỉnh
hướng dẫn.

Hình 4. Hiệu suất với số epoch fine-tuning tăng

Epoch Fine-tuning. Các kết quả trên dựa trên việc fine-tune
từng mô hình trong năm epoch. Sau đó chúng tôi trình bày
xu hướng hiệu suất của các epoch fine-tuning khác nhau. Để
tiết kiệm không gian, chúng tôi trình bày kết quả của hai LLM
được hướng dẫn, tức là ChatGLM-6B lĩnh vực tổng quát và
CodeAlpaca-7B cụ thể về mã trong Hình 4. Như được thể
hiện trong hình, hiệu suất mô hình đạt đến một ngưỡng sau
ba epoch, cho thấy rằng

--- TRANG 10 ---
các LLM được điều chỉnh hướng dẫn có thể nhanh chóng thích
ứng với các tác vụ với một số lượng ít epoch fine-tuning.

Phát hiện 7: Các LLM được điều chỉnh hướng dẫn có
thể nhanh chóng thích ứng với các tác vụ hiểu và sinh
mã bằng cách được fine-tune với một số lượng ít (ví dụ:
ba) epoch.

D. Chi phí (RQ4)

Trong RQ4, chúng tôi phân tích cả chi phí bộ nhớ và chi
phí thời gian của việc áp dụng các LLM được điều chỉnh
hướng dẫn. Bảng XI trình bày chi phí fine-tuning mô hình
mỗi epoch trên từng tác vụ liên quan đến mã (cột "Tuning")
và chi phí trong quá trình inference zero-shot cho 100 mục
dữ liệu testing (cột "Inference"). Ngoài ra, chúng tôi cũng
trình bày chi phí của các mô hình SOTA nhỏ để so sánh.

Thứ nhất, chúng tôi thấy rằng ngay cả các mô hình có quy
mô tương tự cũng tốn chi phí bộ nhớ và thời gian khác nhau
trong quá trình fine-tuning và inference. Đáng chú ý, ChatGLM-
6B thường là mô hình hiệu quả bộ nhớ nhất trong quá trình
fine-tuning. Về chi phí thời gian, Vicuna-7B và StableLM-
7B là hai mô hình hiệu quả thời gian hàng đầu thường tốn
ít thời gian fine-tuning hơn nhiều so với các LLM được điều
chỉnh hướng dẫn khác. Thứ hai, các mô hình lớn hơn không
nhất thiết tốn nhiều thời gian hơn cho fine-tuning hoặc inference.
Ví dụ, các mô hình lớn hơn như WizardCoder-15B và Vicuna-
13B không nhất thiết tốn nhiều thời gian inference hơn các
mô hình nhỏ hơn như Dolly-7B. Cuối cùng, so với các mô
hình SOTA nhỏ, các LLM được hướng dẫn tốn nhiều tài
nguyên bộ nhớ và thời gian hơn, cho thấy rằng các mô hình
SOTA nhỏ có thể vẫn là lựa chọn ưa thích cho kịch bản nhạy
cảm với độ trễ trực tuyến hoặc tài nguyên hạn chế.

Phát hiện 8: Các LLM được hướng dẫn có quy mô tương
tự khác nhau về chi phí bộ nhớ và thời gian, và các LLM
được hướng dẫn lớn hơn không tốn nhiều thời gian hơn
cho fine-tuning hoặc inference. Tuy nhiên, sự khác biệt
chi phí thời gian và bộ nhớ giữa các LLM được hướng
dẫn và các mô hình SOTA nhỏ là rất lớn và không thể
bỏ qua.

V. Ý NGHĨA

Sau đó chúng tôi tóm tắt các ý nghĩa chính dựa trên các
phát hiện của chúng tôi ở trên. Các LLM được điều chỉnh
hướng dẫn rất hứa hẹn trên hiểu và sinh mã. So với các mô
hình pre-train nhỏ hoặc các mô hình lớn mà không có hướng
dẫn, các LLM được hướng dẫn đạt hiệu suất tốt hơn trong
các kịch bản zero-shot, few-shot, và thậm chí fine-tuning cho
hiểu và sinh mã. Do đó, các LLM được hướng dẫn có thể
được ưa thích trong việc giải quyết các tác vụ liên quan đến
mã nếu tài nguyên cho phép. Mặc dù mục đích chính của
nghiên cứu này không phải là đưa ra xếp hạng cụ thể của
các LLM được hướng dẫn hiện có, chúng tôi khuyến nghị
WizardCoder, Vicuna, và ChatGLM cho các nhà nghiên cứu
và nhà phát triển quan tâm đến việc áp dụng LLM được hướng
dẫn trên các tác vụ liên quan đến mã, do hiệu suất ổn định
tổng thể của chúng trên các tác vụ và thiết lập được nghiên
cứu.

Fine-tuning chủ yếu là giải pháp tốt nhất để có được hiệu
suất tốt nhất của các LLM được hướng dẫn trên hiểu và sinh
mã nếu tài nguyên và dữ liệu cho phép. Mặc dù các LLM
được hướng dẫn có thể đạt hiệu suất chấp nhận được trong
cả thiết lập zero-shot và few-shot, các mô hình đôi khi có thể
thể hiện hành vi không ổn định hoặc bất thường trong quá
trình học trong ngữ cảnh; và fine-tuning chúng cụ thể trên
từng tác vụ liên quan đến mã có thể cải thiện hiệu suất một
cách nhất quán, theo cách khá ổn định. Như được thể hiện
trong kết quả của chúng tôi, với chiến lược điều chỉnh hiệu
quả tham số LoRA, các mô hình được hướng dẫn có thể thích
ứng với tác vụ với chỉ một vài epoch fine-tuning. Do đó,
chúng tôi khuyến nghị fine-tune hiệu quả các LLM được hướng
dẫn nếu có tài nguyên và tập dữ liệu khả dụng.

Các chiến lược lựa chọn shot được khuyến nghị trong thiết
lập few-shot. Mặc dù trong hầu hết các trường hợp việc thêm
các ví dụ minh họa có thể cải thiện thêm khả năng mô hình
trên các tác vụ hiểu và sinh mã, những cải thiện có thể không
ổn định hoặc thậm chí âm đặc biệt đối với các vấn đề phân
loại hoặc với prompt đầu vào dài. Do đó, đối với các vấn đề
sinh, chúng tôi khuyến nghị cung cấp cho các LLM được hướng
dẫn các ví dụ minh họa tương tự (ví dụ: chiến lược lựa chọn
shot dựa trên BM25); đối với các vấn đề phân loại, không
có chiến lược lựa chọn shot nào tốt hơn một cách vượt trội,
và cụ thể, chúng tôi đề xuất cân bằng độ dài đầu vào và việc
bao gồm các ví dụ minh họa vì việc thêm ví dụ có thể có hạn
chế giúp ích nhưng làm dài đầu vào để giảm hiệu suất mô
hình.

Đánh đổi giữa hiệu suất và chi phí. Mặc dù có hiệu suất
tốt hơn trên các tác vụ sinh và hiểu mã, các LLM được hướng
dẫn vẫn tốn nhiều thời gian hơn đáng kể trong điều chỉnh
và inference so với các mô hình SOTA nhỏ. Do đó, đối với
các trường hợp yêu cầu nghiêm ngặt phản hồi trực tuyến ngay
lập tức hoặc với tài nguyên điều chỉnh hạn chế, các mô hình
SOTA nhỏ được fine-tune có thể vẫn là lựa chọn an toàn để
đánh đổi giữa hiệu suất hợp lý và chi phí. Ngoài ra, trong
số các LLM được hướng dẫn, chúng tôi thấy các mô hình
lớn hơn không nhất thiết tốn thời gian dài hơn cho inference;
do đó nói một cách vừa phải, chúng tôi khuyến nghị WizardCoder,
Vicuna, và ChatGLM như những lựa chọn hợp lý với sự đánh
đổi tốt hơn giữa hiệu suất và chi phí.

Các hướng tương lai. Chúng tôi tiếp tục đề xuất các hướng
tương lai sau đây về các LLM được hướng dẫn cho hiểu và
sinh mã. Thứ nhất, chúng tôi đề xuất công trình tương lai về
các phương pháp đánh giá âm thanh và kỹ lưỡng hơn cho
các LLM được hướng dẫn trên các tác vụ liên quan đến mã,
bao gồm triển khai giao thức đánh giá tự động, xây dựng
benchmark liên quan đến mã thực tế, và thiết kế các thước
đo đánh giá âm thanh, có thể có lợi cho lĩnh vực đánh giá
khối lượng lớn các LLM được hướng dẫn một cách có hệ
thống. Thứ hai, chúng tôi đề xuất công trình tương lai về
điều chỉnh và inference hiệu quả hơn của các LLM được hướng
dẫn. Trong khi các LLM được hướng dẫn cho thấy hiệu suất
hứa hẹn và mạnh mẽ trên hiểu và sinh mã, chúng vẫn tốn
nhiều tài nguyên bộ nhớ và thời gian hơn các mô hình pre-
train nhỏ. Do đó, xây dựng các phương pháp điều chỉnh và
inference hiệu quả hơn chắc chắn sẽ đưa các LLM được hướng
dẫn hướng tới việc sử dụng thực tế hơn trong lĩnh vực liên
quan đến mã. Cuối cùng, chúng tôi đề xuất nhiều nỗ lực hơn
trong việc xây dựng các LLM được hướng dẫn hướng tới các
tác vụ hiểu và sinh mã. Do tính đa dạng cao trong các hoạt
động bảo trì và phát triển phần mềm, thực sự có một lượng
lớn các hướng dẫn liên quan đến mã có thể được sử dụng
để điều chỉnh LLM, có thể giúp xây dựng các LLM được
hướng dẫn toàn diện và mạnh mẽ hơn để hỗ trợ phần mềm

--- TRANG 11 ---
Bảng XI
CHI PHÍ THỜI GIAN VÀ BỘ NHỚ CỦA LLM CHO TUNING VÀ INFERENCE TRONG TỪNG TÁC VỤ, ĐƯỢC ĐÁNH GIÁ TRÊN MỘT GPU A800-80G DUY NHẤT.

Mô hình DD Task CD Task AG Task CS Task
Tuning Inference Tuning Inference Tuning Inference Tuning Inference

SOTA Model 2.5 h (15 G) 9.0 s (17 G) 3.5 h (7 G) 1.0 s (4 G) 10.0 h (11 G) 4.5 s (12 G) 13.0 h (43 G) 9.0 s (39 G)
CodeGen-6B 6.0 h (80 G) 7.5 m (35 G) 32.0 h (80 G) 9.0 m (35 G) 28.0 h (77 G) 7.5 m (35 G) 16.0 h (77 G) 8.0 m (35 G)
ChatGLM-6B 7.5 h (48 G) 4.5 m (28 G) 34.0 h (39 G) 4.5 m (28 G) 35.0 h (37 G) 4.5 m (28 G) 24.0 h (34 G) 5.0 m (28 G)
Vicuna-7B 5.0 h (80 G) 6.0 m (24 G) 25.0 h (79 G) 4.5 m (24 G) 27.0 h (77 G) 4.5 m (24 G) 15.0 h (77 G) 5.0 m (24 G)
Alpaca-7B 15.5 h (75 G) 6.0 m (24 G) 60.0 h (80 G) 4.5 m (24 G) 67.0 h (77 G) 6.0 m (24 G) 36.0 h (75 G) 5.0 m (24 G)
Dolly-v2-7B 12.0 h (80 G) 6.0 m (29 G) 54.0 h (72 G) 7.5 m (29 G) 54.0 h (77 G) 6.0 m (29 G) 30.0 h (79 G) 6.0 m (29 G)
StableLM-7B 5.0 h (80 G) 3.0 m (31 G) 23.0 h (71 G) 4.5 m (31 G) 22.0 h (76 G) 4.5 m (31 G) 13.0 h (72 G) 4.0 m (31 G)
CodeAlpaca-7B 16.0 h (75 G) 4.5 m (24 G) 70.0 h (77 G) 4.5 m (24 G) 75.0 h (73 G) 4.5 m (24 G) 52.0 h (79 G) 4.0 m (24 G)
Dolly-v2-12B - 7.5 m (44 G) - 7.5 m (44 G) - 7.5 m (44 G) - 8.0 m (44 G)
Vicuna-13B - 6.0 m (49 G) - 6.0 m (49 G) - 10.5 m (49 G) - 6.0 m (49 G)
WizardCoder-15B - 3.0 m (38 G) - 4.5 m (38 G) - 3.0 m (38 G) - 5.0 m (38 G)
Instruct-CodeGen-16B - 10.5 m (71 G) - 10.5 m (71 G) - 9.0 m (71 G) - 7.0 m (71 G)

các hoạt động kỹ thuật.

VI. CÁC MỐI ĐE DỌA CHO TÍNH HỢP LỆ

Tính hợp lệ của kết quả của chúng tôi có thể bị đe dọa bởi
các vấn đề sau. Thứ nhất, các phát hiện có thể bị hạn chế
bởi prompt được sử dụng trong các thí nghiệm của chúng
tôi. Để xây dựng một prompt hợp lý cho các LLM được điều
chỉnh hướng dẫn được nghiên cứu, chúng tôi chủ yếu tuân
theo thực hành phổ biến trong thiết kế prompt, chẳng hạn như
áp dụng system prompt mặc định nếu được cung cấp bởi chính
từng LLM được điều chỉnh hướng dẫn và tái sử dụng hướng
dẫn tác vụ trước đây được sử dụng trong các tác vụ hiểu và
sinh mã trước đây như đã đề cập trong Phần III-C. Ngoài
ra, chúng tôi tiếp tục thực hiện một nghiên cứu thí điểm quy
mô nhỏ để tinh chỉnh prompt thủ công trên tập dữ liệu quy
mô nhỏ (tức là năm mục dữ liệu không trùng lặp với tập dữ
liệu testing của chúng tôi) để đảm bảo prompt không kích
hoạt hành vi bất thường của các mô hình được nghiên cứu.
Mặc dù vậy, chúng tôi vẫn không thể đảm bảo chúng tôi kết
hợp prompt tối ưu cho từng tác vụ và các phát hiện của chúng
tôi có thể bị thiên lệch bởi prompt được sử dụng trong các
thí nghiệm của chúng tôi. Sẽ là công trình tương lai quan
trọng để khám phá rộng rãi tác động của các prompt khác
nhau. Thứ hai, các phát hiện của chúng tôi có thể bị hạn chế
đối với các tác vụ và tập dữ liệu được sử dụng trong các thí
nghiệm của chúng tôi, và không thể được tổng quát hóa cho
các tác vụ hoặc tập dữ liệu khác. Để giảm thiểu những vấn
đề này, chúng tôi xây dựng các tập dữ liệu bằng cách lấy
mẫu các tập dữ liệu được thiết lập tốt đã được sử dụng trong
công trình trước [29]. Ngoài ra, chúng tôi chọn bốn tác vụ
hiểu và sinh mã đại diện đã được nghiên cứu rộng rãi trong
công trình trước [29], [30], [28], bao gồm cả các vấn đề phân
loại và sinh. Chúng tôi dự định bao gồm nhiều tập dữ liệu
và tác vụ hơn trong công trình tương lai. Thứ ba, việc rò rỉ
dữ liệu tiềm ẩn giữa tập dữ liệu hướng dẫn và tập dữ liệu
testing có thể gây ra thiên lệch trong kết quả của chúng tôi.
Để giảm thiểu vấn đề này, chúng tôi so sánh tập dữ liệu hướng
dẫn của các mô hình được nghiên cứu (nếu được phát hành)
và tập dữ liệu testing và thấy rằng không có hướng dẫn trùng
lặp. Cuối cùng, các thước đo đánh giá cũng có thể đe dọa
tính hợp lệ của kết quả của chúng tôi. Để giảm thiểu vấn đề,
chúng tôi áp dụng các thước đo được sử dụng rộng rãi cho
hầu hết các tác vụ, chẳng hạn như accuracy, F1, và exact
match. Ngoài ra đối với tác vụ có phản hồi mở (tức là tóm
tắt mã), chúng tôi không tuân theo thước đo BLEU có vấn
đề mà áp dụng phương pháp đánh giá gần đây [51], [52],
[53], [54], [55], [56], [57], [58] cho các LLM được điều chỉnh
hướng dẫn bằng cách tận dụng LLM mạnh mẽ ChatGPT làm
người phán xét.

VII. CÔNG TRÌNH LIÊN QUAN

Đánh giá các LLM được điều chỉnh hướng dẫn. Do sự phổ
biến mới nổi của các LLM được điều chỉnh hướng dẫn gần
đây, nhiều nỗ lực [13], [14], [15], [16] đã được dành cho
việc đánh giá các LLM được điều chỉnh hướng dẫn. Như được
tóm tắt bởi khảo sát mới nhất [12], các đánh giá hiện có cho
các mô hình được hướng dẫn đã bao gồm một phạm vi rộng,
bao gồm không chỉ các tác vụ NLP tổng quát [17], [18] (ví
dụ: phân tích tình cảm, phân loại văn bản, và hiểu ngữ nghĩa)
mà còn các lĩnh vực cụ thể [19] (ví dụ: y tế, giáo dục, và ứng
dụng agent). Tuy nhiên, ít có đánh giá nào về các LLM được
hướng dẫn đi sâu vào lĩnh vực kỹ thuật phần mềm, ngoại trừ
tác vụ NL-to-Code [20], [21], [2], [22] (tức là tạo một hàm
cho mô tả ngôn ngữ tự nhiên đã cho), chỉ là một trong những
tác vụ liên quan đến mã trong phát triển và bảo trì phần mềm.
Mặc dù có xu hướng mới nổi trong việc tận dụng các mô hình
được hướng dẫn như ChatGPT và Codex [23], [24], [25],
[26] trên nhiều tác vụ kỹ thuật phần mềm hơn (như sinh test
và sửa chữa chương trình), những mô hình thương mại này
là closed-source, do đó thiếu tính minh bạch và khả năng tái
tạo. Trong công trình này, chúng tôi thực hiện nỗ lực đầu
tiên để đánh giá một phổ rộng các LLM được hướng dẫn mã
nguồn mở trên bốn tác vụ hiểu và sinh mã đại diện.

Đánh giá các mô hình pre-train trên các tác vụ liên quan
đến mã. Do tiến bộ gần đây trong các mô hình pre-train, các
nhà nghiên cứu đã đánh giá rộng rãi hiệu suất của các mô
hình pre-train, fine-tuned và prompted trên các tác vụ liên
quan đến mã [27], [28], [29], [30]. Ví dụ, Zeng et al. [28]
đánh giá tám mô hình pre-train trên bảy tác vụ liên quan đến
mã; gần đây hơn, Niu et al. [29] thực hiện đánh giá toàn
diện trên 19 mô hình pre-train trên 13 tác vụ liên quan đến
mã. Wang et al.[30] so sánh hiệu suất fine-tuned và prompt-
tuned của hai mô hình pre-train trên ba tác vụ liên quan đến
mã. Công trình hiện có chủ yếu tập trung vào các mô hình
pre-train nhỏ mà không có điều chỉnh hướng dẫn (như CodeT5
và CodeBERT), và công trình của chúng tôi đánh giá 10 LLM
được điều chỉnh hướng dẫn mã nguồn mở trên các tác vụ liên
quan đến mã lần đầu tiên.

VIII. KẾT LUẬN

Trong công trình này, chúng tôi thực hiện nghiên cứu có
hệ thống đầu tiên để đánh giá hiệu suất của các LLM được
điều chỉnh hướng dẫn trên các tác vụ hiểu và sinh mã. Các
thí nghiệm của chúng tôi bao gồm 10 LLM được hướng dẫn
mã nguồn mở gần đây với năm mô hình baseline bổ sung trên
bốn tác vụ liên quan đến mã đại diện.

--- TRANG 12 ---
Các phát hiện chính của chúng tôi như sau. Thứ nhất, trong
thiết lập zero-shot, chúng tôi thấy rằng các LLM được hướng
dẫn rất cạnh tranh trên các tác vụ hiểu và sinh mã và đôi khi
thậm chí tốt hơn các mô hình SOTA nhỏ được fine-tune cụ
thể trên từng tác vụ. Thứ hai, trong thiết lập few-shot, chúng
tôi thấy rằng việc thêm các ví dụ minh họa giúp các LLM
được hướng dẫn hoạt động tốt hơn đáng kể trên hầu hết các
tác vụ hiểu và sinh mã. Thứ ba, trong thiết lập fine-tuning,
chúng tôi thấy rằng fine-tuning có thể cải thiện thêm hiệu
suất mô hình trên các tác vụ hiểu và sinh mã so với hiệu
suất zero-shot/one-shot. Dựa trên các phát hiện của chúng
tôi, chúng tôi tiếp tục trình bày các ý nghĩa thực tiễn về khuyến
nghị mô hình và sử dụng, đánh đổi hiệu suất và chi phí, và
các hướng tương lai.

TÀI LIỆU THAM KHẢO
[1] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
Kaplan, và et al. Language models are few-shot learners. In Advances
in Neural Information Processing Systems 33: Annual Conference on
Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual , 2020.
[2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, và
et al. Llama: Open and efficient foundation language models. CoRR ,
abs/2302.13971, 2023.
[3] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, và et al.
BLOOM: A 176b-parameter open-access multilingual language model.
CoRR , abs/2211.05100, 2022.
[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma,
Gaurav Mishra, Adam Roberts, và et al. Palm: Scaling language
modeling with pathways. CoRR , abs/2204.02311, 2022.
[5] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao
Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E.
Gonzalez, Ion Stoica, và Eric P. Xing. Vicuna: An open-source chatbot
impressing gpt-4 with 90%* chatgpt quality, March 2023.
[6] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen
Li, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto. Stan-
ford alpaca: An instruction-following llama model. https://github.com/
tatsu-lab/stanford alpaca, 2023.
[7] Sahil Chaudhary. Code alpaca: An instruction-following llama model
for code generation. https://github.com/sahil280114/codealpaca, 2023.
[8] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang
Sutawika, Zaid Alyafeai, và et al. Multitask prompted training enables
zero-shot task generalization. In The Tenth International Conference on
Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 .
OpenReview.net, 2022.
[9] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, và et al. Training
language models to follow instructions with human feedback. In
NeurIPS , 2022.
[10] Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin Guu, Adams Wei
Yu, Brian Lester, Nan Du, Andrew M. Dai, và Quoc V . Le. Finetuned
language models are zero-shot learners. In The Tenth International
Conference on Learning Representations, ICLR 2022, Virtual Event,
April 25-29, 2022 . OpenReview.net, 2022.
[11] ChatGPT . https://openai.com/blog/chatgpt.
[12] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, và
et al. A survey on evaluation of large language models. CoRR ,
abs/2307.03109, 2023.
[13] Yew Ken Chia, Pengfei Hong, Lidong Bing, và Soujanya Poria.
INSTRUCTEV AL: towards holistic evaluation of instruction-tuned large
language models. CoRR , abs/2306.04757, 2023.
[14] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, và
et al. A survey of large language models. CoRR , abs/2303.18223, 2023.
[15] Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang,
Baochang Ma, và Xiangang Li. Exploring the impact of instruction data
scaling on large language models: An empirical study on real-world use
cases. CoRR , abs/2303.14742, 2023.

[16] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
William Fedus, Eric Li, Xuezhi Wang, và et al. Scaling instruction-
finetuned language models. CoRR , abs/2210.11416, 2022.
[17] Vipul Raheja, Dhruv Kumar, Ryan Koo, và Dongyeop Kang. Coedit:
Text editing by task-specific instruction tuning. CoRR , abs/2305.09857,
2023.
[18] Tuhin Chakrabarty, Vishakh Padmakumar, và He He. Help me write a
poem - instruction tuning as a vehicle for collaborative poetry writing.
InProceedings of the 2022 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates,
December 7-11, 2022 , pages 6848–6863. Association for Computational
Linguistics, 2022.
[19] Yanis Labrak, Mickael Rouvier, và Richard Dufour. A zero-shot and
few-shot study of instruction-finetuned large language models applied
to clinical and biomedical tasks. CoRR , abs/2307.12114, 2023.
[20] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pond ´e
de Oliveira Pinto, và et al. Evaluating large language models trained
on code. CoRR , abs/2107.03374, 2021.
[21] Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma,
Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael
Terry, Quoc V . Le, và Charles Sutton. Program synthesis with large
language models. CoRR , abs/2108.07732, 2021.
[22] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, và et al. Wizardcoder:
Empowering code large language models with evol-instruct. CoRR ,
abs/2306.08568, 2023.
[23] Caroline Lemieux, Jeevana Priya Inala, Shuvendu K. Lahiri, và Sid-
dhartha Sen. Codamosa: Escaping coverage plateaus in test generation
with pre-trained large language models. In 45th IEEE/ACM International
Conference on Software Engineering, ICSE 2023, Melbourne, Australia,
May 14-20, 2023 , pages 919–931. IEEE, 2023.
[24] Vasudev Vikram, Caroline Lemieux, và Rohan Padhye. Can large lan-
guage models write good property-based tests? CoRR , abs/2307.04346,
2023.
[25] Sungmin Kang, Juyeon Yoon, và Shin Yoo. Large language models are
few-shot testers: Exploring llm-based general bug reproduction. In 45th
IEEE/ACM International Conference on Software Engineering, ICSE
2023, Melbourne, Australia, May 14-20, 2023 , pages 2312–2323. IEEE,
2023.
[26] Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu,
Bei Guan, Yongji Wang, và Jian-Guang Lou. Large language models
meet nl2code: A survey. In Anna Rogers, Jordan L. Boyd-Graber,
và Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long
Papers), ACL 2023, Toronto, Canada, July 9-14, 2023 , pages 7443–
7464. Association for Computational Linguistics, 2023.
[27] Rosalia Tufano, Luca Pascarella, và Gabriele Bavota. Automating code-
related tasks through transformers: The impact of pre-training. In 45th
IEEE/ACM International Conference on Software Engineering, ICSE
2023, Melbourne, Australia, May 14-20, 2023 , pages 2425–2437. IEEE,
2023.
[28] Zhengran Zeng, Hanzhuo Tan, Haotian Zhang, Jing Li, Yuqun Zhang,
và Lingming Zhang. An extensive study on pre-trained models for
program understanding and generation. In Sukyoung Ryu và Yannis
Smaragdakis, editors, ISSTA '22: 31st ACM SIGSOFT International
Symposium on Software Testing and Analysis, Virtual Event, South
Korea, July 18 - 22, 2022 , pages 39–51. ACM, 2022.
[29] Changan Niu, Chuanyi Li, Vincent Ng, Dongxiao Chen, Jidong Ge, và
Bin Luo. An empirical comparison of pre-trained models of source code.
In45th IEEE/ACM International Conference on Software Engineering,
ICSE 2023, Melbourne, Australia, May 14-20, 2023 , pages 2136–2148.
IEEE, 2023.
[30] Chaozheng Wang, Yuanhang Yang, Cuiyun Gao, Yun Peng, Hongyu
Zhang, và Michael R. Lyu. No more fine-tuning? an experimental eval-
uation of prompt tuning in code intelligence. In Abhik Roychoudhury,
Cristian Cadar, và Miryung Kim, editors, Proceedings of the 30th ACM
Joint European Software Engineering Conference and Symposium on
the Foundations of Software Engineering, ESEC/FSE 2022, Singapore,
Singapore, November 14-18, 2022 , pages 382–394. ACM, 2022.
[31] Yue Wang, Weishi Wang, Shafiq R. Joty, và Steven C. H. Hoi. Codet5:
Identifier-aware unified pre-trained encoder-decoder models for code
understanding and generation. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Processing, EMNLP 2021,
Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021 ,
pages 8696–8708. Association for Computational Linguistics, 2021.

--- TRANG 13 ---
[32] Zhangyin Feng, Daya Guo, Duyu Tang, và et al. Nan Duan. Codebert:
A pre-trained model for programming and natural languages. In Findings
of the Association for Computational Linguistics: EMNLP 2020, Online
Event, 16-20 November 2020 , volume EMNLP 2020 of Findings of ACL ,
pages 1536–1547. Association for Computational Linguistics, 2020.
[33] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley,
Kyle O'Brien, và et al. Pythia: A suite for analyzing large language
models across training and scaling. CoRR , abs/2304.01373, 2023.
[34] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, và et al. GLM-
130B: an open bilingual pre-trained model. In The Eleventh Inter-
national Conference on Learning Representations, ICLR 2023, Kigali,
Rwanda, May 1-5, 2023 . OpenReview.net, 2023.
[35] CodeGPT-adapted . https://huggingface.co/microsoft/
CodeGPT-small-java-adaptedGPT2.
[36] Long N. Phan, Hieu Tran, Daniel Le, Hieu Nguyen, James T. Anibal,
Alec Peltekian, và Yanfang Ye. Cotext: Multi-task learning with code-
text transformer. CoRR , abs/2105.08645, 2021.
[37] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, và Kai-Wei
Chang. Unified pre-training for program understanding and generation.
InProceedings of the 2021 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2021, Online, June 6-11, 2021 , pages 2655–
2668. Association for Computational Linguistics, 2021.
[38] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, và
et al. Codegen: An open large language model for code with multi-
turn program synthesis. In The Eleventh International Conference on
Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 .
OpenReview.net, 2023.
[39] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
Ichter, Fei Xia, Ed H. Chi, Quoc V . Le, và Denny Zhou. Chain-
of-thought prompting elicits reasoning in large language models. In
NeurIPS , 2022.
[40] Murray Shanahan. Talking about large language models. CoRR ,
abs/2212.03551, 2022.
[41] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena
Buchatskaya, Trevor Cai, Eliza Rutherford, và et al. Training compute-
optimal large language models. CoRR , abs/2203.15556, 2022.
[42] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, An-
thony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, và
Robert Stojnic. Galactica: A large language model for science. CoRR ,
abs/2211.09085, 2022.
[43] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone,
Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, và Syl-
vain Gelly. Parameter-efficient transfer learning for NLP. In Proceedings
of the 36th International Conference on Machine Learning, ICML
2019, 9-15 June 2019, Long Beach, California, USA , volume 97 of
Proceedings of Machine Learning Research , pages 2790–2799. PMLR,
2019.
[44] Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, và Jie
Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning
universally across scales and tasks. CoRR , abs/2110.07602, 2021.
[45] Brian Lester, Rami Al-Rfou, và Noah Constant. The power of scale
for parameter-efficient prompt tuning. In Proceedings of the 2021
Conference on Empirical Methods in Natural Language Processing,
EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-
11 November, 2021 , pages 3045–3059. Association for Computational
Linguistics, 2021.
[46] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, và et al.
Lora: Low-rank adaptation of large language models. In The Tenth
International Conference on Learning Representations, ICLR 2022,
Virtual Event, April 25-29, 2022 . OpenReview.net, 2022.
[47] Changan Niu, Chuanyi Li, Bin Luo, và Vincent Ng. Deep learning
meets software engineering: A survey on pre-trained models of source
code. In Luc De Raedt, editor, Proceedings of the Thirty-First Interna-
tional Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna,
Austria, 23-29 July 2022 , pages 5546–5555. ijcai.org, 2022.
[48] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, và et al. Codexglue:
A machine learning benchmark dataset for code understanding and
generation. In Proceedings of the Neural Information Processing
Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and
Benchmarks 2021, December 2021, virtual , 2021.
[49] F1 Metric . https://www.v7labs.com/blog/f1-score-guide.
[50] Devjeet Roy, Sarah Fakhoury, và Venera Arnaoudova. Reassessing au-
tomatic evaluation metrics for code summarization tasks. In ESEC/FSE'21: 29th ACM Joint European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, Athens, Greece,
August 23-28, 2021 , pages 1105–1116. ACM, 2021.
[51] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhang-
hao Wu, và et al. Judging llm-as-a-judge with mt-bench and chatbot
arena. CoRR , abs/2306.05685, 2023.
[52] Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu,
Pieter Abbeel, Sergey Levine, và Dawn Song. The false promise of
imitating proprietary llms. CoRR , abs/2305.15717, 2023.
[53] Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao,
Qi Liu, Tianyu Liu, và Zhifang Sui. Large language models are not
fair evaluators. CoRR , abs/2305.17926, 2023.
[54] S ´ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
Gehrke, Eric Horvitz, Ece Kamar, và et al. Sparks of artificial general
intelligence: Early experiments with GPT-4. CoRR , abs/2303.12712,
2023.
[55] David Cheng-Han Chiang và Hung-yi Lee. Can large language models
be an alternative to human evaluations? In Anna Rogers, Jordan L. Boyd-
Graber, và Naoaki Okazaki, editors, Proceedings of the 61st Annual
Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023 , pages
15607–15631. Association for Computational Linguistics, 2023.
[56] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, và Luke Zettlemoyer.
Qlora: Efficient finetuning of quantized llms. CoRR , abs/2305.14314,
2023.
[57] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani,
Jimmy Ba, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto.
Alpacafarm: A simulation framework for methods that learn from human
feedback. CoRR , abs/2305.14387, 2023.
[58] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, và Jianfeng
Gao. Instruction tuning with GPT-4. CoRR , abs/2304.03277, 2023.
[59] Yaqin Zhou, Shangqing Liu, Jing Kai Siow, Xiaoning Du, và Yang Liu.
Devign: Effective vulnerability identification by learning comprehensive
program semantics via graph neural networks. In Advances in Neural
Information Processing Systems 32: Annual Conference on Neural
Information Processing Systems 2019, NeurIPS 2019, December 8-14,
2019, Vancouver, BC, Canada , pages 10197–10207, 2019.
[60] Jeffrey Svajlenko, Judith F. Islam, Iman Keivanloo, Chanchal Kumar
Roy, và Mohammad Mamun Mia. Towards a big data curated
benchmark of inter-project code clones. In 30th IEEE International Con-
ference on Software Maintenance and Evolution, Victoria, BC, Canada,
September 29 - October 3, 2014 , pages 476–480. IEEE Computer
Society, 2014.
[61] Cody Watson, Michele Tufano, Kevin Moran, Gabriele Bavota, và
Denys Poshyvanyk. On learning meaningful assert statements for unit
test cases. In ICSE '20: 42nd International Conference on Software
Engineering, Seoul, South Korea, 27 June - 19 July, 2020 , pages 1398–
1409. ACM, 2020.
[62] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, và
Marc Brockschmidt. Codesearchnet challenge: Evaluating the state of
semantic code search. CoRR , abs/1909.09436, 2019.
[63] ChatGLM . https://github.com/THUDM/ChatGLM-6B.
[64] Chinese and English corpus . https://github.com/THUDM/ChatGLM-6B/
blob/main/README en.md.
[65] Alpaca Data . https://github.com/tatsu-lab/stanford alpaca/blob/main/
alpaca data.json.
[66] ShareGPT . https://sharegpt.com/.
[67] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam
Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, và Reynold Xin.
Free dolly: Introducing the world's first truly open instruction-tuned llm,
2023.
[68] databricks-dolly-15k . https://huggingface.co/datasets/databricks/
databricks-dolly-15k.
[69] StableLM-tuned-Alpha-7B . https://huggingface.co/stabilityai/
stablelm-tuned-alpha-7b.
[70] StableLM-base-Alpha-7B . https://huggingface.co/stabilityai/
stablelm-base-alpha-7b.
[71] Five conversational dataset . https://huggingface.co/stabilityai/
stablelm-tuned-alpha-7b.
[72] CodeAlpaca Data . https://github.com/sahil280114/codealpaca/blob/
master/data/code alpaca 20k.json.
[73] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis
Kocetkov, và et al. Starcoder: may the source be with you! CoRR ,
abs/2305.06161, 2023.

--- TRANG 14 ---
[74] CodeAlpaca with Evol-Instruct . https://github.com/nlpxucan/WizardLM/
tree/main/WizardCoder.
[75] Instruct-CodeGen-16B . https://huggingface.co/sahil2801/
instruct-codegen-16B.
[76] Code-related instructions . https://huggingface.co/datasets/sahil2801/
code instructions 120k.
[77] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessan-
dro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet,
Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune,
Baptiste Pannier, và Guilherme Penedo. Falcon-40B: an open large
language model with state-of-the-art performance. 2023.
[78] Xin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao Wan, Xiao Liu,
Li Li, Hao Wu, Jin Liu, và Xin Jiang. Syncobert: Syntax-guided multi-
modal contrastive pre-training for code representation, 2021.
[79] Available data . https://anonymous.4open.science/r/An Empirical
Study ofInstructionTuned LLM/README.md.
[80] Noor Nashid, Mifta Sintaha, và Ali Mesbah. Retrieval-based prompt
selection for code-related few-shot learning. In 45th IEEE/ACM Inter-
national Conference on Software Engineering, ICSE 2023, Melbourne,
Australia, May 14-20, 2023 , pages 2450–2462. IEEE, 2023.
[81] Stephen E. Robertson và Hugo Zaragoza. The probabilistic relevance
framework: BM25 and beyond. Found. Trends Inf. Retr. , 3(4):333–389,
2009.
[82] Ren ´e Peinl và Johannes Wirth. Evaluation of medium-large language
models at zero-shot closed book generative question answering. CoRR ,
abs/2305.11991, 2023.
[83] Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Dong-Hyun
Kwak, Kang Min Yoo, và Minjoon Seo. Aligning large language
models through synthetic feedback. CoRR , abs/2305.13735, 2023.
[84] Andreas K ¨opf, Yannic Kilcher, Dimitri von R ¨utte, Sotiris Anagnos-
tidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh
Duc, Oliver Stanley, Rich ´ard Nagyfi, Shahul ES, Sameer Suri, David
Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann,
Huu Nguyen, và Alexander Mattick. Openassistant conversations -
democratizing large language model alignment. CoRR , abs/2304.07327,
2023.

[85] Seonghyeon Ye, Hyeonbin Hwang, Sohee Yang, Hyeongu Yun, Yireun
Kim, và Minjoon Seo. In-context instruction learning. CoRR ,
abs/2302.14691, 2023.
[86] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis,
Hannaneh Hajishirzi, và Luke Zettlemoyer. Rethinking the role of
demonstrations: What makes in-context learning work? In Proceedings
of the 2022 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December
7-11, 2022 , pages 11048–11064. Association for Computational Linguis-
tics, 2022.
[87] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, và Sameer Singh.
Calibrate before use: Improving few-shot performance of language
models. In Proceedings of the 38th International Conference on Machine
Learning, ICML 2021, 18-24 July 2021, Virtual Event , volume 139
ofProceedings of Machine Learning Research , pages 12697–12706.
PMLR, 2021.
[88] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele
Bevilacqua, Fabio Petroni, và Percy Liang. Lost in the middle: How
language models use long contexts. CoRR , abs/2307.03172, 2023.
[89] Shiyang Li, Jun Yan, Hai Wang, Zheng Tang, Xiang Ren, Vijay
Srinivasan, và Hongxia Jin. Instruction-following evaluation through
verbalizer manipulation. CoRR , abs/2307.10558, 2023.
[90] Paired t-test . https://statisticsbyjim.com/hypothesis-testing/
paired-t-test/.
[91] Codex . https://openai.com/blog/openai-codex.
[92] Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow,
và Yanai Elazar. Few-shot fine-tuning vs. in-context learning: A
fair comparison and evaluation. In Findings of the Association for
Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14,
2023 , pages 12284–12314. Association for Computational Linguistics,
2023.
[93] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao
Huang, Mohit Bansal, và Colin Raffel. Few-shot parameter-efficient
fine-tuning is better and cheaper than in-context learning. In NeurIPS ,
2022.
