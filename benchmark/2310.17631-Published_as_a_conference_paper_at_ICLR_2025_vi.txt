# 2310.17631.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/benchmark/2310.17631.pdf
# Kích thước tệp: 2919221 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025
JUDGE LM: CÁC MÔ HÌNH NGÔN NGỮ LỚN ĐƯỢC TINH CHỈNH
LÀ CÁC THẨM PHÁN CÓ THỂ MỞ RỘNG
Lianghui Zhu1,2∗Xinggang Wang1†Xinlong Wang2†
1Trường EIC, Đại học Khoa học và Công nghệ Hoa Trung
2Viện Hàn lâm Trí tuệ Nhân tạo Bắc Kinh
Mã nguồn & Mô hình: https://github.com/baaivision/JudgeLM
TÓM TẮT
Đánh giá các Mô hình Ngôn ngữ Lớn (LLMs) trong các tình huống mở là thách thức 
vì các tiêu chuẩn và chỉ số hiện có không thể đo lường chúng một cách toàn diện. 
Để giải quyết vấn đề này, chúng tôi đề xuất tinh chỉnh các LLMs như những thẩm 
phán có thể mở rộng (JudgeLM) để đánh giá LLMs một cách hiệu quả và hiệu quả 
trong các tiêu chuẩn mở. Trước tiên, chúng tôi đề xuất một bộ dữ liệu toàn diện, 
quy mô lớn, chất lượng cao chứa các hạt giống nhiệm vụ, câu trả lời được tạo bởi 
LLMs, và các phán quyết được tạo bởi GPT-4 để tinh chỉnh các thẩm phán hiệu suất 
cao, cũng như một tiêu chuẩn mới để đánh giá các thẩm phán. Chúng tôi huấn luyện 
JudgeLM ở các quy mô khác nhau từ 7B, 13B, đến 33B tham số, và tiến hành phân 
tích hệ thống về khả năng và hành vi của nó. Sau đó, chúng tôi phân tích các thiên 
lệch chính trong việc tinh chỉnh LLM làm thẩm phán và xem xét chúng như thiên lệch 
vị trí, thiên lệch kiến thức, và thiên lệch định dạng. Để giải quyết những vấn đề 
này, JudgeLM giới thiệu một túi kỹ thuật bao gồm tăng cường hoán đổi, hỗ trợ tham 
chiếu, và loại bỏ tham chiếu, điều này rõ ràng nâng cao hiệu suất của thẩm phán. 
JudgeLM đạt được hiệu suất thẩm phán tốt nhất hiện tại trên cả tiêu chuẩn PandaLM 
hiện có và tiêu chuẩn mới được đề xuất của chúng tôi. JudgeLM của chúng tôi hiệu 
quả và JudgeLM-7B chỉ cần 3 phút để phán xét 5K mẫu với 8 GPU A100. JudgeLM đạt 
được sự đồng thuận cao với thẩm phán giáo viên, đạt sự đồng thuận vượt quá 90% 
thậm chí còn vượt qua sự đồng thuận giữa người với người1. JudgeLM cũng thể hiện 
khả năng mở rộng trong việc là thẩm phán của câu trả lời đơn, mô hình đa phương 
thức, nhiều câu trả lời, trò chuyện nhiều lượt, v.v.

1 GIỚI THIỆU
Những tiến bộ gần đây trong các mô hình ngôn ngữ lớn (LLMs) đã thúc đẩy sự quan tâm đáng kể 
do hiệu suất đáng chú ý của chúng trong việc tuân theo hướng dẫn và khả năng rộng rãi trong việc 
xử lý các tình huống mở. Dựa trên các LLMs mã nguồn mở, bao gồm OPT (Zhang et al., 2022), 
Flan-T5 (Chung et al., 2022), LLaMA (Touvron et al., 2023a), và Pythia (Biderman et al., 2023), 
các nhà nghiên cứu đề xuất nhiều phương pháp để căn chỉnh các mô hình này với sở thích của con 
người thông qua việc tinh chỉnh hướng dẫn. Những LLMs được căn chỉnh này thể hiện khả năng 
nâng cao trong việc hiểu hướng dẫn của con người và tạo ra các phản hồi mạch lạc hơn. Tuy nhiên, 
các tiêu chuẩn hiện có (Hendrycks et al., 2020; Liang et al., 2022) và các chỉ số truyền thống 
(Lin, 2004; Papineni et al., 2002; Zhang et al., 2019; Sellam et al., 2020; Yuan et al., 2021) 
không ước tính đầy đủ khả năng của LLMs trong các tình huống mở. Do đó, một phương pháp tiêu 
chuẩn mới có thể đánh giá LLMs một cách toàn diện trong các nhiệm vụ mở là cần thiết.

Các công trình đồng thời đang nỗ lực khám phá các phương pháp khác nhau để đánh giá hiệu suất 
của LLM. Các phương pháp định dạng đấu trường (Zheng et al., 2023) tận dụng các nền tảng cộng 
đồng để trích xuất kết quả cạnh tranh LLM ẩn danh. Trong khi các đánh giá bởi con người đáng tin 
cậy, chúng cũng tốn thời gian và đòi hỏi tài chính. Một số cách tiếp cận (Chiang et al., 2023) 
sử dụng GPT-4 làm thẩm phán. Tuy nhiên, những phương pháp này vật lộn với những thách thức về 
khả năng tiếp xúc dữ liệu tiềm năng và chuyển đổi mô hình API không ổn định, có thể ảnh hưởng 
đến khả năng tái tạo của thẩm phán. PandaLM (Wang et al., 2023) cố gắng tinh chỉnh LLMs mã 
nguồn mở để đánh giá câu trả lời. Tuy nhiên, những hạn chế xuất phát từ chất lượng dữ liệu huấn 
luyện và các thiên lệch vốn có của LLM, làm suy giảm hiệu quả của các mô hình được tinh chỉnh 
như vậy trong vai trò của một thẩm phán.

∗Công việc này được thực hiện khi Lianghui Zhu là thực tập sinh tại Viện Hàn lâm Trí tuệ Nhân tạo Bắc Kinh.
†Tác giả liên hệ: xgwang@hust.edu.cn và wangxinlong@baai.ac.cn.
1Như một tham chiếu, sự đồng thuận tối đa giữa con người trong MT-bench (Zheng et al., 2023) là 82%.

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

GPT-4 làm thẩm phán. Tuy nhiên, những phương pháp này gặp phải những thách thức về khả năng 
tiếp xúc dữ liệu tiềm năng và chuyển đổi mô hình API không ổn định, có thể ảnh hưởng đến khả 
năng tái tạo của thẩm phán. PandaLM (Wang et al., 2023) cố gắng tinh chỉnh các LLMs mã nguồn 
mở để đánh giá câu trả lời. Tuy nhiên, những hạn chế xuất phát từ chất lượng dữ liệu huấn luyện 
và các thiên lệch vốn có của LLM, làm suy giảm hiệu quả của các mô hình được tinh chỉnh như vậy 
trong vai trò của một thẩm phán.

[Hình ảnh về pipeline tạo dữ liệu và quá trình tinh chỉnh JudgeLM]

Trong bài báo này, chúng tôi đề xuất đánh giá LLMs thông qua các LLMs mã nguồn mở được tinh 
chỉnh, phục vụ như những thẩm phán có thể mở rộng (JudgeLM) đạt được sự đồng thuận thỏa đáng 
với thẩm phán giáo viên. Phương pháp luận của chúng tôi kết hợp các thẩm phán có thể mở rộng 
làm người đánh giá trong các nhiệm vụ mở, cùng với một bộ dữ liệu chất lượng cao có lợi cho cả 
việc huấn luyện và đánh giá các mô hình thẩm phán. Trong khuôn khổ của chúng tôi, chúng tôi điều 
chỉnh các LLMs mã nguồn mở để phục vụ như thẩm phán và phân tích khả năng mở rộng của chúng 
liên quan đến kích thước mô hình (từ 7B đến 33B) và khối lượng dữ liệu huấn luyện (mở rộng từ 
3.5K đến 100K). Bộ dữ liệu được tuyển chọn của chúng tôi bao gồm 105K câu hỏi hạt giống, các 
cặp câu trả lời LLM, và các phán quyết từ thẩm phán giáo viên, GPT-4, như được hiển thị trong 
Hình 1a. Lưu ý rằng chúng tôi đã tạo hai phán quyết cho mỗi nhiệm vụ hạt giống có và không có 
câu trả lời tham chiếu. Bộ dữ liệu này được phân chia, với 100K câu hỏi hạt giống được phân bổ 
cho huấn luyện (lớn gấp 2 lần so với PandaLM) và phần còn lại cho xác thực (lớn gấp 29 lần so 
với PandaLM).

Sử dụng LLMs làm thẩm phán không tránh khỏi việc đưa ra các thiên lệch như thiên lệch vị trí 
(ưu tiên câu trả lời ở các vị trí cụ thể), thiên lệch kiến thức (phụ thuộc quá mức vào kiến thức 
được huấn luyện trước), và thiên lệch định dạng (hiệu suất tối ưu chỉ dưới các định dạng prompt 
cụ thể) như được hiển thị trong Hình 8, 10, 12, 13. Khi việc tinh chỉnh không khả thi, thẩm 
phán dựa trên GPT-4-API (Zheng et al., 2023) cố gắng giảm thiểu điều này bằng các phương pháp 
prompt được thiết kế tốt, tức là Chuỗi suy nghĩ, thẩm phán few-shot, và phán quyết nhiều lần 
với các vị trí khác nhau. JudgeLM trình bày một cách mới có thể giải quyết những thiên lệch này 
trong giai đoạn tinh chỉnh, bỏ qua các phương pháp prompt phức tạp và gọi API nhiều lượt. Hơn 
nữa, hệ thống JudgeLM của chúng tôi trình bày các khả năng mở rộng như được hiển thị trong Hình 
1b, bao gồm chấm điểm câu trả lời đơn, phán quyết nhiều câu trả lời, phán quyết mô hình đa phương 
thức, trò chuyện nhiều lượt, v.v.

Ngược lại với các phương pháp định dạng đấu trường, cách tiếp cận của chúng tôi nhanh chóng và 
có chi phí thấp. Ví dụ, JudgeLM-7B chỉ yêu cầu 8 GPU A100 và có thể đánh giá 5000 cặp phản 
hồi chỉ trong 3 phút. So với các thẩm phán LLM nguồn đóng, JudgeLM đảm bảo khả năng tái tạo 
và bảo vệ quyền riêng tư người dùng. Khi so sánh với các thẩm phán LLM mã nguồn mở đồng thời, 
hệ thống của chúng tôi khám phá cả khả năng mở rộng và thiên lệch trong việc tinh chỉnh LLM. 
Hơn nữa, bộ dữ liệu JudgeLM đứng như một trong những bộ dữ liệu đa dạng và chất lượng cao nhất, 
mang lại lợi ích đáng kể cho nghiên cứu tiếp theo trong các cuộc điều tra mô hình thẩm phán.

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

Những đóng góp chính của chúng tôi có thể được tóm tắt như sau:
• Chúng tôi giới thiệu một bộ dữ liệu chất lượng cao, quy mô lớn cho các mô hình thẩm phán, 
được làm giàu với các nhiệm vụ hạt giống đa dạng, câu trả lời được tạo bởi LLMs, và các phán 
quyết chi tiết từ GPT-4, đặt nền tảng cho nghiên cứu đánh giá LLMs trong tương lai.
• Chúng tôi đề xuất JudgeLM, một thẩm phán mô hình ngôn ngữ có thể mở rộng, được thiết kế 
để đánh giá LLMs trong các tình huống mở. Nó đạt được sự đồng thuận vượt quá 90% vượt qua 
sự đồng thuận giữa người với người. JudgeLM của chúng tôi cũng có thể khái quát hóa cho nhiều 
nhiệm vụ mở rộng.
• Chúng tôi phân tích các thiên lệch vốn có trong việc tinh chỉnh thẩm phán LLM và giới thiệu 
một loạt phương pháp để giải quyết chúng. Các phương pháp của chúng tôi cải thiện đáng kể tính 
nhất quán của mô hình trong các trường hợp khác nhau, làm cho JudgeLM đáng tin cậy và linh hoạt 
hơn.

2 CÔNG TRÌNH LIÊN QUAN

2.1 TINH CHỈNH HƯỚNG DẪN CỦA CÁC MÔ HÌNH NGÔN NGỮ LỚN

Với sự phát triển của các mô hình ngôn ngữ lớn (LLMs), các nhà nghiên cứu phát hiện rằng việc 
tinh chỉnh các LLMs được huấn luyện trước như GPT-3 (Brown et al., 2020), T5 (Raffel et al., 
2020), OPT (Zhang et al., 2022), và PaLM (Chowdhery et al., 2022) cho phép LLMs tuân theo 
hướng dẫn của con người và giúp với các nhiệm vụ mở. Các LLMs được tinh chỉnh hướng dẫn như 
InstructGPT (Ouyang et al., 2022), ChatGPT (OpenAI, 2022), FLAN-T5 (Chung et al., 2022), 
FLAN-PaLM (Chung et al., 2022), OPT-IML (Iyer et al., 2022), và GPT-4 (OpenAI, 2023) thể hiện 
khả năng mạnh hơn trong các nhiệm vụ zero-shot hoặc few-shot so với các mô hình cơ sở của chúng. 
Sau khi Meta phát hành LLM mã nguồn mở mạnh mẽ LLaMA (Touvron et al., 2023a) và LLaMA2 
(Touvron et al., 2023b), nhiều công trình tinh chỉnh hướng dẫn dựa trên LLaMA hoặc LLaMA2 đã 
được đề xuất trong lĩnh vực tạo ngôn ngữ tự nhiên hoặc tạo đa phương thức, như Alpaca, Vicuna 
(Chiang et al., 2023), OpenFlamingo (Awadalla et al., 2023), LLaMA-Adapter (Zhang et al., 
2023), và Emu (Sun et al., 2023). JudgeLM của chúng tôi cũng thuộc họ LLaMA và lấy dòng Vicuna 
làm mô hình cơ sở. JudgeLM của chúng tôi tuân theo cách tinh chỉnh hướng dẫn để tạo ra các thẩm 
phán LLM và đề xuất mô hình hóa nhiệm vụ tạo phán quyết như "chấm điểm, phán quyết, và lý luận". 
Chúng tôi tiếp tục thu thập một bộ dữ liệu chất lượng cao, quy mô lớn cho nghiên cứu về việc phán 
quyết hiệu suất của LLMs.

2.2 ĐÁNH GIÁ CÁC MÔ HÌNH NGÔN NGỮ LỚN

Khi nhiều mô hình ngôn ngữ lớn mã nguồn mở (LLMs) và các biến thể được tinh chỉnh của chúng 
được đề xuất và thể hiện hiệu suất đáng chú ý trên các nhiệm vụ khác nhau, việc đánh giá khả năng 
của LLMs trở thành một nhiệm vụ phổ biến và thách thức. Để giải quyết vấn đề này, Chatbot Arena 
(Zheng et al., 2023) nhằm mục đích xây dựng một nền tảng cộng đồng xếp hạng các LLMs thông qua 
so sánh từng cặp và xếp hạng Elo. Cách cộng đồng đánh giá LLMs có kết quả đáng tin cậy hơn nhưng 
đối mặt với chi phí cao và hiệu quả thấp. Vicuna (Chiang et al., 2023) sử dụng GPT-4 làm thẩm 
phán để chọn câu trả lời tốt hơn. Mặc dù phương pháp dựa trên GPT-4 có thể phán quyết LLMs như 
một chuyên gia con người, các phương pháp dựa trên API có rủi ro tiềm ẩn về rò rỉ dữ liệu và hiệu 
suất không ổn định. Zeno Build (Alex & Graham, 2023) đề xuất đánh giá LLMs trên một bộ dữ liệu 
dịch vụ khách hàng, nhưng việc sử dụng các chỉ số truyền thống như ChrF (Popović, 2015) và 
BERTScore (Zhang et al., 2019) không thể đánh giá đầy đủ các câu trả lời của LLMs trong các 
nhiệm vụ mở. Bên cạnh đó, PandaLM (Wang et al., 2023) và Auto-J Li et al. (2023a) đã phát 
triển các mô hình thẩm phán dựa trên LLaMA (Touvron et al., 2023a) hoặc LLaMA2 (Touvron et al., 
2023b) để so sánh các câu trả lời được tạo bởi LLMs. Khi phục vụ như thẩm phán, PandaLM đạt được 
độ chính xác gần với ChatGPT nhưng việc bỏ qua các thiên lệch vốn có của LLM hạn chế hiệu suất 
của nó hơn nữa. JudgeLM của chúng tôi chứa các thẩm phán có thể mở rộng từ tham số 7B đến 33B 
và đạt được hiệu suất thẩm phán tốt nhất hiện tại trong cả tiêu chuẩn PandaLM và tiêu chuẩn của 
chúng tôi. Hơn nữa, các nhà nghiên cứu có thể sử dụng JudgeLM được đề xuất cục bộ điều này đảm 
bảo khả năng tái tạo và bảo mật dữ liệu.

3 BỘ DỮ LIỆU

Các bộ dữ liệu chất lượng cao, quy mô lớn là rất quan trọng để tinh chỉnh hiệu quả các mô hình 
ngôn ngữ lớn (LLMs) để hoạt động như các thẩm phán đánh giá. Tuy nhiên, các bộ dữ liệu đồng 
thời, như bộ dữ liệu của PandaLM (Wang et al., 2023), thể hiện những hạn chế về tính đa dạng 
và độ chi tiết của tiêu chí phán quyết.

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

[Ví dụ về input và output của mẫu dữ liệu JudgeLM]

Để giải quyết điều này, chúng tôi giới thiệu một bộ dữ liệu mới chứa đầy nhiều nhiệm vụ hạt giống 
phong phú, câu trả lời toàn diện từ các LLMs hiện đại, điểm số câu trả lời từ thẩm phán giáo viên, 
và lý do chi tiết cho các phán quyết. Phần 3.1 làm rõ quá trình tạo dữ liệu, trong khi Phần 3.2 
mô tả các phương pháp được áp dụng để huấn luyện và đánh giá bằng bộ dữ liệu của chúng tôi.

3.1 TẠO DỮ LIỆU

Mục tiêu chính của việc tạo dữ liệu của chúng tôi là tạo ra một bộ dữ liệu quy mô lớn và đa dạng 
tối đa hóa khả năng đánh giá của các mô hình thẩm phán. Chúng tôi lấy mẫu 105K nhiệm vụ hạt 
giống hướng dẫn từ một tập hợp quy mô lớn chứa Alpaca-GPT4 (Peng et al., 2023), Dolly-15K 
(Conover et al., 2023), GPT4All-LAION (Anand et al., 2023), và ShareGPT. Để tăng cường tính 
đa dạng của bộ dữ liệu, các câu trả lời được tổng hợp từ 11 LLMs mã nguồn mở hàng đầu bao gồm, 
nhưng không giới hạn ở, LLaMA (Touvron et al., 2023a), Alpaca, và Vicuna (Chiang et al., 2023). 
Sau đó, chúng tôi kết hợp các câu trả lời được tạo bởi LLM với câu trả lời tham chiếu để tạo ra 
các tập câu trả lời. Các cặp được chọn ngẫu nhiên từ các tập hợp, sau đó, các điểm số chi tiết 
và lý do chi tiết được gán bởi mô hình giáo viên tiên tiến, GPT-4. Để đảm bảo các phán quyết 
mạnh mẽ và toàn diện, chúng tôi sử dụng các mẫu chi tiết như được trình bày trong Hình 3. Ngoài 
ra, để cho phép mô hình phán quyết với câu trả lời tham chiếu, mẫu bao gồm tham chiếu được sử 
dụng như Hình 4. Điều này khuyến khích mô hình tích hợp kiến thức bên ngoài trong quá trình 
đánh giá. Xin lưu ý rằng tất cả các mẫu trong bộ xác thực JudgeLM được kiểm tra thêm và ghi 
chú lại bởi các tác giả để đảm bảo sự phù hợp với sở thích con người.

3.2 HUẤN LUYỆN VÀ ĐÁNH GIÁ

Để sử dụng tốt hơn bộ dữ liệu của chúng tôi để huấn luyện và đánh giá các mô hình thẩm phán, 
chúng tôi phân chia nó thành một phần huấn luyện và một phần xác thực. Tập huấn luyện chứa 
100K mẫu thẩm phán, trong khi tập xác thực có 5K. Sau đó chúng tôi giới thiệu cách chúng tôi 
sử dụng bộ dữ liệu này để huấn luyện và đánh giá, tương ứng.

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

Huấn luyện. Quá trình huấn luyện của JudgeLM tuân theo mô hình tinh chỉnh hướng dẫn. Như được 
minh họa trong Hình 2, mô hình được cung cấp một câu hỏi cùng với một cặp câu trả lời, và một 
câu trả lời tham chiếu tùy chọn, tạo ra các đầu ra bao gồm điểm số và lý do chi tiết. Điều quan 
trọng cần lưu ý là tầm quan trọng của một mẫu prompt được tạo chi tiết để khai thác toàn bộ 
tiềm năng khả năng tuân theo hướng dẫn của JudgeLM. Các mẫu đầu vào riêng biệt phục vụ các 
tình huống có và không có tham chiếu, như được mô tả trong Hình 3 và Hình 4 tương ứng.

Để phân tích thêm khả năng mở rộng của JudgeLM, chúng tôi tinh chỉnh JudgeLM với các kích thước 
7B, 13B, và 33B tham số. Các siêu tham số cụ thể được liệt kê trong Bảng 11. Đối với phân tích 
mở rộng cho kích thước bộ dữ liệu, chúng tôi cũng tinh chỉnh JudgeLM trên các quy mô dữ liệu 
khác nhau từ 3.5K đến 100K mẫu. JudgeLM thể hiện khả năng mở rộng cả về kích thước mô hình 
và khối lượng dữ liệu.

Đánh giá. Đối với kết quả của thẩm phán, chúng tôi mô hình hóa nó như "chấm điểm, phán quyết, 
và lý luận". Mô hình thẩm phán đầu tiên tạo ra điểm số cho các cặp câu trả lời. Sau đó, chúng 
ta có thể nhận được kết quả phán quyết từ ba tình huống: "Câu trả lời 1 thắng" nếu điểm số của 
câu trả lời 1 cao hơn câu trả lời 2, "Câu trả lời 2 thắng" nếu điểm số của câu trả lời 2 cao hơn, 
hoặc "Hòa" nếu điểm số của hai câu trả lời bằng nhau. Cuối cùng, mô hình tạo ra lý do chi tiết 
nếu cần. Ưu điểm của việc mô hình hóa này là mô hình thẩm phán chỉ cần ít thời gian để chấm điểm 
và phán quyết, và tạo ra lý luận tốn thời gian một cách tùy chọn.

Đối với các chỉ số, chúng tôi sử dụng các chỉ số khách quan và chỉ số độ tin cậy để đánh giá 
các mô hình thẩm phán một cách toàn diện. Đối với các chỉ số khách quan, chúng tôi tính toán 
sự đồng thuận, độ chính xác, độ nhạy, và điểm F1 giữa kết quả phán quyết của mô hình và những 
kết quả của giáo viên. Điều này cung cấp cái nhìn sâu sắc về sự căn chỉnh của các mô hình thẩm 
phán với các tiêu chuẩn đã được thiết lập, như GPT-4 hoặc các chuyên gia con người. Đối với các 
chỉ số độ tin cậy, đầu tiên chúng tôi so sánh kết quả trước và sau khi hoán đổi câu trả lời LLM. 
Sau đó chúng tôi tính toán tính tự nhất quán để đo lường độ tin cậy của mô hình thẩm phán. Cuối 
cùng, chúng tôi tính toán thêm các chỉ số như "thiên lệch về thứ nhất", "thiên lệch về thứ hai", 
và "delta thiên lệch" để có cái nhìn sâu sắc từ các thiên lệch vị trí cụ thể và phương sai của chúng.

4 THIÊN LỆCH VỐN CÓ

Trong bài báo này, chúng tôi cũng nghiên cứu các thiên lệch vốn có ảnh hưởng đến độ tin cậy của 
các thẩm phán LLM được tinh chỉnh thông qua các chỉ số độ tin cậy và trực quan hóa.

Thiên lệch Vị trí. Thiên lệch vị trí có nghĩa là các thẩm phán LLM ưa thích câu trả lời ở một 
vị trí nhất định và nó tồn tại rộng rãi trong các nhiệm vụ xử lý ngôn ngữ tự nhiên (Ko et al., 
2020; Wang et al., 2018) và việc ra quyết định của con người (Blunch, 1984; Raghubir & Valenzuela, 
2006). Các LLMs mạnh mẽ, ChatGPT và GPT-4, cũng đối mặt với thách thức này khi làm việc như 
thẩm phán (Wang et al., 2023; Zheng et al., 2023; Li et al., 2023b). Như các kết quả định 
tính và định lượng được hiển thị trong Hình 8 và Bảng 5, JudgeLM cũng đối mặt với thiên lệch 
vị trí và ưa thích câu trả lời đầu tiên khi hoán đổi vị trí của các câu trả lời.

Thiên lệch Kiến thức. Thiên lệch kiến thức phát sinh khi dữ liệu được huấn luyện trước thiếu 
kiến thức về một số nhiệm vụ hạt giống hoặc tạo ra kiến thức có thể không mong muốn (Ko et al., 
2020; Zheng et al., 2023) có thể làm suy giảm khả năng tạo sinh của LLMs. Hình 10 cung cấp một 
ví dụ rằng các thẩm phán LLM không thể đưa ra phán quyết chính xác cho các nhiệm vụ mở nếu 
chúng thiếu sự thật liên quan.

Thiên lệch Định dạng. Các nhà nghiên cứu mong đợi rằng mô hình thẩm phán có thể đưa ra phán 
quyết dựa trên kiến thức được huấn luyện trước khi tham chiếu không có sẵn và có thể đưa ra phán 
quyết theo tham chiếu khi nó có sẵn. Tuy nhiên, các thí nghiệm của chúng tôi tiết lộ rằng các 
mô hình thẩm phán được tinh chỉnh mà không có tham chiếu hoạt động kém trong việc phán quyết 
với tham chiếu, và ngược lại, như được hiển thị trong Hình 12, Hình 13, và Bảng 6. Chúng tôi 
giả thuyết rằng việc tinh chỉnh với tham chiếu khuyến khích mô hình thẩm phán đưa ra phán quyết 
dựa trên kiến thức bên ngoài và việc tinh chỉnh mà không có tham chiếu đẩy mô hình thẩm phán 
đưa ra phán quyết thông qua kiến thức được huấn luyện trước của nó. Chúng tôi đặt tên cho tình 
huống mà một thẩm phán được tinh chỉnh mà không có tham chiếu nhưng được xác thực với tham 
chiếu là một định dạng không khớp, và ngược lại. Thiên lệch định dạng như vậy hạn chế việc khái 
quát hóa thêm của mô hình thẩm phán trong các lĩnh vực khác.

5 PHƯƠNG PHÁP

Trong việc đánh giá các câu trả lời được tạo bởi LLM cho một câu hỏi hạt giống, thẩm phán LLM 
nhằm mục đích xác định câu trả lời vượt trội từ một cặp ứng viên. Được động lực bởi các phương 
pháp gần đây (Touvron et al., 2023a; Chiang et al., 2023; Ouyang et al., 2022), chúng tôi trình 
bày JudgeLM, một mô hình thẩm phán có thể mở rộng, và giải quyết các thiên lệch vốn có trong 
các mô hình như vậy. Phương pháp luận của chúng tôi được mô tả trong Hình 1b. Các phần tiếp 
theo cung cấp một phân tích chi tiết về cách tiếp cận của chúng tôi.

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

5.1 TĂNG CƯỜNG HOÁN ĐỔI

MT-bench (Zheng et al., 2023) và PandaLM (Wang et al., 2023) giảm thiểu thiên lệch vị trí bằng 
cách phán quyết hai lần với thứ tự gốc và ngược lại. Những phương pháp này coi kết quả là hòa 
nếu các phán quyết không giống nhau. Loại phương pháp này bỏ qua thiên lệch vị trí vốn có và 
tốn gấp đôi thời gian để đánh giá, có thể được coi là một sự thỏa hiệp và không cải thiện độ tin 
cậy của các thẩm phán LLM.

Một cách trực quan, việc hoán đổi các vị trí ở giai đoạn tinh chỉnh có thể thúc đẩy mô hình thẩm 
phán chú ý nhiều hơn đến nội dung của câu trả lời thay vì vị trí. Tận dụng dữ liệu thẩm phán 
có cấu trúc của chúng tôi, chúng tôi có thể dễ dàng hoán đổi vị trí của câu trả lời để tạo ra một 
mẫu đầu vào mới. Tương ứng, chúng tôi cũng hoán đổi điểm số và chỉ số câu hỏi của phán quyết 
từ giáo viên (tức là GPT4) để có được sự thật cơ bản mới. Như được hiển thị trong Hình 15, mẫu 
thẩm phán được tăng cường giữ nguyên kết quả nhưng trao đổi vị trí của các câu trả lời. Nhìn 
chung, nó đơn giản nhưng hiệu quả để tăng cường dữ liệu huấn luyện và giải quyết thiên lệch vị 
trí. JudgeLM-with-swap-augmentation có thể đưa ra phán quyết tốt cho cùng một mẫu thẩm phán 
như được hiển thị trong Hình 9.

5.2 HỖ TRỢ THAM CHIẾU

Việc giới thiệu kiến thức bên ngoài trong giai đoạn tinh chỉnh là một cách trực quan để bù đắp 
cho việc thiếu kiến thức được huấn luyện trước liên quan. Để làm như vậy, chúng tôi đề xuất phương 
pháp hỗ trợ tham chiếu để dạy mô hình phán quyết với sự giúp đỡ của câu trả lời tham chiếu. Theo 
Zheng et al. (2023), chúng tôi thu thập câu trả lời tham chiếu cho tất cả các mẫu thẩm phán và 
tái tạo các phán quyết được hướng dẫn bởi tham chiếu bởi GPT-4. Xin lưu ý rằng GPT-4 cũng đưa 
ra điểm số và phán quyết khác nhau cho hầu hết các mẫu thẩm phán có hoặc không có tham chiếu. 
Điều này chứng minh rằng sự khác biệt giữa kiến thức được huấn luyện trước và câu trả lời tham 
chiếu tác động lớn đến các phán quyết. Như được hiển thị trong Hình 11, JudgeLM với hỗ trợ tham 
chiếu có thể tránh các lỗi thực tế và đưa ra các phán quyết đáng tin cậy. Hơn nữa, việc giới 
thiệu hỗ trợ tham chiếu cho các thẩm phán LLM có thể đơn giản chèn sở thích thẩm phán. JudgeLM 
với huấn luyện hỗ trợ tham chiếu có thể linh hoạt thiết lập câu trả lời tham chiếu với các sở 
thích khác nhau cho các tình huống và nhu cầu khác nhau. Như được hiển thị trong Hình 16, việc 
thay đổi câu trả lời tham chiếu không cần huấn luyện thêm và làm cho JudgeLM linh hoạt hơn với 
các sở thích khác nhau.

5.3 LOẠI BỎ THAM CHIẾU

Để giải quyết thiên lệch định dạng, chúng tôi giới thiệu một phương pháp, được đặt tên là loại 
bỏ tham chiếu, trong đó chúng tôi ngẫu nhiên loại bỏ mẫu huấn luyện với tham chiếu và sử dụng 
mẫu tương ứng mà không có tham chiếu. Như được hiển thị trong Hình 14, các mô hình thẩm phán 
với loại bỏ tham chiếu có thể giảm thiểu việc overfitting cho các định dạng tinh chỉnh và đưa ra 
phán quyết dựa trên tham chiếu bên ngoài hoặc kiến thức được huấn luyện trước khi được cung cấp 
tham chiếu hoặc không, tương ứng. Hơn nữa, phương pháp loại bỏ tham chiếu cũng làm cho mô hình 
thẩm phán dễ sử dụng và giảm chi phí phù hợp với các định dạng khác nhau.

6 THÍ NGHIỆM

Chúng tôi nghiên cứu hiệu suất của JudgeLM như sau: Phần 6.1 trình bày kết quả chính của JudgeLM 
so sánh với các phương pháp đồng thời, Phần 6.2 phân tích khả năng mở rộng của JudgeLM từ cả 
kích thước mô hình và quy mô dữ liệu, và Phần 6.3 hiển thị các nghiên cứu loại bỏ các phương 
pháp được đề xuất chi tiết. Các thiết lập chi tiết được hiển thị trong Phần A.2.

6.1 KẾT QUẢ CHÍNH

So sánh trên Tiêu chuẩn JudgeLM. Trước tiên chúng tôi đánh giá JudgeLM được đề xuất trên bộ 
xác thực của chúng tôi. Lưu ý rằng bộ xác thực JudgeLM được kiểm tra thêm và ghi chú lại bởi 
các tác giả để đảm bảo sự phù hợp với sở thích con người. Như được hiển thị trong Bảng 1, chúng 
tôi đưa ra kết quả định lượng của GPT-3.5, Vicuna-13B, PandaLM-7B, Auto-J-13B (Li et al., 
2023a), InstructScore-7B (Xu et al., 2023), và JudgeLM của chúng tôi với ba kích thước mô hình. 
Trong số đó, GPT-3.5 được sử dụng dưới dạng APIs với sự giúp đỡ của các mẫu trong Hình 3 và 
Hình 4. PandaLM-7B và Auto-J-13B được triển khai với các điểm kiểm tra đã phát hành và

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

[Bảng 1: Kết quả chính cho JudgeLM và các phương pháp đồng thời trên bộ xác thực của chúng tôi]

các mẫu. Những phương pháp này có thể được coi là các phương pháp zero-shot vì chúng không được 
tinh chỉnh bởi bộ dữ liệu JudgeLM. Trên bộ xác thực JudgeLM, Vicuna-13B gốc thất bại 77% câu 
hỏi. Cụ thể, Vicuna-13B gốc thậm chí không thể xuất ra một cặp điểm số trong các phán quyết 
trong các trường hợp thất bại. Nhưng phiên bản được tinh chỉnh, tức là JudgeLM, sẽ không thất 
bại bất kỳ câu hỏi nào trong bộ xác thực JudgeLM. Các JudgeLMs của chúng tôi được tinh chỉnh 
với các phương pháp được đề xuất, tức là tăng cường hoán đổi, hỗ trợ tham chiếu, và loại bỏ tham 
chiếu. Vì vậy, chúng có thể xử lý các tình huống có hoặc không có tham chiếu đồng thời. Có thể 
quan sát thấy rằng JudgeLM-7B của chúng tôi vượt trội hơn PandaLM-7B, Auto-J, và InstructScore 
trong tất cả các chỉ số, và thậm chí vượt qua GPT-3.5. Hơn nữa, JudgeLM-33B được đề xuất thể 
hiện khả năng thẩm phán mạnh mẽ nhất.

So sánh trên Các Tiêu chuẩn Đánh giá Con người Khác. Chúng tôi đánh giá thêm JudgeLM của chúng 
tôi trên các tiêu chuẩn đánh giá con người khác, tức là bộ kiểm tra PandaLM và MM-Vet được ghi 
chú bởi con người. Các bộ huấn luyện và xác thực của PandaLM được ghi chú bởi GPT-3.5 và con 
người, tương ứng. Theo cách của bộ xác thực PandaLM, chúng tôi trình bày kết quả zero-shot của 
JudgeLM trong Bảng 2. Có thể quan sát thấy rằng JudgeLM-7B vượt trội hơn GPT-3.5 và PandaLM-7B. 
Khi so sánh với GPT-4, JudgeLM-7B có độ chính xác thấp hơn và điểm Precision, Recall, và F1 
cao hơn GPT-4. Hơn nữa, JudgeLM-33B đạt được kết quả cao hơn GPT-4, điều này chứng minh rằng 
JudgeLM được tinh chỉnh có thể vượt trội hơn giáo viên của nó trong nhiệm vụ cụ thể này. Bên cạnh 
đó, chúng tôi cũng đề xuất một tiêu chuẩn phán quyết đa phương thức được ghi chú bởi con người 
để đánh giá JudgeLM của chúng tôi như được hiển thị trong Bảng 13.

[Bảng 2: Kết quả đánh giá zero-shot JudgeLM trên bộ kiểm tra PandaLM]

So sánh Hiệu quả. Để so sánh thêm hiệu quả giữa JudgeLM và PandaLM của chúng tôi, chúng tôi 
tiến hành thí nghiệm trên bộ xác thực của chúng tôi để hiển thị chi phí thời gian sử dụng cùng 
một máy với 8 GPU NVIDIA-A100 (40G). Như được hiển thị trong Bảng 3, chúng tôi hiển thị các 
phương pháp và kích thước mô hình trong cột thứ nhất và thứ hai. Cột thứ ba hiển thị số GPU cần 
thiết cho mỗi mô hình thẩm phán. Các mô hình với tham số 7B hoặc 13B chạy trên 1 GPU A100 với 
bộ nhớ 40G trong khi tham số 33B cần 2 GPU. Cột thứ tư hiển thị liệu các phương pháp có thể 
phán quyết câu trả lời song song hay không. Cột thứ năm chỉ ra liệu lý do thẩm phán có được tạo 
ra ở thời gian chạy hay không. Cột thứ sáu trình bày tổng chi phí thời gian. Chúng tôi sử dụng 
PandaLM-7B và JudgeLM-7B làm cơ sở hiệu quả, không sử dụng phán quyết song song và tạo ra lý 
do chi tiết cho tất cả các câu hỏi. Nhờ việc mô hình hóa JudgeLM, tức là "chấm điểm, phán quyết, 
và lý luận", JudgeLM có thể bỏ qua giai đoạn lý luận và chỉ yêu cầu 24 phút, nhanh hơn 16.65 
lần so với cơ sở. Khi chúng tôi kích hoạt tối ưu hóa kỹ thuật của phán quyết song song, JudgeLM 
có thể tận dụng đầy đủ 8 GPU và tốn chỉ 50 phút, nhanh hơn 8 lần so với cơ sở chạy trên một 
GPU duy nhất. Khi chúng tôi kích hoạt phán quyết song song và bỏ qua giai đoạn lý luận cho JudgeLM, 
JudgeLM-7B chỉ tiêu thụ 3 phút để phán quyết 5000 cặp phản hồi, nhanh hơn 133.3 lần so với cơ 
sở. Mô hình thẩm phán lớn nhất, JudgeLM-33B, cũng có thể hoàn thành xác thực trong 15 phút. 
Hiệu quả cao của JudgeLM có thể giảm đáng kể thời gian dành cho việc đánh giá LLMs, cho phép 
các nhà nghiên cứu và nhà phát triển tăng tốc độ tiến bộ.

[Bảng 3: So sánh hiệu quả cho JudgeLM và PandaLM trên bộ xác thực của chúng tôi]

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

[Bảng 4: Phân tích hiệu suất cho JudgeLM mở rộng trên bộ xác thực của chúng tôi]

6.2 PHÂN TÍCH MỞ RỘNG CỦA JUDGE LM

Trong phần này, chúng tôi phân tích khả năng mở rộng của JudgeLM đơn giản (không có các phương 
pháp được đề xuất) trên bộ xác thực của chúng tôi mà không có tham chiếu như được minh họa 
trong Bảng 4. Khi chúng tôi tăng kích thước mô hình và quy mô dữ liệu, chúng ta có thể quan sát 
thấy các chỉ số tăng lên. Nó chứng minh rằng JudgeLM được đề xuất có thể mở rộng và có thể đạt 
tới 90.06% đồng thuận và 87.93% tính nhất quán với tham số 33B và dữ liệu tinh chỉnh 100K.

6.3 NGHIÊN CỨU LOẠI BỎ

Trong phần này, chúng tôi trình bày các nghiên cứu loại bỏ của các phương pháp được đề xuất. 
Đối với tất cả các nghiên cứu loại bỏ, chúng tôi sử dụng JudgeLM-7B làm mô hình cơ sở và dữ 
liệu 3.5K để tinh chỉnh. Dựa trên cơ sở này, chúng tôi phân tích các cải thiện được mang lại bởi 
tăng cường hoán đổi, hỗ trợ tham chiếu, và loại bỏ tham chiếu.

[Bảng 5: Nghiên cứu loại bỏ cho tăng cường hoán đổi trên bộ xác thực của chúng tôi]

Cải thiện của Tăng cường Hoán đổi. Như được hiển thị trong Bảng 5, tăng cường hoán đổi có thể 
cải thiện mô hình cơ sở một cách toàn diện. Nó cải thiện tính nhất quán 5.44%, điều này chứng 
minh rằng tăng cường hoán đổi có thể giảm ảnh hưởng của thiên lệch vị trí và thúc đẩy thẩm phán 
chú ý nhiều hơn đến nội dung của câu trả lời.

Cải thiện của Hỗ trợ Tham chiếu. Như được hiển thị trong các hàng với định dạng phù hợp của 
Bảng 6, JudgeLM được tinh chỉnh với hỗ trợ tham chiếu thể hiện hiệu suất vượt trội trên mọi chỉ 
số. Nó chứng minh rằng việc giới thiệu câu trả lời tham chiếu khiến thẩm phán dựa vào kiến thức 
bên ngoài và giải quyết hạn chế của kiến thức được huấn luyện trước.

[Bảng 6: Nghiên cứu loại bỏ cho hỗ trợ tham chiếu và loại bỏ tham chiếu trên bộ xác thực của chúng tôi]

Cải thiện của Loại bỏ Tham chiếu. Như được hiển thị trong Bảng 6, các cơ sở không thể đạt được 
hiệu suất thỏa đáng khi đối mặt với các định dạng không khớp. Với sự giúp đỡ của loại bỏ tham 
chiếu, JudgeLM có thể xử lý cả định dạng có hoặc không có tham chiếu và đạt được đồng thuận và 
tính nhất quán cao hơn. Nó chứng minh rằng loại bỏ tham chiếu có thể giải quyết thiên lệch định 
dạng và tránh JudgeLM overfitting vào một định dạng duy nhất.

Loại bỏ Hình thức Phán quyết Chúng tôi đánh giá thêm hiệu suất của JudgeLM-7B với giải thích 
trước (Chuỗi Suy nghĩ, CoT (Wei et al., 2022)) hoặc điểm số trước (Của chúng tôi) trong Bảng 
7. JudgeLM với CoT hoạt động với đồng thuận tương tự với cơ sở điểm số trước của chúng tôi nhưng 
với tính nhất quán cao hơn, điều này có nghĩa là hình thức giải thích trước, tức là CoT, có thể 
giảm thiểu thiên lệch vị trí của các thẩm phán được tinh chỉnh, nhưng không mang lại cải thiện 
đồng thuận đáng kể. Kết quả là, chúng tôi chọn phương pháp điểm số trước cho JudgeLM, có tính 
nhất quán ít hơn một chút nhưng sử dụng linh hoạt hơn.

[Bảng 7: Hiệu suất của JudgeLM-7B với giải thích trước (CoT) hoặc điểm số trước (Của chúng tôi)]

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

[Bảng 8: So sánh giữa giáo viên GPT-4 và JudgeLM-33B trên bộ xác thực JudgeLM]

6.4 THÍ NGHIỆM BỔ SUNG

So sánh với Giáo viên GPT-4 Như được hiển thị trong Bảng 8, chúng tôi liệt kê thêm các chỉ số 
ngoại trừ sự đồng thuận với chính GPT-4. JudgeLM-33B đạt được tính nhất quán cao hơn GPT-4, 
điều này chứng minh rằng các thẩm phán tinh chỉnh như JudgeLM-33B có thể đạt được tính nhất 
quán cao hơn thông qua các kỹ thuật được đề xuất.

Mô hình học tập của JudgeLM tương tự như chưng cất kiến thức, vì JudgeLM học từ các phán quyết 
chuyên gia được cung cấp bởi GPT-4. Trong lĩnh vực chưng cất kiến thức (Gou et al., 2021), mô 
hình học sinh (JudgeLM) bắt chước mô hình giáo viên (GPT-4) để đạt được hiệu suất cạnh tranh 
hoặc thậm chí vượt trội. Ngoài ra, JudgeLM của chúng tôi sử dụng ba phương pháp chính và sử 
dụng dữ liệu huấn luyện quy mô lớn cụ thể cho nhiệm vụ phán quyết để tăng cường sự đồng thuận 
và tính nhất quán của nó. Như đã đề cập trong "So sánh trên Các Tiêu chuẩn Đánh giá Con người 
Khác" dưới Phần 6.1, các thí nghiệm của chúng tôi chứng minh rằng một mô hình thẩm phán chuyên 
gia được tinh chỉnh có thể vượt qua giáo viên tổng quát của nó trên một số tiêu chuẩn phán quyết, 
tức là bộ xác thực JudgeLM và bộ kiểm tra PandaLM.

6.5 CHI TIẾT CỦA BỘ DỮ LIỆU

Đối với bộ dữ liệu và tiêu chuẩn được đề xuất, chúng tôi cũng cung cấp giải thích về phạm vi sử 
dụng, chi tiết tính toán chỉ số, chất lượng bộ dữ liệu, danh mục câu hỏi & phân phối (trong 19 
danh mục), và so sánh với UltraFeedback (Cui et al., 2023) trong Phần A.1. Chúng tôi hy vọng 
bộ dữ liệu có thể giúp các nhà nghiên cứu xây dựng các công cụ đánh giá mạnh mẽ hơn trong tương 
lai.

6.6 KHẢ NĂNG KHÁI QUÁT HÓA CỦA JUDGE LM

Không chỉ phán quyết các cặp câu trả lời, mà JudgeLM của chúng tôi cũng có thể khái quát hóa 
cho các nhiệm vụ phán quyết khác nhau (bao gồm các vấn đề toán học và tạo mã), các tiêu chuẩn 
phán quyết chưa thấy (tiêu chuẩn được ghi chú bởi con người, tiêu chuẩn phán quyết đa phương 
thức, tiêu chuẩn định dạng truy xuất, tiêu chuẩn định dạng nhiều, tiêu chuẩn trò chuyện độc hại, 
tiêu chuẩn mô hình phần thưởng), và các phần mở rộng phán quyết khác nhau (chấm điểm câu trả 
lời đơn, trò chuyện nhiều lượt). Chúng tôi để lại phân tích chi tiết trong Phần A.3 của phụ lục.

6.7 THẢO LUẬN THÊM

Do hạn chế về số trang, chúng tôi để lại thêm thảo luận trong Phần A.4 của phụ lục.

7 KẾT LUẬN

Trong bài báo này, trước tiên chúng tôi giới thiệu một bộ dữ liệu chất lượng cao, quy mô lớn cho 
đánh giá LLM, cung cấp nền tảng vững chắc cho nghiên cứu tương lai. Tiếp theo, JudgeLM được đề 
xuất như các thẩm phán có thể mở rộng để đánh giá LLMs trong các nhiệm vụ mở một cách hiệu quả, 
đạt được hiệu suất thẩm phán tốt nhất hiện tại trên hai tiêu chuẩn. Sau đó, chúng tôi phân tích 
hai thiên lệch chính và giới thiệu một thiên lệch định dạng mới trong việc tinh chỉnh LLMs làm 
thẩm phán, và giải quyết chúng với các kỹ thuật được đề xuất. Chúng tôi hy vọng công trình của 
chúng tôi có thể thúc đẩy nhiều nghiên cứu hơn để khám phá các mô hình thẩm phán cho LLMs trong 
các nhiệm vụ mở và xây dựng các LLMs mạnh mẽ hơn với sự hướng dẫn từ các mô hình thẩm phán.

Hạn chế. Mặc dù JudgeLM được đề xuất đạt được hiệu suất và hiệu quả đáng khuyến khích, chi phí 
của bộ dữ liệu thẩm phán hạn chế việc mở rộng thêm trong bộ dữ liệu thẩm phán. Hiện tại, chúng 
tôi chi khoảng 4000 đô la để cung cấp 100K dữ liệu thẩm phán chất lượng cao được tạo bởi GPT-4 
cho công chúng. Chúng tôi mong đợi cải thiện thêm hiệu suất của các mô hình thẩm phán với sự 
giúp đỡ của dữ liệu thẩm phán tổng hợp.

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

LỜI CẢM ơN VÀ TIẾT LỘ NGUỒN TÀI TRỢ

Công trình này được hỗ trợ một phần bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (NSFC) 
dưới Số Tài trợ 62276108.

TÀI LIỆU THAM KHẢO

[Danh sách tài liệu tham khảo được dịch sang tiếng Việt, bao gồm các tác giả, tiêu đề, và thông 
tin xuất bản được chuyển đổi phù hợp]

A PHỤ LỤC / TÀI LIỆU BỔ SUNG

A.1 THÊM VỀ BỘ DỮ LIỆU

Phạm vi Sử dụng Bộ dữ liệu Chúng tôi nhấn mạnh rằng bộ dữ liệu JudgeLM chỉ dành cho nghiên 
cứu học thuật và bất kỳ việc sử dụng thương mại nào đều bị cấm. Vì các điều khoản của OpenAI 
cấm phát triển các mô hình cạnh tranh với OpenAI, các bộ dữ liệu tinh chỉnh hướng dẫn được tạo 
bởi API của OpenAI, tức là Alpaca, PandaLM, v.v., đều tuân theo quy tắc này.

[Tiếp tục với các phần chi tiết khác của phụ lục được dịch sang tiếng Việt...]
