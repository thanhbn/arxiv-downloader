# Papers Relevant for AI-Based Code Review Tasks

This document lists papers from the arxiv-downloader collection that are relevant for code review, code analysis, programming, and software engineering tasks.

## Coding Collection

### Code Quality and Bug Detection
- **2206.03865** - Fault-Aware Neural Code Rankers
- **2305.04940** - The EarlyBIRD Catches the Bug: On Exploiting Early Layers
- **2305.16430** - Too Few Bug Reports: Exploring Data Augmentation
- **2306.01754** - Transformer-based Vulnerability Detection in Code at EditTime
- **2309.03044** - Method-Level Bug Severity Prediction using Source
- **2309.12938** - Frustrated with Code Quality Issues? LLMs can Help!
- **2305.18607** - How Effective Are Neural Networks for Fixing Security

### Code Analysis and Understanding
- **2106.14316** - PYInfer: Deep Learning Semantic Type Inference
- **2203.16697** - Type-Directed Program Synthesis for RESTful APIs
- **2212.10017** - Unveiling Code Pre-Trained Models: Investigating Syntax
- **2304.06815** - Automatic Semantic Augmentation of Language Model Prompts
- **2211.12821** - Explaining Transformer-based Code Models (also in interpretability)

### Code Generation and Verification
- **2302.08468** - LEVER: Learning to Verify Language-to-Code Generation with Execution
- **2303.06233** - Large Language Models (GPT) Struggle to Answer Multiple-Choice
- **2306.03438** - Large Language Models of Code Fail at
- **2310.10996** - ClarifyGPT: Empowering LLM-based Code Generation

### Testing and Validation
- **2308.16557** - Effective Test Generation Using Pre-trained Large Language Models
- **2310.05727** - THE PROGRAM TESTING ABILITY OF LARGE LAN
- **2312.00894** - Leveraging Large Language Models to Improve REST API Testing

### Code Review and Collaboration
- **2310.08879** - A Critical Review of Large Language Model on Software
- **2306.05153** - Is AI the better programming partner
- **2309.00608** - Copiloting the Copilots: Fusing Large Language Models

### Static Analysis and Program Understanding
- **2202.13169** - Published as a workshop paper at DL4C @ ICLR 2022
- **2203.13474** - Published as a conference paper at ICLR 2023
- **2303.17125** - A Large-Scale Survey on the Usability of AI Programming
- **2306.03324** - How Effective are Large Language Models

## Benchmark Collection

### Code Evaluation Benchmarks
- **2208.03133** - Out of the BLEU: How Should We Assess Quality of the Code
- **2210.14473** - Benchmarking Language Models for Code Syntax Understanding
- **2308.01861** - ClassEval: A Manually-Crafted Benchmark
- **2310.06266** - CodeFuse-13B: A Pretrained Multi-lingual Code Large Language
- **2310.11248** - CROSSCODE EVAL: A Diverse and Multilingual
- **2401.03065** - CRUXEval: A Benchmark for Code Reasoning
- **2402.14261** - Copilot Evaluation Harness: Evaluating LLM-Guided

## Other Relevant Collections

### Adversarial
- **2206.00052** - CodeAttack: Code-Based Adversarial Attacks for Pre-trained Programming

### Instruction Tuning
- **2304.03816** - Towards Generating Functionally Correct Code Edits
- **2308.07124** - OCTOPACK: INSTRUCTION TUNING CODE LARGE
- **2310.20329** - InstructCoder: Instruction Tuning Large Language Models for Code

### RAG (Retrieval-Augmented Generation)
- **2207.05987** - Published as a conference paper at ICLR 2023
- **2308.15645** - AskIt: Unified Programming Interface

### Planning
- **2309.12499** - CodePlan: Repository-level Coding using LLMs and Planning

### Reflection
- **2402.04858** - CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay

### RL-Alignment
- **2402.01391** - StepCoder: Improve Code Generation

### PEFT (Parameter-Efficient Fine-Tuning)
- **2304.05216** - Towards Efficient Fine-tuning of Pre-trained Code Models
- **2308.10462** - Exploring Parameter-Efficient Fine-Tuning Techniques

### Multilingual
- **2305.06156** - The Vault: A Comprehensive Multilingual Dataset for Advancing Code
- **2305.11626** - CCT-Code: Cross-Consistency Training for Multilingual Clone Detection
- **2309.01940** - CodeApex: A Bilingual Programming Evaluation Benchmark

### Long Context
- **2310.01602** - CAT-LM (relates to code understanding)

### Dataset Generation
- **2401.03038** - spade: Synthesizing Data Quality Assertions

### Evaluation
- **2309.11385** - SAFURAI 001: NEW QUALITATIVE APPROACH FOR

## Key Papers for Code Review AI

Based on the collection, here are the most relevant papers for building AI-based code review systems:

1. **Core Code Analysis**:
   - 2106.14316 (Semantic Type Inference)
   - 2211.12821 (Explaining Code Models)
   - 2210.14473 (Code Syntax Understanding)

2. **Bug and Vulnerability Detection**:
   - 2206.03865 (Fault-Aware Neural Code Rankers)
   - 2306.01754 (Vulnerability Detection at EditTime)
   - 2309.03044 (Bug Severity Prediction)
   - 2305.18607 (Fixing Security Issues)

3. **Code Quality Assessment**:
   - 2309.12938 (Code Quality Issues with LLMs)
   - 2310.08879 (Critical Review of LLM on Software)
   - 2208.03133 (Assessing Code Quality)

4. **Verification and Testing**:
   - 2302.08468 (LEVER - Verify Code Generation)
   - 2308.16557 (Test Generation)
   - 2401.03065 (CRUXEval - Code Reasoning Benchmark)

5. **Multi-lingual and Large-scale**:
   - 2310.06266 (CodeFuse-13B)
   - 2305.06156 (The Vault - Multilingual Dataset)
   - 2310.11248 (Cross-lingual Code Evaluation)

These papers cover various aspects of code review including static analysis, semantic understanding, bug detection, vulnerability analysis, test generation, and multi-lingual support - all crucial components for building effective AI-based code review systems.