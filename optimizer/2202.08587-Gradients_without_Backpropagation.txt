# 2202.08587.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/optimizer/2202.08587.pdf
# File size: 4389532 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Gradients without Backpropagation
Atılım G ¨unes ¸ Baydin1Barak A. Pearlmutter2Don Syme3Frank Wood4Philip Torr5
Abstract
Using backpropagation to compute gradients of
objective functions for optimization has remained
a mainstay of machine learning. Backpropagation,
or reverse-mode differentiation, is a special case
within the general family of automatic differen-
tiation algorithms that also includes the forward
mode. We present a method to compute gradients
based solely on the directional derivative that one
can compute exactly and efﬁciently via the for-
ward mode. We call this formulation the forward
gradient , an unbiased estimate of the gradient
that can be evaluated in a single forward run of the
function, entirely eliminating the need for back-
propagation in gradient descent. We demonstrate
forward gradient descent in a range of problems,
showing substantial savings in computation and
enabling training up to twice as fast in some cases.
1. Introduction
Backpropagation (Linnainmaa, 1970; Rumelhart et al.,
1985) and gradient-based optimization have been the core
algorithms underlying many recent successes in machine
learning (ML) (Goodfellow et al., 2016; Deisenroth et al.,
2020). It is generally accepted that one of the factors con-
tributing to the recent pace of advance in ML has been the
ease with which differentiable ML code can be implemented
via well engineered libraries such as PyTorch (Paszke et al.,
2019) or TensorFlow (Abadi et al., 2016) with automatic dif-
ferentiation (AD) capabilities (Griewank & Walther, 2008;
Baydin et al., 2018). These frameworks provide the compu-
tational infrastructure on which our ﬁeld is built.
Until recently, all major software frameworks for ML have
been built around the reverse mode of AD, a technique
to evaluate derivatives of numeric code using a two-phase
1Department of Computer Science, University of Oxford
2Department of Computer Science, National University of Ire-
land Maynooth3Microsoft4Computer Science Department, Uni-
versity of British Columbia5Department of Engineering Science,
University of Oxford. Correspondence to: Atılım G ¨unes ¸Baydin
<gunes@robots.ox.ac.uk >.
Under Review.forward–backward algorithm, of which backpropagation is a
special case conventionally applied to neural networks. This
is mainly due to the central role of scalar-valued objectives
in ML, whose gradient with respect to a very large number
of inputs can be evaluated exactly and efﬁciently with a
single evaluation of the reverse mode.
Reverse mode is a member of a larger family of AD algo-
rithms that also includes the forward mode (Wengert, 1964),
which has the favorable characteristic of requiring only a
single forward evaluation of a function (i.e., not involving
any backpropagation) at a signiﬁcantly lower computational
cost. Crucially, forward and reverse modes of AD evalu-
ate different quantities. Given a function f:Rn!Rm,
forward mode evaluates the Jacobian–vector product Jfv,
whereJf2Rmnandv2Rn; and revese mode evaluates
the vector–Jacobian product v|Jf, wherev2Rm. For the
case off:Rn!R(e.g., an objective function in ML), for-
ward mode gives us rfv2R, the directional derivative;
and reverse mode gives us the full gradient rf2Rn.1
From the perspective of AD applied to ML, a “holy grail”
is whether the practical usefulness of gradient descent can
be achieved using only the forward mode, eliminating the
need for backpropagation. This could potentially change the
computational complexity of typical ML training pipelines,
reduce the time and energy costs of training, inﬂuence ML
hardware design, and even have implications regarding the
biological plausibility of backpropagation in the brain (Ben-
gio et al., 2015; Lillicrap et al., 2020). In this work we
present results that demonstrate stable gradient descent over
a range of ML architectures using only forward mode AD.
Contributions
•We deﬁne the “forward gradient”, an estimator of the
gradient that we prove to be unbiased, based on forward
mode AD without backpropagation.
•We implement a forward AD system from scratch in
PyTorch, entirely independent of the reverse AD imple-
mentation already present in this library.
•We use forward gradients in stochastic gradient descent
(SGD) optimization of a range of architectures, and show
1We representrfas a column vector.arXiv:2202.08587v1  [cs.LG]  17 Feb 2022

--- PAGE 2 ---
Gradients without Backpropagation
that a typical modern ML training pipeline can be con-
structed with only forward AD and no backpropagation.
•We compare the runtime and loss performance charac-
teristics of forward gradients and backpropagation, and
demonstrate speedups of up to twice as fast compared
with backpropagation in some cases.
A note on naming: When naming the technique, it is tempt-
ing to adopt names like “forward propagation” or “forward-
prop” to contrast it with backpropagation. We do not use
this name as it is commonly used to refer to the forward
evaluation phase of backpropagation, distinct from forward
AD. We observe that the simple name “forward gradient” is
currently not used in ML, and it also captures the aspect that
we are presenting a drop-in replacement for the gradient.
2. Background
In order to introduce our method, we start by brieﬂy review-
ing the two main modes of automatic differentiation.
2.1. Forward Mode AD

vf()
Jf()vForward
Given a function f:Rn!Rmand the values 2Rn,v2
Rn, forward mode AD computes f()and the Jacobian–
vector product2Jf()v, whereJf()2Rmnis the
Jacobian matrix of all partial derivatives of fevaluated at,
andvis a vector of perturbations.3For the case of f:Rn!
Rthe Jacobian–vector product corresponds to a directional
derivativerf()v, which is the projection of the gradient
rfatonto the direction vector v, representing the rate of
change along that direction.
It is important to note that the forward mode evaluates the
functionfand its Jacobian–vector product Jfvsimulta-
neously in a single forward run. Also note that Jfvis
obtained without having to compute the Jacobian Jf, a fea-
ture sometimes referred to as a matrix-free computation.4
2.2. Reverse Mode AD

v|Jf()f()
vForward
Backward
2Popularized recently as a jvp operation in tensor frameworks
such as JAX (Bradbury et al., 2018).
3Also called “tangents”.
4The full Jacobian Jcan be computed with forward AD using
nforward evaluations of Jei; i= 1;:::n using standard basis
vectorseiso that each forward run gives us a single column of J.Given a function f:Rn!Rmand the values 2Rn,
v2Rm, reverse mode AD computes f()and the vector–
Jacobian product5v|Jf(), whereJf2Rmnis the
Jacobian matrix of all partial derivatives of fevaluated
at, andv2Rmis a vector of adjoints. For the case
off:Rn!Randv= 1, reverse mode computes the
gradient, i.e., the partial derivatives of fw.r.t. allninputs
rf() =h
@f
@1;:::;@f
@ni|
.
Note thatv|Jfis computed in a single forward–backward
evaluation, without having to compute the Jacobian Jf.6
2.3. Runtime Cost
The runtime costs of both modes of AD are bounded by
a constant multiple of the time it takes to run the function
fwe are differentiating (Griewank & Walther, 2008). Re-
verse mode has a higher cost than forward mode, because
it involves data-ﬂow reversal and it needs to keep a record
(a “tape”, stack, or graph) of the results of operations en-
countered in the forward pass, because these are needed
in the evaluation of derivatives in the backward pass that
follows. The memory and computation cost characteris-
tics ultimately depend on the features implemented by the
AD system such as exploiting sparsity (Gebremedhin et al.,
2005) or checkpointing (Siskind & Pearlmutter, 2018).
The cost can be analyzed by assuming computational com-
plexities of elementary operations such as fetches, stores, ad-
ditions, multiplications, and nonlinear operations (Griewank
& Walther, 2008). Denoting the time it takes to evaluate the
original function fasruntime (f), we can express the time
taken by the forward and reverse modes as Rfruntime (f)
andRbruntime (f)respectively. In practice, Rfis typi-
cally between 1 and 3, and Rbis typically between 5 and 10
(Hasco ¨et, 2014), but these are highly program dependent.
Note that in ML the original function corresponds to the ex-
ecution of the ML code without any derivative computation
or training, i.e., just evaluating a given model with input
data.7We will call this “base runtime” in this paper.
3. Method
3.1. Forward Gradients
Deﬁnition 1. Given a function f:Rn!R, we deﬁne the
“forward gradient” g:Rn!Rnas
g() = (rf()v)v; (1)
5Popularized recently as a vjp operation in tensor frameworks
such as JAX (Bradbury et al., 2018).
6The full Jacobian Jcan be computed with reverse AD using
mevaluations of e|
iJ; i= 1;:::m using standard basis vectors
eiso that each run gives us a single row of J.
7Sometimes called “inference” by practitioners.

--- PAGE 3 ---
Gradients without Backpropagation
where2Rnis the point at which we are evaluating
the gradient,v2Rnis a perturbation vector taken as a
multivariate random variable vp(v)such thatv’s scalar
components viare independent and have zero mean and
unit variance for all i, andrf()v2Ris the directional
derivative of fat pointin directionv.
We ﬁrst talk brieﬂy about the intuition that led to this deﬁni-
tion, before showing that g()is an unbiased estimator of
the gradientrf()in Section 3.2.
As explained in Section 2, forward mode gives us the direc-
tional derivativerf()v=P
i@f
@ividirectly, without
having to compute rf. Computingrfusing only forward
mode is possible by evaluating fforwardntimes with di-
rection vectors taken as standard basis (or one-hot) vectors
ei2Rn;i= 1:::n , whereeidenotes a vector with a 1 in
theith coordinate and 0s elsewhere. This allows the evalua-
tion of the sensitivity of fw.r.t. each input@f
@iseparately,
which when combined give us the gradient rf.
In order to have any chance of runtime advantage over back-
propagation, we need to work with a single run of the for-
ward mode per optimization iteration, not nruns.8In a
single forward run, we can interpret the direction vas a
weight vector in a weighted sum of sensitivities w.r.t. each
input, that isP
i@f
@ivi, albeit without the possibility of dis-
tinguishing the contribution of each iin the ﬁnal total. We
therefore use the weight vector vto attribute the overall sen-
sitivity back to each individual parameter i, proportional
to the weight viof each parameter i(e.g., a parameter with
a small weight had a small contribution and a large one had
a large contribution in the total sensitivity).
In summary, each time the forward gradient is evaluated, we
simply do the following:
•Sample a random perturbation vector vp(v), which
has the same size with f’s argument.
•Runfvia forward-mode AD, which evaluates f()and
rf()vsimultaneously in the same single forward run,
without having to compute rfat all in the process.
The directional derivative obtained, rf()v, is a scalar,
and is computed exactly by AD (not an approximation).
•Multiply the scalar directional derivative rf()vwith
vectorvand obtaing(), the forward gradient.
Figure 1 illustrates the process showing several evaluations
of the forward gradient for the Beale function. We see how
perturbations vk(orange) transform into forward gradients
(rfvk)vk(blue) fork2[1;5], sometimes reversing the
8This requirement can be relaxed depending on the problem
setting and we would expect the gradient estimation to get better
with more forward runs per optimization iteration.
4
 2
 0 2 4
x4
2
024y
Perturbations vk
Forward gradients (fvk)vk
Forward gradient (empirical mean)
True gradient f
Figure 1. Five samples of forward gradient, the empirical mean
of these ﬁve samples, and the true gradient for the Beale func-
tion (Section 5.1) at x= 1:5;y= 0:1. Star marks the global
minimum.
sense to point towards the true gradient (red) while being
constrained in orientation. The green arrow shows a Monte
Carlo gradient estimate via averaged forward gradients, i.e.,
1
KPK
k=1(rfvk)vkE[(rfv)v].
3.2. Proof of Unbiasedness
Theorem 1. The forward gradient g()is an unbiased
estimator of the gradient rf().
Proof. We start with the directional derivative of fevalu-
ated atin directionvwritten out as follows
d(;v) =rf()v=X
i@f
@ivi
=@f
@1v1+@f
@2v2++@f
@nvn: (2)
We then expand the forward gradient gin Eq. (1) as
g() =d(;v)v
=2
666664@f
@1v2
1+@f
@2v1v2++@f
@nv1vn
@f
@1v1v2+@f
@2v2
2++@f
@nv2vn
...
@f
@1v1vn+@f
@2v2vn++@f
@nv2
n3
777775
and note that the components of ghave the following form
gi() =@f
@iv2
i+X
j6=i@f
@jvivj: (3)
The expected value of each component giis
E[gi()] =E2
4@f
@iv2
i+X
j6=i@f
@jvivj3
5

--- PAGE 4 ---
Gradients without Backpropagation
=E@f
@iv2
i
+E2
4X
j6=i@f
@jvivj3
5
=E@f
@iv2
i
+X
j6=iE@f
@jvivj
=@f
@iE
v2
i
+X
j6=i@f
@jE[vivj] (4)
The ﬁrst expected value in Eq. (4) is of a squared random
variable and all expectations in the summation term are of
two independent and identically distributed random vari-
ables multiplied.
Lemma 1. The expected value of a random variable v
squared E[v2] = 1 when E[v] = 0 andVar[v] = 1 .
Proof. Variance is Var[v] =E
(v E[v])2
=E
v2
 
E[v]2. Rearranging and substituting E[v] = 0 andVar[v] =
1, we get E
v2
=E[v]2+ Var[v] = 0 + 1 = 1 :
Lemma 2. The expected value of two i.i.d. random vari-
ables multiplied E[vivj] = 0 whenE[vi] = 0 orE[vj] = 0 .
Proof. For i.i.d.viandvjthe expected value E[vivj] =
E[vi]E[vj] = 0 whenE[vi] = 0 orE[vj] = 0 .
Using Lemmas 1 and 2, Eq. (4) reduces to
E[gi()] =@f
@i(5)
and therefore
E[g()] =rf(): (6)
3.3. Forward Gradient Descent
We construct a forward gradient descent (FGD) algorithm by
replacing the gradient rfin standard GD with the forward
gradientg(Algorithm 1). In practice we use a mini-batch
stochastic version of this where ftchanges per iteration as
it depends on each mini-batch of data used during training.
We note that the directional derivative dtin Algorithm 1 can
have positive or negative sign. When the sign is negative, the
forward gradient gtcorresponds to backtracking from the
direction ofvt, or reversing the direction to point towards
the true gradient in expectation. Figure 1 shows two vk
samples exemplifying this behavior.
In this paper we limit our scope to FGD to clearly study this
fundamental algorithm and compare it to standard backprop-
agation, without confounding factors such as momentum or
adaptive learning rate schemes. We believe that extensions
of the method to other families of gradient-based optimiza-
tion algorithms are possible.Algorithm 1 Forward gradient descent (FGD)
Require:: learning rate
Require:f: objective function
Require:0: initial parameter vector
t 0 .Initialize
whiletnot converged do
t t+ 1
vtN (0;I) .Sample perturbation
Note: the following computes ftanddtsimultaneously and
without having to compute rfin the process
ft; dt f(t);rf(t)v.Forward AD (Section 3.1)
gt vtdt .Forward gradient
t+1 t gt .Parameter update
end while
returnt
3.4. Choice of Direction Distribution
As shown by the proof in Section 3.2, the multivariate dis-
tributionp(v)from which direction vectors vare sampled
must have two properties: (1) the components must be inde-
pendent from each other (e.g., a diagonal Gaussian) and (2)
the components must have zero mean and unit variance.
In our experiments we use the multivariate standard normal
as the direction distribution p(v)so thatv N (0;I),
that is,viN (0;1)are independent for all i. We leave
exploring other admissible distributions for future work.
4. Related Work
The idea of performing optimization by the use of random
perturbations, thus avoiding adjoint computations, is the in-
tuition behind a variety of approaches, including simulated
annealing (Kirkpatrick et al., 1983), stochastic approxima-
tion (Spall et al., 1992), stochastic convex optimization
(Nesterov & Spokoiny, 2017; Dvurechensky et al., 2021),
and correlation-based learning methods (Barto et al., 1983),
which lend themselves to efﬁcient hardware implementation
(Alspector et al., 1988). Our work here falls in the general
class of so-called weight perturbation methods; see Pearl-
mutter (1994, §4.4) for an overview along with a description
of a method for efﬁciently gathering second-order informa-
tion during the perturbative process, which suggests that
accelerated second-order variants of the present method may
be feasible. Note that our method is novel in avoiding the
truncation error of previous weight perturbation approaches
by using AD rather than small but ﬁnite perturbations, thus
completely avoiding the method of divided differences and
its associated numeric issues.
In neural network literature, alternatives to backpropagation
proposed include target propagation (LeCun, 1986; 1987;
Bengio, 2014; 2020; Meulemans et al., 2020), a technique
that propagates target values rather than gradients backwards
between layers. For recurrent neural networks (RNNs), vari-

--- PAGE 5 ---
Gradients without Backpropagation
ous approaches to the online credit assignment problem have
features in common with forward mode AD (Pearlmutter,
1995). An early example is the real-time recurrent learn-
ing (RTRL) algorithm (Williams & Zipser, 1989) which
accumulates local sensitivities in an RNN during forward
execution, in a manner similar to forward AD. A very recent
example in the RTRL area is an anonymous submission
we identiﬁed at the time of drafting this manuscript, where
the authors are using directional derivatives to improve sev-
eral gradient estimators, e.g., synthetic gradients (Jaderberg
et al., 2017), ﬁrst-order meta-learning (Nichol et al., 2018),
as applied to RNNs (Anonymous, 2022).
Coordinate descent (CD) algorithms (Wright, 2015) have a
structure where in each optimization iteration only a single
component@f
@iof the gradientrfis used compute an up-
date. Nesterov (2012) provides an extension of CD called
random coordinate descent (RCD), based on coordinate di-
rectional derivatives, where the directions are constrained to
randomly chosen coordinate axes in the function’s domain
as opposed to arbitrary directions we use in our method.
A recent use of RCD is by Ding & Li (2021) in Langevin
Monte Carlo sampling, where the authors report no com-
putational gain as the RCD needs to be run multiple times
per iteration in order to achieve a preset error tolerance.
The SEGA (SkEtched GrAdient) method by Hanzely et al.
(2018) is based on gradient estimation via random linear
transformations of the gradient that is called a “sketch” com-
puted using ﬁnite differences. Jacobian sketching by Gower
et al. (2018b) is designed to provide good estimates of the
Jacobian, in a manner similar to how quasi-Newton methods
update Hessian estimates (Gower et al., 2018a).
Lastly there are other, and more distantly related, ap-
proaches concerning gradient estimation such as synthetic
gradients (Jaderberg et al., 2017), motivated by a need to
break the sequential forward–backward structure of back-
propagation, and Monte Carlo gradient estimation (Mo-
hamed et al., 2020), where the gradient of an expectation
of a function is computed with respect to the parameters
deﬁning the distribution that is integrated.
For a review of the origins of reverse mode AD and back-
propagation, we refer the interested readers to Schmidhuber
(2020) and Griewank (2012).
5. Experiments
We implement forward AD in PyTorch to perform the experi-
ments (details given in Section 6). In all experiments, except
in Section 5.1, we use learning rate decay with i=0e ik,
whereiis the learning rate at iteration i,0is the initial
learning rate, and k= 10 4. In all experiments we run
forward gradients and backpropagation for an equal number
of iterations. We run the code with CUDA on a Nvidia Titan
XP GPU and use a minibatch size of 64.First we look at test functions for optimization, and compare
the behavior of forward gradient and backpropagation in the
R2space where we can plot and follow optimization trajec-
tories. We then share results of experiments with training
ML architectures of increasing complexity. We measured
no practical difference in memory usage between the two
methods (less than 0.1% difference in each experiment).
5.1. Optimization Trajectories of Test Functions
In Figure 2 we show the results of experiments with
•the Beale function, f(x;y) = (1:5 x+xy)2+(2:25 
x+xy2)2+ (2:625 x+xy3)2
•and the Rosenbrock function, f(x;y) = (a x)2+b(y 
x2)2, wherea= 1;b= 100 .
Note that forward gradient and backpropagation have
roughly similar time complexity in these cases, forward
gradient being slightly faster per iteration. Crucially, we
see that forward gradient steps behave the same way as
backpropagation in expectation, as seen in loss per iteration
(leftmost) and optimization trajectory (rightmost) plots.
5.2. Empirical Measures of Complexity
In order to compare the two algorithms applied to ML prob-
lems in the rest of this section, we use several measures.
For runtime comparison we use the RfandRbfactors de-
ﬁned in Section 2.3. In order to compute these factors, we
measure runtime (f)as the time it takes to run a given archi-
tecture with a sample minibatch of data and compute the
loss, without performing any derivative computation and
parameter update. Note that in the measurements of Rfand
Rb, the time taken by gradient descent (parameter updates)
are included, in addition to the time spent in computing the
derivatives. We also introduce the ratio Rf=Rbas a measure
of the runtime cost of the forward gradient relative to the
cost of backpropagation in a given architecture.
In order to compare loss performance, we deﬁne Tbas the
time at which the lowest validation loss is achieved by back-
propagation (averaged over runs). Tfis the time the same
validation loss is achieved by the forward gradient for the
same architecture. The Tf=Tbratio gives us a measure of the
time it takes for the forward mode to achieve the minimum
validation loss relative to the time taken by backpropagation.
5.3. Logistic Regression
Figure 3 gives the results of several runs of multinomial lo-
gistic regression for MNIST digit classiﬁcation. We observe
that the runtime cost of the forward gradient and backprop-
agation relative to the base runtime are Rf= 2:435and

--- PAGE 6 ---
Gradients without Backpropagation
0 200 400 600 800 1000
Iterations104
103
102
101
100101f(x)Forward grad
Backprop
0.00 0.05 0.10 0.15 0.20
Time (s)104
103
102
101
100101f(x)
0 200 400 600 800 1000
Iterations0.000.050.100.150.20Time (s)
0 1 2 3
x0.4
0.2
0.00.20.40.6yBeale, lr=0.01, lr_decay=0.0, n=1,000
0 5000 10000 15000 20000 25000
Iterations104
102
100102f(x)Forward grad
Backprop
0 1 2 3 4
Time (s)104
102
100102f(x)
0 5000 10000 15000 20000 25000
Iterations01234Time (s)
1.0
 0.5
 0.0 0.5 1.0
x0.2
0.00.20.40.60.81.0yRosenbrock, lr=0.0005, lr_decay=0.0, n=25,000
Figure 2. Comparison of forward gradient and backpropagation in test functions, showing ten independent runs. Top row: Beale function,
learning rate 0.01. Bottom row: Rosenbrock function. Learning rate 510 4. Rightmost column: Optimization trajectories in each
function’s domain, shown over contour plots of the functions. Star symbol marks the global minimum in the contour plots.
0 5000 10000 15000 20000 25000
Iterations0.51.01.52.02.5LossForward grad (train)
Forward grad (valid)
Backprop (train)
Backprop (valid)
0 50 100 150 200 250 300 350
Time (s)0.51.01.52.02.5LossTf = 197.326 s
Tb = 356.756 s
Tf/Tb = 0.553
0 5000 10000 15000 20000 25000
Iterations0100200300Time (s)Rf = 2.435
Rb = 4.389
Rf/Rb = 0.555
Base runtimelogreg, sgd, lr=0.0001, lr_decay=0.0001, batch_size=64
Figure 3. Comparison of forward gradient and backpropagation in logistic regression, showing ﬁve independent runs. Learning rate 10 4.
Rb= 4:389, which are compatible with what one would
expect from a typical AD system (Section 2.3). The ratios
Rf=Rb= 0:555andTf=Tb= 0:553indicate that the for-
ward gradient is roughly twice as fast as backpropagation in
both runtime and loss performance. In this simple problem
these ratios coincide as both techniques have nearly iden-
tical behavior in the loss per iteration space, meaning that
the runtime beneﬁt is reﬂected almost directly in the loss
per time space. In more complex models in the following
subsections we will see that the relative loss and runtime
ratios can be different in practice.
5.4. Multi-Layer Neural Networks
Figure 4 shows two experiments with a multi-layer neural
network (NN) for MNIST classiﬁcation with different learn-
ing rates. The architecture we use has three fully-connected
layers of size 1024, 1024, 10, with ReLU activation after the
ﬁrst two layers. In this model architecture, we observe the
runtime costs of the forward gradient and backpropagation
relative to the base runtime as Rf= 2:468andRb= 4:165,
and the relative measure Rf=Rb= 0:592on average. These
are roughly the same with the logistic regression case.The top row (learning rate 210 5) shows a result where
forward gradient and backpropagation behave nearly identi-
cal in loss per iteration (leftmost plot), resulting in a Tf=Tb
ratio close to Rf=Rb. We show this result to communicate
an example where the behavior is similar to the one we
observed for logistic regression, where the loss per iteration
behavior between the techniques are roughly the same and
the runtime beneﬁt is the main contributing factor in the loss
per time behavior (second plot from the left).
Interestingly, in the second experiment (learning rate 2
10 4) we see that forward gradient achieves faster descent
in the loss per iteration plot. We believe that this behav-
ior is due to the different nature of stochasticity between
the regular SGD (backpropagation) and the forward SGD
algorithms, and we speculate that the noise introduced by
forward gradients might be beneﬁcial in exploring the loss
surface. When we look at the loss per time plot, which
also incorporates the favorable runtime of the forward mode,
we see a loss performance metric Tf=Tbvalue of 0.211,
representing a case that is more than four times as fast as
backpropagation in achieving the reference validation loss.

--- PAGE 7 ---
Gradients without Backpropagation
0 5000 10000 15000 20000
Iterations2.152.202.252.302.352.402.45LossForward grad (train)
Forward grad (valid)
Backprop (train)
Backprop (valid)
0 100 200 300
Time (s)2.152.202.252.302.352.402.45LossTf = 165.842 s
Tb = 292.359 s
Tf/Tb = 0.567
0 5000 10000 15000 20000
Iterations050100150200250300Time (s)Rf = 2.329
Rb = 3.971
Rf/Rb = 0.586
Base runtime
0 100 200 300
Time (s)0.100.150.200.250.30Validation acuracymlp, sgd, lr=2e-05, lr_decay=0.0001, batch_size=64
0 5000 10000 15000 20000
Iterations0.51.01.52.02.5LossForward grad (train)
Forward grad (valid)
Backprop (train)
Backprop (valid)
0 50 100 150 200 250 300
Time (s)0.51.01.52.02.5LossTf = 68.014 s
Tb = 296.957 s
Tf/Tb = 0.229
0 5000 10000 15000 20000
Iterations050100150200250300Time (s)Rf = 2.607
Rb = 4.359
Rf/Rb = 0.598
Base runtime
0 50 100 150 200 250 300
Time (s)0.20.40.60.8Validation acuracymlp, sgd, lr=0.0002, lr_decay=0.0001, batch_size=64
Figure 4. Comparison of forward gradient and backpropagation for the multi-layer NN, showing two learning rates. Top row: learning
rate210 5. Bottom row: learning rate 210 4. Showing ﬁve independent runs per experiment.
0 10000 20000 30000 40000
Iterations0.51.01.52.02.5LossForward grad (train)
Forward grad (valid)
Backprop (train)
Backprop (valid)
0 200 400 600
Time (s)0.51.01.52.02.5LossTf = 362.247 s
Tb = 704.369 s
Tf/Tb = 0.514
0 10000 20000 30000 40000
Iterations0200400600Time (s)Rf = 1.434
Rb = 2.211
Rf/Rb = 0.649
Base runtime
0 200 400 600
Time (s)0.20.40.60.8Validation acuracycnn4b, sgd, lr=0.0002, lr_decay=0.0001, batch_size=64
Figure 5. Comparison of forward gradient and backpropagation for the CNN. Learning rate 210 4. Showing ﬁve independent runs.
5.5. Convolutional Neural Networks
In Figure 5 we show a comparison between the forward
gradient and backpropagation for a convolutional neural
network (CNN) for the same MNIST classiﬁcation task. The
CNN has four convolutional layers with 33kernels and
64 channels, followed by two linear layers of sizes 1024 and
10. All convolutions and the ﬁrst linear layer are followed
by ReLU activation and there are two max-pooling layers
with22kernel after the second and fourth convolutions.
In this architecture we observe the best forward AD perfor-
mance with respect to the base runtime, where the forward
mode hasRf= 1:434representing an overhead of only
43% on top of the base runtime. Backpropagation with
Rb= 2:211is very close to the ideal case one can expect
from a reverse AD system, taking roughly double the time.
Rf=Rb= 0:649represents a signiﬁcant beneﬁt for the for-
ward AD runtime with respect to backpropagation. In loss
space, we get a ratio Tf=Tb= 0:514which shows that for-
ward gradients are close to twice as fast as backpropagation
in achieving the reference level of validation loss.
5.6. Scalability
The results in the previous subsections demonstrate that•training without backpropagation can feasibly work
within a typical ML training pipeline and do so in a
computationally competitive way, and
•forward AD can even beat backpropagation in loss de-
crease per training time for the same choice of hyperpa-
rameters (learning rate and learning rate decay).
In order to investigate whether these results will scale to
larger NNs with more layers, we measure runtime cost and
memory usage as a function of NN size. In Figure 6 we
show the results for the MLP architecture (Section 5.4),
where we run experiments with an increasing number of
layers in the range [1;100]. The linear layers are of size
1,024, with no bias. We use a mini-batch size 64 as before.
Looking at the cost relative to the base runtime, which
also changes as a function of the number of layers, we
see that backpropagation remains within Rb2[4;5]and
forward gradient remains within Rf2[3;4]for a large
proportion of the experiments. We also observe that forward
gradients remain favorable for the whole range of layer sizes
considered, with the Rf=Rbratio staying below 0.6 up to ten
layers and going slightly over 0.8 at 100 layers. Importantly,
there is virtually no difference in memory consumption
between the two methods.

--- PAGE 8 ---
Gradients without Backpropagation
345Runtime costForward grad Rf
Backprop Rb
0.40.60.81.0Runtime costRelative runtime cost Rf/Rb
1510 20 30 40 50 60 70 80 90 100
Number of layers0.00.51.0Memory use (bytes)1e9
Forward grad
Backpropmlp_no_bias, sgd, layer_size=1,024, batch_size=64, runs=10, n=100, num_layers=[1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
Figure 6. Comparison of how the runtime cost and memory usage
of forward gradients and backpropagation scale as a function NN
depth for the MLP architecture where each layer is of size 1024.
Showing mean and standard deviation over ten independent runs.
6. Implementation
We implement a forward-mode AD system in Python and
base this on PyTorch tensors in order to enable a fair com-
parison with a typical backpropagation pipeline in PyTorch,
which is widely used by the ML community.9We release
our implementation publicly.10
Our forward-mode AD engine is implemented from scratch
using operator overloading and non-differentiable PyTorch
tensors ( requires grad=False ) as a building block.
This means that our forward AD implementation does not
use PyTorch’s reverse-mode implementation (called “au-
tograd”) and computation graph. We produce the back-
propagation results in experiments using PyTorch’s ex-
isting reverse-mode code ( requires grad=True and
.backward() ) as usual.
Note that empirical comparisons of the relative runtimes of
forward- and reverse-mode AD are highly dependent on the
implementation details in a given system and would show
differences across different code bases. When implement-
ing the forward mode of tensor operations common in ML
(e.g., matrix multiplication, convolutions), we identiﬁed op-
portunities to the make forward AD operations even more
efﬁcient (e.g., stacking channels of primal and derivative
parts of tensors in a convolution). Note that the implemen-
9We also experimented with the forward mode implementation
in JAX (Bradbury et al., 2018) but decided to base our implemen-
tation on PyTorch due to its maturity and simplicity allowing us to
perform a clear comparison.
10To be shared in the upcoming revision.tation we use in this paper does not currently have these.
We expect the forward gradient performance to improve
even further as high-quality forward-mode implementations
ﬁnd their way into mainstream ML libraries and get tightly
integrated into tensor code.
Another implementation approach that can enable a straight-
forward application of forward gradients to existing code
can be based on the complex-step method (Martins et al.,
2003), a technique that can approximate directional deriva-
tives with nothing but basic support for complex numbers.
7. Conclusions
We have shown that a typical ML training pipeline can be
constructed without backpropagation, using only forward
AD, while still being computationally competitive. We ex-
pect this contribution to ﬁnd use in distributed ML training,
which is outside the scope of this paper. Furthermore, the
runtime results we obtained with our forward AD prototype
in PyTorch are encouraging and we are cautiously optimistic
that they might be the ﬁrst step towards signiﬁcantly decreas-
ing the time taken to train ML architectures, or alternatively,
enabling the training of more complex architectures with a
given compute budget. We are excited to have the results
conﬁrmed and studied further by the research community.
The work presented here is the basis for several directions
that we would like to follow. In particular, we are interested
in working on gradient descent algorithms other than SGD,
such as SGD with momentum, and adaptive learning rate al-
gorithms such as Adam (Kingma & Ba, 2015). In this paper
we deliberately excluded these to focus on the most isolated
and clear case of SGD, in order to establish the technique
and a baseline. We are also interested in experimenting
with other ML architectures. The components used in our
experiments (i.e., linear and convolutional layers, pooling,
ReLU nonlinearity) are representative of the building blocks
of many current architectures in practice, and we expect the
results to apply to these as well.
Lastly, in the longer term we are interested in seeing whether
the forward gradient algorithm can contribute to the math-
ematical understanding of the biological learning mecha-
nisms in the brain, as backpropagation has been historically
viewed as biologically implausible as it requires a precise
backward connectivity (Bengio et al., 2015; Lillicrap et al.,
2016; 2020). In this context, one way to look at the role of
the directional derivative in forward gradients is to interpret
it as the feedback of a single global scalar quantity that is
identical for all the computation nodes in the network.
We believe that forward AD has computational characteris-
tics that are ripe for exploration by the ML community, and
we expect that its addition to the conventional ML infrastruc-
ture will lead to major breakthroughs and new approaches.

--- PAGE 9 ---
Gradients without Backpropagation
References
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,
J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.
Tensorﬂow: A system for large-scale machine learning. In
12th USENIX Symposium on Operating Systems Design
and Implementation (OSDI 16) , pp. 265–283, 2016.
Alspector, J., Allen, R., Hu, V ., and Satyanarayana, S.
Stochastic learning networks and their electronic im-
plementation. In Anderson, D. (ed.), Neural Informa-
tion Processing Systems . American Institute of Physics,
1988. URL https://proceedings.neurips.cc/paper/1987/
ﬁle/f033ab37c30201f73f142449d037028d-Paper.pdf.
Anonymous. Learning by directional gradient descent.
InSubmitted to The Tenth International Conference on
Learning Representations , 2022. URL https://openreview.
net/forum?id=5i7lJLuhTm. Under Review.
Barto, A. G., Sutton, R. S., and Anderson, C. W. Neuronlike
adaptive elements that can solve difﬁcult learning control
problems. IEEE Transactions on Systems, Man, and
Cybernetics , SMC-13(5):834–846, 1983. doi: 10.1109/
TSMC.1983.6313077.
Baydin, A. G., Pearlmutter, B. A., Radul, A. A., and Siskind,
J. M. Automatic differentiation in machine learning: a
survey. Journal of Machine Learning Research (JMLR) ,
18(153):1–43, 2018. URL http://jmlr.org/papers/v18/17-
468.html.
Bengio, Y . How auto-encoders could provide credit as-
signment in deep networks via target propagation. arXiv
preprint arXiv:1407.7906 , 2014.
Bengio, Y . Deriving differential target propagation
from iterating approximate inverses. arXiv preprint
arXiv:2007.15139 , 2020.
Bengio, Y ., Lee, D.-H., Bornschein, J., Mesnard, T., and Lin,
Z. Towards biologically plausible deep learning. arXiv
preprint arXiv:1502.04156 , 2015.
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary,
C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J.,
Wanderman-Milne, S., and Zhang, Q. JAX: composable
transformations of Python+NumPy programs, 2018. URL
http://github.com/google/jax.
Deisenroth, M. P., Faisal, A. A., and Ong, C. S. Mathemat-
ics for Machine Learning . Cambridge University Press,
2020.
Ding, Z. and Li, Q. Langevin Monte Carlo: random coordi-
nate descent and variance reduction. Journal of Machine
Learning Research , 22(205):1–51, 2021.Dvurechensky, P., Gorbunov, E., and Gasnikov, A. An accel-
erated directional derivative method for smooth stochastic
convex optimization. European Journal of Operational
Research , 290(2):601–621, 2021.
Gebremedhin, A. H., Manne, F., and Pothen, A. What
color is your Jacobian? Graph coloring for computing
derivatives. SIAM Review , 47(4):629–705, 2005.
Goodfellow, I., Bengio, Y ., and Courville, A. Deep Learning .
MIT Press, 2016. http://www.deeplearningbook.org.
Gower, R., Hanzely, F., Richtarik, P., and Stich, S. U. Ac-
celerated stochastic matrix inversion: General theory and
speeding up bfgs rules for faster second-order optimiza-
tion. In Bengio, S., Wallach, H., Larochelle, H., Grauman,
K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances
in Neural Information Processing Systems , volume 31.
Curran Associates, Inc., 2018a.
Gower, R. M., Richt ´arik, P., and Bach, F. Stochastic quasi-
gradient methods: Variance reduction via Jacobian sketch-
ing. arXiv preprint arXiv:1805.02632 , 2018b.
Griewank, A. Who invented the reverse mode of differen-
tiation? Documenta Mathematica , Extra V olume ISMP:
389–400, 2012.
Griewank, A. and Walther, A. Evaluating Derivatives:
Principles and Techniques of Algorithmic Differentiation .
SIAM, 2008.
Hanzely, F., Mishchenko, K., and Richt ´arik, P. SEGA:
Variance reduction via gradient sketching. arXiv preprint
arXiv:1809.03054 , 2018.
Hasco ¨et, L. Adjoints by automatic differentiation. Advanced
Data Assimilation for Geosciences: Lecture Notes of the
Les Houches School of Physics: Special Issue, June 2012 ,
(2012):349, 2014.
Jaderberg, M., Czarnecki, W. M., Osindero, S., Vinyals, O.,
Graves, A., Silver, D., and Kavukcuoglu, K. Decoupled
neural interfaces using synthetic gradients. In Interna-
tional Conference on Machine Learning , pp. 1627–1635.
PMLR, 2017.
Kingma, D. P. and Ba, J. Adam: A method for stochastic
optimization. In International Conference on Learning
Representations (ICLR) , 2015.
Kirkpatrick, S., Gelatt, C. D., and Vecchi, M. P. Opti-
mization by simulated annealing. Science , 220(4598):
671–680, 1983.
LeCun, Y . Learning process in an asymmetric threshold
network. In Disordered Systems and Biological Organi-
zation , pp. 233–240. Springer, 1986.

--- PAGE 10 ---
Gradients without Backpropagation
LeCun, Y . PhD thesis: Modeles connexionnistes de
l’apprentissage (connectionist learning models) . Uni-
versite P. et M. Curie (Paris 6), June 1987.
Lillicrap, T. P., Cownden, D., Tweed, D. B., and Akerman,
C. J. Random synaptic feedback weights support error
backpropagation for deep learning. Nature Communica-
tions , 7(1):1–10, 2016.
Lillicrap, T. P., Santoro, A., Marris, L., Akerman, C. J.,
and Hinton, G. Backpropagation and the brain. Nature
Reviews Neuroscience , 21(6):335–346, 2020.
Linnainmaa, S. The representation of the cumulative round-
ing error of an algorithm as a Taylor expansion of the
local rounding errors. Master’s Thesis (in Finnish), Univ.
Helsinki , pp. 6–7, 1970.
Martins, J. R., Sturdza, P., and Alonso, J. J. The complex-
step derivative approximation. ACM Transactions on
Mathematical Software (TOMS) , 29(3):245–262, 2003.
Meulemans, A., Carzaniga, F., Suykens, J., Sacramento,
J., and Grewe, B. F. A theoretical framework for target
propagation. Advances in Neural Information Processing
Systems , 33:20024–20036, 2020.
Mohamed, S., Rosca, M., Figurnov, M., and Mnih, A.
Monte Carlo gradient estimation in machine learning.
Journal of Machine Learning Research , 21(132):1–62,
2020.
Nesterov, Y . Efﬁciency of coordinate descent methods on
huge-scale optimization problems. SIAM Journal on
Optimization , 22(2):341–362, 2012.
Nesterov, Y . and Spokoiny, V . Random gradient-free mini-
mization of convex functions. Foundations of Computa-
tional Mathematics , 17(2):527–566, 2017.
Nichol, A., Achiam, J., and Schulman, J. On
ﬁrst-order meta-learning algorithms. arXiv preprint
arXiv:1803.02999 , 2018.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,
L., et al. Pytorch: An imperative style, high-performance
deep learning library. Advances in Neural Information
Processing Systems , 32:8026–8037, 2019.
Pearlmutter, B. A. Fast exact multiplication by the Hessian.
Neural Computation , 6(1):147–60, 1994. doi: 10.1162/
neco.1994.6.1.147.
Pearlmutter, B. A. Gradient calculations for dynamic recur-
rent neural networks: A survey. IEEE Transactions on
Neural Networks , 6(5):1212–1228, 1995.Rumelhart, D. E., Hinton, G. E., and Williams, R. J. Learn-
ing internal representations by error propagation. Tech-
nical report, California Univ San Diego La Jolla Inst for
Cognitive Science, 1985.
Schmidhuber, J. Who invented backpropagation?
2020. URL https://people.idsia.ch/ juergen/who-
invented-backpropagation.html.
Siskind, J. M. and Pearlmutter, B. A. Divide-and-conquer
checkpointing for arbitrary programs with no user an-
notation. Optimization Methods and Software , 33(4-6):
1288–1330, 2018.
Spall, J. C. et al. Multivariate stochastic approximation
using a simultaneous perturbation gradient approximation.
IEEE Transactions on Automatic Control , 37(3):332–341,
1992.
Wengert, R. E. A simple automatic derivative evaluation
program. Communications of the ACM , 7(8):463–464,
1964.
Williams, R. J. and Zipser, D. A learning algorithm for con-
tinually running fully recurrent neural networks. Neural
Computation , 1(2):270–280, 1989.
Wright, S. J. Coordinate descent algorithms. Mathematical
Programming , 151(1):3–34, 2015.
