# 2311.04163.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/optimizer/2311.04163.pdf
# File size: 9769723 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Outliers with Opposing Signals Have an
Outsized Effect on Neural Network Optimization
Elan Rosenfeld
Carnegie Mellon University
elan@cmu.eduAndrej Risteski
Carnegie Mellon University
aristesk@andrew.cmu.edu
Abstract
We identify a new phenomenon in neural network optimization which arises from the interaction of depth
and a particular heavy-tailed structure in natural data. Our result offers intuitive explanations for several
previously reported observations about network training dynamics. In particular, it implies a conceptually
new cause for progressive sharpening and the edge of stability; we also highlight connections to other concepts
in optimization and generalization including grokking, simplicity bias, and Sharpness-Aware Minimization.
Experimentally, we demonstrate the significant influence of paired groups of outliers in the training
data with strong opposing signals : consistent, large magnitude features which dominate the network output
throughout training and provide gradients which point in opposite directions. Due to these outliers, early
optimization enters a narrow valley which carefully balances the opposing groups; subsequent sharpening
causes their loss to rise rapidly, oscillating between high on one group and then the other, until the overall
loss spikes. We describe how to identify these groups, explore what sets them apart, and carefully study their
effect on the network’s optimization and behavior. We complement these experiments with a mechanistic
explanation on a toy example of opposing signals and a theoretical analysis of a two-layer linear network
on a simple model. Our finding enables new qualitative predictions of training behavior which we confirm
experimentally. It also provides a new lens through which to study and improve modern training practices for
stochastic optimization, which we highlight via a case study of Adam versus SGD.
1 Introduction
There is a steadily growing list of intriguing properties of neural network (NN) optimization which
are not readily explained by classical tools from optimization. Likewise, we have varying degrees of
understanding of the mechanistic causes for each. Extensive efforts have led to possible explanations
for the effectiveness of Adam (Kingma and Ba, 2014), Batch Normalization (Ioffe and Szegedy, 2015)
and other tools for successful training—but the evidence is not always entirely convincing, and there
is certainly little theoretical understanding. Other findings, such as grokking (Power et al., 2022) or
the edge of stability (Cohen et al., 2021), do not have immediate practical implications but provide
new ways to study what sets NN optimization apart. These phenomena are typically considered in
isolation—though they are not completely disparate, it is unknown what specific underlying causes
they may share. Clearly, a better understanding of NN training dynamics in a specific context can
lead to algorithmic improvements (Chen et al., 2021); this suggests that any commonality will be a
valuable tool for further investigation.
In this work, we identify a phenomenon in NN optimization which offers a new perspective on
many of these prior observations and which we hope will contribute to a deeper understanding of
how they may be connected. While we do not (and do not claim to) give a complete explanation,
we present strong qualitative and quantitative evidence for a single high-level idea—one which
naturally fits into several existing narratives and suggests a more coherent picture of their origin.
1arXiv:2311.04163v1  [cs.LG]  7 Nov 2023

--- PAGE 2 ---
0 60123456Background Color
110 170Lots of Red:
Auto vs. Other
2750 2810Vehicular Features:
Auto vs. Truck
3080 3140Grass Texture:
Frog vs. Other
Train Loss (Group 1)
Train Loss (Group 2)
Full Train Loss
0.0 0.2 0.4 0.6 0.8 1.0
Iteration0.00.20.40.60.81.0Loss
0 250
20Group 1
0 250
20Background Color
0 250
20
0 250
20Group 2
0 250
20
0 250
20
0 250
20Group 1
0 250
20Vehicular Features
0 250
20
0 250
20Group 2
0 250
20
0 250
20
0 250
20Group 1
0 250
20Lots of Red
0 250
20
0 250
20Group 2
0 250
20
0 250
20
0 250
20Group 1
0 250
20Grass Texture
0 250
20
0 250
20Group 2
0 250
20
0 250
20Figure 1: Training dynamics of neural networks are heavily influenced by outliers with opposing signals. We plot the
overall loss of a ResNet-18 trained with GD on CIFAR-10, plus the losses of a small but representative set of outlier groups.
These groups have consistent opposing signals (e.g., wheels and headlights can mean either car ortruck ). Throughout
training, losses on these groups oscillate with growing and shrinking amplitude—this has an obvious correspondence to
the intermittent spikes in overall loss and appears to be a direct cause of the edge of stability phenomenon.
Specifically, we demonstrate the prevalence of paired groups of outliers in natural data which have
a significant influence on a network’s optimization dynamics. These groups are characterized by
the inclusion of one or more (relatively) large magnitude features that dominate the network’s
output at initialization and throughout most of training. In addition to their magnitude, the other
distinctive property of these features is that they provide large, consistent, and opposing gradients,
in that following one group’s gradient to decrease its loss will increase the other’s by a similar
amount. Because of this structure, we refer to them as Opposing Signals . These features share a
non-trivial correlation with the target task, but they are often not the “correct” (e.g., human-aligned)
signal. In fact, in many cases these features perfectly encapsulate the classic statistical conundrum of
“correlation vs. causation”—for example, a bright blue sky background does not determine the label
of a CIFAR image, but it does most often occur in images of planes. Other features arevery relevant,
such as the presence of wheels and headlights in images of trucks and cars, or the fact that a colon
often precedes either “the” or a newline token in written text.
Opposing signals are most easily understood with an example, which we will give along with a
brief outline of their effect on training dynamics; a more detailed description is presented in Section 3.
Figure 1 depicts the training loss of a ResNet-18 (He et al., 2016) trained with full-batch gradient
descent (GD) on CIFAR-10 (Krizhevsky and Hinton, 2009), along with a few dominant outlier groups
and their respective losses. In the early stages of training, the network enters a narrow valley in
weight space which carefully balances the pairs’ opposing gradients; subsequent sharpening of
the loss landscape (Jastrz e ¸bski et al., 2020; Cohen et al., 2021) causes the network to oscillate with
growing magnitude along particular axes, upsetting this balance. Returning to our example of a sky
background, one step results in the class plane being assigned greater probability for all images
with sky, and the next will reverse that effect. In essence, the “sky =plane ” subnetwork grows and
shrinks.1The direct result of this oscillation is that the network’s loss on images of planes with a
sky background will alternate between sharply increasing and decreasing with growing amplitude,
1It would be more precise to say “strengthening connections between regions of the network’s output and neurons
which have large activations for sky-colored inputs”. Though we prefer to avoid informal terminology, this example makes
clear that the more relaxed phrasing is usually much cleaner. We therefore employ it when the intended meaning is clear.
2

--- PAGE 3 ---
with the exact opposite occurring for images of non-planes with sky. Consequently, the gradients of
these groups will alternate directions while growing in magnitude as well. As these pairs represent
a small fraction of the data, this behavior is not immediately apparent from the overall training
loss—but eventually, it progresses far enough that the overall loss spikes. As there is an obvious
direct correspondence between these two events throughout, we conjecture that opposing signals
are a direct cause of the edge of stability phenomenon (Cohen et al., 2021). We also note that the most
influential signals appear to increase in complexity over time (Nakkiran et al., 2019).
We repeat this experiment across a range of vision architectures and training hyperparameters:
though the precise groups and their order of appearance change, the pattern occurs consistently. We
also verify this behavior for transformers on next-token prediction of natural text and small ReLU
MLPs on simple 1D functions; we give some examples of opposing signals in text in Appendix B.
However, we rely on images for exposition because it offers the clearest intuition. To isolate this
effect, most of our experiments use GD, but we observe similar patterns during SGD which we
present in Section 4.
Summary of contributions. The primary contribution of this paper is demonstrating the existence,
pervasiveness, and large influence of opposing signals during NN optimization. We further present
our current best understanding, with supporting experiments, of how these signals cause the
observed training dynamics—in particular, we provide evidence that it is a consequence of depth
and steepest descent methods. We complement this discussion with a toy example and an analysis
of a two-layer linear net on a simple model. Notably, though rudimentary, our explanation enables
concrete qualitative predictions of NN behavior during training, which we confirm experimentally. It
also provides a new lens through which to study modern stochastic optimization methods, which we
highlight via a case study of SGD vs. Adam. We see possible connections between opposing signals
and a wide variety of phenomena in NN optimization and generalization, including grokking (Power
et al., 2022), catapulting/slingshotting (Lewkowycz et al., 2020; Thilak et al., 2022), simplicity bias (Valle-
Perez et al., 2019), double descent (Belkin et al., 2019; Nakkiran et al., 2020), and Sharpness-Aware
Minimization (Foret et al., 2021). We discuss these and other connections in Section 5.
2 Characterizing and Identifying Opposing Signals
Though their influence on aggregate metrics is non-obvious, identifying outliers with opposing
signals is straightforward. Our methodology is as follows: when training a network with GD, we
track its loss on each individual training point. For a given iteration, we select the training points
whose loss exhibited the most positive and most negative change in the preceding step (there is
large overlap between these sets in successive steps). This set will sometimes contain multiple
opposing signals, which we distinguish via visual inspection. This last detail means that the images
we depict are not random, but we emphasize that it would not be correct to describe this process as
cherry-picking: though precise quantification is difficult, these signals consistently obey the maxim
“I know it when I see it”. This is particularly true for images, such as the groups in Figure 1 which
have easily recognizable patterns. To demonstrate this fact more generally, Appendix H contains
the pre-inspection samples for a ResNet-18, VGG-11 (Simonyan and Zisserman, 2014), and a small
Vision Transformer (Dosovitskiy et al., 2020) at several training steps and for multiple seeds; we
believe the implied groupings are immediate, even if not totally objective. We see algorithmic
approaches to automatically clustering these samples as a direction for future study—for example,
one could select samples by correlation in their loss time-series, or by gradient alignment.
3

--- PAGE 4 ---
0 500 1000 1500 2000 2500 3000
Iteration10203040506070NormGradient Norm
0 500 1000 1500 2000 2500 3000
Iteration010002000300040005000EigenvalueTop Eigenvalue of Individual Loss
0 500 1000 1500 2000 2500 3000
Iteration−200−150−100−50050100CurvatureCurvature On Top Full Loss Eigenvector
Signal 0
Signal 1
Signal 2
Signal 3
RandomFigure 2: Tracking other metrics which characterize outliers with opposing signals. Maximal per-step change in loss
relates to other useful metrics, such as per-sample gradient norm and curvature. We combine each pair of groups in
Figure 1 to create training subsets which each exemplify one “signal”: we see that these samples are also significant outliers
according to the other metrics. (For a point x, “Curvature on Top Full Loss Eigenvector” is defined as v⊤H(x)v, where vis
the top eigenvector of the full loss Hessian and H(x)is the Hessian of the loss on xalone.)
Measuring alternative metrics. Given how these samples are selected, several other characteriza-
tions seem appropriate. For instance, one-step loss change is often a reasonable proxy for gradient
norm; we could also consider the largest eigenvalue of the loss of the individual point , or how much
curvature it has in the direction of the overall loss’s top eigenvector. For large networks these options
are far more compute-intensive than our chosen method, but we can evaluate them on specific
groups. In Figure 2 we track these metrics for several opposing group pairs and we find that they
are consistently much larger than that of random samples from the training set.
2.1 On the Possibility of a Formal Definition
Though the features and their exemplar samples are immediately recognizable, we do not attempt to
exactly define a “feature”, nor an “outlier” with respect to that feature. The presence of a particular
feature is often ambiguous, and it is difficult to define a clear threshold for what makes a given point
an outlier.2Thus, instead of trying to exactly partition the data, we simply note that these heavy
tails exist and we use the most obvious outliers as representatives for visualization. In Figures 1
and 2 we choose an arbitrary cutoff of twenty samples per group.
We also note that what qualifies as an opposing signal or outlier may vary over time. For
visual clarity, Figure 1 depicts the loss on only the most dominant group pair in its respective
training phase, but this pattern occurs simultaneously for many different signals and at multiple
scales throughout training. Further, the opposing signals are with respect to the model’s internal
representations (and the label), not the input space itself; this means that the definition is also a
property of the architecture. For example, following Cohen et al. (2021) we train a small MLP to
fit a Chebyshev polynomial on evenly spaced points in the interval [−1,1](Figure 3). This data
has no “outliers” in the traditional sense, and it is not immediately clear what opposing signals
are present. Nevertheless, we observe the same alternating behavior: we find a pair where one
group is a small interval of x-values and the opposing group contains its neighbors, all in the range
[−1,−0.5]. This suggests that the network has internal activations which are heavily influential only
for more negative x-values. In this context, these two groups are the outliers.
2In the case of language—where tokenization is discrete and more interpretable—a precise definition is sometimes
possible. For example, one opposing pair in Appendix B consists of sequences whose penultimate token is a colon and
whose last token is either “the” or a newline.
4

--- PAGE 5 ---
−1.0−0.5 0.0 0.5 1.0
Input Value−1.0−0.50.00.51.0Target ValueTraining Points
800 1000 1200 1400
Iteration0.00.51.01.52.0LossTraining Loss
990 1000 1010 1020 1030
Iteration0.250.500.751.001.251.501.752.00
Group 1
Group 2
Others
Full LossFigure 3: Opposing signals when fitting a Chebyshev polynomial with a small MLP . Though the data lacks traditional
“outliers”, it is apparent that the network has some features which are most influential only on the more negative inputs
(or whose effect is otherwise cancelled out by other features). Since the correct use of this feature is opposite for these two
groups, they provide opposing signals.
3 Understanding the Effect of Opposing Signals
Beyond noting their existence, our eventual goal will be to derive actionable insights from this
finding. To do this, it is necessary to gain a better understanding of how these outliers cause the
observed behavior. In this section we give a simplified “mental picture” which serves as our current
understanding this process. We begin with an informal discussion of the outsized influence of
opposing signals and how they lead to progressive sharpening; this subsection collates and expands
on prior work to give important context for how these signals differ from typically imagined
“noise”. Next, we give a mechanistic description of the specific effect of opposing signals with a
toy example. This explanation is intentionally high-level, but we will eventually see how it gives
concrete predictions of specific behaviors, which we then verify on real networks. Finally, we prove
that this behavior occurs on a two-layer linear network under a simple model.
3.1 Progressive Sharpening, and Intuition for Why these Features are so Influential
At a high level, most variation in the input is unneeded when training a network to minimize
predictive error—particularly with depth and high dimension, only a small fraction of information
will be propagated to the last linear layer (Huh et al., 2021). Starting from random initialization,
training a network aligns adjacent layers’ singular values (Saxe et al., 2013; Mulayoff and Michaeli,
2020) to amplify meaningful signal while downweighting noise,3growing sensitivity to the important
signal. This sensitivity can be measured, for example, by the spectral norm of the input-output
Jacobian, which grows during training (Ma and Ying, 2021); it has also been connected to growth in
the norm of the output layer (Wang et al., 2022).
Observe that with this growth, small changes to how the network processes inputs become more
influential. Hypothetically, a small weight perturbation could massively increase loss by redirecting
unhelpful noise to the subspace to which the network is most sensitive, or by changing how the last
layer uses it. The increase of this sensitivity thus represents precisely the growth of loss Hessian
3In this discussion we use the term “noise” informally. We refer not necessarily to pure randomness, but more generally
to input variation which is least useful in predicting the target.
5

--- PAGE 6 ---
spectrum, with the strength of this effect increasing with depth (Wang et al., 2016; Du et al., 2018;
Mulayoff and Michaeli, 2020).4
Crucially, this sharpening also depends on the structure of the input. If the noise is independent
of the target, it will be downweighted throughout training. In contrast, genuine signals which oppose
each other will be retained and perhaps even further amplified by gradient descent; this is because
the “correct” feature may be much smaller in magnitude (or not yet learned), so using the large,
“incorrect” feature is often the most immediate way of minimizing loss. As a concrete example,
observe that a randomly initialized network will lack the features required for the subtle task of
distinguishing birds from planes. But it will capture the presence of sky, which is very useful
for reducing loss on such images by predicting the conditional p(class|sky)(this is akin to the
“linear/shallow-first” behavior described by Nakkiran et al. (2019); Mangalam and Prabhu (2019)).
Thus, any method attempting to minimize loss as fast as possible (e.g., steepest descent) may actually
upweight these features. Furthermore, amplified opposing signals will cause greater sharpening
than random noise, because using a signal to the benefit of one group is maximally harmful for
the other—e.g., confidently predicting plane whenever there is sky will cause enormous loss on
images of other classes with sky. Since random noise is more diffuse, this effect is less pronounced.
This description is somewhat abstract. To gain a more precise understanding, we illustrate the
dynamics more explicitly on a toy example.
3.2 Illustrating with a Hypothetical Example of Gradient Descent
Consider the global loss landscape of a neural network: this is the function which describes how the
loss changes as we move through parameter space. Suppose we identify a direction in this space
which corresponds to the network’s use of the “sky” feature to predict plane versus some other
class. That is, we will imagine that whenever the input image includes a bright blue background,
moving the parameters in one direction increases the logit of the plane class and decreases the
others, and vice-versa. We will also decompose this loss— among images with a sky background,
we consider separately the loss on those labeled plane versus those with any other label. Because
the sky feature has large magnitude, a small change in weight space will induce a large change in
the network outputs— i.e., a small movement in the direction “sky = plane ” will greatly increase
loss on these non- plane images.
Figure 4 depicts this heavily simplified scenario. Early in training, optimizing this network with
GD will rapidly move towards the minimum along this direction. In particular, until better features
are learned, the direction of steepest descent will lead to a network which upweights the sky feature
and predicts p(class|sky)whenever it occurs. Once sufficiently close to the minimum, the gradient
will point “through the valley” towards amplifying the more relevant signal (Xing et al., 2018).
However, this will also cause the sky feature to grow in magnitude—as well as its potential influence
were the weights to be selectively perturbed, as described above. Both these factors contribute to
progressive sharpening.
Here we emphasize the distinction between the loss on the outliers and the full train loss. As
images without sky are not nearly as sensitive to movement along this axis, their gradient and
curvature is much smaller—and since they comprise the majority of the dataset, the global loss
landscape may not at first be significantly affected. Continued optimization will oscillate across the
minimum with growing magnitude, but this growth may not be immediately apparent. Furthermore,
4The coincident growth of these two measures was previously noted by Ma and Ying (2021); Gamba et al. (2023);
MacDonald et al. (2023), though they did not make explicit this connection to how the network processes different types of
input variance.
6

--- PAGE 7 ---
Loss on planeLoss on otherGradient StepsLoss on OutliersProjection onto  1D Weight Spacep(plane∣sky)≈1p(other∣sky)≈1p(plane∣sky)≈1p(other∣sky)≈1
Overall Loss
Figure 4: A toy illustration of the effect of opposing signals. Images with many blue pixels cause large activations,
with high loss sensitivity. We project the loss to the hypothetical weight-space dimension “sky = plane ”.Left: Early
optimization approaches the minimum, balancing the opposing gradients for plane andother (these are losses for
separate training subsets: those labeled plane vs. those with any other label—the purple curve is their average). Progress
continues through this valley, further growing the feature magnitude. Right : The valley sharpens and the iterates diverge,
alternating between high and low loss for each group. Because most training points are insensitive to this axis, the overall
loss may not be noticeably affected at first. Eventually either (a) the loss growth forces the network to downweight “sky”,
flattening the valley; or (b) the weights “catapult” to a different basin.
progress orthogonal to these oscillations need not be affected —we find some evidence that these two
processes occur somewhat independently, which we present in Section 4. Returning to the loss
decomposition, we see that these oscillations will cause the losses to grow and alternate , with one
group having high loss and then the other. Eventually the outliers’ loss increases sufficiently and
the overall loss spikes, either flattening the valley and returning to the first phase, or “catapulting”
to a different basin (Wu et al., 2018; Lewkowycz et al., 2020; Thilak et al., 2022). This phenomenon is
depicted in Figure 1. Finally, we note that if one visualizes the dynamics in Figure 4 from above—so
the left/right direction on the page becomes up/down—it gives exactly the pattern of a network’s
weights projected onto the top eigenvector of the Hessian (e.g., Figure 6(b) later in this work).
Verifying our toy examples’s predictions. Though this explanation lacks precise details, it does
enable concrete predictions of network behavior during training. Figure 5 tracks the predictions of a
ResNet-18 on an image of sky—to eliminate possible confounders, we create a synthetic image as a
single color block. Though the “ plane vs.other ” example seems almost toosimple, we see exactly
the described behavior—initial convergence to the minimum along with rapid growth in feature
norm, followed by oscillation in class probabilities. Over time, the network learns to use other signal
and downweights the sky feature, as evidenced by the slow decay in feature norm. We reproduce
this figure for many other inputs and for a VGG-11-BN in Appendix C, with similar findings.
Our example also suggests that oscillation serves as a valuable regularizer that reduces reliance
on easily learned opposing signals which may not generalize. When a signal is used to the benefit
of one group and the detriment of another, the advantaged group’s loss goes down while the other’s
goes up, meaning the latter’s gradient grows in magnitude while the former’s shrinks. As the now
gradient-dominating group is also the one disadvantaged by the use of this signal, the network
will be encouraged to downweight this feature. In Appendix C.3 we reproduce Figure 5 with a
VGG-11-BN trained with a very small learning rate to closely approximate gradient flow. We see
that gradient flow and GD are very similar until reaching the edge of stability. After this point,
the feature norm under GD begins to slowly decay while oscillating; in contrast, in the absence of
oscillation, the feature norms of opposing signals under gradient flow grow continuously. If it is the
case that opposing signals represent “simple” features which generalize worse, this could help to
explain the poor generalization of gradient flow. A similar effect was observed by Jastrz e ¸bski et al.
(2020), who noted that large initial learning rate leads to a better-conditioned loss landscape later.
7

--- PAGE 8 ---
0 100 200 300 400 500
Iteration−6−4−20246Logit ValueClass Logits
plane
auto
bird
cat
deer
dog
frog
horse
ship
truck0 1000 2000 3000 4000 5000
Iteration5101520NormFeature Norm
Input Image
Mean Over DataIter 27 Iter 28
Iter 29 Iter 30Class Probabilities During OscillationInput ImageFigure 5: Passing a sky-colored block through a ResNet during GD precisely tracks the predictions of our toy example.
Left: In the first phase, the network rapidly learns to use the sky feature to minimize loss. As signal is amplified, so too is
the sky-colored input, and oscillation begins as depicted in Figure 4. Middle: During oscillation, gradient steps alternate
along the axis “sky =plane ” (and a bit ship ).Right: The initial phase amplifies the sky input, causing rapid growth in
feature norm. The network then oscillates, slowly learning to downweight this feature and rely on other signal (average
feature norm provided for comparison).
3.3 Theoretical Analysis of Opposing Signals in a Simple Model
To demonstrate this effect formally, we study misspecified linear regression on inputs x∈Rdwith
a two-layer linear network. Though this model is simplified, it enables preliminary insight into
the factors we think are most important for these dynamics to occur. Since we are analyzing the
dynamics from initialization until the stability threshold, it will be sufficient to study the trajectory
ofgradient flow —for reasonable step sizes η, a similar result then follows for gradient descent.
Our analysis reproduces the initial phase of quickly reducing loss on the outliers, followed by the
subsequent growth in sensitivity to the waythe opposing signal is used—i.e., progressive sharpening.
We also verify this pattern (and the subsequent oscillation, which we do not formally prove) in
experiments on real and synthetic data in Appendix E.
We remark that one relevant factor which our model lacks is the concept of a “partially useful”
signal as described at the end of Section 3.1. This seems to require a somewhat more complex
model to properly capture (e.g., multinomial logistic regression) so we view this analysis as an early
investigation, capturing only part of relevant aspects of the phenomena we observe.
Model. We model the observed features as a distribution over x∈Rd1, assuming only that its
covariance Σexists—for clarity we treat Σ =Iin the main text. We further model an additional
vector xo∈Rd2representing the opposing signal, with d2≥d1. We will suppose that on some
small fraction of outliers p≪1,xo∼Unifn
±q
α
pd21o
(1is the all-ones vector) for some αwhich
governs the feature magnitude, and we let it be 0on the remainder of the dataset. We model the
target as the linear function y=β⊤x+1√d21⊤|xo|; this captures the idea that the signal xocorrelates
strongly with the target, but in opposing directions of equal strength. Finally, we parameterize the
network with vectors b∈Rd1, bo∈Rd2and scalar cin one single vector θ, asfθ(x) =c·(b⊤x+b⊤
oxo).
Note the specific distribution of xois unimportant—furthermore, in our simulations we observed
the exact same pattern with cross-entropy loss. From our experiments and this analysis, it seems
that depth and a small signal-to-noise ratio are the only elements needed for this behavior to arise.
Setup. A standard initialization would be to sample [b, bo]⊤∼ N(0,1
d1+d2I), which would then
imply highly concentrated distributions for the quantities of interest. As tracking the precise
8

--- PAGE 9 ---
concentration terms would not meaningfully contribute to the analysis, we simplify by directly
assuming that at initialization these quantities are equal to their expected order of magnitude:
∥b∥2
2=1⊤b=d1
d1+d2,∥bo∥2
2=1⊤bo=d2
d1+d2, and b⊤β=∥β∥√d1+d2. Likewise, we let c= 1, ensuring
that both layers have the same norm. We perform standard linear regression by minimizing the
population loss L(θ) :=1
2E[(fθ(x)−y)2]. We see that the minimizer of this objective has bo=0and
cb=β. However, an analysis of gradient flow will elucidate how depth and strong opposing signals
lead to sharpening as this minimum is approached.
Results. In exploring progressive sharpening, Cohen et al. (2021) found that sometimes the model
would have a brief decrease in sharpness, particularly for square loss. In fact, this is consistent with
our above explanation: for larger αand a sharper loss (e.g. the square loss), the network will initially
prioritize minimizing loss on the outliers, thus heavily reducing sharpness. Our first result proves
that this occurs in the presence of large magnitude opposing signals:
Theorem 3.1 (Initial decrease in sharpness) .Letk:=d2
d1≥1, and assume ∥β∥>max
d1√d1+d2,24
5
. At
initialization, the sharpness ∥∇2
θL(θ)∥2lies in (α,3α). Further, if√α= Ω (∥β∥klnk), then both boand the
overall sharpness will decrease as˜O(e−αt)fromt= 0until some time t1≤ln∥β∥/2
2∥β∥.
Proofs can be found in Appendix G. After this decrease, signal amplification can proceed—but
this also means that the sharpness with respect to how the network uses the feature xowill grow, so a
small perturbation to the parameters bowill induce a large increase in loss.
Theorem 3.2 (Progressive sharpening) .If√α= Ω 
1 +∥β∥2klnk
, then at starting at time t1the
sharpness will increase linearly in ∥β∥until some time t2≥1
2∥β∥2
2, reaching at least5
8∥β∥α. This lower
bound on sharpness applies to each dimension of bo.
Oscillation will not occur during gradient flow—but for gradient descent with step size η >16
5∥β∥α,
bowill start to increase in magnitude while oscillating across the origin. If this growth continues, it
will rapidly reintroduce the feature, causing the loss on the outliers to grow and alternate. Such rein-
troduction (an example of which occurs around iteration 3000 in Figure 5) seems potentially helpful
for exploration. In Figure 38 in the Appendix we simulate our model and verify exactly this sequence
of events. We also show that an MLP trained on CIFAR-10 displays the same characteristic behavior.
3.4 Additional Findings
Sharpness often occurs overwhelmingly in the first few layers. Theorem 3.2 shows that pro-
gressive sharpening occurs specifically in bo. Generally, our model suggests that sharpness will
begin in the last layer but that that early in training it will shift to the earlier layers since they have
more capacity to redirect the signal. In Appendix D we track what fraction of curvature5of the
top eigenvector lies in each layer of various networks during training with GD. In a ResNet-18,
sharpness occurs almost exclusively in the first convolutional layer after the first few training steps;
the same pattern appears more slowly while training a VGG-11. In a Vision Transformer curvature
occurs overwhelmingly in the embedding layer and very slightly in the earlier MLP projection heads.
The text transformer (NanoGPT) follows the same pattern, though with less extreme concentration
in the embedding. Thus it does seem to be the case that earlier layers have the most significant
sharpness—especially if they perform dimensionality reduction or have particular influence over
5The “fraction of curvature” is with respect to the top eigenvector of the loss Hessian. We partition this vector by
network layer, so each sub-vector’s squared norm represents that layer’s contribution to the overall curvature.
9

--- PAGE 10 ---
how the signal is propagated to later layers. This seems the likely cause of large gradients in the early
layers of vision models (Chen et al., 2021; Kumar et al., 2022), suggesting that this effect is equally
influential during finetuning and pretraining and that further study can improve optimization.
Batchnorm may smooth training, even if not the loss itself. Cohen et al. (2021) noted that
batchnorm (BN) (Ioffe and Szegedy, 2015) does not prevent networks from reaching the edge
of stability and concluded, contrary to Santurkar et al. (2018), that BN does not smooth the loss
landscape. We conjecture that the benefit of BN may be in downweighting the influence of opposing
signals and mitigating this oscillation. In other words, BN may smooth the optimization trajectory of
neural networks, rather than the loss itself (this is consistent with the distinction made by Cohen
et al. (2021) between regularity and smoothness). In Section 4 we demonstrate that Adam also
smooths the optimization trajectory and that minor changes to emulate this effect can aid stochastic
optimization. We imagine that the effect of BN could also depend on the use of GD vs. SGD.
Specifically, our findings hint at a possible benefit of BN which applies only to SGD: reducing the
variance of imbalanced opposing signals across random minibatches.
For both GD and SGD, approximately half of training points go up in loss on each step. Though
only the outliers are wildly oscillating, many more images contain some small component of the
features they exemplify. Figure 36 in the Appendix shows that the fraction of points which increase
in loss hovers around 50% for every step—to some extent, a small degree of oscillation appears to be
happening to the entire dataset.
Different losses have different effects on sharpening. Our model would predict that adding label
smoothing to the cross-entropy loss should reduce sharpening, because smoothing reduces loss cur-
vature under extreme overconfidence. Indeed, MacDonald et al. (2023) show this to be the case. This
also hints at why logistic loss may be more suitable for NN optimization, because it only has substan-
tial curvature around x= 0(xbeing the logit, i.e. when prediction entropy is high), so unlike square
or exponential loss, large magnitude features will not massively increase sharpness. We expect a
similar property could contribute to the relative behavior of different activations (e.g. ReLU or tanh).
4 The Interplay of Opposing Signals and Stochasticity
Full-batch GD is not used in practice when training NNs. It is therefore pertinent to ask what these
findings imply about stochastic optimization. We begin by verifying that this pattern persists during
SGD. Figure 6(a) displays the losses for four opposing group pairs of a VGG-11-BN trained on
CIFAR-10 with SGD batch size 128. We observe that the paired groups do exhibit clear opposing
oscillatory patterns, but they do not alternate with every step, nor do they always move in opposite
directions. This should not be surprising: we expect that not every batch will have a given signal
in one direction or the other. For comparison, we include the fulltrain loss in each figure—that is,
including the points not in the training batch. We see that the loss on the outliers has substantially
larger variance; to confirm that this is not just because the groups have many fewer samples, we also
plot the loss on a random subset of training points of the same size. We reproduce this plot with a
VGG-11 without BN in Figure 37 in the Appendix.
Having verified that this behavior occurs in the stochastic setting, we conjecture that current best
practices for neural network optimization owe much of their success to how they handle opposing
signals. As a proof of concept, we will make this more precise with a preliminary investigation of
the Adam optimizer (Kingma and Ba, 2014).
10

--- PAGE 11 ---
123LossGroup 1
Group 2
Full Train Loss
Random Subset
1234
10 20 30 40 50
Iteration1234Loss
10 20 30 40 50
Iteration1234VGG-11 SGD Loss on Opposing Groups(a)
−0.20−0.15−0.10−0.050.00Scalar ProjectionProjection on Top Eigenvector
GD
SGD
Adam
Momentum SGD
0 5 10 15 20 25 30 35
Iteration050Step SizeAdam Effective Step Size ( ×3e-4) (b)
Figure 6: Outliers with opposing signals have a significant influence even during SGD. Left: We plot the losses of
paired outlier groups on a VGG-11-BN trained on CIFAR-10, along with the full train loss for comparison. Modulo batch
randomness, the outliers’ loss follow the same oscillatory pattern with large magnitude. See appendix for the same without
batchnorm. Right (top): We train a small MLP on a 5k subset of CIFAR-10 with various optimizers and project the iterates
onto the top Hessian eigenvector. SGD closely tracks GD, bouncing across the valley; momentum somewhat mitigates the
sharp jumps. Adam smoothly oscillates along one side. Right (bottom): Adam’s effective step size drops sharply when
moving too close or far from the valley floor.
4.1 How Adam Handles Gradients with Opposing Signals
To better understand their differences, Figure 6(b) visualizes the parameter iterates of Adam and
SGD with momentum on a ReLU MLP trained on a 5k subset of CIFAR-10, alongside those of GD
and SGD (all methods use the same initialization and sequence of training batches). The top figure is
the projection of these parameters onto the top eigenvector of the loss Hessian of the network trained
with GD, evaluated at the first step where the sharpness crosses 2/η. We observe that SGD tracks a
similar path to GD, though adding momentum mitigates the oscillation somewhat. In contrast, the
network optimized with Adam markedly departs from this pattern, smoothly oscillating along one
side. We identify three components of Adam which potentially contribute to this effect:
Advantage 1: Smaller steps along high curvature directions. Adam’s normalization causes
smaller steps along the top eigenvector, especially near the minimum. The lower plot in Figure 6(b)
shows that the effective step size in this direction—i.e., the absolute inner product of the parameter-
wise step sizes and the top eigenvector—rapidly drops to zero as the iterates approach the valley floor
(in the opposite direction, the gradient negates the momentum for the same effect). We conjecture
that general normalization may not be essential to Adam’s performance; we even expect it could
be somewhat harmful by limiting exploration. On the other hand, normalizing steps by curvature
parameter-wise does seem important; Pan and Li (2023) argue the same and show that parameter-wise
gradient clipping improves SGD substantially. We highlight why this may be useful in the next point.
Advantage 2: Managing heavy-tailed gradients and avoiding steepest descent. Zhang et al. (2020)
identified the “trust region” as an important contributor to Adam’s success in attention models, point-
ing to heavy-tailed noise in the stochastic gradients. More recently, Kunstner et al. (2023) argued that
Adam’s superiority does not come from better handling noise, which they supported by experiment-
ing with large batch sizes. Our result reconciles these contradictory claims by showing that the dif-
ficulty is not heavy-tailed noise , but strong, directed (and perhaps imbalanced) opposing signals.
11

--- PAGE 12 ---
Unlike traditional “gradient noise”, larger batch sizes may not reduce the effect of these signals—that
is, the gradient is heavy-tailed (across parameters) even without being stochastic. Furthermore,
the largest steps emulate Sign SGD, which is notably nota descent method. Figure 6(b) shows that
Adam’s steps are more parallel to the valley floor than those of steepest descent. Thus it seems advan-
tageous to intentionally avoid steps along the gradient which point towards the local minimum,
which might lead to over-reliance on these features. Indeed, Benzing (2022) observe that true second
order methods perform worse than SGD on NNs, and Kunstner et al. (2023) show that Adam shares
some behavior with Sign SGD with momentum. This point is also consistent with the observed gen-
eralization benefits of a large learning rate for SGD on NNs (Jastrz e ¸bski et al., 2020); in fact, opposing
signals naturally fit the concept of “easy-to-generalize” features as modeled by Li et al. (2019).
Advantage 3: Dampening. Lastly, Adam’s third important factor: downweighting the most recent
gradient. Traditional SGD with momentum β <1takes a step which weights the current gradient by
1
1+β>1
2. Though this makes intuitive sense, our results imply that heavily weighting the most recent
gradient can be problematic. Instead, we expect an important addition is dampening , which multiplies
the stochastic gradient at each step by some (1−τ)<1. We observe that Adam’s (unnormalized)
gradient is equivalent to SGD with momentum and dampening both equal to β1, plus a debiasing
step. Recently proposed alternatives also include dampening in their momentum update but do
not explicitly identify the distinction (Zhang et al., 2020; Pan and Li, 2023; Chen et al., 2023).
4.2 Proof of Concept: Using These Insights to Aid Stochastic Optimization
To test whether our findings translate to practical gains, we design a variant of SGD which incorpo-
rates these insights. First, we use dampening τ= 0.9in addition to momentum. Second, we choose
a global threshold: if the gradient magnitude for a parameter is above this threshold, we take a fixed
step size; otherwise, we take a gradient step as normal. The exact method appears in Appendix F. We
emphasize that our goal here is not to propose a new optimization algorithm. Instead, we are ex-
ploring the potential value gained from knowledge of the existence and influence of opposing signals.
Results in Appendix F show that this approach matches Adam when training ResNet-56/110 on
CIFAR-10 with learning rates for the unthresholded parameters across several orders of magnitude
ranging from 10−4to103. Notably, the fraction of parameters above the threshold (whose step size is
fixed) is only around 10-25% per step. This implies that the trajectory and behavior of the network is
dominated by this small fraction of parameters; the remainder can be optimized much more robustly,
but their effect on the network’s behavior is obscured. We therefore see the influence of opposing
signals as a possible explanation for the “hidden” progress in grokking (Barak et al., 2022; Nanda
et al., 2023). We also compare this method to Adam for the early phase of training GPT-2 (Radford
et al., 2019) on the OpenWebText dataset (Gokaslan et al., 2019)—not only do they perform the same,
their loss similarity suggests that their exact trajectory may be very similar (Appendix F.2). Here the
fraction of parameters above the threshold hovers around 50% initially and then gradually decays.
The fact that many more parameters in the attention model are above the threshold suggests that the
attention mechanism is more sensitive to opposing signals and that further investigation of how to
mitigate this instability may be fruitful.
5 Discussion and Future Work
Many of the observations we make in this paper are not new, having been described in various
prior works. Rather, this work identifies a possible higher-order cause which neatly ties these findings
together. There are also many works which pursue a more theoretical understanding of each of
12

--- PAGE 13 ---
these phenomena independently. Such analyses begin with a set of assumptions (on the data, in
particular) and prove that the given behavior follows. In contrast, this work begins by identifying
a condition—the presence of opposing signals—which we argue is likely a major cause of these
behaviors. These two are not at odds: we believe in many cases our result serves as direct evidence
for the validity of these modeling assumptions and that it may enable even more fine-grained
analyses. This work provides an initial investigation which we hope will inspire future efforts
towards a more complete understanding.
We now highlight some connections to these earlier findings. More general related work can be
found in Appendix A.
Heavy-tailed loss spectrum. Earlier studies of the loss landscape noted a small group of very large
outlier Hessian eigenvalues or Jacobian singular values (e.g. Sagun et al. 2016, 2017; Papyan 2018,
see Appendix A for more). Our method of identifying these paired groups, along with the metrics
tracked in Figure 2, indicate that these outlier directions in the spectrum are precisely the directions
with opposing signals in the gradient and that this pattern may be key to better understanding the
generalization ability of NNs trained with SGD.
Progressive sharpening and the edge of stability. More recent focus has shifted to the top Hessian
eigenvalue(s), where it was empirically observed that their magnitude (the loss “sharpness”) grows
during training (Jastrz e ¸bski et al., 2019, 2020; Cohen et al., 2021) (so-called progressive sharpening ),
leading to rapid oscillation in weight space (Xing et al., 2018; Jastrz e ¸bski et al., 2019). Cohen et al.
(2021) also found that for GD this coincides with a consistent yet non-monotonic decrease in training
loss over long timescales, which they named the edge of stability . We observe that prior analyses have
proven the occurrence of progressive sharpening and the edge of stability under various assumptions
(Arora et al., 2022; Wang et al., 2022), but the underlying cause has not been made clear. Our discus-
sion, experiments, and theoretical analysis in Section 3 provide strong evidence for a genuine cause
which aligns with several of these existing modeling assumptions. Roughly, our results seem to im-
ply that progressive sharpening occurs when the network learns to rely on (or notrely on) opposing
signals in a very specific way, while simultaneously amplifying overall sensitivity. This growth in
sensitivity means a small parameter change modifying how opposing signals are used can massively
increase loss. This leads to intermittent instability orthogonal to the “valley floor”, accompanied
by gradual training loss decay and occasional spikes as described by the toy example in Figure 4
and depicted on real data in Figure 1. Empirically, this oscillation seems somewhat independent of
movement parallel to the floor (see Appendix F), but further study of the precise dynamics is needed.
Spurious correlations, grokking, and slingshotting. In images, the features corresponding to
opposing signals match the traditional picture of “spurious correlations” surprisingly closely—
it could be that a network maintaining balance or diverging along a direction also determines
whether it continues to use a “spurious” feature or is forced to find an alternative way to minimize
loss. Indeed, the exact phenomenon of a network “slingshotting” to a new region with improved
generalization has been directly observed (Wu et al., 2018; Lewkowycz et al., 2020; Jastrz e ¸bski et al.,
2021; Thilak et al., 2022). Grokking (Power et al., 2022), whereby a network learns to generalize long
after memorizing the training set, is closely related. Several works have shown that grokking is a
“hidden” phenomenon, with gradual amplification of generalizing subnetworks (Barak et al., 2022;
Nanda et al., 2023; Merrill et al., 2023); it has even been noted to co-occur with weight oscillation
(Notsawo Jr et al., 2023). Our experiments in Section 4 and Appendix F show that the influence of
opposing signals obscures the behavior of the rest of the network, offering one possible explanation.
13

--- PAGE 14 ---
Simplicity bias and double descent. Nakkiran et al. (2019) observed that NNs learn functions of in-
creasing complexity throughout training. Our experiments—particularly the slow decay in the norm
of the feature embedding of opposing signals—lead us to believe it would be more correct to say that
they unlearn simple functions, which enables more complex subnetworks with smaller magnitude
and better performance to take over. At first this seems at odds with the notion of simplicity bias
(Valle-Perez et al., 2019; Shah et al., 2020), defined broadly as a tendency of networks to rely on simple
functions of their inputs. However, it does seem to be the case that the network will use the simplest
(e.g., largest norm) features that it can, so long as such features allow it to approach zero training loss;
otherwise it may eventually diverge. This tendency also suggests a possible explanation for double
descent (Belkin et al., 2019; Nakkiran et al., 2020): even after interpolation, the network pushes to-
wards greater confidence and the weight layers continue to balance (Saxe et al., 2013; Du et al., 2018),
increasing sharpness. This could lead to oscillation, pushing the network to learn new features which
generalize better (Wu et al., 2018; Li et al., 2019; Rosenfeld et al., 2022; Thilak et al., 2022). This be-
havior would also be more pronounced for larger networks because they exhibit greater sharpening.
Note that the true explanation is not quite so straightforward: generalization is sometimes improved
via methods that reduce oscillation (like loss smoothing), implying that this behavior is not always
advantageous. A better understanding of these nuances is an important subject for future study.
Sharpness-Aware Minimization Another connection we think merits further inquiry is Sharpness-
Aware Minimization (SAM) (Foret et al., 2021), which is known to improve generalization of neural
networks for reasons still not fully understood (Wen et al., 2023). In particular, the better-performing
variant is 1-SAM, which takes positive gradient steps on each training point in the batch individually.
It it evident that several of these updates will point along directions of steepest descent/ascent
orthogonal to the valley floor (and, if not normalized, the updates may be very large). Thus it may
be that 1-SAM is in some sense “simulating” oscillation and divergence out of this valley in both
directions, enabling exploration in a manner that would not normally be possible until the sharpness
grows large enough—these intermediate steps would also encourage the network to downweight
these features sooner and faster. In contrast, standard SAM would only take this step in one of
the two directions, or perhaps not at all if the opposing signals are equally balanced. Furthermore,
unlike 1-SAM the intermediate step would blend together all opposing signals in the minibatch.
These possibilities seem a promising direction for further exploration.
6 Conclusion
The existence of groups of training data with such a significant yet non-obvious influence on neural
network training raises as many questions as it answers. This work presents an initial investigation
into the effect of opposing signals on various aspects of optimization, but there is still much more
to understand. Though it is clear they have a large influence on training, less obvious is whether
reducing their influence is necessary for improved optimization or simply coincides with it. At
the same time, there is evidence that the behavior these signals induce may serve as an important
method of exploration and/or regularization. If so, another key question is whether these two
effects can be decoupled—or if the incredible generalization ability of neural networks is somehow
inherently tied to their optimization instability.
14

--- PAGE 15 ---
Acknowledgements
We thank Saurabh Garg for detailed feedback on an earlier version of this work. Thanks also to
Christina Baek and Bingbin Liu for helpful comments and Jeremy Cohen for pointers to related
work. This research is supported in part by NSF awards IIS-2211907, CCF-2238523, an Amazon
Research Award, and the CMU/PwC DT&I Center.
References
Sanjeev Arora, Zhiyuan Li, and Abhishek Panigrahi. Understanding gradient descent on the edge of
stability in deep learning. In International Conference on Machine Learning , pages 948–1024. PMLR,
2022.
Boaz Barak, Benjamin L. Edelman, Surbhi Goel, Sham M. Kakade, eran malach, and Cyril Zhang.
Hidden progress in deep learning: SGD learns parities near the computational limit. In Advances
in Neural Information Processing Systems , 2022. URL https://openreview.net/forum?id=
8XWP2ewX-im .
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning
practice and the classical bias–variance trade-off. Proceedings of the National Academy of Sciences ,
116(32):15849–15854, 2019.
Frederik Benzing. Gradient descent on neurons and its link to approximate second-order optimiza-
tion. In International Conference on Machine Learning , pages 1817–1853. PMLR, 2022.
Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi
Dong, Thang Luong, Cho-Jui Hsieh, et al. Symbolic discovery of optimization algorithms. arXiv
preprint arXiv:2302.06675 , 2023.
Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision
transformers. arXiv preprint arXiv:2104.02057 , 2(5):6, 2021.
Jeremy Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. Gradient descent on
neural networks typically occurs at the edge of stability. In International Conference on Learning
Representations , 2021. URL https://openreview.net/forum?id=jh-rTtvkGeM .
Jeremy M Cohen, Behrooz Ghorbani, Shankar Krishnan, Naman Agarwal, Sourabh Medapati, Michal
Badura, Daniel Suo, David Cardoze, Zachary Nado, George E Dahl, et al. Adaptive gradient
methods at the edge of stability. arXiv preprint arXiv:2207.14484 , 2022.
Alex Damian, Eshaan Nichani, and Jason D. Lee. Self-stabilization: The implicit bias of gradient
descent at the edge of stability. In OPT 2022: Optimization for Machine Learning (NeurIPS 2022
Workshop) , 2022. URL https://openreview.net/forum?id=enoU_Kp7Dz .
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image
is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 ,
2020.
Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous
models: Layers are automatically balanced. In Advances in Neural Information Processing Systems ,
volume 31. Curran Associates, Inc., 2018.
15

--- PAGE 16 ---
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization
for efficiently improving generalization. In International Conference on Learning Representations ,
2021. URL https://openreview.net/forum?id=6Tm1mposlrM .
Stanislav Fort and Surya Ganguli. Emergent properties of the local geometry of neural loss land-
scapes. arXiv preprint arXiv:1910.05929 , 2019.
Matteo Gamba, Hossein Azizpour, and Mårten Björkman. On the lipschitz constant of deep networks
and double descent. arXiv preprint arXiv:2301.12309 , 2023.
Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net optimization
via hessian eigenvalue density. In Proceedings of the 36th International Conference on Machine
Learning , volume 97 of Proceedings of Machine Learning Research . PMLR, 09–15 Jun 2019. URL
https://proceedings.mlr.press/v97/ghorbani19b.html .
Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages
770–778, 2016.
Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, and Phillip Isola.
The low-rank simplicity bias in deep networks. arXiv preprint arXiv:2103.10427 , 2021.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning , pages 448–456.
pmlr, 2015.
Stanisław Jastrz e ¸bski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio,
and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623 , 2017.
Stanisław Jastrz e ¸bski, Zachary Kenton, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amost
Storkey. On the relation between the sharpest directions of DNN loss and the SGD step length.
InInternational Conference on Learning Representations , 2019. URL https://openreview.net/
forum?id=SkgEaj05t7 .
Stanisław Jastrz e ¸bski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor, Kyunghyun
Cho*, and Krzysztof Geras*. The break-even point on optimization trajectories of deep neural net-
works. In International Conference on Learning Representations , 2020. URL https://openreview.
net/forum?id=r1g87C4KwB .
Stanisław Jastrz e ¸bski, Devansh Arpit, Oliver Astrand, Giancarlo B Kerg, Huan Wang, Caiming
Xiong, Richard Socher, Kyunghyun Cho, and Krzysztof J Geras. Catastrophic fisher explosion:
Early phase fisher matrix impacts generalization. In Proceedings of the 38th International Conference
on Machine Learning , Proceedings of Machine Learning Research. PMLR, 2021. URL https:
//proceedings.mlr.press/v139/jastrzebski21a.html .
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Dmitry Kopitkov and Vadim Indelman. Neural spectrum alignment: Empirical study. In Artificial
Neural Networks and Machine Learning–ICANN 2020: 29th International Conference on Artificial Neural
Networks, Bratislava, Slovakia, September 15–18, 2020, Proceedings, Part II 29 , pages 168–179. Springer,
2020.
16

--- PAGE 17 ---
Itai Kreisler, Mor Shpigel Nacson, Daniel Soudry, and Yair Carmon. Gradient descent mono-
tonically decreases the sharpness of gradient flow solutions in scalar networks and beyond.
InProceedings of the 40th International Conference on Machine Learning , Proceedings of Machine
Learning Research. PMLR, 23–29 Jul 2023. URL https://proceedings.mlr.press/v202/
kreisler23a.html .
Alex Krizhevsky and Geoffrey Hinton. Learning Multiple Layers of Features from Tiny Images.
Technical report, Citeseer, 2009.
Ananya Kumar, Ruoqi Shen, Sébastien Bubeck, and Suriya Gunasekar. How to fine-tune vision
models with sgd. arXiv preprint arXiv:2211.09359 , 2022.
Frederik Kunstner, Jacques Chen, Jonathan Wilder Lavington, and Mark Schmidt. Noise is not
the main factor behind the gap between sgd and adam on transformers, but sign descent might
be. In The Eleventh International Conference on Learning Representations , 2023. URL https://
openreview.net/forum?id=a65YK0cqH8g .
Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large
learning rate phase of deep learning: the catapult mechanism. arXiv preprint arXiv:2003.02218 ,
2020.
Xinyan Li, Qilong Gu, Yingxue Zhou, Tiancong Chen, and Arindam Banerjee. Hessian based analysis
of sgd for deep nets: Dynamics and generalization. In Proceedings of the 2020 SIAM International
Conference on Data Mining , pages 190–198. SIAM, 2020.
Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large
learning rate in training neural networks. Advances in Neural Information Processing Systems , 32,
2019.
Chao Ma and Lexing Ying. On linear stability of sgd and input-smoothness of neural networks. In
Advances in Neural Information Processing Systems , volume 34, pages 16805–16817. Curran Associates,
Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/
8c26d2fad09dc76f3ff36b6ea752b0e1-Paper.pdf .
Chao Ma, Daniel Kunin, Lei Wu, and Lexing Ying. Beyond the quadratic approximation: the
multiscale structure of neural network loss landscapes. arXiv preprint arXiv:2204.11326 , 2022.
Lachlan Ewen MacDonald, Jack Valmadre, and Simon Lucey. On progressive sharpening, flat
minima and generalisation. arXiv preprint arXiv:2305.14683 , 2023.
Karttikeya Mangalam and Vinay Uday Prabhu. Do deep neural networks learn shallow learnable
examples first? In ICML 2019 Workshop on Identifying and Understanding Deep Learning Phenomena ,
2019. URL https://openreview.net/forum?id=HkxHv4rn24 .
William Merrill, Nikolaos Tsilivis, and Aman Shukla. A tale of two circuits: Grokking as competition
of sparse and dense subnetworks. In ICLR 2023 Workshop on Mathematical and Empirical Understand-
ing of Foundation Models , 2023. URL https://openreview.net/forum?id=8GZxtu46Kx .
Rotem Mulayoff and Tomer Michaeli. Unique properties of flat minima in deep networks. In
Proceedings of the 37th International Conference on Machine Learning , volume 119 of Proceedings of
Machine Learning Research . PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/
v119/mulayoff20a.html .
17

--- PAGE 18 ---
Preetum Nakkiran, Gal Kaplun, Dimitris Kalimeris, Tristan Yang, Benjamin L Edelman, Fred Zhang,
and Boaz Barak. Sgd on neural networks learns functions of increasing complexity. arXiv preprint
arXiv:1905.11604 , 2019.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In International Conference on Learning
Representations , 2020. URL https://openreview.net/forum?id=B1g5sA4twr .
Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures
for grokking via mechanistic interpretability. In The Eleventh International Conference on Learning
Representations , 2023. URL https://openreview.net/forum?id=9XFSbDPmdW .
Pascal Notsawo Jr, Hattie Zhou, Mohammad Pezeshki, Irina Rish, Guillaume Dumas, et al. Predicting
grokking long before it happens: A look into the loss landscape of models which grok. arXiv
preprint arXiv:2306.13253 , 2023.
Samet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization guaran-
tees for neural networks via harnessing the low-rank structure of the jacobian. arXiv preprint
arXiv:1906.05392 , 2019.
Yan Pan and Yuanzhi Li. Toward understanding why adam converges faster than sgd for transform-
ers. arXiv preprint arXiv:2306.00204 , 2023.
Vardan Papyan. The full spectrum of deepnet hessians at scale: Dynamics with sgd training and
sample size. arXiv preprint arXiv:1811.07062 , 2018.
Vardan Papyan. Measurements of three-level hierarchical structure in the outliers in the spectrum of
deepnet hessians. In Proceedings of the 36th International Conference on Machine Learning , volume 97
ofProceedings of Machine Learning Research . PMLR, 09–15 Jun 2019. URL https://proceedings.
mlr.press/v97/papyan19a.html .
Vardan Papyan. Traces of class/cross-class structure pervade deep learning spectra. The Journal of
Machine Learning Research , 21(1):10197–10260, 2020.
Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Gen-
eralization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177 ,
2022.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. Domain-adjusted regression or: Erm
may already learn features sufficient for out-of-distribution generalization. arXiv preprint
arXiv:2202.06856 , 2022.
Levent Sagun, Leon Bottou, and Yann LeCun. Eigenvalues of the hessian in deep learning: Singular-
ity and beyond. arXiv preprint arXiv:1611.07476 , 2016.
Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the
hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454 , 2017.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normal-
ization help optimization? Advances in neural information processing systems , 31, 2018.
18

--- PAGE 19 ---
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics
of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120 , 2013.
Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The
pitfalls of simplicity bias in neural networks. Advances in Neural Information Processing Systems , 33:
9573–9585, 2020.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556 , 2014.
Vimal Thilak, Etai Littwin, Shuangfei Zhai, Omid Saremi, Roni Paiss, and Joshua Susskind. The
slingshot mechanism: An empirical study of adaptive optimizers and the grokking phenomenon.
arXiv preprint arXiv:2206.04817 , 2022.
Guillermo Valle-Perez, Chico Q. Camargo, and Ard A. Louis. Deep learning generalizes because the
parameter-function map is biased towards simple functions. In International Conference on Learning
Representations , 2019. URL https://openreview.net/forum?id=rye4g3AqFm .
Shengjie Wang, Abdel-rahman Mohamed, Rich Caruana, Jeff Bilmes, Matthai Plilipose, Matthew
Richardson, Krzysztof Geras, Gregor Urban, and Ozlem Aslan. Analysis of deep neural networks
with extended data jacobian matrix. In International Conference on Machine Learning , pages 718–726.
PMLR, 2016.
Zixuan Wang, Zhouzi Li, and Jian Li. Analyzing sharpness along GD trajectory: Progressive
sharpening and edge of stability. In Advances in Neural Information Processing Systems , 2022. URL
https://openreview.net/forum?id=thgItcQrJ4y .
Kaiyue Wen, Tengyu Ma, and Zhiyuan Li. Sharpness minimization algorithms do not only minimize
sharpness to achieve better generalization. arXiv preprint arXiv:2307.11007 , 2023.
Lei Wu, Chao Ma, and Weinan E. How sgd selects the global minima in over-parameterized learning:
A dynamical stability perspective. In Advances in Neural Information Processing Systems , volume 31.
Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/
paper/2018/file/6651526b6fb8f29a00507de6a49ce30f-Paper.pdf .
Lei Wu, Mingze Wang, and Weijie J Su. The alignment property of SGD noise and how it helps select
flat minima: A stability analysis. In Advances in Neural Information Processing Systems , 2022. URL
https://openreview.net/forum?id=rUc8peDIM45 .
Chen Xing, Devansh Arpit, Christos Tsirigotis, and Yoshua Bengio. A walk with sgd. arXiv preprint
arXiv:1802.08770 , 2018.
Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi, Sanjiv
Kumar, and Suvrit Sra. Why are adaptive methods good for attention models? Advances in Neural
Information Processing Systems , 33:15383–15393, 2020.
Xingyu Zhu, Zixuan Wang, Xiang Wang, Mo Zhou, and Rong Ge. Understanding edge-of-stability
training dynamics with a minimalist example. In The Eleventh International Conference on Learning
Representations , 2023. URL https://openreview.net/forum?id=p7EagBsMAEO .
Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in stochastic
gradient descent: Its behavior of escaping from sharp minima and regularization effects. In
Proceedings of the 36th International Conference on Machine Learning , Proceedings of Machine Learning
Research, pages 7654–7663. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/
v97/zhu19e.html .
19

--- PAGE 20 ---
A Related Work
Characterizing the NN loss landscape. Earlier studies of the loss landscape commonly identified
a heavy-tailedness with a small group of very large outlier Hessian eigenvalues or Jacobian singular
values (Sagun et al., 2016, 2017; Papyan, 2018; Oymak et al., 2019; Papyan, 2019; Fort and Ganguli,
2019; Ghorbani et al., 2019; Li et al., 2020; Papyan, 2020; Kopitkov and Indelman, 2020). Later efforts
focused on concretely linking these observations to corresponding behavior, often with an emphasis
on SGD’s bias towards particular solutions (Wu et al., 2018; Jastrz e ¸bski et al., 2017, 2020) and what
this may imply about its resulting generalization (Jastrz e ¸bski et al., 2019; Zhu et al., 2019; Wu et al.,
2022). Our method for identifying these paired groups, along with Figure 2, indicates that these
outlier directions in the Hessian/Jacobian spectrum are precisely the directions with opposing
signals in the gradient, and that this pattern may be key to better understanding the generalization
ability of NNs trained with SGD.
Progressive sharpening and the edge of stability. Shifting away from the overall structure, more
recent focus has been specifically on top eigenvalue(s), where it was empirically observed that their
magnitude (the loss “sharpness”) grows when training with SGD (Jastrz e ¸bski et al., 2019, 2020) and
GD (Kopitkov and Indelman, 2020; Cohen et al., 2021) (so-called “progressive sharpening”). This
leads to rapid oscillation in weight space (Xing et al., 2018; Jastrz e ¸bski et al., 2019; Cohen et al., 2021,
2022). Cohen et al. (2021) also found that for GD this coincides with a consistent yet non-monotonic
decrease in training loss over long timescales, which they named the “edge of stability”; moreover,
they noted that this behavior runs contrary to our traditional understanding of NN convergence.
Many works have since investigated the possible origins of this phenomenon (Zhu et al., 2023;
Kreisler et al., 2023). Several of these are deeply related to our findings: Ma et al. (2022) connect
this behavior to the existence of multiple “scales” of losses; the outliers we identify corroborate
this point. Damian et al. (2022) prove that GD implicitly regularizes the sharpness—we identify
a conceptually distinct source of such regularization, as described in Section 3. Arora et al. (2022)
show under some conditions that the GD trajectory follows a minimum-loss manifold towards lower
curvature regions. This is consistent with our findings, and we believe this manifold to be precisely
the path which evenly balances the opposing gradients. Wang et al. (2022) provide another thorough
analysis of NN training dynamics at the edge of stability; their demonstrated phases closely align
with our own. They further observe that this sharpening coincides with a growth in the norm of the
last layer, which was also noted by MacDonald et al. (2023). Our proposed explanation for the effect
of opposing signals offers some insight into this relationship, but further investigation is needed.
20

--- PAGE 21 ---
B Examples of Opposing Signals in Text
Punctuation Ordering
the EU is “the best war-avoidance mechanism ever invented[”].
the 2008 economic crash and in doing so “triggered a crisis of rejection[”].
Don’t farm this thing out under the guise of a “contest[”].
I really thought she was going to use another C-word besides “coward[”].
because it was one of the few that still “dry-farmed[”].
He describes the taste as “almost minty[”].
I did receive several offers to “help out a bit[”].
we can from our investments, regardless of the costs to the rest of society[”].
Or “it won’t make a difference anyway[”].
Nor is it OK to say “the real solution is in a technological breakthrough[”].
and that’s what they mean by “when complete[”].
and that the next big investment bubble to burst is the “carbon bubble[”].
he had been “driven by ideological and political motives[”].
Prime Minister Najib Razak’s personal bank account was a “genuine donation[”].
exceptional intellect, unparalleled integrity, and record of independence[”].
was the “most consequential decision I’ve ever been involved with[”].
which some lawmakers have called the “filibuster of all filibusters[”].
Democrats vowed to filibuster what some openly called a “stolen seat[”].
His leather belt was usually the delivery method of choice.[”]
I used to catch me a few and make pets out of them.[”]
catch pneumonia because I got my underwear on, but Bob here is naked.’ [”]
My husband-to-be built a gun cabinet. It was that type of community: normal.[”]
‘use both hands.’ That was Ricky to a tee. He’s a jokester.[”]
He added, with a half-smile, “I’m guessing he didn’t mean the drinking.[”]
on the county road. If you can’t get to it, that doesn’t make sense.’ [”]
“It’s laborious and boring. He loved excitement and attention.[”]
‘That little son-of-a-gun is playing favorites,’ and turned it against him.[”]
“For medicinal purposes, for medical purposes, absolutely, it’s fine.[”]
“That’s a choice that growers make. It’s on their side of the issue.[”]
to be a good steward of your land. You have to make big decisions in a hurry.[”]
“But of course modern farming looks for maximum yield no matter what you have to
put in. And in the case of California, that input is water.[”]
And we have been drawing down on centuries of accumulation. Pretty soon those
systems are not going to be able to provide for us.[”]
not a luxury crop like wine. I’m really excited ab out the potential.[”]
“We knew and still believe that it was the right thing to do.[”]
spending lots of time in the wind tunnel, because it shows when we test them.[”]
Compliance was low on the list, but I think it’s a pretty comfortable bike.[”]
the opportunity to do that.’ I just needed to take it and run with it.[”]
Figure 7: Examples of opposing signals in text. Found by training GPT-2 on a subset of OpenWebText. Sequences are on
separate lines, the token in brackets is the target and all prior tokens are (the end of the) context. As both standards are
used, it is not always clear whether punctuation will come before or after the end of a quotation (we include the period
after the quote for clarity—the model does not condition on it). Note that the double quotation is encoded as the pairof
tokens [447, 251] , and the loss oscillation is occurring for sequences that end with this pair, either before (top) or after
(bottom) the occurrence of the period token ( 13).
21

--- PAGE 22 ---
New Line or ‘the’ After Colon
In order to prepare your data, there are three things to do:[\n]
in the FP lib of your choice, namely Scalaz or Cats. It looks like this:[\n]
Let the compiler guide you, it will only accept one implementation:[\n]
Salcedo said of the work:[\n]
Enter your email address:[\n]
According to the CBO update:[\n]
Here’s how the Giants can still make the playoffs:[\n]
described how he copes with his condition in an interview with The Telegraph:[\n]
Here’s a list of 5 reasons as to why self diagnosis is valid:[\n]
successive Lambda invocations. It looks more or less like this:[\n]
data, there are three things to do:[\n]
4.2 percent in early 2018.\n\nAccording to the CBO update:[\n]
other than me being myself.”\n\nWATCH:[\n]
is to make the entire construction plural.\n\nTwo recent examples:[\n]
We offer the following talking points to anyone who is attending the meeting:[\n]
is on the chopping block - and at the worst possible moment:[\n]
as will our MPs in Westminster. But to me it is obvious: [the]
The wheelset is the same as that on the model above: [the]
not get so engrained or in a rut with what I had been doing. Not to worry: [the]
polemics against religion return in various ways to one core issue: [the]
which undergirds all other acts of love, both divine and human: [the]
integrate fighters from the Kurds’ two main political parties: [the]
robs this incredible title of precisely what makes it so wonderful: [the]
you no doubt noticed something was missing: [the]
Neil Gorsuch’s ’sexist’ comments on maternity leave: [the]
Figure 8: Examples of opposing signals in text. Found by training GPT-2 on a subset of OpenWebText. Sequences are on
separate lines, the token in brackets is the target and all prior tokens are (the end of the) context. Sometimes a colon occurs
mid-sentence—and is often followed by “the”—other times it announces the start of a new line. The model must unlearn “:
7→[\n] ” versus “: 7→[the] ” and instead use other contextual information.
0 25 50 75 100 125 150 175 200
Iteration24681012Lossnewline after colon
“the” after colon
Figure 9: Loss of GPT-2 on the above opposing signals.
22

--- PAGE 23 ---
C Reproducing Figure 5 in Other Settings
Though colors are straightforward, for some opposing signals such as grass texture it is not clear
how to produce a synthetic image which properly captures what precisely the model is latching on
to. Instead, we identify a real image which has as much grass and as little else as possible, with the
understanding that the additional signal in the image could affect the results. We depict the grass
image alongside the plots it produced.
C.1 ResNet-18 Trained with GD on Other Inputs
0 200 400 600 800
Iteration−10−50510Logit ValueClass Logits
plane
auto
bird
cat
deer
dog
frog
horse
ship
truck0 200 400 600 800
Iteration510152025303540NormFeature Norm
Input Image
Mean Over DataIter 100 Iter 101
Iter 102 Iter 103Class Probabilities During OscillationInput Image
Figure 10: ResNet-18 on a red color block.
0 200 400 600 800
Iteration−6−4−20246Logit ValueClass Logits
plane
auto
bird
cat
deer
dog
frog
horse
ship
truck0 200 400 600 800
Iteration10203040NormFeature Norm
Input Image
Mean Over DataIter 814 Iter 815
Iter 816 Iter 817Class Probabilities During OscillationInput Image
Figure 11: ResNet-18 on a green color block. As this color seems unnatural, we’ve included two examples of relevant
images in the dataset.
Figure 12: Examples of images with the above green color.
23

--- PAGE 24 ---
0 200 400 600 800
Iteration−6−4−20246Logit ValueClass Logits
plane
auto
bird
cat
deer
dog
frog
horse
ship
truck0 1000 2000 3000 4000 5000
Iteration468101214NormFeature Norm
Input Image
Mean Over DataIter 538 Iter 539
Iter 540 Iter 541Class Probabilities During OscillationInput ImageFigure 13: ResNet-18 on a white color block.
0 200 400 600 800
Iteration−6−4−202468Logit ValueClass Logits
plane
auto
bird
cat
deer
dog
frog
horse
ship
truck0 1000 2000 3000 4000 5000
Iteration68101214NormFeature Norm
Input Image
Mean Over DataIter 188 Iter 189
Iter 190 Iter 191Class Probabilities During OscillationInput Image
Figure 14: ResNet-18 on a black color block.
0 200 400 600 800
Iteration−6−4−20246Logit ValueClass Logits
plane
auto
bird
cat
deer
dog
frog
horse
ship
truck0 200 400 600 800
Iteration5678910NormFeature Norm
Input Image
Mean Over DataIter 264 Iter 265
Iter 266 Iter 267Class Probabilities During Oscillation
Figure 15: ResNet-18 on an image with mostly grass texture.
24

--- PAGE 25 ---
C.2 VGG-11-BN Trained with GD
For VGG-11, we found that the feature norm of the embedded images did not decay nearly as much
over the course of training. We expect this has to do with the lack of a residual component. However,
for the most part these features do still follow the pattern of a rapid increase, followed by a marked
decline.
0 100 200 300
Iteration−4−20246Logit ValueClass Logits
plane
auto
bird
cat
deer
dog
frog
horse
ship
truck0 100 200 300
Iteration468101214NormFeature Norm
Input Image
Mean Over DataIter 91 Iter 92
Iter 93 Iter 94Class Probabilities During OscillationInput Image
Figure 16: VGG-11-BN on a sky color block.
0 100 200 300
Iteration−4−20246Logit ValueClass Logits
plane
auto
bird
cat
deer
dog
frog
horse
ship
truck0 100 200 300
Iteration46810121416NormFeature Norm
Input Image
Mean Over DataIter 91 Iter 92
Iter 93 Iter 94Class Probabilities During OscillationInput Image
Figure 17: VGG-11-BN on a red color block.
0 100 200 300
Iteration−4−2024Logit ValueClass Logits
plane
auto
bird
cat
deer
dog
frog
horse
ship
truck0 100 200 300
Iteration6810121416NormFeature Norm
Input Image
Mean Over DataIter 79 Iter 80
Iter 81 Iter 82Class Probabilities During OscillationInput Image
Figure 18: VGG-11-BN on a green color block. See above for two examples of relevant images in the dataset.
25

--- PAGE 26 ---
0 100 200 300
Iteration−4−20246Logit ValueClass Logits
plane
auto
bird
cat
deer
dog
frog
horse
ship
truck0 100 200 300
Iteration468101214NormFeature Norm
Input Image
Mean Over DataIter 79 Iter 80
Iter 81 Iter 82Class Probabilities During OscillationFigure 19: VGG-11-BN on an image with mostly grass texture.
0 100 200 300
Iteration−2024Logit ValueClass Logits
plane
auto
bird
cat
deer
dog
frog
horse
ship
truck0 100 200 300
Iteration468101214NormFeature Norm
Input Image
Mean Over DataIter 188 Iter 189
Iter 190 Iter 191Class Probabilities During OscillationInput Image
Figure 20: VGG-11-BN on a white color block.
0 100 200 300
Iteration−4−20246Logit ValueClass Logits
plane
auto
bird
cat
deer
dog
frog
horse
ship
truck0 100 200 300
Iteration468101214NormFeature Norm
Input Image
Mean Over DataIter 40 Iter 41
Iter 42 Iter 43Class Probabilities During OscillationInput Image
Figure 21: VGG-11-BN on a black color block.
26

--- PAGE 27 ---
C.3 VGG-11-BN with Small Learning Rate to Approximate Gradient Flow
Here we see that oscillation is a valuable regularizer, preventing the network from continuously
upweighting opposing signals. As described in the main body, stepping too far in one direction
causes an imbalanced gradient between the two opposing signals. Since the group which now has a
larger loss is also the one which suffers from the use of the feature, the network is encouraged to
downweight its influence. If we use a very small learning rate to approximate gradient flow, this
regularization does not occur and the feature norms grow continuously. This leads to over-reliance
on these features, suggesting that failing to downweight opposing signals is a likely cause of the
poor generalization of networks trained with gradient flow.
The following plots depict a VGG-11-BN trained with learning rate .0005 to closely approximate
gradient flow. We compare this to the feature norms of the same network trained with gradient
descent with learning rate 0.1, which closely matches gradient flow until it becomes unstable.
0 1 2 3 4 5 6 7 8
Time−3−2−101234Logit ValueClass Logits
plane
auto
bird
cat
deer
dog
frog
horse
ship
truck0 1 2 3 4 5 6 7 8
Time571012151720NormFeature Norm
Input Image (Gradient Flow)
Mean Over Data
Input Image (Gradient Descent)Input Image
Figure 22: VGG-11-BN on a sky color block with learning rate 0.005 (approximating gradient flow) compared to 0.1.
0 1 2 3 4 5 6 7 8
Time−2−101Logit ValueClass Logits
plane
auto
bird
cat
deer
dog
frog
horse
ship
truck0 1 2 3 4 5 6 7 8
Time571012151720NormFeature Norm
Input Image (Gradient Flow)
Mean Over Data
Input Image (Gradient Descent)Input Image
Figure 23: VGG-11-BN on a white color block with learning rate 0.005 (approximating gradient flow) compared to 0.1.
27

--- PAGE 28 ---
Figure 24: VGG-11-BN on a black color block with learning rate 0.005 (approximating gradient flow) compared to 0.1.
0 1 2 3 4 5 6 7 8
Time−3−2−1012Logit ValueClass Logits
plane
auto
bird
cat
deer
dog
frog
horse
ship
truck0 1 2 3 4 5 6 7 8
Time57101215172022NormFeature Norm
Input Image (Gradient Flow)
Mean Over Data
Input Image (Gradient Descent)Input Image
Figure 25: VGG-11-BN on a red color block with learning rate 0.005 (approximating gradient flow) compared to 0.1.
0 1 2 3 4 5 6 7 8
Time−4−202Logit ValueClass Logits
plane
auto
bird
cat
deer
dog
frog
horse
ship
truck0 1 2 3 4 5 6 7 8
Time510152025NormFeature Norm
Input Image (Gradient Flow)
Mean Over Data
Input Image (Gradient Descent)Input Image
Figure 26: VGG-11-BN on a green color block with learning rate 0.0005 (approximating gradient flow) compared to 0.1.
0 1 2 3 4 5 6 7 8
Time−4−2024Logit ValueClass Logits
plane
auto
bird
cat
deer
dog
frog
horse
ship
truck0 1 2 3 4 5 6 7 8
Time571012151720NormFeature Norm
Input Image (Gradient Flow)
Mean Over Data
Input Image (Gradient Descent)
Figure 27: VGG-11-BN on an image with mostly grass texture with learning rate 0.0005 (approximating gradient flow)
compared to 0.1.
28

--- PAGE 29 ---
C.4 ResNet-18 Trained with Full-Batch Adam
Finally, we plot the same figures for a ResNet-18 trained with full-batch Adam. We see that Adam
consistently and quickly reduces the norm of these features, especially for more complex features
such as texture, and that it also quickly reaches a point where oscillation ends. Note when comparing
to plots above that the maximum iteration on the x-axis differs.
0 200 400 600 800 1000
Iteration−8−6−4−2024Logit ValueClass Logits
plane
auto
bird
cat
deer
dog
frog
horse
ship
truck0 200 400 600 800 1000
Iteration571012151720NormFeature Norm
Input Image
Mean Over DataInput Image
Figure 28: ResNet-18 on a sky color block trained with Adam.
0 200 400 600 800 1000
Iteration−20−1001020Logit ValueClass Logits
plane
auto
bird
cat
deer
dog
frog
horse
ship
truck0 200 400 600 800 1000
Iteration102030405060NormFeature Norm
Input Image
Mean Over DataInput Image
Figure 29: ResNet-18 on a red color block trained with Adam.
0 200 400 600 800 1000
Iteration−10−50510Logit ValueClass Logits
plane
auto
bird
cat
deer
dog
frog
horse
ship
truck0 200 400 600 800 1000
Iteration102030405060NormFeature Norm
Input Image
Mean Over DataInput Image
Figure 30: ResNet-18 on a green color block trained with Adam.
29

--- PAGE 30 ---
0 200 400 600 800 1000
Iteration−6−4−2024Logit ValueClass Logits
plane
auto
bird
cat
deer
dog
frog
horse
ship
truck0 200 400 600 800 1000
Iteration681012NormFeature Norm
Input Image
Mean Over DataFigure 31: ResNet-18 on an image with mostly grass texture trained with Adam.
30

--- PAGE 31 ---
D Tracking the Amount of Curvature in each Parameter Layer
Here we plot the “fraction of curvature” of different architectures at various training steps. Recall
the fraction of curvature is defined with respect to the top eigenvector of the loss Hessian. We
partition this vector by network layer and evaluate each sub-vector’s squared norm. This represents
that layer’s contribution to the overall curvature. To keep the plots readable, we omit layers whose
fraction is never greater than 0.01at any training step (including the intermediate ones not plotted),
though we always include the last linear layer. The total number of layers is 45, 38, 106, and 39 for
the ResNet, VGG-11, ViT, and NanoGPT respectively.
conv1.weight
layer1.0.conv1.weight
layer1.0.conv2.weight
layer1.1.conv1.weight
layer1.1.conv2.weight
layer2.0.conv1.weight
layer2.0.conv2.weight
layer2.1.conv1.weight
layer2.1.conv2.weight
layer3.0.conv1.weight
fc.weight
fc.bias0.00.20.40.60.81.0Sum Per Parameter BlockResNet-18Step 10
Step 210
Step 410
Step 610
Step 810
Figure 32: Sum of squared entries of the top eigenvector of the loss Hessian which lie in each parameter layer of a
ResNet-18 throughout training.
features.0.weight
features.4.weight
features.8.weight
features.11.weight
features.15.weight
features.18.weight
features.22.weight
features.25.weight
classiﬁer.0.weight
classiﬁer.2.weight
fc.weight
fc.bias0.00.20.40.60.8Sum Per Parameter BlockVGG-11-BNStep 10
Step 80
Step 150
Step 220
Step 290
Figure 33: Sum of squared entries of the top eigenvector of the loss Hessian which lie in each parameter layer of a
VGG-11-BN throughout training.
31

--- PAGE 32 ---
to patch embedding.to patch tokens.1.bias
to patch embedding.to patch tokens.2.weight
to patch embedding.to patch tokens.2.bias
transformer.layers.0.0.to qkv.weight
transformer.layers.0.0.to out.0.weight
transformer.layers.0.0.to out.0.bias
transformer.layers.0.1.net.1.weight
transformer.layers.0.1.net.4.weight
transformer.layers.0.1.net.4.bias
transformer.layers.1.0.to qkv.weight
transformer.layers.1.0.to out.0.weight
transformer.layers.1.0.to out.0.bias
transformer.layers.1.1.net.1.weight
transformer.layers.1.1.net.4.weight
transformer.layers.1.1.net.4.bias
transformer.layers.2.0.to qkv.weight
transformer.layers.2.0.to out.0.weight
transformer.layers.2.1.net.1.weight
transformer.layers.2.1.net.4.weight
transformer.layers.3.0.to qkv.weight
transformer.layers.3.0.to out.0.weight
transformer.layers.3.1.net.1.weight
transformer.layers.4.0.to qkv.weight
transformer.layers.4.0.to out.0.weight
transformer.layers.4.1.net.1.weight
transformer.layers.4.1.net.4.weight
transformer.layers.5.0.to out.0.weight
transformer.layers.5.1.net.1.weight
transformer.layers.6.0.to out.0.weight
transformer.layers.6.1.net.1.weight
transformer.layers.7.0.to out.0.weight
transformer.layers.7.1.net.1.weight
transformer.layers.7.1.net.4.weight
mlp head.1.weight
mlp head.1.bias0.00.10.20.30.4Sum Per Parameter BlockViTStep 10
Step 300
Step 590
Step 880
Step 1170Figure 34: Sum of squared entries of the top eigenvector of the loss Hessian which lie in each parameter layer of a ViT
throughout training.
transformer.wte.weight
transformer.h.0.attn.c attn.weight
transformer.h.0.attn.c proj.weight
transformer.h.0.mlp.c fc.weight
transformer.h.0.mlp.c proj.weight
transformer.h.1.attn.c attn.weight
transformer.h.1.attn.c proj.weight
transformer.h.1.mlp.c fc.weight
transformer.h.1.mlp.c proj.weight
transformer.h.2.attn.c proj.weight
transformer.h.2.mlp.c fc.weight
transformer.h.2.mlp.c proj.weight
transformer.h.3.attn.c proj.weight
transformer.h.3.mlp.c fc.weight
transformer.h.3.mlp.c proj.weight
transformer.h.4.attn.c attn.weight
transformer.h.4.attn.c proj.weight
transformer.h.4.mlp.c fc.weight
transformer.h.4.mlp.c proj.weight
transformer.h.5.attn.c attn.weight
transformer.h.5.attn.c proj.weight
transformer.h.5.mlp.c fc.weight
transformer.h.5.mlp.c proj.weight0.000.050.100.150.200.25Sum Per Parameter BlockNanoGPTStep 10
Step 520
Step 1030
Step 1540
Step 2050
Figure 35: NanoGPT (6 layers, 6 head per layer, embedding dimension 384) trained on the default shakespeare character
dataset in the NanoGPT repository. Due to difficulty calculating the true top eigenvector, we approximate it with the
exponential moving average of the squared gradient.
32

--- PAGE 33 ---
E Additional Figures
0 200 400
Iteration0.200.250.300.350.400.450.500.55FractionGD
0 200 400
IterationSGD
ResNet
VGGFraction of Points That Increase in Loss Per Step
Figure 36: The fraction of overall training points which increase in loss on any given step. For both SGD and GD, it hovers
around 0.5 (VGG without batchnorm takes a long time to reach the edge of stability). Though the outliers have much
higher amplitude in their loss change, many more images contain some small component of the features they exemplify (or
are otherwise slightly affected by the weight oscillations), and so these points also oscillate in loss at a smaller scale.
024LossGroup 1
Group 2
Full Train Loss
Random Subset
24
160 180 200 220 240
Iteration24Loss
160 180 200 220 240
Iteration0246VGG-11 SGD Loss on Opposing Groups
Figure 37: We reproduce Figure 6(a) without batch normalization.
33

--- PAGE 34 ---
0 25 50 75 100 125 150 1751.52.0Loss
24
0 25 50 75 100 125 150 175020Sharpness
0 25 50 75 100 125 150 1750.00.1Weight Projection on T op Eigenvectorfull loss
group 1
group 1(a) A 3-layer ReLU MLP trained on a 5k-subset of
CIFAR-10.
0 25 50 75 100 125 150 1750.00.10.2Loss
024
0 25 50 75 100 125 150 17512Sharpness
0 25 50 75 100 125 150 1750.025
0.0000.025Weight Projection on T op Eigenvectorfull loss
group 1
group 2(b) Our model: a 2-layer linear network trained on
mostly Gaussian data with opposing signals.
Figure 38: We compare a small ReLU MLP on a subset of CIFAR-10 to our simple model of linear regression with a
two-layer network.
34

--- PAGE 35 ---
F Comparing our Variant of SGD to Adam
Algorithm 1 SplitSGD
input: Initial parameters θ0, SGD step size η1, SignSGD step size η2, momentum β, dampening τ,
threshold r.
initialize: m0=0.
fort←1, . . . , T do
gt← ∇ θLt(θt−1) ▷Get stochastic gradient
mt←βmt−1+ (1−τ)gt ▷Update momentum with dampening
ˆmt←mt/(1−τt) ▷Debias
vmask←1{|ˆmt| ≤r} ▷Split parameters by threshold
θt←θt−1−η1( ˆmt⊙vmask)−η2(sign( ˆmt)⊙(1−vmask))▷unmasked SGD, masked SignSGD
end for
As described in the main text, we find that simply including dampening and taking a fixed step
size on gradients above a certain threshold results in performance matching that of Adam for the
experiments we tried. We found that setting this threshold equal to the q=.1quantile of the first
gradient worked quite well—this was about 1e-4 for the ResNet-56/110 and 1e-6 for GPT-2.
Simply to have something to label it with, we name the method SplitSGD, because it performs
SGD and SignSGD on different partitions of the parameters. The precise method is given above
in Algorithm 1. We reiterate that we are not trying to suggest a new method—our goal is only to
demonstrate the insight gained from knowledge of opposing signals’ influence on NN optimization.
For all plots, βrepresents the momentum parameter and τis dampening. Adam has a single
parameter β1which represents both simultaneously, which we fix at 0.9, and we do the same for
SplitSGD by setting β=τ= 0.9. As in Algorithm 1, we let η1refer to the learning rate for standard
SGD on the parameters with gradient below the magnitude threshold, and η2refers to the learning
rate for the remainder which are optimized with SignSGD.
F.1 SplitSGD on ResNet
0 500 1000 1500 2000
Iteration0.00.51.01.52.02.53.03.5LossTrain Loss
SplitSGD h=0.1
h=1.0, b=0.0, t=0.0
h=0.1, b=0.0, t=0.0
h=0.1, b=0.9, t=0.0
h=0.1, b=0.9, t=0.9
h=0.01, b=0.9, t=0.0
0 500 1000 1500 2000
Iteration0.20.40.60.8AccuracyTest Accuracy
0 500 1000 1500 2000
Iteration0.20.40.60.8FractionParameters With SignSGD
Figure 39: Standard SGD with varying learning rates and momentum/dampening parameters on a ResNet-56 on CIFAR-
10, with one run of SplitSGD for comparison. Omitted SGD hyperparameter combinations performed much worse. Notice
that SGD is extremely sensitive to hyperparameters. Rightmost plot is the fraction of parameters with fixed step size by
SplitSGD.
35

--- PAGE 36 ---
We begin with a comparison on ResNets trained on CIFAR-10. Figure 39 compares SplitSGD to
standard versions of SGD with varying momentum and dampening on a ResNet-56. As expected,
SGD is extremely sensitive to hyperparameters, particularly the learning rate, and even the best
choice in a grid search underperforms SplitSGD. Furthermore, the rightmost plot depicts the fraction
of parameters for which SplitSGD takes a fixed-size signed step. This means that after the first
few training steps, 70-80% of the parameters are being optimized simply with standard SGD (with
β=τ= 0.9).
0 500 1000 1500 2000
Iteration0.51.01.52.02.5LossTrain Loss
SplitSGD
SplitSGD h1=0
Adam h=.005
0 500 1000 1500 2000
Iteration0.20.40.60.8AccuracyTest Accuracy
0 500 1000 1500 2000
Iteration0.00.20.40.60.8FractionParameters With SignSGD
10−410−310−210−1100101102103
SGD Learning Rate h1
Figure 40: SplitSGD with varying SGD learning rates η1versus Adam on a ResNet-56 on CIFAR-10. The SignSGD learning
rate is fixed at η2=.001; Adam uses η=.005, which was found to be the best performing choice via oracle selection grid
search. The rightmost plot is the fraction of parameters with fixed step size by SplitSGD—that is, 1 minus this value is the
fraction of parameters taking a regular gradient step with step size as given in the legend. This learning rate ranges over
several orders of magnitude, is used for ~70-80% of parameters, and can even be set to 0, with no discernible difference in
performance.
Next, Figure 40 plots SplitSGD with varying η1andη2fixed at .001. This is compared to Adam
with learning rate .005, which was chosen via oracle grid search. Even though the SGD learning
rateη1ranges over seven orders of magnitude and is used for ~70-80% of parameters, we see no real
difference in the train loss or test accuracy of SplitSGD. In fact, we find that we can even eliminate
it completely! This suggests that for most of parameters and most of training, it is only a small
fraction of parameters in the entire network which are influencing the overall performance. We posit
a deeper connection here to the “hidden” progress described in grokking (Barak et al., 2022; Nanda
et al., 2023)—if the correct subnetwork and its influence on the output grows slowly during training,
that behavior will not be noticeable until the dominating signals are first downweighted.
Figures 41 and 42 depict the train loss and test accuracy of Adam and SplitSGD for varying
learning rates (the standard SGD learning rate η1is fixed at 0.1). We see that SplitSGD is at least as
robust as Adam to learning rate choice, if not more. The results also suggest that SplitSGD benefits
from a slightly smaller learning rate than Adam, which we attribute to the fact that it will always
take step sizes of that fixed size, whereas the learning rate for Adam represents an upper bound on
the step size for each parameter.
We repeat these experiments with a ResNet-110, with similar findings. Figure 43(a) compares the
train loss and test accuracy of SGD with β= 0.9, τ= 0to Adam, and again the sensitivity of this
optimizer to learning rate is clear. Figure 43(b) compares Adam to SplitSGD (both with fixed-step
learning rate .0003) but ablates the use of dampening: we find that the fixed-size signed steps appear
to be more important for early in training, while dampening is helpful for maintaining performance
later. It is not immediately clear what causes this bifurcation, nor if it will necessarily transfer to
attention models.
36

--- PAGE 37 ---
0 1000 2000
Iteration123LosshAdam=h1=0.01
Adam
SplitSGD
0 1000 2000
IterationhAdam=h1=0.001
0 1000 2000
IterationhAdam=h1=0.0005
0 1000 2000
IterationhAdam=h1=0.0001Figure 41: Train loss of Adam and SplitSGD for varying learning rates. The regular SGD step size for SplitSGD is fixed at
0.1. SplitSGD seems at least as robust to choice of learning rate as Adam, and it appears to benefit from a slightly smaller
learning rate because it cannot adjust per-parameter.
0 1000 2000
Iteration0.250.500.75AccuracyhAdam=h1=0.01
Adam
SplitSGD
0 1000 2000
IterationhAdam=h1=0.001
0 1000 2000
IterationhAdam=h1=0.0005
0 1000 2000
IterationhAdam=h1=0.0001
Figure 42: Test accuracy of Adam and SplitSGD for varying learning rates. The regular SGD step size for SplitSGD is
fixed at 0.1. SplitSGD seems at least as robust to choice of learning rate as Adam, and it appears to benefit from a slightly
smaller learning rate because it cannot adjust per-parameter.
Finally, Figure 44(a) compares Adam to the full version of SplitSGD; we see essentially the same
performance, and furthermore SplitSGD maintains its robustness to the choice of standard SGD
learning rate.
F.2 SplitSGD on GPT-2
For the transformer, we use the public nanoGPT repository which trains GPT-2 on the OpenWebText
dataset. As a full training run would be too expensive, we compare only for the early stage of
1.01.52.02.53.03.5LossSGD with Momentum
SGD b=.9,t=0, lr=0.001
SGD b=.9,t=0, lr=0.01
SGD b=.9,t=0, lr=0.1
Adam, lr=3e-4
0 50 100 150 200 250
Iteration0.10.20.30.40.5Test Accuracy
(a) Adam versus standard SGD with Momentum. SGD
remains extremely sensitive to choice of learning rate.
1.01.52.02.53.03.5LossSplitSGD without Dampening
SplitSGD t=0, lr=0.001
SplitSGD t=0, lr=0.01
SplitSGD t=0, lr=0.1
Adam, lr=3e-4
0 50 100 150 200 250
Iteration0.10.20.30.40.5Test Accuracy(b) Adam vs. SplitSGD with τ= 0. Fixed-size learning
rate for both is .0003.
37

--- PAGE 38 ---
1.01.52.02.53.03.5LossSplitSGD with Dampening
SplitSGD t=.9, lr=0.001
SplitSGD t=.9, lr=0.01
SplitSGD t=.9, lr=0.1
Adam, lr=3e-4
0 50 100 150 200 250
Iteration0.10.20.30.40.5Test Accuracy(a) Adam vs. SplitSGD with τ= 0.9. Fixed-size learn-
ing rate for both is .0003.
(b) The fraction of parameters for which a fixed-size signed
step was taken for each gradient step.
optimization. All hyperparameters are the defaults from that repository, with the SGD learning rate
η1set equal to the other learning rate η2. We observe that not only do the two methods track each
other closely in training loss, it appears that they experience exactly the same oscillations. Though
we do not track the parameters themselves, this suggests that these two methods follow very similar
optimization trajectories as well, which we believe is an intriguing possibility worth further study.
Figure 45: Adam versus SplitSGD on the initial stage of training GPT-2 on the OpenWebText dataset, and the fraction of
parameters with a fixed-size signed step. All hyperparameters are the defaults from the nanoGPT repository. Observe that
not only is their performance similar, they appear to have exactly the same loss oscillations.
38

--- PAGE 39 ---
G Proofs of Theoretical Results
Before we begin the analysis, we must identify the quantities of interest during gradient flow and
the system of equations that determines how they evolve.
We start by writing out the loss:
2L(θ) =E[(c(b⊤x+b⊤
oxo)−(β⊤x+d−1/2
21⊤|xo|))2] (1)
=E[((cb−β)⊤x)2] +E[((cbo−d−1/2
2sign(xo)1)⊤xo)2] (2)
=∥cb−β∥2+p
2 rα
p(cbo−1)2
+rα
p(cbo+ 1)2!
(3)
=∥cb−β∥2+α(c2∥bo∥2+ 1). (4)
This provides the gradients
∇bL=c(cb−β), (5)
∇boL=αc2bo, (6)
∇cL=b⊤(cb−β) +α∥bo∥2c. (7)
We will also make use of the Hessian to identify its top eigenvalue; it is given by
∇2
θL(θ) =
c2Id10d1×d2 2cb
0d2×d1αc2Id2 2cαbo
2cb⊤2cαb⊤
o∥b∥2+α∥bo∥2
. (8)
The maximum eigenvalue λmaxat initialization is upper bounded by the maximum row sum of this
matrix, and thus λmax≤3d1+αd2
d1+d2<3α. Clearly, we also have λmax≥α.
We observe that tracking the precise vectors b, boare not necessary to uncover the dynamics
when optimizing this loss. First, let us write b:=ϵβ
∥β∥+δv, where vis the direction of the rejection
ofbfrom β(i.e.,β⊤v= 0) and δis its norm. Then we have the gradients
∇ϵL= (∇ϵb)⊤(∇bL) (9)
=β
∥β∥⊤
c2
ϵβ
∥β∥+δv
−cβ
(10)
=c2ϵ−c∥β∥, (11)
∇δL= (∇δb)⊤(∇bL) (12)
=v⊤
c2
ϵβ
∥β∥+δv
−cβ
(13)
=c2δ, (14)
∇cL=
ϵβ
∥β∥+δv⊤
c
ϵβ
∥β∥+δv
−β
+α∥bo∥2c (15)
=c(ϵ2+δ2+α∥bo∥2)−ϵ∥β∥. (16)
39

--- PAGE 40 ---
Finally, define the scalar quantity o:=∥bo∥2, noting that ∇oL= 2b⊤
o∇boL= 2αc2o. Minimizing this
loss via gradient flow is therefore characterized by the following ODE on four scalars:
dϵ
dt=−c2ϵ+c∥β∥, (17)
dδ
dt=−c2δ, (18)
do
dt=−2αc2o, (19)
dc
dt=−c(ϵ2+δ2+αo) +ϵ∥β∥. (20)
(21)
Furthermore, we have the boundary conditions
ϵ(0) =r
1
d1+d2, (22)
δ(0) =r
d1−1
d1+d2, (23)
o(0) =d2
d1+d2, (24)
c(0) = 1 . (25)
Given these initializations and dynamics, we make a few observations: (i) all four scalars are
initialized at a value greater than 0, and remain greater than 0 at all time steps; (ii) δandowill
decrease towards 0 monotonically, and ϵwill increase monotonically until cϵ=∥β∥; (iii) cwill be
decreasing at initialization. Lastly, for conciseness later on we define the quantities
r:= (ϵ(0)2+δ(0)2+αo(0)) =d1+αd2
d1+d2, (26)
k:=d2
d1, (27)
m:=d1
d1+d2=1
1 +k. (28)
Before we can prove the main results, we present a lemma which serves as a key tool for deriving
continuously valid bounds on the scalars we analyze:
Lemma G.1. Consider a vector valued ODE with scalar indices v1, v2, . . ., where each index is described
over the time interval [tmin, tmax]by the continuous dynamicsdvi(t)
dt=ai(v−i(t))·vi(t) +bi(v−i(t))with
ai≤0, bi≥0for all i, t(v−idenotes the vector vwithout index i). That is, each scalar’s gradient is an affine
function of that scalar with a negative coefficient. Suppose we define continuous functions ˆai,ˆbi:R→R
such that ∀i, t,ˆai(t)≤ai(v−i(t))andˆbi(t)≤bi(v−i(t)). Let ˆvbe the vector described by these alternate
dynamics, with the boundary condition ˆvi(tmin) =vi(tmin)andvi(tmin)≥0for all i(if a solution exists).
Then for t∈[tmin, tmax]it holds that
ˆv(t)≤v(t), (29)
elementwise. If ˆai,ˆbiupper bound ai, bi, the inequality is reversed.
40

--- PAGE 41 ---
Proof. Define the vector w(t) := ˆv(t)−v(t). This vector has the dynamics
dwi
dt=dˆvi
dt−dvi
dt(30)
= ˆai(t)·ˆvi(t) +ˆbi(t)−ai(v−i(t))·vi(t)−bi(v−i(t)) (31)
≤ˆai(t)·ˆvi(t)−ai(v−i(t))·vi(t). (32)
The result will follow by showing that w(t)≤0for all t∈[tmin, tmax](this clearly holds at tmin).
Assume for the sake of contradiction there exists a time t′∈(tmin, tmax]and index isuch that
wi(t′)>0(letibe the first such index for which this occurs, breaking ties arbitrarily). By continuity,
we can define t0:= max {t∈[tmin, t′] :wi(t)≤0}. By definition of t0it holds that wi(t0) = 0 and
∀ϵ >0,wi(t0+ϵ)−wi(t0) =wi(t0+ϵ)>0, and thusdwi(t0)
dt>0. But by the definition of wwe also
have
ˆvi(t0) =vi(t0) +wi(t0) (33)
=vi(t0), (34)
and therefore
dwi(t0)
dt≤ˆai(t0)·ˆvi(t0)−ai(v−i(t0))·vi(t0) (35)
= 
ˆai(t0)−ai(v−i(t0))
·vi(t0) (36)
≤0, (37)
with the last inequality following because ˆai(t)≤ai(v−i(t))andvi(t)>0for all i, t∈[tmin, tmax].
Having proven bothdwi(t0)
dt>0anddwi(t0)
dt≤0, we conclude that no such t′can exist. The other
direction follows by analogous argument.
We make use of this lemma repeatedly and its application is clear so we invoke it without direct
reference. We are now ready to prove the main results:
G.1 Proof of Theorem 3.1
Proof. At initialization, we have ∥β∥ ≥d1√d1+d2=⇒ ∥β∥ϵ(0)≥d1
d1+d2=c(0)(ϵ(0)2+δ(0)2). Therefore,
we can remove these terms fromdc
dtat time t= 0, noting simple thatdc
dt≥ −αoc. Further, so long as c
is still decreasing (and therefore less than c(0) = 1 ),
d(∥β∥ϵ−c(ϵ2+δ2))
dt≥d(∥β∥ϵ−(ϵ2+δ2))
dt(38)
= (∥β∥ −2ϵ)dϵ
dt−2δdδ
dt(39)
= (∥β∥ −2ϵ)(−c2ϵ+∥β∥c)−2δ(−c2δ) (40)
=−c2(ϵ∥β∥ −2(ϵ2+δ2) +c(∥β∥2−2ϵ) (41)
≥ −c(ϵ∥β∥ −2(ϵ2+δ2)) +c(∥β∥2−2ϵ) (42)
=c(∥β∥2−2ϵ−ϵ∥β∥+ 2(ϵ2+δ2)) (43)
≥c(∥β∥2−ϵ(2 +∥β∥)). (44)
41

--- PAGE 42 ---
Since c >0at all times, this is non-negative so long as the term in parentheses is non-negative, which
holds so long as ϵ≤∥β∥2
∥β∥+2. Further, since ϵc≤ ∥β∥we have
dϵ2
dt= 2ϵdϵ
dt(45)
=−2c2ϵ2+ 2ϵc∥β∥ (46)
≤2∥β∥2. (47)
This implies ϵ(t)2≤ϵ(0)2+ 2t∥β∥2. Therefore, for t≤ln∥β∥/2
2∥β∥we have ϵ(t)2≤1
d1+d2+∥β∥ln∥β∥/2≤
∥β∥4
(∥β∥+2)2(this inequality holds for ∥β∥ ≥2). This satisfies the desired upper bound.
Thus the term in Equation (44) is non-negative for all t≤ln∥β∥/2
2∥β∥, and so we havedc
dt≥ −αoc
under the above conditions. Since the derivative of ois negative in c, a lower bound ondc
dtgives us
an upper bound ondo
dt, which in turn maintains a valid lower bound ondc
dtThis allows us to solve
for just the ODE given by
dc2
dt=−2αc2o, (48)
do
dt=−2αc2o. (49)
Recalling the initial values of c2, o, The solution to this system is given by
c(t)2=m
1−(1−m)
exp(2 αmt), (50)
o(t) =m
exp(2 αmt)
1−m−1(51)
=m
exp(2 αmt)(1 + k−1)−1(52)
Since these are bounds on the original problem, we have c(t)2≥mando(t)shrinks exponentially
fast in t. In particular, note that under the stated condition√α≥∥β∥lnk
m(ln∥β∥/2)(recalling k:=d2
d1>1), we
havelnk
2√αm≤ln∥β∥/2
2∥β∥. Therefore we can plug in this value for t, implying o(t)≤m
d1
d2√α
=mk−√α
at some time before t=ln∥β∥/2
2∥β∥.
Now we solve for the time at whichdc
dt≥0. Returning to Equation (44), we can instead suppose
thatϵ≤∥β∥2−γ
∥β∥+2=⇒ ∥β∥2−ϵ(2 +∥β∥)≥γfor some γ >0. If this quantity was non-negative and
has had a derivative of at least γuntil time t=lnk
2√αm, then its value at that time must be at least
γlnk
2√αm. Fordc
dtto be non-negative, we need this to be greater than c(t)2αo(t), so it suffices to have
γlnk
2√αm≥αm
exp(2 αmt)(1+k−1)−1⇐=γlnk≥2α3/2m2
k√α(1+k−1)−1⇐=γ≥2α3/2m2k−√α
lnk. Observe that the stated
lower bound on αdirectly implies this inequality.
Finally, note that ∥b∥2=ϵ2+δ2, and therefore
d∥b∥2
dt= 2ϵdϵ
dt+ 2δdδ
dt(53)
=−2c2(ϵ2+δ2) + 2cϵ∥β∥. (54)
42

--- PAGE 43 ---
Since c(0) = 1 andcϵ <∥β∥, this means ∥b∥2will also be decreasing at initialization. Thus we have
shown that all relevant quantities will decrease towards 0 at initialization, but that by time t=lnk
2√αm,
we will havedc
dt≥0.
G.2 Proof of Proof of Theorem 3.2
Proof. Recall from the previous section that we have shown that at some time t1≤lnk
2√αm,c(t)2
will be greater than mand increasing, and o(t)will be upper bounded by mk−√α. Furthermore,
ϵ(t)2≤1
d1+d2+ 2t∥β∥2. To show that the sharpness reaches a particular value, we must demonstrate
thatcgrows large enough before the point cϵ≈ ∥β∥where this growth will rapidly slow. To do this,
we study the relative growth of cvs.ϵ.
Recall the derivatives of these two terms:
dc
dt=−(ϵ2+δ2+αo2)c+∥β∥ϵ, (55)
dϵ
dt=−c2ϵ+∥β∥c. (56)
Considering instead their squares,
dc2
dt= 2cdc
dt(57)
=−2(ϵ2+δ2+αo2)c2+ 2∥β∥ϵc, (58)
dϵ2
dt= 2ϵdϵ
dt(59)
=−2ϵ2c2+ 2∥β∥ϵc. (60)
Since δ, odecrease monotonically, we havedc2
dt≥ −2(ϵ2+d1
d1+d2+αm
d1
d2√α
)c2+ 2∥β∥ϵ. Thus if we
can show that
∥β∥ϵc≥(ϵ2+ 2(d1
d1+d2+αmd1
d2√α
))c2, (61)
we can conclude thatdc2
dt≥(ϵ2c2+∥β∥ϵc) =1
2dϵ2
dt—that is, that c(t)2grows at least half as fast as
ϵ(t)2. And since δ, ocontinue to decrease, this inequality will continue to hold thereafter.
Simplifying the above desired inequality, we get
∥β∥ϵ
c≥ϵ2+ 2m(1 +αk−√α). (62)
Noting thatϵ
c≥1andm=d1
d1+d2≤1
2, and recalling the upper bound on ϵ(t)2, this reduces to
proving
∥β∥ ≥1
d1+d2+ 2t∥β∥2+ 1 + αk−√α. (63)
Since this occurs at some time t1≤lnk
2√αm, and since m−1= 1 + k, we get
∥β∥ ≥1
d1+d2+∥β∥2(1 +k) lnk√α+ 1 + αk−√α. (64)
43

--- PAGE 44 ---
The assumed lower bound on√αmeans the sum of the first three terms can be upper bounded by a
small 1 +o(1)term (say, 9/5) and recalling ∥β∥ ≥ 24/5it suffices to prove
∥β∥ ≥9
5+αk−√α(65)
⇐=αk−√α≤3. (66)
Taking logs,
2 ln√α
lnk−√α≤ln 3, (67)
which is clearly satisfied for√α≥1 +klnk. As argued above, this impliesdc2
dt≥1
2dϵ2
dtby some time
t2≤lnk
2√αm.
Consider the time t2at which this first occurs, whereby c(t2)2is growing by at least one-half the
rate of ϵ(t2)2. Here we note that we can derive an upper bound on candϵat this time using our
lemma and the fact that
dc
dt≤ ∥β∥ϵ, (68)
dϵ
dt≤ ∥β∥c. (69)
The solution to this system implies
c(t2)≤1
2exp(∥β∥t2)−exp(−∥β∥t2)√d1+d2+ exp( ∥β∥t2) + 1
(70)
≤1
2
exp(∥β∥t2)
1 +1√d1+d2
+ 1
(71)
≤1
2
exp∥β∥lnk
2√αm
1 +1√d1+d2
+ 1
, (72)
ϵ(t2)≤1
2
exp(∥β∥t2)
1 +1√d1+d2
+1√d1+d2−1
(73)
≤1
2
exp∥β∥lnk
2√αm
1 +1√d1+d2
+1√d1+d2−1
(74)
Then for α >
∥β∥lnk
m(ln∥β∥−ln 2)2
, the exponential term is upper bounded by√
∥β∥
2, giving
c(t2)≤1
2 p
∥β∥
2
1 +1√d1+d2
+ 1!
(75)
≤p
∥β∥
2, (76)
ϵ(t2)≤1
2 p
∥β∥
2
1 +1√d1+d2
+1√d1+d2−1!
(77)
≤p
∥β∥
2. (78)
We know that optimization will continue until ϵ2c2=∥β∥2, and also thatdc2
dt≥1
2dϵ2
dt. Since c≤ϵ, this
implies that ϵ2≥ ∥β∥before convergence. Suppose that starting from time t2,ϵ2grows until time t′
44

--- PAGE 45 ---
by an additional amount s. Then we have
s=ϵ(t′)2−ϵ(t2)2(79)
=Zt′
t2dϵ(t)2
dt(80)
≤Zt′
t22dc(t)2
dt(81)
= 2(c(t′)2−c(t2)2). (82)
In other words, c2must have grown by at least half that amount. Since ϵ(t2)2≤∥β∥
4and therefore
ϵ(t′)2≤∥β∥
4+s, even if c(t′)2is the minimum possible value ofs
2we must have at convergence
s
2=c2=∥β∥2
ϵ2≥∥β∥2
∥β∥
4+s. This is a quadratic in sand solving tells us that we must have s≥5
4∥β∥.
Therefore, c(t′)2≥5
8∥β∥is guaranteed to occur. Noting our derivation of the loss Hessian, this
implies the sharpness must reach at least5
8α∥β∥for each dimension of bo.
H Additional Samples Under Various Architectures/Seeds
To demonstrate the robustness of our finding we train a ResNet-18, VGG-11, and a Vision Trans-
former for 1000 steps with full-batch GD, each with multiple random initializations. For each run,
we identify the 24 training examples with the most positive and most negative change in loss from
stepito step i+ 1, fori∈ {100,250,500,750}. We then display these images along with their label
(above) and the network’s predicted label before and after the gradient step (below). The change
in the network’s predicted labels display a clear pattern, where certain training samples cause the
network to associate an opposing signal with a new class, which the network then overwhelmingly
predicts whenever that feature is present.
Consistent with our other experiments, we find that early opposing signals tend to be “simpler”,
e.g. raw colors, whereas later signals are more nuanced, such as the presence of a particular texture.
We also see that the Vision Transformer seems to learn complex features earlier, and that they are
less obviously aligned with human perception—this is not surprising since they process inputs in a
fundamentally different manner than traditional ConvNets.
45

--- PAGE 46 ---
ship→shipbird
ship→autofrog
truck→dogtruck
truck→dogautomobile
plane→autodeer
auto→dogautomobile
truck→dogtruck
ship→autobird
auto→autobird
plane→autodeer
plane→shipbird
truck→dogautomobile
truck→dogautomobile
auto→dogautomobile
horse→dogautomobile
ship→shipbird
horse→dogtruck
plane→autobird
truck→dogautomobile
truck→dogautomobile
auto→autodeer
auto→dogautomobile
horse→dogtruck
auto→frogautomobile
truck→horsefrog
auto→autodog
auto→autodog
auto→dogdog
auto→autocat
auto→autodog
auto→autodeer
auto→truckdeer
auto→dogdog
auto→truckdog
auto→truckbird
auto→autofrog
auto→autodog
auto→truckcat
truck→horsebird
auto→truckdog
truck→truckdog
auto→doghorse
auto→dogcat
auto→truckhorse
auto→dogcat
auto→truckdeer
plane→planeautomobile
auto→catdog(a) Step 100 to 101
horse→truckcat
plane→autocat
truck→truckfrog
auto→autofrog
plane→autobird
plane→autocat
plane→truckdeer
plane→autodeer
plane→autohorse
truck→dogtruck
horse→truckcat
plane→shiphorse
truck→truckfrog
truck→cattruck
horse→dogtruck
dog→shipcat
plane→truckdeer
plane→shipfrog
truck→dogautomobile
plane→autobird
auto→frogtruck
truck→truckfrog
horse→truckdeer
plane→truckdeer
truck→truckdog
truck→truckfrog
truck→autodog
truck→dogdog
truck→horsefrog
truck→catdog
auto→catdog
truck→catdog
auto→shipbird
auto→truckhorse
truck→truckcat
truck→catbird
auto→autodog
plane→autoautomobile
dog→catship
auto→catcat
plane→planeautomobile
plane→shipautomobile
bird→planetruck
ship→shipcat
auto→autodeer
dog→truckautomobile
truck→frogfrog
truck→truckcat (b) Step 250 to 251
truck→deertruck
auto→autoship
cat→frogdog
plane→truckdeer
plane→autofrog
cat→froghorse
truck→horsetruck
truck→frogtruck
truck→truckhorse
plane→autobird
cat→deertruck
truck→truckship
horse→frogdog
auto→autoairplane
truck→horsetruck
cat→frogcat
truck→horsetruck
truck→truckship
frog→frogcat
plane→planebird
frog→frogcat
frog→frogship
auto→autoairplane
frog→frogcat
plane→planeautomobile
plane→planeautomobile
plane→planeautomobile
plane→planeautomobile
horse→deerfrog
truck→horsehorse
truck→deercat
plane→autoautomobile
plane→autoautomobile
plane→planeautomobile
truck→deerhorse
plane→planetruck
auto→truckhorse
plane→autoautomobile
bird→planetruck
truck→deerdeer
plane→planetruck
plane→planetruck
plane→shipautomobile
ship→autoautomobile
plane→planeautomobile
ship→shiptruck
plane→autoautomobile
plane→autoautomobile
(c) Step 500 to 501
cat→frogdog
cat→froghorse
deer→froghorse
cat→froghorse
frog→froghorse
cat→frogdog
cat→frogcat
cat→frogdog
cat→frogcat
truck→autohorse
horse→froghorse
ship→shiphorse
cat→frogdog
cat→dogairplane
horse→froghorse
ship→autoship
dog→froghorse
cat→frogdog
horse→froghorse
cat→frogdog
cat→frogdog
cat→frogdog
horse→froghorse
dog→dogfrog
horse→frogfrog
ship→planeairplane
ship→shipairplane
frog→frogdog
ship→planeairplane
ship→planeairplane
truck→truckhorse
ship→shiptruck
dog→catfrog
dog→frogfrog
dog→frogfrog
dog→catfrog
horse→deerfrog
horse→frogfrog
cat→frogfrog
ship→planeairplane
horse→horsedog
horse→frogfrog
dog→frogfrog
frog→catdog
dog→frogfrog
plane→planehorse
horse→horsecat
ship→catdog (d) Step 750 to 751
Figure 46: (ResNet-18, seed 1) Images with the most positive (top 3 rows) and most negative (bottom 3 rows) change to
training loss after steps 100, 250, 500, and 750. Each image has the true label (above) and the predicted label before and
after the gradient update (below).
46

--- PAGE 47 ---
auto→frogautomobile
horse→dogtruck
truck→frogautomobile
horse→frogautomobile
truck→catautomobile
horse→horseautomobile
truck→horseautomobile
frog→frogtruck
auto→frogautomobile
auto→frogautomobile
horse→frogtruck
auto→dogautomobile
auto→frogtruck
horse→dogautomobile
auto→frogautomobile
auto→frogautomobile
frog→frogautomobile
truck→frogtruck
horse→dogautomobile
auto→frogautomobile
horse→horseautomobile
auto→frogautomobile
horse→dogautomobile
truck→frogtruck
auto→autocat
auto→autofrog
auto→autocat
auto→autofrog
auto→autocat
auto→truckcat
auto→truckdog
auto→autodog
auto→truckdog
auto→autocat
auto→truckdeer
auto→truckcat
auto→truckcat
auto→truckcat
truck→truckfrog
truck→horsedog
auto→horsedog
auto→truckdog
auto→truckdog
auto→horsedog
truck→horsecat
truck→truckfrog
auto→horsedog
auto→truckfrog(a) Step 100 to 101
bird→frogairplane
ship→deership
truck→truckdeer
frog→frogship
deer→frogairplane
deer→frogairplane
auto→autoship
bird→birdship
auto→autobird
ship→truckairplane
plane→horseship
truck→truckcat
ship→frogairplane
truck→truckcat
plane→deership
truck→truckfrog
ship→truckbird
ship→truckairplane
frog→frogtruck
bird→planedog
bird→planefrog
truck→truckship
bird→frogairplane
plane→shipdog
horse→horsecat
horse→horsecat
auto→autofrog
truck→truckfrog
ship→trucktruck
ship→shipdog
ship→birddeer
ship→shiptruck
ship→frogfrog
ship→autoautomobile
ship→planeautomobile
truck→truckship
frog→frogship
dog→planeairplane
ship→shipdeer
plane→deerfrog
frog→birdairplane
ship→trucktruck
ship→birdfrog
ship→shiptruck
ship→birdcat
ship→frogfrog
truck→truckship
ship→trucktruck (b) Step 250 to 251
horse→truckhorse
cat→frogdog
ship→shipautomobile
horse→truckhorse
deer→froghorse
horse→truckhorse
auto→shipautomobile
dog→frogdog
horse→truckhorse
plane→planeautomobile
cat→dogautomobile
cat→frogdog
plane→planeautomobile
auto→shipautomobile
plane→planeautomobile
horse→truckhorse
cat→dogship
auto→shipautomobile
horse→froghorse
cat→dogship
truck→horsetruck
plane→birdship
bird→dogship
auto→shipautomobile
truck→truckcat
horse→frogfrog
truck→truckcat
truck→truckbird
auto→shipship
horse→frogfrog
dog→frogfrog
dog→frogfrog
horse→frogfrog
dog→frogfrog
truck→truckcat
horse→frogfrog
truck→dogbird
horse→horsecat
horse→frogfrog
horse→frogfrog
truck→horsedog
dog→frogfrog
horse→trucktruck
auto→planeairplane
auto→autoship
horse→horsecat
dog→frogfrog
horse→frogfrog
(c) Step 500 to 501
bird→dogairplane
plane→dogairplane
ship→planeship
ship→catship
bird→catairplane
deer→deerfrog
frog→frogdog
cat→catairplane
cat→dogship
plane→planeship
deer→deercat
dog→dogbird
bird→dogbird
auto→planeship
bird→dogairplane
plane→dogairplane
cat→catairplane
cat→catairplane
cat→frogdog
horse→dogairplane
bird→dogairplane
bird→frogairplane
frog→frogbird
bird→frogairplane
truck→horsebird
ship→shipdog
cat→shipairplane
ship→shipdog
truck→truckcat
plane→dogdog
plane→planedog
truck→truckcat
auto→autoairplane
ship→catcat
auto→autodog
dog→catfrog
bird→birdairplane
truck→dogdog
truck→truckhorse
ship→dogdog
plane→dogdog
dog→catdeer
plane→birdcat
ship→catfrog
frog→frogship
dog→birdbird
frog→frogairplane
bird→dogdog (d) Step 750 to 751
Figure 47: (ResNet-18, seed 2) Images with the most positive (top 3 rows) and most negative (bottom 3 rows) change to
training loss after steps 100, 250, 500, and 750. Each image has the true label (above) and the predicted label before and
after the gradient update (below).
47

--- PAGE 48 ---
horse→horseairplane
truck→autodog
bird→shipdeer
truck→autodog
truck→autocat
truck→autocat
truck→autofrog
frog→truckfrog
deer→frogairplane
horse→autocat
horse→autodog
truck→autodog
horse→truckfrog
horse→truckdog
truck→autocat
truck→autocat
bird→deerairplane
auto→autodog
auto→autofrog
truck→shipdog
truck→autobird
truck→autofrog
auto→autocat
frog→truckfrog
horse→horsetruck
cat→truckship
dog→autotruck
frog→frogtruck
dog→horseautomobile
horse→horsetruck
frog→trucktruck
cat→truckautomobile
frog→trucktruck
dog→trucktruck
dog→horsetruck
horse→trucktruck
frog→frogautomobile
plane→planecat
frog→trucktruck
frog→frogtruck
dog→trucktruck
auto→autoairplane
cat→catship
horse→horsetruck
horse→trucktruck
frog→cattruck
frog→trucktruck
frog→deership(a) Step 100 to 101
dog→planedog
dog→planecat
ship→frogship
bird→frogship
plane→planecat
bird→frogairplane
bird→frogship
plane→shipcat
bird→shipcat
plane→autofrog
plane→shipcat
truck→truckhorse
ship→frogship
horse→horsetruck
ship→birdship
plane→shipdog
truck→deership
horse→horseautomobile
deer→deerairplane
plane→planehorse
horse→truckdog
auto→frogairplane
bird→frogairplane
truck→cattruck
dog→birdairplane
dog→dogairplane
truck→catdog
auto→autofrog
ship→shipdeer
dog→dogairplane
ship→shipbird
truck→truckcat
auto→autobird
auto→autofrog
auto→catcat
ship→shipbird
dog→dogship
ship→birdcat
dog→dogship
ship→shipbird
ship→catdog
dog→planeship
auto→autobird
ship→birdfrog
ship→truckfrog
ship→shipcat
cat→shipairplane
ship→birddeer (b) Step 250 to 251
frog→frogtruck
frog→frogairplane
frog→shipfrog
deer→froghorse
auto→autoship
frog→frogbird
auto→autoship
truck→truckhorse
deer→frogdeer
deer→froghorse
ship→shipdog
auto→autobird
bird→birdairplane
bird→frogairplane
bird→frogairplane
frog→frogdeer
bird→frogairplane
bird→planecat
frog→frogdeer
frog→frogdeer
frog→dogfrog
frog→frogbird
frog→dogfrog
auto→autoship
truck→truckcat
ship→shipautomobile
frog→birdairplane
frog→frogdog
frog→frogship
deer→deerfrog
deer→deerfrog
deer→deerfrog
deer→frogfrog
ship→shipautomobile
ship→shipautomobile
deer→deerfrog
bird→birdairplane
frog→birdairplane
horse→deerbird
frog→birdairplane
deer→deerfrog
ship→shipautomobile
ship→shiptruck
ship→trucktruck
ship→shipautomobile
deer→frogfrog
truck→truckfrog
deer→deerfrog
(c) Step 500 to 501
auto→autoship
ship→autoship
ship→autoship
auto→autoship
cat→shipcat
auto→autoship
ship→autoship
ship→autoship
truck→cattruck
frog→deerfrog
ship→autoship
truck→cattruck
ship→autoship
ship→autoship
ship→shipdog
truck→cattruck
deer→froghorse
plane→truckairplane
truck→truckairplane
truck→deertruck
auto→cattruck
ship→autoship
auto→autoairplane
deer→froghorse
truck→truckcat
ship→autoautomobile
cat→birdairplane
truck→truckfrog
ship→autoautomobile
ship→planeautomobile
ship→autoautomobile
frog→deerdeer
auto→trucktruck
ship→autoautomobile
ship→autoautomobile
dog→shipairplane
cat→shipship
ship→autoautomobile
auto→autotruck
cat→shipship
truck→catdog
cat→shipship
ship→shipautomobile
ship→autoautomobile
cat→shipship
ship→shipautomobile
plane→autoautomobile
truck→catcat (d) Step 750 to 751
Figure 48: (ResNet-18, seed 3) Images with the most positive (top 3 rows) and most negative (bottom 3 rows) change to
training loss after steps 100, 250, 500, and 750. Each image has the true label (above) and the predicted label before and
after the gradient update (below).
48

--- PAGE 49 ---
frog→frogship
frog→frogairplane
frog→frogship
frog→frogship
frog→frogairplane
frog→frogship
frog→frogship
frog→frogship
frog→frogship
frog→frogship
frog→frogairplane
frog→frogship
frog→frogairplane
frog→frogairplane
frog→frogairplane
frog→frogship
frog→frogairplane
frog→frogairplane
frog→frogship
frog→frogairplane
frog→frogairplane
frog→frogairplane
frog→frogairplane
frog→frogairplane
auto→autotruck
auto→autotruck
auto→autotruck
auto→autotruck
auto→autotruck
auto→autotruck
auto→autotruck
auto→autotruck
horse→autotruck
frog→autotruck
auto→autotruck
auto→autotruck
auto→autotruck
frog→autotruck
auto→autotruck
auto→autotruck
auto→autotruck
frog→frogtruck
frog→autotruck
auto→autotruck
auto→autotruck
horse→autotruck
auto→autotruck
frog→autotruck(a) Step 100 to 101
frog→frogairplane
frog→frogtruck
frog→frogtruck
frog→frogairplane
frog→frogairplane
frog→frogtruck
frog→frogtruck
horse→frogtruck
frog→frogtruck
frog→frogtruck
frog→frogtruck
frog→frogtruck
frog→frogtruck
frog→frogairplane
frog→frogairplane
frog→frogtruck
frog→frogautomobile
frog→frogtruck
frog→frogtruck
horse→frogtruck
horse→frogship
frog→frogairplane
frog→frogtruck
frog→frogtruck
truck→horsefrog
truck→horsefrog
horse→horsefrog
truck→horsefrog
horse→horsefrog
truck→horsefrog
truck→truckfrog
horse→frogfrog
plane→planedog
horse→horsefrog
plane→planefrog
truck→horsefrog
horse→horsefrog
plane→planedog
horse→frogfrog
horse→horsefrog
plane→planefrog
horse→frogfrog
horse→horsefrog
horse→horsefrog
horse→frogfrog
truck→horsefrog
horse→horsefrog
horse→frogfrog (b) Step 250 to 251
dog→horsedog
horse→horsecat
dog→horsedog
auto→autodog
auto→autocat
ship→autoship
dog→horsecat
ship→autoship
cat→frogcat
ship→autoship
truck→autodog
dog→horsecat
ship→autocat
horse→horsecat
auto→autocat
truck→truckcat
auto→autoship
cat→truckcat
plane→truckairplane
truck→truckcat
auto→autocat
auto→autodog
auto→autoship
deer→horsecat
ship→autoautomobile
ship→shipautomobile
ship→autoautomobile
plane→autoautomobile
plane→shipautomobile
ship→autoautomobile
plane→autoautomobile
plane→shipautomobile
plane→planeautomobile
ship→autoautomobile
plane→planeautomobile
plane→shipautomobile
bird→trucktruck
ship→shipautomobile
plane→planeautomobile
cat→trucktruck
plane→autoautomobile
plane→autotruck
plane→planeautomobile
plane→planeautomobile
plane→autoautomobile
plane→planeautomobile
plane→shipautomobile
ship→autoautomobile
(c) Step 500 to 501
ship→shipautomobile
plane→shipautomobile
ship→shipautomobile
truck→truckautomobile
auto→shipautomobile
auto→truckautomobile
auto→shipautomobile
auto→shipautomobile
auto→shipautomobile
ship→shipautomobile
auto→shipautomobile
auto→truckautomobile
auto→shipautomobile
auto→shipautomobile
auto→shipautomobile
ship→shipautomobile
auto→shipautomobile
ship→shipautomobile
auto→shipautomobile
truck→truckautomobile
auto→shipautomobile
plane→shipautomobile
truck→truckautomobile
auto→shipautomobile
auto→autoship
auto→autoship
auto→autoship
auto→autotruck
auto→autotruck
auto→shipship
frog→frogtruck
auto→autoship
auto→shipship
auto→autoship
auto→shipship
auto→truckship
auto→trucktruck
auto→autoship
auto→autoship
auto→shipship
frog→frogtruck
auto→shipship
auto→autoship
auto→shipship
frog→frogtruck
auto→truckship
frog→truckship
auto→autoship (d) Step 750 to 751
Figure 49: (VGG-11, seed 1) Images with the most positive (top 3 rows) and most negative (bottom 3 rows) change to
training loss after steps 100, 250, 500, and 750. Each image has the true label (above) and the predicted label before and
after the gradient update (below).
49

--- PAGE 50 ---
plane→planefrog
plane→planefrog
plane→planefrog
plane→planefrog
plane→planefrog
plane→planefrog
truck→truckdeer
plane→planedeer
plane→planefrog
plane→planefrog
plane→planedeer
plane→planefrog
plane→planedeer
plane→planefrog
plane→planefrog
truck→truckdeer
plane→planedeer
plane→planefrog
plane→planefrog
plane→planedeer
plane→planedeer
truck→truckdeer
plane→planefrog
plane→planefrog
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane(a) Step 100 to 101
auto→autofrog
plane→planedeer
plane→planedeer
plane→planedeer
plane→shipdeer
plane→shipfrog
plane→planefrog
plane→planedeer
plane→planefrog
plane→shipfrog
plane→shipdeer
plane→shipfrog
plane→planefrog
plane→planedeer
plane→planedeer
ship→shipdeer
truck→truckbird
truck→truckfrog
auto→autofrog
plane→planedeer
plane→planedeer
frog→frogairplane
plane→planedeer
auto→shipdeer
plane→planeship
plane→planeship
plane→planeship
plane→planeship
ship→shipship
plane→planeship
truck→truckship
plane→planeship
plane→shipship
plane→planeship
plane→planeship
plane→planeship
plane→planeship
auto→shipship
auto→shipship
plane→planeship
plane→shipship
auto→autoship
plane→planeship
plane→shipship
plane→planeship
plane→planeship
plane→planeship
plane→planeship (b) Step 250 to 251
ship→autoship
auto→autoship
ship→autobird
plane→truckbird
auto→autoship
plane→truckbird
auto→autobird
ship→autobird
auto→autoship
auto→autoairplane
plane→autoairplane
auto→autoairplane
truck→autoairplane
plane→autoairplane
ship→truckbird
horse→horsebird
truck→truckship
plane→autodeer
auto→autoairplane
ship→truckairplane
ship→autoship
ship→autoship
auto→autoairplane
plane→autoship
ship→shipautomobile
plane→autoautomobile
plane→autoautomobile
ship→autoautomobile
plane→planeautomobile
ship→autoautomobile
ship→shipautomobile
plane→shipautomobile
ship→shipautomobile
plane→trucktruck
ship→autoautomobile
plane→shiptruck
deer→horsehorse
plane→planeautomobile
plane→autotruck
plane→planeautomobile
plane→autoautomobile
plane→planeautomobile
plane→shipautomobile
frog→dogdog
plane→autoautomobile
plane→autoautomobile
plane→planeautomobile
deer→horsehorse
(c) Step 500 to 501
truck→truckfrog
frog→truckfrog
frog→truckfrog
frog→truckfrog
frog→truckfrog
frog→truckfrog
frog→truckfrog
frog→horsefrog
deer→horsefrog
deer→horsefrog
truck→truckfrog
frog→truckfrog
frog→truckfrog
auto→autofrog
frog→truckfrog
frog→truckfrog
frog→truckfrog
frog→autofrog
frog→truckfrog
frog→dogfrog
frog→truckfrog
frog→truckfrog
frog→truckfrog
deer→horsefrog
frog→trucktruck
frog→frogtruck
frog→trucktruck
frog→trucktruck
frog→truckship
frog→trucktruck
frog→trucktruck
frog→doghorse
frog→cathorse
frog→trucktruck
frog→horsehorse
frog→trucktruck
frog→trucktruck
frog→trucktruck
frog→horsetruck
frog→trucktruck
frog→cattruck
frog→trucktruck
frog→trucktruck
frog→trucktruck
frog→trucktruck
frog→horsehorse
frog→horsehorse
frog→horsetruck (d) Step 750 to 751
Figure 50: (VGG-11, seed 2) Images with the most positive (top 3 rows) and most negative (bottom 3 rows) change to
training loss after steps 100, 250, 500, and 750. Each image has the true label (above) and the predicted label before and
after the gradient update (below).
50

--- PAGE 51 ---
plane→planefrog
plane→planefrog
plane→planedeer
plane→planedeer
plane→planedeer
plane→planedeer
plane→planedeer
plane→planedeer
auto→autodeer
plane→planedeer
plane→planedeer
plane→planedeer
plane→planefrog
plane→planefrog
plane→planedeer
plane→planedeer
plane→planedeer
plane→planefrog
plane→planedeer
auto→autodeer
auto→autodeer
auto→autodeer
plane→planedeer
plane→planedeer
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane
plane→planeairplane(a) Step 100 to 101
frog→frogship
dog→dogship
dog→dogship
frog→frogship
frog→frogship
frog→frogship
dog→dogship
frog→frogship
dog→dogship
frog→frogship
frog→dogship
frog→frogship
frog→frogship
frog→frogship
dog→dogship
frog→frogship
frog→frogship
frog→frogship
frog→frogship
frog→frogship
dog→dogship
dog→dogship
dog→dogship
frog→frogairplane
frog→frogtruck
frog→frogtruck
frog→frogtruck
frog→frogtruck
frog→frogtruck
frog→frogtruck
frog→frogtruck
frog→frogtruck
frog→frogtruck
frog→frogtruck
frog→frogtruck
frog→frogtruck
frog→frogtruck
frog→frogtruck
frog→frogtruck
frog→frogtruck
frog→frogtruck
frog→frogtruck
frog→frogtruck
frog→frogtruck
frog→frogtruck
frog→frogtruck
frog→frogtruck
frog→frogtruck (b) Step 250 to 251
truck→truckairplane
truck→truckairplane
plane→truckairplane
dog→catairplane
dog→dogairplane
plane→truckairplane
plane→autoairplane
ship→truckairplane
plane→truckairplane
truck→truckairplane
dog→dogairplane
ship→autoship
ship→autoship
plane→truckairplane
truck→truckairplane
truck→autoairplane
ship→autoship
dog→dogairplane
ship→autoairplane
plane→truckairplane
truck→autoairplane
plane→truckairplane
ship→autoship
ship→autoairplane
plane→shipautomobile
plane→shipautomobile
plane→autoautomobile
plane→autoautomobile
plane→shipautomobile
ship→autoautomobile
plane→autoautomobile
plane→autoautomobile
plane→shipautomobile
plane→shipautomobile
plane→planeautomobile
ship→shipautomobile
plane→autoautomobile
plane→shipautomobile
plane→shiptruck
plane→autotruck
plane→shipautomobile
plane→planetruck
plane→planetruck
ship→shipautomobile
plane→shipautomobile
plane→autotruck
ship→shipautomobile
plane→planetruck
(c) Step 500 to 501
horse→truckhorse
horse→autohorse
horse→shiphorse
horse→autohorse
bird→shiphorse
horse→autohorse
horse→shiphorse
auto→autohorse
horse→autohorse
horse→truckhorse
horse→autohorse
truck→truckhorse
horse→autohorse
horse→autohorse
plane→shiphorse
horse→truckhorse
horse→shiphorse
horse→truckhorse
truck→truckhorse
horse→shiphorse
horse→autohorse
horse→froghorse
horse→planehorse
truck→autohorse
dog→catship
dog→catship
horse→planeautomobile
dog→catship
horse→catautomobile
dog→cattruck
dog→catship
horse→horseautomobile
dog→catship
dog→shipship
dog→catautomobile
horse→frogfrog
horse→planeship
horse→frogfrog
horse→autoship
dog→catship
horse→autoautomobile
horse→autoautomobile
horse→planeship
horse→autoautomobile
dog→cattruck
horse→shipship
dog→shipship
dog→catship (d) Step 750 to 751
Figure 51: (VGG-11, seed 3) Images with the most positive (top 3 rows) and most negative (bottom 3 rows) change to
training loss after steps 100, 250, 500, and 750. Each image has the true label (above) and the predicted label before and
after the gradient update (below).
51

--- PAGE 52 ---
bird→birdtruck
plane→planetruck
frog→frogtruck
bird→birdtruck
plane→planehorse
cat→cattruck
plane→planetruck
cat→cattruck
ship→shipairplane
auto→autotruck
cat→cattruck
frog→frogtruck
plane→planetruck
deer→deertruck
bird→birdtruck
cat→cattruck
cat→cattruck
auto→autotruck
bird→birdtruck
auto→autotruck
deer→deertruck
plane→planetruck
truck→truckairplane
truck→truckairplane
horse→horsetruck
bird→birdtruck
deer→deertruck
bird→birdairplane
bird→horseairplane
ship→shiptruck
ship→shiptruck
horse→horsetruck
auto→autotruck
horse→horseairplane
truck→truckairplane
ship→shiptruck
truck→trucktruck
truck→trucktruck
horse→horseairplane
horse→horsetruck
truck→trucktruck
bird→birdtruck
truck→trucktruck
truck→trucktruck
bird→birdtruck
cat→catairplane
plane→planetruck
horse→horseairplane(a) Step 100 to 101
bird→shipdog
bird→shipdog
bird→shipdog
bird→shipdog
bird→shipdog
bird→shipdog
ship→shipdog
plane→shipdog
bird→shipdog
bird→shipdog
bird→shipdog
bird→shipdog
ship→shipdog
bird→shipdog
bird→shipdog
plane→shipdog
ship→shipdog
ship→shipdog
bird→shipdog
bird→shipdog
horse→froghorse
bird→shipdog
horse→froghorse
bird→shipdog
dog→trucktruck
bird→autotruck
dog→autotruck
bird→shiptruck
bird→shiptruck
bird→deerautomobile
bird→horseautomobile
bird→birdautomobile
bird→autotruck
bird→shiptruck
bird→trucktruck
dog→autoautomobile
bird→shiptruck
bird→autotruck
bird→trucktruck
dog→trucktruck
bird→frogautomobile
bird→autotruck
dog→trucktruck
bird→shiptruck
bird→shiptruck
bird→deerautomobile
dog→catautomobile
deer→autotruck (b) Step 250 to 251
truck→autodog
truck→truckdog
truck→autodog
truck→autodog
cat→autodog
truck→autodog
dog→autodog
dog→autodog
dog→autodog
cat→autodog
truck→autocat
auto→autodog
dog→truckdog
cat→truckdog
dog→autodog
cat→autocat
truck→truckdog
dog→autodog
truck→truckdog
truck→truckcat
cat→autocat
dog→autodog
cat→truckcat
auto→autodog
dog→frogautomobile
cat→autoautomobile
dog→autoautomobile
dog→autoautomobile
dog→autoautomobile
cat→autoautomobile
dog→autoautomobile
dog→autoautomobile
cat→autoautomobile
dog→autoautomobile
cat→autoautomobile
dog→horseautomobile
cat→truckautomobile
dog→autoautomobile
dog→autoautomobile
dog→autoautomobile
dog→horseautomobile
dog→autoautomobile
cat→truckautomobile
cat→autoautomobile
cat→autoautomobile
cat→catautomobile
dog→autoautomobile
dog→dogautomobile
(c) Step 500 to 501
deer→deerautomobile
auto→frogautomobile
auto→frogautomobile
truck→shipautomobile
dog→dogautomobile
dog→catautomobile
auto→frogautomobile
auto→deerautomobile
auto→frogautomobile
bird→frogautomobile
auto→shipautomobile
truck→shipautomobile
cat→frogautomobile
cat→catautomobile
bird→frogautomobile
cat→catautomobile
auto→frogautomobile
truck→truckautomobile
auto→catautomobile
auto→catautomobile
bird→catautomobile
auto→frogautomobile
horse→horseautomobile
frog→frogbird
auto→frogfrog
auto→truckfrog
auto→frogfrog
bird→horsefrog
truck→shipfrog
auto→catfrog
bird→horsefrog
auto→frogfrog
auto→frogfrog
bird→birdfrog
auto→frogfrog
auto→frogfrog
auto→truckfrog
auto→frogfrog
dog→dogfrog
auto→truckfrog
auto→frogfrog
horse→horsefrog
auto→autofrog
auto→frogfrog
auto→frogfrog
bird→horsefrog
auto→autofrog
auto→frogfrog (d) Step 750 to 751
Figure 52: (ViT, seed 1) Images with the most positive (top 3 rows) and most negative (bottom 3 rows) change to training
loss after steps 100, 250, 500, and 750. Each image has the true label (above) and the predicted label before and after the
gradient update (below).
52

--- PAGE 53 ---
dog→doghorse
dog→doghorse
cat→cathorse
cat→cattruck
dog→doghorse
auto→autohorse
dog→doghorse
cat→cathorse
dog→doghorse
dog→doghorse
bird→birdship
dog→doghorse
dog→doghorse
truck→truckdeer
cat→cathorse
bird→birdship
cat→cathorse
cat→cathorse
cat→cathorse
dog→doghorse
dog→doghorse
cat→cathorse
dog→dogtruck
cat→cathorse
ship→shipdeer
horse→horseairplane
ship→shipdeer
frog→frogairplane
ship→shipdeer
frog→frogairplane
ship→shipdeer
bird→birdhorse
frog→frogairplane
plane→planedeer
ship→shipdeer
frog→frogairplane
frog→frogairplane
frog→frogairplane
ship→shipdeer
deer→deerairplane
frog→frogairplane
frog→frogairplane
bird→birdhorse
frog→frogairplane
frog→frogairplane
frog→frogairplane
frog→frogairplane
frog→frogairplane(a) Step 100 to 101
plane→truckairplane
plane→truckairplane
plane→truckairplane
plane→truckairplane
ship→truckairplane
ship→shipairplane
plane→autoairplane
plane→truckairplane
plane→truckairplane
ship→autoairplane
plane→truckairplane
plane→truckairplane
ship→truckairplane
plane→autoairplane
plane→truckairplane
ship→autoairplane
plane→autoairplane
ship→truckairplane
ship→truckairplane
ship→truckairplane
ship→truckairplane
ship→autoairplane
plane→autoairplane
plane→autoairplane
plane→shiptruck
plane→trucktruck
plane→shipautomobile
plane→shipautomobile
plane→shiptruck
plane→shipautomobile
plane→shipautomobile
plane→shiptruck
plane→shipautomobile
plane→shipautomobile
plane→shiptruck
plane→shipautomobile
plane→shipautomobile
plane→shipautomobile
plane→shipautomobile
plane→shipautomobile
plane→shipautomobile
plane→shipautomobile
plane→shiptruck
plane→shiptruck
plane→shipautomobile
plane→shipautomobile
plane→shipautomobile
plane→shipautomobile (b) Step 250 to 251
horse→birdtruck
horse→birdtruck
truck→birdtruck
ship→birdtruck
plane→shiptruck
deer→birdtruck
cat→dogtruck
truck→cattruck
truck→birdtruck
truck→birdtruck
cat→dogtruck
truck→cattruck
auto→birdtruck
cat→dogtruck
truck→birdtruck
auto→birdtruck
truck→birdtruck
truck→birdtruck
cat→cattruck
cat→cattruck
truck→birdtruck
truck→dogtruck
truck→cattruck
dog→dogtruck
auto→autobird
truck→autobird
truck→truckbird
truck→birdbird
truck→truckbird
truck→truckbird
truck→truckbird
truck→truckbird
truck→truckbird
truck→truckbird
auto→autobird
truck→birdbird
auto→shipbird
truck→truckbird
truck→planebird
truck→truckbird
truck→shipbird
auto→birdbird
truck→birdbird
truck→truckbird
auto→autobird
truck→catbird
truck→shipbird
truck→shipbird
(c) Step 500 to 501
ship→shipcat
cat→truckdog
cat→planedog
truck→shipdog
auto→shipdog
cat→truckdog
ship→shipcat
cat→truckcat
truck→truckcat
truck→truckdog
truck→shipdog
dog→shipcat
ship→shipcat
plane→planecat
horse→planecat
cat→planedog
dog→planedog
cat→shipdog
dog→planedog
bird→shipcat
plane→planedog
truck→truckdog
truck→shipfrog
cat→truckdog
dog→shipship
cat→shipship
cat→trucktruck
cat→truckship
dog→trucktruck
cat→shipship
cat→catship
cat→shipship
dog→truckship
cat→trucktruck
dog→catship
dog→catship
dog→truckship
dog→deership
dog→shipship
dog→horseship
cat→shipship
dog→catship
bird→deership
dog→horseship
dog→dogtruck
dog→catship
dog→shipship
dog→autoship (d) Step 750 to 751
Figure 53: (ViT, seed 2) Images with the most positive (top 3 rows) and most negative (bottom 3 rows) change to training
loss after steps 100, 250, 500, and 750. Each image has the true label (above) and the predicted label before and after the
gradient update (below).
53

--- PAGE 54 ---
deer→shipfrog
bird→shipfrog
deer→shipfrog
dog→shipfrog
dog→shipfrog
dog→shipdog
deer→shipfrog
bird→shipfrog
dog→shipdog
cat→shipdog
deer→shipfrog
deer→shipfrog
auto→shipdog
plane→shipdog
plane→shipdog
plane→shipdog
auto→shipdog
deer→shipfrog
dog→shipdog
plane→shipdog
auto→shipdog
plane→shipdog
dog→shipdog
plane→shipdog
deer→shipship
dog→shipship
deer→shipship
deer→shipship
deer→shipship
deer→shipship
dog→shipship
cat→shipship
deer→shipship
cat→shipship
dog→shipship
dog→shipship
cat→shipship
dog→shipship
cat→shipship
deer→shipship
dog→shipship
deer→shipship
dog→shipship
dog→shipship
deer→shipship
deer→shipship
dog→shipship
dog→shipship(a) Step 100 to 101
truck→dogtruck
truck→dogtruck
truck→dogtruck
truck→birdtruck
truck→birdtruck
truck→dogtruck
truck→dogtruck
auto→birdtruck
ship→birdtruck
truck→dogtruck
ship→birdtruck
ship→birdtruck
truck→birdtruck
truck→dogtruck
truck→dogtruck
truck→dogtruck
ship→birdtruck
truck→birdtruck
deer→birdtruck
cat→dogtruck
truck→dogtruck
truck→dogtruck
frog→dogtruck
truck→horsetruck
truck→horsehorse
truck→shipbird
frog→horsehorse
frog→horsehorse
frog→froghorse
frog→horsehorse
frog→horsehorse
frog→horsehorse
truck→horsehorse
truck→catbird
frog→horsehorse
frog→horsehorse
truck→horsehorse
frog→horsehorse
frog→horsehorse
truck→horsedog
auto→horsehorse
frog→froghorse
frog→horsehorse
frog→horsehorse
frog→horsehorse
frog→horsehorse
truck→horsedog
truck→planebird (b) Step 250 to 251
horse→horseautomobile
auto→horseautomobile
deer→horseautomobile
cat→deerautomobile
truck→horseautomobile
auto→horseautomobile
auto→deerautomobile
horse→horseautomobile
horse→horseautomobile
auto→horseautomobile
bird→deerautomobile
horse→horseautomobile
deer→deerautomobile
horse→horseautomobile
horse→horseautomobile
auto→horseautomobile
horse→horseautomobile
dog→horseautomobile
deer→deerautomobile
horse→horseautomobile
cat→horseautomobile
ship→shipautomobile
horse→horseautomobile
auto→shipautomobile
dog→doghorse
auto→horsefrog
auto→froghorse
cat→horsefrog
cat→horsehorse
dog→frogfrog
auto→frogfrog
bird→deerfrog
dog→frogfrog
auto→horsefrog
auto→truckfrog
auto→froghorse
cat→cathorse
auto→autofrog
auto→frogfrog
dog→horsehorse
cat→catfrog
cat→frogfrog
bird→frogfrog
auto→horsefrog
dog→frogfrog
dog→frogfrog
dog→dogfrog
cat→catship
(c) Step 500 to 501
frog→frogautomobile
auto→frogtruck
dog→dogautomobile
auto→frogautomobile
deer→frogtruck
truck→catautomobile
auto→catautomobile
cat→catautomobile
horse→horseautomobile
cat→catautomobile
bird→catautomobile
cat→catautomobile
horse→dogautomobile
auto→catautomobile
cat→catautomobile
truck→catautomobile
auto→frogautomobile
auto→frogautomobile
auto→frogautomobile
horse→birdautomobile
horse→dogautomobile
auto→frogautomobile
dog→dogautomobile
horse→frogautomobile
truck→frogfrog
truck→frogfrog
truck→horsefrog
auto→frogfrog
auto→frogfrog
auto→frogfrog
auto→frogfrog
auto→frogfrog
auto→frogfrog
truck→catfrog
auto→frogfrog
auto→frogfrog
truck→truckfrog
truck→frogfrog
truck→frogfrog
truck→frogfrog
truck→frogfrog
truck→frogfrog
truck→frogfrog
truck→catfrog
truck→frogfrog
auto→autofrog
auto→frogfrog
auto→frogfrog (d) Step 750 to 751
Figure 54: (ViT, seed 3) Images with the most positive (top 3 rows) and most negative (bottom 3 rows) change to training
loss after steps 100, 250, 500, and 750. Each image has the true label (above) and the predicted label before and after the
gradient update (below).
54
