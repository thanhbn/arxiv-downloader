# 2309.01825.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/optimizer/2309.01825.pdf
# File size: 3489204 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
LoopTune: Optimizing Tensor Computations with
Reinforcement Learning
Dejan Grubisic
Rice University, Meta AI
Houston, TX, USABram Wasti
Meta AI
New York, NY, USAChris Cummins
Meta AI
Menlo Park, CA, USAJohn Mellor-Crummey
Rice University
Houston, TX, USAAleksandar Zlateski
Meta AI, Dabun AI
New York, NY, USA
Abstract —Advanced compiler technology is crucial for en-
abling machine learning applications to run on novel hardware,
but traditional compilers fail to deliver performance, popular
auto-tuners have long search times and expert-optimized libraries
introduce unsustainable costs. To address this, we developed
LoopTune, a deep reinforcement learning compiler that optimizes
tensor computations in deep learning models for the CPU.
LoopTune optimizes tensor traversal order while using the ultra-
fast lightweight code generator LoopNest to perform hardware-
specific optimizations. With a novel graph-based representation
and action space, LoopTune speeds up LoopNest by 3.2x, gen-
erating an order of magnitude faster code than TVM, 2.8x
faster than MetaSchedule, and 1.08x faster than AutoTVM,
consistently performing at the level of the hand-tuned library
Numpy. Moreover, LoopTune tunes code in order of seconds.
Index Terms —Compiler, Tensor contractions, Reinforcement
learning
I. I NTRODUCTION
Contemporary advances in machine learning (ML) have
led chip designers to develop a plethora of extremely pow-
erful chips to accommodate computationally intensive ML
workloads. For instance, Nvidia introduced tensor cores [17],
[44], Intel and AMD added specialized A VX [35], [43], FMA
[67], and VNNI instructions [59], while Google introduced
TPUs [37]. Moreover, hardware companies started making
ML-specific chips such as Graphcore [36] and Cerebras [52].
To fully harness the power of advanced hardware, advanced
compiler technology is a must. However, traditional compilers
have several limitations that impede their ability to do so.
First, traditional compilers have been developed for a lim-
ited set of ISAs with similar programming models, making it
difficult to adapt them to exotic hardware with different chip
resources. Even with the front/back-end separation introduced
by LLVM, the task remains challenging, because traditional
representations are not easily optimized for novel hardware.
Second, as traditional compilers are extended to cover more
use cases, they become increasingly complex, with hundreds
of optimization passes that frequently depend on one another.
This complexity increases development and maintenance cost.
Finally, traditional “catch-all” compiler techniques fail to
fully utilize novel resources available on emerging hardware
designed for specific workloads.
Correspondence to Dejan Grubisic on dejan.grubisic@rice.edu
Fig. 1: LoopStack architecture.
So, what are our alternatives to traditional compilers?
Expert-optimized libraries, auto-tuners, or something else?
Expert-optimized libraries require an enormous time invest-
ment by experts and the work must be repeated for each new
device. High-performance tensor operation libraries such as
cuDNN [16], OneDNN [19], or XNNPACK [28] are usually
tied to a narrow range of hardware devices and tend to be large
in code size, which may impede their use on mobile devices.
As an alternative approach, projects like Halide [51] and
TVM [14] optimize a high-level representation of a loop nest
performing a tensor computation with a discrete set of transfor-
mations such as loop reordering and tiling before compiling it
to a particular target hardware with the LLVM compiler. This
approach provides high performance and eliminates the need
for expert-optimized libraries, but introduces an astronomical
number of possible schedules.
To optimize a simple problem, such as Local Laplacian
Filters, Halide estimates a lower bound of 10720possible
schedules [51]. To find performant schedules in such a huge
space, Halide and TVM use genetic algorithms and parallel
simulated annealing with a trained cost model [14] respec-
tively. Both approaches suffer from very large compilation
times.
Contemporary breakthroughs in deep reinforcement learning
(deep RL) in complex video games, such as those in Atari
[47], and AlphaGo [56], have inspired the compiler research
communities to attempt to leverage deep RL as well [13],
[31], [32], [64]. Similar to iterative algorithms, the deep
RL agent explores an optimization space. However, there is
one important difference - knowledge of the search space is
embedded into a neural network. Inferring the neural networkarXiv:2309.01825v3  [cs.LG]  8 Nov 2023

--- PAGE 2 ---
Fig. 2: LoopTune training loop. LoopTune transforms generated benchmark to an intermediate representation (IR), and uses
LoopTool API to apply actions and get observations, while LoopNest compiles and executes the loop nest providing the reward.
then replaces part of the search for optimizations. This enables
example-driven, fast optimization-space search is exactly what
ML-specific, as well as general compilers, need.
In our work, we further build on recent RL-based efforts in
compiler research by extending LoopStack [65] - an ultra-fast
tensor compiler backend, with LoopTune to find a performant
loop schedule with deep reinforcement learning (Figure 1).
LoopTune manipulates loop schedule space through an API
called LoopTool and generates the binary from the given
schedule with LoopNest. The goal of LoopTune is to train
the policy network that will find a close-to-optimal schedule
for the given loop nest in a few steps, decreasing auto-tuning
time to the order of seconds.
In this paper, we present the following novel contributions:
•LoopTune - a deep RL framework that finds performant
loop ranges and schedules.
•A novel graph-based embedding of tensor computations
suitable for reinforcement learning.
•A novel action space for tunning tensor computation
suitable for RL training.
•Comparison of 5 popular RL algorithms in optimizing
loop schedule.
By combining reinforcement learning with appropriate rep-
resentations and a well-chosen optimizer, we are able to
generate faster code than baseline traditional search tech-
niques, outperform popular auto-tuners such as autoTVM and
MetaSchedule, and perform at the level of an expert-optimized
library Numpy.
II. B ACKGROUND
The principal component of machine learning workloads
can be expressed as a series of tensor contractions. Tensor
contractions represent the generalization of matrix multiplica-
tion, trace, transpose, and other commonly used operations on
matrices to higher dimensions.Formally, we can define tensor contractions in the fol-
lowing way [45]. Let A,B,Cbe tensors with dimensions
ofdA, dB, dCrespectively. Similar to 2D matrix multipli-
cation, for each pair of tensors AB,AC,BCwe define the
dimensions both tensors will iterate together. Namely, these
indices will have dimensions IAB= (dA+dB−dC)/2,
IAC= (dA+dC−dB)/2andIBC= (dB+dC−dA)/2.
Then, tensor contraction can be defined with:
where ·is scalar multiplication and Πstands for all permuta-
tions of specified dimensions. Note that here we have to do all
permutations to keep the result consistent, since the iterating
dimensions may be chosen in any order. To simplify notation
further, we can use Einstein notation and implicitly sum over
dimensions that don’t exist in the resulting tensor.
CΠC(I,J)=AΠA(I,K)· BΠB(J,K)
To allow the use of the non-linear activation function used in
deep learning, we extend our notation with an element-wise
operation that transforms the final result (post).
CΠC(I,J)=post(AΠA(I,K)· BΠB(J,K))
With these extensions, we can express not only general
matrix-to-matrix multiplication (GEMM), matrix-to-vector
multiplication (GEMV), and vector-to-matrix multiplication
(GEVM) operations, but also general machine learning prim-
itives such as:
•Convolutions [39] : OR,C=IR+K,C+J·ωK,J
•Pooling [39] : OR,C=max(I2R,2C)
•Reductions [3] : OR=IR,C
•Transpositions [3] : OR,C=IC,R
•Concatenations [50] : OR,C 1+C2=AR,C 1|BR,C 2
2

--- PAGE 3 ---
Fig. 3: Optimizing ranges and order of loops for matrix multiplication using LoopTune’s action space.
•Broadcast [6] : OR,C=IR
Besides machine learning, tensor contractions are widely
used in physics simulations, spectral element methods, quan-
tum chemistry, and other fields. Despite many efforts [22],
[29], [45], none of the state-of-the-art production compilers
such as GCC [57] and LLVM [40] can automatically transform
naive tensor contraction loop nests to expertly-tuned imple-
mentations.
III. L EARNING TO OPTIMIZE TENSOR PROGRAMS
To optimize tensor operations, we separate the problem
of finding optimal loop range and order (schedule) from
hardware-dependent low-level optimizations, such as vector-
ization. To find performant schedules LoopTune uses deep
reinforcement learning to train a policy network, while Loop-
Nest [65] applies low-level tensor optimizations and generates
executable code given a schedule. LoopTune could be also
connected to all backends that optimize a given loop schedule
such as Halide and TVM, which could be an interesting future
direction.
Figure 2 outlines the workflow of LoopTune. The process
begins by creating an RL environment by using CompilerGym
[20]. This framework allows us to map the problem of finding
performant schedules to RL methodology and using state-of-
the-art libraries such as RLlib [42] for training. To create
an environment in CompilerGym we define an action space,
an observation space, and a reward that will be used as an
optimization criterion during RL training. Additionally, we
define a set of benchmarks that represent tensor operations.
For all benchmarks, we assume that loop bounds are constant.
In each training epoch, we convert the benchmark to an
intermediate representation by adding an “agent” annotation
to the first loop (Figure 3). In each step, the agent applies an
action from the action space to the current loop changing the
loop schedule. LoopTune encodes our novel graph-based state
representation to a vector representation (described in Section
III-C) and feeds it to the reinforcement learning training loop.
Finally, LoopNest compiles and evaluates the loop schedule
which provides a reward signal for training the policy network.
In the inference phase, LoopTune iteratively calculates the
best action by the policy network and applies it to the currentstate. Since this procedure doesn’t include loop nest evaluation
it is fast and constrained only to the speed of the inference.
Practically, this enables the policy network to quickly reach
the desired state in a matter of seconds.
A. Defining an Action Space
The LoopTool API [65], provides LoopTune with the ability
to swap the positions of two loops, given their line numbers,
and split a loop, given its line number and a specified tile size.
Rather than having such parametric actions, that are inherently
hard to train [38], LoopTune defines a novel action space
shown in Figure 3 and introduces the abstraction of an agent
that traverses loop nests and applies actions on each loop.
The agent uses upand down actions to move the cursor
without changing the loop nest structure. The swap upand
swap down actions allow the agent to exchange the position
of the current loop with its neighbor, moving the agent’s
cursor respectively. The split family of actions creates a new
loop with the same iterator, dividing the loop range with the
specified split parameter. If the split parameter does not evenly
divide the loop range, the current loop will have a remainder
or “tail”, which will be executed at the end of the loop nest
execution.
By limiting the action space in this way, we are able
to simplify the problem in several ways. For example, a
smaller number of possible actions enables the RL algorithm
to explore and become more confident [38] with each action
for different states. This might force the agent to use longer
sequences of actions to reach certain states, but this is not a
problem since each action other than upanddown changes the
loop nest and provides a non-zero reward signal. Furthermore,
many states benefit from similar action sequences, which
allows training to converge faster.
To keep the design simple, we decided to apply a fixed
number of actions for optimization, rather than having an
action that terminates the search. Our experiments have shown
that having such an action often prevents exploration and
converges to local minima. Instead, we rely on an implicit
stop, which occurs when the agent starts oscillating between
states that differ only by the cursor position.
3

--- PAGE 4 ---
Fig. 4: Text representation shows the algorithm. Schematic representation shows the memory layout. Graph representation
explains nesting order (black), access pattern (red), and data flow (blue). Vector representation aggregates graph representation
for the training.
B. Defining a Reward
For the evaluation metric, we use billions of floating-
point operations per second (GFLOPS). To measure GFLOPS,
LoopTune uses LoopNest to compile and execute loops on a
CPU. To ensure reliable results, LoopNest excludes the first
20 iterations as a warm-up and times multiple executions of
the loop nest, taking the fastest measurement.
During training, the agent applies action (A) from state S,
moving it to the next state S’. The feature extractor maps the
internal representation to the vector (S) which is used as input
to the neural network. LoopNest calculates the reward for the
applied action using the formula:
reward =GFLOPS (S′)−GFLOPS (S)
GFLOPS PEAK PERFORMANCE
This normalizes all rewards, making training more stable.
Rather than relying on peak performance from hardware speci-
fications that may be imprecise, we evaluate peak performance
empirically before the training by running the series of kernels
with high arithmetic intensity, which always falls within a few
percent of the theoretical peak. Finally, we send a tuple (S,
S’, A, R) to the RL library that performs one training step.
C. Defining the State Representation
Each loop nest consists of a nest that computes operations
and a write-back nest that writes the result to the memory.
For state representation, we use the graph-based representation
shown in Figure 4. On this graph, there are 3 kinds ofnodes: loops (rectangles), data (ellipses), and computation
(diamonds). There are 3 kinds of edges. Black edges connect
loops and computations that are nested from top to bottom.
Blue edges represent data flow, while red edges represent the
strides of each loop accessing tensors that read from memory
(A, B), or write to memory (T). Stride is the distance in
memory between two elements of a tensor when we increment
only the index of a given loop. If this number is large, the
iterating loop will try to fetch distant data in memory that
may not be stored in the cache, resulting in a cache miss.
To make our representation usable for standard RL opti-
mizers, we map the key features to a vector. In our vector
representation, each loop is described with 20 integer values,
namely:
•(1) Is the agent’s cursor on the loop
•(1) Loop size
•(1) Loop tail
•(1) Does loop belong to computation or write-back nest
•(16) Histogram of strides frequency
The histogram of strides frequency (Figure 5) represents
the cumulative distribution of access strides for each loop. In
other words, it shows how many accesses with given strides
are produced from the given loop. For each loop, we calculate
strides from the tensor shape and iterator position. Since stride
can be an arbitrary integer larger than zero, we discretize
strides to bins of size 2N, where N∈ {0...15}to match the
sizes of cache lines.
4

--- PAGE 5 ---
Fig. 5: Histogram of strides frequency.
Agent bits are necessary since they give meaning to all
actions since they depend on the cursor position. Size and
tail bits define how many times memory is accessed with
each stride, which distribution is captured in strides frequency.
The computation loop shows whether the loop is used for
computation or writeback.
We believe this is a minimal set of features for the RL
algorithm to learn memory access patterns, which is key to
optimizing for memory-bound computations such as tensor
contractions. For applications that are compute-bound, adding
features that describe computation in the loop body would be
beneficial.
D. Library for Reinforcement Learning
To optimize the training process of our reinforcement
learning model, we have chosen to use RLlib [42], the
performant library for reinforcement learning. In our work,
we evaluate several learning algorithms, supported by RLlib,
including Deep Q Learning (DQN), Apex Deep Q Learning
(APEX DQN), Proximate Policy Optimization (PPO), Actor-
Critic (A3C), and Impala.
DQN [48] attempts to learn the state value function by using
experience replay for storing the episode steps in memory for
off-policy learning, where samples are drawn from the replay
memory at random.
APEX DQN [33] creates instances of environment for
each actor and collects the resulting experience in a shared
experience replay memory prioritizing the most significant
data generated by actors.
PPO [55] alternates between sampling data through the in-
teraction with the environment while using stochastic gradient
ascent with minibatch updates.
A3C [46] calculates gradients on the workers directly in
each episode and only broadcasts these gradients to the central
model. Once the central model is updated parameters are sent
back to the workers.
IMPALA [26] provides a scalable solution for collecting
samples from individual agents and running stochastic gradient
descent in the central loop.
To get out of the training choosing the suitable RL algorithm
is crucial. Our experiments suggest that APEX DQN achieves
converges significantly faster than other approaches that we
elaborate on in the evaluation section.IV. L OOPNESTBACKEND OPTIMIZER
LoopNest [65] is a powerful, domain-specific compiler that
is specifically designed to optimize tensor programs with tun-
ing with reinforcement learning in mind. Rather than relying
on extensive general-purpose compilers, it utilizes a small
set of expert-designed HPC optimizations, enabling fast code
generation for each RL step. It includes custom primitives in
code generation, custom assembly codes, instruction reorder-
ing, r-sum, and other optimizations suggested by optimization
manuals for the target hardware [9], [34].
Unlike traditional compilers, LoopNest takes into account
user-defined orders of operations. This simplifies the code
generator design and provides a more direct mapping between
the quality of the user-defined order and its performance. This
feature is particularly important for finding the best sequence
of actions with reinforcement learning, and we firmly believe
it makes the optimization space smoother and enables faster
convergence.
Furthermore, LoopNest performs loop unrolling in a way
that is consistent with hardware requirements and automat-
ically vectorizes the innermost loop. It also applies register
tiling [25], keeping a portion of the output tensor in registers
at all times. To reduce pressure on load/store units, LoopNest
never generates code that spills registers to the stack, unlike
traditional compilers like LLVM and GCC. It achieves this by
finding the largest scope in which any modified values can fit
in the register file.
When compared to LLVM [40], which is commonly used
as a backend choice for tensor compilers such as Halide [51]
and TVM [14], LoopNest achieves orders of magnitude faster
compilation times while generating code that is equal or higher
in performance on AMD (AMX512) architecture (Table I).
Halide is used to emit schedules for LLVM code generation,
turning off runtime assertions and bound checks.
TABLE I: LoopNest vs. LLVM performance on AMD (A VX2)
architecture [65].
Beyond the AMD (A VX2) architecture, LoopNest achieves
similar results for Intel (A VX512), Cortex A57, Cortex A73,
NVIDIA Denver2, and Apple M1 architectures. Additionally,
5

--- PAGE 6 ---
LoopNest has a small binary footprint of 250Kb, compared to
LLVM’s 350Mbfootprint, which makes LoopNest a compiler
of choice for use on mobile and embedded systems.
V. S EARCH TO OPTIMIZE TENSOR PROGRAMS
The traditional approach for auto-tuning tensor programs is
based on hill climbing, genetic, and various search algorithms
[10]. These algorithms can find performant schedules for a
single program, but the search time and the quality of the
solution depend heavily on the smoothness of the optimization
space. If the optimization sequence to highly rewarded states
includes some actions that produce negative rewards, hill
climbing algorithms can converge to local minima. Genetic
algorithms, on the other hand, use a lot of computational
resources, because they converge slowly.
We implemented the following set of search algorithms
(Figure 6) to identify the difficulty of the problem:
•Greedy search with lookahead of 1 and 2
•Beam Depth First Search (BeamDFS) with width 2, 4
•Beam Breath First Search (BeamBFS) with width 2, 4
•Random search
First, we introduce the family of Greedy search algorithms
with arbitrary lookahead. In each step of this algorithm, we
evaluate all possible states after applying lookahead steps
and select the step toward the most promising state. With a
lookahead of 1, the agent stops if there is no better action than
the current state, while the lookahead of 2 enables the agent to
tolerate one bad step that leads to a more promising solution.
Ideally, with a large enough lookahead, Greedy Search would
be able to overcome the problem of local minima for actions
with negative rewards. Unfortunately, such computation comes
with the cost of O(steps∗|action space|lookahead), which is
prohibitively expensive for large lookaheads.
Second, we implemented a family of Beam search algo-
rithms with arbitrary width. In each step, we calculate the best
width actions and expand them further until we reach the spec-
ified depth of the search tree. Expansion of the states could be
done in depth-first (BeamDFS) and breadth-first (BeamBFS)
manner and search properties drastically differ when search
time elapses before the full search graph is constructed.
Complexity of both of these approach is O(widthsteps), where
width < |action space|.
BeamDFS can be seen as an extension to the Greedy
algorithm with a lookahead of 1, with few additions. It doesn’t
terminate if the next state is worse than the current and it
recursively visits all states of the search graph where each
node has maximum width children. This enables it to tolerate
non-convex parts of spaces as long as the optimal action ranks
better than other actions in the current step.
BeamBFS variant finds iteratively a performant action se-
quence as it builds a search graph for each number of steps.
This approach would be beneficial if the performant sequence
is shorter than the specified search depth.Finally, Random search randomly chooses a sequence of
actions with a specified length. The benefit of this search
is that it can uniformly explore a large number of diverse
states providing a general idea about the landscape. From our
experiments, random search provides surprisingly good results
that we elaborate on in the next section.
VI. E VALUATION
We evaluated LoopTune on a series of benchmarks to
answer to following questions:
•How do different RL algorithms compare to each other?
•How does LoopTune compare to traditional search algo-
rithms?
•How does LoopTune compare to optimized libraries and
auto-tuners like TVM?
The benchmark dataset consists of synthesized loop nests
for matrix multiplication. The matrix multiplication dataset has
2197 untiled loop nests for matrices with dimensions in the
range from 64 to 256 with the step of 16. We chose these
ranges since they are used as tile sizes for customized libraries
such as cuBLAS [1]. We train on the 80% split of the dataset
(size 1757) while leaving 20% for the test dataset (size 440).
Experiments are performed on an Intel Xeon CPU running
on 2.20GHz, with 40 CPU cores and 2 Nvidia Quadro GP100
GPUs. The CPU has cache sizes L1(data/instruction) 1.3 MB,
L2 10MB, L3 52MB.
A. RLlib Training Analysis
To train LoopTune, we use Ray’s RLlib library [42]. After
we define our environment in CompilerGym and register it
with RLlib, we need to instantiate the appropriate trainer
and find its most promising hyper-parameters. To find the
best trainer we compare PPO, A3C, DQN, APEX DQN, and
Impala. In all cases as a model, we use the network with
fully connected layers, with arbitrary width and the number
of layers, and the same feature representation.
To find the optimal parameters for each trainer, we run a
hyper-parameter sweep for the learning rate, exploration factor,
depth, and width of the neural network. After finding the best
parameters for each trainer, we run the final training for 4000
iterations and stop training early if the average reward per
epoch converges. In each iteration, the optimizer applies the
episode of 10 actions and updates the neural network. Finally,
we compare trainers by plotting the episode reward mean
which represents the averaged increase of GFLOPS achieved
in the episode normalized to the peak performance of the
device (Figure 7).
We found that the APEX DQN trainer performs an order of
magnitude better than other trainers, converging after roughly
200 steps and providing an average increase of 30% of the
peak performance (peak = 114.204 GFLOPS). In contrast,
PPO required more than 1000 steps to converge to an im-
provement of 8% of the peak, while Impala, A3C, and DQN
have not been able to achieve positive results. We trained
6

--- PAGE 7 ---
Fig. 6: Traditional search approach in finding the optimal sequence. Actions (edges) are sorted by the performance of the next
state.
Fig. 7: Average reward per epoch for RLlib algorithms during
training of 4000 steps.
APEX DQN for 17.5 hours for 1000 iterations, which could
be shortened to 3.5h with early stopping after 200 iterations.
We believe that the superiority of APEX DQN lies in the
capability to prioritize the most significant experiences gener-
ated by the actors. Since all the algorithms are implemented in
RLlib, it takes only one line of change to benefit from novel
algorithms in the future. We further compare the APEX DQN
solution with non-RL approaches.
B. Comparison to Search Based Approaches
To evaluate the difficulty of searching the optimization
space, we run a set of traditional search algorithms including
Greedy search with lookahead of 1 and 2, BFS and DFS
variant of Beam search with widths 2 and 4, and Random
search. We implemented each search with caching to avoid
repeating evaluations of the same states. We run each search
on a test dataset of 440 benchmarks, setting the time limit to 60
seconds. To compare traditional searches to policy generated
from the RL approach, we visualize the search time and
achieved performance of the produced code of 25 random
benchmarks from the test set in Figure 8.
In 88% test benchmarks, the APEX DQN policy network
outperforms the best traditional searches by 1.8x on average
in less than a second, which is an order of magnitude lesstime. To better understand the characteristics of each search
we present the speedup distribution in Figure 9.
Increasing lookahead to 2 improves Greedy search’s per-
formance. Beam2BFS and Beam2DFS achieve poor results,
despite exploring the entire search subtree, which implies
that performant schedules include non-performant actions.
Increasing the width to 4 significantly boosts performance,
outperforming Greedy2. The success of Random search further
emphasizes that optimization space is non-linear. Finally, RL
policy network significantly outperforms all search methods
by optimizing for long-range rewards up to 10 steps ahead,
avoiding local minima.
C. Analysis of the loop schedule optimization space
Next, we visualize the performance and search speed of
search algorithms and the RL approach for each step (Figure
10). The upper figure shows the reward signal in GFLOPS for
the best-found schedule, while the lower figure shows how
long it takes to choose an action for the given step. For the
Depth-First search and Random search, actions are not decided
until the end of the search graph construction and appear flat
on the plot.
Greedy1 terminates quickly, creating a search graph of depth
2 and being stuck to the local minimum. Greedy2 can expand
the graph up to depth 6, avoiding one-step local minima and
achieving better performance, but still exploring only a small
number of states.
Beam2DFS expands the graph in-depth and each layer is
updated during graph construction, keeping the time curve
relatively flat. BeamBFS, on the other hand, builds the search
space layer by layer completing lower layers first. The fact that
Beam2DFS and Beam2BFS finished before the deadline (60s)
means that they constructed the whole search graph of spawn
2. Neither of the two searches found a performant solution
indicating that all performant schedules consist of at least one
action which is best 2 actions.
Beam4DFS and Beam4BFS both terminated with a deadline
which means that they only partially constructed their search
graph of spawn 4. Beam4DFS’s search graph includes solu-
tions with long sequences of up to 10 steps, while Beam4BFS
completely explored all solutions with 5 steps. In both cases,
the best solutions contain long sequences of actions with non-
7

--- PAGE 8 ---
Fig. 8: Achieved performance ( higher is better) and search time ( lower is better) of randomly selected 25 test benchmarks
given 60 seconds for search. Original refers to LoopNest, which was used as a back compiler for greedy, beam, random
searches, and the LoopTune method.
Fig. 9: Speedup distribution for searches from Figure 8 nor-
malized with LoopNest results.
monotonically increasing performance, which enables these
searches to see further than Greedy searches.
The Random search uses all time to expand the search
graph from root to depth 10 without following any metric and
evaluating each state in the graph. This way Random search
can uniformly explore optimization space, including sequences
of non-monotonic actions.
RL policy network is able to outperform all previous
algorithms by learning optimization patterns that maximize
future rewards, speeding up the execution 3.2x times on av-
erage compared to the original LoopNest implementation. Its
solution tolerates long sequences of non-performant actions,
being worse than all other searches from the 4th to the 7th
Fig. 10: Performance and time needed for expanding a search
graph in each step.
step to reach a performant state at the 8th step. Additionally,
RL policy network search time grows linearly in the length
of an action sequence, which enables us to use the policy
network on harder problems that require a larger number of
steps. These capabilities are paramount for auto-tuning general
compilers such as LLVM.
D. Comparison to Numpy, TVM, MetaSchedule, and AutoTVM
Next, we show performance profiles [24] for compilation
and execution of generated code on the test dataset (440
examples), and compare it to a popular hand-tuned library
8

--- PAGE 9 ---
Fig. 11: Compile time and Execution ratio of test benchmarks. For Figure b), test cases were normalized with the best method
sorted from best to worst on the y-axis.
for tensor operations – Numpy0, tensor compiler – TVM (base
version and optimized version with blocking, permutation, and
vectorization) and widely used auto-tuners – autoTVM and
MetaSchedule (Figure 11).
LoopTune outperforms all other approaches in 67% of test
cases while reaching more than 90% of best performance in
92% of test cases. On average LoopTune beats base TVM
by 43x, optimized TVM by 9.7x, MetaSchedule by 2.8x, and
AutoTVM by 1.08x, while being 3% slower than Numpy.
In contrast to Numpy, LoopTune doesn’t require hand-tuning
which reduces development and maintenance costs.
Moreover, LoopTune makes real-time autotuning practi-
cal, generating code in just 1 second, while autoTVM and
MetaSchedule require 33 and 62 seconds on average. This is
particularly important for applications that require download-
ing and tuning in real-time from web-based repositories. An
example of this can be tuning image/video filters for social
media apps and video games for mobile or VR devices.
We used official documentation from TVM [23] to im-
plement matrix multiplication for the examples from the
test set. This implementation of TVM includes blocking,
loop permutation, and vectorization optimizations, which are
the same set of optimizations we are using for LoopTune.
We enable the ”llvm -mcpu=core-avx2” option for TVM,
MetaSchedule, and AutoTVM, to get the best results for our
architecture. For MetaSchedule we used stochastic sampling,
tiling, reordering, and unrolling, while for AutoTVM we used
XGBTuner, evaluating 64 possible schedules for both.
Evaluating more than 64 schedules would require propor-
tionally more time, which makes it prohibitively long for our
use case – auto-tuning in a matter of seconds. For the same
reason, we don’t include in our evaluation popular cost-model
0Numpy uses Intel’s state-of-the-art MKL implementation of BLAS.based frameworks such as Ansor [68], Value Learning [58]
and TenSet [69] and FlexTensor [70] since they have similar
or longer search time.
VII. R ELATED WORK
Tensor specific libraries. Tensor-based mathematical no-
tation was first used by APL [4]. Similar to APL, modern
frameworks such as NumPy [61], Matlab’s Tensor Toolbox
[11], Intel MKL [18], PyTorch [49] and Tensorflow [2] pro-
vide an intuitive interface for manipulating tensors, perform-
ing customized operations, and executing machine learning
algorithms. Although these libraries often vectorize tensor
computation, they hardly find the most performant order and
sizes of the loop for custom hardware.
Search-based compilers. Rather than relying on one-size-
fits-all solutions from expert-written libraries, projects AT-
LAS [66] and FFTW [27] empirically optimize BLAS and
FFT routines given custom hardware. PetaBricks [7] chooses
the most appropriate algorithm of the computation for the
given platform and tunes its parameter by using iterative
methods. OpenTuner [8] provides an ensemble of method-
agnostic search techniques for program autotuning. Although
these methods can be highly effective, auto-tuning requires
significant search times for each program, which can be
prohibitively expensive. LoopTune takes only a second to tune
tensor computations.
Graph-based compilers. nGraph [21] passes its graph
internal representation to a transformer and generates opti-
mized code for the selected backend. XLA [54] automatically
replaces subgraphs from Tensorflow with optimized binaries.
Glow [53] applies domain-specific optimization at a high
level, memory-related optimizations at instruction-based in-
termediate representation, and hardware-specific optimization
at the lowest level. MLIR [41] provides extensible compiler
9

--- PAGE 10 ---
infrastructure that aims to unify domain-specific optimizers,
providing multiple representations and layers of optimiza-
tion. Rather than using a complex representation, LoopTune
encodes graph-based representation to simple vectors with
relevant features to describe memory access patterns. This
enables fast inference with a simple multi-layer perceptron.
Scheduling-based compilers. Halide [51] is the first in-
fluential work to propose the separation of computation and
schedule for optimizing image processing and tensor com-
putations. It uses a declarative language to specify tensor
computations and a separate language for scheduling its ex-
ecution. Similar to LoopTune, Halide’s scheduling language
includes operations such as splitting, and reordering, with
the addition of vectorizing, unrolling, and parallelizing loops.
TVM [14] extends Halide’s compute/schedule concept with
hardware intrinsics and defines new optimizations such as
tensorization and latency hiding. AutoTVM [15] extends the
TVM cost model and adds a template-guided search frame-
work. FlexTensor [70] search directly schedule primitives on
the finner-graned level than templates. In contrast to these
approaches, LoopTune defines the action space with a policy
model in mind eliminating parametrized actions that are hard
to learn [38].
Polyhederal based compilers. To represent tensor com-
putation polyhedral optimizers Polly [30] and others [63] [12]
use linear programming and affine transformations to optimize
loops static control-flow. Tensor comprehensions [62] uses
Halide’s intermediate representation to represent computation,
polyhedral representation to represent loops and just-in-time
compilation for GPU.
Cost-model based compilers. To speed up the evaluation
of a computation, popular frameworks such as Ansor [68],
Value Learning [58], and TenSet [69] learn a cost model
to evaluate performance and use decision trees, evolutionary
search, and monte-carlo tree search to identify the best one.
Although the performance cost model reduces evaluation time,
converging to the optimal state in highly non-convex action
spaces is difficult. Additionally, the inference with the cost
model and basic greedy search requires actions sequence len
* number ofpossible actions inferences, while the policy
network requires only actions sequence leninferences which
is exactly the case for LoopTune.
Policy-model based compilers. Neurovectorizer [31] uses
deep RL to improve the vectorization of CPU loops by tuning
vectorization width and interleaving count. Chameleon [5]
uses a policy network to guide an adaptive sampling algorithm
with domain knowledge to search configuration space. MLGO
[60] uses Policy gradient and Evolution strategies to optimize
binary size by inlining functions. PolyGym [13] explores
loop schedules combining polyhedral representation with RL
and provides infrastructure for the user to apply different
RL algorithms utilizing their representation. In contrast to
these approaches, LoopTune proposes a novel graph-based
representation, action space, and methodology for optimizing
loop nests.VIII. L IMITATIONS AND FUTURE WORK
One of LoopTune’s limitations is that the loop nest shape
needs to be known in compile time. For most of the ML
computation, this is defined by design. Another limitation is
that training time is proportional to the computation workload
since we measure performance explicitly rather than using a
cost model. For small kernels, this is not the problem, while
for larger kernels it might be necessary to use a cost model
during training. Finally, at the moment LoopTune has only
support for CPU, although we looking forward to implement
GPU support in the future.
IX. C ONCLUSIONS
LoopTune is a novel performant auto-tuner for tensor com-
putations on the CPU, capable of auto-tuning code in less than
a second. LoopTune utilizes deep RL to train a policy network
that reorders and tiles loop nests and applies hardware-specific
optimization using LoopNest to tailor the loop nest to the
underlying hardware. To map this problem to reinforcement
learning, LoopTune introduces a unique action space, graph-
based state representation, and reward signal.
By using RLlib’s APEX DQN algorithm, LoopTune speeds
up the original LoopNest implementation by 3.2x given 1
second, on a suite of test problems, while the best tra-
ditional search algorithm achieved 1.8x given 60 seconds.
LoopTune achieves an order of magnitude better results
than the optimized implementation of TVM, which includes
blocking, loop permutation, and vectorization. Additionally,
LoopTune outperforms MetaSchedule and AutoTVM by 2.8x
and 1.08x on average, generating code again in 1 second,
while MetaSchedule and AutoTVM require 33 seconds and
62 seconds, respectively. This makes real-time auto-tuning
possible.
Finally, LoopTune consistently performs at the same level
as the expert-optimized library Numpy, significantly reducing
development efforts. This finding further supports the belief
that deep reinforcement learning techniques will play an
important role in the next generation of compilers.
REFERENCES
[1] Matrix multiplication background user’s guide. https://docs.nvidia.com/
deeplearning/performance/dl-performance-matrix-multiplication/index.
html. Accessed: 2023-04-26.
[2] Mart ´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis,
Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving,
Michael Isard, et al. Tensorflow: A system for large-scale machine
learning. In 12th {USENIX }symposium on operating systems design
and implementation ( {OSDI }16), pages 265–283, 2016.
[3] Herv ´e Abdi and Lynne J Williams. Principal component analysis. Wiley
interdisciplinary reviews: computational statistics , 2(4):433–459, 2010.
[4] Philip Samuel Abrams. An apl machine. Technical report, Stanford
Linear Accelerator Center, Calif., 1970.
[5] Byung Hoon Ahn, Prannoy Pilligundla, Amir Yazdanbakhsh, and Hadi
Esmaeilzadeh. Chameleon: Adaptive code optimization for expedited
deep neural network compilation. arXiv preprint arXiv:2001.08743 ,
2020.
[6] Marjan Albooyeh, Daniele Bertolini, and Siamak Ravanbakhsh. In-
cidence networks for geometric deep learning. arXiv preprint
arXiv:1905.11460 , 2019.
10

--- PAGE 11 ---
[7] Jason Ansel, Cy Chan, Yee Lok Wong, Marek Olszewski, Qin Zhao,
Alan Edelman, and Saman Amarasinghe. Petabricks: A language and
compiler for algorithmic choice. ACM Sigplan Notices , 44(6):38–49,
2009.
[8] Jason Ansel, Shoaib Kamil, Kalyan Veeramachaneni, Jonathan Ragan-
Kelley, Jeffrey Bosboom, Una-May O’Reilly, and Saman Amarasinghe.
Opentuner: An extensible framework for program autotuning. In Pro-
ceedings of the 23rd international conference on Parallel architectures
and compilation , pages 303–316, 2014.
[9] R ARM. Cortex-a57 software optimization guide. ARM , 2016.
[10] Amir H Ashouri, William Killian, John Cavazos, Gianluca Palermo,
and Cristina Silvano. A survey on compiler autotuning using machine
learning. ACM Computing Surveys (CSUR) , 51(5):1–42, 2018.
[11] Brett W Bader and Tamara G Kolda. Algorithm 862: Matlab tensor
classes for fast algorithm prototyping. ACM Transactions on Mathemat-
ical Software (TOMS) , 32(4):635–653, 2006.
[12] Roberto Bagnara, Patricia M Hill, and Enea Zaffanella. The parma
polyhedra library: Toward a complete set of numerical abstractions for
the analysis and verification of hardware and software systems. Science
of Computer Programming , 72(1-2):3–21, 2008.
[13] Alexander Brauckmann, Andr ´es Goens, and Jeronimo Castrillon. A
reinforcement learning environment for polyhedral optimizations. arXiv
preprint arXiv:2104.13732 , 2021.
[14] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie
Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis
Ceze, Carlos Guestrin, and Arvind Krishnamurthy. TVM: An automated
end-to-end optimizing compiler for deep learning. In 13th USENIX
Symposium on Operating Systems Design and Implementation (OSDI
18), pages 578–594, Carlsbad, CA, October 2018. USENIX Association.
[15] Tianqi Chen, Lianmin Zheng, Eddie Yan, Ziheng Jiang, Thierry Moreau,
Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. Learning
to optimize tensor programs. In Advances in Neural Information
Processing Systems , pages 3389–3400, 2018.
[16] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen,
John Tran, Bryan Catanzaro, and Evan Shelhamer. cudnn: Efficient
primitives for deep learning. arXiv preprint arXiv:1410.0759 , 2014.
[17] Jack Choquette, Wishwesh Gandhi, Olivier Giroux, Nick Stam, and
Ronny Krashinsky. Nvidia a100 tensor core gpu: Performance and
innovation. IEEE Micro , 41(2):29–35, 2021.
[18] Intel Corporation. Mkl developer reference. https://software.intel.com/
content/www/us/en/develop/documentation/mkl-developer-reference-c/
top.html, 2020.
[19] Intel Corporation. Onednn. https://github.com/oneapi-src/oneDNN,
2020.
[20] Chris Cummins, Bram Wasti, Jiadong Guo, Brandon Cui, Jason Ansel,
Sahir Gomez, Somya Jain, Jia Liu, Olivier Teytaud, Benoit Steiner, et al.
Compilergym: robust, performant compiler optimization environments
for ai research. In 2022 IEEE/ACM International Symposium on Code
Generation and Optimization (CGO) , pages 92–105. IEEE, 2022.
[21] Scott Cyphers, Arjun K Bansal, Anahita Bhiwandiwalla, Jayaram Bobba,
Matthew Brookhart, Avijit Chakraborty, Will Constable, Christian Con-
vey, Leona Cook, Omar Kanawi, et al. Intel ngraph: An intermediate
representation, compiler, and executor for deep learning. arXiv preprint
arXiv:1801.08058 , 2018.
[22] Edoardo Di Napoli, Diego Fabregat-Traver, Gregorio Quintana-Ort ´ı,
and Paolo Bientinesi. Towards an efficient use of the blas library for
multilinear tensor contractions. Applied Mathematics and Computation ,
235:454–468, 2014.
[23] TVM documentation version(0.11.dev0). How to optimize gemm on
cpu¶. [Online; accessed 28-November-2022].
[24] Elizabeth D Dolan and Jorge J Mor ´e. Benchmarking optimization soft-
ware with performance profiles. Mathematical programming , 91:201–
213, 2002.
[25] Lukasz Domagala, Fabrice Rastello, Sadayappan Ponnuswany, and Duco
Van Amstel. A tiling perspective for register optimization. arXiv preprint
arXiv:1406.0582 , 2014.
[26] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad
Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning,
et al. Impala: Scalable distributed deep-rl with importance weighted
actor-learner architectures. In International conference on machine
learning , pages 1407–1416. PMLR, 2018.
[27] Matteo Frigo and Steven G Johnson. Fftw: An adaptive software
architecture for the fft. In Proceedings of the 1998 IEEE InternationalConference on Acoustics, Speech and Signal Processing, ICASSP’98
(Cat. No. 98CH36181) , volume 3, pages 1381–1384. IEEE, 1998.
[28] Google. Xnnpack. https://github.com/google/XNNPACK, 2020.
[29] Tobias Grosser, Armin Groesslinger, and Christian Lengauer.
Polly—performing polyhedral optimizations on a low-level intermediate
representation. Parallel Processing Letters , 22(04):1250010, 2012.
[30] Tobias Grosser, Hongbin Zheng, Raghesh Aloor, Andreas Simb ¨urger,
Armin Gr ¨oßlinger, and Louis-No ¨el Pouchet. Polly-polyhedral optimiza-
tion in llvm. In Proceedings of the First International Workshop on
Polyhedral Compilation Techniques (IMPACT) , volume 2011, page 1,
2011.
[31] Ameer Haj-Ali, Nesreen K Ahmed, Ted Willke, Yakun Sophia Shao,
Krste Asanovic, and Ion Stoica. Neurovectorizer: End-to-end vec-
torization with deep reinforcement learning. In Proceedings of the
18th ACM/IEEE International Symposium on Code Generation and
Optimization , pages 242–255, 2020.
[32] Ameer Haj-Ali, Qijing Huang, William Moses, John Xiang, Ion Stoica,
Krste Asanovic, and John Wawrzynek. Autophase: Compiler phase-
ordering for high level synthesis with deep reinforcement learning. arXiv
preprint arXiv:1901.04615 , 2019.
[33] Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo
Hessel, Hado Van Hasselt, and David Silver. Distributed prioritized
experience replay. arXiv preprint arXiv:1803.00933 , 2018.
[34] R Intel. Intel 64 and ia-32 architectures optimization reference manual.
Intel Corporation, Sept , 2014.
[35] Hwancheol Jeong, Sunghoon Kim, Weonjong Lee, and Seok-Ho
Myung. Performance of sse and avx instruction sets. arXiv preprint
arXiv:1211.0820 , 2012.
[36] Zhe Jia, Blake Tillman, Marco Maggioni, and Daniele Paolo Scarpazza.
Dissecting the graphcore ipu architecture via microbenchmarking. arXiv
preprint arXiv:1912.03413 , 2019.
[37] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav
Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden,
Al Borchers, et al. In-datacenter performance analysis of a tensor
processing unit. In Proceedings of the 44th annual international
symposium on computer architecture , pages 1–12, 2017.
[38] Anssi Kanervisto, Christian Scheller, and Ville Hautam ¨aki. Action space
shaping in deep reinforcement learning. In 2020 IEEE Conference on
Games (CoG) , pages 479–486. IEEE, 2020.
[39] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet
classification with deep convolutional neural networks. Communications
of the ACM , 60(6):84–90, 2017.
[40] Chris Lattner and Vikram Adve. Llvm: A compilation framework for
lifelong program analysis & transformation. In International Symposium
on Code Generation and Optimization, 2004. CGO 2004. , pages 75–86.
IEEE, 2004.
[41] Chris Lattner, Mehdi Amini, Uday Bondhugula, Albert Cohen, Andy
Davis, Jacques Pienaar, River Riddle, Tatiana Shpeisman, Nicolas Vasi-
lache, and Oleksandr Zinenko. Mlir: Scaling compiler infrastructure
for domain specific computation. In 2021 IEEE/ACM International
Symposium on Code Generation and Optimization (CGO) , pages 2–14.
IEEE, 2021.
[42] Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox,
Ken Goldberg, Joseph Gonzalez, Michael Jordan, and Ion Stoica. Rllib:
Abstractions for distributed reinforcement learning. In International
Conference on Machine Learning , pages 3053–3062. PMLR, 2018.
[43] Chris Lomont. Introduction to intel advanced vector extensions. Intel
white paper , 23, 2011.
[44] Stefano Markidis, Steven Wei Der Chien, Erwin Laure, Ivy Bo Peng,
and Jeffrey S Vetter. Nvidia tensor core programmability, performance &
precision. In Pat Langley, editor, 2018 IEEE international parallel and
distributed processing symposium workshops (IPDPSW) , pages 522–
531, Stanford, CA, 2018. IEEE.
[45] Devin A Matthews. High-performance tensor contraction without
transposition. SIAM Journal on Scientific Computing , 40(1):C1–C24,
2018.
[46] V olodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex
Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray
Kavukcuoglu. Asynchronous methods for deep reinforcement learning.
InInternational conference on machine learning , pages 1928–1937.
PMLR, 2016.
[47] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves,
Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing
11

--- PAGE 12 ---
atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602 ,
2013.
[48] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves,
Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing
atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602 ,
2013.
[49] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Brad-
bury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein,
Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit
Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An
imperative style, high-performance deep learning library. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alch ´e-Buc, E. Fox, and R. Garnett,
editors, Advances in Neural Information Processing Systems 32 , pages
8024–8035. Curran Associates, Inc., 2019.
[50] Valentin Radu, Catherine Tong, Sourav Bhattacharya, Nicholas D Lane,
Cecilia Mascolo, Mahesh K Marina, and Fahim Kawsar. Multimodal
deep learning for activity and context recognition. Proceedings of the
ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies ,
1(4):1–27, 2018.
[51] Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris,
Fr´edo Durand, and Saman Amarasinghe. Halide: a language and
compiler for optimizing parallelism, locality, and recomputation in
image processing pipelines. Acm Sigplan Notices , 48(6):519–530, 2013.
[52] Kamil Rocki, Dirk Van Essendelft, Ilya Sharapov, Robert Schreiber,
Michael Morrison, Vladimir Kibardin, Andrey Portnoy, Jean Francois
Dietiker, Madhava Syamlal, and Michael James. Fast stencil-code com-
putation on a wafer-scale processor. In SC20: International Conference
for High Performance Computing, Networking, Storage and Analysis ,
pages 1–14. IEEE, 2020.
[53] Nadav Rotem, Jordan Fix, Saleem Abdulrasool, Garret Catron, Summer
Deng, Roman Dzhabarov, Nick Gibson, James Hegeman, Meghan Lele,
Roman Levenstein, et al. Glow: Graph lowering compiler techniques
for neural networks. arXiv preprint arXiv:1805.00907 , 2018.
[54] Amit Sabne. Xla : Compiling machine learning for peak performance,
2020.
[55] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and
Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint
arXiv:1707.06347 , 2017.
[56] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre,
George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou,
Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go
with deep neural networks and tree search. nature , 529(7587):484–489,
2016.
[57] Richard M Stallman et al. Using and porting the GNU compiler
collection , volume 86. Free Software Foundation, 1999.
[58] Benoit Steiner, Chris Cummins, Horace He, and Hugh Leather. Value
learning for throughput optimization of deep learning workloads. Pro-
ceedings of Machine Learning and Systems , 3, 2021.
[59] A Tekin, A Tuncer Durak, C Piechurski, D Kaliszan, F Aylin Sungur,
F Roberts ´en, and P Gschwandtner. State-of-the-art and trends for com-
puting and interconnect network solutions for hpc and ai. Partnership
for Advanced Computing in Europe, Available online at www. praceri.
eu, 2021.
[60] Mircea Trofin, Yundi Qian, Eugene Brevdo, Zinan Lin, Krzysztof
Choromanski, and David Li. Mlgo: a machine learning guided compiler
optimizations framework. arXiv preprint arXiv:2101.04808 , 2021.
[61] Stefan Van Der Walt, S Chris Colbert, and Gael Varoquaux. The numpy
array: a structure for efficient numerical computation. Computing in
science & engineering , 13(2):22–30, 2011.
[62] Nicolas Vasilache, Oleksandr Zinenko, Theodoros Theodoridis, Priya
Goyal, Zachary DeVito, William S Moses, Sven Verdoolaege, An-
drew Adams, and Albert Cohen. Tensor comprehensions: Framework-
agnostic high-performance machine learning abstractions. arXiv preprint
arXiv:1802.04730 , 2018.
[63] Sven Verdoolaege. isl: An integer set library for the polyhedral model.
InInternational Congress on Mathematical Software , pages 299–302.
Springer, 2010.
[64] Huanting Wang, Zhanyong Tang, Cheng Zhang, Jiaqi Zhao, Chris
Cummins, Hugh Leather, and Zheng Wang. Automating reinforcement
learning architecture design for code optimization. In Proceedings
of the 31st ACM SIGPLAN International Conference on Compiler
Construction , pages 129–143, 2022.[65] Bram Wasti, Jos ´e Pablo Cambronero, Benoit Steiner, Hugh Leather, and
Aleksandar Zlateski. Loopstack: a lightweight tensor algebra compiler
stack. arXiv preprint arXiv:2205.00618 , 2022.
[66] R Clinton Whaley and Jack J Dongarra. Automatically tuned linear
algebra software. In SC’98: Proceedings of the 1998 ACM/IEEE
conference on Supercomputing , pages 38–38. IEEE, 1998.
[67] Markus Wittmann, Thomas Zeiser, Georg Hager, and Gerhard Wellein.
Short note on costs of floating point operations on current x86-64
architectures: Denormals, overflow, underflow, and division by zero.
arXiv preprint arXiv:1506.03997 , 2015.
[68] Lianmin Zheng, Chengfan Jia, Minmin Sun, Zhao Wu, Cody Hao Yu,
Ameer Haj-Ali, Yida Wang, Jun Yang, Danyang Zhuo, Koushik Sen,
et al. Ansor: Generating high-performance tensor programs for deep
learning. arXiv preprint arXiv:2006.06762 , 2020.
[69] Lianmin Zheng, Ruochen Liu, Junru Shao, Tianqi Chen, Joseph E
Gonzalez, Ion Stoica, and Ameer Haj Ali. Tenset: A large-scale
program performance dataset for learned tensor compilers. In Thirty-
fifth Conference on Neural Information Processing Systems Datasets and
Benchmarks Track (Round 1) , 2021.
[70] Size Zheng, Yun Liang, Shuo Wang, Renze Chen, and Kaiwen Sheng.
Flextensor: An automatic schedule exploration and optimization frame-
work for tensor computation on heterogeneous system. In Proceedings
of the Twenty-Fifth International Conference on Architectural Support
for Programming Languages and Operating Systems , pages 859–873,
2020.
12
