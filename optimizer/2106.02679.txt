# 2106.02679.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/optimizer/2106.02679.pdf
# File size: 386239 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2106.02679v1  [cs.LG]  4 Jun 2021Layered gradient accumulation and modular pipeline parall elism:
fast and eﬃcient training of large language models
Joel Lamy-Poirier∗
June 8, 2021
Abstract
The adventofthe transformer has sparkedaquickgrowth inth e size oflanguage models, far outpacing
hardware improvements. (Dense) transformers are expected to reach the trillion-parameter scale in the
near future, for which training requires thousands or even t ens of thousands of GPUs. We investigate
the challenges of training at this scale and beyond on commer cially available hardware. In particular, we
analyse the shortest possible training time for diﬀerent co nﬁgurations of distributed training, leveraging
empirical scaling laws for language models to estimate the o ptimal (critical) batch size. Contrary to
popular belief, we ﬁnd no evidence for a memory wall, and inst ead argue that the real limitation — other
than the cost — lies in the training duration.
In addition to this analysis, we introduce two new methods, layered gradient accumulation andmod-
ular pipeline parallelism , which together cut the shortest training time by half. The m ethods also reduce
data movement, lowering the network requirement to a point w here a fast InﬁniBand connection is not
necessary. This increased network eﬃciency also improve on the methods introduced with the ZeRO
optimizer, reducing the memory usage to a tiny fraction of th e available GPU memory.
1 Introduction
Large scale language models are rapidly changing the ﬁeld of natural language processing. Transformers
[31] have risen as the preferred architecture for language models , being simpler, more scalable and more
performant than alternatives based on recurrent neural netwo rks. Although transformers come in numerous
ﬂavorsandvariations[2,6,8,14,21,22,32, ...], agrowingbodyo fworksuggeststhatthemodelperformance
is mainly driven by scale. Furthermore, few-shot learning capabilities have been observed in GPT-3 (175
billionparameters)[2]. Thisopenstheﬁeldofnaturallanguageproce ssingtoawiderangeofnewapplications
where ﬁne-tuning is either impossible or impractical. These few-shot learning capabilities have not been
observed in smaller models, so they are believed to emerge only at the scale of GPT-3, reinforcing the need
for extremely large language models.
While large language models present exciting new capabilities, they also pose a diﬃcult and costly
engineering challenge. For example, simply storing the training state of GPT-3 takes about 2 terabytes of
memory, and storing its intermediate activations and gradients tak es several more terabytes. This is dozens
oftimes morememorythanavailableonthe largestGPUto date, the 8 0GB NVIDIAA100. Besidesmemory,
training also requires a gigantic amount of computing power. GPT-3 n eeds about 3600 petaﬂop-day to train,
or 30 years on the same device.
To speed up training of large models, it is necessary to parallelize the t raining process by distributing
the load across multiple GPUs. The leading method is 3d parallelism (see f or example [27]), which combines
the three common form of parallelism: data, pipeline and tensor para llelism. Together with methods such
as mixed precision training and activation checkpointing, 3d parallelism allows quickly training large models
with up hundreds of billions of parameters but is not so fast beyond t hat scale. For example, GPT-3 was
trained in a matter of days, while the trillion-parameter version of Me gatron-LM would need more than
three months to train [18]. For larger models, training times are in the order of years or worse.
∗ServiceNow, joel.lamy-poirier@servicenow.com
1

--- PAGE 2 ---
Recently, several memory optimizations have been suggested for large models, aiming in particular to
simplify ﬁne-tuning. When compared to training from scratch, ﬁne- tuning requires much less computational
powerandcanbe donewithalimited numberofGPUs, but doingsocrea tesamemorybottleneck. TheZeRO
family of methods addresses this bottleneck by partitioning the tra ining state [23], aggressively oﬄoading
memory [25, 24], and breaking down individual operations[24]. These m ethods together shatter any memory
constraint.
While a lot of attention has been given to the memory usage, much less eﬀort has been dedicated to
reducing the training time. To that end, we investigate various para llel training conﬁgurations and strategies
for large and dense transformers, with the goal of minimizing the tr aining time on existing hardware. We
explicitly integrate the concept of critical batch size [9, 15, 26] in our analysis, leveraging the empirical
scaling laws found in [12]. The critical batch size provides an upper bo und on the eﬃcient scaling of the
batch size, and by extension dictates how many GPUs can be used fo r training. To our knowledge, our
analysis is the ﬁrst to directly integrate the critical batch size and t he resulting parallelism bounds for large
language models. Our analysis can be applied to a wide range of scales, from the tiniest thousand-parameter
transformers up to the quadrillion parameter scale and beyond.
Our analysis shows that memory is not a limiting factor even past the t rillion-parameter scale. Instead,
we ﬁnd a computational bottleneck caused by the limitations of distr ibuted training. Due to constraints
from the critical batch size and network connectivity, 3d parallelism can eﬃciently use a limited number
of GPUs, and this upper bound does not increase nearly as fast as t he amount of computation needed to
train larger models. We ﬁnd a minimum training time of about two weeks f or a trillion-parameter model,
and this bound scales worse than linearly with respect to the model s ize. As a result, training times are
in the order of months or years above the trillion-parameter scale. We also ﬁnd that the ZeRO family of
methods counter-productivefor the trainingtime, largelybecaus e of frequent data transfersin micro-batched
approaches — including pipeline parallelism — which prevent eﬃcient 3d pa rallelism.
We introduce two closely related methods which signiﬁcantly improve t he training eﬃciency, while also
reducing the memory usage and network requirement. The ﬁrst on e,layered gradient accumulation , uses
a bandwidth-eﬃcient scheduling for gradient accumulation, which ma kes it easier to overlap the gradient
reduction with the computation. It also reconciles gradient accumu lation with the methods introduced in
the ZeRO optimizer, avoiding frequent data transfers when partit ioning the training state or when oﬄoading
it to CPU memory.The second method, modular pipeline parallelism , uses a modular split of the layers to
improve the eﬃciency of pipeline-parallel training by minimizing the pipelin e “bubble”, which otherwise
limits the eﬃciency of pipeline parallelism. It also builds upon layered grad ient accumulation, enabling its
beneﬁts in the pipeline-parallel case. In particular, it allows partition ing the training state in the fastest
3d parallel settings, reducing the memory usage to a minimum and pre venting a trade-oﬀ between memory
usage and the bubble reduction of modular pipeline parallelism. These m ethods together allow training
at least twice as fast as previous methods, for example reducing th e minimum training time to one week
for a trillion-parameter model. While we focus on large transformers , layered gradient accumulation and
modular pipeline parallelism are not speciﬁc to transformers or even la rge models. In fact, the improved
communication overlap is particularly useful for smaller models, espe cially over slower networks.
Concurrent to our work, two papers appeared that show overlap with our results. The latest version of
Megatron-LM [18] suggests a breakdown of the layers similar to mod ular pipeline parallelism, however the
authors suggest a diﬀerent scheduling aimed at reducing the activa tion memory. Our method instead focuses
on a network-eﬃcient method, which leads to an increased eﬃciency beneﬁt from the new layer breakdown
and when combined with a training state partition also leads to a lower m emory usage. In section 8.2,
we investigate disk oﬄoading in a way similar to ZeRO-Inﬁnity [24]. Our m ethods show improved results,
further reducing the requirements for oﬄoading and enabling oﬄoa d even on slow hard drives. However,
we also little use for this extra space, as the memory usage tends to remain reasonable in most scenarios.
Instead, we suggest leveraging the results to improve checkpoint ing methods.
This paper is structured as follow. In section 2 we describe the exist ing approaches and provide some
additional context for the paper. In sections 3 and4 we introduce the main new methods. In section 5, 6
and 7, we analyze the impact of our method on training speed and mem ory usage, then generalize to a wide
range of scales. In section 8 we investigate some additional concer ns that arise in realistic training scenarios.
Finally, in section 9 we discuss the implications and limitations of our resu lts.
2

--- PAGE 3 ---
2 Background and related work
In this section we describe the optimizations and distributed training methods relevant to our analysis.
2.1 Critical batch size
In stochastic gradient descent, the gradient of the loss with resp ect to the model parameters is estimated
from a micro-batch. When the micro-batch is small, adding more samp les improves this estimate and allow
training with a proportionally lower number of steps, keeping the tot al amount of computation unchanged.
However, past a certain point, the estimate becomes accurate en ough and adding more samples no longer
reduces the required number of steps. The critical batch size [9, 15, 26] provides an upper limit for the small
micro-batch regime, and can obtained by estimating the variation (n oise) of the gradients between individual
samples, relative to the true gradients [15].
2.2 Mixed precision training
Deep neural networks do not require a high numerical accuracy, a nd to some extent can be trained with as
little as 16 bits of precision. The state-of-the-art approach in tha t regard is mixed precision training [16],
in which the bulk of the computation is done in half-precision, while the w eights are stored and updated
in single-precision. Half-precision however has a limited exponent ran ge, and the gradients need to be
dynamically scaled to limit the risk of underﬂows and overﬂows. The pr oblem can also be solved with the
bﬂoat16 dataformat, which hasawiderexponent rangeand is availablein rece ntdevicessuch asthe NVIDIA
A100.
2.3 Communication overlap
In additionto the computationitself, datatransferspresentasig niﬁcantoptimizationchallenge. Theyshould
preferably be done in parallel with the bulk of the computation, othe rwise the computational cores stay idle
during the transfer. As the computation and network communicat ions mostly use separate resources, they
can in principle be overlapped with near-perfect eﬃciency, so the runtime is determined by the slo west of
the operations. The operation is said to be compute-bound ordata-bound depending on which one ﬁnishes
last, with the former scenario being preferable here. In practice, there is a small overhead to overlapping the
operations, but we ignore it for the purpose of this paper.
The threshold for a compute-bound operation is described throug h the concept of arithmetic intensity .
For an operation which requires both computation and data transf er, the arithmetic intensity is deﬁned as
the ratio between the amount of computation and data transfer. In the case of perfect overlap, the operation
is compute-bound if the arithmetic intensity is higher than what the h ardware can support. For example, A
NVIDIA A100 has a peak computational power of 312 teraﬂops for half-precision and a memory bandwidth
of 2039 GB/s, for an arithmetic intensity threshold of 143 ﬂops/B. A binary addition with an arithmetic
intensity of 1/6 lies deeply in the memory-bound region, while the multip lication of two 1024x1024 matrices
has an arithmetic intensity of 341 and is compute-bound.
2.4 Distributed training
There are two main forms of parallelism, data parallelism in which each de vice performs the same computa-
tion on a subset of the data, and model parallelism in which the model a nd computation are split between
devices.
Indata parallelism ,theinputbatchissplitbetweenthe nbdevices. Eachdeviceindependentlyprocesses
its assigned input, then the resulting gradients are reduced(summed) between the devices, and ﬁnally each
independently updates all the weights. All the network communicat ion happens in the gradient reduction,
which for the most part can be overlapped with the backward pass, and this overlap can be made compute-
bound with large enough micro-batches. Data parallelism is the most u sed method for parallel training,
because of its simplicity and wide availability. However, as each device n eeds to store a copy of the model,
the memory usage is excessive for larger models. This can be addres sed by partitioning the model, as
suggested in [23]. In this partitioned case, the model is split between the devices, each being responsible
3

--- PAGE 4 ---
for storing and updating an equal share of the weights1. The weight tensors are reconstructed on each
device as needed, and the reduced gradients are kept only as requ ired. The partition increases the network
communication by 50%. Data parallelism scales up to the critical batch sizebc, but in general a minimum
micro-batch size is required for eﬃcient communication overlap, red uces the limit to a fraction of bc.
Model parallelism comes in two main forms, depending on how the model is split. The ﬁrst form is
pipeline parallelism , where each of the nldevices is responsible for a subset of the layers. The input is
pipelined through the devices, and parallel computation is achieved b y feeding micro-batches sequentially
to the model [11]. Each device processes all the micro-batches for a given batch, then updates the weights.
This however leads to a “bubbling” eﬀect in which some of the devices s tay idle due to input starving (see
the upper part of ﬁgure 3)2. For an input split into nµmicro-batches, this bubble increases the training time
by a factornl−1
nµ. Pipeline parallelism is also limited in magnitude, as it cannot grow beyond t he number of
layers in the model.
The other form of model parallelism, tensor parallelism , involves slicing tensors so that each device
is responsible for evaluating a slice of the activations using its corres ponding slice of the weights3. Tensor
parallelism is more complex than the two other forms of parallelism and g enerally depends on the internal
details of the model. It also requires a lot more network communicatio n and is in general only feasible
with the fastest interconnects such as NVLink. In practice this limit s tensor parallelism to 16 GPUs, the
maximum that can be connected with NVLink.
Although data, pipeline and tensor parallelism are individually limited, the y can be combined together
into (up to) 3d parallelism . The dimensions combine in a multiplicative way, for a total of ngpu=nbnlna
devices. In3dparallelism,the input batchsizeissplit bothbetweenth e data-parallelinstances4, andbetween
thenµ≥nlsequential micro-batches. For a batch of size b, this implies a micro-batch size bµ=b/(nbnµ).
Because of the limitation from the critical batch size (see section 2) , this implies a competition between
data and pipeline parallelism, but the combined method generally allows a higher degree of parallelism than
with either method in isolation. This is because pipeline parallelism increas es the arithmetic intensity for
the gradient reduction, reducing the minimum micro-batch size5. However, pipeline parallelism provides no
such beneﬁt with a partitioned training state, where the network o perations need to be repeated for each
micro-batch.
2.5 Memory optimizations
As the model grows in size, so does its memory usage, and substant ial memory optimizations are needed to
avoid running out of memory. The bulk of the memory usage falls in two categories: the training state and
the activation memory. The training state consists of the model parameters and the optimizer state, which
for Adam includes the running average and variance of the paramet ers. For the purpose of this paper, the
parameter gradients are also included in the training state. The activation memory consists of the layer
activations and their gradients. Both categories can use substan tial amounts of memory for larger models,
but various techniques have been developed to reduce memory con sumption.
The training state is proportional to the model size. At 80 GB, a NVI DIA A100 can store at most 20
billion parameters in single precision, or 6.7 billions with ADAM. As the train ing size is ﬁxed, additional
memory is needed to ﬁt larger models, and distributed training provid es such memory. As described in
1In this paper we consider a partition of the whole training st ate, which in the terminology of [23] corresponds to the thir d
stage of ZeRO-DP.
2The network bubble can be avoided with asynchronous training [19, 17], where the weight updates are interleaved with the
computation. However, this leads to gradients being comput ed onstale(outdated) weights, which can be harmful to training.
Although training is technically done with sequential batc hes rather than micro-batches, staleness adds to the gradient noise
and reduces the critical batch size proportionally to the de lay [7, 10, 29]. Consequently, the limitations of 3d paralle lism (see
below) are the same as with synchronous training .
3The method is often referred to as “model parallelism” in the literature, but the term may also refer to pipeline parallel ism,
so heretensor parallelism is used instead, and the more general term “model parallelis m” is reserved for the combination of
both methods.
4The term instance is used to designate a slice of the cluster i n a speciﬁc parallelism dimension. For example, a data paral lel
instance refers to the nlnadevices handling the same sequence of micro-batches.
5Gradient compression methods such as 1-bit Adam [30] also reduce the network communication, potentially pr oviding the
same beneﬁt without pipeline parallelism. However, compre ssion methods cannot eﬃciently be used with a partitioned tr aining
state. Together with the absence of pipeline parallelism, t his makes the size of the training state diﬃcult to manage for larger
models.
4

--- PAGE 5 ---
section 2.4, the training state is split by construction in the model-pa rallel directions and can be partitioned
in the data parallel direction. The training state can also be oﬄoaded to CPU memory; however, this
requires additional data transfers and may create a communicatio n bottleneck.
With a training state partition or oﬄoading, the model weights are st ored in another device and must be
restored to the before usage, so additional buﬀers are required . Similarly, the gradients need to be created
on a buﬀer before being moved to the appropriate place. While these parameter and gradient buﬀers are
small compared to the whole training state, they dominate the GPU m emory usage for larger models once
all the optimizations are taken into account. These buﬀers are to s ome extent also required in the absence
of partition or oﬄoading, as for example the parameters need to be converted to 16 bits before usage.
Activation memory is also a concern, and naive implementations can ea sily require hundreds of gigabytes
even at the billion-parameter scale. The most signiﬁcant reduction in activation memory is obtained with
activation checkpointing , in which the bulk of the activations are dropped during the forward pass [3]6.
A subset of the activations is kept as checkpoints and are used to recalculate the remaining activations
during the backward pass. This lowers the activation memory by a sig niﬁcant factor, reducing it to the
layer activations , i.e., the activation memory required between the activation checkp oint, and the activation
checkpoints themselves, which can in many cases be oﬄoaded to CPU memory. The method comes at the
cost of a 33% increase in computation, but there is no alternative fo r larger models.
As the activation memory is proportional to the micro-batch size, a n obvious memory optimization
is to lower it, down to a single sample if possible. While single-sample micro- batches are typically not
recommended, they can run eﬃciently for larger models for which th e computation kernels are big enough.
However small micro-batches come with a lower arithmetic intensity w ith respect to the model weights,
potentially creating a bottleneck in the case of data parallelism or mem ory oﬄoad. The layered gradient
accumulation andmodular pipeline parallelism methods introduced in this paper are designed to prevent
such bottlenecks, and to allow running eﬃciently with a micro-batch s ize of one.
On a side note, gradient accumulation allowsincreasing the batch size without running out of memory, by
processing multiple micro-batches sequentially between weight upda tes. This is especially useful with data
parallelism over a slow network, as larger batches reduce the frequ ency of the gradient reduction. However,
this leads to ineﬃcient communication overlap as the gradient reduct ion is concentrated in the last micro-
batch. The method is also counter-productive for optimizing the tr aining time, since the micro-batches are
processed sequentially instead of in parallel. The layered gradient accumulation is designed to improve the
communication overlap, while pipeline parallelism allows processing the mic ro-batches in parallel.
In addition to the training state and activation memory, there can b e some additional memory usage or
hidden memory costs. Manylibrariesfor distributed training, includin g Pytorch DistributedDataParallel , use
a bucketing strategy for network operations, combining several tensors in a temporary network buﬀer . This
is preferable for small tensors, as it avoids the overhead or runnin g the individual operations sequentially,
but comes at the cost of additional memory usage and data moveme nt. In ZeRO-R it is suggested to use a
constant size buﬀer to keep the memory overhead bounded [23]. Ho wever, in the present case, the parameter
and gradient buﬀers already play a role similar to the buckets, and in-place network operations can be done
at no extra memory cost.
Even when enough memory is available, memory fragmentation can still cause the device to run out
of memory. For larger models, memory fragmentation becomes a pr oblem because the allocations involve
very large contiguous chunks of memory, and there is an overlap be tween short-lived activations and their
gradients, and longer-lived tensors such as the activation checkp oints and parameter gradients. A solution
to this is to pre-allocate contiguous buﬀers for the memory whenev er possible. Pre-allocation also allows
combiningtensorsintoasinglecontiguous(fused)chunkofmemory ,whichnotonlyensuresoptimalallocation
but also allowsrunning data operations on a fused tensor. In [23] it is suggestedto pre-allocatethe activation
checkpoints and parameter gradient. This leaves the layer activat ions and gradients as the main source of
fragmentation, which can still be signiﬁcant because of the size imba lance between the various activations,
whichcreateslargememorygaps. Forexample, weobservedanove rheadofupto40%forasingletransformer
layer in the PyTorch implementation. However, it is possible to reduce this overhead to almost zero with a
memory-eﬃcient implementation, so in this paper we assume the memo ry fragmentation cost to be minimal.
6The method is also known as gradient checkpointing in the literature, even though it does not involve any gradie nt.
5

--- PAGE 6 ---
reduce0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 Compute
Network
(available)
timeStandard gradient accumulation
reduce0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 Compute
Network
(available)Layered gradient accumulation
idle layer 0 layer 1 layer 2 layer 3
Figure 1: Computation and network scheduling example with data par allelism for standard gradient ac-
cumulation (top) and layered gradient accumulation (bottom). The colors represent the diﬀerent layers,
with diﬀerent shades for the forward and backward pass (lengths not to scale), and the numbers indicate the
micro-batch index. The layeredversion reduces the networkrequ irement by spreading the gradientreduction
over most of the backward pass.
3 Layered gradient accumulation
Inlayered gradient accumulation , we split the input into micro-batches exactly as in standard gradien t
accumulation, but we process all the micro-batches for a given laye r before proceeding to the next one.
We take such layers as the intervals between activation checkpoint s, so that we can drop the intermediate
activations between the micro-batches.
The layered gradient accumulation method is advantageous from a d ata perspective. With data par-
allelism, it allows an eﬃcient overlap of the gradient reduction with the b ackward pass, unlike traditional
gradient accumulation which only allows overlapping with the last micro- batch. This is illustrated in ﬁgure
1. With a state partition, layered gradient accumulation greatly red uces the bandwidth requirement by
eliminating redundancies in the parameter restoration and gradient reduction operations, as illustrated in
ﬁgure 2. Similarly, layered gradient accumulation helps with oﬄoading b y reducing the amount of data
movement. All the activation checkpoints must be kept, which in the presence of pipeline parallelism is
already a requirement7, but otherwise may cause an increase in the activation checkpoint m emory.
Note that layered gradient accumulation generally cannot be eﬃcien tly combined with standard pipeline
parallelism. Indeed, a given pipeline-parallel instance must process e very micro-batch for all every layer
other than the last before it can pass an output to the next instan ce. This is addressed with the modular
pipeline parallelism.
4 Modular pipeline parallelism
In pipeline parallelism, the layers are generally split into contiguous chu nks. For a network with dllayers
split into nldevices, ﬁrst instance gets the layers 1 to dl/nl, the second gets the layers dl/nl+1 to 2dl/nl,
etc. However, while this “naive” splitting minimizes pipeline-parallel net work operations, it also maximizes
the bubbling eﬀect. In modular pipeline parallelism, the layers are inste ad split in a modular fashion, so
that the ﬁrst instance gets the layers 1, nl+ 1, etc., the second gets the layers 2, nl+ 2, etc., and so on.
The computation is scheduled as with layered gradient accumulation, i.e., a given instance processes all
micro-batches for a given layer, then goes on to the next layer for which the input should be ready, etc.
In the modular formulation, a micro-batch reaches the last instanc e after being processed on nl−1 layers
rather than dl(1−1/nl), reducing the bubbling overhead by a factor dl/nl. This makes it possible to reduce
the bubbling to almost zero without increasing the number of micro-b atchesnµ. Additionally, the gradient
reduction is spread more evenly over the backward pass, dividing th e network overhead by a factor dl/nl.
7Some approaches perform better with a memory-eﬃcient sched uling [17, 18], but in any case the activation checkpoints do
not grow too big.
6

--- PAGE 7 ---
0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 Compute
Network
(available)
timeStandard gradient accumulation
restore restore restore restore reduce restore reduce restore reduc e restore reduce0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 Compute
Network
(available)Layered gradient accumulation
idle layer 0 layer 1 layer 2 layer 3
Figure 2: Computation and networkscheduling example with state pa rtition or oﬄoad, for standardgradient
accumulation (top) and layered gradient accumulation (bottom). A mixed buﬀering method is assumed (see
appendix C.2). The standard version involves frequent context sw itches with respect to the weights, which
leads to unreasonable bandwidth requirements. The layered versio n on the other hand maximizes the reuse
of the restored weights, so requires the same bandwidth as withou t gradient accumulation.
Modularpipeline parallelismcomeswith anincreasedpipeline parallelnetw orkcost sincedataneeds to be
transferred after each layer, but for large models this cost rema ins far below the data parallel network usage.
This data transfer can be overlapped with computation provided th ere are slightly more micro-batches than
pipeline-parallel instances.
5 Methodology
In the following sections we analyse the resource usage and training time for large language models, focusing
on the impact of the methods introduced in this paper, as well as 3d p arallelism and training state parti-
tioning. We present some example and relevant results, leaving the d etailed computation to the appendix C.
We assume the computation is done with a scalable cluster with up to 16 A100 GPUs per node, supporting
both InﬁniBand and NVLink. See appendix A for additional details on t he hardware.
Model We consider a transformer encoder following the original approach [31], up to computationally
unimportant modiﬁcations. Transformer encoders are for examp le used in the BERTmodel [6] and its
derivatives, and our results generalize straightforwardly to deco der-based models such as the GPT family
[20, 21, 2]. For simplicity, we restrict to the transformer part of th e model, and ignore other components
such as the tokenizer, the embedding layer and the language model head.
A transformer encoder consists of dlidentical layers, each composed of a multi-head attention module
followedbyanon-linearity. Theformerconsistsof daattentionheadsofsize dh, foralayerwidth dm=da×dh,
while the latter consists of a two-layer dense feedforward networ k with intermediate size dI=nI×dm. Each
layer holds pl≈(4+2nI)d2
mparameters, for a total of p≈(4+2nI)d2
m×dlparameters.
For concreteness, we consider the X [x]family of models described in appendix B, which allows extrap-
olation to wide range of scales. The exact conﬁguration however ma kes little diﬀerence, so our results
straightforwardly generalize to most conﬁgurations.
Training We use mixed precision training, and activation checkpoints at the ou tput of each transformer
layer. We overlap communication when possible and oﬄoad the training state and activation checkpoints
when needed (and possible). We use the mixed gradient buﬀering method for the parameter and gradient
buﬀers, as described in appendix C.2. This method is not necessarilyt he most memory eﬃcient but combines
well with the bandwidth-eﬃcient methods introduced here and is suﬃ cient for all practical purposes. The
Adam optimizer is assumed.
Weconsiderdistributedtrainingwithuptothreeparallelismdimensions . Fortensorparallelism,wefollow
the transformer-speciﬁc method proposed in [27]. This approach m inimizes the network communication,
7

--- PAGE 8 ---
reduce0 1 2 3 0 1 2 3reduce0 1 2 3 0 1 2 3reduce0 1 2 3 0 1 2 3reduce0 1 2 3 0 1 2 3
Device 0
Device 1
Device 2
Device 3
timeStandard pipeline
idle layers 0-3 layers 4-7 layers 8-11 layers 12-15
reduce0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 3 2 1 0 3 2 1 0 3 2 1 0 3 2 1 0reduce0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 3 2 1 0 3 2 1 0 3 2 1 0 3 2 1 0reduce0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 3 2 1 0 3 2 1 0 3 2 1 0 3 2 1 0reduce0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 3 2 1 0 3 2 1 0 3 2 1 0 3 2 1 0
Device 0
Device 1
Device 2
Device 3Modular pipeline
idle layers 0-3 layers 4-7 layers 8-11 layers 12-15
Figure 3: Computation and network scheduling example with standar d and modular pipelines (no state
partition). The modular version signiﬁcantly reduces the idle time fro m the bubbling eﬀect and makes the
gradient reduction much easier to overlap.
which however cannot be overlapped with computation.
We investigate three training strategies. In the baseline, we use standard data and pipeline parallelism
(without partition), as described in section 2.4. In the partitioned approach , we also partition the training
state in the data-parallel direction. In the improved approach, we implement layered gradient accumulation
and modular pipeline parallelism. Unless speciﬁed otherwise, we partitio n the training state in the improved
approach, as it is preferable to do so in most cases.
Optimal conﬁguration We select the distributed training conﬁguration as follow, with the go al of opti-
mizing the training speed without wasting too much computational po wer. We train at or slightly below the
critical batch size, as training above it is ineﬃcient. The selection is ba sed on the resource usage described
in appendix C.
In the baseline, we select the highest possible pipeline parallelism (if app licable), as it reduces memory
usage while also increasing eﬃciency due to the faster gradient redu ction. We select a micro-batch count
slightly above nlto ensure overlap of the pipeline-parallel computation and the smalle st micro-batch size
that does not cause a communication bottleneck. With pipeline paralle lism, we impose a maximum overhead
of 25% from the gradient reduction, although it is only constraining f or smaller models and slow networks.
When oﬄoading is needed, the micro-batch size is also constrained by the CPU-GPU transfer rate, and there
may be an additional bottleneck in the PCI-express conection which is shared between CPU and Inﬁniband
transfers.
In the partitioned approach, we do not consider pipeline parallelism as it leads to worse results. Instead,
we maximize the data parallelism degree nb. Due to the increased network communication, the gradient
reduction is more constraining on the micro-batch size than in the ba seline approach, but oﬄoading is in
general not constraining at all due to the small size of the oﬄoaded state.
In the improved approach, we can set the micro-batch size to one, and avoid a communication bottleneck
with a high enough count nµ. We maximize nb, thennlunder these considerations, so that the bubbling
overhead is minimized. It is in general preferable not to overlap the p ipeline-parallel communication, as nµ
andnlare both small and rounding up to a single extra micro-batch would sig niﬁcantly reduce the training
speed.
8

--- PAGE 9 ---
Table 6.1: Fastest training conﬁguration for X 160for selected training methods
Parallelism Method Oﬄoad b b µnµngpunbnlnaEﬃciency Time
None Baseline ✓2416 4 604 1 1 1 1 1.00 630 y
Data Baseline ✓2415 5 1 483 483 1 1 1.00 1.3 y
Data Partitioned ✓2415 5 1 483 483 1 1 1.00 1.3 y
Data + pipe Baseline ✓2412 4 201 480 3 160 1 0.56 2.4 y
Data + pipe Improved ✗2415 1 5 2415 483 5 1 0.94 100 d
Data + tensor Baseline ✓2415 5 1 7728 483 1 16 0.93 32 d
Data + tensor Partitioned ✗2415 5 1 7728 483 1 16 0.93 32 d
3d Baseline ✗2408 1 172 35840 14 160 16 0.48 13 d
3d Improved ✗2415 1 5 38640 483 5 16 0.88 6.8 d
Table 6.2: Memory usage breakdown for the same training conﬁgura tions.
Parallelism Method State Checkpoint Buﬀers Activations Oﬄ oadable Non-oﬄoadable
None Baseline 14.1 K 47.2 K 43.9 24.9 61.2 K 68.8
Data Baseline 14.1 K 97.7 43.9 31.1 14.2 K 75.1
Data Partitioned 29.1 97.7 43.9 31.1 127 75.1
Data + pipe Baseline 87.9 98.1 43.9 24.9 186 68.8
Data + pipe Improved 5.82 19.5 43.9 6.23 25.4 50.2
Data + tensor Baseline 879 6.10 2.75 1.95 885 4.69
Data + tensor Partitioned 1.82 6.10 2.75 1.95 7.92 4.69
3d Baseline 5.49 1.31 2.75 0.389 6.81 3.14
3d Improved 0.364 1.22 2.75 0.389 1.58 3.14
In all cases, tensor parallelism allows a near-perfect split of both th e memory and computation. We
impose a maximum overhead of 25%, which for large models (above ∼50 billion parameters) allows the
practical limit na= 16. At extreme scales ( 25 trillion parameters) it becomes possible t o eﬃciently use
tensor parallelism over InﬁniBand with na>16.
Resource usage The computation, memory and bandwidth requirements are evaluat ed in appendix C.
To obtain a training time estimate, we compare the available and requir ed computational power, taking
into account the measurable overheads from the pipeline bubble and non-overlapped data transfers. We do
not however consider the eﬃciency of the computational kernels, the data transfers and the communication
overlap, so the training times are likely underestimated. On the othe r hand, the memory usage is expected
to be accurate for the given training conﬁgurations.
6 Trillion parameter example
To investigate the requirements for training at the trillion-paramet er scale, we consider the 1.26 trillion
parameter model X 160. This model consists of 160 transformer layers with 80 heads of siz e 320, for a width
of 25600. The sequence length is 2560, and the critical batch size is estimated to be around 2420. Training
for 100 k steps requires 6 .24×1024ﬂoating point operations, or 72 exaﬂop/s ·day, which corresponds to 231
k GPU-days on A100s at perfect eﬃciency.
Although the computational requirement strongly hints that a larg e cluster is necessary, for comparison
purpose we investigate various distributed training scenarios, with the training conﬁguration selected as
follow. The resulting conﬁgurations are summarized in table 6.1, toge ther with the expected computational
eﬃciencyandtrainingtime. Weﬁndbothdataandtensorparallelismto benecessary(andtogethersuﬃcient)
to train in a reasonable amount of time. However, modular pipeline par allelism stands out with both a high
GPU count and near-optimal eﬃciency, allowing to train the model in a week. It also out-performs the
baseline in the absence of tensor parallelism, but the training time rem ains above three months.
A breakdown of the memory usage for each conﬁguration is shown in table 6.2. All methods are possible
from a memory perspective, assuming a minimal memory fragmentat ion overhead. However, the baseline
9

--- PAGE 10 ---
Table 6.3: Selected training conﬁguration for X 160for the speciﬁed training times of one and six months
Parallelism Method b n angpuOﬄoadable Non-oﬄoadable Eﬃciency Time
Data + tensor Partitioned 2415 16 7728 7.92 4.69 0.93 32 d
3d Baseline 2416 16 10240 10.1 3.14 0.73 31 d
3d Improved 2220 4 7400 7.76 12.5 0.97 32 d
Data + tensor Partitioned 1660 8 1328 35.0 9.38 0.97 180 d
Pipe + tensor Baseline 2416 8 1280 47.9 6.27 0.91 199 d
3d Improved 792 2 1320 22.4 25.1 0.97 180 d
Data + pipe Improved 1572 1 1310 34.2 50.2 0.98 180 d
3d Improved 102 16 1360 11.8 3.14 0.91 186 d
approach requires an impractical amount of oﬄoaded memory witho ut pipeline parallelism. The improved
method has the lowest memory footprint of 4.72 GB, which is 17 times le ss than the memory available in an
80 GB A100.
Smaller clusters Although training scales to nearly 40000 GPUs, it may be diﬃcult to ﬁnd a cluster of
that size, so there is a strong case for training with fewer GPUs ove r a longer period. There are a variety
of viable strategies for smaller clusters, allowing trade-oﬀs betwee n eﬃciency, memory usage, and the choice
of parallelism methods. In table 6.3 we provide some example conﬁgura tions with high eﬃciency for the
target training times of one and six months. With these time constra ints, it is possible to train with clusters
of sizes 7400 and 1300 respectively, which makes training available to a marginally wider community. The
memory usage is increased when compared with the larger clusters b ut remains far below what is available.
Among the methods considered, ours is the most eﬃcient, although the margin becomes negligible for longer
training times. It is also far more ﬂexible, as shown for example in the la st two entries. For the six-month
training it is the only one able to train without tensor parallelism (with oﬄ oad). It is also able to train with
a much lower batch size, which amounts to an extra eﬃciency gain as t here is an implicit cost to training
with a high batch size.
7 Scaling analysis and practical limits
We analyze how the memory usage and training time scale with the mode l size, using the X [x] model family
described in appendix C.2. We consider the same three training strat egies as before — baseline, partitioned
and improved — but restrict to the fastest conﬁguration for each .
Figure 4 shows the memory usage and training time as a function of th e model size at various scales. The
improved method outperforms the others at most scales, althoug h it becomes identical to the partitioned
approach above the quadrillion parameter scale, as pipeline parallelism is no longer necessary.
Focusing on the improved method, we ﬁnd that 80 GB is enough to tra in models up to 280 trillion
parameters, with oﬄoading being required above 90 trillion. However , such model would take up to four
years to train (and more when taking the real computation eﬃcienc y into account), which is unreasonably
long as for instance waiting for the next generation of hardware wo uld likely be faster. To obtain better
scaling limits, we consider a reasonable threshold of one month, and a more generous threshold of one year.
These thresholds respectively result in limits of about 4.5 and 50 trillion parameters, with total memory
usages of 13 and 62 GB.
The above results show a strong limitation on the model size due to th e training time. It may however
be possible to do better with existing GPUs. While data and pipeline para llelism are fundamentally limited
by the critical batch size, tensor parallelism is limited by a computer de sign choice which limits the node
size to 16. The fully connected NVSwitch topology in DGX and HGX node s is convenient in the general
case, but tensor parallelism only requires a ring topology, which is eas y to scale to an arbitrary size. This
means larger nodes (or separate nodes connected with NVLink) sh ould be possible, although it may require
a certain amount of engineering. For this reason, we consider the s cenario where the node size limitation is
removed, with the results being shown in ﬁgure 5. In this case, ther e is enough memory for models up to 100
10

--- PAGE 11 ---
1010101210141016
Parameters10−1100101102Non-offloadable  (GB)
2d Partiti ned
3d Baseline / Impr ved
80 GB (A100)
1010101210141016
Parameters10−1100101102103Offl adable (GB)
3d Baseline
2d Partiti ned
3d Impr ved
80 GB (A100)
1 TB ( ffl ad)
1010101210141016
Parameters10−210−1100101102103104105Training time (days)
3d Baseline
2d Partiti ned
3d Impr ved
One m nth
One year
Figure 4: Approximate minimal memory usage and training time for sele cted training methods, with a
maximum node size of 16.
1010101210141016
Parameters10−1100101102Non-offloadable (GB)
2d Partitioned
3d Baseline / Improved
80 GB (A100)
1010101210141016
Parameters10−1100101102103Offloadable (GB)
3d Baseline
2d Partitioned
3d Im roved
80 GB (A100)
1 TB (offload)
1010101210141016
Parameters10−210−1100101102103104Training time (days)
3d Baseline
2d Partitioned
3d Im roved
One month
One year
Figure 5: Approximate minimal memory usage and training time for sele cted training methods.
quadrillion parameters, while the training time reduces the limit to 40 tr illion (one month) or 900 trillion
parameters (one year).
There is no memory wall While memory usage is not a problem until astronomical scales, we can go
further and show memory is nevera problem. For this, we suppose at some point in the future it become s
possible to train a given model in a ﬁxed time of one month, either using faster devices or faster network
connections. Then we measure how much memory would be needed, in relation to the computing power.
For simplicity, we assume that the conﬁguration still uses as much da ta and pipeline parallelism as possible,
and scales only with tensor parallelism. The results are shown in ﬁgure 6, which shows that the memory
requirement decreases with the model size. In fact, memory is only an issue at smaller scales, which may in
part explain the perceived memory problem, but it is already possible t o train those models much faster with
a minimal amount of memory (dotted line). These results are not par ticularly surprising, since the highest
memory scaling comes from the state which is proportional to the mo del size, while the computation has a
worse scaling due to the increased input size.
11

--- PAGE 12 ---
1091010101110121013101410151016
Parameters10−410−310−210−1100Non-offloadable (GB/tflops)
2d Partitioned
3d Baseline / Improved
A100 80 GB
1091010101110121013101410151016
Parameters10−410−310−210−1100Offloadable (GB/tflops)
3d Baseline
2d Pa titioned
3d Imp oved
A100 80 GB
Figure 6: Required memory to compute ratio for training a transfor mer in one month, as a function of the
model size. The dotted line shows a faster training scheme which is alr eady possible on A100s.
8 Additional training considerations
While we assumed so far the existence of a suﬃciently large supercom puter, in practice this may be diﬃcult
to achieve. Traininga large languagemodel is very expensive, and se tting up such supercomputer exclusively
for this purpose signiﬁcantlyincreasesthe cost. For this reason, it is preferableto leveragedata centers where
the nodes can quickly switch to and from other workloads so that th ey are used without interruption. In this
scenario, the number of available nodes may vary during training, fo r example if higher priority workloads
need to be run or some nodes need to be removed for maintenance. This means training should preferably
beelastic, i.e., it should support variations in the cluster size. Even in a ﬁxed clus ter, the risk of hardware
failure is high due to the large number of components, and handling th ese failures requires some form of
elasticity. While a complete analysis of elastic training is outside the sco pe of this paper, in this section we
investigate some concerns and potential optimizations related to t he optimizations presented in this paper.
We assume the nodes are located in the same data center and prefe rably connected through InﬁniBand,
although in section 8.3 we investigate the eﬀect of training over a slow er Ethernet connection. We also
assume the various parallelism and network bounds presented in this paper are respected, which can be done
by scaling the data parallelism degree down from the maximum allowed va lue. When the state is partitioned,
this presents additional challenges since the partitioning changes d uring training, but we show in section 8.3
that partitioning actually helpswith elasticity. The varying partition may also increase the memory us age,
but there is plenty of room for it.
8.1 Don’t decay the learning rate, increase the cluster size
The analysis so far wrongly assumed that the critical batch size is co nstant during training. During early
training the gradients contain a strong model-improving signal, but a s the model is trained this signal
becomes less important relative to the noise, which increases the nu mber of samples required to obtain an
accurate estimate, i.e., the critical batch size. The “unique” critica l batch size considered so far corresponds
to the value during late training, and using it implies a wasteful training above the critical batch size during
training. Instead, the critical batch size can be evaluated dynamic ally during training, and the batch size
adjusted accordingly [15, 28]. In the present context, this means the maximum cluster size varies during
training, and when elastic training is possible it should be adjusted dyn amically. Doing so reduces the cost
of training without signiﬁcantly aﬀecting the training time.
8.2 Oﬄoading revisited: real-time checkpoints
In the previous sections we found oﬄoading to be possible, but rare ly necessary from a memory perspective.
Here we take another look at oﬄoading, this time with the objective o f keeping a copy of the weights in a
12

--- PAGE 13 ---
1010101210141016
Parameters103104105106107108Arithmetic intensity (flop/B )
1010101210141016
Parameters10−21021100101102103Minimum ban 0i −h (GB/s)
State, Baseline
State, Partitioned/Improved
Checkpoint offload
CPU-GPU limit
Disk (NVMe)
Disk (Hard drive)
Ethernet (25 Gb /s)
Figure 7: Required memory to compute ratio for training a transfor mer in one month, as a function of the
model size. The dotted line shows a faster training scheme which is alr eady possible on A100s.
secure and accessible location, a practice also known as “saving a ch eckpoint”. Saving checkpoints typically
takes a long time during which the cluster is idle, often taking longer th an training multiple batches. This
means checkpoints cannot be saved often, and failures lead to a sig niﬁcant loss of progress. There is also a
signiﬁcant downtime in elastic training, since a variation of the cluster means a checkpoint must be saved,
then loaded in the new nodes. This downtime is problematic when runnin g in a data center with a lot of
activity, where nodes are added and removed frequently.
Figure 7 shows the arithmetic intensity and bandwidth requirement f or oﬄoading the model. In the
partitioned case, we ﬁnd that the state can not only be oﬄoaded to CPU, but it can also easily be oﬄoaded
on fast SSDs (as suggested in [24]), possibly on a remote location via E thernet, and for larger models even
hard drives are fast enough. This allows keeping an up-to-date cop y of the weights in a secure external
location accessible by all nodes at a negligible cost, which reduces the potential cost of failure to a single
batch8. The overhead of modifying the cluster size can also be reduced to a lmost zero by loading the weights
on the ﬂy, even if a new partition needs to be made.
We can gofurther by consideringactivation checkpoints, which as ﬁ gure7 shows need a higher bandwidth
than the state, but forthe largermodelscanalsobe savedtoa fas tremotestorage. Thisreducesthe potential
loss from a crash to a single layer and allows swapping nodes in the middle of a batch at nearly zero cost.
8.3 Ethernet is enough
Asdatacentersmaynotbe equipped with afast InﬁniBandconnect ion, weevaluatethepossibilityoftraining
over Ethernet. We assume the nodes are equipped with a 400 Gb/s E thernet connection, which amounts to
25 Gb/s per GPU. The analysis is shown in ﬁgure 8. For larger models, t he slower connection makes little
diﬀerence, provided pipeline parallelism is used. However, for smaller m odels, a higher degree of pipeline
parallelism is required to reduce the network usage, which makes it ha rder to mitigate the bubble in the
improved case. For the trillion-parameter model, this slows down tra ining by about 4%, but the eﬀect is
more important at smaller scales. Despite this and the communication overhead from the state partition,
the improved method outperforms the baseline at smaller scales bec ause of the improved communication
overlap. For the smallest models, the partition can be avoided to fur ther reduce the training time at a
minimal memory cost (dotted line).
8This can also be done in the non-partitioned by sharing the lo ad of saving the checkpoint across the data-parallel instan ces,
but all nodes still need to load the entire checkpoint.
13

--- PAGE 14 ---
1010101210141016
Parameters10−1100101102Non-offloadable  (GB)
2d Partiti ned
3d Baseline / Impr ved
Infiniband
80 GB (A100)
1010101210141016
Parameters10−1100101102103Offl adable (GB)3d Baseline
2d Partiti ned
3d Impr ved
N  partiti n
Infiniband
80 GB (A100)
1 TB ( ffl ad)
1010101210141016
Parameters10−210−1100101102103104105Training time (days)3d Baseline
2d Partiti ned
3d Impr ved
N  partiti n
Infiniband
One m nth
One year
Figure 8: Approximate minimal memory usage and training time for sele cted training methods, with a 25
Gb/s Ethernet connection.
9 Conclusions and future work
We showed that 3d parallelism is necessary for training large language models, although it remains limited
by fundamental and practical bounds aﬀecting the training time. W ith a combination of layered gradient
accumulation, modular pipeline parallelism and training state partition, it is possible to nearly achieve these
bounds while using only a small fraction of the available memory. Given a suﬃciently large cluster, this
allow eﬃciently training models above the trillion-parameter scale in a re asonable time.
Our approach is also not particularly demanding on the (inter-node) network, being able to perform
relatively well with only an Ethernet connection. It also helps with the training of smaller models, which
beneﬁt from the improved communication overlap. We expect our me thods to allow training models such
as BERT noticeably faster than with existing state-of-the-art me thods, in a matter of minutes. However,
further research would be needed to make accurate predictions f or models of that size, as there may be an
additional overhead due the small size of the computational kerne ls.
9.1 Fine-tuning and inference
The recent interest in extremely large models has been in large part f uelled by their potential to learn
new tasks on the ﬂy, potentially eliminating the need for ﬁne-tuning. When ﬁne-tuning is still needed, it
requires much less computing power than training from scratch, bu t the computational challenge remains to
some extent. For example, ﬁne-tuning a trillion-parameter model f or 1000 steps requires over 2000 GPU-
days, which remains too high for many researchers. Fine-tuning ca n be done on smaller clusters than those
considered in this paper, which may bring back the need for oﬄoading the training state. In that regard,
our approach improves over existing method such as the ZeRO family . The reduced need for data transfers
allows for an easier oﬄoad, while also reducing the activation memory a nd the network requirement. Note
that ﬁne-tuning models above the trillion-parameter scale brings ba ck the need for 3d parallelism and large
clusters, assuming a pre-trained model is available to begin with.
Inference represents an additional challenge, because it also nee ds to be optimized for low latency. Data
andpipelineparallelismdonothelpinthatregard,andtensorparallelism islimitedbythenodesize. Thissets
alowerboundofseveralsecondspertrillionparameters,dependin gonthesequencelengthandcomputational
precision. Consequently, the largest language models may not be su itable for real-time applications until
with current hardware. As with ﬁne-tuning, oﬄoading may also be ne cessary for inference, which can run
with a minimal number of devices.
14

--- PAGE 15 ---
9.2 Computational wall
Beyond the trillion-parameter scale, our results show an increasing ly high lower bound on the training time
which makes it impossible to train arbitrarily large models. For example, a 50 trillion parameter model
takes at least a year to train, while a quadrillion parameter model wou ld need decades. As hardware speed
is currently increasing at a much slower pace than the model size (an d the computational requirement), this
computational wall will not be addressed by hardware speed alone. Instead, a dedicated eﬀort is needed
to either reduce the computational requirement or provide more c omputational power through distributed
training improvements.
Sparse models There is a signiﬁcant research eﬀort dedicated to reducing the com putational requirement
of language models through sparsity, with some promising results. F or example, mixture of experts methods
have been shown to improve the model performance for a ﬁxed com putational budget [8, 13], while sparse
matrix multiplications enable training on much larger sequences [1, 4, 33]. While we leave a det ailed analysis
for future work, we expect sparsity to increase the network and memory usage due to the reduced arithmetic
intensity, which should not be problematic unless pushed to the extr eme.
Hardware implications Recent trends in hardware have focused on large GPUs with as much memory
as possible. This was motivated in good part by the requirements of s maller models as well as simplicity
considerations, as large devices reduce the need for complex para llelism methods and aggressive memory
optimizations. However, this focus on size increases costs, and sp lits the limited memory bandwidth between
large numbers of computational cores, causing memory bottlenec ks.
Thesituationisdiﬀerentforlargermodels,forwhichsimplicityisnotan option. Inthiscase3dparallelism
becomes necessary, and the unavoidablememory optimizations brin g down the memory usage to a minimum.
Instead, the real challenge for large models lies in the cost and dura tion of training. As emphasised by our
results, the key to a faster training is in large scale tensor parallelism , which needs many GPUs to be
connected through a high bandwidth, low latency interconnect. Th e size of the GPUs is not particularly
important, and in fact, (a lot of) smaller devices may be preferable a s they are less aﬀected by bandwidth
limitations. The GPUs only need a small amount of fast memory, as long as there is a larger amount of
external storage for ﬁne-tuning and inference.
9.3 Scaling concerns
While we focused on howto train a very large transformer, we did not attempt to determine whether
oneshoulddo it, or even can aﬀord it. Training models at the trillion-parameter s cale requires thousands
or preferably tens of thousands of GPUs, which implies astronomica l costs far beyond the means of most
researchers. Training at this scale also has a signiﬁcant environmen tal impact, both from the electricity
usage and the amount of hardware being used.
In addition to the cost, language models are challenging from an ethic al standpoint, and the scale of
the model does not help on that regard (see for example [2]). Lang uage models tend to be highly biased
and unfair, a behavior learned from the training data. Due to their p erformance, they can also be used to
impersonate real humans, enabling a wide range of unethical applica tions. While it is impossible to prevent
such misuse in the long term, we hope that large language models beco me more accessible to the research
community in the near future, so that mitigating solutions can be fou nd as soon as possible.
Acknowledgements
We would like to thank Harm de Vries for providing extensive support w hile writing the paper, Eric Robert
and Simon B´ elanger for supporting the research project, and Na than Schucher for providing additional
feedback.
15

--- PAGE 16 ---
References
[1] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer : The long-document transformer, 2020.
[2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, S andhini Agarwal, Ariel Herbert-
Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Rames h, Daniel M. Ziegler, Jeﬀrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateus z Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radf ord, Ilya Sutskever, and Dario
Amodei. Language models are few-shot learners, 2020.
[3] Tianqi Chen, Bing Xu, Chiyuan Zhang, and CarlosGuestrin. Trainin gdeep nets with sublinear memory
cost, 2016.
[4] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Gen erating long sequences with sparse
transformers, 2019.
[5] NVIDIA Corporation. https://www.nvidia.com/en-us/data-center/ . Accessed: 2020-04-01.
[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutano va. Bert: Pre-training of deep
bidirectional transformers for language understanding, 2019.
[7] John C. Duchi, Sorathan Chaturapruek, and Christopher R´ e. Asynchronous stochastic convex opti-
mization, 2015.
[8] William Fedus, Barret Zoph, and Noam Shazeer. Switch transform ers: Scaling to trillion parameter
models with simple and eﬃcient sparsity, 2021.
[9] Noah Golmant, Nikita Vemuri, Zhewei Yao, Vladimir Feinberg, Amir Gh olami, Kai Rothauge,
Michael W. Mahoney, and Joseph Gonzalez. On the computational in eﬃciency of large batch sizes
for stochastic gradient descent, 2018.
[10] Robert Hannah and Wotao Yin. On unbounded delays in asynchro nous parallel ﬁxed-point algorithms,
2017.
[11] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia X u Chen, Dehao Chen, HyoukJoong
Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. Gpipe : Eﬃcient training of giant
neural networks using pipeline parallelism, 2019.
[12] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Ben jamin Chess, Rewon Child, Scott
Gray, Alec Radford, Jeﬀrey Wu, and Dario Amodei. Scaling laws for ne ural language models, 2020.
[13] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim
Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant mod els with conditional computation
and automatic sharding, 2020.
[14] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abde lrahman Mohamed, Omer Levy,
Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence- to-sequence pre-training for natural
language generation, translation, and comprehension, 2019.
[15] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Te am. An empirical model of large-
batch training, 2018.
[16] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamo s, Erich Elsen, David Garcia, Boris
Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision train-
ing, 2018.
[17] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memory-eﬃcient
pipeline-parallel dnn training, 2021.
16

--- PAGE 17 ---
[18] DeepakNarayanan,MohammadShoeybi, JaredCasper,Patric kLeGresley,MostofaPatwary,VijayKor-
thikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, B ryan Catanzaro, Amar Phanishayee,
and Matei Zaharia. Eﬃcient large-scale language model training on g pu clusters, 2021.
[19] Feng Niu, Benjamin Recht, Christopher Re, and Stephen J. Wrig ht. Hogwild!: A lock-free approach to
parallelizing stochastic gradient descent, 2011.
[20] Alec Radford, KarthikNarasimhan, Tim Salimans, and Ilya Sutske ver. Improvinglanguageunderstand-
ing by generative pre-training, 2018.
[21] Alec Radford, Jeﬀ Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models
are unsupervised multitask learners, 2019.
[22] Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Shar an Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learnin g with a uniﬁed text-to-text
transformer, 2020.
[23] Samyam Rajbhandari, Jeﬀ Rasley, Olatunji Ruwase, and Yuxion g He. Zero: Memory optimizations
toward training trillion parameter models, 2020.
[24] Samyam Rajbhandari, Olatunji Ruwase, Jeﬀ Rasley, Shaden Sm ith, and Yuxiong He. Zero-inﬁnity:
Breaking the gpu memory wall for extreme scale deep learning, 2021 .
[25] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunj i Ruwase, Shuangyan Yang, Minjia
Zhang, Dong Li, and Yuxiong He. Zero-oﬄoad: Democratizing billion-s cale model training, 2021.
[26] Christopher J. Shallue, Jaehoon Lee, Joseph Antognini, Jasch a Sohl-Dickstein, Roy Frostig, and
George E. Dahl. Measuring the eﬀects of data parallelism on neural n etwork training, 2019.
[27] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGr esley, Jared Casper, and Bryan Catan-
zaro. Megatron-lm: Training multi-billion parameter language models u sing model parallelism, 2020.
[28] Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V . Le. Don’t decay the learning rate,
increase the batch size, 2018.
[29] Sebastian U. Stich, Amirkeivan Mohtashami, and Martin Jaggi. Cr itical parameters for scalable dis-
tributed learning with large batches and asynchronous updates, 2 021.
[30] Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhan dari, Conglong Li, Xiangru Lian,
Ji Liu, Ce Zhang, and Yuxiong He. 1-bit adam: Communication eﬃcient large-scale training with
adam’s convergence speed, 2021.
[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need, 2017.
[32] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhu tdinov, and Quoc V. Le. Xlnet:
Generalized autoregressive pretraining for language understand ing, 2020.
[33] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ains lie, Chris Alberti, Santiago Ontanon,
Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big b ird: Transformers for
longer sequences, 2021.
A Hardware details
This appendix summarizes the hardware speciﬁcations relevant to t his paper. For more details, see the
oﬃcial documentation [5]
For the purpose of this paper, the computation is assumed to be do ne on NVIDIA A100 80 GB GPUs.
Such device has a theoretical peak fp16 compute cgpu= 312 Tﬂop/s, although this value is diﬃcult to reach
in practice. The device has mgpu= 80 GB of memory, with a peak bandwidth βgpu= 2039 GB/s. Each
17

--- PAGE 18 ---
Table A.1: Bandwidth and arithmetic intensity with respect to a A100 G PU for the available network
interconnects and data storages.
NetworkBandwidth Arithmetic intensity threshold
Input + Output @ 312 Tﬂop/s
GB/s ﬂops/B
GPU memory 2039 143
NVLINK 600 484
PCI-express 63 4.61 k
InﬁniBand (200 Gb/s) 50 5.81 k
CPU-GPU 31.5 9.22 k
Ethernet (25 Gb/s) 6.25 46.5 k
Disk (NVMe) 3.2 90.8 k
Disk (Hard drive) 0.1 2.91 M
GPU is connected to the rest of the computer with a PCIe 4.0 x16 con nector and can be connected to oher
GPUs with up to 12 NVLink interconnects, for a total bandwidth of 3 00 GB/s in each direction. When
combined with NVSwitch, NVLink allows up to 16 GPUs to be fully connect ed, each pairwise connection
being able to fully utilize the NVLink bandwidth.
Although we do not assume a speciﬁc computer architecture, we us e a 16-GPU DGX or HGX A100 is as
a reference9. The GPUs are fully connected through NVSwitch, and each pair is co nnected to a PCI-express
switchwhichalsoconnectstoapairof200Gb/sInﬁniBandinterconn ectsandtooneoftheCPUs. Whilethere
is atotal of16InﬁniBand connectors,eachGPUcan onlyeﬃciently u se oneconnectordue to the PCI-express
bottleneck. The CPUs are bundled with a large amount of memory, an Ethernet connection, additional
optional InﬁniBand connectors, and other standard computer c omponents. The Ethernet connection scales
to at least 400 Gb/s, which amounts to 25 Gb/s per GPU10.
The PCI-express conﬁguration of the HGX server creates a bott leneck on the CPU side, since a single
PCIe 4.0 x16 connects the CPU to two GPUs and two InﬁniBand interc onnects, eﬀectively dividing the
maximum CPU-GPU throughput by half and preventing the CPUs to eﬃ ciently connect with InﬁniBand.
This is problematic in the oﬄoading scenarios where data is transferr ed through the CPUs. The problem
could be ﬁxed with improved computer designs but remains an importa nt practical limitation.
The various bandwidths and arithmetic intensities are summarized in t able A.1.
B Transformer scaling
Transformers are largely insensitive to the exact values of the hyp erparameters [12], assuming they fall
within a reasonable range, so the driving factor when scaling the mod el is computational eﬃciency. The
intermediate size factor nIshould be kept constant, and we use the common value nI= 4. The sequence
length and head size should be proportional to keep the intermediat e activations balanced. The number
of heads can scale independently but has an impact on parallelism and t he allowed values of the sequence
length. In this paper we assume a mild scaling ds∼d1/2
m, which we achieve with the reasonable relative
scalingds= 8dh= 32da. These ratios lead to values comparable to what is found in the literat ure, and
match BERT for da= 16, but diﬀer slightly from GPT-3 which gives more weight to the head count.
The layer count scaling is relevant to parallelism, as deeper networks are suitable to pipeline parallelism,
while wider ones enable more tensor parallelism. The impact is minimal give n that both are limited by
other factors, but wider networks are slightly preferable as they lead to more eﬃcient tensor parallelism.
In terms of memory, deep and thin networks reduce the size of the buﬀers and activations, at the cost of
9As no detailed architecture could be found, we extrapolated some details from other DGX and HGX nodes. At the time of
writing this paper, there does not appear to be a 16 GPU versio n for the DGX A100.
10In a data parallel scenario, network operations use a ring to pology which eﬀectively makes the entire Ethernet bandwidt h
available to a single device. We ignore this fact for simplic ity, and in any case, this does not happen with tensor paralle lism
where all devices communicate with other nodes.
18

--- PAGE 19 ---
Table B.1: X [x]model conﬁguration examples for a wide range of scales and compar ison to some existing
large transformer. Parameter counts exclude the embedding laye r and language model head.
Model p b c(b)dsdadhdmdl
X2 488 130 32 1 4 4 2
BERT 301 M 751 (256) 512 16 64 1024 24
X32 403 M 826 512 16 64 1024 32
Megatron-LM 8.15 B 1130 (512) 1024 32 96 3072 72
X64 12.9 B 1310 1024 32 128 4096 64
T-NLG 17.0 B 1440 (512) 1024 28 152 4256 78
GPT-3 174 B 1560 2048 96 128 12288 96
X108 176 B 1860 1728 54 216 11664 108
X160 1.26 T 2420 2560 80 320 25600 160
X[x]12x5+13x382.0x2/316x1
2x2x x2x
larger activation checkpoints. As parallelism is much more important t han memory, we select a mild scaling
dl=√dm.
The resulting model family X [x]is parametrized by a single variable x:
da=1
2x, d h= 2x, d l=x,
ds= 16x, d m=x2, d I= 4x2. (1)
Table B.1 shows some examples of X [x]models and a comparison to other large language models.
In [12] it was found empirically that the critical batch size scales appr oximately as p1/3, when measured
in tokens. To obtain a numerical value, we assume that GPT-3 was tr ained at the critical batch size (3.2 M
tokens). This results in the empirical formula
bc≈573p1/3
ds≈82.0x2/3. (2)
Althoughequation2isapproximateandwasnotdemonstratedtosc aleforthe wholeparameterrangestudied
in this paper, we take it as the true value of the critical batch size fo r numerical estimations11.
C Resource usage
C.1 Computation
In a transformer, as with nearly all deep learning models, the bulk of the computation is performed in
the matrix multiplications. These appear in the weight multiplications in t he dense layers, and in the self-
attention mechanism, but the self-attention matrix multiplications a re much smaller in general and can be
neglected. For the forward pass, this leads to a computational co st of two ﬂoating point operations for each
input token and parameter, or 2 bdspﬂops per batch. In the backward pass, the parameter and layer g radient
computation each require a similar amount of computation, to which is added the activation re-computation,
for a total of three times the forward pass computation. Summing up, each batch requires 8 bdspﬂops of
computation, or8bdsp
ngpufor each device.
11The dependence on the sequence length was not demonstrated i n [12], and we make the reasonable approximation that the
critical batch size measured in tokens does not depend on the sequence length.
19

--- PAGE 20 ---
Table C.1: Operation sequences and their resource usage for layer buﬀering methods. All values are relative
to the double buﬀered forward pass.
Stream 1 Stream 2 Parameter Gradient Computation Network Arith metic
buﬀers buﬀers Intensity
Forward
Activations( i−1) Restore( i) 2 0 1 1 1
Activations( i) Restore( i+1) 2 0 1 1 1
Backward
Gradients( i−1) Restore( i) 2 1 2 1 2
Activations( i) Reduce( i−1) 1 1 1 1 1
Gradients( i) Restore( i+1) 2 1 2 1 2
Activations( i+1) Reduce( i) 1 1 1 1 1
C.2 Parameter and gradient buﬀering
To determine the memory usage, we need to determine the size and lif etime of the parameter and gradient
buﬀers. Each parameter is used in both the forward and backward pass, so should be restored at least
twice. A convenient choice is to deﬁne a buﬀer for all the parameter s in a layer, i.e., between two activation
checkpoints, and similarly for the gradients. To allow overlappingthe communication, two parameter buﬀers
are needed, but one gradient buﬀer is suﬃcient. This mixed buﬀering method (as opposed to a strict single
or double buﬀering) is summarized in table C.1.
Note that in the absence of gradient accumulation, it is possible to ac hieve a lower memory usage with
buﬀers deﬁned at the sub-layer level, possibly even splitting the par ameter within a single operation as
suggested in [24]. Doing so requires restoring the parameters an ex tra time in the backward pass, but this
can be done without increasing the network requirement by leverag ing the arithmetic intensity imbalance
(see table C.1). However, with gradient accumulation, the network operations need to happen for each
micro-batch, which leads to an excessive network requirement and defeats the purpose of layered gradient
accumulation. In any case, we ﬁnd mixed buﬀering suﬃcient for all re alistic scenarios.
C.3 Memory
As described in section 2.5, the memory usage breaks down into four categories: the training state, the
activation checkpoints, the parameter and gradient buﬀers, and the layer activations. The ﬁrst two can be
oﬄoaded to CPU memory, but not the latter two which ultimately dete rmine how big the model can grow
from a memory perspective.
With the Adam optimiser, the training state consists of the model pa rameters as well as their running
mean and variance, all stored with single precision, for a total of 12 pbytes. The gradients would take an
extra 2pbytes, but we can e this to a negligible amount by updating the weights as soon as possible. In the
non-partitioned case, the state is split across the model-parallel in stances (12p
nlnabytes per device), while in
the partitioned case it is split across all devices (12p
ngpubytes each).
In the mixed buﬀering method, two parameter and one gradient buﬀ ers are needed, each being the size
of a single transformer layer. The buﬀers are split in the tensor par allel dimension for a total of6pl
nabytes
per GPU. Note that while the buﬀers are much smaller than the trainin g state, they do not shrink much
with parallelism so can still be important.
The checkpoints are assumed to match the output of each transf ormer, which works relatively well in
practice, for a total of 2 bdsdmdl. They split naturally over the data and pipeline parallel dimensions and
can also be partitioned in the tensor parallel dimension, for a memory usage of2bdsdmdl
ngpubytes per device.
This formula is not optimal, as for example some intermediate computa tions can be combined together
into fused kernels, but it is suﬃcient for the purpose of this paper a s the activation memory remains low
enough. The activation memory is split between the micro-batches a nd the tensor-parallel instances, for a
total ofbdsm0
nbnµna
20

--- PAGE 21 ---
C.4 Network
3d parallelism involves three kinds of network communication, one for each parallelism dimension. This
section aims at evaluating the bandwidth requirement for each, as w ell as the associated arithmetic intensity.
For the communication to be overlapped perfectly, the arithmetic in tensityνopfor the computation with
respect to the network transfer needs to be higher than the arit hmetic intensity νnetimplied by the GPU
and the network, or
νop≥νnet, (3)
When eﬃcient overlap is not possible, there is a relative overheadνnet
νop. This overhead is expected to remain
within a chosen threshold ǫ, resulting in the condition
ǫ νop≥νnet. (4)
C.4.1 Data parallel
In the non-partitioned case, the gradient reduction involves a sca tter-reduce and an all-gather, which are
both identical from a computationalperspective and aregenerally implemented with bandwidth-optimal ring
methods. In an all-gather for instance, each device receives all th e gradients except for the ones it already
has and sends the same amount of data. For the gradient reductio n, this results in a bandwidth usage of
8(nb−1)p
ngpu≈8p
nlna. The reduction is overlapped with the backward pass for the last mic ro-batch, except for
boundary eﬀects at the ﬁrst and layers. When comparing with the b ackward pass compute of6bdsp
nµnbnlna
resulting in an arithmetic intensity
νbase
b≈3bds
4nbnµ, (5)
assuming dn/nlislargeenough. Thismeansthe overlapgetsworsewith dataparallelis mand micro-batching,
and by extension with pipeline parallelism. In the latter case case this is due to poor communication overlap,
and fordl=nlthere is no overlap at all. Because of that, the non-overlapped sce nario is more appropriate,
with
νpipe
b≈bds
nb. (6)
In the partitioned case, there is an extra all-gather in the forward pass, and the network operations need
to be done for each micro-batch, resulting in3
2nµtimes the network bandwidth when compared with the
non-partitioned case. The lowest arithmetic intensity is in the forwa rd pass, with the value
νbase-part
b≈bds
2nbnµ. (7)
This is only 33% lower than without the partition, but in the micro-batc hed case the overlap is with all the
micro-batches rather than the last one, so the non-overlapped s cenario does not apply.
With layered gradient accumulation, the network usage remains the same, but can be overlapped with
the entire backward pass even in the micro-batched case (again ex cluding boundary eﬀects). This remains
true with pipeline parallelism, in the recommended setting wheredl
nlis not too small. This results in the
arithmetic intensity
νimpr
b≈3bds
4nb, (8)
or
νimpr-part
b≈bds
2nb, (9)
depending on whether the state is partitioned.
21

--- PAGE 22 ---
C.4.2 Pipeline parallel
With pipeline parallelism, each instance needs to receive its inputs and s end its outputs, and the potential
bottleneck is in the forward pass. In the baseline, each micro-batc h the forward pass involves4bdsdm
nbnabytes
of communication and2bdsp
ngpuﬂops of computation, for an arithmetic intensity of
νbase
l≈(2+nI)dmdl
nl. (10)
With modular pipeline parallelism, the number of transfers is multiplied bydl
nl, for an intensity
νimpr
l≈(2+nI)dm, (11)
which is high enough for large models.
The data transfer can be overlapped with the computation, but it is diﬃcult to do so with the minimal
number of micro-batches nµ=nl. This can be addressed by adding a small number of extra micro-bat ches,
approximately given byνl
νnetnµ, or more is the network speed ﬂuctuates.
C.4.3 Tensor parallel
Following the approach of [27], tensor parallelism requires two all-redu ce operations for each transformer
layer in the forward pass, which are not overlapped with computatio n. An extra two all-reduces are needed
in the gradient computation, for a total of six (when including the ac tivation re-computation). For a given
layer, this implies a network usage of24bdsdm(na−1)
nbnabytes compared with a compute of8bdspl
nbnaﬂop, for an
intensity
νa≈(4+2nI)dm
3(na−1). (12)
C.5 CPU-GPU transfers
Oﬄoading the training state and activation checkpoints requires lar ge data transfers between the CPU and
GPU memory. For the state oﬄoading, the computation for a given la yer is overlapped with the transfer
of the parameters and the gradients for a given layer or its partitio n. In the baseline, the transfer needs to
happen for each micro-batch, while with layeredgradient accumulat ion it happens once for all micro-batches.
The bottleneck is in the forward pass, with four possible values depe nding on the scenario:
νbase
s≈bds
nµnb, νbase−part
s ≈bds
nµνimpr
s≈bds
nb, νimpr−part
s ≈bds(13)
For checkpoint oﬄoad the computation is similar to the pipeline-paralle l network transfer, with half the
computation, for an intensity
νc≈(4+2nI)dm (14)
22
