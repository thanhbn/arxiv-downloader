# DoG là Người Bạn Tốt Nhất của SGD:
Một Lịch Trình Kích Thước Bước Động Không Tham Số
Maor Ivgi
maor.ivgi@cs.tau.ac.ilOliver Hinder
ohinder@pitt.eduYair Carmon
ycarmon@tauex.tau.ac.il
Tóm tắt
Chúng tôi đề xuất một công thức kích thước bước SGD động không cần điều chỉnh, mà chúng tôi gọi là Distance over Gradients (DoG). Kích thước bước DoG phụ thuộc vào các đại lượng thực nghiệm đơn giản (khoảng cách từ điểm khởi đầu và chuẩn của gradients) và không có tham số "learning rate". Về mặt lý thuyết, chúng tôi cho thấy rằng một biến thể nhỏ của công thức DoG có được các bảo đảm hội tụ không tham số mạnh mẽ cho tối ưu hóa lồi ngẫu nhiên chỉ giả định stochastic gradients bị chặn cục bộ.1
Về mặt thực nghiệm, chúng tôi xem xét một loạt rộng các tác vụ transfer learning về thị giác và ngôn ngữ, và cho thấy rằng hiệu suất của DoG gần với hiệu suất của SGD với learning rate được điều chỉnh. Chúng tôi cũng đề xuất một biến thể theo lớp của DoG thường vượt trội hơn SGD được điều chỉnh, tiếp cận hiệu suất của Adam được điều chỉnh. Một triển khai PyTorch có sẵn tại https://github.com/formll/dog.

1 Giới thiệu
Trong khi các phương pháp tối ưu hóa ngẫu nhiên thúc đẩy những cải tiến liên tục trong machine learning, việc chọn các tham số tối ưu hóa—và đặc biệt là learning rate—vẫn là một khó khăn. Các phương pháp tiêu chuẩn bao gồm tìm kiếm trên một tập các learning rate, hoặc đơn giản là chọn learning rate từ công việc trước đó. Cách đầu tiên gây ra chi phí tính toán đáng kể, trong khi cách sau có nguy cơ huấn luyện một mô hình không tối ưu.

Tài liệu phong phú về các phương pháp gradient thích ứng (AdaGrad, Adam, và nhiều biến thể của chúng) cung cấp các thuật toán tối ưu hóa khai thác tốt hơn cấu trúc vấn đề [ví dụ, 29, 48, 34, 84, 57]. Tuy nhiên, các phương pháp này vẫn có một tham số learning rate cần điều chỉnh. Giá trị tối ưu theo lý thuyết của tham số này phụ thuộc vào các tính chất vấn đề không biết trước. Ví dụ, trên các vấn đề lồi, learning rate tối ưu của AdaGrad liên quan đến khoảng cách giữa điểm khởi đầu và nghiệm tối ưu, trong khi trong các thiết lập không lồi, nó liên quan đến tính trơn của hàm và khoảng cách tối ưu ban đầu [33, 95, 30].

Tối ưu hóa không tham số nhằm loại bỏ nhu cầu điều chỉnh như vậy bằng cách thiết kế các thuật toán đạt được tốc độ hội tụ gần tối ưu với gần như không cần biết các tính chất vấn đề [87]. Hầu hết các công trình trong lĩnh vực này [ví dụ, 58, 69, 21, 61, 10, 42, 110] sử dụng các kỹ thuật học trực tuyến tiên tiến để xây dựng các thuật toán mà, đối với thiết lập cơ bản của tối ưu hóa lồi ngẫu nhiên (SCO) với stochastic gradients bị chặn, đạt được các tốc độ hội tụ tối ưu đến các yếu tố logarithmic. Trong khi các thuật toán không tham số thực tế tồn tại [ví dụ 68, 71, 47, 15], có ít nghiên cứu về các phương pháp lựa chọn kích thước bước không tham số thực tế cho SGD. Gần đây, Carmon và Hinder [11] đã chỉ ra rằng việc thực hiện một phép chia đôi cẩn thận trên kích thước bước SGD cho ra một phương pháp tối ưu hóa không tham số tối ưu cho SCO đến một yếu tố logarithmic kép.

1Phiên bản ICML của bài báo sử dụng giả định thông thường hơn và hạn chế hơn về stochastic gradients bị chặn toàn cục.

Trong khi mới lạ về mặt lý thuyết, ở mức độ thực tế, kết quả để lại nhiều điều mong muốn, vì về cơ bản nó quy định công thức tiêu chuẩn của việc chạy SGD nhiều lần với các learning rate khác nhau.

Thuật toán được đề xuất. Trong công việc này, chúng tôi sử dụng các insight chính từ Carmon và Hinder [11] để đi xa hơn một bước và phát triển một lịch trình kích thước bước không tham số. Đối với các vòng lặp SGD có dạng xt+1=xt−ηtgt, trong đó xt biểu thị các tham số mô hình tại vòng lặp thứ t và gt biểu thị stochastic gradient của hàm mất mát, trình tự kích thước bước được đề xuất của chúng tôi là (cho tất cả t≥1)

ηt=max i≤t∥xi−x0∥qP
i≤t∥gi∥2. (DoG )

Nói cách khác, kích thước bước tại vòng lặp t là khoảng cách lớn nhất giữa điểm khởi đầu và các vòng lặp quan sát được, chia cho tổng bình phương chuẩn stochastic gradient, tức là Distance over Gradients (DoG). Ở bước đầu tiên, chúng tôi đặt η0 là rϵ/∥g0∥, tức là, chúng tôi thực hiện một bước gradient chuẩn hóa có kích thước rϵ; chúng tôi chỉ ra rằng, miễn là rϵ nhỏ, việc thiết lập chính xác của nó chỉ có tác động nhẹ.

Quan trọng là, DoG không có tham số "learning rate" nhân: nếu một người xem xét kích thước bước có dạng ηt=c·maxi≤t∥xi−x0∥√P
i≤t∥gi∥2 thì c=1 là một thiết lập tốt phổ quát (xem Mục 2 cho một sự biện minh heuristic và Mục 4.3 cho bằng chứng thực nghiệm cho tuyên bố này).

Hình 1 nêu bật các khía cạnh chính của DoG. Hàng trên cho thấy trình tự kích thước bước DoG cho các giá trị khác nhau của rϵ trong các vấn đề tối ưu hóa ngẫu nhiên lồi (trái) và không lồi (phải). Kích thước bước DoG tăng nhanh (lưu ý thang logarithmic x) và ổn định xung quanh các giá trị gần với kích thước bước SGD tối ưu với ít phụ thuộc vào rϵ. Hàng dưới của hình so sánh các lỗi kiểm tra của DoG và SGD với các kích thước bước khác nhau, cho thấy rằng (đối với tất cả các lựa chọn rϵ) DoG ngang bằng với SGD được điều chỉnh tốt.

1.1 Tóm tắt kết quả
Bảo đảm lý thuyết. Trong Mục 3, chúng tôi phân tích DoG cho tối ưu hóa lồi ngẫu nhiên với stochastic gradients bị chặn và một miền lồi đóng (có thể không bị chặn). Để trình bày kết quả của chúng tôi, hãy để B biểu thị một quả cầu xung quanh điểm khởi đầu x0 với bán kính 3d0, trong đó d0 là khoảng cách giữa x0 và một điểm tối ưu.

Đầu tiên, chúng tôi chỉ ra rằng nếu các vòng lặp của DoG vẫn trong B, thì với xác suất cao DoG đạt được một tốc độ hội tụ tối ưu đến một yếu tố O(log(1+d0/rϵ)). Trong thực tế, DoG có vẻ thực sự ổn định miễn là rϵ đủ nhỏ. Tuy nhiên, DoG không phải lúc nào cũng ổn định: trên các hàm bệnh lý, các vòng lặp của nó có thể di chuyển xa khỏi điểm tối ưu.

Để giải quyết điều này, chúng tôi xem xét một biến thể lý thuyết, được thuần hóa của DoG, mà chúng tôi gọi là T-DoG, có kích thước bước nhỏ hơn một yếu tố logarithmic. Chúng tôi chứng minh rằng, với xác suất cao, các vòng lặp T-DoG không bao giờ rời khỏi B. Do đó, chúng tôi có được một bảo đảm hội tụ không tham số có xác suất cao tối ưu đến các yếu tố logarithmic.

Theo hiểu biết của chúng tôi, đây là lịch trình kích thước bước SGD động đầu tiên đạt được bảo đảm lý thuyết như vậy, và chỉ là bảo đảm không tham số có xác suất cao thứ ba trong tài liệu [theo 11, 109]. Hơn nữa, đây là kết quả không tham số đầu tiên chỉ giả định stochastic gradients bị chặn cục bộ (tức là, trong tập B). Điều này có ý nghĩa quan trọng vì chặn stochastic gradient toàn cục thường được giả định không tồn tại trong nhiều vấn đề, bao gồm bình phương tối thiểu.

Nghiên cứu thực nghiệm. Các thí nghiệm của chúng tôi trong Mục 4 tập trung vào việc tinh chỉnh các mạng neural, bởi vì đây là một thiết lập quan trọng thực tế vẫn cho phép các thí nghiệm kỹ lưỡng với ngân sách tính toán hợp lý. Chúng tôi cũng thực hiện một thí nghiệm quy mô nhỏ với việc huấn luyện một mạng neural từ đầu. Các thí nghiệm của chúng tôi bao gồm 23 tác vụ hiểu ngôn ngữ tự nhiên và phân loại hình ảnh và 8 kiến trúc mô hình phổ biến.

Kết quả của chúng tôi cho thấy rằng, so với DoG, SGD với lịch trình kích thước bước cosine và learning rate cơ bản được điều chỉnh hiếm khi đạt được cải thiện lỗi tương đối lớn hơn 5% (ví dụ, sự khác biệt giữa độ chính xác 95% và 95.25%). Đối với các vấn đề lồi (linear probes), sự khác biệt tương đối trong lỗi dưới 1%. Trong testbed của chúng tôi, Adam được điều chỉnh tốt có xu hướng vượt trội hơn cả SGD và DoG, nhưng một phiên bản theo lớp của DoG (mà chúng tôi gọi là L-DoG) thu hẹp một số khoảng cách hiệu suất này.

Chúng tôi cũng kiểm tra độ nhạy cảm của DoG đối với giá trị rϵ. Chúng tôi thấy rằng đối với hầu hết các kết hợp mô hình/tác vụ, DoG hoạt động nhất quán tốt trên một phạm vi rộng các giá trị rϵ như lý thuyết dự đoán. Tuy nhiên, trong một số trường hợp nhất định, việc chọn rϵ quá thấp dẫn đến hiệu suất kém. Chúng tôi cung cấp một số phát hiện sơ bộ cho thấy điều này một phần là do batch normalization.

Tóm lại, lý thuyết và thí nghiệm của chúng tôi cho thấy DoG có tiềm năng tiết kiệm tính toán đáng kể hiện đang được chi cho việc điều chỉnh learning rate với ít hoặc không có chi phí về hiệu suất—đặc biệt là nếu chúng tôi tái đầu tư một số tính toán tiết kiệm được vào việc huấn luyện một mô hình lớn hơn trên nhiều dữ liệu hơn.

2 Dẫn xuất Thuật toán
Trước khi cung cấp các bảo đảm lý thuyết chặt chẽ cho DoG, trong mục này chúng tôi giải thích nguồn gốc của thuật toán. Điểm khởi đầu của chúng tôi là kết quả sau đây của Carmon và Hinder [11]. Giả sử chúng ta chạy T vòng lặp SGD với kích thước bước cố định η, tức là, phép hồi quy xt+1=xt−ηgt, trong đó xt là vòng lặp SGD và gt là stochastic gradient ở bước t. Nếu, với một số c∈(0,1), điều kiện

η=c·max k≤T∥xk−x0∥qP
k≤T∥gk∥2, (1)

xảy ra, thì các vòng lặp trung bình thỏa mãn một chặn excess loss có tối đa một yếu tố 1/c(1−c²) lớn hơn chặn tối ưu trường hợp xấu nhất đạt được bởi SGD được điều chỉnh hoàn hảo.²

Điều kiện (1) là một phương trình ẩn: nó cho phép chúng ta kiểm tra liệu việc lựa chọn kích thước bước η có tốt hay không chỉ sau khi chạy T bước SGD sử dụng η đó. Việc giải phương trình ẩn này do đó đòi hỏi nhiều lần gọi SGD. Chúng tôi dẫn xuất trình tự kích thước bước DoG bằng cách làm cho phương trình trở nên tường minh: chúng tôi chọn ηt sao cho phương trình (1) thỏa mãn ở mỗi bước. Với c=1, điều này cho ra công thức kích thước bước (DoG). Lý do chúng tôi chọn c=1 là vì nó là ngưỡng mà dưới đó một nghiệm của phương trình ẩn cho ra một tốc độ hội tụ tối ưu. Do đó, trong thực tế chúng tôi mong đợi 1 gần với giá trị c ổn định cao nhất, và do đó có được hiệu suất tốt nhất; chúng tôi xác minh điều này thực nghiệm trong Mục 4.3.

3 Phân tích Lý thuyết
3.1 Kiến thức cơ bản
Thiết lập vấn đề. Mục tiêu của chúng tôi là tối thiểu hóa một hàm mất mát f:X→R trong đó X⊆Rᵐ (bao gồm thiết lập không có ràng buộc X=Rᵐ như một trường hợp đặc biệt quan trọng). Chúng tôi thực hiện phân tích dưới giả định tính lồi tiêu chuẩn sau.

Giả định 1 (Tính lồi). Hàm f lồi, miền X đóng và lồi, và giá trị tối thiểu của nó đạt được tại một số x⋆∈X, tức là, f⋆:= inf x∈X f(x) = f(x⋆).

Trong Phụ lục A, chúng tôi thảo luận về một sự nới lỏng có thể của tính lồi mà dưới đó các kết quả của chúng tôi tiếp tục thỏa mãn.

Để tối thiểu hóa f, chúng tôi giả định có quyền truy cập vào một oracle gradient ngẫu nhiên G. Khi được truy vấn tại một điểm x∈X, oracle trả về một ước lượng stochastic (sub)gradient G(x) thỏa mãn E[G(x)|x]∈∂f(x). Với sự lạm dụng ký hiệu nhỏ, chúng tôi viết ∇f(x):=E[G(x)|x]. Chúng tôi đưa ra giả định sau, trong đó ∥·∥ biểu thị chuẩn Euclidean.

Giả định 2 (Stochastic gradients bị chặn từng điểm). Tồn tại một hàm liên tục ℓ:X→R sao cho ∥G(x)∥≤ℓ(x) gần như chắc chắn.

Giả định 2, xuất hiện trong công việc trước đó [20, 61], yếu hơn các giả định thông thường trong tối ưu hóa ngẫu nhiên không tham số, mà hoặc chặn đều stochastic gradients, tức là, ∥G(x)∥≤L cho tất cả x∈X [xem, ví dụ, 71, 21], hoặc chặn đều phương sai gradient [44]. Tuy nhiên, ngay cả các vấn đề bình phương tối thiểu (với G(x)=(⟨a,x⟩−b)a cho a∈Rᵐ và b∈R ngẫu nhiên) vi phạm cả hai chặn đều. Ngược lại, ℓ hữu hạn dưới giả định nhẹ rằng a, b là các biến ngẫu nhiên bị chặn; xem Phụ lục C.5 để thảo luận thêm.

Phát biểu thuật toán. Chúng tôi nghiên cứu SGD (chiếu) với lịch trình learning rate động {ηt}, tức là,

xt+1= ProjX(xt−ηtgt)

trong đó x0 là một khởi tạo cho trước, gk:=G(xk), và ProjX(·) là phép chiếu Euclidean lên X. Để phát biểu và phân tích DoG một cách súc tích, chúng tôi định nghĩa các đại lượng sau:

rt:=∥xt−x0∥, r̄t= max k≤trk∨rϵ và Gt:=∑ᵗk=0∥gt∥²,

trong đó a∨b:= max{a,b} và rϵ là một tham số kích thước chuyển động ban đầu nhỏ do người dùng chỉ định. Với ký hiệu này, chúng tôi định nghĩa một họ các lịch trình learning rate giống DoG.

Định nghĩa 1. Một lịch trình kích thước bước là giống DoG nếu

ηt=r̄t/√G't

cho một dãy dương không giảm G't phụ thuộc chỉ vào x0, g0, ..., gt và thỏa mãn G't≥Gt.

DoG tương ứng với việc đơn giản đặt G't=Gt; trong Mục 3.3, chúng tôi xem xét một thuật toán giống DoG lý thuyết (hoặc được thuần hóa) mà chúng tôi bảo đảm các vòng lặp bị chặn bằng cách làm cho G't lớn hơn Gt theo các yếu tố polylogarithmic. Trong suốt, chúng tôi chặn lỗi của dãy trung bình có trọng số

x̄t:=1/∑ᵗ⁻¹k=0r̄k ∑ᵗ⁻¹k=0r̄kxk. (2)

Cuối cùng, để sắp xếp phân tích, chúng tôi định nghĩa:

dt:=∥xt−x⋆∥,d̄t:= max k≤tdk,ℓ̄t:= max k≤tℓ(xk),

và

θt,δ:= log(60 log(6t)/δ).

Quy ước logarithm. Trong suốt bài báo, log là cơ số e và log+(·):= 1 + log(·).

3.2 Chặn khoảng cách tối ưu giả định các vòng lặp bị chặn
Trong mục này, chúng tôi chặn khoảng cách tối ưu đạt được bởi bất kỳ thuật toán giống DoG nào. Các chặn của chúng tôi phụ thuộc vào các đại lượng r̄T và GT, và gần như tối ưu khi r̄T=O(d0) (tức là, các vòng lặp DoG không di chuyển quá xa khỏi x0) và G't không lớn hơn GT nhiều. Trong mục tiếp theo, chúng tôi mô tả một thuật toán giống DoG cụ thể được bảo đảm thỏa mãn cả hai yêu cầu.

Tính lồi và bất đẳng thức Jensen ngụ ý rằng x̄t thỏa mãn

f(x̄t)−f⋆≤1/∑ᵗ⁻¹k=0r̄k ∑ᵗ⁻¹k=0r̄k⟨∇f(xk), xk−x⋆⟩. (3)

Tổng trong vế phải phân tách thành hai thành phần:

∑ᵗ⁻¹k=0r̄k⟨gk, xk−x⋆⟩ - ∑ᵗ⁻¹k=0r̄k⟨Δk, xk−x⋆⟩
    regret có trọng số           nhiễu, (4)

trong đó Δk:=gk−∇f(xk). Chúng tôi đưa ra chặn xác suất 1 cho regret có trọng số (Lemma 1) và chặn xác suất cao cho số hạng nhiễu (Lemma 2). Trong mỗi trường hợp, thách thức chính là thay thế các chặn a-priori trên d0 (hoặc kích thước miền) bằng r̄T quan sát được thực nghiệm. Chúng tôi trình bày và thảo luận từng lemma.

Lemma 1 (Chặn regret có trọng số). Nếu X là một tập lồi đóng thì bất kỳ sơ đồ giống DoG nào (Định nghĩa 1) thỏa mãn

∑ᵗ⁻¹k=0r̄k⟨gk, xk−x⋆⟩ ≤ r̄t(2d̄t+ r̄t)√G't-1, ∀t≥1.

Chứng minh. Sử dụng xk+1= ProjX(xk−ηkgk), chúng ta có được bất đẳng thức tiêu chuẩn d²k+1≤∥xk−ηkgk−x⋆∥²=d²k−2ηk⟨gk, xk−x⋆⟩+η²k∥gk∥². Sắp xếp lại điều này cho ra:

⟨gk, xk−x⋆⟩ ≤ (d²k−d²k+1)/(2ηk) + ηk∥gk∥²/2. (5)

Do đó, ∑ᵗ⁻¹k=0r̄k⟨gk, xk−x⋆⟩ không quá

1/2 ∑ᵗ⁻¹k=0 r̄k/ηk(d²k−d²k+1) + 1/2 ∑ᵗ⁻¹k=0r̄kηk∥gk∥²
        (A)                           (B).

Chúng tôi chặn các số hạng (A) và (B) lần lượt, bắt đầu với số hạng đầu tiên:

(A) = ∑ᵗ⁻¹k=0√G'k(d²k−d²k+1) = d²0√G'0−d²t√G't-1+∑ᵗ⁻¹k=1d²k(√G'k−√G'k-1)

(i) ≤ d̄²t√G'0−d²t√G't-1+d̄²t∑ᵗ⁻¹k=1(√G'k−√G'k-1)

= √G't-1(d̄²t−d²t) (ii) ≤ 4r̄td̄t√G't-1.

Bất đẳng thức (i) sử dụng dk≤d̄t và G'k không giảm như theo Định nghĩa 1. Bất đẳng thức (ii) thỏa mãn vì, với s∈arg maxk≤tdk, chúng ta có d̄²t−d²t=d²s−d²t=(ds−dt)(ds+dt)≤∥xs−xt∥(ds+dt)≤(r̄s+r̄t)(ds+dt)≤4r̄td̄t. Chặn số hạng thứ hai (B), chúng ta có:

(B) = ∑ᵗ⁻¹k=0 r̄²k∥gk∥²/√G'k ≤ ∑ᵗ⁻¹k=0 r̄²k∥gk∥²/√Gk ≤ r̄²t∑ᵗ⁻¹k=0 ∥gk∥²/√Gk ≤ 2r̄²t/√Gt-1,

trong đó bất đẳng thức cuối cùng sử dụng Lemma 4 tiêu chuẩn với ak=Gk=∑i≤k∥gi∥².

Trong khi chứng minh của Lemma 1 tương tự như phân tích SGD thích ứng trong đó ηt=ρ/√Gt [33], có một vài khác biệt chính. Đầu tiên, kích thước bước DoG có thể tăng, điều này thường làm cho các phương pháp gradient thích ứng khó phân tích [80]. Chúng tôi vượt qua khó khăn này bằng cách xem xét regret có trọng số bởi r̄k, điều này loại bỏ phần tăng của kích thước bước. Thứ hai, phân tích SGD thích ứng tiêu chuẩn cho ra một chặn tỷ lệ với d̄²t (thường được chặn thêm bằng cách sử dụng đường kính miền) thay vì r̄td̄t như trong chặn của chúng tôi. Đây là một khác biệt quan trọng, vì—như chúng tôi sẽ tranh luận—r̄t "triệt tiêu" khi chia cho ∑k<tr̄k, trong khi d̄t thì không. Chúng tôi có được kết quả cải thiện bằng cách giữ lại số hạng −d²t√G't-1 trong chặn cho (A) ở trên; một thủ thuật tương tự như Carmon và Hinder [11, Lemma 1].

Tiếp theo, chúng tôi xử lý số hạng nhiễu trong (4), nhớ lại ký hiệu Δt:=gt−∇f(xt) và θt,δ:=log(60 log(6t)/δ).

Lemma 2 (Chặn nhiễu). Dưới Giả định 2, với tất cả δ∈(0,1), T∈N và L>0, chúng ta có

P[∃t≤T: ∑ᵗ⁻¹k=0r̄k⟨Δk, xk−x⋆⟩≥8r̄t-1d̄t-1√θt,δGt-1+θ²t,δL²] ≤ δ+P[ℓ̄T>L].

Chứng minh của Lemma 2 xuất hiện trong Phụ lục C.1 và dựa trên một chặn tập trung mới, Lemma 7, cho phép chúng tôi chặn số hạng nhiễu mặc dù không có chặn xác định về độ lớn của dãy hiệu martingale r̄k⟨Δk, xk−x⋆⟩. Chứng minh của Lemma 7 liên quan đến việc kết hợp các chặn Bernstein đều theo thời gian [39] và một chặn tổng quát về tổng tích lũy của tích dãy (Lemma 5), có thể có ích độc lập.

Kết hợp các kết quả trên, chúng tôi có được như sau.

Mệnh đề 1. Với tất cả δ∈(0,1) và L>0, nếu Giả định 1, Giả định 2, và Định nghĩa 1 thỏa mãn thì với xác suất ít nhất 1−δ−P[ℓ̄T>L], với tất cả t≤T, khoảng cách tối ưu f(x̄t)−f⋆ là

O[(d0+r̄t)√G't-1+Gt-1θt,δ+L²θ²t,δ] / [∑i<tr̄i/r̄t].

Chứng minh. Suy ra từ Phương trình (3) và (4), Lemma 1, Lemma 2 và sự kiện rằng d̄t≤d0+r̄t.

Sự kiện đại số sau đây cho thấy rằng luôn có một vòng lặp τ≤T trong đó mẫu số ∑i<tr̄i/r̄t≥Ω(T/log(r̄T/rϵ)); xem Phụ lục B.1 để chứng minh.

Lemma 3. Cho s0, s1, ..., sT là một dãy dương không giảm. Thì

max t≤T ∑i<tsi/st ≥ T/(e log+(sT/s0)).

Kết hợp Mệnh đề 1 và Lemma 3 cho ra như sau (xem chứng minh ngắn trong Phụ lục C.2).

Hệ quả 1. Dưới Giả định 1 và 2, với bất kỳ D≥d0, cho LD:= max x∈X:∥x−x0∥≤D ℓ(x). Thì, với tất cả δ∈(0,1) và với τ∈arg maxt≤T ∑i<τr̄i/r̄t, với xác suất ít nhất 1−δ−P(r̄T>D), các vòng lặp DoG thỏa mãn chặn khoảng cách tối ưu

f(x̄τ)−f⋆=O[D√Gτ-1θτ,δ+L²Dθ²τ,δ/(T log+(D/rϵ))] = O[DLD√θτ,δ/(√T log+(D/rϵ))].

Hệ quả 1 hữu ích ngay lập tức khi X bị chặn nhưng đường kính chính xác của nó không được biết, ví dụ khi X là một polytope như thông thường trong lập trình ngẫu nhiên hai giai đoạn [64].

Đơn giản hóa chặn cho các quỹ đạo DoG điển hình. Giả sử rằng các vòng lặp DoG thỏa mãn r̄T≤3d0, điều này ngụ ý rằng ℓ̄T≤L⋆:=L3d0 và do đó (cho DoG) G't=Gt≤L²⋆T. Thay thế vào Hệ quả 1 cho ra một chặn khoảng cách tối ưu O[d0L⋆√θT,δ log(r̄T/rϵ)/√T], tối ưu minimax đến một số hạng logarithmic kép trong T và logarithmic trong 1/rϵ [2].

Hơn nữa, trong các quỹ đạo DoG thực tế, ngay cả số hạng nhân log(r̄T/rϵ) cũng có thể quá bi quan. Điều này là bởi vì r̄t thường tăng nhanh chóng trong t0<1000 bước và sau đó ổn định (xem Hình 12 trong phụ lục). Do đó, r̄i/r̄t≥1/10 cho hầu hết quỹ đạo tối ưu hóa, và ∑i<tr̄i/r̄t≥t/10−t0. Thay thế trở lại vào Mệnh đề 2, chúng ta có rằng x̄T là O[d0L⋆√θT,δ/(√T−t0)] không tối ưu.

DoG có thể chạy hoang dại. Trong khi DoG ổn định thực nghiệm, tồn tại các ví dụ (không ngẫu nhiên) trong đó r̄t phát triển lớn hơn d0 nhiều: trong Phụ lục C.3, chúng tôi mô tả một biến thể của hàm Nemirovski [63, 62] mà với nó r̄t=rϵ√t và do đó r̄t/d0 phân kỳ khi t tăng. Tiếp theo, chúng tôi chỉ ra rằng bằng cách giảm nhẹ kích thước bước DoG, chúng tôi có thể bảo đảm rằng r̄T/d0≤3 với xác suất cao.

3.3 Chặn ổn định vòng lặp
Mục này giới thiệu một sơ đồ kích thước bước giống DoG mới có các vòng lặp được bảo đảm vẫn bị chặn với xác suất cao. Chúng tôi gọi sơ đồ này là T-DoG, trong đó T có nghĩa là "lý thuyết" hoặc "được thuần hóa." Kích thước bước được cho bởi ηt=r̄t/√G't, trong đó

G't= 84θ²T,δlog²+[1+tℓ̄²t/ℓ̄²0](Gt-1+ 16ℓ̄²t), (T-DoG)

sử dụng G-1:= 0, và nhớ lại rằng ℓ̄t:= max i≤tℓ(xi) cho một hàm ℓ thỏa mãn Giả định 2.

Công thức T-DoG phụ thuộc yếu vào ngân sách vòng lặp T và xác suất thất bại δ qua θt,δ:= log(log(6t)/δ); như chúng tôi chỉ ra dưới đây, trong thiết lập không ngẫu nhiên, chúng tôi có thể đơn giản thay thế θt,δ bằng 1. Hơn nữa, số hạng 16ℓ̄t thường phát triển chậm với t, trở nên không đáng kể so với Gt-1.

Đáng chú ý, kích thước bước T-DoG không yêu cầu chặn trên toàn cục về chuẩn stochastic gradient.

Chúng tôi sẵn sàng phát biểu tính chất chính của T-DoG: bảo đảm ổn định vòng lặp.

Mệnh đề 2. Giả sử rằng Giả định 1 và 2 thỏa mãn và rϵ≤3d0. Với bất kỳ δ∈(0,1) và T∈N, các vòng lặp của T-DoG thỏa mãn P(r̄T>3d0)≤δ.

Chúng tôi trì hoãn chứng minh đầy đủ cho Phụ lục C.4 và tiếp tục nêu bật lập luận chính bằng cách chứng minh kết quả trong trường hợp không có nhiễu.

Chứng minh Mệnh đề 2 trong trường hợp không có nhiễu. Trong trường hợp không có nhiễu, chúng ta có gk=∇f(xk) và do đó ⟨gk, xk−x⋆⟩≥f(xk)−f⋆≥0. Thay thế vào (5) và sắp xếp lại cho ra d²k+1−d²k≤η²k∥gk∥². Giả sử bằng quy nạp rằng r̄t≤3d0 và tích phân từng phần cho ra

d²t+1−d²0≤r̄²t∑ᵗk=0∥gk∥²/G'k (i)≤ r̄²t/(84)∑ᵗk=0(Gk−Gk-1)/[(Gk+ℓ̄²k) log²+(Gk+ℓ̄²k)/ℓ̄²0]
(ii)≤ r̄²t/84 (iii)≤ 9d²0/84 ⟹ dt+1≤2d0,

trong đó (i) sử dụng rằng ∥gk∥²=Gk−Gk-1 (với ký hiệu tắt G-1:= 0) và

G'k≥84(Gk-1+∥gk∥²+ℓ̄²k) log²+[∑i≤tℓ̄²t/ℓ̄²0] ≥ 84(Gk+ℓ̄²k) log²+[(Gk+ℓ̄²k)/ℓ̄²0]

theo Giả định 2, ngụ ý ∥gk∥≤ℓ̄k với tất cả k, (ii) sử dụng Lemma 6 với ak=Gk+ℓ̄²k, và (iii) sử dụng giả định quy nạp r̄t≤3d0. Do đó, rt+1≤dt+1+d0≤3d0 theo bất đẳng thức tam giác, hoàn thành bước quy nạp. Lưu ý rằng chứng minh này đã bỏ qua số hạng θt,δ trong (T-DoG), chứng minh nó không cần thiết trong trường hợp không có nhiễu.

Cho Giả định 2, chúng tôi định nghĩa

L⋆= max x∈X:∥x−x0∥≤3∥x0−x⋆∥ ℓ(x). (6)

Với tất cả các thành phần trong tay, chúng tôi phát biểu bảo đảm chính cho T-DoG.

Định lý 1. Giả sử rằng Giả định 1 và 2 thỏa mãn. Với bất kỳ δ∈(0,1/2), T∈N, xem xét T vòng lặp của T-DoG với rϵ≤3d0. Thì với τ∈arg maxt≤T∑i<τr̄i/r̄t, chúng ta có, với xác suất ít nhất 1−2δ, rằng

f(x̄τ)−f⋆=O[cδ,rϵ,Td0√Gτ-1+L²⋆/T] = O[cδ,rϵ,Td0L⋆/√T],

trong đó cδ,rϵ,T= log+[Td0L⋆/(f(x0)−f⋆)] log+(d0/rϵ) log log+(T)/δ.

Chứng minh. Định lý suy ra từ Hệ quả 1, Mệnh đề 2 và định nghĩa của T-DoG, trong đó chúng tôi lưu ý rằng Giả định 2 và tính lồi của f ngụ ý ℓ̄0≥∥∇f(x0)∥≥(f(x0)−f(x⋆))/d0, trong khi r̄T≤3d0 cho ra ℓ̄T≤L⋆. Do đó, log+[1+Tℓ̄²T/ℓ̄²0] = O[log+[Td0L⋆/(f(x0)−f⋆)]].

Định lý 1 cho ra chặn hội tụ tối ưu [2] đến các yếu tố logarithmic. Theo hiểu biết tốt nhất của chúng tôi, đây là phương pháp tối ưu hóa ngẫu nhiên không tham số đầu tiên không yêu cầu stochastic gradients bị chặn đều trên miền X và thay vào đó tạo ra một chặn phụ thuộc vào chặn gradient 'cục bộ' L⋆. Quan trọng, công thức kích thước bước T-DoG không yêu cầu kiến thức trước về L⋆.³

Mở rộng cho việc lấy trung bình vòng lặp không có trọng số. Trong khi trung bình vòng lặp có trọng số (2) thuận tiện cho phân tích của chúng tôi, các chặn tương tự như Mệnh đề 1, Hệ quả 1 và Định lý 1 cũng thỏa mãn cho trung bình vòng lặp tiêu chuẩn không có trọng số x̂T=1/T∑T-1t=0xt. Đối với x̂T, cũng đơn giản để chỉ ra chặn 1/T lỗi cho DoG trong trường hợp trơn không có nhiễu. Xem Phụ lục D để biết chi tiết.

4 Thí nghiệm
Để kiểm tra DoG trong các kịch bản thực tế, chúng tôi thực hiện các thí nghiệm rộng rãi trên một tập đa dạng các tác vụ và kiến trúc mô hình trong cả miền thị giác và ngôn ngữ. Chúng tôi xây dựng một testbed bao gồm hơn 20 tác vụ và 7 kiến trúc mô hình, bao gồm hiểu ngôn ngữ tự nhiên và thị giác máy tính (Mục 4.1). Trong testbed này, chúng tôi so sánh DoG với SGD và Adam (Mục 4.2), cho thấy rằng DoG hoạt động ngang bằng với SGD được điều chỉnh, nhưng không tốt bằng Adam được điều chỉnh. Tuy nhiên, một phiên bản theo lớp của DoG (được định nghĩa dưới đây) thu hẹp phần lớn khoảng cách này với Adam mà không yêu cầu điều chỉnh. Chúng tôi cũng sử dụng testbed để phân tích độ nhạy cảm của DoG đối với các tham số cố định (Mục 4.3), và chứng minh tính hiệu quả trong các thiết lập hồi quy logistic lồi (Mục 4.4). Cuối cùng, chúng tôi áp dụng DoG và L-DoG cho việc tinh chỉnh mô hình CLIP trên ImageNet (Mục 4.5) và huấn luyện mô hình CIFAR10 từ đầu (Mục 4.6), và cung cấp so sánh sơ bộ với các phương pháp không cần điều chỉnh đã được đề xuất trước đây (Mục 4.7). Một triển khai PyTorch của DoG có sẵn tại https://github.com/formll/dog.

DoG theo lớp. Các mô hình neural nói chung và các mô hình dựa trên transformer nói riêng thường được hưởng lợi từ việc sử dụng kích thước bước theo từng tham số hoặc theo từng lớp [48, 106]. Với điều này trong tâm trí, chúng tôi xem xét một phiên bản theo lớp của DoG, mà chúng tôi gọi là L-DoG, trong đó chúng tôi áp dụng công thức (DoG) riêng biệt cho mỗi lớp. Cụ thể, nếu chúng tôi xem xét xlt là các trọng số trong lớp⁴ l ở bước t, thì chúng tôi đặt learning rate cho lớp đó là ηlt=maxi≤t∥xli−xl0∥/√∑i≤t∥gli∥²+ϵ, trong đó ϵ=10⁻⁸ được thêm vào mẫu số để ổn định số học. Trong khi chúng tôi không cung cấp bảo đảm lý thuyết cho L-DoG, chúng tôi chỉ ra dưới đây rằng nó hoạt động tốt trong thực tế.

4.1 Testbed tinh chỉnh
Các thí nghiệm chính của chúng tôi tập trung vào việc tinh chỉnh các mô hình đã được huấn luyện trước, điều này cho phép chúng tôi thí nghiệm với các mô hình tiên tiến trong khi cũng điều chỉnh kỹ lưỡng learning rate cho các optimizer cơ bản, sử dụng ngân sách tính toán học thuật.

Siêu tham số chung. Đối với mỗi thuật toán cơ bản, chúng tôi sử dụng lịch trình learning rate theo thực tiễn tốt nhất (cosine annealing cho tất cả thí nghiệm, với giai đoạn warmup cho các thí nghiệm ngôn ngữ) và quét qua learning rate đỉnh cho mỗi cặp mô hình/tác vụ. Chúng tôi cho mỗi cặp một ngân sách bước cố định được thiết kế để đủ cho hội tụ, thực hiện đánh giá trong suốt quá trình huấn luyện. Trong tất cả trường hợp, chúng tôi sử dụng polynomial decay averaging⁵ như đề xuất bởi Shamir và Zhang [83], và chọn checkpoint tốt nhất (có hoặc không có trung bình hóa) dựa trên hiệu suất đánh giá. Chúng tôi lặp lại các thiết lập học tập liên quan với 5 seed khác nhau, và báo cáo hiệu suất trung bình qua các seed. Để đơn giản, chúng tôi không sử dụng weight decay trong suốt. Tập hợp hoàn chỉnh các siêu tham số xuất hiện trong Phụ lục E.

Hiểu ngôn ngữ tự nhiên (NLU). Để kiểm tra hiệu quả của DoG trong NLU hiện đại, chúng tôi sử dụng nó để tinh chỉnh các mô hình ngôn ngữ transformer [89] trên benchmark GLUE được nghiên cứu kỹ [94] đo lường hiệu suất của các mô hình trên các tác vụ phân loại văn bản đa dạng (được liệt kê trong Phụ lục E.3). Ngoài ra, chúng tôi tinh chỉnh các mô hình trên SQuAD 1.1, một tập dữ liệu trả lời câu hỏi [79]. Chúng tôi tinh chỉnh một checkpoint RoBERTa-base [54] và T5-base [78].⁶ Đối với mỗi tác vụ, chúng tôi sử dụng các metric đánh giá chính thức được định nghĩa trong Wang et al. [94] và Rajpurkar et al. [79] cũng như các phân chia ban đầu được đề xuất của họ, và báo cáo kết quả trên tập đánh giá.

Thị giác máy tính. Chúng tôi cũng tinh chỉnh 5 kiến trúc mô hình trên 12 tác vụ thị giác máy tính khác nhau từ benchmark VTAB [108] (xem Phụ lục E.3); trong số 7 tác vụ khác trong VTAB, 5 là tầm thường (độ chính xác lớn hơn 99%) và 2 có phân chia xác thực nhỏ dẫn đến việc lựa chọn mô hình không đáng tin cậy. Chúng tôi tuân theo các phân chia huấn luyện, xác thực và kiểm tra được định nghĩa trong VTAB, và báo cáo hiệu suất trên phân chia kiểm tra (sử dụng phân chia xác thực để lựa chọn mô hình). Chúng tôi tinh chỉnh 5 mô hình: VGG11 [85], ResNet50 [37], Densenet121 [40], ViT-B/32 [28], và ConvNeXt-T [55], trong đó mô hình ViT được huấn luyện trước trên ImageNet 21K và các mô hình khác được huấn luyện trên ImageNet 1K [26].

Metric hiệu suất chuẩn hóa. Vì các metric hiệu suất trong testbed của chúng tôi khác nhau đáng kể giữa các tác vụ và mô hình, chúng khó so sánh tổng hợp. Để giải quyết điều này, chúng tôi xem xét khái niệm sau đây về sự khác biệt lỗi tương đối (RED), cung cấp một thước đo sự khác biệt hiệu suất chuẩn hóa. Cụ thể, cho một tác vụ và một kiến trúc mô hình, cho errx là lỗi⁷ của mô hình khi được huấn luyện với optimizer x (Adam hoặc SGD với một learning rate nhất định, hoặc L-DoG) và cho errDoG là lỗi khi được huấn luyện với DoG. Thì

RED(errx,errDoG):=(errDoG−errx)/errDoG.

Giá trị RED dương cho thấy rằng optimizer x tốt hơn DoG, và giá trị âm cho thấy ngược lại. Khi giá trị tuyệt đối của RED dưới một vài điểm phần trăm, các phương pháp được so sánh gần như tương đương. Ví dụ, RED 5% tương đương với sự khác biệt giữa độ chính xác 95% và 95.25%.

Thiết lập rϵ. Phân tích lý thuyết của chúng tôi cho thấy rằng lựa chọn cụ thể của rϵ không quan trọng miễn là nó đủ nhỏ so với khoảng cách giữa khởi tạo trọng số x0 và điểm tối ưu. Do đó, đối với các thí nghiệm thị giác, chúng tôi đặt rϵ=α·(1+∥x0∥) với α=10⁻⁴, giả định rằng khoảng cách đến điểm tối ưu lớn hơn 0.01% chuẩn khởi tạo. Đối với các thí nghiệm ngôn ngữ, giả định này hóa ra sai (gây ra DoG phân kỳ trong một số trường hợp), và chúng tôi giảm α xuống 10⁻⁶ cho DoG và 10⁻⁸ cho L-DoG, trong đó số hạng cộng 10⁻⁶ quá lớn trong một số lớp. Chúng tôi tin rằng 10⁻⁶ và 10⁻⁸ nên là các giá trị mặc định tốt cho DoG và L-DoG, tương ứng, mặc dù các mạng có batch normalization hoặc các sơ đồ khởi tạo khác nhau có thể yêu cầu một giá trị lớn hơn; xem Mục 4.3 để thảo luận thêm.

4.2 So sánh hiệu suất tinh chỉnh
Hình 2 mô tả median, IQR (khoảng tứ phân vị) và RED trung bình của mỗi mô hình,⁸ khi được huấn luyện với SGD và Adam với các learning rate đỉnh khác nhau. Hình cho thấy rằng, khi so sánh giữa các mô hình, không có learning rate mặc định tốt nào cho cả SGD và Adam. Hơn nữa, ngay cả đối với một mô hình duy nhất, chỉ có learning rate SGD rất cụ thể mới hoạt động tốt, trong khi hầu hết đều kém hơn đáng kể so với việc sử dụng DoG. Ngay cả khi được điều chỉnh đến giá trị learning rate cố định tốt nhất cho mỗi mô hình (mà chúng tôi gọi là model tuned LR), một số tác vụ vẫn có thể thất bại (so với DoG) như được chỉ ra bởi IQR lớn và khoảng cách giữa RED trung bình (tam giác) và RED median (vòng tròn) trong các mô hình như ViT-B/32 và Densenet121. Trong khi Adam cũng yêu cầu điều chỉnh, nó ít nhạy cảm hơn SGD đối với việc chọn learning rate đỉnh. Để xem phân tích đầy đủ hiệu suất theo tác vụ, xem Hình 7 và Bảng 4 và 5 trong Phụ lục F.1.

DoG hoạt động tương tự như SGD được điều chỉnh tốt trong 79 trong số 80 kết hợp mô hình/tác vụ trong testbed của chúng tôi. Ngoại lệ duy nhất là điều chỉnh T5-b trên CoLA, trong đó DoG hoạt động bất thường trong khi SGD chỉ thành công với một số learning rate. Ngược lại, cả Adam và L-DoG đều đạt được hiệu suất hợp lý một cách nhất quán. Hiệu suất kém của DoG trên CoLA dẫn đến các thước đo RED cao cho trường hợp này, làm cho RED trung bình (tam giác) vượt qua RED median trong Hình 2 cho T5-b. Chúng tôi phân tích thêm ngoại lệ này trong Phụ lục F.3 và chỉ ra rằng việc chọn rϵ nhỏ hơn đáng kể cho DoG giảm thiểu vấn đề.

Hình 3 (trên) so sánh DoG với SGD có model tuned LR như định nghĩa ở trên, cũng như instance tuned LR, trong đó đối với mỗi cặp mô hình/tác vụ, chúng tôi chọn learning rate tốt nhất, với chi phí tính toán lớn hơn 5–7 lần so với chạy DoG. Hiệu suất của DoG vẫn gần với SGD có instance-tuned LR, với RED median lớn nhất được quan sát cho ResNet50 và ViT-B/32.

Hình 3 (dưới) so sánh DoG với model-tuned và instance-tuned Adam, cũng như với L-DoG. Trong một số trường hợp (cụ thể là ResNet50 và ConvNeXt-T), khoảng cách giữa DoG và Adam đáng kể và có lợi cho Adam. Chúng tôi đặt giả thuyết rằng điều này là do kích thước bước theo tham số và các cơ chế momentum của Adam, mà DoG không khai thác. L-DoG, có kích thước bước theo lớp, có RED median dương cho tất cả các mô hình, và thu hẹp khoảng cách giữa DoG và Adam, đặc biệt là cho ResNet50.

Các baseline instance-tuned tiêu thụ tính toán nhiều hơn đáng kể so với DoG và L-DoG. Trong Phụ lục F.2, chúng tôi cân bằng ngân sách tính toán bằng cách giảm số bước cho SGD và Adam. Điều này làm cho DoG vượt trội hơn instance-tune SGD trong hầu hết các trường hợp, và đưa L-DoG gần hơn đáng kể với Adam.

4.3 Độ nhạy cảm của các tham số cố định của DoG
Kích thước chuyển động ban đầu rϵ. Lý thuyết của chúng tôi cho thấy rằng tất cả các lựa chọn rϵ đủ nhỏ nên hoạt động tương tự, nhưng việc chọn rϵ quá lớn (so với khoảng cách ban đầu đến điểm tối ưu) có thể làm tổn hại hiệu suất của thuật toán. Trong Hình 4 (trái), chúng tôi vẽ hiệu suất kiểm tra theo hàm của rϵ cho 8 kết hợp mô hình/tác vụ. Đối với 7 trong số 8, DoG rất mạnh mẽ đối với giá trị rϵ miễn là nó đủ nhỏ, như dự đoán. Tuy nhiên, ResNet50 trên CIFAR-100 (dưới trái) là một ngoại lệ, trong đó các giá trị rϵ nhỏ hơn dẫn đến sụt giảm độ chính xác. Chúng tôi đặt giả thuyết rằng điều này là do tính bất biến thang đo được giới thiệu bởi batch normalization (BN), và cung cấp bằng chứng hỗ trợ cho điều đó trong Phụ lục F.4 (Hình 10), trong đó chúng tôi chỉ ra rằng DoG không nhạy cảm với rϵ khi chúng tôi tắt BN. Trong phụ lục, chúng tôi cũng cung cấp một chẩn đoán bổ sung cho độ nhạy cảm rϵ bằng cách vẽ ηt vs. η0 cho các giá trị t khác nhau (xem Hình 8).

Learning rate cơ bản. Chỉ cho thí nghiệm này, chúng tôi xem xét các biến thể của DoG với các giá trị learning rate cơ bản khác nhau, tức là, kích thước bước có dạng ηt=c·maxi≤t∥xi−x0∥/√∑i≤t∥gi∥² với các giá trị c khác nhau. Chúng tôi mong đợi hiệu suất tối ưu khi c gần 1. Cụ thể hơn, chúng tôi mong đợi thuật toán không ổn định khi c>1 và chậm hội tụ hơn (và ít khả năng tổng quát hóa tốt) khi c<1. Như có thể quan sát trong Hình 4 (phải), các giá trị xung quanh c=1 hoạt động tốt cho tất cả các mô hình. Đối với các giá trị nhỏ hơn, thực sự có hiệu suất kém hơn trong một số mô hình (chủ yếu là ResNet50 và RoBERTa-b)—cho thấy T-DoG sẽ không hoạt động tốt trong thực tế—trong khi các giá trị lớn hơn dẫn đến phân kỳ (trong 6 trong số 8 trường hợp). Do đó, phạm vi hữu ích cho c rất hẹp (khoảng [0.5, 1.5]) và việc điều chỉnh nó không có khả năng tạo ra những cải thiện đáng kể. Điều này trái ngược với Adam và SGD thường yêu cầu tìm kiếm trên một không gian trải rộng vài bậc độ lớn để huấn luyện đúng một mô hình.

4.4 Tối ưu hóa lồi
Chúng tôi cũng đánh giá DoG trên các tác vụ tối ưu hóa lồi, phù hợp với các giả định của phân tích lý thuyết. Để làm như vậy, chúng tôi thực hiện hồi quy logistic đa lớp trên các đặc trưng thu được từ các mô hình thị giác máy tính trong testbed của chúng tôi, tức là, linear probes. Chúng tôi thấy rằng SGD model-tuned hoạt động ngang bằng hoặc tệ hơn DoG, trong khi SGD instance-tuned hầu như không có lợi thế nào (Hình 5), với các giá trị RED dưới 1% (tương ứng với sự khác biệt giữa độ chính xác 90% và 90.1%). Hơn nữa, ngay cả trong thiết lập đơn giản này, SGD nhạy cảm với việc chọn learning rate, khác nhau đáng kể giữa các mô hình (Hình 6).

4.5 Tinh chỉnh trên ImageNet
Để bổ sung cho testbed tinh chỉnh chính, chúng tôi thực hiện một thí nghiệm hạn chế hơn liên quan đến ImageNet như một tác vụ downstream, đắt đỏ hơn để điều chỉnh do quy mô lớn hơn. Chúng tôi tinh chỉnh một mô hình ViT-B/32 CLIP [77] và so sánh DoG và L-DoG với huấn luyện bằng SGD hoặc AdamW [57]. Chúng tôi sử dụng một quy trình huấn luyện tương tự như Wortsman et al. [101]; xem Phụ lục E.7 để biết chi tiết bổ sung. Bảng 1 cho thấy độ chính xác ImageNet top-1 validation của các checkpoint mô hình cuối cùng, với và không có polynomial decay averaging được sử dụng trong suốt các thí nghiệm của chúng tôi. DoG hoạt động tương tự như SGD, nhưng cả hai thuật toán đều hoạt động tệ hơn đáng kể so với AdamW, có thể do ngân sách vòng lặp không đủ. L-DoG hoạt động tốt trong thiết lập này, cải thiện so với AdamW hơn 1 điểm.

4.6 Huấn luyện từ đầu
Chúng tôi tiến hành một thí nghiệm sơ bộ với việc huấn luyện một mô hình từ đầu, cụ thể là một Wide ResNet 28-10 [107] trên CIFAR-10 [50]; xem Phụ lục E.8 để biết chi tiết. Bảng 2 cho thấy độ chính xác kiểm tra của checkpoint cuối cùng, với và không có polynomial averaging được sử dụng trong suốt các thí nghiệm của chúng tôi. Ở đây DoG hoạt động ngang bằng với quy trình huấn luyện tiêu chuẩn của thiết lập là SGD với momentum 0.9 và learning rate 0.1 [19]. Trong thiết lập này, Adam tạo ra kết quả kém hơn, và L-DoG tệ hơn 0.5 điểm so với Adam được điều chỉnh với learning rate tốt nhất, có thể do không đạt được hội tụ.

4.7 So sánh với các phương pháp không cần điều chỉnh khác
Chúng tôi thực hiện so sánh sơ bộ giữa DoG và L-DoG và các phương pháp khác để loại bỏ tham số learning rate: Stochastic Polyak Step [56], D-Adaptation [24] và Continuous Coin Betting (COCOB) [71]. Trong tất cả các trường hợp, chúng tôi thấy rằng DoG và L-DoG cung cấp hiệu suất tốt hơn trên hầu hết các tác vụ và trung bình (xem Bảng 6 và 7). Chúng tôi cung cấp kết quả chi tiết trong Phụ lục G, trong đó chúng tôi cũng thảo luận về triển vọng thực tế của quy trình chia đôi của Carmon và Hinder [11].

5 Công việc liên quan
Các nỗ lực trước đây để thiết kế các thuật toán tối ưu hóa có nguyên tắc lý thuyết và thực tế không yêu cầu điều chỉnh learning rate tiếp cận vấn đề từ nhiều góc độ khác nhau, dẫn đến một loạt lớn các thuật toán được đề xuất. Rolinek và Martius [81], Vaswani et al. [90], Paquette và Scheinberg [72] nâng các kỹ thuật tìm kiếm đường từ tối ưu hóa không ngẫu nhiên lên thiết lập ngẫu nhiên, trong khi Berrada et al. [9], Loizou et al. [56] làm tương tự cho kích thước bước Polyak cổ điển [76, 36]. Asi và Duchi [4] phát triển một lớp thuật toán dựa trên các phương pháp proximal ngẫu nhiên và chứng minh sự cải thiện tính mạnh mẽ của chúng cả về mặt lý thuyết và thực nghiệm. Schaul et al. [82] sử dụng một xấp xỉ bậc hai ngẫu nhiên để thiết kế learning rate tối đa hóa sự giảm mục tiêu một bước mong đợi. Chandra et al. [14] lồng hypergradient descent để tạo ra một phương pháp không nhạy cảm với các lựa chọn siêu tham số ban đầu. Tuy nhiên, không có kết quả nào trong số này là không tham số theo nghĩa giống như DoG: chúng hoặc không có bảo đảm hội tụ, hoặc có các chặn không tối ưu phình to đa thức khi các tham số của phương pháp không khớp với một giá trị phụ thuộc vào vấn đề. Ngược lại, các phương pháp không tham số có tốc độ hội tụ phụ thuộc tối đa logarithmically vào các tham số thuật toán.

Trong khi tài liệu tối ưu hóa không tham số tập trung chủ yếu vào các sơ đồ lý thuyết, một số công trình cũng bao gồm các nghiên cứu thực nghiệm [68, 71, 47, 15]. Đặc biệt, Orabona và Tommasi [71] xây dựng dựa trên các sơ đồ coin-betting để thiết kế một thuật toán huấn luyện mạng neural có bảo đảm hội tụ kiểu AdaGrad cho các hàm quasi-convex, cho thấy kết quả đầy hứa hẹn trên các vấn đề huấn luyện mạng neural. Trong công việc gần đây, Chen et al. [15] có được kết quả thực nghiệm cải thiện với một thuật toán tận dụng coin betting và truncated linear models. Tuy nhiên, phương pháp này thiếu bảo đảm lý thuyết.

Trong công việc độc lập gần đây, Defazio và Mishchenko [24] đề xuất một lịch trình kích thước bước động không tham số của dual averaging. Trong khi công việc của chúng tôi có cùng động lực và chia sẻ một số điểm tương đồng kỹ thuật (bao gồm việc sử dụng các chặn regret có trọng số và một Lemma 3 được thu được độc lập), các thuật toán được đề xuất khá khác nhau, và dual averaging hiếm khi được sử dụng trong huấn luyện mạng neural. (Xem thảo luận thêm trong Phụ lục G.3). Hơn nữa, Defazio và Mishchenko [24] chỉ chứng minh tốc độ hội tụ không tham số trong thiết lập không ngẫu nhiên, trong khi chúng tôi thiết lập bảo đảm xác suất cao trong thiết lập ngẫu nhiên. Đồng thời với công việc của chúng tôi, Defazio và Mishchenko [25] mở rộng heuristic sơ đồ dual averaging của họ cho các thuật toán giống SGD- và Adam, báo cáo kết quả thực nghiệm đầy hứa hẹn.

Cuối cùng, một số phương pháp tối ưu hóa mạng neural—LARS [104], LAMB [105], Adafactor [84], và Fromage [8]—sử dụng chuẩn của trọng số mạng neural để mở rộng kích thước bước. DoG và L-DoG tương tự trong việc cũng sử dụng một chuẩn để mở rộng kích thước bước của chúng, nhưng chúng khác với công việc trước đó bằng cách xem xét khoảng cách từ khởi tạo thay vì chuẩn của trọng số. Chúng tôi tin rằng sự khác biệt này quan trọng trong việc làm cho DoG không tham số, trong khi các phương pháp được đề cập ở trên có một tham số learning-rate để điều chỉnh (mặc dù Bernstein et al. [8] báo cáo rằng một giá trị mặc định duy nhất hoạt động tốt trên các tác vụ khác nhau).

6 Hạn chế và Triển vọng
Kết quả lý thuyết và thực nghiệm của chúng tôi đặt DoG như một bước đầy hứa hẹn hướng tới một thế hệ mới của các thuật toán tối ưu hóa không cần điều chỉnh có nguyên tắc và hiệu quả. Tuy nhiên, cần nhiều công việc bổ sung để các thuật toán này trở nên phổ biến. Đầu tiên, quan trọng là phải hiểu cách kết hợp đúng DoG với các kỹ thuật đã được chứng minh như momentum, learning rate theo tham số, và annealing learning rate—đây là một thách thức cả từ góc độ lý thuyết và thực tế. Thứ hai, quan trọng là phải hiểu rõ hơn về các tình huống mà DoG nhạy cảm hơn với việc chọn rϵ so với lý thuyết dự đoán. Các điều tra sơ bộ của chúng tôi cho thấy một mối liên hệ với batch normalization, và theo dõi manh mối đó có thể dẫn đến các phương pháp huấn luyện mạnh mẽ hơn nữa. Cuối cùng, trong khi các thí nghiệm của chúng tôi nhằm bao gồm một loạt rộng các tác vụ và kiến trúc, công việc tương lai cần khám phá DoG trong các thiết lập bổ sung, đặc biệt là những thiết lập liên quan đến huấn luyện từ đầu.

Lời cảm ơn
Chúng tôi cảm ơn Francesco Orabona, Mitchell Wortsman, Simon Kornblith và các reviewer ẩn danh vì những bình luận sâu sắc của họ. Công việc này được hỗ trợ bởi chương trình NSF-BSF, dưới grant NSF #2239527 và grant BSF #2022663. MI nhận được hỗ trợ từ hội đồng giáo dục đại học Israel. OH nhận được hỗ trợ từ Pitt Momentum Funds, và grant AFOSR #FA9550-23-1-0242. YC nhận được hỗ trợ từ grant Quỹ Khoa học Israel (ISF) số 2486/21, Alon Fellowship, Sáng kiến Yandex cho Machine Learning, và Len Blavatnik cùng Quỹ Gia đình Blavatnik.
