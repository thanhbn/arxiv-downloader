# Các điểm bất thường với tín hiệu đối lập có tác động lớn đến tối ưu hóa mạng thần kinh

Elan Rosenfeld
Đại học Carnegie Mellon
elan@cmu.edu

Andrej Risteski
Đại học Carnegie Mellon
aristesk@andrew.cmu.edu

## Tóm tắt

Chúng tôi xác định một hiện tượng mới trong tối ưu hóa mạng thần kinh phát sinh từ sự tương tác giữa độ sâu và cấu trúc đuôi dài đặc biệt trong dữ liệu tự nhiên. Kết quả của chúng tôi đưa ra lời giải thích trực quan cho một số quan sát đã được báo cáo trước đây về động lực học huấn luyện mạng. Đặc biệt, nó ngụ ý một nguyên nhân hoàn toàn mới cho hiện tượng tăng độ sắc nét tiến triển và ranh giới ổn định; chúng tôi cũng nhấn mạnh những kết nối với các khái niệm khác trong tối ưu hóa và tổng quát hóa bao gồm grokking, thiên hướng đơn giản, và Tối ưu hóa Nhận thức Độ sắc nét.

Thông qua thí nghiệm, chúng tôi chứng minh ảnh hưởng đáng kể của các nhóm điểm bất thường ghép đôi trong dữ liệu huấn luyện với tín hiệu đối lập mạnh: các đặc trưng nhất quán, có độ lớn lớn chi phối đầu ra mạng trong suốt quá trình huấn luyện và cung cấp các gradient chỉ theo hướng ngược lại. Do những điểm bất thường này, quá trình tối ưu hóa ban đầu đi vào một thung lũng hẹp cân bằng cẩn thận các nhóm đối lập; việc tăng độ sắc nét tiếp theo khiến tổn thất của chúng tăng nhanh, dao động giữa cao ở một nhóm rồi nhóm kia, cho đến khi tổn thất tổng thể tăng vọt. Chúng tôi mô tả cách xác định những nhóm này, khám phá điều gì khiến chúng khác biệt, và nghiên cứu kỹ lưỡng tác động của chúng đối với tối ưu hóa và hành vi của mạng. Chúng tôi bổ sung những thí nghiệm này bằng lời giải thích cơ chế trên một ví dụ đơn giản về tín hiệu đối lập và phân tích lý thuyết của mạng tuyến tính hai lớp trên mô hình đơn giản. Phát hiện của chúng tôi cho phép dự đoán định tính mới về hành vi huấn luyện mà chúng tôi xác nhận thông qua thí nghiệm. Nó cũng cung cấp góc nhìn mới để nghiên cứu và cải thiện các phương pháp huấn luyện hiện đại cho tối ưu hóa ngẫu nhiên, điều mà chúng tôi nhấn mạnh qua nghiên cứu trường hợp về Adam so với SGD.

## 1 Giới thiệu

Có một danh sách ngày càng tăng các đặc tính thú vị của tối ưu hóa mạng thần kinh (NN) không được giải thích dễ dàng bằng các công cụ cổ điển từ tối ưu hóa. Tương tự, chúng ta có mức độ hiểu biết khác nhau về các nguyên nhân cơ chế cho từng đặc tính. Những nỗ lực rộng lớn đã dẫn đến các lời giải thích khả thi cho hiệu quả của Adam (Kingma và Ba, 2014), Chuẩn hóa Batch (Ioffe và Szegedy, 2015) và các công cụ khác cho việc huấn luyện thành công—nhưng bằng chứng không phải lúc nào cũng hoàn toàn thuyết phục, và chắc chắn có rất ít hiểu biết lý thuyết. Các phát hiện khác, như grokking (Power et al., 2022) hay ranh giới ổn định (Cohen et al., 2021), không có ý nghĩa thực tiễn trực tiếp nhưng cung cấp cách thức mới để nghiên cứu điều gì khiến tối ưu hóa NN khác biệt. Những hiện tượng này thường được xem xét riêng lẻ—mặc dù chúng không hoàn toàn khác biệt, nhưng không biết các nguyên nhân cơ bản cụ thể mà chúng có thể chia sẻ. Rõ ràng, việc hiểu biết tốt hơn về động lực học huấn luyện NN trong bối cảnh cụ thể có thể dẫn đến những cải tiến thuật toán (Chen et al., 2021); điều này gợi ý rằng bất kỳ điểm chung nào cũng sẽ là công cụ có giá trị cho việc điều tra sâu hơn.

Trong nghiên cứu này, chúng tôi xác định một hiện tượng trong tối ưu hóa NN đưa ra góc nhìn mới về nhiều quan sát trước đây này và mà chúng tôi hy vọng sẽ góp phần vào hiểu biết sâu sắc hơn về cách chúng có thể được kết nối. Mặc dù chúng tôi không (và không tuyên bố) đưa ra lời giải thích hoàn chỉnh, chúng tôi trình bày bằng chứng định tính và định lượng mạnh mẽ cho một ý tưởng cấp cao duy nhất—một ý tưởng tự nhiên phù hợp với một số câu chuyện hiện có và gợi ý một bức tranh mạch lạc hơn về nguồn gốc của chúng.

Cụ thể, chúng tôi chứng minh sự phổ biến của các nhóm điểm bất thường ghép đôi trong dữ liệu tự nhiên có ảnh hưởng đáng kể đến động lực học tối ưu hóa của mạng. Những nhóm này được đặc trưng bởi việc bao gồm một hoặc nhiều đặc trưng có độ lớn (tương đối) lớn chi phối đầu ra của mạng khi khởi tạo và trong suốt hầu hết quá trình huấn luyện. Ngoài độ lớn, tính chất đặc biệt khác của những đặc trưng này là chúng cung cấp các gradient lớn, nhất quán và đối lập, theo nghĩa là tuân theo gradient của một nhóm để giảm tổn thất của nó sẽ tăng tổn thất của nhóm kia một lượng tương tự. Do cấu trúc này, chúng tôi gọi chúng là Tín hiệu Đối lập. Những đặc trưng này có tương quan không tầm thường với nhiệm vụ mục tiêu, nhưng chúng thường không phải là tín hiệu "đúng" (ví dụ, phù hợp với con người). Thực tế, trong nhiều trường hợp, những đặc trưng này hoàn toàn thể hiện câu đố thống kê cổ điển về "tương quan so với nhân quả"—ví dụ, bầu trời xanh sáng không quyết định nhãn của hình ảnh CIFAR, nhưng nó thường xuất hiện nhất trong hình ảnh của máy bay. Các đặc trưng khác rất có liên quan, chẳng hạn như sự hiện diện của bánh xe và đèn pha trong hình ảnh xe tải và ô tô, hoặc thực tế là dấu hai chấm thường đi trước "the" hoặc token xuống dòng trong văn bản viết.

Tín hiệu đối lập được hiểu dễ nhất qua một ví dụ, mà chúng tôi sẽ đưa ra cùng với phác thảo ngắn gọn về tác động của chúng đối với động lực học huấn luyện; mô tả chi tiết hơn được trình bày trong Phần 3. Hình 1 mô tả tổn thất huấn luyện của ResNet-18 (He et al., 2016) được huấn luyện với gradient descent toàn batch (GD) trên CIFAR-10 (Krizhevsky và Hinton, 2009), cùng với một vài nhóm bất thường chủ đạo và tổn thất tương ứng của chúng. Trong giai đoạn đầu của quá trình huấn luyện, mạng đi vào một thung lũng hẹp trong không gian trọng số cân bằng cẩn thận các gradient đối lập của các cặp; việc tăng độ sắc nét tiếp theo của cảnh quan tổn thất (Jastrzębski et al., 2020; Cohen et al., 2021) khiến mạng dao động với biên độ ngày càng tăng dọc theo các trục cụ thể, làm đảo lộn sự cân bằng này. Quay lại ví dụ về nền trời của chúng tôi, một bước dẫn đến việc lớp máy bay được gán xác suất lớn hơn cho tất cả hình ảnh có trời, và bước tiếp theo sẽ đảo ngược hiệu ứng đó. Về bản chất, mạng con "trời = máy bay" tăng và giảm. Kết quả trực tiếp của dao động này là tổn thất của mạng trên hình ảnh máy bay có nền trời sẽ luân phiên tăng giảm mạnh với biên độ ngày càng tăng, với điều ngược lại hoàn toàn xảy ra đối với hình ảnh không phải máy bay có trời. Do đó, gradient của những nhóm này sẽ luân phiên hướng trong khi cũng tăng về độ lớn. Vì những cặp này đại diện cho một phần nhỏ dữ liệu, hành vi này không rõ ràng ngay từ tổn thất huấn luyện tổng thể—nhưng cuối cùng, nó tiến triển đủ xa đến mức tổn thất tổng thể tăng vọt. Vì có sự tương ứng trực tiếp rõ ràng giữa hai sự kiện này trong suốt quá trình, chúng tôi đưa ra giả thuyết rằng tín hiệu đối lập là nguyên nhân trực tiếp của hiện tượng ranh giới ổn định (Cohen et al., 2021). Chúng tôi cũng lưu ý rằng các tín hiệu có ảnh hưởng nhất dường như tăng về độ phức tạp theo thời gian (Nakkiran et al., 2019).

Chúng tôi lặp lại thí nghiệm này trên một loạt kiến trúc thị giác và siêu tham số huấn luyện: mặc dù các nhóm chính xác và thứ tự xuất hiện của chúng thay đổi, mẫu xảy ra một cách nhất quán. Chúng tôi cũng xác minh hành vi này cho transformer trên dự đoán token tiếp theo của văn bản tự nhiên và MLP ReLU nhỏ trên các hàm 1D đơn giản; chúng tôi đưa ra một số ví dụ về tín hiệu đối lập trong văn bản ở Phụ lục B. Tuy nhiên, chúng tôi dựa vào hình ảnh để trình bày vì nó cung cấp trực quan rõ ràng nhất. Để cô lập hiệu ứng này, hầu hết các thí nghiệm của chúng tôi sử dụng GD, nhưng chúng tôi quan sát các mẫu tương tự trong SGD mà chúng tôi trình bày trong Phần 4.

**Tóm tắt đóng góp.** Đóng góp chính của bài báo này là chứng minh sự tồn tại, tính phổ biến và ảnh hưởng lớn của tín hiệu đối lập trong quá trình tối ưu hóa NN. Chúng tôi tiếp tục trình bày hiểu biết tốt nhất hiện tại của chúng tôi, với các thí nghiệm hỗ trợ, về cách những tín hiệu này gây ra động lực học huấn luyện được quan sát—đặc biệt, chúng tôi cung cấp bằng chứng rằng đó là hệ quả của độ sâu và các phương pháp gradient dốc nhất. Chúng tôi bổ sung cuộc thảo luận này bằng một ví dụ đơn giản và phân tích mạng tuyến tính hai lớp trên mô hình đơn giản. Đáng chú ý, mặc dù còn sơ khai, lời giải thích của chúng tôi cho phép dự đoán định tính cụ thể về hành vi NN trong quá trình huấn luyện, mà chúng tôi xác nhận thông qua thí nghiệm. Nó cũng cung cấp góc nhìn mới để nghiên cứu các phương pháp tối ưu hóa ngẫu nhiên hiện đại, mà chúng tôi nhấn mạnh qua nghiên cứu trường hợp về SGD so với Adam. Chúng tôi thấy những kết nối có thể giữa tín hiệu đối lập và nhiều hiện tượng trong tối ưu hóa và tổng quát hóa NN, bao gồm grokking (Power et al., 2022), catapulting/slingshotting (Lewkowycz et al., 2020; Thilak et al., 2022), thiên hướng đơn giản (Valle-Perez et al., 2019), giảm kép (Belkin et al., 2019; Nakkiran et al., 2020), và Tối ưu hóa Nhận thức Độ sắc nét (Foret et al., 2021). Chúng tôi thảo luận về những kết nối này và các kết nối khác trong Phần 5.

## 2 Đặc trưng và Xác định Tín hiệu Đối lập

Mặc dù ảnh hưởng của chúng đối với các chỉ số tổng hợp không rõ ràng, việc xác định các điểm bất thường với tín hiệu đối lập là đơn giản. Phương pháp của chúng tôi như sau: khi huấn luyện mạng với GD, chúng tôi theo dõi tổn thất của nó trên từng điểm huấn luyện riêng lẻ. Đối với một lần lặp nhất định, chúng tôi chọn các điểm huấn luyện có tổn thất thể hiện thay đổi tích cực nhất và tiêu cực nhất trong bước trước đó (có sự chồng chéo lớn giữa các tập hợp này trong các bước liên tiếp). Tập hợp này đôi khi sẽ chứa nhiều tín hiệu đối lập, mà chúng tôi phân biệt qua kiểm tra trực quan. Chi tiết cuối cùng này có nghĩa là các hình ảnh chúng tôi mô tả không ngẫu nhiên, nhưng chúng tôi nhấn mạnh rằng sẽ không đúng khi mô tả quá trình này như việc chọn lọc có chủ đích: mặc dù việc định lượng chính xác là khó khăn, những tín hiệu này luôn tuân theo câu châm ngôn "tôi biết khi tôi thấy nó". Điều này đặc biệt đúng với hình ảnh, chẳng hạn như các nhóm trong Hình 1 có các mẫu dễ nhận biết. Để chứng minh thực tế này tổng quát hơn, Phụ lục H chứa các mẫu trước kiểm tra cho ResNet-18, VGG-11 (Simonyan và Zisserman, 2014), và Vision Transformer nhỏ (Dosovitskiy et al., 2020) tại một số bước huấn luyện và với nhiều seed; chúng tôi tin rằng các nhóm ngụ ý là tức thì, ngay cả khi không hoàn toàn khách quan. Chúng tôi coi các cách tiếp cận thuật toán để tự động gom cụm những mẫu này là hướng nghiên cứu trong tương lai—ví dụ, người ta có thể chọn mẫu theo tương quan trong chuỗi thời gian tổn thất của chúng, hoặc theo sự căn chỉnh gradient.

**Đo lường các chỉ số thay thế.** Với cách những mẫu này được chọn, một số đặc trưng khác có vẻ phù hợp. Chẳng hạn, thay đổi tổn thất một bước thường là proxy hợp lý cho chuẩn gradient; chúng ta cũng có thể xem xét giá trị eigen lớn nhất của tổn thất của điểm riêng lẻ, hoặc nó có bao nhiêu độ cong theo hướng của eigenvector đầu của tổn thất tổng thể. Đối với mạng lớn, những tùy chọn này tốn nhiều tính toán hơn phương pháp chúng tôi đã chọn, nhưng chúng ta có thể đánh giá chúng trên các nhóm cụ thể. Trong Hình 2, chúng tôi theo dõi những chỉ số này cho một số cặp nhóm đối lập và chúng tôi thấy rằng chúng luôn lớn hơn nhiều so với các mẫu ngẫu nhiên từ tập huấn luyện.

### 2.1 Về Khả năng Định nghĩa Chính thức

Mặc dù các đặc trưng và mẫu điển hình của chúng có thể nhận biết ngay lập tức, chúng tôi không cố gắng định nghĩa chính xác một "đặc trưng", cũng không định nghĩa "điểm bất thường" liên quan đến đặc trưng đó. Sự hiện diện của một đặc trưng cụ thể thường mơ hồ, và khó định nghĩa ngưỡng rõ ràng cho điều gì khiến một điểm nhất định trở thành điểm bất thường. Do đó, thay vì cố gắng phân vùng dữ liệu một cách chính xác, chúng tôi chỉ lưu ý rằng những đuôi dài này tồn tại và chúng tôi sử dụng các điểm bất thường rõ ràng nhất làm đại diện cho việc trực quan hóa. Trong Hình 1 và 2, chúng tôi chọn một ngưỡng tùy ý là hai mươi mẫu mỗi nhóm.

Chúng tôi cũng lưu ý rằng điều gì đủ tiêu chuẩn làm tín hiệu đối lập hoặc điểm bất thường có thể thay đổi theo thời gian. Để rõ ràng về mặt hình ảnh, Hình 1 chỉ mô tả tổn thất trên cặp nhóm chiếm ưu thế nhất trong giai đoạn huấn luyện tương ứng, nhưng mẫu này xảy ra đồng thời cho nhiều tín hiệu khác nhau và ở nhiều quy mô khác nhau trong suốt quá trình huấn luyện. Hơn nữa, các tín hiệu đối lập là đối với các biểu diễn nội bộ của mô hình (và nhãn), không phải không gian đầu vào chính nó; điều này có nghĩa là định nghĩa cũng là tính chất của kiến trúc. Ví dụ, theo Cohen et al. (2021), chúng tôi huấn luyện một MLP nhỏ để khớp với đa thức Chebyshev trên các điểm cách đều trong khoảng [−1,1] (Hình 3). Dữ liệu này không có "điểm bất thường" theo nghĩa truyền thống, và không rõ ràng ngay tín hiệu đối lập nào có mặt. Tuy nhiên, chúng tôi quan sát cùng hành vi luân phiên: chúng tôi tìm thấy một cặp trong đó một nhóm là khoảng nhỏ các giá trị x và nhóm đối lập chứa các láng giềng của nó, tất cả trong phạm vi [−1,−0.5]. Điều này gợi ý rằng mạng có các kích hoạt nội bộ chỉ có ảnh hưởng lớn đối với các giá trị x âm hơn. Trong bối cảnh này, hai nhóm này là các điểm bất thường.

## 3 Hiểu về Tác động của Tín hiệu Đối lập

Ngoài việc ghi nhận sự tồn tại của chúng, mục tiêu cuối cùng của chúng tôi sẽ là rút ra những hiểu biết khả thi từ phát hiện này. Để làm điều này, cần phải hiểu rõ hơn về cách những điểm bất thường này gây ra hành vi được quan sát. Trong phần này, chúng tôi đưa ra "bức tranh tinh thần" đơn giản hóa phục vụ như hiểu biết hiện tại của chúng tôi về quá trình này. Chúng tôi bắt đầu với cuộc thảo luận không chính thức về ảnh hưởng lớn của tín hiệu đối lập và cách chúng dẫn đến tăng độ sắc nét tiến triển; tiểu mục này tập hợp và mở rộng các nghiên cứu trước để đưa ra bối cảnh quan trọng về cách những tín hiệu này khác với "nhiễu" thường được tưởng tượng. Tiếp theo, chúng tôi đưa ra mô tả cơ chế về tác động cụ thể của tín hiệu đối lập với một ví dụ đơn giản. Lời giải thích này có chủ ý ở cấp độ cao, nhưng chúng tôi cuối cùng sẽ thấy cách nó đưa ra dự đoán cụ thể về các hành vi cụ thể, mà sau đó chúng tôi xác minh trên các mạng thực. Cuối cùng, chúng tôi chứng minh rằng hành vi này xảy ra trên mạng tuyến tính hai lớp dưới mô hình đơn giản.

### 3.1 Tăng Độ sắc nét Tiến triển, và Trực quan về Tại sao những Đặc trưng này có Ảnh hưởng lớn

Ở mức độ cao, hầu hết sự biến đổi trong đầu vào là không cần thiết khi huấn luyện mạng để tối thiểu hóa lỗi dự đoán—đặc biệt với độ sâu và chiều cao, chỉ một phần nhỏ thông tin sẽ được truyền đến lớp tuyến tính cuối cùng (Huh et al., 2021). Bắt đầu từ khởi tạo ngẫu nhiên, việc huấn luyện mạng căn chỉnh các giá trị đơn lẻ của các lớp liền kề (Saxe et al., 2013; Mulayoff và Michaeli, 2020) để khuếch đại tín hiệu có ý nghĩa trong khi giảm trọng lượng nhiễu, tăng độ nhạy với tín hiệu quan trọng. Độ nhạy này có thể được đo lường, ví dụ, bằng chuẩn phổ của Jacobian đầu vào-đầu ra, tăng trong quá trình huấn luyện (Ma và Ying, 2021); nó cũng đã được kết nối với sự tăng trưởng trong chuẩn của lớp đầu ra (Wang et al., 2022).

Quan sát rằng với sự tăng trưởng này, những thay đổi nhỏ trong cách mạng xử lý đầu vào trở nên có ảnh hưởng hơn. Giả thuyết, một nhiễu động trọng số nhỏ có thể tăng tổn thất lớn bằng cách chuyển hướng nhiễu không hữu ích đến không gian con mà mạng nhạy cảm nhất, hoặc bằng cách thay đổi cách lớp cuối sử dụng nó. Sự gia tăng của độ nhạy này do đó đại diện chính xác cho sự tăng trưởng của phổ Hessian tổn thất, với cường độ của hiệu ứng này tăng theo độ sâu (Wang et al., 2016; Du et al., 2018; Mulayoff và Michaeli, 2020).

Quan trọng, việc tăng độ sắc nét này cũng phụ thuộc vào cấu trúc của đầu vào. Nếu nhiễu độc lập với mục tiêu, nó sẽ được giảm trọng lượng trong suốt quá trình huấn luyện. Ngược lại, các tín hiệu thực sự đối lập nhau sẽ được giữ lại và thậm chí có thể được khuếch đại thêm bởi gradient descent; điều này là do đặc trưng "đúng" có thể nhỏ hơn nhiều về độ lớn (hoặc chưa được học), vì vậy việc sử dụng đặc trưng lớn, "không đúng" thường là cách tức thì nhất để tối thiểu hóa tổn thất. Như một ví dụ cụ thể, quan sát rằng một mạng được khởi tạo ngẫu nhiên sẽ thiếu các đặc trưng cần thiết cho nhiệm vụ tinh tế là phân biệt chim với máy bay. Nhưng nó sẽ nắm bắt sự hiện diện của bầu trời, rất hữu ích để giảm tổn thất trên những hình ảnh như vậy bằng cách dự đoán điều kiện p(lớp|trời) (điều này tương tự như hành vi "tuyến tính/nông trước" được mô tả bởi Nakkiran et al. (2019); Mangalam và Prabhu (2019)). Do đó, bất kỳ phương pháp nào cố gắng tối thiểu hóa tổn thất nhanh nhất có thể (ví dụ, gradient dốc nhất) thực sự có thể tăng trọng lượng những đặc trưng này. Hơn nữa, các tín hiệu đối lập được khuếch đại sẽ gây ra độ sắc nét lớn hơn nhiễu ngẫu nhiên, vì việc sử dụng tín hiệu có lợi cho một nhóm là có hại tối đa cho nhóm kia—ví dụ, tự tin dự đoán máy bay bất cứ khi nào có trời sẽ gây ra tổn thất lớn trên hình ảnh của các lớp khác có trời. Vì nhiễu ngẫu nhiên khuếch tán hơn, hiệu ứng này ít rõ ràng hơn.

Mô tả này có phần trừu tượng. Để hiểu rõ hơn, chúng tôi minh họa động lực học một cách rõ ràng hơn trên một ví dụ đơn giản.

### 3.2 Minh họa với Ví dụ Giả thuyết về Gradient Descent

Xem xét cảnh quan tổn thất toàn cục của một mạng thần kinh: đây là hàm mô tả cách tổn thất thay đổi khi chúng ta di chuyển qua không gian tham số. Giả sử chúng ta xác định một hướng trong không gian này tương ứng với việc mạng sử dụng đặc trưng "trời" để dự đoán máy bay so với một số lớp khác. Nghĩa là, chúng ta sẽ tưởng tượng rằng bất cứ khi nào hình ảnh đầu vào bao gồm nền xanh sáng, việc di chuyển tham số theo một hướng sẽ tăng logit của lớp máy bay và giảm các lớp khác, và ngược lại. Chúng ta cũng sẽ phân tách tổn thất này—trong số các hình ảnh có nền trời, chúng ta xem xét riêng biệt tổn thất trên những hình được gán nhãn máy bay so với những hình có nhãn khác. Vì đặc trưng trời có độ lớn lớn, một thay đổi nhỏ trong không gian trọng số sẽ tạo ra thay đổi lớn trong đầu ra mạng—nghĩa là, một chuyển động nhỏ theo hướng "trời = máy bay" sẽ tăng tổn thất lớn trên những hình ảnh không phải máy bay này.

Hình 4 mô tả kịch bản đơn giản hóa mạnh này. Sớm trong quá trình huấn luyện, việc tối ưu hóa mạng này với GD sẽ nhanh chóng di chuyển về phía cực tiểu dọc theo hướng này. Đặc biệt, cho đến khi các đặc trưng tốt hơn được học, hướng gradient dốc nhất sẽ dẫn đến một mạng tăng trọng lượng đặc trưng trời và dự đoán p(lớp|trời) bất cứ khi nào nó xuất hiện. Một khi đủ gần với cực tiểu, gradient sẽ chỉ "qua thung lũng" về phía khuếch đại tín hiệu có liên quan hơn (Xing et al., 2018). Tuy nhiên, điều này cũng sẽ khiến đặc trưng trời tăng về độ lớn—cũng như ảnh hưởng tiềm tăng của nó nếu trọng số bị nhiễu động có chọn lọc, như mô tả ở trên. Cả hai yếu tố này đều góp phần vào tăng độ sắc nét tiến triển.

Ở đây chúng tôi nhấn mạnh sự phân biệt giữa tổn thất trên các điểm bất thường và tổn thất huấn luyện toàn bộ. Vì các hình ảnh không có trời không gần như nhạy cảm với chuyển động dọc theo trục này, gradient và độ cong của chúng nhỏ hơn nhiều—và vì chúng bao gồm phần lớn tập dữ liệu, cảnh quan tổn thất toàn cục ban đầu có thể không bị ảnh hưởng đáng kể. Việc tối ưu hóa tiếp tục sẽ dao động qua cực tiểu với biên độ ngày càng tăng, nhưng sự tăng trưởng này có thể không rõ ràng ngay lập tức. Hơn nữa, tiến triển trực giao với những dao động này không cần bị ảnh hưởng—chúng tôi tìm thấy một số bằng chứng rằng hai quá trình này xảy ra phần nào độc lập, mà chúng tôi trình bày trong Phần 4. Quay lại phân tách tổn thất, chúng ta thấy rằng những dao động này sẽ khiến tổn thất tăng và luân phiên, với một nhóm có tổn thất cao rồi nhóm kia. Cuối cùng, tổn thất của các điểm bất thường tăng đủ và tổn thất tổng thể tăng vọt, hoặc làm phẳng thung lũng và quay lại giai đoạn đầu, hoặc "bắn" đến một lưu vực khác (Wu et al., 2018; Lewkowycz et al., 2020; Thilak et al., 2022). Hiện tượng này được mô tả trong Hình 1. Cuối cùng, chúng tôi lưu ý rằng nếu người ta trực quan hóa động lực học trong Hình 4 từ trên xuống—vì vậy hướng trái/phải trên trang trở thành lên/xuống—nó cho chính xác mẫu của trọng số mạng được chiếu lên eigenvector đầu của Hessian (ví dụ, Hình 6(b) sau này trong nghiên cứu này).

**Xác minh dự đoán của ví dụ đơn giản chúng tôi.** Mặc dù lời giải thích này thiếu chi tiết chính xác, nó cho phép dự đoán cụ thể về hành vi mạng trong quá trình huấn luyện. Hình 5 theo dõi các dự đoán của ResNet-18 trên hình ảnh trời—để loại bỏ các yếu tố gây nhiễu có thể, chúng tôi tạo hình ảnh tổng hợp như một khối màu duy nhất. Mặc dù ví dụ "máy bay so với khác" có vẻ quá đơn giản, chúng tôi thấy chính xác hành vi được mô tả—hội tụ ban đầu đến cực tiểu cùng với tăng trưởng nhanh trong chuẩn đặc trưng, tiếp theo là dao động trong xác suất lớp. Theo thời gian, mạng học sử dụng tín hiệu khác và giảm trọng lượng đặc trưng trời, như được chứng minh bởi sự suy giảm chậm trong chuẩn đặc trưng. Chúng tôi tái tạo hình này cho nhiều đầu vào khác và cho VGG-11-BN trong Phụ lục C, với những phát hiện tương tự.

Ví dụ của chúng tôi cũng gợi ý rằng dao động đóng vai trò như một bộ điều hòa có giá trị giúp giảm sự phụ thuộc vào các tín hiệu đối lập dễ học có thể không tổng quát hóa. Khi một tín hiệu được sử dụng có lợi cho một nhóm và bất lợi cho nhóm khác, tổn thất của nhóm được lợi giảm xuống trong khi nhóm kia tăng lên, có nghĩa là gradient của nhóm sau tăng về độ lớn trong khi nhóm trước giảm. Vì nhóm hiện chiếm ưu thế gradient cũng là nhóm bị bất lợi bởi việc sử dụng tín hiệu này, mạng sẽ được khuyến khích giảm trọng lượng đặc trưng này. Trong Phụ lục C.3, chúng tôi tái tạo Hình 5 với VGG-11-BN được huấn luyện với tốc độ học rất nhỏ để xấp xỉ chặt chẽ dòng chảy gradient. Chúng tôi thấy rằng dòng chảy gradient và GD rất giống nhau cho đến khi đạt ranh giới ổn định. Sau điểm này, chuẩn đặc trưng dưới GD bắt đầu suy giảm chậm trong khi dao động; ngược lại, trong trường hợp không có dao động, chuẩn đặc trưng của tín hiệu đối lập dưới dòng chảy gradient tăng liên tục. Nếu đúng là các tín hiệu đối lập đại diện cho các đặc trưng "đơn giản" tổng quát hóa kém hơn, điều này có thể giúp giải thích tổng quát hóa kém của dòng chảy gradient. Một hiệu ứng tương tự được quan sát bởi Jastrz e ¸bski et al. (2020), người đã lưu ý rằng tốc độ học ban đầu lớn dẫn đến cảnh quan tổn thất được điều hòa tốt hơn sau này.

### 3.3 Phân tích Lý thuyết của Tín hiệu Đối lập trong Mô hình Đơn giản

Để chứng minh hiệu ứng này một cách chính thức, chúng tôi nghiên cứu hồi quy tuyến tính sai lệch trên đầu vào x∈R^d với mạng tuyến tính hai lớp. Mặc dù mô hình này được đơn giản hóa, nó cho phép hiểu biết sơ bộ về các yếu tố mà chúng tôi nghĩ là quan trọng nhất để những động lực học này xảy ra. Vì chúng tôi đang phân tích động lực học từ khởi tạo cho đến ngưỡng ổn định, sẽ đủ để nghiên cứu quỹ đạo của dòng chảy gradient—đối với kích thước bước hợp lý η, một kết quả tương tự sau đó theo cho gradient descent.

Phân tích của chúng tôi tái tạo giai đoạn ban đầu của việc nhanh chóng giảm tổn thất trên các điểm bất thường, tiếp theo là sự tăng trưởng tiếp theo trong độ nhạy với cách tín hiệu đối lập được sử dụng—nghĩa là, tăng độ sắc nét tiến triển. Chúng tôi cũng xác minh mẫu này (và dao động tiếp theo, mà chúng tôi không chứng minh chính thức) trong các thí nghiệm trên dữ liệu thực và tổng hợp trong Phụ lục E.

Chúng tôi nhận xét rằng một yếu tố liên quan mà mô hình của chúng tôi thiếu là khái niệm về tín hiệu "hữu ích một phần" như được mô tả ở cuối Phần 3.1. Điều này dường như đòi hỏi một mô hình phức tạp hơn để nắm bắt đúng (ví dụ, hồi quy logistic đa thức) vì vậy chúng tôi coi phân tích này như một cuộc điều tra sơ bộ, chỉ nắm bắt một phần các khía cạnh liên quan của hiện tượng chúng tôi quan sát.

**Mô hình.** Chúng tôi mô hình hóa các đặc trưng quan sát được như một phân phối trên x∈R^d1, giả sử chỉ rằng hiệp phương sai Σ tồn tại—để rõ ràng chúng tôi coi Σ = I trong văn bản chính. Chúng tôi tiếp tục mô hình hóa một vector bổ sung x_o∈R^d2 đại diện cho tín hiệu đối lập, với d2≥d1. Chúng tôi sẽ giả sử rằng trên một phần nhỏ các điểm bất thường p≪1, x_o∼Unif{±q α/√d2 1} (1 là vector toàn một) với một số α điều chỉnh độ lớn đặc trưng, và chúng tôi để nó bằng 0 trên phần còn lại của tập dữ liệu. Chúng tôi mô hình hóa mục tiêu như hàm tuyến tính y=β^T x+1/√d2 1^T |x_o|; điều này nắm bắt ý tưởng rằng tín hiệu x_o tương quan mạnh với mục tiêu, nhưng theo hướng đối lập có cường độ bằng nhau. Cuối cùng, chúng tôi tham số hóa mạng với vector b∈R^d1, b_o∈R^d2 và vô hướng c trong một vector đơn θ, như f_θ(x) = c·(b^T x + b_o^T x_o).

Lưu ý rằng phân phối cụ thể của x_o không quan trọng—hơn nữa, trong các mô phỏng của chúng tôi, chúng tôi quan sát chính xác cùng mẫu với tổn thất cross-entropy. Từ các thí nghiệm và phân tích này, có vẻ như độ sâu và tỷ lệ tín hiệu trên nhiễu nhỏ là các yếu tố duy nhất cần thiết để hành vi này phát sinh.

**Thiết lập.** Một khởi tạo tiêu chuẩn sẽ là lấy mẫu [b, b_o]^T ∼ N(0, 1/(d1+d2) I), sau đó sẽ ngụ ý các phân phối tập trung cao cho các đại lượng quan tâm. Vì việc theo dõi các số hạng tập trung chính xác sẽ không đóng góp có ý nghĩa vào phân tích, chúng tôi đơn giản hóa bằng cách trực tiếp giả định rằng khi khởi tạo, những đại lượng này bằng thứ tự độ lớn kỳ vọng của chúng: ||b||_2^2 = 1^T b = d1/(d1+d2), ||b_o||_2^2 = 1^T b_o = d2/(d1+d2), và b^T β = ||β||/√(d1+d2). Tương tự, chúng tôi đặt c = 1, đảm bảo rằng cả hai lớp có cùng chuẩn. Chúng tôi thực hiện hồi quy tuyến tính tiêu chuẩn bằng cách tối thiểu hóa tổn thất tổng thể L(θ) := 1/2 E[(f_θ(x) - y)^2]. Chúng tôi thấy rằng bộ tối thiểu hóa của mục tiêu này có b_o = 0 và cb = β. Tuy nhiên, phân tích dòng chảy gradient sẽ làm sáng tỏ cách độ sâu và tín hiệu đối lập mạnh dẫn đến tăng độ sắc nét khi tiếp cận cực tiểu này.

**Kết quả.** Trong việc khám phá tăng độ sắc nét tiến triển, Cohen et al. (2021) thấy rằng đôi khi mô hình sẽ có sự giảm ngắn gọn về độ sắc nét, đặc biệt đối với tổn thất bình phương. Thực tế, điều này phù hợp với lời giải thích ở trên của chúng tôi: đối với α lớn hơn và tổn thất sắc nét hơn (ví dụ tổn thất bình phương), mạng ban đầu sẽ ưu tiên tối thiểu hóa tổn thất trên các điểm bất thường, do đó giảm mạnh độ sắc nét. Kết quả đầu tiên của chúng tôi chứng minh rằng điều này xảy ra khi có các tín hiệu đối lập độ lớn lớn:

**Định lý 3.1 (Giảm độ sắc nét ban đầu).** Đặt k := d2/d1 ≥ 1, và giả sử ||β|| > max{d1/√(d1+d2), 24/5}. Khi khởi tạo, độ sắc nét ||∇^2_θ L(θ)||_2 nằm trong (α, 3α). Hơn nữa, nếu √α = Ω(||β||k ln k), thì cả b_o và độ sắc nét tổng thể sẽ giảm như Õ(e^(-αt)) từ t = 0 cho đến một thời điểm t_1 ≤ ln||β||/2/2||β||.

Chứng minh có thể tìm thấy trong Phụ lục G. Sau sự giảm này, việc khuếch đại tín hiệu có thể tiếp tục—nhưng điều này cũng có nghĩa là độ sắc nét liên quan đến cách mạng sử dụng đặc trưng x_o sẽ tăng, vì vậy một nhiễu động nhỏ đến các tham số b_o sẽ tạo ra sự gia tăng lớn trong tổn thất.

**Định lý 3.2 (Tăng độ sắc nét tiến triển).** Nếu √α = Ω(1 + ||β||^2 k ln k), thì bắt đầu từ thời điểm t_1, độ sắc nét sẽ tăng tuyến tính theo ||β|| cho đến một thời điểm t_2 ≥ 1/2 ||β||_2^2, đạt ít nhất 5/8 ||β||α. Cận dưới này về độ sắc nét áp dụng cho từng chiều của b_o.

Dao động sẽ không xảy ra trong dòng chảy gradient—nhưng đối với gradient descent với kích thước bước η > 16/(5||β||α), b_o sẽ bắt đầu tăng về độ lớn trong khi dao động qua gốc tọa độ. Nếu sự tăng trưởng này tiếp tục, nó sẽ nhanh chóng tái giới thiệu đặc trưng, khiến tổn thất trên các điểm bất thường tăng và luân phiên. Việc tái giới thiệu như vậy (một ví dụ xảy ra khoảng lần lặp 3000 trong Hình 5) có vẻ có khả năng hữu ích cho việc khám phá. Trong Hình 38 trong Phụ lục, chúng tôi mô phỏng mô hình của chúng tôi và xác minh chính xác chuỗi sự kiện này. Chúng tôi cũng cho thấy rằng một MLP được huấn luyện trên CIFAR-10 hiển thị cùng hành vi đặc trưng.

### 3.4 Các Phát hiện Bổ sung

**Độ sắc nét thường xảy ra áp đảo trong vài lớp đầu tiên.** Định lý 3.2 cho thấy rằng tăng độ sắc nét tiến triển xảy ra cụ thể trong b_o. Nói chung, mô hình của chúng tôi gợi ý rằng độ sắc nét sẽ bắt đầu ở lớp cuối nhưng sớm trong quá trình huấn luyện, nó sẽ chuyển sang các lớp trước vì chúng có khả năng chuyển hướng tín hiệu nhiều hơn. Trong Phụ lục D, chúng tôi theo dõi phần độ cong của eigenvector đầu nằm trong mỗi lớp của các mạng khác nhau trong quá trình huấn luyện với GD. Trong ResNet-18, độ sắc nét xảy ra hầu như hoàn toàn trong lớp tích chập đầu tiên sau vài bước huấn luyện đầu tiên; cùng mẫu xuất hiện chậm hơn khi huấn luyện VGG-11. Trong Vision Transformer, độ cong xảy ra áp đảo trong lớp nhúng và rất nhẹ trong các đầu chiếu MLP trước đó. Transformer văn bản (NanoGPT) theo cùng mẫu, mặc dù với sự tập trung ít cực đoan hơn trong nhúng. Do đó, có vẻ như đúng là các lớp trước có độ sắc nét quan trọng nhất—đặc biệt nếu chúng thực hiện giảm chiều hoặc có ảnh hưởng cụ thể đến cách tín hiệu được truyền đến các lớp sau. Điều này có vẻ là nguyên nhân có thể của gradient lớn trong các lớp đầu của mô hình thị giác (Chen et al., 2021; Kumar et al., 2022), gợi ý rằng hiệu ứng này có ảnh hưởng bằng nhau trong quá trình tinh chỉnh và tiền huấn luyện và rằng nghiên cứu thêm có thể cải thiện tối ưu hóa.

**Batchnorm có thể làm mượt quá trình huấn luyện, ngay cả khi không làm mượt tổn thất.** Cohen et al. (2021) lưu ý rằng batchnorm (BN) (Ioffe và Szegedy, 2015) không ngăn các mạng đạt ranh giới ổn định và kết luận, trái với Santurkar et al. (2018), rằng BN không làm mượt cảnh quan tổn thất. Chúng tôi đưa ra giả thuyết rằng lợi ích của BN có thể là trong việc giảm ảnh hưởng của tín hiệu đối lập và giảm thiểu dao động này. Nói cách khác, BN có thể làm mượt quỹ đạo tối ưu hóa của mạng thần kinh, thay vì tổn thất (điều này phù hợp với sự phân biệt được thực hiện bởi Cohen et al. (2021) giữa tính quy tắc và tính mượt). Trong Phần 4, chúng tôi chứng minh rằng Adam cũng làm mượt quỹ đạo tối ưu hóa và rằng những thay đổi nhỏ để mô phỏng hiệu ứng này có thể hỗ trợ tối ưu hóa ngẫu nhiên. Chúng tôi tưởng tượng rằng hiệu ứng của BN cũng có thể phụ thuộc vào việc sử dụng GD so với SGD. Cụ thể, những phát hiện của chúng tôi gợi ý về lợi ích có thể của BN chỉ áp dụng cho SGD: giảm phương sai của tín hiệu đối lập không cân bằng qua các minibatch ngẫu nhiên.

**Đối với cả GD và SGD, khoảng một nửa điểm huấn luyện tăng tổn thất ở mỗi bước.** Mặc dù chỉ có các điểm bất thường dao động dữ dội, nhiều hình ảnh hơn chứa một số thành phần nhỏ của các đặc trưng mà chúng minh họa. Hình 36 trong Phụ lục cho thấy rằng phần điểm tăng tổn thất lơ lửng khoảng 50% cho mỗi bước—ở một mức độ nào đó, một mức độ dao động nhỏ dường như đang xảy ra với toàn bộ tập dữ liệu.

**Các tổn thất khác nhau có tác động khác nhau đến tăng độ sắc nét.** Mô hình của chúng tôi sẽ dự đoán rằng việc thêm làm mượt nhãn vào tổn thất cross-entropy sẽ giảm tăng độ sắc nét, vì làm mượt giảm độ cong tổn thất dưới sự tự tin quá mức. Thực sự, MacDonald et al. (2023) cho thấy đây là trường hợp. Điều này cũng gợi ý tại sao tổn thất logistic có thể phù hợp hơn cho tối ưu hóa NN, vì nó chỉ có độ cong đáng kể xung quanh x = 0 (x là logit, nghĩa là khi entropy dự đoán cao), vì vậy không giống tổn thất bình phương hoặc mũ, các đặc trưng độ lớn lớn sẽ không tăng độ sắc nét lớn. Chúng tôi mong đợi một tính chất tương tự có thể đóng góp vào hành vi tương đối của các hàm kích hoạt khác nhau (ví dụ ReLU hoặc tanh).

## 4 Sự Tương tác giữa Tín hiệu Đối lập và Tính Ngẫu nhiên

GD toàn batch không được sử dụng trong thực tế khi huấn luyện NN. Do đó, việc hỏi những phát hiện này ngụ ý gì về tối ưu hóa ngẫu nhiên là thích hợp. Chúng tôi bắt đầu bằng cách xác minh rằng mẫu này duy trì trong SGD. Hình 6(a) hiển thị tổn thất cho bốn cặp nhóm đối lập của VGG-11-BN được huấn luyện trên CIFAR-10 với SGD kích thước batch 128. Chúng tôi quan sát rằng các nhóm ghép đôi thực sự thể hiện các mẫu dao động đối lập rõ ràng, nhưng chúng không luân phiên với mọi bước, cũng không phải lúc nào cũng di chuyển theo hướng ngược lại. Điều này không đáng ngạc nhiên: chúng tôi mong đợi rằng không phải mọi batch sẽ có tín hiệu nhất định theo hướng này hay hướng khác. Để so sánh, chúng tôi bao gồm tổn thất huấn luyện toàn bộ trong mỗi hình—nghĩa là, bao gồm các điểm không có trong batch huấn luyện. Chúng tôi thấy rằng tổn thất trên các điểm bất thường có phương sai lớn hơn đáng kể; để xác nhận rằng điều này không chỉ vì các nhóm có ít mẫu hơn nhiều, chúng tôi cũng vẽ tổn thất trên tập con ngẫu nhiên của các điểm huấn luyện cùng kích thước. Chúng tôi tái tạo biểu đồ này với VGG-11 không có BN trong Hình 37 trong Phụ lục.

Sau khi xác minh rằng hành vi này xảy ra trong thiết lập ngẫu nhiên, chúng tôi đưa ra giả thuyết rằng các phương pháp tốt nhất hiện tại để tối ưu hóa mạng thần kinh nợ phần lớn thành công của chúng vào cách chúng xử lý tín hiệu đối lập. Như một bằng chứng khái niệm, chúng tôi sẽ làm điều này rõ ràng hơn với cuộc điều tra sơ bộ về bộ tối ưu hóa Adam (Kingma và Ba, 2014).

### 4.1 Cách Adam Xử lý Gradient với Tín hiệu Đối lập

Để hiểu rõ hơn về sự khác biệt của chúng, Hình 6(b) trực quan hóa các lần lặp tham số của Adam và SGD với momentum trên ReLU MLP được huấn luyện trên tập con 5k của CIFAR-10, cùng với những của GD và SGD (tất cả phương pháp sử dụng cùng khởi tạo và chuỗi batch huấn luyện). Hình trên là phép chiếu của những tham số này lên eigenvector đầu của Hessian tổn thất của mạng được huấn luyện với GD, được đánh giá tại bước đầu tiên mà độ sắc nét vượt qua 2/η. Chúng tôi quan sát rằng SGD theo đường dẫn tương tự với GD, mặc dù thêm momentum giảm thiểu dao động phần nào. Ngược lại, mạng được tối ưu hóa với Adam khác biệt rõ rệt khỏi mẫu này, dao động mượt mà dọc theo một bên. Chúng tôi xác định ba thành phần của Adam có khả năng đóng góp vào hiệu ứng này:

**Lợi thế 1: Bước nhỏ hơn dọc theo hướng độ cong cao.** Việc chuẩn hóa của Adam gây ra các bước nhỏ hơn dọc theo eigenvector đầu, đặc biệt gần cực tiểu. Biểu đồ dưới trong Hình 6(b) cho thấy rằng kích thước bước hiệu quả theo hướng này—nghĩa là, tích vô hướng tuyệt đối của kích thước bước từng tham số và eigenvector đầu—nhanh chóng giảm về không khi các lần lặp tiếp cận sàn thung lũng (theo hướng ngược lại, gradient phủ định momentum cho cùng hiệu ứng). Chúng tôi đưa ra giả thuyết rằng việc chuẩn hóa tổng quát có thể không cần thiết cho hiệu suất của Adam; chúng tôi thậm chí mong đợi nó có thể phần nào có hại bằng cách hạn chế khám phá. Mặt khác, việc chuẩn hóa bước theo độ cong từng tham số có vẻ quan trọng; Pan và Li (2023) lập luận tương tự và cho thấy rằng cắt gradient từng tham số cải thiện SGD đáng kể. Chúng tôi nhấn mạnh tại sao điều này có thể hữu ích trong điểm tiếp theo.

**Lợi thế 2: Quản lý gradient đuôi dài và tránh gradient dốc nhất.** Zhang et al. (2020) xác định "vùng tin cậy" như một đóng góp quan trọng cho thành công của Adam trong các mô hình attention, chỉ ra nhiễu đuôi dài trong gradient ngẫu nhiên. Gần đây hơn, Kunstner et al. (2023) lập luận rằng sự vượt trội của Adam không đến từ việc xử lý nhiễu tốt hơn, mà họ hỗ trợ bằng cách thử nghiệm với kích thước batch lớn. Kết quả của chúng tôi hòa giải những tuyên bố mâu thuẫn này bằng cách cho thấy rằng khó khăn không phải là nhiễu đuôi dài, mà là tín hiệu đối lập mạnh, có hướng (và có thể không cân bằng). Không giống "nhiễu gradient" truyền thống, kích thước batch lớn hơn có thể không giảm hiệu ứng của những tín hiệu này—nghĩa là, gradient có đuôi dài (qua các tham số) ngay cả khi không ngẫu nhiên. Hơn nữa, các bước lớn nhất mô phỏng Sign SGD, đáng chú ý là không phải phương pháp descent. Hình 6(b) cho thấy rằng các bước của Adam song song hơn với sàn thung lũng so với những của gradient dốc nhất. Do đó, có vẻ có lợi khi cố ý tránh các bước dọc theo gradient chỉ về phía cực tiểu địa phương, có thể dẫn đến sự phụ thuộc quá mức vào những đặc trưng này. Thực sự, Benzing (2022) quan sát rằng các phương pháp bậc hai thực sự hoạt động kém hơn SGD trên NN, và Kunstner et al. (2023) cho thấy rằng Adam chia sẻ một số hành vi với Sign SGD với momentum. Điểm này cũng phù hợp với lợi ích tổng quát hóa được quan sát của tốc độ học lớn cho SGD trên NN (Jastrz e ¸bski et al., 2020); thực tế, tín hiệu đối lập tự nhiên phù hợp với khái niệm về các đặc trưng "dễ tổng quát hóa" như được mô hình hóa bởi Li et al. (2019).

**Lợi thế 3: Giảm chấn.** Cuối cùng, yếu tố quan trọng thứ ba của Adam: giảm trọng lượng gradient gần đây nhất. SGD truyền thống với momentum β < 1 thực hiện một bước có trọng lượng gradient hiện tại bằng 1/(1+β) > 1/2. Mặc dù điều này có ý nghĩa trực quan, kết quả của chúng tôi ngụ ý rằng việc tăng trọng lượng gradient gần đây nhất có thể có vấn đề. Thay vào đó, chúng tôi mong đợi một bổ sung quan trọng là giảm chấn, nhân gradient ngẫu nhiên tại mỗi bước với một số (1-τ) < 1. Chúng tôi quan sát rằng gradient (chưa chuẩn hóa) của Adam tương đương với SGD với momentum và giảm chấn đều bằng β1, cộng với bước khử thiên vị. Các thay thế được đề xuất gần đây cũng bao gồm giảm chấn trong cập nhật momentum của chúng nhưng không xác định rõ ràng sự phân biệt (Zhang et al., 2020; Pan và Li, 2023; Chen et al., 2023).

### 4.2 Bằng chứng Khái niệm: Sử dụng Những Hiểu biết này để Hỗ trợ Tối ưu hóa Ngẫu nhiên

Để kiểm tra liệu những phát hiện của chúng tôi có chuyển thành lợi ích thực tế không, chúng tôi thiết kế một biến thể của SGD kết hợp những hiểu biết này. Đầu tiên, chúng tôi sử dụng giảm chấn τ = 0.9 ngoài momentum. Thứ hai, chúng tôi chọn một ngưỡng toàn cục: nếu độ lớn gradient cho một tham số trên ngưỡng này, chúng tôi thực hiện kích thước bước cố định; nếu không, chúng tôi thực hiện bước gradient như bình thường. Phương pháp chính xác xuất hiện trong Phụ lục F. Chúng tôi nhấn mạnh rằng mục tiêu của chúng tôi ở đây không phải là đề xuất thuật toán tối ưu hóa mới. Thay vào đó, chúng tôi đang khám phá giá trị tiềm năng thu được từ kiến thức về sự tồn tại và ảnh hưởng của tín hiệu đối lập.

Kết quả trong Phụ lục F cho thấy rằng cách tiếp cận này phù hợp với Adam khi huấn luyện ResNet-56/110 trên CIFAR-10 với tốc độ học cho các tham số không bị ngưỡng trên một số bậc độ lớn từ 10^-4 đến 10^3. Đáng chú ý, phần tham số trên ngưỡng (có kích thước bước cố định) chỉ khoảng 10-25% mỗi bước. Điều này ngụ ý rằng quỹ đạo và hành vi của mạng bị chi phối bởi phần nhỏ tham số này; phần còn lại có thể được tối ưu hóa mạnh mẽ hơn nhiều, nhưng hiệu ứng của chúng đối với hành vi mạng bị che khuất. Do đó, chúng tôi coi ảnh hưởng của tín hiệu đối lập như lời giải thích có thể cho "tiến triển ẩn" trong grokking (Barak et al., 2022; Nanda et al., 2023). Chúng tôi cũng so sánh phương pháp này với Adam cho giai đoạn đầu của việc huấn luyện GPT-2 (Radford et al., 2019) trên tập dữ liệu OpenWebText (Gokaslan et al., 2019)—không chỉ chúng hoạt động giống nhau, sự tương tự tổn thất của chúng gợi ý rằng quỹ đạo chính xác của chúng có thể rất giống nhau (Phụ lục F.2). Ở đây, phần tham số trên ngưỡng lơ lửng khoảng 50% ban đầu và sau đó giảm dần. Thực tế là nhiều tham số hơn trong mô hình attention trên ngưỡng gợi ý rằng cơ chế attention nhạy cảm hơn với tín hiệu đối lập và rằng điều tra thêm về cách giảm thiểu bất ổn này có thể có kết quả.

## 5 Thảo luận và Nghiên cứu Tương lai

Nhiều quan sát chúng tôi đưa ra trong bài báo này không mới, đã được mô tả trong các nghiên cứu trước khác nhau. Thay vào đó, nghiên cứu này xác định một nguyên nhân bậc cao có thể gắn kết những phát hiện này lại với nhau một cách gọn gàng. Cũng có nhiều nghiên cứu theo đuổi hiểu biết lý thuyết hơn về từng hiện tượng này một cách độc lập. Những phân tích như vậy bắt đầu với một tập hợp giả định (đặc biệt là về dữ liệu) và chứng minh rằng hành vi nhất định theo sau. Ngược lại, nghiên cứu này bắt đầu bằng cách xác định một điều kiện—sự hiện diện của tín hiệu đối lập—mà chúng tôi lập luận có thể là nguyên nhân chính của những hành vi này. Hai điều này không mâu thuẫn: chúng tôi tin rằng trong nhiều trường hợp, kết quả của chúng tôi phục vụ như bằng chứng trực tiếp cho tính hợp lệ của những giả định mô hình hóa này và rằng nó có thể cho phép những phân tích chi tiết hơn nữa. Nghiên cứu này cung cấp cuộc điều tra ban đầu mà chúng tôi hy vọng sẽ truyền cảm hứng cho những nỗ lực tương lai hướng đến hiểu biết hoàn chỉnh hơn.

Bây giờ chúng tôi nhấn mạnh một số kết nối với những phát hiện trước đây này. Nghiên cứu liên quan tổng quát hơn có thể tìm thấy trong Phụ lục A.

**Phổ tổn thất đuôi dài.** Các nghiên cứu trước đây về cảnh quan tổn thất đã lưu ý một nhóm nhỏ các giá trị eigen Hessian hoặc giá trị đơn lẻ Jacobian rất lớn (ví dụ Sagun et al. 2016, 2017; Papyan 2018, xem Phụ lục A để biết thêm). Phương pháp xác định những nhóm ghép đôi này của chúng tôi, cùng với các chỉ số được theo dõi trong Hình 2, chỉ ra rằng những hướng ngoại lai này trong phổ chính xác là những hướng với tín hiệu đối lập trong gradient và rằng mẫu này có thể là chìa khóa để hiểu rõ hơn khả năng tổng quát hóa của NN được huấn luyện với SGD.

**Tăng độ sắc nét tiến triển và ranh giới ổn định.** Tiêu điểm gần đây hơn đã chuyển sang giá trị eigen Hessian đầu, nơi người ta quan sát thực nghiệm rằng độ lớn của chúng (độ "sắc nét" tổn thất) tăng trong quá trình huấn luyện (Jastrz e ¸bski et al., 2019, 2020; Cohen et al., 2021) (được gọi là tăng độ sắc nét tiến triển), dẫn đến dao động nhanh trong không gian trọng số (Xing et al., 2018; Jastrz e ¸bski et al., 2019). Cohen et al. (2021) cũng thấy rằng đối với GD, điều này trùng với sự giảm nhất quán nhưng không đơn điệu trong tổn thất huấn luyện trên thời gian dài, mà họ gọi là ranh giới ổn định. Chúng tôi quan sát rằng những phân tích trước đây đã chứng minh sự xuất hiện của tăng độ sắc nét tiến triển và ranh giới ổn định dưới các giả định khác nhau (Arora et al., 2022; Wang et al., 2022), nhưng nguyên nhân cơ bản chưa được làm rõ. Cuộc thảo luận, thí nghiệm và phân tích lý thuyết của chúng tôi trong Phần 3 cung cấp bằng chứng mạnh mẽ cho một nguyên nhân thực sự phù hợp với một số giả định mô hình hóa hiện có này. Đại khái, kết quả của chúng tôi dường như ngụ ý rằng tăng độ sắc nét tiến triển xảy ra khi mạng học dựa vào (hoặc không dựa vào) tín hiệu đối lập theo cách rất cụ thể, đồng thời khuếch đại độ nhạy tổng thể. Sự tăng trưởng độ nhạy này có nghĩa là một thay đổi tham số nhỏ sửa đổi cách tín hiệu đối lập được sử dụng có thể tăng tổn thất lớn. Điều này dẫn đến bất ổn gián đoạn trực giao với "sàn thung lũng", kèm theo sự suy giảm tổn thất huấn luyện dần dần và những đợt tăng đột biến như được mô tả bởi ví dụ đơn giản trong Hình 4 và được mô tả trên dữ liệu thực trong Hình 1. Theo thực nghiệm, dao động này có vẻ phần nào độc lập với chuyển động song song với sàn (xem Phụ lục F), nhưng cần nghiên cứu thêm về động lực học chính xác.

**Tương quan giả, grokking và slingshotting.** Trong hình ảnh, các đặc trưng tương ứng với tín hiệu đối lập khớp với bức tranh truyền thống về "tương quan giả" một cách đáng ngạc nhiên—có thể là việc một mạng duy trì cân bằng hoặc phân kỳ dọc theo một hướng cũng quyết định liệu nó tiếp tục sử dụng đặc trưng "giả" hay bị buộc phải tìm cách thay thế để tối thiểu hóa tổn thất. Thực sự, hiện tượng chính xác của một mạng "bắn" đến một vùng mới với tổng quát hóa cải thiện đã được quan sát trực tiếp (Wu et al., 2018; Lewkowycz et al., 2020; Jastrz e ¸bski et al., 2021; Thilak et al., 2022). Grokking (Power et al., 2022), theo đó một mạng học tổng quát hóa lâu sau khi ghi nhớ tập huấn luyện, có liên quan chặt chẽ. Một số nghiên cứu đã chỉ ra rằng grokking là hiện tượng "ẩn", với việc khuếch đại dần dần của các mạng con tổng quát hóa (Barak et al., 2022; Nanda et al., 2023; Merrill et al., 2023); nó thậm chí đã được lưu ý là cùng xuất hiện với dao động trọng số (Notsawo Jr et al., 2023). Các thí nghiệm của chúng tôi trong Phần 4 và Phụ lục F cho thấy rằng ảnh hưởng của tín hiệu đối lập che khuất hành vi của phần còn lại của mạng, đưa ra một lời giải thích có thể.

**Thiên hướng đơn giản và giảm kép.** Nakkiran et al. (2019) quan sát rằng NN học các hàm có độ phức tạp tăng dần trong suốt quá trình huấn luyện. Các thí nghiệm của chúng tôi—đặc biệt là sự suy giảm chậm trong chuẩn của việc nhúng đặc trưng của tín hiệu đối lập—khiến chúng tôi tin rằng sẽ đúng hơn khi nói rằng chúng học hỏi các hàm đơn giản, điều này cho phép các mạng con phức tạp hơn với độ lớn nhỏ hơn và hiệu suất tốt hơn chiếm lĩnh. Lúc đầu, điều này có vẻ mâu thuẫn với khái niệm thiên hướng đơn giản (Valle-Perez et al., 2019; Shah et al., 2020), được định nghĩa rộng rãi như xu hướng của mạng dựa vào các hàm đơn giản của đầu vào của chúng. Tuy nhiên, có vẻ như đúng là mạng sẽ sử dụng các đặc trưng đơn giản nhất (ví dụ, chuẩn lớn nhất) mà nó có thể, miễn là những đặc trưng đó cho phép nó tiếp cận tổn thất huấn luyện bằng không; nếu không, nó cuối cùng có thể phân kỳ. Xu hướng này cũng gợi ý một lời giải thích có thể cho giảm kép (Belkin et al., 2019; Nakkiran et al., 2020): ngay cả sau khi nội suy, mạng đẩy về phía tự tin lớn hơn và các lớp trọng số tiếp tục cân bằng (Saxe et al., 2013; Du et al., 2018), tăng độ sắc nét. Điều này có thể dẫn đến dao động, đẩy mạng học các đặc trưng mới tổng quát hóa tốt hơn (Wu et al., 2018; Li et al., 2019; Rosenfeld et al., 2022; Thilak et al., 2022). Hành vi này cũng sẽ rõ ràng hơn đối với các mạng lớn hơn vì chúng thể hiện độ sắc nét lớn hơn. Lưu ý rằng lời giải thích thực sự không hoàn toàn đơn giản: tổng quát hóa đôi khi được cải thiện thông qua các phương pháp giảm dao động (như làm mượt tổn thất), ngụ ý rằng hành vi này không phải lúc nào cũng có lợi. Hiểu biết tốt hơn về những sắc thái này là chủ đề quan trọng cho nghiên cứu tương lai.

**Tối ưu hóa Nhận thức Độ sắc nét** Một kết nối khác mà chúng tôi nghĩ đáng điều tra thêm là Tối ưu hóa Nhận thức Độ sắc nét (SAM) (Foret et al., 2021), được biết đến là cải thiện tổng quát hóa của mạng thần kinh vì những lý do vẫn chưa được hiểu đầy đủ (Wen et al., 2023). Đặc biệt, biến thể hoạt động tốt hơn là 1-SAM, thực hiện các bước gradient tích cực trên từng điểm huấn luyện trong batch riêng lẻ. Rõ ràng rằng một số bản cập nhật này sẽ chỉ dọc theo hướng dốc nhất/tăng nhất trực giao với sàn thung lũng (và, nếu không được chuẩn hóa, các bản cập nhật có thể rất lớn). Do đó, có thể 1-SAM đang theo một nghĩa nào đó "mô phỏng" dao động và phân kỳ ra khỏi thung lũng này theo cả hai hướng, cho phép khám phá theo cách thường không thể cho đến khi độ sắc nét phát triển đủ lớn—những bước trung gian này cũng sẽ khuyến khích mạng giảm trọng lượng những đặc trưng này sớm hơn và nhanh hơn. Ngược lại, SAM tiêu chuẩn sẽ chỉ thực hiện bước này theo một trong hai hướng, hoặc có thể không thực hiện chút nào nếu các tín hiệu đối lập được cân bằng đều. Hơn nữa, không giống 1-SAM, bước trung gian sẽ pha trộn tất cả tín hiệu đối lập trong minibatch. Những khả năng này có vẻ là hướng đầy hứa hẹn cho khám phá thêm.

## 6 Kết luận

Sự tồn tại của các nhóm dữ liệu huấn luyện có ảnh hưởng đáng kể nhưng không rõ ràng như vậy đối với việc huấn luyện mạng thần kinh đặt ra nhiều câu hỏi cũng như câu trả lời. Nghiên cứu này trình bày cuộc điều tra ban đầu về hiệu ứng của tín hiệu đối lập đối với các khía cạnh khác nhau của tối ưu hóa, nhưng vẫn còn nhiều điều cần hiểu. Mặc dù rõ ràng rằng chúng có ảnh hưởng lớn đến việc huấn luyện, ít rõ ràng hơn là liệu việc giảm ảnh hưởng của chúng có cần thiết cho tối ưu hóa cải thiện hay chỉ trùng hợp với nó. Đồng thời, có bằng chứng rằng hành vi mà những tín hiệu này tạo ra có thể phục vụ như phương pháp quan trọng của khám phá và/hoặc điều hòa. Nếu vậy, một câu hỏi chìa khóa khác là liệu hai hiệu ứng này có thể được tách rời—hoặc liệu khả năng tổng quát hóa đáng kinh ngạc của mạng thần kinh có gắn liền với bất ổn tối ưu hóa của chúng hay không.
