# 2307.15663.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/optimizer/2307.15663.pdf
# File size: 6419353 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
CoRe Optimizer: An All-in-One Solution for Machine Learning
Marco Eckhoff∗and Markus Reiher†
ETH Zurich, Department of Chemistry and Applied Biosciences,
Vladimir-Prelog-Weg 2, 8093 Zurich, Switzerland.
(Dated: January 17, 2024)
The optimization algorithm and its hyperparameters can significantly affect the training speed
and resulting model accuracy in machine learning applications. The wish list for an ideal opti-
mizer includes fast and smooth convergence to low error, low computational demand, and general
applicability. Our recently introduced continual resilient (CoRe) optimizer has shown superior
performance compared to other state-of-the-art first-order gradient-based optimizers for training
lifelong machine learning potentials. In this work we provide an extensive performance comparison
of the CoRe optimizer and nine other optimization algorithms including the Adam optimizer and
resilient backpropagation (RPROP) for diverse machine learning tasks. We analyze the influence of
different hyperparameters and provide generally applicable values. The CoRe optimizer yields best
or competitive performance in every investigated application, while only one hyperparameter needs
to be changed depending on mini-batch or batch learning.
Keywords: Continual Resilient (CoRe) Optimizer, Adam, RPROP, AdaMax, RMSprop, AdaGrad, AdaDelta,
NAG, Momentum, SGD
1. INTRODUCTION
Machine learning (ML) is a part of the general field of
artificial intelligence. ML is employed in a wide range of
applications such as computer vision, natural language
processing, and speech recognition [1, 2]. It involves sta-
tistical models whose performance on tasks can be im-
proved by learning from sample data or past experience.
ML models include very many parameters, the so-called
weights. In the learning process, these weights are opti-
mized according to a performance measure. To evaluate
this measure, training data or experience are required.
In supervised learning, the model is trained on labeled
data to obtain a function that maps the data to its label
as in classification and regression tasks. By contrast, in
unsupervised learning unlabeled data is trained for cat-
egorization. In addition, in reinforcement learning the
model is trained through trial and error aiming to max-
imize its reward. Hence, ML models predict tasks only
based on a learned pattern of the data and do not require
explicit program instructions for predictions.
The performance measure can be a loss function (also
called cost function) that needs to be minimized [3]. This
loss function is usually a sum over contributions from
the training data points. Instead of calculating it simul-
taneously for the full training data set (deterministic or
batch learning), a (semi-)randomly chosen subset of the
training data is often employed (stochastic or mini-batch
learning). This approach can accelerate the convergence
with respect to the total computation time because the
loss-function accuracy increase is sub-linear for larger
batch sizes. To update the weights of the ML model,
first-order gradient-based iterative optimization schemes
∗eckhoffm@ethz.ch
†mreiher@ethz.chare dominating the field, since the memory demand and
computation time per step of second-order optimizers is
oftentoohigh. Ingeneral, theoptimizationaimsataloss
function’s local minimum as a function of the model’s
weights because it is sufficient for most ML applications
to find weight values with low loss rather than the global
minimum.
The optimization algorithm can crucially determine
the training speed and final performance of ML models
[4]. Therefore, the development of advanced optimizers
is an active field of research which can significantly im-
pact the accuracy of predictions in all ML applications.
The simplest form of stochastic first-order minimization
for high-dimensional parameter spaces is stochastic gra-
dient decent (SGD) [5]. In SGD, the negative gradient
of the loss function with respect to each weight is mul-
tiplied by a constant learning rate and the product is
subtracted from the respective weight in each update.
The loss function gradient is adapted in stochastic gra-
dient decent with momentum (Momentum) [6] and Nes-
terov accelerated gradient (NAG) [7, 8]. These meth-
ods aim to improve convergence by a momentum in the
weight updates, as the gradients are based on stochas-
tic estimates. In a different fashion, adaptive gradient
(AdaGrad) [9], adaptive delta (AdaDelta) [10], and root
mean square propagation (RMSprop) [11] apply the or-
dinary loss function gradient combined with a weight-
specific, adapted learning rate. Adaptive moment esti-
mation (Adam) [12], adaptive moment estimation with
infinity norm (AdaMax) [12], and our recently devel-
oped continual resilient (CoRe) optimizer [13] combine
momentum with individually adapted learning rates. In
resilient backpropagation (RPROP) [14, 15] only the sign
of the loss function gradient is employed with individu-
ally adapted learning rates.
Apart from these optimizers, which are applied in
this work, many more optimizers have been developed
for ML applications in recent years. For example, thearXiv:2307.15663v2  [cs.LG]  18 Feb 2024

--- PAGE 2 ---
2
modification of the first moment estimation of Adam
yields Nesterov-accelerated adaptive moment estimation
(NAdam)[16]andthemodificationofthesecondmoment
AMSGrad [17]. Nesterov momentum is also employed in
adaptive Nesterov momentum (Adan) [18]. Moreover,
AdaFactor [19], AdaBound [20], AdaBelief [21], AdamW
[22], PAdam [23], RAdam [24], AdamP [25], Lamb [26],
Gravity [27], and Lion [28] are further examples of the
large zoo of optimizers. They often represent incre-
mental improvements of parent algorithms. We note
that these optimizers can be used in applications beyond
ML as well. Furthermore, second-order optimizers have
been proposed such as adaptive estimates of the Hes-
sian (AdaHessian) [29] and second-order clipped stochas-
tic optimization (Sophia) [30]. To acquire an overview
of the performance differences among these optimizers,
extensive benchmarks are required [31–33]. Statistical
averaging and uncertainty quantification are indispens-
able in these benchmarks for validation.
To ease the burden on an ML practitioner in the opti-
mizer choice, an optimizer is desired which performs well
on diverse ML tasks. Moreover, a generally applicable
set of optimizer hyperparameters is required which works
out-of-the-box avoiding time consuming hyperparameter
tuning. At most, a single intuitive hyperparameter may
require to be adapted coarsely, while its value needs to
be easy to estimate. Furthermore, the ideal optimizer
features fast and smooth convergence to high accuracy
with low computational burden.
The Adam optimizer is not an obviously superior, but
viable choice for many ML tasks [33]. Therefore, Adam
became the most frequently applied optimizer with adap-
tive learning rates. Since our CoRe optimizer has outper-
formed Adam on the task of training a lifelong machine
learning potential (lMLP) [13], it is obvious to assess
its performance on diverse ML tasks and compare the
outcome with that of various aforementioned optimizers.
Such a broad performance evaluation further allows us
to obtain generally valid hyperparameters for the CoRe
optimizer to obtain an all-in-one solution.
As a benchmark, we examine a set of fast running ML
tasksprovidedinPyTorch[34]. Thebenchmarksetspans
the range from small mini-batch learning to full batch
learning as well as reinforcement learning. Moreover, it
includes different tasks, models, and data sets to enable
a broad comparison of different optimizers. First, for
the MNIST handwritten digits [35] and Fashion-MNIST
[36] data sets we run mini-batch learning to do varia-
tional auto-encoding (AED and ADF) [37] and image
classification (ICD and ICF). The latter is done by con-
volutional neural networks [38] with rectified linear units
(ReLU)[39],dropout[40],maxpooling[41],andsoftmax.
Second, for the cart-pole problem [42] we perform naive
reinforcement learning (NR) with a feed-forward linear
neural network [43], dropout, ReLU, and softmax and
reinforcement learning by an actor-critic algorithm (RA)
[44]. Third, fortheBSD300dataset[45]wecarryoutsin-
gle image super-resolution (SR) with upscale factor fourby sub-pixel convolutional neural networks [46] employ-
ing relatively large mini-batches. Fourth, we run batch
learning of the Cora data set [47] for semi-supervised
classification (SS) with graph convolutional networks [48]
and dropout as well as of a sine wave for time sequence
prediction (TS) with a long short-term memory (LSTM)
cell [49].
In addition, we evaluate the optimizers in training of
a machine learning potential [50–55], i.e., a regression
task. A machine learning potential is a representation of
the potential energy surface of a chemical system. It
can be employed in atomistic simulations to calculate
chemical properties and reactivity. One method exam-
ple among many others is a high-dimensional neural net-
work potential [56, 57] which takes as input the chemical
element types and atomic coordinates and in required
cases atomic charges and spins [58–60] to calculate the
energy and atomic forces of systems ranging from organic
molecules over liquids to inorganic materials including
multi-component systems such as interfaces [13, 61–63].
In this work, we repeat the stationary learning of an
lMLP based on an ensemble of ten high-dimensional neu-
ral network potentials, which employ element-embracing
atom-centered symmetry functions as descriptors [13].
The lMLP is trained on 8600 S N2 reaction systems with
lifelong adaptive data selection.
This work is organized as follows: In Section 2, we
summarize the applied optimization algorithms, and in
Section 3, we compile the computational details. In Sec-
tion 4, we analyze the resulting training speed and final
accuracy for the PyTorch ML task examples and lMLPs.
This work ends with a conclusion in Section 5.
2. METHODS
2.1. Continual Resilient (CoRe) Optimizer
The CoRe optimizer [13] is a first-order gradient-based
optimizer for stochastic and deterministic iterative opti-
mizations. It adapts the learning rates individually for
each weight wξdepending on the optimization progress.
These learning rate adjustments are inspired by the
Adam optimizer [12], RPROP [14, 15], and the synap-
tic intelligence method [64].
Exponential moving averages of the loss function gra-
dient and its square,
gτ
ξ=βτ
1·gτ−1
ξ+ (1−βτ
1)∂Lt
∂wt−1
ξ, (1)
hτ
ξ=β2·hτ−1
ξ+ (1−β2) 
∂Lt
∂wt−1
ξ!2
,(2)
with decay rates βτ
1, β2∈[0,1), are employed in mini-
mization in analogy to the Adam optimizer. For maxi-
mization, the sign of the loss function gradient in Equa-
tion (1) has to be inverted. In the CoRe optimizer, β1is

--- PAGE 3 ---
3
a function of the individual weight update counter τ,
βτ
1=βb
1+ 
βa
1−βb
1
exp"
−τ−1
βc
12#
,(3)
whereby τcan vary from the counter of gradient calcu-
lations tif some optimization steps do not update every
weight. The initial decay βa
1∈[0,1)is converted by a
Gaussian with width βc
1>0to the final decay βb
1∈[0,1).
The smaller βτ
1, the higher is the dependence on the cur-
rent gradient, while a larger βτ
1leads to a slower decay
of previous gradient contributions.
The Adam-like adaption of the weight-specific learning
rates,
uτ
ξ=gτ
ξ
1−(βτ
1)τ

hτ
ξ
1−(β2)τ1
2
+ϵ

−1
,(4)
employs the quotient of the moving averages gτ
ξand
(hτ
ξ)1
2, which are corrected with respect to their initial-
ization bias toward zero ( g0
ξ, h0
ξ= 0). For numerical sta-
bility ϵ⪆0is added in the denominator. This quotient
is invariant to gradient rescaling and introduces a form
of step size annealing. Therefore, uτ
ξchanges from ±1
in the first optimization step τ= 1toward zero in well-
behaving optimizations.
The plasticity factor,
Pτ
ξ=

0forτ > t hist
∧Sτ−1
ξtop-nfrozen ,χinSτ−1
χ
1otherwise,(5)
aims to improve the stability-plasticity balance by reg-
ularization in the weight updates. Therefore, weight
groups χare specified—for example, a layer in a neu-
ral network—and the weight-specific importance scores
Sτ−1
χ(see Equation (8) below) are compared within these
groups. When τ > t hist>0,Pτ
ξcan freeze the weights
with the nfrozen ,χ≥0highest importance scores in their
group in update τto mitigate forgetting of previous
knowledge.
The RPROP-like learning rate adaption,
sτ
ξ=

min
η+·sτ−1
ξ, smax
forgτ−1
ξ·gτ
ξ·Pτ
ξ>0
max
η−·sτ−1
ξ, smin
forgτ−1
ξ·gτ
ξ·Pτ
ξ<0
sτ−1
ξforgτ−1
ξ·gτ
ξ·Pτ
ξ= 0,
(6)
depends only on the sign of the gradient moving aver-
agegτ
ξand not on its magnitude leading to a robust op-
timization. Sign inversions from gτ−1
ξtogτ
ξoften sig-
nalize a jump over a minimum in the previous update.
Hence, the step size sτ−1
ξis reduced by the decrease fac-
torη−∈(0,1]in this case, while it is enlarged by the
increase factor η+≥1for constant signs to speed upconvergence. The updated step size sτ
ξis bounded by
the minimal and maximal step sizes smin, smax>0. For
gτ−1
ξ·gτ
ξ·Pτ
ξ= 0, the step size update is omitted. The
initial step size s0
ξ=s1
ξis a hyperparameter of the opti-
mization.
The weight decay,
wt
ξ= 
1−dχ·uτ
ξ·Pτ
ξ·sτ
ξ
wt−1
ξ−uτ
ξ·Pτ
ξ·sτ
ξ,(7)
with group-specific hyperparameter dχ∈[0,(smax)−1),
targets to reduce the overfitting risk by prevention of
strong weight in- or decreases. It is proportional to the
productof dχandtheabsoluteweightupdate |uτ
ξ|·Pτ
ξ·sτ
ξ,
i.e., the more stable the weight value the less it is affected
by the weight decay. Subsequently, the signed weight
update uτ
ξ·Pτ
ξ·sτ
ξis subtracted to obtain the updated
weight wt
ξ. The weight values are therefore bound be-
tween −(dχ)−1and(dχ)−1in well-behaving optimiza-
tions, i.e., uτ
ξ≤ ±1.
The importance score value,
Sτ
ξ=

Sτ−1
ξ+ (thist)−1gτ
ξ·uτ
ξ·Pτ
ξ·sτ
ξforτ≤thisth
1−(thist)−1i
Sτ−1
ξ
+ (thist)−1gτ
ξ·uτ
ξ·Pτ
ξ·sτ
ξotherwise,
(8)
ranks the weight importance by taking into account
weight-specific contributions to previously estimated loss
function decreases. This ansatz is inspired by the synap-
tic intelligence method. The importance scores enable to
identify the most important weights in previous updates,
which can be frozen by the plasticity factors (Equa-
tion (5)) in following updates to improve the stability-
plasticity balance. The product of gradient moving av-
erage and signed weight update is employed to estimate
the loss function decrease. Since the weight update sign
is not inverted, the higher positive the importance score,
the larger is the loss function decrease. Starting with
S0
ξ= 0, the mean of gτ
ξ·uτ
ξ·Pτ
ξ·sτ
ξoverτ≤thistis calcu-
lated. For τ > t hist, the importance score is determined
as exponential moving average with decay 1−(thist)−1.
We note that the relative large number of hyperpa-
rameters in the CoRe optimizer is, on the one hand, an
advantage to obtain good results even in very difficult
or edge cases. On the other hand, the hyperparameter
tuning is more complicated. However, a set of generally
applicable values, which are provided in this work, can
overcome this drawback.
2.2. SGD
SGD [5] subtracts the product of a constant learning
rateγand the loss function gradient from the weights
wt−1
ξin the weight updates,
wt
ξ=wt−1
ξ−γGτ
ξ, (9)

--- PAGE 4 ---
4
with
Gτ
ξ=∂Lt
∂wt−1
ξ. (10)
2.3. Momentum
An additional momentum (Momentum) [6] can be in-
troduced in SGD by replacing Gτ
ξin Equation (9) by
mτ
ξ=µ·mτ−1
ξ+Gτ
ξ, (11)
with the momentum factor µandm1
ξ=G1
ξ[34].
2.4. NAG
NAG [7, 8] is SGD with Nesterov momentum, i.e, Gτ
ξ
in Equation (9) is substituted by
nτ
ξ=µ·mτ
ξ+Gτ
ξ. (12)
2.5. Adam
The algorithm of the Adam optimizer [12] is given
by Equations (1) (with constant β1), (2), (4), and (9),
whereby Gτ
ξin Equation (9) is replaced by uτ
ξ. In com-
parisontotheCoReoptimizer, Adammissesthe τdepen-
dence of the decay rate β1, the plasticity factors Pτ
ξ, the
RPROP-like learning rate adaption sτ
ξ, and the weight
decay. The latter can be introduced in Adam as well
as in many other optimizers also by adding d·wt−1
ξto
the loss function gradient as second operation of an op-
timization iteration after the possible sign inversion for
maximization. A further alternative is to subtract in-
stead d·γ·wt−1
ξfrom wt−1
ξas in AdamW [22].
2.6. AdaMax
The difference of the AdaMax optimizer [12] compared
to Adam is that the term in curly brackets in Equation
(4) is replaced by the infinity norm,
kτ
ξ= max
β2·kτ−1
ξ,Gτ
ξ+ϵ
,(13)
with k0
ξ= 0.
2.7. RMSprop
In RMSprop [11] the loss function gradient is divided
by the moving average of its magnitude,
wt
ξ=wt−1
ξ−γGτ
ξ 
hτ
ξ1
2+ϵ−1
.(14)Hence, the difference to the Adam optimizer is that the
loss function gradient Gτ
ξis applied instead of the gradi-
ent moving average gτ
ξand the initialization bias correc-
tion is omitted.
2.8. AdaGrad
The AdaGrad optimizer [9] differs from RMSprop by
the replacement of hτ
ξin Equation (14) by
bτ
ξ=bτ−1
ξ+ 
Gτ
ξ2, (15)
with b0
ξ= 0.
2.9. AdaDelta
The adaptive learning rate in the AdaDelta optimizer
[10] is established by
wt
ξ=wt−1
ξ−γGτ
ξ 
lτ−1
ξ+ϵ
hτ
ξ+ϵ!1
2
,(16)
with
lτ
ξ=β2·lτ−1
ξ+ (1−β2)lτ−1
ξ+ϵ
hτ
ξ+ϵ 
Gτ
ξ2(17)
andl0
ξ= 0. Hence, in comparison to the RMSprop al-
gorithm the factor
lτ−1
ξ+ϵ1
2is applied additionally in
the weight update and the order of adding ϵtohτ
ξand
taking the square root is inverted.
2.10. RPROP
RPROP [14, 15] is based on Equation (6), whereby
Gτ−1
ξ·Gτ
ξisemployedinsteadof gτ−1
ξ·gτ
ξ·Pτ
ξ. Inaddition,
a backtracking weight step is applied by setting Gτ
ξ= 0
when Gτ−1
ξ·Gτ
ξ<0. The weight update is given by
wt
ξ=wt−1
ξ−sτ
ξ·sgn 
Gτ
ξ
. (18)
3. COMPUTATIONAL DETAILS
The PyTorch ML task examples [65] were solely mod-
ified to embed them in the extensive benchmark without
touching the ML models and trainings. The only excep-
tion was the removal of the learning rate scheduler in
ICD and ICF to assess exclusively the performance of
the optimizer. The tasks performed originally only on
the MNIST data set (AED and ICD) were also carried
outfortheFashion-MNISTdataset(AEFandICF).The

--- PAGE 5 ---
5
batch sizes of the ML tasks AED, AEF, ICD, and ICF
were 64 of in total 60000 training data points to obtain
test cases for small mini-batch learning (64 was the de-
fault value in the ICD PyTorch ML task example). The
batch size of SR was 10 of 200 training data points to get
an example of a batch size which is a rather large fraction
of the total number of data points ( 5%). The employed
scripts with all details on the models, trainings, and er-
ror definitions are available on Zenodo [66] alongside the
compiled raw results as well as plot and analysis scripts.
Moreover, this repository as well as the Zenodo reposi-
tory [67] contain the CoRe optimizer software, which is
compatible to use with PyTorch. In addition, the lMLP
software [68] was extended to integrate all optimizers and
is also available in the Zenodo repository [66] alongside
lMLP results as well as model and training details. The
latter were taken over from Reference [13]. The lMLP
training employed lifelong adaptive data selection and a
fit fraction per epoch of 10%of all 7740 training struc-
tures.
EachMLtaskwasperformedforeachoptimizersetting
with 20 different sets of random numbers. For reinforce-
ment learning (NR and RA) even 100 different sets of
random numbers were employed as the fluctuations in
the respective results were the largest. These sets were
the same for each optimizer and they ensured differently
initialized weights (and different selection of training and
test data). The mean test set error Etest
iand its standard
deviation ∆Etest
iof ML task iwere calculated for each
set as a function of the training epoch nepochto eval-
uate convergence. To determine the final accuracy, for
the minimal test set error in each of the 20 trainings the
mean Etest,min
iand standard deviation ∆Etest,min
iwere
calculated, i.e., earlystoppingwasapplied. Forreinforce-
ment learning (NR and RA) the mean number of training
episodes until a reward of 475 [69] was taken to quantify
Etest
i. The maximum number of training episodes was
2500, which was also used as error of unsuccessful train-
ings. For AEF 7 of 20 Momentum∗trainings, 8 of 20
NAG trainings, and 12 of 20 NAG∗trainings failed even
for the best learning rate value. These trainings were
penalized with a constant error of 1000. For lMLPs the
total test loss according to Equation (10) in Reference
[13] determined the training epoch with minimal error.
In this way, the mean squared error of the energies was
weighted with a factor q2= 10 .92in the loss function,
whilethatoftheatomicforcecomponentswasnotscaled.
We evaluated the mean error based on the errors of all 20
lMLPs in each of the 20 training epochs where an indi-
vidual lMLP showed minimal error, i.e., 400 error values
were included. In this way, the error was still calculated
from advanced training states, while it was also sensitive
to the smoothness of the training processes as early stop-
ping is difficult to apply in practise in lifelong machine
learning.
To compare the final accuracy among different opti-
mizers kfor ML task i, the inverse of the minimum test
seterror Etest,min
i,krelativetotheresultofbestperformingoptimizer in ML task iwas calculated,
Ai(k) =mink
Etest,min
i,k
Etest,min
i,k. (19)
The uncertainty of the accuracy score was calculated
from an error propagation based on the test set error’s
standard deviation ∆Etest
i,k,
∆Ai(k) =mink
Etest,min
i,k

Etest,min
i,k2∆Etest,min
i,k.(20)
For comparison of different optimizers kwith regard to
the overall accuracy, the arithmetic mean A(k)over all
NtasksML task accuracy scores Ai(k)was calculated,
A(k) =1
NtasksNtasksX
iAi(k). (21)
Its uncertainty was determined by propagating the errors
of the independent variables Ai(k),
∆A(k) =1
Ntasks(NtasksX
i[∆Ai(k)]2)1
2
.(22)
The PyTorch version 2.0.0 [34] and its default settings
were applied for the optimizers AdaDelta, AdaGrad,
Adam, AdaMax, Momentum, NAG, RMSprop, RPROP,
and SGD (see Tables S1 and S3 in the Supporting Infor-
mation for all hyperparameter values). The momentum
factor in Momentum and NAG was µ= 0.9. In addition,
scans of the performance determining hyperparameters
β1,β2,µ,η−, and η+were carried out for the PyTorch
ML task examples in order to find their optimal values
for this set of ML tasks. If the default values turned out
to be the best ones, the second best choice was applied.
The optimizers employing these modified hyperparame-
ters(seeTablesS1andS3intheSupportingInformation)
are marked with an asterisk (∗). Weight decay was by de-
fault only applied in the CoRe optimizer. The learning
rates s0
ξof RPROP, RPROP∗, and the CoRe optimizer
were set to 10−3. For the learning rate γof the other
optimizers and the maximal step size smaxof RPROP,
RPROP∗, and the CoRe optimizer, the values 0.0001,
0.001,0.01,0.1, and 1were tested for each PyTorch ML
task example. The value yielding the lowest ∆Etest,min
i
was employed in the performance evaluation (see Table
S2 in the Supporting Information). For lMLP training
the two most likely options according to the PyTorch ML
task results were tested (see Table S4 in the Supporting
Information).

--- PAGE 6 ---
6
4. RESULTS AND DISCUSSION
4.1. General Recommendations for CoRe
Optimizer Hyperparameter Values
A generally applicable set of CoRe optimizer hyper-
parameter values has been obtained from our bench-
mark on nine ML tasks including seven different mod-
els and six different data sets. The training processes
spantheentirerangefromlearningonsmallmini-batches
to full data set batch learning. Based on this bench-
markwegenerallyrecommendthehyperparametervalues
βa
1= 0.7375,βb
1= 0.8125,βc
1= 250,β2= 0.99,ϵ= 10−8,
η−= 0.7375,η+= 1.2,smin= 10−6,s0
ξ= 10−3,
dχ= 0.1, and thist= 250. The number of frozen weights
per group nfrozen ,χcan often be specified as a fraction of
frozen weights per group pfrozen ,χ. Well working values of
pfrozen ,χare typically in the interval between 0(without
stability-plasticity balance) and about 10%. The maxi-
mal step size smaxis recommended to be 10−3for mini-
batch learning, 1for batch learning, and 10−2for inter-
mediate cases. smaxis the main hyperparameter like the
learning rate γin many other optimizers.
4.2. Optimizer Performance Evaluation for Diverse
Machine Learning Tasks
To assess the performance of the CoRe optimizer in
comparison to nine other optimizers with in total 16 dif-
ferent hyperparameter settings, relative accuracy scores
for nine ML tasks were calculated for these optimizers
(Figures 1 (a) and (b)). For mini-batch learning on small
batch sizes ( 0.1%for AED, AEF, ICD, and ICF) the
popular Adam optimizer and our CoRe optimizer per-
form best, while especially RPROP yields poor accuracy
because it cannot handle well stochastic gradient fluc-
tuations. RPROP is intended for batch learning which
becomes obvious by the high accuracy scores for SS and
TS.FortheseMLtasks, RPROPandtheCoReoptimizer
achieve the highest accuracy scores. In the intermedi-
ate case, i.e., mini-batch learning with rather large batch
sizes ( 5%for SR and 10%for lMLP training (Figure 4)),
both Adam and RPROP perform well with Adam hav-
ing a small advantage over RPROP. However, the CoRe
optimizer outperforms both in this case.
Moreover, the learning speed and reliability of the
CoRe optimizer in reinforcement learning (NR and RA)
is also better than for the other optimizers (Figures 1
(a) and (b)). RPROP is not able to learn the task in the
maximal number of episodes for NR in any training. The
CoRe optimizer’s convergence speed of the mean test set
errors for the other ML tasks is similar to Adam for mini-
batch learning and similar to RPROP for batch learning
(see Figures S1 to S7 and S9 in the Supporting Informa-
tion).
In total, the CoRe optimizer achieves the highest final
accuracy score in six tasks and lMLP training, Adam∗inTable 1: Overview of the ML tasks and the respective
data sets including their acronyms.
ML task Data set
AED auto-encoding MNIST digits [35]
AEF auto-encoding Fashion-MNIST [36]
ICD image classification MNIST digits [35]
ICF image classification Fashion-MNIST [36]
NR naive reinforcement learning cart-pole problem [42]
RA reinforcement learning cart-pole problem [42]
(actor-critic)
SR single image super-resolution BSD300 [45]
SS semi-supervised classification Cora [47]
TS time sequence prediction sine waves
two tasks, and RPROP∗in one task (Figures 1 (a) and
(b)). However, in the six cases where the CoRe optimiz-
ers performs best, the second best optimizer is always
within the uncertainty interval of the CoRe optimizer’s
accuracy score. Still, there is no single optimizer which
is always within the uncertainty interval. For example,
Adam, RMSprop, RMSprop∗, and SGD are within the
uncertainty interval for ML task ICF, only AdaMax∗
for SR, and AdaMax∗, RPROP, and RPROP∗for TS,
whereas the CoRe optimizer is always within the uncer-
tainty interval of the best optimizer for the other three
ML tasks. Hence, even if there is no clear dominance
for individual ML tasks, the CoRe optimizer is among
the best optimizers in all these ML tasks resulting in, on
average, the best performance and the broadest appli-
cability. Therefore, the CoRe optimizer is well-rounded
and achieves the highest overall accuracy score (Figure
2). The overall accuracy score of Adam is second high-
est, while those of AdaMax∗and Adam∗are almost equal
to that of Adam. The uncertainty interval of the CoRe
optimizer’s overall accuracy score overlaps slightly with
that of the Adam optimizer. We note that the uncer-
tainty interval of the Adam optimizer’s results is also the
largest among all results.
In general, for the chosen set of ML tasks the optimiz-
ers which combine momentum and individually adapted
learning rates (CoRe, Adam, and AdaMax) perform bet-
ter than those which only apply individually adapted
learningrates(RMSprop, AdaGrad, andAdaDelta)(Fig-
ure 2). However, the differences among the CoRe op-
timizer, Adam, and AdaMax are larger than that of
RMSprop and AdaMax. The final accuracy obtained by
pure SGD is significantly worse than that of the afore-
mentioned optimizers. However, for these nine ML tasks
it is still slightly better than that of the optimizers which
employ only momentum (Momentum and NAG). The
overall accuracy of RPROP is in between those apply-
ingindividuallyadaptedlearningratesandSGDforthese
MLtasks. However, thisorderis, ofcourse, dependenton
the fraction of mini-batch and batch learning ML tasks.
The best single model performances obtained by the

--- PAGE 7 ---
7
7
(a)
(b)
Figure 1: Bar chart of the final accuracy scores Ai(Equation (19)) of various ML tasks itrained by different
optimizers. The uncertainty interval (Equation (20)) is shown as cross-hatched bar around the upper edge of the bar
which equals the mean of Aiover 20 trainings (100 for NR and RA). 100%corresponds to the highest obtained final
accuracy of all optimizer specifications. The acronyms of the ML tasks are explained in Table 1. The optimizers are
listed in the legends and are represented by different colors. The learning rate (maximal step size for RPROP,
RPROP∗, and CoRe) was adjusted, while all other hyperparameters of the optimizers were set to (a)their general
recommendation and (b)modified values. Exceptions are AdaGrad and SGD which do not include additional
hyperparameters beyond the learning rate. The CoRe results are shown as reference in (b).
CoRe optimizer are provided in Table S5 and Figures
S10 (a) and (b) and S11 in the Supporting Information.
For SS we can compare the final accuracy directly to the
original work with 81.5%correct test set classifications
[48]. Due to training by the CoRe optimizer, the best
graph convolutional network for SS achieves a test set
classification accuracy of 84.2%.4.3. Performance Dependence on Hyperparameter
Values
The CoRe optimizer’s hyperparameters were tuned on
this set of ML tasks, while the general hyperparame-
ter recommendations of PyTorch for the other optimizers
were not based on this benchmark set. To provide a fair
comparison, we also applied hyperparameter values for
the other optimizers which were adjusted on this set of
Figure 1: Bar chart of the final accuracy scores Ai(Equation (19)) of various ML tasks itrained by different
optimizers. The uncertainty interval (Equation (20)) is shown as cross-hatched bar around the upper edge of the bar
which equals the mean of Aiover 20 trainings (100 for NR and RA). 100%corresponds to the highest obtained final
accuracy of all optimizer specifications. The acronyms of the ML tasks are explained in Table 1. The optimizers are
listed in the legends and are represented by different colors. The learning rate (maximal step size for RPROP,
RPROP∗, and CoRe) was adjusted, while all other hyperparameters of the optimizers were set to (a)their general
recommendation and (b)modified values. Exceptions are AdaGrad and SGD which do not include additional
hyperparameters beyond the learning rate. The CoRe results are shown as reference in (b).
CoRe optimizer are provided in Table S5 and Figures
S10 (a) and (b) and S11 in the Supporting Information.
For SS we can compare the final accuracy directly to the
original work with 81.5%correct test set classifications
[48]. Due to training by the CoRe optimizer, the best
graph convolutional network for SS achieves a test set
classification accuracy of 84.2%.4.3. Performance Dependence on Hyperparameter
Values
The CoRe optimizer’s hyperparameters were tuned on
this set of ML tasks, while the general hyperparame-
ter recommendations of PyTorch for the other optimizers
were not based on this benchmark set. To provide a fair
comparison, we also applied hyperparameter values for
the other optimizers which were adjusted on this set of

--- PAGE 8 ---
8
8
Figure 2: Bar chart of the final accuracy score A
(Equation (21)) averaged over all ML tasks shown in
Figures 1 (a) and (b) for different optimizers. The
uncertainty interval (Equation (22)) is shown as
cross-hatched bar around the right edge of the bar
which equals the value of A. A value of 100%means
that the optimizer achieves highest accuracy in every
ML task. The bars are labeled and colored according to
the respective optimizer.
ML tasks. The adjusted hyperparameters of AdaDelta∗,
AdaMax∗, Momentum∗, and RPROP∗yielded an im-
provement of their overall accuracy scores (Figure 2).
However, the gain is not sufficient to reach the overall
accuracy scores in the next better class of optimizers de-
scribed in the last section. Therefore, the choice of the
optimization algorithm is confirmed to be crucial for the
finalaccuracyoftheMLmodel. Thehighestoverallaccu-
racy scores of Adam, NAG, and RMSprop were obtained
with their generally recommended hyperparameter val-
ues. The second best choices of the hyperparameters
yielded very similar overall accuracy scores.
Another difference between the CoRe optimizer and
the other optimizers was the application of a weight de-
cay. However, Figures S13 and S14 in the Supporting In-
formationshowthatthestandardweightdecayalgorithm
of Adam employed with four different hyperparameter
values in general reduces the accuracy score for Adam.
Only the weight decay algorithm of AdamW can lead to
a small increase of the overall accuracy score. However,
the gain is only a fraction of the overall accuracy score
difference between the CoRe optimizer and the Adam
optimizer. The weight decay of the CoRe optimizer only
marginally affects the final accuracy on average (Figures
S13 and S14 in the Supporting Information).In the analysis of individual ML task performances, we
note that RPROP and the CoRe optimizer show a slow
convergence in the initial epochs of SS training (see Fig-
ureS6intheSupportingInformation). Thereasonisthat
large weight changes are required in the optimization and
the initial step size s0
ξis only set to 0.001. Higher values
ofs0
ξresult in faster convergence to a similar final accu-
racy, with s0
ξ= 0.1yielding a much faster convergence
than obtained with Adam (see Figure S7 in the Support-
ing Information). However, this ML task is an extreme
example with few weight updates to adjust sτ
ξin batch
learning and the need of large weight changes. Still, as
the final accuracy is the same and in most applications
sτ
ξis fast adapted in a relatively small fraction of weight
updates, the initialization of s0
ξis in general noncritical.
Another edge case can be obtained for high maxi-
mal step size values smaxin the CoRe optimizer. While
smax= 1yields a high final accuracy in TS training when
early stopping is applied, the training can become unsta-
ble when continued (see Figure S8 in the Supporting In-
formation). However, reducing smaxto0.1already solves
this issue (see Figure S9 in the Supporting Information).
4.4. Optimizer Performance in Training Lifelong
Machine Learning Potentials
In the training of lMLPs rather large fractions of train-
ing data ( 10%) were employed in the loss function gradi-
ent calculation. In line with the results of the PyTorch
ML task examples, this kind of training best suits the
CoRe optimizer followed by Adam∗, Adam, Adamax∗,
and RPROP∗(Figures 3 (a) and (b) and 4). Moreover,
the general trend is confirmed that adaptive and momen-
tum based optimizers perform best, while only adaptive
optimizers still yield better results than only momentum
basedoptimizers. IncontrasttothePyTorchMLtaskex-
amples,wherethestability-plasticitybalanceoftheCoRe
optimizer with pfrozenaround 0.025can only marginally
improve the accuracy scores for AED, AEF, and SR and
worsens the final accuracy for ICD and ICF (see Figure
S12 in the Supporting Information), the lMLP training
largely benefits from the stability-plasticity balance with
pfrozen = 0.1. We note that tuning pfrozen, in addition
to the maximal step size, extends the hyperparameter
optimization capability for CoRepfrozen =0.1compared to
CoRe and all other optimizers, for which only the max-
imal step size/learning rate was adjusted while all other
hyperparameters were taken from the PyTorch ML task
example results. This higher degree of freedom can also
contribute to the optimization performance. However, a
significant performance improvement was not obtained
for any other hyperparameter tuning with the exception
of tuning η−.
Moreover, the stability-plasticity balance smoothens
the training convergence as shown in the test set root
meansquareerrors(RMSEs)ofenergiesandatomicforce
components as a function of the training epochs (Figures
Figure 2: Bar chart of the final accuracy score A
(Equation (21)) averaged over all ML tasks shown in
Figures 1 (a) and (b) for different optimizers. The
uncertainty interval (Equation (22)) is shown as
cross-hatched bar around the right edge of the bar
which equals the value of A. A value of 100%means
that the optimizer achieves highest accuracy in every
ML task. The bars are labeled and colored according to
the respective optimizer.
ML tasks. The adjusted hyperparameters of AdaDelta∗,
AdaMax∗, Momentum∗, and RPROP∗yielded an im-
provement of their overall accuracy scores (Figure 2).
However, the gain is not sufficient to reach the overall
accuracy scores in the next better class of optimizers de-
scribed in the last section. Therefore, the choice of the
optimization algorithm is confirmed to be crucial for the
finalaccuracyoftheMLmodel. Thehighestoverallaccu-
racy scores of Adam, NAG, and RMSprop were obtained
with their generally recommended hyperparameter val-
ues. The second best choices of the hyperparameters
yielded very similar overall accuracy scores.
Another difference between the CoRe optimizer and
the other optimizers was the application of a weight de-
cay. However, Figures S13 and S14 in the Supporting In-
formationshowthatthestandardweightdecayalgorithm
of Adam employed with four different hyperparameter
values in general reduces the accuracy score for Adam.
Only the weight decay algorithm of AdamW can lead to
a small increase of the overall accuracy score. However,
the gain is only a fraction of the overall accuracy score
difference between the CoRe optimizer and the Adam
optimizer. The weight decay of the CoRe optimizer only
marginally affects the final accuracy on average (Figures
S13 and S14 in the Supporting Information).In the analysis of individual ML task performances, we
note that RPROP and the CoRe optimizer show a slow
convergence in the initial epochs of SS training (see Fig-
ureS6intheSupportingInformation). Thereasonisthat
large weight changes are required in the optimization and
the initial step size s0
ξis only set to 0.001. Higher values
ofs0
ξresult in faster convergence to a similar final accu-
racy, with s0
ξ= 0.1yielding a much faster convergence
than obtained with Adam (see Figure S7 in the Support-
ing Information). However, this ML task is an extreme
example with few weight updates to adjust sτ
ξin batch
learning and the need of large weight changes. Still, as
the final accuracy is the same and in most applications
sτ
ξis fast adapted in a relatively small fraction of weight
updates, the initialization of s0
ξis in general noncritical.
Another edge case can be obtained for high maxi-
mal step size values smaxin the CoRe optimizer. While
smax= 1yields a high final accuracy in TS training when
early stopping is applied, the training can become unsta-
ble when continued (see Figure S8 in the Supporting In-
formation). However, reducing smaxto0.1already solves
this issue (see Figure S9 in the Supporting Information).
4.4. Optimizer Performance in Training Lifelong
Machine Learning Potentials
In the training of lMLPs rather large fractions of train-
ing data ( 10%) were employed in the loss function gradi-
ent calculation. In line with the results of the PyTorch
ML task examples, this kind of training best suits the
CoRe optimizer followed by Adam∗, Adam, Adamax∗,
and RPROP∗(Figures 3 (a) and (b) and 4). Moreover,
the general trend is confirmed that adaptive and momen-
tum based optimizers perform best, while only adaptive
optimizers still yield better results than only momentum
basedoptimizers. IncontrasttothePyTorchMLtaskex-
amples,wherethestability-plasticitybalanceoftheCoRe
optimizer with pfrozenaround 0.025can only marginally
improve the accuracy scores for AED, AEF, and SR and
worsens the final accuracy for ICD and ICF (see Figure
S12 in the Supporting Information), the lMLP training
largely benefits from the stability-plasticity balance with
pfrozen = 0.1. We note that tuning pfrozen, in addition
to the maximal step size, extends the hyperparameter
optimization capability for CoRepfrozen =0.1compared to
CoRe and all other optimizers, for which only the max-
imal step size/learning rate was adjusted while all other
hyperparameters were taken from the PyTorch ML task
example results. This higher degree of freedom can also
contribute to the optimization performance. However, a
significant performance improvement was not obtained
for any other hyperparameter tuning with the exception
of tuning η−.
Moreover, the stability-plasticity balance smoothens
the training convergence as shown in the test set root
meansquareerrors(RMSEs)ofenergiesandatomicforce
components as a function of the training epochs (Figures

--- PAGE 9 ---
9
9
(a)
(b)
Figure 3: Bar chart of the final accuracy scores Ai
(Equation (19)) of energy and force prediction of lMLPs
trained by different optimizers. The uncertainty interval
(Equation (20)) is shown as cross-hatched bar around
the upper edge of the bar which equals the mean of Ai
over 20 trainings. 100%corresponds to the highest
obtained final accuracy of all optimizer specifications,
i.e., the lowest RMSE in the prediction of energies or
atomic force components. The colors of most optimizers
are listed in the legends of Figures 1 (a) and (b). The
learning rate (maximal step size for RPROP and CoRe
specifications) was adjusted, while all other
hyperparameters of the optimizers were set to (a)their
general recommendation and (b)modified values.
Exceptions are AdaGrad and SGD which do not include
additional hyperparameters beyond the learning rate.
The CoRe results are shown as reference in (b).
5 (a) and (b)). The CoRe optimizer yields smoother con-
Figure 4: Bar chart of the final accuracy score A
(Equation (21)) combining energy and force prediction
of lMLPs for different optimizers. The uncertainty
interval (Equation (22)) is shown as cross-hatched bar
around the right edge of the bar which equals the value
ofA. A value of 100%means that the optimizer
achieves highest accuracy in energy and force
prediction. The bars are labeled and colored according
to the respective optimizer.
vergence than Adam, which is beneficial, for example, in
lifelong machine learning where the lMLP needs to be
ready for application in every training stage. The ac-
curacy scores in Figures 3 (a) and (b) and 4 take into
account the convergence smoothness (see Section 3) in
contrast to the accuracy scores in Figures S15 and S16 in
the Supporting Information which are only based on the
individuallMLPearlystoppingresults. Thelatterisben-
eficial for the Adam results but still the CoRe optimizer
with stability-plasticity balance outperforms Adam. The
convergence speed is also higher for the CoRe optimizer
than for Adam. This observation is in line with the
convergence of the SR ML task (see Figure S5 in the
Supporting Information) which also represents a training
case between mini-batch and batch learning. To demon-
strate the benefit of more stabilized learning in lMLP
training, we additionally decreased the η−value which
smoothens and improves the training process similarly.
Both, a large pfrozenand a small η−, lead also to a better
interplay with the lifelong adaptive data selection. How-
ever, this interplay is only a minor factor of the large ac-
Figure 3: Bar chart of the final accuracy scores Ai
(Equation (19)) of energy and force prediction of lMLPs
trained by different optimizers. The uncertainty interval
(Equation (20)) is shown as cross-hatched bar around
the upper edge of the bar which equals the mean of Ai
over 20 trainings. 100%corresponds to the highest
obtained final accuracy of all optimizer specifications,
i.e., the lowest RMSE in the prediction of energies or
atomic force components. The colors of most optimizers
are listed in the legends of Figures 1 (a) and (b). The
learning rate (maximal step size for RPROP and CoRe
specifications) was adjusted, while all other
hyperparameters of the optimizers were set to (a)their
general recommendation and (b)modified values.
Exceptions are AdaGrad and SGD which do not include
additional hyperparameters beyond the learning rate.
The CoRe results are shown as reference in (b).
5 (a) and (b)). The CoRe optimizer yields smoother con-
vergence than Adam, which is beneficial, for example, in
9
(a)
(b)
Figure 3: Bar chart of the final accuracy scores Ai
(Equation (19)) of energy and force prediction of lMLPs
trained by different optimizers. The uncertainty interval
(Equation (20)) is shown as cross-hatched bar around
the upper edge of the bar which equals the mean of Ai
over 20 trainings. 100%corresponds to the highest
obtained final accuracy of all optimizer specifications,
i.e., the lowest RMSE in the prediction of energies or
atomic force components. The colors of most optimizers
are listed in the legends of Figures 1 (a) and (b). The
learning rate (maximal step size for RPROP and CoRe
specifications) was adjusted, while all other
hyperparameters of the optimizers were set to (a)their
general recommendation and (b)modified values.
Exceptions are AdaGrad and SGD which do not include
additional hyperparameters beyond the learning rate.
The CoRe results are shown as reference in (b).
5 (a) and (b)). The CoRe optimizer yields smoother con-
Figure 4: Bar chart of the final accuracy score A
(Equation (21)) combining energy and force prediction
of lMLPs for different optimizers. The uncertainty
interval (Equation (22)) is shown as cross-hatched bar
around the right edge of the bar which equals the value
ofA. A value of 100%means that the optimizer
achieves highest accuracy in energy and force
prediction. The bars are labeled and colored according
to the respective optimizer.
vergence than Adam, which is beneficial, for example, in
lifelong machine learning where the lMLP needs to be
ready for application in every training stage. The ac-
curacy scores in Figures 3 (a) and (b) and 4 take into
account the convergence smoothness (see Section 3) in
contrast to the accuracy scores in Figures S15 and S16 in
the Supporting Information which are only based on the
individuallMLPearlystoppingresults. Thelatterisben-
eficial for the Adam results but still the CoRe optimizer
with stability-plasticity balance outperforms Adam. The
convergence speed is also higher for the CoRe optimizer
than for Adam. This observation is in line with the
convergence of the SR ML task (see Figure S5 in the
Supporting Information) which also represents a training
case between mini-batch and batch learning. To demon-
strate the benefit of more stabilized learning in lMLP
training, we additionally decreased the η−value which
smoothens and improves the training process similarly.
Both, a large pfrozenand a small η−, lead also to a better
interplay with the lifelong adaptive data selection. How-
ever, this interplay is only a minor factor of the large ac-Figure 4: Bar chart of the final accuracy score A
(Equation (21)) combining energy and force prediction
of lMLPs for different optimizers. The uncertainty
interval (Equation (22)) is shown as cross-hatched bar
around the right edge of the bar which equals the value
ofA. A value of 100%means that the optimizer
achieves highest accuracy in energy and force
prediction. The bars are labeled and colored according
to the respective optimizer.
lifelong machine learning where the lMLP needs to be
ready for application in every training stage. The ac-
curacy scores in Figures 3 (a) and (b) and 4 take into
account the convergence smoothness (see Section 3) in
contrast to the accuracy scores in Figures S15 and S16 in
the Supporting Information which are only based on the
individuallMLPearlystoppingresults. Thelatterisben-
eficial for the Adam results but still the CoRe optimizer
with stability-plasticity balance outperforms Adam. The
convergence speed is also higher for the CoRe optimizer
than for Adam. This observation is in line with the
convergence of the SR ML task (see Figure S5 in the
Supporting Information) which also represents a training
case between mini-batch and batch learning. To demon-
strate the benefit of more stabilized learning in lMLP
training, we additionally decreased the η−value which
smoothens and improves the training process similarly.
Both, a large pfrozenand a small η−, lead also to a better
interplay with the lifelong adaptive data selection. How-
ever, this interplay is only a minor factor of the large ac-
curacy score improvement since the improvement is sim-

--- PAGE 10 ---
10
10
(a)
(b)
Figure 5: Test set RMSEs of (a)energy Etestand(b)
atomic force components Ftest
α,nas a function of the
training epoch nepochfor the lMLP compared to the
DFT reference. The results are shown for the seven
optimizers yielding highest final accuracy. The less often
a line is broken, the lower is the final error. Uncertainty
intervals are shown in pale color of the respective line.
curacy score improvement since the improvement is sim-
ilar in training with random data selection (see Figure
S17 in the Supporting Information). Lifelong adaptive
data selection increases the final accuracy in general. In
conclusion, averysmoothconvergenceisdesiredinlMLP
training making a smaller η−value beneficial. However,
thefinalaccuracyandconvergencespeedandsmoothness
are alreadyhigher than thoseof other state-of-the-art op-
timizers when the generally recommended hyperparame-
ter values with a stability-plasticity balance enabled by
pfrozenare applied.
In comparison to our previous work, where the best 10
of 20 lMLPs yielded RMSE( Etest)andRMSE( Ftest
α,n)to
be(4.5±0.6) meV atom−1and(116±4) meVÅ−1after
2000 training epochs with the CoRe optimizer, the gener-
ally recommended hyperparameters of this work in com-
binationwith pfrozen = 0.1(CoRepfrozen =0.1)improvedthe
accuracyto (4.1±0.7) meV atom−1and(90±5) meVÅ−1.
With an adjusted η−value for even smoother train-ing (CoRepfrozen =0.025
η−=0.55) the respective test set RMSE val-
ues decreased to only (3.4±0.4) meV atom−1and(92±
4) meVÅ−1.
Finally, the comparison of computation time for train-
ing with Adam and the CoRe optimizer shows that not
only the final accuracy but also the accuracy-cost ratio
of the CoRe optimizer is better than that of Adam. For
comparisonofmultipletrainingswiththelMLPsoftware,
the time fraction of model fitting in the entire training
process (including initialization, descriptor calculation,
model fitting (about 87%), final prediction, and final-
ization) is calculated to reduce the influence of different
computers and computation loads. The resulting speed
is the same within the uncertainty interval for Adam and
the CoRe optimizer. The additional operations in the
CoRe optimizer algorithm cause only little increase of
computational costwhichis not significant incomparison
to the cost for evaluating the loss function gradient. For
the presented lMLP example, an optimizer step requires
less than 0.2%of the time needed for a loss function
gradient calculation. Since the CoRe optimizer requires
only the loss function gradient as input like Adam and
the other optimizers, the computation time per training
epoch is similar for all optimizers.
5. CONCLUSION
TheCoReoptimizercombinesAdam-likeandRPROP-
like weight-specific learning rate adaption. Moreover, in
the CoRe optimizer step-dependent decay rates are em-
ployed in the calculation of Adam-like gradient moving
averages, which are the basis of the RPROP-like step size
updates. Itsweightdecaydependsontheabsoluteweight
update and an optional stability-plasticity balance based
on a weight importance score can be applied. In this
way, the CoRe optimizer combines the high performance
of the Adam optimizer in small mini-batch learning and
that of RPROP in full data set batch learning, while it is
superior to both in intermediate cases. With the general
hyperparameter recommendation obtained in this work
based on diverse ML tasks, the CoRe optimizer is a well-
rounded all-in-one solution with broad applicability and
highconvergencespeedandfinalaccuracyon-parandbe-
yond state-of-the-art first-order gradient-based optimiz-
ers.
The performance evaluation has further confirmed a
general advantage for optimizers which combine momen-
tum and individually adapted learning rates in terms of
convergence speed and final accuracy compared to opti-
mizers which are only adaptive or momentum based or
none of these. Moreover, adaptive and/or momentum
based methods need only marginally more computation
time than simple SGD which is negligible compared to
the time required for loss function gradient calculation.
Besides the general CoRe optimizer hyperparameter
recommendation, only the maximal step size needs to be
set depending on the fluctuations in the gradient calcu-
Figure 5: Test set RMSEs of (a)energy Etestand(b)
atomic force components Ftest
α,nas a function of the
training epoch nepochfor the lMLP compared to the
DFT reference. The results are shown for the eight
optimizers yielding highest final accuracy. The less often
a line is broken, the lower is the final error. Uncertainty
intervals are shown in pale color of the respective line.
ilar in training with random data selection (see Figure
S17 in the Supporting Information). Lifelong adaptive
data selection increases the final accuracy in general. In
conclusion, averysmoothconvergenceisdesiredinlMLP
training making a smaller η−value beneficial. However,
thefinalaccuracyandconvergencespeedandsmoothness
are alreadyhigher than thoseof other state-of-the-art op-
timizers when the generally recommended hyperparame-
ter values with a stability-plasticity balance enabled by
pfrozenare applied.
In comparison to our previous work, where the best 10
of 20 lMLPs yielded RMSE( Etest)andRMSE( Ftest
α,n)to
be(4.5±0.6) meV atom−1and(116±4) meVÅ−1after
2000 training epochs with the CoRe optimizer, the gener-
ally recommended hyperparameters of this work in com-
binationwith pfrozen = 0.1(CoRepfrozen =0.1)improvedthe
accuracyto (4.1±0.7) meV atom−1and(90±5) meVÅ−1.
With an adjusted η−value for even smoother train-
ing (CoRepfrozen =0.025
η−=0.55) the respective test set RMSE val-ues decreased to only (3.4±0.4) meV atom−1and(92±
4) meVÅ−1.
Finally, the comparison of computation time for train-
ing with Adam and the CoRe optimizer shows that not
only the final accuracy but also the accuracy-cost ratio
of the CoRe optimizer is better than that of Adam. For
comparisonofmultipletrainingswiththelMLPsoftware,
the time fraction of model fitting in the entire training
process (including initialization, descriptor calculation,
model fitting (about 87%), final prediction, and final-
ization) is calculated to reduce the influence of different
computers and computation loads. The resulting speed
is the same within the uncertainty interval for Adam and
the CoRe optimizer. The additional operations in the
CoRe optimizer algorithm cause only little increase of
computational costwhichis not significant incomparison
to the cost for evaluating the loss function gradient. For
the presented lMLP example, an optimizer step requires
less than 0.2%of the time needed for a loss function
gradient calculation. Since the CoRe optimizer requires
only the loss function gradient as input like Adam and
the other optimizers, the computation time per training
epoch is similar for all optimizers.
5. CONCLUSION
TheCoReoptimizercombinesAdam-likeandRPROP-
like weight-specific learning rate adaption. Moreover, in
the CoRe optimizer step-dependent decay rates are em-
ployed in the calculation of Adam-like gradient moving
averages, which are the basis of the RPROP-like step size
updates. Itsweightdecaydependsontheabsoluteweight
update and an optional stability-plasticity balance based
on a weight importance score can be applied. In this
way, the CoRe optimizer combines the high performance
of the Adam optimizer in small mini-batch learning and
that of RPROP in full data set batch learning, while it is
superior to both in intermediate cases. With the general
hyperparameter recommendation obtained in this work
based on diverse ML tasks, the CoRe optimizer is a well-
rounded all-in-one solution with broad applicability and
highconvergencespeedandfinalaccuracyon-parandbe-
yond state-of-the-art first-order gradient-based optimiz-
ers.
The performance evaluation has further confirmed a
general advantage for optimizers which combine momen-
tum and individually adapted learning rates in terms of
convergence speed and final accuracy compared to opti-
mizers which are only adaptive or momentum based or
none of these. Moreover, adaptive and/or momentum
based methods need only marginally more computation
time than simple SGD which is negligible compared to
the time required for loss function gradient calculation.
Besides the general CoRe optimizer hyperparameter
recommendation, only the maximal step size smaxneeds
to be set depending on the fluctuations in the gradient
calculation which can be estimated easily based on the

--- PAGE 11 ---
11
application of mini-batch ( 0.001) or batch learning ( 1)
or intermediate cases ( 0.01). Additionally, the stability-
plasticity balance can be enabled by the hyperparame-
terpfrozen. It can achieve smoother training convergence
to even higher final accuracy yielding a large improve-
ment in the example of lMLP training. We note that hy-
perparameter fine-tuning for individual ML tasks can, of
course, improve the performance to some degree for all
optimizers but comes with the drawback of being very
time consuming.
CODE AVAILABILITY
The CoRe optimizer software is available on
GitHub ( https://github.com/ReiherGroup/CoRe_
optimizer ) and PyPI ( https://pypi.org/project/
core-optimizer ‌).‌ACKNOWLEDGEMENT
This work was supported by an ETH Zurich Postdoc-
toral Fellowship.
SUPPORTING INFORMATION
Optimizer hyperparameters including adjusted learn-
ing rates or maximal step sizes; optimizer performance
comparison for ML tasks AED, AEF, ICD, ICF, SR, SS,
and TS; performance of best single models trained by
the CoRe optimizer; final accuracy for ML tasks AED,
AEF, ICD, ICF, and SR trained by the Core optimizer
with stability-plasticity balance; final accuracy obtained
by the Adam optimizer with weight decay; optimizer
performance comparison for lMLPs applying early stop-
ping; final accuracy for lMLPs trained by the CoRe opti-
mizerapplyingrandomdataselectionorlifelongadaptive
data selection (PDF file available free of charge at DOI
10.1088/2632-2153/ad1f76).
[1] C. M. Bishop, Pattern Recognition and Machine Learn-
ing(Springer, New York, NY, USA, 2006).
[2] S. Russell and P. Norvig, Artificial Intelligence: A Mod-
ern Approach , 4th ed. (Pearson, Harlow, United King-
dom, 2021).
[3] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learn-
ing(MIT Press, 2016).
[4] S. Sun, Z. Cao, H. Zhu, and J. Zhao, arXiv:1906.06821
[cs.LG] (2019).
[5] H. Robbins and S. Monro, Ann. Math. Stat. 22, 400
(1951).
[6] B. T. Polyak, USSR Comput. Math. Math. Phys. 4, 1
(1964).
[7] Y. E. Nesterov, Dokl. Akad. Nauk SSSR 269, 543 (1983).
[8] I. Sutskever, J. Martens, G. Dahl, and G. Hinton, in 30th
International Conference on Machine Learning (ICML)
(Atlanta, GA, USA, 2013) pp. 1139–1147.
[9] J. Duchi, E. Hazan, and Y. Singer, J. Mach. Learn. Res.
12, 2121 (2011).
[10] M. D. Zeiler, arXiv:1212.5701 [cs.LG] (2012).
[11] G. Hinton, N. Srivastava, and K. Swersky, in Neural Net-
works for Machine Learning (Toronto, Canada, 2012).
[12] D. P. Kingma and J. Ba, in 3rdInternational Conference
on Learning Representations (ICLR) (San Diego, CA,
USA, 2015).
[13] M. Eckhoff and M. Reiher, J. Chem. Theory Comput.
19, 3509 (2023).
[14] M. Riedmiller and H. Braun, in International Conference
on Neural Networks (ICNN) (San Francisco, CA, USA,
1993) pp. 586–591.
[15] M. Riedmiller, Comput. Stand. Interfaces 16, 265 (1994).
[16] T. Dozat, in 4thInternational Conference on Learning
Representations (ICLR) (San Juan, Puerto Rico, 2016).
[17] S. J. Reddi, S. Kale, and S. Kumar, in 6thInternational
Conference on Learning Representations (ICLR) (Van-
couver, Canada, 2018).[18] X. Xie, P. Zhou, H. Li, Z. Lin, and S. Yan,
arXiv:2208.06677 [cs.LG] (2023).
[19] N. Shazeer and M. Stern, in 35thInternational Confer-
ence on Machine Learning (ICML) (Stockholm, Sweden,
2018).
[20] L.Luo, Y.Xiong, Y.Liu,andX.Sun,in 7thInternational
Conference on Learning Representations (ICLR) (New
Orleans, LA, USA, 2019).
[21] J. Zhuang, T. Tang, Y. Ding, S. Tatikonda, N. Dvornek,
X. Papademetris, and J. S. Duncan, in 34thConference
on Neural Information Processing Systems (NeurIPS)
(Vancouver, Canada, 2020).
[22] I. Loshchilov and F. Hutter, in 7thInternational Confer-
ence on Learning Representations (ICLR) (New Orleans,
LA, USA, 2019).
[23] J. Chen, D. Zhou, Y. Tang, Z. Yang, Y. Cao, and Q. Gu,
in29thInternational Joint Conference on Artificial In-
telligence (IJCAI) (Yokohama, Japan, 2020).
[24] L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and
J. Han, in 8thInternational Conference on Learning Rep-
resentations (ICLR) (2020).
[25] B. Heo, S. Chun, S. J. Oh, D. Han, S. Yun, G. Kim,
Y. Uh, and J.-W. Ha, in 9thInternational Conference on
Learning Representations (ICLR) (2021).
[26] Y. You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojana-
palli, X. Song, J. Demmel, K. Keutzer, and C.-J. Hsieh,
in8thInternational Conference on Learning Representa-
tions (ICLR) (2020).
[27] D. Bahrami and S. P. Zadeh, arXiv:2101.09192 [cs.LG]
(2021).
[28] X. Chen, C. Liang, D. Huang, E. Real, K. Wang, Y. Liu,
H. Pham, X. Dong, T. Luong, C.-J. Hsieh, Y. Lu, and
Q. V. Le, arXiv:2302.06675 [cs.LG] (2023).
[29] Z. Yao, A. Gholami, S. Shen, M. Mustafa, K. Keutzer,
and M. Mahoney, in 35thAAAI Conference on Artificial
Intelligence (2021) pp. 10665–10673.

--- PAGE 12 ---
12
[30] H. Liu, Z. Li, D. Hall, P. Liang, and T. Ma,
arXiv:2305.14342 [cs.LG] (2023).
[31] F. Schneider, L. Balles, and P. Hennig, in 7thInterna-
tional Conference on Learning Representations (ICLR)
(New Orleans, LA, USA, 2019).
[32] D. Choi, C. J. Shallue, Z. Nado, J. Lee, C. J. Maddi-
son, and G. E. Dahl, in 8thInternational Conference on
Learning Representations (ICLR) (2020).
[33] R. M. Schmidt, F. Schneider, and P. Hennig, in 38th
International Conference on Machine Learning (ICML)
(2021).
[34] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury,
G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga,
A. Desmaison, A. Köpf, E. Yang, Z. DeVito, M. Rai-
son, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang,
J. Bai, and S. Chintala, in 33rdInternational Conference
on Neural Information Processing Systems (NIPS) (Van-
couver, Canada, 2019) pp. 8026–8037.
[35] L. Deng, IEEE Signal Process. Mag. 29, 141 (2012).
[36] H. Xiao, K. Rasul, and R. Vollgraf, arXiv:1708.07747
[cs.LG] (2017).
[37] D. P. Kingma and M. Welling, in 2rdInternational
Conference on Learning Representations (ICLR) (Banff,
Canada, 2014).
[38] K. Fukushima, Biol. Cybern. 36, 193 (19).
[39] A. F. Agarap, arXiv:1803.08375 [cs.NE] (2019).
[40] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever,
and R. Salakhutdinov, J. Mach. Learn. Res. 15, 1929
(2014).
[41] D. Scherer, A. Müller, and S. Behnke, in 20thIn-
ternational Conference on Artificial Neural Networks
(ICANN) (Thessaloniki, Greece, 2010) pp. 92–101.
[42] A. G. Barto, R. S. Sutton, and C. W. Anderson, IEEE
Trans. Syst. Man Cybern. 13, 834 (1983).
[43] F. Rosenblatt, Psychol. Rev. 65, 386 (1958).
[44] V. Konda and J. Tsitsiklis, in 12thInternational Confer-
ence on Neural Information Processing Systems (NIPS)
(Denver CO, USA, 1999) pp. 1008–1014.
[45] D. Martin, C. Fowlkes, D. Tal, and J. Malik, in 8thIEEE
International Conference on Computer Vision (ICCV)
(Vancouver, Canada, 2001) pp. 416–423.
[46] W. Shi, J. Caballero, F. Huszár, J. Totz, A. P. Aitken,
R. Bishop, D. Rueckert, and Z. Wang, in IEEE Con-
ference on Computer Vision and Pattern Recognition
(CVPR) (Las Vegas, NV, USA, 2016) pp. 1874–1883.[47] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher,
and T. Eliassi-Rad, AI Mag. 29, 93 (2008).
[48] T. N. Kipf and M. Welling, in 5thInternational Con-
ference on Learning Representations (ICLR) (Toulon,
France, 2017).
[49] S. Hochreiter and J. Schmidhuber, Neural Comput. 9,
1735 (1997).
[50] J. Behler, J. Chem. Phys. 145, 170901 (2016).
[51] A. P. Bartók, S. De, C. Poelking, N. Bernstein, J. R. Ker-
mode, G. Csányi, and M. Ceriotti, Sci. Adv. 3, e1701816
(2017).
[52] V. L. Deringer, M. A. Caro, and G. Csányi, Adv. Mater.
31, 1902765 (2019).
[53] F. Noé, A. Tkatchenko, K.-R. Müller, and C. Clementi,
Ann. Rev. Phys. Chem. 71, 361 (2020).
[54] J. Westermayr, M. Gastegger, K. T. Schütt, and R. J.
Maurer, J. Chem. Phys. 154, 230903 (2021).
[55] S. Käser, L. I. Vazquez-Salazar, M. Meuwly, and
K. Töpfer, Digital Discovery 2, 28 (2023).
[56] J. Behler and M. Parrinello, Phys. Rev. Lett. 98, 146401
(2007).
[57] J. Behler, Angew. Chem. Int. Ed. 56, 12828 (2017).
[58] J. Behler, Chem. Rev. 121, 10037 (2021).
[59] M. Eckhoff, K. N. Lausch, P. E. Blöchl, and J. Behler, J.
Chem. Phys. 153, 164107 (2020).
[60] M. Eckhoff and J. Behler, npj Comput. Mater. 7, 170
(2021).
[61] M. Eckhoff and J. Behler, J. Chem. Theory Comput. 15,
3793 (2019).
[62] M.Eckhoff, F.Schönewald, M.Risch, C.A.Volkert, P.E.
Blöchl, and J. Behler, Phys. Rev. B 102, 174102 (2020).
[63] M. Eckhoff and J. Behler, J. Chem. Phys. 155, 244703
(2021).
[64] F. Zenke, B. Poole, and S. Ganguli, in 34thInternational
Conference on Machine Learning (ICML) (Sydney, Aus-
tralia, 2017) pp. 3987–3995.
[65] PyTorch Examples, GitHub repository, commit 7f7c222
(2023).
[66] M. Eckhoff and M. Reiher, Zenodo 10.5281/zen-
odo.10391807 (2023).
[67] M. Eckhoff and M. Reiher, Zenodo 10.5281/zen-
odo.10512719 (2024).
[68] M. Eckhoff and M. Reiher, Zenodo 10.5281/zen-
odo.7912832 (2023).
[69] G. Brockman, V. Cheung, L. Pettersson, J. Schnei-
der, J. Schulman, J. Tang, and W. Zaremba,
arXiv:1606.01540 [cs.LG] (2016).
