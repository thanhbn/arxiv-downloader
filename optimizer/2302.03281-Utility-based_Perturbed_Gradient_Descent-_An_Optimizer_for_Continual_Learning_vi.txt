# 2302.03281.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/optimizer/2302.03281.pdf
# Kích thước tệp: 7960860 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Thuật toán Gradient Descent Nhiễu dựa trên Tiện ích: Một Bộ Tối ưu cho Học Liên tục
Mohamed Elsayed1 2A. Rupam Mahmood1 2 3

Tóm tắt
Các phương pháp học biểu diễn hiện đại thường
gặp khó khăn trong việc thích ứng nhanh chóng
trong điều kiện phi dừng vì chúng bị ảnh hưởng
bởi việc quên thảm khốc và tính dẻo dai suy giảm.
Những vấn đề này ngăn cản người học thích ứng
nhanh vì chúng có thể quên các đặc trưng hữu ích
hoặc gặp khó khăn trong việc học những đặc trưng
mới. Do đó, các phương pháp này trở nên không
hiệu quả cho việc học liên tục. Bài báo này đề xuất
Thuật toán Gradient Descent Nhiễu dựa trên Tiện
ích (UPGD), một thuật toán học trực tuyến phù hợp
cho các tác nhân học liên tục. UPGD bảo vệ các
trọng số hoặc đặc trưng hữu ích khỏi việc quên và
nhiễu các trọng số ít hữu ích hơn dựa trên tiện ích
của chúng. Kết quả thực nghiệm của chúng tôi cho
thấy UPGD giúp giảm việc quên và duy trì tính dẻo
dai, cho phép các phương pháp học biểu diễn hiện
đại hoạt động hiệu quả trong học liên tục.

1. Giới thiệu
Học trực tuyến là quan trọng đối với các hệ thống cần
liên tục thích ứng với thế giới luôn thay đổi. Những
người học này thực hiện cập nhật ngay khi dữ liệu
đến, cho phép thích ứng nhanh. Các người học suốt
đời như vậy không bao giờ ngừng học và có thể chạy
vô hạn. Do đó, tính toán và bộ nhớ cần thiết của chúng
không nên tăng khi được trình bày với nhiều kinh
nghiệm hơn. Hơn nữa, người ta mong muốn nếu
những người học này có chi phí tính toán rẻ và duy
trì dấu chân bộ nhớ nhỏ, một tiêu chí mà chúng tôi
gọi là hiệu quả tính toán. Do đó, các tiêu chí mong
muốn cho học liên tục là thích ứng trực tuyến nhanh
và hiệu quả tính toán.

Các phương pháp gradient descent hiện tại ngăn cản
thích ứng trực tuyến nhanh của hệ thống học liên tục.
Các phương pháp này thường gặp phải hai vấn đề chính
làm chậm quá trình thích ứng: tính dẻo dai suy giảm
(Dohare et al. 2021) và quên thảm khốc (McCloskey
& Cohen 1989). Tính dẻo dai suy giảm xảy ra khi khả
năng học biểu diễn của người học bị cản trở (ví dụ,
khi số lượng đặc trưng bão hòa tăng, dẫn đến gradient
nhỏ cản trở những thay đổi nhanh chóng đối với trọng
số). Quên thảm khốc có thể được xem một phần như
kết quả của các biểu diễn được chia sẻ, thay vì biểu
diễn thưa thớt, trong xấp xỉ hàm (French 1991, Liu
et al. 2019). Các cập nhật trọng số là phi cục bộ—
thay đổi nhiều trọng số—trong trường hợp biểu diễn
dày đặc vì nhiều đặc trưng hoạt động cho bất kỳ đầu
vào nào, can thiệp vào các biểu diễn đã học trước đó
và có thể gây ra quên. Ngoài ra, quên thảm khốc được
làm trầm trọng hơn bởi việc các phương pháp gradient
descent hiện tại không thể tái sử dụng các đặc trưng
hữu ích đã học trước đó hoặc bảo vệ chúng khỏi thay
đổi. Đã được chứng minh rằng các phương pháp hiện
tại có thể phá hủy các đặc trưng hữu ích nhất trong
điều kiện phi dừng (Sutton 1986), làm cho những
phương pháp này phải hủy học và học lại các đặc trưng
khi người học đối mặt với những tình huống tương tự
hoặc giống nhau một lần nữa.

Tự nhiên, tính dẻo dai suy giảm và quên thảm khốc
xảy ra đồng thời trong mạng neural vì mạng neural
sử dụng biểu diễn dày đặc được học bởi các phương
pháp dựa trên gradient, và có thể khó để nhanh chóng
thay đổi các hàm mà chúng biểu diễn. Biểu diễn dày
đặc tạo ra tính phi cục bộ, dẫn đến quên, và nhiều
yếu tố, chẳng hạn như gradient biến mất hoặc các
đặc trưng tương quan, góp phần vào tính dẻo dai suy
giảm.

Các cách tiếp cận khác nhau đã được đề xuất để giảm
thiểu quên thảm khốc. Thông thường, các phương pháp
như vậy là phương pháp dựa trên phát lại (ví dụ,
Chaudhry et al. 2019, Isele & Cosgun 2018, Rolnick
et al. 2019), dựa trên chính quy hóa (ví dụ, Kirkpatrick
et al. 2017, Aljundi et al. 2018, Aljundi et al. 2019),
tạo ra tính thưa thớt (ví dụ, Liu et al. 2019, Pan et
al. 2021), hoặc sử dụng kiến trúc động (ví dụ, Rusu
et al. 2016, Schwarz et al. 2018). Tuy nhiên, hầu hết
các phương pháp không thể thỏa mãn tiêu chí hiệu
quả hoặc thích ứng trực tuyến nhanh của chúng tôi.
Ngoài ra, tất cả các phương pháp giải quyết quên thảm
khốc vẫn gặp phải tính dẻo dai suy giảm, có thể được
giảm thiểu bằng cách liên tục tiêm nhiễu (ví dụ, Ash
& Adams 2020, Zhou et al. 2019, Dohare et al. 2021,
Orvieto et al. 2022). Tuy nhiên, các phương pháp như
vậy vẫn có thể gặp phải quên thảm khốc.

Cả hai vấn đề của học liên tục có thể được xác định
với thực tế là các phương pháp gradient descent thờ
ơ với việc một đặc trưng hữu ích như thế nào. Khi
một đặc trưng trở nên khó sửa đổi, góp phần vào tính
dẻo dai suy giảm, vấn đề có thể được khắc phục bằng
cách đặt lại hoặc khởi tạo lại đặc trưng đó. Ngược lại,
khi một đặc trưng hữu ích và đóng góp tốt cho một
tác vụ, nó có thể được bảo vệ khỏi thay đổi tiếp theo
và quên thảm khốc. Tuy nhiên, không có phương pháp
gradient descent hiện tại nào chứa khả năng như vậy.

Trong bài báo này, chúng tôi trình bày Thuật toán
Gradient Descent Nhiễu dựa trên Tiện ích (UPGD),
một cơ chế học trực tuyến mới bảo vệ các trọng số
hoặc đặc trưng hữu ích và nhiễu các trọng số ít hữu
ích hơn dựa trên tiện ích của chúng. UPGD tái sử dụng
các đặc trưng hữu ích và xây dựng các đặc trưng mới
dựa trên những đặc trưng hiện có, cho phép thích ứng
nhanh. Hơn nữa, UPGD ước tính tiện ích của mỗi trọng
số hoặc đặc trưng bằng cách sử dụng độ phức tạp tính
toán tuyến tính như SGD mà không lưu trữ các mẫu
trong quá khứ, làm cho phương pháp của chúng tôi
có thể mở rộng về mặt tính toán. Ngoài ra, UPGD
không yêu cầu kiến thức về ranh giới tác vụ, làm cho
nó trở thành một phương pháp học liên tục không có
tác vụ (Aljundi et al. 2019).

Quy tắc học của UPGD chứa hai thành phần giúp
thích ứng trực tuyến nhanh: tìm kiếm và gradient.
Mỗi thành phần được trọng số hóa bởi các ước tính
tiện ích của các trọng số hoặc đặc trưng tương ứng.
Thành phần tìm kiếm có thông tin tiện ích giúp tìm
ra các trọng số và đặc trưng tốt hơn thông qua nhiễu
liên tục, giúp chống lại tính dẻo dai suy giảm. Gradient
dựa trên tiện ích là một tín hiệu gradient được điều
chế—nhỏ cho các trọng số hữu ích và lớn cho các
trọng số ít hữu ích hơn—giúp chống lại quên thảm
khốc.

2. Công thức hóa Vấn đề
Chúng tôi sử dụng thiết lập học có giám sát liên tục
trực tuyến nơi có một dòng các ví dụ dữ liệu. Các
ví dụ dữ liệu này được tạo ra từ một số hàm mục tiêu
phi dừng ft ánh xạ đầu vào thành đầu ra, trong đó
cặp đầu vào-đầu ra tại thời điểm t là (xt; yt). Người
học được yêu cầu dự đoán đầu ra cho vector đầu vào
xt ∈ Rd bằng cách ước tính hàm mục tiêu ft. Chúng
tôi xem xét trường hợp ft không phi dừng một cách
tùy tiện mà là cục bộ dừng theo thời gian (ví dụ,
thay đổi chậm hoặc có thể được chia thành các tác
vụ dừng). Chúng tôi cũng xem xét các hàm mục tiêu
với các tính chất quy luật có thể tái xuất hiện. Hiệu
suất được đo bằng một số hàm mất mát, L(yt, ŷt),
trong đó yt ∈ Rm là vector mục tiêu và ŷt ∈ Rm là
đầu ra được dự đoán. Cụ thể, sai số bình phương trung
bình được sử dụng trong hồi quy, và entropy chéo
được sử dụng trong phân loại. Người học được yêu
cầu giảm hàm mất mát bằng cách khớp mục tiêu. Hiệu
suất của người học được đo dựa trên chỉ số đánh giá
trực tuyến trung bình E trên tất cả các bước thời gian,
được đưa ra bởi

E(T) = 1/T ∑[t=1 to T] E(yt, ŷt);                (1)

trong đó E là chỉ số đánh giá mẫu (ví dụ, độ chính
xác hoặc mất mát), và T là tổng số bước thời gian.
Chúng tôi lưu ý ở đây rằng công thức hóa vấn đề này
đã được giới thiệu trước đó bởi Caccia et al. (2020),
nơi hiệu suất được đo dựa trên chỉ số đánh giá trực
tuyến; tuy nhiên, người ta giả định rằng chúng ta bắt
đầu với những người học được huấn luyện trước.
Ở đây, chúng tôi xem xét thiết lập nơi người học học
từ đầu. Lưu ý rằng chỉ số đánh giá trực tuyến này
tương tự như tổng tích lũy của phần thưởng trong
học tăng cường.

Xem xét một mạng neural với L lớp xuất ra đầu ra
được dự đoán ŷ. Mạng neural được tham số hóa bởi
tập hợp các trọng số W = {W1, ..., WL}, trong đó
Wl là ma trận trọng số tại lớp thứ l, và phần tử của
nó tại hàng thứ i và cột thứ j được ký hiệu bởi Wl,i,j.
Trong quá trình học, các tham số của mạng neural
được thay đổi để giảm hàm mất mát. Tại mỗi lớp l,
chúng ta nhận được đầu ra kích hoạt hl bằng cách
áp dụng hàm kích hoạt vào đầu vào kích hoạt al:
hl = σ(al). Chúng tôi đơn giản hóa ký hiệu bằng
cách định nghĩa h0 := x. Đầu ra kích hoạt hl sau đó
được nhân với ma trận trọng số Wl+1 của lớp l+1
để tạo ra đầu vào kích hoạt tiếp theo: al+1,i = 
∑[j=1 to |hl|] Wl+1,i,j hl,j. Chúng tôi giả định ở đây
rằng hàm kích hoạt là kích hoạt theo từng phần tử
cho tất cả các lớp ngoại trừ lớp cuối cùng L trong
các tác vụ phân loại, nơi nó trở thành hàm softmax.

2.1. Tính dẻo dai Suy giảm
Tính dẻo dai neural có thể được định nghĩa là khả
năng của mạng neural thay đổi để đáp ứng với một
số kích thích (Konorski 1948, Hebb 1949). Trong
mạng neural nhân tạo, chúng ta có thể nghĩ về tính
dẻo dai như khả năng thay đổi hàm được biểu diễn
bởi mạng neural. Lưu ý rằng định nghĩa này dựa
trên khả năng thay đổi hàm, không phải các trọng
số cơ bản. Ví dụ, một người học có thể cập nhật các
trọng số trong một mạng neural được khởi tạo hằng
số. Tuy nhiên, khả năng thay đổi hàm của nó khá
hạn chế vì các tính đối xứng đặc trưng, mặc dù các
trọng số có thể được thay đổi. Chúng tôi lưu ý ở đây
rằng thuật ngữ tính dẻo dai khác với sức mạnh biểu
diễn hoặc dung lượng, tham chiếu đến phạm vi các
hàm có thể được biểu diễn bởi một số kiến trúc. Trong
bài báo này, chúng tôi sử dụng các mạng có cùng
dung lượng và cho thấy chúng mất tính dẻo dai khi
được sử dụng với các phương pháp dựa trên gradient
điển hình.

Chỉ số đánh giá trực tuyến của chúng tôi ưu tiên
các phương pháp thích ứng nhanh với các thay đổi
vì nó không chỉ là một hàm của hiệu suất cuối cùng.
Một người học có tính dẻo dai có thể thích ứng nhanh
hơn với các thay đổi so với một người học ít dẻo dai
hơn và do đó đạt được hiệu suất tích lũy trung bình
cao hơn. Chỉ số đánh giá như vậy là tự nhiên để đo
tính dẻo dai khi người học được trình bày với các
tác vụ tuần tự yêu cầu ít chuyển giao giữa chúng.
Các tác vụ hoán vị đầu vào (ví dụ, MNIST hoán vị)
thỏa mãn tiêu chí này vì các biểu diễn được học trong
một tác vụ có phần không liên quan đến tác vụ tiếp
theo. Khi các phương pháp dựa trên gradient hiện tại
được trình bày với các tác vụ hoán vị đầu vào tuần
tự, hiệu suất của chúng giảm. Sự giảm sút như vậy
được giải thích một phần bởi nhiều giả thuyết, chẳng
hạn như số lượng đặc trưng bão hòa tăng không thể
thích ứng nhanh (Dohare et al. 2021, Abbas et al.
2023) hoặc cảnh quan hàm mất mát thay đổi trong
điều kiện phi dừng, làm cho việc điều chỉnh hàm khó
khăn hơn (Lyle et al. 2023).

2.2. Quên Thảm khốc
Quên thảm khốc là xu hướng của mạng neural quên
thông tin trong quá khứ sau khi học từ những kinh
nghiệm mới. Ví dụ, khi một tác nhân học hai tác vụ
tuần tự, A và B, khả năng nhớ A của nó sẽ giảm sút
thảm khốc sau khi học B. Chúng tôi mở rộng định
nghĩa đó để quan tâm đến các đặc trưng đã học thay
vì các tác vụ.

Việc quên được kiểm tra thiếu trong thiết lập trực
tuyến nơi các tác nhân không bao giờ ngừng học.
Chỉ số của chúng tôi cho phép chúng tôi tự nhiên
có chỉ số dựa trên học lại (Hetherington & Seidenberg
1989) để đo việc quên bằng cách xem xét thời gian
cần thiết để học lại một tác vụ mới. Điều này phù
hợp cho học trực tuyến vì chúng tôi không ngừng
tác nhân học để đánh giá hiệu suất ngoại tuyến của
nó, điều này được yêu cầu bởi chỉ số dựa trên duy
trì (McCloskey & Cohen 1989).

Việc tách biệt quên thảm khốc khỏi tính dẻo dai suy
giảm là thách thức vì không rõ làm thế nào để tạo
ra một mạng không mất tính dẻo dai. Hầu hết các
công trình giải quyết việc quên sử dụng các mạng
mất tính dẻo dai, điều này làm tăng thêm sự giảm
sút hiệu suất đến từ việc quên. Chúng tôi cũng cho
thấy rằng ngay cả các mạng neural tuyến tính (ví dụ,
sử dụng kích hoạt định danh) cũng có thể mất tính
dẻo dai, điều này làm cho vấn đề tách biệt chúng trở
nên thách thức hơn.

Việc nghiên cứu quên thảm khốc có thể được thực
hiện bằng cách sử dụng một vấn đề yêu cầu chuyển
giao từ tác vụ này sang tác vụ khác. Một người học
lý tưởng nên có thể sử dụng các đặc trưng hữu ích
đã học trước đó từ tác vụ này sang tác vụ khác. Tuy
nhiên, một phương pháp dựa trên gradient điển hình
(ví dụ, SGD) tiếp tục học lại các đặc trưng hữu ích
lặp đi lặp lại vì việc quên, dẫn đến sự giảm sút hiệu
suất.

Trong bài báo này, chúng tôi trình bày một phương
pháp để giảm việc quên và duy trì tính dẻo dai một
cách có nguyên tắc bằng cách bảo vệ các trọng số
hoặc đặc trưng hữu ích và loại bỏ những đặc trưng
ít hữu ích hơn.

3. Phương pháp
Việc ước tính tiện ích của trọng số và đặc trưng có
thể giúp bảo vệ các trọng số và đặc trưng hữu ích
và loại bỏ những cái ít hữu ích hơn. Tiện ích của
một trọng số hoặc một đặc trưng trong mạng neural
được sử dụng bởi người học liên tục trực tuyến từ
bước thời gian t đến bước thời gian t+h có thể được
định nghĩa là tiện ích tức thời trung bình trong các
bước thời gian này, có thể được viết như sau:

Ut:t+h := 1/h ∑[k=0 to h] Ut+k

Vì các hệ thống liên tục trực tuyến chạy vô hạn,
chúng tôi quan tâm đến tiện ích tức thời trung bình
trên tất cả các bước thời gian tương lai bắt đầu từ
bước thời gian t, được đưa ra bởi

Ut := lim[k→∞] Ut:t+k

Tuy nhiên, trung bình của các tiện ích tương lai là
không biết tại bước thời gian t. Người học chỉ có
thể tính toán tiện ích tức thời Ut. Người ta có thể
ước tính đại lượng không biết như vậy bằng cách
trung bình hóa các tiện ích tức thời trong quá khứ
(ví dụ, dấu vết tiện ích), hy vọng rằng quá khứ giống
với tương lai. Trong phân tích của chúng tôi, chúng
tôi tập trung vào việc tính toán tiện ích tức thời,
nhưng chúng tôi sử dụng dấu vết tiện ích trong các
thí nghiệm của mình.

Tiện ích tức thời thực của một trọng số trong mạng
neural có thể được định nghĩa là thay đổi trong hàm
mất mát sau khi loại bỏ trọng số đó (Mozer & Smolensky
1988, Karnin 1990). Một trọng số quan trọng, khi
được loại bỏ, sẽ làm tăng hàm mất mát. Ma trận
tiện ích tương ứng với các kết nối lớp thứ l là Ul(Z),
trong đó Z = (X, Y) biểu thị mẫu. Chúng tôi bỏ ký
hiệu thời gian để dễ viết nhưng nhấn mạnh tính tức
thời của tiện ích bằng cách viết nó như một hàm
của mẫu. Tiện ích thực của trọng số i,j trong lớp
thứ l cho mẫu Z được định nghĩa như sau:

Ul,i,j(Z) := L(W\[l,i,j], Z) - L(W, Z);           (2)

trong đó L(W, Z) là hàm mất mát mẫu cho tập hợp
các tham số W, và W\[l,i,j] giống như W ngoại trừ
trọng số Wl,i,j được đặt thành 0.

Tương tự, chúng tôi định nghĩa tiện ích thực của
đặc trưng i tại lớp l, được biểu diễn như thay đổi
trong hàm mất mát sau khi đặc trưng được loại bỏ.
Tiện ích của đặc trưng i trong lớp thứ l được đưa ra
bởi

ul,j(Z) := L(W, Z|hl,j = 0) - L(W, Z);           (3)

trong đó hl,j = 0 biểu thị việc đặt kích hoạt của
đặc trưng thành không (ví dụ, bằng cách thêm một
mặt nạ được đặt thành không).

Lưu ý rằng cả hai thước đo tiện ích này đều là thước
đo toàn cục, và chúng cung cấp một thứ tự tổng thể
cho các trọng số và đặc trưng theo tầm quan trọng
của chúng. Tuy nhiên, việc tính toán tiện ích thực
như vậy là cấm đoán vì nó yêu cầu thêm Nw hoặc
Nf lần lan truyền tiến, trong đó Nw là tổng số trọng
số và Nf là tổng số đặc trưng. Việc xấp xỉ tiện ích
thực giúp giảm tính toán cần thiết để tính toán tiện
ích sao cho không cần thêm lần lan truyền tiến nào.

3.1. Tiện ích Trọng số Xấp xỉ
Chúng tôi xấp xỉ tiện ích thực của trọng số bằng
xấp xỉ Taylor bậc hai. Để viết tiện ích ul,i,j của kết
nối ij tại lớp l, chúng tôi mở rộng tiện ích thực xung
quanh trọng số hiện tại của kết nối ij tại lớp l và
đánh giá nó tại giá trị của trọng số đó bằng không.
Xấp xỉ bậc hai của Ul,i,j(Z) có thể được viết như

Ul,i,j(Z) = L(W\[l,i,j], Z) - L(W, Z)
         ≈ L(W, Z) + ∂L(W, Z)/∂Wl,i,j (0 - Wl,i,j)
         + 1/2 ∂²L/∂W²l,i,j (0 - Wl,i,j)² - L(W, Z)
         = -∂L(W, Z)/∂Wl,i,j Wl,i,j + 1/2 ∂²L(W, Z)/∂W²l,i,j W²l,i,j

Chúng tôi gọi thước đo tiện ích chứa số hạng đầu tiên
là tiện ích trọng số xấp xỉ bậc nhất, và thước đo tiện
ích chứa cả hai số hạng là tiện ích trọng số xấp xỉ
bậc hai. Tính toán cần thiết cho số hạng bậc hai có
độ phức tạp bậc hai. Do đó, chúng tôi tiếp tục xấp
xỉ nó bằng cách sử dụng HesScale (Elsayed & Mahmood
2022; xem Phụ lục E), làm cho việc tính toán của
cả hai xấp xỉ này có độ phức tạp tuyến tính. Hơn
nữa, chúng tôi trình bày một cách để lan truyền các
tiện ích xấp xỉ của chúng tôi bằng định lý lan truyền
tiện ích trong Phụ lục A.

3.2. Tiện ích Đặc trưng Xấp xỉ
Ở đây, chúng tôi dẫn xuất tiện ích toàn cục của đặc
trưng j tại lớp l, được biểu diễn như sự khác biệt
giữa hàm mất mát khi đặc trưng được loại bỏ và hàm
mất mát gốc. Để có một dẫn xuất dễ dàng hơn, chúng
tôi thêm một mặt nạ (hoặc cổng) lên trên đầu ra kích
hoạt: hl = ml ⊙ hl. Lưu ý rằng các trọng số của các
mặt nạ như vậy được đặt thành một và không bao
giờ thay đổi trong suốt quá trình học. Xấp xỉ bậc
hai của ul,j(Z) có thể được viết như

ul,j(Z) = L(W, Z|ml,j = 0) - L(W, Z)
        ≈ L(W, Z) + ∂L/∂ml,i (0 - ml,j)
        + 1/2 ∂²L/∂m²l,i (0 - ml,j)² - L(W, Z)
        = -∂L/∂ml,i + 1/2 ∂²L/∂m²l,i

Chúng tôi tiếp tục xấp xỉ nó bằng cách sử dụng HesScale,
làm cho việc tính toán có độ phức tạp tính toán tuyến
tính.

Khi chúng tôi sử dụng hàm kích hoạt đi qua gốc,
chúng tôi thay vào đó có thể mở rộng hàm mất mát
xung quanh đầu vào kích hoạt hiện tại al,i mà không
cần sử dụng mặt nạ. Lưu ý rằng các đạo hàm đối
với đầu vào kích hoạt có sẵn từ lan truyền ngược.
Tiện ích đặc trưng có thể đơn giản là:

ul,j(Z) = L(W, Z|al,j = 0) - L(W, Z)
        ≈ -∂L/∂al,i al,i + 1/2 ∂²L/∂a²l,i a²l,i

Hơn nữa, tiện ích đặc trưng xấp xỉ có thể được tính
toán bằng cách sử dụng tiện ích trọng số xấp xỉ, điều
này làm nảy sinh tính chất bảo toàn tiện ích. Chúng
tôi cho thấy mối quan hệ như vậy và chứng minh của
nó trong Phụ lục B.

3.3. Tìm kiếm Nhiễu dựa trên Tiện ích
Tìm kiếm trong không gian trọng số hoặc không gian
đặc trưng giúp tìm ra các trọng số và đặc trưng tốt
hơn. Chúng tôi tạo ra một quy tắc học mới dựa trên
tìm kiếm có thông tin tiện ích. Cụ thể, chúng tôi bảo
vệ các trọng số hoặc đặc trưng hữu ích và nhiễu những
cái ít hữu ích hơn. Chúng tôi cho thấy quy tắc của
Tìm kiếm Nhiễu dựa trên Tiện ích (UPS) để cập nhật
các trọng số. Chúng tôi nhấn mạnh ở đây rằng tìm
kiếm nhiễu dựa trên tiện ích không di chuyển theo
cùng hướng với gradient; do đó, nó không phải là
cập nhật gradient descent. Quy tắc cập nhật của UPS
được định nghĩa như sau:

wl,i,j ← wl,i,j - α ε (1 - Ul,i,j);               (4)

trong đó ε là một mẫu nhiễu, α là kích thước bước,
và Ul,i,j là tiện ích được tỷ lệ với tối thiểu là 0 và
tối đa là 1. Việc tỷ lệ như vậy giúp bảo vệ các trọng
số hữu ích và nhiễu những cái ít hữu ích hơn. Ví dụ,
một trọng số với Ul,i,j = 1 không bị nhiễu, trong
khi một trọng số với Ul,i,j = 0 bị nhiễu bởi toàn
bộ mẫu nhiễu. Loại nhiễu và lượng nhiễu đóng vai
trò quan trọng trong tìm kiếm. Hơn nữa, độ khó tìm
kiếm là một hàm của kích thước không gian tìm kiếm,
có nghĩa là chúng ta có thể mong đợi tìm kiếm trong
không gian đặc trưng dễ dàng hơn so với trong không
gian trọng số. Chúng ta có thể lấy lại các thuật toán
UPS theo trọng số và theo đặc trưng bằng cách bỏ
thông tin gradient từ phương trình cập nhật trong
Thuật toán 1 và Thuật toán 2.

Quy tắc học UPS giống như một quá trình tiến hóa
trên các trọng số hoặc đặc trưng nơi mỗi bước thời
gian giống như một quần thể mới của trọng số hoặc
đặc trưng, và tiện ích được tỷ lệ giống như hàm thích
ứng giữ lại các trọng số hoặc đặc trưng hữu ích (thích
ứng nhất) hoặc nhiễu những cái ít hữu ích hơn.

3.4. Thuật toán Gradient Descent Nhiễu dựa trên Tiện ích
Mục tiêu của chúng tôi là viết một phương trình cập
nhật để bảo vệ các trọng số hữu ích và thay thế những
cái ít hữu ích hơn bằng cách sử dụng tìm kiếm và
gradient. Chúng tôi cho thấy ở đây quy tắc của Thuật
toán Gradient Descent Nhiễu dựa trên Tiện ích (UPGD)
để cập nhật các trọng số. Phương trình cập nhật được
đưa ra bởi

wl,i,j ← wl,i,j - α ∂L/∂wl,i,j + αε(1 - Ul,i,j).   (5)

Thông tin tiện ích trong UPGD hoạt động như một
cổng cho các gradient nhiễu. Đối với các trọng số
quan trọng với tiện ích Ul,i,j = 1, trọng số không
được cập nhật, trong khi các trọng số không quan
trọng với tiện ích Ul,i,j = 0 được cập nhật bởi toàn
bộ thông tin gradient nhiễu. Chúng tôi lưu ý ở đây
rằng UPGD có thể tạo ra tính dẻo dai bằng cách nhiễu
các trọng số ít hữu ích hơn và giảm việc quên bằng
cách bảo vệ các trọng số hữu ích.

Một biến thể khác của UPGD, chúng tôi gọi là UPGD
không bảo vệ, là thêm nhiễu dựa trên tiện ích vào
gradient như:

wl,i,j ← wl,i,j - α ∂L/∂wl,i,j + α(1 - Ul,i,j)ε.   (6)

Tuy nhiên, quy tắc cập nhật như vậy chỉ có thể giúp
chống lại tính dẻo dai suy giảm, không phải quên
thảm khốc. Điều này là do các trọng số hữu ích không
được bảo vệ khỏi thay đổi bởi các gradient.

Việc tỷ lệ tiện ích là quan trọng đối với các phương
trình cập nhật UPGD và UPS. Chúng tôi tạo ra hai
loại tỷ lệ: toàn cục và cục bộ. Tiện ích được tỷ lệ
toàn cục yêu cầu tiện ích tối đa của tất cả các trọng
số hoặc đặc trưng tại mỗi bước thời gian. Tiện ích
được tỷ lệ cho UPGD và UPS toàn cục được đưa ra
bởi Ul,i,j = φ(Ul,i,j/λ) cho trọng số, trong đó λ là
tiện ích tối đa của các trọng số và φ là hàm tỷ lệ
(ví dụ, Sigmoid). Đối với UPGD và UPS theo đặc trưng,
tiện ích được tỷ lệ được đưa ra bởi ul,j = φ(ul,j/λ).
Chúng tôi cho thấy mã giả của phương pháp của chúng
tôi bằng cách sử dụng tiện ích được tỷ lệ toàn cục
trong Thuật toán 1 cho UPGD theo trọng số và trong
Thuật toán 2 cho UPGD theo đặc trưng. UPGD và UPS
cục bộ không yêu cầu thao tác lấy tối đa toàn cục
vì nó chuẩn hóa vector trọng số đi ra cho mỗi đặc
trưng. Cụ thể, tiện ích được tỷ lệ được đưa ra bởi
Ul,i,j = Ul,i,j/√(∑j U²l,i,j) cho UPGD và UPS theo
trọng số. Đối với các đặc trưng, tiện ích được tỷ lệ
được đưa ra bởi ul,j = ul,j/√(∑j u²l,j). Chúng tôi
cho thấy mã giả của phương pháp của chúng tôi bằng
cách sử dụng tiện ích được tỷ lệ cục bộ trong Thuật
toán 3 cho UPGD theo trọng số và trong Thuật toán
4 cho UPGD theo đặc trưng.

3.5. Các Phương pháp Gradient Descent Nhiễu
Người ta có thể nhiễu tất cả các trọng số một cách
đều nhau, điều này có thể dẫn đến một lớp thuật toán
nổi tiếng gọi là Gradient Descent Nhiễu (PGD). Quy
tắc học của PGD được đưa ra bởi:

wl,i,j ← wl,i,j - α ∂L/∂wl,i,j + αε;              (7)

trong đó ε là nhiễu được nhiễu có thể không tương
quan, đưa ra PGD (Zhou et al. 2019), hoặc phản tương
quan, đưa ra Anti PGD (Orvieto et al. 2022). Nhiễu
phản tương quan tại thời điểm t+1 được đưa ra bởi
εt+1 = εt+1 - εt, trong đó εt+1 và εt là các nhiễu
được lấy mẫu từ N(0, 1), trong khi nhiễu không tương
quan tại thời điểm t+1 được đưa ra bởi εt+1 = εt+1.
Trong bài báo này, chúng tôi quan tâm đến các phương
pháp trực tuyến, vì vậy chúng tôi chỉ xem xét các
phiên bản ngẫu nhiên của các phương pháp này. Khi
tất cả các tiện ích được tỷ lệ là không trong UPGD
(xem Phương trình 5 và Phương trình 6), UPGD giảm
xuống thành các phương pháp PGD.

Nhiều công trình đã chỉ ra vai trò của nhiễu trong
việc cải thiện khái quát hóa và tránh cực tiểu xấu
(ví dụ, PGD, Anti-PGD) trong các vấn đề dừng. Một
phiên bản ngẫu nhiên của gradient descent nhiễu
(Neelakantan et al. 2015) giúp thoát khỏi cực tiểu
xấu và cải thiện hiệu suất và khái quát hóa. Chưa
được chỉ ra trước đây rằng các phương pháp PGD
này có thể giúp cải thiện tính dẻo dai; tuy nhiên,
công trình gần đây của Dohare et al. (2021) cho thấy
rằng việc tiêm nhiễu bằng cách đặt lại các đặc trưng
giúp duy trì tính dẻo dai. Chúng tôi giả định rằng
các phương pháp gradient descent nhiễu ngẫu nhiên
cũng có thể giúp.

Mặt khác, đã được chỉ ra rằng một PGD ngẫu nhiên
với thuật toán phân rã trọng số, được biết đến là
Shrink and Perturb (Ash & Adams 2020), có thể giúp
duy trì tính dẻo dai trong các vấn đề phân loại liên
tục (Dohare et al. 2022). Quy tắc học của shrink
and perturb có thể được viết như

wl,i,j ← wl,i,j - α ∂L/∂wl,i,j + αε - λwl,i,j;    (8)

trong đó λ = 1 - λ và λ là hệ số phân rã trọng số.
Khi không có nhiễu nào được thêm vào, việc cập nhật
giảm xuống thành SGD với phân rã trọng số (Loshchilov
& Hutter 2019), được biết đến là SGDW.

3.6. Thuật toán Gradient Descent Nhiễu dựa trên Tiện ích với Phân rã Trọng số
Kết hợp phân rã trọng số với các quy tắc học UPGD
của chúng tôi, người ta có thể viết UPGD với phân
rã trọng số như sau:

wl,i,j ← wl,i,j - α ∂L/∂wl,i,j + αε(1 - Ul,i,j) - λwl,i,j;  (9)

Tương tự, quy tắc học UPGD không bảo vệ với phân
rã trọng số có thể được viết như

wl,i,j ← wl,i,j - α ∂L/∂wl,i,j + α(1 - Ul,i,j)ε - λwl,i,j;  (10)

4. Thí nghiệm
Trong phần này, chúng tôi thiết kế và thực hiện một
loạt thí nghiệm để ước tính chất lượng của các tiện
ích trọng số hoặc đặc trưng xấp xỉ của chúng tôi.
Ngoài ra, chúng tôi trình bày tính hiệu quả của việc
tìm kiếm trong không gian trọng số hoặc đặc trưng.
Sau đó chúng tôi đánh giá UPGD theo trọng số và
theo đặc trưng trong việc giảm thiểu các vấn đề tính
dẻo dai suy giảm và quên thảm khốc bằng cách sử
dụng các vấn đề phi dừng dựa trên các bộ dữ liệu
MNIST (LeCun et al. 1998), EMNIST (Cohen et al.
2017) và CIFAR-10 (Krizhevsky 2009).

Mặc dù UPS và UPGD có thể được sử dụng về nguyên
tắc trong thiết lập học ngoại tuyến, trọng tâm của
bài báo chúng tôi là học liên tục trực tuyến. Hiệu
suất của những người học liên tục được đánh giá
bằng cách sử dụng chỉ số trong Phương trình 1, nơi
chúng tôi sử dụng hàm mất mát trực tuyến trung bình
cho các tác vụ hồi quy và độ chính xác trực tuyến
trung bình cho các tác vụ phân loại.

Trong mỗi thí nghiệm sau đây, một tìm kiếm siêu
tham số được tiến hành. Tiêu chí của chúng tôi là
tìm tập hợp siêu tham số tốt nhất cho mỗi phương
pháp trong không gian tìm kiếm đó để tối thiểu hóa
diện tích dưới đường cong hàm mất mát (trong các
tác vụ hồi quy) hoặc tối đa hóa diện tích dưới đường
cong độ chính xác (trong các tác vụ phân loại). Trừ
khi có quy định khác, chúng tôi tính trung bình hiệu
suất của mỗi phương pháp trên 20 lần chạy độc lập.

4.1. Chất lượng của Thứ tự Tiện ích Xấp xỉ
Một xấp xỉ chất lượng cao nên đưa ra một thứ tự
tương tự của các trọng số hoặc đặc trưng. Chúng tôi
sử dụng thước đo tương quan thứ tự Spearman để
định lượng chất lượng của các xấp xỉ tiện ích của
chúng tôi. Chúng tôi bắt đầu bằng cách cho thấy
tương quan giữa các tiện ích trọng số xấp xỉ khác
nhau và tiện ích thực. Một mạng của một lớp ẩn
duy nhất chứa 50 đơn vị được sử dụng. Mạng có năm
đầu vào và một đầu ra duy nhất. Mục tiêu của một
vector đầu vào là tổng của hai đầu vào trong số năm
đầu vào. Các đầu vào được lấy mẫu từ U[-0.5, 0.5].
Các trọng số được khởi tạo bằng khởi tạo Kaiming
(He et al. 2015). SGD được sử dụng để tối ưu hóa
các tham số của mạng để tối thiểu hóa sai số bình
phương trung bình với tổng cộng 2000 mẫu. Tại mỗi
bước thời gian, chỉ số tương quan Spearman được
tính toán cho tiện ích trọng số xấp xỉ toàn cục bậc
nhất và bậc hai so với tiện ích ngẫu nhiên và tiện
ích độ lớn trọng số. Tương quan Spearman được đo
mỗi mẫu dựa trên 5×50+50+50+1 = 351 mục đến
từ trọng số và độ lệch. Chúng tôi báo cáo các tương
quan toàn cục giữa tiện ích thực và các tiện ích
trọng số xấp xỉ trong Hình 1(a) và hoãn các tương
quan cục bộ đến Phụ lục F.3. Chúng tôi sử dụng
kích hoạt ReLU (Nair & Hinton 2010) ở đây và báo
cáo kết quả với Tanh và LeakyReLU (Maas et al.
2013) trong Phụ lục F.3.

Tương quan cao nhất đối với tiện ích xấp xỉ bậc
hai trong quá trình học cho đến khi hội tụ. Mặt khác,
tiện ích xấp xỉ bậc nhất trở nên ít tương quan hơn
khi người học hội tụ vì các gradient trở nên rất nhỏ.
Tiện ích độ lớn trọng số cho thấy một tương quan
nhỏ với tiện ích thực. Chúng tôi sử dụng tiện ích
ngẫu nhiên như một tham chiếu duy trì tương quan
bằng không với tiện ích thực, như mong đợi.

Chúng tôi lặp lại thí nghiệm để tính toán các tiện
ích đặc trưng toàn cục xấp xỉ so với tiện ích thực
và báo cáo kết quả trong Hình 1(b). Mạng được sử
dụng có hai lớp ẩn, mỗi lớp chứa 50 đơn vị. Tương
quan Spearman được đo tại mỗi mẫu dựa trên 50×2
= 100 mục đến từ các đặc trưng. Chúng tôi sử dụng
kích hoạt ReLU ở đây, và chúng tôi báo cáo kết quả
với Tanh và LeakyReLU trong Phụ lục F.3 cùng với
các tương quan cục bộ. Kết quả tương tự như kết
quả theo trọng số. Tuy nhiên, cả xấp xỉ bậc nhất
và bậc hai đều có tương quan thấp. Kết quả này cho
thấy rằng xấp xỉ Taylor bậc hai có thể không đủ để
xấp xỉ tiện ích đặc trưng thực. Vì lý do đó, chúng
tôi có thể mong đợi rằng các phương pháp UPGD theo
trọng số sẽ hoạt động tốt hơn so với những phương
pháp theo đặc trưng.

4.2. Hiệu suất Tìm kiếm dựa trên Tiện ích trên MNIST
Trong thí nghiệm này, chúng tôi đánh giá phương
pháp tìm kiếm dựa trên tiện ích của chúng tôi trong
việc tối thiểu hóa hàm mất mát bằng cách sử dụng
tìm kiếm có thông tin tiện ích. Chúng tôi sử dụng
MNIST như tác vụ dừng của mình. Người học được
huấn luyện trực tuyến trong 1 triệu bước thời gian
nơi chỉ một cặp đầu vào-đầu ra được trình bày tại
mỗi bước thời gian. Một mạng của hai lớp ẩn, lớp
đầu tiên có 300 đơn vị, và lớp thứ hai có 150 đơn
vị được sử dụng. Các trọng số được khởi tạo bằng
khởi tạo Kaiming. Chúng tôi báo cáo kết quả trong
Hình 2 với kích hoạt ReLU. Loại nhiễu được sử dụng
kiểm soát hiệu suất của các bộ tối ưu tìm kiếm. Chúng
tôi nhận thấy rằng nhiễu phản tương quan giúp trong
tiện ích xấp xỉ bậc nhất và bậc hai nhiều hơn so
với nhiễu không tương quan (được hiển thị trong
Phụ lục F.1). Kết quả cho thấy tính hiệu quả của
tìm kiếm dựa trên tiện ích trong việc giảm hàm mất
mát so với đường cơ sở nhiễu tất cả các trọng số
hoặc đặc trưng đều nhau (ví dụ, sử dụng tiện ích
ngẫu nhiên). Chúng tôi lưu ý rằng tìm kiếm có thông
tin tiện ích một mình, mặc dù có thể giảm hàm mất
mát, không hiệu quả bằng việc sử dụng thông tin
gradient. Khi UPGD được sử dụng với MNIST, chúng
tôi nhận thấy một cải thiện trong hiệu suất so với
SGD (xem Phụ lục F.2).

4.3. Hiệu suất UPGD trên Vấn đề Đồ chơi Phi dừng
Chúng tôi trình bày một vấn đề hồi quy đơn giản
mà mục tiêu tại thời điểm t được đưa ra bởi yt =
a∑i∈S xt,i, trong đó xt,i là mục thứ i của vector
đầu vào tại thời điểm t, S là tập hợp đầu vào, và
a ∈ R. Chúng tôi giới thiệu tính phi dừng bằng hai
cách: thay đổi hệ số nhân a hoặc thay đổi tập hợp
đầu vào S. Trong vấn đề này, tác vụ là cộng hai đầu
vào trong số 16 đầu vào. Người học được yêu cầu
khớp các mục tiêu bằng cách tối thiểu hóa sai số
bình phương trung bình. Người học sử dụng một mạng
tuyến tính đa lớp có hai lớp ẩn chứa 300 và 150
đơn vị, tương ứng. Mạng là tuyến tính vì kích hoạt
được sử dụng là kích hoạt định danh (σ(x) = x).
Ở đây, chúng tôi cho thấy kết quả và đưa ra chi tiết
thí nghiệm trong Phụ lục D.1.

Trong biến thể đầu tiên của vấn đề, tập hợp đầu
vào S có kích thước là hai, nhưng các phần tử thay
đổi mỗi 200 bước thời gian bằng cách dịch chuyển
hai trong các chỉ số đầu vào. Ví dụ, nếu tác vụ đầu
tiên có S = {1, 2}, tác vụ tiếp theo sẽ là {3, 4}
và như vậy. Vì các tác vụ có ít chuyển giao giữa
chúng, chúng tôi mong đợi những người học liên tục
học nhanh nhất có thể và duy trì tính dẻo dai của
họ. Chúng tôi so sánh UPGD với SGD, PGD, Shrink
& Perturb, và Non-protecting UPGD. Hơn nữa, chúng
tôi sử dụng một đường cơ sở có một lớp tuyến tính
ánh xạ đầu vào thành đầu ra. Lưu ý rằng chúng tôi
sử dụng tiện ích xấp xỉ bậc nhất ở đây và hoãn kết
quả với tiện ích bậc hai trong Phụ lục F.4. Chúng
tôi báo cáo kết quả của thí nghiệm này trong Hình
3(a) cho UPGD theo trọng số và trong Hình 3(b) cho
UPGD theo đặc trưng. Hiệu suất của SGD giảm sút
với các mục tiêu thay đổi, cho thấy SGD mất tính
dẻo dai mỗi khi mục tiêu thay đổi. Điều này có thể
gợi ý rằng các trọng số đi ra đến một số đặc trưng
trở nên nhỏ hơn, cản trở khả năng thay đổi các trọng
số đầu vào của đặc trưng. Mặt khác, Shrink & Perturb
có thể duy trì một số tính dẻo dai so với đường cơ
sở lớp tuyến tính. PGD và Non-protecting UPGD hoạt
động tốt hơn Shrink & Perturb, cho thấy rằng phân
rã trọng số không hữu ích trong vấn đề này, và tốt
hơn là chỉ tiêm nhiễu mà không thu nhỏ các tham
số. UPGD có thể duy trì tính dẻo dai của nó. Hơn
nữa, hiệu suất tiếp tục cải thiện với các mục tiêu
thay đổi so với các phương pháp khác.

Trong biến thể thứ hai, dấu của tổng mục tiêu được
lật mỗi 200 bước thời gian bằng cách thay đổi a từ
1 thành -1 và ngược lại. Chúng tôi mong đợi các
tác nhân học liên tục học một số đặc trưng trong
200 bước đầu tiên. Sau khi thay đổi dấu mục tiêu,
chúng tôi mong đợi người học thay đổi dấu của chỉ
các trọng số đầu ra vì các đặc trưng đã học nên giống
nhau. Tần suất thay đổi a cao để trừng phạt những
người học vì việc học lại các đặc trưng từ đầu. Lưu
ý rằng chúng tôi sử dụng tiện ích xấp xỉ bậc nhất
ở đây và hoãn kết quả với tiện ích xấp xỉ bậc hai
trong Phụ lục F.4. Chúng tôi báo cáo kết quả của
thí nghiệm này trong Hình 4(a) cho UPGD theo trọng
số và trong Hình 4(b) cho UPGD theo đặc trưng.
Hiệu suất của SGD giảm sút với các mục tiêu thay
đổi, cho thấy rằng nó không sử dụng các đặc trưng
đã học và học lại chúng mỗi khi mục tiêu thay đổi.
Shrink & Perturb, Non-protecting UPGD, và PGD duy
trì tính dẻo dai, nhưng chúng không thể bảo vệ các
trọng số hữu ích; do đó, hiệu suất của chúng tệ hơn
UPGD. UPGD có thể bảo vệ các trọng số hoặc đặc
trưng hữu ích và sử dụng chúng mỗi khi mục tiêu
thay đổi. Hơn nữa, hiệu suất tiếp tục cải thiện với
các mục tiêu thay đổi so với các phương pháp khác.

4.4. Hiệu suất UPGD trên MNIST Hoán vị Đầu vào
Chúng ta có thể nghiên cứu tính dẻo dai khi người
học được trình bày với các tác vụ tuần tự yêu cầu
ít chuyển giao giữa chúng. MNIST hoán vị đầu vào
thỏa mãn tiêu chí này vì các biểu diễn được học
trong một tác vụ không liên quan đến các tác vụ
khác. Chúng tôi hoán vị các đầu vào mỗi 5000 bước
thời gian và trình bày những người học với 1 triệu
ví dụ, một ví dụ mỗi bước thời gian. Người học được
yêu cầu tối đa hóa độ chính xác trực tuyến bằng
cách khớp mục tiêu. Người học sử dụng một mạng
tuyến tính đa lớp với kích hoạt ReLU có hai lớp ẩn
chứa 300 và 150 đơn vị, tương ứng.

Dohare et al. (2022) đã chỉ ra rằng phân rã trọng
số giúp trong MNIST hoán vị đầu vào và gợi ý rằng
phân rã trọng số thúc đẩy các trọng số dẻo dai vì
duy trì các trọng số nhỏ ngăn cản các trọng số khỏi
việc cam kết quá mức, làm cho chúng dễ thay đổi.
Do đó, chúng tôi so sánh SGD với phân rã trọng số
(SGDW), Shrink & Perturb, Adam với phân rã trọng
số (Loshchilov & Hutter 2019) được biết đến là (AdamW),
UPGD với phân rã trọng số (UPGD-W) và Non-protecting
UPGD-W. Chúng tôi lưu ý ở đây rằng chúng tôi không
so sánh với phương pháp Continual Backprop (2021),
duy trì tính dẻo dai vì nó yêu cầu một cơ chế học
phức tạp, không phải một quy tắc cập nhật đơn giản.
Ngoài ra, Continual Backprop có hiệu suất tương tự
như Shrink & Perturb (Dohare et al. 2022). Hình
5 cho thấy độ chính xác trực tuyến trung bình với
số tác vụ. Tất cả các phương pháp được điều chỉnh
mạnh mẽ (được hiển thị trong Phụ lục D.3). Khi
nhiều tác vụ được trình bày, độ chính xác trực tuyến
của PGD, SGDW, và AdamW giảm sút theo thời gian.
Chỉ có UPGD-W, Non-protecting UPGD-W, và Shrink
& Perturb có thể duy trì tính dẻo dai. Chúng tôi cũng
lưu ý rằng UPGD-W có hiệu suất tốt hơn một chút
so với Non-protecting UPGD-W và Shrink & Perturb.
Trong Phụ lục G, chúng tôi trình bày một nghiên
cứu phân tích về ảnh hưởng của phân rã trọng số
và nhiễu đối với hiệu suất trong Shrink & Perturb,
UPGD-W, và Non-protecting UPGD-W. Chúng tôi lặp
lại thí nghiệm này cho UPGD theo đặc trưng trong
Phụ lục F.5.

4.5. Hiệu suất UPGD trên EMNIST Hoán vị Đầu ra
Ở đây, chúng tôi nghiên cứu sự tương tác của quên
thảm khốc và tính dẻo dai suy giảm với EMNIST hoán
vị đầu ra. Điều này thêm một mức độ phức tạp lên
trên tính dẻo dai suy giảm được hiển thị trong phần
trước. Bộ dữ liệu EMNIST là một dạng mở rộng của
MNIST có 47 lớp và có cả chữ số và chữ cái. Chúng
tôi chọn EMNIST thay vì MNIST vì MNIST hoán vị
đầu ra là một vấn đề rất dễ sao cho tất cả các phương
pháp đạt được độ chính xác rất cao. Trong EMNIST
hoán vị đầu ra của chúng tôi, các nhãn được hoán
vị mỗi 2500 bước thời gian. Thay đổi như vậy không
nên làm cho tác nhân thay đổi các biểu diễn đã học
của nó vì nó có thể chỉ cần thay đổi các trọng số
của lớp cuối cùng để thích ứng với thay đổi đó. Điều
này làm cho tác vụ MNIST hoán vị đầu ra phù hợp
để nghiên cứu quên thảm khốc và tính dẻo dai suy
giảm.

Chúng tôi so sánh SGDW, Shrink & Perturb, AdamW,
UPGD-W, và Non-protecting UPGD-W. Tất cả các phương
pháp được điều chỉnh mạnh mẽ (được hiển thị trong
Phụ lục D.3). Hình 6 cho thấy độ chính xác trực
tuyến trung bình với số tác vụ. Khi nhiều tác vụ được
trình bày, độ chính xác trực tuyến của PGD, SGDW,
và AdamW giảm sút theo thời gian. Non-protecting
UPGD-W và Shrink & Perturb có thể duy trì tính dẻo
dai của chúng, nhưng chúng từ từ mất nó khi độ chính
xác trực tuyến của chúng giảm dần. Ngược lại, UPGD-W
có hiệu suất tốt hơn đáng kể so với các thuật toán
khác. Điều này gợi ý rằng UPGD-W duy trì tính dẻo
dai và giảm việc quên sao cho tại mỗi tác vụ, nó
có thể cải thiện các biểu diễn của mình. Chúng tôi
thấy rằng UPGD-W hoạt động tốt nhất khi nó không
có phân rã trọng số trong tác vụ này. Trong Phụ lục
G, chúng tôi trình bày một nghiên cứu phân tích về
ảnh hưởng của phân rã trọng số và nhiễu đối với
hiệu suất. Chúng tôi lặp lại thí nghiệm này cho UPGD
theo đặc trưng trong Phụ lục F.6.

4.6. Hiệu suất UPGD trên CIFAR10 Hoán vị Đầu ra
Ở đây, chúng tôi nghiên cứu sự tương tác của quên
thảm khốc và tính dẻo dai suy giảm với CIFAR-10
hoán vị đầu ra. Các nhãn được hoán vị mỗi 2500 bước
thời gian. Thay đổi như vậy không nên làm cho tác
nhân thay đổi các biểu diễn đã học của nó vì nó có
thể chỉ cần thay đổi các trọng số của lớp cuối cùng
để thích ứng với thay đổi đó. Trong vấn đề này, chúng
tôi làm cho những người học sử dụng một mạng với
hai lớp tích chập với max-pooling tiếp theo bởi hai
lớp được kết nối đầy đủ với kích hoạt ReLU. Ở đây,
chúng tôi cho thấy kết quả và đưa ra chi tiết thí
nghiệm trong Phụ lục D.4.

Chúng tôi so sánh SGDW, Shrink & Perturb, AdamW,
UPGD-W, và Non-protecting UPGD-W. Tất cả các phương
pháp được điều chỉnh mạnh mẽ (được hiển thị trong
Phụ lục D.4). Hình 7 cho thấy độ chính xác trực
tuyến trung bình với số tác vụ trên 10 lần chạy độc
lập. Khi nhiều tác vụ được trình bày, độ chính xác
trực tuyến của tất cả các phương pháp cải thiện.
Chúng tôi gợi ý rằng vấn đề dễ hơn EMNIST hoán
vị đầu ra vì số lượng lớp trong CIFAR-10 ít hơn
EMNIST. Số lượng lớp lớn hơn có nghĩa là xác suất
một lớp không thay đổi sau một hoán vị là ít hơn
đáng kể.

Tất cả các phương pháp duy trì tính dẻo dai của
chúng. Chúng tôi nhận thấy rằng PGD hoạt động tệ
hơn các phương pháp khác nhấn mạnh vai trò của
phân rã trọng số trong việc cải thiện hiệu suất trong
tác vụ này. Ngược lại với hai tác vụ trước, chúng
tôi thấy rằng AdamW hoạt động tốt hơn SGDW. Tương
tự như các tác vụ trước, UPGD-W có hiệu suất tốt
hơn đáng kể so với các thuật toán khác. Điều này
gợi ý rằng UPGD-W duy trì tính dẻo dai và giảm việc
quên sao cho tại mỗi tác vụ nó có thể cải thiện các
biểu diễn của mình. Chúng tôi thấy rằng UPGD-W
hoạt động tốt nhất khi nó không có phân rã trọng
số trong tác vụ này. Trong Phụ lục G, chúng tôi trình
bày một nghiên cứu phân tích về ảnh hưởng của phân
rã trọng số và nhiễu trọng số đối với hiệu suất trong
Shrink & Perturb, UPGD-W, và Non-protecting UPGD-W.

5. Các Công trình Liên quan
Việc cắt tỉa mạng neural yêu cầu một chỉ số độ quan
trọng hoặc tầm quan trọng để chọn trọng số nào cần
cắt tỉa. Thông thường, sau khi huấn luyện hoàn thành,
mạng được cắt tỉa bằng cách sử dụng các thước đo
như độ lớn trọng số (ví dụ, Han et al. 2015, Park
et al. 2020). Các chỉ số khác đã được đề xuất bằng
cách sử dụng thông tin bậc nhất (ví dụ, Mozer & Smolensky
1988, Hassibi & Stork 1992, Molchanov et al. 2016),
thông tin bậc hai (ví dụ, LeCun et al. 1989, Dong
et al. 2017), hoặc cả hai (ví dụ, Tresp et al. 1996,
Molchanov et al. 2019). Molchanov et al. (2019) cho
thấy rằng xấp xỉ Taylor bậc hai của một số tiện ích
thực, yêu cầu tính toán siêu tuyến tính, khớp chặt
chẽ với tiện ích thực. Kết quả này phù hợp với các
dẫn xuất và kết luận của chúng tôi. Tuy nhiên, chúng
tôi sử dụng xấp xỉ Taylor bậc hai hiệu quả bằng cách
sử dụng HesScale (Elsayed & Mahmood 2022), làm
cho phương pháp của chúng tôi không tốn kém về
mặt tính toán. Cuối cùng, phương pháp lan truyền
tiện ích của chúng tôi (được dẫn xuất trong Phụ lục
A) giống, về nguyên tắc, với cách tiếp cận được theo
đuổi bởi Karnin (1990) và gần đây hơn bởi Yu et
al. (2018), đề xuất một phương pháp để lan truyền
điểm số quan trọng từ lớp cuối cùng. Tuy nhiên, phương
pháp của chúng tôi sử dụng tiện ích dựa trên xấp
xỉ bậc hai thay vì lỗi tái tạo hoặc xấp xỉ bậc nhất.

Chiến lược tiến hóa (Rudolph 1997, Rechenberg 1973)
là một lớp phương pháp tối ưu hóa không cần đạo
hàm sử dụng các ý tưởng lấy cảm hứng từ tiến hóa
tự nhiên. Thông thường, một quần thể các hàm được
tham số hóa được tạo ra với các khởi tạo khác nhau,
có thể được cải thiện bằng cách sử dụng một số điểm
thành công. Các phương pháp gần đây (ví dụ, Salimans
et al. 2017, Such et al. 2017) sử dụng các chiến lược
tiến hóa để cải thiện một tập hợp các chính sách được
tham số hóa dựa trên lợi nhuận đến từ mỗi chính
sách. Quần thể được tạo ra bằng cách nhiễu chính
sách hiện tại, sau đó mỗi chính sách được chạy cho
một tập để có lợi nhuận. Cập nhật tham số chính
sách mới là trung bình của các tham số chính sách
nhiễu này được trọng số hóa bởi lợi nhuận tương ứng
của chúng. Các chiến lược tiến hóa, so với UPS, tìm
kiếm trong không gian nghiệm bằng cách sử dụng
một quần thể nghiệm, làm cho chúng không khả thi
ngoài mô phỏng. Tuy nhiên, UPS sử dụng tìm kiếm
trong không gian trọng số hoặc đặc trưng, làm cho
UPS phù hợp cho học trực tuyến.

UPS theo đặc trưng có thể được xem như một sự tổng
quát hóa của phương pháp tạo ra và kiểm tra (Mahmood
& Sutton 2013) cho các mạng đa lớp với các hàm
mục tiêu tùy ý. Phương pháp tạo ra và kiểm tra chỉ
hoạt động với các mạng có lớp ẩn đơn trong các vấn
đề hồi quy đầu ra đơn và cập nhật các đặc trưng theo
cách lựa chọn có điều kiện (ví dụ, thay thế một đặc
trưng nếu tiện ích của nó thấp hơn các tiện ích khác).
Tiện ích đặc trưng là dấu vết của độ lớn trọng số
đi ra của nó. Tuy nhiên, đã được chỉ ra rằng độ lớn
trọng số không phù hợp cho các vấn đề khác, chẳng
hạn như phân loại (Elsayed 2022). Ngược lại, UPS
theo đặc trưng sử dụng một khái niệm tiện ích tốt
hơn cho phép tìm kiếm tốt hơn trong không gian
đặc trưng và hoạt động với các cấu trúc mạng hoặc
hàm mục tiêu tùy ý.

Hầu hết các phương pháp học liên tục sử dụng các
tác vụ tuần tự với ranh giới đã biết. Khái niệm như
vậy là không thực tế để gặp trong thực tế. Có một
số phương pháp không có tác vụ (ví dụ, Aljundi et
al. 2019, Lee et al. 2020, He et al. 2019). Tuy nhiên,
các phương pháp không có tác vụ hiện tại hoặc là
tiến bộ để chứa các kinh nghiệm mới, yêu cầu bộ
đệm phát lại, hoặc chứa các thành phần suy luận
tác vụ rõ ràng. So với đó, UPGD có thể mở rộng về
bộ nhớ và tính toán, làm cho nó phù hợp cho các
tác nhân suốt đời chạy trong thời gian dài.

6. Kết luận
UPGD là một cách tiếp cận mới cho phép các tác nhân
học hoạt động trong thời gian dài, làm cho nó phù
hợp cho học liên tục. Chúng tôi đã tạo ra các quy
tắc học có thông tin tiện ích bảo vệ các trọng số
hoặc đặc trưng hữu ích và nhiễu những cái ít hữu
ích hơn. Các quy tắc cập nhật như vậy giúp giảm
thiểu các vấn đề gặp phải bởi các phương pháp học
biểu diễn hiện đại trong học liên tục, cụ thể là quên
thảm khốc và tính dẻo dai suy giảm. Chúng tôi đã
thực hiện một loạt thí nghiệm cho thấy UPGD giúp
duy trì tính dẻo dai mạng và tái sử dụng các đặc
trưng đã học trước đó. Kết quả của chúng tôi cho
thấy UPGD phù hợp cho học liên tục nơi tác nhân
yêu cầu thích ứng nhanh với thế giới luôn thay đổi.

7. Tác động Rộng lớn
Dữ liệu tương quan theo thời gian gây ra quên thảm
khốc, điều này làm cho học tăng cường với xấp xỉ
hàm trở nên thách thức và không hiệu quả về mẫu
(Fedus et al. 2020). Các phương pháp gradient chính
sách hoạt động tốt nhất (ví dụ, Mnih et al. 2015,
Haarnoja et al. 2018) sử dụng một bộ đệm phát lại
lớn để giảm tương quan trong các chuyển tiếp liên
tiếp và làm cho chúng có vẻ độc lập và phân phối
đồng nhất với người học. Vì UPGD giúp chống lại
quên thảm khốc mà không cần bộ đệm, nó có thể
cải thiện hiệu suất của hầu hết các thuật toán học
tăng cường và cải thiện khả năng mở rộng của chúng.

UPGD có thể được tích hợp với các phương pháp thích
ứng kích thước bước (ví dụ, Schraudolph 1999, Jacobsen
et al. 2019), làm cho chúng hoạt động tốt trong điều
kiện phi dừng. Hơn nữa, UPGD có thể giúp nghiên
cứu và phân tích tốt hơn các phương pháp thích ứng
kích thước bước một cách tách biệt với quên thảm
khốc và tính dẻo dai suy giảm.

Lời cảm ơn
Chúng tôi biết ơn sự tài trợ từ chương trình Ghế
AI CIFAR Canada, phòng thí nghiệm Học Tăng cường
và Trí tuệ Nhân tạo (RLAI), Viện Trí tuệ Máy Alberta
(Amii), và Hội đồng Khoa học Tự nhiên và Kỹ thuật
(NSERC) của Canada. Chúng tôi cũng muốn cảm ơn
Compute Canada đã cung cấp các tài nguyên tính
toán cần thiết.

Tài liệu tham khảo
[Các tài liệu tham khảo được giữ nguyên như trong
bản gốc do chúng là danh sách các công trình học
thuật chuẩn]

--- TIẾP TỤC VỚI CÁC TRANG CÒN LẠI ---

[Tôi sẽ tiếp tục dịch phần còn lại của tài liệu nếu bạn cần. Đây là bản dịch của trang đầu tiên của tài liệu academic này từ tiếng Anh sang tiếng Việt.]
