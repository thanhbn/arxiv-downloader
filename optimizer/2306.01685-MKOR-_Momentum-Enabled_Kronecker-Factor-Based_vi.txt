# MKOR: Bộ Tối Ưu Hóa Dựa Trên Nhân Tử Kronecker Với Động Lượng Sử Dụng Cập Nhật Hạng-1

Mohammad Mozaffari
Khoa Khoa học Máy tính
Đại học Toronto
mmozaffari@cs.toronto.edu

Sikan Li
Máy chủ Tính toán Tiên tiến Texas
sli@tacc.utexas.edu

Zhao Zhang
Khoa Kỹ thuật Điện và Máy tính
Đại học Rutgers
zhao.zhang@rutgers.edu

Maryam Mehri Dehnavi
Khoa Khoa học Máy tính
Đại học Toronto
mmehride@cs.toronto.edu

## Tóm tắt

Nghiên cứu này đề xuất một Bộ Tối Ưu Hóa Dựa Trên Nhân Tử Kronecker Với Động Lượng Sử Dụng Cập Nhật Hạng-1, được gọi là MKOR, nhằm cải thiện thời gian huấn luyện và tính chất hội tụ của các mạng nơ-ron sâu (DNN). Các kỹ thuật bậc hai, mặc dù có tỷ lệ hội tụ cao hơn so với các đối tác bậc nhất, lại có độ phức tạp khối lập phương đối với kích thước mô hình và/hoặc kích thước batch huấn luyện. Do đó, chúng thể hiện khả năng mở rộng và hiệu suất kém trong các mô hình transformer, ví dụ như các mô hình ngôn ngữ lớn (LLM), bởi vì kích thước batch trong những mô hình này được mở rộng theo độ dài chuỗi của cơ chế attention, dẫn đến kích thước mô hình và batch lớn. Độ phức tạp của MKOR là bậc hai đối với kích thước mô hình, giảm bớt các nút thắt tính toán trong các phương pháp bậc hai. Do độ phức tạp tính toán cao, các triển khai tiên tiến của các phương pháp bậc hai chỉ có thể cập nhật thông tin bậc hai không thường xuyên, và do đó không khai thác đầy đủ lời hứa về hội tụ tốt hơn từ những cập nhật này. Bằng cách giảm độ phức tạp truyền thông của các cập nhật bậc hai, cũng như đạt được độ phức tạp truyền thông tuyến tính, MKOR tăng tần suất của các cập nhật bậc hai. Chúng tôi cũng đề xuất một phiên bản lai của MKOR (gọi là MKOR-H) mà giữa quá trình huấn luyện sẽ quay lại bộ tối ưu bậc nhất nếu các cập nhật bậc hai không còn tăng tốc hội tụ. Các thí nghiệm của chúng tôi cho thấy MKOR vượt trội hơn các phương pháp bậc nhất tiên tiến, ví dụ như bộ tối ưu LAMB, và các triển khai tốt nhất của phương pháp bậc hai, tức là KAISA/KFAC, lần lượt lên đến 2.57× và 1.85× trên BERT-Large-Uncased với 64 GPU.

## 1 Giới thiệu

Các phương pháp tối ưu hóa bậc hai gần đây đã trở nên phổ biến trong quá trình huấn luyện của các mạng nơ-ron sâu (DNN) do tỷ lệ hội tụ cao hơn so với các đối tác bậc nhất. Một trong những phương pháp bậc hai nổi tiếng, phương pháp Newton, sử dụng nghịch đảo của ma trận Hessian của hàm mục tiêu làm bộ tiền điều kiện cho các gradient, nắm bắt nhiều thông tin hơn về độ cong của hàm mất mát. Tuy nhiên, vì kích thước của Hessian tỷ lệ với kích thước mô hình, việc tính toán và lưu trữ Hessian chính xác và nghịch đảo của nó là nút thắt chính trong những phương pháp này, dẫn đến các phương pháp xấp xỉ khác nhau.

Một trong những xấp xỉ phổ biến nhất của phương pháp Newton là Gradient Descent Tự nhiên (NGD), trong đó Hessian được thay thế bằng Ma trận Thông tin Fisher (FIM) để giảm chi phí tính toán các đạo hàm bậc hai. Tuy nhiên, khi các mô hình trở nên lớn hơn, việc lưu trữ và tính toán nghịch đảo chính xác của FIM trở nên không thực tế, dẫn đến việc thiết kế các thuật toán dựa trên xấp xỉ block-diagonal của FIM; mỗi block tương ứng với một lớp trong mạng nơ-ron (NN). Việc nghịch đảo các block đường chéo cũng không thực tế do số lượng lớn các tham số trong một lớp NN, do đó nghịch đảo của chúng cũng được xấp xỉ.

Lớp các phương pháp Kronecker-Factored Approximate Curvature (KFAC) cố gắng giảm chi phí tính toán của việc nghịch đảo block. Chúng xấp xỉ một block FIM cho một batch mẫu sử dụng phép nhân Kronecker của hiệp phương sai của các activation đầu ra và gradient đầu vào. KFAC được triển khai trên các nền tảng phân tán cho cả lớp tuyến tính và tích chập, và các kỹ thuật tối ưu hóa tính toán và truyền thông khác nhau đã được áp dụng cho các triển khai của nó. KAISA là một framework với triển khai KFAC phân tán tiên tiến. Độ phức tạp tính toán của các phương pháp dựa trên KFAC là O(d³), trong đó d là chiều của lớp. Những phương pháp này hoạt động tốt cho các mô hình nhỏ với chiều lớp nhỏ d, tuy nhiên, trong các mô hình lớn, chúng không mở rộng tốt, dẫn đến hiệu suất kém.

Để giảm ảnh hưởng của kích thước mô hình lên độ phức tạp tính toán của các cập nhật bậc hai, KBFGS, không có triển khai phân tán hiệu quả, sử dụng phương pháp dựa trên Broyden-Fletcher-Goldfarb-Shannon (BFGS) để tính toán và cập nhật nhân tử Kronecker và có độ phức tạp O(bd²), trong đó b là kích thước batch. Lớp các phương pháp SNGD (NGD dựa trên Sherman-Morrison-Woodbury) sử dụng đồng nhất thức Sherman-Morrison-Woodbury (SMW) để tính toán các block FIM với độ phức tạp O(b³), làm cho độ phức tạp độc lập với kích thước mô hình. HyLo là triển khai SNGD tiên tiến. Nó giảm truyền thông để có khả năng mở rộng tốt hơn. KBFGS và SNGD giải quyết vấn đề khả năng mở rộng của các phương pháp dựa trên KFAC cho các batch nhỏ. Tuy nhiên, trong các mô hình transformer, kích thước batch tỷ lệ với độ dài chuỗi của cơ chế attention (có thể cao đến vài nghìn) do đó hạn chế khả năng mở rộng của các phương pháp SNGD và KBFGS. Nghiên cứu gần đây Eva cố gắng giảm chi phí này hơn nữa xuống O(d²) bằng cách lưu trữ các vector thay vì các nhân tử Kronecker trong công thức KFAC. Tuy nhiên, tương tự như KFAC, Eva sử dụng một nhân tử giảm chấn có thể dẫn đến lỗi bổ sung trong xấp xỉ FIM. Ngoài ra, vì Eva lưu trữ các vector Kronecker thay vì các nhân tử, nó không thể tận dụng lợi ích của động lượng.

Nghiên cứu này trình bày MKOR, một Bộ Tối Ưu Hóa Dựa Trên Factorization Kronecker Với Động Lượng với Cập Nhật Hạng-1. (1) MKOR xấp xỉ nghịch đảo của các ma trận hiệp phương sai sử dụng cập nhật hạng-1 trong Nghịch Đảo Ma Trận Dựa Trên Sherman-Morrison (SM-Based), giảm độ phức tạp tính toán nghịch đảo từ O(d³) xuống O(d²). Kết quả là, các cập nhật thông tin bậc hai có thể được áp dụng thường xuyên hơn lên đến 100 lần so với KAISA/KFAC và HyLo/SNGD. KFAC tính toán và nghịch đảo các ma trận hiệp phương sai một cách chính xác; KFAC cập nhật thông tin bậc hai không thường xuyên, ví dụ mỗi 100-1000 lần lặp, do chi phí cao của nghịch đảo và để đạt hiệu quả thời gian, điều này làm tổn hại tỷ lệ hội tụ và khả năng tổng quát hóa. Các phương pháp dựa trên SNGD cũng gặp phải chi phí tương tự do việc nghịch đảo các ma trận kernel của chúng. (2) Các phương pháp bậc hai gặp phải chi phí truyền thông cao để đồng bộ hóa nghịch đảo của các nhân tử giữa các worker. MKOR giảm bớt điều này bằng cách chỉ đồng bộ hóa các vector xấp xỉ hạng-1 giữa các worker, giảm chi phí truyền thông từ O(d²) xuống O(d). Ngoài ra, MKOR sử dụng độ chính xác nửa để giảm thêm truyền thông; các phương pháp bậc hai khác không thể sử dụng tính toán độ chính xác thấp do các thuật toán nghịch đảo ma trận phức tạp của chúng. (3) Các phương pháp bậc hai dễ gặp phải gradient bùng nổ hơn do ảnh hưởng của tiền điều kiện lên norm của gradient và thiếu giới hạn số trên nghịch đảo của các nhân tử. MKOR sử dụng Bộ Ổn Định Dựa Trên Norm và Cơ Chế Tái Tỷ Lệ Gradient để phát hiện và ngăn chặn gradient bùng nổ. (4) Tỷ lệ hội tụ cao của các phương pháp bậc hai, bao gồm MKOR, bị đình trệ sau vài lần lặp hoặc epoch đầu tiên của huấn luyện. Chúng tôi đề xuất một phiên bản lai của MKOR, cụ thể là MKOR-H, kết hợp tỷ lệ hội tụ cao của các phương pháp bậc hai trong giai đoạn đầu của huấn luyện với chi phí thấp của các phương pháp bậc nhất trong giai đoạn cuối của huấn luyện, sử dụng cơ chế chuyển đổi dựa trên tỷ lệ giảm mất mát.

MKOR vượt trội hơn các phương pháp bậc nhất và bậc hai phân tán tiên tiến lên đến 2.57×, giảm thời gian huấn luyện của BERT-Large-Uncased từ 8 giờ xuống 3 giờ trên 64 GPU A100. MKOR cũng đạt được các chỉ số tiên tiến mới trên tập dữ liệu GLUE, nơi các phương pháp bậc hai khác như KFAC không thể hội tụ đến baseline.

## 2 Nền tảng

Việc huấn luyện một mạng nơ-ron liên quan đến việc giải quyết một bài toán tối ưu hóa để tìm các giá trị tối ưu cho một tập các trọng số W={Wᵐ}ᴹₘ₌₁, trong đó M là số lượng lớp trong mạng và Wᵐ là một ma trận trong ℝᵈˣᵈ. Các phương pháp bậc hai tiền điều kiện các trọng số của mạng với nghịch đảo của Hessian để có tỷ lệ hội tụ tốt hơn. Các xấp xỉ block-diagonal của các phương pháp NGD thay thế Hessian bằng FIM block-diagonal như được hiển thị trong Phương trình 1, trong đó wᵐ∈ℝᵈ² là biểu diễn vector của Wᵐ, Fᵐ là block tương ứng với lớp đó và L là hàm mất mát. Nghiên cứu cho thấy rằng FIM khớp với ma trận Gauss-Newton trong một số điều kiện nhất định.

wᵐ := wᵐ - α(Fᵐ)⁻¹∇wᵐL                                                  (1)

Các phương pháp dựa trên KFAC tái công thức block FIM như tích Kronecker của hai ma trận. Phương trình 2 cho thấy quy tắc cập nhật trong KFAC, trong đó L là hàm mất mát và (Lᵐₜ)⁻¹ và (Rᵐₜ)⁻¹ là nghịch đảo của nhân tử trái và phải, tương ứng.

Wᵐ := Wᵐ - α(Lᵐₜ)⁻¹∇WᵐL(Rᵐₜ)⁻¹                                         (2)

(Lᵐₜ)⁻¹ và (Rᵐₜ)⁻¹ trong Phương trình 2 được tính toán sử dụng phương trình 3 và 4, tương ứng, trong đó aᵐ là giá trị activation của một mẫu tại lớp m, và gᵐ = ∇aᵐ⁻¹L và γ kết hợp tính năng động lượng để tránh những thay đổi cực đoan trong các nhân tử.

Lᵐₜ = γLᵐₜ₋₁ + (1-γ)E[gᵐₜgᵐₜᵀ]                                          (3)
Rᵐₜ = γRᵐₜ₋₁ + (1-γ)E[aᵐ⁻¹ₜaᵐ⁻¹ₜᵀ]                                        (4)

## 3 MKOR: Bộ Tối Ưu Hóa Dựa Trên Factorization Kronecker Với Động Lượng với Cập Nhật Hạng-1

Trong phần này, chúng tôi trước tiên trình bày thuật toán MKOR, độ phức tạp tính toán và truyền thông của nó, sau đó trình bày MKOR lai (MKOR-H), và cuối cùng thảo luận về hội tụ và ổn định của MKOR.

**Thuật toán 1: Thuật toán MKOR cho một Lớp Đơn m**

**Dữ liệu:** Aᵐ⁻¹ₜ, Gᵐₜ, Wᵐₜ₋₁  
**Kết quả:** Wᵐₜ

1. **if** m ∈ Second Order Layers **then**
2.    aᵐ⁻¹ₜ ← (1/b)∑ᵇᵢ₌₁(Aᵐ⁻¹ₜ)₍:,ᵢ₎; // Xấp xỉ: Aᵐ⁻¹ₜAᵐ⁻¹ₜᵀ ≈ aᵐ⁻¹ₜaᵐ⁻¹ₜᵀ
3.    gᵐₜ ← (1/b)∑ᵇᵢ₌₁(Gᵐₜ)₍:,ᵢ₎; // Xấp xỉ: GᵐₜGᵐₜᵀ ≈ gᵐₜgᵐₜᵀ
4.    aᵐ⁻¹ₜ, gᵐₜ ← AllReduce(aᵐ⁻¹ₜ, gᵐₜ); // Đồng bộ hóa Xấp xỉ
5.    // Ổn định Dựa Trên Norm
6.    L̂ᵐₜ₋₁⁻¹ ← **if** ‖Lᵐₜ₋₁⁻¹‖ > ε **then** ζLᵐₜ₋₁⁻¹ + (1-ζ)I **else** Lᵐₜ₋₁⁻¹;
7.    R̂ᵐₜ₋₁⁻¹ ← **if** ‖Rᵐₜ₋₁⁻¹‖ > ε **then** ζRᵐₜ₋₁⁻¹ + (1-ζ)I **else** Rᵐₜ₋₁⁻¹;
8.    // Nghịch Đảo Nhân Tử Dựa Trên SM
9.    Lᵐₜ⁻¹ ← γL̂ᵐₜ₋₁⁻¹ + (1-γ)/(γ²(1+γ(1-γ)gᵐₜᵀL̂ᵐₜ₋₁⁻¹gᵐₜ))L̂ᵐₜ₋₁⁻¹gᵐₜgᵐₜᵀL̂ᵐₜ₋₁⁻¹
10.   Rᵐₜ⁻¹ = γR̂ᵐₜ₋₁⁻¹ + (1-γ)/(γ²(1+γ(1-γ)aᵐₜᵀR̂ᵐₜ₋₁⁻¹aᵐₜ))R̂ᵐₜ₋₁⁻¹aᵐₜaᵐₜᵀR̂ᵐₜ₋₁⁻¹
11.   ΔŴᵐₜ ← Lᵐₜ⁻¹∇WᵐLRᵐₜ⁻¹; // Tiền điều kiện Gradient
12.   ΔWᵐₜ ← (‖∇WᵐL‖/‖ΔŴᵐₜ‖)ΔŴᵐₜ; // Tái tỷ lệ Gradient
13. **else**
14.   ΔWᵐₜ ← ∇WᵐL;
15. **end**
16. Wᵐₜ ← Optimizer.step(ΔWᵐₜ, Wᵐₜ₋₁);

### 3.1 Thuật toán MKOR

Thuật toán 1 tóm tắt bộ tối ưu MKOR cho một lớp đơn và Hình 1 cho thấy quy trình làm việc. Đối với mỗi lớp (dòng 1 trong Thuật toán 1), MKOR cập nhật thông tin bậc hai và tiền điều kiện các gradient, và cuối cùng bộ tối ưu backend cập nhật trọng số sử dụng các gradient được tiền điều kiện (dòng 14 trong Thuật toán 1).

**Xấp xỉ Hạng-1.** Đối với các xấp xỉ hạng-1 của các ma trận hiệp phương sai, chúng tôi sử dụng trung bình của các giá trị trên tất cả các mẫu, tức là aᵐ⁻¹ₜ = E[aᵐ⁻¹ₜ] và gᵐₜ = E[gᵐₜ] (dòng 2 và 3 trong Thuật toán 1 và Hình 1-a). (Aᵐ⁻¹ₜ)₍:,ᵢ₎ và (Gᵐₜ)₍:,ᵢ₎ cho thấy cột thứ i của (Aᵐ⁻¹ₜ) và (Gᵐₜ) tương ứng, trong đó Aᵐ⁻¹ₜ và Gᵐₜ là các activation và gradient của lớp m tương ứng.

**Bộ Ổn Định Dựa Trên Norm.** Các giá trị trong nghịch đảo nhân tử trong các phương pháp bậc hai có thể trở nên lớn hoặc biến mất do các giá trị cực lớn hoặc nhỏ trong activation và gradient, dẫn đến bất ổn định số và tràn trên/dưới. Vì nghịch đảo của các nhân tử được nhân trực tiếp với các gradient để tìm giá trị cập nhật, nó có thể gây ra dao động hoặc thậm chí phân kỳ. MKOR sử dụng bộ ổn định dựa trên norm để phát hiện điều này và giải quyết nó bằng cách sửa đổi nghịch đảo của các nhân tử tương ứng (dòng 5 và 6 trong Thuật toán 1 và Hình 1-b). Chi tiết thêm về bộ ổn định dựa trên norm có trong Phần 3.3.

**Bộ Nghịch Đảo Dựa Trên SM.** MKOR trực tiếp sửa đổi nghịch đảo của các nhân tử trái và phải sử dụng cập nhật hạng-1, trong khi sử dụng động lượng để hội tụ tốt hơn. Nếu E[gᵐgᵐᵀ] được xấp xỉ sử dụng ma trận hạng-1 gᵐgᵐᵀ và sử dụng đồng nhất thức Sherman-Morrison, Phương trình 5 được thu được (dòng 7 trong Thuật toán 1 và Hình 1-c).

Lᵐₜ⁻¹ = γLᵐₜ₋₁⁻¹ + (1-γ)/(γ²(1 + γ(1-γ)gᵐₜᵀLᵐₜ₋₁⁻¹gᵐₜ))Lᵐₜ₋₁⁻¹gᵐₜgᵐₜᵀLᵐₜ₋₁⁻¹     (5)

Hơn nữa, nếu Phương trình 4 được xấp xỉ sử dụng E[aᵐ⁻¹ₜaᵐ⁻¹ₜᵀ] ≈ aᵐₜaᵐₜᵀ với một phép suy dẫn tương tự, Phương trình 6 được thu được (dòng 8 trong Thuật toán 1 và Hình 1-c).

Rᵐₜ⁻¹ = γRᵐₜ₋₁⁻¹ + (1-γ)/(γ²(1 + γ(1-γ)aᵐₜᵀRᵐₜ₋₁⁻¹aᵐₜ))Rᵐₜ₋₁⁻¹aᵐₜaᵐₜᵀRᵐₜ₋₁⁻¹     (6)

**Tái tỷ lệ Gradient.** Việc tiền điều kiện gradient sử dụng các nhân tử được tính toán có thể thay đổi norm gradient. Đôi khi, những thay đổi này can thiệp vào ảnh hưởng của tỷ lệ học tập lên quá trình huấn luyện. Để giảm bớt điều này và làm cho các bộ lập lịch tỷ lệ học tập hiệu quả hơn, các gradient được tiền điều kiện được tỷ lệ hóa để norm của chúng khớp với norm gốc (dòng 10 trong Thuật toán 1 và Hình 1-d).

**Phân tích Độ phức tạp.** MKOR giảm chi phí bộ nhớ, truyền thông và tính toán cho việc nghịch đảo nhân tử. Bảng 1 so sánh các chi phí của các bộ tối ưu khác nhau. (1) **Độ phức tạp Tính toán.** MKOR nghịch đảo các nhân tử trái và phải trong Phương trình 2 sử dụng phương trình 5 và 6, cả hai đều có thể được tính toán sử dụng phép nhân ma trận-vector, và có độ phức tạp tính toán O(d²), trái ngược với các phương pháp KFAC và SNGD cần độ phức tạp O(d³) và O(b³) để nghịch đảo ma trận trong ℝᵈˣᵈ và ℝᵇˣᵇ tương ứng. (2) **Độ phức tạp Truyền thông.** Dữ liệu duy nhất được đồng bộ hóa giữa các worker khác nhau trong MKOR là hai xấp xỉ hạng-1 có 2d phần tử. Với lượng tử hóa, kích thước này có thể được giảm một nửa. Trong KFAC, các ma trận hiệp phương sai activation và gradient và nghịch đảo của các nhân tử trái và phải cần được đồng bộ hóa giữa tất cả các worker, dẫn đến 4d² truyền dữ liệu. Trong SNGD, các activation và gradient được đồng bộ hóa, dẫn đến 2bd truyền dữ liệu và các kernel đã nghịch đảo được broadcast, dẫn đến b² truyền dữ liệu. Việc giảm độ phức tạp truyền thông của MKOR từ bậc hai xuống tuyến tính dẫn đến hiệu suất tốt hơn trên số lượng lớn worker. (3) **Chi phí Bộ nhớ.** MKOR cần lưu trữ nghịch đảo của các nhân tử trái và phải và hai vector xấp xỉ hạng-1, dẫn đến chi phí bộ nhớ 2d² + 2d, và sử dụng tính toán độ chính xác nửa làm giảm thêm điều này. KFAC lưu trữ các ma trận hiệp phương sai activation và gradient và các nhân tử trái và phải, dẫn đến chi phí bộ nhớ 4d². SNGD lưu trữ các activation, gradient và các kernel chúng sử dụng làm thông tin bậc hai, dẫn đến độ phức tạp bộ nhớ 2bd + b².

Chi phí bộ nhớ thấp của MKOR cho phép chúng tôi sử dụng kích thước batch lớn hơn so với các phương pháp bậc hai khác. Trong thực tế, các phương pháp tích lũy gradient được sử dụng để tăng kích thước batch hiệu quả trong huấn luyện, điều này làm giảm tốc độ huấn luyện đáng kể. Vấn đề này trở nên tồi tệ hơn với KAISA và HyLo, nhưng MKOR giảm bớt điều này. Để công bằng, trong các thí nghiệm của chúng tôi, chúng tôi đặt kích thước batch cục bộ thành giá trị giống nhau trong tất cả các bộ tối ưu và không tận dụng tính năng này của MKOR.

### 3.2 MKOR Lai

Chúng tôi quan sát thấy rằng các phương pháp bậc hai, bao gồm MKOR, thường tăng tốc huấn luyện nhiều hơn trong các lần lặp đầu tiên của thời gian huấn luyện, và khi mất mát làm phẳng, lợi thế của chúng so với các đối tác bậc nhất trở nên ít rõ ràng hơn. Điều này là do thông tin bậc hai của các hàm mất mát tiếp cận đồng nhất gần các điểm hội tụ. Do đó chúng tôi đã thiết kế một bộ tối ưu lai bậc hai và bậc nhất với phương pháp chuyển đổi dựa trên tỷ lệ giảm mất mát (MKOR-H). MKOR-H đánh giá các thay đổi trong hàm mất mát ở các lần lặp khác nhau và chuyển trở lại các phương pháp bậc nhất nếu cần thiết để có sự cân bằng hiệu quả giữa các cập nhật bậc hai tốn kém và lợi ích của chúng cho hội tụ.

### 3.3 Hội tụ và Ổn định của MKOR

**Tần suất Nghịch đảo** Do chi phí nghịch đảo nhân tử cao trong các phương pháp dựa trên KFAC và SNGD, các nhà nghiên cứu sử dụng phương pháp nhân tử cũ, cập nhật các nhân tử đã nghịch đảo mỗi f lần lặp và tái sử dụng kết quả trong các lần lặp khác trong tiền điều kiện của họ để giảm chi phí tính toán và truyền thông. f là nghịch đảo của tần suất nghịch đảo và thay đổi từ vài trăm đến vài nghìn. Các thí nghiệm của chúng tôi cho thấy rằng trong các mô hình kích thước trung bình như ResNet-50, trong một lần lặp mà việc nghịch đảo các nhân tử được thực hiện, chi phí của KAISA và HyLo là 150× nhiều hơn một lần lặp SGD. Hơn nữa, hơn 98% tổng chi phí trong những lần lặp đó được dành cho việc nghịch đảo ma trận.

Phương pháp nhân tử cũ có thể dẫn đến các bộ tiền điều kiện tốt nếu cảnh quan hàm mất mát không thay đổi đáng kể trong mỗi lần lặp. Tuy nhiên, đây là một giả định mạnh và không nhất thiết đúng trong thực tế. Ngoài ra, việc tăng tần suất nghịch đảo có thể có lợi cho tỷ lệ hội tụ của các phương pháp bậc hai. Thêm vào đó, các thí nghiệm của chúng tôi cho thấy rằng việc sử dụng các nhân tử cũ có thể dẫn đến hội tụ đến các cực tiểu cục bộ trong hàm mất mát và làm tổn hại khả năng tổng quát hóa của mô hình.

**Ổn định Số** Trong các kỹ thuật bậc hai, chúng ta cần nghịch đảo hoặc tìm căn của các ma trận có kích thước khác nhau, thường không có hạng đầy đủ, dẫn đến các vấn đề số. Triển khai KFAC sử dụng phân tích giá trị đơn (SVD) của các nhân tử và che các eigenvalue gần bằng không để xử lý các vấn đề nghịch đảo ma trận đơn lẻ. Trong thực tế, các eigenvalue của các nhân tử trái và phải trong các phương pháp dựa trên KFAC được tính toán từ phương trình 3 và 4 được tăng lên thủ công bằng cách thêm μI vào mỗi cái để cải thiện ổn định số (μ > 0 được gọi là nhân tử giảm chấn), nhưng MKOR không cần những sửa chữa số như vậy. Hơn nữa, HyLo sử dụng hai phương pháp phân tích để lấy mẫu batch đầu vào, cụ thể là KID và KIS. KID yêu cầu nghịch đảo ma trận trong ℝᵇˣᵇ có hạng min(b, d), do đó đối với kích thước batch lớn hơn d trong một lớp cụ thể, phương pháp này thất bại.

Không giống như SVD hoặc các phương pháp lặp khác được sử dụng để nghịch đảo nhân tử, MKOR không gặp phải bất ổn định số phát sinh từ số điều kiện lớn. MKOR có một phép chia vô hướng duy nhất, trong đó mẫu số được đảm bảo khác không dựa trên bổ đề 3.1, loại bỏ khả năng tràn trên/dưới số và nhu cầu các nhân tử giảm chấn (yêu cầu bởi các phương pháp bậc hai khác để ổn định tính toán).

**Bổ đề 3.1.** Các nhân tử được tính toán sử dụng Phương trình 5 và 6 đều là xác định dương.

Nghiên cứu đề xuất sử dụng biểu diễn độ chính xác kép của các số để tránh tràn trên/dưới số trong việc nghịch đảo hoặc tính toán căn của ma trận. Phương pháp này thêm chi phí nhiều hơn cho việc nghịch đảo ma trận và tăng độ phức tạp thời gian của nút thắt chính trong các phương pháp bậc hai.

MKOR không cần tính toán độ chính xác cao hơn, và có thể sử dụng các phép toán dấu phẩy động độ chính xác nửa để giảm chi phí đáng kể. Điều này sẽ cải thiện việc sử dụng bộ nhớ và giảm chi phí truyền thông trong GPU 2× trong khi sử dụng các khối tính toán rẻ hơn cho các phép toán độ chính xác nửa. Bổ đề 3.2 cho thấy giới hạn trên về ảnh hưởng lỗi lượng tử hóa trong các cập nhật MKOR.

**Bổ đề 3.2.** Giả sử rằng lỗi lượng tử hóa tối đa là ε, số tối đa trong ma trận và vector là m, và chiều của các vector và ma trận là d và d×d tương ứng, lỗi lượng tử hóa của công thức 5 và 6 là O((γ + 4(1-γ)/(γ²)m³d²)ε)

**Vấn đề Gradient Bùng nổ** Trong các phương pháp bậc hai, nơi các gradient được tiền điều kiện bởi các nhân tử khác nhau, vấn đề gradient bùng nổ trở nên tồi tệ hơn. Các thí nghiệm của chúng tôi cho thấy rằng trong các phương pháp bậc nhất, bằng cách chọn tỷ lệ học tập không dẫn đến phân kỳ trong vài lần lặp đầu tiên, việc bùng nổ gradient hầu như không bao giờ xảy ra. Mặt khác, trong các phương pháp bậc hai, chúng tôi quan sát thấy rằng việc bùng nổ có thể xảy ra tại bất kỳ lần lặp nào, và cả triển khai KFAC và SNGD đều dễ gặp phải vấn đề này. Điều này có thể dẫn đến gợn sóng trong độ chính xác và phân kỳ. Một trong những phương pháp chính để giải quyết vấn đề gradient bùng nổ là chọn các giá trị nhỏ cho tỷ lệ học tập, hạn chế tỷ lệ hội tụ đáng kể. Đặc biệt, tỷ lệ học tập nhỏ làm tổn hại các phương pháp bậc hai và làm cho chúng gần như có hiệu suất như các đối tác bậc nhất của chúng. Xem xét rằng SGD mạnh mẽ hơn chống lại gradient bùng nổ và tận dụng việc kiểm soát trực tiếp của MKOR trên nghịch đảo của các nhân tử, các nhân tử trong MKOR được sửa đổi để nghiêng về SGD một khi khả năng gradient bùng nổ được phát hiện sử dụng phương trình 7 và 8, trong đó ζ là một siêu tham số kiểm soát lượng thông tin từ các nhân tử gốc cần được lưu trong các nhân tử mới.

L̂ᵐₜ = ζLᵐₜ + (1-ζ)I                                                    (7)
R̂ᵐₜ = ζRᵐₜ + (1-ζ)I                                                    (8)

Bằng cách mở rộng Phương trình 2 với các nhân tử mới, chúng ta sẽ có Phương trình 9, làm giảm mất mát dựa trên bổ đề 3.3. Số hạng đầu tiên ở phía tay phải của 9 là số hạng KFAC, số hạng thứ hai và thứ ba là các phiên bản được tiền điều kiện trái và phải, và số hạng cuối cùng là số hạng SGD.

L̂ᵐ⁻¹∇WᵐLR̂ᵐ⁻¹ = ζ²Lᵐ⁻¹∇WᵐLRᵐ⁻¹
                + ζ(1-ζ)Lᵐ⁻¹∇WᵐL + ζ(1-ζ)∇WᵐLRᵐ⁻¹ + (1-ζ)²∇WᵐL    (9)

**Bổ đề 3.3.** Cho một hàm khả vi L(w) với xấp xỉ chuỗi Taylor bậc nhất L̂(w-Δw) = L(w₀) - Δwᵀ∇wL(w₀) quanh điểm w₀, giả sử rằng tại điểm w₀ đạo hàm bậc hai của hàm L(w) được cho là ∇²wL(w₀) = H = L⊗R, trong đó L và R là các ma trận nửa xác định dương, với giá trị Δw = ((ζL⁻¹ + (1-ζ)I)⊗(ζR⁻¹ + (1-ζ)I))∇L(w₀), bất đẳng thức L̂(w₀-Δw) < L(w₀) tồn tại.

Mặc dù sự sửa đổi này có thể tránh gradient bùng nổ, việc sử dụng quá mức nó với các giá trị nhỏ của ζ sẽ chuyển đổi MKOR thành SGD. MKOR sử dụng chỉ số dựa trên norm nhân tử quan sát norm vô cùng của các nhân tử, và nếu chúng lớn hơn một ngưỡng cụ thể, quá trình sửa đổi nhân tử sẽ được kích hoạt.

## 4 Kết quả Thí nghiệm

Trong phần này, chúng tôi trình bày hiệu suất của MKOR trên một mô hình ngôn ngữ lớn sử dụng các benchmark khác nhau, và phân tích thời gian của các thành phần khác nhau trong các thuật toán bậc nhất và bậc hai khác nhau. Để có kết quả trên nhiều mô hình và tập huấn luyện hơn, vui lòng tham khảo tài liệu bổ sung.

**Mô hình Ngôn ngữ Lớn.** Chúng tôi tiền huấn luyện BERT-Large Uncased và tinh chỉnh nó cho các tác vụ hỏi đáp và phân loại văn bản khác nhau. Một thiết lập tương tự được sử dụng cho tiền huấn luyện và tinh chỉnh. Baseline bậc nhất được sử dụng là Fused LAMB. Tương tự, đối với quá trình tiền huấn luyện, tập dữ liệu Wikipedia tiếng Anh và Toronto BookCorpus, được sử dụng trong tiền huấn luyện BERT gốc, được sử dụng; tập dữ liệu sau không hoàn toàn có sẵn dẫn đến giảm nhỏ độ chính xác baseline đạt được trong các thí nghiệm của chúng tôi so với kết quả BERT gốc. Theo nghiên cứu, do quá trình điều chỉnh siêu tham số tốn thời gian cho giai đoạn đầu của tiền huấn luyện, chúng tôi báo cáo hiệu quả của MKOR chỉ trong giai đoạn thứ hai của tiền huấn luyện trong khi sử dụng các checkpoint của giai đoạn đầu được tạo ra sử dụng bộ tối ưu LAMB.

Như mong đợi, độ phức tạp tính toán, truyền thông và bộ nhớ của HyLo là cao, và phương pháp xấp xỉ Khatri-Rao-based Interpolative Decomposition (KID), ý tưởng chính của HyLo, không thể được thực hiện vì một mẫu đơn không thể vừa với bộ nhớ 40GB của GPU A100. Ngoài ra, HyLo không hỗ trợ tích lũy gradient do độ phức tạp bộ nhớ của nó, phụ thuộc vào kích thước batch; trong các LLM như BERT, kích thước batch lớn như 64k.

Đối với tác vụ hỏi đáp, chúng tôi tinh chỉnh các checkpoint BERT đã được tiền huấn luyện trên tập dữ liệu SQuAD v1.1. Bảng 2 cho thấy F1 Score đạt được sử dụng các bộ tối ưu khác nhau và so sánh tỷ lệ hội tụ và tăng tốc của chúng. MKOR vanilla và KAISA đều hội tụ sau 1000 lần lặp, trong khi bộ tối ưu LAMB yêu cầu 1,536 bước. Xem xét rằng mỗi bước trong MKOR nhanh hơn KAISA, MKOR đạt được tăng tốc end-to-end. MKOR-H sẽ hội tụ trong 600 bước, giảm số bước trong LAMB 2.6×, trong khi đạt được độ chính xác tương tự. Ngoài ra, nó đạt được tăng tốc 2.57× so với bộ tối ưu LAMB và tăng tốc 1.75× so với KAISA. Như một baseline bậc hai khác, chúng tôi xem xét Eva, hội tụ trong 1000 lần lặp, và MKOR đạt được tăng tốc 1.69× so với nó.

Đối với các tác vụ phân loại, chúng tôi tinh chỉnh BERT trên tập dữ liệu GLUE. Bảng 3 so sánh kết quả cho các tác vụ phân loại khác nhau trong tập dữ liệu GLUE. MKOR với 1500 bước đạt được độ chính xác tiên tiến mới trong tập dữ liệu GLUE trên BERT-Large Uncased, và MKOR và MKOR-H với 600 bước đạt được chỉ số trung bình giống như baseline, trong khi giảm số bước với hệ số 2.6×. MKOR và MKOR-H đều đạt được tăng tốc end-to-end 2.57×. Sau khi huấn luyện KAISA trong 1,563 bước, mô hình không hội tụ đến độ chính xác trung bình baseline, trong khi làm chậm hội tụ 0.89×. Eva yêu cầu 1000 bước để hội tụ đến chỉ số trung bình mục tiêu, chậm hơn 1.69× so với MKOR-H với 600 bước và kém chính xác hơn 1.24% so với MKOR với 1500 bước.

Theo hình 2, cho thấy lỗi huấn luyện trong quá trình huấn luyện BERT, MKOR giảm lỗi trong ít lần lặp hơn so với KAISA, Eva và LAMB, dẫn đến hội tụ nhanh hơn. Từ Bảng 2 và 3, MKOR-H hội tụ chỉ trong 600 bước.

**Tần suất Nghịch đảo.** Do độ phức tạp tính toán thấp của các cập nhật trên MKOR, tần suất nghịch đảo nhân tử (f) trong MKOR nằm trong khoảng 10. Hình 4-a cho thấy rằng trong khi chi phí lần lặp trung bình trong KAISA phụ thuộc nhiều vào tần suất nghịch đảo, chi phí của MKOR gần như độc lập với tần suất nghịch đảo. Ngoài ra 4-b cho thấy rằng việc tăng tần suất nghịch đảo dẫn đến tỷ lệ hội tụ cao hơn. Thêm vào đó, việc sử dụng các nhân tử cũ có thể dẫn đến hội tụ đến cực tiểu cục bộ. Do đó, trong MKOR chúng tôi tăng tỷ lệ hội tụ bằng cách cập nhật các nhân tử thường xuyên hơn, mà không ảnh hưởng đến chi phí mỗi lần lặp, dẫn đến tăng tốc end-to-end trong huấn luyện. Chúng tôi sử dụng một autoencoder đơn giản trên CIFAR-100 trong thí nghiệm này.

**Phân tích Hiệu suất.** Chúng tôi so sánh hiệu suất của các phần khác nhau của các bộ tối ưu để minh họa các nút thắt và lợi thế của các phương pháp khác nhau. Quá trình huấn luyện cho một bộ tối ưu có ba bước: tính toán nhân tử, tiền điều kiện và cập nhật trọng số. Hình 3 cho thấy thời gian dành cho mỗi tác vụ trong các bộ tối ưu khác nhau trên hai mô hình; ResNet-50, một CNN và BERT-Large-Uncased, một LLM dựa trên transformer với độ dài chuỗi lớn. Vì các bộ tối ưu bậc nhất như SGD, ADAM và LAMB không yêu cầu factorization và tiền điều kiện, thời gian tối ưu hóa của chúng chỉ được dành cho việc cập nhật trọng số. Trong ResNet-50, vì kích thước mô hình lớn hơn so với kích thước batch, việc tính toán và nghịch đảo nhân tử đắt hơn đối với KAISA so với HyLo. Chi phí này được giảm đáng kể trong MKOR.

Đối với BERT-Large-Uncased, do kích thước lớn của mô hình, thời gian nghịch đảo nhân tử cho KAISA là lớn. Ngoài ra, do giá trị độ dài chuỗi lớn trong mô hình này, thời gian nghịch đảo kernel cho HyLo tương đương với thời gian nghịch đảo của KAISA. Nhưng như mong đợi, do độ phức tạp tính toán thấp, chi phí nói trên trong phương pháp của chúng tôi nhỏ hơn nhiều so với tổng thời gian huấn luyện, dẫn đến tăng tốc. Quan trọng cần lưu ý rằng HyLo phân kỳ trong quá trình huấn luyện này, do đó thời gian hội tụ không được báo cáo cho HyLo.

**Kết quả Thí nghiệm Lỗi Xấp xỉ.** Do tính chất hạng thấp của các ma trận hiệp phương sai, MKOR sử dụng xấp xỉ hạng-1 của các ma trận hiệp phương sai để tăng tốc các tính toán và truyền thông trong các bộ tối ưu dựa trên KFAC. Ở đây, chúng tôi nhằm hỗ trợ lý thuyết và thực nghiệm cho lựa chọn này. Như được hiển thị trong Hình 5, các thí nghiệm của chúng tôi cho thấy rằng các ma trận hiệp phương sai có thể được xấp xỉ với các ma trận hạng-1 với lỗi thấp và các xấp xỉ hạng cao hơn là không cần thiết trong thực tế. Hình 5 cho thấy phân bố lỗi của các phương pháp xấp xỉ hạng-1 tối ưu của các ma trận hiệp phương sai trong tiền huấn luyện ResNet-50 và BERT-Large-Uncased. Các thử nghiệm rộng rãi của chúng tôi trên các benchmark nổi tiếng cho thấy tính chất này tồn tại cho tất cả các mô hình và chúng tôi chưa gặp phải benchmark nào không có ma trận hiệp phương sai hạng thấp.

**Phân tích Lỗi Xấp xỉ và Mở rộng đến Hạng Cao hơn.** Kích thước batch nhỏ và việc tham số hóa quá mức của mạng sẽ dẫn đến các ma trận hiệp phương sai hạng thấp trong DNN. Hãy xem xét ma trận hiệp phương sai C = XXᵀ, trong đó C∈ℝᵈˣᵈ là ma trận hiệp phương sai và X∈ℝᵈˣᵇ là ma trận trong đó mỗi cột tương ứng với một mẫu đơn và d và b là chiều mẫu và kích thước batch trên mỗi GPU tương ứng. Hạng của ma trận hiệp phương sai là min(b, d). Nếu kích thước batch trên mỗi GPU nhỏ, các ma trận hiệp phương sai trong mỗi GPU sẽ có hạng thấp. Các phương pháp xấp xỉ hạng-1 có thể hoạt động tốt trong những kịch bản này. Nếu kích thước batch trong mỗi GPU lớn, chúng tôi quan sát thấy rằng các ma trận hiệp phương sai sẽ vẫn có hạng thấp. Lý do cơ bản cho quan sát này là các mạng nơ-ron hiện tại được tham số hóa quá mức, và kết quả là, các đặc trưng khác nhau trong các ma trận hiệp phương sai của các activation và gradient đầu ra sẽ không độc lập tuyến tính, dẫn đến các ma trận hiệp phương sai hạng thấp.

**Mở rộng MKOR đến Hạng Cao hơn:** Hơn nữa, người ta có thể mở rộng MKOR để sử dụng các ma trận hiệp phương sai hạng cao hơn. Hãy giả sử rằng C = Σᵢ₌₁ʳ cᵢcᵢᵀ trong đó r là hạng của ma trận hiệp phương sai C. Chúng ta có thể áp dụng đồng nhất thức SMW để tính toán C₁ᵐᵉʷ = (Cᵒˡᵈ + c₁c₁ᵀ)⁻¹ với độ phức tạp tính toán O(d²). Sau đó chúng ta có thể tính toán C₂ᵐᵉʷ = (C₁ᵐᵉʷ + c₂c₂ᵀ)⁻¹ sử dụng đồng nhất thức SMW với độ phức tạp tính toán O(d²). Chúng ta có thể tiếp tục cùng một mẫu bằng cách tính toán Cᵢᵐᵉʷ = (Cᵢ₋₁ᵐᵉʷ + cᵢcᵢᵀ)⁻¹. Tổng độ phức tạp tính toán của quá trình này sẽ là O(rd²). Chúng ta nên thêm chi phí này vào chi phí tính toán xấp xỉ hạng thấp của C yêu cầu SVD. Việc sử dụng SVD giết chết lợi thế chính của việc sử dụng tính toán hạng thấp, vì độ phức tạp tính toán của việc áp dụng SVD giống như nghịch đảo các nhân tử trực tiếp. Chúng tôi không thể tìm thấy cách nào rẻ hơn để tính toán xấp xỉ hạng thấp của các ma trận hiệp phương sai, ngoại trừ xấp xỉ hạng-1 được sử dụng trong bài báo này.

## 5 Kết luận

Chúng tôi đề xuất MKOR, một Bộ Tối Ưu Hóa Dựa Trên Nhân Tử Kronecker Với Động Lượng Sử Dụng Cập Nhật Hạng-1 nhằm cải thiện thời gian huấn luyện end-to-end và tỷ lệ hội tụ của các phương pháp bậc hai bằng cách giảm độ phức tạp sử dụng tính toán, truyền thông và bộ nhớ của chúng. Các thí nghiệm của chúng tôi minh họa rằng MKOR vượt trội hơn các bộ tối ưu bậc nhất và bậc hai tiên tiến trên các mô hình ngôn ngữ lớn.

## 6 Tác động Rộng lớn

Nghiên cứu được mô tả trong bài báo này giới thiệu một phương pháp mới để huấn luyện DNN đạt được hội tụ nhanh hơn trong LLM. Sử dụng công trình của chúng tôi có thể giúp tiết kiệm rất nhiều năng lượng và thời gian cho các nhà thực hành học máy. Các tính toán trong công trình này hoàn toàn minh bạch. DNN có thể được áp dụng cho các vấn đề khác nhau, bao gồm chẩn đoán y tế, nhận dạng giọng nói và giải quyết biến đổi khí hậu. Tuy nhiên, cần thừa nhận rằng các thuật toán tối ưu hóa cho DNN cũng có thể được áp dụng cho các mô hình với các tác động tiêu cực tiềm tàng như xâm phạm quyền riêng tư. Quan trọng cần lưu ý rằng việc sử dụng có trách nhiệm và đạo đức của bất kỳ thuật toán tối ưu hóa hiệu quả nào, bao gồm những gì chúng tôi đề xuất, vượt ra ngoài phạm vi của nghiên cứu này.

## 7 Lời cảm ơn

Chúng tôi cảm ơn các nhà đánh giá về phản hồi xây dựng của họ. Chúng tôi muốn bày tỏ lòng biết ơn sâu sắc nhất đến Tiến sĩ Behrooz Zarebavani vì tất cả thời gian và năng lượng mà ông đã dành để giúp đỡ việc viết và định dạng bài báo này. Công trình này được hỗ trợ bởi NSERC Discovery Grants (RGPIN06516, DGECR00303, RGPIN-2023-04897, DGECR-2023-00133), National Science Foundation (NSF CCF-2106621), chương trình Canada Research Chairs, Ontario Early Researcher Award, Digital Research Alliance of Canada (https://www.alliancecan.ca) và Texas Advanced Computing Center (https://www.tacc.utexas.edu). Công trình của Zhao Zhang được hỗ trợ bởi OAC-2106661.
