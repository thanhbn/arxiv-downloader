# 2208.08503.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/optimizer/2208.08503.pdf
# File size: 1733894 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Learning with Local Gradients at the Edge
Michael Lomnitz, Zachary Daniels, David Zhang , Michael Piacentino
September 19, 2022
Abstract
To enable learning on edge devices with fast convergence and low memory,
we present a novel backpropagation-free optimization algorithm dubbed Target
Projection Stochastic Gradient Descent (tpSGD). tpSGD generalizes direct ran-
dom target projection to work with arbitrary loss functions and extends target pro-
jection for training recurrent neural networks (RNNs) in addition to feedforward
networks. tpSGD uses layer-wise stochastic gradient descent (SGD) and local tar-
gets generated via random projections of the labels to train the network layer-by-
layer with only forward passes. tpSGD doesn‚Äôt require retaining gradients during
optimization, greatly reducing memory allocation compared to SGD backpropa-
gation (BP) methods that require multiple instances of the entire neural network
weights, input/output, and intermediate results. Our method performs comparably
to BP gradient-descent within 5% accuracy on relatively shallow networks of fully
connected layers, convolutional layers, and recurrent layers. tpSGD also outper-
forms other state-of-the-art gradient-free algorithms in shallow models consisting
of multi-layer perceptrons, convolutional neural networks (CNNs), and RNNs with
competitive accuracy and less memory and time. We evaluate the performance of
tpSGD in training deep neural networks (e.g. VGG) and extend the approach to
multi-layer RNNs. These experiments highlight new research directions related to
optimized layer-based adaptor training for domain-shift using tpSGD at the edge.
1 Introduction
AI-based systems operating in rapidly changing environments need to adapt in real-
time. The adaptability of the current edge devices is limited by Size, Weight and Power
(SWaP) constraints, the memory wall from current system architectures [1], and the
lack of efÔ¨Åcient on-device learning algorithms. As a result, current AI systems adapt
by retraining on the cloud due to the overwhelming complexity of AI models. Adapta-
tion, or domain transfer for a new data distribution, often requires Ô¨Åne-tuning of the en-
tire or partial network (e.g., last few layers). Similarly, it is often necessary to perform
conversion from full precision [2] to fewer bits in order to Ô¨Åt the limited hardware ar-
chitecture, and doing so might require updating the network by distillation. Despite the
fact that gradient-descent (GD) backpropagation (BP) learning is one of the most stud-
ied and successful approaches, the dominant approach still exhibits some limitations,
including vanishing and exploding gradients [3], difÔ¨Åculty with data quantization, and
1arXiv:2208.08503v2  [cs.LG]  16 Sep 2022

--- PAGE 2 ---
Figure 1: Diagram showing different families of learning algorithms.
inability to handle non-differentiable parameters [4]. Extending GD BP to on-device
training exhibits several additional problems related to limited memory availability: 1)
The weight transport problem requires each layer have full knowledge of other weights
in the neural network (NN); 2) The update locking problem requires a full forward pass
before the feedback pass, leading to the high memory overhead. In GD BP, all layer in-
puts and activations in both forward and backward passes need to be buffered before
weight updates are complete.
To overcome issues associated with GD BP, especially for training at the edge,
many gradient-free (GF) methods, as summarized in Figure 1 (Bio-inspired, ADMM,
and ELM/KR), have been studied. Early GF approaches such as Genetic Algorithms,
Swarm Optimization, ADMM, etc. solved the learning problem without using GD.
However, they do not show distinctive energy-time saving nor are they memory-efÔ¨Åcient
and scalable. Biologically-plausible approaches such as Hyperdimensional (HD) com-
puting, and Spike NN (SNN) exhibit energy and/or memory-efÔ¨Åcient learning; never-
theless, training these binary represented weights and inputs is most successful based
on the approximation from the Ô¨Çoating point NNs with distillation. Alternating Di-
rection Method of Multipliers (ADMM) [5] decomposes the network training into a
sequence of sub-steps that can be solved as simple linear least-squares problems, e.g.,
using Moore-Penrose (MP) pseudoinverse. ADMM optimizes each layer using the fol-
lowing layer as a condition (Lagrange multiplier). As such, it is a forward pass-only
algorithm. ADMM variants (Scalable ADMM, dlADMM, pdADMM [6, 7, 8]) have
tried to efÔ¨Åciently utilize available hardware resources and reduce compute time. How-
ever, they take many iterations to converge to the Ô¨Ånal solution. Although relatively
mature and scalable to large problems, the learning time of ADMM is still much longer
compared to other GF methods such as KPnet [9].
2

--- PAGE 3 ---
Most of the gradient-free approaches have demonstrated their performance in shal-
low NN models and in fully connected layers. They are less accurate than deep NN
models trained with BP nor are they scalable. However, the combination of these ap-
proaches shows a promising trend. For example, KPnet combines Extreme Learning
Machines (ELM) [10] with label encoding and target propagation to speed up train-
ing and improve scalability. LM-KARnet[9] trains with time savings of 10-100X in
shallow networks. ZORB [11] is another GF training algorithm that combines rapid
training of ELMs and MP pseudoinverse. It is 300X faster than the Adam optimizer on
shallow networks and also achieves comparable accuracy.
Frenkel et al. proposed Direct Random Target Projection (DRTP) [12, 13], a method
introducing target projection into GD learning algorithms without feedback. DRTP al-
leviates two key BP issues (weight transport problem and update lock) by enabling
each layer to be updated with local information as the forward evaluation proceeds.
It allows training hidden layers at low computational and memory costs. The locality
concept was proven to work on 1-2 shallow layers, and the implemented DRTP in an
event-based CNN processor [14] required only 16.8% power and 11.8% silicon area
overheads. Based on this concept, we propose a novel backpropagation-free optimiza-
tion algorithm called target projection Stochastic Gradient Descent (tpSGD) , capable
of training NN models at the edge. tpSGD trains neural networks layer-by-layer us-
ing only forward passes. As far as we know, tpSGD is the Ô¨Årst method to extend local
training to arbitrary loss function, use target projection for training RNNs, and scale
up the locality concept to deep neural networks (DNNs). More speciÔ¨Åcally, we do not
focus on Ô¨Ånetuning the entire network at the edge. Instead we study whether tpSGD
can Ô¨Ånetune optimized shallow layers to adapt to new environments with less memory
usage, more efÔ¨Åcient computing, and faster convergence. In summary, tpSGD has the
following unique features:
‚Ä¢ Local efÔ¨Åcient computation per layer using SGD and target projection. Our method
performs comparably to GD BP (within 1% accuracy) on relatively shallow net-
works with 1-6 trainable convolutional and/or fully connected layers, while elim-
inating the need to retain and compute through the network in the backward pass.
‚Ä¢ Extends target projection to multichannel convolutional layers via Ô¨Ålter-based
sampling, enabling CNN training.
‚Ä¢ Extends target projection to recurrent neural networks (RNN) (standard, stacked
multi-layer RNNs, and bidirectional RNNs), demonstrating non-trivial learning
and in many cases, performing within 5% accuracy of BP-trained networks on
several benchmark datasets.
‚Ä¢ Demonstrates state-of-the-art (SoA) performance and decreased training time
compared to other BP-free algorithms (e.g., dlADMM, ZORB, KARnet).
‚Ä¢ Scalable to DNNs by training the optimized shallow network adaptors that con-
nect to the Ô¨Åxed DNN encoders.
3

--- PAGE 4 ---
2 Methodology
2.1 Target Projection and Target Propagation
In target propagation [11], the labels are ‚Äúpropagated‚Äù backward through the network
by sequentially inverting each operation (layers and activations) in order to obtain the
corresponding output to the labels. For instance, for a linear layer zi=xiWi, this
involves obtaining an approximate value for the inverse of the layer weights Wy
ivia
the MP pseudoinverse given xias input from the output of the previous layer i 1and
zias the output features of layer itarget propagated from the labels through inversion
of the Ô¨ÅnalN ilayers.
In target projection [15, 12], one-hot encodings of the labels are directly projected
to a given layer during the optimization step. Given an intermediate layer Liin a NN,
we generate local targets yifor the layer by projecting the data labels yvia a random
projection matrix ( Pi). The focus of tpSGD is to provide a fast and scalable BP-free
algorithm for training NNs, so we chose target projection, replacing the need to invert
all of the layers and activations after a given layer, with a single matrix multiplication.
2.2 Target Projection SGD
tpSGD is designed with the goal of scaling BP-free training to larger datasets and
deeper networks required for complex tasks. Unlike ZORB [11] or LM-KARnet [9],
we Ô¨Årst utilize GD-based optimization for the individual layers instead of MP pseudo-
inverse which optimizes weights on an entire data set at once. While ZORB and LM-
KARnet struggle to process the entire CIFAR dataset (especially when convolutions
are included) our algorithm can process them in batches providing more freedom to
extend to datasets and tasks with larger memory requirements. Second, and in contrast
to BP, which calculates a full forward and backward pass through the network, tpSGD
is a feedforward only optimization algorithm. In tpSGD, we train each layer in the
network sequentially starting from the layers closest to the input. For a given layer
Liin a NN with i2Nlayers, the input to that layer xiis obtained by running the
forward pass over all previous j= 1 toi 1layers. The target output yiis obtained
via a random projection of the one-hot encoding of the data labels. The input xiand
projected targets yiare used to train the layer using Adam optimizer and the Mean
Squared Error (MSE) between the predictions and yi(either before or after activation).
Once a layer is trained, we Ô¨Åx the weights and move on to the next layer following the
same approach until the Ô¨Ånal layer is reached as we no longer require a projection. The
next sections will discuss connections to existing projection-based training algorithms
before moving on to the the details of generating the random projection matrices Pifor
the different supported trainable layers: dense, 2d convolutions and recurrent.
2.3 Connections to Existing Projection-Based Training Algorithms
tpSGD shares close connections with a few prominent BP-free, projection-based train-
ing algorithms. In some cases, tpSGD acts as a generalization. We highlight con-
nections between Direct Feedback Alignment (DFA) [13], Error-Sign-Based Direct
4

--- PAGE 5 ---
Method Ji(:) w(t+1)
i
DFA Cross-Entropy( y,yN)w(t)
i+Pi(y yN)0
i(zi)
sDFA Cross-Entropy( y,yN)w(t)
i+Pisign(y yN)0
i(zi)
DRTP Cross-Entropy( y,yN)w(t)
i+Piy0
i(zi)
tpSGD`1jjPiy yijj1w(t)
i+sign(Piy yi)0
i(zi)
tpSGD`2jjPiy yijj2
2w(t)
i+(Piy yi)0
i(zi)
Table 1: We show the relation between various projection-based methods for training
neural networks. In particular, we look at the loss functions Ji(:)that each approach
is attempting to minimize as well as the weight update w(t+1)
i at layerifor iteration
t+ 1. Note that we incorporate multiplicative constants into .
Feedback Alignment (sDFA) [12], Direct Random Target Projection (DRTP) [12], and
our novel tpSGD approach (tpSGD using layer-wise `1-error (tpSGD `1) and`2-error
(tpSGD`2)).
Consider the case of a simple linear layer iwith non-linear activation function:
yi=i(zi) =i(xiWi). We deÔ¨Åne the following quantities, J(:)is a loss function,
xiis the input of layer i,yiis the output of layer i,Wiare the weights associated
with layeri,i(:)is the non-linear activation function associated with layer i,yiis
the estimated loss gradients for the outputs of layer i,yNis the predicted output of the
Ô¨Ånal layer,yis the ground truth one-hot encoding of the labels, 0
i(:)is derivative of
the non-linear activation function of layer i,Piis the layer-dependent projection matrix
of the targets for layer i, andis the learning rate.
In Table 1, we inspect the optimization objectives of the aforementioned algorithms
and investigate how this affects the weight update steps at layer i. The layer-wise
weight update rules are similar to one another, but each algorithm has subtle distinc-
tions in their assumptions, leading to different optimization behaviors and Ô¨Çexibility in
the types of training pipelines they are compatible with.
DFA, sDFA, and DRTP all attempt to minimize a global objective (cross-entropy
between the ground truth labels and Ô¨Ånal layer output) using noisy gradient steps. DFA
and sDFA require a complete forward pass through the network. Then these approaches
compute estimated loss gradients for a given layer based on random projections of the
gradients of the loss at the Ô¨Ånal layer. sDFA uses only the sign of the loss gradients
whereas DFA uses the full magnitude of the loss gradients. Since both of the methods
require a full pass through the network, they are not compatible with tpSGD.
DRTP makes use of the fact that labels are one-hot encodings to simplify sDFA,
where DRTP‚Äôs loss gradients are a surrogate of those of sDFA with shift and rescaling
operations applied. Following the reasoning of Frenkel et al. [12], we look at the be-
havior of the loss gradient esd fa=sign(y yN)wheny
cis 1 and when y
cis 0. We
assumey
c2f0;1g(i.e., the ground truth label for a given class is exactly zero or one)
andyNc2(0;1)(i.e., the prediction of the Ô¨Ånal layer for a given class is in the range
zero or one, non-inclusive). Under this assumption, y
cnever exactly equals yNc(i.e.,
5

--- PAGE 6 ---
there is always some prediction error). The loss gradient for sDFA can be expressed as:
esd fa c=sign(y
c yNc) =(
1 ifc=c
 1otherwise;y
c6=yNc
cis the index of the one-hot vector and cis the true class.
The key observation here is that esd fa cwill always be +1wheny= 1, and it will
always be 1wheny= 0, regardless of the values of the prediction yN, effectively
decoupling the estimated loss gradients from the Ô¨Ånal layer predictions. The loss gra-
dient associated with DRTP is simply a scaling and shifting of esd fa cunder the above
assumptions:
edrtp c=1 +sign(y
c yNc)
2=(
1ifc=c
0otherwise;y
c6=yNc
Noteedrtp=y, meaning the one-hot labels can serve as the estimated gradient of the
loss. To obtain layer-wise gradients w.r.t. the global loss, DRTP uses a random projec-
tion matrix to project these one-hot labels as the estimated loss gradients for the global
objective. Frenkel et al. showed that for networks consisting of an arbitrary number of
linear layers followed by a non-linear activation with cross-entropy as the loss function,
DRTP produces estimated gradients within 90of the BP gradients, leading to learning,
and they experimentally validated that DRTP works with less constrained feedforward
architectures. DRTP does not require a full forward pass before updating the weights
(no update locking), and DRTP can update layers independent of one another (no issues
with weight transport), making DRTP fully compatible with tpSGD. Different layers
in the same network can be optimized via either tpSGD or DRTP without issue. In the
rest of the paper, we regard DRTP as a subset of the capabilities provided by tpSGD.
When DRTP is mentioned, we speciÔ¨Åcally refer to the aforementioned variations of
cost functions and weight updates.
Finally, we consider our novel tpSGD-style learning. In contrast to other approaches,
tpSGD tries to minimize a collection of local losses, with the intention of learning rep-
resentations that promote reduction of the global loss. tpSGD locally optimizes layers,
forcing the layer outputs to align with random projections of the one-hot ground truth
labels (serving as layer-wise targets for optimization). tpSGD tries to learn representa-
tions such that instances from the same class are well-separated from instances of other
classes within the same layer, and representations from two different layers should be
well-separated from one another. tpSGD is compatible with any generic layer-wise loss
between the layer-wise targets and layer-wise outputs, but we explicitly consider the
`1-norm and`2-norm for this paper.
2.4 Layer Projection DeÔ¨Ånitions
In tpSGD, we provide learning targets for intermediate layers by projecting the ground
truth labels via randomly generated matrices. This section discusses the projection
strategies for linear, convolutional and recurrent layers in tpSGD.
6

--- PAGE 7 ---
Figure 2: Illustration showing the difference between approaches to constructing target
features T for convolutional layers using random matrices P. Panel a) shows the basic
or ‚Äùnaive‚Äù approach vs. Ô¨Ålter-based sampling. In b), the tuples in parenthesis under the
labels illustrate the tensor sizes.
Linear layer projections
In the simplest case of a fully connected layer with n-nodes and a classiÔ¨Åcation problem
withbsbatch size,ncclasses, we map a batch of labels with dimensions (bs;nc )to one
with(bs;n)by multiplying by a random matrix Pwith dimensions (nc;n). Among our
experiments we tested sampling from different distribution functions (uniform, normal,
...), but have found no noticeable difference on the performance.
Conv2D layer projections
To extend target projection via random matrices for CNN, we implemented two ap-
proaches to generate the target features for these intermediate Conv2D layers, which
are illustrated in Figure 2 a) and b).
Assume the output of a given Conv2D layer has dimensions (bs;nx;ny;nf ), where
bsis the size of the mini-batch being processed, nxandnyare thexandydimensions
of the Ô¨Åltered image, and nfare the number of Ô¨Ålters. In panel a), the naive approach,
we generate a single, long projection matrix Pwith dimensions (nc;nxnynf)
and reshape the output to match the target dimensions. In panel b), we propose a
Ô¨Ålter-based sampling approach: we generate nfdifferent projection matrices Piof size
(bs;nxny)for each of the i2f1;2;:::;nfgÔ¨Ålters, sampling from a different ran-
dom distribution for each. We sample from normal distributions with varying standard
deviations in the range [0;1)at equally spaced intervals i=i=nf .
Figure 3 shows the difference in performance between these two sampling meth-
ods. A model consisting of a Conv2D layer, leaky ReLU activation and a Ô¨Ånal linear
classiÔ¨Åcation layer was trained using our tpSGD algorithm on MNIST using both the
basic sampling and proposed Ô¨Ålter-based sampling method. Whereas the basic sam-
pling shows little or no improvement as the number of Ô¨Ålters in the layer increases,
Ô¨Ålter-based sampling projections show steady improvement, albeit with diminishing
returns. In our studies we observed a small, constant increase in training time due to
the increased cost in initializing the projections for each Conv2D layer.
7

--- PAGE 8 ---
Figure 3: Basic and Ô¨Ålter-based projection (F-projections) performance for Conv2D
layers as a function of the number of Ô¨Ålters.
Recurrent layer projections
Past work [12] showed that target projection can train feedforward networks. To the
best of our knowledge, target projection has not previously been shown to work with
recurrent neural networks (RNNs). We describe how recurrent layers can be modeled
so that simple modiÔ¨Åcations of either DRTP and tpSGD makes target projection an
effective algorithm for training RNNs. In subsequent sections, we demonstrate experi-
mentally for the Ô¨Årst time that random target projection promotes learning in recurrent
networks.
We formulate a simple recurrent cell (pictured in Appendix C), which computes the
following:
Ht+1=sigmoid (HtWH+bH+XtWX+bX)
whereHtis the hidden state input at time t,Xtare the input features at time t,WH,
bH,WX, andbXare the learnable parameters of the cell (shared over all time steps),
andHt+1is the hidden state that serves as input to the next time step. While there are
more complex cell types used in practice (e.g., LSTM [16] and GRU [17] cells), these
are designed to solve issues with modeling long-range dependencies (e.g., vanishing
gradients), which random projection-based approaches are not susceptible to, so we do
not consider them for this paper.
In order to apply TP to this recurrent cell, similar to BP through time [18], we unroll
the recurrent cell over time for a Ô¨Åxed problem-speciÔ¨Åc sequence length (see Appendix
C). This produces a feedforward-like network where every layer shares a common set
of learnable parameters but uses the hidden state output by the previous layer with time
step-dependent input features.
We see two key differences between the unrolled recurrent layer and generic feed-
forward MLPs: 1) there are two linear functions that feed into the non-linearity per
unrolled ‚Äúlayer‚Äù, and 2) the weights are shared between the unrolled ‚Äúlayers‚Äù. The Ô¨Årst
difference is handled by updating the parameters of each function separately. For (2),
we can make use of the fact that we can freeze the weights of the recurrent cell for the
current training iteration and update over all time steps simultaneously, i.e., with frozen
8

--- PAGE 9 ---
weights, a forward pass through the unrolled recurrent cell is:
2
664H1
H2
:::
Hend3
775= 2
664H0X0
H1X1
:::
HNXN3
775WH
WX
(bH+bX)!
(1)
whererepresents row-wise addition of a vector to a matrix. Since these weights
are frozen and updated simultaneously for all time steps and tpSGD doesn‚Äôt enforce
any backward propagation requirements, the Hs can be treated as independent inputs.
Then, the above formulation of the recurrent layer is equivalent to a feedforward linear
layer plus nonlinear activation.
When computing the gradient of WH, there is no dependency on Xand when com-
puting the gradients of WX, there is no dependency on H, so each set of parameters can
be updated separately: when WHis updatedWXis frozen and vice versa. Secondly,
when gradients are computed using this formulation, it is equivalent to computing gra-
dients over each ( Ht,Xt) separately and summing over all time steps (i.e., gradient
accumulation). This means we can compute the individual pseudo-gradients per time
step and sum or average them over all time steps, storing only the accumulated gradi-
ents, reducing memory requirements . To train the parameters of a recurrent cell for a
given time step, we use the following updates, where we assume different projection
matricesBtfor the optimal one-hot encoded labels Yfor each time step with learning
ratefor tpSGD`1:
WH
bH(k+1)
=WH
bH(k)
+rWH
rbH(k)
(2)
WX
bX(k+1)
=WX
bX(k)
+rWX
rbX(k)
(3)
rWH
rbH
=NX
t=0HT
t
1T
DH;rWX
rbX
=NX
t=0XT
t
1T
DH (4)
DH=sign(YBt Ht+1)Ht+1(1 Ht+1) (5)
whereis element-wise multiplication and ‚Äú1‚Äù represents the generalized scalar/vec-
tor/matrix of all ones.
We can similarly derive update rules for tpSGD `2and DRTP. We found experi-
mentally that tpSGD `2does demonstrate non-trivial training of RNNs, but it signiÔ¨Å-
cantly underperforms tpSGD `1and DRTP in this case.
The initial weights of the recurrent cells can be all zeros or drawn uniformly ran-
domly with small magnitude. To initialize the projection matrices for tpSGD `1, we
use an random binary matrices with approximately orthogonal rows. To initialize the
projection matrices for DRTP, we use random matrices with orthogonal rows.
9

--- PAGE 10 ---
Algorithm MNIST Training
Test accuracy (%) Time (s)
ZORB 89.4(0.22) 7.9(0.39)
LM-KARNet 89.9(0.12) 4.9(0.38)
tpSGD 88.3(0.25) 1.7(0.01)
Table 2: Comparison of gradient free approaches with a 2-layer MLP trained on MNIST
Weights can be updated locally using multiple forward passes through the recurrent
layer before sending the Ô¨Ånal Hendfeature vector to the next tpSGD-compatible layer.
This means it is trivially easy to extend the system to stacked RNNs [19] and with
simple modiÔ¨Åcations, the recurrent cell can be made to be compatible with bidirectional
RNNs [20].
3 Experiments and Results
We compare tpSGD to BP over a set of classiÔ¨Åcation tasks and different datasets, and
combinations of Linear, Conv2D, and Recurrent layers to study how effectively the
algorithm can scale. We also compare it to a series of backpropagation-free algorithms
designed speciÔ¨Åcally to work at the edge. Both ZORB and LM-KARnet ingest an entire
dataset in a single step during optimization. They excel in training with small datasets,
but fail to extend to large datasets, deep NNs and complex tasks.
3.1 Shallow networks, MNIST and CIFAR
We begin by studying shallow networks on MNIST [21] and CIFAR [22]. Table 2
shows a comparison between the proposed tpSGD, ZORB and LM-KARNet on MNIST
using a 2-layer MLP with leaky ReLU activation. We report mean value and standard
deviation in parenthesis over 25 random restarts for the training time and model ac-
curacy. All three algorithms were benchmarked using implementations in TensorFlow
2.9 running on a single NVIDIA RTX A5000 GPU. While tpSGD‚Äôs performance is
1-2% below both ZORB and LM-KARNet, we see sizeable time savings for tpSGD.
While both ZORB and LM-KARnet use the MP inverse to obtain the new weights, LM-
KARnet replaces target propagation in ZORB with random projection. This accounts
for the substantial speed up when comparing their training times: ZORB inverts each
layer starting from the output while LM-KARnet projects using a single random ma-
trix. However, LM-KARnet still uses the MP inverse to optimize each layer‚Äôs weights.
Replacing this expensive operation in tpSGD leads to further decreases in the training
time with near negligible loss in accuracy.
In Table 3 we compare the performance (mean and standard deviation per metric)
of a shallow model consisting of two Conv2D layers with leaky ReLU activations, and
one Ô¨Ånal Linear layer on MNIST and CIFAR-10 using the same settings described
earlier. The results show that tpSGD can nearly match the accuracy (within 1%) and
training time with BP. Appendix A explores the scaling of tpSGD (as compared to BP)
10

--- PAGE 11 ---
tpSGD BP
Acc.(%) Time(s) Acc.(%) Time(s)
MNIST 96.6(0.2) 2.7(0.1) 97.1(0.21) 2.3(0.3)
CIFAR 46.5(1.2) 4.6(0.1) 46.8(1.1) 4.8(0.4)
Table 3: Comparison between tpSGD and BP on training a shallow CNN on MNIST
and CIFAR-10
in more detail in CNNs with up to 7 trainable (Dense and Conv2D) layers.
3.2 VGG and Imagenette
Next, we experiment extending tpSGD to large images and deeper networks. For this
we employ Imagenette [23], a subset of 10 easy classes from Imagenet [24]. Imagenette
consists of roughly 1000 color images, at least 320x320 pixels in size. The Ô¨Årst row
in Table 4 compares the performance between VGG11 networks (with 11 trainable
layers) trained from a random initialization using BP and tpSGD. We report mean and
variation (in parenthesis) of the performance over 25 random restarts. We present the
Ô¨Årst practical demonstration of backpropagation-free training applied to images and
networks of this scale, the SoA results obtained by tpSGD ( 65%) is roughly 10-15%
drop in performance compared with the BP baseline.
Given the challenge of directly scaling-up tpSGD, we studied an alternative ap-
proach to scaling to DNNs by adapting to domain-shift with shallow adaptor NNs
running tpSGD. The concept is to design and optimize a few adaptor layers that con-
nect to the layers of the pretrained DNNs. During learning, only the shallow adaptor
weights are trained and the pretrained DNNs are Ô¨Åxed. Different from the SoA Ô¨Ånetun-
ing methods, the conÔ¨Ågurations of the adaptors at the edge are determined via network
architecture search [25] and optimized based on out-of-distribution detection [26]. The
discussion of these is not the focus of this paper. The second and third rows in Ta-
ble 4 show studies using tpSGD and BP in a transfer learning setting. The second
row shows the baseline approach using BP to Ô¨Ånetune the Ô¨Ånal fully connected lay-
ers in the network. To compare, the third row shows the performance obtained using
tpSGD and BP using a lightweight adaptor conÔ¨Åguration obtained via a basic network
architecture search (NAS) (detail discussed in appendix B). The adaptor consists of:
Conv2D(Ô¨Ålters=64, kernel size=5, strides=1), LeakyReLU, Dense(256), LeakyReLU,
Dense(n classes=10).
The model trained using tpSGD on adaptors is within 3-5% of the result obtained
using BP and Adam on VGG16 Ô¨Ånetuning echoing the results shown in earlier sections
discussing shallow networks, while signiÔ¨Åcantly reducing the in memory footprint due
to the selected adaptor architecture. We envision leveraging a pretrained and quantized
or pruned feature extractor on specialized hardware together with tpSGD-based shal-
low adaptors to enable training on edge devices after deployment and adapting to new
domains, novel tasks and distribution shifts.
11

--- PAGE 12 ---
Experiment Imagenette Accuracy (%)
tpSGD BP
Full net (180 MB) 64(3.1) 78(1.4)
Transfer (492 MB) - 87(1.3)
Adaptor (6 MB) 83(0.95) 88(1.7)
Table 4: Performance comparison between tpSGD and SGD BP on the Imagenette
dataset in three secenarios: training all layers of a VGG11, the transfer learning baseline
and using our own adaptor with a Ô¨Åxed pretrained VGG16. We include, in MB, the total
size of layers being trained.
Dataset Random Random RNN + Trained RNN (DRTP) Trained RNN (tpSGD `1) + Trained RNN (BP) +
Trained ClassiÔ¨Åer (tpSGD `2)ClassiÔ¨Åer (tpSGD `2) ClassiÔ¨Åer (tpSGD `2) ClassiÔ¨Åer (BP)
Seq. MNIST (rows) 9.58% (10%) 9.76% 83.39% 79.81% 92.53%
Seq. MNIST (pixels) 10.28% (10%) 11.35% 62.83% 66.46% 75.55%
EEG Seizure 48.35% (50%) 79.48% 89.57% 83.57% 94.14%
UCF101 0.80% (0.99%) 53.78% 60.18% 60.76% 61.58%
Twitter (one-hot) 49.92% (50%) 50.08% 66.82% 67.72% 71.75%
Twitter (Word2Vec) 49.92% (50%) 64.84% 69.60% 68.67% 72.26%
Table 5: Results of training a single-layer RNN with linear classiÔ¨Åer. Numbers in paren-
theses are true random chance. We show the algorithm used to train each layer in paren-
theses where BP is traditional backpropagation.
3.3 Time Series Analysis Using Recurrent Neural Networks
We also conducted experiments over a wide range of datasets with differing properties
between them (see Table 6 in Appendix D), to validate the training of the recurrent lay-
ers in combination with multi-layered perceptrons for classiÔ¨Åcation of time series data.
In the case of the UCF101 and ‚ÄúTwitter Sentiment Analysis - Word2Vec Encoding‚Äù
datasets, we use an external model to Ô¨Årst preprocess the data into feature vectors. All
other datasets work over the raw representations.
We consider three experiments: (1) a single-layer RNN in combination with a linear
classiÔ¨Åer in Table 5, (2) a two-layer stacked RNN in combination with a two-layered
perceptron (Table 7 of Appendix E), and (3) a single-layer Bidirectional RNN in com-
bination with a linear classiÔ¨Åer (Table 8 of Appendix E). In each case, we use a hidden
vector size of 512, a batch size of 128, and train for 20 epochs. We compare against
random chance, a completely random network of equivalent structure, a network where
the RNN is random, but the classiÔ¨Åer is trained, and a structurally-equivalent model
trained only using backpropagation. We also demonstrate that the method effectively
trains with either DRTP or tpSGD `1as the optimizer for the recurrent layers, and
performance is generally similar.
We verify that there is non-trivial learning of the recurrent layer(s) by comparing
the model with randomly weighted recurrent layers + trained classiÔ¨Åer with the model
where both the recurrent layers and classiÔ¨Åers are trained using target projection. We
see signiÔ¨Åcant improvement in all cases except the Seizure Activity Recognition dataset
where even random features achieve relatively high performance.
We also compare performance of the target projection-based models to the per-
formance of the gold-standard BP based models to understand how well the model
12

--- PAGE 13 ---
learns compared to an ‚Äúupper bound‚Äù and similarly, compare to random chance and
models with random weights to and how well the model learns compared to a ‚Äúlower
bound‚Äù. In all cases, the trained model signiÔ¨Åcantly outperforms the completely ran-
dom models. Generally, the models trained with target projection perform well. The
target projection-based model performs within 5% accuracy of the backpropagation
based model on 3 ( tpSGD `1) / 4 (DRTP) of the 6 datasets. A larger gap exists when
tested on both forms of sequential MNIST, but the model still exhibits signiÔ¨Åcant per-
formance.
Similar results are seen for the stacked and bidirectional RNNs. One interesting ob-
servation about these models is that performance is very similar to the basic single layer
RNN with linear classiÔ¨Åer model. This suggests that target projection is not preventing
the stacked and bidirectional RNNs from learning meaningful representations.
4 Conclusions and Future Work
In this paper, we explored a novel BP-free, feedforward-only optimization algorithm
designed to enable training in resource constrained environments, such as edge de-
vices. We discussed the connections between tpSGD and other existing BP-free algo-
rithms and compared their performance when training in architectures such as MLPs,
CNNs and RNNs. We found that tpSGD in training performs comparably to the BP
SGD and BP-free algorithms in shallow MLPs, CNNs, and RNNs, and is superior to
other BP-free algorithms in terms of memory and time. Finally, we showed tpSGD
scales-up to DNNs using transfer learning via an optimized shallow adaptor concept.
Although tpSGD-based training of full DNNs underperforms compared to BP SGD,
the algorithms are effective for training shallow adaptors connected to Ô¨Åxed encoders.
This alternative scale-up architecture has great potential for on-device learning in edge
scenarios in order to tackle domain shift, novel tasks, and other similarly challenging
problems.
For future work, we desire to better understand tpSGD from a theoretical perspec-
tive. We have noticed that tpSGD-based models do not seem to achieve their full po-
tential in DNNs. We hypothesize that this may be due to (the combination of) two
reasons: (1) more complex models might not be needed for relatively simple bench-
mark datasets; (2) because the layers of the model are trained without feedback from
later to earlier layers, these more complex models might be overly restricted in what
they can learn, thus showing minimal growth in performance.
From the Ô¨Ålter-based sampling in CNN, we have noticed that carefully selecting
the distinctive projection distribution is important. In the future, we want to explore
whether we can learn the distribution of layer-wise target projections from pretrained
models for the dataset instead of blindly estimating these projections. Another future
direction is to analyze eigenvector correlations among input data samples based on per
layer Jacobians [25] so that the random projection is selected with the least redundancy
among the estimated projections.
13

--- PAGE 14 ---
Acknowledgements
This research is based upon work supported in part by the OfÔ¨Åce of the Director
of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity
(IARPA), via Contract No: 2022-21100600001. The views and conclusions contained
herein are those of the authors and should not be interpreted as necessarily representing
the ofÔ¨Åcial policies, either expressed or implied, of ODNI, IARPA, or the U.S. Gov-
ernment. The U.S. Government is authorized to reproduce and distribute reprints for
governmental purposes notwithstanding any copyright annotation therein.
References
[1] S. A. McKee and R. W. Wisniewski, Memory Wall , pp. 1110‚Äì1116. Boston, MA:
Springer US, 2011.
[2] A. Raghavan, M. Amer, S. Chai, and G. Taylor, ‚ÄúBitnet: Bit-regularized deep
neural networks,‚Äù arXiv preprint arXiv:1708.04788 , 2017.
[3] R. Pascanu, T. Mikolov, and Y . Bengio, ‚ÄúOn the difÔ¨Åculty of training recurrent
neural networks,‚Äù 2012.
[4] S. Liu, P.-Y . Chen, B. Kailkhura, G. Zhang, A. Hero, and P. K. Varshney, ‚ÄúA
primer on zeroth-order optimization in signal processing and machine learning,‚Äù
2020.
[5] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, ‚ÄúDistributed optimiza-
tion and statistical learning via the alternating direction method of multipliers,‚Äù
Foundations and Trends in Machine Learning , vol. 3, pp. 1‚Äì122, 01 2011.
[6] G. Taylor, R. Burmeister, Z. Xu, B. Singh, A. Patel, and T. Goldstein, ‚ÄúTraining
neural networks without gradients: A scalable admm approach,‚Äù in International
conference on machine learning , pp. 2722‚Äì2731, PMLR, 2016.
[7] J. Wang, F. Yu, X. Chen, and L. Zhao, ‚ÄúAdmm for efÔ¨Åcient deep learning with
global convergence,‚Äù in Proceedings of the 25th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining , pp. 111‚Äì119, 2019.
[8] J. Wang, Z. Chai, Y . Cheng, and L. Zhao, ‚ÄúToward model parallelism for deep
neural network based on gradient-free admm framework,‚Äù 2020.
[9] H. Zhuang, Z. Lin, and K.-A. Toh, ‚ÄúTraining a multilayer network with low-
memory kernel-and-range projection,‚Äù Journal of the Franklin Institute , vol. 357,
no. 1, pp. 522‚Äì550, 2020.
[10] G.-B. Huang, D. H. Wang, and Y . Lan, ‚ÄúExtreme learning machines: a survey,‚Äù
International journal of machine learning and cybernetics , vol. 2, no. 2, pp. 107‚Äì
122, 2011.
14

--- PAGE 15 ---
[11] V . Ranganathan and A. Lewandowski, ‚ÄúZorb: A derivative-free backpropagation
algorithm for neural networks,‚Äù arXiv preprint arXiv:2011.08895 , 2020.
[12] C. Frenkel, M. Lefebvre, and D. Bol, ‚ÄúLearning without feedback: Fixed random
learning signals allow for feedforward training of deep neural networks,‚Äù Fron-
tiers in Neuroscience , vol. 15, feb 2021.
[13] A. N√∏kland, ‚ÄúDirect feedback alignment provides learning in deep neural net-
works,‚Äù 2016.
[14] C. Frenkel, J.-D. Legat, and D. Bol, ‚ÄúA 28-nm convolutional neuromorphic pro-
cessor enabling online learning with spike-based retinas,‚Äù 2020.
[15] H. Zhuang, Z. Lin, and K.-A. Toh, ‚ÄúTraining multilayer neural networks analyti-
cally using kernel projection,‚Äù in 2021 IEEE International Symposium on Circuits
and Systems (ISCAS) , pp. 1‚Äì5, 2021.
[16] S. Hochreiter and J. Schmidhuber, ‚ÄúLong short-term memory,‚Äù Neural computa-
tion, vol. 9, no. 8, pp. 1735‚Äì1780, 1997.
[17] K. Cho, B. Van Merri ¬®enboer, D. Bahdanau, and Y . Bengio, ‚ÄúOn the proper-
ties of neural machine translation: Encoder-decoder approaches,‚Äù arXiv preprint
arXiv:1409.1259 , 2014.
[18] P. J. Werbos, ‚ÄúBackpropagation through time: what it does and how to do it,‚Äù
Proceedings of the IEEE , vol. 78, no. 10, pp. 1550‚Äì1560, 1990.
[19] S. Hihi and Y . Bengio, ‚ÄúHierarchical recurrent neural networks for long-term de-
pendencies,‚Äù Advances in neural information processing systems , vol. 8, 1995.
[20] M. Schuster and K. K. Paliwal, ‚ÄúBidirectional recurrent neural networks,‚Äù IEEE
transactions on Signal Processing , vol. 45, no. 11, pp. 2673‚Äì2681, 1997.
[21] L. Deng, ‚ÄúThe mnist database of handwritten digit images for machine learning
research,‚Äù IEEE Signal Processing Magazine , vol. 29, no. 6, pp. 141‚Äì142, 2012.
[22] A. Krizhevsky, ‚ÄúLearning multiple layers of features from tiny images,‚Äù tech. rep.,
2009.
[23] J. Howard, ‚ÄúImagewang,‚Äù https://github.com/ fastai/imagenette/ , 2019.
[24] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ‚ÄúImageNet: A Large-
Scale Hierarchical Image Database,‚Äù in CVPR09 , 2009.
[25] J. Mellor, J. Turner, A. Storkey, and E. J. Crowley, ‚ÄúNeural architecture search
without training,‚Äù in International Conference on Machine Learning , pp. 7588‚Äì
7598, PMLR, 2021.
[26] S. Wilson, N. S ¬®underhauf, and F. Dayoub, ‚ÄúHyperdimensional feature fusion for
out-of-distribution detection,‚Äù arXiv preprint arXiv:2112.05341 , 2021.
15

--- PAGE 16 ---
[27] A. Lamb, A. Goyal, Y . Zhang, S. Zhang, A. Courville, and Y . Bengio, ‚ÄúProfessor
forcing: A new algorithm for training recurrent networks,‚Äù Advances in neural
information processing systems , vol. 29, 2016.
[28] R. G. Andrzejak, K. Lehnertz, F. Mormann, C. Rieke, P. David, and C. E. El-
ger, ‚ÄúIndications of nonlinear deterministic and Ô¨Ånite-dimensional structures in
time series of brain electrical activity: Dependence on recording region and brain
state,‚Äù Physical Review E , vol. 64, no. 6, p. 061907, 2001.
[29] K. Soomro, A. R. Zamir, and M. Shah, ‚ÄúUcf101: A dataset of 101 human actions
classes from videos in the wild,‚Äù arXiv preprint arXiv:1212.0402 , 2012.
[30] A. Go, R. Bhayani, and L. Huang, ‚ÄúTwitter sentiment classiÔ¨Åcation using distant
supervision,‚Äù CS224N project report, Stanford , vol. 1, no. 12, p. 2009, 2009.
[31] T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsch, and A. Joulin, ‚ÄúAdvances in
pre-training distributed word representations,‚Äù arXiv preprint arXiv:1712.09405 ,
2017.
16

--- PAGE 17 ---
A CNN Scaling Studies
To study the performance and feasibility for scaling to deeper models, we perform a
basic grid search over different network conÔ¨Ågurations, and study the performance of
these using tpSGD and Adam on CIFAR. These results are summarized in Figure 4.
The left diagrams, or heat maps, correspond to the results obtained using tpSGD while
the results from Adam are shown on the right. The top row displays the average model
performance over 25 random restarts while the bottom row shows the average training
time.
Moving along the x-axis of any of the heat maps corresponds to adding a Conv2D
layer with kernel size 5, stride 1 and leaky ReLU activation at the beginning of the
network. Moving up the y-axis corresponds to adding Linear layers with 256 hidden
nodes and leaky ReLU activation before the Ô¨Ånal classiÔ¨Åcation layer. As such the small-
est model shown (bottom left) has a single trainable layer, and the largest has 10.
Appendix A shows a detailed scaling study comparing models trained with tpSGD
as compared to backpropagation. These results show that the proposed Ô¨Ålter-based sam-
pling approach for generating random projections is close to capturing the expressive
power of CNNs trained using Adam. In particular for Number-of-hidden-layers (Nhid-
den) = 0, the scaling as a function of nconv is within 3-5% to the baseline across
the networks studied. However, looking at the upper right corner, at the deepest net-
works, the difference in performance is much larger (10-15%), as adding Linear layers
in tpSGD is providing less of an improvement relative to the Adam baseline. We will
discuss ideas of better random projections for linear layers in the future work section.
B VGG16 Adaptor ConÔ¨Åguration
This section discusses brieÔ¨Çy the adaptor deÔ¨Ånition and related studies used to ben-
chamrk tpSGD against BP in transfer learning or domain adaptation setting. Diagram 5
schematically illustrates the procedure. The backbone on the left is the feature extractor
portion of VGG16, and the red arrows illustrate ‚Äùtap‚Äù points where we extract inter-
mediate representations, which can then be used to train models on the classiÔ¨Åcation
task.
The heatmaps associated to each adaptor level Lishow the model performance
(left) and memory footprint as we increase the number of convolutional layers (x axis)
or hidden, dense layers (y axis). The Ô¨Ånal selected conÔ¨Åguration for each layer is shown
in green, consisting in each case of the following: Conv2D(Ô¨Ålters=64, kernel size=5,
strides=1), LeakyReLU, Dense(256), LeakyReLU, Dense(10).
17

--- PAGE 18 ---
Figure 4: Heatmap showing model performance (top) and training time (bottom) on
CIFAR as a function the number of hidden linear ( y-axis) and Conv2D ( x-axis) layers
using tpSGD (left) and Adam (right) benchmark. All hidden layers have 256 nodes,
while all Conv2Ds have kernel size=(5,5), Ô¨Ålters=16, stride=(1,1). All models are fol-
lowed by a Ô¨Ånal, fully connected layer mapping to the classiÔ¨Åcation space.
Figure 5: Schematic showing the naive adaptor search and relevant studies for VGG16.
The left represents the feature extractor and the rightmost heat maps show the model
performance and size for different conÔ¨Ågurations.
18

--- PAGE 19 ---
Figure 6: tpSGD adaptor performance on Imagenette using different adaptor conÔ¨Ågu-
rations, compared to the BP transfer learning baseline. The size of the trained models
is show in parenthesis.
Figure 6 shows a comparison between the performance of the different adaptors
(L0;L1;L2) and combinations of them, as well as the transfer learning baseline ob-
tained by Ô¨Ånetuning the Ô¨Ånal fully connected layers of VGG16 with BP. The results
with the single L0 adaptor are within 3% of the BP result, with a substantial decrease
in the memory footprint due to the selected network architecture. It is interesting to
note that for this speciÔ¨Åc dataset adding additional adaptors does not improve the per-
formance. We have experimented with other datasets domain-transferring from RGB to
the depth map using EfÔ¨ÅcientNet v2 at the edge. The optimized adaptors are selected
by NAS to be connected to the 2 middle layers (block 4 and 5) of the network and
surprisingly not to the last fully connected layer.
19

--- PAGE 20 ---
C Basic Unrolled Recurrent Cell
Figure 7: Formulation of the simple recurrent cell used in our system and its ‚Äúunrolled‚Äù
equivalent.
In Figure 7, we show a pictorial representation of the unrolled recurrent cell used to
formulate the basic tpSGD update rules for recurrent neural networks.
D RNN Dataset Properties
In this section we discuss in more detail the different datasets used to benchmark
tpSGD on time series data using RNNs. Table 6 shows the general properties of six
different benchmark datasets: Sequential MNIST (row- and pixel-wise), EEG seizure
signals, UCF 101 activity detection dataset, and the Twitter Sentiment Analysis dataset
(where input features are one-hot word vectors and wave2vec encodings). Note the
wide range in dataset characteristics. These datasets include a mix of modalities (im-
ages, language, 1-d signals), different sequence lengths, variance in the number of fea-
tures per sequence token, several different data types of the features with different
ranges of values and amount of sparsity, varying amounts of training data, and a wide
range in the number of labels for classiÔ¨Åcation. Altogether, this set of benchmarks
captures the ability of the proposed tpSGD method to generalize to diverse sequence
classiÔ¨Åcation problems.
E RNN Scaling Studies
As mentioned in the main experiments section, we demonstrate how tpSGD can scale
to more complex RNN architectures. We consider two such cases: multi-layer stacked
RNNs and bidirectional RNNs. We report full results in Tables 7 and 8, and we discuss
Ô¨Åndings in Section 3.3.
20

--- PAGE 21 ---
Dataset Modality Number of Number of Number of Sequence Number of Feature Sparsity Range
Samples (Training) Samples(Test) Classes Length Features Type of Features
Sequential MNIST Grayscale Images ‚Äù60,000‚Äù ‚Äù10,000‚Äù 10 28 28 Float 80.90% [0, 1]
Rows [27, 21]
Sequential MNIST Grayscale Images ‚Äù60,000‚Äù ‚Äù10,000‚Äù 10 49 1 Float 80.90% [0, 1]
Pixels [27, 21]
EEG Seizure 1-D EEG Signal ‚Äù3,450‚Äù ‚Äù1,150‚Äù 2 45 1 Int 0.00% [-1885, 2047]
Recognition [28]
UCF101 Action RGB Videos ‚Äù9,518‚Äù ‚Äù3,769‚Äù 101 10 2048 Float 41.20% [0, 15.9375]
Recognition [29]
Twitter Sentiment Analysis Natural Language ‚Äù75,000‚Äù ‚Äù25,000‚Äù 2 15 1000 Binary 99.90%f0, 1g
One-Hot Encoding [30]
Twitter Sentiment Analysis Natural Language ‚Äù75,000‚Äù ‚Äù25,000‚Äù 2 15 100 Float 54.30% [-3.35, 3.12]
Word2Vec Encoding [30, 31]
Table 6: Properties of the datasets used for testing the capabilities of the recurrent cell
Dataset Random Random RNN + Trained RNN (DRTP) Trained RNN (tpSGD `1) + Trained RNN (BP) +
Trained ClassiÔ¨Åer (tpSGD `2)ClassiÔ¨Åer (tpSGD `2) ClassiÔ¨Åer (tpSGD `2) ClassiÔ¨Åer (BP)
Seq. MNIST (rows) 10.32% (10%) 19.02% 83.16% 79.96% 92.59%
Seq. MNIST (pixels) 10.32% (10%) 13.66% 68.60% 70.01% 82.04%
EEG Seizure 51.57% (50%) 80.87% 80.78% 80.87% 93.75%
UCF101 1.22% (0.99%) 52.64% 59.80% 59.75% 63.39%
Twitter (one-hot) 50.08% (50%) 52.55% 67.04% 67.35% 69.94%
Twitter (Word2Vec) 50.08% (50%) 64.35% 70.24% 68.48% 72.84%
Table 7: Results of training a two-layer stacked RNN with two-layer perceptron clas-
siÔ¨Åer. Numbers in parentheses are true random chance. We show the algorithm used to
train each layer in parentheses where BP is traditional backpropagation.
Dataset Random Random RNN + Trained RNN (DRTP) Trained RNN (tpSGD `1) + Trained RNN (BP) +
Trained ClassiÔ¨Åer (tpSGD `2)ClassiÔ¨Åer (tpSGD `2) ClassiÔ¨Åer (tpSGD `2) ClassiÔ¨Åer (BP)
Seq. MNIST (rows) 11.35% (10%) 10.70% 83.13% 79.05% 92.10%
Seq. MNIST (pixels) 11.35% (10%) 10.32% 67.19% 68.01% 80.95%
EEG Seizure 48.52% (50%) 85.04% 84.09% 88.61% 90.53%
UCF101 1.01% (0.99%) 56.78% 61.37% 61.63% 64.25%
Twitter (one-hot) 50.08% (50%) 51.07% 65.00% 66.41% 71.65%
Twitter (Word2Vec) 50.08% (50%) 65.76% 70.31% 68.96% 69.74%
Table 8: Results of training a single-layer Bidirectional RNN with linear classiÔ¨Åer.
Numbers in parentheses are true random chance. We show the algorithm used to train
each layer in parentheses where BP is traditional backpropagation.
21
