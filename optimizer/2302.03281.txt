# 2302.03281.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/optimizer/2302.03281.pdf
# File size: 7960860 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Utility-based Perturbed Gradient Descent: An Optimizer for Continual Learning
Mohamed Elsayed1 2A. Rupam Mahmood1 2 3
Abstract
Modern representation learning methods often
struggle to adapt quickly under non-stationarity
because they suffer from catastrophic forgetting
and decaying plasticity. Such problems prevent
learners from fast adaptation since they may for-
get useful features or have difÔ¨Åculty learning new
ones. Hence, these methods are rendered inef-
fective for continual learning. This paper pro-
poses Utility-based Perturbed Gradient Descent
(UPGD), an online learning algorithm well-suited
for continual learning agents. UPGD protects
useful weights or features from forgetting and
perturbs less useful ones based on their utilities.
Our empirical results show that UPGD helps re-
duce forgetting and maintain plasticity, enabling
modern representation learning methods to work
effectively in continual learning.
1. Introduction
Learning online is crucial for systems that need to con-
tinually adapt to an ever-changing world. These learners
perform updates as soon as the data arrives, allowing for
fast adaptation. Such lifelong learners never stop learning
and can run indeÔ¨Ånitely. Thus, their required computation
and memory should not grow when presented with more
experiences. Moreover, it is desired if the learners are com-
putationally cheap and maintain a small memory footprint,
a criterion which we refer to as computational efÔ¨Åciency .
Therefore, desirable criteria for continual learning are fast
online adaptation and computational efÔ¨Åciency.
The current gradient-descent methods prevent fast online
adaption of continual learning systems. Such methods typi-
cally suffer from two main problems that slow down adap-
tation: decaying plasticity (Dohare et al. 2021) and catas-
trophic forgetting (McCloskey & Cohen 1989). Decaying
Preprint. Work in Progress.1Department of Computing Science,
University of Alberta, Edmonton, Canada2Alberta Machine In-
telligence Institute (Amii)3CIFAR AI Chair. Correspondence to:
Mohamed Elsayed <mohamedelsayed@ualberta.ca >, A. Rupam
Mahmood <armahmood@ualberta.ca >.plasticity arises when the learner‚Äôs ability to learn repre-
sentations is hindered (e.g., when the number of saturated
features increases, resulting in small gradients that obstruct
fast changes to the weights). Catastrophic forgetting can
be partly viewed as a result of shared representations, in-
stead of sparse representations, in function approximation
(French 1991, Liu et al. 2019). The weight updates are
non-local‚Äîchanges many weights‚Äîin the case of dense
representations because many features are active for any
given input, interfering with previously learned representa-
tions and potentially causing forgetting. Additionally, catas-
trophic forgetting is exacerbated by the inability of current
gradient-descent methods to reuse previously learned useful
features or protect them from change. It has been shown
that current methods may destroy the most useful features
under non-stationarity (Sutton 1986), making these methods
de- and re-learn the features when the learners face similar
or the same situations again.
Naturally, decaying plasticity and catastrophic forgetting
co-occur in neural networks since neural networks use dense
representations learned by gradient-based methods, and it
might be hard to quickly change the functions they repre-
sent. Dense representation creates non-locality, resulting in
forgetting, and many factors, such as vanishing gradients or
correlated features, contribute to decaying plasticity.
Different approaches have been proposed to mitigate catas-
trophic forgetting. Typically, such methods are either replay-
based methods (e.g., Chaudhry et al. 2019, Isele & Cosgun
2018, Rolnick et al. 2019), regularization-based (e.g., Kirk-
patrick et al. 2017, Aljundi et al. 2018, Aljundi et al. 2019),
sparsity-inducing (e.g., Liu et al. 2019, Pan et al. 2021), or
use dynamic architectures (e.g., Rusu et al. 2016, Schwarz et
al. 2018). However, most methods are not able to satisfy our
criteria of efÔ¨Åciency or fast online adaptation. Additionally,
all methods addressing catastrophic forgetting still suffer
from decaying plasticity, which can be mitigated by contin-
ual injection of noise (e.g., Ash & Adams 2020, Zhou et al.
2019, Dohare et al. 2021, Orvieto et al. 2022). However,
such methods may still suffer from catastrophic forgetting.
Both problems of continual learning can be identiÔ¨Åed with
the fact that gradient-descent methods are indifferent to how
useful a feature is. When a feature becomes difÔ¨Åcult to mod-
ify, contributing to decaying plasticity, the problem couldarXiv:2302.03281v2  [cs.LG]  27 Apr 2023

--- PAGE 2 ---
Utility-based Perturbed Gradient Descent
be overcome by resetting or re-initializing that feature. Con-
versely, when a feature is useful and well-contributing to a
task, it could be protected from further change and catas-
trophic forgetting. However, no existing gradient-descent
methods contain such capabilities.
In this paper, we present Utility-based Perturbed Gradient
Descent (UPGD), a novel online learning mechanism that
protects useful weights or features and perturbs less useful
ones based on their utilities. UPGD reuses useful features
and builds new features based on existing ones, allowing fast
adaptation. Moreover, UPGD estimates the utility of each
weight or feature using linear computational complexity
like SGD without storing past samples, making our method
computationally scalable. Additionally, UPGD does not
require the knowledge of task boundaries, making it a task-
free continual learning method (Aljundi et al. 2019).
The learning rule of UPGD contains two components help-
ing in fast online adaptation: search and gradient. Each
component is weighted by the utility estimates of the corre-
sponding weights or features. The utility-informed search
component helps Ô¨Ånd better weights and features through
continual perturbation, which helps against decaying plas-
ticity. The utility-based gradient is a modulated gradient
signal‚Äîsmall for useful weights and large for less useful
weights‚Äîthat helps against catastrophic forgetting.
2. Problem Formulation
We use the online continual supervised learning setting
where there is a stream of data examples. These data exam-
ples are generated from some non-stationary target function
ftmapping the input to the output, where the input-output
pair at timetis(xt;yt). The learner is required to predict
the output given the input vector xt2Rdby estimating
the target function ft. We consider the case where ftis
not arbitrarily non-stationary but rather locally stationary in
time (e.g., slowly changing or can be divided into stationary
tasks). We further consider target functions with regularities
that can potentially reoccur. The performance is measured
with some loss,L(yt;^yt), whereyt2Rmis the target vec-
tor and ^yt2Rmis the predicted output. SpeciÔ¨Åcally, the
mean squared error is used in regression, and cross-entropy
is used in classiÔ¨Åcation. The learner is required to reduce
the loss by matching the target. The performance of the
learner is measured based on the average online evaluation
metricEover all time steps, which is given by
E(T) =1
TTX
t=1E(yt;^yt); (1)
whereEis the sample evaluation metric (e.g., accuracy or
loss), andTis the total number of time steps. We note here
that this problem formulation has been introduced beforeby Caccia et al. (2020), where the performance is measured
based on the online evaluation metric; however, it was as-
sumed that we start with pre-trained learners. Here, we
consider the setting where learners learn from scratch. Note
that this online evaluation metric is similar to the cumulative
sum of rewards in reinforcement learning.
Consider a neural network with Llayers that outputs the
predicted output ^y. The neural network is parametrized by
the set of weights W=fW1;:::;WLg, whereWlis the
weight matrix at the l-th layer, and its element at the ith row
and thej-th column is denoted by Wl;i;j. During learning,
the parameters of the neural network are changed to reduce
the loss. At each layer l, we get the activation output hlby
applying the activation function to the activation input al:
hl=(al). We simplify notations by deÔ¨Åning h0:=x.
The activation output hlis then multiplied by the weight
matrixWl+1of layerl+ 1to produce the next activation
input:al+1;i=Pjhlj
j=1Wl+1;i;jhl;j. We assume here that
the activation function is element-wise activation for all
layers except for the Ô¨Ånal layer Lin classiÔ¨Åcation tasks,
where it becomes the softmax function.
2.1. Decaying Plasticity
Neural plasticity can be deÔ¨Åned as the ability of neural
networks to change in response to some stimulus (Konorski
1948, Hebb 1949). In artiÔ¨Åcial neural networks, we can think
of plasticity as the ability to change the function represented
by the neural network. Note that this deÔ¨Ånition is based
on the ability to change the function, not the underlying
weights. For example, a learner can update the weights in
a constant-initialized neural network. Still, its ability to
change the function is quite limited because of the feature
symmetries, although the weights can be changed. We note
here that the term plasticity is different from representation
power or capacity , which refers to the range of functions
that can be represented by some architecture. In this paper,
we use networks with the same capacity and show they lose
plasticity when used with typical gradient-based methods.
Our online evaluation metric favors methods that adapt
quickly to the changes since it is not a function of the Ô¨Ånal
performance only. A plastic learner can adapt more quickly
to changes than a less plastic one and therefore achieve
higher cumulative average performance. Such an evaluation
metric is natural to measure plasticity when the learner is
presented with sequential tasks that require little transfer be-
tween them. Input-permuted tasks (e.g., permuted MNIST)
satisfy this criterion since the representations learned in one
task are somewhat irrelevant to the next task. When cur-
rent gradient-based methods are presented with sequential
input-permuted tasks, their performance degrades. Such
degradation is partially explained by many hypotheses, such
as increased numbers of saturated features incapable of

--- PAGE 3 ---
Utility-based Perturbed Gradient Descent
fast adaptation (Dohare et al. 2021, Abbas et al. 2023) or
the changing loss landscape under non-stationarity, making
function adjustment harder (Lyle et al. 2023).
2.2. Catastrophic Forgetting
Catastrophic forgetting is the tendency of a neural network
to forget past information after learning from new experi-
ences. For example, when an agent learns two tasks sequen-
tially, A and B, its ability to remember A would degrade
catastrophically after learning B. We extend that deÔ¨Ånition
to be concerned about learned features instead of tasks.
Forgetting is underexamined in the online setting where
agents never stop learning. Our metric allows us to naturally
have the relearning-based metric (Hetherington & Seiden-
berg 1989) to measure forgetting by looking at how long it
takes to relearn a new task again. This is suitable for online
learning since we do not stop the agent from learning to
evaluate its performance ofÔ¨Çine, which is required by the
retention-based metric (McCloskey & Cohen 1989).
Separating catastrophic forgetting from decaying plasticity
is challenging since it is unclear how to create a network that
does not lose plasticity. Most works addressing forgetting
use networks that lose plasticity, which adds to performance
degradation coming from forgetting. We also show that even
linear neural networks (e.g., using identity activations) can
lose plasticity too, which makes the problem of separating
them more challenging.
Studying catastrophic forgetting can be done using a prob-
lem that requires transfer from one task to another. An
ideal learner should be able to utilize previously learned
useful features from one task to another. However, a typical
gradient-based method (e.g., SGD) keeps re-learning useful
features again and again because of forgetting, resulting in
performance degradation.
In this paper, we present a method to reduce forgetting and
maintain plasticity in a principled way by protecting useful
weights or features and removing less useful ones.
3. Method
Estimating the utility of weights and features can help pro-
tect useful weights and features and get rid of less useful
ones. The utility of a weight or a feature in a neural network
used by a continual online learner from time step tto time
stept+hcan be deÔ¨Åned as the averaged instantaneous utility
over these time steps, which can be written as follows:
Ut:t+h:=1
khX
k=0Ut+k:
Since continual online systems run indeÔ¨Ånitely, we are in-
terested in the averaged instantaneous utility over all futuretime steps starting from time step t, which is given by
Ut:= lim
k!1Ut:t+k:
However, the average of future utilities is unknown at time
stept. The learner can only compute the instantaneous
utilityUt. One can estimate such an unknown quantity by
averaging over the past instantaneous utilities (e.g., utility
traces), hoping that the past resembles the future. In our
analysis, we focus on computing the instantaneous utility,
but we use utility traces in our experiments.
The true instantaneous utility of a weight in a neural network
can be deÔ¨Åned as the change in the loss after removing
that weight (Mozer & Smolensky 1988, Karnin 1990). An
important weight, when removed, should increase the loss.
The utility matrix corresponding to l-th layer connections
isUl(Z), whereZ= (X;Y )denotes the sample. We
drop the time notation for ease of writing but emphasize
the instantaneity of utility by writing it as a function of the
sample. The true utility of the weight i;jin thel-th layer
for the sample Zis deÔ¨Åned as follows:
Ul;i;j(Z):=L(W:[l;i;j];Z) L(W;Z); (2)
whereL(W;Z)is the sample loss given the set of param-
etersW, andW:[l;i;j]is the same asWexcept the weight
Wl;i;jis set to 0.
Similarly, we deÔ¨Åne the true utility of a feature iat layer
l, which is represented as the change in the loss after the
feature is removed. The utility of the feature iin thel-th
layer is given by
ul;j(Z):=L(W;Zjhl;j= 0) L(W;Z); (3)
wherehl;j= 0denotes setting the activation of the feature
to zero (e.g., by adding a mask set to zero).
Note that both these utility measures are global measures,
and they provide a total ordering for weights and features
according to their importance. However, computing such a
true utility is prohibitive since it requires additional Nwor
Nfforward passes, where Nwis the total number of weights
andNfis the total number of features. Approximating the
true utility helps reduce the computation needed to compute
the utility such that no additional forward passes are needed.
3.1. Approximated Weight Utility
We approximate the true utility of weights by a second-
order Taylor approximation. To write the utility ul;i;jof the
connectionijat layerl, we expand the true utility around the
current weight of the connection ijat layerland evaluate
it at the value of that weight being zero. The quadratic

--- PAGE 4 ---
Utility-based Perturbed Gradient Descent
approximation of Ul;i;j(Z)can be written as
Ul;i;j(Z) =L(W:[l;i;j];Z) L(W;Z)
L(W;Z) +@L(W;Z)
@Wl;i;j(0 Wl;i;j)
+1
2@2L
@W2
l;ij(0 Wl;ij)2 L(W;Z)
= @L(W;Z)
@Wl;i;jWl;i;j+1
2@2L(W;Z)
@W2
l;ijW2
l;ij:
We refer to the utility measure containing the Ô¨Årst term as
theÔ¨Årst-order approximated weight utility , and the utility
measure containing both terms as the second-order approx-
imated weight utility . The computation required for the
second-order term has quadratic complexity. Therefore, we
further approximate it using HesScale (Elsayed & Mahmood
2022; see Appendix E), making the computation of both of
these approximations have linear complexity. Moreover, we
present a way for propagating our approximated utilities by
theutility propagation theorem in Appendix A.
3.2. Approximated Feature Utility
Here, we derive the global utility of a feature jat layer
l, which is represented as the difference between the loss
when the feature is removed and the original loss. To have
an easier derivation, we add a mask (or gate) on top of the
activation output: hl=mlhl. Note that the weights of
such masks are set to ones and never change throughout
learning. The quadratic approximation of ul;j(Z)can be
written as
ul;j(Z) =L(W;Zjml;j= 0) L(W;Z)
L(W;Z) +@L
@ml;i(0 ml;j)
+1
2@2L
@m2
l;i(0 ml;j)2 L(W;Z)
= @L
@ml;i+1
2@2L
@m2
l;i:
We further approximate it using HesScale, making the com-
putation have linear computational complexity.
When we use an origin-passing activation function, we can
instead expand the loss around the current activation input
al;iwithout the need to use a mask. Note that derivatives
with respect to the activation inputs are available from back-
propagation. The feature utility can be simply:
ul;j(Z) =L(W;Zjal;j= 0) L(W;Z)
 @L
@al;ial;i+1
2@2L
@a2
l;ia2
l;i:Moreover, the approximated feature utility can be computed
using the approximated weight utility, which gives rise to the
conservation of utility property. We show such a relationship
and its proof in Appendix B.
3.3. Utility-based Perturbed Search
Searching in the weight space or the feature space help Ô¨Ånd
better weights and features. We devise a novel learning rule
based on utility-informed search. SpeciÔ¨Åcally, we protect
useful weights or features and perturb less useful ones. We
show the rule of Utility-based Perturbed Search (UPS) to
update the weights. We emphasize here that utility-based
perturbed search does not move in the same direction as
the gradient; hence, it is not a gradient-descent update. The
update rule of UPS is deÔ¨Åned as follows:
wl;i;j wl;i;j (1 Ul;i;j); (4)
whereis a noise sample, is the step size, and Ul;i;jis
a scaled utility with a minimum of 0and a maximum of 1.
Such scaling helps protect useful weights and perturb less
useful ones. For example, a weight with Ul;i;j= 1 is not
perturbed, whereas a weight with Ul;i;j= 0is perturbed by
the whole noise sample. The noise type and amount play
an important role in search. Moreover, search difÔ¨Åculty is
a function of the search space size, meaning that we can
expect searching in the feature space is easier than in the
weight space. We can retrieve the weight-wise and feature-
wise UPS algorithms by dropping the gradient information
from the update equation in Algorithm 1 and Algorithm 2.
UPS learning rule resembles an evolutionary process on the
weights or the features where each time step resembles a
new population of weights or features, and the scaled utility
resembles the Ô¨Åtness function that either keeps the useful
(Ô¨Åttest) weights or features or perturbs the less useful ones.
3.4. Utility-based Perturbed Gradient Descent
Our aim is to write an update equation to protect useful
weights and replace less useful ones using search and gra-
dients. We show here the rule of Utility-based Perturbed
Gradient Descent (UPGD) to update the weights. The up-
date equation is given by
wl;i;j wl;i;j @L
@wl;i;j+ 
1 Ul;i;j
:(5)
The utility information in UPGD works as a gate for
perturbed gradients. For important weights with utility
Ul;i;j= 1, the weight is not updated, whereas unimpor-
tant weights with utility Ul;i;j= 0get updated by the whole
perturbed gradient information. We note here that UPGD is
able to introduce plasticity by perturbing less useful weights
and reduce forgetting by protecting useful weights.

--- PAGE 5 ---
Utility-based Perturbed Gradient Descent
Another variation of UPGD, we call non-protecting UPGD ,
is to add the utility-based perturbation to the gradient as:
wl;i;j wl;i;j @L
@wl;i;j+(1 Ul;i;j)
:(6)
However, such an update rule can only help against decaying
plasticity, not catastrophic forgetting. This is because useful
weights are not protected from change by gradients.
Utility scaling is important for the UPGD and UPS up-
date equations. We devise two kinds of scaling: global
and local. The global scaled utility requires the maximum
utility of all weights or features at every time step. The
scaled utility for the global-utility UPGD and UPS is given
byUl;i;j=(Ul;i;j=)for weights, where is the maxi-
mum utility of the weights and is the scaling function
(e.g., Sigmoid). For feature-wise UPGD and UPS, the
scaled utility is given by ul;j=(ul;j=). We show the
pseudo-code of our method using the global scaled utility
in Algorithm 1 for weight-wise UPGD and in Algorithm
2 for feature-wise UPGD. The local UPGD and UPS do
not require the global max operation since it normalizes
the outgoing weight vector for each feature. SpeciÔ¨Åcally,
the scaled utility is given by Ul;i;j= 
Ul;i;j=qP
jU2
l;i;j
for weight-wise UPGD and UPS. For features, the scaled
utility is given by ul;j= 
ul;j=qP
ju2
l;j
. We show the
pseudo-code of our method using the local scaled utility in
Algorithm 3 for weight-wise UPGD and in Algorithm 4 for
feature-wise UPGD.
3.5. Perturbed Gradient Descent Methods
One can perturb all weights evenly, which can give rise to a
well-known class of algorithms called Perturbed Gradient
Descent (PGD). The learning rule of PGD is given by:
wl;i;j wl;i;j @L
@wl;i;j+
: (7)
whereis a noise perturbation that can be uncorrelated,
giving PGD (Zhou et al. 2019), or anti-uncorrelated, giving
Anti PGD (Orvieto et al. 2022). Anti-correlated perturbation
at timet+ 1 is given by t+1=t+1 t, wheret+1
andtare perturbations sampled from N(0;1), whereas
uncorrelated perturbation at time t+ 1is given byt+1=
t+1. In this paper, we care about online methods, so we
only consider stochastic versions of these methods. When
all scaled utilities are zeros in UPGD (see Eq. 5 and Eq. 6),
UPGD reduces to PGD methods.
Many works have shown the role of noise in improving gen-
eralization and avoiding bad minima (e.g., PGD, Anti-PGD)
in stationary problems. A stochastic version of perturbed
gradient descent (Neelakantan et al. 2015) helps escape bad
minima and improve performance and generalization.It has not been shown before that these PGD methods can
help improve plasticity; however, recent work by Dohare et
al. (2021) showed that noise injection by resetting features
helps maintain plasticity. We hypothesize that stochastic
perturbed gradient descent methods might help too.
On the other hand, it has been shown that a stochastic PGD
with a weight decay algorithm, known as Shrink and Per-
turb (Ash & Adams 2020), can help maintain plasticity in
continual classiÔ¨Åcation problems (Dohare et al. 2022). The
learning rule of shrink and perturb can be written as
wl;i;j wl;i;j @L
@wl;i;j+
: (8)
where= 1 andis the weight decay factor. When
no noise is added, the update reduces to SGD with weight
decay (Loshchilov & Hutter 2019), known as SGDW.
3.6. Utility-based Perturbed Gradient Descent with
Weight Decay
Combining weight decay with our UPGD learning rules,
one can write the UPGD with weight decay as follows:
wl;i;j wl;i;j @L
@wl;i;j+ 
1 Ul;i;j
:(9)
Similarly, the non-protecting UPGD learning rule with
weight decay can be written as
wl;i;j wl;i;j @L
@wl;i;j+(1 Ul;i;j)
:(10)
4. Experiments
In this section, we design and perform a series of experi-
ments to estimate the quality of our approximated weight or
feature utilities. In addition, we showcase the effectiveness
of searching in weight or feature space. We then evaluate
weight-wise and feature-wise UPGD in mitigating decay-
ing plasticity and catastrophic forgetting issues using non-
stationary problems based on MNIST (LeCun et al. 1998),
EMNIST (Cohen et al. 2017), and CIFAR-10 (Krizhevsky
2009) datasets.
Although UPS and UPGD can be used in principle in the
ofÔ¨Çine learning setting, the focus of our paper is continual
online learning. The performance of continual learners
is evaluated using the metric in Eq. 1, where we use the
average online loss for regression tasks and average online
accuracy for classiÔ¨Åcation tasks.
In each of the following experiments, a hyperparameter
search is conducted. Our criterion was to Ô¨Ånd the best set
of hyperparameters for each method in that search space

--- PAGE 6 ---
Utility-based Perturbed Gradient Descent
Algorithm 1 Weight-wise UPGD with global scaled utility
Require: Neural network fwith weightsfW1;:::;WLg
and a stream of data D.
Require: Step sizeand decay rate 2[0;1).
Require: InitializefW1;:::;WLg
Require: Initialize time step t 0;  1
forlinfL;L 1;:::;1gdo
Ul 0;^Ul 0
end for
for(x;y)inDdo
t t+ 1
forlinfL;L 1;:::;1gdo
Fl;Sl GetWeightDerivatives (f;x;y;l)
Ml 1=2SlW2
l FlW
Ul Ul+ (1 )Ml
^Ul Ul=(1 t)
if<max(^Ul)then
 max(^Ul)
end if
end for
forlinfL;L 1;:::;1gdo
Sample noise matrix 
Ul (^Ul=)
Wl Wl (Fl+)(1 Ul)
end for
end for
which minimizes the area under the loss curve (in regression
tasks) or maximizes the area under the accuracy curve (in
classiÔ¨Åcation tasks). Unless stated otherwise, we averaged
the performance of each method over 20independent runs.
4.1. Quality of Approximated Utility Ordering
A high-quality approximation should give a similar order-
ing of weights or features. We use the ordinal correlation
measure of Spearman to quantify the quality of our utility ap-
proximations. We start by showing the correlation between
different approximated weight utilities and the true utility.
A network of a single hidden layer containing 50units is
used. The network has Ô¨Åve inputs and a single output. The
target of an input vector is the sum of two inputs out of
the Ô¨Åve inputs. The inputs are sampled from U[ 0:5;0:5].
The weights are initialized by Kaiming initialization (He et
al. 2015). SGD is used to optimize the parameters of the
network to minimize the mean squared error with a total of
2000 samples. At each time step, the Spearman correlation
metric is calculated for Ô¨Årst-order and second-order approxi-
mated global weight utility against the random utility and the
weight-magnitude utility. The Spearman correlation is mea-
sured per sample based on 550+50+50+1 = 351 items
coming from weights and biases. We report the global cor-
relations between the true utility and approximated weightutilities in Fig. 1(a) and defer the local correlations to Ap-
pendix F.3. We use ReLU (Nair & Hinton 2010) activations
here and report the results with Tanh and LeakyReLU (Maas
et al. 2013) in Appendix F.3.
The correlation is the highest for the second-order approx-
imated utility during learning up to convergence. On the
other hand, the Ô¨Årst-order approximated utility becomes less
correlated when the learner converges since the gradients
become very small. The weight-magnitude utility shows
a small correlation to the true utility. We use the random
utility as a reference which maintains zero correlation with
the true utility, as expected.
0 500 1000 1500 2000
Number of Samples0.00.10.20.30.4Spearman's Coefficient
(a) Weight-wise
0 500 1000 1500 2000
Number of Samples0.00.10.20.30.40.50.6Spearman's Coefficient (b) Feature-wise
Figure 1. Spearman rank correlation between the true utility and
global approximated utilities.
We repeat the experiment to compute the approximated
global feature utilities against the true utility and report the
results in Fig. 1(b). The network used has two hidden layers,
each containing 50 units. The Spearman correlation is mea-
sured at each sample based on 502 = 100 items coming
from features. We use ReLU activations here, and we report
the results with Tanh and LeakyReLU in Appendix F.3 in
addition to the local correlations. The results are similar to
the weight-wise results. However, the approximated Ô¨Årst-
order and second-order both have a low correlation. This
result suggests that second-order Taylor‚Äôs approximation
may not be sufÔ¨Åcient to approximate the true feature utility.
For that reason, We can expect that the weight-wise UPGD
methods would perform better than the feature-wise ones.
4.2. Utility-based Search Performance on MNIST
In this experiment, we evaluate our utility-based search
method in minimizing a loss using utility-informed search.
We use MNIST as our stationary task. The learner is trained
online for a 1M time step where only one input-output pair
is presented at each time step. A network of two hidden
layers, the Ô¨Årst has 300 units, and the second has 150 units
is used. The weights are initialized by Kaiming initializa-
tion. We report the results in Fig. 2 with ReLU activations.
The type of noise used controls the performance of search
optimizers. We noticed that the anti-correlated noise helps
in Ô¨Årst-order and second-order approximated utilities more

--- PAGE 7 ---
Utility-based Perturbed Gradient Descent
than uncorrelated noise (shown in Appendix F.1). The re-
sults show the effectiveness of utility-based search in reduc-
ing the loss against the baseline that perturbs all weights
or features evenly (e.g., uses random utility). We note that
utility-informed search alone, although being able to reduce
the loss, is not as effective as using gradient information.
When UPGD is used with MNIST, we notice an improve-
ment in performance compared to SGD (see Appendix F.2).
0.0 0.2 0.4 0.6 0.8 1.0
Number of Samples1e60.10.20.30.40.50.60.70.80.9Accuracy
(a) Weight-wise
0.0 0.2 0.4 0.6 0.8 1.0
Number of Samples1e60.10.20.30.40.50.60.70.80.9Accuracy (b) Feature-wise
Global Utility Local Utility Random Utility
Figure 2. Performance of Utility-based Search on MNIST with
Ô¨Årst-order global and local utilities against a random utility.
4.3. UPGD Performance on Nonstationary Toy
Problem
We present a simple regression problem for which the target
at timetis given byyt=aP
i2Sxt;i, wherext;iis theith
entry of input vector at time t,Sis the input set, and a2R.
We introduce non-stationarity using two ways: changing the
multiplieraor changing the input set S. In this problem,
the task is to add two inputs out of 16 inputs. The learner
is required to match the targets by minimizing the mean-
squared error. The learner uses a multi-layer linear network
that has two hidden layers containing 300and150units,
respectively. The network is linear since the activation used
is the identity activation ( (x) =x). Here, we show the
results and give the experimental details in Appendix D.1.
0 1000 2000 3000 4000 5000
T ask Number0.060.080.100.120.140.160.180.200.22Loss
(a) Weight-wise
0 1000 2000 3000 4000 5000
T ask Number0.060.080.100.120.140.160.180.200.22Loss (b) Feature-wise
SGD
PGDShrink and Perturb
UPGDNon-protecting UPGD
Reference (Linear Layer)
Figure 3. Performance of UPGD on the toy problem with a chang-
ing input set using the Ô¨Årst-order approximated utility against SGD,
PGD, and Shrink & Perturb.
In the Ô¨Årst variation of the problem, the input set Shas a size
of two, but the elements change every 200time steps by a
shift of two in the input indices. For example, if the Ô¨Årst taskhasS=f1;2g, the next would be f3;4gand so on. Since
the tasks have little transfer between them, we expect the
continual learners to learn as quickly as possible and main-
tain their plasticity. We compare UPGD against SGD, PGD,
Shrink & Perturb, and Non-protecting UPGD. Moreover, we
use a baseline that has one linear layer mapping the input to
the output. Note that we use Ô¨Årst-order approximated utility
here and defer the results with the second-order one in Ap-
pendix F.4. We report the results of this experiment in Fig.
3(a) for weight-wise UPGD and in Fig. 3(b) for feature-wise
UPGD. The performance of SGD degrades with changing
targets, indicating SGD loses plasticity every time the tar-
gets change. This may suggest that the outgoing weights to
some features get smaller, hindering the ability to change
the features‚Äô input weights. On the other hand, Shrink &
Perturb can maintain some plasticity over the linear-layer
baseline. PGD and Non-protecting UPGD perform better
than Shrink & Perturb, indicating that weight decay is not
helpful in this problem, and it is better to just inject noise
without shrinking the parameters. UPGD is able to maintain
its plasticity. Moreover, the performance keeps improving
with changing targets compared to other methods.
0 1000 2000 3000 4000 5000
T ask Number0.10.20.30.40.5Loss
(a) Weight-wise
0 1000 2000 3000 4000 5000
T ask Number0.10.20.30.40.5Loss (b) Feature-wise
SGD
PGDShrink and Perturb
UPGDNon-protecting UPGD
Reference (Linear Layer)
Figure 4. Performance of UPGD on the toy problem with changing
outputs using the Ô¨Årst-order approximated utility against SGD,
PGD, and Shrink & Perturb.
In the second variation, the sign of the target sum is Ô¨Çipped
every 200time steps by changing afrom 1to 1and vice
versa. We expect continual learning agents to learn some
features during the Ô¨Årst 200steps. After the target sign
change, we expect the learner to change the sign of only the
output weights since the learned features should be the same.
The frequency of changing ais high to punish learners for re-
learning features from scratch. Note that we use Ô¨Årst-order
approximated utility here and defer the results with second-
order approximated utility in Appendix F.4. We report the
results of this experiment in Fig. 4(a) for weight-wise UPGD
and in Fig. 4(b) for feature-wise UPGD. The performance
of SGD degrades with changing targets, indicating that it
does not utilize learned features and re-learn them every
time the targets change. Shrink & Perturb, Non-protecting
UPGD, and PGD maintain plasticity, but they are not able
to protect useful weights; therefore, their performance is

--- PAGE 8 ---
Utility-based Perturbed Gradient Descent
worse than UPGD. UPGD is able to protect useful weights
or features and utilize them each time the targets change.
Moreover, the performance keeps improving with changing
targets compared to the other methods.
4.4. UPGD Performance on Input-Permuted MNIST
We can study plasticity when the learner is presented with
sequential tasks that require little transfer between them.
Input-permuted MNIST satisÔ¨Åes this criterion since the rep-
resentations learned in one task are not relevant to the other
tasks. We permute the inputs every 5000 time steps and
present the learners with 1million examples, one example
per time step. The learner is required to maximize online
accuracy by matching the target. The learner uses a multi-
layer linear network with ReLU activations that has two
hidden layers containing 300and150units, respectively.
0 50 100 150 200
Task Number0.6000.6250.6500.6750.7000.7250.7500.775Accuracy
SGDW
PGDShrink and Perturb
UPGD-WNon-protecting UPGD-W
AdamW
Figure 5. Performance of UPGD, SGD, and Adam with weight
decay, PGD and Shrink & Perturb on Input-Permuted MNIST. A
global Ô¨Årst-order utility is used with the two UPGDs.
Dohare et al. (2022) have shown that weight decay helps
in input-permuted MNIST and suggested that weight decay
promotes plastic weights since maintaining small weights
prevents weights from overcommitting, making them easy
to change. Therefore, we compare SGD with weight de-
cay (SGDW), Shrink & Perturb, Adam with weight decay
(Loshchilov & Hutter 2019) known as (AdamW), UPGD
with weight decay (UPGD-W) and Non-protecting UPGD-
W. We note here that we do not compare with the method of
Continual Backprop (2021), which maintain plasticity since
it requires a complex learning mechanism, not a simple up-
date rule. Additionally, Continual Backprop has a similar
performance to Shrink & Perturb (Dohare et al. 2022). Fig.
5 shows the average online accuracy with the task number.
All methods are tuned extensively (shown in Appendix D.3).
When more tasks are presented, the online accuracy of PGD,
SGDW, and AdamW degrades with time. Only UPGD-W,
Non-protecting UPGD-W, and Shrink & Perturb could main-
tain plasticity. We also note that UPGD-W has slightly better
performance than Non-protecting UPGD-W and Shrink &
Perturb. In Appendix G, we present an ablation study forthe effect of weight decay and perturbation on performance
in Shrink & Perturb, UPGD-W, and Non-protecting UPGD-
W. We repeat this experiment for feature-wise UPGD in
Appendix F.5.
4.5. UPGD Performance on Output-permuted
EMNIST
Here, we study the interplay of catastrophic forgetting and
decaying plasticity with Output-permuted EMNIST. This
adds one level of complexity on top of the decaying plastic-
ity shown in the previous section. The EMNIST dataset is
an extended form of MNIST that has 47classes and has both
digits and letters. We chose EMNIST instead of MNIST
since Ouput-permuted MNIST was a very easy problem
such that all methods achieved very high accuracy. In our
Output-permuted EMNIST, the labels are permuted every
2500 time steps. Such a change should not make the agent
change its learned representations since it can simply change
the weights of the last layer to adapt to that change. This
makes the Output-permuted MNIST task suitable for study-
ing catastrophic forgetting and decaying plasticity.
0 100 200 300 400
Task Number0.10.20.30.40.50.60.7Accuracy
SGDW
PGDShrink and Perturb
UPGD-WNon-protecting UPGD-W
AdamW
Figure 6. Performance of UPGD, SGD, and Adam with weight
decay, PGD and Shrink & Perturb on Output-Permuted EMNIST.
A global Ô¨Årst-order utility is used with the two UPGDs.
We compare SGDW, Shrink & Perturb, AdamW, UPGD-W,
and Non-protecting UPGD-W. All methods are tuned exten-
sively (shown in Appendix D.3). Fig. 6 shows the average
online accuracy with the task number. When more tasks are
presented, the online accuracy of PGD, SGDW, and AdamW
degrades with time. Non-protecting UPGD-W and Shrink
& Perturb could maintain their plasticity, but they slowly
lose it as their online accuracy gradually goes down. In con-
trast, UPGD-W has signiÔ¨Åcantly better performance than
other algorithms. This suggests that UPGD-W maintains
plasticity and reduces forgetting such that at every task, it
can improve its representations. We found that UPGD-W
performs the best when it has no weight decay in this task.
In Appendix G, we present an ablation study for the effect of
weight decay and perturbation on performance. We repeat
this experiment for feature-wise UPGD in Appendix F.6.

--- PAGE 9 ---
Utility-based Perturbed Gradient Descent
4.6. UPGD Performance on Output-permuted
CIFAR10
Here, we study the interplay of catastrophic forgetting and
decaying plasticity with Output-permuted CIFAR-10. The
labels are permuted every 2500 time step. Such a change
should not make the agent change its learned representations
since it can simply change the weights of the last layer to
adapt to that change. In this problem, we make learners use
a network with two convolutional layers with max-pooling
followed by two fully connected layers with ReLU activa-
tions. Here, we show the results and give the experimental
details in Appendix D.4.
0 100 200 300 400
T ask Number0.10.20.30.40.50.6Accuracy
SGDW
PGDShrink and Perturb
UPGD-WNon-protecting UPGD-W
AdamW
Figure 7. Performance of UPGD, SGD, and Adam with weight
decay, PGD and Shrink & Perturb on Output-Permuted CIFAR-10.
A global Ô¨Årst-order utility is used with the two UPGDs.
We compare SGDW, Shrink & Perturb, AdamW, UPGD-W,
and Non-protecting UPGD-W. All methods are tuned exten-
sively (shown in Appendix D.4). Fig. 7 shows the average
online accuracy with the task number over 10independent
runs. When more tasks are presented, the online accuracy
of all methods improves. We suggest that the problem is
easier than Output-permuted EMNIST since the number of
classes in CIFAR-10 is less than EMNIST. A larger number
of classes means that the probability of a class not changing
after a permutation is signiÔ¨Åcantly less.
All methods maintain their plasticity. We notice that PGD
performs worse than other methods emphasizing the role
of weight decay in improving the performance in this task.
In contrast to the previous two tasks, we see that AdamW
performs better than SGDW. Similar to the previous tasks,
UPGD-W has signiÔ¨Åcantly better performance than other al-
gorithms. This suggests that UPGD-W maintains plasticity
and reduces forgetting such that at every task it can improve
its representations. We found that UPGD-W performs the
best when it has no weight decay in this task. In Appendix
G, we present an ablation study for the effect of weight
decay and perturbation on performance in Shrink & Perturb,
UPGD-W, and Non-protecting UPGD-W.5. Related Works
Neural network pruning requires a saliency or importance
metric to choose which weights to prune. Typically, after
training is complete, the network is pruned using measures
such as the weight magnitude (e.g., Han et al. 2015, Park
et al. 2020). Other metrics have been proposed using Ô¨Årst-
order information (e.g., Mozer & Smolensky 1988, Hassibi
& Stork 1992, Molchanov et al. 2016), second-order infor-
mation (e.g., LeCun et al. 1989, Dong et al. 2017), or both
(e.g., Tresp et al. 1996, Molchanov et al. 2019). Molchanov
et al. (2019) showed that the second-order Taylor‚Äôs approxi-
mation of some true utility, which requires superlinear com-
putation, closely matches the true utility. This result matches
our derivations and conclusions. However, we use an efÔ¨Å-
cient second-order Taylor‚Äôs approximation using HesScale
(Elsayed & Mahmood 2022), making our method computa-
tionally inexpensive. Finally, our utility propagation method
(derived in Appendix A) resembles, in principle, the ap-
proach pursued by Karnin (1990) and more recently by Yu
et al. (2018), which proposes a method to propagate the im-
portance scores from the Ô¨Ånal layer. However, our method
uses a utility based on second-order approximation instead
of reconstruction error or Ô¨Årst-order approximation.
Evolutionary strategy (Rudolph 1997, Rechenberg 1973)
is a derivative-free optimization class of methods that uses
ideas inspired by natural evolution. Typically, a popula-
tion of parameterized functions is created with different
initializations, which can be improved using some score of
success. Recent methods (e.g., Salimans et al. 2017, Such
et al. 2017) use evolutionary strategies to improve a collec-
tion of parameterized policies based on the return coming
from each one. The population is created by perturbing
the current policy, then each policy is run for an episode
to get a return. The new policy parameter update is the
average of these perturbed policies‚Äô parameters weighted by
their return corresponding returns. Evolutionary strategies,
in comparison to UPS, search in the solution space using
a population of solutions, making them infeasible beyond
simulation. UPS, however, uses search in the weight or
feature space, making UPS suitable for online learning.
Feature-wise UPS can be seen as a generalization of the
generate-and-test method (Mahmood & Sutton 2013) for
multi-layered networks with arbitrary objective functions.
The generate-and-test method works only with networks
with single-hidden layers in single-output regression prob-
lems and updates features in a conditional-selection fashion
(e.g., replaces a feature if its utility is lower than other utili-
ties). The feature utility is the trace of its outgoing weight
magnitude. However, it has been shown that weight magni-
tude is not suitable for other problems, such as classiÔ¨Åcation
(Elsayed 2022). On the contrary, feature-wise UPS uses
a better notion of utility that enables better search in the

--- PAGE 10 ---
Utility-based Perturbed Gradient Descent
feature space and works with arbitrary network structures
or objective functions.
Most continual learning methods use sequential tasks with
known boundaries. Such a notion is unrealistic to be met in
reality. There are a few task-free methods (e.g., Aljundi et al.
2019, Lee et al. 2020, He et al. 2019). However, the current
task-free methods are either progressive to accommodate
new experiences, require a replay buffer, or contain explicit
task inference components. In comparison, UPGD is scal-
able in memory and computation, making it well-suited for
lifelong agents that run for extended periods of time.
6. Conclusion
UPGD is a novel approach enabling learning agents to work
for extended periods of time, making it well-suited for con-
tinual learning. We devised utility-informed learning rules
that protect useful weights or features and perturb less useful
ones. Such update rules help mitigate the issues encountered
by modern representation learning methods in continual
learning, namely catastrophic forgetting and decaying plas-
ticity. We performed a series of experiments showing that
UPGD helps in maintaining network plasticity and reusing
previously learned features. Our results show that UPGD is
well-suited for continual learning where the agent requires
fast adaptation to an ever-changing world.
7. Broader Impact
Temporally correlated data cause catastrophic forgetting,
which makes reinforcement learning with function approx-
imation challenging and sample inefÔ¨Åcient (Fedus et al.
2020). The best-performing policy gradient methods (e.g.,
Mnih et al. 2015, Haarnoja et al. 2018) use a large replay
buffer to reduce the correlation in successive transitions and
make them seem independent and identically distributed to
the learner. Since UPGD helps against catastrophic forget-
ting without the need for buffers, it can potentially improve
the performance of most reinforcement learning algorithms
and ameliorate their scalability.
UPGD can be integrated with step-size adaptation methods
(e.g., Schraudolph 1999, Jacobsen et al. 2019), making them
work well under non-stationarity. Moreover, UPGD can help
better study and analyze step-size adaptation methods in
isolation of catastrophic forgetting and decaying plasticity.
Acknowledgement
We gratefully acknowledge funding from the Canada CI-
FAR AI Chairs program, the Reinforcement Learning and
ArtiÔ¨Åcial Intelligence (RLAI) laboratory, the Alberta Ma-
chine Intelligence Institute (Amii), and the Natural Sciences
and Engineering Research Council (NSERC) of Canada.Algorithm 2 Feature-wise UPGD with global scaled utility
Require: Neural network fwith weightsfW1;:::;WLg
and a stream of data D.
Require: Step sizeand decay rate 2[0;1).
Require: Initialize weightsfW1;:::;WLgrandomly and
masksfg1;:::;gL 1gto ones.
Require: Initialize time step t 0; 1 ;
forlinfL;L 1;:::;1gdo
ul 0;^ul 0
end for
for(x;y)inDdo
t t+ 1
forlinfL 1;:::;1gdo
fl;sl GetMaskDerivatives (f;x;y;l)
ml 1=2sl fl
ul ul+ (1 )ml
^ul ul=(1 t)
if<max(^ul)then
 max(^ul)
end if
end for
forlinfL;L 1;:::;1gdo
Fl;Sl GetWeightDerivatives (f;x;y;l)
ifl=Lthen
Wl Wl Fl
else
ul (^ul=)
Sample noise matrix 
Wl Wl (Fl+)(1 1u>
l)
end if
end for
end for
We would also like to thank Compute Canada for providing
the computational resources needed.
References
Abbas, Z., Zhao, R., Modayil, J., White, A., & Machado,
M. C. (2023). Loss of Plasticity in Continual Deep Rein-
forcement Learning. arXiv preprint arXiv:2303.07507 .
Ash, J., & Adams, R. P. (2020). On warm-starting neural
network training. Advances in Neural Information
Processing Systems ,33, 3884-3894.
Aljundi, R., Kelchtermans, K., & Tuytelaars, T. (2019).
Task-free continual learning. Conference on Computer
Vision and Pattern Recognition (pp. 11254-11263).
Aljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, M., &

--- PAGE 11 ---
Utility-based Perturbed Gradient Descent
Tuytelaars, T. (2018). Memory aware synapses: Learning
what (not) to forget. Proceedings of the European
Conference on Computer Vision (pp. 139-154).
Caccia, M., Rodriguez, P., Ostapenko, O., Normandin, F.,
Lin, M., Page-Caccia, L., ... & Charlin, L. (2020). Online
fast adaptation and knowledge accumulation (OSAKA): a
new approach to continual learning. Advances in Neural
Information Processing Systems ,33, 16532-16545.
Chaudhry, A., Rohrbach, M., Elhoseiny, M., Ajanthan, T.,
Dokania, P. K., Torr, P. H., & Ranzato, M. A. (2019).
On tiny episodic memories in continual learning. arXiv
preprint arXiv:1902.10486 .
Cohen, G., Afshar, S., Tapson, J., & Van Schaik, A. (2017).
EMNIST: Extending MNIST to handwritten letters.
International Joint Conference on Neural Networks (pp.
2921-2926).
Dohare, S., Hernandez-Garcia, J. F., Rahman, P., Sutton, R.
S., & Mahmood, A. R. (2022). Maintaining Plasticity in
Deep Continual Learning . In preparation.
Dohare, S., Sutton, R. S., & Mahmood, A. R. (2021).
Continual backprop: Stochastic gradient descent with
persistent randomness. arXiv preprint arXiv:2108.06325 .
Dong, X., Chen, S., & Pan, S. (2017). Learning to prune
deep neural networks via layer-wise optimal brain
surgeon. Advances in Neural Information Processing
Systems ,30.
Elsayed, M., & Mahmood, A. R. (2022). HesScale:
Scalable Computation of Hessian Diagonals. arXiv
preprint arXiv:2210.11639 .
Elsayed, M. (2022). Investigating Generate and Test for
Online Representation Search with Softmax Outputs .
M.Sc. thesis, University of Alberta.
Fedus, W., Ghosh, D., Martin, J. D., Bellemare, M. G.,
Bengio, Y ., & Larochelle, H. (2020). On catastrophic
interference in atari 2600 games. arXiv preprint
arXiv:2002.12499 .
French, R. M. Using semi-distributed representations
to overcome catastrophic forgetting in connectionist
networks. (1991) Cognitive Science Society Conference(pp. 173-178).
Haarnoja, T., Zhou, A., Abbeel, P., & Levine, S. (2018).
Soft actor-critic: Off-policy maximum entropy deep rein-
forcement learning with a stochastic actor. International
Conference on Machine Learning (pp. 1861-1870).
Hetherington, P. A., & Seidenberg, M. S. (1989). Is there
‚Äòcatastrophic interference‚Äô in connectionist networks?
Conference of the Cognitive Science Society (pp. 26-33).
Hebb, D. O. (1949). The organization of behavior: A
neuropsychological theory. Wiley.
Han, S., Pool, J., Tran, J., & Dally, W. (2015). Learning
both weights and connections for efÔ¨Åcient neural network.
Advances in neural information processing systems , 28.
He, X., Sygnowski, J., Galashov, A., Rusu, A. A., Teh, Y .
W., & Pascanu, R. (2019). Task agnostic continual learn-
ing via meta learning. arXiv preprint arXiv:1906.05201 .
He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving deep
into rectiÔ¨Åers: Surpassing human-level performance on
imagenet classiÔ¨Åcation. IEEE International Conference
on Computer Vision (pp. 1026-1034).
Hassibi, B., & Stork, D. (1992). Second order derivatives
for network pruning: Optimal brain surgeon. Advances
in neural information processing systems ,5.
Isele, D., & Cosgun, A. (2018). Selective experience replay
for lifelong learning. AAAI Conference on ArtiÔ¨Åcial
Intelligence (pp. 3302-3309)
Jacobsen, A., Schlegel, M., Linke, C., Degris, T., White, A.,
& White, M. (2019). Meta-descent for online, continual
prediction. AAAI Conference on ArtiÔ¨Åcial Intelligence
(pp. 3943-3950).
Karnin, E. D. (1990). A simple procedure for pruning
back-propagation trained neural networks. IEEE
transactions on neural networks ,1(2), 239-242.
Kingma, D. P. & Ba, J. (2015). Adam: A method for
stochastic optimization. International Conference on
Learning Representations .

--- PAGE 12 ---
Utility-based Perturbed Gradient Descent
Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J.,
Desjardins, G., Rusu, A. A., ... & Hadsell, R. (2017).
Overcoming catastrophic forgetting in neural networks.
National Academy of Sciences ,114(13), 3521-3526.
Konorski J. (1948). Conditioned ReÔ¨Çexes and Neuron
Organization . Cambridge University Press.
Krizhevsky, A. (2009) Learning Multiple Layers of Features
from Tiny Images . Ph.D. dissertation, University of
Toronto.
Kunin, D., Sagastuy-Brena, J., Ganguli, S., Yamins, D. L.,
& Tanaka, H. (2020). Neural mechanics: Symmetry and
broken conservation laws in deep learning dynamics.
arXiv preprint arXiv:2012.04728 .
Lee, S., Ha, J., Zhang, D., & Kim, G. (2020). A neural
dirichlet process mixture model for task-free continual
learning. arXiv preprint arXiv:2001.00689 .
LeCun, Y ., Denker, J., & Solla, S. (1989). Optimal brain
damage. Advances in neural information processing
systems ,2.
LeCun, Y ., Bottou, L., Bengio, Y ., & Haffner, P. (1998).
Gradient-based learning applied to document recognition.
Proceedings of the IEEE ,86(11), 2278-2324.
Liu, V ., Kumaraswamy, R., Le, L., & White, M. (2019).
The utility of sparse representations for control in
reinforcement learning. AAAI Conference on ArtiÔ¨Åcial
Intelligence (pp. 4384-4391).
Loshchilov, I., & Hutter, F. (2019). Decoupled weight decay
regularization. International Conference on Learning
Representations .
Maas, A. L., Hannun, A. Y ., & Ng, A. Y . (2013). RectiÔ¨Åer
nonlinearities improve neural network acoustic models.
ICML Workshop on Deep Learning for Audio, Speech,
and Language Processing .
Mahmood, A. R., & Sutton, R. S. (2013). Representation
search through generate and test. AAAI Conference on
Learning Rich Representations from Low-Level Sensors
(pp. 16-21).Mozer, M. C., & Smolensky, P. (1988). Skeletonization:
A technique for trimming the fat from a network via
relevance assessment. Advances in neural information
processing systems, 1.
Melchior, J., & Wiskott, L. (2019). Hebbian-descent. arXiv
preprint arXiv:1905.10585 .
Molchanov, P., Mallya, A., Tyree, S., Frosio, I., & Kautz,
J. (2019). Importance estimation for neural network
pruning. IEEE/CVF Conference on Computer Vision and
Pattern Recognition (pp. 11264-11272).
Molchanov, P., Tyree, S., Karras, T., Aila, T., & Kautz,
J. (2016). Pruning convolutional neural networks
for resource efÔ¨Åcient inference. arXiv preprint
arXiv:1611.06440 .
Mnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A.,
Veness, J., Bellemare, M. G., ... & Hassabis, D. (2015).
Human-level control through deep reinforcement
learning. Nature ,518(7540), 529-533.
McCloskey, M., & Cohen, N. J. (1989). Catastrophic
interference in connectionist networks: The sequential
learning problem. Psychology of Learning and Motiva-
tion,24, 109‚Äì165.
Nair, V ., & Hinton, G. E. (2010). RectiÔ¨Åed linear
units improve restricted boltzmann machines. Inter-
national Conference on Machine Learning (pp. 807-814).
Neelakantan, A., Vilnis, L., Le, Q. V ., Sutskever, I., Kaiser,
L., Kurach, K., & Martens, J. (2015). Adding gradient
noise improves learning for very deep networks. arXiv
preprint arXiv:1511.06807 .
Lyle, C., Zheng, Z., Nikishin, E., Pires, B. A., Pascanu, R.,
& Dabney, W. (2023). Understanding plasticity in neural
networks. arXiv preprint arXiv:2303.01486 .
Orvieto, A., Kersting, H., Proske, F., Bach, F., & Lucchi,
A. (2022). Anticorrelated noise injection for improved
generalization. arXiv preprint arXiv:2202.02831 .
Park, S., Lee, J., Mo, S., & Shin, J. (2020). Lookahead:
a far-sighted alternative of magnitude-based pruning.
arXiv preprint arXiv:2002.04809 .

--- PAGE 13 ---
Utility-based Perturbed Gradient Descent
Tresp, V ., Neuneier, R., & Zimmermann, H. G. (1996).
Early brain damage. Advances in Neural Information
Processing Systems , 9.
Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer,
H., Kirkpatrick, J., Kavukcuoglu, K., ... & Hadsell, R.
(2016). Progressive neural networks. arXiv preprint
arXiv:1606.04671 .
Rolnick, D., Ahuja, A., Schwarz, J., Lillicrap, T., & Wayne,
G. (2019). Experience replay for continual learning.
Advances in Neural Information Processing Systems ,32.
Rudolph, G. (1997). Convergence properties of evolution-
ary algorithms . Verlag Dr. Kova Àác.
Rechenberg, I. (1973). Evolutionsstrategie. Optimierung
technischer Systeme nach Prinzipien derbiologischen
Evolution .
Salimans, T., Ho, J., Chen, X., Sidor, S., & Sutskever, I.
(2017). Evolution strategies as a scalable alternative to re-
inforcement learning. arXiv preprint arXiv:1703.03864 .
Such, F. P., Madhavan, V ., Conti, E., Lehman, J., Stanley, K.
O., & Clune, J. (2017). Deep neuroevolution: Genetic
algorithms are a competitive alternative for training
deep neural networks for reinforcement learning. arXiv
preprint arXiv:1712.06567 .
Schraudolph, N. N. (1999) Local gain adaptation in
stochastic gradient descent. International Conference on
ArtiÔ¨Åcial Neural Networks (pp. 569-574).
Schwarz, J., Czarnecki, W., Luketina, J., Grabska-
Barwinska, A., Teh, Y . W., Pascanu, R., & Hadsell, R.
(2018). Progress & compress: A scalable framework for
continual learning. International Conference on Machine
Learning (pp. 4528-4537).
Sutton, R. S. (1986). Two problems with backpropagation
and other steepest-descent learning procedures for
networks. Annual Conference of the Cognitive Science
Society (pp. 823-832).
Tanaka, H., Kunin, D., Yamins, D. L., & Ganguli, S.
(2020). Pruning neural networks without any data by
iteratively conserving synaptic Ô¨Çow. Advances in Neural
Information Processing Systems ,33, 6377-6389.Yu, R., Li, A., Chen, C. F., Lai, J. H., Morariu, V . I., Han,
X., ... & Davis, L. S. (2018). Nisp: Pruning networks
using neuron importance score propagation. IEEE
conference on computer vision and pattern recognition
(pp. 9194-9203).
Zhou, M., Liu, T., Li, Y ., Lin, D., Zhou, E., & Zhao, T.
(2019). Toward understanding the importance of noise in
training neural networks. International Conference on
Machine Learning (pp. 7594-7602).

--- PAGE 14 ---
Utility-based Perturbed Gradient Descent
A. Utility Propagation
The instantaneous utility measure can be used in a recursive formulation, allowing for backward propagation. We can get a
recursive formula for the utility equation for connections in a neural network. This property is a result of Theorem A.1.
Theorem A.1. If the second-order off-diagonal terms in all layers in a neural network except for the last one are zero and
all higher-order derivatives are zero, the true weight utility for the weight ijat the layer lcan be propagated using the
following recursive formulation:
Ul;i;j(Z):=fl;i;j+sl;i;j
where
fl;i;j:= 0(al;i)hl 1;j
hl;jWl;i;jjal+1jX
k=1 
fl+1;k;jWl+1;k;i
Wl+1;k;j!
;
sl;i;j:=h2
l 1;jW2
l;i;j
hl;jWl+1;i;jjal+1jX
k=1 
sl+1;k;jW2
l+1;k;i0(al;i)2
hl;jWl+1;i;j+fl+1;k;jWl+1;k;i00(al;i)!
:
Proof. First, we start by writing the partial derivative of the loss with respect to each weight in terms of earlier partial
derivatives in the next layers as follows:
@L
@al;i=jal+1jX
k=1@L
@al+1;k@al+1;k
@hl;i@hl;i
@al;i=jal+1jX
k=1@L
@al+1;kWl+1;k;i0(al;i); (11)
@L
@Wl;i;j=@L
@al;i@al;i
@Wl;i;j=@L
@al;ihl 1;j: (12)
Next, we do the same with second-order partial derivatives as follows:
d@2L
@a2
l;i:=jal+1jX
k=1"\@2L
@a2
l+1;kW2
l+1;k;i0(al;i)2+@L
@al+1;kWl+1;k;i00(al;i)#
; (13)
\@2L
@W2
l;i;j:=d@2L
@a2
l;ih2
l 1;j: (14)
From here, we can derive the utility propagation formulation as the sum of two recursive quantities, fl;ijandsl;ij. These
two quantities represent the Ô¨Årst and second-order terms in the Taylor approximation. Using Eq. 11, Eq. 12, Eq. 13, and Eq.
14, we can derive the recursive formulation as follows:
Ul;i;j(Z):= @L(W;Z)
@Wl;i;jWl;i;j+1
2@2L(W;Z)
@W2
l;ijW2
l;ij
 @L(W;Z)
@Wl;i;jWl;i;j+1
2\@2L(W;Z)
@W2
l;ijW2
l;ij
= jal+1jX
k=1@L
@al+1;kWl+1;k;i0(al;i)hl 1;jWl;i;j
+1
2jal+1jX
k=1"\@2L
@a2
l+1;kW2
l+1;k;i0(al;i)2+@L
@al+1;kWl+1;k;i00(al;i)#
h2
l 1;jW2
l;i;j
=fl;i;j+sl;i;j: (15)

--- PAGE 15 ---
Utility-based Perturbed Gradient Descent
Now, we can write the Ô¨Årst-order part fl;i;jand the second-order part sl;i;jas follows:
fl;i;j= jal+1jX
k=1 
@L
@al+1;kWl+1;k;i0(al;i)hl 1;jWl;i;j!
(16)
sl;i;j=1
2jal+1jX
k=1 \@2L
@a2
l+1;kW2
l+1;k;i0(al;i)2+@L
@al+1;kWl+1;k;i00(al;i)!
h2
l 1;jW2
l;i;j (17)
Using Eq. 16 and Eq. 17, we can write the recursive formulation for fl;ijandsl;ijas follows:
fl;ij= jal+1jX
k=1 jal+2jX
m=1@L
@al+2;mWl+2;m;k0(al+1;k)hl;jWl+1;k;j
hl;jWl+1;k;jWl+1;k;i0(al;i)hl 1;jWl;i;j!
= jal+1jX
k=1 
fl+1;k;j0(al;i)hl 1;jWl;i;j
hl;jWl+1;k;jWl+1;k;i!
= 0(al;i)hl 1;j
hl;jWl;i;jjal+1jX
k=1 
fl+1;k;jWl+1;k;i
Wl+1;k;j!
(18)
sl;i;j=1
2jal+1jX
k=1 \@2L
@a2
l+1;kW2
l+1;k;i0(al;i)2+@L
@al+1;kWl+1;k;i00(al;i)!
h2
l 1;jW2
l;i;j
=1
2jal+1jX
k=1 jal+2jX
m=1h\@2L
@a2
l+2;mW2
l+2;m;k0(al+1;k)2+@L
@al+2;mWl+2;m;k00(al;k)i
W2
l+1;k;i0(al;i)2
+jal+2jX
m=1@L
@al+2;mWl+2;m;k0(al;m)
Wl+1;k;i00(al;i)!
h2
l 1;jW2
l;i;j
=jal+1jX
k=1 
1
2jal+2jX
m=1h\@2L
@a2
l+2;mW2
l+2;m;k0(al+1;k)2+@L
@al+2;mWl+2;m;k00(al;k)ih2
l;jW2
l+1;i;j
h2
l;jW2
l+1;i;jW2
l+1;k;i0(al;i)2
+jal+2jX
m=1@L
@al+2;mWl+2;m;k0(al+1;m)hl;jWl+1;i;j
hl;jWl+1;i;j
Wl+1;k;i00(al;i)!
h2
l 1;jW2
l;i;j
=h2
l 1;jW2
l;i;j
hl;jWl+1;i;jjal+1jX
k=1 
sl+1;k;jW2
l+1;k;i0(al;i)2
hl;jWl+1;i;j+fl+1;k;jWl+1;k;i00(al;i)!
(19)
B. Feature Utility Approximation using Weight utility
Here, we derive the global utility of a feature pat layerl, which is represented as the difference between the loss when
the feature is removed and the regular loss. Instead of developing a direct feature utility, we derive the utility of a feature
based on the utility of the input or output weights. The utility vector corresponding to l-th layer features is ul(Z), where
Z= (X;Y )denotes the sample, which is given by
ul;i(Z) =L(W:[l;i];Z) L(W;Z);
whereWis the set of matrices fW1;:::;WLg,L(W;Z)is the sample loss of a network parameterized by Won sampleZ,
andW:[l;i]is the same asWexcept the weight Wl+1;i;jis set to 0 for all values of i.
Note that the second-order Talyor‚Äôs approximation of this utility depends on the off-diagonal elements of the Hessian matrix
at each layer, since more than one weight is removed at once. For our analysis, we drop these elements and derive our

--- PAGE 16 ---
Utility-based Perturbed Gradient Descent
feature utility. We expand the difference around the current input weights of the feature iat layerland evaluate it by setting
the weight to zero. The quadratic approximation of ul;i(Z)can be written as
ul;j(Z) =L(W:[l;i];Z) L(W;Z)
=jal+1jX
i=1
 @L
@Wl;i;jWl;i;j+1
2@2L
@W2
l;ijW2
l+1;i;j+ 2X
j6=i@2L
@Wl;i;j@Wl;i;kWl;i;jWl;i;k
jal+1jX
i=1
 @L
@Wl;ijWl;ij+1
2@2L
@W2
l;ijW2
l;ij
=jal+1jX
i=1Ul;i;j: (20)
Alternatively, we can derive the utility of the pat layerlby dropping the input weights when the activation function passes
through the origin (zero input leads to zero output). This gives rise to the property of conservation of utility shown by
Theorem B.1.
Theorem B.1. If the second-order off-diagonals in all layers in a neural network except for the last one are zero, all
higher-order derivatives are zero, and an origin-passing activation function is used, the sum of output-weight utilities to a
feature equals the sum of input-weight utilities to the same feature.
jal+1jX
i=1Ul;i;j=jaljX
i=1Ul l;j;i:
Proof. From Eq. 20, we can write the utility of the feature pat layerlby dropping the input weights when the activation
function passes through the origin (zero input leads to zero output) as follows:
ul;j(Z)jaljX
i=1Ul l;j;i(Z):
In addition, we can write the utility of the feature pat layerlby dropping the output weights as follows:
ul;j(Z)jaljX
i=1Ul;i;j(Z):
Therefore, we can write the following equality:
jal+1jX
i=1Ul;i;j=jaljX
i=1Ul l;j;i:
This theorem shows the property of the conservation of instantaneous utility. The sum of utilities of the outgoing weights to
a feature equals to the sum of utilities of the incoming weights to the same feature when origin-passing activation functions
are used and the off-diagonal elements are dropped.
This conservation law resembles the one by Tanaka et al. (2020). Such a conservation law comes from a corresponding
symmetry, namely the rescale symmetry (Kunin et al. 2020)

--- PAGE 17 ---
Utility-based Perturbed Gradient Descent
C. Utility-based Perturbed Gradient Descent Algorithms with Local Utilities
Algorithm 3 Weight-wise UPGD with local scaled utility
Require: Neural network fwith weightsfW1;:::;WLg
and a stream of data D
Require: Step sizeand decay rate 2[0;1)
Require: InitializefW1;:::;WLg
Initialize time step t 0
forlinfL;L 1;:::;1gdo
Ul 0
end for
for(x;y)inDdo
t t+ 1
forlinfL;L 1;:::;1gdo
Fl;Sl GetWeightDerivatives (f;x;y;l)
Ml 1=2SlW2
l FlW
Ul Ul+ (1 )Ml
^Ul Ul=(1 t)
Sample noise matrix .
Ul (^UlD)// where Dii= 1=jUi;:;lj
// andDij= 0;8i6=j
Wl Wl (Fl+)(1 Ul)
end for
end forAlgorithm 4 Feature-wise UPGD with local scaled utility
Require: Neural network fwith weightsfW1;:::;WLg
and a stream of data D
Require: Step sizeand decay rate 2[0;1)
Require: Initialize weightsfW1;:::;WLgrandomly and
masksfg1;:::;gL 1gto ones
Require: Initialize time step t 0
forlinfL;L 1;:::;1gdo
ul 0
end for
for(x;y)inDdo
t t+ 1
forlinfL;L 1;:::;1gdo
Fl;Sl GetWeightDerivatives (f;x;y;l)
ifl=Lthen
Wl Wl Fl
else
fl;sl GetMaskDerivatives (f;x;y;l)
ml 1=2sl fl
ul ul+ (1 )ml
^ul ul=(1 t)
ul (^ul=j^ulj)
Sample noise matrix .
Wl Wl (Fl+)(1 1u>
l)
end if
end for
end for
D. Experimental Details
D.1. Toy Problem
The learners use a network of two hidden layers containing 300and150units with identity activations, respectively. A
thorough hyperparameter search is conducted. The search space for step size is f10 ig4
i=1, for the utility trace decay buis
f0:0;0:9;0:99;0:999g, for the noise standard deviation is f10 ig4
i=0, and for the weight decay factor isf0:1;0:01;0:001g.
The utility traces are computed using exponential moving averages given by ~Ut=u~Ut 1+ (1 u)Ut, where ~Utis the
utility trace at time tandUtis the instantaneous utility at time t. Each learner is trained for 1million time steps. The results
are averaged over 20independent runs.
D.2. Stationary MNIST
The learners use a network of two hidden layers containing 300and150units with ReLU activations, respectively. A thorough
hyperparameter search is conducted. For the search experiment using UPS, search space for the standard deviation of noise 
isf10 ig5
i=1, the step size is Ô¨Åxed for a value of 1:0, and the utility trace decay buisf0:0;0:9;0:99;0:999;0:9999g. In the
experiments using UPGD, the search space for step size isf10 ig4
i=1, the utility trace decay buisf0:0;0:9;0:99;0:999g,
the standard deviation of the noise is f10 ig4
i=0, and the weight decay factor isf0:1;0:01;0:001gfor Shrink & Perturb.
The utility traces are computed using exponential moving averages given by ~Ut=u~Ut 1+ (1 u)Ut, where ~Utis the
utility trace at time tandUtis the instantaneous utility at time t. Each learner is trained for 1million time steps. The results
are averaged over 20independent runs.

--- PAGE 18 ---
Utility-based Perturbed Gradient Descent
D.3. Input-permuted MNSIT and Output-permuted EMNIST
The learners use a network of two hidden layers containing 300and150units with ReLU activations, respectively. A
thorough hyperparameter search is conducted. The search space for step size isf0:01;0:001;0:0001g, for the utility
trace decay buisf0:999;0:9999g, for the standard deviation of the noise is f0:01;0:1;1:0g, and for the weight decay
factorisf0:0;0:1;0:01;0:001;0:0001g. The utility traces are computed using exponential moving averages given by
~Ut=u~Ut 1+ (1 u)Ut, where ~Utis the utility trace at time tandUtis the instantaneous utility at time t. For AdamW,
we used a search space for 1off0:0;0:9g, for2off0:99;0:999;0:9999g, and foroff10 4;10 8;10 16g. Each learner
is trained for 1million time steps. The results are averaged over 20independent runs.
D.4. Output-permuted CIFAR-10
The learners use a network with two convolutional layers with max-pooling followed by two fully connected layers with
ReLU activations. The Ô¨Årst convolutional layer uses a kernel of 5and outputs 6Ô¨Ålters, whereas the second convolutional
layer uses a kernel of 5and outputs 16Ô¨Ålters. The max-pooling layers use a kernel of 2. The data is Ô¨Çattened after the second
max-pooling layer and fed to a fully connected network with two hidden layers containing 120and84units, respectively.
A thorough hyperparameter search is conducted. The search space for step size isf0:01;0:001;0:0001g, for the utility
trace decay buisf0:999;0:9999g, for the standard deviation of the noise is f0:01;0:1;1:0g, and for the weight decay
factorisf0:0;0:1;0:01;0:001;0:0001g. The utility traces are computed using exponential moving averages given by
~Ut=u~Ut 1+ (1 u)Ut, where ~Utis the utility trace at time tandUtis the instantaneous utility at time t. For AdamW,
we used a search space for 1off0:0;0:9g, for2off0:99;0:999;0:9999g, and foroff10 4;10 8;10 16g. Each learner
is trained for 1million time steps. The results are averaged over 10independent runs.
E. The HesScale Algorithm (Elsayed & Mahmood 2022)
Algorithm 5 The HesScale Algorithm in ClassiÔ¨Åcation (Elsayed & Mahmood 2022)
Require: Neural network fand a layer number l
Require:\@L
@al+1;iand\@2L
@a2
l+1;i;j, unlessl=L
Require: Input-output pair (x;y).
Compute preference vector aL f(x)and target one-hot-encoded vector p onehot (y).
Compute the predicted probability vector q (aL).
Compute the error L(p;q).
ifl=Lthen
Compute@L
@aL q p
Compute@L
@WLusing Eq. 12
d@2L
@a2
L q qq
Compute[@2L
@W2
Lusing Eq. 14
else ifl6=Lthen
Compute@L
@aland@L=@Wlusing Eq. 11 and Eq. 12
Computed@2L
@a2
land[@2L
@W2
lusing Eq. 13 and Eq. 14
end if
Return@L
@Wl,[@2L
@W2
l,@L
@al, andd@2L
@a2
l
F. Additional Results
F.1. Utility-based Perturbed Search on Stationary MNIST using uncorrelated noise
We repeat the stationary MNIST experiment on Utility-based Perturbed Search (UPS) but using uncorrelated noise. We
report the results in Fig. 8 with uncorrelated noise. The UPS is unable to learn using uncorrelated noise in the weight space,
but it is still able to learn in the feature space. This may suggest that learning in the weight space is harder since it is larger
than the feature space, and the anti-correlated noise is more effective in larger spaces.

--- PAGE 19 ---
Utility-based Perturbed Gradient Descent
0.0 0.2 0.4 0.6 0.8 1.0
Number of Samples1e60.10.20.30.40.50.60.70.80.9Accuracy
(a) Weight-wise
0.0 0.2 0.4 0.6 0.8 1.0
Number of Samples1e60.10.20.30.40.50.60.70.80.9Accuracy (b) Feature-wise
Global Utility Local Utility Random Utility
Figure 8. Utility-based Perturbed Search with Uncorrelated Noise
F.2. UPGD on Stationary MNIST
We repeat the stationary MNIST experiment but with utility-based gradient descent. A desirable property of continual
learning systems is that they should not asymptotically impose any extra performance reduction, which can be studied in a
stationary task such as MNIST. We report the results in Fig. 9 with uncorrelated noise. We notice that UPGD improves
performance over SGD since it uses search in addition to the gradient information.
0.0 0.2 0.4 0.6 0.8 1.0
Number of Samples1e60.750.800.850.900.951.00Accuracy
(a) Weight-wise with Global Utility
0.0 0.2 0.4 0.6 0.8 1.0
Number of Samples1e60.750.800.850.900.951.00Accuracy (b) Feature-wise with Global Utility
0.0 0.2 0.4 0.6 0.8 1.0
Number of Samples1e60.750.800.850.900.951.00Accuracy
(c) Weight-wise with Local Utility
0.0 0.2 0.4 0.6 0.8 1.0
Number of Samples1e60.750.800.850.900.951.00Accuracy (d) Feature-wise with Local Utility
SGD
PGDShrink and Perturb
UPGDNon-protecting UPGD
Figure 9. Utility-based Perturbed Gradient Descent with Ô¨Årst-order approximated utilities using uncorrelated noise on stationary MNIST.

--- PAGE 20 ---
Utility-based Perturbed Gradient Descent
F.3. The Quality of the Approximated Utility
Here, we provide more results for the quality of the approximated utility. We use the Spearman correlation to calculate the
similarity between the ordering of the approximated utility against the true utility using different activation functions. Fig.
10 shows the Spearman correlation at every time step for the global and local cases.
0 500 1000 1500 2000
Number of Samples0.00.10.20.30.4Spearman's Coefficient
(a) Global weight utility (ReLU)
0 500 1000 1500 2000
Number of Samples0.00.10.20.30.40.5Spearman's Coefficient (b) Local weight utility (ReLU)
0 500 1000 1500 2000
Number of Samples0.00.10.20.30.40.50.6Spearman's Coefficient (c) Global feature utility (ReLU)
0 500 1000 1500 2000
Number of Samples0.00.20.40.60.8Spearman's Coefficient (d) Local feature utility (ReLU)
0 500 1000 1500 2000
Number of Samples0.000.050.100.150.200.250.30Spearman's Coefficient
(e) Global weight utility
(LeakyReLU)
0 500 1000 1500 2000
Number of Samples0.00.10.20.30.40.5Spearman's Coefficient(f) Local weight utility
(LeakyReLU)
0 500 1000 1500 2000
Number of Samples0.00.10.20.30.40.5Spearman's Coefficient(g) Global feature utility
(LeakyReLU)
0 500 1000 1500 2000
Number of Samples0.00.10.20.30.40.50.60.7Spearman's Coefficient(h) Local feature utility
(LeakyReLU)
0 500 1000 1500 2000
Number of Samples‚àí0.10.00.10.20.30.40.50.6Spearman's Coefficient
(i) Global weight utility (Tanh)
0 500 1000 1500 2000
Number of Samples0.00.10.20.30.40.50.60.7Spearman's Coefficient (j) Local weight utility (Tanh)
0 500 1000 1500 2000
Number of Samples0.00.10.20.30.40.50.60.7Spearman's Coefficient (k) Global feature utility (Tanh)
0 500 1000 1500 2000
Number of Samples0.00.20.40.60.8Spearman's Coefficient (l) Local feature utility (Tanh)
Figure 10. Spearman Correlation between the true weight/feature utility and approximated weight/feature utilities with different activations
F.4. UPGD with Second-order approximated Utility on the Toy Problem using Second-order Approximated
Utilities
We repeat the experiment on the toy problem but with second-order approximated utility. Fig. 11 shows the results on
the Ô¨Årst variation of the problem, and Fig. 12 shows the results on the second variation of the problem. We note that the
second-order approximated utility improves performance over the Ô¨Årst-order approximated utility in this problem.
F.5. UPGD on the Input-permuted MNIST
We repeat the experiment on the input-permuted MNIST but with feature-wise approximated utility. In addition, we show
the results using the local approximated utility for both weight-wise and feature-wise UPGD in Fig. 13.
F.6. UPGD on Output-permuted EMNIST
We repeat the experiment on the input-permuted MNIST but with feature-wise approximated utility. In addition, we show
the results using the local approximated utility for both weight-wise and feature-wise UPGD in Fig. 14.

--- PAGE 21 ---
Utility-based Perturbed Gradient Descent
0 1000 2000 3000 4000 5000
T ask Number0.060.080.100.120.140.160.180.200.22Loss
(a) Weight-wise with Global Utility
0 1000 2000 3000 4000 5000
T ask Number0.060.080.100.120.140.160.180.200.22Loss (b) Feature-wise with Global Utility
0 1000 2000 3000 4000 5000
T ask Number0.060.080.100.120.140.160.180.200.22Loss
(c) Weight-wise with Local Utility
0 1000 2000 3000 4000 5000
T ask Number0.060.080.100.120.140.160.180.200.22Loss (d) Feature-wise with Local Utility
SGD
PGDShrink and Perturb
UPGD with FO UtilityNon-protecting UPGD with FO Utility
UPGD with SO UtilityNon-protecting UPGD with SO Utility
Reference (Linear Layer)
Figure 11. Utility-based Perturbed Gradient Descent with Ô¨Årst-order and second-order approximated utilities using uncorrelated noise on
the toy problem with changing inputs.
F.7. UPGD on Output-permuted CIFAR-10
We repeat the experiment on the Output-permuted CIFAR-10 but with the local approximated utility. Fig. 15 shows the
results using local and global approximated utilities for UPGD and Non-protecting UPGD.
G. Ablation Study on the Effect of Weight Decay and Weight Perturbation
We conduct a short ablation study on the effect of weight decay and weight perturbation on performance. Fig. 16 shows that
compared algorithms with weight decay (Ô¨Årst row) and without weight decay (second row). Note that we removed Shrink &
Perturb from the second row and used PGD since Shrink & Perturb is PGD with weight decay. We notice the signiÔ¨Åcant
effect of weight decay on most algorithms. However, UPGD can learn quite well without weight decay. We also notice that
only using weight decay without perturbation (e.g., SGDW and AdamW) does not help maintain plasticity in all tasks. We
conclude that both weight decay and weight perturbation are important in continual online learning.

--- PAGE 22 ---
Utility-based Perturbed Gradient Descent
0 1000 2000 3000 4000 5000
T ask Number0.10.20.30.40.5Loss
(a) Weight-wise with Global Utility
0 1000 2000 3000 4000 5000
T ask Number0.10.20.30.40.5Loss (b) Feature-wise with Global Utility
0 1000 2000 3000 4000 5000
T ask Number0.10.20.30.40.5Loss
(c) Weight-wise with Local Utility
0 1000 2000 3000 4000 5000
T ask Number0.10.20.30.40.5Loss (d) Feature-wise with Local Utility
SGD
PGDShrink and Perturb
UPGD with FO UtilityNon-protecting UPGD with FO Utility
UPGD with SO UtilityNon-protecting UPGD with SO Utility
Reference (Linear Layer)
Figure 12. Utility-based Perturbed Gradient Descent with Ô¨Årst-order and second-order approximated utilities using uncorrelated noise on
the toy problem with changing outputs.
0 50 100 150 200
Task Number0.6000.6250.6500.6750.7000.7250.7500.775Accuracy
(a) Weight-wise with Global Utility
0 50 100 150 200
Task Number0.6000.6250.6500.6750.7000.7250.7500.775Accuracy (b) Feature-wise with Global Utility
0 50 100 150 200
Task Number0.6000.6250.6500.6750.7000.7250.7500.775Accuracy
(c) Weight-wise with Local Utility
0 50 100 150 200
Task Number0.6000.6250.6500.6750.7000.7250.7500.775Accuracy (d) Feature-wise with Local Utility
SGDW
PGDShrink and Perturb
UPGD-WNon-protecting UPGD-W
AdamW
Figure 13. Performance of UPGD, SGD, and Adam with weight decay, PGD and Shrink & Perturb on Input-Permuted MNIST. A
Ô¨Årst-order utility is used with the two UPGDs.

--- PAGE 23 ---
Utility-based Perturbed Gradient Descent
0 100 200 300 400
Task Number0.10.20.30.40.50.60.7Accuracy
(a) Weight-wise with Global Utility
0 100 200 300 400
Task Number0.10.20.30.40.50.60.7Accuracy (b) Feature-wise with Global Utility
0 100 200 300 400
Task Number0.10.20.30.40.50.60.7Accuracy
(c) Weight-wise with Local Utility
0 100 200 300 400
Task Number0.10.20.30.40.50.60.7Accuracy (d) Feature-wise with Local Utility
SGDW
PGDShrink and Perturb
UPGD-WNon-protecting UPGD-W
AdamW
Figure 14. Performance of UPGD, SGD, and Adam with weight decay, PGD and Shrink & Perturb on Output-Permuted EMNIST. A
Ô¨Årst-order utility is used with the two UPGDs.
0 100 200 300 400
T ask Number0.10.20.30.40.50.6Accuracy
(a) Weight-wise with Global Utility
0 100 200 300 400
T ask Number0.10.20.30.40.50.6Accuracy (b) Weight-wise with Local Utility
SGDW
PGDShrink and Perturb
UPGD-WNon-protecting UPGD-W
AdamW
Figure 15. Performance of UPGD, SGD, and Adam with weight decay, PGD and Shrink & Perturb on Output-Permuted CIFAR-10. A
Ô¨Årst-order utility is used with the two UPGDs.

--- PAGE 24 ---
Utility-based Perturbed Gradient Descent
0 50 100 150 200
Task Number0.6000.6250.6500.6750.7000.7250.7500.775Accuracy
(a) MNIST (weight decay)
0 100 200 300 400
Task Number0.10.20.30.40.50.60.7Accuracy
 (b) EMNIST (weight decay)
0 100 200 300 400
Task Number0.10.20.30.40.50.6Accuracy
 (c) CIFAR-10 (weight decay)
0 50 100 150 200
Task Number0.6000.6250.6500.6750.7000.7250.7500.775Accuracy
(d) MNIST (no decay)
0 100 200 300 400
Task Number0.10.20.30.40.50.60.7Accuracy
 (e) EMNIST (no decay)
0 100 200 300 400
Task Number0.10.20.30.40.50.6Accuracy
 (f) CIFAR-10 (no decay)
SGDW
PGDShrink and Perturb
UPGD-WNon-protecting UPGD-W
AdamW
Figure 16. Effect of weight perturbation against weight decay with weight-wise UPGD. Performance of UPGD, SGDW, AdamW, PGD,
and Shrink & Perturb on Input-Permuted MNIST, Output-Permuted EMNIST, and Output-Permuted CIFAR10. A global weight-wise
Ô¨Årst-order utility is used with the two UPGDs.
