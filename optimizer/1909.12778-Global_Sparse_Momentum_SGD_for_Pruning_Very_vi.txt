# 1909.12778.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/optimizer/1909.12778.pdf
# Kích thước tệp: 621287 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Global Sparse Momentum SGD để Cắt tỉa Mạng Nơ-ron Sâu Rất Sâu
Xiaohan Ding1 Guiguang Ding1 Xiangxin Zhou2
Yuchen Guo1, 3 Jungong Han4 Ji Liu5
1Beijing National Research Center for Information Science and Technology (BNRist);
School of Software, Tsinghua University, Beijing, China
2Department of Electronic Engineering, Tsinghua University, Beijing, China
3Department of Automation, Tsinghua University;
Institute for Brain and Cognitive Sciences, Tsinghua University, Beijing, China
4WMG Data Science, University of Warwick, Coventry, United Kingdom
5Kwai Seattle AI Lab, Kwai FeDA Lab, Kwai AI platform
dxh17@mails.tsinghua.edu.cn dinggg@tsinghua.edu.cn
xx-zhou16@mails.tsinghua.edu.cn yuchen.w.guo@gmail.com
jungonghan77@gmail.com ji.liu.uwisc@gmail.com

Tóm tắt
Mạng Nơ-ron Sâu (DNN) mạnh mẽ nhưng tốn kém về mặt tính toán và sử dụng nhiều bộ nhớ, do đó cản trở việc sử dụng thực tế trên các thiết bị đầu cuối có tài nguyên hạn chế. Cắt tỉa DNN là một phương pháp nén mô hình sâu, nhằm loại bỏ một số tham số với sự suy giảm hiệu suất có thể chấp nhận được. Trong bài báo này, chúng tôi đề xuất một phương pháp tối ưu hóa mới dựa trên momentum-SGD để giảm độ phức tạp của mạng bằng cách cắt tỉa trong quá trình hoạt động. Cụ thể, với một tỷ lệ nén toàn cục cho trước, chúng tôi phân loại tất cả các tham số thành hai phần ở mỗi lần lặp huấn luyện, được cập nhật bằng các quy tắc khác nhau. Bằng cách này, chúng tôi dần dần đặt các tham số dư thừa về không, vì chúng tôi cập nhật chúng chỉ bằng phân rã trọng số thông thường nhưng không có gradient được tính từ hàm mục tiêu. Khác với các phương pháp trước đó yêu cầu công việc thủ công nặng nề để điều chỉnh tỷ lệ thưa thớt theo từng lớp, cắt tỉa bằng cách giải quyết các vấn đề không khả vi phức tạp hoặc tinh chỉnh mô hình sau khi cắt tỉa, phương pháp của chúng tôi được đặc trưng bởi 1) nén toàn cục tự động tìm ra tỷ lệ thưa thớt phù hợp cho từng lớp; 2) huấn luyện đầu cuối đến đầu cuối; 3) không cần quá trình huấn luyện lại tốn thời gian sau khi cắt tỉa; và 4) khả năng vượt trội để tìm ra những vé trúng thưởng tốt hơn đã thắng trong xổ số khởi tạo.

1 Giới thiệu
Những năm gần đây đã chứng kiến thành công to lớn của Mạng Nơ-ron Sâu (DNN) trong nhiều ứng dụng thực tế. Tuy nhiên, các mô hình rất sâu ngày nay đi kèm với hàng triệu tham số, do đó khiến chúng khó triển khai trên các thiết bị có khả năng tính toán hạn chế. Trong bối cảnh này, các phương pháp cắt tỉa DNN đã thu hút nhiều sự chú ý, nơi chúng ta loại bỏ một số kết nối (tức là các tham số riêng lẻ) [21,22,31], hoặc các kênh [32], do đó có thể giảm không gian lưu trữ cần thiết và tính toán. Bài báo này tập trung vào cắt tỉa kết nối, nhưng phương pháp đề xuất có thể dễ dàng được tổng quát hóa cho cắt tỉa có cấu trúc (ví dụ: cấp độ nơ-ron, kernel hoặc bộ lọc). Để đạt được sự cân bằng tốt giữa độ chính xác và kích thước mô hình, nhiều phương pháp cắt tỉa đã được đề xuất, có thể được phân loại thành hai mô hình điển hình. 1) Một số nhà nghiên cứu [13,18,21,22,26,31,32,39,41] đề xuất cắt tỉa mô hình bằng một số phương tiện để đạt một mức độ tỷ lệ nén nhất định, sau đó tinh chỉnh nó bằng SGD thông thường để khôi phục độ chính xác. 2) Các phương pháp khác tìm cách tạo ra tính thưa thớt trong mô hình thông qua một quy trình học tùy chỉnh [1, 12, 33, 34, 51, 54, 56].

--- TRANG 2 ---
Mặc dù các phương pháp hiện tại đã đạt được thành công lớn trong cắt tỉa, vẫn có một số nhược điểm điển hình. Cụ thể, khi chúng ta tìm cách cắt tỉa một mô hình trước và tinh chỉnh nó, chúng ta phải đối mặt với hai vấn đề:

Tỷ lệ thưa thớt theo từng lớp vốn là khó khăn để đặt làm siêu tham số. Nhiều công trình trước đây [21,24,26,32] đã chỉ ra rằng một số lớp trong DNN nhạy cảm với việc cắt tỉa, nhưng một số có thể được cắt tỉa đáng kể mà không làm giảm độ chính xác của mô hình. Kết quả là, cần có kiến thức trước để điều chỉnh các siêu tham số theo từng lớp nhằm tối đa hóa tỷ lệ nén toàn cục mà không có sự sụt giảm độ chính xác không thể chấp nhận được.

Các mô hình đã cắt tỉa khó huấn luyện, và chúng ta không thể dự đoán độ chính xác cuối cùng sau khi tinh chỉnh. Ví dụ, các mô hình đã cắt tỉa ở cấp độ bộ lọc có thể dễ dàng rơi vào cực tiểu cục bộ xấu, và đôi khi thậm chí không thể đạt được mức độ chính xác tương tự với một mô hình tương đương được huấn luyện từ đầu [10,38]. Và trong bối cảnh cắt tỉa kết nối, mạng càng thưa thớt thì việc học càng chậm và độ chính xác kiểm tra cuối cùng càng thấp [15].

Mặt khác, cắt tỉa bằng học tập không dễ dàng hơn do:

Trong một số trường hợp, chúng ta giới thiệu một siêu tham số để kiểm soát sự cân bằng, điều này không trực tiếp phản ánh tỷ lệ nén kết quả. Ví dụ, MorphNet [17] sử dụng group Lasso [44] để làm không một số bộ lọc cho cắt tỉa có cấu trúc, trong đó siêu tham số chính là hệ số Lasso. Tuy nhiên, với một giá trị cụ thể của hệ số, chúng ta không thể dự đoán tỷ lệ nén cuối cùng trước khi quá trình huấn luyện kết thúc. Do đó, khi chúng ta nhắm đến một tỷ lệ nén cuối cùng cụ thể, chúng ta phải thử nhiều giá trị hệ số trước và chọn giá trị mang lại kết quả gần nhất với kỳ vọng của chúng ta.

Một số phương pháp cắt tỉa bằng cách giải quyết một vấn đề tối ưu hóa liên quan trực tiếp đến tính thưa thớt. Vì vấn đề này không khả vi, nó không thể được giải quyết bằng các phương pháp dựa trên SGD một cách đầu cuối đến đầu cuối. Một thảo luận chi tiết hơn sẽ được cung cấp trong Phần 3.2.

Trong bài báo này, chúng tôi tìm cách khắc phục những nhược điểm đã thảo luận ở trên bằng cách thay đổi trực tiếp dòng gradient dựa trên momentum SGD, điều này liên quan rõ ràng đến tỷ lệ nén cuối cùng và có thể được triển khai thông qua huấn luyện đầu cuối đến đầu cuối. Cụ thể, chúng tôi sử dụng chuỗi Taylor bậc nhất để đo tầm quan trọng của một tham số bằng cách ước lượng mức độ thay đổi giá trị hàm mục tiêu khi loại bỏ nó [41,49]. Dựa trên đó, với một tỷ lệ nén toàn cục cho trước, chúng tôi phân loại tất cả các tham số thành hai phần sẽ được cập nhật bằng các quy tắc khác nhau, được gọi là lựa chọn kích hoạt. Đối với các tham số không quan trọng, chúng tôi thực hiện cập nhật thụ động không có gradient được tính từ hàm mục tiêu nhưng chỉ có phân rã trọng số thông thường (tức là điều chuẩn ℓ-2) để phạt giá trị của chúng. Mặt khác, thông qua cập nhật chủ động, các tham số quan trọng được cập nhật bằng cả gradient liên quan đến hàm mục tiêu và phân rã trọng số để duy trì độ chính xác của mô hình. Việc lựa chọn như vậy được thực hiện ở mỗi lần lặp huấn luyện, sao cho một kết nối đã bị hủy kích hoạt có cơ hội được kích hoạt lại ở lần lặp tiếp theo. Thông qua các cập nhật thụ động được tăng tốc bởi momentum liên tục, chúng ta có thể làm cho hầu hết các tham số vô cùng gần với không, sao cho việc cắt tỉa chúng không gây thiệt hại cho độ chính xác của mô hình. Nhờ điều này, không cần quá trình tinh chỉnh. Ngược lại, một số điều khoản điều chuẩn được đề xuất trước đây chỉ có thể giảm các tham số đến một mức độ nhất định, do đó việc cắt tỉa vẫn làm giảm mô hình. Những đóng góp của chúng tôi được tóm tắt như sau:

Để cắt tỉa không mất mát và huấn luyện đầu cuối đến đầu cuối, chúng tôi đề xuất thay đổi trực tiếp dòng gradient, điều này được phân biệt rõ ràng với các phương pháp hiện tại hoặc thêm một điều khoản điều chuẩn hoặc tìm cách giải quyết một số vấn đề tối ưu hóa không khả vi.

Chúng tôi đề xuất Global Sparse Momentum SGD (GSM), một phương pháp tối ưu hóa SGD mới, chia quy tắc cập nhật của momentum SGD thành hai phần. Cắt tỉa DNN dựa trên GSM yêu cầu một tỷ lệ nén cuối cùng toàn cục duy nhất làm siêu tham số và có thể tự động khám phá tỷ lệ thưa thớt phù hợp cho từng lớp để đạt được nó.

Từ các thực nghiệm, chúng tôi đã xác nhận khả năng của GSM để đạt được tỷ lệ nén cao trên MNIST, CIFAR-10 [29] và ImageNet [9] cũng như tìm ra những vé trúng thưởng tốt hơn [15]. Mã nguồn có sẵn tại https://github.com/DingXiaoH/GSM-SGD.

2 Công trình liên quan
2.1 Momentum SGD
Stochastic gradient descent chỉ xem xét các đạo hàm bậc nhất của hàm mục tiêu chứ không phải các đạo hàm bậc cao hơn [28]. Momentum là một kỹ thuật phổ biến được sử dụng cùng với SGD, tích lũy các gradient của các bước trước để xác định hướng đi, thay vì chỉ sử dụng gradient của bước hiện tại. Tức là, momentum cho SGD một bộ nhớ ngắn hạn [16]. Chính thức, cho L là hàm mục tiêu, w là một tham số duy nhất, η là tốc độ học, μ là hệ số momentum kiểm soát tỷ lệ phần trăm gradient được giữ lại mỗi lần lặp, λ là hệ số phân rã trọng số thông thường (ví dụ: 1×10^-4 cho ResNets [23]), quy tắc cập nhật là:

z^(k+1) ← μz^(k) + λw^(k) + ∂L/∂w^(k);
w^(k+1) ← w^(k) - ηz^(k+1).                                                      (1)

Có một câu chuyện phổ biến về momentum [16,42,45,48]: gradient descent là một người đàn ông đi bộ xuống đồi. Anh ta đi theo con đường dốc nhất xuống dưới; tiến bộ của anh ta chậm nhưng ổn định. Momentum là một quả bóng nặng lăn xuống cùng một ngọn đồi. Quán tính được thêm vào hoạt động như một chất làm mịn và một máy gia tốc, làm giảm dao động và khiến chúng ta lao qua các thung lũng hẹp, những gò nhỏ và cực tiểu cục bộ. Trong bài báo này, chúng tôi sử dụng momentum như một máy gia tốc để tăng tốc các cập nhật thụ động.

2.2 Cắt tỉa DNN và các kỹ thuật khác để nén và tăng tốc
Cắt tỉa DNN tìm cách loại bỏ một số tham số mà không làm giảm đáng kể độ chính xác, có thể được phân loại thành các kỹ thuật không có cấu trúc và có cấu trúc dựa trên độ chi tiết cắt tỉa. Cắt tỉa không có cấu trúc (còn gọi là cắt tỉa kết nối) [7,21,22,31] nhắm đến việc giảm đáng kể số lượng tham số khác không, tạo ra một mô hình thưa thớt, có thể được lưu trữ bằng ít không gian hơn nhiều, nhưng không thể giảm hiệu quả gánh nặng tính toán trên các nền tảng phần cứng và phần mềm sẵn có. Mặt khác, cắt tỉa có cấu trúc loại bỏ các cấu trúc (ví dụ: nơ-ron, kernel hoặc toàn bộ bộ lọc) khỏi DNN để có được tăng tốc thực tế. Ví dụ, cắt tỉa kênh [10,11,32,35,37,38] không thể đạt được tỷ lệ nén cực cao của kích thước mô hình, nhưng có thể chuyển đổi một CNN rộng thành một CNN hẹp hơn (nhưng vẫn dày đặc) để giảm chi phí bộ nhớ và tính toán. Trong các ứng dụng thực tế, cắt tỉa không có cấu trúc và có cấu trúc thường được sử dụng cùng nhau để đạt được sự cân bằng mong muốn.

Bài báo này tập trung vào cắt tỉa kết nối (nhưng phương pháp đề xuất có thể dễ dàng được tổng quát hóa cho cắt tỉa có cấu trúc), đã thu hút nhiều sự chú ý kể từ khi Han et al. [21] cắt tỉa các kết nối DNN dựa trên độ lớn của các tham số và khôi phục độ chính xác thông qua SGD thông thường. Một số công trình truyền cảm hứng đã cải thiện mô hình cắt tỉa và tinh chỉnh bằng cách nối các kết nối khi chúng trở nên quan trọng trở lại [18], nhắm mục tiêu trực tiếp vào tiêu thụ năng lượng [55], sử dụng các đạo hàm bậc hai theo từng lớp [13], v.v. Các phương pháp cắt tỉa dựa trên học tập khác sẽ được thảo luận trong Phần 3.2.

Ngoài cắt tỉa, chúng ta cũng có thể nén và tăng tốc DNN bằng các cách khác. Một số công trình [2,46,57] phân tích hoặc xấp xỉ các tensor tham số; các kỹ thuật lượng tử hóa và nhị phân [8,19,20,36] xấp xỉ một mô hình bằng ít bit hơn mỗi tham số; chưng cất kiến thức [3,25,43] chuyển giao kiến thức từ một mạng lớn sang một mạng nhỏ hơn; một số nhà nghiên cứu tìm cách tăng tốc tích chập với sự hỗ trợ của việc đục lỗ [14], FFT [40,50] hoặc DCT [53]; Wang et al. [52] nén các bản đồ đặc trưng bằng cách trích xuất thông tin thông qua một ma trận Circulant.

3 GSM: Global Sparse Momentum SGD
3.1 Xây dựng công thức
Chúng tôi trước tiên làm rõ các ký hiệu trong bài báo này. Đối với một lớp kết nối đầy đủ với đầu vào p chiều và đầu ra q chiều, chúng tôi sử dụng W ∈ R^(p×q) để biểu thị ma trận kernel. Đối với một lớp tích chập với tensor kernel K ∈ R^(h×w×r×s), trong đó h và w là chiều cao và chiều rộng của kernel tích chập, r và s là số lượng kênh đầu vào và đầu ra, tương ứng, chúng tôi mở rộng tensor K thành W ∈ R^(hwr×s). Cho N là số lượng tất cả các lớp như vậy, chúng tôi sử dụng Θ = [W_i] (∀1≤i≤N) để biểu thị tập hợp tất cả các ma trận kernel như vậy, và tỷ lệ nén toàn cục C được cho bởi:

C = |Θ|/||Θ||_0;                                                                  (2)

trong đó |Θ| là kích thước của Θ và ||Θ||_0 là chuẩn ℓ-0, tức là số lượng các mục khác không. Cho L, X, Y là hàm mất mát liên quan đến độ chính xác (ví dụ: cross entropy cho các tác vụ phân loại), các ví dụ kiểm tra và nhãn, tương ứng, chúng tôi tìm cách có được sự cân bằng tốt giữa độ chính xác và kích thước mô hình bằng cách đạt được tỷ lệ nén cao C mà không có sự gia tăng không thể chấp nhận được trong mất mát L(X;Y;Θ).

--- TRANG 3 ---
3.2 Suy ngẫm lại về cắt tỉa dựa trên học tập
Mục tiêu hoặc hướng tối ưu hóa của huấn luyện DNN thông thường chỉ là giảm thiểu hàm mục tiêu, nhưng khi chúng ta tìm cách tạo ra một mô hình thưa thớt thông qua một quy trình học tập tùy chỉnh, điều chính là làm lệch hướng huấn luyện ban đầu bằng cách xem xét tính thưa thớt của các tham số. Thông qua huấn luyện, tính thưa thớt xuất hiện dần dần, và cuối cùng chúng ta đạt được sự cân bằng mong đợi giữa độ chính xác và kích thước mô hình, thường được kiểm soát bởi một hoặc một loạt siêu tham số.

3.2.1 Cân bằng rõ ràng như tối ưu hóa có ràng buộc
Sự cân bằng có thể được mô hình hóa rõ ràng như một vấn đề tối ưu hóa có ràng buộc [56], ví dụ:

minimize L(X;Y;Θ) + Σ_{i=1}^N g_i(W_i);                                         (3)

trong đó g_i là một hàm chỉ thị,

g_i(W) = {0 nếu ||W||_0 ≤ l_i;
         +∞ nếu ngược lại;}                                                      (4)

và l_i là số lượng tham số khác không cần thiết ở lớp i. Vì điều khoản thứ hai của hàm mục tiêu không khả vi, vấn đề không thể được giải quyết một cách phân tích hoặc bằng stochastic gradient descent, nhưng có thể được giải quyết bằng cách thay phiên áp dụng SGD và giải quyết vấn đề không khả vi, ví dụ: sử dụng ADMM [6]. Bằng cách này, hướng huấn luyện bị lệch, và sự cân bằng được thu được.

3.2.2 Cân bằng ngầm sử dụng điều chuẩn
Việc áp dụng một số điều chuẩn khả vi bổ sung trong quá trình huấn luyện để giảm độ lớn của một số tham số là một thực hành phổ biến, sao cho việc loại bỏ chúng gây ra ít thiệt hại hơn [1,21,54]. Cho R(Θ) là điều khoản điều chuẩn liên quan đến độ lớn, λ là siêu tham số cân bằng, vấn đề là:

minimize L(X;Y;Θ) + λR(Θ).                                                      (5)

Tuy nhiên, những điểm yếu có hai mặt. 1) Một số điều chuẩn phổ biến, ví dụ: ℓ-1, ℓ-2 và Lasso [44], không thể thực sự làm không các mục trong Θ, mà chỉ có thể giảm độ lớn đến một mức độ nhất định, sao cho việc loại bỏ chúng vẫn làm giảm hiệu suất. Chúng tôi gọi hiện tượng này là cao nguyên độ lớn. Nguyên nhân đằng sau rất đơn giản: đối với một tham số có thể huấn luyện cụ thể w, khi độ lớn |w| của nó lớn ở đầu, gradient được tính từ R, tức là ∂R/∂w, áp đảo ∂L/∂w, do đó |w| được giảm dần. Tuy nhiên, khi |w| thu nhỏ, ∂R/∂w cũng giảm, sao cho xu hướng giảm của |w| ổn định khi ∂R/∂w tiến đến ∂L/∂w, và w duy trì một độ lớn tương đối nhỏ. 2) Siêu tham số λ không trực tiếp phản ánh tỷ lệ nén kết quả, do đó chúng ta có thể cần thực hiện nhiều lần thử để có được một số kiến thức thực nghiệm trước khi chúng ta có được mô hình với tỷ lệ nén mong đợi.

3.3 Dòng gradient thưa thớt toàn cục thông qua momentum SGD
Để khắc phục những nhược điểm của hai mô hình đã thảo luận ở trên, chúng tôi định kiểm soát rõ ràng tỷ lệ nén cuối cùng thông qua huấn luyện đầu cuối đến đầu cuối bằng cách thay đổi trực tiếp dòng gradient của momentum SGD để làm lệch hướng huấn luyện nhằm đạt được tỷ lệ nén cao cũng như duy trì độ chính xác. Theo trực giác, chúng tôi tìm cách sử dụng các gradient để hướng dẫn một số tham số hoạt động để giảm thiểu hàm mục tiêu, và phạt hầu hết các tham số để đẩy chúng vô cùng gần với không. Do đó, điều đầu tiên là tìm một số liệu phù hợp để phân biệt phần hoạt động. Với một tỷ lệ nén toàn cục C, chúng tôi sử dụng Q = |Θ|/C để biểu thị số lượng các mục khác không trong Θ. Ở mỗi lần lặp huấn luyện, chúng tôi đưa một mini-batch dữ liệu vào mô hình, tính toán các gradient bằng quy tắc chuỗi thông thường, tính toán các giá trị số liệu cho mỗi tham số, thực hiện cập nhật chủ động trên Q tham số có giá trị số liệu lớn nhất và cập nhật thụ động trên những tham số khác. Để làm cho GSM khả thi trên các mô hình rất sâu, các số liệu nên được tính toán chỉ bằng các kết quả tính toán trung gian ban đầu, tức là các tham số và gradient, nhưng không có đạo hàm bậc hai. Được truyền cảm hứng bởi hai phương pháp trước đó đã sử dụng chuỗi Taylor bậc nhất để cắt tỉa kênh tham lam [41,49], chúng tôi định nghĩa số liệu theo cách tương tự. Chính thức, ở mỗi lần lặp huấn luyện với một mini-batch các ví dụ x và nhãn y, cho T(x;y;w) là giá trị số liệu của một tham số cụ thể w, chúng ta có:

T(x;y;w) = |∂L(x;y;Θ)/∂w · w|.                                                  (6)

Lý thuyết là đối với mini-batch hiện tại, chúng ta mong đợi giảm những tham số có thể được loại bỏ với ít tác động hơn đến L(x;y;Θ). Sử dụng chuỗi Taylor, nếu chúng ta đặt một tham số cụ thể w về 0, giá trị mất mát trở thành:

L(x;y;Θ|w←0) = L(x;y;Θ) - ∂L(x;y;Θ)/∂w(0-w) + o(w²).                         (7)

Bỏ qua điều khoản bậc cao hơn, chúng ta có:

|L(x;y;Θ|w←0) - L(x;y;Θ)| = |∂L(x;y;Θ)/∂w · w| = T(x;y;w),                   (8)

đây là một xấp xỉ của sự thay đổi trong giá trị mất mát nếu w được đặt về không.

Chúng tôi viết lại quy tắc cập nhật của momentum SGD (Công thức 1). Ở lần lặp huấn luyện thứ k với một mini-batch các ví dụ x và nhãn y trên một lớp cụ thể với kernel W, quy tắc cập nhật là:

Z^(k+1) ← μZ^(k) + λW^(k) + B^(k) ⊙ ∂L(x;y;Θ)/∂W^(k);
W^(k+1) ← W^(k) - ηZ^(k+1);                                                      (9)

trong đó ⊙ là phép nhân theo từng phần tử (còn gọi là tích Hadamard), và B^(k) là ma trận mặt nạ,

B^(k)_{m,n} = {1 nếu T(x;y;W^(k)_{m,n}) ≥ giá trị lớn thứ Q trong T(x;y;Θ^(k));
               0 nếu ngược lại.}                                                  (10)

Chúng tôi gọi việc tính toán B cho mỗi kernel là lựa chọn kích hoạt. Rõ ràng, có chính xác Q số một trong tất cả các ma trận mặt nạ, và GSM suy giảm thành momentum SGD thông thường khi Q = |Θ|.

Cần lưu ý rằng GSM không phụ thuộc vào mô hình vì nó không đưa ra giả định nào về cấu trúc mô hình hoặc dạng của hàm mất mát. Tức là, việc tính toán gradient thông qua lan truyền ngược liên quan đến mô hình, tất nhiên, nhưng việc sử dụng chúng để cắt tỉa GSM không phụ thuộc vào mô hình.

3.4 GSM cho phép kích hoạt lại ngầm và giảm liên tục nhanh chóng
Vì GSM thực hiện lựa chọn kích hoạt ở mỗi lần lặp huấn luyện, nó cho phép các kết nối bị phạt được kích hoạt lại, nếu chúng được phát hiện là quan trọng đối với mô hình một lần nữa. So với hai công trình trước đó đã chèn rõ ràng một giai đoạn nối [18] hoặc khôi phục [55] vào toàn bộ quy trình để nối lại các kết nối bị cắt tỉa nhầm, GSM có đặc điểm triển khai đơn giản hơn và huấn luyện đầu cuối đến đầu cuối.

Tuy nhiên, như sẽ được chỉ ra trong Phần 4.4, việc kích hoạt lại chỉ xảy ra trên thiểu số các tham số, nhưng hầu hết chúng trải qua một loạt các cập nhật thụ động, do đó tiếp tục di chuyển về phía không. Vì chúng tôi muốn biết cần bao nhiêu lần lặp huấn luyện để làm cho các tham số đủ nhỏ để thực hiện cắt tỉa không mất mát, chúng tôi cần dự đoán giá trị cuối cùng của một tham số w sau k lần cập nhật thụ động, cho trước η, μ và λ. Chúng ta có thể sử dụng Công thức 1 để dự đoán w^(k), điều này thực tế nhưng cồng kềnh. Trong các trường hợp sử dụng phổ biến của chúng tôi, trong đó z^(0) = 0 (từ đầu quá trình huấn luyện), k lớn (ít nhất hàng chục nghìn), và η nhỏ (ví dụ: η = 5×10^-3, λ = 5×10^-4), chúng tôi đã quan sát một công thức thực nghiệm đủ chính xác (Hình 1) để xấp xỉ giá trị kết quả,

w^(k) ≈ w^(0)(1 - η λ/(1-μ))^k.                                                 (11)

Trong thực tế, chúng tôi cố định λ (ví dụ: 1×10^-4 cho ResNets [23] và DenseNets [27]) và điều chỉnh η như chúng tôi làm cho huấn luyện DNN thông thường, và sử dụng μ = 0.98 hoặc μ = 0.99 để làm không nhanh hơn 50× hoặc 100×.

Khi huấn luyện hoàn tất, chúng tôi cắt tỉa mô hình toàn cục bằng cách chỉ bảo tồn Q tham số trong Θ có độ lớn lớn nhất. Chúng tôi quyết định số lần lặp huấn luyện k bằng Phương trình 11 dựa trên quan sát thực nghiệm rằng với (1 - η λ/(1-μ))^k < 1×10^-4, một hoạt động cắt tỉa như vậy không gây ra sụt giảm độ chính xác trên các mô hình rất sâu như ResNet-56 và DenseNet-40.

Momentum rất quan trọng để cắt tỉa dựa trên GSM được hoàn thành với chi phí thời gian có thể chấp nhận được. Vì hầu hết các tham số liên tục tăng trưởng theo cùng một hướng được xác định bởi phân rã trọng số (tức là về phía không), xu hướng như vậy tích lũy trong momentum, do đó quá trình làm không được tăng tốc đáng kể. Mặt khác, nếu một tham số không luôn thay đổi theo cùng một hướng, việc tăng μ ít ảnh hưởng đến động lực học huấn luyện của nó. Ngược lại, nếu chúng ta tăng tốc độ học để làm không nhanh hơn, các tham số quan trọng đang dao động xung quanh cực tiểu toàn cục sẽ lệch đáng kể khỏi giá trị hiện tại của chúng đã đạt được với tốc độ học thấp hơn nhiều trước đó.

4 Thực nghiệm
4.1 Kết quả cắt tỉa và so sánh
Chúng tôi đánh giá GSM bằng cách cắt tỉa một số mô hình chuẩn phổ biến trên MNIST, CIFAR-10 [29] và ImageNet [9], và so sánh với các kết quả được báo cáo từ một số đối thủ cạnh tranh gần đây. Đối với mỗi thử nghiệm, chúng tôi bắt đầu từ một mô hình cơ sở được huấn luyện tốt và áp dụng huấn luyện GSM trên tất cả các lớp đồng thời.

MNIST. Chúng tôi đầu tiên thử nghiệm trên MNIST với LeNet-300-100 và LeNet-5 [30]. LeNet-300-100 là một mạng kết nối đầy đủ ba lớp với 267K tham số, đạt độ chính xác Top1 98.19%. LeNet-5 là một mạng tích chập bao gồm hai lớp tích chập và hai lớp kết nối đầy đủ, chứa 431K tham số và đạt độ chính xác Top1 99.21%. Để đạt được nén 60× và 125×, chúng tôi đặt Q = 267K/60 = 4.4K cho LeNet-300-100 và Q = 431K/125 = 3.4K cho LeNet-5, tương ứng. Chúng tôi sử dụng hệ số momentum μ = 0.99 và kích thước batch 256. Lịch trình tốc độ học là η = 3×10^-2, 3×10^-3, 3×10^-4 cho 160, 40 và 40 epoch, tương ứng. Sau huấn luyện GSM, chúng tôi thực hiện cắt tỉa không mất mát và kiểm tra trên bộ dữ liệu xác thực. Như được chỉ ra trong Bảng 1, GSM có thể tạo ra các mô hình có tính thưa thớt cao vẫn duy trì độ chính xác. Bằng cách tăng thêm tỷ lệ nén trên LeNet-5 lên 300×, chúng tôi chỉ quan sát thấy sự sụt giảm độ chính xác nhỏ (0.15%), điều này cho thấy GSM có thể mang lại hiệu suất hợp lý với tỷ lệ nén cực cao.

CIFAR-10. Chúng tôi trình bày kết quả của một bộ thực nghiệm khác trên CIFAR-10 trong Bảng 2 sử dụng ResNet-56 [23] và DenseNet-40 [27]. Chúng tôi sử dụng μ = 0.98, kích thước batch 64 và tốc độ học η = 5×10^-3, 5×10^-4, 5×10^-5 cho 400, 100 và 100 epoch, tương ứng. Chúng tôi áp dụng tăng cường dữ liệu tiêu chuẩn bao gồm đệm lên 40×40, cắt ngẫu nhiên và lật trái-phải. Mặc dù ResNet-56 và DenseNet-40 sâu hơn và phức tạp hơn đáng kể, GSM cũng có thể giảm các tham số 10× và vẫn duy trì độ chính xác.

ImageNet. Chúng tôi cắt tỉa ResNet-50 để xác minh GSM trên các ứng dụng nhận dạng hình ảnh quy mô lớn. Chúng tôi sử dụng kích thước batch 64 và huấn luyện mô hình với η = 1×10^-3, 1×10^-4, 1×10^-5 cho 40, 10 và 10 epoch, tương ứng. Chúng tôi so sánh kết quả với L-OBS [13], đây là phương pháp trước đây duy nhất có thể so sánh đã báo cáo kết quả thực nghiệm trên ResNet-50, theo hiểu biết tốt nhất của chúng tôi. Rõ ràng, GSM vượt trội hơn L-OBS với một biên độ rõ ràng (Bảng 3). Chúng tôi giả định rằng hiệu quả của GSM trên một mạng rất sâu như vậy là do khả năng khám phá tỷ lệ thưa thớt phù hợp theo từng lớp, cho một tỷ lệ nén toàn cục mong muốn. Ngược lại, L-OBS thực hiện cắt tỉa từng lớp bằng cùng một tỷ lệ nén. Giả định này được xác minh thêm trong Phần 4.2.

--- TRANG 4 ---
Bảng 1: Kết quả cắt tỉa trên MNIST.
Mô hình | Kết quả | Top1 cơ sở | Top1 đã cắt tỉa | Tham số gốc/còn lại | Tỷ lệ nén | Tỷ lệ khác không
LeNet-300 | Han et al. [21] | 98.36 | 98.41 | 267K / 22K | 12.1× | 8.23%
LeNet-300 | L-OBS [13] | 98.24 | 98.18 | 267K / 18.6K | 14.2× | 7%
LeNet-300 | Zhang et al. [56] | 98.4 | 98.4 | 267K / 11.6K | 23.0× | 4.34%
LeNet-300 | DNS [18] | 97.72 | 98.01 | 267K / 4.8K | 55.6× | 1.79%
LeNet-300 | GSM | 98.19 | 98.18 | 267K / 4.4K | 60.0× | 1.66%
LeNet-5 | Han et al. [21] | 99.20 | 99.23 | 431K / 36K | 11.9× | 8.35%
LeNet-5 | L-OBS [13] | 98.73 | 98.73 | 431K / 3.0K | 14.1× | 7%
LeNet-5 | Srinivas et al. [47] | 99.20 | 99.19 | 431K / 22K | 19.5× | 5.10%
LeNet-5 | Zhang et al. [56] | 99.2 | 99.2 | 431K / 6.05K | 71.2× | 1.40%
LeNet-5 | DNS [18] | 99.09 | 99.09 | 431K / 4.0K | 107.7× | 0.92%
LeNet-5 | GSM | 99.21 | 99.22 | 431K / 3.4K | 125.0× | 0.80%
LeNet-5 | GSM | 99.21 | 99.06 | 431K / 1.4K | 300.0× | 0.33%

Bảng 2: Kết quả cắt tỉa trên CIFAR-10.
Mô hình | Kết quả | Top1 cơ sở | Top1 đã cắt tỉa | Tham số gốc/còn lại | Tỷ lệ nén | Tỷ lệ khác không
ResNet-56 | GSM | 94.05 | 94.10 | 852K / 127K | 6.6× | 15.0%
ResNet-56 | GSM | 94.05 | 93.80 | 852K / 85K | 10.0× | 10.0%
DenseNet-40 | GSM | 93.86 | 94.07 | 1002K / 150K | 6.6× | 15.0%
DenseNet-40 | GSM | 93.86 | 94.02 | 1002K / 125K | 8.0× | 12.5%
DenseNet-40 | GSM | 93.86 | 93.90 | 1002K / 100K | 10.0× | 10.0%

4.2 GSM cho quyết định tỷ lệ thưa thớt tự động theo từng lớp
Các DNN hiện đại thường chứa hàng chục hoặc thậm chí hàng trăm lớp. Khi kiến trúc trở nên sâu hơn, việc đặt tỷ lệ thưa thớt theo từng lớp theo cách thủ công để đạt được tỷ lệ nén toàn cục mong muốn trở nên ngày càng không thực tế. Do đó, cộng đồng nghiên cứu đang tìm kiếm các kỹ thuật có thể tự động khám phá tỷ lệ thưa thớt phù hợp trên các mô hình rất sâu. Trong thực tế, chúng tôi nhận thấy rằng nếu việc cắt tỉa trực tiếp một lớp duy nhất của mô hình gốc theo một tỷ lệ cố định dẫn đến việc giảm độ chính xác đáng kể, GSM tự động chọn cắt tỉa nó ít hơn, và ngược lại.

Trong phần này, chúng tôi trình bày một phân tích định lượng về độ nhạy cảm với việc cắt tỉa, đây là một tính chất cơ bản của một lớp được xác định thông qua một proxy tự nhiên: việc giảm độ chính xác do cắt tỉa một tỷ lệ nhất định các tham số từ nó. Chúng tôi đầu tiên đánh giá độ nhạy cảm như vậy thông qua các thử nghiệm cắt tỉa một lớp với các tỷ lệ cắt tỉa khác nhau (Hình 2). Ví dụ, đối với đường cong được gán nhãn "cắt tỉa 90%" của LeNet-5, chúng tôi đầu tiên thử nghiệm trên lớp đầu tiên bằng cách đặt 90% các tham số có độ lớn nhỏ hơn về không, sau đó kiểm tra trên tập xác thực. Sau đó chúng tôi khôi phục lớp đầu tiên, cắt tỉa lớp thứ hai và kiểm tra. Cùng một quy trình được áp dụng cho lớp thứ ba và thứ tư. Sau đó, chúng tôi sử dụng các tỷ lệ cắt tỉa khác nhau là 99%, 99.5%, 99.7%, và thu được ba đường cong theo cách tương tự. Từ các thực nghiệm như vậy, chúng ta học được rằng lớp đầu tiên nhạy cảm hơn nhiều so với lớp thứ ba, vì việc cắt tỉa 99% tham số từ lớp đầu tiên làm giảm độ chính xác Top1 khoảng 85% (tức là chỉ trên 10%), nhưng làm như vậy trên lớp thứ ba chỉ làm giảm nhẹ độ chính xác 3%.

Sau đó chúng tôi chỉ ra tỷ lệ khác không theo từng lớp kết quả của các mô hình được cắt tỉa bằng GSM (LeNet-5 được cắt tỉa 125× và DenseNet-40 được cắt tỉa 6.6×, như được báo cáo trong Bảng 1, 2) như một proxy khác cho độ nhạy cảm, trong đó các đường cong được gán nhãn "GSM discovered" trong Hình 2. Vì hai đường cong thay đổi theo cùng xu hướng qua các lớp như những đường cong khác, chúng tôi phát hiện ra rằng các độ nhạy cảm được đo trong hai proxy có liên quan chặt chẽ, điều này cho thấy GSM tự động quyết định cắt tỉa các lớp nhạy cảm ít hơn (ví dụ: lớp thứ 14, 27 và 40 trong DenseNet-40, thực hiện chuyển đổi giữa các giai đoạn [27]) và các lớp không nhạy cảm nhiều hơn để đạt được tỷ lệ nén toàn cục mong muốn, loại bỏ nhu cầu công việc thủ công nặng nề để điều chỉnh tỷ lệ thưa thớt làm siêu tham số.

Bảng 3: Kết quả cắt tỉa trên ImageNet.
Mô hình | Kết quả | Top1/Top5 cơ sở | Top1/Top5 đã cắt tỉa | Tham số gốc/còn lại | Tỷ lệ nén | Tỷ lệ khác không
ResNet-50 | L-OBS[13] | - / 92 | - / 92 | 25.5M / 16.5M | 1.5× | 65%
ResNet-50 | L-OBS[13] | - / 92 | - / 85 | 25.5M / 11.4M | 2.2× | 45%
ResNet-50 | GSM | 75.72 / 92.75 | 75.33 / 92.47 | 25.5M / 6.3M | 4.0× | 25%
ResNet-50 | GSM | 75.72 / 92.75 | 74.30 / 91.98 | 25.5M / 5.1M | 5.0× | 20%

--- TRANG 5 ---
[Hình 2: Điểm độ nhạy cảm của lớp được ước tính bởi cả thử nghiệm cắt tỉa theo từng lớp và GSM trên LeNet-5 (trái) và DenseNet-40 (phải). Mặc dù các giá trị số của hai proxy độ nhạy cảm trên cùng một lớp không thể so sánh được, chúng thay đổi theo cùng xu hướng qua các lớp.]

4.3 Momentum để tăng tốc việc làm không tham số
Chúng tôi điều tra vai trò của momentum trong GSM bằng cách chỉ thay đổi hệ số momentum và giữ nguyên tất cả các cấu hình huấn luyện khác như DenseNet-40 được cắt tỉa 8× trong Phần 4.1. Trong quá trình huấn luyện, chúng tôi đánh giá mô hình cả trước và sau khi cắt tỉa mỗi 8000 lần lặp (tức là 10.24 epoch). Chúng tôi cũng trình bày trong Hình 3 tỷ lệ toàn cục của các tham số có độ lớn dưới 1×10^-3 và 1×10^-4, tương ứng. Có thể quan sát thấy, một hệ số momentum lớn có thể làm tăng đáng kể tỷ lệ các tham số có độ lớn nhỏ. Ví dụ, với tỷ lệ nén mục tiêu 8× và μ = 0.98, GSM có thể làm cho 87.5% tham số gần với không (dưới 1×10^-4) trong khoảng 150 epoch, do đó việc cắt tỉa mô hình không gây thiệt hại. Và với μ = 0.90, 400 epoch không đủ để làm không các tham số một cách hiệu quả, do đó việc cắt tỉa làm giảm độ chính xác xuống khoảng 65%. Mặt khác, vì một giá trị μ lớn hơn mang lại sự thay đổi cấu trúc nhanh hơn trong mô hình, độ chính xác ban đầu giảm ở đầu nhưng tăng khi sự thay đổi như vậy trở nên ổn định và việc huấn luyện hội tụ.

[Hình 3: Các đường cong độ chính xác được thu được bằng cách đánh giá cả mô hình gốc và được cắt tỉa toàn cục 8×, và tỷ lệ các tham số có độ lớn dưới 1×10^-3 hoặc 1×10^-4, tương ứng, sử dụng các giá trị khác nhau của hệ số momentum μ. Xem tốt nhất trong màu.]

4.4 GSM để kích hoạt lại kết nối ngầm
GSM ngầm triển khai việc nối lại kết nối bằng cách thực hiện lựa chọn kích hoạt ở mỗi lần lặp để khôi phục các tham số đã bị phạt sai (tức là đã trải qua ít nhất một lần cập nhật thụ động). Chúng tôi điều tra tầm quan trọng của việc làm như vậy bằng cách cắt tỉa DenseNet-40 8× một lần nữa sử dụng μ = 0.98 và các cấu hình huấn luyện giống như trước nhưng không có lựa chọn lại (Hình 4). Cụ thể, chúng tôi sử dụng các ma trận mặt nạ được tính toán ở lần lặp đầu tiên để hướng dẫn các cập nhật cho đến cuối quá trình huấn luyện. Có thể quan sát thấy rằng nếu lựa chọn lại bị hủy, mất mát huấn luyện trở nên cao hơn, và độ chính xác bị giảm. Điều này là do việc lựa chọn đầu tiên quyết định loại bỏ một số kết nối không quan trọng cho lần lặp đầu tiên nhưng có thể quan trọng cho các ví dụ đầu vào tiếp theo. Không có lựa chọn lại, GSM khăng khăng làm không các tham số như vậy, dẫn đến độ chính xác thấp hơn. Và bằng cách mô tả tỷ lệ kích hoạt lại (tức là tỷ lệ số lượng tham số chuyển từ thụ động sang chủ động so với tổng số tham số) ở việc lựa chọn lại của mỗi lần lặp huấn luyện, chúng ta học được rằng việc kích hoạt lại xảy ra trên thiểu số các kết nối, và tỷ lệ giảm dần, sao cho việc huấn luyện hội tụ và tỷ lệ thưa thớt mong muốn được thu được.

[Hình 4: Quá trình huấn luyện của GSM cả có và không có lựa chọn lại.]

4.5 GSM cho vé trúng thưởng mạnh mẽ hơn
Frankle và Carbin [15] báo cáo rằng các tham số được phát hiện là quan trọng sau khi huấn luyện thực sự quan trọng từ đầu (sau khi khởi tạo ngẫu nhiên nhưng trước khi huấn luyện), được gọi là vé trúng thưởng, vì chúng đã thắng trong xổ số khởi tạo. Người ta phát hiện ra rằng nếu chúng ta 1) khởi tạo ngẫu nhiên một mạng được tham số hóa bởi Θ_0, 2) huấn luyện và thu được Θ, 3) cắt tỉa một số tham số từ Θ dẫn đến một mạng con được tham số hóa bởi Θ', 4) đặt lại các tham số còn lại trong Θ' về giá trị ban đầu của chúng trong Θ_0, được gọi là vé trúng thưởng Θ̂, 5) cố định các tham số khác về không và chỉ huấn luyện Θ̂, chúng ta có thể đạt được mức độ chính xác tương đương với mô hình đã huấn luyện rồi cắt tỉa Θ'. Trong công trình đó, bước thứ ba được hoàn thành bằng cách đơn giản bảo tồn các tham số có độ lớn lớn nhất trong Θ. Chúng tôi phát hiện ra rằng GSM có thể tìm ra một bộ vé trúng thưởng tốt hơn, vì việc huấn luyện các vé được khám phá bởi GSM mang lại độ chính xác cuối cùng cao hơn so với những vé được tìm thấy bằng độ lớn (Bảng 4). Cụ thể, chúng tôi chỉ thay thế bước 3 bằng một quá trình cắt tỉa thông qua GSM trên Θ, và sử dụng các tham số khác không kết quả làm Θ', và tất cả các thiết lập thực nghiệm khác được giữ nguyên để có thể so sánh. Thú vị thay, 100% tham số trong lớp kết nối đầy đủ đầu tiên của LeNet-5 được cắt tỉa bởi cắt tỉa độ lớn 300×, sao cho các vé trúng thưởng được tìm thấy hoàn toàn không thể huấn luyện được. Nhưng GSM vẫn có thể tìm ra các vé trúng thưởng hợp lý. Thêm chi tiết thực nghiệm có thể được tìm thấy trong mã nguồn.

Bảng 4: Độ chính xác Top1 cuối cùng của việc huấn luyện vé trúng thưởng (bước 5).
Mô hình | Tỷ lệ nén | Vé độ lớn | Vé GSM
LeNet-300 | 60× | 97.39 | 98.22
LeNet-5 | 125× | 97.60 | 99.04
LeNet-5 | 300× | 11.35 | 98.88

Hai giải thích có thể cho sự vượt trội của GSM là 1) GSM phân biệt các tham số không quan trọng bằng lựa chọn kích hoạt sớm hơn nhiều (ở mỗi lần lặp) so với tiêu chí dựa trên độ lớn (sau khi hoàn thành huấn luyện), và 2) GSM quyết định các vé trúng thưởng cuối cùng theo cách mạnh mẽ với sai lầm (tức là thông qua lựa chọn kích hoạt lại). Trực giác là vì chúng ta mong đợi tìm ra các tham số đã "thắng xổ số khởi tạo", thời điểm khi chúng ta đưa ra quyết định nên gần hơn với khi việc khởi tạo diễn ra, và chúng ta mong muốn sửa chữa các sai lầm ngay lập tức khi chúng ta nhận thức được các quyết định sai. Frankle và Carbin cũng lưu ý rằng có thể mang lại lợi ích khi cắt tỉa càng sớm càng tốt [15], đây chính xác là điều GSM làm, vì GSM tiếp tục đẩy các tham số không quan trọng liên tục về không từ đầu.

5 Kết luận
Chúng tôi đã đề xuất Global Sparse Momentum SGD (GSM) để thay đổi trực tiếp dòng gradient cho việc cắt tỉa DNN, chia việc cập nhật dựa trên momentum-SGD thông thường thành hai phần: cập nhật chủ động sử dụng các gradient được tính từ hàm mục tiêu để duy trì độ chính xác của mô hình, và cập nhật thụ động chỉ thực hiện phân rã trọng số được tăng tốc bởi momentum để đẩy các tham số dư thừa vô cùng gần với không. GSM được đặc trưng bởi huấn luyện đầu cuối đến đầu cuối, triển khai dễ dàng, cắt tỉa không mất mát, nối lại kết nối ngầm, khả năng tự động khám phá tỷ lệ thưa thớt phù hợp cho từng lớp trong các mạng nơ-ron rất sâu hiện đại và khả năng tìm ra những vé trúng thưởng mạnh mẽ.

--- TRANG 6 ---
Lời cảm ơn
Chúng tôi chân thành cảm ơn tất cả các nhà đánh giá về những nhận xét của họ. Công trình này được hỗ trợ bởi Chương trình R&D Trọng điểm Quốc gia Trung Quốc (Số 2018YFC0807500), Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số 61571269, Số 61971260), Chương trình Quốc gia về Nhân tài Sáng tạo Sau tiến sĩ (Số BX20180172), và Quỹ Khoa học Sau tiến sĩ Trung Quốc (Số 2018M640131). Tác giả liên hệ: Guiguang Ding, Jungong Han.

Tài liệu tham khảo
[1] Jose M Alvarez và Mathieu Salzmann. Learning the number of neurons in deep networks. In Advances in Neural Information Processing Systems, trang 2270–2278, 2016.

[2] Jose M Alvarez và Mathieu Salzmann. Compression-aware training of deep networks. In Advances in Neural Information Processing Systems, trang 856–867, 2017.

[3] Jimmy Ba và Rich Caruana. Do deep nets really need to be deep? In Advances in neural information processing systems, trang 2654–2662, 2014.

[4] Yoshua Bengio và Yann LeCun, biên tập. 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.

[5] Yoshua Bengio và Yann LeCun, biên tập. 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.

[6] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, và cộng sự. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends® in Machine learning, 3(1):1–122, 2011.

[7] Giovanna Castellano, Anna Maria Fanelli, và Marcello Pelillo. An iterative pruning algorithm for feedforward neural networks. IEEE transactions on Neural networks, 8(3):519–531, 1997.

[8] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, và Yoshua Bengio. Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1. arXiv preprint arXiv:1602.02830, 2016.

[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, và Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, trang 248–255. IEEE, 2009.

[10] Xiaohan Ding, Guiguang Ding, Yuchen Guo, và Jungong Han. Centripetal sgd for pruning very deep convolutional networks with complicated structure. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, trang 4943–4953, 2019.

[11] Xiaohan Ding, Guiguang Ding, Yuchen Guo, Jungong Han, và Chenggang Yan. Approximated oracle filter pruning for destructive cnn width optimization. In International Conference on Machine Learning, trang 1607–1616, 2019.

[12] Xiaohan Ding, Guiguang Ding, Jungong Han, và Sheng Tang. Auto-balanced filter pruning for efficient convolutional neural networks. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.

[13] Xin Dong, Shangyu Chen, và Sinno Pan. Learning to prune deep neural networks via layer-wise optimal brain surgeon. In Advances in Neural Information Processing Systems, trang 4857–4867, 2017.

[14] Mikhail Figurnov, Aizhan Ibraimova, Dmitry P Vetrov, và Pushmeet Kohli. Perforatedcnns: Acceleration through elimination of redundant convolutions. In Advances in Neural Information Processing Systems, trang 947–955, 2016.

[15] Jonathan Frankle và Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In Bengio và LeCun [5].

[16] Gabriel Goh. Why momentum really works. Distill, 2017.

[17] Ariel Gordon, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, và Edward Choi. Morphnet: Fast & simple resource-constrained structure learning of deep networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, trang 1586–1595, 2018.

[18] Yiwen Guo, Anbang Yao, và Yurong Chen. Dynamic network surgery for efficient dnns. In Advances In Neural Information Processing Systems, trang 1379–1387, 2016.

[19] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, và Pritish Narayanan. Deep learning with limited numerical precision. In International Conference on Machine Learning, trang 1737–1746, 2015.

[20] Song Han, Huizi Mao, và William J. Dally. Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. In Yoshua Bengio và Yann LeCun, biên tập, 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016.

[21] Song Han, Jeff Pool, John Tran, và William Dally. Learning both weights and connections for efficient neural network. In Advances in Neural Information Processing Systems, trang 1135–1143, 2015.

[22] Babak Hassibi và David G Stork. Second order derivatives for network pruning: Optimal brain surgeon. In Advances in neural information processing systems, trang 164–171, 1993.

[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, trang 770–778, 2016.

[24] Yihui He, Xiangyu Zhang, và Jian Sun. Channel pruning for accelerating very deep neural networks. In International Conference on Computer Vision (ICCV), tập 2, trang 6, 2017.

[25] Geoffrey Hinton, Oriol Vinyals, và Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.

[26] Hengyuan Hu, Rui Peng, Yu-Wing Tai, và Chi-Keung Tang. Network trimming: A data-driven neuron pruning approach towards efficient deep architectures. arXiv preprint arXiv:1607.03250, 2016.

[27] Gao Huang, Zhuang Liu, Laurens van der Maaten, và Kilian Q. Weinberger. Densely connected convolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, trang 2261–2269. IEEE Computer Society, 2017.

[28] Ayoosh Kathuria. Intro to optimization in deep learning: Momentum, rmsprop and adam. https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/, 2018.

[29] Alex Krizhevsky và Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.

[30] Yann LeCun, Léon Bottou, Yoshua Bengio, và Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

[31] Yann LeCun, John S Denker, và Sara A Solla. Optimal brain damage. In Advances in neural information processing systems, trang 598–605, 1990.

[32] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, và Hans Peter Graf. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016.

[33] Shaohui Lin, Rongrong Ji, Yuchao Li, Cheng Deng, và Xuelong Li. Towards compact convnets via structure-sparsity regularized filter pruning. arXiv preprint arXiv:1901.07827, 2019.

[34] Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, và Marianna Pensky. Sparse convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, trang 806–814, 2015.

[35] Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Tim Kwang-Ting Cheng, và Jian Sun. Metapruning: Meta learning for automatic neural network channel pruning. arXiv preprint arXiv:1903.10258, 2019.

[36] Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, và Kwang-Ting Cheng. Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm. In Proceedings of the European Conference on Computer Vision (ECCV), trang 722–737, 2018.

[37] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, và Changshui Zhang. Learning efficient convolutional networks through network slimming. In 2017 IEEE International Conference on Computer Vision (ICCV), trang 2755–2763. IEEE, 2017.

[38] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, và Trevor Darrell. Rethinking the value of network pruning. In Bengio và LeCun [5].

[39] Jian-Hao Luo, Jianxin Wu, và Weiyao Lin. Thinet: A filter level pruning method for deep neural network compression. In Proceedings of the IEEE international conference on computer vision, trang 5058–5066, 2017.

[40] Michaël Mathieu, Mikael Henaff, và Yann LeCun. Fast training of convolutional networks through ffts. In Yoshua Bengio và Yann LeCun, biên tập, 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014.

[41] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, và Jan Kautz. Pruning convolutional neural networks for resource efficient inference. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.

[42] Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1–17, 1964.

[43] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, và Yoshua Bengio. Fitnets: Hints for thin deep nets. In Bengio và LeCun [4].

[44] Volker Roth và Bernd Fischer. The group-lasso for generalized linear models: uniqueness of solutions and efficient algorithms. In Proceedings of the 25th international conference on Machine learning, trang 848–855. ACM, 2008.

[45] Heinz Rutishauser. Theory of gradient methods. In Refined iterative methods for computation of the solution and the eigenvalues of self-adjoint boundary value problems, trang 24–49. Springer, 1959.

[46] Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, và Bhuvana Ramabhadran. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, trang 6655–6659. IEEE, 2013.

[47] Suraj Srinivas, Akshayvarun Subramanya, và R Venkatesh Babu. Training sparse neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, trang 138–145, 2017.

[48] Ilya Sutskever, James Martens, George Dahl, và Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, trang 1139–1147, 2013.

[49] Lucas Theis, Iryna Korshunova, Alykhan Tejani, và Ferenc Huszár. Faster gaze prediction with dense networks and fisher pruning. arXiv preprint arXiv:1801.05787, 2018.

[50] Nicolas Vasilache, Jeff Johnson, Michaël Mathieu, Soumith Chintala, Serkan Piantino, và Yann LeCun. Fast convolutional nets with fbfft: A GPU performance evaluation. In Bengio và LeCun [4].

[51] Huan Wang, Qiming Zhang, Yuehai Wang, và Haoji Hu. Structured pruning for efficient convnets via incremental regularization. arXiv preprint arXiv:1811.08390, 2018.

[52] Yunhe Wang, Chang Xu, Chao Xu, và Dacheng Tao. Beyond filters: Compact feature map for portable deep model. In International Conference on Machine Learning, trang 3703–3711, 2017.

[53] Yunhe Wang, Chang Xu, Shan You, Dacheng Tao, và Chao Xu. Cnnpack: Packing convolutional neural networks in the frequency domain. In Advances in neural information processing systems, trang 253–261, 2016.

[54] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, và Hai Li. Learning structured sparsity in deep neural networks. In Advances in Neural Information Processing Systems, trang 2074–2082, 2016.

[55] Tien-Ju Yang, Yu-Hsin Chen, và Vivienne Sze. Designing energy-efficient convolutional neural networks using energy-aware pruning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, trang 5687–5695, 2017.

[56] Tianyun Zhang, Shaokai Ye, Kaiqi Zhang, Jian Tang, Wujie Wen, Makan Fardad, và Yanzhi Wang. A systematic dnn weight pruning framework using alternating direction method of multipliers. In Proceedings of the European Conference on Computer Vision (ECCV), trang 184–199, 2018.

[57] Xiangyu Zhang, Jianhua Zou, Kaiming He, và Jian Sun. Accelerating very deep convolutional networks for classification and detection. IEEE transactions on pattern analysis and machine intelligence, 38(10):1943–1955, 2016.
