# 2305.18240.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/optimizer/2305.18240.pdf
# File size: 1644804 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1
XGrad: Boosting Gradient-Based Optimizers with
Weight Prediction
Lei Guan, Member, IEEE, Dongsheng Li, Yanqi Shi, Jian Meng
Abstract —In this paper, we propose a general deep learning
training framework XGrad which introduces weight prediction
into the popular gradient-based optimizers to boost their conver-
gence and generalization when training the deep neural network
(DNN) models. In particular, ahead of each mini-batch training,
the future weights are predicted according to the update rule
of the used optimizer and are then applied to both the forward
pass and backward propagation. In this way, during the whole
training period, the optimizer always utilizes the gradients w.r.t.
the future weights to update the DNN parameters, making
the gradient-based optimizer achieve better convergence and
generalization compared to the original optimizer without weight
prediction. XGrad is rather straightforward to implement yet
pretty effective in boosting the convergence of gradient-based
optimizers and the accuracy of DNN models. Empirical results
concerning five popular optimizers including SGD with momen-
tum, Adam, AdamW, AdaBelief, and AdaM3 demonstrate the
effectiveness of our proposal. The experimental results validate
that XGrad can attain higher model accuracy than the baseline
optimizers when training the DNN models. The code of XGrad
will be available at: https://github.com/guanleics/XGrad.
Index Terms —Deep learning, gradient-based, optimizer, con-
vergence, generalization, weight prediction
I. I NTRODUCTION
THE training of deep neural network (DNN) models is
to find the optimal parameters using an optimizer which
has a decisive influence on the accuracy of the models. The
gradient-based optimization methods are currently of core
practical importance in deep learning as they can attain rapid
training of modern deep neural network models. Among all
gradient-based optimizers, stochastic gradient descent (SGD)
with momentum [1], [2] and adaptive methods such as
Adam [3] and AdamW [4] are the most popular optimizers
and have become the default choices for training many DNN
models including convolutional neural networks (CNN) [5]–
[8], recurrent neural networks (RNN), graph neural networks
(GNN) [9]–[11], generative adversarial networks (GAN) [12]
and lots of transformer-based DNN models such as Trans-
Manuscript received April 19, 2021; revised August 16, 2021.
This work is sponsored in part by the National Natural Science Foundation
of China under Grant 62025208, and in part by the State Administration
of Science, Technology and Industry for National Defence under Grant
WDZC20235250118.
Lei Guan is with the Department of Mathematics, National University of
Defense Technology, Changsha, China. (E-mail: guanleimath@163.com)
Dongsheng Li and Yanqi Shi are with the National Key Laboratory of Par-
allel and Distributed Computing, National University of Defense Technology.
(E-mails: dsli@nudt.edu.cn,sconcer@outlook.com)
Jian Meng is with the Department of Mathematics, National University of
Defense Technology, Changsha, China. (E-mail:mengjian23@nudt.edu.cn)
Lei Guan is the corresponding author.former [13], BERT [14], Vision Transformer [15], [16] and
GPT-2/3 [17], [18], etc.
When training a DNN model using the gradient-based
optimizers, each mini-batch training generally consists of
one forward pass and one backward propagation, where the
gradients w.r.t. all the parameters (also known as weights) are
computed during the backward propagation. After that, the
generated gradients are utilized by the optimizer to calculate
the update values for all parameters, which are finally applied
to updating the DNN weights. We exemplify this using the
training process of mini-batch SGD [19]. We assume that
the mini-batch size is b, the available neural network weights
at the t-th iteration are θt−1, and the loss function is f(·).
Given a mini-batch training data (xi,yi), the forward pass
first computes the loss with zi=f(θt−1,xi,yi). Then,
the gradients generated in the backward propagation can be
calculated as
gt=1
bbX
i=1∇θf(θt−1,zi). (1)
After that, given the learning rate γt, the DNN weights are
updated via
θt=θt−1−γtgt. (2)
For gradient-based optimization methods, the differences
among different optimization methods lie in that the ways
using the gradients generated by (1) to update model param-
eters are different. To be general, the weights are updated by
θt←θt−1+ ∆θt, where ∆θtdenotes the relative increments
ofθtover θt−1and is computed by the used optimizer.
Figure 1 illustrates the optimization of DNN weights. The
remarkable features of existing gradient-based optimizers can
be summarized as follows. First, the updates of weights
are continuous. Second, each mini-batch uses the currently
available weights to do both forward pass and backward
propagation. Third, in comparison with the current weights,
the updated weights tend to be closer to the optimal point.
In other words, in each mini-batch training, the weights tend
to be updated in a “correct” direction to move towards the
optimal point.
Motivated by the fact that DNN weights are updated in a
continuous manner and the update values calculated by each
gradient-based optimizer should reflect the “correct” direction
for updating the weights, we introduce weight prediction [20],
[21] into the DNN training and, in particular, propose a general
framework XGrad to boost the convergence of gradient-based
optimizers and to improve the accuracy of DNN models.
The XGrad framework is not only very straightforward toarXiv:2305.18240v2  [cs.LG]  7 Apr 2024

--- PAGE 2 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2
Fig. 1. The optimization of DNN weights.
implement but also works well for many commonly used deep
learning optimizers. Previously, we explored weight prediction
for DNN training in a short conference paper [22] where
we introduced weight prediction into the DNN training when
using AdamW as an optimizer. In this paper, we study weight
prediction in detail and enable it to cover a large class
of gradient-based optimizers including SGD with momen-
tum (SGDM) and adaptive methods such as RMSprop [23],
Adam [3], AdamW [4], AdaBelief [24], and AdaM3 [25].
Generally, given the available weights θtand the learning
rateγ, XGrad uses the following formula to predict the
weights:
ˆθt+s≈θt−γ·s·∆θt+1, (3)
where ˆθt+sdenote the approximately predicted weights used
for both forward pass and backward propagation, sis known as
weight prediction steps, and ∆θtcan be calculated according
to the update rule of the used optimizers. In each mini-
batch training, XGrad consists of the following three steps:
1) Ahead of the forward pass, cache the current weights θt
and apply Equation (3) to predict the future weights based on
the specific update rule of the used gradient-based optimizer.
2) Use the predicted weights ˆθt+sto perform both the forward
pass and backward propagation after which the gradients w.r.t.
the parameters of each layer are generated. 3) Recover the
cached weights θtand then update all parameters with the
newly generated gradients.
We conduct extensive experiments to validate the effective-
ness of our proposal. The experiment results demonstrate that
XGrad can improve the model accuracy compared with the
original optimizer. For example, XGrad achieves an average of
0.98% top-1 accuracy improvement over the SGDM optimizer
when training on the CIFAR-10 datset. Compared to Adam,
XGrad also averages a 0.76% accuracy improvement and
obtains a 0.74 higher BLEU score when training GNMT-
8 on WMT-16 EN →De dataset. Similar conclusions can be
drawn when comparing XGrad with AdamW, AdaBelief, and
AdaM3.
The contributions of this paper can be summarized as
follows:
(1) We, for the first time, construct the mathematical rela-
tionship between currently available weights and future
weights after several continuous updates when using
six popular deep learning optimizers including SGDM,
RMSprop, Adam, AdamW, AdaBelief, and AdaM3.
(2) We devise a general workflow for incorporating weight
prediction into the DNN training. To the best of our
knowledge, this is the first time that applies weightprediction strategy to boost the convergence and general-
ization of popular gradient-based optimizers.
(3) We conducted extensive experimental evaluations by us-
ing 19 different DNN models spanning image classifica-
tion, natural language processing, and image generaliza-
tion tasks to validate the effectiveness of our proposal.
The experiment results demonstrate that XGrad works
well for boosting the convergence and generalization
of both SGDM and adaptive methods such as Adam,
AdamW, AdaBelief, and AdaM3.
The rest of this paper is organized as follows: Section II
lays the foundation of weight prediction where we build up
the mathematical relationship between the initial weights and
the future weights after scontinuous updates when respec-
tively using SGDM, RMSprop, Adam, AdamW, AdaBelief,
and AdaM3 as the optimization method. Section III further
constructs the XGrad framework on the basis of Section II.
Extensive experiments are conducted in Section IV to validate
the effectiveness of our proposal. Following that, Section V
briefly presents the related research work about gradient-based
optimizers. Finally, we conclude this paper and discuss the
future work in Section VI.
II. G RADIENT -BASED OPTIMIZERS
In this section, we construct the mathematical relation-
ship between the initial weights (denoted as θ0) and the
future weights after s(s > 1) times of continuous updates
(dubbed as θs) when training the DNN models using popular
gradient-based optimizers including SGDM, RMSprop, Adam,
AdamW, AdaBelief, and AdaM3.
Ahead of any t-th (t≥1) iteration, we assume that the
current available DNN weights are θt−1. Throughout this
paper, we always let γ(γ∈R) denote the learning rate and
refer to λ(λ∈R) as the weight decay.
A. SGD with momentum
We first reformulate the update of SGDM as
θt=θt−1−γ·vt,
s.t.

gt=∇θft(θt−1),
gt=gt+λθt−1,
vt=u·vt−1+ (1−τ)·gt.(4)
where uis the momentum factor and τis the dampening for
momentum.
Letting θ0denote the initial weights of a DNN model, then
in the following stimes of continuous mini-batch training, the
DNN weights are updated via
θ1=θ0−γ·v1,
θ2=θ1−γ·v2,
···
θs=θs−1−γ·vs,(5)
where for any i∈ {1,2,···, s}, we have


gi=∇θfi(θi−1),
gi=gi+λθi−1,
vi=u·vi−1+ (1−τ)·gi.(6)

--- PAGE 3 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3
When summing up all weight update equations in (5), we
have
θs=θ0−γsX
i=1vi,
s.t.

gi=∇θfi(θi−1),
gi=gi+λθi−1,
vi=u·vi−1+ (1−τ)·gi.(7)
B. RMSprop
We reformulate the update of RMSprop as
θt=θt−1−γ·gt√vt+ϵ,
s.t.
gt=∇θft(θt−1),
vt=α·vt−1+ (1−α)·g2
t,(8)
where adenotes the smoothing constant and ϵ(default: 1e−8)
is used to improve numerical stability.
Likewise, during the first stimes of continuous mini-batch
training, the DNN weights are updated via
θ1=θ0−γ·g1√v1+ϵ,
θ2=θ1−γ·g2√v2+ϵ,
···
θs=θs−1−γ·gs√vs+ϵ,(9)
where for any i∈ {1,2,···, s}, we have

gi=∇θfi(θi−1),
vi=α·vi−1+ (1−α)·g2
i.(10)
When summing up all weight update equations in (9), we
have
θs=θ0−sX
i=1γ·gi√vi+ϵ,
s.t.gi=∇θfi(θi−1),
vi=α·vi−1+ (1−α)·g2
i.(11)
C. Adam
We reformulate the update of Adam as
θt=θt−1−γˆmt√ˆvt+ϵ,
s.t.

gt=∇θft(θt−1),
mt=β1·mt−1+ (1−β1)·gt,
vt=β2·vt−1+ (1−β2)·g2
t,
ˆmt=mt
1−βt
1,
ˆvt=vt
1−βt
2.(12)
In (12), mtandvtrefer to the exponential moving average
(EMA) of gtandg2
trespectively, β1andβ2are coefficients
used for computing mtandvtrespectively, ϵis the smoothing
term that can prevent division by zero.When training the DNN weights from θ0, in the following
stimes of continuous mini-batch training, the DNN weights
are updated via
θ1=θ0−γˆm1√ˆv1+ϵ,
θ2=θ1−γˆm2√ˆv2+ϵ,
···
θs=θs−1−γˆms√ˆvs+ϵ,(13)
where for any i∈ {1,2,···, s}, we have


gi=∇θfi(θi−1),
mi=β1·mi−1+ (1−β1)·gi,
vi=β2·vi−1+ (1−β2)·g2
i,
ˆmi=mi
1−βi
1,
ˆvi=vi
1−βi
2.(14)
When summing up all weight update equations in (13), we
have
θs=θ0−sX
i=1γˆmi√ˆvi+ϵ,
s.t.

gi=∇θfi(θi−1),
mi=β1·mi−1+ (1−β1)·gi,
vi=β2·vi−1+ (1−β2)·g2
i,
ˆmi=mi
1−βi
1,
ˆvi=vi
1−βi
2.(15)
D. AdamW
Given the momentum factor β1∈Randβ2∈R, we
reformulate the update of AdamW [4] as
θt= (1−γλ)θt−1−γˆmt√ˆvt+ϵ,
s.t.

gt=∇θft(θt−1),
mt=β1·mt−1+ (1−β1)·gt,
vt=β2·vt−1+ (1−β2)·g2
t,
ˆmt=mt
1−βt
1,
ˆvt=vt
1−βt
2.(16)
Likewise, the first stimes of continuous mini-batch training
can be formulated as
θ1= (1−γλ)θ0−γˆm1√ˆv1+ϵ,
θ2= (1−γλ)θ1−γˆm2√ˆv2+ϵ,
···
θs= (1−γλ)θs−1−γˆms√ˆvs+ϵ,(17)
where for any i∈ {1,2,···, s}, we have


gi=∇θfi(θi−1),
mi=β1·mi−1+ (1−β1)·gi,
vi=β2·vi−1+ (1−β2)·g2
i,
ˆmi=mi
1−βi
1,
ˆvi=vi
1−βi
2.(18)

--- PAGE 4 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4
When summing up all weight update equations in (17), we
have
θs=θ0−γλs−1X
i=0θi−sX
i=1γˆmi√ˆvi+ϵ,
s.t.

gi=∇θfi(θi−1),
mi=β1·mi−1+ (1−β1)·gi,
vi=β2·vi−1+ (1−β2)·g2
i,
ˆmi=mi
1−βi
1,
ˆvi=vi
1−βi
2.(19)
It is well known that the weight decay value λis generally
set to an extremely small value (e.g., 5e−4), and the learning
rateγis commonly set to a value smaller than 1 (e.g., 1e−3).
Consequently, γλis pretty close to zero, and thus, the second
term of the right-hand of (19) can be neglected. This, therefore,
generates the following equation:
θs≈θ0−sX
i=1γˆmi√ˆvi+ϵ.
s.t.

gi=∇θfi(θi−1),
mi=β1·mi−1+ (1−β1)·gi,
vi=β2·vi−1+ (1−β2)·g2
i,
ˆmi=mi
1−βi
1,
ˆvi=vi
1−βi
2.(20)
E. AdaBelief
We reformulate the update of AdaBelief [24] as
θt=θt−1−γˆmt√ˆvt+ϵ,
s.t.

gt=∇θft(θt−1),
mt=β1·mt−1+ (1−β1)·gt,
vt=β2·vt−1+ (1−β2)·(gt−mt)2+ϵ,
ˆmt=mt
1−βt
1,
ˆvt=vt
1−βt
2.(21)
In (21), mtandvtrefer to the EMA of gtand(gt−mt)2
respectively, β1andβ2are coefficients used for computing mt
andvtrespectively, ϵis the smoothing term that can prevent
division by zero.
When training the DNN weights from θ0, in the following
stimes of continuous mini-batch training, the DNN weights
are updated via
θ1=θ0−γˆm1√ˆv1+ϵ,
θ2=θ1−γˆm2√ˆv2+ϵ,
···
θs=θs−1−γˆms√ˆvs+ϵ,(22)
where for any i∈ {1,2,···, s}, we have


gi=∇θfi(θi−1),
mi=β1·mi−1+ (1−β1)·gi,
vi=β2·vi−1+ (1−β2)·(gi−mi)2+ϵ,
ˆmi=mi
1−βi
1,
ˆvi=vi
1−βi
2.(23)When summing up all weight update equations in (22), we
have
θs=θ0−sX
i=1γˆmi√ˆvi+ϵ,
s.t.

gi=∇θfi(θi−1),
mi=β1·mi−1+ (1−β1)·gi,
vi=β2·vi−1+ (1−β2)·(gi−mi)2+ϵ,
ˆmi=mi
1−βi
1,
ˆvi=vi
1−βi
2.(24)
F . AdaM3
We reformulate the update of AdaM3 [25] as
θt=θt−1−γˆmt√ˆvt,
s.t.

gt=∇θft(θt−1),
mt=β1·mt−1+ (1−β1)·gt,
vt=β2·vt−1+ (1−β2)·m2
t+ϵ,
ˆmt=mt
1−βt
1,
ˆvt=vt
1−βt
2.(25)
where ηis the learning rate decay and ϵis used to improve
numerical stability.
The DNN weights are updated via
θ1=θ0−γˆm1√ˆv1,
θ2=θ1−γˆm2√ˆv2,
···
θs=θs−1−γˆms√ˆvs,(26)
where for any i∈ {1,2,···, s}, we have


gi=∇θfi(θi−1),
mi=β1·mi−1+ (1−β1)·gi,
vi=β2·vi−1+ (1−β2)·m2
i+ϵ,
ˆmi=mi
1−βi
1,
ˆvi=vi
1−βi
2.(27)
Summing up all equations in (26) further yields
θs=θ0−sX
i=1γˆmi√ˆvi,
s.t.

gi=∇θfi(θi−1),
mi=β1·mi−1+ (1−β1)·gi,
vi=β2·vi−1+ (1−β2)·m2
i+ϵ,
ˆmi=mi
1−βi
1,
ˆvi=vi
1−βi
2.(28)
III. T HEXG RAD FRAMEWORK
A. Derivation of XGrad
When summarizing Equations (7), (11), (15), (20), (24),
and (28) in Section II, we can immediately reach a common
conclusion. That is, given the initial weights θ0, the weights

--- PAGE 5 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5
after stimes of continuous updates can be approximately
calculated via
θs=θ0−γsX
i=1∆θi, (29)
where ∆θirepresents the relative increments of θioverθi−1.
In each iteration, the gradients of stochastic objective w.r.t.
thei-th mini-batch, i.e.gi=∇θfi(θi−1), can be calculated
after the backward propagation is completed. Given gi, one
can easily compute ∆θiaccording to the update rule of the
used optimization method.
Equation (29) illustrates that given the initial weights θ0,
θsis calculated by letting θ0subtract the learning rate times
the sum of scontinuous relative variation of the weights.
Correspondingly, given θt, the weights of any t-th (t≥1)
iteration, the weights after stimes of continuous updates can
be approximately calculated via
θt+s=θt−γt+sX
i=t+1∆θi. (30)
Equation (30) illustrates that θt+scan be calculated by
letting θtsubtract the sum of scontinuous relative variation
of the weights when θtis available. Note that when using an
effective gradient-based optimizer, the relative increments of
the weights in each iteration should reflect the trend of the
weight updates. Consequently, ∆θiin (30) should reflect the
“correct” direction for updating the weights θi−1because ∆θi
is calculated on the basis of the update rule of the optimizer
and because the weights are updated in a continuous manner
and along the way of inertia directions.
We can therefore replacePt+s
i=t+1∆θiin (30) with s·∆θt+1
in an effort to approximately compute θt+sfor the case when
only θtand∆θt+1are available. Letting ˆθt+sdenote the
approximately predicted weights for θt+s, we get the weight
prediction formula of XGrad as follows.
ˆθt+s≈θt−γ·s·∆θt+1. (31)
It is worth noting that when using Equation (31) as the
weight prediction formula, sdenotes the weight prediction step
which can be manually set and ∆θt+1can be easily computed
according to the type of used optimizer (dubbed as base
optimizer). Table I summarizes the mathematical formulas of
∆θt+1for SGDM, RMSprop, Adam, AdamW, AdaBelief, and
AdaM3. Here, we note that XGrad directly uses the cached
optimizer states of the t-th iteration step to calculate ∆θt+1so
as to avoid computing the gradients when performing weight
prediction. Notably, other gradient-based optimizers such as
AdaGrad [26], AdaBound [27], RAdam [28], and Lion [29]
can be easily incorporated into the proposed framework.
B. Workflow of XGrad
In the following, we showcase how to incorporate weight
prediction into DNN training. Algorithm 1 illustrates the de-
tails of training the DNN models using the XGrad framework.
The weight prediction steps sand other hyper-parameters are
required before the DNN training starts. At each iteration,
the currently available weights θtshould be cached beforeTABLE I
THE CALCULATION OF ∆θt+1FOR SGDM, RMS PROP , ADAM , ADAM W,
ADABELIEF ,AND ADAM3.
Optimizer ∆θt+1
SGDM∆θt+1=vt,
s.t.
gt=∇θft(θt−1),
vt=u·vt−1+ (1−τ)·gt.
RMSprop*∆θt+1=gt√vt+ϵ,
s.t.gt=∇θft(θt−1),
vt=α·vt−1+ (1−α)·g2
t.
Adam*∆θt+1=ˆmt√ˆvt+ϵ,
s.t.

gt=∇θft(θt−1),
mt=β1·mt−1+ (1−β1)·gt,
vt=β2·vt−1+ (1−β2)·g2
t,
ˆmt=mt
1−βt
1,
ˆvt=vt
1−βt
2.
AdamW*∆θt+1=ˆmt√ˆvt+ϵ,
s.t.

gt=∇θft(θt−1),
mt=β1·mt−1+ (1−β1)·gt,
vt=β2·vt−1+ (1−β2)·g2
t,
ˆmt=mt
1−βt
1,
ˆvt=vt
1−βt
2.
AdaBelief∆θt+1=ˆmt√ˆvt+ϵ,
s.t.

gt=∇θft(θt−1),
mt=β1·mt−1+ (1−β1)·gt,
vt=β2·vt−1+ (1−β2)·(gt−mt)2+ϵ,
ˆmt=mt
1−βt
1,
ˆvt=vt
1−βt
2.
AdaM3∆θt+1=ˆmt√ˆvt,
s.t.

gt=∇θft(θt−1),
mt=β1·mt−1+ (1−β1)·gt,
vt=β2·vt−1+ (1−β2)·m2
t+ϵ,
ˆmt=mt
1−βt
1,
ˆvt=vt
1−βt
2.
*g2
irefers to element-wise square with g2
i=gi⊙gi.
Algorithm 1 The workflow of XGrad
Require: Weight prediction step s, other hyper-parameters
such as γ,β1,β2,ϵ.
1:Initialize or load DNN weights θ0.
2:t←0.
3:while the stopping criterion is not met do
4: Cache the current weights θt.
5: Compute ∆θt+1according to Table I.
6: Calculate ˆθt+susing (31).
7: Do forward pass with ˆθt+s.
8: Do backward propagation with ˆθt+s.
9: Recover the cached weights θt.
10: Update the weights θtusing the specific optimizer.
11: t←t+ 1.
12:end while
the forward pass starts (Line 4). Then the relative increments
ofθt+1over θtare computed according to the update rule
of the used base optimizer by a quick lookup of Table I
(Line 5). After that, weight prediction is performed using
Equation (31) to compute ˆθt+s(Line 6). Following that, the

--- PAGE 6 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6
predicted weights ˆθt+sare directly applied to both forward
pass and backward propagation (Lines 7 and 8). Then, the
cached weights θtare required to be recovered (Line 9).
Finally, the weights are updated by the optimizer using the
gradients generated in the backward propagation (Line 10).
C. Analysis of XGrad
In this section, we give a brief analysis of why XGrad
should be more effective than the base optimizers without
weight prediction. The key point is that XGrad can be regarded
as an approximation of the extragradient (EG) method [30]
where the weight prediction step plays the role of extrapolation
step.
We consider the optimization of Equation (1). At the (t+1)-
th iteration, EG updates the weights with
˜θt=θt−γ· ∇θft(θt),
θt+1=θt−γ· ∇θft(˜θt),(32)
where γis the learning rate and the computation of ˜θtis
known as the extrapolation step.
Meanwhile, for any (t+1)-th iteration, XGrad actually does
the following update:
ˆθt≈θt−γ·s·∆θt+1,
θt+1=θt−γ∇θft(ˆθt).(33)
When comparing Equations (32) and (33), we can see that
˜θtandˆθtare very similar in their calculation formulas. The
main differences between EM and XGrad lie in the calculation
of˜θtandˆθt. EG computes ˜θtby directly performing one time
of gradient descent. In contrast, XGrad computes ˆθtby mainly
utilizing ∆θt+1, which is computed according to the update
rule of the used optimizer and also incorporates the gradient
information.
Similar to ˜θt, the calculation of ˆθtcan be regarded as
an extrapolation step before performing the update of θt.
As a result, we can see that XGrad can be understood as
an approximate approach to EG, where performing weight
prediction can be seen as an extrapolation step in EG. It is
well-known that EG can address many of the non-convergence
issues that afflict gradient descents thanks to the extrapolation
step [31]. Therefore, it is very natural to expect that XGrad
should be more effective than the base optimizers without
weight prediction.
Here, we also note that EG requires computing two gra-
dients for updating the model weights θt. In contrast, XGrad
only requires computing one time of gradients because ∆θt+1
can be directly computed by utilizing the cached optimizer
states (e.g., vtfor SGDM in Table I) of the previous iteration.
IV. E XPERIMENTS
A. Experimental Settings1) Environments: All experiments were conducted on two
machines. The first machine is a multi-GPU platform that is
equipped with four NVIDIA Tesla P100 GPUs, each with
16 GB of memory size. The second machine includes one
NVIDIA Tesla M40 GPU with 24 GB of memory size.
The CPU of both machines is an Intel Xeon E5-2680 with
128GB DDR4-2400 off-chip main memory. We used PyTorch
(v1.12.0) to train all DNN models.
2) Benchmark models: For experimental evaluations, we
used 18 different benchmark models spanning image classi-
fication, natural language processing (NLP), and image gen-
eralization tasks. For the image classification tasks, we used
nine CNN models and Vision Transformer (ViT) [15] as the
benchmark DNN models. The evaluated CNN models include
LeNet [32], AlexNet [8], VGG-11 [5], VGG-16, ResNet-
34 [7], ResNet-101, GoogleNet [6], DenseNet-121 [33], and
Inception-V3 [34]. The ViT model is configured with 6 trans-
former blocks and a patch size of 4. The number of heads
in the multi-head attention layer is 8 and the dimension of
the MLP layer is 512. Both the dropout rate and embedding
dropout rate are set to 0.1. The NLP tasks consist of three
parts. For the language modeling task, we trained LSTM mod-
els with 1, 2, and 3 layers (denoted as LSTM-1, LSTM-2, and
LSTM-3). Each layer was configured with an embedding size
of 400 and 1,150 hidden activations. For machine translation
task, we evaluated GNMT [35] with 8 LSTM layers and 16
LSTM layers (respectively denoted as GNMT-8 and GNMT-
16). Furthermore, we also used BERT [14] with 12 Trans-
former blocks, 768 hidden sizes, and 12 self-attention blocks
(denoted as BERT BASE) as the NLP benchmark model. For
image generalization tasks, we used V AE [36] and Wassertein-
GAN (WAGN) [37] as the evaluated benchmark models.
3) Datasets: We used three image datasets for image
classification tasks, including Fashion-MNIST [38], CIFAR-
10 [39], and CIFAR-100. For NLP tasks, the used datasets
include Penn Treebank Dataset [40], WMT-16 [41] English-
to-German (WMT En →De), and Microsoft Research Para-
phrase Corpus (MRPC). We note that MRPC belongs to the
General Language Understanding Evaluation (GLUE) [42]
benchmarks. For image generalization tasks, we used MNIST
and CIFAR-10 as the benchmark datasets. Table II summarizes
all the evaluated DNN models and the used datasets in the
experiments.
TABLE II
SUMMARY OF EVALUATED DNN MODELS AND THE DATASETS USED IN
THE EXPERIMENTS .
Tasks Models Data Sets
Image
ClassificationLeNet FashionMNIST
Alexnet, VGG-11, VGG-16,
ResNet-34, ResNet-101, GoogleNet,
DenseNet-121, Inception-V3, ViTCIFAR-10,
CIFAR-100
NLPLSTM-1, LSTM-2, LSTM-3 Penn Treebank
GNMT-8, GNMT-16 WMT-16 En →De
BERT BASE MRPC
Image
GeneralizaitonV AE MNIST
WGAN CIFAR-10

--- PAGE 7 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7
4) Comparison settings: To validate the effectiveness of our
proposal, we mainly compare XGrad with five widely used
optimizers including SGDM, Adam, AdamW, AdaBelief, and
AdaM3. We evaluated our proposal with four different weight
prediction steps ( i.e.,s= 1,s= 2,s= 3, and s= 4),
where we respectively denoted them as XGrad-S1, XGrad-
S2, XGrad-S3, and XGrad-S4 for convenience purposes. We
divided the comparisons into five groups, that is, SGDM vs.
XGrad, Adam vs. XGrad, AdamW vs. XGrad, AdaBelief
vs. XGrad, and AdaM3 vs. XGrad. Table III summarizes
the comparison settings. It’s worth noting that XGrad auto-
matically selects the base optimizer according to the type
of compared optimizer. For example, when comparing with
SGDM, XGrad automatically selected SGDM as the base
optimizer and means SGDM with weight prediction. When
comparing with Adam, AdamW, AdaBelief, and AdaM3,
XGrad correspondingly means Adam, AdamW, AdaBelief, and
AdaM3 with weight prediction, respectively.
TABLE III
SUMMARIZATION OF COMPARISON GROUPS .
No. Comparisons Base optimizer of XGrad
1 SGDM vs. XGrad-S1/S2/S3/S4 SGDM
2 Adam vs. XGrad-S1/S2/S3/S4 Adam
3 AdamW vs. XGrad-S1/S2/S3/S4 AdamW
4 AdaBelief vs. XGrad-S1/S2/S3/S4 AdaBelief
5 AdaM3 vs. XGrad-S1/S2/S3/S4 AdaM3
5) Model accuracy measurement: For image classification
tasks, the model accuracy means the top-1 accuracy (higher
is better). For training LSTM-1, LSTM-2, and LSTM-3, the
model accuracy is denoted by the perplexity (lower is better).
For training GNMT models, the model accuracy refers to the
BLEU score (higher is better). For training BERT BASE on
MRPC, the model accuracy is defined as Dev set accuracy.
For training V AE, we use the sum of reconstruction error and
Kullback-Leibler divergence, denoted as total loss (lower is
better), to assess the model accuracy. For training WGAN, we
computed the Frechet Inception Distance (FID) score (lower is
better) between the fake images and the real dataset to assess
the generative models.
B. Convergence and generalization
1) Comparisons of XGrad and SGDM: In this section, we
report the experimental results when training eight CNN mod-
els on the CIFAR-10 and CIFAR-100 datasets respectively. For
all CNN models and ViT, we trained them for 200 epochs with
a mini-batch size of 128. We initialized the learning rate with
1e−2and decreased it by a factor of 10 at the 120th and 150th
epochs. For both XGrad and SGDM, we set momentum with
0.9 and weight decay with 5e−4.
Figure 2 presents the learning curves for training CNN and
ViT models on the CIFAR-10 dataset. Table IV summarizes
the obtained maximum validation top-1 accuracy. Figure 3
shows the learning curves for training CNN models on the
CIFAR-100 dataset. Table V summarizes the obtained maxi-
mum top-1 validation accuracy on CIFAR-100. In all figures
of Figures 2 and 3, we let the red lines illustrate the learningcurves of SGDM and let other colored lines represent the
learning curves of XGrad with different weight prediction
steps.
Based on the observation of Table IV and Figure 2, we
can immediately reach the following conclusions. First, the
learning curves of XGrad with different weight prediction
steps tend to be lower than that of SGDM at the beginning
training period. Yet, as the decaying of the learning rate,
XGrad gradually surpasses SGDM and always gets higher
top-1 accuracy than SGDM at the end of the training pe-
riod. This demonstrates that XGrad tends to be inferior to
SGDM with a large learning rate but outperforms SGDM
with a small learning rate. Second, Table IV illustrates that
XGrad outperforms SGDM on all evaluated DNN models in
terms of the obtained top-1 validation accuracy. In particular,
XGrad achieves consistently higher top-1 validation accu-
racy than SGDM. Compared to SGDM, XGrad respectively
achieves an improvement of 0.42%, 1.81%, 1.43%, 1.00%,
0.57%, 0.53%, 0.52%, 0.92%, and 1.60% top-1 accuracy for
training AlexNet, VGG-11, VGG-16, ResNet-34, ResNet-101,
GoogleNet, DenseNet-121, Inception-V3, and ViT. On aver-
age, our proposal yields 0.98% (up to 1.81%) top-1 accuracy
improvement over SGDM. Third, comparing the experimental
results of XGrad-S1, XGrad-S2, XGrad-S3, and XGrad-S4,
we can see that XGrad with different weight prediction steps
consistently gets good results for all CNN models. The special
case is training ViT on CIFAR-10, where XGrad-S4 performs
poorly and obtains the minimum top-1 accuracy.
We can draw similar conclusions from the observation of
the experiment results shown in Table V and Figure 3. First,
the learning curves of SGDM seem to be higher than that of
XGrad at the beginning of the training but lower than that
of XGrad at the end of the training. Second, Table V illus-
trates that XGrad achieves consistently higher top-1 validation
accuracy than SGDM. Specially, XGrad yields 0.86%, 4.84%,
3.40%, 2.63%, 2.29%, 2.18%, 2.59%, 2.79%, and 5.54% more
top-1 accuracy than SGDM when training AlexNet, VGG-11,
VGG-16, ResNet-34, ResNet-101, GoogleNet, DenseNet-121
and Inception-V3, respectively. That is to say, XGrad can
achieve an average of 3.01% (up to 5.54%) top-1 accuracy
improvement over SGDM. Third, when comparing the ex-
perimental results of XGrad-S1, XGrad-S2, XGrad-S3, and
XGrad-S4, we can see that XGrad always achieves comparable
high performance.
2) Comparisons of XGrad with Adam: In this section,
we selected LeNet, ResNet-34, DenseNet-121, Inception-V3,
LSTM-1, LSTM-2, LSTM-3, GNMT-8, BERT BASE, V AE, and
WGAN as the benchmark models. Concretely, we trained
LeNet on Fashion-MNIST for 50 epochs with a fixed learning
rate of 1e−3and a mini-batch size of 128. We trained ResNet-
34, DenseNet-121, and Inception-V3 on CIFAR-10 for 120
epochs with a mini-batch size of 128. The initial learning
rate was 1e−3and was divided by 10 at the 90th epoch. We
trained LSTM-1, LSTM-2, and LSTM-3 on the Penn Treebank
Dataset for 200 epochs with a mini-batch size of 20. The
initial learning rate was 1e−3and was decayed by 0.1 at the
100th and 145th epochs. We trained GNMT-8 on WMT-16
En→De for 6 epochs with a steady learning rate of 3e−4and

--- PAGE 8 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8
0 25 50 75 100 125 150 175 200
Epochs405060708090T op-1 Acc.(%)SGDM
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4180 190 20090.591.091.5
(a) AlexNet
0 25 50 75 100 125 150 175 200
Epochs30405060708090T op-1 Acc.(%)SGDM
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4180 190 200909192 (b) VGG-11
0 25 50 75 100 125 150 175 200
Epochs2030405060708090T op-1 Acc.(%)SGDM
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4180 190 2009293 (c) VGG-16
0 25 50 75 100 125 150 175 200
Epochs405060708090T op-1 Acc.(%)SGDM
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4180 190 200949596
(d) ResNet-34
0 25 50 75 100 125 150 175 200
Epochs2030405060708090T op-1 Acc.(%)SGDM
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4180 190 2009596 (e) ResNet-101
0 25 50 75 100 125 150 175 200
Epochs556065707580859095T op-1 Acc.(%)SGDM
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4180 190 2009596 (f) GoogleNet
0 25 50 75 100 125 150 175 200
Epochs556065707580859095T op-1 Acc.(%)SGDM
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4180 190 20095.095.596.0
(g) DenseNet-121
0 25 50 75 100 125 150 175 200
Epochs30405060708090T op-1 Acc.(%)SGDM
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4180 190 200949596 (h) Inception-V3
0 25 50 75 100 125 150 175 200
Epochs1020304050607080T op-1 Acc.(%)SGDM
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4 180 190 2008284 (i) ViT
Fig. 2. Validation Top-1 accuracy (higher is better) vs. epochs when training CNNs and ViT on CIFAR-10.
TABLE IV
COMPARISONS OF XG RAD AND SGDM WHEN TRAINING ON CIFAR-10. T HE BEST RESULTS ARE HIGHLIGHTED IN BOLDFACE .
Optimizers AlexNet VGG-11 VGG-16 ResNet-34 ResNet-101 GoogleNet DenseNet-121 Inception-V3 ViT
Maximum Top-1 Accuracy
SGDM 91.00% 90.09% 91.76% 94.74% 95.08% 95.07% 95.25% 94.64% 82.30%
XGrad-S1 91.32% 91.23% 92.36% 95.21% 95.42% 95.43% 95.72% 95.36% 83.71%
XGrad-S2 91.09% 91.34% 92.87% 95.06% 95.44% 95.23% 95.73% 95.39% 83.90 %
XGrad-S3 91.42 % 91.50% 93.04% 95.74 % 95.65 % 95.37% 95.62% 95.40% 81.77%
XGrad-S4 91.01% 91.90 % 93.19 % 95.71% 95.63% 95.60 % 95.77 % 95.56 % 61.96%
TABLE V
COMPARISONS OF XG RAD AND SGDM WHEN TRAINING ON CIFAR-100. T HE BEST RESULTS ARE HIGHLIGHTED IN BOLDFACE .
Optimizers AlexNet VGG-11 VGG-16 ResNet-34 ResNet-101 GoogleNet DenseNet-121 Inception-V3 ViT
Maximum Top-1 Accuracy
SGDM 69.48% 64.42% 68.17% 75.99% 77.02% 77.43% 77.96% 77.60% 58.23%
XGrad-S1 70.34 % 67.33% 71.01% 76.57% 77.76% 78.17% 78.60% 79.25% 61.27%
XGrad-S2 70.31% 68.18% 71.19% 77.05% 78.34% 79.40% 80.19% 78.37% 62.51%
XGrad-S3 70.21% 69.06% 71.57 % 77.66% 79.31 % 79.26% 80.05% 79.09% 63.12%
XGrad-S4 69.48% 69.26 % 70.83% 78.62 % 78.47% 79.61 % 80.55 % 80.39 % 63.77 %
a mini-batch size of 128. We trained BERT BASE on MRPC
for 3 epochs with a learning rate of 2e−5. We trained V AE on
MNIST for 50 epochs with a learning rate of 1e−3and a mini-
batch size of 128. We trained WGAN for 100 epochs with alearning rate of 2e−4. When training WGAN with XGrad, we
empirically only applied weight prediction to the generative
model and not to the discriminative model. Furthermore, for
both XGrad and Adam, unless specially mentioned, we always

--- PAGE 9 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9
0 25 50 75 100 125 150 175 200
Epochs010203040506070T op-1 Acc.(%)SGDM
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4180 190 200697071
(a) AlexNet
0 25 50 75 100 125 150 175 200
Epochs010203040506070T op-1 Acc.(%)SGDM
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4180 190 2006570 (b) VGG-11
0 25 50 75 100 125 150 175 200
Epochs010203040506070T op-1 Acc.(%)SGDM
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4180 190 20067.570.0 (c) VGG-16
0 25 50 75 100 125 150 175 200
Epochs1020304050607080T op-1 Acc.(%)SGDM
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4180 190 2007678
(d) ResNet-34
0 25 50 75 100 125 150 175 200
Epochs1020304050607080T op-1 Acc.(%)SGDM
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4180 190 200767880 (e) ResNet-101
0 25 50 75 100 125 150 175 200
Epochs20304050607080T op-1 Acc.(%)SGDM
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4180 190 2007880 (f) GoogleNet
0 25 50 75 100 125 150 175 200
Epochs20304050607080T op-1 Acc.(%)SGDM
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4180 190 2007880
(g) DenseNet-121
0 25 50 75 100 125 150 175 200
Epochs1020304050607080T op-1 Acc.(%)SGDM
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4180 190 2007880 (h) Inception-V3
0 25 50 75 100 125 150 175 200
Epochs102030405060T op-1 Acc.(%)SGDM
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4 180 190 20060.062.5 (i) ViT
Fig. 3. Validation Top-1 accuracy (higher is better) vs. epochs of training CNNs and ViT on CIFAR-100.
TABLE VI
SUMMARIZATION OF BEST MODEL ACCURACY OF XG RAD AND ADAM . MAXIMUM TOP -1ACCURACY FOR LENET, RESNET-34, D ENSE NET-121, AND
INCEPITON -V3; M INIMUM PERPLEXITY FOR LSTM-1/2/3; M AXIMUM BLEU SCORE FOR GNMT-8; M AXIMUM DEV SET ACCURACY FOR BERT BASE ;
MINIMUM TOTAL LOSS FOR VAE; M INIMUM FID SCORE FOR WGAN. T HE BEST MODEL ACCURACY RESULTS ARE HIGHLIGHTED IN BOLDFACE .
Optimizers LeNet ResNet-34 DenseNet-121 Inception-V3 LSTM-1 LSTM-2 LSTM-3 GNMT-8 BERT BASE V AE WGAN
Best Accuracy
Adam 89.24% 93.30% 92.75% 92.47% 88.08 73.79 69.91 23.84 86.76% 138.67 95.60
XGrad-S1 89.95 % 94.07% 93.75 % 92.69% 87.88 73.22 69.71 24.42 87.25 % 137.72 75.12
XGrad-S2 89.57% 94.15 % 93.48% 92.94 % 88.83 73.87 69.88 24.58 86.52% 137.30 72.90
XGrad-S3 89.51% 94.09% 93.60% 92.56% 88.52 73.36 70.02 24.45 87.01% 137.62 87.17
XGrad-S4 89.94% 93.85% 93.72% 92.11% 88.50 73.37 69.45 24.31 86.52% 138.02 84.61
evaluated them with the default settings, i.e.,γ= 1e−3
and(β1, β2)=(0.9, 0.999). For training LSTM models, we set
λ= 1.2e−4andϵ= 1e−12. For training WGAN, we set
(β1, β2) = (0 .5,0.999) ,ϵ= 1e−12.
Figure 4 illustrates the learning curves about model accuracy
vs. epochs. Table VI summarizes the obtained best model
accuracy. The experiment results verify the effectiveness of
XGrad in boosting the convergence and generalization of
Adam. First, Figures 4a, 4b, 4c, and 4d validate the superiority
of XGrad over Adam when training CNN models. XGrad
always tends to achieve higher accuracy than Adam when
training them with the same number of epochs. Figures 4e,4f, and 4g illustrate similar phenomena. XGrad tends to
demonstrate a smaller perplexity value with the increase of
the epochs. The learning curves depicted in Figures 4h and 4i
show that XGrad tends to get a higher BLEU score and lower
total loss than Adam. The experiment results shown in Ta-
ble VI again validate the effectiveness of XGrad. Compared to
Adam, XGrad respectively achieves an accuracy improvement
of 0.71%, 0.85%, 1.00%, and 0.47% when training LeNet,
ResNet-34, DenseNet-121, and Inception-V3. This means that
XGrad leads to an average of 0.76% accuracy improvement
over Adam. Meanwhile, XGrad achieves 0.20, 0.57, and 0.46
less perplexity than Adam when training LSTM-1, LSTM-

--- PAGE 10 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10
0 10 20 30 40 50
Epochs657075808590T est Acc.(%)Adam
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4
(a) LeNet
0 20 40 60 80 100 120
Epochs405060708090T op-1 Acc.(%)Adam
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4100 110 1209394 (b) ResNet-34
0 20 40 60 80 100 120
Epochs5060708090T op-1 Acc.(%)Adam
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4100 110 120929394 (c) DenseNet-121
0 20 40 60 80 100 120
Epochs30405060708090T op-1 Acc.(%)Adam
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4100 110 1209293
(d) Inception-V3
0 25 50 75 100 125 150 175 200
Epochs8890929496PerplexityAdam
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4 (e) LSTM-1
0 25 50 75 100 125 150 175 200
Epochs73747576777879808182PerplexityAdam
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4 (f) LSTM-2
0 25 50 75 100 125 150 175 200
Epochs7072747678PerplexityAdam
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4
(g) LSTM-3
1 2 3 4 5 6
Epochs2021222324BLEU Score
Adam
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4 (h) GNMT -8
0 10 20 30 40 50
Epochs140145150155160165T otal LossAdam
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4 (i) VAE
Fig. 4. Comparison of Adam and XGrad. Figures 4a, 4b, 4c, and 4d: Top-1 accuracy (higher is better) vs. Epochs; Figures 4e, 4f, and 4g: Perplexity (lower
is better) vs. Epochs; Figure 4h: BLEU score (higher is better) vs. Epochs; Figure 4i: Total loss (lower is better) vs. Epochs.
TABLE VII
SUMMARIZATION OF BEST MODEL ACCURACY OF XG RAD AND ADAM W. M AXIMUM TOP -1ACCURACY FOR LENET, RESNET-34, D ENSE NET-121, AND
INCEPITON -V3; M INIMUM PERPLEXITY FOR LSTM-1/2/3; M AXIMUM BLEU SCORE FOR GNMT-8; M AXIMUM DEV SET ACCURACY FOR BERT BASE ;
MINIMUM TOTAL LOSS FOR VAE; M INIMUM FID SCORE FOR WGAN. T HE BEST MODEL ACCURACY RESULTS ARE HIGHLIGHTED IN BOLDFACE .
Optimizers LeNet ResNet-34 DenseNet-121 Inception-V3 LSTM-1 LSTM-2 LSTM-3 GNMT-8 BERT BASE V AE WGAN
Best Accuracy
AdamW 88.61% 94.03% 93.97% 93.53% 88.28 73.56 69.90 24.14 85.29% 139.03 97.02
XGrad-S1 89.04% 94.06% 94.13% 93.90 % 88.39 73.53 69.75 24.53 85.05% 138.53 74.94
XGrad-S2 88.33% 93.95% 94.04% 93.60% 88.47 73.55 69.81 24.52 84.56% 137.27 75.42
XGrad-S3 89.23 % 94.37% 94.39 % 93.61% 88.62 73.44 69.75 24.50 84.31% 137.84 79.18
XGrad-S4 88.42% 94.44 % 94.29% 93.41% 88.60 73.73 69.72 24.54 87.01 % 138.25 89.40
2, and LSTM-3, respectively. XGrad also obtains a 0.74
higher BLEU score than Adam when training GNMT-8 and
gets 0.49% higher accuracy than Adam when training the
BERT BASE. Furthermore, XGrad gets 1.37 less total loss than
Adam when training V AE and also obtains a much lower FID
score when training WGAN (72.90 vs. 95.60).
3) Comparisons of XGrad with AdamW: In this section, we
again selected LeNet, ResNet-34, DenseNet-121, Inception-
V3, LSTM-1, LSTM-2, LSTM-3, GNMT-8, BERT BASE, V AE,
and WGAN as the benchmark models and evaluated them with
the same experimental settings as described in Section IV-B2.Figure 5 depicts the learning curves about model accuracy
vs. epochs. Table VII summarizes the obtained best model
accuracy.
We can reach the following conclusions based on the
observation of the experiment results. First, XGrad performs
better than AdamW on the image classification tasks. On
average, XGrad yields 0.46% (up to 0.62%) top-1 accuracy
improvement over AdamW. Second, XGrad also outperforms
AdamW on NLP tasks. XGrad performs slightly better than
AdamW on the language modeling tasks. In particular, XGrad
is inferior to AdamW when training LSTM-1 (88.39 vs.

--- PAGE 11 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11
0 10 20 30 40 50
Epochs7678808284868890T est Acc.(%)AdamW
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4
(a) LeNet
0 20 40 60 80 100 120
Epochs5060708090Validation Acc.(%)AdamW
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4100 110 12093.594.094.5 (b) ResNet-34
0 20 40 60 80 100 120
Epochs6065707580859095Validation Acc.(%)AdamW
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4100 110 12093.594.094.5 (c) DenseNet-121
0 20 40 60 80 100 120
Epochs5060708090Validation Acc.(%)AdamW
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4100 110 12093.093.594.0
(d) Inception-V3
0 25 50 75 100 125 150 175 200
Epochs889092949698PerplexityAdamW
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4 (e) LSTM-1
0 25 50 75 100 125 150 175 200
Epochs73747576777879808182PerplexityAdamW
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4 (f) LSTM-2
0 25 50 75 100 125 150 175 200
Epochs7072747678PerplexityAdamW
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4
(g) LSTM-3
1 2 3 4 5 6
Epochs2021222324BLEU Score
AdamW
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4 (h) GNMT -8
0 10 20 30 40 50
Epochs140145150155160165T otal LossAdamW
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4 (i) VAE
Fig. 5. Comparison of AdamW and XGrad. Figures 5a, 5b, 5c, and 5d: Top-1 accuracy (higher is better) vs. Epochs; Figures 5e, 5f, and 5g: Perplexity
(lower is better) vs. Epochs; Figure 5h: BLEU score (higher is better) vs. epochs; Figure 5i: Total loss (lower is better) vs. Epochs.
88.28) but gets 0.12 and 0.18 less perplexity than AdamW
when training LSTM-2 and LSTM-3, respectively. XGrad also
achieves a higher BLEU score (24.54 versus 24.14) when
training GNMT-8 and obtains 1.72% higher accuracy than
AdamW when training BERT BASE on MRPC. Third, for both
the image classification and NLP tasks, the weight prediction
step has a slight effect on the model accuracy generated
by the XGrad. However, the performance of XGrad varies
sharply with different weight prediction steps when training
sophisticated DNN models such as BERT BASE and WGAN.
4) Comparisons of XGrad and AdaBelief: In this sec-
tion, we compared XGrad with AdaBelief using 11 different
DNN models including LeNet, AlexNet, VGG-11, ResNet-34,
DenseNet-121, LSTM-1, LSTM-2, LSTM-3, BERT BASE, V AE,
and WGAN. Figure 6 shows learning curves about model
accuracy vs. epochs. Table VIII lists the obtained best model
accuracy.
The learning curves depicted in Figure 6 again validate the
effectiveness of XGrad. Given the same number of training
epochs, XGrad always tends to achieve better model accuracy
than AdaBelief. When training LeNet, AlexNet, VGG-11,
ResNet-34, and DenseNet-121, XGrad respectively yields atop-1 accuracy improvement of 0.84%, 0.50%, -0.21%, 0.09%,
and 0.11% over AdaBelief, leading to an average of 0.27%
top-1 accuracy improvement. When training LSTM-1, LSTM-
2, and LSTM-3, XGrad consistently achieves lower perplexity
than AdaBelief, averaging 0.41 less perplexity than AdaBelief.
For training BERT BASE on MRPC, XGrad achieves 0.98%
higher Dev set accuracy than AdaBelief. Moreover, compared
to AdaBelief, XGrad respectively gets 1.29 less total loss when
training V AE and 2.69 less FID score when training WGAN.
5) Comparisons of XGrad and AdaM3: As with Sec-
tion IV-B4, we again selected LeNet, AlexNet, VGG-
11, ResNet-34, DenseNet-121, LSTM-1, LSMT-2, LSTM-3,
BERT BASE, V AE, and WGAN as the benchmark DNN models.
Figure 7 depicts the learning curves about model accuracy vs.
epochs and Table IX summarizes the obtained best accuracy.
Similar conclusions can be drawn from the observation
of the experiment results shown in Figure 7 and Table IX.
Compared to AdaM3, XGrad generates a top-1 accuracy im-
provement of 0.89%, 0.29%, 0.13%, -0.01%, and 0.12% when
training LeNet, AlexNet, VGG-11, ResNet-34, and DenseNet-
121, respectively. This gives rise to a top-1 accuracy im-
provement of 0.28% on average. On language modeling tasks,

--- PAGE 12 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12
0 10 20 30 40 50
Epochs60657075808590T est Acc.(%)AdaBelief
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4
(a) LeNet
0 20 40 60 80 100 120
Epochs767880828486889092T op-1 Acc.(%)AdaBelief
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4100 110 1209091 (b) AlexNet
0 20 40 60 80 100 120
Epochs767880828486889092T op-1 Acc.(%)AdaBelief
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4100 110 1209091 (c) VGG-11
0 20 40 60 80 100 120
Epochs7580859095T op-1 Acc.(%)AdaBelief
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4100 110 12094.5094.7595.00
(d) ResNet-34
0 20 40 60 80 100 120
Epochs808284868890929496T op-1 Acc.(%)AdaBelief
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4100 110 12095.095.5 (e) DenseNet-121
0 25 50 75 100 125 150 175 200
Epochs868890929496PerplexityAdam
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4 (f) LSTM-1
0 25 50 75 100 125 150 175 200
Epochs707274767880PerplexityAdaBelief
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4
(g) LSTM-2
0 25 50 75 100 125 150 175 200
Epochs666870727476PerplexityAdaBelief
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4 (h) LSTM-3
0 10 20 30 40 50
Epochs140145150155160T otal LossAdaBelief
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4 (i) VAE
Fig. 6. Comparison of AdaBelief and XGrad. Figures 6a, 6b, 6c, 6d, and 6e: Top-1 accuracy (higher is better) vs. Epochs; Figures 6f, 6g, and 6h:
Perplexity (lower is better) vs. Epochs; Figure 6i: Total loss (lower is better) vs. Epochs.
TABLE VIII
SUMMARIZATION OF BEST MODEL ACCURACY OF XG RAD AND ADABELIEF . MAXIMUM TOP -1ACCURACY FOR LENET, ALEXNET, VGG-11,
RESNET-34, AND DENSE NET-121; M INIMUM PERPLEXITY FOR LSTM-1/2/3; M AXIMUM DEV SET ACCURACY FOR BERT BASE ; M INIMUM TOTAL LOSS
FOR VAE; M INIMUM FID SCORE FOR WGAN. T HE BEST MODEL ACCURACY RESULTS ARE HIGHLIGHTED IN BOLDFACE .
Optimizers LeNet AlexNet VGG-11 ResNet-34 DenseNet-121 LSTM-1 LSTM-2 LSTM-3 BERT BASE V AE WGAN
Best Accuracy
AdaBelief 87.55% 90.63% 91.18 % 94.92% 95.46% 86.09 71.16 66.76 84.56% 138.38 86.66
XGrad-S1 88.17% 90.98% 90.88% 94.89% 95.51% 85.69 71.00 66.60 83.09% 137.37 86.05
XGrad-S2 88.39 % 90.99% 90.97% 94.94% 95.57 % 86.27 70.66 67.02 84.07% 137.09 83.97
XGrad-S3 88.08% 91.13 % 90.62% 95.01 % 95.29% 86.51 71.29 66.72 84.31% 137.81 97.42
XGrad-S4 88.12% 91.02% 90.93% 94.94% 95.50% 85.52 71.17 67.21 85.54 % 138.13 89.79
XGrad achieves less perplexity than AdaM3 when training
LSTM-1 and LSTM-2 and is slightly inferior to AdaM3 when
training LSTM-3. Especially when training BERT BASE, XGrad
achieves significant accuracy improvement over AdaM3 (68.38
vs. 80.64). On image generalization tasks, XGrad gets 14.51
less total loss when training V AE and 22.29 less FID score
when training WGAN. Again, the experiment results demon-
strate that the model accuracy obtained by XGrad varies with
the settings of the weight prediction step, especially when
training WGAN.
6) Discussion about the effectiveness of XGrad: The exper-
iment results demonstrate the effectiveness of XGrad in boost-ing the convergence and generalization of the gradient-based
optimizers including SGDM, Adam, AdamW, AdaBelief, and
AdaM3. In the vast majority of cases, XGrad tends to achieve
better model accuracy than the base optimizers on all evaluated
DNN models, verifying that XGrad can help the optimization
method train to obtain more optimal DNN parameters.
However, applying weight prediction doesn’t mean that the
optimization approach can always move along the optimal
trajectories. Summing up the experiment results, we see that
the performance of XGrad varies with the selection of weight
prediction steps and the evaluated tasks. Different weight
prediction steps may lead to quite a different model accuracy,

--- PAGE 13 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13
0 10 20 30 40 50
Epochs60657075808590T est Acc.(%)Adam
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4
(a) LeNet
0 20 40 60 80 100 120
Epochs7678808284868890T op-1 Acc.(%)AdaM3
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4100 110 1208990 (b) AlexNet
0 20 40 60 80 100 120
Epochs7678808284868890T op-1 Acc.(%)AdaM3
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4100 110 1208990 (c) VGG-11
0 20 40 60 80 100 120
Epochs75.077.580.082.585.087.590.092.595.0T op-1 Acc.(%)AdaM3
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4100 110 120929394
(d) ResNet-34
0 20 40 60 80 100 120
Epochs8082848688909294T op-1 Acc.(%)AdaM3
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4100 110 1209394 (e) DenseNet-121
0 25 50 75 100 125 150 175 200
Epochs100102104106108110112114PerplexityAdaM3
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4 (f) LSTM-1
0 25 50 75 100 125 150 175 200
Epochs848688909294PerplexityAdaM3
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4
(g) LSTM-2
0 25 50 75 100 125 150 175 200
Epochs8082848688PerplexityAdaM3
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4 (h) LSTM-3
0 10 20 30 40 50
Epochs145150155160165170175T otal LossAdaM3
XGrad-S1
XGrad-S2
XGrad-S3
XGrad-S4 (i) VAE
Fig. 7. Comparison of AdaM3 and XGrad. Figures 7a, 7b, 7c, 7d, and 7e: Top-1 accuracy (higher is better) vs. Epochs; Figures 7f, 7g, and 7h: Perplexity
(lower is better) vs. Epochs; Figure 7i: Total loss (lower is better) vs. Epochs.
TABLE IX
SUMMARIZATION OF BEST MODEL ACCURACY OF XG RAD AND ADAM3. M AXIMUM TOP -1ACCURACY FOR LENET, ALEXNET, VGG-11, R ESNET-34,
AND DENSE NET-121; M INIMUM PERPLEXITY FOR LSTM-1/2/3; M AXIMUM DEV SET ACCURACY FOR BERT BASE ; M INIMUM TOTAL LOSS FOR VAE;
MINIMUM FID SCORE FOR WGAN. T HE BEST MODEL ACCURACY RESULTS ARE HIGHLIGHTED IN BOLDFACE .
Optimizers LeNet AlexNet VGG-11 ResNet-34 DenseNet-121 LSTM-1 LSTM-2 LSTM-3 BERT BASE V AE WGAN
Best Accuracy
AdaM3 87.31% 89.96% 89.67% 93.57 % 94.06% 100.73 84.40 79.88 68.38% 160.64 104.56
XGrad-S1 87.50% 90.25 % 89.80 % 93.56% 93.74% 99.92 83.56 79.95 80.64 % 146.13 85.10
XGrad-S2 87.61% 89.92% 89.79% 92.95% 94.18 % 99.69 83.14 79.95 80.39% 162.25 82.27
XGrad-S3 88.20 % 90.05% 89.53% 93.53% 93.78% 100.24 83.53 80.37 83.82% 151.42 119.86
XGrad-S4 87.30% 89.93% 89.46% 92.96% 94.04% 99.98 83.20 80.29 68.38% 156.43 119.42
and some choices of step size may even result in model
accuracy lower than that of the base optimizer. For example,
when comparing XGrad with Adam with the LSTM-1 model,
XGrad-S1 achieves lower perplexity than Adam while XGrad
with step sizes 2, 3, and 4 performs worse than Adam (see Ta-
ble VI). Furthermore, the experiment results also demonstrate
that on sophisticated tasks (e.g., BERT BASE and WGAN), the
performance of XGrad varies sharply when selecting different
weight prediction steps. This reveals that applying weight
prediction is easy to mislead the optimization approach when
training on sophisticated tasks.
We note that the superiority of XGrad over the base opti-
mizers is validated through extensive experimental evaluations.
For most cases, XGrad can obtain better model accuracy thanthe base optimizer without weight prediction. Studying the
optimal weight prediction step size and the impact of complex
training tasks on weight prediction performance will be the
main focus of future research.
C. Computational cost
In this section, we evaluate the computational cost of
XGrad. We mainly compare XGrad with SGDM and Adam.
For the comparison of XGrad and SGDM, we select
VGG-16, ResNet-34, ResNet-101, GoogleNet, DenseNet-121,
Inception-V3, and ViT as the benchmark models. For the com-
parison of XGrad and Adam, we select ResNet-34, DenseNet-
121, Inception-V3, ViT, LSTM-1, LSTM-2, and GNMT-8 as
the benchmark models. For the comparison of XGrad and

--- PAGE 14 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 14
TABLE X
1-EPOCH RUNNING TIME (IN SECONDS )COMPARISON OF XG RAD VS . SGDM.
Optimizers VGG-16 ResNet-34 ResNet-101 GoogleNet DenseNet-121 Inception-V3 ViT
SGDM 12.59 s 36.20 s 131.73 s 94.81 s 82.41 s 191.81 s 40.34 s
XGrad 13.29s 38.25s 139.09s 99.43s 92.70s 196.42s 41.60s
TABLE XI
1-EPOCH RUNNING TIME (IN SECONDS /MINUTES )COMPARISON OF XG RAD VS . ADAM .
Optimizers ResNet-34 DenseNet-121 Inception-V3 ViT LSTM-1 LSTM-2 GNMT-8
Adam 37.01 s 95.34 s 200.63 s 40.72 s 21.03 s 25.38 s 386m
XGrad 44.36s 115.99s 207.78s 43.54s 24.96s 26.43s 400m
/uni00000039/uni0000002a/uni0000002a/uni00000014/uni00000019 /uni00000035/uni00000048/uni00000056/uni00000031/uni00000048/uni00000057/uni00000010/uni00000016/uni00000017 /uni00000035/uni00000048/uni00000056/uni00000031/uni00000048/uni00000057/uni00000010/uni00000014/uni00000013/uni00000014 /uni0000002a/uni00000052/uni00000052/uni0000004a/uni0000004f/uni00000048/uni00000031/uni00000048/uni00000057 /uni00000027/uni00000048/uni00000051/uni00000056/uni00000048/uni00000031/uni00000048/uni00000057/uni00000010/uni00000014/uni00000015/uni00000014 /uni0000002c/uni00000051/uni00000046/uni00000048/uni00000053/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000010/uni00000039/uni00000016 /uni00000039/uni0000004c/uni00000037/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni00000033/uni00000048/uni00000044/uni0000004e/uni00000003/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000026/uni00000052/uni00000051/uni00000056/uni00000058/uni00000050/uni00000053/uni00000057/uni0000004c/uni00000052/uni00000051/uni0000000b/uni0000002a/uni00000025/uni0000000c/uni00000036/uni0000002a/uni00000027/uni00000030
/uni0000003b/uni0000002a/uni00000055/uni00000044/uni00000047
(a) XGrad vs. SGDM
/uni00000035/uni00000048/uni00000056/uni00000031/uni00000048/uni00000057/uni00000010/uni00000016/uni00000017 /uni00000027/uni00000048/uni00000051/uni00000056/uni00000048/uni00000031/uni00000048/uni00000057/uni00000010/uni00000014/uni00000015/uni00000014 /uni0000002c/uni00000051/uni00000046/uni00000048/uni00000053/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000010/uni00000039/uni00000016 /uni00000039/uni0000004c/uni00000037 /uni0000002a/uni00000031/uni00000030/uni00000037/uni00000010/uni0000001b /uni0000002a/uni00000031/uni00000030/uni00000037/uni00000010/uni00000014/uni00000019/uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013/uni0000001a/uni00000011/uni00000018/uni00000014/uni00000013/uni00000011/uni00000013/uni00000014/uni00000015/uni00000011/uni00000018/uni00000014/uni00000018/uni00000011/uni00000013/uni00000014/uni0000001a/uni00000011/uni00000018/uni00000015/uni00000013/uni00000011/uni00000013/uni00000033/uni00000048/uni00000044/uni0000004e/uni00000003/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000026/uni00000052/uni00000051/uni00000056/uni00000058/uni00000050/uni00000053/uni00000057/uni0000004c/uni00000052/uni00000051/uni0000000b/uni0000002a/uni00000025/uni0000000c/uni00000024/uni00000047/uni00000044/uni00000050
/uni0000003b/uni0000002a/uni00000055/uni00000044/uni00000047
(b) XGrad vs. Adam
Fig. 8. Comparisons in terms of Peak GPU memory consumption (in GB). Figure 8a: XGrad vs. SGDM; Figure 8b: XGrad vs. Adam.
SGDM, we trained all the DNN models on CIFAR-10 with
the same experimental settings described in Section IV-B1.
For the comparison of XGrad and Adam, we evaluated all the
DNN models with the same experimental settings described
in Section IV-B2. Table X reports the running time of 1-
epoch training when comparing XGrad with SGDM. Table XI
summarizes the experimental results of the comparison of
XGrad and Adam.
The experimental results shown in both Tables X and XI
demonstrate that XGrad always incurs longer training time
than the original gradient-based optimizer. XGrad incurs an
average of 5.67% (up to 12.48%) longer training time than
SGDM. In comparison with Adam, XGrad leads to 11.21%longer training time on average. The experimental results
are understandable as performing weight prediction in each
iteration leads to additional computations compared to the
original optimizers. Nevertheless, we see that the increased
computations are quite limited and sometimes can even be
neglected.
D. Memory consumption
In this section, we evaluate the memory consumption of
XGrad. We mainly compare the peak GPU memory con-
sumption of XGrad with that of SGDM and Adam. Likewise,
the comparisons are divided into two groups: XGrad vs.
SGDM and XGrad vs. Adam. For the comparison of XGrad

--- PAGE 15 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 15
vs. SGDM, we evaluated seven CNN models on CIFAR-
10 including VGG-16, ResNet-34, ResNet-101, GoogleNet,
DenseNet-121, Inception-V3, and ViT. For the comparisons of
XGrad vs. Adam, the evaluated DNN models include ResNet-
34, DenseNet-121, Inception-V3, ViT, LSTM-2, and GNMT-8.
The experimental settings are the same as in Section IV-C.
Figure 8 illustrates the peak GPU memory consumption of
XGrad and the base optimizers including SGDM and Adam.
From the observation of Figure 8a, we can immediately reach
the following conclusions. First, for any evaluated model,
XGrad always consumes almost the same (only slightly more)
GPU memory as SGDM. On average, XGrad only incurs 1.8%
(up to 3.0%) more GPU memory than SGDM. We can draw
similar conclusions from the experimental results shown in
Figure 8b. XGrad leads to an average of 5.0% (up to 9.7%)
more GPU memory consumption than Adam.
Overall, the experimental results shown in Figure 8 reveal
that XGrad incurs slightly more GPU memory consumption
than the original baseline optimizers. This is reasonable be-
cause XGrad requires storing more intermediate variables in
GPU memory to achieve weight prediction. However, we note
that the increased GPU memory consumption is quite limited.
V. R ELATED WORK
The research on optimization methods always remains a
hotspot in the field of deep learning as the convergence trait of
the used optimizer directly affects convergence and the accu-
racy of the model. Generally, the training procedure of a DNN
model is built upon backpropagation [43] where each layer
of neural network weights is fine-tuned along the backward
propagation of “errors”. Backpropagation always comes along
with gradient descent. Currently, first-order gradient methods,
such as SGDM [2] and adaptive methods [3], [24], [44] are
the most widely used deep learning optimization methods.
SGDM has been regarded as the default optimizer, espe-
cially for image classification tasks [5]–[8]. The momentum
term of SGDM accumulates the gradient information from all
previous iterations, making the weights update along the way
of inertia directions. Another noteworthy feature of SGDM
is that a unified learning rate for all parameters is used
throughout the entire training period.
Noticing that the learning rate plays a significant role in
DNN training, many researchers turn to studying adaptive
methods (also known as adaptive learning methods), which
compute a specific learning rate for each individual param-
eter. In 2011, Duchi et al. [26] proposed AdaGrad, which
dynamically adjusts the learning rate according to the history
gradients from previous iterations and utilizes the quadratic
sum of all previous gradients to update the model parame-
ters. Zeiler [44] proposed AdaDelta, seeking to alleviate the
continual decay of the learning rate of AdaGrad. AdaDelta
does not require manual tuning of a learning rate and is
robust to noisy gradient information. Tieleman and Hinton [23]
refined AdaGrad and proposed RMSprop. The same as Ada-
Grad, RMSprop adjusts the learning rate via element-wise
computation and then updates the variables. One remarkable
feature of RMSprop is that it can avoid decaying the learningrate too quickly. In order to combine the advantages of both
AdaGrad and RMSprop, Kingma and Ba [3] proposed another
famous adaptive gradient method, Adam, which has become
the most important choice for training many Transformer-
based DNN models [3], [4], [23]. Loshchilov and Hutter [4]
found that the major factor of the poor generalization of
Adam is due to that L2regularization for it is not as ef-
fective as for its competitor SGDM. They further proposed
decoupled weight decay regularization for Adam, which is
also known as AdamW. The experimental results demonstrate
that AdamW substantially improves the generalization perfor-
mance of Adam and illustrates competitive performance as
SGDM [2] when tackling image classification tasks. Currently,
Adam as well as AdamW have become the default optimizer
for DNN training. To simultaneously achieve fast convergence
and good generalization, Zhuang et al. [24] proposed another
adaptive gradient method called AdaBelief, which adapts the
stepsize according to the “belief” in the current gradient direc-
tion. Noticing that the second-moment estimation of Adam is
a more favorable option for learning rate scaling than that of
the raw gradient, Wang et al. [25] proposed AdaMomentum.
Reddi et al. [45] studied why RMSProp and Adam probably
do not converge to an optimal solution and proposed a new
exponential moving average variant: AMSGrad. Considering
that Nesterov’s accelerated gradient [46] has a better bound
than gradient descent, Timothy Dozat incorporated Nestorv
momentum into Adam and proposed a new optimization
method named NAdam [47]. Other proposed adaptive methods
include Yogi [48], AdaBound [27], RAdam [28], etc. Chen et
al.[49] studied the convergence of adaptive gradient methods
including the popular algorithms such as Adam, AMSGrad,
and AdaGrad.
Notably, SGDM as well as all adaptive methods share a
common feature: weight updates are performed in a continuous
manner and each mini-batch training always utilizes currently
available weights to do both forward pass and backward prop-
agation. Weight prediction was previously used to overcome
the weight inconsistency and weight staleness issues in the
asynchronous pipeline parallelism. Chen et al. [21] used the
smoothed gradient to replace the true gradient to predict future
weights when using SGDM [2] as the optimizer. Guan et
al. [20] proposed using the update values of Adam [3] to
perform weight predictions. Yet, both approaches use weight
prediction to ensure effective parameter learning in the asyn-
chronous pipeline training rather than studying the impact of
weight prediction on the optimizers.
VI. C ONCLUDING REMARKS
To further boost the convergence and generation of popular
gradient-based optimizers, in this paper, we introduce weight
prediction into the DNN training and propose a new DNN
training framework called XGrad. Our proposal can improve
the convergence and generalization of popular gradient-based
optimizers with a cost of slightly increasing the training time
and GPU memory consumption. The remarkable feature of our
proposal is that we perform both forward pass and backward
propagation using the future weights which are predicted

--- PAGE 16 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 16
according to the specific update rule of the used optimizer. In
particular, we construct the mathematical relationship between
currently available weights and future weights and devise
an effective way to incorporate weight prediction into DNN
training. The proposed framework covers many of the most
frequently used optimizers such as SGDM, RMSprop, Adam,
AdamW, AdaBelief, and AdaM3.
XGrad is easy to implement and works well in boosting
the convergence of DNN training. Extensive experimental
results on image classification, natural language processing,
and image generalization tasks verify the effectiveness of our
proposal. We believe that other adaptive gradient methods such
as AdaBound, RAdam, Yogi, Lion [29], etc can be easily
incorporated into our proposed framework.
For future work, we would like to explore the internal
mechanism of weight prediction from a mathematical per-
spective. Furthermore, we will study the relationship between
the weight prediction steps and the learning rate, as well as
their impacts on the convergence and generalization of DNN
training, especially when training on sophisticated tasks such
as BERT and GAN.
ACKNOWLEDGMENTS
Lei Guan thanks Kangkang Deng at the National University
of Defense Technology for stimulating discussions about the
extragradient method and its relationship with XGrad. Lei
Guan thanks Shaofeng Zhang at Ant Group for helping run
the BERT benchmark.
REFERENCES
[1] N. Qian, “On the momentum term in gradient descent learning algo-
rithms,” Neural networks , vol. 12, no. 1, pp. 145–151, 1999.
[2] I. Sutskever, J. Martens, G. Dahl, and G. Hinton, “On the importance
of initialization and momentum in deep learning,” in International
conference on machine learning . PMLR, 2013, pp. 1139–1147.
[3] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
arXiv preprint arXiv:1412.6980 , 2014.
[4] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”
arXiv preprint arXiv:1711.05101 , 2017.
[5] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” arXiv preprint arXiv:1409.1556 , 2014.
[6] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V . Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”
inProceedings of the IEEE conference on computer vision and pattern
recognition , 2015, pp. 1–9.
[7] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition , 2016, pp. 770–778.
[8] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
with deep convolutional neural networks,” Communications of the ACM ,
vol. 60, no. 6, pp. 84–90, 2017.
[9] Z. Li, K. Meidani, P. Yadav, and A. Barati Farimani, “Graph neural
networks accelerated molecular dynamics,” The Journal of Chemical
Physics , vol. 156, no. 14, p. 144103, 2022.
[10] C. Chen, W. Ye, Y . Zuo, C. Zheng, and S. P. Ong, “Graph networks
as a universal machine learning framework for molecules and crystals,”
Chemistry of Materials , vol. 31, no. 9, pp. 3564–3572, 2019.
[11] K. Choudhary and B. DeCost, “Atomistic line graph neural network for
improved materials property predictions,” npj Computational Materials ,
vol. 7, no. 1, p. 185, 2021.
[12] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y . Bengio, “Generative adversarial networks,”
Communications of the ACM , vol. 63, no. 11, pp. 139–144, 2020.
[13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in
neural information processing systems , vol. 30, 2017.[14] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
of deep bidirectional transformers for language understanding,” arXiv
preprint arXiv:1810.04805 , 2018.
[15] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al. ,
“An image is worth 16x16 words: Transformers for image recognition
at scale,” arXiv preprint arXiv:2010.11929 , 2020.
[16] K. Han, Y . Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y . Tang, A. Xiao,
C. Xu, Y . Xu et al. , “A survey on vision transformer,” IEEE transactions
on pattern analysis and machine intelligence , vol. 45, no. 1, pp. 87–110,
2022.
[17] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al. ,
“Language models are unsupervised multitask learners,” OpenAI blog ,
vol. 1, no. 8, p. 9, 2019.
[18] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al. , “Language mod-
els are few-shot learners,” Advances in neural information processing
systems , vol. 33, pp. 1877–1901, 2020.
[19] L. Bottou, F. E. Curtis, and J. Nocedal, “Optimization methods for large-
scale machine learning,” SIAM review , vol. 60, no. 2, pp. 223–311, 2018.
[20] L. Guan, W. Yin, D. Li, and X. Lu, “Xpipe: Efficient pipeline model par-
allelism for multi-gpu dnn training,” arXiv preprint arXiv:1911.04610 ,
2019.
[21] C.-C. Chen, C.-L. Yang, and H.-Y . Cheng, “Efficient and robust parallel
dnn training through model parallelism on multi-gpu platform,” arXiv
preprint arXiv:1809.02839 , 2018.
[22] L. Guan, “Weight prediction boosts the convergence of adamw,” in 27th
Pacific-Asia Conference on Knowledge Discovery and Data Mining , vol.
13935, 2023, pp. 329–340.
[23] T. Tieleman and G. Hinton, “Lecture 6.5-rmsprop, coursera: Neural
networks for machine learning,” University of Toronto, Technical Report ,
vol. 6, 2012.
[24] J. Zhuang, T. Tang, Y . Ding, S. C. Tatikonda, N. Dvornek, X. Pa-
pademetris, and J. Duncan, “Adabelief optimizer: Adapting stepsizes
by the belief in observed gradients,” Advances in neural information
processing systems , vol. 33, pp. 18 795–18 806, 2020.
[25] Y . Wang, Y . Kang, C. Qin, H. Wang, Y . Xu, Y . Zhang, and Y . Fu,
“Rethinking adam: A twofold exponential moving average approach,”
arXiv preprint arXiv:2106.11514 , 2021.
[26] J. Duchi, E. Hazan, and Y . Singer, “Adaptive subgradient methods
for online learning and stochastic optimization.” Journal of machine
learning research , vol. 12, no. 7, 2011.
[27] L. Luo, Y . Xiong, Y . Liu, and X. Sun, “Adaptive gradient methods
with dynamic bound of learning rate,” arXiv preprint arXiv:1902.09843 ,
2019.
[28] L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han, “On
the variance of the adaptive learning rate and beyond,” arXiv preprint
arXiv:1908.03265 , 2019.
[29] X. Chen, C. Liang, D. Huang, E. Real, K. Wang, Y . Liu, H. Pham,
X. Dong, T. Luong, C.-J. Hsieh et al. , “Symbolic discovery of opti-
mization algorithms,” arXiv preprint arXiv:2302.06675 , 2023.
[30] G. M. Korpelevich, “The extragradient method for finding saddle points
and other problems,” Matecon , vol. 12, pp. 747–756, 1976.
[31] Y .-G. Hsieh, F. Iutzeler, J. Malick, and P. Mertikopoulos, “Explore ag-
gressively, update conservatively: Stochastic extragradient methods with
variable stepsize scaling,” Advances in Neural Information Processing
Systems , vol. 33, pp. 16 223–16 234, 2020.
[32] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE , vol. 86,
no. 11, pp. 2278–2324, 1998.
[33] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely
connected convolutional networks,” in Proceedings of the IEEE confer-
ence on computer vision and pattern recognition , 2017, pp. 4700–4708.
[34] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
the inception architecture for computer vision,” in Proceedings of the
IEEE conference on computer vision and pattern recognition , 2016, pp.
2818–2826.
[35] Y . Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi, W. Macherey,
M. Krikun, Y . Cao, Q. Gao, K. Macherey et al. , “Google’s neural
machine translation system: Bridging the gap between human and
machine translation,” arXiv preprint arXiv:1609.08144 , 2016.
[36] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv
preprint arXiv:1312.6114 , 2013.
[37] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein generative ad-
versarial networks,” in International conference on machine learning .
PMLR, 2017, pp. 214–223.

--- PAGE 17 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 17
[38] H. Xiao, K. Rasul, and R. V ollgraf, “Fashion-mnist: a novel image
dataset for benchmarking machine learning algorithms,” arXiv preprint
arXiv:1708.07747 , 2017.
[39] A. Krizhevsky, G. Hinton et al. , “Learning multiple layers of features
from tiny images,” 2009.
[40] M. A. Marcinkiewicz, “Building a large annotated corpus of english:
The penn treebank,” Using Large Corpora , vol. 273, 1994.
[41] R. Sennrich, B. Haddow, and A. Birch, “Edinburgh neural machine
translation systems for wmt 16,” arXiv preprint arXiv:1606.02891 , 2016.
[42] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,
“Glue: A multi-task benchmark and analysis platform for natural lan-
guage understanding,” arXiv preprint arXiv:1804.07461 , 2018.
[43] G. E. Hinton, S. Osindero, and Y .-W. Teh, “A fast learning algorithm for
deep belief nets,” Neural computation , vol. 18, no. 7, pp. 1527–1554,
2006.
[44] M. D. Zeiler, “Adadelta: an adaptive learning rate method,” arXiv
preprint arXiv:1212.5701 , 2012.
[45] S. J. Reddi, S. Kale, and S. Kumar, “On the convergence of adam and
beyond,” in International Conference on Learning Representations .
[46] Y . E. Nesterov, “A method of solving a convex programming problem
with convergence rate o(1/k2),” in Doklady Akademii Nauk , vol. 269,
no. 3. Russian Academy of Sciences, 1983, pp. 543–547.
[47] T. Dozat, “Incorporating nesterov momentum into adam,” 2016.
[48] M. Zaheer, S. Reddi, D. Sachan, S. Kale, and S. Kumar, “Adaptive
methods for nonconvex optimization,” Advances in neural information
processing systems , vol. 31, 2018.
[49] X. Chen, S. Liu, R. Sun, and M. Hong, “On the convergence of a class
of adam-type algorithms for non-convex optimization,” arXiv preprint
arXiv:1808.02941 , 2018.
