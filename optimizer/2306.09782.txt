# 2306.09782.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/optimizer/2306.09782.pdf
# File size: 1096546 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Full Parameter Fine-tuning for Large Language Models
with Limited Resources
Kai Lv1,2, Yuqing Yang1, Tengxiao Liu1, Qinghui Gao1, Qipeng Guo2*, Xipeng Qiu1
1School of Computer Science, Fudan University
2Shanghai AI Laboratory
{klv21, yuqingyang21, txliu21}@m.fudan.edu.cn
guoqipeng@pjlab.org.cn ,xpqiu@fudan.edu.cn
Abstract
Large Language Models (LLMs) have revolu-
tionized Natural Language Processing (NLP)
but demand massive GPU resources for train-
ing. Lowering the threshold for LLMs training
would encourage greater participation from re-
searchers, benefiting both academia and soci-
ety. While existing approaches have focused on
parameter-efficient fine-tuning, which tunes or
adds a small number of parameters, few have
addressed the challenge of tuning the full pa-
rameters of LLMs with limited resources. In
this work, we propose a new optimizer, LOw-
Memory Optimization ( LOMO ), which fuses
the gradient computation and the parameter up-
date in one step to reduce memory usage. By in-
tegrating LOMO with existing memory saving
techniques, we reduce memory usage to 10.8%
compared to the standard approach (DeepSpeed
solution). Consequently, our approach enables
the full parameter fine-tuning of a 65B model
on a single machine with 8 ×RTX 3090, each
with 24GB memory.1
1 Introduction
Large Language Models (LLMs) have revolution-
ized Natural Language Processing (NLP), demon-
strating remarkable abilities such as emergence and
grokking (Wei et al., 2022), pushing model size to
become larger and larger. However, training these
models with billions of parameters, such as those
with 30B to 175B parameters, raises the bar for
NLP research. Tuning LLMs often requires ex-
pensive GPU resources, such as 8 ×80GB devices,
making it difficult for small labs and companies to
participate in this area of research.
Recently, parameter-efficient fine-tuning meth-
ods (Ding et al., 2022), such as LoRA (Hu et al.,
2022) and Prefix-tuning (Li and Liang, 2021),
provide solutions for tuning LLMs with limited
*Corresponding author.
1Code and data are available at https://github.com/
OpenLMLab/LOMO .resources. However, these methods do not of-
fer a practical solution for full parameter fine-
tuning, which has been acknowledged as a more
powerful approach than parameter-efficient fine-
tuning (Ding et al., 2022; Sun et al., 2023). In this
work, we aim to explore techniques for accomplish-
ing full parameter fine-tuning in resource-limited
scenarios.
We analyze the four aspects of memory usage in
LLMs, namely activation, optimizer states, gradi-
ent tensor and parameters, and optimize the training
process in three folds: 1) We rethink the function-
ality of an optimizer from an algorithmic perspec-
tive and find that SGD is a good replacement in
terms of fine-tuning full parameter for LLMs. This
allows us to remove the entire part of optimizer
states since SGD does not store any intermediate
state (Sec-3.1). 2) Our proposed optimizer, LOMO
as illustrated in Figure 1, reduces the memory us-
age of gradient tensors to O(1), equivalent to the
largest gradient tensor’s memory usage (Sec-3.2).
3) To stabilize mix-precision training with LOMO,
we integrate gradient normalization, loss scaling,
and transition certain computations to full precision
during training (Sec-3.3).
Our technique results in memory usage that
equals the usage of parameters plus activation and
the largest gradient tensor. We push the memory
usage of full parameter fine-tuning to an extreme,
making it merely equivalent to the usage of infer-
ence. This is because the memory usage of the
forward + backward process should not be less
than the forward process alone. It is worth noting
that, when employing LOMO to save memory, we
ensure that the fine-tuning process remains uncom-
promised, as the parameter update process is still
equivalent to SGD.
We empirically assess the memory and through-
put performance of LOMO and show that the us-
age of LOMO enables successful training of a 65B
model with only 8 RTX 3090 GPUs. Additionally,arXiv:2306.09782v2  [cs.CL]  6 Jun 2024

--- PAGE 2 ---
Figure 1: Comparison of SGD and LOMO in backpropagation and parameter update stages. Pirefers to the
parameter of the model and Girefers to the gradient corresponding to Pi. LOMO fused gradient computation and
parameter update in one step to minimize the size of gradient tensors.
to validate the downstream performance of our pro-
posed technique, we apply LOMO to tune the full
parameters of LLMs on the SuperGLUE dataset
collection (Wang et al., 2019). The empirical re-
sults demonstrate the efficiency and effectiveness
of LOMO for optimizing LLMs with billions of pa-
rameters. Overall, our contributions are as follows:
•We provide a theoretical analysis suggesting
that SGD can successfully fine-tune the full
parameters of LLMs. The issues that previ-
ously hindered the widespread usage of SGD
may no longer be severe problems for fine-
tuning LLMs.
•We propose LOw-Memory Optimization,
named LOMO, to significantly save GPU
memory usage without harming the fine-
tuning process.
•Through a thorough evaluation of memory
usage and throughput performance, we empir-
ically validate the effectiveness of LOMO in
optimizing LLMs under resource-constrained
scenarios. This is further supported by perfor-
mance evaluations on downstream tasks.
2 Related Work
In this section, we present related work on memory-
saving techniques during full parameter fine-tuning.These techniques can be effectively combined with
LOMO to further reduce memory consumption.
Activation Checkpointing During vanilla back-
propagation, all activations from the forward pass
are stored in memory to compute gradients. This
can be a significant memory overhead, especially
for large language models. Alternatively, one could
discard all activations and recompute them on de-
mand for gradients computation in order to save
memory. However, this can result in a substan-
tial additional computation cost. Activation check-
pointing (or gradient checkpointing) takes into
account both memory usage and computational
cost, providing a compromise solution (Chen et al.,
2016). The activations of strategically selected
checkpoint nodes in the computational graph are
kept in memory after the forward pass, while the
activations of remaining nodes are recomputed at
most once. The activation memory can be reduced
to the square root of the original amount at the cost
of one extra forward pass.
Mixed-Precision Training Mixed-precision
training has become a prevalent approach for
training large language models due to its ability
to accelerate training speed and reduce memory
footprint (Narayanan et al., 2021; Rajbhandari
et al., 2020). By employing half-precision
storage for parameters, activations, and gradients,

--- PAGE 3 ---
mixed-precision training enables high-throughput
computations. In order to uphold stability and
model accuracy, Micikevicius et al. (2018)
proposed three techniques which involve the use of
full precision copies of weights, loss scaling, and
the execution of specific arithmetic operations in
full precision.
Heterogeneous Training System Multiple stud-
ies (Rhu et al., 2016; Wang et al., 2018; Ren et al.,
2021a) have attempted to reduce GPU memory
consumption by leveraging heterogeneous memory,
such as CPU and NVMe memory. L2L (Pudipeddi
et al., 2020) employs a layer-to-layer strategy,
where only the tensors necessary for the computa-
tion of a particular layer are transferred to the GPU
memory, while the remaining tensors are retained
in the CPU memory. ZeRO-Offload (Ren et al.,
2021b), an extension of ZeRO-2 (Rajbhandari et al.,
2020), reserves the gradients and optimizer states in
the CPU memory and updates parameters through
CPU computation. ZeRO-Infinity (Rajbhandari
et al., 2021), a subsequent advancement of ZeRO-
Offload on ZeRO-3 (Rajbhandari et al., 2020), en-
ables further scaling of the model size.
In addition to the methods orthogonal to LOMO
mentioned above, recent developments have intro-
duced several memory-efficient optimization tech-
niques. MeZO (Malladi et al., 2023) employs a
zero-order optimization approach, estimating gra-
dients using two forward passes and updating pa-
rameters in place. GaLore (Zhao et al., 2024) per-
forms low-rank decomposition on gradients and
uses these approximated gradients for parameter
updates. Other approaches reduce memory us-
age by quantizing optimizer states (Dettmers et al.,
2022; Sun et al., 2020b). Compared to these meth-
ods, LOMO neither approximates gradients nor
requires low-bit quantization.
3 Method
3.1 Rethink the Functionality of Optimizer
The optimizer states occupy a large part of the mem-
ory used for training LLMs. Modern optimizer like
Adam (Kingma and Ba, 2015) stores intermediate
states that are twice the size of parameters. As the
size of parameters increases, the optimizer states
become the dominant term of memory usage.
3.1.1 Using SGD
Although Adam has achieved great success in train-
ing deep models, we ask the question “Can we usea cheaper optimizer for fine-tuning LLMs?" Our
answer is SGD, the most basic optimizer. Fortu-
nately, we find that it is an acceptable solution for
fine-tuning LLMs when we limit the scope.
Prior works often discuss three challenges of
SGD: 1) large curvature loss surface, 2) local op-
timum, and 3) saddle points (Ruder, 2016; Sun
et al., 2020a). Modern optimizers have shown ef-
fectiveness in dealing with the 1) problem and can
mitigate 2) and 3) in some cases. However, when
we limit the scope to fine-tuning LLMs, these three
challenges could be different.
Smoother loss surface One important assump-
tion is that the parameter space of LLMs is quite
smooth and small perturbations on the parameters
will not change the loss too much. There are em-
pirical results and theoretical analyses supporting
this assumption (Hao et al., 2019). If we believe
that larger models have a smoother loss surface, we
can conclude that the 1) problem is not an issue
since the loss surface of LLMs should not have a
large curvature. Note that this holds only when we
teach the LLMs natural language-based tasks (or
code-based if pre-trained with code). A synthetic
loss function unrelated to pre-training tasks will
indeed face the large curvature problem.
Local optimum is good enough The goal of
fine-tuning is to adapt LLMs to new tasks and do-
mains without significantly changing the model
itself. Therefore, a local optimum is often a good
enough solution (Kawaguchi et al., 2019), and the
limited training data (compared to pre-training cor-
pus) makes it difficult to push the model to a far-
away global optimum.
Distant saddle points Similarly, for a common
NLP task, the initial point of LLMs should be in
a valley. If the model is pre-trained with instruc-
tions (tasks), the phenomenon could be much more
apparent since we have more chances of finding
pre-trained tasks that are similar to the new task.
Saddle points typically appear on ridges and have
a distance from valleys, so we may not encounter
the saddle point problem if we do not change the
parameter too far from the pre-trained value.
However, there is no guarantee that SGD is a
powerful optimizer compared to modern optimiz-
ers. Our intuition is to create a simple and practical
solution for fine-tuning LLMs and identify its flaws
to continually improve it.

--- PAGE 4 ---
3.1.2 Implicit Batch Size
Besides the above qualitative discussion, we want
to provide a deeper analysis of the stability of fine-
tuning LLMs with SGD. Suppose we have a pre-
trained model f(·)with the parameter θ, a training
setD={d1, d2,···, dn}, and a loss function L.
One step update of SGD on a batch with two data
points could be,
θ′=θ−α[∇L(di, f(di,θ)) +∇L(dj, f(dj,θ))],
(1)
where αis the learning rate, and di, djare two
different training samples.
Next, two steps update of SGD on these two
training samples di, djsequentially could be,
θ1=θ−α∇L(di, f(di,θ)), (2)
θ2=θ1−α∇L(dj, f(dj,θ1)). (3)
By differential mean value theorem, we have
L(dj, f(dj,θ1)) =L(dj, f(dj,θ))
+∇L(dj, ξ)(f(dj,θ1)−f(dj,θ)), (4)
θ2=θ−α∇L(di, f(di,θ))
−α∇L(dj, f(dj,θ))
−α∇[∇L(dj, ξ)(f(dj,θ1)−f(dj,θ))],(5)
θ2=θ−α[∇L(di, f(di,θ))
+∇L(dj, f(dj,θ))]
−α∇[∇L(dj, ξ)(f(dj,θ1)−f(dj,θ))],(6)
where ξis a point between f(dj,θ)andf(dj,θ1),
and we can see that (6)minus (1)equals the
α∇[∇L(dj, ξ)(f(dj,θ1)−f(dj,θ))]. Suppose
the loss surface is smooth enough, this term be-
come negligible. It suggests that utilizing SGD
optimizer over a smooth loss surface could imply a
larger batch size.
As we mentioned above, it’s reasonable to as-
sume that the loss surface of LLMs is smooth, and
a larger batch size indicates stronger training stabil-
ity, so we believe that finetuning process of LLMs
with the SGD optimizer is stable. This also explains
why SGD failed on small models but worked for
large models.
3.2 LOMO: LOw-Memory Optimization
The gradient tensor represents the gradient of a
parameter tensor and has the same size as the pa-
rameter, resulting in a large memory overhead.
Modern deep learning training frameworks likeAlgorithm 1 Fusion Update in LOMO
Require: model f(·)withLlayers and pparame-
ters, parameter θ∈Rp, learning rate α, max
stepT, training dataset D, loss function L
1:fort= 1, . . . , T do
2: Sample batch B= (x,y)⊂ D
3: ˆy←f(x,θ) ▷Forward pass
4: ℓ← L(y,ˆy)
5: forl=L, . . . , 1do ▷Backward
6: θl←[θiforθi∈layer l]
7: gl←∂ℓ
∂θl
8: θl←θl−α∗gl
9: gl←None ▷Clear gradients
10: end for
11:end for
PyTorch (Paszke et al., 2017) store gradient tensors
for all parameters. There are two reasons for stor-
ing gradient tensors: computing optimizer states
and normalizing gradients.
Since we take SGD as the optimizer, there are no
optimizer states relying on gradients, and we have
some alternatives to gradient normalization. Thus,
we proposed LOw-Memory Optimization (LOMO)
as illustrated in Algorithm 1, fusing the gradient
computation and parameter update in one step to
avoid storing any gradient tensors.
In detail, we can express the vanilla gradient
descent as grad =∂L
∂p, p=p−lr∗grad , which is a
two-step process, computing the gradients first and
updating it to the parameters. The fusion version is
p=p−lr∗∂L
∂p.
The key idea is to update the parameter imme-
diately when its gradient is computed so that we
do not store gradient tensor in memory. This can
be achieved by injecting hook functions into the
backward propagation.2PyTorch provides relevant
APIs for injecting hook functions, but we cannot
implement the exact immediate update with current
APIs. Instead, we store at most one parameter’s
gradient in memory and update each parameter one
by one along with the backward propagation. Our
approach reduces the memory usage of gradients
from storing of all parameters’ gradients to storing
only one parameter’s gradient.
The majority of LOMO memory usage coincides
with that of parameter-efficient fine-tuning (PEFT)
methods, indicating that combining LOMO with
2We should inject different hook functions accordingly if
some of them share the weight.

--- PAGE 5 ---
these methods only introduces a minor increase
in memory occupied by gradients. This enables
tuning much more parameters for PEFT methods.
3.3 Stabilize Training with LOMO
3.3.1 Alternatives to Gradient Normalization
and Clipping
Gradient normalization and clipping are essential
tools to deal with the gradient explosion and vanish-
ing problem (Chen et al., 2018), but their computa-
tion process requires using the gradient tensors of
all parameters. We propose two alternatives here:
•Clipping gradient tensors by its values rather
than the norm.
•Compute the gradient norm in an additional
backward pass.
Clipping gradient tensors by their values is a
simple but effective solution for gradient explosion
before gradient norm approaches. The main con-
cern of clipping by values is that truncating some
gradient elements could change the direction of the
gradient tensor. For example, a two-dim vector
[1.3,0.8]and its clipped version [1.0,0.8](clipped
to1.0) indicate different directions. Our experi-
ence is that the clipping by values performs worse
when the learning rate is high because truncations
happened more often in that case. However, clip-
ping by values performs well for medium and small
learning rates. Note that the scale of the learning
rate largely depends on the task and data, but in
general, we suggest using clipping by values for a
learning rate less than 1e−3.
Our approach cannot directly compute the gra-
dient norm because we update parameters along
with the backpropagation, so we do not know the
norm of rest parameters when updating a certain pa-
rameter. However, we can introduce an additional
pass to compute and accumulate each parameter’s
gradient norm, resulting in two backward passes,
one for computing the gradient norm and one for
updating parameters. The memory usage leaves
unchanged but sacrifices the speed.
A controversial solution Our current training
framework computes the gradient norm based on
all parameters and requires two backward passes.
One solution to save the additional backward pass
is to approximate the norm of gradient tensors with
a group of parameters, for example, the adjacent
layers. This method is indeed biased, because itresults in different update step sizes for different
parameters. When updating, the parameters are
multiplied by a scale factor according to the gradi-
ent norms. Since the gradient norms differ among
parameter groups, such an approximation leads to
a difference in scale factors. This grouped gradi-
ent clipping method can be considered as applying
a dynamic learning rate to different groups of pa-
rameters based on their gradient norms. Sun et al.
(2020a) suggests that it is not always appropriate
to use the same learning rate for all parameters
in SGD, thus we believe our approach also holds
the potential to further benefit SGD. We leave the
exploration as a compelling future direction.
3.3.2 Mitigating Precision Degradation
Mixed-precision training is commonly employed
to speed up the training process. To mitigate the
degradation in precision, we utilize dynamic loss
scaling and transition certain computations to full
precision. The approach of loss scaling is crucial in
preventing underflows during FP16 training, mag-
nifying the loss with a specific factor prior to the
backward pass and diminishing the gradients by
the same factor.
In this context, we integrate a dynamic loss
scaler with LOMO, which dynamically adjusts the
scaling factor throughout the training procedure. If
no overflow occurs during a specified number of
backward passes, the scale factor is doubled. Oth-
erwise, this step is dropped and the scale factor is
halved. This process echoes the scenario encoun-
tered during gradient normalization. It is unknown
whether there will be an overflow until the back-
ward has completed. Consequently, we perform
two backward passes: the first pass to identify any
overflow, and the second pass to update the parame-
ters if no overflow is detected. These two backward
passes for dynamic loss scaling can be executed
simultaneously with gradient normalization. To
effectively update parameter and handle gradient
for operations like normalization and scaling, the
gradient and its associated parameter are converted
to full precision within these computations.
4 Experiment
In this section, we evaluate our proposed method
from three aspects, namely memory profile,
throughput and downstream performance. If not
further explained, all our experiments are con-
ducted with LLaMA models (Touvron et al., 2023),
ranging from 7B to 65B.

--- PAGE 6 ---
Parameters
12.3%Gradients
12.3%
Optimizer States73.7%Activations 1.8%
(a) Training with AdamWParameters
24.1%Gradients
24.1%
Optimizer States48.3%Activations3.4%
(b) Training with SGDParameters
86.1%
Gradients1.6% Activations12.3%
(c) Training with LOMOFigure 2: The memory usage ratio of each part when using different optimizers to train LLaMA-7B. The sequence
length and batch size are set to 512 and 8, respectively.
AC Params Gradients Optim States Activations Total Memory
AdamW✗12.55 12.55 75.3145.61 147.02
✓ 1.79 102.20
SGD✗12.55 12.55 25.1045.61 96.81
✓ 1.79 51.99
LOMO✗12.55 0.24 0.0045.61 59.40
✓ 1.79 14.58
Table 1: Memory usage (GB) when training LLaMA-7B under different settings. ACrefers to Activation Check-
pointing. The sequence length and batch size are set to 512 and 8, respectively.
4.1 Memory Profile
We first profile the memory usage of model states
and activations during the training under differ-
ent settings. As demonstrated in Table 1, the us-
age of the LOMO optimizer leads to a substan-
tial reduction in memory footprint from 102.20GB
to 14.58GB, when compared to the AdamW op-
timizer (Loshchilov and Hutter, 2019), and from
51.99GB to 14.58GB, when compared to SGD,
in the context of training the LLaMA-7B model.
This significant decrease in memory usage can be
attributed primarily to the reduced memory require-
ments of the gradient and optimizer states. As a
result, memory is mostly occupied by parameters in
the training process, commensurate with memory
usage during inference.
Optimizer States Figure 2 illustrates that em-
ploying the AdamW optimizer for LLaMA-7B
training, a widely adopted configuration, yields
a substantial proportion of memory (73.7%) being
allocated to optimizer states. This outcome is a con-
sequence of the mixed-precision training approach,
where full-precision copies of weights, momentum,
and variance are maintained within the optimizer
states for weight updates. Replacing the AdamWoptimizer with the SGD optimizer can effectively
reduce the percentage of optimizer states in mem-
ory, and therefore alleviate the GPU memory usage
(from 102.20GB to 51.99GB). This reduction is
due to the fact that the SGD optimizer does not re-
quire the storage of full-precision momentums and
variances. For LOMO, parameter update and back-
ward are fused into one step, further eliminating
the need for optimizer state memory.
Gradients During the training process using
LOMO, parameters are immediately updated upon
receiving gradients, following which the gradients
are discarded from memory. As a result, the upper
bound of gradient memory consumption is deter-
mined by the gradient associated with the param-
eter matrix of greatest magnitude. This approach
considerably reduces memory usage by almost the
size of parameters.
Activations The training of a 7B model with
512×8 tokens in one batch demands a substan-
tial amount of memory for activations. LOMO
is compatible with activation memory reduction
techniques such as activation checkpointing. By in-
tegrating activation checkpointing with LOMO, the
memory footprint due to activation can be reduced

--- PAGE 7 ---
Params Optimizer Hardware Memory (GB) Throughput (TGS)
7B AdamW 8 ×RTX 3090 15.76 67.37
7B SGD 8 ×RTX 3090 9.49 69.66
7B LOMO 1 ×RTX 3090 13.61 769.92
13B SGD 8 ×RTX 3090 15.74 32.51
13B LOMO 2 ×RTX 3090 15.92 66.19
30B LOMO 4 ×RTX 3090 19.78 11.61
65B LOMO 8 ×RTX 3090 19.18 4.93
Table 2: Throughput tested on a server with 8 RTX 3090 GPUs. The sequence length and batch size are set to
1024 and 1, respectively. Memory represents the peak memory allocated per GPU during training. Throughput
represents the number of tokens processed by each GPU per second (TGS).
from 45.61GB to 1.79GB.
4.2 Throughput
We evaluate the throughput performance of LOMO
compared to AdamW and SGD. The experiments
are conduct on a server equipped with 8 RTX 3090
GPUs, interconnected via a PCIe motherboard. The
sequence length and batch size are set to 1024
and 1, respectively. Throughput is measured in
terms of the number of tokens processed per GPU
per second (TGS), and parameter partitioning was
achieved using ZeRO-3 (Rajbhandari et al., 2020).
For the 7B model, LOMO demonstrates remark-
able throughput, surpassing AdamW and SGD by
about 11 times. This significant improvement can
be attributed to LOMO’s ability to train the 7B
model on a single GPU, thereby reducing inter-
GPU communication overhead. The slightly higher
throughput of SGD compared to AdamW can be
attributed to SGD’s exclusion of momentum and
variance calculations.
As for the 13B model, it could not be trained
with AdamW on the available 8 RTX 3090 GPUs
due to memory limitations. In this scenario where
model parallelism is necessary for LOMO, LOMO
still outperforms SGD in terms of throughput.
This advantage is attributed to LOMO’s memory-
efficient properties and the requirement of only two
GPUs to train the model with the same settings, re-
sulting in reduced communication costs and greater
throughput. Furthermore, when training the 30B
model, SGD encounters out-of-memory (OOM) is-
sues with the 8 RTX 3090 GPUs, while LOMO
performs well with only 4 GPUs.
Finally, we successfully train the 65B model
using 8 RTX 3090 GPUs, achieving a throughput
of 4.93 TGS. Utilizing such a server configurationand LOMO, the training process on 1000 samples,
each containing 512 tokens, requires approximately
3.6 hours.
4.3 Downstream Performance
To assess the effectiveness of LOMO in fine-tuning
large language models, we conduct an extensive
set of experiments. We compare LOMO against
two other methods, Zero-shot, which does not re-
quire fine-tuning, and LoRA, which is currently
one of the most popular parameter-efficient fine-
tuning techniques. As descirbed in (Hu et al., 2022),
LoRA reparameterizes the dense layers and only
updates low rank matrices while introducing no
latency during inference.
We use the SuperGLUE dataset collection to
evaluate model performance, specifically focusing
on RTE (Dagan et al., 2005), BoolQ (Clark et al.,
2019), WSC (Levesque et al., 2012), WIC (Pilehvar
and Camacho-Collados, 2019), MultiRC (Khashabi
et al., 2018), and COPA (Roemmele et al., 2011).
Given the high computational cost associated
with running large language models, we follow
MeZO (Malladi et al., 2023) to randomly sample
1000 training data from training set and 1000 test
data from validation set, and report the best results
obtained using the same random seed. The prompts
used in our experiments are the same as MeZO, and
the hyperparameters are detailed in Appendix-A.
During inference, we insert different labels or
candidates into the prompt and calculate the aver-
age log-likelihood for each label. The label with
the highest score is selected as the model’s answer.
To evaluate the performance, we use Accuracy as
the evaluation metric.

--- PAGE 8 ---
Method Params RTE BoolQ WSC WIC MultiRC COPA Avg.
Zero-shot 7B 57.0 66.5 36.5 49.7 42.3 85.0 56.2
LoRA 7B 85.9 85.2 64.4 65.5 84.8 87.0 78.8
LOMO 7B 86.6 87.5 66.4 71.2 84.0 89.0 80.8
Zero-shot 13B 60.6 65.0 36.5 49.5 43.4 88.0 57.2
LoRA 13B 89.9 87.1 63.5 69.9 86.1 92.0 81.4
LOMO 13B 89.9 87.3 75.0 74.3 85.7 93.0 84.2
Zero-shot 30B 53.4 74.6 36.5 50.0 46.9 89.0 58.4
LoRA 30B 91.0 89.7 83.7 74.0 87.0 93.0 86.4
LOMO 30B 92.8 89.3 85.6 74.1 87.9 93.0 87.1
Zero-shot 65B 59.6 73.6 44.2 51.3 48.3 91.0 61.3
LoRA 65B 93.1 90.9 88.5 74.5 90.0 97.0 89.0
LOMO 65B 93.9 90.7 92.3 75.4 89.9 97.0 89.9
Table 3: Main results on SuperGLUE using LLaMA at all sizes (with 1,000 training examples).
4.3.1 Main results
The downstream performances of LOMO com-
pared with Zero-shot and LoRA are presented in
Table 3. Based on the results, we reach the follow-
ing observations.
LOMO performs significantly better than
Zero-shot. Across all six datasets and model sizes,
LOMO consistently achieves superior results over
Zero-shot, with average gains of more than 20
points using LLaMA-13B. While previous research
has showcased the impressive capabilities of large
language models in zero-shot settings, fine-tuning
still yields remarkable performance enhancements
for specific downstream tasks. The experimental
results confirm the effectiveness of LOMO in opti-
mizing large language models of different sizes.
LOMO generally outperforms LoRA in most
experiments. We show that LOMO delivers strong
performance compared to LoRA, for instance, re-
sulting in average gains of 2.8 points using LLaMA-
13B. This suggests that the model performance
benefits more from full-parameter fine-tuning than
parameter-efficient fine-tuning, as the former ad-
justs more parameters. LOMO strikes a good bal-
ance between performance and efficiency, making
it a competitive choice for fine-tuning.
In some cases, LOMO performs worse than
LoRA. One possible reason is the relatively small
training set we use, which may not be sufficient for
full-parameter fine-tuning of large models. Addi-
tionally, LoRA and LOMO employ different model
architectures. To be specific, LoRA offers a short-
cut for model tuning, which can be advantageous incertain scenarios. Actually, these two methods are
not conflicting or mutually exclusive. In the next
subsection, we validate that combing LoRA with
LOMO does not harm model performance and, in
most cases, leads to performance gains.
LOMO efficiently scales up to 65 billion pa-
rameter models. Despite conducting all experi-
ments on a single machine equipped with 8 ×RTX
3090, LOMO consistently exhibits strong perfor-
mance even on a 65-parameter scale. This further
supports the effectiveness of LOMO in optimizing
LLMs under resource-constrained scenarios.
4.3.2 LoRA with LOMO
LOMO and LoRA are fundamentally independent
of each other. In order to verify this claim, we
perform experiments using LLaMA-13B on the
BoolQ and MultiRC datasets. Results are shown in
Figure 3. We find that LOMO consistently en-
hances the performance of LoRA regardless of
the higher results LoRA achieved. This suggests
that different fine-tuning methods employed by
LOMO and LoRA are complementary. Specifi-
cally, LOMO focuses on fine-tuning the pre-trained
models weights, while LoRA tunes additional mod-
ules. As a result, LOMO dose not compromise the
performance of LoRA; rather, it facilitates better
model tuning for downstream tasks.
5 Conclusion
In this paper, we introduce LOw-Memory Opti-
mization (LOMO), a new optimizer designed to
facilitate full parameter fine-tuning for large lan-

--- PAGE 9 ---
w/o LoRA 1 2 4 8
Lora attention dimension86.086.587.087.588.088.589.0Accuracy (%)87.7
87.187.5 87.5
87.388.2
87.58888.1
(a) BoolQLoRA
LoRA+LOMO
w/o LoRA 1 2 4 8
Lora attention dimension84.084.585.085.586.086.587.0Accuracy (%)85.586.1 86.1
86
85.786.3 86.386.7
86.3
(b) MultiRCLoRA
LoRA+LOMOFigure 3: Results using LLaMA-13B on the BoolQ and MultiRC datasets (with 1,000 training examples).
“LoRA+LOMO" means injecting LoRA modules while fine-tuning the pre-trained model weights using LOMO.
guage models with limited resources. We have
demonstrated the feasibility of fine-tuning a 65B
model on a server equipped with consumer GPUs
such as RTX 3090. By analyzing the memory us-
age of LOMO, conducting throughput tests, and
performing experiments on SuperGLUE, we have
showcased its effectiveness and potential impact.
Looking ahead, our future work aims to further
lower the resource threshold required for training
large language models, thus enabling wider ac-
cess and adoption of these models. The majority
of memory are currently occupied by parameters
when training with LOMO. Thus, one promising
direction is the exploration of parameter quantiza-
tion techniques, which could significantly reduce
memory usage. Additionally, we intend to inves-
tigate more applicable scenarios for LOMO and
delve into theoretical analyses for optimizing large
language models, which hold substantial value for
advancing the field.
Limitations
In response to the challenges associated with gradi-
ent normalization and clipping, we have developed
alternative optimization methods. Although gra-
dient normalization for LOMO does not increase
memory usage, our current implementation necessi-
tates an additional backward pass, which can slow
down the training speed in scenarios where gradi-
ent normalization is essential.
Due to time and resource constraints, our experi-
ments were limited to a subset of the SuperGLUE
benchmark, and we did not evaluate LOMO’s
throughput on advanced GPUs such as A100.Ethics statement
This paper employs open-source models LLaMA,
in compliance with their respective licenses. The
datasets utilized, including RTE, BoolQ, WSC,
WIC, MultiRC and COPA, permit public and
freeusage.
Acknowledgments
This work was supported by the National Key
Research and Development Program of China
(No.2022ZD0160102). The computations in this
research were performed using the CFFF platform
of Fudan University.
References
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos
Guestrin. 2016. Training deep nets with sublinear
memory cost. arXiv preprint arXiv:1604.06174 .
Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and
Andrew Rabinovich. 2018. Gradnorm: Gradient nor-
malization for adaptive loss balancing in deep mul-
titask networks. In Proceedings of the 35th Inter-
national Conference on Machine Learning, ICML
2018, Stockholmsmässan, Stockholm, Sweden, July
10-15, 2018 , volume 80 of Proceedings of Machine
Learning Research , pages 793–802. PMLR.
Christopher Clark, Kenton Lee, Ming-Wei Chang,
Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. 2019. Boolq: Exploring the surprising
difficulty of natural yes/no questions. In Proceedings
of the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, NAACL-HLT 2019,
Minneapolis, MN, USA, June 2-7, 2019, Volume 1
(Long and Short Papers) , pages 2924–2936. Associa-
tion for Computational Linguistics.

--- PAGE 10 ---
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognising textual entailment
challenge. In Machine Learning Challenges, Eval-
uating Predictive Uncertainty, Visual Object Classi-
fication and Recognizing Textual Entailment, First
PASCAL Machine Learning Challenges Workshop,
MLCW 2005, Southampton, UK, April 11-13, 2005,
Revised Selected Papers , volume 3944 of Lecture
Notes in Computer Science , pages 177–190. Springer.
Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke
Zettlemoyer. 2022. 8-bit optimizers via block-wise
quantization. In The Tenth International Conference
on Learning Representations, ICLR 2022, Virtual
Event, April 25-29, 2022 . OpenReview.net.
Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zong-
han Yang, Yusheng Su, Shengding Hu, Yulin Chen,
Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao,
Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei
Chen, Yang Liu, Jie Tang, Juanzi Li, and Maosong
Sun. 2022. Delta tuning: A comprehensive study of
parameter efficient methods for pre-trained language
models. CoRR , abs/2203.06904.
Yaru Hao, Li Dong, Furu Wei, and Ke Xu. 2019. Visu-
alizing and understanding the effectiveness of BERT.
InProceedings of the 2019 Conference on Empiri-
cal Methods in Natural Language Processing and
the 9th International Joint Conference on Natural
Language Processing, EMNLP-IJCNLP 2019, Hong
Kong, China, November 3-7, 2019 , pages 4141–4150.
Association for Computational Linguistics.
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. 2022. Lora: Low-rank adaptation of
large language models. In The Tenth International
Conference on Learning Representations, ICLR 2022,
Virtual Event, April 25-29, 2022 . OpenReview.net.
Kenji Kawaguchi, Jiaoyang Huang, and Leslie Pack
Kaelbling. 2019. Every local minimum value is the
global minimum value of induced model in noncon-
vex machine learning. Neural Comput. , 31(12):2293–
2323.
Daniel Khashabi, Snigdha Chaturvedi, Michael Roth,
Shyam Upadhyay, and Dan Roth. 2018. Looking
beyond the surface: A challenge set for reading com-
prehension over multiple sentences. In Proceedings
of the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, NAACL-HLT 2018,
New Orleans, Louisiana, USA, June 1-6, 2018, Vol-
ume 1 (Long Papers) , pages 252–262. Association
for Computational Linguistics.
Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings .
Hector J. Levesque, Ernest Davis, and Leora Morgen-
stern. 2012. The winograd schema challenge. InPrinciples of Knowledge Representation and Rea-
soning: Proceedings of the Thirteenth International
Conference, KR 2012, Rome, Italy, June 10-14, 2012 .
AAAI Press.
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizing continuous prompts for generation. In
Proceedings of the 59th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 11th
International Joint Conference on Natural Language
Processing, ACL/IJCNLP 2021, (Volume 1: Long
Papers), Virtual Event, August 1-6, 2021 , pages 4582–
4597. Association for Computational Linguistics.
Ilya Loshchilov and Frank Hutter. 2019. Decoupled
weight decay regularization. In 7th International
Conference on Learning Representations, ICLR 2019,
New Orleans, LA, USA, May 6-9, 2019 . OpenRe-
view.net.
Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex
Damian, Jason D. Lee, Danqi Chen, and Sanjeev
Arora. 2023. Fine-tuning language models with just
forward passes. CoRR , abs/2305.17333.
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gre-
gory Diamos, Erich Elsen, David Garcia, Boris Gins-
burg, Michael Houston, Oleksii Kuchaiev, Ganesh
Venkatesh, and Hao Wu. 2018. Mixed precision
training. In International Conference on Learning
Representations .
Deepak Narayanan, Mohammad Shoeybi, Jared Casper,
Patrick LeGresley, Mostofa Patwary, Vijay Kor-
thikanti, Dmitri Vainbrand, Prethvi Kashinkunti,
Julie Bernauer, Bryan Catanzaro, et al. 2021. Ef-
ficient large-scale language model training on gpu
clusters using megatron-lm. Proceedings of the Inter-
national Conference for High Performance Comput-
ing, Networking, Storage and Analysis , pages 1–15.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin,
Alban Desmaison, Luca Antiga, and Adam Lerer.
2017. Automatic differentiation in pytorch.
Mohammad Taher Pilehvar and José Camacho-Collados.
2019. Wic: the word-in-context dataset for evaluat-
ing context-sensitive meaning representations. In
Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7,
2019, Volume 1 (Long and Short Papers) , pages 1267–
1273. Association for Computational Linguistics.
Bharadwaj Pudipeddi, Maral Mesmakhosroshahi, Jin-
wen Xi, and Sujeeth Bharadwaj. 2020. Training
large neural networks with constant memory us-
ing a new execution algorithm. arXiv preprint
arXiv:2002.05645 .
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
and Yuxiong He. 2020. Zero: Memory optimizations

--- PAGE 11 ---
toward training trillion parameter models. SC20: In-
ternational Conference for High Performance Com-
puting, Networking, Storage and Analysis , pages 1–
16.
Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley,
Shaden Smith, and Yuxiong He. 2021. Zero-infinity:
Breaking the gpu memory wall for extreme scale
deep learning. Proceedings of the International Con-
ference for High Performance Computing, Network-
ing, Storage and Analysis , pages 1–14.
Jie Ren, Jiaolin Luo, Kai Wu, Minjia Zhang, Hyeran
Jeon, and Dong Li. 2021a. Sentinel: Efficient tensor
migration and allocation on heterogeneous memory
systems for deep learning. In 2021 IEEE Interna-
tional Symposium on High-Performance Computer
Architecture (HPCA) , pages 598–611.
Jie Ren, Samyam Rajbhandari, Reza Yazdani Am-
inabadi, Olatunji Ruwase, Shuangyan Yang, Minjia
Zhang, Dong Li, and Yuxiong He. 2021b. Zero-
offload: Democratizing billion-scale model training.
USENIX Annual Technical Conference , pages 551–
564.
Minsoo Rhu, Natalia Gimelshein, Jason Clemons,
Arslan Zulfiqar, and Stephen W. Keckler. 2016.
vdnn: Virtualized deep neural networks for scalable,
memory-efficient neural network design. In 2016
49th Annual IEEE/ACM International Symposium on
Microarchitecture (MICRO) , pages 1–13.
Melissa Roemmele, Cosmin Adrian Bejan, and An-
drew S. Gordon. 2011. Choice of plausible alter-
natives: An evaluation of commonsense causal rea-
soning. In Logical Formalizations of Commonsense
Reasoning, Papers from the 2011 AAAI Spring Sym-
posium, Technical Report SS-11-06, Stanford, Cali-
fornia, USA, March 21-23, 2011 . AAAI.
Sebastian Ruder. 2016. An overview of gradient descent
optimization algorithms. CoRR , abs/1609.04747.
Shiliang Sun, Zehui Cao, Han Zhu, and Jing Zhao.
2020a. A survey of optimization methods from a
machine learning perspective. IEEE Trans. Cybern. ,
50(8):3668–3681.
Xianghui Sun, Yunjie Ji, Baochang Ma, and Xian-
gang Li. 2023. A comparative study between full-
parameter and lora-based fine-tuning on chinese in-
struction data for instruction following large language
model. CoRR , abs/2304.08109.
Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni,
Ankur Agrawal, Xiaodong Cui, Swagath Venkatara-
mani, Kaoutar El Maghraoui, Vijayalakshmi Srini-
vasan, and Kailash Gopalakrishnan. 2020b. Ultra-
low precision 4-bit training of deep neural networks.
InAdvances in Neural Information Processing Sys-
tems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual .Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurélien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models. CoRR ,
abs/2302.13971.
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-
preet Singh, Julian Michael, Felix Hill, Omer Levy,
and Samuel R. Bowman. 2019. Superglue: A stickier
benchmark for general-purpose language understand-
ing systems. In Advances in Neural Information
Processing Systems 32: Annual Conference on Neu-
ral Information Processing Systems 2019, NeurIPS
2019, December 8-14, 2019, Vancouver, BC, Canada ,
pages 3261–3275.
Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang
Li, Shuaiwen Song, Zenglin Xu, and Tim Kraska.
2018. Superneurons: dynamic gpu memory man-
agement for training deep neural networks. ACM
SIGPLAN Notices , 53:41–53.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.
Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy
Liang, Jeff Dean, and William Fedus. 2022. Emer-
gent abilities of large language models. Trans. Mach.
Learn. Res. , 2022.
Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang
Wang, Anima Anandkumar, and Yuandong Tian.
2024. Galore: Memory-efficient LLM train-
ing by gradient low-rank projection. CoRR ,
abs/2403.03507.
A Hyperparameters
The hyperparameters we use in the experiments
are listed in Table 4. Due to limited computational
resources, we report the highest results of experi-
ments conducted with the same random seed.
B Training Dynamics
To analyze the training dynamics of LOMO, we
present the training loss curve and validation ac-
curacy for LLaMA-7B trained on BoolQ (Clark
et al., 2019) using LOMO and LoRA in Figure 4
and Figure 5, respectively. During training pro-
cess with LOMO, the loss converges rapidly in the
initial phase and then tends to stabilize and gradu-
ally decline. The accuracy on the development set
generally shows an upward trend as the number of
training steps increases.

--- PAGE 12 ---
Experiments Hyperparameters Values
LR Schedule Linear
Max Grad Norm 1.0
Batch size 16
# Epochs 10
LOMOLearning Rate {5e-2, 3e-2}
Warmup Ratio {0.05, 0.1, 0.2}
LoRAOptimizer AdamW
Learning Rate 5e-4
Warmup Ratio 0.05
LoRA Config. rq=rv= 2
LoRA α 16
LoRA+LOMOLoRA Optimizer AdamW
LoRA Learning Rate 5e-4
LoRA Warmup Ratio 0.05
LoRA Config. rq=rv={1,2,4,8}
LoRA α 16
LOMO Learning Rate {5e-3, 1e-3, 5e-4}
LOMO Warmup Ratio 0.05, 0.1
Table 4: The hyperparameters used in our experiments.
0 100 200 300 400 500 600
Step0123456Training LossLoRA
LOMO
Figure 4: Training loss curve for LLaMA-7B on BoolQ.
100 200 300 400 500 600
Step0.780.800.820.840.860.88Validation AccuracyLoRA
LOMO Figure 5: Validation accuracy of LLaMA-7B on BoolQ.
