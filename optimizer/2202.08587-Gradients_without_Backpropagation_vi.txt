# Gradient mà không cần Backpropagation

## Tóm tắt

Việc sử dụng backpropagation để tính gradient của các hàm mục tiêu cho tối ưu hóa vẫn là nền tảng của machine learning. Backpropagation, hay vi phân chế độ ngược, là một trường hợp đặc biệt trong họ các thuật toán vi phân tự động tổng quát cũng bao gồm chế độ thuận. Chúng tôi trình bày một phương pháp tính gradient chỉ dựa trên đạo hàm theo hướng mà người ta có thể tính chính xác và hiệu quả thông qua chế độ thuận. Chúng tôi gọi công thức này là gradient thuận, một ước lượng không thiên lệch của gradient có thể được đánh giá trong một lần chạy thuận duy nhất của hàm, hoàn toàn loại bỏ nhu cầu backpropagation trong gradient descent. Chúng tôi chứng minh forward gradient descent trong một loạt các bài toán, cho thấy tiết kiệm đáng kể về tính toán và cho phép huấn luyện nhanh hơn gấp đôi trong một số trường hợp.

## 1. Giới thiệu

Backpropagation (Linnainmaa, 1970; Rumelhart et al., 1985) và tối ưu hóa dựa trên gradient đã là những thuật toán cốt lõi làm nền tảng cho nhiều thành công gần đây trong machine learning (ML) (Goodfellow et al., 2016; Deisenroth et al., 2020). Người ta thường công nhận rằng một trong những yếu tố góp phần vào tốc độ tiến bộ gần đây trong ML là sự dễ dàng mà với đó mã ML có thể vi phân được thực hiện qua các thư viện được thiết kế tốt như PyTorch (Paszke et al., 2019) hoặc TensorFlow (Abadi et al., 2016) với khả năng vi phân tự động (AD) (Griewank & Walther, 2008; Baydin et al., 2018). Các framework này cung cấp cơ sở hạ tầng tính toán mà lĩnh vực của chúng ta được xây dựng trên đó.

Cho đến gần đây, tất cả các framework phần mềm chính cho ML đều được xây dựng xung quanh chế độ ngược của AD, một kỹ thuật đánh giá đạo hàm của mã số sử dụng thuật toán hai pha thuận-ngược, trong đó backpropagation là một trường hợp đặc biệt thường được áp dụng cho mạng neural. Điều này chủ yếu do vai trò trung tâm của các mục tiêu có giá trị vô hướng trong ML, mà gradient của chúng đối với một số lượng rất lớn đầu vào có thể được đánh giá chính xác và hiệu quả với một lần đánh giá duy nhất của chế độ ngược.

Chế độ ngược là thành viên của một họ lớn hơn các thuật toán AD cũng bao gồm chế độ thuận (Wengert, 1964), có đặc tính thuận lợi là chỉ yêu cầu một lần đánh giá thuận duy nhất của một hàm (tức là không liên quan đến bất kỳ backpropagation nào) với chi phí tính toán thấp hơn đáng kể. Quan trọng là, chế độ thuận và ngược của AD đánh giá các đại lượng khác nhau. Cho một hàm f: R^n → R^m, chế độ thuận đánh giá tích Jacobian-vector J_f v, trong đó J_f ∈ R^{m×n} và v ∈ R^n; và chế độ ngược đánh giá tích vector-Jacobian v^T J_f, trong đó v ∈ R^m. Đối với trường hợp f: R^n → R (ví dụ, một hàm mục tiêu trong ML), chế độ thuận cho ta ∇f·v ∈ R, đạo hàm theo hướng; và chế độ ngược cho ta gradient đầy đủ ∇f ∈ R^n.

Từ góc độ AD được áp dụng cho ML, một "chén thánh" là liệu tính hữu ích thực tế của gradient descent có thể đạt được chỉ sử dụng chế độ thuận, loại bỏ nhu cầu backpropagation. Điều này có thể thay đổi độ phức tạp tính toán của các pipeline huấn luyện ML điển hình, giảm chi phí thời gian và năng lượng của huấn luyện, ảnh hưởng đến thiết kế phần cứng ML, và thậm chí có ý nghĩa về tính hợp lý sinh học của backpropagation trong não (Bengio et al., 2015; Lillicrap et al., 2020). Trong công trình này, chúng tôi trình bày kết quả chứng minh gradient descent ổn định trên một loạt kiến trúc ML chỉ sử dụng AD chế độ thuận.

## Đóng góp

• Chúng tôi định nghĩa "gradient thuận", một ước lượng của gradient mà chúng tôi chứng minh là không thiên lệch, dựa trên AD chế độ thuận mà không cần backpropagation.

• Chúng tôi thực hiện một hệ thống AD thuận từ đầu trong PyTorch, hoàn toàn độc lập với việc thực hiện AD ngược đã có sẵn trong thư viện này.

• Chúng tôi sử dụng gradient thuận trong tối ưu hóa stochastic gradient descent (SGD) của một loạt kiến trúc, và cho thấy rằng một pipeline huấn luyện ML hiện đại điển hình có thể được xây dựng chỉ với AD thuận và không cần backpropagation.

• Chúng tôi so sánh đặc tính runtime và hiệu suất loss của gradient thuận và backpropagation, và chứng minh tăng tốc lên đến gấp đôi so với backpropagation trong một số trường hợp.

Lưu ý về đặt tên: Khi đặt tên cho kỹ thuật, có xu hướng áp dụng tên như "forward propagation" hoặc "forward-prop" để tương phản với backpropagation. Chúng tôi không sử dụng tên này vì nó thường được sử dụng để chỉ giai đoạn đánh giá thuận của backpropagation, khác biệt với AD thuận. Chúng tôi quan sát thấy rằng tên đơn giản "gradient thuận" hiện tại không được sử dụng trong ML, và nó cũng nắm bắt được khía cạnh mà chúng tôi đang trình bày một sự thay thế trực tiếp cho gradient.

## 2. Kiến thức nền tảng

Để giới thiệu phương pháp của chúng tôi, chúng tôi bắt đầu bằng việc xem xét ngắn gọn hai chế độ chính của vi phân tự động.

### 2.1. AD Chế độ Thuận

Cho một hàm f: R^n → R^m và các giá trị θ ∈ R^n, v ∈ R^n, AD chế độ thuận tính f(θ) và tích Jacobian-vector J_f(θ)v, trong đó J_f(θ) ∈ R^{m×n} là ma trận Jacobian của tất cả các đạo hàm riêng của f được đánh giá tại θ, và v là một vector nhiễu loạn. Đối với trường hợp f: R^n → R, tích Jacobian-vector tương ứng với đạo hàm theo hướng ∇f(θ)·v, là phép chiếu của gradient ∇f tại θ lên vector hướng v, đại diện cho tốc độ thay đổi dọc theo hướng đó.

Quan trọng là lưu ý rằng chế độ thuận đánh giá hàm f và tích Jacobian-vector J_f v đồng thời trong một lần chạy thuận duy nhất. Cũng lưu ý rằng J_f v được thu được mà không cần tính ma trận Jacobian J_f, một đặc tính đôi khi được gọi là tính toán không cần ma trận.

### 2.2. AD Chế độ Ngược

Cho một hàm f: R^n → R^m và các giá trị θ ∈ R^n, v ∈ R^m, AD chế độ ngược tính f(θ) và tích vector-Jacobian v^T J_f(θ), trong đó J_f ∈ R^{m×n} là ma trận Jacobian của tất cả các đạo hàm riêng của f được đánh giá tại θ, và v ∈ R^m là một vector kề (adjoint). Đối với trường hợp f: R^n → R và v = 1, chế độ ngược tính gradient, tức là các đạo hàm riêng của f đối với tất cả n đầu vào ∇f(θ) = [∂f/∂θ₁, ..., ∂f/∂θₙ]^T.

Lưu ý rằng v^T J_f được tính trong một lần đánh giá thuận-ngược duy nhất, mà không cần tính ma trận Jacobian J_f.

### 2.3. Chi phí Runtime

Chi phí runtime của cả hai chế độ AD đều bị giới hạn bởi một bội số không đổi của thời gian cần để chạy hàm f mà chúng ta đang vi phân (Griewank & Walther, 2008). Chế độ ngược có chi phí cao hơn chế độ thuận, vì nó liên quan đến việc đảo ngược luồng dữ liệu và cần giữ một bản ghi (một "băng", stack, hoặc đồ thị) của kết quả các phép toán gặp phải trong lần đi thuận, vì chúng cần thiết trong việc đánh giá đạo hàm trong lần đi ngược tiếp theo. Đặc tính chi phí bộ nhớ và tính toán cuối cùng phụ thuộc vào các tính năng được thực hiện bởi hệ thống AD như khai thác tính thưa thớt (Gebremedhin et al., 2005) hoặc checkpointing (Siskind & Pearlmutter, 2018).

Chi phí có thể được phân tích bằng cách giả định độ phức tạp tính toán của các phép toán cơ bản như truy xuất, lưu trữ, cộng, nhân, và các phép toán phi tuyến (Griewank & Walther, 2008). Ký hiệu thời gian cần để đánh giá hàm gốc f là runtime(f), chúng ta có thể biểu diễn thời gian của chế độ thuận và ngược là R_f runtime(f) và R_b runtime(f) tương ứng. Trong thực tế, R_f thường nằm giữa 1 và 3, và R_b thường nằm giữa 5 và 10 (Hascoët, 2014), nhưng những con số này phụ thuộc rất nhiều vào chương trình. Lưu ý rằng trong ML, hàm gốc tương ứng với việc thực thi mã ML mà không có bất kỳ tính toán đạo hàm hoặc huấn luyện nào, tức là chỉ đánh giá một mô hình cho trước với dữ liệu đầu vào. Chúng tôi sẽ gọi đây là "runtime cơ sở" trong bài báo này.

## 3. Phương pháp

### 3.1. Gradient Thuận

**Định nghĩa 1.** Cho một hàm f: R^n → R, chúng tôi định nghĩa "gradient thuận" g: R^n → R^n như sau:

g(θ) = (∇f(θ)·v)v,                    (1)

trong đó θ ∈ R^n là điểm mà chúng ta đánh giá gradient, v ∈ R^n là một vector nhiễu loạn được lấy như một biến ngẫu nhiên đa biến v ~ p(v) sao cho các thành phần vô hướng v_i của v độc lập và có trung bình bằng không và phương sai đơn vị cho tất cả i, và ∇f(θ)·v ∈ R là đạo hàm theo hướng của f tại điểm θ theo hướng v.

Chúng tôi trước tiên nói ngắn gọn về trực quan dẫn đến định nghĩa này, trước khi chứng minh rằng g(θ) là một ước lượng không thiên lệch của gradient ∇f(θ) trong Mục 3.2.

Như đã giải thích trong Mục 2, chế độ thuận cho ta đạo hàm theo hướng ∇f(θ)·v = Σᵢ ∂f/∂θᵢ vᵢ trực tiếp, mà không cần tính ∇f. Việc tính ∇f chỉ sử dụng chế độ thuận là có thể bằng cách đánh giá f thuận n lần với các vector hướng được lấy như các vector cơ sở chuẩn (hoặc one-hot) eᵢ ∈ R^n; i = 1...n, trong đó eᵢ biểu thị một vector có 1 ở tọa độ thứ i và 0 ở những nơi khác. Điều này cho phép đánh giá độ nhạy của f đối với từng đầu vào ∂f/∂θᵢ riêng biệt, khi kết hợp lại cho ta gradient ∇f.

Để có bất kỳ cơ hội nào về lợi thế runtime so với backpropagation, chúng ta cần làm việc với một lần chạy duy nhất của chế độ thuận mỗi lần lặp tối ưu hóa, không phải n lần chạy. Trong một lần chạy thuận duy nhất, chúng ta có thể diễn giải hướng v như một vector trọng số trong một tổng có trọng số của độ nhạy đối với từng đầu vào, tức là Σᵢ ∂f/∂θᵢ vᵢ, mặc dù không có khả năng phân biệt sự đóng góp của mỗi i trong tổng số cuối cùng. Do đó, chúng ta sử dụng vector trọng số v để gán tổng độ nhạy trở lại cho từng tham số riêng lẻ i, tỷ lệ thuận với trọng số vᵢ của mỗi tham số i (ví dụ, một tham số có trọng số nhỏ có đóng góp nhỏ và một tham số lớn có đóng góp lớn trong tổng độ nhạy).

Tóm lại, mỗi lần gradient thuận được đánh giá, chúng ta đơn giản làm như sau:

• Lấy mẫu một vector nhiễu loạn ngẫu nhiên v ~ p(v), có cùng kích thước với đối số của f.

• Chạy f qua AD chế độ thuận, đánh giá f(θ) và ∇f(θ)·v đồng thời trong cùng một lần chạy thuận duy nhất, mà không cần tính ∇f trong quá trình. Đạo hàm theo hướng thu được, ∇f(θ)·v, là một vô hướng, và được tính chính xác bởi AD (không phải là một xấp xỉ).

• Nhân đạo hàm theo hướng vô hướng ∇f(θ)·v với vector v và thu được g(θ), gradient thuận.

Hình 1 minh họa quá trình cho thấy một số đánh giá của gradient thuận cho hàm Beale. Chúng ta thấy cách các nhiễu loạn vₖ (màu cam) biến thành gradient thuận (∇f·vₖ)vₖ (màu xanh) cho k ∈ [1,5], đôi khi đảo ngược chiều để hướng về gradient thực (màu đỏ) trong khi bị ràng buộc về hướng. Mũi tên xanh lá cây cho thấy một ước lượng gradient Monte Carlo thông qua gradient thuận được tính trung bình, tức là (1/K)ΣₖKₖ₌₁(∇f·vₖ)vₖ ≈ E[(∇f·v)v].

### 3.2. Chứng minh Tính không thiên lệch

**Định lý 1.** Gradient thuận g(θ) là một ước lượng không thiên lệch của gradient ∇f(θ).

**Chứng minh.** Chúng ta bắt đầu với đạo hàm theo hướng của f được đánh giá tại θ theo hướng v được viết như sau:

d(θ,v) = ∇f(θ)·v = Σᵢ ∂f/∂θᵢ vᵢ
= ∂f/∂θ₁ v₁ + ∂f/∂θ₂ v₂ + ... + ∂f/∂θₙ vₙ     (2)

Chúng ta sau đó mở rộng gradient thuận g trong Phương trình (1) như sau:

g(θ) = d(θ,v)v = [∂f/∂θ₁ v₁² + ∂f/∂θ₂ v₁v₂ + ... + ∂f/∂θₙ v₁vₙ; ∂f/∂θ₁ v₁v₂ + ∂f/∂θ₂ v₂² + ... + ∂f/∂θₙ v₂vₙ; ...; ∂f/∂θ₁ v₁vₙ + ∂f/∂θ₂ v₂vₙ + ... + ∂f/∂θₙ vₙ²]

và lưu ý rằng các thành phần của g có dạng sau:

gᵢ(θ) = ∂f/∂θᵢ vᵢ² + Σⱼ≠ᵢ ∂f/∂θⱼ vᵢvⱼ     (3)

Giá trị kỳ vọng của mỗi thành phần gᵢ là:

E[gᵢ(θ)] = E[∂f/∂θᵢ vᵢ² + Σⱼ≠ᵢ ∂f/∂θⱼ vᵢvⱼ]
= E[∂f/∂θᵢ vᵢ²] + E[Σⱼ≠ᵢ ∂f/∂θⱼ vᵢvⱼ]
= E[∂f/∂θᵢ vᵢ²] + Σⱼ≠ᵢ E[∂f/∂θⱼ vᵢvⱼ]
= ∂f/∂θᵢ E[vᵢ²] + Σⱼ≠ᵢ ∂f/∂θⱼ E[vᵢvⱼ]     (4)

Giá trị kỳ vọng đầu tiên trong Phương trình (4) là của một biến ngẫu nhiên bình phương và tất cả các kỳ vọng trong hạng tử tổng là của hai biến ngẫu nhiên độc lập và phân phối giống nhau được nhân với nhau.

**Bổ đề 1.** Giá trị kỳ vọng của một biến ngẫu nhiên v bình phương E[v²] = 1 khi E[v] = 0 và Var[v] = 1.

**Chứng minh.** Phương sai là Var[v] = E[(v - E[v])²] = E[v²] - E[v]². Sắp xếp lại và thay thế E[v] = 0 và Var[v] = 1, ta được E[v²] = E[v]² + Var[v] = 0 + 1 = 1.

**Bổ đề 2.** Giá trị kỳ vọng của hai biến ngẫu nhiên i.i.d nhân với nhau E[vᵢvⱼ] = 0 khi E[vᵢ] = 0 hoặc E[vⱼ] = 0.

**Chứng minh.** Đối với vᵢ và vⱼ i.i.d, giá trị kỳ vọng E[vᵢvⱼ] = E[vᵢ]E[vⱼ] = 0 khi E[vᵢ] = 0 hoặc E[vⱼ] = 0.

Sử dụng Bổ đề 1 và 2, Phương trình (4) giảm xuống thành:

E[gᵢ(θ)] = ∂f/∂θᵢ     (5)

và do đó:

E[g(θ)] = ∇f(θ)     (6)

### 3.3. Forward Gradient Descent

Chúng tôi xây dựng một thuật toán forward gradient descent (FGD) bằng cách thay thế gradient ∇f trong GD chuẩn bằng gradient thuận g (Thuật toán 1). Trong thực tế, chúng tôi sử dụng một phiên bản stochastic mini-batch của điều này trong đó fₜ thay đổi theo từng lần lặp vì nó phụ thuộc vào mỗi mini-batch dữ liệu được sử dụng trong quá trình huấn luyện.

Chúng tôi lưu ý rằng đạo hàm theo hướng dₜ trong Thuật toán 1 có thể có dấu dương hoặc âm. Khi dấu là âm, gradient thuận gₜ tương ứng với việc lùi lại từ hướng của vₜ, hoặc đảo ngược hướng để hướng về gradient thực trong kỳ vọng. Hình 1 cho thấy hai mẫu vₖ minh họa hành vi này.

**Thuật toán 1** Forward gradient descent (FGD)
**Yêu cầu:** α: tốc độ học
**Yêu cầu:** f: hàm mục tiêu  
**Yêu cầu:** θ₀: vector tham số ban đầu
t ← 0    // Khởi tạo
**while** t chưa hội tụ **do**
    t ← t + 1
    vₜ ~ N(0,I)    // Lấy mẫu nhiễu loạn
    // Lưu ý: phần sau tính fₜ và dₜ đồng thời và mà không cần tính ∇f trong quá trình
    fₜ, dₜ ← f(θₜ), ∇f(θₜ)·v    // Forward AD (Mục 3.1)
    gₜ ← vₜdₜ    // Gradient thuận
    θₜ₊₁ ← θₜ - αgₜ    // Cập nhật tham số
**end while**
**return** θₜ

Trong bài báo này, chúng tôi giới hạn phạm vi của mình vào FGD để nghiên cứu rõ ràng thuật toán cơ bản này và so sánh nó với backpropagation chuẩn, mà không có các yếu tố gây nhiễu như momentum hoặc các sơ đồ tốc độ học thích ứng. Chúng tôi tin rằng các phần mở rộng của phương pháp cho các họ thuật toán tối ưu hóa dựa trên gradient khác là có thể.

### 3.4. Lựa chọn Phân phối Hướng

Như được chứng minh trong Mục 3.2, phân phối đa biến p(v) từ đó các vector hướng v được lấy mẫu phải có hai thuộc tính: (1) các thành phần phải độc lập với nhau (ví dụ, một Gaussian chéo) và (2) các thành phần phải có trung bình bằng không và phương sai đơn vị.

Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng chuẩn tắc đa biến làm phân phối hướng p(v) sao cho v ~ N(0,I), tức là vᵢ ~ N(0,1) độc lập cho tất cả i. Chúng tôi để việc khám phá các phân phối khác có thể chấp nhận được cho công việc tương lai.

## 4. Công trình liên quan

Ý tưởng thực hiện tối ưu hóa bằng cách sử dụng nhiễu loạn ngẫu nhiên, do đó tránh tính toán liên hợp, là trực quan đằng sau nhiều phương pháp khác nhau, bao gồm simulated annealing (Kirkpatrick et al., 1983), stochastic approximation (Spall et al., 1992), stochastic convex optimization (Nesterov & Spokoiny, 2017; Dvurechensky et al., 2021), và các phương pháp học dựa trên tương quan (Barto et al., 1983), cho phép thực hiện phần cứng hiệu quả (Alspector et al., 1988). Công trình của chúng tôi ở đây thuộc lớp tổng quát của các phương pháp gọi là nhiễu loạn trọng số; xem Pearlmutter (1994, §4.4) để có cái nhìn tổng quan cùng với mô tả về một phương pháp thu thập thông tin bậc hai một cách hiệu quả trong quá trình nhiễu loạn, điều này cho thấy rằng các biến thể bậc hai được tăng tốc của phương pháp hiện tại có thể khả thi. Lưu ý rằng phương pháp của chúng tôi là mới trong việc tránh lỗi cắt ngắn của các phương pháp nhiễu loạn trọng số trước đây bằng cách sử dụng AD thay vì nhiễu loạn nhỏ nhưng hữu hạn, do đó hoàn toàn tránh phương pháp sai phân chia và các vấn đề số học liên quan.

Trong tài liệu mạng neural, các lựa chọn thay thế cho backpropagation được đề xuất bao gồm target propagation (LeCun, 1986; 1987; Bengio, 2014; 2020; Meulemans et al., 2020), một kỹ thuật truyền các giá trị mục tiêu thay vì gradient ngược giữa các lớp. Đối với mạng neural tái phát (RNN), các phương pháp khác nhau cho vấn đề gán tín dụng trực tuyến có những đặc điểm chung với AD chế độ thuận (Pearlmutter, 1995). Một ví dụ sớm là thuật toán real-time recurrent learning (RTRL) (Williams & Zipser, 1989) tích lũy độ nhạy cục bộ trong một RNN trong quá trình thực thi thuận, theo cách tương tự như AD thuận. Một ví dụ rất gần đây trong lĩnh vực RTRL là một bài nộp ẩn danh mà chúng tôi xác định vào thời điểm soạn thảo bản thảo này, trong đó các tác giả sử dụng đạo hàm theo hướng để cải thiện một số ước lượng gradient, ví dụ synthetic gradients (Jaderberg et al., 2017), first-order meta-learning (Nichol et al., 2018), như được áp dụng cho RNN (Anonymous, 2022).

Các thuật toán coordinate descent (CD) (Wright, 2015) có cấu trúc mà trong mỗi lần lặp tối ưu hóa chỉ có một thành phần duy nhất ∂f/∂θᵢ của gradient ∇f được sử dụng để tính cập nhật. Nesterov (2012) cung cấp một phần mở rộng của CD gọi là random coordinate descent (RCD), dựa trên đạo hàm theo hướng tọa độ, trong đó các hướng được ràng buộc vào các trục tọa độ được chọn ngẫu nhiên trong miền của hàm khác với các hướng tùy ý mà chúng tôi sử dụng trong phương pháp của mình. Một sử dụng gần đây của RCD là bởi Ding & Li (2021) trong lấy mẫu Langevin Monte Carlo, trong đó các tác giả báo cáo không có lợi ích tính toán vì RCD cần được chạy nhiều lần mỗi lần lặp để đạt được dung sai lỗi đặt trước.

Phương pháp SEGA (SkEtched GrAdient) bởi Hanzely et al. (2018) dựa trên ước lượng gradient thông qua các biến đổi tuyến tính ngẫu nhiên của gradient được gọi là "sketch" được tính bằng sai phân hữu hạn. Jacobian sketching bởi Gower et al. (2018b) được thiết kế để cung cấp ước lượng tốt của Jacobian, theo cách tương tự như các phương pháp quasi-Newton cập nhật ước lượng Hessian (Gower et al., 2018a).

Cuối cùng, có những phương pháp khác, và liên quan xa hơn, liên quan đến ước lượng gradient như synthetic gradients (Jaderberg et al., 2017), được thúc đẩy bởi nhu cầu phá vỡ cấu trúc thuận-ngược tuần tự của backpropagation, và ước lượng gradient Monte Carlo (Mohamed et al., 2020), trong đó gradient của kỳ vọng của một hàm được tính đối với các tham số định nghĩa phân phối được tích phân.

Để xem xét về nguồn gốc của AD chế độ ngược và backpropagation, chúng tôi giới thiệu độc giả quan tâm đến Schmidhuber (2020) và Griewank (2012).

## 5. Thí nghiệm

Chúng tôi thực hiện AD thuận trong PyTorch để thực hiện các thí nghiệm (chi tiết được đưa ra trong Mục 6). Trong tất cả các thí nghiệm, ngoại trừ trong Mục 5.1, chúng tôi sử dụng suy giảm tốc độ học với αᵢ = α₀e^(-ik), trong đó αᵢ là tốc độ học tại lần lặp i, α₀ là tốc độ học ban đầu, và k = 10⁻⁴. Trong tất cả các thí nghiệm, chúng tôi chạy gradient thuận và backpropagation cho số lần lặp bằng nhau. Chúng tôi chạy mã với CUDA trên GPU Nvidia Titan XP và sử dụng kích thước minibatch là 64.

Đầu tiên, chúng tôi xem xét các hàm kiểm tra cho tối ưu hóa, và so sánh hành vi của gradient thuận và backpropagation trong không gian R² nơi chúng ta có thể vẽ và theo dõi quỹ đạo tối ưu hóa. Sau đó, chúng tôi chia sẻ kết quả của các thí nghiệm với việc huấn luyện các kiến trúc ML có độ phức tạp tăng dần. Chúng tôi đo không có sự khác biệt thực tế nào về sử dụng bộ nhớ giữa hai phương pháp (ít hơn 0,1% khác biệt trong mỗi thí nghiệm).

### 5.1. Quỹ đạo Tối ưu hóa của Các Hàm Kiểm tra

Trong Hình 2, chúng tôi cho thấy kết quả của các thí nghiệm với:

• hàm Beale, f(x,y) = (1.5 - x + xy)² + (2.25 - x + xy²)² + (2.625 - x + xy³)²

• và hàm Rosenbrock, f(x,y) = (a - x)² + b(y - x²)², trong đó a = 1, b = 100.

Lưu ý rằng gradient thuận và backpropagation có độ phức tạp thời gian gần như tương tự trong những trường hợp này, gradient thuận nhanh hơn một chút mỗi lần lặp. Quan trọng là, chúng ta thấy rằng các bước gradient thuận hoạt động giống như backpropagation trong kỳ vọng, như được thấy trong các biểu đồ loss theo lần lặp (cột trái nhất) và quỹ đạo tối ưu hóa (cột phải nhất).

### 5.2. Các Thước đo Thực nghiệm về Độ phức tạp

Để so sánh hai thuật toán được áp dụng cho các vấn đề ML trong phần còn lại của mục này, chúng tôi sử dụng một số thước đo.

Để so sánh runtime, chúng tôi sử dụng các hệ số Rₓ và Rᵦ được định nghĩa trong Mục 2.3. Để tính các hệ số này, chúng tôi đo runtime(f) là thời gian cần để chạy một kiến trúc cho trước với một minibatch mẫu dữ liệu và tính loss, mà không thực hiện bất kỳ tính toán đạo hàm và cập nhật tham số nào. Lưu ý rằng trong các phép đo Rₓ và Rᵦ, thời gian dành cho gradient descent (cập nhật tham số) được bao gồm, thêm vào thời gian dành cho tính toán đạo hàm. Chúng tôi cũng giới thiệu tỷ lệ Rₓ/Rᵦ như một thước đo chi phí runtime của gradient thuận so với chi phí của backpropagation trong một kiến trúc cho trước.

Để so sánh hiệu suất loss, chúng tôi định nghĩa Tᵦ là thời gian mà tại đó validation loss thấp nhất được đạt bởi backpropagation (trung bình trên các lần chạy). Tₓ là thời gian mà cùng validation loss được đạt bởi gradient thuận cho cùng kiến trúc. Tỷ lệ Tₓ/Tᵦ cho chúng ta thước đo thời gian cần thiết cho chế độ thuận để đạt được validation loss tối thiểu so với thời gian của backpropagation.

### 5.3. Hồi quy Logistic

Hình 3 cho kết quả của một số lần chạy hồi quy logistic đa thức cho phân loại chữ số MNIST. Chúng tôi quan sát thấy rằng chi phí runtime của gradient thuận và backpropagation so với runtime cơ sở là Rₓ = 2.435 và Rᵦ = 4.389, tương thích với những gì người ta có thể mong đợi từ một hệ thống AD điển hình (Mục 2.3). Các tỷ lệ Rₓ/Rᵦ = 0.555 và Tₓ/Tᵦ = 0.553 cho thấy rằng gradient thuận nhanh hơn khoảng gấp đôi so với backpropagation trong cả runtime và hiệu suất loss. Trong vấn đề đơn giản này, những tỷ lệ này trùng khớp vì cả hai kỹ thuật đều có hành vi gần như giống hệt nhau trong không gian loss theo lần lặp, có nghĩa là lợi ích runtime được phản ánh gần như trực tiếp trong không gian loss theo thời gian. Trong các mô hình phức tạp hơn trong các tiểu mục sau, chúng ta sẽ thấy rằng các tỷ lệ loss và runtime tương đối có thể khác nhau trong thực tế.

### 5.4. Mạng Neural Đa lớp

Hình 4 cho thấy hai thí nghiệm với mạng neural đa lớp (NN) cho phân loại MNIST với các tốc độ học khác nhau. Kiến trúc chúng tôi sử dụng có ba lớp fully-connected kích thước 1024, 1024, 10, với kích hoạt ReLU sau hai lớp đầu tiên. Trong kiến trúc mô hình này, chúng tôi quan sát chi phí runtime của gradient thuận và backpropagation so với runtime cơ sở là Rₓ = 2.468 và Rᵦ = 4.165, và thước đo tương đối Rₓ/Rᵦ = 0.592 trung bình. Những con số này gần như giống với trường hợp hồi quy logistic.

Hàng trên (tốc độ học 2×10⁻⁵) cho thấy kết quả trong đó gradient thuận và backpropagation hoạt động gần như giống hệt nhau trong loss theo lần lặp (biểu đồ trái nhất), dẫn đến tỷ lệ Tₓ/Tᵦ gần với Rₓ/Rᵦ. Chúng tôi cho thấy kết quả này để truyền đạt một ví dụ trong đó hành vi tương tự như những gì chúng tôi quan sát được cho hồi quy logistic, trong đó hành vi loss theo lần lặp giữa các kỹ thuật là gần như giống nhau và lợi ích runtime là yếu tố đóng góp chính trong hành vi loss theo thời gian (biểu đồ thứ hai từ trái).

Thú vị là, trong thí nghiệm thứ hai (tốc độ học 2×10⁻⁴), chúng ta thấy rằng gradient thuận đạt được sự giảm nhanh hơn trong biểu đồ loss theo lần lặp. Chúng tôi tin rằng hành vi này là do bản chất khác nhau của tính ngẫu nhiên giữa SGD thông thường (backpropagation) và các thuật toán forward SGD, và chúng tôi suy đoán rằng nhiễu được giới thiệu bởi gradient thuận có thể có lợi trong việc khám phá bề mặt loss. Khi chúng ta nhìn vào biểu đồ loss theo thời gian, cũng kết hợp runtime thuận lợi của chế độ thuận, chúng ta thấy một thước đo hiệu suất loss Tₓ/Tᵦ có giá trị 0.211, đại diện cho một trường hợp nhanh hơn bốn lần so với backpropagation trong việc đạt được validation loss tham chiếu.

### 5.5. Mạng Neural Tích chập

Trong Hình 5, chúng tôi cho thấy so sánh giữa gradient thuận và backpropagation cho mạng neural tích chập (CNN) cho cùng tác vụ phân loại MNIST. CNN có bốn lớp tích chập với kernel 3×3 và 64 kênh, theo sau là hai lớp tuyến tính kích thước 1024 và 10. Tất cả các tích chập và lớp tuyến tính đầu tiên được theo sau bởi kích hoạt ReLU và có hai lớp max-pooling với kernel 2×2 sau tích chập thứ hai và thứ tư.

Trong kiến trúc này, chúng tôi quan sát hiệu suất AD thuận tốt nhất so với runtime cơ sở, trong đó chế độ thuận có Rₓ = 1.434 đại diện cho chi phí phụ chỉ 43% trên runtime cơ sở. Backpropagation với Rᵦ = 2.211 rất gần với trường hợp lý tưởng mà người ta có thể mong đợi từ một hệ thống AD ngược, mất khoảng gấp đôi thời gian. Rₓ/Rᵦ = 0.649 đại diện cho lợi ích đáng kể cho runtime AD thuận so với backpropagation. Trong không gian loss, chúng ta có tỷ lệ Tₓ/Tᵦ = 0.514 cho thấy rằng gradient thuận gần như nhanh gấp đôi backpropagation trong việc đạt được mức validation loss tham chiếu.

### 5.6. Khả năng Mở rộng

Kết quả trong các tiểu mục trước chứng minh rằng:

• huấn luyện mà không có backpropagation có thể khả thi hoạt động trong pipeline huấn luyện ML điển hình và làm như vậy theo cách cạnh tranh về mặt tính toán, và

• AD thuận thậm chí có thể đánh bại backpropagation trong giảm loss trên thời gian huấn luyện cho cùng lựa chọn siêu tham số (tốc độ học và suy giảm tốc độ học).

Để điều tra liệu những kết quả này có mở rộng đến NN lớn hơn với nhiều lớp hơn hay không, chúng tôi đo chi phí runtime và sử dụng bộ nhớ như một hàm của kích thước NN. Trong Hình 6, chúng tôi cho thấy kết quả cho kiến trúc MLP (Mục 5.4), trong đó chúng tôi chạy các thí nghiệm với số lượng lớp tăng dần trong khoảng [1,100]. Các lớp tuyến tính có kích thước 1.024, không có bias. Chúng tôi sử dụng kích thước mini-batch 64 như trước.

Nhìn vào chi phí so với runtime cơ sở, cũng thay đổi như một hàm của số lượng lớp, chúng ta thấy rằng backpropagation vẫn trong khoảng Rᵦ ∈ [4,5] và gradient thuận vẫn trong khoảng Rₓ ∈ [3,4] cho một tỷ lệ lớn của các thí nghiệm. Chúng tôi cũng quan sát thấy rằng gradient thuận vẫn thuận lợi cho toàn bộ khoảng kích thước lớp được xem xét, với tỷ lệ Rₓ/Rᵦ duy trì dưới 0.6 lên đến mười lớp và vượt hơi quá 0.8 ở 100 lớp. Quan trọng là, có hầu như không có sự khác biệt về tiêu thụ bộ nhớ giữa hai phương pháp.

## 6. Thực hiện

Chúng tôi thực hiện một hệ thống AD chế độ thuận bằng Python và dựa trên tensor PyTorch để cho phép so sánh công bằng với pipeline backpropagation điển hình trong PyTorch, được sử dụng rộng rãi bởi cộng đồng ML. Chúng tôi công bố thực hiện của mình công khai.

Engine AD chế độ thuận của chúng tôi được thực hiện từ đầu sử dụng operator overloading và tensor PyTorch không thể vi phân (requires_grad=False) như một khối xây dựng. Điều này có nghĩa là thực hiện AD thuận của chúng tôi không sử dụng thực hiện chế độ ngược của PyTorch (được gọi là "autograd") và đồ thị tính toán. Chúng tôi tạo ra kết quả backpropagation trong các thí nghiệm sử dụng mã chế độ ngược hiện có của PyTorch (requires_grad=True và .backward()) như thường lệ.

Lưu ý rằng các so sánh thực nghiệm về runtime tương đối của AD chế độ thuận và ngược phụ thuộc rất nhiều vào các chi tiết thực hiện trong một hệ thống cho trước và sẽ cho thấy sự khác biệt trên các cơ sở mã khác nhau. Khi thực hiện chế độ thuận của các phép toán tensor phổ biến trong ML (ví dụ, nhân ma trận, tích chập), chúng tôi xác định các cơ hội để làm cho các phép toán AD thuận thậm chí hiệu quả hơn nữa (ví dụ, xếp chồng các kênh của các phần primal và đạo hàm của tensor trong một tích chập). Lưu ý rằng thực hiện chúng tôi sử dụng trong bài báo này hiện tại không có những điều này. Chúng tôi mong đợi hiệu suất gradient thuận cải thiện hơn nữa khi các thực hiện chế độ thuận chất lượng cao tìm đường vào các thư viện ML chủ đạo và được tích hợp chặt chẽ vào mã tensor.

Một phương pháp thực hiện khác có thể cho phép áp dụng straightforward gradient thuận vào mã hiện có có thể dựa trên phương pháp complex-step (Martins et al., 2003), một kỹ thuật có thể xấp xỉ đạo hàm theo hướng chỉ với hỗ trợ cơ bản cho số phức.

## 7. Kết luận

Chúng tôi đã chỉ ra rằng một pipeline huấn luyện ML điển hình có thể được xây dựng mà không có backpropagation, chỉ sử dụng AD thuận, trong khi vẫn cạnh tranh về mặt tính toán. Chúng tôi mong đợi đóng góp này sẽ tìm thấy ứng dụng trong huấn luyện ML phân tán, nằm ngoài phạm vi của bài báo này. Hơn nữa, kết quả runtime chúng tôi thu được với nguyên mẫu AD thuận của chúng tôi trong PyTorch rất khích lệ và chúng tôi thận trọng lạc quan rằng chúng có thể là bước đầu tiên hướng tới việc giảm đáng kể thời gian cần để huấn luyện các kiến trúc ML, hoặc thay thế, cho phép huấn luyện các kiến trúc phức tạp hơn với ngân sách tính toán cho trước. Chúng tôi hào hứng được kết quả xác nhận và nghiên cứu thêm bởi cộng đồng nghiên cứu.

Công trình được trình bày ở đây là cơ sở cho một số hướng mà chúng tôi muốn theo đuổi. Đặc biệt, chúng tôi quan tâm đến việc làm việc trên các thuật toán gradient descent khác ngoài SGD, như SGD với momentum, và các thuật toán tốc độ học thích ứng như Adam (Kingma & Ba, 2015). Trong bài báo này, chúng tôi cố tình loại trừ những điều này để tập trung vào trường hợp SGD cô lập và rõ ràng nhất, để thiết lập kỹ thuật và một đường cơ sở. Chúng tôi cũng quan tâm đến việc thử nghiệm với các kiến trúc ML khác. Các thành phần được sử dụng trong các thí nghiệm của chúng tôi (tức là các lớp tuyến tính và tích chập, pooling, phi tuyến ReLU) đại diện cho các khối xây dựng của nhiều kiến trúc hiện tại trong thực tế, và chúng tôi mong đợi kết quả áp dụng cho những kiến trúc này.

Cuối cùng, về lâu dài, chúng tôi quan tâm đến việc xem liệu thuật toán gradient thuận có thể góp phần vào hiểu biết toán học về các cơ chế học sinh học trong não hay không, vì backpropagation đã được xem theo lịch sử là không hợp lý về mặt sinh học vì nó yêu cầu kết nối ngược chính xác (Bengio et al., 2015; Lillicrap et al., 2016; 2020). Trong bối cảnh này, một cách để nhìn vào vai trò của đạo hàm theo hướng trong gradient thuận là diễn giải nó như phản hồi của một đại lượng vô hướng toàn cục duy nhất giống hệt nhau cho tất cả các nút tính toán trong mạng.

Chúng tôi tin rằng AD thuận có các đặc tính tính toán chín muồi để khám phá bởi cộng đồng ML, và chúng tôi mong đợi rằng việc thêm nó vào cơ sở hạ tầng ML thông thường sẽ dẫn đến những đột phá lớn và các phương pháp mới.
