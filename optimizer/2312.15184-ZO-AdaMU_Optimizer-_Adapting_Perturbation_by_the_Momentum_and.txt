# 2312.15184.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/optimizer/2312.15184.pdf
# File size: 3372772 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
ZO-AdaMU Optimizer: Adapting Perturbation by the Momentum and
Uncertainty in Zeroth-order Optimization
Shuoran Jiang1, Qingcai Chen1,2*, Youcheng Pan2∗, Yang Xiang2,
Yukang Lin1, Xiangping Wu1, Chuanyi Liu3,2, Xiaobao Song3
1School of Computer Science and Technology, Harbin Institute of Technology (Shenzhen), Shenzhen, China
2Peng Cheng Laboratory, Shenzhen, China
3Institute of Data Security, Harbin Institute of Technology (Shenzhen), Shenzhen, China
shuoran.chiang@gmail.com, qingcai.chen@hit.edu.cn, panyoucheng4@gmail.com, xiangy@pcl.ac.cn
Abstract
Lowering the memory requirement in full-parameter train-
ing on large models has become a hot research area. MeZO
fine-tunes the large language models (LLMs) by just forward
passes in a zeroth-order SGD optimizer (ZO-SGD), demon-
strating excellent performance with the same GPU mem-
ory usage as inference. However, the simulated perturba-
tion stochastic approximation for gradient estimate in MeZO
leads to severe oscillations and incurs a substantial time over-
head. Moreover, without momentum regularization, MeZO
shows severe over-fitting problems. Lastly, the perturbation-
irrelevant momentum on ZO-SGD does not improve the con-
vergence rate. This study proposes ZO-AdaMU to resolve
the above problems by adapting the simulated perturbation
with momentum in its stochastic approximation. Unlike ex-
isting adaptive momentum methods, we relocate momentum
on simulated perturbation in stochastic gradient approxima-
tion. Our convergence analysis and experiments prove this is
a better way to improve convergence stability and rate in ZO-
SGD. Extensive experiments demonstrate that ZO-AdaMU
yields better generalization for LLMs fine-tuning across var-
ious NLP tasks than MeZO and its momentum variants.
Introduction
Large-scale models demonstrate remarkable abilities such as
emergence and grokking (Wei et al. 2022), especially the
large language models (LLMs) show excellent in-context
learning (ICL) abilities and revolutionize the dominant
methodology in various natural language processing (NLP)
tasks. However, full-parameter fine-tuning with billions of
parameters raises the bar for most NLP researches (Lv et al.
2023; Li et al. 2023). Malladi et al. (2023a) experimentally
proved that back-propagation optimization necessitates ap-
proximately 12 times of memory cost of forward inference.
full-parameter fine-tuning a 6.7 billion (B) OPT (Zhang et al.
2022) with Adam (Kingma and Ba 2015) requires at least
3×A100 GPUs (240GB memory).
Therefore, the memory-efficient fine-tuning method has
become an important research topic in the large-scale model
era. Some approaches have been proposed, such as ICL
does not need optimization (Sun et al. 2023a) and quickly
*Corresponding Authors.
Copyright © 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.adapts LLMs to specific use cases through demonstrations
before test examples. However, a few demonstrations can
only cover some possible case types due to the limited max-
imum sequence length for LLMs inputs. The parameter-
efficient fine-tuning methods (PEFT) (Fu et al. 2023), e.g.,
LoRA (Hu et al. 2021), ZeRA (Rajbhandari et al. 2020),
EMAT (Wu et al. 2022) et. al., update only a fraction of
the model parameters. Even though these methods can tune
LLMs with low memory and computation resources, there
are more practical solutions to fully use the emergent ability
as full-parameters fine-tuning (Ding et al. 2022; Sun et al.
2023b).
The memory-efficient full-parameter fine-tuning method
is a better way to exploit the deployment of LLMs on limited
resources. The low-memory optimization (LOMO) gives
up gradient storage in stochastic gradient descent (SGD)
(Shamir and Zhang 2013), but computes gradients at each
training step. Zeroth-order optimization (ZO) relies solely
on forward passes to estimate gradients and update param-
eters, costing the same memory size as inference. Mal-
ladi et al. (2023a) proposed the memory-efficient zeroth-
order optimizer (MeZO) to fine-tune LLMs with just for-
ward passes, and they achieved excellent performance on
various NLP tasks. The surprising performance of MeZO
is attributed to the small local effective rank of Hession pa-
rameter matrices in pre-trained deep neural networks (Pa-
pyan 2018, 2020; Ghorbani, Krishnan, and Xiao 2019; Yao
et al. 2020; Wu et al. 2020; Sagun et al. 2017). MeZO com-
putes gradients from two forward passes with random per-
turbations. Therefore, MeZO greatly reduces the latency be-
tween GPU calculations compared to LOMO. However, the
simulated perturbation stochastic approximation for the gra-
dient in MeZO is non-smoothness in consecutive training
steps. Without the regularization from momentum, like in
back-propagation optimization methods, MeZO causes the
over-fitting problem. ZO-AdaMM (Chen et al. 2019) first
efforts to integrate adaptive momentum methods with ZO
optimization, but the perturbation-irrelevant momentum re-
quires longer training steps to convergence.
Motivated by these challenges, we propose the ZO-
AdaMU optimizer, in which we first make an effort to in-
troduce adaptive momentum and uncertainty on simulated
perturbation to approximate gradients. Specifically, we re-
locate the momentum from gradient to simulated perturba-arXiv:2312.15184v1  [cs.LG]  23 Dec 2023

--- PAGE 2 ---
tion, which aims to improve the smoothness between gra-
dient approximation and actual parameter moving. In ad-
dition, the simulated perturbation stochastic approximation
(SPSA) (Maryak and Chin 2001) includes two parts sam-
pled from momentum-centered and zero-centered Gaussian
distribution. The momentum-centered Gaussian in SPSA in-
cludes a uncertainty and its value is set by a simulated an-
nealing function. With the introduction of adaptive momen-
tum and uncertainty, ZO-AdaMU demonstrates high stabil-
ity and faster convergence speed. Our convergence analy-
sis and comprehensive experiments prove these for stable
convergence rates and better global convergence. As some
gradients in ZO-AdaMU inevitably deviate from the predic-
tion, we propose a second-order momentum approximation
to improve the convergence rate further. The details of ZO-
AdaMU are summarized in Algorithm 1.
Contributions of our paper can be summarized as follows:
• Motivated by the problems of oscillation in MeZO and
perturbation-irrelevant momentum in ZO-AdaMM, we
first make an effort to explore a way to adapt momen-
tum and uncertainty in the zeroth-order oracle, called
ZO-AdaMU.
• Our theoretical analysis proved that momentum with un-
certainty and its relocation to simulated perturbation im-
proved the convergence rate. Meanwhile, these also con-
tribute to a better local optimal point than a well-tuned
MeZO counterpart.
• Our comprehensive experiments evaluate a variety of
NLP tasks and demonstrate that ZO-AdaMU shows a
faster convergence rate and better generalization capa-
bility than MeZO, LOMO, and Adam fine-tuned full-
parameter LLMs.
Preliminaries
Zeroth-order optimization (ZO) is a gradient-free method,
and it estimates the gradient via the simulated perturbations
stochastic approximation (SPSA). This section briefly intro-
duces the multi-point estimate version as in MeZO. In ad-
dition, the momentum-based regularization methods for ZO
optimization are also described.
Zeroth-Order Optimization
Zeroth-order (ZO) optimization has long been studied in the
context of convex and strongly convex objectives. A clas-
sical ZO gradient estimator is a simultaneous perturbation
stochastic approximation (SPSA) (Maryak and Chin 2001),
and it can replace the gradient calculation in stochastic gra-
dient descent (ZO-SGD) (Ghadimi and Lan 2013).
Consider a labelled dataset D={(xi,yi)}i∈∥D∥, a mini-
batchB ⊂ D of size B, and let L(θ;B)represents the loss
on the minibatch for a model with parameters θ∈Rd. The
ZO gradient estimate via SPSA is defined as,
b∇L(θ;B) =L(θ+ϵz;B)− L(θ−ϵz;B)
2ϵz
≈zz⊤∇L(θ;B)(1)where z∈Rdwithz∼ N (0,I),I∈R∥θ∥andϵis the
perturbation scale.
SPSA requires only two forward passes through model
θto estimate the gradient. SPSA estimated gradients can
be used to replace the gradient computation in any back-
propagation optimizer such as SGD.
θt+1=θt−ηb∇L(θ;Bt) (2)
where Btdenotes the monibatch at step tandb∇is the SPSA
estimated gradient.
Memory-efficient ZO-SGD (MeZO)
MeZO (Malladi et al. 2023a) is an in-place implementation
of ZO-SGD, and it further reduces memory requirements
with the same memory usage as the inference. Specifically,
MeZO keeps the random seed sfor all random vector zsam-
pling to generate each perturbation zat each step.
B ∈ D , s ←rand ()
θ←PerturbParameters (θ, ϵ, s)
ℓ+← L (θ,B)
θ←PerturbParameters (θ,−2ϵ, s)
ℓ−← L (θ,B)
θ←PerturbParameters (θ, ϵ, s)
grad←(ℓ+−ℓ−)/(2ϵ)
z∼ N(0,1) with seed s
θ←θ−η∗grad∗z(3)
where s←rand()samples a random seed keeping same in
one gradient updating step, ηis the learning rate and ϵis the
perturbation scale.
Adaptive Momentum Method for ZO
ZO-AdaMM (Chen et al. 2019) first make an effort to in-
tegrate adaptive momentum methods with ZO-SGD, and it
shows theoretically convergence guarantees for both convex
and non-convex constrained optimization. The method is in-
spired by the Adam optimization algorithm and extends its
capabilities to handle scenarios where gradient information
is unavailable. Zo-AdaMM leverages both the zeroth-order
moments, which capture the function’s behavior, and the his-
torical information of previous iterations to dynamically ad-
just the step size and momentum for optimization.
This method has the potential to advance optimization
techniques in scenarios where gradient information is not
available, opening new avenues for solving complex real-
world optimization problems. However, it suffers a slow-
down factor O(√
d)compared with first-order Adam.
Methodology
In this section, we propose the ZO-AdaMU optimizer by
adapting perturbation with the momentum and uncertainty
in the zeroth-order oracle. Unlike the post-hoc momentum
estimate in back-propagation optimization methods, ZO-
AdaMU adapts the simulated perturbation with momentum
and introduces the adaptive uncertainties in stochastic ap-
proximation. In addition, we propose a simulated annealing

--- PAGE 3 ---
mechanism on uncertainty in SPSA and smoothing param-
eters to balance the weight of momentum in gradient ap-
proximation. We theoretically analyze that ZO-AdaMU has
a faster convergence rate and better global convergence.
Adapting Momentum by Momentum and
Uncertainty in SPSA
The simulated perturbation in ZO-AdaMU is computed from
two parts of a momentum-centered and a zero-centered
Gaussian distribution:
˙zt+1∼ N (0,√αt+1)
¨zt+1∼ N
mt,p
1−αt+1
mt+1=β1˙zt+1+ (1−β1)¨zt+1
s.t. 0⩽αt+1⩽1,0⩽β1⩽1(4)
where mtandαt+1denote the momentum of history per-
turbations and the adaptive uncertainty, these variables also
imply the mean and variance for momentum-centered Gaus-
sian distribution. Consistently, 0and1−αt+1represent
the mean and variance for zero-centered Gaussian distri-
bution. ˙zt+1and¨zt+1represent the momentum with un-
certainty and purely stochastic perturbation in SPSA. The
hyper-parameter β1
t+1is the smoothing parameter, and it is
also adaptive with a simulated annealing function.
In this way, the gradient on the minibatch Bt∈ D for a
given model f(θ)is estimated by,
b∇L(θ;Bt) =L(θ+ϵmt;Bt)− L(θ−ϵmt;Bt)
2ϵmt(5)
ZO-AdaMU also mimics the exponential moving average
(EMA) on the square of the gradient (Zhuang et al. 2020)
to regularize step size, and proposes a substitute defined as
follows,
vt+1=β2
t+1˙z2
t+1+ (1−β2
t+1)¨z2
t+1
θt+1=θt−ηb∇L(θ,Bt)q
v2
t+1+σ(6)
where β2
t+1is an adaptive smoothing parameter and σis a
small noise and typically set as 10−8.
Figure 1: The simulated annealing on α,β1andβ3.The big change from traditional adaptive momentum
methods is that the smoothing parameters β1
t,β2
tand un-
certainty αtare adaptive in a simulated annealing way as
follows,
Anneal (t) =

1, t∈[1, T1)
0.5 + 0 .5 cos
π
T3−T1
T3−tφT3−T2
T2−T1
, t∈[T1, T2)
0.9, t∈[T2, T3)(7)
where φ= 1 forα,φ= 0.1forβ1andφ= 1.5for
β2.t∈[1, T1)is the warm-up process to estimate a ma-
jor optimization orientation in SPSA without any influence
of momentum. The second t∈[T1, T2)process accelerates
the optimization by momentum-based stochastic approxi-
mation, the uncertainty αon momentum gradually increases
until0.5as shown in Figure 1. The last t∈[T2, T3)process
fixesα= 0.5,β(1)
t= 0.9andβ(2)
t= 0.01to find a global
convergence. The simulated annealing is plotted in Figure 1.
The proposed ZO-AdaMU is summarized in Algorithm 1.
Algorithm 1: ZO-AdaMU Optimizer
1:Input: parameters θ∈Rd, lossL:Rd→R, step
budget T1,T2,T3, perturbation scale ϵ, small num-
berσ= 10−8, batch size B, momentum uncertainty
α, learning rate η, EMA of perturbation mandvEMA
on its square.
2:fort= 1,···, Tdo
3: Sample batch Bt⊂ D and random seed s
4:θ←Perturb (θ,m(t)ϵ, s, t ),ℓ+← L(θ;Bt)
5:θ←Perturb (θ,m(t),−2ϵ, s, t ),ℓ−← L(θ;Bt)
6:θ←Perturb (θ,m(t), ϵ, s, t ),gt←(ℓ+−
ℓ−)/(2ϵ)
7: Reset random seed s
8: forθi∈θdo
9: α, β(t)
1, β(t)
2=Anneal (t)
10: ˙z∼ N(0,√α),¨z∼ N(m(t−1)
i,√1−α)
11: m(t)
i←β(t)
1·˙z+ (1−β(t)
1)·¨z
12: vi=β(t)
2·˙z2+ (1−β(t)
2)·¨z2
13: θi←θi−ηt·gt√v+σ·m(t)
i
14: end for
15:end for
1:Subroutine Perturb (θ,m, ϵ, s, t )
2: Reset random seed s
3: α(t), β(t)
1←Anneal (t)
4: ˙z∼ N(0,I∗√
α(t)),¨z∼ N(m,I∗√
1−α(t))
5:θ←ϵ∗(β(t)
1·˙z+ (1−β(t)
1)·¨z)
1:Subroutine Anneal (t, T1, T2, T3)
2: α←Anneal (t) # refer to Eq. (7)
Convergence Analysis
We give the theoretical analysis about why ZO-AdaMU has
higher convergence rate and better global convergence. We

--- PAGE 4 ---
Task SST-2 RTE CB BoolQ WSC WIC MultiRC COPA ReCoRD SQuAD DROP
Task type classification multiple choice generation
Zero-shot 58.8 59.6 46.4 59.0 38.5 55.0 46.9 80.0 81.2 46.2 14.6
In-context 87.0 62.1 57.1 66.9 39.4 50.5 53.1 87.0 82.5 75.9 29.6
linear probing 93.4 68.6 67.9 59.3 63.5 60.2 63.5 55.0 27.1 3.7 11.1
MeZO 91.4 66.1 67.9 67.6 63.5 61.1 60.1 88.0 81.7 84.7 30.9
MeZO (LoRA) 89.6 67.9 66.1 73.8 64.4 59.7 61.5 87.0 81.4 83.8 31.4
MeZO (prefix) 90.7 70.8 69.6 73.1 57.7 59.9 63.7 84.0 81.2 84.2 28.9
ZO-AdaMU (2 ×) 92.1 72.9 67.9 73.0 61.5 60.7 63.0 89.0 83.0 82.4 32.0
ZO-AdaMU (LoRA) 88.0 72.0 71.6 72.6 60.1 56.4 58.9 88.0 83.2 76.8 32.4
ZO-AdaMU (prefix) 88.0 61.8 72.3 74.9 56.5 58.2 61.9 86.0 82.8 85.2 30.4
Adam (FT) (12 ×) 92.0 70.8 83.9 77.1 63.5 70.1 71.1 79.0 74.1 84.9 31.3
Table 1: Experiments on OPT-13B (with 1,000 examples).
follow the convergence analysis in MeZO and pay more at-
tention to why adapting perturbation with momentum and
uncertainty can improve the stability of ZO-SGD. There-
fore, this analysis highlights the positive gains on conver-
gence rate from perturbation momentum and uncertainty.
Stable Convergence Rate
Classical descent lemma on SGD optimization highlights
that the larger gradient covariance results in slower decrease
in loss (Megerle et al. 2023).
Lemma 1 (Descent Lemma). LetL(θ)beℓ-smooth
(Wang and Xu 2019). For any unbiased gradient estimate
g(θ,B),
E[L(θt−1)|θt]− L(θt)≤ −η∥∇L (θt)∥2
+1
2η2ℓ·Eh
∥g(θ,Bt)∥2i (8)
The gradient norm plays important role in the descent
lemma. We derive the gradient norms for MeZO and ZO-
AdaMU respectively as below.
Lemma 2. LetBbe a random minibatch of size B, so the
gradient norms of MeZO and ZO-AdaMU are
b∇L(θ,B)2∼ Nd+n−1
n∥∇L (θ,B)∥,1·ϵ2
(9)
where ϵrepresents the perturbation sampling scale.
Thus,
∥∇L (θt)∥2
d+n−1
n∥∇L (θt)∥2+ϵ| {z }
η−
MeZO⩽∥∇L (θt)∥2
∥g(θt,B)∥2⩽∥∇L (θt)∥2
d+n−1
n∥∇L (θt)∥2+ϵ| {z }
η+
MeZO
(10)
In ZO-AdaMU, the simulated perturbation includes two
Gaussian distributions with variances αand1−αand
smoothing parameter β1.
b∇L(θ,B)2∼ N(d+n−1
n∥∇L (θ,B)∥,

β2
1α2+ (1−β1)2(1−α)2
ϵ2)(11)So that,
η−
AdaMU⩽∥∇L (θt)∥2
∥g(θt,B)∥2⩽η+
AdaMU
η−
AdaMU =∥∇L (θt)∥2
d+n−1
n∥∇L (θt)∥2+ϵq
β2
1α2+ (1−β1)2(1−α)2
η+
AdaMU =∥∇L (θt)∥2
d+n−1
n∥∇L (θt)∥2−ϵq
β2
1α2+ (1−β1)2(1−α)2(12)
Asp
β2
1α2+ (1−β1)2(1−α)2<1, we can conclude
thatη−
MeZO < η−
AdaMU ⩽η+
AdaMU < η+
MeZO andηMeZO =
n
d+n−1ηSGD< η AdaMU . Therefore, ZO-AdaMU has a faster
convergence rate than MeZO optimization.
The uncertainty in simulated perturbation also decreases
the local effective rank of the Hessian of the loss (Papyan
2018, 2020; Ghorbani, Krishnan, and Xiao 2019; Yao et al.
2020; Sagun et al. 2017; Wu et al. 2020).
Lemma 3. LetG(θt) = max (x,y)∈B∥∇L (θt;{(x,y)})∥,
for all θtsuch that ∥θ−θt∥ ≤ ηdG[(θ)]there is
∇2L(θ)⪯H(θt), therefore the maximum of effective rank
of gradient is tr(H(θt))/∥H(θt)∥op≈r.
With the same parameter size dand minibatch size B,
the averaged bG(θt)on gradient estimates of MeZO and ZO-
AdaMU have
bGMeZO(θt)>bGAdaMU (θt) (13)
and thus,
tr(HMeZO(θt))
∥HMeZO(θt)∥op⩽tr(HAdaMU (θt))
∥HAdaMU (θt)∥op(14)
The above analysis proves that ZO-AdaMU has a faster
speed than MeZO to decrease the loss at each step.
Better Global Convergence
The upper bound on expected average regret reflects whether
the optimization method can converge to a local optimum
(Shamir 2017; Zhuang et al. 2020).
Lemma 4. The convergence of SGD optimization is com-
monly measured by the expected average regret,
E[R(T)] =XT
t=1
ft(θt)−ft 
θ∗
(15)

--- PAGE 5 ---
where ft(θ∗)is the best value with optimal solution θ∗on
t-th step.
Assume that the loss L(θ)has bounded gradients that
∥∇L t(θ)∥2≤Gand∥∇L t(θ)∥∞≤G∞, and the
distance between any θMeZO
t,θAdaMU
t generated by MeZO
and ZO-AdaMU are both bounded as ∥θMeZO
n−θMeZO
m∥2≤
D&∥θMeZO
n−θMeZO
m∥∞≤D∞andθAdaMU
n−θAdaMU
m
2≤
D&θAdaMU
n−θAdaMU
∞
2≤D∞respectively for any m, n∈
{1,···, T}. The maximum smoothing parameters β1= 0.9
andβ2= 0.01in ZO-AdaMU and β1= 0,β2= 0in MeZO
respectively. ZO-AdaMU and MeZO achieve the following
guarantees respectively, for all T⩾1.
RMeZO(T)⩽D2
2αdX
i=1√
T+αG∞dX
i=1∥g1:T,i∥2+dX
i=1D2
∞G∞
2αλ2
RAdaMU(T)⩽D2
0.2αdX
i=1p
TvT,i+1.9αG∞
0.099×7.12dX
i=1∥g1:T,i∥2
+dX
i=1D2
∞G∞√1−β2
1.8α(1−λ)2
RMeZO(T)> R AdaMU(T)(16)
Above regret bound analysis shows that ZO-AdaMU has a
smaller expected average regret than the on of MeZO, which
proves that ZO-AdaMU has a better global convergence than
MeZO. This is also the reason why ZO-AdaMU achieves
better generalization on LMs.
Experiment
Preliminary researches (Brown et al. 2020; Gao, Fisch,
and Chen 2021; Schick and Sch ¨utze 2021) experimentally
demonstrated that zeroth-order optimization only works
with prompt learning on LLMs fine-tuning. All experiments
in this section use prompts to train the LLMs with just for-
ward passes fine-tuning (MeZO (Malladi et al. 2023a) and
ZO-AdaMU) and back-propagation fine-tuning (Adam).
To evaluate the effectiveness of the proposed adaptive per-
turbation with momentum in ZO-AdaMU for LLMs fine-
tuning, we conduct the same experiments as MeZO on both
masked language model (MLM) pre-trained LLMs (like
RoBERTa-large 350M) (Liu et al. 2019) and auto-regressive
pre-trained LLMs (OPT-13B) (Zhang et al. 2022) in few-
shot and many-shot settings with prompts. In addition, all
optimization methods are explored on full-parameter, LoRA
and prefix fine-tuning (Li and Liang 2021). Finally, we give
the visualizations of ZO-AdaMU, MeZO and Adam on 6
popular test functions for optimization.
Please refer to our code1for the details about datasets and
prompt templates in Tabels 1, 2 and 4, and hyper-parameters,
grid searching for best values in Eq. 7 and experimental re-
sults to evaluate stable convergence.
Auto-Regressive Language Models
As the auto-regressive LLMs have become the predominant
base models in NLP, like GPT-3.5, GPT-4 (Lin et al. 2023),
LLaMA (Touvron et al. 2023) and ChatGLM (Du et al.
1https://github.com/MathIsAll/ZO-AdaMU.git2022), we conduct experiments with OPT-13B on three NLP
task paradigms - sentence classification, multiple choice and
text generation. All benchmarks are selected from Super-
GLUE (Wang et al. 2019) (includes COPA, SST-2, RTE, CB,
WSC, WIC, MultiRC, ReCoRD), BoolQ (Clark et al. 2019),
SQuAD (Rajpurkar et al. 2016) and DROP (Dua et al. 2019).
The few-shot training, validation and test sets are randomly
sampled from each dataset with numbers of 1,000,500and
1,000respectively. The main results are listed in Table 1,
and which can reach the following observations and sum-
maries.
I. ZO-AdaMU has obvious advantages in complex rea-
soning tasks. Table 1 shows that ZO-AdaMU and its LoRA,
prefix variants outperform the MeZO and Adam fine-tuned
OPT on all multiple choice and text generation tasks. Specif-
ically, ZO-AdaMU and its LoRA, prefix variants outperform
MeZO’s results with 1.0%,1.0%and2.0%on COPA and
1.3%,1.8%and1.6%on ReCoRD respectively. Moreover,
the best F1 scores of ZO-AdaMU on SQuAD and DROP are
both1.0higher than the best ones of MeZO. The advantage
of ZO-AdaMU for Adam is more evident with gaps of 10.0,
9.1,0.3and1.1respectively.
II. ZO-AdaMU performs closest to back-propagation op-
timization methods across classification tasks. Experimental
results in Table 1 show that the back-propagation optimiza-
tion methods have more advantages than gradient-free meth-
ods on text classification tasks. Specifically, ZO-AdaMU ob-
tains 3 best results out of 7 classification benchmarks, and
beats MeZO counterparts on all tasks.
We design an ablation study (Table 3) by adapting mo-
mentum schedules of Adam, AdamW, AdaMax, Rmsgrad on
perturbation and gradients in ZO for LLMs prompt-tuning,
respectively. These results show that our momentum sched-
ule achieves the best results. In addition, the momentum
schedules on perturbation are generally better than the ones
on gradients, which verifies our idea that adapting momen-
tum on the perturbation is the right way.
Masked Language Models
Experimental results on OPT-13B demonstrated the promis-
ing results of ZO-AdaMU on auto-regressive pre-rained
LLMs. The second experiment extends ZO-AdaMU to the
RoBERTa-large, a popular medium-size LM in the MLM
family. This experiment follows the few-shot and many-shot
settings from Gao, Fisch, and Chen (2021) and Malladi et al.
(2023b), where k= 512 examples are sampled from per
class for many-shot fine-tuning. The results are summarized
in Table 2.
These results evaluate that (i) both ZO-AdaMU and
MeZO significantly outperform the zero-shot and linear
probing methods, which proves gradient-free ZOs really
tune the LLMs. (ii) ZO-AdaMU and MeZO outperform
Adam on 6 benchmarks, which demonstrates that the ZO-
SGD methods effectively alleviate the over-fitting problem
when the LLMs are fine-tuned on limited training data. (iii)
The SPSA estimated gradient in Adam shows just a 0.43
average improvement, while ZO-AdaMU exhibits more dra-
matic increases with an average of 1.25.

--- PAGE 6 ---
Task SST-2 SST-5 SNLI MNLI RTE TREC
Type sentiment natural language inference topic
Zero-shot 79.0 35.5 50.2 48.8 51.4 32.0
Adam 91.9 ( ±1.8) 47.5 ( ±1.9) 77.5 ( ±2.6) 70.0 ( ±2.3) 66.4 ( ±7.2) 85.0 ( ±2.5)
Adam (LoRA) 91.4 ( ±1.7) 46.7 ( ±1.1) 74.9 ( ±4.3) 67.7 ( ±1.4) 66.1 ( ±3.5) 82.7 ( ±4.1)
Adam (prefix) 91.9 ( ±1.0) 47.7 ( ±1.1) 77.2 ( ±1.3) 66.5 ( ±2.5) 66.6 ( ±2.0) 85.7 ( ±1.3)
LP 91.3 ( ±0.5) 51.7 ( ±0.5) 80.9 ( ±1.0) 71.5 ( ±1.1) 73.1 ( ±1.5) 89.4 ( ±0.5)
MeZO 93.3 ( ±0.7) 53.2 ( ±1.4) 83.0 ( ±1.0) 78.3 ( ±0.5) 78.6 ( ±2.0) 94.3 ( ±1.3)
MeZO (LoRA) 93.4 ( ±0.4) 52.4 ( ±0.8) 84.0 ( ±0.8) 77.9 ( ±0.6) 77.6 ( ±1.3) 95.0 ( ±0.7)
MeZO (prefix) 93.3 ( ±0.1) 53.6 ( ±0.5) 84.8 ( ±1.1) 79.8 (±1.2) 77.2 ( ±0.8) 94.4 ( ±0.7)
MeZO-Adam 93.3 ( ±0.6) 53.9 (±0.8) 85.3 ( ±0.8) 79.6 (±0.4) 79.2 (±1.2) 95.1 ( ±0.3)
ZO-AdaMU 93.8 (±0.3) 53.7 ( ±0.8) 83.3 ( ±0.7) 78.5 ( ±1.3) 77.9 ( ±2.0) 93.7 ( ±2.5)
ZO-AdaMU (LoRA) 93.1 ( ±0.6) 51.6 ( ±1.2) 84.4 ( ±0.5) 78.6 ( ±0.8) 78.2 ( ±1.2) 95.2 (±0.5)
ZO-AdaMU (prefix) 93.4 ( ±0.4) 53.6 ( ±0.7) 85.5 (±0.2) 79.4 ( ±1.0) 78.9 ( ±1.5) 95.0 ( ±0.7)
Table 2: Experiments on RoBERTa-large (350M parameters) that include zero-shot learning, linear probing (LP), full-parameter
fune-tuning with Adam, MeZO and ZO-AdaMU, and parameter-efficient Fine-tuning (LoRA and prefix learning) with Adam,
MeZO and ZO-AdaMU respectively. All reported numbers are averaged accuracy (standard deviation) over 5 times runs.
TasksAdaMU Adam AdamW AdaMax AadMM
δ g δ g δ g δ g δ g
SST2 92.1 90.6 90.4 90.0 88.2 91.3 87.9 64.8 90.6 78.3
ReCoRD 83.0 81.2 73.1 71.3 83.0 79.1 77.6 82.0 80.3 80.4
SQuAD 82.4 82.1 82.0 81.0 77.3 68.4 80.0 68.3 79.7 73.8
Table 3: Ablation study by adapting different momentum
schedules on perturbation ( δ) and gradient ( g) respectively.
The above experiments on MLM pre-trained LMs demon-
strate that the concepts of adapting perturbation with mo-
mentum and uncertainty in SPSA are more suitable for ZO-
SGD methods for LLMs fine-tuning.
Non-differentiable objectives
Model RoBERTa-large (350M) OPT-13B
Task SST-2 SST-5 SNLI TREC SQuAD
Zero-shot 79.00 35.50 50.20 32.00 46.20
MeZO 92.70 48.90 82.70 68.60 78.50
ZO-AdaMU 93.40 51.60 82.40 77.50 81.30
Table 4: ZO-AdaMU and MeZO with non-differentiable ob-
jectives. For classification tasks of SST-2, SST-5, SNLI and
TREC, RoBERTa-large is optimized with full-parameter and
accuracy on 500 examples; for SQuAD, OPT-13B is opti-
mized with prefix and F1 on 1,000 examples.
As our proposed ZO-AdaMU is also a gradient-free opti-
mization method, we also conduct an experiment to evaluate
ZO-AdaMU on RoBERTa-large and OPT with accuracy or
F1 as objectives. Table 4 lists all results and they demon-
strate that ZO-AdaMU outperforms its MeZO counterpart
on 4 out of 5 non-differentiable objectives.HardwareLargest OPT that can fit
Adam MeZO ZO-AdaMU
1×A100 (80GB) 2.7B 30B 30B
2×A100 (160GB) 6.7B 66B 66B
4×A100 (320GB) 13B 66B 66B
8×A100 (640GB) 30B 175B 175B
1×V100 (30GB) 2.7B 6.7B 6.7B
2×V100 (60GB) 2.7B 13B 13B
4×V100 (120GB) 6.7B 30B 30B
8×V100 (240GB) 13B 66B 66B
Table 5: Largest OPT models that the mainstream hardwares
of Nvidia A100 and V100 can tune.
Memory usage
As the storage of the expected moving average (EMA)
for perturbation momentum, ZO-AdaMU slightly increases
memory usage compared to MeZO. In Figure 3, we sum-
marize the memory usage of zero-shot, in-context learn-
ing (ICL), prefix learning and full-parameter fine-tuning
with Adam, MeZO and ZO-AdaMU. These statistics report
the peak GPU memory usage by testing OPT models with
Nvidia A100 GPUs on the SQuAD task (maximum length
2048 and minibatch size 1).
As shown in Figure 3 that MeZO exhibits the same mem-
ory consumption as zero-shot, which saves of up to 7 times
of memory at least compared to back-propagation fine-
tuning and 6 times compared to prefix fine-tuning. Even
though our proposed ZO-AdaMU has a slight of memory
usage increase compared to MeZO, it does not raise the re-
quirements for mainstream hardware (like Nvidia A100 and
V100) as shown in Table 5. This advantage enables train-
ing larger models within a fixed hardware budget, as illus-
trated in Figure 3. Specifically, using a single A100 GPU,
ZO-AdaMU allows for tuning a model that is 11 times larger

--- PAGE 7 ---
Figure 2: 2D trajectories of Adam, AdaMax, MeZO and ZO-AdaMU on 6 test functions. In all cases, ZO-AdaMU performs
comparably to that first-order optimization methods, like Adam and AdaMax, but MeZO does not reach optimal points.
than what is feasible with full-parameter fine-tuning.
Figure 3: GPU memory consumption with OPT models fine-
tuning on 2048 maximum length per example.
Trajectory Visualization on Test Functions
In this section, we validate the training trajectories of Adam,
AdaMax, MeZO and ZO-AdaMU on 6 test functions, and
the 2D trajectories are shown in Figure 2. These test func-
tions are useful to evaluate characteristics of optimization
algorithms, such as convergence rate, precision, robustness
and general performance.
• Function a: f(x, y) =|x|+|y|with global minimum
f(0,0) = 0 and search domain −3⩽x, y⩽3;
• Function b: f(x, y) =|x+y|+|x−y|/10with global
minimum f(0,0) = 0 and search domain −3⩽x, y⩽3;
• Function c: f(x, y) = (x+y)2+ (x−y)2/10with global
minimum f(0,0) = 0 and search domain −3⩽x, y⩽3;
• Function d: f(x, y) =|x|/10 +|y|with global minimum
f(0,0) = 0 and search domain −3⩽x, y⩽3;• Function Beale : f(x, y) = (1 .5−x+xy)2+ (2.25−
x+xy2)2+ (2.625−x+xy3)2with global minimum
f(3,0.5) = 0 and search domain −4.5⩽x, y⩽4.5;
• Function Rosenbrock: f(x, y) = 100( x−y2)2+ (1−y)2
with global minimum f(1,1) = 0 and search domain
−4.5⩽x, y⩽4.5.
In all test functions, adapting perturbation with momen-
tum in ZO-SGD reaches optimal points on all test functions,
while MeZO optimizer fails to find any global minimum.
Compared with momentum-based back-propagation opti-
mizers, like Adam and AdaMax, ZO-AdaMU shows similar
trajectories and reaches the optimal points on test functions
of (a), (b) and (c). In addition, on test functions of (d), Beale
and Rosenbrock, even though ZO-AdaMU shows different
optimization trajectories, it reaches the optimal points with
a faster speed than Adam and AdaMax.
Conclusion
We propose a ZO-AdaMU optimizer in this work, which
adapts the momentum and uncertainty on simulated pertur-
bation in the zeroth-order optimizer (ZO). To our knowl-
edge, ZO-AdaMU is the first ZO-SGD optimizer that adapts
the momentum on the stochastic approximation for sim-
ulated perturbation. Even though storing perturbation mo-
mentum requires a little extra memory cost compared with
MeZO, ZO-AdaMU is consistent with MeZO for require-
ments of mainstream GPU hardware. We experimentally
validate that ZO-AdaMU outperforms MeZO and back-
propagation optimizers on convergence rate and generaliza-
tion across various NLP tasks. Our visualizations prove that
ZO-AdaMU performs comparably with Adam and AdaMax
on popular test functions in machine learning.

--- PAGE 8 ---
Acknowledgments
This work is jointly supported by grants from the Na-
tional Key R&D Program of China (No. 2022ZD0116002),
the Project funded by China Postdoctoral Science Founda-
tion (No. 2023M741843), Shenzhen Science and Technol-
ogy Plan (No. ShenKeJiChuangXinZhi[2023]87), the Sci-
ence and Technology Department of Guizhou Province
(No. Qiankehe Support[2022]General019), the National So-
cial Science Foundation - Major Project (No. 20&ZD226),
the Shenzhen Development and Reform Commission (No.
XMHT20190108009), the National Natural Science Foun-
dation of China (No. 62276075, 62106115, 62006062 and
62176076), the Guangdong Provincial Key Laboratory (No.
2022B1212010005), the Major Key Project of PCL (No.
PCL2022D01, PCL2023A09), the Key Laboratory of Intel-
ligent Computing in Network Environment.
References
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,
A.; et al. 2020. Language models are few-shot learners. Ad-
vances in neural information processing systems , 33: 1877–
1901.
Chen, X.; Liu, S.; Xu, K.; Li, X.; Lin, X.; Hong, M.; and
Cox, D. 2019. Zo-adamm: Zeroth-order adaptive momen-
tum method for black-box optimization. Advances in neural
information processing systems , 32.
Clark, C.; Lee, K.; Chang, M.-W.; Kwiatkowski, T.; Collins,
M.; and Toutanova, K. 2019. BoolQ: Exploring the Surpris-
ing Difficulty of Natural Yes/No Questions. In Proceedings
of NAACL-HLT , 2924–2936.
Ding, N.; Qin, Y .; Yang, G.; Wei, F.; Yang, Z.; Su, Y .;
Hu, S.; Chen, Y .; Chan, C.-M.; Chen, W.; et al. 2022.
Delta tuning: A comprehensive study of parameter efficient
methods for pre-trained language models. arXiv preprint
arXiv:2203.06904 .
Du, Z.; Qian, Y .; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and
Tang, J. 2022. GLM: General Language Model Pretraining
with Autoregressive Blank Infilling. In Proceedings of the
60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , 320–335.
Dua, D.; Wang, Y .; Dasigi, P.; Stanovsky, G.; Singh, S.;
and Gardner, M. 2019. DROP: A Reading Comprehension
Benchmark Requiring Discrete Reasoning Over Paragraphs.
InProceedings of NAACL-HLT , 2368–2378.
Fu, Z.; Yang, H.; So, A. M.-C.; Lam, W.; Bing, L.; and Col-
lier, N. 2023. On the effectiveness of parameter-efficient
fine-tuning. In Proceedings of the AAAI Conference on Ar-
tificial Intelligence , 11, 12799–12807.
Gao, T.; Fisch, A.; and Chen, D. 2021. Making pre-trained
language models better few-shot learners. In Joint Con-
ference of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International Joint
Conference on Natural Language Processing, ACL-IJCNLP
2021 , 3816–3830. Association for Computational Linguis-
tics (ACL).Ghadimi, S.; and Lan, G. 2013. Stochastic first-and
zeroth-order methods for nonconvex stochastic program-
ming. SIAM Journal on Optimization , 23(4): 2341–2368.
Ghorbani, B.; Krishnan, S.; and Xiao, Y . 2019. An inves-
tigation into neural net optimization via hessian eigenvalue
density. In International Conference on Machine Learning ,
2232–2241. PMLR.
Hu, E. J.; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang, S.; Wang,
L.; Chen, W.; et al. 2021. LoRA: Low-Rank Adaptation of
Large Language Models. In International Conference on
Learning Representations .
Kingma, D.; and Ba, J. 2015. Adam: A Method for Stochas-
tic Optimization. In International Conference on Learning
Representations (ICLR) . San Diega, CA, USA.
Li, X. L.; and Liang, P. 2021. Prefix-Tuning: Optimiz-
ing Continuous Prompts for Generation. In Proceedings of
the 59th Annual Meeting of the Association for Computa-
tional Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1: Long Pa-
pers) , 4582–4597.
Li, Y .; Hu, B.; Chen, X.; Ma, L.; and Zhang, M. 2023. LM-
Eye: An Interactive Perception Network for Large Language
Models. arXiv preprint arXiv:2305.03701 .
Lin, J. C.; Younessi, D. N.; Kurapati, S. S.; Tang, O. Y .; and
Scott, I. U. 2023. Comparison of GPT-3.5, GPT-4, and hu-
man user performance on a practice ophthalmology written
examination. Eye, 1–2.
Liu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;
Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .
2019. Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Lv, K.; Yang, Y .; Liu, T.; Gao, Q.; Guo, Q.; and Qiu,
X. 2023. Full Parameter Fine-tuning for Large Lan-
guage Models with Limited Resources. arXiv preprint
arXiv:2306.09782 .
Malladi, S.; Gao, T.; Nichani, E.; Damian, A.; Lee,
J. D.; Chen, D.; and Arora, S. 2023a. Fine-Tuning Lan-
guage Models with Just Forward Passes. arXiv preprint
arXiv:2305.17333 .
Malladi, S.; Wettig, A.; Yu, D.; Chen, D.; and Arora, S.
2023b. A kernel-based view of language model fine-tuning.
InInternational Conference on Machine Learning , 23610–
23641. PMLR.
Maryak, J. L.; and Chin, D. C. 2001. Global random op-
timization by simultaneous perturbation stochastic approxi-
mation. In Proceedings of the 2001 American control con-
ference.(Cat. No. 01CH37148) , volume 2, 756–762. IEEE.
Megerle, D.; Otto, F.; V olpp, M.; and Neumann, G. 2023.
Stable Optimization of Gaussian Likelihoods.
Papyan, V . 2018. The full spectrum of deepnet hessians at
scale: Dynamics with sgd training and sample size. arXiv
preprint arXiv:1811.07062 .
Papyan, V . 2020. Traces of class/cross-class structure per-
vade deep learning spectra. The Journal of Machine Learn-
ing Research , 21(1): 10197–10260.

--- PAGE 9 ---
Rajbhandari, S.; Rasley, J.; Ruwase, O.; and He, Y . 2020.
Zero: Memory optimizations toward training trillion param-
eter models. In SC20: International Conference for High
Performance Computing, Networking, Storage and Analysis ,
1–16. IEEE.
Rajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.
SQuAD: 100,000+ Questions for Machine Comprehension
of Text. In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing , 2383–2392.
Sagun, L.; Evci, U.; G ¨uney, V . U.; Dauphin, Y .; and Bot-
tou, L. 2017. Empirical Analysis of the Hessian of Over-
Parametrized Neural Networks. ArXiv , abs/1706.04454.
Schick, T.; and Sch ¨utze, H. 2021. Exploiting Cloze-
Questions for Few-Shot Text Classification and Natural Lan-
guage Inference. In Proceedings of the 16th Conference of
the European Chapter of the Association for Computational
Linguistics: Main Volume , 255–269.
Shamir, O. 2017. An optimal algorithm for bandit and zero-
order convex optimization with two-point feedback. The
Journal of Machine Learning Research , 18(1): 1703–1713.
Shamir, O.; and Zhang, T. 2013. Stochastic gradient de-
scent for non-smooth optimization: Convergence results and
optimal averaging schemes. In International conference on
machine learning , 71–79. PMLR.
Sun, S.; Liu, Y .; Iter, D.; Zhu, C.; and Iyyer, M. 2023a. How
does in-context learning help prompt tuning? arXiv preprint
arXiv:2302.11521 .
Sun, X.; Ji, Y .; Ma, B.; and Li, X. 2023b. A Com-
parative Study between Full-Parameter and LoRA-based
Fine-Tuning on Chinese Instruction Data for Instruc-
tion Following Large Language Model. arXiv preprint
arXiv:2304.08109 .
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;
Babaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,
S.; et al. 2023. Llama 2: Open foundation and fine-tuned
chat models. arXiv preprint arXiv:2307.09288 .
Wang, A.; Pruksachatkun, Y .; Nangia, N.; Singh, A.;
Michael, J.; Hill, F.; Levy, O.; and Bowman, S. 2019. Su-
perglue: A stickier benchmark for general-purpose language
understanding systems. Advances in neural information pro-
cessing systems , 32.
Wang, D.; and Xu, J. 2019. Differentially private empirical
risk minimization with smooth non-convex loss functions: A
non-stationary view. In Proceedings of the AAAI Conference
on Artificial Intelligence , 01, 1182–1189.
Wei, J.; Tay, Y .; Bommasani, R.; Raffel, C.; Zoph, B.;
Borgeaud, S.; Yogatama, D.; Bosma, M.; Zhou, D.; Met-
zler, D.; Chi, E. H.; Hashimoto, T.; Vinyals, O.; Liang, P.;
Dean, J.; and Fedus, W. 2022. Emergent Abilities of Large
Language Models. Transactions on Machine Learning Re-
search . Survey Certification.
Wu, Y .; Zhao, Y .; Hu, B.; Minervini, P.; Stenetorp, P.; and
Riedel, S. 2022. An Efficient Memory-Augmented Trans-
former for Knowledge-Intensive NLP Tasks. In Proceedings
of the 2022 Conference on Empirical Methods in Natural
Language Processing , 5184–5196.Wu, Y .; Zhu, X.; Wu, C.; Wang, A.; and Ge, R. 2020. Dis-
secting hessian: Understanding common structure of hessian
in neural networks. arXiv preprint arXiv:2010.04261 .
Yao, Z.; Gholami, A.; Keutzer, K.; and Mahoney, M. W.
2020. Pyhessian: Neural networks through the lens of the
hessian. In 2020 IEEE international conference on big data
(Big data) , 581–590. IEEE.
Zhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;
Chen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V .; et al. 2022.
Opt: Open pre-trained transformer language models. arXiv
preprint arXiv:2205.01068 .
Zhuang, J.; Tang, T.; Ding, Y .; Tatikonda, S. C.; Dvornek,
N.; Papademetris, X.; and Duncan, J. 2020. Adabelief op-
timizer: Adapting stepsizes by the belief in observed gradi-
ents. Advances in neural information processing systems ,
33: 18795–18806.
