# 2302.12022.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/optimizer/2302.12022.pdf
# File size: 1514585 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
DoG is SGD’s Best Friend:
A Parameter-Free Dynamic Step Size Schedule
Maor Ivgi
maor.ivgi@cs.tau.ac.ilOliver Hinder
ohinder@pitt.eduYair Carmon
ycarmon@tauex.tau.ac.il
Abstract
We propose a tuning-free dynamic SGD step size formula, which we call Distance over Gra-
dients (DoG). The DoG step sizes depend on simple empirical quantities (distance from the
initial point and norms of gradients) and have no “learning rate” parameter. Theoretically, we
show that a slight variation of the DoG formula enjoys strong parameter-free convergence guar-
antees for stochastic convex optimization assuming only locally bounded stochastic gradients.1
Empirically, we consider a broad range of vision and language transfer learning tasks, and show
that DoG’s performance is close to that of SGD with tuned learning rate. We also propose a
per-layer variant of DoG that generally outperforms tuned SGD, approaching the performance
of tuned Adam. A PyTorch implementation is available at https://github.com/formll/dog.
1 Introduction
While stochastic optimization methods drive continual improvements in machine learning, choosing
the optimization parameters—and particularly the learning rate—remains a difficulty. Standard
methodologies include searching over a set of learning rates, or simply picking the learning rate
from prior work. The former incurs a substantial computational overhead, while the latter risks
training a suboptimal model.
The rich literature on adaptive gradient methods (AdaGrad, Adam, and their many variants) of-
fers optimization algorithms that better exploit problem structure [e.g., 29, 48, 34, 84, 57]. However,
these methods still have a learning rate parameter that requires tuning. The theoretically-optimal
value of this parameter depends on unknown problem properties. For example, on convex problems
the optimal learning rate of AdaGrad is related to the distance between the initial point and the
optimal solution, while in non-convex settings it is related to the function’s smoothness and initial
optimality gap [33, 95, 30].
Parameter-free optimization aims to remove the need for such tuning by designing algorithms
that achieve a near-optimal rate of convergence with almost no knowledge of the problem proper-
ties [87]. Most works in this field [e.g., 58, 69, 21, 61, 10, 42, 110] use advanced online learning
techniques to construct algorithms that, for the fundamental setting of stochastic convex opti-
mization (SCO) with bounded stochastic gradients, achieve optimal rates of convergence up to
logarithmic factors. While practical parameter-free algorithms exist [e.g. 68, 71, 47, 15], there is
little research into practical parameter-free step size selection methods for SGD. Recently, Carmon
and Hinder [11] have shown that performing a careful bisection over the SGD step size yields a
parameter-free optimization method that is optimal for SCO up to a double-logarithmic factor.
1The ICML version of our paper uses the more conventional and restrictive assumption of globally bounded
stochastic gradients.
1arXiv:2302.12022v3  [cs.LG]  16 Jul 2023

--- PAGE 2 ---
100101102103104
step index t108
105
102
DoG step size t
Logistic regression
r = 1e-01
r = 1e-03
r = 1e-05
r = 1e-07
r = 1e-09
best SGD LR
104
102
100102104
SGD peak learning rate0.800.850.90
109
107
105
103
101
DoG r
100101102103104
step index t105
103
DoG step size t
Fine-tuning neural network
r = 7e-02
r = 7e-03
r = 7e-04
r = 7e-05
r = 7e-06
best SGD LR
104
103
102
101
100101
SGD peak learning rate0.800.850.90
105
104
103
102
101
DoG r
Figure 1: Illustration of DoG for CIFAR-100 classification using logistic regression on last-layer
features of a pre-trained ViT-B/32 (left) or end-to-end fine-tuning of the model (right). The top
row shows the DoG step size sequence ηtfor different values of the initial movement rϵ, and the
bottom row shows that DoG attains test error on par with carefully tuned SGD (with cosine
annealing), even when varying rϵby several orders of magnitude. See details in Appendix E.6.
While theoretically novel, on a practical level the result leaves much to be desired, as it essentially
prescribes the standard recipe of running SGD multiple times with different learning rates.
Proposed algorithm. In this work, we use key insights from Carmon and Hinder [11] to go
a step further and develop a parameter-free step size schedule. For SGD iterations of the form
xt+1=xt−ηtgt, where xtdenotes the model parameters at the t’th iteration and gtdenotes the
stochastic gradient of the loss function, our proposed steps size sequence is (for all t≥1)
ηt=max i≤t∥xi−x0∥qP
i≤t∥gi∥2. (DoG )
In words, the step size at iteration tis the maximum distance to between the initial point and
observed iterates, divided by the sum of squared stochastic gradient norms, i.e., Distance over
Gradients ( DoG ). At the first step, we set η0to be rϵ/∥g0∥, i.e., we take a normalized gradient
step of size rϵ; we show that, as long as rϵis small, its precise setting has only mild effect.
Crucially, DoG has no multiplicative “learning rate” parameter: if one considers step sizes of
the form ηt=c·maxi≤t∥xi−x0∥√P
i≤t∥gi∥2then c= 1 is a universally good setting (see Section 2 for a heuristic
justification and Section 4.3 for empirical evidence for this claim).
Figure 1 highlights key aspects of DoG . The top row shows the DoG step size sequence for
different values of rϵin convex (left) and non-convex (right) stochastic optimization problems. The
DoG step size increases rapidly (note the logarithmic xscale) and stabilizes around values close
to the optimal SGD step size with little dependence on rϵ. The bottom row of the figure compares
the test errors of DoG and SGD with various step sizes, showing that (for all choices of rϵ)DoG
is on par with well-tuned SGD.
2

--- PAGE 3 ---
1.1 Summary of results
Theoretical guarantees. In Section 3 we analyze DoG for stochastic convex optimization with
bounded stochastic gradients and a (potentially unbounded) closed convex domain. To present our
results, let Bdenote a ball around the initial point x0with radius 3 d0, where d0is the distance
between x0and an optimum.
First, we show that if the iterates of DoG remain in B, then with high probability DoG
achieves a convergence rate that is optimal up to a factor of O(log(1 +d0
rϵ)). In practice, DoG
appears to indeed be stable as long as rϵis sufficiently small. However, DoG is not always stable:
on pathological functions its iterates can move far from the optimum.
To address this, we consider a theoretical, tamed variant of DoG , which we call T-DoG , whose
step sizes are smaller by a logarithmic factor. We prove that, with high probability, the T-DoG
iterates never leave B. Thus, we obtain a high probability parameter-free convergence guarantee
that is optimal up logarithmic factors.
To our knowledge, this is the first dynamic SGD step size schedule to attain such theoretical
guarantee, and only the third high probability parameter-free guarantee in the literature [following
11, 109]. Moreover, it is the first parameter-free result assuming only locally bounded stochastic
gradients (i.e., in the set B). This is significant since the usually-assumed global stochastic gradient
bound does not exist in many problems, including least squares.
Empirical study. Our experiments in Section 4 focus on fine-tuning neural networks, because
this is a practically important setting that still allows for thorough experiments at a reasonable
computational budget. We also perform a small-scale experiment with training a neural network
from scratch. Our experiments span 23 natural language understanding and image classification
tasks and 8 popular model architectures.
Our results indicate that, compared to DoG , SGD with a cosine step size schedule and tuned
base learning rarely attains a relative error improvement of more than 5% (e.g., the difference
between accuracy 95% and 95.25%). For convex problems (linear probes), the relative difference
in errors is below 1%. In our testbed, well-tuned Adam tends to outperform both SGD and DoG ,
but a layer-wise version of DoG (which we call L-DoG ) closes some of this performance gap.
We also test the sensitivity of DoG to the value of rϵ. We find that for most model/task
combinations, DoG performs consistently well across a wide range of rϵvalues as our theory
predicts. However, in certain cases, choosing rϵto be too low results in poor performance. We
provide some preliminary findings showing that this is due in part to batch normalization.
Put together, our theory and experiments suggest DoG has the potential to save significant
computation currently spent on learning rate tuning at little or no cost in performance—especially
if we reinvest some of the saved computation in training a larger model on more data.
2 Algorithm Derivation
Before providing rigorous theoretical guarantees for DoG , in this section we explain the origin of
the algorithm. Our starting point is the following result by Carmon and Hinder [11]. Suppose we
runTiterations of SGD with fixed step size η, i.e., the recursion xt+1=xt−ηgt, where xtis the
SGD iterate and gtis the stochastic gradient at step t. If, for some c∈(0,1), it happens to hold
3

--- PAGE 4 ---
that
η=c·max k≤T∥xk−x0∥qP
k≤T∥gk∥2, (1)
then the averaged iterates satisfies an excess loss bound that is at most a factor1
c(1−c2)larger than
the worst-case optimal bound achieved by perfectly tuned SGD.2
The condition (1) is an implicit equation: it allows us to check whether the choice of step size
ηis good only after running Tsteps of SGD using that η. Solving this implicit equation therefore
requires multiple calls to SGD. We derive the DoG step size sequence by making the equation
explicit: we choose ηtso that equation (1) holds at each step. For c= 1, this yields the step size
formula ( DoG ). Our reason for choosing c= 1 is that it is the threshold under which a solution
to the implicit equation yields an optimal rate of convergence. Therefore, in practice we expect 1
to be close to the highest stable value of c, and thus obtain the best performance; we verify this
empirically in Section 4.3.
3 Theoretical Analysis
3.1 Preliminaries
Problem setting. Our goal is to minimize a loss function f:X →Rwhere X ⊆Rm(including
the unconstrained setting X=Rmas an important special case). We perform our analysis under
the following standard convexity assumption.
Assumption 1 (Convexity) .The function fis convex, its domain Xis closed and convex, and its
minimum is attained at some x⋆∈ X, i.e., f⋆:= inf x∈Xf(x) =f(x⋆).
In Appendix A we discuss a possible relaxation of convexity under which our results continue
to hold.
To minimize fwe assume access to a stochastic gradient oracle G. When queried at a point
x∈ X the oracle returns a stochastic (sub)gradient estimator G(x) satisfying E[G(x)|x]∈∂f(x).
With slight abuse of notation, we write ∇f(x):=E[G(x)|x]. We make the following assumption,
where ∥·∥denotes the Euclidean norm.
Assumption 2 (Pointwise bounded stochastic gradients) .There exists some continuous function
ℓ:X →Rsuch that ∥G(x)∥ ≤ℓ(x)almost surely.
Assumption 2, which appeared in prior work [20, 61], is weaker than conventional assumptions in
parameter-free stochastic optimization, which either uniformly bound the stochastic gradients, i.e.,
∥G(x)∥ ≤Lfor all x∈ X[see, e.g., 71, 21], or uniformly bound the gradient variance [44]. However,
even least squares problems (with G(x) = (⟨a, x⟩ −b)afor random a∈Rmandb∈R) violate both
uniform bounds. In contrast, ℓis finite under the mild assumption that a, bare bounded random
variables; see Appendix C.5 for additional discussion.
Algorithm statement. We study (projected) SGD with dynamic learning rate schedule {ηt},
i.e.,
xt+1= ProjX(xt−ηtgt)
2This results holds in the non-stochastic case [11, Proposition 1], but a qualitatively similar results holds with
high probability in the stochastic case as well [11, Proposition 3].
4

--- PAGE 5 ---
where x0is a given initialization, gk:=G(xk), and ProjX(·) is the Euclidean projection onto X. To
succinctly state and analyze DoG , we define the following quantities:
rt:=∥xt−x0∥, ¯rt= max
k≤trk∨rϵandGt:=tX
k=0∥gt∥2,
where a∨b:= max {a, b}andrϵis a small user-specified initial movement size parameter. With
this notation, we define a family of DoG -like learning rate schedules.
Definition 1. A step size schedule is DoG -like if
ηt=¯rtp
G′
t
for a positive nondecreasing sequence G′
tthat depends only on x0, g0, . . . , g tand satisfies G′
t≥Gt.
DoG corresponds to simply setting G′
t=Gt; in Section 3.3 we consider a theoretical (or
tamed) DoG -like algorithm for which we guarantee bounded iterates by making G′
tlarger than Gt
by polylogarithmic factors. Throughout, we bound the error of the weighted average sequence
¯xt:=1Pt−1
k=0¯rkt−1X
k=0¯rkxk. (2)
Finally, to streamline the analysis we define:
dt:=∥xt−x⋆∥,¯dt:= max
k≤tdk,¯ℓt:= max
k≤tℓ(xk),
and
θt,δ:= log60 log(6 t)
δ
.
Logarithm conventions. Throughout the paper log is base eand log+(·):= 1 + log( ·).
3.2 Optimality gap bounds assuming bounded iterates
In this section, we bound the optimality gap attained by any DoG -like algorithm. Our bounds
depend on the quantities ¯ rTandGT, and are nearly optimal when ¯ rT=O(d0) (i.e., the DoG
iterates don’t move too far away from x0) and G′
Tis not much larger than GT. In the next section
we describe a specific DoG -like algorithm that is guaranteed to satisfy both requirements.
Convexity and Jensen’s inequality imply that ¯ xtsatisfies
f(¯xt)−f⋆≤1Pt−1
k=0¯rkt−1X
k=0¯rk⟨∇f(xk), xk−x⋆⟩. (3)
The sum in the RHS decomposes to two components:
t−1X
k=0¯rk⟨gk, xk−x⋆⟩
| {z }
weighted regret−t−1X
k=0¯rk⟨∆k, xk−x⋆⟩
| {z }
noise, (4)
where ∆ k:=gk− ∇f(xk). We give probability 1 bounds for the weighted regret (Lemma 1) and
high probability bounds for the noise term (Lemma 2). In each case, the key challenge is replacing
a-priori bounds on d0(or the domain size) with the empirically observed ¯ rT. We present and discuss
each lemma in turn.
5

--- PAGE 6 ---
Lemma 1 (Weighted regret bound) .IfXis a closed convex set then any DoG -like scheme (Def-
inition 1) satisfiesPt−1
k=0¯rk⟨gk, xk−x⋆⟩ ≤¯rt(2¯dt+ ¯rt)q
G′
t−1,∀t≥1.
Proof. Using xk+1= ProjX(xk−ηkgk) we obtain the standard inequality d2
k+1≤ ∥xk−ηkgk−x⋆∥2=
d2
k−2ηk⟨gk, xk−x⋆⟩+η2
k∥gk∥2. Rearranging this gives:
⟨gk, xk−x⋆⟩ ≤d2
k−d2
k+1
2ηk+ηk∥gk∥2
2. (5)
Therefore,Pt−1
k=0¯rk⟨gk, xk−x⋆⟩is at most
1
2t−1X
k=0¯rk
ηk(d2
k−d2
k+1)
| {z }
(A)+1
2t−1X
k=0¯rkηk∥gk∥2
|{z}
(B).
We bound the terms ( A) and ( B) in turn, beginning with the former:
(A) =t−1X
k=0q
G′
k(d2
k−d2
k+1) =d2
0q
G′
0−d2
tq
G′
t−1+t−1X
k=1d2
kq
G′
k−q
G′
k−1
(i)
≤¯d2
tq
G′
0−d2
tq
G′
t−1+¯d2
tt−1X
k=1q
G′
k−q
G′
k−1
=q
G′
t−1 ¯d2
t−d2
t(ii)
≤4¯rt¯dtq
G′
t−1.
Inequality ( i) uses dk≤¯dtand that G′
kis nondecreasing as per Definition 1. Inequality ( ii) holds
since, for s∈arg maxk≤tdk, we have ¯d2
t−d2
t=d2
s−d2
t= (ds−dt)(ds+dt)≤ ∥xs−xt∥(ds+dt)≤
(¯rs+ ¯rt)(ds+dt)≤4¯rt¯dt. Bounding the second term ( B), we have:
(B) =t−1X
k=0¯r2
k∥gk∥2
p
G′
k≤t−1X
k=0¯r2
k∥gk∥2
√Gk≤¯r2
tt−1X
k=0∥gk∥2
√Gk≤2¯r2
tp
Gt−1,
where the final inequality uses the standard Lemma 4 with ak=Gk=P
i≤k∥gi∥2.
While the proof of Lemma 1 is similar to the analysis of adaptive SGD where ηt=ρ√Gt[33],
there are a couple of key differences. First, the DoG step sizes can increase, which typically makes
adaptive gradient methods difficult to analyze [80]. We bypass this difficulty by considering regret
weighted by ¯ rk, which factors out the increasing portion of the step size. Second, the standard
adaptive SGD analysis yields a bound proportional to ¯d2
t(typically further bounded using the
domain diameter) rather than ¯ rt¯dtas in our bound. This is a crucial difference, since—as we soon
argue—¯ rt“cancels” when dividing through byP
k<t¯rk, while ¯dtdoes not. We obtain the improved
result by keeping around the term −d2
tq
G′
t−1in the bound for ( A) above; a trick similar to Carmon
and Hinder [11, Lemma 1].
Next, we handle the noise term in (4), recalling the notation ∆ t:=gt− ∇f(xt) and θt,δ:=
log60 log(6 t)
δ.
Lemma 2 (Noise bound) .Under Assumption 2, for all δ∈(0,1),T∈NandL >0we have
P 
∃t≤T:t−1X
k=0¯rk⟨∆k, xk−x⋆⟩≥8¯rt−1¯dt−1q
θt,δGt−1+θ2
t,δL2!
≤δ+P ¯ℓT> L
.
6

--- PAGE 7 ---
The proof of Lemma 2 appears in Appendix C.1 and is based on a new concentration bound,
Lemma 7, which allows us to bound the noise term despite having no deterministic bound on the
magnitude of the martingale difference sequence ¯ rk⟨∆k, xk−x⋆⟩. The proof of Lemma 7 involves
combining time-uniform Bernstein bounds [39] and a general bound on the cumulative sums of
sequence products (Lemma 5), which may be of independent interest.
Combining the above results, we obtain the following.
Proposition 1. For all δ∈(0,1)andL > 0, if Assumption 1, Assumption 2, and Definition 1
hold then with probability at least 1−δ−P ¯ℓT> L
, for all t≤Tthe optimality gap f(¯xt)−f⋆is
O
(d0+ ¯rt)q
G′
t−1+Gt−1θt,δ+L2θ2
t,δP
i<t¯ri/¯rt
.
Proof. Follows from Equations (3) and (4), Lemma 1, Lemma 2 and the fact that ¯dt≤d0+ ¯rt.
The following algebraic fact shows that there is always an iteration τ≤Twhere the denominatorP
i<t¯ri
¯rt≥Ω(T/log¯rT
rϵ); see Appendix B.1 for proof.
Lemma 3. Lets0, s1, . . . , s Tbe a positive nondecreasing sequence. Then
max
t≤TX
i<tsi
st≥1
eT
log+(sT/s0)−1
.
Combining Proposition 1 and Lemma 3 yields the following (see short proof in Appendix C.2).
Corollary 1. Under Assumptions 1 and 2, for any D≥d0, let LD:= max x∈X:∥x−x0∥≤Dℓ(x).
Then, for all δ∈(0,1)and for τ∈arg maxt≤TP
i<τ¯ri
¯rt, with probability at least 1−δ−P(¯rT> D),
theDoG iterates satisfy the optimality gap bound
f(¯xτ)−f⋆=O
Dq
Gτ−1θτ,δ+L2
Dθ2
τ,δ
Tlog+D
rϵ
=ODLD√
Tθτ,δlog+D
rϵ
.
Corollary 1 is immediately useful when Xis bounded but its exact diameter is unknown, for example
whenXis a polytope as is common in two-stage stochastic programming [64].
Simplifying the bound for typical DoG trajectories. Suppose that the DoG iterates satisfy
¯rT≤3d0, which implies that ¯ℓT≤L⋆:=L3d0and therefore (for DoG )G′
t=Gt≤L2
⋆T. Substi-
tuting into Corollary 1 yields an optimality gap bound of O
d0L⋆√
TθT,δlog¯rT
rϵ
, which is minimax
optimal up to a term double-logarithmic in Tand logarithmic in1
rϵ[2].
Furthermore, in realistic DoG trajectories, even the multiplicative term log¯rT
rϵis likely too
pessimistic. This is because ¯ rttypically increases rapidly for t0<1000 steps and then plateaus (see
Figure 12 in the appendix). Consequently, ¯ ri/¯rt≥1/10 for most of the optimization trajectory,
andP
i<t¯ri
¯rt≥t/10−t0. Substituting back into Proposition 2, we get that ¯ xTisO
d0L⋆√T−t0θT,δ
suboptimal.
DoG can run wild. While DoG is empirically stable, there exist (non-stochastic) examples
where ¯ rtgrows much larger than d0: in Appendix C.3 we describe a variant of Nemirovski’s func-
tion [63, 62] for which ¯ rt=rϵ√
tand therefore ¯ rt/d0diverges as tgrows. Next, we show that by
slightly decreasing the DoG step sizes we can guarantee that ¯ rT/d0≤3 with high probability.
7

--- PAGE 8 ---
3.3 Iterate stability bound
This section introduces a new DoG -like step size scheme whose iterates are guaranteed to remain
bounded with high probability. We call this scheme T-DoG , where the T stands for “theoretical”
or “tamed.” The step sizes are given by ηt= ¯rt/p
G′
t, where
G′
t= 84θ2
T,δlog2
+
1 +t¯ℓ2
t
¯ℓ2
0
(Gt−1+ 16¯ℓ2
t), (T-DoG )
using G−1:= 0, and recalling that ¯ℓt:= max i≤tℓ(x) for a function ℓsatisfying Assumption 2.
TheT-DoG formula depends weakly on the iteration budget Tand the failure probability δvia
θt,δ:= log
log(6t)
δ
; as we show below, in the non-stochastic setting we may simply replace θt,δwith
1. Moreover, the term 16 ¯ℓttypically grows slowly with t, becoming negligible compared to Gt−1.
Notably, the T-DoG step size requires no global upper bound on stochastic gradient norms.
We are ready to state T-DoG ’s key property: guaranteed iterate stability.
Proposition 2. Suppose that Assumptions 1 and 2 hold and rϵ≤3d0. For any δ∈(0,1), and
T∈N, the iterations of T-DoG satisfy P(¯rT>3d0)≤δ.
We defer the full proof to Appendix C.4 and proceed to highlight the key argument by proving
the result in the noiseless case.
Proof of Proposition 2 in the noiseless case. In the noiseless case we have gk=∇f(xk) and there-
fore⟨gk, xk−x⋆⟩ ≥f(xk)−f⋆≥0. Substituting into (5) and rearranging gives d2
k+1−d2
k≤η2
k∥gk∥2.
Assuming by induction that ¯ rt≤3d0and telescoping yields d2
k+1−d2
k≤η2
k∥gk∥2. Assuming by
induction that ¯ rt≤3d0and telescoping yields
d2
t+1−d2
0≤¯r2
ttX
k=0∥gk∥2
G′
k(i)
≤¯r2
t
84tX
k=0Gk−Gk−1
(Gk+¯ℓ2
k) log2
+Gk+¯ℓ2
k¯ℓ2
0(ii)
≤¯r2
t
84(iii)
≤9d2
0
84=⇒dt+1≤2d0,
where ( i) uses that ∥gk∥2=Gk−Gk−1(with the shorthand G−1:= 0) and
G′
k≥84(Gk−1+∥gk∥2+¯ℓ2
k) log2
+ P
i≤t¯ℓ2
t
¯ℓ2
0!
≥84(Gk+¯ℓ2
k) log2
+Gk+¯ℓ2
k
¯ℓ2
0
by Assumption 2 which implies ∥gk∥ ≤ ¯ℓkfor all k, (ii) uses Lemma 6 with ak=Gk+¯ℓ2
k, and
(iii) uses the inductive assumption ¯ rt≤3d0. Therefore, rt+1≤dt+1+d0≤3d0by the triangle
inequality, completing the induction step. Note that this proof ignored the θt,δterm in ( T-DoG ),
demonstrating it is not necessary in the noiseless case.
Given Assumption 2 we define
L⋆= max
x∈X:∥x−x0∥≤3∥x0−x⋆∥ℓ(x). (6)
With all the ingredients in hand, we state the main guarantee for T-DoG .
Theorem 1. Suppose that Assumptions 1 and 2 hold. For any δ∈(0,1
2),T∈N, consider T
iterations of T-DoG withrϵ≤3d0. Then for τ∈arg maxt≤TP
i<τ¯ri/¯rtwe have, with probability
at least 1−2δ, that
f(¯xτ)−f⋆=O 
cδ,rϵ,Td0p
Gτ−1+L2⋆
T!
=O
cδ,rϵ,Td0L⋆√
T
,
where cδ,rϵ,T= log+
Td0L⋆
f(x0)−f⋆
log+
d0
rϵ
loglog+(T)
δ
.
8

--- PAGE 9 ---
Proof. The theorem follows from Corollary 1, Proposition 2 and the definition of T-DoG , where
we note that Assumption 2 and convexity of fimply ¯ℓ0≥ ∥∇ f(x0)∥ ≥(f(x0)−f(x⋆))/d0, while
¯rT≤3d0gives ¯ℓT≤L⋆. Therefore, log+
1 +T¯ℓ2
T¯ℓ2
0
=O
log+
Td0L⋆
f(x0)−f⋆
.
Theorem 1 yields the optimal convergence bound [2] up to logarithmic factors. To the best of
our knowledge this is the first parameter-free stochastic optimization method that does not require
the stochastic gradients to be uniformly bounded across the domain Xand instead produces a
bound that depends on the ‘local’ gradient bound L⋆. Crucially, the T-DoG step size formula does
not require advance knowledge of L⋆.3
Extension to unweighted iterate averaging. While the weighted iterate average (2) is con-
venient to our analysis, bounds similar to Proposition 1, Corollary 1 and Theorem 1 hold also for
the standard unweighted iterate average ˆ xT=1
TPT−1
t=0xt. For ˆ xTit is also straightforward to show
a 1/Terror bound for DoG in the smooth noiseless case. See Appendix D for details.
4 Experiments
To test DoG in practical scenarios, we perform extensive experiments over a diverse set of tasks and
model architectures in both the vision and language domains. We construct a testbed that consists
of over 20 tasks and 7 model architecture, covering natural language understanding and computer
vision (Section 4.1). In this testbed we compare DoG to SGD and Adam (Section 4.2), showing
thatDoG performs on par with tuned SGD, but not as well as tuned Adam. Nevertheless, a per-
layer version of DoG (defined below) closes much of this gap with Adam without requiring tuning.
We also use our testbed to analyze the sensitivity of DoG to its fixed parameters (Section 4.3),
and demonstrate its effectiveness in convex logistic regression settings (Section 4.4). Finally, we
apply DoG andL-DoG to fine-tuning a CLIP model on ImageNet (Section 4.5) and training a
CIFAR10 model from scratch (Section 4.6), and provide preliminary comparison to previously-
proposed tuning free methods (Section 4.7). A PyTorch implementation of DoG is available at
https://github.com/formll/dog .
Layer-wise DoG. Neural models in general and transformer-based models in particular often
benefit from using a per-parameter or per-layer step sizes [48, 106]. With this in mind, we consider
a per-layer version of DoG , which we call L-DoG , where we apply the ( DoG ) formula separately
for every layer. Namely, if we consider xl
tto be the weights in layer4lat step t, then we set the
learning rate for that layer to be ηl
t=maxi≤t∥xl
i−xl
0∥qP
i≤t∥gl
i∥2+ϵ, where ϵ= 10−8is added to the denominator
for numerical stability. While we do not provide theoretical guarantees for L-DoG , we show below
that it performs well in practice.
4.1 Fine-tuning testbed
Our main experiments focus on fine-tuning pre-trained models, which allows us to experiment with
advanced models while also thoroughly tuning the learning rate for the baseline optimizers, using
an academic computational budget.
3There is prior work that develop methods with steps that do not require a global Lipschitz bound [20, 61], but
these methods do not guarantee that iterates remain in a ball of radius O(d0) around the initial point. Consequently,
the rates of convergence of these methods cannot be expressed in terms of a quantity like L⋆.
4More precisely, our implementation treats each element in the PyTorch .parameters() list as a separate layer.
9

--- PAGE 10 ---
Common hyperparameters. For each baseline algorithm, we use best-practice learning rate
schedule (cosine annealing for all experiments, with a warmup stage for language experiments) and
sweep over the peak learning rate for each model/task pair. We give each pair a fixed step budget
designed to suffice for convergence, performing evaluation throughout the training. In all cases,
we use polynomial decay averaging5as proposed by Shamir and Zhang [83], and select the best
checkpoint (either averaged or not) based on evaluation performance. We repeat relevant learning
setups with 5 different seeds, and report the mean performance across the seeds. For simplicity, we
do not use weight decay throughout. The complete set of hyper-parameters appears in Appendix E.
Natural language understanding (NLU). To test DoG ’s efficacy in modern NLU, we use
it to fine-tune transformer language models [89] on the well-studied GLUE benchmark [94] which
measures models’ performance on diverse text classification tasks (listed in Appendix E.3).
Additionally, we fine-tune models on SQuAD 1.1, a question answering dataset [79]. We fine-
tune a RoBERTa-base [54] checkpoint and T5-base [78].6For each task, we use the official evaluation
metrics defined in Wang et al. [94] and Rajpurkar et al. [79] as well as their original proposed splits,
and report the results over the evaluation set.
Computer vision. We also fine-tune 5 models architectures on 12 different computer vision tasks
from the VTAB benchmark [108] (see Appendix E.3); of the other 7 tasks in VTAB, 5 are trivial
(accuracy greater than 99%) and 2 have small validation splits leading to unreliable model selection.
We follow the training, validation and test splits defined in VTAB, and report performance on the
test split (using the validation split for model selection). We fine-tune 5 models: VGG11 [85],
ResNet50 [37], Densenet121 [40], ViT-B/32 [28], and ConvNeXt-T [55], where the ViT model is
pre-trained on ImageNet 21K and the others are trained on ImageNet 1K [26].
Normalized performance metric. Since the performance metrics in our testbed vary substan-
tially across tasks and models, they are challenging to compare in aggregate. To address this, we
consider the following notion of relative error difference (RED), that provides a normalized per-
formance difference measure. In particular, given a task and a model architecture, let err xbe the
error7of the model when trained with optimizer x(Adam or SGD with a certain learning rate, or
L-DoG ) and let err DoGbe the error when trained with DoG . Then
RED(err x,errDoG):=errDoG−errx
errDoG.
A positive RED value indicates that optimizer xis better than DoG , and a negative value indicates
the opposite. When the absolute value of RED is beneath a few percentage points, the compared
methods are nearly equivalent. For example, a 5% RED is equivalent to the difference between
accuracy 95% and 95.25%.
Setting rϵ.Our theoretical analysis suggests that the particular choice of rϵdoes not matter as
long as it is sufficiently small relative to the distance between the weight initialization x0and the
optimum. Consequently, for vision experiments we set rϵ=α·(1 +∥x0∥) for α= 10−4, assuming
that the distance to the optimum is more than 0.01% of the initialization norm. For language
5We apply the weight averaging with a fixed parameter ( γ= 8, following [52]); we did not try any other parameter
in our experiments.
6Throughout the paper we often use the shorthand names RoBERTa-b and T5-b, respectively.
7We consider the error to be 1 minus the respective performance metric, as detailed in Table 3.
10

--- PAGE 11 ---
104
102
100
Learning rate40%
20%
0%20%Relative Error Difference
RoBERTa-b
104
102
100
Learning rate
T5-b
104
102
100
Learning rate
ConvNeXt-T
104
102
100
Learning rate
Densenet121
104
102
100
Learning rate
ResNet50
104
102
100
Learning rate
VGG11
104
102
100
Learning rate
ViT-B/32
Adam (median) SGD (median) Adam (mean) SGD (mean) L-DoGFigure 2: Relative error difference statistics (median, mean, and error bars showing IQR) across
tasks for each model, as a function of peak learning rate. The red horizontal line and shaded region
indicate the median and IQR RED for L-DoG , respectively.
experiments, this assumption turned out to be wrong (causing DoG to diverge in some cases),
and we decreased αto 10−6forDoG and to 10−8forL-DoG , where the additive 10−6term was
too large in some layers. We believe that 10−6and 10−8should be good defaults for DoG and
L-DoG , respectively, though networks with batch normalization or different initialization schemes
could require a larger value; see Section 4.3 for additional discussion.
4.2 Comparison of fine-tuning performance
Figure 2 depicts the median, IQR (inter-quantile range) and mean RED of each model,8when
trained with SGD and Adam with different peak learning rates. The figure shows that, when com-
paring across models, there is no good default learning rate for neither SGD nor Adam. Moreover,
even for a single model only very specific SGD learning rate performs well, while most are con-
siderably inferior to using DoG . Even when tuned to the best fixed learning-rate value per model
(which we refer to as model tuned LR ), some tasks may still fail (compared to DoG ) as indicated
by the large IQR and the gap between the mean (triangles) and the median RED (circles) in models
such as ViT-B/32 ad Densenet121. While Adam also requires tuning, it is somewhat less sensitive
than SGD to the choice of peak learning rate. For a full breakdown of performance per task, see
Figure 7 and Tables 4 and 5 in Appendix F.1.
DoG performs similarly to well-tuned SGD in 79 out of the 80 model/task combinations in our
testbed. The one exception is tuning T5-b on CoLA, where DoG behaves erratically while SGD
succeeds only with a few learning rates. In contrast, both Adam and L-DoG achieved reasonable
performance consistently. DoG ’s poor performance on CoLA results in high RED measures for
this case, which draw the mean RED (triangles) above the median one in Figure 2 for T5-b. We
further analyze this exception in Appendix F.3 and show that choosing significantly smaller rϵfor
DoG alleviates the problem.
Figure 3 (top) compares DoG to SGD with model tuned LR as defined above, as well as instance
tuned LR , where for each model/task pair we select the best learning rate, at a computational
expense 5–7 times larger than running DoG . The performance of DoG remains close to that of
SGD with instance-tuned LR, with the largest median RED observed for ResNet50 and ViT-B/32.
Figure 3 (bottom) compares DoG to model-tuned and instance-tuned Adam, as well as to
L-DoG . In a few cases (namely ResNet50 and ConvNeXt-T) the gaps between DoG and Adam
are significant, and favor Adam. We hypothesize this is due to Adam’s per-parameter step-sizes
and momentum mechanisms, which DoG does not exploit. L-DoG , which has per-layer steps, has
8When aggregating results over tasks, we always report the RED statistics across tasks, where for each task we
average the RED values over seeds. See Appendix E.5 for details.
11

--- PAGE 12 ---
RoBERTa-b T5-b ConvNeXt-T Densenet121 ResNet50 VGG11 ViT-B/325.0%
0.0%5.0%10.0%15.0%20.0%Median Relative Error Difference
0.86 %
-0.05 %
0.30 %
2.73 %
2.21 %
0.34 %
0.45 %1.56 %
0.57 %
2.29 %
3.51 %
6.68 %
1.32 %
4.54 %SGD (model tuned LR) SGD (instance tuned LR)
RoBERTa-b T5-b ConvNeXt-T Densenet121 ResNet50 VGG11 ViT-B/325.0%
0.0%5.0%10.0%15.0%20.0%Median Relative Error Difference
4.87 %
4.00 %
7.09 %
3.58 %
12.33 %
3.43 %
0.28 %6.35 %
4.00 %
12.13 %
5.69 %
12.33 %
6.94 %
4.49 %1.57 %
3.50 %
4.01 %
-0.57 %
7.84 %
0.24 %
2.96 %Adam (model tuned LR) Adam (instance tuned LR) L-DoGFigure 3: RED median (bar chart) and IQR (error bars) of each model on the set of applicable
tasks. Top: Comparison with SGD when the LR is optimally tuned per model ( model tuned LR )
or per task ( instance tuned LR ).DoG is competitive with model-tuned SGD and often performs
nearly as well as instance-tuned SGD. Bottom : Comparison of DoG with adaptive optimizers.
L-DoG closes most of the gap to Adam.
positive median RED for all models, and narrows the gap between DoG and Adam, particularly
for ResNet50.
The instance-tuned baselines consume significantly more compute than DoG andL-DoG . In
Appendix F.2 we equalize the compute budget by reducing the number of steps for SGD and Adam.
This makes DoG outperform instance-tune SGD in most cases, and brings L-DoG substantially
closer to Adam.
4.3 Sensitivity of DoG’s fixed parameters
Initial movement size rϵ.Our theory suggests that all sufficiently small choices of rϵshould
perform similarly, but choosing rϵtoo large (compared to the initial distance to the optimum) can
hurt the performance of the algorithm. In Figure 4 (left) we plot the test performance as a function
ofrϵfor 8 model/task combinations. For 7 out of the 8, DoG is highly robust to the value of rϵ
as long as it small enough, as predicted. However, ResNet50 on CIFAR-100 (bottom left) is an
exception, where smaller values of rϵresult in an accuracy drop. We hypothesize this is due to
scale invariance introduced by batch normalization (BN), and provide supporting evidence for that
in Appendix F.4 (Figure 10), where we show that DoG is insensitive to rϵwhen we turn off BN.
In the appendix we also provide a complementary diagnostic for rϵsensitivity by plotting ηtvs.η0
for different values of t(see Figure 8).
Base learning rate. For this experiment only, we consider variants of DoG with different values
of base learning, i.e., step sizes of the form ηt=c·maxi≤t∥xi−x0∥√P
i≤t∥gi∥2with different values of c. We
12

--- PAGE 13 ---
0.8500.8750.9000.9250.9500.9751.000Performance metric
RoBERTa-b on SQuAD
T5-b on SQuAD
RoBERTa-b on SST-2
T5-b on SST-2
108
105
102
r/(1+x0)
0.700.750.800.850.900.951.00Performance metric
0.10.5 1.0 1.5 2.0 2.5 3.0
Base LR (c)
ResNet50 on Retinopathy
ResNet50 on CIFAR-100
ViT-B/32 on CIFAR-100
ViT-B/32 on RetinopathyFigure 4: Performance metrics of models trained with DoG as a function of η0(left) or the base
learning rate (right).
expect optimal performance when cis close to 1. More specifically, we expect the algorithm to be
unstable when c >1 and to be slower to converge (and less likely to generalize well) when c <1.
As can be observed in Figure 4 (right), values around c= 1 perform well for all models. For smaller
values, there is indeed inferior performance in some models (mainly ResNet50 and RoBERTa-b)—
indicating T-DoG would not work well in practice—while larger values result in divergence (in 6
out of 8 cases). Hence, the useful range for cis very narrow (about [0.5, 1.5]) and tuning it is not
likely to produce significant improvements. This is in contrast to Adam and SGD which generally
require searching over a space spanning a few orders of magnitude to properly train a model.
4.4 Convex optimization
We also evaluate DoG on convex optimization tasks, matching the assumptions of our theoretical
analysis. To do so, we perform multi-class logistic regression on features obtained from the computer
vision models in our testbed, i.e., linear probes. We find that model-tuned SGD performs on
par or worse than DoG , while instance-tuned SGD barely gains any advantage (Figure 5), with
RED values well under 1% (corresponding to the difference between accuracies 90% and 90.1%).
Moreover, even in this simple setting, SGD is sensitive to the choice of learning rate, which differ
significantly between models (Figure 6).
4.5 Fine-tuning on ImageNet
To complement our main fine-tuning testbed, we perform a more limited experiment involving
ImageNet as a downstream task, which is more expensive to tune due its larger scale. We fine-tune
a ViT-B/32 CLIP model [77] and compare DoG andL-DoG to training with SGD or AdamW [57].
We use a training prescription similar to Wortsman et al. [101]; see Appendix E.7 for additional
details. Table 1 shows the ImageNet top-1 validation accuracies of the final model checkpoints, with
and without the polynomial decay averaging used throughout our experiments. DoG performs
13

--- PAGE 14 ---
ConvNeXt-T Densenet121 ResNet50 VGG11 ViT-B/324.0%
2.0%
0.0%2.0%4.0%6.0%8.0%10.0%Median Relative Error Difference
-0.42 %
-0.49 %
-0.17 %
0.12 %
-0.07 %0.13 %
0.26 %
0.01 %
0.47 %
0.65 %-0.20 %
3.32 %
0.53 %
-0.22 %
1.14 %0.21 %
3.38 %
0.77 %
-0.17 %
1.92 %SGD (model tuned LR)
SGD (instance tuned LR)
Adam (model tuned LR)
Adam (instance tuned LR)Figure 5: RED median and IQR (as in Figure 3) in tn the convex optimization setting (Section 4.4).
104
102
100
Learning rate40%
20%
0%20%Relative Error Difference
ConvNeXt-T
104
102
100
Learning rate
Densenet121
104
102
100
Learning rate
ResNet50
104
102
100
Learning rate
VGG11
104
102
100
Learning rate
ViT-B/32
Adam (median) SGD (median) Adam (mean) SGD
Figure 6: Per-learning rate RED statistics (as in Figure 2) in the convex optimization setting
(Section 4.4).
similarly to SGD, but both algorithms perform significantly worse than AdamW, perhaps due to
an insufficient iteration budget. L-DoG performs well in this setting, improving on AdamW by a
little over 1 point.
4.6 Training from scratch
We conduct a preliminary experiment with training a model from scratch, specifically a Wide
ResNet 28-10 [107] on CIFAR-10 [50]; see Appendix E.8 for details. Table 2 shows the test ac-
curacy of the final checkpoint, with and without the polynomial averaging used throughout our
experiments. Here DoG performs on par with the setting’s canonical training prescription of SGD
with momentum 0.9 and learning rate 0.1 [19]. In this setting Adam produces poorer results,
andL-DoG is 0.5 point worse than tuned Adam with the best learning rate, perhaps due to not
reaching convergence.
4.7 Comparison to other tuning-free methods
We perform preliminary comparisons between DoG andL-DoG and other methods for removing
the learning rate parameter: the Stochastic Polyak Step [56], D-Adaptation [24] and Continuous
14

--- PAGE 15 ---
Algorithm LR Acc. w/o averaging Acc. with averaging
SGD1e-03 60.70% 60.49%
3e-03 73.62% 73.54%
1e-02 76.82% 76.80%
3e-02 77.51% 77.54%
1e-01 75.73% 75.71%
DoG - 74.78% 77.22%
AdamW1e-05 78.23% 78.25%
3e-05 79.04% 79.01%
1e-04 75.02% 74.97%
L-DoG - 78.20% 80.12%
Table 1: ImageNet top-1 validation accuracies after fine-tuning a CLIP ViT-B/32 model for 25K
training steps, with and without polynomial decay averaging (see Section 4.5).
Algorithm LR Acc. w/o averaging Acc. with averaging
SGD0.1 94.9% 94.9%
0.3 95.8% 95.6%
1 96.4% 84.4%
3 95.9% 21.7%
10 10.0% 10.0%
SGD w/
mom. 0.90.01 95.0% 95.1%
0.03 95.8% 95.7%
0.1† 96.3% 88.5%
0.3 95.8% 27.5%
1 42.0% 63.4%
DoG - 85.2% 96.4%
Adam3e-05 91.1% 91.1%
1e-04 94.0% 94.0%
3e-04 93.5% 93.8%
1e-03 91.4% 91.6%
L-DoG - 83.2% 93.5%
Table 2: CIFAR-10 test accuracies after training a Wide ResNet 28-10 model from scratch for 200
epochs, with and without polynomial decay averaging (see Section 4.6). †denotes the standard
training configuration [cf. 19, Table 2].
15

--- PAGE 16 ---
Coin Betting (COCOB) [71]. In all cases, we find that DoG andL-DoG provide better performance
on most tasks and on average (see Tables 6 and 7). We provide detailed results in Appendix G,
where we also discuss the practical prospects of the bisection procedure of Carmon and Hinder [11].
5 Related Work
Previous attempts to design theoretically principled and practical optimization algorithms that do
not require learning rate tuning approach the problem from a variety of perspectives, resulting in
a large variety of proposed algorithms. Rolinek and Martius [81], Vaswani et al. [90], Paquette
and Scheinberg [72] lift classical line search technique from non-stochastic optimization to the
stochastic setting, while Berrada et al. [9], Loizou et al. [56] do the same for the classical Polyak
step size [76, 36]. Asi and Duchi [4] develop a class of algorithms based on stochastic proximal
methods and demonstrate their improved robustness both theoretically and empirically. Schaul
et al. [82] use a stochastic quadratic approximation for designing learning rates that maximize the
expected one-step objective decrease. Chandra et al. [14] nest hypergradient descent to make a
method that is insensitive to initial hyper-parameter choices. However, none of these results are
parameter-free in the same sense as DoG : they either do not have converges guarantees, or have
suboptimality bounds that blow up polynomially when the method’s parameters do not match a
problem-dependent value. In contrast, parameter-free methods have convergence rates that depend
at most logarithmically on algorithmic parameters.
While the parameter-free optimization literature has focused mainly on theoretical schemes,
a number of works also include empirical studies [68, 71, 47, 15]. In particular, Orabona and
Tommasi [71] build on coin-betting schemes to design an algorithm for training neural networks that
has AdaGrad-style convergence guarantees for quasi-convex functions, showing promising results
on neural network training problems. In recent work Chen et al. [15] obtain improved empirical
results with an algorithm that leverages coin betting and truncated linear models. However, this
method lacks theoretical guarantees.
In recent independent work Defazio and Mishchenko [24] propose a parameter-free dynamic step
size schedule of dual averaging. While our work has the same motivation and shares a number of
technical similarities (including the use of weighted regret bounds and an independently obtained
Lemma 3), the proposed algorithms are quite different, and dual averaging is rarely used in training
neural networks. (See additional discussion in Appendix G.3). Moreover, Defazio and Mishchenko
[24] only prove parameter-free rates of convergence in the non-stochastic setting, while we estab-
lish high probability guarantees in the stochastic setting. Concurrently with our work, Defazio
and Mishchenko [25] heuristically extended their dual averaging scheme to SGD- and Adam-like
algorithms, reporting promising experimental results.
Finally, a number of neural network optimization methods—LARS [104], LAMB [105], Adafac-
tor [84], and Fromage [8]—use the norm of neural network weights to scale the step size. DoG and
L-DoG are similar in also using a norm to scale their step size, but they differ from prior work by
considering the distance from initialization rather than the norm of the weights. We believe that
this difference is crucial in making DoG parameter-free, while the above-mentioned method have
a learning-rate parameter to tune (though Bernstein et al. [8] report that a single default value
works well across different tasks).
16

--- PAGE 17 ---
6 Limitations and Outlook
Our theoretical and empirical results place DoG as a promising step toward a new generation
of principled and efficient tuning-free optimization algorithms. However, much additional work is
necessary for these algorithms to become ubiquitous. First, it is important to understand how to
correctly combine DoG with proven technique such as momentum, per-parameter learning rates,
and learning rate annealing—this is a challenge both from a theoretical and a practical perspective.
Second, it is important to gain a better understanding of situations where DoG is more sensitive
to the choice of rϵthan theory would have us expect. Our preliminary investigations suggest a
connection to batch normalization, and following that lead could lead to even more robust training
methods. Finally, while our experiments aim to cover a broad range of tasks and architectures,
future work needs to explore DoG in additional settings, particularly those involving training from
scratch.
Acknowledgments
We thank Francesco Orabona, Mitchell Wortsman, Simon Kornblith and our anonymous reviewers
for their insightful comments. This work was supported by the NSF-BSF program, under NSF grant
#2239527 and BSF grant #2022663. MI acknowledges support from the Israeli council of higher
education. OH acknowledges support from Pitt Momentum Funds, and AFOSR grant #FA9550-
23-1-0242. YC acknowledges support from the Israeli Science Foundation (ISF) grant no. 2486/21,
the Alon Fellowship, the Yandex Initiative for Machine Learning, and the Len Blavatnik and the
Blavatnik Family Foundation.
References
[1] Mart´ ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,
Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian
Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefow-
icz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Man´ e, Rajat Monga,
Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner,
Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda
Vi´ egas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xi-
aoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015.
URL https://www.tensorflow.org/ . Software available from tensorflow.org.
[2] Alekh Agarwal, Peter L Bartlett, Pradeep Ravikumar, and Martin J Wainwright.
Information-theoretic lower bounds on the oracle complexity of stochastic convex optimiza-
tion. IEEE Transactions on Information Theory , 58(5):3235–3249, 2012.
[3] Kenneth J Arrow and Alain C Enthoven. Quasi-concave programming. Econometrica: Jour-
nal of the Econometric Society , pages 779–800, 1961.
[4] Hilal Asi and John C Duchi. The importance of better models in stochastic optimization.
Proceedings of the National Academy of Sciences , 116(46):22924–22930, 2019.
[5] Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini,
and Idan Szpektor. The second PASCAL recognising textual entailment challenge. In Pro-
17

--- PAGE 18 ---
ceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment ,
2006.
[6] Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich
K¨ uttler, Andrew Lefrancq, Simon Green, V´ ıctor Vald´ es, Amir Sadik, et al. Deepmind lab.
arXiv:1612.03801 , 2016.
[7] Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini.
The fifth PASCAL recognizing textual entailment challenge. In Text Analysis Conference
(TAC) , 2009.
[8] Jeremy R. Bernstein, Arash Vahdat, Yisong Yue, and Ming-Yu Liu. On the distance between
two neural networks and the stability of learning. arXiv:2002.03432 , 2020.
[9] Leonard Berrada, Andrew Zisserman, and M Pawan Kumar. Training neural networks for
and by interpolation. In International Conference on Machine Learning (ICML) , 2020.
[10] Aditya Bhaskara, Ashok Cutkosky, Ravi Kumar, and Manish Purohit. Online learning with
imperfect hints. In International Conference on Machine Learning (ICML) , 2020.
[11] Yair Carmon and Oliver Hinder. Making SGD parameter-free. In Conference on Learning
Theory (COLT) , 2022.
[12] Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang.
Unlabeled data improves adversarial robustness. Advances in Neural Information Processing
Systems (NeurIPS) , 2019.
[13] Daniel Matthew Cer, Mona T. Diab, Eneko Agirre, I˜ nigo Lopez-Gazpio, and Lucia Spe-
cia. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused
evaluation. In International Workshop on Semantic Evaluation , 2017.
[14] Kartik Chandra, Audrey Xie, Jonathan Ragan-Kelley, and Erik Meijer. Gradient descent:
The ultimate optimizer. Advances in Neural Information Processing Systems (NeurIPS) ,
2022.
[15] Keyi Chen, John Langford, and Francesco Orabona. Better parameter-free stochastic opti-
mization with ODE updates for coin-betting. In AAAI Conference on Artificial Intelligence ,
2022.
[16] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification:
Benchmark and state of the art. Proceedings of the IEEE , 105(10):1865–1883, 2017.
[17] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi.
Describing textures in the wild. In Conference on Computer Vision and Pattern Recognition
(CVPR) , 2014.
[18] Comet.ML. Comet.ML home page, 2021. URL https://www.comet.ml/ .
[19] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. AutoAug-
ment: Learning augmentation strategies from data. In Conference on Computer Vision and
Pattern Recognition (CVPR) , 2019.
[20] Ashok Cutkosky. Artificial constraints and hints for unbounded online learning. In Conference
on Learning Theory (COLT) , 2019.
18

--- PAGE 19 ---
[21] Ashok Cutkosky and Francesco Orabona. Black-box reductions for parameter-free online
learning in Banach spaces. In Conference on Learning Theory (COLT) , 2018.
[22] Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual en-
tailment challenge. In Machine learning challenges. Evaluating predictive uncertainty, visual
object classification, and recognising tectual entailment . Springer, 2006.
[23] Damek Davis and Dmitriy Drusvyatskiy. Stochastic model-based minimization of weakly
convex functions. SIAM Journal on Optimization , 29(1):207–239, 2019.
[24] Aaron Defazio and Konstantin Mishchenko. Parameter free dual averaging: Optimizing
lipschitz functions in a single pass. In OPT 2022: NeurIPS Workshop on Optimization for
Machine Learning , 2022.
[25] Aaron Defazio and Konstantin Mishchenko. Learning-rate-free learning by D-adaptation. In
International Conference on Machine Learning (ICML) , 2023.
[26] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-
scale hierarchical image database. In Conference on Computer Vision and Pattern Recognition
(CVPR) , 2009.
[27] William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential
paraphrases. In International Workshop on Paraphrasing (IWP2005) , 2005.
[28] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image
recognition at scale. In International Conference on Learning Representations (ICLR) , 2021.
[29] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learn-
ing and stochastic optimization. Journal of Machine Learning Research , 12(7), 2011.
[30] Matthew Faw, Isidoros Tziotis, Constantine Caramanis, Aryan Mokhtari, Sanjay Shakkottai,
and Rachel Ward. The power of adaptivity in SGD: Self-tuning step sizes with unbounded
gradients and affine variance. In Conference on Learning Theory (COLT) , 2022.
[31] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few
training examples: An incremental Bayesian approach tested on 101 object categories. In
CVPR Workshop , 2004.
[32] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL
recognizing textual entailment challenge. In ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing , 2007.
[33] Vineet Gupta, Tomer Koren, and Yoram Singer. A unified approach to adaptive regularization
in online and stochastic optimization. arXiv:1706.06569 , 2017.
[34] Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor
optimization. In International Conference on Machine Learning (ICML) , 2018.
[35] Charles R. Harris, K. Jarrod Millman, St´ efan J. van der Walt, Ralf Gommers, Pauli Virtanen,
David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert
Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane,
19

--- PAGE 20 ---
Jaime Fern´ andez del R´ ıo, Mark Wiebe, Pearu Peterson, Pierre G´ erard-Marchant, Kevin
Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E.
Oliphant. Array programming with NumPy. Nature , 585(7825):357–362, 2020.
[36] Elad Hazan and Sham Kakade. Revisiting the Polyak step size. arXiv:1905.00313 , 2019.
[37] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. Conference on Computer Vision and Pattern Recognition (CVPR) , 2016.
[38] Oliver Hinder, Aaron Sidford, and Nimit Sohoni. Near-optimal methods for minimizing star-
convex functions and beyond. In Conference on Learning Theory (COLT) , 2020.
[39] Steven R Howard, Aaditya Ramdas, Jon McAuliffe, and Jasjeet Sekhon. Time-uniform,
nonparametric, nonasymptotic confidence sequences. The Annals of Statistics , 49(2):1055–
1080, 2021.
[40] Gao Huang, Zhuang Liu, and Kilian Q. Weinberger. Densely connected convolutional net-
works. In Conference on Computer Vision and Pattern Recognition (CVPR) , 2017.
[41] Shankar Iyer, Nikhil Dandekar, and Kornel Csernai. First quora dataset release: Question
pairs, 2017. URL https://data.quora.com/First-Quora-Dataset-Release-Question-P
airs.
[42] Andrew Jacobsen and Ashok Cutkosky. Parameter-free mirror descent. In Conference on
Learning Theory (COLT) , 2022.
[43] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick,
and Ross Girshick. CLEVR: A diagnostic dataset for compositional language and elementary
visual reasoning. In Conference on Computer Vision and Pattern Recognition (CVPR) , 2017.
[44] Kwang-Sung Jun and Francesco Orabona. Parameter-free online convex optimization with
sub-exponential noise. In Conference on Learning Theory (COLT) , 2019.
[45] Kaggle and EyePacs. Kaggle diabetic retinopathy detection, 2015. URL https://www.kagg
le.com/c/diabetic-retinopathy-detection/data .
[46] Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the polyak- lojasiewicz condition. In European Conference on Machine
Learning and Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD) ,
2016.
[47] Michal Kempka, Wojciech Kotlowski, and Manfred K Warmuth. Adaptive scale-invariant
online algorithms for learning linear models. In International Conference on Machine Learning
(ICML) , 2019.
[48] Diederik P Kingma and Jimmy Ba. ADAM: A method for stochastic optimization. In
International Conference on Learning Representations (ICLR) , 2015.
[49] Bobby Kleinberg, Yuanzhi Li, and Yang Yuan. An alternative view: When does SGD escape
local minima? In International Conference on Machine Learning (ICML) , 2018.
[50] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report,
University of Toronto, 2009.
20

--- PAGE 21 ---
[51] Hector J Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge.
InInternational Conference on Principles of Knowledge Representation and Reasoning , 2011.
[52] Daniel Levy, Yair Carmon, John C Duchi, and Aaron Sidford. Large-scale methods for
distributionally robust optimization. Advances in Neural Information Processing Systems
(NeurIPS) , 2020.
[53] Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von
Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe
Davison, Mario ˇSaˇ sko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao,
Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain
Gugger, Cl´ ement Delangue, Th´ eo Matussi` ere, Lysandre Debut, Stas Bekman, Pierric Cistac,
Thibault Goehringer, Victor Mustar, Fran¸ cois Lagunas, Alexander Rush, and Thomas Wolf.
Datasets: A community library for natural language processing. In Conference on Empirical
Methods in Natural Language Processing (EMNLP): System Demonstrations , 2021.
[54] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,
Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized
BERT pretraining approach. arXiv:1907.11692 , 2019.
[55] Zhuang Liu, Hanzi Mao, Chaozheng Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining
Xie. A ConvNet for the 2020s. In Conference on Computer Vision and Pattern Recognition
(CVPR) , 2022.
[56] Nicolas Loizou, Sharan Vaswani, Issam Hadj Laradji, and Simon Lacoste-Julien. Stochastic
Polyak step-size for SGD: An adaptive learning rate for fast convergence. In International
Conference on Artificial Intelligence and Statistics (AISTATS) , 2021.
[57] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International
Conference on Learning Representations (ICLR) , 2019.
[58] Haipeng Luo and Robert E Schapire. Achieving all with no parameters: AdaNormalHedge.
InConference on Learning Theory (COLT) , 2015.
[59] Olvi L Mangasarian. Pseudo-convex functions. In Stochastic optimization models in finance .
Elsevier, 1975.
[60] Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dSprites: Disentan-
glement testing Sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017.
[61] Zakaria Mhammedi and Wouter M Koolen. Lipschitz and comparator-norm adaptivity in
online learning. In Conference on Learning Theory (COLT) , 2020.
[62] Arkadi Nemirovski. On parallel complexity of nonsmooth convex optimization. Journal of
Complexity , 10(4):451–463, 1994.
[63] Arkadi Nemirovski and David Yudin. Problem complexity and method efficiency in optimiza-
tion. Wiley-Interscience, 1983.
[64] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochas-
tic approximation approach to stochastic programming. SIAM Journal on Optimization , 19
(4):1574–1609, 2009.
21

--- PAGE 22 ---
[65] Yurii Nesterov and Boris T Polyak. Cubic regularization of Newton method and its global
performance. Mathematical Programming , 108(1):177–205, 2006.
[66] Yuval Netzer, Tao Wang, Adam Coates, A. Bissacco, Bo Wu, and A. Ng. Reading digits in
natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and
Unsupervised Feature Learning 2011 , 2011.
[67] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large
number of classes. In Indian Conference on Computer Vision, Graphics and Image Processing
(ICVGIP) , 2008.
[68] Francesco Orabona. Simultaneous model selection and optimization through parameter-free
stochastic learning. Advances in Neural Information Processing Systems (NeurIPS) , 2014.
[69] Francesco Orabona and D´ avid P´ al. Coin betting and parameter-free online learning. In
Advances in Neural Information Processing Systems (NeurIPS) , 2016.
[70] Francesco Orabona and D´ avid P´ al. Parameter-free stochastic optimization of variationally
coherent functions. arXiv:2102.00236 , 2021.
[71] Francesco Orabona and Tatiana Tommasi. Training deep networks without learning rates
through coin betting. In Advances in Neural Information Processing Systems (NeurIPS) ,
2017.
[72] Courtney Paquette and Katya Scheinberg. A stochastic line search method with expected
complexity analysis. SIAM Journal on Optimization , 30(1):349–376, 2020.
[73] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In
Conference on Computer Vision and Pattern Recognition (CVPR) , 2012.
[74] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style,
high-performance deep learning library. In Advances in Neural Information Processing Sys-
tems (NeurIPS) , 2019.
[75] Fabian Pedregosa, Ga¨ el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion,
Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al.
Scikit-learn: Machine learning in Python. Journal of Machine Learning Research , 12:2825–
2830, 2011.
[76] Boris T. Polyak. Introduction to Optimization . Optimization Software, Inc, 1987.
[77] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable
visual models from natural language supervision. In International Conference on Machine
Learning (ICML) , 2021.
[78] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified
text-to-text transformer. Journal of Machine Learning Research , 21:1–67, 2020.
22

--- PAGE 23 ---
[79] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+
questions for machine comprehension of text. In Conference on Empirical Methods in Natural
Language Processing (EMNLP) , 2016.
[80] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of Adam and beyond.
InInternational Conference on Learning Representations (ICLR) , 2018.
[81] Michal Rolinek and Georg Martius. L4: Practical loss-based stepsize adaptation for deep
learning. In Advances in Neural Information Processing Systems (NeurIPS) , 2018.
[82] Tom Schaul, Sixin Zhang, and Yann LeCun. No more pesky learning rates. In International
Conference on Machine Learning (ICML) , 2013.
[83] Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization:
Convergence results and optimal averaging schemes. In International Conference on Machine
Learning (ICML) , 2013.
[84] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory
cost. In International Conference on Machine Learning (ICML) , 2018.
[85] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale
image recognition. arXiv:1409.1556 , 2014.
[86] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew
Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a
sentiment treebank. In Conference on Empirical Methods in Natural Language Processing
(EMNLP) , 2013.
[87] Matthew Streeter and H Brendan McMahan. No-regret algorithms for unconstrained online
convex optimization. In Advances in Neural Information Processing Systems (NeurIPS) ,
2012.
[88] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov,
Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolu-
tions. In Conference on Computer Vision and Pattern Recognition (CVPR) , 2015.
[89] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems (NeurIPS) , 2017.
[90] Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and Simon
Lacoste-Julien. Painless stochastic gradient: Interpolation, line-search, and convergence rates.
InAdvances in Neural Information Processing Systems (NeurIPS) , 2019.
[91] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David
Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St´ efan J.
van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew
R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, ˙Ilhan Polat, Yu Feng, Eric W.
Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen,
E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antˆ onio H. Ribeiro, Fabian Pedregosa,
Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for
Scientific Computing in Python. Nature Methods , 17:261–272, 2020.
23

--- PAGE 24 ---
[92] Vladimir Vovk. On-line regression competitive with reproducing kernel hilbert spaces. In
Theory and Applications of Models of Computation (TAMC) , 2006.
[93] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. SuperGLUE: A stickier benchmark for general-
purpose language understanding. In Advances in Neural Information Processing Systems
(NeurIPS) , 2019.
[94] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding.
InInternational Conference on Learning Representations (ICLR) , 2019.
[95] Rachel Ward, Xiaoxia Wu, and Leon Bottou. AdaGrad stepsizes: Sharp convergence over
nonconvex landscapes. In International Conference on Machine Learning (ICML) , 2019.
[96] Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability
judgments. Transactions of the Association for Computational Linguistics , 7:625–641, 2019.
[97] Wes McKinney. Data Structures for Statistical Computing in Python. In Proceedings of the
9th Python in Science Conference , 2010.
[98] Ross Wightman. PyTorch image models. https://github.com/rwightman/pytorch-ima
ge-models , 2019.
[99] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for
sentence understanding through inference. In Conference of the North American Chapter of
the Association for Computational Linguistics (NAACL) , 2018.
[100] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony
Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,
Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain
Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-
art natural language processing. In Conference on Empirical Methods in Natural Language
Processing (EMNLP): System Demonstrations , 2020.
[101] Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael
Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Korn-
blith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy
without increasing inference time. In International Conference on Machine Learning (ICML) ,
2022.
[102] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun
database: Large-scale scene recognition from abbey to zoo. In Conference on Computer
Vision and Pattern Recognition (CVPR) , 2010.
[103] Jianxiong Xiao, Krista A Ehinger, James Hays, Antonio Torralba, and Aude Oliva. Sun
database: Exploring a large collection of scene categories. International Journal of Computer
Vision , 119(1):3–22, 2016.
[104] Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks.
arXiv:1708.03888 , 2017.
24

--- PAGE 25 ---
[105] Yang You, Igor Gitman, and Boris Ginsburg. Scaling SGD batch size to 32k for ImageNet
training. arXiv:1708.03888 , 2017.
[106] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli,
Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization
for deep learning: Training BERT in 76 minutes. In International Conference on Learning
Representations (ICLR) , 2020.
[107] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision
Conference (BMVC) , 2016.
[108] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme,
Mario Lucic, Josip Djolonga, Andr´ e Susano Pinto, Maxim Neumann, Alexey Dosovitskiy,
Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Syl-
vain Gelly, and Neil Houlsby. A large-scale study of representation learning with the visual
task adaptation benchmark. arXiv:1910.04867 , 2019.
[109] Jiujia Zhang and Ashok Cutkosky. Parameter-free regret in high probability with heavy tails.
InAdvances in Neural Information Processing Systems (NeurIPS) , 2022.
[110] Zhiyu Zhang, Ashok Cutkosky, and Ioannis Paschalidis. PDE-based optimal strategy for
unconstrained online learning. In International Conference on Machine Learning (ICML) ,
2022.
[111] Yi Zhou, Junjie Yang, Huishuai Zhang, Yingbin Liang, and Vahid Tarokh. SGD converges
to global minimum in deep learning via star-convex path. In International Conference on
Learning Representations (ICLR) , 2019.
25

--- PAGE 26 ---
A Relaxing the Convexity Assumption
This section describes relaxations of convexity under which our main theoretical results still hold.
In particular, our results naturally extend to star-convex functions [65] which satisfy
f(x)−f⋆≤ ⟨∇ f(x), x−x⋆⟩for all x∈ X.
Our results also extend (with changed constants) to quasarconvex functions [38], which require that
f(x)−f⋆≤c⟨∇f(x), x−x⋆⟩holds for some c <∞and all x∈ X. A further relaxation of star
convexity requires it to hold only along the optimization trajectory:
Assumption 3 (Zhou et al. [111, Definition 2]) .There exists x⋆∈arg minxf(x)and constant
c <∞such that the iterates of SGD satisfy
f(xk)−f⋆≤c⟨∇f(xk), xk−x⋆⟩for all k
almost surely.
Zhou et al. [111] introduce this notion of a “star-convex path” and provide some empirical
evidence that it may hold when training deep neural networks with SGD (see also Kleinberg et al.
[49] for a related assumption). Zhou et al. [111] also prove that the assumption suffices to prove
that SGD converges to the global minimizer; it suffices for DoG for similar reasons.
When substituting Assumption 1 with Assumption 3, our analysis goes through unchanged,
except we can no longer use Jensen’s inequality to argue directly about the suboptimality of the
point ¯ xτ. Instead, Theorem 1 with Assumption 3 says that, with probability at least 1 −δ,
τ−1X
k=0ωk(f(xk)−f⋆)≤O
cδ,rϵ,T·d0√
Gτ+L2
T
,
with ωk:=¯rkPt−1
i=0¯ri, the final iterate tau τ, and the coefficient cδ,rϵ,Tas defined in Theorem 1. (Note
that Assumption 3 impliesPt−1
k=0ωk(f(xk)−f⋆)≤Pt−1
k=0ωk⟨∇f(xk), xk−x⋆⟩which replaces (3)).
We can turn the above bound into a constant-probability guarantee for a specific T-DoG iterate
xKby sampling K∼ωand using Markov’s inequality:
P 
f(xK)−f⋆≤eτ−1X
k=0ωk(f(xk)−f⋆)!
≤e−1.
To obtain a high probability guarantee, we can make l=⌈log1
δ⌉independent draws from ω,
denoted K1, . . . , K land use the fact that
P 
min
i≤lf(xKi)−f⋆≤eτ−1X
k=0ωk(f(xk)−f⋆)!
≤δ.
Finding the ithat minimizes f(xKi) requires a logarithmic number of evaluations of the exact
objective. When this is not feasible, we can instead consider a statistical learning setup where
we have sample access to stochastic functions F(x) such that EF(x) =f(x) for all xand, almost
surely, FisL⋆Lipschitz in a ball of radius 3 d0around x0. (The stochastic subgradient oracle G(x)
is then implemented by sampling Fand returning its subgradient at x). We can then sample Tnew
26

--- PAGE 27 ---
stochastic functions F1, . . . , F Tand select K⋆∈arg min k∈{K1,...,K l}PT
i=1Fi(xk). Straightforward
application of Hoeffding’s inequality shows that (when ¯ rT≤3d0)
f(xK⋆)−f⋆≤min
i≤lf(xKi)−f⋆+O 
L⋆d0√
Tr
log1
δ!
with probability at least 1 −δ.
We remark that the literature contains a plethora of other convexity relaxations such as qua-
siconvexity [3], pseudoconvexity [59], Polyak- Lojasiewicz conditions [46] and weak convexity [23].
Exploring the convergence of DoG under these additional convexity relaxations is left to future
work.
B Useful Algebraic Facts
B.1 Proof of Lemma 3
Proof. Define K:=⌈log(sT/s0)⌉, and n:=⌊T
K⌋. Then, we have
logsT
s0
≥K−1X
k=0logsn(k+1)
snk
≥Kmin
k<Klogsn(k+1)
snk
.
Rearranging and using the definition of Kgives
min
k<Klogsn(k+1)
snk
≤log
sT
s0
K≤1 =⇒min
k<Ksn(k+1)
snk≤e.
where the implication follows from monotonicity of the exponential function. Therefore,
max
t≤TX
i<tsi
st≥max
t∈[n,T]nst−n
st= max
k≤Knsn(k−1)
snk≥ne−1=e−1T
⌈log(sT/s0)⌉
≥e−1 T
log(sT/s0) + 1−e−1,
where the first inequality uses that sis positive nondecreasing sequence and the second inequality
uses min k<Ksn(k+1)
snk≤eas shown above.
B.2 Lemma 4
Lemma 4. Leta0, . . . , a tbe a nondecreasing sequence of nonnegative numbers. Then
tX
k=1ak−ak−1√ak≤2(√at−√a0).
Proof. We have
tX
k=1ak−ak−1√ak=tX
k=1(√ak−√ak−1)(√ak+√ak−1)√ak≤2tX
k=1 √ak−√ak−1
= 2(√at−√a0).
27

--- PAGE 28 ---
B.3 Lemma 5
Lemma 5. Leta1, . . . , a Tandb1, . . . , b Tbe sequences in Rsuch that a1, . . . , a Tis nonnegative and
nondecreasing. Then, for all t≤T,
tX
i=1aibi≤2atmax
i≤tiX
j=1bj.
Proof. Leta′
i=ai−ai−1andBi=P
j≤ibj. Then (by discrete integration by parts)
tX
i=1aibi=tX
i=1ai(Bi−Bi−1) =t−1X
i=1(ai−ai+1)Bi+atBt=atBt−t−1X
i=1a′
i+1Bi.
ThereforetX
i=1aibi(i)
≤ |atBt|+ t−1X
i=1a′
i+1!
max
i<t|Bi| ≤ 
|at|+t−1X
i=1|ai+1−ai|!
max
i≤t|Bi|(ii)
≤2atmax
i≤t|Bi|,
where ( i) uses the triangle and H¨ older’s inequalities, and ( ii) uses that atis nonnegative and
nondecreasing and thereforePt−1
i=1|ai+1−ai|=at−a1≤at.
B.4 Lemma 6
Recall that log+(z):= 1 + log( t).
Lemma 6. Leta−1, a0, a1, . . . , a tbe a nondecreasing sequence of nonnegative numbers, then
tX
k=0ak−ak−1
aklog2
+(ak/a−1)≤1.
Proof. We have
tX
k=0ak−ak−1
aklog2
+(ak/a−1)≤tX
k=0Zak/a−1
ak−1/a0dα
αlog2
+(α)=Zat/a−1
1dα
αlog2
+(α)
≤Z∞
1dα
αlog2
+(α)=1
1 + log( α)∞
1= 1.
C Proofs for Section 3
C.1 Proof of Lemma 2
We begin by citing the following corollary of a general bound due to Howard et al. [39]. (Recall
thatθt,δ:= log60 log(6 t)
δ).
Corollary 2 (Carmon and Hinder [11, Corollary 1]) .Letc >0andXtbe a martingale difference
sequence adapted to Ftsuch that |Xt| ≤cwith probability 1 for all t. Then, for all δ∈(0,1), and
ˆXt∈ Ft−1such that |ˆXt| ≤cwith probability 1,
P
∃t≤T:tX
s=1Xs≥4vuutθt,δtX
s=1
Xs−ˆXs2
+c2θ2
t,δ
≤δ.
28

--- PAGE 29 ---
Building on Corollary 2 we prove the following result, which allows for the sequence Xtto be
almost-surely bounded by a random (rather than deterministic) quantity.
Corollary 3. LetCt∈ F t−1and let Xtbe a martingale difference sequence adapted to Ftsuch
that|Xt| ≤Ctwith probability 1 for all t. Then, for all δ∈(0,1),c >0, and ˆXt∈ Ft−1such that
|ˆXt| ≤Ctwith probability 1,
P
∃t≤T:tX
s=1Xs≥4vuutθt,δtX
s=1
Xs−ˆXs2
+c2θ2
t,δ
≤δ+P(∃t≤T:Ct> c).
Proof. Define the random variables
Wt:=Xt
max{c, Ct}and ˆWt:=ˆXt
max{c, Ct}
and note that they satisfy the requirements of Corollary 2: Wtis a martingale difference sequence
adapted to Ftwhile ˆWt∈ Ft−1and they both have absolute value bounded by 1 almost surely.
Next, define the events
ET:=

∃t < T :tX
s=1Xs≥4vuutθt,δtX
s=1
Xs−ˆXs2
+c2θ2
t,δ

and HT:={∃t≤T:Ct> c}.
Then we have,
P(ET) =P(ET∩ ¬HT) +P(ET∩HT)≤P(ET∩ ¬HT) +P(HT).
Writing ¯Ct= max {c, Ct}for short, we have
P(ET∩ ¬HT) =P
∃t≤T:tX
s=1¯CsWs≥4vuutθt,δtX
s=1¯C2s
Ws−ˆWs2
+c2θ2
t,δ∩ ¬HT

(i)=P
∃t≤T:tX
s=1Ws≥4vuutθt,δtX
s=1
Ws−ˆWs2
+θ2
t,δ∩ ¬HT

≤P
∃t≤T:tX
s=1Ws≥4vuutθt,δtX
s=1
Ws−ˆWs2
+θ2
t,δ
(ii)
≤δ,
where ( i) uses the fact that ¯Cs=cfor all s≤Twhen¬HTholds, and ( ii) uses Corollary 2.
Next, we connect Corollary 3 with a handy algebraic fact (Lemma 5) to obtain the following
result, which underpins Lemma 2.
Lemma 7. LetSbe the set of nonnegative and nondecreasing sequences. Let Ct∈ F t−1and let
Xtbe a martingale difference sequence adapted to Ftsuch that |Xt| ≤Ctwith probability 1 for all
t. Then, for all δ∈(0,1),c >0, and ˆXt∈ Ft−1such that |ˆXt| ≤Ctwith probability 1,
P
∃t≤T,∃{yi}∞
i=1∈S:tX
i=1yiXi≥8ytvuutθt,δtX
i=1
Xi−ˆXi2
+c2θ2
t,δ
≤δ+P(∃t≤T:Ct> c).
29

--- PAGE 30 ---
Proof. Follows from Lemma 5 (with yiandXitaking the roles of aiandbi, respectively), and
Corollary 3 that bounds max i≤tP
i≤tXifor all t≤T.
Proof of Lemma 2. Fork∈[T] define the random variables:
Yk= ¯rk¯dk, X k=
∆k,xk−x⋆
¯dk
,and ˆXk=−
∇f(xk),xk−x⋆
¯dk
.
From these definitions we get
t−1X
k=0YkXk=t−1X
k=0¯rk⟨∆k, xk−x⋆⟩.
Therefore,
P 
∃t≤T:t−1X
k=0¯rk⟨∆k, xk−x⋆⟩≥8¯rt−1¯dt−1q
θt,δGt−1+L2θ2
t,δ!
≤P
∃t≤T:t−1X
k=0YkXk≥8Ytvuutθt,δt−1X
k=0
Xk−ˆXk2
+L2θ2
t,δ
≤δ+P ¯ℓT> L
where the last inequality uses Lemma 7.
C.2 Proof of Corollary 1
Proof. IfT >2 log+(¯rT/rϵ) then the corollary follows by Proposition 1 and Lemma 3 with st= ¯rt,
noting that the event ¯ rT≤Dimplies that ¯ℓT≤LD. For the corner case when T≤2 log+(¯rT/rϵ) we
use that f(¯xτ)−f⋆≤O(L¯dτ)≤O(L(¯rτ+d0)) where the first inequality uses (3), Cauchy-Schwartz
and that ∥∇f(xt)∥ ≤L; the second inequality uses the triangle inequality.
C.3 DoG can diverge on a pathological instance
Consider the following variant of Nemirovski’s function [63, 62] defined on Rm:
f(x) = max
i≤mmax
[x]i,−1√m[x]i
,
where [ x]idenotes the i’th coordinate of xand [ x0]i= 10rϵ/√mfor all i, so that d0= 10rϵ> rϵ.
We show that applying DoG on this function gives ¯ rT/d0=√
T/10 for all T≤m, meaning that
the ratio ¯ rT/d0can be made arbitrarily large by increasing Tandm.
Define
i(x):= min arg max
i≤m
[x]i,−[x]i√m
,
i.e.,i(x) is the smallest coordinate which is candidate for providing a subgradient. Using this
notation, a valid subgradient for fis:
∇f(x) =(
ei(x) xi(x)>0
−1√mei(x)otherwise
30

--- PAGE 31 ---
where ejis a vector with one in the jth entry and zero elsewhere. With this subgradient choice for
k≤mthe iterates become:
[xk]j=(
10rϵ/√m−rϵj < k
10rϵ/√m j ≥k(7)
and therefore ¯ rk=√
krϵ=√
kd0/10 as claimed. We confirm (7) by induction. Since [ x0]i=
10rϵ/√mfor all i, the expression (7) holds for k= 0. If (7) holds for all k≤n < m then
∇f(xk) =ek
and therefore Gn=nso that ηn=rϵ√n√n=rϵandxn+1=xn−√n√nrϵen, meaning that
[xn+1]j=(
10rϵ/√m−rϵj < n + 1
10rϵ/√m j ≥n+ 1
which completes the induction.
C.4 Proof of Proposition 2
To show iterate boundedness in the stochastic setting we define the stopping time
Tout= min {t: ¯rt>3d0},
so that the event {¯rT≤3d0}is the same as {Tout> T}. Let ηkdenote the sequence of T-DoG
step sizes (for given L, T andδ). To facilitate our analysis we also define the following truncated
step size sequence:
˜ηk:=(
ηkk <Tout
0 otherwise .(8)
Truncating the step size allows us to rigorously handle the possibility that ¯ rTexceeds 3 d0. In
particular, the following holds for {˜ηk}but not for {ηk}. (Recall that ∆ t:=gt− ∇f(xk)).
Lemma 8. Under Assumption 2 the truncated T-DoG step sizes (8)satisfy, for all t≤T,
˜ηt∈σ(g0, . . . , g t−1), (9)
|˜ηt⟨γ, xt−x⋆⟩| ≤6d2
0
82θT,δforγ∈ {gt,∇f(xt),∆t}, (10)
tX
k=0˜η2
k∥gk∥2≤9d2
0
84θT,δ, and (11)
tX
k=0(˜ηk⟨gk, xk−x⋆⟩)2≤122d4
0
84θT,δ. (12)
Proof. Equation (9) holds directly from the definition of ( T-DoG ) and (8).
To see the bound (10), first note that that ∥∆k∥ ≤ ∥ gk∥+∥∇f(xk)∥ ≤2ℓ(xk). Since G′
t≥
4284ℓ2(xt)θ2
T,δfor all t, the Cauchy-Schwartz inequality gives
|˜ηt⟨∆t, xt−x⋆⟩| ≤¯rtp
G′
t∥∆t∥dt≤1
2·82θT,δ¯rTdt≤6d2
0
82θT,δ,
31

--- PAGE 32 ---
where the last inequality uses ¯ rt≤3d0(or else ˜ ηt= 0) and dt≤d0+ ¯rt. Bounds for |˜ηt⟨γ, xt−x⋆⟩|
forγ∈ {gt,∇f(xt)}follow by the same argument.
To establish (11), first note thatPt
k=0˜η2
k∥gk∥2≤PTout−1
k=0 η2
k∥gk∥2by the definition of ˜ ηk.
Furthermore
Tout−1X
k=0η2
k∥gk∥2=Tout−1X
k=0¯r2
k∥gk∥2
G′
k(i)
≤¯r2
Tout−1
84θT,δTout−1X
k=0Gk−Gk−1
(Gk+¯ℓ2
k) log2
+Gk+¯ℓ2
k¯ℓ2
0(ii)
≤9d2
0
84θT,δ,
where ( i) uses that ∥gk∥2=Gk−Gk−1(with the shorthand G−1:= 0) and
G′
k≥84θT,δ(Gk−1+∥gk∥2+¯ℓ2
k) log2
+ P
i≤t¯ℓ2
t
¯ℓ2
0!
≥84θT,δ(Gk+¯ℓ2
k) log2
+Gk+¯ℓ2
k
¯ℓ2
0
by Assumption 2 which implies ∥gk∥ ≤¯ℓkfor all k, while ( ii) uses Lemma 6 with ak=Gk+¯ℓ2
kand
¯rTout−1≤3d0.
The final bound (12) follows immediately from (11) by noting that
tX
k=0(˜ηk⟨gk, xk−x⋆⟩)2≤tX
k=0˜η2
k∥gk∥2d2
k≤(4d0)2tX
k=0˜η2
k∥gk∥2,
where the first inequality follows from Cauchy-Schwartz and the second inequality from the fact
that only terms with k <Toutcontribute to the sum.
The above properties allow us to establish the following concentration bound.
Lemma 9. In the setting of Lemma 8,
P 
∃t≤T:t−1X
k=0˜ηk⟨∆k, x⋆−xk⟩> d2
0!
≤δ.
Proof. Consider the filtration Ft=σ(g0, . . . , g t) and define Xt= ˜ηt⟨∆t, x⋆−xt⟩and ˆXt=
−˜ηt⟨∇f(xt), x⋆−xt⟩. Then, by (9) we have that Xtis a martingale difference sequence adapted to
Ftand ˆXt∈ Ft−1. Moreover, by (10) we have that max {|Xt|,|ˆXt|} ≤calmost surely for c=24d2
0
84θT,δ.
Substituting into Corollary 3 (and shifting the start of the summation from 1 to 0) we have
P
∃t≤T:t−1X
k=0Xk≥4vuutθt,δt−1X
k=0
Xk−ˆXk2
+c2θ2
t,δ
≤δ.
Noting that Xt−ˆXt= ˜ηt⟨gt, x⋆−xt⟩and substituting the definition of cand the bound (12) gives,
for every t < T ,
4vuutθt,δt−1X
k=0
Xk−ˆXk2
+c2θ2
t,δ≤4s
θt,δ122d4
0
84θT,δ+6θt,δd2
0
82θT,δ2
≤d2
0,
concluding the proof of lemma.
Finally, we show that the event defined in Lemma 9 implies the desired distance bound.
32

--- PAGE 33 ---
Lemma 10. In the setting of Proposition 2, ifPt−1
k=0˜ηk⟨∆k, x⋆−xk⟩ ≤d2
0for all t≤Tthen
Tout> T, i.e., ¯rT≤3d0.
Proof. To condense notation, let Bt:= max t′≤tPt′−1
k=0˜ηk⟨∆k, x⋆−xk⟩, so that the claim becomes
Bt≤d2
0implies Tout> tfor all t≤T. We prove the claim by induction on t. The basis of the
induction is that Tout>0 always holds since ¯ r0=rϵ≤3d0by assumption. For the induction step,
we assume that Bt−1implies Tout≥tand show that Bt≤d2
0implies Tout> t. To that end, we use
⟨∇f(xt), xt−x⋆⟩ ≥f(xt)−f⋆≥0 to rearrange (5) and obtain
d2
k+1−d2
k≤η2
k∥gk∥2+ 2ηk⟨∆k, x⋆−xk⟩
for all k. Summing this inequality from k= 0 to k=t−1, we get
d2
t−d2
0≤t−1X
k=0η2
k∥gk∥2+ 2t−1X
k=0ηk⟨∆k, x⋆−xk⟩=t−1X
k=0˜η2
k∥gk∥2+ 2t−1X
k=0˜ηk⟨∆k, xk−x⋆⟩,
where the equality holds since Tout> t−1 and therefore ηk= ˜ηkfor all k≤t−1. Now, by the
bound (11) we havePt−1
k=0˜η2
k∥gk∥2≤92
84θT,δd2
0≤d2
0. Moreover,Pt−1
k=0˜ηk⟨∆k, xk−x⋆⟩ ≤Bt≤d2
0by
definition and assumption, from which we conclude that d2
t≤4d2
0and hence rt≤d0+dt≤3d0.
Since ¯ rt= max {¯rt−1, rt}and ¯rt−1≤3d0by the induction assumption, we have that ¯ rt≤3d0as
well, concluding the proof.
Proposition 2 follows immediately from Lemmas 9 and 10.
C.5 Illustrating DoG’s guarantees for least squares problems
In order to illustrate the advantage of T-DoG ’s iterate boundedness guarantee, we now instantiate
Theorem 1 and Proposition 2 for stochastic least squares problems. Let Pbe a distribution over
pairs ( a, b)∈Rm×R, and for ( a, b)∼Pconsider the gradient oracle
G(x) = (⟨a, x⟩ −b)a,
corresponding to the objective function
f(x) =1
2E(a,b)∼P(⟨a, x⟩ −b)2.
For simplicity, we set x0= 0, and let x⋆be the minimum norm minimizer of f.
If we assume that ∥a∥ ≤Aand|b| ≤Bwith probability 1, then G(·) satisfies Assumption 2
with
ℓ(x) =A∥x∥+B.
The bounds ∥a∥ ≤Aand|b| ≤Bare often easy to verify (e.g., via data normalization).
With the expression for ¯ℓTin hand, we may apply T-DoG and use Proposition 2 to guarantee
that ¯ rT=O(∥x⋆∥) and hence ¯ℓT≤L⋆=O(A∥x⋆∥+B) with high probability. As a consequence,
we may bound the observed squared gradient norm sum GτbyO((A∥x⋆∥+B)2T). Substituting
into Theorem 1, the optimality of T-DoG is
eOA∥x⋆∥2+B∥x⋆∥√
T
, (13)
33

--- PAGE 34 ---
where eOhides polylogarithmic terms in T, δ, and rϵ. We emphasize that the bound above depends
on the value of ∥x⋆∥(the smallest minimizer norm) even though T-DoG assumes no knowledge of
this value.
When the domain Xis unbounded (e.g., X=Rm), previously-proposed general-purpose9
parameter-free methods [e.g. 69, 21, 11] do not directly apply, since there is no global bound on the
magnitude of the stochastic gradients.10To use these methods, one must assume an a-priori bound
Don∥x⋆∥(e.g., by positing strong convexity of f) and constrain the domain to X′={x| ∥x∥ ≤D}
where the stochastic gradients are globally bounded by L=AD+B. With such bounds in place,
previous parameter-free methods yield optimality gap bounds of the form
eO 
∥x⋆∥p
GT+ (AD+B)2
T!
,
where GTdenotes the sum of square stochastic gradient norms observed by the algorithm. The
coarseness of the upper bound D≥ ∥x⋆∥affects the leading order term in the above display: since
the iterates can be anywhere in X′, the best we can guarantee about GTisGT=O(L2T) =
O((AD+B)2T). Therefore, the best deterministic upper bound on the optimality gap of previous
methods is
eOAD∥x⋆∥+B∥x⋆∥√
T
. (14)
Comparing the bounds (13) and (14), we see that, for least squares problems, T-DoG can offer
substantially stronger guarantees than previous parameter-free methods, even when we bound the
domain to ensure that the latter methods apply.
D Guarantees for the unweighted DoG iterate average
In this section we derive guarantees similar to those presented in Section 3 for the unweighted
iterate average
ˆxt:=1
tt−1X
i=0xi.
The following proposition shows that the bound resulting from combining Proposition 1 with
Lemma 3 holds also for uniform iterate averaging.
Proposition 3. For all δ∈(0,1)andL > 0, if Assumption 1, Assumption 2 hold then with
probability at least 1−δ−P ¯ℓT> L
after Titerations of any DoG -like algorithm (Definition 1)
we have
f(ˆxT)−f(x⋆) =O
(d0log+¯rT
rϵ+ ¯rt)q
G′
T−1+GT−1θT,δ+L2θ2
T,δ
T
. (15)
Moreover, in the noiseless case we have (with probability 1)
f(ˆxT)−f(x⋆)≤1
TT−1X
t=0f(xt)−f(x⋆) =O
(d0log+¯rT
rϵ+ ¯rT)q
G′
T−1
T
. (16)
9There exist parameter-free methods specialized to least-squares problems that obtain better without requiring
an a-priori bound on the solution norm [92].
10Carmon and Hinder [11] guarantee boundedness of the point they output, but do not have guarantees on the
magnitude of intermediate query points. Orabona and P´ al [70] develop a parameter-free method with bounded
iterates, but not by O(∥x0−x⋆∥).
34

--- PAGE 35 ---
Proof. Define the times
τi= min
min
i|¯ri≥2¯rτi−1	
, T	
, (17)
with τ0:= 0. Moreover, let Kbe the first index such that τK=Tand note that K≤1 + log2¯rT
rϵ
by construction.
The argument proving Lemma 1 shows that
τk−1X
t=τk−1¯rt⟨gt, xt−x⋆⟩ ≤¯rτk(2¯dτk+ ¯rτk)q
G′
τk−1=O
¯rτk(d0+ ¯rτk)q
G′
T−1
for all k≤K. Moreover, by Lemma 2 we have that
τk−1X
t=τk−1¯rt⟨∆t, xt−x⋆⟩≤τk−1X
t=0¯rt⟨∆t, xt−x⋆⟩+τk−1−1X
t=0¯rt⟨∆t, xt−x⋆⟩
≤16¯rτk−1¯dτk−1q
θt,δGT−1+θ2
t,δL2
holds for all k≤Kwith probability at least 1 −δ−P(¯ℓT> L). (The rest of the analysis assumes
this event holds).
Combining these two bounds, we have
τk−1X
t=τk−1[f(xk)−f(x⋆)]≤1
¯rτk−1τk−1X
t=τk−1¯rt[f(xk)−f(x⋆)]≤1
¯rτk−1τk−1X
t=τk−1¯rt⟨∇f(xt), xt−x⋆⟩
=1
¯rτk−1τk−1X
t=τk−1¯rt⟨gt, xt−x⋆⟩ −1
¯rτk−1τk−1X
t=τk−1¯rt⟨∆t, xt−x⋆⟩
=O¯rτk
¯rτk−1(d0+ ¯rτk)q
G′
T−1+θt,δGT−1+θ2
t,δL2
.
=O
(d0+ ¯rτk)q
G′
T−1+θt,δGT−1+θ2
t,δL2
, (18)
where the last transition holds since, for any DoG -like method and all t,
¯rt+1≤¯rt+∥xt+1−xt∥= ¯rt 
1 +∥gt∥p
G′
t!
≤2¯rt.
Summing Equation (18) over kfrom 1 to K, we obtain
T−1X
t=0[f(xk)−f(x⋆)] =KX
k=1τk−1X
t=τk−1[f(xk)−f(x⋆)]
=O  
d0K+KX
k=1¯rτk!q
G′
T−1+θt,δGT−1+θ2
t,δL2!
.
Finally, recall that K=O
log+¯rT
rϵ
and note thatPK
k=1¯rτk=O(¯rT) since¯rτi
¯rτK−1≤2−K−1+ifor
alli≤K−1. The proof of Equation (15) is complete upon noting that f(ˆxT)≤1
TPT−1
t=0f(xt) by
Jensen’s inequality. To show Equation (17) we simply set ∆ t= 0 in the bound above.
35

--- PAGE 36 ---
Using Proposition 3 we may replace the early-stopped weighted average ¯ xτwith the final un-
weighted average ˆ xTin Corollary 1 and Theorem 1.
Proposition 3 also shows that (as long as iterates remain bounded) DoG attains a 1 /Trate of
convergence in the smooth noiseless cases. In particular, if we assume that ∇fisS-Lipschitz and
noiseless gradients (i.e., gi=∇f(xi)), then we have
GT−1≤2ST−1X
t=0[f(xt)−f(x⋆)].
Substituting this bound back into Equation (17) (with G′
T−1=GT−1forDoG ), we obtain
1
TT−1X
t=0f(xt)−f(x⋆) =O
(d0log+¯rT
rϵ+ ¯rT)q
S
TPT−1
t=0f(xt)−f(x⋆)
√
T
.
Dividing through byq
1
TPT−1
t=0f(xt)−f(x⋆) and squaring yields
f(ˆxT)−f(x⋆)≤1
TT−1X
t=0f(xt)−f(x⋆) =O 
S(d0log+¯rT
rϵ+ ¯rT)2
T!
.
E Experiment Details
E.1 Environment settings
All experiments were based on PyTorch [74] (version 1.12.0).
Language experiments were done with the transformers [100] library (version 4.21.0) and tracked
using the Comet.ML [18]. All datasets were provided by the Datasets library [53] (version 2.4.0)
and were left as is, including train-eval-test splits.
Vision experiments were based on the pytorch-image-models (timm, version0.7.0dev0) repository
[98], with TensorFlow datasets (version 4.6.0) as a dataset backend [1].
To support the training and analysis of the results, we used numpy [35], scipy [91], pandas [97]
andscikit-learn [75].
E.2 Implementation details
Whenever possible, we used existing scripts and recipes provided by timm andtransformers to
fine-tune the models. We implemented DoG ,L-DoG and the polynomial model averaging as a
subclass of PyTorch Optimizer interface. We provide implementation of both in https://github
.com/formll/dog .
E.3 Datasets
The datasets used in the language experiments are: CoLA [96],SST-2 [86],MRPC [27],QQP
[41],STS-B [13],MNLI [99],QNLI [79], and RTE [22, 5, 32, 7]. Following Liu et al. [54], we
discard WNLI [51] as it was found to be ill-defined and was reformulated differently in SuperGLUE
[93].
The datasets used in the vision experiments are: Caltech101 [31],CIFAR-100 [50],CLEVR-
Dist [43],DMLab [6],dSprites-Ori [60],DTD [17],Flowers102 [67],Pets [73],Resisc45 [16],
Retinopathy [45],Sun397 [102, 103], and SVHN [66].
36

--- PAGE 37 ---
Task Batch size Steps Metric LR warmup LR annealing Grad. clipping
VTAB datasets 128 20K Accuracy None Cosine None
SQuAD 48 5475 F1 10% Cosine 1
SST-2 32 31407 Accuracy 10% Cosine 1
CoLA 32 10000 Matthews correlation 10% Cosine 1
MRPC 32 1734 F1 10% Cosine 1
STSB 32 3281 Pearson correlation 10% Cosine 1
QNLI 32 49218 Accuracy 10% Cosine 1
RTE 32 10000 Accuracy 10% Cosine 1
QQP 32 160625 F1 10% Cosine 1
MNLI 32 184218 Accuracy 10% Cosine 1
Table 3: Configuration used for each dataset in our testbed (Section 4.1). For all language tasks,
we used the batch size as in Liu et al. [54], and at least 150% the number of steps used there, in
order to ensure convergence. Learning rate (LR) warmup and annealing refers to tuning with SGD
and Adam. In all cases, both DoG andL-DoG used neither warmup nor annealing.
E.4 Models
When fine-tuning RoBERTa (from the ‘roberta-base’ checkpoint) on classification tasks, we follow
the common technique of prepending a CLS token to the input, and feeding its final representa-
tion to a one hidden-layer, randomly initialized MLP that is used as a classification head. For
SQuAD, the classification head is tasked with multi-label classification, predicting the probability
that each word (token) in the input is the beginning/end of the answer span, and we then used
the span that has the maximum likelihood as the model’s output. When fine-tuning T5 (from the
‘t5-base’ checkpoint), we treated all tasks as sequence-to-sequence tasks, translating classification
labels to appropriate words (e.g. 0/1 to positive/negative) and then evaluated accuracy with exact
match. The computer vision pre-trained models were accessed via timm, and had randomly ini-
tialized classification heads. The strings used to load the models were: ‘convnext tiny’, ‘resnet50’,
‘densenet121’, ‘vit base patch32 224in21k’ and ‘vgg11’.
E.5 Hyper-parameters
We trained each model/task combination a fixed number of steps (see Table 3), performing eval-
uation every 500 update steps (except for the smaller datasets Caltech101, DTD, Flowers102 and
Pets where we evaluated every 200) with both the latest checkpoint, and the polynomial averaged
one (see below). We did not use any weight decay. For language models, we left dropout at its
default value in the transformers library. We used batch sizes as is common practice for each task,
as detailed in Table 3.
Data augmentation in vision experiments. The VTAB suite [108] divides its datasets into
three categories: natural, specialized and structured, and we used a suitable data augmentation
strategy for each of the categories. In particular, for structured datasets we simply resized the
images to a (224, 224) resolution, while for the natural and specialized datasets we used the standard
“inception crop” [88] at training time and a 0.875 center crop at test time. For natural datasets we
additionally applied a color jitter operation with parameter 0.4 (as implemented in timm). Finally,
we applied a random horizontal flip for all datasets except SVHN and dSprites-Ori, where such
augmentation clearly interferes with the task.
37

--- PAGE 38 ---
Model selection in vision experiments. For computer vision experiments, we used the VTAB
evaluation splits to select the best checkpoint, and then reported performance on the test split.
Unlike the experiments accompanying the VTAB suite [108], we did not retrain selected models on
the combination of training and validation data.
Repeated runs. To account for randomness, we repeated our fine-tuning experiments using mul-
tiple seeds. In most cases (with exceptions listed below) we repeated each DoG andL-DoG training
5 times. For SGD and Adam repeating the training with all learning rates was computationally
prohibitive, so instead for each task / model pair we repeated 5 times only the best-performing LR
(i.e., instance-tuned LR) and the best-performing LR across all tasks for that model (i.e., model-
tuned LR) according the validation split. A few experiments were too computationally expensive
to repeat: for QQP and MNLI (which require a large step budget) we have only 1–3 repetitions
per training configuration, and for ConvNeXt-T (which takes a long time per step) we did repeat
the training runs.
Each relative error difference (RED) score combines the error of two optimization methods (one
being DoG ) on a particular model task combination. Given multiple seeds for each optimization
method, we computed the RED scores for each possible seed combination. In Figures 2, 3, 5
and 6 (which aggregate multiple tasks) we average over those multiple RED values and compute
the statistics of the average RED. In per-task breakdowns such as Figure 7 and tables 4 and 5 we
report the statistics over the multiple RED values.
Baseline optimizers. For both SGD and Adam, we used cosine learning rate decay, and searched
over appropriate values for peak learning rate. The base learning rate search space used when
performing fine-tuning for each model/task combination can be found in Tables 4 and 5. We did
not use momentum for SGD. For Adam we used β1= 0.9 for all experiments, and β2= 0.999 for
language experiments and β2= 0.99 for vision experiments. For language models only, we used
warmup of 10% of the maximum steps count, and gradient clipping at global norm 1. We did
not perform learning rate warmup or gradient clipping for the vision experiment since we did not
encounter any training stability issues there.
Setting rϵ.As explained in Section 4.1, setting rϵ:=α(1 +∥x0∥) generally works well for α=
10−4. However, in some cases such as with T5, ∥x0∥can be very large, causing destructively large
first updates, with ηtincreasing exponentially and the model diverging. This is easily detectable
early during training, as usually ηtexceeds 1000 within the first 100 steps. Since the theory
requires rϵto be small, we simply decreased αby a factor of 100. While preliminary experiments
with RoBERTa indicated that DoG also performed well with α= 10−4, for the sake of consistency
we use the same values in all models of the same domain. Thus, models fine-tuned on vision tasks
used α= 10−4, while language models used 10−6forDoG and 10−8forL-DoG .
Model averaging. As mentioned in Section 4.1, we used the polynomial decay averaging as
proposed by Shamir and Zhang [83]. Namely, we kept an additional copy of the model weights,
and in every update step we updated our running average of the model parameters as follows:
¯xγ
t=
1−1 +γ
t+γ
¯xγ
t−1+1 +γ
t+γxt (19)
The vector ¯ xγ
troughly corresponds to an average of the last t/γiterates preceding iteration t. For
all models, we set γ= 8. We did not perform any tuning of the parameter γ; we chose the value 8
38

--- PAGE 39 ---
because 1 /8 seemed like a good fraction of iterates to average, and because it worked well in the
experiments of Levy et al. [52].
To ensure that iterate averaging is never harmful, for each optimization method we selected the
best-performing checkpoint across both xtand ¯xγ
t(i.e., with or without averaging).
E.6 Figure 1 details
We generated Figure 1 as part of our fine-tuning testbed. In particular, SGD used a cosine learning
rate annealing (without warmup), both algorithms use polynomial decay averaging, and we report
test performance on the best checkpoint selected on a validation set.
E.7 Fine-tuning ImageNet
Our training setup mostly followed the default configuration in Wortsman et al. [101]. In partic-
ular, we used batch size 512 and the default timm augmentation (as in our main computer vision
experiments) which Wortsman et al. [101] refer to as ‘medium aug.’ We trained for 25K steps,
corresponding to roughly 10 passes over the data. However (keeping with our computer vision
testbed setting) we did not perform learning rate warmup or gradient clipping, and we initialized
the classification head to be random.
For AdamW [57] we used weight decay 0.1 and cosine learning rate annealing as in Worts-
man et al. [101]. We obtained accuracies within 0.5% of the numbers reported in Appendix L of
Wortsman et al. [101].
ForDoG andL-DoG we used weight decay 0 since the value 0.1 is meant for decoupled weight
decay and we did not wish to re-tune a weight decay parameter. We set rϵto be 10−6·(1 +∥x0∥)
without trying different values of this parameter.
For SGD we used cosine learning rate annealing and set weight decay to 0 for a more direct
comparison to DoG .
E.8 Training from scratch
Our training setup followed the basic training configuration of Cubuk et al. [19], which is typical
for training ResNets on CIFAR-10: data augmentations comprising a random crop after 4 pixel
padding and random horizontal flip, batch size of 128, weight decay of 0.0005 and 200 epochs of
training. SGD used cosine learning weight annealing and (when applicable) Nesterov momentum.
We did not use dropout or any other additional form of regularization. For DoG andL-DoG ,
we set rϵ= 10−4·(1 +∥x0∥) without trying different values of this parameter. The accuracies
we obtained using SGD and DoG are consistent (and slightly better) than the baseline number
reported in Table 2 of Cubuk et al. [19] and within 0.1% of the one reported in Table 1 of Carmon
et al. [12].
F Additional experiment results
F.1 Full breakdown of main experiment results
Figure 7 as well as Tables 4 and 5 provide the full breakdown of our main fine-tuning results,
comparing DoG andL-DoG to SGD and Adam with different learning rates for each model/task
combination.
39

--- PAGE 40 ---
Model Optimizer LR CoLA MNLI MRPC QNLI QQP RTE SQuAD SST-2 STS-B Avg.
RoBERTa-bAdam5e-06 60.8 87.8 89.8 93.0 88.7 77.6 90.3 95.1 (0.34) 90.4 85.46
1e-05 63.4 87.9 (0.05) 90.5 93.1 (0.22) 89.0 (0.11) 77.6 91.5 95.1 90.8 86.22
3e-05 63.5 (1.33) 87.3 92.3 (0.44) 92.9 (0.15) 88.8 80.6 (1.19) 92.3 (0.05) 94.8 (0.16) 91.1 (0.19) 86.94
5e-05 61.8 86.8 92.0 92.3 88.0 78.7 92.4 (0.07) 94.3 90.9 85.93
0.0001 57.5 86.2 91.8 91.3 0.0 79.4 91.9 94.7 89.8 75.33
SGD0.003 56.3 86.4 81.9 91.5 85.1 75.5 79.4 93.6 87.0 81.44
0.01 59.1 87.4 89.7 92.5 87.0 79.0 (0.69) 86.2 94.8 90.3 (0.25) 84.81
0.03 62.3 (1.38) 87.8 (0.04) 91.8 (0.21) 92.7 (0.18) 88.3 78.9 (0.79) 89.5 (0.12) 95.0 (0.15) 90.7 (0.12) 86.30
0.1 58.7 87.4 91.0 92.2 88.7 (0.06) 78.3 91.0 94.0 90.4 85.52
0.3 0.0 86.0 81.2 85.3 87.7 64.6 91.3 (0.11) 92.8 27.0 68.14
1.0 0.0 81.5 81.2 83.8 79.4 52.7 82.8 89.7 13.0 62.22
DoG - 62.8 (1.17) 87.7 (0.12) 91.6 (0.29) 92.6 (0.15) 88.2 (0.02) 78.5 (2.91) 91.3 (0.17) 94.9 (0.26) 90.5 (0.33) 86.46
L-DoG - 63.3 (0.32) 87.7 (0.12) 91.5 (0.19) 92.8 (0.28) 88.7 (0.14) 80.1 (1.00) 91.8 (0.18) 94.8 (0.54) 90.6 (0.34) 86.81
T5-bAdam5e-06 53.4 (0.93) 86.8 91.4 93.4 88.0 79.8 90.3 93.9 90.4 84.82
1e-05 56.0 (0.63) 86.9 91.2 93.5 88.2 80.5 90.4 94.2 90.6 85.33
3e-05 58.9 (1.10) 87.1 91.6 93.4 (0.16) 88.8 82.3 90.8 94.8 (0.24) 90.7 86.12
5e-05 58.9 (0.80) 87.3 91.8 93.3 89.0 80.9 90.7 94.8 90.8 (0.10) 85.97
0.0001 58.3 (0.80) 86.9 92.9 (0.35) 93.5 (0.10) 89.2 82.5 (0.48) 90.9 (0.15) 94.9 (0.26) 90.8 (0.18) 86.53
0.0005 55.4 (0.45) 86.1 92.3 92.7 88.8 81.2 (1.48) 90.0 94.6 89.7 85.29
SGD0.003 22.9 (1.64) 85.8 90.1 92.7 87.5 66.8 90.3 92.1 90.3 79.43
0.01 49.4 (0.27) 86.4 92.2 93.1 87.4 80.9 90.3 93.0 90.5 84.49
0.03 56.4 (0.52) 86.5 92.0 93.2 (0.03) 88.1 80.9 90.5 93.6 90.6 85.40
0.1 58.9 (0.82) 86.8 91.8 93.1 88.7 84.1 90.7 (0.04) 93.7 90.6 (0.09) 86.13
0.3 56.7 (0.83) 86.1 92.8 (0.47) 93.0 (0.13) 88.7 82.8 (1.24) 90.7 (0.05) 93.9 (0.15) 90.7 (0.21) 86.07
1.0 0.0(0.00) 32.8 81.2 91.7 56.9 78.7 90.1 92.1 88.7 67.56
DoG - 7.3(6.78) 86.9 (0.21) 92.8 (0.35) 93.1 (0.09) 88.5 81.7 (3.06) 90.6 (0.05) 94.1 (0.19) 90.7 (0.09) 80.58
L-DoG - 59.9 (1.43) 87.3 (0.10) 91.9 (0.32) 93.6 (0.02) 87.8 83.1 (0.78) 90.3 (0.02) 95.0 (0.19) 90.5 (0.05) 86.51
Table 4: Average (std) performance of RoBERTa-b and T5-b on language tasks, when fine-tuned
with different optimization algorithms and their respective base learning rate (when applicable).
DoG usesrϵ= 10−6(1 +∥x0∥) and L-DoG usesrϵ= 10−8(1 +∥x0∥). Scores are reported as mean
across seeds, measured in the corresponding performance metric as detailed in Table 3.
40

--- PAGE 41 ---
50%
40%
30%
20%
10%
0%10%Relative Error Difference
RoBERTa-b on MRPC
50%
40%
30%
20%
10%
0%10%
T5-b on MRPC
40%
20%
0%20%40%
ResNet50 on SVHN
40%
20%
0%20%40%
ViT-B/32 on SVHN
50%
40%
30%
20%
10%
0%10%20%
Densenet121 on SVHN
40%
20%
0%20%40%
ConvNeXt-T on SVHN
40%
20%
0%20%40%
VGG11 on SVHN
50%
40%
30%
20%
10%
0%10%Relative Error Difference
RoBERTa-b on SST-2
40%
30%
20%
10%
0%10%
T5-b on SST-2
40%
20%
0%20%
ResNet50 on Resisc45
40%
20%
0%20%40%
ViT-B/32 on Resisc45
50%
40%
30%
20%
10%
0%10%20%
Densenet121 on Resisc45
40%
20%
0%20%40%
ConvNeXt-T on Resisc45
40%
20%
0%20%40%
VGG11 on Resisc45
40%
20%
0%20%40%Relative Error Difference
RoBERTa-b on STS-B
20.0%
15.0%
10.0%
5.0%
0.0%
T5-b on STS-B
50%
40%
30%
20%
10%
0%
ResNet50 on Retinopathy
30.0%
25.0%
20.0%
15.0%
10.0%
5.0%
0.0%
ViT-B/32 on Retinopathy
30.0%
20.0%
10.0%
0.0%
Densenet121 on Retinopathy
50%
40%
30%
20%
10%
0%
ConvNeXt-T on Retinopathy
40%
30%
20%
10%
0%
VGG11 on Retinopathy
50%
40%
30%
20%
10%
0%10%20%Relative Error Difference
RoBERTa-b on SQuAD
6.0%
4.0%
2.0%
0.0%2.0%4.0%
T5-b on SQuAD
40%
20%
0%20%40%
ResNet50 on Caltech101
40%
20%
0%20%40%
ViT-B/32 on Caltech101
50%
40%
30%
20%
10%
0%10%20%
Densenet121 on Caltech101
40%
20%
0%20%40%
ConvNeXt-T on Caltech101
50%
40%
30%
20%
10%
0%10%20%
VGG11 on Caltech101
50%
40%
30%
20%
10%
0%10%Relative Error Difference
RoBERTa-b on QNLI
20.0%
15.0%
10.0%
5.0%
0.0%5.0%
T5-b on QNLI
40%
20%
0%20%40%
ResNet50 on Pets
40%
20%
0%20%40%
ViT-B/32 on Pets
40%
20%
0%20%
Densenet121 on Pets
40%
20%
0%20%40%
ConvNeXt-T on Pets
40%
20%
0%20%40%
VGG11 on Pets
50%
40%
30%
20%
10%
0%Relative Error Difference
RoBERTa-b on MNLI
50%
40%
30%
20%
10%
0%10%20%
T5-b on MNLI
40%
20%
0%20%
ResNet50 on DMLab
50%
40%
30%
20%
10%
0%10%
ViT-B/32 on DMLab
40%
30%
20%
10%
0%10%20%
Densenet121 on DMLab
50%
40%
30%
20%
10%
0%10%20%
ConvNeXt-T on DMLab
50%
40%
30%
20%
10%
0%10%20%
VGG11 on DMLab
40%
20%
0%20%40%Relative Error Difference
RoBERTa-b on QQP
50%
40%
30%
20%
10%
0%10%20%
T5-b on QQP
40%
20%
0%20%40%
ResNet50 on Flowers102
40%
20%
0%20%40%
ViT-B/32 on Flowers102
40%
20%
0%20%
Densenet121 on Flowers102
40%
20%
0%20%40%
ConvNeXt-T on Flowers102
40%
20%
0%20%
VGG11 on Flowers102
50%
40%
30%
20%
10%
0%10%Relative Error Difference
RoBERTa-b on CoLA
10%
0%10%20%30%40%50%
T5-b on CoLA
50%
40%
30%
20%
10%
0%10%
ResNet50 on Sun397
50%
40%
30%
20%
10%
0%10%20%
ViT-B/32 on Sun397
50%
40%
30%
20%
10%
0%
Densenet121 on Sun397
50%
40%
30%
20%
10%
0%10%20%
ConvNeXt-T on Sun397
50%
40%
30%
20%
10%
0%10%
VGG11 on Sun397
104
102
100
Learning rate50%
40%
30%
20%
10%
0%10%20%Relative Error Difference
RoBERTa-b on RTE
104
102
100
Learning rate50%
40%
30%
20%
10%
0%10%20%
T5-b on RTE
40%
20%
0%20%40%
ResNet50 on dSprites-Ori
40%
20%
0%20%40%
ViT-B/32 on dSprites-Ori
40%
20%
0%20%40%
Densenet121 on dSprites-Ori
0%10%20%30%40%50%
ConvNeXt-T on dSprites-Ori
0%10%20%30%40%50%
VGG11 on dSprites-Ori
50%
40%
30%
20%
10%
0%10%20%Relative Error Difference
ResNet50 on DTD
50%
40%
30%
20%
10%
0%10%20%
ViT-B/32 on DTD
50%
40%
30%
20%
10%
0%10%
Densenet121 on DTD
50%
40%
30%
20%
10%
0%10%
ConvNeXt-T on DTD
50%
40%
30%
20%
10%
0%
VGG11 on DTD
50%
40%
30%
20%
10%
0%10%20%Relative Error Difference
ResNet50 on CIFAR-100
40%
20%
0%20%40%
ViT-B/32 on CIFAR-100
50%
40%
30%
20%
10%
0%
Densenet121 on CIFAR-100
40%
20%
0%20%40%
ConvNeXt-T on CIFAR-100
50%
40%
30%
20%
10%
0%10%
VGG11 on CIFAR-100
104
102
100
Learning rate40%
20%
0%20%40%Relative Error Difference
ResNet50 on Clevr-Dist
104
102
100
Learning rate40%
20%
0%20%40%
ViT-B/32 on Clevr-Dist
104
102
100
Learning rate40%
20%
0%20%
Densenet121 on Clevr-Dist
104
102
100
Learning rate40%
20%
0%20%40%
ConvNeXt-T on Clevr-Dist
104
102
100
Learning rate40%
20%
0%20%40%
VGG11 on Clevr-DistAdam (median)
SGD (median)
Adam (mean)
SGD (mean)
L-DoGFigure 7: Relative error difference (RED) statistics across seeds (median, mean and IQR shown as
shaded region) for all model/task combinations. The red horizontal line shows the median RED of
L-DoG .
41

--- PAGE 42 ---
Model Optimizer LR Caltech101 CIFAR-100 Clevr-Dist DMLab dSprites-Ori DTD Flowers102 Pets Resisc45 Retinopathy Sun397 SVHN Avg.
ConvNeXt-TAdam3e-06 - 70.0 89.2 70.5 86.5 72.1 92.5 93.1 91.2 - 36.7 - -
1e-05 89.0 84.7 91.7 72.7 95.7 70.9 93.5 93.3 94.7 83.2 64.2 96.6 85.33
3e-05 89.4 87.8 91.4 73.2 96.3 71.3 93.3 92.9 95.6 83.2 74.3 97.3 86.75
0.0001 87.5 88.5 91.9 76.0 96.4 73.3 93.9 92.6 95.9 83.7 76.0 97.4 87.25
0.0003 91.1 88.2 93.2 77.5 7.6 72.5 94.3 93.8 96.3 83.9 76.3 97.8 80.58
0.001 87.5 85.9 93.2 78.5 7.6 69.1 93.4 92.3 95.9 83.5 74.7 97.5 79.42
0.003 87.6 73.7 90.2 22.2 7.6 63.4 87.9 88.3 94.7 73.6 71.7 19.6 64.50
SGD0.01 90.2 85.0 90.8 70.3 7.6 72.0 92.0 94.3 93.4 82.9 68.4 96.2 78.25
0.03 89.5 87.2 91.1 72.2 7.6 72.6 91.8 94.1 94.8 83.4 74.2 97.0 79.25
0.1 89.1 87.5 90.9 74.3 7.6 72.3 92.9 94.7 95.4 83.6 75.4 97.2 79.58
0.3 89.7 87.4 24.5 75.2 7.6 71.4 92.4 93.8 95.9 83.4 75.1 97.3 74.00
1.0 88.1 86.1 24.5 22.2 7.6 64.0 0.4 13.4 2.1 73.6 74.5 19.6 39.33
3.0 0.4 1.0 20.0 22.2 7.4 2.1 0.3 2.7 2.2 73.6 0.5 6.7 11.25
10.0 - 1.0 20.0 22.2 7.4 2.1 0.3 2.7 2.2 - 0.5 - -
DoG - 89.9 87.7 90.7 75.3 7.6 71.6 92.4 93.8 95.5 83.5 75.0 97.3 79.50
L-DoG - 87.7 88.2 88.5 76.3 96.4 73.8 92.7 93.7 95.9 83.7 75.9 97.7 86.92
Densenet121Adam3e-06 - 62.3 84.3 65.0 84.8 65.9 88.1 88.4 90.6 - 37.9 - -
1e-05 86.5 (1.24) 76.9 86.9 66.3 95.4 66.4 88.6 89.7 93.9 81.1 59.6 95.6 81.71
3e-05 85.2 82.0 89.0 69.0 95.9 66.4 (0.76) 90.0 89.0 94.4 81.0 68.7 96.8 83.70
0.0001 86.7 (1.40) 82.8 (0.26) 91.4 (0.16) 72.7 (0.69) 96.3 (0.07) 65.8 (0.39) 89.4 (1.08) 89.4 (0.57) 94.8 (0.16) 81.7 (0.10) 70.8 (0.45) 97.3 (0.05) 84.92
0.0003 84.5 83.3 92.7 76.3 96.4 (0.03) 65.1 88.9 89.2 95.1 (0.20) 82.7 71.6 97.7 (0.08) 84.93
0.001 81.8 80.8 93.9 76.6 (0.39) 96.4 62.8 87.2 84.7 94.9 83.0 (0.08) 69.7 97.7 83.55
0.003 76.0 76.7 93.7 (0.17) 74.6 96.0 56.1 81.5 82.7 93.9 82.8 66.2 97.4 81.06
SGD0.01 87.7 (0.74) 83.6 89.5 68.5 96.1 65.7 87.4 90.9 94.2 81.6 68.9 96.7 83.73
0.03 87.0 84.0 (0.08) 91.1 71.4 96.3 (0.07) 66.6 (1.59) 88.6 90.5 (0.47) 94.7 82.0 71.1 97.1 84.87
0.1 87.9 (0.57) 83.7 (0.31) 92.8 (0.26) 74.5 (0.53) 96.4 (0.05) 65.9 (0.49) 88.3 (0.82) 90.6 (0.28) 94.9 (0.36) 82.5 (0.08) 71.5 (0.21) 97.4 (0.09) 85.53
0.3 85.5 82.5 92.4 (0.56) 69.1 (3.67) 95.9 62.9 87.6 87.9 95.0 (0.25) 82.6 (0.28) 70.9 97.2 83.67
1.0 61.6 65.1 91.4 61.9 7.6 35.4 55.4 64.6 82.6 80.0 62.0 95.6 63.17
3.0 50.1 61.6 88.5 59.2 7.6 23.4 44.1 39.8 85.8 76.5 45.6 94.7 55.92
DoG - 87.4 (0.65) 84.0 (0.18) 91.4 (0.19) 71.9 (0.43) 96.2 (0.04) 66.1 (0.90) 88.5 (0.92) 90.3 (0.40) 94.8 (0.12) 82.0 (0.08) 71.6 (0.22) 97.2 (0.13) 85.12
L-DoG - 86.9 (0.27) 83.5 (0.18) 89.8 (0.31) 71.5 (0.24) 96.1 (0.03) 66.4 (0.61) 88.7 (0.70) 90.6 (0.13) 95.3 (0.17) 81.9 (0.10) 71.4 (0.25) 97.5 (0.09) 84.97
ResNet50Adam0.0003 87.8 (1.52) 84.8 91.0 73.3 96.2 68.5 (0.97) 92.7 93.1 (0.27) 95.5 82.2 73.9 97.0 86.03
0.001 88.3 (1.18) 83.9 (0.31) 92.4 (0.21) 76.5 (0.53) 96.3 (0.04) 67.2 (0.94) 89.2 (1.83) 92.0 (0.37) 95.5 (0.25) 83.0 (0.18) 73.7 (0.33) 97.6 (0.07) 86.30
0.003 86.9 83.1 93.3 (0.37) 78.3 (0.30) 96.1 64.4 90.1 90.2 95.4 83.4 (0.15) 72.2 97.7 85.67
0.01 79.9 75.9 93.5 75.0 95.9 58.0 85.1 85.0 94.4 83.0 66.5 97.4 82.08
0.03 62.8 64.9 92.4 71.3 95.3 46.9 63.1 67.0 89.3 82.0 50.8 96.4 73.08
SGD0.01 86.7 62.9 83.7 52.5 68.3 66.2 80.9 91.9 84.0 75.9 48.0 80.2 72.92
0.03 86.7 77.0 88.0 63.0 88.5 67.2 82.1 92.8 90.5 78.8 64.7 91.4 80.50
0.1 87.6 (0.66) 82.8 90.2 67.0 95.5 67.5 (1.71) 89.2 93.5 93.8 81.7 71.6 95.0 84.26
0.3 87.4 84.8 91.1 70.7 95.9 70.5 85.8 (2.54) 92.9 94.7 82.3 73.9 96.1 84.98
1.0 86.6 (0.50) 84.5 (0.46) 90.1 (0.51) 72.6 (1.56) 96.0 (0.08) 66.4 (1.16) 85.5 (2.39) 93.0 (0.30) 94.6 (0.28) 82.6 (0.09) 73.6 (0.44) 96.8 (0.06) 85.19
3.0 87.4 84.7 91.8 70.1 96.0 66.9 90.3 92.1 94.8 (0.49) 82.1 73.7 97.0 (0.10) 85.23
10.0 86.2 81.2 91.4 (0.71) 67.2 7.6 59.4 86.6 82.2 94.6 78.2 70.0 96.9 74.78
30.0 0.4 12.1 20.0 22.2 7.4 24.1 43.6 16.5 83.6 73.6 3.1 6.7 25.75
DoG - 86.8 (0.62) 84.8 (0.37) 89.3 (0.58) 71.4 (0.77) 95.7 (0.09) 66.4 (1.48) 85.8 (2.64) 92.9 (0.38) 94.6 (0.33) 82.6 (0.16) 73.5 (0.41) 96.5 (0.10) 85.02
L-DoG - 87.6 (1.15) 84.6 (0.38) 90.8 (0.38) 75.0 (0.44) 96.0 (0.05) 69.1 (1.30) 90.2 (2.02) 92.4 (0.36) 95.6 (0.46) 82.2 (0.15) 74.6 (0.32) 97.4 (0.08) 86.29
VGG11Adam3e-06 80.2 (0.71) - - - - - - - - 79.8 (0.07) - 93.8 (0.15) -
1e-05 80.5 73.7 89.3 63.5 94.3 61.5 82.1 87.3 91.4 80.0 65.4 95.3 80.00
3e-05 82.2 (0.48) 74.9 90.5 67.7 96.1 (0.06) 61.4 (0.84) 84.0 88.1 (0.17) 92.9 81.1 66.6 96.4 81.48
0.0001 82.6 75.6 (0.23) 92.4 71.9 7.6 61.6 83.5 (1.06) 87.2 93.5 (0.23) 82.3 66.9 (0.12) 96.8 74.79
0.0003 82.3 74.1 93.2 (0.25) 74.4 (0.47) 7.5 59.6 83.0 86.0 93.4 82.9 (0.08) 66.3 96.8 (0.14) 74.77
0.001 61.7 61.1 24.5 64.6 7.5 48.0 53.7 65.0 89.1 73.6 50.5 96.5 57.58
0.003 - 1.0 89.4 68.4 7.6 2.1 0.5 2.7 76.2 - 30.5 - -
0.01 - 1.0 24.5 22.2 7.6 2.1 1.2 2.7 2.2 - 2.0 - -
SGD0.001 81.0 68.5 85.0 62.3 14.8 (3.59) 61.3 80.3 88.0 89.3 78.9 61.9 92.0 71.65
0.003 81.8 72.4 90.3 64.2 12.3 62.2 80.9 88.0 90.9 80.3 64.9 94.4 73.08
0.01 82.4 73.5 91.9 (0.19) 67.0 9.8 61.2 82.0 (0.99) 89.0 (0.37) 91.7 81.7 65.8 95.7 73.91
0.03 83.0 (0.50) 74.7 (0.21) 92.1 (0.19) 69.1 (0.36) 7.6(0.04) 61.9 (0.41) 82.3 (1.09) 89.4 (0.32) 92.5 (0.28) 82.2 (0.06) 66.1 (0.10) 96.4 (0.08) 74.77
0.1 49.6 (44.91) 74.6 (0.08) 20.0 22.2 7.6 60.5 0.3 87.5 2.2 73.6 53.2 (29.49) 6.7 37.87
0.3 - 1.0 20.0 22.2 7.4 2.1 0.3 2.7 2.2 - 0.5 - -
1.0 - 1.0 20.0 22.2 7.4 2.1 0.3 2.7 2.2 - 0.5 - -
DoG - 82.9 (0.45) 74.7 (0.22) 91.5 (0.17) 68.4 (0.53) 10.4 (0.82) 62.5 (1.19) 82.8 (0.95) 89.5 (0.23) 92.4 (0.22) 81.6 (0.07) 66.3 (0.33) 96.3 (0.09) 74.94
L-DoG - 82.4 (0.60) 74.8 (0.16) 92.0 (0.11) 69.9 (0.44) 92.1 (8.59) 62.6 (0.95) 82.7 (1.15) 88.9 (0.62) 92.4 (0.15) 81.9 (0.18) 65.8 (0.09) 96.5 (0.12) 81.83
ViT-B/32Adam3e-06 90.7 (0.76) 92.3 (0.22) 89.5 (0.35) 66.0 (0.69) 94.0 (0.24) 74.9 (0.24) 98.5 (0.53) 91.6 (0.11) 95.5 (0.07) 79.7 (0.08) 75.5 (0.23) 96.8 (0.06) 87.08
1e-05 90.2 (0.47) 92.8 (0.21) 89.9 (0.52) 67.5 51.5 (48.10) 76.9 98.7 (0.29) 90.7 96.3 79.8 78.0 (0.12) 97.6 83.84
3e-05 88.6 92.5 89.7 70.1 7.6 75.3 (0.24) 98.7 91.6 (0.21) 96.5 (0.13) 80.1 (0.09) 78.3 97.7 80.21
0.0001 89.5 91.2 89.1 70.8 (0.32) 7.6 72.9 98.1 90.0 96.1 80.1 77.0 97.8 (0.07) 79.80
0.0003 87.9 87.3 87.9 68.5 7.6 69.0 94.9 87.9 94.9 79.3 72.5 97.9 77.33
0.001 80.3 62.1 88.1 51.2 7.6 50.4 71.0 58.3 88.6 73.6 53.6 96.3 64.75
0.003 - 13.5 64.6 29.3 7.6 14.1 26.7 12.1 71.9 - 19.5 - -
SGD3e-05 - 6.1 52.7 40.8 32.7 45.9 61.4 80.3 59.7 - 5.2 - -
0.0001 85.0 73.7 71.3 50.4 57.0 69.0 98.1 90.1 83.0 75.3 24.7 79.1 71.17
0.0003 88.7 88.7 83.1 60.3 69.6 74.7 98.8 91.9 90.2 76.6 59.6 90.7 80.50
0.001 90.8 91.7 88.1 65.6 87.5 74.8 98.7 (0.51) 93.0 (0.21) 93.5 78.2 73.4 95.2 85.48
0.003 90.9 (0.89) 92.8 (0.10) 87.3 (1.28) 66.3 (0.40) 85.6 (15.77) 75.3 (0.29) 98.9 (0.31) 92.5 (0.27) 95.3 (0.10) 79.4 (0.02) 77.3 (0.16) 96.6 (0.17) 86.52
0.01 90.7 (0.63) 92.9 (0.14) 85.9 68.8 56.1 75.2 99.3 92.5 95.8 79.7 78.9 (0.08) 97.4 84.04
0.03 89.8 92.5 83.1 69.7 (0.29) 65.8 75.8 (0.59) 99.3 92.2 96.2 (0.06) 78.9 (2.51) 78.3 97.7 (0.05) 84.69
0.1 88.0 91.2 25.2 22.2 7.6 74.9 98.7 91.0 95.9 73.6 76.8 97.8 69.75
0.3 0.4 1.0 24.5 22.2 7.6 70.5 1.8 2.7 2.3 73.6 0.5 19.6 18.42
DoG - 89.5 (1.26) 92.5 (0.22) 85.0 (0.27) 69.5 (0.11) 67.7 (36.66) 75.5 (0.71) 98.9 (0.25) 92.4 (0.16) 96.4 (0.10) 79.7 (0.01) 77.8 (0.13) 97.7 (0.08) 85.22
L-DoG - 89.6 (0.81) 92.8 (0.15) 86.0 (0.40) 70.7 (0.29) 95.3 (0.08) 75.8 (0.71) 99.0 (0.26) 92.3 (0.45) 96.5 (0.17) 79.8 (0.07) 78.3 (0.26) 97.8 (0.04) 87.82
Table 5: Average (std) test accuracy across seeds for vision tasks, when fine-tuned with different
optimization algorithms and their respective base learning rate when applicable. DoG andL-DoG
userϵ= 10−4(1 +∥x0∥).
42

--- PAGE 43 ---
108
106
104
102
1001011
109
107
105
103
101
t
T5-b on SST-2
108
106
104
102
1001011
109
107
105
103
101
T5-b on STS-B
108
106
104
102
1001010
108
106
104
102
100
T5-b on RTE
108
106
104
102
1001010
108
106
104
102
100
T5-b on MRPC
108
106
104
102
1001011
109
107
105
103
101
T5-b on CoLA
108
106
104
102
1001010
108
106
104
102
100
T5-b on QNLI
108
106
104
102
1001011
109
107
105
103
101
T5-b on MNLI
108
106
104
102
1001011
109
107
105
103
101
T5-b on QQP
108
106
104
102
1001011
109
107
105
103
101
T5-b on SQuAD
108
106
104
102
100109
107
105
103
101
t
RoBERTa-b on SST-2
108
106
104
102
100109
107
105
103
101
RoBERTa-b on STS-B
108
106
104
102
100109
107
105
103
101
RoBERTa-b on RTE
108
106
104
102
100109
107
105
103
101
RoBERTa-b on MRPC
108
106
104
102
100109
107
105
103
101
RoBERTa-b on CoLA
108
106
104
102
100109
107
105
103
101
RoBERTa-b on QNLI
108
106
104
102
1001010
108
106
104
102
100
RoBERTa-b on MNLI
108
106
104
102
100109
107
105
103
101
RoBERTa-b on QQP
108
106
104
102
100109
107
105
103
101
RoBERTa-b on SQuAD
107
105
103
101
101107
105
103
101
101t
ResNet50 on Retinopathy
107
105
103
101
101108
106
104
102
100102
 ResNet50 on dSprites-Ori
107
105
103
101
101108
106
104
102
100102
 ResNet50 on CIFAR-100
107
105
103
101
101108
106
104
102
100102
ResNet50 on SVHN
107
105
103
101
101109
107
105
103
101
101
ResNet50 on Flowers102
107
105
103
101
101108
106
104
102
100102
ResNet50 on DMLab
107
105
103
101
101109
107
105
103
101
101
ResNet50 on Sun397
107
105
103
101
101108
106
104
102
100102
ResNet50 on Pets
107
105
103
101
101109
107
105
103
101
101
ResNet50 on Resisc45
107
105
103
101
101109
107
105
103
101
101
ResNet50 on Caltech101
107
105
103
101
101108
106
104
102
100102
ResNet50 on DTD
107
105
103
101
101
0
107
105
103
101
101t
ViT-B/32 on Retinopathy
107
105
103
101
101
0
107
105
103
101
101
ViT-B/32 on dSprites-Ori
107
105
103
101
101
0
108
106
104
102
100102
 ViT-B/32 on CIFAR-100
107
105
103
101
101
0
107
105
103
101
101
ViT-B/32 on SVHN
107
105
103
101
101
0
107
105
103
101
101
ViT-B/32 on Flowers102
107
105
103
101
101
0
107
105
103
101
101
ViT-B/32 on DMLab
107
105
103
101
101
0
108
106
104
102
100102
ViT-B/32 on Sun397
107
105
103
101
101
0
107
105
103
101
101
ViT-B/32 on Pets
107
105
103
101
101
0
107
105
103
101
101
ViT-B/32 on Resisc45
107
105
103
101
101
0
108
106
104
102
100102
ViT-B/32 on Caltech101
107
105
103
101
101
0
108
106
104
102
100102
ViT-B/32 on DTDt=2
t=10
t=100
t=1000
y=x
Figure 8: Stabilizing behavior of DoG onηtas a function of η0(x-axis) and t(color). In most cases ηtquickly stabilizes around a value
close to the optimal SGD base learning rate (dashed horizontal line) for all sufficiently small η0=rϵ/∥g0∥. The main exceptions (where
ηtdepends strongly on η0) are dSprites-Ori, CIFAR-100 and SVHN when trained with ResNet50; see F.4 for further discussion.
43

--- PAGE 44 ---
F.2 Comparison with equalized compute budget
Throughout the paper, our experiments focus on comparing different methods by the final test
performance they are able to reach given sufficient compute budget to essentially fully converge.
As a consequence, SGD and Adam—which require learning rate tuning—use up significantly more
computation than DoG andL-DoG . More specifically, for each model/task combination we tune
the SGD and Adam learning rates over a grid of at least 5 values (and often 6 or more), resulting
in computational cost increased by the same factor.
In this subsection only, we compare different optimizers using roughly the same computational
budget, by measuring the performance of Adam and SGD after roughly 20% of their overall step
budget.11Figure 9 shows the result of this comparison, contrasting it to our main experiment. The
figure shows that DoG often exceeds the performance of instance-tuned SGD with equalized step
budget.
We note a number of caveats regarding the equalized compute budget comparison:
1. Since our experiments are focused on getting the best possible generalization, we substantially
over-provisioned the iteration budget, and hence the performance of instance-tuned SGD and
Adam declines only mildly when we cut the budget by roughly 5. Our tightest budget was
for RoBERTa (150% the iterations in Liu et al. [54]), and there we can see that performance
degraded more substantially. If we were to instead take the number of iterations DoG actually
needs to reach its peak performance, its advantage over equalized-compute SGD would likely
be far larger.
2. Since the results reported here are obtained by re-analysis of our original experiments, the
cosine learning rate schedule for SGD and Adam is not properly set for using 20% of the
iteration budget; in particular, the learning rate does not decay to zero at the end of the
training. Running these methods with a properly set cosine schedule would likely improve
their performance. However, we note that the addition of iterate averaging appears to par-
tially compensate for the lack of sharp learning rate decay.
3. Given sufficient resources, it is possible to run all the different learning rates of SGD and
Adam in parallel. Therefore, the comparison equalizes overall computational cost, but not
necessarily wall time.
4. The comparison also does not take into account more sophisticated learning rate tuning
schemes that early-stop unpromising learning rate candidates. However, such schemes run
the risk of choosing a suboptimal learning rate.
F.3 Fine-tuning CoLA
As discussed in Section 4.2, DoG with rϵ= 10−6(1 +∥x0∥) failed in fine-tuning T5-b on CoLA.
To investigate this issue, we ran DoG andL-DoG with different choices of rϵ. Figure 11 depicts
the results of this experiment as well as the performance of SGD and Adam with different learning
rates. The figure shows that using lower values of rϵallows DoG to reach reasonable results, but
with some seeds still failing. In contrast, L-DoG shows consistent and superior performance across
a large range of rϵvalues. We leave further investigations on the cause of failure in CoLA to future
work.
11Since these results are just a re-analysis of our original experiments, for language experiments we take all the
warmup iterates plus the first 20% of the remaining iterates, overall using 28% of the budget.
44

--- PAGE 45 ---
RoBERTa-b T5-b ConvNeXt-T Densenet121 ResNet50 VGG11 ViT-B/325.0%
0.0%5.0%10.0%15.0%20.0%Median Relative Error Difference
0.86 %
-0.05 %
0.30 %
2.73 %
2.21 %
0.34 %
0.45 %1.56 %
0.57 %
2.29 %
3.51 %
6.68 %
1.32 %
4.54 %SGD (model tuned LR) SGD (instance tuned LR)
RoBERTa-b T5-b ConvNeXt-T Densenet121 ResNet50 VGG11 ViT-B/325.0%
0.0%5.0%10.0%15.0%20.0%Median Relative Error Difference
4.87 %
4.00 %
7.09 %
3.58 %
12.33 %
3.43 %
0.28 %6.35 %
4.00 %
12.13 %
5.69 %
12.33 %
6.94 %
4.49 %1.57 %
3.50 %
4.01 %
-0.57 %
7.84 %
0.24 %
2.96 %Adam (model tuned LR) Adam (instance tuned LR) L-DoG(a) SGD/Adam have >5x the DoG /L-DoG budget.
RoBERTa-b T5-b ConvNeXt-T Densenet121 ResNet50 VGG11 ViT-B/3210.0%
0.0%10.0%20.0%30.0%Median Relative Error Difference
-9.29 %
-2.63 %
-0.46 %
0.89 %
-4.40 %
-1.58 %
-1.68 %-9.25 %
-1.13 %
-0.09 %
1.73 %
0.59 %
-0.79 %
-0.03 %SGD (model tuned LR) SGD (instance tuned LR)
RoBERTa-b T5-b ConvNeXt-T Densenet121 ResNet50 VGG11 ViT-B/3210.0%
0.0%10.0%20.0%30.0%Median Relative Error Difference
3.55 %
2.06 %
7.09 %
0.62 %
6.70 %
2.74 %
-0.66 %3.55 %
3.19 %
12.13 %
3.36 %
8.50 %
6.48 %
1.26 %1.57 %
3.50 %
4.01 %
-0.57 %
7.84 %
0.24 %
2.96 %Adam (model tuned LR) Adam (instance tuned LR) L-DoG (b) SGD/Adam have roughly the same budget as
DoG /L-DoG
Figure 9: RED median (bar chart) and IQR (error bars) of each model on the set of applicable
tasks, where we either (a)give the same iteration budget for each optimizer run, resulting in SGD
and Adam using more than 5x total compute than DoG andL-DoG due to learning rate tuning
(this is a reproduction of Figure 3), or (b)give each algorithm roughly equal compute budget by
running SGD and Adam (with 5 or more learning rates) for roughly 20% of the steps that DoG
andL-DoG use. With equalized compute budget, DoG outperforms model-tuned SGD almost
always and often outperforms the instance-tuned SGD as well, while L-DoG closes most of the
gap to Adam.
F.4 Sensitivity of DoG to rϵand the effect of batch normalization
In Section 4.3, we discuss DoG ’s insensitivity to the choice of rϵas long as it is small enough.
Here, we expand on this analysis by testing how the DoG step size at iteration t, denoted ηt,
depends on its initial step size η0=rϵ/∥g0∥. For each task in our testbed and for 4 models, we
perform short training runs with a large number of η0values. In Figure 8 we plot ηtvs.η0for
t∈ {2,10,100,1000}. We also show a horizontal line for the learning rate of of SGD reaching
the best validation accuracy, and the y=xdiagonal. The figure shows that for most model/task
combinations, ηtconverges quickly (within the first 1000 steps) to a value near the optimal one for
SGD, and mostly independent of η0as long as it is small enough.
However, we also observe some failure cases where ηtstrongly depends on η0, such as fine-
tuning ResNet50 on CIFAR-100. This provides a complementary perspective on the fact DoG is
sensitive to rϵin this setting, as already shown in Figure 3: when η0is to low, DoG fails to reach a
suitable value of ηtin a reasonable time. We hypothesize that this is due to the batch normalization
(BN) layers in the model causing many different step size to “look” like solutions to the implicit
equation motivating DoG . To test this hypothesis, we repeat the CIFAR-100 training experiment
but without BN (we disable BN by fine-tuning the model in evaluation mode). Figure 10(a) shows
that removing BN allows DoG to recover its stabilizing behavior. Moreover, Figure 10(b) further
shows that without batch normalization, the performance of DoG again becomes insensitive to
the choice of rϵprovided it is sufficiently small. Unsurprisingly, we also observe that removing BN
slightly hurts generalization performance in this task. As mentioned in Section 6, improving DoG
45

--- PAGE 46 ---
107
105
103
101
101
0
1011
109
107
105
103
101
101t
ResNet50 w/o BN on CIFAR-100
107
105
103
101
101
0
108
106
104
102
100102
 ResNet50 w/ BN on CIFAR-100
t=2
t=10
t=100
t=1000
y=x(a)
109
107
105
103
101
r/(1+x0)
0.00.20.40.60.8Test accuracy
Without BN
With BN (b)
Figure 10: ResNet50 fine-tuned on CIFAR-100 with and without batch normalization. The dashed
horizontal line indicates the best SGD learning rate. (a)Stabilizing behavior of DoG onηtas a
function of η0(x-axis) and t(color). Turning off batch normalization (left) mitigates the sensitivity
ofηttoη0observed in batch normalized model (right). (b)Accuracies of models trained with DoG
(for 20K steps) as a function of rϵ. Without batch normalization, DoG is robust to smaller values
ofrϵ.
to be more robust in the presence of normalization layers in general and batch normalization in
particular is an important direction for future research.
F.5 The growth rate of ¯rt
Figure 12 plots ¯ rtforDoG as a function of the iteration index t. As the figure shows, ¯ rtgrows very
rapidly and then approximately plateaus. Therefore, the quantityP
i≤t¯ri
¯rtgrows roughly linearly
int, implying a near-optimal rate of convergence for DoG , as discussed in Section 3.2.
G Comparison to Other Tuning-Free Methods
Section 4.7 discusses a comparison between DoG and other parameter-free optimizers. In this
section, we provide further details on the experiments.
G.1 Parameter-free SGD
Carmon and Hinder [11] propose a bisection procedure for tuning the SGD learning rate. A direct
implementation of this method would need to perform at least 4 or 5 bisection steps and therefore,
in the best case , perform similarly to our instance-tuned SGD baseline. Since our learning rate
tuning employs a tight grid of values selected using some prior knowledge of the problem, and
since we select learning rates based on validation set performance and not a step size certificate,
instance-tuned SGD is likely a conservative upper bound on the performance of bisection approach.
Similar to instance tuned SGD, the bisection procedure has increased computational cost relative
toDoG that is due to the need for multiple SGD runs. That is, performing 5 steps of bisection
where each SGD call has the same step budget as DoG consumes 5 times more compute than DoG .
46

--- PAGE 47 ---
105
104
103
102
101
100
Learning rate0.200.250.300.350.400.450.500.550.600.65Evaluation MCC on CoLA
1013
1011
109
107
r/(1+xo)
SGD
Adam
DoG
L-DoGFigure 11: Matthews correlation of T5-base fine-tuned on CoLA with SGD and Adam with different
base learning rates (bottom axis), as well as with DoG andL-DoG with different rϵ(top axis).
Only L-DoG and Adam perform consistently well across different parameters. The lines and shaded
regions show the average Matthews correlation and the min-max range, respectively, computed over
3 seeds.
We may also consider a situation where each bisection step uses only 20% of the DoG compute
budget, leading to equal overall cost. In this setting, the “equalized compute budget” comparison
we perform in Appendix F.2 and Figure 9 provides a conservative upper bound on the bisection
performance, indicating it is likely to under-perform DoG .
G.2 Stochastic Polyak step-size
We apply the Stochastic Polyak Step (SPS) proposed by Loizou et al. [56] using their open-source
implementation12to a subset of our fine-tuning testbed, and present the results in Tables 6 and 7.
For the vision experiments, the SPS with the hyper-parameters proposed in the paper ( c= 0.2,
τ= 2) and initial step size of 1.0 (the default in the code) worked reasonably well, but not as well
as DoG. For the language experiments the same algorithm diverges; we find initial learning rate of
0.01 worked reasonably well, but again not as well as DoG (we also attempted an initial learning
rate of 0.0001, which produced worse results). For vision tasks, similarly tuning the initial step size
did not significantly improve performance. We run 5 random seeds per experiment, and average
the results across seeds. DoG outperforms SPS in 22 out of 34 task/model combinations, and by
2.3 percentage points on average. L-DoG further increases this gap by outperforming SPS in 24
out of 34 pairs, with an average of 5.3 percentage points.
G.3 D-adaptation
Empirical comparison to D-adapt SGD and Adam. We perform a preliminary empirical
evaluation of the practical algorithms proposed in Defazio and Mishchenko [25] using the code they
release13and a subset of our fine-tuning testbed. As Tables 6 and 7 show, D-adapt SGD and and
D-adapt Adam perform reasonably well but slightly worse than DoG , and noticeably worse than
12https://github.com/IssamLaradji/sps
13https://github.com/facebookresearch/dadaptation
47

--- PAGE 48 ---
102
101
100101102rt
CIFAR-100
102
101
100101102Caltech101
102
101
100101Clevr-Dist
102
101
100101102 DMLab
102
101
100101rt
DTD
102
101
100101102 Flowers102
102
101
100101102Pets
102
101
100101102Resisc45
0 5000 10000 15000 20000
t102
101
100101rt
Retinopathy
0 5000 10000 15000 20000
t102
101
100101SVHN
0 5000 10000 15000 20000
t102
101
100101102Sun397
0 5000 10000 15000 20000
t102
101
100101dSprites-Ori
ResNet50 ViT-B/32 Densenet121 ConvNeXt-T VGG11Figure 12: The quantity ¯ rt= max i≤t∥xi−x0∥as a function of the number of steps tin our
computer vision testbed. The value of ¯ rtgrows rapidly at first and then almost plateaus.
L-DoG and Adam. DoG outperforms D-adapt SGD in 24 out of 34 task/model combinations,
and by 1.9 percentage points on average. L-DoG further increases this gap by outperforming D-
adapt SGD in 30 out of 34 pairs, with an average of 4.9 percentage points. D-adapt Adam is less
stable on many of the tasks in our testbed, being outperformed by DoG in 26 out of 34 task/model
combinations, and by L-DoG in 27, with an average of 10.8 and 13.8 percentage points respectively.
Theoretical comparison to Algorithm 1 of Defazio and Mishchenko [25]. Defazio and
Mishchenko [25] carry out their main theoretical analysis on a “Parameter Free Dual Averaging”
(PFDA) method. We now provide some additional remarks comparing PFDA and DoG . The
iterate xtin PFDA is
xt=x0−1√GtX
i≤tqigi
where Gt=P
i≤t∥gi∥2andqiis a lower bound on the distance to optimality (denoted diin [25]).
In contrast, the DoG iterates are
xt=x0−X
i≤t¯ri√Gigi
48

--- PAGE 49 ---
Model Optimizer CoLA MRPC QNLI RTE SQuAD SST-2 Avg.
RoBERTa-bD-Adapt (Adam) 0.0(0.00) 83.4 (4.93) 66.7 (22.13) 68.4 (14.40) 7.0(0.16) 58.2 (17.88) 47.28
D-Adapt (SGD) 49.4 (27.59) 91.6 (0.43) 91.9 (0.67) 81.4 (1.04) 91.5 (0.13) 94.1 (0.38) 83.32
SPS 49.8 (6.79) 88.4 (3.21) 91.8 (0.21) 52.7 90.1 (0.10) 94.1 (0.28) 77.70
COCOB 0.0(0.00) 81.2 (0.00) 92.5 (0.33) 52.7 (0.00) 1.1(0.00) 92.0 (1.19) 53.25
DoG 62.8 (1.17) 91.6 (0.29) 92.6 (0.15) 78.5 (2.91) 91.3 (0.17) 94.9 (0.26) 85.28
L-DoG 63.3 (0.32) 91.5 (0.19) 92.8 (0.28) 80.1 (1.00) 91.8 (0.18) 94.8 (0.54) 85.72
T5-bD-Adapt (Adam) 11.1 (2.51) 81.8 (0.65) 85.0 (1.91) 58.1 (1.35) 90.4 (0.06) 87.1 (3.00) 68.92
D-Adapt (SGD) 0.0(0.00) 92.5 (0.59) 92.9 (0.04) 81.2 (0.78) 90.4 (0.06) 80.3 (2.67) 72.88
SPS 39.1 (2.35) 92.9 (1.21) 93.2 (0.10) 80.9 (1.82) 90.5 (0.04) 94.3 (0.31) 81.82
COCOB 59.2 (1.02) 86.2 (2.93) 93.4 (0.12) 83.5 (1.66) 90.5 (0.05) 94.8 (0.35) 84.60
DoG 7.3(6.78) 92.8 (0.35) 93.1 (0.09) 81.7 (3.06) 90.6 (0.05) 94.1 (0.19) 76.60
L-DoG 59.9 (1.43) 91.9 (0.32) 93.6 (0.02) 83.1 (0.78) 90.3 (0.02) 95.0 (0.19) 85.63
Table 6: Average (std) performance of RoBERTa-b and T5-b on language tasks, when fine-tuned
with different optimization algorithms. DoG uses rϵ= 10−6(1 +∥x0∥) and L-DoG uses rϵ=
10−8(1 +∥x0∥). SPS uses c= 0.2,τ= 2 as recommended by Loizou et al. [56], but initial step size
of 0.01 as the recommended value 1.0 diverged for some tasks. Still, when fine-tuning RoBERTa-b
on RTE, 3 out of 5 training runs diverged; for this case we report the mean of the two successful
runs and omit the standard deviation. For COCOB we set αto be 10% of the total steps per
task, as the default of α= 100 failed to outperform a random guess. We measure performance as
detailed in Table 3.
where ¯ rt= max i≤t∥xi−x0∥. While both dtin PFDA and ¯ rtare lower bounds for (a constant
factor times) the distance to optimality, only DoG aims to approximate η⋆=∥x0−x⋆∥√Gt; PFDA
instead approximates the optimal step size for dual averaging.
The dual averaging prescription of putting the factor 1 /√Gtoutside the summation defining
xtlikely hurts performance in practice. The practical D-adapt SGD and D-adapt Adam methods
that Defazio and Mishchenko [25] propose do not follow this prescription. Consequently, these
algorithms are very different from PFDA and have no theoretical guarantees.
G.4 Continuous Coin Betting
Orabona and Tommasi [71] propose a parameter-free algorithm based on coin-betting schemes and
demonstrate its effectiveness in training neural networks. We use an open source implementation14
to apply COCOB on the same subset of our testbed as the other experiments in this section, and
present the results in Tables 6 and 7. While the default parameters work well for ResNet50 (slightly
better than DoG and similar to L-DoG ), they produced poor results on the other models. We
found that increasing the αparameter (which roughly corresponds to the number of “warmup”
steps) from a constant 100 to 10% of the step budget improves the results dramatically for all
transformer-based models, though in most cases DoG continues to significantly outperform it.
Additionally, we find that in almost all cases, our averaging scheme benefits COCOB.
14https://github.com/bremen79/parameterfree
49

--- PAGE 50 ---
Model Optimizer Caltech101 CIFAR-100 Clevr-Dist DMLab dSprites-Ori DTD Flowers102 Pets Resisc45 Retinopathy Sun397 SVHN Avg.
ResNet50D-Adapt (Adam) 79.0 (1.60) 82.9 (0.23) 92.3 (0.50) 75.2 (0.97) 96.3 (0.01) 60.1 (0.51) 84.6 (1.73) 85.1 (0.95) 94.1 (0.28) 83.4 (0.07) 72.9 (0.11) 97.0 (0.21) 83.58
D-Adapt (SGD) 86.2 (0.93) 84.3 (0.15) 89.6 (0.60) 74.8 (0.73) 95.9 (0.04) 64.4 (1.31) 84.9 (1.12) 92.0 (0.25) 94.7 (0.17) 82.7 (0.12) 73.0 (0.09) 97.1 (0.06) 84.97
SPS 86.4 (1.14) 80.2 (1.20) 92.9 (0.25) 76.2 (0.60) 96.0 (0.05) 63.7 (0.83) 84.3 (1.45) 92.4 (0.19) 94.9 (0.22) 83.3 (0.17) 70.9 (0.82) 97.5 (0.03) 84.89
COCOB 86.3 (1.53) 83.5 (0.21) 87.4 (0.43) 68.4 (0.72) 95.8 (0.03) 66.4 (0.31) 87.0 (1.11) 92.4 (0.35) 93.8 (0.18) 82.0 (0.10) 72.2 (0.16) 96.8 (0.07) 84.33
DoG 86.8 (0.62) 84.8 (0.37) 89.3 (0.58) 71.4 (0.77) 95.7 (0.09) 66.4 (1.48) 85.8 (2.64) 92.9 (0.38) 94.6 (0.33) 82.6 (0.16) 73.5 (0.41) 96.5 (0.10) 85.02
L-DoG 87.6 (1.15) 84.6 (0.38) 90.8 (0.38) 75.0 (0.44) 96.0 (0.05) 69.1 (1.30) 90.2 (2.02) 92.4 (0.36) 95.6 (0.46) 82.2 (0.15) 74.6 (0.32) 97.4 (0.08) 86.29
ViT-B/32D-Adapt (Adam) 87.6 (0.48) 91.8 (0.31) 89.0 (0.42) 70.7 (0.48) 7.6(0.00) 69.4 (1.55) 94.7 (0.92) 86.6 (1.42) 95.5 (0.32) 80.1 (0.09) 76.2 (0.38) 97.9 (0.05) 78.92
D-Adapt (SGD) 88.7 (0.58) 91.8 (0.20) 84.2 (2.39) 70.5 (0.17) 44.0 (31.87) 74.9 (0.38) 98.2 (0.43) 91.5 (0.72) 96.3 (0.06) 79.8 (0.05) 76.3 (0.13) 97.7 (0.03) 82.82
SPS 89.7 (0.51) 91.4 (0.18) 84.6 (1.18) 71.3 (0.27) 7.6(0.00) 74.4 (0.96) 98.4 (0.33) 91.1 (0.49) 96.2 (0.17) 79.9 (0.09) 77.5 (0.46) 98.0 (0.04) 80.01
COCOB 89.2 (1.01) 91.5 (0.19) 84.9 (0.88) 70.7 (0.70) 7.6(0.00) 74.7 (0.92) 98.4 (0.48) 90.8 (0.26) 96.5 (0.12) 80.0 (0.15) 77.1 (0.22) 97.9 (0.07) 79.94
DoG 89.5 (1.26) 92.5 (0.22) 85.0 (0.27) 69.5 (0.11) 67.7 (36.66) 75.5 (0.71) 98.9 (0.25) 92.4 (0.16) 96.4 (0.10) 79.7 (0.01) 77.8 (0.13) 97.7 (0.08) 85.22
L-DoG 89.6 (0.81) 92.8 (0.15) 86.0 (0.40) 70.7 (0.29) 95.3 (0.08) 75.8 (0.71) 99.0 (0.26) 92.3 (0.45) 96.5 (0.17) 79.8 (0.07) 78.3 (0.26) 97.8 (0.04) 87.82
Table 7: Average (std) test accuracy across seeds for vision tasks, when fine-tuned with different
optimization algorithms. DoG andL-DoG userϵ= 10−4(1 +∥x0∥). SPS uses c= 0.2,τ= 2 and
initial step size of 1 as recommended by Loizou et al. [56]. For COCOB we set αto be 10% of the
total steps per task; the default value of α= 100 performed well on ResNet50 but very badly on
ViT-B/32.
50
