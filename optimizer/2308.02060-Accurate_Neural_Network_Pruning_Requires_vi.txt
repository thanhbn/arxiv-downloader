# Tỉa Neural Network chính xác đòi hỏi
phải suy nghĩ lại về tối ưu hóa thưa thớt

Denis Kuznedelev∗
Skoltech & Yandex
denis.kuznedelev@skoltech.ru

Eldar Kurtic∗
IST Austria
eldar.kurtic@ista.ac.at

Eugenia Iofinova∗
IST Austria
eugenia.iofinova@ista.ac.at

Elias Frantar
IST Austria
elias.frantar@ista.ac.at

Alexandra Peste
IST Austria
alexandra.peste@ista.ac.at

Dan Alistarh
IST Austria & Neural Magic
dan.alistarh@ista.ac.at

Tóm tắt

Việc thu được các phiên bản mạng nơ-ron sâu vừa có độ chính xác cao vừa thưa thớt cao là một trong những thách thức chính trong lĩnh vực nén mô hình, và một số kỹ thuật tỉa hiệu suất cao đã được cộng đồng nghiên cứu. Tuy nhiên, ít được biết đến hơn về tương tác giữa độ thưa thớt và các kỹ thuật tối ưu hóa ngẫu nhiên tiêu chuẩn được sử dụng để huấn luyện mạng thưa thớt, và hầu hết các công trình hiện có sử dụng lịch trình dày đặc tiêu chuẩn và siêu tham số cho việc huấn luyện mạng thưa thớt. Trong công trình này, chúng tôi kiểm tra tác động của độ thưa thớt cao đối với việc huấn luyện mô hình sử dụng các điểm chuẩn thưa thớt tiêu chuẩn của thị giác máy tính và xử lý ngôn ngữ tự nhiên. Chúng tôi bắt đầu bằng cách chỉ ra rằng việc sử dụng các công thức huấn luyện dày đặc tiêu chuẩn cho huấn luyện thưa thớt là không tối ưu, và dẫn đến việc huấn luyện thiếu. Chúng tôi cung cấp các phương pháp mới để giảm thiểu vấn đề này cho cả huấn luyện trước thưa thớt của mô hình thị giác (ví dụ: ResNet50/ImageNet) và tinh chỉnh thưa thớt của mô hình ngôn ngữ (ví dụ: BERT/GLUE), đạt được kết quả tiên tiến trong cả hai thiết lập ở chế độ thưa thớt cao, và cung cấp phân tích chi tiết về khó khăn của việc huấn luyện thưa thớt trong cả hai kịch bản. Công trình của chúng tôi thiết lập một ngưỡng mới về mặt độ chính xác có thể đạt được dưới độ thưa thớt cao, và sẽ truyền cảm hứng cho các nghiên cứu tiếp theo nhằm cải thiện việc huấn luyện mô hình thưa thớt, để đạt được độ chính xác cao hơn dưới độ thưa thớt cao, mà còn làm điều đó một cách hiệu quả.

1 Giới thiệu

Khó khăn trong việc tìm ra các mạng nơ-ron sâu (DNN) vừa chính xác vừa thưa thớt, tức là khớp chặt chẽ với độ chính xác của mô hình dày đặc trong khi có phần lớn trọng số được đặt bằng không, là một trong những thách thức chính trong lĩnh vực nén mô hình. Về mặt khái niệm, thách thức này kết nối với các câu hỏi cơ bản liên quan đến Giả thuyết Vé số may mắn (LTH) [18,19], đã đưa ra giả thuyết rằng các mặt nạ thưa thớt như vậy tồn tại, và trong một số trường hợp, chúng thậm chí có thể cho phép huấn luyện chính xác các mô hình thưa thớt từ đầu, tức là áp dụng mặt nạ thưa thớt ngay từ khởi tạo. Về mặt thực tế, việc thu được các mạng có độ thưa thớt cao và chính xác có thể dẫn đến việc tăng tốc đáng kể về mặt thực tế, cả cho suy luận [56] và huấn luyện [57].

Trong công trình này, chúng tôi tập trung vào thách thức thu được các DNN chính xác trong chế độ thưa thớt cao, và điều tra các rào cản để thu được các biến thể có độ thưa thớt cao và độ chính xác cao của DNN cho các nhiệm vụ thị giác và ngôn ngữ tiêu chuẩn. Chúng tôi chủ yếu tập trung vào hai nhiệm vụ có thể nói là các điểm chuẩn tiêu chuẩn cho độ thưa thớt trong thị giác và ngôn ngữ, tương ứng: phân loại hình ảnh sử dụng mô hình ResNet50 [25] trên bộ dữ liệu ImageNet-1K [61], ví dụ [27,14,21,16,67,65,60], và mô hình hóa ngôn ngữ sử dụng mô hình BERT-base [12] trên các bộ dữ liệu điểm chuẩn GLUE [73], ví dụ [64,27,42,43]. Đại khái, đối với cả hai điểm chuẩn, người ta biết rằng độ thưa thớt thấp hơn 90% có thể đạt được với độ mất mát độ chính xác khoảng 1% so với mô hình dày đặc gốc, nhưng độ chính xác giảm nhanh chóng trong phạm vi 90-95% [27,16], và việc giảm là nghiêm trọng ở độ thưa thớt cao hơn (≥95%) [67,43]. Trong bài báo này, chúng tôi điều tra các lý do đằng sau việc mất mát độ chính xác này do độ thưa thớt, chủ yếu nhắm đến độ thưa thớt cao, tức là độ thưa thớt từ 90% đến 99%, nghiên cứu khó khăn của việc thu được mô hình chính xác trong phạm vi này.

Đóng góp. Chúng tôi bắt đầu từ quan sát rằng, khi huấn luyện mô hình thưa thớt từ đầu, theo lịch trình huấn luyện dày đặc tiêu chuẩn, các mô hình thưa thớt cho thấy bằng chứng rõ ràng về việc huấn luyện thiếu: cả độ chính xác và mất mát của chúng đều không bão hòa, và đầu ra của chúng tiếp tục có entropy cao. Phát hiện này gợi ý rằng việc tối đa hóa độ chính xác của mô hình thưa thớt đòi hỏi thời gian huấn luyện dài hơn so với các công thức tối ưu hóa dày đặc được áp dụng trong hầu hết các công trình về việc làm thưa thớt mô hình.

Được thúc đẩy bởi quan sát này, chúng tôi đề xuất sự kết hợp các kỹ thuật có thể giảm thiểu khó khăn vốn có của việc huấn luyện thưa thớt. Kết quả là, chúng tôi cải thiện đáng kể so với những thỏa thuận tốt nhất hiện biết về độ thưa thớt-độ chính xác trên các điểm chuẩn thưa thớt tiêu chuẩn cho cả phân loại hình ảnh và mô hình hóa ngôn ngữ. Cụ thể hơn, chúng tôi thu được, lần đầu tiên, các phiên bản thưa thớt có độ chính xác cao của ResNet50, chẳng hạn như mô hình thưa thớt 90% với độ chính xác Top-1 78.5%, mô hình thưa thớt 95% với độ chính xác Top-1 77.7%, và mô hình thưa thớt 98% với độ chính xác Top-1 75.2%. Ngoài ra, chúng tôi chỉ ra rằng kết quả ổn định có thể thu được ngay cả đối với độ thưa thớt cực đoan (ví dụ: 99%). Đối với mô hình ngôn ngữ, chúng tôi chỉ ra rằng trên các nhiệm vụ thách thức nhất, được đo bằng sự sụt giảm độ chính xác so với mô hình dày đặc, chúng tôi có thể cải thiện kết quả 3 điểm độ chính xác so với kết quả hiện tại tiên tiến nhất ở độ thưa thớt 90%. Chúng tôi đạt được những kết quả này như sau:

• Chúng tôi thực hiện phân tích về đầu ra và đặc điểm huấn luyện của các mô hình được huấn luyện sử dụng các kỹ thuật hiện tại tiên tiến, so với các đối tác dày đặc của chúng. Trước tiên, chúng tôi chỉ ra rằng các DNN thưa thớt thu được thông qua nhiều kỹ thuật hiện tại hoạt động tương tự như mô hình dày đặc huấn luyện thiếu: cụ thể, chúng có xu hướng có entropy đầu ra cao (hoặc "độ tin cậy đầu ra" thấp), điều này tương quan với độ chính xác giảm của chúng.

• Phân tích này cung cấp bằng chứng rõ ràng rằng việc tối ưu hóa mô hình thưa thớt khó khăn hơn so với tối ưu hóa dày đặc tiêu chuẩn [15]. Quan sát này trái ngược với thực tế là hầu hết các kỹ thuật làm thưa thớt hiện tại sử dụng công thức huấn luyện dày đặc tiêu chuẩn cho việc tinh chỉnh và phục hồi. Chúng tôi khai thác hiểu biết này để thu được độ chính xác tiên tiến cho mô hình thưa thớt trong hai kịch bản phổ biến: huấn luyện trước thưa thớt, tức là huấn luyện mô hình thưa thớt từ đầu, và chuyển giao thưa thớt, tức là tối ưu hóa mô hình được huấn luyện trước thưa thớt vào nhiệm vụ chuyển giao mục tiêu.

• Trong kịch bản huấn luyện trước thưa thớt, được minh họa bởi nhiệm vụ tiêu chuẩn thu được mô hình ResNet50 có độ thưa thớt cao trên bộ dữ liệu ImageNet, chúng tôi chỉ ra rằng chúng ta có thể phá vỡ khó khăn của việc huấn luyện thưa thớt bằng cách áp dụng một biến thể của thuật toán Nén/Giải nén Xen kẽ (AC/DC) [60] để huấn luyện DNN thưa thớt, thuật toán này có đảm bảo cho việc phục hồi thưa thớt. Cụ thể, chúng tôi chỉ ra rằng, bằng cách mở rộng thời gian chạy của thuật toán, chúng tôi có thể thu được kết quả tiên tiến cho việc huấn luyện trước thưa thớt trên ImageNet cho mô hình ResNet50 và MobileNet, và đạt được độ thưa thớt cực cao (ví dụ: 98% và 99%) trong khi vẫn thu được kết quả ổn định. Hơn nữa, chỉ có mô hình thưa thớt hưởng lợi từ việc huấn luyện kéo dài, trong khi mô hình dày đặc bắt đầu quá khớp với việc huấn luyện dài hơn.

• Trong kịch bản chuyển giao thưa thớt, phổ biến trong lĩnh vực ngôn ngữ, khó khăn của việc huấn luyện thưa thớt có thể biểu hiện bản thân thông qua cả huấn luyện thiếu và quá khớp, tùy thuộc vào tham số hóa của công thức học chuyển giao đã chọn, cụ thể là độ dài huấn luyện. Chúng tôi giải quyết điều này thông qua phiên bản sửa đổi của phương pháp mở khóa tầng từ từ [31], được điều chỉnh cho kịch bản học chuyển giao thưa thớt, cho phép chúng tôi thu được kết quả tiên tiến trong trường hợp chuyển giao BERT-base trên các bộ dữ liệu hạ lưu.

Thảo luận. Nhìn chung, kết quả của chúng tôi gợi ý rằng khó khăn trong việc thu được mô hình thưa thớt có độ chính xác cao có liên quan chặt chẽ đến khó khăn của việc tối ưu hóa thưa thớt chính xác bằng cách sử dụng các kỹ thuật hiện tại tiên tiến. Cụ thể, công trình của chúng tôi cải thiện các kết quả tốt nhất đã biết trên các điểm chuẩn thưa thớt tiêu chuẩn, cho cả huấn luyện trước thưa thớt và tinh chỉnh thưa thớt, cả về độ chính xác tuyệt đối và mất mát độ chính xác so với đường cơ sở dày đặc. Hơn nữa, chúng tôi quan sát thấy những điều sau:

• Việc đạt được thỏa thuận độ thưa thớt-độ chính xác tiên tiến hiện tại đòi hỏi sử dụng độ phức tạp tính toán bổ sung đáng kể và nhiều epoch hơn để huấn luyện mô hình thưa thớt, so với các phương pháp huấn luyện dày đặc tốt nhất đã biết. Đến lượt nó, điều này gợi ý rằng tối ưu hóa thưa thớt có thể khó khăn hơn bản chất so với đối tác dày đặc của nó.

• Việc đạt được độ chính xác xác thực cao cho mô hình thưa thớt có liên quan mạnh mẽ đến việc đạt được mất mát huấn luyện thấp, điều này xảy ra với tốc độ chậm hơn đối với mô hình thưa thớt trong trường hợp tối ưu hóa dựa trên SGD. Cùng lúc, chúng tôi quan sát thấy hành vi quá khớp (giảm độ chính xác xác thực w.r.t. tăng thời gian huấn luyện), đặc biệt là ở độ thưa thớt thấp hơn.

• Để điều tra thêm về độ khó của tối ưu hóa thưa thớt, chúng tôi thực hiện phân tích về cảnh quan mất mát của mạng thưa thớt chính xác cả về mặt độ sắc nét và nội suy mất mát / kết nối chế độ. Chúng tôi quan sát thấy rằng việc đạt được mạng thưa thớt có độ chính xác cao từ khởi tạo đòi hỏi phải vượt qua nhiều rào cản mất mát, và việc khám phá mặt nạ thưa thớt có thể là thành phần quan trọng để vượt qua những rào cản này.

• Ngoài ra, chúng tôi điều tra mối quan hệ giữa các siêu tham số tiêu chuẩn như suy giảm trọng số, một mặt, và cấu trúc thưa thớt, mặt khác. Chúng tôi thấy rằng việc thiết lập cẩn thận suy giảm trọng số là quan trọng cho độ thưa thớt chính xác, và suy giảm trọng số cũng tạo ra độ thưa thớt có cấu trúc (một phần) trong mô hình có độ thưa thớt cao. Điều này cung cấp lời giải thích đầu tiên cho sự xuất hiện của độ thưa thớt có cấu trúc trong mạng thưa thớt không có cấu trúc, điều này đã được quan sát trước đây [60,33,77].

Kết quả của chúng tôi thiết lập ngưỡng độ chính xác mới cho mô hình thưa thớt bằng cách sử dụng các kỹ thuật tương đối đơn giản. Chúng nên phục vụ như động lực cho cộng đồng nghiên cứu để đưa ra các kỹ thuật tối ưu hóa nhận biết độ thưa thớt được cải thiện, cụ thể cho phép phục hồi độ chính xác nhanh hơn, hiệu quả hơn.

2 Công trình liên quan

Mục tiêu của hầu hết các phương pháp làm thưa thớt [27] là tạo ra một DNN chính xác nhất có thể, đồng thời tối đa hóa độ thưa thớt. Mục tiêu này có thể đạt được thông qua các chiến lược khác nhau: ví dụ, các phương pháp làm thưa thớt sau huấn luyện giả định một mô hình dày đặc được huấn luyện trước, từ đó các trọng số được loại bỏ trong một bước duy nhất (một lần) hoặc từ từ (tỉa từ từ). Ngược lại, trong các phương pháp huấn luyện thưa thớt, các tham số được tỉa khỏi mô hình trong quá trình huấn luyện từ đầu, hoặc gần khởi tạo [16,37,46,70,66], hoặc từ từ khi mô hình được huấn luyện [24,21,65]. Một tập con của các phương pháp huấn luyện thưa thớt là động, theo nghĩa là các trọng số có thể được đưa trở lại trong quá trình huấn luyện [16, 60].

Trong công trình này, chúng tôi chủ yếu tập trung vào chế độ thưa thớt cao, trong đó các phương pháp huấn luyện thưa thớt cung cấp thỏa thuận độ chính xác-độ thưa thớt tốt nhất đã biết. Chúng tôi bắt đầu bằng việc thảo luận các phương pháp cho thị giác máy tính. Ở đây, Tỉa Độ lớn Từ từ (GMP), trong đó các trọng số có độ lớn thấp nhất được loại bỏ từ từ trong suốt quá trình huấn luyện, là một đường cơ sở phổ biến. Trong [21], GMP đã được chỉ ra là cạnh tranh với các phương pháp tỉa tinh vi hơn trên các mô hình phân loại hình ảnh khi được điều chỉnh đúng cách; kết quả tương tự sau đó được chỉ ra cho mô hình ngôn ngữ [42].

Phương pháp tỉa RigL [16] là một điểm chuẩn phổ biến, hiệu suất cao cho huấn luyện thưa thớt động. Trong phương pháp này, các trọng số ban đầu được tỉa đến độ thưa thớt mục tiêu và được huấn luyện thông qua gradient descent ngẫu nhiên (thưa thớt). Tuy nhiên, định kỳ, mặt nạ được cập nhật bằng cách chọn các trọng số có gradient có độ lớn cao nhất, phụ thuộc vào giới hạn về tổng thay đổi mặt nạ. Các tác giả chạy phương pháp này sử dụng hai mục tiêu thưa thớt - độ thưa thớt Đồng nhất, nơi tất cả các lớp (ngoại trừ lớp đầu tiên và cuối cùng) được tỉa theo cùng một tỷ lệ, và Erdős–Rényi Kernel (ERK), nơi mục tiêu thưa thớt lớp được thiết lập để tối ưu hóa hiệu suất. Các tác giả kiểm tra phương pháp của họ trong chế độ lịch trình bình thường (100 epoch trên Imagenet) và chế độ huấn luyện 5x, nhận được kết quả 73.22% độ chính xác xác thực và 74.63% độ chính xác xác thực ở độ thưa thớt toàn cục (ERK) và đồng nhất 95% tương ứng khi huấn luyện trong 500 epoch. Mở rộng huấn luyện đến 10 000 epoch (100x) tiếp tục cho phép các tác giả tạo ra mô hình ResNet50 thưa thớt 99% (ERK) với độ chính xác 68.5% trên ImageNet. RigL đã được cải thiện bằng cách kết hợp với ITOP [49], bằng cách thay đổi siêu tham số huấn luyện để khuyến khích khám phá mặt nạ, điều này đã được chỉ ra là cải thiện kết quả RigL ở độ thưa thớt trung bình (80-90%) (xem Bảng 1).

Phương pháp GraNet [50] mở rộng phương pháp này bằng cách làm cho nó từ từ - hoặc bắt đầu từ một mạng dày đặc và thực hiện các cập nhật giống RigL đồng thời tăng độ thưa thớt cho đến khi đạt được độ thưa thớt mục tiêu, hoặc bằng cách bắt đầu bằng một mạng thưa thớt một phần (50%) và làm điều tương tự. Các mô hình được huấn luyện với phiên bản khởi tạo thưa thớt của GraNet đạt được độ chính xác xác thực 72.3% ở độ thưa thớt toàn cục 95% khi huấn luyện trong 100 epoch.

Phương pháp tỉa AC/DC [60] xen kẽ các giai đoạn tỉa dày đặc và thưa thớt của vài epoch mỗi giai đoạn, hiệu quả là đồng huấn luyện các mô hình dày đặc và thưa thớt. Tương tự như RigL, AC/DC đã được kiểm tra trong chế độ huấn luyện bình thường và mở rộng, tạo ra mô hình ResNet50 ImageNet-1K thưa thớt toàn cục 95% với độ chính xác top-1 73.14%, và mô hình thưa thớt 98% với độ chính xác top-1 68.44% sau 100 epoch huấn luyện. Các tác giả cũng thử nghiệm với thời gian huấn luyện mở rộng, tạo ra mô hình ResNet50 thưa thớt đồng nhất 95% với độ chính xác xác thực 74.3%.

Một phương pháp tỉa thành công khác là sự kết hợp của Powerpropagation [66] với Top-KAST [37]. Trong Powerpropagation, các trọng số được tham số hóa lại bằng cách sử dụng f(w) = w|w|^(α-1) cho α > 1, hiệu quả khuyến khích các trọng số có độ lớn cao tiếp tục tăng trong khi các trọng số có độ lớn thấp hơn được đẩy về 0. Top-KAST là một sơ đồ huấn luyện thưa thớt động phần lớn tương tự như RigL: trong Top-KAST, đối với mật độ mục tiêu D, các gradient của D' < D trọng số hàng đầu được tính toán trong mỗi vòng lan truyền ngược và được phép tích lũy, và các mặt nạ ở các độ thưa thớt tương ứng này được tính toán lại định kỳ. Sự kết hợp của hai phương pháp này dẫn đến độ chính xác 77.16% ở độ thưa thớt 90% khi được huấn luyện trong 3x đường cơ sở của họ là 32K bước.

Phương pháp ST-3 được đề xuất gần đây [69] sử dụng kỹ thuật ngưỡng mềm với ước lượng gradient thông qua thẳng để từ từ tỉa mạng nơ-ron trong khi cho phép các trọng số di chuyển mượt mà hơn giữa các trạng thái dày đặc và thưa thớt. Sử dụng phương pháp này, các tác giả đã có thể đạt được độ chính xác ImageNet từ 74% đến 75% ở độ thưa thớt 96% trên ResNet-50, tùy thuộc vào biến thể phương pháp được sử dụng.

Ngoài ra, một số công trình đã khám phá khó khăn của tối ưu hóa thưa thớt [15], khám phá các thay đổi đối với quy trình huấn luyện dày đặc để cải thiện huấn luyện thưa thớt [1,35], hoặc tập trung vào việc tạo ra mạng nơ-ron thưa thớt chính xác bên ngoài mô hình tiêu chuẩn của việc đồng thời tìm kiếm mặt nạ và trọng số tối ưu. Đáng chú ý, [49] đã khám phá tác động của khám phá mặt nạ (tức là tổng số tham số được khám phá tại bất kỳ thời điểm nào trong huấn luyện thưa thớt), chứng minh tác động tích cực của huấn luyện mở rộng trên cả hiệu suất mạng thưa thớt và tổng số tham số được khám phá. Phương pháp học STEP [52] khám phá tương tác của độ thưa thớt với bộ tối ưu hóa Adam [38], phát hiện ra rằng các trọng số bị che dẫn đến ước lượng không chính xác của moment thứ hai trong quá trình tối ưu hóa; những quan sát này đã dẫn đến đề xuất của họ về một phương pháp mới cho độ thưa thớt N:M để giảm thiểu những tác động này. Phương pháp GradMax [17] khởi tạo một mạng nơ-ron nhỏ, sau đó sử dụng các gradient được dự đoán để phát triển một mạng nơ-ron lớn hơn (trong khi vẫn nhỏ) bằng cách thêm các nơ-ron bổ sung.

Mô hình ngôn ngữ Đối với mô hình ngôn ngữ, quy trình nén tiêu chuẩn bao gồm hai giai đoạn: huấn luyện trước trên một kho văn bản lớn không được gắn nhãn, tiếp theo là tinh chỉnh trên một bộ dữ liệu nhỏ và được gắn nhãn cụ thể cho nhiệm vụ. Giai đoạn trước được sử dụng để nắm bắt các mẫu và mối quan hệ thống kê tồn tại trong ngôn ngữ tự nhiên, cho phép mô hình nhận biết và thậm chí tạo ra các mẫu ngôn ngữ khác nhau. Giai đoạn sau, tinh chỉnh trên nhiệm vụ hạ lưu, xây dựng trên các biểu diễn đã học và thích ứng chúng để giải quyết các nhiệm vụ cụ thể như phân loại văn bản, phân tích cảm xúc, phát hiện trùng lặp, v.v. Độ thưa thớt đã được khám phá trong cả hai giai đoạn: tỉa trong quá trình huấn luyện trước và tỉa trong quá trình tinh chỉnh.

Các phương pháp như Movement Pruning [63] và The Optimal BERT Surgeon (oBERT) [43] sử dụng thông tin bậc nhất (gradient) và bậc hai (độ cong) tương ứng để hướng dẫn quyết định tỉa trong giai đoạn tinh chỉnh. Tuy nhiên, công trình gần đây đã quan sát thấy hai vấn đề với phương pháp này khi được áp dụng trên các bộ dữ liệu nhỏ: [80] đã chứng minh sự không ổn định do biến thiên lớn của điểm quan trọng ước lượng, trong khi [32] đã quan sát thấy quá khớp mặc dù giảm sức mạnh biểu đạt do tỉa. Từ phía thực tế, phương pháp này ít thuận lợi hơn cho các nhà thực hành vì nó đòi hỏi kiến thức chuyên sâu về lĩnh vực tỉa để cấu hình đúng cách các bộ tỉa cho từng kết hợp mô hình và bộ dữ liệu. Do đó, trọng tâm chính của công trình của chúng tôi là giai đoạn khác, tận dụng các mô hình được huấn luyện trước thưa thớt đã có với học chuyển giao để thu được các mô hình được tinh chỉnh chính xác cao cụ thể cho nhiệm vụ.

Prune Once for All (Prune OFA) [79] và oBERT [43] đại diện cho các kỹ thuật tiên tiến nhất gần đây nhất giải quyết vấn đề này. Cả hai phương pháp đều trước tiên tỉa mô hình trong giai đoạn huấn luyện trước, và sau đó áp dụng học chuyển giao với mặt nạ thưa thớt cố định để thu được các mô hình được tinh chỉnh và thưa thớt trên các bộ dữ liệu hạ lưu khác nhau.

Tác động của việc làm thưa thớt ngoài độ chính xác top-1 Một lĩnh vực nghiên cứu mở là tác động mà việc tỉa nói chung, và lựa chọn phương pháp tỉa nói riêng, có đối với mô hình kết quả. Cụ thể, các mô hình được tỉa đã được chỉ ra là dễ bị thiên vị hơn [28,29,34], và tệ hơn về độ chính xác dự đoán dưới sự thay đổi phân phối [48]. Các công trình gần đây của [8] và [34] điều tra tác động của việc tỉa trên một loạt các chỉ số độ tin cậy của mô hình và tìm thấy kết quả hỗn hợp, với mạng nơ-ron thưa thớt có hiệu chuẩn tốt hơn, nhưng phóng đại các mẫu giả tạo trong dữ liệu hiện có. Cuối cùng, các công trình như [33] và [9] đã điều tra khả năng của CNN thưa thớt cho thích ứng miền thông qua học chuyển giao, phát hiện ra rằng mạng được huấn luyện thưa thớt có thể có các đặc trưng tổng quát hóa hơn so với mạng dày đặc.

3 Khó khăn của Huấn luyện trước Thưa thớt của Mô hình Thị giác

3.1 Nền tảng

Chính thức, việc tỉa chính xác là một bài toán tối ưu hóa có ràng buộc, với mục tiêu tối thiểu hóa một hàm mất mát L, nhằm tìm một mặt nạ thưa thớt "tối ưu" M⋆ với độ thưa thớt mục tiêu s, phần trăm tham số bằng không, và trọng số W⋆ sao cho

M⋆,W⋆ = argmin_mask M,weights W [L(M⊙W)] với nonzero(M) ≤ (1-s)numel(M). (1)

Trong dạng tổng quát của nó, nơi cả mặt nạ tối ưu và trọng số tối ưu phải được xác định, phương pháp này là NP-complete [5], ngay cả đối với mất mát bình phương tối thiểu đơn giản. Tuy nhiên, bài toán này có thể được giải quyết nếu chúng ta giả định một mặt nạ cố định, hoặc chúng ta muốn xấp xỉ độ thưa thớt của mặt nạ [2].

Trong bối cảnh của việc tỉa, quy trình này có thể được phân chia logic thành 1) xác định mặt nạ thưa thớt M, thường được tách biệt khỏi 2) quy trình tối ưu hóa trên các trọng số khác không. Ví dụ, phương pháp Giả thuyết Vé số may mắn (LTH) tiêu chuẩn [18,9] là trước tiên xác định mặt nạ "vé" bằng cách thực hiện lựa chọn trọng số theo độ lớn trên một mô hình đã được huấn luyện, tiếp theo là tinh chỉnh dựa trên SGD, sử dụng khởi tạo và cùng bộ siêu tham số như để huấn luyện dày đặc.

Trong khi một số cách mới để chọn hoặc cập nhật lựa chọn mặt nạ thưa thớt (bước 1), đã được điều tra, nói chung, đối với bước thứ hai, tức là tối ưu hóa các trọng số còn lại, các phương pháp huấn luyện thưa thớt phần lớn mô phỏng các siêu tham số của mô hình dày đặc cơ sở, bao gồm tổng số epoch huấn luyện [21,36,16,60]. Tuy nhiên, trực quan rằng vấn đề đồng thời tìm các trọng số gần tối ưu và mặt nạ gần tối ưu có thể khó giải quyết hơn so với bài toán tối thiểu hóa mất mát dày đặc tiêu chuẩn.

Điều này tự nhiên thúc đẩy một cuộc điều tra sâu sắc về các câu hỏi sau: việc tối ưu hóa trên mạng thưa thớt có thể hội tụ với cùng tốc độ như trên mạng dày đặc không?, và các công thức huấn luyện dày đặc có phù hợp với huấn luyện thưa thớt không? Trong bài báo này, chúng tôi cung cấp bằng chứng rằng câu trả lời cho cả hai câu hỏi đều là phủ định, gợi ý rằng các bộ tối ưu hóa được cải thiện có thể được yêu cầu để thu được mô hình thưa thớt chính xác dưới ngân sách huấn luyện giảm.

3.2 Mô hình Thị giác Thưa thớt Cho thấy Bằng chứng Huấn luyện Thiếu

Chúng tôi bắt đầu bằng cách điều tra các mối tương quan giữa hiệu suất và đặc điểm đầu ra của mô hình dày đặc và thưa thớt được huấn luyện cho số lượng epoch ngày càng tăng. Cụ thể, chúng tôi kiểm tra ba chỉ số chính: độ chính xác Top-1 trên tập xác thực/kiểm tra, mất mát trên tập huấn luyện, và entropy dự đoán trên tập xác thực/kiểm tra cho các mô hình được huấn luyện, trong khi mở rộng số lượng epoch huấn luyện và các siêu tham số liên quan tương ứng. Chúng tôi trình bày chi tiết các chỉ số này dưới đây.

Mất mát Huấn luyện và Entropy Đầu ra. Chúng tôi kiểm tra độ khớp của mô hình với dữ liệu huấn luyện thông qua mất mát huấn luyện (cross-entropy) tại epoch cuối cùng, và dự đoán đầu ra thông qua khái niệm lý thuyết thông tin về entropy. Entropy dự đoán thấp ngụ ý rằng trọng số dự đoán chủ yếu tập trung vào một lớp duy nhất, trong khi entropy cao gợi ý rằng nó được trải ra trên nhiều lớp. Trực quan, entropy của mô hình liên quan đến "độ tin cậy" của nó trong các dự đoán, và độc lập với việc các dự đoán có chính xác không (và do đó có thể được đo trên dữ liệu không được gắn nhãn). Ngược lại, mất mát huấn luyện thấp đo độ khớp của mô hình với dữ liệu huấn luyện.

Chúng tôi tính toán mất mát cross-entropy và entropy dự đoán bằng cách lấy softmax trên vector giá trị đầu ra của mạng và sau đó áp dụng các công thức tiêu chuẩn tương ứng, trong đó cross-entropy được lấy đối với phân phối nhãn chính xác cho mô hình (1 cho lớp chính xác và 0 cho các lớp khác). Đối với đầu ra của một mạng xuất ra vector Z = (z₁, z₂, ..., z_C) kích thước C với nhãn chính xác L, entropy H và cross-entropy CE được cho bởi các công thức sau:

H(Z) = -∑ᵢ₌₁ᶜ (e^(zᵢ)/∑ⱼ₌₁ᶜ e^(zⱼ)) log(e^(zᵢ)/∑ⱼ₌₁ᶜ e^(zⱼ))

và CE(Z) = -log(e^(zₗ)/∑ⱼ₌₁ᶜ e^(zⱼ)) (2)

Như chúng tôi sẽ chứng minh, chúng tôi mong đợi một mô hình đủ lớn và được huấn luyện tốt sẽ có (a) mất mát thấp trên dữ liệu huấn luyện và (b) entropy dự đoán trung bình khá thấp, trong khi một mô hình không được huấn luyện tốt sẽ có entropy dự đoán cao. Tuy nhiên, như đã biết thông thường, việc tiếp tục huấn luyện trên mô hình dày đặc và thưa thớt thấp dẫn đến quá khớp sẽ làm giảm thêm các chỉ số này.

3.2.1 Ví dụ 1: Huấn luyện Thưa thớt trên ImageNet

Thiết lập thí nghiệm. Chúng tôi trước tiên kiểm tra độ chính xác xác thực trên các mô hình ResNet50 thưa thớt và dày đặc được huấn luyện trên bộ dữ liệu ImageNet-1K và so sánh với (a) entropy dự đoán trên bộ dữ liệu xác thực và (b) mất mát huấn luyện tại epoch cuối cùng của huấn luyện. Tất cả các mô hình được huấn luyện sử dụng siêu tham số tiêu chuẩn (xem Phụ lục A) ngoại trừ sự khác biệt về số lượng epoch huấn luyện trong các thí nghiệm khác nhau. Các phép đo thể hiện độ chính xác cuối cùng và entropy sau epoch huấn luyện cuối cùng, vì vậy mỗi điểm đánh dấu trên biểu đồ đại diện cho một thí nghiệm đầy đủ, thay vì một checkpoint trung gian. Các mô hình thưa thớt được tỉa bằng Nén/Giải nén Xen kẽ (AC/DC) [60], tương tự điều chỉnh tổng số giai đoạn nén và giải nén theo tổng độ dài chạy. AC/DC được chọn vì nó nằm trong số các phương pháp hoạt động tốt nhất trên tất cả các độ thưa thớt và độ dài huấn luyện (xem Phần 3.3.1). Chúng tôi sử dụng thư viện FFCV [45] để tải dữ liệu nhanh chóng. Trái ngược với các lần chạy khác được trình bày trong bài báo này, chúng tôi không sử dụng thay đổi kích thước dần dần hoặc làm mượt nhãn, vì cái sau một cách rõ ràng khuyến khích entropy dự đoán và cross-entropy cao. Trong những thí nghiệm này, chúng tôi giữ lớp đầu tiên và cuối cùng dày đặc.

Kết quả. Kết quả của chúng tôi được trình bày trong Hình 1. Trên bảng bên trái, chúng tôi hiển thị độ chính xác top-1 của các mô hình cuối cùng. Chúng tôi quan sát thấy rằng các mô hình thưa thớt 80% và 90% đạt được độ chính xác tương tự như mô hình dày đặc, thậm chí vượt hơi hơn độ chính xác dày đặc ở độ thưa thớt 80%. Độ chính xác thực sự giảm ở độ thưa thớt cao hơn (95% và 98%); điều này phù hợp với bài báo AC/DC gốc và kết quả từ các phương pháp tỉa khác. Kiểm tra độ chính xác trên các ngân sách epoch, và tập trung vào mô hình hoạt động tốt nhất cho mỗi mức độ thưa thớt, chúng tôi quan sát thấy những điều sau:

• Mô hình dày đặc yêu cầu ít epoch nhất (88) để đạt được độ chính xác xác thực tốt nhất, và việc mở rộng công thức huấn luyện dẫn đến hiệu suất tệ hơn cho mô hình dày đặc, thường được biết đến là "quá khớp".

• Kết quả thay đổi nếu chúng ta kiểm tra các mô hình thưa thớt, đối với chúng độ dài huấn luyện lý tưởng tăng với độ thưa thớt: 250 epoch cho mô hình thưa thớt 80% và 90%, và ít nhất 500 epoch—lịch trình dài nhất chúng tôi thử trong thí nghiệm này—cho mô hình thưa thớt 95% và 98%. Ngay cả ở 500 epoch, việc tăng độ chính xác/giảm mất mát cho những mô hình này dường như không bão hòa.

Bây giờ chúng tôi kiểm tra mất mát trên bộ dữ liệu huấn luyện và entropy dự đoán cho cùng thí nghiệm chi tiết hơn. Hai chỉ số này cung cấp cho chúng tôi hai cách khác nhau để xem xét sự hội tụ của mô hình. Mất mát (cross-entropy) trên dữ liệu huấn luyện cho thấy các tham số của mô hình khớp với mục tiêu học như thế nào; ngược lại, entropy dự đoán trên dữ liệu xác thực, mặc dù tương tự trong tính toán, phản ánh độ tin cậy của mô hình trong các dự đoán của nó khi được trình bày với dữ liệu chưa thấy trước đó; ngoài ra, không giống như cross-entropy, nó độc lập với nhãn.

Chúng tôi quan sát thấy rằng, đối với tất cả các mức độ thưa thớt, cả hai chỉ số đều hoạt động rất tương tự. Cụ thể, cả hai đều giảm khi số lượng epoch huấn luyện tăng, và các mô hình thưa thớt được huấn luyện trong 100 epoch tiêu chuẩn cho thấy mất mát huấn luyện và entropy dự đoán tương tự như mô hình dày đặc được huấn luyện trong ít epoch hơn nhiều. Ví dụ, mô hình dày đặc được huấn luyện trong 24 epoch có mất mát huấn luyện và entropy dự đoán tương tự như mô hình thưa thớt 95% được huấn luyện trong 100 epoch, trong khi mô hình dày đặc được huấn luyện trong 100 epoch có mất mát huấn luyện và entropy dự đoán hơi thấp hơn so với mô hình thưa thớt 80% được huấn luyện trong 250 epoch. Khi chúng tôi xem xét các mô hình hoạt động tốt nhất ở các mức độ thưa thớt tương ứng, chúng tôi thấy rằng chúng có mất mát huấn luyện và entropy dự đoán tương tự như mô hình dày đặc hoạt động tốt nhất, trong các trường hợp mà mất mát/entropy thấp như vậy có thể đạt được trong một số lượng epoch hợp lý (ở độ thưa thớt 80% và 90%); ở tất cả các độ thưa thớt, hiệu suất giảm đối với các mô hình có mất mát huấn luyện và entropy dự đoán giảm dưới giá trị này.

Thảo luận. Những phát hiện này hỗ trợ thêm cho giả thuyết của chúng tôi rằng, do khó khăn vốn có của tối ưu hóa thưa thớt, việc sử dụng công thức huấn luyện tiêu chuẩn là không đủ cho huấn luyện thưa thớt, và gợi ý rằng huấn luyện dài hơn có thể giảm thiểu hiệu ứng này. Hơn nữa, kết quả gợi ý rằng mất mát huấn luyện và/hoặc entropy dự đoán có thể là tiêu chí hữu ích để xác thực rằng các mô hình thưa thớt được huấn luyện đúng cách, với tiêu chí sau cũng hữu ích trong các trường hợp không có quyền truy cập vào dữ liệu huấn luyện, hoặc vào bất kỳ dữ liệu được gắn nhãn nào.

3.2.2 Ví dụ 2: Huấn luyện Thưa thớt trên Celeb-A

Để xác thực các phát hiện của chúng tôi, chúng tôi lặp lại thí nghiệm này trên Bộ dữ liệu Celeb-A [51]. Bộ dữ liệu này bao gồm tổng cộng 202599 hình ảnh khuôn mặt của 10177 người nổi tiếng được thu thập từ miền công cộng, tự động cắt đến khuôn mặt, và được chú thích với 40 nhãn nhị phân. Do nội dung của nó, bộ dữ liệu này thường được sử dụng để nghiên cứu thiên vị trong các mô hình học máy, và cũng đã được sử dụng trong các nghiên cứu về tác động của độ thưa thớt đối với thiên vị [28, 34].

Thiết lập Thí nghiệm. Theo [34], chúng tôi huấn luyện kiến trúc ResNet18 trên nhiệm vụ này. Chúng tôi huấn luyện các mô hình dày đặc và thưa thớt (90%, 95%, 98%, 99%, và 99.5%) cho một số lượng epoch khác nhau (5-200); trong một số trường hợp, các lần chạy epoch rất thấp hoặc rất cao được bỏ qua nếu rõ ràng là thời lượng sẽ không tối ưu cho độ thưa thớt đó. Các mô hình thưa thớt được tạo ra bằng một biến thể của AC/DC, trong đó độ thưa thớt của các giai đoạn thưa thớt tăng dần từ 90% đến độ thưa thớt mục tiêu cuối cùng; điều này cần thiết để ngăn chặn sự sụp đổ lớp ở độ thưa thớt rất cao. Không giống như các thí nghiệm ImageNet, ở đây độ dài giai đoạn thay đổi hơi với thời lượng, do thời lượng cực ngắn của một số lần chạy. Đối với mỗi thí nghiệm, được đặc trưng bởi một cặp độ thưa thớt/độ dài-epoch, chúng tôi đo độ chính xác và mất mát huấn luyện của mô hình kết quả. Ngoài ra, theo [34], chúng tôi đo sự bất định, thay vì entropy, của các dự đoán mô hình trên tập kiểm tra. Sự bất định dự đoán được tính toán như sau: trước tiên, toán tử sigmoid được áp dụng cho đầu ra của mỗi logit để thu được xác suất giả của nhãn dương giữa 0 và 1; nếu số lượng này nằm giữa 0.1 và 0.9, dự đoán được coi là bất định. Sau đó chúng tôi tính toán tỷ lệ dự đoán bất định trên bộ dữ liệu xác thực.

Kết quả. Kết quả được trình bày trong Hình 2. Chúng tôi quan sát thấy rằng, phù hợp với các quan sát trước đó của chúng tôi trên ImageNet, thời lượng huấn luyện tối ưu tăng theo độ thưa thớt, với mô hình dày đặc đạt được độ chính xác tối ưu ở 25 epoch, và mô hình thưa thớt 99% và 95% ở 150 epoch. Hơn nữa, chúng tôi quan sát thấy rằng, ngay cả khi mất mát huấn luyện và sự bất định kiểm tra luôn giảm với huấn luyện dài hơn, mất mát huấn luyện tổng thể và tỷ lệ dự đoán bất định tăng theo độ thưa thớt ở độ dài huấn luyện cố định. Như trong ví dụ ImageNet, các mô hình hoạt động cao nhất ở mỗi độ thưa thớt có mất mát huấn luyện tương tự khoảng 0.17 và bất định dự đoán trung bình khoảng 24%, ngoại trừ mô hình thưa thớt rất cao 99.5%, có bất định dự đoán hơi cao hơn 26%.

Thảo luận. Chúng tôi diễn giải những kết quả này như bằng chứng củng cố rằng các mô hình thưa thớt yêu cầu huấn luyện dài hơn so với mô hình dày đặc để đạt được độ chính xác tối ưu trước khi quá khớp bắt đầu diễn ra.

3.3 Huấn luyện trước Thưa thớt Chính xác Tiên tiến trên ImageNet

Các quan sát trên đây đối với mô hình thị giác gợi ý rằng huấn luyện thưa thớt thành công có thể hưởng lợi từ lịch trình huấn luyện mở rộng. Bây giờ chúng tôi xây dựng trên ý tưởng này để đạt được kết quả tiên tiến cho điểm chuẩn ResNet50/ImageNet cổ điển bằng cách sử dụng phiên bản huấn luyện mở rộng của AC/DC, mà chúng tôi gọi là AC/DC++.

3.3.1 So sánh Các Phương pháp Huấn luyện Thưa thớt

Đối với các thí nghiệm sau, chúng tôi bắt đầu từ phương pháp huấn luyện tiên tiến hiện tại cho huấn luyện ResNet50/ImageNet, sử dụng gói Pytorch FFCV [45]. Ngoài lịch trình huấn luyện mở rộng, chúng tôi sử dụng làm mượt nhãn và suy giảm tốc độ học tuyến tính với warm-up, cũng như thay đổi kích thước dần dần của các mẫu đầu vào. Trong bối cảnh này, chúng tôi đã triển khai ba phương pháp huấn luyện thưa thớt hàng đầu: Tỉa Độ lớn Từ từ (GMP) [81], RigL [16] và AC/DC [60], mà chúng tôi thực hiện cho số lượng epoch tăng dần từ 100 (tiêu chuẩn) đến 1000 (10x). Để làm điều này, chúng tôi mở rộng lịch trình huấn luyện gốc theo tỷ lệ, theo các tỷ lệ được sử dụng bởi các phương pháp gốc. Đối với thí nghiệm này, các mô hình được nén đến độ thưa thớt 80%, 90%, và 95%. Theo thiết lập thí nghiệm phổ biến nhất, chúng tôi tỉa tất cả trọng số trong các lớp tích chập và tuyến tính (bao gồm tích chập đầu vào và đầu phân loại). Công thức huấn luyện chính xác được trình bày chi tiết trong Phụ lục A. Chúng tôi lưu ý rằng tất cả các thí nghiệm được trình bày trong bài báo mất ít hơn một ngày trên một máy chủ 8-GPU tiêu chuẩn. Kết quả, về mặt độ chính xác và mất mát so với số lượng epoch huấn luyện được trình bày trong Hình 3 và Hình 4, tương ứng.

Kết quả. Kết quả cho thấy mối tương quan mạnh mẽ giữa mức độ các phương pháp đạt được giảm mất mát và độ chính xác xác thực của chúng. Điều này củng cố quan điểm rằng các phương pháp huấn luyện thưa thớt bão hòa chậm hơn, cả về mặt mất mát huấn luyện và độ chính xác xác thực. Điều này cũng đã được công trình trước đây điều tra: Gale et al. [21] đã thấy rằng huấn luyện mở rộng đã cải thiện kết quả cho GMP trong một số trường hợp, trong khi RigL [16] và Powerpropagation [66] đã tìm thấy những cải thiện giảm dần. Cùng lúc, chúng tôi nhận thấy sự khác biệt đáng kể giữa các phương pháp: cụ thể, AC/DC bắt đầu ở một điểm độ chính xác hơi tốt hơn, và luôn luôn vượt trội hơn các phương pháp khác cả về mặt mất mát đạt được, và về mặt độ chính xác xác thực, khi chúng tôi tăng thời gian huấn luyện. (Điều này phù hợp với kết quả AC/DC gốc, được thực hiện ở 100 epoch [60].) Chúng tôi quan sát thấy rằng điều này tương quan với chi phí tính toán lý thuyết (FLOPs) của các phương pháp: AC/DC sẽ sử dụng nhiều FLOPs hơn so với các phương pháp khác do các giai đoạn huấn luyện dày đặc, trong khi GMP sử dụng nhiều FLOPs hơn RigL do độ thưa thớt tăng dần. Đến lượt nó, điều này cũng có thể tương quan với lượng khám phá mặt nạ được thực hiện bởi thuật toán trong quá trình huấn luyện. Ở độ thưa thớt thấp, RigL hoạt động hơi tốt hơn GMP, nhưng đối với độ thưa thớt cao hơn, GMP dường như hoạt động tốt hơn. Đối với 80%, 90% nhỏ nhất, AC/DC đạt được một điểm bão hòa, trong khi ở tất cả các thiết lập khác, hiệu suất mô hình tiếp tục cải thiện với ngân sách huấn luyện.

3.3.2 Kết quả Độ thưa thớt-Độ chính xác

Mục tiêu và Chỉ số. Dựa trên những kết quả này, trong phần này, chúng tôi nhằm cải thiện thỏa thuận độ thưa thớt-độ chính xác tốt nhất đã biết bằng cách thực hiện một nghiên cứu triệt để về các tham số độ thưa thớt và độ dài huấn luyện. Chúng tôi so sánh kết quả của chúng tôi với các phương pháp huấn luyện thưa thớt hiệu suất cao nhất đã được công bố trước đây. Cụ thể, chúng tôi so sánh phiên bản huấn luyện mở rộng của AC/DC, mà chúng tôi gọi là AC/DC++, với kết quả được báo cáo trong các bài báo RigL, ST-3, và Powerpropagation gốc, cũng như nhiều phương pháp tỉa hiện có khác. Tất cả các phương pháp được mô tả trong Phần 2. Trong các trường hợp mà các tác giả đã tiến hành huấn luyện mở rộng bằng phương pháp của họ, chúng tôi trình bày những con số đó, và chúng tôi sử dụng biến thể ST-3σ được tối ưu hóa FLOPs. Các mô hình ứng cử viên AC/DC++ được huấn luyện trong bốn độ dài huấn luyện được thiết lập trước (1x, 2.5x, 5x và 10x thời gian huấn luyện ImageNet tiêu chuẩn trên ResNet50) ở tất cả các mức độ thưa thớt, và chúng tôi đã chọn kết quả tốt nhất thu được bằng cách nghiên cứu về độ dài của lần chạy huấn luyện.

Như các phương pháp khác nhau có ngân sách tính toán khác nhau và đường cơ sở dày đặc khác nhau, để đảm bảo so sánh công bằng, chúng tôi kiểm tra hiệu suất mô hình cả về mặt độ chính xác Xác thực Top-1, và sự khác biệt độ chính xác Xác thực Top-1 từ đường cơ sở dày đặc tương ứng. Chúng tôi sử dụng các con số tốt nhất có sẵn được báo cáo ban đầu trong các bài báo giới thiệu các phương pháp để so sánh.

Thiết lập Thí nghiệm. Chúng tôi so sánh hai chế độ tỉa. Trước tiên, chúng tôi xem xét Tỉa Đồng nhất, trong đó mỗi lớp được tỉa chính xác đến độ thưa thớt mục tiêu, ngoại trừ lớp đầu tiên và cuối cùng, được để lại dày đặc. Thứ hai, chúng tôi xem xét chế độ Tỉa Toàn cục/Không đồng nhất, trong đó ngân sách độ thưa thớt được thiết lập toàn cục. Các công trình khác nhau phân bổ ngân sách toàn cục khác nhau, và cũng khác nhau về việc phần nào của mạng phải tuân theo ràng buộc toàn cục. Cụ thể, Extended GMP [21] và Top-KAST không tỉa lớp đầu tiên, tỉa lớp cuối cùng đến độ thưa thớt cố định 80%, và tỉa các lớp khác sử dụng tiêu chí độ lớn toàn cục. RigL sử dụng phân phối Erdős–Rényi-Kernel cho mục tiêu độ thưa thớt lớp, và chỉ để lại lớp đầu tiên dày đặc. Công trình AC/DC gốc sử dụng độ thưa thớt toàn cục và tỉa tất cả các lớp tích chập và FC. Do đó, để tạo ra một so sánh công bằng hơn, chúng tôi xem xét các Phép toán Dấu phẩy động (FLOPs) ước tính cần thiết cho suy luận; những con số này được tính toán như trong [16]. Sử dụng FLOPs cũng làm cân bằng các phương pháp trên các biến thể kiến trúc ResNet50 nhỏ, và vì vậy chúng tôi sử dụng nó cũng cho so sánh tỉa Đồng nhất. Ngoài ra, chúng tôi sử dụng hai lịch trình tỉa cho AC/DC++: một lịch trình để lại lớp đầu tiên và cuối cùng dày đặc và tỉa các lớp còn lại sử dụng tiêu chí độ lớn toàn cục, và một lịch trình tỉa tất cả các lớp sử dụng tiêu chí độ lớn toàn cục. Chúng tôi không nghiên cứu giữa hai lịch trình này, mà thay vào đó trình bày cả hai bộ kết quả trong Hình 5 (chung) và Bảng 1 (riêng biệt).

Chúng tôi nhấn mạnh hai điểm quan trọng về so sánh của chúng tôi:

1. Chỉ nhìn vào độ chính xác có lợi cho AC/DC++, vì nó có đường cơ sở dày đặc cao hơn: vì chúng tôi sử dụng một số cải tiến huấn luyện gần đây, mô hình dày đặc có thể đạt gần 79% độ chính xác dày đặc trong 100 epoch. Do đó, nó trở nên thách thức hơn để duy trì hiệu suất của mô hình dày đặc cho mô hình thưa thớt cao so với đường cơ sở kém tối ưu hóa hơn.

2. Đây là lý do chúng tôi cũng kiểm tra sự khác biệt độ chính xác so với đường cơ sở dày đặc: điều này có lợi cho các phương pháp khác, vì chúng được đánh giá so với một mô hình công thức tiêu chuẩn đạt được độ chính xác thấp hơn 76.8% (77.1% đối với ST-3).

Kết quả. Kết quả được trình bày trong Hình 5 và Bảng 1. Chúng tôi quan sát thấy rằng, đối với ngân sách tỉa đồng nhất, các mô hình AC/DC++ vượt trội hơn các phương pháp khác, cả về mặt độ chính xác xác thực tuyệt đối và tương đối. Điều này đúng ngay cả khi chúng tôi xem xét lịch trình huấn luyện mở rộng cho các phương pháp khác, mặc dù chúng tôi tin rằng chúng tôi là những người đầu tiên điều tra một cách có hệ thống tác động của việc tăng lịch trình huấn luyện ở các mức độ thưa thớt này. Khi nhìn vào các mô hình được huấn luyện với ngân sách tỉa toàn cục, chúng tôi quan sát thấy rằng AC/DC++ thu được độ chính xác xác thực tuyệt đối cao nhất, so với kết quả đã được báo cáo trước đây trong tài liệu. Khi xem xét sự thay đổi độ chính xác từ đường dày đặc, AC-DC++ mất ít độ chính xác hơn so với các phương pháp khác ở độ thưa thớt rất cao (FLOPs thấp nhất), mặc dù có đường cơ sở dày đặc hoạt động cao nhất; ở độ thưa thớt thấp hơn (90%), nó cạnh tranh với các phương pháp huấn luyện mở rộng khác.

3.4 Kết quả Bổ sung

Các mẫu độ thưa thớt khác nhau Do các mô hình ResNet tăng số lượng kênh với việc giảm độ phân giải bản đồ đặc trưng, người ta sẽ mong đợi rằng các lớp dưới cùng (những lớp có nhiều kênh hơn) nên được tỉa tích cực hơn so với những lớp có ít kênh hơn. Ở đây kết quả của chúng tôi phù hợp với các quan sát đã biết từ tài liệu rằng độ thưa thớt toàn cục đạt hiệu suất cao hơn cho cùng một độ thưa thớt. Ngoài ra, chúng tôi đã tiến hành thí nghiệm với mẫu độ thưa thớt block-4 thân thiện với phần cứng hơn.

Kết quả MobileNet Ngoài ra, chúng tôi đã tiến hành việc làm thưa thớt mô hình MobileNet-V1 [30], một CNN được tối ưu hóa cho suy luận trên thiết bị di động. Chúng tôi áp dụng AC/DC trong 1000 epoch với mục tiêu độ thưa thớt 75% và 90% sử dụng công thức huấn luyện tương tự như ResNet50 ngoại trừ một số khác biệt được chỉ định trong Phụ lục A. Để đạt được kết quả tốt nhất, chúng tôi không tỉa tích chập đầu vào, cũng như đầu phân loại và tích chập theo chiều sâu, do đóng góp nhỏ của chúng vào tổng lượng FLOPs và tác động đáng kể đến hiệu suất của mô hình.

Với công thức huấn luyện dài hơn, người ta có thể đạt được sự sụt giảm độ chính xác gần như không đáng kể ở độ thưa thớt 75% và giảm hiệu suất vừa phải ở 90%. Kết quả có thể được tìm thấy trong Bảng 2.

3.5 Phân tích mặt nạ

Các phương pháp thưa thớt được xem xét trong công trình của chúng tôi khác nhau về lượng khám phá mặt nạ thưa thớt. GMP từ từ tăng độ thưa thớt; một khi trọng số được tỉa, nó không bao giờ được đưa trở lại. RigL giảm phần trăm tham số được cập nhật theo quy tắc ủ cosine:

f_decay(t;α;T_end) = α/2 * (1 + cos(πt/T_end)) (3)

Phần trăm kết nối này được giảm và đưa trở lại trong một bước duy nhất. AC/DC làm cho tất cả các tham số có thể huấn luyện được trong các giai đoạn giải nén, do đó bất kỳ tham số nào có thể được đưa trở lại. Tuy nhiên, như được hiển thị sau, một số phần trăm trọng số vẫn bằng không ngay cả khi giải nén. Để đo sự khác biệt giữa hai mặt nạ thưa thớt liên tiếp, chúng tôi tính toán IoU (Intersection over Union) của chúng, lượng tham số khác không cho cả hai checkpoint chia cho lượng tham số khác không trong một trong hai checkpoint. Giá trị IoU cao (gần 1) có nghĩa là hai mặt nạ thưa thớt trùng lặp đáng kể, trong khi IoU thấp (gần 0) ngụ ý sự tương đồng thấp giữa các mặt nạ thưa thớt.

Chúng tôi đã lấy các checkpoint được lưu vào epoch thứ 109, 119, ..., 999 (được lấy ở cuối mỗi bước AC/DC và tại các epoch tương tự cho các phương pháp khác để so sánh nhất quán) được thu thập trong quá trình chạy 1000 epoch với độ thưa thớt mục tiêu 95% và 98% và đo IoU giữa hai mặt nạ liên tiếp cho các tham số đang được tỉa (bỏ qua tham số bias và batch norm). Đối với GMP và RigL, IoU mặt nạ có thể được tính toán phân tích dựa trên quy tắc cập nhật. Sự tiến hóa của IoU mặt nạ thưa thớt trong quá trình huấn luyện được trình bày trong Hình 7. Người ta có thể quan sát thấy rằng AC/DC cho thấy khám phá mặt nạ mạnh mẽ hơn đáng kể so với GMP và RigL. Hành vi này có thể tính đến hiệu suất tốt hơn của AC/DC như một bộ huấn luyện thưa thớt.

Độ thưa thớt kênh

Trong công trình này, chúng tôi chỉ xem xét độ thưa thớt không có cấu trúc, do đó các nhóm trọng số và toàn bộ kênh đặc biệt không phải thưa thớt. Tuy nhiên, chúng tôi đã quan sát thấy rằng một số kênh trong các kernel tích chập được tỉa hoàn toàn. Hiệu ứng trở nên rõ rệt hơn với độ thưa thớt cao hơn và huấn luyện dài hơn.

Trong Bảng 3, chúng tôi hiển thị phần trăm kênh đầu ra thưa thớt cho các lớp ở phía trước, giữa và cuối của mô hình và phần trăm toàn cục của các kênh bằng không trên tất cả các lớp tích chập trong mô hình. Chúng tôi quan sát thấy rằng độ thưa thớt kênh tăng theo tỷ lệ với mục tiêu độ thưa thớt không có cấu trúc và với thời gian huấn luyện, và rằng, đối với độ thưa thớt cao, chúng tôi thu được tỷ lệ rất cao các kênh đầu ra bị làm bằng không, đặc biệt là trong các lớp dưới cùng rộng hơn. Điều này phù hợp với công trình trước đây quan sát sự xuất hiện của độ thưa thớt có cấu trúc trong huấn luyện thưa thớt động [60, 33, 77]. Chúng tôi cung cấp lời giải thích đầu tiên cho hành vi này trong phần tiếp theo.

3.7 Tác động của suy giảm trọng số đối với độ thưa thớt và hiệu suất mô hình

Nhớ lại rằng AC/DC làm cho tất cả các tham số có thể huấn luyện được khi giải nén, do đó, người ta có thể mong đợi rằng độ thưa thớt trong giai đoạn giải nén sẽ gần bằng không. Tuy nhiên, chúng tôi đã quan sát thấy rằng một phần lớn trọng số vẫn bằng không ngay cả khi mặt nạ thưa thớt không được áp dụng. Hiệu ứng này liên quan đến độ thưa thớt kênh được thảo luận trong phần trước: một khi một kênh bị làm bằng không hoàn toàn, nó sẽ tiếp tục nhận gradient bằng không ngay cả khi mặt nạ thưa thớt được loại bỏ. Hơn nữa, chúng tôi cung cấp bằng chứng rằng hiện tượng này liên quan đến việc tăng cơ chế suy giảm trọng số, và cụ thể là giá trị của tham số này: trực quan, suy giảm trọng số từ từ đẩy trọng số về phía không; bất cứ khi nào một kênh đầy đủ bị làm bằng không trong giai đoạn nén, nó vẫn bị "bắt giữ" dưới mặt nạ thưa thớt cho đến khi kết thúc huấn luyện.

Chúng tôi đã điều tra điều này một cách thực nghiệm bằng cách huấn luyện các mô hình ImageNet trên ResNet50 với độ thưa thớt nén 95% sử dụng AC/DC++ trong 100 epoch, thay đổi tham số suy giảm trọng số từ 10^-6 đến 10^-3. Kết quả của chúng tôi được hiển thị trong Hình 8. Chúng tôi quan sát thấy rằng phần trăm tham số bằng không tăng theo độ lớn của suy giảm trọng số và cũng qua quá trình huấn luyện. Cụ thể, chúng tôi quan sát thấy rằng tất cả các giá trị suy giảm trọng số dẫn đến các mô hình gần như hoàn toàn dày đặc trong giai đoạn giải nén đầu tiên. Từ đó, các giá trị suy giảm trọng số rất thấp 10^-6 và 10^-5 dẫn đến rất ít độ thưa thớt trong hai giai đoạn giải nén tiếp theo, và khoảng 10% độ thưa thớt trong năm giai đoạn cuối cùng. Ngược lại, suy giảm trọng số rất cao 10^-3 dẫn đến một bước nhảy ngay lập tức đến 50% độ thưa thớt trong giai đoạn giải nén thứ hai, sau đó tăng lên 60% trong phần còn lại của huấn luyện. Giá trị trung gian 10^-4, là thiết lập tiêu chuẩn và được sử dụng trong các thí nghiệm của chúng tôi, dẫn đến độ thưa thớt trung gian, từ từ tăng lên khoảng 24% qua các giai đoạn giải nén liên tiếp.

Chúng tôi tiếp tục trình bày độ chính xác của các mô hình kết quả trong Bảng 4. Chúng tôi quan sát thấy rằng việc thiết lập đúng cách siêu tham số suy giảm trọng số là rất quan trọng đối với hiệu suất tốt của AC/DC++, và xác nhận rằng giá trị tiêu chuẩn 10^-4 gần với giá trị tối ưu, như Bảng 4 hiển thị.

Giải nén Thưa thớt. Xây dựng trên quan sát này, chúng tôi đặt câu hỏi về việc áp dụng một độ thưa thớt tối thiểu cố định cũng trên giai đoạn giải nén tác động đến hiệu suất cuối cùng như thế nào. Chúng tôi đã tiến hành một vài thí nghiệm AC/DC dài 100 epoch với độ thưa thớt mục tiêu 95% và thiết lập độ thưa thớt khi giải nén thành một giá trị cố định, nhỏ hơn độ thưa thớt nén. Chúng tôi quan sát thấy rằng hiệu suất gần như không bị ảnh hưởng lên đến 80% độ thưa thớt giải nén, cho thấy rằng khám phá mặt nạ đầy đủ không cần thiết trong quá trình huấn luyện.

3.8 Phân tích cảnh quan mất mát

Để có được thêm hiểu biết về lý do khó khăn trong việc tối ưu hóa mạng nơ-ron thưa thớt, chúng tôi điều tra hai tính chất của cảnh quan mất mát. Trước tiên, chúng tôi đo độ sắc nét cảnh quan ở cuối huấn luyện, được định nghĩa là giá trị riêng tối đa của ma trận Hessian, đối với tất cả các phương pháp huấn luyện thưa thớt được xem xét, độ thưa thớt khác nhau và số lượng epoch huấn luyện và so sánh với độ sắc nét của huấn luyện dày đặc tiêu chuẩn. Thứ hai, chúng tôi nội suy mất mát huấn luyện và xác thực giữa các checkpoint thu được tại các bước trung gian trong suốt lần chạy AC/DC 1000 epoch với độ thưa thớt mục tiêu 95%.

Giá trị riêng Hessian lớn nhất được ước tính thông qua phương pháp lặp power dựa trên tích Hessian-vector sử dụng phiên bản tùy chỉnh của thư viện Eigenthings [22]. Thông tin chi tiết hơn về thiết lập thí nghiệm của chúng tôi được cung cấp trong Phụ lục I. Trong Hình 9, chúng tôi quan sát thấy rằng, trên tất cả các phương pháp, độ sắc nét tăng theo độ dài của lần chạy huấn luyện, cho thấy rằng các cực tiểu sắc nét hơn đòi hỏi huấn luyện mở rộng để được đạt đến thông qua SGD. Ngoài ra, độ sắc nét giảm theo sự tăng của độ thưa thớt. Tất cả các phương pháp huấn luyện thưa thớt đạt được độ sắc nét thấp hơn so với mô hình dày đặc. Các mô hình được huấn luyện với AC/DC và RigL có độ sắc nét hơi thấp hơn so với các mô hình được huấn luyện với GMP, có thể do hai phương pháp trước đó quản lý để đạt đến các cực tiểu phẳng hơn được phỏng đoán có tính chất tổng quát hóa tốt hơn [54].

Để kiểm tra hành vi kết nối chế độ, trong Hình 10, chúng tôi đã kết nối checkpoint thu được vào epoch thứ 99, 199, ..., 999 thông qua các đường cong tuyến tính từng phần. Một quan sát đáng chú ý là tất cả các checkpoint được tách biệt bởi một rào cản mất mát, có chiều cao tăng theo số lượng epoch. Có thể, hành vi này là một biểu hiện của hiện tượng sắc nét hóa dần dần [11] nơi độ sắc nét mô hình tăng dần với huấn luyện cho đến khi đạt giá trị đỉnh và sau đó ổn định. Tuy nhiên, đối với các mô hình thưa thớt, thời lượng của các lần chạy dài nhất không đủ dài để đạt đến ổn định độ sắc nét.

3.9 Đánh giá chất lượng bổ sung của AC/DC++

Đã chứng minh rằng huấn luyện mở rộng có tác động tích cực mạnh mẽ đối với độ chính xác kiểm tra top-1 của mô hình thưa thớt, chúng tôi tiếp tục điều tra tác động của huấn luyện mở rộng đối với các khía cạnh khác của chất lượng mô hình. Chúng tôi xem xét hai chỉ số chất lượng bổ sung: hiệu suất của chúng trong các kịch bản học chuyển giao và độ bền chắc đối với các nhiễu loạn hình ảnh phổ biến.

[33] đã chứng minh rằng các mô hình thưa thớt bằng nhau với hiệu suất có thể so sánh trên nhiệm vụ gốc có thể thay đổi rộng rãi trong hiệu suất của chúng một khi được tinh chỉnh trên các nhiệm vụ chuyển giao khác, nhỏ hơn. Chúng tôi so sánh hiệu suất chuyển giao của mô hình AC/DC++ dày đặc và thưa thớt 95%, cả hai được huấn luyện trong 100 và 1000 epoch trong hai chế độ học chuyển giao: tinh chỉnh tuyến tính, nơi các lớp ẩn của mô hình chỉ được huấn luyện trên nhiệm vụ lớn hơn (ImageNet), và chỉ lớp FC cuối cùng được huấn luyện trên nhiệm vụ chuyển giao, và tinh chỉnh toàn mạng, nơi tất cả các lớp được tinh chỉnh trên nhiệm vụ chuyển giao. Chúng tôi thấy rằng huấn luyện mở rộng cải thiện hiệu suất chuyển giao cho cả hai kịch bản chuyển giao đối với mô hình thưa thớt 95%, nhưng phần lớn là trung tính đối với mô hình dày đặc. Chi tiết đầy đủ của thí nghiệm và đánh giá được cung cấp trong Phụ lục F.

Chúng tôi kiểm tra độ bền chắc bằng cách đo hiệu suất mô hình trên bộ dữ liệu ImageNet-C [26], nó thêm kỹ thuật số 19 loại nhiễu loạn vào tập xác thực ImageNet-1K. [48] và [28] đã thấy rằng các mô hình nén kém bền chắc hơn dưới nhiều loại nhiễu loạn, so với mô hình dày đặc. Như trước đây, chúng tôi xem xét mô hình AC/DC++ dày đặc và thưa thớt 95% được huấn luyện trong 100–1000 epoch tổng cộng. Chúng tôi thấy rằng độ bền chắc đối với nhiễu loạn tăng theo thời gian huấn luyện đối với mô hình thưa thớt, nhưng vẫn giữ nguyên đối với mô hình dày đặc. Chi tiết đầy đủ của thí nghiệm và đánh giá được cung cấp trong Phụ lục G.

4 Khó khăn của Chuyển giao Thưa thớt trong Mô hình hóa Ngôn ngữ

Được thúc đẩy bởi các phát hiện của chúng tôi đối với mô hình thị giác máy tính trong các phần trước, chúng tôi mở rộng phân tích cho mô hình ngôn ngữ, cụ thể là kịch bản rất phổ biến trong đó một mô hình ngôn ngữ lớn (BERT-base) được thích ứng với một nhiệm vụ cụ thể thông qua tinh chỉnh. Trái ngược với Phần 3.9, nơi chúng tôi kiểm tra tác động của huấn luyện mở rộng đối với chất lượng của các đặc trưng được tạo ra trong huấn luyện ngược dòng, ở đây chúng tôi kiểm tra tác động của độ thưa thớt đối với công thức tối ưu cho nhiệm vụ tinh chỉnh mô hình trên bộ dữ liệu hạ lưu. Thiết lập này tự nhiên dẫn đến các câu hỏi sau: "các mô hình ngôn ngữ thưa thớt được tinh chỉnh có bị huấn luyện thiếu trên nhiệm vụ hạ lưu không?", và "nếu có, liệu công thức đơn giản của huấn luyện mở rộng có đủ để giảm thiểu vấn đề không?". Trong phần này, chúng tôi sẽ chỉ ra rằng khi các công thức tinh chỉnh dày đặc được sử dụng cho học chuyển giao thưa thớt trong mô hình ngôn ngữ, các mô hình kết quả thực sự bị huấn luyện thiếu và có hiệu suất chuyển giao kém. Tuy nhiên, chúng tôi cũng lưu ý một khó khăn bổ sung: huấn luyện mở rộng không đủ để giảm thiểu vấn đề, vì mô hình ngôn ngữ thưa thớt nhanh chóng chuyển từ việc bị huấn luyện thiếu sang chế độ quá khớp. Cái sau là một vấn đề lớn hơn nhiều trong các nhiệm vụ hiểu ngôn ngữ so với trong nhiệm vụ thị giác, điều này có thể là lý do chúng tôi không quan sát các vấn đề tương tự với học chuyển giao thị giác trong Phụ lục F - ở đó chúng tôi chỉ đơn giản sử dụng lịch trình tinh chỉnh dài trong tất cả các trường hợp. Trong phần này, chúng tôi khám phá vấn đề cân bằng giữa huấn luyện thiếu và quá khớp trong mô hình ngôn ngữ thưa thớt và đề xuất một công thức tinh chỉnh thưa thớt để tạo ra mô hình thưa thớt được điều chỉnh đúng cách.

4.1 Dưới Các Công thức Chuyển giao Dày đặc Tiêu chuẩn, Mô hình Thưa thớt bị Huấn luyện Thiếu

Thiết lập Thí nghiệm. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng các mô hình BERT-base được huấn luyện trước thưa thớt nguồn mở thu được bởi [43]. Trên đó, chúng tôi áp dụng các công thức học chuyển giao khác nhau để thu được mô hình thưa thớt được tinh chỉnh trên các bộ dữ liệu từ điểm chuẩn GLUE phổ biến [73]. Để so sánh công bằng với kết quả từ công trình trước đây, chúng tôi sử dụng dừng sớm cho tất cả các phương pháp. Chúng tôi cung cấp thông tin chi tiết hơn về mỗi bộ dữ liệu trong Phụ lục H.

Công thức học chuyển giao dày đặc phổ biến và được áp dụng rộng rãi nhất bao gồm tinh chỉnh tất cả trọng số với tốc độ học giảm tuyến tính trong tối đa hai hoặc ba epoch trên nhiệm vụ hạ lưu mục tiêu. Trong Bảng 6, chúng tôi trình bày kết quả thu được với phương pháp này khi được áp dụng cho mô hình thưa thớt, và ký hiệu nó như công thức chuyển giao dày đặc. Dưới cùng công thức học chuyển giao, chúng tôi quan sát rõ ràng khoảng cách đáng kể (lên đến 14 điểm độ chính xác trên RTE và CoLA) giữa độ chính xác chuyển giao của mô hình dày đặc (Dense BERT-base), và độ chính xác chuyển giao của mô hình thưa thớt (công thức chuyển giao dày đặc).

4.2 Huấn luyện Mở rộng Chuyển từ Huấn luyện Thiếu sang Quá khớp

Quan sát rằng công thức học chuyển giao dày đặc không tạo ra mô hình thưa thớt được tinh chỉnh cạnh tranh, chúng tôi cố gắng mở rộng độ dài của công thức để giảm thiểu huấn luyện thiếu. Đáng ngạc nhiên, đối với mô hình ngôn ngữ thưa thớt, kỹ thuật đơn giản này không mang lại một thiết lập duy nhất với kết quả tốt hơn một cách nhất quán khi mô hình nhanh chóng chuyển từ huấn luyện thiếu sang chế độ quá khớp, trong đó mất mát huấn luyện giảm về không, trong khi độ chính xác xác thực giảm mạnh. Để chứng minh hiệu ứng quá khớp này với công thức mở rộng, trong Bảng 6, chúng tôi so sánh kết quả thu được với phương pháp này (công thức chuyển giao dày đặc mở rộng) với kết quả thu được bằng cách thực hiện một cuộc quét đầy đủ các lần chạy tinh chỉnh với công thức được tái tỷ lệ thành #epochs ∈ {1,2,3, ..., mở rộng -1} (Cuộc quét đầy đủ các công thức được tái tỷ lệ).

Kết quả gợi ý rằng với các công thức hiện có, không có giải pháp một-kích-thước-phù-hợp-tất-cả. Các phiên bản của phương pháp tái tỷ lệ này đã được sử dụng bởi các công trình trước đây như [43] và [79] để thu được mô hình thưa thớt chính xác trên các bộ dữ liệu hạ lưu khác nhau. Tuy nhiên, phương pháp này đi kèm với gánh nặng tính toán lớn: đối với mỗi công thức được tái tỷ lệ, một cuộc quét siêu tham số đầy đủ trên các tham số liên quan phải được thực hiện để thu được mô hình thưa thớt được tinh chỉnh cạnh tranh. Do tính thực tế và chi phí liên quan, đây không phải là giải pháp mong muốn trong thực tế.

4.3 Học Chuyển giao Thưa thớt cho Mô hình Ngôn ngữ

Trong phần trước, chúng tôi đã chứng minh ba vấn đề sau với phương pháp hiện có là sử dụng công thức tinh chỉnh dày đặc, hoặc chỉ đơn giản mở rộng nó cho tinh chỉnh thưa thớt:

1. theo các công thức chuyển giao dày đặc, mô hình ngôn ngữ thưa thớt bị huấn luyện thiếu;
2. ngay cả ở độ thưa thớt cao, những mô hình này vẫn có thể thể hiện hành vi quá khớp dưới chế độ huấn luyện mở rộng;
3. việc tìm công thức tối ưu để giảm thiểu huấn luyện thiếu và quá khớp có gánh nặng tính toán lớn.

Để giải quyết những vấn đề này, chúng tôi đề xuất một phương pháp đơn giản cho chuyển giao thưa thớt trong NLP, tạo ra mô hình thưa thớt có độ chính xác cao và cạnh tranh trên một loạt rộng các bộ dữ liệu hạ lưu với việc điều chỉnh siêu tham số tối thiểu. Kỹ thuật của chúng tôi được lấy cảm hứng từ ý tưởng mở khóa lớp từ từ được trình bày trong khung ULMFiT [31], đã giới thiệu một khung phổ quát để tinh chỉnh mô hình ngôn ngữ dày đặc cho các nhiệm vụ phân loại văn bản, với trọng tâm trên mô hình LSTM [54]. Dựa trên ULMFiT và các phát hiện của [78], gợi ý rằng các lớp khác nhau nắm bắt thông tin khác nhau và do đó nên được tinh chỉnh ở mức độ khác nhau, chúng tôi áp dụng ý tưởng mở khóa từ từ và điều chỉnh nó cho mô hình ngôn ngữ thưa thớt dựa trên transformer [71].

Cụ thể hơn, chúng tôi tập trung vào mô hình BERT-base phổ biến bao gồm ba nhóm lớp: embedding, 12 khối transformer giống nhau, và một đầu phân loại cụ thể cho nhiệm vụ. Các phiên bản được làm thưa thớt của mô hình này, là mối quan tâm chính của công trình này, tỉa tất cả các lớp tuyến tính trên tất cả các khối transformer, đây là thực hành tiêu chuẩn trong tài liệu [64,42,43,79] và mang lại thỏa thuận độ chính xác-độ trễ tốt nhất [43].

Phương pháp của chúng tôi có thể được tóm tắt như sau. Đối với mỗi nhiệm vụ hạ lưu, chúng tôi bắt đầu từ một mô hình được huấn luyện trước thưa thớt được tạo ra bởi [43] và khởi tạo ngẫu nhiên một đầu phân loại cụ thể cho nhiệm vụ. Sau đó chúng tôi đóng băng tất cả embedding và trọng số tuyến tính được làm thưa thớt, trong khi giữ bias của chúng và các lớp LayerNorm [3] tương ứng không bị đóng băng và có thể huấn luyện. Chúng tôi bắt đầu bằng cách tinh chỉnh chỉ đầu phân loại và tất cả các tham số có thể huấn luyện khác (bias và LayerNorm) trong một epoch, và sau đó theo cùng quy trình từ sau ra trước bằng cách mở khóa trọng số tuyến tính chưa bị tỉa trong các khối transformer trước đó. Sau khi lớp cuối cùng được mở khóa và tinh chỉnh, chúng tôi tiếp tục tinh chỉnh tất cả các lớp cùng nhau trong một epoch nữa.

Cho rằng tại mỗi epoch chúng tôi có một kiến trúc mô hình khác nhau (một khối transformer thưa thớt được mở khóa thêm so với epoch trước), chúng tôi tinh chỉnh nó với tốc độ học giảm tuyến tính và sau đó tua lại về giá trị ban đầu cho epoch tiếp theo. Chúng tôi cũng đã thử lịch trình tốc độ học tam giác xiên được đề xuất trong ULMFiT, nhưng chúng tôi thấy giai đoạn khởi động không hữu ích lắm vì đã biết rằng mô hình ngôn ngữ thưa thớt thường yêu cầu tốc độ học cao hơn nhiều so với đối tác dày đặc của chúng để huấn luyện và hội tụ thành công [42].

Để xác thực hiệu quả của phương pháp chuyển giao thưa thớt được đề xuất, chúng tôi đánh giá nó so với hai kết quả chuyển giao thưa thớt tiên tiến hiện tại được trình bày trong các bài báo Prune Once for All (Prune OFA) [79] và The Optimal BERT Surgeon (oBERT) [43]. Cái trước sử dụng chưng cất kiến thức từ một mô hình giáo viên dày đặc được tinh chỉnh, trong khi cái sau sử dụng cuộc quét đầy đủ trên các công thức chuyển giao dày đặc mở rộng và được tái tỷ lệ, như những cái chúng tôi đã trình bày trong Phần 4.2. Như có thể thấy từ Bảng 7, phương pháp của chúng tôi vượt trội hơn kết quả có tính cạnh tranh cao của Prune OFA trong tất cả, và oBERT trong tám trên mười hai bộ dữ liệu, thiết lập kết quả độ chính xác-độ thưa thớt tiên tiến mới cho nhiều nhiệm vụ trong bộ điểm chuẩn GLUE. Điều đáng nhấn mạnh là tất cả kết quả của chúng tôi được thu được với việc điều chỉnh siêu tham số ít hơn đáng kể so với hai phương pháp cạnh tranh khác, điều này phù hợp với mục tiêu của chúng tôi là tìm một giải pháp ổn định một-kích-thước-phù-hợp-tất-cả cho vấn đề chuyển giao thưa thớt. Chúng tôi tìm kiếm và điều chỉnh tốc độ học ban đầu trong {1e-4, 2e-4, 3e-4}, và dropout trong {0.05, 0.1}, và báo cáo hiệu suất trung bình trên hai lần chạy tốt nhất. Do đó, lưới của chúng tôi bao gồm chỉ 6 kết hợp khác nhau cho mỗi bộ dữ liệu được xem xét, trong khi các phương pháp cạnh tranh quét trên 54 ([79]) và 24 ([43]) kết hợp khác nhau. Điều đáng nhấn mạnh là tất cả các phương pháp được xem xét, bao gồm cả phương pháp của chúng tôi, có biến thiên đáng chú ý trong kết quả trên các bộ dữ liệu nhỏ trên các seed và cấu hình siêu tham số khác nhau, điều này phù hợp với các phát hiện của [12].

Để hiểu rõ hơn về những gì xảy ra trong thiết lập học chuyển giao thưa thớt được đề xuất của chúng tôi, và để phát triển trực quan về tại sao nó có thể cung cấp kết quả ổn định và cạnh tranh trên nhiều bộ dữ liệu khác nhau có kích thước từ 2.4k (RTE) và 392k (MNLI) mẫu được gắn nhãn, chúng tôi trực quan hóa mất mát đánh giá và các chỉ số độ chính xác đánh giá trong toàn bộ quá trình học chuyển giao trong Hình 11 và 12. Như có thể thấy, phương pháp của chúng tôi cho phép học chuyển giao chậm hơn và do đó ổn định hơn trên các bộ dữ liệu mục tiêu, điều này hiệu quả ngăn chặn quá khớp, mặc dù tổng số epoch lớn gấp hai lần so với các công thức chuyển giao dày đặc mở rộng được phân tích trong Phần 4.2. Điều này phù hợp với các phát hiện trong ULMFiT, chứng minh rằng mở khóa từ từ kết hợp với lịch trình tốc độ học được thiết kế cẩn thận ngăn chặn quên thảm khốc và cho phép học chuyển giao mạnh mẽ trên một loạt rộng các nhiệm vụ hạ lưu khác nhau.

5 Kết luận

Trong công trình này, chúng tôi đã kiểm tra tác động của độ thưa thớt cao đối với việc huấn luyện mô hình dưới các kịch bản thị giác máy tính và nhận dạng ngôn ngữ tự nhiên tiêu chuẩn, và cung cấp bằng chứng rằng các công thức huấn luyện truyền thống được sử dụng cho mô hình dày đặc thường quá ngắn đối với huấn luyện thưa thớt. Bắt đầu từ quan sát này, chúng tôi đã có thể tạo ra mô hình tiên tiến cho thị giác máy tính thưa thớt trên hai điểm chuẩn cổ điển cho việc tỉa: điểm chuẩn huấn luyện từ đầu ResNet50/ImageNet, và học chuyển giao từ BERT-base trên một số bộ dữ liệu NLP. Công trình của chúng tôi tập trung vào sự khác biệt giữa động lực huấn luyện thưa thớt và dày đặc và tác động của chúng đối với huấn luyện tối ưu, cung cấp phân tích bổ sung về khó khăn của huấn luyện thưa thớt. Động lực chính cho công trình của chúng tôi là truyền cảm hứng cho nghiên cứu tiếp theo trong việc điều chỉnh lịch trình huấn luyện, siêu tham số, và bộ tối ưu hóa để cải thiện huấn luyện mô hình thưa thớt nhằm đạt được độ chính xác cao hơn dưới độ thưa thớt, mà còn làm điều đó một cách hiệu quả. Chúng tôi để lại điều này như một thách thức cho cộng đồng.

Lời cảm ơn

Chúng tôi trân trọng cảm ơn nguồn tài trợ từ Hội đồng Nghiên cứu Châu Âu (ERC) dưới chương trình Horizon 2020 của Liên minh Châu Âu (thỏa thuận tài trợ số 805223 ScaleML). E.I. được hỗ trợ một phần bởi FWF DK VGSCO, thỏa thuận tài trợ số W1260-N35. D.K. được hỗ trợ bởi Quỹ Khoa học Nga, tài trợ 21-11-00373.
