# 2305.18240.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/optimizer/2305.18240.pdf
# Kích thước tệp: 1644804 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 1
XGrad: Tăng cường các bộ tối ưu hóa dựa trên gradient với
Dự đoán trọng số
Lei Guan, Thành viên, IEEE, Dongsheng Li, Yanqi Shi, Jian Meng
Tóm tắt —Trong bài báo này, chúng tôi đề xuất một khung đào tạo học sâu tổng quát XGrad, giới thiệu dự đoán trọng số vào các bộ tối ưu hóa dựa trên gradient phổ biến để tăng cường sự hội tụ và khái quát hóa của chúng khi đào tạo các mô hình mạng thần kinh sâu (DNN). Cụ thể, trước mỗi lần đào tạo mini-batch, các trọng số tương lai được dự đoán theo quy tắc cập nhật của bộ tối ưu hóa được sử dụng và sau đó được áp dụng cho cả bước truyền tiến và lan truyền ngược. Theo cách này, trong suốt quá trình đào tạo, bộ tối ưu hóa luôn sử dụng các gradient w.r.t. trọng số tương lai để cập nhật các tham số DNN, làm cho bộ tối ưu hóa dựa trên gradient đạt được sự hội tụ và khái quát hóa tốt hơn so với bộ tối ưu hóa gốc không có dự đoán trọng số. XGrad khá đơn giản để triển khai nhưng rất hiệu quả trong việc tăng cường sự hội tụ của các bộ tối ưu hóa dựa trên gradient và độ chính xác của các mô hình DNN. Kết quả thực nghiệm liên quan đến năm bộ tối ưu hóa phổ biến bao gồm SGD với momentum, Adam, AdamW, AdaBelief, và AdaM3 chứng minh tính hiệu quả của đề xuất của chúng tôi. Kết quả thực nghiệm xác nhận rằng XGrad có thể đạt được độ chính xác mô hình cao hơn so với các bộ tối ưu hóa cơ sở khi đào tạo các mô hình DNN. Mã của XGrad sẽ có sẵn tại: https://github.com/guanleics/XGrad.
Từ khóa chỉ mục —Học sâu, dựa trên gradient, bộ tối ưu hóa, hội tụ, khái quát hóa, dự đoán trọng số

I. GIỚI THIỆU
VIỆC đào tạo các mô hình mạng thần kinh sâu (DNN) là tìm kiếm các tham số tối ưu bằng cách sử dụng một bộ tối ưu hóa có ảnh hưởng quyết định đến độ chính xác của các mô hình. Các phương pháp tối ưu hóa dựa trên gradient hiện đang có tầm quan trọng thực tiễn cốt lõi trong học sâu vì chúng có thể đạt được việc đào tạo nhanh chóng các mô hình mạng thần kinh sâu hiện đại. Trong số tất cả các bộ tối ưu hóa dựa trên gradient, gradient descent ngẫu nhiên (SGD) với momentum [1], [2] và các phương pháp thích ứng như Adam [3] và AdamW [4] là những bộ tối ưu hóa phổ biến nhất và đã trở thành lựa chọn mặc định để đào tạo nhiều mô hình DNN bao gồm mạng thần kinh tích chập (CNN) [5]–[8], mạng thần kinh hồi quy (RNN), mạng thần kinh đồ thị (GNN) [9]–[11], mạng đối nghịch sinh (GAN) [12] và nhiều mô hình DNN dựa trên transformer như Trans-

Bản thảo nhận ngày 19 tháng 4 năm 2021; sửa đổi ngày 16 tháng 8 năm 2021.
Công việc này được tài trợ một phần bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc theo Hợp đồng 62025208, và một phần bởi Cơ quan Quản lý Nhà nước về Khoa học, Công nghệ và Công nghiệp cho Quốc phòng theo Hợp đồng WDZC20235250118.
Lei Guan thuộc Khoa Toán học, Đại học Quốc phòng Công nghệ, Trường Sa, Trung Quốc. (E-mail: guanleimath@163.com)
Dongsheng Li và Yanqi Shi thuộc Phòng thí nghiệm Khóa Quốc gia về Tính toán Song song và Phân tán, Đại học Quốc phòng Công nghệ. (E-mails: dsli@nudt.edu.cn,sconcer@outlook.com)
Jian Meng thuộc Khoa Toán học, Đại học Quốc phòng Công nghệ, Trường Sa, Trung Quốc. (E-mail:mengjian23@nudt.edu.cn)
Lei Guan là tác giả liên hệ. former [13], BERT [14], Vision Transformer [15], [16] và GPT-2/3 [17], [18], v.v.

Khi đào tạo một mô hình DNN sử dụng các bộ tối ưu hóa dựa trên gradient, mỗi lần đào tạo mini-batch thường bao gồm một bước truyền tiến và một bước lan truyền ngược, trong đó các gradient w.r.t. tất cả các tham số (cũng được gọi là trọng số) được tính toán trong quá trình lan truyền ngược. Sau đó, các gradient được tạo ra được sử dụng bởi bộ tối ưu hóa để tính toán các giá trị cập nhật cho tất cả các tham số, cuối cùng được áp dụng để cập nhật các trọng số DNN. Chúng tôi minh họa điều này bằng cách sử dụng quá trình đào tạo của mini-batch SGD [19]. Chúng tôi giả sử rằng kích thước mini-batch là b, các trọng số mạng thần kinh có sẵn tại lần lặp thứ t là θt−1, và hàm mất mát là f(·). Với dữ liệu đào tạo mini-batch (xi,yi), bước truyền tiến đầu tiên tính toán mất mát với zi=f(θt−1,xi,yi). Sau đó, các gradient được tạo ra trong quá trình lan truyền ngược có thể được tính toán như
gt=1
bbX
i=1∇θf(θt−1,zi). (1)
Sau đó, với tốc độ học γt, các trọng số DNN được cập nhật thông qua
θt=θt−1−γtgt. (2)

Đối với các phương pháp tối ưu hóa dựa trên gradient, sự khác biệt giữa các phương pháp tối ưu hóa khác nhau nằm ở chỗ cách sử dụng các gradient được tạo ra bởi (1) để cập nhật các tham số mô hình là khác nhau. Để tổng quát, các trọng số được cập nhật bằng θt←θt−1+ ∆θt, trong đó ∆θt biểu thị các gia tăng tương đối của θt so với θt−1 và được tính toán bởi bộ tối ưu hóa được sử dụng.

Hình 1 minh họa việc tối ưu hóa các trọng số DNN. Các đặc điểm đáng chú ý của các bộ tối ưu hóa dựa trên gradient hiện có có thể được tóm tắt như sau. Thứ nhất, việc cập nhật trọng số là liên tục. Thứ hai, mỗi mini-batch sử dụng các trọng số hiện có để thực hiện cả bước truyền tiến và lan truyền ngược. Thứ ba, so với các trọng số hiện tại, các trọng số được cập nhật có xu hướng gần hơn với điểm tối ưu. Nói cách khác, trong mỗi lần đào tạo mini-batch, các trọng số có xu hướng được cập nhật theo hướng "đúng" để di chuyển về phía điểm tối ưu.

Được thúc đẩy bởi thực tế rằng các trọng số DNN được cập nhật theo cách liên tục và các giá trị cập nhật được tính toán bởi mỗi bộ tối ưu hóa dựa trên gradient sẽ phản ánh hướng "đúng" để cập nhật các trọng số, chúng tôi giới thiệu dự đoán trọng số [20], [21] vào việc đào tạo DNN và, cụ thể, đề xuất một khung tổng quát XGrad để tăng cường sự hội tụ của các bộ tối ưu hóa dựa trên gradient và cải thiện độ chính xác của các mô hình DNN. Khung XGrad không chỉ rất đơn giản để arXiv:2305.18240v2 [cs.LG] 7 Apr 2024

--- TRANG 2 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 2
Hình 1. Việc tối ưu hóa các trọng số DNN.

triển khai mà còn hoạt động tốt với nhiều bộ tối ưu hóa học sâu thường được sử dụng. Trước đây, chúng tôi đã khám phá dự đoán trọng số cho việc đào tạo DNN trong một bài báo hội nghị ngắn [22] nơi chúng tôi giới thiệu dự đoán trọng số vào việc đào tạo DNN khi sử dụng AdamW làm bộ tối ưu hóa. Trong bài báo này, chúng tôi nghiên cứu dự đoán trọng số một cách chi tiết và cho phép nó bao quát một lớp lớn các bộ tối ưu hóa dựa trên gradient bao gồm SGD với momentum (SGDM) và các phương pháp thích ứng như RMSprop [23], Adam [3], AdamW [4], AdaBelief [24], và AdaM3 [25].

Nói chung, với các trọng số có sẵn θt và tốc độ học γ, XGrad sử dụng công thức sau để dự đoán các trọng số:
ˆθt+s≈θt−γ·s·∆θt+1, (3)

trong đó ˆθt+s biểu thị các trọng số được dự đoán gần đúng được sử dụng cho cả bước truyền tiến và lan truyền ngược, s được gọi là số bước dự đoán trọng số, và ∆θt có thể được tính toán theo quy tắc cập nhật của các bộ tối ưu hóa được sử dụng. Trong mỗi lần đào tạo mini-batch, XGrad bao gồm ba bước sau:
1) Trước bước truyền tiến, lưu trữ các trọng số hiện tại θt và áp dụng Phương trình (3) để dự đoán các trọng số tương lai dựa trên quy tắc cập nhật cụ thể của bộ tối ưu hóa dựa trên gradient được sử dụng.
2) Sử dụng các trọng số được dự đoán ˆθt+s để thực hiện cả bước truyền tiến và lan truyền ngược, sau đó các gradient w.r.t. các tham số của mỗi lớp được tạo ra.
3) Khôi phục các trọng số được lưu trữ θt và sau đó cập nhật tất cả các tham số với các gradient mới được tạo ra.

Chúng tôi tiến hành các thí nghiệm mở rộng để xác thực tính hiệu quả của đề xuất của chúng tôi. Kết quả thí nghiệm chứng minh rằng XGrad có thể cải thiện độ chính xác mô hình so với bộ tối ưu hóa gốc. Ví dụ, XGrad đạt được trung bình 0,98% cải thiện độ chính xác top-1 so với bộ tối ưu hóa SGDM khi đào tạo trên tập dữ liệu CIFAR-10. So với Adam, XGrad cũng trung bình cải thiện 0,76% độ chính xác và đạt được điểm BLEU cao hơn 0,74 khi đào tạo GNMT-8 trên tập dữ liệu WMT-16 EN →De. Các kết luận tương tự có thể được rút ra khi so sánh XGrad với AdamW, AdaBelief, và AdaM3.

Các đóng góp của bài báo này có thể được tóm tắt như sau:
(1) Chúng tôi, lần đầu tiên, xây dựng mối quan hệ toán học giữa các trọng số hiện có và các trọng số tương lai sau một số lần cập nhật liên tục khi sử dụng sáu bộ tối ưu hóa học sâu phổ biến bao gồm SGDM, RMSprop, Adam, AdamW, AdaBelief, và AdaM3.
(2) Chúng tôi thiết kế một quy trình làm việc tổng quát để tích hợp dự đoán trọng số vào việc đào tạo DNN. Theo hiểu biết tốt nhất của chúng tôi, đây là lần đầu tiên áp dụng chiến lược dự đoán trọng số để tăng cường sự hội tụ và khái quát hóa của các bộ tối ưu hóa dựa trên gradient phổ biến.
(3) Chúng tôi tiến hành đánh giá thực nghiệm mở rộng bằng cách sử dụng 19 mô hình DNN khác nhau trải dài qua các nhiệm vụ phân loại hình ảnh, xử lý ngôn ngữ tự nhiên, và tổng quát hóa hình ảnh để xác thực tính hiệu quả của đề xuất của chúng tôi. Kết quả thí nghiệm chứng minh rằng XGrad hoạt động tốt để tăng cường sự hội tụ và khái quát hóa của cả SGDM và các phương pháp thích ứng như Adam, AdamW, AdaBelief, và AdaM3.

Phần còn lại của bài báo này được tổ chức như sau: Phần II đặt nền tảng cho dự đoán trọng số nơi chúng tôi xây dựng mối quan hệ toán học giữa các trọng số ban đầu và các trọng số tương lai sau s lần cập nhật liên tục khi tương ứng sử dụng SGDM, RMSprop, Adam, AdamW, AdaBelief, và AdaM3 làm phương pháp tối ưu hóa. Phần III tiếp tục xây dựng khung XGrad trên cơ sở của Phần II. Các thí nghiệm mở rộng được tiến hành trong Phần IV để xác thực tính hiệu quả của đề xuất của chúng tôi. Tiếp theo, Phần V trình bày ngắn gọn các công trình nghiên cứu liên quan về các bộ tối ưu hóa dựa trên gradient. Cuối cùng, chúng tôi kết luận bài báo này và thảo luận về công việc trong tương lai trong Phần VI.

II. CÁC BỘ TỐI ƯU HÓA DỰA TRÊN GRADIENT

Trong phần này, chúng tôi xây dựng mối quan hệ toán học giữa các trọng số ban đầu (ký hiệu là θ0) và các trọng số tương lai sau s (s > 1) lần cập nhật liên tục (được gọi là θs) khi đào tạo các mô hình DNN sử dụng các bộ tối ưu hóa dựa trên gradient phổ biến bao gồm SGDM, RMSprop, Adam, AdamW, AdaBelief, và AdaM3.

Trước bất kỳ lần lặp thứ t (t≥1) nào, chúng tôi giả sử rằng các trọng số DNN hiện có là θt−1. Trong suốt bài báo này, chúng tôi luôn để γ (γ∈R) biểu thị tốc độ học và gọi λ (λ∈R) là độ suy giảm trọng số.

A. SGD với momentum
Chúng tôi đầu tiên tái công thức hóa việc cập nhật của SGDM như
θt=θt−1−γ·vt,
s.t.
{
gt=∇θft(θt−1),
gt=gt+λθt−1,
vt=u·vt−1+ (1−τ)·gt.(4)

trong đó u là hệ số momentum và τ là độ giảm chấn cho momentum.

Để θ0 biểu thị các trọng số ban đầu của một mô hình DNN, thì trong s lần đào tạo mini-batch liên tục tiếp theo, các trọng số DNN được cập nhật thông qua
θ1=θ0−γ·v1,
θ2=θ1−γ·v2,
···
θs=θs−1−γ·vs,(5)

trong đó với bất kỳ i∈ {1,2,···, s}, chúng ta có
{
gi=∇θfi(θi−1),
gi=gi+λθi−1,
vi=u·vi−1+ (1−τ)·gi.(6)

--- TRANG 3 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 3

Khi tổng hợp tất cả các phương trình cập nhật trọng số trong (5), chúng ta có
θs=θ0−γsX
i=1vi,
s.t.
{
gi=∇θfi(θi−1),
gi=gi+λθi−1,
vi=u·vi−1+ (1−τ)·gi.(7)

B. RMSprop
Chúng tôi tái công thức hóa việc cập nhật của RMSprop như
θt=θt−1−γ·gt√vt+ϵ,
s.t.
gt=∇θft(θt−1),
vt=α·vt−1+ (1−α)·g2t,(8)

trong đó α biểu thị hằng số làm mịn và ϵ (mặc định: 1e−8) được sử dụng để cải thiện tính ổn định số học.

Tương tự, trong s lần đào tạo mini-batch liên tục đầu tiên, các trọng số DNN được cập nhật thông qua
θ1=θ0−γ·g1√v1+ϵ,
θ2=θ1−γ·g2√v2+ϵ,
···
θs=θs−1−γ·gs√vs+ϵ,(9)

trong đó với bất kỳ i∈ {1,2,···, s}, chúng ta có
{
gi=∇θfi(θi−1),
vi=α·vi−1+ (1−α)·g2i.(10)

Khi tổng hợp tất cả các phương trình cập nhật trọng số trong (9), chúng ta có
θs=θ0−sX
i=1γ·gi√vi+ϵ,
s.t.gi=∇θfi(θi−1),
vi=α·vi−1+ (1−α)·g2i.(11)

C. Adam
Chúng tôi tái công thức hóa việc cập nhật của Adam như
θt=θt−1−γˆmt√ˆvt+ϵ,
s.t.
{
gt=∇θft(θt−1),
mt=β1·mt−1+ (1−β1)·gt,
vt=β2·vt−1+ (1−β2)·g2t,
ˆmt=mt1−βt1,
ˆvt=vt1−βt2.(12)

Trong (12), mt và vt tham chiếu đến trung bình động hàm mũ (EMA) của gt và g2t tương ứng, β1 và β2 là các hệ số được sử dụng để tính toán mt và vt tương ứng, ϵ là thuật ngữ làm mịn có thể ngăn việc chia cho số không. Khi đào tạo các trọng số DNN từ θ0, trong s lần đào tạo mini-batch liên tục tiếp theo, các trọng số DNN được cập nhật thông qua
θ1=θ0−γˆm1√ˆv1+ϵ,
θ2=θ1−γˆm2√ˆv2+ϵ,
···
θs=θs−1−γˆms√ˆvs+ϵ,(13)

trong đó với bất kỳ i∈ {1,2,···, s}, chúng ta có
{
gi=∇θfi(θi−1),
mi=β1·mi−1+ (1−β1)·gi,
vi=β2·vi−1+ (1−β2)·g2i,
ˆmi=mi1−βi1,
ˆvi=vi1−βi2.(14)

Khi tổng hợp tất cả các phương trình cập nhật trọng số trong (13), chúng ta có
θs=θ0−sX
i=1γˆmi√ˆvi+ϵ,
s.t.
{
gi=∇θfi(θi−1),
mi=β1·mi−1+ (1−β1)·gi,
vi=β2·vi−1+ (1−β2)·g2i,
ˆmi=mi1−βi1,
ˆvi=vi1−βi2.(15)

D. AdamW
Với hệ số momentum β1∈R và β2∈R, chúng tôi tái công thức hóa việc cập nhật của AdamW [4] như
θt= (1−γλ)θt−1−γˆmt√ˆvt+ϵ,
s.t.
{
gt=∇θft(θt−1),
mt=β1·mt−1+ (1−β1)·gt,
vt=β2·vt−1+ (1−β2)·g2t,
ˆmt=mt1−βt1,
ˆvt=vt1−βt2.(16)

Tương tự, s lần đào tạo mini-batch liên tục đầu tiên có thể được công thức hóa như
θ1= (1−γλ)θ0−γˆm1√ˆv1+ϵ,
θ2= (1−γλ)θ1−γˆm2√ˆv2+ϵ,
···
θs= (1−γλ)θs−1−γˆms√ˆvs+ϵ,(17)

trong đó với bất kỳ i∈ {1,2,···, s}, chúng ta có
{
gi=∇θfi(θi−1),
mi=β1·mi−1+ (1−β1)·gi,
vi=β2·vi−1+ (1−β2)·g2i,
ˆmi=mi1−βi1,
ˆvi=vi1−βi2.(18)

--- TRANG 4 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 4

Khi tổng hợp tất cả các phương trình cập nhật trọng số trong (17), chúng ta có
θs=θ0−γλs−1X
i=0θi−sX
i=1γˆmi√ˆvi+ϵ,
s.t.
{
gi=∇θfi(θi−1),
mi=β1·mi−1+ (1−β1)·gi,
vi=β2·vi−1+ (1−β2)·g2i,
ˆmi=mi1−βi1,
ˆvi=vi1−βi2.(19)

Như đã biết, giá trị suy giảm trọng số λ thường được đặt thành một giá trị cực kỳ nhỏ (ví dụ: 5e−4), và tốc độ học γ thường được đặt thành một giá trị nhỏ hơn 1 (ví dụ: 1e−3). Do đó, γλ khá gần với số không, và do đó, thuật ngữ thứ hai của phía bên phải của (19) có thể được bỏ qua. Điều này, do đó, tạo ra phương trình sau:
θs≈θ0−sX
i=1γˆmi√ˆvi+ϵ.
s.t.
{
gi=∇θfi(θi−1),
mi=β1·mi−1+ (1−β1)·gi,
vi=β2·vi−1+ (1−β2)·g2i,
ˆmi=mi1−βi1,
ˆvi=vi1−βi2.(20)

E. AdaBelief
Chúng tôi tái công thức hóa việc cập nhật của AdaBelief [24] như
θt=θt−1−γˆmt√ˆvt+ϵ,
s.t.
{
gt=∇θft(θt−1),
mt=β1·mt−1+ (1−β1)·gt,
vt=β2·vt−1+ (1−β2)·(gt−mt)2+ϵ,
ˆmt=mt1−βt1,
ˆvt=vt1−βt2.(21)

Trong (21), mt và vt tham chiếu đến EMA của gt và (gt−mt)2 tương ứng, β1 và β2 là các hệ số được sử dụng để tính toán mt và vt tương ứng, ϵ là thuật ngữ làm mịn có thể ngăn việc chia cho số không.

Khi đào tạo các trọng số DNN từ θ0, trong s lần đào tạo mini-batch liên tục tiếp theo, các trọng số DNN được cập nhật thông qua
θ1=θ0−γˆm1√ˆv1+ϵ,
θ2=θ1−γˆm2√ˆv2+ϵ,
···
θs=θs−1−γˆms√ˆvs+ϵ,(22)

trong đó với bất kỳ i∈ {1,2,···, s}, chúng ta có
{
gi=∇θfi(θi−1),
mi=β1·mi−1+ (1−β1)·gi,
vi=β2·vi−1+ (1−β2)·(gi−mi)2+ϵ,
ˆmi=mi1−βi1,
ˆvi=vi1−βi2.(23)

Khi tổng hợp tất cả các phương trình cập nhật trọng số trong (22), chúng ta có
θs=θ0−sX
i=1γˆmi√ˆvi+ϵ,
s.t.
{
gi=∇θfi(θi−1),
mi=β1·mi−1+ (1−β1)·gi,
vi=β2·vi−1+ (1−β2)·(gi−mi)2+ϵ,
ˆmi=mi1−βi1,
ˆvi=vi1−βi2.(24)

F. AdaM3
Chúng tôi tái công thức hóa việc cập nhật của AdaM3 [25] như
θt=θt−1−γˆmt√ˆvt,
s.t.
{
gt=∇θft(θt−1),
mt=β1·mt−1+ (1−β1)·gt,
vt=β2·vt−1+ (1−β2)·m2t+ϵ,
ˆmt=mt1−βt1,
ˆvt=vt1−βt2.(25)

trong đó η là sự suy giảm tốc độ học và ϵ được sử dụng để cải thiện tính ổn định số học.

Các trọng số DNN được cập nhật thông qua
θ1=θ0−γˆm1√ˆv1,
θ2=θ1−γˆm2√ˆv2,
···
θs=θs−1−γˆms√ˆvs,(26)

trong đó với bất kỳ i∈ {1,2,···, s}, chúng ta có
{
gi=∇θfi(θi−1),
mi=β1·mi−1+ (1−β1)·gi,
vi=β2·vi−1+ (1−β2)·m2i+ϵ,
ˆmi=mi1−βi1,
ˆvi=vi1−βi2.(27)

Tổng hợp tất cả các phương trình trong (26) tiếp tục cho ra
θs=θ0−sX
i=1γˆmi√ˆvi,
s.t.
{
gi=∇θfi(θi−1),
mi=β1·mi−1+ (1−β1)·gi,
vi=β2·vi−1+ (1−β2)·m2i+ϵ,
ˆmi=mi1−βi1,
ˆvi=vi1−βi2.(28)

III. KHUNG XGRAD

A. Suy ra XGrad
Khi tóm tắt các Phương trình (7), (11), (15), (20), (24), và (28) trong Phần II, chúng ta có thể ngay lập tức đạt đến một kết luận chung. Đó là, với các trọng số ban đầu θ0, các trọng số

--- TRANG 5 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 5

sau s lần cập nhật liên tục có thể được tính toán gần đúng thông qua
θs=θ0−γsX
i=1∆θi, (29)

trong đó ∆θi đại diện cho các gia tăng tương đối của θi so với θi−1. Trong mỗi lần lặp, các gradient của mục tiêu ngẫu nhiên w.r.t. mini-batch thứ i, tức là gi=∇θfi(θi−1), có thể được tính toán sau khi hoàn thành quá trình lan truyền ngược. Với gi, người ta có thể dễ dàng tính toán ∆θi theo quy tắc cập nhật của phương pháp tối ưu hóa được sử dụng.

Phương trình (29) minh họa rằng với các trọng số ban đầu θ0, θs được tính toán bằng cách để θ0 trừ đi tốc độ học nhân với tổng của s biến thiên tương đối liên tục của các trọng số. Tương ứng, với θt, các trọng số của bất kỳ lần lặp thứ t (t≥1) nào, các trọng số sau s lần cập nhật liên tục có thể được tính toán gần đúng thông qua
θt+s=θt−γt+sX
i=t+1∆θi. (30)

Phương trình (30) minh họa rằng θt+s có thể được tính toán bằng cách để θt trừ đi tổng của s biến thiên tương đối liên tục của các trọng số khi θt có sẵn. Lưu ý rằng khi sử dụng một bộ tối ưu hóa dựa trên gradient hiệu quả, các gia tăng tương đối của các trọng số trong mỗi lần lặp sẽ phản ánh xu hướng cập nhật trọng số. Do đó, ∆θi trong (30) sẽ phản ánh hướng "đúng" để cập nhật các trọng số θi−1 vì ∆θi được tính toán trên cơ sở của quy tắc cập nhật của bộ tối ưu hóa và vì các trọng số được cập nhật theo cách liên tục và dọc theo đường hướng quán tính.

Do đó, chúng ta có thể thay thế Pt+s i=t+1∆θi trong (30) bằng s·∆θt+1 nhằm tính toán gần đúng θt+s cho trường hợp khi chỉ có θt và ∆θt+1 có sẵn. Để ˆθt+s biểu thị các trọng số được dự đoán gần đúng cho θt+s, chúng ta có công thức dự đoán trọng số của XGrad như sau.
ˆθt+s≈θt−γ·s·∆θt+1. (31)

Đáng chú ý rằng khi sử dụng Phương trình (31) làm công thức dự đoán trọng số, s biểu thị bước dự đoán trọng số có thể được đặt thủ công và ∆θt+1 có thể được tính toán dễ dàng theo loại bộ tối ưu hóa được sử dụng (được gọi là bộ tối ưu hóa cơ sở). Bảng I tóm tắt các công thức toán học của ∆θt+1 cho SGDM, RMSprop, Adam, AdamW, AdaBelief, và AdaM3. Ở đây, chúng tôi lưu ý rằng XGrad trực tiếp sử dụng các trạng thái bộ tối ưu hóa được lưu trữ của bước lặp thứ t để tính toán ∆θt+1 nhằm tránh tính toán các gradient khi thực hiện dự đoán trọng số. Đáng chú ý, các bộ tối ưu hóa dựa trên gradient khác như AdaGrad [26], AdaBound [27], RAdam [28], và Lion [29] có thể được tích hợp dễ dàng vào khung được đề xuất.

B. Quy trình làm việc của XGrad
Trong phần sau, chúng tôi trình bày cách tích hợp dự đoán trọng số vào việc đào tạo DNN. Thuật toán 1 minh họa các chi tiết về việc đào tạo các mô hình DNN sử dụng khung XGrad. Các bước dự đoán trọng số s và các siêu tham số khác được yêu cầu trước khi việc đào tạo DNN bắt đầu. Tại mỗi lần lặp, các trọng số hiện có θt sẽ được lưu trữ trước

BẢNG I
TÍNH TOÁN ∆θt+1 CHO SGDM, RMS PROP, ADAM, ADAM W, ADABELIEF, VÀ ADAM3.

Bộ tối ưu hóa ∆θt+1
SGDM ∆θt+1=vt,
s.t.
gt=∇θft(θt−1),
vt=u·vt−1+ (1−τ)·gt.
RMSprop* ∆θt+1=gt√vt+ϵ,
s.t.gt=∇θft(θt−1),
vt=α·vt−1+ (1−α)·g2t.
Adam* ∆θt+1=ˆmt√ˆvt+ϵ,
s.t.
{
gt=∇θft(θt−1),
mt=β1·mt−1+ (1−β1)·gt,
vt=β2·vt−1+ (1−β2)·g2t,
ˆmt=mt1−βt1,
ˆvt=vt1−βt2.
AdamW* ∆θt+1=ˆmt√ˆvt+ϵ,
s.t.
{
gt=∇θft(θt−1),
mt=β1·mt−1+ (1−β1)·gt,
vt=β2·vt−1+ (1−β2)·g2t,
ˆmt=mt1−βt1,
ˆvt=vt1−βt2.
AdaBelief ∆θt+1=ˆmt√ˆvt+ϵ,
s.t.
{
gt=∇θft(θt−1),
mt=β1·mt−1+ (1−β1)·gt,
vt=β2·vt−1+ (1−β2)·(gt−mt)2+ϵ,
ˆmt=mt1−βt1,
ˆvt=vt1−βt2.
AdaM3 ∆θt+1=ˆmt√ˆvt,
s.t.
{
gt=∇θft(θt−1),
mt=β1·mt−1+ (1−β1)·gt,
vt=β2·vt−1+ (1−β2)·m2t+ϵ,
ˆmt=mt1−βt1,
ˆvt=vt1−βt2.

*g2i tham chiếu đến bình phương theo từng phần tử với g2i=gi⊙gi.

Thuật toán 1 Quy trình làm việc của XGrad
Yêu cầu: Bước dự đoán trọng số s, các siêu tham số khác như γ,β1,β2,ϵ.
1: Khởi tạo hoặc tải các trọng số DNN θ0.
2: t←0.
3: while tiêu chí dừng chưa được thỏa mãn do
4: Lưu trữ các trọng số hiện tại θt.
5: Tính toán ∆θt+1 theo Bảng I.
6: Tính toán ˆθt+s sử dụng (31).
7: Thực hiện bước truyền tiến với ˆθt+s.
8: Thực hiện lan truyền ngược với ˆθt+s.
9: Khôi phục các trọng số được lưu trữ θt.
10: Cập nhật các trọng số θt sử dụng bộ tối ưu hóa cụ thể.
11: t←t+ 1.
12: end while

khi bước truyền tiến bắt đầu (Dòng 4). Sau đó, các gia tăng tương đối của θt+1 so với θt được tính toán theo quy tắc cập nhật của bộ tối ưu hóa cơ sở được sử dụng bằng cách tra cứu nhanh Bảng I (Dòng 5). Sau đó, dự đoán trọng số được thực hiện bằng cách sử dụng Phương trình (31) để tính toán ˆθt+s (Dòng 6). Tiếp theo, các

--- TRANG 6 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 6

trọng số được dự đoán ˆθt+s được áp dụng trực tiếp cho cả bước truyền tiến và lan truyền ngược (Dòng 7 và 8). Sau đó, các trọng số được lưu trữ θt cần được khôi phục (Dòng 9). Cuối cùng, các trọng số được cập nhật bởi bộ tối ưu hóa sử dụng các gradient được tạo ra trong quá trình lan truyền ngược (Dòng 10).

C. Phân tích XGrad
Trong phần này, chúng tôi đưa ra một phân tích ngắn gọn về lý do tại sao XGrad nên hiệu quả hơn so với các bộ tối ưu hóa cơ sở không có dự đoán trọng số. Điểm mốt quan trọng là XGrad có thể được coi là một xấp xỉ của phương pháp extragradient (EG) [30] nơi bước dự đoán trọng số đóng vai trò của bước ngoại suy.

Chúng tôi xem xét việc tối ưu hóa của Phương trình (1). Tại lần lặp thứ (t+1), EG cập nhật các trọng số với
˜θt=θt−γ· ∇θft(θt),
θt+1=θt−γ· ∇θft(˜θt),(32)

trong đó γ là tốc độ học và việc tính toán ˜θt được gọi là bước ngoại suy.

Trong khi đó, với bất kỳ lần lặp thứ (t+1) nào, XGrad thực sự thực hiện việc cập nhật sau:
ˆθt≈θt−γ·s·∆θt+1,
θt+1=θt−γ∇θft(ˆθt).(33)

Khi so sánh các Phương trình (32) và (33), chúng ta có thể thấy rằng ˜θt và ˆθt rất giống nhau trong các công thức tính toán của chúng. Sự khác biệt chính giữa EM và XGrad nằm ở việc tính toán ˜θt và ˆθt. EG tính toán ˜θt bằng cách trực tiếp thực hiện một lần gradient descent. Ngược lại, XGrad tính toán ˆθt bằng cách chủ yếu sử dụng ∆θt+1, được tính toán theo quy tắc cập nhật của bộ tối ưu hóa được sử dụng và cũng tích hợp thông tin gradient.

Tương tự như ˜θt, việc tính toán ˆθt có thể được coi là một bước ngoại suy trước khi thực hiện việc cập nhật θt. Kết quả là, chúng ta có thể thấy rằng XGrad có thể được hiểu là một phương pháp tiếp cận gần đúng với EG, nơi việc thực hiện dự đoán trọng số có thể được xem như một bước ngoại suy trong EG. Như đã biết, EG có thể giải quyết nhiều vấn đề không hội tụ làm khổ các gradient descent nhờ vào bước ngoại suy [31]. Do đó, rất tự nhiên khi mong đợi rằng XGrad sẽ hiệu quả hơn so với các bộ tối ưu hóa cơ sở không có dự đoán trọng số.

Ở đây, chúng tôi cũng lưu ý rằng EG yêu cầu tính toán hai gradient để cập nhật các trọng số mô hình θt. Ngược lại, XGrad chỉ yêu cầu tính toán một lần gradient vì ∆θt+1 có thể được tính toán trực tiếp bằng cách sử dụng các trạng thái bộ tối ưu hóa được lưu trữ (ví dụ: vt cho SGDM trong Bảng I) của lần lặp trước.

IV. THÍ NGHIỆM

A. Cài đặt thí nghiệm
1) Môi trường: Tất cả các thí nghiệm được tiến hành trên hai máy. Máy đầu tiên là một nền tảng đa GPU được trang bị bốn GPU NVIDIA Tesla P100, mỗi GPU có 16 GB dung lượng bộ nhớ. Máy thứ hai bao gồm một GPU NVIDIA Tesla M40 với 24 GB dung lượng bộ nhớ. CPU của cả hai máy là Intel Xeon E5-2680 với 128GB bộ nhớ chính DDR4-2400 off-chip. Chúng tôi sử dụng PyTorch (v1.12.0) để đào tạo tất cả các mô hình DNN.

2) Các mô hình chuẩn: Để đánh giá thí nghiệm, chúng tôi sử dụng 18 mô hình chuẩn khác nhau trải dài qua các nhiệm vụ phân loại hình ảnh, xử lý ngôn ngữ tự nhiên (NLP), và tổng quát hóa hình ảnh. Đối với các nhiệm vụ phân loại hình ảnh, chúng tôi sử dụng chín mô hình CNN và Vision Transformer (ViT) [15] làm các mô hình DNN chuẩn. Các mô hình CNN được đánh giá bao gồm LeNet [32], AlexNet [8], VGG-11 [5], VGG-16, ResNet-34 [7], ResNet-101, GoogleNet [6], DenseNet-121 [33], và Inception-V3 [34]. Mô hình ViT được cấu hình với 6 khối transformer và kích thước patch là 4. Số lượng head trong lớp multi-head attention là 8 và chiều của lớp MLP là 512. Cả tỷ lệ dropout và tỷ lệ embedding dropout đều được đặt thành 0.1. Các nhiệm vụ NLP bao gồm ba phần. Đối với nhiệm vụ mô hình hóa ngôn ngữ, chúng tôi đào tạo các mô hình LSTM với 1, 2, và 3 lớp (ký hiệu là LSTM-1, LSTM-2, và LSTM-3). Mỗi lớp được cấu hình với kích thước embedding là 400 và 1.150 kích hoạt ẩn. Đối với nhiệm vụ dịch máy, chúng tôi đánh giá GNMT [35] với 8 lớp LSTM và 16 lớp LSTM (tương ứng ký hiệu là GNMT-8 và GNMT-16). Hơn nữa, chúng tôi cũng sử dụng BERT [14] với 12 khối Transformer, 768 kích thước ẩn, và 12 khối self-attention (ký hiệu là BERT BASE) làm mô hình chuẩn NLP. Đối với các nhiệm vụ tổng quát hóa hình ảnh, chúng tôi sử dụng V AE [36] và Wassertein-GAN (WAGN) [37] làm các mô hình chuẩn được đánh giá.

3) Tập dữ liệu: Chúng tôi sử dụng ba tập dữ liệu hình ảnh cho các nhiệm vụ phân loại hình ảnh, bao gồm Fashion-MNIST [38], CIFAR-10 [39], và CIFAR-100. Đối với các nhiệm vụ NLP, các tập dữ liệu được sử dụng bao gồm Penn Treebank Dataset [40], WMT-16 [41] English-to-German (WMT En →De), và Microsoft Research Paraphrase Corpus (MRPC). Chúng tôi lưu ý rằng MRPC thuộc về các chuẩn General Language Understanding Evaluation (GLUE) [42]. Đối với các nhiệm vụ tổng quát hóa hình ảnh, chúng tôi sử dụng MNIST và CIFAR-10 làm các tập dữ liệu chuẩn. Bảng II tóm tắt tất cả các mô hình DNN được đánh giá và các tập dữ liệu được sử dụng trong các thí nghiệm.

BẢNG II
TÓM TẮT CÁC MÔ HÌNH DNN ĐƯỢC ĐÁNH GIÁ VÀ CÁC TẬP DỮ LIỆU ĐƯỢC SỬ DỤNG TRONG CÁC THÍ NGHIỆM.

Nhiệm vụ Mô hình Tập dữ liệu
Phân loại
Hình ảnh LeNet FashionMNIST
Alexnet, VGG-11, VGG-16,
ResNet-34, ResNet-101, GoogleNet,
DenseNet-121, Inception-V3, ViT CIFAR-10,
CIFAR-100
NLP LSTM-1, LSTM-2, LSTM-3 Penn Treebank
GNMT-8, GNMT-16 WMT-16 En →De
BERT BASE MRPC
Tổng quát hóa
Hình ảnh V AE MNIST
WGAN CIFAR-10

--- TRANG 7 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 7

4) Cài đặt so sánh: Để xác thực tính hiệu quả của đề xuất của chúng tôi, chúng tôi chủ yếu so sánh XGrad với năm bộ tối ưu hóa được sử dụng rộng rãi bao gồm SGDM, Adam, AdamW, AdaBelief, và AdaM3. Chúng tôi đánh giá đề xuất của mình với bốn bước dự đoán trọng số khác nhau (tức là, s= 1, s= 2, s= 3, và s= 4), trong đó chúng tôi tương ứng ký hiệu chúng là XGrad-S1, XGrad-S2, XGrad-S3, và XGrad-S4 để thuận tiện. Chúng tôi chia các so sánh thành năm nhóm, đó là, SGDM vs. XGrad, Adam vs. XGrad, AdamW vs. XGrad, AdaBelief vs. XGrad, và AdaM3 vs. XGrad. Bảng III tóm tắt các cài đặt so sánh. Đáng chú ý rằng XGrad tự động chọn bộ tối ưu hóa cơ sở theo loại bộ tối ưu hóa được so sánh. Ví dụ, khi so sánh với SGDM, XGrad tự động chọn SGDM làm bộ tối ưu hóa cơ sở và có nghĩa là SGDM với dự đoán trọng số. Khi so sánh với Adam, AdamW, AdaBelief, và AdaM3, XGrad tương ứng có nghĩa là Adam, AdamW, AdaBelief, và AdaM3 với dự đoán trọng số, tương ứng.

BẢNG III
TÓM TẮT CÁC NHÓM SO SÁNH.

Số So sánh Bộ tối ưu hóa cơ sở của XGrad
1 SGDM vs. XGrad-S1/S2/S3/S4 SGDM
2 Adam vs. XGrad-S1/S2/S3/S4 Adam
3 AdamW vs. XGrad-S1/S2/S3/S4 AdamW
4 AdaBelief vs. XGrad-S1/S2/S3/S4 AdaBelief
5 AdaM3 vs. XGrad-S1/S2/S3/S4 AdaM3

5) Đo lường độ chính xác mô hình: Đối với các nhiệm vụ phân loại hình ảnh, độ chính xác mô hình có nghĩa là độ chính xác top-1 (cao hơn thì tốt hơn). Đối với việc đào tạo LSTM-1, LSTM-2, và LSTM-3, độ chính xác mô hình được ký hiệu bởi perplexity (thấp hơn thì tốt hơn). Đối với việc đào tạo các mô hình GNMT, độ chính xác mô hình tham chiếu đến điểm BLEU (cao hơn thì tốt hơn). Đối với việc đào tạo BERT BASE trên MRPC, độ chính xác mô hình được định nghĩa là độ chính xác tập Dev. Đối với việc đào tạo V AE, chúng tôi sử dụng tổng lỗi tái tạo và phân kỳ Kullback-Leibler, ký hiệu là tổng mất mát (thấp hơn thì tốt hơn), để đánh giá độ chính xác mô hình. Đối với việc đào tạo WGAN, chúng tôi tính toán điểm Frechet Inception Distance (FID) (thấp hơn thì tốt hơn) giữa các hình ảnh giả và tập dữ liệu thực để đánh giá các mô hình sinh.

B. Hội tụ và khái quát hóa
1) So sánh XGrad và SGDM: Trong phần này, chúng tôi báo cáo các kết quả thí nghiệm khi đào tạo tám mô hình CNN trên các tập dữ liệu CIFAR-10 và CIFAR-100 tương ứng. Đối với tất cả các mô hình CNN và ViT, chúng tôi đào tạo chúng trong 200 epoch với kích thước mini-batch là 128. Chúng tôi khởi tạo tốc độ học với 1e−2 và giảm nó đi 10 lần tại epoch thứ 120 và 150. Đối với cả XGrad và SGDM, chúng tôi đặt momentum với 0.9 và suy giảm trọng số với 5e−4.

Hình 2 trình bày các đường cong học tập để đào tạo các mô hình CNN và ViT trên tập dữ liệu CIFAR-10. Bảng IV tóm tắt độ chính xác top-1 xác thực tối đa đạt được. Hình 3 hiển thị các đường cong học tập để đào tạo các mô hình CNN trên tập dữ liệu CIFAR-100. Bảng V tóm tắt độ chính xác top-1 xác thực tối đa đạt được trên CIFAR-100. Trong tất cả các hình của Hình 2 và 3, chúng tôi để các đường màu đỏ minh họa các đường cong học tập của SGDM và để các đường màu khác đại diện cho các đường cong học tập của XGrad với các bước dự đoán trọng số khác nhau.

Dựa trên việc quan sát Bảng IV và Hình 2, chúng ta có thể ngay lập tức đạt đến các kết luận sau. Thứ nhất, các đường cong học tập của XGrad với các bước dự đoán trọng số khác nhau có xu hướng thấp hơn so với SGDM ở giai đoạn đầu của quá trình đào tạo. Tuy nhiên, khi tốc độ học suy giảm, XGrad dần dần vượt qua SGDM và luôn đạt được độ chính xác top-1 cao hơn so với SGDM ở cuối giai đoạn đào tạo. Điều này chứng minh rằng XGrad có xu hướng kém hơn SGDM với tốc độ học lớn nhưng vượt trội hơn SGDM với tốc độ học nhỏ. Thứ hai, Bảng IV minh họa rằng XGrad vượt trội hơn SGDM trên tất cả các mô hình DNN được đánh giá về độ chính xác xác thực top-1 đạt được. Cụ thể, XGrad đạt được độ chính xác xác thực top-1 cao hơn một cách nhất quán so với SGDM. So với SGDM, XGrad tương ứng đạt được sự cải thiện 0.42%, 1.81%, 1.43%, 1.00%, 0.57%, 0.53%, 0.52%, 0.92%, và 1.60% độ chính xác top-1 khi đào tạo AlexNet, VGG-11, VGG-16, ResNet-34, ResNet-101, GoogleNet, DenseNet-121, Inception-V3, và ViT. Trung bình, đề xuất của chúng tôi mang lại 0.98% (lên đến 1.81%) cải thiện độ chính xác top-1 so với SGDM. Thứ ba, khi so sánh các kết quả thí nghiệm của XGrad-S1, XGrad-S2, XGrad-S3, và XGrad-S4, chúng ta có thể thấy rằng XGrad với các bước dự đoán trọng số khác nhau luôn đạt được kết quả tốt cho tất cả các mô hình CNN. Trường hợp đặc biệt là đào tạo ViT trên CIFAR-10, nơi XGrad-S4 hoạt động kém và đạt được độ chính xác top-1 tối thiểu.

Chúng ta có thể rút ra những kết luận tương tự từ việc quan sát các kết quả thí nghiệm được hiển thị trong Bảng V và Hình 3. Thứ nhất, các đường cong học tập của SGDM dường như cao hơn so với XGrad ở đầu quá trình đào tạo nhưng thấp hơn so với XGrad ở cuối quá trình đào tạo. Thứ hai, Bảng V minh họa rằng XGrad đạt được độ chính xác xác thực top-1 cao hơn một cách nhất quán so với SGDM. Đặc biệt, XGrad mang lại 0.86%, 4.84%, 3.40%, 2.63%, 2.29%, 2.18%, 2.59%, 2.79%, và 5.54% độ chính xác top-1 cao hơn so với SGDM khi đào tạo AlexNet, VGG-11, VGG-16, ResNet-34, ResNet-101, GoogleNet, DenseNet-121 và Inception-V3, tương ứng. Nghĩa là, XGrad có thể đạt được trung bình 3.01% (lên đến 5.54%) cải thiện độ chính xác top-1 so với SGDM. Thứ ba, khi so sánh các kết quả thí nghiệm của XGrad-S1, XGrad-S2, XGrad-S3, và XGrad-S4, chúng ta có thể thấy rằng XGrad luôn đạt được hiệu suất cao tương đương.

2) So sánh XGrad với Adam: Trong phần này, chúng tôi chọn LeNet, ResNet-34, DenseNet-121, Inception-V3, LSTM-1, LSTM-2, LSTM-3, GNMT-8, BERT BASE, V AE, và WGAN làm các mô hình chuẩn. Cụ thể, chúng tôi đào tạo LeNet trên Fashion-MNIST trong 50 epoch với tốc độ học cố định là 1e−3 và kích thước mini-batch là 128. Chúng tôi đào tạo ResNet-34, DenseNet-121, và Inception-V3 trên CIFAR-10 trong 120 epoch với kích thước mini-batch là 128. Tốc độ học ban đầu là 1e−3 và được chia cho 10 tại epoch thứ 90. Chúng tôi đào tạo LSTM-1, LSTM-2, và LSTM-3 trên Penn Treebank Dataset trong 200 epoch với kích thước mini-batch là 20. Tốc độ học ban đầu là 1e−3 và được suy giảm 0.1 tại epoch thứ 100 và 145. Chúng tôi đào tạo GNMT-8 trên WMT-16 En→De trong 6 epoch với tốc độ học ổn định là 3e−4 và

--- TRANG 8 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 8

[Hình 2-3 và các bảng IV-V chứa các biểu đồ và bảng dữ liệu về kết quả thí nghiệm so sánh XGrad với SGDM trên các tập dữ liệu CIFAR-10 và CIFAR-100]

kích thước mini-batch là 128. Chúng tôi đào tạo BERT BASE trên MRPC trong 3 epoch với tốc độ học là 2e−5. Chúng tôi đào tạo V AE trên MNIST trong 50 epoch với tốc độ học là 1e−3 và kích thước mini-batch là 128. Chúng tôi đào tạo WGAN trong 100 epoch với tốc độ học là 2e−4. Khi đào tạo WGAN với XGrad, chúng tôi chỉ áp dụng dự đoán trọng số cho mô hình sinh và không áp dụng cho mô hình phân biệt. Hơn nữa, đối với cả XGrad và Adam, trừ khi được đề cập đặc biệt, chúng tôi luôn

--- TRANG 9 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 9

[Hình 4 và bảng VI chứa các biểu đồ và bảng dữ liệu về kết quả thí nghiệm so sánh XGrad với Adam]

đánh giá chúng với các cài đặt mặc định, tức là γ= 1e−3 và (β1, β2)=(0.9, 0.999). Đối với việc đào tạo các mô hình LSTM, chúng tôi đặt λ= 1.2e−4 và ϵ= 1e−12. Đối với việc đào tạo WGAN, chúng tôi đặt (β1, β2) = (0.5,0.999), ϵ= 1e−12.

Hình 4 minh họa các đường cong học tập về độ chính xác mô hình vs. epoch. Bảng VI tóm tắt độ chính xác mô hình tốt nhất đạt được. Kết quả thí nghiệm xác minh tính hiệu quả của XGrad trong việc tăng cường sự hội tụ và khái quát hóa của Adam. Thứ nhất, Hình 4a, 4b, 4c, và 4d xác thực tính ưu việt của XGrad so với Adam khi đào tạo các mô hình CNN. XGrad luôn có xu hướng đạt được độ chính xác cao hơn so với Adam khi đào tạo chúng với cùng số epoch. Hình 4e, 4f, và 4g minh họa các hiện tượng tương tự. XGrad có xu hướng thể hiện giá trị perplexity nhỏ hơn với sự tăng của các epoch. Các đường cong học tập được mô tả trong Hình 4h và 4i cho thấy rằng XGrad có xu hướng đạt được điểm BLEU cao hơn và tổng mất mát thấp hơn so với Adam. Kết quả thí nghiệm được hiển thị trong Bảng VI một lần nữa xác thực tính hiệu quả của XGrad. So với Adam, XGrad tương ứng đạt được sự cải thiện độ chính xác 0.71%, 0.85%, 1.00%, và 0.47% khi đào tạo LeNet, ResNet-34, DenseNet-121, và Inception-V3. Điều này có nghĩa là XGrad dẫn đến trung bình 0.76% cải thiện độ chính xác so với Adam. Trong khi đó, XGrad đạt được 0.20, 0.57, và 0.46 perplexity thấp hơn so với Adam khi đào tạo LSTM-1, LSTM-

--- TRANG 10 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 10

[Tiếp tục mô tả các kết quả thí nghiệm và so sánh với các bộ tối ưu hóa khác]

2, và LSTM-3, tương ứng. XGrad cũng đạt được điểm BLEU cao hơn 0.74 so với Adam khi đào tạo GNMT-8 và đạt được 0.49% độ chính xác cao hơn so với Adam khi đào tạo BERT BASE. Hơn nữa, XGrad đạt được 1.37 tổng mất mát thấp hơn so với Adam khi đào tạo V AE và cũng đạt được điểm FID thấp hơn nhiều khi đào tạo WGAN (72.90 vs. 95.60).

3) So sánh XGrad với AdamW: Trong phần này, chúng tôi một lần nữa chọn LeNet, ResNet-34, DenseNet-121, Inception-V3, LSTM-1, LSTM-2, LSTM-3, GNMT-8, BERT BASE, V AE, và WGAN làm các mô hình chuẩn và đánh giá chúng với các cài đặt thí nghiệm tương tự như được mô tả trong Phần IV-B2. Hình 5 mô tả các đường cong học tập về độ chính xác mô hình vs. epoch. Bảng VII tóm tắt độ chính xác mô hình tốt nhất đạt được.

Chúng ta có thể đạt đến các kết luận sau dựa trên việc quan sát các kết quả thí nghiệm. Thứ nhất, XGrad hoạt động tốt hơn so với AdamW trên các nhiệm vụ phân loại hình ảnh. Trung bình, XGrad mang lại 0.46% (lên đến 0.62%) cải thiện độ chính xác top-1 so với AdamW. Thứ hai, XGrad cũng vượt trội hơn AdamW trên các nhiệm vụ NLP. XGrad hoạt động tốt hơn một chút so với AdamW trên các nhiệm vụ mô hình hóa ngôn ngữ. Cụ thể, XGrad kém hơn AdamW khi đào tạo LSTM-1 (88.39 vs.

--- TRANG 11 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 11

[Tiếp tục mô tả kết quả thí nghiệm với AdamW, AdaBelief và AdaM3]

88.28) nhưng đạt được 0.12 và 0.18 perplexity thấp hơn so với AdamW khi đào tạo LSTM-2 và LSTM-3, tương ứng. XGrad cũng đạt được điểm BLEU cao hơn (24.54 so với 24.14) khi đào tạo GNMT-8 và đạt được 1.72% độ chính xác cao hơn so với AdamW khi đào tạo BERT BASE trên MRPC. Thứ ba, đối với cả các nhiệm vụ phân loại hình ảnh và NLP, bước dự đoán trọng số có ảnh hưởng nhẹ đến độ chính xác mô hình được tạo ra bởi XGrad. Tuy nhiên, hiệu suất của XGrad thay đổi mạnh với các bước dự đoán trọng số khác nhau khi đào tạo các mô hình DNN phức tạp như BERT BASE và WGAN.

4) So sánh XGrad và AdaBelief: Trong phần này, chúng tôi so sánh XGrad với AdaBelief sử dụng 11 mô hình DNN khác nhau bao gồm LeNet, AlexNet, VGG-11, ResNet-34, DenseNet-121, LSTM-1, LSTM-2, LSTM-3, BERT BASE, V AE, và WGAN. Hình 6 hiển thị các đường cong học tập về độ chính xác mô hình vs. epoch. Bảng VIII liệt kê độ chính xác mô hình tốt nhất đạt được.

Các đường cong học tập được mô tả trong Hình 6 một lần nữa xác thực tính hiệu quả của XGrad. Với cùng số epoch đào tạo, XGrad luôn có xu hướng đạt được độ chính xác mô hình tốt hơn so với AdaBelief. Khi đào tạo LeNet, AlexNet, VGG-11, ResNet-34, và DenseNet-121, XGrad tương ứng mang lại sự cải thiện độ chính xác top-1 là 0.84%, 0.50%, -0.21%, 0.09%, và 0.11% so với AdaBelief, dẫn đến trung bình 0.27% cải thiện độ chính xác top-1. Khi đào tạo LSTM-1, LSTM-2, và LSTM-3, XGrad luôn đạt được perplexity thấp hơn so với AdaBelief, trung bình 0.41 perplexity thấp hơn so với AdaBelief. Đối với việc đào tạo BERT BASE trên MRPC, XGrad đạt được 0.98% độ chính xác tập Dev cao hơn so với AdaBelief. Hơn nữa, so với AdaBelief, XGrad tương ứng đạt được 1.29 tổng mất mát thấp hơn khi đào tạo V AE và 2.69 điểm FID thấp hơn khi đào tạo WGAN.

5) So sánh XGrad và AdaM3: Như với Phần IV-B4, chúng tôi một lần nữa chọn LeNet, AlexNet, VGG-11, ResNet-34, DenseNet-121, LSTM-1, LSMT-2, LSTM-3, BERT BASE, V AE, và WGAN làm các mô hình DNN chuẩn. Hình 7 mô tả các đường cong học tập về độ chính xác mô hình vs. epoch và Bảng IX tóm tắt độ chính xác tốt nhất đạt được.

Các kết luận tương tự có thể được rút ra từ việc quan sát các kết quả thí nghiệm được hiển thị trong Hình 7 và Bảng IX. So với AdaM3, XGrad tạo ra sự cải thiện độ chính xác top-1 là 0.89%, 0.29%, 0.13%, -0.01%, và 0.12% khi đào tạo LeNet, AlexNet, VGG-11, ResNet-34, và DenseNet-121, tương ứng. Điều này dẫn đến sự cải thiện độ chính xác top-1 trung bình là 0.28%. Trên các nhiệm vụ mô hình hóa ngôn ngữ,

--- TRANG 12 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 12

[Tiếp tục mô tả kết quả thí nghiệm và thảo luận về hiệu quả của XGrad]

XGrad đạt được perplexity thấp hơn so với AdaM3 khi đào tạo LSTM-1 và LSTM-2 và hơi kém hơn AdaM3 khi đào tạo LSTM-3. Đặc biệt khi đào tạo BERT BASE, XGrad đạt được sự cải thiện độ chính xác đáng kể so với AdaM3 (68.38 vs. 80.64). Trên các nhiệm vụ tổng quát hóa hình ảnh, XGrad đạt được 14.51 tổng mất mát thấp hơn khi đào tạo V AE và 22.29 điểm FID thấp hơn khi đào tạo WGAN. Một lần nữa, kết quả thí nghiệm chứng minh rằng độ chính xác mô hình đạt được bởi XGrad thay đổi với các cài đặt bước dự đoán trọng số, đặc biệt khi đào tạo WGAN.

6) Thảo luận về tính hiệu quả của XGrad: Kết quả thí nghiệm chứng minh tính hiệu quả của XGrad trong việc tăng cường sự hội tụ và khái quát hóa của các bộ tối ưu hóa dựa trên gradient bao gồm SGDM, Adam, AdamW, AdaBelief, và AdaM3. Trong đại đa số các trường hợp, XGrad có xu hướng đạt được độ chính xác mô hình tốt hơn so với các bộ tối ưu hóa cơ sở trên tất cả các mô hình DNN được đánh giá, xác minh rằng XGrad có thể giúp phương pháp tối ưu hóa đào tạo để đạt được các tham số DNN tối ưu hơn.

Tuy nhiên, việc áp dụng dự đoán trọng số không có nghĩa là phương pháp tối ưu hóa luôn có thể di chuyển dọc theo các quỹ đạo tối ưu. Tóm tắt các kết quả thí nghiệm, chúng ta thấy rằng hiệu suất của XGrad thay đổi với việc lựa chọn các bước dự đoán trọng số và các nhiệm vụ được đánh giá. Các bước dự đoán trọng số khác nhau có thể dẫn đến độ chính xác mô hình khá khác nhau,

--- TRANG 13 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 13

[Tiếp tục thảo luận về hiệu quả và chi phí tính toán của XGrad]

và một số lựa chọn kích thước bước thậm chí có thể dẫn đến độ chính xác mô hình thấp hơn so với bộ tối ưu hóa cơ sở. Ví dụ, khi so sánh XGrad với Adam với mô hình LSTM-1, XGrad-S1 đạt được perplexity thấp hơn so với Adam trong khi XGrad với kích thước bước 2, 3, và 4 hoạt động tệ hơn so với Adam (xem Bảng VI). Hơn nữa, kết quả thí nghiệm cũng chứng minh rằng trên các nhiệm vụ phức tạp (ví dụ: BERT BASE và WGAN), hiệu suất của XGrad thay đổi mạnh khi lựa chọn các bước dự đoán trọng số khác nhau. Điều này tiết lộ rằng việc áp dụng dự đoán trọng số dễ dàng làm sai lệch phương pháp tối ưu hóa khi đào tạo trên các nhiệm vụ phức tạp.

Chúng tôi lưu ý rằng tính ưu việt của XGrad so với các bộ tối ưu hóa cơ sở được xác thực thông qua các đánh giá thí nghiệm mở rộng. Đối với hầu hết các trường hợp, XGrad có thể đạt được độ chính xác mô hình tốt hơn so với bộ tối ưu hóa cơ sở không có dự đoán trọng số. Việc nghiên cứu kích thước bước dự đoán trọng số tối ưu và tác động của các nhiệm vụ đào tạo phức tạp lên hiệu suất dự đoán trọng số sẽ là trọng tâm chính của nghiên cứu trong tương lai.

C. Chi phí tính toán
Trong phần này, chúng tôi đánh giá chi phí tính toán của XGrad. Chúng tôi chủ yếu so sánh XGrad với SGDM và Adam. Đối với việc so sánh XGrad và SGDM, chúng tôi chọn VGG-16, ResNet-34, ResNet-101, GoogleNet, DenseNet-121, Inception-V3, và ViT làm các mô hình chuẩn. Đối với việc so sánh XGrad và Adam, chúng tôi chọn ResNet-34, DenseNet-121, Inception-V3, ViT, LSTM-1, LSTM-2, và GNMT-8 làm các mô hình chuẩn. Đối với việc so sánh XGrad và

--- TRANG 14 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 14

[Tiếp tục mô tả chi phí tính toán và tiêu thụ bộ nhớ]

SGDM, chúng tôi đào tạo tất cả các mô hình DNN trên CIFAR-10 với các cài đặt thí nghiệm tương tự được mô tả trong Phần IV-B1. Đối với việc so sánh XGrad và Adam, chúng tôi đánh giá tất cả các mô hình DNN với các cài đặt thí nghiệm tương tự được mô tả trong Phần IV-B2. Bảng X báo cáo thời gian chạy của việc đào tạo 1-epoch khi so sánh XGrad với SGDM. Bảng XI tóm tắt các kết quả thí nghiệm của việc so sánh XGrad và Adam.

Các kết quả thí nghiệm được hiển thị trong cả Bảng X và XI chứng minh rằng XGrad luôn phát sinh thời gian đào tạo dài hơn so với bộ tối ưu hóa dựa trên gradient gốc. XGrad phát sinh trung bình 5.67% (lên đến 12.48%) thời gian đào tạo dài hơn so với SGDM. So với Adam, XGrad dẫn đến 11.21% thời gian đào tạo dài hơn trung bình. Các kết quả thí nghiệm có thể hiểu được vì việc thực hiện dự đoán trọng số trong mỗi lần lặp dẫn đến các tính toán bổ sung so với các bộ tối ưu hóa gốc. Tuy nhiên, chúng ta thấy rằng các tính toán tăng thêm khá hạn chế và đôi khi thậm chí có thể được bỏ qua.

D. Tiêu thụ bộ nhớ
Trong phần này, chúng tôi đánh giá mức tiêu thụ bộ nhớ của XGrad. Chúng tôi chủ yếu so sánh mức tiêu thụ bộ nhớ GPU đỉnh của XGrad với SGDM và Adam. Tương tự, các so sánh được chia thành hai nhóm: XGrad vs. SGDM và XGrad vs. Adam. Đối với việc so sánh XGrad vs. SGDM, chúng tôi đánh giá bảy mô hình CNN trên CIFAR-10 bao gồm VGG-16, ResNet-34, ResNet-101, GoogleNet, DenseNet-121, Inception-V3, và ViT. Đối với việc so sánh XGrad vs. Adam, các mô hình DNN được đánh giá bao gồm ResNet-34, DenseNet-121, Inception-V3, ViT, LSTM-2, và GNMT-8. Các cài đặt thí nghiệm giống như trong Phần IV-C.

Hình 8 minh họa mức tiêu thụ bộ nhớ GPU đỉnh của XGrad và các bộ tối ưu hóa cơ sở bao gồm SGDM và Adam. Từ việc quan sát Hình 8a, chúng ta có thể ngay lập tức đạt đến các kết luận sau. Thứ nhất, đối với bất kỳ mô hình được đánh giá nào, XGrad luôn tiêu thụ gần như cùng (chỉ hơi nhiều hơn) bộ nhớ GPU như SGDM. Trung bình, XGrad chỉ phát sinh 1.8% (lên đến 3.0%) bộ nhớ GPU nhiều hơn so với SGDM. Chúng ta có thể rút ra những kết luận tương tự từ các kết quả thí nghiệm được hiển thị trong Hình 8b. XGrad dẫn đến trung bình 5.0% (lên đến 9.7%) mức tiêu thụ bộ nhớ GPU nhiều hơn so với Adam.

Nhìn chung, các kết quả thí nghiệm được hiển thị trong Hình 8 tiết lộ rằng XGrad phát sinh mức tiêu thụ bộ nhớ GPU hơi nhiều hơn so với các bộ tối ưu hóa cơ sở gốc. Điều này hợp lý vì XGrad yêu cầu lưu trữ nhiều biến trung gian hơn trong bộ nhớ GPU để đạt được dự đoán trọng số. Tuy nhiên, chúng tôi lưu ý rằng mức tiêu thụ bộ nhớ GPU tăng thêm khá hạn chế.

V. CÔNG TRÌNH LIÊN QUAN
Nghiên cứu về các phương pháp tối ưu hóa luôn là một điểm nóng trong lĩnh vực học sâu vì đặc tính hội tụ của bộ tối ưu hóa được sử dụng ảnh hưởng trực tiếp đến sự hội tụ và độ chính xác của mô hình. Nói chung, quy trình đào tạo của một mô hình DNN được xây dựng dựa trên lan truyền ngược [43] nơi mỗi lớp trọng số mạng thần kinh được tinh chỉnh dọc theo sự lan truyền ngược của "lỗi". Lan truyền ngược luôn đi cùng với gradient descent. Hiện tại, các phương pháp gradient bậc nhất, như SGDM [2] và các phương pháp thích ứng [3], [24], [44] là các phương pháp tối ưu hóa học sâu được sử dụng rộng rãi nhất.

SGDM đã được coi là bộ tối ưu hóa mặc định, đặc biệt đối với các nhiệm vụ phân loại hình ảnh [5]–[8]. Thuật ngữ momentum của SGDM tích lũy thông tin gradient từ tất cả các lần lặp trước, làm cho việc cập nhật trọng số dọc theo hướng quán tính. Một đặc điểm đáng chú ý khác của SGDM là tốc độ học thống nhất cho tất cả các tham số được sử dụng trong suốt toàn bộ giai đoạn đào tạo.

Nhận thấy rằng tốc độ học đóng vai trò quan trọng trong việc đào tạo DNN, nhiều nhà nghiên cứu chuyển sang nghiên cứu các phương pháp thích ứng (còn được gọi là các phương pháp học thích ứng), tính toán một tốc độ học cụ thể cho từng tham số riêng lẻ. Vào năm 2011, Duchi et al. [26] đề xuất AdaGrad, điều chỉnh tốc độ học một cách động theo các gradient lịch sử từ các lần lặp trước và sử dụng tổng bình phương của tất cả các gradient trước để cập nhật các tham số mô hình. Zeiler [44] đề xuất AdaDelta, tìm cách giảm thiểu sự suy giảm liên tục của tốc độ học của AdaGrad. AdaDelta không yêu cầu điều chỉnh thủ công tốc độ học và mạnh mẽ với thông tin gradient nhiễu. Tieleman và Hinton [23] tinh chỉnh AdaGrad và đề xuất RMSprop. Giống như AdaGrad, RMSprop điều chỉnh tốc độ học thông qua tính toán theo từng phần tử và sau đó cập nhật các biến. Một đặc điểm đáng chú ý của RMSprop là nó có thể tránh suy giảm tốc độ học quá nhanh. Để kết hợp các ưu điểm của cả AdaGrad và RMSprop, Kingma và Ba [3] đề xuất một phương pháp gradient thích ứng nổi tiếng khác, Adam, đã trở thành lựa chọn quan trọng nhất để đào tạo nhiều mô hình DNN dựa trên Transformer [3], [4], [23]. Loshchilov và Hutter [4] phát hiện ra rằng yếu tố chính của khái quát hóa kém của Adam là do chuẩn hóa L2 cho nó không hiệu quả bằng đối thủ cạnh tranh SGDM. Họ tiếp tục đề xuất chuẩn hóa suy giảm trọng số tách rời cho Adam, còn được gọi là AdamW. Các kết quả thí nghiệm chứng minh rằng AdamW cải thiện đáng kể hiệu suất khái quát hóa của Adam và minh họa hiệu suất cạnh tranh như SGDM [2] khi giải quyết các nhiệm vụ phân loại hình ảnh. Hiện tại, Adam cũng như AdamW đã trở thành bộ tối ưu hóa mặc định cho việc đào tạo DNN. Để đồng thời đạt được sự hội tụ nhanh và khái quát hóa tốt, Zhuang et al. [24] đề xuất một phương pháp gradient thích ứng khác gọi là AdaBelief, thích ứng kích thước bước theo "niềm tin" vào hướng gradient hiện tại. Nhận thấy rằng ước tính moment thứ hai của Adam là một lựa chọn thuận lợi hơn cho việc mở rộng tốc độ học so với gradient thô, Wang et al. [25] đề xuất AdaMomentum. Reddi et al. [45] nghiên cứu lý do tại sao RMSProp và Adam có thể không hội tụ đến một giải pháp tối ưu và đề xuất một biến thể trung bình động hàm mũ mới: AMSGrad. Xem xét rằng gradient tăng tốc của Nesterov [46] có giới hạn tốt hơn so với gradient descent, Timothy Dozat tích hợp momentum Nestorv vào Adam và đề xuất một phương pháp tối ưu hóa mới tên là NAdam [47]. Các phương pháp thích ứng được đề xuất khác bao gồm Yogi [48], AdaBound [27], RAdam [28], v.v. Chen et al.[49] nghiên cứu sự hội tụ của các phương pháp gradient thích ứng bao gồm các thuật toán phổ biến như Adam, AMSGrad, và AdaGrad.

Đáng chú ý, SGDM cũng như tất cả các phương pháp thích ứng chia sẻ một đặc điểm chung: việc cập nhật trọng số được thực hiện theo cách liên tục và mỗi lần đào tạo mini-batch luôn sử dụng các trọng số hiện có để thực hiện cả bước truyền tiến và lan truyền ngược. Dự đoán trọng số trước đây được sử dụng để khắc phục các vấn đề không nhất quán trọng số và lạc hậu trọng số trong song song pipeline bất đồng bộ. Chen et al. [21] sử dụng gradient làm mịn để thay thế gradient thực để dự đoán trọng số tương lai khi sử dụng SGDM [2] làm bộ tối ưu hóa. Guan et al. [20] đề xuất sử dụng các giá trị cập nhật của Adam [3] để thực hiện dự đoán trọng số. Tuy nhiên, cả hai phương pháp đều sử dụng dự đoán trọng số để đảm bảo học tập tham số hiệu quả trong việc đào tạo pipeline bất đồng bộ thay vì nghiên cứu tác động của dự đoán trọng số lên các bộ tối ưu hóa.

VI. KẾT LUẬN
Để tiếp tục tăng cường sự hội tụ và sinh sản của các bộ tối ưu hóa dựa trên gradient phổ biến, trong bài báo này, chúng tôi giới thiệu dự đoán trọng số vào việc đào tạo DNN và đề xuất một khung đào tạo DNN mới gọi là XGrad. Đề xuất của chúng tôi có thể cải thiện sự hội tụ và khái quát hóa của các bộ tối ưu hóa dựa trên gradient phổ biến với chi phí hơi tăng thời gian đào tạo và tiêu thụ bộ nhớ GPU. Đặc điểm đáng chú ý của đề xuất của chúng tôi là chúng tôi thực hiện cả bước truyền tiến và lan truyền ngược sử dụng các trọng số tương lai được dự đoán

--- TRANG 16 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 16

theo quy tắc cập nhật cụ thể của bộ tối ưu hóa được sử dụng. Cụ thể, chúng tôi xây dựng mối quan hệ toán học giữa các trọng số hiện có và trọng số tương lai và thiết kế một cách hiệu quả để tích hợp dự đoán trọng số vào việc đào tạo DNN. Khung được đề xuất bao phủ nhiều bộ tối ưu hóa được sử dụng thường xuyên nhất như SGDM, RMSprop, Adam, AdamW, AdaBelief, và AdaM3.

XGrad dễ triển khai và hoạt động tốt trong việc tăng cường sự hội tụ của việc đào tạo DNN. Các kết quả thí nghiệm mở rộng trên các nhiệm vụ phân loại hình ảnh, xử lý ngôn ngữ tự nhiên, và tổng quát hóa hình ảnh xác minh tính hiệu quả của đề xuất của chúng tôi. Chúng tôi tin rằng các phương pháp gradient thích ứng khác như AdaBound, RAdam, Yogi, Lion [29], v.v. có thể được tích hợp dễ dàng vào khung được đề xuất của chúng tôi.

Đối với công việc trong tương lai, chúng tôi muốn khám phá cơ chế nội tại của dự đoán trọng số từ góc độ toán học. Hơn nữa, chúng tôi sẽ nghiên cứu mối quan hệ giữa các bước dự đoán trọng số và tốc độ học, cũng như tác động của chúng lên sự hội tụ và khái quát hóa của việc đào tạo DNN, đặc biệt khi đào tạo trên các nhiệm vụ phức tạp như BERT và GAN.

LỜI CẢM ơN
Lei Guan cảm ơn Kangkang Deng tại Đại học Quốc phòng Công nghệ vì các cuộc thảo luận kích thích về phương pháp extragradient và mối quan hệ của nó với XGrad. Lei Guan cảm ơn Shaofeng Zhang tại Ant Group vì đã giúp chạy chuẩn BERT.

TÀI LIỆU THAM KHẢO
[1] N. Qian, "On the momentum term in gradient descent learning algorithms," Neural networks, vol. 12, no. 1, pp. 145–151, 1999.
[2] I. Sutskever, J. Martens, G. Dahl, and G. Hinton, "On the importance of initialization and momentum in deep learning," in International conference on machine learning. PMLR, 2013, pp. 1139–1147.
[3] D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," arXiv preprint arXiv:1412.6980, 2014.
[4] I. Loshchilov and F. Hutter, "Decoupled weight decay regularization," arXiv preprint arXiv:1711.05101, 2017.
[5] K. Simonyan and A. Zisserman, "Very deep convolutional networks for large-scale image recognition," arXiv preprint arXiv:1409.1556, 2014.
[6] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, "Going deeper with convolutions," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 1–9.
[7] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770–778.
[8] A. Krizhevsky, I. Sutskever, and G. E. Hinton, "Imagenet classification with deep convolutional neural networks," Communications of the ACM, vol. 60, no. 6, pp. 84–90, 2017.
[9] Z. Li, K. Meidani, P. Yadav, and A. Barati Farimani, "Graph neural networks accelerated molecular dynamics," The Journal of Chemical Physics, vol. 156, no. 14, p. 144103, 2022.
[10] C. Chen, W. Ye, Y. Zuo, C. Zheng, and S. P. Ong, "Graph networks as a universal machine learning framework for molecules and crystals," Chemistry of Materials, vol. 31, no. 9, pp. 3564–3572, 2019.
[11] K. Choudhary and B. DeCost, "Atomistic line graph neural network for improved materials property predictions," npj Computational Materials, vol. 7, no. 1, p. 185, 2021.
[12] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, "Generative adversarial networks," Communications of the ACM, vol. 63, no. 11, pp. 139–144, 2020.
[13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," Advances in neural information processing systems, vol. 30, 2017.
[14] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018.
[15] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., "An image is worth 16x16 words: Transformers for image recognition at scale," arXiv preprint arXiv:2010.11929, 2020.
[16] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao, C. Xu, Y. Xu et al., "A survey on vision transformer," IEEE transactions on pattern analysis and machine intelligence, vol. 45, no. 1, pp. 87–110, 2022.
[17] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., "Language models are unsupervised multitask learners," OpenAI blog, vol. 1, no. 8, p. 9, 2019.
[18] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., "Language models are few-shot learners," Advances in neural information processing systems, vol. 33, pp. 1877–1901, 2020.
[19] L. Bottou, F. E. Curtis, and J. Nocedal, "Optimization methods for large-scale machine learning," SIAM review, vol. 60, no. 2, pp. 223–311, 2018.
[20] L. Guan, W. Yin, D. Li, and X. Lu, "Xpipe: Efficient pipeline model parallelism for multi-gpu dnn training," arXiv preprint arXiv:1911.04610, 2019.
[21] C.-C. Chen, C.-L. Yang, and H.-Y. Cheng, "Efficient and robust parallel dnn training through model parallelism on multi-gpu platform," arXiv preprint arXiv:1809.02839, 2018.
[22] L. Guan, "Weight prediction boosts the convergence of adamw," in 27th Pacific-Asia Conference on Knowledge Discovery and Data Mining, vol. 13935, 2023, pp. 329–340.
[23] T. Tieleman and G. Hinton, "Lecture 6.5-rmsprop, coursera: Neural networks for machine learning," University of Toronto, Technical Report, vol. 6, 2012.
[24] J. Zhuang, T. Tang, Y. Ding, S. C. Tatikonda, N. Dvornek, X. Papademetris, and J. Duncan, "Adabelief optimizer: Adapting stepsizes by the belief in observed gradients," Advances in neural information processing systems, vol. 33, pp. 18795–18806, 2020.
[25] Y. Wang, Y. Kang, C. Qin, H. Wang, Y. Xu, Y. Zhang, and Y. Fu, "Rethinking adam: A twofold exponential moving average approach," arXiv preprint arXiv:2106.11514, 2021.
[26] J. Duchi, E. Hazan, and Y. Singer, "Adaptive subgradient methods for online learning and stochastic optimization." Journal of machine learning research, vol. 12, no. 7, 2011.
[27] L. Luo, Y. Xiong, Y. Liu, and X. Sun, "Adaptive gradient methods with dynamic bound of learning rate," arXiv preprint arXiv:1902.09843, 2019.
[28] L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han, "On the variance of the adaptive learning rate and beyond," arXiv preprint arXiv:1908.03265, 2019.
[29] X. Chen, C. Liang, D. Huang, E. Real, K. Wang, Y. Liu, H. Pham, X. Dong, T. Luong, C.-J. Hsieh et al., "Symbolic discovery of optimization algorithms," arXiv preprint arXiv:2302.06675, 2023.
[30] G. M. Korpelevich, "The extragradient method for finding saddle points and other problems," Matecon, vol. 12, pp. 747–756, 1976.
[31] Y.-G. Hsieh, F. Iutzeler, J. Malick, and P. Mertikopoulos, "Explore aggressively, update conservatively: Stochastic extragradient methods with variable stepsize scaling," Advances in Neural Information Processing Systems, vol. 33, pp. 16223–16234, 2020.
[32] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, "Gradient-based learning applied to document recognition," Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998.
[33] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, "Densely connected convolutional networks," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 4700–4708.
[34] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, "Rethinking the inception architecture for computer vision," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 2818–2826.
[35] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey et al., "Google's neural machine translation system: Bridging the gap between human and machine translation," arXiv preprint arXiv:1609.08144, 2016.
[36] D. P. Kingma and M. Welling, "Auto-encoding variational bayes," arXiv preprint arXiv:1312.6114, 2013.
[37] M. Arjovsky, S. Chintala, and L. Bottou, "Wasserstein generative adversarial networks," in International conference on machine learning. PMLR, 2017, pp. 214–223.

--- TRANG 17 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 17

[38] H. Xiao, K. Rasul, and R. Vollgraf, "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms," arXiv preprint arXiv:1708.07747, 2017.
[39] A. Krizhevsky, G. Hinton et al., "Learning multiple layers of features from tiny images," 2009.
[40] M. A. Marcinkiewicz, "Building a large annotated corpus of english: The penn treebank," Using Large Corpora, vol. 273, 1994.
[41] R. Sennrich, B. Haddow, and A. Birch, "Edinburgh neural machine translation systems for wmt 16," arXiv preprint arXiv:1606.02891, 2016.
[42] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, "Glue: A multi-task benchmark and analysis platform for natural language understanding," arXiv preprint arXiv:1804.07461, 2018.
[43] G. E. Hinton, S. Osindero, and Y.-W. Teh, "A fast learning algorithm for deep belief nets," Neural computation, vol. 18, no. 7, pp. 1527–1554, 2006.
[44] M. D. Zeiler, "Adadelta: an adaptive learning rate method," arXiv preprint arXiv:1212.5701, 2012.
[45] S. J. Reddi, S. Kale, and S. Kumar, "On the convergence of adam and beyond," in International Conference on Learning Representations.
[46] Y. E. Nesterov, "A method of solving a convex programming problem with convergence rate o(1/k2)," in Doklady Akademii Nauk, vol. 269, no. 3. Russian Academy of Sciences, 1983, pp. 543–547.
[47] T. Dozat, "Incorporating nesterov momentum into adam," 2016.
[48] M. Zaheer, S. Reddi, D. Sachan, S. Kale, and S. Kumar, "Adaptive methods for nonconvex optimization," Advances in neural information processing systems, vol. 31, 2018.
[49] X. Chen, S. Liu, R. Sun, and M. Hong, "On the convergence of a class of adam-type algorithms for non-convex optimization," arXiv preprint arXiv:1808.02941, 2018.
