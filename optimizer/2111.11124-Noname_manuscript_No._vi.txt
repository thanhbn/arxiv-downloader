# 2111.11124.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/optimizer/2111.11124.pdf
# Kích thước tệp: 1110423 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Bản thảo không tên số.
(sẽ được chèn bởi biên tập viên)
Mesa: Một Framework Huấn luyện Tiết kiệm Bộ nhớ cho Transformers
Zizheng Pan, Peng Chen, Haoyu He, Jing Liu, Jianfei Cai
và Bohan Zhuang y
Tóm tắt Đã có sự bùng nổ quan tâm trong việc thiết kế các Transformers hiệu suất cao. Trong khi Transformers đã mang lại những cải tiến hiệu suất đáng kể, việc huấn luyện các mạng như vậy cực kỳ tốn bộ nhớ do phải lưu trữ tất cả các kích hoạt trung gian cần thiết cho việc tính toán gradient trong quá trình lan truyền ngược, đặc biệt là với các chuỗi dài. Để giải quyết vấn đề này, chúng tôi trình bày Mesa, một framework huấn luyện tiết kiệm bộ nhớ cho Transformers. Cụ thể, Mesa sử dụng các kích hoạt chính xác trong quá trình truyền tiến trong khi lưu trữ phiên bản độ chính xác thấp của các kích hoạt để giảm tiêu thụ bộ nhớ trong quá trình huấn luyện. Các kích hoạt độ chính xác thấp sau đó được giải lượng tử hóa trong quá trình lan truyền ngược để tính toán gradient. Bên cạnh đó, để giải quyết các phân phối kích hoạt không đồng nhất trong các lớp tự chú ý nhiều đầu, chúng tôi đề xuất một chiến lược lượng tử hóa kích hoạt theo từng đầu, lượng tử hóa các kích hoạt dựa trên thống kê của từng đầu để giảm thiểu lỗi xấp xỉ. Để tăng cường hiệu quả huấn luyện, chúng tôi học các tham số lượng tử hóa bằng ước lượng chạy. Quan trọng hơn, bằng cách tái đầu tư bộ nhớ tiết kiệm được vào việc sử dụng kích thước batch lớn hơn hoặc mở rộng quy mô mô hình, chúng tôi có thể cải thiện hiệu suất hơn nữa dưới tài nguyên tính toán bị hạn chế. Các thí nghiệm mở rộng trên ImageNet, CIFAR-100 và ADE20K cho thấy Mesa có thể đạt được tiết kiệm bộ nhớ linh hoạt (lên đến 50%) trong quá trình huấn luyện trong khi đạt được hiệu suất tương đương hoặc thậm chí tốt hơn. Mã nguồn có sẵn tại https://github.com/ziplab/Mesa .

Từ khóa Vision Transformers, Huấn luyện Hiệu quả

Zizheng Pan, Peng Chen, Haoyu He, Jing Liu, Jianfei Cai và Bohan Zhuang
Khoa Công nghệ Thông tin, Đại học Monash
E-mail: {zizeng.pan, haoyu.he, jing.liu1, jianfei.cai, bohan.zhuang}@monash.edu, blueardour@gmail.com
yTác giả liên hệ.

1 Giới thiệu
Transformers đã chứng minh được thành công đáng kinh ngạc trong một loạt các nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) [1,2] và thị giác máy tính (CV) [3-5]. Lấy cảm hứng từ các nghiên cứu trước đây về mở rộng quy mô mô hình [6, 7], các nghiên cứu gần đây về Transformers tiếp tục đẩy hiệu suất tiến lên với kích thước mô hình ngày càng tăng [3, 8, 9]. Tuy nhiên, huấn luyện các mô hình Transformer đòi hỏi một lượng bộ nhớ khổng lồ, cấm các người dùng thông thường với tài nguyên tính toán hạn chế thực hiện các nghiên cứu liên quan. Ví dụ, huấn luyện Swin-T [9] với kích thước batch 128 trên hình ảnh 224×224 tiêu thụ ít nhất 12G bộ nhớ GPU, trong khi Swin-B không thể vừa với GPU V100 32GB dưới cùng cài đặt. Do đó, chỉ một số ít bên có thể đủ khả năng huấn luyện các mô hình lớn như vậy. Tiêu thụ bộ nhớ khổng lồ và sự bất bình đẳng tài nguyên ngày càng tăng khiến cộng đồng học thuật khó theo kịp lĩnh vực này, dựa trên thực tế rằng hầu hết các Transformers tiên tiến gần đây đều được phát triển với sự tham gia của ngành công nghiệp.

May mắn thay, đã có những nỗ lực lớn để huấn luyện các mạng nơ-ron sâu với bộ nhớ thấp. Ví dụ, các kỹ thuật đại diện hiện tại bao gồm thưa thớt [11], huấn luyện độ chính xác thấp [10], micro-batching [12], checkpointing [13] và lượng tử hóa gradient [14]. Trong số các kỹ thuật tiết kiệm bộ nhớ hiện có, một hướng đầy hứa hẹn là huấn luyện nén kích hoạt (ACT) [15]. Cụ thể, ACT lưu trữ các bản sao xấp xỉ độ chính xác thấp của kích hoạt tại mỗi lớp trong khi tính toán quá trình truyền tiến chính xác, điều này giúp giảm tổng tiêu thụ bộ nhớ trong quá trình huấn luyện. Các kích hoạt được lưu sau đó được giải lượng tử hóa về độ chính xác gốc trong quá trình truyền ngược để tính toán gradient. Phương pháp này đã được áp dụng thành công để huấn luyện các biến thể ResNet [6]. Tuy nhiên, các nghiên cứu hiện có về ACT [15-17] hoặc tập trung vào các kiến trúc chuyên dụng hoặc nhắm mục tiêu cụ thể vào các

--- TRANG 2 ---
2 Z. Pan, P. Chen, H. He, J. Liu, J. Cai, B. Zhuang

[Hình 1 - Biểu đồ so sánh dấu chân bộ nhớ trong quá trình huấn luyện với một số vision Transformers hiện đại trên ImageNet]

mạng nơ-ron tích chập (CNN). Hiện tại, không có tài liệu nào về việc nén các phép toán thường được sử dụng trong Transformers (ví dụ: Softmax, GELU [18] và LayerNorm [19]). Hơn nữa, không có nghiên cứu nào trước đây xem xét các kích hoạt phân phối không đồng nhất trong các lớp tự chú ý nhiều đầu (MSA), đặc biệt là các sự chú ý. Do đó, chúng tôi quan sát thực nghiệm rằng việc áp dụng chiến lược phân nhóm kích hoạt ngây thơ như trong các nghiên cứu trước [16] cho các bản đồ chú ý sẽ khiến Transformers không thể hội tụ. Do đó, không có framework ACT hiện có nào có thể được áp dụng trực tiếp cho các mô hình dựa trên Transformer. Nhắm mục tiêu các phép toán phổ biến trong Transformer vẫn còn thách thức và hiệu quả vẫn chưa được biết.

Để giải quyết các thách thức trên, chúng tôi trình bày Mesa, một framework huấn luyện tiết kiệm bộ nhớ 8-bit cho Transformers. Mesa bao phủ tất cả các phép toán tốn bộ nhớ trong Transformers, bao gồm phép nhân ma trận (MatMul), Softmax, LayerNorm và GELU. Hơn nữa, chúng tôi đề xuất một chiến lược lượng tử hóa kích hoạt theo từng đầu, lượng tử hóa các kích hoạt dựa trên từng đầu tự chú ý. Động lực đến từ hai khía cạnh. Thứ nhất, lượng tử hóa theo nhóm, xuất phát từ lượng tử hóa sản phẩm [20], đã được chứng minh là hiệu quả trong việc giảm thiểu lỗi lượng tử hóa. Lượng tử hóa theo đầu được đề xuất có thể được xem như một trường hợp đặc biệt của lượng tử hóa theo nhóm trong đó số lượng nhóm bằng số lượng đầu tự chú ý. Thứ hai, các nghiên cứu trước đã tiết lộ rằng các đầu tự chú ý khác nhau trong Transformers có xu hướng học các mẫu chú ý khác nhau [21]. Thực nghiệm, chúng tôi cũng có thể quan sát sự khác biệt lớn về thống kê giữa các đầu khác nhau như được hiển thị trong Hình 4. Do đó, việc sử dụng các tham số lượng tử hóa chung giữa các đầu tự chú ý có thể dẫn đến ước lượng các đại lượng thống kê bị suy giảm nghiêm trọng.

Bên cạnh đó, Mesa học các tham số lượng tử hóa với ước lượng chạy hiệu quả trong quá trình huấn luyện. Phương pháp như vậy mang lại lợi ích bổ sung so với các phương pháp dựa trên gradient [22, 23] hoặc thống kê từng mẫu [16] vì nó tránh chi phí tính toán và bộ nhớ bổ sung để học các tham số lượng tử hóa. Trong thực tế, chúng tôi cũng quan sát thấy rằng ước lượng chạy hoạt động tốt hơn so với thống kê từng mẫu về cả tốc độ huấn luyện và hiệu suất.

Theo hiểu biết tốt nhất của chúng tôi, Mesa là framework ACT đầu tiên cho các mô hình dựa trên Transformer. Nó cũng trực giao với các kỹ thuật tiết kiệm bộ nhớ khác như huấn luyện độ chính xác thấp [10] và checkpointing [13]. Do đó, trong thực tế, người ta có thể đồng thời áp dụng huấn luyện độ chính xác hỗn hợp tự động (AMP) và checkpointing cùng với Mesa để đạt được tiết kiệm bộ nhớ nhiều hơn. Như một sản phẩm phụ của việc giảm bộ nhớ đáng kể trong quá trình huấn luyện, chúng tôi có thể sử dụng kích thước batch lớn hơn và/hoặc huấn luyện mô hình Transformer lớn hơn để cho phép tận dụng đầy đủ các lõi GPU có sẵn. Hơn nữa, chúng tôi có thể tái đầu tư bộ nhớ tiết kiệm được trong quá trình huấn luyện bằng cách xây dựng DeiT-B sâu hơn 3.3× hoặc rộng hơn 2.2×, hoặc huấn luyện DeiT-B với độ phân giải hình ảnh lớn hơn 1.5×. Lưu ý rằng như trong thực tế thông thường [13, 16, 24], có sự đánh đổi giữa chi phí bộ nhớ và tốc độ huấn luyện cho Mesa. Trong trường hợp này, Mesa được thiết kế để nén linh hoạt các phép toán/module khác nhau để đạt được tiết kiệm bộ nhớ mục tiêu với chi phí huấn luyện có thể chấp nhận được. Hơn nữa, đối với một số lượng lớn các nhà nghiên cứu chỉ có GPU cấp nhập cảnh/trung cấp, điều đầu tiên của họ là Transformer có thể chạy trên máy của họ ngay cả với các mô hình cỡ nhỏ, trước khi xem xét throughput. Để kết thúc, Mesa mang lại cho cộng đồng là một nghiên cứu toàn diện về việc tùy chỉnh ACT cho Transformers, cùng với một module plug-and-play hiệu quả và hiệu suất với triển khai CUDA có thể dễ dàng được sử dụng trong nhiều mô hình Transformer để tiết kiệm bộ nhớ linh hoạt. Tóm lại, chúng tôi tóm tắt đóng góp như sau:

1. Chúng tôi đề xuất một framework huấn luyện tiết kiệm bộ nhớ 8-bit cho Transformers, gọi là Mesa. Mesa được triển khai với kernel CUDA nhanh và có thể dễ dàng thích ứng với bất kỳ dự án Transformer nào.

2. Chúng tôi quan sát các kích hoạt phân phối không đồng nhất trong các đầu tự chú ý. Để giải quyết điều này, chúng tôi đề xuất một chiến lược lượng tử hóa kích hoạt theo từng đầu để giảm thiểu lỗi xấp xỉ trong các lớp MSA. Bên cạnh đó, chúng tôi sử dụng ước lượng chạy để học các tham số lượng tử hóa, hoạt động tốt với chi phí bổ sung không đáng kể.

3. Các thí nghiệm mở rộng trên ImageNet, CIFAR-100 và ADE20K đã chứng minh rằng Mesa có thể giảm 50% dấu chân bộ nhớ trong quá trình huấn luyện với hiệu suất tương đương hoặc thậm chí tốt hơn so với sơ đồ huấn luyện độ chính xác hỗn hợp tiêu chuẩn.

--- TRANG 3 ---
Mesa: Một Framework Huấn luyện Tiết kiệm Bộ nhớ cho Transformers 3

2 Nghiên cứu Liên quan

2.1 Transformers

Transformer ban đầu được đề xuất bởi Vaswani et al. [25] cho dịch máy. Một Transformer tiêu chuẩn bao gồm một lớp embedding, một số khối Transformer và một đầu cụ thể cho nhiệm vụ, trong đó mỗi khối chứa một lớp MSA và một lớp feed-forward theo vị trí (FFN). Sau đó, Transformer đã được mở rộng sang một loạt các nhiệm vụ. Trong lĩnh vực thị giác máy tính, vision Transformers (ViTs) đã thu hút sự chú ý lớn gần đây. Ví dụ, Dosovitskiy et al. [26] đã đề xuất một kiến trúc Transformer tiêu chuẩn cho nhận dạng hình ảnh, đạt được kết quả cạnh tranh trên ImageNet so với CNN. Các nghiên cứu tiếp theo đã cải thiện ViTs từ các khía cạnh khác nhau, chẳng hạn như kết hợp các tính năng hình tháp [3, 9, 27], áp dụng các lớp tích chập để tăng cường tính địa phương của mô hình [28], hoặc khám phá một kiến trúc hoạt động tốt với tìm kiếm kiến trúc mạng nơ-ron (NAS) [29,30]. Tuy nhiên, để huấn luyện một Transformer thường đòi hỏi tài nguyên tính toán mạnh mẽ. Ví dụ, cài đặt điển hình [31] để huấn luyện một ViT trên ImageNet yêu cầu kích thước batch 1,024 trên 8 GPU NVIDIA V100. Kết quả là, chỉ một số ít bên có khả năng chạy những thí nghiệm như vậy. Bên cạnh đó, nó cũng khiến các nhà nghiên cứu khó khăn trong việc khám phá không gian thiết kế lớn hơn cho các kiến trúc Transformer. Để giải quyết vấn đề này, chúng tôi đề xuất giảm chi phí bộ nhớ của Transformers trong quá trình huấn luyện bằng huấn luyện nén kích hoạt 8-bit, làm cho các thí nghiệm có thể chi trả được.

2.2 Huấn luyện Lượng tử hóa

Huấn luyện lượng tử hóa nhằm cải thiện hiệu quả mô hình tại thời gian huấn luyện hoặc thời gian suy luận bằng cách lượng tử hóa trọng số mô hình, kích hoạt hoặc gradient thành các giá trị độ chính xác thấp. Các phương pháp hiện có có thể được phân loại thô thành hai nhóm: 1) lượng tử hóa một mô hình được huấn luyện trước với hoặc không huấn luyện lại [32-35], và 2) huấn luyện một mô hình lượng tử hóa từ đầu [10,14,36,37]. Trong Transformers, phần lớn tài liệu thuộc về nhóm đầu tiên. Ví dụ, lượng tử hóa 8-bit [38,39] hoặc thậm chí bit thấp hơn [40] đã được đề xuất để tăng tốc suy luận. Ngược lại, bài báo này tập trung vào huấn luyện Transformers từ đầu. Khác với các nghiên cứu trước [38,41] thực hiện tính toán độ chính xác thấp trong quá trình truyền tiến hoặc truyền ngược, chúng tôi lưu trữ các kích hoạt độ chính xác thấp xấp xỉ để tiết kiệm bộ nhớ trong quá trình huấn luyện trong khi vẫn tính toán quá trình truyền tiến chính xác. Do đó, chúng tôi không thay đổi hành vi truyền tiến của các mô hình.

2.3 Huấn luyện Hiệu quả Bộ nhớ

Huấn luyện bộ nhớ thấp rất hấp dẫn vì nó cho phép huấn luyện mô hình quy mô lớn trong các tình huống hạn chế tài nguyên. Một loạt các phương pháp đã được đề xuất trong lĩnh vực này. Ví dụ, Mostafa et al. [11] đã đề xuất giảm bộ nhớ mô hình và optimizer bằng tái tham số hóa thưa thớt động. Micikevicius et al. [10] đã giới thiệu huấn luyện độ chính xác hỗn hợp, sử dụng độ chính xác nửa hỗn hợp (16-bit) và độ chính xác đầy đủ (32-bit) cho huấn luyện. Huang et al. [12] đã đề xuất chia mini-batch thành các tập con nhỏ hơn và sau đó tích lũy gradient cho đến khi toàn bộ minibatch được xử lý. Bên cạnh đó, checkpointing gradient [13] cũng là một thực hành phổ biến để giảm bộ nhớ kích hoạt.

Trực giao với các phương pháp trên, huấn luyện nén kích hoạt (ACT) [15] gần đây đã được đề xuất để giảm việc lưu trữ kích hoạt cần thiết cho tính toán gradient. Phương pháp này lần đầu tiên được giới thiệu bởi Chakrabarti et al. [15] cho huấn luyện ResNet. Các nghiên cứu tiếp theo như TinyScript [17] và ActNN [16] mở rộng framework này bằng cách giới thiệu lượng tử hóa không đồng nhất và lượng tử hóa độ chính xác hỗn hợp, tương ứng. Tuy nhiên, tất cả các phương pháp này đều nhắm mục tiêu cụ thể vào CNN và không xem xét các thành phần độc đáo của Transformers, chẳng hạn như LayerNorm, Softmax và GELU. Hơn nữa, sơ đồ lượng tử hóa cho chúng có vấn đề đối với Transformers do phân phối kích hoạt không đồng nhất trong một lớp MSA. Do đó, không có phương pháp hiện có nào có thể được áp dụng trực tiếp cho các mô hình dựa trên Transformer. Trong bài báo này, chúng tôi tùy chỉnh framework ACT để giảm đáng kể yêu cầu tài nguyên huấn luyện Transformers trong khi vẫn giữ hiệu suất xuất sắc của chúng.

3 Phương pháp

Trong phần này, chúng tôi đầu tiên mô tả framework tổng thể của Mesa. Sau đó chúng tôi giới thiệu chiến lược lượng tử hóa kích hoạt theo từng đầu được đề xuất và chiến lược học các tham số lượng tử hóa. Cuối cùng, chúng tôi cung cấp chi tiết về triển khai hệ thống và thảo luận về chi phí của Mesa.

--- TRANG 4 ---
4 Z. Pan, P. Chen, H. He, J. Liu, J. Cai, B. Zhuang

[Hình 2 - Đường ống của Mesa được đề xuất cho huấn luyện Transformers]

3.1 Tổng quan về Mesa

Để giảm tiêu thụ bộ nhớ của Transformers tại thời gian huấn luyện, chúng tôi giới thiệu Mesa, một framework huấn luyện tiết kiệm bộ nhớ tổng quát cho Transformers. Đường ống tổng thể của Mesa được mô tả trong Hình 2. Nói chung, Mesa lưu các kích hoạt xấp xỉ độ chính xác thấp trong quá trình huấn luyện cho lan truyền ngược trong khi vẫn sử dụng kích hoạt chính xác cho quá trình truyền tiến. Cụ thể, ký hiệu X^(l-1) là đầu vào của lớp thứ l trong một Transformer, đầu ra của lớp thứ l có thể được công thức hóa bằng:

X^l = F^l(W^l; X^(l-1)); (1)

trong đó F^l và W^l đại diện cho hàm và các tham số có thể học được của lớp thứ l, tương ứng.

Trong một huấn luyện tiêu chuẩn, đầu vào X^(l-1) được lưu trong bộ nhớ GPU để tính toán gradient trong quá trình lan truyền ngược, trong đó các kích hoạt được lưu tại tất cả các lớp chiếm phần lớn tiêu thụ bộ nhớ trong quá trình huấn luyện, đặc biệt khi được trang bị kích thước batch lớn. Để giảm dấu chân bộ nhớ, chúng tôi đề xuất chỉ lưu các kích hoạt nén X̃^(l-1) thay vì các đối tác độ chính xác đầy đủ X^(l-1) trong quá trình truyền tiến. Việc nén như vậy được thực hiện bằng lượng tử hóa, lượng tử hóa các kích hoạt chính xác thành các giá trị độ chính xác thấp. Trong quá trình lan truyền ngược, X̃^(l-1) được giải lượng tử hóa thành các giá trị độ chính xác gốc X̂^(l-1) để tính toán gradient. Theo cách này, các gradient tại lớp thứ l có thể được xấp xỉ bằng:

∇̂X^(l-1), ∇̂W^l = G^l(∇̂X^l, X̂^(l-1), W^l); (2)

trong đó ∇̂X^(l-1), ∇̂W^l đại diện cho các gradient xấp xỉ cho đầu vào X^(l-1) và các tham số có thể học được W^l, và G^l là hàm gradient tại lớp thứ l.

Đáng chú ý rằng chiến lược này có ít ảnh hưởng đến hiệu suất huấn luyện, vì nó chỉ giới thiệu lỗi xấp xỉ khiêm tốn vào nhiễu gradient tự nhiên từ huấn luyện phân tán và giảm gradient ngẫu nhiên (SGD) [42,43]. Trong phần tiếp theo, chúng tôi giới thiệu một sơ đồ lượng tử hóa kích hoạt theo từng đầu để cải thiện hơn nữa độ trung thực của gradient trong quá trình huấn luyện.

3.2 Lượng tử hóa Kích hoạt Theo từng Đầu

Như được chỉ ra trong [20], độ chi tiết lượng tử hóa tinh là thuận lợi để giảm thiểu lỗi xấp xỉ. Trong thực tế, lượng tử hóa theo lớp nhanh nhưng có thể giới thiệu lỗi lượng tử hóa lớn, trong khi lượng tử hóa theo kênh có thể chính xác hơn nhưng đi kèm với chi phí tính toán và bộ nhớ bổ sung. Có tính đến điều này, lượng tử hóa theo nhóm cân bằng hai mặt và đã được áp dụng rộng rãi trong tài liệu [38,39].

Một chiến lược nhóm ngây thơ trong các nghiên cứu trước [16, 17] là cắt tensor thành các nhóm có kích thước cố định bất kể kích thước tensor, tuy nhiên điều này có vấn đề đối với Transformers vì nó bỏ qua thực tế rằng các đầu tự chú ý khác nhau thường có các mẫu chú ý khá khác nhau [21], tức là phân phối kích hoạt giữa các đầu có mean và variance khác biệt trong một lớp MSA. Trong Hình 4, chúng tôi hình ảnh hóa các kích hoạt trước lớp Softmax tại khối thứ 11 của DeiT-S. Rõ ràng, kích hoạt tại mỗi đầu tự chú ý phải có phạm vi cắt và offset duy nhất. Hiện tượng như vậy có thể được quan sát cho các queries, keys, values và attentions trên tất cả các khối Transformer.

Sơ đồ lượng tử hóa. Để giải quyết các kích hoạt phân phối không đồng nhất trong các đầu tự chú ý, chúng tôi đề xuất một chiến lược lượng tử hóa kích hoạt theo từng đầu. Cụ thể, với x ∈ X_h là một phần tử trong các kích hoạt đầu vào tại đầu thứ h trong một lớp MSA, chúng tôi lượng tử hóa các kích hoạt thành 8-bit bằng:

x̃ = clip(round((x - β_h) × 255 / α_h), 0, 255); (3)

trong đó α_h và β_h là các tham số có thể học được đại diện cho phạm vi cắt lượng tử hóa và offset tại đầu thứ h, tương ứng. clip(x, x_low, x_up) cắt bất kỳ số x nào vào phạm vi [x_low, x_up]. round() biểu thị hàm làm tròn. Ở đây chúng tôi áp dụng làm tròn ngẫu nhiên [37] vì về mặt lý thuyết nó đảm bảo giới hạn lỗi xác suất nhỏ hơn [44] so với làm tròn gần nhất. Cụ thể, nó có thể được công thức hóa như:

round(x) = {
  ⌈x⌉ với xác suất p = x - ⌊x⌋;
  ⌊x⌋ ngược lại.
} (4)

--- TRANG 5 ---
Mesa: Một Framework Huấn luyện Tiết kiệm Bộ nhớ cho Transformers 5

[Hình 3 - Các chiến lược nhóm khác nhau trong Mesa]

[Hình 4 - Phân phối kích hoạt trước lớp Softmax tại khối thứ 11 của DeiT-S]

Trong quá trình lan truyền ngược, chúng tôi giải lượng tử hóa các kích hoạt tại lớp này về độ chính xác gốc bằng:

x̂ = (x̃ × α_h) / 255 + β_h: (5)

Các kích hoạt đã giải lượng tử hóa sau đó được sử dụng để tính toán gradient như trong Eq. (2). Trong Phần 5.2, chúng tôi sẽ cho thấy hiệu quả của sơ đồ lượng tử hóa được đề xuất bằng cách so sánh nó với các chiến lược khác, chẳng hạn như lượng tử hóa đối xứng và làm tròn gần nhất.

Học các tham số lượng tử hóa với ước lượng chạy. Để tính toán các tham số lượng tử hóa α và β tại mỗi lớp, một số nghiên cứu trước đề xuất sử dụng thống kê từng mẫu [16, 17] hoặc các phương pháp dựa trên gradient [22, 23]. Cụ thể, các phương pháp thống kê từng mẫu sử dụng các giá trị min-max hiện tại của mỗi mẫu tại mỗi lớp để tính toán các tham số lượng tử hóa, điều này không hiệu quả và tiêu thụ bộ nhớ bổ sung để lưu trữ các tham số lượng tử hóa. Đối với các phương pháp dựa trên gradient, các phương pháp lượng tử hóa bất đối xứng như LSQ+ [23] có thể tăng dấu chân bộ nhớ vì chúng cần lưu cả các kích hoạt nén và một lượng lớn ngữ cảnh (ví dụ: các lỗi làm tròn để học α) cần thiết để tính toán gradient trong quá trình lan truyền ngược, trái ngược với mục tiêu tiết kiệm bộ nhớ của chúng ta. Ngoài ra, các phương pháp lượng tử hóa đối xứng như PACT [22] không có vấn đề này vì chúng chỉ yêu cầu các giá trị nhị phân để tính toán gradient cho α. Tuy nhiên, chúng tôi sẽ cho thấy trong Phần 5.2.3 rằng lượng tử hóa đối xứng đạt hiệu suất kém trên Transformers so với cả baseline và lượng tử hóa bất đối xứng.

Để kết thúc, chúng tôi đề xuất sử dụng ước lượng chạy [45] để cập nhật các tham số lượng tử hóa trong quá trình huấn luyện, có thể được biểu diễn như:

α_h = μα_h + (1 - μ)(max(X_h) - min(X_h)); (6)
β_h = μβ_h + (1 - μ)min(X_h); (7)

trong đó μ là một siêu tham số. Vì các tham số lượng tử hóa được chia sẻ trong mỗi đầu giữa các mẫu khác nhau, việc sử dụng ước lượng chạy có thể tiết kiệm bộ nhớ nhiều hơn tại thời gian huấn luyện. Ví dụ, để N_h là số đầu tự chú ý trong một lớp MSA và B là kích thước batch, với ước lượng chạy, chúng tôi chỉ cần lưu trữ 2×N_h tham số bổ sung cho lớp Softmax, điều này không đáng kể so với tổng dấu chân bộ nhớ. Ngược lại, việc sử dụng thống kê từng mẫu yêu cầu lưu 2×B×N_h tham số bổ sung cho một lớp. Hơn nữa, chúng tôi sẽ cho thấy trong Phần 5.2 rằng việc sử dụng ước lượng chạy có thể đạt được throughput nhanh hơn tại thời gian huấn luyện.

Các loại kích hoạt khác. Đáng chú ý rằng chúng tôi áp dụng chiến lược nhóm theo đầu cho queries, keys, values và attentions trong một mô hình Transformer. Đối với các loại kích hoạt khác như chuỗi đầu vào tiêu chuẩn trong các lớp FFN, chúng tôi áp dụng sơ đồ lượng tử hóa theo nhóm kênh trong đó nhiều kích thước kênh của kích hoạt được nhóm. Hình 3 tóm tắt các chiến lược nhóm khác nhau của Mesa.

--- TRANG 6 ---
6 Z. Pan, P. Chen, H. He, J. Liu, J. Cai, B. Zhuang

3.3 Triển khai Hệ thống

Mesa được xây dựng dựa trên framework PyTorch [46] phổ biến. Nó là một gói độc lập có thể được áp dụng trực tiếp vào bất kỳ dự án Transformer nào. Để tăng tốc hơn nữa quy trình lượng tử hóa (tức là Eq. (3) và Eq. (5)) trong quá trình huấn luyện, chúng tôi triển khai một kernel CUDA nhanh để cải thiện hiệu quả huấn luyện. Tổng thể, Mesa có thể nén tất cả các phép toán tốn bộ nhớ trong một Transformer, bao gồm MatMul, LayerNorm, Softmax và GELU. Hơn nữa, để hỗ trợ các nhiệm vụ downstream như phân đoạn ngữ nghĩa, chúng tôi cũng bao phủ các lớp thường được sử dụng trong CNN, chẳng hạn như lớp tích chập và ReLU. Đối với các lớp này, chúng tôi áp dụng chiến lược lượng tử hóa theo nhóm tiêu chuẩn như đã đề cập trong Phần 3.2 trong đó chúng tôi nhóm các kích hoạt tích chập dựa trên một số lượng kênh cố định. Theo mặc định, chúng tôi đặt số nhóm lượng tử hóa bằng số đầu tại một khối Transformer.

4 Thảo luận

4.1 Sự Đánh đổi Giữa Tốc độ và Bộ nhớ

Trong khi cả tốc độ huấn luyện và tiêu thụ bộ nhớ đều là bottleneck quan trọng cho huấn luyện Transformer, nghiên cứu này tập trung vào cái sau. Như trong thực tế thông thường (ví dụ: checkpointing [13] và hashing [24]), có sự đánh đổi giữa tốc độ và tiêu thụ bộ nhớ tại thời gian huấn luyện cho tất cả các framework ACT [16, 17]. Trong Mesa, chi phí huấn luyện bổ sung bao gồm lượng tử hóa/giải lượng tử hóa, tính toán các giá trị min/max, làm tròn ngẫu nhiên và định hình lại tensor để điều phối CUDA chính xác cao. Các phép toán này có thể xảy ra trong hầu hết tất cả các lớp để đạt được tiết kiệm bộ nhớ đáng kể trong khi hy sinh tốc độ huấn luyện. Trong trường hợp cực đoan, Mesa có thể dẫn đến throughput giảm một nửa trên một GPU khi nén toàn bộ mô hình.

Mặt khác, xem xét rằng huấn luyện phân tán đã được áp dụng rộng rãi trong học sâu hiện đại [1,3,26], chi phí huấn luyện có thể bị chi phối bởi tải dữ liệu và chi phí giao tiếp giữa các GPU, đặc biệt là đối với các mô hình nhỏ. Ví dụ, chúng tôi sẽ cho thấy trong Phần 5.1 rằng huấn luyện PVT-Ti [3] với Mesa chỉ yêu cầu thêm 15% giờ GPU. Bên cạnh đó, để giảm thiểu thêm chi phí huấn luyện bổ sung, chúng tôi module hóa Mesa sao cho nó có thể nhắm mục tiêu linh hoạt các thành phần khác nhau trong một mô hình trong quá trình huấn luyện. Trong thực tế, chúng tôi đề xuất nén các module tốn bộ nhớ nhất (ví dụ: backbones) để đạt được tiết kiệm bộ nhớ mục tiêu với tốc độ huấn luyện có thể chấp nhận được. Trong Phần 5.2, chúng tôi cho thấy trong Bảng 7 rằng việc nén các phép toán khác nhau (ví dụ: Softmax và GELU) có thể đạt được các sự đánh đổi tốc độ và bộ nhớ khác nhau. Chúng tôi cũng sẽ cung cấp các thí nghiệm về việc nén các module khác nhau (ví dụ: MSA và FFN) trong Bảng 8 để tham khảo.

4.2 Lượng tử hóa Độ chính xác Hỗn hợp và Bits Thấp hơn

Ngoài lượng tử hóa 8-bit được sử dụng trong Mesa, lượng tử hóa độ chính xác hỗn hợp [47] có thể mang lại nhiều lợi ích hơn. Tuy nhiên, nó cũng sẽ giới thiệu chi phí huấn luyện bổ sung vì nó phải tính toán các bit lượng tử hóa cho mỗi lớp, do đó lại làm chậm tốc độ huấn luyện. Hơn nữa, so với số nguyên 8-bit, việc tính toán và lưu trữ các giá trị bit thấp hơn chưa được hỗ trợ tốt trong CPU/GPU hiện có và các framework học sâu (ví dụ: PyTorch). Vì lý do này, chúng tôi chọn lượng tử hóa 8-bit cố định trong Mesa để có sự đánh đổi tốt hơn giữa tốc độ huấn luyện và hiệu suất mô hình.

4.3 Quan hệ với Các Kỹ thuật Huấn luyện Bộ nhớ Thấp Khác

Như đã đề cập trong Phần 2, Mesa trực giao với checkpointing [13], micro-batching [12] và huấn luyện độ chính xác hỗn hợp [10]. Do đó, Mesa có thể được áp dụng cùng với các kỹ thuật này để giảm bộ nhớ nhiều hơn, như được hiển thị trong Bảng 14 và Hình 1. Bên cạnh đó, checkpointing nhắm mục tiêu đồ thị tính toán và giữ hiệu suất không đổi. Trong thực tế, người ta phải thiết lập thủ công các vị trí checkpointing, điều này không tối ưu cho sự đánh đổi bộ nhớ-tốc độ. Ngược lại, Mesa linh hoạt hơn để nén các phép toán được chỉ định để phù hợp với bộ nhớ huấn luyện mục tiêu, như được hiển thị trong Bảng 7. Mesa cũng có thể cải thiện hiệu suất cũng như dễ dàng xác định các phép toán/module tốn bộ nhớ nhất để huấn luyện. Đối với micro-batching [12], nó bắt chước kích thước batch lớn hơn nhưng có thể ảnh hưởng đến hiệu suất mô hình. Hơn nữa, huấn luyện với các kích thước batch khác nhau khiến việc so sánh công bằng với các nghiên cứu liên quan trở nên khó khăn. Đối với các nghiên cứu ACT hiện có, chúng hoặc không thể áp dụng cho Transformers [15,17] hoặc áp dụng đầy đủ cho Transformers [16], và do đó chúng không thể so sánh trực tiếp. Một nghiên cứu liên quan gần gũi là ActNN [16], mà chúng tôi cho thấy trong Phần 5.2.11 rằng Mesa đạt được sự đánh đổi tiết kiệm bộ nhớ và tốc độ tốt hơn.

5 Thí nghiệm

5.1 Kết quả Chính

Tập dữ liệu và metrics đánh giá. Chúng tôi tiến hành thí nghiệm trên tập dữ liệu ImageNet (ILSVRC2012) [48],

--- TRANG 7 ---
Mesa: Một Framework Huấn luyện Tiết kiệm Bộ nhớ cho Transformers 7

[Hình 5 - So sánh đường cong huấn luyện cho DeiT-B với/không có Mesa trên ImageNet]

[Bảng 1 - Kết quả phân loại trên ImageNet]

chứa ~1.2M hình ảnh huấn luyện từ 1K danh mục và 50K hình ảnh validation. Theo thực tế thông thường [26, 31], chúng tôi đo hiệu suất mô hình bằng độ chính xác Top-1. Ngoài ra, chúng tôi cũng báo cáo tiêu thụ bộ nhớ tại thời gian huấn luyện và tổng giờ GPU để huấn luyện mỗi mô hình.

Các phương pháp so sánh. Để chứng minh hiệu quả của Mesa, chúng tôi đánh giá framework của chúng tôi trên một số vision Transformers hiện đại, bao gồm DeiT [31], Swin [9] và PVT [3]. DeiT là một vision Transformer tiêu chuẩn kế thừa kiến trúc tương tự từ Transformer gốc [25]. Swin và PVT là các vision Transformers phân cấp (HVTs) được đề xuất gần đây đạt được kết quả đầy hứa hẹn trên nhiều nhiệm vụ thị giác khác nhau. Hơn nữa, vì các mô hình gần đây thường có nhiều biến thể về độ sâu và chiều rộng mô hình, chúng tôi ký hiệu chúng là "Model-Ti/S/B" để biểu thị cài đặt tiny, small và base của chúng.

Chi tiết triển khai. Theo mặc định, tất cả các mô hình được huấn luyện trên 8 GPU V100 với tổng kích thước batch 1,024 (128 mỗi GPU) trên ImageNet. Chúng tôi áp dụng optimizer AdamW [49] với bộ lập lịch tốc độ học cosine decay. Chúng tôi đặt tốc độ học ban đầu và weight decay lần lượt là 1×10^-3 và 5×10^-2. Hơn nữa, chúng tôi áp dụng cùng chiến lược huấn luyện khi so sánh với từng baseline. Tất cả thí nghiệm đều áp dụng huấn luyện độ chính xác hỗn hợp tự động [10] (còn gọi là FP16 hoặc huấn luyện nửa độ chính xác) vì nó được sử dụng rộng rãi trong nghiên cứu gần đây để tăng tốc quá trình huấn luyện. μ trong Eqs. (6) và (7) được đặt là 0.9, được xác định bằng tìm kiếm lưới đơn giản trên CIFAR-100. α và β được khởi tạo bằng các giá trị min-max từ các kích hoạt tại iteration huấn luyện đầu tiên. Trong Phần 5.2, chúng tôi tiến hành thí nghiệm sử dụng μ khác nhau để huấn luyện DeiT-Ti trên CIFAR-100 và cung cấp hình ảnh hóa cho sự phát triển của các tham số lượng tử hóa.

Kết quả. Trong Bảng 1, chúng tôi báo cáo kết quả phân loại ImageNet của việc huấn luyện DeiT và các HVT gần đây với Mesa. Nói chung, Mesa có thể giảm khoảng một nửa tiêu thụ bộ nhớ tại thời gian huấn luyện trong khi đạt được hiệu suất tương đương hoặc thậm chí tốt hơn so với các baseline mạnh. Ví dụ, trên DeiT-B và Swin-Ti, Mesa đạt được hiệu suất tương tự như các baseline trong khi giảm dấu chân bộ nhớ 51% và 55%^1, tương ứng. Đáng chú ý, DeiT-Ti/S với Mesa thậm chí vượt trội hơn baseline 0.2% về độ chính xác Top-1. Phỏng đoán của chúng tôi là các kích hoạt xấp xỉ giúp điều chỉnh các gradient ngẫu nhiên khi huấn luyện Transformers, do đó cải thiện hiệu suất mô hình. Ngoài ra, chúng tôi cũng hình ảnh hóa các đường cong huấn luyện của DeiT-B với Mesa trong Hình 5. Như được hiển thị, tất cả các đường cong dưới Mesa đều nhất quán với những đường cong của baseline DeiT-B hoặc thậm chí hoạt động tốt hơn một chút. Đối với PVT-Ti, chúng tôi quan sát thấy sự giảm hiệu suất nhẹ 0.2% về độ chính xác Top-1. Suy đoán của chúng tôi là PVT khá nhạy cảm để huấn luyện vì tác giả thấy rằng PVT sâu hơn thậm chí không thể hội tụ với cài đặt tương tự của PVT-Ti/S^2. Trong thực tế, tác động của Mesa lên hiệu suất mô hình có thể phụ thuộc vào kiến trúc. Nói chung, Mesa hoạt động nhất quán tốt trên các mô hình đại diện. Như được hiển thị trong Bảng 1, hiệu ứng lên hiệu suất chỉ khoảng ±0.2% cho các baseline so sánh.

Bên cạnh đó, chúng tôi nhận thấy rằng Mesa làm chậm tốc độ huấn luyện. Như đã thảo luận trong Phần 4, sự đánh đổi không gian-thời gian là thực tế thông thường trong tài liệu. Trong Bảng 1, các thí nghiệm với Mesa nén hầu hết tất cả các phép toán để đạt được giảm bộ nhớ đáng kể, do đó làm chậm throughput nhiều nhất. Tuy nhiên, trong thực tế, người ta có thể xác định các phép toán tốn bộ nhớ nhất với Mesa để đạt được mục tiêu tiết kiệm bộ nhớ lý tưởng với tốc độ huấn luyện có thể chấp nhận được. Chúng tôi sẽ cho thấy trong Bảng 7 và Bảng 13 rằng Mesa có thể nén các phép toán khác nhau để đạt được tiết kiệm bộ nhớ linh hoạt. Hơn nữa, chi phí huấn luyện có thể được bù đắp phần lớn bởi chi phí tải dữ liệu và giao tiếp trong huấn luyện phân tán. Ví dụ, thời gian huấn luyện PVT-Ti với Mesa trên ImageNet chỉ dài hơn 15% (520 vs. 600 giờ GPU). Tuy nhiên, chúng tôi cũng nhận thấy rằng tổng giờ GPU để huấn luyện DeiT-B và Swin-Ti với Mesa gần như tăng gấp đôi. Điều này cho thấy việc giảm tốc độ huấn luyện khác nhau giữa các kiến trúc khác nhau và kích thước mô hình, trong khi trường hợp tệ nhất có thể tăng gấp đôi thời gian huấn luyện.

5.2 Nghiên cứu Ablation

Trong phần này, chúng tôi cung cấp các nghiên cứu ablation của Mesa. Theo mặc định, chúng tôi sử dụng triển khai CUDA của Mesa cho các thí nghiệm sau. Throughput và tiêu thụ bộ nhớ tại thời gian huấn luyện được đo với kích thước batch 128 và độ phân giải hình ảnh 224×224 trên một GPU NVIDIA RTX 3090. Trừ khi có quy định khác, chúng tôi áp dụng cùng chiến lược huấn luyện như trong Phần 5.1 cho các thí nghiệm ImageNet. Trên CIFAR-100, chúng tôi huấn luyện mô hình với tổng kích thước batch 256 trên 2 GPU và giữ tất cả các cài đặt thí nghiệm khác giống như trong Phần 5.1 ngoại trừ tốc độ học ban đầu được scale tuyến tính về 2.5×10^-4.

5.2.1 Thống kê Từng mẫu vs. Ước lượng Chạy để Cập nhật Tham số Lượng tử hóa

Các framework ACT trước đây như TinyScript [17] và ActNN [16] sử dụng thống kê từng mẫu (PS) để tính toán các tham số lượng tử hóa. Như đã thảo luận trong 3.2, phương pháp như vậy có thể dẫn đến tiêu thụ bộ nhớ và chi phí tính toán bổ sung nhiều hơn trong quá trình huấn luyện. Trong Bảng 2, chúng tôi so sánh phương pháp sử dụng thống kê từng mẫu với việc sử dụng ước lượng chạy (RE) trên ImageNet và CIFAR-100. Từ kết quả, chúng tôi quan sát thấy rằng trong khi cả hai phương pháp đều hoạt động tốt so với baseline về độ chính xác Top-1, chiến lược sử dụng ước lượng chạy đạt được hiệu suất tốt hơn, tiêu thụ bộ nhớ ít hơn và throughput nhanh hơn so với việc sử dụng thống kê từng mẫu. Lưu ý rằng cả hai thí nghiệm PS/RE trong Bảng 2 đều áp dụng triển khai PyTorch của Mesa, do đó tiêu thụ bộ nhớ và throughput tại thời gian huấn luyện hơi khác so với triển khai CUDA. Bên cạnh đó, mặc dù throughput trên một GPU giảm bằng cách sử dụng Mesa, tổng thời gian huấn luyện trên ImageNet tương tự như baseline DeiT-Ti vì chi phí giao tiếp có tác động lớn hơn đến tốc độ huấn luyện dưới sơ đồ huấn luyện phân tán.

^1 Huấn luyện AMP [10] lưu trữ kích hoạt FP16 và FP32 hỗn hợp. Với Mesa chỉ kích hoạt Int8 được lưu, và do đó chúng tôi có thể đạt được giảm bộ nhớ hơn 2×.

^2 https://github.com/whai362/PVT/issues/2

--- TRANG 8 ---
8 Z. Pan, P. Chen, H. He, J. Liu, J. Cai, B. Zhuang

[Bảng 2 - So sánh hiệu suất trên DeiT-Ti với Mesa bằng cách sử dụng thống kê từng mẫu (PS) và ước lượng chạy (RE)]

[Bảng 3 - So sánh hiệu suất trên DeiT với Mesa dưới làm tròn ngẫu nhiên (SR) và làm tròn gần nhất (NR)]

5.2.2 Làm tròn Ngẫu nhiên vs. Làm tròn Gần nhất

Để khám phá hiệu ứng của làm tròn ngẫu nhiên trong Mesa, chúng tôi so sánh nó với làm tròn gần nhất thường được sử dụng bằng cách huấn luyện DeiT-Ti với Mesa trên CIFAR-100 và ImageNet. Chúng tôi báo cáo kết quả trong Bảng 3. Như được hiển thị, mặc dù huấn luyện DeiT-Ti với làm tròn gần nhất có thể đạt được kết quả cạnh tranh trên CIFAR-100, nó không thể hội tụ trên ImageNet. Do đó, chúng tôi suy đoán rằng làm tròn ngẫu nhiên quan trọng để đảm bảo hiệu suất tốt trong Mesa. Cũng lưu ý rằng làm tròn ngẫu nhiên không làm tăng dấu chân bộ nhớ trong quá trình huấn luyện. Tuy nhiên, nó hơi chậm hơn so với việc sử dụng làm tròn gần nhất, điều này do chi phí bổ sung từ việc triển khai làm tròn ngẫu nhiên.

5.2.3 Lượng tử hóa Bất đối xứng vs. Lượng tử hóa Đối xứng

Ngoài lượng tử hóa bất đối xứng được sử dụng trong Mesa, lượng tử hóa đối xứng cũng được áp dụng rộng rãi trong các nghiên cứu lượng tử hóa mô hình trước đây [22, 32]. Dưới sơ đồ này, đầu vào X được lượng tử hóa chỉ bằng một hệ số tỷ lệ s = (2^b - 1) / max(|X|), trong đó b là độ rộng bit. Trong Bảng 4, chúng tôi so sánh hai sơ đồ lượng tử hóa dựa trên CIFAR-100. Từ kết quả, chúng tôi quan sát thấy rằng mặc dù lượng tử hóa đối xứng đạt được throughput nhanh hơn trong quá trình huấn luyện, nó không vượt qua baseline về độ chính xác Top-1. Kết quả này cũng nhất quán với quan sát trước đây [41] để lượng tử hóa các mô hình BERT. Thực tế, các kích hoạt trong Transformers bị lệch về các giá trị âm. Chúng tôi giả định lượng tử hóa bất đối xứng có thể cung cấp phạm vi cắt chặt chẽ hơn sao cho nó giúp giảm thiểu lỗi lượng tử hóa trong quá trình huấn luyện.

5.2.4 Hiệu ứng của Các Độ chi tiết Lượng tử hóa Khác nhau

Để khám phá hiệu ứng của các độ chi tiết lượng tử hóa khác nhau trong Mesa, chúng tôi huấn luyện DeiT-Ti với Mesa và so sánh chiến lược được đề xuất với lượng tử hóa theo lớp trên CIFAR-100. Kết quả được hiển thị trong Bảng 5. Tổng thể, hưởng lợi từ việc sử dụng ước lượng chạy trong Mesa, tất cả các chiến lược tiêu thụ một lượng bộ nhớ tương tự trong quá trình huấn luyện vì mỗi lớp chỉ cần lưu một vài tham số lượng tử hóa. Với lượng tử hóa theo lớp, tất cả các kích hoạt tại cùng một lớp được lượng tử hóa dựa trên cùng phạm vi cắt và offset, nhưng nó không vượt trội hơn baseline. Mặt khác, lượng tử hóa theo đầu được đề xuất sử dụng các chiến lược nhóm như trong Hình 3 phân biệt các thống kê khác nhau qua các đầu và nhóm kênh khác nhau. Nó đạt được hiệu suất tốt hơn so với cả chiến lược theo lớp và baseline, trong khi tạo ra sự đánh đổi tốt với tốc độ huấn luyện. Ngoài ra, chúng tôi nhận thấy rằng việc nhóm kích hoạt (trừ queries, keys, values và attentions) dựa trên từng kênh ẩn khiến DeiT-Ti không thể hội tụ trên ImageNet. Điều này cho thấy độ chi tiết lượng tử hóa thích hợp là cần thiết để ổn định huấn luyện Transformer.

5.2.5 Hiệu ứng của Các Tỷ lệ Decay Khác nhau để Học Tham số Lượng tử hóa

Trong Mesa, chúng tôi sử dụng ước lượng chạy để học các tham số lượng tử hóa, điều này yêu cầu tỷ lệ decay μ để điều chỉnh. Để hiểu hiệu ứng của μ khác nhau trong Mesa, chúng tôi tiến hành thí nghiệm với DeiT-Ti trên CIFAR-100 và báo cáo kết quả trong Bảng 6. Tổng thể, chúng tôi thấy rằng μ phù hợp khá cần thiết để giúp Mesa đạt được hiệu suất tốt. Đặc biệt, khi μ bằng 0, các tham số lượng tử hóa chỉ dựa vào thống kê batch hiện tại tại mỗi iteration huấn luyện. Tuy nhiên, phương pháp như vậy không thể vượt trội hơn baseline. Bên cạnh đó, chúng tôi thấy μ lớn khiến DeiT-Ti không thể hội tụ vì nó không thể thích ứng kịp thời với phân phối kích hoạt động trong quá trình huấn luyện. Tổng thể, chúng tôi thấy μ = 0.9 đạt được hiệu suất tốt nhất trong thực tế, trong trường hợp đó chúng tôi đặt μ = 0.9 cho tất cả các thí nghiệm theo mặc định.

--- TRANG 9 ---
Mesa: Một Framework Huấn luyện Tiết kiệm Bộ nhớ cho Transformers 9

[Bảng 4 - So sánh hiệu suất giữa lượng tử hóa đối xứng và lượng tử hóa bất đối xứng trên DeiT-Ti với Mesa]

[Bảng 5 - So sánh hiệu suất trên DeiT-Ti với Mesa dưới các độ chi tiết lượng tử hóa khác nhau trên CIFAR-100]

[Bảng 6 - So sánh hiệu suất của μ khác nhau dựa trên DeiT-Ti]

5.2.6 Hiệu ứng của Việc Nén Các Phép toán Khác nhau

Đáng chú ý là Mesa có thể nén các phép toán khác nhau, hoặc bất kỳ tổ hợp nào (ví dụ: GELU và LayerNorm) để đạt được các sự đánh đổi bộ nhớ-tốc độ khác nhau. Ví dụ, một Transformer tiêu chuẩn [26, 31] bao gồm nhân ma trận (MatMul), GELU, LayerNorm và Softmax. Các kích hoạt được tạo ra từ những phép toán này tiêu thụ phần lớn bộ nhớ GPU trong quá trình huấn luyện. Trong Bảng 7, chúng tôi báo cáo kết quả của việc nén các phép toán khác nhau trong DeiT-Ti dựa trên CIFAR-100. Nói chung, huấn luyện với Mesa có thể đạt được hiệu suất ngang bằng hoặc thậm chí tốt hơn so với baseline. Đồng thời, việc nén các phép toán khác nhau có thể đạt được tiết kiệm bộ nhớ khác nhau trong khi cũng giới thiệu chi phí huấn luyện khác nhau. Khi nén tất cả các phép toán, chúng tôi đạt được tiết kiệm bộ nhớ nhiều nhất (khoảng 50%) trong khi làm chậm tốc độ huấn luyện trên một GPU đi một nửa. Trong thực tế, Mesa cho phép các nhà nghiên cứu linh hoạt thiết lập sự đánh đổi theo tiết kiệm bộ nhớ mục tiêu và chi phí huấn luyện có thể chấp nhận được của riêng họ. Chúng tôi cho thấy trong Phần 5.2.12 rằng bằng cách sử dụng các tổ hợp khác nhau của các phép toán nén, chúng tôi có thể huấn luyện các mô hình lớn hơn với sự đánh đổi bộ nhớ-tốc độ tốt hơn.

5.2.7 Hiệu ứng của Việc Nén Các Module Khác nhau

Các lớp MSA và FFN là các module chính của một mô hình Transformer. Đồng thời, chúng tiêu thụ phần lớn bộ nhớ GPU tại thời gian huấn luyện. Để nghiên cứu hiệu ứng của việc nén các module khác nhau, chúng tôi huấn luyện DeiT-Ti với Mesa trên CIFAR-100 và báo cáo kết quả trong Bảng 8. Tổng thể, huấn luyện DeiT-Ti với Mesa đạt được hiệu suất ngang bằng hoặc tốt hơn so với baseline. Đặc biệt, việc nén các lớp MSA hoặc FFN trong DeiT-Ti có thể giảm 27% và 21% dấu chân bộ nhớ tại thời gian huấn luyện, tương ứng. Tuy nhiên, trong khi việc nén các lớp MSA có thể tiết kiệm bộ nhớ nhiều hơn, nó cũng dẫn đến throughput chậm hơn trong quá trình huấn luyện so với việc nén các lớp FFN. Cuối cùng, việc nén các lớp MSA và FFN đồng thời mang lại tiết kiệm bộ nhớ nhiều nhất, nhưng nó cũng dẫn đến tốc độ huấn luyện chậm nhất. Tuy nhiên, chi phí này có thể được bù đắp bởi chi phí giao tiếp trong học phân tán. Trong thực tế, người ta cũng có thể nhắm mục tiêu các module khác nhau trong một mô hình để đạt được sự đánh đổi bộ nhớ-tốc độ có thể chấp nhận được.

5.2.8 Các Mô hình Lớn nhất mà Mesa có thể Huấn luyện

Với sự giúp đỡ của Mesa, chúng tôi có thể tái đầu tư bộ nhớ được giảm bằng cách xây dựng mô hình lớn hơn hoặc huấn luyện với độ phân giải hình ảnh lớn hơn. Trong Bảng 9, chúng tôi báo cáo các mô hình lớn nhất mà Mesa có thể huấn luyện trước khi hết bộ nhớ dựa trên DeiT-B. Tổng thể, Mesa có thể scale up độ sâu mô hình 3.3× và chiều rộng 2.2×. Hơn nữa, Mesa có thể huấn luyện DeiT-B với độ phân giải hình ảnh lớn hơn 1.5×. Trong thực tế, người ta cũng có thể giảm tỷ lệ mở rộng trong các lớp FFN để scale up nhiều hơn về độ sâu/chiều rộng/độ phân giải. Trong Bảng 9, chúng tôi áp dụng tỷ lệ mở rộng mặc định là bốn trong DeiT-B. Hơn nữa, chúng tôi sẽ cho thấy trong Phần 5.2.10 rằng Mesa cho phép chúng tôi huấn luyện mô hình với kích thước batch lớn hơn dưới cùng ngân sách bộ nhớ.

--- TRANG 10 ---
10 Z. Pan, P. Chen, H. He, J. Liu, J. Cai, B. Zhuang

[Bảng 7 - So sánh hiệu suất của việc nén các phép toán khác nhau dựa trên DeiT-Ti với Mesa]

[Bảng 8 - So sánh hiệu suất của việc nén các module khác nhau dựa trên DeiT-Ti với Mesa]

[Bảng 9 - So sánh các mô hình lớn nhất mà Mesa có thể huấn luyện trước khi hết bộ nhớ với cùng kích thước batch 128 trên một GPU NVIDIA V100 32GB]

5.2.9 Phân đoạn Ngữ nghĩa

Để khám phá hiệu suất của Mesa trên các nhiệm vụ downstream, chúng tôi sử dụng Swin-Ti làm backbone và Semantic FPN [51] làm framework để đánh giá hiệu suất phân đoạn ngữ nghĩa trên ADE20K [50]. Theo thực tế thông thường [52], chúng tôi sử dụng optimizer AdamW với lịch trình tốc độ học poly và huấn luyện mô hình với 80,000 iterations. Chúng tôi đặt tốc độ học ban đầu là 1×10^-4. Tất cả các backbone được huấn luyện trước trên tập dữ liệu ImageNet. Như Bảng 11 cho thấy, huấn luyện với Mesa vượt trội hơn baseline 0.7% về mIoU. Hơn nữa, với dấu chân bộ nhớ giảm, chúng tôi huấn luyện mô hình chỉ với hai GPU dưới cùng cài đặt trong khi đạt được thêm 0.2% lợi ích về mIoU, điều này một lần nữa chứng minh lợi thế của Mesa dưới tài nguyên phần cứng hữu hạn.

5.2.10 Hiệu ứng của Kích thước Batch Lớn hơn dưới Cùng Ngân sách Bộ nhớ

Trong kết quả chính của chúng tôi về phân loại ImageNet, chúng tôi đã cho thấy rằng Mesa có thể giảm khoảng một nửa tiêu thụ bộ nhớ tại thời gian huấn luyện. Quan trọng hơn, với Mesa, có thể huấn luyện một Transformer với kích thước batch lớn hơn dưới cùng ngân sách bộ nhớ. Ví dụ, huấn luyện DeiT-Ti với chiến lược mặc định yêu cầu ít nhất 34GB bộ nhớ GPU dưới tổng kích thước batch 1,024. Tuy nhiên, với cùng ngân sách bộ nhớ, chúng tôi có thể tăng gấp đôi kích thước batch để tận dụng tốt hơn các lõi GPU có sẵn và khám phá lợi ích tiềm năng của việc sử dụng kích thước batch lớn hơn. Cụ thể, chúng tôi huấn luyện DeiT-Ti với Mesa sử dụng tổng kích thước batch 2,048 trên ImageNet với 8 GPU NVIDIA V100. Như chúng tôi có thể thấy từ Bảng 10, Mesa vẫn tiêu thụ ít bộ nhớ hơn so với huấn luyện độ chính xác hỗn hợp với kích thước batch 1,024 (33.4GB so với 28.8GB). Hơn nữa, nó đạt được 0.8% lợi ích về độ chính xác Top-1. Hơn nữa, trong khi huấn luyện mặc định cho Swin-S với kích thước batch 2,048 sẽ dẫn đến hết bộ nhớ trên 8 GPU V100, huấn luyện với Mesa dưới cùng kích thước batch lại tiêu thụ ít bộ nhớ hơn và thậm chí đạt được 0.1% cải thiện về độ chính xác Top-1 so với baseline. Hiệu suất tổng thể cho thấy Mesa cho phép chúng tôi huấn luyện mô hình với kích thước batch lớn hơn dưới cùng ngân sách bộ nhớ, trong khi vẫn hoạt động tốt so với baseline.

5.2.11 So sánh với ActNN

Các framework ACT hiện có bao gồm BLPA [15], TinyScript [17] và ActNN [16]. Tuy nhiên, cả BLPA và TinyScript đều nhắm mục tiêu vào các kiến trúc CNN cụ thể (ví dụ: ResNet [6]), khiến việc áp dụng phương pháp của chúng vào Transformers trở nên khó khăn. Bên cạnh đó, ActNN chỉ thí nghiệm với các mô hình dựa trên CNN bỏ qua các kích hoạt phân phối không đồng nhất trong các lớp MSA. Trong Bảng 12, chúng tôi huấn luyện DeiT-Ti với cài đặt mặc định (L3, 2-bit) của ActNN trên CIFAR-100. So với ActNN, Mesa bao phủ nhiều lớp hơn và đạt được tiết kiệm bộ nhớ và hiệu suất tốt hơn với throughput hơi chậm hơn. Hơn nữa, Mesa có thể đạt được sự đánh đổi bộ nhớ-tốc độ tốt hơn so với ActNN bằng cách nhắm mục tiêu các phép toán khác nhau. Ví dụ, trong Hình 6, chúng tôi cho thấy lợi thế của Mesa bằng cách so sánh nó với các mức tối ưu hóa khác nhau trong ActNN.

--- TRANG 11 ---
Mesa: Một Framework Huấn luyện Tiết kiệm Bộ nhớ cho Transformers 11

[Bảng 10 - So sánh hiệu suất với kích thước batch lớn hơn dựa trên DeiT-Ti và Swin-S với Mesa]

[Bảng 11 - Hiệu suất phân đoạn ngữ nghĩa trên ADE20K dựa trên Semantic FPN]

[Bảng 12 - So sánh hiệu suất giữa ActNN và Mesa được đề xuất dựa trên DeiT-Ti]

[Hình 6 - So sánh tiêu thụ bộ nhớ và throughput tại thời gian huấn luyện giữa ActNN và Mesa dựa trên DeiT-Ti]

5.2.12 Hiệu ứng của Huấn luyện trên Các Mô hình Lớn hơn

Để khám phá hiệu ứng của việc huấn luyện các mô hình lớn hơn với Mesa, chúng tôi huấn luyện Swin-Base và Swin-Large với Mesa trên CIFAR-100. Theo cài đặt mặc định trong Swin [9], chúng tôi huấn luyện Swin-Large trong 90 epochs. Như được hiển thị trong Bảng 13, Mesa giảm một nửa dấu chân bộ nhớ trong khi giữ hiệu suất của chúng khi nén tất cả các phép toán. Hơn nữa, vì throughput có thể kiểm soát được, chúng tôi có thể nén các phép toán đã chọn để đạt được sự đánh đổi tốc độ-bộ nhớ tốt hơn với độ chính xác thậm chí cao hơn một chút trên CIFAR-100.

5.2.13 So sánh với Các Kỹ thuật Tiết kiệm Bộ nhớ Khác

Như đã thảo luận trong Phần 4, các framework ACT bao gồm Mesa bổ sung cho checkpointing (CP), tích lũy gradient (GA) và AMP. Thực tế, CP nhằm lưu trữ ít tensor hơn trong khi ACT nén các tensor đã lưu. Bên cạnh đó, GA tiết kiệm bộ nhớ bằng cách giảm kích thước batch, điều này khiến việc so sánh công bằng với các nghiên cứu liên quan trở nên khó khăn. Hơn nữa, ngay cả khi hầu hết các Transformers áp dụng LayerNorm thay vì BatchNorm [53], huấn luyện ViTs như DeiT có thể nhạy cảm với kích thước batch. Do đó, việc giảm kích thước batch có thể làm hại hiệu suất mô hình. Ngược lại, Mesa tiết kiệm bộ nhớ mà không sửa đổi các siêu tham số thí nghiệm. Hơn nữa, Mesa trực giao với AMP. Trong quá trình huấn luyện, Mesa sử dụng kích hoạt chính xác trong quá trình truyền tiến trong khi lưu trữ kích hoạt Int8 cho lan truyền ngược để giảm tiêu thụ bộ nhớ, trong khi AMP lượng tử hóa trọng số, kích hoạt và gradient thành FP16.

Trong Bảng 14, chúng tôi so sánh Mesa với CP và GA. Tổng thể, Mesa có thể tiết kiệm bộ nhớ nhiều hơn trong khi đạt được tốc độ tương đương với CP. Lưu ý rằng CP đã được tối ưu hóa cao bởi nhóm PyTorch, điều này hơi không công bằng so với việc triển khai của chúng tôi. Quan trọng hơn,

--- TRANG 12 ---
12 Z. Pan, P. Chen, H. He, J. Liu, J. Cai, B. Zhuang

[Bảng 13 - Thí nghiệm với Swin-B/L trên CIFAR-100]

[Bảng 14 - So sánh với checkpointing (CP) và tích lũy gradient (GA) dựa trên Swin-Ti]

Mesa có thể được kết hợp với những kỹ thuật này để tiết kiệm bộ nhớ nhiều hơn do tính bổ sung.

6 Hình ảnh hóa

Trong phần này, chúng tôi hình ảnh hóa sự phát triển của α và β trong DeiT-Ti trong quá trình huấn luyện trên CIFAR-100. Chúng tôi đặt số nhóm lượng tử hóa tại mỗi lớp là 3.

6.1 Sự phát triển của α và β trong Các Lớp MSA

Trong Hình 7, chúng tôi cho thấy sự phát triển của α và β trong các lớp MSA trong quá trình huấn luyện, tương ứng. Nói chung, các tham số lượng tử hóa tại mỗi đầu phát triển khác nhau, nhấn mạnh sự cần thiết của lượng tử hóa kích hoạt theo từng đầu trong các lớp MSA. Bên cạnh đó, chúng tôi thấy phạm vi cắt lượng tử hóa α tăng đáng kể ở các giai đoạn đầu, sau đó trở nên ổn định hoặc giảm ở các giai đoạn sau. Mặt khác, offset lượng tử hóa β tiếp tục giảm ở đầu trong khi có xu hướng ổn định hoặc tăng sau này trong quá trình huấn luyện. Đặc biệt, chúng tôi thấy các offset lượng tử hóa bị lệch về các giá trị âm, điều này cho thấy rằng các kích hoạt trong các lớp MSA chứa nhiều giá trị âm hơn, ngoại trừ các attentions sau Softmax vì chúng là các giá trị không âm.

6.2 Sự phát triển của α và β trong Các Lớp FFN

Hình 8 cho thấy sự phát triển của α và β trong các lớp FFN trong quá trình huấn luyện, tương ứng. Tổng thể, hiện tượng khá tương tự như của các lớp MSA. Đặc biệt, chúng tôi thấy offset lượng tử hóa tại lớp FC thứ hai của FFN luôn là giá trị tối thiểu (-0.17) của GELU. Điều này cho thấy β có thể được cố định ở -0.17 tại lớp này trong quá trình huấn luyện, điều này có thể giúp giảm thêm chi phí huấn luyện. Chúng tôi để lại không gian tối ưu hóa thêm cho nghiên cứu tương lai.

7 Kết luận

Trong nghiên cứu này, chúng tôi đã trình bày Mesa, một framework huấn luyện tiết kiệm bộ nhớ hiệu quả tài nguyên cho Transformers. Cụ thể, chúng tôi lưu các kích hoạt xấp xỉ độ chính xác thấp trong quá trình huấn luyện để đạt được tiết kiệm bộ nhớ trong khi sử dụng kích hoạt chính xác cho quá trình truyền tiến. Các kích hoạt đã lưu được sử dụng để tính toán gradient trong quá trình lan truyền ngược. Hơn nữa, chúng tôi xác định các phân phối kích hoạt không đồng nhất trong một lớp MSA của một Transformer. Để giải quyết điều này, chúng tôi đề xuất một chiến lược lượng tử hóa kích hoạt theo từng đầu, nhóm các kích hoạt dựa trên từng đầu tự chú ý để nắm bắt phạm vi cắt lượng tử hóa và offset tốt hơn. Với Mesa, chúng tôi có thể tái đầu tư dấu chân bộ nhớ giảm bằng cách xây dựng mô hình lớn hơn hoặc huấn luyện với kích thước batch lớn hơn để khám phá lợi ích tiềm năng. Các thí nghiệm mở rộng trên ImageNet, CIFAR-100 và ADE20K đã cho thấy rằng Mesa có thể đạt được tiết kiệm bộ nhớ linh hoạt trong quá trình huấn luyện trong khi đạt được hiệu suất tương đương hoặc thậm chí tốt hơn so với chiến lược huấn luyện mặc định.

[References section continues with numbered citations 1-53]

--- TRANG 13 ---
Mesa: Một Framework Huấn luyện Tiết kiệm Bộ nhớ cho Transformers 13

[Hình 7 - Sự phát triển của α và β trong các lớp MSA của DeiT-Ti]

[Hình 8 - Sự phát triển của α và β trong các lớp FFN của DeiT-Ti]

--- TRANG 14 ---
14 Z. Pan, P. Chen, H. He, J. Liu, J. Cai, B. Zhuang

--- TRANG 15 ---
Mesa: Một Framework Huấn luyện Tiết kiệm Bộ nhớ cho Transformers 15

[Tiếp tục danh sách tài liệu tham khảo từ 15-53]

--- TRANG 16 ---
16 Z. Pan, P. Chen, H. He, J. Liu, J. Cai, B. Zhuang

[Tiếp tục danh sách tài liệu tham khảo 52-53]
