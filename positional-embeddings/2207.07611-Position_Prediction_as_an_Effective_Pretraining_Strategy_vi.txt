Dự đoán vị trí như một chiến lược tiền huấn luyện hiệu quả

Shuangfei Zhai1Navdeep Jaitly1Jason Ramapuram1Dan Busbridge1Tatiana Likhomanenko1
Joseph Yitan Cheng1Walter Talbott1Chen Huang1Hanlin Goh1Joshua Susskind1

Tóm tắt
Transformers (Vaswani et al., 2017) ngày càng trở nên phổ biến trong nhiều ứng dụng khác nhau, bao gồm Xử lý Ngôn ngữ Tự nhiên (NLP), Thị giác Máy tính và Nhận dạng Giọng nói, nhờ vào khả năng biểu diễn mạnh mẽ của chúng. Tuy nhiên, việc khai thác khả năng biểu diễn này một cách hiệu quả đòi hỏi một lượng lớn dữ liệu, regularization mạnh, hoặc cả hai, để giảm thiểu overfitting. Gần đây, sức mạnh của Transformer đã được khai phá thông qua các chiến lược tiền huấn luyện tự giám sát dựa trên masked autoencoders, dựa vào việc tái tạo các đầu vào bị che khuất, trực tiếp hoặc thông qua contrastive learning từ nội dung không bị che khuất. Chiến lược tiền huấn luyện này đã được sử dụng trong các mô hình BERT trong NLP (Devlin et al., 2019), các mô hình Wav2Vec trong Speech (Baevski et al., 2020) và gần đây, trong các mô hình MAE trong Vision (Bao et al., 2021; He et al., 2021), buộc mô hình phải học về mối quan hệ giữa nội dung trong các phần khác nhau của đầu vào bằng cách sử dụng các mục tiêu liên quan đến autoencoding. Trong bài báo này, chúng tôi đề xuất một phương pháp mới nhưng đáng ngạc nhiên đơn giản thay thế cho việc tái tạo nội dung - đó là dự đoán vị trí từ nội dung, mà không cung cấp thông tin vị trí cho nó. Làm như vậy đòi hỏi Transformer phải hiểu được mối quan hệ vị trí giữa các phần khác nhau của đầu vào, chỉ từ nội dung của chúng. Điều này tương đương với một triển khai hiệu quả trong đó tác vụ pretext là một bài toán phân loại giữa tất cả các vị trí có thể có cho mỗi token đầu vào. Chúng tôi thử nghiệm trên cả benchmark Vision và Speech, nơi phương pháp của chúng tôi mang lại cải thiện so với các baseline huấn luyện có giám sát mạnh và có thể so sánh với các phương pháp tiền huấn luyện không giám sát/tự giám sát hiện đại. Phương pháp của chúng tôi cũng cho phép các Transformer được huấn luyện không có position embeddings vượt trội hơn những mô hình được huấn luyện với thông tin vị trí đầy đủ.

1. Giới thiệu
Transformers (Vaswani et al., 2017) đã trở thành một kiến trúc thống nhất trong NLP, Thị giác Máy tính và Speech. Khả năng cao và thiếu các inductive bias cụ thể theo domain có nghĩa là Transformers đòi hỏi lượng lớn dữ liệu huấn luyện để đạt được khả năng tổng quát hóa tốt. Một biện pháp khắc phục hiệu quả, được phát triển đầu tiên trong cộng đồng NLP, là tiền huấn luyện không giám sát. Ví dụ, BERT (Devlin et al., 2019) huấn luyện một Transformer với văn bản không được gán nhãn bằng cách giải quyết việc dự đoán token bị che khuất. Điều này mang lại lợi ích rất lớn cho các ứng dụng downstream, và đã trở thành phương pháp tiêu chuẩn cho các tác vụ NLP khác nhau.

Gần đây, đã có một số nỗ lực áp dụng ý tưởng tiền huấn luyện BERT vào Thị giác Máy tính, với Vision Transformers (ViTs) (Dosovitskiy et al., 2021) là kiến trúc backbone. Cụ thể, BEiT (Bao et al., 2021) chuyển đổi các patch hình ảnh thành các token rời rạc với một VQVAE được huấn luyện riêng (van den Oord et al., 2017). Điều này làm cho việc sử dụng cùng một loss cross entropy cho việc dự đoán patch hình ảnh bị che khuất như đối với việc dự đoán token trong BERT trở nên khả thi. MAE (He et al., 2021) tiếp tục đơn giản hóa công thức của BEiT bằng cách trực tiếp dự đoán các patch bị che khuất với một loss regression trong không gian pixel.

Trong bài báo này, chúng tôi đề xuất một phương pháp đơn giản và hiệu quả cho việc tiền huấn luyện Transformer mà loại bỏ nhu cầu tái tạo các giá trị patch dày đặc. Ý tưởng của chúng tôi được truyền cảm hứng từ quan sát rằng Transformers tương đối không nhạy cảm với thứ tự của các token đầu vào. Trong (Naseer et al., 2021), nó được chỉ ra rằng các ViT được tiền huấn luyện thể hiện khả năng bền vững mạnh mẽ trước nhiễu xáo trộn patch hình ảnh tại thời điểm test. (Sinha et al., 2021) cho thấy rằng việc huấn luyện mô hình BERT với thứ tự từ được xáo trộn ngẫu nhiên mang lại hiệu suất cạnh tranh đáng ngạc nhiên. (Chen et al., 2021) cũng gợi ý rằng một ViT không có positional embeddings chỉ cho thấy sự suy giảm nhỏ trong tác vụ linear probing cho self-supervised learning. Bằng chứng này gợi ý rằng phần lớn sức mạnh của Transformers bắt nguồn từ khả năng suy luận về sự đồng xuất hiện của tập hợp các token đầu vào không có thứ tự. Do đó, chúng tôi đặt câu hỏi: Việc tiền huấn luyện không giám sát có thể học được bao nhiều chỉ sử dụng nội dung để dự đoán? Điều này thúc đẩy chúng tôi công thức hóa một chiến lược tiền huấn luyện mới, được giải thích như sau.

Trong giai đoạn tiền huấn luyện, mô hình (ví dụ, một ViT) nhận một tập hợp các token (ví dụ, các patch hình ảnh) nhưng không có vị trí của chúng, và tác vụ pretext là khôi phục vị trí của mỗi token đầu vào, được đúc khuôn như một bài toán phân loại giữa tất cả các vị trí. Bằng cách làm như vậy, chúng tôi công thức hóa một trường hợp đặc biệt của masked autoencoder, trong đó các vị trí, thay vì các token, được loại bỏ khỏi đầu vào và được dự đoán bởi mô hình. Mục tiêu huấn luyện này cũng có thể được hiểu là huấn luyện một Set Transformer (Lee et al., 2019) để giải quyết một câu đố Jigsaw (Noroozi & Favaro, 2016). Để giải quyết tác vụ, Transformer cần suy luận về tương tác bậc cao của các token đầu vào, điều này tương đương với việc hiểu ngữ nghĩa cơ bản (ví dụ, mối quan hệ phần-toàn của đối tượng được cho) được biểu diễn bởi các đầu vào. Thực nghiệm, chúng tôi đã phát hiện ra rằng các Transformer lớn thường có thể đạt được độ chính xác gần hoàn hảo trong tác vụ dự đoán vị trí. Sau đó, chúng tôi đề xuất tăng độ khó của tác vụ bằng cách chọn một tập con ngẫu nhiên các token làm context, và sửa đổi các layer attention sao cho chỉ các token context được sử dụng làm keys và values. Bằng cách này, Transformer không chỉ cần sắp xếp các token context, mà còn suy ra các vị trí cho các token bị che khuất bằng cách query vào context. Do đó, chúng tôi gọi phương pháp của mình là MP3, ký hiệu Masked Patch Position Prediction.

Trong quá trình finetuning, chúng tôi vô hiệu hóa token masking và thêm absolute positional embeddings theo cách tương tự như các Transformer tiêu chuẩn. Sau đó, chúng tôi loại bỏ linear position prediction head và thay thế nó bằng một linear head cho tác vụ downstream (ví dụ, phân loại). Tất cả các tham số được cập nhật cho số lượng bước finetuning mong muốn với mục tiêu huấn luyện của tác vụ downstream.

MP3 tương đương với một triển khai đơn giản. Trong giai đoạn tiền huấn luyện, không cần thêm module nào khác ngoài một linear head với d×n tham số, trong đó d là dimension đặc trưng của mô hình và n là số lượng vị trí. Mục tiêu huấn luyện đơn giản là cross entropy loss. Ngoài ra, nhờ vào context masking, full self-attention được giảm xuống sparse attention, điều này hiệu quả làm cho chi phí tiền huấn luyện thấp hơn so với finetuning.

Chúng tôi tiến hành thử nghiệm trên cả tác vụ Vision và Speech. MP3 liên tục cải thiện hiệu suất của các mô hình Transformer so với các baseline huấn luyện có giám sát mạnh, và sánh ngang với các phương pháp tiền huấn luyện không giám sát/tự giám sát tinh vi hơn, bất chấp sự đơn giản của nó. Đáng chú ý, MP3 cho phép hiệu suất finetuning mạnh mẽ ngay cả khi không sử dụng position embeddings, đôi khi vượt trội hơn các baseline huấn luyện có giám sát bằng một khoảng cách lớn.

2. Công trình liên quan
Denoising Autoencoders (DAEs). DAEs (Vincent et al., 2010) là các mô hình được nghiên cứu kỹ lưỡng trong bối cảnh tiền huấn luyện không giám sát. Ý tưởng là tái tạo các đầu vào được đưa ra các phiên bản nhiễu của chính chúng. Masked autoencoder (MAE) là một trường hợp đặc biệt của DAE, trong đó một phần của đầu vào bị che khuất (với nhiễu Bernoulli nhân). Khi kết hợp với Transformers, MAEs đã cho thấy thành công lớn như một kỹ thuật tiền huấn luyện không giám sát, với BERT (Devlin et al., 2019), BEiT (Bao et al., 2021) và MAE (He et al., 2021) là những ví dụ đáng chú ý. MP3 cũng có thể được xem như một trường hợp đặc biệt của MAEs, nhưng nó che khuất thông tin vị trí (và tùy chọn các token đầu vào), thay vì các token đầu vào. Mục tiêu tái tạo sau đó được chuyển thành một tác vụ sắp xếp, có ý nghĩa rất khác so với việc tái tạo các token bị thiếu được đưa ra các vị trí.

Self-supervised learning với dự đoán thứ tự. Học đặc trưng không giám sát với dự đoán thứ tự của các patch hình ảnh được đề xuất đầu tiên trong (Noroozi & Favaro, 2016), và sau đó được mở rộng trong các công trình tiếp theo như (Lee et al., 2017; Ahsan et al., 2019; Xu et al., 2019; Santa Cruz et al., 2018; El-Nouby et al., 2019). Điều mà các công trình này chia sẻ là chúng thường áp dụng một encoder dựa trên CNN cho một patch hình ảnh hoặc một clip video, và một mạng dự đoán dựa trên MLP để xuất ra thứ tự đúng của một tập hợp đầu vào (ngoại trừ (El-Nouby et al., 2019) sử dụng dự đoán thứ tự để xấp xỉ dự đoán tương lai trong video). Đầu ra của các phương pháp này sau đó là một biểu diễn địa phương cho các patch hình ảnh hoặc clip video, vì mạng dự đoán thứ tự được loại bỏ. Điều này trái ngược hoàn toàn với công trình của chúng tôi, MP3 tập trung vào việc học biểu diễn toàn cục thông qua attention. Điều này chỉ có thể thực hiện được nhờ kiến trúc Transformer mạnh mẽ, tập trung vào việc học các tương tác giữa các phần tử đầu vào, và cùng một kiến thức toàn cục được chuyển giao cho các tác vụ downstream trong bước finetuning.

Tầm quan trọng của positional embedding trong Transformers. Positional embeddings (PEs) có tầm quan trọng đặc biệt đối với Transformers, và cải thiện PEs là một lĩnh vực nghiên cứu tích cực, xem (Dufter et al., 2021) để có cái nhìn tổng quan. Tuy nhiên, người ta quan sát thực nghiệm rằng hiệu suất của Transformers có sự bền vững đáng ngạc nhiên đối với thứ tự của các đầu vào. Đối với ViTs, (Naseer et al., 2021) cho thấy rằng các ViT được tiền huấn luyện chịu ít tổn thất hơn nhiều từ nhiễu xáo trộn patch so với CNNs. (Sinha et al., 2021) cho thấy rằng các mô hình ngôn ngữ bị che khuất hoạt động tốt ngay cả khi được huấn luyện với các câu được xáo trộn. (Chen et al., 2021) cũng cho thấy rằng một Transformer không có PEs chỉ cho thấy sự suy giảm nhỏ khi được đánh giá với linear probing trong thiết lập contrastive learning. MP3 xác nhận giả thuyết rằng phần lớn sức mạnh của Transformer nằm ở khả năng mô hình hóa sự đồng xuất hiện của các token đầu vào. Cụ thể, phương pháp tiền huấn luyện của chúng tôi không sử dụng hoặc huấn luyện PEs chút nào (thay vì xáo trộn ngẫu nhiên các token đầu vào trong khi sử dụng PE), và vẫn hoạt động cạnh tranh so với các baseline khác.

Contrastive Learning. Đây là một họ các phương pháp cho self-supervised learning, trong đó mục tiêu học tập là gán độ tương tự cao cho các view được augment từ cùng một ví dụ (van den Oord et al., 2018; Chen et al., 2020; 2021; Caron et al., 2021). MP3 khác biệt vì nó không dựa vào data augmentation như nguồn tín hiệu huấn luyện, điều này mang lại cho nó sự linh hoạt hơn nhiều. Bên cạnh đó, MP3 không ép buộc clustering của biểu diễn cho các vị trí khác nhau trong một đầu vào, điều này làm cho nó không phù hợp cho các tác vụ linear probing. Những khác biệt này cũng gợi ý khả năng kết hợp MP3 và contrastive learning để đạt được điều tốt nhất của cả hai thế giới. Cũng đã có những nỗ lực kết hợp contrastive learning với các tác vụ dự đoán (Dangovski et al., 2021), điều này gợi ý các cách có thể kết hợp MP3 với contrastive learning theo cách tương tự.

Dự đoán vị trí trong NLP. Trong các công trình đồng thời, ý tưởng dự đoán vị trí cũng đã được khám phá trong lĩnh vực NLP (Cui et al., 2022; Brüel-Gabrielsson & Scarvelis, 2022). Những công trình này, kết hợp với MP3, gợi ý rằng dự đoán vị trí là một kỹ thuật đầy hứa hẹn trên một loạt rộng các vấn đề.

3. Phương pháp
3.1. Kiến trúc
Đối với Vision, kiến trúc của chúng tôi dựa trên ViTs (Dosovitskiy et al., 2021). Tóm lại, ViTs chia một hình ảnh thành các patch không chồng lắp với kích thước cho trước (ví dụ, 16×16). Một phép chiếu tuyến tính với trọng số được chia sẻ cho tất cả các patch hình ảnh để thu được một chuỗi các token hình ảnh sau đó được áp dụng. Các vector token được kết hợp cộng với các positional embedding tương ứng của chúng để tạo thành chuỗi đầu vào. Các layer self-attention tiêu chuẩn sau đó được áp dụng để xử lý chuỗi đầu vào.

Đối với Speech, kiến trúc của chúng tôi dựa trên Transformer vanilla. Đầu vào của mô hình là một chuỗi các frame của 40 mel filterbank cepstral coefficients (MFCCs), được tính từ 30ms raw waveforms, với stride 10ms giữa các frame, theo (Choi et al., 2019). Mỗi frame được biến đổi bởi cùng một phép chiếu tuyến tính vào dimension của mô hình transformer (do đó, mỗi frame được coi như một patch 1D). Một fixed sinusoidal positional embedding (Vaswani et al., 2017) được thêm vào các biểu diễn được chiếu này và kết quả được đưa vào một Transformer 8 layer. Như với ViTs, chúng tôi thêm một frame "cls" token có thể học được ở đầu chuỗi đầu vào của mô hình. So với Vision, trong Speech chúng ta có một patch là 1D thay vì 2D vì chúng ta bỏ qua cấu trúc trong domain tần số. Sau này trong văn bản, chúng tôi gọi một frame là một patch trong ngữ cảnh của các tác vụ Speech.

3.2. Masked Position Prediction Pretraining
Trong giai đoạn tiền huấn luyện, chúng tôi áp dụng cùng phép chiếu patch như các ViT tiêu chuẩn nhưng loại bỏ các positional embedding từ tất cả các biểu diễn patch. Điều này tạo ra một tập hợp các biểu diễn patch. Tiếp theo, chúng tôi chọn ngẫu nhiên tỷ lệ 1-η các patch làm "context patches", trong đó η ký hiệu tỷ lệ masking. Sau đó, chúng tôi sửa đổi các layer self-attention tương ứng, trong đó chỉ các context patch tham gia vào việc tính toán keys và values; queries được tính cho tất cả các patch. Nói cách khác, chúng tôi thực hiện cross attention từ tất cả các patch đầu vào đến các context patch. Với η > 0, Transformer cần công thức hóa một biểu diễn tốt của đầu vào chỉ được đưa ra một tập con của các patch đầu vào, trong khi sắp xếp tất cả các patch đầu vào. Điều này buộc mô hình phải suy luận về mối quan hệ của các context patch và suy ra các patch bị che khuất cùng một lúc. Như một sản phẩm phụ, tỷ lệ masking cao hiệu quả giảm chi phí tính toán của Transformer trong giai đoạn tiền huấn luyện.

Chúng tôi gắn một prediction head tuyến tính sau layer attention cuối cùng, với các dimension đầu vào và đầu ra lần lượt là dimension đặc trưng d và số lượng patch n. Các đầu ra của linear head được truyền qua Softmax để tạo thành một phân phối trên các vị trí patch. Position prediction loss được thu được với cross entropy giữa chỉ số vị trí và đầu ra của prediction head. Xem Hình 1 để có minh họa, và Phụ lục A để có sketch triển khai.

3.3. Supervised Finetuning
Sau bước tiền huấn luyện không giám sát, chúng tôi finetune mạng với nhãn. Cụ thể, chúng tôi loại bỏ position prediction head, và gắn một classifier head tuyến tính sau token "cls", như trong các ViT tiêu chuẩn. Chúng tôi cũng áp dụng các positional embedding (học được) được khởi tạo ngẫu nhiên (hoặc sinusoidal cố định) cho các patch embedding, cũng theo các ViT tiêu chuẩn. Random masking được vô hiệu hóa trong giai đoạn này và full self-attention được sử dụng. Thiết lập còn lại của bước finetuning phần lớn giống với supervised training.

4. Đánh giá
4.1. Thiết lập thử nghiệm
Trong khi đã có rất nhiều quan tâm đến việc scaling Transformers trên các dataset lớn trong tài liệu, hiệu suất của chúng trên các dataset nhỏ vẫn chưa được khám phá đầy đủ. Vì Transformers có xu hướng overfit dễ dàng với pure supervised learning, chúng tôi tin rằng việc điều tra sức mạnh của tiền huấn luyện không giám sát trong các thiết lập dữ liệu khan hiếm là rất quan trọng. Trong lĩnh vực vision, chúng tôi thử nghiệm với các dataset từ nhỏ đến vừa: CIFAR-100 (Krizhevsky et al., 2009), Tiny ImageNet và ImageNet-1K (Deng et al., 2009). Trong lĩnh vực Speech, chúng tôi không cố gắng áp dụng MP3 đầy đủ cho Automatic Speech Recognition vì khái niệm về vị trí là mơ hồ, với tính chất streaming của dữ liệu. Thay vào đó, chúng tôi đã chọn ở đây để chỉ ra proof of concept bằng cách áp dụng MP3 cho tác vụ keyword spotting, đó là một bài toán phân loại trên một đoạn âm thanh có độ dài cố định. Chúng tôi sử dụng dataset Google Speech Commands v1 (Warden, 2018) và triển khai các mô hình của chúng tôi bằng cách sử dụng triển khai có sẵn công khai của TC-ResNet (Choi et al., 2019), giữ nguyên các routine tiền xử lý âm thanh, data splits và các chi tiết khác của họ. Đối với mỗi dataset ở trên, chúng tôi chọn một cấu hình mô hình Transformer baseline, các chi tiết được tóm tắt trong Bảng 1.

4.2. Tiền huấn luyện và Finetuning trên dữ liệu Vision
Chi tiết triển khai. Đối với CIFAR-100, Tiny ImageNet và ImageNet-1k, cả thiết lập tiền huấn luyện và finetuning của chúng tôi phần lớn theo DeiT (Touvron et al., 2021), sử dụng optimizer AdamW (Loshchilov & Hutter, 2017), weight decay 0.05, drop path (Ghiasi et al., 2018) rate 0.1, RandAugment (Cubuk et al., 2020), CutMix (Yun et al., 2019), MixUp (Zhang et al., 2017), Random Erasing (Zhong et al., 2020), Repeated Augmentation (Hoffer et al., 2020) và label smoothing. Trong giai đoạn tiền huấn luyện, chúng tôi không sử dụng CutMix, MixUP, Random Erasing, Repeated Augmentation và label smoothing. Giai đoạn finetuning theo chính xác cùng protocol như các công thức supervised training được đề xuất trong (Touvron et al., 2021). Chúng tôi tìm kiếm η tối ưu cho mỗi dataset trong giai đoạn tiền huấn luyện, lần lượt là 0.5, 0.8, 0.75 cho CIFAR-100, Tiny ImageNet và ImageNet-1K. Batch size lần lượt là 256, 512 và 2048.

Baselines. Trên mỗi dataset, supervised baseline được huấn luyện với regularization mạnh. Chúng tôi cố định tổng số training epoch là 400 epoch cho CIFAR-100 và Tiny ImageNet, và 300 cho ImageNet-1K. Chúng tôi cũng xem xét hai supervised training baseline bổ sung, một không có positional embedding và một khác với 2D relative position biases (Shaw et al., 2018). Chúng tôi cũng xem xét hai phương pháp tiền huấn luyện tự giám sát dựa trên Transformer, MOCO V3 (Chen et al., 2021) và MAE (He et al., 2021). Trong cả hai trường hợp, chúng tôi sử dụng các code base chính thức và tìm kiếm siêu tham số tối ưu cho mỗi trường hợp (data augmentation, learning rate cho MOCO V3; masking ratio và learning rate cho MAE).

4.2.1. Hiệu quả tiền huấn luyện
Trước tiên, chúng tôi đo lường hiệu quả huấn luyện của MP3, so với MAE cũng như supervised training baseline ViT-B. Trong Bảng 2, chúng tôi báo cáo thời gian huấn luyện (giây mỗi iteration) và tiêu thụ bộ nhớ (gigabyte) trên ImageNet-1K với một GPU A100 duy nhất. So với ViT-B, MP3 có chi phí thời gian và bộ nhớ thấp hơn đáng kể trên các giá trị khác nhau của tỷ lệ masking η. So với MAE, MP3 có hiệu quả thuận lợi cho hầu hết các giá trị, đặc biệt khi η nhỏ.

4.2.2. Dự đoán vị trí
Tiếp theo, chúng tôi kiểm tra khả năng của một Transformer để giải quyết tác vụ dự đoán vị trí. Chúng tôi chỉ ra kết quả cho ImageNet-1K trong đó chúng tôi thay đổi tỷ lệ masking trong {0, 0.75} và huấn luyện trong 100 epoch. Chúng tôi đo lường độ chính xác dự đoán vị trí trên các tập validation với η test khác nhau. Kết quả được hiển thị trong Hình 3. Thú vị là, khi được huấn luyện với η = 0, các Transformer có thể giải quyết tác vụ gần như hoàn hảo. Tỷ lệ masking lớn η = 0.75 dẫn đến độ chính xác giảm như mong đợi, nhưng độ chính xác vẫn ở mức tốt cho đến tỷ lệ masking cao. Điều này gợi ý rằng có đủ thông tin trong chính các patch đầu vào để khôi phục thông tin vị trí tương ứng của chúng.

Để hiểu hành vi của MP3 với η lớn, chúng tôi hiển thị một ví dụ trong Hình 2. Cụ thể, chúng tôi thu được một mô hình được huấn luyện với η = 0.75, và thay đổi η tại thời điểm test. Đối với mỗi test, chúng tôi tạo ra một tập hợp ngẫu nhiên các context patch, và hiển thị các hình ảnh được tái tạo với các vị trí được dự đoán. Chúng ta thấy rằng mô hình tạo ra các tái tạo hợp lý, ngay cả khi độ chính xác tổng thể không cao (ví dụ, với η test = 0.75). Điều này gợi ý mô hình có thể học cách suy luận hiệu quả về các đối tượng cơ bản chỉ được đưa ra một tập con nhỏ, không có vị trí của các patch đầu vào. Nhiều ví dụ hơn có thể được xem trong Phụ lục F.

4.2.3. Kết quả định lượng
Chúng tôi báo cáo độ chính xác finetuning trong Bảng 3 cho CIFAR-100 và Bảng 4 cho ImageNet-1K. Trong tất cả các thử nghiệm của chúng tôi, MP3 cải thiện đáng kể độ chính xác của supervised training baseline, đôi khi bằng một khoảng cách lớn. Lưu ý rằng chúng tôi không thay đổi các siêu tham số finetuning, so với supervised training baseline, và lợi ích đến hoàn toàn từ tiền huấn luyện hiệu quả.

So với các phương pháp tiền huấn luyện tự giám sát khác, MP3 đạt được kết quả có thể so sánh. Điều này cũng đáng ngạc nhiên ở một mức độ nào đó, vì MP3 không sử dụng hoặc huấn luyện thông tin positional embedding trong giai đoạn tiền huấn luyện. Chúng tôi tiếp tục thực hiện các nghiên cứu về việc thêm zero initialized relative position biases, tương tự như BEiT (Bao et al., 2021), và không sử dụng PE trong finetuning. Relative position bias liên tục cải thiện so với phiên bản absolute PE, mặc dù với khoảng cách nhỏ. Thú vị là, phiên bản không sử dụng PE cho thấy hiệu suất mạnh mẽ, vượt trội hơn tất cả các supervised training baseline (bao gồm cả những baseline có relative position biases).

Cuối cùng, trên dataset lớn nhất ImageNet-1K của chúng tôi, MP3 chỉ cần 100 epoch tiền huấn luyện để vượt trội hơn supervised training baseline, trong đó tổng số epoch được cân bằng. Do tỷ lệ masking lớn (η = 0.75) và việc sử dụng masked attention, điều này dẫn đến giảm hiệu quả tổng chi phí huấn luyện (xem Bảng 2 cho các biện pháp hiệu quả).

Chúng tôi cũng đã thử nghiệm với backbone ViT-L lớn hơn. Với 150 epoch tiền huấn luyện, chúng tôi có thể vượt trội hơn supervised training baseline 1 điểm, cũng như MAE được tiền huấn luyện với 200 epoch (số lượng được lấy từ bài báo). Lưu ý rằng mặc dù MP3 không vượt trội hơn hiệu suất state of the art của MAE, chúng tôi tin rằng MP3 học các biểu diễn bổ sung. Để chỉ ra điều này, chúng tôi đã thực hiện một thử nghiệm ensemble đơn giản bằng cách lấy trung bình các đầu ra của một mô hình MP3 và MAE được finetuned từ Tab 4 (những mô hình có độ chính xác top 1 83.0% và 82.7%, tương ứng). Điều này dẫn đến một classifier mạnh mẽ với độ chính xác 84.0%, vượt trội hơn MAE được tiền huấn luyện với 1600 epoch. Điều này gợi ý tiềm năng lớn của việc có thể kết hợp MP3 và MAE và đạt được lợi ích thậm chí còn lớn hơn từ tiền huấn luyện.

4.2.4. Ablations
Epoch tiền huấn luyện. Chúng tôi thay đổi tổng số epoch tiền huấn luyện với mọi thứ khác được cố định, và hiển thị độ chính xác kết quả trong Hình 4. Chúng ta thấy rằng MP3 hoạt động tốt với số lượng epoch nhỏ (ví dụ, 100) nhưng liên tục được hưởng lợi từ việc tiền huấn luyện nhiều hơn.

Epoch finetuning. Đối với MP3, các position embedding không được học hoặc sử dụng trong tiền huấn luyện, điều này gợi ý rằng nó có thể được hưởng lợi từ epoch finetuning dài hơn. Để thấy điều này, chúng tôi lấy một mô hình MP3 dựa trên ViT-B được tiền huấn luyện ở 100 epoch (xem Bảng 4) và thay đổi epoch finetuning. Trong Hình 5, chúng ta thấy rằng đây thực sự là trường hợp. Hơn nữa, MP3 có thể vượt trội hơn supervised training baseline chỉ với 60 epoch finetuning (tương đương với 160 epoch huấn luyện tổng). Điều này tương ứng với việc giảm khoảng 50% thời gian huấn luyện.

Tỷ lệ masking. Chúng tôi đánh giá các mô hình được tiền huấn luyện với cùng số epoch (200) dưới các tỷ lệ masking khác nhau. Hình 6 cho thấy rằng tồn tại một giá trị η tối ưu mang lại độ chính xác finetuning cao nhất. η cực lớn dẫn đến suy giảm đáng chú ý, điều này gợi ý rằng việc huấn luyện với một tập token context khá lớn là quan trọng.

Kích thước patch. Đối với ViTs, kích thước patch ảnh hưởng đến hiệu suất của mô hình. Chúng tôi thử nghiệm với hai cấu hình patch bổ sung trên CIFAR-100 với kiến trúc ViT-S mặc định. Hình 7 cho thấy độ chính xác cho các supervised training baseline và kết quả finetuning. Chúng ta thấy những cải thiện nhất quán trên các kích thước patch nhỏ và lớn.

4.2.5. Trực quan hóa và hiểu attention
Những cải thiện được chứng minh bởi MP3 trong Phần 4 đặt ra hai câu hỏi quan trọng: các đặc tính định tính của cơ chế attention mà MP3 học là gì, và những khía cạnh nào được bảo tồn dưới finetuning?

Chúng tôi quan sát rằng, ở tất cả các layer, MP3 tạo ra các head có tính địa phương hơn, cũng như các head có tính toàn cục hơn so với những head được tìm thấy trong các ViT có giám sát. Khi finetuning, tính địa phương của head trở nên giống hơn với một ViT có giám sát, với tính địa phương layer đầu bị sửa đổi ít hơn nhiều so với tính địa phương của các layer sau. Kết quả cho các head có tính địa phương cao ở tỷ lệ masking η = 0 được minh họa trong Hình 8. Để có lựa chọn không thiên vị đầy đủ và chi tiết hơn, xem Phụ lục E.

4.3. Tiền huấn luyện và Finetuning trên dữ liệu Speech
Đối với Google Speech Commands, chúng tôi sử dụng một mô hình Transformer với 8 layer self-attention, dropout 0.1, dimension đặc trưng 32 và dimension layer feedforward được kết nối đầy đủ 64. Mô hình có khoảng 70K tham số tổng cộng để có thể so sánh với các mô hình convolutional nhỏ nhất từ (Choi et al., 2019). Tất cả các mô hình tiền huấn luyện và finetuning được huấn luyện với thiết lập thử nghiệm hoàn toàn giống nhau như sau. Tối ưu hóa được thực hiện với Adam (Kingma & Ba, 2015) với batch size 256 và early stopping được thực hiện dựa trên độ chính xác validation. Warmup của learning được thực hiện trong 500 update với learning rate không đổi 10^-4. Sau đó, learning rate được tăng lên 10^-3 và giảm bằng hệ số 2 mỗi 10k update. Đối với supervised baseline và giai đoạn finetuning, chúng tôi cũng sử dụng label smoothing (ε=0.1) để regularization và chúng tôi huấn luyện các mô hình trong 30K update.

So với Vision, tác vụ dự đoán vị trí (bước tiền huấn luyện) rất khó - ngay cả với η = 0, độ chính xác top-1 chỉ là 4%. Tuy nhiên, giá trị top-5 accuracy cao hơn là 11% chứng minh rằng mô hình có thể học cách định vị gần đúng các patch nhưng không thể giải quyết thêm. Kết quả này cho thấy sự khác biệt giữa dữ liệu hình ảnh và âm thanh: các đặc tính độ chi tiết và tính địa phương khác nhau. Để đơn giản hóa tác vụ dự đoán vị trí, trái ngược với vision, chúng tôi sử dụng η = 0 và hơn nữa, cung cấp thông tin vị trí cho 5% patch được chọn ngẫu nhiên cho mỗi sample.

Bảng 6 cho thấy kết quả đạt được với số lượng bước tiền huấn luyện MP3 khác nhau. Có thể thấy rằng 5K bước tiền huấn luyện là đủ để cải thiện độ chính xác của mô hình. Kết quả tập test cho mô hình validation cơ sở ở trên là 94.2%, tốt hơn 2.3% so với supervised baseline với cùng kiến trúc (=91.9%).

5. Kết luận
Chúng tôi đã trình bày MP3, một phương pháp đơn giản nhưng hiệu quả cho việc tiền huấn luyện Transformer. MP3 khác biệt với hầu hết các phương pháp tiền huấn luyện dựa trên Transformer và token prediction khác, vượt qua sự phức tạp của việc thiết kế decoder tinh vi cho các đầu vào dày đặc, như hình ảnh và speech. MP3 cung cấp hiệu suất cạnh tranh trên dữ liệu và kích thước mô hình từ nhỏ đến trung bình, cho cả Vision và Speech. Cụ thể, MP3 được finetuned mà không có position embedding vượt trội hơn các supervised training baseline mạnh mẽ. Chúng tôi cũng chứng minh các đặc tính hấp dẫn của tác vụ dự đoán vị trí, điều này có ý nghĩa độc lập từ thiết lập tiền huấn luyện. Chúng tôi tin rằng các Transformer bất biến hoán vị có hiệu suất mạnh mẽ sẽ có ý nghĩa lớn đối với cộng đồng ML bền vững.

Có những hạn chế rõ ràng của công trình này. Trước hết, MP3 không được thiết kế để tạo ra các đặc trưng phân tách tuyến tính mà nhiều phương pháp tự giám sát xuất sắc (ví dụ, contrastive learning). Ngoài ra, bất chấp trực giác cấp cao về việc sắp xếp các token đầu vào và mối quan hệ của nó với hiểu biết ngữ nghĩa, không hoàn toàn rõ ràng làm thế nào finetuning được hưởng lợi từ mục tiêu tiền huấn luyện như vậy. Cuối cùng, cũng thú vị khi kiểm tra MP3 trên các ứng dụng NLP, và chúng tôi để lại nó như công việc tương lai.
