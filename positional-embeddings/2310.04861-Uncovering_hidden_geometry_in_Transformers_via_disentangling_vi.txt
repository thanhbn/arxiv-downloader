# Khám phá hình học ẩn trong Transformers thông qua việc tách biệt vị trí và ngữ cảnh

Jiajun Song*, Yiqiao Zhong†
6 tháng 2, 2024

Tóm tắt
Transformers được sử dụng rộng rãi để trích xuất ý nghĩa ngữ nghĩa từ các token đầu vào, tuy nhiên chúng thường hoạt động như các mô hình hộp đen. Trong bài báo này, chúng tôi trình bày một phép phân tách đơn giản nhưng có tính thông tin cao của các trạng thái ẩn (hoặc embeddings) của các transformer đã được huấn luyện thành các thành phần có thể diễn giải. Đối với bất kỳ lớp nào, các vector embedding của các mẫu chuỗi đầu vào được biểu diễn bởi một tensor h∈RC×T×d. Cho vector embedding hc,t∈Rd tại vị trí chuỗi t≤T trong một chuỗi (hoặc ngữ cảnh) c≤C, việc trích xuất các hiệu ứng trung bình tạo ra phép phân tách

hc,t=µ+post+ctxc+residc,t

trong đó µ là vector trung bình toàn cục, post và ctxc lần lượt là các vector trung bình qua các ngữ cảnh và qua các vị trí, và residc,t là vector dư. Đối với các kiến trúc transformer phổ biến và các tập dữ liệu văn bản đa dạng, về mặt thực nghiệm chúng tôi tìm thấy cấu trúc toán học phổ biến: (1) (post)t tạo thành một hình dạng liên tục, thấp chiều, và thường có dạng xoắn ốc qua các lớp, (2) (ctxc)c thể hiện cấu trúc cụm rõ ràng rơi vào các chủ đề ngữ cảnh, và (3) (post)t và (ctxc)c gần như trực giao. Chúng tôi lập luận rằng tính mượt mà là phổ biến và có lợi cho các transformer được huấn luyện trên ngôn ngữ, và phép phân tách của chúng tôi dẫn đến khả năng diễn giải mô hình được cải thiện.

1 Giới thiệu
Transformers (Vaswani et al., 2017) là các mô hình mạng nơ-ron thực tế làm nền tảng cho những thành công gần đây của các mô hình ngôn ngữ lớn (LLMs) (Brown et al., 2020; Bubeck et al., 2023). Thật không may, transformers thường được sử dụng như các mô hình hộp đen do thiếu các phân tích sâu về cơ chế nội bộ, điều này gây ra những lo ngại như thiếu khả năng diễn giải, thiên vị mô hình, vấn đề bảo mật, v.v., (Bommasani et al., 2021). Đặc biệt, việc hiểu thông tin nào được các embedding từ mỗi lớp nắm bắt còn kém. Chúng tôi xác định hai yêu cầu: (1) các phép đo định lượng nội bộ, đặc biệt cho các lớp trung gian; (2) các công cụ trực quan hóa và chẩn đoán được thiết kế riêng cho transformers ngoài các biểu đồ ma trận attention.

Hãy giới thiệu các ký hiệu cơ bản. Một chuỗi đầu vào bao gồm T token liên tiếp (ví dụ, từ hoặc từ con), và một corpus là một tập hợp tất cả các chuỗi đầu vào. Gọi C là tổng số chuỗi đầu vào và c≤C biểu thị một chuỗi chung, có thể được biểu diễn bởi xc,1, . . . ,xc,T trong đó mỗi

*Phòng thí nghiệm Khóa quốc gia về Trí tuệ nhân tạo tổng quát, Viện Trí tuệ nhân tạo tổng quát Bắc Kinh (BIGAI), Bắc Kinh 100080, Trung Quốc, songjiajun@bigai.ai
†Khoa Thống kê, Đại học Wisconsin–Madison, WI, 53706, Hoa Kỳ, yiqiao.zhong@wisc.edu

xc,t tương ứng với một token. Chúng ta bắt đầu từ các embedding tĩnh (và vị trí) ban đầu (h(0)c,t)t≤T và sau đó tính toán các embedding của lớp trung gian (h(ℓ)c,t)t≤T:

h(0)c,1, . . . ,h(0)c,T=Embed (xc,1, . . . ,xc,T)
h(ℓ)c,1, . . . ,h(ℓ)c,T=TFLayer ℓ(h(ℓ−1)c,1, . . . ,h(ℓ−1)c,T),

cho ℓ= 1, . . . , L , trong đó Embed và TFLayer ℓ là các ánh xạ tổng quát. Định nghĩa tổng quát này bao gồm nhiều mô hình transformer, phụ thuộc vào các attention heads được định nghĩa như sau. Cho dhead≤d và ma trận đầu vào X∈RT×d, với các trọng số có thể huấn luyện Wq,Wk,Wv∈Rd×dhead, định nghĩa

AttnHead( X) = softmaxXWq(Wk)⊤X⊤/√dheadXWv. (1)

Các multi-head attention heads, ký hiệu bởi MHA, về cơ bản là sự nối các nhiều attention heads. Ký hiệu một lớp fully-connected chung bởi FFN(x) =W2max{0,W1x+b1}+b2 cho bất kỳ x∈Rd với các trọng số có thể huấn luyện W1∈Rd′×d,W2∈Rd×d′,b1∈Rd′,b2∈Rd (thường d′= 4d), và gọi LN là một lớp layer normalization chung. Transformer tiêu chuẩn được biểu diễn như

h(ℓ+0.5)c =h(ℓ)c+MHA(ℓ)(LN(ℓ,1)(h(ℓ)c)),
h(ℓ+1)c =h(ℓ+0.5)c +FFN(ℓ)(LN(ℓ,2)((h(ℓ+0.5)c )))

trong đó h(ℓ+0.5)c = (h(ℓ+0.5)c,1, . . . ,h(ℓ+0.5)c,T) và h(ℓ)c= (h(ℓ)c,1, . . . ,h(ℓ)c,T).

1.1 Một phép phân tách dựa trên giá trị trung bình

Đối với mỗi vector embedding h(ℓ)c,t∈Rd từ bất kỳ transformer đã được huấn luyện nào, xem xét phép phân tách

h(ℓ)c,t=µ(ℓ)+pos(ℓ)t+ctx(ℓ)c+resid(ℓ)c,t, (2)

µ(ℓ):=1/(CT)∑c,th(ℓ)c,t,pos(ℓ)t:=1/C∑ch(ℓ)c,t−µ(ℓ), (3)

ctx(ℓ)c:=1/T∑th(ℓ)c,t−µ(ℓ). (4)

Mỗi thành phần trong bốn thành phần có những diễn giải như sau. Đối với bất kỳ lớp ℓ cho trước,

• chúng ta gọi µ(ℓ) là vector trung bình toàn cục, không phân biệt các ngữ cảnh hay vị trí;
• chúng ta gọi (pos(ℓ)t)t≤T là cơ sở vị trí, vì chúng định lượng các hiệu ứng vị trí trung bình;
• chúng ta gọi (ctx(ℓ)c)c≤C là cơ sở ngữ cảnh, vì chúng định lượng các hiệu ứng chuỗi/ngữ cảnh trung bình;
• chúng ta gọi (resid(ℓ)c,t)t≤T,c≤C là các vector dư, nắm bắt các hiệu ứng bậc cao hơn.
• Ngoài ra, chúng ta định nghĩa cvec(ℓ)c,t=ctx(ℓ)c+resid(ℓ)c,t.

Một corpus có thể chứa hàng tỷ token. Để sử dụng thực tế, trong bài báo này C nhỏ hơn nhiều: chúng tôi lấy mẫu các chuỗi đầu vào từ corpus; ví dụ, C= 6.4K trong Hình 1.

Cơ sở vị trí so với embeddings vị trí. Trong khi embeddings vị trí ở Lớp 0 được khám phá nhiều trong tài liệu (xem Mục 6), cấu trúc ở các lớp trung gian được hiểu kém. Ngược lại, phương pháp của chúng tôi cung cấp thông tin cấu trúc cho tất cả các lớp.

1.2 Kết nối với ANOVA

Phép phân tách embedding của chúng tôi tương tự với ANOVA hai chiều đa biến về hình thức. Mượn thuật ngữ tiêu chuẩn từ ANOVA, vị trí và ngữ cảnh có thể được coi là hai yếu tố hoặc xử lý, vì vậy xem embedding hc,t như biến phản hồi, thì các cơ sở vị trí/ngữ cảnh biểu diễn các hiệu ứng trung bình.

Về thuật ngữ. (i) Chúng tôi sử dụng ngữ cảnh để chỉ một chuỗi vì các token của nó cùng nhau mã hóa thông tin ngữ cảnh. (ii) Chúng tôi gọi cơ sở vị trí/ngữ cảnh để thuận tiện. Một thuật ngữ chính xác hơn là khung hoặc cơ sở quá hoàn chỉnh, vì (pos(ℓ)t)t≤T và (ctx(ℓ)t)c≤C thường phụ thuộc tuyến tính.

1.3 Tính tái tạo dễ tiếp cận

Chúng tôi cung cấp một triển khai nhanh qua Google Colab tái tạo hầu hết các hình và phân tích cho GPT-2 (dưới vài phút với tùy chọn GPU):
https://colab.research.google.com/drive/1ubsJQvLkOSQtiU8LoBA_79t1bd5-5ihi?usp=sharing .
Việc triển khai hoàn chỉnh, cũng như các biểu đồ và phép đo bổ sung, có thể được tìm thấy trên trang GitHub sau.
https://github.com/JiajunSong629/uncover-hidden-geometry

1.4 Ký hiệu

Đối với một vector x, chúng ta ký hiệu chuẩn ℓ2 của nó bởi ∥x∥. Đối với một ma trận A∈Rn×m, chúng ta ký hiệu chuẩn toán tử của nó bởi ∥A∥op:= max u:∥u∥=1∥Au∥ và chuẩn max bởi ∥A∥max:= max i,j|Aij|. Chúng ta sử dụng ký hiệu big-O tiêu chuẩn: đối với các số vô hướng dương a, b, chúng ta viết a=O(b) nếu a≤Cb cho một hằng số C. Chúng ta sử dụng span(M) để ký hiệu không gian tuyến tính sinh bởi các vector cột trong M.

2 Cấu trúc hình học phổ biến

Chúng tôi áp dụng phép phân tách của mình cho nhiều transformer đã được huấn luyện trước bao gồm GPT-2 Radford et al. (2019), BERT Devlin et al. (2018), BLOOM Scao et al. (2022), Llama-2 Touvron et al. (2023), và các tập dữ liệu khác nhau như WikiText, OpenWebText, GitHub. Xem Mục A để biết chi tiết. Các phát hiện hình học của chúng tôi được tóm tắt dưới đây.

1. Cơ sở vị trí là một thành phần quan trọng và xấp xỉ low-rank, tạo thành một hình dạng liên tục và cong.
2. Cơ sở ngữ cảnh có các mẫu cụm mạnh tương ứng với tài liệu/chủ đề.
3. Cơ sở vị trí và cơ sở ngữ cảnh gần như trực giao (hoặc không liên kết).

Các phát hiện của chúng tôi chỉ ra rằng embeddings chứa hai yếu tố có thể diễn giải chính, được tách rời do không liên kết.

Các mẫu hình sin được học, không được định trước. Các transformer mà chúng tôi kiểm tra được huấn luyện từ đầu bao gồm cả embeddings vị trí (PEs). Không rõ a priori tại sao một mẫu mượt mà và hình sin như vậy được quan sát nhất quán. Hơn nữa, các mẫu hình sin đã học khác với embedding hình sin cố định ban đầu Vaswani et al. (2017): ví dụ, PE ban đầu được tập trung ít hơn vào các thành phần tần số thấp (Mục B.1).

Mẫu nhất quán qua các lớp và mô hình không được giải thích hoàn toàn bởi các kết nối dư. Cấu trúc hình học (i) nhất quán qua các lớp và (ii) không phụ thuộc vào mô hình. Liệu tính nhất quán qua các lớp có thể được giải thích bởi các kết nối dư? Chúng tôi chỉ ra điều này không đúng: chuẩn trung bình của embeddings tăng hơn 100 lần trong GPT-2, và các embeddings gần như trực giao giữa lớp 0 và 1 (Mục B.2).

Một số ngoại lệ. Trong vài lớp cuối, embeddings có xu hướng bất đẳng hướng Ethayarajh (2019) và cấu trúc hình học có thể sụp đổ thành không gian chiều thấp hơn, đặc biệt là cơ sở vị trí. Có thể do việc hoàn thành ngữ cảnh hóa hoặc các hiện tượng tối ưu hóa. Chúng tôi cũng thấy rằng BERT thể hiện các mẫu tần số cao hơn, có thể do mục tiêu huấn luyện khác.

3 Các tính chất chính của cơ sở vị trí: low rank và tần số thấp

Trong mục này, chúng tôi sử dụng các phép đo định lượng để liên kết tính mượt mà với các khái niệm trong biểu diễn tiết kiệm.

Đo lường tính mượt mà. Khái niệm "tính mượt mà" của cơ sở vị trí của chúng tôi đề cập đến ma trận Gram (chuẩn hóa) của nó:

G=¯P⊤¯P∈RT×T,trong đó ¯P= [pos1/∥pos1∥, . . . ,posT/∥posT∥] (5)

là mượt mà về mặt hình ảnh (về mặt toán học, có đạo hàm rời rạc bị chặn).

3.1 Low rank thông qua phân tích phổ

Hình 3: Phân tích phổ và Fourier dựa trên mô hình GPT-2 và OpenWebText. Trái: 60 giá trị kỳ dị hàng đầu của P. Phải: Áp dụng biến đổi cosine rời rạc 2D cho ¯P⊤¯P, chúng tôi hiển thị 10 hệ số tần số đầu tiên.

Low rank. Chúng tôi thấy rằng cơ sở vị trí tập trung xung quanh một không gian con chiều thấp. Trong cột "rank estimate" của Bảng 1, chúng tôi báo cáo ước lượng rank của cơ sở vị trí được tính trung bình qua tất cả các lớp sử dụng phương pháp của Donoho et al. (2023). Trong Hình 3, chúng tôi vẽ biểu đồ các giá trị kỳ dị hàng đầu theo thứ tự giảm dần của P= [pos1, . . . ,posT]. Có thể thấy, có một sự thay đổi sắc nét trong biểu đồ, chỉ ra một cấu trúc low-rank chiếm ưu thế. Trong Mục C.2, chúng tôi báo cáo các ước lượng rank chi tiết.

Quan trọng về chuẩn tương đối. Chúng tôi cũng thấy rằng thường thì, cơ sở vị trí chiếm một tỷ lệ đáng kể của embeddings. Trong Bảng 1, chúng tôi báo cáo chuẩn tương đối (trung bình qua các lớp) ∥P∥op/∥M∥op, trong đó M chứa các vector embedding đã được căn giữa hc,t−µ và các cột của P là các post tương ứng. Các chuẩn tương đối cho thấy cơ sở vị trí đóng góp đáng kể vào độ lớn tổng thể (hầu hết các số lớn hơn 10%).

3.2 Tần số thấp thông qua phân tích Fourier

Trong Hình 3 (phải), chúng tôi áp dụng biến đổi cosine rời rạc 2D cho ma trận Gram chuẩn hóa ¯P⊤¯P; cụ thể, chúng tôi tính toán ma trận tần số ˆG bằng cách áp dụng biến đổi cosine rời rạc (loại-II) với ma trận trực giao ˜F:

ˆG=˜FG˜F⊤.

Mỗi phần tử ˆGij mã hóa hệ số tần số (i, j)-th. Chúng tôi khám phá rằng năng lượng được tập trung chủ yếu trong các thành phần tần số thấp, điều này phản ánh cấu trúc mượt mà và cong trong Hình 1.

3.3 Góc nhìn lý thuyết: tính mượt mà giải thích low-rank và tần số thấp

Việc tính mượt mà của một hàm được kết nối với sự phân rã nhanh hoặc tính thưa thớt trong miền tần số là điều được biết đến (Pinsky, 2008, Mục 1.2.3). Từ quan điểm cổ điển, chúng tôi thiết lập tính mượt mà như một tính chất quan trọng tạo ra hình học quan sát được.

Tính mượt mà của ma trận Gram của cơ sở vị trí tạo ra hình dạng chiều thấp và xoắn ốc. Để thuận tiện, chúng ta giả sử rằng các vector vị trí post có chuẩn đơn vị, vì vậy theo định nghĩa, pos1+. . .+ posT=0. Để định lượng tính mượt mà, chúng tôi giới thiệu định nghĩa về hiệu hữu hạn. Như với biến đổi cosine rời rạc trong 1D, chúng tôi mở rộng và phản chiếu ma trận Gram để tránh các hiệu ứng biên.

Gọi G(1)=G và G(2),G(3),G(4)∈RT×T được định nghĩa bởi G(2)t,t′=Gt,T+1−t′, G(3)t,t′=GT+1−t,t′, G(4)t,t′=GT+1−t,T+1−t′ với bất kỳ t, t′= 1,2, . . . T . Chúng ta mở rộng và phản chiếu G bởi

˜G:=[G(1) G(2); G(3) G(4)]. (6)

Chúng ta định nghĩa hiệu hữu hạn bậc nhất bởi (sử dụng mở rộng tuần hoàn ˜Gt±2T,t′±2T=˜Gt,t′)

[∆(1,1)˜G]t,t′=T²(˜Gt,t′−˜Gt−1,t′−˜Gt,t′−1+˜Gt−1,t′−1) (7)

cho tất cả các số nguyên t, t′. Các hiệu hữu hạn bậc cao hơn được định nghĩa đệ quy bởi ∆(m,m)˜G= ∆(1,1)[∆(m−1,m−1)˜G]. Lưu ý rằng ∆(m,m)˜G đo lường tính mượt mà bậc cao hơn của ˜G. Thật vậy, nếu Gt,t′=f(t/T, t′/T) cho một hàm mượt mà f(x, y) nào đó được định nghĩa trên [0,1]², thì [∆(m,m)˜G]t,t′≈∂mx∂myf(t/T, t′/T).

Định lý 1. Cố định các số nguyên dương k≤T và m. Định nghĩa vector tần số thấp fs= (1 ,cos((s− 0.5)π/T), . . . , cos((s−0.5)(T−1)π/T))⊤∈RT trong đó s= 1, . . . , k , và ký hiệu F≤k= [f1, . . . ,fk]∈ RT×k. Khi đó tồn tại B∈Rk×k sao cho

1/T∥G−F≤kB(F≤kB)⊤∥op≤6/(8k)m∥∆(m,m)˜G∥max. (8)

Định lý này ngụ ý rằng nếu ma trận Gram mở rộng có tính mượt mà bậc cao hơn, cụ thể là ∥∆(m,m)˜G∥max bị chặn bởi một hằng số, thì ngay cả với k và m vừa phải, chúng ta có xấp xỉ G≈F≤kB(F≤kB)⊤. Lưu ý rằng F≤kB bao gồm các tổ hợp tuyến tính của các vector tần số thấp. Điều này giải thích tại sao G có một thành phần low-rank và tần số thấp chiếm ưu thế.

4 Tính mượt mà: phúc lành của ngôn ngữ tự nhiên

Chúng tôi chứng minh rằng tính mượt mà là một tính chất tự nhiên và có lợi được học từ dữ liệu ngôn ngữ: nó mạnh mẽ với sự dịch chuyển phân phối và cho phép tính toán attention hiệu quả. Tính mượt mà này có thể phản ánh bản chất của dữ liệu ngôn ngữ.

4.1 Tính mượt mà mạnh mẽ trong dữ liệu ngoài phân phối

Cho đến nay, chúng tôi đã phân tích phép phân tách và hình học liên quan của mình chủ yếu trên các mẫu trong phân phối. Ví dụ, chúng tôi lấy mẫu các chuỗi từ OpenWebText—cùng corpus mà GPT-2 được tiền huấn luyện, để tính toán phép phân tách (3)–(4).

Bây giờ chúng tôi lấy mẫu dữ liệu ngoài phân phối (OOD) và tiến hành phân tích và phân tách tương tự. Chúng tôi thấy rằng cơ sở vị trí có cấu trúc low-rank và xoắn ốc tương tự. Đáng ngạc nhiên, đối với các chuỗi bao gồm các token được lấy mẫu ngẫu nhiên, cấu trúc như vậy vẫn tồn tại. Xem tóm tắt trong Bảng 2 và chi tiết trong Mục D.1.

Nhiều LLMs có thể tổng quát hóa tốt cho dữ liệu OOD. Chúng tôi tin rằng khả năng tổng quát hóa này ít nhất một phần được quy cho tính mạnh mẽ của cơ sở vị trí như được chứng minh ở đây.

4.2 Tính mượt mà thúc đẩy attention cục bộ và thưa thớt

Nhiều attention heads trong các transformer quy mô lớn cho dữ liệu ngôn ngữ thể hiện các mẫu cục bộ và thưa thớt (Beltagy et al., 2020). Một giải thích phỏng đoán thường gặp là, hầu hết thông tin để dự đoán token được chứa trong một cửa sổ cục bộ.

Chúng tôi đưa ra một giải thích thay thế: Attention cục bộ là hệ quả của tính mượt mà của cơ sở vị trí. Chúng tôi đưa ra lý luận của mình. Đầu tiên, về mặt tính mượt mà, ma trận Gram PP⊤ có liên quan chặt chẽ với PWP⊤, trong đó W=Wq(Wk)⊤/√dhead. Thật vậy, nói chung một phép biến đổi tuyến tính của các vector vị trí, cụ thể là WP⊤, không nên ảnh hưởng nhiều đến tính mượt mà.

Thứ hai, PWP⊤ là một thành phần quan trọng—thường chiếm ưu thế—của ma trận QK (ma trận QK là ma trận bên trong softmax trong (1)); xem Mục 5.2 để biết ví dụ. Do đó, ma trận QK có thể kế thừa tính mượt mà từ PWP⊤.

Thứ ba, nếu ma trận QK (ký hiệu bởi B) mượt mà và được tối đa hóa tại vị trí đường chéo dọc theo một hàng nào đó được chỉ mục bởi t (ràng buộc t′≤t là do causal masking):

arg max1≤t′≤t Bt,t′=t, (8)

thì các vị trí t′ gần t cũng có giá trị QK cao do tính mượt mà. Do đó, chúng ta mong đợi các vị trí lân cận xung quanh t sẽ nhận được trọng số attention cao sau softmax.

Lý luận của chúng tôi được hỗ trợ bởi tính mượt mà phổ biến được thể hiện trong Bảng 3. Hơn nữa, chúng tôi thấy rằng các thành phần vị trí trong hơn 43% heads trong GPT-2 thỏa mãn (8) cho hơn 80% vị trí t. Xem Mục D.2 để biết chi tiết.

4.3 Lời nguyền của tính gián đoạn cho các tác vụ số học

Hình 5: Tác vụ cộng được huấn luyện trên NanoGPT thể hiện các mẫu không mượt mà: tính gián đoạn như hệ quả của việc huấn luyện dữ liệu không phải ngôn ngữ. Trái: Ma trận Gram của cơ sở vị trí chuẩn hóa. So sánh với phần trên bên trái của các biểu đồ trong Hình 2. Phải: Ma trận QK.

Chúng tôi thấy rằng sự xuất hiện của tính mượt mà phụ thuộc vào dữ liệu: trong khi các transformer được tiền huấn luyện trên ngôn ngữ tự nhiên/lập trình thể hiện tính chất mượt mà, chúng có thể gặp khó khăn do thiếu tính mượt mà trên dữ liệu tiền huấn luyện khác.

Chúng tôi khám phá một tác vụ số học đơn giản—Cộng, trong đó đầu vào được định dạng như một chuỗi "a+b= c" với a, b, c được biểu diễn bởi các chữ số có độ dài nhất định. Chúng tôi lấy mẫu độ dài của mỗi thành phần cộng một cách đồng đều từ {L/2, . . . , L} trong đó L= 10. Sau đó, chúng tôi huấn luyện một transformer 4-lớp 4-head (NanoGPT) với tokenization cấp ký tự để dự đoán 'c' dựa trên prompt "a+b=". Chúng tôi huấn luyện transformer này 10.000 lần lặp cho đến khi hội tụ.

Mẫu không mượt mà. Hình 5 cho thấy ma trận Gram của cơ sở vị trí chuẩn hóa và ma trận QK có thể nhìn thấy là gián đoạn và thể hiện nhiều vùng bị vỡ. Về mặt định lượng, 10 thành phần tần số thấp hàng đầu của ma trận Gram giải thích khoảng 50% của ∑ijĜ²ij (Mục D.3), ít hơn nhiều so với 99% trong GPT-2 như trong Bảng 3. Điều này gợi ý một sự phân biệt sắc nét giữa dữ liệu ngôn ngữ vs. không phải ngôn ngữ về mặt hình học được tạo ra.

Thất bại trong tổng quát hóa độ dài. NanoGPT của chúng tôi đạt trên 99% độ chính xác kiểm tra trong phân phối, nhưng thất bại trong tổng quát hóa OOD: nó có ít hơn 20% độ chính xác trung bình trên các chữ số có độ dài nhỏ hơn 5. Chúng tôi tin rằng tính không mượt mà có thể là một điểm nghẽn nội tại cho các tác vụ số học.

Kết quả của chúng tôi cũng đúng cho các transformer với embeddings vị trí tương đối; xem Mục D.3.

5 Tính không liên kết tăng cường khả năng diễn giải

Tính gần-trực giao, hoặc (mutual) incoherence, được biết đến là một tính chất quan trọng cho học thưa thớt. Nói chung, incoherence có nghĩa là các yếu tố hoặc đặc trưng gần như không tương quan và tách rời. Trong tình huống lý tưởng của tính trực giao, các yếu tố có thể được phân tách thành các không gian con trực giao không can thiệp. Incoherence có liên quan chặt chẽ với restricted isometry (Candes & Tao, 2005), điều kiện irrepresentable (Zhao & Yu, 2006), v.v.

Chúng tôi quan sát tính chất incoherence giữa cơ sở vị trí và cơ sở ngữ cảnh, như Bảng 1 cho thấy rằng maxt,c|⟨post/∥post∥,ctxc/∥ctxc∥⟩| thường nhỏ. Incoherence thấp trong Bảng 1 (đường cơ sở ngẫu nhiên khoảng 0.1) có nghĩa là hai cơ sở gần như trực giao với nhau.

Để tách rời các hiệu ứng khác nhau, trong mục này, chúng tôi sẽ tập trung vào hiệu ứng vị trí vs. hiệu ứng không phải vị trí, do đó làm việc với cvecc,t thay vì ctxc.

5.1 Tách rời các hiệu ứng vị trí trong các trọng số đã được huấn luyện trước

Chúng tôi trình bày bằng chứng rằng ma trận trọng số đã được huấn luyện W:=Wq(Wk)⊤/√dhead trong self-attention có một phép phân tách rõ ràng và có thể diễn giải: theo phỏng đoán, một thành phần low rank điều chế các hiệu ứng vị trí, và một thành phần đường chéo tăng cường hoặc giảm bớt các hiệu ứng token.

Chính xác hơn, chúng tôi xác định một cấu trúc low-rank cộng noise phổ biến trong đa số các attention heads trong các transformer đã được huấn luyện trước.

W=VLV⊤ + D + Noise. (9)

Ở đây, các cột của V∈Rd×K là K vector kỳ dị phải hàng đầu của ma trận cơ sở vị trí P, và L∈RK×K, trong đó K không lớn.

Hình 4 cho thấy hỗ trợ thực nghiệm cho tuyên bố cấu trúc (9). Cho một trọng số đã được huấn luyện trước W, chúng tôi lấy D= diag(W) và hiển thị các phần tử màu đỏ. Sau đó, chúng tôi xoay phần ngoài đường chéo của W bởi các vector kỳ dị phải của P và áp dụng khử nhiễu, cụ thể là đặt bằng không các phần tử có giá trị tuyệt đối nhỏ hơn một ngưỡng. Đối với nhiều heads, các giá trị tuyệt đối lớn còn sót lại được tập trung ở phần trên bên trái (K≈20)—điều này gợi ý rằng thật vậy một thành phần đáng kể của W được căn chỉnh với cơ sở vị trí.

Phép phân tách này gợi ý một cơ chế có thể bên trong tính toán attention. Xem xét một tình huống lý tưởng trong đó D là bội số của ma trận đơn vị, và mỗi embedding có phép phân tách trực giao h= t+c với t∈span(V) mã hóa thông tin vị trí và c∈span(V)⊥ mã hóa thông tin không phải vị trí. Khi đó, đối với hai vector embedding h,h′ với h=t+c và h′=t′+c′,

h⊤Wh′≈t⊤(VLV⊤+D)t′ + c⊤Dc′

Các hiệu ứng vị trí được tách rời khỏi các hiệu ứng ngữ cảnh, vì các số hạng chéo liên quan đến t,c′ hoặc t′,c biến mất. Phỏng đoán này có thể cho phép chúng ta kiểm tra cách thông tin được xử lý trong các attention heads, điều này sẽ được khám phá trong nghiên cứu tương lai.

5.2 Vượt ra ngoài trực quan hóa attention

Trực quan hóa attention thường được sử dụng như một chẩn đoán của self-attention trong transformers. Chúng tôi chỉ ra rằng có thể tiết lộ thêm cấu trúc bằng cách phân tách ma trận QK sử dụng phép phân tách embeddings của chúng tôi.

Chúng tôi bắt đầu với việc phân tách ma trận QK. Giả sử µ=0 (bỏ qua hiệu ứng trung bình toàn cục để thuận tiện), thì đối với các vector embedding h,h′∈Rd chúng ta có

h⊤Wq(Wk)⊤h= pos⊤Wq(Wk)⊤pos + pos⊤Wq(Wk)⊤cvec +cvec⊤Wq(Wk)⊤pos + cvec⊤Wq(Wk)⊤cvec. (10)

Mỗi thành phần trong bốn thành phần cho thấy một attention head nắm bắt bao nhiều thông tin từ các cặp chéo pos/cvec—pos/cvec của một embedding. Chúng tôi đề xuất trực quan hóa từng thành phần QK riêng biệt, điều này cho biết liệu một attention head có nắm bắt thông tin vị trí hay thông tin ngữ cảnh/token từ embeddings.

Nghiên cứu trường hợp: induction heads. Elhage et al. (2021) đã xác định các thành phần trong transformers hoàn thành một mẫu chuỗi dựa trên các token quá khứ đã quan sát, cụ thể là, dự đoán token tiếp theo [B] dựa trên chuỗi đã quan sát [A],[B], . . . , [A]. Khả năng sao chép các token trước đó này được biết đến là do induction heads gây ra.

Có ba mẫu attention đại diện như được thể hiện trong Hình 6: (i) attention đến các token giống hệt, cụ thể là trọng số attention tập trung vào các token giống hệt với token hiện tại, (ii) attention đến các token lân cận, (iii) attention đến các token sẽ được sao chép. Xem thêm Elhage et al. (2021).

Chỉ trực quan hóa attention không tiết lộ tại sao một số heads nhất định xuất hiện. Ngược lại, phép phân tách QK của chúng tôi tiết lộ thành phần QK nào chiếm ưu thế và chịu trách nhiệm cho các mẫu attention. Ví dụ, trong Hình 6,

• attention đến các token lân cận (biểu đồ giữa) chủ yếu được xác định bởi thành phần pos-pos;
• attention đến token giống hệt (trái) hoặc token được dịch chuyển (phải) được xác định bởi thành phần cvec-cvec.

Điều này ngụ ý rằng induction heads không dựa trên việc ghi nhớ các vị trí tương đối, mà dựa trên việc khớp thông tin token.

Thêm chi tiết trong Mục E.2.

5.3 Thông tin lý thuyết từ phân tích nhân tử kernel

Tại sao incoherence xuất hiện từ các transformer đã được huấn luyện trước? Trong khi đã được biết đến trong sparse coding và compressed sensing rằng cơ sở incoherent được chế tạo thủ công tạo điều kiện thuận lợi cho việc khôi phục các tín hiệu thưa thớt (Donoho & Stark, 1989; Donoho & Elad, 2003; Donoho, 2006; Candès et al., 2006), thật ngạc nhiên khi incoherence phát sinh từ học đặc trưng tự động.

Ở đây chúng tôi tập trung vào cơ chế self-attention của transformers. Bằng cách áp dụng quan điểm kernel, chúng tôi trình bày một lý thuyết sơ bộ cho phỏng đoán sau:

Incoherence cho phép một kernel phân tích thành các thành phần nhỏ hơn, mỗi thành phần hoạt động độc lập.

Cho các ma trận query/key Wq,Wk∈Rd×dhead, chúng tôi định nghĩa kernel (bất đối xứng) bởi (nhớ lại W= Wq(Wk)⊤/√dhead)

KW(z,z′) := exp(z⊤Wz′/√dhead) = exp(⟨Wqz,Wkz′⟩/√dhead).

Sử dụng KW, attention có thể được biểu diễn như làm mượt kernel: đối với embeddings (xt)t≤T⊂Rd,

AttnHead(xt;KW) =∑k≤t KW(xk,xt)/∑k′≤t KW(xk′,xt) v(xk) (11)

trong đó v:Rd→R là một hàm giá trị chung. Quan điểm kernel này được khám phá trong Tsai et al. (2019), nơi được lập luận rằng hiệu quả của self-attention phụ thuộc chủ yếu vào dạng của kernel.

Giả sử rằng có hai cơ sở quá hoàn chỉnh B₁⁰,B₂⁰⊂Rd. Để đơn giản, giả sử rằng ∥u∥₂≤1 nếu u∈B₁⁰ hoặc B₂⁰. Mutual incoherence là incoh := max{|⟨c,t⟩|:c∈B₁⁰,t∈B₂⁰}. Xem xét cơ sở quá hoàn chỉnh (mở rộng) Bα:={λu:u∈B₀α, λ∈[−1,1]} trong đó α∈{1,2}. Cho các vector query/key xq,xk∈Rd, giả sử rằng chúng ta có thể phân tách chúng theo hai cơ sở.

xq=cq+tq,xk=ck+tk,cho cq,ck∈B₁;tq,tk∈B₂. (12)

Một cách chung, chúng ta có thể phân tách KW(xq,xk) thành

KW(cq,ck)KW(cq,tk)·KW(tq,ck)KW(tq,tk).

Mỗi thành phần đo lường độ tương tự chéo của các cặp giữa cq,tq và ck,tk, sau đó chuyển thành trọng số cho attention. Thật không may, phép phân tách tổng quát này yêu cầu các kernel riêng lẻ chia sẻ cùng trọng số W, điều này cản trở việc nắm bắt các tương tác chéo một cách linh hoạt.

Hóa ra nếu ma trận trọng số được biểu diễn thưa thớt bởi các cơ sở, thì tính linh hoạt của kernel có thể đạt được. Chính xác, chúng tôi sẽ nói rằng W∈Rd×d được biểu diễn s-thưa thớt bởi các cơ sở B,B′ nếu tồn tại (ak)k≤s⊂[−1,1],(uk)k≤s⊂B,(vk)k≤s⊂B′ sao cho

W=∑k≤s akukv⊤k. (13)

Định lý 2. Gọi W11,W12,W21,W22∈Rd×d là các ma trận bất kỳ có các tính chất sau: với α, β∈{1,2}, Wαβ∈Rd×d được biểu diễn O(1)-thưa thớt bởi các cơ sở Bα,Bβ. Khi đó với tất cả xq,xk∈Rd thỏa mãn (12), W=W11+W12+W21+W22 thỏa mãn

KW(xq,xk) = (1+O(incoh))·KW11(cq,ck)KW12(cq,tk)·KW21(tq,ck)KW22(tq,tk) (14)

Hơn nữa, (14) đúng với xác suất ít nhất 1−O((|B₁⁰|·|B₂⁰|) exp(−incoh²·d) nếu mỗi Wαβ được thay thế bởi Wαβ+Zαβ/√d trong đó (Zαβ)kk′ là một biến ngẫu nhiên subgaussian độc lập.

Phép phân tích (14) nói rằng mỗi thành phần kernel có một ma trận trọng số riêng, và tất cả các thành phần đóng góp theo cách nhân cho KW. Phần "hơn nữa" tổng quát hóa khái niệm biểu diễn thưa thớt bằng cách cho phép nhiễu cộng, điều này khớp với cấu trúc thực nghiệm trong (9). Việc xây dựng cộng của W được kết nối với task arithmetic (Ilharco et al., 2022; Ortiz-Jimenez et al., 2023) được nghiên cứu gần đây.

Nhận xét 1. Nếu chúng ta giả sử incoh≍d^−γ với 1/2>γ>0, thì phát biểu xác suất cao là không tầm thường nếu |B₁⁰|·|B₂⁰|=o(exp(d^(1−2γ))). Giới hạn kích thước từ điển này thường hợp lý.

6 Nghiên cứu liên quan

Các phân tích về transformers đã thu hút sự quan tâm nghiên cứu kể từ Vaswani et al. (2017). Nhiều nghiên cứu về GPT-2 (Radford et al., 2019) và BERT (Devlin et al., 2018) cho thấy rằng các embeddings được ngữ cảnh hóa ở lớp cuối nắm bắt cấu trúc ngôn ngữ học và thể hiện hiệu suất tuyệt vời trong các tác vụ hạ nguồn (Hewitt & Manning, 2019; Chi et al., 2020; Thompson & Mimno, 2020). Ít bài báo hơn tập trung vào hình học hoặc embeddings của lớp trung gian: trong Ethayarajh (2019), người ta thấy rằng các embeddings của lớp sau ngày càng bất đẳng hướng và đặc thù cho ngữ cảnh; Cai et al. (2020); Reif et al. (2019); Hernandez & Andreas (2021); Gao et al. (2019) quan sát các cấu trúc hình học thú vị và các hiện tượng mà không có phân tích kỹ lưỡng; Yeh et al. (2023) cung cấp các công cụ trực quan hóa cho embeddings. Các bài báo gần đây cung cấp bằng chứng thực nghiệm/lý thuyết về cấu trúc low-rank hoặc đường chéo trong các ma trận trọng số attention (Boix-Adsera et al., 2023; Trockman & Kolter, 2023). Phép phân tách của chúng tôi thống nhất các hiện tượng thực nghiệm rải rác, tiết lộ hình học nhất quán và giải thích các hiện tượng quan sát được (bất đẳng hướng, hình dạng xoắn ốc, v.v.).

Nhiều biến thể của positional embedding được đề xuất (Shaw et al., 2018; Dai et al., 2019; Su et al., 2021; Scao et al., 2022; Press et al., 2021) kể từ Vaswani et al. (2017). Kể từ GPT-4, nhiều bài báo tập trung vào tổng quát hóa độ dài cho các tác vụ số học (Kazemnejad et al., 2023; Lee et al., 2023). Các phân tích trước về positional embeddings chỉ tập trung vào embeddings tĩnh (lớp thứ 0) cho các transformer được chọn (Wang et al., 2020; Ke et al., 2020; Wang & Chen, 2020; Tsai et al., 2019; Yamamoto & Matsuzaki, 2023), trong khi chúng tôi cung cấp một bức tranh chi tiết hơn.

Nghiên cứu trước về LSTMs thấy rằng các phương pháp dựa trên phân tách tăng cường khả năng diễn giải (Murdoch et al., 2018). Hiểu cách hoạt động bên trong của transformers thường được thực hiện thông qua trực quan hóa attention (Clark et al., 2019; Wang et al., 2022). Sự xuất hiện của induction heads (Elhage et al., 2021; Olsson et al., 2022) được hỗ trợ bởi trực quan hóa attention, điều này được củng cố thêm bởi phân tích của chúng tôi.

7 Hạn chế

Trong bài báo này, chúng tôi chủ yếu tập trung vào các transformer đã được huấn luyện trước do tài nguyên tính toán hạn chế. Sẽ thú vị khi điều tra tác động của các định dạng đầu vào/prompt đến hình học của embeddings trong quá trình huấn luyện, đặc biệt cho các tác vụ ngôn ngữ học khác nhau và các tác vụ số học.

Ngoài ra, chúng tôi chủ yếu tập trung vào các vector trung bình post và ctxc nhưng không nghiên cứu kỹ lưỡng residc,t. Chúng tôi thấy rằng thành phần dư không thể bỏ qua (ví dụ, chứa thông tin đặc thù cho token). Sẽ thú vị khi nghiên cứu tương tác bậc cao hơn trong residc,t và đề xuất một phép phân tách phi tuyến của embeddings, điều này được để lại cho nghiên cứu tương lai.

8 Lời cảm ơn

Chúng tôi cảm ơn Junjie Hu, Tim Ossowski, Harmon Bhasin, Wei Wang cho các cuộc thảo luận hữu ích.

Hỗ trợ cho nghiên cứu này được cung cấp bởi Văn phòng Phó Hiệu trưởng về Nghiên cứu và Giáo dục Sau đại học tại Đại học Wisconsin–Madison với tài trợ từ Quỹ Nghiên cứu Cựu sinh viên Wisconsin.
