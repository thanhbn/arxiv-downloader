# 2310.04861.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/positional-embeddings/2310.04861.pdf
# File size: 12263477 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Uncovering hidden geometry in Transformers via disentangling
position and context
Jiajun Song*, Yiqiao Zhong†
February 6, 2024
Abstract
Transformers are widely used to extract semantic meanings from input tokens, yet they usually op-
erate as black-box models. In this paper, we present a simple yet informative decomposition of hidden
states (or embeddings) of trained transformers into interpretable components. For any layer, embedding
vectors of input sequence samples are represented by a tensor h∈RC×T×d. Given embedding vector
hc,t∈Rdat sequence position t≤Tin a sequence (or context) c≤C, extracting the mean effects
yields the decomposition
hc,t=µ+post+ctxc+resid c,t
where µis the global mean vector, postandctxcare the mean vectors across contexts and across
positions respectively, and resid c,tis the residual vector. For popular transformer architectures and
diverse text datasets, empirically we find pervasive mathematical structure: (1) (post)tforms a low-
dimensional, continuous, and often spiral shape across layers, (2) (ctxc)cshows clear cluster structure
that falls into context topics, and (3) (post)tand(ctxc)care nearly orthogonal. We argue that smooth-
ness is pervasive and beneficial to transformers trained on languages, and our decomposition leads to
improved model interpretability.
1 Introduction
Transformers (Vaswani et al., 2017) are practical neural network models that underlie recent successes of
large language models (LLMs) (Brown et al., 2020; Bubeck et al., 2023). Unfortunately, transformers
are often used as black-box models due to lack of in-depth analyses of internal mechanism, which raises
concerns such as lack of interpretability, model biases, security issues, etc., (Bommasani et al., 2021).
In particular, it is poorly understood what information embeddings from each layer capture. We identify
two desiderata: (1) internal quantitative measurements, particularly for the intermediate layers; (2) visual-
ization tools and diagnostics tailored to transformers beyond attention matrix plots.
Let us introduce basic notations. An input sequence consists of Tconsecutive tokens (e.g., words
or subwords), and a corpus is a collection of all input sequences. Let Cbe the total number of input
sequences and c≤Cdenote a generic sequence, which may be represented by xc,1, . . . ,xc,Twhere each
*National Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI), Beijing
100080, China, songjiajun@bigai.ai
†Department of Statistics, University of Wisconsin–Madison, WI, 53706, USA, yiqiao.zhong@wisc.edu
1arXiv:2310.04861v2  [cs.LG]  4 Feb 2024

--- PAGE 2 ---
Figure 1: PCA visualization of positional basis (blue) and cvecs (red) from GPT-2 on OpenWebText. For
every layer ℓ, each pos(ℓ)
tand randomly selected cvec(ℓ)
c,tare projected using top- 2principal directions
of(pos(ℓ)
t)t≤T. Darker blue/red colors correspond to larger t. Principal components have dramatically
increasing scales across layers, but for aesthetic purposes we rescaled all plots.
xc,tcorresponds to a token. We start from the initial static (and positional) embeddings (h(0)
c,t)t≤Tand then
calculate the intermediate-layer embeddings (h(ℓ)
c,t)t≤T:
h(0)
c,1, . . . ,h(0)
c,T=Embed (xc,1, . . . ,xc,T)
h(ℓ)
c,1, . . . ,h(ℓ)
c,T=TFLayer ℓ(h(ℓ−1)
c,1, . . . ,h(ℓ−1)
c,T),
forℓ= 1, . . . , L , where Embed andTFLayer ℓare general mappings. This general definition encompasses
many transformer models, which depend on attention heads defined as follows. Given dhead≤dand input
matrix X∈RT×d, for trainable weights Wq,Wk,Wv∈Rd×dhead, define
AttnHead( X) = softmaxXWq(Wk)⊤X⊤
√dhead
XWv. (1)
Multi-head attention heads, denoted by MHA, are essentially the concatenation of many attention heads.
Denote a generic fully-connected layer by FFN(x) =W2max{0,W1x+b1}+b2given any x∈Rdfor
trainable weights W1∈Rd′×d,W2∈Rd×d′,b1∈Rd′,b2∈Rd(often d′= 4d), and let LNbe a generic
layer normalization layer. The standard transformer is expressed as
h(ℓ+0.5)
c =h(ℓ)
c+MHA(ℓ)(LN(ℓ,1)(h(ℓ)
c)),
h(ℓ+1)
c =h(ℓ+0.5)
c +FFN(ℓ)(LN(ℓ,2)((h(ℓ+0.5)
c )))
where h(ℓ+0.5)
c = (h(ℓ+0.5)
c,1, . . . ,h(ℓ+0.5)
c,T)andh(ℓ)
c= (h(ℓ)
c,1, . . . ,h(ℓ)
c,T).
1.1 A mean-based decomposition
For each embedding vector h(ℓ)
c,t∈Rdfrom any trained transformer, consider the decomposition
h(ℓ)
c,t=µ(ℓ)+pos(ℓ)
t+ctx(ℓ)
c+resid(ℓ)
c,t, (2)
2

--- PAGE 3 ---
µ(ℓ):=1
CTX
c,th(ℓ)
c,t,pos(ℓ)
t:=1
CX
ch(ℓ)
c,t−µ(ℓ), (3)
ctx(ℓ)
c:=1
TX
th(ℓ)
c,t−µ(ℓ). (4)
Each of the four components has the following interpretations. For any given layer ℓ,
• we call µ(ℓ)the global mean vector, which differentiates neither contexts nor positions;
• we call (pos(ℓ)
t)t≤Tthe positional basis, as they quantify average positional effects;
• we call (ctx(ℓ)
c)c≤Cthe context basis, as they quantify average sequence/context effects;
• we call (resid(ℓ)
c,t)t≤T,c≤Cthe residual vectors, which capture higher-order effects.
• In addition, we define cvec(ℓ)
c,t=ctx(ℓ)
c+resid(ℓ)
c,t.
A corpus may contain billions of tokens. For practical use, in this paper Cis much smaller: we subsample
input sequences from the corpus; for example, C= 6.4Kin Figure 1.
Positional basis vs. positional embeddings. While positional embeddings at Layer 0 is much explored
in the literature (see Section 6), the structure in intermediate layers is poorly understood. In contrast, our
approach offers structural insights to alllayers.
1.2 Connections to ANOV A
Our embedding decomposition is similar to multivariate two-way ANOV A in form. Borrowing standard
terminology from ANOV A, positions and contexts can be regarded as two factors ortreatments , so viewing
the embedding hc,tas the response variable, then positional/context bases represent mean effects.
On terminology. (i) We use context to refer to a sequence since its tokens collectively encode context
information. (ii) We call positional/context basis for convenience. A more accurate term is frame orover-
complete basis , since (pos(ℓ)
t)t≤Tand(ctx(ℓ)
t)c≤Care often linearly dependent.
1.3 Accessible reproducibility
We provide a fast implementation via Google Colab that reproduces most of the figures and analysis for
GPT-2 (under several minutes with the GPU option):
https://colab.research.google.com/drive/1ubsJQvLkOSQtiU8LoBA_79t1bd5-5ihi?usp=sharing .
The complete implementation, as well as additional plots and measurements, can be found on the following
GitHub page.
https://github.com/JiajunSong629/uncover-hidden-geometry
1.4 Notations
For a vector x, we denote its ℓ2norm by ∥x∥. For a matrix A∈Rn×m, we denote its operator norm
by∥A∥op:= max u:∥u∥=1∥Au∥and max norm by ∥A∥max:= max i,j|Aij|. We use the standard big- O
notation: for positive scalars a, b, we write a=O(b)ifa≤Cbfor a constant C. We use span(M)to
denote the linear span of column vectors in M.
3

--- PAGE 4 ---
Table 1: Averaged (and std of) measurements across layers. Measurements based on 6.4K samples.
All values are in [0,1]except ‘rank estimate’: ‘relative norm’ means magnitude of positional basis relative
to centered embeddings; ‘similarity’ and ‘incoherence’ are averaged cosine similarity (inner products of
normalized vectors) between ctx, and between ctxandpos, respectively. We find (i) positional basis is a
low-rank andsignificant component (ii) inter similarity ≪intra similarity (iii) incoherence between two
bases.
Positional basis Context basis
Incoherence rank
estimaterelative
norminter-cluster
similarityintra-cluster
similarity
NanoGPT Shakespeare 7.86 (1.96) 0.66 (0.28) ——– ——– ——–
GPT-2OpenWebText 11.38 (1.86) 0.84 (0.18) 0.10 (0.01) 0.44 (0.04) 0.051 (0 .05)
WikiText 11.69 (1.64) 0.76 (0.18) 0.11 (0.01) 0.41 (0.03) 0.039 (0 .04)
BERTOpenWebText 12.54 (2.73) 0.78 (0.18) 0.13 (0.04) 0.26 (0.04) 0.046 (0 .05)
WikiText 12.62 (2.70) 0.76 (0.18) 0.17 (0.03) 0.31 (0.04) 0.043 (0 .04)
BLOOMOpenWebText 10.23 (1.31) 0.30 (0.16) 0.15 (0.14) 0.32 (0.09) 0.158 (0 .23)
WikiText 10.00 (1.47) 0.31 (0.13) 0.14 (0.13) 0.31 (0.09) 0.148 (0 .23)
Llama 2OpenWebText 9.38 (1.15) 0.14 (0.03) 0.17 (0.19) 0.43 (0.12) 0.190 (0 .24)
WikiText 8.69 (0.91) 0.07 (0.04) 0.47 (0.35) 0.60 (0.25) 0.316 (0 .27)
GitHub 8.69 (1.67) 0.21 (0.05) 0.17 (0.10) 0.40 (0.07) 0.189 (0 .20)
2 Pervasive geometrical structure
We apply our decomposition to a variety of pretrained transformers including GPT-2 Radford et al. (2019),
BERT Devlin et al. (2018), BLOOM Scao et al. (2022), Llama-2 Touvron et al. (2023), and various datasets
such as WikiText, OpenWebText, GitHub. See Section A for details. Our geometric findings are summarized
below.
1. Positional basis is a significant and approximately low-rank component, forming a continuous and
curving shape.
2. Context basis has strong cluster patterns corresponding to documents/topics.
3. Positional basis and context basis are nearly orthogonal (or incoherent ).
Our findings indicate that embeddings contain two main interpretable factors, which are decoupled due
to incoherence.
Sinusoidal patterns are learned, not predefined. The transformers we examined are trained from scratch
including the positional embeddings (PEs). It is unclear a priori why such a smooth and sinusoidal pattern
is consistently observed. Moreover, the learned sinusoidal patterns are different from the original fixed
sinusoidal embedding Vaswani et al. (2017): for example, the original PE is concentrated less on the low-
frequency components (Section B.1).
Consistent pattern across layers and models is not solely explained by residual connections. The
geometric structure is (i) consistent across layers and (ii) agnostic to models. Can consistency across layers
explained by residual connections? We show this is not true: the average norm of embeddings increases by
more than 100-fold in GPT-2, and the embeddings are nearly orthogonal between layer 0 and 1 (Section B.2).
4

--- PAGE 5 ---
Figure 2: Normalized Gram matrix [¯P,¯C]⊤[¯P,¯C]where ¯P= [pos1
∥pos1∥, . . . ,posT
∥posT∥]and ¯C=
[ctx1
∥ctx1∥, . . . ,ctxC
∥ctxC∥]based on GPT-2. Here, T= 128 , andctxcis sampled from 4documents with sam-
ple size 32in OpenWebText. We find (i) Smoothness ,pos-pos part (top left) of Gram matrix is smooth;
(ii)Incoherence ,pos-ctxpart (top right/bottom left) has values close to 0; (iii) Clustering ,ctx-ctxpart
(bottom right) shows strong cluster patterns.
Some exceptions. In the last few layers, embeddings tend to be anisotropic Ethayarajh (2019) and geo-
metric structure may collapse to a lower dimensional space, particularly the positional basis. It is possibly
due to the completion of contextualization or optimization artifacts. We also find that BERT shows higher
frequency patterns, possibly due to its different training.
3 Key properties of positional basis: low rank and low frequency
In this section, we use quantitative measurements to relate smoothness to concepts in parsimonious repre-
sentations.
Measuring smoothness. Our notion of “smoothness” of the positional basis refers to its (normalized)
Gram matrix:
G=¯P⊤¯P∈RT×T,where ¯P= [pos1
∥pos1∥, . . . ,posT
∥posT∥] (5)
being visually smooth (mathematically, having bounded discrete derivatives).
3.1 Low rank via spectral analysis
Figure 3: Spectral and Fourier analysis based on
GPT-2 model and OpenWebText. Left: Top-60 sin-
gular values of P.Right: Applying 2D discrete co-
sine transform to ¯P⊤¯P, we show first 10frequency
coefficients.Low rank. We find that the positional basis con-
centrates around a low-dimensional subspace. In
Table 1 “rank estimate” column, we report the rank
estimate of positional basis averaged across all lay-
ers using the method of Donoho et al. (2023). In
Figure 3, we plot the top singular values in descend-
ing order of P= [pos1, . . . ,posT]. Visibly, there
is a sharp change in the plot, which indicate a dom-
inant low-rank structure. In Section C.2, we report
detailed rank estimates.
Significant in relative norms. We also find that
usually, the positional basis accounts for a signifi-
5

--- PAGE 6 ---
cant proportion of embeddings. In Table 1, we re-
port the relative norm (averaged across layers) ∥P∥op/∥M∥op, where Mcontains centered embedding
vectors hc,t−µand columns of Pare corresponding post. The relative norms show positional basis is
contributing significantly to the overall magnitude (most numbers bigger than 10%).
3.2 Low frequency via Fourier analysis
In Figure 3 (right), we apply the 2D discrete cosine transform to the normalized Gram matrix ¯P⊤¯P; namely,
we calculate the frequency matrix ˆGby apply the (type-II) discrete cosine transform with orthogonal matrix
˜F:
ˆG=˜FG˜F⊤.
Each entry ˆGijencodes the (i, j)-th frequency coefficient. We discover that energies are concentrated mostly
in the low-frequency components, which echos the smooth and curving structure in Figure 1.
3.3 Theoretical lens: smoothness explains low-rank and low-frequency
It is well known that the smoothness of a function is connected to fast decay or sparsity in the frequency
domain (Pinsky, 2008, Sect. 1.2.3). From the classical perspective, we establish smoothness as a critical
property that induces the observed geometry.
Smoothness of Gram matrix of positional basis induces the low-dimensional and spiral shape.
For convenience, we assume that positional vectors posthave unit norm, so by definition, pos1+. . .+
posT=0. To quantify smoothness, we introduce the definition of finite difference. As with the discrete
cosine transform in 1D, we extend and reflect the Gram matrix to avoid boundary effects.
LetG(1)=GandG(2),G(3),G(4)∈RT×Tbe defined by G(2)
t,t′=Gt,T+1−t′,G(3)
t,t′=GT+1−t,t′,
G(4)
t,t′=GT+1−t,T+1−t′for any t, t′= 1,2, . . . T . We extend and reflect Gby
˜G:=G(1)G(2)
G(3)G(4)
. (6)
We define the first-order finite difference by (using periodic extension ˜Gt±2T,t′±2T=˜Gt,t′)
[∆(1,1)˜G]t,t′=T2 ˜Gt,t′−˜Gt−1,t′−˜Gt,t′−1+˜Gt−1,t′−1
(7)
for all integers t, t′. Higher-order finite differences are defined recursively by ∆(m,m)˜G= ∆(1,1) 
∆(m−1,m−1)˜G
.
Note that ∆(m,m)˜Gmeasures higher-order smoothness of ˜G. Indeed, if Gt,t′=f(t/T, t′/T)for certain
smooth function f(x, y)defined on [0,1]2, then [∆(m,m)˜G]t,t′≈∂m
x∂m
yf(t/T, t′/T).
Theorem 1. Fix positive integers k≤Tandm. Define the low-frequency vector fs= (1 ,cos((s−
0.5)π/T), . . . , cos((s−0.5)(T−1)π/T))⊤∈RTwhere s= 1, . . . , k , and denote F≤k= [f1, . . . ,fk]∈
RT×k. Then there exists B∈Rk×ksuch that
1
TG−F≤kB(F≤kB)⊤
op≤6
(8k)m∥∆(m,m)˜G∥max.
This theorem implies that if the extended Gram matrix has higher-order smoothness, namely ∥∆(m,m)˜G∥max
is bounded by a constant, then even for moderate kandm, we have approximation G≈F≤kB(F≤kB)⊤.
Note that F≤kBconsists of linear combinations of low-frequency vectors. This explains why Ghas a
dominant low-rank and low-frequency component.
6

--- PAGE 7 ---
Table 2: Robustness of positional basis . Similar geometric structures found on OOD samples: NanoGPT
(trained on a Shakespeare dataset) evaluated on WikiText, GPT-2 and BERT (trained on language datasets)
evaluated on GitHub data, BLOOM evaluated on random tokens.
rank estimate of P ratio explained by low-freq ( K= 10 )
NanoGPT 5.43 (1.84) 84 .0%
GPT-2 11.54 (1.55) 99 .5%
BERT 12.46 (2.47) 70 .2%
BLOOM 10.44 (2.25) 95 .6%
Table 3: Inheriting smoothness from positional basis . For K= 1,3,5,10, we apply 2D DCT and
calculate the ratio1of up to K-th low-frequency coefficients based on GPT-2 and positional basis.
Ratio explained K= 1 K= 3 K= 5 K= 10
PP⊤39.4% 95.4% 98.2% 99.7%
PWP⊤45.9% 96.5% 98.8% 99.9%
QK matrix 84.0% 90.0% 90.4% 91.2%
4 Smoothness: blessing of natural languages
We demonstrate that smoothness is a natural and beneficial property learned from language data: it is robust
to distribution shifts and allows efficient attention computation. This smoothness likely reflects the nature
of language data.
4.1 Smoothness is robust in out-of-distribution data
So far, we have analyzed our decomposition and associated geometry primarily on in-distribution samples.
For example, we sample sequences from OpenWebText—the same corpus GPT-2 is pretrained on, to calcu-
late decomposition (3)–(4).
Now we sample out-of-distribution (OOD) data and conduct similar decomposition and analyses. We
find that the positional basis possesses similar low-rank and spiral structure. Surprisingly, for sequences
consisting of randomly sampled tokens, such structure persists. See summaries in Table 2 and details in
Section D.1.
Many LLMs can generalize well to OOD data. We believe that this generalization ability is at least
partially attributed to the robustness of positional basis as evidenced here.
4.2 Smoothness promotes local and sparse attention
Many attention heads in large-scale transformers for language data show local and sparse patterns (Beltagy
et al., 2020). A usual heuristic explanation is that, most information for token prediction is contained in a
local window.
We offer an alternative explanation: Local attention is a consequence of smoothness of positional basis .
We provide our reasoning. First, in terms of smoothness, the Gram matrix PP⊤is closely related to
PWP⊤, where W=Wq(Wk)⊤/√dhead. Indeed, generally a linear transformation of the positional
vectors, namely WP⊤, should not affect smoothness much.
7

--- PAGE 8 ---
Figure 4: Decoupling trained weight matrices. For12attention heads (layer L= 6shown here) in GPT-2,
we study the matrix W=Wq(Wk)⊤/√dhead∈Rd×d. Red: diagonal entries D:= diagg( W). Blue:
take off-diagonal matrix W−diagg( W), rotate it by the right singular vectors of positional basis V, then
apply denoising. Large absolute values concentrate in small top-left part L.
Second, PWP⊤is an important—often dominant—constituent of the QK matrix (QK matrix is the ma-
trix inside softmax in (1)); see Section 5.2 for an example. Thus, the QK matrix can inherit the smoothness
fromPWP⊤.
Third, if the QK matrix (denoted by B) is smooth and maximized at the diagonal position along a certain
row indexed by t(the constraint t′≤tis due to causal masking):
arg max
1≤t′≤tBt,t′=t, (8)
then positions t′close to talso have high QK values by smoothness. Therefore, we expect the neighboring
positions around tto receive high attention weights after softmax.
Our reasoning is supported by pervasive smoothness shown in Table 3. Moreover, we find that the
positional constituents in more than 43% heads in GPT-2 satisfy (8) for more than 80% positions t. See
Section D.2 for details.
4.3 Curse of discontinuity for arithmetic tasks
Figure 5: Addition task trained on NanoGPT exhibits
nonsmooth patterns: discontinuity as a consequence
of non-language data training. Left: Gram matrix of
normalized positional basis. Compare with top-left of
plots in Figure 2. Right : QK matrix.We find that the emergence of smoothness is data
dependent: while transformers pretrained on natu-
ral/programming languages exhibit the smoothness
property, they may suffer from a lack of smoothness
on other pretrained data.
We explore a simple arithmetic task— Addition ,
where inputs are formatted as a string “ a+b=
c” with a, b, c represented by digits of a certain
length. We sample the length of each addition
component uniformly from {L/2, . . . , L }where
L= 10 . Then, we train a 4-layer 4-head trans-
former (NanoGPT) with character-level tokeniza-
tion to predict ‘ c’ based on the prompt “ a+b=”.
We train this transformer 10,000 iterations until
convergence.
1Given 2D frequencies (fij), we calculate the ratio as rK=P
i,j≤Kf2
ij/P
i,jf2
i,j.
8

--- PAGE 9 ---
Nonsmooth pattern. Figure 5 shows that the Gram matrix of normalized positional basis and QK matrix
are visibly discontinuous and exhibit many fractured regions. Quantitatively, the top-10 low-frequency
components of Gram matrix explains around 50% ofP
ijˆG2
ij(Section D.3), much less than 99% in GPT-2
as in Table 3. This suggests a sharp distinction between language vs. non-language data in terms of induced
geometry.
Failure of length generalization. Our NanoGPT achieves above 99% in-distribution test accuracy, yet
fails at OOD generalization: it has less than 20% accuracy on average on digits of length smaller than 5. We
believe nonsmoothness is likely an intrinsic bottleneck for arithmetic tasks.
Our results hold for transformers with relative positional embeddings as well; see Section D.3.
5 Incoherence enhances interpretability
Near-orthogonality, or (mutual) incoherence, is known to be a critical property for sparse learning. Gener-
ally speaking, incoherence means that factors or features are nearly uncorrelated and decoupled. In the ideal
scenario of orthogonality, factors can be decomposed into orthogonal non-intervening subspaces. Incoher-
ence is closely related to restricted isometry (Candes & Tao, 2005), irrepresentable conditions (Zhao & Yu,
2006), etc.
We observe the incoherence property between the positional basis and the context basis, as Table 1
shows that max t,c|⟨post
∥post∥,ctxc
∥ctxc∥⟩|is typically small. The low incoherence in Table 1 (random baseline is
about 0.1) means that the two bases are nearly orthogonal to each other.
To decouple different effects, in this section, we will focus on positional effects vs. non-positional ef-
fects, thus working with cvec c,tinstead of ctxc.
5.1 Decoupling positional effects in pretrained weights
We present evidence that the trained weight matrix W:=Wq(Wk)⊤/√dhead in self-attention has a clear
and interpretable decomposition: heuristically, a low rank component that modulates the positional effects,
and a diagonal component that strengthens or attenuates token effects.
More precisely, we identify a common low-rank plus noise structure in the majority of attention heads
in pretrained transformers.
W=V LV⊤
|{z}
low rank+D|{z}
diagonal+Noise . (9)
Here, the columns of V∈Rd×Kare the top- Kright singular vectors of positional basis matrix P, and
L∈RK×K, where Kis not large.
Figure 4 shows empirical support to structural claim (9). Given a pretrained weight W, we take D=
diagg( W)and show entries in red. Then, we rotate the off-diagonal part of Wby the right singular vectors
ofPand apply denoising, namely zeroing entries whose absolute values are smaller than a threshold. For
many heads, the surviving large absolute values are concentrated in the top left ( K≈20)—which suggests
that indeed a significant component of Wis aligned with the positional basis.
This decomposition suggests a possible mechanism inside attention computation. Consider an ideal
scenario where Dis a multiple of identity matrix, and each embedding has orthogonal decomposition h=
t+cwitht∈span(V)encoding positional information and c∈span(V)⊥encoding non-positional
information. Then, for two embedding vectors h,h′withh=t+candh′=t′+c′,
h⊤Wh′≈t⊤(V LV⊤+D)t′
| {z }
positional effect+ c⊤Dc′
|{z}
context / token effect
9

--- PAGE 10 ---
Figure 6: QK constituent plots enhance attention visualization :3representative attention patterns in
induction heads can be explained by their respective dominant QK constituents. Left pair: cvec -cvec
constituent drives attention to identical tokens. Middle pair: pos-pos constituent determines attention to
neighboring tokens. Right pair: cvec -cvec shows attentions to shifted tokens.
Positional effects are decoupled from context effects, since cross terms involving t,c′ort′,cvanish. This
heuristics may allow us to examine how information is processed in attention heads, which will be explored
in future work.
5.2 Beyond attention visualization
Attention visualization is often used as a diagnosis of self-attention in transformers. We show that more
structure can be revealed by decomposing the QK matrix using our decomposition of embeddings.
We start with decomposing the QK matrix. Assuming µ=0(ignoring global mean effect for conve-
nience), then for embedding vectors h,h′∈Rdwe have
h⊤Wq(Wk)⊤h=pos⊤Wq(Wk)⊤pos
+pos⊤Wq(Wk)⊤cvec +cvec⊤Wq(Wk)⊤pos
+cvec⊤Wq(Wk)⊤cvec . (10)
Each of the four components shows how much an attention head captures information from cross-pairs
pos/cvec —pos/cvec of an embedding. We propose to visualize each QK component separately, which
tells if an attention head captures positional information or context/token information from embeddings.
Case study: induction heads. Elhage et al. (2021) identified components in transformers that complete
a sequence pattern based on observed past tokens, namely, predicting the next token [B]based on observed
sequence [A],[B], . . . , [A]. This ability of copying previous tokens in known to be caused by induction
heads .
There are three representative attention patterns as shown in Figure 6: (i) attention to identical tokens,
namely attention weights concentrate on tokens identical to the present token, (ii) attention to neighboring
tokens, (iii) attention to tokens to be copied. See also Elhage et al. (2021).
Attention visualization alone does not reveal why certain heads emerge. In contrast, our QK decom-
position reveals which QK constituent is dominant and responsible for attention patterns. For example, in
Figure 6,
• attention to neighboring tokens (middle plots) is predominately determined by the pos-pos con-
stituent;
• attention to identical token (left) or shifted token (right) is determined by the cvec -cvec constituent.
This implies that induction heads are not based on memorizing relative positions, but on matching
token information.
More details are in Section E.2.
10

--- PAGE 11 ---
5.3 Theoretical insight from kernel factorization
Why does incoherence emerge from pretrained transformers? While it is well known in sparse coding
and compressed sensing that handcrafted incoherent basis facilitates recovery of sparse signals (Donoho &
Stark, 1989; Donoho & Elad, 2003; Donoho, 2006; Cand `es et al., 2006), it is surprising that incoherence
arises from automatic feature learning .
Here we focus on the self-attention mechanism of transformers. By adopting the kernel perspective, we
present a preliminary theory for the following heuristics:
Incoherence enables a kernel to factorize into smaller components, each operating independently.
Given query/key matrices Wq,Wk∈Rd×dhead, we define the (asymmetric) kernel by (recalling W=
Wq(Wk)⊤/√dhead)
KW(z,z′) := exp
z⊤Wz′
= exp⟨Wqz,Wkz′⟩√dhead
.
Using KW, the attention can be expressed as kernel smoothing: for embeddings (xt)t≤T⊂Rd,
AttnHead( xt;KW) =X
k≤tKW(xk,xt)P
k′≤tKW(xk′,xt)v(xk) (11)
where v:Rd→Ris a generic value function. This kernel perspective is explored in Tsai et al. (2019),
where it is argued that the efficacy of self-attention largely depends on the form of the kernel.
Suppose that there are two overcomplete bases B0
1,B0
2⊂Rd. For simplicity, assume that ∥u∥2≤1
ifu∈ B0
1orB0
2. The mutual incoherence is incoh := max
|⟨c,t⟩:c∈ B0
1,t∈ B0
2	
. Consider the
(extended) overcomplete basis Bα:={λu:u∈ B0
α, λ∈[−1,1]}where α∈ {1,2}. Given query/key
vectors xq,xk∈Rd, suppose that we can decompose them according to the two bases.
xq=cq+tq,xk=ck+tk,forcq,ck∈ B 1;tq,tk∈ B 2. (12)
Generically, we can decompose KW(xq,xk)into
KW(cq,ck)KW(cq,tk)·KW(tq,ck)KW(tq,tk).
Each component measures cross similarity of pairs between cq,tqandck,tk, which then translates into
a weight for the attention. Unfortunately, this general decomposition requires the individual kernels to share
the same weight W, which hinders capturing cross interactions flexibly.
It turns out that if the weight matrix is sparsely represented by the bases, then kernel flexibility can be
achieved. To be precise, we will say that W∈Rd×diss-sparsely represented by bases B,B′if there exist
(ak)k≤s⊂[−1,1],(uk)k≤s⊂ B,(vk)k≤s⊂ B′such that
W=X
k≤sakukv⊤
k. (13)
Theorem 2. LetW11,W12,W21,W22∈Rd×dbe any matrices with the following properties: for α, β∈
{1,2},Wαβ∈Rd×disO(1)-sparsely represented by bases Bα,Bβ. Then for all xq,xk∈Rdsatisfying
(12),W=W11+W12+W21+W22satisfies
KW(xq,xk) = 
1 +O(incoh)
·KW11(cq,ck)KW12(cq,tk)
·KW21(tq,ck)KW22(tq,tk) (14)
11

--- PAGE 12 ---
Moreover, (14) holds with probability at least 1−O((|B0
1|·|B0
2|) exp(−incoh2·d)if each Wαβis replaced
byWαβ+Zαβ√
dwhere (Zαβ)kk′is an independent subgaussian2random variable.
The factorization (14) says that each kernel component has a separate weight matrix, and all components
contribute multiplicatively to KW. The “moreover” part generalizes the sparse representation notion by
allowing additive noise, which matches the empirical structure in (9). The additive construction of Wis
connected to task arithmetic (Ilharco et al., 2022; Ortiz-Jimenez et al., 2023) recently studied.
Remark 1. If we suppose incoh≍d−γwith1/2> γ > 0, then the high probability statement is nontrivial
if|B0
1| · |B0
2|=o(exp( d1−2γ)). This dictionary size limit is generally reasonable.
6 Related work
Analyses of transformers have attracted research interest since Vaswani et al. (2017). Many studies on
GPT-2 (Radford et al., 2019) and BERT (Devlin et al., 2018) show that last-layer contextualized embed-
dings capture linguistic structure and exhibit excellent downstream performance (Hewitt & Manning, 2019;
Chi et al., 2020; Thompson & Mimno, 2020). Fewer papers focus on the geometry or intermediate-layer
embeddings: in Ethayarajh (2019), it is found that later-layer embeddings are increasingly anisotropic and
context-specific; Cai et al. (2020); Reif et al. (2019); Hernandez & Andreas (2021); Gao et al. (2019) ob-
served interesting geometric structures and artifacts without thorough analysis; Yeh et al. (2023) provide
visualization tools for embeddings. Very recent papers provide empirical/theoretical evidence about either
low-rank or diagonal structure in attention weight matrices (Boix-Adsera et al., 2023; Trockman & Kolter,
2023). Our decomposition unifies scattered empirical phenomena, reveals consistent geometry and explains
observed artifacts (anisotropic, spiral shape, etc.).
Many variants of positional embedding are proposed (Shaw et al., 2018; Dai et al., 2019; Su et al., 2021;
Scao et al., 2022; Press et al., 2021) since Vaswani et al. (2017). Since GPT-4, many papers focus on length
generalization for arithmetic tasks (Kazemnejad et al., 2023; Lee et al., 2023). Prior analyses on positional
embeddings focus only on static (0-th layer) embeddings for selected transformers (Wang et al., 2020; Ke
et al., 2020; Wang & Chen, 2020; Tsai et al., 2019; Yamamoto & Matsuzaki, 2023), whereas we provide a
more detailed picture.
Prior work on LSTMs finds decomposition-based methods enhance interpretability (Murdoch et al.,
2018). Understanding the inner workings of transformers is usually done through attention visualization
(Clark et al., 2019; Wang et al., 2022). The emergence of induction heads (Elhage et al., 2021; Olsson et al.,
2022) is supported by attention visualization, which is further reinforced by our analysis.
7 Limitations
In this paper, we mostly focus on pretrained transformers due to limited computational resources. It would
be interesting to investigate the impact of input/prompt formats on the geometry of embeddings over the
course of training, especially for different linguistic tasks and arithmetic tasks.
Also, we mostly focus on the mean vectors postandctxcbut not study resid c,tthoroughly. We
find that the residual component is not negligible (e.g., containing token-specific information). It would
be interesting to study the higher-order interaction in resid c,tand propose a nonlinear decomposition of
embeddings, which is left to future work.
2We say that a random variable ξis subgaussian if E[ξ] = 0 andE[exp(λξ)]≤exp(λ2/2)for all λ∈R.
12

--- PAGE 13 ---
8 Acknowledgement
We thank Junjie Hu, Tim Ossowski, Harmon Bhasin, Wei Wang for helpful discussions.
Support for this research was provided by the Office of the Vice Chancellor for Research and Graduate
Education at the University of Wisconsin–Madison with funding from the Wisconsin Alumni Research
Foundation.
13

--- PAGE 14 ---
References
Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. arXiv preprint
arXiv:2004.05150 , 2020.
Boix-Adsera, E., Littwin, E., Abbe, E., Bengio, S., and Susskind, J. Transformers learn through gradual
rank increase. arXiv preprint arXiv:2306.07042 , 2023.
Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J.,
Bosselut, A., Brunskill, E., et al. On the opportunities and risks of foundation models. arXiv preprint
arXiv:2108.07258 , 2021.
Broughton, S. A. and Bryan, K. Discrete Fourier analysis and wavelets: applications to signal and image
processing . John Wiley & Sons, 2018.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P.,
Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information
processing systems , 33:1877–1901, 2020.
Bubeck, S., Chandrasekaran, V ., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y . T., Li, Y .,
Lundberg, S., et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint
arXiv:2303.12712 , 2023.
Cai, X., Huang, J., Bian, Y ., and Church, K. Isotropy in the contextual embedding space: Clusters and
manifolds. In International Conference on Learning Representations , 2020.
Candes, E. J. and Tao, T. Decoding by linear programming. IEEE transactions on information theory , 51
(12):4203–4215, 2005.
Cand `es, E. J., Romberg, J., and Tao, T. Robust uncertainty principles: Exact signal reconstruction from
highly incomplete frequency information. IEEE Transactions on information theory , 52(2):489–509,
2006.
Chi, E. A., Hewitt, J., and Manning, C. D. Finding universal grammatical relations in multilingual bert.
arXiv preprint arXiv:2005.04511 , 2020.
Clark, K., Khandelwal, U., Levy, O., and Manning, C. D. What does bert look at? an analysis of bert’s
attention. arXiv preprint arXiv:1906.04341 , 2019.
Dai, Z., Yang, Z., Yang, Y ., Carbonell, J., Le, Q. V ., and Salakhutdinov, R. Transformer-xl: Attentive
language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860 , 2019.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers
for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
Donoho, D., Gavish, M., and Romanov, E. Screenot: Exact mse-optimal singular value thresholding in
correlated noise. The Annals of Statistics , 51(1):122–148, 2023.
Donoho, D. L. Compressed sensing. IEEE Transactions on information theory , 52(4):1289–1306, 2006.
Donoho, D. L. and Elad, M. Optimally sparse representation in general (nonorthogonal) dictionaries via ℓ1
minimization. Proceedings of the National Academy of Sciences , 100(5):2197–2202, 2003.
14

--- PAGE 15 ---
Donoho, D. L. and Stark, P. B. Uncertainty principles and signal recovery. SIAM Journal on Applied
Mathematics , 49(3):906–931, 1989.
Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y ., Chen, A.,
Conerly, T., DasSarma, N., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Jones, A., Kernion,
J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C. A
mathematical framework for transformer circuits. Transformer Circuits Thread , 2021. https://transformer-
circuits.pub/2021/framework/index.html.
Ethayarajh, K. How contextual are contextualized word representations? comparing the geometry of bert,
elmo, and gpt-2 embeddings. arXiv preprint arXiv:1909.00512 , 2019.
Gao, J., He, D., Tan, X., Qin, T., Wang, L., and Liu, T.-Y . Representation degeneration problem in training
natural language generation models. arXiv preprint arXiv:1907.12009 , 2019.
Hernandez, E. and Andreas, J. The low-dimensional linear geometry of contextualized word representations.
arXiv preprint arXiv:2105.07109 , 2021.
Hewitt, J. and Manning, C. D. A structural probe for finding syntax in word representations. In Proceedings
of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and Short Papers) , pp. 4129–4138, 2019.
Ilharco, G., Ribeiro, M. T., Wortsman, M., Gururangan, S., Schmidt, L., Hajishirzi, H., and Farhadi, A.
Editing models with task arithmetic. arXiv preprint arXiv:2212.04089 , 2022.
Kazemnejad, A., Padhi, I., Ramamurthy, K. N., Das, P., and Reddy, S. The impact of positional encoding
on length generalization in transformers. arXiv preprint arXiv:2305.19466 , 2023.
Ke, G., He, D., and Liu, T.-Y . Rethinking positional encoding in language pre-training. arXiv preprint
arXiv:2006.15595 , 2020.
Lee, N., Sreenivasan, K., Lee, J. D., Lee, K., and Papailiopoulos, D. Teaching arithmetic to small trans-
formers. arXiv preprint arXiv:2307.03381 , 2023.
Murdoch, W. J., Liu, P. J., and Yu, B. Beyond word importance: Contextual decomposition to extract
interactions from lstms. arXiv preprint arXiv:1801.05453 , 2018.
Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A., Bai, Y .,
Chen, A., Conerly, T., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Johnston, S., Jones, A.,
Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and
Olah, C. In-context learning and induction heads. Transformer Circuits Thread , 2022. https://transformer-
circuits.pub/2022/in-context-learning-and-induction-heads/index.html.
Ortiz-Jimenez, G., Favero, A., and Frossard, P. Task arithmetic in the tangent space: Improved editing of
pre-trained models. arXiv preprint arXiv:2305.12827 , 2023.
Pinsky, M. A. Introduction to Fourier analysis and wavelets , volume 102. American Mathematical Soc.,
2008.
Press, O., Smith, N. A., and Lewis, M. Train short, test long: Attention with linear biases enables input
length extrapolation. arXiv preprint arXiv:2108.12409 , 2021.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised
multitask learners. OpenAI blog , 1(8):9, 2019.
15

--- PAGE 16 ---
Reif, E., Yuan, A., Wattenberg, M., Viegas, F. B., Coenen, A., Pearce, A., and Kim, B. Visualizing and
measuring the geometry of bert. Advances in Neural Information Processing Systems , 32, 2019.
Rudelson, M. and Vershynin, R. Sampling from large matrices: An approach through geometric functional
analysis. Journal of the ACM (JACM) , 54(4):21–es, 2007.
Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili ´c, S., Hesslow, D., Castagn ´e, R., Luccioni, A. S., Yvon, F.,
Gall´e, M., et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint
arXiv:2211.05100 , 2022.
Shaw, P., Uszkoreit, J., and Vaswani, A. Self-attention with relative position representations. arXiv preprint
arXiv:1803.02155 , 2018.
Su, J., Lu, Y ., Pan, S., Murtadha, A., Wen, B., and Liu, Y . Roformer: Enhanced transformer with rotary
position embedding. arXiv preprint arXiv:2104.09864 , 2021.
Thompson, L. and Mimno, D. Topic modeling with contextualized word representation clusters. arXiv
preprint arXiv:2010.12626 , 2020.
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y ., Bashlykov, N., Batra, S., Bhar-
gava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 , 2023.
Trockman, A. and Kolter, J. Z. Mimetic initialization of self-attention layers. arXiv preprint
arXiv:2305.09828 , 2023.
Tsai, Y .-H. H., Bai, S., Yamada, M., Morency, L.-P., and Salakhutdinov, R. Transformer dissection: a unified
understanding of transformer’s attention via the lens of kernel. arXiv preprint arXiv:1908.11775 , 2019.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I.
Attention is all you need. Advances in neural information processing systems , 30, 2017.
Vershynin, R. High-dimensional probability: An introduction with applications in data science , volume 47.
Cambridge university press, 2018.
Vig, J. and Belinkov, Y . Analyzing the structure of attention in a transformer language model. arXiv preprint
arXiv:1906.04284 , 2019.
Wang, B., Shang, L., Lioma, C., Jiang, X., Yang, H., Liu, Q., and Simonsen, J. G. On position embeddings
in bert. In International Conference on Learning Representations , 2020.
Wang, K., Variengien, A., Conmy, A., Shlegeris, B., and Steinhardt, J. Interpretability in the wild: a circuit
for indirect object identification in gpt-2 small. arXiv preprint arXiv:2211.00593 , 2022.
Wang, Y .-A. and Chen, Y .-N. What do position embeddings learn? an empirical study of pre-trained
language model positional encoding. arXiv preprint arXiv:2010.04903 , 2020.
Yamamoto, Y . and Matsuzaki, T. Absolute position embedding learns sinusoid-like waves for attention
based on relative position. In Proceedings of the 2023 Conference on Empirical Methods in Natural
Language Processing , pp. 15–28, 2023.
Yeh, C., Chen, Y ., Wu, A., Chen, C., Vi ´egas, F., and Wattenberg, M. Attentionviz: A global view of
transformer attention. arXiv preprint arXiv:2305.03210 , 2023.
Zhao, P. and Yu, B. On model selection consistency of lasso. The Journal of Machine Learning Research ,
7:2541–2563, 2006.
16

--- PAGE 17 ---
Appendix
Table of Contents
A Models, datasets, and implementations 17
A.1 Pretrained models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.2 Training small transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
A.3 Removing artifacts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
A.4 Positional basis calculation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B Additional empirical results for Section 2 19
B.1 Comparison with original sinusoidal positional embedding . . . . . . . . . . . . . . . . . 19
B.2 Do residual connections explain consistent geometry? . . . . . . . . . . . . . . . . . . . . 19
C Additional empirical results for Section 3 20
C.1 PCA visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C.2 Low rank measurements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C.3 On cluster structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
D Additional empirical results for Section 4 25
D.1 On robustness of positional basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
D.2 On sparse and local attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
D.3 On Addition experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
E Additional empirical results for Section 5 32
E.1 On trained weight matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
E.2 On enhanced QK/attention visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
F Proofs for theoretical results 32
F.1 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
F.2 Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
A Models, datasets, and implementations
We present the details of our experiments and measurements.
A.1 Pretrained models
We downloaded and used pretrained models from Huggingface. In addition, we trained a NanoGPT on a
Shakespeare dataset and on addition tasks.
• GPT-2 (Radford et al., 2019): 12-layer, 12-head, 768-dim, 124M parameters, autoregressive, absolute
positional encoding at 0th-layer, pretrained on OpenWebText;
17

--- PAGE 18 ---
• BERT (Devlin et al., 2018): 12-layer, 12-head, 768-dim, 124M parameters, masked prediction, abso-
lute positional encoding at 0th-layer, pretrained on BooksCorpus and English Wikipedia;
• BLOOM (Scao et al., 2022): 24-layer, 16-head, 1024-dim, 560M parameters, ALiBI positional en-
codings (Press et al., 2021) at each layer, pretrained on 45 natural languages and 12 programming
languages;
• Llama2-7B (Touvron et al., 2023): 32-layer, 32-head, 4096-dim, 7B parameters, autoregressive, Ro-
tary positional embedding (Su et al., 2021) at every layer, pretrained on a variety of data.
Note that (i) the training objective for pretraining BERT is different from the other models, and (ii)
Llama 2 uses rotary positional encoding for each layer and BLOOM uses ALiBI positional encoding—
which is different from absolute positional encoding that is added at the 0-th layer (Vaswani et al., 2017).
A.2 Training small transformers
We train a few smaller transformers in this paper. Models are based on the GPT-2 architecture with adjusted
parameters, and we adopt the implementation of the GitHub Project by Andrej Karpathy. The hardware we
use is mainly RTX3090ti. All the following experiments take under 1 hour to train.
•NanoGPT in Table 1 and 2 : The model is a vanilla Transformer with 6 layers, 6 heads, 384 di-
mensional embeddings, residual/embedding/attention dropout set to 0.1, weight decay set to 0.1, and
a context window of 128. The dataset is Shakespeare with character-level tokenization. We train
5000 iterations using the AdamW optimizer, with a batch size of 64 and a cosine scheduler (100 step
warmup) up to a learning rate of 1e-3;
•Addition : Similarly, we use a vanilla Transformer with 4 layers, 4 heads, 256 dimensional embed-
dings, and weight decay set to 0.1. The context window is set as the length of the longest sequence,
i.e., 33 for the 10-digits addition task here. We train 10,000 iterations using the AdamW optimizer,
with a batch size 64 and a linear scheduler up to a learning rate of 1e-4.
A.3 Removing artifacts
There are two likely artifacts in the measurements and visualization that we removed in the paper.
1. First token in a sequence. We find that a large proportion of attention is focused on the first token,
which usually distorts visualization significantly. It has been known that the first token functions as a
“null token”, which is removed in analysis (Vig & Belinkov, 2019). We also adopt removing the first
token in our measurements and visualization.
2. Final-layer embeddings. We find that the embeddings of the final layer typically do not have a sig-
nificant positional basis component. It is likely that positional information is no longer needed since
last-layer embeddings are directly connected to the loss function.
A.4 Positional basis calculation
We calculate positional bases based on sampled sequences of length Tfrom a subset of the corpus, which
includes OpenWebText, WikiText, and GitHub. The implementation and weights of the pretrained models
are obtained from HuggingFace.
For the curated corpus subset, we utilize the streaming version of the HuggingFace datasets and extract
the first 10K samples from the train split. Then we tokenize the dataset using the same tokenizer employed
18

--- PAGE 19 ---
by the pretrained model. The size of the final datasets vary across tasks and datasets, and we ensure that
there are at least 1M tokens in each case to prevent the occurrence of overlapping sequences.
We set the context window T= 512 for BERT, BLOOM, and GPT-2, as this maintains the maximum
context window utilized during pretraining. For Llama-2, we set T= 512 instead of a longer maximum
sequence length due to computational resource limitations.
B Additional empirical results for Section 2
B.1 Comparison with original sinusoidal positional embedding
Figure 7: Original sinusoidal positional embedding proposed in Vaswani et al. (2017) is fixed. Compare
the spectral and Fourier plots with Figure 3 and Figure 2.
The original sinusoidal positional embedding (PE) proposed in Vaswani et al. (2017) is fixed:
[post]i=

sin
t
100002i/d
, i= 2k
cos
t
100002i/d
. i= 2k+ 1
Although this PE and learned positional basis both have sinusoidal patterns, we observe differences by
comparing Figure 7 and Figure 3 and 2:
1. The Gram matrix of learned positional basis is visually smooth, whereas the original PE has large
spiky values in the diagonal.
2. The learned positional basis has a more pronounced low-rank structure, as there is a sharper drop of
top singular values.
3. The learned positional basis has more low-frequency 2D Fourier coefficients.
B.2 Do residual connections explain consistent geometry?
The residual connections in transformers provide an identity mapping across layers, which may seem to be
a plausible reason for the consistency of observed geometry in Section 2.
We show evidence that this is not the case. We use pretrained GPT-2 to generate embeddings by feeding
the transformer with random tokens of length 512uniformly sampled from the vocabulary. Then, we calcu-
late the average norm of embeddings in each layer, and the cosine similarity of embeddings between every
two layers.
In Figure 8, we find
19

--- PAGE 20 ---
Figure 8: Average embedding norm and cosine similarity from GPT-2 based on a random sampled se-
quence.
1. The embedding norms increase significantly across layers. In particular, the norm ratio between Layer
1 and Layer 0 is 10.92, and between final layer and Layer 0 is 109.8.
2. The cosine similarity between the 0-th layer and any other layer is close to 0, which indicates that
embeddings experience dramatic changes from Layer 0 to Layer 1.
These observations imply that residual connections cannot explain the consistent geometry in Section 2.
C Additional empirical results for Section 3
We provide visualization and analysis for models other than GPT-2.
C.1 PCA visualization
See Figure 9—Figure 14. Note: BERT displays a more complex circular shape, likely because its training
objective is different from the others.
C.2 Low rank measurements
We provide rank estimates for all layers.
Rank estimate. We report the rank estimate for all pretrained models and datasets in Table 4. Additionally,
we include another rank estimate—the Stable rank estimate (Rudelson & Vershynin, 2007) in Table 5.
Relative norm. We report the relative norm for all pretrained models and datasets in Table 6.
C.3 On cluster structure
See Figure 15—Figure 17. The bottom right part of the normalized Gram matrix, namely ctx-ctx part,
exhibit progressively clear block structures. Four blocks indicate that there are 4clusters among all ctx
vectors—which correspond to exactly 4documents we sample the sequences from.
20

--- PAGE 21 ---
Figure 9: Top-2 principal components of positional basis; GitHub, GPT2
Figure 10: Top-2 principal components of positional basis; WikiText, GPT2
Figure 11: Top-2 principal components of positional basis; OpenWebText, BLOOM
21

--- PAGE 22 ---
Figure 12: Top-2 principal components of positional basis; OpenWebText, BERT
Figure 13: Top-2 principal components of positional basis; OpenWebText, Llama2
Figure 14: Top-2 principal components of positional basis; GitHub, Llama2
22

--- PAGE 23 ---
Table 4: ScreeNOT Rank Estimate for models, datasets and at each layer.
Layer 0 Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Layer 7 Layer 8 Layer 9 Layer 10 Layer 11 Layer 12
BERTGitHub 15 16 16 16 14 11 11 9 10 10 11 11 12
OpenWebText 15 16 18 16 11 11 9 9 11 11 11 11 13
WikiText 15 16 18 16 12 11 9 9 11 11 11 12 12
BLOOMGitHub 8 9 9 8 9 10 10 11 10 10 10 10 10
OpenWebText 6 10 10 11 11 10 11 11 11 11 10 10 11
WikiText 6 8 9 10 10 11 11 11 11 11 11 10 11
GPT2GitHub 15 14 13 12 12 11 11 10 10 10 11 11 10
OpenWebText 15 13 14 12 13 11 10 10 10 10 9 9 12
WikiText 15 14 14 12 11 11 11 11 11 11 9 10 12
Llama2GitHub 6 10 9 8 10 8 8 9 9 9 9 8 10
OpenWebText 7 10 10 11 11 10 9 10 9 8 9 8 10
WikiText 8 10 10 10 9 8 8 8 8 8 8 8 10
Table 5: Stable rank for models, datasets and at each layer.
Layer 0 Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Layer 7 Layer 8 Layer 9 Layer 10 Layer 11 Layer 12
BERTGitHub 9.19 7.79 5.26 4.73 4.34 3.84 3.48 3.20 2.70 2.45 2.04 1.84 1.91
OpenWebText 9.19 7.63 5.25 4.73 4.10 3.53 3.16 2.84 2.46 2.30 2.18 2.22 2.15
WikiText 9.19 7.78 5.03 4.58 3.99 3.48 3.14 2.82 2.42 2.27 2.13 2.16 2.12
BLOOMGitHub 8.39 1.25 1.20 1.21 1.21 1.23 1.29 1.29 1.28 1.25 1.21 1.02 1.00
OpenWebText 8.33 1.27 1.30 1.24 1.24 1.27 1.32 1.34 1.33 1.26 1.16 1.01 1.00
WikiText 8.42 1.27 1.28 1.30 1.31 1.34 1.41 1.43 1.41 1.32 1.22 1.01 1.00
GPT2GitHub 2.05 1.92 1.91 1.89 1.90 1.90 1.92 1.94 1.98 2.03 2.05 1.70 1.11
OpenWebText 2.05 1.92 1.91 1.89 1.88 1.88 1.88 1.90 1.91 1.96 2.02 2.24 1.49
WikiText 2.05 1.92 1.91 1.89 1.88 1.88 1.88 1.90 1.91 1.97 2.03 2.19 1.56
Llama2GitHub 24.87 1.00 1.00 1.00 1.00 1.00 1.00 1.01 1.01 1.01 1.02 1.03 1.17
OpenWebText 52.23 1.00 1.00 1.00 1.00 1.00 1.01 1.01 1.02 1.02 1.03 1.05 1.44
WikiText 24.70 1.00 1.00 1.01 1.01 1.02 1.03 1.05 1.09 1.16 1.20 1.26 1.30
Table 6: Relative norm for models, datasets and at each layer.
Layer 0 Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Layer 7 Layer 8 Layer 9 Layer 10 Layer 11 Layer 12
BERTGitHub 0.445 0.483 0.569 0.616 0.648 0.707 0.764 0.786 0.768 0.686 0.631 0.562 0.473
OpenWebText 0.465 0.546 0.660 0.759 0.877 0.977 0.973 0.967 0.953 0.901 0.777 0.658 0.596
WikiText 0.454 0.502 0.626 0.695 0.798 0.916 0.968 0.965 0.949 0.887 0.756 0.682 0.627
BLOOMGitHub 0.013 0.123 0.232 0.279 0.343 0.385 0.343 0.306 0.301 0.306 0.325 0.219 0.181
OpenWebText 0.012 0.138 0.194 0.264 0.315 0.342 0.297 0.267 0.264 0.300 0.392 0.575 0.589
WikiText 0.013 0.149 0.222 0.287 0.328 0.352 0.336 0.301 0.295 0.325 0.407 0.494 0.491
GPT2GitHub 0.999 0.994 0.996 0.972 0.812 0.762 0.672 0.600 0.489 0.442 0.386 0.303 0.123
OpenWebText 1.000 0.996 0.990 0.989 0.986 0.983 0.981 0.979 0.974 0.841 0.631 0.480 0.075
WikiText 1.000 0.995 0.991 0.989 0.987 0.986 0.984 0.984 0.981 0.933 0.680 0.555 0.110
Llama2GitHub 0.029 0.221 0.221 0.222 0.222 0.222 0.223 0.223 0.224 0.225 0.226 0.227 0.183
OpenWebText 0.038 0.146 0.146 0.146 0.147 0.147 0.148 0.148 0.150 0.152 0.154 0.154 0.148
WikiText 0.034 0.060 0.060 0.060 0.060 0.061 0.062 0.063 0.065 0.068 0.072 0.076 0.191
Figure 15: Gram matrix of positional basis and context basis; Openwebtext, BLOOM
23

--- PAGE 24 ---
Figure 16: Gram matrix of positional basis and context basis; Openwebtext, Llama2
Figure 17: Gram matrix of positional basis and context basis; GitHub, Llama
24

--- PAGE 25 ---
D Additional empirical results for Section 4
D.1 On robustness of positional basis
In Table 2, we sampled OOD sequences to test if positional basis is robust and reported the ratio explained
by top-10 frequencies.
Here we provide a visual examination, which further confirms that positional basis possesses similar
low-rank and spiral structure. We choose BLOOM, which has been pretrained on both natural languages
and programming languages, and we use random sampled tokens to test whether its associated positional
basis still has similar structure.
We find that similar structure persists even for the above randomly sampled sequences. See Figure 24.
D.2 On sparse and local attention
Here we provide evidence that the argmax property (8) is satisfied in GPT-2 at a substantial level. We
consider a variant of pos-pos QK constituent matrix ˜PW ˜P⊤where each column of ˜Pispost+µ,
because we find the global mean µhelps satisfy this argmax property.
Note that checking this pos-pos QK constituent instead of the QK matrix is more straightforward:
pos-pos QK constituent reflects the property of the model and positional basis, without needing the context
information.
For each layer and each head, we calculate ˜PW ˜P⊤and examine the fraction of positions tsuch that
arg max
t′≤t[˜PW ˜P⊤]t,t′=t.
Figure 25 shows the ratios for each layer and head. It is clear that this argmax property is mostly satisfied in
many heads, especially among early layers.
D.3 On Addition experiments
We have manually generated the addition dataset. The number of digits of the addition ranges from 5 to 10
, and is sampled under uniform distribution in the training process. The model achieve above 99% accuracy
on training set and in-distribution test set. However, the model does not achieve length generalization. It
achieves 32.6%, 8.0%, 11.1%, and 9.9% accuracy on 1k samples of addition with digits length 1, 2, 3, 4
respectively.
Similar findings for transformers with relative embedding. Additionally, we implement a rotary-embedding
based model by replacing the absolute positional embedding with RoPE Su et al. (2021). For the rotary-
embedding based model, the corresponding accuracy is 35.6%, 13.4%, 20.0%, 38.5%.
Visualizing discontinuity. We plot the QK matrices and Gram matrices for both transformers—with abso-
lute embedding and with rotary position embedding. See Figure 18—Figure 23. Notice that the unsmooth-
ness is pervasive in the Gram matrix of positional basis and QK matrix across different layers, heads, and
models .
Quantitative measurements. We calculated the ratios that low-frequency components can explain in the
Fourier space at various layers and heads. The lack of smoothness is further confirmed by the Fourier
analysis.
25

--- PAGE 26 ---
Figure 18: Gram matrix of positional basis; Addition with absolute PE shows nonsmoothness.
26

--- PAGE 27 ---
Figure 19: QK matrix of positional basis; Addition with absolute PE shows nonsmoothness.
27

--- PAGE 28 ---
Figure 20: Fourier of positional basis; Addition with absolute PE depends on higher-frequency components.
28

--- PAGE 29 ---
Figure 21: Fourier of positional basis; Addition with rotary PE shows nonsmoothness.
29

--- PAGE 30 ---
Figure 22: QK matrix of positional basis; Addition with absolute PE shows nonsmoothness.
30

--- PAGE 31 ---
Figure 23: Fourier of positional basis; Addition with absolute PE depends on higher-frequency components.
Figure 24: Top-2 PC plot of positional basis ; Bloom on random tokens
31

--- PAGE 32 ---
Figure 25: Ratio of positions that satisfy the argmax property (8).
E Additional empirical results for Section 5
E.1 On trained weight matrix
We provide decomposition of trained weight matrices similar to Figure 4. See Figure 26—Figure 29 for
more plots about W=Wq(Wk)⊤/√dhead at various layers and heads for BERT and GPT-2 model.
E.2 On enhanced QK/attention visualization
On global mean vector. We show the QK matrix decomposition with global mean (Figure 30), and with-
out global mean (Figure 31). Note that adding a constant to all entries of the QK matrix will not change the
attention matrix, because softmax computes the ratio. We conclude that the global mean vector µhas little
effect on interpretations.
Visualizing QK constituents. We use the QK decomposition (10) to provide fine-grained visualization
for attention heads. In Figure 32—Figure 34, we select various heads on GPT-2 and BLOOM model and
provide 6 subplots based on (i) the 4 QK constituents, (ii) the original QK matrix, and (iii) the attention
matrix.
We find that visualization based on individual QK constituents provides additional information beyond
visualizing the attention matrix alone. Our selection of heads is representative, as many similar heads appear
in GPT-2 and BLOOM.
F Proofs for theoretical results
We introduce some additional notations. Denote the indicator function by 1. We denote by 1Nthe vector
(1,1, . . . , 1)⊤∈RN. For a complex matrix A, we denote the conjugate transpose by A∗. For convenience,
for a matrix A, we will write ∆Ainstead of ∆(1,1)A. For a vector x∈CN, we also write ∆xto denote
the finite difference vector N·(x1−x0, x2−x1, . . . , x N−xN−1)⊤(where xN=x0). We will say that a
Hermitian matrix A∈CN×Nis positive semidefinite (PSD) if and only if x∗Ax≥0for every x∈CN.
Denote by ℜ(x)the real part of a complex number x∈C.
32

--- PAGE 33 ---
Figure 26: Dissecting attention weights, GPT2 L6
Figure 27: Dissecting attention weights, GPT2 L10
Figure 28: Dissecting attention weights, BERT L6
Figure 29: Dissecting attention weights, BERT L10
33

--- PAGE 34 ---
Figure 30: QK matrix decomposition with global mean; GPT2, Layer 10 Head 7
Figure 31: QK matrix decomposition without global mean; GPT2, Layer 10 Head 7
Figure 32: QK Decomposition; BLOOM L10H1
Figure 33: QK Decomposition; BLOOM L7H6
Figure 34: QK Decomposition; GPT2 L4H11
34

--- PAGE 35 ---
F.1 Proof of Theorem 1
In this subsection, we denote a generic dimension by Nandω= exp( −2πi/N ). We need some standard
definitions and properties; see Broughton & Bryan (2018) for example.
The discrete Fourier transform (DFT) matrix F∈CN×Nis given by Ftt′=ω(t−1)(t′−1)for1≤t, t′≤
N. The inverse discrete Fourier transform (IDFT) matrix is N−1F∗. Both the DFT matrix and the IDFT
matrix are symmetric (not Hermitian). Sometimes we prefer to write F⊤instead of Fsimply for formality.
For a generic matrix A∈RN×N, we denote by ˆA∈CN×Nthe matrix after its 2-d DFT. It satisfies
ˆA=FAF⊤,
A=N−2F∗ˆA(F∗)⊤.(15)
The following simple lemma is a consequence of integration-by-parts for the discrete version. For
completeness we include a proof.
Lemma 1. Letx∈RNbe a vector, and ˆx=Fxbe its DFT. Then for t= 1, . . . , N ,
ˆxt=γt(F∆x)t+1{t= 1} ·NX
t′=1xt′, where (16)
γt:=N−1
1−exp−2πi(t−1)
N−1
fort >1andγ1:= 1. (17)
Proof. Ift= 1, then (Fx)t=PN
t′=1xt′, and (F∆x)t=NPN
t′=1(xt′−xt′−1) = 0 . Fort̸= 1,
(F∆x)t=NNX
t′=1ω(t−1)(t′−1)(xt′−xt′−1) =NNX
t′=1 
ω(t−1)(t′−1)−ω(t−1)t′
xt′
=N 
1−ω(t−1)NX
t′=1ω(t−1)(t′−1)xt′=γ−1
t(Fx)t.
This shows ˆxt=γt(F∆x)tfort̸= 1and completes the proof.
A simple bound on the modulus |γt|is given by the following lemma.
Lemma 2. Letγtbe defined by (17). For positive integer 1< t≤N/2, we have
|γN−t+2|=|γt| ≤1
8(t−1).
Proof. The equality part is obvious. For any θ∈(−π, π), we have
|1−exp(iθ)|2= (1−cosθ)2+ sin2θ= 4 sin2(θ/2) = 4 sin2(|θ|/2)
Since sinθ/θis monotone decreasing in (0, π/2), we have sin(|θ|/2)≥sin(π/2)|θ|/(π/2), Thus,
|1−exp(iθ)| ≥4|θ|/π .
Setting θ=−2π(t−1)/N, we obtain the desired upper bound on |γt|.
35

--- PAGE 36 ---
Denote Γ= diag {γ1, . . . , γ N}. By Lemma 1, for any vector x∈RN,
Fx=ΓF∆x+
x⊤1N
0
...
0
.
Below we will assume that the generic matrix A∈RN×Nis symmetric and satisfies A1N=0.Observe
that
FA= [FA :,1, . . . ,FA :,N] =ΓF[∆A:,1, . . . , ∆A:,N],
FAF⊤=ΓF(∆A)F⊤Γ
where we used A1N=0and(∆A)1N=0. Repeating the second equality mtimes, we obtain
ˆA=FAF⊤=ΓmF(∆(m,m)A)F⊤Γm.
Now fix a generic nonempty index sets I ⊂ { 1,2, . . . , N }and denote J={1, . . . , N } \ I . Consider the
block matrix form of ˆA:
ˆA=ˆAI,IˆAI,J
ˆAJ,IˆAJ,J
.
Using the block matrix notation, we derive
∥ˆAI,J∥op=∥Γm
I,I 
F(∆(m,m)A)F⊤
I,JΓm
J,J∥op≤max
t∈I,t′∈J|γtγt′|m∥F(∆(m,m)A)F⊤∥op.
Similar inequalities hold for other three blocks. By Lemma 2 we have |γt| ≤1for all t. Adding the three
inequalities that involve at least one index set J, we get
ˆA−ˆAI,I0
0 0
op≤3 max
t∈J|γt|m∥F(∆(m,m)A)F⊤∥op.
SinceFF∗=N, we have ∥F∥op=∥FF∗∥1/2
op=N1/2. Denoting
A(res):=N−2F∗h
ˆA−ˆAI,I0
0 0i
(F∗)⊤,
we find
∥A(res)∥op≤3 max
t∈J|γt|mN−2∥F∥4
op∥∆(m,m)A∥op≤3 max
t∈J|γt|mN∥∆(m,m)A∥max (18)
where the first inequality is due to Lemma 2 and the second inequality is due to the inequality between
matrix operator norm and max norm.
To finish the proof, let us make some specification: we identify Nwith ˜T(namely 2T), identify Awith
˜G, and identify I={1, . . . , k }∪{ ˜T−k+ 1, . . . , ˜T}. These choices satisfy the requirement for Abecause
˜Gis symmetric and it satisfies ˜G1˜T= 2G1T=0due to the assumption pos1+. . .+posT=0.
By Lemma 2, max t∈J|γt| ≤1/(8k), so (18) gives
∥A(res)∥op≤6(8k)−mT∥∆(m,m)˜G∥max
By the definition of A(res)and the identities in 15,
˜G=A(res)+N−2F∗ˆAI,I0
0 0
(F∗)⊤.
We make the following claim.
36

--- PAGE 37 ---
Lemma 3. There exists B∈Rk×ksuch that
N−2
F∗ˆAI,I0
0 0
(F∗)⊤
1:T,1:T=F≤kB(F≤kB)⊤(19)
While the DFT matrix is complex, the above lemma claims that the left-hand side is a Gram matrix
of real low-frequency vectors. Once this lemma is proved, we can combine this lemma with (18) and
G=˜G1:T,1:Tto obtain the desired inequality 1 in Theorem 1.
Proof of Lemma 3. Recall ω= exp( −2πi/2T). We further introduce some notations. Denote I1=
{1, . . . , k }andI2={T−k+ 1, . . . , T }so that I=I1∪ I2. Let qt=ωt−1for positive integer t,
matrix Q∈RT×kand matrix D∈Rk×kbe given by
Qt,s=ℜ(qs−0.5
t), where t≤T, s≤k, D= diag( q1/2
1, . . . , q1/2
k)
Fort, t′∈ I,
ˆAt,t′= (FAFT)t,t′
=TX
s,s′=1Ft,sGs,s′Ft′,s′+TX
s,s′=1Ft,2T+1−sGs,s′Ft′,s′
+TX
s,s′=1Ft,sGs,s′Ft′,2T+1−s′+TX
s,s′=1Ft,2T+1−sGs,s′Ft′,2T+1−s′
=q1/2
tq1/2
t′TX
s,s′=1Gs,s′
qs−0.5
tqs′−0.5
t′ +q−s+0.5
t qs′−0.5
t′ +qs−0.5
tq−s′+0.5
t′ +q−s+0.5
t q−s′+0.5
t′
= 4q1/2
tq1/2
t′TX
s,s′=1Gs,s′ℜ(qs−1/2
t )ℜ(qs′−1/2
t′ ).
Ift, t′∈ I1, the above equality leads to
ˆAI1,I1= 4DQ⊤GQD ;
and more generally ˆAI,Iis given by symmetrically extending ˆAI1,I1as in the definition of ˜G. Since
4Q⊤GQ is a PSD, we can find B0∈Rk×ksuch that
4Q⊤GQ=B0B⊤
0.
We want to simplify F∗
:,IˆAI,IF∗
I,:, namely
F∗
:,I1ˆAI1,I1F∗
I1,:+F∗
:,I1ˆAI1,I2F∗
I2,:+F∗
:,I2ˆAI2,I1F∗
I1,:+F∗
:,I2ˆAI2,I2F∗
I2,: (20)
For any t, t′∈ I1,
(F∗)t,I1ˆAI1,I1(F∗)I1,t′= (F∗)t,I1DB 0B⊤
0D(F∗)I1,t′
and similar equations hold for other three cases. Observe that
(F∗)t,I1D= (¯q1−0.5
t, . . . , ¯qk−0.5
t), (F∗)t,I2D= (¯q2T−0.5
t , . . . , ¯q2T−k+0.5
t ).
When we add the four terms in 20, the imaginary part cancels out. Thus,
F∗
t,IˆAI,IF∗
I,t′= 4
ℜ(¯q1−0.5
t), . . . ,ℜ(¯qk−0.5
t)
B0B⊤
0
ℜ(¯q1−0.5
t), . . . ,ℜ(¯qk−0.5
t)⊤
.
Note that ℜ(¯qs−0.5
t) = cos( π(t−1)(s−0.5)/T) = (F≤k)t,s. Expressing F∗
:,IˆAI,IF∗
I,:in the matrix form
and denote B=B0/N, we find that (19) holds.
37

--- PAGE 38 ---
F.2 Proof of Theorem 2
First we note that
KW(xq,xk) =KW11(xq,xk)KW12(xq,xk)KW21(xq,xk)KW22(xq,xk). (21)
We will prove that
KW11(xq,xk) = 
1 +O(incoh)
·KW11(cq,ck). (22)
To prove this, it suffices to show that
max
KW11(cq,tk), KW11(tq,ck), KW11(tq,tk)	
= 1 + O(incoh) . (23)
We decompose W11as in (13) and find
log
KW11(cq,tk)
= (cq)⊤W11tk=sX
k=1ak(u⊤
kcq)(v⊤
ktk).
By mutual incoherence, |v⊤
ktk| ≤incoh sincevk∈ B 1andtk∈ B 2; and trivially |u⊤
kck| ≤1, so
log
KW11(cq,tk)≤sX
k=1|u⊤
kck| · |v⊤
ktq| ≤s·incoh .
Since by assumption s=O(1)andexp(incoh) = 1 + O(incoh) , we derive
KW11(cq,tk) = 1 + O(incoh) .
The other two terms in (23) follow a similar argument and thus are all bounded by 1 +O(incoh) . We can
prove similarly that
KW12(xq,xk) = 
1 +O(incoh)
·KW12(cq,tk),
KW21(xq,xk) = 
1 +O(incoh)
·KW21(tq,ck),
KW21(xq,xk) = 
1 +O(incoh)
·KW22(tq,tk).
and together with (22) and (21), this leads to the desired (14).
Below we prove the “moreover” part. By standard properties of independent subgaussian random vari-
ables (Vershynin, 2018, Sect. 2), (cq)⊤Z11tkis still a subgaussian random variable, and with probability at
least1−O(exp(−incoh2·d)), for certain constant C >0,
log
KW11+Z/√
d(cq,tk)≤s·incoh + Cincoh = O(incoh) .
Similar high-probability bounds hold for other terms. By the union bound over all possible choice of vectors
inB0
1andB0
2, we arrive at our claim.
38
