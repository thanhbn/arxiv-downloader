# 2203.16634.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/positional-embeddings/2203.16634.pdf
# Kích thước tệp: 310805 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Mô hình ngôn ngữ Transformer không có mã hóa vị trí
vẫn học được thông tin vị trí
Adi HavivOri RamOﬁr Press!Peter IzsakOmer Levy
Tel Aviv University!University of WashingtonIntel LabsMeta AI
{adi.haviv, ori.ram, levyomer}@cs.tau.ac.il ,ofirp@cs.washington.edu, peter.izsak@intel.com
Tóm tắt
Các mô hình ngôn ngữ transformer nhân quả (LMs),
như GPT-3, thường yêu cầu một số hình thức
mã hóa vị trí, chẳng hạn như nhúng vị trí.
Tuy nhiên, chúng tôi cho thấy rằng các LMs không có
bất kỳ mã hóa vị trí rõ ràng nào vẫn có thể
cạnh tranh với các mô hình tiêu chuẩn, và hiện tượng này
mạnh mẽ trên các tập dữ liệu khác nhau,
kích thước mô hình và độ dài chuỗi. Các thí nghiệm
thăm dò tiết lộ rằng các mô hình như vậy có được
khái niệm ngầm về vị trí tuyệt đối xuyên suốt
mạng, hiệu quả bù đắp cho
thông tin bị thiếu. Chúng tôi phỏng đoán rằng
sự chú ý nhân quả cho phép mô hình suy ra
số lượng tiền nhiệm mà mỗi token có thể chú
ý đến, do đó xấp xỉ vị trí tuyệt đối của nó.
Những phát hiện của chúng tôi chỉ ra rằng các LMs nhân quả
có thể có được nhận thức vị trí không chỉ
từ cơ chế định vị rõ ràng, mà còn
từ những ảnh hưởng của mặt nạ nhân quả.
1 Giới thiệu
Cơ chế chú ý (Bahdanau et al., 2015)
của transformer (Vaswani et al., 2017) bất khả tri
về vị trí và thứ tự của các token trong chuỗi
đầu vào. Do đó, thông thường người ta
chèn thông tin vị trí thông qua nhúng vị trí tuyệt đối
(Vaswani et al., 2017; Radford et al.,
2018) hoặc các yếu tố thiên lệch tương đối (Shaw et al., 2018;
Raffel et al., 2020; Press et al., 2022). Ở đây, chúng tôi
chứng minh rằng các mô hình ngôn ngữ transformer không có
bất kỳ thông tin vị trí rõ ràng nào có thể và thực sự
học được khái niệm ngầm về vị trí tuyệt đối
đủ để đạt hiệu suất cạnh tranh.
Chúng tôi so sánh hiệu suất của các mô hình ngôn ngữ
được huấn luyện không có thông tin vị trí rõ ràng
( mô hình ngôn ngữ NoPos ) với những mô hình được huấn luyện
với ba cơ chế nhận biết vị trí khác nhau,
cụ thể: nhúng hình sin (Vaswani et al.,
2017), nhúng đã học (Gehring et al., 2017),
và ALiBi (Press et al., 2022). Kết quả cho thấy rằng
các mô hình NoPos cạnh tranh với các mô hình nhận biết vị trí
NoPos Learned Sinusoidal ALiBi1011121314Perplexity13.10 13.0512.93
12.51Hình 1: Các mô hình ngôn ngữ Transformer được huấn luyện mà
không mã hóa thông tin vị trí một cách rõ ràng ( NoPos )
tiếp cận hiệu suất của các mô hình được huấn luyện với các
phương pháp mã hóa vị trí khác nhau. Tất cả các mô hình có
1.3B tham số, và được huấn luyện trên một đoạn trích của
Pile.
các mô hình một cách nhất quán trên các tập dữ liệu, kích thước mô hình,
và độ dài chuỗi đầu vào (ví dụ, Hình 1).
Để làm sáng tỏ những phát hiện của chúng tôi, chúng tôi thăm dò
nhận thức vị trí của các mô hình ngôn ngữ NoPos,
so sánh với các mô hình sử dụng cơ chế vị trí tương đối hoặc tuyệt đối.
Cụ thể, chúng tôi huấn luyện các bộ phân loại
để dự đoán vị trí của một token dựa trên biểu diễn của nó
trên các lớp khác nhau trong mạng.
Các thăm dò của chúng tôi tiết lộ rằng mô hình NoPos đạt được
khoảng cách tuyệt đối trung bình tương tự giữa dự đoán
và vị trí mong đợi, như một mô hình với
nhúng vị trí tuyệt đối đã học.
Chúng tôi giả thuyết rằng hành vi đáng ngạc nhiên này
gắn liền với mặt nạ chú ý nhân quả, mà ngầm
chèn thông tin vị trí vào lớp tự chú ý
để bảo tồn bản chất tự hồi quy của các mô hình ngôn ngữ.
Một cách trực quan, một mô hình có thể
đếm các tiền nhiệm của một token cho trướcarXiv:2203.16634v2  [cs.CL]  5 Dec 2022

--- TRANG 2 ---
có thể suy ra vị trí tuyệt đối của nó một cách cơ bản. Để kiểm tra
giả thuyết của chúng tôi, chúng tôi chạy các thí nghiệm tương tự cho
các mô hình ngôn ngữ có mặt nạ (MLM) (Devlin et al.,
2019), sử dụng sự chú ý bất biến thứ tự (vì
không có mặt nạ nhân quả nào được áp dụng). Thật vậy, các mô hình
hai chiều không hội tụ khi thông tin vị trí
vắng mặt, củng cố giả thuyết của chúng tôi. Để kết luận,
những đóng góp chính của chúng tôi là:
•Chúng tôi chứng minh tính mạnh mẽ của mô hình NoPos
(so với các mô hình nhận biết vị trí)
đối với kích thước mô hình, tập dữ liệu và độ dài chuỗi.
•Chúng tôi cung cấp một phân tích của mô hình NoPos đã huấn luyện,
và cho thấy rằng nó mã hóa các vị trí tuyệt đối.
•Chúng tôi cho thấy rằng sự thành công của các mô hình NoPos
là duy nhất đối với các mô hình ngôn ngữ nhân quả.
2 Mã hóa vị trí
Các mô hình Transformer bao gồm các lớp tự chú ý
và feed-forward xen kẽ, cả hai đều
bất biến thứ tự. Do đó, để truyền đạt thứ tự của
các token đầu vào, một số hình thức thông tin vị trí
được giới thiệu một cách rõ ràng vào mô hình. Các vị trí tuyệt đối
thường được mã hóa dưới dạng vector
(một cho mỗi vị trí), sau đó được cộng vào
các nhúng của token đầu vào và đưa vào lớp đầu tiên
của transformer. Các vị trí tương đối thường
được mã hóa dưới dạng thiên lệch (được cộng vào điểm chú ý)
trong các lớp tự chú ý. Trong công trình này, chúng tôi
xem xét ba phương pháp phổ biến làm cơ sở:
Learned. Các nhúng được huấn luyện để biểu diễn các vị trí tuyệt đối
(Sukhbaatar et al., 2015; Gehring
et al., 2017). Các nhúng vị trí đã học
thường được sử dụng trong MLMs (Devlin et al., 2019; Liu
et al., 2019) cũng như trong các mô hình ngôn ngữ
tự hồi quy lớn, chẳng hạn như GPT-3 (Brown et al., 2020).
Sinusoidal. Các vector hằng số được tính bởi một hàm
không tham số của vị trí tuyệt đối của token đầu vào.
Các hàm sin và cosin với tần số khác nhau
được sử dụng, sao cho mỗi chiều
của mã hóa vị trí tương ứng với một
sinusoid. Các nhúng hình sin được giới thiệu trong
Vaswani et al. (2017) cho dịch máy, và
cũng được sử dụng trong mô hình hóa ngôn ngữ (Baevski and
Auli, 2019).
ALiBi. Attention with LInear BIases (Press et al.,
2022) chèn thông tin về khoảng cách tương đối
giữa các token bằng cách thêm thiên lệch âm vào điểm chú ý, tăng tuyến tính với
khoảng cách giữa mỗi cặp token.
3 Thiết lập thí nghiệm
Một cách trực quan, mã hóa thông tin vị trí một cách rõ ràng
là quan trọng để cho phép các mô hình ngôn ngữ transformer
dự đoán token tiếp theo trong một chuỗi. Để
kiểm tra trực quan này, chúng tôi so sánh độ phức tạp của tập xác thực
của các mô hình được huấn luyện từ đầu không có
thông tin vị trí rõ ràng (được ký hiệu là NoPos )
với những mô hình được huấn luyện với các phương pháp mã hóa vị trí
khác nhau đã thảo luận trong Phần 2. Chúng tôi điều tra
thiết lập WikiText-103 kinh điển (Merity
et al., 2017; Baevski and Auli, 2019), cũng như một
thiết lập quy mô lớn mới hơn dựa trên kho ngữ liệu Pile
(Gao et al., 2020) trên các kiến trúc mô hình được lấy cảm hứng
từ Brown et al. (2020), nơi chúng tôi bao phủ một phổ
kích thước mô hình và độ dài chuỗi.
Thiết lập kinh điển (WikiText-103). Kho ngữ liệu
WikiText-103 (Merity et al., 2017) bao gồm
hơn 100 triệu từ được trích xuất từ một tập
các bài báo Wikipedia chất lượng cao. Kho ngữ liệu
được token hóa ở cấp độ từ, dẫn đến một từ vựng
gồm hơn 267K token. Đối với kho ngữ liệu này, chúng tôi
sử dụng mô hình transformer nhúng thích ứng của
Baevski và Auli (2019), chứa 16 lớp transformer
với 1024 chiều mô hình, 4096
chiều feed-forward, và 8 đầu chú ý.
Tổng thể, mô hình này có 247M tham số.
Chúng tôi huấn luyện với các siêu tham số tối ưu hóa chính xác của họ,
như được triển khai trong fairseq (Ott et al.,
2019), ngoại trừ độ dài chuỗi đầu vào, được rút ngắn
xuống 512 token (thay vì 3072), như trong Press et al. (2022).
Xem App. C để biết siêu tham số chi tiết.
Thiết lập quy mô lớn (The Pile). The Pile
(Gao et al., 2020) là một tập dữ liệu văn bản tiếng Anh 800GB
bao gồm Common Crawl và 22 nguồn đa dạng khác.
Đối với các thí nghiệm của chúng tôi, chúng tôi sử dụng 2 trong số
30 shard;1trong số này, chúng tôi lọc ra các nguồn GitHub
và DM Mathematics và loại bỏ 1% ví dụ ngắn nhất
và 1% dài nhất từ mỗi
nguồn để giảm nhiễu. Chúng tôi sử dụng tokenizer của GPT-2
(Radford et al., 2019) để chuyển đổi văn bản thành các chuỗi token
trên từ vựng 50K token. Chúng tôi lấy mẫu ngẫu nhiên
một tập xác thực gồm 2000 tài liệu (2.6M token) từ kho ngữ liệu,
trong khi 15M tài liệu còn lại (21B token) bao gồm
1Shards 00 và 01 có thể được tải xuống từ: https://
the-eye.eu/public/AI/pile/train/

--- TRANG 3 ---
WikiText-103 The Pile
NoPos 20.97 13.10
Learned 20.42 13.05
Sinusoidal 20.16 12.93
ALiBi 19.71 12.51
Bảng 1: Độ phức tạp của tập xác thực của các mô hình ngôn ngữ transformer
được huấn luyện với các phương pháp mã hóa vị trí khác nhau.
Thiết lập WikiText-103 (Merity et al.,
2017) sử dụng mô hình của Baevski và Auli (2019) trên
các chuỗi 512 token, trong khi thiết lập Pile (Gao
et al., 2020) sử dụng kiến trúc 1.3B tham số gần đây hơn
(Brown et al., 2020) trên các chuỗi 1024 token.
tập huấn luyện. Mô hình cơ sở trong thiết lập này
tuân theo kiến trúc 1.3B tham số của Brown
et al. (2020), còn được biết đến là GPT-3 XL: 24 lớp transformer
với 2048 chiều mô hình, 8192
chiều feed-forward, và 32 đầu chú ý.
Độ dài chuỗi đầu vào mặc định là 1024 token.
Chúng tôi tham khảo App.C để biết siêu tham số chi tiết.
Để chứng minh tính nhất quán của kết quả
trong các thiết lập khác nhau, chúng tôi thực hiện hai thí nghiệm mở rộng.
Đầu tiên chúng tôi mở rộng kích thước mô hình bằng cách
thí nghiệm với các biến thể small (125M tham số),
medium (350M tham số), large (760M tham số)
và XL (1.3B tham số) của kiến trúc
Brown et al. (2020) trên thiết lập Pile.
Ngoài ra, chúng tôi đánh giá tác động của việc thay đổi
độ dài chuỗi sử dụng mô hình XL (1.3B tham số).
Cụ thể, chúng tôi thí nghiệm với các chuỗi
có độ dàif256;512;1024;2048g.
Cuối cùng, để làm sáng tỏ thêm về sự khác biệt giữa
mô hình NoPos với các phương pháp khác, chúng tôi so sánh
hiệu suất của mô hình trên các phần khác nhau của
chuỗi. Chi tiết của phân tích này và kết quả
được đưa ra trong App. A.
4 Kết quả
Bảng 1 so sánh hiệu suất của việc huấn luyện LMs
với các phương pháp mã hóa vị trí khác nhau. Chúng tôi quan sát
rằng các LMs NoPos tiếp cận hiệu suất của
các mô hình khác, với khoảng cách 0.55 (WikiText-103)
và 0.05 (the Pile) độ phức tạp từ các mô hình với
nhúng vị trí đã học. Trong thiết lập Pile,
sự khác biệt hiệu suất giữa NoPos ,Learned ,
vàSinusoidal nhỏ cả về mặt tuyệt đối và
đối với sự khác biệt của chúng với ALiBi . Trong
thiết lập WikiText-103, khoảng cách hiệu suất
rộng hơn nhưng vẫn khiêm tốn đối với phương sai seed ngẫu nhiên.2Những kết quả này mạnh mẽ gợi ý rằng huấn luyện
các mô hình ngôn ngữ transformer không có mã hóa vị trí rõ ràng
thực sự có thể thực hiện được.
Bảng 2 khám phá tác động của việc mở rộng số lượng
tham số trong thiết lập Pile. Trong khi các mô hình nhỏ hơn
hưởng lợi từ các mã hóa vị trí cố định, không tham số
( Sinusoidal vàALiBi ), những khoảng cách hiệu suất này
thu hẹp trong các mô hình lớn hơn. Bảng 3
cho thấy tác động của việc thay đổi độ dài chuỗi
trong cùng thiết lập. Trong thí nghiệm này, khoảng cách
giữa NoPos ,Learned , và Sinusoidal gần như
không đổi, trong khi lợi ích của việc sử dụng ALiBi
tăng lên khi các chuỗi trở nên dài hơn. Tổng thể, chúng tôi
cho thấy rằng mô hình hóa ngôn ngữ transformer không có
mã hóa vị trí rõ ràng mạnh mẽ đối với việc lựa chọn
kho ngữ liệu, kích thước mô hình, và độ dài chuỗi.
Vì huấn luyện các mô hình ở quy mô 1.3B tham số
tốn nhiều tài nguyên, chúng tôi công khai phát hành các mô hình đã huấn luyện
để nghiên cứu và phân tích trong tương lai.3
Kích thước mô hình 125M 350M 760M 1.3B
NoPos 22.15 16.87 14.29 13.10
Learned 22.04 16.84 14.21 13.05
Sinusoidal 21.49 16.58 14.04 12.93
ALiBi 19.94 15.66 13.53 12.51
Bảng 2: Độ phức tạp tập xác thực trên Pile, như một hàm
của phương pháp mã hóa vị trí và kích thước mô hình. Tất cả
các mô hình hoạt động trên các chuỗi 1024 token. Các mô hình nhỏ hơn
hưởng lợi từ các mã hóa vị trí cố định, không tham số
( Sinusoidal và ALiBi ), nhưng những khoảng cách hiệu suất này
giảm dần khi các mô hình mở rộng quy mô.
Độ dài chuỗi 256 512 1024 2048
NoPos 14.98 13.82 13.10 12.87
Learned 14.94 13.77 13.05 12.72
Sinusoidal 14.84 13.66 12.93 12.62
ALiBi 14.65 13.37 12.51 12.06
Bảng 3: Độ phức tạp tập xác thực trên Pile, như một hàm
của phương pháp mã hóa vị trí và độ dài chuỗi. Tất cả
các mô hình có 1.3B tham số. Sự khác biệt hiệu suất
giữa NoPos ,Learned , và Sinusoidal nhất quán nhỏ,
trong khi ALiBi từ từ trở nên có lợi hơn khi các chuỗi
trở nên dài hơn.
Trong một công trình đồng thời, Scao et al. (2022) có
quan sát tương tự trong một trong các thí nghiệm ablation
của họ và tiếp tục cho thấy rằng các mô hình NoPos đạt được
2Để tham khảo, Press et al. (2020) báo cáo rằng huấn luyện
mô hình sinusoidal với đầu vào có độ dài 3072 trên WikiText-103
với 5 seed khác nhau có thể dẫn đến khoảng cách lên đến 0.9 độ phức tạp
giữa các lần chạy (độ lệch chuẩn 0.34).
3https://github.com/adihaviv/NoPos

--- TRANG 4 ---
0 4 8 12 16 20 24
Layer050100150200250300350Mean Absolute Distance
NoPos
Learned
Sinusoidal
ALiBi
RandomHình 2: Thông qua thăm dò, chúng tôi thấy rằng mô hình NoPos
có hành vi tương tự như các mô hình sử dụng nhúng vị trí tuyệt đối
đã học. Chúng tôi đánh giá hiệu suất sử dụng khoảng cách tuyệt đối trung bình
trên các mô hình 1.3B tham số được huấn luyện trên Pile.
hiệu suất cạnh tranh cho các nhiệm vụ downstream cũng vậy.
Cụ thể, họ đánh giá 27 nhiệm vụ downstream đa dạng.
Họ cho thấy rằng mô hình NoPos đạt độ chính xác trung bình
41:23% trên tất cả các nhiệm vụ, so với Learned vàALiBi
đạt 41:72% và43:70% tương ứng.
5 Phân tích
Trong phần này, chúng tôi kiểm tra xem liệu mô hình NoPos
có thể mã hóa thông tin vị trí và
cho thấy rằng thông tin như vậy là cần thiết cho sự thành công của nó.
Các mô hình NoPos có được thông tin vị trí
Liệu các LMs NoPos có học một số hình thức mã hóa vị trí
để bù đắp cho sự vắng mặt của mô hình vị trí rõ ràng không?
Để trả lời câu hỏi này, chúng tôi
thăm dò mỗi lớp của các mô hình đã huấn luyện4để tìm thông tin vị trí.
Cụ thể, chúng tôi sử dụng biểu diễn ẩn cuối cùng của các token
sau mỗi lớp transformer, được tạo ra bởi LM được đánh giá,
và huấn luyện một mạng feed-forward ReLU 2 lớp để dự đoán
vị trí tuyệt đối (0 đến 1023) của mỗi token (tức là, như
một bài toán phân loại đa lớp). Đáng chú ý, chúng tôi
không thay đổi trọng số của các LMs được đánh giá
và do đó, không cung cấp bất kỳ thông tin vị trí nào
4Chúng tôi sử dụng các mô hình 1.3B tham số được huấn luyện trên các chuỗi 1024 token
của Pile (Phần 3).
của các token cho LM trong thí nghiệm này, điều này
đảm bảo tính hợp lệ của những phát hiện của chúng tôi.
Mỗi thăm dò của lớp được huấn luyện riêng biệt (các siêu tham số
được cung cấp trong App. C). Như một thước đo độ chính xác mềm,
chúng tôi đo khoảng cách tuyệt đối trung bình giữa dự đoán của thăm dó
và vị trí thực tế của token.
Hình 2 cho thấy rằng mặc dù mô hình NoPos
bắt đầu, như mong đợi, không có thông tin vị trí nào
trong lớp đầu tiên (ngang bằng với cơ sở ngẫu nhiên),
nó trở nên nhận biết vị trí trong vòng bốn lớp và
dường như chứa nhiều thông tin vị trí hơn
ALiBi. Đến lớp giữa, NoPos có thể dự đoán
các vị trí tuyệt đối gần như tốt bằng mô hình với
nhúng vị trí đã học. Cuối cùng, chúng tôi quan sát
rằng tất cả các mô hình đều loại bỏ một lượng đáng kể
thông tin vị trí trong các lớp cuối, phù hợp
với những phát hiện của V oita et al. (2019). Tổng thể,
thăm dò tiết lộ rằng các mô hình NoPos học
khái niệm ngầm về vị trí tuyệt đối.
Để làm rõ thông tin vị trí nào mà mô hình NoPos học,
chúng tôi trực quan hóa các dự đoán của thăm dò.
Chúng tôi kiểm tra một mẫu 100 dự đoán
từ tập xác thực của thăm dò hoạt động tốt nhất
được huấn luyện trên mô hình NoPos. Hình 3 cho thấy
các dự đoán trên các chuỗi 512 token được lấy mẫu
ngẫu nhiên từ tập xác thực và một ví dụ đơn
từ cùng tập. Chúng tôi quan sát rằng thăm dò
chính xác hơn ở đầu chuỗi,
nhưng trở nên mơ hồ hơn khi nó tiến triển.
Thông tin vị trí quan trọng NoPos có thể
suy ra các vị trí tuyệt đối, nhưng chúng có cần thiết không?
Chúng tôi trả lời điều này bằng cách sử dụng một mô hình NoPos đã huấn luyện.
Thay vì tính toán loss trên toàn bộ chuỗi,
chúng tôi chọn một token ngẫu nhiên duy nhất, xáo trộn
các token trước đó mà nó phụ thuộc vào, và
so sánh với cơ sở nơi tiền tố vẫn
nguyên vẹn. Chúng tôi thấy rằng trong trường hợp hậu tố
bị xáo trộn, loss cấp token trung bình tăng
mạnh (từ 4 lên11). Chi tiết của thí nghiệm này
được đưa ra trong App. B.
Phát hiện này chỉ ra rằng mô hình NoPos
thực sự sử dụng thông tin vị trí mà nó có được, vì
nếu không chúng ta sẽ mong đợi các giá trị loss tương tự trong
hai thiết lập này.
6 Phỏng đoán
Làm thế nào các transformer không có mã hóa vị trí rõ ràng
học được các vị trí tuyệt đối? Chúng tôi phỏng đoán
rằng sự chú ý nhân quả trong các mô hình ngôn ngữ transformer
tự hồi quy cho phép chúng dự đoán

--- TRANG 5 ---
0 64 128 192 256 320 384 448 512
Target Position064128192256320384448512Predicted Position
NoPos Probe Predictions
(mean and conf. interval)
NoPos Probe
Single Example Predictions
Ground TruthHình 3: Một trực quan hóa các dự đoán vị trí tuyệt đối
của một thăm dò được huấn luyện trên một mô hình ngôn ngữ NoPos.
Đường màu xanh cho thấy trung bình của các dự đoán được tạo ra
cho mỗi vị trí mục tiêu và vùng màu xanh
đại diện cho khoảng tin cậy 95%. Các dự đoán
cho một chuỗi ngẫu nhiên đơn được mô tả như
các chấm màu xanh lá cây.
số lượng token có thể chú ý tại mỗi vị trí, tức là
số lượng token trong chuỗi mà đi trước
token hiện tại. Một cơ chế như vậy có thể hiệu quả
mã hóa vị trí tuyệt đối của mỗi token
vào biểu diễn vector của nó. Thật vậy, phân tích của chúng tôi
(Phần 5) tiết lộ rằng một số khái niệm về vị trí tuyệt đối
tồn tại trong các lớp ẩn của các mô hình ngôn ngữ
ngay cả khi chúng được huấn luyện không có mã hóa vị trí rõ ràng,
và thông tin này được có được
xuyên suốt vài lớp đầu tiên. Mặt khác, các bộ mã hóa transformer
hai chiều (được sử dụng trong mô hình hóa ngôn ngữ có mặt nạ,
ví dụ Devlin et al. 2019) không chứa mặt nạ chú ý nhân quả
hoặc bất kỳ giới hạn nào khác đối với cơ chế chú ý;
do đó, chúng không thể học các vị trí tuyệt đối
mà không có mã hóa vị trí rõ ràng. Chúng tôi
đã kiểm tra hệ quả này bằng cách huấn luyện một mô hình ngôn ngữ có mặt nạ
dựa trên RoBERTa large (Liu et al., 2019)
trên Pile (xem App. C để biết siêu tham số). Bảng 4 cho thấy rằng, thật vậy, mô hình NoPos có
độ phức tạp tệ hơn đáng kể so với các cơ sở được thông báo vị trí.
Kết quả này phản ánh những phát hiện của Sinha et al. (2021),
những người cũng quan sát rằng các MLMs không có nhúng vị trí
gặp phải sự suy giảm hiệu suất đáng kể.
7 Công trình liên quan
Mặc dù đã có nhiều nghiên cứu về các biến thể mã hóa vị trí,
nhưng tương đối ít công trình trước đây điều tra khả năng của các mô hình
suy ra vị trí một cách ngầm. Trước công trình của chúng tôi,
Irie et al. (2019) đã khám phá các mô hình ngôn ngữ transformer
cho nhận dạng giọng nói và thấy rằng các mô hình như vậy,
khi được huấn luyện không có mã hóa vị trí, vượt trội hơn
những mô hình được huấn luyện với nhúng hình sin. Ngoài ra,
một thí nghiệm mô hình hóa ngôn ngữ tập trung của
Stella Rose Biderman5cho thấy rằng phương pháp NoPos
đạt kết quả tương tự như các phương pháp nhúng vị trí khác;
tuy nhiên, thí nghiệm đó trên một mô hình 350M tham số nhỏ
được huấn luyện trên một tập dữ liệu cấp ký tự nhỏ (enwik8).
Ở đây chúng tôi cho thấy rằng kết quả này đúng trên nhiều tập dữ liệu
và kích thước mô hình, cung cấp phân tích về các biểu diễn
nội tại của mô hình, và giả thuyết về cách hiện tượng này có thể xảy ra.
8 Kết luận
Chúng tôi cho thấy rằng, trái với niềm tin phổ biến, các mô hình ngôn ngữ transformer
thực sự học thông tin vị trí ngay cả khi không được cung cấp
bất kỳ mã hóa vị trí rõ ràng nào. Các thí nghiệm của chúng tôi
một cách có hệ thống chứng minh rằng hiện tượng này mạnh mẽ
trên các thiết lập mô hình hóa ngôn ngữ khác nhau, và
người ta có thể xấp xỉ vị trí tuyệt đối của
mỗi token từ các biểu diễn nội tại của mô hình
đến một mức độ đáng ngạc nhiên. Tuy nhiên, hiện tượng này
không mở rộng đến các bộ mã hóa transformer được huấn luyện
trên mục tiêu MLM. Chúng tôi phỏng đoán rằng
cơ chế chú ý nhân quả, giới hạn sự chú ý
theo một hướng của chuỗi, chịu trách nhiệm
cho việc ngầm thấm vào transformer thông tin vị trí.
5https://twitter.com/BlancheMinerva/status/
1394089508723900422

--- TRANG 6 ---
MLM Perplexity
NoPos 147.18
Learned 4.06
Sinusoidal 4.07
ALiBi 4.00
Bảng 4: Độ phức tạp tập xác thực của các mô hình ngôn ngữ có mặt nạ
(Devlin et al., 2019) được huấn luyện với các phương pháp mã hóa vị trí khác nhau
trên một đoạn trích của Pile (Gao et al., 2020). Kiến trúc mô hình
dựa trên RoBERTa large (Liu et al., 2019), và xử lý
128 token mỗi chuỗi. Trong khi các mô hình nhận biết vị trí
hội tụ đến độ phức tạp rất thấp, huấn luyện không có
mã hóa vị trí ( NoPos ) thất bại.
9 Hạn chế
Công trình của chúng tôi khám phá các mô hình ngôn ngữ trong phạm vi
125M đến 1.3B tham số. Chúng tôi cho thấy rằng khi số lượng tham số
tăng, khoảng cách giữa phương pháp NoPos
và các phương pháp vị trí khác thu hẹp lại. Xu hướng này
khiến chúng tôi tin rằng những phát hiện của chúng tôi sẽ đúng đối với
các mô hình thậm chí còn lớn hơn, nhưng các mô hình lớn nhất hiện tại
lớn hơn hơn một trăm lần (về mặt tham số)
so với các mô hình 1.3B tham số của chúng tôi,
và do đó kết quả trong thiết lập đó có thể bất ngờ.
Ngoài ra, huấn luyện các mô hình ở quy mô 1.3B tham số
tốn nhiều tài nguyên và có thể cản trở khả năng tái tạo.
Do đó chúng tôi phát hành các mô hình đã huấn luyện của mình.
Ngoài ra, khi so sánh độ phức tạp của NoPos
với các mô hình khác, mặc dù các biên độ rất nhỏ,
NoPos luôn hơi tệ hơn, gợi ý rằng
thiên lệch quy nạp của mã hóa vị trí
thực sự quan trọng.
Lời cảm ơn
Công trình này được hỗ trợ bởi Intel Corporation,
Meta Platforms Inc và Deutsch Foundation.
Tài liệu tham khảo
Alexei Baevski and Michael Auli. 2019. Adaptive input representations for neural language modeling. In
7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May
6-9, 2019 . OpenReview.net.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2015. Neural machine translation by
jointly learning to align and translate. CoRR ,
abs/1409.0473.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Amanda Askell, Sandhini Agarwal, Ariel Herbert-
V oss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,
Clemens Winter, Chris Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In
Advances in Neural Information Processing Systems ,
volume 33, pages 1877–1901. Curran Associates,
Inc.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human LanguageTechnologies, Volume 1 (Long and Short Papers) ,
pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,
Horace He, Anish Thite, Noa Nabeshima, Shawn
Presser, and Connor Leahy. 2020. The Pile: An
800gb dataset of diverse text for language modeling.
arXiv preprint arXiv:2101.00027 .
Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, and Yann Dauphin. 2017. Convolutional sequence to sequence learning. In ICML .
Kazuki Irie, Albert Zeyer, Ralf Schlüter, and Hermann
Ney. 2019. Language modeling with deep transformers. In INTERSPEECH .
Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings .
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized BERT pretraining approach. CoRR , abs/1907.11692.
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2017. Pointer sentinel mixture models. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April
24-26, 2017, Conference Track Proceedings . OpenReview.net.
Yurii Nesterov. 1983. A method for unconstrained convex minimization problem with the rate of convergence o(1=k2).
Myle Ott, Sergey Edunov, Alexei Baevski, Angela
Fan, Sam Gross, Nathan Ng, David Grangier, and
Michael Auli. 2019. fairseq: A fast, extensible
toolkit for sequence modeling. In Proceedings of
the 2019 Conference of the North American Chapter of the Association for Computational Linguistics
(Demonstrations) , pages 48–53, Minneapolis, Minnesota. Association for Computational Linguistics.
Oﬁr Press, Noah Smith, and Mike Lewis. 2022. Train
short, test long: Attention with linear biases enables
input length extrapolation. In International Conference on Learning Representations .
Oﬁr Press, Noah A. Smith, and Omer Levy. 2020. Improving transformer models by reordering their sublayers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics ,
pages 2996–3005, Online. Association for Computational Linguistics.
Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language understanding by generative pre-training.

--- TRANG 7 ---
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring
the limits of transfer learning with a uniﬁed text-totext transformer. Journal of Machine Learning Research , 21(140):1–67.
Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile
Saulnier, Stas Bekman, M Saiful Bari, Stella Biderman, Hady Elsahar, Jason Phang, Oﬁr Press, Colin
Raffel, Victor Sanh, Sheng Shen, Lintang Sutawika,
Jaesung Tae, Zheng Xin Yong, Julien Launay, and
Iz Beltagy. 2022. What language model to train if
you have one million GPU hours? In Challenges &
Perspectives in Creating Large Language Models .
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
2018. Self-attention with relative position representations. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers) , pages 464–468,
New Orleans, Louisiana. Association for Computational Linguistics.
Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle
Pineau, Adina Williams, and Douwe Kiela. 2021.
Masked language modeling and the distributional
hypothesis: Order word matters pre-training for little. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing ,
pages 2888–2913, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.
Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,
and Rob Fergus. 2015. End-to-end memory networks. In Proceedings of the 28th International
Conference on Neural Information Processing Systems - Volume 2 , NIPS'15, page 2440–2448, Cambridge, MA, USA. MIT Press.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Processing Systems , volume 30. Curran Associates, Inc.
Elena V oita, Rico Sennrich, and Ivan Titov. 2019. The
bottom-up evolution of representations in the transformer: A study with machine translation and language modeling objectives. In Proceedings of the
2019 Conference on Empirical Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 4396–4406, Hong Kong,
China. Association for Computational Linguistics.

--- TRANG 8 ---
A Hiệu suất NoPos trên các phân đoạn khác nhau
của đầu vào
Để làm sáng tỏ thêm về những phát hiện được hiển thị trong
phần 4, chúng tôi khám phá xem liệu có những phần nào của
chuỗi mà mô hình NoPos dự đoán tốt hơn
so với các phương pháp vị trí khác không (ví dụ, liệu mô hình NoPos
có hoạt động tốt hơn ở đầu hay cuối chuỗi không).
Chúng tôi tính toán độ phức tạp của mô hình trong các phần khác nhau
của các chuỗi. Cụ thể, chúng tôi chia mỗi chuỗi đầu vào
thành tám phân đoạn liên tiếp và tính toán độ phức tạp
cho mỗi phân đoạn riêng biệt.
Chúng tôi đánh giá các mô hình NoPos và Learned 1.3B tham số
được huấn luyện trên Pile, với độ dài chuỗi đầu vào
là 1024, và sử dụng tập xác thực tiêu chuẩn.
Hình 4 cho thấy kết quả của thí nghiệm này. Mô hình NoPos
hoạt động tương tự hoặc hơi tệ hơn
so với mô hình cơ sở trên tất cả các phần đầu vào.
1:64 65:128 129:192 193:256 257:320 321:384 385:448 449:512
Sequence Split4.24.34.44.54.64.74.84.9Loss
NoPos
Sinusoidal
Hình 4: Mô hình NoPos cho thấy hiệu suất tương tự trên
mỗi phần của chuỗi, so với mã hóa vị trí tuyệt đối
Learned cơ sở.
B Phân tích thứ tự từ
Liệu thông tin vị trí có cần thiết cho mô hình hóa ngôn ngữ,
hay thứ tự của các token đầu vào không quan trọng? Để trả lời điều này,
chúng tôi tiến hành thí nghiệm sau: thay vì tính toán loss
trên chuỗi hoàn chỉnh, chúng tôi chọn một token cụ thể
trong chuỗi. Dự đoán token tiếp theo được điều kiện
trên các token trước đó trong chuỗi, và do đó chúng tôi
xáo trộn thứ tự của các token trong tiền tố và
tính toán loss chỉ cho token cụ thể đó. Chúng tôi
lặp lại thí nghiệm với chuỗi tiền tố gốc, không bị xáo trộn
làm cơ sở và so sánh kết quả.
Thí nghiệm được tiến hành trên mô hình NoPos
với độ dài chuỗi đầu vào là 512 sử dụng
tập dữ liệu WikiText-103. Chúng tôi lấy mẫu ngẫu nhiên
một chỉ số giữa 5 và 512 cho token mà chúng tôi chọn
từ mỗi chuỗi đầu vào từ tập xác thực.
Hình 5 cho thấy kết quả của thí nghiệm này cho 100 đầu vào khác nhau. Những kết quả này rõ ràng cho thấy
rằng các dự đoán từ tiếp theo của mô hình ngôn ngữ transformer
không bất biến thứ tự.
Baseline Shuffled Prefix4681012Token-Level Loss
Hình 5: Xáo trộn các token đầu vào (cho mô hình hóa ngôn ngữ nhân quả)
dẫn đến suy giảm lớn trong loss cấp token.
C Siêu tham số
Bảng 5 cung cấp các siêu tham số tối ưu hóa
cho từng thí nghiệm của chúng tôi, và Bảng 6 cho thấy
các siêu tham số mô hình trong thiết lập hiện đại (Pile).

--- TRANG 9 ---
WikiText-103 The Pile Probe Masked LM
Độ dài chuỗi 512 1024 1024 128
Optimizer NAG Adam Adam Adam
Tốc độ học tối đa 1 2e-3 2e-3 1e-3
Bước khởi động 16,000 500 500 500
Tổng số bước 286,000 10,000 10,000 10,000
Token mỗi batch 72,000 256,000 64,000 1,024,000
Dropout 0.3 0 0 0.1
Weight Decay 0 0.01 0.01 0.01
Bảng 5: Các siêu tham số tối ưu hóa được sử dụng trong công trình này. Optimizer NAG đề cập đến Nesterov accelerated
gradient (Nesterov, 1983), và Adam đề cập đến (Kingma and Ba, 2015).
125M 350M 760M 1.3B
Lớp 12 24 24 24
Chiều mô hình 768 1024 1536 2048
Chiều Feed-forward 3072 4096 6144 8192
Đầu chú ý 12 16 16 32
Bảng 6: Các siêu tham số mô hình theo kích thước.
