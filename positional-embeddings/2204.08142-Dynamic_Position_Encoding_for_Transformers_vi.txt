# 2204.08142.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/positional-embeddings/2204.08142.pdf
# Kích thước tệp: 335112 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Mã hóa Vị trí Động cho Transformers
Joyce Zheng
Phòng thí nghiệm Noah's Ark của Huawei
jy6zheng@uwaterloo.caMehdi Rezagholizadeh
Phòng thí nghiệm Noah's Ark của Huawei
mehdi.rezagholizadeh@huawei.com
Peyman Passbany
Amazon
passban.peyman@gmail.com
Tóm tắt
Các mô hình tuần hoàn đã thống trị lĩnh vực dịch máy thần kinh (NMT) trong vài năm qua. Transformers (Vaswani et al., 2017) đã thay đổi hoàn toàn điều này bằng việc đề xuất một kiến trúc mới dựa trên backbone feed-forward và cơ chế self-attention. Mặc dù Transformers rất mạnh mẽ, chúng có thể thất bại trong việc mã hóa thông tin tuần tự/vị trí một cách phù hợp do tính chất không tuần hoàn của chúng. Để giải quyết vấn đề này, các embedding vị trí được định nghĩa độc quyền cho từng bước thời gian để làm phong phú thông tin từ. Tuy nhiên, những embedding như vậy được cố định sau quá trình huấn luyện bất kể nhiệm vụ và hệ thống sắp xếp từ của ngôn ngữ nguồn và đích.

Trong bài báo này, chúng tôi giải quyết khuyết điểm này bằng cách đề xuất một kiến trúc mới với các embedding vị trí mới có tính đến thứ tự của các từ đích. Thay vì sử dụng các embedding vị trí được định nghĩa trước, giải pháp của chúng tôi tạo ra các embedding mới để tinh chỉnh thông tin vị trí của từng từ. Vì chúng tôi không quy định vị trí của các token nguồn và học chúng theo cách end-to-end, chúng tôi gọi phương pháp của mình là mã hóa vị trí động (DPE). Chúng tôi đánh giá tác động của mô hình trên nhiều tập dữ liệu để dịch từ tiếng Anh sang tiếng Đức, Pháp và Ý và quan sát được những cải thiện có ý nghĩa so với Transformer gốc.

1 Giới thiệu
Trong dịch máy thống kê (SMT), nhiệm vụ chung của việc dịch bao gồm việc rút gọn câu đầu vào thành các đơn vị nhỏ hơn (còn được gọi là các cụm từ thống kê), chọn một bản dịch tối ưu cho từng đơn vị, và đặt chúng theo đúng thứ tự (Koehn, 2009). Bước cuối cùng, còn được gọi là vấn đề sắp xếp lại, là một nguồn phức tạp và quan trọng lớn, được xử lý bằng nhiều giải pháp thống kê cũng như dựa trên chuỗi và cây (Bisazza và Federico, 2016).

Như đã thấy rõ trong SMT, cấu trúc và vị trí của các từ trong câu là rất quan trọng để dịch chính xác. Tầm quan trọng của thông tin như vậy cũng có thể được khám phá trong NMT và cho Transformers. Các tài liệu trước đây, như Chen et al. (2020), đã chứng minh rằng các câu đầu vào nguồn được làm phong phú với thông tin thứ tự đích có khả năng cải thiện chất lượng dịch trong các mô hình thần kinh. Họ cho thấy mã hóa vị trí dường như đóng vai trò quan trọng trong việc dịch và điều này đã thúc đẩy chúng tôi khám phá thêm lĩnh vực này.

Vì Transformers có kiến trúc không tuần hoàn, chúng có thể gặp vấn đề khi mã hóa dữ liệu tuần tự. Kết quả là, chúng cần một phép cộng tường minh của các embedding đầu vào với mã hóa vị trí để cung cấp thông tin về thứ tự của từng từ. Tuy nhiên, cách tiếp cận này giả định sai rằng vị trí đúng của mỗi từ luôn là vị trí gốc của nó trong câu nguồn. Diễn giải này có thể đúng khi chỉ xem xét phía nguồn, trong khi chúng ta biết từ SMT (Bisazza và Federico, 2016; Cui et al., 2016) rằng việc sắp xếp các từ đầu vào theo thứ tự của các cặp đích của chúng có thể dẫn đến kết quả tốt hơn.

Trong công trình này, chúng tôi khám phá việc tiêm thông tin vị trí đích cùng với các từ nguồn để nâng cao chất lượng dịch của Transformers. Trước tiên, chúng tôi kiểm tra độ chính xác và hiệu quả của một Transformer hai lần (2PT), bao gồm một pipeline kết nối hai Transformers. Transformer đầu tiên sắp xếp lại câu nguồn và Transformer thứ hai dịch các câu đã được sắp xếp lại. Mặc dù cách tiếp cận này kết hợp thông tin thứ tự từ ngôn ngữ đích, nhưng nó thiếu huấn luyện end-to-end và cần nhiều tài nguyên hơn một Transformer thông thường. Theo đó, chúng tôi giới thiệu một giải pháp thay thế tốt hơn, có thể học hiệu quả các vị trí được sắp xếp lại theo cách end-to-end và sử dụng thông tin này với chuỗi nguồn gốc để tăng độ chính xác dịch. Chúng tôi gọi giải pháp thay thế này là Mã hóa Vị trí Động (DPE).

Đóng góp của chúng tôi trong công trình này có ba khía cạnh:
• Đầu tiên, chúng tôi chứng minh rằng việc cung cấp các biểu diễn phía nguồn với thông tin vị trí đích cải thiện chất lượng dịch trong Transformers.
• Chúng tôi cũng đề xuất một kiến trúc mới, DPE, có thể học hiệu quả các vị trí được sắp xếp lại theo cách end-to-end và kết hợp thông tin này vào quá trình mã hóa.
• Cuối cùng, chúng tôi sử dụng một thiết kế hai lần sơ bộ để chỉ ra tầm quan trọng của việc học end-to-end trong vấn đề này.

2 Nền tảng
2.1 Sắp xếp lại trước trong Dịch máy
Trong SMT tiêu chuẩn, sắp xếp lại trước là một kỹ thuật nổi tiếng. Thông thường, câu nguồn được sắp xếp lại bằng heuristics sao cho nó tuân theo thứ tự từ của ngôn ngữ đích. Hình 1 minh họa khái niệm này với một ví dụ tưởng tượng.

Hình 1: Thứ tự của các từ nguồn trước (bên trái) và sau (bên phải) sắp xếp lại trước. Si và Tj thể hiện từ nguồn thứ i và từ đích thứ j, tương ứng.

Như hình cho thấy, sự liên kết ban đầu giữa các từ nguồn (Si) và đích (Ti) được sử dụng để xác định thứ tự mới cho câu nguồn. Với thứ tự mới, máy dịch không cần học mối quan hệ giữa hệ thống sắp xếp nguồn và đích, vì nó dịch trực tiếp từ một vị trí bên nguồn sang cùng vị trí bên đích. Rõ ràng, điều này có thể giảm đáng kể độ phức tạp của nhiệm vụ dịch. Công trình của Wang et al. (2007), cung cấp một ví dụ tốt về các hệ thống với sắp xếp lại trước, trong đó các tác giả nghiên cứu kỹ thuật này cho cặp tiếng Anh-Trung Quốc.

Khái niệm sắp xếp lại không nhất thiết cần được giải quyết trước khi dịch; trong Koehn et al. (2007), các chuỗi được tạo ra được xem xét bởi một bộ phân loại sau khi dịch để sửa vị trí của các từ được đặt ở thứ tự sai. Toàn bộ quá trình sắp xếp lại cũng có thể được nhúng vào quá trình giải mã (Feng et al., 2013).

2.2 Giải quyết vấn đề Thứ tự trong NMT
Sắp xếp lại trước và mã hóa vị trí cũng phổ biến trong NMT và đã được các nhà nghiên cứu khác nhau điều tra. Du và Way (2017) khám phá liệu các mô hình thần kinh tuần hoàn có thể hưởng lợi từ sắp xếp lại trước hay không. Phát hiện của họ cho thấy những mô hình này có thể không cần bất kỳ điều chỉnh thứ tự nào vì bản thân mạng đã đủ mạnh để học những ánh xạ như vậy.

Kawara et al. (2020), không giống như công trình trước đó, nghiên cứu cùng vấn đề và báo cáo những quan sát đầy hứa hẹn về tính hữu ích của sắp xếp lại trước. Họ sử dụng phương pháp dựa trên ngữ pháp chuyển đổi với mạng thần kinh đệ quy và cho thấy việc sắp xếp lại trước có thể tác động như thế nào.

Liu et al. (2020) theo một cách tiếp cận khác và đề xuất mô hình hóa mã hóa vị trí như một hệ thống động liên tục thông qua neural ODE. Ke et al. (2020) điều tra việc cải thiện mã hóa vị trí bằng cách tháo rời mối quan hệ giữa từ và vị trí. Họ cho rằng không có mối tương quan mạnh giữa từ và vị trí tuyệt đối, vì vậy họ loại bỏ mối tương quan nhiễu này. Hình thức tách biệt này có những ưu điểm riêng, nhưng bằng cách loại bỏ mối quan hệ này giữa từ và vị trí khỏi quá trình dịch, họ có thể mất thông tin ngữ nghĩa có giá trị về phía nguồn và đích.

Shaw et al. (2018a) khám phá phương pháp mã hóa vị trí tương đối bằng cách để cơ chế self-attention xem xét khoảng cách giữa các từ nguồn. Garg et al. (2019) kết hợp thông tin vị trí đích thông qua multitasking trong đó loss dịch được kết hợp với loss liên kết giám sát một decoder head để học thông tin vị trí. Chen et al. (2020) thay đổi kiến trúc Transformer để kết hợp thông tin thứ tự ở mỗi lớp. Họ chọn một vị trí từ đích được sắp xếp lại từ đầu ra của mỗi lớp và tiêm nó vào lớp tiếp theo. Đây là công trình gần gũi nhất với chúng tôi, vì vậy chúng tôi coi nó là baseline chính.

3 Phương pháp
3.1 Dịch Hai Lần cho Sắp xếp lại Trước
Mục tiêu của chúng tôi là tăng chất lượng dịch trong Transformers bằng cách tiêm thông tin thứ tự đích vào quá trình mã hóa phía nguồn. Chúng tôi đề xuất mã hóa vị trí động (DPE) để đạt được điều này, nhưng trước đó, chúng tôi thảo luận về kiến trúc Transformer hai lần (2PT) sơ bộ để chứng minh tác động của thông tin thứ tự trong Transformers.

Sự khác biệt chính giữa 2PT và DPE là DPE là một giải pháp end-to-end, khả vi trong khi 2PT là một pipeline kết nối hai Transformers khác nhau. Cả hai đều hướng tới việc tận dụng thông tin thứ tự để cải thiện dịch nhưng theo những cách khác nhau. Kiến trúc 2PT được minh họa trong Hình 2.

Hình 2: Kiến trúc Transformer hai lần. Chuỗi đầu vào trước tiên được sắp xếp lại thành dạng mới và ít phức tạp hơn cho Transformer dịch. Sau đó, Transformer dịch sử dụng chuỗi đầu vào đã được sắp xếp lại để giải mã chuỗi đích.

2PT có hai Transformers khác nhau. Cái đầu tiên được sử dụng cho mục đích sắp xếp lại thay vì dịch. Nó nhận câu nguồn và tạo ra phiên bản được sắp xếp lại của chúng, ví dụ quay lại Hình 1, nếu đầu vào cho Transformer đầu tiên là [S1,S2,S3,S4] thì đầu ra mong đợi từ transformer đầu tiên là [S1,S4,S3,S2]. Chúng tôi đã tạo một corpus mới sử dụng FastAlign để huấn luyện mô hình sắp xếp lại này (Dyer et al., 2013).

FastAlign là một bộ liên kết từ không giám sát xử lý câu nguồn và đích cùng nhau và cung cấp các liên kết cấp từ. Nó có thể sử dụng được ở thời điểm huấn luyện nhưng không cho suy luận vì nó cần truy cập cả hai phía và chúng ta chỉ có phía nguồn (ở thời điểm test). Như một giải pháp, chúng tôi sử dụng các liên kết để tạo một tập huấn luyện và sử dụng nó để huấn luyện Transformer đầu tiên trong 2PT. Hình 3 cho thấy định dạng đầu vào và đầu ra trong FastAlign và cách nó giúp tạo ra các mẫu huấn luyện.

--- TRANG 2 ---
mation với chuỗi nguồn gốc để tăng độ chính xác dịch. Chúng tôi gọi giải pháp thay thế này là Mã hóa Vị trí Động (DPE).

Đóng góp của chúng tôi trong công trình này có ba khía cạnh:
• Đầu tiên, chúng tôi chứng minh rằng việc cung cấp các biểu diễn phía nguồn với thông tin vị trí đích cải thiện chất lượng dịch trong Transformers.
• Chúng tôi cũng đề xuất một kiến trúc mới, DPE, có thể học hiệu quả các vị trí được sắp xếp lại theo cách end-to-end và kết hợp thông tin này vào quá trình mã hóa.
• Cuối cùng, chúng tôi sử dụng một thiết kế hai lần sơ bộ để chỉ ra tầm quan trọng của việc học end-to-end trong vấn đề này.

2 Nền tảng
2.1 Sắp xếp lại trước trong Dịch máy
Trong SMT tiêu chuẩn, sắp xếp lại trước là một kỹ thuật nổi tiếng. Thông thường, câu nguồn được sắp xếp lại bằng heuristics sao cho nó tuân theo thứ tự từ của ngôn ngữ đích. Hình 1 minh họa khái niệm này với một ví dụ tưởng tượng.

Hình 1: Thứ tự của các từ nguồn trước (bên trái) và sau (bên phải) sắp xếp lại trước. Si và Tj thể hiện từ nguồn thứ i và từ đích thứ j, tương ứng.

Như hình cho thấy, sự liên kết ban đầu giữa các từ nguồn (Si) và đích (Ti) được sử dụng để xác định thứ tự mới cho câu nguồn. Với thứ tự mới, máy dịch không cần học mối quan hệ giữa hệ thống sắp xếp nguồn và đích, vì nó dịch trực tiếp từ một vị trí bên nguồn sang cùng vị trí bên đích. Rõ ràng, điều này có thể giảm đáng kể độ phức tạp của nhiệm vụ dịch. Công trình của Wang et al. (2007), cung cấp một ví dụ tốt về các hệ thống với sắp xếp lại trước, trong đó các tác giả nghiên cứu kỹ thuật này cho cặp tiếng Anh-Trung Quốc.

Khái niệm sắp xếp lại không nhất thiết cần được giải quyết trước khi dịch; trong Koehn et al. (2007), các chuỗi được tạo ra được xem xét bởi một bộ phân loại sau khi dịch để sửa vị trí của các từ được đặt ở thứ tự sai. Toàn bộ quá trình sắp xếp lại cũng có thể được nhúng vào quá trình giải mã (Feng et al., 2013).

2.2 Giải quyết vấn đề Thứ tự trong NMT
Sắp xếp lại trước và mã hóa vị trí cũng phổ biến trong NMT và đã được các nhà nghiên cứu khác nhau điều tra. Du và Way (2017) khám phá liệu các mô hình thần kinh tuần hoàn có thể hưởng lợi từ sắp xếp lại trước hay không. Phát hiện của họ cho thấy những mô hình này có thể không cần bất kỳ điều chỉnh thứ tự nào vì bản thân mạng đã đủ mạnh để học những ánh xạ như vậy.

Kawara et al. (2020), không giống như công trình trước đó, nghiên cứu cùng vấn đề và báo cáo những quan sát đầy hứa hẹn về tính hữu ích của sắp xếp lại trước. Họ sử dụng phương pháp dựa trên ngữ pháp chuyển đổi với mạng thần kinh đệ quy và cho thấy việc sắp xếp lại trước có thể tác động như thế nào.

Liu et al. (2020) theo một cách tiếp cận khác và đề xuất mô hình hóa mã hóa vị trí như một hệ thống động liên tục thông qua neural ODE. Ke et al. (2020) điều tra việc cải thiện mã hóa vị trí bằng cách tháo rời mối quan hệ giữa từ và vị trí. Họ cho rằng không có mối tương quan mạnh giữa từ và vị trí tuyệt đối, vì vậy họ loại bỏ mối tương quan nhiễu này. Hình thức tách biệt này có những ưu điểm riêng, nhưng bằng cách loại bỏ mối quan hệ này giữa từ và vị trí khỏi quá trình dịch, họ có thể mất thông tin ngữ nghĩa có giá trị về phía nguồn và đích.

Shaw et al. (2018a) khám phá phương pháp mã hóa vị trí tương đối bằng cách để cơ chế self-attention xem xét khoảng cách giữa các từ nguồn. Garg et al. (2019) kết hợp thông tin vị trí đích thông qua multitasking trong đó loss dịch được kết hợp với loss liên kết giám sát một decoder head để học thông tin vị trí. Chen et al. (2020) thay đổi kiến trúc Transformer để kết hợp thông tin thứ tự ở mỗi lớp. Họ chọn một vị trí từ đích được sắp xếp lại từ đầu ra của mỗi lớp và tiêm nó vào lớp tiếp theo. Đây là công trình gần gũi nhất với chúng tôi, vì vậy chúng tôi coi nó là baseline chính.

3 Phương pháp
3.1 Dịch Hai Lần cho Sắp xếp lại Trước
Mục tiêu của chúng tôi là tăng chất lượng dịch trong Transformers bằng cách tiêm thông tin thứ tự đích vào quá trình mã hóa phía nguồn. Chúng tôi đề xuất mã hóa vị trí động (DPE) để đạt được điều này, nhưng trước đó, chúng tôi thảo luận về kiến trúc Transformer hai lần (2PT) sơ bộ để chứng minh tác động của thông tin thứ tự trong Transformers.

Sự khác biệt chính giữa 2PT và DPE là DPE là một giải pháp end-to-end, khả vi trong khi 2PT là một pipeline kết nối hai Transformers khác nhau. Cả hai đều hướng tới việc tận dụng thông tin thứ tự để cải thiện dịch nhưng theo những cách khác nhau. Kiến trúc 2PT được minh họa trong Hình 2.

Hình 2: Kiến trúc Transformer hai lần. Chuỗi đầu vào trước tiên được sắp xếp lại thành dạng mới và ít phức tạp hơn cho Transformer dịch. Sau đó, Transformer dịch sử dụng chuỗi đầu vào đã được sắp xếp lại để giải mã chuỗi đích.

2PT có hai Transformers khác nhau. Cái đầu tiên được sử dụng cho mục đích sắp xếp lại thay vì dịch. Nó nhận câu nguồn và tạo ra phiên bản được sắp xếp lại của chúng, ví dụ quay lại Hình 1, nếu đầu vào cho Transformer đầu tiên là [S1,S2,S3,S4] thì đầu ra mong đợi từ transformer đầu tiên là [S1,S4,S3,S2]. Chúng tôi đã tạo một corpus mới sử dụng FastAlign để huấn luyện mô hình sắp xếp lại này (Dyer et al., 2013).

FastAlign là một bộ liên kết từ không giám sát xử lý câu nguồn và đích cùng nhau và cung cấp các liên kết cấp từ. Nó có thể sử dụng được ở thời điểm huấn luyện nhưng không cho suy luận vì nó cần truy cập cả hai phía và chúng ta chỉ có phía nguồn (ở thời điểm test). Như một giải pháp, chúng tôi sử dụng các liên kết để tạo một tập huấn luyện và sử dụng nó để huấn luyện Transformer đầu tiên trong 2PT. Hình 3 cho thấy định dạng đầu vào và đầu ra trong FastAlign và cách nó giúp tạo ra các mẫu huấn luyện.

--- TRANG 3 ---
Như hình minh họa, cho một cặp câu tiếng Anh-Đức, các liên kết từ được tạo ra. Để xử lý các liên kết, chúng tôi thiết kế các quy tắc để xử lý các trường hợp khác nhau:

• Liên kết Một-tới-Nhiều: Chúng tôi chỉ xem xét vị trí đích đầu tiên trong các liên kết một-tới-nhiều (xem hình).
• Liên kết Nhiều-tới-Một: Nhiều từ nguồn được sắp xếp lại cùng nhau (như một đơn vị trong khi duy trì vị trí tương đối của chúng với nhau) sử dụng vị trí của từ đích tương ứng.
• Không có Liên kết: Các từ không có liên kết nào được bỏ qua và chúng tôi không thay đổi vị trí của chúng.
• Chúng tôi cũng đảm bảo rằng không có từ nguồn nào được liên kết với vị trí vượt quá độ dài câu nguồn.

Xem xét những quy tắc này và những gì FastAlign tạo ra, câu đầu vào ví dụ "mr hän@@ sch represented you on this occasion ." (trong Hình 3) được sắp xếp lại thành "mr hän sch you this represented .". @@ là các ký hiệu phụ trợ được thêm vào giữa các đơn vị sub-word trong quá trình tiền xử lý. Xem Phần 4 để biết thêm thông tin.

Sử dụng các câu được sắp xếp lại, Transformer đầu tiên trong 2PT được huấn luyện để sắp xếp lại các câu nguồn gốc, và Transformer thứ hai, chịu trách nhiệm dịch, nhận các câu nguồn được sắp xếp lại và ánh xạ chúng vào bản dịch đích của chúng. Mặc dù có định dạng dữ liệu khác nhau và đầu vào/đầu ra khác nhau, 2PT vẫn là một pipeline dịch từ ngôn ngữ nguồn sang ngôn ngữ đích thông qua các sửa đổi nội bộ được ẩn khỏi người dùng.

3.2 Mã hóa Vị trí Động
Không giống như 2PT, phương pháp mã hóa vị trí động (DPE) tận dụng ưu thế của huấn luyện end-to-end, trong khi phía nguồn vẫn học thông tin vị trí sắp xếp lại đích. Nó tăng cường đầu vào của encoder Transformer thông thường với thông tin vị trí đích, nhưng để nguyên kiến trúc của nó, như được minh họa trong Hình 4.

Đầu vào cho DPE là một embedding từ nguồn (wi) cộng với mã hóa vị trí sinusoidal (pi) (wi⊕pi). Chúng tôi gọi những embedding này là embedding được làm phong phú. Mã hóa vị trí sinusoidal là một phần của thiết kế gốc của Transformers và chúng tôi giả định người đọc đã quen thuộc với khái niệm này. Để biết thêm chi tiết, xem Vaswani et al. (2017).

--- TRANG 4 ---
Hình 3: Sắp xếp lại dựa trên FastAlign sử dụng câu mẫu từ tập dữ liệu tiếng Anh-Đức của chúng tôi. Sử dụng các liên kết từ, chúng tôi tạo ra dạng sắp xếp lại mới từ mỗi câu nguồn làm chuỗi đích mới. Sau đó chúng tôi sử dụng các cặp câu nguồn và chuỗi đích mới để huấn luyện Transformer đầu tiên của 2PT.

Hình 4: Bên trái là kiến trúc Transformer gốc và hình bên phải là kiến trúc được đề xuất của chúng tôi. E1 là lớp encoder đầu tiên của Transformer thông thường và DP1 là lớp đầu tiên của mạng DPE.

DPE là một mạng thần kinh khác được đặt giữa các embedding được làm phong phú và lớp encoder đầu tiên của Transformer (dịch). Nói cách khác, đầu vào cho mạng DPE là bảng embedding của Transformer, và lớp cuối cùng của nó xuất ra lớp encoder đầu tiên của Transformer. Do đó, mạng DPE có thể được huấn luyện cùng với Transformer sử dụng các cặp câu song song gốc.

DPE xử lý các embedding được làm phong phú và tạo ra dạng mới của chúng được biểu diễn là ri trong bài báo này, tức là DPE(wi⊕pi) = ri. Các embedding được tạo bởi DPE nhằm bảo tồn thông tin thứ tự phía đích về mỗi từ. Trong Transformer gốc, vị trí của wi được quy định bằng cách cộng pi, nhưng vị trí gốc của từ này không phải lúc nào cũng là tốt nhất cho dịch; do đó ri được định nghĩa để giải quyết vấn đề này. Nếu wi xuất hiện ở vị trí thứ i nhưng j là vị trí tốt nhất của nó đối với ngôn ngữ đích, ri được cho là học thông tin về vị trí thứ j và bắt chước pj. Theo đó, sự kết hợp của pi và ri sẽ cung cấp cho wi thông tin sắp xếp lại trước mà nó cần để cải thiện độ chính xác dịch.

Trong thiết kế của chúng tôi, DPE bao gồm hai lớp Transformer. Chúng tôi xác định số này thông qua một nghiên cứu thực nghiệm để tìm sự cân bằng hợp lý giữa chất lượng dịch và tiêu thụ tài nguyên. Hai lớp này được kết nối với một hàm loss phụ trợ để đảm bảo rằng đầu ra của DPE là những gì chúng ta cần cho sắp xếp lại.

Loss bổ sung này đo lường mean squared error giữa các embedding được tạo bởi DPE (ri) và các vị trí giám sát (PE) được định nghĩa bởi các liên kết FastAlign. Quá trình học này được công thức hóa đơn giản trong Phương trình 1:

Lorder = ΣjSj i=1 MSE(PEi; ri) / |S|     (1)

trong đó S là độ dài chuỗi nguồn và MSE() là hàm mean-square error. Vị trí giám sát PEi được thu được bằng cách lấy vị trí đích liên kết với wi được định nghĩa bởi FastAlign như mô tả trong Phần 3.1.

Để làm rõ cách Lorder hoạt động, chúng tôi sử dụng kịch bản được đề cập trước đó làm ví dụ. Chúng tôi giả định rằng vị trí đúng cho wi theo các liên kết FastAlign là j, vậy PEi = pj và do đó chúng tôi tính MSE(pj; ri). Thông qua kỹ thuật này, chúng tôi khuyến khích mạng DPE học sắp xếp lại trước theo cách end-to-end và cung cấp cho wi thông tin tinh chỉnh vị trí.

Hàm loss tổng khi huấn luyện toàn bộ mô hình bao gồm hàm loss sắp xếp lại phụ trợ Lorder cộng với loss Transformer tiêu chuẩn Ltranslation, như trong Phương trình 2:

Ltotal = Ltranslation + α(1-α)Lorder     (2)

trong đó α là một siêu tham số đại diện cho trọng số của loss sắp xếp lại. α được xác định bằng cách tối thiểu hóa loss tổng trên tập phát triển trong quá trình huấn luyện.

4 Nghiên cứu Thực nghiệm
4.1 Tập dữ liệu
Để huấn luyện và đánh giá các mô hình của chúng tôi, chúng tôi sử dụng bộ sưu tập IWSLT-14 (Cettolo et al., 2012) và tập dữ liệu WMT-14. Các tập dữ liệu của chúng tôi thường được sử dụng trong lĩnh vực này, điều này làm cho kết quả của chúng tôi dễ dàng tái tạo. Mã của chúng tôi cũng có sẵn công khai để giúp các nhà nghiên cứu khác điều tra thêm chủ đề này. Bộ sưu tập IWSLT-14 được sử dụng để nghiên cứu tác động của mô hình đối với các cặp tiếng Anh-Đức (En-De), tiếng Anh-Pháp (En-Fr), và tiếng Anh-Ý (En-It). Chúng tôi cũng báo cáo kết quả trên tập dữ liệu WMT, cung cấp corpus huấn luyện lớn hơn. Chúng tôi biết rằng chất lượng của các mô hình NMT thay đổi tỷ lệ thuận với kích thước corpus, vì vậy những thí nghiệm này cung cấp thêm thông tin để hiểu rõ hơn mô hình của chúng tôi.

Để chuẩn bị dữ liệu, các chuỗi được chuyển thành chữ thường, chuẩn hóa, và token hóa sử dụng các script được cung cấp bởi bộ công cụ Moses (Koehn et al., 2007) và phân tách thành sub-words thông qua Byte-Pair Encoding (BPE) (Sennrich et al., 2016). Kích thước từ vựng được trích xuất cho các tập dữ liệu IWSLT và WMT lần lượt là 32K và 40K. Đối với cặp En-De của WMT-14, newstest2013 được sử dụng làm tập phát triển và newstest2014 là tập test của chúng tôi. Đối với các thí nghiệm IWSLT, tập test và phát triển của chúng tôi được đề xuất bởi Zhu et al. (2020). Bảng 1 cung cấp thống kê của các tập dữ liệu của chúng tôi.

Dữ liệu | Huấn luyện | Phát triển | Test
WMT-14 (En→De) | 4.45M | 3k | 3k
IWSLT-14 (En→De) | 160k | 7k | 6k
IWSLT-14 (En→Fr) | 168k | 7k | 4k
IWSLT-14 (En→It) | 167k | 7k | 6k

Bảng 1: Thống kê của các tập dữ liệu được sử dụng trong thí nghiệm của chúng tôi. Train, Dev, và Test đại diện cho tập huấn luyện, phát triển, và test, tương ứng.

4.2 Thiết lập Thực nghiệm
Để so sánh công bằng, chúng tôi sử dụng cùng thiết lập như Chen et al. (2020) để xây dựng baseline cho các thí nghiệm WMT-14 En-De. Thiết lập baseline này cũng được sử dụng cho mô hình DPE và các thí nghiệm liên quan đến DPE. Các mô hình của chúng tôi được huấn luyện trên 8 GPU V100. Vì các mô hình của chúng tôi dựa vào backbone Transformer, tất cả các siêu tham số liên quan đến kiến trúc Transformer chính, như kích thước embedding, số head attention, v.v., được đặt thành các giá trị mặc định được đề xuất cho Transformer Base trong Vaswani et al. (2017). Tham khảo công trình gốc để biết thông tin chi tiết.

Đối với các thí nghiệm IWSLT, chúng tôi sử dụng kiến trúc nhẹ hơn vì các tập dữ liệu nhỏ hơn WMT. Kích thước ẩn là 256 cho tất cả các lớp encoder và decoder, và kích thước 1024 được sử dụng cho lớp mạng feed-forward bên trong. Có 2 lớp encoder và 2 lớp decoder, và 2 head attention. Chúng tôi tìm thấy thiết lập này thông qua một nghiên cứu thực nghiệm để tối đa hóa hiệu suất của các mô hình IWSLT.

Đối với các thí nghiệm WMT-14 En-De, tương tự như Chen et al. (2020), chúng tôi huấn luyện mô hình trong 300K cập nhật và sử dụng một mô hình duy nhất thu được từ việc tính trung bình 5 checkpoint cuối cùng. Mô hình được validate với khoảng thời gian 2K trên tập dữ liệu phát triển. Kích thước beam decoding là 5. Trong các trường hợp IWSLT-14, chúng tôi huấn luyện các mô hình trong 15.000 cập nhật và sử dụng một mô hình duy nhất thu được từ việc tính trung bình 5 checkpoint cuối cùng được validate với khoảng thời gian 1000 cập nhật. Chúng tôi đánh giá tất cả các mô hình với BLEU được detokenized (Papineni et al., 2002).

--- TRANG 5 ---
Mô hình | Loại dữ liệu | Điểm BLEU
1 Transformer Sắp xếp lại | En→En sắp xếp lại | 35.21
2 Transformer Base | En→De | 27.76
3 + được cho ăn với đầu ra của Transformer sắp xếp lại | En sắp xếp lại→De | 21.96
4 + được cho ăn với đầu ra của FastAlign | En sắp xếp lại→De | 31.82

Bảng 2: Điểm BLEU cho loạt thí nghiệm 2PT.

4.3 Thí nghiệm 2PT
Kết quả liên quan đến kiến trúc hai lần được tóm tắt trong Bảng 2. Transformer sắp xếp lại (Hàng 1) hoạt động với các câu nguồn và sắp xếp lại chúng theo thứ tự của ngôn ngữ đích. Đây là một nhiệm vụ dịch đơn ngữ với điểm BLEU là 35.21. Đây là điểm số tương đối thấp cho thiết lập đơn ngữ cho thấy vấn đề sắp xếp lại phức tạp như thế nào. Ngay cả việc dành toàn bộ một Transformer cũng không thể hoàn toàn vượt qua vấn đề sắp xếp lại. Phát hiện này cũng chỉ ra rằng các máy NMT có thể hưởng lợi từ việc sử dụng một module phụ trợ để xử lý các phức tạp về thứ tự. Thường người ta giả định rằng máy dịch nên hoạt động theo cách end-to-end trong đó nó xử lý tất cả việc sắp xếp lại, dịch, và các phức tạp khác thông qua một mô hình duy nhất cùng lúc. Tuy nhiên, nếu chúng ta có thể tách các nhiệm vụ phụ này một cách có hệ thống và giải quyết chúng riêng lẻ, có cơ hội chúng ta có thể cải thiện chất lượng tổng thể.

Trong Hàng 3, chúng tôi sử dụng thông tin được tạo trước đó (trong Hàng 1) và cho thấy mô hình dịch hoạt động như thế nào khi được cho ăn với các câu được sắp xếp lại. Điểm BLEU cho nhiệm vụ này là 21.96, thấp hơn đáng kể so với baseline (Hàng 2). Thông tin thứ tự được cho là tăng hiệu suất tổng thể, nhưng chúng ta quan sát thấy sự suy giảm. Điều này là do Transformer đầu tiên không thể phát hiện thứ tự đúng (do độ khó của nhiệm vụ này). Trong Hàng 4, chúng tôi cho các máy dịch cùng ăn với thông tin thứ tự chất lượng cao hơn (được tạo bởi FastAlign), và điểm BLEU tăng lên 31.82.

Chúng ta không thể sử dụng FastAlign ở thời điểm test nhưng thí nghiệm này cho thấy giả thuyết của chúng tôi về tính hữu ích của thông tin thứ tự có vẻ đúng. Được thúc đẩy bởi điều này, chúng tôi phát minh ra DPE để tận dụng tốt hơn thông tin thứ tự, và những kết quả này được báo cáo trong phần tiếp theo.

4.4 Thí nghiệm DPE
Kết quả liên quan đến DPE được báo cáo trong Bảng 3. Theo các điểm số được báo cáo, DPE dẫn đến cải thiện +0.81 trong điểm BLEU so với Transformer Base. Để đảm bảo rằng chúng tôi đánh giá DPE trong thiết lập công bằng, chúng tôi tái triển khai Transformer Base trong môi trường riêng của chúng tôi. Điều này loại bỏ tác động của các yếu tố khác nhau và đảm bảo rằng lợi ích là do thiết kế của chính module DPE. Chúng tôi cũng so sánh mô hình của chúng tôi với các mô hình được thảo luận trong tài liệu liên quan như của Shaw et al. (2018b), các embedding sắp xếp lại của Chen et al. (2019), và các embedding sắp xếp lại tường minh gần đây hơn của Chen et al. (2020). Mô hình của chúng tôi đạt điểm số tốt nhất và chúng tôi tin rằng đó là do việc sử dụng trực tiếp thông tin thứ tự.

Đối với kiến trúc DPE, chúng tôi quyết định có hai lớp (DP1 và DP2) vì nó tạo ra điểm BLEU tốt nhất trên các tập phát triển mà không áp đặt overhead huấn luyện đáng kể. Một siêu tham số quan trọng ảnh hưởng trực tiếp đến hiệu suất của DPE là trọng số loss vị trí (α). Chúng tôi chạy một nghiên cứu ablation trên tập phát triển để điều chỉnh α. Bảng 4 tóm tắt các phát hiện của chúng tôi. Giá trị α tốt nhất trong thiết lập của chúng tôi là 0.5. Giá trị này cung cấp sự cân bằng chấp nhận được giữa độ chính xác dịch và chi phí sắp xếp lại trước trong quá trình huấn luyện, và cho thấy thông tin thứ tự có thể quan trọng như thông tin dịch khác.

Thiết kế Transformer của chúng tôi (Transformer Base + DPE) có thể gây ra lo ngại rằng việc kết hợp thông tin sắp xếp lại trước hoặc định nghĩa một loss phụ trợ có thể không cần thiết. Người ta có thể đề xuất rằng nếu chúng ta sử dụng cùng lượng tài nguyên để tăng các tham số encoder của Transformer Base, chúng ta nên thu được kết quả cạnh tranh hoặc thậm chí tốt hơn so với Transformer được nâng cao bởi DPE. Để giải quyết mối lo ngại này, chúng tôi thiết kế một thí nghiệm khác tăng số lượng tham số/lớp trong encoder Transformer Base để phù hợp với số lượng trong các tham số mô hình của chúng tôi. Kết quả liên quan đến thí nghiệm này được hiển thị trong Bảng 5.

--- TRANG 6 ---
Mô hình | # Tham số | En→De (WMT)
Transformer Base (Vaswani et al., 2017) | 65.0 M | 27.30
+ Relative PE (Shaw et al., 2018b) | N/A | 26.80
+ Explicit Global Reordering Embeddings (Chen et al., 2020) | 66.5 M | 28.44
+ Reorder fusion-based source representation (Chen et al., 2020) | 66.5 M | 28.55
+ Reordering Embeddings (Encoder Only) (Chen et al., 2019) | 102.1 M | 28.03
+ Reordering Embeddings (Encoder/Decoder) (Chen et al., 2019) | 106.8 M | 28.22
Transformer Base (tái triển khai của chúng tôi) | 66.5 M | 27.78
Dynamic Position Encoding (α = 0.5) | 72.8 M | 28.59

Bảng 3: So sánh điểm BLEU của DPE với các đồng nghiệp khác.

Mô hình | WMT'14 En→De
Baseline | 27.78
DPE (α = 0.1) | 28.17
DPE (α = 0.3) | 28.16
DPE (α = 0.5) | 28.59
DPE (α = 0.7) | 27.98

Bảng 4: Điểm BLEU của DPE với các giá trị α khác nhau.

Mô hình | WMT'14 En→De
Baseline | 27.78
8E | 28.07
10E | 28.54
DPE (N = 2) | 28.59 (+0.81)

Bảng 5: So sánh điểm BLEU của DPE với các mô hình Transformer baseline cộng thêm các lớp encoder (8E cho 8 lớp encoder và 10E cho 10 lớp encoder)

So sánh DPE với các phần mở rộng khác nhau của Transformer Base, cụ thể là 8E (8 lớp encoder) và 10E (10 lớp encoder), chứng minh rằng sự tăng trong BLEU là do thông tin vị trí được cung cấp bởi DPE chứ không phải các tham số bổ sung của các lớp DPE. Trong 8E, chúng tôi cung cấp cùng số lượng tham số bổ sung như module DPE thêm vào, nhưng trải nghiệm ít lợi ích hơn trong chất lượng dịch. Trong 10E, chúng tôi thậm chí nhân đôi số lượng tham số bổ sung để vượt qua số lượng tham số mà DPE sử dụng, và tuy nhiên phần mở rộng DPE với 8 lớp mã hóa (hai cho sắp xếp lại trước và sáu từ encoder dịch gốc) vẫn vượt trội. Điều này củng cố ý tưởng rằng module DPE của chúng tôi cải thiện độ chính xác dịch bằng cách tiêm thông tin vị trí cùng với đầu vào encoder.

4.5 Kết quả Thực nghiệm trên Ngôn ngữ Khác
Ngoài các thí nghiệm được báo cáo trước đó, chúng tôi đánh giá mô hình DPE trên các tập dữ liệu IWSLT-14 khác nhau của tiếng Anh-Đức (En-De), tiếng Anh-Pháp (En-Fr), và tiếng Anh-Ý (En-It). Sau khi điều chỉnh với các trọng số loss vị trí khác nhau trên tập phát triển, chúng tôi xác định α = 0.3 là lý tưởng cho thiết lập này. Kết quả trong Bảng 6 cho thấy với DPE, độ chính xác dịch được cải thiện cho các thiết lập khác nhau và sự cải thiện không chỉ dành riêng cho cặp ngôn ngữ En-De WMT.

Kiến trúc DPE của chúng tôi hoạt động với nhiều cặp ngôn ngữ có kích thước khác nhau và điều này tăng sự tin tưởng của chúng tôi vào tác động có lợi của thông tin thứ tự. Thông thường khó để chỉ ra tác động của các tín hiệu phụ trợ trong các mô hình NMT và điều này có thể khó khăn hơn với các tập dữ liệu nhỏ hơn, nhưng kết quả IWSLT của chúng tôi đầy hứa hẹn. Theo đó, sẽ không bất công khi tuyên bố rằng DPE hữu ích bất kể ngôn ngữ và kích thước tập dữ liệu.

5 Kết luận và Công việc Tương lai
Trong bài báo này, trước tiên chúng tôi khám phá liệu Transformers có được hưởng lợi từ các tín hiệu thứ tự hay không. Sau đó, chúng tôi đề xuất một kiến trúc mới, DPE, tạo ra các embedding chứa thông tin vị trí từ đích để tăng chất lượng dịch.

Kết quả thu được trong các thí nghiệm của chúng tôi chứng minh rằng DPE cải thiện quá trình dịch bằng cách giúp phía nguồn học thông tin vị trí đích. Mô hình DPE liên tục vượt trội so với các baseline của tài liệu liên quan. Nó cũng cho thấy cải thiện với các cặp ngôn ngữ khác nhau và kích thước tập dữ liệu.

--- TRANG 7 ---
Mô hình | En→De | En→Fr | En→It
Transformer | 26.42 | 38.86 | 27.94
Phần mở rộng dựa trên DPE | 27.47 (+1.05) | 39.42 (+0.56) | 28.35 (+0.41)

Bảng 6: Kết quả BLEU cho các cặp ngôn ngữ IWSLT-14 khác nhau.

5.1 Công việc Tương lai
Các thí nghiệm của chúng tôi có thể cung cấp nền tảng cho việc khám phá thêm về mã hóa vị trí động trong Transformers. Đầu tiên, chúng tôi thừa nhận rằng có một số phần mở rộng cho công việc hiện tại của chúng tôi. Các quy tắc bổ sung có thể được thiết kế để xử lý các trường hợp khác nhau của liên kết từ được tạo bởi FastAlign. Ví dụ, các trường hợp như liên kết Nhiều-tới-Nhiều và biểu thức đa từ cũng thường được tìm thấy trong văn bản viết. Một phần mở rộng có thể khác sẽ là điều tra các công cụ liên kết chính xác hơn ngoài FastAlign. Tuy nhiên, điều quan trọng cần lưu ý là chúng tôi không đầu tư mạnh vào tiền xử lý ngôn ngữ học vì nó cần quá nhiều tài nguyên. Tiền xử lý cực kỳ chính xác có thể không cần thiết vì các mô hình thần kinh được mong đợi vẫn giải quyết vấn đề với quyền truy cập hạn chế vào thông tin miền. Khi xem xét các công cụ liên kết khác, chúng ta cũng phải xem xét hiệu quả và khả năng mở rộng của giải pháp.

Cuối cùng, chúng tôi dự định khám phá việc tiêm thông tin thứ tự vào các mô hình xử lý ngôn ngữ khác thông qua DPE hoặc cơ chế tương tự. Thông tin như vậy có vẻ hữu ích cho các nhiệm vụ như phân tích cú pháp phụ thuộc hoặc gắn thẻ chuỗi.

Tài liệu tham khảo
Arianna Bisazza và Marcello Federico. 2016. Surveys: A survey of word reordering in statistical machine translation: Computational models and language phenomena. Computational Linguistics, 42(2):163–205.

Mauro Cettolo, Christian Girardi, và Marcello Federico. 2012. WIT3: Web inventory of transcribed and translated talks. Trong Proceedings of the 16th Annual conference of the European Association for Machine Translation, trang 261–268, Trento, Italy. European Association for Machine Translation.

Kehai Chen, Rui Wang, Masao Utiyama, và Eiichiro Sumita. 2019. Neural machine translation with reordering embeddings. Trong Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, trang 1787–1799, Florence, Italy. Association for Computational Linguistics.

Kehai Chen, Rui Wang, Masao Utiyama, và Eiichiro Sumita. 2020. Explicit reordering for neural machine translation.

Yiming Cui, Shijin Wang, và Jianfeng Li. 2016. LSTM neural reordering feature for statistical machine translation. Trong Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 977–982, San Diego, California. Association for Computational Linguistics.

Jinhua Du và Andy Way. 2017. Pre-reordering for neural machine translation: helpful or harmful? Prague Bulletin of Mathematical Linguistics, (108):171–181.

Chris Dyer, Victor Chahuneau, và Noah A. Smith. 2013. A simple, fast, and effective reparameterization of IBM model 2. Trong Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 644–648, Atlanta, Georgia. Association for Computational Linguistics.

Minwei Feng, J. Peter, và H. Ney. 2013. Advancements in reordering models for statistical machine translation. Trong ACL.

Sarthak Garg, Stephan Peitz, Udhyakumar Nallasamy, và Matthias Paulik. 2019. Jointly learning to align and translate with transformer models.

Yuki Kawara, Chenhui Chu, và Yuki Arase. 2020. Preordering encoding on transformer for translation. IEEE/ACM Transactions on Audio, Speech, and Language Processing.

Guolin Ke, Di He, và Tie-Yan Liu. 2020. Rethinking positional encoding in language pre-training.

Philipp Koehn. 2009. Statistical machine translation. Cambridge University Press.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra Constantin, và Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. Trong Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, trang 177–180, Prague, Czech Republic. Association for Computational Linguistics.

--- TRANG 8 ---
Xuanqing Liu, Hsiang-Fu Yu, Inderjit Dhillon, và Cho-Jui Hsieh. 2020. Learning to encode position for transformer with continuous dynamical model.

Kishore Papineni, Salim Roukos, Todd Ward, và Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. Trong Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, trang 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

Rico Sennrich, Barry Haddow, và Alexandra Birch. 2016. Neural machine translation of rare words with subword units.

Peter Shaw, Jakob Uszkoreit, và Ashish Vaswani. 2018a. Self-attention with relative position representations. CoRR, abs/1803.02155.

Peter Shaw, Jakob Uszkoreit, và Ashish Vaswani. 2018b. Self-attention with relative position representations.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. Trong Advances in neural information processing systems, trang 5998–6008.

Chao Wang, Michael Collins, và Philipp Koehn. 2007. Chinese syntactic reordering for statistical machine translation. Trong Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), trang 737–745, Prague, Czech Republic. Association for Computational Linguistics.

Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, và Tieyan Liu. 2020. Incorporating bert into neural machine translation. Trong International Conference on Learning Representations.
