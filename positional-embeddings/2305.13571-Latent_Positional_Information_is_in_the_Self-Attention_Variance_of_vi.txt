# 2305.13571.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/positional-embeddings/2305.13571.pdf
# Kích thước tệp: 592593 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Thông Tin Vị Trí Tiềm Ẩn Nằm Trong Phương Sai Self-Attention Của
Các Mô Hình Ngôn Ngữ Transformer Không Có Positional Embeddings

Ta-Chung Chi†
Đại học Carnegie MellonTing-Han Fan
Đại học PrincetonLi-Wei Chen
Đại học Carnegie Mellon
Alexander I. Rudnicky
Đại học Carnegie MellonPeter J. Ramadge
Đại học Princeton

Tóm tắt
Việc sử dụng positional embeddings trong các mô hình ngôn ngữ transformer được chấp nhận rộng rãi. Tuy nhiên, nghiên cứu gần đây đã đặt câu hỏi về tính cần thiết của những embeddings này. Chúng tôi mở rộng thêm cuộc điều tra này bằng cách chứng minh rằng một mô hình ngôn ngữ transformer được khởi tạo ngẫu nhiên và đóng băng, không có positional embeddings, vốn dĩ mã hóa thông tin vị trí mạnh mẽ thông qua việc thu hẹp phương sai của self-attention. Để định lượng phương sai này, chúng tôi suy ra phân phối cơ bản của mỗi bước trong một lớp transformer. Thông qua xác thực thực nghiệm sử dụng một mô hình được pretrain đầy đủ, chúng tôi cho thấy hiệu ứng thu hẹp phương sai vẫn tồn tại sau các cập nhật gradient rộng rãi. Các phát hiện của chúng tôi phục vụ để biện minh cho quyết định loại bỏ positional embeddings và do đó tạo điều kiện thuận lợi cho việc pretrain hiệu quả hơn của các mô hình ngôn ngữ transformer.

1 Giới thiệu & Công trình Liên quan
Các mô hình Transformer đã trở thành xương sống của các ứng dụng xử lý ngôn ngữ tự nhiên (Vaswani et al., 2017; Devlin et al., 2019; Radford et al., 2019). Trong kiến trúc transformer, có hai loại chính: 1) các mô hình hai chiều, như BERT (Devlin et al., 2019), được huấn luyện sử dụng mục tiêu masked language modeling, và 2) các mô hình ngôn ngữ (nhân quả), như GPT (Radford et al., 2019), được huấn luyện sử dụng mục tiêu language modeling truyền thống. Cả hai loại này đều chia sẻ đặc điểm chung là sử dụng positional embeddings để mã hóa khoảng cách token.

Liệu positional embeddings có thực sự cần thiết đã là chủ đề của nghiên cứu đang diễn ra. Trong khi chúng được coi là cần thiết cho các mô hình transformer hai chiều (Lee et al., 2019; Luo et al., 2021; Sinha et al., 2021; Haviv et al., 2022), tình hình khác đối với các mô hình ngôn ngữ transformer (Irie et al., 2019; Yang et al., 2019; Tsai et al., 2019; Scao et al., 2022; Haviv et al., 2022). Trong các mô hình ngôn ngữ transformer, việc loại bỏ positional embeddings chỉ dẫn đến sự suy giảm hiệu suất nhỏ, trong khi cho phép huấn luyện hiệu quả hơn (Haviv et al., 2022). Ngoài bằng chứng thực nghiệm, đã được chứng minh (Bhattamishra et al., 2020) rằng các mô hình ngôn ngữ transformer không có positional embeddings là Turing-complete và có thể mô hình hóa các chuỗi tương tự như mạng nơ-ron hồi quy (Rumelhart và McClelland, 1987; Jordan, 1986). Mặc dù vậy, vẫn còn một câu hỏi mở về việc thông tin vị trí được lưu trữ ở đâu khi không có positional embeddings. Điều này thúc đẩy điều tra sâu hơn về các phép toán riêng lẻ trong một lớp transformer.

Ví dụ về kiến trúc của một mô hình ngôn ngữ transformer pre-LN (Xiong et al., 2020) đa lớp không có positional embeddings được sử dụng trong công trình này được hiển thị trong Hình 1. Chúng tôi sau đây gọi cấu hình này là TLM. Trọng tâm chính của chúng tôi là mô-đun multi-head attention (MHA) của một TLM được khởi tạo ngẫu nhiên, vì đây là mô-đun duy nhất cho phép trao đổi thông tin giữa các token. Để hiểu sâu hơn, chúng tôi tính toán giá trị trung bình và phương sai của các đầu ra MHA. Thật ngạc nhiên, chúng tôi phát hiện rằng phương sai đã mã hóa thông tin vị trí tiềm ẩn, với các token sau trong chuỗi hiển thị phương sai nhỏ hơn. Điều này thúc đẩy chúng tôi định lượng phương sai bằng cách suy ra phân phối đầu ra sau các phép toán MHA. Cuối cùng, thông qua xác thực thực nghiệm sử dụng một TLM được pretrain đầy đủ, chúng tôi xác nhận rằng cùng một hiệu ứng thu hẹp phương sai vẫn tồn tại sau các cập nhật gradient rộng rãi.

Theo hiểu biết tốt nhất của chúng tôi, chúng tôi là những người đầu tiên xác định và định lượng thông tin vị trí tiềm ẩn trong TLM. Kết quả của chúng tôi cung cấp những hiểu biết lý thuyết về việc loại bỏ positional embeddings, cho phép pretrain hiệu quả hơn của các TLM tương lai.

2 Thí nghiệm Khảo sát
Với BERT và TLM (GPT) có positional embeddings được loại bỏ, công trình trước đây (Haviv et al., 2022) cho thấy chỉ TLM có thể duy trì hiệu suất language modeling giống như phiên bản gốc có positional embeddings. Sự khác biệt có thể được giải thích bởi thực tế rằng chỉ TLM mã hóa thông tin vị trí trong các lớp của nó, như được thể hiện bởi thí nghiệm khảo sát vị trí trong Haviv et al. (2022). Vì cả BERT và TLM đều có quyền truy cập vào cùng một đầu vào ngữ nghĩa và sự khác biệt duy nhất là việc sử dụng causal attention masks trong TLM, chúng tôi đưa ra giả thuyết rằng thông tin vị trí có thể được quy cho sự tương tác giữa causal attention masks và kiến trúc TLM.

Để khám phá thêm giả thuyết này, chúng tôi sử dụng một TLM được khởi tạo ngẫu nhiên và đóng băng để loại bỏ bất kỳ ảnh hưởng ngữ nghĩa nào và chỉ tập trung vào thiết kế kiến trúc. Ngoài ra, để ngăn mô hình ghi nhớ thứ tự của các chuỗi đầu vào, chúng tôi không thực hiện tra cứu embedding và cung cấp cho mô hình các vector đầu vào được lấy mẫu ngẫu nhiên. Một bộ phân loại tuyến tính hai lớp có thể huấn luyện với kích hoạt ReLU ở giữa được nối thêm vào TLM để khảo sát vị trí của mỗi token (chi tiết thêm có thể tìm thấy trong Phụ lục B). Chúng tôi vẽ mean absolute error (MAE) theo số lượng lớp transformer trong Hình 2. Biểu đồ cho thấy một TLM được khởi tạo ngẫu nhiên và đóng băng với các vector đầu vào được lấy mẫu ngẫu nhiên vốn dĩ cung cấp thông tin vị trí, với việc tăng số lượng lớp dẫn đến hiệu suất khảo sát cao hơn. Kết quả đáng ngạc nhiên này thúc đẩy điều tra sâu hơn về việc mã hóa thông tin vị trí tiềm ẩn bên trong kiến trúc TLM.

3 Phân tích Lý thuyết
Chúng tôi phân tích hoạt động bên trong của một TLM bằng cách suy ra phân phối của các phép toán TLM với hy vọng rằng chúng làm sáng tỏ nơi thông tin vị trí tiềm ẩn được lưu trữ. Việc suy ra trở nên khả thi nhờ việc sử dụng một TLM được khởi tạo ngẫu nhiên và đóng băng. Chúng tôi áp dụng các cài đặt khởi tạo theo những cài đặt được sử dụng trong GPT (Radford et al., 2019). WLOG, việc suy ra của chúng tôi được giới hạn trong các phép toán của lớp đầu tiên trong một TLM và thành phần FFN được bỏ qua (được biện minh trong §3.4). Các siêu tham số được sử dụng trong các mô phỏng là: hidden dimension d = 768, số lượng attention heads H = 12, head dimension d/H = 64, sequence length L = 512, độ lệch chuẩn cho khởi tạo σ = 0.02. Tất cả các bằng chứng của các bổ đề được hoãn lại Phụ lục A.

Cho một chuỗi embeddings đầu vào được lấy mẫu ngẫu nhiên {xm}L m=1, trong đó mỗi phần tử của xm ∈ Rd được lấy mẫu i.i.d từ N(0, σ2), một TLM bao gồm các phép toán sau:

3.1 Layer Normalization
Đối với mỗi embedding đầu vào xm, nó tính toán sample mean và (biased) sample variance:

xm,: := (Σd i=1 xmi)/d, S(xm,:) = (Σd i=1 (xmi - xm,:)2)/d

Sau đó mỗi entry i của xm, được ký hiệu là xmi, được chuẩn hóa bằng mean và variance thành emi:

emi = (xmi - xm,:)/√S(xm,:) * γ + β
(*) ≈ (xmi - E[xmi])/√V[xmi] ~ N(0,1),

trong đó V[x] biểu thị phương sai của x. Vì sơ đồ khởi tạo đặt γ = 1 và β = 0, (*) đúng với d đủ lớn bởi Luật số lớn và định lý ánh xạ liên tục.

3.2 Self Attention
Mỗi attention head tính toán các vector query, key, và value trong Rd/H:

qm = Wq em, km = Wk em, vm = Wv em,

trong đó Wq, Wk, Wv ∈ Rd/H×d là các ma trận với mỗi phần tử được lấy mẫu i.i.d từ N(0, σ2).

Để chính xác, hầu hết các ma trận (W(h)q, W(h)k, W(h)v), vector (q(h)m, k(h)m, v(h)m), và vô hướng (l(h)mn, a(h)mn) được liên kết với số head h. Để đơn giản ký hiệu, chúng tôi chỉ hiển thị sự phụ thuộc vào h khi chúng tôi cần nó.

Bổ đề 1. qm, km, và vm có mean bằng không và ma trận covariance (dσ2)·I.

Các vector kết quả được xử lý bởi mô-đun self-attention cho pre-Softmax logits:

lmn = {
⟨qm, kn⟩, if m ≥ n
-inf, otherwise
}

tiếp theo là chuẩn hóa scaled softmax:

amn = exp(lmn/√(d/H)) / (ΣL i=1 exp(lmi/√(d/H)))

Bổ đề 2. lmn có mean bằng không và phương sai d3σ4/H2.
lmn/√(d/H) có phương sai d2σ4/H.

Phương sai số của lmn/√(d/H) trong trường hợp của chúng tôi là 7682 · 0.024/12 ≈ 0.0079. Bổ đề 2 gợi ý xấp xỉ sau:

Tính chất 1. Khi σ4 ≪ H/d2, lm,: có phương sai nhỏ, làm cho attention weights am,: gần như được phân phối đều giữa tất cả các vị trí.

Trong Hình 3, chúng tôi xác minh Tính chất 1 bằng cách cho thấy rằng amn gần như được phân phối đều trong mô phỏng.

Quan sát rằng output vector om tại vị trí m là:

om = Wo(⊕H h=1 ΣL n=1 a(h)mn v(h)n),

trong đó ⊕ biểu thị sự nối của các vector từ tất cả H attention heads. Giả sử rằng Tính chất 1 là hợp lệ và rằng Wo ∈ Rd×d có các phần tử i.i.d được lấy mẫu từ N(0, σ2), chúng tôi suy ra phân phối của om dưới đây.

Bổ đề 3. om có mean bằng không và ma trận covariance d2σ4/m I.

Hình 4 là một mô phỏng xác minh Bổ đề 3 dưới giả định của Tính chất 1. Chúng ta có thể thấy rằng phương sai của om đã mã hóa thông tin vị trí m.

3.3 Residual Connection
Như được ký hiệu bởi khối Addition của Hình 1, residual connection đặt đầu ra là ym = xm + om. Nó cho phép mô hình truyền đầu ra MHA đầu tiên đến các mô-đun MHA sau này cũng như bộ phân loại cuối cùng. Vì thông tin vị trí đã được truyền bởi residual connection, chúng tôi bỏ qua phần FFN trong phân tích của chúng tôi.

3.4 Layer Normalization Cuối cùng
Layer normalization là một phép toán có thể loại bỏ thông tin vị trí được suy ra trong Bổ đề 3, điều này xảy ra trước các mô-đun MHA và position classifier. Như đã đề cập trong §3.1, LN(ym) cho:

y'mi ≈ (ymi - E[ymi])/√V[ymi] ≈ (xmi + Wo Wv Σm n eni)/(√(σ2 + d2σ4/m)),

E[ymi] = 0, V[ymi] = V[xmi] + V[omi] = σ2 + d2σ4/m

Bổ đề 4. Phương sai của chiều thứ j của ym là:

(mσ2 + Σi(Wo,j: Wv,:i)2)/(mσ2 + d2σ4),

trong đó Wo,j: ∈ R1×d là hàng thứ j của Wo.
Wv,:i ∈ Rd×1 là cột thứ i của Wv. Miễn là Σi(Wo,j: Wv,:i)2 ≠ d2σ4, bộ phân loại sẽ có thể khai thác sự khác biệt để suy ra m.

Độc giả có thể thắc mắc tại sao Wo,j: và Wv,:i trong tử số không thể được coi là các biến ngẫu nhiên. Lý do là chúng tôi chỉ tập trung vào một chiều (thứ j) tại một thời điểm. Điều này có nghĩa là chúng tôi không thể sử dụng luật số lớn để xấp xỉ sample variance của ymj như chúng tôi đã làm đối với mẫu số.

3.5 Nới lỏng các Giả định
Chúng tôi thảo luận về khả năng nới lỏng các giả định được sử dụng trong §3.2.

Điều gì sẽ xảy ra nếu Tính chất 1 không đúng? Hoặc tương đương, σ4 ≪ H/d2. Điều này thúc đẩy chúng tôi thay đổi giá trị của σ. Trong Hình 5, chúng ta thấy rằng σ nhỏ hơn làm cho Bổ đề 3 phù hợp tốt hơn với các mô phỏng, điều này không đáng ngạc nhiên vì Bổ đề 3 giả định σ nhỏ. Ngay cả khi σ không quá nhỏ (tức là, σ = 0.2, 0.02), phương sai vẫn mã hóa thông tin vị trí vì phương sai của om có tương quan âm với vị trí m của nó.

Các Sơ đồ Khởi tạo Khác Cho đến nay chúng tôi giả định các ma trận trọng số (Wq, Wk, Wv, Wo) được khởi tạo i.i.d từ N(0, σ2). Tuy nhiên, chúng ta có thể nới lỏng giả định thành các mẫu i.i.d. từ một phân phối có mean bằng không và phương sai hữu hạn. Điều này là do bằng chứng trong Phụ lục A tính toán covariance. Việc tính toán phương sai dựa vào E[rir⊤i] = σ2I trong đó r⊤i là vector hàng thứ i của một ma trận trọng số.

4 Thảo luận
Tại sao các vị trí của các token sau trong một chuỗi khó được dự đoán hơn trong Hình 3 của Haviv et al. (2022)? Bổ đề 3 nêu rằng phương sai tỷ lệ nghịch với vị trí m, vì vậy phương sai của các token sau (m lớn) bằng phẳng, dẫn đến một bài toán tối ưu hóa số khó hơn. Điều này cũng gợi ý một nhược điểm tiềm ẩn của việc loại bỏ positional embeddings: Có thể thách thức cho mô hình suy ra thông tin vị trí của các token sau trong các chuỗi đầu vào cực dài.

Tại sao các lớp thấp hơn (gần với đầu vào) cho hiệu suất khảo sát tệ hơn trong cả Hình 2 và Haviv et al. (2022)? Điều này có thể được giải thích bởi Hình 4. Hầu hết các vị trí tại lớp thứ 0 có phương sai rất nhỏ (exp(-10) = 4.5e-5), điều này một lần nữa đặt ra một bài toán tối ưu hóa số khó.

Tại sao BERT không thể hội tụ mà không có positional embeddings? Trong một mô hình BERT (Devlin et al., 2019), mỗi token có quyền truy cập vào tất cả các token khác, làm cho phương sai tại tất cả các vị trí là d2σ4/L. Do đó, một mô hình BERT không thể sử dụng sự khác biệt phương sai làm chỉ báo vị trí của nó.

5 Kết quả Sau Huấn luyện
Các suy ra của chúng tôi chỉ áp dụng cho giai đoạn ban đầu khi TLM và input embeddings được khởi tạo ngẫu nhiên, điều này có thể không đúng sau các cập nhật gradient. Điều cần thiết là xác minh sự tồn tại của các tính chất phương sai và bổ đề trên một TLM được pretrain đầy đủ trên OpenWebText2 (chi tiết trong Phụ lục C).

Chúng tôi kỳ vọng rằng các tính chất của các lớp thấp hơn của một TLM được pretrain sẽ phù hợp chặt chẽ hơn với các kết quả lý thuyết vì hai lý do: 1) Có nhiều bước hơn giữa các lớp thấp hơn và loss language modeling cuối cùng, dẫn đến gradient nhỏ hơn và do đó ít cập nhật tham số hơn, và 2) Các lớp thấp hơn thường mã hóa thông tin cấp thấp hơn phụ thuộc vào thông tin vị trí (Vulić et al., 2020; de Vries et al., 2020).

Hình 6 và 7 chứng minh rằng lớp thứ 0 (thấp nhất) thể hiện xác suất attention tích lũy và phương sai decay-with-position rất tương tự như các kết quả lý thuyết. Ngược lại, các lớp cao hơn lệch khỏi các phân tích trong §3. Chúng tôi cho rằng mô hình học để dựa vào thông tin ngữ nghĩa hơn là thông tin vị trí. Điều này cũng giải thích tại sao dự đoán vị trí sử dụng đầu ra của các lớp transformer cao hơn là thách thức hơn như được chứng minh trong Hình 2 của Haviv et al. (2022).

6 Kết luận
Chúng tôi đã phân tích toán học một mô hình ngôn ngữ transformer được khởi tạo ngẫu nhiên không có positional embeddings. Chúng tôi cho thấy rằng phương sai của đầu ra self-attention giảm khi vị trí tăng, điều này phục vụ như một chỉ báo cho thông tin vị trí. Chúng tôi xác thực rằng, sau các cập nhật gradient rộng rãi, các lớp thấp của một mô hình ngôn ngữ được pretrain vẫn thể hiện hành vi giảm phương sai rất tương tự. Kết quả của chúng tôi mở đường cho việc pretrain các mô hình ngôn ngữ transformer hiệu quả hơn và không có positional embedding.

--- TRANG 6 ---
Hạn chế
Các hạn chế của công trình này chủ yếu đến từ các giả định của chúng tôi: 1) Một TLM được khởi tạo ngẫu nhiên và đóng băng, và 2) Các token đầu vào đều khác nhau và được lấy mẫu ngẫu nhiên. Hai giả định này rõ ràng không đúng đối với ngôn ngữ con người và các TLM được pretrain. Do đó, chúng tôi đã cố gắng xác minh thực nghiệm sự tồn tại của các bổ đề và tính chất trên một TLM được pretrain không có positional embeddings trong §5.

Điều đó được nói, một số phương pháp có thể được thử để loại bỏ những giả định này. Thứ nhất, chúng ta có thể phân tích động lực huấn luyện của một TLM để làm sáng tỏ phân phối tham số mô hình sau pretrain. Thứ hai, luật Zipf hoặc một mô hình ngôn ngữ n-gram đơn giản có thể được sử dụng để định lượng mức độ trùng lặp token đầu vào trong ngôn ngữ con người. Điều này có thể cho chúng ta một ước tính chính xác hơn về phương sai tại các vị trí khác nhau. Chúng tôi để lại những ý tưởng này như công việc tương lai.

Tuyên bố Đạo đức
Công trình của chúng tôi cung cấp hiểu biết sâu sắc hơn về lý do tại sao một mô hình ngôn ngữ transformer vẫn có thể hoạt động tốt mà không có positional embeddings, có thể cho phép ứng dụng phát triển các transformer tương lai xanh hơn và hiệu quả chi phí hơn. Việc sử dụng không đúng kỹ thuật của chúng tôi có thể có tác động tiêu cực đến xã hội. Những điều này bao gồm các thách thức đạo đức của việc tạo văn bản không đúng và các vấn đề riêng tư vốn có trong quá trình thu thập dữ liệu. Những hàm ý này áp dụng cho bất kỳ nghiên cứu xử lý ngôn ngữ tự nhiên nào và không độc đáo đối với công trình cụ thể này.

Lời cảm ơn
Các tác giả ghi nhận sự hỗ trợ từ Boeing (2019-STU-PA-259), Amazon (CC ADV 00474341 2021 TR), NSF MRI Award 1919452, và Princeton Research Computing.

Tài liệu tham khảo
Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Shivanshu Purohit, Tri Songz, Wang Phil, và Samuel Weinbach. 2021. GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch.

Satwik Bhattamishra, Arkil Patel, và Navin Goyal. 2020. On the computational power of transformers and its implications in sequence modeling. Trong Proceedings of the 24th Conference on Computational Natural Language Learning, trang 455–475, Online. Association for Computational Linguistics.

Wietse de Vries, Andreas van Cranenburgh, và Malvina Nissim. 2020. What's so special about BERT's layers? a closer look at the NLP pipeline in monolingual and multilingual models. Trong Findings of the Association for Computational Linguistics: EMNLP 2020, trang 4339–4350, Online. Association for Computational Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. Trong Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), trang 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, và Connor Leahy. 2020. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027.

Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, và Omer Levy. 2022. Transformer language models without positional encodings still learn positional information. arXiv preprint arXiv:2203.16634.

Kazuki Irie, Albert Zeyer, Ralf Schlüter, và Hermann Ney. 2019. Language modeling with deep transformers. Trong INTERSPEECH.

M I Jordan. 1986. Serial order: a parallel distributed processing approach. báo cáo kỹ thuật, tháng 6 năm 1985-tháng 3 năm 1986.

Diederik P. Kingma và Jimmy Ba. 2014. Adam: A method for stochastic optimization. Trích dẫn arxiv:1412.6980 Ghi chú: Được xuất bản như một bài báo hội nghị tại Hội nghị Quốc tế lần thứ 3 về Biểu diễn Học tập, San Diego, 2015.

Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, và Yee Whye Teh. 2019. Set transformer: A framework for attention-based permutation-invariant neural networks. Trong Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, trang 3744–3753. PMLR.

Ziyang Luo, Artur Kulmizev, và Xiaoxi Mao. 2021. Positional artefacts propagate through masked language model embeddings. Trong Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), trang 5312–5327, Online. Association for Computational Linguistics.

--- TRANG 7 ---
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, và Soumith Chintala. 2019. Pytorch: An imperative style, high-performance deep learning library. Trong Advances in Neural Information Processing Systems 32, trang 8024–8035. Curran Associates, Inc.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.

David E. Rumelhart và James L. McClelland. 1987. Learning Internal Representations by Error Propagation, trang 318–362.

Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M Saiful Bari, Stella Biderman, Hady Elsahar, Jason Phang, Ofir Press, Colin Raffel, Victor Sanh, Sheng Shen, Lintang Sutawika, Jaesung Tae, Zheng Xin Yong, Julien Launay, và Iz Beltagy. 2022. What language model to train if you have one million GPU hours? Trong Challenges & Perspectives in Creating Large Language Models.

Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, và Douwe Kiela. 2021. Masked language modeling and the distributional hypothesis: Order word matters pre-training for little. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, trang 2888–2913, Online và Punta Cana, Dominican Republic. Association for Computational Linguistics.

Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, và Ruslan Salakhutdinov. 2019. Transformer dissection: An unified understanding for transformer's attention via the lens of kernel. Trong Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), trang 4344–4353, Hong Kong, China. Association for Computational Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. Trong Advances in Neural Information Processing Systems, trang 5998–6008.

Ivan Vulić, Edoardo Maria Ponti, Robert Litschko, Goran Glavaš, và Anna Korhonen. 2020. Probing pretrained language models for lexical semantics. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), trang 7222–7240, Online. Association for Computational Linguistics.

Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, và Tie-Yan Liu. 2020. On layer normalization in the transformer architecture. Trong International Conference on Machine Learning.

Baosong Yang, Longyue Wang, Derek F. Wong, Lidia S. Chao, và Zhaopeng Tu. 2019. Assessing the ability of self-attention networks to learn word order. Trong Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, trang 3635–3644, Florence, Italy. Association for Computational Linguistics.

--- TRANG 8 ---
A Bằng chứng
Bằng chứng của Bổ đề 1 và 2 phụ thuộc vào head trong khi của Bổ đề 3 là độc lập với head. Để đơn giản ký hiệu, tại Bổ đề 1 và 2, chúng tôi bỏ sự phụ thuộc head trên các ma trận (W(h)q, W(h)k, W(h)v), vector (q(h)m, k(h)m, v(h)m), và vô hướng (l(h)mn, a(h)mn).

Bằng chứng của Bổ đề 1 Ở đây, chúng tôi sử dụng r⊤i để biểu thị vector hàng thứ i của Wv.

cov(vm, vn) = E[vmv⊤n]
= E[Wveme⊤nW⊤v]
= E[...]
= [...các bước tính toán...]
= (1m=ndσ2) · Id/H

(*) đúng vì ri và rj độc lập khi i ≠ j (tương tự cho em và en) và covariance của một vector ngẫu nhiên Gaussian là ma trận đơn vị. Id và Id/H biểu thị các ma trận đơn vị d×d và d/H×d/H.

Bằng chứng của Bổ đề 2 Ở đây, chúng tôi sử dụng r⊤i để biểu thị vector hàng thứ i của Wq và Wk.

cov(lmn, lmp)
= E[(e⊤mW⊤qWken)(e⊤mW⊤qWkep)⊤]
= [...các bước tính toán...]
= (1n=p)d3σ4/H2

(*) đúng vì:
E[WqW⊤q] = [...tính toán...] = d/H σ2 · I

Bằng chứng của Bổ đề 3 Vì Wo ∈ Rd×d được áp dụng trên một nối của các vector tại tất cả heads, chúng ta lấy vi = ⊕H h=1 v(h)i. vi ở đây là độc lập với head trong khi vi tại Bổ đề 1 là phụ thuộc head. Ở đây, chúng tôi sử dụng r⊤i để biểu thị vector hàng thứ i của Wo.

cov(om, om)
Tính chất 1≈ E[Wo Σm i=1 vi/m Σm j=1 v⊤j/m W⊤o]
= [...các bước tính toán...]
= d2σ4/m I

(*) theo từ Bổ đề 1: vì cov(v(h)i, v(h)j) = (1i=j dσ2) · Id/H, một nối cho tất cả h ∈ H cho E[viv⊤j] = (1i=j dσ2) · Id.

B Chi tiết Thí nghiệm Khảo sát
Chúng tôi huấn luyện một TLM được khởi tạo ngẫu nhiên và đóng băng với 12 lớp, d = 768, H = 12, L = 512, và σ = 0.02. Chúng tôi sử dụng bộ tối ưu Adam (Kingma và Ba, 2014) với learning rate 1e-3 và 5000 cập nhật gradient. Batch size được đặt thành 32. Chúng tôi triển khai mô hình của chúng tôi sử dụng PyTorch (Paszke et al., 2019).

--- TRANG 9 ---
# Lớp | Kích thước Ẩn | # Attention Heads | Độ dài Chuỗi Huấn luyện | # Tham số Có thể Huấn luyện
12 | 64 | 12 | 512 | 162M

Bộ tối ưu | Kích thước Batch | Bước Huấn luyện | Độ chính xác | Tập dữ liệu
Adam (lr 6e-4) | 32 | 50,000 | bfloat16 | OpenWebText2

Bảng 1: Cấu hình Mô hình Được Pretrain.

C Chi tiết Mô hình Ngôn ngữ Transformer Được Pretrain
Chúng tôi sử dụng thư viện gpt-neox (Andonian et al., 2021) để huấn luyện một TLM không có positional embeddings. Các siêu tham số chi tiết được liệt kê trong Bảng 1. Việc pretrain mất 5 giờ trên một NVIDIA A100-40GB.

D Hiện vật Khoa học
Chúng tôi sử dụng thư viện gpt-neox (Andonian et al., 2021) dưới giấy phép Apache-2.0. OpenWebText2 (Gao et al., 2020) được phát hành bởi các tác giả của gpt-neox. Codebase và tập dữ liệu được phát hành công khai cho mục đích nghiên cứu. Các bước được thực hiện để bảo vệ quyền riêng tư và ẩn danh được thảo luận trong Phần 6 và 7 của Gao et al. (2020). Phân phối và thống kê của OpenWebext2 cũng được thảo luận trong Gao et al. (2020).
