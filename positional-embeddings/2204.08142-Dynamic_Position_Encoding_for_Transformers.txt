# 2204.08142.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/positional-embeddings/2204.08142.pdf
# File size: 335112 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Dynamic Position Encoding for Transformers
Joyce Zheng
Huawei Noah’s Ark Lab
jy6zheng@uwaterloo.caMehdi Rezagholizadeh
Huawei Noah’s Ark Lab
mehdi.rezagholizadeh@huawei.com
Peyman Passbany
Amazon
passban.peyman@gmail.com
Abstract
Recurrent models have been dominating the
ﬁeld of neural machine translation (NMT) for
the past few years. Transformers (Vaswani
et al., 2017) have radically changed it by
proposing a novel architecture that relies on
a feed-forward backbone and self-attention
mechanism. Although Transformers are pow-
erful, they could fail to properly encode se-
quential/positional information due to their
non-recurrent nature. To solve this problem,
position embeddings are deﬁned exclusively
for each time step to enrich word informa-
tion. However, such embeddings are ﬁxed af-
ter training regardless of the task and word
ordering system of the source and target lan-
guages.
In this paper, we address this shortcoming by
proposing a novel architecture with new posi-
tion embeddings that take the order of the tar-
get words into consideration. Instead of us-
ing predeﬁned position embeddings, our solu-
tiongenerates new embeddings to reﬁne each
word’s position information. Since we do not
dictate the position of the source tokens and
we learn them in an end-to-end fashion, we
refer to our method as dynamic position en-
coding (DPE). We evaluated the impact of our
model on multiple datasets to translate from
English to German, French, and Italian and ob-
served meaningful improvements in compari-
son to the original Transformer.
1 Introduction
In statistical machine translation (SMT), the gen-
eral task of translation consists of reducing the
input sentence into smaller units (also known as
statistical phrases), selecting an optimal translation
for each unit, and placing them in the correct order
(Koehn, 2009). The last step, which is also referred
to as the reordering problem, is a great source of
Work done while Joyce Zheng was an intern at Huawei
yWork done while Peyman Passban was at Huawei.complexity and importance, which is handled with
a variety of statistical as well as string- and tree-
based solutions (Bisazza and Federico, 2016).
As evident in SMT, the structure and position
of words within a sentence is crucial for accurate
translation. The importance of such information
can also be explored in NMT and for Transform-
ers. Previous literature, such as Chen et al. (2020),
demonstrated that source input sentences enriched
with target order information have the capacity to
improve translation quality in neural models. They
showed that position encoding seems to play a key
role in translation and this motivated us to further
explore this area.
Since Transformers have a non-recurrent archi-
tecture, they could face problems when encoding
sequential data. As a result, they require an explicit
summation of the input embeddings with position
encoding to provide information about the order of
each word. However, this approach falsely assumes
that the correct position of each word is always its
original position in the source sentence. This inter-
pretation might be true when only considering the
source side, whereas we know from SMT (Bisazza
and Federico, 2016; Cui et al., 2016) that arranging
input words with respect to the order of their target
pairs can lead to better results.
In this work, we explore injecting target posi-
tion information alongside the source words to en-
hance Transformers’ translation quality. We ﬁrst
examine the accuracy and efﬁciency of a two-pass
Transformer (2PT), which consists of a pipeline
connecting two Transformers. The ﬁrst Trans-
former reorders the source sentence and the second
one translates the reordered sentences. Although
this approach incorporates order information from
the target language, it lacks end-to-end training
and requires more resources than a typical Trans-
former. Accordingly, we introduce a better alter-
native, which effectively learns the reordered posi-
tions in an end-to-end fashion and uses this infor-arXiv:2204.08142v2  [cs.CL]  22 Oct 2022

--- PAGE 2 ---
mation with the original source sequence to boost
translation accuracy. We refer to this alternative as
Dynamic Position Encoding (DPE).
Our contribution in this work is threefold:
•First, we demonstrate that providing source-
side representations with target position infor-
mation improves translation quality in Trans-
formers.
•We also propose a novel architecture, DPE,
that efﬁciently learns reordered positions in
an end-to-end fashion and incorporates this
information into the encoding process.
•Finally, we use a preliminary two-pass design
to show the importance of end-to-end learning
in this problem.
2 Background
2.1 Pre-Reordering in Machine Translation
In standard SMT, pre-reordering is a well-known
technique. Usually, the source sentence is re-
ordered using heuristics such that it follows the
word order of the target language. Figure 1 illus-
trates this concept with an imaginary example.
Figure 1: The order of source words before (left-hand
side) and after (right-hand side) pre-reordering. S iand
Tjshow thei-th source and j-th target words, accord-
ingly.
As the ﬁgure shows, the original alignment be-
tween the source ( Si) and target ( Ti) words are
used to deﬁne a new order for the source sentence.
With the new order, the translation engine does not
need to learn the relation between source and target
ordering systems, as it directly translates from one
position on the source side to the same position on
the target side. Clearly, this can signiﬁcantly reduce
the complexity of the translation task. The work
of Wang et al. (2007), provides a good example of
systems with pre-reordering, in which the authors
studied this technique for the English–Chinese pair.
The concept of re-ordering does not necessarily
need to be tackled prior to translation; in Koehn
et al. (2007), generated sequences are reviewed bya classiﬁer after translation to correct the position
of words that are placed in the wrong order. The
entire reordering process can also be embedded
into the decoding process (Feng et al., 2013).
2.2 Tackling the Order Problem in NMT
Pre-reordering and position encoding are also com-
mon in NMT and have been investigated by various
researchers. Du and Way (2017) explored if recur-
rent neural models can beneﬁt from pre-reordering.
Their ﬁndings showed that these models might not
require any order adjustments because the network
itself was powerful enough to learn such mappings.
Kawara et al. (2020), unlike the previous work,
studied the same problem and reported promising
observations on the usefulness of pre-reordering.
They used a transduction-grammar-based method
with recursive neural networks and showed how
impactful pre-reordering could be.
Liu et al. (2020) followed a different approach
and proposed modeling position encoding as a
continuous dynamical system through a neural
ODE. Ke et al. (2020) investigated improving po-
sitional encoding by untying the relationship be-
tween words and positions. They suggested that
there is no strong correlation between words and
absolute positions, so they removed this noisy cor-
relation. This form of separation has its own ad-
vantages, but by removing this relationship be-
tween words and positions from the translation
process, they might lose valuable semantic infor-
mation about the source and target sides.
Shaw et al. (2018a) explored a relative position
encoding method by having the self-attention mech-
anism consider the distance between source words.
Garg et al. (2019) incorporated target position in-
formation via multitasking where a translation loss
was combined with an alignment loss that super-
vised one decoder head to learn position informa-
tion. Chen et al. (2020) changed the Transformer ar-
chitecture to incorporate order information at each
layer. They selected a reordered target word posi-
tion from the output of each layer and injected it
into the next layer. This is the closest work to ours,
so we consider it as our main baseline.
3 Methodology
3.1 Two-Pass Translation for Pre-Reordering
Our goal is to boost translation quality in Trans-
formers by injecting target order information into
the source-side encoding process. We propose dy-

--- PAGE 3 ---
namic position encoding (DPE) to achieve this,
but before that, we discuss a preliminary two-pass
Transformer (2PT) architecture to demonstrate the
impact of order information in Transformers.
The main difference between 2PT and DPE is
that DPE is an end-to-end, differential solution
whereas 2PT is a pipeline that connects two differ-
entTransformers. They both work towards lever-
aging order information to improve translation but
in different fashions. The 2PT architecture is illus-
trated in Figure 2.
Figure 2: The two-pass Transformer architecture. The
input sequence is ﬁrst re-ordered to a new and less com-
plex form for the translation Transformer. Then, the
translation Transformer uses the re-ordered input se-
quence to decode a target sequence.
2PT has two different Transformers. The ﬁrst
one is used for reordering purposes instead of trans-
lation. It takes source sentences and generates a
reordered version of them, e.g. referring back to
Figure 1, if the input to the ﬁrst Transformer is
[S1,S2,S3,S4] the expected output from the ﬁrst
transformer is [ S1,S4,S3,S2]. We created a new
corpus using FastAlign to train this reordering
model (Dyer et al., 2013).1
FastAlign is an unsupervised word aligner
that processes source and target sentences together
and provides word-level alignments. It is usable
at training time but not for inference because it
requires access to both sides and we only have the
source side (at test time). As a solution, we used
the alignments to create a training set and utilized
it to train the ﬁrst Transformer in 2PT. Figure 3
shows the input and output format in FastAlign
and how it helps generate training samples.
As the ﬁgure demonstrates, given a pair of
1https://github.com/clab/fast_alignEnglish–German sentences, word alignments are
generated. In order to process the alignments, we
designed rules to handle different cases:
•One-to-Many Alignments : We only con-
sider the ﬁrst target position in one-to-many
alignments (see the ﬁgure).
•Many-to-One Alignments : Multiple source
words are reordered together (as one unit
while maintaining their relative positions with
each other) using the position of the corre-
sponding target word.
•No Alignment : Words that do not have any
alignments are skipped and we do not change
their position
•We also ensure that no source word would
be aligned with a position beyond the source
sentence length.
Considering these rules and what FastAlign
generates, the example input sentence “ mr hän@@
sch represented you on this occasion . ” (in Figure
3) is reordered to “ mr hän sch you this represented
.” @@ are auxiliary symbols added in between
sub-word units during preprocessing. See Section
4 for more information.
Using re-ordered sentences, the ﬁrst Transformer
in 2PT is trained to reorder the original source sen-
tences, and the second Transformer, which is re-
sponsible for translation, receives the re-ordered
source sentences and maps them to their target
translations. Despite different data formats and
different inputs/outputs, 2PT is still a pipeline that
translates a source language to a target one through
internal modiﬁcations that are hidden from the user.
3.2 Dynamic Position Encoding
Unlike 2PT, the dynamic position encoding (DPE)
method takes advantage of end-to-end training,
while the source side still learns target reordering
position information. It boosts the input of an or-
dinary Transformer’s encoder with target position
information, but leaves its architecture untouched,
as illustrated in Figure 4.
The input to DPE is a source word embedding
(wi) summed with sinusoidal position ( pi) encod-
ing (wipi). We refer to these embeddings as
enriched embeddings . Sinusoidal position encod-
ing is part of the original design of Transformers
and we assume the reader is familiar with this con-
cept. For more details see Vaswani et al. (2017).

--- PAGE 4 ---
Figure 3: FastAlign -based reordering using a sample sentence from our English–German dataset. Using word
alignments, we generate a new reordered form from each source sentence as the new target sequence. We then use
the pairs of source and new target sequences to train the ﬁrst Transformer of 2PT.
Figure 4: The left-hand side is the original Transformer’s architecture and the ﬁgure on the right is our proposed
architecture. E1 is the ﬁrst encoder layer of the ordinary Transformer and DP1 is the ﬁrst layer of the DPE network.
DPE is another neural network placed in-
between the enriched embeddings and the ﬁrst en-
coder layer of the (translation) Transformer. In
other words, the input to the DPE network is the
embedding table of the Transformer, and its ﬁnal
layer outputs into the ﬁrst encoder layer of the
Transformer. Thus, the DPE network can be trained
jointly with the Transformer using the original par-
allel sentence pairs.
DPE processes enriched embeddings and gener-
ates a new form of them that is represented as riin
this paper, i.e. DPE( wipi) =ri. DPE-generated
embeddings are intended to preserve target-side
order information about each word. In the origi-
nal Transformer, the position of wiis dictated by
addingpi, but the original position of this word is
not always the best one for translation; thus riis
deﬁned to address this problem. If wiappears in
thei-thposition but jis its best position with re-
spect to the target language, riis supposed to learninformation about the j-thposition and mimic pj.
Accordingly, the combination of piandrishould
providewiwith the pre-reordering information it
requires to improve translation accuracy.
In our design, DPE consists of two Transformer
layers. We determined this number through an
empirical study to ﬁnd a reasonable balance be-
tween translation quality and resource consump-
tion. These two layers are connected to an auxiliary
loss function to ensure that the output of DPE is
what we need for re-ordering.
This additional loss measures the mean squared
error between the embeddings produced by DPE
(ri) and the supervising positions ( PE) deﬁned by
FastAlign alignments. This learning process is
simply formulated in Equation 1:
Lorder =PjSj
i=1MSE (PEi;ri)
jSj(1)
whereSis the source sequence length and MSE ()

--- PAGE 5 ---
is the mean-square error function. The supervis-
ing position PEiis obtained by taking the target
position associated with withat was deﬁned by
FastAlign as described in Section 3.1.
To clarify howLorder works, we use the afore-
mentioned scenario as an example. We assume
that the correct position for wiaccording to the
FastAlign alignments is j, soPEi=pjand
we thus compute MSE (pj;ri). Through this tech-
nique, we encourage the DPE network to learn pre-
reordering in an end-to-end fashion and provide wi
with position reﬁnement information.
The total loss function when training the entire
model includes the auxiliary reordering loss func-
tionLorder summed with the standard Transformer
lossLtranslation , as in Equation 2:
Ltotal =L translation + (1 )L order (2)
whereis a hyper-parameter that represents the
weight of the reordering loss. was determined by
minimizing the total loss on the development set
during training.
4 Experimental Study
4.1 Dataset
To train and evaluate our models, we used the
IWSLT-14 collection (Cettolo et al., 2012) and
the WMT-14 dataset.2Our datasets are commonly
used in the ﬁeld, which makes our results easily
reproducible. Our code is also publicly available
to help other researchers further investigate this
topic.3The IWSLT-14 collection was used to study
the impact of our model for the English–German
(En–De), English–French (En–Fr), and English–
Italian (En–It) pairs. We also reported results on
the WMT dataset, which provides a larger training
corpus. We know that the quality of NMT mod-
els vary in proportion to the corpus size, so these
experiments provide more information to better
understand our model.
To prepare the data, sequences were lower-cased,
normalized, and tokenized using the scripts pro-
vided by the Moses toolkit4(Koehn et al., 2007)
and decomposed into sub-words via Byte-Pair En-
coding (BPE) (Sennrich et al., 2016). The vocab-
ulary sizes extracted for the IWSLT and WMT
2http://statmt.org/wmt14/
translation-task.html
3https://github.com/jy6zheng/
DynamicPositionEncodingModule
4https://github.com/moses-smt/
mosesdecoderdatasets were 32K and 40K, respectively. For the
En–De pair of WMT-14, newstest2013 was used as
a development set and newstest2014 was our test
set. For the IWSLT experiments, our test and devel-
opment sets were as suggested by Zhu et al. (2020).
Table 1 provides the statistics of our datasets.
Data Train Dev Test
WMT-14 (En!De) 4.45M 3k 3k
IWSLT-14 (En!De) 160k 7k 6k
IWSLT-14 (En!Fr) 168k 7k 4k
IWSLT-14 (En!It) 167k 7k 6k
Table 1: The statistics of the datasets used in our ex-
periments. Train, Dev, and Test stand for the training,
development, and test sets, respectively.
4.2 Experimental Setup
In the interest of fair comparisons, we used the
same setup as Chen et al. (2020) to build our base-
line for the WMT-14 En–De experiments. This
baseline setting was also used for our DPE model
and DPE-related experiments. Our models were
trained on 8V100 GPUs. Since our models
rely on the Transformer’s backbone, all hyper-
parameters that were related to the main Trans-
former architecture, such as embedding dimen-
sions, the number of attention heads, etc., were
set to the default values proposed for Transformer
Base in Vaswani et al. (2017). Refer to the original
work for detailed information.
For IWSLT experiments, we used a lighter archi-
tecture since the datasets were smaller than WMT.
The hidden dimension was 256for all encoder and
decoder layers, and a dimension of 1024 was used
for the inner feed-forward network layer. There
were 2encoder and 2decoder layers, and 2at-
tention heads. We found this setting through an
empirical study to maximize the performance of
our IWSLT models.
For the WMT-14 En–De experiments, similar to
Chen et al. (2020), we trained the model for 300K
updates and used a single model obtained from
averaging the last 5 checkpoints. The model was
validated with an interval of 2K on the development
dataset. The decoding beam size was 5. In the
IWSLT-14 cases, we trained the models for 15,000
updates and used a single model obtained from
averaging the last 5 checkpoints that were validated
with an interval of 1000 updates. We evaluated
all our models with detokenized BLEU (Papineni
et al., 2002).

--- PAGE 6 ---
Model Data type BLEU Score
1Reordering Transformer En!Enreordered 35.21
2Transformer Base En!De 27.76
3+ fed with the output of reordering Transformer Enreordered!De 21.96
4+ fed with the output of FastAlign Enreordered!De 31.82
Table 2: BLEU scores for the 2PT series of experiments.
4.3 2PT Experiments
Results related to the two-pass architecture are sum-
marized in Table 2. The reordering Transformer
(Row 1) works with the source sentences and re-
orders them with respect to the order of the target
language. This was a monolingual translation task
with a BLEU score of 35:21. This is a relatively
low score for a monolingual setting which indicates
how complicated the reordering problem is. Even
dedicating a whole Transformer could not fully
overcome the reordering problem. This ﬁnding
also indicates that NMT engines can beneﬁt from
using an auxiliary module to handle order com-
plexities. It is usually assumed that the translation
engine should perform in an end-to-end fashion
where it deals with all the reordering, translation,
and other complexities via a single model at the
same time. However, if we can separate these sub-
tasks systematically and tackle them individually,
there is a chance that we might be able to improve
the overall quality.
In Row 3, we used the information previously
generated (in Row 1) and showed how a translation
model performs when it is fed with reordered sen-
tences. The BLEU score for this task was 21:96,
which was signiﬁcantly lower than the baseline
(Row 2). Order information was supposed to in-
crease the overall performance, but we observe a
degradation. This is because the ﬁrst Transformer
was unable to detect the correct order (due to the
difﬁculty of this task). In Row 4, we fed the same
translation engines with higher-quality order in-
formation (generated by FastAlign ), and the
BLEU score rose to 31.82.
We cannot use FastAlign at test time but this
experiment shows that our hypothesis on the useful-
ness of order information seems to be correct. Mo-
tivated by this, we invented DPE to better leverage
order information, and these results are reported in
the next section.4.4 DPE Experiments
Results related to DPE are reported in Table 3.
According to the reported scores, DPE led to a
+0.81 improvement in the BLEU score compared
toTransformer Base . To ensure that we evaluated
DPE in a fair setup, we re-implemented the Trans-
former Base in our own environment. This elimi-
nated the impact of different factors and ensured
that the gain was due to the design of the DPE mod-
ule itself. We also compared our model to models
discussed in the related literature such as that of
Shaw et al. (2018b), the reordering embeddings of
Chen et al. (2019), and the more recent explicit
reordering embeddings of Chen et al. (2020). Our
model achieved the best score and we believe it
was due to the direct use of order information.
For the DPE architecture, we decided to have
two layers (DP1 and DP2) as it produced the best
BLEU scores on the development sets without im-
posing signiﬁcant training overhead. One impor-
tant hyper-parameter that directly affects DPE’s
performance is the position loss weight ( ). We ran
an ablation study on the development set to adjust
. Table 4 summarizes our ﬁndings. The best 
value in our setting was 0:5. This value provided
an acceptable balance between translation accuracy
and pre-reordering costs during training, and shows
that the order information can be as important as
other translation information.
The design of our Transformer (Transformer
Base + DPE) might raise the concern that incor-
porating pre-reordering information or deﬁning an
auxiliary loss might not be necessary. One might
suggest that if we use the same amount of resources
to increase the Transformer Base’s encoder param-
eters, we should obtain competitive or even better
results than the DPE-enhanced Transformer. To ad-
dress this concern, we designed another experiment
that increased the number of parameters/layers in
theTransformer Base encoder to match the number
in our model’s parameters. Results related to this
experiment are shown in Table 5.

--- PAGE 7 ---
Model # Params En!De (WMT)
Transformer Base (Vaswani et al., 2017) 65.0 M 27.30
+ Relative PE (Shaw et al., 2018b) N/A 26.80
+ Explicit Global Reordering Embeddings (Chen et al., 2020) 66.5 M 28.44
+ Reorder fusion-based source representation (Chen et al., 2020) 66.5 M 28.55
+ Reordering Embeddings (Encoder Only) (Chen et al., 2019) 102.1 M 28.03
+ Reordering Embeddings (Encoder/Decoder) (Chen et al., 2019) 106.8 M 28.22
Transformer Base (our re-implementation) 66.5 M 27.78
Dynamic Position Encoding ( = 0:5) 72.8 M 28.59
Table 3: A BLEU score comparison of DPE versus other peers.
Model WMT’14 En!De
Baseline 27.78
DPE (= 0:1)28.17
DPE (= 0:3)28.16
DPE (= 0:5)28.59
DPE (= 0:7)27.98
Table 4: BLEU scores of DPE with different values.
Model WMT’14 En!De
Baseline 27.78
8E 28.07
10E 28.54
DPE (N= 2)28.59 ( + 0.81 )
Table 5: A BLEU score comparison of DPE with the
baseline Transformer models plus additional encoder
layers (8E for 8 encoder layers and 10E for 10 encoder
layers)
The comparison of DPE with the different exten-
sions of the Transformer Base, namely 8E (8 en-
coder layers) and 10E (10 encoder layers), demon-
strated that the increase in BLEU was due to the
position information provided by DPE rather than
the additional parameters of the DPE layers. In 8E,
we provided the same number of additional param-
eters as the DPE module adds, but experienced less
gain in translation quality. In 10E, we even dou-
bled the number of additional parameters to surpass
the number of parameters that DPE uses, and yet
the DPE extension with 8 encoding layers (two for
pre-reordering and six from the original translation
encoder) was still superior. This reinforces the idea
that our DPE module improves translation accu-
racy by injecting position information alongside
the encoder input.4.5 Experimental Results on Other
Languages
In addition to the previously reported experiments,
we evaluated the DPE model on different IWSLT-
14 datasets of English–German (En–De), English–
French (En–Fr), and English–Italian (En–It). After
tuning with different position loss weights on the
development set, we determined = 0:3to be
ideal for this setting. The results in Table 6 show
that with DPE, the translation accuracy improved
for different settings and the improvement was not
unique to the En–De WMT language pair.
Our DPE architecture works with a variety of
language pairs of different sizes and this increases
our conﬁdence in the beneﬁcial effect of order in-
formation. It is usually hard to show the impact of
auxiliary signals in NMT models and this could be
more difﬁcult with smaller datasets, but our IWSLT
results are promising. Accordingly, it would not be
unfair to claim that DPE is useful regardless of the
language and dataset size.
5 Conclusion and Future Work
In this paper, we ﬁrst explored whether Transform-
ers would beneﬁt from order signals. Then, we
proposed a new architecture, DPE, that generates
embeddings containing target word position infor-
mation to boost translation quality.
The results obtained in our experiments demon-
strate that DPE improves the translation process by
helping the source side learn target position infor-
mation. The DPE model consistently outperformed
the baselines of related literature. It also showed
improvements with different language pairs and
dataset sizes.

--- PAGE 8 ---
Model En!De En!Fr En!It
Transformer 26.42 38.86 27.94
DPE-based Extension 27.47 ("1.05) 39.42 ("0.56) 28.35 ("0.41)
Table 6: BLEU results for different IWSLT-14 Language pairs.
5.1 Future Work
Our experiments can provide the groundwork for
further exploration of dynamic position encoding in
Transformers. First, we acknowledge that there are
some extensions to our current work. Additional
rules can be designed to handle different cases of
word alignments generated by FastAlign . For
example, cases such as Many-to-Many alignments
and multi-word expressions are also frequently
found in written text. Another possible extension
would be to investigate more precise alignment
tools in addition to FastAlign . However, it is
important to note that we did not heavily invest
in linguistic preprocessing because it requires too
many resources. Extremely precise preprocessing
might not be necessary as neural models are ex-
pected to still solve problems with limited access
to domain information. When considering other
alignment tools, we must also consider the efﬁ-
ciency and scalability of our solution.
Finally, we plan to explore injecting order in-
formation into other language processing models
through DPE or a similar mechanism. Such infor-
mation seems to be useful for tasks such as depen-
dency parsing or sequence tagging.
References
Arianna Bisazza and Marcello Federico. 2016. Sur-
veys: A survey of word reordering in statistical
machine translation: Computational models and
language phenomena. Computational Linguistics ,
42(2):163–205.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. WIT3: Web inventory of transcribed
and translated talks. In Proceedings of the 16th An-
nual conference of the European Association for Ma-
chine Translation , pages 261–268, Trento, Italy. Eu-
ropean Association for Machine Translation.
Kehai Chen, Rui Wang, Masao Utiyama, and Eiichiro
Sumita. 2019. Neural machine translation with re-
ordering embeddings. In Proceedings of the 57th
Annual Meeting of the Association for Computa-
tional Linguistics , pages 1787–1799, Florence, Italy.
Association for Computational Linguistics.Kehai Chen, Rui Wang, Masao Utiyama, and Eiichiro
Sumita. 2020. Explicit reordering for neural ma-
chine translation.
Yiming Cui, Shijin Wang, and Jianfeng Li. 2016.
LSTM neural reordering feature for statistical ma-
chine translation. In Proceedings of the 2016 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies , pages 977–982, San Diego,
California. Association for Computational Linguis-
tics.
Jinhua Du and Andy Way. 2017. Pre-reordering
for neural machine translation: helpful or harm-
ful? Prague Bulletin of Mathematical Linguistics ,
(108):171–181.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameter-
ization of IBM model 2. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies , pages 644–648, At-
lanta, Georgia. Association for Computational Lin-
guistics.
Minwei Feng, J. Peter, and H. Ney. 2013. Advance-
ments in reordering models for statistical machine
translation. In ACL.
Sarthak Garg, Stephan Peitz, Udhyakumar Nallasamy,
and Matthias Paulik. 2019. Jointly learning to align
and translate with transformer models.
Yuki Kawara, Chenhui Chu, and Yuki Arase. 2020.
Preordering encoding on transformer for translation.
IEEE/ACM Transactions on Audio, Speech, and Lan-
guage Processing .
Guolin Ke, Di He, and Tie-Yan Liu. 2020. Rethinking
positional encoding in language pre-training.
Philipp Koehn. 2009. Statistical machine translation .
Cambridge University Press.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond ˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Ses-
sions , pages 177–180, Prague, Czech Republic. As-
sociation for Computational Linguistics.

--- PAGE 9 ---
Xuanqing Liu, Hsiang-Fu Yu, Inderjit Dhillon, and
Cho-Jui Hsieh. 2020. Learning to encode position
for transformer with continuous dynamical model.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics , pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units.
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
2018a. Self-attention with relative position represen-
tations. CoRR , abs/1803.02155.
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
2018b. Self-attention with relative position repre-
sentations.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information pro-
cessing systems , pages 5998–6008.
Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL) , pages 737–745, Prague,
Czech Republic. Association for Computational Lin-
guistics.
Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,
Wengang Zhou, Houqiang Li, and Tieyan Liu. 2020.
Incorporating bert into neural machine translation.
InInternational Conference on Learning Represen-
tations .
