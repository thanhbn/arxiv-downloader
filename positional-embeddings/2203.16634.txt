# 2203.16634.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/positional-embeddings/2203.16634.pdf
# File size: 310805 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Transformer Language Models without Positional Encodings
Still Learn Positional Information
Adi HavivOri RamOﬁr Press!Peter IzsakOmer Levy
Tel Aviv University!University of WashingtonIntel LabsMeta AI
{adi.haviv, ori.ram, levyomer}@cs.tau.ac.il ,ofirp@cs.washington.edu, peter.izsak@intel.com
Abstract
Causal transformer language models (LMs),
such as GPT-3, typically require some form of
positional encoding, such as positional embed-
dings. However, we show that LMs without
any explicit positional encoding are still com-
petitive with standard models, and that this
phenomenon is robust across different datasets,
model sizes, and sequence lengths. Probing ex-
periments reveal that such models acquire an
implicit notion of absolute positions through-
out the network, effectively compensating for
the missing information. We conjecture that
causal attention enables the model to infer the
number of predecessors that each token can at-
tend to, thereby approximating its absolute po-
sition. Our ﬁndings indicate that causal LMs
might derive positional awareness not only
from the explicit positioning mechanism, but
also from the effects of the causal mask.
1 Introduction
The attention mechanism (Bahdanau et al., 2015)
of the transformer (Vaswani et al., 2017) is agnos-
tic to the position and order of tokens in the input
sequence. It is therefore common practice to in-
ject positional information via absolute positional
embeddings (Vaswani et al., 2017; Radford et al.,
2018) or relative bias factors (Shaw et al., 2018;
Raffel et al., 2020; Press et al., 2022). Here, we
demonstrate that transformer language models with-
outany explicit positional information can and do
learn an implicit notion of absolute positions that
is sufﬁcient to achieve competitive performance.
We compare the performance of language mod-
els trained with no explicit positional informa-
tion ( NoPos language models) to those trained
with three different position-aware mechanisms,
namely: sinusoidal embeddings (Vaswani et al.,
2017), learned embeddings (Gehring et al., 2017),
and ALiBi (Press et al., 2022). Results show that
NoPos models are competitive with position-aware
NoPos Learned Sinusoidal ALiBi1011121314Perplexity13.10 13.0512.93
12.51Figure 1: Transformer language models trained with-
out explicitly encoding positional information ( NoPos )
approach the performance of models trained with var-
ious positional encoding methods. All models have
1.3B parameters, and are trained on an excerpt of the
Pile.
models consistently across datasets, model sizes,
and input sequence lengths (e.g., Figure 1).
To shed light on our ﬁndings, we probe into
the position-awareness of NoPos language models,
compared to models that use relative orabsolute
position mechanisms. Speciﬁcally, we train classi-
ﬁers to predict the position of a token given its rep-
resentation across different layers in the network.
Our probes reveal that the NoPos model achieves
similar mean absolute distance between the pre-
dicted and the expected positions, as a model with
learned absolute position embeddings.
We hypothesize that this surprising behavior is
tied to the causal attention mask, which implicitly
injects positional information into the self-attention
layer in order to preserve the autoregressive nature
of language models. Intuitively, a model that is
able to count the predecessors of a given tokenarXiv:2203.16634v2  [cs.CL]  5 Dec 2022

--- PAGE 2 ---
can essentially infer its absolute position. To test
our hypothesis, we run similar experiments for
masked language models (MLM) (Devlin et al.,
2019), which use order-invariant attention (since
no causal mask is applied). Indeed, bidirectional
models fail to converge when position information
is absent, substantiating our hypothesis. To con-
clude, our main contributions are:
•We demonstrate the robustness of the NoPos
model (compared to position-aware models)
with respect to model size, dataset and se-
quence length.
•We provide an analysis of the trained NoPos
model, and show that it encoded absolute po-
sitions.
•We show that the success of NoPos models is
unique to causal language models.
2 Positional Encodings
Transformer models consist of interleaved self-
attention and feed-forward layers, which are both
order-invariant. Therefore, to convey the order of
the input tokens, some form of positional informa-
tion is explicitly introduced into the model. Abso-
lute positions are commonly encoded as vectors
(one for each position), which are then added to
the input tokens’ embeddings and fed to the ﬁrst
layer of the transformer. Relative positions are typ-
ically encoded as biases (added to attention scores)
within the self-attention layers. In this work, we
consider three popular methods as baselines:
Learned. Embeddings trained to represent abso-
lute positions (Sukhbaatar et al., 2015; Gehring
et al., 2017). Learned positional embeddings are
commonly used in MLMs (Devlin et al., 2019; Liu
et al., 2019) as well as in large autoregressive lan-
guage models, such as GPT-3 (Brown et al., 2020).
Sinusoidal. Constant vectors computed by a non-
parametric function of the input token’s absolute
position. Sine and cosine functions of different
frequencies are used, such that each dimension
of the positional encoding corresponds to a sinu-
soid. Sinusoidal embeddings were introduced in
Vaswani et al. (2017) for machine translation, and
are also used in language modeling (Baevski and
Auli, 2019).
ALiBi. Attention with LInear BIases (Press et al.,
2022) injects information about the relative dis-
tances between tokens by adding negative biasesto attention scores, which grow linearly with the
distance between each pair of tokens.
3 Experiment Setup
Intuitively, encoding positional information explic-
itly is crucial for enabling transformer language
models to predict the next token in a sequence. To
test this intuition, we compared the validation set
perplexity of models trained from scratch with no
explicit positional information (denoted as NoPos )
to those trained with the various positional encod-
ing methods discussed in Section 2. We investi-
gated the canonical WikiText-103 setting (Merity
et al., 2017; Baevski and Auli, 2019), as well as a
newer, large-scale setting based on the Pile corpus
(Gao et al., 2020) on model architectures inspired
by Brown et al. (2020), where we cover a spectrum
of models sizes and sequence lengths.
The Canonical Setting (WikiText-103). The
WikiText-103 corpus (Merity et al., 2017) consists
of over 100 million words extracted from a set
of high-quality Wikipedia articles. The corpus is
tokenized at the word level, resulting in a vocab-
ulary of over 267K tokens. For this corpus, we
used the adaptive embedding transformer model of
Baevski and Auli (2019), which contains 16 trans-
former layers with 1024 model dimensions, 4096
feed-forward dimensions, and 8 attention heads.
Overall, this model has 247M parameters in total.
We trained with their exact optimization hyperpa-
rameters, as implemented in fairseq (Ott et al.,
2019), with the exception of the input sequence
length, which was shortened to 512 tokens (instead
of 3072), as in Press et al. (2022). See App. C for
detailed hyperparameters.
The Large-Scale Setting (The Pile). The Pile
(Gao et al., 2020) is an 800GB English text dataset
composed of Common Crawl and 22 other diverse
sources. For our experiments, we used 2 out of
30 shards;1of these, we ﬁltered out the GitHub
and DM Mathematics sources and removed the
shortest 1% and longest 1% of examples from each
source to reduce noise. We used GPT-2’s tokenizer
(Radford et al., 2019) to convert the text into token
sequences over a vocabulary of 50K tokens. We
randomly sampled a validation set of 2000 doc-
uments (2.6M tokens) from the corpus, while the
remaining 15M documents (21B tokens) comprised
1Shards 00 and 01 can be downloaded from: https://
the-eye.eu/public/AI/pile/train/

--- PAGE 3 ---
WikiText-103 The Pile
NoPos 20.97 13.10
Learned 20.42 13.05
Sinusoidal 20.16 12.93
ALiBi 19.71 12.51
Table 1: Validation set perplexity of transformer lan-
guage models trained with various positional encod-
ing methods. The WikiText-103 setting (Merity et al.,
2017) uses the model of Baevski and Auli (2019) on
sequences of 512 tokens, while the Pile settings (Gao
et al., 2020) uses a more recent 1.3B parameter archi-
tecture (Brown et al., 2020) over 1024 token sequences.
the training set. The baseline model in this setting
follows the 1.3B parameter architecture of Brown
et al. (2020), also known as GPT-3 XL: 24 trans-
former layers with 2048 model dimensions, 8192
feed-forward dimensions, and 32 attention heads.
The default input sequence length is 1024 tokens.
We refer to App.C for detailed hyperparameters.
To demonstrate the consistency of our results
in different settings, we perform two scaling ex-
periments. We ﬁrst scale the model size by ex-
perimenting with the small (125M parameters),
medium (350M parameters), large (760M parame-
ters) and the XL (1.3B parameters) variants of the
Brown et al. (2020) architecture on the Pile set-
tings. In addition, we evaluate the effect of varying
the sequence length using the XL (1.3B parameter)
model. Speciﬁcally, we experiment with sequences
of lengthsf256;512;1024;2048g.
Last, to shed additional light on differences be-
tween the NoPos model to other methods, we com-
pare the model’s performance on different parts of
the sequence. Details of this analysis and results
are given in App. A.
4 Results
Table 1 compares the performance of training LMs
with different position encoding methods. We ob-
serve that NoPos LMs approach the performance of
the other models, with gaps of 0.55 (WikiText-103)
and 0.05 (the Pile) perplexity from models with
learned positional embeddings. In the Pile setting,
performance differences between NoPos ,Learned ,
andSinusoidal are small both in absolute terms
and with respect to their difference with ALiBi . In
the WikiText-103 setting, performance gaps are
wider but still modest with respect to random seedvariance.2These results strongly suggest that train-
ing transformer language models without explicit
positional encoding is indeed possible.
Table 2 explores the effects of scaling the num-
ber of parameters in the Pile setting. While smaller
models beneﬁt from ﬁxed, non-parametric posi-
tional encodings ( Sinusoidal andALiBi ), these per-
formance gaps narrow in larger models. Table 3
shows the effect of varying the sequence length
in the same setting. In this experiment, the gaps
between NoPos ,Learned , and Sinusoidal remain
almost constant, while the beneﬁt of using ALiBi
increases as sequences become longer. Overall, we
show that transformer language modeling without
explicit positional encoding is robust to the selec-
tion of corpus, model size, and sequence length.
As training models at the 1.3B parameter scale is
resource-intensive, we publicly release our trained
models for future research and analysis.3
Model Size 125M 350M 760M 1.3B
NoPos 22.15 16.87 14.29 13.10
Learned 22.04 16.84 14.21 13.05
Sinusoidal 21.49 16.58 14.04 12.93
ALiBi 19.94 15.66 13.53 12.51
Table 2: Validation set perplexity on the Pile, as a func-
tion of positional encoding method and model size. All
models operate on sequences of 1024 tokens. Smaller
models beneﬁt from ﬁxed, non-parametric positional
encodings ( Sinusoidal and ALiBi ), but these perfor-
mance gaps diminish as the models scale up.
Seq Length 256 512 1024 2048
NoPos 14.98 13.82 13.10 12.87
Learned 14.94 13.77 13.05 12.72
Sinusoidal 14.84 13.66 12.93 12.62
ALiBi 14.65 13.37 12.51 12.06
Table 3: Validation set perplexity on the Pile, as a
function of positional encoding method and sequence
length. All models have 1.3B parameters. The perfor-
mance differences between NoPos ,Learned , and Sinu-
soidal are consistently small, while ALiBi slowly be-
comes more beneﬁcial as sequences become longer.
In a Concurrent work, Scao et al. (2022) makes
a similar observation in one of their ablation exper-
iments and further show that NoPos models gain
2For context, Press et al. (2020) report that training the
sinusoidal model with inputs of length 3072 on WikiText-103
with 5 different seeds can result in gaps of up to 0.9 perplexity
between runs (0.34 standard deviation).
3https://github.com/adihaviv/NoPos

--- PAGE 4 ---
0 4 8 12 16 20 24
Layer050100150200250300350Mean Absolute Distance
NoPos
Learned
Sinusoidal
ALiBi
RandomFigure 2: Through probing, we ﬁnd that the NoPos
model behaves similarly to models that use absolute
learned position embeddings. We evaluated perfor-
mance using mean absolute distance on 1.3B parameter
models trained on the Pile.
competitive performances for downstream tasks as
well. Speciﬁcally, they evaluated 27 diverse down-
stream tasks. They showed that the NoPos model
reached an average accuracy of 41:23% over all
tasks, comparing to Learned andALiBi who gained
41:72% and43:70% respectively.
5 Analysis
In this section, we examine whether the NoPos
model is able to encode positional information and
show that such information is essential for its suc-
cess.
NoPos models acquire positional information
Do NoPos LMs learn some form of positional en-
coding to compensate for the absence of explicit
positional modeling? To answer this question, we
probe each layer of our trained models4for posi-
tional information. Speciﬁcally, we use the tokens’
last hidden representation after each transformer
layer, produced by the evaluated LM, and train a
2-layer feed-forward ReLU network to predict the
absolute position (0 to 1023) of each token (i.e., as
a multiclass classiﬁcation problem). Notably, we
do not change the weights of the evaluated LMs
and thus, do not provide any position information
4We used the 1.3B parameter models trained over 1024-
token sequences of the Pile (Section 3).of the tokens to the LM in this experiment, which
ensures the validity of our ﬁndings.
Each layer’s probe was trained separately (hy-
perparameters are provided in App. C). As a soft
accuracy metric, we measured the mean absolute
distance between the probe’s prediction and the
token’s actual position.
Figure 2 shows that even though NoPos model
starts, as expected, with no positional information
in the ﬁrst layer (on par with a random baseline),
it becomes position-aware within four layers and
appears to contain more positional information than
ALiBi. By the middle layer, NoPos can predict
absolute positions about as well as the model with
learned positional embeddings. Finally, we observe
that all models shed off a signiﬁcant amount of
positional information in the ﬁnal layers, in line
with the ﬁndings of V oita et al. (2019). Overall,
the probe reveals that the NoPos models learn an
implicit notion of absolute positions.
To elucidate what positional information the No-
Pos model learns, we visualize the predictions of
the probe. We examine a sample of 100 predictions
from the validation set of the best-performing probe
trained over the NoPos model. Figure 3 shows the
predictions over the 512 token sequences sampled
randomly from the validation set and a single exam-
ple from the same set. We observe that the probe
is more accurate at the beginning of the sequence,
but becomes fuzzier as it progresses.
Positional information matters NoPos is able
to infer absolute positions, but are they necessary?
We answer this using a trained NoPos model. In-
stead of computing the loss over the entire se-
quence, we select a single random token, shufﬂe
the previous tokens that it is conditioned on, and
compare to a baseline where the preﬁx remains
intact. We ﬁnd that in the case where the sufﬁx
is shufﬂed, the average token-level loss increases
dramatically (from 4 to11). Details of this
experiment are given in App. B.
This ﬁnding indicates that the NoPos model in-
deed uses the positional information it acquires, as
otherwise we would expect similar loss values in
these two settings.
6 Conjecture
How do transformers without explicit positional
encoding learn absolute positions? We conjecture
that the causal attention in autoregressive trans-
former language models allows them to predict the

--- PAGE 5 ---
0 64 128 192 256 320 384 448 512
Target Position064128192256320384448512Predicted Position
NoPos Probe Predictions
(mean and conf. interval)
NoPos Probe
Single Example Predictions
Ground TruthFigure 3: A visualization of the absolute position pre-
dictions of a probe trained over a NoPos language
model. The blue line shows the mean of the gener-
ated predictions for every target position and the blue
area represents the 95%-conﬁdence interval. The pre-
dictions for a single random sequence are depicted as
green dots.
number of attendable tokens at each position, i.e.
the number of tokens in the sequence that precede
the current one. Such a mechanism could effec-
tively encode the absolute position of each token
into its vector representation. Indeed, our analysis
(Section 5) reveals that some notion of absolute
positions exists in the hidden layers of language
models even when they are trained without explicit
positional encoding, and that this information is ac-
quired throughout the ﬁrst few layers. On the other
hand, bidirectional transformer encoders (which
are used in masked language modeling, e.g. Devlin
et al. 2019) do not contain causal attention masks
or any other limitation on the attention mechanism;
thus, they should be unable to learn absolute po-
sitions without explicit positional encoding. We
tested this corollary by training a masked language
model based on RoBERTa large (Liu et al., 2019)
on the Pile (see App. C for hyperparameters). Ta-
ble 4 shows that, indeed, the NoPos model has
signiﬁcantly worse perplexities than the position-
informed baselines. This result echoes the ﬁnd-
ings of Sinha et al. (2021), who also observed that
MLMs without positional embeddings suffer sig-
niﬁcant performance degradation.
7 Related Work
While there has been ample research on positional
encoding variants, there has been relatively littleMLM Perplexity
NoPos 147.18
Learned 4.06
Sinusoidal 4.07
ALiBi 4.00
Table 4: Validation set perplexity of masked language
models (Devlin et al., 2019) trained with various po-
sitional encoding methods on an excerpt of the Pile
(Gao et al., 2020). The model architecture is based
on RoBERTa large (Liu et al., 2019), and processes
128 tokens per sequence. While position-aware mod-
els converge to very low perplexities, training without
positional encodings ( NoPos ) fails.
prior work that investigate models’ ability to infer
positions implicitly. Prior to our work, Irie et al.
(2019) explored transformer language models for
speech recognition and found that such models,
when trained without positional encoding, outper-
form those trained with sinusoidal embeddings. In
addition, a focused language modeling experiment
by Stella Rose Biderman5showed that the NoPos
method attains similar results to other position em-
bedding methods; however, that experiment was on
a small 350M parameter model trained on a small
character-level dataset (enwik8). Here we show
that this result holds across multiple datasets and
model sizes, provide an analysis of the model’s
internal representations, and hypothesize how this
phenomenon could occur.
8 Conclusion
We show that, contrary to popular belief, transform-
ers language models do learn positional informa-
tion even when are not provided with any explicit
positional encoding. Our experiments systemati-
cally demonstrate that this phenomenon is robust
across different language modeling settings, and
that one can approximate the absolute position of
each token from the model’s internal representa-
tions to a surprising degree. However, this phe-
nomenon does not extend to transformer encoders
trained on the MLM objective. We conjecture that
the causal attention mechanism, which limits atten-
tion in one direction of the sequence, is responsible
for implicitly imbuing the transformer with posi-
tional information.
5https://twitter.com/BlancheMinerva/status/
1394089508723900422

--- PAGE 6 ---
9 Limitations
Our work explores language models in the 125M to
1.3B parameter range. We show that as parameter
count increases the gap between the NoPos method
and the other position methods narrows. This trend
leads us to believe that our ﬁndings should hold for
even larger models, but the current biggest models
are more than one hundred times bigger (in terms
of parameters) than our 1.3B parameter models,
and so the results in that setting can be unexpected.
In addition, training models at the 1.3B parameter
scale is resource-intensive and might hinder repro-
ducibility. We therefore release our trained models.
In Addition, when comparing the perplexity of No-
Pos to other models, although the margins are very
small, NoPos is always slightly worse, suggesting
that the inductive bias of positional encoding is
indeed important.
Acknowledgements
This work was supported by Intel Corporation,
Meta Platforms Inc and Deutsch Foundation.
References
Alexei Baevski and Michael Auli. 2019. Adaptive in-
put representations for neural language modeling. In
7th International Conference on Learning Represen-
tations, ICLR 2019, New Orleans, LA, USA, May
6-9, 2019 . OpenReview.net.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2015. Neural machine translation by
jointly learning to align and translate. CoRR ,
abs/1409.0473.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Amanda Askell, Sandhini Agarwal, Ariel Herbert-
V oss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,
Clemens Winter, Chris Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In
Advances in Neural Information Processing Systems ,
volume 33, pages 1877–1901. Curran Associates,
Inc.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human LanguageTechnologies, Volume 1 (Long and Short Papers) ,
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Leo Gao, Stella Biderman, Sid Black, Laurence Gold-
ing, Travis Hoppe, Charles Foster, Jason Phang,
Horace He, Anish Thite, Noa Nabeshima, Shawn
Presser, and Connor Leahy. 2020. The Pile: An
800gb dataset of diverse text for language modeling.
arXiv preprint arXiv:2101.00027 .
Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, and Yann Dauphin. 2017. Convolutional se-
quence to sequence learning. In ICML .
Kazuki Irie, Albert Zeyer, Ralf Schlüter, and Hermann
Ney. 2019. Language modeling with deep trans-
formers. In INTERSPEECH .
Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings .
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized BERT pretraining ap-
proach. CoRR , abs/1907.11692.
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2017. Pointer sentinel mixture mod-
els. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April
24-26, 2017, Conference Track Proceedings . Open-
Review.net.
Yurii Nesterov. 1983. A method for unconstrained con-
vex minimization problem with the rate of conver-
gence o(1=k2).
Myle Ott, Sergey Edunov, Alexei Baevski, Angela
Fan, Sam Gross, Nathan Ng, David Grangier, and
Michael Auli. 2019. fairseq: A fast, extensible
toolkit for sequence modeling. In Proceedings of
the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics
(Demonstrations) , pages 48–53, Minneapolis, Min-
nesota. Association for Computational Linguistics.
Oﬁr Press, Noah Smith, and Mike Lewis. 2022. Train
short, test long: Attention with linear biases enables
input length extrapolation. In International Confer-
ence on Learning Representations .
Oﬁr Press, Noah A. Smith, and Omer Levy. 2020. Im-
proving transformer models by reordering their sub-
layers. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics ,
pages 2996–3005, Online. Association for Computa-
tional Linguistics.
Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training.

--- PAGE 7 ---
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring
the limits of transfer learning with a uniﬁed text-to-
text transformer. Journal of Machine Learning Re-
search , 21(140):1–67.
Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile
Saulnier, Stas Bekman, M Saiful Bari, Stella Bider-
man, Hady Elsahar, Jason Phang, Oﬁr Press, Colin
Raffel, Victor Sanh, Sheng Shen, Lintang Sutawika,
Jaesung Tae, Zheng Xin Yong, Julien Launay, and
Iz Beltagy. 2022. What language model to train if
you have one million GPU hours? In Challenges &
Perspectives in Creating Large Language Models .
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
2018. Self-attention with relative position represen-
tations. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers) , pages 464–468,
New Orleans, Louisiana. Association for Computa-
tional Linguistics.
Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle
Pineau, Adina Williams, and Douwe Kiela. 2021.
Masked language modeling and the distributional
hypothesis: Order word matters pre-training for lit-
tle. In Proceedings of the 2021 Conference on Em-
pirical Methods in Natural Language Processing ,
pages 2888–2913, Online and Punta Cana, Domini-
can Republic. Association for Computational Lin-
guistics.
Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,
and Rob Fergus. 2015. End-to-end memory net-
works. In Proceedings of the 28th International
Conference on Neural Information Processing Sys-
tems - Volume 2 , NIPS’15, page 2440–2448, Cam-
bridge, MA, USA. MIT Press.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems , volume 30. Curran Associates, Inc.
Elena V oita, Rico Sennrich, and Ivan Titov. 2019. The
bottom-up evolution of representations in the trans-
former: A study with machine translation and lan-
guage modeling objectives. In Proceedings of the
2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 4396–4406, Hong Kong,
China. Association for Computational Linguistics.

--- PAGE 8 ---
A NoPos Performance Across Different
Segments of the Input
To shed more light on the ﬁndings shown in sec-
tion 4, we explore whether there are parts of the
sequence that the NoPos model better predicts com-
pared to other positional methods (e.g., is the No-
Pos model performs better at the beginning or the
end the sequence). We compute the model’s per-
plexity in different parts of the sequences. Speciﬁ-
cally, we split each input sequence into eight con-
secutive segments and compute the perplexity for
each segment separately.
We evaluate the NoPos and Learned 1.3B param-
eter models trained on the Pile, with input sequence
length of 1024, and use the standard validation set.
Figure 4 shows the results of this experiment. The
NoPos model performs similarly or slightly worse
than the baseline model on all input parts.
1:64 65:128 129:192 193:256 257:320 321:384 385:448 449:512
Sequence Split4.24.34.44.54.64.74.84.9Loss
NoPos
Sinusoidal
Figure 4: NoPos model shows similar performances on
each part of the sequence, comparing to the baseline
Learned absolute position encoding.
B Word Order Analysis
Is positional information necessary for language
modeling, or does the order of the input tokens not
matter? To answer this, we conduct the following
experiment: instead of computing the loss on the
complete sequence, we pick a speciﬁc token in the
sequence. The next token prediction is conditioned
on the previous tokens in the sequence, and so we
shufﬂe the order of the tokens in the preﬁx and
compute the loss only for that speciﬁc token. We
repeat the experiment with the original, un-shufﬂed
preﬁx sequence as the baseline and compare the
results.
The experiment was conducted on the NoPos
model with an input sequence length of 512 using
the WikiText-103 dataset. We randomly sample
an index between 5 and 512 for the token we pick
from each input sequence from the validation set.
Figure 5 shows the results of this experiment for100 different inputs. These results clearly show
that the transformer language model’s next word
predictions are not order-invariant.
Baseline Shuffled Prefix4681012Token-Level Loss
Figure 5: Shufﬂing input tokens (for causal langauge
modeling) leads to a massive degradation in token-level
loss.
C Hyperparameters
Table 5 provides the optimization hyperparameters
for each one of our experiments, and Table 6 shows
the model hyperparameters in the modern (Pile)
setting.

--- PAGE 9 ---
WikiText-103 The Pile Probe Masked LM
Sequence Length 512 1024 1024 128
Optimizer NAG Adam Adam Adam
Peak Learning Rate 1 2e-3 2e-3 1e-3
Warmup Steps 16,000 500 500 500
Total Steps 286,000 10,000 10,000 10,000
Tokens per Batch 72,000 256,000 64,000 1,024,000
Dropout 0.3 0 0 0.1
Weight Decay 0 0.01 0.01 0.01
Table 5: The optimization hyperparameters used in this work. The NAG optimizer refers to Nesterov accelerated
gradient (Nesterov, 1983), and Adam refers to (Kingma and Ba, 2015).
125M 350M 760M 1.3B
Layers 12 24 24 24
Model Dimensions 768 1024 1536 2048
Feed-forward Dimensions 3072 4096 6144 8192
Attention Heads 12 16 16 32
Table 6: The models hyperparameters by size.
