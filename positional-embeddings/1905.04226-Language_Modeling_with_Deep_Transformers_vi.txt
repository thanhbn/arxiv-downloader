# 1905.04226.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/positional-embeddings/1905.04226.pdf
# Kích thước tệp: 532319 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Mô hình hóa Ngôn ngữ với Transformers Sâu
Kazuki Irie1, Albert Zeyer1;2, Ralf Schl ¨uter1, Hermann Ney1;2
1Nhóm Công nghệ Ngôn ngữ Tự nhiên và Nhận dạng Mẫu, Khoa Khoa học Máy tính
Đại học RWTH Aachen, 52074 Aachen, Đức
2AppTek GmbH, 52062 Aachen, Đức
firie, zeyer, schlueter, ney g@cs.rwth-aachen.de
Tóm tắt
Chúng tôi khám phá các mô hình Transformer tự hồi quy sâu trong mô hình hóa ngôn ngữ cho nhận dạng giọng nói. Chúng tôi tập trung vào hai khía cạnh. Đầu tiên, chúng tôi xem xét lại các cấu hình mô hình Transformer cụ thể cho mô hình hóa ngôn ngữ. Chúng tôi chỉ ra rằng các mô hình Transformer được cấu hình tốt vượt trội so với các mô hình cơ sở của chúng tôi dựa trên chồng nông các lớp mạng nơ-ron hồi quy LSTM. Chúng tôi thực hiện thí nghiệm trên tác vụ LibriSpeech 960hr mã nguồn mở, cho cả mô hình hóa ngôn ngữ mức từ vựng 200K từ và mức từ phụ mã hóa cặp byte 10K. Chúng tôi áp dụng các mô hình mức từ của chúng tôi cho nhận dạng giọng nói hybrid truyền thống bằng cách chấm điểm lại lattice, và các mô hình mức từ phụ cho các mô hình encoder-decoder dựa trên attention bằng cách fusion nông. Thứ hai, chúng tôi chỉ ra rằng các mô hình ngôn ngữ Transformer sâu không yêu cầu mã hóa vị trí. Mã hóa vị trí là một bổ sung thiết yếu cho cơ chế self-attention mà không biến đổi theo thứ tự chuỗi. Tuy nhiên, trong thiết lập tự hồi quy, như trong trường hợp mô hình hóa ngôn ngữ, lượng thông tin tăng dần theo chiều vị trí, đây chính là một tín hiệu vị trí. Phân tích trọng số attention cho thấy rằng các mô hình self-attention tự hồi quy sâu có thể tự động sử dụng thông tin vị trí như vậy. Chúng tôi phát hiện rằng việc loại bỏ mã hóa vị trí thậm chí cải thiện nhẹ hiệu suất của những mô hình này.
Thuật ngữ chỉ mục: mô hình hóa ngôn ngữ, self-attention, Transformer, nhận dạng giọng nói

1. Giới thiệu
Các mô hình encoder-decoder Transformer [1] đã trở nên phổ biến trong xử lý ngôn ngữ tự nhiên. Kiến trúc Transformer cho phép huấn luyện thành công một chồng sâu các lớp self-attention [2 –4] thông qua các kết nối dư [5] và chuẩn hóa lớp [6]. Các mã hóa vị trí [1, 7], thường dựa trên các hàm sinusoidal, được sử dụng để cung cấp cho self-attention thông tin thứ tự chuỗi. Trong nhiều ứng dụng khác nhau, các cải tiến hệ thống đã được báo cáo so với các mô hình tiêu chuẩn, đa lớp dựa trên mạng nơ-ron hồi quy bộ nhớ ngắn-dài hạn (LSTM) [8]. Trong khi ban đầu được thiết kế như một kiến trúc encoder-decoder trong dịch máy, các thành phần encoder (ví dụ, [9]) và decoder (ví dụ, [10]) cũng được sử dụng riêng biệt trong các vấn đề tương ứng tùy thuộc vào việc vấn đề có sử dụng toàn bộ chuỗi để dự đoán hay không.

Một số công trình gần đây cũng đã cho thấy hiệu suất ấn tượng trong mô hình hóa ngôn ngữ sử dụng thành phần decoder Transformer [10 –15]. Ví dụ sớm nhất có thể tìm thấy trong [10] nơi những mô hình như vậy được nghiên cứu cho việc tạo văn bản. Các công trình gần đây về huấn luyện các mô hình lớn hơn và sâu hơn [12, 14, 15] đã cho thấy tiềm năng tiếp theo của Transformer trong mô hình hóa ngôn ngữ. Mặt khác, một hạn chế rõ ràng của Transformers là yêu cầu bộ nhớ của chúng tăng tuyến tính theo số lượng token trong chuỗi, điều này đòi hỏi phải làm việc với cửa sổ ngữ cảnh hạn chế (về cơ bản là một mô hình n-gram trong đó số điển hình cho n là 512) cho các tác vụ xử lý chuỗi dài như mô hình hóa ngôn ngữ cấp ký tự [12]. Dai et al. [11] đã giới thiệu một sự hồi quy cấp đoạn và mã hóa vị trí tương đối trong mô hình ngôn ngữ Transformer để có thể xử lý ngữ cảnh không giới hạn.

Trong công trình này, chúng tôi nghiên cứu các Transformers tự hồi quy sâu cho mô hình hóa ngôn ngữ trong nhận dạng giọng nói. Cụ thể, chúng tôi tập trung vào hai khía cạnh. Đầu tiên, chúng tôi xem xét lại các cấu hình tham số của Transformers, ban đầu được thiết kế cho vấn đề sequence-to-sequence [1], cụ thể cho mô hình hóa ngôn ngữ. Chúng tôi tiến hành thí nghiệm trên tác vụ nhận dạng giọng nói tự động (ASR) LibriSpeech [16] cho cả nhận dạng giọng nói truyền thống mức từ và nhận dạng giọng nói end-to-end mức mã hóa cặp byte (BPE) [17]. Chúng tôi áp dụng các mô hình mức từ của chúng tôi cho nhận dạng giọng nói hybrid bằng cách chấm điểm lại lattice [20], và các mô hình mức BPE cho các mô hình end-to-end bằng cách fusion nông [21, 22]. Chúng tôi chỉ ra rằng các mô hình ngôn ngữ Transformer được cấu hình tốt vượt trội so với các mô hình dựa trên chồng đơn giản các lớp LSTM RNN về cả perplexity và tỷ lệ lỗi từ (WER).

Thứ hai, chúng tôi chứng minh thực nghiệm rằng mã hóa vị trí không cần thiết cho các mô hình self-attention tự hồi quy đa lớp. Việc trực quan hóa trọng số attention cho thấy rằng khi mã hóa vị trí sinusoidal được cung cấp với đầu vào, lớp đầu tiên của Transformers học để trích xuất các đặc trưng n-gram (do đó sử dụng thông tin vị trí). Tuy nhiên, trong vấn đề tự hồi quy nơi một token mới được cung cấp cho mô hình ở mỗi bước thời gian, lượng thông tin mà mô hình có quyền truy cập tăng nghiêm ngặt từ trái sang phải ở cấp thấp nhất của mạng, điều này sẽ cung cấp một số thông tin vị trí của chính nó. Chúng tôi quan sát thấy rằng các mô hình ngôn ngữ Transformer sâu không có mã hóa vị trí tự động sử dụng thông tin như vậy, và thậm chí cho cải thiện nhẹ so với các mô hình có mã hóa vị trí.

2. Công trình Liên quan
Phần đầu tiên của công trình chúng tôi theo tinh thần của công trình của Al-Rfou et al. [12] và công trình của Radford et al. [14,15] trong việc nghiên cứu các Transformers lớn hơn và sâu hơn cho mô hình hóa ngôn ngữ. Chúng tôi chỉ ra rằng các mô hình ngôn ngữ Transformer sâu có thể được áp dụng thành công cho nhận dạng giọng nói và cho hiệu suất tốt. Phần thứ hai của công trình này liên quan đến mã hóa vị trí, là một thành phần quan trọng trong Transformer gốc. Một số công trình trước đây đã nghiên cứu các biến thể mã hóa vị trí để cải thiện self-attention (ví dụ, [11, 23 –25]). Các công trình trước đây trong mô hình ngôn ngữ Transformer hệ thống sử dụng mã hóa vị trí, hoặc là học chung hoặc là sinusoidal (cả hai trường hợp đều được báo cáo cho hiệu suất tương tự trong [12]). Chúng tôi chỉ ra rằng các mô hình self-attention tự hồi quy sâu không yêu cầu bất kỳ mô hình rõ ràng nào để mã hóa vị trí để cho hiệu suất tốt nhất.

3. Self-Attention Tự hồi quy
Mô hình ngôn ngữ mà chúng tôi xem xét dựa trên thành phần decoder của kiến trúc Transformer [1]. Tương tự như công trình trước đây [10 –15], chúng tôi định nghĩa lớp như một chồng của hai thành phần: self-attention và các mô-đun feed-forward1.

1Thường được gọi là mô-đun feed-forward theo vị trí [1]. Ở đây chúng tôi bỏ qua theo vị trí vì nó rõ ràng đối với các mô hình tự hồi quy.

arXiv:1905.04226v2  [cs.CL]  11 Jul 2019

--- TRANG 2 ---
Mô-đun self-attention tự hồi quy trong lớp thứ l biến đổi đầu vào z(l1)
t tại vị trí t như sau:
x(l)
t= LayerNorm( z(l1)
t)
q(l)
t; k(l)
t; v(l)
t =Qx(l)
t; Kx(l)
t; V x(l)
t
h(l)
t=
h(l)
t1;(k(l)
t; v(l)
t)
y(l)
t =z(l1)
t +W0SelfAttention( h(l)
t; q(l)
t)
trong đó Q,K,V, tương ứng biểu thị các ma trận chiếu query, key, value, LayerNorm biểu thị chuẩn hóa lớp [6], SelfAttention biểu thị self-attention tích chấm đa đầu có tỷ lệ [1], và W0 biểu thị ma trận chiếu cho kết nối dư [5].

Đầu ra y(l)
t sau đó được đưa vào mô-đun feed-forward:
m(l)
t= LayerNorm( y(l)
t)
z(l)
t =y(l)
t+W2Activation( W1m(l)
t)
trong đó Activation là rectifier [26], đơn vị tuyến tính lỗi Gaussian (GELU) [15, 27], hoặc đơn vị tuyến tính có cổng (GLU) [28] trong công trình này.
Mô hình cuối cùng được xây dựng bằng cách chồng các lớp này nhiều lần. Đầu vào của mạng bao gồm tổng của embedding token (từ hoặc BPE trong công trình này) và mã hóa vị trí sinusoidal như được chỉ định trong [1]. Lớp softmax đầu ra cho phân phối xác suất cho token tiếp theo. Như được thể hiện trong các phương trình ở trên, h(l)
t có thể được xem như các trạng thái của mô hình Transformer2 (có kích thước, trái ngược với các trạng thái RNN, tăng tuyến tính theo chiều vị trí). Trong quá trình suy luận, các trạng thái này được lưu trữ để tránh tính toán dư thừa. Trong quá trình huấn luyện, việc tính toán theo chiều vị trí được song song hóa để tăng tốc.

4. Tập dữ liệu LibriSpeech
4.1. Mô tả Dữ liệu Mô hình hóa Ngôn ngữ
Tập dữ liệu LibriSpeech [16] cho mô hình hóa ngôn ngữ bao gồm dữ liệu văn bản 800M từ và 960 giờ phiên âm âm thanh tương ứng với dữ liệu văn bản 10M từ. Dựa trên phân tích perplexity mô hình đếm, chúng tôi quan sát thấy rằng phần phiên âm âm thanh không chứa tín hiệu miền đặc biệt nào phù hợp với tập phát triển. Do đó, chúng tôi đơn giản hợp nhất hai tập dữ liệu để tạo thành một tập dữ liệu duy nhất cho huấn luyện mô hình ngôn ngữ. Độ dài câu trung bình trong dữ liệu huấn luyện kết quả là 21 từ với độ dài tối đa là 600 từ. Các tập phát triển và kiểm tra tương ứng có hai phần [16]: dev-clean, dev-other, test-clean, và test-other. Sự phân tách này dựa trên các đặc tính cấp âm thanh, do đó nó không có ý nghĩa đặc biệt cho mô hình hóa ngôn ngữ. Trong phần thí nghiệm, chúng tôi ký hiệu bằng "Dev" và "Test" việc nối các phần clean và other của dữ liệu tương ứng. Cả hai tập dữ liệu đều bao gồm khoảng 110K từ chạy với trung bình 20 từ mỗi câu. Từ vựng mức từ chứa 200K từ. Chúng tôi báo cáo tất cả perplexity mà không sử dụng ngữ cảnh vượt quá ranh giới câu.

4.2. Baseline 4-gram đếm và LSTM-RNN
Chúng tôi sử dụng mô hình ngôn ngữ 4-gram đếm chính thức được cung cấp với tập dữ liệu LibriSpeech [16]. Không có cải thiện nào trong perplexity được quan sát khi tăng lên 5-gram. Đối với các mô hình ngôn ngữ LSTM-RNN [29], chúng tôi đầu tiên huấn luyện cấu hình cơ sở của chúng tôi; mô hình có 2 lớp LSTM-RNN với 2048 nút và lớp chiếu đầu vào là 128, trong đó dropout với tỷ lệ 0.2 được áp dụng giữa mỗi lớp. Vì chúng tôi quan sát thấy rằng mô hình này underfit tập huấn luyện LibriSpeech, chúng tôi loại bỏ dropout và tăng thêm kích thước mô hình, điều này hiệu quả cho perplexity tốt hơn như được thể hiện trong Bảng 1. Chúng tôi thấy rằng các cải thiện từ việc đơn giản chồng các lớp bão hòa ở 4 lớp ngay cả khi không overfit. Việc giới thiệu một lớp bottleneck tuyến tính nhỏ (kích thước 512 ở đây) trước lớp đầu ra có thể làm cho các mô hình nhỏ gọn nhưng với mất mát hiệu suất. Mô hình tốt nhất chúng tôi có được có 2 lớp với 4096 nút. Các cải thiện tương đối lớn hơn 58% được có được bởi LSTM so với mô hình ngôn ngữ 4-gram.

Bảng 1: Perplexity của các mô hình baseline.
Mô hình Drop- Bottle- Số Số Tham số Dev Test
out neck đơn vị lớp tính bằng M
4-gram - - - - 230 146.2 151.8
LSTM0.2
None20482 48771.3 74.8
0.066.6 69.9
3 520 64.0 67.2
4 554 61.9 64.9
5 587 62.7 65.9
6 621 64.5 67.5
8 688 67.2 70.3
4096 21048 60.2 63.2
512334 63.1 66.3
2048 4 248 64.5 67.7

5. Thí nghiệm dựa trên Văn bản
Chúng tôi thực hiện thí nghiệm cho cả mô hình hóa ngôn ngữ mức từ và mức BPE. Chúng tôi đầu tiên tập trung vào mức từ.

5.1. Siêu tham số trong Transformers
Kiến trúc Transformer là một không gian tìm kiếm Odyssey mới [30]. Các siêu tham số mô hình đầy đủ cho các mô hình ngôn ngữ Transformer được chỉ định bởi các phương trình trong Mục 3 là kích thước embedding token đầu vào, số lớp, chiều của kết nối dư, và cho mỗi lớp số đầu attention, chiều của key và query, chiều của value, và chiều của lớp feed-forward.

Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng cùng chiều cho key, query và value, cũng như kết nối dư. Chúng tôi sử dụng cùng chiều trong tất cả các lớp. Do đó, các mô hình của chúng tôi có thể được chỉ định đầy đủ bởi tuple (số lớp L, chiều feed-forward dff, chiều dư dres, số đầu H). Chúng tôi không áp dụng bất kỳ phương pháp regularization nào bao gồm dropout. Chúng tôi huấn luyện tất cả các mô hình sử dụng gradient descent ngẫu nhiên đơn giản và điều chỉnh tỷ lệ học new-bob trên một GPU duy nhất. Chúng tôi định nghĩa sub-epoch huấn luyện của chúng tôi (cho new-bob) là 1/10 của dữ liệu huấn luyện đầy đủ. Tất cả các triển khai của chúng tôi đều dựa trên bộ công cụ mã nguồn mở RETURNN [32]3 dựa trên Tensorflow [31].

5.2. Điều chỉnh Siêu tham số
Với lượng dữ liệu huấn luyện LibriSpeech (810M từ), việc huấn luyện tất cả các biến thể mô hình cho đến hội tụ hoàn toàn là không hợp lý. Giai đoạn đầu của huấn luyện đã chỉ ra một cách nhất quán tiềm năng hiệu suất của các mô hình. Do đó, chúng tôi đầu tiên thực hiện so sánh giữa các mô hình với cấu hình khác nhau ở số lần cập nhật bằng nhau, đủ lớn, nhưng hợp lý.

Tập so sánh đầu tiên nghiên cứu tác động của độ sâu và độ rộng. Kết quả perplexity có thể được tìm thấy trong Bảng 2. Tất cả các mô hình trong bảng sử dụng 8 đầu attention. Các tham số khác được chỉ định trong bảng. Bảng được tổ chức thành ba phần. Phần trên của Bảng 2 cho thấy tác động của số lớp; chúng tôi quan sát thấy rằng việc tăng số lớp (do đó số tham số) từ 1 đến 42 dần dần cải thiện perplexity. Trong phần giữa của Bảng 2, chúng tôi thay đổi cả số lớp, chiều feed-forward, và chiều dư. Đầu tiên, mô hình 12-lớp (12;4096;512;8) vượt trội so với mô hình 6-lớp (6;8192;512;8), trong khi có số tham số tương tự, điều này dường như chỉ ra rằng độ sâu hiệu quả có lợi cho các mô hình ngôn ngữ Transformer. Chúng tôi cũng huấn luyện một mô hình cực đoan chỉ có 2 lớp với các chiều rộng (2;8192;2048;8). Số tham số thực tế tăng vọt vì giá trị lớn của dres dẫn đến một ma trận lớn trong lớp softmax đầu ra với từ vựng 200K4. Chúng tôi quan sát thấy rằng những mô hình rộng nhưng nông như vậy không hoạt động tốt5. Cuối cùng, phần dưới của Bảng 2 cho thấy các mô hình sâu hơn với chiều đầu vào nhỏ hơn.

Bảng 2: Perplexity sau 2.5 epoch (25 sub-epoch trong thiết lập của chúng tôi; 6.5M cập nhật). Số đầu H là 8 cho tất cả các mô hình dưới đây.
Đầu vào L dff dres Tham số Perplexity
emb. tính bằng M Huấn luyện Dev
5121
2048 512208 108.3 104.9
6 224 75.7 74.3
12 243 67.6 67.1
24 281 62.2 62.3
32 306 60.1 60.6
42 338 59.0 59.6
512281922048 536 73.1 73.8
6
512262 66.7 66.7
12 4096 268 63.5 63.8
416384 277 67.6 67.4
32768 344 65.4 68.4
12864
2048 512330 56.3 57.6
80 380 53.1 55.5
96 431 51.9 54.9
112 481 51.5 54.5

Bảng 3 cho thấy tác động của số đầu attention. 16 đầu là số lớn nhất chúng tôi thử trong thiết lập này cho hiệu suất tốt nhất. Ngoài ra, chúng tôi kiểm tra loại hàm kích hoạt (Bảng 4). Trái ngược với công trình trước đây về các mô hình ngôn ngữ feed-forward sử dụng GLU [28, 34], chúng tôi không quan sát thấy hội tụ nhanh hơn. Vì chúng tôi quan sát thấy rằng tác động của việc lựa chọn các hàm kích hoạt đối với perplexity nhìn chung là hạn chế, tất cả các mô hình khác của chúng tôi sử dụng ReLU tiêu chuẩn. Như được báo cáo trong Transformer gốc, chúng tôi xác nhận rằng cả chuẩn hóa lớp và kết nối dư đều cần thiết cho những mô hình này để huấn luyện ổn định6.

Cuối cùng, chúng tôi huấn luyện các mô hình với các cấu hình tốt nhất trong thời gian dài hơn. Bảng 5 cho thấy các perplexity tốt hơn so với những gì có được bởi các mô hình dựa trên LSTM của chúng tôi (Bảng 1).

Bảng 3: Tác động của số đầu. Perplexity sau 2.5 epoch cho (12, 2048, 512, H).
H Tham số Perplexity
tính bằng M Huấn luyện Dev
1
24371.9 70.8
4 69.1 68.6
8 67.6 67.1
16 66.9 66.6

Bảng 4: Tác động của các hàm kích hoạt. Perplexity sau 1 epoch (10 sub-epoch trong thiết lập của chúng tôi) cho (24, 2048, 512, 8).
Kích hoạt Perplexity
Huấn luyện Dev
ReLU [1, 26] 76.4 72.5
GLU [28] 76.5 72.8
GELU [15, 27] 75.7 72.2

4Chúng tôi lưu ý rằng đây cũng là lý do tại sao số tham số của các mô hình ngôn ngữ LSTM baseline của chúng tôi trong Bảng 1 tương đối cao.
5Vì chiều bottleneck softmax thường cần phải lớn để có hiệu suất tốt nhất ([33]; Bảng 1), chúng tôi cũng huấn luyện một mô hình (12, 2048, 512, 8) trong đó chúng tôi chèn một lớp chiếu bổ sung với chiều lớn (2048) trước lớp đầu ra; không có cải thiện nào được có được.
6Chúng tôi đã thử huấn luyện nhiều mô hình mà không có kết nối dư hoặc chuẩn hóa lớp. Ngoài ra, theo [15], chúng tôi đã thử tổ chức lại mô-đun feed-forward để chèn một lớp chuẩn hóa tiền kích hoạt bổ sung [35] và một hàm kích hoạt nữa. Tuy nhiên, chúng tôi không quan sát thấy bất kỳ cải thiện nào. Các Transformer gốc dù sao cũng không có bất kỳ kích hoạt nào trên đường dư trong toàn bộ mạng.

Bảng 5: Perplexity sau huấn luyện dài hơn.
Max. Hội tụ L dff dres Tham số Perplexity
Epoch trong M Huấn luyện Dev Test
5.5
Có124096
512268 57.3 59.9 62.3
524
2048281 55.6 58.0 60.7
32 306 53.4 56.6 59.5
42 338 51.2 55.0 57.7
3 Không802048 512380 51.9 54.3 56.9
96 431 50.9 53.7 56.3

5.3. Chia sẻ Tham số
Dehghani et al. [36] báo cáo Universal Transformers hoạt động đặc biệt tốt cho mô hình hóa ngôn ngữ. Điều này thúc đẩy chúng tôi thí nghiệm với việc chia sẻ tham số giữa các lớp. Để những mô hình như vậy có số tham số so sánh được với các Transformer sâu tiêu chuẩn, các chiều trong mỗi lớp phải được tăng lên, dẫn đến huấn luyện chậm hơn; ở đây chúng tôi đơn giản nghiên cứu tác động của số lần hồi quy. Bảng 6 cho thấy kết quả perplexity. Đầu tiên, chúng tôi quan sát thấy rằng hiệu suất mô hình thua kém so với Transformer tiêu chuẩn7 (Bảng 2). Tuy nhiên, chúng tôi rõ ràng quan sát thấy rằng việc tăng số lớp từ 3 đến 12 liên tục cải thiện perplexity. Cải thiện này mà không có tham số bổ sung thúc đẩy công trình tương lai để nghiên cứu thêm các chiến lược chia sẻ tham số cho Transformers.

Bảng 6: Perplexity sau 2.5 epoch cho các mô hình (L, 8192, 1024, 16) với tham số được chia sẻ giữa tất cả các lớp.
L Tham số Perplexity
tính bằng M Huấn luyện Dev
3
32982.6 79.9
6 76.7 74.6
12 74.2 72.1

6. Thí nghiệm ASR
6.1. Kết quả Chấm điểm lại Lattice
Chúng tôi áp dụng các mô hình ngôn ngữ Transformer mức từ của chúng tôi cho nhận dạng giọng nói hybrid truyền thống bằng cách chấm điểm lại lattice. Thuật toán chấm điểm lại lattice push-forward tiêu chuẩn [20] cho các mô hình ngôn ngữ phạm vi dài có thể được áp dụng trực tiếp cho các mô hình dựa trên self-attention. Các sửa đổi duy nhất từ phiên bản RNN là định nghĩa "trạng thái" như tất cả các trạng thái ẩn (h(l)
t trong Mục 3) trong tất cả các lớp từ tất cả các vị trí tiền nhiệm và vị trí hiện tại (t; cho mã hóa vị trí). Bảng 7 cho thấy WER và perplexity (PPL). Mô hình acoustic baseline của chúng tôi dựa trên LSTM hai chiều đa lớp [37]. Mô tả thêm về mô hình acoustic baseline của chúng tôi có thể được tìm thấy trong [38]. Chúng tôi có được các cải thiện nhất quán về WER so với baseline LSTM.

Bảng 7: WER (%) cho các hệ thống hybrid trên LibriSpeech 960hr. Mô hình 4-gram được sử dụng trong lần đầu tiên để tạo lattice cho việc chấm điểm lại. Dòng "Lattice" cho thấy WER oracle của lattice.
LM L Tham số dev test
tính bằng M clean other clean other
PPL WER PPL WER PPL WER PPL WER
4-gram -230 151.7 3.4140.6 8.3158.1 3.8145.7 8.8
Lattice -- -1.0 -2.3 -1.3 -2.6
LSTM 21048 60.2 2.3 60.2 5.4 64.8 2.6 61.7 5.9
Trans-24281 57.8 2.2 58.3 5.2 62.2 2.5 59.4 5.7
former42338 54.5 2.1 55.5 5.2 59.1 2.5 56.4 5.7
96431 53.2 2.154.2 5.257.6 2.555.0 5.6

7Chúng tôi lưu ý rằng ở đây việc so sánh trực tiếp không đơn giản như giữa các Transformer tiêu chuẩn. Thực tế, chúng tôi quan sát thấy rằng các siêu tham số huấn luyện được điều chỉnh cho Transformer tiêu chuẩn không thể được áp dụng trực tiếp cho Universal Transformers; cụ thể, chúng tôi thấy rằng việc giảm ngưỡng cắt chuẩn gradient từ 1 xuống 0.1 là quan trọng, điều này có thể làm chậm hội tụ.

--- TRANG 3 ---
<bos>
so
they
went
on
to
the
verandah
and
looked
down
upon
the
lights
of
the
prison
and
listened
to
the
sea
lapping
the
shoresotheywentontotheverandahandlookeddownuponthelightsoftheprisonandlistenedtothesealappingtheshore<eos>(a)Lớp đầu tiên với PE
<bos>
so
they
went
on
to
the
verandah
and
looked
down
upon
the
lights
of
the
prison
and
listened
to
the
sea
lapping
the
shoresotheywentontotheverandahandlookeddownuponthelightsoftheprisonandlistenedtothesealappingtheshore<eos> (b)Lớp đầu tiên không có PE
<bos>
so
they
went
on
to
the
verandah
and
looked
down
upon
the
lights
of
the
prison
and
listened
to
the
sea
lapping
the
shoresotheywentontotheverandahandlookeddownuponthelightsoftheprisonandlistenedtothesealappingtheshore<eos> (c)Lớp "Blur"
<bos>
so
they
went
on
to
the
verandah
and
looked
down
upon
the
lights
of
the
prison
and
listened
to
the
sea
lapping
the
shoresotheywentontotheverandahandlookeddownuponthelightsoftheprisonandlistenedtothesealappingtheshore<eos> (d)Lớp "Window"
<bos>
so
they
went
on
to
the
verandah
and
looked
down
upon
the
lights
of
the
prison
and
listened
to
the
sea
lapping
the
shoresotheywentontotheverandahandlookeddownuponthelightsoftheprisonandlistenedtothesealappingtheshore<eos> (e)Lớp "Có cấu trúc"

Hình 1: Các loại lớp trong mô hình ngôn ngữ Transformer 24-lớp mức từ. Trục x tương ứng với các từ đầu vào. Trục y cho thấy các từ đích; mỗi vị trí từ đích có 8 hàng phụ tương ứng với 8 đầu. "PE" biểu thị mã hóa vị trí.

6.2. Kết quả Fusion Nông ASR End-to-End
Chúng tôi huấn luyện các mô hình ngôn ngữ Transformer mức BPE 10K để kết hợp với một mô hình giọng nói encoder-decoder dựa trên attention bằng fusion nông [21,22]. Dữ liệu huấn luyện mức BPE 10K có độ dài trung bình dài hơn là 24 token mỗi câu với độ dài câu dài nhất là 1343, vẫn có thể quản lý được mà không cần cắt bớt cho self-attention. Chúng tôi sử dụng kiến trúc Transformer của (24, 4096, 1024, 8). Mô hình LSTM có 4 lớp với 2048 nút. Chúng tôi tham khảo công trình trước đây của chúng tôi [19] để mô tả mô hình attention baseline; WER baseline tốt hơn so với công trình trước đây của chúng tôi [19] được có được bằng cách cải thiện học curriculum và huấn luyện dài hơn. Bảng 8 cho thấy cả perplexity và WER. Theo [39], chúng tôi giới thiệu phạt cuối câu trong fusion nông để hưởng lợi từ kích thước beam lớn là 64. Một lần nữa, chúng tôi có được các cải thiện nhất quán so với baseline LSTM. Những kết quả này tốt hơn so với WER được báo cáo trước đây [39 –41] cho các mô hình end-to-end mà không có tăng cường dữ liệu [42].

Bảng 8: WER (%) cho các mô hình dựa trên attention trên tập dữ liệu LibriSpeech 960hr. Perplexity ở mức BPE 10K.
LM
Beam dev test
clean other clean other
PPL WER PPL WER PPL WER PPL WER
Không có 12 - 4.3 -12.9 - 4.4 -13.5
LSTM6443.7 2.946.4 8.947.1 3.247.2 9.9
Transfo. 35.9 2.638.9 8.438.8 2.839.0 9.3

7. Phân tích
So với các trạng thái ẩn trong RNN, trọng số attention dễ dàng được trực quan hóa hơn, điều này tạo cơ hội cho phân tích. Đặc biệt, chúng tôi tập trung vào việc so sánh các mô hình ngôn ngữ Transformer có và không có mã hóa vị trí.

7.1. Transformer LM không có mã hóa vị trí
Trong vấn đề tự hồi quy nơi một token mới được cung cấp cho mô hình ở mỗi bước thời gian, lượng thông tin mà mô hình có quyền truy cập tăng nghiêm ngặt từ trái sang phải ở cấp thấp nhất của mạng; các lớp sâu hơn sẽ có thể nhận ra cấu trúc này mà sẽ cung cấp cho mô hình một số thông tin vị trí của chính nó. Để kiểm tra giả thuyết này, chúng tôi huấn luyện các mô hình không có bất kỳ mã hóa vị trí nào. Đầu tiên, chúng tôi quan sát thấy rằng chúng cho perplexity tốt hơn so với các mô hình có mã hóa vị trí sinusoidal (Bảng 9).

7.2. Lớp đầu tiên
Attention trong lớp đầu tiên là đơn giản nhất để diễn giải vì đặc trưng ở mỗi vị trí chính xác tương ứng với từ ở vị trí đó (trong khi các lớp sâu hơn có thể xáo trộn nội dung đặc trưng). Trọng số attention trong lớp đầu tiên của các mô hình ngôn ngữ Transformer 24-lớp có và không có mã hóa vị trí được trực quan hóa trong Hình 1. Chúng tôi quan sát thấy rằng lớp đầu tiên của mô hình có mã hóa vị trí (Hình 1(a)) học để tạo các đặc trưng n-gram (khoảng 2 hoặc 3-gram), điều này chỉ ra rằng thông tin vị trí được sử dụng trực tiếp.

Bảng 9: Tác động của mã hóa vị trí sinusoidal. Perplexity sau 5 epoch (13M cập nhật) cho các mô hình (L, 2048, 512, 8).
L Mã hóa vị trí Tham số Perplexity
tính bằng M. Huấn luyện Dev Test
12Sinusoidal24361.8 63.1 66.1
Không có 58.0 60.5 63.4
24Sinusoidal28155.6 58.0 60.8
Không có 52.7 56.6 59.2
42Sinusoidal33851.2 55.0 57.7
Không có 50.5 54.2 56.8

Ngược lại, lớp đầu tiên của mô hình không có mã hóa vị trí học để tập trung vào token đầu vào mới có thể được xem như đường chéo trong Hình 1(b) (thú vị, chúng tôi cũng thấy rằng nó bỏ qua một số từ chức năng như "the", "and", "to" có thể được mô hình hóa bởi một số giá trị offset, do đó chú ý đến token bắt đầu câu thay thế), điều này chứng minh rằng mô hình nhận thức được vị trí của đầu vào mới.

7.3. Các lớp khác
Chúng tôi quan sát thấy rằng hành vi của các lớp khác khá tương tự cho cả mô hình Transformer có và không có mã hóa vị trí. Chúng tôi tìm thấy 3 loại lớp trong 23 lớp khác; lớp thứ hai và thứ ba là các lớp "blur" như được thể hiện trong Hình 1(c), dường như trung bình thô qua tất cả các vị trí (trong khi chúng tôi cũng có thể thấy rằng một số đầu tập trung vào các từ khó, ở đây "verandah"). Lớp 4 đến 9 là các lớp window tập trung vào n-gram cục bộ. Một ví dụ đại diện được thể hiện trong Hình 1(d). Cuối cùng, chúng tôi thấy các lớp trên cùng 10 đến 24 có cấu trúc hơn, chú ý đến một số mẫu cụ thể; một ví dụ được thể hiện trong Hình 1(e).

8. Kết luận
Chúng tôi áp dụng các mô hình ngôn ngữ Transformer sâu cho nhận dạng giọng nói. Chúng tôi chỉ ra rằng những mô hình như vậy vượt trội so với chồng nông các LSTM-RNN trên cả mô hình hóa mức từ và mức BPE. Công trình tương lai nghiên cứu việc áp dụng các thành phần quan trọng của Transformer sâu (như chuẩn hóa lớp) cho các mô hình LSTM sâu hơn; ví dụ, kiến trúc decoder RNMT+ [43] cho mô hình hóa ngôn ngữ. Hơn nữa, chúng tôi không áp dụng bất kỳ regularization nào trên các mô hình cho tác vụ LibriSpeech, vì không quan sát thấy overfit trong phạm vi kích thước mô hình chúng tôi thí nghiệm (cho các mô hình mức từ). Chúng tôi có thể vẫn cải thiện các mô hình của chúng tôi đơn giản bằng cách mở rộng kích thước của chúng và sử dụng regularization.

Lời cảm ơn
Công trình này đã nhận được tài trợ từ Hội đồng Nghiên cứu Châu Âu (ERC) trong chương trình nghiên cứu và đổi mới Horizon 2020 của Liên minh Châu Âu (thỏa thuận tài trợ số 694537, dự án "SEQCLAS") và từ Google Focused Award. Công trình chỉ phản ánh quan điểm của các tác giả và không có bên tài trợ nào chịu trách nhiệm cho bất kỳ việc sử dụng nào có thể được thực hiện đối với thông tin mà nó chứa. Chúng tôi cảm ơn Liuhui Deng đã đóng góp vào mã chấm điểm lại lattice của chúng tôi, Arne Nix và Julian Schamper đã chia sẻ các cấu hình Transformer cơ sở của họ, cũng như Eugen Beck, Christoph L¨uscher và Wei Zhou đã giúp đỡ tạo lattice. Các thí nghiệm được thực hiện một phần với tài nguyên tính toán được cấp bởi Đại học RWTH Aachen trong dự án nova0003.

--- TRANG 4 ---
Tài liệu tham khảo
[1]A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, và I. Polosukhin, "Attention is all you need," trong Proc. NIPS, Long Beach, CA, USA, Tháng 12 2017, tr. 5998–6008.
[2]J. Cheng, L. Dong, và M. Lapata, "Long short-term memory-networks for machine reading," trong Proc. EMNLP, Austin, TX, USA, Tháng 11 2016, tr. 551–561.
[3]Z. Lin, M. Feng, C. N. d. Santos, M. Yu, B. Xiang, B. Zhou, và Y. Bengio, "A structured self-attentive sentence embedding," Int. Conf. on Learning Representations (ICLR), Tháng 4 2017.
[4]A. P. Parikh, O. T¨ackstr¨om, D. Das, và J. Uszkoreit, "A decomposable attention model for natural language inference," trong Proc. EMNLP, Austin, TX, USA, Tháng 11 2016, tr. 2249–2255.
[5]K. He, X. Zhang, S. Ren, và J. Sun, "Deep residual learning for image recognition," trong IEEE Conf. on Computer Vision and Patt. Recog. (CVPR), Las Vegas, NV, USA, Tháng 6 2016, tr. 770–778.
[6]J. L. Ba, J. R. Kiros, và G. E. Hinton, "Layer normalization," arXiv preprint arXiv:1607.06450, 2016.
[7]J. Gehring, M. Auli, D. Grangier, D. Yarats, và Y. N. Dauphin, "Convolutional sequence to sequence learning," trong Proc. ICML, Sydney, Australia, Tháng 8 2017, tr. 1243–1252.
[8]S. Hochreiter và J. Schmidhuber, "Long short-term memory," Neural computation, tập 9, số 8, tr. 1735–1780, 1997.
[9]J. Devlin, M.-W. Chang, K. Lee, và K. Toutanova, "BERT: Pre-training of deep bidirectional transformers for language understanding," trong Proc. NAACL, Minneapolis, USA, Tháng 6 2019.
[10] P. J. Liu, M. Saleh, E. Pot, B. Goodrich, R. Sepassi, Ł. Kaiser, và N. Shazeer, "Generating wikipedia by summarizing long sequences," trong ICLR, Vancouver, Canada, Tháng 4 2018.
[11] Z. Dai, Z. Yang, Y. Yang, W. W. Cohen, J. Carbonell, Q. V. Le, và R. Salakhutdinov, "Transformer-XL: Attentive language models beyond a fixed-length context," trong ACL, Florence, Italy, Tháng 7 2019.
[12] R. Al-Rfou, D. Choe, N. Constant, M. Guo, và L. Jones, "Character-level language modeling with deeper self-attention," trong Proc. AAAI Conf. on Artif. Int., Honolulu, HI, USA, Tháng 1 2019.
[13] A. Baevski và M. Auli, "Adaptive input representations for neural language modeling," trong ICLR, New Orleans, LA, USA, Tháng 5 2019.
[14] A. Radford, K. Narasimhan, T. Salimans, và I. Sutskever, "Improving language understanding by generative pre-training," [Trực tuyến]: https://blog.openai.com/language-unsupervised/, 2018.
[15] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, và I. Sutskever, "Language models are unsupervised multitask learners," [Trực tuyến]: https://blog.openai.com/better-language-models/, 2019.
[16] V. Panayotov, G. Chen, D. Povey, và S. Khudanpur, "LibriSpeech: an ASR corpus based on public domain audio books," trong ICASSP, South Brisbane, Queensland, Australia, Tháng 4 2015, tr. 5206–5210.
[17] R. Sennrich, B. Haddow, và A. Birch, "Neural machine translation of rare words with subword units," trong Proc. ACL, Berlin, Germany, Tháng 8 2016, tr. 1715–1725.
[18] W. Chan, N. Jaitly, Q. Le, và O. Vinyals, "Listen, attend and spell: a neural network for large vocabulary conversational speech recognition," trong Proc. ICASSP, Shanghai, China, Tháng 3 2016, tr. 4960–4964.
[19] A. Zeyer, K. Irie, R. Schl¨uter, và H. Ney, "Improved training of end-to-end attention models for speech recognition," trong Proc. Interspeech, Hyderabad, India, Tháng 9 2018, tr. 7–11.
[20] M. Sundermeyer, Z. T¨uske, R. Schl¨uter, và H. Ney, "Lattice decoding and rescoring with long-span neural network language models," trong Interspeech, Singapore, Tháng 9 2014, tr. 661–665.
[21] Ç. G¨ulçehre, O. Firat, K. Xu, K. Cho, L. Barrault, H.-C. Lin, F. Bougares, H. Schwenk, và Y. Bengio, "On using monolingual corpora in neural machine translation," Computer Speech & Language, tập 45, tr. 137–148, Tháng 9 2017.
[22] S. Toshniwal, A. Kannan, C.-C. Chiu, Y. Wu, T. N. Sainath, và K. Livescu, "A comparison of techniques for language model integration in encoder-decoder speech recognition," trong Proc. SLT, Athens, Greece, Tháng 12 2018.
[23] P. Shaw, J. Uszkoreit, và A. Vaswani, "Self-attention with relative position representations," trong Proc. NAACL, New Orleans, LA, USA, Tháng 6 2018, tr. 464–468.
[24] M. Sperber, J. Niehues, G. Neubig, S. St¨uker, và A. Waibel, "Self-attentional acoustic models," trong Proc. Interspeech, Hyderabad, India, Tháng 9 2018, tr. 3723–3727.
[25] J. Salazar, K. Kirchhoff, và Z. Huang, "Self-attention networks for connectionist temporal classification in speech recognition," trong Proc. ICASSP, Brighton, UK, Tháng 5 2019, tr. 7115–7119.
[26] V. Nair và G. E. Hinton, "Rectified linear units improve restricted Boltzmann machines," trong Proc. Int. Conf. on Machine Learning (ICML), Haifa, Israel, Tháng 6 2010, tr. 807–814.
[27] D. Hendrycks và K. Gimpel, "Gaussian error linear units (GELUs)," arXiv preprint arXiv:1606.08415, 2018.
[28] Y. N. Dauphin, A. Fan, M. Auli, và D. Grangier, "Language modeling with gated convolutional networks," trong Proc. ICML, Sydney, Australia, Tháng 8 2017, tr. 933–941.
[29] M. Sundermeyer, R. Schl¨uter, và H. Ney, "LSTM neural networks for language modeling." trong Proc. Interspeech, Portland, OR, USA, Tháng 9 2012, tr. 194–197.
[30] K. Greff, R. K. Srivastava, J. Koutník, B. R. Steunebrink, và J. Schmidhuber, "LSTM: A search space odyssey," IEEE Trans. Neural Netw. Learn. Syst., tập 28, số 10, tr. 2222–2232, 2017.
[31] M. Abadi et al., "Tensorflow: A system for large-scale machine learning," trong Proc. USENIX Sympo. on Operating Sys. Design and Impl. (OSDI 16), Savannah, GA, USA, Tháng 11 2016, tr. 265–283.
[32] A. Zeyer, T. Alkhouli, và H. Ney, "RETURNN as a generic flexible neural toolkit with application to translation and speech recognition," trong Proc. ACL, Melbourne, Australia, Tháng 7 2018.
[33] Z. Yang, Z. Dai, R. Salakhutdinov, và W. W. Cohen, "Breaking the softmax bottleneck: A high-rank RNN language model," trong ICLR, Vancouver, Canada, Tháng 4 2018.
[34] K. Irie, Z. Lei, R. Schl¨uter, và H. Ney, "Prediction of LSTM-RNN full context states as a subtask for N-gram feedforward language models," trong ICASSP, Calgary, Canada, Tháng 4 2018, tr. 6104–6108.
[35] K. He, X. Zhang, S. Ren, và J. Sun, "Identity mappings in deep residual networks," trong Proc. European Conf. on Computer Vision (ECCV), Amsterdam, Netherlands, Tháng 10 2016, tr. 630–645.
[36] M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, và Ł. Kaiser, "Universal Transformers," trong Int. Conf. on Learning Representations (ICLR), New Orleans, LA, USA, Tháng 5 2019.
[37] A. Zeyer, P. Doetsch, P. Voigtlaender, R. Schl¨uter, và H. Ney, "A comprehensive study of deep bidirectional lstm rnns for acoustic modeling in speech recognition," trong Proc. ICASSP, New Orleans, LA, USA, Tháng 3 2017, tr. 2462–2466.
[38] C. L¨uscher, E. Beck, K. Irie, M. Kitza, W. Michel, A. Zeyer, R. Schl¨uter, và H. Ney, "RWTH ASR systems for LibriSpeech: Hybrid vs Attention," trong Interspeech, Graz, Austria, Tháng 9 2019.
[39] A. Hannun, A. Lee, Q. Xu, và R. Collobert, "Sequence-to-sequence speech recognition with time-depth separable convolutions," arXiv preprint arXiv:1904.02619, 2019.
[40] N. Zeghidour, Q. Xu, V. Liptchinsky, N. Usunier, G. Synnaeve, và R. Collobert, "Fully convolutional speech recognition," arXiv preprint arXiv:1812.06864, 2018.
[41] K. Irie, R. Prabhavalkar, A. Kannan, A. Bruguier, D. Rybach, và P. Nguyen, "Model unit exploration for sequence-to-sequence speech recognition," preprint arXiv:1902.01955, 2019.
[42] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, và Q. V. Le, "SpecAugment: A simple data augmentation method for automatic speech recognition," arXiv preprint arXiv:1904.08779, 2019.
[43] M. X. Chen, O. Firat, A. Bapna, M. Johnson, W. Macherey, G. Foster, L. Jones, M. Schuster, N. Shazeer, N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, Z. Chen, Y. Wu, và M. Hughes, "The best of both worlds: Combining recent advances in neural machine translation," trong ACL, Melbourne, Australia, Tháng 7 2018, tr. 76–86.
