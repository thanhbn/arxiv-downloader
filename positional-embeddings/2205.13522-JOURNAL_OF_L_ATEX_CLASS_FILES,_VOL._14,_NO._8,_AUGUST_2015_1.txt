# 2205.13522.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/positional-embeddings/2205.13522.pdf
# File size: 800557 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1
Dynamically Relative Position Encoding-Based
Transformer for Automatic Code Edit
Shiyi Qi, Yaoxian Li, Cuiyun Gao, Xiaohong Su, Shuzheng Gao, Zibin Zheng, and Chuanyi Liu
Abstract ‚ÄîAdapting Deep Learning (DL) techniques to auto-
mate non-trivial coding activities, such as code documentation
and defect detection, has been intensively studied recently. Learn-
ing to predict code changes is one of the popular and essential
investigations. Prior studies have shown that DL techniques such
as Neural Machine Translation (NMT) can beneÔ¨Åt meaningful
code changes, including bug Ô¨Åxing and code refactoring. However,
NMT models may encounter bottleneck when modeling long
sequences, thus are limited in accurately predicting code changes.
In this work, we design a Transformer-based approach, consid-
ering that Transformer has proven effective in capturing long-
term dependencies. SpeciÔ¨Åcally, we propose a novel model named
DTrans. For better incorporating the local structure of code, i.e.,
statement-level information in this paper, DTrans is designed
with dynamically relative position encoding in the multi-head
attention of Transformer. Experiments on benchmark datasets
demonstrate that DTrans can more accurately generate patches
than the state-of-the-art methods, increasing the performance
by at least 5.45%-46.57% in terms of the exact match metric
on different datasets. Moreover, DTrans can locate the lines to
change with 1.75%-24.21% higher accuracy than the existing
methods.
Index Terms ‚ÄîCode edit, Transformer, position encoding
I. I NTRODUCTION
Deep Learning (DL) techniques have been adapted to solve
many traditional software engineering problems and tasks
recently [1], [2], e.g., fault localization [3]‚Äì[5], automatic
program repair [6]‚Äì[8], code summarization [9]‚Äì[11], code
prediction [12], [13], and defect prediction [14]‚Äì[16]. Among
these Ô¨Åelds, learning from code for code change prediction
draws more and more research investigations [17], [18]. Pre-
cisely editing code can signiÔ¨Åcantly facilitate the software
maintenance process for developers [19], [20].
In the process of program development and maintenance,
developers usually need to modify the source code for various
reasons, including program repair [21], [22], code refactor-
ing [23]‚Äì[25]and API-related changes [26], [27], etc. Such
behavior is known as ‚Äúcode edit‚Äù or ‚Äúcode change‚Äù [19],
[20]. Prior research [19], [20], [28], [29] discovers that
code edits generally follow repetitive edit patterns and can
S.Qi, Y .Li, C. Gao, X. Su, S. Gao, C. Liu were with Harbin
Institute of Technology, China (e-mail: syqi981125@163.com,
yaoxian0803@icloud.com, gaocuiyun@hit.edu.cn, sxh@hit.edu.cn,
szgao98@gmail.com, liuchuanyi@hit.edu.cn).
C. Gao and C. Liu were also with Peng Cheng Laboratory and Guangdong
Provincial Key Laboratory of Novel Security Intelligence Technologies,
China.
Z. Zheng was with Sun Yat-sen University, China (email: zib-
inzheng2@yeah.net).
C. Gao and C. Liu are the corresponding authors.
Manuscript received April 19, 2005; revised August 26, 2015.be employed to automatically generate targeted code based on
original code. Figure 1 shows two examples for illustrating
the code edit task. In the original code of Figure 1 (a), the
method testEmpty needs to return the object‚Äôs ID and name.
However, the functions id() andname() do not exist for
the object, which leads to a program bug. The correct code
edit operation is to generate a correct patch for Ô¨Åxing the bug,
i.e, changing to the corresponding correct functions getId()
andgetName() , respectively. For the example in Figure 1(b),
the parameter name is changed from type tomethod for
enhancing the readability of the code. The code edit task aims
at generating the edited code given the original code [19]. Due
to the complex code edit patterns, automatically identifying the
lines for editing and producing accurate edits are challenging.
In recent years, deep learning has made great progress and
been applied to many code-related tasks [30], [31]. The large
software engineering datasets, such as GitHub which includes
over 100 million repositories with over 200 million merged
pull requests (PRs) and 2 billion commits [19], provide us
with sufÔ¨Åcient source code for training DL models. Prior
studies [19], [29], [32] have shown that DL techniques such
as Neural Machine Translation (NMT) [33] can automatically
apply developers‚Äô pull request code to generate meaningful
code changes. NMT models treat pull request code as a
series of tokens or use the parsed tree structure as input,
then creating an intermediate representation with an encoder
network and decoding the representation into target sequence
with a decoder network [9], [34]. This mechanism makes
NMT models can learn complex code change patterns between
input and output sequences [20]. However, NMT models
have proven ineffective in modeling long sequences [35], thus
may be limited in accurately editting code. Considering that
Transformer [36]‚Äì[38] has shown more effective than NMT in
modeling long sequences, it is more applicable for the task.
But directly adopting Transformer still cannot well capture
the structural dependencies between tokens [9], [39]. Thus, to
mitigate the issue of NMT models and better capture the code
dependencies, we propose a novel Transformer-based model,
named as DTrans.
Prior research [20] extracts the AST of original code for
capturing the structural information. In this work, to alleviate
the complexity caused by the AST extraction, we focus on
exploiting the local structure, i.e., the statement-level informa-
tion, which can be easily obtained without parsing. Besides,
for the code editing task, the changes generally happen within
several statements, indicating the importance of local structure
[20], [29]. SpeciÔ¨Åcally, we propose a dynamically relative po-
sition encoding strategy to integrate the variational statement-arXiv:2205.13522v3  [cs.SE]  31 Jul 2022

--- PAGE 2 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2
Public void testEmpty() {
   org.ovirt.engine.api.types.V4Vm object = 
objectFromJson("{}");
   org.junit.Assert.assertNotNull(object);
   org.junit.Assert.assertNull(object.getId());
   org.junit.Assert.assertNull(object.getName());
 }Public void testEmpty(){
   org.ovirt.engine.api.types.V4Vm object = 
objectFromJson("{}");
   org.junit.Assert.assertNotNull(object);
   org.junit.Assert.assertNull(object.id());
   org.junit.Assert.assertNull(object.name());
}
    Original code
Targeted code
(a) Example 1.
public void endTrace(@Nonnull JMethod method) {composedStatus.pop();for (TracerBrush config : brushes) {config.endTrace(method);}}public void endTrace(@Nonnull JMethod type) {composedStatus.pop();for (TracerBrush config : brushes) {config.endTrace(type);}} Original code
Targeted code (b) Example 2.
Fig. 1: Examples for illustrating the code edit task. The grey blocks highlight the changed parts in the code.
level information. Different from the Transformer [36] and
Transformer with relative position [40], which represent po-
sition embedding by absolute position and relative position
respectively, DTrans conducts positional encoding guided by
statement information.
To evaluate the performance of our proposed DTrans model,
we choose three pull request repositories utilized by the
work [19] as benchmark datasets, including Android [41],
Google Source [42], and Ovirt [43]. Besides, we also involve
the 2.3 million 121,895 pair-wise code changes from GitHub
open-source projects [32]. During evaluation, we group project
datasets to two levels, i.e., small and medium levels, ac-
cording to the token numbers of original code following
prior studies [29], [32], [44]. Experiments demonstrate that
DTrans accurately predicts more code edits in both small-level
and medium-level projects, increasing the performance of the
best baseline [29] by at least 5.45% and 25.76% in terms
of the exact match metric, respectively. Moreover, DTrans
successfully locates the lines to change with 1.75%-24.21%
higher accuracy than the best baseline.
Overall, we make the following contributions:
A novel Transformer-based approach is proposed to in-
corporate dynamically relative position encoding strategy
in the multi-head attention of Transformer, which explic-
itly incorporates the statement-level syntactic information
for better capturing the local structure of code.
We evaluate our approach on benchmark datasets, and
the results demonstrate the effectiveness of DTrans in
predicting accurate code changes.
Paper structure. We introduce the background in Sec-
tion II. The proposed approach is illustrated in Section III.
Experimental setup and results are depicted in Section IV and
Section V, respectively. We show some cases in Section VI.
The threats to validity and related work are introduced in
Section VII and Section VIII, respectively. We conclude ourwork in Section IX.
II. B ACKGROUND
In this section, we Ô¨Årst formulate the code change prediction
task and then introduce the basic approach - Transformer.
A. Deep learning (DL) in Code Change Prediction
DL-based techniques aim at learning the mapping relations
between the original code and the target code by training, and
generating edited code for facilitating software development
[29], [45], [46]. Programming languages can be treated as
sequences of code tokens. Therefore, the problem of code
change prediction can be tackled as a neural machine transla-
tion problem [20], [32], that is, to ‚Äútranslate‚Äù from a sequence
of code tokens (the original code) to another sequence of code
tokens (the target code).
We take a a sequence of original code Oas an example,
and let
O= (o1;o2;:::;o i;:::om);
where each oiis thei-th token in the code. Each input
sequence Ocorresponds to a target code C, denoted as:
C= (c1;c2;:::;c n);
wheremandnindicate the lengths of the original and target
sequences, respectively. Our goal is to learn the conditional
distribution and generate changed code sequence by maximiz-
ing the conditional likelihood:
C= arg max
CP(CjO):
Finally, we achieve an optimized target sequence as the
predicted code change.

--- PAGE 3 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3
Multi-HeadAttentionAdd & Norm
InputEmbedding
OutputEmbeddingFeedForwardAdd & NormMaskedMulti-HeadAttentionAdd & NormMulti-HeadAttentionAdd & NormFeedForwardAdd & NormLinearSoftmax
Positional EncodingPositional EncodingOriginal Code
Targeted Code
Extract Syntactic Characteristic111000000111000000111000000000111000000111000000111000000000111000000111000000111
‚Ä¶..1Statement Mask Matrix Dynamically Relative Position
Relative PositionS1S2S3S8
‚Ä¶..
‚Ä¶..Targeted Code
Encode BlockDecode Block
Fig. 2: Framework of the proposed DTrans. The statement mask matrix takes the code snippet shown in Figure 5 as example.
B. Transformer
Transformer employs the typical encoder-decoder struc-
ture [44], and is composed of stacked Transformer blocks.
Each block contains a multi-head self-attention sub-layer
followed by a fully connected positional-wise feed-forward
network sub-layer. The sub-layers are connected by residual
connections [47] and layer normalization [48]. In addition,
Transformer augments the input features by adding a positional
embedding since the self-attention mechanism lacks a natural
way to encode the word position information. Transformer also
applies pad masking to resolve the problem of variable input
lengths and its decoder uses sequence masking in its self-
attention to ensure that the predictions for the i-th position
can only depend on the known outputs at positions less
thani. We introduce the major components of Transformer,
including multi-head self-attention, position-wise feed-forward
networks, and basic blocks of Transformer in the following.
1) Multi-Head Self-Attention: Multi-head self-attention in-
volves multiple attention heads and performs self-attention
mechanism on every head. One attention head obtains one
representation space for the same text, and multi-head attention
obtains multiple different representation spaces. The self-
attention mechanism can be described as mapping a query
and a set of key-value pairs to an output, where the query, key,
value, and output are all d-dimensional vectors. The output of
each head is concatenated and results in the Ô¨Ånal output vector
once again projected.
Scaled Dot-Product Attention. The self attention used in
Transformer is also known as scaled dot-product attention.Scaled dot-product attention aims to pay more attention to the
important information of input sequence [36]. It transposes
the sequence of input vectors X= (x1;x2;:::;x n)into the
sequence of output vectors Z= (z1;z2;:::;z n), wherexi,
zi2Rdmodel . When doing self attention, Transformer Ô¨Årst
projects the input vector Xinto three vectors: the query Q, key
Kand valueVby trainable parameters WQ,WK,WV. The
attention weight is calculated using dot product and softmax
function. The output vector is the weighted sum of the value
vector:
eij=(xiWQ)(xjWK)T
p
d; (1)
ij=expe ijPn
k=1expe ij; (2)
zi=nX
j=1ij(xjWV); (3)
wheredis the dimension of each vector, and is used to scale
the dot product.
Multi-Head Attention. Multi-head attention captures dif-
ferent context with multiple individual self-attention functions.
This mechanism allows Transformer to jointly attend to infor-
mation from different representation sub-spaces. Multi-head
attention is computed after scaled dot-product attention:
MultiHead (X) =Concat (head 1;:::;head h)WO;(4)

--- PAGE 4 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4
head i=Attention (XWQ
i;XWK
i;XWV
i); (5)
whereWOindicates the learnable parameters and the param-
etersWQ
i,WK
i,WV
iare independent in each head.
2) Position-wise Feed-Forward Networks: In addition to
multi-head self-attention sub-layers, each block in encoder and
decoder also contains a fully connected feed-forward network
(FFN) sub-layer. FFN transforms the current feature space into
another space through non-linear mapping, aiming at learning
a better representation of the input. The parameters of each
position are shared. This FFN can be computed by two linear
transformations and a ReLU activation function between them.
FFN (x) =max(0;xW 1+b1)W2+b2; (6)
whereW1,W2,b1, andb2are learnable parameters.
3) Basic Blocks of Transformer: Transformer is composed
of stacked encoder-decoder blocks. Every block in the Trans-
former has a multi-head self-attention sub-layer and an FFN
sub-layer. These sub-layers are connected by the residual
connections (He et al. [47]) and layer normalization (Ba
et al. [48]). Different from the encoder block, the decoder
block has another attention sub-layers that use the key and
value matrices from the encoder instead of calculating them
from the projection of input (DTrans is a Transformer-based
architecture, so the structure of encoder and decoder block also
can refer to Fig. 2). Besides, the number of encoder-decoder
blocks will affect the performance of Transformer, and more
encoder-decoder block will increase the model size and require
more time to train
III. M ETHODOLOGY
In this section, we introduce the Transformer-based model
DTrans for automatic code change prediction. The overall
architecture of DTrans is shown in Figure 2, following the
general Transformer framework (as introduced in Section II).
In order to mitigate the out-of-vocabulary (OOV) problem, we
Ô¨Årst perform code abstraction following the prior work [32],
[49]. Also, different from the vanilla Transformer, we propose
a novel position encoding strategy, named dynamically relative
position encoding , to incorporate statement-level syntactic
information into Transformer for better capturing the local
structure of code. We elaborate on the code abstraction pro-
cess, and the proposed dynamically relative position encoding
in more details in the following.
A. Code Abstraction
Different from natural language, tokens in programming
language are more diverse since developers can deÔ¨Åne variable
names and function names in variant ways. The diversity
of identiÔ¨Åes and literals in the code leads to more serious
OOV problem during program comprehension. Thus, follow-
ing Ahmed et al. [49] and Tufano et al. [32]‚Äôs good practice,
we adopt code abstraction to reduce vocabulary size and
mitigate the OOV problem.
An example of code abstraction is shown in Figure 3.
SpeciÔ¨Åcally, we use src2abs provided by [19], [32] to
source code
abstracted source codeFig. 3: Example of code abstraction
abstract source code. It feeds sequence of source code to a Java
parser [50] which can recognize identiÔ¨Åers and literals, and
then generate and substitute a unique ID for each identiÔ¨Åer and
literal. If the identiÔ¨Åer or literal appears multiple times in the
same source code, it will be replaced with the same ID. Since
some identiÔ¨Åers and literals appear frequently in the source
code, they can be treated as keywords of the dataset [32]. The
frequent identiÔ¨Åers and literals should not be abstracted but
regarded as idioms that src2abs has provided for us.
B. Dynamically Relative Position Representations
Relation-aware Self-Attention. Using different position
embeddings for different positions helps Transformer capture
the position information of input words. However, absolute
positional encoding in the vanilla Transformer is ineffective to
capture the relative word orders [40]. To encode the pairwise
positional relationships between input elements, Shaw et al.
[40] propose the relative position encoding which models the
relation of two elements through their distance in the input
sequence. Formally, the relative position embedding between
input element xiandxjis represented as aV
ij,aK
ij2Rd.
In this way, the self attention calculated in Equ. (1) and
Equ. (3) can be rewritten as:
eij=(xiWQ)(xjWK+aK
ij)T
pdz; (7)
zi=nX
j=1ij(xjWV+aV
ij): (8)
Shaw et al. [40] also clip the maximum relative position
to a maximum absolute value of ksince they hypothesize that
precise relative position information is not useful beyond a
certain distance. Clipping the maximum distance also enables
the model to generalize to sequence lengths unseen during
training:
aK
i;j=wK
clip(j i;k); (9)
aV
i;j=wV
clip(j i;k); (10)

--- PAGE 5 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5
clip(x;k) =max( k;min (k;x)): (11)
Hence, we learn 2k+ 1relative position representations,
i.e.,wK= (wK
 k;:::;wK
k)andwV= (wv
 k;:::;wV
k)where
wK
i,wV
i2Rdmodel .
Dynamically relation position. The relative position encod-
ing [40] captures the pairwise positional relationships between
the input elements by simply Ô¨Åxing the maximum relative
distance at k. To involve the local structure information of
code, we propose to incorporate the statement-level informa-
tion into the position encoding. Different from pre-deÔ¨Åning
a maximum clipping distance k, we propose to dynamically
adjust the distance based on the length of the statement
during the relative position encoding, named as dynamically
relation position encoding. The difference between relative
position embedding and the proposed strategy is illustrated
in Figure 4, and the clipping distance kis deÔ¨Åned as 3. For
the token VAR_1 , the relative position encoding enhances the
relationship among the tokens before and behind the token
VAR_1 , which is indicated with dotted lines in the relative
position encoding method of Figure 4 (a). We hypothesize
that tokens in one statement have stronger relations with the
tokens in other statements, e.g., the token VAR_1 tends to
be weakly relevant to the token METHOD_1 compared with
the tokens in the same statement (e.g., token METHOD_2 ).
To incorporate statement-level syntactic information into the
position embedding, we propose a dynamically relation po-
sition encoding strategy. The proposed position encoding can
help Transformer pay more attention to the tokens in the same
statement (denoted as the solid lines in Figure 4 (b)) and the
tokens with a relative distance smaller than k(denoted as the
dotted lines in Figure 4 (b)). In addition, the two kinds of
attention can be superimposed in our strategy. For example,
the token METHOD_2 receives the two kinds of attention,
while the last two tokens ‚Äú )‚Äù and ‚Äú ;‚Äù do not receive the
relative position attention because their relative distance to
VAR_1 is bigger than the clipping distance k(k= 3 here).
In decoder, the current token cannot see the token behind, so
it is impossible to get the statement mask matrix. Therefore,
we only use the dynamically relative position in encoder.
Similar to padding mask and sequence mask, we propose a
statement mask operation to divide the code into a sequence
of statements. For the code example shown in Figure 5, we
illustrate the statement mask matrix for its statements s1,s2
ands3in Figure 2. SpeciÔ¨Åcally, the statement mask matrix
WLis annmatrix which records the statement-aware
information of the source code, where nis the length of code
tokens. For the tokens xi,xjin the same statement, the value
WL
ijbetween them is set as 1; otherwise the value WL
ijis set
as 0.
We compute the dynamically relative position embeddings
as below:
zi=(Pn
j=1ij(xjWV+aV
ij+aV0
)WL
ij= 1Pn
j=1ij(xjWV+aV
ij)WL
ij= 0(12)whereWL2Rnnis the statement mask matrix, and aV0
2
Rdmodel is a learnable parameter vector. We then recalculate
the attention weight to incorporate the dynamically relative
position embeddings:
eij=8
<
:xiWQ(xjWK+ak
ij+aK0
)T
pdzWL
ij= 1
xiWQ(xjWK+ak
ij)T
pdzWL
ij= 0(13)
whereaK0
2Rdmodel is a learnable parameter vector.
IV. E XPERIMENTAL SETUP
In this section, we will introduce the benchmark datasets,
implementation details, evaluation metrics and comparison
models for experimentation.
A. Benchmark Datasets
We conduct evaluation on two benchmark datasets12fol-
lowing the previous work [19], [28], Gerrit3code reviews
repository and open-source projects in GitHub (namely Git-
Projs). Gerrit includes Android [41], Google Source [42], and
Ovirt [43], while GitProjs contains 121,895 PRs commits from
GitHub open-source projects. We classify all projects in the
datasets into two levels, i.e., small level Msmall and medium
levelMmedium , according to the tokens numbers of original
code.Msmall andMmedium contain 0-50 tokens and 500-100
tokens in each piece of original code, respectively. The two
benchmark datasets are partitioned into training set (80%),
validation set (10%) and test set (10%) following the prior
studies, with detailed statistics shown in Table I.
TABLE I: Statistics of the two benchmark datasets.
Dataset Msmall Mmedium
GerritGoogle 2,165 2,286
Android 4,162 3,617
Ovirt 4,456 5,088
All 10,783 10,991
GitProjs 58,350 65,545
B. Implementation and Supporting Tools/Platform
Data Preparation. We Ô¨Årst abstract the source code according
to Section III-A. Then, we compute the statement mask ma-
trices for the two benchmark datasets, respectively. However,
computing the matrices for a large amount of code is time-
consuming and inefÔ¨Åcient, so we convert the computation
into a series of matrix operations to fully use the computing
resources of GPU and improve the efÔ¨Åciency (Section IV-C
shows details). We test the time cost of the matrix computation
before and after the acceleration, respectively. The results
on the training set of GitProjs show that it reduces the
computation time from 10 minutes to 7 seconds, indicating
the efÔ¨Åciency of the acceleration operation.
Hyper-parameters Setting. DTrans is composed of 6 hidden
layers and 8 heads. The hidden layer size of the model and
1https://sites.google.com/view/learning-codechanges
2https://sites.google.com/view/learning-Ô¨Åxes
3https://www.gerritcodereview.com

--- PAGE 6 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6
protected void METHOD_1 ( ) {super . METHOD_1 ( ) ;VAR_1 . METHOD_2 ( ) ;VAR_2 . METHOD_3 ( ) ;}(a)super.METHOD_1();VAR_1.METHOD_2();(b)super.METHOD_1();VAR_1.METHOD_2();
Fig. 4: Illustration of the token relations for relative positions (a) and dynamically relative positions (b). For each relative
position, the clipping distance kis assumed as 3. Only the second and third statements of the source code are illustrated here
for simplicity. Solid lines and dotted lines indicate different token relations. The dotted line represents the relative distance is
smaller than kand the solid line represents the tokens in the same statement with VAR_1 .
public static long METHOD_1( ) {long a ;long b ;long c ;a = INT_1;b = INT_2 ;c = a + b ;return c ;}S0S1S2S3S4S5S6S7S8
Fig. 5: Example of source code to represent statement mask
matrix. Note that we only take statements s1,s2, ands3as
example for illustrating the statement mask matrix in Figure 2.
the size of every head are deÔ¨Åned as 512 and 64, respectively.
We train DTrans using Adam optimizer [51] with an initial
learning rate of 1:0and use warm-up [52] to optimize the
learning rate. We set the mini-batch size as 32 and the dropout
as 0.1 during training. DTrans is trained for a maximum
of 20,000 steps and performed early stops if the validation
performance does not improve during 2,000 steps. We also
use beam search during inference and set the beam size as 10.
Platform. Our experiments are conducted on a single Tesla
p100 GPU for about 10 hours for Mmedium datasets and
5 hours for Msmall datasets for both benchmark datasets,
respectively.
C. GPU Acceleration
Algorithm 1 shows how we use matrix operations to replace
inefÔ¨Åcient nested loops during computing the statement mask
matrix.
The inputX=x1;x2;:::;x n2Rnis the sequence of
source code token. We Ô¨Årst compute I=i1;i2;:::in2Rn,
which is a vector consisting of 0 and 1, from X. The rule
for generating Iis : ifxm2Xis an identiÔ¨Åer, the valueofim2Iis 1; otherwise, it is 0 (Line 2). We can get
WA2Rnby multiplying Iand the lower triangular matrix
WM(WM2Rnn) (Line 3-4). Next we will repeat WAto
getWB2Rnn(Line 5) and can Ô¨Ånd that if i;jare in
the same statement, WB
i;j=WB
j;i, and vice versa. So Ô¨Ånally,
ifWB
i;j=WB
j;i, we letWB
i;j=WB
j;i= 1; otherwise it is
WB
i;j=WB
j;i= 0. In this step (Line 6-10), we also use the
matrix operations completely instead of nested loops, so this
step is also efÔ¨Åcient.
Algorithm 1 Computation of the statement mask matrix
Input:X= (x1;x2;:::;x n)2Rn, which is a sequence of
source code tokens.
Output: the statement mask matrix, WL2Rnn
1:function COMPUTE SMM(X)
2:I find the identifiers from X
//WMis lower triangular matrix, WM2Rnn
3:WM lower triangular matrix
4:WA I(WM)
// repeatWAntimes to get WB2Rnn
5:WB repeat (WA)
6:WBT (WB)T
7:WS1=jWB WBT 1j jWB WBTj
8:WS2=jWBT WB 1j jWB WBTj
9:WS= (WS1+WS2)=2
10: returnWS
11:end function
D. Evaluation Metrics
We evaluate the performance of DTrans in code editting
using three popular metrics, including Exact Match [19], [32],
BLEU-4 [53] and ROUGE-L [54].

--- PAGE 7 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7
Exact Match computes the number and percentage of
predicted code changes that exactly match the changed code
in the test sets.
BLEU-4 is a widely-used metric in natural language pro-
cessing and software engineering Ô¨Åelds to evaluate the quality
of generated texts, e.g., machine translation, code comment
generation, and code commit message generation [53], [55],
[56]. It computes the frequencies of the co-occurrence of n-
grams between the ground truth ^yand the generated sequence
yto judge the similarity:
BLEU-N = b(y;^ y)exp NX
n=1nlogpn(y;^y)!
;
where b(y;^ y)indicates the brevity penalty, and pn(y;^y)and
nrepresent the geometric average of the modiÔ¨Åed n-gram
precision and the weighting parameter, respectively. We use
corpus-level BLEU-4, i.e., N = 4 for evaluation since it is
demonstrated to be more correlated with human judegments
than other evaluation metrics [57].
ROUGE-L is commonly used in natural language transla-
tion [54], and is a F-measure based on the Longest Common
Subsequence (LCS) between candidate and target sequences,
where the LCS is a set of words appearing in the two
sequences in the same order.
ROUGE -L= 
1 +2
RlcsPlcs
Rlcs+2Plcs;
whereRlcs=LCS (X;Y )
len(Y)andPlcs=LCS (X;Y )
len(X).XandYde-
note candidate sequence and reference sequence, respectively.
LCS (X;Y )represents the length of the longest common sub-
sequence between XandY.
E. Comparison Model
We compare DTrans with three baseline models, including
Tufano el al. (an NMT-based model) [19], [32], SequenceR
[29] and CODIT [20]. Tufano el al. [19], [32] employ a
typical encoder-decoder model LSTM to edit method-level
code, where the input is a sequence of code tokens. SequenceR
[29] is also LSTM-based encoder-decoder model, but it uses
copy mechanism to copy code tokens from the source code
during decoding. The input of SequenceR is also code token
sequence. CODIT [20] is a tree-based model, which uses the
ASTs of source code as input and predicts code edit at the
AST level.
V. E XPERIMENT RESULTS
In this section, we aim at verifying the effectiveness of the
proposed approach, speciÔ¨Åcally by answering the following
research questions:
RQ1: What is the performance of the proposed approach
compared with the baseline models?
RQ2: What is the impact of the proposed dynamically
relative position encoding on the model performance?
RQ3: What is the effectiveness of DTrans in generating
multi-lines code change prediction?RQ4: Whether DTrans can accurately locate the lines to
edit for code change prediction?
RQ5: What is the impact of different parameters on the
model performance?
RQ6: What is the performance of DTrans in cross-project
setting?
SpeciÔ¨Åcally, RQ1 is to evaluate the performance of the
proposed model compared with baselines, including token-
based models and tree-based models. To verify the advantage
of the proposed dynamically relative position embedding,
we compare DTrans with Transformer [36] and Transformer
with relative position embedding (namely Transformer relative )
[40] in RQ2. For RQ3, since we Ô¨Ånd that more than 30%
of the code samples in the datasets need multi-line code
changes, the research question is to evaluate the capacity of
DTrans for generating multiple-line code changes. RQ4 is to
validate the ability of locating lines to edit. Finally, since
the hyper-parameters can impact the performance of DTrans,
RQ5 discusses the hyper-parameter conÔ¨Ågurations. RQ6 is to
evaluate the performance of DTrans in cross-project setting.
A. Answer to RQ1: Performance of the proposed DTrans
1) Comparison with token-based models: Table II presents
the experimental results of our proposed model and the token-
based baselines on the benchmark datasets. From the table,
we can observe that DTrans performs better than the token-
based baselines in predicting exact-matched code changes for
all the datasets. For example, DTrans successfully generates
489 exact-matched code changes in Gerrit for Msmall and
409 forMmedium , while Tufano et al. only generates 388 and
334 exact-matched code changes, respectively, and SequenceR
only generates 405 and 284 exact-matched code changes for
Msmall andMmedium . Compared with Tufano et al. , DTrans
outperforms 26.04% and 22.45% for Msmall andMmedium ,
respectively. Compared with SequenceR, DTrans outperforms
20.74% and 44.01% for Msmall andMmedium respectively.
For GitProjs, SequenceR outputs 2,255 code changes that
are consistent with the ground truth for Msmall and 1,214
forMmedium , while DTrans successfully produces 2,573 and
1,625 for the two types of datasets, respectively. Besides, the
ground truth is human-writing code [19], [28], so the higher
scores of BLEU-4 and ROUGE-L represent that the results
generated by DTrans are semantically similar to the human-
writing code. For example, DTrans increases the performance
of SequenceR by 2.59% and 0.66% in Google Mmedium with
respect to the BLEU-4 and ROUGE-L metrics, respectively.
The results demonstrate the effectiveness of the proposed
DTrans over the token-based models.
2) Comparison with tree-based models: Because CODIT
does not provide the source code for data processing, and
the data processing process of CODIT is very complex, we
directly compare it on the code change dataset used by
CODIT. Table IV shows the experimental results of DTrans
and CODIT. In the abstracted code change dataset provided
by CODIT, the result of CODIT is not good. Compared with
SequenceR, CODIT is lower than SequenceR by 17.80%,

--- PAGE 8 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8
TABLE II: Comparison results of DTrans and token-based baselines. The bold indicates the best results.
Dataset ApproachMsmall Mmedium
Exact Match BLEU-4 ROUGE-L Exact Match BLEU-4 ROUGE-L
GerritGoogleTufano et al. 20/216(9.25%) 55.29% 83.81% 17/228(7.45%) 75.12% 91.46%
SequenceR 55/216(25.46%) 76.87% 93.13% 43/228(18.85%) 89.35% 96.85%
DTrans 58/216(26.85%) 78.09% 93.30% 63/228(27.63%) 91.67% 97.49%
AndroidNMT-based 79/416(18.99%) 64.29% 88.16% 76/361(21.05%) 87.33% 96.14%
Tufano et al. 157/416(37.74%) 83.86% 95.64% 83/361(22.99%) 90.67% 97.48%
DTrans 174/416(41.82%) 84.63% 95.64% 112/361(31.02%) 91.80% 97.81%
OvirtTufano et al. 113/445(25.39%) 73.60% 91.14% 102/509(20.03%) 82.66% 94.07%
SequenceR 173/445(38.87%) 85.10% 95.79% 167/509(32.80%) 91.69% 97.52%
DTrans 204/445(45.84%) 86.81% 95.81% 210/509(41.25%) 92.82% 97.60%
OverallTufano et al. 388/1,077(36.02%) 82.47% 94.57% 334/1,098(30.41%) 91.57% 97.53%
SequenceR 405/1,077(37.60%) 85.82% 95.69% 284/1,098(25.86%) 92.04% 97.57%
DTrans 489/1,077(45.40%) 86.82% 96.10% 409/1,098(37.24%) 92.79% 97.85%
GitProjsTufano et al. 2,119/5,835(36.31%) 85.84% 96.06% 1,166/6,545(17.82%) 90.97% 97.58%
SequenceR 2,255/5,835(38.64%) 86.72% 96.33% 1,214/6,545(18.54%) 91.03% 97.62%
DTrans 2,573/5,835(44.09%) 87.14% 96.55% 1,625/6,545(24.82%) 91.56% 97.81%
TABLE III: Comparison results of DTrans, Transformer and Transformer relative . The bold indicates the best results. ‚Äú*‚Äù denotes
statistical signiÔ¨Åcance in comparison to the baselines(i.e.,two-sided t-test withp-value<0.05)
Dataset ApproachMsmall Mmedium
Exact Match BLEU-4 ROUGE-L Exact Match BLEU-4 ROUGE-L
GerritGoogleTransformer 40/216(18.52%)73.20%91.89%38/228(16.66%)88.49%97.11%
Transformer relative 58/216(26.85%) 78.06% 93.04% 61/228(26.75%) 91.08% 97.38%
DTrans 58/216(26.85%) 78.09% 93.30% 63/228(27.63%) 91.67% 97.49%
AndriodTransformer 146/416(35.09%)83.00%95.29% 97/361(26.86%)91.42% 97.77%
Transformer relative 173/416(41.58%) 84.37% 95.58% 99/361(27.42%)91.66% 97.62%
DTrans 174/416(41.82%) 84.63% 95.64% 112/361(31.02%) 91.80% 97.81%
OvirtTransformer 182/445(40.89%)83.73%94.84%172/509(33.79%)92.16%97.42%
Transformer relative 189/445(42.47%)84.82%95.29%188/509(36.93%)92.56% 97.55%
DTrans 204/445(45.84%) 86.81% 95.81% 210/509(41.25%) 92.82% 97.60%
OverallTransformer 428/1,077(39.74%)84.37%95.35%355/1,098(32.33%)92.19%97.70%
Transformer relative 472/1,077(43.82%) 86.18% 95.95% 388/1,098(35.33%) 92.29%97.72%
DTrans 489/1,077(45.40%) 86.82% 96.10% 409/1,098(37.24%) 92.79% 97.85%
GitProjsTransformer 2,503/5,835(42.89%)86.49%96.40%1,509/6,545(23.05%)91.29%97.73%
Transformer relative 2,540/5,835(43.53%) 87.01% 96.47% 1,574/6,545(24.04%)91.56% 97.79%
DTrans 2,573/5,835(44.09%) 87.14% 96.55% 1,625/6,545(24.82%) 91.56% 97.81%
4.59%, and 3.41% with respect to the Exact Match, BLEU-
4 and ROUGE-L metrics, respectively. DTrans improves the
performance of SequenceR by 16.10%, 2.38% and 0.83%
regarding the three metrics respectively.
TABLE IV: Comparison results of DTrans and tree-based
baseline CODIT. The bold indicates the best results.
‚Äú*‚Äù denotes statistical signiÔ¨Åcance in comparison to the
baselines(i.e.,two-sided t-test withp-value<0.05)
Method Exact Match BLEU-4 ROUGE-L
Tufano et al. 1,898/5,143(36.90%)75.54%93.03%
SequenceR 2,130/5,143(41.41%)78.02%93.87%
CODIT 1,808/5,143(35.15%)74.59%90.77%
Transformer 2,293/5,143(44.58%)77.87%93.98%
Transformer relative 2,426/5,143(47.17%)79.31%94.56%
DTrans 2,473/5,143(48.08%) 79.88% 94.65%
Answer to RQ1: In summary, DTrans can more accurately
predict code changes, and the generated code changes are
more semantically relevant to the ground truth.
B. Answer to RQ2: Impact of the proposed dynamically rela-
tive position encoding on the model performance
To evaluate the effectiveness of the proposed dynamically
relative position encoding strategy, we compare DTrans withthe original Transformer [36] and Transformer with relative
position (Transformer relative ) [40]. We reproduce their experi-
ments under the same hyper-parameter settings as DTrans for
fair comparison.
Table III shows the experimental results, we Ô¨Ånd that
Transformer performs better than Tufano et al.. In more details,
Transformer improves the performance of Tufano et al. by
6.3%-123.62%, 0.35%-32.39%, 0.15%-9.64% regarding the
three metrics on the two benchmark datasets, respectively.
Besides, Transformer can generate 4,795 code changes that
exactly match the ground truth on the two benchmark datasets,
which outperforms 15.32% than SequenceR. The experimental
results suggest that Transformer-based models can predict
more effective code edits than token-based models. Moreover,
Transformer relative performs better than the vanilla Transformer
in most cases, which indicates that the relative position en-
coding in Transformer is more effective in capturing the code
edit patterns. Finally, DTrans achieves better performance than
Transformer relative , with increase rates at 1.28% and 3.24% in
terms of the exact match metric on the Msmall andMmedium
datasets, respectively. The results indicate the efÔ¨Åcacy of the
proposed dynamically relative position encoding strategy.
Answer to RQ2: In summary, the Transformer-based mod-
els outperform baselines. The statement-level syntactic infor-

--- PAGE 9 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9
mation for position encoding facilitates more accurate code
change prediction.
C. Answer to RQ3: Effectiveness of code change prediction
for multiple lines
According to the statistics [29], [49], most edits are ac-
complished through a single-line code change. However, some
edits still need multi-line code changes in practice. We analyze
the multi-line code changes in the Gerrit dataset [58] in
Table V, and observe that nearly 35% edits involve changes
of more than one line of code.
TABLE V: Statistics of the edits that need multi-line code
changes.
Dataset Msmall Mmedium
GerritGoogle 74/216(34.26%) 103/228(45.18%)
Android 177/416(42.55%) 162/361(44.88%)
Ovirt 130/445(29.21%) 171/509(33.60%)
Overall 381/1,077(35.38%) 436/1,098(39.71%)
We then investigate the effectiveness of DTrans in producing
multi-line code changes, with the evaluation results shown in
Figure 6. As illustrated in the table, DTrans achieves the best
performance among all the baselines in multi-line code change
prediction. For example, DTrans overall produces 42.25%
exact-matched code changes for Msmall projects and 33.02%
forMmedium projects, while Tufano et al. only outputs 30.97%
and 23.16% for the two types of datasets, respectively. The
results indicate the usefulness of DTrans in multi-line code
prediction. We can also observe that compared with Tufano
et al., the improvement of DTrans on the Mmedium projects
(42.57%) is more signiÔ¨Åcant than that on the Msmall projects
(36.42%). We then analyze the average lines of code in the
test sets of the Msmall andMmedium projects, with the results
shown in Table VI. We can Ô¨Ånd that the code in the Mmedium
projects are longer than that in the Msmall projects on average.
Therefore, we suppose the signiÔ¨Åcant performance of DTrans
on theMmedium projects may be attributed to that DTrans is
more effective for predicting the changes of long code snippets
than baseline models.
TABLE VI: Statistics of the average line of code in test sets
of theMsmall andMmedium projects.
Dataset Msmall Mmedium
GerritGoogle 4.47 9.42
Android 4.89 10.31
Ovirt 4.41 9.01
Overall 4.60 9.52
Answer to RQ3: In summary. DTrans demonstrates the
superior ability of accurately generating multiple-line code
changes, and has a great improvement over the baselines.
D. Answer to RQ4: Accuracy of DTrans in locating lines to
edit for predicting code change
Locating correct lines to edit is the premise of the accurate
code changes in the subsequent step. So in this researchTABLE VII: Comparison results in locating lines to edit of
DTrans with other techniques. The bold fonts indicate the best
results.
Dataset Approach Msmall Mmedium
GoogleTufano et al. 63/216(29.16%) 45/228(19.73%)
Sequencer 114/216(52.77%) 95/228(41.66%)
Transformer 91/216(42.12%) 84/228(36.84%)
Transformer relative 110/216(50.92%) 118/228(51.75%)
DTrans 116/216(53.70%) 118/228(51.75%)
AndroidTufano et al. 164/416(39.42%) 138/361(38.22%)
Sequencer 255/416(61.29%) 163/361(45.15%)
Transformer 235/416(56.49%) 170/361(47.09%)
Transformer relative 255/416(61.29%) 179/361(49.58%)
DTrans 258/416(62.01%) 189/361(52.35%)
OveritTufano et al. 212/445(47.60%) 189/509(37.13%)
Sequencer 303/445(68.08%) 311/509(61.10%)
Transformer 287/445(64.49%) 299/509(58.74%)
Transformer relative 302/445(67.86%) 307/509(60.31%)
DTrans 315/445(70.78%) 328/509(64.44%)
OverallTufano et al. 666/1,077(61.83%) 587/1,098(53.46%)
Sequencer 718/1,077(66.66%) 598/1,098(54.46%)
Transformer 690/1,077(64.06%) 601/1,098(54.73%)
Transformer relative 727/1,077(67.50%) 630/1,098(57.37%)
DTrans 757/1,077(70.28%) 648/1,098(59.01%)
question, we analyze whether the proposed approach can
accurately predict which lines to edit.
Table VII shows the experimental results of locating the
lines for editing. We can observe that DTrans performs better
than other techniques on all projects. For example, SequenceR
can only locate 66.66% correct lines for Msmall and 54.46%
correct lines for Mmedium , while DTrans can locate 70.28%
and 59.01%, respectively. This observation demonstrates that
DTrans can obtain more contextual information than other
techniques.
Answer to RQ4: In summary, DTrans can greatly outper-
form the baselines in locating the lines to change (e.g.,
achieving 8.35% higher accuracy than the best baseline).
E. Answer to RQ5: Impact of the model parameters
In this section, we extend our experiments with different
parameters to investigate the inÔ¨Çuence of internal factors of
DTrans.
Figure 7 (a) presents the impact of the clipping distance
(k) on the effectiveness of DTrans using other default conÔ¨Åg-
urations (DeÔ¨Åned in Section.III-B). In this Ô¨Ågure, the xaxis
presents various clipping distances, while the yaxis presents
the values of different evaluation metrics. We can Ô¨Ånd that the
clipping distance does not impact the DTrans effectiveness
much. For example, the largest performance difference among
different clipping distances is within 2% for all evaluation
metrics. Since DTrans achieves good performance on the
datasets when the clipping distance is 32, we choose the
parameter as 32 during experimentation.
Figure 7 (b) presents the impact of the number of encoder-
decoder block ( l) on the effectiveness of DTrans using other
default conÔ¨Ågurations (DeÔ¨Åned in Section.II-B). In this Ô¨Ågure,
thexaxis presents different number of encoder-decoder block,
while theyaxis presents the values of different evaluation
metrics.From the Ô¨Ågure, we observe that the number of
encoder-decoder block has a signiÔ¨Åcant impact on the model.
For example, in GitProjs, the Exact Match of 2-blocks DTrans

--- PAGE 10 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10
Benchmark DatasetExactMatch9.46%*14.12%*20.77%*30.97%*22.97%31.63%*30.76%33.33%*16.22%*31.64%*29.23%*38.58%*21.62%34.46%35.38%41.46%24.32%38.41%35.38%42.25%
0%5%10%15%20%25%30%35%40%45%
GoogleAndroidOvirtAllTufano et al.SequenceRTransformerTransformerreltiveDTransrelative
(a)Msmall projects
Benchmark DatasetExactMatch8.73%*17.28%*9.94%*23.16%*20.38%*18.51%*23.97%*17.88%*16.50%*24.07%*25.73%27.52%*30.10%25.31%26.90%31.42%32.04%27.77%29.23%33.02%
0%5%10%15%20%25%30%35%
GoogleAndroidOvirtAllTufano et al.SequenceRTransformerTransformerreltiveDTransrelative
(b)Mmedium projects
Fig. 6: Comparison results of generating multi-lines patch on the Msmall (a) andMmedium (b) projects. ‚Äú*‚Äù denotes statistical
signiÔ¨Åcance in comparison to the baselines (i.e., two-sided t-test withp-value<0.05).
43.0%44.0%45.0%46.0%4816326442.0%43.0%44.0%45.0%4816326485.0%86.0%87.0%4816326486.8%87.0%87.2%87.4%87.6%4816326495.4%95.6%95.8%96.0%96.2%4816326496.4%96.5%96.6%96.7%48163264Gerrit-AllGitProjsROUGE-LBLEU-4Exact Match
(a) Clipping distance k.
38.0%40.0%42.0%44.0%46.0%246841.0%42.0%43.0%44.0%45.0%246882.0%84.0%86.0%88.0%246886.0%86.5%87.0%87.5%88.0%246895.0%95.5%96.0%96.5%246896.3%96.4%96.5%96.6%96.7%2468Exact MatchBLEU-4ROUGE-LGerrit-AllGitProjs (b) Number of encoder-decoder block lof Transformer.
Fig. 7: Impact of key parameters on the model performance.
is only 42.26%, while the Exact Match of 6-blocks DTrans
is 44.09%. Besides, more encoder-decoder blocks do not
mean better performance. For example, 6-blocks DTrans is
better than the 8-blocks DTrans in Gerrit-All. Moreover, more
encoder-decoder block will increase the model size and require
more time to train. In terms of overall considerations, DTrans
with 6 encoder-decoder blocks is a good option.
Answer to RQ5: In summary, the experimental results can
be inÔ¨Çuenced by parameter conÔ¨Åguration. Moreover, the
clipping distance has little inÔ¨Çuence on DTrans, but thenumber of layers has much inÔ¨Çuence on DTrans.
F . Answer to RQ6: erformance of DTrans in cross-project
setting
In this section, we train the models in one project and test
them in another project to simulate a more practical setting. We
use the Gerrit dataset which contains three different projects
for the evaluation. We adopt the best Transformer-based base-
lines for comparison. The results are shown in Table VIII.
We can observe that DTrans consistently performs better than

--- PAGE 11 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11
Transformer and Transformer relative in the cross-project setting.
For example, when we train the models in Google Msmall
and then test them in Android Msmall , DTrans can gener-
ate 17 exact-matched code changes, while Transformer only
generates 11 exact-matched code changes. Overall, DTrans in-
creases the performance of the Transformer-based baselines by
0200%,0.035.66%,0.091.30% with respect to the Exact
Match, BLEU-4, ROUGE-L metrics, respectively. We can also
Ô¨Ånd that despite the good performance of DTrans in cross-
project setting, it presents obvious decline compared with the
in-project performance, e.g., the exact match score drops by
more than 80%. The Transformer-based baselines show the
similar trend. The phenomenon is reasonable since the edit
patterns of different projects may be greatly different.
Answer to RQ6: In summary, DTrans performs better than
the baseline models in the cross-project setting. However, the
performance of all the models drops greatly comparing with
the in-project setting, indicating that cross-project evaluation
is a more challenging setting for the code edit task.
VI. C ASE STUDY
To evaluate the performance of DTrans in predicting accu-
rate code edits, we select three cases from benchmark datasets
as shown in Figure 8.
Figure 8 (a) presents an example of code edit operation
prediction. The method addSlices lacks an object to con-
duct the method function slices.addAll(slices) , so
the correct edit operation is to add an object this . However,
Tufano et al. mistakenly predicts the operation is to change the
return from true tofalse . Similarly, SequenceR incorrectly
returns the variable slices . DTrans successfully predicts the
correct edit operation and adds this to point to the variable
inside the class.
In Figure 8 (b), the original code needs to remove
grade from curve , but it does not check whether the
variable grade isnull . The correct edit operation is
to check variable grade before executing remove . Tu-
fano et al. does not check variable grade and just refac-
tors original code. SequenceR predicts the correct operation
method but the incorrect operation object. It checks null
forcurve.remove(grade) rather than variable grade .
DTrans successfully predicts both the correct operation method
and operation object. It checks whether the variable grade
isnull before executing curve.remove(grade) .
In addition, DTrans does not always predict accurate
code changes. In Figure 8 (c), the original code needs a
return statement because the modiÔ¨Åer void does not
appear in the method deÔ¨Ånition. Tufano et al. successfully
addsreturn before getCFlags() . Sequencer mistakenly
thinks that the original code should be a static function, so
it inserts the modiÔ¨Åer static . For DTrans, it successfully
adds the return token, but incorrectly changes the API from
java.lang.Iterable tojava.lang.Set , which is a
non-existent interface. This example motivates us to create
an API knowledge base to facilitate the code edit process in
future.VII. T HREAT TO VALIDITY
Internal validity is mainly about the hyper-parameter con-
Ô¨Åguration we adopted in our DTrans model. To reduce this
threat, we conduct an experiment to study the impact of
conÔ¨Åguration, and we explain in Section V-E about how hyper-
parameters inÔ¨Çuence our model‚Äôs performance.
Construct validity is mainly the suitability of our evalu-
ation metrics. To reduce this risk, we additionally introduce
BLEU-4 (Bilingual evaluation understudy in 4-gram) [53] and
ROUGE-L (Recall-Oriented Understudy for Gisting Evalua-
tion in Longest Common Sub-sequence) [54] to evaluate the
effectiveness of our approaches, which can well simulate the
non-trivial coding activities in evaluating generated code.
External validity is mainly concerned with whether the
performance of our DTrans techniques can still effective in
other datasets. To reduce these threats, we additional select
2.3 million pair-wise code changes generated from GitHub
open-source projects [32] to evaluate the effectiveness of our
approach. And experimental results demonstrate the effective-
ness of our approach (in Section V-A). To further reduce the
threats, we are planning to collect more open-source projects
to evaluate our approach. Besides, the quality of the datasets
may be another threat. In this paper, we simply follow the
previous work [19], [20] by directly adopting the benchmark
datasets without further cleaning. As illustrated in Sun et
al. [59], high-quality datasets are very important for DL
models. We will study the quality of the datasets in future
work.
VIII. R ELATED WORK
Related works focus on two key aspects: position represen-
tations of Transformer and automatic code edit.
A. Position Representations of Transformer
Unlike RNN [60], which incorporates inductive bias by
successively loading the input tokens, Transformer is less
position-sensitive [36]. It is critical to incorporate position
encoding into the Transformer.
Absolute Position Representations. Vaswani et al. [36]
proposed Transformer and trigonometric function to calculate
positional information for each token, but the positional infor-
mation cannot change, while Devlin et al. [61] and Liu et al
[62] use parameter matrix to calculate positional information.
Liu et al. [63] proposed FLOATER, which models position
encoding as a continuous dynamical system and admit the
standard sinusoidal position encoding as a special case, making
more Ô¨Çexible in theory. Dehghani et al. [64]and Lan et al. [65]
found that injecting the position information into layers can
improve performance of Transformer in some tasks.
Relative Position Representations. Relative position Rep-
resentations take the relative distance into calculating attention
rather than absolute position, which performs more effective
and Ô¨Çexible. Shaw et al. [40] Ô¨Årst proposed the concept of
relative position embedding and its application scope. Yang
et al. [66] and Dai et al. [67] improved the relative position
embedding to boost the effectiveness. Raffel et al. [68] and
Ke et al. [69] evaluated the effective of ‚Äúinput-position‚Äù and

--- PAGE 12 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14
TABLE VII: Qualitative analysis of DTrans model and the baseline models on two examples.
Example (a):
Original Code :
public boolean addSlices(java.util.Collection<it.polito.atlas.alea2.Slice> slices) {
slices.addAll(slices);
return true ;
}
Tufano et al. :
public boolean addSlices(java.util.Collection<it.polito.atlas.alea2.Slice> slices) {
slices.addAll(slices);
return false ;
}
SequenceR :
public boolean addSlices(java.util.Collection<it.polito.atlas.alea2.Slice> slices) {
slices.addAll(slices);
return slices ;
}
Dtrans :
public boolean addSlices(java.util.Collection<it.polito.atlas.alea2.Slice> slices) {
this .slices.addAll(slices);
return true ;
}
Example (b):
Original Code :
public boolean remove(model.curve.Grade grade) {
return curve.remove(grade);
}
Tufano et al. :
public boolean remove(model.curve.Grade grade) {
boolean result=curve.remove(grade);
return result;
}
SequenceR :
public boolean remove(model.surve.Grade grade) {
return !(curve.remove(grade).isEmpty());
}
Dtrans :
public boolean remove(model.surve.Grade grade) {
if(grade != null) {
return curve.remove(grade); }
else {
return false ;}
}
Example (c):
Original Code :
public java.lang.Iterable<java.lang.String> getCppFlags() {
getCFlags(); }
Tufano et al. :
public java.lang.Iterable<java.lang.String> getCppFlags() {
return getCFlags(); }
SequenceR :
public static java.lang.Iterable<java.lang.String> getCppFlags() {
getCFlags(); }
Dtrans :
public java.lang. Set <java.lang.String> getCppFlags() {
return getCFlags(); }
Fig. 8: Qualitative analysis of our DTrans model and the baseline models on three examples.

--- PAGE 13 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13
TABLE VIII: Cross-project comparison results of DTrans and Transformer-based baselines. The bold indicates the best results.
DatasetApproachMsmall Mmedium
Training Test Exact Match BLEU-4 ROUGE-L Exact Match BLEU-4 ROUGE-L
GoogleAndroidTransformer 11/416(2.64%) 58.38% 86.06% 0/361(0%) 71.58% 90.11%
Transformer relative 16/416(3.84%) 62.33% 87.52% 4/361(1.1%) 80.19% 91.82%
DTrans 17/416(4.08%) 63.92% 87.74% 7/361(1.93%) 80.23% 91.92%
OvirtTransformer 13/445(2.92%) 54.67% 81.53% 0/509(0%) 64.23% 83.72%
Transformer relative 20/445(4.49%) 59.49% 82.44% 0/509(0%) 71.61% 84.94%
DTrans 22/445(4.94%) 61.10% 82.93% 1/509(0.20%) 71.90% 85.06%
AndroidGoogleTransformer 6/216(2.77%) 59.66% 83.69% 2/228(0.87%) 73.35% 87.81%
Transformer relative 8/216(3.70%) 62.37% 83.82% 2/228(0.87%) 75.30% 87.83%
DTrans 10/216(4.62%) 63.28% 84.09% 2/228(0.87%) 76.15% 88.26%
OvirtTransformer 21/445(4.71%) 61.20% 82.86% 0/509(0%) 68.23% 84.61%
Transformer relative 22/445(4.94%) 64.84% 83.61% 1/509(0.19%) 71.42% 85.02%
DTrans 30/445(6.74%) 64.86% 83.70% 3/509(0.58%) 72.05% 85.27%
OvirtGoogleTransformer 6/216(2.77%) 56.78% 82.29% 0/228(0%) 62.90% 81.34%
Transformer relative 10/216(4.62%) 57.99% 82.78% 1/228(0.43%) 65.13% 82.54%
DTrans 10/216(4.62%) 59.06% 82.86% 1/228(0.43%) 68.82% 83.60%
AndroidTransformer 25/416(6.01%) 60.94% 86.70% 1/361(0.27%) 72.24% 88.60%
Transformer relative 23/416(5.52%) 62.60% 87.02% 4/361(1.10%) 73.38% 88.91%
DTrans 25/416(6.01%) 63.75% 87.50% 5/361(1.38%) 76.64% 90.07%
‚Äúposition-input‚Äù and remove them from Transformer. He et
al. [70] evaluated the absolute and relative position embed-
ding and proved the usability of relation position embedding.
Since these approaches are developed for natural language
processing, they are unable to capture the statement-level
information included in code; while our proposed dynamically
relative position encoding strategy is speciÔ¨Åcally designed for
involving the statement-level syntax information of source
code.
B. Automatic Code Edit
Code edit throughout the program development and main-
tenance relates to various behaviors, e.g., automatic program
repair [71]‚Äì[75], API-related update [26], and code refactoring
[25], [76]. In recent years more and more proposed works
adapted Deep Learning (DL) techniques in automatic code
edit [20], [24], [75], [77], aiming at automatically predicting
code changes using a data-driven approach. Tufano et al.
[19] applied Neural Machine Translation (NMT) techniques
to generate target code at the method level. They treated code
as natural language, converting it into tokens and using code
abstraction to overcome the issue of out-of-vocabulary . Chen
et al. [29] presented the SequenceR, an NMT-based approach,
which uses the attention mechanism and outperforms Tufano et
al. [19]. Chakraborty et al. [20] presented CODIT, a tree-based
NMT model for predicting concrete source code changes and
learning code change patterns in the wild, and it is the state-
of-the-art NMT-based model in code edit. Above approaches
ignore the statement-level information, so we propose DTrans,
a novel Transformer-based approach, which explicitly incor-
porates the statement-level syntactic information for better
capturing the local structure of code, to predict code changes.
IX. C ONCLUSION AND FUTURE WORK
In this paper, we introduced DTrans, a Transformer-based
technique that can predict code changes from merged pull re-
quests codes from developers. To better capture the statement-
level information of code, DTrans is designed with dynam-
ically relative position encoding in multi-head attention ofTransformer. Compared with other DL-based techniques such
as neural machine translation (NMT), DTrans can capture
the syntactic information, which makes the generated code
changes higher-quality. The experimental results show that
DTrans can more accurately generate program changes in
automatic code edit.
Our experiments also demonstrate the difÔ¨Åculties in the
cross-project code edit task. In the future, we plan to inves-
tigate the cross-project challenges and incorporate more se-
mantic information(e.g., Control-Flow Graph, Abstract Syntax
Tree) to increase our capacity in code editing for the cross-
project task.
X. A CKNOWLEDGMENTS
This research was supported by National Natural
Science Foundation of China Grant under project No.
62002084, 61872110, 61672191, Stable support plan for
colleges and universities in Shenzhen under project No.
GXWD2020 1230155427003-20200730101839009, the
Major Key Project of PCL (Grant No. PCL2022A03,
PCL2021A02, PCL2021A09), Guangdong Provincial Key
Laboratory of Novel Security Intelligence Technologies
(2022B1212010005), the Science and Technology Program
of Guangzhou, China (202103050004).
REFERENCES
[1] F. Ferreira, L. L. Silva, and M. T. Valente, ‚ÄúSoftware engineering meets
deep learning: a mapping study,‚Äù in Proceedings of the 36th Annual
ACM Symposium on Applied Computing , 2021, pp. 1542‚Äì1549.
[2] Y . Yang, X. Xia, D. Lo, and J. Grundy, ‚ÄúA survey on deep learning for
software engineering,‚Äù arXiv preprint arXiv:2011.14597 , 2020.
[3] H. F. Eniser, S. Gerasimou, and A. Sen, ‚ÄúDeepfault: Fault localization
for deep neural networks,‚Äù in International Conference on Fundamental
Approaches to Software Engineering . Springer, 2019, pp. 171‚Äì191.
[4] X. Li, W. Li, Y . Zhang, and L. Zhang, ‚ÄúDeepÔ¨Ç: Integrating multiple fault
diagnosis dimensions for deep fault localization,‚Äù in Proceedings of the
28th ACM SIGSOFT International Symposium on Software Testing and
Analysis , 2019, pp. 169‚Äì180.
[5] M. Wardat, W. Le, and H. Rajan, ‚ÄúDeeplocalize: Fault localization
for deep neural networks,‚Äù in 2021 IEEE/ACM 43rd International
Conference on Software Engineering (ICSE) . IEEE, 2021, pp. 251‚Äì
262.

--- PAGE 14 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14
[6] T. Lutellier, L. Pang, V . H. Pham, M. Wei, and L. Tan, ‚ÄúEncore:
Ensemble learning using convolution neural machine translation for
automatic program repair,‚Äù arXiv preprint arXiv:1906.08691 , 2019.
[7] Y . Li, S. Wang, and T. N. Nguyen, ‚ÄúDlÔ¨Åx: Context-based code trans-
formation learning for automated program repair,‚Äù in Proceedings of
the ACM/IEEE 42nd International Conference on Software Engineering ,
2020, pp. 602‚Äì614.
[8] R. Gupta, S. Pal, A. Kanade, and S. Shevade, ‚ÄúDeepÔ¨Åx: Fixing com-
mon c language errors by deep learning,‚Äù in Proceedings of the aaai
conference on artiÔ¨Åcial intelligence , vol. 31, no. 1, 2017.
[9] W. U. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, ‚ÄúA
transformer-based approach for source code summarization,‚Äù arXiv
preprint arXiv:2005.00653 , 2020.
[10] A. LeClair, S. Haque, L. Wu, and C. McMillan, ‚ÄúImproved code
summarization via a graph neural network,‚Äù in Proceedings of the 28th
International Conference on Program Comprehension , 2020, pp. 184‚Äì
195.
[11] Y . Wan, Z. Zhao, M. Yang, G. Xu, H. Ying, J. Wu, and P. S. Yu,
‚ÄúImproving automatic source code summarization via deep reinforce-
ment learning,‚Äù in Proceedings of the 33rd ACM/IEEE International
Conference on Automated Software Engineering , 2018, pp. 397‚Äì407.
[12] S. Kim, J. Zhao, Y . Tian, and S. Chandra, ‚ÄúCode prediction by feeding
trees to transformers,‚Äù in 2021 IEEE/ACM 43rd International Conference
on Software Engineering (ICSE) . IEEE, 2021, pp. 150‚Äì162.
[13] Y . Huang, X. Hu, N. Jia, X. Chen, Z. Zheng, and X. Luo, ‚ÄúCommtpst:
Deep learning source code for commenting positions prediction,‚Äù Jour-
nal of Systems and Software , vol. 170, p. 110754, 2020.
[14] A. Hasanpour, P. Farzi, A. Tehrani, and R. Akbari, ‚ÄúSoftware defect
prediction based on deep learning models: Performance study,‚Äù arXiv
preprint arXiv:2004.02589 , 2020.
[15] J. Chen, K. Hu, Y . Yu, Z. Chen, Q. Xuan, Y . Liu, and V . Filkov,
‚ÄúSoftware visualization and deep transfer learning for effective software
defect prediction,‚Äù in Proceedings of the ACM/IEEE 42nd International
Conference on Software Engineering , 2020, pp. 578‚Äì589.
[16] S. Wang, T. Liu, J. Nam, and L. Tan, ‚ÄúDeep semantic feature learning for
software defect prediction,‚Äù IEEE Transactions on Software Engineering ,
vol. 46, no. 12, pp. 1267‚Äì1293, 2018.
[17] M. J. Islam, G. Nguyen, R. Pan, and H. Rajan, ‚ÄúA comprehensive study
on deep learning bug characteristics,‚Äù in Proceedings of the 2019 27th
ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering , 2019, pp. 510‚Äì
520.
[18] M. Wen, R. Wu, and S.-C. Cheung, ‚ÄúHow well do change sequences
predict defects? sequence learning from software changes,‚Äù IEEE Trans-
actions on Software Engineering , vol. 46, no. 11, pp. 1155‚Äì1175, 2018.
[19] M. Tufano, J. Pantiuchina, C. Watson, G. Bavota, and D. Poshyvanyk,
‚ÄúOn learning meaningful code changes via neural machine translation,‚Äù
in2019 IEEE/ACM 41st International Conference on Software Engi-
neering (ICSE) . IEEE, 2019, pp. 25‚Äì36.
[20] S. Chakraborty, Y . Ding, M. Allamanis, and B. Ray, ‚ÄúCodit: Code
editing with tree-based neural models,‚Äù IEEE Transactions on Software
Engineering , 2020.
[21] T. Lutellier, H. V . Pham, L. Pang, Y . Li, M. Wei, and L. Tan, ‚ÄúCoconut:
combining context-aware neural translation models using ensemble for
program repair,‚Äù in Proceedings of the 29th ACM SIGSOFT international
symposium on software testing and analysis , 2020, pp. 101‚Äì114.
[22] N. Jiang, T. Lutellier, and L. Tan, ‚ÄúCure: Code-aware neural machine
translation for automatic program repair,‚Äù in 2021 IEEE/ACM 43rd
International Conference on Software Engineering (ICSE) . IEEE, 2021,
pp. 1161‚Äì1173.
[23] W. Tansey and E. Tilevich, ‚ÄúAnnotation refactoring: inferring upgrade
transformations for legacy applications,‚Äù in Proceedings of the 23rd
ACM SIGPLAN conference on Object-oriented programming systems
languages and applications , 2008, pp. 295‚Äì312.
[24] N. Meng, L. Hua, M. Kim, and K. S. McKinley, ‚ÄúDoes automated
refactoring obviate systematic editing?‚Äù in 2015 IEEE/ACM 37th IEEE
International Conference on Software Engineering , vol. 1. IEEE, 2015,
pp. 392‚Äì402.
[25] X. Ge, Q. L. DuBose, and E. Murphy-Hill, ‚ÄúReconciling manual
and automatic refactoring,‚Äù in 2012 34th International Conference on
Software Engineering (ICSE) . IEEE, 2012, pp. 211‚Äì221.
[26] H. A. Nguyen, T. T. Nguyen, G. Wilson Jr, A. T. Nguyen, M. Kim, and
T. N. Nguyen, ‚ÄúA graph-based approach to api usage adaptation,‚Äù ACM
Sigplan Notices , vol. 45, no. 10, pp. 302‚Äì321, 2010.
[27] A. T. Nguyen, M. Hilton, M. Codoban, H. A. Nguyen, L. Mast,
E. Rademacher, T. N. Nguyen, and D. Dig, ‚ÄúApi code recommendation
using statistical learning from Ô¨Åne-grained changes,‚Äù in Proceedings ofthe 2016 24th ACM SIGSOFT International Symposium on Foundations
of Software Engineering , 2016, pp. 511‚Äì522.
[28] M. Tufano, C. Watson, G. Bavota, M. Di Penta, M. White, and
D. Poshyvanyk, ‚ÄúAn empirical investigation into learning bug-Ô¨Åxing
patches in the wild via neural machine translation,‚Äù in Proceedings of
the 33rd ACM/IEEE International Conference on Automated Software
Engineering , 2018, pp. 832‚Äì837.
[29] Z. Chen, S. J. Kommrusch, M. Tufano, L.-N. Pouchet, D. Poshyvanyk,
and M. Monperrus, ‚ÄúSequencer: Sequence-to-sequence learning for end-
to-end program repair,‚Äù IEEE Transactions on Software Engineering ,
2019.
[30] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan,
A. Svyatkovskiy, S. Fu, M. Tufano, S. K. Deng, C. B. Clement,
D. Drain, N. Sundaresan, J. Yin, D. Jiang, and M. Zhou, ‚ÄúGraph-
codebert: Pre-training code representations with data Ô¨Çow,‚Äù CoRR , vol.
abs/2009.08366, 2020.
[31] H. Tian, K. Liu, A. K. Kabor ¬¥e, A. Koyuncu, L. Li, J. Klein, and
T. F. Bissyand ¬¥e, ‚ÄúEvaluating representation learning of code changes
for predicting patch correctness in program repair,‚Äù in 35th IEEE/ACM
International Conference on Automated Software Engineering, ASE
2020, Melbourne, Australia, September 21-25, 2020 . IEEE, 2020, pp.
981‚Äì992.
[32] M. Tufano, C. Watson, G. Bavota, M. D. Penta, M. White, and
D. Poshyvanyk, ‚ÄúAn empirical study on learning bug-Ô¨Åxing patches in
the wild via neural machine translation,‚Äù ACM Transactions on Software
Engineering and Methodology (TOSEM) , vol. 28, no. 4, pp. 1‚Äì29, 2019.
[33] D. Bahdanau, K. Cho, and Y . Bengio, ‚ÄúNeural machine translation by
jointly learning to align and translate,‚Äù arXiv preprint arXiv:1409.0473 ,
2014.
[34] U. Alon, S. Brody, O. Levy, and E. Yahav, ‚Äúcode2seq: Generating
sequences from structured representations of code,‚Äù arXiv preprint
arXiv:1808.01400 , 2018.
[35] A. N. Le, A. Martinez, A. Yoshimoto, and Y . Matsumoto, ‚ÄúImproving
sequence to sequence neural machine translation by utilizing syntactic
dependency information,‚Äù in Proceedings of the Eighth International
Joint Conference on Natural Language Processing, IJCNLP 2017,
Taipei, Taiwan, November 27 - December 1, 2017 - Volume 1: Long
Papers , G. Kondrak and T. Watanabe, Eds. Asian Federation of Natural
Language Processing, 2017, pp. 21‚Äì29.
[36] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù arXiv preprint
arXiv:1706.03762 , 2017.
[37] J. W. Rae, A. Potapenko, S. M. Jayakumar, C. Hillier, and T. P. Lillicrap,
‚ÄúCompressive transformers for long-range sequence modelling,‚Äù in 8th
International Conference on Learning Representations, ICLR 2020,
Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020.
[38] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti,
S. Onta Àún¬¥on, P. Pham, A. Ravula, Q. Wang, L. Yang, and A. Ahmed,
‚ÄúBig bird: Transformers for longer sequences,‚Äù in Advances in Neural
Information Processing Systems 33: Annual Conference on Neural
Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual , H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and
H. Lin, Eds., 2020.
[39] Y . Wang and H. Li, ‚ÄúCode completion by modeling Ô¨Çattened abstract
syntax trees as graphs,‚Äù Proceedings of AAAIConference on ArtiÔ¨Åcial
Intellegence , 2021.
[40] P. Shaw, J. Uszkoreit, and A. Vaswani, ‚ÄúSelf-attention with relative
position representations,‚Äù arXiv preprint arXiv:1803.02155 , 2018.
[41] ‚ÄúGerrit - android.‚Äù https://android-review.googlesource.com/.
[42] ‚ÄúGerrit - goggle source.‚Äù https://gerrit-review.googlesource.com/.
[43] ‚ÄúGerrit - ovirt.‚Äù https://gerrit.ovirt.org/.
[44] K. Cho, B. Van Merri ¬®enboer, C. Gulcehre, D. Bahdanau, F. Bougares,
H. Schwenk, and Y . Bengio, ‚ÄúLearning phrase representations using
rnn encoder-decoder for statistical machine translation,‚Äù arXiv preprint
arXiv:1406.1078 , 2014.
[45] S. Bhatia, P. Kohli, and R. Singh, ‚ÄúNeuro-symbolic program corrector
for introductory programming assignments,‚Äù in 2018 IEEE/ACM 40th
International Conference on Software Engineering (ICSE) . IEEE, 2018,
pp. 60‚Äì70.
[46] E. Dinella, H. Dai, Z. Li, M. Naik, L. Song, and K. Wang, ‚ÄúHoppity:
Learning graph transformations to detect and Ô¨Åx bugs in programs,‚Äù in
International Conference on Learning Representations (ICLR) , 2020.
[47] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image
recognition,‚Äù in Proceedings of the IEEE conference on computer vision
and pattern recognition , 2016, pp. 770‚Äì778.
[48] J. L. Ba, J. R. Kiros, and G. E. Hinton, ‚ÄúLayer normalization,‚Äù arXiv
preprint arXiv:1607.06450 , 2016.

--- PAGE 15 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 15
[49] U. Z. Ahmed, P. Kumar, A. Karkare, P. Kar, and S. Gulwani, ‚ÄúCompila-
tion error repair: for the student programs, from the student programs,‚Äù in
Proceedings of the 40th International Conference on Software Engineer-
ing: Software Engineering Education and Training , 2018, pp. 78‚Äì87.
[50] D. van Bruggen, ‚ÄúJavaparser,‚Äù https://javaparser.org/about.html.
[51] D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù
arXiv preprint arXiv:1412.6980 , 2014.
[52] P. Goyal, P. Doll ¬¥ar, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola,
A. Tulloch, Y . Jia, and K. He, ‚ÄúAccurate, large minibatch sgd: Training
imagenet in 1 hour,‚Äù arXiv preprint arXiv:1706.02677 , 2017.
[53] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, ‚ÄúBleu: a method for
automatic evaluation of machine translation,‚Äù in Proceedings of the 40th
annual meeting of the Association for Computational Linguistics , 2002,
pp. 311‚Äì318.
[54] C.-Y . Lin, ‚ÄúRouge: A package for automatic evaluation of summaries,‚Äù
inText summarization branches out , 2004, pp. 74‚Äì81.
[55] S. Liu, C. Gao, S. Chen, L. Y . Nie, and Y . Liu, ‚ÄúATOM: commit
message generation based on abstract syntax tree and hybrid ranking,‚Äù
Transactions on Software Engineering , 2020.
[56] L. Y . Nie, C. Gao, Z. Zhong, W. Lam, Y . Liu, and Z. Xu, ‚ÄúContex-
tualized code representation learning for commit message generation,‚Äù
Neurocomputing , 2021.
[57] C. Gao, W. Zhou, X. Xia, D. Lo, Q. Xie, and M. R. Lyu, ‚ÄúAutomating
app review response generation based on contextual knowledge,‚Äù ACM
Transactions on Software Engineering and Methodology , 2020.
[58] ‚ÄúGerrit .‚Äù https://www.gerritcodereview.com.
[59] Z. Sun, L. Li, Y . Liu, and X. Du, ‚ÄúOn the importance of building
high-quality training datasets for neural code search,‚Äù arXiv preprint
arXiv:2202.06649 , 2022.
[60] M. Schuster and K. K. Paliwal, ‚ÄúBidirectional recurrent neural net-
works,‚Äù IEEE transactions on Signal Processing , vol. 45, no. 11, pp.
2673‚Äì2681, 1997.
[61] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBert: Pre-training
of deep bidirectional transformers for language understanding,‚Äù arXiv
preprint arXiv:1810.04805 , 2018.
[62] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
L. Zettlemoyer, and V . Stoyanov, ‚ÄúRoberta: A robustly optimized bert
pretraining approach,‚Äù arXiv preprint arXiv:1907.11692 , 2019.
[63] X. Liu, H.-F. Yu, I. Dhillon, and C.-J. Hsieh, ‚ÄúLearning to encode posi-
tion for transformer with continuous dynamical model,‚Äù in International
Conference on Machine Learning . PMLR, 2020, pp. 6327‚Äì6335.
[64] M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and ≈Å. Kaiser,
‚ÄúUniversal transformers,‚Äù arXiv preprint arXiv:1807.03819 , 2018.
[65] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,
‚ÄúAlbert: A lite bert for self-supervised learning of language representa-
tions,‚Äù arXiv preprint arXiv:1909.11942 , 2019.
[66] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. Salakhutdinov, and Q. V . Le,
‚ÄúXlnet: Generalized autoregressive pretraining for language understand-
ing,‚Äù arXiv preprint arXiv:1906.08237 , 2019.
[67] Z. Dai, Z. Yang, Y . Yang, J. Carbonell, Q. V . Le, and R. Salakhutdi-
nov, ‚ÄúTransformer-xl: Attentive language models beyond a Ô¨Åxed-length
context,‚Äù arXiv preprint arXiv:1901.02860 , 2019.
[68] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
Y . Zhou, W. Li, and P. J. Liu, ‚ÄúExploring the limits of trans-
fer learning with a uniÔ¨Åed text-to-text transformer,‚Äù arXiv preprint
arXiv:1910.10683 , 2019.
[69] G. Ke, D. He, and T.-Y . Liu, ‚ÄúRethinking the positional encoding in
language pre-training,‚Äù arXiv preprint arXiv:2006.15595 , 2020.
[70] P. He, X. Liu, J. Gao, and W. Chen, ‚ÄúDeberta: Decoding-enhanced bert
with disentangled attention,‚Äù arXiv preprint arXiv:2006.03654 , 2020.
[71] K. Liu, L. Li, A. Koyuncu, D. Kim, Z. Liu, J. Klein, and T. F.
Bissyand ¬¥e, ‚ÄúA critical review on the evaluation of automated program
repair systems,‚Äù Journal of Systems and Software , vol. 171, p. 110817,
2021.
[72] K. Liu, D. Kim, A. Koyuncu, L. Li, T. F. Bissyand ¬¥e, and Y . Le Traon,
‚ÄúA closer look at real-world patches,‚Äù in 2018 IEEE International
Conference on Software Maintenance and Evolution (ICSME) . IEEE,
2018, pp. 275‚Äì286.
[73] S. Wang, K. Liu, B. Lin, L. Li, J. Klein, X. Mao, and T. F. Bissyand ¬¥e,
‚ÄúBeep: Fine-grained Ô¨Åx localization by learning to predict buggy code
elements,‚Äù arXiv preprint arXiv:2111.07739 , 2021.
[74] X. Wang, Y . Wang, F. Mi, P. Zhou, Y . Wan, X. Liu, L. Li, H. Wu,
J. Liu, and X. Jiang, ‚ÄúSyncobert: Syntax-guided multi-modal contrastive
pre-training for code representation,‚Äù arXiv preprint arXiv:2108.04556 ,
2021.[75] H. Tian, K. Liu, A. K. Kabor ¬¥e, A. Koyuncu, L. Li, J. Klein, and
T. F. Bissyand ¬¥e, ‚ÄúEvaluating representation learning of code changes for
predicting patch correctness in program repair,‚Äù in 2020 35th IEEE/ACM
International Conference on Automated Software Engineering (ASE) .
IEEE, 2020, pp. 981‚Äì992.
[76] V . Raychev, M. Sch ¬®afer, M. Sridharan, and M. Vechev, ‚ÄúRefactoring
with synthesis,‚Äù ACM SIGPLAN Notices , vol. 48, no. 10, pp. 339‚Äì354,
2013.
[77] M. Boshernitsan, S. L. Graham, and M. A. Hearst, ‚ÄúAligning develop-
ment tools with the way programmers think about code changes,‚Äù in
Proceedings of the SIGCHI conference on Human factors in computing
systems , 2007, pp. 567‚Äì576.
