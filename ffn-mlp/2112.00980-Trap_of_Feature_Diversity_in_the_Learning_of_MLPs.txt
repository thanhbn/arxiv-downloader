# 2112.00980.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/ffn-mlp/2112.00980.pdf
# File size: 5903265 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Trap of Feature Diversity in the Learning of MLPs
Dongrui LiuaShaobo WangbJie RenaKangrui WangaSheng Yina
Huiqi DengaQuanshi Zhangay
aShanghai Jiao Tong UniversitybHarbin Institute of Technology
Abstract
In this paper, we focus on a typical two-phase phenomenon in the learning of
multi-layer perceptrons (MLPs), and we aim to explain the reason for the decrease
of feature diversity in the ﬁrst phase. Speciﬁcally, people ﬁnd that, in the training
of MLPs, the training loss does not decrease signiﬁcantly until the second phase.
To this end, we further explore the reason why the diversity of features over
different samples keeps decreasing in the ﬁrst phase, which hurts the optimization
of MLPs. We explain such a phenomenon in terms of the learning dynamics of
MLPs. Furthermore, we theoretically explain why four typical operations can
alleviate the decrease of the feature diversity. The code will be released when the
paper is accepted.
1 Introduction
Deep neural networks (DNNs) have achieved signiﬁcant success in various tasks. However, the
essential reason for the superior performance of DNNs has not been fully investigated. Many
studies aim to explain typical phenomena in DNNs, e.g.investigating the phenomenon of the lottery
ticket hypothesis [ 16], explaining the double-descent phenomenon [ 23,36,19], understanding the
information bottleneck hypothesis [ 48,41], exploring the gradient noise and regularization [ 43,34],
and analyzing the nonlinear learning dynamics [27, 38].
In this paper, we focus on the learning dynamics of multi-layer perceptrons (MLPs). It has been
widely discovered that when MLPs have many layers and the optimization is difﬁcult, the learning
process of MLPs is more likely to have two phases. As Figure 1(b) shows, the ﬁrst phase is usually
not long, in which the training loss does not decrease. Then, in the second phase, the training loss
suddenly begins to decrease. Note that when the task is simple the ﬁrst phase may be extremely short
(e.g.a few iterations within an epoch), and thus cannot be observed.
To this end, we discover an interesting phenomenon in the ﬁrst phase, i.e., as Figure 1(a) shows,
features of different categories become increasingly similar to each other. The feature diversity keeps
decreasing (the cosine similarity between features keeps increasing) until the second phase.
This phenomenon is widely shared by MLPs, convoluational neural networks, and recurrent neural
networks (see both Figure 2 and the supplementary material). Neural networks trained with different
depths and widths, different activation functions, and different learning rates, on different types of
data may all exhibit such a phenomenon, which hurts the optimization.
More crucially, the investigation and alleviation of the two-phase phenomenon are of considerable
value, because this is a typical learning-sticking problem with MLPs. As Figure 1(c) shows, when
the loss minimization gets stuck, we can consider it as a strong ﬁrst phase with an inﬁnite length.
Equal contribution
yThis research is done under the supervision of Dr. Quanshi Zhang. He is with the Department of Computer
Science and Engineering, the John Hopcroft Center and the MoE Key Lab of Artiﬁcial Intelligence, AI Institute,
at the Shanghai Jiao Tong University, China. Correspondence to: Quanshi Zhang <zqs1022@sjtu.edu.cn>.
Preprint. Under review.arXiv:2112.00980v4  [cs.LG]  1 Jun 2022

--- PAGE 2 ---
Training Loss2.4 100
7090
80
iteration2.4
1.2
102 103104
iteration0 1500 3000
Samples Features
MLP
MLP
Cosine Similarity
iteration1.0
0.9
0.8
0 500 10002.3
1.2
0.0Training Loss
(a) (d)Testing ErrorTraining Loss
1.6
0.8
104102thefirst
iteration103thesecond
(%)
(c)feature diversity
decreases in the
first phasethe stuck learning 
can be considered 
as the first phase
with an infinite
length phase
phase
0.0
when thefirstphase 
is extremely short(b)Figure 1: (a, b) We aim to explain the two-phase phenomenon in the training of MLPs, i.e., the reason
why the cosine similarity between features of different categories keeps increasing in the ﬁrst phase
in (a). (c) When the loss minimization gets stuck (orange curve), we can consider it as the ﬁrst phase
with an inﬁnite length. The learning-sticking problem can be solved by techniques of shortening the
ﬁrst phase (blue curve). (d) The two-phase phenomenon aligns the double-descent behavior.
Therefore, we aim to investigate the optimization behavior in the ﬁrst phase, i.e., why and how the
feature diversity decreases during the early training process of the MLP. To this end, we ﬁnd that
in intermediate layers of the MLP, both features and parameters are mainly optimized towards a
special direction, namely the primary common direction. In this study, we theoretically clarify certain
dynamics of the optimization, which increases the likelihood of further enhancing such a primary
common direction, just like a self-enhanced system. This may explain the decrease of the feature
diversity in early iterations.
Based on our theoretical analysis, we further discover and explain the reason why four typical
operations ( i.e., batch normalization, momentum, initialization, and L2regularization) can effectively
alleviate the two-phase phenomenon. The above ﬁndings provide theoretical supports for heuristic
solutions to the learning-sticking problem.
Although the decrease of feature diversity can be alleviated by traditional operations, the core
contribution of this study is to discover and explain a fundamental yet counter-intuitive two-
phase phenomenon with the MLP, which has not been theoretically explained for a long time.
Instead, previous studies simply owed the learning-sticking problem in the ﬁrst phase to the difﬁculty
of the training task, without insightful analysis or theoretically supported solutions.
Contributions of this study can be summarized as follows. (1) We discover the common phenomenon
of feature diversity decreasing in early learning of the MLP, which has been ignored for a long time.
(2) We explain this phenomenon from the perspective of learning dynamics. (3) We explain why four
types of operations can alleviate the decrease of feature diversity.
2 Related Work
Understanding the optimization and the representation capacity of DNNs is an important direction to
explain DNNs. The information bottleneck theory [48, 41] quantitatively explained the information
encoded by features in intermediate layers of DNNs. Xu and Raginsky [49], Achille and Soatto
[1], and Cheng et al. [8]used the information bottleneck theory to evaluate and improve the DNN’s
representation capacity. Arpit et al. [5]analyzed the representation capacity of DNNs with real
training data and noises. In addition, several metrics were proposed to measure the generalization
capacity or robustness of DNNs, including the stiffness [ 15], the sensitivity metrics [ 37], the Fourier
analysis [ 51], and the CLEVER score [ 47]. Some studies focused on proving the generalization
bound of DNNs [ 32,31]. In comparison, we explain the MLP from the perspective of the learning
dynamics, i.e., we explain the decrease of feature diversity in early iterations of the MLP.
Analyzing the learning dynamics is another perspective to understand DNNs. Many studies analyzed
the local minima in the optimization landscape of linear networks [ 7,40,18,10] and nonlinear
networks [ 9,25,39]. Some studies discussed the convergence rate of gradient descent on separable
data [ 45,50,35]. Hoffer et al. [20] and Jastrz˛ ebski et al. [24] have investigated the effects of the
batch size and the learning rate on SGD dynamics. In addition, some studies analyzed the dynamics
of gradient descent in the overparameterization regime [ 4,22,30,14]. Unlike previous studies, we
analyze the learning dynamics of features and weights of the MLP, in order to explain the decrease of
feature diversity in the ﬁrst phase.
2

--- PAGE 3 ---
Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Layer 7
Cosine 
similarity of 
pseudo -neuronsCosine 
similarity
of gradientsCosine 
similarity
of featuresCIFAR-10 Commercial Census
iteration iteration iteration iteration(a)
(b)(c)
CoLA SST-2
iteration iterationCIFAR-109-layer MLPs with
512 neurons/layer9-layer MLPs with
128 neurons/layerRevised LSTM
CIFAR-10
iteration iterationVGG-11
Tiny ImageNet9-layer MLPs with
512 neurons/layerCosine 
similarity of 
pseudo -neuronsTiny ImageNet
iterationTiny ImageNetFigure 2: The two-phase phenomenon. (a) Cosine similarity of features between samples in different
categories Ex;x02X[cos(F(l)
tjx;F(l)
tjx0)]keeps increasing in the ﬁrst phase (left to the dotted line), until
the second phase. The low cosine similarity indicates the high diversity. (b) Cosine similarity of
gradients w.r.t. features between different samples of a category Ex;x02Xc[cos( _F(l)
tjx;_F(l)
tjx0)]keeps
increasing in the ﬁrst phase until the second phase, where Xcdenotes samples of the category c. (c)
Cosine similarity of weight changes between “pseudo-neurons” in a layer Ex2Xcos(w(l)
t;ijx;w(l)
t;jjx)
keeps increasing in the ﬁrst phase. Please see the supplementary material for results on more DNNs.
3 Discovering the decrease of feature diversity
The training process of the MLP can usually be divided into the following two phases, according to the
training loss. As Figure 1(b) shows, the training loss does not decrease signiﬁcantly in the ﬁrst phase,
and the training loss suddenly begins to decrease in the second phase. In this paper, we discover an
interesting phenomenon in the ﬁrst phase that both the diversity of intermediate-layer features
over different samples and the diversity of gradients w.r.t. features keep decreasing. In other
words, in the ﬁrst phase, the cosine similarity between features and the cosine similarity between
feature gradients keep increasing.
We consider an MLP fwithLconcatenated linear layers, each being followed by a ReLU layer. Only
the last linear layer is followed by a softmax operation. Let W(l)
t2Rhddenote the weight matrix of
thel-th linear layer with hneurons (16l6L), andW(l)
thas been learned for titerations. Given a
speciﬁc input sample x, the layer-wise forward propagation in the l-th layer is represented as
F(l)
t=ReLU (W(l)
tF(l 1)
t ) =D(l)
tW(l)
tF(l 1)
t; (1)
whereF(l)
t2Rhdenotes the output feature of the l-th layer after the t-th iteration. D(l)
tdenotes a
diagonal matrix, which represents gating states in the ReLU layer and D(l)
t;(i;i)2f0;1g.
Visualizing that the two-phase phenomenon is widely shared by different DNNs learned for
different tasks. We observed such a two-phase phenomenon on MLPs, VGG-11 [ 42], and the
revised long short-term memory (LSTM) on different types of data, including image data (MNIST
[29], CIFAR-10 [ 26], and the Tiny ImageNet dataset [ 28]), tabular data (two UCI datasets of census
income and TV news [ 6]), and natural language data (CoLA [ 46], SST-2 [ 44], and AGNews [ 13]). We
also observed MLPs with Leaky ReLU layers [ 33], with different learning rates, and with different
batch sizes. Figure 2(a,b) shows the two-phase phenomenon on some DNNs, and please see the
supplementary material for results on more DNNs. Speciﬁcally, given two input samples x1andx2,
the feature similarity between x1andx2cos(F(l)
tjx1;F(l)
tjx2), and the cosine similarity of gradients
cos(_F(l)
tjx1;_F(l)
tjx2)keep increasing, which demonstrates the phenomenon. _F(l)
tdenotes the gradient
of the loss w.r.t. the featureF(l)
tin Eq. (1).
Note that such a decrease of feature diversity sometimes appears in very early epochs (or iterations)
of the training process. More crucially, as Figure 1(c) shows, when the loss directly decreases in the
ﬁrst epoch, it can be considered as a short ﬁrst phase in very early iterations. Besides, the learning
process is sometimes stuck when the task is difﬁcult. Then we can consider the learning-sticking
problem as the ﬁrst phase with an inﬁnite length (please see the supplementary material for more
discussions).
Connection to the epoch-wise double descent. Figure 1(d) shows that the epoch-wise double-
descent behavior [ 36,19] is temporally aligned with the ﬁrst phase in the aforementioned two-phase
phenomenon. Please see the supplementary material for more discussions. Instead of explaining the
epoch-wise double descent, in this paper, we mainly explain the decrease of the feature diversity.
3

--- PAGE 4 ---
Discussed in Section 4.2.Explained in Lemma 3 and Theorem 2 𝑤𝑤𝑡𝑡=1 ,1𝑤𝑤𝑡𝑡=2 ,1𝑤𝑤𝑡𝑡=3 ,1𝑤𝑤𝑡𝑡=4 ,1𝑤𝑤𝑡𝑡=5 ,1
𝑤𝑤𝑡𝑡=1 ,2𝑤𝑤𝑡𝑡=2 ,2
𝑤𝑤𝑡𝑡=3 ,2 𝑤𝑤𝑡𝑡=4 ,2 𝑤𝑤𝑡𝑡=5 ,2
𝑤𝑤𝑡𝑡=1 ,3𝑤𝑤𝑡𝑡=2 ,3
𝑤𝑤𝑡𝑡=3 ,3𝑤𝑤𝑡𝑡=4 ,3 𝑤𝑤𝑡𝑡=5 ,3neuron 1
neuron 2
neuron 3direction
direction
directionFormulated in Eq.4 Formulated in Theorem 1. 
(a) (b)The common direction of weight changes 
can be decomposed from two perspectives. 
Such enhancement explains the 
decrease of feature diversity Perspective 1 Perspective 2
These two perspectives enhance each other.Figure 3: (a)
Weights of neu-
rons are changed
towards a common
direction. (b) The
logic of explaining
the decrease of
feature diversity.
4 Explaining the dynamics of the decrease of feature diversity
In this section, we aim to investigate certain dynamics of network parameters, so as to explain the
condition that may boost the likelihood of decreasing the feature diversity in early epochs under some
common assumptions. In Section 4.1, we ﬁnd that the decreasing diversity of feature gradients over
different samples is owing to the phenomenon that different neurons in a layer are optimized towards
a common direction in the ﬁrst phase. Then, in Section 4.2, we further clarify learning dynamics that
may potentially enhance the signiﬁcance of the common direction, just like a self-enhanced system.
The self-enhanced common direction boosts the likelihood of the feature diversity’s decreasing. The
overall logic of the explanation is illustrated in Figure 3(b). In Section 4.3, we explain why four types
of operations can alleviate the decrease of feature diversity based on our analysis.
4.1 Two perspectives to analyze the common direction of learning effects
The decreasing diversity of feature gradients over different samples is owing to the phe-
nomenon that different neurons in a layer are optimized towards a common direction in the
ﬁrst phase. For example, as Figure 3(a) shows, at the beginning of the learning, different neurons are
optimized towards different directions. Along with the learning process, different neurons gradually
tend to be optimized along a similar direction. According to the forward propagation in Eq. (1),
feature gradients _F(l)
tat thel-th layer during the back propagation can be rewritten as
_F(l 1)
t =W(l)>
tD(l)
t_F(l)
t: (2)
In this way, we can consider feature gradients _F(l 1)
t as the result of a pseudo-forward propagation.
In the pseudo-forward propagation, the input is the gradient w.r.t. features of the l-th layer _F(l)
t, and
the output is the gradient of the (l 1)-th layer _F(l 1)
t2Rd. Accordingly, the equivalent weight matrix
is given asW(l)>
t= [w(l)
t;1;w(l)
t;2;;w(l)
t;d]>2Rdh, which consists of d“pseudo-neurons.”
Before explaining the common direction phenomenon, let us ﬁrst clarify that the increasing similarity
between feature gradients _F(l 1)
t of different samples is caused by the experimental observation that
different “pseudo-neurons” are approximately optimized to a common direction. Let us consider
the following conjecture that different “pseudo-neurons” [w(l)
t;1;w(l)
t;2;;w(l)
t;d]>have been roughly
optimized towards a common direction C(l)2Rhfor many epochs. Then, we can roughly represent
w(l)
t;i=iC(l)+i, wherei2R, and i2Rh. Here, i2Rhdenotes a small residual. According to
Eq. (2), we have
_F(l 1)
t = (C(l)>D(l)
t_F(l)
t)+D(l)
t_F(l)
t (3)
Thus, if is large enough ( i.e., keeping optimizing W(l)>
talong the common direction C(l)for a
long time), then the feature gradients _F(l 1)
t of different samples will be roughly parallel to the same
vector . This is because C(l)>D(l)
t_F(l)
tis a scalar and the term D(l)
t_F(l)
tis small. In other words, the
diversity between feature gradients _F(l 1)
t of different samples decreases. Here, = [1;2;;d],
and= [1;2;;d]>.
Therefore, the ﬁrst core task of proving the decreasing diversity of feature gradients is to
explain the conjecture of the common optimization direction shared by different “pseudo-
neurons.” To this end, let us ﬁrst propose two perspectives to disentangle such a common direction.
Perspective 1. Perspective 1 focuses on the weight change in a certain layer. For clarity, we
omit the superscript (l)to simplify the notation in the following paragraphs in Section 4.1, i.e.,
w(l)
t;isimplify ! wt;i;W(l)
tsimplify !Wt;C(l)simplify !C. Let W>
t= [wt;1;wt;2;;wt;d]>denote
4

--- PAGE 5 ---
Cat
StrengthTruck
StrengthLayer 2 Layer 3 Layer 4 Layer 5 Layer 6 Category
Flag pole
StrengthBottle
Strength(a)(b)
direction direction direction direction direction direction direction direction direction directionLayer 2 Layer 3 Layer 4 Layer 5 Layer 6 Category
1e-4
1e-49.5
0.0 0.0 0.0 0.0 0.0
9.51.2 1.51e-3 1e-3 1e-3 1e-3
1.91e-3 1e-3
1e-3 1e-3 1e-3 1e-32.1
1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 51e-3 1e-3 1e-3 1e-3 1e-3
1e-3 1e-3 1e-3 1e-3 1e-3
1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 51.1
1.41.4 1.9
0.0
1.8 2.22.4 3.2
2.8 3.7
0.00.0
0.00.0
0.00.0
0.00.0
0.0 0.0 0.0 0.0 0.0 0.01.4 1.6 1.8 2.2Figure 4: The strength of different common directions in the (a) CIFAR-10 dataset, (b) Tiny ImageNet
dataset. We trained 9-layer MLPs, where each layer of the MLP had 512 neurons. We illustrated
results on the two categories with the highest training accuracies. si=kCiV>
ikFmeasures the
strength of weight changes along the i-th common direction, where Vi=Et[Vi;t]. The strength
of the primary direction was much greater than the strength of other directions. Please see the
supplementary material for results on the MNIST.
weight changes of d“pseudo-neurons” in the l-th layer. We decompose W>
tinto the component
along a common direction Cand a component along other directions as follows.
W>
t= VtC>+ "t; (4)
where Vt= [vt;1;vt;2;;vt;d]2Rddenotes the coefﬁcient vector for weight changes of
different “pseudo-neurons” along the common direction C,i.e., using vt;iC>to approximate w>
t;i.
Speciﬁcally, "tis relatively small “noise” term, which is orthogonal to C,i.e.,"tC=0. In this
way, the common direction Ccan determined by minimizing the “noise” term over different samples
across different iterations, as follows.
minC;Vtjx 
Et2[Tstart;Tend]Ex2Xk"tjxk2
F
;s.t."tjx= W>
tjx VtjxC>(5)
Lemma 1. (Proof in the supplementary material) For the decomposition W>
t=VtC>+"t, given
weight changes over different samples W>
t, we can compute the common direction Cin Eq. (5)and
obtain Vt=W>
tC
C>Cand"t=W>
t W>
tCC>
C>Cs.t."tC=0. Such settings minimize k"tkF.
Lemma 2. (We can also decompose the weight W(l)
tinto the component along the common direc-
tionCand the component "tin other directions . Proof is in the supplementary material.) Given the
weightW>
tand the common direction C, the decomposition W>
t=VtC>+"tcan be conducted as
Vt=W>
tC
C>Cand"t=W>
t W>
tCC>
C>Cs.t."tC=0. Such settings minimize k"tkF.
Experimental veriﬁcation of the strength of the primary common direction C. To this end, let us focus
on the average weight change over different samples Wt=Ex2XWtjx.Then, we decompose Wt
into components along ﬁve common directions as Wt=C1V>
1;t+C2V>
2;t++C5V>
5;t+">
5;t,
whereC1=Cis termed the primary common direction .C2;C3;C4andC5represent the second, third,
forth, and ﬁfth common directions, respectively. C1,C2,C3,C4, andC5are orthogonal to each other.
CiandVi;tare computed based on Eq. (5) when we remove the ﬁrst i 1components along
the direction C;;Ci 1from the Wt. Figure 4 shows that the strength of the primary common
component C1V>
1was approximately ten times greater than the strength of the secondary common
component C2V>
2. Please see the supplementary material for more discussions.
Perspective 2 by using gradients w.r.t. features _F(l+1)
t .
We decompose the weight change by considering the inﬂuence of the common direction of the upper
layerC(l+1). In order to distinguish variables belonging to different layers, we add the superscript (l)
back to W(l)
t;V(l)
t, and "(l)
tto denote the layer in the following paragraphs.
Theorem 1. (Proof in the supplementary material) The weight change made by a sample can be
decomposed into (h+ 1) terms after the t-th iteration as follows.
W(l)
t= W(l)
primary ;t+Xh
k=1W(l;k)
noise;trewritten= = = =  (l)
tF(l 1)>
t +(l)>
t; (6)
where W(l)
primary ;t=D(l)
tV(l+1)
tC(l+1)>C(l+1)V(l+1)>
tF(l)
tF(l 1)>
t=kF(l)
tk2
2denotes the component
along the primary common direction, and W(l;k)
noise;t=D(l)
t"(l+1;k)
t "(l+1)>
tF(l)
tF(l 1)>
t=kF(l)
tk2
2de-
notes the component along the k-th common direction in the noise term. "(l+1;k)
t =kkUkV>
k,
where the SVD of "(l+1)
t2Rhh0is given as "(l+1)
t =UV>(hh0), and kkdenotes
5

--- PAGE 6 ---
Table 1: Strength of components of weight changes along the common direction and other directions.
We trained 9-layer MLPs on the CIFAR-10 dataset and the Tiny ImageNet dataset, respectively. Each
layer of the MLP had 512 neurons. The strength of the primary common direction was much greater
than those of other directions. The supplementary material provides results on the MNIST dataset
and explains the phenomenon that S(l)
1,S(l)
2, andS(l)
3does not decrease monotonically.CIFAR-10Category Cat Truck
S(10 3) Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6
S(l)
primary 154:017:1176:516:8201:618:7253:624:6277:425:6169:920:8208:121:5223:620:1248:419:2281:520:4
S(l)
1 11:51:5 13:00:9 11:61:7 16:11:8 9:00:8 15:62:1 14:01:8 14:31:1 14:31:7 10:01:1
S(l)
2 12:71:7 11:91:3 10:91:3 11:90:8 8:81:1 14:41:4 15:12:0 11:31:4 12:30:9 12:91:2
S(l)
3 11:01:1 14:41:7 12:52:2 13:91:7 8:61:1 14:32:2 12:41:9 12:81:6 13:11:2 9:71:0Tiny ImageNetCategory Flagpole Bottle
S(10 3) Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6
S(l)
primary 97:83:7 143:95:6198:98:1259:810:1322:812:7202:312:2234:413:1276:813:9345:216:6440:222:2
S(l)
1 10:60:9 9:50:8 14:41:4 24:91:3 8:81:0 10:31:4 11:21:6 12:21:3 11:91:1 13:21:6
S(l)
2 7:50:9 7:91:2 9:71:2 9:21:2 8:30:6 10:41:1 11:61:0 13:81:3 10:00:8 13:61:2
S(l)
3 7:10:8 9:11:1 11:31:0 17:92:2 16:61:5 11:61:4 15:71:4 10:71:1 10:81:2 19:81:6
thek-th singular value 2R."(l+1)
t =P
k"(l+1;k)
t .UkandVkdenote the k-th column of
the matrixUandV, respectively. Besides, we have 8k2 f1;2;:::;hg;U>
kC(l+1)= 0.Con-
sequently, we have  (l)
t=D(l)
tV(l+1)
tC(l+1)>C(l+1)V(l+1)>
tF(l)
t=kF(l)
tk2
22Rh, and(l)>
t=
D(l)
t"(l+1)
t"(l+1)>
tF(l)
tF(l 1)>
t=kF(l)
tk2
22Rhd.
Given weight changes W(l)
tmade by a sample x, the primary term W(l)
primary ;trepresents the
component of weight changes along the common direction C(l+1). Thek-th noise term W(l;k)
noise;t
represents the component along the k-th directionUk, which is orthogonal to C(l+1).
Experimental veriﬁcation of the signiﬁcant strength of the component along the common direction
C(l+1).To this end, we computed the average strength of the component along the common direction
C(l+1)over all samples in XasS(l)
primary =Et2[Tstart;Tend]Ex2X[kW(l)
primary ;tjxkF]. Similarly, the strength of
the component along the k-th noise direction was computed as S(l)
k=Et2[Tstart;Tend]Ex2X[kW(l;k)
noise;tjxkF].
Table 1 illustrates that the strength of the primary component S(l)
primary was more than ten times greater
than the strength of components along other noise directions S(l)
1;S(l)
2, andS(l)
3.
Discussion about comparing with the sum of all other directions’ signiﬁcance. According to Table
1, it seems that the sum of strengths of components along other directions is also large. However,
different directions decomposed by the above method are orthogonal to each other. Therefore, weight
changes along different directions are independent, and their strengths cannot be summed up. Thus,
we can directly compare the strength of the component of weight changes along each direction to
verify the signiﬁcant strength of the primary direction.
4.2 Explaining the enhancement of the signiﬁcance of the common direction
The previous subsection owes the decreasing diversity of feature gradients to the typical phenomenon
that there exists a common optimization direction shared by different “pseudo-neurons.” In the
current subsection, we explain that the common optimization direction phenomenon is very likely
to be further enhanced, just like a self-enhanced system, when features F(l 1)
t of different samples
have been pushed a little bit towards a speciﬁc common direction (see the following background
assumption). The self-enhancement of the optimization direction will explain the decreasing diversity.
According to Eq. (4) and Eq. (6), weight changes made by the sample xcan be given as
Perspective 1: W(l)
t=C(l)V(l)>
t+ "(l)>
t Perspective 2: W(l)
t=  (l)
tF(l 1)>
t +(l)>
t (7)
Explaining the guess about the relationship between features and weights. Figure 5 shows a
phenomenon that the feature F(l 1)
t is in the similar direction of the vector V(l)
t, where2
f 1;+1g.By comparing the above two perspectives of such a phenomenon, we guess that the
common direction C(l)is similar to (l)
t, and the feature F(l 1)
t is similar to the vector V(l)
t.
Therefore, in this subsection, we aim to explain that the feature F(l 1)
t and the vector V(l)
tbecome
more and more similar to each other in the ﬁrst phase. This explains the self-enhancement of the
signiﬁcance of the common direction.
6

--- PAGE 7 ---
(b)
Flag pole
SimilarityBottle
Similarity
𝛼𝛼=1iteration𝛼𝛼=1iteration𝛼𝛼=1iteration𝛼𝛼=1iteration𝛼𝛼=1iteration𝛼𝛼=1𝛼𝛼=1𝛼𝛼=1𝛼𝛼=1𝛼𝛼=1(a)Category Layer 2 Layer 4 Layer 5 Layer 6 Layer 3Cat
SimilarityTruck
Similarity-0.8
-1.0
𝛼𝛼=1𝛼𝛼=−1𝛼𝛼=1𝛼𝛼=−1𝛼𝛼=−1𝛼𝛼=−1𝛼𝛼=−1𝛼𝛼=−1𝛼𝛼=−1𝛼𝛼=−1Category Layer 2 Layer 4 Layer 5 Layer 6 Layer 3
iteration iteration iteration iteration iteration-0.6
-0.9-0.6
-0.9-0.8
-1.0-0.9
-1.0-0.9
0.60.9 -0.6
-0.90.81.0-0.8
-1.0-0.9
-1.0-0.9
800 900 800 900 800 900 800 900 800 9000.9
0.6
0.3
0.9
0.6
0.30.9
0.6
0.30.9
0.6
0.30.9
0.6
0.9
0.61.0
0.81.0
0.8
1.0
0.81.0
0.8
2900 3000 2900 3000 2900 3000 2900 3000 2900 3000Figure 5: The average cosine similarity between the feature F(l 1)
t and the vector V(l)
tover different
samples in the ﬁrst phase. We conducted experiments on 9-layer MLPs trained on the (a) CIFAR-10
dataset, and (b) the Tiny ImageNet dataset. The shade in each subﬁgure represents the standard
deviation of the cosine similarity over different samples.
Cat TruckLayer 2 Layer 3 Layer 4 Layer 5 Layer 6 Category
(a)
Flag pole Bottle(b)
iteration iteration iteration iteration iterationLayer 2 Layer 3 Layer 4 Layer 5 Layer 6 Category
0.4
0.20.6
0.4
0.20.6
0.4
0.2
0.4
0.20.6
0.4
0.20.6
0.5
0.40.6
0.5
0.4
0.6
0.5
0.40.6
0.5
0.40.6
0.4
0.20.6
0.4
0.2
0.6
0.4
0.20.6
0.4
0.2
0.6
0.4
0.20.6
0.4
0.2
0.6
0.4
0.20.6
0.4
0.6
0.40.6
0.40.6
0.4
800 900 800 900 800 900 800 900 800 900 2900 3000 2900 3000 2900 3000 2900 3000 2900 3000iteration iteration iteration iteration iteration
Figure 6: The change of o(l)in the ﬁrst phase. We trained 9-layer MLPs on the (a) CIFAR-10 and the
(b) Tiny ImageNet. Each layer of the MLP had 512 neurons. The supplementary material provides
results on the MNIST. The shade represents the standard deviation over different samples.
Discussions on the background assumption. Directly proving how such a “self-enhanced system”
emerges from the very beginning of training an initialized MLP is too difﬁcult. Therefore, we hope to
explain the signiﬁcance of the common direction is probably further enhanced under the assumption
that features F(l 1)
t of different samples have been pushed a little bit towards a speciﬁc common
direction. This assumption can derive that there exists at least one learning iteration in the ﬁrst phase,
in which F(l 1)
t andF(l 1)
t of most samples have similar directions, and V(l)
tandV(l)
thave similar
directions. The derivation is introduced in the supplementary material. The future self-enhancement
ofF(l 1)
t andV(l)
tis proved under this assumption.
Lemma 3. (Proof in the supplementary material) Given an input sample x2Xand a com-
mon direction C(l)after thet-th iteration, if the noise term "(l)
tis small enough to satisfy
jV(l)>
tF(l 1)
tV(l)>
tV(l)
tC(l)>C(l)V(l)>
tF(l 1)
tj  j V(l)>
tF(l 1)
tV(l)>
t"(l)
t"(l)>
tF(l 1)
tj, we can
obtain cos(V(l)
t;F(l 1)
t )cos(V(l)
t;F(l 1)
t )0, where V(l)
t=W(l)>
tC(l)
C(l)>C(l), andV(l)
t=W(l)>
tC(l)
C(l)>C(l).
F(l 1)
t denotes the change of features F(l 1)
t =F(l 1)
t+1 F(l 1)
t made by the training sample
xafter thet-th iteration. To this end, we approximately consider the change of features F(l 1)
t
after thet-th iteration negatively parallel to feature gradients _F(l 1)
t , although strictly speaking, the
change of features is not exactly equal to the gradient w.r.t. features.
Theorem 2. (Proof in the supplementary material) Under the background assumption, for any train-
ing samples x;x02Xcin the category c, if[C(l)>D(l)
tjx_F(l)
tjx][C(l)>D(l)
tjx0_F(l)
tjx0]>0(means that
F(l)
tjxandF(l)
tjx0have kinds of similarity in very early iterations), then cos(cV(l)
tjx;F(l 1)
tjx)0,
andcos(cV(l)
t;F(l 1)
tjx)0, wherec2f 1;+1gis a constant shared by all samples in category c.
Explaining the enhancement of the signiﬁcance of the common direction caused by all training
samples in a certain category. Theorem 2 shows that each category has a dominating training
direction. Speciﬁcally, let us combine Theorem 2 and the background assumption that F(l 1)
t and
F(l 1)
t have similar directions, and V(l)
tandV(l)
thave similar directions. Thus, we can consider
cos(cV(l)
t;F(l 1)
tjx)0in Theorem 2 means that features of training samples in the same category
care all pushed towards a common direction cV(l)
t, making F(l 1)
tjxhighly similar to cV(l)
t.
This keeps a high similarity between features within the same category c. On the other hand,
cos(cV(l)
tjx;F(l 1)
tjx)0in Theorem 2 means that training samples in the category call pushV(l)
t
towardscEx2Xc[F(l 1)
tjx], making V(l)
troughly parallel to cEx2Xc[F(l 1)
tjx]. This phenomenon
is veriﬁed in Figure 5, where cos(cV(l)
t;F(l 1)
t )is always positive over different samples of the
same category. The above analysis also well explains the dynamics behind cos(V(l)
t;F(l 1)
t )
cos(V(l)
t;F(l 1)
t )0in Lemma 3.
7

--- PAGE 8 ---
CIFAR -10 MNIST
category indexAccuracy (%)random guessing random guessing
category indexAccuracy (%)Figure 7: The training accuracy of MLPs on
the CIFAR-10 dataset and the MNIST dataset.
The accuracy was evaluated at the end of the
ﬁrst phase. The MLP only learned features of
a single or two categories in the ﬁrst phase.
In sum, if we only consider training samples in the sample category c, then features of samples
in this category would become increasingly similar to each other. On the other hand, such
training samples have similar training effects, i.e., pushing weights of different neurons all
towards the average feature.
Experimental veriﬁcation of the relationship between the feature F(l 1)
t and the vector V(l)
t.To this
end, we measured the change of the value o(l)= cos(V(l)
t;F(l 1)
t )cos(V(l)
t;F(l 1)
t ). Figure 6 reports
the average o(l)value over different samples at each iteration. For each sample x,o(l)was always
positive and usually increased over iterations, which veriﬁed Lemma 3. Besides, the assumption for a
tiny"(l)
tin Lemma 3 was veriﬁed by experimental results in the supplementary material.
Assumption 1. We assume that the MLP encodes features of very few (a single or two) categories in
the ﬁrst phase, while other categories have not been learned in this phase.
Figure 7 veriﬁes this assumption. It shows that only a single or two categories exhibit much higher
accuracies than the random guessing at the end of the ﬁrst phase. This indicates that the learning of
the MLP is dominated by training samples of a single or two categories in very early iterations.
Enhancement of the signiﬁcance of the common direction caused by all training samples. The
overall learning dynamics in the ﬁrst phase can be roughly described as follows, by combining
Theorem 2 and Assumption 1. The overall learning effects of all training samples are dominated
by very few categories ^c. There are two effects. First, features F(l 1)
t of different samples are all
pushed towards the vector ^cV(l)
t, where^cis determined by the dominating category/categories ^c.
Second,V(l)
tis pushed towards ^cEx2X^c[F(l 1)
tjx]. Therefore, features F(l 1)
t of different samples
and^cV(l)
tenhance each other, just like a self-enhanced system, in the scenario that F(l 1)
t and
F(l 1)
t of most samples have similar directions, and V(l)
tandV(l)
thave similar directions (see the
background assumption). In other words, the component along the common direction C(l)V(l)>
t in
W(l)
t=C(l)V(l)>
t+ "(l)>
twill be further enhanced.
Explaining the increasing feature similarity and the increasing gradient similarity. As afore-
mentioned, features F(l 1)
t of different samples are consistently pushed towards the same vector ^cV(l)
t.
Therefore, the similarity between features of different samples Ex;x02X[cos(F(l 1)
tjx;F(l 1)
tjx0)]in-
creases in the ﬁrst phase. On the other hand, the increasing similarity between feature gradients can
be explained from two views. (1) The increasing feature similarity over different samples makes
different training samples generate similar gating states D(l)
tin each ReLU layer. The increasing
similarity between gating states of each ReLU layer over different samples leads to the increasing
similarity between feature gradients over different samples Ex;x02Xc[cos( _F(l 1)
tjx;_F(l 1)
tjx0)]of the
same category. (2) Another view is that the component along the common direction C(l)V(l)>
t inW(l)
t
is enhanced in the ﬁrst phase. Because C(l)denotes the principle weight direction of each the i-th
columnw(l)
t;iofW(l)
t,i.e.,each “pseudo-neuron” w(l)
t;iis optimized towards the common direction C(l).
Eq.(3)has proved that the increasing cosine similarity between w(l)
t;iandC(l)for all “pseudo-neurons”
will lead to the increasing similarity between feature gradients over different samples.
How to escape from the ﬁrst phase? In the ﬁrst phase, the MLP only discovers a single direction
to optimize a single or two categories. However, the optimization of a single or two categories
will soon saturate, and the gradient mainly comes from training samples of other categories, which
disturbs the dominating roles of a single or two categories in the learning of the MLP. Therefore, the
learning effects of training samples from different categories may conﬂict with each other. Thus, the
self-enhanced system is destroyed, and the learning of the MLP enters the second phase.
8

--- PAGE 9 ---
Cosine Similarity
of featuresCIFAR-10 MNIST Tiny ImageNet
iteration(a)
iteration iteration iteration iteration iterationCIFAR-10 MNIST Tiny ImageNet
L=9, BN L=9 L=7, BN L=70.6
0.5
0.4(b)
0 1000 20000.6
0.4
0 1000 20000.6
0.3
0 2500 50000.6
0.4
0 1000 20000.6
0.40.8
0 1000 20000.5
0.40.6
0 2500 5000
L=9, 𝛾𝛾=0.5 L=9, 𝛾𝛾=1 L=7, 𝛾𝛾=0.5 L=7, 𝛾𝛾=1Figure 8: Effects of (a) normalization and (b) initialization. We trained L-layer MLPs, where each
layer had 512 neurons. A shorter ﬁrst phase indicates that the decrease of feature diversity is more
alleviated. Effects of normalization and L2regularization are shown in the supplementary material.
4.3 Theoretically alleviating the decrease of feature diversity
In previous sections, we have discovered and explained a fundamental yet counter-intuitive two-phase
phenomenon with the MLP. This is the distinctive contribution of this study, which has not been
theoretically explained for a long time. Besides, we ﬁnd that we can use the above ﬁndings to explain
that four typical operations can usually alleviate the decrease of feature diversity, i.e., normalization,
momentum, initialization, and L2regularization. Although these operations have been widely used,
previous studies failed to theoretically explain their effectiveness. To this end, our analysis can
explain a high likelihood for such operations to alleviate the decrease of feature diversity, but it is not
a proof of a strict sufﬁcient condition or a necessary condition for the feature diversity.
Normalization. Based on theoretical analysis, we explain that normalization operations ( e.g.batch
normalization (BN)) can alleviate the decrease of feature diversity in the ﬁrst phase. Speciﬁcally,
according to Theorem 2, the self-enhanced system of decreasing feature diversity requires features F(l)
t
of any two training samples xandx0in the same category to be similar to each other. However, the BN
layer prevents features F(l)
tof different samples from being similar to each other, because the mean
feature F(l)
t=Ex2X[F(l)
tjx]is subtracted from features of all samples, i.e.,F0(l)
tjx=F(l)
tjx F(l)
t.
Therefore, features F0(l)
tof different samples are no longer similar to each other, thereby being more
likely to break the self-enhanced system. Please see the supplementary material for more discussions.
We conducted experiments to verify the above analysis. We compared MLPs trained with and without
BN layers. Speciﬁcally, we added a BN layer after each linear layer to construct MLPs. Figure
8(a) shows that the feature similarity in MLPs with BN layers kept decreasing. This veriﬁed that
BN layers alleviated the decrease of feature diversity. Besides, based on our analysis, we further
simpliﬁed the BN layer, which could also alleviate the decrease of feature diversity. Please see the
supplementary material for more details.
Momentum. Based on the theoretical explanation for the decrease of feature diversity, we can explain
that momentum in gradient descent can alleviate this phenomenon. Based on Lemma 3, the self-
enhanced system of the decreasing of feature diversity requires weights along other directions "(l)
tto
be small enough. However, because the momentum operation strengthens inﬂuences of the initialized
noisy weights W(l)
t=0, it strengthens singular values of "(l)
t, to some extent, thereby alleviating the
decrease of feature diversity. Speciﬁcally, a larger coefﬁcient of momentum musually more alleviate
the decrease of feature diversity.
Please see the supplementary material for the detailed discussion. To verify the above analysis, we
trained MLPs with m= 0;0:5;0:9, respectively. The veriﬁcation of our claim that a larger value of m
usually more alleviates the decrease of feature diversity is introduced in the supplementary material.
Initialization. We explain that the initialization of MLPs also affects the decrease of feature diversity.
According to Lemma 3, such a self-enhanced system requires weights along other directions "(l)
tto be
small enough. However, because increasing the variance of the initialized weights W(l)
t=0will increase
singular values of "(l)
t, alleviating the decrease of feature diversity. Please see the supplementary
material for the detailed discussion.
We conducted experiments to verify the above claim by comparing MLPs trained using different
initializations with different variances. We used to control the variance of the initialization, i.e.,
W(l)
t=0N(0;2
var), wherevaris a constant. Figure 8(b) veriﬁes that the initialization with a large
variance alleviated the decrease of feature diversity.
L2regularization. We also explain that a small coefﬁcient of the L2regularization can alleviate
the decrease of feature diversity. I.e., The total lossL(Wt) =LCE(Wt) +kWtk2
2, whereLCE(Wt)
9

--- PAGE 10 ---
represents the cross entropy loss and kWtk2
2denotes theL2regularization loss. As aforementioned,
the decrease of feature diversity requires singular values of "(l)
tto be small enough. However, because
a smaller coefﬁcient of theL2regularization yields larger singular values of "(l)
t, it alleviates the
decrease of feature diversity. Please see the supplementary material for the detailed discussion. The
veriﬁcation that a smaller coefﬁcient more alleviated the decrease of feature diversity is introduced
in the supplementary material.
5 Conclusion
In this paper, we ﬁnd that in the early stage of the training process, the MLP exhibits a fundamental
yet counter-intuitive two-phase phenomenon, i.e., the feature diversity keeps decreasing in the ﬁrst
phase. We explain this phenomenon by analyzing the learning dynamics of the MLP. Furthermore,
we explain the reason why four typical operations can alleviate the decrease of feature diversity.
References
[1]Alessandro Achille and Stefano Soatto. Information dropout: Learning optimal representations
through noisy computation. IEEE transactions on pattern analysis and machine intelligence ,
40(12):2897–2905, 2018.
[2]Ben Adlam and Jeffrey Pennington. The neural tangent kernel in high dimensions: Triple
descent and a multi-scale theory of generalization. In International Conference on Machine
Learning , pages 74–84. PMLR, 2020.
[3]Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in
neural networks. arXiv preprint arXiv:1710.03667 , 2017.
[4]Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In International Conference on Machine Learning , pages
244–253. PMLR, 2018.
[5]Devansh Arpit, Stanisław Jastrz˛ ebski, Nicolas Ballas, David Krueger, Emmanuel Bengio,
Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al.
A closer look at memorization in deep networks. In International Conference on Machine
Learning , pages 233–242. PMLR, 2017.
[6] Arthur Asuncion and David Newman. Uci machine learning repository, 2007.
[7]Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning
from examples without local minima. Neural networks , 2(1):53–58, 1989.
[8]Hao Cheng, Dongze Lian, Shenghua Gao, and Yanlin Geng. Evaluating capability of deep
neural networks for image classiﬁcation via information plane. In European Conference on
Computer Vision , pages 181–195. Springer, 2018.
[9]Anna Choromanska, Mikael Henaff, Michael Mathieu, Gérard Ben Arous, and Yann LeCun.
The loss surfaces of multilayer networks. In Artiﬁcial intelligence and statistics , pages 192–204.
PMLR, 2015.
[10] Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. Advances In Neural Information
Processing Systems , 29:2253–2261, 2016.
[11] Stéphane d’Ascoli, Maria Reﬁnetti, Giulio Biroli, and Florent Krzakala. Double trouble in
double descent: Bias and variance (s) in the lazy regime. In International Conference on
Machine Learning , pages 2280–2290. PMLR, 2020.
[12] Stéphane d’Ascoli, Levent Sagun, and Giulio Biroli. Triple descent and the two kinds of
overﬁtting: where & why do they appear? In NeurIPS , 2020.
[13] Gianna M Del Corso, Antonio Gulli, and Francesco Romani. Ranking a stream of news. In
Proceedings of the 14th international conference on World Wide Web , pages 97–106, 2005.
10

--- PAGE 11 ---
[14] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations ,
2018.
[15] Stanislav Fort, Paweł Krzysztof Nowak, Stanislaw Jastrzebski, and Srini Narayanan. Stiffness:
A new perspective on generalization in neural networks. arXiv preprint arXiv:1901.09491 ,
2019.
[16] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable
neural networks. In International Conference on Learning Representations , 2018.
[17] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedfor-
ward neural networks. In Proceedings of the thirteenth international conference on artiﬁcial
intelligence and statistics , pages 249–256. JMLR Workshop and Conference Proceedings, 2010.
[18] Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint
arXiv:1611.04231 , 2016.
[19] Reinhard Heckel and Fatih Furkan Yilmaz. Early stopping in deep networks: Double descent
and how to eliminate it. In International Conference on Learning Representations , 2020.
[20] Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the
generalization gap in large batch training of neural networks. In Proceedings of the 31st
International Conference on Neural Information Processing Systems , pages 1729–1739, 2017.
[21] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In International conference on machine learning , pages
448–456. PMLR, 2015.
[22] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. arXiv preprint arXiv:1806.07572 , 2018.
[23] Arthur Jacot, Berﬁn Simsek, Francesco Spadaro, Clément Hongler, and Franck Gabriel. Implicit
regularization of random feature models. In International Conference on Machine Learning ,
pages 4631–4640. PMLR, 2020.
[24] Stanisław Jastrz˛ ebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua
Bengio, and Amos Storkey. Three factors inﬂuencing minima in sgd. arXiv preprint
arXiv:1711.04623 , 2017.
[25] Kenji Kawaguchi. Deep learning without poor local minima. Advances in Neural Information
Processing Systems , 29:586–594, 2016.
[26] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
[27] Andrew K Lampinen and Surya Ganguli. An analytic theory of generalization dynamics and
transfer learning in deep linear networks. In International Conference on Learning Representa-
tions , 2018.
[28] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N , 7:7, 2015.
[29] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning
applied to document recognition. Proceedings of the IEEE , 86(11):2278–2324, 1998.
[30] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and
Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. In International Conference
on Learning Representations , 2018.
[31] Xingguo Li, Junwei Lu, Zhaoran Wang, Jarvis Haupt, and Tuo Zhao. On tighter generalization
bound for deep neural networks: Cnns, resnets, and beyond. arXiv preprint arXiv:1806.05159 ,
2018.
[32] Philip M Long and Hanie Sedghi. Generalization bounds for deep convolutional neural networks.
InInternational Conference on Learning Representations , 2019.
11

--- PAGE 12 ---
[33] Andrew L Maas, Awni Y Hannun, Andrew Y Ng, et al. Rectiﬁer nonlinearities improve neural
network acoustic models. In International Conference on Machine Learning , volume 30, page 3.
Citeseer, 2013.
[34] Michael Mahoney and Charles Martin. Traditional and heavy tailed self regularization in neural
network models. In International Conference on Machine Learning , pages 4284–4293. PMLR,
2019.
[35] Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan
Srebro, and Daniel Soudry. Convergence of gradient descent on separable data. In The 22nd
International Conference on Artiﬁcial Intelligence and Statistics , pages 3420–3428. PMLR,
2019.
[36] Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever.
Deep double descent: Where bigger models and more data hurt. In International Conference on
Learning Representations , 2019.
[37] Roman Novak, Yasaman Bahri, Daniel A Abolaﬁa, Jeffrey Pennington, and Jascha Sohl-
Dickstein. Sensitivity and generalization in neural networks: an empirical study. In International
Conference on Learning Representations , 2018.
[38] Jeffrey Pennington, Samuel S Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in
deep learning through dynamical isometry: theory and practice. In Proceedings of the 31st
International Conference on Neural Information Processing Systems , pages 4788–4798, 2017.
[39] Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural
networks. In International Conference on Machine Learning , pages 4433–4441. PMLR, 2018.
[40] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear
dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120 , 2013.
[41] Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via
information. arXiv preprint arXiv:1703.00810 , 2017.
[42] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale
image recognition. arXiv preprint arXiv:1409.1556 , 2014.
[43] Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban. A tail-index analysis of stochastic
gradient noise in deep neural networks. In International Conference on Machine Learning ,
pages 5827–5837. PMLR, 2019.
[44] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y
Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a
sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural
language processing , pages 1631–1642, 2013.
[45] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The
implicit bias of gradient descent on separable data. The Journal of Machine Learning Research ,
19(1):2822–2878, 2018.
[46] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability
judgments. Transactions of the Association for Computational Linguistics , 7:625–641, 2019.
[47] Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh,
and Luca Daniel. Evaluating the robustness of neural networks: An extreme value theory
approach. In International Conference on Learning Representations , 2018.
[48] Wolchover. New theory cracks open the black box of deep learning. Quanta Magazine , 3, 2017.
[49] Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of
learning algorithms. Advances in Neural Information Processing Systems , 2017:2525–2534,
2017.
[50] Tengyu Xu, Yi Zhou, Kaiyi Ji, and Yingbin Liang. Convergence of sgd in learning relu models
with separable data. arXiv preprint arXiv:1806.04339 , 2018.
12

--- PAGE 13 ---
[51] Zhiqin John Xu. Understanding training and generalization in deep learning by fourier analysis.
arXiv preprint arXiv:1808.04295 , 2018.
[52] Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and Yi Ma. Rethinking bias-variance
trade-off for generalization of neural networks. In International Conference on Machine
Learning , pages 10767–10777. PMLR, 2020.
13

--- PAGE 14 ---
A Common phenomenon shared by different DNNs for different tasks.
In this section, we aim to demonstrate an interesting two-phase phenomenon when we train an MLP
in early iterations. Speciﬁcally, the training process of the MLP can usually be divided into the
following two phases according to the training loss. In the ﬁrst phase, the training loss does not
decrease signiﬁcantly, and the training loss suddenly begins to decrease in the second phase. More
crucially, the feature diversity decreases in the ﬁrst phase. This phenomenon is widely shared by
different DNNs with different architectures for different tasks.
Let us take the 9-layer MLP trained on the CIFAR-10 dataset for an example, where each layer of the
MLP had 512 neurons. As Figure 1(e)(f) shows, before the 1300-th iteration (the ﬁrst phase), both the
feature diversity and the gradient diversity kept decreasing, i.e., both the cosine similarity between
features over different samples and the cosine similarity between gradients kept increasing. After the
1300-th iteration (the second phase), the feature diversity and the gradient diversity suddenly began to
increase, i.e.their similarities began to decrease. Therefore, the MLP had the lowest feature diversity
and the lowest gradient diversity at around the 1300-th iteration. Speciﬁcally, the training loss was
evaluated on the whole training set.
A.1 On the CIFAR-10 dataset
In this subsection, we demonstrated that the two-phase phenomenon was shared by different MLPs
on the CIFAR-10 dataset [ 26]. For different MLPs, we adopted the learning rate = 0:1, the batch
sizebs= 100 , the SGD optimizer, and the ReLU activation function. Besides, we used two data
augmentation methods, including random cropping and random horizontal ﬂipping. The training loss,
the testing loss, the training accuracy, the testing accuracy, the cosine similarity of features, and the
cosine similarity of feature gradients of MLPs trained on the CIFAR-10 dataset are shown in Figure
1.
5-layer MLP with 128
neurons in each layer
7-layer MLP with 256neurons in each layer
7-layer MLP with 512neurons in each layerTraining Accuracy(%)Testing Accuracy(%)
C
osine similarity 
of features
Cosine similarity 
of gradientsTraining Loss
Testing Loss
iteration iteration iteration(a) (b) (c
)
(d) (e) (f)9-layer MLP with 512neurons in each layer
cifar10
Figure 1: (a) The training loss of four MLPs trained on the CIFAR-10 dataset. (b) The testing loss of
four MLPs. (c) Training accuracies of four MLPs. (d) Testing accuracies of four MLPs. (e) Cosine
similarity between features of different categories. (f) Cosine similarity between gradients of different
samples in a category. The feature and the feature gradient were used in the third linear layer of
MLPs.
A.2 On the MNIST dataset
In this subsection, we demonstrated that the two-phase phenomenon was shared by different MLPs on
the MNIST dataset [ 29]. For different MLPs, we adopted the learning rate = 0:01, the batch size
bs= 100 , the SGD optimizer, and the ReLU activation function. The training loss, the testing loss,
the training accuracy, the testing accuracy, the cosine similarity of features, and the cosine similarity
of feature gradients of MLPs trained on the MNIST are shown in Figure 2.
A.3 On the Tiny ImageNet dataset
In this subsection, we demonstrated that the two-phase phenomenon was shared by different MLPs
on the Tiny ImageNet dataset [ 28]. Speciﬁcally, we randomly selected the following 50 categories,
14

--- PAGE 15 ---
5-layer MLP with 128
neurons in each layer
5-layer MLP with 256neurons in each layer
7-layer MLP with 256neurons in each layerTraining Accuracy(%)Testing Accuracy(%)
C
osine similarity 
of features
Cosine similarity 
of gradientsTraining Loss
Testing Loss
iteration iteration iteration(a) (b) (c)
(d) (e) (f)7-layer 
MLP with 512
neurons in each layer
mnist102103104
102103104102103104102103104Figure 2: (a) The training loss of four MLPs tranined on the MNIST dataset. (b) The testing loss of
four MLPs. (c) Training accuracies of four MLPs. (d) Testing accuracies of four MLPs. (e) Cosine
similarity between features of different categories. (f) Cosine similarity between gradients of different
samples in a category. The feature and the feature gradient were used in the third linear layer of
MLPs.
orangutan, parking meter, snorkel, American alligator, oboe, basketball, rocking chair, hopper, neck
brace, candy store, broom, seashore, sewing machine, sunglasses, panda, pretzel, pig, volleyball,
puma, alp, barbershop, ox, ﬂagpole, lifeboat, teapot, walking stick, brain coral, slug, abacus, comic
book, CD player, school bus, banister, bathtub, German shepherd, black stork, computer keyboard,
tarantula, sock, Arabian camel, bee, cockroach, cannon, tractor, cardigan, suspension bridge, beer
bottle, viaduct, guacamole , and iPod for training. For different MLPs, we adopted the learning rate
= 0:1, the batch size bs= 100 , the SGD optimizer, and the ReLU activation function. Besides, we
used two data augmentation methods, including random cropping and random horizontal ﬂipping.
Note that we took a random cropping with 32 32 sizes.The training loss, the testing loss, the training
accuracy, the testing accuracy, the cosine similarity of features, and the cosine similarity of feature
gradients of MLPs trained on the Tiny ImageNet are shown in Figure 3.
5-layer MLP with 128
neurons in each layer
7-layer MLP with 256neurons in each layer
9-layer MLP with 512neurons in each layerTraining Accuracy(%)Testing Accuracy(%)
C
osine similarity 
of features
Cosine similarity 
of gradientsTraining Loss
Testing Loss
iteration iteration iteration(a) (b) (c)
(d) (e) (f)5
55 5
Figure 3: (a) The training loss of three MLPs tranined on the Tiny ImageNet dataset. (b) The testing
loss of three MLPs. (c) Training accuracies of three MLPs. (d) Testing accuracies of three MLPs. (e)
Cosine similarity between features of different categories. (f) Cosine similarity between gradients of
different samples in a category. The features and the feature gradient were used in the second linear
layer of MLPs.
A.4 On the Census dataset
In this subsection, we demonstrated that the two-phase phenomenon was shared by different MLPs
on the UCI census income tabular dataset (Census) [ 6]. For different MLPs, we adopted the learning
rate= 0:1, the batch size bs= 1000 , the SGD optimizer, and the ReLU activation function. The
training loss, the testing loss, the training accuracy, the testing accuracy, the cosine similarity of
features, and the cosine similarity of feature gradients of MLPs trained on the census are shown in
Figure 4.
15

--- PAGE 16 ---
8-layer MLP with 128
neurons in each layer
9-layer MLP with 128neurons in each layerTraining Accuracy(%)Testing Accuracy(%)
C
osine similarity 
of features
Cosine similarity 
of gradientsTraining Loss
Testing Loss
iteration iteration iteration(a) (b) (c)
(d) (e) (f)
census10-layer MLP with 128neurons in each layerFigure 4: (a) The training loss of three MLPs trained on the Census dataset. (b) The testing loss
of three MLPs. (c) Training accuracies of three MLPs. (d) Testing accuracies of three MLPs. (e)
Cosine similarity between features of different categories. (f) Cosine similarity between gradients of
different samples in a category. The feature and the feature gradient were used in the ﬁfth linear layer
of MLPs.
A.5 On the Commercial dataset
In this subsection, we demonstrated that the two-phase phenomenon was shared by different MLPs
on the UCI TV news channel commercial detection dataset (Commercial) [ 6]. For different MLPs,
we adopted the learning rate = 0:1, the batch size bs= 1000 , the SGD optimizer, and the ReLU
activation function. The training loss, the testing loss, the training accuracy, the testing accuracy, the
cosine similarity of features, and the cosine similarity of feature gradients of MLPs trained on the
census are shown in Figure 5.
7-layer MLP with 128
neurons in each layer
8-layer MLP with 128neurons in each layerTraining Accuracy(%)Testing Accuracy(%)
C
osine similarity 
of features
Cosine similarity 
of gradientsTraining Loss
Testing Loss
iteration iteration iteration(a) (b) (c)
(d) (e) (f)
commercial9-layer MLP with 128neurons in each layer
Figure 5: (a) The training loss of three MLPs trained on the Commercial dataset. (b) The testing loss
of three MLPs. (c) Training accuracies of three MLPs. (d) Testing accuracies of three MLPs. (e)
Cosine similarity between features of different categories. (f) Cosine similarity between gradients of
different samples in a category. The feature and the feature gradient were used in the ﬁfth linear layer
of MLPs.
A.6 On the CoLA dataset
In this subsection, we demonstrated that the two-phase phenomenon was shared by the revised
LSTMs on the CoLA dataset [ 46]. We used two-layer unidirectional LSTMs concatenated with
MLPs. Speciﬁcally, we trained two LSTMs with 5-layer MLPs, where each layer of the MLP has
256 and 512 neurons. We adopted the learning rate = 0:1, the batch size bs= 1000 , the SGD
optimizer, and the ReLU activation function. The training loss, the testing loss, the training accuracy,
the testing accuracy, the cosine similarity of features, and the cosine similarity of feature gradients of
LSTMs trained on the CoLA are shown in Figure 6. Since training samples in the CoLA dataset were
imbalanced, we constructed a new training set by randomly sampling 2000 training samples from
two categories, respectively. DNNs were trained on this new training set.
16

--- PAGE 17 ---
2-layer LSTM, 5-layer MLP
with 128 neurons in each layer
2-layer LSTM, 5-layer MLPwith 256 neurons in each layerTraining Accuracy(%)Testing Accuracy(%)
C
osine similarity 
of features
Cosine similarity 
of gradientsTraining Loss
Testing Loss
iteration iteration iteration(a) (b) (c)
(d) (e) (f)
CoLA layer 3Figure 6: (a) The training loss of two LSTMs trained on the CoLA dataset. (b) The testing loss
of two LSTMs. (c) Training accuracies of two LSTMs. (d) Testing accuracies of two LSTMs. (e)
Cosine similarity between features of different categories. (f) Cosine similarity between gradients
of different samples in a category. The feature and the feature gradient were used in the third linear
layer of MLPs.
A.7 On the SST-2 dataset
In this subsection, we demonstrated that the two-phase phenomenon was shared by the revised LSTMs
on the SST-2 dataset [ 44]. We used unidirectional LSTMs concatenated with MLPs. Speciﬁcally,
we trained three LSTMs with 4-layer MLPs, 4-layer MLPs, and 5-layer MLPs, respectively, where
each layer of the MLP has 32, 64, 128 neurons. We adopted the learning rate = 0:1, the batch size
bs= 500 , the SGD optimizer, and the ReLU activation function. Since the training of LSTMs on the
SST-2 with the SGD optimizer is unstable, we randomly selected 15000 training samples from the
training set. We trained LSTMs on these 15000 training samples. The training loss, the testing loss,
the training accuracy, the testing accuracy, the cosine similarity of features, and the cosine similarity
of feature gradients of LSTMs trained on the SST-2 are shown in Figure 7.
1-layer LSTM, 4-layer MLP
with 32 neurons in each layer
2-layer LSTM, 4-layer MLPwith 64 neurons in each layerTraining Accuracy(%)Testing Accuracy(%)
C
osine similarity 
of features
Cosine similarity 
of gradientsTraining Loss
Testing Loss
iteration iteration iteration(a) (b) (c)
(d) (e) (f)
SST-2 layer 23-layer LSTM, 5-layer MLPwith 128 neurons in each layer
Figure 7: (a) The training loss of three LSTMs trained on the SST-2 dataset. (b) The testing loss of
three LSTMs. (c) Training accuracies of three LSTMs. (d) Testing accuracies of three LSTMs. (e)
Cosine similarity between features of different categories. (f) Cosine similarity between gradients of
different samples in a category. The feature and the feature gradient were used in the second linear
layer of MLPs.
A.8 On the AGNews dataset
In this subsection, we demonstrated that the two-phase phenomenon was shared by the revised
LSTMs on the AGNEWS dataset. We used two-layer unidirectional LSTMs concatenated with MLPs.
Speciﬁcally, we trained three LSTMs with 4-layer MLPs, 4-layer MLPs, 5-layer MLPs, respectively,
where each layer of the MLP had 32, 64, and 128 neurons, respectively. We adopted the learning rate
= 0:1, the batch size bs= 500 , the SGD optimizer, and the ReLU activation function. The training
loss, the testing loss, the training accuracy, the testing accuracy, the cosine similarity of features, and
the cosine similarity of feature gradients of LSTMs trained on the AGNEWS are shown in Figure 8.
17

--- PAGE 18 ---
2-layer LSTM, 4-layer MLP
with 32 neurons in each layer
2-layer LSTM, 4-layer MLPwith 64 neurons in each layerTraining Accuracy(%)Testing Accuracy(%)
C
osine similarity 
of features
Cosine similarity 
of gradientsTraining Loss
Testing Loss
iteration iteration iteration(a) (b) (c)
(d) (e) (f)
AGNEWS layer 22-layer LSTM, 5-layer MLPwith 128 neurons in each layerFigure 8: (a) The training loss of three LSTMs trained on the AGNEWS dataset. (b) The testing loss
of three LSTMs. (c) Training accuracies of three LSTMs. (d) Testing accuracies of three LSTMs. (e)
Cosine similarity between features of different categories. (f) Cosine similarity between gradients of
different samples in a category. The feature and the feature gradient were used in the second linear
layer of MLPs.
A.9 Different training batch sizes
In this subsection, we demonstrated that the two-phase phenomenon was shared by MLPs trained on
the CIFAR-10 dataset with different training batch sizes. For different MLPs, we adopted the learning
rate= 0:1, the SGD optimizer, and the ReLU activation function. Besides, we used two data
augmentation methods, including random cropping and random horizontal ﬂipping. We trained three
7-layer MLPs with 256 neurons in each layer, with bs= 100;500;1000 respectively. The training
loss, the testing loss, the training accuracy, the testing accuracy, the cosine similarity of features, and
the cosine similarity of feature gradients of MLPs trained with different batch sizes are shown in
Figure 9.
batch size = 100
batch size = 500
batch size = 1000Training Accuracy(%)Testing Accuracy(%)
C
osine similarity 
of features
Cosine similarity 
of gradientsTraining Loss
Testing Loss
iteration iteration iteration(a) (b) (c)
(d) (e) (f)4 4 4
4
Figure 9: (a) The training loss of three MLPs trained with different batch sizes. (b) The testing loss
of three MLPs. (c) Training accuracies of three MLPs. (d) Testing accuracies of three MLPs. (e)
Cosine similarity between features of different categories. (f) Cosine similarity between gradients of
different samples in a category. The feature and the feature gradient were used in the second linear
layer of MLPs.
A.10 Different learning rates
In this subsection, we demonstrated that the two-phase phenomenon was shared by MLPs trained
on the CIFAR-10 dataset with different learning rates. For different MLPs, we adopted the batch
sizebs= 100 , the SGD optimizer, and the ReLU activation function. Besides, we used two data
augmentation methods, including random cropping and random horizontal ﬂipping. We trained two
7-layer MLPs with 256 neurons in each layer, with learning rates = 0:1;0:01respectively. The
training loss, the testing loss, the training accuracy, the testing accuracy, the cosine similarity of
features, and the cosine similarity of feature gradients of MLPs trained with different learning rates
are shown in Figure 10.
18

--- PAGE 19 ---
learning rate = 0.1
learning rate = 0.01Training Accuracy(%)Testing Accuracy(%)
C
osine similarity 
of features
Cosine similarity 
of gradientsTraining Loss
Testing Loss
iteration iteration iteration(a) (b) (c)
(d) (e
) (f)
Different learning rateFigure 10: (a) The training loss of two MLPs trained with different learning rates. (b) The testing
loss of two MLPs. (c) The training accuracies of two MLPs. (d) The testing accuracies of two MLPs.
(e) Cosine similarity between features of different categories. (f) Cosine similarity between gradients
of different samples in a category. The feature and the feature gradient were used in the second linear
layer of MLPs.
A.11 Different activation functions
In this subsection, we demonstrated that the two-phase phenomenon was shared by MLPs with
different activation functions. For different MLPs, we adopted the learning rate = 0:1, the batch
sizebs= 100 , the SGD optimizer. Besides, we used two data augmentation methods, including
random cropping and random horizontal ﬂipping. We trained three 9-layer MLPs with 512 neurons
in each layer with the ReLU activation function, the Leaky ReLU (slope=0.1) activation function,
and the Leaky ReLU (slope=0.01) activation function, respectively. The training loss, the testing loss,
the training accuracy, the testing accuracy, the cosine similarity of features, and the cosine similarity
of feature gradients of MLPs trained with different activation functions are shown in Figure 11.
MLP with ReLU
MLP with LeakyReLU (slope=0.01)
MLP with LeakyReLU (slope=0.1)Training Accuracy(%)Testing Accuracy(%)
C
osine similarity 
of features
Cosine similarity 
of gradientsTraining Loss
Testing Loss
iteration iteration iteration(a) (b) (c
)
(d) (e) (f)
Different activation layer
Figure 11: (a) The training loss of three MLPs with different activation functions. (b) The testing
loss of three MLPs. (c) Training accuracies of three MLPs. (d) Testing accuracies of three MLPs. (e)
Cosine similarity between features of different categories. (f) Cosine similarity between gradients of
different samples in a category. The feature and the feature gradient were used in the second linear
layer of MLPs.
A.12 Different momentums
In this subsection, we demonstrated that the two-phase phenomenon was shared by MLPs trained on
the CIFAR-10, MNIST and Tiny Imagenet dataset with different momentums. For different MLPs,
we adopted the learning rate = 0:1, the batch size bs= 100 , the SGD optimizer. Besides, we used
two data augmentation methods, including random cropping and random horizontal ﬂipping. We
trained 7-layer MLPs and 9-layer MLPs with 512 neurons in each layer with the ReLU activation
function. The training loss, the testing loss, the training accuracy, the testing accuracy, the cosine
similarity of features, and the cosine similarity of feature gradients of MLPs trained with different
momentum are shown in Figure 12, Figure 13, Figure 14, Figure 15, Figure 16, and Figure 17.
19

--- PAGE 20 ---
Training Accuracy(%)Testing Accuracy(%)
C
osine similarity 
of features
Cosine similarity 
of gradientsTraining Loss
Testing Loss
iteration iteration iteration(a) (b) (c)
(d) (e) (f)
Cifar10 7 -layer2- mom7-layer MLP with 512
neurons in each layer,
momentum = 0.97-layer MLP with 512
neurons in each layer,
momentum = 0.57-layer MLP with 512
neurons in each layer,
momentum = 0Figure 12: (a) The training loss of three MLPs with different momentums trained on the CIFAR-10
dataset. (b) The testing loss of three MLPs. (c) Training accuracies of three MLPs. (d) Testing
accuracies of three MLPs. (e) Cosine similarity between features of different categories. (f) Cosine
similarity between gradients of different samples in a category. The feature and the feature gradient
were used in the second linear layer of MLPs.
Training Accuracy(%)Testing Accuracy(%)
C
osine similarity 
of features
Cosine similarity 
of gradientsTraining Loss
Testing Loss
iteration iteration iteration(a) (b) (c)
(d) (e) (f)
mnist 7-layer1- mom7-layer MLP with 512
neurons in each layer,
momentum = 0.97-layer MLP with 512
neurons in each layer,
momentum = 0.57-layer MLP with 512
neurons in each layer,
momentum = 0
Figure 13: (a) The training loss of three MLPs with different momentums trained on the MNIST
datasets. (b) The testing loss of three MLPs. (c) Training accuracies of three MLPs. (d) Testing
accuracies of three MLPs. (e) Cosine similarity between features of different categories. (f) Cosine
similarity between gradients of different samples in a category. The feature and the feature gradient
were used in the second linear layer of MLPs.
Training Accuracy(%)Testing Accuracy(%)
C
osine similarity 
of features
Cosine similarity 
of gradientsTraining Loss
Testing Loss
iteration iteration iteration(a) (b) (c)
(d) (e) (f)
Tiny-7-layer4- mom7-layer MLP with 512
neurons in each layer,
momentum = 0.97-layer MLP with 512
neurons in each layer,
momentum = 0.57-layer MLP with 512
neurons in each layer,
momentum = 0
Figure 14: (a) The training loss of three MLPs with different momentums trained on the Tiny
ImageNet dataset. (b) The testing loss of three MLPs. (c) Training accuracies of three MLPs. (d)
Ttesting accuracies of three MLPs. (e) Cosine similarity between features of different categories. (f)
Cosine similarity between gradients of different samples in a category. The feature and the feature
gradient were used in the fourth linear layer of MLPs.
20

--- PAGE 21 ---
9-layer MLP with 512
neurons in each layer,
momentum = 0.9Training Accuracy(%)Testing Accuracy(%)
C
osine similarity 
of features
Cosine similarity 
of gradientsTraining Loss
Testing Loss
iteration iteration iteration(a) (b) (c)
(d) (e) (f)
Cifar10 9 -layer2- mom9-layer MLP with 512
neurons in each layer,
momentum = 0.59-layer MLP with 512
neurons in each layer,
momentum = 0Figure 15: (a) The training loss of three MLPs with different momentums trained on the CIFAR-10
dataset. (b) The testing loss of three MLPs. (c) Training accuracies of three MLPs. (d) Testing
accuracies of three MLPs. (e) Cosine similarity between features of different categories. (f) Cosine
similarity between gradients of different samples in a category. The feature and the feature gradient
were used in the second linear layer of MLPs.
Training Accuracy(%)Testing Accuracy(%)
C
osine similarity 
of features
Cosine similarity 
of gradientsTraining Loss
Testing Loss
iteration iteration iteration(a) (b) (c)
(d) (e) (f)
mnist 9-layer2- mom9-layer MLP with 512
neurons in each layer,
momentum = 0.59-layer MLP with 512
neurons in each layer,
momentum = 0
Figure 16: (a) The training loss of two MLPs with different momentums trained on the MNIST dataset.
(b) The testing loss of two MLPs. (c) Training accuracies of two MLPs. (d) Testing accuracies of two
MLPs. (e) Cosine similarity between features of different categories. (f) Cosine similarity between
gradients of different samples in a category. The feature and the feature gradient were used in the
second linear layer of MLPs.
Training Accuracy(%)Testing Accuracy(%)
C
osine similarity 
of features
Cosine similarity 
of gradientsTraining Loss
Testing Loss
iteration iteration iteration(a) (b) (c)
(d) (e) (f)
Tiny-9-layer4- mom9-layer MLP with 512
neurons in each layer,
momentum = 0.99-layer MLP with 512
neurons in each layer,
momentum = 0.59-layer MLP with 512
neurons in each layer,
momentum = 0
Figure 17: (a) The training loss of three MLPs with different momentums trained on the Tiny
ImageNet dataset. (b) The testing loss of three MLPs. (c) Training accuracies of three MLPs. (d)
Testing accuracies of three MLPs. (e) Cosine similarity between features of different categories. (f)
Cosine similarity between gradients of different samples in a category. The feature and the feature
gradient were used in the fourth linear layer of MLPs.
21

--- PAGE 22 ---
A.13 Different weight decays
In this subsection, we demonstrated that the two-phase phenomenon was shared by MLPs trained on
the CIFAR-10, MNIST and Tiny Imagenet dataset with different weight decays. For different MLPs,
we adopted the learning rate = 0:1, the batch size bs= 100 , the SGD optimizer. Besides, we used
two data augmentation methods, including random cropping and random horizontal ﬂipping. We
trained 7-layer MLPs and 9-layer MLPs with 512 neurons in each layer with the ReLU activation
function. The training loss, the testing loss, the training accuracy, the testing accuracy, the cosine
similarity of features, and the cosine similarity of feature gradients of MLPs trained with different
weight decays are shown in Figure 18, Figure 19, Figure 20, Figure 21, Figure 22, and Figure 23.
Training Accuracy(%)Testing Accuracy(%)
C
osine similarity 
of features
Cosine similarity 
of gradientsTraining Loss
Testing Loss
iteration iteration iteration(a) (b) (c)
(d) (e) (f)
Cifar10 7 -layer2- wd7-layer MLP with 512
neurons in each layer,
weight decay = 0.001
7-layer MLP with 512
neurons in each layer,
weight decay =  0.0001
Figure 18: (a) The training loss of two MLPs with different weight decays trained on the CIFAR-
10 dataset. (b) The testing loss of two MLPs. (c) Training accuracies of two MLPs. (d) Testing
accuracies of two MLPs. (e) Cosine similarity between features of different categories. (f) Cosine
similarity between gradients of different samples in a category. The feature and the feature gradient
were used in the second linear layer of MLPs.
Training Accuracy(%)Testing Accuracy(%)
C
osine similarity 
of features
Cosine similarity 
of gradientsTraining Loss
Testing Loss
iteration iteration iteration(a) (b) (c)
(d) (e) (f)
mnist 7-layer2- wd7-layer MLP with 512
neurons in each layer,
weight decay = 0.001
7-layer MLP with 512
neurons in each layer,
weight decay =  0.0001
Figure 19: (a) The training loss of two MLPs with different weight decays trained on the MNIST
dataset. (b) The testing loss of two MLPs. (c) Training accuracies of two MLPs. (d) Testing
accuracies of two MLPs. (e) Cosine similarity between features of different categories. (f) Cosine
similarity between gradients of different samples in a category. The feature and the feature gradient
were used in the second linear layer of MLPs.
B Discussion of the learning-sticking problem
In this section, we aim to discuss the learning-sticking problem in the learning of MLPs. In fact, this
problem appears in various DNNs, including MLPs, CNNs, and RNNs, when the task is difﬁcult
enough. In this paper, we just take the MLP as an example for discussion without loss of generality.
Explaining and solving the occasional sticking of the training of MLPs are of signiﬁcant values
on different tasks. Speciﬁcally, previous studies simply owed the learning-sticking problem to the
difﬁculty of the training task and solved this problem by heuristically applying some optimization
tricks. They have no insightful analysis or theoretically supported solutions. In comparison, our
study explains the learning-sticking problem as the ﬁrst phase with an inﬁnite length. Moreover, we
theoretically explain mechanisms of several heuristic solutions to the learning-sticking problem.
22

--- PAGE 23 ---
Training Accuracy(%)Testing Accuracy(%)
C
osine similarity 
of features
Cosine similarity 
of gradientsTraining Loss
Testing Loss
iteration iteration iteration(a) (b) (c)
(d) (e) (f)
Tiny-7-layer3- wd7-layer MLP with 512
neurons in each layer,
weight decay = 0.0001
7-layer MLP with 512
neurons in each layer,
weight decay =  0.00001Figure 20: (a) The training loss of two MLPs with different weight decays trained on the Tiny
ImageNet dataset. (b) The testing loss of two MLPs. (c) Training accuracies of two MLPs. (d)
Testing accuracies of two MLPs. (e) Cosine similarity between features of different categories. (f)
Cosine similarity between gradients of different samples in a category. The feature and the feature
gradient were used in the third linear layer of MLPs.
Training Accuracy(%)Testing Accuracy(%)
C
osine similarity 
of features
Cosine similarity 
of gradientsTraining Loss
Testing Loss
iteration iteration iteration(a) (b) (c)
(d) (e) (f)
Cifar10 9 -layer2- wd9-layer MLP with 512
neurons in each layer,
weight decay = 0.001
9-layer MLP with 512
neurons in each layer,
weight decay =  0.0001
Figure 21: (a) The training loss of two MLPs with different weight decays trained on the CIFAR-
10 dataset. (b) The testing loss of two MLPs. (c) Training accuracies of two MLPs. (d) Testing
accuracies of two MLPs. (e) Cosine similarity between features of different categories. (f) Cosine
similarity between gradients of different samples in a category. The feature and the feature gradient
were used in the second linear layer of MLPs.
To this end, the learning-sticking problem can be solved based on our study, as shown in Figure 24.
Speciﬁcally, we trained a 9-layer MLP on the CIFAR-10 dataset, where each layer of the MLP had
512 neurons and its initial weights were sample from N(0;12
var)(1= 0:1). We observed that the
training of the MLP got stuck (orange curve). According to our study, the technique of increasing the
variance of initial weights can shorten the ﬁrst phase, thereby solving the learning-sticking problem.
To this end, we trained another 9-layer MLP, and the only difference from the previous MLP is that
the variance of initial weights was increased to 22
var(2= 1). Figure 24(a) shows that the ﬁrst
phase is shortened by increasing the variance of initial weights (another MLP in the blue curve),
thereby the learning-sticking problem is solved.
Actually, far beyond solving the learning-sticking problem, the two-phase phenomenon of MLPs is
generally considered a counter-intuitive phenomenon. In this paper, our distinctive contribution is to
explain the counter-intuitive two-phase phenomenon of MLPs theoretically.
C Double descent
There are usually two types of double-descent phenomena. The model-wise double descent behavior
has emerged in many deep learning tasks, which means that as the model size increases, performance
ﬁrst decreases, then increases, and ﬁnally decreases [ 3,23,52,11]. Furthermore, some recent studies
discussed the existence of the triple descent curve [ 12,2]. Besides, the double descent behavior also
occurs with respect to training epochs [ 36,19], called epoch-wise double descent, i.e.as the epoch
increases, the testing error ﬁrst decreases, then increases, and ﬁnally decreases. As Figure 1(d) in
23

--- PAGE 24 ---
Training Accuracy(%)Testing Accuracy(%)
C
osine similarity 
of features
Cosine similarity 
of gradientsTraining Loss
Testing Loss
iteration iteration iteration(a) (b) (c)
(d) (e) (f)
mnist 9-layer2- wd9-layer MLP with 512
neurons in each layer,
weight decay = 0.001
9-layer MLP with 512
neurons in each layer,
weight decay =  0.0001Figure 22: (a) The training loss of two MLPs with different weight decays trained on the MNIST
dataset. (b) The testing loss of two MLPs. (c) Training accuracies of two MLPs. (d) Testing
accuracies of two MLPs. (e) Cosine similarity between features of different categories. (f) Cosine
similarity between gradients of different samples in a category. The feature and the feature gradient
were used in the second linear layer of MLPs.
Training Accuracy(%)Testing Accuracy(%)
C
osine similarity 
of features
Cosine similarity 
of gradientsTraining Loss
Testing Loss
iteration iteration iteration(a) (b) (c)
(d) (e) (f)
Tiny-9-layer3- wd9-layer MLP with 512
neurons in each layer,
weight decay = 0.0001
9-layer MLP with 512
neurons in each layer,
weight decay =  0.00001
Figure 23: (a) The training loss of two MLPs with different weight decays trained on the Tiny
ImageNet dataset. (b) The testing loss of two MLPs. (c) Training accuracies of two MLPs. (d)
Testing accuracies of two MLPs. (e) Cosine similarity between features of different categories. (f)
Cosine similarity between gradients of different samples in a category. The feature and the feature
gradient were used in the third linear layer of MLPs.
the main paper shows, the ﬁrst and the second stages in the epoch-wise double descent behavior are
temporally aligned with the ﬁrst phase in the aforementioned two-phase phenomenon, where the
training loss does not change signiﬁcantly.
D More results on the MNIST dataset
In this section, we provide more results on the MNIST dataset. Fig 25 and Tables 1 empirically verify
the strength of the primary common direction, which are supplementary to Figure 4 and Table 1 in the
main paper, respectively. Fig 26 illustrates the change of o(l)= cos(V(l)
t;F(l 1)
t )cos(V(l)
t;F(l 1)
t )
in the ﬁrst phase, which is supplementary to Figure 6 in the main paper.
Table 1: Strength of components of weight changes along the primary common direction and other
directions. We trained a 9-layer MLP on the MNIST dataset. Each layer of the MLP had 512 neurons.
It can be observed that the strength of the primary common direction was much greater than those of
other directions.
MNISTCategory Eight Zero
S(10 3) Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6
S(l)
primary 367:156:8364:552:8381:956:3444:468:7504:081:3441:786:0448:283:5429:078:1493:187:2504:189:0
S(l)
1 14:90:8 15:91:4 15:51:1 15:61:5 13:52:0 24:63:1 30:04:3 18:42:6 17:22:2 15:61:8
S(l)
2 16:31:7 13:10:9 16:40:8 18:13:2 11:71:6 16:61:7 23:94:2 17:92:4 14:31:5 12:21:9
S(l)
3 15:11:5 16:31:7 13:50:6 15:11:4 15:01:1 29:45:2 21:14:2 15:51:8 21:23:6 14:71:6
24

--- PAGE 25 ---
Training Loss
iteration iteration9-layer MLPs 
Trained on 
CIFAR-10𝛾𝛾2=1.0𝛾𝛾1=0.1Training Accuracy loss minimization 
gets stuck
the first phase 
is shorten
(a) (b)0 100002.4
2.0
1.6
1.20.6
0.4
0.2
0.00 100002.4
2.0
1.60.6
0.4
0.2
0.0Testing Loss
0 10000
iteration
(c)
Testing Accuracy 
iteration
(d)0 10000Figure 24: (a) The training loss of two MLPs trained on the CIFAR-10 dataset. When the loss
minimization gets stuck (orange curve), we can consider it as the ﬁrst phase with an inﬁnite length.
Therefore, the learning-sticking problem can be solved by techniques of shortening the ﬁrst phase,
such as the technique of increasing the variance of initial weights, which is a theoretically certiﬁcated
solution in our study (blue curve). (b) The training accuracy of two MLPs. (c) The testing loss of two
MLPs. (d) The testing accuracy of two MLPs.
Eight
StrengthZero
StrengthLayer 2 Layer 3 Layer 4 Layer 5 Layer 6 Category1e-3 1e-3 1e-3 1e-3 1e-3
1e-3 1e-3 1e-3 1e-3
direction2.3
0.02.6
0.02.7
0.01e-33.2
0.03.4
0.03.6
0.03.1
0.02.5
0.02.3
0.02.0
0.0
1 2 3 4 5direction1 2 3 4 5direction1 2 3 4 5direction1 2 3 4 5
direction1 2 3 4 5
Figure 25: The strength of top-ranked common directions on the MNIST dataset. We trained a 9-layer
MLP, where each layer of the MLP had 512 neurons. We computed the strength of common directions
on the two categories with the highest training accuracies. si=kCiV>
ikFmeasures the strength of
weight changes along the i-th common direction, where Vi=Et[Vi;t]. It can be observed that
the strength of the primary direction was much greater than the strength of other directions.
Eight Zero
Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Category
iteration iteration iteration iteration iteration
Figure 26: The change of o(l)= cos(V(l)
t;F(l 1)
t )cos(V(l)
t;F(l 1)
t )in the ﬁrst phase. We trained a
9-layer MLP on the MNIST dataset. Each layer of the MLP had 512 neurons. The shade represents
the standard deviation over different samples.
25

--- PAGE 26 ---
E Proof for the lemma 1
In this section, we present the detailed proof for Lemma 1.
Lemma 1. For the decomposition W>
t=VtC>+ "t, given weight changes over different
samples W>
t, we can compute the common direction Cand obtain Vt=W>
tC
C>Cand"t=
W>
t W>
tCC>
C>Cs.t."tC=0. Such settings minimize k"tkF.
proof . Let ">
t[j]denote thej-th column of the matrix ">
t2Rhd. Given a sample x, we can
represent ">
t[j]by the vector Cand a residual term ">
t[j]0as follows:
">
t[j] =C+ ">
t[j]0; (1)
whereC>">
t[j]0= 0, andis a scalar.
Then,
">
t[j]2
2=C+ ">
t[j]02
2
= (C+ ">
t[j]0)>(C+ ">
t[j]0)
=2C>C+ (">
t[j]0)>">
t[j]0
=2C>C+">
t[j]02
2(2)
Obviously,">
t[j]2
2is the smallest when = 0. In other words, ">
t[j]does not contain the
component along the direction CandC>">
t[j] = 0 . Therefore,">
t[j]2
2reaches its minimum
if and only if "tC=0.
When">
t[j]2
2reaches its minimum, k"tk2
Fbecomes the smallest. Thus, we have:
Wt=CV>
t+ ">
t
C>Wt=C>CV>
t+CT">
t
=C>CV>
t+0(3)
Then, V>
tcan be represented as follows.
V>
t=C>Wt
C>C(4)
Substituting Eq. 4 into Wt=CV>
t+ ">
t, we have
"t= W>
t W>
tCC>
C>C(5)
F Proof for the lemma 2
In this section, we present the detailed proof for Lemma 2.
Lemma 2. (We can also decompose the weight W(l)
tinto the component along the common
directionCand the component "tin other directions .) Given the weight W>
tand the common
directionC, the decomposition W>
t=VtC>+"tcan be conducted as Vt=W>
tC
C>Cand"t=
W>
t W>
tCC>
C>Cs.t."tC=0. Such settings minimize k"tkF.
proof . Let">
t[j]denote thej-th column of the matrix ">
t2Rhd. We can represent ">
t[j]by the
vectorCand a residual term ">
t[j]0as follows:
">
t[j] =C+">
t[j]0; (6)
26

--- PAGE 27 ---
whereC>">
t[j]0= 0andis a scalar.
Then,
">
t[j]2
2=C+">
t(x)[j]02
2
= (C+">
t[j]0)>(C+">
t[j]0)
=2C>C+ (">
t[j]0)>">
t[j]0
=2C>C+">
t[j]02
2(7)
Obviously,">
t[j]2
2becomes the smallest when = 0. In other words, ">
t[j]does not contain the
component along the direction CandC>">
t[j] = 0 . Therefore,">
t[j]2
2reaches its minimum if
and only if"tC=0.
When">
t[j]2
2reaches its minimum, k"tk2
Fbecomes the smallest. Thus, we have:
Wt=CV>
t+">
t
C>Wt=C>CV>
t+C>">
t
=C>CV>
t+0(8)
Then,V>
tcan be written as follows.
V>
t=C>Wt
C>C(9)
Substituting Eq. 9 into Wt=CV>
t+">
t, we have
"t= W>
t W>
tCC>
C>C(10)
G Decomposition of common directions
Actually, the estimation of the common direction Cis similar to the singular value decomposition
(SVD), although there are slight differences.
We compute the average weight change Wt=Ex2XWtjx, where Wtjxdenotes the weight
change made by the sample x. Then, we decompose Wtinto components along ﬁve common
directions as Wt=C1V>
1;t+C2V>
2;t++C5V>
5;t+">
5;t, whereC1=Cis termed the primary
common direction .C1,C2,C3,C4, andC5are orthogonal to each other. C2;C3;C4andC5represent
the second, third, forth, and ﬁfth common directions, respectively. Cirepresents the i-th common
direction. Vi;tdenotes the average weight change along the i-th common direction decomposed
from Wt.
Speciﬁcally, we ﬁrst decompose the average weight change Wtafter thet-th iteration as Wt=
CV>
t+ ">
t. We remove all components along the common direction Cfrom Wt, and obtain
Wnew;t= Wt CV>
t= ">
t. Then, we further decompose Wnew;t=C2V>
2;t+ ">
2;t. In
this way, we can consider C2as the secondary common direction, while C1=Cis termed as the
primary common direction. Thus, we conduct this process recursively and obtain common directions
fC1;C2;C5g. Accordingly, Wtis decomposed into Wt=C1V>
1;t+C2V>
2;t++C5V>
5;t+
">
5;t.
H Decomposition of the weight change made by a sample x
H.1 Proof for Theorem 1.
In this subsection, we present the detailed proof for Theorem 1.
27

--- PAGE 28 ---
Theorem 1. The weight change made by a sample can be decomposed into (h+ 1) terms after the
t-th iteration as follows.
W(l)
t= W(l)
primary ;t+Xh
k=1W(l;k)
noise;trewritten= = = =  (l)
tF(l 1)>
t +(l)>
t; (11)
where W(l)
primary ;t=D(l)
tV(l+1)
tC(l+1)>C(l+1)V(l+1)>
tF(l)
tF(l 1)>
t=kF(l)
tk2
2denotes the component
along the primary common direction, and W(l;k)
noise;t=D(l)
t"(l+1;k)
t "(l+1)>
tF(l)
tF(l 1)>
t=kF(l)
tk2
2de-
notes the component along the k-th common direction in the noise term. "(l+1;k)
t =kkUkV>
k,
where the SVD of "(l+1)
t2Rhh0is given as "(l+1)
t =UV>(hh0), and kkdenotes
thek-th singular value 2R."(l+1)
t =P
k"(l+1;k)
t .UkandVkdenote the k-th column of
the matrixUandV, respectively. Besides, we have 8k2 f1;2;:::;hg;U>
kC(l+1)= 0.Con-
sequently, we have  (l)
t=D(l)
tV(l+1)
tC(l+1)>C(l+1)V(l+1)>
tF(l)
t=kF(l)
tk2
22Rh, and(l)>
t=
D(l)
t"(l+1)
t"(l+1)>
tF(l)
tF(l 1)>
t=kF(l)
tk2
22Rhd.
proof. We can represent weight matrix as W(l)
t=C(l)V(l)
t>+"(l)>
t. In addition, according to the
back propagation and chain rule, we have W(l)
t= D(l)
t_F(l)
tF(l 1)>
t , where _F(l)
t=@Loss
@F(l)
t, and
denotes the learning rate.
According to Lemma 1 and Lemma 2, we have "(l+1)
tC(l+1)=0and"(l+1)
tC(l+1)=0. After the
t-th iteration, the weight change made by a training sample xcan be computed as follows.
W(l)
t= D(l)
t_F(l)
tF(l 1)>
t
= D(l)
tW(l+1)>
tD(l+1)
t_F(l+1)
tF(l 1)>
t
=D(l)
tW(l+1)>
t W(l+1)
tF(l)
tF(l 1)>
t=F(l)
t2
2
=D(l)
th
V(l+1)
tC(l+1)>+"(l+1)
tih
C(l+1)V(l+1)>
t + "(l+1)>
ti
F(l)
tF(l 1)>
t=F(l)
t2
2
=D(l)
t[V(l+1)
tC(l+1)>C(l+1)V(l+1)>
t +V(l+1)
tC(l+1)>"(l+1)>
t
+"(l+1)
tC(l+1)V(l+1)>
t +"(l+1)
t"(l+1)>
t ]F(l)
tF(l 1)>
t=F(l)
t2
2
=D(l)
th
V(l+1)
tC(l+1)>C(l+1)V(l+1)>
t +"(l+1)
t"(l+1)>
ti
F(l)
tF(l 1)>
t=F(l)
t2
2
=D(l)
tV(l+1)
tC(l+1)>C(l+1)V(l+1)>
tF(l)
tF(l 1)>
t=F(l)
t2
2
+D(l)
t"(l+1)
t"(l+1)>
tF(l)
tF(l 1)>
t=F(l)
t2
2(12)
"(l+1;k)
t = kkUkV>
k, where the singular value decomposition of "(l+1)
t is given as "(l+1)
t =UV>,
andkkdenotes thek-th singular value. UkandVkdenote thek-th column of the matrix UandV,
respectively. We can derive the following equations.
W(l)
t=D(l)
tV(l+1)
tC(l+1)TC(l+1)V(l+1)>
tF(l)
tF(l 1)>
t=F(l)
t2
2
+D(l)
t"(l+1)
t"(l+1)>
tF(l)
tF(l 1)>
t=F(l)
t2
2
=D(l)
tV(l+1)
tC(l+1)TC(l+1)V(l+1)>
tF(l)
tF(l 1)>
t=F(l)
t2
2
+hX
k=1D(l)
t"(l+1;k)
t "(l+1)>
tF(l)
tF(l 1)>
t=F(l)
t2
2:
= W(l)
primary ;t+hX
k=1W(l;k)
t;noise(13)
28

--- PAGE 29 ---
In addition, if we set  (l)
t=D(l)
tV(l+1)
tC(l+1)>C(l+1)V(l+1)>
tF(l)
t=kF(l)
tk2
2, and(l)>
t =
D(l)
t"(l+1)
t"(l+1)>
tF(l)
tF(l 1)>
t=kF(l)
tk2
2. Then we can re-write the Eq. (13) as follows.
W(l)
trewritten= = = =  (l)
tF(l 1)>
t +(l)>
t (14)
H.2 The explanation for the phenomenon that S(l)
1,S(l)
2, andS(l)
3does not decrease
monotonically.
In this subsection, we explain the phenomenon that S(l)
1,S(l)
2, andS(l)
3does not decrease mono-
tonically in Table 1 in the supplementary material and Table 1 in the main paper (Page 6). In
fact, we ﬁrst decompose "(l+1)
t =P
k"(l+1;k)
t according to the SVD. Then W(l;k)
noise;tis computed
asW(l;k)
noise;t=D(l)
t"(l+1;k)
t "(l+1)>
tF(l)
tF(l 1)>
t=F(l)
t2
2:Accordingly, the strength of weight
changes along the primary direction is computed as S(l)
primary =Et2[Tstart;Tend]Ex2Xh
kW(l;k)
primary ;tjxkFi
.
The strength of weight changes along the k-th noise direction is computed as S(l)
k=
Et2[Tstart;Tend]Ex2Xh
kW(l;k)
noise;tjxkFi
. In this way, S(l)
1,S(l)
2, andS(l)
3do not decrease mono-
tonically, although k"(l+1;1)
tkF,k"(l+1;2)
tkF, andk"(l+1;3)
tkFare directly decomposed from "(l+1)
t
based on the SVD and decrease monotonically.
I Analysis based on Eq. (3) in the main paper and explanation for the
parallelism.
According the Eq. (3) in the main paper, we have
_F(l 1)
t = (C(l)>D(l)
t_F(l)
t)+D(l)
t_F(l)
t (15)
Thus, ifC(l)>D(l)
t_F(l)
tis large enough ( i.e., keeping optimizing W(l)>
t along the common direction
C(l)for a long time), then the feature gradients _F(l 1)
t of different samples will be roughly parallel
to the same vector . This is because C(l)>D(l)
t_F(l)
tis a scalar and the term D(l)
t_F(l)
tis small. In
other words, the diversity between feature gradients _F(l 1)
t of different samples decreases. Here,
= [1;2;;d], and = [1;2;;d]>.
J Discussion on the background assumption.
In the above section, we demonstrate that on the ideal state, i.e.,W(l)>
t has been optimized towards
the common direction C(l)for a long time, we can consider that the feature gradients _F(l 1)
t of
different samples will be roughly parallel to the same vector . In this way, we can explain that the
diversity between feature gradients _F(l 1)
t of different samples decreases.
In comparison, in the current section, we mainly discuss the trustworthiness of the background
assumption in Line 214 in the main paper. We aim to discuss that on the assumption that features
F(l 1)
t of different samples have been pushed a little bit towards a speciﬁc common direction, we
can ﬁnd at least one learning iteration in the ﬁrst phase where F(l 1)
t andF(l 1)
t of most samples
have similar directions, and V(l)
tandV(l)
thave similar directions. The assumption that features
F(l 1)
t of different samples have been pushed a little bit towards a speciﬁc common direction is an
intermediate state between the chaotic initial state of the MLP and the ideal state introduced in the
above section. In this way, we can assume that C(l)>D(l)
t_F(l)
tis large.
According to Eq. (2) in the main paper and Lemma 2, we have _F(l 1)
t =W(l)>
tD(l)
t_F(l)
tand
W(l)>
t=V(l)
tC(l)>+"(l)>
t. Thus, we have
_F(l 1)
t =W(l)>
tD(l)
t_F(l)
t
= (V(l)
tC(l)>+"(l)>
t)D(l)
t_F(l)
t
=V(l)
tC(l)>D(l)
t_F(l)
t+"(l)>
tD(l)
t_F(l)
t(16)
29

--- PAGE 30 ---
If the scalar C(l)>D(l)
t_F(l)
tis large, we can roughly consider
_F(l 1)
tV(l)
tC(l)>D(l)
t_F(l)
t
=V(l)
t(C(l)>D(l)
t_F(l)
t)==V(l)
t(17)
It means that the feature gradient _F(l 1)
t is roughly parallel to the vector V(l)
t. Furthermore, the
feature gradient _F(l 1)
t and the change of feature F(l 1)
t can be considered negatively parallel to
each other, we have
F(l 1)
t==_F(l 1)
t==V(l)
t (18)
Similarly, we have F(l 1)
t+1==V(l)
t+1. Therefore, we can roughly consider that V(l)
tktF(l 1)
t ,
andV(l)
t+1kt+1F(l 1)
t+1, wherekt;kt+12Rare two scalars. Then, we can derive that
V(l)
t=V(l)
t+1 V(l)
tkt+1F(l 1)
t+1 ktF(l 1)
t (19)
If featuresF(l 1)
t of different samples have been pushed a little bit towards a speciﬁc common
direction, then it is easy to ﬁnd at least one learning iteration that F(l 1)
t andF(l 1)
t of most
samples have similar directions, i.e.F(l 1)
t==F(l 1)
t . Meanwhile, we can ﬁnd at least one learning
iteration in the ﬁrst phase where the change of feature in t-th iteration F(l 1)
t and(t+1)-th iteration
F(l 1)
t+1 are roughly the same. In other words, F(l 1)
tF(l 1)
t+1. Thus, we have
V(l)
t(kt+1 kt)F(l 1)
t==F(l 1)
t==V(l)
t (20)
In this way, we can obtain that V(l)
tandV(l)
thave similar directions.
30

--- PAGE 31 ---
K Proof for Lemma 3
In this section, we present the detailed proof for Lemma 3.
Lemma 3. Given an input sample x2Xand a common direction C(l)after thet-th iteration, if
the noise term "(l)
tis small enough to satisfy jV(l)>
tF(l 1)
tV(l)>
tV(l)
tC(l)>C(l)V(l)>
tF(l 1)
tj 
jV(l)>
tF(l 1)
tV(l)>
t"(l)
t"(l)>
tF(l 1)
tj, we can obtain cos(V(l)
t;F(l 1)
t )cos(V(l)
t;F(l 1)
t )0,
where V(l)
t=W(l)>
tC(l)
C(l)>C(l), andV(l)
t=W(l)>
tC(l)
C(l)>C(l).F(l 1)
t denotes the change of features
F(l 1)
t =F(l 1)
t+1 F(l 1)
t made by the training sample xafter thet-th iteration. To this end,
we approximately consider the change of features F(l 1)
t after thet-th iteration negatively parallel
to feature gradients _F(l 1)
t , although strictly speaking, the change of features is not exactly equal to
the gradient w.r.t. features.
proof. Given a sample x, we can prove that cos(V(l)
t;F(l 1)
t )cos(V(l)
t;F(l 1)
t )0.
According to chain rule, we have
W(l)
t= D(l)
t_F(l)
tF(l 1)T
t (21)
According to Lemma 1 and Lemma 2, we have C(l)>"(l)>
t= 0and"(l)
tC(l)= 0. Then, we have
cos(V(l)
t;F(l 1)
t )cos(V(l)
t;_F(l 1)
t ) ="
V(l)>
tF(l 1)
t
kV(l)
tkkF(l 1)
tk#
"
V(l)>
t_F(l 1)
t
kV(l)
tkk_F(l 1)
tk#
(22)
Therefore, we have
sign(cos(V(l)
t;F(l 1)
t )cos(V(l)
t;_F(l 1)
t ))
=sign([V(l)>
tF(l 1)
t ][V(l)>
t_F(l 1)
t ]=(kV(l)
tk2kF(l 1)
tk2kV(l)
tk2k_F(l 1)
tk2))
=sign([V(l)>
tF(l 1)
t ][V(l)>
tW(l)>
tD(l)
t_F(l)
t]=(kV(l)
tk2kF(l 1)
tk2kV(l)
tk2k_F(l 1)
tk2))
=sign([V(l)>
tF(l 1)
t ][V(l)>
t(V(l)
tC(l)>+"(l)
t)D(l)
t_F(l)
t]=(kV(l)
tk2kF(l 1)
tk2kV(l)
tk2k_F(l 1)
tk2))
=sign([V(l)>
tF(l 1)
t ][V(l)>
t(V(l)
tC(l)>+"(l)
t)(W(l)
tF(l 1)
t=( F(l 1)
t2
2))]
=(kV(l)
tk2kF(l 1)
tk2kV(l)
tk2k_F(l 1)
tk2))
=sign([V(l)>
tF(l 1)
t ][(V(l)>
tV(l)
tC(l)>+V(l)>
t"(l)
t)W(l)
tF(l 1)
t ]
=( F(l 1)
t2
2kV(l)
tk2kF(l 1)
tk2kV(l)
tk2k_F(l 1)
tk2))
=sign([V(l)>
tF(l 1)
t ][(V(l)>
tV(l)
tC(l)>+V(l)>
t"(l)
t)(C(l)V(l)>
t+ "(l)>
t)F(l 1)
t ]
=( F(l 1)
t2
2kV(l)
tk2kF(l 1)
tk2kV(l)
tk2k_F(l 1)
tk2))
=sign([V(l)>
tF(l 1)
t ][(V(l)>
tV(l)
tC(l)>C(l)V(l)>
t+V(l)>
t"(l)
t"(l)>
t
+V(l)>
tV(l)
tC(l)>"(l)>
t+V(l)>
t"(l)
tC(l)V(l)>
t)F(l 1)
t ]=( F(l 1)
t2
2kV(l)
tk2k_F(l 1)
tk2kV(l)
tk2kF(l 1)
tk2))
=sign([V(l)>
tF(l 1)
t ][(V(l)>
tV(l)
tC(l)>C(l)V(l)>
t+V(l)>
t"(l)
t"(l)>
t)F(l 1)
t ]
=( F(l 1)
t2
2kV(l)
tk2kF(l 1)
tk2kV(l)
tk2k_F(l 1)
tk2))
=sign([V(l)>
tF(l 1)
t ][V(l)>
tV(l)
tC(l)>C(l)V(l)>
tF(l 1)
t +V(l)>
t"(l)
t"(l)>
tF(l 1)
t ]
=( F(l 1)
t2
2kV(l)
tk2kF(l 1)
tk2kV(l)
tk2k_F(l 1)
tk2))
=sign([V(l)>
tF(l 1)
tV(l)>
tV(l)
tC(l)>C(l)V(l)>
tF(l 1)
t + V(l)>
tF(l 1)
tV(l)>
t"(l)
t"(l)>
tF(l 1)
t ]
=( F(l 1)
t2
2kV(l)
tk2kF(l 1)
tk2kV(l)
tk2k_F(l 1)
tk2))
(23)
31

--- PAGE 32 ---
iteration
Frobenius Norm
Figure 27: Visualization of the Frobenius norm of the two components
V(l)>
tF(l 1)
tV(l)>
tV(l)
tC(l)>C(l)V(l)>
tF(l 1)
t andV(l)>
tF(l 1)
tV(l)>
t"(l)
t"(l)>
tF(l 1)
t . We
trained a 9-layer MLP on the MNIST dataset, where each layer had 512 neurons. Iterations were
chosen at the end of the ﬁrst phase.
According to our assumption, the noise term "(l)
t is small enough to satisfy
jV(l)>
tF(l 1)
tV(l)>
tV(l)
tC(l)>C(l)V(l)>
tF(l 1)
tj  j V(l)>
tF(l 1)
tV(l)>
t"(l)
t"(l)>
tF(l 1)
tj.
This assumption is veriﬁed in Figure 27. Then we can ignore the last term and obtain
sign([V(l)>
tF(l 1)
tV(l)>
tV(l)
tC(l)>C(l)V(l)>
tF(l 1)
t + V(l)>
tF(l 1)
tV(l)>
t"(l)
t"(l)>
tF(l 1)
t ]
=( F(l 1)
t2
2kV(l)
tk2kF(l 1)
tk2kV(l)
tk2k_F(l 1)
tk2))
sign([V(l)>
tF(l 1)
tV(l)>
tV(l)
tC(l)>C(l)V(l)>
tF(l 1)
t ]
( F(l 1)
t2
2kV(l)
tk2kF(l 1)
tk2kV(l)
tk2k_F(l 1)
tk2))0
(24)
Thus,
sign(cos(V(l)
t;F(l 1)
t )cos(V(l)
t;_F(l 1)
t ))0 (25)
In this paper, we approximately consider F(l 1)
t and _F(l 1)
t are negatively parallel to each
other. Thus, we have sign(cos(V(l)
t;F(l 1)
t )cos(V(l)
t;F(l 1)
t )) = sign(cos(V(l)
t;F(l 1)
t )
( cos(V(l)
t;_F(l 1)
t )))0.
L Proof for Theorem 2
In this section, we aim to prove that training samples of the same category have the same effect in the
ﬁrst phase.
Theorem 2. Under the background assumption, for any training samples x;x02Xcin the category
c, if[C(l)>D(l)
tjx_F(l)
tjx][C(l)>D(l)
tjx0_F(l)
tjx0]>0(means that F(l)
tjxandF(l)
tjx0have kinds of
similarity in very early iterations), then cos(cV(l)
tjx;F(l 1)
tjx)0, and cos(cV(l)
t;F(l 1)
tjx)
0, wherec2f 1;+1gis a constant shared by all samples in category c.
proof. Given a sample xand a sample x0from the same category, we can prove that
cos(V(l)
tjx;F(l 1)
tjx)cos(V(l)
tjx0;F(l 1)
tjx0)0.
sign(cos(V(l)
tjx;F(l 1)
tjx)cos(V(l)
tjx0;F(l 1)
tjx0))
=sign([V(l)>
tjxF(l 1)
tjx][V(l)>
tjx0F(l 1)
tjx0])
=sign([C(l)>W(l)
tjx
C(l)>C(l)F(l 1)
tjx][C(l)>W(l)
tjx0
C(l)>C(l)F(l 1)
tjx0])
=sign([C(l)>W(l)
tjxF(l 1)
tjx][C(l)>W(l)
tjx0F(l 1)
tjx0])
=sign([C(l)>( D(l)
tjx_F(l)
tjxF(l 1)>
tjx)F(l 1)
tjx][C(l)>( D(l)
tjx0_F(l)
tjx0F(l 1)>
tjx0)F(l 1)
tjx0])
=sign([C(l)>D(l)
tjx_F(l)
tjx][C(l)>D(l)
tjx0_F(l)
tjx0])
(26)
32

--- PAGE 33 ---
Cosine Similarity
of featuresCIFAR -10 MNIST Tiny ImageNet
iteration iteration iteration
Figure 28: Cosine similarity of features between samples in different categories. We trained 7-layer
MLPs and 9-layer MLPs on the CIFAR-10, the MNIST, and the Tiny ImageNet dataset.
According to the assumption that F(l)
tjxandF(l)
tjx0have kinds of similarity, we can consider
[C(l)>D(l)
tjx_F(l)
tjx][C(l)>D(l)
tjx0_F(l)
tjx0]>0. In this way, for the category c, there exists a constant
c, which satisﬁes sign(cos(cV(l)
tjx;F(l 1)
tjx)0, wherec2f  1;+1gand training sample
x2Xcbelongs to the category c.
According to Lemma 3, we have cos(V(l)
tjx;F(l 1)
tjx)cos(V(l)
t;F(l 1)
tjx)0. Thus, we have
sign(cos(cV(l)
tjx;F(l 1)
tjx)cos(cV(l)
t;F(l 1)
tjx))0. In addition, the above proof indicates
that sign (cos(cV(l)
tjx;F(l 1)
tjx)0. Therefore, we have sign (cos(cV(l)
tjx;F(l 1)
tjx)0
M Discussion for four typical operations
M.1 Normalization
The output feature of the l-th linear layer w.r.t. the input sample xcan be described as
[f1;f2;:::;f h] =W(l)
tF(l 1)
t2Rh, wherefidenotes the i-th dimension of the feature. In this
way, the batch normalization operation can be formulated as BN(fi) =scale[(fi i)=i] +shift ,
wherescale andshift denote the scaling and the shifting parameters, respectively. In this way, the
batch normalization operation subtracts the mean feature F(l)
t=Ex2X[F(l)
tjx]from features of all
samples. Therefore, features of different samples in a same category are no longer similar to each
other.
We also propose a simpliﬁed normalization operation to alleviate the decrease of feature diversity
in the ﬁrst phase. The simpliﬁed normalization operation is given as norm 1(fi) = (fi i)=i,
whereiandidenote the mean value and the standard deviation of fiover different samples,
respectively. This operation is similar to the batch normalization [ 21], but we do not compute the
scaling and shifting parameters in the batch normalization.
In order to verify the simpliﬁed normalization operation can alleviate the decrease of feature diversity
during the training process of the MLP, we trained 7-layer MLPs and 9-layer MLPs with and without
the normalization operation. Speciﬁcally, for the normalization operation norm 1, we added the
normalization operation after each linear layer, except the last linear layer. Each linear layer in the
MLP had 512 neurons. Figure 28 shows that the feature similarity in MLPs with normalization
operations kept decreasing, while the feature similarity of the MLP without normalization operations
kept increasing. This indicated that normalization operations alleviate the decreasing of feature
diversity.
M.2 Momentum
We can explain that momentum in gradient descent can alleviate this phenomenon. Based on Lemma
3, the self-enhanced system of the decreasing of feature diversity requires singular values of weights
along other directions "(l)
tto be small enough. However, because the momentum operation strengthens
inﬂuences of the initialized noisy weights W(l)
t=0, it strengthens singular values of "(l)
t, to some extent,
thereby alleviating the decrease of feature diversity.
33

--- PAGE 34 ---
Cosine Similarity
of features
iteration iteration(a) (b)
iteration iteration iteration iteration0.6
0.4
0.20.6
0.4
0.20.8
0.4
0.0
0 1000 2000 0 1000 2000 0 2500 50000.6
0.40.6
0.40.8
0.50.6
0 1000 2000 0 1000 2000 0 2500 5000
L=9, m=0 L=9, m=0.5 L=9, m=0.9
L=7, m=0 L=7, m=0.5 L=7, m=0.9L=9, 𝜆𝜆=1e-3 L=9, 𝜆𝜆=1e-4
L=7, 𝜆𝜆=1e-3L=7, 𝜆𝜆=1e-4
L=9, 𝜆𝜆=1e-5 L=7, 𝜆𝜆=1e-5CIFAR-10 MNIST Tiny ImageNet CIFAR-10 MNIST Tiny ImageNetFigure 29: Effects of (a) momentum and (b) L2regularization. We trained L-layer MLPs, where
each layer had 512 neurons. A shorter ﬁrst phase indicates that the decrease of feature diversity is
more alleviated.
Speciﬁcally, consider the momentum with the coefﬁcient m, the dynamics of weights Wt+1can be
described as,
Wt+1=Wt @Loss
@Wt m@Loss
@Wt 1; (27)
wheredenotes the learning rate. Because we only focus on weights in a single layer, without
causing ambiguity, we omit the superscript (l)to simplify the notation in this subsection. In this way,
we can write the gradient descent as
WT+1=W0+TX
t1 mT+1 t
1 m@Loss
@Wt: (28)
Since 0<m< 1, the coefﬁcient1 mT+1 t
1 mdecreases when the variable tincreases. Thus, a large m
represents that inﬂuences of W0onWT+1are signiﬁcant. Because "T+1is decomposed from WT+1
and singular values of "T+1are mainly determined by the noisy W0. Accordingly, singular values
of"T+1are relatively large, which disturb the self-enhanced system and alleviate the decrease of
feature diversity. Figure 29(a) veriﬁes that a larger value of musually more alleviates the decrease
of feature diversity.
M.3 Initialization
We explain that the initialization of MLPs also affects the decrease of feature diversity. According
to Lemma 3, such self-enhanced system requires singular values of weights along other directions
"(l)
tto be small enough. However, because increasing the variance of the initialized weights W(l)
0
will increase singular values of "(l)
tbased on Lemma 2, alleviating the decrease of feature diversity.
Speciﬁcally, we initialize weights with Xavier normal distribution [ 17],i.e.W0N (0;2
var),
wherevar=q
2
fanout+fanin.faninandfanoutdenote the input dimension and the output
dimension of the linear layer, respectively. In this way, a large yields large singular values of initial
weightsW0. Based on Lemma 2, we also have "(l)
0=W(l)>
0 W(l)>
0C(l)C(l)>
C(l)>C(l). Large singular
values of initial weights W0lead to large singular values of "(l)
0. Therefore, a large variance of
initialized weights disturbs the self-enhanced system and alleviates the decrease of feature diversity.
M.4L2regularization
L2regularization is equivalent to the weight decay in the case of gradient descent. The loss function
withL2regularization and cross entropy loss can be formulated as Lt(Wt) =LCE
t(Wt) +kWtk2
2,
whereWtdenotes weights of MLPs. In this way, we have the following iterates by using gradient
descent
Wt+1=Wt rLt(Wt)
=Wt rLCE
t(Wt) 2W t
= (1 2)Wt rLCE
t(Wt); (29)
According to Lemma 3, such self-enhanced system requires singular values of weights along other
directions"(l)
tto be small enough. Based on Lemma 2, we also have "(l)
t=W(l)>
t W(l)>
tC(l)C(l)>
C(l)>C(l).
In this way, a smaller yields larger singular values of "(l)
t, which disturbs the self-enhanced system
34

--- PAGE 35 ---
and alleviates the decrease of feature diversity. Figure 29(b) Figure 9(d) veriﬁes that a smaller
coefﬁcientlambada more alleviated the decrease of feature diversity.
35
