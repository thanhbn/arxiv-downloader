# 2306.01470.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/ffn-mlp/2306.01470.pdf
# Kích thước tệp: 4429904 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Hiểu MLP-Mixer như một MLP Rộng và Thưa
Tomohiro Hayase1Ryo Karakida2
Tóm tắt
Perceptron nhiều lớp (MLP) là một thành phần cơ bản của học sâu, và các kiến trúc dựa trên MLP gần đây, đặc biệt là MLP-Mixer, đã đạt được thành công thực nghiệm đáng kể. Tuy nhiên, sự hiểu biết của chúng ta về lý do tại sao và cách thức MLP-Mixer vượt trội hơn các MLP thông thường vẫn còn chưa được khám phá nhiều. Trong công trình này, chúng tôi tiết lộ rằng độ thưa là cơ chế chính làm nền tảng cho MLP-Mixer. Đầu tiên, các Mixer có một biểu thức hiệu quả như một MLP rộng hơn với trọng số tích Kronecker, làm rõ rằng các Mixer một cách hiệu quả thể hiện một số thuộc tính thưa được khám phá trong học sâu. Trong trường hợp các lớp tuyến tính, biểu thức hiệu quả làm sáng tỏ một regularization thưa ngầm được gây ra bởi kiến trúc mô hình và một mối quan hệ ẩn với ma trận Monarch, cũng được biết đến như một dạng tham số hóa thưa khác. Tiếp theo, đối với các trường hợp tổng quát, chúng tôi chứng minh thực nghiệm sự tương tự định lượng giữa Mixer và các MLP với trọng số thưa không có cấu trúc. Theo nguyên tắc hướng dẫn được đề xuất bởi Golubeva, Neyshabur và Gur-Ari (2021), đó là cố định số lượng kết nối và tăng độ rộng và độ thưa, các Mixer có thể thể hiện hiệu suất cải thiện.

1. Giới thiệu
Perceptron nhiều lớp (MLP) và các biến thể của nó là những thành phần cơ bản của học sâu được sử dụng trong nhiều bài toán khác nhau và để hiểu các tính chất cơ bản của mạng nơ-ron. Mặc dù tính đơn giản và lịch sử lâu dài (Rosenblatt, 1958; Schmidhuber, 2015), chỉ gần đây mới trở nên rõ ràng rằng vẫn còn một lượng đáng kể cơ hội cải thiện hiệu suất dự đoán của các kiến trúc dựa trên MLP. Độ thưa được biết đến như một hướng chính để nâng cao hiệu suất của các lớp MLP dày đặc (Neyshabur et al., 2014; d'Ascoli et al., 2019; Neyshabur, 2020; Golubeva et al., 2021; Pellegrini & Biroli, 2022). Ví dụ, Golubeva et al. (2021) báo cáo rằng hiệu suất dự đoán được cải thiện bằng cách tăng cả độ rộng và độ thưa của kết nối khi số lượng tham số có thể huấn luyện được cố định. MLP-Mixer là một hướng đáng chú ý khác trong các phát triển gần đây về kiến trúc dựa trên MLP (Tolstikhin et al., 2021; Touvron et al., 2022). Nó không dựa vào convolution hay self-attention và hoàn toàn được tạo thành từ các lớp MLP; thay vào đó, nó sử dụng các MLP được áp dụng trên các vị trí không gian hoặc các kênh đặc trưng. Điều này có thể được coi như một MLP áp dụng các lớp kết nối đầy đủ (FC) trên cả hai phía của ma trận đặc trưng. Mặc dù đơn giản, MLP-Mixer đã đạt được hiệu suất trên các benchmark phân loại ảnh có thể so sánh với các mạng nơ-ron sâu có cấu trúc hơn.

Tuy nhiên, có ít nghiên cứu về các nỗ lực thực nghiệm và lý thuyết để hiểu các cơ chế nội bộ của MLP-Mixer (Yu et al., 2022; Sahiner et al., 2022). Điều này trái ngược với việc làm sáng tỏ rộng rãi các bias rõ ràng hoặc ngầm trong các thành phần kiến trúc hiện đại khác (Neyshabur et al., 2014; Bjorck et al., 2018; Cordonnier et al., 2019). Để tiếp tục phát triển kiến trúc dựa trên MLP, việc tiết lộ cơ chế cơ bản của MLP-Mixer và giải quyết các câu hỏi như: MLP-Mixer có những bias quy nạp khác biệt gì so với MLP naïve? Những yếu tố kiến trúc nào đóng góp đáng kể vào hiệu suất vượt trội của chúng? sẽ là rất quan trọng.

Trong nghiên cứu này, chúng tôi tiết lộ rằng độ thưa, vốn có vẻ như là một khái niệm nghiên cứu khác biệt, là cơ chế chính làm nền tảng cho MLP-Mixer. Có thể thấy rằng MLP-Mixer là một MLP rộng hơn với độ thưa được nhúng vốn có như một bias quy nạp. Sự tương đương này cũng cung cấp một hiểu biết định lượng rằng kích thước token và kênh thích hợp tối đa hóa độ thưa có thể cải thiện hiệu suất dự đoán. Các đóng góp chi tiết được tóm tắt như sau:

• Chúng tôi đầu tiên xác định một biểu thức hiệu quả của MLP-Mixer như một MLP bằng cách vector hóa các lớp mixing (trong Mục 3). Nó được tạo thành từ ma trận hoán vị và tích Kronecker và cung cấp một diễn giải của các lớp mixing như một MLP cực kỳ rộng với trọng số thưa (có cấu trúc).

--- TRANG 2 ---
Hiểu MLP-Mixer như một MLP Rộng và Thưa

• Chúng tôi xem xét một trường hợp kích hoạt tuyến tính của một Mixer đơn giản và biểu thức hiệu quả của nó. Đầu tiên, điều này tiết lộ rằng trọng số tích Kronecker có một bias hướng đến regularization L1 ngầm (trong Mục 3.2). Trong học sâu, có hai khía cạnh của độ thưa: một là một số lượng lớn các tham số bằng không (ví dụ: Neyshabur (2020)) và khía cạnh khác là số lượng hạn chế các tham số độc lập (ví dụ: Dao et al. (2022)). Đánh giá của chúng tôi về regularization ngầm một cách hiệu quả liên kết hai khía cạnh này của độ thưa. Thứ hai, liên quan đến số lượng hạn chế các tham số độc lập, MLP-Mixer có thể được coi như một xấp xỉ của một MLP với ma trận Monarch (trong Mục 3.3). Do đó, độ thưa được thảo luận trong các tài liệu khác nhau được kết hợp vốn có và nguyên bản vào các Mixer.

• Đối với các trường hợp thực tế với các hàm kích hoạt phi tuyến, chúng tôi đánh giá định lượng sự tương tự cao giữa MLP với trọng số thưa truyền thống (SW-MLP) và MLP-Mixer. Đầu tiên, chúng tôi xác nhận sự tương tự của các đặc trưng ẩn bằng centered kernel alignment. Thứ hai, chúng tôi tiết lộ các xu hướng hiệu suất tương tự khi tăng độ thưa (tương đương với việc mở rộng) với số lượng kết nối cố định. Điều này có nghĩa là các quan sát thực nghiệm của Golubeva et al. (2021) về kích thước thích hợp của độ thưa vẫn giữ nguyên cả trong MLP-Mixer theo nghĩa của cả việc cải thiện hiệu suất dự đoán (trong Mục 4.2) và đảm bảo khả năng huấn luyện (trong Mục 4.3). Chúng tôi cũng xác minh thực nghiệm rằng Random-Permuted (RP) Mixer được giới thiệu trong Mục 5.2, là một biến thể ít có cấu trúc hơn của Mixer bình thường, thể hiện các xu hướng hiệu suất tương tự (Mục 5.3 & 5.4). Điều này củng cố thêm hiểu biết của chúng tôi rằng độ thưa là một yếu tố cơ bản làm nền tảng cho các Mixer.

2. Kiến thức cơ bản
2.1. Công trình liên quan
Kiến trúc dựa trên MLP. Tính chất nổi bật của MLP-Mixer là nó hoàn toàn được tạo thành từ các lớp FC. Tính chất này là duy nhất đối với MLP-Mixer (và công trình đồng thời ResMLP (Touvron et al., 2022)) và khác biệt với các kiến trúc dựa trên attention (Dosovitskiy et al., 2021). Trong khi một số công trình trước đây tập trung vào việc cung cấp đánh giá tương đối về hiệu suất so với module attention (Yu et al., 2022; Sahiner et al., 2022), mục đích của chúng tôi là làm sáng tỏ bias ẩn của MLP-Mixer như một MLP rộng và thưa. Golubeva et al. (2021) đã điều tra rằng hiệu suất tổng quát hóa có thể được cải thiện bằng cách tăng độ rộng trong các lớp FC. Bởi vì họ cố định số lượng trọng số, một sự tăng độ rộng gây ra độ thưa cao hơn. Họ tiết lộ rằng ngay cả đối với kết nối thưa cố định trong suốt quá trình huấn luyện, một độ rộng lớn có thể cải thiện hiệu suất tốt hơn so với lớp dày đặc.

Ma trận trọng số có cấu trúc: (i) Ma trận thưa. Độ thưa tham số được sử dụng rộng rãi để cải thiện hiệu suất và hiệu quả của mạng nơ-ron. Người ta biết rằng các MLP naïve yêu cầu thậm chí nhiều dữ liệu hơn so với các mô hình sâu có cấu trúc để cải thiện hiệu suất do bias quy nạp yếu của chúng (Bachmann et al., 2023). Độ thưa được coi là hữu ích như một bias tối thiểu cần thiết. Một cách tiếp cận để làm cho trọng số thưa là xác định các trọng số khác không một cách động, như huấn luyện từ dày đặc đến thưa (Neyshabur, 2020), pruning (Frankle & Carbin, 2019), và huấn luyện từ thưa đến thưa (Dettmers & Zettlemoyer, 2019; Evci et al., 2020). Cách khác là ràng buộc các trọng số có thể huấn luyện từ đầu quá trình huấn luyện một cách tĩnh (Dao et al., 2022; Golubeva et al., 2021; Liu et al., 2022; Gadhikar et al., 2023). Nghiên cứu hiện tại theo cách tiếp cận thứ hai; cụ thể, chúng tôi tiết lộ rằng các lớp mixing của MLP-Mixer có liên quan ngầm đến kết nối thưa cố định như vậy. (ii) Tích Kronecker. Việc ràng buộc ma trận trọng số thành tích Kronecker và tổng của nó đã được nghiên cứu trong tài liệu nén mô hình. Một số công trình thành công trong việc giảm số lượng tham số có thể huấn luyện mà không làm suy giảm hiệu suất dự đoán (Zhou et al., 2015; Zhang et al., 2021) trong khi những công trình khác áp dụng chúng để nén các tham số đã huấn luyện (Hameed et al., 2022). Ngược lại, chúng tôi tìm thấy một biểu thức tích Kronecker ẩn trong MLP-Mixer, có thể được coi như một xấp xỉ của ma trận Monarch được đề xuất trong Dao et al. (2022).

Đáng chú ý, nghiên cứu của chúng tôi hoàn toàn khác biệt với việc chỉ đơn thuần áp dụng các regularizer thưa, trọng số tích Kronecker, hoặc ma trận Monarch (Dao et al., 2022) vào các ma trận trọng số dày đặc trong các lớp mixing như Fu et al. (2023). Phát hiện của chúng tôi là các MLP-Mixer và tổng quát hóa của chúng (tức là họ PK) vốn có các tính chất này.

2.2. Ký hiệu
MLP-Mixer. Một MLP-Mixer được định nghĩa như sau (Tolstikhin et al., 2021). Ban đầu, nó chia một ảnh đầu vào thành các patch. Tiếp theo, một lớp FC cho mỗi patch được thực hiện. Sau đó, các khối được mô tả như sau được áp dụng lặp lại cho chúng: đối với ma trận đặc trưng từ lớp ẩn trước X∈RS×C,

Token-MLP (X) =W2ϕ(W1X), (1)
Channel-MLP (X) =ϕ(XW 3)W4, (2)

trong đó ϕ biểu thị hàm kích hoạt theo từng phần tử, W1∈RγS×S, W2∈RS×γS, W3∈RC×γC và W4∈RγC×C. Trong bài báo này, chúng tôi đặt hệ số mở rộng của các lớp ẩn của token và channel-mixing MLPs thành cùng một giá trị γ cho đơn giản. Khối của MLP-Mixer được cho bởi ánh xạ X7→Y, trong đó

U=X+Token-MLP (LN(X)), (3)
Y=U+Channel-MLP (LN(U)). (4)

Cuối cùng, global average pooling và bộ phân loại tuyến tính được áp dụng cho lớp ẩn cuối cùng.

S-Mixer. Để tạo điều kiện cho các cái nhìn sâu sắc lý thuyết, chúng tôi giới thiệu S-Mixer, một phiên bản lý tưởng hóa của MLP Mixer:

H=ϕ(ϕ(WX)V), (5)

trong đó V và W đại diện cho các ma trận trọng số. Trong công thức này, các mạng nơ-ron nông thường được tìm thấy trong khối mixing của MLP-Mixer được đơn giản hóa thành các lớp đơn. Để đơn giản trong phân tích lý thuyết của chúng tôi về S-Mixer, chúng tôi bỏ qua layer normalization và skip connection. Tuy nhiên, điều quan trọng cần lưu ý là trong các thí nghiệm số của chúng tôi về huấn luyện các mô hình sâu trong Mục 5, các thành phần này được kết hợp, chứng minh rằng việc đưa chúng vào không làm giảm kết quả cơ bản của nghiên cứu. Chúng tôi đã tóm tắt các kiến trúc chi tiết được sử dụng cho thí nghiệm trong Phụ lục A.

3. Tính chất như một MLP thưa
3.1. Vector hóa
Để giải quyết sự tương tự giữa MLP-Mixer và MLP, chúng tôi xem xét vector hóa các tensor đặc trưng và độ rộng hiệu quả. Chúng tôi biểu diễn phép toán vector hóa của ma trận X∈RS×C bằng vec(X); chính xác hơn, (vec(X))(j−1)d+i= Xij,(i= 1, . . . , S, j = 1, . . . , C ). Chúng tôi cũng định nghĩa một phép toán nghịch đảo mat(·) để khôi phục biểu diễn ma trận bằng mat(vec(X)) =X. Tồn tại một phương trình nổi tiếng cho phép toán vector hóa và tích Kronecker được ký hiệu bởi ⊗;

vec(WXV ) = (V⊤⊗W)vec(X), (6)

đối với W∈RS×S và V∈RC×C. Vector hóa của ma trận đặc trưng WXV tương đương với một lớp kết nối đầy đủ có độ rộng m:=SC với ma trận trọng số V⊤⊗W. Chúng tôi gọi m này là độ rộng hiệu quả của các lớp mixing.

Trong MLP-Mixer, khi chúng tôi coi mỗi ma trận đặc trưng S×C X như một vector SC chiều vec(X), phép nhân bên phải bởi một trọng số C×C V và phép nhân trọng số bên trái bởi một trọng số S×S W được biểu diễn như sau:

vec(XV) = (V⊤⊗IS)vec(X), (7)
vec(WX) = (IC⊗W)vec(X) (8)

trong đó In biểu thị ma trận đồng nhất n×n. Biểu thức này làm rõ rằng các lớp mixing hoạt động như một MLP với ma trận trọng số đặc biệt với tích Kronecker. Như thường lệ, kích thước của S và C là khoảng 102∼103, và điều này ngụ ý rằng Mixer tương đương với một MLP cực kỳ rộng với m= 104∼106. Hơn nữa, tỷ lệ các mục khác không trong ma trận trọng số IC⊗W là 1/C và của V⊤⊗IS là 1/S. Do đó, trọng số của MLP hiệu quả là rất thưa theo nghĩa của các mục khác không.

Ở đây, để chỉ xem xét phép nhân bên trái của trọng số, chúng tôi giới thiệu các ma trận hoán vị.

Định nghĩa 3.1. Ma trận hoán vị Jc là một ma trận m×m được định nghĩa là

Jcvec(X) =vec(X⊤), (9)

trong đó X là ma trận S×C.

Lưu ý rằng đối với bất kỳ x∈Rm, Jcϕ(x) =ϕ(Jcx). Ngoài ra, chúng ta có J⊤c(IS⊗V⊤)Jc=V⊤⊗IS đối với bất kỳ ma trận C×C V. Sử dụng ma trận hoán vị, chúng tôi tìm thấy điều sau:

Mệnh đề 3.2 (Biểu thức hiệu quả của MLP-Mixer như MLP). Ma trận đặc trưng của S-Mixer (5) là một MLP nông với độ rộng m=SC như sau:

vec(H) =ϕ(J⊤c(IS⊗V⊤)ϕ(Jc(IC⊗W)vec(X))). (10)

Việc suy ra rất đơn giản như được mô tả trong Phụ lục B.1. Biểu thức này làm rõ rằng các lớp mixing hoạt động như một MLP với ma trận trọng số đặc biệt với ma trận hoán vị và tích Kronecker.

Thật dễ dàng để tổng quát hóa biểu thức trên cho S-Mixer thành MLP-Mixer, trong đó mỗi phép toán mixing được tạo thành từ các mạng nơ-ron nông (2) (xem Phụ lục B.1). Sự tương đương này với một MLP rộng với trọng số thưa là đơn giản và dễ theo dõi nhưng đã bị thiếu trong tài liệu.

3.2. Regularization ngầm của các lớp mixing tuyến tính
Việc xem xét trường hợp kích hoạt tuyến tính để có được cái nhìn sâu sắc lý thuyết là một cách tiếp cận phổ biến được sử dụng trong học sâu (Saxe et al., 2014; Arora et al., 2019). Tuy nhiên chưa có công trình nào về các Mixer. Chúng tôi phát hiện rằng phân tích lý thuyết thậm chí còn khó khăn ngay cả trong trường hợp đơn giản nhất của S-Mixer tuyến tính (6). Mô hình này tương đương với một mô hình hồi quy song tuyến tính gọi là, mà nghiệm phân tích tối thiểu hóa MSE loss là không biết (Hoff, 2015). Điều này làm cho việc phân tích S-Mixer tuyến tính khó khăn hơn so với các mạng tuyến tính thông thường. Bất chấp khó khăn này, chúng tôi khám phá bất đẳng thức sau đây đặc trưng cho regularization ngầm của mô hình:

Mệnh đề 3.3.
min V,W L(V⊗W) +λ/2(∥V∥²F+∥W∥²F) ≥ min B∈RSC×SC L(B) +λ̃∥B∥₁ với λ̃=λ/CS (11)

trong đó ∥ · ∥F là chuẩn Frobenius, ∥ · ∥ ₁ là chuẩn L1.

--- TRANG 3 ---
Hiểu MLP-Mixer như một MLP Rộng và Thưa

[Hình 1. Sơ đồ tổng quan của độ thưa được xử lý trong công trình này. (a) Một ma trận trọng số được che M⊙A trong một MLP trọng số thưa (SW-MLP). Độ rộng của nó là O(1/√p), trong đó p là tỷ lệ các mục khác không trong mặt nạ M. (b) Một lớp mixing trong MLP-Mixer với vector hóa. Trọng số hoạt động như một ma trận đường chéo khối. (c) Một trọng số của random permuted mixer (RP-Mixer), được giới thiệu trong Mục 5. Cấu trúc đường chéo khối bị phá hủy bởi các ma trận hoán vị ngẫu nhiên J1, J2 để đạt được sự tương tự với SW-MLP.]

Việc suy ra được trình bày trong Phụ lục B.4. Nó mở rộng thực tế rằng tham số hóa tích Hadamard có bias ngầm hướng đến regularization L1 đến trường hợp tham số hóa tích Kronecker (Hoff, 2017; Yasuda et al., 2023). Lưu ý rằng số lượng tham số độc lập khác nhau giữa bên trái và bên phải của bất đẳng thức. Do đó, λ̃ có thể xuất hiện nhỏ, nhưng nó chỉ chuẩn hóa sự thay đổi trong kích thước tham số (xem Phụ lục B.4 để biết thêm chi tiết). Bias ngầm hướng đến độ thưa được coi là một tính chất mong muốn trong các kiến trúc mạng nơ-ron hiện đại (Neyshabur et al., 2014; Woodworth et al., 2020).

3.3. Ma trận Monarch ẩn đằng sau các Mixer
Dao et al. (2022) đã đề xuất một ma trận Monarch M∈Rⁿˣⁿ được định nghĩa bởi

M=J⊤cLJcR, (12)

trong đó L và R là các ma trận đường chéo khối có thể huấn luyện, mỗi ma trận có √n khối kích thước √n×√n. Công trình trước đây tuyên bố rằng ma trận Monarch thưa ở chỗ số lượng tham số có thể huấn luyện nhỏ hơn nhiều so với trong ma trận dày đặc n×n. Bất chấp độ thưa này, bằng cách thay thế ma trận dày đặc bằng ma trận Monarch, người ta thấy rằng các kiến trúc khác nhau có thể đạt được hiệu suất gần như tương đương trong khi thành công trong việc rút ngắn thời gian huấn luyện. Hơn nữa, tích của một số ít ma trận Monarch có thể biểu diễn nhiều ma trận có cấu trúc thường được sử dụng như convolution và biến đổi Fourier.

Một cách đáng ngạc nhiên, MLP Mixer và ma trận Monarch, hai khái niệm hoàn toàn khác biệt, có các kết nối ẩn. Bằng cách so sánh (10) và (12), chúng tôi thấy rằng

Hệ quả 3.4. Xem xét một S-Mixer không có hàm kích hoạt trung gian, tức là ma trận đặc trưng của nó được cho bởi H=ϕ(WXV ). Thì vec(H) tương đương với một MLP có ma trận trọng số được cho bởi ma trận Monarch với các ma trận đường chéo chia sẻ trọng số, tức là ϕ(Mx) với n= SC, L=IS⊗V⊤ và R=IC⊗W.

Chúng tôi xác nhận sự tương tự giữa ma trận Monarch và Mixer thông qua các thí nghiệm. Ở đây, chúng tôi xem xét một MLP nông ( x7→BcB2ReLU( B1x)) trong đó Bc là lớp phân loại. Chúng tôi so sánh hiệu suất của các mô hình trong đó trọng số Bi(i= 1,2) được thay thế bằng ma trận Monarch Mi, và các mô hình trong đó chúng được thay thế bằng tích Kronecker Vi⊗Wi. Trong Hình 2 (trái), hai mô hình cho thấy hiệu suất tương đương. Điều này gợi ý rằng mặc dù Mixer kết hợp cấu trúc chia sẻ trọng số so với Monarch, hiệu suất của nó không bị ảnh hưởng.

3.4. So sánh các đặc trưng ẩn
Hãy quay lại tình huống trong thực tế nơi mô hình có kích hoạt phi tuyến trung gian. Để điều tra sự tương tự của các mạng phi tuyến, ở đây chúng tôi xem xét một MLP trọng số thưa không có cấu trúc (SW-MLP tóm tắt), là một triển khai cơ bản trong tài liệu về các MLP naïve với trọng số thưa (Golubeva et al., 2021). Trọng số của nó được cho bởi một mặt nạ thưa ngẫu nhiên, tức là M⊙A trong đó A là ma trận trọng số gốc và M là ma trận mặt nạ tĩnh có các mục được rút ra từ phân phối Bernoulli với xác suất p >0 bằng một tại giai đoạn khởi tạo. Trong mục này, để so sánh MLP-Mixer với SW-MLP chia sẻ điều kiện, chúng tôi xem xét trung bình p= (S⁻¹+C⁻¹)/2 của độ thưa của MLP-Mixer cho thiết lập của SW-MLP. Hình 1 tổng quan về các mô hình mà chúng tôi so sánh ở dưới. Một cái được hoán vị ngẫu nhiên là một lựa chọn thay thế cho SW-MLP và hiển thị kết quả trong Mục 5.

Như một thước đo tương tự, chúng tôi sử dụng centered kernel alignment (CKA) (Nguyen et al., 2021) giữa các đặc trưng ẩn của MLP với trọng số thưa và của MLP-Mixer. Trong thực tế, chúng tôi tính toán mini-batch CKA (Nguyen et al., 2021, Mục 3.1(2)) giữa các đặc trưng của mạng đã huấn luyện. Trong Hình 2(a), chúng tôi quan sát CKA trung bình đạt tối đa như một độ thưa thích hợp của MLP. Bằng cách so sánh Hình 2(b) và (c), chúng tôi thấy rằng ma trận CKA với MLP thưa rõ ràng cao hơn MLP dày đặc. Đặc biệt, Mixer thưa tương tự như MLP thưa hơn trong các đặc trưng ẩn. Thiết lập chi tiết của tất cả các thí nghiệm được tóm tắt trong Phụ lục C.

--- TRANG 4 ---
Hiểu MLP-Mixer như một MLP Rộng và Thưa

[Hình 2. (a) Trung bình của các mục đường chéo của CKA giữa MLP-Mixer đã huấn luyện ( S=C= 64,32) và MLP với độ thưa khác nhau, trong đó p là tỷ lệ các mục khác không trong M. (b) CKA giữa MLP-Mixer ( S=C= 64 ) và MLP với p= 1/64 tương ứng, và (c) CKA giữa Mixer và MLP dày đặc. (d) Lỗi kiểm tra trên MNIST của MLP nông với trọng số ma trận Monarch và trọng số Kronecker. Kết quả là trung bình của năm lần thử với các seed ngẫu nhiên khác nhau.]

4. Tính chất như một MLP rộng
Trong mục này, chúng tôi tiếp tục thảo luận về việc liệu MLP-Mixer (hoặc S-Mixer) có thể hiện xu hướng tương tự như của MLP trọng số thưa hay không. Đặc biệt, chúng tôi tìm kiếm độ thưa mong muốn và thảo luận về việc liệu có một thiết lập mong muốn của các siêu tham số trên các Mixer hay không.

4.1. Tối đa hóa độ thưa
Giả thuyết sau đây có vai trò cơ bản:

Giả thuyết 4.1 (Golubeva et al. (2021)). Tăng độ rộng đến một điểm nhất định, trong khi giữ số lượng tham số trọng số cố định, dẫn đến cải thiện độ chính xác kiểm tra.

Một cách trực quan, Golubeva et al. (2021) thách thức câu hỏi về việc liệu sự cải thiện hiệu suất của các mạng nơ-ron sâu quy mô lớn có phải do sự tăng số lượng tham số hay sự tăng độ rộng. Họ thành công trong việc xác minh thực nghiệm Giả thuyết 4.1; tức là, sự cải thiện là do sự tăng độ rộng trong MLP bình thường và ResNet (lưu ý rằng độ rộng của ResNet chỉ kích thước kênh).

Hãy ký hiệu bằng Ω số lượng kết nối trung bình trên mỗi lớp. Chúng ta có

Ω =pγm², (13)

trong đó p là tỷ lệ các mục khác không trong mặt nạ tĩnh. Ở đây, đối với MLP-Mixer,

Ω =γ(CS²+C²S)/2. (14)

Số lượng trung bình Ω của S-Mixer được giảm xuống γ= 1 trong (14), điều này duy trì khả năng đọc của các phương trình.

Bằng (14), chúng ta có S= (√(C²+ 8Ω /(γC))−C)/2. Đối với Ω và γ cố định, độ rộng hiệu quả được điều khiển bởi m=SC. Hình 3 cho thấy m như một hàm của C. Độ rộng m có dạng đỉnh đơn và được tối đa hóa tại ( C*, S*) như sau:

C*=S*= (Ω/γ)^(1/3), max(S,C) m= (Ω/γ)^(2/3). (15)

Tỷ lệ các mục khác không p= Ω/γm² được tối thiểu hóa tại điểm này, tức là độ thưa được tối đa hóa.

4.2. So sánh độ chính xác với độ rộng hiệu quả
Để xác nhận sự tương tự, chúng tôi so sánh lỗi kiểm tra của cả hai mạng với độ thưa khác nhau. Dưới số lượng kết nối cố định Ω per lớp, độ thưa tương đương với độ rộng. Hình 4 (trái) cho thấy lỗi kiểm tra của MLP-Mixer và MLP trọng số thưa tương ứng dưới Ω = 2¹⁹ cố định và γ= 2, cho một số độ rộng γm. Chúng tôi quan sát lỗi kiểm tra của cả hai mạng được cải thiện khi độ rộng tăng. Theo nghĩa này, MLP và MLP-Mixer có xu hướng tương tự về hiệu suất với độ rộng tăng. Tuy nhiên, chúng tôi quan sát đối với các trường hợp quá rộng xung quanh γm= 8000 trong Hình 4 (trái), lỗi kiểm tra của SW-MLP cao hơn MLP-Mixer và có ít thay đổi để đáp ứng với việc tăng độ rộng. Chúng tôi thảo luận xu hướng này trong mục tiếp theo.

4.3. Phân tích phổ với độ rộng tăng
Golubeva et al. (2021) báo cáo rằng nếu độ thưa trở nên quá cao, hiệu suất tổng quát hóa của SW-MLP hơi giảm. Họ thảo luận rằng sự giảm này được gây ra bởi sự suy giảm của khả năng huấn luyện, tức là nó trở nên khó khăn cho gradient descent để giảm hàm loss. Một số công trình trước đây báo cáo rằng các giá trị kỳ dị lớn của ma trận trọng số tại khởi tạo ngẫu nhiên gây ra sự suy giảm của khả năng huấn luyện trong các mạng sâu (Bjorck et al., 2018). Do đó, chúng tôi thảo luận sự khác biệt của các giá trị kỳ dị của trọng số giữa MLP-Mixer và SW-MLP.

Trong trường hợp của Mixer, mỗi trọng số γC×C V(tương ứng γS×S trọng số W) được khởi tạo bởi các biến ngẫu nhiên độc lập phân phối với N(0,1/C)(tương ứng N(0,1/S)). Sau đó, bởi lý thuyết của luật Marchenko-Pastur (Bai & Silverstein, 2010), giá trị kỳ dị tối đa của trọng số được xấp xỉ bởi cγ:= 1 +√γ. Vì tích Kronecker với ma trận đồng nhất không thay đổi giá trị kỳ dị tối đa, mỗi giá trị kỳ dị tối đa của V⊤⊗IS hoặc IC⊗W được xấp xỉ bởi cγ.

Xem xét trường hợp của SW-MLP. Chúng tôi khởi tạo các mục của mỗi ma trận mặt nạ M bằng các biến ngẫu nhiên Bernoulli độc lập với xác suất bằng 1/√p là √p và bằng 0 là 1−p. Chúng tôi khởi tạo mỗi trọng số A bằng độc lập N(0,1/m). Xem xét giá trị kỳ dị tối đa λmax của M⊙A. Đặt q=√pm. Sau đó bởi (Hwang et al., 2019, Định lý 2.9), λmax được xấp xỉ bởi √L+ trong nghĩa sau:

|λ²max−L+| ≺1/q⁴+ 1/m^(2/3), (16)

trong đó ≺ biểu thị sự thống trị ngẫu nhiên (Hwang et al., 2019, Định nghĩa 2.3), và

L+=c²γ+ 3c²γ√γ(1−p)/q²+O(1/q⁴). (17)

Dưới Ω cố định, bằng (13), số hạng thống trị của L+ trong (17) là tuyến tính theo m như sau:

(1−p)/q²=γm/Ω−1/m=O(m) khi m→ ∞ . (18)

Do đó, giá trị kỳ dị tối đa λmax của M⊙A tăng khi độ rộng tăng. Trong Phụ lục D.1, chúng tôi thảo luận phổ trong giới hạn Ω lớn và cho thấy xu hướng tương tự về λmax như giới hạn m lớn.

Trong Hình 4 (phải), chúng tôi quan sát giá trị kỳ dị tối đa của M⊙A tăng bằng cách mở rộng m với Ω và γ cố định. Đặc biệt, giá trị luôn cao hơn giá trị lý thuyết của giá trị kỳ dị của Mixer. Những giá trị kỳ dị lớn như vậy làm suy giảm hiệu suất của SW-MLP. Ngược lại, chúng ta có thể mở rộng độ rộng và độ thưa của MLP-Mixer mà không có sự tăng không mong muốn trong giá trị kỳ dị tối đa.

5. Vượt ra ngoài MLP naïve về độ rộng
Như đã thấy trong Mục 4.2, MLP-Mixer có xu hướng tương tự như SW-MLP. Trong các mô hình rộng hơn nhiều, để tiếp tục so sánh MLP-Mixer và MLP trọng số thưa không có cấu trúc, chúng tôi cần một lựa chọn thay thế cho MLP mặt nạ tĩnh vì chi phí tính toán lớn, yêu cầu bộ nhớ (Phụ lục D.9), và hành vi xấu về phổ (Mục 4.3). Do đó chúng tôi tiếp tục thảo luận về việc phá hủy cấu trúc của MLP-Mixer một phần và đề xuất một mô hình thay thế của SW-MLP, được gọi là random permuted mixer (RP-Mixer).

5.1. Họ PK
Để giới thiệu một lựa chọn thay thế cho MLP trọng số thưa, chúng tôi đề xuất một họ Kronecker hoán vị (PK) như một tổng quát hóa của MLP-Mixer.

Ma trận hoán vị: Một ma trận hoán vị m×m J là một ma trận được cho bởi (Jx)i=xσ(i)(i= 1,2, . . . , m )cho một hoán vị chỉ số σ. Đặc biệt, ma trận hoán vị Jc là một ma trận hoán vị (Magnus & Neudecker, 2019). Đối với bất kỳ ma trận hoán vị J, x∈Rm, Jϕ(x) =ϕ(Jx).

Định nghĩa 5.1 (Lớp PK và họ PK). Hãy J1, J2 là các ma trận hoán vị m×m. Đối với X∈Rⁿ¹ˣⁿ², chúng tôi định nghĩa lớp PK như sau:

PK-LayerW(X;J1, J2) :=ϕ[J2(In1⊗W)J1vec(X)],

trong đó chúng tôi đặt m=n1n2, W∈Rⁿ²ˣⁿ². Chúng tôi gọi tập hợp các kiến trúc có các lớp ẩn được tạo thành từ các lớp PK là họ PK.

Vì Jc là một ma trận hoán vị, S-Mixer bình thường và MLP-Mixer thuộc họ PK (Xem Mục B.3 cho chi tiết). Điểm quan trọng của PK-Layer là độ rộng m của nó có thể lớn, nhưng không cần phải bảo toàn ma trận trọng số m×m một cách rõ ràng trong bộ nhớ. Chúng ta có thể tính toán sự lan truyền tín hiệu tiến bằng một phép nhân ma trận tương đối nhỏ theo cách tương tự như MLP-Mixer: Đầu tiên, J1vec(X) =:y là một sắp xếp lại của các mục X. Tiếp theo, chúng ta tính toán pre-activation bằng cách sử dụng tích ma trận (In1⊗W)y=Wmat(y). Cuối cùng, chúng ta áp dụng kích hoạt theo từng phần tử và sắp xếp lại bằng J2. Do đó, lớp PK thân thiện với bộ nhớ, trong khi MLP dày đặc naïve yêu cầu bảo toàn ma trận trọng số m×m và đòi hỏi tính toán lớn.

--- TRANG 5 ---
Hiểu MLP-Mixer như một MLP Rộng và Thưa

[Hình 5. Lỗi kiểm tra được cải thiện khi độ rộng hiệu quả tăng. Hình này trình bày các mô hình S-Mixer, RP S-Mixer, và SW-MLP trên CIFAR-10 (trái), CIFAR-100 (giữa), và STL-10 (phải). Các thí nghiệm được thực hiện sử dụng ba seed ngẫu nhiên khác nhau, và lỗi kiểm tra trung bình được mô tả. Độ lệch chuẩn quan sát được nhỏ hơn 0.026 cho CIFAR-10, 0.056 cho CIFAR-100, và 0.008 cho STL-10.]

[Bảng 1. Lỗi kiểm tra trên CIFAR-10/CIFAR-100/ImageNet-1k từ đầu. (Bảng trên) Bằng cách đặt S và C gần nhau hơn dưới cùng số lượng kết nối tổng qua các lớp, độ rộng tối đa của các lớp trở nên lớn hơn trong của chúng tôi (Mixer-SS-W). Lỗi kiểm tra của nó cuối cùng được cải thiện nhiều hơn so với β-LASSO. (Bảng dưới) Bằng cách đặt S và C gần nhau hơn dưới cùng Ω, lỗi kiểm tra trong của chúng tôi (Mixer-B-W) được cải thiện so với MLP-Mixer gốc (Mixer-B/16). Mỗi thí nghiệm được thực hiện với ba seed ngẫu nhiên.]

5.2. Random Permuted Mixer
Trong Mixer bình thường, J1 và J2 bị hạn chế vào đồng nhất hoặc hoán vị. Điều này có nghĩa là các ma trận trọng số thưa của MLP hiệu quả có cấu trúc cao vì các ma trận khối của chúng là đường chéo. Để phá hủy cấu trúc, chúng tôi giới thiệu RP-Mixer. Một RP S-Mixer có (J1,J2) trong mỗi lớp PK, được cho bởi các ma trận hoán vị ngẫu nhiên như U=PK-LayerW(X;J1, J2) và PK-LayerV⊤(U;J'1, J'2).

Tương tự, đối với RP MLP-Mixer, chúng tôi đặt lớp PK tương ứng với token mixing và channel mixing thành các ma trận hoán vị ngẫu nhiên. Từ định nghĩa của lớp PK (5.1), điều này tương đương với MLP hiệu quả với độ rộng SC (và γSC cho MLP-Mixer) và trọng số thưa

Weff=J2(In1⊗W)J1. (19)

Bởi vì (J1, J2) là các hoán vị ngẫu nhiên, các mục khác không của Weff được rải rác khắp ma trận. Theo nghĩa này, RP Mixer có vẻ như trở nên gần gũi hơn nhiều với trọng số thưa ngẫu nhiên so với Mixer bình thường. Hình 1(d) minh họa cấu hình trọng số ngẫu nhiên rải rác này, trong khi Hình S.9 trình bày một ví dụ số thực tế. Vì các ma trận hoán vị là ma trận trực giao, các giá trị kỳ dị của Weff vẫn giống hệt với của Mixer bình thường, do đó bảo toàn phổ được thảo luận trong Mục 4.3.

5.3. Tăng độ rộng
Hình 5 cho thấy lỗi kiểm tra cải thiện khi độ rộng hiệu quả của các Mixer tăng. Chúng tôi huấn luyện S-Mixer bình thường và RP cho các giá trị khác nhau của S và C với Ω cố định. S-Mixer bình thường và RP cho thấy xu hướng tương tự về việc tăng lỗi kiểm tra đối với độ rộng hiệu quả m. MLP-Mixer bình thường và RP cũng cho thấy xu hướng tương tự như được trình bày trong Phụ lục D.2. Vì SW-MLP mặt nạ tĩnh yêu cầu ma trận trọng số m×m, một SW-MLP với Ω lớn không thể được hiển thị trong hình. Ngược lại, chúng ta có thể sử dụng độ rộng hiệu quả lớn cho các Mixer, và lỗi tiếp tục tăng khi độ rộng tăng. Điều này có thể được diễn giải là các Mixer thực hiện một thiết lập độ rộng mà MLP naïve không thể đạt được đủ. Cuối cùng, hình gợi ý rằng một độ rộng cực lớn như vậy là một trong những yếu tố đóng góp vào sự thành công của Mixer.

Bảng 1 cho thấy một so sánh của MLP-Mixer với độ thưa động β-LASSO (Neyshabur, 2020). Chúng tôi thấy một Mixer rộng hơn (Mixer-SS-W) có hiệu suất tốt hơn β-LASSO. Bảng 1 cho thấy một so sánh của Mixer-B/16 (Tolstikhin et al., 2021) và một Mixer rộng hơn (Mixer-B-W). Bằng cách đặt S và C gần nhau hơn dưới Ω cố định, độ rộng tối đa của các lớp trở nên lớn hơn trong của chúng tôi (Mixer-B-W). Lỗi kiểm tra của nó cuối cùng được cải thiện nhiều hơn so với MLP-Mixer gốc (Mixer-B/16). Trong cả hai kết quả, độ rộng cải thiện hiệu suất ngay cả khi Ω được cố định.

--- TRANG 6 ---
Hiểu MLP-Mixer như một MLP Rộng và Thưa

[Hình 6. Họ PK đạt được lỗi kiểm tra thấp nhất xung quanh C=S. S-Mixer trên (a) CIFAR-10, (b) CIFAR-100, (c) STL-10. MLP-Mixer trên (d) ImageNet-1k. Đường đỏ: Mixer bình thường, đường vàng: RP Mixer, đường gạch ngang: C=S.]

[Hình 7. RP Mixer có thể trở nên tương đương hoặc thậm chí vượt trội hơn những cái bình thường nếu độ sâu tăng. Chúng tôi đặt C=S= 128.]

5.4. Hiệu suất tại độ rộng và độ thưa tối ưu
Hình 6 xác nhận rằng độ rộng tối đa (15) được suy ra từ Giả thuyết 4.1 giải thích thích hợp hiệu suất thực nghiệm của các Mixer. Các mô hình được huấn luyện sử dụng phân loại có giám sát cho mỗi tập dữ liệu. Đối với CIFAR-10, CIFAR-100 và STL-10, chúng tôi huấn luyện S-Mixer bình thường và RP. Chúng tôi cố định chiều của lớp FC mỗi patch và thay đổi S và C trong khi duy trì số lượng Ω cố định. Có thể quan sát thấy rằng lỗi kiểm tra được tối thiểu hóa xung quanh C=S, như mong đợi từ Giả thuyết 4.1 và (15). Xu hướng này phổ biến đối với Mixer bình thường và RP.

Đối với ImageNet-1k, chúng tôi huấn luyện MLP-Mixer bình thường và RP. Tương tự, hiệu suất của nó được tối đa hóa xung quanh C=S.

Chúng tôi quan sát một xu hướng tương tự cũng trong các thiết lập khác nhau của siêu tham số trong Phụ lục D.3 và Phụ lục D.5.

Nhận xét về độ sâu: Như được hiển thị trong Hình 7, chúng tôi quan sát rằng RP Mixer có xu hướng hoạt động kém hơn ở độ sâu hạn chế nhưng có thể đạt được kết quả tương đương hoặc tốt hơn so với Mixer bình thường với độ sâu tăng. Điều này có vẻ hợp lý do khả năng chống overfitting của RP-Mixer sâu hoặc trường nhận thức nhỏ của RP-Mixer nông (xem Phụ lục D.4 cho chi tiết). Chúng tôi cũng xác nhận rằng sự phụ thuộc vào độ sâu này không thay đổi sự tương tự cơ bản liên quan đến độ rộng và độ thưa.

6. Kết luận và hướng tương lai
Công trình này cung cấp cái nhìn sâu sắc mới rằng MLP-Mixer hiệu quả hoạt động như một MLP rộng với trọng số thưa. Phân tích trong trường hợp kích hoạt tuyến tính làm sáng tỏ regularization thưa ngầm thông qua biểu thức tích Kronecker và tiết lộ một kết nối với ma trận Monarch. SW-MLP, Mixer bình thường và RP thể hiện sự tương tự định lượng trong xu hướng hiệu suất, xác minh rằng độ thưa là cơ chế chính làm nền tảng cho MLP-Mixer. Tối đa hóa độ rộng hiệu quả và độ thưa dẫn đến hiệu suất cải thiện. Chúng tôi mong đợi rằng công trình hiện tại sẽ phục vụ như một nền tảng để khám phá các thiết kế tinh vi hơn của kiến trúc dựa trên MLP và việc triển khai hiệu quả của mạng nơ-ron với bias quy nạp mong muốn.

Khám phá tiềm năng của MLP với trọng số có cấu trúc hơn nữa sẽ thú vị. Đánh giá độ rộng và độ thưa tối ưu một cách lý thuyết có thể là một chủ đề nghiên cứu thú vị (Edelman et al., 2023). Như chúng tôi đã lưu ý, tính giải được của cực tiểu toàn cục và động học trong các lớp mixing, ngay cả với kích hoạt tuyến tính, vẫn không chắc chắn, và lý thuyết vẫn chưa giải quyết đầy đủ hoặc vượt qua vấn đề này. Việc làm rõ liệu có các ứng viên tiềm năng khác cho các kiến trúc thân thiện với bộ nhớ với bias quy nạp mong muốn cũng sẽ thú vị. Đặc biệt, chia sẻ trọng số được biết đến hoạt động tốt cho các miền ảnh, thỉnh thoảng vượt trội hơn các mạng không có chia sẻ trọng số (Ott et al., 2020). Cho rằng các Mixer có thể được coi như xấp xỉ của MLP với ma trận Monarch chia sẻ trọng số, việc đánh giá tính hợp lệ của các xấp xỉ như vậy sẽ là một chủ đề thú vị.

--- TRANG 7 ---
Hiểu MLP-Mixer như một MLP Rộng và Thưa

Tài liệu tham khảo
Arora, S., Cohen, N., Hu, W., and Luo, Y. Regularization ngầm trong phân tích ma trận sâu. Advances in Neural Information Processing Systems, 32, 2019.

Bachmann, G., Anagnostidis, S., and Hofmann, T. Scaling mlps: Một câu chuyện về bias quy nạp. Trong Advances in Neural Information Processing Systems, 2023.

Bai, Z. and Silverstein, J. W. Phân tích phổ của ma trận ngẫu nhiên chiều lớn, tập 20. Springer, 2010.

Bjorck, N., Gomes, C. P., Selman, B., and Weinberger, K. Q. Hiểu batch normalization. Advances in Neural Information Processing Systems, 2018.

Cordonnier, J.-B., Loukas, A., and Jaggi, M. Về mối quan hệ giữa self-attention và các lớp convolution. Trong International Conference on Learning Representations, 2019.

Cubuk, E. D., Zoph, B., Mane, D., Vasudevan, V., and Le, Q. V. Autoaugment: Học chiến lược tăng cường từ dữ liệu. Trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 113–123, 2019.

Dao, T., Chen, B., Sohoni, N. S., Desai, A., Poli, M., Grogan, J., Liu, A., Rao, A., Rudra, A., and Ré, C. Monarch: Ma trận có cấu trúc biểu cảm cho huấn luyện hiệu quả và chính xác. Trong International Conference on Machine Learning, pp. 4690–4721. PMLR, 2022.

d'Ascoli, S., Sagun, L., Biroli, G., and Bruna, J. Tìm kim trong đống cỏ khô với convolution: về lợi ích của bias kiến trúc. Advances in Neural Information Processing Systems, 2019.

Dettmers, T. and Zettlemoyer, L. Mạng thưa từ đầu: Huấn luyện nhanh hơn mà không mất hiệu suất. arXiv preprint arXiv:1907.04840, 2019.

Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. Một ảnh trị giá 16x16 từ: Transformer cho nhận dạng ảnh ở quy mô. Trong International Conference on Learning Representations, 2021.

Edelman, B. L., Goel, S., Kakade, S., Malach, E., and Zhang, C. Biên giới Pareto trong học đặc trưng nơ-ron: Dữ liệu, tính toán, độ rộng, và may mắn. Trong Advances in Neural Information Processing Systems, 2023.

Evci, U., Gale, T., Menick, J., Castro, P. S., and Elsen, E. Rigging the lottery: Làm cho tất cả vé đều thắng. Trong International Conference on Machine Learning, pp. 2943–2952. PMLR, 2020.

Folland, G. B. Real Analysis: Modern Techniques and Their Applications. John Wiley & Sons, 2013.

Frankle, J. and Carbin, M. Giả thuyết vé số: Tìm mạng nơ-ron thưa, có thể huấn luyện. Trong International Conference on Learning Representations, 2019.

Fu, D. Y., Arora, S., Grogan, J., Johnson, I., Eyuboglu, S., Thomas, A. W., Spector, B. F., Poli, M., Rudra, A., and Re, C. Monarch mixer: Một kiến trúc dựa trên GEMM đơn giản sub-quadratic. Trong Advances in Neural Information Processing Systems, 2023.

Gadhikar, A. H., Mukherjee, S., and Burkholz, R. Tại sao pruning ngẫu nhiên là tất cả những gì chúng ta cần để bắt đầu thưa. Trong International Conference on Machine Learning, pp. 10542–10570. PMLR, 2023.

Golubeva, A., Neyshabur, B., and Gur-Ari, G. Mạng rộng hơn có tốt hơn với cùng số lượng tham số không? Trong International Conference on Learning Representations, 2021.

Hameed, M. G. A., Tahaei, M. S., Mosleh, A., and Nia, V. P. Nén mạng nơ-ron convolution thông qua phân tích tích Kronecker tổng quát. Trong Proceedings of the AAAI Conference on Artificial Intelligence, pp. 36:771–779, 2022.

Hoff, P. D. Hồi quy tensor đa tuyến tính cho dữ liệu quan hệ dọc. The annals of applied statistics, 9(3): 1169, 2015.

Hoff, P. D. Lasso, chuẩn phân số và ước lượng thưa có cấu trúc sử dụng tham số hóa tích Hadamard. Computational Statistics & Data Analysis, 115:186–198, 2017.

Hwang, J. Y., Lee, J. O., and Schnelli, K. Luật địa phương và giới hạn Tracy–Widom cho ma trận hiệp phương sai mẫu thưa. The Annals of Applied Probability, 29(5):3006 – 3036, 2019.

Liu, S., Chen, T., Chen, X., Shen, L., Mocanu, D. C., Wang, Z., and Pechenizkiy, M. Hiệu quả không hợp lý của pruning ngẫu nhiên: Sự trở lại của baseline naïve nhất cho huấn luyện thưa. Trong International Conference on Learning Representations, 2022.

Magnus, J. R. and Neudecker, H. Tính toán vi phân ma trận với ứng dụng trong thống kê và kinh tế lượng. John Wiley & Sons, 2019.

Martens, J. and Grosse, R. Tối ưu hóa mạng nơ-ron với xấp xỉ độ cong Kronecker-factored. Trong International Conference on Machine Learning, pp. 2408–2417. PMLR, 2015.

Neyshabur, B. Hướng tới học convolution từ đầu. Trong Advances in Neural Information Processing Systems, 2020.

Neyshabur, B., Tomioka, R., and Srebro, N. Trong tìm kiếm bias quy nạp thực sự: Về vai trò của regularization ngầm trong học sâu. arXiv preprint arXiv:1412.6614, 2014.

Nguyen, T., Raghu, M., and Kornblith, S. Mạng rộng và sâu có học những điều giống nhau không? Khám phá cách biểu diễn mạng nơ-ron thay đổi theo độ rộng và độ sâu. Trong International Conference on Learning Representations, 2021.

Ott, J., Linstead, E., LaHaye, N., and Baldi, P. Học trong máy: Chia sẻ hay không chia sẻ? Neural Networks, 126:235–249, 2020.

Pellegrini, F. and Biroli, G. Pruning mạng nơ-ron loại bỏ tiếng ồn khỏi các đặc trưng và làm cho kết nối địa phương xuất hiện trong các tác vụ thị giác. Trong International Conference on Machine Learning, pp. 17601–17626. PMLR, 2022.

Rosenblatt, F. Perceptron: một mô hình xác suất cho lưu trữ và tổ chức thông tin trong não. Psychological review, 65(6):386, 1958.

Sahiner, A., Ergen, T., Ozturkler, B., Pauly, J., Mardani, M., and Pilanci, M. Giải mã attention qua tính đối ngẫu lồi: Phân tích và diễn giải của Vision Transformer. Trong International Conference on Machine Learning, pp. 19050–19088. PMLR, 2022.

Saxe, A., McClelland, J., and Ganguli, S. Nghiệm chính xác cho động học phi tuyến của học trong mạng nơ-ron tuyến tính sâu. Trong Proceedings of the International Conference on Learning Representations, 2014.

Schmidhuber, J. Học sâu trong mạng nơ-ron: Một tổng quan. Neural Networks, 61:85–117, 2015.

Tolstikhin, I., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., Yung, J., Keysers, D., Uszkoreit, J., Lucic, M., et al. MLP-Mixer: Một kiến trúc hoàn toàn MLP cho thị giác. Trong Advances in Neural Information Processing Systems, 2021.

Touvron, H., Bojanowski, P., Caron, M., Cord, M., El-Nouby, A., Grave, E., Izacard, G., Joulin, A., Synnaeve, G., Verbeek, J., et al. ResMLP: Mạng feedforward cho phân loại ảnh với huấn luyện hiệu quả dữ liệu. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.

Wightman, R. Pytorch image models. https://github.com/rwightman/pytorch-image-models, 2019.

Woodworth, B., Gunasekar, S., Lee, J. D., Moroshko, E., Savarese, P., Golan, I., Soudry, D., and Srebro, N. Kernel và chế độ phong phú trong các mô hình quá tham số hóa. Trong Conference on Learning Theory, pp. 3635–3673. PMLR, 2020.

Yasuda, T., Bateni, M., Chen, L., Fahrbach, M., Fu, G., and Mirrokni, V. Sequential attention cho lựa chọn đặc trưng. Trong International Conference on Learning Representations, 2023.

Yu, W., Luo, M., Zhou, P., Si, C., Zhou, Y., Wang, X., Feng, J., and Yan, S. MetaFormer thực sự là những gì bạn cần cho thị giác. Trong Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pp. 10819–10829, 2022.

Zhang, A., Tay, Y., Zhang, S., Chan, A., Luu, A. T., Hui, S., and Fu, J. Vượt ra ngoài các lớp kết nối đầy đủ với quaternion: Tham số hóa của phép nhân siêu phức với 1/n tham số. Trong International Conference on Learning Representations, 2021.

Zhou, S., Wu, J.-N., Wu, Y., and Zhou, X. Khai thác các cấu trúc địa phương với lớp Kronecker trong mạng convolution. arXiv:1512.09194, 2015.

--- TRANG 8 ---
Hiểu MLP-Mixer như một MLP Rộng và Thưa

A. Chi tiết về Kiến trúc
Ở đây, chúng tôi tổng quan chi tiết kỹ thuật hơn về tất cả các mô hình: MLP-Mixer, Simple Mixer (S-Mixer), và MLP với trọng số thưa (SW-MLP). Trong Mục A.1, chúng tôi giới thiệu sự biến đổi từ ảnh đầu vào đến lớp ẩn đầu tiên. Trong Mục A.2, chúng tôi tổng quan một số công thức chi tiết của các mô hình bao gồm skip connection và layer normalization.

[Bảng S.1. Bảng tóm tắt các mô hình. Ký hiệu ẩn trong khối có nghĩa là liệu mô hình có lớp ẩn trong mỗi khối MLP hay không. Ký hiệu (*) chỉ ra thiết lập phụ thuộc vào loại thí nghiệm. Ở đây, GAP là global average pooling.]

A.1. Lớp FC Per-patch
Lớp đầu tiên của MLP-Mixer được cho bởi cái gọi là lớp FC per-patch, là một channel mixing lớp đơn. Trong tất cả các thí nghiệm, đối với kích thước patch P, ảnh đầu vào được phân tách thành HW/P² patch ảnh không chồng lắp với kích thước P×P; chúng tôi sắp xếp lại các ảnh đầu vào H×W với 3 kênh thành một ma trận có kích thước được cho bởi (HW/P²)×3P²=S₀×C₀. Đối với ảnh được sắp xếp lại X∈R^(S₀×C₀), lớp kết nối đầy đủ (FC) per-patch được cho bởi

Y=XW^T, (S.1)

trong đó W là ma trận trọng số C×C₀. Chúng tôi sử dụng lớp FC per-patch không chỉ cho Mixer mà còn cho SW-MLP.

Nhận xét về lớp FC per-patch: Nghiên cứu gốc đặt kích thước của các lớp mixing thành S=S₀. Ngược lại, để điều tra đóng góp của mỗi kích thước ảnh đầu vào và kích thước lớp ẩn một cách độc lập, việc thay đổi (S, C) độc lập với (S₀, C₀) là hợp lý. Do đó, chúng tôi làm cho lớp FC per-patch biến đổi kích thước đầu vào C₀ thành kích thước đầu ra C và lớp token mixing đầu tiên biến đổi S₀ thành S.

A.2. MLP-Mixer và S-Mixer
Hãy ký hiệu một khối của MLP-Mixer bởi

f_(W1,W2)(X) =ϕ(XW₁^T)W₂^T, (S.2)

và của S-Mixer bởi

f_(W1)(X) =ϕ(XW₁^T). (S.3)

Chúng tôi đặt ϕ= GELU.

A.2.1. MLP-MIXER
Chúng tôi đặt layer normalization ( LN) bởi

LN(X) =(X−m(X))/√(v(X) +ε)⊙γ+β, X ∈R^(S×C), (S.4)

trong đó ⊙ biểu thị tích Hadamard, m(X)(tương ứng. v(X)) là trung bình thực nghiệm (tương ứng. phương sai thực nghiệm) của X đối với trục kênh, và γ, β là các tham số có thể huấn luyện. Chúng tôi đặt ε= 10⁻⁵ trong tất cả các thí nghiệm.

Trong việc triển khai các lớp kết nối đầy đủ, chúng tôi chỉ sử dụng phép nhân ma trận bên phải theo cách tương tự như MLP-Mixer gốc (Tolstikhin et al., 2021). Một khối token-mixing X7→U của MLP-Mixer được cho bởi

U=X+f_(W1,W2)(LN(X)^T)^T, (S.5)

trong đó W₁ là S×γS và W₂ là γS×S. Tương tự, chúng tôi đặt một khối channel-mixing U7→Y như

Y=U+f_(W3,W4)(LN(U)), (S.6)

trong đó W₃, W₄ là các ma trận trọng số.

Chúng tôi gọi hàm hợp thành X7→Y của khối token-mixing và khối channel-mixing là một khối cơ sở của MLP-Mixer. MLP-Mixer với L-khối được tạo thành theo thứ tự của lớp FC per-patch, L khối cơ sở, và global average pooling với layer normalization, và lớp phân loại kết nối đầy đủ cuối cùng.

A.2.2. S-MIXER
S-Mixer không có hoán vị ngẫu nhiên được triển khai bằng cách thay thế khối MLP f_(W1,W2) và f_(W3,W4) trong MLP-Mixer bằng các khối FC. Tức là, các khối token-mixing và channel-mixing được cho bởi

U=X+f_W(LN(X)^T)^T, (S.7)
Y=U+f_V(LN(U)), (S.8)

trong đó W và V là các ma trận trọng số. Chuyển vị của ma trận đầu vào trong khối token-mixing được triển khai bằng sắp xếp lại các mục. Chúng tôi quyết định áp dụng cả skip-connection và layer normalization ngay cả trong S-Mixer. Đây là một yêu cầu kỹ thuật khá để đảm bảo sự giảm của loss huấn luyện trong các kiến trúc sâu.

A.2.3. MIXER VỚI CÁC LỚP KRONECKER HOÁN VỊ
Ở đây chúng tôi triển khai MLP-Mixer và S-Mixer tổng quát hóa với ma trận hoán vị và lớp PK. Nhớ lại rằng đối với bất kỳ ma trận X,

X^T= Mat( J_c vec(X)), (S.9)

trong đó J_c là ma trận hoán vị m×m. Do đó, khối token-mixing của S-Mixer là

U=X+ Mat ◦J_c^T◦vec◦f_W◦Mat◦J_c◦vec◦LN(X) (S.10)
=X+ Mat ◦PK-Layer_W(LN(X);J_c, J_c^T). (S.11)

Tương tự, khối channel-mixing của S-Mixer bằng

Y=U+ Mat ◦PK-Layer_V(LN(U);I, I), (S.12)

trong đó I là ma trận đồng nhất. Lưu ý rằng skip-connection thu thập J_c và J_c^T trong cùng một khối mixing để tương thích về hình dạng của các đơn vị ẩn.

Để có được ví dụ về họ PK và để tổng quát hóa Mixer, chúng tôi triển khai random permuted (RP) S-Mixer với skip-connection bằng cách thay thế J_c và J_c^T bằng các ma trận hoán vị ngẫu nhiên độc lập J₁ và J₂:

U=X+ Mat ◦PK-Layer_W(LN(X);J₁, J₂). (S.13)

Chúng tôi triển khai hoán vị ngẫu nhiên bằng xáo trộn ngẫu nhiên của các chỉ số đầu ra m=SC của vector. Chúng tôi đóng băng nó trong bước huấn luyện. Lưu ý rằng chúng tôi tránh sử dụng biểu diễn ma trận m×m của J_x để hiệu quả bộ nhớ. Chúng tôi triển khai random permuted (RP) MLP-Mixer bằng cách tương tự như RP-S-Mixer.

A.2.4. SKIP-CONNECTION TRONG KHỐI ĐẦU TIÊN
Khối token-mixing đầu tiên có hình dạng đầu vào (S₀, C) và hình dạng đầu ra (S, C). Tuy nhiên, chúng tôi cần thay đổi S với việc cố định S₀ trong một số thí nghiệm. Để kiểm soát sự khác biệt của S₀ và S, chúng tôi đặt khối token-mixing đầu tiên như sau:

U=SkipLayer (X) +PK-Layer_W(LN(X);J₁, J₂), (S.14)

trong đó skip layer được cho bởi

SkipLayer (X) = LN( W̃X), (S.15)

trong đó W̃ là ma trận trọng số S×S₀. Để so sánh công bằng, chúng tôi sử dụng skip layer ngay cả khi S=S₀ trong các thí nghiệm mà chúng tôi quét S. Chúng tôi sử dụng thiết lập tương tự cho MLP-Mixer như cho S-Mixer.

A.2.5. MLP TRỌNG SỐ THƯA (SW)
Cho 0< p≤1 và m∈N. Chúng tôi triển khai mỗi ma trận của một khối FC trọng số thưa tĩnh với tỷ lệ đóng băng p như sau:

x7→x+ϕ((M⊙W) LN( x)), x∈R^m, (S.16)

trong đó M là ma trận mặt nạ có m²p mục được chọn ngẫu nhiên và đặt thành một với xác suất p và những cái khác được đặt thành không với xác suất 1−p. Ma trận mặt nạ M được khởi tạo trước huấn luyện và nó được đóng băng trong quá trình huấn luyện. Chúng tôi cũng xem xét SW-MLP bao gồm các khối MLP trọng số thưa như sau:

x7→x+ϕ((M₂⊙W₂)ϕ((M₁⊙W₂) LN( x))), x∈R^m. (S.17)

W₁ và W₂ là các ma trận trọng số với đặc trưng ẩn γm, trong đó γ là hệ số mở rộng. M₁, M₂ là các ma trận mặt nạ có γm²p mục được chọn ngẫu nhiên và đặt thành một với xác suất p và những cái khác được đặt thành không với xác suất 1−p.

SW-MLP với L-khối được tạo thành theo thứ tự của lớp FC per-patch, vector hóa, L khối FC trọng số thưa tĩnh (hoặc khối MLP), và lớp phân loại FC cuối cùng.

B. Phân tích
B.1. Suy dẫn của Mệnh đề 4.1
Đối với H=ϕ(ϕ(WX)V), bằng cách sử dụng vec (WXV ) = (V^T⊗W)vec(X), chúng ta có

vec(H) =ϕ((V^T⊗I_S)vec(ϕ(WX)) (S.18)
=ϕ((V^T⊗I_S)ϕ((I_C⊗W)x)). (S.19)

Bởi vì J_c^T(A⊗B)J_c= (B⊗A)(Magnus & Neudecker, 2019) và bất kỳ ma trận hoán vị J nào đều giao hoán với hàm kích hoạt theo từng phần tử: Jϕ(x) =ϕ(Jx), chúng ta có

vec(H) =ϕ(J_c^T(I_S⊗V^T)ϕ(J_c(I_C⊗W)x)). (S.20)

MLP-Mixer không có skip (4) được biểu thị như sau: Y=Channel-MLP (Token-MLP (X))), và sau đó

u=ϕ(J_c(I_C⊗W₂)ϕ((I_C⊗W₁)x)),
y=ϕ(J_c^T(I_S⊗W₄^T)ϕ((I_S⊗W₃^T)u)).

trong đó u=vec(U) và y=vec(Y).

Có thể thông tin rằng một sự biến đổi tương tự giữa ma trận và vector được sử dụng trong một bối cảnh hoàn toàn khác biệt của học sâu, tức là tính toán Kronecker-factored Approximate Curvature (K-FAC) cho gradient descent tự nhiên (Martens & Grosse, 2015). K-FAC giả sử preconditioner theo lớp được cho bởi tích Kronecker, tức là (B⊗A)^(-1)vec(∇_W Loss(W)) trong đó A và B tương ứng với ma trận Gram của tín hiệu tiến và lùi. Gradient K-FAC này có thể được tính toán hiệu quả vì nó được rút gọn thành tính toán ma trận của A^(-1)∇_W Loss(W)(B^T)^(-1). Do đó, thủ thuật công thức hóa một tích ma trận-vector lớn cho tích giữa các ma trận tương đối nhỏ là phổ biến giữa K-FAC và biểu thức hiệu quả đã nêu ở trên.

B.2. Tích của Hoán vị Ngẫu nhiên
Ma trận hoán vị ngẫu nhiên m×m phân phối đều được cho bởi J=J_g trong đó g là biến ngẫu nhiên phân phối đều trên nhóm hoán vị S_m của m phần tử. Sau đó phân phối đều trên S_m là thước đo xác suất Haar, bất biến dịch chuyển (xem (Folland, 2013) để biết chi tiết), tức là J=J_σ J_g cũng phân phối đều nếu σ là biến ngẫu nhiên có giá trị S_m phân phối đều và g∈S_m là hằng số. Do đó, J=J_σ J_ρ là ma trận hoán vị ngẫu nhiên phân phối đều cho các biến ngẫu nhiên độc lập và phân phối đều σ và ρ trên S_m.

B.3. Biểu diễn như một họ PK
Bởi (10), có thể thấy rằng khối của S-Mixer là

U=PK-Layer_W(X;I, J_c),PK-Layer_{V^T}(U;I, J_c^T) (S.21)

Đối với MLP-Mixer, Token-MLP là

U=PK-Layer_{W₂}(PK-Layer_{W₁}(X;I, J₁);J₁^T, J_c) (S.22)

và Channel-MLP là

PK-Layer_{W₄^T}(PK-Layer_{W₃^T}(U;I, J₂);J₂^T, J_c^T) (S.23)

đối với các ma trận hoán vị tùy ý J₁ và J₂.

B.4. Chứng minh của regularization ngầm
Ở đây chúng tôi chứng minh Mệnh đề 3.3 và cho thấy một kết nối giữa đó và (Hoff, 2017). Vì mỗi trọng số V, W là vector C², S² chiều, chúng tôi chỉ cần chứng minh Bổ đề B.1 sau đây.

Bổ đề B.1. Cho m, n∈N. Chúng tôi viết ||x||_p cho chuẩn L_p của bất kỳ vector thực x nào. Cho f:R^{mn}→R. Đặt h:R^{mn}→R, g:R^m×R^n→R như sau:

ϕ(β) =f(β) +λ||β||₂, (S.24)
h(β) =f(β) +λ√{mn}||β||₁, (S.25)
g(u, v) =f(u⊗v) +λ(||u||₂² +||v||₂²)/2. (S.26)

Đặt

Θ_p:={u⊗v|u∈R^m, v∈R^n}. (S.27)

Thì nó giữ rằng

inf_{u∈R^m,v∈R^n} g(u, v) = inf_{β∈Θ_p} ϕ(β)≥ inf_{β∈R^{mn}} h(β). (S.28)

Chứng minh. Vì ||u⊗v||₂=||u||₂||v||₂, chúng ta có

inf_{u∈R^m,v∈R^n} g(u, v) = inf_{β∈Θ_p} f(β) +λ/2 inf_{v∈R^n}(||β||₂²/||v||₂² +||v||₂²). (S.29)

Inf trong đạt tối đa khi ||v||₂² =||β||₂, và tối đa là ||β||₂. (Lưu ý rằng nó không phải là chuẩn L2 bình phương.)

Do đó,

inf_{u∈R^m,v∈R^n} g(u, v) = inf_{β∈Θ_p}(f(β) +λ||β||₂). (S.30)

Cuối cùng, bởi bất đẳng thức Hölder, nó giữ rằng ||β||₁≤ ||β||₂√{mn}, điều này hoàn thành chứng minh.

Để mô tả kết nối của regularization L1 ngầm trong trường hợp tích Hadamard (Hoff, 2017), xem xét ma trận 11^T, trong đó 1 là vector với tất cả các mục đều bằng một. Để đơn giản, xem xét trường hợp S=C. Thật dễ dàng để khôi phục trường hợp tổng quát bằng cách thay đổi các hệ số tỷ lệ. Sau đó chúng ta có

Tr(W^T W⊗11^T) + Tr( 11^T⊗V^T V) =C²||W||_F² +S²||V||_F² =C²(||W||_F² +||V||_F²) (S.31)

Đặt

g(W, V ) :=L(W⊗V) +λ/2(∥W∥_F² +∥V∥_F²). (S.32)

Chúng ta có

g(W, V ) =L(W⊗V) +λ/(2C²)(Tr(W^T W⊗11^T) + Tr( 11^T⊗V^T V)) (S.33)

Ở đây xem xét vector hóa: u(W) := vec( W⊗11^T), v(V) := vec( 11^T⊗V). Thì

u(W)⊙v(V) = vec( W⊗V). (S.34)

Do đó chúng ta có

inf_{W,V} g(u, v) = inf_{W,V} L(u◦v) +λ/(2C²)(∥u∥₂² +∥v∥₂²), (S.35)

Bởi vì miền của u và v, chúng ta có

inf_{W,V} g(u, v)≥ inf_{u,v∈R^{S²C²}} L(u◦v) +λ/(2C²)(∥u∥₂² +∥v∥₂²). (S.36)

Bởi Hoff (2017), vế phải bằng

inf_{β∈R^{S²C²}} L(β) +λ/(2C²)∥β∥₁. (S.37)

Do đó bất đẳng thức cho tích Kronecker có mối quan hệ với (Hoff, 2017) từ vector hóa và tích Kronecker với 11^T.

Hãy tập trung vào hệ số chuẩn hóa 1/mn và 1/C². Nói chung, có một mối quan hệ tầm thường giữa regularization L1 và L2 như sau: Đối với θ∈R^M,

inf_θ L(θ) +√M λ∥θ∥₂≥ inf_θ L(θ) +λ∥θ∥₁, (S.38)

điều này trực tiếp theo từ bất đẳng thức Hölder. Tuy nhiên, trong tình huống của chúng tôi, không gian tham số của W⊗V là một không gian con của R^{S²C²}, vì vậy không rõ ràng liệu cùng một regularization hằng số cho cả regularization L1 và L2 có phù hợp hay không. Tại khởi tạo, chúng tôi đặt các mục của W, V phân phối độc lập với N(0,1/C), do đó bởi luật số lớn,

∥W∥_F², ∥V∥_F² =O(C). (S.39)

Trong trường hợp MLP dày đặc, chúng tôi khởi tạo β là N(0,1/SC), do đó

∥β∥₁=O(S²C²/√{SC}) =O(C³). (S.40)

Do đó

∥β∥₁/∥W∥_F², ∥β∥₁/∥V∥_F² =O(C²). (S.41)

Do đó, hệ số chuẩn hóa 1/C² trong (S.37) phù hợp với sự biến đổi tỷ lệ theo chiều của không gian tham số. Không có hệ số chuẩn hóa, số hạng regularization L1 sẽ lớn hơn C² lần so với loss L.

C. Thiết lập Thí nghiệm
C.1. Hình 2
Chúng tôi đặt số lượng khối của MLP-Mixer, ký hiệu bởi L, thành 3. Cả khối Token-mixing và khối Channel-Mixing đều có mặt L lần mỗi khối, dẫn đến tổng cộng 6 khối khi đếm riêng biệt. Chúng tôi cũng đặt γ= 2.

Để so sánh, MLP trọng số thưa thay thế các thành phần trong MLP-Mixer, cụ thể là (I_C⊗W₁), (I_C⊗W₂) và (W₃^T⊗I_S), (W₄^T⊗I_S), với dạng M⊙A. Trong bối cảnh này, không có sự phân biệt giữa khối token-mixing và channel-mixing, dẫn đến tổng cộng 6 khối.

Trong Hình 2 (a), chúng tôi so sánh MLP-Mixer với S=C= 64,32 và SW-MLP. Độ thưa của SW-MLP được lấy là 2^{-n} trong đó n dao động từ 0 đến 10. Chúng tôi đặt kích thước patch là 4. Mỗi mạng được huấn luyện trên CIFAR10 với batch size 128, trong 600 epoch, learning rate 0.01, sử dụng auto-augmentation, optimizer AdamW, momentum đặt thành 0.9, và cosine annealing. Chúng tôi sử dụng ba seed ngẫu nhiên khác nhau cho mỗi lần huấn luyện.

Hình 2(b) cho thấy CKA (cho một seed cụ thể) của MLP-Mixer với S=C= 64 và MLP với độ thưa 1/64, dựa trên kết quả từ Hình 2(a). Tuy nhiên, các đặc trưng được nhắm mục tiêu cho CKA được lấy từ lớp ngay trước skip-connection trong mỗi khối, và chúng được hiển thị trên trục theo thứ tự gần với đầu vào, được ghi nhãn từ 1 đến 6. Hình 2(c) tương tự so sánh MLP dày đặc (tức là độ thưa = 1).

Hình 2(d) cho thấy lỗi kiểm tra của MLP nông với trọng số ma trận monarch và ma trận trọng số Kronecker. Việc huấn luyện sử dụng MNIST, với optimizer adamw với 200 epoch, và cosine annealing của learning rate với giá trị ban đầu 0.01. Bảng S.1 tóm tắt sự khác biệt của các mô hình nông giữa các mô hình được xử lý khác.

C.2. Hình 4
Trong Hình 4 (trái), chúng tôi so sánh MLP-Mixer và MLP dưới cùng thiết lập huấn luyện như trong Hình 2(a). Bây giờ, ở đây Ω = 2¹⁹ và γ= 2, với các giá trị C là 4,8,12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 64. Các giá trị S được xác định tương ứng với C trong khi giữ Ω cố định, dẫn đến các giá trị: 360, 252, 203, 173, 152, 136, 124, 113, 104, 96, 89, 83, 64. Chúng tôi sử dụng bốn seed ngẫu nhiên khác nhau cho mỗi lần huấn luyện.

Trong Hình 4 (phải), chúng tôi sử dụng cùng giá trị trên Ω, γ và C, S như Hình 4 (trái). Chúng tôi chạy 10 lần thử với các seed ngẫu nhiên khác nhau để vẽ các giá trị kỳ dị của ma trận trọng số thưa.

C.3. Bảng 1
CIFAR10, CIFAR100 Chúng tôi sử dụng GPU Tesla V100 đơn để tính toán thời gian chạy. Việc huấn luyện bằng AdamW với mini-batch size 128. Chúng tôi sử dụng float 32-bit. Thời gian chạy được tính trung bình trên 600 epoch. Thời gian chạy trung bình trên bốn seed ngẫu nhiên khác nhau.

Đối với các thí nghiệm của chúng tôi, chúng tôi sử dụng GPU Tesla V100, tích lũy khoảng 300 giờ GPU. Các mạng được huấn luyện trên các tập dữ liệu, hoặc CIFAR-10 hoặc CIFAR-100. Chúng tôi đặt L= 2 và γ= 4 cho Mixer-SS/8 và Mixer-SS-W. Đối với cấu hình Mixer-SS/8, các tham số được đặt là p= 8, S= 16, và C= 986. Trong thiết lập rộng hơn của chúng tôi (Mixer-SS-W), các tham số được định nghĩa là p= 4, S= 64, và C= 487. Việc huấn luyện được thực hiện qua 4000 epoch với cosine-annealing. Chúng tôi sử dụng optimizer AdamW và kết hợp các kỹ thuật auto-augmentation. Kích thước mini-batch được chọn là 1024, và các thí nghiệm được chạy với ba seed ngẫu nhiên khác biệt. Để tối ưu kết quả, chúng tôi thực hiện tìm kiếm siêu tham số cho learning rate ban đầu tốt nhất từ tập {0.04,0.05,0.06}. Tỷ lệ cung cấp độ chính xác cao nhất, khi tính trung bình trên các seed ngẫu nhiên, sau đó được chọn cho các thí nghiệm. Cụ thể, learning rate 0.06 được chọn cho Mixer-SS/8 trên CIFAR-100, trong khi 0.04 là tỷ lệ ưa thích cho tất cả các tình huống khác.

ImageNet-1k Đối với Mixer-B-W, chúng tôi đặt L= 8, p= 14, C= 588, S= 256, và γ= 4.57. Các thiết lập khác giống với các thí nghiệm khác trên ImgeNet-1k (Phụ lục C.5).

C.4. Hình 5
Chúng tôi sử dụng GPU Tesla V100 và khoảng 400 giờ GPU cho thí nghiệm này. Chúng tôi huấn luyện ba loại MLP; kiến trúc S-Mixer, RP S-Mixer, và SW-MLP. Tất cả MLP đều kết hợp lớp FC per-patch như khối đầu tiên, với kích thước patch P= 4. Kích thước token đầu vào được cố định tại S₀= (32 /P)²= 64. Chúng tôi huấn luyện các mô hình trên tập dữ liệu CIFAR-10, CIFAR-100, và STL-10, cùng với các phép tăng cường dữ liệu như random cropping và random horizontal flipping. Ảnh đầu vào được thay đổi kích thước thành 32×32×3. Chúng tôi sử dụng Nesterov SGD với mini-batch size 128 và momentum 0.9 cho huấn luyện, chạy trong 200 epoch. Learning rate ban đầu được đặt thành 0.02, và chúng tôi sử dụng cosine annealing cho lập lịch learning rate. Để đảm bảo tính mạnh mẽ, chúng tôi thực hiện ba lần thử cho mỗi cấu hình và báo cáo trung bình và độ lệch chuẩn của kết quả. Trừ khi được chỉ định khác, các thiết lập này được sử dụng trong toàn bộ nghiên cứu về CIFAR-10, CIFAR-100, và STL-10.

(i) S-Mixer và RP S-Mixer Chúng tôi thực hiện các thí nghiệm huấn luyện trên kiến trúc S-Mixer với tám khối. Để khám phá các trường hợp khác nhau của các cặp số nguyên (S, C) gần như thỏa mãn phương trình

Ω =(CS²+SC²)/2. (S.42)

Số lượng kết nối, ký hiệu là Ω, được cố định tại Ω = 2¹⁸, 2²¹ và 2²⁷. Đối với mỗi giá trị của Ω, các cặp (C, S) được chọn theo cách đối xứng. Cần lưu ý rằng nếu (C, S) = (a, b) là một nghiệm, thì (C, S) = (b, a) cũng là một nghiệm. Các cặp được chọn cho mỗi giá trị Ω như sau:

• Ω = 2¹⁸: (C, S) = (16,173), (32,113), (48,83), (83,48), (113,32), (173,16).
• Ω = 2²¹: (C, S) = (16,504), (32,346), (64,226), (226,64), (346,32), (504,16).
• Ω = 2²⁷: (C, S) = (128,1386), (256,904), (904,256), (1386,128).

(ii) SW-MLP. Để so sánh công bằng giữa Mixer và SW-MLP, chúng tôi đặt lớp đầu tiên của cả hai mô hình thành cùng cấu trúc FC per-patch. Chúng tôi huấn luyện SW-MLP với tám khối, trong đó các lớp ẩn của các MLP này chia sẻ mẫu kết nối Ω = 2¹⁸ chung. Sau lớp kết nối đầy đủ per-patch, ma trận đặc trưng được vector hóa và xử lý như MLP chuẩn với kết nối thưa được che.

Đối với mỗi tỷ lệ đóng băng 1−p, chúng tôi xác định độ rộng m của các đơn vị ẩn bằng phương trình:

Ω =m²p, m =√(Ω/p). (S.43)

Chúng tôi đặt 1−p= 0.1,0.3,0.5,0.7,0.9, tương ứng với m= 540,612,724,935,1619.

C.5. Hình 6
Chúng tôi đặt số lượng khối L và các siêu tham số khác như trong Bảng S.2.

[Bảng S.2. Siêu tham số của các mô hình cho Mixer bình thường/RP trong Hình 6.]

CIFAR-10, 100, STL-10. Chúng tôi sử dụng GPU Tesla V100 và khoảng 200 giờ GPU cho các thí nghiệm của chúng tôi. Chúng tôi sử dụng một GPU duy nhất cho mỗi lần chạy. Chúng tôi đặt Ω = 2¹⁸ và sử dụng cùng các cặp (S, C) thỏa mãn phương trình S.42 như Mục C.4. Chúng tôi đặt learning rate ban đầu thành 0.1. Trên CIFAR-10, chúng tôi huấn luyện các mô hình 200 epoch, 600-epoch trên CIFAR-100, và chúng tôi thực hiện năm lần thử với seed ngẫu nhiên. Chúng tôi huấn luyện các mô hình 2000-epoch trên STL-10 với ba seed ngẫu nhiên.

ImageNet-1k. Chúng tôi sử dụng GPU Tesla V100 và khoảng 4000 giờ GPU cho các thí nghiệm của chúng tôi. Để huấn luyện MLP-Mixer và RP MLP-Mixer trên ImageNet-1k; chúng tôi sử dụng cụm GPU gồm 32 nút của 4 GPU mỗi nút cho mỗi lần chạy. Chúng tôi đặt hệ số mở rộng γ= 4 cho cả token-mixing MLP và channel-mixing MLP. Chúng tôi đặt Ω = 290217984 = (768²·196 + 768 ·196²)γ/2 trên baseline P= 16, (S, C) = (196,768). Chúng tôi quét P= 7,8,14,16,28,32 và đặt C= 3P² và đặt S sao cho nó gần như thỏa mãn phương trình

Ω =γ(CS²+SC²)/2. (S.44)

Đối với mỗi thiết lập, chúng tôi thực hiện ba lần thử với seed ngẫu nhiên. Việc huấn luyện trên ImageNet-1k dựa trên thư viện timm (Wightman, 2019). Chúng tôi sử dụng AdamW với learning rate ban đầu 10⁻³ và 300 epoch. Chúng tôi đặt mini-batch size thành 4096 và sử dụng huấn luyện data-parallel với batch size 32 trong mỗi GPU. Chúng tôi sử dụng warm-up với warm-up learning rate 10⁻⁶ và warm-up epoch 5. Chúng tôi sử dụng cosine annealing của learning rate với learning rate tối thiểu 10⁻⁵. Chúng tôi sử dụng weight-decay 0.05. Chúng tôi áp dụng random erasing trong ảnh với tỷ lệ 0.25. Chúng tôi cũng áp dụng random auto-augmentation với policy rand-m9-mstd0.5-inc1. Chúng tôi sử dụng mix-up với α= 0.8 và cut-mix với α= 1.0 bằng cách chuyển đổi chúng với xác suất 0.5. Chúng tôi sử dụng label smoothing với ε= 0.1.

C.6. Hình 7
Đối với các thí nghiệm của chúng tôi trong Hình 7, chúng tôi sử dụng GPU Tesla V100, với khoảng 70 giờ GPU được sử dụng. Chúng tôi huấn luyện cả mô hình S-Mixer và RP S-Mixer trên tập dữ liệu CIFAR-10, CIFAR-100, và STL-10. Chúng tôi xem xét số lượng khối khác nhau, cụ thể là L= 4,8,12,16,20. Các giá trị của S và C được cố định tại 128. Mỗi cấu hình được đánh giá bằng ba lần thử với seed ngẫu nhiên khác nhau.

D. Thí nghiệm Bổ sung
D.1. Khả năng huấn luyện của trọng số rất thưa
[Hình S.1. Về khả năng huấn luyện của SW-MLP. (Trái) Khả năng huấn luyện giảm khi độ thưa 1−p trở nên quá cao. Chúng tôi đặt γ theo (Golubeva et al., 2021) dưới Ω = 2¹⁶ cố định. Chúng tôi thực hiện năm lần thử cho mỗi p với seed ngẫu nhiên. (Giữa) Phân phối giá trị kỳ dị của trọng số thưa tại khởi tạo ngẫu nhiên. Chúng tôi đặt a= 1.5, Ω = 10³ và thực hiện 50 lần thử. (Phải) Giá trị riêng lớn nhất tăng đơn điệu khi độ thưa tăng.]

Golubeva et al. (2021) thấy rằng khi độ thưa (độ rộng) tăng đến một mức độ nào đó, hiệu suất tổng quát hóa được cải thiện. Họ cũng báo cáo rằng nếu độ thưa trở nên quá cao, hiệu suất tổng quát hóa hơi giảm. Họ thảo luận rằng sự giảm này được gây ra bởi sự suy giảm của khả năng huấn luyện, tức là nó trở nên khó khăn cho gradient descent để giảm hàm loss. Thực tế, chúng tôi xác nhận sự giảm hiệu suất của họ trong SW-MLP như được hiển thị trong Hình S.1 (trái). Ngược lại, chúng tôi hầu như không quan sát thấy sự giảm hiệu suất như vậy đối với các Mixer. Điều này có vẻ hợp lý vì chúng ta có thể lấy một độ thưa tùy ý nhỏ 1−p cho SW-MLP trong khi nó bị giới hạn dưới đối với Mixer như được mô tả trong Mục 4.

Như một ghi chú phụ, chúng tôi đưa ra ở đây cái nhìn sâu sắc định lượng về khả năng huấn luyện từ góc độ giá trị kỳ dị của ma trận trọng số. Một số công trình trước đây báo cáo rằng các giá trị kỳ dị lớn của ma trận trọng số tại khởi tạo ngẫu nhiên gây ra sự suy giảm của khả năng huấn luyện trong mạng nơ-ron sâu (Bjorck et al., 2018). Theo dòng nghiên cứu này, hãy xem xét một trọng số ngẫu nhiên của SW-MLP. Đặt độ rộng bởi m= Ω^{(1+a)/2} và độ thưa bởi p= 1/Ω^a với hằng số a >0 và lấy giới hạn Ω lớn. Chúng tôi sử dụng phép tỷ lệ này vì quan tâm của chúng tôi là trong trường hợp số lượng trọng số dự kiến, tức là m²p= Ω, độc lập với phép tỷ lệ của p. Chúng tôi tạo Z=M⊙W trong đó W_{ij}∼ N(0,1) và M là ma trận mặt nạ tĩnh có các mục được cho bởi phân phối Bernoulli với xác suất p. Giá trị kỳ dị của ma trận trọng số Z tương đương với căn bậc hai của giá trị riêng của Q=ZZ^T. Bởi vì chúng tôi không quan tâm đến một hệ số tỷ lệ tầm thường, chúng tôi tỷ lệ ma trận Q là Q/c trong đó c biểu thị trung bình trên các mục đường chéo của Q. Điều này làm cho trace của Q, tức là tổng (hoặc trung bình) của các giá trị riêng, một hằng số độc lập với a. Chúng tôi tính toán các giá trị riêng của Q và thu được các giá trị kỳ dị của Z ký hiệu bởi λ.

Như được hiển thị trong Hình S.1 (giữa), phổ của các giá trị kỳ dị đạt đỉnh xung quanh không nhưng lan rộng lên đến cạnh của nó. Hình S.1 (phải) chứng minh rằng giá trị kỳ dị lớn nhất trở nên đơn điệu lớn đối với sự tăng của a. Bởi vì giá trị kỳ dị lớn hơn ngụ ý khả năng huấn luyện thấp hơn (Bjorck et al., 2018), điều này phù hợp với quan sát thực nghiệm của (Golubeva et al., 2021) và Hình S.1 (trái) của chúng tôi.

Ngược lại, các Mixer không có khả năng gặp phải các giá trị kỳ dị lớn như sau. Giả sử S-Mixer với S=C≫1 để đơn giản. Sau đó, mỗi lớp của MLP hiệu quả có p= 1/C tương ứng với chỉ số tỷ lệ a= 1/3 trong SW-MLP. Thực tế, giá trị kỳ dị của nó trở nên tốt hơn so với a= 1/3, bởi vì các ma trận trọng số của Mixer bình thường và RP có cấu trúc: Xem xét các giá trị kỳ dị của Z=J₂(I_C⊗W)J₁ với ma trận Gaussian ngẫu nhiên C×C W và ma trận hoán vị (J₁, J₂). Sau đó, các giá trị kỳ dị của Z tương đương với của W, loại trừ trùng lặp. Do đó, các giá trị kỳ dị của Mixer chỉ được xác định bởi ma trận trọng số dày đặc W. Định nghĩa Q=WW^T. Bởi vì ma trận chuẩn hóa Q/c tuân theo luật Marchenko-Pastur trong lý thuyết ma trận ngẫu nhiên và giá trị riêng lớn nhất của nó được cho bởi 4 trong giới hạn C vô hạn (Bai & Silverstein, 2010). Điều này có nghĩa là giá trị kỳ dị lớn nhất của W chuẩn hóa là 2 và tương ứng với a= 0 của SW-MLP (tức là MLP dày đặc) với giới hạn Ω vô hạn trong Hình S.1 (phải). Do đó, chúng ta có thể nói rằng từ góc độ trọng số ngẫu nhiên, khả năng huấn luyện của Mixer dự kiến tốt hơn so với SW-MLP.

Chúng ta cũng có thể mở rộng phân tích của chúng tôi cho các mô hình kết hợp hệ số mở rộng γ. Đối với SW-MLP với các khối MLP, số lượng trọng số dự kiến được cho bởi Ω =γpm². Chúng ta chỉ cần thay thế p trong trường hợp S-Mixer thành γp và sự tăng đơn điệu của giá trị kỳ dị lớn nhất xuất hiện tương tự. Đối với MLP-Mixer, W chuẩn hóa của nó là ma trận γC×C. Theo luật Marchenko-Pastur cho ma trận ngẫu nhiên hình chữ nhật, khi C→ ∞, giá trị kỳ dị lớn nhất tiến tới giá trị hằng số 1 +√γ. Điều này tương ứng với giá trị kỳ dị của a= 0 trong SW-MLP tương ứng, và kết quả tương tự như trong S-Mixer.

Lưu ý rằng độ rộng hiệu quả của các lớp mixing đủ lớn nhưng vẫn có giới hạn trên (15). Nó thỏa mãn

(√(1 + 8Ω /γ)−1)/2≤m≤(Ω/γ)^{2/3}, (S.45)

trong đó đẳng thức của giới hạn dưới giữ cho S= 1 hoặc C= 1. Ngược lại, đối với SW-MLP, chúng ta không có giới hạn trên và chỉ có giới hạn dưới √Ω≤m, trong đó đẳng thức này giữ cho lớp dày đặc. Chúng ta có thể xem xét một p tùy ý nhỏ và một m lớn cho Ω cố định nếu chúng ta bỏ qua vấn đề về bộ nhớ (Golubeva et al., 2021). Golubeva et al. (2021) báo cáo rằng p cực nhỏ có thể gây ra sự giảm độ chính xác kiểm tra do sự suy giảm của khả năng huấn luyện. Chúng tôi quan sát sự suy giảm tương tự đối với SW-MLP, như được hiển thị trong mục này, nhưng không đối với Mixer. Điều này được mong đợi vì m bị giới hạn trên trong Mixer và khả năng huấn luyện ít bị tổn hại hơn so với SW-MLP với độ thưa cao.

D.2. MLP-Mixer với độ rộng tăng
Hình S.2 cho thấy độ chính xác kiểm tra cải thiện khi độ rộng tăng trên các mô hình SW-MLP và MLP-Mixer bình thường và RP ngay cả khi hệ số mở rộng γ được kết hợp trong các mô hình. Chúng tôi đặt hệ số mở rộng γ= 4. Chúng tôi đặt learning rate ban đầu thành 0.1. Đối với MLP-Mixer bình thường và RP, chúng tôi đặt C= 16,32,48,64,83,113,173 và xác định S bằng tổ hợp của C và Ω = 2¹⁸, 2²⁰. Đối với SW-MLP, chúng tôi đặt p= 0.1,0.3,0.5,0.7,0.9 và đặt độ rộng bởi m=√(Ω/γp).

D.3. Tăng hệ số mở rộng
Biểu thức MLP hiệu quả của MLP-Mixer (S.21) có hai độ rộng: m=SC và γSC. Vì cả hai đều tỷ lệ với SC, chúng tôi đã tập trung vào việc thay đổi SC và cố định γ cho đến nay. Ở đây, chúng tôi xem xét một thiết lập bổ sung, tức là thay đổi γ với m=SC cố định. Bằng cách thay thế S=m/C vào (14), chúng ta có γ= 2Ω /(m(C+m/C )). Tương tự như m của C được hiển thị trong Hình 3, γ này là một hàm đỉnh đơn của C và đạt tối đa của nó như

C*=S*=√m, max_{S,C} γ= Ω/(m√m). (S.46)

Hình S.3 (trái) xác nhận rằng tăng độ rộng ( γ) dẫn đến cải thiện hiệu suất như mong đợi từ Giả thuyết 4.1. Chúng tôi huấn luyện MLP-Mixer bình thường và RP với γ khác nhau trong một phạm vi thực tế. Chúng tôi vẽ một số trường hợp của Ω cố định dưới cùng m. Hình S.3 (phải) cho thấy độ chính xác kiểm tra được tối đa hóa xung quanh C=S như mong đợi từ (S.46).

D.4. Phụ thuộc vào độ sâu
Như được hiển thị trong Hình 5-S.3, cả Mixer bình thường và RP đều thể hiện xu hướng tương tự cho độ sâu cố định. Hình 7 xác nhận rằng bằng cách tăng độ sâu, tức là số lượng khối, RP S-Mixer thậm chí có thể trở nên tương đương với những cái bình thường hoặc tốt hơn chúng trong một số trường hợp. Đầu tiên, chúng tôi quan sát rằng, khi độ sâu bị hạn chế, RP Mixer kém hơn Mixer bình thường trong hầu hết các trường hợp. Khi chúng tôi tăng độ sâu, chúng tôi quan sát rằng trong một số trường hợp, overfitting xảy ra đối với Mixer bình thường, nhưng không đối với RP (xem thêm các loss huấn luyện và kiểm tra được hiển thị trong Mục D.6). Trong những trường hợp như vậy, kết quả của RP Mixer có thể so sánh được (trong Hình 7(trái, phải)) hoặc tốt hơn (trong Hình 7(giữa)). Mặc dù RP Mixer không nhất thiết tốt hơn những cái bình thường, thật thú vị là ngay cả RP Mixer được định nghĩa bởi cấu trúc ngẫu nhiên có thể cạnh tranh với Mixer bình thường.

Hoán vị ngẫu nhiên trong RP-Mixer ban đầu chọn và cố định kích thước của trường nhận thức (tức là số lượng pixel đầu vào chảy vào một nơ-ron trong mỗi lớp trên thông qua trọng số) với tỷ lệ 1/√m. Do đó, việc huấn luyện trở nên khó khăn trong các mạng nông. Tuy nhiên, trong RP-Mixer, vì một hoán vị ngẫu nhiên độc lập được chọn cho mỗi lớp, trường nhận thức mở rộng khi số lượng lớp tăng. Từ đây, hiệu suất của RP-Mixer cải thiện khi các lớp được thêm vào. MLP-Mixer bình thường hoạt động tốt ngay cả với ít lớp hơn vì trường nhận thức và cấu trúc đường chéo khối được căn chỉnh.

D.5. Thay thế J_c trong các lớp cụ thể
Hình S.4 cung cấp cái nhìn sâu sắc chi tiết hơn về trường hợp độ sâu bị hạn chế và RP Mixer hoạt động tệ hơn so với Mixer bình thường. Chúng tôi điều tra các S-Mixer đặc biệt có khối thứ l được thay thế bằng đối tác RP trong khi các lớp khác vẫn giữ nguyên. Thú vị, khi sự suy giảm độ chính xác xuất hiện rõ ràng (ví dụ: trường hợp của CIFAR-10 và STL-10 trong Hình 7), sự suy giảm này được quy cho khối đầu tiên. Điều này có vẻ hợp lý vì các patch ảnh lân cận có khả năng tương quan, điều này làm cho các đơn vị đầu vào cho token mixing đầu tiên tương quan. Mặc dù trọng số mixing thông thường có thể phản ánh các cấu trúc lân cận như vậy, RP Mixer chọn ngẫu nhiên các token và có thể mất cấu trúc lân cận cụ thể cho ảnh. Tuy nhiên, khi độ sâu tăng, các lớp token mixing có thể hợp nhất tất cả các token, điều này phù hợp với sự tăng độ chính xác của RP Mixer, như được xác nhận trong Hình 7. Do đó, chúng tôi kết luận rằng RP và mixing bình thường có gần như cùng bias quy nạp, đặc biệt là trong các lớp thượng lưu.

Chúng tôi sử dụng GPU Tesla V100 và khoảng 10 giờ GPU cho các thí nghiệm của chúng tôi. Xem xét kiến trúc S-Mixer bao gồm bốn khối và S=C= 64. Trong nghiên cứu này, chúng tôi huấn luyện phiên bản sửa đổi của kiến trúc S-Mixer bằng cách thay thế một trong bốn khối bằng khối kết hợp hoán vị ngẫu nhiên. Việc huấn luyện được thực hiện trên tập dữ liệu CIFAR-10, CIFAR-100, và STL-10. Optimizer và thiết lập huấn luyện được sử dụng trong thí nghiệm này phù hợp với những cái được mô tả trong Mục C.6.

D.6. Nhận xét về kích thước patch đầu vào
Trong nghiên cứu này, chúng tôi tập trung vào việc thay đổi kích thước của các lớp mixing và cố định kích thước token và kênh đầu vào ( S₀, C₀). Nói cách khác, kích thước patch P, thỏa mãn C₀= 3P², được cố định. Trong khi chúng tôi quan sát rằng kết quả thí nghiệm của chúng tôi giữ nguyên bất kể kích thước patch, một câu hỏi naïve là liệu có kích thước patch tối ưu nào để đạt được độ chính xác cao nhất hay không. Mặc dù điều này vượt ra ngoài phạm vi của nghiên cứu này, chúng tôi hiển thị hiệu suất phụ thuộc vào C₀ trong Hình S.5 như một ghi chú phụ. Số lượng lớp mixing được cố định tại C=S= 64. Chúng tôi quan sát rằng C₀ tối ưu phụ thuộc vào dữ liệu; C₀= 48 ( S₀= 64) cho CIFAR-10 và 100, và C₀= 108 ( S₀= 225) cho STL-10. Lưu ý rằng chiều của ảnh đầu vào là S₀C₀= 3,072 cho tập dữ liệu CIFAR và 24,300 cho STL-10. Sẽ hợp lý rằng kích thước patch tối ưu phụ thuộc vào thông tin chi tiết của dữ liệu.

Chúng tôi sử dụng GPU Tesla V100 và khoảng 30 giờ GPU cho các thí nghiệm của chúng tôi. Chúng tôi thực hiện các thí nghiệm huấn luyện trên tập dữ liệu CIFAR-10, CIFAR-100, và STL-10 sử dụng cả kiến trúc S-Mixer và RP S-Mixer với S=C= 64, cùng với bốn khối. Để tối ưu hóa, chúng tôi đặt learning rate ban đầu thành 0.1.

Đối với CIFAR-10 và CIFAR-100, chúng tôi huấn luyện các mô hình trong 200 epoch và đánh giá chúng với các kích thước patch khác nhau (P= 1,2,4,8,16,32). Chúng tôi thực hiện ba lần thử với seed ngẫu nhiên khác nhau cho mỗi thiết lập. Trên tập dữ liệu STL-10, chúng tôi thay đổi kích thước ảnh thành 90×90×3 và huấn luyện các mô hình trong 400 epoch. Chúng tôi thay đổi kích thước patch ( P= 1,3,6,10,18,30) và thực hiện năm lần thử với seed ngẫu nhiên khác nhau cho mỗi thiết lập.

D.7. RP có thể ngăn ngừa overfitting
Trong Hình S.6, chúng tôi khám phá một số giá trị của C cố định Ω, mô hình bình thường cho thấy overfitting hơn gấp đôi so với mô hình RP, đặc biệt là C lớn nhất trong phạm vi khám phá. Trong trường hợp này, S có giá trị nhỏ nhất trong số các giá trị được khám phá. Điều này gợi ý rằng RP có hiệu ứng regularization vượt ra ngoài phía token-mixing và ảnh hưởng đến phía channel-mixing, đặc biệt là khi C lớn.

Để giảm thiểu overfitting, phép tăng cường bổ sung (auto-augmentation, dựa trên (Cubuk et al., 2019)) được áp dụng cho tập dữ liệu, và mô hình được chuyển từ S-Mixer sang MLP-Mixer ( γ= 4) do sự giảm chậm hơn được quan sát trong loss huấn luyện đối với S-Mixer trong Hình S.7. RP S-Mixer vượt trội hơn S-Mixer về loss kiểm tra đối với C= 173, chỉ ra rằng RP vẫn cung cấp ngăn ngừa overfitting ngay cả với các phép tăng cường dữ liệu tương đối mạnh.

[Hình S.6. (Trái) Các đường cong loss kiểm tra trung bình được hiển thị cho C= 16,32,64,114,173 và năm lần thử với seed ngẫu nhiên khác nhau. Các mô hình được sử dụng trong thí nghiệm này là S-Mixer Bình thường và RP được huấn luyện trên CIFAR-10 với L= 8 trong 600 epoch. Learning rate ban đầu được đặt thành 0.1. (Phải) Đường cong loss kiểm tra cho C= 173 đại diện cho trường hợp overfitting tồi tệ nhất. Vùng tô màu trong cả hai hình đại diện cho phạm vi giữa các giá trị tối đa và tối thiểu.]

Hình S.7 (phải) minh họa rằng RP không đạt được loss huấn luyện tương đối nhỏ như mô hình bình thường. Để giải quyết điều này, SGD được thay thế bằng AdamW làm optimizer, với learning rate ban đầu giảm (lr = 0.01) do sự bất ổn được quan sát với lr = 0.1 trong Hình S.8. Điều này dẫn đến việc giảm overfitting trong vùng C > S, và RP hoạt động đặc biệt tốt so với mô hình bình thường đối với C=S= 64. Trong Hình S.8, không mô hình bình thường nào và RP thể hiện sự tăng đáng kể trong loss kiểm tra đối với C=S. Tuy nhiên, trong khi loss kiểm tra của mô hình bình thường ổn định, mô hình RP tiếp tục giảm loss kiểm tra của nó, cuối cùng vượt trội hơn mô hình bình thường về độ chính xác kiểm tra. Điều này nêu bật tiềm năng của RP vượt trội hơn mô hình bình thường với sự lựa chọn tối ưu hóa như AdamW.

D.8. Hình ảnh hóa cách hoán vị ngẫu nhiên phá vỡ cấu trúc
Quan sát sự khác biệt giữa RP-Mixer, MLP-Mixer, và SW-MLP từ góc độ trọng số. Hình S.9 cho thấy ba loại mô hình trong đó các thành phần chỉ có thể là 0 hoặc 1. Trong RP-Mixer, rõ ràng là cấu trúc khối của MLP-Mixer đã bị phá vỡ.

D.9. Chi phí Tính toán
Bảng S.3 cho thấy tài nguyên tính toán của SW-MLP và RP-Mixer. Trong thiết lập cho ImageNet, SW-MLP yêu cầu bộ nhớ và thời gian chạy lớn, trong khi RP-Mixer cần ít hơn 10³ đến 10⁶ lần. Lưu ý rằng độ phức tạp không gian của SW-MLP (tương ứng. RP-Mixer) là O(m²) =O(S²C²)(tương ứng. O(C²+S²)) khi S, C→ ∞. Do đó, RP-Mixer hiệu quả hơn về bộ nhớ và đòi hỏi tính toán ít hơn so với SW-MLP.

[Bảng S.3. So sánh về yêu cầu bộ nhớ, FLOP(floating point operations), và thời gian chạy trung bình. Đối với ba mô hình, chúng tôi đặt S= 256, C= 588, L= 8, γ= 4 cho ImageNet và S= 64, C= 48, L= 3, γ= 2 cho CIFAR.]

[Hình S.7. (Trái) Trung bình của các đường cong loss kiểm tra. (Phải) Loss huấn luyện trung bình với C= 173. Trong cả hai hình, vùng hiển thị các giá trị max và min. Chúng tôi huấn luyện MLP-Mixer bình thường và RP với γ= 4 với learning rate ban đầu 0.1 và mini-batch size 256 trong 600 epoch. Kết quả là trung bình của năm lần thử. Vùng tô màu trong hình đại diện cho phạm vi giá trị giữa các giá trị tối đa và tối thiểu.]

[Hình S.8. Kết quả của huấn luyện với AdamW và auto-augmentation với S=C= 64. RP MLP-Mixer vượt trội kết quả của cái bình thường trong loss kiểm tra và độ chính xác kiểm tra. (Trái) Trung bình của các đường cong loss kiểm tra. (Phải) Loss huấn luyện trung bình. (Dưới) Đường cong Độ chính xác Kiểm tra. Chúng tôi đặt learning rate ban đầu thành 0.01 với mini-batch size 256 và 600 epoch. Trong tất cả các hình, kết quả là trung bình của năm lần thử và vùng hiển thị phạm vi giữa các giá trị tối đa và tối thiểu.]

[Hình S.9. Mẫu của các ma trận trọng số tại khởi tạo. (Trái) M⊙A, (giữa) I_C⊗W, (phải) J₁(I_C⊗W)J₂, C=S= 8, m= CS, p = 1/S, và J₁ và J₂ là các ma trận hoán vị ngẫu nhiên độc lập. Mỗi mẫu là một thực hiện của một lần thử.]
