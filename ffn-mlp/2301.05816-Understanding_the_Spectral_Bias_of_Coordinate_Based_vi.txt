# 2301.05816.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/ffn-mlp/2301.05816.pdf
# Kích thước tệp: 4013306 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Hiểu về Thiên Lệch Phổ của MLPs Dựa trên Tọa độ 
Thông qua Động lực học Huấn luyện
John Lazzaria;*và Xiuwen Liub
aĐại học Bang Florida, Khoa Toán học
bĐại học Bang Florida, Khoa Khoa học Máy tính
ORCiD ID: John Lazzari https://orcid.org/0000-0002-2276-3741
Tóm tắt.
Thiên lệch phổ là một quan sát quan trọng về huấn luyện mạng nơ-ron,
khẳng định rằng mạng sẽ học một biểu diễn tần số thấp của
hàm mục tiêu trước khi hội tụ đến các thành phần tần số cao hơn.
Tính chất này thú vị do liên kết của nó với khả năng khái quát hóa tốt trong
các mạng over-parameterized. Tuy nhiên, trong các thiết lập chiều thấp,
một thiên lệch phổ nghiêm trọng xảy ra cản trở sự hội tụ đến các thành
phần tần số cao hoàn toàn. Để vượt qua hạn chế này, có thể
mã hóa các đầu vào bằng cách sử dụng mã hóa sin tần số cao.
Các công trình trước đây đã cố gắng giải thích hiện tượng này bằng cách sử dụng
Neural Tangent Kernel (NTK) và phân tích Fourier. Tuy nhiên, NTK không
nắm bắt được động lực học mạng thực, và phân tích Fourier chỉ cung cấp
một góc nhìn toàn cục về các tính chất mạng gây ra thiên lệch này.
Trong bài báo này, chúng tôi cung cấp một phương pháp tiếp cận mới để hiểu
thiên lệch phổ bằng cách trực tiếp nghiên cứu động lực học huấn luyện ReLU MLP.
Cụ thể, chúng tôi tập trung vào kết nối giữa các tính toán của
mạng ReLU (vùng kích hoạt), và tốc độ hội tụ của gradient descent.
Chúng tôi nghiên cứu các động lực học này liên quan đến thông tin không gian
của tín hiệu để hiểu cách chúng ảnh hưởng đến thiên lệch phổ.
Sau đó chúng tôi sử dụng công thức này để nghiên cứu mức độ nghiêm trọng của thiên lệch phổ trong
các thiết lập chiều thấp, và cách mã hóa vị trí vượt qua
điều này.

1 Giới thiệu
Trong vài năm gần đây, mạng nơ-ron đã ngày càng được
sử dụng để cung cấp một cách học các biểu diễn khái quát hóa
tốt trong các miền dày đặc, chiều thấp. Cụ thể trong lĩnh vực
đồ họa máy tính, các perceptron đa lớp (MLPs) với kích hoạt ReLU
đã rất quan trọng cho các ứng dụng như trường bức xạ nơ-ron
(NeRF) [15] và chiếm chỗ hình dạng [14]. Những mạng này được
gọi là MLPs dựa trên tọa độ, vì chúng nhập các tọa độ dày đặc, chiều thấp
và hồi quy biểu diễn tương ứng
của màu sắc, hình dạng, hoặc mật độ cho các tín hiệu hình ảnh khác nhau. Để đơn giản,
chúng tôi định nghĩa MLP dựa trên tọa độ như một ReLU MLP tiêu chuẩn có
nhiệm vụ hồi quy bất kỳ tín hiệu chiều thấp nào (ví dụ: sinusoid 1D, hình ảnh 2D,
hoặc cảnh 3D), trong đó các đầu vào v∈Rd (thường d≤3) được
gọi là tọa độ. MLPs dựa trên tọa độ hấp dẫn do
thiên lệch phổ nghiêm trọng của chúng, có nghĩa là chúng thực tế không thể
học các tín hiệu mục tiêu tần số cao [15]. Hạn chế này có thể được
vượt qua thông qua mã hóa vị trí γ(v)∈R2dL của tọa độ
bao gồm các sinusoid tần số cao:
Tác giả Liên hệ. Email: jcl19h@fsu.edu.

(a) Tọa độ
(b) Mã hóa L=4
(c) Mã hóa L=8
Hình 1: Trực quan hóa thiên lệch phổ lớn được gây ra bởi tọa độ 2D
chiều thấp dày đặc (a), được khắc phục bởi mã hóa vị trí tần số cao
(b-c). Đối với nhiệm vụ này, MLP dựa trên tọa độ được giao nhiệm vụ hồi quy
giá trị RGB tương ứng với vị trí pixel 2D, hoặc mã hóa sinusoidal của nó.
Các đầu vào dựa trên tọa độ gặp khó khăn trong việc hội tụ đến các thành phần tần số cao
của hình ảnh, và chỉ có thể tạo ra biểu diễn mượt mà, tần số thấp.

γ(v) = [sin(2⁰v), cos(2⁰v), ..., sin(2^L v), cos(2^L v)],

trong đó L∈N xác định tần số tối đa cũng như chiều mã hóa [25,22].
Hành vi này đặt ra hai câu hỏi quan trọng:
Những tính chất nào của động lực học huấn luyện mạng nơ-ron gây ra thiên lệch phổ,
và tại sao nó trở nên nghiêm trọng trong các thiết lập dày đặc, chiều thấp?

Thiên lệch phổ là hành vi sao cho mạng nơ-ron học một
biểu diễn đơn giản hơn, tần số thấp hơn của hàm mục tiêu trước khi
hội tụ đến các thành phần tần số cao, hoặc chi tiết tinh vi hơn (xem
Hình 1 để có ví dụ). Hiện tượng này được quan tâm do
tác động của nó đến khả năng khái quát hóa, vì người ta tin rằng một chính quy hóa ngầm
thiên về các giải pháp tần số thấp hơn tránh overfitting
trong các mạng over-parameterized [21]. Trong khi thiên lệch phổ thường
được tin là giúp ích cho khả năng khái quát hóa, trong chế độ dựa trên tọa độ nó
trở nên nghiêm trọng đến mức mạng sẽ về cơ bản underfitting tín hiệu
mục tiêu.

Hiểu rõ hơn về bản chất của thiên lệch phổ sẽ rất quan trọng
trong việc xác định tác động của nó đến khả năng khái quát hóa. Cho đến nay, Neural Tangent
Kernel (NTK) [10] và phân tích Fourier là những công cụ chính
để phân tích thiên lệch phổ [25,21]. Trong khi NTK mô hình hóa động lực học gradient
descent thông qua phương pháp kernel (có ma trận kernel trở thành
một hằng số ở giới hạn các lớp rất rộng), các tính chất động
của mạng thực không được nắm bắt chính xác. Mặt khác, phân tích
Fourier của mạng ReLU bị giới hạn bởi tổng số
vùng tuyến tính cũng như hằng số Lipschitz của nó [21], điều này
cung cấp những hiểu biết về mạng thực. Tốc độ phân rã phổ của gradient
tham số cũng đã được tìm thấy. Tuy nhiên, điều này chỉ cung cấp một
góc nhìn toàn cục về các tính chất mạng, chủ yếu thông qua các cận trên,
do đó động lực học địa phương của mạng gây ra thiên lệch phổ
vẫn chưa được chứng minh. Ví dụ, trong khi tổng số
vùng tuyến tính trong mạng cho một thước đo khả năng biểu đạt,
không nhất thiết mạng sẽ sử dụng hầu hết các vùng này,
đặc biệt nếu có mẫu dày đặc các đầu vào.

Trong bài báo này, chúng tôi thực hiện một phương pháp tiếp cận khác để hiểu
thiên lệch phổ. Những đóng góp của chúng tôi như sau:

Chúng tôi phát triển một khung mới để hiểu thiên lệch phổ
thông qua động lực học huấn luyện mạng thực, kết hợp các tính toán mạng ReLU,
hội tụ gradient descent, cũng như
thông tin không gian của tín hiệu. Cụ thể, chúng tôi liên kết
khả năng biểu đạt của mạng (vùng kích hoạt) với khả năng
tăng tốc hội tụ của gradient descent, thông qua một metric
được gọi là sự nhầm lẫn gradient [24]. Chúng tôi tìm thấy nhiều sự nhầm lẫn hơn (hội tụ chậm hơn)
khi khả năng biểu đạt giữa các đầu vào bị hạn chế,
và ít sự nhầm lẫn hơn (hội tụ nhanh hơn) khi khả năng biểu đạt được
tăng cường. Điều này dẫn đến hội tụ chậm hơn đến các chi tiết địa phương
của tín hiệu, nơi các đầu vào thường bị hạn chế trong cùng một hoặc
các vùng kích hoạt gần đó.

Chúng tôi sử dụng công thức này để khám phá mức độ nghiêm trọng của thiên lệch phổ trong
chế độ dựa trên tọa độ, và cách mã hóa vị trí vượt qua
điều này. Chúng tôi tìm thấy rằng mã hóa vị trí tăng cường rất nhiều khả năng biểu đạt
trên tín hiệu, dẫn đến hội tụ nhanh hơn
đến các thành phần tần số cao. Chúng tôi cung cấp thêm phân tích,
bằng cách nghiên cứu các tính chất độc đáo của các vùng kích hoạt khi
tần số mã hóa tăng, và phơi bày các ReLU chết
xảy ra trong thiết lập chiều thấp.

Bài báo được cấu trúc như sau. Phần 2 cung cấp các công trình liên quan.
Trong Phần 3, chúng tôi định nghĩa các vùng kích hoạt và khám phá cách mật độ
hạn chế khả năng biểu đạt của mạng. Phần 4 thảo luận về
sự nhầm lẫn gradient, và kết nối sự nhầm lẫn với các vùng kích hoạt
của mạng. Trong Phần 5, chúng tôi phân tích các tính chất của các vùng kích hoạt
được tạo ra bởi các mã hóa tần số cao hơn, và Phần 6 chứng minh
cách các tọa độ dày đặc tắt các nơ-ron ReLU trong quá trình huấn luyện.
Phần 7 là kết luận chi tiết các hướng tương lai cho phương pháp
này.

2 Các Công trình Liên quan
Trong [21,3], đã được chỉ ra rằng mạng nơ-ron có thiên lệch về
việc học các hàm tần số thấp trước, được gọi là thiên lệch phổ,
và rằng ánh xạ sinusoidal có thể cho phép các tần số cao hơn
được học nhanh hơn. Phương pháp này được NeRF [15] áp dụng để
tăng tốc hội tụ của các thành phần tần số cao trong quá trình tổng hợp góc nhìn mới,
vì một thiên lệch phổ lớn đã được phát hiện. Như một hệ quả
của điều này, mối quan hệ giữa tọa độ và mã hóa vị trí
đã được phân tích bởi cùng các tác giả sử dụng NTK [25]. Họ tìm thấy rằng
phổ eigenvalue của NTK phân rã nhanh chóng với tọa độ dày đặc,
và mở rộng cho mã hóa vị trí. Điều này cho phép hội tụ nhanh hơn
dọc theo các hướng của các hàm eigen tương ứng. Công trình của chúng tôi
có thể được coi là một mở rộng của [25], tuy nhiên chúng tôi tập trung vào
động lực học mạng thực.

Ngoài mức độ nghiêm trọng trong các thiết lập chiều thấp, bản chất của
thiên lệch phổ đã được nghiên cứu từ góc độ lý thuyết. [21]
cung cấp khám phá nghiêm ngặt đầu tiên về thiên lệch phổ sử dụng phân tích Fourier.
Trong [4], đã được chỉ ra cách các hàm eigen của NTK có
tốc độ hội tụ riêng được cho bởi eigenvalue tương ứng của chúng,
sử dụng các đầu vào có mật độ đều trên S¹. Các tác giả trong [3] cũng
sử dụng NTK để đưa ra tốc độ hội tụ cho các đầu vào có mật độ
không đều trong S¹. Đã có một dòng công trình gọi thiên lệch phổ
là F-principle [27], trong đó phân tích Fourier được sử dụng
cho các đầu vào chiều cao với kích hoạt mềm. Các công trình khác đã
cố gắng tính toán thiên lệch phổ và xác định tốc độ hội tụ [11,23].
Lưu ý rằng các công trình trước đây chủ yếu tập trung vào việc đạt được hiểu biết
từ các MLP thực hiện các nhiệm vụ dựa trên hồi quy.

Phân tích thiên lệch phổ của chúng tôi sử dụng các vùng kích hoạt/tuyến tính. Khám phá
độ phức tạp của các hàm có thể tính toán bởi các mạng tuyến tính từng phần
ban đầu được khám phá trong [19]. Phương pháp này được mở rộng trong
[16], nơi các cận trên và dưới cho số lượng tối đa các vùng
tuyến tính được đưa ra, và được chỉ ra là theo cấp số nhân với độ sâu. Trong [20],
các vùng kích hoạt được định nghĩa, và các cận được tính toán bằng cách sử dụng
các quỹ đạo đầu vào x(t) qua các vùng. Gần đây hơn, [8,9]
đã cố gắng tính toán các cận thực tế cho số lượng các vùng kích hoạt,
điều này được tìm thấy là độc lập với độ sâu. [29] cung cấp
các đại lượng thực nghiệm hữu ích để nghiên cứu các tính chất của các vùng tuyến tính
theo cách thực tế hơn.

Trong bài báo này, chúng tôi liên kết khả năng biểu đạt với sự tương quan của
các gradient trong quá trình huấn luyện. Chúng tôi tập trung cụ thể vào sự nhầm lẫn
gradient [24], được tìm thấy là cao hơn trong các mạng sâu hơn, làm cho
chúng khó huấn luyện hơn nếu độ rộng của chúng không tăng đồng thời.
Các công trình khác đã sử dụng tương quan giữa các gradient trong
các thiết lập khác nhau [2, 7, 28].

3 Khả năng Biểu đạt
3.1 Thiết lập Thực nghiệm
Chúng tôi bắt đầu nghiên cứu bằng cách khám phá các vùng kích hoạt, và cung cấp
chi tiết thực nghiệm. Đối với tất cả các kết quả trong bài báo này (trừ khi
nói khác), chúng tôi huấn luyện một MLP bốn lớp với 512 nơ-ron ẩn trên
nhiệm vụ hồi quy hình ảnh. Mạng học hồi quy các giá trị RGB
tương ứng với các vị trí pixel 2D (xi, yi) sử dụng hàm mất mát MSE.
Chúng tôi tính trung bình kết quả trên năm hình ảnh tự nhiên từ tập dữ liệu div2k [1]
(hình ảnh 512x512). Hồi quy các hình ảnh này đã là một benchmark thông thường
trong các công trình gần đây về mã hóa vị trí [25,6]. Kết quả
tính trung bình trên năm sinusoid 1D được hiển thị trong Phụ lục nơi
các phát hiện của chúng tôi được giữ. Chúng tôi sử dụng bộ tối ưu hóa Adam [12] với
tốc độ học .001, và mini-batch kích thước 8192. Các thí nghiệm được thực hiện trên
GPU NVIDIA A5000.

Ngoài chế độ dựa trên tọa độ, chúng tôi cũng tiến hành
cùng các thí nghiệm trên dữ liệu thế giới thực cho cả MLP và CNN
(chỉ Phần 4). Chúng tôi sử dụng MNIST [5], Fashion MNIST [26], CIFAR10 [13]
và CIFAR100 [13]. MLP có cùng kích thước, và
chi tiết thêm về kiến trúc CNN (chủ yếu theo [29])
và thiết lập thực nghiệm có thể được tìm thấy trong Phụ lục. Chúng tôi hiển thị
kết quả cho nhiệm vụ phân loại đơn giản để chứng minh rằng hành vi
tương tự như chế độ dựa trên tọa độ, nơi thiên lệch phổ
có thể được diễn giải dễ dàng. Tuy nhiên, thiên lệch phổ đã được
khám phá rất ít trong thiết lập phân loại (xem Các Công trình Liên quan).
Vì điều này, chúng tôi không đưa ra những tuyên bố mạnh mẽ về thiên lệch phổ trong
phân loại, vì cần phân tích sâu hơn. Thay vào đó, chúng tôi
thảo luận về kết quả của chúng tôi có thể có giá trị cho các công trình tương lai.

3.2 Các Vùng Kích hoạt
Chúng tôi phân tích khả năng biểu đạt của mạng nơ-ron với các đầu vào dày đặc và
đều thông qua các vùng kích hoạt của nó. Chúng tôi bắt đầu bằng cách định nghĩa

--- TRANG 3 ---
(a) Vùng Kích hoạt Tọa độ
(b) Vùng Kích hoạt Mã hóa
(c) Mất mát Huấn luyện
(d) Tăng trưởng Vùng Kích hoạt
Hình 2: Trực quan hóa các vùng mà mỗi đầu vào thuộc về (a-b) (được tô màu theo vùng duy nhất) ở đầu quá trình huấn luyện.
Nhiều tọa độ nằm trong cùng vùng kích hoạt, nhưng mã hóa vị trí cho phép mỗi đầu vào nằm trong một vùng kích hoạt duy nhất. Trong (c), chúng tôi
hiển thị mất mát huấn luyện của mỗi phương pháp, và trong (d) chúng tôi vẽ số lượng mẫu kích hoạt duy nhất được tạo ra khi huấn luyện tiến triển. Chúng tôi chỉ
tập trung vào các mẫu kích hoạt được sử dụng cho tập dữ liệu đã cho (512x512 tổng vùng), và chứng minh cách các đầu vào dựa trên tọa độ và mã hóa
tần số thấp không thể ánh xạ mỗi đầu vào đến một vùng kích hoạt duy nhất trong suốt quá trình huấn luyện.

(a) Tọa độ
(b) Mã hóa L=5
(c) Mã hóa L=16
Hình 3: Ma trận khoảng cách tọa độ (a) và khoảng cách mã hóa
(b-c) cho các tần số khác nhau. Mã hóa vị trí tạo ra một kernel
với đường chéo mạnh vì nó là tĩnh.

các vùng kích hoạt và mẫu. Cho f: R^nin → R^nout là một hàm
tuyến tính từng phần liên tục được cho bởi một mạng ReLU chứa
L lớp ẩn với N nơ-ron ẩn mỗi lớp. Mạng được định nghĩa
là hợp thành của các phép biến đổi affine fl(x) = Wlx + bl
với kích hoạt ReLU σ = max(0, fl) sao cho

f = fout ∘ σL ∘ ⋯ ∘ σ1(x),

trong đó θ biểu thị vector của các tham số có thể huấn luyện. Mỗi nơ-ron
z^l_i(x) = W^l_i x + b^l_i kết hợp với σ biểu thị một phương trình siêu phẳng
với vô hướng được xác định bởi bias. Tập hợp của các siêu phẳng này
trong lớp đầu tiên cho một sự sắp xếp siêu phẳng trong R^nin,
chia không gian đầu vào thành các phần tính toán các hàm tuyến tính
riêng biệt. Khi ảnh của mỗi hàm tuyến tính riêng biệt trong lớp
trước l-1 được phân vùng độc đáo bởi các nơ-ron tiếp theo z^l_i(x), các
siêu phẳng N-1 chiều trong các lớp l > 1 sẽ xuất hiện bị uốn cong.
Nhìn chung, điều này dẫn đến một mạng ReLU phân vùng không gian đầu vào
thành các polytope lồi trên đó các hàm tuyến tính duy nhất được tính toán.
Một cách để đánh giá các polytope này là thông qua các vùng kích hoạt,
được xác định bởi các mẫu kích hoạt của mạng, được định nghĩa như

Định nghĩa 1 (Mẫu Kích hoạt). Cho f là một mạng ReLU, và
z^l_i(x) biểu thị tiền kích hoạt của một nơ-ron trong lớp thứ l. Khi đó,
một mẫu kích hoạt A của mạng là một vector trong {0,1}^#neurons
bao gồm các kích hoạt nơ-ron sao cho 0 được gán nếu z^l_i(x) < 0
và 1 nếu z^l_i(x) > 0.

Định nghĩa 2 (Vùng Kích hoạt). Đối với một mạng ReLU f, các vùng kích hoạt
R của f là các tập hợp các mẫu đầu vào tương ứng
với cùng A,
R(f, A) = {x ∈ R^nin | 1_{R+}(ReLU(fl(x))) = a^l,
∀l ∈ {1, ..., k}, ∀a^l ∈ A} trong đó a^l là mẫu kích hoạt của lớp l.

Phân tích các vùng kích hoạt quan trọng vì chúng cung cấp một
thước đo khả năng biểu đạt của mạng, hoặc độ phức tạp
của các hàm nó có thể tính toán. Trong [9], một cận trên thực tế trên
số lượng tối đa các vùng kích hoạt được tạo ra bởi một kiến trúc mạng ReLU
được đưa ra là

#Regions ∩ F intersecting C ≤ vol(C)(TN)^nin / nin!     (1)

trong đó N là số lượng nơ-ron, T là một hằng số, và C là một khối lập phương trong
không gian đầu vào.

Phương trình (1) được xác định bởi số lượng nơ-ron và chiều
đầu vào. Do đó, chiều của chính các đầu vào
cho thấy rằng mạng với mã hóa vị trí nên có nhiều
khả năng biểu đạt hơn. Tuy nhiên, mật độ của việc lấy mẫu đầu vào cũng là
một yếu tố chính, do đó chúng tôi cũng phân tích cách mạng
sử dụng các vùng kích hoạt của nó trên tín hiệu, vì chỉ tập trung vào
tổng số vùng sẽ không cung cấp hiểu biết về
động lực học địa phương của mạng. Chúng tôi có thể định nghĩa tập hợp quan tâm trên một tập dữ liệu
D = {xi, yi}^n_{i=1}, được đưa ra dưới dạng các mẫu kích hoạt
tương ứng là

LD = {1_{R+}(ReLU(fl(x))) | x ∈ D, ∀l ∈ {1, ..., k}},

được trang bị với khoảng cách Hamming Σ^n_{i=1} |ai - bi|, cho a, b ∈ LD
để xác định tính khác biệt của chúng. Sau này chúng tôi sử dụng khoảng cách Hamming
như một đại diện để xác định khả năng biểu đạt giữa các đầu vào (khoảng cách
cao hơn, khả năng biểu đạt nhiều hơn).

3.3 So sánh
Trong Hình 2, chúng tôi có cái nhìn thoáng qua về các hạn chế được áp đặt bởi
các đầu vào dày đặc, chiều thấp. Số lượng vùng kích hoạt được sử dụng với
tọa độ thấp hơn số lượng phần tử trong tập dữ liệu (Hình
2 (a,b)), có nghĩa là nhiều đầu vào sẽ được hồi quy bằng cách sử dụng cùng
hàm tuyến tính. Hạn chế này tồn tại trong suốt quá trình huấn luyện (Hình 2(d)). Mặt khác,
mã hóa vị trí có thể dễ dàng ánh xạ mỗi phần tử đến
một vùng kích hoạt duy nhất bằng cách sử dụng cùng kiến trúc. Nhìn chung, chúng ta
có thể thấy rằng các đầu vào dựa trên tọa độ có rất ít khả năng biểu đạt
trong quá trình huấn luyện so với mã hóa vị trí.

Lý do cho khả năng biểu đạt giảm này là kết quả của cả
mật độ đầu vào và thiếu các vùng kích hoạt trong chiều thấp. Tọa độ
áp đặt một lưới trên một tập con của R^d trong đó độ phân giải,

--- TRANG 4 ---
được đưa ra bởi khoảng cách của chúng, rất mịn. Do đó, việc khớp đủ
nơ-ron trong lớp đầu tiên sao cho một lượng đáng kể siêu phẳng
có thể tách biệt mỗi đầu vào trở nên khó khăn. Nếu đầu vào của các lớp
l > 1 bị hạn chế về các ảnh của các hàm tuyến tính tương tự (hoặc giống nhau)
trên tập dữ liệu, thì điều này hạn chế nghiêm trọng khả năng tổng thể
của mạng ngay cả khi độ sâu tăng (xem Phụ lục).

Với mã hóa vị trí, mật độ được giảm bớt vì khoảng cách
||γ(xi) - γ(xj)|| cho mỗi xi, xj được điều chỉnh bởi tần số,
đồng thời tăng chiều của nó và tạo ra nhiều
vùng kích hoạt hơn. Mật độ của đầu vào có thể được diễn giải thông qua
hàm kernel được tạo ra, được biết là tĩnh cho mã hóa vị trí [25]. Điều này có nghĩa là sự tương tự giữa các đầu vào
chỉ phụ thuộc vào khoảng cách của chúng, tạo ra một đường chéo mạnh
thiếu vắng với các đầu vào dựa trên tọa độ (Hình 3).

Để có trực giác, chúng ta cũng có thể liên kết mật độ của đầu vào với
các thành phần tần số của hàm mục tiêu. Mật độ cao hơn dẫn đến
một hàm mục tiêu chứa các thành phần tần số cao hơn, và ngược lại. Do đó,
mã hóa vị trí cũng có thể được xem như tạo ra một
hàm tần số thấp hơn, phân tán các đầu vào tốt hơn trên các vùng kích hoạt
có sẵn. Điều này không thể thực hiện được với việc lấy mẫu mịn
của tọa độ, như được hiển thị trong Hình 2.

4 Sự Nhầm lẫn Gradient Cao

Trong [24], ảnh hưởng của kiến trúc mạng đến tốc độ hội tụ
được mô hình hóa bằng cách sử dụng sự nhầm lẫn gradient, một thước đo xác định
tương quan của gradient cho các đầu vào khác nhau. Đã được chỉ ra rằng sự nhầm lẫn
thấp hơn trong quá trình huấn luyện có thể tăng tốc hội tụ của SGD, hoặc làm cho
việc huấn luyện mạng dễ dàng hơn. Sự nhầm lẫn xảy ra khi hai hàm mục tiêu
L(f(xi), yi) và L(f(xj), yj), i ≠ j, có gradient
sao cho ⟨∇θL(f(xi), yi), ∇θL(f(xj), yj)⟩ < 0. Điều này tạo ra
một bất đồng về hướng mà các tham số cần di chuyển, làm chậm
hội tụ. Tốc độ hội tụ của SGD được đưa ra thông qua
cận nhầm lẫn κ ≥ 0, được đưa ra là

κ ≥ ⟨∇θL(f(xi), yi), ∇θL(f(xj), yj)⟩

cho tất cả i ≠ j và θ cố định. Chúng tôi tập trung vào metric này vì chúng tôi quan tâm
đến tốc độ mà gradient descent hội tụ và mối quan hệ của nó với
thông tin không gian của tín hiệu.

4.1 Trực giác về Sự Nhầm lẫn Cao hơn

Trong phần này, chúng tôi nhằm xây dựng trực giác về lý do tại sao khả năng biểu đạt
hạn chế sẽ gây ra lượng nhầm lẫn cao hơn khi các giá trị mục tiêu
dao động nhanh. Giả sử chúng ta đang hồi quy một tín hiệu 1D, và
cho fA là một hàm tuyến tính được cho bởi một mạng ReLU tương ứng
với một mẫu kích hoạt A. Giả sử hai đầu vào dày đặc xi, xj ∈ [0,1]
thuộc về cùng R(f, A). Khi đó chúng ta có thể viết xj = c⊙xi, và
sgn(w^(l)T i(c^l-1 ⊙ x^l-1 i) + b^l i) = sgn(w^(l)T i x^l-1 i + b^l i) phải đúng
cho tất cả nơ-ron, trong đó c^l là một vector của các giá trị dương. Do đó,

⟨∇θfA(xi), ∇θfA(xj)⟩ = ⟨∇θfA(xi), d⊙∇θfA(xi)⟩ > 0,    (2)

trong đó d ∈ R^#weights là các giá trị dương điều chỉnh các đầu ra
ẩn. Bây giờ chúng ta sẽ đánh giá bằng cách sử dụng hàm mất mát Mean Squared Error (MSE)
1/N ∑^N_{i=1} (yi - f(xi))^2:

Chúng ta có gradient của MSE đối với các tham số cho một
đầu vào x đơn lẻ là

∇θL(fA(x), y) = -2(y - fA(x))∇θfA(x).

Từ (2), chúng ta có thể thấy rằng sự nhầm lẫn sẽ được gây ra trực tiếp bởi
phần dư y - fA(x). Nếu yi dao động mạnh cho mỗi xi ∈ R(f, A),
thì sự nhầm lẫn sẽ xảy ra trong bất kỳ tình huống nào mà yi > fA(xi) và
yj < fA(xj). Vì hướng của việc cập nhật gradient cho mỗi
trọng số về cơ bản được xác định bởi phần dư, sự nhầm lẫn chỉ có thể
được giảm nếu các giá trị mục tiêu thay đổi tuyến tính hoặc có thể nếu không đổi.

Bây giờ, giả sử chúng ta có các đầu vào xi, xj ∈ [0,1] sao cho chúng được
hồi quy bởi hai hàm tuyến tính riêng biệt fA và fB của các mẫu A
và B. Cho S = {w^l_i | A^l_i, B^l_i = 1} biểu thị tập hợp các trọng số
được kích hoạt chung giữa các lớp. Tích vô hướng của các gradient
sẽ chỉ phụ thuộc vào các tham số trong S vì tất cả các trọng số khác sẽ
triệt tiêu do ReLU

⟨∇S L(fA(xi), yi), ∇S L(fB(xj), yj)⟩.

Trong khi phần dư của ∇S L vẫn có thể dao động mạnh, không nhất thiết
⟨∇S fA(xi), ∇S fB(xj)⟩ > 0, vì quá trình lan truyền ngược
sẽ sử dụng các trọng số w^l_i ∉ S, số lượng được ảnh hưởng
bởi khoảng cách Hamming giữa các mẫu kích hoạt. Chúng ta có thể
xem điều này như mạng sử dụng thông tin từ các vùng kích hoạt riêng biệt
khi tối ưu hóa các trọng số chung. Về cơ bản, mạng có thể
cấu hình các vùng kích hoạt sao cho việc cập nhật các trọng số chung trong
cùng hướng có thể có lợi cho việc giảm thiểu mất mát. Trong trường hợp này,
tác động của phần dư trong việc xác định các hướng gradient
được giảm đáng kể, trong khi các hướng cho các đầu vào bị hạn chế trong một
vùng duy nhất (hoặc các vùng gần đó) sẽ bị ảnh hưởng mạnh bởi các giá trị mục tiêu.

4.2 Thiên lệch Phổ

Mối quan hệ giữa khả năng biểu đạt và sự nhầm lẫn được chứng minh
bằng cách phân tích mật độ nhầm lẫn và khoảng cách Hamming
trong Hình 4. Khoảng cách Hamming là một thước đo khả năng biểu đạt
giữa các đầu vào, vì nó định lượng sự khác biệt của các mẫu kích hoạt
của chúng. Chúng tôi tính toán cả sự nhầm lẫn và khoảng cách Hamming
cho các đầu vào được lấy mẫu trong các vùng địa phương của không gian đầu vào và toàn cục
(các khoảng cách ra) trên không gian đầu vào, điều này thường tương ứng
với các thành phần tần số cao và thấp tương ứng
(trong chế độ dựa trên tọa độ). Các tương quan gradient theo cặp
giữa các đầu vào ở các giai đoạn khác nhau của quá trình huấn luyện sau đó được tính toán bằng cách sử dụng
độ tương tự cosine, và mật độ của chúng được vẽ. Một mật độ rộng hơn hoặc
nồng độ cao hơn trong phạm vi âm chỉ ra nhiều sự nhầm lẫn hơn,
trong khi một mật độ tập trung hơn xung quanh số không hoặc các giá trị dương gợi ý
ít sự nhầm lẫn hơn. Điều này tương ứng với các hướng gradient
trực giao hoặc tương quan dương giữa các đầu vào.

Trong Hình 4, chúng tôi hiển thị ba khía cạnh chính của các quan sát của chúng tôi. Chi tiết thực nghiệm
thêm được giải thích trong chú thích của Hình 4. Chúng tôi đầu tiên
tập trung vào chế độ dựa trên tọa độ vì nó liên quan trực tiếp đến thiên lệch phổ:

1. Cả mã hóa vị trí và tọa độ đều kém hiệu quả trong việc biểu diễn
các thành phần tần số cao hơn so với các thành phần tần số thấp,
được đo bằng khoảng cách Hamming thấp hơn giữa

--- TRANG 5 ---
MLP Dựa trên Tọa độ
(a) Khoảng cách Hamming Địa phương
(b) Khoảng cách Hamming Toàn cục
(c) Sự Nhầm lẫn Địa phương
(d) Sự Nhầm lẫn Toàn cục

MLP
CNN
Epoch 15    Epoch 1000    Epoch 100

(h) Sự Nhầm lẫn Lân cận
(e) Hamming Lân cận
(g) Hamming Xa
(j) Sự Nhầm lẫn Xa
(i) Sự Nhầm lẫn Lân cận

Các Lớp Khác nhau
(f) Hamming Lân cận Các Lớp Khác nhau
(n) Sự Nhầm lẫn Lân cận
(o) Sự Nhầm lẫn Lân cận Các Lớp Khác nhau
(p) Sự Nhầm lẫn Xa

(k) Hamming Lân cận
(l) Hamming Lân cận Các Lớp Khác nhau
(m) Hamming Xa

Hình 4: Khoảng cách Hamming và mật độ nhầm lẫn trong chế độ dựa trên tọa độ cũng như thiết lập phân loại. Đối với chế độ dựa trên tọa độ, chúng tôi lấy mẫu đầu vào trong 100 vùng địa phương 50x50 khác nhau của hình ảnh, cũng như 50k đầu vào xa. Sự nhầm lẫn và khoảng cách Hamming được tính toán giữa các cặp trong các mẫu tương ứng của chúng. Điều này tương đương với 50k cặp địa phương và toàn cục. Cùng kết quả được tiến hành cho MLP và CNN trên các tập dữ liệu phân loại khác nhau, nơi các cặp địa phương là k=25 lân cận gần nhất của một đầu vào được lấy mẫu ngẫu nhiên (có thể chứa chủ yếu các nhãn giống nhau cho các tập dữ liệu nhất định), cũng như k=25 lân cận gần nhất có lớp hoàn toàn khác. Các cặp xa là k=25 đầu vào xa nhất đến một mẫu đã cho. Mỗi thí nghiệm sử dụng tổng cộng 62.5k cặp được lấy mẫu trên toàn bộ tập dữ liệu. Chúng tôi hiển thị 1000 epoch cho tọa độ, và 100 cho phân loại.

các mẫu kích hoạt cho các đầu vào được lấy mẫu địa phương (Hình 4(a)) so với
các đầu vào được lấy mẫu toàn cục (Hình 4(b)). Kết quả là, trong quá trình huấn luyện,
mô hình bị nhầm lẫn thường xuyên hơn khi xử lý các thành phần tần số cao
hơn so với các thành phần tần số thấp (Hình 4(c)-(d)).

2. Mã hóa vị trí chịu đựng ít nhầm lẫn nhất cho
các thành phần tần số cao do khả năng biểu đạt tăng cường của nó (Hình 4(a)),
điều này gợi ý hội tụ nhanh hơn.

3. Tọa độ tạo ra một sự chênh lệch lớn trong lượng nhầm lẫn
giữa các mẫu đầu vào, điều này không xảy ra với mã hóa vị trí.

Nhìn chung, những kết quả này chứng minh cách mỗi đầu vào có thể sẽ có
sự hội tụ bị cản trở bởi các lân cận gần nhất của nó, gây ra bởi sự
tương tự trong các mẫu kích hoạt của chúng tạo ra nồng độ nhầm lẫn cao hơn.
Ngược lại, các đầu vào xa được lấy mẫu toàn cục
trên tín hiệu gặp ít nhầm lẫn hơn vì nhiều khả năng biểu đạt hơn
có thể được sử dụng, dẫn đến các cập nhật gradient hài hòa. Do đó,
mạng hội tụ nhanh hơn đến cấu trúc tổng thể của hàm mục tiêu
hơn so với các chi tiết địa phương, dẫn đến các biểu diễn tần số thấp được quan sát.
Mã hóa vị trí cho phép mạng sử dụng khả năng biểu đạt tăng cường
một cách hiệu quả, cho phép hội tụ nhanh hơn đến các thành phần tần số cao.
Hơn nữa, sự chênh lệch đáng kể trong sự nhầm lẫn khi sử dụng tọa độ (giữa các
thành phần tần số khác nhau) có thể được quy cho một thiên lệch phổ lớn hơn.

Đối với các nhiệm vụ phân loại, một hành vi tương tự xảy ra cho cả hai
kiến trúc. Các lân cận gần nhất tạo ra nhiều nhầm lẫn hơn so với
các đầu vào xa, với các lân cận của các lớp hoàn toàn riêng biệt tạo ra
nhầm lẫn nhiều nhất. Khoảng cách Hamming giữa các mẫu kích hoạt
theo tương ứng. Chúng tôi nhắc lại rằng, trong khi phương pháp của chúng tôi cung cấp
hiểu biết về thiên lệch phổ khi hồi quy tín hiệu (hội tụ chậm hơn
đến các chi tiết địa phương thay đổi nhanh), việc liên kết trực tiếp những kết quả này với
thiết lập phân loại nên đòi hỏi phân tích sâu hơn. Cho đến nay, các tác giả
của [21] đã sử dụng phiên bản nhị phân của tập dữ liệu MNIST
(chỉ hai nhãn) với tiếng ồn nhãn tăng để chứng minh thiên lệch phổ,
nhưng thiết lập này hạn chế so với phân loại đa lớp,

--- TRANG 6 ---
(a) L=4 (epoch 1)
(b) L=8 (epoch 1)
(c) L=16 (epoch 1)
(d) L=4 (epoch 2500)
(e) L=8 (epoch 2500)
(f) L=16 (epoch 2500)
Hình 5: Độ tương tự cosine giữa các pháp tuyến siêu phẳng trong quá trình huấn luyện
qua các lớp. Pháp tuyến của mỗi siêu phẳng đơn giản được cho là
các trọng số tương ứng (Được thực hiện trên một hình ảnh đơn, đúng cho
các hình ảnh khác).

và mục tiêu là MSE. Trong [27], thiên lệch phổ được phân tích trong
thiết lập phân loại bằng cách sử dụng cái mà họ định nghĩa là tần số phản hồi,
được đưa ra thông qua biến đổi Fourier rời rạc không đều (NUDFT)
trên tập dữ liệu {(xi, yi)}^N_{i=0} của hình ảnh và nhãn. Trong trường hợp này, chúng ta có thể
liên kết các thành phần tần số cao của tập dữ liệu với
các lân cận gần nhất của các lớp khác nhau, nơi sự nhầm lẫn được
tập trung. Việc so sánh thêm kết quả của chúng tôi với các phương pháp được sử dụng trong
các công trình trước đây có thể cung cấp hiểu biết về các tính chất gây ra
thiên lệch phổ trong thiết lập phân loại chiều cao.

5 Tần số Mã hóa và Tính chất Vùng Kích hoạt

Bây giờ chúng tôi khám phá các động lực học vùng kích hoạt riêng biệt được tạo ra bởi
các mã hóa vị trí tần số cao hơn. Việc làm như vậy có thể cung cấp thêm
hiểu biết về các giải pháp riêng biệt mà mạng khám phá, và chúng tôi sử dụng
các thí nghiệm được lấy cảm hứng từ các công trình trước đây [8,29,18,17]. Chúng tôi bắt đầu
bằng cách phân tích tương quan giữa các hướng siêu phẳng, được cho là
∇x z(x), bằng cách lấy độ tương tự cosine giữa tất cả các trọng số (Hình
5). Chúng tôi tìm thấy rằng các mã hóa tần số cao hơn cho phép tăng
các hướng siêu phẳng trực giao ở đầu quá trình huấn luyện (chiều cao hơn),
sau đó trở nên ngày càng song song khi quá trình huấn luyện tiến triển.
Cụ thể, Hình 5 chứng minh rằng các siêu phẳng ít tương quan âm hơn
trong lớp đầu tiên khi tần số mã hóa tăng,
điều này ảnh hưởng đến các lớp tiếp theo tương ứng.

Chúng tôi cũng tìm thấy rằng các vùng kích hoạt sẽ mở rộng (hoặc tăng
thể tích) với tần số mã hóa sau trong quá trình huấn luyện, điều này có thể
gây ra sự giảm trong tính khác biệt giữa các mẫu kích hoạt
trên tập dữ liệu. Sự giảm này được hiển thị trong Hình 4 cho các mã hóa
tần số cao hơn (L=8 và L=16), vì có một sự giảm lớn trong
khoảng cách Hamming trung bình. Lưu ý rằng mã hóa tần số thấp
(L=4) bắt đầu co lại khoảng cách Hamming cũng như, nhưng với
tốc độ chậm hơn nhiều. Để chứng minh thêm rằng tăng tần số mã hóa
dẫn đến các vùng kích hoạt rộng hơn, chúng tôi trực quan hóa các lát cắt 2D
của các vùng kích hoạt cho một mạng đã được huấn luyện trong Hình 6, nơi
sự khác biệt được hiển thị.

L = 4    L = 16
Lớp Đầu tiên
Cao    Thấp
Bốn Lớp
Cao    Thấp
Hình 6: Lát cắt 2D của các vùng kích hoạt cho mã hóa tần số cao và thấp
sau 2500 epoch. Các hình ảnh được tìm thấy bằng cách di chuyển dọc theo một mặt phẳng 2D
(tất cả các chiều khác cố định) tại gốc tọa độ, với "Thấp" biểu thị
một mặt phẳng 2D dọc theo các chiều của các đặc trưng tần số thấp của
mã hóa, và "Cao" các đặc trưng tần số cao. Trực quan hóa được thực hiện
trên hình ảnh đơn.

Cuối cùng, trong Hình 7 chúng tôi vẽ khoảng cách của mỗi đầu vào đến
ranh giới gần nhất, được cho là

d(x, Bf) = min_{z(x)∈f} |z(x) - bz| / ||∇z(x)||,

ban đầu được hiển thị trong [8]. Khoảng cách này đo
độ nhạy của các nơ-ron đối với các mẫu đầu vào đã cho, hoặc nói cách khác
độ nhạy của chúng đối với các chuyển đổi ranh giới. Lưu ý rằng khoảng cách này có thể bị
ảnh hưởng bởi kích thước của các vùng, nhưng không cung cấp thước đo
trực tiếp. Thay vào đó, có thể các khu vực của không gian đầu vào tương ứng
với các vùng kích hoạt rộng hơn sẽ gợi ý mật độ chuyển đổi thấp
trong vùng lân cận này (tương đối với kiến trúc và kích thước đầu vào), đây là một
metric độ nhạy riêng biệt [18].

Trong Hình 7, các mã hóa tần số cao hơn đầu tiên có một
co lại lớn hơn của khoảng cách này, sau đó nhanh chóng tăng nó trong quá trình huấn luyện,
chỉ ra một sự chuyển đổi từ độ nhạy cao sang thấp. Thước đo này thú vị
vì nó được sử dụng trong [8] cho tập dữ liệu MNIST, nơi một
hành vi tương tự được tìm thấy. Họ phát hiện rằng mạng sẽ nhanh chóng
co lại các khoảng cách, được tương quan với hiệu suất khái quát hóa tốt hơn
về cuối quá trình huấn luyện. Trong thiết lập này, chúng tôi cũng nhận thấy
một sự tăng sau sự co lại này lớn hơn cho các mã hóa tần số cao hơn.

--- TRANG 7 ---
Hình 7: Khoảng cách trung bình của các điểm huấn luyện đến ranh giới Bf của
các nơ-ron ReLU. Điều này cho một thước đo độ nhạy của các nơ-ron đối với
các mẫu đầu vào trong quá trình huấn luyện, và cách điều này biến động.

Nhìn chung, các mạng sử dụng mã hóa tần số cao hơn có xu hướng tìm
các giải pháp trong đó các siêu phẳng có tương quan cao, thể tích
của các vùng kích hoạt tăng, và độ nhạy của các nơ-ron đối với
các mẫu đầu vào được giảm sau trong quá trình huấn luyện. Những tính chất này có thể
tạo ra hội tụ nhanh hơn đến tín hiệu mục tiêu như được hiển thị trong Phần 4,
mà các mã hóa tần số thấp hơn không thể hoàn thành. Tuy nhiên, chúng
cũng có thể được liên kết với hiệu suất khái quát hóa tệ hơn, vì các mã hóa tần số cao hơn
được biết là overfitting [25]. Trong khi việc kết nối mỗi
tính chất riêng biệt này khó khăn do động lực học phức tạp của
các vùng, việc làm như vậy có thể cung cấp hiểu biết về khả năng khái quát hóa
của mã hóa vị trí trong các công trình tương lai.

6 ReLU Chết

Mặc dù các mạng dựa trên tọa độ chậm chạp tăng số lượng
vùng kích hoạt trong suốt quá trình huấn luyện như được hiển thị trong Hình 2 (d), có
một sự thay thế đáng ngạc nhiên đang diễn ra đồng thời. Đó là,
mạng đang ngày càng tắt các nơ-ron ReLU trong quá trình huấn luyện,
hạn chế tiềm năng biểu đạt đầy đủ của nó trong khi đồng thời cố gắng
sử dụng các hàm tuyến tính khác nhau với các nơ-ron có sẵn. Chúng tôi tin
điều này là do sự tăng theo cấp số nhân trong chuẩn tham số xảy ra
khi huấn luyện trên các hàm mục tiêu tần số cao, có thể gây ra vấn đề
do mật độ của dữ liệu. Lưu ý điều này chỉ xảy ra với
tọa độ, và không với mã hóa vị trí.

Chúng tôi tính toán chuẩn tham số ∏^L_{n=1} ||W^(k)||f qua các lớp,
trong đó ||·||f là chuẩn phổ được tìm thấy bởi giá trị kỳ dị lớn nhất,
được hiển thị trong Hình 8. Chuẩn này đại diện cho cận trên của
hằng số Lipschitz Lf của mạng, và có thể hơi phản trực giác
khi thấy rằng cận này cao hơn cho mạng với
thiên lệch phổ lớn hơn. Trong thiết lập này, có thể trường hợp các tham số lớn hơn
có thể dễ dàng ánh xạ tọa độ đến một phía của ngưỡng bias, gây ra
nó không hoạt động trong phần còn lại của quá trình huấn luyện. Chúng ta có thể thấy một số trực giác
cho ý tưởng này từ Hình 8 (d). Số lượng nơ-ron ReLU chết
giảm khi khoảng chuẩn hóa tăng, có nghĩa là một khi việc lấy mẫu
trở nên ít dày đặc hơn thì có nhiều nơ-ron hoạt động hơn. Trong khi
điều này có thể đúng, vẫn còn xu hướng tổng thể của một
lượng nơ-ron chết tăng, điều này hiển thị một hạn chế khác của các mạng ReLU
trong thiết lập dày đặc, chiều thấp này. Ngoài ra, điều này
dường như là một vấn đề tại thời điểm khởi tạo, có thể được giảm
với khởi tạo trọng số phù hợp, hoặc các biện pháp tiềm năng khác.

7 Kết luận

Trong bài báo này, chúng tôi đã cung cấp một nghiên cứu kỹ lưỡng về thiên lệch phổ trong
thiết lập dựa trên tọa độ bằng cách sử dụng mô hình trực tiếp của động lực học huấn luyện.

(a) Tọa độ
(b) Mã hóa Vị trí L=8
(c) Tất cả Các Phương pháp
(d) Nơ-ron ReLU Chết
Hình 8: Chuẩn phổ cho cả mã hóa vị trí (L=8) (b) và tọa độ ([0,1]) (a) trong suốt quá trình huấn luyện (a-c), cùng với số lượng nơ-ron ReLU chết trong quá trình huấn luyện cho tọa độ (d). Không có nơ-ron ReLU chết cho mã hóa vị trí.

Phân tích này cung cấp cái nhìn đầu tiên về các tính chất của mạng
gây ra thiên lệch phổ mà không sử dụng NTK hoặc phân tích Fourier.
Chúng tôi tìm thấy rằng sự hội tụ của gradient descent trong các vùng địa phương của
không gian đầu vào, sử dụng ít khả năng biểu đạt hơn cho các đầu vào
lân cận, chậm hơn so với các đầu vào được lấy mẫu toàn cục trên
tín hiệu. Điều này dẫn đến các thành phần tần số cao của tín hiệu
được học sau khi một biểu diễn tần số thấp được đạt được, với
mức độ nghiêm trọng của thiên lệch phổ phụ thuộc vào cách mạng gán
các đầu vào cho các vùng kích hoạt. Mã hóa vị trí có thể tăng cường
khả năng biểu đạt tổng thể, có thể dẫn đến hội tụ nhanh hơn đến
các thành phần tần số cao. Ngoài điều này, chúng tôi đã khám phá
các tính chất của các vùng kích hoạt khi tần số mã hóa tăng,
điều này đạt được hiểu biết về các giải pháp khác nhau mà mạng
đến. Một hạn chế khác của ReLU trong thiết lập chiều thấp này đến từ
các ReLU chết, rất có thể được gây ra bởi mật độ
của các đầu vào và sự tăng theo cấp số nhân trong chuẩn tham số. Cuối cùng,
chúng tôi chỉ ra rằng cùng hành vi chung tồn tại trong thiết lập phân loại
với các kiến trúc khác nhau, có thể cung cấp hiểu biết cho
các công trình tương lai cố gắng phân tích chính thức và đo lường thiên lệch phổ
trong các thiết lập phức tạp hơn. Cũng có thể một phương pháp như
của chúng tôi có thể vượt qua các gánh nặng tính toán của các phương pháp trước đây
trong các thiết lập phức tạp này, chẳng hạn như phân tích Fourier trong các thiết lập chiều cao.

Tài liệu Tham khảo

[1] Eirikur Agustsson và Radu Timofte, 'Ntire 2017 challenge on single
image super-resolution: Dataset and study', trong The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) Workshops, (July
2017).

[2] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo
Ma, và Brian McWilliams, 'The shattered gradients problem: If resnets
are the answer, then what is the question?', (2017).

[3] Ronen Basri, Meirav Galun, Amnon Geifman, David Jacobs, Yoni Kasten, và Shira Kritchman, 'Frequency bias in neural networks for input
of non-uniform density', trong International Conference on Machine Learning, pp. 685–694. PMLR, (2020).

--- TRANG 8 ---
[4] Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, và Quanquan
Gu, 'Towards understanding the spectral bias of deep learning', trong Proceedings of the Thirtieth International Joint Conference on Artificial
Intelligence, IJCAI-21, ed., Zhi-Hua Zhou, pp. 2205–2211. International
Joint Conferences on Artificial Intelligence Organization, (8 2021). Main
Track.

[5] Li Deng, 'The mnist database of handwritten digit images for machine
learning research', IEEE Signal Processing Magazine, 29(6), 141–142,
(2012).

[6] Rizal Fathony, Anit Kumar Sahu, Devin Willmott, và J Zico Kolter,
'Multiplicative filter networks', trong International Conference on Learning
Representations, (2021).

[7] Stanislav Fort, Paweł Krzysztof Nowak, Stanislaw Jastrzebski, và Srini
Narayanan. Stiffness: A new perspective on generalization in neural
networks, 2019.

[8] Boris Hanin và David Rolnick, 'Complexity of linear regions in deep
networks', trong Proceedings of the 36th International Conference on Machine Learning, eds., Kamalika Chaudhuri và Ruslan Salakhutdinov,
volume 97 of Proceedings of Machine Learning Research, pp. 2596–
2604. PMLR, (09–15 Jun 2019).

[9] Boris Hanin và David Rolnick, Deep ReLU Networks Have Surprisingly Few Activation Patterns, Curran Associates Inc., Red Hook, NY,
USA, 2019.

[10] Arthur Jacot, Franck Gabriel, và Clément Hongler, 'Neural tangent
kernel: Convergence and generalization in neural networks', Advances
in neural information processing systems, 31, (2018).

[11] Jonas Kiessling và Filip Thor, 'A computable definition of the spectral
bias', 36, 7168–7175, (Jun. 2022).

[12] Diederik P. Kingma và Jimmy Ba, 'Adam: A method for stochastic
optimization', trong 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference
Track Proceedings, eds., Yoshua Bengio và Yann LeCun, (2015).

[13] Alex Krizhevsky, 'Learning multiple layers of features from tiny images',
Technical report, (2009).

[14] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian
Nowozin, và Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space, 2018.

[15] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, và Ren Ng. Nerf: Representing scenes as
neural radiance fields for view synthesis, 2020.

[16] Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, và Yoshua Bengio, 'On the number of linear regions of deep neural networks', trong Advances in Neural Information Processing Systems, eds., Z. Ghahramani,
M. Welling, C. Cortes, N. Lawrence, và K.Q. Weinberger, volume 27.
Curran Associates, Inc., (2014).

[17] Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, và Nati
Srebro, 'Exploring generalization in deep learning', trong Advances in
Neural Information Processing Systems, eds., I. Guyon, U. Von Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, và R. Garnett,
volume 30. Curran Associates, Inc., (2017).

[18] Roman Novak, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington,
và Jascha Narain Sohl-Dickstein, 'Sensitivity and generalization in
neural networks: an empirical study', ArXiv, abs/1802.08760, (2018).

[19] Razvan Pascanu, Guido Montufar, và Yoshua Bengio. On the number
of response regions of deep feed forward networks with piece-wise
linear activations, 2013.

[20] Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, và
Jascha Sohl Dickstein, 'On the expressive power of deep neural networks', trong Proceedings of the 34th International Conference on Machine
Learning - Volume 70, ICML'17, p. 2847–2854. JMLR.org, (2017).

[21] Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min
Lin, Fred Hamprecht, Yoshua Bengio, và Aaron Courville, 'On the
spectral bias of neural networks', trong International Conference on Machine Learning, pp. 5301–5310. PMLR, (2019).

[22] Ali Rahimi và Benjamin Recht, 'Random features for large-scale kernel machines', trong Advances in Neural Information Processing Systems,
eds., J. Platt, D. Koller, Y. Singer, và S. Roweis, volume 20. Curran
Associates, Inc., (2007).

[23] Basri Ronen, David Jacobs, Yoni Kasten, và Shira Kritchman, 'The
convergence rate of neural networks for learned functions of different
frequencies', trong Advances in Neural Information Processing Systems,
eds., H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox,
và R. Garnett, volume 32. Curran Associates, Inc., (2019).

[24] Karthik A. Sankararaman, Soham De, Zheng Xu, W. Ronny Huang, và Tom Goldstein, 'The impact of neural network overparameterization
on gradient confusion and stochastic gradient descent', trong Proceedings
of the 37th International Conference on Machine Learning, ICML'20.
JMLR.org, (2020).

[25] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan
Barron, và Ren Ng, 'Fourier features let networks learn high frequency
functions in low dimensional domains', Advances in Neural Information
Processing Systems, 33, 7537–7547, (2020).

[26] Han Xiao, Kashif Rasul, và Roland Vollgraf, 'Fashion-mnist: a novel
image dataset for benchmarking machine learning algorithms', ArXiv,
abs/1708.07747, (2017).

[27] Zhi-Qin John Xu, 'Frequency principle: Fourier analysis sheds light
on deep neural networks', Communications in Computational Physics,
28(5), 1746–1767, (jun 2020).

[28] Dong Yin, Ashwin Pananjady, Max Lam, Dimitris Papailiopoulos, Kannan Ramchandran, và Peter Bartlett, 'Gradient diversity: a key ingredient for scalable distributed learning', trong Proceedings of the Twenty-First
International Conference on Artificial Intelligence and Statistics, eds.,
Amos Storkey và Fernando Perez-Cruz, volume 84 of Proceedings of
Machine Learning Research, pp. 1998–2007. PMLR, (09–11 Apr 2018).

[29] Xiao Zhang và Dongrui Wu, 'Empirical studies on the properties of
linear regions in deep neural networks', ArXiv, abs/2001.01072, (2020).
