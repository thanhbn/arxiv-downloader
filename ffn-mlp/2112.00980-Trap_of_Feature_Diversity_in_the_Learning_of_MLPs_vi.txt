# Bẫy đa dạng đặc trưng trong việc học của MLPs

Dongrui Liu, Shaobo Wang, Jie Ren, Kangrui Wang, Sheng Yin, Huiqi Deng, Quanshi Zhang

Trường Đại học Giao thông Thượng Hải, Viện Công nghệ Harbin

## Tóm tắt

Trong bài báo này, chúng tôi tập trung vào hiện tượng hai giai đoạn điển hình trong việc học của perceptron đa lớp (MLPs), và chúng tôi nhằm giải thích lý do cho việc giảm đa dạng đặc trưng trong giai đoạn đầu tiên. Cụ thể, mọi người nhận thấy rằng, trong quá trình huấn luyện MLPs, mất mát huấn luyện không giảm đáng kể cho đến giai đoạn thứ hai. Để làm điều này, chúng tôi tiếp tục khám phá lý do tại sao đa dạng của đặc trưng trên các mẫu khác nhau tiếp tục giảm trong giai đoạn đầu tiên, điều này làm tổn hại đến việc tối ưu hóa của MLPs. Chúng tôi giải thích hiện tượng này từ góc độ động lực học học tập của MLPs. Hơn nữa, chúng tôi giải thích về mặt lý thuyết tại sao bốn phép toán điển hình có thể làm giảm sự giảm đa dạng đặc trưng. Mã nguồn sẽ được phát hành khi bài báo được chấp nhận.

## 1 Giới thiệu

Mạng nơ-ron sâu (DNNs) đã đạt được thành công đáng kể trong nhiều nhiệm vụ khác nhau. Tuy nhiên, lý do cơ bản cho hiệu suất vượt trội của DNNs chưa được điều tra đầy đủ. Nhiều nghiên cứu nhằm giải thích các hiện tượng điển hình trong DNNs, ví dụ như điều tra hiện tượng giả thuyết vé số may mắn [16], giải thích hiện tượng giảm kép [23,36,19], hiểu giả thuyết nút cổ chai thông tin [48,41], khám phá nhiễu gradient và chính quy hóa [43,34], và phân tích động lực học học tập phi tuyến [27, 38].

Trong bài báo này, chúng tôi tập trung vào động lực học học tập của perceptron đa lớp (MLPs). Nó đã được khám phá rộng rãi rằng khi MLPs có nhiều lớp và việc tối ưu hóa khó khăn, quá trình học của MLPs có nhiều khả năng có hai giai đoạn. Như Hình 1(b) cho thấy, giai đoạn đầu tiên thường không dài, trong đó mất mát huấn luyện không giảm. Sau đó, trong giai đoạn thứ hai, mất mát huấn luyện đột nhiên bắt đầu giảm. Lưu ý rằng khi nhiệm vụ đơn giản, giai đoạn đầu tiên có thể cực kỳ ngắn (ví dụ: một vài lần lặp trong một epoch), và do đó không thể quan sát được.

Để làm điều này, chúng tôi khám phá một hiện tượng thú vị trong giai đoạn đầu tiên, tức là, như Hình 1(a) cho thấy, đặc trưng của các loại khác nhau trở nên ngày càng giống nhau. Đa dạng đặc trưng tiếp tục giảm (độ tương đồng cosin giữa các đặc trưng tiếp tục tăng) cho đến giai đoạn thứ hai.

Hiện tượng này được chia sẻ rộng rãi bởi MLPs, mạng nơ-ron tích chập và mạng nơ-ron hồi quy (xem cả Hình 2 và tài liệu bổ sung). Mạng nơ-ron được huấn luyện với độ sâu và độ rộng khác nhau, hàm kích hoạt khác nhau, và tốc độ học khác nhau, trên các loại dữ liệu khác nhau đều có thể thể hiện hiện tượng như vậy, điều này làm tổn hại đến việc tối ưu hóa.

Quan trọng hơn, việc điều tra và giảm thiểu hiện tượng hai giai đoạn có giá trị đáng kể, bởi vì đây là vấn đề học tập bị mắc kẹt điển hình với MLPs. Như Hình 1(c) cho thấy, khi việc tối thiểu hóa mất mát bị mắc kẹt, chúng ta có thể coi nó như một giai đoạn đầu tiên mạnh với độ dài vô hạn.

Do đó, chúng tôi nhằm điều tra hành vi tối ưu hóa trong giai đoạn đầu tiên, tức là tại sao và làm thế nào đa dạng đặc trưng giảm trong quá trình huấn luyện sớm của MLP. Để làm điều này, chúng tôi thấy rằng trong các lớp trung gian của MLP, cả đặc trưng và tham số chủ yếu được tối ưu hóa theo một hướng đặc biệt, cụ thể là hướng chung chính. Trong nghiên cứu này, chúng tôi làm rõ về mặt lý thuyết một số động lực học của việc tối ưu hóa, điều này làm tăng khả năng tăng cường thêm hướng chung chính như vậy, giống như một hệ thống tự tăng cường. Điều này có thể giải thích sự giảm đa dạng đặc trưng trong các lần lặp sớm.

Dựa trên phân tích lý thuyết của chúng tôi, chúng tôi tiếp tục khám phá và giải thích lý do tại sao bốn phép toán điển hình (tức là, chuẩn hóa batch, động lượng, khởi tạo và chính quy hóa L2) có thể giảm thiểu hiệu quả hiện tượng hai giai đoạn. Các phát hiện trên cung cấp hỗ trợ lý thuyết cho các giải pháp heuristic cho vấn đề học tập bị mắc kẹt.

Mặc dù việc giảm đa dạng đặc trưng có thể được giảm thiểu bởi các phép toán truyền thống, đóng góp cốt lõi của nghiên cứu này là khám phá và giải thích hiện tượng hai giai đoạn cơ bản nhưng phản trực giác với MLP, điều này đã không được giải thích về mặt lý thuyết trong thời gian dài. Thay vào đó, các nghiên cứu trước đây đơn giản chỉ đổ lỗi vấn đề học tập bị mắc kẹt trong giai đoạn đầu tiên cho sự khó khăn của nhiệm vụ huấn luyện, mà không có phân tích sâu sắc hoặc giải pháp được hỗ trợ về mặt lý thuyết.

Đóng góp của nghiên cứu này có thể được tóm tắt như sau. (1) Chúng tôi khám phá hiện tượng chung của việc giảm đa dạng đặc trưng trong việc học sớm của MLP, điều này đã bị bỏ qua trong thời gian dài. (2) Chúng tôi giải thích hiện tượng này từ góc độ động lực học học tập. (3) Chúng tôi giải thích tại sao bốn loại phép toán có thể giảm thiểu việc giảm đa dạng đặc trưng.

## 2 Công trình liên quan

Hiểu việc tối ưu hóa và khả năng biểu diễn của DNNs là một hướng quan trọng để giải thích DNNs. Lý thuyết nút cổ chai thông tin [48, 41] đã giải thích định lượng thông tin được mã hóa bởi các đặc trưng trong các lớp trung gian của DNNs. Xu và Raginsky [49], Achille và Soatto [1], và Cheng et al. [8] đã sử dụng lý thuyết nút cổ chai thông tin để đánh giá và cải thiện khả năng biểu diễn của DNN. Arpit et al. [5] đã phân tích khả năng biểu diễn của DNNs với dữ liệu huấn luyện thực và nhiễu. Ngoài ra, một số chỉ số đã được đề xuất để đo lường khả năng tổng quát hóa hoặc độ bền vững của DNNs, bao gồm độ cứng [15], các chỉ số độ nhạy [37], phân tích Fourier [51], và điểm CLEVER [47]. Một số nghiên cứu tập trung vào việc chứng minh ranh giới tổng quát hóa của DNNs [32,31]. Để so sánh, chúng tôi giải thích MLP từ góc độ động lực học học tập, tức là, chúng tôi giải thích việc giảm đa dạng đặc trưng trong các lần lặp sớm của MLP.

Phân tích động lực học học tập là góc độ khác để hiểu DNNs. Nhiều nghiên cứu đã phân tích các cực tiểu cục bộ trong cảnh quan tối ưu hóa của mạng tuyến tính [7,40,18,10] và mạng phi tuyến [9,25,39]. Một số nghiên cứu đã thảo luận về tốc độ hội tụ của descent gradient trên dữ liệu có thể phân tách [45,50,35]. Hoffer et al. [20] và Jastrz˛ ebski et al. [24] đã điều tra tác động của kích thước batch và tốc độ học trên động lực học SGD. Ngoài ra, một số nghiên cứu đã phân tích động lực học của descent gradient trong chế độ tham số hóa quá mức [4,22,30,14]. Khác với các nghiên cứu trước đây, chúng tôi phân tích động lực học học tập của đặc trưng và trọng số của MLP, để giải thích việc giảm đa dạng đặc trưng trong giai đoạn đầu tiên.

## 3 Khám phá việc giảm đa dạng đặc trưng

Quá trình huấn luyện của MLP thường có thể được chia thành hai giai đoạn sau, theo mất mát huấn luyện. Như Hình 1(b) cho thấy, mất mát huấn luyện không giảm đáng kể trong giai đoạn đầu tiên, và mất mát huấn luyện đột nhiên bắt đầu giảm trong giai đoạn thứ hai. Trong bài báo này, chúng tôi khám phá một hiện tượng thú vị trong giai đoạn đầu tiên rằng cả đa dạng của đặc trưng lớp trung gian trên các mẫu khác nhau và đa dạng của gradient đối với đặc trưng tiếp tục giảm. Nói cách khác, trong giai đoạn đầu tiên, độ tương đồng cosin giữa các đặc trưng và độ tương đồng cosin giữa các gradient đặc trưng tiếp tục tăng.

Chúng tôi xem xét một MLP f với L lớp tuyến tính được nối tiếp, mỗi lớp được theo sau bởi một lớp ReLU. Chỉ có lớp tuyến tính cuối cùng được theo sau bởi phép toán softmax. Cho W_t^(l) ∈ R^(h×d) biểu thị ma trận trọng số của lớp tuyến tính thứ l với h nơ-ron (1≤l≤L), và W_t^(l) đã được học trong t lần lặp. Cho một mẫu đầu vào cụ thể x, việc truyền xuôi theo từng lớp trong lớp thứ l được biểu diễn như

F_t^(l) = ReLU(W_t^(l) F_t^(l-1)) = D_t^(l) W_t^(l) F_t^(l-1), (1)

trong đó F_t^(l) ∈ R^h biểu thị đặc trưng đầu ra của lớp thứ l sau lần lặp thứ t. D_t^(l) biểu thị ma trận đường chéo, đại diện cho trạng thái cổng trong lớp ReLU và D_t^(l),(i,i) ∈ {0,1}.

Trực quan hóa rằng hiện tượng hai giai đoạn được chia sẻ rộng rãi bởi các DNN khác nhau được học cho các nhiệm vụ khác nhau. Chúng tôi quan sát hiện tượng hai giai đoạn như vậy trên MLPs, VGG-11 [42], và bộ nhớ dài ngắn hạn đã được sửa đổi (LSTM) trên các loại dữ liệu khác nhau, bao gồm dữ liệu hình ảnh (MNIST [29], CIFAR-10 [26], và bộ dữ liệu Tiny ImageNet [28]), dữ liệu dạng bảng (hai bộ dữ liệu UCI về thu nhập điều tra dân số và tin tức TV [6]), và dữ liệu ngôn ngữ tự nhiên (CoLA [46], SST-2 [44], và AGNews [13]). Chúng tôi cũng quan sát MLPs với các lớp Leaky ReLU [33], với tốc độ học khác nhau, và với kích thước batch khác nhau. Hình 2(a,b) cho thấy hiện tượng hai giai đoạn trên một số DNNs, và vui lòng xem tài liệu bổ sung để biết kết quả trên nhiều DNNs hơn. Cụ thể, cho hai mẫu đầu vào x1 và x2, độ tương đồng đặc trưng giữa x1 và x2 cos(F_t^(l)|x1, F_t^(l)|x2), và độ tương đồng cosin của gradient cos(∇F_t^(l)|x1, ∇F_t^(l)|x2) tiếp tục tăng, điều này chứng minh hiện tượng. ∇F_t^(l) biểu thị gradient của mất mát đối với đặc trưng F_t^(l) trong Eq. (1).

Lưu ý rằng việc giảm đa dạng đặc trưng như vậy đôi khi xuất hiện trong các epoch (hoặc lần lặp) rất sớm của quá trình huấn luyện. Quan trọng hơn, như Hình 1(c) cho thấy, khi mất mát giảm trực tiếp trong epoch đầu tiên, nó có thể được coi là một giai đoạn đầu tiên ngắn trong các lần lặp rất sớm. Ngoài ra, quá trình học đôi khi bị mắc kẹt khi nhiệm vụ khó khăn. Sau đó chúng ta có thể coi vấn đề học tập bị mắc kẹt như giai đoạn đầu tiên với độ dài vô hạn (vui lòng xem tài liệu bổ sung để biết thêm thảo luận).

Kết nối với hiện tượng giảm kép theo epoch. Hình 1(d) cho thấy rằng hành vi giảm kép theo epoch [36,19] được căn chỉnh về mặt thời gian với giai đoạn đầu tiên trong hiện tượng hai giai đoạn được đề cập ở trên. Vui lòng xem tài liệu bổ sung để biết thêm thảo luận. Thay vì giải thích hiện tượng giảm kép theo epoch, trong bài báo này, chúng tôi chủ yếu giải thích việc giảm đa dạng đặc trưng.

## 4 Giải thích động lực học của việc giảm đa dạng đặc trưng

Trong phần này, chúng tôi nhằm điều tra một số động lực học của các tham số mạng, để giải thích điều kiện có thể làm tăng khả năng giảm đa dạng đặc trưng trong các epoch sớm dưới một số giả định chung. Trong Phần 4.1, chúng tôi thấy rằng việc giảm đa dạng của gradient đặc trưng trên các mẫu khác nhau là do hiện tượng rằng các nơ-ron khác nhau trong một lớp được tối ưu hóa theo hướng chung trong giai đoạn đầu tiên. Sau đó, trong Phần 4.2, chúng tôi tiếp tục làm rõ động lực học học tập có thể tăng cường tính quan trọng của hướng chung, giống như một hệ thống tự tăng cường. Hướng chung tự tăng cường làm tăng khả năng giảm đa dạng đặc trưng. Logic tổng thể của giải thích được minh họa trong Hình 3(b). Trong Phần 4.3, chúng tôi giải thích tại sao bốn loại phép toán có thể giảm thiểu việc giảm đa dạng đặc trưng dựa trên phân tích của chúng tôi.

### 4.1 Hai góc độ để phân tích hướng chung của tác động học tập

Việc giảm đa dạng của gradient đặc trưng trên các mẫu khác nhau là do hiện tượng rằng các nơ-ron khác nhau trong một lớp được tối ưu hóa theo hướng chung trong giai đoạn đầu tiên. Ví dụ, như Hình 3(a) cho thấy, vào đầu quá trình học, các nơ-ron khác nhau được tối ưu hóa theo các hướng khác nhau. Cùng với quá trình học, các nơ-ron khác nhau dần dần có xu hướng được tối ưu hóa theo một hướng tương tự. Theo việc truyền xuôi trong Eq. (1), gradient đặc trưng ∇F_t^(l) tại lớp thứ l trong quá trình truyền ngược có thể được viết lại như

∇F_t^(l-1) = W_t^(l)T D_t^(l) ∇F_t^(l). (2)

Theo cách này, chúng ta có thể coi gradient đặc trưng ∇F_t^(l-1) như kết quả của một việc truyền xuôi giả. Trong việc truyền xuôi giả, đầu vào là gradient đối với đặc trưng của lớp thứ l ∇F_t^(l), và đầu ra là gradient của lớp thứ (l-1) ∇F_t^(l-1) ∈ R^d. Tương ứng, ma trận trọng số tương đương được cho như W_t^(l)T = [w_{t,1}^(l), w_{t,2}^(l), ..., w_{t,d}^(l)]T ∈ R^{d×h}, bao gồm d "nơ-ron giả."

Trước khi giải thích hiện tượng hướng chung, chúng ta hãy đầu tiên làm rõ rằng sự tăng độ tương đồng giữa gradient đặc trưng ∇F_t^(l-1) của các mẫu khác nhau được gây ra bởi quan sát thực nghiệm rằng các "nơ-ron giả" khác nhau được tối ưu hóa gần đúng theo một hướng chung. Chúng ta hãy xem xét phỏng đoán sau đây rằng các "nơ-ron giả" khác nhau [w_{t,1}^(l), w_{t,2}^(l), ..., w_{t,d}^(l)]T đã được tối ưu hóa gần đúng theo hướng chung C^(l) ∈ R^h trong nhiều epoch. Sau đó, chúng ta có thể biểu diễn gần đúng w_{t,i}^(l) = α_i C^(l) + ε_i, trong đó α_i ∈ R, và ε_i ∈ R^h. Ở đây, ε_i ∈ R^h biểu thị một phần dư nhỏ. Theo Eq. (2), chúng ta có

∇F_t^(l-1) = α(C^(l)T D_t^(l) ∇F_t^(l)) + εD_t^(l) ∇F_t^(l) (3)

Do đó, nếu α đủ lớn (tức là, tiếp tục tối ưu hóa W_t^(l)T theo hướng chung C^(l) trong thời gian dài), thì gradient đặc trưng ∇F_t^(l-1) của các mẫu khác nhau sẽ gần đúng song song với cùng một vector α. Điều này là do C^(l)T D_t^(l) ∇F_t^(l) là một vô hướng và số hạng εD_t^(l) ∇F_t^(l) nhỏ. Nói cách khác, đa dạng giữa gradient đặc trưng ∇F_t^(l-1) của các mẫu khác nhau giảm. Ở đây, α = [α_1, α_2, ..., α_d], và ε = [ε_1, ε_2, ..., ε_d]T.

Do đó, nhiệm vụ cốt lõi đầu tiên của việc chứng minh việc giảm đa dạng của gradient đặc trưng là giải thích phỏng đoán về hướng tối ưu hóa chung được chia sẻ bởi các "nơ-ron giả" khác nhau. Để làm điều này, chúng ta hãy đầu tiên đề xuất hai góc độ để phân tách hướng chung như vậy.

**Góc độ 1.** Góc độ 1 tập trung vào sự thay đổi trọng số trong một lớp nhất định. Để rõ ràng, chúng tôi bỏ qua chỉ số trên (l) để đơn giản hóa ký hiệu trong các đoạn tiếp theo trong Phần 4.1, tức là, w_{t,i}^(l) đơn giản! w_{t,i}; W_t^(l) đơn giản! W_t; C^(l) đơn giản! C. Cho W_t^T = [w_{t,1}, w_{t,2}, ..., w_{t,d}]T biểu thị sự thay đổi trọng số của d "nơ-ron giả" trong lớp thứ l. Chúng tôi phân tách W_t^T thành thành phần dọc theo hướng chung C và thành phần dọc theo các hướng khác như sau.

W_t^T = V_t C^T + ε_t, (4)

trong đó V_t = [v_{t,1}, v_{t,2}, ..., v_{t,d}] ∈ R^d biểu thị vector hệ số cho sự thay đổi trọng số của các "nơ-ron giả" khác nhau dọc theo hướng chung C, tức là, sử dụng v_{t,i} C^T để xấp xỉ w_{t,i}^T. Cụ thể, ε_t là số hạng "nhiễu" tương đối nhỏ, vuông góc với C, tức là, ε_t C = 0. Theo cách này, hướng chung C có thể được xác định bằng cách tối thiểu hóa số hạng "nhiễu" trên các mẫu khác nhau qua các lần lặp khác nhau, như sau.

min_{C,V_t|x} E_{t∈[T_{start},T_{end}]} E_{x∈X} ||ε_t|x||_F^2; s.t. ε_t|x = W_t^T|x - V_t|x C^T (5)

**Bổ đề 1.** (Chứng minh trong tài liệu bổ sung) Đối với phép phân tách W_t^T = V_t C^T + ε_t, cho sự thay đổi trọng số trên các mẫu khác nhau W_t^T, chúng ta có thể tính hướng chung C trong Eq. (5) và có được V_t = W_t^T C/(C^T C) và ε_t = W_t^T - W_t^T C C^T/(C^T C) s.t. ε_t C = 0. Những thiết lập như vậy tối thiểu hóa ||ε_t||_F.

**Bổ đề 2.** (Chúng ta cũng có thể phân tách trọng số W_t^(l) thành thành phần dọc theo hướng chung C và thành phần ε_t trong các hướng khác. Chứng minh trong tài liệu bổ sung.) Cho trọng số W_t^T và hướng chung C, phép phân tách W_t^T = V_t C^T + ε_t có thể được thực hiện như V_t = W_t^T C/(C^T C) và ε_t = W_t^T - W_t^T C C^T/(C^T C) s.t. ε_t C = 0. Những thiết lập như vậy tối thiểu hóa ||ε_t||_F.

Xác minh thực nghiệm về sức mạnh của hướng chung chính C. Để làm điều này, chúng ta hãy tập trung vào sự thay đổi trọng số trung bình trên các mẫu khác nhau △W_t = E_{x∈X} △W_t|x. Sau đó, chúng ta phân tách △W_t thành các thành phần dọc theo năm hướng chung như △W_t = C_1 V_{1,t}^T + C_2 V_{2,t}^T + ... + C_5 V_{5,t}^T + ε_{5,t}^T, trong đó C_1 = C được gọi là hướng chung chính. C_2, C_3, C_4 và C_5 đại diện cho hướng chung thứ hai, thứ ba, thứ tư và thứ năm, tương ứng. C_1, C_2, C_3, C_4 và C_5 vuông góc với nhau. C_i và V_{i,t} được tính dựa trên Eq. (5) khi chúng ta loại bỏ i-1 thành phần đầu tiên dọc theo hướng C_1, ..., C_{i-1} từ △W_t. Hình 4 cho thấy rằng sức mạnh của thành phần chung chính C_1 V_1^T xấp xỉ gấp mười lần sức mạnh của thành phần chung thứ cấp C_2 V_2^T. Vui lòng xem tài liệu bổ sung để biết thêm thảo luận.

**Góc độ 2 bằng cách sử dụng gradient đối với đặc trưng ∇F_t^(l+1).**

Chúng tôi phân tách sự thay đổi trọng số bằng cách xem xét ảnh hưởng của hướng chung của lớp trên C^(l+1). Để phân biệt các biến thuộc về các lớp khác nhau, chúng tôi thêm chỉ số trên (l) trở lại W_t^(l), V_t^(l), và ε_t^(l) để biểu thị lớp trong các đoạn tiếp theo.

**Định lý 1.** (Chứng minh trong tài liệu bổ sung) Sự thay đổi trọng số được thực hiện bởi một mẫu có thể được phân tách thành (h+1) số hạng sau lần lặp thứ t như sau.

△W_t^(l) = △W_{primary,t}^(l) + Σ_{k=1}^h △W_{noise,t}^(l,k) viết lại = α_t^(l) F_{t}^(l-1)T + β_t^(l)T, (6)

trong đó △W_{primary,t}^(l) = D_t^(l) V_t^(l+1) C^(l+1)T C^(l+1) V_t^(l+1)T F_t^(l) F_t^(l-1)T/||F_t^(l)||_2^2 biểu thị thành phần dọc theo hướng chung chính, và △W_{noise,t}^(l,k) = D_t^(l) ε_t^(l+1,k) ε_t^(l+1)T F_t^(l) F_t^(l-1)T/||F_t^(l)||_2^2 biểu thị thành phần dọc theo hướng chung thứ k trong số hạng nhiễu. ε_t^(l+1,k) = σ_k U_k V_k^T, trong đó SVD của ε_t^(l+1) ∈ R^{h×h'} được cho như ε_t^(l+1) = U Σ V^T (h×h'), và σ_k biểu thị giá trị kỳ dị thứ k ∈ R. ε_t^(l+1) = Σ_k ε_t^(l+1,k). U_k và V_k biểu thị cột thứ k của ma trận U và V, tương ứng. Ngoài ra, chúng ta có ∀k ∈ {1,2,...,h}; U_k^T C^(l+1) = 0. Do đó, chúng ta có α_t^(l) = D_t^(l) V_t^(l+1) C^(l+1)T C^(l+1) V_t^(l+1)T F_t^(l)/||F_t^(l)||_2^2 ∈ R^h, và β_t^(l)T = D_t^(l) ε_t^(l+1) ε_t^(l+1)T F_t^(l) F_t^(l-1)T/||F_t^(l)||_2^2 ∈ R^{h×d}.

Cho sự thay đổi trọng số △W_t^(l) được thực hiện bởi mẫu x, số hạng chính △W_{primary,t}^(l) đại diện cho thành phần của sự thay đổi trọng số dọc theo hướng chung C^(l+1). Số hạng nhiễu thứ k △W_{noise,t}^(l,k) đại diện cho thành phần dọc theo hướng thứ k U_k, vuông góc với C^(l+1).

Xác minh thực nghiệm về sức mạnh đáng kể của thành phần dọc theo hướng chung C^(l+1). Để làm điều này, chúng tôi đã tính sức mạnh trung bình của thành phần dọc theo hướng chung C^(l+1) trên tất cả các mẫu trong X như S_{primary}^(l) = E_{t∈[T_{start},T_{end}]} E_{x∈X} [||△W_{primary,t}^(l)|x||_F]. Tương tự, sức mạnh của thành phần dọc theo hướng nhiễu thứ k được tính như S_k^(l) = E_{t∈[T_{start},T_{end}]} E_{x∈X} [||△W_{noise,t}^(l,k)|x||_F]. Bảng 1 minh họa rằng sức mạnh của thành phần chính S_{primary}^(l) lớn hơn mười lần so với sức mạnh của các thành phần dọc theo các hướng nhiễu khác S_1^(l), S_2^(l), và S_3^(l).

Thảo luận về việc so sánh với tổng của tất cả các hướng khác. Theo Bảng 1, có vẻ như tổng sức mạnh của các thành phần dọc theo các hướng khác cũng lớn. Tuy nhiên, các hướng khác nhau được phân tách bằng phương pháp trên vuông góc với nhau. Do đó, sự thay đổi trọng số dọc theo các hướng khác nhau là độc lập, và sức mạnh của chúng không thể được cộng lại. Vì vậy, chúng ta có thể so sánh trực tiếp sức mạnh của thành phần sự thay đổi trọng số dọc theo từng hướng để xác minh sức mạnh đáng kể của hướng chính.

### 4.2 Giải thích việc tăng cường tính quan trọng của hướng chung

Phần trước đã đổ lỗi việc giảm đa dạng của gradient đặc trưng cho hiện tượng điển hình rằng tồn tại một hướng tối ưu hóa chung được chia sẻ bởi các "nơ-ron giả" khác nhau. Trong phần hiện tại, chúng tôi giải thích rằng hiện tượng hướng tối ưu hóa chung rất có thể được tăng cường thêm, giống như một hệ thống tự tăng cường, khi đặc trưng F_t^(l-1) của các mẫu khác nhau đã được đẩy một chút theo hướng chung cụ thể (xem giả định nền tảng sau). Việc tự tăng cường hướng tối ưu hóa sẽ giải thích việc giảm đa dạng.

Theo Eq. (4) và Eq. (6), sự thay đổi trọng số được thực hiện bởi mẫu x có thể được cho như

Góc độ 1: △W_t^(l) = C^(l) V_t^(l)T + ε_t^(l)T Góc độ 2: △W_t^(l) = α_t^(l) F_t^(l-1)T + β_t^(l)T (7)

Giải thích phỏng đoán về mối quan hệ giữa đặc trưng và trọng số. Hình 5 cho thấy hiện tượng rằng đặc trưng F_t^(l-1) có hướng tương tự như vector V_t^(l), trong đó γ ∈ {-1, +1}. Bằng cách so sánh hai góc độ trên của hiện tượng như vậy, chúng tôi đoán rằng hướng chung C^(l) tương tự như α_t^(l), và đặc trưng F_t^(l-1) tương tự như vector V_t^(l).

Do đó, trong phần này, chúng tôi nhằm giải thích rằng đặc trưng F_t^(l-1) và vector V_t^(l) trở nên ngày càng tương tự nhau trong giai đoạn đầu tiên. Điều này giải thích việc tự tăng cường tính quan trọng của hướng chung.

Thảo luận về giả định nền tảng. Chứng minh trực tiếp cách một "hệ thống tự tăng cường" như vậy xuất hiện từ ban đầu của việc huấn luyện MLP được khởi tạo là quá khó khăn. Do đó, chúng tôi hy vọng giải thích tính quan trọng của hướng chung có thể được tăng cường thêm dưới giả định rằng đặc trưng F_t^(l-1) của các mẫu khác nhau đã được đẩy một chút theo hướng chung cụ thể. Giả định này có thể suy ra rằng tồn tại ít nhất một lần lặp học trong giai đoạn đầu tiên, trong đó F_t^(l-1) và F_t^(l-1) của hầu hết các mẫu có hướng tương tự, và V_t^(l) và V_t^(l) có hướng tương tự. Việc suy dẫn được giới thiệu trong tài liệu bổ sung. Việc tự tăng cường trong tương lai của F_t^(l-1) và V_t^(l) được chứng minh dưới giả định này.

**Bổ đề 3.** (Chứng minh trong tài liệu bổ sung) Cho một mẫu đầu vào x ∈ X và hướng chung C^(l) sau lần lặp thứ t, nếu số hạng nhiễu ε_t^(l) đủ nhỏ để thỏa mãn |V_t^(l)T F_t^(l-1) - V_t^(l)T V_t^(l) C^(l)T C^(l) V_t^(l)T F_t^(l-1)| ≤ |V_t^(l)T F_t^(l-1) - V_t^(l)T ε_t^(l) ε_t^(l)T F_t^(l-1)|, chúng ta có thể thu được cos(V_t^(l), F_t^(l-1)) - cos(V_t^(l), F_t^(l-1)) ≥ 0, trong đó V_t^(l) = W_t^(l)T C^(l)/(C^(l)T C^(l)), và V_t^(l) = W_t^(l)T C^(l)/(C^(l)T C^(l)). △F_t^(l-1) biểu thị sự thay đổi của đặc trưng △F_t^(l-1) = F_{t+1}^(l-1) - F_t^(l-1) được thực hiện bởi mẫu huấn luyện x sau lần lặp thứ t. Để làm điều này, chúng tôi xấp xỉ xem xét sự thay đổi của đặc trưng △F_t^(l-1) sau lần lặp thứ t song song âm với gradient đặc trưng ∇F_t^(l-1), mặc dù nói chặt chẽ, sự thay đổi của đặc trưng không hoàn toàn bằng gradient đối với đặc trưng.

**Định lý 2.** (Chứng minh trong tài liệu bổ sung) Dưới giả định nền tảng, đối với bất kỳ mẫu huấn luyện x, x' ∈ X_c trong loại c, nếu [C^(l)T D_t^(l)|x ∇F_t^(l)|x][C^(l)T D_t^(l)|x' ∇F_t^(l)|x'] > 0 (có nghĩa là F_t^(l)|x và F_t^(l)|x' có loại tương tự trong các lần lặp rất sớm), thì cos(γ_c V_t^(l)|x, F_t^(l-1)|x) ≥ 0, và cos(γ_c V_t^(l), F_t^(l-1)|x) ≥ 0, trong đó γ_c ∈ {-1, +1} là hằng số được chia sẻ bởi tất cả các mẫu trong loại c.

Giải thích việc tăng cường tính quan trọng của hướng chung gây ra bởi tất cả các mẫu huấn luyện trong một loại nhất định. Định lý 2 cho thấy rằng mỗi loại có một hướng huấn luyện chiếm ưu thế. Cụ thể, hãy kết hợp Định lý 2 và giả định nền tảng rằng F_t^(l-1) và △F_t^(l-1) có hướng tương tự, và V_t^(l) và △V_t^(l) có hướng tương tự. Vì vậy, chúng ta có thể xem xét cos(γ_c V_t^(l), F_t^(l-1)|x) ≥ 0 trong Định lý 2 có nghĩa là các đặc trưng của các mẫu huấn luyện trong cùng loại c đều được đẩy theo hướng chung γ_c V_t^(l), làm cho F_t^(l-1)|x rất tương tự với γ_c V_t^(l). Điều này giữ độ tương đồng cao giữa các đặc trưng trong cùng loại c. Mặt khác, cos(γ_c V_t^(l)|x, F_t^(l-1)|x) ≥ 0 trong Định lý 2 có nghĩa là các mẫu huấn luyện trong loại c đều đẩy V_t^(l) theo hướng γ_c E_{x∈X_c}[F_t^(l-1)|x], làm cho V_t^(l) gần đúng song song với γ_c E_{x∈X_c}[F_t^(l-1)|x]. Hiện tượng này được xác minh trong Hình 5, trong đó cos(γ_c V_t^(l), F_t^(l-1)) luôn dương trên các mẫu khác nhau của cùng loại. Phân tích trên cũng giải thích tốt động lực học đằng sau cos(V_t^(l), F_t^(l-1)) - cos(V_t^(l), F_t^(l-1)) ≥ 0 trong Bổ đề 3.

Tóm lại, nếu chúng ta chỉ xem xét các mẫu huấn luyện trong loại mẫu c, thì đặc trưng của các mẫu trong loại này sẽ trở nên ngày càng tương tự nhau. Mặt khác, các mẫu huấn luyện như vậy có tác động huấn luyện tương tự, tức là, đẩy trọng số của các nơ-ron khác nhau đều theo hướng đặc trưng trung bình.

Xác minh thực nghiệm về mối quan hệ giữa đặc trưng F_t^(l-1) và vector V_t^(l). Để làm điều này, chúng tôi đã đo sự thay đổi của giá trị o^(l) = cos(V_t^(l), F_t^(l-1)) - cos(V_t^(l), F_t^(l-1)). Hình 6 báo cáo giá trị o^(l) trung bình trên các mẫu khác nhau tại mỗi lần lặp. Đối với mỗi mẫu x, o^(l) luôn dương và thường tăng theo các lần lặp, điều này xác minh Bổ đề 3. Ngoài ra, giả định cho ε_t^(l) nhỏ trong Bổ đề 3 được xác minh bởi kết quả thực nghiệm trong tài liệu bổ sung.

**Giả định 1.** Chúng tôi giả định rằng MLP mã hóa đặc trưng của rất ít (một hoặc hai) loại trong giai đoạn đầu tiên, trong khi các loại khác chưa được học trong giai đoạn này.

Hình 7 xác minh giả định này. Nó cho thấy rằng chỉ một hoặc hai loại thể hiện độ chính xác cao hơn nhiều so với đoán ngẫu nhiên vào cuối giai đoạn đầu tiên. Điều này cho thấy rằng việc học của MLP bị chi phối bởi các mẫu huấn luyện của một hoặc hai loại trong các lần lặp rất sớm.

Tăng cường tính quan trọng của hướng chung gây ra bởi tất cả các mẫu huấn luyện. Động lực học học tập tổng thể trong giai đoạn đầu tiên có thể được mô tả gần đúng như sau, bằng cách kết hợp Định lý 2 và Giả định 1. Tác động học tập tổng thể của tất cả các mẫu huấn luyện bị chi phối bởi rất ít loại ĉ. Có hai tác động. Đầu tiên, đặc trưng F_t^(l-1) của các mẫu khác nhau đều được đẩy theo hướng vector γ_ĉ V_t^(l), trong đó γ_ĉ được xác định bởi loại/các loại chi phối ĉ. Thứ hai, V_t^(l) được đẩy theo hướng γ_ĉ E_{x∈X_ĉ}[F_t^(l-1)|x]. Do đó, đặc trưng F_t^(l-1) của các mẫu khác nhau và γ_ĉ V_t^(l) tăng cường lẫn nhau, giống như một hệ thống tự tăng cường, trong kịch bản mà F_t^(l-1) và △F_t^(l-1) của hầu hết các mẫu có hướng tương tự, và V_t^(l) và △V_t^(l) có hướng tương tự (xem giả định nền tảng). Nói cách khác, thành phần dọc theo hướng chung C^(l) V_t^(l)T trong △W_t^(l) = C^(l) V_t^(l)T + ε_t^(l)T sẽ được tăng cường thêm.

Giải thích độ tương đồng đặc trưng tăng và độ tương đồng gradient tăng. Như đã đề cập, đặc trưng F_t^(l-1) của các mẫu khác nhau liên tục được đẩy theo cùng vector γ_ĉ V_t^(l). Do đó, độ tương đồng giữa các đặc trưng của các mẫu khác nhau E_{x,x'∈X}[cos(F_t^(l-1)|x, F_t^(l-1)|x')] tăng trong giai đoạn đầu tiên. Mặt khác, độ tương đồng tăng giữa các gradient đặc trưng có thể được giải thích từ hai quan điểm. (1) Độ tương đồng đặc trưng tăng trên các mẫu khác nhau làm cho các mẫu huấn luyện khác nhau tạo ra trạng thái cổng D_t^(l) tương tự trong mỗi lớp ReLU. Độ tương đồng tăng giữa trạng thái cổng của mỗi lớp ReLU trên các mẫu khác nhau dẫn đến độ tương đồng tăng giữa gradient đặc trưng trên các mẫu khác nhau E_{x,x'∈X_c}[cos(∇F_t^(l-1)|x, ∇F_t^(l-1)|x')] của cùng loại. (2) Quan điểm khác là thành phần dọc theo hướng chung C^(l) V_t^(l)T trong △W_t^(l) được tăng cường trong giai đoạn đầu tiên. Bởi vì C^(l) biểu thị hướng trọng số chính của mỗi cột thứ i w_{t,i}^(l) của W_t^(l), tức là, mỗi "nơ-ron giả" w_{t,i}^(l) được tối ưu hóa theo hướng chung C^(l). Eq.(3) đã chứng minh rằng độ tương đồng cosin tăng giữa w_{t,i}^(l) và C^(l) cho tất cả "nơ-ron giả" sẽ dẫn đến độ tương đồng tăng giữa gradient đặc trưng trên các mẫu khác nhau.

Làm thế nào để thoát khỏi giai đoạn đầu tiên? Trong giai đoạn đầu tiên, MLP chỉ khám phá một hướng duy nhất để tối ưu hóa một hoặc hai loại. Tuy nhiên, việc tối ưu hóa một hoặc hai loại sẽ sớm bão hòa, và gradient chủ yếu đến từ các mẫu huấn luyện của các loại khác, điều này làm xáo trộn vai trò chi phối của một hoặc hai loại trong việc học của MLP. Do đó, tác động học tập của các mẫu huấn luyện từ các loại khác nhau có thể xung đột với nhau. Vì vậy, hệ thống tự tăng cường bị phá hủy, và việc học của MLP bước vào giai đoạn thứ hai.

### 4.3 Giảm thiểu về mặt lý thuyết việc giảm đa dạng đặc trưng

Trong các phần trước, chúng tôi đã khám phá và giải thích hiện tượng hai giai đoạn cơ bản nhưng phản trực giác với MLP. Đây là đóng góp đặc biệt của nghiên cứu này, điều này đã không được giải thích về mặt lý thuyết trong thời gian dài. Ngoài ra, chúng tôi thấy rằng chúng ta có thể sử dụng các phát hiện trên để giải thích rằng bốn phép toán điển hình thường có thể giảm thiểu việc giảm đa dạng đặc trưng, tức là, chuẩn hóa, động lượng, khởi tạo và chính quy hóa L2. Mặc dù các phép toán này đã được sử dụng rộng rãi, các nghiên cứu trước đây đã thất bại trong việc giải thích về mặt lý thuyết hiệu quả của chúng. Để làm điều này, phân tích của chúng tôi có thể giải thích khả năng cao cho các phép toán như vậy để giảm thiểu việc giảm đa dạng đặc trưng, nhưng nó không phải là chứng minh của điều kiện đủ nghiêm ngặt hoặc điều kiện cần thiết cho đa dạng đặc trưng.

**Chuẩn hóa.** Dựa trên phân tích lý thuyết, chúng tôi giải thích rằng các phép toán chuẩn hóa (ví dụ: chuẩn hóa batch (BN)) có thể giảm thiểu việc giảm đa dạng đặc trưng trong giai đoạn đầu tiên. Cụ thể, theo Định lý 2, hệ thống tự tăng cường của việc giảm đa dạng đặc trưng đòi hỏi đặc trưng F_t^(l) của bất kỳ hai mẫu huấn luyện x và x' trong cùng loại phải tương tự nhau. Tuy nhiên, lớp BN ngăn cản đặc trưng F_t^(l) của các mẫu khác nhau tương tự nhau, bởi vì đặc trưng trung bình F̄_t^(l) = E_{x∈X}[F_t^(l)|x] được trừ khỏi đặc trưng của tất cả các mẫu, tức là, F'^(l)|x = F_t^(l)|x - F̄_t^(l). Do đó, đặc trưng F'^(l) của các mẫu khác nhau không còn tương tự nhau, do đó có nhiều khả năng phá vỡ hệ thống tự tăng cường. Vui lòng xem tài liệu bổ sung để biết thêm thảo luận.

Chúng tôi đã thực hiện thí nghiệm để xác minh phân tích trên. Chúng tôi đã so sánh MLPs được huấn luyện với và không có lớp BN. Cụ thể, chúng tôi đã thêm một lớp BN sau mỗi lớp tuyến tính để xây dựng MLPs. Hình 8(a) cho thấy rằng độ tương đồng đặc trưng trong MLPs với lớp BN tiếp tục giảm. Điều này xác minh rằng các lớp BN đã giảm thiểu việc giảm đa dạng đặc trưng. Ngoài ra, dựa trên phân tích của chúng tôi, chúng tôi đã đơn giản hóa thêm lớp BN, điều này cũng có thể giảm thiểu việc giảm đa dạng đặc trưng. Vui lòng xem tài liệu bổ sung để biết thêm chi tiết.

**Động lượng.** Dựa trên giải thích lý thuyết cho việc giảm đa dạng đặc trưng, chúng tôi có thể giải thích rằng động lượng trong descent gradient có thể giảm thiểu hiện tượng này. Dựa trên Bổ đề 3, hệ thống tự tăng cường của việc giảm đa dạng đặc trưng đòi hỏi trọng số dọc theo các hướng khác ε_t^(l) phải đủ nhỏ. Tuy nhiên, bởi vì phép toán động lượng tăng cường ảnh hưởng của trọng số nhiễu được khởi tạo W_t=0^(l), nó tăng cường các giá trị kỳ dị của ε_t^(l), ở một mức độ nào đó, do đó giảm thiểu việc giảm đa dạng đặc trưng. Cụ thể, hệ số động lượng m lớn hơn thường giảm thiểu việc giảm đa dạng đặc trưng nhiều hơn.

Vui lòng xem tài liệu bổ sung để biết thảo luận chi tiết. Để xác minh phân tích trên, chúng tôi đã huấn luyện MLPs với m = 0, 0.5, 0.9, tương ứng. Việc xác minh tuyên bố của chúng tôi rằng giá trị m lớn hơn thường giảm thiểu việc giảm đa dạng đặc trưng nhiều hơn được giới thiệu trong tài liệu bổ sung.

**Khởi tạo.** Chúng tôi giải thích rằng việc khởi tạo MLPs cũng ảnh hưởng đến việc giảm đa dạng đặc trưng. Theo Bổ đề 3, hệ thống tự tăng cường như vậy đòi hỏi trọng số dọc theo các hướng khác ε_t^(l) phải đủ nhỏ. Tuy nhiên, bởi vì việc tăng phương sai của trọng số được khởi tạo W_t=0^(l) sẽ tăng các giá trị kỳ dị của ε_t^(l), giảm thiểu việc giảm đa dạng đặc trưng. Vui lòng xem tài liệu bổ sung để biết thảo luận chi tiết.

Chúng tôi đã thực hiện thí nghiệm để xác minh tuyên bố trên bằng cách so sánh MLPs được huấn luyện bằng các phép khởi tạo khác nhau với phương sai khác nhau. Chúng tôi đã sử dụng γ để kiểm soát phương sai của việc khởi tạo, tức là, W_t=0^(l) ~ N(0, γ²/var), trong đó var là hằng số. Hình 8(b) xác minh rằng việc khởi tạo với phương sai lớn đã giảm thiểu việc giảm đa dạng đặc trưng.

**Chính quy hóa L2.** Chúng tôi cũng giải thích rằng hệ số nhỏ của chính quy hóa L2 có thể giảm thiểu việc giảm đa dạng đặc trưng. Tức là, tổng mất mát L(W_t) = L_CE(W_t) + λ||W_t||²₂, trong đó L_CE(W_t) đại diện cho mất mát entropy chéo và λ||W_t||²₂ biểu thị mất mát chính quy hóa L2. Như đã đề cập, việc giảm đa dạng đặc trưng đòi hỏi các giá trị kỳ dị của ε_t^(l) phải đủ nhỏ. Tuy nhiên, bởi vì hệ số λ nhỏ hơn của chính quy hóa L2 tạo ra các giá trị kỳ dị lớn hơn của ε_t^(l), nó giảm thiểu việc giảm đa dạng đặc trưng. Vui lòng xem tài liệu bổ sung để biết thảo luận chi tiết. Việc xác minh rằng hệ số nhỏ hơn giảm thiểu việc giảm đa dạng đặc trưng nhiều hơn được giới thiệu trong tài liệu bổ sung.

## 5 Kết luận

Trong bài báo này, chúng tôi thấy rằng trong giai đoạn đầu của quá trình huấn luyện, MLP thể hiện hiện tượng hai giai đoạn cơ bản nhưng phản trực giác, tức là, đa dạng đặc trưng tiếp tục giảm trong giai đoạn đầu tiên. Chúng tôi giải thích hiện tượng này bằng cách phân tích động lực học học tập của MLP. Hơn nữa, chúng tôi giải thích lý do tại sao bốn phép toán điển hình có thể giảm thiểu việc giảm đa dạng đặc trưng.
