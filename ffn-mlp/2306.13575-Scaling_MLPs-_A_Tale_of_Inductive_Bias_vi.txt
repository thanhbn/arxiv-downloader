# 2306.13575.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/ffn-mlp/2306.13575.pdf
# Kích thước file: 9234763 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Mở rộng MLPs: Một Câu chuyện về Độ lệch Quy nạp
Gregor Bachmann∗, Sotiris Anagnostidis∗, Thomas Hofmann
ETH Zürich, Thụy Sĩ
Tóm tắt
Trong công trình này, chúng tôi xem xét lại khối xây dựng cơ bản nhất trong học sâu, mạng perceptron đa lớp (MLP), và nghiên cứu giới hạn hiệu suất của nó trên các tác vụ thị giác. Những hiểu biết thực nghiệm về MLPs quan trọng vì nhiều lý do. (1) Với tường thuật gần đây "ít độ lệch quy nạp hơn sẽ tốt hơn", được phổ biến do các transformers vượt trội hơn các mô hình tích chập, việc khám phá giới hạn của giả thuyết này là tự nhiên. Với mục đích đó, MLPs cung cấp một môi trường thử nghiệm lý tưởng, vì chúng thiếu bất kỳ độ lệch quy nạp đặc thù cho thị giác nào. (2) MLPs gần như hoàn toàn là nhân vật chính trong tài liệu lý thuyết học sâu do tính đơn giản toán học của chúng, phục vụ như một proxy để giải thích các hiện tượng thực nghiệm quan sát được cho các kiến trúc phức tạp hơn. Ngạc nhiên thay, các điểm dữ liệu thực nghiệm cho MLPs rất khó tìm thấy trong tài liệu, đặc biệt khi kết hợp với các giao thức tiền huấn luyện lớn. Sự khác biệt giữa thực tiễn và lý thuyết này đáng lo ngại: Liệu MLPs có phản ánh những tiến bộ thực nghiệm được thể hiện bởi các mô hình thực tế? Hay các nhà lý thuyết cần suy nghĩ lại về vai trò của MLPs như một proxy? Chúng tôi cung cấp hiểu biết về cả hai khía cạnh này. Chúng tôi chỉ ra rằng hiệu suất của MLPs được cải thiện đáng kể với quy mô (95% trên CIFAR10, 82% trên CIFAR100, 58% trên ImageNet ReaL), nhấn mạnh rằng việc thiếu độ lệch quy nạp thực sự có thể được bù đắp. Chúng tôi quan sát thấy rằng MLPs bắt chước hành vi của các đối tác hiện đại của chúng một cách trung thực, với một số thành phần trong thiết lập học tập tuy nhiên thể hiện hành vi mạnh mẽ hơn hoặc bất ngờ. Do hiệu quả tính toán vốn có của chúng, các thí nghiệm tiền huấn luyện lớn trở nên dễ tiếp cận hơn cho các nhà nghiên cứu học thuật. Tất cả các thí nghiệm của chúng tôi đều được chạy trên một GPU duy nhất.

Hình 1: Lỗi kiểm tra trên CIFAR100 như một hàm của PFLOPS.

∗Đóng góp bằng nhau. Liên hệ {gregorb, sanagnos}@ethz.ch. Code và checkpoints có sẵn tại https://github.com/gregorbachmann/scaling_mlps
Hội nghị lần thứ 37 về Hệ thống Xử lý Thông tin Thần kinh (NeurIPS 2023).arXiv:2306.13575v3 [cs.LG] 3 Oct 2023

--- TRANG 2 ---
1 Giới thiệu
Học sâu đã trải qua tiến bộ thực nghiệm to lớn trong những thập kỷ qua. Các phương pháp tiếp cận thống trị trong thực tế ngày nay dựa vào các mô hình rất lớn, đã được tiền huấn luyện sau đó được tinh chỉnh cho tác vụ cụ thể. Đối với xử lý ngôn ngữ tự nhiên, những mô hình này thường là một biến thể nào đó của kiến trúc Transformer (Vaswani et al., 2017), trong khi ở thị giác máy tính, cả mô hình tích chập và dựa trên transformer đều rất phổ biến (He et al., 2015; Tan and Le, 2020; Dosovitskiy et al., 2021). Mặt khác, hiểu biết lý thuyết về những tiến bộ này vẫn rất kém và khoảng cách giữa thế giới lý thuyết và thực tiễn đang tăng lên với tốc độ báo động. Một khía cạnh của khoảng cách này là họ các mô hình được điều tra; do tính đơn giản toán học, các công trình lý thuyết phần lớn tập trung vào các perceptron đa lớp đơn giản (MLPs). Bao gồm một chuỗi các phép nhân ma trận không có cấu trúc, xen kẽ với các phi tuyến tính theo từng phần tử, MLP phục vụ như một môi trường thử nghiệm lý tưởng để phân tích các hiện tượng thực nghiệm được thể hiện bởi các mô hình phức tạp hơn được sử dụng trong thực tế. Do hiệu suất kém hơn, MLPs hiếm khi được sử dụng và rất ít được biết về hành vi của chúng trong các thiết lập hiện đại hơn. Ví dụ, theo hiểu biết của chúng tôi, không có một kết quả đã xuất bản nào trình bày MLP được huấn luyện trên ImageNet1k, tiêu chuẩn đánh giá de-facto trong thị giác, chứ đừng nói đến bất kỳ nghiên cứu tiền huấn luyện/học chuyển giao nào. Việc thiếu dữ liệu thực nghiệm này đáng lo ngại vì lý thuyết nhằm hiểu các đặc tính của kiến trúc hiện đại thông qua góc nhìn của MLPs, tuy nhiên chỉ có ít đánh giá được thực hiện về mức độ hoạt động tốt của proxy như vậy. Điều này đặt ra câu hỏi,

Liệu MLPs có phản ánh những tiến bộ thực nghiệm được thể hiện bởi các mô hình thực tế? (1)

Điều tra MLPs không chỉ thú vị cho lý thuyết mà còn cho thực tế. Với Vision Transformer (ViT) vượt trội hơn các đối thủ tích chập trong các thiết lập quy mô rất lớn, vai trò của độ lệch quy nạp gần đây đã bị đặt câu hỏi. Vì ViT được trang bị ít độ lệch quy nạp hơn đáng kể cho thị giác so với các mô hình tích chập (ví dụ: nó thiếu tính bất biến dịch chuyển), một tường thuật mới gần đây đã xuất hiện:

Ở quy mô tính toán lớn, có ít độ lệch quy nạp hơn sẽ có lợi cho hiệu suất. (2)

Nhiều bằng chứng hơn cho giả thuyết này đã được thu thập dưới dạng MLP-Mixer (Tolstikhin et al., 2021), một kiến trúc với độ lệch quy nạp còn ít hơn, chỉ dựa vào các perceptron đa lớp như các bộ xử lý và trộn patch. Kiến trúc MLP là ứng cử viên lý tưởng để thử nghiệm giới hạn của giả thuyết như vậy, vì nó thể hiện ít độ lệch quy nạp nhất cho thị giác do tính bất biến của nó với các hoán vị của pixels. Thật không may, quy mô mà Transformers và MLP-Mixers bắt đầu vượt trội hơn các mô hình tích chập nằm ngoài tầm với của hầu hết các nhà nghiên cứu, đòi hỏi hàng tỷ hình ảnh có chú thích và hàng nghìn TPUs. Do đó, chúng tôi mong đợi quy mô yêu cầu tương tự cho MLPs và do đó thay vào đó điều tra giả thuyết yếu hơn sau:

Việc thiếu độ lệch quy nạp có thể được bù đắp bằng cách mở rộng tính toán. (3)

tức là chúng tôi nhằm đo lường mức độ thiếu độ lệch quy nạp cản trở hiệu suất ngay cả khi một mô hình được áp dụng số lượng tham số lớn và được huấn luyện trên các tập dữ liệu với nhiều ví dụ (mặc dù nhỏ hơn so với những gì được sử dụng trong Dosovitskiy et al. (2021)).

Trong công trình này, chúng tôi cung cấp câu trả lời cho câu hỏi 1 và cung cấp thêm bằng chứng cho giả thuyết 2 và 3 bằng cách điều tra chúng ta có thể đẩy hiệu suất thực nghiệm của các mô hình được xây dựng hoàn toàn từ việc kết hợp nhiều khối MLP đến đâu. Chúng tôi đưa ra những câu trả lời phần lớn tích cực cho câu hỏi 1, quan sát thấy rằng MLPs hành xử rất tương tự như các đối tác hiện đại của chúng khi được áp dụng quy mô, tức là hiệu suất của chúng tăng theo cách có thể dự đoán như một luật lũy thừa trong số lượng tham số và kích thước mẫu, giống như Hestness et al. (2017, 2019); Kaplan et al. (2020); Zhai et al. (2022) (xem ví dụ Hình 1). Tuy nhiên, trái ngược với công trình trước đó, chúng tôi thấy rằng MLPs tối ưu về tính toán phân bổ ngân sách của chúng mạnh mẽ hơn vào kích thước mẫu, một lần nữa nhấn mạnh độ lệch quy nạp nhỏ của chúng. Trong khi chính quy hóa dưới dạng tăng cường dữ liệu cũng hữu ích cho CNNs, vai trò của nó được khuếch đại đáng kể cho MLPs ngay cả ở kích thước mẫu lớn, dẫn đến sự suy giảm nghiêm trọng nếu tắt đi. Chúng tôi tiếp tục điều tra cách độ lệch ẩn của SGD ảnh hưởng đến hiệu suất, và chúng tôi thực hiện một khám phá rất phản trực giác: trái ngược với CNNs, chúng tôi thấy rằng kích thước batch lớn hơn khái quát hóa tốt hơn đáng kể cho MLPs. Kết quả này đặt câu hỏi về tính hợp lệ của vai trò proxy mà MLP đóng trong các công trình lý thuyết điều tra độ lệch ẩn của SGD. Mặc dù, như mong đợi, quy mô được sử dụng trong công trình này không đủ cho giả thuyết 2, chúng tôi cung cấp bằng chứng mạnh mẽ cho 3, mà chúng tôi xem như một bước đầu quan trọng. Chúng tôi quan sát thấy rằng quy mô đủ lớn thực sự đủ để khắc phục độ lệch quy nạp xấu có trong MLPs, dẫn đến hiệu suất downstream đáng ngạc nhiên mạnh mẽ, ví dụ ≈95% trên CIFAR10, ≈82% trên CIFAR100 và ≈58% trên ImageNet ReaL. Tóm lại, chúng tôi có những đóng góp sau:

• Chúng tôi lấp đầy khoảng trống giữa lý thuyết và thực tiễn, cung cấp những kết quả đầu tiên cho MLPs được huấn luyện trong các thiết lập hiện đại.
• Chúng tôi chỉ ra rằng MLPs hầu hết hành xử tương tự như các đối tác hiện đại của chúng, làm cho chúng trở thành proxy tốt cho lý thuyết. Tuy nhiên, chúng tôi quan sát thấy rằng vai trò của chính quy hóa và độ lệch ẩn của SGD khác biệt đáng kể và do đó lý thuyết cần thích ứng.
• Chúng tôi cung cấp thêm bằng chứng rằng độ lệch quy nạp không quan trọng ở quy mô lớn, chỉ ra rằng ngay cả các kiến trúc "xấu" như MLPs cũng có thể đạt được hiệu suất downstream mạnh mẽ. Tuy nhiên, chúng tôi xác định sự chuyển đổi trong tính tối ưu về tính toán, chỉ ra rằng MLPs tối ưu đầu tư tính toán của chúng đáng kể hơn vào kích thước tập dữ liệu so với kích thước mô hình.

2 Bối cảnh
Các Công trình Lý thuyết. MLP đã phục vụ như đối tượng nghiên cứu chính cho các công trình lý thuyết trong học sâu trên các lĩnh vực khác nhau. Các kết quả nền tảng cho các lĩnh vực như hội tụ của mạng thần kinh được huấn luyện SGD (Mei et al., 2018; Du et al., 2019; Zou et al., 2020; Li and Yuan, 2017; Saxe et al., 2014), hầu hết các giới hạn khái quát hóa (Arora et al., 2019b; Mei and Montanari, 2021; Jacot et al., 2018; Allen-Zhu et al., 2019a), lợi ích của việc tham số hóa quá mức (Neyshabur et al., 2019; Allen-Zhu et al., 2019b; Arora et al., 2018), độ lệch ẩn của SGD hướng tới các giải pháp thuận lợi (Soudry et al., 2018; Neyshabur et al., 2014; Chizat and Bach, 2020), tính chất truyền tín hiệu (Poole et al., 2016; Schoenholz et al., 2017) và luật mở rộng (Bahri et al., 2021; Maloney et al., 2022) đều phần lớn thu được cho MLPs. Để trích dẫn cuốn sách Principles of Deep Learning Theory rất có ảnh hưởng (Roberts et al., 2022):

"MLPs là những kiến trúc mạng thần kinh đơn giản nhất dựa trên ý tưởng xếp chồng này, và do đó cung cấp một mô hình tối thiểu cho một lý thuyết hiệu quả về học sâu."

Cũng có một số công trình lý thuyết nghiên cứu các thiết lập hiện đại hơn như mạng tích chập hoặc dựa trên transformer bao gồm Arora et al. (2019a); Gunasekar et al. (2018); Brutzkus and Globerson (2017); Hron et al. (2020) để kể tên một vài, nhưng trọng tâm lý thuyết chính theo hiểu biết của chúng tôi vẫn là kiến trúc MLP. Do đó, chúng tôi tin rằng việc khám phá giới hạn của proxy lý thuyết như vậy trong các thiết lập thực tế là quan trọng.

MLPs. Perceptron đa lớp có nguồn gốc từ Rosenblatt (1958), phục vụ như một mở rộng của Perceptron cổ điển với các lớp ẩn tuy nhiên được cố định ở khởi tạo ngẫu nhiên. Ivakhnenko et al. (1965) đã phát minh ra phương pháp đầu tiên để cập nhật các lớp ẩn thông qua tự tổ chức. Amari (1967) sau đó đưa ra ý tưởng huấn luyện các tham số với gradient descent ngẫu nhiên. Về mặt toán học, một MLP có độ sâu L∈N có thể được mô tả rất hiệu quả; cho một đầu vào x∈Rd, nó áp dụng một chuỗi các biến đổi tuyến tính, xen kẽ với phi tuyến tính theo từng phần tử σ:R→R:

z(l)=W(l)x(l−1)→x(l)=σ(z(l))

--- TRANG 3 ---
Patch 1
Patch 2
Patch 3
Patch 4MLP 1
MLP 2Trộn Patch Xử lý PatchMLP-Mixer MLPHình 3: Mô tả đơn giản về sự khác biệt giữa MLP-Mixer và MLP.

trong đó chúng tôi định nghĩa x(0):=x và W(l)∈Rdl×dl−1 với l=1,...,L là các ma trận trọng số có thể học. Để dễ đọc, chúng tôi bỏ qua các bias. Tính đơn giản toán học này làm cho MLP trở thành một mô hình rất hấp dẫn để nghiên cứu từ góc độ lý thuyết (mặc dù vẫn rất xa vời so với điều tầm thường) và thực sự nhiều công trình đóng khung kết quả của họ xung quanh lớp mô hình tổng quát hơn này. Khi được sử dụng cho thị giác, tensor đầu vào x∈Rh×w×3 được làm phẳng thành một vector vec(x)∈R3hw và sau đó được truyền qua MLP. Chú ý cách kiến trúc như vậy hoàn toàn thiếu tính cục bộ và chia sẻ trọng số, mọi đơn vị chỉ đơn giản xử lý toàn bộ hình ảnh cùng một lúc. Đáng lo ngại hơn, phép vector hóa vec có thể được áp dụng theo bất kỳ cách nào, tức là bất kỳ hoán vị nào của x đều trông giống hệt nhau đối với MLP.

Chúng tôi muốn nhấn mạnh rằng tất nhiên MLPs không hoàn toàn không có độ lệch quy nạp, theo nghĩa chúng khuyến khích học cấu trúc tính năng phân cấp. Mặt khác, không có độ lệch quy nạp đặc thù cho thị giác trong MLPs, đây là thiết lập chính mà chúng tôi điều tra ở đây. Chúng tôi tham khảo Battaglia et al. (2018) để có cách xử lý sâu hơn về độ lệch quy nạp.

Tích chập. MLP là một mô hình rất tổng quát và không có cấu trúc nào được tích hợp vào nó để làm cho nó phù hợp hơn cho các tác vụ thị giác. Mặt khác, tích chập được thiết kế đặc biệt cho thị giác với các đặc tính mong muốn được tích hợp vào mô hình. Tích chập có thể được xem như một trường hợp đặc biệt của MLP, trong đó ma trận trọng số W rất có cấu trúc bằng cách thưa thớt và có các mục được chia sẻ, dẫn đến học tập cục bộ về không gian. Điều này có thể được minh họa dễ dàng nhất trong trường hợp tích chập hình ảnh 2×3×1 x với bộ lọc 2×2 f như phép nhân ma trận sau:

f∗x=Wfvec(x) = [f1f20f3f40; 0f1f20f3f4]vec(x)

Ở đây vec biểu thị sơ đồ vector hóa tiêu chuẩn, theo hàng để làm phẳng hình ảnh. Thay vì hoạt động với ma trận dày đặc như MLP, tích chập sử dụng ma trận có cấu trúc Wf được thiết kế riêng cho tác vụ thị giác, dẫn đến độ lệch quy nạp tốt hơn. Hơn nữa, tích chập thể hiện tính đẳng biến dịch chuyển, tức là các dịch chuyển của hình ảnh được xử lý tương đương với bản gốc. Quan trọng là, trái ngược với MLP, tích chập bị ảnh hưởng nghiêm trọng nếu một hoán vị được áp dụng cho hình ảnh.

Vision Transformer. Được truyền cảm hứng từ những thành công trong NLP, gần đây kiến trúc Transformer đã được điều chỉnh cho thị giác (Dosovitskiy et al., 2021). Một hình ảnh x∈Rh×w×3 được chia thành các patch nhỏ hơn (còn gọi là tokens) và mỗi patch như vậy được nhúng tuyến tính (xem Hình 2) và được tăng cường bằng cái gọi là positional embedding, đánh dấu vị trí không gian của nó trong hình ảnh. Các embeddings thu được sau đó được xử lý bởi các lớp self-attention nơi các patch có thể trao đổi thông tin, và các lớp MLP, được chia sẻ giữa các patch và biến đổi chúng riêng lẻ. Trong khi độ lệch quy nạp của ViT chắc chắn yếu hơn so với CNN (nó thiếu tính đẳng biến dịch chuyển), việc patch hóa và chia sẻ tham số vẫn làm cho kiến trúc phù hợp cho thị giác.

MLP-Mixer. Tương tự như ViT, MLP-Mixer cũng hoạt động với hình ảnh được patch hóa (Tolstikhin et al., 2021). Không giống như ViT, token-mixing không được thực hiện bằng self-attention mà thay vào đó một khối MLP khác được sử dụng để trao đổi thông tin giữa các patch. Chúng tôi muốn nhấn mạnh rõ ràng sự khác biệt giữa MLP-Mixer và MLP: MLP-Mixer hoạt động trên các patch, trong đó ở mỗi khối nó áp dụng một MLP được chia sẻ cho mỗi patch để xử lý, và một MLP khác để trộn các patch dọc theo các kênh. Chúng tôi trực quan hóa sự khác biệt trong Hình 3 để rõ ràng. Chúng tôi một lần nữa muốn nhấn mạnh rằng việc chia hình ảnh thành các patch và chia sẻ tham số giữa chúng làm tăng đáng kể lượng độ lệch quy nạp, so với MLP tiêu chuẩn.

4

--- TRANG 4 ---
CIFAR10 CIFAR100 TINYIMAGENET IMAGENET
S-MLP (@ 100E) 54.2 28.8 8.5 9.2
S-MLP + DA (@ 1000E) 68.9 43.3 25.2 24.3
S-MLP + DA (@ 5000E) 72.3 44.5 27.3 26.8
B-MLP (@ 100E) 58.1 30.5 8.9 8.7
B-MLP + DA (@ 1000E) 70.1 48.3 27.2 28.7
B-MLP + DA (@ 5000E) 75.4 50.4 31.2 31.7
RESNET182+ DA 93.2 75.6 68.9 69.7

Bảng 1: Độ chính xác kiểm tra (tính theo %) không có bất kỳ tiền huấn luyện nào. S-MLP có độ sâu 6 và chiều rộng 1024 trong khi B-MLP có độ sâu 6, chiều rộng 1024 và hệ số mở rộng 4.

Patch hóa. Như đã nêu trên, ViTs và Mixers chủ yếu có được độ lệch quy nạp của chúng thông qua việc chia hình ảnh thành các patch. Lựa chọn này dường như có lợi ngay cả đối với các kiến trúc đã có độ lệch quy nạp mạnh, như ConvMixer (Trockman and Kolter, 2022), trong đó các tích chập được thực hiện trên các patch riêng lẻ. Metaformer rất gần đây (Yu et al., 2022) tiếp tục chỉ ra rằng ngay cả việc pooling không gian đơn giản thay vì attention cũng có thể dẫn đến hiệu suất mạnh mẽ nếu hình ảnh được patch hóa. Trong khi thành công của cơ chế này chắc chắn đáng được điều tra thêm, trong công trình này chúng tôi quyết định tập trung có chủ ý vào MLPs vì chúng đặc biệt thiếu loại bias này.

3 Kiến trúc
Chúng tôi nghiên cứu các biến thể khác nhau của kiến trúc MLP, bắt đầu từ thiết lập vanilla tiêu chuẩn và sau đó thêm các thành phần khác như kết nối residual và các lớp bottleneck.

MLP Tiêu chuẩn. Như điểm khởi đầu đầu tiên, chúng tôi điều tra các MLP đơn giản với activation ReLU và thiết kế đẳng hướng, tức là ngoại trừ lớp đầu tiên, mọi lớp đều có cùng chiều rộng m∈N. Để tránh bất ổn khi huấn luyện, chúng tôi tiếp tục nâng cao MLP tiêu chuẩn với layer normalizations (Ba et al., 2016) được đặt sau các activations. Do đó, chúng tôi kết hợp một số khối có dạng

Block(z) = σ(WLN(z))

với W∈Rm×m. Để nhúng hình ảnh x∈Rd×d×3, chúng tôi sử dụng lớp tuyến tính emb(x) = Wembvec(x) với Wemb∈Rm×3d2. Lớp embedding như vậy rất quan trọng vì đối với hình ảnh độ phân giải cao, 3d2 có thể khá lớn và do đó m cần được chọn nhỏ hơn. Chúng tôi thấy thực nghiệm rằng thiết kế mạng như vậy là lựa chọn tối thiểu để đảm bảo huấn luyện thành công trên tất cả các quy mô của số lượng tham số và kích thước mẫu. Chúng tôi sẽ sử dụng từ viết tắt S-MLP để biểu thị kiến trúc như vậy.

Inverted Bottleneck MLP. Được truyền cảm hứng bởi Lin et al. (2015); Tolstikhin et al. (2021), chúng tôi thêm cấu trúc bottleneck vào khối MLP cũng như skip connections như sau:

Block(z) = z + Wcσ(WeLN(z))

trong đó We∈Rkm×m mở rộng chiều thành km với k∈N và W(c)∈Rm×km co lại về chiều rộng m. Đối với hầu hết các thí nghiệm, chúng tôi đặt k=4. Trong khi việc bổ sung skip connections và lớp bottleneck vào kiến trúc có thể thêm một số lượng độ lệch quy nạp, chúng tôi tin rằng so với các kiến trúc hiện đại, những cải tiến như vậy vẫn không đáng kể. Chúng tôi sẽ biểu thị biến thể này bằng B-MLP.

2Trái ngược với MLPs, ResNet18 được huấn luyện ở độ phân giải hình ảnh gốc.

5

--- TRANG 5 ---
CIFAR10 CIFAR100 STL10 TINY-IN IN REAL
B-6/Wi-1024 69.9±0.1 43.0±0.4 51.5±0.1 47.1±0.1 15.2±0.2 20.3±0.2
B-6/Wi-1024 + DA 91.5±0.02 76.4±0.2 85.0±0.2 62.7±0.1 38.7±0.1 47.0±0.15
B-12/Wi-1024 + DA 94.2±0.05 80.0±0.05 89.9±0.1 69.9±0.4 43.3±0.06 48.6±0.2
B-12/Wi-1024 + DA + TTA 95.5±0.05 82.6±0.2 92.2±0.05 73.1±0.5 51.5±0.1 57.9±0.1

Bảng 2: Độ chính xác Top-1 fine-tuning (tính theo %) khi được tiền huấn luyện trên ImageNet21k. Độ chính xác được tính trung bình qua 3 lần chạy. Để dễ đọc, chúng tôi viết tắt ImageNet thành IN.

4 Thí nghiệm
4.1 Thiết lập

Trong công trình này, chúng tôi chỉ tập trung vào các tác vụ thị giác vì độ lệch quy nạp được hiểu dễ dàng hơn trong thiết lập này. Hơn nữa, hầu hết các công trình lý thuyết tập trung vào các tác vụ phân loại hình ảnh, làm cho nó trở thành một môi trường thử nghiệm tự nhiên để đánh giá hiệu suất của MLPs. Chúng tôi nghiên cứu các tác vụ phổ biến CIFAR10, CIFAR100 (Krizhevsky, 2009), STL10 (Coates et al., 2011), TinyImageNet (Le and Yang, 2015), ImageNet1k để đánh giá, cũng như ImageNet21k (Deng et al., 2009) để tiền huấn luyện. Để giới hạn kích thước của lớp embedding và nhu cầu tính toán, chúng tôi thu nhỏ tất cả hình ảnh xuống độ phân giải 64×64×3 (nếu cần) như đã làm trong Chrabaszcz et al. (2017). Chúng tôi căn giữa và chuẩn hóa tất cả hình ảnh như một bước tiền xử lý. Đối với data augmentations, chúng tôi xem xét random flips và crops cũng như MixUp (Zhang et al., 2018).

4.2 Huấn luyện từ đầu

Chúng tôi bắt đầu khám phá thực nghiệm về MLPs bằng cách huấn luyện chúng từ đầu (tức là không có dữ liệu bổ sung) trên các benchmark thị giác phổ biến. Tất cả mô hình đều được huấn luyện với optimizer LION (Chen et al., 2023) với learning rate η=5e-5. Để chống overfitting, chúng tôi sử dụng label smoothing mạnh α=0.3. Chúng tôi hiển thị độ chính xác kiểm tra kết quả trong Bảng 1. Chúng tôi quan sát thấy rằng cả kiến trúc tiêu chuẩn và bottleneck mà không có bất kỳ data augmentation nào đều bị overfitting, dẫn đến hiệu suất không tối ưu. Tuy nhiên, khi bật lên, data augmentation như một regularizer thực sự phát huy toàn bộ sức mạnh của nó, đẩy hiệu suất lên đáng kể khoảng 20% trên tất cả các tác vụ. Như quan sát trong Lin et al. (2015), kiến trúc inverted bottleneck dẫn đến cải thiện hiệu suất trên tất cả các tập dữ liệu. Mặt khác, học tập chậm lại đáng kể với các augmentations mạnh như MixUp, cho phép huấn luyện lên đến 5000 epochs mà không bị overfitting. Tuy nhiên, so với các baseline hiện đại đơn giản như ResNet18 (He et al., 2015), vẫn còn một sự khác biệt lớn về hiệu suất, nhấn mạnh tầm quan trọng của độ lệch quy nạp trong chế độ mẫu nhỏ. Chúng tôi nhận xét rằng ViTs và MLP-Mixers cũng thể hiện nhiều khó khăn trong học tập hơn nếu kích thước tập dữ liệu nhỏ (Dosovitskiy et al., 2021; Tolstikhin et al., 2021). Chúng tôi cung cấp thêm nghiên cứu ablation trong Phụ lục A.2.

4.3 Học chuyển giao

Trong phần này, chúng tôi nhằm phân tích mức độ có thể chuyển giao của các đặc trưng được học bởi MLPs trên các tác vụ thị giác khác nhau. Khả năng chuyển giao là một trong những đặc điểm nổi bật của học sâu hiện đại, cho phép các chuyên gia fine-tune các mô hình lớn trên tập dữ liệu cụ thể của họ, dẫn đến hiệu suất vượt trội. Theo hiểu biết của chúng tôi, chúng tôi là những người đầu tiên đo lường khả năng chuyển giao của MLPs, điều này rất quan trọng để đánh giá nhằm xây dựng hiểu biết lý thuyết về quy trình. Trong phần này, chúng tôi tập trung vào inverted bottleneck MLP vì nó khái quát hóa tốt hơn và dễ tối ưu hóa hơn. Chúng tôi cung cấp kết quả kép cho MLP tiêu chuẩn trong Phụ lục B.1. Chúng tôi giới hạn k=4 cho hệ số mở rộng và biểu thị bằng B-L/Wi-m một mạng với L khối và chiều rộng m. Để tiền huấn luyện, chúng tôi sử dụng ImageNet21k, tập dữ liệu hình ảnh lớn nhất có sẵn công khai với các lớp được chú thích. Sau khi tiền xử lý tập dữ liệu theo Ridnik et al. (2021), nó bao gồm khoảng 12 triệu hình ảnh và 11 nghìn lớp. Sau đó, chúng tôi tiền huấn luyện MLP với cross-entropy loss trong 800 epochs, sử dụng label smoothing và optimizer LION. Để đảm bảo tải dữ liệu nhanh, chúng tôi dựa vào framework FFCV (Leclerc et al., 2023) cho tất cả các thí nghiệm.

Để đo lường khả năng chuyển giao của các đặc trưng đã học, chúng tôi fine-tune mạng trên tác vụ mới. Chúng tôi cũng nghiên cứu việc huấn luyện một lớp tuyến tính trên đầu các embeddings nhưng hoãn những kết quả đó đến Phụ lục A.3. Chúng tôi một lần nữa khám phá các hiệu ứng của data augmentation trong giai đoạn tiền huấn luyện. Để fine-tuning, chúng tôi sử dụng SGD với momentum với learning rate ηhead=0.01 cho head và ηbody=0.001 cho encoder trong 50 epochs. Chúng tôi nâng cấp hình ảnh CIFAR lên độ phân giải 64×64×3 tại thời điểm fine-tuning để đảm bảo tính tương thích. Chúng tôi hiển thị kết quả fine-tuning trong Bảng 2. Để trực quan hóa các đặc trưng đã học, chúng tôi giới thiệu độc giả quan tâm đến Phụ lục C. Chúng tôi một lần nữa quan sát thấy rằng việc sử dụng data augmentation trong giai đoạn tiền huấn luyện là thiết yếu để huấn luyện thành công, tăng hiệu suất lên đến 30% trong trường hợp CIFAR100. Đáng ngạc nhiên là, các đặc trưng đã học có khả năng chuyển giao cao, cải thiện hiệu suất được báo cáo trước đó trong Bảng 1 một cách đáng kể. Mặc dù tất nhiên được tiền huấn luyện trên một lượng lớn dữ liệu, chúng tôi vẫn muốn nhấn mạnh rằng MLP như vậy trở nên cạnh tranh với ResNet18 được huấn luyện từ đầu cho tất cả các tập dữ liệu, ngoại trừ ImageNet1k nơi hiệu suất giảm ngạc nhiên. Chúng tôi giả định rằng MLPs gặp khó khăn với các phân biệt tinh tế hơn giữa các lớp, kết hợp với độ phân giải giảm của hình ảnh.

Test-Time Augmentations. Đối với ImageNet1k, chúng tôi tiếp tục nhận thấy rằng các đối tượng có xu hướng không được căn giữa, trái ngược với các tập dữ liệu như CIFAR10. Chúng tôi nghi ngờ rằng điều này có thể dẫn đến hiệu suất tương đối yếu hơn. Để kiểm tra điều này, chúng tôi tận dụng test-time augmentations (TTA). Như được giới thiệu bởi Krizhevsky et al. (2012), đối với mỗi hình ảnh kiểm tra, chúng tôi tạo ra một số cố định 100 crops ngẫu nhiên và sử dụng logits trung bình để dự đoán. Chúng tôi quan sát cải thiện đáng kể trên tất cả các tập dữ liệu, đặc biệt đối với ImageNet, chúng tôi có được sự gia tăng khoảng 8%. Điều này thực sự chỉ ra rằng MLPs gặp khó khăn trong việc định vị đối tượng quan tâm, đặc biệt là đối với tác vụ ImageNet1k phức tạp hơn. Việc sử dụng một số lượng lớn crops làm giảm bớt vấn đề này ở một mức độ nào đó. Điều này cũng giải thích tại sao các lợi ích trên các tác vụ như CIFAR10 nhỏ hơn vì các đối tượng ở đó thường được căn giữa hoàn hảo.

Độ chính xác ReaL. Như quan sát trong (Beyer et al., 2020), các nhãn ImageNet không nắm bắt được việc một hình ảnh duy nhất có thể chứa nhiều đối tượng của các lớp khác nhau. Do đó, độ chính xác ImageNet có thể gây hiểu lầm theo nghĩa là các lớp mô hình như mạng tích chập có thể đã thích ứng ngầm với chiến lược gán nhãn cụ thể do việc đánh giá lặp lại trên cùng một tập validation. MLPs rất có thể thiếu sự thích ứng ngầm như vậy vì công trình này theo hiểu biết của chúng tôi là công trình đầu tiên đánh giá chúng trên ImageNet1k. Để giải quyết điều này, Beyer et al. (2020) đã giới thiệu một tập nhãn validation mới nắm bắt tốt hơn bản chất đa nhãn, trong đó một dự đoán được coi là đúng nếu nó khớp với một trong các danh mục có trong hình ảnh. Chúng tôi quan sát thêm những cải thiện rất đáng kể khoảng 7% khi sử dụng ImageNet ReaL.

Nhìn chung, những kết quả này nhấn mạnh rằng độ lệch quy nạp xấu như được thể hiện bởi MLP thực sự có thể được khắc phục nếu được áp dụng đủ quy mô. Đối với lý thuyết, kết quả có hai mặt; trong khi MLPs chứng minh là proxy tốt để hiểu học chuyển giao, data augmentation chứng minh là một thành phần quan trọng. Ngoài ra, test-time augmentations cũng tăng hiệu suất đáng kể. Cả hai thành phần này mặt khác vẫn còn khá ít được nghiên cứu trong các công trình lý thuyết.

Kích thước batch lớn. Chúng tôi tiếp tục thực hiện quan sát phản trực giác rằng huấn luyện với kích thước batch lớn hơn tăng hiệu suất đáng kể cả up- và downstream. Trong Hình 4, chúng tôi vẽ kích thước batch tiền huấn luyện so với độ chính xác downstream tuyến tính kết quả trên CIFAR100 cho số lượng epochs tiền huấn luyện khác nhau. Chúng tôi quan sát thấy rằng trên tất cả thời gian huấn luyện, việc sử dụng kích thước batch lớn hơn dẫn đến hiệu suất tốt hơn đáng kể. Hơn nữa, chúng tôi muốn nhấn mạnh rằng biểu đồ như vậy thậm chí còn ưu ái kích thước batch nhỏ vì những mô hình đó thực hiện nhiều cập nhật gradient hơn cho một số epochs cố định. Hiệu ứng này hoàn toàn trái ngược với các kiến trúc tích chập nơi toàn bộ dòng công trình đã tập trung vào việc bảo tồn hiệu suất của chế độ kích thước batch nhỏ cho những cái lớn hơn (Goyal et al., 2017; You et al., 2017; Hoffer et al., 2017; Keskar et al., 2017). Huấn luyện với kích thước batch lớn mà không suy giảm có mối quan tâm cao vì nó có thể dẫn đến các pipeline huấn luyện hiệu quả hơn tiềm năng vì tính toán có thể được phân đoạn giữa nhiều thiết bị hơn. Quan sát này về kích thước batch tối ưu phù hợp với những kết luận tương tự gần đây trong Transformers (Kaplan et al., 2020; Touvron et al., 2023).

Vai trò của augmentations. Vai trò của data augmentation rất rõ ràng đối với MLPs, phần lớn vì nó cung cấp độ lệch quy nạp gián tiếp cho mô hình. Đáng chú ý là, một mô hình được tiền huấn luyện trên 12 triệu ví dụ mà không có data augmentation cho thấy hiệu suất kém hơn trên CIFAR10 so với một mạng được huấn luyện từ đầu với augmentations được bật. Điều này nhấn mạnh rằng augmentations vượt ra ngoài việc chỉ dẫn đến một tập dữ liệu lớn hơn mà cung cấp cho mô hình các bất biến hữu ích. Chúng tôi điều tra các trọng số đã học một cách sâu sắc trong Phụ lục C, chỉ ra rằng rất rõ ràng, các đặc trưng cục bộ hơn được học nếu data augmentation được sử dụng. Sức mạnh của augmentations đã được chứng minh trước đó thông qua sự ra đời của học tự giám sát (Grill et al., 2020; Caron et al., 2021; Chen et al., 2020). Ngay cả khi huấn luyện trên các nhãn hoàn toàn ngẫu nhiên, nó vẫn cung cấp tín hiệu học tập mạnh mẽ (Anagnostidis et al., 2023).

4.4 Luật mở rộng

Một trong những điều bí ẩn chính trong học sâu là các mạng có xu hướng cải thiện về mặt khái quát hóa khi tính toán, dưới dạng số lượng tham số và kích thước tập dữ liệu, được mở rộng. Gần đây, đã được quan sát trong một số công trình rằng lợi ích của quy mô có thể dự đoán cao, tức là hiệu suất khái quát hóa thể hiện cấu trúc luật lũy thừa khi được vẽ so với tính toán được đo bằng FLOPS (Rosenfeld et al., 2020; Hestness et al., 2017, 2019; Kaplan et al., 2020; Zhai et al., 2022). Dạng hàm gần đây đã được tinh chỉnh thêm (Caballero et al., 2023). Tính chất có thể dự đoán của hiệu suất kiểm tra thậm chí đã được tận dụng để ước tính mô hình tối ưu trước khi huấn luyện (Hoffmann et al., 2022; OpenAI, 2023). Để hiểu đặc điểm quan trọng này của học sâu về mặt lý thuyết, việc phân tích xem MLPs có thể hiện tính chất tương tự hay không là quan trọng.

Hình 5: Lỗi kiểm tra (tính theo %) trên CIFAR10 (trái) và ImageNet1k (phải) khi được chuyển giao tuyến tính như một hàm của PFLOPS, được đo theo Eq.(4), trên thang log-log.

8

--- TRANG 6 ---
Hình 6: Luật lũy thừa trong lỗi đánh giá tuyến tính trên CIFAR100 (tính theo %) khi bị bottleneck bởi số lượng tham số (trái) hoặc số lượng ví dụ (phải), trên thang log-log. Đường chấm chấm trực quan hóa dạng hàm được fitted.

Tính toán. Theo OpenAI (2018), chúng tôi định nghĩa chi phí tính toán C phát sinh từ việc huấn luyện mô hình f trên N ví dụ trong T epochs như

C=FLOP(f)×3×N×T, (4)

trong đó FLOP(f) biểu thị số lượng FLOPs cần thiết để hoàn thành forward pass của f cho một ví dụ duy nhất. Chúng tôi lưu ý rằng số lượng tham số P có trong f đi vào phương trình này một cách ngầm định dưới dạng FLOP(f)∝P. Quan sát rằng một mức độ tính toán nhất định có thể đạt được theo các cách khác nhau, tức là sử dụng nhiều tham số P hơn, huấn luyện trên nhiều ví dụ N hơn, hoặc huấn luyện trong thời gian dài hơn T. Khi phân bổ một mức độ tính toán nhất định một cách tối ưu, người ta quan sát thấy rằng đối với các kiến trúc tích chập và dựa trên transformer, lỗi kiểm tra E(C) như một hàm của tính toán hành xử như một luật lũy thừa

E(C) = a(b+C)^(-α)+E∞, (5)

trong đó a, b, E∞∈R+ và α>0 là hệ số mở rộng xác định tốc độ suy giảm. E∞ biểu thị lỗi không thể giảm được, tức là ngay cả khi tính toán vô hạn được sử dụng, hiệu suất vẫn không hoàn hảo. Lỗi kiểm tra có thể được đo upstream (tức là trên tác vụ tiền huấn luyện) hoặc downstream khi fine-tuning trên tác vụ khác. Chúng tôi điều tra các scheme tiền huấn luyện khác nhau với số lượng ví dụ, số lượng tham số và thời gian huấn luyện khác nhau. Chúng tôi lấy mẫu con ImageNet21k một cách tỷ lệ trên các lớp và tiền huấn luyện các inverted bottleneck MLPs có kích thước khác nhau. Chúng tôi tóm tắt các cấu hình trong Bảng 3.

Sau đó, chúng tôi đo lỗi kiểm tra trên tác vụ downstream CIFAR100 trong Hình 1 cũng như CIFAR10 và ImageNet1k trong Hình 5 bằng cách chuyển giao tuyến tính các đặc trưng đã học (không có test-time augmentations). Phong cách vẽ được truyền cảm hứng từ Zhai et al. (2022). Mỗi điểm trong đường cong là hiệu suất downstream của một MLP, trong đó màu của điểm chỉ ra loại mô hình (xanh biểu thị mô hình nhỏ hơn và đỏ mô hình lớn hơn) và kích thước của điểm chỉ ra số lượng ví dụ tiền huấn luyện. Các điểm được nối bởi một đường chỉ ra thời gian huấn luyện dài hơn trong đó T∈{50,100,200,400,800} được đo bằng epochs. Trong tất cả các thí nghiệm, chúng tôi sử dụng data augmentation để tiền huấn luyện. Chúng tôi quan sát thấy rằng hiệu suất tối ưu về tính toán của MLPs mạnh mẽ thể hiện các đặc điểm của luật lũy thừa với các hệ số α∈{0.12,0.25,0.35}. Điều này rất khuyến khích cho công trình lý thuyết trong tương lai, chỉ ra rằng MLPs thực sự phản ánh hành vi mở rộng của các mô hình hiện đại. Chúng tôi cung cấp kết quả kép cho MLPs tiêu chuẩn trong Phụ lục B.2, lưu ý rằng chúng thể hiện hành vi mở rộng về cơ bản giống nhau, mặc dù với độ dốc và intercept hơi yếu hơn.

Chúng tôi tiếp tục nghiên cứu cách hiệu suất E phát triển khi tính toán bị bottleneck bởi số lượng tham số P hoặc kích thước tập dữ liệu N. Chúng tôi trực quan hóa các luật mở rộng kết quả trong Hình 6. Chúng tôi tìm thấy tốc độ suy giảm rất dốc về mặt tham số P trong đó khoảng αP≈1, trong khi đối với kích thước tập dữ liệu N, chúng tôi xác định tốc độ chậm hơn đáng kể là αN≈0.35. Điều này cho thấy rằng hiệu suất của MLPs bị hạn chế đáng kể hơn bởi kích thước tập dữ liệu, phù hợp với thực tế là MLPs thể hiện độ lệch quy nạp xấu. Chúng tôi điều tra vai trò của kích thước tập dữ liệu và tham số nhiều hơn trong đoạn tiếp theo.

Tham số hoặc ví dụ. Với một mức độ tính toán C cố định, cách tối ưu để phân bổ nó cho số lượng tham số P và số lượng ví dụ N là gì? Để có thể so sánh hơn với công trình trước đó, chúng tôi giả định thời gian huấn luyện cố định T=50. Để trả lời câu hỏi này, chúng tôi theo phương pháp được nêu trong Hoffmann et al. (2022) và vẽ các mô hình tính toán tối ưu được xác định trong Hình 1 cả so với kích thước mô hình P và số lượng ví dụ N. Chúng tôi trực quan hóa kết quả trong Hình 7. Chúng tôi quan sát thực nghiệm rằng số lượng tham số tối ưu P*(C) và kích thước tập dữ liệu N*(C) như một hàm của tính toán C thể hiện hành vi luật lũy thừa có dạng xấp xỉ

P*(C)∝C^0.35    N*(C)∝C^0.65

Trong khi đối với transformers, số lượng ví dụ (hoặc tokens) N và tham số P được mở rộng đều nhau (Hoffmann et al., 2022) (tức là αP≈αN≈0.5), trái ngược chúng tôi quan sát thấy rằng chiến lược tối ưu cho MLPs đầu tư tính toán đáng kể hơn vào kích thước tập dữ liệu N. Điều này là bằng chứng tiếp theo cho độ lệch quy nạp yếu hơn có trong MLPs, cần nhiều ví dụ hơn để được bù đắp.

4.5 Tính khả thi về mặt tính toán

Chúng tôi tin rằng một tính năng thú vị khác của nghiên cứu của chúng tôi là tính khả thi về mặt tính toán, trong khi cùng lúc bảo tồn các đặc điểm chính của tiền huấn luyện quy mô lớn. Tất cả các thí nghiệm của chúng tôi đều được thực hiện trên một GPU NVIDIA RTX A5000 duy nhất với bộ nhớ 24GB. Kết hợp với framework tải dữ liệu được tối ưu hóa mạnh mẽ FFCV (Leclerc et al., 2023) và hiệu quả vốn có của MLPs, chúng tôi có thể thực hiện huấn luyện rất nhanh. Ví dụ, chúng tôi hoàn thành một epoch duy nhất trên ImageNet21k với kiến trúc B-12/Wi-1024, được trang bị 124 triệu tham số, chỉ trong khoảng 450 giây, trong khi biến thể nhỏ hơn B-6/Wi-1024 ở số lượng tham số 74 triệu cần khoảng 250 giây trên phần cứng được chỉ định. Yêu cầu bộ nhớ thấp cho phép chúng tôi huấn luyện với batch-size 16384 mà không cần phải phân đoạn tính toán giữa nhiều GPUs. Chúng tôi so sánh hiệu quả tính toán của MLPs với các mạng đương đại có kích thước tương tự như ResNet-152, ViT-B/4 và ViT-B/8 trong Phụ lục A.5.

5 Các công trình liên quan

Có một số công trình trước đây điều tra MLPs trên các tác vụ thị giác. Lin et al. (2015) nghiên cứu hiệu suất của MLPs trên các tập dữ liệu quy mô nhỏ như CIFAR10. Họ quan sát những cải thiện tương tự khi sử dụng các lớp inverted bottleneck nhưng không nghiên cứu các thiết lập quy mô lớn hơn, học chuyển giao hay thảo luận về ý nghĩa đối với các công trình lý thuyết. Cấu trúc bottleneck được sử dụng trong công trình này cũng đã được điều tra về mặt lý thuyết (Parhi and Nowak, 2021; Shenouda et al., 2023; Parkinson et al., 2023), tiếp tục nhấn mạnh rằng kiến trúc như vậy thể hiện các tính chất mong muốn. Urban et al. (2017) nghiên cứu mức độ cần thiết của tích chập cho hiệu suất tốt và kết luận rằng ngay cả với các kỹ thuật distillation, việc huấn luyện MLPs hiệu quả trên CIFAR10 vẫn rất khó khăn. Các phương pháp khác đã tập trung vào việc làm thưa các lớp fully-connected thông qua huấn luyện tiến hóa (Mocanu et al., 2018; Fernando et al., 2016), nhằm học độ lệch quy nạp tốt từ đầu. Tương tự, Neyshabur (2020) nghiên cứu cách độ lệch quy nạp của MLPs có thể được cải thiện bằng cách làm thưa chúng một cách có hệ thống với thuật toán kiểu LASSO, làm cho chúng giống tích chập hơn. d'Ascoli et al. (2019) mặt khác trước tiên huấn luyện một mạng tích chập trong một khoảng thời gian nhất định và sau đó tiếp tục huấn luyện mạng như một MLP (bằng cách sử dụng sự tương ứng giữa CNNs và MLPs được nhấn mạnh trong Phần 2). Họ chỉ ra rằng hiệu suất tốt có thể đạt được nếu mạng được huấn luyện đủ lâu như một CNN. Trái ngược với các công trình này, mục tiêu của chúng tôi không phải là nâng cao độ lệch quy nạp vốn có của MLPs mà nghiên cứu liệu nó có thể được khắc phục với đủ quy mô hay không.

Sự ra đời của MLP-Mixer (Tolstikhin et al., 2021) đã dẫn đến một loạt công trình theo sau, tương tự sử dụng MLPs như bộ xử lý patch và token mixer (Touvron et al., 2021; Chen et al., 2022; Lian et al., 2022; Guo et al., 2021; Liu et al., 2021). Một lần nữa, chúng tôi nhận xét rằng tất cả những kiến trúc này đều có độ lệch quy nạp đáng kể hơn.

Cuối cùng, chúng tôi muốn nhận xét rằng MLPs được sử dụng thành công trong các lĩnh vực khác như tổng hợp góc nhìn mới (ví dụ NeRF (Mildenhall et al., 2021)).

6 Thảo luận

Trong công trình này, chúng tôi đã khám phá giới hạn của perceptron đa lớp như một kiến trúc cho các tác vụ thị giác. Nghiên cứu của chúng tôi tiết lộ rằng (1) việc thiếu độ lệch quy nạp có thể được bù đắp bằng quy mô và (2) MLPs tạo thành một proxy (phần lớn) chính xác cho các kiến trúc hiện đại, tiếp tục củng cố vai trò của chúng như đối tượng nghiên cứu lý thuyết chính. Tuy nhiên, vai trò của data augmentation và độ lệch ẩn của SGD khác biệt mạnh mẽ đối với MLPs trong thiết lập được xem xét trong công trình này và các công trình lý thuyết nên tính đến điều này. Tiền huấn luyện quy mô lớn của MLPs chứng minh là rất hiệu quả, cho phép các nhà nghiên cứu có ít quyền truy cập vào tài nguyên tính toán hơn nghiên cứu hướng công việc rất thú vị này. Trong khi việc thiếu độ lệch quy nạp không ngăn cản MLPs đạt hiệu suất ấn tượng, nó dẫn đến sự chuyển đổi thú vị trong tính tối ưu về tính toán hướng tới nhiều ví dụ huấn luyện hơn. Việc áp dụng MLPs cho lượng tính toán thậm chí còn lớn hơn tương tự như Zhai et al. (2022), đặc biệt dưới dạng nhiều ví dụ huấn luyện hơn, vẫn là công việc tương lai rất thú vị.

Tài liệu tham khảo

Allen-Zhu, Z., Li, Y., và Liang, Y. (2019a). Learning and generalization in overparameterized neural networks, going beyond two layers. Trong Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., và Garnett, R., biên tập viên, Advances in Neural Information Processing Systems, tập 32. Curran Associates, Inc.

Allen-Zhu, Z., Li, Y., và Song, Z. (2019b). A convergence theory for deep learning via over-parameterization. Trong International Conference on Machine Learning, trang 242–252. PMLR.

Amari, S. (1967). A theory of adaptive pattern classifiers. IEEE Transactions on Electronic Computers, EC-16(3):299–307.

Anagnostidis, S., Bachmann, G., Noci, L., và Hofmann, T. (2023). The curious case of benign memorization. Trong The Eleventh International Conference on Learning Representations.

Arora, S., Cohen, N., và Hazan, E. (2018). On the optimization of deep networks: Implicit acceleration by overparameterization. Trong International Conference on Machine Learning, trang 244–253. PMLR.

Arora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R., và Wang, R. (2019a). On exact computation with an infinitely wide neural net. Trong Neural Information Processing Systems.

Arora, S., Du, S. S., Hu, W., Li, Z., và Wang, R. (2019b). Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. Trong International Conference on Machine Learning.

Ba, J. L., Kiros, J. R., và Hinton, G. E. (2016). Layer normalization.

Bahri, Y., Dyer, E., Kaplan, J., Lee, J., và Sharma, U. (2021). Explaining neural scaling laws.

Battaglia, P. W., Hamrick, J. B., Bapst, V., Sanchez-Gonzalez, A., Zambaldi, V., Malinowski, M., Tacchetti, A., Raposo, D., Santoro, A., Faulkner, R., Gulcehre, C., Song, F., Ballard, A., Gilmer, J., Dahl, G., Vaswani, A., Allen, K., Nash, C., Langston, V., Dyer, C., Heess, N., Wierstra, D., Kohli, P., Botvinick, M., Vinyals, O., Li, Y., và Pascanu, R. (2018). Relational inductive biases, deep learning, and graph networks.

11

--- TRANG 7 ---
Hình 7: Kích thước mô hình tối ưu (trái) và số lượng ví dụ (phải) cho một mức độ tính toán nhất định để đánh giá tuyến tính trên CIFAR100, trên thang log-log.

và chiến lược tối ưu cho MLPs thì đầu tư tính toán một cách đáng kể hơn vào kích thước tập dữ liệu. Điều này là bằng chứng tiếp theo cho độ lệch quy nạp yếu hơn có trong MLPs, cần nhiều ví dụ hơn để bù đắp. Tiền huấn luyện quy mô lớn của MLPs chứng minh là rất hiệu quả, cho phép các nhà nghiên cứu có ít quyền truy cập vào tài nguyên tính toán hơn nghiên cứu hướng công việc rất thú vị này. Trong khi việc thiếu độ lệch quy nạp không ngăn cản MLPs đạt hiệu suất ấn tượng, nó dẫn đến sự chuyển đổi thú vị trong tính tối ưu về tính toán hướng tới nhiều ví dụ huấn luyện hơn. Việc áp dụng MLPs cho lượng tính toán thậm chí còn lớn hơn tương tự như Zhai et al. (2022), đặc biệt dưới dạng nhiều ví dụ huấn luyện hơn, vẫn là công việc tương lai rất thú vị.

Tài liệu tham khảo

Beyer, L., Hénaff, O. J., Kolesnikov, A., Zhai, X., và van den Oord, A. (2020). Are we done with imagenet?

Brutzkus, A. và Globerson, A. (2017). Globally optimal gradient descent for a ConvNet with Gaussian inputs. Trong Precup, D. và Teh, Y. W., biên tập viên, Proceedings of the 34th International Conference on Machine Learning, tập 70 của Proceedings of Machine Learning Research, trang 605–614. PMLR.

Caballero, E., Gupta, K., Rish, I., và Krueger, D. (2023). Broken neural scaling laws. Trong The Eleventh International Conference on Learning Representations.

Caron, M., Touvron, H., Misra, I., J'egou, H., Mairal, J., Bojanowski, P., và Joulin, A. (2021). Emerging properties in self-supervised vision transformers. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), trang 9630–9640.

Chen, S., Xie, E., GE, C., Chen, R., Liang, D., và Luo, P. (2022). CycleMLP: A MLP-like architecture for dense prediction. Trong International Conference on Learning Representations.

Chen, T., Kornblith, S., Norouzi, M., và Hinton, G. (2020). A simple framework for contrastive learning of visual representations. Trong III, H. D. và Singh, A., biên tập viên, Proceedings of the 37th International Conference on Machine Learning, tập 119 của Proceedings of Machine Learning Research, trang 1597–1607. PMLR.

Chen, X., Liang, C., Huang, D., Real, E., Wang, K., Liu, Y., Pham, H., Dong, X., Luong, T., Hsieh, C.-J., Lu, Y., và Le, Q. V. (2023). Symbolic discovery of optimization algorithms.

Chizat, L. và Bach, F. (2020). Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss. Trong Abernethy, J. và Agarwal, S., biên tập viên, Proceedings of Thirty Third Conference on Learning Theory, tập 125 của Proceedings of Machine Learning Research, trang 1305–1338. PMLR.

Chrabaszcz, P., Loshchilov, I., và Hutter, F. (2017). A downsampled variant of imagenet as an alternative to the cifar datasets.

Coates, A., Ng, A., và Lee, H. (2011). An analysis of single-layer networks in unsupervised feature learning. Trong Gordon, G., Dunson, D., và Dudík, M., biên tập viên, Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, tập 15 của Proceedings of Machine Learning Research, trang 215–223, Fort Lauderdale, FL, USA. PMLR.

Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., và Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. Trong 2009 IEEE Conference on Computer Vision and Pattern Recognition, trang 248–255.

Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., và Houlsby, N. (2021). An image is worth 16x16 words: Transformers for image recognition at scale. Trong International Conference on Learning Representations.

d'Ascoli, S., Sagun, L., Biroli, G., và Bruna, J. (2019). Finding the needle in the haystack with convolutions: on the benefits of architectural bias. Trong Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., và Garnett, R., biên tập viên, Advances in Neural Information Processing Systems, tập 32. Curran Associates, Inc.

Du, S. S., Zhai, X., Poczos, B., và Singh, A. (2019). Gradient descent provably optimizes over-parameterized neural networks. Trong International Conference on Learning Representations.

Fernando, C., Banarse, D., Reynolds, M., Besse, F., Pfau, D., Jaderberg, M., Lanctot, M., và Wierstra, D. (2016). Convolution by evolution: Differentiable pattern producing networks. Trong Proceedings of the Genetic and Evolutionary Computation Conference 2016, GECCO '16, trang 109–116, New York, NY, USA. Association for Computing Machinery.

12

--- TRANG 8 ---
Hình 4: Lỗi downstream tuyến tính trên CIFAR100 (tính theo %) khi được tiền huấn luyện với các kích thước batch khác nhau trên ImageNet21k, trên thang log-log.

Mô hình #tham số
B-6/Wi-256 9M
B-12/Wi-256 12M
B-6/Wi-512 24M
B-12/Wi-512 37M
B-6/Wi-1024 74M
B-12/Wi-1024 124M

Bảng 3: Các mô hình khác nhau và số lượng tham số tương ứng tính bằng triệu.

được huấn luyện từ đầu với augmentations được bật. Điều này nhấn mạnh rằng augmentations vượt ra ngoài việc chỉ dẫn đến một tập dữ liệu lớn hơn mà cung cấp cho mô hình các bất biến hữu ích. Chúng tôi điều tra các trọng số đã học một cách sâu sắc trong Phụ lục C, chỉ ra rằng rất rõ ràng, các đặc trưng cục bộ hơn được học nếu data augmentation được sử dụng. Sức mạnh của augmentations đã được chứng minh trước đó thông qua sự ra đời của học tự giám sát (Grill et al., 2020; Caron et al., 2021; Chen et al., 2020). Ngay cả khi huấn luyện trên các nhãn hoàn toàn ngẫu nhiên, nó vẫn cung cấp tín hiệu học tập mạnh mẽ (Anagnostidis et al., 2023).

Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., và He, K. (2017). Accurate, large minibatch sgd: Training imagenet in 1 hour.

Grill, J.-B., Strub, F., Altché, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., Piot, B., kavukcuoglu, k., Munos, R., và Valko, M. (2020). Bootstrap your own latent - a new approach to self-supervised learning. Trong Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., và Lin, H., biên tập viên, Advances in Neural Information Processing Systems, tập 33, trang 21271–21284. Curran Associates, Inc.

Gunasekar, S., Lee, J. D., Soudry, D., và Srebro, N. (2018). Implicit bias of gradient descent on linear convolutional networks. Trong Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., và Garnett, R., biên tập viên, Advances in Neural Information Processing Systems, tập 31. Curran Associates, Inc.

Guo, J., Tang, Y., Han, K., Chen, X., Wu, H., Xu, C., Xu, C., và Wang, Y. (2021). Hire-mlp: Vision mlp via hierarchical rearrangement. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), trang 816–826.

He, K., Zhang, X., Ren, S., và Sun, J. (2015). Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), trang 770–778.

Hestness, J., Ardalani, N., và Diamos, G. (2019). Beyond human-level accuracy: Computational challenges in deep learning.

Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., Patwary, M. M. A., Yang, Y., và Zhou, Y. (2017). Deep learning scaling is predictable, empirically.

Hoffer, E., Hubara, I., và Soudry, D. (2017). Train longer, generalize better: closing the generalization gap in large batch training of neural networks. Trong Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., và Garnett, R., biên tập viên, Advances in Neural Information Processing Systems, tập 30. Curran Associates, Inc.

Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Rae, J. W., Vinyals, O., và Sifre, L. (2022). Training compute-optimal large language models.

Hron, J., Bahri, Y., Sohl-Dickstein, J., và Novak, R. (2020). Infinite attention: NNGP and NTK for deep attention networks. Trong III, H. D. và Singh, A., biên tập viên, Proceedings of the 37th International Conference on Machine Learning, tập 119 của Proceedings of Machine Learning Research, trang 4376–4386. PMLR.

Ivakhnenko, A., Lapa, V., và ENGINEERING., P. U. L. I. S. O. E. (1965). Cybernetic Predicting Devices. JPRS 37, 803. Joint Publications Research Service [có sẵn từ Clearinghouse for Federal Scientific and Technical Information].

Jacot, A., Gabriel, F., và Hongler, C. (2018). Neural tangent kernel: Convergence and generalization in neural networks. Trong Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., và Garnett, R., biên tập viên, Advances in Neural Information Processing Systems, tập 31. Curran Associates, Inc.

Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., và Amodei, D. (2020). Scaling laws for neural language models.

Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M., và Tang, P. T. P. (2017). On large-batch training for deep learning: Generalization gap and sharp minima. Trong International Conference on Learning Representations.

Krizhevsky, A. (2009). Learning multiple layers of features from tiny images.

Krizhevsky, A., Sutskever, I., và Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. Trong Pereira, F., Burges, C., Bottou, L., và Weinberger, K., biên tập viên, Advances in Neural Information Processing Systems, tập 25. Curran Associates, Inc.

13

--- TRANG 9 ---
Le, Y. và Yang, X. S. (2015). Tiny imagenet visual recognition challenge.

Leclerc, G., Ilyas, A., Engstrom, L., Park, S. M., Salman, H., và Madry, A. (2023). FFCV: Accelerating training by removing data bottlenecks.

Li, Y. và Yuan, Y. (2017). Convergence analysis of two-layer neural networks with relu activation. Trong Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., và Garnett, R., biên tập viên, Advances in Neural Information Processing Systems, tập 30. Curran Associates, Inc.

Lian, D., Yu, Z., Sun, X., và Gao, S. (2022). AS-MLP: An axial shifted MLP architecture for vision. Trong International Conference on Learning Representations.

Lin, Z., Memisevic, R., và Konda, K. R. (2015). How far can we go without convolution: Improving fully-connected networks. ArXiv, abs/1511.02580.

Liu, H., Dai, Z., So, D., và Le, Q. V. (2021). Pay attention to MLPs. Trong Beygelzimer, A., Dauphin, Y., Liang, P., và Vaughan, J. W., biên tập viên, Advances in Neural Information Processing Systems.

Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., và Xie, S. (2022). A convnet for the 2020s. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 11976–11986.

Maloney, A., Roberts, D. A., và Sully, J. (2022). A solvable model of neural scaling laws.

Mei, S. và Montanari, A. (2021). The generalization error of random features regression: Precise asymptotics and the double descent curve. Communications on Pure and Applied Mathematics, 75.

Mei, S., Montanari, A., và Nguyen, P.-M. (2018). A mean field view of the landscape of two-layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665–E7671.

Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., và Ng, R. (2021). Nerf: Representing scenes as neural radiance fields for view synthesis. Commun. ACM, 65(1):99–106.

Mocanu, D., Mocanu, E., Stone, P., Nguyen, P., Gibescu, M., và Liotta, A. (2018). Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Nature Communications, 9.

Neyshabur, B. (2020). Towards learning convolutions from scratch. Trong Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., và Lin, H., biên tập viên, Advances in Neural Information Processing Systems, tập 33, trang 8078–8088. Curran Associates, Inc.

Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., và Srebro, N. (2019). The role of over-parametrization in generalization of neural networks. Trong International Conference on Learning Representations.

Neyshabur, B., Tomioka, R., và Srebro, N. (2014). In search of the real inductive bias: On the role of implicit regularization in deep learning.

OpenAI (2018). Ai and compute.

OpenAI (2023). Gpt-4 technical report.

Parhi, R. và Nowak, R. D. (2021). What kinds of functions do deep neural networks learn? insights from variational spline theory. SIAM J. Math. Data Sci., 4:464–489.

Parkinson, S., Ongie, G., và Willett, R. (2023). Linear neural network layers promote learning single- and multiple-index models.

Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., và Chintala, S. (2019). Pytorch: An imperative style, high-performance deep learning library. Trong Advances in Neural Information Processing Systems 32, trang 8024–8035. Curran Associates, Inc.

14

--- TRANG 10 ---
Poole, B., Lahiri, S., Raghu, M., Sohl-Dickstein, J., và Ganguli, S. (2016). Exponential expressivity in deep neural networks through transient chaos. Trong Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., và Garnett, R., biên tập viên, Advances in Neural Information Processing Systems, tập 29. Curran Associates, Inc.

Ridnik, T., Ben-Baruch, E., Noy, A., và Zelnik, L. (2021). Imagenet-21k pretraining for the masses. Trong Vanschoren, J. và Yeung, S., biên tập viên, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, tập 1. Curran.

Roberts, D. A., Yaida, S., và Hanin, B. (2022). The Principles of Deep Learning Theory: An Effective Theory Approach to Understanding Neural Networks. Cambridge University Press.

Rosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review, 65(6):386.

Rosenfeld, J. S., Rosenfeld, A., Belinkov, Y., và Shavit, N. (2020). A constructive prediction of the generalization error across scales. Trong International Conference on Learning Representations.

Saxe, A. M., McClelland, J. L., và Ganguli, S. (2014). Exact solutions to the nonlinear dynamics of learning in deep linear neural networks.

Schoenholz, S. S., Gilmer, J., Ganguli, S., và Sohl-Dickstein, J. (2017). Deep information propagation. Trong International Conference on Learning Representations.

Shenouda, J., Parhi, R., Lee, K., và Nowak, R. D. (2023). Vector-valued variation spaces and width bounds for dnns: Insights on weight decay regularization. ArXiv, abs/2305.16534.

Soudry, D., Hoffer, E., và Srebro, N. (2018). The implicit bias of gradient descent on separable data. Trong International Conference on Learning Representations.

Tan, M. và Le, Q. V. (2020). Efficientnet: Rethinking model scaling for convolutional neural networks.

Tolstikhin, I., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., Yung, J., Steiner, A. P., Keysers, D., Uszkoreit, J., Lucic, M., và Dosovitskiy, A. (2021). MLP-mixer: An all-MLP architecture for vision. Trong Beygelzimer, A., Dauphin, Y., Liang, P., và Vaughan, J. W., biên tập viên, Advances in Neural Information Processing Systems.

Touvron, H., Bojanowski, P., Caron, M., Cord, M., El-Nouby, A., Grave, E., Izacard, G., Joulin, A., Synnaeve, G., Verbeek, J., và Jegou, H. (2021). ResMLP: Feedforward networks for image classification with data-efficient training.

Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. (2023). Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.

Trockman, A. và Kolter, J. Z. (2022). Patches are all you need?

Urban, G., Geras, K. J., Kahou, S. E., Aslan, O., Wang, S., Mohamed, A., Philipose, M., Richardson, M., và Caruana, R. (2017). Do deep convolutional nets really need to be deep and convolutional? Trong International Conference on Learning Representations.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., và Polosukhin, I. (2017). Attention is all you need. Trong Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., và Garnett, R., biên tập viên, Advances in Neural Information Processing Systems, tập 30. Curran Associates, Inc.

Virtanen, P., Gommers, R., Oliphant, T. E., Haberland, M., Reddy, T., Cournapeau, D., Burovski, E., Peterson, P., Weckesser, W., Bright, J., van der Walt, S. J., Brett, M., Wilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J., Jones, E., Kern, R., Larson, E., Carey, C. J., Polat, İ., Feng, Y., Moore, E. W., VanderPlas, J., Laxalde, D., Perktold, J., Cimrman, R., Henriksen, I., Quintero, E. A., Harris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa, F., van Mulbregt, P., và SciPy 1.0 Contributors (2020). SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261–272.

15

--- TRANG 11 ---
You, Y., Gitman, I., và Ginsburg, B. (2017). Large batch training of convolutional networks.

Yu, W., Luo, M., Zhou, P., Si, C., Zhou, Y., Wang, X., Feng, J., và Yan, S. (2022). Metaformer is actually what you need for vision.

Zhai, X., Kolesnikov, A., Houlsby, N., và Beyer, L. (2022). Scaling vision transformers. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), trang 12104–12113.

Zhang, H., Cisse, M., Dauphin, Y. N., và Lopez-Paz, D. (2018). mixup: Beyond empirical risk minimization. Trong International Conference on Learning Representations.

Zou, D., Cao, Y., Zhou, D., và Gu, Q. (2020). Gradient descent optimizes over-parameterized deep relu networks. Machine Learning, 109:1–26.

16

--- TRANG 12 ---
Phụ lục
A Chi tiết Thí nghiệm
A.1 Tài nguyên
Cho tất cả các thí nghiệm, chúng tôi dựa vào GPU NVIDIA RTX A5000 với bộ nhớ 24GB. Mọi thí nghiệm có thể được thực hiện trên một GPU duy nhất. Chúng tôi tận dụng framework dataloader FFCV vì thời gian chuyển dữ liệu đến GPU trở thành nút cổ chai về thời gian huấn luyện trong trường hợp MLPs. Tất cả các thí nghiệm của chúng tôi đều được thực hiện trong PyTorch (Paszke et al., 2019).

A.2 Ablations bổ sung
Ablations. Chúng tôi cung cấp thêm một số ablations trong Hình 8. Cụ thể hơn, với ngân sách tính toán (xấp xỉ) cố định, chúng tôi điều tra các lựa chọn kiến trúc và tối ưu hóa khác nhau, khi tiền huấn luyện trên ImageNet1k và thực hiện linear probing trên CIFAR100.

Hình ảnh đầu vào Làm phẳng MLP
B{X}_{ACT} :=Phép chiếu Logits 
ACT ACT ACT ...
X lớp tuyến tính
B{X}x{Y}_{ACT} ACT ... := ...
X lớp tuyến tínhY khối
:=ACT
ACT ... ...
X lớp tuyến tínhY khốiACT NORM

NO_AUG: Không Augmentation  NO_MIXUP: Không mixup  NO_CLIP: Không clip gradients  
NO_WD: Không weight decay, DROPOUT: Sử dụng dropout, AdamW: Thay thế Lion bằng AdamW

Kiến trúc
Tối ưu hóa
B{X}x{Y}_IB_{NORM}_{ACT}
(IB: Inverted Bottleneck)

Hình 8: Ablations về các lựa chọn kiến trúc và tối ưu hóa khác nhau khi huấn luyện trên ImageNet. Các số chỉ ra độ chính xác Top-1 linear probing trên CIFAR100.

Chuẩn hóa. Chúng tôi điều tra tầm quan trọng của phương pháp chuẩn hóa (LayerNorm vs BatchNorm) chi tiết hơn trong Bảng 4. Chúng tôi tiền huấn luyện hai B-MLPs trên ImageNet21k với layer normalization và batch normalization và so sánh hiệu suất fine-tuning trên các tác vụ khác nhau. Chúng tôi thấy rằng các kỹ thuật hoạt động tương tự, trong đó layer normalization có một chút lợi thế.

CIFAR-10 CIFAR-100 TINYIMAGENET IMAGENET
LAYER NORM 90.0 74.6 59.6 36.2
BATCH NORM 89.4 73.8 57.7 35.9

Bảng 4: Tiền huấn luyện B-6/Wi-1024 B-MLP với BatchNorm và LayerNorm trên ImageNet21k và sau đó fine-tuning.

Label smoothing. Chúng tôi tiếp tục ablate ảnh hưởng của label smoothing đến hiệu suất downstream. Chúng tôi tiền huấn luyện B-MLPs với lượng label smoothing khác nhau (α∈{0.0,0.1,0.3}) và đánh giá hiệu suất fine-tuning downstream kết quả. Chúng tôi báo cáo kết quả trong Bảng 5. Mặc dù label smoothing cung cấp một số tăng hiệu suất, nhưng lợi ích rất khiêm tốn. Do đó, label smoothing hữu ích nhưng không thiết yếu cho việc huấn luyện MLPs.

17

--- TRANG 13 ---
CIFAR10 CIFAR100 TINYIMAGENET IMAGENET
α=0.3 90.0 74.6 59.6 36.2
α=0.1 89.5 73.7 58.2 36.0
α=0.0 89.2 72.2 57.1 35.7

Bảng 5: Tiền huấn luyện B-6/Wi-1024 B-MLP với các lượng label smoothing khác nhau trên ImageNet21k và sau đó fine-tuning.

Kiến trúc. Chúng tôi thực hiện những quan sát/khuyến nghị sau để tăng hiệu suất của mô hình, phù hợp với kết quả được báo cáo trong tài liệu (Liu et al., 2022); (1) thay thế ReLUs bằng GELUs tăng kết quả đáng kể, (2) thêm skip connections mỗi hai lớp giúp tối ưu hóa, đặc biệt cho các mạng sâu hơn. (3) Sử dụng inverted bottleneck tăng hiệu suất hơn nữa. (4) Sử dụng lớp chuẩn hóa trong cấu hình PRE-LN giúp tối ưu hóa và (4) layer normalization dẫn đến kết quả tốt hơn đáng kể so với batch normalization, đồng thời cũng ổn định hơn trong quá trình huấn luyện.

Tối ưu hóa. Như đã thảo luận trong văn bản chính, augmentations rất quan trọng, và vô hiệu hóa chúng có thể có tác động có hại. Chúng tôi cũng thấy rằng việc cắt gradients, sử dụng weight decay và dropout có tác động tích cực nhỏ đến hiệu suất downstream. Cuối cùng, việc thay thế LION (Chen et al., 2023) bằng Adam(W) dẫn đến giảm hiệu suất.

A.3 Linear Probing
Chúng tôi thể hiện khả năng chuyển giao của MLPs bằng cách huấn luyện một bộ phân loại tuyến tính trên đầu các đặc trưng đã đông lạnh. Để huấn luyện lớp tuyến tính, chúng tôi sử dụng optimizer LION với learning rate η=0.00001 trong 50 epochs. Chúng tôi hiển thị kết quả trong Bảng 6. Chúng tôi quan sát hiệu suất downstream rất mạnh

CIFAR10 CIFAR100 STL10 TINYIMAGENET IMAGENET
B-6/Wi-1024 65.1 41.3 53.4 45.6 13.0
B-6/Wi-1024 + DA 87.8 73.2 85.2 61.3 39.2
B-12/Wi-1024 + DA 90.6 74.5 88.3 68.5 40.7

Bảng 6: Độ chính xác Top-1 linear probing khi tiền huấn luyện trên ImageNet21k.

ngay cả trong thiết lập hạn chế hơn này, nhấn mạnh mức độ có thể chuyển giao của các đặc trưng được học bởi MLPs.

A.4 Luật mở rộng
Chi tiết triển khai. Đối với các biểu đồ luật mở rộng, chúng tôi huấn luyện tất cả các mô hình với batch-size 16384 và optimizer LION với learning rate η=0.00001 và weight decay có độ mạnh 0.001. Chúng tôi tiếp tục sử dụng label smoothing có độ mạnh 0.3. Chúng tôi một lần nữa sử dụng augmentations dưới dạng random flips và crops cũng như MixUp với độ mạnh 0.8. Chúng tôi dựa vào hàm curvefit từ thư viện SciPy (Virtanen et al., 2020) để fit các luật lũy thừa có dạng E(C)=a(b+C)^(-α)+E∞.

A.5 Hiệu quả Tính toán
Chúng tôi nhấn mạnh thực tế là mặc dù MLPs đòi hỏi rất nhiều dữ liệu huấn luyện, suy luận cực kỳ hiệu quả từ góc độ tính toán. Để minh họa điều này, chúng tôi thực hiện so sánh sau; chúng tôi nghiên cứu suy luận trên hình ảnh độ phân giải 64×64 trong MLP so với các kiến trúc thị giác phổ biến khác có kích thước và độ phức tạp tương tự trong Bảng 7. Cụ thể hơn, chúng tôi so sánh với ResNet-152, nơi chúng tôi thay thế stride trong lớp convolutional đầu tiên và loại bỏ thao tác max-pooling đầu tiên để bù đắp cho kích thước hình ảnh nhỏ hơn. Chúng tôi cũng so sánh với mô hình ViT và Mixer cơ bản, nơi chúng tôi trích xuất các patch từ các vùng 4×4 trong hình ảnh gốc.

Như nó nhanh chóng trở nên rõ ràng, MLPs yêu cầu ít FLOPs hơn đáng kể để đưa ra dự đoán trên các hình ảnh riêng lẻ, về cơ bản sử dụng các tham số của chúng một cách có phương pháp hơn nhiều. Kết quả là, độ trễ và thông lượng tốt hơn đáng kể so với các kiến trúc ứng cử viên khác. Chúng tôi đo thông lượng sử dụng kích thước batch tối ưu trên NVIDIA RTX A5000. Chúng tôi nhấn mạnh rằng MLPs của chúng tôi, trái ngược với các kiến trúc khác, bị giới hạn bởi bộ nhớ, có nghĩa là thông lượng của chúng được xác định bởi băng thông prefetching của GPU của chúng tôi. Tiến bộ phần cứng và các kiến trúc chuyên biệt có thể giảm thiểu đáng kể hiệu ứng này. Bỏ qua thời gian chuyển bộ nhớ bằng cách truyền cùng một đầu vào qua mạng của chúng tôi cho tăng 6 lần nữa trong thông lượng tiềm năng.

THAM SỐ ĐỘ TRỄ (MSEC) THÔNG LƯỢNG (HÌNH ẢNH/GIÂY) FLOPS MỖI FORWARD PASS
B-12/Wi-768 66.89 M 21.2 16063 66.8 M
ResNet-152 60.19 M 423 506 13.07 G
ViT-B/4 86.06 M 424 222 23.08 G
Mixer-B/4 63.82 M 400 319 19.36 G

Bảng 7: Các biện pháp khác nhau đánh giá hiệu quả tính toán của các kiến trúc khác nhau.

B Kết quả cho MLPs Tiêu chuẩn
B.1 Học chuyển giao
Để đầy đủ, chúng tôi cũng phân tích hiệu suất chuyển giao của MLPs tiêu chuẩn khi được tiền huấn luyện trên ImageNet21k. Chúng tôi so sánh S-MLP có độ sâu 6 và chiều rộng 2048 với B-MLP có độ sâu 6 và chiều rộng 1024, đảm bảo rằng cả hai mô hình đều có số lượng tham số khoảng ≈70 triệu. Chúng tôi hiển thị kết quả trong Bảng 8. Chúng tôi quan sát thấy rằng ngay cả các đặc trưng của MLP tiêu chuẩn (tức là không có residual connections và cấu trúc bottleneck) cũng chuyển giao rất tốt trên các tác vụ downstream khác nhau. Tuy nhiên, inverted-bottleneck MLP vẫn vượt trội.

CIFAR10 CIFAR100 TINYIMAGENET IMAGENET
S-MLP 87.1 68.3 52.1 30.2
B-MLP 90.0 74.6 59.6 36.2

Bảng 8: So sánh S-MLP có chiều rộng 2048 và độ sâu 6 được tiền huấn luyện trên ImageNet21k, với B-6/Wi-1024 B-MLP (cả hai mô hình khoảng 70M tham số) về hiệu suất fine-tuning.

B.2 Luật mở rộng
Chúng tôi cũng đánh giá luật mở rộng của MLPs tiêu chuẩn bằng cách huấn luyện các mô hình có kích thước khác nhau trên các tập con khác nhau của ImageNet21k và sau đó linear probing các đặc trưng trên CIFAR100. Thiết lập giống hệt với thiết lập được mô tả trong 4.4. Chúng tôi quan sát thấy rằng MLPs tiêu chuẩn cũng thể hiện hành vi luật lũy thừa. Tuy nhiên, độ dốc (0.22 so với 0.25) và intercept (0.18 so với 0.16) tệ hơn khi so sánh với inverted-bottleneck MLP.

C Trực quan hóa trọng số
Chúng tôi trực quan hóa các trọng số lớp đầu tiên W(1)∈R^(3wh×m) bằng cách reshape chúng trở lại R^(w×h×3×m). Sau đó, chúng tôi tạo ra biểu diễn R^(w×h×m) bằng cách lấy giá trị tối đa dọc theo chiều kênh. Chúng tôi hiển thị những trực quan hóa như vậy của 5×5=25 "bộ lọc" đầu tiên cho các kích thước tập dữ liệu tiền huấn luyện khác nhau.

19

--- TRANG 14 ---
Hình 9: Lỗi kiểm tra của MLPs tiêu chuẩn trên CIFAR100 khi được chuyển giao tuyến tính như một hàm của PFLOPS, được đo theo Eq.(4), trên thang log-log.

Hình 10: Trực quan hóa trọng số lớp đầu tiên cho các kích thước tập dữ liệu tiền huấn luyện khác nhau.

bao gồm cả trọng số tại khởi tạo ngẫu nhiên trong Hình 10. Tất cả các mô hình đều được huấn luyện với data augmentation. Chúng tôi quan sát thấy rằng các bộ lọc ngày càng phát triển cấu trúc khi chúng tôi tăng kích thước tập dữ liệu và trở nên ngày càng cục bộ hóa. Chúng tôi tiếp tục so sánh với các mô hình được tiền huấn luyện trên ImageNet21k đầy đủ, có và không có data augmentation trong Hình 11. Chúng tôi quan sát thấy rằng mặc dù chúng tôi cung cấp cho mô hình một lượng mẫu dồi dào, các trọng số vẫn phần lớn không có cấu trúc

20

--- TRANG 15 ---
Hình 11: Trực quan hóa trọng số lớp đầu tiên cho các mô hình được huấn luyện có và không có data augmentation.

và chưa phát triển bất kỳ tính chất cục bộ nào. Mặt khác, việc sử dụng data augmentation dẫn đến các bộ lọc thích ứng hơn.

D Code Inverted Bottleneck MLP
Chúng tôi cung cấp pseudo-code kiểu PyTorch cho inverted bottleneck MLP để nhấn mạnh tính đơn giản của nó.

1from torch import nn
2
3class Block(nn.Module):
4 def __init__(self, dim, expansion_factor=4, dropout=0.):
5 super().__init__()
6 self.fn = nn.Sequential(
7 nn.Linear(dim, int(expansion_factor * dim)),
8 nn.GELU(),
9 nn.Dropout(dropout),
10 nn.Linear(int(expansion_factor * dim), dim),
11 nn.Dropout(dropout)
12 )
13 self.ln = nn.LayerNorm(dim)
14
15 def forward(self, x):
16 return x + self.fn(self.ln(x))
17
18
19def MLP(image_size, channels, dim, depth, num_classes,
expansion_factor=4, dropout=0.):
20 return nn.Sequential(
21 nn.Flatten(start_dim=1, end_dim=-1),
22 nn.Linear(image_size * image_size * channels, dim),
23 *[Block(dim, expansion_factor, dropout) for _ in range(depth)],
24 nn.Linear(dim, num_classes)
25 )

21
