Yuki Tatsunami1,2[0000−0002−7889−8143]và Masato Taki1[0000−0002−5375−7862]
1Đại học Rikkyo, Tokyo, Nhật Bản
{y.tatsunami, taki_m}@rikkyo.ac.jp
2Công ty AnyTech Co., Ltd., Tokyo, Nhật Bản

A Thêm mã giả
Phần này mô tả mã giả cho các phương pháp được thảo luận trong bài báo này. Mã giả cho Multi-scale Patch Embedding được chi tiết trong Liệt kê 1.1.

1# b: kích thước mini-batch, h: chiều cao, w: chiều rộng,
2# kernels: danh sách kích thước kernel cho unfold.
3# ví dụ: [4, 8]
4
5def __init__(self, in_channels, out_channels, kernels):
6 mlp_in_channels = 0
7 for k in kernels:
8 mlp_in_channels += k ** 2
9 mlp_in_channels *= in_channels
10 self.embeddings = nn.ModuleList([
11 nn.Sequential(*[nn.Unfold(
12 kernel_size=k,
13 stride=self.stride,
14 padding=(k - self.stride) // 2),
15 Rearrange("b c hw -> b hw c")
16 ]) for k in kernels
17 ])
18 self.fc = nn.Linear(
19 mlp_in_channels, out_channels
20 )
21
22def forward(self, input):
23 b, _, h, w = input.shape
24 outputs = []
25 for emb in self.embeddings:
26 output = emb(input)
27 outputs.append(output)
28 return self.fc(torch.cat(outputs, dim=2))

Liệt kê 1.1: Mã giả của multi-scale patch embedding (giống Pytorch)

B Ứng dụng tác vụ downstream
Trong phần này, chúng tôi thảo luận về việc áp dụng mô hình của chúng tôi vào các tác vụ downstream. Để áp dụng các mô hình của chúng tôi vào các tác vụ downstream như phân đoạn ngữ nghĩa, phân đoạn thể hiện, và phát hiện đối tượng, nhiều độ phân giải khác nhau cần được hỗ trợ. Do đó, chúng tôi chèn nội suy bicubic trước và sau khối raft-token-mixing, như được hiển thị trong Hình 1. Trong nội suy bicubic trước khối, chúng tôi chuyển đổi đầu vào thành độ phân giải được sử dụng cho tiền huấn luyện. Nội suy bicubic sau khối khôi phục độ phân giải trước nội suy bicubic đầu tiên. Hơn nữa, vì độ phân giải của hình ảnh đầu vào không phải lúc nào cũng chia hết cho kích thước patch, chúng tôi áp dụng nội suy bicubic để có được độ phân giải trước multi-scale embedding là một thừa số của kích thước patch. Phương pháp này có thể được áp dụng cho các mô hình dựa trên MLP toàn cục khác như MLP-Mixer cũng.

Khối trộn kênh Khối Nội suy Bicubic Nội suy Bicubic Bản đồ đặc trưng đầu vào Bản đồ đặc trưng đầu ra

Hình 1: Ứng dụng khối RaftMLP sử dụng nội suy bicubic

B.1 Phát hiện đối tượng
Để đánh giá phát hiện đối tượng và phân đoạn thể hiện, chúng tôi tạo ra một mô hình trong đó backbone của RetinaNet [?] và Mask R-CNN [?], cả hai đều là triển khai tiêu chuẩn trên framework phát hiện đối tượng mmdetection [?], được thay thế bằng RaftMLP và MLP-Mixer. Đối với tập dữ liệu, chúng tôi sử dụng MS COCO [?], một trong những tập dữ liệu benchmark phổ biến nhất cho phát hiện đối tượng. Thiết lập huấn luyện tương tự như ConvMLP [?], với AdamW làm optimizer, tốc độ học được đặt thành 10⁻⁴, weight decay được đặt thành 10⁻⁴, và 12 epoch huấn luyện với batch size 16. Kết quả được so sánh với PureMLP [?], ResNet [?], và ConvMLP [?], và tóm tắt được cung cấp trong Hình 2. Xem Phụ lục B.4 để biết thêm chi tiết.

B.2 Phân đoạn ngữ nghĩa
Chúng tôi thay thế backbone của Semantic FPN [?] được triển khai trên mmsegmentation [?] bằng RaftMLP và MLP-Mixer và đánh giá hiệu suất của chúng trong tác vụ phân đoạn. Chúng tôi áp dụng AdamW làm optimizer với tốc độ học 2.0×10⁻⁴ và weight decay 10⁻⁴. Lịch học tuân theo chính sách tốc độ học giảm đa thức với lũy thừa 0.9. Chúng tôi sử dụng tập dữ liệu ADE20K nổi tiếng [?] để huấn luyện mô hình, với hình ảnh đầu vào được thay đổi kích thước và cắt ngẫu nhiên thành độ phân giải 512×512. Mô hình đã được huấn luyện trong 40000 lần lặp. Các cài đặt trên tuân theo ConvMLP [?]. Tóm tắt kết quả thí nghiệm được hiển thị trong Hình 2. Xem Phụ lục B.4 để biết thêm chi tiết.

[Bốn biểu đồ hiển thị so sánh hiệu suất của các mô hình khác nhau trên RetinaNet, Mask R-CNN (bbox), Mask R-CNN (seg), và Semantic FPN]

Hình 2: Phần trên so sánh kết quả huấn luyện của RetinaNet và Mask R-CNN trên MS COCO và Semantic FPN trên ADE20K. Chúng tôi so sánh kết quả với ResNet, PureMLP, ConvMLP, Mixer, và RaftMLP làm backbone trong mỗi trường hợp. RetinaNet sử dụng AP cho bounding box, Mask R-CNN AP cho bounding box và phân đoạn, và Semantic FPN sử dụng mIoU làm metric của chúng.

B.3 Chi tiết về kiến trúc
RaftMLP Chi tiết về kiến trúc của RaftMLP-S, RaftMLP-M, và RaftMLP-L được sử dụng trong bài báo này được chi tiết trong Bảng 1.

Phát hiện đối tượng, phân đoạn thể hiện và phân đoạn ngữ nghĩa Đối với RetinaNet [?] và Mask R-CNN [?] và Semantic FPN [?] chúng tôi sử dụng, chúng tôi tham khảo kết quả của [?], đây là cùng một thiết lập cho ResNet, PureMLP, và ConvMLP, là sự so sánh của chúng tôi. Các backbone chúng tôi đã thí nghiệm là RaftMLP và Mixer-B/16. Tất cả các kiến trúc chúng tôi đã sắp xếp sử dụng Feature Pyramid Network [?]. Do đó, chúng tôi phải nêu rõ pyramid đặc trưng nào được nhập vào các kiến trúc này từ các backbone RaftMLP và Mixer-B/16. RaftMLP sử dụng đầu ra ngay sau multi-scale patch embedding đầu tiên và đầu ra của Level-2 đến Level-4 làm bản đồ đặc trưng được nhập vào detector và segmentor. Tương tự, Mixer-B/16, cùng với RaftMLP, sử dụng đầu ra ngay sau patch embedding và đầu ra của Block-4, Block-10, và Block-12 làm bản đồ đặc trưng được đề cập trước đó.

B.4 Chi tiết kết quả định lượng
Phát hiện đối tượng và phân đoạn thể hiện Bảng 2 chứa kết quả chi tiết của thí nghiệm cho RetinaNet được thực hiện trong Phần B.1, Bảng 3 bao gồm kết quả chi tiết của thí nghiệm cho Mask R-CNN được thực hiện trong Phần B.1. Kết quả của RetinaNet nhìn chung không tốt như PureMLP, ngay cả với RaftMLP, đảm bảo một số cấu trúc không gian. Đặc biệt, nó gặp khó khăn trong việc phát hiện các đối tượng nhỏ. Kết quả này có thể được thấy trong B.6, nơi RaftMLP thêm artifact vào bản đồ đặc trưng, gây hại cho việc phát hiện đối tượng.

Phân đoạn ngữ nghĩa Bảng 4 chứa kết quả chi tiết của thí nghiệm được thực hiện trong Phần B.2.

B.5 Kết quả định tính
Phát hiện đối tượng và phân đoạn thể hiện Hình 3a hiển thị ground truth cho một mẫu của tập dữ liệu validation MS COCO. Hình 3b hiển thị kết quả suy luận cho RetinaNet với ResNet-50 làm backbone được cài đặt trong mmdetection, và Hình 3c, 3d, 3e, và 3f kết quả suy luận cho bốn RetinaNet được huấn luyện trong Phần B.1. Hình 3g hiển thị kết quả suy luận cho Mask R-CNN với ResNet-50 làm backbone được cài đặt trong mmdetection, và Hình 3h, 3i, 3j, và 3k kết quả suy luận cho bốn Mask R-CNN được huấn luyện trong Phần B.1. Mặc dù thiếu chính xác, các hình cho thấy rằng kết quả của các mô hình dựa trên MLP toàn cục cho các tác vụ phát hiện đối tượng và phân đoạn thể hiện là đáng hài lòng.

Phân đoạn ngữ nghĩa Hình 4a trình bày một hình ảnh với tập dữ liệu validation ADE20k được phủ lên ground truth của nó. Hình 4b hiển thị kết quả suy luận của mô hình áp dụng ResNet-50, mà mmsegmentation cung cấp, làm backbone của Semantic FPN. Hình 4c, 4d, 4e, và 4f hiển thị kết quả suy luận cho bốn mô hình chúng tôi huấn luyện trong thí nghiệm Phần B.2.

[Các hình ảnh hiển thị kết quả định tính cho phát hiện đối tượng, phân đoạn thể hiện và phân đoạn ngữ nghĩa]

B.6 Trực quan hóa
Chúng tôi sử dụng một hình ảnh với ImageNet để trực quan hóa và so sánh bản đồ đặc trưng của nó. Hình ảnh được sử dụng làm đầu vào là hình ảnh chồn ở bên trái của Hình 5, được nhập vào ResNet-50, Mixer-B/16, và RaftMLP-M được tiền huấn luyện. Một số đầu ra của các lớp trung gian được tóm tắt ở bên phải của Hình 5. Đối với ResNet-50, chúng tôi sử dụng đầu ra của các lớp 1 đến 4; đối với Mixer-B/16, chúng tôi sử dụng đầu ra của các Block 2, 4, 10, và 12; đối với RaftMLP-M, chúng tôi sử dụng đầu ra của mỗi Level. Chúng tôi cũng đã bao gồm thêm các đầu ra lớp trung gian cho ba mô hình, xem Hình 6, 7, 8, và 9 cho ResNet-50, Hình 10, 11, 12 và 13 cho Mixer-B/16, và Hình 14, 15, 16, và 17 cho RaftMLP-M.

Như đã đề cập trong Phần 5, hình thức của các đặc trưng trong lớp giữa của các mô hình dựa trên MLP toàn cục khác với của cơ sở tích chập được đại diện bởi ResNet. Chúng tôi tin rằng đây là lý do tại sao các mô hình dựa trên MLP toàn cục không hoạt động tốt khi được chọn làm backbone của các kiến trúc hiện có cho phát hiện đối tượng, phân đoạn thể hiện, và phân đoạn ngữ nghĩa. Bản đồ đặc trưng của RaftMLP-M khác với của ResNet ở chỗ các lớp thấp hơn có bản đồ đặc trưng nắm bắt các đặc trưng của chồn. Ngược lại, các lớp trên có bản đồ đặc trưng với artifact hiển thị của các đường dọc và ngang. Bản đồ đặc trưng của Mixer-B/16 không nắm bắt các đặc trưng của chồn, và chúng nhìn chung bị xáo trộn và có nhiều bản đồ đặc trưng tương tự. Các tác vụ như phát hiện đối tượng, phân đoạn ngữ nghĩa, hoặc thậm chí tạo hình ảnh sẽ yêu cầu các đổi mới cụ thể cho các mô hình dựa trên MLP toàn cục. Việc xuất hiện artifact có thể có tác động nhỏ đối với phân loại, nơi global average pooling được sử dụng. Tuy nhiên, đối với các tác vụ như phân đoạn và tạo hình ảnh, nó trở thành một vấn đề nghiêm trọng. Do đó, sẽ cần thiết phải thiết kế các kiến trúc và hàm loss không phát ra artifact này. Hoặc các phương pháp dựa trên tích chập như RetinaNet, Mask R-CNN, và Semantic FPN có thể không đủ để khôi phục toàn bộ thông tin bị xáo trộn bởi các mô hình dựa trên MLP toàn cục. Để khôi phục thông tin bị xáo trộn toàn cục bởi mô hình dựa trên MLP toàn cục, các mô hình dựa trên MLP toàn cục có thể thiếu một module có thể nắm bắt các mối quan hệ toàn cục, như các module self-attention và các khối token-mixing.

[Các bảng và hình ảnh chi tiết hiển thị cài đặt mô hình, kết quả định lượng, và trực quan hóa bản đồ đặc trưng]
