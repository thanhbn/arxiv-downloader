# 2309.01826.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/ffn-mlp/2309.01826.pdf
# Kích thước tệp: 587169 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Một Mạng Truyền Thẳng Rộng Là Tất Cả Những Gì Bạn Cần
Telmo Pessoa Pires∗†
Equall
telmo@equall.ai António V. Lopes Yannick Assogba Hendra Setiawan∗
Apple
{antoniovilarinholopes, yassogba, hendra}@apple.com
Tóm tắt
Kiến trúc Transformer có hai thành phần chính không phải nhúng: Attention và Mạng Truyền Thẳng (FFN). Attention nắm bắt các phụ thuộc lẫn nhau giữa các từ bất kể vị trí của chúng, trong khi FFN biến đổi phi tuyến mỗi token đầu vào một cách độc lập. Trong nghiên cứu này, chúng tôi khám phá vai trò của FFN và thấy rằng mặc dù chiếm một phần đáng kể tham số của mô hình, nó có tính dư thừa cao. Cụ thể, chúng tôi có thể giảm đáng kể số lượng tham số với chỉ một mức độ giảm khiêm tốn về độ chính xác bằng cách loại bỏ FFN ở các tầng decoder và chia sẻ một FFN duy nhất trên encoder. Cuối cùng, chúng tôi mở rộng kiến trúc này trở lại kích thước ban đầu bằng cách tăng chiều ẩn của FFN được chia sẻ, đạt được những cải thiện đáng kể về cả độ chính xác và độ trễ so với Transformer Big ban đầu.

1 Giới thiệu
Kiến trúc Transformer (Vaswani et al., 2017) đã trở thành mô hình tiêu chuẩn trong nhiều tác vụ Xử lý Ngôn ngữ Tự nhiên (NLP), bao gồm Dịch máy (MT). Một số nghiên cứu đã chỉ ra rằng Transformer thể hiện các đặc tính luật tỷ lệ ấn tượng (Gordon et al., 2021; Bansal et al., 2022; Ghorbani et al., 2022), trong đó việc tăng số lượng tham số mô hình dẫn đến những cải thiện độ chính xác tiếp theo. Song song với việc mở rộng ấn tượng về số lượng tham số của kiến trúc này (Chowdhery et al., 2022), có một xu hướng ngày càng tăng hướng tới việc giảm dấu chân mô hình cho triển khai thực tế, để thỏa mãn các ràng buộc thực tế như yêu cầu về độ trễ cũng như các hạn chế về bộ nhớ và không gian đĩa. Ngược lại, các nhà nghiên cứu đang tích cực khám phá việc chia sẻ tham số (Ge et al., 2022; Takase and Kiyono, 2023; Lou et al., 2022), giảm chiều của các thành phần Transformer, và cắt tỉa các thành phần như attention heads (Voita et al., 2019; Michel et al., 2019).

Mặc dù vai trò của attention trong việc học các phụ thuộc cặp đôi giữa các token được hiểu tương đối rõ (Voita et al., 2019; Clark et al., 2019; Vig and Belinkov, 2019), vai trò của Mạng Truyền Thẳng (FFN) vẫn chưa được khám phá đầy đủ. Gần đây, Geva et al. (2021) đã thiết lập một kết nối giữa FFN và attention bằng cách đưa ra giả thuyết rằng FFN tương ứng với các cặp key-value có thể học được, trong đó các trọng số của tầng đầu tiên của FFN tương ứng với các key và những trọng số của tầng thứ hai tương ứng với các value. Họ thấy rằng các key có thể nắm bắt các mẫu văn bản nổi bật ở mỗi tầng, và họ nhận thấy rằng các lớp mẫu có xu hướng chồng chéo giữa các tầng lân cận, cho thấy sự dư thừa trong biểu diễn.

Quan sát này thúc đẩy nghiên cứu của chúng tôi, nơi chúng tôi xem xét lại thực hành thông thường về việc phân bổ một FFN riêng lẻ cho mỗi tầng. Chúng tôi điều tra tác động của việc chia sẻ và loại bỏ FFN trên các tầng khác nhau của các mô hình MT. Chúng tôi tiến hành các thí nghiệm kỹ lưỡng với các cấu hình khác nhau của Transformer, trên các cặp ngôn ngữ khác nhau, bao gồm một cặp ngôn ngữ tài nguyên thấp và đa ngôn ngữ. Ngoài ra, chúng tôi điều tra tác động của FFN trong mô hình chỉ decoder dựa trên Transformer. Chúng tôi thấy rằng một mức độ dư thừa đáng kể tồn tại giữa các FFN của encoder và decoder. Kết quả là, chúng tôi có thể loại bỏ FFN decoder và chia sẻ một FFN duy nhất trên encoder mà không làm giảm đáng kể độ chính xác của mô hình. Bước này không chỉ dẫn đến việc tiết kiệm tham số đáng kể mà còn mở ra cơ hội cho những cải thiện tiếp theo. Chúng tôi cũng đề xuất sử dụng FFN rộng hơn trong encoder trong khi loại bỏ FFN của decoder, dẫn đến một mô hình có kích thước tương tự nhưng cải thiện độ chính xác và giảm độ trễ.

Cuối cùng, chúng tôi tiến hành phân tích chi tiết về sự tương đồng biểu diễn giữa mô hình ban đầu, sử dụng một FFN độc lập cho mỗi tầng, và các mô hình khác nhau với FFN được chia sẻ. Kết quả của chúng tôi tiết lộ rằng cả độ chính xác mô hình và biểu diễn nội tại của các khối Transformer vẫn ổn định khi chia sẻ FFN.

2 Nền tảng và Phương pháp
2.1 Transformer
Kiến trúc Transformer có hai thành phần chính: attention và FFN, được kết nối thông qua một kết nối dư (He et al., 2016) và chuẩn hóa tầng (Ba et al., 2016). Trong mô hình encoder-decoder, có hai loại attention: self-attention và cross-attention. Self-attention được sử dụng trong cả encoder và decoder, cho phép mô hình tập trung vào thông tin liên quan trong cùng một chuỗi. Cross-attention chỉ dành riêng cho decoder và cho phép nó chú ý đến đầu ra của encoder. Attention nhận đầu vào là một tập hợp các queries, keys và values, được chiếu bằng bốn ma trận R^(dmodel×dmodel) (một cho queries, keys, values, và đầu ra cuối cùng) trong đó dmodel là chiều ẩn của mô hình. Sau đó nó áp dụng hàm SOFTMAX để cho phép nó tập trung vào các value có liên quan nhất.

FFN được áp dụng sau attention trên cả encoder và decoder và bao gồm phép biến đổi tuyến tính 2 tầng sau:

FFN(x) = max(0, xW₁ + b₁)W₂ + b₂,     (1)

trong đó một phi tuyến tính RELU được áp dụng cho phép biến đổi của chuỗi đầu vào (x). Ở mỗi tầng, FFN được tham số hóa với hai ma trận, W₁ ∈ R^(dmodel×dff) và W₂ ∈ R^(dff×dmodel) trong đó dff là chiều FFN và thường được đặt thành 4×dmodel (Vaswani et al., 2017).

Nghiên cứu gần đây đã vẽ ra một liên kết đáng kể giữa attention và FFN (Geva et al., 2021), trong đó W₁ và W₂ đảm nhận vai trò tương tự như keys và values đối với một attention không chuẩn hóa trong đó đầu vào (x) đóng vai trò như query. Không giống như attention thông thường, FFN sử dụng RELU, cho phép nhiều key đóng góp đáng kể vào đầu ra cuối cùng (Geva et al., 2021). Ngoài ra, những key này tương ứng với một kho các mẫu nổi bật được học từ dữ liệu huấn luyện. Geva et al. (2021) đề xuất rằng ở các tầng thấp hơn, FFN học các mẫu cú pháp nông và tiệm tiến học các mẫu ngữ nghĩa sâu ở các tầng sâu hơn. Hơn nữa, các tác giả thấy rằng có một sự chồng chéo đáng kể giữa các mẫu được nắm bắt bởi các tầng liền kề, cho thấy rằng có những dư thừa trong FFN và đề xuất một sự phân bổ tốt hơn của những tham số này có thể có lợi cho hiệu suất.

2.2 Chia sẻ và Mở rộng FFN
Transformer vanilla phân bổ một FFN cho mỗi tầng của encoder và decoder, tức là FFN^enc_i hoặc FFN^dec_i, tương ứng. Loại trừ các tham số nhúng, những FFN này chiếm khoảng hai phần ba ngân sách tham số, trong khi attention chiếm phần ba còn lại¹. Nghiên cứu trước đây thấy rằng việc ràng buộc tham số hóa của FFN decoder không gây ra sự suy giảm độ chính xác (Ge et al., 2022). Trong nghiên cứu này, chúng tôi chia sẻ các tham số của FFN trên các tầng và/hoặc trên encoder và decoder để giảm thiểu sự dư thừa giữa các FFN.

Đặt N_enc, N_dec là số tầng encoder và decoder, tương ứng. Chúng tôi xem xét nhiều cấu hình cho việc chia sẻ tham số như sau:

• Một FFN^enc_all cho toàn bộ encoder:
  FFN^enc_i(·)_tied = FFN^enc_all(·), ∀i: 1 ≤ i ≤ N_enc

• Một FFN^dec_all cho toàn bộ decoder:
  FFN^dec_j(·)_tied = FFN^dec_all(·), ∀j: 1 ≤ j ≤ N_dec

• Một FFN^encdec_all cho cả encoder và decoder:
  FFN^enc_i(·)_tied = FFN^dec_j(·)_tied = FFN^encdec_all(·),
  ∀i, j: 1 ≤ i ≤ N_enc, 1 ≤ j ≤ N_dec

Ngoài ra, chúng tôi khám phá việc thay đổi chiều của FFN được chia sẻ, mà chúng tôi ký hiệu là d'_ff. Đặt d'_ff > d_ff sẽ mở rộng FFN được chia sẻ trong khi d'_ff < d_ff sẽ thu hẹp nó. Chúng tôi cũng xem xét các trường hợp cực đoan của việc đặt d'_ff thành 0 hoặc thành (N_enc + N_dec) × d_ff (và hơn thế nữa). Đặt d'_ff = 0 tương đương với việc loại bỏ FFN² trong khi đặt d'_ff = (N_enc + N_dec) × d_ff tương tự như việc chia sẻ phép nối của tất cả các FFN riêng lẻ.

Việc chia sẻ FFN ảnh hưởng trực tiếp đến số lượng tham số và, ở một mức độ nhất định, độ trễ. Ví dụ, việc chia sẻ FFN^enc_all cho toàn bộ encoder giảm số lượng tham số đi (N_enc - 1) × 2 × d_model × d'_ff³; trong khi việc loại bỏ FFN trên decoder, tức là đặt d'_ff = 0 cho FFN^dec_all, giảm tham số đi (N_dec) × 2 × d_model × d'_ff và giảm lượng tính toán cần thực hiện. Điều này đặc biệt quan trọng trong quá trình suy luận vì quá trình truyền thẳng của decoder là tự hồi quy, và việc thay đổi chiều FFN của decoder có tác động độ trễ cao hơn so với encoder.

Vì các cấu hình khác nhau có tác động khác nhau, chúng tôi phân tích sự đánh đổi giữa kích thước mô hình, độ trễ và độ chính xác: (i) Có thể chia sẻ/cắt tỉa bao nhiêu tham số với sự suy giảm độ chính xác không đáng kể (nếu có)? (ii) Các FFN của encoder và decoder có bị ảnh hưởng tương tự không? (iii) Giữ nguyên kích thước mô hình, liệu các tham số FFN có thể được phân bổ hiệu quả hơn không?

Chúng tôi đề xuất một cấu hình mới, mà chúng tôi gọi là mô hình One Wide FFN, bao gồm một FFN rộng được chia sẻ duy nhất trên encoder và không có FFN trên decoder. Để giữ số lượng tham số giống như trong baseline, chúng tôi tăng chiều FFN được chia sẻ tương ứng: FFN^enc_all với d'_ff = (N_enc + N_dec) × d_ff.

Để đầy đủ, chúng tôi bao gồm các thí nghiệm tương tự trên cơ chế attention trong Phụ lục B. Những thí nghiệm này cho thấy rằng, trái ngược với FFN, các trọng số attention cụ thể cho từng tầng riêng lẻ quan trọng hơn và không dư thừa, vì việc chia sẻ attention dẫn đến sự sụt giảm độ chính xác đáng kể.

2.3 Tương đồng Biểu diễn
Bên cạnh việc điều tra tác động lên độ chính xác, chúng tôi nghiên cứu sự tương đồng giữa các mô hình khác nhau về mặt biểu diễn nội tại và không gian ngữ nghĩa mà chúng tạo ra.

Chúng tôi sử dụng Linear Centered Kernel Alignment (CKA, Kornblith et al., 2019) để đo sự tương đồng giữa các biểu diễn nội tại của các mô hình khác nhau. CKA sử dụng tích vô hướng để ước tính mức độ tương đồng của các ma trận kernel của hai biểu diễn khác nhau và dựa trên Hilbert-Schmidt Independence Criterion (HSIC, Gretton et al., 2005), một thước đo thống kê về sự độc lập của hai biến ngẫu nhiên. Linear CKA sử dụng tích vô hướng làm kernel và có thể được viết là:

CKA(A,B) = ||AB^T||²_F / (||A^T A||_F ||B^T B||_F),

trong đó ||·||_F là chuẩn Frobenius trong khi A và B là các ma trận đặc trưng được trung tâm hóa (tức là, chúng tôi trừ đi giá trị trung bình) của các tầng được so sánh, được tính trên cùng một tập dữ liệu. Cả hai ma trận đều có kích thước n×d, trong đó n là số câu trong tập dữ liệu và d là chiều đầu ra của thành phần, và được thu được bằng cách lấy trung bình kích hoạt của tất cả các token trong mỗi câu⁴. Kernel tuyến tính đơn giản để tính toán và Kornblith et al., 2019 báo cáo hiệu suất thực nghiệm mạnh mẽ của linear CKA so với các kernel và phương pháp khác.

Để đo sự tương đồng giữa các không gian ngữ nghĩa của các mô hình khác nhau, chúng tôi sử dụng Local Neighborhood Similarity (LNS, Boggust et al., 2022). Các tương đồng lân cận cục bộ đã được sử dụng trước đây trong việc phân tích các thay đổi ngữ nghĩa trong word embeddings (Hamilton et al., 2016). Tiền đề của LNS là hai không gian ngữ nghĩa tương tự nếu một câu có những lân cận tương tự trong hai không gian. LNS của một câu s giữa các mô hình 1 và 2 được định nghĩa là:

LNS(s) = Sim(k-NN₁(s), k-NN₂(s)),

trong đó k-NN(s) là tập hợp k lân cận gần nhất của câu s cho một mô hình và Sim là intersection-over-union (tương đồng Jaccard) của hai tập hợp lân cận. Đối với mỗi cặp thành phần (attention và FFN) trong các mô hình 1 và 2, chúng tôi tính toán LNS của tất cả các câu trong tập dữ liệu đánh giá và lấy LNS trung bình làm thước đo tương đồng tầng của chúng tôi. Giá trị k càng nhỏ thì các lân cận chúng tôi so sánh càng cục bộ, và tác vụ truy xuất càng cụ thể. Chúng tôi chọn k đủ nhỏ để có thể kiểm tra trực quan các lân cận câu nếu cần thiết. Trong phân tích của chúng tôi, chúng tôi sử dụng khoảng cách cosine làm thước đo khoảng cách giữa các kích hoạt và đặt k thành 5% kích thước tập dữ liệu (~100 câu).

3 Thiết lập Thí nghiệm
Dữ liệu Trong các thí nghiệm của chúng tôi, chúng tôi trình bày kết quả trên WMT22 Tiếng Anh (EN) → Tiếng Đức (DE) (296M cặp), mà chúng tôi thu được bằng cách sử dụng các script mt-data được cung cấp⁵, WMT16 EN → Romania (RO) (610K cặp), và cho thiết lập đa ngôn ngữ của Pires et al. (2023), bao gồm 10 ngôn ngữ: Đức, Anh, Tây Ban Nha, Pháp, Ý, Nhật, Hàn, Bồ Đào Nha, Swahili, và Trung Quốc. Trong phân tích của chúng tôi, chúng tôi chủ yếu tập trung vào WMT22 EN → DE.

Theo Schmidt et al. (2022), chúng tôi sử dụng các script được cung cấp bởi WMT'16 để chuẩn hóa phía RO. EN → RO giữ các dấu để tạo ra các bản dịch chính xác. Để biết thêm chi tiết, tham khảo Schmidt et al. (2022). Đối với các thí nghiệm đa ngôn ngữ, chúng tôi sao chép thiết lập của Pires et al. (2023), bao gồm tất cả các chi tiết, bao gồm tiền xử lý dữ liệu và kích thước tập dữ liệu.

Chỉ số Chúng tôi tính toán BLEU⁶ sử dụng sacreBLEU⁷ phiên bản 2.3.1, với chữ ký đánh giá nrefs:1 | case:mixed | eff:no | tok:13a | smooth:exp cho BLEU, và nrefs:1 | case:mixed | eff:no | tok:flores101 | smooth:exp cho SPBLEU. Đối với kết quả chính của chúng tôi, chúng tôi cũng báo cáo COMET sử dụng mô hình wmt20-comet-da và CHRF sử dụng chữ ký nrefs:1 | case:mixed | eff:yes | nc:6 | nw:0 | space:no.

Độ trễ Chúng tôi báo cáo thời gian suy luận bằng token/giây (càng cao càng tốt), được tính trung bình trên 5 lần chạy. Đối với các mô hình đa ngôn ngữ, chúng tôi sử dụng tập kiểm tra DE → EN. Các phép đo của chúng tôi được thu thập bằng cách sử dụng một GPU NVIDIA V100 duy nhất trên một CPU Intel(R) Xeon(R) Gold 6148 đơn luồng @ 2.40GHz với kích thước batch là 1 và kích thước beam là 5, để mô phỏng thực tế việc suy luận của một mô hình được triển khai. Đối với các thí nghiệm với kích thước batch lớn hơn, xem Phụ lục D.

Tokenization Đối với WMT22 EN → DE, chúng tôi sử dụng SENTENCE PIECE (Kudo and Richardson, 2018), với kích thước từ vựng 32K và phạm vi ký tự 1.0, trong khi đối với các thí nghiệm đa ngôn ngữ, chúng tôi sử dụng kích thước từ vựng 250k và phạm vi ký tự 0.9995. Đối với WMT16 EN → RO, chúng tôi sử dụng byte-pair encoding (BPE, Sennrich et al., 2016) với 40,000 phép merge.

Kiến trúc Mô hình Chúng tôi tập trung phân tích vào Transformer Big trong đó N_enc = N_dec = 6, d_model = 1024, d_ff = 4096, và có 16 attention heads. Chúng tôi cũng báo cáo kết quả trên Transformer Base (N_enc = N_dec = 6, d_model = 512, d_ff = 2048, và 8 attention heads), và encoder sâu decoder nông (Kasai et al., 2021) Transformer Big với 12 tầng encoder và 2 tầng decoder. Đối với các thí nghiệm chỉ decoder, mô hình giống hệt Transformer Big, ngoại trừ việc tất cả 12 tầng đều ở decoder. Mô hình chỉ decoder của chúng tôi tương tự như mô hình ngôn ngữ dựa trên Transformer, đặc biệt là Prefix-LM (Raffel et al., 2020), nơi chúng tôi áp dụng mask không tự hồi quy ở phía nguồn và mask tự hồi quy ở phía đích. Các embeddings nguồn và đích và ma trận chiếu đầu ra được chia sẻ trong tất cả các mô hình (Press and Wolf, 2017).

Siêu tham số Tất cả các thí nghiệm được thực hiện bằng FAIRSEQ (Ott et al., 2019). Tối ưu hóa của chúng tôi là ADAM (Kingma and Ba, 2015) với tốc độ học 0.0007. Chúng tôi huấn luyện trong 80k, 80k, 150k bước trên WMT22, WMT16, và đa ngôn ngữ, tương ứng, tại thời điểm đó các mô hình đã hội tụ. Chúng tôi sử dụng 4000 bước khởi động, và một bộ lập lịch tốc độ học căn bậc hai nghịch đảo (Vaswani et al., 2017). Chúng tôi sử dụng tỷ lệ dropout 0.1 cho WMT22, 0.3 cho WMT16, và 0 cho các thí nghiệm đa ngôn ngữ do sự dồi dào của dữ liệu, theo Pires et al. (2023). Tất cả các mô hình được huấn luyện bằng fp16 (Ott et al., 2018).

Danh pháp Trong các thí nghiệm của chúng tôi, chúng tôi chạy một số cấu hình khác nhau cho mỗi kiến trúc mô hình khác nhau về cách FFN được sử dụng, chia sẻ hoặc loại bỏ, cũng như kích thước của FFN được chia sẻ (d'_ff). Để tạo điều kiện cho cuộc thảo luận của chúng tôi, chúng tôi giới thiệu trong Bảng 1 danh pháp sẽ phục vụ làm tham chiếu cho phần còn lại của văn bản. Trừ khi có quy định khác, chiều của FNN*_all được chia sẻ, tức là d'_ff bằng với d_ff của mô hình ban đầu.

Đối với các mô hình chỉ decoder, chỉ có các cấu hình SharedDec và NoDec được định nghĩa. Để ngắn gọn, chúng tôi bỏ qua việc đề cập đến FFN khỏi văn bản khi có thể, tức là SharedEnc thay vì SharedEncFFN.

[Bảng 1: Danh pháp được sử dụng trong các thí nghiệm của chúng tôi]

Tương đồng Biểu diễn Chúng tôi sử dụng tập đánh giá WMT22 EN → DE cho cả phân tích CKA và LNS. Chúng tôi phân tích các biểu diễn encoder và decoder một cách độc lập và trình bày các chỉ số này trong biểu đồ heatmap ma trận thể hiện sự tương đồng từng cặp giữa các tầng. Đường chéo của ma trận này là sự tương đồng của các tầng tương ứng, tức là tầng i trên cả hai kiến trúc. Để tạo điều kiện cho việc so sánh "táo với táo" giữa các mô hình, chúng tôi trích xuất các biểu diễn decoder bằng cách ép buộc giải mã (tham chiếu đầu tiên). Chúng tôi thiết lập 2 điểm số tương đồng quan trọng: một điểm chuẩn về sự tương đồng cho mỗi chỉ số này, nơi chúng tôi huấn luyện hai mô hình bổ sung sử dụng cùng kiến trúc nhưng với các hạt giống ngẫu nhiên khác nhau; một giới hạn tương đồng thấp, nơi chúng tôi so sánh Transformer Big baseline với một mô hình được khởi tạo ngẫu nhiên (tức là, chưa được huấn luyện) với cùng kiến trúc. Chúng tôi trình bày những giới hạn này trong Phụ lục C.

4 Kết quả Thí nghiệm
4.1 Chia sẻ FFN
Kết quả của các cấu hình chia sẻ FFN khác nhau được tóm tắt trong Bảng 2, bao gồm tác động của chúng lên độ chính xác và kích thước mô hình (tính bằng triệu tham số và phần trăm). Việc chia sẻ FFN của encoder (SharedEnc) hoặc decoder (SharedDec) dẫn đến sự giảm chỉ 0.2 đến 0.3 điểm BLEU, trong khi giảm số lượng tham số gần 20%. Việc chia sẻ FFN ở mỗi bên (ShareEncShareDec) dẫn đến sự suy giảm đáng kể hơn là 0.9 điểm BLEU, mặc dù giảm số lượng tham số 37%, trong khi việc chia sẻ một FFN duy nhất trên encoder và decoder (ShareEncDec) dẫn đến sự suy giảm cao hơn một chút là 1.1 điểm BLEU. Tuy nhiên, những phát hiện này hỗ trợ giả thuyết rằng FFN chứa một mức độ dư thừa nhất định, như chúng tôi mong đợi một sự suy giảm độ chính xác lớn hơn với việc giảm kích thước mô hình đáng kể (20-40%).

[Bảng 2: Kết quả sacreBLEU trên WMT 22 EN → DE cho các cấu hình chia sẻ FFN khác nhau]

Trong khi chúng tôi tập trung vào việc chia sẻ một FFN cho tất cả các tầng trong một module, chúng tôi so sánh với việc chia sẻ nhiều FFN theo Takase and Kiyono (2023) trong Phụ lục A. Chúng tôi thấy rằng việc chia sẻ một FFN có độ chính xác tương đương với việc chia sẻ nhiều FFN trong một module, đồng thời hiệu quả hơn về tham số.

4.2 Loại bỏ FFN
Bảng 3 tóm tắt hiệu suất của các mô hình không có FFN. Bên cạnh BLEU và số lượng tham số, chúng tôi báo cáo tốc độ suy luận cho mỗi kiến trúc. Việc loại bỏ FFN trên encoder (NoEnc) dẫn đến sự giảm 0.9 điểm BLEU trong khi giảm số lượng tham số 22% và có tác động tối thiểu đến tốc độ suy luận. Việc loại bỏ FFN trên decoder (NoDec), mặt khác, chỉ gây ra sự suy giảm 0.4 điểm BLEU trong khi tăng tốc độ suy luận 20%⁸. Việc giảm độ trễ cao nhất đạt được bằng cách loại bỏ FFN trên cả encoder và decoder (NoEncNoDec), nhưng nó đi kèm với sự suy giảm lớn hơn đáng kể hơn 2 điểm BLEU.

[Bảng 3: Kết quả sacreBLEU trên WMT 22 EN → DE cho các cấu hình loại bỏ FFN khác nhau]

Kết hợp chia sẻ và loại bỏ Những kết quả này, cùng với những kết quả từ Bảng 2, đề xuất rằng các FFN của encoder và decoder có những đóng góp khác nhau: của decoder dư thừa hơn, khẳng định lại nghiên cứu trước đây về tham số hóa FFN (Ge et al., 2022). Với điều này trong tâm trí, chúng tôi thí nghiệm với một FFN được chia sẻ trên encoder và loại bỏ nó trên decoder, được báo cáo là SharedEncNoDec trong Bảng 3. Như được thể hiện, với chỉ khoảng 60% tham số của Transformer Big, chúng tôi quan sát thấy cải thiện 22% về tốc độ suy luận, với chi phí là 0.3 điểm BLEU.

4.3 Mô hình One Wide FFN
Các phần trước mô tả các mô hình chia sẻ và/hoặc loại bỏ FFN, hiệu quả giảm kích thước mô hình với một chi phí độ chính xác khiêm tốn. Trong phần này, chúng tôi điều tra liệu chúng tôi có thể lấy lại độ chính xác bị mất trong khi duy trì hiệu quả tham số và giảm độ trễ không. Chúng tôi tập trung vào mô hình ShareEncNoDec vì nó cung cấp một baseline mạnh mẽ với việc tiết kiệm tham số đáng kể và tăng tốc độ suy luận.

Chúng tôi đề xuất tăng chiều của FFN được chia sẻ để phù hợp với số lượng tham số của mô hình ban đầu (được tham số hóa đầy đủ), để tránh tăng chi phí lưu trữ mô hình. Cụ thể, ShareEncNoDec tiết kiệm khoảng (N_enc + N_dec - 1) × 2 × d_model × d_ff tham số vì chỉ có một FFN được chia sẻ duy nhất trong encoder. Mặt khác, Transformer Big có (N_enc + N_dec) FFN. Do đó, chúng tôi phù hợp với kích thước của mô hình ban đầu bằng cách đặt chiều của FFN được chia sẻ, d'_ff, thành (N_enc + N_dec) × d_ff.

Bảng 4 tóm tắt kết quả của chúng tôi. Nó bao gồm mô hình đề xuất của chúng tôi, mô hình One Wide FFN (d'_ff = 49,152), cũng như Transformer Big baseline, và ShareEncNoDec tương ứng (d'_ff = 4,096). Nó cũng bao gồm một mô hình rộng với d'_ff = 24,576, sử dụng cùng số lượng tham số như NoDec, với d'_ff = N_enc × d_ff. Mô hình này đạt được độ chính xác ngang bằng (hoặc cao hơn một chút) với Transformer Big baseline với 20% ít tham số hơn và tăng tốc độ suy luận đáng kể.

Mô hình đề xuất của chúng tôi với d'_ff = 49,152 vượt xa điều đó, đạt được mức tăng 1.2 điểm BLEU so với ShareEncNoDec vanilla và 0.9 điểm BLEU so với Transformer Big. Những cải thiện này vẫn nhất quán trên CHRF và COMET. Hơn nữa, nó có tốc độ suy luận tương tự như mô hình ShareEncNoDec. Để đầy đủ, chúng tôi bao gồm một mô hình rộng hơn với d'_ff = 98,304. Mặc dù có thêm khả năng, mô hình này không cung cấp bất kỳ cải thiện độ chính xác bổ sung nào, mà chúng tôi nghi ngờ là do thiếu dữ liệu để huấn luyện một mô hình lớn như vậy.

[Bảng 4: Độ chính xác của One Wide FFN cho Transformer Big EN → DE trên WMT 22]

4.4 Phân tích Biểu diễn Nội tại
Chúng tôi hiện báo cáo một phân tích hậu hoc về các biểu diễn nội tại của các mô hình được giới thiệu trong các phần trước. Mục tiêu của chúng tôi là hai mặt: 1) xác định liệu các biểu diễn nội tại của các mô hình đề xuất có thể hiện mức độ tương đồng đáng kể với những mô hình của mô hình cơ sở ban đầu không; 2) để đi sâu vào tác động của các phương pháp đề xuất lên sự dư thừa. Chúng tôi áp dụng định nghĩa dư thừa của Dalvi et al. (2020), những người kiểm tra trực quan sự tương đồng giữa các module liền kề trong một mô hình (tương đồng cao kéo theo dư thừa cao).

[Bảng 5: Tương đồng của các biểu diễn (%) của các module tương ứng của các kiến trúc khác nhau so với Transformer Big cho WMT22 EN → DE]

4.4.1 Tương đồng với Baseline
Chúng tôi khẳng định các chỉ số tương đồng từng cặp, bằng cách chuẩn hóa chúng so với một điểm chuẩn. Như đã đề cập trong Phần 3, chúng tôi thiết lập các điểm số chuẩn bằng cách huấn luyện hai mô hình Transformer Big bổ sung, nhưng sử dụng các hạt giống ngẫu nhiên khác nhau. Những mô hình này đạt được độ chính xác tương tự như mô hình baseline (xem Phụ lục C.1 để biết thêm chi tiết). Điểm số chuẩn là sự tương đồng giữa baseline và những mô hình này. Bởi vì điểm chuẩn được tính toán bằng cách lấy trung bình các điểm số tương đồng từ các lần huấn luyện khác nhau của baseline của chúng tôi, các lần chạy riêng lẻ có thể có điểm số chuẩn hóa trên 100%.

Bảng 5 cho thấy các điểm số tương đồng chuẩn hóa cho một số mô hình. Dưới các cột Encoder, chúng tôi so sánh các biểu diễn encoder, và dưới các cột Decoder, chúng tôi so sánh các biểu diễn decoder. Việc chia sẻ FFN dẫn đến các điểm số tương đồng (chuẩn hóa) thấp hơn một cách nhất quán so với các mô hình không chia sẻ, cả về mặt biểu diễn nội tại (CKA) và không gian ngữ nghĩa (LNS). Như được thể hiện, mặc dù các mô hình chia sẻ FFN có điểm số tương đồng thấp hơn so với những mô hình không chia sẻ, các điểm số vẫn rất gần với 100%. Hơn nữa, những sự giảm này phù hợp với sự giảm BLEU thấy trong Bảng 2, nơi mô hình có điểm số tương đồng thấp nhất (ShareEncDec) cũng là mô hình kém chính xác nhất. Chúng tôi quan sát xu hướng tương tự đối với các mô hình loại bỏ FFN trong encoder hoặc decoder, những mô hình này thể hiện điểm số tương đồng thấp hơn với thành phần tương ứng so với các mô hình chia sẻ chúng, như được thể hiện bởi NoEnc và NoDec. Ngoài ra, kết quả trước đây một lần nữa đề xuất rằng các FFN trong encoder quan trọng hơn so với trong decoder vì sự tương đồng thay đổi mạnh mẽ so với tất cả các thiết lập khác.

Để đầy đủ, chúng tôi báo cáo ở hàng cuối cùng các điểm số tương đồng cho mô hình One Wide FFN, có độ chính xác cao hơn mô hình cơ sở. Các biểu diễn nội tại được tạo ra bởi mô hình đó khác biệt so với những mô hình của mô hình cơ sở. Điều thú vị là, chúng tôi quan sát thấy sự giảm lớn hơn trong điểm số LNS so với điểm số CKA, cho thấy rằng sự thay đổi xảy ra chủ yếu trong không gian ngữ nghĩa, thay vì không gian Euclidean được nắm bắt bởi CKA. Để có phân tích tương đồng theo từng tầng chi tiết phá vỡ phân tích tổng hợp trong Bảng 5, xem Phụ lục C.2.

4.4.2 Cái nhìn Định tính về Dư thừa
Chúng tôi hiện nghiên cứu tác động của mô hình One Wide FFN của chúng tôi lên sự dư thừa của các biểu diễn nội tại. Ngoài việc áp dụng định nghĩa dư thừa của họ, chúng tôi cũng áp dụng phương pháp tính toán tự tương đồng của Dalvi et al. (2020), cụ thể là xem xét cách các biểu diễn thay đổi khi chúng đi qua mỗi module (self-attention, FFN, hoặc cross-attention) của mô hình. Cụ thể, chúng tôi sử dụng CKA để tính toán sự tương đồng giữa đầu ra của các module khác nhau trong cùng một mô hình.

Trong Hình 1a, chúng tôi hiển thị các ma trận tự tương đồng CKA cho các encoder của mô hình One Wide FFN và Transformer Big. Chúng tôi làm tương tự cho các decoder trong Hình 1b. Những ma trận này cho thấy mức độ tương đồng của mỗi module của mạng với tất cả các module khác trong mạng đó. Đường chéo của ma trận là sự tương đồng giữa một module và chính nó và luôn luôn là 1.

Như được hiển thị, có sự tương đồng cao giữa các module liền kề của Transformer Big, cả trên encoder và decoder, được chỉ ra bởi các vùng có màu đỏ đậm hơn xung quanh đường chéo. Sự phổ biến của các mẫu tương đồng cao giữa các module liền kề cho thấy một mức độ dư thừa đáng kể, và việc loại bỏ một module có tác động không đáng kể đến các biểu diễn cuối cùng. Mặt khác, chúng tôi quan sát thấy một mẫu bàn cờ riêng biệt trên các ma trận tự tương đồng của mô hình One Wide FFN, nơi các module riêng lẻ có xu hướng thể hiện sự tương đồng thấp hơn với những lân cận trực tiếp của chúng so với những lân cận thứ hai (tức là, những lân cận của những lân cận). Trên encoder, mẫu bàn cờ xuất hiện đặc biệt trong các module sớm hơn trong khi trên decoder, mẫu đó xuất hiện nhất quán hơn khắp các tầng. Mẫu này đưa ra một chỉ dẫn rằng mô hình của chúng tôi đang học các biến đổi không tầm thường của đầu vào, dẫn đến sự giảm dư thừa trong mạng.

4.5 Các kiến trúc và Ngôn ngữ khác
Cho đến nay, tất cả các thí nghiệm của chúng tôi đều tập trung vào Transformer Big và trên WMT22 EN → DE. Trong phần này, chúng tôi áp dụng những gì chúng tôi đã học vào các kiến trúc và cặp ngôn ngữ khác. Chúng tôi chạy các thí nghiệm trên hướng ngôn ngữ tài nguyên thấp EN → RO và một mô hình đa ngôn ngữ quy mô lớn.

Đối với EN → DE, chúng tôi áp dụng đề xuất của chúng tôi vào mô hình Transformer Base, mô hình Deep Encoder Shallow Decoder (Kasai et al., 2021), và mô hình Decoder-Only. Đối với Transformer Base, chúng tôi quan sát thấy mức tăng độ chính xác 0.5 BLEU (2.2 BLEU so với mô hình SharedEncNoDec vanilla) và tăng tốc độ suy luận khoảng 25%. Trong mô hình Deep Encoder Shallow Decoder, chúng tôi quan sát thấy mức tăng độ chính xác khiêm tốn hơn là 0.2 điểm BLEU (0.9 BLEU so với mô hình SharedEncNoDec vanilla). Tuy nhiên, việc tăng tốc độ suy luận từ việc loại bỏ FFN decoder là tối thiểu (<1%), điều này được mong đợi vì độ sâu nhỏ của decoder trong kiến trúc này.

Mô hình chỉ decoder Với sự xuất hiện của các Mô hình Ngôn ngữ Lớn (LLM) như GPT (Brown et al., 2020) và PaLM (Chowdhery et al., 2022), rất nhiều nỗ lực đã được đặt vào các mô hình Transformer chỉ decoder. Chúng tôi huấn luyện một mô hình chỉ decoder trên WMT22 EN → DE, như được hiển thị trong Bảng 6. Do không có encoder, chúng tôi bị giới hạn trong việc áp dụng một FFN rộng ở phía decoder. Như trong các thiết lập khác, chúng tôi đạt được mức tăng độ chính xác +0.3 BLEU so với mô hình chỉ decoder baseline (+1.7 BLEU so với ShareDec), nhưng độ trễ giảm 12%. Điều này không đáng ngạc nhiên: do tính chất tự hồi quy của decoder, việc tăng kích thước FFN của nó có tác động lớn hơn đến tốc độ.

Ngôn ngữ tài nguyên thấp Trong EN → RO, độ chính xác của Mô hình One Wide FFN chỉ ngang bằng so với mô hình cơ sở, mặc dù nó cao hơn mô hình SharedEncNoDec vanilla. Chúng tôi đưa ra giả thuyết rằng do điều kiện tài nguyên thấp, mô hình đề xuất của chúng tôi đã đạt đến độ bão hòa vì không có nhiều mẫu văn bản nổi bật để FFN học.

Đa ngôn ngữ Cuối cùng, chúng tôi quan sát xu hướng tương tự trong thiết lập đa ngôn ngữ, nơi Mô hình One Wide FFN chính xác hơn +1.2 điểm SPBLEU so với Transformer Big baseline và +2.5 điểm SPBLEU so với SharedEncNoDec vanilla, mức tăng này đáng kể trong 79 trên 90 hướng và khi tất cả các tập kiểm tra được nối lại. Ngoài ra, mức tăng độ chính xác lớn này cũng đi kèm với tăng tốc độ suy luận khoảng 18%, phù hợp với các kết quả trước đây của chúng tôi.

5 Nghiên cứu Liên quan
Cắt tỉa trọng số và chia sẻ tham số là những kỹ thuật nổi tiếng để giảm dấu chân của mô hình. Với quy mô của các mô hình mới nhất (Chowdhery et al., 2022), đã có nhiều nỗ lực để cắt tỉa neuron dựa trên các phương pháp tự động khác nhau (Dalvi et al., 2020; Michel et al., 2019; Voita et al., 2019), chia sẻ tham số hiệu quả (Ge et al., 2022; Reid et al., 2021), và phân tích nhân tố một số thành phần (Lan et al., 2020; Hu et al., 2022).

Các phương pháp cắt tỉa neuron thường tập trung vào việc tìm và cắt tỉa các neuron dư thừa thông qua các phương pháp tương quan (Dalvi et al., 2020), nhưng cũng về cách các thành phần Transformer như multi-head attention có thể được cắt tỉa đáng kể do sự dư thừa mô hình trong encoder hoặc decoder bằng cách kiểm tra tính nổi bật gradient (Michel et al., 2019) hoặc một sự nới lỏng có thể vi phân của regularization l0 tại thời điểm huấn luyện (Voita et al., 2019).

Đối với việc chia sẻ tham số, Universal Transformer (Dehghani et al., 2019) đã đề xuất một mô hình nơi tất cả các tầng được chia sẻ (tức là, trên thực tế nó giảm mô hình thành một tầng được chia sẻ duy nhất). Takase and Kiyono (2023) đề xuất tìm một cấu hình tối ưu của các tầng được chia sẻ trong encoder hoặc decoder thông qua các phương pháp chia sẻ khác nhau (theo thứ tự, theo chu kỳ, hoặc theo chu kỳ ngược) luôn giữ một số tầng cuối cùng được chỉ định⁹. Tương tự, Reid et al. (2021) đề xuất một cách tiếp cận nơi chỉ các tầng giữa được chia sẻ, trong khi các tầng dưới và trên độc lập, và sử dụng chiều thấp hơn cho tầng nhúng. Tương tự, Ge et al. (2022) tập trung vào việc giảm thiểu số lượng tham số và số lượng cuộc gọi đến nhóm tham số của mỗi tham số để tối ưu hóa các mô hình trên thiết bị. Họ đạt được điều này bằng cách chia sẻ encoder và decoder theo cách tương tự như cả hai phương pháp trước đây, đặc biệt là bằng cách chia sẻ tất cả các tham số tầng theo chu kỳ như Takase and Kiyono (2023).

Các nghiên cứu trước đây cũng tập trung vào việc giảm chiều của một số tham số, chủ yếu thông qua phân tích nhân tố thứ hạng thấp. Lan et al. (2020) phân tích tầng nhúng thành một ma trận nhúng thứ hạng thấp hơn và một phép chiếu đến kích thước ẩn thực tế đồng thời cũng chia sẻ tất cả các tham số trên tất cả các tầng. Ngoài việc chia sẻ tham số hiệu quả, Ge et al. (2022) đề xuất một sự phân tích nhẹ của FFN nơi thay vì một thành phần duy nhất có 2 phép chiếu với chiều nhỏ hơn so với Transformer vanilla. Nghiên cứu của chúng tôi gần với Ge et al. (2022) nhưng thay vì phân tích nhân tố, chúng tôi khám phá việc chia sẻ và cắt tỉa hoàn toàn FFN. Ngược lại với các nghiên cứu trước đây, chúng tôi cũng khám phá việc tăng kích thước FFN encoder trong khi loại bỏ hoàn toàn FFN của decoder.

6 Kết luận
Trong nghiên cứu này, chúng tôi đã nghiên cứu tầm quan trọng của FFN trong các mô hình Transformer. Chúng tôi đã phân tích tác động của việc loại bỏ và/hoặc chia sẻ FFN trên các tầng và thấy rằng, do sự dư thừa của thành phần này, kích thước mô hình có thể được giảm đáng kể với ít tác động đến độ chính xác cho Dịch máy. Cụ thể, chúng tôi thấy rằng việc chia sẻ FFN trên tất cả các tầng encoder trong khi làm cho nó lớn hơn và loại bỏ nó khỏi các tầng decoder dẫn đến các mô hình chính xác hơn và nhanh hơn trong suy luận.

Các phát hiện của chúng tôi có thể áp dụng trên nhiều thiết lập, bao gồm các mô hình chỉ decoder và đa ngôn ngữ. Trong thiết lập tài nguyên thấp, kết quả khiêm tốn nhưng cách tiếp cận của chúng tôi vẫn có thể khôi phục hiệu suất của baseline với suy luận nhanh hơn.

Cuối cùng, chúng tôi đã tiến hành phân tích tương đồng kỹ lưỡng giữa Transformer vanilla và các kiến trúc đề xuất của chúng tôi, và thấy rằng các biểu diễn nội tại của những kiến trúc sau không khác biệt đáng kể so với những kiến trúc trước, ngoại trừ việc chúng ít dư thừa hơn.

Hạn chế
Trong nghiên cứu này, trọng tâm của chúng tôi là Dịch máy. Mặc dù chúng tôi mong đợi kết quả sẽ tổng quát hóa cho các tác vụ chuỗi-đến-chuỗi khác, cần có thêm các thí nghiệm, mà chúng tôi để dành cho nghiên cứu tương lai.

Tuyên bố Đạo đức
Một cân nhắc quan trọng là tiêu thụ năng lượng cho việc huấn luyện mô hình, dẫn đến phát thải khí nhà kính (Strubell et al., 2019). Nghiên cứu của chúng tôi sử dụng các tập dữ liệu hiện có và kế thừa một số rủi ro liên quan đến chúng, như rò rỉ riêng tư (Carlini et al., 2021) và thiên vị giới tính (Cho et al., 2019). Các chiến lược giảm thiểu như những chiến lược từ Vanmassenhove et al. (2018) có thể cần thiết.

Lời cảm ơn
Chúng tôi muốn cảm ơn Robin Schmidt, Matthias Sperber, và Stephan Peitz vì phản hồi và hỗ trợ của họ trong việc xem xét nghiên cứu này.

--- TRANG 10 ---
[Danh sách tài liệu tham khảo được dịch sang tiếng Việt]

--- TRANG 11 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 12 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 13 ---
[Phụ lục A - Chia sẻ Tùy chỉnh của Nhiều FFN]

--- TRANG 14 ---
[Phụ lục B - Chia sẻ hoặc Loại bỏ Attention]

[Phụ lục C - Chi tiết về Phân tích Biểu diễn Nội tại]

[Phụ lục D - Tác động của kích thước batch đến tốc độ giải mã]
