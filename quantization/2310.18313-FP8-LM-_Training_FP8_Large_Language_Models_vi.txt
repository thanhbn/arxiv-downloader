# FP8-LM: Huấn luyện Mô hình Ngôn ngữ Lớn FP8
Houwen Peng∗Kan Wu∗Yixuan Wei∗
Guoshuai Zhao Yuxiang Yang Ze Liu Yifan Xiong Ziyue Yang
Bolin Ni Jingcheng Hu Ruihang Li Miaosen Zhang Chen Li Jia Ning Ruizhe Wang Zheng Zhang
Shuguang Liu Joe Chau Han Hu†Peng Cheng†
Microsoft Azure và Microsoft Research
Tóm tắt
Trong bài báo này, chúng tôi khám phá các định dạng dữ liệu bit thấp FP8 để huấn luyện hiệu quả các mô hình ngôn ngữ lớn (LLM). Hiểu biết quan trọng của chúng tôi là hầu hết các biến, chẳng hạn như gradient và trạng thái optimizer, trong huấn luyện LLM có thể sử dụng các định dạng dữ liệu độ chính xác thấp mà không làm giảm độ chính xác của mô hình và không yêu cầu thay đổi siêu tham số. Cụ thể, chúng tôi đề xuất một khung độ chính xác hỗn hợp tự động FP8 mới để huấn luyện LLM. Khung này cung cấp ba cấp độ sử dụng FP8 để đơn giản hóa huấn luyện song song phân tán và độ chính xác hỗn hợp cho LLM. Nó từ từ kết hợp gradient 8-bit, trạng thái optimizer và học phân tán theo cách tăng dần. Kết quả thí nghiệm cho thấy rằng, trong quá trình huấn luyện mô hình GPT-175B trên nền tảng GPU H100, khung huấn luyện độ chính xác hỗn hợp FP8 của chúng tôi không chỉ đạt được mức giảm đáng kể 39% về việc sử dụng bộ nhớ thực mà còn chạy nhanh hơn 75% so với khung BF16 được áp dụng rộng rãi (tức là Megatron-LM), vượt qua tốc độ của Nvidia Transformer Engine 37%. Điều này giảm đáng kể chi phí huấn luyện cho các mô hình nền tảng lớn. Hơn nữa, phương pháp huấn luyện độ chính xác hỗn hợp FP8 của chúng tôi là tổng quát. Nó có thể được áp dụng một cách liền mạch cho các tác vụ khác như điều chỉnh hướng dẫn LLM và học tăng cường với phản hồi của con người, mang lại tiết kiệm trong chi phí tinh chỉnh. Khung huấn luyện độ chính xác thấp FP8 của chúng tôi được mở nguồn tại aka.ms/MS.AMP.

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) (Brown et al., 2020; Smith et al., 2022; Chowdhery et al., 2022; Zhang et al., 2022) đã thể hiện khả năng chưa từng có trong hiểu và tạo ngôn ngữ, dẫn đến những đột phá trong lý luận, toán học, khoa học và nhiều tác vụ khác (OpenAI, 2023; Anil et al., 2023). Tuy nhiên, việc huấn luyện LLM cực kỳ tốn kém. Ví dụ, PaLM cần 6.144 chip TPUv4 để huấn luyện mô hình 540B, trong khi GPT-3 175B tiêu thụ vài nghìn petaflop/s-ngày tính toán cho tiền huấn luyện (Chowdhery et al., 2022; Brown et al., 2020). Điều này thúc đẩy nhu cầu giảm chi phí huấn luyện LLM, đặc biệt là cho việc mở rộng quy mô các mô hình siêu thông minh thế hệ tiếp theo.

Huấn luyện độ chính xác thấp là một trong những hướng đầy hứa hẹn nhất để giảm chi phí, vì nó có thể cung cấp tốc độ cao, dung lượng bộ nhớ nhỏ và chi phí giao tiếp thấp. Hầu hết các hệ thống huấn luyện hiện có, ví dụ như Megatron-LM (Shoeybi et al., 2019), MetaSeq (Zhang et al., 2022) và Colossal-AI (Li et al., 2023a), huấn luyện LLM với độ chính xác đầy đủ FP32 hoặc độ chính xác hỗn hợp FP16/BF16 theo mặc định. Tuy nhiên, điều này không cần thiết để đạt được độ chính xác đầy đủ cho các mô hình lớn. Với việc phát hành GPU Nvidia H100, FP8 đang trở thành kiểu dữ liệu thế hệ tiếp theo cho biểu diễn độ chính xác thấp (Nvidia, 2022a; Micikevicius et al., 2022). Về mặt lý thuyết, FP8 có thể đạt được tăng tốc 2×, tiết kiệm chi phí bộ nhớ 50% - 75% và tiết kiệm giao tiếp 50% - 75% so với huấn luyện độ chính xác hỗn hợp điểm thả nổi 16-bit và 32-bit hiện tại, điều này rất hứa hẹn cho việc mở rộng quy mô các mô hình nền tảng thế hệ tiếp theo.

Thật không may, sự hỗ trợ hiện tại cho huấn luyện FP8 là hiếm hoi và hạn chế. Khung duy nhất có thể sử dụng là Nvidia Transformer Engine (TE) (Nvidia, 2022b), nhưng nó chỉ áp dụng FP8 cho tính toán GEMM và vẫn giữ lại trọng số chủ và gradient sử dụng độ chính xác cao, ví dụ như FP16 hoặc FP32. Kết quả là, tăng tốc từ đầu đến cuối, tiết kiệm chi phí bộ nhớ và giao tiếp rất hạn chế, điều này không hoàn toàn khai thác sức mạnh của FP8. Để giải quyết vấn đề này, chúng tôi đề xuất một khung độ chính xác hỗn hợp FP8 được tối ưu hóa cực kỳ cho huấn luyện LLM. Ý tưởng cốt lõi là thâm nhập tính toán FP8, lưu trữ và giao tiếp vào toàn bộ quá trình huấn luyện mô hình lớn, làm cho lượt truyền thuận và nghịch đều sử dụng FP8 độ chính xác thấp, từ đó giảm đáng kể khối lượng công việc hệ thống so với các khung trước đây (Micikevicius et al., 2017; Nvidia, 2022b; Micikevicius et al., 2022). Cụ thể, chúng tôi thiết kế ba cấp độ tối ưu hóa sử dụng FP8 để đơn giản hóa huấn luyện phân tán và độ chính xác hỗn hợp. Ba cấp độ này từ từ kết hợp giao tiếp tập thể 8-bit, optimizer và huấn luyện song song phân tán theo cách tăng dần. Cấp độ tối ưu hóa cao hơn cho thấy việc sử dụng nhiều FP8 hơn trong quá trình huấn luyện LLM. Hơn nữa, đối với huấn luyện quy mô lớn, chẳng hạn như GPT-175B được huấn luyện trên hàng nghìn GPU, khung của chúng tôi cung cấp song song bit thấp FP8, bao gồm song song tensor, pipeline và sequence, mở đường cho huấn luyện song song độ chính xác thấp thế hệ tiếp theo.

Huấn luyện LLM với FP8 không đơn giản. Các thách thức bắt nguồn từ các vấn đề như tràn dưới hoặc tràn trên dữ liệu, kết hợp với lỗi lượng tử hóa phát sinh từ phạm vi động hẹp hơn và độ chính xác giảm vốn có trong các định dạng dữ liệu FP8. Những thách thức này gây ra bất ổn định số và phân kỳ không thể đảo ngược trong suốt quá trình huấn luyện. Để giải quyết chúng, chúng tôi đề xuất hai kỹ thuật: tách rời độ chính xác và chia tỷ lệ tự động để ngăn chặn mất thông tin quan trọng. Kỹ thuật đầu tiên liên quan đến việc tách rời ảnh hưởng của độ chính xác dữ liệu đối với các tham số như trọng số, gradient, trạng thái optimizer và gán độ chính xác giảm cho các thành phần không nhạy cảm với độ chính xác. Kỹ thuật thứ hai là bảo tồn các giá trị gradient trong phạm vi biểu diễn của các định dạng dữ liệu FP8 thông qua việc điều chỉnh động các hệ số chia tỷ lệ tensor, từ đó giảm thiểu các trường hợp tràn dưới và tràn trên trong quá trình giao tiếp all-reduce.

Để xác thực khung độ chính xác thấp FP8 được đề xuất, chúng tôi áp dụng nó cho huấn luyện mô hình kiểu GPT, bao gồm cả tiền huấn luyện và điều chỉnh có giám sát (SFT). Kết quả thí nghiệm chứng minh hiệu quả của phương pháp FP8 của chúng tôi, mang lại lợi ích đáng kể bao gồm giảm 29% đến 39% việc sử dụng bộ nhớ thực (ví dụ, giảm 29% cho GPT-7B trong khi 39% cho GPT-175B) và giảm đáng kể 63% đến 65% chi phí giao tiếp liên quan đến trọng số so với phương pháp huấn luyện độ chính xác hỗn hợp BF16 phổ biến. Không có thay đổi nào đối với bất kỳ siêu tham số nào, chẳng hạn như tốc độ học và suy giảm trọng số, các mô hình được huấn luyện bằng FP8 thể hiện hiệu suất tương đương với những mô hình sử dụng độ chính xác cao BF16, cả trong tiền huấn luyện và các tác vụ xuôi dòng. Đáng chú ý là trong quá trình huấn luyện mô hình GPT-175B, khung độ chính xác hỗn hợp FP8 của chúng tôi giảm thời gian huấn luyện 37% so với TE (Nvidia, 2022b), trong khi tiêu thụ ít bộ nhớ hơn 42% trên nền tảng GPU H100. Quan trọng hơn, việc giảm chi phí đạt được thông qua việc sử dụng FP8 độ chính xác thấp có thể được tăng thêm, khi quy mô mô hình tiếp tục mở rộng, được trình bày trong Hình 1.

Đối với tinh chỉnh, chúng tôi sử dụng độ chính xác hỗn hợp FP8 cho điều chỉnh hướng dẫn và học tăng cường với phản hồi của con người (RLHF) để căn chỉnh tốt hơn các LLM được tiền huấn luyện với các tác vụ cuối và sở thích của người dùng. Cụ thể, chúng tôi tinh chỉnh các mô hình được tiền huấn luyện trên dữ liệu tuân theo hướng dẫn được chia sẻ công khai bởi người dùng (ShareGPT, 2023). Các mô hình được điều chỉnh với độ chính xác hỗn hợp FP8 của chúng tôi thể hiện hiệu suất tương đương với những mô hình sử dụng BF16 nửa độ chính xác (Zheng et al., 2023) trên các benchmark AlpacaEval (Li et al., 2023b) và MT-Bench (Zheng et al., 2023), trong khi đạt được cải thiện 27% về tốc độ huấn luyện. Hơn nữa, độ chính xác hỗn hợp FP8 thể hiện tiềm năng đáng kể trong RLHF, một quá trình cần thiết phải tải nhiều mô hình trong quá trình huấn luyện. Thông qua việc sử dụng FP8 trong huấn luyện, khung RLHF phổ biến AlpacaFarm (Dubois et al., 2023) có thể mang lại giảm 32% trọng số mô hình và giảm 62% tiêu thụ bộ nhớ trạng thái optimizer. Điều này tiếp tục chứng minh tính linh hoạt và khả năng thích ứng của khung huấn luyện độ chính xác thấp FP8 của chúng tôi.

Chúng tôi đang đóng góp những điều sau đây để thúc đẩy thiết kế huấn luyện độ chính xác thấp FP8 thế hệ tiếp theo cho LLM.

• Một khung huấn luyện độ chính xác hỗn hợp FP8 mới. Nó mở khóa trọng số 8-bit, gradient, optimizer và huấn luyện phân tán từ từ theo cách bổ sung, thuận tiện sử dụng. Khung 8-bit này có thể được sử dụng như một sự thay thế đơn giản cho các đối tác độ chính xác hỗn hợp 16/32-bit hiện có, mà không yêu cầu bất kỳ thay đổi nào đối với siêu tham số và công thức huấn luyện. Ngoài ra, chúng tôi cung cấp một triển khai Pytorch cho phép huấn luyện độ chính xác thấp 8-bit chỉ trong vài dòng mã.

• Một họ mô hình kiểu GPT mới được huấn luyện với FP8. Chúng tôi áp dụng sơ đồ FP8 được đề xuất cho tiền huấn luyện và tinh chỉnh GPT (tức là SFT và RLHF), và chứng minh tiềm năng của nó trên nhiều quy mô mô hình khác nhau từ 7B đến 175B tham số. Chúng tôi trang bị các mô hình tính toán song song phổ biến với hỗ trợ FP8, bao gồm song song tensor, pipeline và sequence, cho phép sử dụng FP8 để huấn luyện các mô hình nền tảng lớn. Chúng tôi mở nguồn cơ sở mã huấn luyện GPT FP8 đầu tiên dựa trên triển khai Megatron-LM (Shoeybi et al., 2019).

Chúng tôi mong đợi việc phát hành khung FP8 của chúng tôi sẽ thiết lập một mô hình mới cho hệ thống huấn luyện độ chính xác thấp thế hệ tiếp theo dành cho các mô hình nền tảng lớn.

2 FP8 LLMs

Độ chính xác hỗn hợp (Micikevicius et al., 2017) đã được sử dụng rộng rãi trong huấn luyện LLM để cải thiện hiệu quả tính toán và bộ nhớ. Các sơ đồ độ chính xác hỗn hợp phổ biến nhất là FP16-FP32 và BF16-FP32. Do phạm vi số hạn chế của FP16, sơ đồ FP16-FP32 đã được biết đến với sự bất ổn định khi huấn luyện các mô hình lớn (Rae et al., 2021; Zeng et al., 2022). Do đó, cộng đồng hiện nay thường áp dụng BF16-FP32 để huấn luyện LLM, chẳng hạn như Megatron-Turing NLG-530B (Smith et al., 2022), Bloom-175B (Scao et al., 2022) và Gopher (Rae et al., 2021). Lý do cơ bản là BF16 có phạm vi động rộng để duy trì sự ổn định số trong khi khớp với hiệu suất của FP32 độ chính xác đầy đủ. Hơn nữa, BF16 sử dụng một nửa số bit so với FP32, do đó giảm đáng kể dung lượng bộ nhớ trong khi cải thiện hiệu quả tính toán.

FP8 là một sự tiến hóa tự nhiên từ các định dạng dữ liệu 16-bit để giảm thêm chi phí tính toán. Tuy nhiên, huấn luyện LLM với FP8 độ chính xác giảm đặt ra những thách thức mới. Phạm vi động và độ chính xác biểu diễn của FP8 thấp hơn nhiều so với BF16 và FP16, điều này không tránh khỏi gây ra nhiều sự sụp đổ huấn luyện hơn, chẳng hạn như đột biến mất mát hoặc thậm chí NaN. Để giải quyết các vấn đề này, các kỹ thuật chia tỷ lệ tensor được đề xuất (Sun et al., 2019; Micikevicius et al., 2022). Ý tưởng cốt lõi là nhân các giá trị độ chính xác cao hơn với hệ số chia tỷ lệ trước khi ép chúng thành FP8 để di chuyển chúng vào một phạm vi tốt hơn chồng lấp với phạm vi có thể biểu diễn của định dạng FP8 tương ứng (Micikevicius et al., 2022). Kỹ thuật chia tỷ lệ theo tensor như vậy giảm lỗi lượng tử hóa dữ liệu trong khi cải thiện sự ổn định số và độ chính xác, do đó cho phép sử dụng FP8 độ chính xác thấp hơn để huấn luyện các mô hình lớn.

Thật không may, sự hỗ trợ hiện tại cho huấn luyện độ chính xác thấp FP8 bị hạn chế. Nvidia TE (Nvidia, 2022b) chỉ hỗ trợ tính toán FP8 cho các lớp tuyến tính trong Transformer (Vaswani et al., 2017), trong khi để lại tất cả các hoạt động khác, chẳng hạn như cập nhật trọng số và đồng bộ hóa gradient, vẫn sử dụng độ chính xác cao hơn. Trong công trình này, chúng tôi trình bày một chiến lược độ chính xác hỗn hợp FP8 được tối ưu hóa cực kỳ cho huấn luyện LLM. Tối ưu hóa FP8 mới bao gồm ba quan điểm chính: giao tiếp FP8, optimizer FP8 và huấn luyện phân tán FP8. Bằng cách tích hợp các khía cạnh này, việc huấn luyện LLM như mô hình GPT-3 175B có thể khai thác đầy đủ những lợi thế của độ chính xác thấp FP8 và cải thiện hiệu quả huấn luyện.

2.1 Gradient FP8 và Giao tiếp All-Reduce

Các phương pháp huấn luyện độ chính xác hỗn hợp hiện có (Micikevicius et al., 2017; Nvidia, 2022b) thường sử dụng kiểu dữ liệu 16-bit hoặc 32-bit cho tính toán và lưu trữ gradient, dẫn đến yêu cầu băng thông cao cho giao tiếp tập thể trong suốt quá trình huấn luyện. Chúng tôi phát hiện rằng việc áp dụng trực tiếp FP8 cho gradient dẫn đến giảm độ chính xác. Vấn đề cơ bản nằm ở các vấn đề tràn dưới và tràn trên phát sinh từ hoạt động all-reduce bit thấp. Cụ thể, có hai phương pháp tiêu chuẩn tổng hợp gradient trên các GPU trong quá trình all-reduce: chia tỷ lệ trước và chia tỷ lệ sau. Chia tỷ lệ trước chia gradient gi được tính toán trên GPU thứ i cho tổng số GPU (tức là N) trước khi được tổng hợp, được công thức hóa như sau:

g = g1/N + g2/N + ··· + gN/N. (1)

Khi N lớn, phép chia này có thể gây ra tràn dưới dữ liệu, đặc biệt đối với biểu diễn độ chính xác thấp FP8 của gradient. Để giảm thiểu vấn đề này, chia tỷ lệ sau thực hiện tổng gradient trước, sau đó là chia tỷ lệ trong quá trình thu thập gradient:

g = (g1 + g2 + ··· + gN)/N. (2)

Phương pháp chia tỷ lệ sau này giữ gradient gần với giá trị tối đa của kiểu dữ liệu FP8, hiệu quả giảm thiểu vấn đề tràn dưới. Tuy nhiên, phương pháp này gặp phải vấn đề tràn trên khi tổng hợp gradient.

Ngược lại, chúng tôi đề xuất một kỹ thuật chia tỷ lệ tự động để giải quyết cả vấn đề tràn dưới và tràn trên trong các phương pháp chia tỷ lệ trước và chia tỷ lệ sau. Cụ thể, chúng tôi giới thiệu một hệ số tự động chia tỷ lệ μ, thay đổi một cách linh hoạt trong quá trình huấn luyện, để giảm các trường hợp tràn trên và tràn dưới trong gradient:

g'i = μ·gi. (3)

Một phân tích thống kê được tiến hành trên các giá trị gradient của g'i, với mục tiêu lượng hóa tỷ lệ các giá trị đạt được giá trị tối đa khả thi trong phạm vi biểu diễn FP8. Nếu tỷ lệ giá trị tối đa vượt quá ngưỡng chỉ định, tức là 0,001%, μ được đặt thành 1/2 trong bước huấn luyện tiếp theo, từ đó giảm thiểu rủi ro tràn trên. Ngược lại, khi tỷ lệ luôn duy trì dưới ngưỡng, chúng tôi chọn tăng μ lên 2 một cách theo hàm mũ trong khoảng 1.000 bước huấn luyện, từ đó hiệu quả giảm thiểu rủi ro xảy ra tràn dưới.

Một trở ngại chính khác của giao tiếp tập thể FP8 nằm ở việc thiết kế một chiến lược hiệu quả để quản lý các hệ số chia tỷ lệ theo tensor được liên kết với mỗi tensor gradient. Triển khai NCCL hiện tại (Nvidia, 2020) thiếu khả năng thực hiện hoạt động all-reduce có xem xét các hệ số chia tỷ lệ theo tensor bổ sung. Trong khi đó, triển khai hiệu quả cũng rất thách thức, đặc biệt là xem xét rằng tổng gradient NCCL hoạt động ở cấp độ tensor con. Độ phức tạp này tăng đáng kể khi kết hợp các cập nhật cho các hệ số chia tỷ lệ theo tensor. Để khắc phục vấn đề này, chúng tôi đề xuất một cơ chế mới chia tỷ lệ gradient FP8 trên các GPU bằng cách sử dụng một vô hướng được chia sẻ duy nhất. Cụ thể, hãy để (g'i, s'i) biểu thị một tensor chia tỷ lệ lưu trữ gradient trọng số trong GPU thứ i, trong đó g'i là tensor FP8 và s'i là hệ số chia tỷ lệ tương ứng. Gradient trọng số thực tế là g'i/s'i. Trước hoạt động all-reduce trên các tensor gradient, chúng tôi trước tiên thu thập các hệ số chia tỷ lệ s'i của mỗi tensor gradient trên tất cả GPU và tính toán hệ số chia tỷ lệ tối thiểu toàn cục s'g như sau:

s'g = min(s'1, s'2, ..., s'N), (4)

trong đó hệ số chia tỷ lệ tối thiểu toàn cục s'g được chia sẻ trên các GPU. Chúng tôi sử dụng hệ số chia tỷ lệ được chia sẻ này s'g để thống nhất việc chia tỷ lệ lại các tensor gradient trên các GPU. Bằng cách này, tất cả các tensor gradient liên kết với cùng trọng số sử dụng cùng hệ số chia tỷ lệ được chia sẻ để lượng tử hóa các tensor thành định dạng FP8 trên tất cả GPU:

g''i = FP8(s'g·(g'i/s'i)). (5)

Phương pháp này giảm chi phí giao tiếp bằng cách chỉ truyền một vô hướng duy nhất s'g, làm cho bước đồng bộ hóa bổ sung rất hiệu quả. Vì các tensor đầu vào chia sẻ cùng hệ số chia tỷ lệ, nó loại bỏ nhu cầu xem xét all-reduce các hệ số chia tỷ lệ song song và cho phép thực hiện hoạt động all-reduce NCCL tiêu chuẩn. Gradient được thu thập cuối cùng được tính như sau:

g = g''1 + g''2 + ··· + g''N, s = N·s'g, (6)

trong đó g là gradient được tổng hợp cuối cùng và s là hệ số chia tỷ lệ tương ứng. Chia tỷ lệ lại hệ số chia tỷ lệ cho gradient được tổng g tương đương với việc chia g cho N về mặt lý thuyết. Bằng cách triển khai các chiến lược kép về chia tỷ lệ phân tán và tự động nói trên, chúng tôi có thể thành công thực hiện giao tiếp gradient bit thấp FP8 trong khi bảo tồn độ chính xác mô hình. Hơn nữa, phương pháp này bao gồm lưu trữ gradient trong FP8 và tiến hành giao tiếp trong FP8, từ đó mang lại giảm việc sử dụng bộ nhớ GPU và tiêu thụ băng thông giao tiếp.

2.2 Optimizer FP8

Trong việc huấn luyện LLM, Adam và các biến thể của nó (Kingma và Ba, 2015; Loshchilov và Hutter, 2018) là các phương pháp tối ưu hóa được sử dụng thường xuyên nhất, duy trì các bản sao của trọng số mô hình, gradient, moment gradient bậc một và bậc hai để cập nhật mô hình. Huấn luyện độ chính xác hỗn hợp (Micikevicius et al., 2017) với optimizer Adam thường lưu trữ trọng số chủ, gradient và moment gradient trong định dạng float 32-bit để ổn định số (Shoeybi et al., 2019; Rajbhandari et al., 2020; Zhang et al., 2022; Scao et al., 2022). Do đó, optimizer Adam tiêu thụ 16 byte bộ nhớ cho mỗi tham số trong quá trình huấn luyện:

4 (trọng số chủ) + 4 (gradient) + 4 + 4 (trạng thái Adam) = 16 byte. (7)

Khi kích thước mô hình lớn, việc tiêu thụ bộ nhớ của các biến trong Adam sẽ trở thành nút thắt cổ chai. Công trình trước đây (Rae et al., 2021; Zeng et al., 2022; Liu et al., 2022) đã tiết lộ rằng việc giảm độ chính xác của các biến trong optimizer xuống 16-bit dẫn đến suy giảm độ chính xác khi huấn luyện các mô hình quy mô tỷ. Điều này thúc đẩy việc đánh giá biến nào trong optimizer nên được phân bổ độ chính xác cao và biến nào có thể được chứa với độ chính xác thấp.

Để làm rõ, chúng tôi tách rời ảnh hưởng của độ chính xác dữ liệu đối với các biến trong optimizer và điều tra biến nào có thể được gán độ chính xác thấp hơn, tức là tách rời độ chính xác. Chúng tôi tìm thấy một nguyên tắc hướng dẫn: thống kê gradient có thể sử dụng độ chính xác thấp hơn, trong khi trọng số chủ cần độ chính xác cao. Cụ thể hơn, moment gradient bậc một có thể chịu đựng lỗi lượng tử hóa cao và có thể được gán với FP8 độ chính xác thấp, trong khi moment bậc hai yêu cầu độ chính xác cao hơn, như được phân tích trong Mục 3.3. Điều này bắt nguồn từ thực tế rằng, trong quá trình cập nhật mô hình trong Adam, hướng của gradient có ý nghĩa lớn hơn độ lớn của nó. FP8 với chia tỷ lệ tensor có thể hiệu quả bảo tồn phân phối của moment bậc một như tensor độ chính xác cao, mặc dù nó gây ra suy giảm độ chính xác ở một mức độ nào đó. Tính toán bình phương gradient cho moment gradient bậc hai có thể dẫn đến tràn dưới dữ liệu do các giá trị gradient thường nhỏ. Do đó, việc phân bổ độ chính xác cao hơn 16-bit là cần thiết để bảo tồn độ chính xác số.

Mặt khác, chúng tôi thấy rằng việc giữ trọng số chủ sử dụng độ chính xác cao là rất quan trọng. Lý do cơ bản là các cập nhật trọng số đôi khi có thể trở nên cực kỳ nhỏ hoặc lớn trong quá trình huấn luyện, độ chính xác cao hơn cho trọng số chủ giúp ngăn chặn mất thông tin khi cập nhật trọng số, đảm bảo huấn luyện ổn định và chính xác hơn. Trong triển khai, trọng số chủ có hai tùy chọn khả thi: sử dụng độ chính xác đầy đủ FP32 hoặc FP16 với chia tỷ lệ tensor. FP16 với chia tỷ lệ tensor cung cấp lợi thế tiết kiệm bộ nhớ mà không làm giảm độ chính xác. Do đó, lựa chọn mặc định của chúng tôi là sử dụng FP16 với chia tỷ lệ tensor để lưu trữ trọng số chủ trong optimizer. Optimizer độ chính xác hỗn hợp FP8 của chúng tôi tiêu thụ 6 byte bộ nhớ cho mỗi tham số trong quá trình huấn luyện:

2 (trọng số chủ) + 1 (gradient) + 1 + 2 (trạng thái Adam) = 6 byte. (8)

Optimizer bit thấp mới này giảm dung lượng bộ nhớ 2,6 lần so với giải pháp trước đây, như được minh họa trong Phương trình (7). Đáng chú ý, đây là optimizer FP8 đầu tiên cho huấn luyện LLM. Các thí nghiệm trong Mục 3.2 cho thấy optimizer FP8 có thể bảo tồn độ chính xác mô hình ở các quy mô khác nhau, từ 125M đến 175B tham số.

2.3 Huấn luyện Song song Phân tán FP8

Huấn luyện LLM như GPT-3 yêu cầu các chiến lược học phân tán để song song hóa trên các GPU. Các chiến lược được sử dụng thường xuyên bao gồm song song dữ liệu, song song tensor, song song pipeline và song parallel sequence. Mỗi song song có ưu điểm riêng và đã được sử dụng theo cách bổ sung trong các hệ thống hiện có (Smith et al., 2022; Shoeybi et al., 2019; Zhang et al., 2022; Scao et al., 2022; Li et al., 2023a). Đối với hỗ trợ FP8 của các chiến lược này, cả song song dữ liệu và song song pipeline đều không cần bất kỳ sửa đổi cụ thể nào, vì chúng không liên quan đến tính toán và giao tiếp FP8 bổ sung khi chia các batch dữ liệu hoặc các lớp mô hình thành các phân đoạn trên các thiết bị.

Song song tensor phân vùng các lớp riêng lẻ của mô hình trên nhiều thiết bị, sao cho các phần của tensor trọng số, gradient và kích hoạt được đặt trên các GPU riêng biệt, thay vì trên một GPU duy nhất. Để trang bị song song tensor với FP8, chúng tôi chuyển đổi các tensor trọng số và kích hoạt được phân đoạn thành định dạng FP8 cho tính toán lớp tuyến tính, cho phép tính toán truyền thuận và giao tiếp tập thể gradient nghịch đều sử dụng FP8.

Mặt khác, song song sequence chia các chuỗi đầu vào thành nhiều đoạn và các chuỗi con được đưa vào các thiết bị khác nhau để tiết kiệm bộ nhớ kích hoạt. Như được hiển thị trong Hình 2, song song sequence và tensor được thực hiện song song với các phần khác nhau của mô hình Transformer để tận dụng tối đa bộ nhớ có sẵn và cải thiện hiệu quả huấn luyện. Có một bộ chuyển đổi g giữa các vùng song song sequence và tensor để all-gather các phân vùng sequence trong lượt truyền thuận (hoặc reduce-scatter các phân đoạn tensor trong lượt truyền nghịch). Chúng tôi thêm chuyển đổi kiểu dữ liệu FP8 trước g, sao cho hoạt động all-gather (hoặc reduce-scatter) sử dụng kích hoạt bit thấp FP8 để tiết kiệm chi phí giao tiếp trên các GPU.

Ngoài ra, Zero Redundancy Optimizer (ZeRO) (Rajbhandari et al., 2020) là một kỹ thuật học phân tán khác được sử dụng thường xuyên trong huấn luyện mô hình lớn. Ý tưởng cốt lõi của ZeRO là che khuất các trạng thái mô hình trên các thiết bị, sao cho mỗi thiết bị chỉ giữ một phần dữ liệu (ví dụ: trọng số chủ, gradient và trạng thái optimizer) cần thiết cho một bước huấn luyện. Để giảm tiêu thụ bộ nhớ, phương pháp ZeRO thường chia một tensor duy nhất thành nhiều phân vùng và phân phối chúng cho các thiết bị khác nhau. Việc áp dụng trực tiếp FP8 cho ZeRO là không khả thi, vì rất khó xử lý các hệ số chia tỷ lệ liên kết với các phân vùng FP8. Các hệ số chia tỷ lệ theo tensor nên được phân phối cùng với các phân vùng FP8. Để giải quyết vấn đề này, chúng tôi triển khai một sơ đồ phân phối FP8 mới phân phối mỗi tensor như một tổng thể trên các thiết bị, thay vì phân vùng nó thành nhiều tensor con như trong ZeRO. Việc phân phối tensor FP8 được xử lý theo cách tham lam, như được nêu trong Thuật toán 1. Cụ thể, phương pháp của chúng tôi trước tiên sắp xếp các tensor của trạng thái mô hình theo kích thước của chúng, sau đó phân phối các tensor cho các GPU khác nhau dựa trên kích thước bộ nhớ còn lại của mỗi GPU. Việc phân phối tuân theo nguyên tắc rằng các GPU có bộ nhớ còn lại lớn hơn được ưu tiên cao hơn trong việc nhận các tensor phân tán mới. Bằng cách này, các hệ số chia tỷ lệ tensor có thể được phân phối cùng với các tensor một cách mượt mà, trong khi giảm độ phức tạp giao tiếp và tính toán. Hình 3 trình bày minh họa trực quan về sự khác biệt trong phân vùng tensor ZeRO giữa các tình huống có và không có hệ số chia tỷ lệ.

3 Thí nghiệm

Trong phần này, chúng tôi đánh giá hiệu quả của phương pháp huấn luyện độ chính xác hỗn hợp FP8 được đề xuất trên các LLM kiểu GPT, bao gồm một loạt quy mô mô hình rộng, từ 125 triệu đến 175 tỷ tham số. Để đánh giá hiệu suất, chúng tôi so sánh các mô hình GPT được huấn luyện với FP8 với những mô hình được huấn luyện với BF16 nửa độ chính xác và FP32 độ chính xác đầy đủ. Để đánh giá tính tổng quát, chúng tôi tiến hành các thí nghiệm bao gồm cả tiền huấn luyện và tinh chỉnh bit thấp FP8, xem xét điều chỉnh hướng dẫn và căn chỉnh sở thích con người.

3.1 Thiết lập Thí nghiệm

3.1.1 Dữ liệu Huấn luyện

Dữ liệu tiền huấn luyện của chúng tôi được xây dựng bằng cách sử dụng các bộ sưu tập ngôn ngữ mã nguồn mở từ nhiều nguồn, bao gồm CommonCrawl, The Pile (Gao et al., 2020), C4 (Raffel et al., 2020), OpenWebText (Radford et al., 2019; Gokaslan và Cohen, 2019), CC-NEWS (Liu et al., 2019), CC-Stories (Trinh và Le, 2018), Redpajama (Redpajama, 2023) và Wikipedia. Chúng tôi áp dụng khử trùng lặp mờ (Lee et al., 2022) trên các ảnh chụp CommonCrawl để nâng cao chất lượng dữ liệu. Bảng 10 trong Phụ lục A.3 cung cấp chi tiết về dữ liệu tiền huấn luyện của chúng tôi, bao gồm thông tin như số lượng token từ mỗi nguồn và trọng số lấy mẫu liên quan. Để hiểu toàn diện hơn về dữ liệu và quy trình làm sạch của nó, độc giả được khuyến khích tham khảo Phụ lục A.3.

Hơn nữa, để điều chỉnh hướng dẫn, chúng tôi tuân theo cùng thiết lập như Vicuna-v1.1 (Vicuna Team, 2023), sử dụng dữ liệu tuân theo hướng dẫn được chia sẻ công khai bởi người dùng (ShareGPT, 2023). Để học tăng cường với phản hồi của con người, dữ liệu huấn luyện chúng tôi sử dụng là sự kết hợp của bộ dữ liệu Helpful and Harmless của Anthropic (Bai et al., 2022) và bộ dữ liệu Open-Assistant (Köpf et al., 2023). Khung huấn luyện và các cấu hình liên quan phù hợp với AlpacaFarm có sẵn công khai (Dubois et al., 2023).

3.1.2 Cấu hình Mô hình

Kiến trúc mô hình chúng tôi sử dụng là Transformer chỉ giải mã (Brown et al., 2020), được sử dụng rộng rãi trong các LLM tạo sinh gần đây như PaLM (Chowdhery et al., 2022), OPT (Zhang et al., 2022) và LLaMA (Touvron et al., 2023). Ngoài kiến trúc cơ bản, chúng tôi tích hợp một số sửa đổi được đề xuất gần đây để cải thiện hiệu quả và hiệu suất mô hình. 1) Rotary Positional Embedding: Lấy cảm hứng từ các thí nghiệm thành công gần đây (Black et al., 2022; Touvron et al., 2023), chúng tôi kết hợp mã hóa vị trí xoay (RoPE) (Su et al., 2021) vào phương pháp của chúng tôi. Bổ sung này cho phép chúng tôi nắm bắt cả thông tin vị trí tuyệt đối và tương đối, nâng cao hiệu suất đặc biệt khi ngoại suy đến cửa sổ ngữ cảnh lớn hơn. 2) Flash Attention: Triển khai attention tiêu chuẩn bị nút thắt cổ chai bởi truy cập bộ nhớ (Ivanov et al., 2021). Flash Attention (Dao et al., 2022) đề xuất một thuật toán attention chính xác nhận biết IO sử dụng tiling để giảm lượng truy cập HBM, đạt được tăng tốc đáng kể.

Chúng tôi huấn luyện các mô hình bằng optimizer FP8 được đề xuất, được xây dựng dựa trên Adam (Kingma và Ba, 2015) với suy giảm trọng số tách rời (Loshchilov và Hutter, 2018), tuân theo thực hành phổ biến với tỷ lệ suy giảm β1 = 0,9, β2 = 0,95 và suy giảm trọng số = 0,1. Lịch trình tốc độ học giống như cosine, và tốc độ học cuối cùng là 10% của tốc độ học tối đa. Chúng tôi huấn luyện các mô hình tổng cộng 100B token với kích thước batch 4M token, và độ dài chuỗi đầu vào được đặt thành 2048. Quá trình khởi động mô hình được tiến hành trong 1.000 lần lặp. Bảng 1 trình bày chi tiết các cấu hình mô hình và thiết lập huấn luyện tương ứng. Việc huấn luyện được tiến hành trên nền tảng GPU Azure NDv5 H100 (Microsoft, 2023).

3.2 Kết quả Chính

3.2.1 Hiệu suất Mô hình

Chúng tôi trước tiên so sánh hiệu suất của các mô hình được huấn luyện bằng độ chính xác hỗn hợp FP8 với những mô hình được huấn luyện bằng BF16. Trong Hình 4, mất mát tiền huấn luyện theo token được hiển thị cho các mô hình GPT 7B, 13B và 175B tham số. Các cấu hình huấn luyện và siêu tham số vẫn nhất quán trên các mô hình được huấn luyện với FP8 và BF16. Sự khác biệt duy nhất nằm ở các sơ đồ độ chính xác hỗn hợp được sử dụng. Như được hiển thị trong Hình 4, các đường cong mất mát gần như chồng lấp lên nhau. Kết quả rõ ràng chứng minh rằng sơ đồ độ chính xác hỗn hợp FP8 được đề xuất có thể đạt được hiệu suất tương đương với sơ đồ BF16 độ chính xác cao hơn phổ biến (Shoeybi et al., 2019; Rae et al., 2021; Hoffmann et al., 2022) trên một loạt quy mô mô hình đa dạng. Ngoài ra, chúng tôi đánh giá các mô hình được tiền huấn luyện trên một loạt các tác vụ xuôi dòng, bao gồm HellaSwag (HS) (Zellers et al., 2019), Lambada (Paperno et al., 2016) BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), COPA (Roemmele et al., 2011), Winogrande (Sakaguchi et al., 2021), Arc (Clark et al., 2018) và OpenbookQA (ObQA) (Mihaylov et al., 2018). Như được báo cáo trong Bảng 2, các mô hình được tiền huấn luyện FP8 thể hiện hiệu suất zero-shot tương đương so với các đối tác BF16 của chúng. Kết quả này cung cấp xác thực thêm rằng các mô hình được tiền huấn luyện với độ chính xác thấp FP8 duy trì cả độ chính xác và khả năng học trong ngữ cảnh nội tại ở mức tương đương với các đối tác độ chính xác cao của chúng.

Hơn nữa, chúng tôi tận dụng phương pháp độ chính xác hỗn hợp FP8 được đề xuất để tinh chỉnh LLM trong việc tuân theo hướng dẫn. Để so sánh công bằng, chúng tôi tuân theo cùng thiết lập điều chỉnh hướng dẫn như Vicuna-v1.1 (Vicuna Team, 2023), sử dụng LLaMA-7B mã nguồn mở (Touvron et al., 2023) làm mô hình cơ sở để tinh chỉnh. Hình 5 trình bày mất mát tinh chỉnh, trong đó các đường cong tương ứng với BF16 và FP8 hiển thị mức độ chồng lấp đáng chú ý. Trong khi đó, tỷ lệ thắng của các mô hình được tinh chỉnh FP8 của chúng tôi so với Davinci-003 (OpenAI, 2022) cũng tương đương với Vicuna-v1.1, được tinh chỉnh bằng BF16 nửa độ chính xác, như được báo cáo trong Bảng 3. Điều này cho thấy rằng sơ đồ huấn luyện bit thấp FP8 của chúng tôi là linh hoạt, vì nó có thể áp dụng không chỉ cho giai đoạn tiền huấn luyện mà còn cho các tác vụ tinh chỉnh xuôi dòng.

Ngoài ra, chúng tôi tiếp tục áp dụng sơ đồ độ chính xác hỗn hợp FP8 được đề xuất cho học tăng cường từ phản hồi của con người (RLHF), một quá trình phức tạp hơn để căn chỉnh LLM với sở thích của người dùng. Tuân theo cùng thiết lập huấn luyện như AlpacaFarm (Dubois et al., 2023), một khung RL gần đây cho căn chỉnh LLM, chúng tôi tối ưu hóa các mô hình chính sách với thuật toán PPO (Schulman et al., 2017). Sự khác biệt duy nhất nằm ở việc lựa chọn các sơ đồ huấn luyện độ chính xác hỗn hợp, tức là BF16 so với FP8. Từ kết quả được báo cáo trong Hình 6 và Bảng 4, chúng tôi quan sát thấy sự giảm đáng kể trong việc sử dụng bộ nhớ, ví dụ, giảm 32% bộ nhớ liên quan đến trọng số mô hình và giảm 62% liên quan đến trạng thái optimizer. Do đó, có thể suy ra rằng FP8 có khả năng sao chép độ chính xác hỗn hợp BF16 cho huấn luyện RLHF. Điều này nhấn mạnh khả năng áp dụng rộng hơn và tính linh hoạt của giải pháp huấn luyện bit thấp FP8 của chúng tôi.

3.2.2 Hiệu suất Hệ thống

Trong phần này, chúng tôi đánh giá hiệu suất cấp hệ thống của độ chính xác hỗn hợp FP8, xem xét hiệu quả giao tiếp, sử dụng bộ nhớ và tốc độ tổng thể, với trọng tâm vào tiết kiệm chi phí. Phương pháp của chúng tôi sử dụng gradient 8-bit cho giao tiếp tập thể all-reduce giữa các GPU. Về mặt lý thuyết, điều này dẫn đến giảm 75% chi phí giao tiếp so với sơ đồ 32-bit chủ lưu (Mặc dù tính toán độ chính xác hỗn hợp BF16 gradient sử dụng độ chính xác 16-bit, nó vẫn sử dụng độ chính xác 32-bit cho giao tiếp all-reduce (Shoeybi et al., 2019)). Do ảnh hưởng của mất mát truyền hệ thống, việc giảm thực tế quan sát được trong quá trình huấn luyện mô hình GPT nằm trong khoảng 63% đến 65%, như được chỉ ra trong Bảng 5. Hơn nữa, đáng chú ý là Nvidia Transformer Engine (TE) gần đây (Nvidia, 2022b) vẫn dựa vào FP32 độ chính xác đầy đủ cho giao tiếp tập thể, dẫn đến cùng mức độ giảm cho giải pháp FP8 của chúng tôi.

Khi huấn luyện các mô hình GPT với kích thước batch giống hệt nhau, độ chính xác hỗn hợp FP8 có thể dẫn đến giảm dung lượng bộ nhớ từ 28% đến 39% so với BF16, như được báo cáo trong Bảng 5. Những giảm này trong tiêu thụ bộ nhớ được quy cho các kỹ thuật gradient FP8 và optimizer FP8 mà chúng tôi đã giới thiệu. Hơn nữa, so với TE (Nvidia, 2022b), giải pháp của chúng tôi cũng rất cạnh tranh, đạt được giảm bộ nhớ bổ sung 36,1%, 36,0% và 42,1% cho các kích thước mô hình khác nhau, tức là GPT-7B, 13B và 175B. Mặc dù TE sử dụng FP8 để tính toán, nó vẫn sử dụng optimizer và gradient độ chính xác cao, tiêu thụ nhiều bộ nhớ hơn giải pháp của chúng tôi. Ngoài ra, bộ nhớ được tiết kiệm trong phương pháp của chúng tôi có thể được sử dụng để huấn luyện kích thước batch lớn hơn hoặc chuỗi dài hơn. Ví dụ, khi sử dụng 32 GPU H100 với dung lượng bộ nhớ 80GB, phương pháp của chúng tôi cho phép huấn luyện các mô hình với ngữ cảnh 4.096 token, chứa lên đến 175 tỷ tham số. Ngược lại, TE chỉ có thể chứa các mô hình với ngữ cảnh 2.048 token. Điều này thể hiện tiềm năng tích hợp huấn luyện độ chính xác hỗn hợp FP8 của chúng tôi vào các LLM hiện có, trao quyền cho chúng huấn luyện các chuỗi dài hơn với cùng tài nguyên GPU.

Hơn nữa, sơ đồ độ chính xác hỗn hợp FP8 của chúng tôi cho thấy thông lượng huấn luyện vượt trội so với sơ đồ BF16 phổ biến, đạt được tăng tốc đáng chú ý 75% khi áp dụng cho mô hình GPT-175B. Việc sử dụng FLOPS mô hình (MFU) của huấn luyện độ chính xác hỗn hợp FP8 là 34,2% trên GPU H100, cao hơn 37,3% so với TE. Những phát hiện này cung cấp bằng chứng đáng kể rằng sơ đồ FP8 của chúng tôi hiệu quả tiết kiệm bộ nhớ, giảm chi phí giao tiếp trong quá trình huấn luyện các mô hình lớn và cuối cùng nâng cao hiệu quả sử dụng hệ thống trên nền tảng GPU H100 mới nhất.

3.3 Nghiên cứu Loại bỏ

Chúng tôi loại bỏ các lựa chọn thiết kế khác nhau của chiến lược huấn luyện độ chính xác hỗn hợp FP8 cho LLM và báo cáo hiệu suất trong Bảng 6 – 8 và Hình 7 – 8. Các thí nghiệm nghiên cứu loại bỏ được tiến hành trên các mô hình GPT, có kiến trúc và thiết lập huấn luyện được trình bày chi tiết trong Bảng 1. Quan trọng, nghiên cứu loại bỏ của chúng tôi mang lại một số hướng dẫn cho việc sử dụng hiệu quả kiểu dữ liệu 8-bit trong huấn luyện LLM, có thể tạo thuận lợi cho nghiên cứu tương lai về huấn luyện mô hình bit thấp.

Giao tiếp. Chúng tôi trước tiên phân tích các hạn chế của các phương pháp chia tỷ lệ trước và chia tỷ lệ sau thông thường khi tổng hợp gradient bit thấp trong quá trình all-reduce. Như được hiển thị trong Hình 7, chúng tôi tiến hành phân tích thống kê về SNR, tỷ lệ tràn dưới và tỷ lệ tràn trên của gradient trọng số trên các khối Transformer khác nhau. Quan sát thấy rằng phương pháp chia tỷ lệ trước có tỷ lệ tràn dưới tương đối lớn hơn khi lượng tử hóa gradient từ 32-bit đến 8-bit, trong khi phương pháp chia tỷ lệ sau có tỷ lệ tràn trên cao hơn. Ngược lại, kỹ thuật chia tỷ lệ tự động được đề xuất có thể giảm cả tỷ lệ tràn dưới và tỷ lệ tràn trên, trong khi đạt được SNR tốt hơn nhiều, như được hiển thị trong Hình 7 (a). Điều này chứng minh hiệu quả của phương pháp chia tỷ lệ tự động trong việc giảm lỗi lượng tử hóa khi sử dụng kiểu dữ liệu 8-bit cho all-reduce gradient.

Optimizer. Chúng tôi tiếp tục loại bỏ tác động của độ chính xác giảm đối với các biến trong optimizer AdamW. Chúng tôi đặt optimizer độ chính xác hỗn hợp BF16 làm đường cơ sở, vì nó đã được sử dụng rộng rãi trong các khung huấn luyện LLM hiện có (Micikevicius et al., 2017; Shoeybi et al., 2019; Nvidia, 2022b). Bảng 6 trình bày thiết lập độ chính xác giảm cho các biến, trong khi Hình 8 vẽ các mất mát huấn luyện tương ứng. Chúng tôi quan sát thấy: 1) Trọng số chủ FP8 gây ra suy giảm hiệu suất (xem các dòng #2a so với #3 trong Hình 8), trong khi FP16 có thể duy trì độ chính xác như FP32 (xem #2a so với #0 và #1) nhưng yêu cầu sử dụng chia tỷ lệ tensor. Nó tiết lộ rằng trọng số chủ nhạy cảm với độ chính xác. Điều này có thể được quy cho vai trò của trọng số chủ trong việc cập nhật trọng số, có xu hướng thể hiện độ lớn nhỏ, cần độ chính xác cao để duy trì độ chính xác. 2) Mất mát huấn luyện của trọng số chủ BF16 hơi cao hơn so với FP16 với hệ số chia tỷ lệ vì BF16 có ít bit mantissa hơn, dẫn đến độ chính xác thấp hơn (xem #2a so với #2b). 3) Moment gradient bậc hai nhạy cảm với độ chính xác hơn moment bậc một, vì tính toán bình phương dễ gây ra tràn dưới và dẫn đến suy giảm độ chính xác. Sử dụng FP8 cho moment gradient bậc hai có thể dẫn đến mất mát huấn luyện phân kỳ (xem điểm #4 trong Hình 8).

Song song. Trong khung huấn luyện LLM FP8 của chúng tôi, chúng tôi giới thiệu các bộ chuyển đổi bit thấp FP8 vào song song sequence và song song tensor để giảm chi phí giao tiếp kích hoạt trên các GPU. Ở đây chúng tôi tiến hành thí nghiệm phân tích để đếm khối lượng giao tiếp liên quan đến kích hoạt trong quá trình huấn luyện mô hình GPT, và báo cáo các số trong Bảng 7. Quan sát thấy rằng sơ đồ song song FP8 của chúng tôi dẫn đến giảm đáng kể 34% chi phí giao tiếp liên quan đến kích hoạt so với phương pháp ban đầu sử dụng BF16. Hơn nữa, trong huấn luyện phân tán ZeRO, phương pháp của chúng tôi phân phối mỗi tensor FP8 cùng với hệ số chia tỷ lệ liên kết của nó như một tổng thể, thay vì phân vùng tensor thành các phần trên các GPU. Chiến lược này không chỉ dẫn đến tiết kiệm bộ nhớ GPU nhiều hơn mà còn duy trì tải bộ nhớ cân bằng trên các GPU, như được chứng minh trong Bảng 8.

4 Công trình Liên quan

Huấn luyện Độ chính xác Hỗn hợp. Huấn luyện hiệu quả thông qua độ chính xác hỗn hợp giảm đã được sử dụng rộng rãi trong học sâu hiện đại để tiết kiệm chi phí tính toán. Trong khi một số công trình đã đưa việc giảm bit đến mức cực đại, tức là mạng nhị phân 1-bit (Hubara et al., 2016; Rastegari et al., 2016), chúng đã không thành công trong việc duy trì độ chính xác mô hình (Micikevicius et al., 2022). Sơ đồ thực tế nhất hiện nay là phương pháp nửa độ chính xác FP16 (Micikevicius et al., 2017), có thể duy trì độ chính xác trong khi cải thiện hiệu quả huấn luyện. Các tính toán trong quá trình truyền thuận và lan truyền ngược sử dụng FP16 trong khi trọng số chủ sử dụng FP32. Vì FP16 có phạm vi động hẹp hơn, độ chính xác hỗn hợp FP16 đòi hỏi chia tỷ lệ mất mát (Micikevicius et al., 2017) để ngăn chặn mất độ chính xác. May mắn thay, nhu cầu chia tỷ lệ mất mát có thể được tránh bằng cách sử dụng kiểu dữ liệu BF16, vì BF16 duy trì cùng phạm vi động như FP32 độ chính xác đầy đủ. Điều này dẫn đến việc huấn luyện mô hình lớn hiện nay ưa thích sử dụng sơ đồ độ chính xác hỗn hợp BF16, ổn định hơn trong quá trình huấn luyện (Smith et al., 2022; Scao et al., 2022; Zeng et al., 2022).

FP8 là một tiến bộ tự nhiên từ các định dạng dữ liệu 16-bit để giảm thêm chi phí tính toán. Những nỗ lực tiên phong ban đầu trong huấn luyện mô hình bit thấp FP8 (Wang et al., 2018; Sun et al., 2019; Dettmers et al., 2021) phần lớn vẫn ở giai đoạn mô phỏng. Do đó, tồn tại khoảng cách đáng chú ý giữa khả năng dự kiến của các phương pháp này và hiệu suất thực tế của chúng trên phần cứng (Micikevicius et al., 2022). Với sự ra đời của kiến trúc GPU Nvidia Hopper (Nvidia, 2022a), FP8 đang nổi lên như một kiểu dữ liệu khả thi và thực tế cho huấn luyện độ chính xác thấp thế hệ tiếp theo, như được thảo luận trong (Micikevicius et al., 2022). Hiện tại, Nvidia Transformer Engine (TE) (Nvidia, 2022b) đóng vai trò là khung chính cho huấn luyện độ chính xác hỗn hợp FP8. Tuy nhiên, sự hỗ trợ của nó cho việc sử dụng FP8 vẫn hơi hạn chế. Triển khai hiện tại của TE hạn chế việc sử dụng FP8 chỉ cho tính toán trọng số, giữ lại việc lưu trữ trọng số mô hình và tính toán gradient với kiểu dữ liệu 16-bit. Do đó, tăng tốc từ đầu đến cuối, tiết kiệm chi phí bộ nhớ và giao tiếp bị hạn chế. Ngược lại, công trình của chúng tôi thâm nhập gradient FP8, optimizer và huấn luyện phân tán vào toàn bộ quá trình huấn luyện mô hình, hoàn toàn khai thác khả năng của FP8.

Mô hình Ngôn ngữ Lớn. Những năm gần đây đã chứng kiến sự tiến hóa đáng kể trong lĩnh vực LLM. Mô hình ngôn ngữ tự hồi quy – dự đoán tương lai của một chuỗi văn bản từ quá khứ của nó – cung cấp một mục tiêu đơn giản nhưng mạnh mẽ cho phép công thức hóa nhiều tác vụ. Trong khi tồn tại các phương pháp thay thế, chẳng hạn như mô hình ngôn ngữ có mặt nạ (Devlin et al., 2019) và mô hình ngôn ngữ hoán vị (Yang et al., 2019), phương pháp tự hồi quy hiện nay hứa hẹn hơn vì hiệu suất mạnh mẽ của nó. Tuân theo các quy luật mở rộng (Brown et al., 2020) và các quy luật tinh chỉnh (Hoffmann et al., 2022), nhiều LLM khác nhau đã được đề xuất, bao gồm các mô hình dày đặc: GPT-3 (Brown et al., 2020), Jurassic-1 (Lieber et al., 2021), Gopher (Rae et al., 2021), Chinchilla (Hoffmann et al., 2022), Bloom (Scao et al., 2022), OPT (Zhang et al., 2022) Megatron-Turing NLG (Smith et al., 2022), PaLM (Chowdhery et al., 2022), LaMDA (Thoppilan et al., 2022), LLaMA (Touvron et al., 2023), và các mô hình thưa: GLaM (Du et al., 2022), và Switch transformers (Fedus et al., 2022). Mỗi mô hình trong số chúng đã thể hiện hiệu suất few-shot cạnh tranh đáng kể trên một loạt các tác vụ tại thời điểm phát hành tương ứng. Tuy nhiên, những mô hình này vẫn gặp phải các thách thức, chẳng hạn như yêu cầu tính toán quá lớn và nhu cầu thu thập thêm dữ liệu huấn luyện chất lượng cao. Trong công trình này, chúng tôi đi sâu vào việc sử dụng các kỹ thuật độ chính xác thấp để giảm thiểu chi phí huấn luyện, đây là bước quan trọng cho việc tiếp tục mở rộng các mô hình ngôn ngữ.

Huấn luyện độ chính xác thấp đã được sử dụng rộng rãi trong huấn luyện LLM để giảm chi phí tính toán. OPT (Zhang et al., 2022) và GLM (Zeng et al., 2022) sử dụng FP16 cho truyền thuận và nghịch và FP32 cho trạng thái optimizer và trọng số chủ, để giảm việc sử dụng bộ nhớ GPU và cải thiện hiệu quả huấn luyện. Bloom (Scao et al., 2022) phát hiện rằng FP16 có thể gây ra bất ổn định số và phân kỳ không thể đảo ngược, đặc biệt khi huấn luyện các mô hình lớn hơn 100B tham số, vì phạm vi động của FP16 bị hạn chế. Do đó, Bloom và các LLM khác, chẳng hạn như Gopher (Rae et al., 2021) và Chinchilla (Hoffmann et al., 2022), áp dụng độ chính xác hỗn hợp BF16, vì BF16 có phạm vi động rộng giống như FP32. Huấn luyện và điều chỉnh LLM với độ chính xác thấp 8-bit không được khám phá kỹ trong các công trình trước đây, vì sự hỗ trợ phần cứng cho FP8 không có sẵn trước khi phát hành cơ sở hạ tầng Nvidia Hopper. Công trình này trình bày khám phá đầu tiên về tiền huấn luyện và tinh chỉnh FP8 cho LLM, đồng thời đề xuất một sơ đồ độ chính xác hỗn hợp FP8 được tối ưu hóa cực kỳ. Chúng tôi hy vọng công trình này có thể tạo thuận lợi cho nghiên cứu tương lai trong FP8 và, có khả năng, mở rộng để khám phá huấn luyện độ chính xác thậm chí thấp hơn, chẳng hạn như 4-bit và 1-bit.

5 Kết luận

Trong công trình này, chúng tôi khám phá huấn luyện 8-bit cho LLM. Chúng tôi giới thiệu một khung huấn luyện độ chính xác hỗn hợp FP8 mới, kết hợp giao tiếp tập thể 8-bit, optimizer và huấn luyện song song phân tán theo cách tăng dần. Theo hiểu biết tốt nhất của chúng tôi, đây là công trình đầu tiên thâm nhập tính toán FP8, lưu trữ và giao tiếp vào toàn bộ quá trình huấn luyện mô hình ngôn ngữ lớn. Các thí nghiệm rộng rãi chứng minh phương pháp được đề xuất hiệu quả giảm chi phí giao tiếp và cắt giảm việc sử dụng bộ nhớ trong bối cảnh huấn luyện mô hình GPT ở các quy mô khác nhau. Trong công việc tương lai, chúng tôi dự định mở rộng quy mô kích thước và các bước huấn luyện của các mô hình GPT FP8 và tiếp tục huấn luyện chúng với sơ đồ độ chính xác hỗn hợp 8-bit của chúng tôi. Hơn nữa, chúng tôi cũng sẽ sử dụng sơ đồ FP8 được đề xuất để huấn luyện các mô hình lớn đa phương thức, và khám phá triển khai bit thấp của LLM trên các thiết bị biên khác nhau, chẳng hạn như điện thoại thông minh.

Đóng góp và Ghi nhận

Dự án này ban đầu được đề xuất bởi Han Hu và Peng Cheng, những người dẫn dắt định hướng. Shuguang Liu đóng vai trò là trưởng sản phẩm trong suốt dự án.

Các đóng góp cho tất cả các đồng tác giả được trình bày chi tiết như sau:

Khung FP8: Kan Wu, Houwen Peng, Ze Liu, Peng Cheng, Han Hu
Hệ thống: Yifan Xiong, Ziyue Yang, Yuxiang Yang, Guoshuai Zhao, Peng Cheng
Cơ sở hạ tầng Phần cứng: Guoshuai Zhao, Yuxiang Yang, Yifan Xiong, Peng Cheng, Shuguang Liu, Joe Chau
Dữ liệu: Ruihang Li, Miaosen Zhang, Jia Ning, Chen Li, Ruizhe Wang, Houwen Peng, Han Hu
Tiền huấn luyện: Yixuan Wei, Kan Wu, Ze Liu, Miaosen Zhang, Zheng Zhang, Houwen Peng, Han Hu
Căn chỉnh (SFT, RS và RLHF): Bolin Ni, Jingcheng Hu, Yixuan Wei, Houwen Peng, Han Hu
Đánh giá: Yixuan Wei, Bolin Ni, Jingcheng Hu
Kỹ thuật Sản phẩm: Yuxiang Yang, Kan Wu, Yifan Xiong, Ziyue Yang, Guoshuai Zhao, Peng Cheng

Chúng tôi cảm ơn Eric Chung, Bita Darvish Rouhani, Yu Pei, Hyunseung Harry Yoo, Zhenghong Zhou, Gongrui Zhang và Zhirong Wu vì những thảo luận hữu ích.

Chúng tôi cảm ơn Baining Guo và Lidong Zhou vì sự hướng dẫn và hỗ trợ cho dự án này.
