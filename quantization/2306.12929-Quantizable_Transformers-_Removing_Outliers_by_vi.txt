# 2306.12929.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/quantization/2306.12929.pdf
# Kích thước tệp: 13347145 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Transformers Có Thể Lượng Tử Hóa: Loại Bỏ Outliers Bằng Cách
Giúp Attention Heads Không Làm Gì
Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort
Qualcomm AI Research∗
Amsterdam, Hà Lan
{ybond, markusn, tijmen}@qti.qualcomm.com
Tóm tắt
Các mô hình Transformer đã được áp dụng rộng rãi trong nhiều lĩnh vực khác nhau trong những năm qua, và đặc biệt các mô hình ngôn ngữ lớn đã thúc đẩy lĩnh vực AI một cách đáng kể. Do kích thước của chúng, khả năng của những mạng này đã tăng lên rất nhiều, nhưng điều này đi kèm với chi phí tăng đáng kể về tính toán cần thiết. Lượng tử hóa là một trong những cách hiệu quả nhất để giảm thời gian tính toán và tiêu thụ bộ nhớ của mạng nơ-ron. Tuy nhiên, nhiều nghiên cứu đã chỉ ra rằng các mô hình transformer hiện đại có xu hướng học các outliers mạnh trong các activations của chúng, khiến chúng khó lượng tử hóa. Để giữ được hiệu suất chấp nhận được, sự tồn tại của những outliers này đòi hỏi các activations phải ở bitwidth cao hơn hoặc sử dụng các định dạng số khác nhau, fine-tuning thêm, hoặc các giải pháp thay thế khác. Chúng tôi chỉ ra rằng các outliers mạnh có liên quan đến hành vi rất cụ thể của attention heads cố gắng học một "no-op" hoặc chỉ một cập nhật một phần của residual. Để đạt được các số không chính xác cần thiết trong ma trận attention cho một no-update, đầu vào của softmax bị đẩy để trở nên lớn hơn và lớn hơn trong quá trình huấn luyện, gây ra outliers trong các phần khác của mạng. Dựa trên những quan sát này, chúng tôi đề xuất hai thay đổi đơn giản (độc lập) cho cơ chế attention - clipped softmax và gated attention. Chúng tôi chứng minh thực nghiệm rằng các mô hình được pre-train bằng phương pháp của chúng tôi học được các outliers nhỏ hơn đáng kể trong khi duy trì và đôi khi thậm chí cải thiện hiệu suất tác vụ floating-point. Điều này cho phép chúng tôi lượng tử hóa transformers thành lượng tử hóa INT8 đầy đủ của các activations mà không cần nỗ lực bổ sung nào. Chúng tôi chứng minh hiệu quả của phương pháp trên cả mô hình ngôn ngữ (BERT, OPT) và vision transformers. Mã nguồn của chúng tôi có sẵn tại https://github.com/qualcomm-ai-research/outlier-free-transformers.

1 Giới thiệu
Lượng tử hóa đã là một trong những cách có tác động nhất để giảm độ phức tạp tính toán của mạng transformer. Các nghiên cứu trước đây đã chỉ ra rằng lượng tử hóa mạng thành weights 4-bit là có thể mà không mất quá nhiều độ chính xác [66,69]. Một số nghiên cứu thậm chí cho thấy weights 4-bit có thể là tối ưu khi đánh đổi giữa kích thước mô hình và bit-width [12].

Tuy nhiên, lượng tử hóa transformers không phải lúc nào cũng đơn giản. Khi lượng tử hóa các activations của một transformer, các vấn đề đáng kể xuất hiện với outliers trong các lớp cụ thể. Điều này đã được nhiều nhà nghiên cứu ghi nhận và đề xuất các biện pháp khắc phục cho transformers sau khi huấn luyện để giảm thiểu tác động của chúng [13,67]. Những phương pháp này thường tẻ nhạt và hoặc đòi hỏi huấn luyện lại mạng, đòi hỏi triển khai phần cứng cụ thể cho lượng tử hóa input-channel [13] hoặc đòi hỏi các phần của activations vẫn ở bitwidths cao hơn, giảm hiệu quả của lượng tử hóa activation [67].

Trong bài báo này, chúng tôi đặt mục tiêu giải quyết hoàn toàn vấn đề outlier của transformer bằng cách thay đổi kiến trúc của mạng. Chúng tôi hy vọng làm cho transformers dễ lượng tử hóa ngay từ đầu mà không cần bất kỳ xử lý hậu kỳ nào. Để làm như vậy, chúng tôi phân tích kỹ lưỡng tại sao những outliers này xuất hiện. Các nghiên cứu trước đây đã phát hiện sự tồn tại của những outliers này [4,13], nhưng trong nghiên cứu của chúng tôi, chúng tôi đến được hiểu biết đầy đủ hơn về những giá trị outlying này. Chúng tôi phát hiện rằng các outliers xảy ra vì attention heads đang cố gắng không cập nhật hidden state, và trong quá trình này, các outliers mạnh xuất hiện do hàm softmax. Điều này xảy ra cho cả language và vision transformers và các kiến trúc transformer cụ thể khác nhau. Hiểu biết này là nền tảng cho hai thay đổi mới mà chúng tôi đề xuất cho kiến trúc transformer có thể loại bỏ hoàn toàn vấn đề outliers.

2 Bối cảnh và nghiên cứu liên quan
Trong phần này, chúng tôi tóm tắt ngắn gọn những kiến thức cơ bản về lượng tử hóa mạng nơ-ron và thảo luận về lý do tại sao các transformers hiện đại khó lượng tử hóa.

Lượng tử hóa Một trong những cách mạnh mẽ nhất để giảm thời gian tính toán và tiêu thụ bộ nhớ của mạng nơ-ron là lượng tử hóa, sử dụng các biểu diễn bit thấp cho các tensors weights và activation. Hơn nữa, sử dụng các biểu diễn fixed-point bit thấp, chẳng hạn như INT8, có thể giảm thêm tiêu thụ năng lượng vì các phép toán fixed-point hiệu quả hơn so với các đối tác floating-point của chúng [23, 59].

Chúng tôi mô phỏng quá trình lượng tử hóa trong floating-point theo Jacob et al. [26]. Chúng tôi sử dụng định nghĩa sau của hàm lượng tử hóa:

bx:=q(x;s, z, b ) =s·
clipjx
sm
+z; 0,2b−1
−z
, (1)

trong đó x biểu thị đầu vào của quantizer (tức là, weights hoặc activations của mạng), s∈R+ là hệ số tỷ lệ hoặc step-size, z∈Z là zero point, và b∈N là bitwidth. ⌊·⌉ biểu thị toán tử round-to-nearest-integer. Sơ đồ lượng tử hóa này được gọi là uniform affine hoặc asymmetric quantization [24,32,76] và đây là một trong những sơ đồ lượng tử hóa được sử dụng phổ biến nhất vì nó cho phép triển khai hiệu quả của số học fixed-point. Trong trường hợp symmetric quantization, chúng tôi hạn chế lưới lượng tử hóa để đối xứng quanh z= 0.

Trong nghiên cứu này, chúng tôi tập trung vào các phương pháp post-training quantization (PTQ), lấy một mạng FP32 đã được pre-train và chuyển đổi trực tiếp thành mạng fixed-point mà không cần pipeline huấn luyện ban đầu [2,5,7,25,32,35,41,43,44,75]. Những phương pháp này không đòi hỏi dữ liệu hoặc chỉ cần một dataset hiệu chỉnh nhỏ và dễ sử dụng hơn so với các phương pháp quantization-aware training (QAT, Bhalgat et al. 3, Esser et al. 16, Gupta et al. 21, Jacob et al. 26, Krishnamoorthi 32) yêu cầu bạn huấn luyện toàn bộ mạng trong nhiều epochs hơn. Để biết thêm chi tiết về lượng tử hóa mạng nơ-ron, chúng tôi giới thiệu độc giả tham khảo [19, 46].

Outliers trong Transformers Nhiều nghiên cứu đã chỉ ra rằng các mô hình ngôn ngữ dựa trên transformer hiện đại có xu hướng học outliers trong weights và activations [4,13,31]. Những outliers này chỉ có mặt trong một tập nhỏ cố định các chiều embedding, nhưng chúng xuất hiện thường xuyên và nhất quán qua nhiều lớp và chuỗi dữ liệu. Cũng được chỉ ra rằng những outliers đó đóng vai trò quan trọng trong dự đoán của mô hình và việc cắt chúng hoặc đặt thành zero các tham số tương ứng làm suy giảm đáng kể hiệu suất tác vụ của mô hình [31,49]. Các outliers mạnh nhất về độ lớn thường xuất hiện ở đầu ra của feed-forward network, FFN, mặc dù Dettmers et al. [13] đã chỉ ra rằng đối với các mô hình ngôn ngữ dựa trên transformer đủ lớn, chúng bắt đầu xuất hiện sau mỗi lớp tuyến tính, bao gồm các lớp projection query, key, và value. Hiện tượng này đúng cho nhiều tác vụ, mục tiêu huấn luyện và mô hình (cả encoder và decoder transformers), bao gồm BERT [14], RoBERTa [37], DistilBERT [53], MobileBERT [55], ELECTRA [9], BART [33], XLNet [68], GPT-2 [50], và OPT [74].

Do những outliers mạnh này, việc áp dụng PTQ per-tensor cho đầu ra của FFN và tổng residual có thể gây ra lỗi đáng chú ý vì sự đánh đổi sau đây giữa phạm vi và độ chính xác. Một mặt, sử dụng phạm vi lượng tử hóa lớn cho các giá trị phạm vi nhỏ dẫn đến mất biểu diễn (lỗi làm tròn cao). Mặt khác, phạm vi lượng tử hóa nhỏ cho các giá trị lớn dẫn đến lỗi cắt rất cao. Đối với trường hợp outliers transformer đáng kể, thường không thể tìm được sự đánh đổi tốt giữa lỗi làm tròn và cắt, dẫn đến lỗi tổng thể cao.

Đã có nhiều nỗ lực để khắc phục vấn đề lượng tử hóa transformer [4,12,13,17,27,28, 51,54,62,63,69,71]. Hầu hết các phương pháp này đều sử dụng độ chi tiết lượng tử hóa tinh hơn (row-wise, channel-wise, group-wise weight và activation quantization), sử dụng bitwidth cao hơn và/hoặc định dạng số khác để biểu diễn những outliers đó tốt hơn hoặc đòi hỏi fine-tuning thêm (dưới dạng QAT và/hoặc knowledge distillation). Nói cách khác, chúng điều chỉnh lượng tử hóa để làm việc với outliers, điều này thường phải trả giá bằng khả năng áp dụng chung hoặc overhead inference thêm.

Ngược lại, trong nghiên cứu này, chúng tôi muốn giải quyết nguyên nhân gốc rễ của vấn đề và hiểu tại sao outliers được học ngay từ đầu và đề xuất một giao thức pre-training mới giảm đáng kể độ lớn của outliers tạo ra các mô hình thân thiện với lượng tử hóa hơn nhiều có thể được lượng tử hóa dễ dàng bằng PTQ mà không suy giảm mạnh hiệu suất.

3 Phân tích outlier
Outliers trong mô hình BERT Trong Phần 2 chúng tôi đã thảo luận rằng outliers chỉ có mặt trong một vài chiều embedding được chỉ định nhưng chúng xuất hiện thường xuyên và nhất quán qua nhiều lớp và chuỗi dữ liệu. Chúng tôi cũng thảo luận rằng các outliers độ lớn mạnh nhất trong BERT thường xuất hiện ở đầu ra của FFN trong các lớp encoder cuối.

Chúng tôi bắt đầu bằng cách lấy checkpoint BERT-base-uncased đã được pre-train từ HuggingFace [65] và fine-tune nó trên dataset MNLI từ benchmark GLUE nổi tiếng [61] (xem chi tiết thực nghiệm trong C.1). Để xác định các chiều outlier, chúng tôi truyền tập validation MNLI-m qua mạng và ghi lại tất cả outliers¹ tại đầu ra FFN trong các lớp #10 và #11². Như chúng ta có thể thấy trong Hình 1, thực sự chỉ có một vài chiều ẩn nơi outliers từng xảy ra. Chúng tôi cũng nhận thấy rằng đa số outliers (>97%) tương quan với vị trí của delimiter tokens – [SEP], ".", và ",".

Để hiểu rõ hơn vai trò của những outliers đó, chúng tôi phân tích các mẫu attention của các attention heads tương ứng. BERT-base sử dụng multi-head attention với nheads= 12 và mỗi head hoạt động trên một tập con liên tiếp của dhead= 64 features. Do đó, chiều ẩn #180, tình cờ có số lượng outlier cao nhất trong cả lớp #10 và #11, tương ứng với attention head #3. Trong Hình 2 (và thêm ví dụ trong Phụ lục A.1) chúng tôi hiển thị các ví dụ về ma trận attention, values và tích của chúng cho head đó.

Một mẫu phổ biến mà chúng tôi tìm thấy là attention head gán hầu như tất cả khối lượng xác suất của nó cho tokens [SEP], và các tokens ít thông tin khác như dấu chấm/phẩy, trong khi những tokens này cũng có values nhỏ trong V liên kết với những tokens đó. Điều này dẫn đến một tích độ lớn nhỏ giữa hai cái (xem Hình 2a). Điều này hiệu quả tương ứng với một (soft) no-update của biểu diễn ẩn, nơi chỉ tiếng ồn nhỏ được thêm vào sau residual. Trong các trường hợp khác (Hình 2b và 2c), chúng tôi quan sát thấy rằng một phần đáng kể xác suất attention vẫn được dành cho delimiter tokens. Tuy nhiên, bằng cách phân bổ một số khối lượng xác suất trên các tokens khác (cùng với các values nhỏ cho delimiter tokens), điều này dẫn đến một (soft) selective update của biểu diễn ẩn.

Những mẫu này trong self-attention dường như là một "giải pháp thay thế" đã học được cho các hạn chế của việc có softmax và các kết nối residual trong các trường hợp mà attention head không muốn cập nhật biểu diễn của một số hoặc tất cả các tokens. Những quan sát này phù hợp với Clark et al. [8], Kovaleva et al. [30] cũng lập luận rằng việc attention độc quyền hoặc gần như độc quyền vào delimiter tokens như [SEP], dấu chấm/phẩy hoạt động như một "no-op" khi chức năng của attention head không áp dụng được.

¹Chúng tôi tuân theo Bondarenko et al. [4] và xem outliers là các giá trị vượt quá 6 độ lệch chuẩn từ trung bình của tensor activation tương ứng.
²Chúng tôi sử dụng chỉ số 1-based cho các lớp encoder và attention heads trong suốt bài báo.

--- TRANG 4 ---
(a) Attention layer #11, data sequence #1
(b) Attention layer #11, data sequence #5
(c) Attention layer #10, data sequence #5
Hình 2: Trực quan hóa các mẫu trong self-attention, cụ thể là xác suất attention, values, và tích của chúng (cột trái, giữa và phải, tương ứng), trong attention head #3 cho BERT-base, được tính trên một số chuỗi dữ liệu từ tập validation MNLI-m.

(a)
 (b)
 (c)
 (d)
 (e)
Hình 3: Tóm tắt phân tích outlier của chúng tôi cho ViT được thể hiện trên một hình ảnh ngẫu nhiên từ tập validation ImageNet. (a) Một hình ảnh đầu vào. (b) Outliers trong đầu ra của lớp #11. (c) Trọng số attention tích lũy được dành cho mỗi patch (ma trận xác suất attention được tổng hợp theo hàng) trong attention head #1, lớp #12. (d) Ma trận xác suất attention tương ứng. (e) Độ lớn trung bình của values cho outlier và non-outlier patches.

Outliers trong ViT Chúng tôi tiến hành phân tích tương tự cho Vision transformer [15] được huấn luyện trên ImageNet [52]. Cho nghiên cứu này, chúng tôi sử dụng checkpoint đã được pre-train theo thiết lập thực nghiệm của chúng tôi từ Phần 5. Chúng tôi nêu bật các phát hiện trong Hình 3 và cung cấp thêm ví dụ trong Phụ lục A.2. Phân tích của chúng tôi cho thấy nhiều điểm tương đồng với trường hợp BERT. Thay vì delimiter tokens, đa số outliers dường như tương quan với một số patches ngẫu nhiên không mang thông tin (ví dụ, trong nền). Chúng tôi cũng thấy rằng attention head tương ứng trong lớp tiếp theo phân bổ đa số xác suất attention cho cùng những patches đó. Cuối cùng, những outlier patches đó trung bình có độ lớn values nhỏ hơn rõ rệt so với những patches không phải outlier, dẫn đến hành vi no-update tương tự. Thực tế là những values đó không gần bằng zero như trong trường hợp BERT có thể liên quan đến khả năng mô hình nhỏ hơn³, hoặc quy trình huấn luyện tương đối ngắn hơn.

Giả thuyết Dựa trên những quan sát này, chúng tôi đưa ra giả thuyết sau về cách hành vi này của attention heads liên quan đến outliers:

1. Để một attention block không cập nhật biểu diễn của một token trên residual, một số attention heads muốn phân bổ hầu hết khối lượng xác suất attention của chúng cho một tập hợp cố định và phổ biến nào đó của các tokens có nội dung thông tin thấp (ví dụ, delimiter tokens hoặc background patches) có thể được học để có đầu ra hàm value nhỏ.

³Chúng tôi sử dụng cấu hình ViT/S-16 chỉ có 22M tham số.

--- TRANG 5 ---
2. Từ định nghĩa của hàm softmax⁴, dễ thấy rằng điều này sẽ đòi hỏi đầu vào của softmax phải có phạm vi động tương đối lớn (Hình 4, 1). Thực tế, trong trường hợp giới hạn nơi softmax chính xác bằng không, điều này sẽ đòi hỏi phạm vi động vô hạn:

softmax(x)ᵢ = 0 ⇔ ∃ j≠i, xⱼ−xᵢ = +∞ (2)

3. Vì Layer Normalization ([1], 2) chuẩn hóa các outliers, độ lớn của đầu ra FFN trong lớp trước đó (3) phải rất cao để vẫn tạo ra phạm vi động đủ lớn sau LayerNorm. Lưu ý rằng điều này cũng áp dụng cho các mô hình transformer với LayerNorm được áp dụng trước self-attention hoặc các biến đổi tuyến tính thay vì, một biến thể được áp dụng bởi GPT, OPT, và nhiều vision transformers [15, 38, 57, 58].

4. Cuối cùng, vì softmax sẽ không bao giờ đưa ra các số không chính xác, nó sẽ luôn back-propagate một tín hiệu gradient để phát triển outliers lớn hơn⁵. Do đó, các outliers sẽ có xu hướng trở nên mạnh hơn về độ lớn, càng lâu mạng được huấn luyện.

4 Phương pháp
Hình 4: Minh họa sơ đồ của attention layer trong BERT. Tensor activation ẩn được ký hiệu bởi x. ⊕ là phép cộng theo từng phần tử. Đầu ra có vấn đề của FFN tạo ra outliers có độ lớn lớn nhất được tô đỏ. Chú ý cách những outliers đó trong lớp trước ảnh hưởng đến hành vi trong cơ chế attention trong lớp tiếp theo.

Trong phần này, chúng tôi giới thiệu các thay đổi đề xuất của chúng tôi cho cơ chế softmax attention. Dựa trên những hiểu biết của chúng tôi từ Phần 3, ý tưởng cốt lõi của những thay đổi này là cấp cho mô hình khả năng tạo ra đầu ra hàm attention có độ lớn rất nhỏ (hoặc thậm chí chính xác bằng không), mà không tạo ra outliers.

Nhớ lại rằng self-attention [60] được định nghĩa như sau:
Attention(x) := softmax(Q(x)K(x)ᵀ/√dₕₑₐd)V(x) (3)

trong đó Q, K và V là các projection tuyến tính có thể học được của đầu vào x.

Hầu hết các mô hình transformer hiện đại sử dụng biến thể multi-headed của self-attention, nơi dₘₒdₑₗ features được phân chia thành nₕₑₐdₛ nhóm của dₕₑₐd features, và đầu ra cuối cùng là sự nối của các đầu ra của (3) được áp dụng cho mỗi nhóm.

4.1 Clipped softmax
Đầu tiên, chúng tôi đề xuất thay thế hàm softmax trong (3) bằng clipped softmax sau:

clipped_softmax(x;ζ,γ) := clip((ζ−γ)·softmax(x) + γ, 0, 1). (4)

Ở đây x là đầu vào và ζ≥1, γ≤0 là các hệ số kéo giãn là siêu tham số của phương pháp. Công thức tương tự đã được sử dụng trước đây cho hàm sigmoid [40,45]. Chúng ta có thể xem (4) như việc kéo giãn đầu ra của softmax từ (0,1) đến (γ,ζ) và sau đó cắt lại về (0,1) để chúng ta có thể biểu diễn số không chính xác nếu γ<0 và số một chính xác nếu ζ>1. Cụ thể, các giá trị của softmax lớn hơn (1−γ)/(ζ−γ) được làm tròn thành một trong khi các giá trị nhỏ hơn −γ/(ζ−γ) được làm tròn thành không.

Với sự thay thế drop-in này, chúng ta có thể đạt được số không chính xác (và số một) với phạm vi hữu hạn cho đầu vào softmax. Ngoài ra, bất cứ khi nào các giá trị bị cắt, chúng sẽ không đưa ra gradient, ngăn chặn các outliers phát triển thêm.

⁴softmax(x)ᵢ = exp(xᵢ)/Σⱼ₌₁ᵈ exp(xⱼ)
⁵Gọi y = softmax(x). Thì ∂yᵢ/∂xⱼ ≠ 0 ∀i,j.

--- TRANG 6 ---
4.2 Gated attention
Một cách thay thế để thiết kế mô hình có đầu ra attention nhỏ mà không có outliers là trang bị cho nó một cơ chế gating có điều kiện rõ ràng, như được hiển thị trong Hình 5. Ý tưởng là mô hình có thể sử dụng gating để giữ hoặc vô hiệu hóa việc cập nhật biểu diễn của các tokens nhất định và không dựa vào xác suất attention và values để đạt được kết quả tương tự.

Cụ thể, chúng tôi đề xuất thay đổi sau cho hàm attention:
Gated_attention(x) := sigmoid(G(x)) ⊙ softmax(Q(x)K(x)ᵀ/√dₕₑₐd)V(x). (5)

Ở đây G là hàm gating, ⊙ là phép nhân theo từng phần tử qua trục token và mọi thứ khác vẫn giống như trong (3). Hàm gating G được tham số hóa bởi một mạng nơ-ron nhỏ được học cùng với phần còn lại của mô hình. Chúng tôi thay thế công thức attention bằng biến thể đề xuất trong mỗi lớp trên mạng transformer.

Hình 5: Minh họa sơ đồ gated attention đề xuất của chúng tôi.

Thiết kế module gating Nhớ lại rằng đầu vào cho attention layer x có hình dạng (T, dₘₒdₑₗ) được reshape thành (nₕₑₐdₛ, T, dₕₑₐd) cho multi-headed self-attention, trong đó T là độ dài chuỗi. Chúng tôi chọn định nghĩa hàm gating trên cơ sở mỗi head. Cho mỗi head i ∈ {1, ..., nₕₑₐdₛ}, chúng tôi chỉ định Gᵢ: ℝᵈʰᵉᵃᵈ → ℝ và đầu ra của module gating là πᵢ ∈ ℝᵀ được tính như sau:

π̂ᵢ,ₜ = Gᵢ(xᵢ,ₜ,:) ∀t ∈ {1, ..., T} (6)
πᵢ,: = sigmoid(π̂ᵢ,:), (7)

lưu ý rằng các module gating được chia sẻ giữa các vị trí token khác nhau nhưng không được chia sẻ qua các attention heads.

Chúng tôi muốn module gating của mình nhẹ nhất có thể. Để bắt đầu, chúng tôi thử nghiệm với Gᵢ được tham số hóa bởi một lớp tuyến tính duy nhất. Điều này cung cấp cho chúng tôi một module gating tiết kiệm tính toán và có overhead bộ nhớ chỉ nₕₑₐdₛ·(dₕₑₐd + 1) ∼ dₘₒdₑₗ tham số thêm (tương đương với 1 token thêm) mỗi attention layer⁶. Chúng tôi cũng điều tra hiệu ứng của việc sử dụng một số hàm gating khác trong Phụ lục B.1.

5 Thí nghiệm
Trong phần này, chúng tôi đánh giá các thay đổi đề xuất cho self-attention trên một số mô hình ngôn ngữ (BERT, OPT) và vision transformers (ViT). Chúng tôi trước tiên kiểm tra các siêu tham số khác nhau cho các phương pháp và cung cấp hiểu biết về cách chúng hoạt động. Sau đó chúng tôi đặt mục tiêu kiểm tra phương pháp của mình về độ chính xác, và sự khác biệt trong cải thiện lượng tử hóa sau khi huấn luyện. Tất cả các siêu tham số chi tiết của thí nghiệm của chúng tôi có trong Phụ lục C.

BERT Chúng tôi thử nghiệm với BERT-base-uncased (109M tham số) pre-training sử dụng mục tiêu masked language modeling (MLM). Theo [14], chúng tôi sử dụng sự nối của các tập huấn luyện của BookCorpus [77] và English Wikipedia⁷. Chúng tôi triển khai các phương pháp của mình trong PyTorch [48] và sử dụng các pipeline huấn luyện và đánh giá từ thư viện HuggingFace [20,34,65]. Chúng tôi tuân theo chặt chẽ quy trình pre-training từ [14]. Để tăng tốc huấn luyện và thử nghiệm, chúng tôi huấn luyện với độ dài chuỗi tối đa là 128 trong toàn bộ thời gian huấn luyện. Chúng tôi đánh giá trên tập validation Wikipedia và báo cáo perplexity MLM.

OPT Chúng tôi thử nghiệm với biến thể 125M sized của OPT [74] pre-training sử dụng mục tiêu causal language modeling (CLM). Do hạn chế tính toán, chúng tôi huấn luyện mô hình trên cùng dataset được sử dụng cho BERT pre-training (BookCorpus + Wikipedia) với độ dài chuỗi tối đa là 512

⁶Ví dụ, trong trường hợp BERT-base, điều này tương đương với ít hơn 0.009% tổng kích thước mô hình.
⁷Cụ thể, chúng tôi sử dụng tập con tiếng Anh của Wiki-40b, https://huggingface.co/datasets/wiki40b, có chứa văn bản được làm sạch của Wikipedia tiếng Anh và các phần chia huấn luyện/validation.

--- TRANG 7 ---
[Bảng 1 hiển thị tác động của các siêu tham số clipped softmax trên BERT-base với các cột: γ, ζ, FP16 ppl. ↓, Max inf. norm, Avg. kurtosis, W8A8 ppl. ↓]

và batch size là 192. Tương tự như các thí nghiệm BERT của chúng tôi, chúng tôi sử dụng các pipeline huấn luyện và đánh giá từ thư viện HuggingFace. Chúng tôi đánh giá trên tập validation Wikipedia và báo cáo perplexity CLM.

ViT Cuối cùng, chúng tôi khám phá hiệu quả của các kỹ thuật đề xuất trên vision transformer [15] (cấu hình ViT-S/16, 22M tham số) được huấn luyện trên ImageNet-1K [11,52]. Cho những thí nghiệm này, chúng tôi áp dụng các pipeline huấn luyện và validation từ thư viện PyTorch Image models [64]. Chúng tôi báo cáo độ chính xác top-1 trên tập validation của ImageNet.

Thiết lập lượng tử hóa Trong tất cả các thí nghiệm, sau khi mô hình được huấn luyện, chúng tôi áp dụng PTQ 8-bit. Chúng tôi sử dụng uniform affine quantization – symmetric weights, asymmetric activations – với thiết lập static activation range, như được thảo luận trong Phần 2. Chúng tôi lượng tử hóa tất cả weights và activations (cả input và output), ngoại trừ lớp tuyến tính cuối cùng cho các mô hình BERT và OPT. Chúng tôi khám phá một số lựa chọn ước lượng phạm vi (xem Phụ lục C.4) và báo cáo cấu hình tốt nhất cho mỗi thí nghiệm, dựa trên hiệu suất mô hình. Chúng tôi lặp lại mỗi thí nghiệm PTQ 3 lần với các random seeds khác nhau⁸ và báo cáo mean và standard deviation cho accuracy/perplexity.

Chúng tôi huấn luyện mỗi mạng hai lần với các random seeds khác nhau và báo cáo mean và standard deviation. Để đánh giá lượng outliers trong mô hình đã huấn luyện, chúng tôi sử dụng hai chỉ số: maximum ∥x∥∞ trung bình qua tập validation, và kurtosis của x trung bình qua tất cả các lớp, trong đó x là đầu ra của một attention layer. Những chỉ số này đã được chỉ ra có tương quan tốt với khả năng quantizable của mô hình [4, 6].

5.1 Tác động của các siêu tham số clipped softmax (γ và ζ)
Chúng tôi điều tra hiệu ứng của các giá trị khác nhau của các tham số kéo giãn clipped softmax và trình bày kết quả trong Bảng 1. Chúng ta có thể thấy rằng hầu hết cải thiện xảy ra khi chúng ta sử dụng γ < 0 (cắt tại zero). Ví dụ, sử dụng giá trị γ=−0.03 dẫn đến infinity norm, kurtosis nhỏ hơn đáng kể, và perplexity mô hình lượng tử hóa, so với baseline. Cũng rõ ràng rằng trong giới hạn |γ| → 0 chúng ta tiếp cận vanilla softmax attention. Sử dụng ζ > 1 (cắt tại một) cho kết quả tương tự như vanilla softmax. Cuối cùng, khi chúng ta kết hợp cả γ < 0 và ζ > 1, kết quả có vẻ tương tự như chỉ cắt tại 0. Do đó, chúng tôi kết luận rằng để giảm outliers, chỉ việc cắt phạm vi thấp hơn cho phép số không chính xác mới quan trọng. Tiến lên phía trước chúng tôi chỉ sử dụng γ < 0 và trong Phụ lục B.5 chúng tôi xác nhận rằng ζ > 1 không cần thiết cho ViT.

Những quan sát này phù hợp với giả thuyết của chúng tôi rằng bằng cách cung cấp cho mô hình cơ chế để biểu diễn số không chính xác trong attention, chúng ta không cần học các outliers mạnh.

5.2 Clipped softmax γ vs. độ dài chuỗi
Vì có thêm một siêu tham số cần được điều chỉnh cho mỗi mô hình hoặc thiết lập thường không mong muốn, chúng tôi nghiên cứu độ nhạy của hệ số kéo giãn γ và mối quan hệ của nó với độ dài chuỗi T. Nhớ lại rằng ma trận xác suất attention P có chiều T×T và mỗi hàng tổng bằng một. Vì điều đó, giá trị trung bình trong P là 1/T. Hợp lý khi giả định rằng nếu chúng ta định nghĩa

⁸Các tập con ngẫu nhiên khác nhau của dữ liệu huấn luyện được sử dụng để ước lượng phạm vi quantizer.

--- TRANG 8 ---
γ := −α/T, trong đó α > 0 là một siêu tham số mới, có thể có một tập hợp hoặc phạm vi giá trị α hoạt động tốt qua các độ dài chuỗi khác nhau.

Để nghiên cứu điều này, chúng tôi huấn luyện biến thể 6-layer của BERT-base (BERT-6L) trong 500000 steps trên WikiText-103 [42] với batch size 128 với một số giá trị độ dài chuỗi tối đa T ∈ {32,64,128,192,256} và giá trị α ∈ {1/4,1/2,1,2,4,8}. Như chúng ta có thể thấy từ Hình 6, sử dụng clipped softmax với α ∈ [2,4] giảm đáng kể độ lớn của outliers trong khi duy trì perplexity FP16 tốt qua tất cả độ dài chuỗi đã khám phá.

[Hình 6: Hiệu suất clipped softmax sử dụng tham số hóa γ=−α/T trên BERT-6L]

5.3 Tác động của khởi tạo bias trong gated attention
Trong tất cả các thí nghiệm gated attention của chúng tôi, chúng tôi khởi tạo ngẫu nhiên weights của G, theo [22]. Tuy nhiên, bằng cách khởi tạo bias thành một giá trị cụ thể, chúng ta có thể đặt gates mở hơn hoặc đóng hơn ban đầu. Mở hơn lúc bắt đầu có nghĩa là chúng ta khởi tạo gần với mạng gốc hơn, nhưng với bản chất exponential của gate, có thể mất nhiều lần lặp để gate học cách đóng. Tương tự, nếu tất cả gates đều đóng lúc bắt đầu, chúng ta sai lệch quá xa khỏi huấn luyện mô hình gốc, gây ra suy giảm tiềm năng về hiệu suất. Giả định Linear Gi với weights ban đầu nhỏ, nếu chúng ta đặt bias thành giá trị binit, thì Gi(·) ≈ binit và πi(·) = sigmoid(Gi(·)) ≈ sigmoid(binit) =: πinit, tại thời điểm bắt đầu huấn luyện.

Chúng tôi nghiên cứu hiệu ứng của các giá trị khác nhau của binit cho Linear gated attention trên BERT-6L và ViT. Chúng tôi đặt bias cho tất cả Gi thành cùng giá trị binit. Cho BERT-6L, chúng tôi sử dụng thiết lập tương tự như trong Phần 5.2, với độ dài chuỗi cố định là 128. Cho ViT, chúng tôi sử dụng thiết lập chính, ngoại trừ chúng tôi huấn luyện nó trong 150 epochs thay vì 300.

[Hình 7: Hiệu suất Linear gated attention sử dụng các thiết lập khởi tạo bias khác nhau]

Trong Hình 7 chúng ta thấy trong cả trường hợp BERT và ViT rằng sử dụng bias với πinit rất cao thường hoạt động tương tự như vanilla attention (hiệu suất floating-point tương đương nhưng outliers mạnh và hiệu suất lượng tử hóa kém) trong khi đặt bias để có πinit rất thấp giảm outliers khá tốt nhưng dẫn đến suy giảm mạnh trong hiệu suất floating-point và lượng tử hóa. Phạm vi hợp lý của πinit có vẻ khoảng [0.25,0.9] cho BERT và [0.1,0.5] cho ViT. Phạm vi rộng cho thấy tính bền vững tương đối của phương pháp chúng tôi đối với siêu tham số này.

--- TRANG 9 ---
[Bảng 2: Tóm tắt kết quả cho các phương pháp đề xuất của chúng tôi được áp dụng trên BERT, OPT-125m, và ViT]

5.4 Kết quả chính
Chúng tôi tóm tắt tập kết quả chính của mình trong Bảng 2. Như chúng ta có thể thấy, trong hầu hết tất cả các trường hợp, cả hai kỹ thuật đề xuất của chúng tôi đều giảm độ lớn của outliers một cách đáng kể, giảm kurtosis, và tạo ra các mô hình với hiệu suất lượng tử hóa cao hơn đáng kể, gần với hiệu suất FP16/32 ban đầu. Ngoài ra, cho mỗi mô hình, ít nhất một trong các phương pháp của chúng tôi cũng cải thiện hiệu suất tác vụ floating-point. Chúng tôi giả thuyết điều này là vì mạng được giúp đỡ với việc học các cập nhật "no-op" dễ dàng hơn. Tuy nhiên, chúng tôi thận trọng về hiệu suất được cải thiện vì điều này không nhất quán qua tất cả các siêu tham số và không rõ liệu nó có tổng quát hóa cho nhiều kiến trúc và mô hình lớn hơn hay không.

Trường hợp duy nhất mà phương pháp của chúng tôi không hoạt động tốt là clipped softmax được áp dụng cho OPT. Hiện tại, chúng tôi không có lời giải thích tại sao điều này xảy ra và để lại cho nghiên cứu tương lai. Chúng tôi liệt kê các siêu tham số được chọn và hiển thị kết quả mở rộng trong Phụ lục B. Chúng tôi cũng hiển thị kết quả của các phương pháp đề xuất được lượng tử hóa thành bitwidths thấp hơn trong Phụ lục B.7.

Kết quả cho các mô hình lớn hơn Chúng tôi nghiên cứu câu hỏi về khả năng mở rộng của các phương pháp cho các mô hình lớn hơn. Trong Bảng 3 chúng tôi hiển thị kết quả gated attention cho các biến thể 350m và 1.3B của OPT. Do hạn chế tính toán, chúng tôi huấn luyện mạng trong 10⁵ steps với batch size 256 và phần còn lại giống như thiết lập pre-training chính của chúng tôi. Như chúng ta có thể thấy, gated attention đề xuất của chúng tôi cũng rất hiệu quả trong việc giảm outliers và cải thiện đáng kể hiệu suất mô hình lượng tử hóa khi được áp dụng cho các mô hình lớn hơn. Chúng tôi nghiên cứu thêm trong Phụ lục B.6 về cách gated attention có thể giảm outliers khi fine-tuning các mô hình lớn hơn đã được pre-train với outliers.

[Bảng 3: Hiệu suất gated attention được áp dụng trên các biến thể OPT lớn hơn]

5.5 Kết quả định tính
Trong Hình 8 chúng tôi so sánh các mẫu attention đã học sử dụng vanilla softmax và các phương pháp đề xuất của chúng tôi (thêm ví dụ trong Phụ lục A.1). Như chúng ta có thể thấy, cả hai phương pháp đều có thể biểu diễn hành vi partial/soft no-op, nhưng trong trường hợp các phương pháp của chúng tôi điều này không đòi hỏi outliers mạnh ở nơi khác trong mạng. Lưu ý rằng chúng tôi tìm thấy các mẫu tương tự trong nhiều attention heads, nhưng các chỉ số head chính xác nơi chúng tôi quan sát những mẫu như vậy phụ thuộc vào khởi tạo ngẫu nhiên. Trong trường hợp clipped softmax, các trọng số attention nhỏ hơn thường phân tán hơn trong khi các trọng số cao hơn bão hòa hơn (đến từ việc kéo giãn và cắt). Trong trường hợp gated attention, đầu ra của softmax khác đáng kể vì việc cập nhật biểu diễn ẩn bây giờ được điều chế thêm bởi xác suất gating.

--- TRANG 10 ---
[Hình 8: Trực quan hóa các mẫu self-attention cho BERT-base được huấn luyện bằng vanilla và các kỹ thuật đề xuất của chúng tôi]

6 Thảo luận
Hành vi "No-op" Thú vị khi lưu ý rằng hành vi "no-op" đã xác định có thể không giới hạn ở transformers và các kiến trúc convolutional có thể học điều gì đó tương tự. Chúng tôi cũng thấy rằng mặc dù mạng cố gắng học một "no-op" đầy đủ, vẫn có một lượng nhỏ tiếng ồn được thêm vào mỗi residual, có thể tạo thành một dạng regularization mạng. Điều tra điều này thêm có thể cho chúng ta manh mối về lý do tại sao mạng nơ-ron tổng quát hóa mặc dù được tham số hóa quá mức đáng kể nếu nhiều tham số bị vô hiệu hóa bằng cách không cập nhật biểu diễn trong các lớp sau [72].

Hạn chế Trong khi chúng tôi nghiên cứu khả năng mở rộng của phương pháp cho các mô hình lên đến kích thước 1.3B, chúng tôi chưa khám phá trường hợp các transformers rất lớn được huấn luyện trong thời gian dài hơn nhiều. Với hiểu biết cơ bản về vấn đề cơ sở các giải pháp của chúng tôi, chúng tôi kỳ vọng hiệu ứng tương tự trên các mô hình quy mô lớn hơn. Chúng tôi thấy cải thiện rất nhỏ trong hiệu suất FP16/FP32 do các phương pháp của chúng tôi, nhưng chúng tôi không coi kết quả của mình đủ toàn diện để khẳng định rằng điều này sẽ đúng nói chung. Cuối cùng, các phương pháp của chúng tôi có một siêu tham số mỗi cái, mặc dù chúng tôi chỉ ra rằng cả hai phương pháp đều tương đối bền vững đối với siêu tham số của nó, có một cái không bao giờ là tối ưu.

Tác động Vì các phương pháp của chúng tôi giúp transformers hiệu quả hơn, chúng tôi chỉ kỳ vọng kết quả tích cực từ công việc của mình. Làm cho mạng nơ-ron hiệu quả hơn sẽ giúp với tiêu thụ điện năng cao của chúng tại inference. Nó tiếp tục giúp di chuyển inference từ cloud đến các thiết bị edge có thể khắc phục mối quan tâm tiềm năng về quyền riêng tư. Chúng tôi không thể tưởng tượng bất kỳ tác động tiêu cực nào từ công việc của mình mà không bị hiểu sai nghiêm trọng.

7 Kết luận
Chúng tôi đã phân tích kỹ lưỡng vấn đề outlier activation khiến transformers khó lượng tử hóa. Chúng tôi chỉ ra rằng mạng transformer cố gắng học không cập nhật residuals và bằng cách làm như vậy, thông qua sự kết hợp của softmax, kết nối residual và LayerNorm, các outliers đáng kể xuất hiện trong transformers. Dựa trên hiểu biết này, chúng tôi đề xuất hai phương pháp để giải quyết điều này tại cốt lõi – clipped softmax và gated attention. Những thay đổi cấu trúc này cho transformers cho hiệu suất floating-point tương tự, nếu không tốt hơn, sau khi huấn luyện nhưng cải thiện đáng kể kết quả lượng tử hóa post-training. Chúng tôi hy vọng rằng với hai thay đổi kiến trúc này cho transformers, bất kỳ ai cũng có thể huấn luyện transformers hiệu suất cao dễ lượng tử hóa và có thể hưởng lợi từ inference integer hiệu quả.
