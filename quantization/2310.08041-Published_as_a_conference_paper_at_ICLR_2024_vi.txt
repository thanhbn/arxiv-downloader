# 2310.08041.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/quantization/2310.08041.pdf
# Kích thước tệp: 2031857 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
QLLM: LƯỢNG TỬ HÓA CHÍNH XÁC VÀ HIỆU QUẢ BIT THẤP
CHO CÁC MÔ HÌNH NGÔN NGỮ LỚN
Jing Liu1,2∗, Ruihao Gong2,3, Xiuying Wei2,4, Zhiwei Dong2,5, Jianfei Cai1, Bohan Zhuang1†
1ZIP Lab, Đại học Monash2SenseTime Research3Đại học Beihang
4Trường Khoa học Máy tính và Truyền thông, EPFL
5Đại học Khoa học và Công nghệ Bắc Kinh

TÓM TẮT
Các Mô hình Ngôn ngữ Lớn (LLMs) đã chứng minh hiệu quả vô song trong xử lý ngôn ngữ tự nhiên. Tuy nhiên, nhu cầu tính toán cao và chi phí bộ nhớ lớn cản trở việc triển khai rộng rãi của chúng. Để giải quyết vấn đề này, hai chiến lược lượng tử hóa xuất hiện, bao gồm Huấn luyện Nhận biết Lượng tử hóa (QAT) và Lượng tử hóa Sau Huấn luyện (PTQ). Đối với LLMs, hàng tỷ tham số khiến QAT trở nên không thực tế do chi phí huấn luyện cấm đoán và do đó PTQ trở nên phổ biến hơn. Trong các nghiên cứu hiện tại, các ngoại lệ kích hoạt trong các kênh cụ thể được xác định là thách thức lớn nhất đối với độ chính xác PTQ. Họ đề xuất chuyển đổi độ lớn từ kích hoạt sang trọng số, tuy nhiên điều này chỉ mang lại sự giảm thiểu hạn chế hoặc gặp phải gradient không ổn định, dẫn đến sự sụt giảm hiệu suất nghiêm trọng ở bit-width thấp. Trong bài báo này, chúng tôi đề xuất QLLM, một phương pháp PTQ bit-width thấp chính xác và hiệu quả được thiết kế cho LLMs. QLLM giới thiệu một kỹ thuật tái lắp ráp kênh thích ứng để phân bổ lại độ lớn của các ngoại lệ sang các kênh khác, từ đó giảm thiểu tác động của chúng lên phạm vi lượng tử hóa. Điều này được thực hiện bằng cách tháo rời kênh và lắp ráp kênh, trước tiên chia nhỏ các kênh ngoại lệ thành nhiều kênh con để đảm bảo phân phối cân bằng hơn của độ lớn kích hoạt. Sau đó, các kênh tương tự được hợp nhất để duy trì số lượng kênh ban đầu cho hiệu quả. Ngoài ra, một chiến lược thích ứng được thiết kế để tự động xác định số lượng tối ưu của các kênh con để tháo rời kênh. Để bù đắp thêm cho việc mất hiệu suất do lượng tử hóa gây ra, chúng tôi đề xuất một phương pháp điều chỉnh hiệu quả chỉ học một số lượng nhỏ trọng số rank thấp trong khi đóng băng mô hình lượng tử hóa đã được huấn luyện trước. Sau khi huấn luyện, các tham số rank thấp này có thể được hợp nhất vào các trọng số đóng băng mà không ảnh hưởng đến suy luận. Các thí nghiệm mở rộng trên LLaMA-1 và LLaMA-2 cho thấy QLLM có thể thu được các mô hình lượng tử hóa chính xác một cách hiệu quả. Ví dụ, QLLM lượng tử hóa LLaMA-2-70B 4-bit trong vòng 10 giờ trên một GPU A100-80G duy nhất, vượt trội hơn phương pháp tối tân trước đó 7.89% về độ chính xác trung bình trên năm nhiệm vụ zero-shot. Mã nguồn có sẵn tại ZIP Lab và ModelTC.

1 GIỚI THIỆU
Gần đây, các Mô hình Ngôn ngữ Lớn (LLMs) như GPT-4 (OpenAI, 2023) và LLaMA (Touvron et al., 2023a;b) đã đạt được những tiến bộ chưa từng có trong xử lý ngôn ngữ tự nhiên (NLP). Những mô hình này xuất sắc trong một loạt các nhiệm vụ, từ lý luận nâng cao về mã và toán học đến phân loại và trả lời câu hỏi. Tuy nhiên, hiệu suất đặc biệt của chúng đi kèm với nhu cầu tính toán đáng kể và kích thước mô hình khổng lồ. Ví dụ, GPT-3 (Brown et al., 2020), tiền thân của GPT-4, đã chứa 175 tỷ tham số đáng kinh ngạc, yêu cầu tối thiểu 325 GB bộ nhớ để lưu trữ ở định dạng nửa độ chính xác (FP16). Điều này đòi hỏi việc sử dụng ít nhất 5×80GB NVIDIA A100 hoặc 8×48GB NVIDIA A40 GPU trong giai đoạn suy luận. Kết quả là, việc triển khai những mô hình này vào các ứng dụng thực tế đặt ra những thách thức đáng kể.

∗Công việc được thực hiện trong thời gian thực tập tại SenseTime Research.
†Tác giả liên hệ. Email: bohan.zhuang@gmail.com
1arXiv:2310.08041v3 [cs.CL] 6 Apr 2024

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[Hình 1: Minh họa các giá trị tối đa và tối thiểu theo kênh cho các kích hoạt đầu vào của một tầng tuyến tính trong LLaMA-65B cho (a) mô hình được huấn luyện trước ban đầu (b) sau SmoothQuant (Xiao et al., 2023) và (c) sau tái lắp ráp kênh của chúng tôi.]

Xét đến những thách thức nói trên, lượng tử hóa mạng (Zhou et al., 2016) nổi lên như một giải pháp hấp dẫn, ánh xạ trọng số và/hoặc kích hoạt thành các biểu diễn bit thấp hơn, dẫn đến dung lượng bộ nhớ thấp hơn nhiều và suy luận nhanh hơn. Các phương pháp lượng tử hóa hiện tại cho LLMs có thể được phân loại thành hai loại: huấn luyện nhận biết lượng tử hóa (QAT) (Liu et al., 2023) và lượng tử hóa sau huấn luyện (PTQ) (Wei et al., 2022b; 2023; Xiao et al., 2023). Mặc dù có hiệu suất đầy hứa hẹn, QAT gặp phải chi phí huấn luyện không thể chịu đựng được vì nó cần tinh chỉnh toàn bộ mô hình lượng tử hóa với các tham số lượng tử hóa sử dụng một lượng lớn dữ liệu, khiến nó không thực tế cho việc triển khai hiệu quả của LLMs. Hạn chế thực tế này đã chuyển sự chú ý sang PTQ chỉ sử dụng ít dữ liệu để điều chỉnh các trọng số lượng tử hóa. Tuy nhiên, khi nói đến lượng tử hóa bit-width cực thấp cho LLMs, ví dụ như lượng tử hóa trọng số và/hoặc kích hoạt 4-bit, các phương pháp PTQ hiện tại (Xiao et al., 2023; Dettmers et al., 2022) gặp phải sự suy giảm hiệu suất đáng kể.

Các nghiên cứu gần đây (Dettmers et al., 2022; Xiao et al., 2023; Wei et al., 2023) đã tiết lộ một mẫu độc đáo trong các kích hoạt của LLMs là chúng chứa các kênh ngoại lệ cụ thể với độ lớn đáng kể. Điều này khiến các phương pháp lượng tử hóa hiện tại kém hiệu quả hơn, vì các ngoại lệ khuếch đại phạm vi lượng tử hóa của các kích hoạt tầng, khiến phần lớn các giá trị kích hoạt bình thường bị lượng tử hóa không chính xác và do đó dẫn đến sự suy giảm hiệu suất đáng kể. Vấn đề này sẽ trở nên tồi tệ hơn với việc sử dụng phổ biến lượng tử hóa kích hoạt theo tầng hoặc theo token, một thực hành phổ biến để tối đa hóa hiệu quả phần cứng. Để giải quyết thách thức này, các nghiên cứu gần đây (Xiao et al., 2023; Wei et al., 2022b; 2023; Shao et al., 2023) đã tập trung vào việc làm mượt các ngoại lệ kích hoạt bằng cách chuyển đổi độ lớn từ kích hoạt sang trọng số thông qua một phép biến đổi tương đương về mặt toán học. Phép biến đổi như vậy có thể được học bằng các phương pháp không gradient (Xiao et al., 2023; Wei et al., 2022b; 2023) hoặc các phương pháp dựa trên gradient (Shao et al., 2023). Tuy nhiên, như được hiển thị trong Hình 1, đối với các ngoại lệ kích hoạt cực kỳ rõ rệt (những ngoại lệ lớn hơn 50× so với những ngoại lệ khác), phương pháp trước chỉ mang lại sự giảm thiểu hạn chế trong khi phương pháp sau gặp phải gradient không ổn định. Kết quả là, cả hai phương pháp đều dẫn đến sự suy giảm hiệu suất đáng kể trong lượng tử hóa bit-width thấp. Để bù đắp cho sự sụt giảm hiệu suất của lượng tử hóa, một chiến lược PTQ được áp dụng rộng rãi (Wei et al., 2023; Shao et al., 2023; Yao et al., 2022) tiếp tục đề xuất điều chỉnh LLM lượng tử hóa trực tiếp bằng cách tối thiểu hóa lỗi tái tạo theo khối. Trong LLMs, khối được điều chỉnh đề cập đến module Attention-FFN. Tuy nhiên, xét đến số lượng tham số khổng lồ trong một LLM, phương pháp này vẫn yêu cầu chi phí huấn luyện đáng kể và đòi hỏi một lượng bộ nhớ GPU đáng kể.

Trong bài báo này, chúng tôi đề xuất QLLM, một phương pháp lượng tử hóa sau huấn luyện bit-width thấp chính xác và hiệu quả được thiết kế riêng cho LLMs. Để xử lý vấn đề ngoại lệ, chúng tôi giới thiệu một kỹ thuật tái lắp ráp kênh không gradient để phân phối lại độ lớn kích hoạt lớn của các kênh ngoại lệ trên các kênh. Cụ thể, chúng tôi trước tiên tháo rời các kênh ngoại lệ thành nhiều kênh con. Bằng cách phân tán độ lớn của các ngoại lệ, nó đảm bảo phạm vi kích hoạt đồng đều hơn trên các kênh, tạo điều kiện cho lượng tử hóa cân bằng và chính xác, từ đó cải thiện hiệu suất của LLMs lượng tử hóa. Sau đó chúng tôi giới thiệu lắp ráp kênh, hợp nhất các kênh tương tự lại với nhau để duy trì số lượng kênh ban đầu. Hơn nữa, với các mẫu ngoại lệ khác nhau trên các tầng khác nhau và sự tồn tại của các ngoại lệ cực đoan, chúng tôi đề xuất một chiến lược thích ứng để xác định số lượng tối ưu của các kênh được tháo rời cho mỗi tầng, dựa trên việc tối thiểu hóa lỗi tái lắp ráp giữa các kích hoạt đầu ra ban đầu và đối tác với các kích hoạt đầu vào được tái lắp ráp.

Để cải thiện thêm hiệu suất của LLMs lượng tử hóa, được thúc đẩy bởi mô hình tinh chỉnh hiệu quả tham số rank thấp LoRA (Hu et al., 2022; Dettmers et al., 2023a), chúng tôi tiếp tục đề xuất

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

một chiến lược hiệu chỉnh lỗi hiệu quả dựa trên gradient, đóng băng mô hình được huấn luyện trước và giới thiệu một tập hợp nhỏ các trọng số rank thấp có thể học được vào mỗi tầng của LLM. Sau đó, QLLM học các trọng số rank thấp bằng cách tối thiểu hóa lỗi lượng tử hóa theo khối một cách tuần tự. Nhờ số lượng tham số có thể huấn luyện giảm, cả thời gian huấn luyện và yêu cầu bộ nhớ GPU đều được giảm đáng kể. Việc tăng hiệu quả như vậy cho phép chúng tôi thực hiện tái tạo đa khối đồng thời tái tạo một tập hợp các khối Attention-FFN liên tiếp, giảm thiểu thêm việc tích lũy lỗi lượng tử hóa trong quá trình lan truyền trong LLMs bit thấp. Đáng chú ý, sau khi huấn luyện, những trọng số rank thấp có thể học này có thể được hợp nhất một cách liền mạch với các trọng số đóng băng theo sau bởi lượng tử hóa, từ đó đảm bảo không có gánh nặng tính toán bổ sung trong quá trình suy luận.

Các đóng góp của chúng tôi có thể được tóm tắt như sau: 1) Chúng tôi giới thiệu một phương pháp tái lắp ráp kênh đơn giản nhưng hiệu quả để ngăn chặn các ngoại lệ kích hoạt trong LLMs, được thực hiện bằng cách ban đầu tháo rời các kênh ngoại lệ để làm cho các kích hoạt thân thiện hơn với lượng tử hóa và sau đó hợp nhất các kênh tương tự để bảo tồn số lượng kênh ban đầu cho hiệu quả. Chúng tôi cũng đề xuất xác định số lượng tối ưu của các kênh được tháo rời cho mỗi tầng, xem xét các mẫu ngoại lệ đa dạng trên các tầng và sự hiện diện của các ngoại lệ cực đoan. Toàn bộ quá trình không có gradient và có hiệu quả cao. 2) Một cơ chế hiệu chỉnh lỗi hiệu quả được đề xuất để tăng cường thêm cho tái lắp ráp kênh không gradient. Nó tận dụng việc học các tham số rank thấp để chống lại lỗi lượng tử hóa một cách có cấu trúc, dẫn đến việc giảm đáng kể thời gian huấn luyện và yêu cầu bộ nhớ GPU mà không phát sinh bất kỳ chi phí suy luận bổ sung nào. 3) Các thí nghiệm mở rộng cho thấy hiệu suất đầy hứa hẹn và hiệu quả huấn luyện của QLLM. Ví dụ, QLLM lượng tử hóa LLaMA-2-70B 4-bit trong vòng 10 giờ và vượt trội hơn các phương pháp SOTA trước đó 7.89% về độ chính xác trung bình trên năm nhiệm vụ zero-shot.

2 CÔNG VIỆC LIÊN QUAN

Lượng tử hóa mạng. Lượng tử hóa mạng (Zhou et al., 2016) biểu diễn trọng số, kích hoạt và thậm chí gradient với độ chính xác thấp, là một phương pháp hiệu quả để giảm kích thước mô hình và gánh nặng tính toán. Các kỹ thuật hiện tại rơi vào hai danh mục chính: huấn luyện nhận biết lượng tử hóa (QAT) (Esser et al., 2020; Kim et al., 2021; Li et al., 2022) và lượng tử hóa sau huấn luyện (PTQ) (Nagel et al., 2020; Li et al., 2021; Wei et al., 2022a). QAT tích hợp quá trình lượng tử hóa trực tiếp vào giai đoạn huấn luyện và học chung bộ lượng tử hóa cũng như tham số mô hình (Zhang et al., 2018; Jung et al., 2019; Choi et al., 2019; Bhalgat et al., 2020; Esser et al., 2020; Liu et al., 2022) với sự trợ giúp của ước lượng xuyên suốt (STE) (Bengio et al., 2013), điều này giảm thiểu đáng kể sự suy giảm độ chính xác do nén gây ra. Tuy nhiên, chi phí huấn luyện của QAT có thể cực kỳ cao, chủ yếu vì nó yêu cầu tinh chỉnh mô hình lượng tử hóa trên tập dữ liệu huấn luyện ban đầu của mô hình được huấn luyện trước. PTQ cung cấp một lựa chọn thay thế ít tốn tài nguyên hơn, cho phép các mô hình được lượng tử hóa sau khi được huấn luyện đầy đủ chỉ với một lượng nhỏ dữ liệu. Để giảm sự sụt giảm hiệu suất, một số phương pháp đã được đề xuất để thực hiện hiệu chuẩn theo tầng (Nagel et al., 2019; 2020; Wu et al., 2020; Hubara et al., 2020; Li et al., 2021) hoặc thậm chí theo khối (Li et al., 2021). Các đổi mới tiếp theo đi sâu vào giảm thiểu ngoại lệ, áp dụng các chiến lược như cắt ngắn (Banner et al., 2019; McKinstry et al., 2019; Choukroun et al., 2019) hoặc tách giá trị (Zhao et al., 2019) cho trọng số và kích hoạt để cải thiện độ chính xác bằng cách phân bổ nhiều bit hơn cho các giá trị trung gian. Tuy nhiên, đối với LLMs, một nghiên cứu gần đây (Liu et al., 2023) đã phát hiện rằng lượng tử hóa MinMax, duy trì phạm vi giá trị đầy đủ, hoạt động tốt hơn các phương pháp dựa trên cắt ngắn, vì các ngoại lệ rất quan trọng đối với hiệu suất. Khác với các phương pháp này, QLLM của chúng tôi nhắm mục tiêu lượng tử hóa cho LLMs.

Lượng tử hóa trên LLMs. Với các ràng buộc như dữ liệu huấn luyện hạn chế và nhu cầu tính toán đắt đỏ, các kỹ thuật lượng tử hóa phổ biến cho LLMs chủ yếu dựa trên PTQ. Các phương pháp lượng tử hóa LLM hiện tại có thể được phân loại thành hai danh mục: lượng tử hóa chỉ trọng số (Frantar et al., 2022; Park et al., 2023; Lin et al., 2023; Dettmers et al., 2023b; Chai et al., 2023; Cheng et al., 2023; Dettmers et al., 2023a; Kim et al., 2023; Chee et al., 2023; Lee et al., 2023) và lượng tử hóa trọng số-kích hoạt (Dettmers et al., 2022; Xiao et al., 2023; Wei et al., 2022b; 2023; Yao et al., 2022; 2023; Yuan et al., 2023; Liu et al., 2023; Wu et al., 2023). Phương pháp trước tập trung vào nén số lượng lớn trọng số trong LLMs để giảm dung lượng bộ nhớ, trong khi phương pháp sau nén cả trọng số và kích hoạt thành các giá trị bit thấp, nhằm tăng tốc phép nhân ma trận đòi hỏi tính toán cao. Để xử lý các phạm vi giá trị khác nhau của ma trận trọng số, các nghiên cứu gần đây đã đi sâu vào lượng tử hóa chi tiết hơn, như lượng tử hóa theo kênh (Frantar et al., 2022) hoặc lượng tử hóa theo nhóm (Frantar et al., 2022; Lin et al., 2023). Để bù đắp thêm cho sự sụt giảm hiệu suất đối với lượng tử hóa bit-width cực thấp, QLoRA (Dettmers et al., 2023a) và INT2.1 (Chai et al., 2023) giới thiệu các trọng số độ chính xác đầy đủ bổ sung (Yao et al., 2023). Trong khi phương pháp của chúng tôi cũng trình bày một tập hợp nhỏ trọng số rank thấp, nó khác biệt với QLoRA và INT2.1 vì các tham số có thể học của chúng tôi có thể được tham số hóa lại thành trọng số được huấn luyện trước theo sau bởi lượng tử hóa.

Nghiên cứu gần đây (Dettmers et al., 2022) đã cho thấy rằng các ngoại lệ kích hoạt tồn tại trong một số chiều đặc trưng trên các token khác nhau. Một số công việc (Wei et al., 2022b; 2023; Xiao et al., 2023; Shao et al., 2023) đã được đề xuất để di chuyển khó khăn lượng tử hóa từ kích hoạt sang trọng số trong cùng kênh, dựa trên các phương pháp không gradient (Wei et al., 2022b; Xiao et al., 2023; Wei et al., 2023) hoặc các phương pháp dựa trên gradient (Shao et al., 2023). Tuy nhiên, khi xử lý các ngoại lệ kích hoạt rất rõ rệt, các phương pháp hiện tại thường cho thấy cải thiện hạn chế hoặc phát sinh gradient không ổn định. Trong một sự khác biệt đáng chú ý, phương pháp QLLMs được đề xuất của chúng tôi phân phối lại một cách hiệu quả độ lớn kích hoạt lớn của các kênh ngoại lệ giữa tất cả các kênh, cung cấp một phương pháp tiếp cận khác biệt so với các phương pháp hiện tại này.

3 SƠ BỘ

Ký hiệu cơ bản. Trong bài báo này, ma trận được đánh dấu là X và vector được ký hiệu bằng x. LLMs thường có hai phần cốt lõi: các tầng tự chú ý đa đầu (MSA) và các tầng mạng feed-forward (FFN), chủ yếu được cấu thành từ các tầng tuyến tính. Ở đây, chúng tôi đưa ra công thức của các tầng tuyến tính tại kênh đầu ra k:
yk=∑(i=1 đến M) xi*Wik,                                                    (1)
trong đó x∈R^M đề cập đến đầu vào, W∈R^(M×N) biểu thị trọng số, và y∈R^N đại diện cho đầu ra. Theo cách này, số lượng kênh đầu vào và đầu ra lần lượt là M và N.

Lượng tử hóa. Chúng tôi áp dụng lượng tử hóa đồng đều cho cả trọng số và kích hoạt vì tính chất thân thiện với phần cứng của nó (Jacob et al., 2018). Đối với ma trận X với các giá trị dấu phẩy động như FP16 hoặc FP32, lượng tử hóa b-bit lượng tử hóa nó theo cách sau:
Xq = quant(X) = clamp(⌊X/α⌉ + β, 0, 2^b-1),
trong đó α = (max(X) - min(X))/(2^b-1), β = -⌊min(X)/α⌋,                  (2)

trong đó hàm clamp(v, vmin, vmax) cắt bất kỳ giá trị v nào vào phạm vi [vmin, vmax] và ⌊·⌉ là toán tử làm tròn trả về số nguyên gần nhất của một giá trị cho trước. Ở đây, α biểu thị hệ số tỷ lệ và β đại diện cho giá trị điểm không.

Các nghiên cứu gần đây (Dettmers et al., 2022; Xiao et al., 2023; Wei et al., 2022b) chỉ ra rằng có các ngoại lệ cực lớn trong các kênh nhất định của kích hoạt trong LLMs, điều này khiến lượng tử hóa gặp khó khăn trong việc cân bằng biểu diễn chính xác cho các giá trị lớn và số nhỏ. Để giải quyết vấn đề này, một số phương pháp (Bondarenko et al., 2021; Yuan et al., 2023) áp dụng sơ đồ lượng tử hóa chi tiết, gán các tham số lượng tử hóa khác nhau cho các kênh khác nhau. Tuy nhiên, cách như vậy cần thiết kế kernel tinh tế và rõ ràng làm tăng chi phí tính toán cho suy luận. Ngoài ra, một số công việc Wei et al. (2022b); Xiao et al. (2023) đề xuất sử dụng tỷ lệ theo kênh giữa kích hoạt và trọng số, vẫn còn các ngoại lệ trong các trường hợp cực đoan, như được hiển thị trong Hình 1.

4 PHƯƠNG PHÁP ĐỀ XUẤT

Trong phần này, chúng tôi đề xuất khung tái lắp ráp kênh thích ứng để phân phối lại các ngoại lệ kích hoạt đầu vào trên nhiều kênh. Khung bao gồm ba thành phần: tháo rời kênh để phân tách kênh ngoại lệ, lắp ráp kênh để cân bằng hiệu quả, và một chiến lược thích ứng để tìm tỷ lệ tái lắp ráp phù hợp cho mỗi tầng. Kỹ thuật tái lắp ráp kênh không có gradient và hiệu quả để thực hiện. Hơn nữa, nó có thể được trang bị với một module hiệu chỉnh lỗi dựa trên gradient và được thiết kế tốt để tăng cường thêm.

4.1 TÁI LẮP RÁP KÊNH THÍCH ỨNG

4.1.1 THÁO RỜI KÊNH

Trong phần này, chúng tôi giới thiệu việc tháo rời kênh của chúng tôi để phân tách các kênh ngoại lệ đầu vào thành nhiều kênh con, có thể giảm độ lớn ngoại lệ và làm cho các kích hoạt thân thiện hơn với lượng tử hóa mà không thay đổi đầu ra tầng.

Xem xét rằng các ngoại lệ có xu hướng tập trung trong các kênh cụ thể trên các đầu vào khác nhau và mong muốn bảo tồn thông tin của chúng trong quá trình lượng tử hóa, chúng tôi đề xuất chia nhỏ những kênh ngoại lệ này thành nhiều kênh con để phân phối lại các giá trị lớn của chúng. Không mất tính tổng quát, bằng cách giả định kênh thứ M là kênh ngoại lệ, chúng ta có thể tháo rời nó thành xM/T và nhân bản kênh này T lần, giảm độ lớn ngoại lệ theo hệ số T. Đồng thời, cũng tự nhiên khi nhân đôi kênh trọng số tương ứng T lần, cho phép chúng ta duy trì đầu ra tương đương:
yk=∑(i=1 đến M-1) xi*Wik + (xM/T)*WMk + ··· + (xM/T)*WMk
                                            T lần.                         (3)

Phương trình trên tạo ra cùng một đầu ra với phương trình tầng tuyến tính ban đầu trong Eq. (1) và giới thiệu thêm T-1 kênh cho cả đầu vào và trọng số.

Xem xét rằng phạm vi lượng tử hóa ảnh hưởng đến độ chính xác, chúng tôi giới thiệu một ngưỡng ngoại lệ, ký hiệu là θ, để xác định các kênh ngoại lệ và xác định số lượng kênh con cùng nhau, với T=⌈max(|xM|)/θ⌉. Phương pháp này đảm bảo rằng các kênh có giá trị nhỏ hơn θ vẫn không thay đổi với T=1, trong khi độ lớn của các ngoại lệ được chia cho T.

Phương pháp tháo rời kênh của chúng tôi cho phép chúng ta giữ lại thông tin ngoại lệ với đầu ra tương đương và giảm nhẹ khó khăn lượng tử hóa với phạm vi giá trị nhỏ hơn nhiều. Nhược điểm duy nhất của nó là việc tăng số lượng kênh, có thể dẫn đến chi phí tính toán bổ sung và sẽ được giải quyết trong tiểu mục tiếp theo.

4.1.2 LẮP RÁP KÊNH

Lưu ý rằng số lượng kênh đầu vào tăng lên M+T-1 sau khi tháo rời kênh. Với số lượng kênh đáng kể trong LLMs, có thể bỏ qua một số kênh không quan trọng hoặc hợp nhất các kênh đầu vào tương tự để giữ số lượng kênh ban đầu M cho hiệu quả trong khi duy trì đầu ra. Để đạt được điều này, một phương pháp đơn giản là sử dụng cắt tỉa kênh (Ma et al., 2023; Sun et al., 2023) loại bỏ trực tiếp các kênh không quan trọng. Tuy nhiên, phương pháp như vậy có thể dẫn đến mất thông tin đáng kể, đặc biệt khi T lớn. Được thúc đẩy bởi các nghiên cứu gần đây (Bolya et al., 2023; Bolya & Hoffman, 2023) kết hợp các token tương tự, chúng tôi đề xuất một phương pháp lắp ráp kênh đi sâu vào việc hợp nhất T-1 kênh đầu vào tương tự. Với các kênh i và j, phù hợp với các kỹ thuật hợp nhất token (Bolya et al., 2023; Bolya & Hoffman, 2023), mục tiêu của chúng tôi là tổng hợp chúng bằng cách tính trung bình của các đặc trưng đầu vào của chúng, ký hiệu là (xi+xj)/2 và sử dụng đặc trưng tổng hợp trong các tính toán tiếp theo, được định nghĩa là:
xi*Wik + xj*Wjk ≈ ((xi+xj)/2)*(Wik+Wjk),                                (4)
trong đó Wik+Wjk đại diện cho trọng số được hợp nhất. Với mục tiêu tối thiểu hóa mất thông tin của lắp ráp kênh trong Eq. (4), chúng ta có thể định nghĩa một thước đo khoảng cách D(i,j) giữa các kênh i và j là
D(i,j) = ||xi(Wik-Wjk)/2 + xj(Wjk-Wik)/2||₂²,                           (5)
trong đó ||·||₂ biểu thị chuẩn ℓ2. Thước đo khoảng cách trên tính đến sự khác biệt trong cả kích hoạt đầu vào và trọng số giữa hai kênh.

Với khoảng cách kênh được định nghĩa, bước tiếp theo là xác định hiệu quả những kênh nào cần tổng hợp, với mục tiêu giảm tổng số kênh đi T-1. Để giải quyết điều này, chúng tôi đề xuất sử dụng khớp mềm lưỡng phần (Bolya et al., 2023; Bolya & Hoffman, 2023) trước tiên phân chia các kênh thành hai tập hợp, mỗi tập chứa kích thước gần bằng nhau, và sau đó tìm T-1 cặp tương tự nhất giữa hai tập hợp này (xem Phụ lục A để biết chi tiết). Lưu ý rằng chúng tôi không lắp ráp các kênh được tháo rời từ các kênh ngoại lệ vì chúng đóng vai trò quan trọng trong hiệu suất của LLMs. Sau khi tái lắp ráp kênh, bao gồm cả tháo rời và lắp ráp, chúng tôi có được các kích hoạt đầu vào được tái lắp ráp thuận tiện hơn cho lượng tử hóa, cùng với các trọng số được tái lắp ráp tương ứng cho tầng l.

4.1.3 TÁI LẮP RÁP THÍCH ỨNG

Trong phần này, chúng tôi trình bày một phương pháp để xác định thích ứng tỷ lệ tái lắp ráp phù hợp cho mỗi tầng. Đối với tháo rời kênh, việc chọn giá trị cao cho T với θ nhỏ giảm đáng kể độ lớn ngoại lệ và có lợi cho lượng tử hóa, trong khi dẫn đến việc tăng lớn hơn trong lỗi hợp nhất kênh do tỷ lệ hợp nhất cao hơn. Ngược lại, việc chọn T nhỏ với θ lớn sẽ không tăng nhiều số lượng kênh, làm cho giai đoạn lắp ráp dễ dàng hơn để giữ thông tin trong khi có thể vẫn giữ lại các ngoại lệ, gây ra lỗi lượng tử hóa đáng kể. Do đó, việc xác định cẩn thận ngưỡng ngoại lệ θ hoặc số lượng kênh tái lắp ráp T là rất quan trọng.

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Tuy nhiên, khó có thể chọn θ trong thực tế vì các tầng khác biệt có các mẫu ngoại lệ khác nhau, như được hiển thị trong Hình D. Được thúc đẩy bởi (Wei et al., 2023), chúng tôi đề xuất một chiến lược thích ứng để tìm θ tối ưu bằng cách tối thiểu hóa lỗi tái lắp ráp giữa các kích hoạt đầu ra ban đầu và đối tác của chúng được tạo ra với các kích hoạt đầu vào được tái lắp ráp cho mỗi tầng.

Lưu ý rằng kỹ thuật tái lắp ráp kênh của chúng tôi có thể tạo ra kích hoạt được tái lắp ráp X̂∈R^(L×M) với độ dài chuỗi L, sau đó có thể được đưa vào tầng MSA hoặc tầng FFN. Ví dụ, hãy xem xét trường hợp X̂ được đưa vào tầng MSA. Một tầng MSA chuẩn tính toán các truy vấn, khóa và giá trị với ba ma trận chiếu có thể học W^Q, W^K, W^V∈R^(M×N) là Q=XW^Q, K=XW^K, V=XW^V, trong đó X∈R^(L×M) đại diện cho kích hoạt đầu vào ban đầu. Để Ŵ^Q, Ŵ^K, Ŵ^V là các trọng số chiếu được tái lắp ráp. Theo cách này, các truy vấn, khóa và giá trị được tái tạo có thể được công thức hóa là Q̃=quant(X̂)quant(Ŵ^Q), K̃=quant(X̂)quant(Ŵ^K), Ṽ=quant(X̂)quant(Ŵ^V). Sau đó chúng tôi tìm θ bằng cách giải quyết vấn đề là
arg min_θ ||Softmax(QK^⊤)V - Softmax(Q̃K̃^⊤)V̂||²_F,                     (6)
trong đó ||·||_F biểu thị chuẩn Frobenius. Để giải quyết vấn đề (6) một cách hiệu quả, chúng tôi sử dụng tìm kiếm lưới theo (Choukroun et al., 2019; Wei et al., 2023) (xem Thuật toán 1 trong Phụ lục để biết chi tiết).

4.2 HIỆU CHỈNH LỖI HIỆU QUẢ DỰA TRÊN GRADIENT

Dựa trên tái lắp ráp kênh thích ứng không gradient ở trên, một kỹ thuật hiệu chỉnh lỗi hiệu quả dựa trên gradient được đề xuất thêm để cải thiện hiệu suất của LLMs lượng tử hóa sử dụng một tập hợp nhỏ dữ liệu hiệu chuẩn.

Được truyền cảm hứng từ những phát triển gần đây trong các phương pháp tinh chỉnh hiệu quả tham số (Hu et al., 2022; Dettmers et al., 2023a), hiệu chỉnh lỗi hiệu quả giới thiệu hai tham số rank thấp A∈R^(M×r) và B∈R^(r×N) với rank r vào mỗi tầng chiếu của QLLM chúng tôi. Sau đó, chúng ta có thể thu được đầu ra Y của một tầng tuyến tính lượng tử hóa bằng Y = quant(X)quant(W) + quant(X)AB. Thay vì điều chỉnh trực tiếp các trọng số lượng tử hóa, chúng tôi học các tham số rank thấp được giới thiệu bằng cách tối thiểu hóa lỗi tái tạo giữa đầu ra ban đầu và lượng tử hóa của khối Attention-FFN. Nhờ số lượng tham số có thể huấn luyện giảm, cả chi phí tối ưu hóa và sử dụng bộ nhớ GPU đều có thể được giảm đáng kể. Việc tăng hiệu quả như vậy cho phép chúng tôi ngăn chặn thêm việc tích lũy lỗi lượng tử hóa trong quá trình lan truyền tiến thông qua tái tạo có cấu trúc, tức là thực hiện tái tạo đa khối cho QLLM, đồng thời điều chỉnh một tập hợp các khối Attention-FFN liên tiếp bằng cách tập trung vào việc tái tạo đầu ra khối cuối cùng.

Sau khi tái tạo, chúng tôi chỉ cần lưu trữ trọng số lượng tử hóa quant(W+AB), không gây ra chi phí suy luận bổ sung. Lưu ý rằng không thể tránh khỏi việc quá trình hấp thụ sẽ gây ra lỗi lượng tử hóa bổ sung. Để chống lại điều này, theo (He et al., 2017; Nagel et al., 2020; Hubara et al., 2020), chúng tôi thực hiện tái tạo tuần tự thay vì song song, cho phép chúng tôi tính đến lỗi lượng tử hóa xuất phát từ các tầng trước đó.

4.3 THẢO LUẬN VỀ HIỆU QUẢ

Hiệu quả tái lắp ráp. Tái lắp ráp kênh thích ứng của chúng tôi nổi bật về hiệu quả, chủ yếu do tính chất không gradient của nó, loại trừ nhu cầu lan truyền ngược. Nguồn chính của chi phí tính toán của phương pháp chúng tôi đến từ lắp ráp kênh, yêu cầu tính toán khoảng cách theo cặp. May mắn thay, việc sử dụng khớp mềm lưỡng phần hiệu quả loại bỏ nhu cầu tính toán khoảng cách cho mọi cặp kênh, tăng cường hiệu quả. Đối với hiệu chỉnh lỗi dựa trên gradient, số lượng tham số giảm làm giảm đáng kể chi phí tối ưu hóa của nó, khiến nó hiệu quả hơn việc điều chỉnh trực tiếp các trọng số lượng tử hóa.

Hiệu quả suy luận. Chi phí suy luận của tháo rời và lắp ráp kênh nhỏ vì hai lý do. 1) các nghiên cứu gần đây (Xiao et al., 2023; Wei et al., 2023) đã tiết lộ rằng các ngoại lệ kích hoạt thường tập trung trong các kênh cụ thể trên các đầu vào khác nhau. Tính chất này cũng được phản ánh trong các kênh tương tự để lắp ráp. Do đó, chúng tôi có thể tính toán trước các chỉ số kênh để tháo rời và lắp ráp sử dụng một số lượng nhỏ dữ liệu hiệu chuẩn, giảm đáng kể chi phí thời gian chạy. 2) Cả tháo rời kênh và lắp ráp đều có thể được thực hiện hiệu quả nếu tầng trước l-1 là tầng tuyến tính. Vui lòng tham khảo Phụ lục B để biết thêm chi tiết. Trong các trường hợp tầng trước l-1 là tầng phi tuyến, như chuẩn hóa tầng (Ba et al., 2016), chúng tôi giới thiệu các tầng tháo rời và lắp ráp bổ sung được thiết kế để phân tách và tổng hợp

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 1: So sánh hiệu suất của các phương pháp khác nhau cho lượng tử hóa trọng số và kích hoạt trên họ mô hình LLaMA-1. PPL biểu thị perplexity.

[Bảng chi tiết với kết quả so sánh hiệu suất của các phương pháp khác nhau...]

các kênh trong thời gian chạy, với các chỉ số kênh để phân tách và tổng hợp được tính toán ngoại tuyến sử dụng dữ liệu hiệu chuẩn. Mã giả của tháo rời và lắp ráp kênh trong thời gian chạy có thể được tìm thấy tại Phần D của tài liệu bổ sung. Hơn nữa, nhờ kernel hiệu quả của chúng tôi được thực hiện bởi Triton (Tillet et al., 2019) và tỷ lệ tái lắp ráp hạn chế được tìm kiếm bởi chiến lược thích ứng của chúng tôi (Xem Hình C), chi phí suy luận được giới thiệu được kiểm soát trong mức độ nhỏ.

5 THÍ NGHIỆM

Mô hình và tập dữ liệu. Chúng tôi áp dụng QLLM để lượng tử hóa các họ LLaMA-1 (Touvron et al., 2023a) và LLaMA-2 (Touvron et al., 2023b). Để đánh giá hiệu suất của LLM lượng tử hóa, chúng tôi báo cáo độ chính xác zero-shot trên các benchmark khác nhau, bao gồm PIQA (Bisk et al., 2020), ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), và WinoGrande (Sakaguchi et al., 2021). Ngoài ra, chúng tôi đánh giá perplexity, một chỉ báo quan trọng của hiệu suất tạo sinh của mô hình có tương quan đáng kể với kết quả zero-shot, trên WikiText2 (Merity et al., 2017), PTB (Marcus et al., 1993) và C4 (Raffel et al., 2020).

Cài đặt lượng tử hóa. Phù hợp với nghiên cứu trước đó (Dettmers et al., 2022; Shao et al., 2023), chúng tôi sử dụng lượng tử hóa trọng số theo kênh và lượng tử hóa kích hoạt theo token. Theo (Shao et al., 2023; Liu et al., 2023), chúng tôi lượng tử hóa tất cả trọng số và kích hoạt trung gian, ngoại trừ đầu ra xác suất Softmax, được duy trì ở độ chính xác đầy đủ. Theo OmniQuant (Shao et al., 2023), chúng tôi tập trung vào lượng tử hóa trọng số và kích hoạt 4- và 6-bit. Ngoài ra, chúng tôi cũng khám phá lượng tử hóa trọng số 4-bit và kích hoạt 8-bit, nhằm mục tiêu cấu hình thân thiện với phần cứng trong khi duy trì hiệu suất cao. Chúng tôi loại trừ lượng tử hóa 8-bit vì SmoothQuant (Xiao et al., 2023) có thể đạt được hiệu suất không mất mát.

Các phương pháp so sánh. Chúng tôi so sánh QLLM của chúng tôi với một số phương pháp lượng tử hóa PTQ tối tân (SOTA), như OmniQuant (Shao et al., 2023), SmoothQuant (SQ) (Xiao et al., 2023), Outlier Suppression+ (OS+) (Wei et al., 2023) và phương pháp QAT gần đây LLM-QAT (Liu et al., 2023). Để so sánh công bằng, chúng tôi tái tạo SmoothQuant và Outlier Suppression+ với lượng tử hóa trọng số theo kênh và lượng tử hóa kích hoạt theo token.

Chi tiết thực hiện. Theo OmniQuant (Shao et al., 2023), chúng tôi xây dựng tập hiệu chuẩn với 128 chuỗi được lấy mẫu ngẫu nhiên từ WikiText2, mỗi chuỗi có độ dài 2048. QLLM bắt đầu bằng việc áp dụng tái lắp ráp kênh trước tất cả các tầng chiếu tuyến tính, loại trừ tầng chiếu đầu ra attention, theo sau bởi việc thực hiện hiệu chỉnh lỗi trên mô hình kết quả. Rank r của các tham số rank thấp được giới thiệu được đặt thành 4, và các tham số này được huấn luyện trong 10 epoch với kích thước mini-batch là 1. Chúng tôi thực hiện tái tạo sử dụng 4 khối Attention-FFN. AdamW (Loshchilov & Hutter, 2019) với bộ lập lịch giảm tốc độ học tuyến tính được sử dụng theo (Yao et al., 2022). Tốc độ học được đặt thành 5×10⁻⁴ trong hầu hết các thí nghiệm; đối với LLaMA-2-70B, nó được đặt thành 1×10⁻⁴. Tất cả các thí nghiệm huấn luyện được thực hiện trên một GPU NVIDIA A100 80G duy nhất. Chúng tôi sử dụng hộp công cụ Language Model Evaluation Harness (Gao et al., 2021) để đánh giá.

5.1 KẾT QUẢ CHÍNH

Chúng tôi báo cáo kết quả trên các họ LLaMA-1 và LLaMA-2 trong Bảng 1 và Bảng A trong Phụ lục. Lưu ý rằng W6A6 có hỗ trợ phần cứng hạn chế trong các ứng dụng thực tế. Tuy nhiên, QLLM của chúng tôi vẫn chứng minh lợi ích hiệu suất trong những cài đặt này, liên tục vượt trội OmniQuant về perplexity thấp hơn trên tất cả các mô hình trên cả WikiText2 và C4 và đạt được độ chính xác tương đương trên 5 nhiệm vụ zero-shot. Đáng chú ý, với lượng tử hóa W4A8, phương pháp của chúng tôi chỉ phát sinh sự giảm hiệu suất tối thiểu. Trong khi những cải thiện hiệu suất tuyệt đối với lượng tử hóa 6-bit có thể có vẻ khiêm tốn, điều này một phần do tác động ít rõ rệt hơn của các ngoại lệ kích hoạt ở bit-width này. Khi tập trung vào lượng tử hóa bit-width cực thấp (tức là 4-bit), các ngoại lệ kích hoạt phục vụ như nút thắt cổ chai hiệu suất, từ đó làm nổi bật tầm quan trọng của việc ngăn chặn các ngoại lệ. Trong trường hợp này, QLLM của chúng tôi đạt được độ chính xác zero-shot cao hơn đáng kể và perplexity thấp hơn nhiều so với các đối thủ. Ví dụ, QLLM lượng tử hóa LLaMA-1-65B 4-bit vượt trội hơn đối tác OmniQuant trung bình 3.42% về độ chính xác trên năm nhiệm vụ zero-shot. Đáng chú ý, đối với LLaMA-7B, QLLM của chúng tôi thậm chí vượt trội hơn phương pháp QAT, LLM-QAT + SQ, 8.6% về độ chính xác trung bình, điều này chứng minh mạnh mẽ hiệu quả của QLLM chúng tôi.

[Bảng 2 và 3 với các kết quả so sánh khác...]

5.2 NGHIÊN CỨU PHÂN TÍCH

Tác động của các thành phần khác nhau trong tái lắp ráp kênh. Để cho thấy hiệu quả của các thành phần đa dạng liên quan đến tái lắp ráp kênh, chúng tôi áp dụng các phương pháp khác nhau với hiệu chỉnh lỗi hiệu quả của chúng tôi để tạo ra LLaMA-13B 4-bit và hiển thị kết quả trong Bảng 2. Đối với tháo rời kênh, chúng tôi xác định θ bằng cách khám phá các tỷ lệ mở rộng kênh γ khác nhau. Chúng tôi quan sát thấy rằng phương pháp của chúng tôi với tháo rời kênh vượt trội đáng kể so với đối tác không sử dụng nó. Với tỷ lệ mở rộng γ tăng, hiệu suất của mô hình lượng tử hóa có thể được cải thiện thêm.

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Những kết quả này cho thấy mạnh mẽ rằng tháo rời kênh có thể làm cho các kích hoạt thân thiện hơn với lượng tử hóa bằng cách phân tách các kênh ngoại lệ.

Hơn nữa, bằng cách kết hợp lắp ráp kênh, phương pháp của chúng tôi quản lý để bảo tồn số lượng kênh ban đầu với sự sụt giảm hiệu suất ít. So với cắt tỉa kênh, lắp ráp kênh của chúng tôi dẫn đến mất thông tin thấp hơn, từ đó đạt được hiệu suất tốt hơn nhiều, đặc biệt ở γ cao hơn. Thay vì xác định θ sử dụng tỷ lệ mở rộng được định nghĩa trước, phương pháp của chúng tôi, được trang bị với chiến lược thích ứng, có khả năng tự động tìm θ tối ưu, dẫn đến hiệu suất gần như không mất mát so với phương pháp chỉ sử dụng tháo rời kênh. Các tỷ lệ mở rộng kết quả cho các tầng khác nhau được hiển thị trong Hình C của Phụ lục.

Bảng 4: So sánh giữa hiệu chỉnh lỗi hiệu quả (EEC) và điều chỉnh trọng số lượng tử hóa trực tiếp (TQW) cho LLaMA-1-65B 4-bit. "OOM" biểu thị hết bộ nhớ.

[Bảng chi tiết về thời gian huấn luyện và bộ nhớ GPU]

Tác động của hiệu chỉnh lỗi hiệu quả dựa trên gradient. Sau tái lắp ráp kênh, chúng tôi triển khai QLLM của chúng tôi để tạo ra các mô hình LLaMA-7B 4-bit với hiệu chỉnh lỗi hiệu quả dựa trên gradient (EEC) và điều chỉnh trọng số lượng tử hóa trực tiếp (TQW) được nêu trong Phần 4.2 để cải thiện thêm hiệu suất của LLMs lượng tử hóa và hiển thị kết quả trong Bảng 4. So với TQW điều chỉnh tất cả trọng số lượng tử hóa, EEC tập trung vào việc học một tập hợp nhỏ trọng số rank thấp, giảm đáng kể chi phí huấn luyện và sử dụng bộ nhớ GPU trong khi mang lại hiệu suất tương đương. Hơn nữa, nhu cầu bộ nhớ GPU giảm cho phép EEC lượng tử hóa LLaMA-1-65B trên một GPU cấp tiêu dùng 24GB duy nhất, như NVIDIA RTX 4090, một nhiệm vụ không khả thi với TQW. Do trang hạn chế, chúng tôi đặt thêm kết quả trong Phần L của tài liệu bổ sung.

Hiệu quả suy luận. Để đánh giá hiệu quả suy luận của kỹ thuật tái lắp ráp kênh của chúng tôi, chúng tôi đo tốc độ suy luận của QLLM trên GPU NVIDIA RTX 3090. Chúng tôi sử dụng kernel W4A4 từ cơ sở mã QUIK (Ashkboos et al., 2023). Chúng tôi cũng thực hiện phân tích so sánh chỉ sử dụng lượng tử hóa trọng số, sử dụng kernel CUDA từ AutoGPTQ¹. Như được hiển thị trong Bảng 3, QLLM 4-bit của chúng tôi chỉ phát sinh 4% chi phí bổ sung so với W4A4 nhưng đạt được tăng tốc đáng kể 1.96× so với FP16. Đáng chú ý, chiến lược tái lắp ráp kênh của chúng tôi giảm thiểu đáng kể các mất mát do lượng tử hóa các ngoại lệ (xem Bảng E), chỉ với chi phí tính toán bổ sung nhỏ. Để biết chi phí suy luận chi tiết của tháo rời và lắp ráp kênh, vui lòng tham khảo Phần N của tài liệu bổ sung.

6 KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI

Trong bài báo này, chúng tôi đã đề xuất một phương pháp lượng tử hóa sau huấn luyện chính xác và hiệu quả cho LLMs bit thấp, được gọi là QLLM. Cốt lõi của QLLM chúng tôi nằm trong mô hình tái lắp ráp kênh thích ứng mới hiệu quả giải quyết các ngoại lệ kích hoạt, một yếu tố then chốt góp phần vào nút thắt cổ chai hiệu suất trong việc lượng tử hóa LLMs. Ý tưởng chính liên quan đến việc phân bổ lại độ lớn ngoại lệ cho các kênh khác, được thực hiện thông qua quá trình tháo rời kênh theo sau bởi lắp ráp. Chúng tôi đã đề xuất thêm một chiến lược tinh chỉnh hiệu quả nhận biết lượng tử hóa tận dụng dữ liệu hiệu chuẩn để bù đắp cho việc mất thông tin do lượng tử hóa gây ra. Các thí nghiệm mở rộng trên dòng mô hình LLaMA đã chứng minh hiệu suất đầy hứa hẹn và hiệu quả huấn luyện của QLLM. Về mặt hạn chế, tái lắp ráp kênh được đề xuất của chúng tôi liên quan đến việc giới thiệu các hoạt động bổ sung để phân tách và tổng hợp các kênh trong thời gian chạy, từ đó phát sinh chi phí suy luận bổ sung. Một giải pháp tiềm năng để cải thiện hiệu quả suy luận là khám phá hợp nhất kernel (Wang et al., 2010), nhằm hợp nhất tháo rời, lắp ráp và chuẩn hóa tầng thành một toán tử duy nhất. Cách khác là tổng hợp nhiều kênh tương tự hoặc không quan trọng hơn (Sun et al., 2023) so với những kênh được tháo rời để đạt được tăng tốc cao hơn.

¹https://github.com/PanQiWei/AutoGPTQ

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

LỜI CẢM ƠN
Chúng tôi chân thành cảm ơn Shenghu Jiang vì sự giúp đỡ trong việc triển khai kernel Triton hiệu quả.

TÀI LIỆU THAM KHẢO

[Danh sách tài liệu tham khảo đầy đủ với các nghiên cứu và bài báo khoa học]

--- TRANG 11-24 ---
[Phần phụ lục với các chi tiết kỹ thuật bổ sung, thuật toán, bảng kết quả mở rộng, và phân tích chi tiết hơn]
