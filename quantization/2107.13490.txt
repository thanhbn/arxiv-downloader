# 2107.13490.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/quantization/2107.13490.pdf
# File size: 1411361 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Adaptive Precision Training (AdaPT):
A dynamic quantized training approach for DNNs
Lorenz Kummer12Kevin Sidak13Tabea Reichmann14Wilfried Gansterer15
August 2021
Abstract
Quantization is a technique for reducing deep neural
networks (DNNs) training and inference times, which
is crucial for training in resource constrained environ-
ments or applications where inference is time critical.
State-of-the-art (SOTA) quantization approaches fo-
cus on post-training quantization, i.e., quantization of
pre-trained DNNs for speeding up inference. While
work on quantized training exists, most approaches
require renement in full precision (usually single
precision) in the nal training phase or enforce a
global word length across the entire DNN. This leads
to suboptimal assignments of bit-widths to layers
and, consequently, suboptimal resource usage. In
an attempt to overcome such limitations, we intro-
duce AdaPT, a new xed-point quantized sparsify-
ing training strategy. AdaPT decides about preci-
sion switches between training epochs based on in-
formation theoretic conditions. The goal is to de-
termine on a per-layer basis the lowest precision
that causes no quantization-induced information loss
while keeping the precision high enough such that
future learning steps do not suer from vanishing
gradients. The benets of the resulting fully quan-
tized DNN are evaluated based on an analytical per-
formance model which we develop. We illustrate
that an average speedup of 1 :27 compared to stan-
1Faculty of Computer Science, University of Vienna
2lorenz.kummer@univie.ac.at
3kevin.sidak@univie.ac.at
4tabea.reichmann@univie.ac.at
5wilfried.gansterer@univie.ac.atdard training in oat32 with an average accuracy in-
crease of 0:98% can be achieved for AlexNet/ResNet
on CIFAR10/100 and we further demonstrate these
AdaPT trained models achieve an average inference
speedup of 2 :33 with a model size reduction of 0 :52.
1 Introduction
With the general trend in machine learning towards
large model sizes to solve increasingly complex prob-
lems, inference in time critical applications or train-
ing under resource and/or productivity constraints is
becoming more and more challenging. Applications,
where time- and space ecient models are crucial,
include robotics, augmented reality, self driving ve-
hicles, mobile applications, applications running on
consumer hardware, or scientic research where a
high number of trained models is often needed for hy-
per parameter optimization. Accompanied by this,
already some of the most common DNN architec-
tures, like AlexNet [1] or ResNet [2], suer from over-
parameterization and overtting [3, 4]
Possible solutions to the aforementioned problems
include pruning (see, for example, [5, 6, 7, 8, 9]), or
quantization. While network pruning, which aims at
reducing the number of parameters in a given DNN
architecture (e.g. by sparsication of weights ten-
sors and using a sparse tensor format), is a success-
ful strategy for reducing network size and runtime,
it generally does not attempt to tailor parameter bit-
width to exploit advantages available in low bit-width
arithmetic, thus neglecting this potential to reduce
1arXiv:2107.13490v4  [cs.LG]  27 Aug 2021

--- PAGE 2 ---
computational resource consumption. We use prun-
ing in the form of sparsication but our focus lies on
making better use of quantization. When quantizing,
the precision of the parameters and of the computa-
tion is decreased and the resultingmore coarse bit-
width allows for more ecient use of computing re-
sources and for runtime reductions. However, quan-
tization has to be performed with caution, as naive
approaches or a too low bit-width can have a negative
impact on the accuracy of the network and its ability
to converge during training, which is unacceptable
for important use cases. For example, binary quan-
tization as proposed in [10] reduces memory require-
ments and can eectively speed up computation, as
multiplication can be performed as bit shifts, but the
accuracy suers slightly from this approach and con-
vergence is shown to much slower compared to oat32
training.
Existing quantization approaches do not fully
leverage the potential of quantized training. They
do not take the dierences quantization can have
on dierent layers during training into account,
nor dynamically raise or lower the precision used.
AdaPT extends these approaches and introduces a
new precision switching mechanism based on the
Kullback-Leibler-Divergence (KLD) [11], that calcu-
lates the average number of bits lost due to a pre-
cision switch. This is done not only for the entire
network, but on a per-layer basis, therefore consid-
ering dierent quantization eects on the dierent
layers, leading to AdaptivE Precision Training of
DNNs over time during training. Our approach does
not need any renement phase in full precision and
produces an already xed-point quantized network
that can then also be deployed on high-performance
application-specic integrated circuits (ASICs) or or
eld-programmable gate arrays (FPGAs) hardware.
1.1 Related Work
In general studies of the sensitivity of neural networks
(NNs), perturbation analysis has historically been ap-
plied with a focus on the theoretical sensitivity of sin-
gle neurons or multi-layer perceptrons [12, 13, 14],
but recently also yielded interesting results for the
sensitivity of simple modern architectures by provid-ing ecient algorithms for evaluating networks com-
parable to the complexity of LeNet-5 [15, 16]. While
these studies certainly improve our understanding of
the eect perturbation has on NNs, they lack practi-
cal applicability for quantized DNN training due to
beeing largely analytical in nature.
For exploring the accuracy degradation induced
by quantizations of weights, activation functions and
gradients, [17] and [18] introduced the frameworks
TensorQuant and QPyTorch , capable of simulating
the most common quantizations for training and
inference tasks on a oat32 basis. Both frame-
works allow to freely choose exponent and mantissa
for oating-point representation, word and fractional
bit length for xed-point representation and word
length for block-oating-point representation as well
as signed/unsigned representations. Since quantiza-
tion is only simulated in these frameworks, no run-
time speedup can be achieved on this basis.
Several approaches have tried to minimize infer-
ence accuracy degradation induced by quantizing
weights and/or activation functions while leverag-
ing associated performance increases. Quantization
Aware Training (QAT) [19, 20] incorporates simu-
lated quantization into model training and trains the
model itself to compensate the introduced errors. A
similar approach is take by Uniform Noise Injection
for Non-Uniform Quantization (UNIQ) [21], which
emulates a non-uniform quantizer to inject noise at
training time to obtain a model suitable for quantized
inference. Learned Quantization Nets (LQ-Nets) [22]
learn optimal quantization schemes through jointly
training DNNs and associated quantizers. The
Reinforcement-Learning Framework (ReLeQ) intro-
duced by [23] uses a reinforcement learning agent
to learn the nal classication accuracy w.r.t. the
bit-width of each of the DNNs layers to nd op-
timal layer to bit-width assignments for inference.
Dedicated quantization friendly operations are used
in [24]. A dierent approach is taken by Variational
Network Quantization (VNQ) [25] which uses varia-
tional dropout training [26] with a structured sparsity
inducing prior [27] to formulate post-training quan-
tization as the variational inference problem search-
ing the posterior optimizing the KLD. High-order en-
tropy minimization for neural network compression
2

--- PAGE 3 ---
(HEMP) [28] introduces a entropy coding-based regu-
larizer minimizing the quantized parameters entropy
in gradient based learning and pairs it with a sep-
arate pruning scheme [29] to reach a high degree of
model compression after training.
Machine learning frameworks such as PyTorch or
Tensorow already provide built-in quantization ca-
pabilities. These quantization methods focus on ei-
ther QAT or on post-training quantization [30, 31].
Both of these methods only quantize the model af-
ter the training and consequently provide no e-
ciency gain during training. Additional processing
steps during or after training are needed which add
computational overhead.
From a theoretical perspective, quantized training
has been investigated in [32] with a particular focus
on rounding methods and on convergence guarantees
while [33] provided an analysis of the behaviour of
the straight-through estimator (STE) [34] during
quantized training. For distributed training on multi-
node environments, Quantized Stochastic Gradient
Descent (QSGD) [35] incorporates a family of gra-
dient compression schemes aimed at reducing inter-
node communication occurring during SGD's gradi-
ent updates. This produces a signicant reduction
in runtime and network training can be guaranteed
to converge under standard assumptions [36]. Very
low bit width single-node training (binary, ternary
or similarly quantized weights and/or activations),
usually in combination with STE, has been shown to
yield non-trivial reductions in runtime and model size
as well but often either at comparably high costs to
model accuracy, decreased convergence speed or not
fully quantizing the network [10, 37, 38, 39, 40, 41]
and its most capable representative, Quantized ten-
sor train neural networks (QTTNet) [42], which com-
bines tensor decomposition and full low bit-width
quantization during training to reduce runtime and
memory requirements signicantly, still suers up to
2:5% accuracy degradation compared to it's respec-
tive baseline. A low bit-width integer quantized (re-
) training approach reporting accuracy drops of 1%
or less compared to a oat32 baseline was introduced
by [43], but it requires networks pre-trained in full
precision as starting points and it's explicitly stated
goal is reducing computational cost and model sizeduring inference. For speeding up training on a single
node via block-oating point quantization, [44] intro-
duced a dynamic training quantization scheme ( Multi
Precision Policy Enforced Training , MuPPET). Af-
ter quantized training it produces a model for oat32
inference.
1.2 Contributions
Existing quantized training and inference solutions
leave room for improvements in several areas. QAT,
LQ-Nets, ReLeQ, UNIQ, HEMP and VNQ are ca-
pable of producing networks such that inference can
be performed under quantization with small accu-
racy degradation (relative to baselines, which are not
comparable and where it is unclear how well they
are optimized), but require computationally expen-
sive oat32 training and the algorithms themselves
incur a certain overhead as well. QSGD training only
quantizes gradients and focuses on reducing commu-
nication overhead in multi-node environments. Bi-
nary, ternary or similarly quantized training has been
shown to commonly come at the cost of reduced ac-
curacy, reduced convergence speed or only quantiz-
ing weights and not activations and its most capa-
ble representative QTTNet still does not achieve iso-
accuracy.
MuPPET requires at least Nepochs forNquan-
tization levels (pre-dened bit-widths applied glob-
ally to all layers) because precision switches are only
triggered at the end of an epoch and the algorithm
needs to go through all precision levels. Potential
advantages from having dierent precision levels at
dierent layers of the network are not exploited. The
precision switching criterion is only based on the di-
versity of the gradients of the last kepochs, no metric
is used to measure the amount of information lost by
applying a certain quantization to the weights. Preci-
sion levels can only increase during training and never
decrease. Furthermore, MuPPET outputs a oat32
network s.t. inference hast to be done in expensive
full precision.
We advance the SOTA by providing an easy-to-use
solution for quantized training of DNNs by using an
information-theoretical intra-epoch precision switch-
ing mechanism capable of dynamically increasing and
3

--- PAGE 4 ---
decreasing the precision of the network on a per-layer
basis. Weights and activations are quantized to the
lowest bit-width possible without information loss ac-
cording to our heuristic and a certain degree of spar-
sity is induced while at the same time the bit-width
is kept high enough for further learning steps to suc-
ceed. This results in a network which has advantages
in terms of model size and time for training and infer-
ence. By training AlexNet and ResNet20 on the CI-
FAR10/100 datasets, we demonstrate on the basis of
an analytical model for the computational cost that
in comparison to a oat32-baseline AdaPT is compet-
itive in terms of accuracy and produces a non-trivial
reduction of computational costs (speedup). Com-
pared to MuPPET, AdaPT also has certain intrinsic
methodological advantages. After AdaPT training,
the model is fully quantized and sparsied to a cer-
tain degree s.t., unlike the case with MuPPET, which
outputs a oat32 model, AdaPT carries over it's ad-
vantages to the inference phase as well.
2 Background
2.1 Quantization
Numerical representation describes how numbers are
stored in memory (illustrated by g. 1) and how
arithmetic operations on those numbers are con-
ducted. Commonly available on consumer hardware
are oating-point and integer representations while
xed-point or block-oating-point representations are
used in high-performance ASICs or FPGAs. The nu-
merical precision used by a given numerical represen-
tation refers to the amounts of bits allocated for the
representation of a single number, e.g. a real num-
ber stored in oat32 refers to oating-point repre-
sentation in 32-bit precision. With these denitions
of numerical representation and precision in mind,
most generally speaking, quantization is the concept
of running a computation or parts of a computation
at reduced numerical precision or a dierent numeri-
cal representation with the intent of reducing compu-
tational costs and memory consumption. Quantized
execution of a computation however can lead to the
introduction of an error either through the quantizedrepresentations the machine epsilon mach being too
large (underow) to accurately depict resulting real
values or the representable range being too small to
store the result (overow).
Floating-Point Quantization The value vof a
oating point number is given by v=s
bp 1be
wheresis the signicand (mantissa), pis the pre-
cision (number of digits in s),bis the base and
eis the exponent [45]. Hence quantization using
oating-point representation can be achieved by re-
ducing the number of bits available for mantissa and
exponent, e.g. switching from a oat32 to a oat16
representation, and is oered out of the box by com-
mon machine learning frameworks for post-training
quantization[46, 47].
Integer Quantization Integer representation is
available for post-training quantization and QAT
(int8, int16 due to availability on consumer hard-
ware) in common machine learning frameworks [46,
47]. Quantized training however is not supported
due to integer quantized activations being not
meaningfully dierentiable, making standard back-
propagation inapplicable [33]. Special cases of integer
quantization are 1-bit and 2-bit quantization, which
are often referred to as binary and ternary quantiza-
tion in literature.
Block-Floating-Point Quantization Block-
oating-point represents each number as a pair
ofWL (word length) bit signed integer xand a
scale factor ss.t. the value vis represented as
v=xb swith baseb= 2 orb= 10. The scaling
factorsis shared across multiple variables (blocks),
hence the name block-oating point, and is typically
determined s.t. the modulus of the larges element is
2[1
b;1] [48]. Block-oating-point arithmetic is used
in cases where variables cannot be expressed with
sucient accuracy on native xed-point hardware.
Fixed-Point Quantization Fixed-point numbers
have a xed number of decimal digits assigned and
hence every computation must be framed s.t. the
4

--- PAGE 5 ---
Figure 1: Examples of oating-point, xed-point and
block-oating-point representations at dierent nu-
merical precisions. Sign (S), Exponent (E), Mantissa
(M) bits.
results lies withing the given boundaries of the rep-
resentation [49]. By denition of [50], a signed xed-
point numbers of world length WL =i+s+ 1 can
be represented by a 3-tuple hs;i;piwheresdenotes
whether the number is signed, idenotes the number
of integer bits and pdenotes the number of fractional
bits.
2.2 MuPPET
MuPPET is a mixed-precision DNN training algo-
rithm that combines the use of block-oating and
oating-point representations. The algorithm stores
two copies of the networks weights: a oat32 master
copy of the weights that is updated during backwards
passes and a quantized copy used for forward passes.
MuPPETs uses block-oating point representation as
outlined in sec. 2.1 with base b= 2 for quantization.
The precision level iof a layerlof all layers Lis
dened as
qi
l=D
WLnet;sweights
l;sact
lEi
withWLnetbeing global across the network, and
scaling factor s(for weights and activation functions)
varying per layer, determined each time precisionswitch is triggered. The scaling factor for a matrix
Xis given by
sweights;act
=$
log2min  
UB+ 0:5
Xfweights;actg
max;LB 0:5
Xfweights;actg
min!!%
withXfweights;actg
max;min describing the maximum or min-
imum value in the weights or feature maps matrix of
the layer. UBandLBdescribe the upper bounds
and lower bounds of the of the word length WLnet.
Its individual elements xare quantized:
xfweights;actg
quant =bxfweights;actg2sfweights;actg
+ Unif( 0:5;0:5)e
Unif (a;b) is the sampling from a uniform distribu-
tion in the interval [ a;b]. The parameters 0 :5 and
 0:5 to add to the bounds is chosen for maximum
utilisation of WLnet. During training the precision of
the quantized weights is increased using a precision-
switching heuristic based on gradient diversity [51],
where the gradient of the last mini-batch rfj
l(w) of
each layerl2Lat epochjis stored and after a cer-
tain number of epochs ( r), the inter-epoch gradient
diversity  sat epochjis computed by:
s(w)j=P
8l2LPj
k=j rkrfk
l(w)k2
2
kPj
k=j rrfk
l(w)k2
2
L
At epochjthere exists a set of gradient diversi-
tiesS(j) =fs(w)i8ei < jg(e denotes the
epoch in which it was switched into the quantiza-
tion scheme) of which the ratio p=maxS(j)
s(w)iis calcu-
lated. Ifpviolates a threshold for a certain number
of times, a precision switch is triggered. Speedups
claimed by MuPPET are based on an estimated per-
formance model simulating xed-point arithmetic us-
ing NVIDIA CUTLASS [52] for compatibility with
GPUs, only supporting oating and integer arith-
metic.
We chose MuPPET as baseline to compare AdaPT
against because of the examined related work for the
quantized training task, MuPPET comes closest to
the goal of iso-accuracy.
5

--- PAGE 6 ---
3 AdaPT
3.1 Quantization Friendly Initializa-
tion
Despite exhaustive literature research and to our
best knowledge, it has not yet been explored how
weights initialization as counter-strategy for van-
ishing/exploding gradients impacts quantized train-
ing. However the preliminary experimental results
of our rst investigation of the impact of dierent
weights initialization strategies showed the resilience
of DNNs trained under a xed forwards pass in-
teger quantization scheme (int2, int4, int8, int16)
with oat32 master copies for gradient computations
correlates strongly with initializer choice (g. 2).
Using Adam [53] as optimizer, we trained LeNet-
5 on MNIST/FMNIST [54] and AlexNet on CI-
FAR10/100 [55] and examined the degree to which
the quantized networks are inferior in accuracy
compared to baseline networks trained in oat32
dependent on initializer choice (Random Normal,
Truncated Normal, Random Uniform, Glorot Nor-
mal/Uniform [56], He Normal/Uniform [57], Variance
Scaling [58], Lecun Normal/Uniform [59, 60]) and ini-
tializer parameters. We found that DNNs initialized
by fan-in truncated normal variance scaling (TNVS)
degrade least under quantized training with a xed
integer quantization scheme as described above. This
correlates with results published by [61] who intro-
duced a learnable scale parameters in their asymmet-
ric quantization scheme to achieve more stable train-
ing. We thus initialize networks with TNVS and an
empirically chosen scaling factor swherenlis the
number of input units of a weights tensor Wlbefore
quantized training with AdaPT.
WlN 
=rs
nl;= 0;=r
3s
nl!
The examination of how other counter strategies
(eg. gradient amplication, [62], gradient normaliza-
tion [63], weights normalization [64]) aect quantized
training remains an open question.
Figure 2: Correlation of initializer choice, hyperpa-
rameters (HP), quantizer, layer, training and val-
idation accuracy, duration of training (epochs) for
LeNet5 on MNIST and FMNIST datasets
3.2 Precision Levels
AdaPT uses xed-point quantization as dened in
sec. 2.1 because unlike block oating point as used by
MuPPET with a global word length across the whole
network and per-layer shared exponent, we conjec-
ture that optimal word and fractional lengths are lo-
cal properties of each layer under the hypothesis that
dierent layers contain dierent amounts of informa-
tion during dierent points of time in training. We
6

--- PAGE 7 ---
decided against using oating-point quantization be-
cause xed-point gives us better control over numer-
ical precision and is available in high performance
ASICs which are our target platform and against in-
teger quantization, particularly its extreme forms bi-
narization and ternarization, because our goal is iso-
accuracy which literature shows is dicult to reach
with such coarse quantization. In principle however,
the AdaPT concept could be extended to represen-
tations other than xed-point. Furthermore, AdaPT
employs stochastic rounding, which in combination
with a xed-point representation has been shown to
consistently outperform nearest-rounding by [50], for
quantizing oat32 numbers. Given that AdaPT is ag-
nostic towards whether a number is signed or not, we
represent the precision level of each l2Lsimply as
WLl;FLl
whereby fractional length FLldenotes
the number of fractional bits. For a random number
P2[0;1],xis stochastically rounded by
SR(x) =(
bxc;ifPx bxc
mach
bxc+ 1;ifP <x bxc
mach
3.3 Precision Switching Mechanism
Precision switching in quantized DNN training is the
task of carefully balancing the need to keep precision
as low as possible in order to improve runtime and
model size, yet still keep enough precision for the net-
work to keep learning. In AdaPT, we have encoded
these opposing interests in two operations, the Push-
Down Operation and the PushUp Operation.
The PushDown Operation Determining the
amount of information lost if the precision of the
xed-point representation of a layer's weight tensor is
lowered, can be heuristically accomplished by inter-
preting the precision switch as a change of encoding.
Assume a weights tensor WlQlof layerl2Lwith
its quantized counterpart cWlPl, wherePl;Ql
are the respective distributions. Then the continu-
ous Kullback-Leibler-Divergence [11] (1) represents
the average number of bits lost through changing the
encoding of lfromQltoPl, withpandqdenoting
probabilities, and wthe elements of the weights ten-sor:
D(PlkQl) =Z1
 1pl(w)logpl(w)
ql(w)dw
Using discretization via binning, we obtain Pland
Qlat resolution rlthrough the empirical distribution
function:
cFrl(w) =1
rl+ 1rlX
i=11Wl
iw (1)
that can then be used in the discrete Kullback-
Leibler-Divergence (2).
KL(PlkQl) =X
w2WlPl(w)logPl(w)
Ql(w)(2)
Using a bisection approach, AdaPT eciently nds
the smallest quantization
WLl
min;FLl
min
ofWl
s.t.KL(PlkQl) = 08l2L
The PushUp Operation However, determining
the precision of the xed-point representation of Wl
at batchj, s.t. the information lost through quan-
tization is minimal but there is still sucient preci-
sion for subsequent batches j+ 1 to keep learning, is
a non-trivial task. Solely quantizing Wlat the be-
ginning of the training to a low precision xed-point
representation (e.g.
WLl;FLl
=h8;4i) would re-
sult in the network failing to learn, because at such
low precision levels, gradients would vanish very early
on in the backwards pass. Hence, AdaPT tracks for
each layer a second heuristic over the last jbatches
to determine how much precision is required for the
network to keep learning. If gradients are quantized,
a gradient diversity based heuristic is employed (3),
(4).
s(w)l
j=Pj
k=j rkrfl
k(w)k2
kPj
k=j rrfl
k(w)k2(3)
~s(w)l
j=(
logs(w)l
jif 0<s(w)l
j<1
1 otherwise
If ~s(w)l
j>0, two suggestions for an increase in pre-
cision are computed, sl
1=max(d1
logs(w)l
j 1e;1) and
7

--- PAGE 8 ---
sl
2=max(min(32log2s(w)l
j 1;32) FLl
min;1)
and the nal suggestion is computed dependent on a
global strategy stvia
sl=8
><
>:min(sl
1;sl
2) if st = min
d0:5(sl
1+sl
2)eif st = mean
max(sl
1;sl
2) if st = max(4)
Otherwise, i.e. ~ s(w)l
j>0,sl= 1. The
new xed-point quantization of layer lis then ob-
tained by FLl= (min(FLl
min+sl;32),WLl=
min(max(WLl
min;FLl
min) + 1;32)).
Dealing with Fixed-Points Limited Range As
outlined in sec. 2.1, xed-point computations must
be framed s.t. results t within the given bound-
aries. We approach this by adding a number of buer
bitsbuff to every layers word-length, i.e. at the
end of PushUp, FLl= (min(FLl
min;32 buff ),
WLl=max(min(FLl
min+buff; 32);WLl
min). Ad-
ditionally, we normalize gradients to limit weight
growth and reduce chances of weights weights be-
coming unrepresentable in the given precision after
an update step.
rfl(w) =rfl(w)
krfl(w)k2
Strategy, Resolution and Lookback For adapt-
ing the strategy stmentioned in (4), we employ a
simple loss-based heuristic. First we compute the av-
erage lookback over all layers lbavg=jLj 1PjLj
i=0lbl
and average lossLavg=jLj 1Plbavg
i=0Liover the last
lbavgbatches. Then via (5), the strategy is adapted.
st=8
><
>:max ifjLavgjjLijand st = 'mean'
mean ifjLavgjjLijand st = 'min'
min ifjLavgj>jLjj
Because the number of gradients collected for each
layer aects the result of the gradient diversity based
heuristic (3), (4), we introduce a parameter lookback
lblbounded by hyperparameters lblwrlbllbuprwhich is estimated at runtime. First, lbnewis com-
puted:
lbl
new
=(
min(max(dlbupr
s(w)l
je;lblwr);lbupr) if 0<s(w)l
j
lbupr otherwise
Then, to prevent jitter, a simple momentum is ap-
plied to obtain the updated lbl=dlbl
new+ (1 )
lblewith2[0;1].
Similarly, the number of bins used in the dis-
cretization step (1) aects the result of the dis-
crete Kullback-Leibler-Divergence (2). We control
the number of bins via a parameter referred to as res-
olutionrl, which is derived at runtime and bounded
by hyperparameters rlwrrlrupr.
rl=(
min(max(rl+ 1;rlwr);rupr) iflbl=lbupr
min(max(rl 1;rlwr);rupr) iflbl=lblwr
(5)
3.4 AdaPT-SGD (ASGD)
Although AdaPT can in principle be combined with
any iterative gradient based optimizer (e.g. Adam),
we chose to implement AdaPT with Stochastic Gradi-
ent Descent (SGD), because it generalizes better than
adaptive gradient algorithms [65]. The implementa-
tion of AdaPT employs the precision switching mech-
anism and numeric representation described in sec. 3,
by splitting it in two operations: the PushDown Op-
eration (alg. 3), which, for a given layer, nds the
smallest xed-point representation that causes no in-
formation loss, and the PushUp Operation (alg. 4),
which, for a given layer, seeks the precision that is
required for the network to keep learning. The in-
tegration of these two operations into the precision
switching mechanism is depicted in alg. 2, and the
integration into SGD training process is depicted in
alg. 1.
Inducing Sparsity In addition to the AdaPT pre-
cision switching mechanism, we used an L1 sparsify-
ing regularizer [66, 67, 68] to obtain sparse and partic-
ularly quantization-friendly weight tensors, combined
8

--- PAGE 9 ---
linearly with L2 regularization for better accuracy in
a similar way as proposed by [69]. Additionally us-
ing a technique similar to [5], we penalize learning
steps that lead to increased word-length or decreased
sparsity by:
P=WLl
32spl
wheresplis the percentage of non-zero elements of
layerl. Thus loss in ASGD is computed by:
bL(Wl) =L+Wl
1+
2Wl2
2+P
Data: Untrained Float32 DNN
Result: Trained and Quantized DNN
1bL= InitFixedPointTNVS()
2Q= InitQuantizationMapping( bL)
3L= Float32Copy( bL)
4forEpoch in Epochs do
5 forBatch in Batches do
6bG,L= ForwardPass( bL,Batch )
7 Q= PrecisionSwitch( bG,L,Q,L)
8 SGDBackwardsPass( L,bG)
9 forl2Ldo
10 bL[l] = Quantize( L[l],Q[l][quant ])
11 end
12 end
13end
Algorithm 1: AdaPT-SGD
Algorithm When AdaPT-SGD (alg. 1) is started
on an untrained oat32 DNN, it rst initializes the
DNNs layers (denoted as bL) weights with TNVS (alg.
1, ln. 1). Next the quantization mapping Qis initial-
ized, which assigns each l2La tuple
WLl;FLl
, a
lookbacklbland a resolution rl(alg. 1, ln. 2). Then
a oat32 master copy LofbLis created (alg. 1, ln.
3). During training for each forward pass on a batch,
quantized gradients bGand lossLare computed with
a forward pass using quantized layers bL(alg. 1, ln.
4-6). The precision switching mechanism described
in sec. 3 is then called (alg. 1, ln. 7) and after
adapting the push up strategy as described in sec.Data:bG,L;Q;L
Result: Q
1AdaptStrategy(L)
2forl2Ldo
3Q[l][grads ]:append (bG[l])
4 AdaptLookback( Q[l][grads ];Q[l][lb])
5 AdaptResolution( Q[l][lb];Q[l][res])
6 ifjQ[l][grads ]jQ[l][lb]then
7WLl,FLl=Q[l][quant ]
8WLl
min,FLl
min= PushDown( L[l],
WLl,FLl)
9WLl
new,FLl
new= PushUp( l,WLl,
FLl,WLl
min,FLl
min)
10 Q[l][quant ] =WLl
new,FLl
new
11 end
12end
Algorithm 2: PrecisionSwitch
3, it iterates over l2L(alg. 2 ln. 2), rst adapt-
ing resolution rland lookback lbl(alg. 2 ln. 4-5)
and then executing PushDown and PushUp on layer
l(alg. 2 ln. 6 -10) to update
WLl;FLl
2Q. When
Data:L,WLl,FLl
Result:WLl
min,FLl
min
1WLl
min,FLl
min=L,WLl,FLl
2repeat
3WLl
min,FLl
min= Decrease( WLl
min,
FLl
min)
4bL= Quantize( L,WLl
min,FLl
min)
5untilKL(EDF (Li);EDF (bL))<
Algorithm 3: PushDown Operation
PushDown is called on l, it decreases the quantization
mapping in a bisectional fashion until KLindicates
amore coarse quantization would cause information
loss at resolution rl(3, ln. 1-5). After computing
the most coarse quantization
WLl
min;FLl
min
2Q
not causing information loss in l, PushUp is called
onlto increase the quantization to the point where
the network is expected to keep learning, based on
gradient diversity of the last lblbatches (alg. 4, ln.
1-6). After PushUp the PrecisionSwitch returns an
9

--- PAGE 10 ---
Data:L,WLl,FLl,WLl
min,FLl
min
Result:WLl
new,FLl
new
1Compute \s(L) usingWLl
min,WLl
min
2Compute  s(L) usingWLl,FLl
3while s(L)\s(L)do
4WLl
min,FLl
min= Increase( WLl
min,
FLl
min)
5 Compute \s(L) usingWLl
min,FLl
min
6end
Algorithm 4: PushUp Operation
updated Qto the training loop, and a regular SGD
backward pass updates the oat32 master copy L, us-
ing quantized gradients bG(alg. 1, ln. 7,8). Finally,
the now updated weights Lare quantized using the
updated Qand written back to bLto be using in the
next forward pass (alg. 1, ln. 9-11).
4 Experimental Evaluation
4.1 Setup
For experimental evaluation of AdaPT, we trained
AlexNet and ResNet20 on the CIFAR-10/100
datasets with reduce on plateau learning rate (ROP)
scheduling which will reduce learning rate by a given
factor if loss has not decreased for a given num-
ber of epochs [46]. Due to unavailability of xed-
point hardware, we used QPyTorch to simulate xed
point arithmetic and our own performance model
(sec. 4.1.2) to simulate speedups, model size reduc-
tions and memory consumption. Experiments were
conducted on an Nvidia DGX-1. It has 8 x Tesla
V100 GPUs, which is capable of integer and oating-
point arithmetic [70].
4.1.1 Hyper Parameters
All layersl2Lwere quantized with
WLl;FLl
=
h8;4iat the beginning of AdaPT training. Other hy-
perparameters specic to AdaPT were set to rlwr=
50,rupr= 150,lblwr= 25,lbupr= 100, lookback
momentum = 0:33 for all experiments, buer bitswas set tobuff = 4 for training AlexNet on CIFAR10
andbuff = 8 for training ResNet and AlexNet on CI-
FAR100. Hyperparameters unspecic to AdaPT ( lr,
L1decay,L2decay,ROPpatience ,ROPthreshold batch
size,accumulation steps ) were selected using grid
search and 10-fold cross-validation and we refer to
our code repository1for the exact conguration les
of each experiment.
4.1.2 Performance Model
Speedups and model size reductions were computed
using a performance model taking forward and back-
ward passes, as well as batch sizes, gradient accu-
mulations numerical precision and sparsity, into ac-
count. Our performance model computes per layer
operations (MAdds, subsequently referred to as ops),
weights them with the layer's world length and a
tensors percentage of non-zero elements at a spe-
cic stage in training to simulate quantization and
a sparse tensor format, and aggregates them to ob-
tain the overall incurred computational costs of all
forwards and backwards passes. Additionally, the
performance model estimates AdaPTs overhead for
each layers lpush up operation puland push down
operationpdlby
opsl
pd;i2log2(32 8)rl
i3Y
dim2ldim (6)
opsl
pu;i(lbi+ 1)Y
dim2ldim+ 1 (7)
Using (6), (7) and the simplifying assumption that a
backwards pass incurs as many operations as a quan-
tized forwards pass but is conducted in full precision
i.e. 32 bits word length with non-sparse gradients,
AdaPTs training costs are then bounded by
coststrainnX
i=1jLjX
l=1opsl
spl
iWLl
i+32
accs
(8)
and AdaPTs overhead is bounded by
costsAdaPTnX
i=1jLjX
l=132spl
iopsl
pd;i+opsl
pu;i
accslbl
i(9)
1https://gitlab.cs.univie.ac.at/sidakk95cs/marvin2
10

--- PAGE 11 ---
wherenis the number of training steps, Lare the net-
works layers and WLl
iis thel thslayers word length
at training step iandspl
iis the percentage of non-zero
elements of layer lat stepi. Using (8), (9), we obtain
total costs via costs =costtrain+costsAdaPT and fur-
ther the speedup of our training approach compared
to MuPPET or a oat32 baseline incorporating batch
sizebs, gradient accumulation steps accs and compu-
tational costs costs is thus obtained via
SU=bsothercostsother
bsourscostsours
whereby naturally we exclude AdaPTs overhead
when computing costsother. Models size reductions
SZwere calculated by rst computing individual
model sizes sz
sz=jLjX
l=1spl
iWLl
i
withi=nand then forming the quotient SZ=
szother=szours. Similarly, average memory consump-
tion during training MEM =memother=memoursis
formed by the quotient of each models memory con-
sumptionmem
mem =0
@nX
i=1jLjX
l=1 
spl
iWLl
i+ 321
A1
n
Becauseszandmem ignore tensor dimensions, they
can not be interpreted as absolute values. However
given that in the quotients MEM andSZthe eects
of tensor dimensions would cancel out when compar-
ing identical architectures (i.e. oat32 AlexNet and
quantized AlexNet), these values provide valid rel-
ative measures for memory consumption and model
size under this constraint.
4.2 Results
4.2.1 Training
Tab. 1 shows the top-1 validation accuracies and
accuracy dierences achieved by AdaPT and MuP-
PET for both quantized training on CIFAR100, and aoat32 basis. Tab. 2 shows results for training on CI-
FAR10. As can be seen in the tables, AdaPT is capa-
ble of quantized training not only to an accuracy com-
parable to oat32 training but even reaches or sur-
passes iso-accuracy in all examined cases. The cho-
sen experiments furthermore illustrate that AdaPT
delivers this performance independent of the under-
lying DNN architecture or the dataset used. Fig. 3
CIFAR100
Float32 Quantized 
AlexNet AdaPT 41.3100
512 42.4100
512 1.1
AlexNet MuPPET 39.2150
128 38.2?
128 -1.0
ResNet AdaPT 64.3100
512 65.2100
512 0.9
ResNet MuPPET 64.6150
128 65.8?
128 1.2
Table 1: Top-1 accuracies, AdaPT (ours) vs MuP-
PET, CIFAR10, 100 to 150 epochs. Float 32 indi-
cates 32-bit oating-point training (baseline), Quan-
tized indicates variable bit x-point (AdaPT) or
block-oating-point (MuPPET) quantized training,
subscript indicates batch size used, superscript indi-
cates epochs used
CIFAR10
Float32 Quantized 
AlexNet AdaPT 73.1100
512 74.5100
512 1.4
AlexNet MuPPET 75.5150
128 74.599
128 -1.0
ResNet AdaPT 89.5100
512 90.0100
512 0.5
ResNet MuPPET 90.1150
128 90.9114
128 0.8
Table 2: Top-1 accuracies, AdaPT (ours) vs MuP-
PET, CIFAR100
and 4 display word length usage over time for individ-
ual layers for ResNet20 and AlexNet on CIFAR100.
In both cases, we observe that individual layers have
dierent precision preferences dependent on progres-
sion of the training, an eect that is particularly pro-
nounced in AlexNet. For ResNet20, the word length
interestingly decreased notably in the middle of train-
ing and later decreased, which we conjecture is at-
tributable to sparsifying L1 regularization as well as
the wordlength/sparsity penalty introduced in sec.
3 leading to a reduction in irrelevant weights, that
11

--- PAGE 12 ---
would otherwise oset the KL heuristic used in the
PushDown Operation. Tabs. 4 and 3 show AdaPTs
Figure 3: Wordlengths (bit) of ASGD optimized
ResNet20 on CIFAR100, 100 epochs, Linear (L),
Convolutional (C), Downsampling (D) layers
CIFAR10
MEM SU1SU2SU3
AlexNet AdaPT 1.47 1.42 2.37 6.42
AlexNet MuPPET 1.66 ? ? 1.2
ResNet AdaPT 1.63 1.20 1.49 4.01
ResNet MuPPET 1.61 ? ? 1.25
Table 3: Memory Footprint, Final Model Size, Mem-
ory Footprint, Speedup, AdaPT (ours) vs MuPPET
on respective baseline oat32 training on CIFAR10.
SU1: our baseline, our performance model, SU2: our
baseline, our performance model adjusted for iso-
accuracy, SU3: MuPPET baseline, our performance
model
CIFAR100
MEM SU1SU2SU3
AlexNet AdaPT 1.41 1.32 1.57 5.23
ResNet AdaPT 1.66 1.13 1.62 3.36
Table 4: Memory Footprint, Final Model Size, Mem-
ory Footprint, Speedup, AdaPT (ours) vs MuPPET,
CIFAR100, training
Figure 4: Wordlengths (bit) of ASGD optimized
AlexNet on CIFAR100, 100 epochs
Figure 5: Sparsity of ASGD optimized AlexNet on
CIFAR100, 100 epochs
estimated speedups vs. our oat32 baseline (100
epochs, same number of gradient accumulation steps
and batch size as AdaPT trained models) and MuP-
PETs oat32 baseline. AdaPT achieves speedups
comparable with SOTA solutions on our own baseline
and outperforms MuPPET on MuPPETs baseline in
every scenario. Unfortunately, MuPPETs code base
could not be executed so we were unable to apply
MuPPET to the more ecient AdaPT baseline and
were limited to comparing against results provided
by the original authors. We used our performance
12

--- PAGE 13 ---
model for simulating MuPPETs performance based
on the precision switches stated in the MuPPET pa-
per because MuPPETs authors did not publish their
performance model. An overview over the reduction
of computational costs through ASGD relative to a
oat32 baseline is provided by g .8 as well Fig. 5
Figure 6: Sparsity of ASGD optimized ResNet on CI-
FAR100, 100 epochs
and g. 6 illustrate the induction of sparsity dur-
ing AdaPT training. Interestingly in some cases, we
observe an increasing degree of sparsity as AdaPT
training progresses, with some layers reaching 80%
sparsity or more at the end of training as is the case
with AlexNet trained on CIFAR100, while less spar-
sity could be induced in fewer layers training ResNet
on CIFAR10/100. As can be seen in tab. 5, AdaPT
induces most sparsity in AlexNet, with a 45% nal
model sparsity and an average intra-training sparsity
of 34% for training on CIFAR100. In the residual
architecture ResNet, we observe that AdaPT intro-
duces 7% sparsity in the nal model and 3% intra-
training sparsity. As g. 6 illustrates increasing spar-
sity towards the end training, we conjecture that for
ResNet-like architectures, the sparsity inducing eect
could be more pronounced if training is conducted for
a higher number of epochs.
A side eect illustrated by g. 7 and tab. 4 and
tab. 2 of AdaPT for reducing the computational
cost of network training (g. 8) is it's increased
intra-training memory consumption that is caused bySparsity
Final Model Average
AlexNet CIFAR10 0.26 0.27
ResNet CIFAR10 0.07 0.04
AlexNet CIFAR100 0.44 0.35
ResNet CIFAR100 0.07 0.03
Table 5: Final model sparsity and average intra-
training sparsity for AdaPT Training
Figure 7: ASGD Memory Consumption relative to
oat32 SGD
Figure 8: ASGD computational cost relative to
oat32 SGD
13

--- PAGE 14 ---
Inference
SZ SU
AlexNet CIFAR10 0.54 3.56
ResNet CIFAR10 0.60 1.63
AlexNet CIFAR100 0.36 2.60
ResNet CIFAR100 0.57 1.52
Table 6: Inference with AdaPT trained models
AdaPT maintaining a complete oat32 mastercopy of
the models weights tensors used for backwards pass
updates. However this eect is only present during
training as the master-copies are discarded once the
model is deployed and used for inference.
4.2.2 Inference
Given AdaPT trained networks are fully quantized
and sparsied, they have advantage that extends be-
yond the training phase into the inference phase.
Tab. 6 shows that inference speed ups for AdaPT
trained networks range from 1 :63 to 3:56. The speed
ups achieved during the inference phase are even
higher than those achieved during training because
during inference, no expensive backwards pass has to
be conducted and no overhead induced by AdaPT is
occurring. This is a non-trivial advantage over MuP-
PET which does not provide any post-training advan-
tages in terms of speed up or memory consumption
at all due to the resulting network being oat32.
5 Conclusion
With AdaPT we introduce a novel quantized DNN
sparsifying training algorithm that can be com-
bined with iterative gradient based training schemes.
AdaPT exploits fast xed-point operations for for-
ward passes, while executing backward passes in
highly precise oat32. By carefully balancing the
need to minimize the xed-point precision for max-
imum speedup and minimal memory requirements,
while at the same time keeping precision high enough
for the network to keep learning, our approach pro-
duces top-1 accuracies comparable or better than
SOTA oat32 techniques or other quantized train-ing algorithms. Furthermore AdaPT has the intrinsic
methodical advantage of not only training the net-
work in a quantized fashion, but also the trained net-
work itself is quantized and sparsied s.t. it can be
deployed to fast ASIC hardware at reduced memory
cost and executed with reduced computational com-
pared to a full precision model. Additionally we con-
tribute a performance model for xed point quantized
training.
6 Future Work
AdaPT as presented employs a xed-point repre-
sentation to enable ne granular precision switches.
However, xed-point hardware is not as common as
oating-point hardware, so we plan to extend the
concept to oating point quantization s.t. AdaPT
becomes compatible with oat16/oat32 consumer
hardware. Further, we intend to explore whether
AdaPT can be used to generate very low bit-width
(binarization/ternarization) networks through grad-
ually reducing the quantization search space during
training. Additionally, we conjecture the heuristics
used by AdaPT can be used for intra-training DNN
pruning as well, which will be subject to future re-
search. We plan ablation testing to reduce the com-
plexity of AdaPT. Another interesting application of
AdaPT which we intend to explore is it's usefulness
in other elds of research such as Drug Discovery.
7 Acknowledgements
We thank Prof. Torsten M oller for sponsoring the
DGX-1 GPU Cluster used in our experiments and we
thank Prof. Sebastian Tschiatschek for feedback and
useful discussions. Both were sta of the University
of Vienna's Faculty of Computer Science at the time
of writing.
References
[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton,
\Imagenet classication with deep convolutional
14

--- PAGE 15 ---
neural networks," in Advances in neural infor-
mation processing systems , pp. 1097{1105, 2012.
[2] K. He, X. Zhang, S. Ren, and J. Sun, \Deep
residual learning for image recognition," in Pro-
ceedings of the IEEE conference on computer vi-
sion and pattern recognition , pp. 770{778, 2016.
[3] Z. Allen-Zhu, Y. Li, and Z. Song, \A con-
vergence theory for deep learning via over-
parameterization," in Proceedings of the 36th
International Conference on Machine Learning
(K. Chaudhuri and R. Salakhutdinov, eds.),
vol. 97 of Proceedings of Machine Learning Re-
search , pp. 242{252, PMLR, 09{15 Jun 2019.
[4] M. Li, M. Soltanolkotabi, and S. Oymak, \Gra-
dient descent with early stopping is provably ro-
bust to label noise for overparameterized neural
networks," in Proceedings of the Twenty Third
International Conference on Articial Intelli-
gence and Statistics (S. Chiappa and R. Calan-
dra, eds.), vol. 108 of Proceedings of Machine
Learning Research , pp. 4313{4324, PMLR, 26{
28 Aug 2020.
[5] A. Gordon, E. Eban, O. Nachum, B. Chen,
H. Wu, T.-J. Yang, and E. Choi, \Morphnet:
Fast & simple resource-constrained structure
learning of deep networks," in Proceedings of the
IEEE conference on computer vision and pattern
recognition , pp. 1586{1595, 2018.
[6] F. N. Iandola, S. Han, M. W. Moskewicz,
K. Ashraf, W. J. Dally, and K. Keutzer,
\Squeezenet: Alexnet-level accuracy with 50x
fewer parameters and< 0.5 mb model size," arXiv
preprint arXiv:1602.07360 , 2016.
[7] S. Han, H. Mao, and W. J. Dally, \Deep com-
pression: Compressing deep neural network with
pruning, trained quantization and human cod-
ing," in 4th International Conference on Learn-
ing Representations, ICLR 2016, San Juan,
Puerto Rico, May 2-4, 2016, Conference Track
Proceedings (Y. Bengio and Y. LeCun, eds.),
2016.[8] M. Lis, M. Golub, and G. Lemieux, \Full deep
neural network training on a pruned weight bud-
get," in Proceedings of Machine Learning and
Systems (A. Talwalkar, V. Smith, and M. Za-
haria, eds.), vol. 1, pp. 252{263, 2019.
[9] R. Stewart, A. Nowlan, P. Bacchus, Q. Ducasse,
and E. Komendantskaya, \Optimising hardware
accelerated neural networks with quantisation
and a knowledge distillation evolutionary algo-
rithm," Electronics , vol. 10, no. 4, 2021.
[10] M. Courbariaux, I. Hubara, D. Soudry, R. El-
Yaniv, and Y. Bengio, \Binarized neural net-
works: Training deep neural networks with
weights and activations constrained to +1 or -
1," 02 2016.
[11] S. Kullback and R. A. Leibler, \On informa-
tion and suciency," The annals of mathemati-
cal statistics , vol. 22, no. 1, pp. 79{86, 1951.
[12] K. Wang and A. N. Michel, \Robustness and
perturbation analysis of a class of articial neu-
ral networks," Neural Networks , vol. 7, no. 2,
pp. 251{259, 1994.
[13] A. Meyer-Base, \Perturbation analysis of a
class of neural networks," in Proceedings of
International Conference on Neural Networks
(ICNN'97) , vol. 2, pp. 825{828 vol.2, 1997.
[14] Xiaoqin Zeng and D. S. Yeung, \Sensitivity anal-
ysis of multilayer perceptron to input and weight
perturbations," IEEE Transactions on Neural
Networks , vol. 12, no. 6, pp. 1358{1366, 2001.
[15] L. Xiang, X. Zeng, Y. Niu, and Y. Liu, \Study
of sensitivity to weight perturbation for con-
volution neural network," IEEE Access , vol. 7,
pp. 93898{93908, 2019.
[16] Y. LeCun, L. Bottou, Y. Bengio, and P. Haner,
\Gradient-based learning applied to document
recognition," Proceedings of the IEEE , vol. 86,
no. 11, pp. 2278{2324, 1998.
[17] D. M. Loroch, F.-J. Pfreundt, N. Wehn, and
J. Keuper, \Tensorquant: A simulation toolbox
15

--- PAGE 16 ---
for deep neural network quantization," in Pro-
ceedings of the Machine Learning on HPC Envi-
ronments , pp. 1{8, 2017.
[18] T. Zhang, Z. Lin, G. Yang, and C. De Sa,
\Qpytorch: A low-precision arithmetic simula-
tion framework," in 2019 Fifth Workshop on En-
ergy Ecient Machine Learning and Cognitive
Computing - NeurIPS Edition (EMC2-NIPS) ,
pp. 10{13, 2019.
[19] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang,
A. Howard, H. Adam, and D. Kalenichenko,
\Quantization and training of neural networks
for ecient integer-arithmetic-only inference,"
inProceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition , pp. 2704{
2713, 2018.
[20] J. Yang, X. Shen, J. Xing, X. Tian, H. Li,
B. Deng, J. Huang, and X.-s. Hua, \Quantiza-
tion networks," in Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recog-
nition , pp. 7308{7316, 2019.
[21] C. Baskin, N. Liss, E. Schwartz, E. Zheltonozh-
skii, R. Giryes, A. M. Bronstein, and A. Mendel-
son, \Uniq: Uniform noise injection for non-
uniform quantization of neural networks," ACM
Trans. Comput. Syst. , vol. 37, Mar. 2021.
[22] D. Zhang, J. Yang, D. Ye, and G. Hua, \Lq-
nets: Learned quantization for highly accurate
and compact deep neural networks," in Proceed-
ings of the European Conference on Computer
Vision (ECCV) , September 2018.
[23] A. T. Elthakeb, P. Pilligundla, F. Mireshghallah,
A. Yazdanbakhsh, and H. Esmaeilzadeh, \Releq
: A reinforcement learning approach for auto-
matic deep quantization of neural networks,"
IEEE Micro , vol. 40, no. 5, pp. 37{45, 2020.
[24] T. Sheng, C. Feng, S. Zhuo, X. Zhang, L. Shen,
and M. Aleksic, \A quantization-friendly sepa-
rable convolution for mobilenets," in 2018 1st
Workshop on Energy Ecient Machine Learn-
ing and Cognitive Computing for Embedded Ap-
plications (EMC2) , pp. 14{18, IEEE, 2018.[25] J. Achterhold, J. M. Koehler, A. Schmeink, and
T. Genewein, \Variational network quantiza-
tion," in International Conference on Learning
Representations , 2018.
[26] D. P. Kingma, T. Salimans, and M. Welling,
\Variational dropout and the local reparameteri-
zation trick," in Advances in Neural Information
Processing Systems (C. Cortes, N. Lawrence,
D. Lee, M. Sugiyama, and R. Garnett, eds.),
vol. 28, Curran Associates, Inc., 2015.
[27] K. Neklyudov, D. Molchanov, A. Ashukha, and
D. P. Vetrov, \Structured bayesian pruning via
log-normal multiplicative noise," in Advances in
Neural Information Processing Systems 30: An-
nual Conference on Neural Information Process-
ing Systems 2017, December 4-9, 2017, Long
Beach, CA, USA (I. Guyon, U. von Luxburg,
S. Bengio, H. M. Wallach, R. Fergus, S. V. N.
Vishwanathan, and R. Garnett, eds.), pp. 6775{
6784, 2017.
[28] E. Tartaglione, S. Lathuili ere, A. Fiandrotti,
M. Cagnazzo, and M. Grangetto, \Hemp: High-
order entropy minimization for neural net-
work compression," Neurocomputing , vol. 461,
pp. 244{253, 2021.
[29] E. Tartaglione, A. Bragagnolo, A. Fiandrotti,
and M. Grangetto, \Loss-based sensitivity reg-
ularization: towards deep sparse neural net-
works," CoRR , vol. abs/2011.09905, 2020.
[30] M. Abadi, A. Agarwal, P. Barham, E. Brevdo,
Z. Chen, C. Citro, G. S. Corrado, A. Davis,
J. Dean, M. Devin, S. Ghemawat, I. Good-
fellow, A. Harp, G. Irving, M. Isard, Y. Jia,
R. Jozefowicz, L. Kaiser, M. Kudlur, J. Leven-
berg, D. Man e, R. Monga, S. Moore, D. Mur-
ray, C. Olah, M. Schuster, J. Shlens, B. Steiner,
I. Sutskever, K. Talwar, P. Tucker, V. Van-
houcke, V. Vasudevan, F. Vi egas, O. Vinyals,
P. Warden, M. Wattenberg, M. Wicke, Y. Yu,
and X. Zheng, \TensorFlow: Model optimiza-
tion," 2015. Software available from tensor-
ow.org.
16

--- PAGE 17 ---
[31] A. Paszke, S. Gross, F. Massa, A. Lerer,
J. Bradbury, G. Chanan, T. Killeen, Z. Lin,
N. Gimelshein, L. Antiga, A. Desmaison,
A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Te-
jani, S. Chilamkurthy, B. Steiner, L. Fang,
J. Bai, and S. Chintala, \Pytorch: Quantiza-
tion," in Advances in Neural Information Pro-
cessing Systems 32 (H. Wallach, H. Larochelle,
A. Beygelzimer, F. d 'Alch e-Buc, E. Fox, and
R. Garnett, eds.), pp. 8024{8035, Curran As-
sociates, Inc., 2019.
[32] H. Li, S. De, Z. Xu, C. Studer, H. Samet,
and T. Goldstein, \Training quantized nets: A
deeper understanding," in Proceedings of the
31st International Conference on Neural In-
formation Processing Systems , pp. 5813{5823,
2017.
[33] P. Yin, J. Lyu, S. Zhang, S. J. Osher, Y. Qi, and
J. Xin, \Understanding straight-through estima-
tor in training activation quantized neural nets,"
2019.
[34] Y. Bengio, N. L eonard, and A. C. Courville,
\Estimating or propagating gradients through
stochastic neurons for conditional computation,"
CoRR , vol. abs/1308.3432, 2013.
[35] D. Alistarh, D. Grubic, J. Li, R. Tomioka, and
M. Vojnovic, \Qsgd: Communication-ecient
sgd via gradient quantization and encoding,"
Advances in Neural Information Processing Sys-
tems, vol. 30, pp. 1709{1720, 2017.
[36] D. Alistarh, D. Grubic, J. Li, R. Tomioka,
and M. Vojnovic, \Qsgd: Communication-
ecient sgd via gradient quantization and en-
coding," arXiv preprint arXiv:1610.02132 , 2016.
https://arxiv.org/pdf/1610.02132.pdf .
[37] I. Hubara, M. Courbariaux, D. Soudry, R. El-
Yaniv, and Y. Bengio, \Quantized neural net-
works: Training neural networks with low pre-
cision weights and activations," The Journal
of Machine Learning Research , vol. 18, no. 1,
pp. 6869{6898, 2017.[38] F. Li, B. Zhang, and B. Liu, \Ternary weight
networks," arXiv preprint arXiv:1605.04711 ,
2016.
[39] P. Yin, S. Zhang, Y. Qi, and J. Xin, \Quantiza-
tion and training of low bit-width convolutional
neural networks for object detection," Journal
of Computational Mathematics , vol. 37, no. 3,
pp. 349{359, 2018.
[40] L. Hou and J. T. Kwok, \Loss-aware weight
quantization of deep networks," in 6th Interna-
tional Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30
- May 3, 2018, Conference Track Proceedings ,
OpenReview.net, 2018.
[41] T. Chu, Q. Luo, J. Yang, and X. Huang, \Mixed-
precision quantized neural networks with pro-
gressively decreasing bitwidth," Pattern Recog-
nition , vol. 111, p. 107647, 2021.
[42] D. Lee, D. Wang, Y. Yang, L. Deng, G. Zhao,
and G. Li, \Qttnet: Quantized tensor train neu-
ral networks for 3d object and video recogni-
tion," Neural Networks , vol. 141, pp. 420{432,
2021.
[43] P. Peng, M. You, W. Xu, and J. Li, \Fully
integer-based quantization for mobile convolu-
tional neural network inference," Neurocomput-
ing, vol. 432, pp. 194{205, 2021.
[44] A. Rajagopal, D. A. Vink, S. I. Venieris, and
C.-S. Bouganis, \Multi-precision policy enforced
training (muppet): A precision-switching strat-
egy for quantised xed-point training of cnns,"
arXiv preprint arXiv:2006.09049 , 2020.
[45] J. H. Wilkinson, \Rounding errors in algebraic
processes," p. 2, 1994.
[46] A. Paszke, S. Gross, F. Massa, A. Lerer,
J. Bradbury, G. Chanan, T. Killeen, Z. Lin,
N. Gimelshein, L. Antiga, A. Desmaison,
A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Te-
jani, S. Chilamkurthy, B. Steiner, L. Fang,
17

--- PAGE 18 ---
J. Bai, and S. Chintala, \Pytorch: An imper-
ative style, high-performance deep learning li-
brary," in Advances in Neural Information Pro-
cessing Systems 32 (H. Wallach, H. Larochelle,
A. Beygelzimer, F. d 'Alch e-Buc, E. Fox, and
R. Garnett, eds.), pp. 8024{8035, Curran As-
sociates, Inc., 2019.
[47] M. Abadi, A. Agarwal, P. Barham, E. Brevdo,
Z. Chen, C. Citro, G. S. Corrado, A. Davis,
J. Dean, M. Devin, S. Ghemawat, I. Good-
fellow, A. Harp, G. Irving, M. Isard, Y. Jia,
R. Jozefowicz, L. Kaiser, M. Kudlur, J. Leven-
berg, D. Man e, R. Monga, S. Moore, D. Mur-
ray, C. Olah, M. Schuster, J. Shlens, B. Steiner,
I. Sutskever, K. Talwar, P. Tucker, V. Van-
houcke, V. Vasudevan, F. Vi egas, O. Vinyals,
P. Warden, M. Wattenberg, M. Wicke, Y. Yu,
and X. Zheng, \TensorFlow: Large-scale ma-
chine learning on heterogeneous systems," 2015.
Software available from tensorow.org.
[48] J. H. Wilkinson, \Rounding errors in algebraic
processes," pp. 26{27, 1994.
[49] J. H. Wilkinson, \Rounding errors in algebraic
processes," p. 1, 1994.
[50] M. Hopkins, M. Mikaitis, D. R. Lester, and
S. Furber, \Stochastic rounding and reduced-
precision xed-point arithmetic for solving neu-
ral ordinary dierential equations," Philosophi-
cal Transactions of the Royal Society A , vol. 378,
no. 2166, p. 20190052, 2020.
[51] D. Yin, A. Pananjady, M. Lam, D. Papailiopou-
los, K. Ramchandran, and P. Bartlett, \Gra-
dient diversity: a key ingredient for scalable
distributed learning," in International Confer-
ence on Articial Intelligence and Statistics ,
pp. 1998{2007, 2018.
[52] A. Kerr, H. Wu, M. Gupta, D. Blasig, P. Ra-
mani, N. Farooqui, P. Majcher, P. Springer,
J. Wang, S. Yokim, M. Hohnerbach, A. Atluri,
D. Tanner, T. Costa, J. Demouth, B. Fahs,
M. Goldfarb, M. Hagog, F. Hu, A. Kaatz,T. Li, T. Liu, D. Merrill, K. Siu, M. Taven-
rath, J. Tran, V. Wang, J. Wu, F. Xie, A. Xu,
J. Yang, X. Zhang, N. Zhao, G. Bharambe,
C. Cecka, L. Durant, O. Giroux, S. Jones,
R. Kulkarni, B. Lelbach, J. McCormack, and
K. P. and, \Nvidia cutlass," 2020. Online, ac-
cessed 15.07.2020.
[53] D. P. Kingma and J. Ba, \Adam: A method
for stochastic optimization," arXiv preprint
arXiv:1412.6980 , 2014.
[54] L. Deng, \The mnist database of handwritten
digit images for machine learning research [best
of the web]," IEEE Signal Processing Magazine ,
vol. 29, no. 6, pp. 141{142, 2012.
[55] G. H. Alex Krizhevsky, Vinod Nair, \The cifar-
10 and cifar-100 dataset," 2019. Online, accessed
15.07.2020.
[56] X. Glorot and Y. Bengio, \Understanding the
diculty of training deep feedforward neural
networks," in Proceedings of the thirteenth inter-
national conference on articial intelligence and
statistics , pp. 249{256, 2010.
[57] K. He, X. Zhang, S. Ren, and J. Sun, \Delv-
ing deep into rectiers: Surpassing human-level
performance on imagenet classication," in Pro-
ceedings of the IEEE international conference on
computer vision , pp. 1026{1034, 2015.
[58] B. Hanin and D. Rolnick, \How to start training:
The eect of initialization and architecture," in
Advances in Neural Information Processing Sys-
tems, pp. 571{581, 2018.
[59] G. Klambauer, T. Unterthiner, A. Mayr, and
S. Hochreiter, \Self-normalizing neural net-
works," in Advances in neural information pro-
cessing systems , pp. 971{980, 2017.
[60] Y. A. LeCun, L. Bottou, G. B. Orr, and K.-
R. M uller, \Ecient backprop," in Neural net-
works: Tricks of the trade , pp. 9{48, Springer,
2012.
18

--- PAGE 19 ---
[61] Y. Bhalgat, J. Lee, M. Nagel, T. Blankevoort,
and N. Kwak, \Lsq+: Improving low-bit quan-
tization through learnable osets and better ini-
tialization," in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern
Recognition Workshops , pp. 696{697, 2020.
[62] S. Basodi, C. Ji, H. Zhang, and Y. Pan, \Gradi-
ent amplication: An ecient way to train deep
neural networks," CoRR , vol. abs/2006.10560,
2020.
[63] Z. Chen, V. Badrinarayanan, C.-Y. Lee, and
A. Rabinovich, \Gradnorm: Gradient normal-
ization for adaptive loss balancing in deep mul-
titask networks," in International Conference on
Machine Learning , pp. 794{803, 2018.
[64] T. Salimans and D. P. Kingma, \Weight normal-
ization: A simple reparameterization to accel-
erate training of deep neural networks," in Ad-
vances in neural information processing systems ,
pp. 901{909, 2016.
[65] P. Zhou, J. Feng, C. Ma, C. Xiong, S. C. H. Hoi,
and W. E, \Towards theoretically understand-
ing why sgd generalizes better than adam in
deep learning," in Advances in Neural Informa-
tion Processing Systems (H. Larochelle, M. Ran-
zato, R. Hadsell, M. F. Balcan, and H. Lin,
eds.), vol. 33, pp. 21285{21296, Curran Asso-
ciates, Inc., 2020.
[66] P. M. Williams, \Bayesian regularization and
pruning using a laplace prior," Neural compu-
tation , vol. 7, no. 1, pp. 117{143, 1995.
[67] A. Y. Ng, \Feature selection, l 1 vs. l 2 regular-
ization, and rotational invariance," in Proceed-
ings of the twenty-rst international conference
on Machine learning , p. 78, 2004.
[68] R. Tibshirani, \Regression shrinkage and selec-
tion via the lasso," Journal of the Royal Statis-
tical Society: Series B (Methodological) , vol. 58,
no. 1, pp. 267{288, 1996.
[69] H. Zou and T. Hastie, \Regularization and vari-
able selection via the elastic net," Journal ofthe royal statistical society: series B (statistical
methodology) , vol. 67, no. 2, pp. 301{320, 2005.
[70] N. Corporation, \NVIDIA DGX-1essential in-
strument for ai research," 2017. Online, accessed
07.07.2021.
19
