# 2302.10899.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/quantization/2302.10899.pdf
# File size: 1508639 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
1
Feature Affinity Assisted Knowledge Distillation
and Quantization of Deep Neural Networks on
Label-Free Data
Zhijian Li, Biao Yang, Penghang Yin, Yingyong Qi, and Jack Xin
Abstract —In this paper, we propose a feature affinity
(FA) assisted knowledge distillation (KD) method to improve
quantization-aware training of deep neural networks (DNN). The
FA loss on intermediate feature maps of DNNs plays the role
of teaching middle steps of a solution to a student instead of
only giving final answers in the conventional KD where the loss
acts on the network logits at the output level. Combining logit
loss and FA loss, we found that the quantized student network
receives stronger supervision than from the labeled ground-truth
data. The resulting FAQD is capable of compressing model on
label-free data, which brings immediate practical benefits as
pre-trained teacher models are readily available and unlabeled
data are abundant. In contrast, data labeling is often laborious
and expensive. Finally, we propose a fast feature affinity (FFA)
loss that accurately approximates FA loss with a lower order
of computational complexity, which helps speed up training for
high resolution image input.
Index Terms —Quantization, Convolutional Neural Network,
Knowledge Distillation, Model Compression, Image Classification
I. I NTRODUCTION
Quantization is one of the most popular methods for deep
neural network compression, by projecting network weights
and activation functions to lower precision thereby accelerate
computation and reduce memory consumption. However, there
is inevitable loss of accuracy in the low bit regime. One way
to mitigate such an issue is through knowledge distillation
(KD [10]). In this paper, we study a feature affinity assisted
KD so that the student and teacher networks not only try to
match their logits at the output level but also match feature
maps in the intermediate stages. This is similar to teaching a
student through intermediate steps of a solution instead of just
showing the final answer (as in conventional KD [10]). Our
method does not rely on ground truth labels while enhancing
student network learning and closing the gaps between full
and low precision models.
A. Weight Quantization of Neural Network
Quantization-aware training (QAT) searches the optimal
model weight in training. Given an objective L, the classical
Zhijian Li, Biao Yang, Yingyong Qi, and Jack Xin are with Department of
Mathematics, University of California, Irvine, CA, USA
Penghang Yin is with Department of Mathematics and Statistics, State
University of New York at Albany, Albany, NY , USA
Corresponding author: Zhijian Li (e-mail: zhijil2@uci.edu)
This work was partly supported by NSF grants DMS-1924935, DMS-
1952644, DMS-2151235, DMS-2208126; and a Qualcomm faculty award.QAT scheme ( [6], [21]) is formulated as
(
wt+1=wt− ∇ uL(ut),
ut+1=Quant( wt+1),(1)
where Quant is projection to a low precision quantized space.
Yin et al. [28] proposed BinaryRelax, a relaxation form of
QAT, which replaces the second update of (1) by
ut+1=wt+1+λt+1Quant( wt+1)
1 +λt+1,
λt+1=ηλtwithη >1.(2)
Darkhorn et al. [7] further improved (2) by designing a
more sophisticated learnable growing scheme for λtand
adding a learnable parameter into Quant( ·). Polino et al. [19]
proposed quantized distillation (QD), a QAT framework that
leverages knowledge distillation for quantization. Under QD,
the quantized model receives supervision from both ground
truth (GT) labels and a trained teacher in float precision (FP).
The objective function has the generalized form ( α∈(0,1)):
LQD=αLKD+ (1−α)LGT (3)
where LKD is Kullback–Leibler divergence (KL) loss, and
LGTis negative log likelihood (NLL) loss. In order to compare
different methods fairly, we introduce two technical terms:
end-to-end quantization and fine-tuning quantization. End-to-
end quantization is to train a quantized model from scratch,
and fine-tuning quantization is to train a quantized model
from a pre-trained float precision (FP) model. With the same
method, the latter usually lands a better result than the former.
Li et al. [15] proposed a mixed quantization (a.k.a. BRECQ)
that takes a pre-trained model and partially retrains the model
on a small subset of data.
B. Activation Quantization
In addition to weight quantization, the inference of neural
networks can be further accelerated through activation quanti-
zaton. Given a resolution α >0, a quantized ReLU activation
function of bit-width b∈Nisσ=σ(x, α):
σ=

0 x <0
kα (k−1)α≤x < kα, 1≤k≤2b−1
(2b−1)α x≥(2b−1)α
(4)
where the resolution parameter αis learned from data. A plot
of2-bit quantized ReLU is shown in Fig. 1. However, sucharXiv:2302.10899v3  [cs.LG]  18 Aug 2023

--- PAGE 2 ---
2
Fig. 1: Plot of 2-bit quantized ReLU σ(x, α)
quantized activation function leads to vanished gradient during
training, which makes the standard backpropagation inapplica-
ble. Indeed, it is clear that∂σ
∂x= 0almost everywhere. Bengio
et al. [2] proposed to use a straight through estimator (STE) in
backward pass to handle the zero gradient issue. The idea is
to simply replace the vanished∂σ
∂xwith a non-trivial derivative
∂˜σ
∂xof a surrogate function ˜σ(x, α). Theoretical studies on STE
and convergence vs. recurrence issues of training algorithms
have been conducted in ( [17], [27]). Among a variety of STE
choices, a widely-used STE is the x-derivative of the so-called
clipped ReLU [3] ˜α(x, α) = min {max{x,0},(2b−1)α},
namely,
∂˜σ
∂x=(
1 0 < x < (2b−1)α
0else.
In addition, a few proxies of∂σ
∂αhave been proposed ( [4],
[29]). In this work, we follow [29] and use the three-valued
proxy:
∂σ
∂α≈

0 x≤0
2b−10< x < (2b−1)α
2b−1x≥(2b−1)α.(5)
C. Knowledge Distillation
Several works have proposed to impose closeness of the
probabilistic distributions between the teacher and student
networks, e.g. similarity between feature maps. A flow of
solution procedure (FSP) matrix in [26] measures the informa-
tion exchange between two layers of a given model. Then l2
loss regularizes the distance between FSP matrices of teacher
and student in knowledge distillation. An attention transform
(AT) loss [30] directly measures the distance of feature maps
outputted by teacher and student, which enhances the learning
of student from teacher. Similarly, feature affinity (FA) loss
[24] measures the distance of two feature maps. In a dual
learning framework for semantic segmentation [24], the FA
loss is applied on the output feature maps of a segmentation
decoder and a high-resolution decoder. In [25], FA loss is
applied on multi-resolution paths in knowledge distillation
of semantic segmentation models. It improves mean Average
Precision of the lightweight student model. Given two feature
maps with the same height and width (interpolate if different),
FS∈RC1×H×WandFT∈RC2×H×W, we first normalize
the feature map along the channel dimension. Given a pixelof feature map Fi∈RC, we construct an affinity matrix
S∈RWH×WHas:
Sij=∥Fi−Fj∥θ:= cos θij=⟨Fi,Fj⟩
||Fi||||Fj||.
where θijmeasures the angle between FiandFj. Hence, the
FA loss measures the similarity of pairwise angular distance
between pixels of two feature maps, which can be formulated
as
Lfa(FS,FT) =1
W2H2||ST−SS||2
2. (6)
D. Contributions
In this paper, our main contributions are:
1) We find that using mean squares error (MSE) gives bet-
ter performance than KL on QAT, which is a significant
improvement of QD ( [19]).
2) We consistently improve the accuracies of various quan-
tized student networks by imposing the FA loss on
feature maps of each convolutional block. We also unveil
the theoretical underpinning of feature affinity loss in
terms of the celebrated Johnson-Lindenstrass lemma for
low-dimensional embeddings.
3) We achieve state-of-art quantization accuracy on
CIFAR-10, CIFAR-100, and Tiny ImageNet. Our FAQD
framework can train a quantized student network on
unlabeled data up to or exceeding the accuracy of its
full precision counterpart.
4) We propose a randomized Fast FA (FFA) loss to ac-
celerate the computation of training loss, and prove its
convergence and error bound.
E. Organization
This paper is organized as follows: In Sec. II, we introduce
the main objective of FAQD. In particular, we present feature
affinity loss and go over the comparison between MSE and KL
loss. In Sec. III, we numerically verify that FAQD outperforms
baseline methods. In Sec. IV , we introduce Fast feature affinity
loss and verify its acceleration to FAQD.
II. F EATURE AFFINITY ASSISTED DISTILLATION AND
QUANTIZATION
A. Feature Affinity Loss
In quantization setting, it is unreasonable to require that FS
be close to FT, as they are typically in different spaces ( FS∈
Qin full quantization) and of different dimensions. However,
FScan be viewed as a compression of FTin dimension,
and preserving information under such compression has been
studied in compressed sensing. Researchers ( [20], [22]) have
proposed to compress graph embedding to lower dimension
so that graph convolution can be computed efficiently. In K-
means cluttering problem, several methods ( [1], [18]) have
been designed to project the data into a low-dimensional space
such that
||Proj(x)−Proj(y)|| ≈ || x−y||,∀(x,y), (7)

--- PAGE 3 ---
FIRST A. AUTHOR et al. : BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE 3
and so pairwise distances from data points to the centroids can
be computed at a lower cost.
In view of the feature maps of student model as a compres-
sion of teacher’s feature maps, we impose a similar property
in terms of pairwise angular distance:
||FS
i−FS
j||θ≈ ||FT
i−FT
j||θ,∀(i, j)
which is realized by minimizing the feature affinity loss. On
the other hand, a Johnson–Lindenstrauss (JL [11]) like lemma
can guarantee that we have student’s feature affinity matrix
close to the teacher’s, provided that the number of channels
of student network is not too small. In contrast, the classical JL
lemma states that a set of points in a high-dimensional space
can be embedded into a space of much lower dimension in
such a way that the Euclidean distances between the points
are nearly preserved. To tailor it to our application, we prove
the following JL-like lemma in the angular distance case:
Theorem 2.1 (Johnson–Lindenstrauss lemma, Angular Case):
Given any ϵ∈(0,1), an embedding matrix F∈Rn×d, for
k∈(16ϵ−2lnn, d), there exists a linear map T(F)∈Rn×k
so that
(1−ϵ)||Fi−Fj||θ≤ ||T(F)i−T(F)j||θ
≤(1 +ϵ)||Fi−Fj||θ,∀1≤i, j≤n(8)
where ||Fi−Fj||θ=⟨Fi,Fj⟩
∥Fi∥∥Fj∥is the angular distance.
It is thus possible to reduce the embedding dimension down
from dtok, while roughly preserving the pairwise angular dis-
tances between the points. In a convolutional neural network,
we can view intermediate feature maps as FS∈RHW×C1
andFT∈RHW×C2, and feature affinity loss will help the
student learn a compressed feature embedding. The FA loss
can be flexibly placed between teacher and student in different
positions (encoder/decoder, residual block, etc.) for different
models. In standard implementation of ResNet, residual blocks
with the same number of output channels are grouped into a
sequential layer. We apply FA loss to the features of such
layers.
LFA=LX
l=1Lfa(FT
l,FS
l)
where FT
landFS
lare the feature maps of teacher and student
respectively. For example, the residual network family of
ResNet20, ResNet56, ResNet110, and ResNet164 have L= 3,
whereas the family of ResNet18, ResNet34, and ResNet50
have L= 4.
B. Choice of Loss Functions
In this work, we propose two sets of loss function choices
for the end-to-end quantization and pretrained quantization,
where end-to-end quantization refers to having an untrained
student model with randomly initialized weights. We investi-
gate both scenarios of quantization and propose two different
strategies for each.
The Kullback–Leibler divergence (KL) is a metric of the
similarity between two probabilistic distributions. Given aground-truth distribution P, it computes the relative entropy
of a given distribution Qfrom P:
LKL(P||Q) =X
x∈XP(x) lnP(x)
Q(x). (9)
While KD is usually coupled with KL loss ( [10], [19]), it is
not unconventional to choose other loss functions. Kim et al.
[14] showed that MSE, in certain cases, can outperform KL in
the classic teacher-student knowledge distillation setting. KL
loss is also widely used for trade-off between accuracy and
robustness under adversarial attacks, which can be considered
as self-knowledge distillation. Given a classifier f, an original
data point xand its adversarial example x′, TRADES [31] is
formulated as
LTRADES =LCE(f(x),y) +LKL(f(x)||f(x′)).
Li et al. [16] showed that LCE 
f(x′), y
outperforms
LKL(f(x)||f(x′))both experimentally and theoretically.
Inspired by the studies above, we conduct experiments on
different choices of the loss function. We compare KD on
quatization from scratch (end-to-end). As shown in Tab. I,
MSE outperforms KL in quantization.
Student Teacher 1-bit 2-bit 4-bit
LKD=KL loss in (3)
ResNet20 ResNet110 89.06% 90.86% 92.01%
LKD=MSE in (3)
ResNet20 ResNet110 90.00% 91.01% 92.17%
TABLE I: Comparision of KL loss and MSE loss on CIFAR-
10 data set. All teachers are pre-trained FP models, and all
students are initial models (end-to-end quantization).
On the other hand, we find that KL loss works better for
fine-tuning quantization. One possible explanation is that when
training from scratch, the term lnP(x)
Q(x)is large. However, the
derivative of logarithm is small at large values, which makes
it converge slower and potentially worse. On the other hand,
whenP(x)
Q(x)is close to 1, the logarithm has sharp slope and
converges fast.
C. Feature Affinity Assisted Distillation and Quantization
Inspired by previous studies ( [13], [15], [19]), we propose
a feature affinity assisted quantized distillation (FAQD). The
end-to-end quantization objective function is formulated as:
L=αLKD+βLFA+γLGT
=αLMSE 
fT(x), fS(x)
+βLX
l=1Lfa(FT
l,FS
l)
+γLNLL(fS(x), y).(10)
In fine-tuning quantization, we replace MSE loss in (10) by
KL divergence loss. In FAQD, the student model learns not
only the final logits of the teacher but also the intermediate
extracted feature maps of the teacher using feature affinity
norm computed as in [24].

--- PAGE 4 ---
4
Fig. 2: FAQD framework. The intermediate feature maps are supervised by FA loss, and the raw logits by MSE loss.
In addition to (10), we also propose a label-free objective
which does not require the knowledge of labels:
Llabel-free =αLMSE 
fT(x), fS(x)
+βLX
l=1Lfa(FT
l,FS
l).
(11)
Despite the pre-trained computer vision models being available
from cloud service such as AWS and image/video data abun-
dantly collected, the data labeling is still expensive and time
consuming. Therefore, a label-free quantization framework has
significant value in the real world. In this work, we verify that
the FA loss can significantly improve KD performance. The
label-free loss in Eq. (11) can outperform the baseline methods
in Tab. II as well as the prior supervised QD in (3).
III. E XPERIMENTAL RESULTS
In Tab. II, we listed the performance of previous methods
mentioned in the introduction section. We would like to remark
that the BRECQ results are from channel-wise quantization.
Namely, each channel of a convolutional layer has its own
float scaler and projection map. All other results in Tab. II are
layer-wise quantization.
All experiments reported here were conducted on a desktop
with Nvidia RTX6000 8GB GPU card at UC Irvine.Method 1-bit 2-bit 4-bit
Model: ResNet20
QAT ( [6], [21]) 87.07% 90.26% 91.47%
BinaryRelax [28] 88.64% 90.47% 91.75%
QD [19] 89.06% 90.86% 92.01%
DSQ [8] 90.24% 91.06% 91.92%
BRECQ∗[15] N/A 88.10% 89.01%
TABLE II: End-to-end quantization accuracies of some exist-
ing quantization-aware training methods on CIFAR-10 dataset.
To stick with the original work, we apply channel-wise quan-
tization in BRECQ, denoted by ∗. All the other methods are
under layer-wise quantization.
A. Weight Quantization
In this section we test FAQD on the dataset CIFAR-10. First,
we experiment on fine-tuning quantization. The float precision
(FP) ResNet110 teaches ResNet20 and ResNet56. The teacher
has 93.91% accuracy, and the two pre-trained models have
accuracy 92.11% and 93.31% respectively. While both SGD
and Adam optimization work well on the problem, we found
KL loss with Adam slightly outperform SGD in this scenario.
The objective is
L=LKL+LFA
for the label-free quantization. When calibrating the ground-
truth label, the cross-entropy loss LNLL is used as the super-

--- PAGE 5 ---
FIRST A. AUTHOR et al. : BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE 5
vision criterion.
Cifar-10
Teacher ResNet110: 93.91%
Pre-trained FP ResNet20: 92.21%
Method 1-bit 2-bit 4-bit
Label-free FAQD 89.97% 91.40% 92.55%
FAQD with Supervision 90.92% 91.93% 92.74%
Cifar-100
Teacher ResNet164: 74.50%
Pre-trained FP ResNet110:72.96%
Method 1-bit 2-bit 4-bit
label-free FAQD 73.33% 75.02% 75.78%
FAQD with Supervision 73.35% 75.24% 76.10%
Tiny ImageNet
Teacher ResNet34: 65.60%
Pre-trained FP ResNet18: 64.23%
Method 1-bit 2-bit 4-bit
label-free FAQD 64.89% 65.02% 65.69%
FAQD with Supervision 65.77% 66.49% 66.62%
TABLE III: Fine-tuning knowledge distillation for quantiza-
tion of all convolutional layers.
For end-to-end quantization, we found that MSE loss
performs better than KL loss. Adam optimization struggles
to reach acceptable performance on end-to-end quantization
(with either KL or MSE loss). We further test the performance
of FAQD on larger dataset CIFAR-100 where an FP ResNet
164 teaches a quantized ResNet110. We report the accuracies
for both label-free and label-present supervision. We evaluate
FAQD on both fine-tuning quantization and end-to-end quan-
tization. In the CIFAR-100 experiment, the teacher ResNet164
Cifar-10
Teacher ResNet110: 93.91%
Method 1-bit 2-bit 4-bit
FP student ResNet20: 92.21 %
Label-free FAQD 89.88% 91.23% 92.19%
FAQD with Supervision 90.56% 91.65% 92.43%
Cifar-100
Teacher ResNet164: 74.50%
Method 1-bit 2-bit 4-bit
label-free FAQD 72.78% 74.35% 74.90%
FAQD with Supervision 73.35% 74.40% 75.31%
Tiny ImageNet
Teacher ResNet34: 65.60%
Method 1-bit 2-bit 4-bit
label-free FAQD 64.37% 65.05% 65.40%
FAQD with Supervision 65.13% 65.67% 65.92%
TABLE IV: End-to-end FAQD of ResNet110 on CIFAR-100.
The accuracy of 4-bit label-free quantization surpasses 72.96%
of FP ResNet110 and is close to FP ResNet164.
has 74.50% testing accuracy. For the pretrained FAQD, the FP
student ResNet110 has 72.96% accuracy. As shown in Tab. III
and Tab. IV, FAQD has surprisingly superior performance on
CIFAR-100. The binarized student almost reaches the accuracy
of FP model, and the 4-bit model surpasses the FP teacher.
B. Full Quantization
In this section, we extend our results to full quantization
where the activation function is also quantized. In Tab. V,
we list the fine-tuning results from aforementioned methods.
Among the methods in Tab. V, only Quantized DistillationMethod 1W4A 4W4A
Model: ResNet20
BinaryRelax [28] 89.22% 91.37%
QD [19] 90.15% 92.06%
BCGD [29] 89.98% 91.65%
BRECQ∗[15] N/A 88.71%
TABLE V: Fine-tuning full quantization results of existing
methods on CIFAR-10. The ∗means the same as in Tab. 2.
(QD) is stable under end-to-end full quantization. We extend
our results to the tiny Tiny ImageNet dataset, which contains
100K downsampled 64 ×64 images across 200 classes for
training. To simulate ImageNet, we interpolate the resolution
back to the original 224 ×224. As shown in Tab. VI, the
4W4A fune-tuning quantization has accuracy similar to float
ResNet20. Meanwhile, we close the long existing performance
gap [9] when reducing activation precision to 1-bit, as the ac-
curacy drop is linear (with respect to activation precision) and
small. When fine-tuning a fully quantized model, we follow a
two-step process. First, we train an activation quantized model
with floating-point weights. Subsequently, we apply full quan-
tization using the FAQD. This technique proves to be essential,
especially when scaling up the Tiny ImageNet dataset. In our
experiments, we observed the following phenomenon when
replacing all ReLU activation functions with 1-bit Quantized
ReLU. For a pretrained 32A32W ResNet20 model, originally
trained on CIFAR-10, the accuracy dropped to 80.03% from
its original accuracy of 92.21%. However, when working with
a pretrained ResNet-18 model on the Tiny ImageNet dataset,
the accuracy plummeted to 0.62% from its initial accuracy of
64.23%.
CIFAR-10
Pretrained ResNet20: 1A32W-91.89%, 4A32W-92.01%
Model pre-trained 1W1A 4W4A
ResNet20 No N/A 91.07%
ResNet20 Yes 89.70% 92.53%
CIFAR-100
Pretrained ResNet56: 1A32W-70.96%, 4A32W-71.42%
Model pre-trained 1W1A 4W4A
ResNet56 No N/A 68.84%
ResNet56 Yes 68.18 73.53%
Tiny ImageNet
Pretrained ResNet18: 1A32W-63.82%, 4A32W-64.15%
Model pre-trained 1W1A 4W4A
ResNet18 No N/A 64.67%
ResNet18 Yes 65.01 65.55%
TABLE VI: End-to-end and fine-tuning full quantization on
CIFAR-10, CIFAR-100 and Tiny ImageNet, with teacher net-
works same as in Tab. 4.
IV. F AST FEATURE AFFINITY LOSS
A. Proposed Method
Despite the significant increase of KD performance, we
note that introducing FA loss will increase the training
time. If we normalize the feature maps by row beforehand,
computing FA loss between multiple intermediate feature
maps can be expensive.

--- PAGE 6 ---
6
Fig. 3: Fast feature affinity loss with a low-rank random matrix
Z.
Lfa(F1, F2) =∥F1FT
1−F2FT
2∥2
2. (12)
As we freeze the pre-trained teacher, feature map of the teacher
model F1=fT(x)is a constant, in contrast to student feature
mapF2=fS(Θ,x). Denote S1=F1FT
1∈RWH×WHand
g(Θ,x) =fS(Θ,x)[fS(Θ,x)]T. The feature affinity can be
formulated as
Lfa(Θ) =1
|X|X
x∈X∥S1−g(Θ,x)∥2
2. (13)
Computing S1andg(Θ, X)requires O(W2H2C)complexity
each (C is the number of channels), which is quite expensive.
We introduce a random estimator of Lffa(Θ):
Lffa(F1, F2,z) =1
|X|X
x∈X∥(S1−g(Θ,x))z∥2
2, (14)
where z∈RHWis a vector with i.i.d unit normal components
N(0,1). We show below that Eq. (14) is an unbiased estimator
of FA loss (13).
Proposition 1:
Ez∼N(0,1)[Lffa(F1, F2,z)] =Lfa(Θ).
This estimator can achieve computing complexity O(HWC )
by performing two matrix-vector multiplication F1 
FT
1z
.
We define the Fast Feature Affinity (FFA) loss to be the k
ensemble of (14):
Lffa,k(Θ) =1
|X|X
x∈X1
k∥(S1−g(Θ,x))Zk∥2
2 (15)
where Zk∈RHW×kwith i.i.d N(0,1)components, and we
have k≪WH . The computational complexity of Lffa,k(Θ)
isO(kWHC ).
Finally, we remark that FFA loss can accelerate computation
of pairwise Euclidean distance in dimensional reduction such
as in (7). The popular way to compute the pairwise distance
of rows for a matrix A∈Rn×cis to broadcast the vector
of row norms and compute AAT. Given the row norm vector
v= (∥A1∥2,···,∥An∥2), the similarity matrix (Sij),Sij=
∥Ai−Aj∥2, is computed as
S=1⊗v−2AAT+v⊗1.
The term 2AATcan be efficiently approximated by FFA loss.B. Experimental Results
We test Fast FA loss on CIFAR-10 and Tiny ImageNet. As
mentioned in the previous section, ResNet-20 has 3 residual
blocks. The corresponding width and height for feature maps
are 32, 16, and 8, H=Wfor all groups, so the dimension
(HW ) of similarity matrices are 1024, 256, and 64. We
test the fast FA loss with the number of ensemble k=1,
5, and 15. The results are shown in Tab. VII. Meanwhile,
FFA has added training time for each step. When k= 1,
the accuracies are inconsistent due to large variance. With
too few samples in the estimator, the fast FA norm is too
noisy and jeopardizes distillation. At k= 5 , the fast FA
loss stabilizes and the accuracy improves towards that of the
baseline, L=LMSE +LCEin Tab. I. When kincreases to
15, the performance of fast FA loss is comparable to that of
the exact FA loss. Moreover, we experiment with the time
consumption for computing FA loss and FFA loss. We plot
the time in log scale vs. H, (H=W) for feature maps.
Theoretical time complexity for computing exact FA loss is
O(H4)and that for FFA loss is O(H2). Fig. 4(a) shows the
agreement with the theoretical estimate. The larger the H, the
Dataset CIFAR-10 Tiny ImageNet
Model ResNet20 ResNet18
Ensemble Number k Fast FA Loss Accuracy
1 (ResNet20)/1 (ResNet18) 88.89±2.95% 52.32±4.35%
5/40 90.55% 56.12%
15/80 90.72% 61.12%
Ensemble Number k Fast FA Loss Training Time Per Epoch
1/1 29.71s 5m19s
5/40 29.77s 5m32s
15/80 30.74s 5m51s
Ensemble Number k Exact FA Loss Training Time Per Step
N/A 36.17s 7m36s
TABLE VII: 4A4W FFA accuracy and training time per epoch
for ResNet20 on CIFAR-10 and ResNet18 on Tiny ImageNet,
with teacher networks same as in Tab. 4. The FFA loss
accelerates training and approaches the performance of exact
FA loss with a proper choice of the ensemble number k.
more advantageous the FFA loss. For (medical) images with
resolutions in the thousands, the FFA loss will have significant
computational savings. In Tab. VII, we report training time
per epoch. We train models 200 epochs with cosine annealing
learning rate.
C. Theoretical Analysis of FFA Loss
As shown in Proposition 4.1, the FFA loss is a k-ensemble
unbiased estimator of FA loss. By the strong law of large
numbers, the FFA loss converges to the exact FA loss with
probability 1.
Theorem 4.1: For given Θ, suppose that |Lfa(Θ)|<∞,
then
∀ϵ >0,∃N s.t. ∀k > N, |Lffa,k(Θ)− Lfa(Θ)|< ϵ.
Namely, the FFA loss converges to FA loss pointwise:
∀Θ,lim
k→∞Lffa,k(Θ) = Lfa(Θ).
We also establish the following error bound for finite k.

--- PAGE 7 ---
FIRST A. AUTHOR et al. : BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE 7
(a) Log-log plot for inference time of FA loss and FFA loss.
(b) Zoomed in plot for inference time of FA loss and FFA loss.
Fig. 4: Plots for inference time of FA loss and FFA loss with
k= 1.
Proposition 2:
P 
|Lffa,k(Θ)− Lfa(Θ)|> ϵ
≤C
ϵ2k,
where C≤3∥Lfa(Θ)∥4
2.
Proposition 4.2 says that the probability that the FFA
estimation has an error beyond a target value decays like
O(1
k). The analysis guarantees the accuracy of FFA loss as
an efficient estimator of FA loss. Another question one might
ask is whether minimizing the FFA loss is equivalent to
minimizing the FA loss. Denote Θ∗= arg min Lfa(Θ) and
Θ∗
k= arg min Lffa,k(Θ), and assume the minimum is unique
for each function. In order to substitute FA loss by FFA loss,
one would hope that Θ∗
kconverges to Θ∗. Unfortunately, the
point-wise convergence in Theorem 4.1 is not sufficient to
guarantee the convergence of the optimal points, as a counter-
example can be easily constructed. In the rest of this section,
we show that such convergence can be established under an
additional assumption.
Theorem 4.2 (Convergence in the general case): Suppose
thatLffa,k(Θ) converges to Lfa(Θ) uniformly, that is
∀ϵ >0,∃N s.t. ∀k > N, |Lffa,k(Θ)− Lfa(Θ)|< ϵ
and|Lfa(Θ)|<∞,∀Θ.Then
lim
k→∞||Θ∗
k−Θ∗||2= 0. (16)
The uniform convergence assumption can be relaxed if Lfais
convex in Θ. A consequence of Theorem 4.2 is below.
Corollary 4.2.1 (Convergence in the convex case): Let
Lfa:Rn→Rbe convex and L-smooth, and that ∃constant
M > 0such that ||Θ∗
k|| ≤M,∀k. Then Lffa,k is also convex
for any k, and limk→∞||Θ∗
k−Θ∗||2= 0.V. C ONCLUSION
We presented FAQD, a feature assisted (FA) knowledge
distillation method for quantization-aware training. It couples
MSE loss with FA loss and significantly improves the accuracy
of the quantized student network. FAQD applies to both weight
only and full quantization, and outperforms baseline Resnets
on CIFAR-10/100 and Tiny ImageNet. We also analyzed an
efficient randomized approximation (FFA) to the FA loss for
large dimensional feature maps, which provided theoretical
foundation for FFA loss to benefit future model training on
high resolution images in applications.
VI. A PPENDIX
Proof of Theorem 2.1: It suffices to prove that for any set
ofnunit vectors in Rd, there is a linear map nearly preserving
pairwise angular distances, because the angular distance is
scale-invariant.
LetTbe a linear transformation induced by a random Gaus-
sian matrix1√
kA∈Rk×dsuch that T(F) =FAT. Define the
events A−
ij={T: (1−ϵ)∥Fi−Fj∥2≤ ∥T(F)i−T(F)j∥2≤
(1+ϵ)∥Fi−Fj∥2fails}andA+
ij={T: (1−ϵ)∥Fi+Fj∥2≤
∥T(F)i+T(F)j∥2≤(1 +ϵ)∥Fi+Fj∥2fails}.
Following the proof of the classical JL lemma in the
Euclidean case [23], we have:
P(A−
ij)≤2e−(ϵ2−ϵ3)k
4, P(A+
ij)≤2e−(ϵ2−ϵ3)k
4. (17)
LetBij={T:|Fi·Fj−T(F)i·T(F)j|> ϵ}, where ·is the
shorthand for inner product. We show that Bij⊂A−
ij∪ A+
ij
for∥Fi∥=∥Fj∥= 1 by showing A−
ijC∩ A+
ijC⊂ BC
ij.
IfA−
ijC∩ A+
ijCholds, we have
4T(F)i·T(F)j
=∥T(F)i+T(F)j∥2− ∥T(F)i−T(F)j∥2
≤(1 +ϵ)∥Fi+Fj∥2−(1−ϵ)∥Fi−Fj∥2
=4Fi·Fj+ 2ϵ(∥Fi∥2+∥Fj∥2)
=4Fi·Fj+ 4ϵ.
Therefore, Fi·Fj−T(F)i·T(F)j≥ −ϵ. By a similar
argument, we have Fi·Fj−T(F)i·T(F)j≤ϵ. Then we
haveA−
ijC∩ A+
ijC⊂ BC
ij, and thus
P(Bij)≤P(A−
ij∪A+
ij)≤4 exp{−(ϵ2−ϵ3)k
4}
and
P(∪i<jBij)≤X
i<jP(Bij)≤4n2exp{−(ϵ4−ϵ3)k
4}.
This probability is less than 1if we take k >16 lnn
ϵ2.
Therefore, there must exist a Tsuch that ∩i<jBC
ijholds, which
completes the proof.

--- PAGE 8 ---
8
Proof of Proposition 4.1: Letting N=WH ,aij=
(F1FT
1)ij, and bij= (F2FT
2)ijin equation (14), we have:
EzLffa(F1, F2; 2) = EzNX
i=1(NX
j=1|aij−bij|zj)2
=EzNX
i=1(NX
j=1|aij−bij|2z2
j+2X
j̸=k|aij−bij||aik−bik|zjzk)
=EzNX
i=1NX
j=1|aij−bij|2z2
j+2NX
i=1X
j̸=k|aij−bij||aik−bik|zjzk
=NX
i=1NX
j=1|aij−bij|2Ezz2
j
+ 2NX
i=1X
j̸=k|aij−bij||aik−bik|Ezzjzk
=NX
i=1NX
j=1|aij−bij|2=Lfa(F1, F2; 2).
Proof of Theorem 4.1: Given a Gaussian matrix Zk=
[z1,···,zk]∈Rn×k,
Lffa,k(Θ) =1
kkX
l=1Lffa(F1, F2,zl).
For any fixed Θ,Lffa(F1, F2,zl),l= 1,···, k, are i.i.d
random variables. Suppose the first moment of each random
variable is finite, by the strong law of large numbers,
Lffa,k(Θ)converges to E[Lffa(F1, F2,z1)]almost surely. In
other words, limk→∞Lffa,k(Θ) = Lfa(Θ) with probability
1.
Proof of Proposition 4.2: By Chebyshev’s inequality,
we have
P Lffa,k(Θ)−E[Lffa,k(Θ)]> ϵ
≤
Var(Lffa,k(Θ))
ϵ2=Var(Lffa(F1, F2,z1))
ϵ2k.(18)
In order to estimate
Var(Lffa(F1, F2,z1) =
E[L2
ffa(F1, F2,z1)]− 
E[Lffa(F1, F2,z1)]2,(19)
it suffices to estimate
E[L2
ffa(F1, F2,z1)] =
Ez NX
i=1NX
j=1|aij−bij|2z2
j+NX
i=1X
j̸=k|aij−bij||aik−bik|zjzk2
which equals (as cross terms are zeros):
Ez NX
i=1NX
j=1|aij−bij|2z2
j2
+ NX
i=1X
j̸=k|aij−bij||aik−bik|zjzk2.Direct computation yields:
NX
i=1NX
j=1|aij−bij|4z4
j+NX
i=1NX
j=1NX
l̸=i|aij−bij|2|alj−blj|2z4
j
+2NX
i=1NX
j=1NX
l̸=j|aij−bij|2|ail−bil|2z2
jz2
l
+NX
i=1NX
j=1NX
k=1NX
l̸=j|aij−bij|2|akl−bkl|2z2
jz2
l
Notice that E[z4
i] = 3 . Taking E[·], we derive the upper bound
3∥Lfa∥4
2.
Proof of Theorem 4.2: Since lim
k→∞Lffa,k(Θ∗) =
Lfa(Θ∗), it suffices to show that
lim
k→∞inf
ΘLffa,k(Θ) = Lfa(Θ∗).
Note that
∀Θ,lim
k→∞Lffa,k(Θ) = Lfa(Θ)≤ Lfa(Θ∗).
Then,
Lfa(Θ∗)≥lim
k→∞inf
ΘLffa,k(Θ).
On the other hand, for arbitrary ϵ >0, we have:
∃N s.t. ∀k > N |Lffa,k(Θ)− Lfa(Θ)|<ϵ
2,∀Θ
and there exists a sequence {Θk}s.t.
Lffa,k(Θk)<inf
ΘLffa,k(Θ) +ϵ
2.
Note that |Lffa,k(Θk)− Lfa(Θk)|<ϵ
2fork > N , so:
Lfa(Θ∗)−ϵ≤ Lfa(Θk)−ϵ <inf
ΘLffa,k(Θ),∀k > N.
Since ϵis arbitrary, taking k→ ∞ , we have
Lfa(Θ∗)≤lim
k→∞inf
ΘLffa,k(Θ).
Proof of Corollary 4.2.1: For readability, we shorthand:
Lffa,k =fkandLfa=f. Let
H=∇2f
∇Θ∇ΘT≽0∈ Rn×n
be the Hessian matrix of FA loss, which is positive semi-
definite by convexity of Lfa. Then,
∇2fk
∇Θ∇ΘT=ZT
kHZk≽0∈Rk×k
which implies the convexity of fkfor all k. Moreover, it is
clear that fkis smooth for all ksince
∥∇fk(x)− ∇fk(y)∥=∥Zk(∇f(x)− ∇f(y))∥
≤L· ∥Zk∥ · ∥x−y∥.(20)
We note that fkis also smooth. Although we cannot claim
equi-smoothness since we cannot bound ∥Zk∥uniformly in k,
the above is sufficient for us to prove the desired result.

--- PAGE 9 ---
FIRST A. AUTHOR et al. : BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE 9
For∀k, given any initial parameters Θ0, by smoothness and
convexity of fk, it is well-known that
∥Θt
k−Θ∗
k∥ ≤ ∥ Θ0−Θ∗
k∥
where Θt
kis the parameter we arrive after tsteps of gradient
descent. Hence, we can pick a compact set K=BR(Θ∗)for
Rlarge enough such that {Θk}∞
k=1⊂K(denote Θ∗
∞= Θ∗).
Now, it’s suffices to prove fkconverges to funiformly on K.
In fact, fkconverges to fon any compact set. To begin with,
we state a known result from functional analysis ( [5], [12]):
Lemma 6.1: (Uniform boundedness and equi-Lipschitz) Let
Fbe a family of convex function on RnandK⊂Rnbe a
compact subset. Then, Fis equi-bounded and equi-Lipschitz
onK.
This result is established in any Banach space in [12], so it
automatically holds in finite dimensional Euclidean space. By
Lemma 6.1, we have that the sequence {fk}∞
k=1, where f∞=
f, is equi-Lipschitz. ∀>0,∃δ >0s.t.|fk(x)−fk(y)|< ϵ
for all kandx, y∈Kwhen|x−y|< δ. Since {B(x, δ)}x∈K
forms an open cover for K, we have a finite sub-cover
{B(xj, δ)}m
j=1ofK. Since there are finitely many points xj,
there exists Nϵsuch that
∀k > N ϵ,|fk(xj)−f(xj)|< ϵ, forj= 1,···, m.
For any x∈K,x∈B(xj∗, δ)for some j∗. For all k > N ϵ,
we have
|fk(x)−f(x)| ≤
|fk(x)−fk(xj∗)|+|fk(xj∗)−f(xj∗)|+|f(xj∗)−f(x)|
≤(2˜L+ 1)ϵ(21)
where ˜Lis the Lipschitz constant for equi-Lipschitz family.
Therefore, fkconverges to funiformly on K.REFERENCES
[1] Luca Becchetti, Marc Bury, Vincent Cohen-Addad, Fabrizio Grandoni,
and Chris Schwiegelshohn. Oblivious dimension reduction for k-means:
beyond subspaces and the Johnson-Lindenstrauss lemma. In Proceedings
of the 51st annual ACM SIGACT symposium on theory of computing ,
pages 1039–1050, 2019.
[2] Yoshua Bengio, Nicholas L ´eonard, and Aaron Courville. Estimating
or propagating gradients through stochastic neurons for conditional
computation. arXiv preprint arXiv:1308.3432 , 2013.
[3] Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos. Deep
learning with low precision by half-wave gaussian quantization. In
Proceedings of the IEEE conference on computer vision and pattern
recognition , pages 5918–5926, 2017.
[4] Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen
Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. Pact:
Parameterized clipping activation for quantized neural networks. arXiv
preprint arXiv:1805.06085 , 2018.
[5] S Cobzas. Lipschitz properties of convex mappings. Adv. Oper. Theory ,
2(1):21–49, 2017.
[6] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Bina-
ryconnect: Training deep neural networks with binary weights during
propagations. Advances in neural information processing systems , 28,
2015.
[7] Tim Dockhorn, Yaoliang Yu, Eyy ¨ub Sari, Mahdi Zolnouri, and Vahid
Partovi Nia. Demystifying and generalizing binaryconnect. Advances in
Neural Information Processing Systems , 34:13202–13216, 2021.
[8] Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng
Hu, Jiazhen Lin, Fengwei Yu, and Junjie Yan. Differentiable soft
quantization: Bridging full-precision and low-bit neural networks. In
Proceedings of the IEEE/CVF International Conference on Computer
Vision , pages 4852–4861, 2019.
[9] Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng
Hu, Jiazhen Lin, Fengwei Yu, and Junjie Yan. Differentiable soft
quantization: Bridging full-precision and low-bit neural networks. In
Proceedings of the IEEE/CVF International Conference on Computer
Vision , pages 4852–4861, 2019.
[10] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowl-
edge in a neural network. arXiv preprint arXiv:1503.02531 , 2(7), 2015.
[11] William B Johnson, Joram Lindenstrauss, and Gideon Schechtman.
Extensions of Lipschitz maps into Banach spaces. Israel Journal of
Mathematics , 54(2):129–138, 1986.
[12] Mohamed Jouak and Lionel Thibault. Equicontinuity of families of con-
vex and concave-convex operators. Canadian Journal of Mathematics ,
36(5):883–898, 1984.
[13] Jangho Kim, Yash Bhalgat, Jinwon Lee, Chirag Patel, and Nojun
Kwak. Qkd: Quantization-aware knowledge distillation. arXiv preprint
arXiv:1911.12491 , 2019.
[14] Taehyeon Kim, Jaehoon Oh, NakYil Kim, Sangwook Cho, and Se-Young
Yun. Comparing kullback-leibler divergence and mean squared error loss
in knowledge distillation. pages 2628–2635, 2021.
[15] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang,
Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit
of post-training quantization by block reconstruction. arXiv preprint
arXiv:2102.05426 , 2021.
[16] Zhijian Li, Bao Wang, and Jack Xin. An integrated approach to produce
robust deep neural network models with high efficiency. In International
Conference on Machine Learning, Optimization, and Data Science ,
pages 451–465. Springer, 2021.
[17] Ziang Long, Penghang Yin, and Jack Xin. Recurrence of optimum
for training weight and activation quantized networks. Applied and
Computational Harmonic Analysis , 62:41–65, 2023.
[18] Konstantin Makarychev, Yury Makarychev, and Ilya Razenshteyn. Per-
formance of Johnson-Lindenstrauss transform for k-means and k-
medians clustering. In Proceedings of the 51st Annual ACM SIGACT
Symposium on Theory of Computing , pages 1027–1038, 2019.
[19] Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression
via distillation and quantization. arXiv preprint arXiv:1802.05668 , 2018.
[20] Dinesh Ramasamy and Upamanyu Madhow. Compressive spectral
embedding: sidestepping the svd. Advances in neural information
processing systems , 28, 2015.
[21] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali
Farhadi. Xnor-net: Imagenet classification using binary convolutional
neural networks. In European conference on computer vision , pages
525–542. Springer, 2016.

--- PAGE 10 ---
10
[22] Nicolas Tremblay, Gilles Puy, R ´emi Gribonval, and Pierre Van-
dergheynst. Compressive spectral clustering. In International conference
on machine learning , pages 1002–1011. PMLR, 2016.
[23] Santosh S Vempala. The random projection method , volume 65.
American Mathematical Soc., 2005.
[24] Li Wang, Dong Li, Yousong Zhu, Lu Tian, and Yi Shan. Dual super-
resolution learning for semantic segmentation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pages 3774–3783, 2020.
[25] Biao Yang, Fanghui Xue, Yinyong Qi, and Jack Xin. Improving effi-
cient semantic segmentation networks by enhancing multi-scale feature
representation via resolution path based knowledge distillation and pixel
shuffle. In Proceedings of the 16th International Symposium on Visual
Computing , pages 325–336. Springer, Cham, 2021.
[26] Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A gift from
knowledge distillation: Fast optimization, network minimization and
transfer learning. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pages 4133–4141, 2017.
[27] Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong
Qi, and Jack Xin. Understanding straight-through estimator in train-
ing activation quantized neural nets. in Proceedings of International
Conference on Learning Representations (ICLR) , 2019.
[28] Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong
Qi, and Jack Xin. Binaryrelax: A relaxation approach for training deep
neural networks with quantized weights. SIAM Journal on Imaging
Sciences , 11(4):2205–2223, 2018.
[29] Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong
Qi, and Jack Xin. Blended coarse gradient descent for full quantization
of deep neural networks. Research in the Mathematical Sciences , 6(1):1–
23, 2019.
[30] Sergey Zagoruyko and Nikos Komodakis. Paying more attention to
attention: Improving the performance of convolutional neural networks
via attention transfer. arXiv preprint arXiv:1612.03928 , 2016.
[31] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent
El Ghaoui, and Michael Jordan. Theoretically principled trade-off
between robustness and accuracy. In International conference on
machine learning , pages 7472–7482. PMLR, 2019.
