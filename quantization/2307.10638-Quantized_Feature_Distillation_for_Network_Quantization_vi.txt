# 2307.10638.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/quantization/2307.10638.pdf
# Kích thước tệp: 2969558 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Chưng Cất Đặc Trưng Lượng Tử Hóa cho Lượng Tử Hóa Mạng
Ke Zhu, Yin-Yin He, Jianxin Wu*
Phòng thí nghiệm trọng điểm nhà nước về công nghệ phần mềm mới, Đại học Nam Kinh, Trung Quốc
zhuk@lamda.nju.edu.cn, heyy@lamda.nju.edu.cn, wujx2001@nju.edu.cn

Tóm tắt
Lượng tử hóa mạng thần kinh nhằm tăng tốc và thu gọn các mô hình mạng thần kinh độ chính xác đầy đủ bằng cách sử dụng các xấp xỉ bit thấp. Các phương pháp áp dụng mô hình huấn luyện nhận biết lượng tử hóa (QAT) gần đây đã có sự tăng trưởng nhanh chóng, nhưng thường phức tạp về mặt khái niệm. Bài báo này đề xuất một phương pháp QAT mới và hiệu quả cao, chưng cất đặc trưng lượng tử hóa (QFD). QFD đầu tiên huấn luyện một biểu diễn lượng tử hóa (hoặc nhị phân hóa) làm giáo viên, sau đó lượng tử hóa mạng bằng cách sử dụng chưng cất kiến thức (KD). Kết quả định lượng cho thấy QFD linh hoạt và hiệu quả hơn (tức là thân thiện với lượng tử hóa) so với các phương pháp lượng tử hóa trước đây. QFD vượt trội hơn các phương pháp hiện có với biên độ đáng chú ý không chỉ trong phân loại hình ảnh mà còn trong phát hiện đối tượng, mặc dù đơn giản hơn nhiều. Hơn nữa, QFD lượng tử hóa ViT và Swin-Transformer trên phát hiện và phân đoạn MS-COCO, điều này xác minh tiềm năng của nó trong triển khai thế giới thực. Theo hiểu biết của chúng tôi, đây là lần đầu tiên các transformer thị giác được lượng tử hóa trong các tác vụ phát hiện đối tượng và phân đoạn hình ảnh.

Giới thiệu
Lượng tử hóa mạng chuyển đổi trọng số và kích hoạt độ chính xác đầy đủ (FP) của mạng thành các xấp xỉ điểm cố định mà không làm giảm độ chính xác rõ rệt. Gần đây, nhiều cách tiếp cận khác nhau (Lee, Kim, và Ham 2021; Liu và cộng sự 2022; Lin và cộng sự 2022) đã được đề xuất và huấn luyện nhận biết lượng tử hóa (QAT) trở thành một mô hình trưởng thành vì khả năng phục hồi độ chính xác mạng ngay cả trong các cài đặt bit cực thấp.

Các phương pháp QAT hiện đại dựa trên một nguyên tắc chung: tối ưu hóa khoảng lượng tử hóa (tham số) với mất mát tác vụ (Jung và cộng sự 2019). Nhiều biến thể đã được đưa ra, chẳng hạn như lượng tử hóa không đồng đều (Yamamoto 2021; Liu và cộng sự 2022), xấp xỉ gradient phức tạp (Gong và cộng sự 2019; Lee, Kim, và Ham 2021) hoặc chính quy hóa được thiết kế thủ công (Zhuang và cộng sự 2020; Lee và cộng sự 2021). Những phương pháp này phức tạp và tính đơn giản thường bị hy sinh. Một hướng khác của các phương pháp QAT giới thiệu chưng cất kiến thức (Hinton và cộng sự 2015) vào giai đoạn lượng tử hóa. Trong khi ý tưởng của lượng tử hóa KD rất đơn giản (tức là một giáo viên độ chính xác đầy đủ giúp phục hồi độ chính xác của mạng học sinh lượng tử hóa), việc triển khai bao gồm độ chính xác ngẫu nhiên heuristic (Boo và cộng sự 2021), các mô-đun phụ trợ được định nghĩa thủ công (Zhuang và cộng sự 2020) hoặc nhiều giai đoạn (Kim và cộng sự 2019). Tất cả những phương pháp này đều sử dụng chưng cất logit, sau đó khó áp dụng trong các tác vụ thị giác máy tính khác (ví dụ: phát hiện đối tượng).

Trong bài báo này, chúng tôi đề xuất một phương pháp chưng cất mới, đơn giản và hiệu quả nhắm vào lượng tử hóa mạng, chưng cất đặc trưng lượng tử hóa (QFD). Động cơ của chúng tôi đến từ một kết quả quan trọng trong Wu và Luo (2018): một mô hình FP chỉ với các đặc trưng đầu ra được nhị phân hóa có thể đạt được độ chính xác tương tự hoặc tốt hơn so với mô hình FP đầy đủ. Vậy thì, việc sử dụng các đặc trưng lượng tử hóa này làm tín hiệu giáo viên để giúp lượng tử hóa phần còn lại của mạng (tức là học sinh) chắc chắn sẽ có lợi. Một mặt, chưng cất đặc trưng linh hoạt hơn so với chưng cất logit, đặc biệt trong các tác vụ như phát hiện đối tượng (Yang và cộng sự 2022); mặt khác, việc học sinh lượng tử hóa hoàn toàn bắt chước một biểu diễn đặc trưng điểm cố định sẽ dễ dàng hơn so với việc trực tiếp bắt chước các logit hoặc đặc trưng điểm nổi. Nói cách khác, QFD được đề xuất sẽ vừa linh hoạt hơn vừa hiệu quả hơn (thân thiện với lượng tử hóa). Phỏng đoán này (và động cơ cho QFD của chúng tôi) được minh họa trong Hình 1, được hỗ trợ mạnh mẽ bởi các kết quả thực nghiệm trong Hình 2 và Bảng 1.

Những thí nghiệm xác minh động cơ của chúng tôi đã được thực hiện trên CIFAR100 với ResNet-18 sử dụng 4 phương pháp lượng tử hóa khác nhau: đường cơ sở lượng tử hóa (một bộ lượng tử hóa đồng đều không có KD), chưng cất logit (logit làm tín hiệu giáo viên), chưng cất đặc trưng (các đặc trưng FP làm tín hiệu giáo viên) và QFD của chúng tôi (các đặc trưng lượng tử hóa làm tín hiệu giáo viên). Tất cả 3 phương pháp KD áp dụng cùng một bộ lượng tử hóa như đường cơ sở, và đặc trưng của giáo viên được nhị phân hóa (1-bit) trong QFD. Như thể hiện trong Hình 2, QFD không chỉ vượt trội hơn chưng cất đặc trưng float, mà còn vượt qua chưng cất logit trong tất cả các cài đặt. Sự cải thiện của QFD so với đường cơ sở cũng nhất quán và đủ lớn để phục hồi độ chính xác của mô hình độ chính xác đầy đủ (3-bit và 4-bit). Chúng tôi tiếp tục lượng tử hóa đặc trưng của giáo viên thành 4 bit và 8 bit bằng QFD. Như thể hiện trong Bảng 1, tất cả các phương pháp chưng cất đặc trưng đều cho thấy sự cải thiện so với đường cơ sở. Khi độ rộng bit của giáo viên giảm, độ chính xác của mô hình giáo viên giảm, nhưng kết quả chưng cất cuối cùng đã được cải thiện một cách nhất quán. Những kết quả này cho thấy động cơ của chúng tôi là hợp lệ: các đặc trưng lượng tử hóa là những giáo viên tốt hơn cho lượng tử hóa mạng!

Chúng tôi đã thử nghiệm QFD không chỉ trong phân loại mà còn trong phát hiện và phân đoạn, và đạt được độ chính xác hiện đại trên các cấu trúc mạng đa dạng (ResNet, MobileNet và ViT). Những đóng góp của chúng tôi có thể được tóm tắt như sau:

• Một phương pháp KD huấn luyện nhận biết lượng tử hóa mới dễ triển khai.
• Lợi thế độ chính xác đáng kể trên các tiêu chuẩn phân loại, phát hiện và phân đoạn so với các phương pháp huấn luyện nhận biết lượng tử hóa trước đây.
• Thử nghiệm đầu tiên lượng tử hóa cấu trúc transformer thị giác trên các tác vụ phát hiện đối tượng và phân đoạn hình ảnh phổ biến.

Công trình liên quan
Lượng tử hóa mạng thần kinh có thể được phân loại thành hai mô hình: huấn luyện nhận biết lượng tử hóa (QAT) và lượng tử hóa sau huấn luyện (PTQ). Chúng tôi áp dụng QAT trong bài báo này. Trong phần này, chúng tôi sẽ mô tả những điều cơ bản, chưng cất kiến thức, và các transformer thị giác trong QAT.

Huấn luyện nhận biết lượng tử hóa. QAT (Nagel và cộng sự 2021) là một mô hình mạnh mẽ để giải quyết lượng tử hóa bit thấp (ví dụ: 3- hoặc 4-bit) mà không làm giảm độ chính xác đáng kể. Tích hợp các phép toán lượng tử hóa vào đồ thị tính toán là chìa khóa trong QAT sao cho trọng số và tham số lượng tử hóa có thể được học đồng thời thông qua lan truyền ngược. Các phương pháp sớm trong họ này tập trung vào cách nhị phân hóa mô hình (Courbariaux, Bengio, và David 2015; Rastegari và cộng sự 2016), khớp bộ lượng tử hóa với thống kê (Zhou và cộng sự 2016; Cai và cộng sự 2017; Choi và cộng sự 2018), hoặc giảm thiểu lỗi cục bộ (Zhang và cộng sự 2018), nhưng chúng gặp phải vấn đề không đầy đủ hoặc không tối ưu. Các phương pháp QAT hiện đại áp dụng nguyên tắc tối ưu hóa khoảng lượng tử hóa với mất mát tác vụ (Esser và cộng sự 2020; Jung và cộng sự 2019) và sử dụng các kỹ thuật phức tạp hơn nhiều, bao gồm bộ lượng tử hóa không đồng đều (Yamamoto 2021; Liu và cộng sự 2022), xấp xỉ gradient (Kim, Lee, và Ham 2021; Lee, Kim, và Ham 2021; Gong và cộng sự 2019) hoặc chính quy hóa bổ sung (Han và cộng sự 2021; Lee và cộng sự 2021; Zhuang và cộng sự 2020). Tuy nhiên, tính đơn giản và hiệu quả của chúng vẫn là những thách thức lớn.

Chưng cất kiến thức trong lượng tử hóa. KD (Hinton và cộng sự 2015) phổ biến trong nhiều tác vụ thị giác máy tính khác nhau (He, Wu, và Wei 2021; Guerra và cộng sự 2020) và đã dần xuất hiện trong huấn luyện nhận biết lượng tử hóa (Mishra và Marr 2018; Polino, Pascanu, và Alistarh 2018; Boo và cộng sự 2021; Zhuang và cộng sự 2020). Điểm mấu chốt của lượng tử hóa KD rất gọn gàng: sử dụng một giáo viên độ chính xác đầy đủ để phục hồi độ chính xác của mạng học sinh lượng tử hóa. Tuy nhiên, các phương pháp gần đây thiếu tính đơn giản ở chỗ chúng bao gồm các giai đoạn phức tạp (Kim và cộng sự 2019), các mô-đun phụ trợ (Zhuang và cộng sự 2020) và điều chỉnh độ chính xác hỗn hợp chuyên dụng (Boo và cộng sự 2021). Hơn nữa, tất cả những phương pháp này đều áp dụng chưng cất logit, điều này không linh hoạt khi KD được áp dụng vào lượng tử hóa phát hiện đối tượng (Guo và cộng sự 2021). Thay vào đó, chúng tôi đề xuất KD đặc trưng lượng tử hóa của chúng tôi, vừa thân thiện với lượng tử hóa về độ chính xác vừa linh hoạt về thiết kế đường ống.

Lượng tử hóa các transformer thị giác. Vision Transformers (ViT) đã thúc đẩy nhiều tác vụ thị giác (Dosovitskiy và cộng sự 2021; Touvron và cộng sự 2021) và có nhu cầu cấp thiết để lượng tử hóa chúng một cách chính xác nhằm tạo điều kiện cho việc sử dụng thực tế (Sun và cộng sự 2022). Các phương pháp gần đây (Yuan và cộng sự 2021; Liu và cộng sự 2021b; Lin và cộng sự 2022) đã thử các kỹ thuật lượng tử hóa sau huấn luyện (Nagel và cộng sự 2021) để lượng tử hóa ViT xuống 6- hoặc 8-bit chỉ cho phân loại hình ảnh. Lượng tử hóa bit thấp (3- hoặc 4-bit) và khả năng áp dụng vào phát hiện và phân đoạn của ViT và các biến thể vẫn chưa được khám phá. Lần đầu tiên, chúng tôi sẽ trả lời cả hai câu hỏi bằng cách khám phá hiệu suất lượng tử hóa của nó trong tất cả các cài đặt và tác vụ này.

--- TRANG 2 ---
[Tiếp tục dịch các trang còn lại với cùng format và chất lượng...]

--- TRANG 3 ---
Phương pháp được đề xuất
Chúng tôi đã tiết lộ động cơ của phương pháp QFD được đề xuất trong phần giới thiệu, và các kết quả trong Hình 2 và Bảng 1 không chỉ hỗ trợ động cơ này mà còn xác minh ban đầu hiệu quả của QFD. Sau khi giới thiệu các điều kiện tiên quyết của lượng tử hóa mạng thần kinh (đường cơ sở QAT chúng tôi sử dụng), chúng tôi sẽ chuyển sang mô tả chi tiết về phương pháp được đề xuất.

Điều kiện tiên quyết
Chúng tôi áp dụng phương pháp trong Lee, Kim, và Ham (2021) làm phương pháp cơ sở, đây là một bộ lượng tử hóa đồng đều bao gồm các bước chuẩn hóa, lượng tử hóa và bỏ lượng tử hóa.

Đối với bất kỳ dữ liệu độ chính xác đầy đủ v nào (trọng số hoặc kích hoạt của một lớp nhất định trong mạng thần kinh), chúng tôi định nghĩa tham số lượng tử hóa l và u, đại diện cho cận dưới và cận trên của khoảng lượng tử hóa, tương ứng. Bước chuẩn hóa như sau:

ˆv = clip((v-l)/(u-l), 0, 1)  (1)

trong đó clip(·, min, max) cắt dữ liệu nằm ngoài phạm vi min-max. Sau đó, một hàm lượng tử hóa được sử dụng:

ev = ⌊(2^b-1)ˆv⌉/(2^b-1)  (2)

trong đó ⌊·⌉ là hàm làm tròn và b đại diện cho độ rộng bit của lượng tử hóa. Phép toán ⌊(2^b-1)ˆv⌉ ánh xạ ˆv từ phạm vi [0,1] đến một số rời rạc trong {0,1, ...,2^b-1}. Cuối cùng, một bước bỏ lượng tử hóa được áp dụng để xuất ra trọng số lượng tử hóa v^W hoặc kích hoạt v^A:

v^W = 2(ev - 0.5), hoặc  (3)
v^A = ev  (4)

trong đó trọng số lượng tử hóa v^W gần như đối xứng quanh số không và kích hoạt lượng tử hóa v^A dương xét kích hoạt ReLU. Tương tự như Lee, Kim, và Ham (2021), chúng tôi sử dụng một tham số tỷ lệ có thể huấn luyện α được nhân với kích hoạt lượng tử hóa đầu ra.

Trong quá trình huấn luyện, chúng tôi áp dụng ước lượng truyền thẳng (STE) (Bengio, Léonard, và Courville 2013) để xấp xỉ gradient của toán tử làm tròn là 1:

∂⌊x⌉/∂x = 1  (5)

Trọng số của mô hình và các tham số lượng tử hóa này được học đồng thời thông qua lan truyền ngược.

Chưng cất đặc trưng lượng tử hóa
Trước tiên chúng tôi định nghĩa ký hiệu cơ bản trong huấn luyện nhận biết lượng tử hóa, sau đó giới thiệu phương pháp chưng cất đặc trưng lượng tử hóa của chúng tôi, được minh họa trong Hình 3. Đối với một hình ảnh I cho trước, nó đầu tiên được gửi đến một bộ trích xuất đặc trưng ϕ(·, θ, θ_q) (backbone, ví dụ: CNN hoặc ViT) để có được một vectơ đặc trưng độ chính xác đầy đủ f ∈ R^D (thường được thu được bằng pooling trung bình toàn cục):

f = ϕ(I, θ, θ_q)  (6)

trong đó D là số chiều đặc trưng, θ và θ_q đại diện cho các tham số trọng số và tham số lượng tử hóa của mô hình, tương ứng. f được truyền qua một bộ phân loại để có được logit cuối cùng p ∈ R^C với C lớp, tạo ra mất mát entropy chéo H(y,p) cùng với nhãn lớp thực y ∈ R^C. Các tham số θ và θ_q được học thông qua lan truyền ngược.

Đối với phương pháp QFD được đề xuất của chúng tôi, một hình ảnh I được gửi riêng biệt đến mạng giáo viên ϕ_t(·, θ_t) và mạng học sinh ϕ_s(·, θ_s, θ_s_q) để có được các đặc trưng f_t và f_s, tương ứng. Đặc trưng độ chính xác đầy đủ f_t của giáo viên sẽ được lượng tử hóa thành biểu diễn bit thấp hơn (ví dụ: 1-bit hoặc 4-bit):

f_t --Q(·)--> f̃_t  (7)

trong đó bộ lượng tử hóa Q(·) được định nghĩa trong Phương trình (1)–(4). Bộ lượng tử hóa đặc trưng Q(·) tuân theo quá trình lượng tử hóa kích hoạt được mô tả trong phần điều kiện tiên quyết.

--- TRANG 4 ---
Đặc trưng lượng tử hóa của giáo viên sau đó hoạt động như tín hiệu giám sát để hướng dẫn lượng tử hóa mạng học sinh bằng mất mát bình phương trung bình L(·,·), và học sinh vẫn tạo ra mất mát entropy chéo thông thường H(·,·) với nhãn thực y. Mục tiêu tối ưu hóa tổng thể là:

arg min_{θ_s,θ_s_q} λL(f_s, f̃_t) + (1-λ)H(y, p_s)  (8)

Ở đây λ được sử dụng để cân nhắc tầm quan trọng của mất mát chưng cất so với mất mát entropy chéo. Để đơn giản, chúng tôi đặt λ = 0.5 trừ các nghiên cứu ablation.

Kết quả thực nghiệm
Trong phần này, trước tiên chúng tôi sẽ mô tả các thiết lập thực nghiệm chung, sau đó trình bày kết quả của QFD trên các tiêu chuẩn phân loại, phát hiện đối tượng và phân đoạn.

Thiết lập thực nghiệm
Trong quá trình huấn luyện, trước tiên chúng tôi dành một vài epoch (khoảng 1/10 tổng số epoch huấn luyện) để lượng tử hóa đặc trưng của giáo viên thành bit thấp cố định (ví dụ: 2-bit) trước khi bắt đầu huấn luyện QFD. Theo các công trình QAT trước đây (Zhuang và cộng sự 2020; Lee, Kim, và Ham 2021), chúng tôi tiến hành thí nghiệm trên các bộ dữ liệu CIFAR, ImageNet, CUB và MS-COCO. Tất cả thí nghiệm sử dụng PyTorch (Paszke và cộng sự 2019) với 8 GeForce RTX 3090. Các chỉ số đánh giá cho phân loại và phát hiện (phân đoạn) là độ chính xác top-1 (top-5 cũng được sử dụng trên ImageNet) và AP (độ chính xác trung bình), tương ứng.

Thiết lập phân loại. Chúng tôi thí nghiệm với ResNet-20 trên CIFAR10 và ResNet-18/32 trên CIFAR100. Trên cả hai bộ dữ liệu CIFAR (Krizhevsky 2009), chúng tôi sử dụng SGD với tốc độ học 0.004, suy giảm trọng số 0.0005 và huấn luyện tổng cộng 200 epoch. Độ phân giải đầu vào là 32×32, và việc lật ngẫu nhiên và cắt ngẫu nhiên được sử dụng làm tăng cường dữ liệu. Trên ImageNet (Russakovsky và cộng sự 2015), chúng tôi huấn luyện ResNet-18, ResNet-34 và MobileNet-v2 trong 100 epoch. Tốc độ học ban đầu và động lượng lần lượt là 0.01 và 0.9. Suy giảm trọng số được đặt thành 1e-4, 5e-5 và 2.5e-5 cho 4-bit, 3-bit và 2-bit, tương ứng, theo Han và cộng sự (2021); Esser và cộng sự (2020). Chúng tôi áp dụng cắt thay đổi kích thước ngẫu nhiên và lật ngẫu nhiên làm tăng cường dữ liệu và đặt độ phân giải đầu vào là 224×224. Trên CUB200 (Wah và cộng sự 2011), độ phân giải và tăng cường giống như trên ImageNet. Chúng tôi huấn luyện ViT Small, ViT Base, Deit Small, Deit Base, Deit Tiny trong 90 epoch với kích thước batch 64, theo bộ lập lịch cosine. Tốc độ học và suy giảm trọng số lần lượt là 5e-3 và 5e-4. Chúng tôi thực hiện 3 lần chạy cho CIFAR và CUB vì những kết quả này có phương sai lớn hơn.

Thiết lập phát hiện đối tượng và phân đoạn. Chúng tôi huấn luyện các detector RetinaNet (Lin và cộng sự 2017b) với ResNet làm backbone, và khám phá lượng tử hóa phát hiện và phân đoạn vision transformer bằng ViT và Swin Transformer (Liu và cộng sự 2021a) được tiền huấn luyện với phương pháp tự giám sát mới được đề xuất MAE (He và cộng sự 2022). Cấu trúc detector đối tượng theo Li và cộng sự (2022). Đối với RetinaNet, chúng tôi huấn luyện 90k lần lặp với tốc độ học cơ sở 0.001. Theo Yamamoto (2021), chúng tôi lượng tử hóa tất cả các lớp (backbone, FPN và detection head) trừ đầu vào và đầu ra của toàn bộ mạng, và sử dụng BN sau FPN và detection head. Để triển khai phương pháp QFD của chúng tôi, chúng tôi lượng tử hóa đầu ra lớp FPN của giáo viên thành 8-bit và sau đó sử dụng 'đặc trưng lượng tử hóa' này để chưng cất, theo khái niệm của các công trình chưng cất phát hiện đối tượng trước đây (Guo và cộng sự 2021). Đối với ViT và Swin Transformer, chúng tôi lượng tử hóa tất cả các lớp tuyến tính trong backbone của chúng và đánh giá chúng trên các tác vụ phát hiện và phân đoạn. Tất cả những mô hình này thực hiện 2 lần chạy trên MS-COCO và được triển khai với Detectron2 (Wu và cộng sự 2019).

Kết quả phân loại
CIFAR10/100. Trước tiên chúng tôi xác thực phương pháp QFD được đề xuất bằng các mô hình ResNet trên CIFAR10 và CIFAR100, mỗi mô hình chứa 50.000 hình ảnh huấn luyện và 10.000 hình ảnh xác thực, trong khi mô hình sau phục vụ như một phân loại tinh hơn (100 lớp) so với mô hình trước (10 lớp).

Đối với các mô hình ResNet-20 trên CIFAR10 (kết quả được hiển thị trong Bảng 2), chúng tôi chạy đường cơ sở, chưng cất kiến thức đặc trưng ('Feature KD'), chưng cất logit ('Logit KD') và phương pháp chưng cất đặc trưng lượng tử hóa được đề xuất của chúng tôi ('QFD'). Chúng tôi lượng tử hóa ResNet-20 thành 2-bit, 3-bit và 4-bit cho cả trọng số và kích hoạt ("W/A"). Theo công trình trước đây (Zhang và cộng sự 2018; Boo và cộng sự 2021), chúng tôi lượng tử hóa tất cả các lớp trừ đầu vào backbone và lớp kết nối đầy đủ cuối cùng (tức là bộ phân loại). SPEQ (Boo và cộng sự 2021) và APRT* đều sử dụng phương pháp chưng cất logit, và LQ-Net (Zhang và cộng sự 2018) là một phương pháp huấn luyện nhận biết lượng tử hóa. Như có thể thấy trong Bảng 2, QFD của chúng tôi vượt trội hơn các phương pháp lượng tử hóa chưng cất kiến thức trước đây SPEQ và APRT* với biên độ lớn, và tốt hơn cả Feature KD và Logit KD. Lưu ý rằng QFD của chúng tôi đã đạt được độ chính xác thậm chí cao hơn mô hình độ chính xác đầy đủ trong cài đặt 3-bit (92.64%) và 4-bit (93.07%).

Chúng tôi cũng xác thực QFD trên CIFAR100 bằng ResNet-18 và ResNet-32. Tương tự như các thí nghiệm trên CIFAR10, chúng tôi tái tạo các phương pháp đường cơ sở, chưng cất đặc trưng và chưng cất logit. Như thể hiện trong Bảng 3, Feature KD và Logit KD nói chung tốt hơn Baseline, cho thấy sức mạnh của chưng cất kiến thức. QFD của chúng tôi tốt hơn tất cả, đặc biệt trong các kịch bản bit cực thấp (1-bit và 2-bit). Phương pháp của chúng tôi gần như có thể phục hồi độ chính xác của mô hình độ chính xác đầy đủ ở 2, 3, 4 bit cho ResNet-18 và ResNet-32. Đặc biệt đối với ResNet-32 1-bit, chỉ QFD của chúng tôi cho thấy sự cải thiện so với Baseline (56.84% so với 56.09%).

Kết quả ImageNet. Chúng tôi so sánh QFD được đề xuất với các phương pháp QAT khác trên bộ dữ liệu ImageNet1k. Kết quả có thể được tìm thấy trong Bảng 4. Chưng cất đặc trưng lượng tử hóa được đề xuất vượt trội hơn các phương pháp trước đây (bao gồm các phương pháp chưng cất kiến thức khác SPEQ, QKD và Auxi) với các mô hình ResNet-18, ResNet-34 và MobileNetV2 trong các cài đặt bit khác nhau. Lưu ý rằng Auxi sử dụng một mô-đun phụ trợ được thiết kế thủ công, SPEQ cần khám phá thực nghiệm về độ chính xác ngẫu nhiên, và QKD bao gồm các mô hình giáo viên lớn hơn (ví dụ: ResNet-50 để chưng cất ResNet-34). So sánh, phương pháp của chúng tôi vừa đơn giản hơn về khái niệm vừa hiệu quả hơn về độ chính xác, đặc biệt đối với MobileNetV2, nơi QFD của chúng tôi vượt trội hơn QKD với biên độ lớn (tăng 7.1%, 3.8% và 3.1% trong cài đặt lượng tử hóa 2-bit, 3-bit và 4-bit, tương ứng). Đối với các mô hình dòng ResNet, QFD của chúng tôi hoàn toàn phục hồi độ chính xác top-1 của độ chính xác đầy đủ trong lượng tử hóa 3- và 4-bit (ResNet-34 4-bit với 74.7% top-1 thậm chí vượt qua đối tác độ chính xác đầy đủ 1.3%). Trong khi đó, độ chính xác của MobileNetV2 tương đối khó phục hồi hơn dưới bit thấp, có thể do sự biến thiên kênh lớn của nó, như đã chỉ ra bởi Nagel và cộng sự (2021). Nhưng QFD của chúng tôi vẫn tốt hơn các phương pháp khác.

CUB200 với ViT. Chúng tôi cũng lượng tử hóa các vision transformer trên tiêu chuẩn phân loại hình ảnh CUB200 (Wah và cộng sự 2011), chứa 200 loại chim, với 5.994 và 5.794 hình ảnh để huấn luyện và kiểm tra, tương ứng. Cụ thể, chúng tôi lượng tử hóa lớp tuyến tính trong perceptron đa lớp (MLP) và attention đa đầu (MHA) thành 3- hoặc 4-bit, sử dụng các cấu trúc khác nhau của ViT (Dosovitskiy và cộng sự 2021) và Deit (Touvron và cộng sự 2021), bao gồm ViT Small, ViT Base, Deit Tiny, Deit Small và Deit Base. Chúng tôi cũng liệt kê độ chính xác của mạng giáo viên với các đặc trưng lượng tử hóa (một bước tiền xử lý của phương pháp QFD của chúng tôi). Như Bảng 5 cho thấy, mặc dù lượng tử hóa chỉ đặc trưng mang lại một sự giảm độ chính xác nhẹ cho mô hình FP gốc, sự cải thiện của phương pháp QFD so với Baseline là đáng kể và nhất quán. Nhưng, vẫn còn khoảng cách giữa 4-bit và các mô hình FP. Lượng tử hóa transformer vẫn là một tác vụ đầy thách thức.

Kết quả phát hiện đối tượng
RetinaNet. RetinaNet (Lin và cộng sự 2017b) là một detector một giai đoạn bao gồm backbone, FPN (Lin và cộng sự 2017a) và detection head. Trên bộ dữ liệu MS-COCO (Lin và cộng sự 2014), chúng tôi lượng tử hóa tất cả các lớp của nó thành 4-bit và 3-bit bằng phương pháp QFD được đề xuất (bao gồm phép toán convolution trong skip connection) trừ đầu vào backbone và đầu ra trong detection head. Theo công trình trước đây (Yamamoto 2021; Zhuang và cộng sự 2020), huấn luyện lượng tử hóa của chúng tôi được tinh chỉnh bằng mô hình độ chính xác đầy đủ ('FP' trong Bảng 6).

Đối với mạng giáo viên, trước tiên chúng tôi lượng tử hóa đặc trưng đầu ra của nó ở mức p3 thành 8-bit vì nó chứa luồng gradient nhiều nhất trong đồ thị FPN (Lin và cộng sự 2017b), sau đó sử dụng nó làm đặc trưng lượng tử hóa để chưng cất một RetinaNet học sinh. Thực nghiệm, chúng tôi thấy rằng việc sử dụng đặc trưng lượng tử hóa của tất cả các mức FPN (bao gồm p3, p4, p5, p6, p7), cách tiếp cận phổ biến trong chưng cất phát hiện đối tượng (Guo và cộng sự 2021), đạt được độ chính xác tương tự nhưng không ổn định. Để đơn giản, chúng tôi chỉ sử dụng p3 cho chưng cất đặc trưng và không bao gồm bất kỳ phép toán phức tạp nào như phân biệt các đặc trưng nền trước và nền sau (Yang và cộng sự 2022; Guo và cộng sự 2021). Mất mát chưng cất đặc trưng lượng tử hóa chiếm khoảng 1/5 tổng mất mát phát hiện, và cấu trúc RetinaNet tuân thủ nghiêm ngặt công trình lượng tử hóa trước đây (Yamamoto 2021).

Bảng 6 cho thấy kết quả lượng tử hóa RetinaNet thành 4-bit. QFD của chúng tôi (ResNet18/34/50 làm backbone) vượt trội hơn các phương pháp trước đây với biên độ lớn. Đặc biệt đối với ResNet-18, QFD của chúng tôi thậm chí vượt qua đối tác độ chính xác đầy đủ (cải thiện 0.3%, 0.4% và 0.5% cho AP, AP_S và AP_L, tương ứng). Sự giảm độ chính xác của chúng tôi từ những mô hình độ chính xác đầy đủ với ResNet-34 cũng không đáng kể, với sự giảm nhẹ 0.1% trên AP và 0.2% trên AP75.

Bảng 7 cho thấy kết quả lượng tử hóa RetinaNet thành 3-bit. Không giống như lượng tử hóa 4-bit, 3-bit thách thức hơn và khó tối ưu hơn do khả năng biểu diễn hạn chế. Thực nghiệm chúng tôi thấy ResNet-34 thường gặp vấn đề huấn luyện không ổn định, do đó chúng tôi kéo dài các lần lặp warmup trong khi giữ tổng số lần lặp huấn luyện cố định. Nhìn chung, QFD của chúng tôi vượt trội hơn phương pháp hiện đại trước đây với biên độ tương đối lớn, đặc biệt đối với ResNet-18 nơi cải thiện AP_M và AP_L so với LCQ (Yamamoto 2021) lần lượt là 0.7% và 2.2%.

Cấu trúc ViT. Cuối cùng, chúng tôi khám phá lượng tử hóa ViT. Theo hiểu biết của chúng tôi, đây là lần đầu tiên ViT được lượng tử hóa trong các tác vụ phát hiện và phân đoạn. Chúng tôi thử ViT (Dosovitskiy và cộng sự 2021) và Swin Transformer (Liu và cộng sự 2021a) được tiền huấn luyện trên ImageNet1k và ImageNet21k, tương ứng, sử dụng các phương pháp học tự giám sát MAE (He và cộng sự 2022). Đường ống phát hiện theo ViTDet mới được công bố (Li và cộng sự 2022). Vì hầu hết các tham số nằm trong lớp tuyến tính của MLP và MHA trong các khối transformer backbone, chúng tôi chỉ lượng tử hóa các lớp tuyến tính trong backbone và chạy lượng tử hóa đường cơ sở (không có chưng cất QFD) trong cài đặt 8, 6, 4 bit.

Như thể hiện trong Bảng 8, lượng tử hóa 8-bit hoặc 6-bit cho lớp tuyến tính của ViT và Swin Transformer ('SwinB' có nghĩa là cấu trúc cơ sở của nó) gần như đủ để phục hồi độ chính xác phát hiện và phân đoạn của nó, chứng minh tiềm năng triển khai vision transformer trên các thiết bị phần cứng thế giới thực (Li và cộng sự 2021). Ngược lại, lượng tử hóa vision transformer thành 4-bit dẫn đến sự giảm hiệu suất đáng chú ý, có thể do khả năng biểu diễn hạn chế của nó. Chúng tôi tiếp tục phân tích tác động của MHA và MLP bằng cách lượng tử hóa từng cái một cách riêng biệt, và kết quả trong Bảng 8 truyền tải một quan sát thú vị: lượng tử hóa vision transformer trong phát hiện và phân đoạn hoàn toàn không nhạy cảm với lớp attention, mà với các lớp tuyến tính trong MLP. Lưu ý rằng hiệu suất của 4/4a trong ViT và SwinB thậm chí vượt qua đối tác 8/8 của nó. Có thể vì lớp MLP bị ảnh hưởng nghiêm trọng bởi sự biến thiên giữa các kênh trong đầu vào LayerNorm (Lin và cộng sự 2022; Ba, Kiros, và Hinton 2016), trong khi lớp MHA chứa các phép toán bổ sung có thể giảm thiểu hiệu ứng này.

--- TRANG 7 ---
Nghiên cứu Ablation
Tác động của λ & Lượng tử hóa đặc trưng. Chúng tôi xác minh siêu tham số duy nhất λ được định nghĩa trong Phương trình 8 trên ImageNet và CUB dưới các cài đặt bit khác nhau (2-bit, 3-bit và 4-bit). Kết quả có thể được tìm thấy trong Bảng 9 đến 11. Đối với cả CNN và vision transformer trên nhiều bộ dữ liệu khác nhau, tất cả các giá trị λ đều dẫn đến cải thiện so với phương pháp lượng tử hóa đường cơ sở và phương pháp của chúng tôi không nhạy cảm với giá trị của λ. Thú vị, trong Bảng 11 (lượng tử hóa ResNet-34 4-bit trên ImageNet), QFD của chúng tôi tiếp tục tăng độ chính xác thêm 1.1% độ chính xác top-1 ngay cả khi phương pháp lượng tử hóa đường cơ sở đã vượt qua đối tác độ chính xác đầy đủ. Do đó, chúng tôi có thể chọn λ = 0.5 theo mặc định để đơn giản.

Tác động của lượng tử hóa đặc trưng của giáo viên. Trong khi đó, chúng tôi cho thấy độ chính xác của mạng giáo viên (với đặc trưng của nó được lượng tử hóa). Như thể hiện trong Bảng 12, các mạng giáo viên lượng tử hóa đặc trưng gần như không có sự khác biệt với bản gốc về độ chính xác. Do đó, sự cải thiện độ chính xác được mang lại bởi QFD, không phải vì độ chính xác của giáo viên cao hơn đường cơ sở.

Cải thiện nhất quán của phát hiện. Cuối cùng trong phần này, chúng tôi vẽ đường cong hội tụ của RetinaNet sử dụng lượng tử hóa đường cơ sở hoặc chưng cất QFD của chúng tôi trên các tác vụ phát hiện MS-COCO. Kết quả sử dụng backbone ResNet-18 và ResNet-50 dưới lượng tử hóa 3-bit có thể được tìm thấy trong Hình 4. Không nghi ngờ gì rằng QFD tạo ra sự cải thiện nhất quán so với đường cơ sở trong suốt toàn bộ quá trình huấn luyện, chứng minh tính tổng quát của các phương pháp của chúng tôi: nó không chỉ phù hợp cho phân loại, mà còn thúc đẩy hiệu suất phát hiện đối tượng.

Kết luận
Trong bài báo này, chúng tôi đã đề xuất một phương pháp chưng cất đặc trưng mới và dễ triển khai QFD trong huấn luyện nhận biết lượng tử hóa. Trước tiên chúng tôi minh họa định tính các lợi thế của QFD: đơn giản, thân thiện với lượng tử hóa và linh hoạt. Các thí nghiệm rộng rãi trên các tiêu chuẩn phân loại hình ảnh, phát hiện đối tượng và phân đoạn với cả mạng tích chập (ResNet và MobileNetV2) và vision transformer đều nhất quán tốt hơn các phương pháp huấn luyện nhận biết lượng tử hóa hiện đại trước đây.

--- TRANG 8 ---
Lời cảm ơn
Nghiên cứu này được hỗ trợ một phần bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc theo Grant 62276123 và Grant 61921006.

Tài liệu tham khảo
Ba, J. L.; Kiros, J. R.; và Hinton, G. E. 2016. Layer normalization. arXiv:1607.06450.

Bengio, Y.; Léonard, N.; và Courville, A. 2013. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv:1308.3432.

Bhalgat, Y.; Lee, J.; Nagel, M.; Blankevoort, T.; và Kwak, N. 2020. LSQ+: Improving low-bit quantization through learnable offsets and better initialization. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 696–697.

Boo, Y.; Shin, S.; Choi, J.; và Sung, W. 2021. Stochastic precision ensemble: Self-knowledge distillation for quantized deep neural networks. Trong Proceedings of the AAAI Conference on Artificial Intelligence, 6794–6802.

Cai, Z.; He, X.; Sun, J.; và Vasconcelos, N. 2017. Deep learning with low precision by half-wave gaussian quantization. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 5918–5926.

Choi, J.; Wang, Z.; Venkataramani, S.; Chuang, P. I.-J.; Srinivasan, V.; và Gopalakrishnan, K. 2018. PACT: Parameterized clipping activation for quantized neural networks. arXiv:1805.06085.

Courbariaux, M.; Bengio, Y.; và David, J.-P. 2015. BinaryConnect: Training deep neural networks with binary weights during propagations. Trong Advances in Neural Information Processing Systems, 3123–3131.

Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; Uszkoreit, J.; và Houlsby, N. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. Trong Proceedings of the International Conference on Learning Representations.

Esser, S. K.; McKinstry, J. L.; Bablani, D.; Appuswamy, R.; và Modha, D. S. 2020. Learned step size quantization. Trong Proceedings of the International Conference on Learning Representations.

Gong, R.; Liu, X.; Jiang, S.; Li, T.; Hu, P.; Lin, J.; Yu, F.; và Yan, J. 2019. Differentiable soft quantization: Bridging full-precision and low-bit neural networks. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, 4852–4861.

Guerra, L.; Zhuang, B.; Reid, I.; và Drummond, T. 2020. Switchable precision neural networks. arXiv:2002.02815.

Guo, J.; Han, K.; Wang, Y.; Wu, H.; Chen, X.; Xu, C.; và Xu, C. 2021. Distilling object detectors via decoupled features. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2154–2164.

Han, T.; Li, D.; Liu, J.; Tian, L.; và Shan, Y. 2021. Improving low-precision network quantization via bin regularization. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, 5261–5270.

He, K.; Chen, X.; Xie, S.; Li, Y.; Dollár, P.; và Girshick, R. 2022. Masked autoencoders are scalable vision learners. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 16000–16009.

He, Y.-Y.; Wu, J.; và Wei, X.-S. 2021. Distilling virtual examples for long-tailed recognition. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, 235–244.

Hinton, G.; Vinyals, O.; Dean, J.; và cộng sự. 2015. Distilling the knowledge in a neural network. arXiv:1503.02531.

Jung, S.; Son, C.; Lee, S.; Son, J.; Han, J.-J.; Kwak, Y.; Hwang, S. J.; và Choi, C. 2019. Learning to quantize deep networks by optimizing quantization intervals with task loss. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4350–4359.

Kim, D.; Lee, J.; và Ham, B. 2021. Distance-aware quantization. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, 5271–5280.

Kim, J.; Bhalgat, Y.; Lee, J.; Patel, C.; và Kwak, N. 2019. QKD: Quantization-aware knowledge distillation. arXiv:1911.12491.

Krizhevsky, A. 2009. Learning multiple layers of features from tiny images. Báo cáo kỹ thuật, Đại học Toronto.

Lee, J.; Kim, D.; và Ham, B. 2021. Network quantization with element-wise gradient scaling. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 6448–6457.

Lee, J. H.; Yun, J.; Hwang, S. J.; và Yang, E. 2021. Cluster-promoting quantization with bit-drop for minimizing network quantization loss. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, 5370–5379.

Li, Y.; Dong, X.; và Wang, W. 2020. Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks. Trong Proceedings of the International Conference on Learning Representations.

Li, Y.; Mao, H.; Girshick, R.; và He, K. 2022. Exploring plain vision transformer backbones for object detection. arXiv:2203.16527.

Li, Y.; Shen, M.; Ma, J.; Ren, Y.; Zhao, M.; Zhang, Q.; Gong, R.; Yu, F.; và Yan, J. 2021. MQBench: Towards Reproducible and Deployable Model Quantization Benchmark. Trong In Advances in Neural Information Processing Systems Datasets and Benchmarks Track.

Lin, T.-Y.; Dollár, P.; Girshick, R.; He, K.; Hariharan, B.; và Belongie, S. 2017a. Feature pyramid networks for object detection. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2117–2125.

Lin, T.-Y.; Goyal, P.; Girshick, R.; He, K.; và Dollár, P. 2017b. Focal loss for dense object detection. Trong Proceedings of the IEEE International Conference on Computer Vision, 2980–2988.

Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, D.; Dollár, P.; và Zitnick, C. L. 2014. Microsoft COCO: Common objects in context. Trong European Conference on Computer Vision, volume 8693 của Lecture Notes in Computer Science, 740–755. Springer.

Lin, Y.; Zhang, T.; Sun, P.; Li, Z.; và Zhou, S. 2022. FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer. Trong Proceedings of the International Joint Conference on Artificial Intelligence, 1173–1179.

Liu, Z.; Cheng, K.-T.; Huang, D.; Xing, E. P.; và Shen, Z. 2022. Nonuniform-to-uniform quantization: Towards accurate quantization via generalized straight-through estimation. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4942–4952.

Liu, Z.; Lin, Y.; Cao, Y.; Hu, H.; Wei, Y.; Zhang, Z.; Lin, S.; và Guo, B. 2021a. Swin transformer: Hierarchical vision transformer using shifted windows. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, 10012–10022.

Liu, Z.; Wang, Y.; Han, K.; Zhang, W.; Ma, S.; và Gao, W. 2021b. Post-training quantization for vision transformer. Trong Advances in Neural Information Processing Systems, 28092–28103.

Mishra, A.; và Marr, D. 2018. Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy. Trong Proceedings of the International Conference on Learning Representations.

Nagel, M.; Fournarakis, M.; Amjad, R. A.; Bondarenko, Y.; van Baalen, M.; và Blankevoort, T. 2021. A white paper on neural network quantization. arXiv:2106.08295.

Paszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.; Chanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.; Desmaison, A.; Kopf, A.; Yang, E.; DeVito, Z.; Raison, M.; Tejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.; Bai, J.; và Chintala, S. 2019. Pytorch: An imperative style, high-performance deep learning library. Trong Advances in Neural Information Processing Systems, 8024–8035.

Polino, A.; Pascanu, R.; và Alistarh, D. 2018. Model compression via distillation and quantization. Trong Proceedings of the International Conference on Learning Representations.

Rastegari, M.; Ordonez, V.; Redmon, J.; và Farhadi, A. 2016. XNOR-Net: ImageNet classification using binary convolutional neural networks. Trong European Conference on Computer Vision, volume 9908 của Lecture Notes in Computer Science, 525–542. Springer.

Russakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.; Ma, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.; Berg, A.; và Fei-Fei, L. 2015. ImageNet large scale visual recognition challenge. International Journal of Computer Vision, 115(3): 211–252.

Sun, M.; Ma, H.; Kang, G.; Jiang, Y.; Chen, T.; Ma, X.; Wang, Z.; và Wang, Y. 2022. VAQF: Fully automatic software-hardware co-design framework for low-bit vision transformer. arXiv:2201.06618.

Touvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles, A.; và Jégou, H. 2021. Training data-efficient image transformers & distillation through attention. Trong International Conference on Machine Learning, 10347–10357.

Wah, C.; Branson, S.; Welinder, P.; Perona, P.; và Belongie, S. 2011. The Caltech-UCSD Birds-200-2011 Dataset. Báo cáo kỹ thuật CNS-TR-2011-001, Viện Công nghệ California.

Wu, J.; và Luo, J.-H. 2018. Learning effective binary visual representations with deep networks. arXiv:1803.03004.

Wu, Y.; Kirillov, A.; Massa, F.; Lo, W.-Y.; và Girshick, R. 2019. Detectron2. https://github.com/facebookresearch/detectron2. Truy cập: 2022-06-01.

Yamamoto, K. 2021. Learnable companding quantization for accurate low-bit neural networks. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 5029–5038.

Yang, Z.; Li, Z.; Jiang, X.; Gong, Y.; Yuan, Z.; Zhao, D.; và Yuan, C. 2022. Focal and global knowledge distillation for detectors. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4643–4652.

Yuan, Z.; Xue, C.; Chen, Y.; Wu, Q.; và Sun, G. 2021. PTQ4ViT: Post-training quantization framework for vision transformers. arXiv:2111.12293.

Zhang, D.; Yang, J.; Ye, D.; và Hua, G. 2018. LQ-Nets: Learned quantization for highly accurate and compact deep neural networks. Trong Proceedings of the European Conference on Computer Vision, volume 11212 của Lecture Notes in Computer Science, 365–382. Springer.

Zhou, S.; Wu, Y.; Ni, Z.; Zhou, X.; Wen, H.; và Zou, Y. 2016. DoReFa-Net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv:1606.06160.

Zhuang, B.; Liu, L.; Tan, M.; Shen, C.; và Reid, I. 2020. Training quantized neural networks with a full-precision auxiliary module. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1488–1497.
