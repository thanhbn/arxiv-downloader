# 2308.05600.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/quantization/2308.05600.pdf
# File size: 1191841 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
PRE-PRINT VERSION 1
NUPES : Non-Uniform Post-Training
Quantization via Power Exponent Search
Edouard Yvinec, Arnaud Dapogny and Kevin Bailly
Abstract —Deep neural network (DNN) deployment has been confined to larger hardware devices due to their expensive
computational requirements. This challenge has recently reached another scale with the emergence of large language models (LLMs).
In order to reduce both their memory footprint and latency, a promising technique is quantization. It consists in converting floating point
representations to low bit-width fixed point representations, usually by assuming a uniform mapping onto a regular grid. This process,
referred to in the literature as uniform quantization, may however be ill-suited as most DNN weights and activations follow a bell-shaped
distribution. This is even worse on LLMs whose weight distributions are known to exhibit large, high impact, outlier values. In this work,
we propose an improvement over the most commonly adopted way to tackle this limitation in deep learning models quantization,
namely, non-uniform quantization. NUPES leverages automorphisms to preserve the scalar multiplications. Such transformations are
derived from power functions. However, the optimization of the exponent parameter and weight values remains a challenging and novel
problem which could not be solved with previous post training optimization techniques which only learn to round up or down weight
values in order to preserve the predictive function. We circumvent this limitation with a new paradigm: learning new quantized weights
over the entire quantized space. Similarly, we enable the optimization of the power exponent, i.e.the optimization of the quantization
operator itself during training by alleviating all the numerical instabilities. The resulting predictive function is compatible with
integer-only low-bit inference. We show the ability of the method to achieve state-of-the-art compression rates in both, data-free and
data-driven configurations. Our empirical benchmarks highlight the ability of NUPES to circumvent the limitations of previous
post-training quantization techniques on transformers and large language models in particular.
Index Terms —Deep Learning, Quantization, post-training, Machine Learning, Neural Networks, Large Language Models.
✦
1 I NTRODUCTION
THEneed for effective deep neural networks (DNNs)
compression and acceleration techniques has grown
as the computational requirements of DNNs increased.
In particular, the recent surge of large language models
(LLMs [48]), characterized by their billions of parameters,
has brought their specificities to the challenge of efficient
inference. To address the limitations on DNN deployment,
quantization has become one of the crucial steps to DNNs
deployment [5], especially on edge and low power devices.
In its most general formulation, quantization consists in the
conversion of large floating point representations to low-
bit fixed point representations. As a result, the quantized
model has a lower memory footprint as well as an improved
latency.
As defined by Nagel et al. [35], quantization techniques
are classified based on their data requirements. In its most
practical setup from the user perspective, but most challeng-
ing in terms of compression, quantization only leverages
the weight values of a pre-trained model. Data-free quan-
tization techniques [8], [35], [45], [51] are motivated by the
growing concerns regarding data privacy as well as their
scalability with respect to the model size. On the other end
of the spectrum, quantization aware training techniques [9],
[10], [50] enable the quantization of ConvNets into binary
•E. Yvinec is a PhD student at Datakalab, 114 boulevard Malesherbes
75017 Paris and Sorbonne Universit´ e, CNRS, Institut des Syst` emes
Intelligents et de Robotique, ISIR, F-75005 Paris, France.
•A. Dapogny is a ML researcher at Datakalab.
•K. Bailly is the Head of research at Datakalab and associate professor at
Sorbonne Universit´ e.
top1 accuracy
020406080
w3/a4 w4/a4 w5/a4 w6/a4DFQ
SQuant
AdaRound
BrecQ
PowerQ
NUPESQuantized ResNet
top1 accuracy
020406080
w3/a4 w4/a4 w5/a4 w6/a4DFQ
SQuant
AdaRound
BrecQ
PowerQ
NUPESQuantized ViTFig. 1. NUPES unlocks near full-precision accuracy in W4/A4 across
ConvNets and Transformers. We can observe that while other meth-
ods are either more effective on ConvNets or Transformers, NUPES
achieves state-of-the-art results on both use-cases.
representations at the cost of a full retraining burdened by
extra operations. In order to offer more trade-offs in terms
practical usage, gradient-based post training quantization
(GPTQ) [28], [34] was introduced and consists in learning
the quantized weight values on a small calibration set. InarXiv:2308.05600v1  [cs.LG]  10 Aug 2023

--- PAGE 2 ---
PRE-PRINT VERSION 2
practice, it is usually limited to the optimization of the
rounding operation (rounding up or down).
As opposed to most quantization methods [28], [35], [45],
[50] which, for the sake of practicality, map floating point
values to an evenly spread, discrete space, non-uniform
quantization achieves a tighter fit to the target distribution
by using a non-uniform spread quantized target space.
However such methods [2], [33] require custom implemen-
tations that significantly shift away from uniform quantiza-
tion. In our previous work, PowerQuant [46], we proposed a
novel non-uniform quantization technique which both mas-
sively improved the performance of data-free quantization
and tackled the requirement of the aforementioned methods
for custom operations. As we wanted to map multiplications
to multiplications, we searched for the best possible quanti-
zation operator that preserves the mathematical operations
among automorphisms of (R∗
+,×). In practice, we showed
that it boiled down to the optimization of the exponent
parameter of a power function with respect to the error
introduced by quantization.
In this work, we propose NUPES, a novel post-training
non-uniform quantization to address the limitations of
power quantization and study its performance on large
language models. First, we adapt PowerQuant to the GPTQ
framework [34]. While other GPTQ methods optimize a
continuous value ϵ∈[0; 1] in the quantized space as a
way to learn the rounding operation, this cannot be applied
to PowerQuant. The challenge arises from the disparity in
ranges, for values close to zero, a shift of 1in the quantized
space is negligible in the full precision space, while a change
of1for larger values leads to a substantial change in the
predictive function. To circumvent this limitation, we draw
inspiration from quantization aware training in order to
design a novel, robust way to learn ϵ∈Z. This new method,
dubbed NUPES, enables us to reach new levels of compres-
sion while maintaining the accuracy of the original method
as shown on Figure 1. While PowerQuant already outper-
formed GPTQ methods on transformers, NUPES further
improves it and extends this ability to convolutional neural
networks. Second, we propose to alleviate a limitation of
PowerQuant: the search for the power exponent. In the
original method, we proposed to optimize a single value
for the whole network based on the weights. In NUPES, we
propose to learn the exponent parameter per-layer through
stochastic gradient descent [38] during the GPTQ process
which results in more flexibility and paves the way towards
the integration of PowerQuant in a full quantization aware
training. Third, we demonstrate the particular efficiency of
PowerQuant regarding transformers quantization and out-
liers handling. As transformers grow in size, they become
more challenging to quantize due to the emergence of out-
liers [13]. These values stretch the distribution range, which
leads to a significant part of the uniform target space being
wasted. Consequently, we provide an extensive study on
the ability of PowerQuant to efficiently address this pitfall
on large transformers and large language models (LLMs).2 R ELATED WORK
2.1 Quantization
In this section, we describe the current state-of-the-art in
artificial neural network quantization. As our goal is both
memory footprint and latency reduction, we will omit ap-
proaches that are designed for weights storage (on disk not
on RAM) only [4], [19], [52].
2.2 Non-Uniform Quantization
In general, mapping a non-uniform continuous distribution
to a discrete evenly spread space linearly is unlikely to
achieve the best usage of the target quantized space. To
alleviate this limitation, non-uniform quantization has been
studied [2], [23], [25]. Among these techniques, code-base
approaches [2], [25] usually come at the price of complex
implementations at runtime. The remaining methods alter
the quantization operator [33], [52]. In particular, Zhang et al.
[49] proposed a logarithm-based quantization mechanism.
Hence, the values in the quantized space no longer represent
scalar values for a multiplication but rather an exponent
for a bit-shift which is applied to the uniformly quantized
inputs. Consequently, log-quant methods require a change
in the nature of the supported mathematical operations
and are bound to a low support until their performance
justify investing in new inference instruction sets. In this
work, we propose an alternative to these two non-uniform
paradigms, namely NUPES, which consists in the search for
quantized weights and a quantization operator defined by a
power exponent which maps scalar multiplications to scalar
multiplications. Hence, by preserving the mathematical op-
erations, NUPES is easier to leverage. We adapt the method
for gradient-based optimizations, thus tremendously im-
proving its performance.
2.3 Gradient-based Post-Training Quantization
The surge of large language models and the struggle to
deploy them efficiently has sparked a novel interest for post-
training quantization. For instance, a quantization aware
training [50] on a small model such as ResNet 50 [21]
takes 7 days on 8 gpus A100. In this context, Nagel et
al.[34] pioneered with the so-called AdaRound method,
that consists in two steps. First, quantizing the weights
and activations in a naive fashion [27] with a rounding
step for the activations and a flooring step for the weights.
Second, using a calibration set (subset from the training set)
in order to learn the rounding ϵ∈[0; 1] using stochastic
gradient descent. This method is performed layer per layer
and uses as ground truth the intermediate features of the
original, full-precision, model. Since its publication, several
works [28], [31], [43] aimed at improving the accuracy of the
quantized model in the low bit setup. The most noticeable
one, BrecQ [28], groups layers during the optimization step
based on the architecture. For instance, all the layers in a
residual block [21] or a transformer block [42] are optimized
at once, thus reducing the duration of the process while
improving the final accuracy. However, non-uniform quanti-
zation is not compatible with GPTQ methods in their current
form. NUPES solves this problem and achieves state-of-the-
art accuracy on most commonly used architectures by a)

--- PAGE 3 ---
PRE-PRINT VERSION 3
generalizing the rounding search and b) optimizing over
the power exponent, leading to superior performance. In
particular, LLMs and outliers.
2.4 Quantization of Large Language Models
The rise of large language models has brought its own
share of challenges in terms of quantization. In particular,
the presence of outliers among weight and activations [13],
has proven to lead to significant accuracy loss when per-
forming previously introduced state-of-the-art quantization
schemes such as DFQ [35] or SQuant [8]. This phenomenon
is illustrated in Figure 7 where an outlier is defined as
the number of standard deviations from the mean absolute
weight value. We observe that LLMs have outliers that are
over 17 standard deviations away from the mean (more than
the number of values that can represented by an int4) while
ResNet architectures are bounded by 8 standard deviations.
Consequently, such neurons will see most of their values
quantized to zero almost as if they were quantized in ternary
values. Furthermore, due to the size of the models, even
efficient quantization methods such as AdaRound [34] fail
to efficiently scale and introduce a expensive processing
time, taking a few days to quantize a mid-sized LLM. Most
recent works [14], [17], that tackle weight quantization of
LLMs, all leverage a new quantization granularity, namely
group-wise quantization, introduced in nuQmm [37]. In our
experiments, we demonstrate that power quantization in
general enables state-of-the-art performance on the chal-
lenging LLMs.
3 M ETHODOLOGY
Let’s consider F, a trained neural network defined by
its layers (fl)l∈{1,...,L}of real valued weight tensors
(Wl)l∈{1,...,L}. Formally, a quantization operator Qis a
transformation that maps all the elements x∈Rof any
tensor Xto a quantized interval [−2b−1; 2b−1−1]∩Z. Hence,
the computation of Qinvolves a rounding operation that we
note⌊·⌉. Furthermore, in order to cover the entire quantized
space, the quantization process Q, almost systematically,
involves a scaling step. Consequently, we define the baseline
operator Qas
Q(x) =x
s(X)
with s(X) =max x∈X{|x|}
2b−1−1(1)
TheQoperator comes with two limitations. First, by defi-
nition any quantization operator introduces an information
loss as Q−1(Q(X)) = s(X)×Q(X)̸=X. Worse, given
that in practice Xwill follow a non-uniform distribution,
the use of a linear scaling transformation is likely to be sub-
optimal. Intuitively, most values in Xwill be closer to zero
than the edge of their support in Rwhile Qgives as much
precision to every piece of that support. This phenomenon
is illustrated in Fig 2 (a). Second, by definition of the scaling
term s, the quantization process is very sensitive to outliers
inX. Any such outlier would either be clipped to fit the
quantized space or stretch the support out, leading to many
non-zero values being quantized to zero as illustrated in
Fig 2 (b). In our previous work, we introduced PowerQuant
[46], a novel quantization operator that tackles both of these
issues at once.
Fig. 2. Illustration of the two pitfalls of naive quantization. First (a),
the quantization error is uniformly distributed regardless of the prior
weight distribution (Gaussian). In short, extreme values will have the
same quantization error as redundant values despite their much rarer
occurrences. Second (b), outliers stretch the distribution out and lead to
zero quantization of most of the weight values.
3.1 PowerQuant
We generalize the definition of a quantization operator Q
from equation 1 through transformations tof the tensor X
to quantize as
Q(x) =t(x)
s(t(X))
with t:R→R. (2)
In practice tcan be learned [28], [34] or based on some
heuristic [8]. In PowerQuant, we define tsuch that for any
pair(x, y)of positive real values we get t(x)×t(y) =t(x×
y). The set of such functions is the set of automorphisms of
(R∗
+,×). This set corresponds to the set of power functions
t:x7→xafora∈R(see Appendix A). Consequently, the
setQof power operators is defined as
Q=
Qa:X7→sign(X)× |X|a
s(sign(X)× |X|a)a∈R
.(3)
In other words, for a given power value a, the transforma-
tiontin equation 2 is t:x7→sign(x)×|x|awhere the sign (·)
function generalizes the automorphisms of R∗
+toR. In prior
work, we proposed to search, in a data-free manner, for the
quantization operator in Qthat minimizes the quantization
reconstruction error. Formally, this boiled down to solving
the following minimization problem:
min
a∈R(LX
l=1Wl−Q−1(Q(Wl))
2)
. (4)
This problem is locally convex around its unique global
solution (we recall the demonstration in Appendix B). Based
on these properties, Equation 4 can be solved using the

--- PAGE 4 ---
PRE-PRINT VERSION 4
Nelder-Mead method [36]. Empirically, we observed that
this problem was better solved using a single parameter
shared for all layers and based only on the weight values.
Furthermore, a= 0.5enabled near optimal performance
in terms of accuracy while requiring the implementation of
simpler power functions (square and square roots). Thus
leading to a more straightforward integration at inference.
Intuitively, a value of a= 1 corresponds to the naive
uniform quantization as defined in equation 1. While using
a value of a < 1puts more emphasis (and precision) to
scalar values with a small magnitude. Such values of aresult
in a strong similarity with floating representations and log-
based quantization (see Fig 3). In short, such representations
are known for their strong ability to better fit to empirical
distributions of weights and activations. For instance, the
new floating format introduced in QLoRa [14] converges
to a more rigid version of PowerQuant. While QLoRa is
optimal for Gaussian distributions, weight and activations
do not strictly follow such prior. PowerQuant offers an
empirically more accurate solution which we attribute to
its greater flexibility (optimization of a) and smoother step
size changes.
Nonetheless, PowerQuant has several limitations. First,
weight values in power operators cannot be straightfor-
wardly optimized in a GPTQ fashion. Intuitively, GPTQ
methods only learn to round up or down which corresponds
to the same constraint for all values in the quantized space.
However, in non-uniform quantization, all values in the
quantized space have significantly different impact in the
full-precision space. This discrepancy leads to incoherent
optimization and a degradation of the performance. To solve
this issue, we propose a novel approach to GPTQ methods
for non-uniform quantization. Second, the optimization of a
is shared across the network (one value for the model and
not one value per-layer) and is limited to the weight values
and does not account for the activations. Furthermore, the
optimization of a, in its current form, has to be performed
post-training. In the following sections, we describe how the
proposed NUPES method addresses these two limitations.
3.2 Gradient Based Optimization of the Weight
GPTQ was introduced by Nagel et al. [34] as a layer-wise
self-distillation framework (rather than a full model opti-
mization at once). For a given layer l, we assume that every
upstream layer has already been optimized. We note Xfp
andXqthe intermediate input features of layer lfrom the
full precision and quantized models respectively. Then the
goal is to learn the rounding operation by minimizing
min
ϵfl(Xfp, Wl)−fl
Xq,Wl
s(Wl)
+σ(ϵ)2
2.(5)
Intuitively, as Wlis rounded down ⌊·⌋, we learn through
stochastic gradient descent a real valued parameter ϵini-
tialized such that s(Wl)j
Wl
s(Wl)k
+σ(ϵ)
=Wl. At the end
of the optimization process, ϵdefines whether we round
Wlup or down. In order to bound the influence of ϵto
[0;1], GPTQ methods apply a variant of sigmoid, called the
rectified sigmoid:
σ:ϵ7→clip1
1 +e−ϵ×1.2−0.1,0,1
. (6)During the optimisation process over the calibration set
(usually consisting in ≈1kexamples), a regularization term
is added, which forces σ(ϵ)to converge to either zero or one.
Consequently, the minimization loss Lis
L=fl(Xfp, Wl)−fl
Xq,Wl
s(Wl)
+σ(ϵ)2
2
+λ×
1−(2×σ(ϵ)−1)β(7)
where λandβare two hyper-parameters with their respec-
tive schedulers: the lambda parameter defines whether the
rounding term of the loss is used or not, in practice, it is
set to zero for the first 20% of the optimization of each
layer and then set to 0.01. The parameter βdefines the
steepness of the rounding loss (the larger βthe steeper the
rounding). The schedule of βhas a great influence on the
performance and will be discussed in the experiments. As a
result, one needs to store in memory both ϵandj
Wl
s(Wl)k
and compute the two loss terms. This slows down the
optimization process and hinders its scalability especially
on larger models. Furthermore, as illustrated in Figure 3,
PowerQuant quantized space has a very small step size
for values near 0which implies that optimizing ϵcan only
lead to marginal corrections. On the other hand, for larger
values, a change in the rounding operation may lead to
tremendous changes and thus introduce instability in the
optimization process. In order to circumvent all of these
limitations at once, we propose to leverage the differentiable
soft quantization activation [18]
dsq(ϵ) =tanh β× ϵ−1
2− ⌊ϵ⌋
2tanh
β
2 +⌊ϵ⌋+1
2(8)
where βis the hyper-parameter replacing βin equation 7
and eliminating the need for λ. Fig 4 illustrates the influence
of steepness parameter βon the dsq function. Consequently,
our new, simplified objective and loss function become
(
minϵfl(Xfp, Wl)−fl(Xq,dsq(ϵ))2
2
L=fl(Xfp, Wl)−fl(Xq,dsq(ϵ))2
2(9)
and is optimized from the initial value ϵ=Wl
s(Wl)which
learns the new quantized values of Wlas a single tensor,
thus enabling values to be shifted by more than 1(we no
longer only learn to round up or down). Consequently,
we can learn larger modifications of values near 0and
compensate for modifications of larger values. Furthermore,
as we only use ϵand no longer need the floored weight
values, NUPES only requires half the memory footprint (for
the model) and only computes one loss term. The memory
footprint reduction is particularly important for transformer
architectures, for which weight representations are typically
more memory-consuming than activations. Thus, NUPES
addresses the first limitation of PowerQuant: learning the
weight values in a GPTQ fashion. Furthermore, it also
allows to learn the power exponent parameter aalongside
the weight values.

--- PAGE 5 ---
PRE-PRINT VERSION 5
Fig. 3. Illustration of the quantization in 4 bits of the positive part of a Gaussian distribution in uniform, logarithm, floating point and power quantization
(from left to right). This highlights the balance between precision for very low bit values and larger values that is achieved with PowerQuant versus
other quantization formats.
Fig. 4. Illustration of the graph of the dsq transformation for different
values of the steepness hyperparameter β.
3.3 Gradient Based Optimization of the Power Expo-
nent
Learning the power exponent athrough stochastic gradient
descent requires the computation of the derivative
∂Xa
∂a=Xalog(X). (10)
This, however, raises a number of challenges. First, by
definition of the power quantization operator (eq 3), this
derivative appears in both the numerator and the input of
the scaling function s. However, sshall not be optimized
through stochastic gradient descent as it can be analytically
derived from the ranges of the tensor X. Consequently, we
only compute the derivative for the numerator and derive
the new scaling value ( update scale ) with the updated value
ofa, thus addressing this first issue. Second, the current
formulation of the gradient is not numerically stable in
practice, as near zero values in Xlead to infinite gradients
due to the log function. This can be solved by clipping the
values of X(num. stability ) while computing gradients.
Third, for a given layer, we compute the update of the
parameter afrom two transformations: the quantization
of the weights (which depends on the batch size) and the
quantization of the inputs. Consequently, a simple addition
of these two update terms would lead to an unbalance and a
strong dependency on the batch size. We alleviate this issue
by averaging the gradients contributions ( balanced grads ),
from the weight and input quantization, independently
before their combination.
Algorithm 1 summarizes these adaptations of the op-
timization process that are necessary to learn aduringAlgorithm 1 Learn the exponent parameter a
Require: a layer flwith weights Wl∈RN×Mand inputs
X∈RB×N
scale x←s(sign(X)× |X|a) ▶update scale (no ∇)
scale w←s(sign(Wl)× |Wl|a)▶update scale (no ∇)
Forward Pass
X←j
sign(X)×|X|a
scale xm
Wl←j
sign(Wl)×|Wl|a
scale wm
Y←fl(X, W l)
Backward Pass
Xclipped←clip(|X|,10−6,∞) ▶num stability
Wclipped←clip(|W|,10−6,∞)▶num stability
∇from inputs ←Xa
clippedlog(Xclipped )
B×N
∇from weights ←Wa
clippedlog(Wclipped )
N×M
∇a← ∇ from inputs +∇from weights ▶balanced grads
any training process from GPTQ [34] to QAT [52]. As a
result, NUPES solves all the aforementioned shortcomings
of GPTQ methods applied to PowerQuant, a claim that we
experimentally validate further below.
4 E XPERIMENTS
In this section, we evaluate NUPES on three main aspects:
•the necessity to address the aforementioned short-
comings related to the optimization of the power
exponent and quantized weight values,
•the accuracy improvement from NUPES over the
data-free PowerQuant and other GPTQ methods
such as AdaRound [34] or BrecQ [28],
•the interest of power quantization methods to tackle
LLMs weight quantization without the use of un-
practical mix of floating point and integer represen-
tation (contrary to e.g. QLoRa [14] or OPTQ [17]).
4.1 Implementation and Datasets
We conducted our evaluations on a wide range of Con-
vNets and Transformer networks. Regarding ConvNets, we
considered the ResNet [21] family, MobileNet v2 [40], Effi-
cientNet B0 [41] for image classification on ImageNet [12]

--- PAGE 6 ---
PRE-PRINT VERSION 6
Fig. 5. Graph plots of the βschedulers with
respect to the number of optimization steps.
TABLE 2
Evaluation of the different βschedulers for W4/A4 quantization. We also provide the performance of
the PowerQuant method as a reference (ref)
model AdaRound Const 2Const 5Const 10Const 20Power 1Power 2Power 3ref
ResNet 18 17.366 19.840 41.534 54.500 64.528 14.600 24.640 29.134 56.386
ResNet 34 26.934 38.940 57.022 59.402 68.236 31.708 31.648 31.150 62.904
ResNet 50 6.254 14.578 28.496 56.320 68.758 8.276 14.034 8.408 62.142
ResNet 101 4.842 9.980 42.656 53.328 71.736 22.622 25.346 23.054 64.562
RetinaNet 23.118 21.472 20.692 23.652 32.692 19.486 18.814 19.392 3.618
ViT b16 55.454 58.258 59.004 59.004 79.578 59.026 53.180 57.788 74.134
ViT l16 11.144 23.624 33.318 32.750 34.544 14.072 23.070 6.842 33.310
ViT h14 53.219 32.362 70.878 84.796 87.190 36.490 36.436 36.364 85.906
TABLE 1
Evaluation, in W4/A8, of PowerQuant with AdaRound in a na ¨ıve
combination.
ResNet 50 RetinaNet ViT
PowerQuant 74.892 21.562 80.354
AdaRound 75.322 27.258 79.590
PowerQ + AdaR 72.384 20.346 77.214
and RetinaNet [30] with a ResNet backbone for dense pre-
dictions (object detection) on COCO [29]. Regarding Trans-
former architectures, we considered the ViT [15] family for
image classification on ImageNet and OPT models [48] for
LLM evaluation on a wide set of common sense reasoning
tasks [3], [6], [7], [32], [39], [47]. We also included results on
the more recent Dolly v2 network [11].
In our experiments, the baseline performance of these
models come from TensorFlow [1] implementations for
image classification, and from torchvision [16] for object
detection. The LLMs implementation and weight values
were downloaded from HuggingFace [24]. When reporting
accuracy, we use the commonly adopted notation W4/A8
for a 4 bits quantization of the weights and 8 bits for the
activations. In all our experiments, we follow the standard
format [34], [35] and use W8/A8 representations for the first
and last layers of convolutional neural networks.
NUPES as well as other quantization methods were
implemented in both Torch and TensorFlow for experiments
and run on a single A100 Nvidia GPU. We use our own im-
plementations for SQuant, DFQ and PowerQuant while we
report performance of other methods based upon SQuant
paper [8]. In all our experiments with gradient descent opti-
mization, we share the same set of hyper-parameter values.
We use the Adam optimizer [26] with base parameters, a
batch-size of 32, for10ksteps over 1024 data samples from
the training set and no learning scheduler. The scheduler
choice for βfrom equation 8, is discussed in section 4.2.1.
In the next section, we conduct an ablation study to
highlight the impact of each component in NUPES.4.2 Ablation study
4.2.1 GPTQ Loss Scheduler
In Table 1, we report results obtained using a na ¨ıve com-
bination of PowerQuant and AdaRound. This motivates
the need for the adaptations introduced in Section 3.2:
indeed, on top of the memory footprint reduction, we show
that the integration of PowerQuant within AdaRound is
not straightforward. Intuitively, this comes from the non-
uniform quantized distribution with creates unbalance be-
tween weight values. This is solved by NUPES with allows
for more flexibility on the values of ϵ.
We control this flexibility through the steepness of the
soft rounding function from equation 8, we defined several
schedulers for the βparameter. Our first candidate was
introduced in AdaRound [34] to also control the steepness
of the soft rounding they proposed which has been success-
fully leveraged in many subsequent works [28], [31], [43].
For the other candidates, we considered naive schedulers
that have also been applied to, general training, learning
rate scheduling. Formally, our schedulers are


AdaRound (s) = 20 +−18
2 1 + cos s
Sπ
Const c(s) =c
Power c(s) = 20 s
Sc(11)
In Figure 5, we illustrate the evolution of each candidate
scheduler with respect to the optimization step sout of
S. The original GPTQ method [34] and the subsequent
counterparts have been using a sophisticated scheduler for
β. The intuition behind the value of βis the cost for any
value v=n+ϵ, with n∈Nandϵ∈[0; 1] , to learn a final
value v∗=n−1orv∗=n+ 2. The smaller β, the easier it
gets to drift outside of the [0; 1] interval.
In Table 2, we compare these schedulers on several ar-
chitectures and tasks. We observe that the constant strategy
with c= 20 systemically outperforms the other candi-
dates as well as the PowerQuant baseline (ref column).
This highlights that although it is important to allow the
weights to shift by more than a single quantization step, this
phenomenon should be constrained to a few occurrences.
From now on, the NUPES method will always use the

--- PAGE 7 ---
PRE-PRINT VERSION 7
TABLE 3
Comparison to state-of-the-art GPTQ techniques. We report the W4/A4 quantized accuracies across convolutional neural networks and
transformers. The first set of quantization methods are data-free while the second and third sets leverage a calibration set.
method ResNet 18 ResNet 34 ResNet 50 MobileNet v2 EfficientNet B0 RetinaNet ViT b16 ViT h14
full-precision 69.674 73.230 76.150 72.074 77.618 37.294 80.978 88.434
DFQ [35] ICCV 2019 29.602 40.698 28.548 0.232 0.112 0.256 3.354 0.176
SQuant [8] ICLR 2022 48.126 49.100 52.042 0.398 0.104 0.191 3.280 0.100
SPIQ [45] WACV 2023 50.257 52.517 52.752 0.572 3.623 0.382 4.007 0.514
PowerQuant [46] ICLR 2023 56.386 62.904 62.142 0.348 3.618 2.241 74.134 85.906
AdaRound [34] ICML 2020 60.258 65.174 61.656 8.840 0.102 21.392 29.906 23.070
BrecQ [28] ICLR 2021 28.650 40.050 63.782 38.230 0.110 18.922 22.228 25.686
QDrop [43] ICLR 2022 63.448 64.176 65.766 40.984 0.100 20.812 - -
PDQuant [31] CVPR 2023 63.471 64.148 66.440 41.464 0.100 21.562 - -
NUPES (learn a) 57.524 62.378 62.468 5.902 15.241 22.424 74.552 86.600
NUPES (learn W) 64.528 68.236 68.758 42.239 18.132 32.692 79.578 87.190
NUPES (learn W&a) 65.876 69.954 70.684 42.386 45.902 33.078 80.100 87.204
constant scheduler with β= 20 . The resulting optimization
process requires less memory footprint, as we only store the
weights and not the ϵtensor. This is crucial, especially for
transformer architectures where the weight values represent
most of the memory footprint.
As a result, we can optimize one of the decisive factors of
the performance of a power quantized model: the quantized
weight values. In the next section, we evaluate the ability
of NUPES to also optimize the remaining decisive factor:
exponent parameter.
4.2.2 Power Exponent Optimization Backward Pass
In Table 4, we evaluate the impact of the proposed learning
process (sec 3.3) for the exponent parameter. We consider
the challenging int4 quantization configuration (W4/A4)
and initialize the power values at 0.5. First, we can observe
that, in the absence of numerical stability safety nets, the
learning processes never completes (hence the ”-” accuracy).
More precisely, we observe that the learning crashes after
4 optimization steps on average due to the computation
of the logarithm of some zero values. Second, regardless
on the architecture (transformer/convnet) and task (classifi-
cation/detection), balancing the contributions of the inputs
and weights as well as separating the update of the scales
from the stochastic gradient descent process systematically
improve the final accuracy. Furthermore, our results on
RetinaNet show that the most important step is the isolation
of the scales update. This can be explained by two facts:
first, the choice of the scaling factor can be analytically
computed. Second, the scaling factor is set to reduce the
error introduced by the rounding and clipping processes,
however theses two steps are not considered during gra-
dient descent. In conclusion, the learning process of the
exponent parameter is non-trivial and can lead to significant
improvements to the accuracy of the quantized model,
provided it is done correctly. For example, on RetinaNet,
the PowerQuant method only achieves a 2.241 MAP , while
NUPES improves this score by 1006% (see Table 3).
In Figure 6, we display the learned power exponent
values with respect to the layer’s depth. Although the
result seems chaotic, we can still draw several conclusions.
First, we observe that the first layer tends to require higherTABLE 4
Evaluation of the impact of learning the exponent (and not the weights)
in W4/A4 quantization. We provide the full-precision original
performance (accuracy or MAP) of the pre-trained model.
model num. stability balanced grads update scale Accuracy
ResNet50
(76.15)✗ ✗ ✗ -
✓ ✗ ✗ 46,552
✓ ✓ ✗ 58.950
✓ ✗ ✓ 61.997
✓ ✓ ✓ 62.468
RetinaNet
(37.294)✗ ✗ ✗ -
✓ ✗ ✗ 4.084
✓ ✓ ✗ 4.128
✓ ✗ ✓ 19.838
✓ ✓ ✓ 22.424
ViT b16
(78.05)✗ ✗ ✗ -
✓ ✗ ✗ 39.282
✓ ✓ ✗ 62.142
✓ ✗ ✓ 73,794
✓ ✓ ✓ 74.552
exponent values, this suggests that they are characterized
by less peaky distributions. This behavior, can also be
attributed to the larger bit-width (8 bits) for ResNet and
RetinaNet. Second, we observe that all layers fall in the
range al∈[0.213; 0 .774] which is in adequacy with the
great performance of the value 0.5that we leveraged in
PowerQuant [46]. This result is consistent regardless of the
initial value of a. Third, the most important conclusion, we
solved the problem we faced in our initial method: we can
learn distinct power exponent values per layer and improve
the final accuracy.
In summary, we demonstrated the strong added value
from each components of NUPES over the already strong
PowerQuant baseline. In the following section, we show
how PowerQuant and NUPES compare to other data-free
and PTQ methods respectively.
4.3 Main result: Comparison to other GPTQ methods
Among state-of-the-art data-free quantization techniques,
PowerQuant stands out as the most effective technique. In

--- PAGE 8 ---
PRE-PRINT VERSION 8
TABLE 5
Evaluation on data-free quantization methods on large language models W4/A16 quantization for common sense reasoning tasks.
model method OpenBookQA ARC-E ARC-C WinoGrande HellaSwag PIQA BoolQ Average
Dolly v2 3Bfull-precision 27.600 61.742 34.044 59.274 49.861 73.885 58.315 52.103
DFQ [35] ICCV 2019 26.000 55.808 27.730 57.616 43.567 71.273 53.456 47.921 (-4.182)
SQuant [8] ICLR 2022 26.200 55.934 28.328 57.301 43.587 71.491 53.700 48.077 (-4.026)
PowerQuant [46] ICLR 2023 27.200 61.880 33.253 58.950 48.560 73.905 57.609 51.622 (-0.481)
Dolly v2 7Bfull-precision 30.600 64.141 37.713 61.010 52.778 74.755 64.862 55.123
DFQ [35] ICCV 2019 23.600 53.956 33.020 54.775 44.633 69.260 64.801 49.149 (-5.973)
SQuant [8] ICLR 2022 24.200 54.167 33.106 54.854 44.573 69.260 64.801 49.280 (-5.842)
PowerQuant [46] ICLR 2023 30.400 62.386 35.214 60.537 52.542 74.776 65.034 54.413 (-0.710)
OPT 13Bfull-precision 27.000 61.953 33.020 65.746 52.390 76.714 64.954 54.540
DFQ [35] ICCV 2019 25.400 59.975 29.836 63.062 49.044 75.734 49.786 50.405 (-4.135)
SQuant [8] ICLR 2022 25.547 60.145 29.911 63.097 49.032 75.734 49.786 50.465 (-4.075)
PowerQuant [46] ICLR 2023 27.000 61.816 32.741 64.088 51.115 76.354 67.217 54.333 (-0.207)
OPT 30Bfull-precision 30.600 64.941 34.471 68.272 54.272 77.911 70.061 57.218
DFQ [35] ICCV 2019 27.400 56.860 30.119 63.931 50.119 75.680 67.339 53.064 (-4.154)
SQuant [8] ICLR 2022 27.400 57.077 30.231 63.967 50.140 75.676 67.339 53.119 (-4.099)
PowerQuant [46] ICLR 2023 30.500 63.552 34.276 67.930 53.625 78.183 69.966 56.862 (-0.356)
Layer depthValue of the power exponent0,2000,4000,6000,800RetinaNet ResNet ViT
Fig. 6. Plot of the learned power exponent values for several architec-
tures in W4/A4.
Fig. 7. Distribution of outlier weight values defined by their distance to
the mean value in terms of standard deviations for a ResNet 18 and
OPT model. This highlights the specific challenge introduced by LLMs
in terms of quantization ranges.
this study, we removed the comparison to methods that
require data generation such as GDFQ [44], because such
methods fail to scale to both very large models and arenot suited for models that do not use batch normalization
layers [20]. For these reasons, we focused on fully data-free
techniques that are effective at all model sizes.
In Table 3, we report our extensive study of post-training
W4/A4 quantization techniques on convolutional neural
networks (ResNets, MobileNets and EfficientNets) as well
as transformers from ViT b16 (86M parameters) to ViT h14
(600M parameters). In this extreme compression regime,
we observe the limits of previous state-of-the art methods
SQuant [8] and SPIQ [45]. This is not the case for Pow-
erQuant which already achieves strong results on ResNets
and transformers and, as such, offers a very strong baseline
for the proposed NUPES method.
The proposed evaluation is particularly challenging as
compared to results shared in SQuant [8] or PowerQuant
[46] as we use symmetric quantization with no zero points.
Still, PowerQuant achieves results on par with post-training
methods such as AdaRound on ResNet architectures and
even outperforms AdaRound on vision transformer quanti-
zation. Interestingly, our empirical study shows that exten-
sions of AdaRound, such as BrecQ [28], QDrop [43] and
PD-Quant [31] offer strong improvements on MobileNet
architectures but are less effective on other architectures.
This can be attributed to two factors: first, we applied the
same training procedure as in AdaRound [34] with 10K
optimization steps (contrary to the 20K used in QDrop and
PD-Quant). Second, their initialization corresponds to the
DFQ baseline, which suffers from huge accuracy drops in
W4/A4 and thus limits the ability of such methods to match
NUPES, especially on vision transformers or at low bit-
width formats.
The proposed NUPES method significantly and con-
sistently outperforms state-of-the-art methods on all the
benchmarked models and tasks in W4/A4 quantization. In
particular, we observe that the largest contribution comes
from the optimization of the weight values as the second
last row is also the second best performance on each ar-
chitecture. We also emphasize that the improvements over
AdaRound introduced in other post-training techniques

--- PAGE 9 ---
PRE-PRINT VERSION 9
TABLE 6
Evaluation of the proposed method with group-wise quantization on
LLMs in W3/A16 and grouping of size 128. We report the average
score on common sense reasoning tasks like in Table 5.
LLM DFQ OPTQ PowerQ PowerQ + Group-Wise
OPT 13B 35.210 52.643 46.448 54.472
OPT 30B 32.184 51.132 47.758 56.377
[28], [31], [43] could also be adapted to NUPES for further
accuracy improvements.
As our results suggest, NUPES shows its highest scores
on the transformer architecture. This can be, in part, at-
tributed to the performance of the baseline PowerQuant
method which is well suited for the weight and activation
distributions learned by such architectures. In the following
section, we study the empirical ability of PowerQuant to
quantize the largest transformers i.e.large language models.
In our evaluation, we propose two discussions: first, Pow-
erQuant without in its current form, second, PowerQuant in
combination with group-wise quantization.
4.4 Quantization at all Sizes: handling outliers
In Table 5, we report our results for several large language
models on common sense reasoning tasks. We do not use
group-wise quantization as it leads to incompatibility with
activation quantization due to the constraint of dimension-
ality as explained in SPIQ [45]. In other words, while we
can demonstrate that group-wise quantization can lead to
higher compression rate for the weights, such methods are
bound to never quantize the activations. Consequently, we
first highlight the fact that, to the best of our knowledge,
PowerQuant bares the most promising results for the future
of LLM inference. In fact, our results suggest that it pre-
serves the performance of the full-precision model with up
to 1 point drop using 4 bits quantization without any spe-
cific techniques to address the presence of outliers. This can
be explained by the ability to stretch out the quantization
step of larger values as illustrated in Figure 3 (sec 3.1).
In Table 6, we report the the influence of group-wise
quantization [37] on post-training quantization for LLMs
(and add DFQ as a reference). While some methods, like
OPTQ [17], leverage re-ordering and grouping with success
in terms of accuracy, we foresee that this will lead to future
bottlenecks. Nonetheless, these methods can be combined
with PowerQuant in a straightforward manner and imme-
diately offer significant accuracy improvements for 3 bits
quantization. We observe that PowerQuant with group-wise
quantization unlocks near full-precsion performance (less
than one percent drop) using only 3 bits. As a result, an
OPT model with 13 billion parameters can be loaded on a
device with only 5GB of memory rather than 26GB initially.
5 C ONCLUSION
In this work, we proposed a novel, non-uniform, post-
training quantization technique. This method is based on
the search for the best quantization operator among trans-
formations that map multiplications to multiplications that
we introduced in PowerQuant, a data-free quantizationmethod. Because such transformations are power functions,
the search for the best operator consists in the optimization
of the power exponent. However, PowerQuant suffered
from two limitations that we addressed in NUPES. First, we
observed that PowerQuant could not be naively combined
with gradient-based post-training quantization schemes
such as AdaRound which are known for their strong re-
sults. NUPES alleviates this limitation by introducing dif-
ferentiable soft quantization. Second, the optimization of
the power exponent was initially derived from the weight
values only and was not compatible with stochastic gradient
descent. In NUPES, we solved this problem, enabling per-
layer adaptation of PowerQuant and thus unlocking even
higher accuracies. On top of significantly improving the per-
formance of PowerQuant, NUPES also diminishes the mem-
ory footprint and computational requirements of GPTQ
methods by using a single weight tensor and a single loss
term instead of two. As a result, our empirical benchmark
on ConvNets and transformers highlight that NUPES vastly
outperforms other post-training compression techniques.
In particular, on transformers, PowerQuant alone already
shows remarkable result, in particular, at handling outliers.
Consequently, we evaluated it on large language models.
Our empirical results show that this non-uniform quanti-
zation scheme unlocks 4 bits and 3 bits weight encoding
without damaging the accuracy nor requiring re-ordering
and grouping which are incompatible with activation quan-
tization.
Edouard YVINEC received his master’s degree
from Ecole Normale Superieure Paris-Saclay in
2020 and is currently a P .h.D. student at ISIR
in Sorbonne Universit ´e. His research interests
include but are not limited to DNN solutions for
computer vision tasks, compression and accel-
eration of such models.
Arnaud DAPOGNY is a computer vision re-
searcher at Datakalab. He graduated from
Sup´elec in 2011, Sorbonne University in 2013,
and obtained a PhD at Institute for Intelli-
gent Systems and Robotics (ISIR) in 2016. He
worked as a post-doctoral fellow at LIP6. His
works concern deep learning for computer vision
and its application to automatic facial behavior
as well as gesture analysis.
Kevin BAILLY is associate professor with the In-
stitute of Intelligent Systems and Robotics (ISIR)
at Sorbonne University and Head of Research
of Datakalab. He received the PhD degree in
computer science from the Pierre et Marie Curie
University in 2010 and was a postdoctoral re-
searcher at Telecom Paris from 2010 to 2011.
His research interests are in machine learning
and computer vision applied to face processing
and behavior analysis.

--- PAGE 10 ---
PRE-PRINT VERSION 10
REFERENCES
[1] Mart ´ın Abadi et al. TensorFlow: Large-scale machine learning
on heterogeneous systems, 2015. Software available from tensor-
flow.org.
[2] Ron Banner, Yury Nahshan, and Daniel Soudry. Post training 4-
bit quantization of convolutional networks for rapid-deployment.
NeurIPS , pages 7950–7958, 2019.
[3] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa:
Reasoning about physical commonsense in natural language. In
AAAI , volume 34, pages 7432–7439, 2020.
[4] Wenlin Chen, James Wilson, et al. Compressing neural networks
with the hashing trick. ICML , pages 2285–2294, 2015.
[5] Yu Cheng, Duo Wang, et al. A survey of model compres-
sion and acceleration for deep neural networks. arXiv preprint
arXiv:1710.09282 , 2017.
[6] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom
Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq:
Exploring the surprising difficulty of natural yes/no questions.
arXiv preprint arXiv:1905.10044 , 2019.
[7] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sab-
harwal, Carissa Schoenick, and Oyvind Tafjord. Think you have
solved question answering? try arc, the ai2 reasoning challenge.
arXiv preprint arXiv:1803.05457 , 2018.
[8] Guo Cong, Qiu Yuxian, Leng Jingwen, Gao Xiaotian, Zhang Chen,
Liu Yunxin, Yang Fan, Zhu Yuhao, and Guo Minyi. Squant: On-
the-fly data-free quantization via diagonal hessian approximation.
ICLR , 2022.
[9] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Bi-
naryconnect: Training deep neural networks with binary weights
during propagations. NeurIPS , pages 3123–3131, 2015.
[10] Matthieu Courbariaux, Itay Hubara, et al. Binarized neural net-
works: Training deep neural networks with weights and activa-
tions constrained to+ 1 or-1. NeurIPS , 2016.
[11] Data-Bricks. Databricks’ dolly-v2, an instruction-following large
language model trained on the databricks machine learning plat-
form. huggingface reference (https://huggingface.co/databricks),
2023.
[12] J. Deng, W. Dong, et al. ImageNet: A Large-Scale Hierarchical
Image Database. CVPR , 2009.
[13] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettle-
moyer. Llm. int8 (): 8-bit matrix multiplication for transformers
at scale. arXiv preprint arXiv:2208.07339 , 2022.
[14] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettle-
moyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint
arXiv:2305.14314 , 2023.
[15] Alexey Dosovitskiy et al. An image is worth 16x16 words:
Transformers for image recognition at scale. ICLR , 2021.
[16] Daniel Falbel. torchvision: Models, Datasets and Transfor-
mations for Images , 2023. https://torchvision.mlverse.org,
https://github.com/mlverse/torchvision.
[17] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.
Optq: Accurate quantization for generative pre-trained transform-
ers. In ICLR , 2023.
[18] Ruihao Gong et al. Differentiable soft quantization: Bridging full-
precision and low-bit neural networks. In ICCV , pages 4852–4861,
2019.
[19] Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Com-
pressing deep convolutional networks using vector quantization.
arXiv preprint arXiv:1412.6115 , 2014.
[20] Ali Hatamizadeh, Hongxu Yin, Holger R Roth, Wenqi Li, Jan
Kautz, Daguang Xu, and Pavlo Molchanov. Gradvit: Gradient
inversion of vision transformers. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pages 10021–
10030, 2022.
[21] Kaiming He, Xiangyu Zhang, et al. Deep residual learning for
image recognition. CVPR , pages 770–778, 2016.
[22] Horst Herrlich. Axiom of choice , volume 1876. Springer, 2006.
[23] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv,
and Yoshua Bengio. Binarized neural networks. NeurIPS , 29, 2016.
[24] HuggingFace. The opt model was proposed in open pre-trained
transformer language models by meta ai. https://huggingface.
co/docs/transformers/model doc/opt.
[25] Yongkweon Jeon, Baeseong Park, Se Jung Kwon, Byeongwook
Kim, Jeongin Yun, and Dongsoo Lee. Biqgemm: matrix multipli-
cation with lookup table for binary-coding-based quantized dnns.
SC20: International Conference for High Performance Computing, Net-
working, Storage and Analysis , pages 1–14, 2020.
[26] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980 , 2014.[27] Raghuraman Krishnamoorthi. Quantizing deep convolutional
networks for efficient inference: A whitepaper. arXiv preprint
arXiv:1806.08342 , 2018.
[28] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang,
Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of
post-training quantization by block reconstruction. ICLR , 2021.
[29] Tsung-Yi Lin et al. Microsoft COCO: common objects in context.
CoRR , abs/1405.0312, 2014.
[30] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr
Doll ´ar. Focal loss for dense object detection. In ICCV , pages 2980–
2988, 2017.
[31] Jiawei Liu, Lin Niu, Zhihang Yuan, Dawei Yang, Xinggang Wang,
and Wenyu Liu. Pd-quant: Post-training quantization based on
prediction difference metric. In CVPR , pages 24427–24437, 2023.
[32] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
Can a suit of armor conduct electricity? a new dataset for open
book question answering. arXiv preprint arXiv:1809.02789 , 2018.
[33] Daisuke Miyashita, Edward H Lee, and Boris Murmann. Con-
volutional neural networks using logarithmic data representation.
arXiv preprint arXiv:1603.01025 , 2016.
[34] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos
Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding
for post-training quantization. In ICML , pages 7197–7206. PMLR,
2020.
[35] Markus Nagel, Mart van Baalen, et al. Data-free quantization
through weight equalization and bias correction. ICCV , pages
1325–1334, 2019.
[36] John A Nelder and Roger Mead. A simplex method for function
minimization. The computer journal , 7(4):308–313, 1965.
[37] Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim,
Youngjoo Lee, and Dongsoo Lee. nuqmm: Quantized matmul for
efficient inference of large-scale generative language models. arXiv
preprint arXiv:2206.09557 , 2022.
[38] Sebastian Ruder. An overview of gradient descent optimization
algorithms. arXiv preprint arXiv:1609.04747 , 2016.
[39] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and
Yejin Choi. Winogrande: An adversarial winograd schema chal-
lenge at scale. Communications of the ACM , 64(9):99–106, 2021.
[40] Mark Sandler, Andrew Howard, et al. Mobilenetv2: Inverted
residuals and linear bottlenecks. CVPR , pages 4510–4520, 2018.
[41] Mingxing Tan and Quoc V Le. Efficientnet: Rethinking model
scaling for convolutional neural networks. ICML , pages 6105–
6114, 2019.
[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin.
Attention is all you need. NeurIPS , 2017.
[43] Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, and Feng-
wei Yu. Qdrop: randomly dropping quantization for extremely
low-bit post-training quantization. ICLR , 2022.
[44] Shoukai Xu, Haokun Li, Bohan Zhuang, Jing Liu, Jiezhang Cao,
Chuangrun Liang, and Mingkui Tan. Generative low-bitwidth
data free quantization. ECCV , pages 1–17, 2020.
[45] Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, and Kevin
Bailly. Spiq: Data-free per-channel static input quantization. arXiv
preprint arXiv:2203.14642 , 2022.
[46] Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, and Kevin
Bailly. Powerquant: Automorphism search for non-uniform quan-
tization. In ICLR , 2023.
[47] Rowan Zellers et al. Hellaswag: Can a machine really finish your
sentence? arXiv preprint arXiv:1905.07830 , 2019.
[48] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya
Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li,
Xi Victoria Lin, et al. Opt: Open pre-trained transformer language
models. arXiv preprint arXiv:2205.01068 , 2022.
[49] Sai Qian Zhang, Bradley McDanel, HT Kung, and Xin Dong.
Training for multi-resolution inference using reusable quantiza-
tion terms. In ICASP , pages 845–860, 2021.
[50] Yichi Zhang, Zhiru Zhang, and Lukasz Lew. Pokebnn: A binary
pursuit of lightweight accuracy. In CVPR , pages 12475–12485,
2022.
[51] Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru
Zhang. Improving neural network quantization without retraining
using outlier channel splitting. ICML , pages 7543–7552, 2019.
[52] Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen.
Incremental network quantization: Towards lossless cnns with
low-precision weights. ICLR , 2017.

--- PAGE 11 ---
PRE-PRINT VERSION 11
APPENDIX A
DISCUSSION AROUND THE SET OF AUTOMOR -
PHISMS OF (R∗
+,×)
In this section, we provide a simple proof on the definition
of the set of Automorphisms of (R∗
+,×).
Lemma 1. The set of continuous automorphisms of (R∗
+,×)is
defined by the set of power functions Q={Q:x7→xa|a∈R}.
Proof. We have that ∀x∈R+, Q(x)×Q(0) = Q(0)and
∀x∈R+, Q(x)×Q(1) = Q(x)which induces that Qis
either the constant 1orQ(0) = 0 andQ(1) = 1 . Because Q
is an automorphism we can eliminate the first option. Now,
we will demonstrate that Qis necessarily a power function.
Letnbe an integer, then
Q(xn) =Q(x)×Q(xn−1) =Q(x)2×Q(xn−2) =···=Q(x)n.
(12)
Similarly, for fractions, we get Q(x1
n)× ··· × Q(x1
n) =
Q(x)⇔Q(x1
n) =Q(x)1
n. Assuming Qis continuous, we
deduce that for any rational a∈R, we have
Q(xa) =Q(x)a(13)
In order to verify that the solution is limited to power
functions, we use a reductio ad absurdum . Assume Qis not
a power function. Therefore, there exists (x, y)∈R2
+and
a∈Rsuch that Q(x)̸=xaandQ(y) =ya. By definition
of the logarithm, there exists bsuch that xb=y. We get the
following contradiction, from (13),
(
Q(xba) =Q(ya) =ya
Q(xba) =Q(xab) =Q(xa)b̸=
xab=ya(14)
Consequently, the suited functions Qare limited to power
functions i.e.Q={Q:x7→xa|a∈R}.
The definition of the other automorphisms of (R∗
+,×)
requires the axiom of choice [22]. Such automorphisms are
not applicable in our case as we work on a finite subset of R
defined by floating values.
APPENDIX B
CONVEXITY AND UNIQUENESS
B.1 Local Convexity
We provide the main steps of the proof, from PowerQuant
[46], of the local convexity of the minimization problem
from equation 4.
Lemma 2. The minimization problem defined as
arg min
anx−Q−1
a(Qa(x))
po
(15)
is locally convex around any solution a∗.
Proof. We recall that∂xa
∂a=xalog(x). The functionx−Q−1
a(Qa(x))is differentiable. We assume xpositive
without loss of generality and note y= max |x|, then
∂Q−1
a(Qa(x))
∂a=∂j
(2b−1−1)xa
yak
ya
2b−1−11
a
∂a. (16)with B= 2b−1−1. By using the standard differentiation
rules, we get,
∂Q−1
a(Qa(x))
∂a=−a2y
j
B
x
yak
B
1
a
log
j
B
x
yak
B
.
(17)
Now we can compute the second derivative of Q−1
a(Qa(x)),
and get


T1 =1−p
p2|xi−Q−1
a(Qa(xi)|1
p
(xi−Q−1
a(Qa(xi))2
∂Q−1
a(Qa(x))
∂a2
T2 =(xi−Q−1
a(Qa(xi))|xi−Q−1
a(Qa(xi)|1
p−2
p
T3 =∂2Q−1
a(Qa(x))
∂a2
∂2|x−Q−1
a(Qa(x))|
∂a2 =T1+T2×T3
(18)
We know that T1>0and T3>0, consequently,
and T2is continuous in a. At a∗the terms with
|xi−Q−1
a(Qa(xi))|are negligible in comparison with
∂2Q−1
a(Qa(x))
∂a2 and
∂Q−1
a(Qa(x))
∂a2
. Consequently, there ex-
ists an open set around a∗where T1>|T2|T3, and
∂2|xi−Q−1
a(Qa(xi))|
∂a2 >0. This concludes the proof.
B.2 Uniqueness of the Solution
In this section we provide a summary of the proof on
the uniqueness of the solution of the minimization of the
quantization reconstruction error from PowerQuant [46].
Lemma 3. The minimization problem over x∈RNdefined as
arg min
anx−Q−1
a(Qa(x))
po
(19)
has almost surely a unique global minimum a∗.
Proof. We assume that xcannot be exactly quantized,
i.e.minanx−Q−1
a(Qa(x))
po
>0which is true al-
most everywhere. We use a reductio ad absurdum and as-
sume that there exist two optimal solutions a1anda2
to the optimization problem. We expand the expressionx−Q−1
a(Qa(x))
pand note the rounding term Ra, we
get
x−Q−1
a(Qa(x))
p=x−Ramax|x|a
2b−1−11
a
sign(x)
p.
(20)
•Assume Ra1=Ra2=R, the minimization problem
is convex and has a unique solution, thus a1=a2.
•Assume Ra1̸=Ra2, we note D(Ra)the set of values
asuch that the rounding function outputs the same
value. Then there exists a value a∗=ta1+ (1−
t)a2with t∈[0; 1] such that a∗has a strictly lower
quantization error than a1anda2which is absurd
