# 2203.14645.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/quantization/2203.14645.pdf
# File size: 736559 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
REx: Data-Free Residual Quantization Error
Expansion
Edouard Yvinec1,2, Arnaud Dapogny2, Matthieu Cord1, Kevin Bailly1,2
Sorbonne Université1, CNRS, ISIR, f-75005, 4 Place Jussieu 75005 Paris, France
Datakalab2, 114 boulevard Malesherbes, 75017 Paris, France
ey@datakalab.com
Abstract
Deep neural networks (DNNs) are ubiquitous in computer vision and natural
language processing, but suffer from high inference cost. This problem can be
addressed by quantization, which consists in converting floating point operations
into a lower bit-width format. With the growing concerns on privacy rights, we
focus our efforts on data-free methods. However, such techniques suffer from their
lack of adaptability to the target devices, as a hardware typically only supports
specific bit widths. Thus, to adapt to a variety of devices, a quantization method
shall be flexible enough to find good accuracy v.s. speed trade-offs for every
bit width and target device. To achieve this, we propose REx, a quantization
method that leverages residual error expansion, along with group sparsity. We show
experimentally that REx enables better trade-offs (in terms of accuracy given any
target bit-width) on both convnets and transformers for computer vision, as well as
NLP models. In particular, when applied to large language models, we show that
REx elegantly solves the outlier problem that hinders state-of-the-art quantization
methods. In addition, REx is backed off by strong theoretical guarantees on the
preservation of the predictive function of the original model. Lastly, we show that
REx is agnostic to the quantization operator and can be used in combination with
previous quantization work.
1 Introduction
Deep neural networks (DNNs) achieve outstanding performance on several challenging computer
vision tasks such as image classification [ 1], object detection [ 2] and semantic segmentation [ 3]
as well as natural language processing benchmarks such as text classification [ 4]. However, their
accuracy comes at a high computational inference cost which limits their deployment, moreso on edge
devices when real-time treatment as well as energy consumption are a concern. This problem can be
tackled viaDNN quantization, i.e.by reducing the bit-width representation of the computations from
floating point operations (FP) to e.g. int8 (8-bits integer representation), int4, int3 or even lower-bit
representation such as ternary (where weights values are either −1,0or+1) quantization. Because
DNN inference principally relies on matrix multiplication, such quantization dramatically diminishes
the number of bit-wise operations (as defined by [ 5]), thus limiting the DNN latency and energy
consumption. However, DNN quantization usually comes at the expense of the network accuracy. As
a consequence, DNN quantization is an active field of research [ 6,7,8,9,10,11,12,13] that aims at
limiting this accuracy drop while reducing the number of bit-wise operations.
All the aforementioned methods are data-driven as they either involve training a network from scratch
or fine-tune an already trained and quantized one. However, while such approaches usually allow
lower quantization errors using low bit-wise representations, due to the growing concerns on privacy
rights and data privacy, there is an ever increasing number of real-case scenarios (e.g. health and
Preprint. Under review.arXiv:2203.14645v3  [cs.CV]  29 May 2023

--- PAGE 2 ---
military services) where data may not be available for quantization purpose. Furthermore, the bloom
of large langage models (LLMs) that are very expensive to train further motivates the use of post-hoc
data-free quantization methods. Motivated by these observations, several data-free quantization
algorithms were published in the recent years [ 14,15,16,17,18,19], which focus on the quantization
operator, i.e.the transformation which maps the floating point weights to their low-bit, fixed point,
values. However, these approaches still struggle to offer an interesting alternative to data-driven
techniques in terms of accuracy.
Furthermore, when considering a specific target device for deployment, traditional quantization
methods, usually focusing on the quantization operator, offer limited options: given a supported bit
width (given by the device, as most hardware usually support only a few representation formats [ 20])
they either achieve satisfactory accuracy or not. To address this concern, we wish to design a flexible
quantization method, i.e.one that can provide several accuracy vs.speed trade-off points for each
bit width. Drawing inspiration from wavelets-based methods for image compression [ 21,22], we
tackle this limitation by considering the successive residual quantization errors between the quantized
and original model. Increasing the number of residuals in the expansion ( i.e.the expansion order)
increases the fidelity to the original, non-quantized model at the expense of additional computations.
In addition, we propose a group-sparse expansion which allows us to maintain the accuracy using
significantly less bit operations. Hence, given a target device, our approach allows to find the best
accuracy vs.speed trade-off. Our contributions are thus four-fold:
•REx, a data-free quantization method that is both efficient and flexible. REx lever-
ages residual quantization, along with group-sparsity to enable finding suitable trade-offs
depending on a target bit-width.
•Theoretical guarantees on both the exponential convergence of the quantized model towards
the full-precision model and the maximum error with respect to the predictive function.
This is of paramount importance in a data-free context, where we cannot easily measure the
accuracy degradation.
•Extensive empirical validation we show through a thorough empirical validation that, as
a standalone method, REx significantly outperforms every state-of-the-art data-free quan-
tization technique, allowing to find better trade-offs on a variety of benchmarks involving
ConvNet for classification, object detection or semantic segmentation as well as transformers
on GLUE text classification.
•A ready-to-use solution that uses a single binary residual to handle outliers within the
weight distributions, which is a well-known pitfall when attempting to quantize LLMs.
2 Related Work
2.1 Quantization
In this section, we review existing methods for DNN quantization, with an emphasis on approaches
geared towards run-time acceleration. The vast majority of DNN quantization techniques rely on data
usage (Quantization Aware Training). Furthermore, methods such as [ 7,8,9,10,11,23,24] rely on
variants of straight through estimation to alleviate the rounding operation gradients. Among these
methods, [ 25] bears the most resemblance with the proposed REx method. It minimizes the residual
error during training, using weight decay over the residue. The similarity with REx comes from the
use of a second order expansion of the quantization errors. However, it discards the quantization error
after training while we propose to keep the extra operations in order to ensure a high fidelity to the
provided pre-trained model.
2.2 Data-Free Quantization
Nagel et al. [14] discuss the necessity to have data available so as to successfully design a quantization
pipeline. They proposed a method that consists in balancing the weight ranges over the different
layers of a model, using scale invariance properties that are specific to piece-wise affine (e.g. ReLU)
activation functions, and relying on a traditional, naive quantization operator [ 5]. In the current state
of data-free quantization research, we see two major trends: methods that focus on the rounding
operator itself [ 19,26] and methods that generate synthetic data [ 27,12,13]. With REx, we aim
2

--- PAGE 3 ---
F(4)
(x)
(a) residual expansionx
R(k=1)
R(k=2)
R(k=3)
R(k=4)
R(k=1)
R(k=2)
R(k=3)
R(k=4)
F(4)
(x)x
Rγ=0.5(k=1)
Rγ=0.5(k=2)
Rγ=0.5(k=3)
Rγ=0.5(k=4)
R(k=1)
Rγ=0.5(k=2)
Rγ=0.5(k=3)
Rγ=0.5(k=4)
~
F(4)
(x)x
Rγ=0.5(k=1)
Rγ=0.5(k=2)
Rγ=0.5(k=3)
Rγ=0.5(k=4)
R(k=1)
Rγ=0.5(k=2)
Rγ=0.5(k=3)
Rγ=0.5(k=4)
(b) group-sparse expansion (c) ensemble expansionFigure 1: Illustration of the proposed method for a two-layers neural network. (a)residual expansion at order 4:
the intensity of the colormap indicates the magnitude of the residual error. (b)group-sparse expansion for orders
k≥1(γ= 50% sparsity).
at enabling hardware flexibility for these methods by allowing to find better trade-offs in terms of
accuracy and compression rate given a fixed bit-width.
2.3 Flexibility in Quantization
In practice, the existing data-free quantization methods only offer a single possible quantized model
given a supported bit-width. Nevertheless, most hardwares do not support a wide range of bit-width.
For instance, Turing [ 28] and Untether [ 29] architectures support int4 and int8 quantization while the
Nvidia A100 [ 20] supports int8, int4 and binary (int1) quantization. Conversely, REx circumvents
this limitation by offering several trade-offs given a bit-width representation.
3 Methodology
Let’s consider F, a trained network with Llayers and trained weights Wl. Given a target integer
representation in bbits, e.g. int8 or int4, we consider a quantization operator Q. Formally, Qmaps
[min{Wl}; max{Wl}]⊂Rto the quantized interval [−2b−1; 2b−1−1]∩Z. The most straightforward
way to do so is to apply a scaling sWland round ⌊·⌉the scaled tensor, i.e.:
Q(Wl) =Wl
sWl
(1)
With sWlthe quantization scale for Wlcomputed as in [ 5], without loss of generality. Following the
standard formulation [ 30], a quantization operator Q, comes with a de-quantization operator Q−1. For
the simple quantization operator Qin Equation (1), a natural choice is Q−1(Q(Wl)) =sWl×Q(Wl).
Note that, despite the notation, Q−1is not a true inverse , as by definition of the quantized space, there
is some loss of information. This loss, called the quantization error, is defined as: Wl−Q−1(Q(Wl)).
In data-free quantization, we want to minimize this error in order to achieve the highest possible
fidelity to the original model. In the following section, we describe how we can efficiently reduce the
quantization error for a fixed target bit-width b.
3.1 Residual Expansion
We propose to quantize the residual errors introduced by the quantization process. Although the
proposed method can be applied to any tensor, let’s consider a weight tensor W. In the full-precision
space ( R), its first approximation is R1=Q−1(Q(W)). To reduce the quantization error, we define
R2as the quantized residual error
R2=Q−1 
Q 
W−R1
(2)
Consequently, during the quantized inference, we compute R1X+R2X≈WX which provides
a finer approximation than the simple evaluation R1X. The process can be generalized to any
3

--- PAGE 4 ---
expansion order K, leading to the following:
RK=Q−1 
Q 
W−K−1X
k=1Rk!!
(3)
The resulting expanded layer is illustrated in Figure 1 (a) in the case K= 4. Intuitively, an expansion
(R1, ..., RK)provides the approximationPK
k=1RkofWand this approximation converges exponen-
tially fast to the original full-precision weights with respect to K. As the support of the quantization
error space is smaller than one quantization step, the error decreases by a factor larger than 2bwith
each expansion term (more details in Appendix A). Furthermore, as the quantization error decreases,
it is expected that the prediction of the quantized model would achieve a closer match to the original
one. This is especially important in the context of data-free quantization as not only do we not have
the option to perform fine-tuning to recover accuracy, but also we cannot evaluate the degradation
of the model on a calibration/validation set. Nonetheless, we can estimate an upper bound on the
maximum error ϵmaxintroduced by quantization on the predictions as
ϵmax≤U=LY
l=1 lX
i=11
2b−1−1K−1sRi
2+ 1!
−1 (4)
where sRiis the scaling factor from equation 1 applied to each residue. The detailed derivations are
provided in Appendix B. This implies that, in practice and regardless on the quantization operator, a
network can be quantized with high fidelity with only a few expansion orders to fit a given bit-width.
Furthermore, this process can also be applied to the activations.
3.2 Input Expansion
Quantizing the weights of a DNN with the aforementioned method already leads to significant
memory footprint reduction. However, to significantly decrease the inference runtime, the inputs and
activations of each layer also have to be quantized so that each the computations can be processed in
the quantized bit-width. For that matter, let Ibe the input tensor of a layer l. We define the expansion
ofIin quantized residual errors similarly to the weights expansion. Using the generic quantization
operator Q, we get I(1)=Q(I)and define the Kthorder of quantization as
I(K)=Q 
I−K−1X
k=1Q−1(I(k))!
(5)
In order to efficiently exploit the resulting expansions, we propose to bound the accumulated order
of the weights and inputs. In other words, if we note k1the expansion order of a residue from the
inputs and k2a residue from the weights, then we only perform the computations for orders such that
k1+k2< K (the rest being negligible in comparison). As a result, the quantized layer lcomputes:
f:I7→k1+k2≤K+1X
k1,k2∈{1,...,K}2Q−1
I(k1)⊗R(k2)
(6)
where ⊗is the base operation of the layer, e.g. a convolution for a convolutional layer or a matrix
multiplication for a fully-connected layer. Similarly to the weights, the error between the full-
precision inputs and the proposed expansion of the inputs converges exponentially fast to 0with
respect to the order Kof the expansion. However, with formulations from equations (3)and(5),
the overhead computations induced by the expansion is non-negligible. In the following section, we
provide a solution to tackle this issue.
3.3 Sparse Expansion
The residual expansion as defined in equation 3 is based upon the assumption that the quantization
error is equally important for every neuron. Thus, we propose to reduce the overhead cost by only
expanding the most important neurons. However, in data-free compression we do not have access
to activations or gradients: hence, we measure the relative importance of a neuron in a layer by the
norm of its weights [ 31]. The resulting expanded layer is illustrated in Figure 1 (b). Given a target
4

--- PAGE 5 ---
budget γ(in %) of overhead computations, we only expand theγ
K−1% most important neurons. The
sparse residue is defined as:
R(k)
γ
i= (R(k))i· 1(k)
γ (7)
where 1(k)
γindicates the indices of the most important neurons. Similarly to what precedes, each
expansion order is derived sequentially from previous orders and we can bound the quantization error
for the sparse expansion (see Appendix A). The method for computing the weights of the expanded
model is summarized in Algorithm 1.
Algorithm 1 Expansion Algorithm
Require: trained DNN fwithLlayers, hyper-parameters : Kandγ, operator Q
initialize γland initialize f(K)as a clone of fwithKper-layer kernels
forl∈ {1, . . . , L }do
W←base kernel of layer linf
Wacc←0accumulated quantization error
fork∈ {1, . . . , K }do
R(k)
γl←Q(W−Wacc) 1(k)
γ ▶equation 7
setkthkernel of layer loff(K)withR(k)
γl
Wacc←Wacc+Q−1(R(k)
γl)
end for
end for
return f(K)
Also note that in the sparse expansion, we allow higher expansion orders to re-consider neurons
that were previously considered unimportant. Consequently, on top of improving the exponential
convergence as well as lowering the upper bound on the maximum error with respect to the overhead
computations, this method systematically outperforms the standard residual expansion in practice.
Proof of this result can be found in Appendix C. The budget γof overhead computations can be set so
as not to introduce computational overhead, depending on the bit-width b. For example, let’s consider
a device supporting only 8 and 1 bit (binary) quantization. If we want to achieve the same latency
as 8 bit quantization using only 1bit quantization we will have a budget lower than 700% overhead
w.r.t. a naive 1 bit quantization. Consequently, for full expansions, we get γ≤8
1−1 = 700% . This
budget is then split across layers using a simple linear repartition. This strategy gives more emphasis
to the layers closest to the prediction head which also correspond to the largest layers, and empirically
provides the best results [ 32]. As a result, given a number bit operations (BOPS), the expanded model
can better fit the inference device while preserving the full-precision accuracy. Furthermore, all the
added computations are performed in parallel which reduces their cost in practice. It allows better
trade-offs in terms of accuracy and quantization compression rate, as will be shown in the upcoming
experiments.
4 Quantization Experiments
In the following sections, we first go through the implementation requirements and efficient strategies
to fully leverage the proposed expansions. Second, we perform a comparison of each expansion
methods in order to show the flexibility of REx with respect to the bit-width. Third, we compare
REx to other quantization schemes under the constraint of equal bit operations. Finally, we validate
for each expansion their respective upper bound on the maximum error with respect to the original
predictions.
4.1 Implementation Details and Benchmarks
We ran our tests on 6 different backbones, including ConvNets and transformers and 5 tasks from
both computer vision and natural language processing. We used ImageNet [ 33], Pascal VOC 2012
[34], CityScapes dataset [ 35] and GLUE [ 36] and common sense reasoning benchmarks (details in
Appendix D).
5

--- PAGE 6 ---
% of the bit operations with respect to W8/A8% accuracy of FP model0255075100
0.20 0.30 0.40 0.50 0.60 0.70W2/A8 REx - sparse
W2/A8 REx
W4/A8 REx - sparse
W4/A8 REx
W3/A8
W4/A8
W5/A8
W6/A8Figure 2: Accuracy vs.inference time, for EfficientNet B0. The higher (accuracy) and the further to the left
(inference cost) the better. The circles show the baseline results with W3/A8, W4/A8, W5/A8 and W6/A8
quantization. The dashed lines show the trade-offs performance of REx in W4/A8 and ternary quantization
(W2/A8). Finally, the plain lines show REx (with sparsity at 10% ) also in W4/A4 and ternary quantization. The
numbers in the symbols stands for the expansion order. REx, and a fortiori the sparse version, enables better
trade-offs.
Unless stated otherwise, we apply symmetric, static, per-channel quantization as defined in [ 30]
and perform batch-normalization folding prior to any processing using the optimal method from
[37]. In order to leverage the existing efficient implementations of the convolutional layers and
fully-connected layers in CUDA, we propose to implement the expanded layer using a single kernel
rather than Kkernels. This is achieved by concatenating the kernels along the output dimension.
Consequently, the challenge of efficiently splitting the computations to fully leverage the target device
computational power is left to the inference engine. In practice, this results in both better performance
and less work in order to adapt the method to existing engines such as OpenVino [ 38] and TensorRT
[39]. We detail the implementation and overhead of the addition of the residual computations in
Appendix E. In the following section, we demonstrate the ability of REx to find good accuracy vs
speed trade-offs.
4.2 Flexible Quantization
Figure 2 shows different trade-offs enabled by REx on different bit-widths for an EfficientNet-B0
on ImageNet. First, the baseline quantization with the baseline quantization operator from [ 5] (as
depicted by the circles of different colors, one for each bit width) offers no trade-off possibility given
a specific bit-width and usually performs poorly below int8 quantization (e.g. barely reaching 20.29%
top1 accuracy in W6/A8 quantization). REx, however, in the same setup, offers several trade-offs for
each specific bit-width (e.g. int4 and ternary on Figure 2) and supporting hardware. Furthermore,
the sparse expansion enables finding more potential trade-offs (by varying the budget and expansion
order) for every bit-width. Those trade-offs are generally more interesting than comparable ones
obtained using the baseline method, which empirically confirms the theoretical results (Appendix C).
Furthermore, Figure 2 shows that using higher order, sparse residues allows to find even better
trade-offs, as, in this case, e.g. in W2/A8 we reach full-precision accuracy at order 10with 10%
sparse residues. This shows that the process converges fast with respect to the sparsity rates. All in
all, these results demonstrate the flexibility of REx to find good accuracy v.s.speed trade-offs, given
a budget of total bit operations (BOPs) to fit. In the following section, we evaluate the ability of REx
to outperform existing quantization methods in terms of equal bops.
4.3 Main Results
4.3.1 Experiments on Computer Vision Models
In order to highlight the benefits of residual quantization errors expansions as a stand alone improve-
ment upon existing methods with equal BOPs, we compare REx using the naive quantization operator
from [ 5] on a variety of reference benchmarks. First, in Table 1, we report the performance on three
different computer vision networks between state-of-the-art methods in W6/A6 quantization and
REx using a sparse expansion at order K= 2using 50% of a 4 bit representation in order to get a
similar total number of bit operations (150% of 4 bits ≈6 bits). For all networks, REx significantly
outperforms recent state-of-the-art data-free quantization methods at equal BOPs. Furthermore, we
6

--- PAGE 7 ---
Table 1: Comparison at equal BOPs with existing methods in W6/A6 and REx with W4/A6 +50% of one 4 bit
residue.
DNN method year bits Accuracy
ResNet 50full-precision 76.15
DFQ [14] ICCV’19 W6/A6 71.36
ZeroQ [17] CVPR’20 W6/A6 72.93
DSG [18] CVPR’21 W6/A6 74.07
GDFQ [40] ECCV’20 W6/A6 74.59
SQuant [19] ICLR’22 W6/A6 75.95
SPIQ [26] WACV’23 W6/A6 75.98
REx - 150%×W4/A6 76.01
MobNet v2full-precision 71.80
DFQ [14] ICCV’19 W6/A6 45.84
SQuant [19] ICLR’22 W6/A6 61.87
SPIQ [26] WACV’23 W6/A6 63.24
REx - 150%×W4/A6 64.20
EffNet B0full-precision 77.10
DFQ [14] ICCV’19 W6/A6 43.08
SPIQ [26] ICLR’22 W6/A6 54.51
REx - 150%×W4/A6 57.63
Figure 3: (left) Mean intersection over union (mIoU) of a Deeplab V3+ with MobileNet V2 backbone on
CityScapes for semantic segmentation. (right) Mean average precision (mAP) of a SSD with MobileNet V2
backbone on Pascal VOC for object detection. We add the performance of a data-free quantization solution,
DFQ [14] for comparison.
confirm these results on object detection and image segmentation as shown in Figure 3. We can
observe that REx can maintain the full precision accuracy while dividing by 3.23the number of bit
operations required to run an inference.
4.3.2 Experiments on NLP
In Table 2, we perform a similar experiment on NLP using Bert [ 4]. We can observe the generalization
of our results from ConvNets to Transformers, REx can find better accuracy per bits trade-offs as
compared to four references including non-uniform quantization [ 41]. Bert is a pre-trained model
with 86 million parameters which is now considered a medium sized model. Both the full-precision
and quantized models can fit on a single middle range GPU. However, recent state-of-the-art models,
such as OPT [ 42], are so large that they need multiple gpus just to be loaded on memory. These
models ought to be compressed for sustainable usage. In the following section, we generalize the
performance of REx to extreme model sizes.
4.3.3 Application to Handling Outliers in LLMs
A known pitfall [ 43] for quantization on LLMs, comes from the presence of extreme outliers among
their weight values. These outliers stretch out the weight values range and increase the scaling factor
in Equation 1, which, in turn, causes smaller weights to be rounded abruptly to zero. Worse, as
suggested in [ 43], this phenomenon seems to occur more as the model size increases and might
appear as a major problem for future work in large DNN quantization. In order to overcome this
7

--- PAGE 8 ---
Table 2: GLUE task quantized in W4/A8. We consider the BERT transformer architecture [ 4] and provide the
original performance from the article (original) of BERT on GLUE as well as our reproduced results (reproduced).
REx is applied to the weights with 3 bits + 33% sparse expansion.
task original reproduced
CoLA 49.23 47.90
SST-2 91.97 92.32
MRPC 89.47/85.29 89.32/85.41
STS-B 83.95/83.70 84.01/83.87
QQP 88.40/84.31 90.77/84.65
MNLI 80.61/81.08 80.54/80.71
QNLI 87.46 91.47
RTE 61.73 61.82
WNLI 45.07 43.76uniform [5] log [41] SQuant [19] SPIQ [26] REx
45.60 45.67 46.88 46.23 47.02
91.81 91.53 91.09 91.01 91.88
88.24/84.49 86.54/82.69 88.78 /85.24 88.78/85.06 88.71/85.12
83.89 /83.85 84.01/83.81 83.80/83.65 83.49/83.47 83.92 /83.85
89.56/83.65 90.30/84.04 90.34 /84.32 90.30/84.21 90.50 /84.35
78.96 /79.13 78.96/79.71 78.35/79.56 78.52/79.86 79.03 /79.96
89.36 89.52 90.08 89.64 90.08
60.96 60.46 60.21 60.21 61.20
39.06 42.19 42.56 42.12 42.63
Table 3: Evaluation on Common sense reasoning benchmarks for OPT-13B [ 42] LLM quantized in W4/A16.
For each quantization operator DFQ [ 14], SQuant [ 19] and PowerQuant [ 44], we share performance with and
without REx (noted with check marks). We also provide the original full-precision (FP) performance.
FP DFQ [14] SQuant [19] PowerQuant [44]
Use REx - ✗ ✓ ✗ ✓ ✗ ✓
HellaSwag 52.43 49.25 50.14 49.23 50.21 51.29 50.98
OpenBookQA 27.20 25.80 25.40 25.40 26.20 25.80 27.80
ARC-E 61.91 59.93 61.91 59.97 61.95 60.82 60.52
ARC-C 32.94 30.2 32.42 30.12 32.34 31.57 32.94
Winogrande 65.04 64.56 64.72 64.48 64.88 64.88 65.04
PiQA 76.88 75.84 76.17 75.84 76.30 75.90 76.93
BoolQ 65.90 54.71 65.54 54.28 65.38 70.43 69.45
Average Score 54.61 51.47 53.76 51.33 53.91 54.38 54.81
Table 4: Upper bound U(see theorem B.1 and B.2) over the maximum error as compared to the corresponding
empirical measurement Uempirical of that error for a VGG 16 [ 45] trained on ImageNet. The closer the upper
bound Uto the value Uempirical the better.
weights bit-width expansion order Ksparsity U Uempirical
8 1 ✗ 0.12 0.05
8 4 ✗ 1.99×10−71.78×10−7
8 2 50% 0.06 0.05
8 4 50% 1.17×10−70.65×10−7
challenge, we adapt REx to only quantize the outliers in a residue using binary values (W1/A16)
while the remaining weights are quantized in int4 (W4/A16). As a result, the overhead from REx
is limited to a binary expansion with over 99.8% sparsity. As listed in Table 3, our evaluation on
common sense reasoning tasks demonstrates that REx provides a significant improvement over other
quantization operators at virtually no cost. Hence, REx appears as a ready-to-use solution to the
outlier problem for quantization of LLMs.
4.4 Empirical Validation of the Theoretical Bounds
Having shown the interest of REx for quantizing various architectures for computer vision and
NLP tasks, we now empirically confirm its mathematical guarantees. In Table 4, we validate the
proposed upper bound Uin Equation 4 on the maximum error on the predictions on a VGG-16
[45] trained on ImageNet. The tightness of the provided theoretical results can be estimated from
the gap between our estimation and the empirical maximum error Uempirical from quantization on
the predictions, which is measured as the infinite norm between the full-precision and quantized
logits. We observe that a naïve 8-bits quantization ( i.e.no expansion) leads to an upper bound
U= 0.12, while we observe Uempirical = 0.05. The norms of the logits is equal to 0.3423 . Therefore,
the proposed upper bound is relatively tight and significantly lower than the logits magnitude: in such
a case, due to overconfidence, the error shouldn’t affect the classification. The proposed upper bound
8

--- PAGE 9 ---
Table 5: We report the different trade-offs achieved with REx expanding over different proposed quantization
operators in W4/A4 as compared to their performance in W8/A8, on a MobileNet V2.
method W4/A4 W4 + 25% /A4 W4 + 50% /A4 W4 + 75% /A4 W6/A6 W8/A8
naive [5] 0.1 53.11 64.20 71.61 51.47 70.92
SQuant [19] 4.23 58.64 67.43 71.74 60.19 71.68
SPIQ [26] 5.81 59.37 68.82 71.79 63.24 71.79
AdaRound [46] 56.17 61.30 69.80 71.77 68.71 71.75
BrecQ [47] 66.57 70.94 71.28 71.76 70.45 71.76
is even tighter for larger values of K, and becomes lower and lower (for both the theoretical and
corresponding empirical maximum errors) when introducing sparsity. This further demonstrates the
good properties of the proposed expansion approximation in REx in addition to the relevance of its
theoretical guarantees, which are critical in data-free quantization.
4.5 Flexibility with respect to the Quantization Operator
Most recent approaches for data-free quantization focus on designing better quantization operators.
Interestingly, as we already hinted on large language models, our approach is agnostic to the choice
of the quantization operator and can thus be combined with these approaches without bells and
whistles. In Table 5, we report the possible trade-offs achievable with REx combined with recent
approaches focusing on the quantization operator on MobileNet V2. The different trade-offs are
sorted in ascending order in terms of added overhead operations, e.g. W4 + 25% leads to less operations
than W4 + 50% . First, when used with SQuant [ 19], REx achieves full-precision accuracy in W4/A4
with only 75% overhead, even outperforming W8/A8 quantization. SPIQ [ 26], can also be adapted
with REx in order to achieve good accuracy using only 4 bits representation as it benefits from finer
weight quantization. This explains the slightly higher accuracies than SQuant using 25% and 50%
sparsity. Finally, with AdaRound [ 46] and BrecQ [ 47], two PTQ techniques, we observe similar
results as expected. In particular, BrecQ which already achieves decent accuracy in W4/A4 with a
5.23points accuracy drop gets closer to the original accuracy ( 0.86point accuracy drop) using a
quarter of the expansion. Those results demonstrate REx versatility.
5 Conclusion
In this work, we proposed a novel data-free quantization method, dubbed REx, that consists in an
expansion of residual quantization errors. Furthermore, we proposed a group-sparse version of the
residual expansion that allows to find the best accuracy v.s.speed trade-offs. We demonstrated
the exponential convergence of the quantized weights obtained through the different expansion
methods towards the full-precision model. These theoretical guarantees are crucial in the context
of data-free quantization where we cannot empirically measure the accuracy degradation in an
industrial application context. As such, REx allows to find superior trade-offs for several bit-width
representations, which allows better flexibility and adaptability to specific hardwares.
In particular, we showed the added value of REx through extensive empirical validation. It appears
that REx significantly outperforms recent data-free quantization methods on a wide range of ConvNet
architectures applied to image classification, object detection, semantic segmentation as well as
transformers architectures on GLUE text classification. Furthermore, we showed that REx allows to
efficiently handle outliers within the weight distributions, a well-known pitfall when attempting to
quantize LLMs, using a single binary residual to account for outliers. Lastly, the ideas presented in
this paper are orthogonal to most recent approaches focusing on improving the quantization operator,
and hence can straightforwardly be combined with those approaches.
5.1 Limitations:
The residual expansion method introduced in this paper does not adapt to the inter-layer importance
and runtime cost discrepancies. An interesting future work would thus consist in applying more
expansion orders on the most important layers w.r.t. the model accuracy, as well as using fewer orders
for the most computationally expensive layers.
9

--- PAGE 10 ---
References
[1]Kaiming He, Xiangyu Zhang, et al. Deep residual learning for image recognition. In CVPR , pages 770–778,
2016.
[2]Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and
Alexander C Berg. Ssd: Single shot multibox detector. In ECCV , pages 21–37. Springer, 2016.
[3]Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution
for semantic image segmentation. arXiv preprint arXiv:1706.05587 , 2017.
[4]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-
tional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
[5]Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A whitepaper.
arXiv preprint arXiv:1806.08342 , 2018.
[6]Matthieu Courbariaux, Itay Hubara, et al. Binarized neural networks: Training deep neural networks with
weights and activations constrained to+ 1 or-1. NeurIPS , 2016.
[7]Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference with integers in deep neural
networks. ICLR , 2018.
[8]Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig
Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-
arithmetic-only inference. In CVPR , pages 2704–2713, 2018.
[9]Jan Achterhold, Jan Mathias Koehler, Anke Schmeink, and Tim Genewein. Variational network quantiza-
tion. In ICLR , 2018.
[10] Christos Louizos, Matthias Reisser, Tijmen Blankevoort, Efstratios Gavves, and Max Welling. Relaxed
quantization for discretized neural networks. ICLR , 2018.
[11] Tao Sheng, Chen Feng, Shaojie Zhuo, Xiaopeng Zhang, Liang Shen, and Mickey Aleksic. A quantization-
friendly separable convolution for mobilenets. In 2018 1st Workshop on Energy Efficient Machine Learning
and Cognitive Computing for Embedded Applications (EMC2) , pages 14–18. IEEE, 2018.
[12] Kanghyun Choi, Hye Yoon Lee, Deokki Hong, Joonsang Yu, Noseong Park, Youngsok Kim, and Jinho Lee.
It’s all in the teacher: Zero-shot quantization brought closer to the teacher. In CVPR , pages 8311–8321,
2022.
[13] Yunshan Zhong, Mingbao Lin, Gongrui Nan, Jianzhuang Liu, Baochang Zhang, Yonghong Tian, and
Rongrong Ji. Intraq: Learning synthetic images with intra-class heterogeneity for zero-shot network
quantization. In CVPR , pages 12339–12348, 2022.
[14] Markus Nagel, Mart van Baalen, et al. Data-free quantization through weight equalization and bias
correction. In ICCV , pages 1325–1334, 2019.
[15] Eldad Meller, Alexander Finkelstein, Uri Almog, and Mark Grobman. Same, same but different: Re-
covering neural network quantization error through weight factorization. In ICML , pages 4486–4495,
2019.
[16] Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural network
quantization without retraining using outlier channel splitting. In ICML , pages 7543–7552, 2019.
[17] Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Zeroq: A
novel zero shot quantization framework. In CVPR , pages 13169–13178, 2020.
[18] Xiangguo Zhang, Haotong Qin, Yifu Ding, Ruihao Gong, Qinghua Yan, Renshuai Tao, Yuhang Li, Fengwei
Yu, and Xianglong Liu. Diversifying sample generation for accurate data-free quantization. In CVPR ,
pages 15658–15667, 2021.
[19] Guo Cong et al. Squant: On-the-fly data-free quantization via diagonal hessian approximation. ICLR ,
2022.
[20] Nvidia. Nvidia a100 tensor core gpu architecture. web tech report ( https://images.nvidia.
com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-
whitepaper.pdf ), 2021.
[21] Majid Rabbani. Jpeg2000: Image compression fundamentals, standards and practice. Journal of Electronic
Imaging , 11(2):286, 2002.
[22] Stephane G Mallat. A theory for multiresolution signal decomposition: the wavelet representation. In
Fundamental Papers in Wavelet Theory , pages 494–513. Princeton University Press, 2009.
[23] Karen Ullrich, Edward Meeds, and Max Welling. Soft weight-sharing for neural network compression.
ICLR , 2017.
10

--- PAGE 11 ---
[24] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low
bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160 ,
2016.
[25] Sangyun Oh, Hyeonuk Sim, Sugil Lee, and Jongeun Lee. Automated log-scale quantization for low-cost
deep neural networks. In CVPR , pages 742–751, 2021.
[26] Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, and Kevin Bailly. Spiq: Data-free per-channel static
input quantization. WACV , 2023.
[27] Yuhang Li, Feng Zhu, Ruihao Gong, Mingzhu Shen, Xin Dong, Fengwei Yu, Shaoqing Lu, and Shi Gu.
Mixmix: All you need for data-free compression are feature and data mixing. In ICCV , pages 4410–4419,
2021.
[28] Boyuan Feng, Yuke Wang, Tong Geng, Ang Li, and Yufei Ding. Apnn-tc: Accelerating arbitrary precision
neural networks on ampere gpu tensor cores. In Proceedings of the International Conference for High
Performance Computing, Networking, Storage and Analysis , pages 1–13, 2021.
[29] Cliff Robinson. Untether.ai boqueria 1458 risc-v core ai accelerator, Aug 2022.
[30] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey
of quantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630 , 2021.
[31] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural
networks for resource efficient inference. arXiv preprint arXiv:1611.06440 , 2016.
[32] Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, and Kevin Bailly. Red: Looking for redundancies for
data-free structured compression of deep neural networks. NeurIPS , 2021.
[33] J. Deng, W. Dong, et al. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR , 2009.
[34] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The
PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. http://www.pascal-
network.org/challenges/VOC/voc2012/workshop/index.html, 2012.
[35] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,
Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding.
InCVPR , pages 3213–3223, 2016.
[36] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE:
A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of
the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pages
353–355. Association for Computational Linguistics, 2018.
[37] Edouard Yvinec, Arnaud Dapogny, and Kevin Bailly. To fold or not to fold: a necessary and sufficient
condition on batch-normalization layers folding. IJCAI , 2022.
[38] Intel. Intel® distribution of openvino™ toolkit. Intel, 2022.
[39] Nvidia. Nvidia distribution of tensorrt toolkit. Nvidia , 2022.
[40] Shoukai Xu, Haokun Li, Bohan Zhuang, Jing Liu, Jiezhang Cao, Chuangrun Liang, and Mingkui Tan.
Generative low-bitwidth data free quantization. In ECCV , pages 1–17. Springer, 2020.
[41] Daisuke Miyashita, Edward H Lee, and Boris Murmann. Convolutional neural networks using logarithmic
data representation. arXiv preprint arXiv:1603.01025 , 2016.
[42] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher
Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068 , 2022.
[43] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication
for transformers at scale. arXiv preprint arXiv:2208.07339 , 2022.
[44] Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, and Kevin Bailly. Powerquant: Automorphism search
for non-uniform quantization. In ICLR , 2023.
[45] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-
tion. BMVC 2014 , 2014.
[46] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or
down? adaptive rounding for post-training quantization. In ICML , pages 7197–7206. PMLR, 2020.
[47] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu.
Brecq: Pushing the limit of post-training quantization by block reconstruction. NeurIPS , 2021.
[48] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint
arXiv:1905.10044 , 2019.
11

--- PAGE 12 ---
[49] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical common-
sense in natural language. In AAAI , volume 34, pages 7432–7439, 2020.
[50] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine
really finish your sentence? arXiv preprint arXiv:1905.07830 , 2019.
[51] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial
winograd schema challenge at scale. Communications of the ACM , 64(9):99–106, 2021.
[52] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint
arXiv:1803.05457 , 2018.
[53] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity?
a new dataset for open book question answering. arXiv preprint arXiv:1809.02789 , 2018.
[54] Mark Sandler, Andrew Howard, et al. Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR ,
pages 4510–4520, 2018.
[55] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder
with atrous separable convolution for semantic image segmentation. In ECCV , pages 801–818, 2018.
[56] Erica Klarreich. Multiplication hits the speed limit. Communications of the ACM , 63(1):11–13, 2019.
12

--- PAGE 13 ---
A Exponential Convergence
The exponential convergence can be proved for the two methods: expansion and sparse expansion.
We first prove it for the expansion on sequential models, then generalize the result to more diverse
architectures. Before detailing the proof of lemma A.1, we empirically motivate the assumption of
symmetry over the weight values distribution. In Figure 4, we plot the distributions of the weights
of several layers of a ResNet 50 trained on ImageNet. The assumption is often satisfied in practice.
Furthermore, in any instances where it would not be satisfied, it can be enforced using asymmetric
quantization.
Lemma A.1. Letfbe a layer with weights W∈Rnwith a symmetric distribution. We denote R(k)
thekthquantized weight from the corresponding residual error. Then the error between the rescaled
W(K)=Q−1(R(K))and original weights Wdecreases exponentially, i.e.:w−KX
k=1w(k)≤1
2b−1−1K−1(sR(K))i
2(8)
where wandw(k)denote the elements of WandW(k)and(sR(k))idenotes the row-wise rescaling
factor at order kcorresponding to w, as defined in equation 1.
We work on expanded layers which compute
f(K):x7→σ KX
k=1R(k)Q(x)sR(k)sx+b!
(9)
Proof. Assume K= 1, then W(1)is the result of the composition of inverse quantization operator
and quantization operator, i.e. W(1)=sWj
W
sWm
. By definition of the rounding operator we know
that|⌊a⌉ −a| ≤0.5. Thus we have |w−w(1)| ≤sW/2. Now in the case k= 2, we have by
definition of the quantization of the residual error and the property of the rounding operatorw−w(1)
sR(2)
−w−w(1)
sR(2)≤1
2(10)
where sR(2)is the rescaling factor in the second order residual R2computed from w−w(1). The
quantized weights are thus given by:w−2X
i=1w(i)≤sR(2)
2(11)
Because the weight distribution is symmetric we know that for any k,sR(K)=max{w−PK−1
k=1w(k)}
2b−1−1
or any other definition of the delta in the full-precision space. Also, by definition we have max{w−PK−1
k=1w(k)} ≤sR(K). Thus:w−KX
k=1w(k)≤1
2b−1−1sR(K)
2(12)
We conclude by using a trivial induction proof.
As an immediate consequence we have the following corollary which justifies the expansion appella-
tion:
Corollary A.2. Letfbe a layer of real-valued weights Wwith a symmetric distribution and R(k)
the kthquantized weight from the corresponding residual error. Then,
E"f−KX
k=1f(k)#
≥E"f−K+1X
k=1f(k)#
(13)
andf=P∞
k=1f(k).
The first inequality results from detailing the induction in the previous proof. Instead of an upper
bound on the error over all the scalar values we consider each error and show using the same properties
that they go down after each step. f=P∞
k=1f(k)is a direct consequence of equation 8.
13

--- PAGE 14 ---
50
40
30
20
10
−0.3−0.2−0.100.10.20.3Figure 4: Distribution of the scalar weight values of different layers of a ResNet 50 trained on ImageNet. We
observe that every distribution is symmetric around 0.
Sparse Expansion LetN(k)
idenotes the L1norm of an output channel iof the k-th order residue
R(k). The sparse residue is defined as:
R(k)
γ
i= (R(k))i· 1(k)
γ (14)
where·is the element-wise multiplication, 1(k)
γ= 1{N(k)
i≥τ(k)
γ}andτ(k)
γis a threshold defined as the
γpercentile of N(k). In other words, we remove a proportion γof channels from residue R(k)that
are the least important, as indicated by their norm N(k). Note however that these pruned channels can
be encoded in subsequent residuals, i.e.R(k′), with k′> k. The result from Lemma A.1 becomes:
Lemma A.3. Letfbe a layer of real-valued weights Wwith a symmetric distribution. Then we havew− K−1X
k=1w(k)+Q−1
R(K)
γ!
≤N(K)· 1(K)
γ
∞(sR(k))i
(2b−1−1)K2(15)
where∥∥∞is the infinite norm operator with the convention that ∥0∥∞= 1and(sR(k))idenotes the
row-wise rescaling factor at order Kcorresponding to w.
Proof. From equation 8, we have:w− K−1X
k=1w(k)+Q−1
R(K)
1!≤(sR(K))i
21
2b−1−1K
(16)
which corresponds to the case where γl= 1. Ifγl<1, we have two possibilities for w. First, the
coordinate in N(K)associated to is greater than τ(K)
γlthen we fall in the case where R(K)
γ=R(K)
and as such we have the result from equation 8 which is stronger than equation 15. Second, the
coordinate in N(K)associated to is lower than τ(K)
γl. Then we have that the difference between
the baseline weight wand the slim expansion is bounded by the expansion of lower order and the
maximum of the norm N(K)which leads to the result in equation 15.
Empirical validation: In lemma A.1 and A.3 we stated the exponential convergence to 0of the
approximation error on the weight values. In order to empirically confirm this theoretical result, we
quantize a ResNet 50 trained on ImageNet in ternary values for different orders K. As can be seen in
Figure 5, the average error per layer, exponentially converges to 0which matches our expectations.
The figure also confirms the empirical result on the strategies for γ. The higher errors are located on
the last layers, thus these layers require more attention.
14

--- PAGE 15 ---
100
10−1101102103
10−2
0 1 0.2 0.4 0.6 0.8
depth of the layeraverage neuron normFigure 5: Comparison of the average norm of the quantization error for each layers of a ResNet 50 trained on
ImageNet. We observe the exponential convergence stated in lemma A.1 and A.3.
B Upper Bound Error
Theorem B.1. LetFbe a trained Llayers sequential DNN. We note σlthe largest singular value of
Wl−P
kR(k), i.e. the spectral norm of Wl−P
kR(k). Then we have
max
∥X∥=1∥F(X)−F(X)(K)∥∞≤Ures
Ures=LY
l=1 lX
i=1σiu(K)
i+ 1!
−1(17)
where u(K)
l=
1
2b−1−1K−1(sR(K))i
2from equation 8.
Proof. Let’s consider L= 2, and F:X7→Bσ(Ax). For any Xin the domain of Fsuch that
∥X∥= 1, we have
∥F(X)∥2≤σB+σA+σBσA (18)
where σBis the largest singular value of BandσAis the largest singular value of A. Following the
definition of the 2-norm and ∞-norm, we get that
σA−A(K)≤σAu(K)
A (19)
where σA−A(K)is the largest singular value of the residual error of order K,A−A(K)andu(K)
Ais
derived from equation 8. Consequently, we get
∥F(X)−F(K)(X)∥2≤σBu(K)
B+σAu(K)
A+σBu(K)
BσAu(K)
A (20)
Sparse Expansion
Theorem B.2. LetFbe a trained Llayers sequential DNN. We note σlthe largest singular value of
Wl−P
kR(k), i.e. the spectral norm of Wl−P
kR(k). Then we have
max
∥X∥=1∥F(X)−F(X)(K)∥∞≤Usparse
Usparse =LY
l=1 lX
i=1σiu(K)
i+ 1!
−1(21)
where u(K)
l=∥N(K)· 1(K)
γ∥∞(sR(k))i
(2b−1−1)K2from equation 15.
This results is directly derived from Theorem B.1. This result can be extended to more sophisticated
architectures. To do so we simply need to address specific attributes such as skip connections,
concatenations and other activation functions.
15

--- PAGE 16 ---
Skip Connections and Concatenations In the case of skip connections, the graph is split from a
starting layer l1and split in at least two branches that are added after layer l2andl3. Assuming we
can compute the upper bound for each branch (sub-networks) we simply add these sub-errors. In the
case of U-nets, where skip connections contain skip connections, we simply perform this process
recursively.
A similar approach can be applied to address concatenations. However in this case we keep the
largest value instead of adding them.
Self-Attention and Cross-Attention blocks In order to generalize to attention modules, we need
to generalize our formula to a product of layers. Let’s consider the weight tensors of the keys Wkeys
and queries Wqueries . Then the attention scores are computed as follows
Att(X) = (Wkeys×X)T×(Wqueries×X) (22)
We want to bound the quantization error on the attention mechanism. However, the process involves
the magnitude of the inputs Xas we highlight
Error Att(X) =(Wkeys×X)T×(Wqueries×X)−  X
kR(k)
keys!
×X!T
×  X
kR(k)
queries!
×X!
(23)
If we note σkandσqthe spectral norms of the residual errors of the keys and queries respectively,
then we can simplify the previous formulation
Error Att(X) =(σk×X)T(Wqueries×X) + (Wkeys×X)T(σq×X) + (σk×X)T(σq×X)
(24)
In order to measure this influence on the softmax in the worst case scenario, we can simply compare
theσkandσqto the smallest singular values of Wqueries andWkeys. If we note αkandαqthe largest
singular values of WkeysandWqueries respectively, then we get
Error Att(X)
∥X∥≤1≤σkαq+σqαk+σkσq (25)
If we note ϵ=σkαq+σqαk+σkσqthis upper bound, then the error on the softmax scores becomes
Error Softmax (X)
∥X∥≤1≤1−e−2ϵ(26)
Other Activation Functions Although ReLU activations are predominant in modern DNNs, there
are still many other widely used activation functions such as SiLU, GeLU or even sigmoid. SiLU
and GeLU are bounded by the ReLU on the positive side which is where the highest errors occur.
Consequently, the upperbound is invariant to GeLU and SiLU activation functions (although under
more assumptions on the support, the upper bound could be tightened for ReLU and should be
modified for GeLU and SiLU). On the other hand, for sigmoid activations or similar activations (e.g.
tanh), the upper bound becomes an upper bound on Xin the domain of Finstead of Xon the unit
circle.
C Sparse Expansion Outperforms Standard Expansion
Lemma C.1. Letfbe a layer of real-valued weights Wwith a symmetric distribution. Then, for
K′< K two integers, we have:
Err
R(1)+K′X
k=2R(k)
γ1
≥Err 
R(1)+KX
k=2R(k)
γ2!
(27)
where Erris the quantization error (i.e. the absolute difference between the quantized and original
weights, as in Equation 8) and K′×γ1=K×γ2=β.
Proof. Let’s assume the layers outputs two channels. Then, we have γ1= 1 andγ2= 0.5. We
simply need to prove the result for k1= 2andk2= 1as the result will extend naturally from this
16

--- PAGE 17 ---
case. The idea of the proof consists in showing that using lower βvalues enables more possibilities
of expansions which may lead to better performance. Let’s note (W)1and(W)2the weights
corresponding to the computation of the first and second output channels respectively. Using γ1= 1,
the second order expansion correspond to either quantizing (W)1or(W)2. Assume (W)1is
chosen for R(2)
γ1. Then, R(3)
γ1will either quantize the error from (W)2or further quantizes the error
from R(2)
γ1. In the first case we end up with R(1)+Pk1
i=2R(i)
γ1=R(1)+Pk2
n=2R(i)
γ2. Otherwise,
Err
R(1)+Pk1
i=2R(i)
γ1
>Err
R(1)+Pk2
i=2R(i)
γ2
.
D Implementation Details and Datasets
We validate the proposed method on three challenging computer vision tasks which are commonly
used for comparison of quantization methods. First, we evaluate on ImageNet [ 33] (≈1.2M images
train/50k test) classification. Second, we report results on object detection on Pascal VOC 2012 [ 34]
(≈17k images in the test set). Third, we benchmark on image segmentation on CityScapes dataset
[35] (500 validation images). Our NLP results were obtained on the transfer learning task GLUE [ 36].
We also evaluate the OPT-13B [ 42] LLM on the standard common sense reasoning datasets: BoolQ
[48], PIQA [ 49], HellaSwag [ 50], WinoGrande [ 51], ARC easy and challenge [ 52] and OpenBookQA
[53]. In our experiments we used MobileNets [ 54] and ResNets [ 1] on ImageNet. For Pascal VOC
object detection we employed an SSD [ 2] architecture with MobileNet backbone. On CityScapes we
used DeepLab V3+ [ 55] with MobileNet backbone. We also test our method on VGG 16 [ 45] and
transformers such as BERT model [4] as well as large language models such as OPT-13B [42].
In our experiments, the inputs and activations are quantized using the same method as [ 14]. We count
the bit-wise operations as follows: let Wbe the real-valued weights of a d×dconvolutional layer
on input feature maps of shape D×D×niandnooutputs and stride s. Then the convolutional
product requires d2D2
s2ninofloating point multiplications. The quantized layer requires two rescaling
operations (for the quantization of the inputs and the Q−1operation) and an int- bconvolution, i.e.
niD2+D2
s2nofloating point multiplications and d2D2
s2ninoint-bmultiplications. Note that the
number of additions remains unchanged. According to [ 56] the lowest complexity for b-digits scalar
multiplication is o(blog(b))bit operations. This is theoretically achieved using Harvey-Hoeven
algorithm (also the asymptomatic bound has yet to be proved). We use this value as it is the least
favorable setup for the proposed method. As a consequence the number Ooriginal bit operations
required for the original layer, OR(1)the number of bit operations for the naively quantized layer and
OR(k)for the ithorder residual quantization expansion are


Ooriginal =D2d2nino
s232 log(32)
OR(1)=D2h
(ni+no
s2)32 log(32) +d2nino
s2blog(b)i
OR(k−1)=D2h
(ni+no
s2)32 log(32) + kd2nino
s2blog(b)i(28)
Using this result we can estimate the maximum order of expansion before which the number of
operations in f(k)exceeds the Obaseline . Note that in the case of fully-connected layers, D= 1,
s= 1 andd= 1. In the following section, we use the induced metric of accuracy with respect
to the total number of bit-wise operations performed by the DNN on a single input. This metric
doesn’t consider the fact that the added operations can be performed in parallel. For SQuant [ 19],
we use our own implementation which achieve different accuracy results due to different initial
accuracies for baseline models. As for ZeroQ [ 17], we use results provided by SQuant [ 19]. Similarly
to prior work [ 15,14,19], we denote W ·/A·the quantization setup (number of bits for weight
quantization and number of bit for activation quantization). We used Tensorflow implementations of
the baseline models from the official repository when possible or other publicly available resources
when necessary. MobileNets and ResNets for ImageNet come from tensorflow models zoo. In object
detection, we tested he SSD model with a MobileNet backbone from Manish’s git repository. Finally,
in image semantic segmentation, the DeepLab V3+ model came from Bonlime’s git repository. The
networks pre-trained weights provide standard baseline accuracies on each tasks. The computations
of the residues as well as the work performed on the weights were done using the Numpy python’s
library.
17

--- PAGE 18 ---
Table 6: Overhead induced by the sum reduction in full integer implementation of REx. In this table Rmeans
one full residue without sparsity.
model W4+25% /A4 W4+50% /A4 W4+100% /A4 W4+1R/A4 W4+2R/A4
ResNet 0.035% 0.070% 0.138% 0.415% 0.830%
MobileNet v2 0.016% 0.032% 0.063% 0.189% 0.378%
BERT 0.004% 0.009% 0.018% 0.054% 0.107%
Table 7: Overhead induced by the sum reduction in full integer implementation of REx. In this table we study a
1 by 1 convolution on inputs of shape 224×224×320and output of 1280 channels as well as a depthwise
convolutions on inputs of shape 224×224×96.
Filter expansion order full runtime (ops) overhead (ops) relative cost
Conv 1x1 2 2.0×1076.2×1040.15
Conv 1x1 3 4.0×1076.1×1040.10
Conv 1x1 4 6.0×1076.2×1040.08
Depthwise Conv 3x3 2 2.7×1053.0×1045.49
Depthwise Conv 3x3 3 5.4×1053.0×1043.38
Depthwise Conv 3x3 4 8.7×1052.9×1042.43
E Expansion Reduction in the Accumulator
Let’s go though the detailed procedure we applied in order to go from the simulated quantization
with floating point scaling factors to integer only inference. We rely on the procedure introduced
in [1]. First, let’s consider the quantization of a single tensor A. Quantization is simulated using
A≈sA× ⌊A/s(A)⌉=sA×AQIn the present situation, AQis quantized and actually fits on
the target bit-width while sAis stored as a floating point value. In order to achieve integer-only
inference, we need to convert the multiplication by sAto an integer multiplication. From [1], we
rely on equation (6) and, using similar notations, we get sA×AQ≈MA×2−n×AQ. All these
operations are integer-only operations. However, in practice, these operations may add errors on
top of the quantization scheme itself. To measure this error, we conducted our own experiment
and observed that the extra error does not change the quantized output of the layer; this is due to
the fact that the term MAhas at least 30 bits of precision while AQhas 1, 4 or 8 bits of precision
(depending on the quantization bit-width). This difference in precision comes from the use of a larger
accumulator which is standard in quantized inference.
Now that we detailed how to quantize, we detail how to add two distinct tensors AandB(which will
be of special importance to add the residues, as you pointed out). In the simulated quantization, we
would get A+B≈sA× ⌊A/s(A)⌉+sB× ⌊B/s(B)⌉=sA×AQ+sB×BQ. Now, by applying
the same technique as above, we get sA×AQ+sB×BQ≈MA×2−n×AQ+MB/A×2−n×BQ,
where MB/A is the integer closest to MB/MAencoded with 30 bits of precision. This was discussed
in Appendix A.2 in [1]. The authors state that this operation is costly as it requires to perform the
integer multiplication prior to the addition. This is a result of the fact that we need to go from the
accumulator down to the quantized bit-width and then back up to the accumulator size.
In our pipeline, we limit this cost by using a fused operation in order to introduce low overhead
as compared to simply using a larger kernel size. Formally, we used the above mentioned formula
directly on the multiplication result. In other words, we get the following formula for Aand its
quantized residue RAin the case of quantization using bbits:sA×AQ+sRA×RA≈2−n×
(MA×AQ+MRA×2−b×RA). Consequently, the overhead from residual summation is limited
to a bit-shift on the residue during reduction of the accumulator. In Table 6, we report the relative
overhead introduced by this extra bit-shift in the residual summation scheme with respect to the total
inference cost. For instance, we list in Table 7 some results with Gap9 hardware: we compare the
overhead of computing several convolutional layers with the cost of the reduction of the residuals.
On convolutions, as expected the cost are completely negligible. Due to the parallelization abilities
of the hardware, as the expansion order increases, the overhead decreases. Furthermore, it should be
noted that depthwise convolutional layers are not well supported by most hardware to this day, hence
the less impressive results but similar absolute overhead cost.
18
