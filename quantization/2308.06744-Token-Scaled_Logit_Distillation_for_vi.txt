# 2308.06744.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/quantization/2308.06744.pdf
# Kích thước tệp: 2252314 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Chưng Cất Logit Có Tỷ Lệ Token cho
Mô Hình Ngôn Ngữ Sinh Với Trọng Số Tam Phân
Minsoo Kim1Sihwa Lee1Janghwan Lee1
Sukjin Hong1,2Du-Seong Chang2Wonyong Sung3Jungwook Choi1∗
1Đại học Hanyang, Seoul, Cộng hòa Hàn Quốc
2KT, Seoul, Cộng hòa Hàn Quốc  
3Đại học Quốc gia Seoul, Seoul, Cộng hòa Hàn Quốc
{minsoo2333, macto94, hwanii0288, choij}@hanyang.ac.kr
{sukjin.hong, dschang}@kt.com ,wysung@snu.ac.kr
Tóm tắt
Các Mô hình Ngôn ngữ Sinh (GLM) đã cho thấy hiệu suất ấn tượng trong các nhiệm vụ như sinh văn bản, hiểu và lý luận. Tuy nhiên, kích thước mô hình lớn đặt ra thách thức cho việc triển khai thực tế. Để giải quyết vấn đề này, Huấn luyện Nhận biết Lượng tử hóa (QAT) đã trở nên ngày càng phổ biến. Tuy nhiên, các phương pháp QAT hiện tại cho mô hình sinh đã dẫn đến sự sụt giảm độ chính xác đáng chú ý. Để chống lại vấn đề này, chúng tôi đề xuất một phương pháp chưng cất kiến thức mới được thiết kế đặc biệt cho GLM. Phương pháp của chúng tôi, được gọi là chưng cất logit có tỷ lệ token, ngăn chặn việc quá khớp và cung cấp học tập vượt trội từ mô hình giáo viên và sự thật cơ bản. Nghiên cứu này đánh dấu lần đầu tiên đánh giá huấn luyện nhận biết lượng tử hóa trọng số tam phân của GLM quy mô lớn với sự suy giảm perplexity ít hơn 1.0 và đạt được độ chính xác nâng cao trong các nhiệm vụ như QA thường thức và lý luận số học cũng như hiểu ngôn ngữ tự nhiên.2

1 Giới thiệu
Các mô hình ngôn ngữ sinh (GLM) đã đạt được những bước tiến ấn tượng trong sinh văn bản, hiểu và lý luận, thu hút sự chú ý đáng kể trong lĩnh vực này [1–7]. Tuy nhiên, việc triển khai GLM vẫn là một thách thức do kích thước mô hình khổng lồ của chúng. Có sự quan tâm tăng lên đối với GLM thực tế với ít hơn 10 tỷ tham số. Khả năng của chúng có thể được cải thiện thông qua điều chỉnh tinh hướng dẫn [8–11]. Ví dụ, Alpaca [12] đã cho thấy rằng một mô hình 7 tỷ tham số được điều chỉnh tinh có thể sánh bằng hiệu suất sinh văn bản của GLM 175 tỷ tham số, làm nổi bật tiềm năng của các mô hình nhỏ hơn, dễ quản lý hơn.

Vì GLM thực tế vẫn chứa hàng tỷ tham số, có nghiên cứu rộng rãi về các kỹ thuật nén mô hình để triển khai hiệu quả của chúng. Một phương pháp như vậy là lượng tử hóa sau huấn luyện (PTQ), giúp đơn giản hóa quá trình bằng cách giảm độ chính xác bit xuống 8 hoặc 4 bit mà không cần điều chỉnh tinh GLM đã được huấn luyện trước [13–17]. Cách tiếp cận này đã được quan tâm do thời gian xử lý đơn giản và nhanh chóng. Tuy nhiên, người ta quan sát thấy rằng những kỹ thuật này gây ra sự giảm độ chính xác đáng kể khi số lượng tham số giảm xuống dưới 10 tỷ hoặc khi độ chính xác bit giảm xuống dưới 4 bit. Kết quả là, có nhu cầu rõ ràng về một cách tiếp cận lượng tử hóa đáng tin cậy hơn cho GLM với độ chính xác dưới 4 bit.

Để đáp ứng, chúng tôi đề xuất một phương pháp thay thế, huấn luyện nhận biết lượng tử hóa (QAT), để giải quyết các vấn đề mà PTQ đặt ra cho GLM được điều chỉnh tinh. QAT là một kỹ thuật lượng tử hóa phổ biến chống lại sự mất mát độ chính xác và đạt được tỷ lệ nén cao để triển khai hiệu quả [18]. Đáng chú ý, việc điều chỉnh tinh thành công các mô hình hiểu ngôn ngữ tự nhiên dưới 4 bit đã được đạt được thông qua chưng cất kiến thức (KD) từng lớp (L2L), một phương pháp được sử dụng để bù đắp lỗi do lượng tử hóa tích cực, chẳng hạn như trọng số nhị phân hoặc tam phân [19–23]. Tuy nhiên, việc áp dụng QAT cho GLM có thành công hạn chế. Trong khi [24] giới thiệu mất mát đối lập cấp token và [25] cung cấp hiểu biết ban đầu về những thách thức của việc lượng tử hóa GLM, cả hai nghiên cứu đều gặp phải sự gia tăng đáng kể perplexity trong mô hình hóa ngôn ngữ. Hơn nữa, không có nghiên cứu nào hiện tại áp dụng QAT cho GLM với hàng tỷ tham số, chủ yếu do bản chất đắt đỏ của việc huấn luyện với KD.

Nghiên cứu này đi sâu vào những thách thức cơ bản của việc áp dụng QAT cho GLM được điều chỉnh tinh. Chúng tôi xác định hai vấn đề chính. Thứ nhất, cấu trúc của bản đồ tự chú ý trong tự chú ý có mặt nạ gây ra lỗi lượng tử hóa tích lũy qua các token, mà KD L2L thông thường khó có thể bù đắp. Thứ hai, cơ chế buộc giáo viên [26] được sử dụng trong điều chỉnh tinh bộ giải mã Transformer đòi hỏi mất mát sự thật cơ bản (GT Loss) – một yếu tố phần lớn bị bỏ qua trong các phương pháp QAT trước đây – nhưng bao gồm GT Loss có nguy cơ quá khớp. Điều tra của chúng tôi tiết lộ rằng chưng cất logit có thể vượt qua những hạn chế của L2L KD trong khôi phục dự đoán token bằng cách cải tạo các biểu diễn trung gian. Ngoài ra, chúng tôi phát hiện rằng việc áp dụng tỷ lệ logit theo token có thể giảm thiểu đáng kể nguy cơ quá khớp.

Dựa trên những phát hiện của chúng tôi, chúng tôi giới thiệu một kỹ thuật KD mới được gọi là Chưng cất Logit Có Tỷ lệ Token (TSLD), được thiết kế để tăng cường QAT cho suy luận lượng tử hóa tam phân. Chúng tôi đánh giá TSLD trên một loạt GLM – bắt nguồn từ GPT-2 [2], OPT [4] và LLaMA [5] – với nhiều kích thước khác nhau, bao gồm mô hình 7 tỷ lần đầu tiên. Kết quả cho thấy TSLD đạt được hiệu suất tương đương, nếu không muốn nói là vượt trội, trong mô hình hóa ngôn ngữ trên suy luận tam phân và 4-bit. Khi TSLD được áp dụng cho các nhiệm vụ lý luận, nó một cách đáng ngạc nhiên ngăn chặn quá khớp để đạt được độ chính xác nhiệm vụ ít nhất là ngang bằng, nếu không muốn nói là tốt hơn. Những kết quả đáng chú ý này nhấn mạnh tiềm năng của phương pháp TSLD được đề xuất trong việc tạo điều kiện triển khai GLM độ chính xác siêu thấp.

2 Công trình liên quan
Điều chỉnh tinh cho Mô hình Ngôn ngữ Sinh. GLM nổi tiếng với khả năng sinh văn bản, hiểu và lý luận tuyệt vời [1–7]. Các nghiên cứu tiết lộ rằng hiệu suất của chúng có thể được nâng cao thông qua các phương pháp điều chỉnh tinh hướng dẫn như Prefix-Tuning [11] hoặc sử dụng hướng dẫn và ví dụ ngôn ngữ tự nhiên [8]. Thú vị là các mô hình được điều chỉnh hướng dẫn, bao gồm cả những mô hình nhỏ hơn, có thể vượt trội hơn những mô hình lớn hơn trong các nhiệm vụ cụ thể [9,8,12]. Tuy nhiên, số lượng tham số lớn trong những mô hình này, so với các mô hình phổ biến như BERT [27], có thể hạn chế tính hữu ích thực tế của chúng. Để giảm thiểu điều này, chúng tôi đề xuất nghiên cứu các kỹ thuật hiệu quả, nhẹ cho GLM bao gồm tới 7 tỷ tham số.

Lượng tử hóa cho Mô hình Ngôn ngữ Sinh. Lượng tử hóa, một phương pháp giảm thiểu chi phí suy luận của các mô hình lớn bằng cách sử dụng số lượng bit hạn chế để biểu diễn trọng số, gần đây đã được áp dụng cho GLM [13–17]. Quá trình này đã cắt giảm đáng kể việc sử dụng bộ nhớ GPU và thời gian thực thi. Tồn tại hai loại lượng tử hóa chính, huấn luyện nhận biết lượng tử hóa (QAT) và lượng tử hóa sau huấn luyện (PTQ), khác nhau về yêu cầu huấn luyện lại hoặc điều chỉnh tinh. Mặc dù QAT đã được sử dụng hiệu quả trong các mô hình bộ mã hóa Transformer [22,25], việc áp dụng nó cho GLM đặt ra thách thức [24], với sự suy giảm hiệu suất được quan sát khi áp dụng cho các mô hình chỉ có bộ giải mã như GLM [25]. Bài báo này đánh giá sự mất cân bằng của lỗi lượng tử hóa dựa trên đặc điểm chú ý và mẫu sinh mô hình ngôn ngữ. Chúng tôi chứng minh rằng QAT có thể được tiến hành mà không mất mát hiệu suất đáng kể trên các nhiệm vụ khác nhau, ngay cả trong các mô hình vượt quá một tỷ tham số.

Chưng cất Kiến thức cho Nén Mô hình Ngôn ngữ. Chưng cất kiến thức (KD) là một khung học chuyển giao phổ biến truyền đạt kiến thức từ mô hình "giáo viên" lớn hơn sang mô hình "học sinh" nhỏ hơn, và nó được sử dụng hiệu quả để kiềm chế sự suy giảm độ chính xác trong các mô hình được nén thông qua huấn luyện nhận biết lượng tử hóa (QAT) [28,19,21–23]. Trong các mô hình bộ mã hóa, KD huấn luyện mô hình lượng tử hóa (học sinh) sử dụng biểu diễn trung gian của mô hình độ chính xác đầy đủ (giáo viên) làm mục tiêu trong QAT. Mặc dù hiệu quả, phương pháp này yêu cầu nhiều bộ nhớ hơn do biểu diễn trung gian từ mô hình giáo viên và đã được khám phá ít hơn trong các mô hình bộ giải mã như GLM [25,24]. Bài báo này giới thiệu một phương pháp KD mới, ít tốn bộ nhớ hơn, có thể áp dụng cho các mô hình có tới 7 tỷ tham số. Chúng tôi cung cấp phân tích kỹ lưỡng về việc chuyển giao thông tin của giáo viên trong mô hình bộ giải mã và đề xuất một phương pháp KD dựa trên QAT duy trì sự suy giảm hiệu suất tối thiểu, ngay cả khi áp dụng trọng số tam phân cho các GLM khác nhau.

--- TRANG 2 ---
sự mất mát độ chính xác và đạt được tỷ lệ nén cao để triển khai hiệu quả [18]. Đáng chú ý, việc điều chỉnh tinh thành công các mô hình hiểu ngôn ngữ tự nhiên dưới 4-bit đã được đạt được thông qua chưng cất kiến thức (KD) từng lớp (L2L), một phương pháp được sử dụng để bù đắp lỗi do lượng tử hóa tích cực, chẳng hạn như trọng số nhị phân hoặc tam phân [19–23]. Tuy nhiên, việc áp dụng QAT cho GLM có thành công hạn chế. Trong khi [24] giới thiệu mất mát đối lập cấp token và [25] cung cấp hiểu biết ban đầu về những thách thức của việc lượng tử hóa GLM, cả hai nghiên cứu đều gặp phải sự gia tăng đáng kể perplexity trong mô hình hóa ngôn ngữ. Hơn nữa, không có nghiên cứu nào hiện tại áp dụng QAT cho GLM với hàng tỷ tham số, chủ yếu do bản chất đắt đỏ của việc huấn luyện với KD.

Nghiên cứu này đi sâu vào những thách thức cơ bản của việc áp dụng QAT cho GLM được điều chỉnh tinh. Chúng tôi xác định hai vấn đề chính. Thứ nhất, cấu trúc của bản đồ tự chú ý trong tự chú ý có mặt nạ gây ra lỗi lượng tử hóa tích lũy qua các token, mà KD L2L thông thường khó có thể bù đắp. Thứ hai, cơ chế buộc giáo viên [26] được sử dụng trong điều chỉnh tinh bộ giải mã Transformer đòi hỏi mất mát sự thật cơ bản (GT Loss) – một yếu tố phần lớn bị bỏ qua trong các phương pháp QAT trước đây – nhưng bao gồm GT Loss có nguy cơ quá khớp. Điều tra của chúng tôi tiết lộ rằng chưng cất logit có thể vượt qua những hạn chế của L2L KD trong khôi phục dự đoán token bằng cách cải tạo các biểu diễn trung gian. Ngoài ra, chúng tôi phát hiện rằng việc áp dụng tỷ lệ logit theo token có thể giảm thiểu đáng kể nguy cơ quá khớp.

Dựa trên những phát hiện của chúng tôi, chúng tôi giới thiệu một kỹ thuật KD mới được gọi là Chưng cất Logit Có Tỷ lệ Token (TSLD), được thiết kế để tăng cường QAT cho suy luận lượng tử hóa tam phân. Chúng tôi đánh giá TSLD trên một loạt GLM – bắt nguồn từ GPT-2 [2], OPT [4] và LLaMA [5] – với nhiều kích thước khác nhau, bao gồm mô hình 7 tỷ lần đầu tiên. Kết quả cho thấy TSLD đạt được hiệu suất tương đương, nếu không muốn nói là vượt trội, trong mô hình hóa ngôn ngữ trên suy luận tam phân và 4-bit. Khi TSLD được áp dụng cho các nhiệm vụ lý luận, nó một cách đáng ngạc nhiên ngăn chặn quá khớp để đạt được độ chính xác nhiệm vụ ít nhất là ngang bằng, nếu không muốn nói là tốt hơn. Những kết quả đáng chú ý này nhấn mạnh tiềm năng của phương pháp TSLD được đề xuất trong việc tạo điều kiện triển khai GLM độ chính xác siêu thấp.

2 Công trình liên quan

Điều chỉnh tinh cho Mô hình Ngôn ngữ Sinh. GLM nổi tiếng với khả năng sinh văn bản, hiểu và lý luận tuyệt vời [1–7]. Các nghiên cứu tiết lộ rằng hiệu suất của chúng có thể được nâng cao thông qua các phương pháp điều chỉnh tinh hướng dẫn như Prefix-Tuning [11] hoặc sử dụng hướng dẫn và ví dụ ngôn ngữ tự nhiên [8]. Thú vị là các mô hình được điều chỉnh hướng dẫn, bao gồm cả những mô hình nhỏ hơn, có thể vượt trội hơn những mô hình lớn hơn trong các nhiệm vụ cụ thể [9,8,12]. Tuy nhiên, số lượng tham số lớn trong những mô hình này, so với các mô hình phổ biến như BERT [27], có thể hạn chế tính hữu ích thực tế của chúng. Để giảm thiểu điều này, chúng tôi đề xuất nghiên cứu các kỹ thuật hiệu quả, nhẹ cho GLM bao gồm tới 7 tỷ tham số.

Lượng tử hóa cho Mô hình Ngôn ngữ Sinh. Lượng tử hóa, một phương pháp giảm thiểu chi phí suy luận của các mô hình lớn bằng cách sử dụng số lượng bit hạn chế để biểu diễn trọng số, gần đây đã được áp dụng cho GLM [13–17]. Quá trình này đã cắt giảm đáng kể việc sử dụng bộ nhớ GPU và thời gian thực thi. Tồn tại hai loại lượng tử hóa chính, huấn luyện nhận biết lượng tử hóa (QAT) và lượng tử hóa sau huấn luyện (PTQ), khác nhau về yêu cầu huấn luyện lại hoặc điều chỉnh tinh. Mặc dù QAT đã được sử dụng hiệu quả trong các mô hình bộ mã hóa Transformer [22,25], việc áp dụng nó cho GLM đặt ra thách thức [24], với sự suy giảm hiệu suất được quan sát khi áp dụng cho các mô hình chỉ có bộ giải mã như GLM [25]. Bài báo này đánh giá sự mất cân bằng của lỗi lượng tử hóa dựa trên đặc điểm chú ý và mẫu sinh mô hình ngôn ngữ. Chúng tôi chứng minh rằng QAT có thể được tiến hành mà không mất mát hiệu suất đáng kể trên các nhiệm vụ khác nhau, ngay cả trong các mô hình vượt quá một tỷ tham số.

Chưng cất Kiến thức cho Nén Mô hình Ngôn ngữ. Chưng cất kiến thức (KD) là một khung học chuyển giao phổ biến truyền đạt kiến thức từ mô hình "giáo viên" lớn hơn sang mô hình "học sinh" nhỏ hơn, và nó được sử dụng hiệu quả để kiềm chế sự suy giảm độ chính xác trong các mô hình được nén thông qua huấn luyện nhận biết lượng tử hóa (QAT) [28,19,21–23]. Trong các mô hình bộ mã hóa, KD huấn luyện mô hình lượng tử hóa (học sinh) sử dụng biểu diễn trung gian của mô hình độ chính xác đầy đủ (giáo viên) làm mục tiêu trong QAT. Mặc dù hiệu quả, phương pháp này yêu cầu nhiều bộ nhớ hơn do biểu diễn trung gian từ mô hình giáo viên và đã được khám phá ít hơn trong các mô hình bộ giải mã như GLM [25,24]. Bài báo này giới thiệu một phương pháp KD mới, ít tốn bộ nhớ hơn, có thể áp dụng cho các mô hình có tới 7 tỷ tham số. Chúng tôi cung cấp phân tích kỹ lưỡng về việc chuyển giao thông tin của giáo viên trong mô hình bộ giải mã và đề xuất một phương pháp KD dựa trên QAT duy trì sự suy giảm hiệu suất tối thiểu, ngay cả khi áp dụng trọng số tam phân cho các GLM khác nhau.

--- TRANG 3 ---
3 Nền tảng và Thách thức

3.1 Lớp Transformer

Các mô hình ngôn ngữ sinh [1] được xây dựng với các lớp Transformer [29]. Một lớp Transformer tiêu chuẩn bao gồm hai mô-đun con chính: Chú ý Đa đầu (MHA) và Mạng Truyền tiến (FFN). Đầu vào cho lớp Transformer thứ l là Xl∈Rn×d trong đó n và d lần lượt là độ dài chuỗi và kích thước trạng thái ẩn. Gọi NH là số đầu chú ý và dh=d/NH. WQ_h,WK_h,WV_h∈Rd×dh là các tham số trọng số chiếu Xl thành Query (Qh=XlWQ_h), Key (Kh=XlWK_h), và Value (Vh=XlWV_h), tương ứng. Điểm chú ý (Ah) được tính với tích vô hướng của Query và Key được chiếu (Ah=QhK⊤_h). Phiên bản chuẩn hóa của kết quả này sau đó được truyền qua hàm softmax và nhân với Value để có đầu ra là head_h=softmax(Ah/√dh)Vh. Khi đó, đầu ra của Chú ý Đa đầu (MHA) được định nghĩa như sau:

MHA(Xl) = Concat(head_1, ..., head_NH)WO. (1)

FFN bao gồm hai lớp kết nối đầy đủ với các tham số trọng số W1 và W2:

FFN(Yl) = GeLU(YlW1+b1)W2+b2. (2)

Do đó, các phép toán tại lớp Transformer thứ l có thể được định nghĩa là:

Yl=Xl+MHA(LayerNorm(Xl)); Xl+1=Yl+FFN(LayerNorm(Yl)). (3)

3.2 QAT với KD cho Bộ Giải mã Transformer

QAT mô phỏng lượng tử hóa thời gian suy luận trong quá trình huấn luyện để học các tham số bền vững với lỗi lượng tử hóa. Cụ thể, lượng tử hóa tam phân biểu diễn tất cả các tham số trọng số (WQ,WK,WV,WO,W1,W2) thành các giá trị tam phân t∈{+1,0,−1}k cùng với hệ số tỷ lệ α cho suy luận dưới 2-bit khi triển khai. Trong công trình này, chúng tôi tuân theo cách tiếp cận của TWN [30] ước tính tối ưu α và t một cách phân tích để giảm thiểu ||w−αt||²₂, trong đó w=vec(W) và k là số phần tử của các tham số trọng số.

Do việc giảm bit tích cực, lượng tử hóa tam phân gây ra mất mát độ chính xác đáng kể. KD có thể giúp bù đắp sự suy giảm độ chính xác, trong đó mô hình độ chính xác đầy đủ gốc hoạt động như một giáo viên để hướng dẫn việc huấn luyện mô hình được lượng tử hóa như một học sinh. Trong trường hợp các mô hình Transformer, các công trình trước [19,21,22,31] áp dụng KD trên đầu ra kích hoạt Xl của mỗi lớp Transformer cũng như điểm chú ý Al với mất mát bình phương trung bình (MSE), được ký hiệu là LL2L:

LL2L=∑(l=1 to L+1) MSE(XSₗ,XTₗ) + ∑(l=1 to L) MSE(ASₗ,ATₗ), (4)

trong đó chỉ số trên S và T đại diện cho mô hình học sinh và giáo viên, tương ứng.

Các logit đầu ra cuối cùng của học sinh (ZS) và giáo viên (ZT) được sử dụng để tính mất mát entropy chéo. Cho rằng N là tổng số token trong các chuỗi đầu vào, và V là kích thước từ vựng mà mô hình ngôn ngữ có thể nhận biết và sinh ra. Sử dụng hàm softmax, chúng ta có thể chuyển đổi đầu ra logit dự đoán token thứ i của mỗi mô hình thành phân phối xác suất, sau đó được sử dụng cho mất mát chưng cất logit (Llogit):

Pi=e^(Zi,j)/∑(j=1 to V)e^(Zi,j), Llogit=−(1/N)∑(n=1 to N)∑(i=1 to V)PTn,i log(PSn,i). (5)

Mất mát tổng thể cho KD thường được tính là LKD=LL2L+Llogit, mà không có GT Loss như đã ghi nhận trong các nghiên cứu trước [19,22,31]. Tuy nhiên, một số phương pháp chỉ sử dụng Llogit [24]. Nghiên cứu của chúng tôi nhấn mạnh sự cần thiết của việc tích hợp Llogit và GT Loss để áp dụng hiệu quả QAT trong bộ giải mã Transformer.

--- TRANG 4 ---
: Lỗi Token 1 do Lượng tử hóa (Qerr): Lỗi Token 3 do Lượng tử hóa (Qerr)
Bộ mã hóa
x2x1x0xcls Tôi thích sữa
Các lớp bộ mã hóa
y2y1y0ycls zcls
Đầu z
Có thể lực lượng
Bộ giải mã
Các lớp bộ giải mã
y2y1y0
Đầu
x3x2x1
x3x2x1x0 là
1000 với lực lượng là linh hồn z2z1z0
Mất mát
đầu vào đầu ra logit nhãn
với là
Đúng Đúng với là
: Chú ý
Mất mát
Tự Chú ý Có mặt nạ
Value=
(b) Bộ mã hóa Bộ giải mã
Từ vựng (~50k)
Qerr++ Đầu ra Đầu ra
Qerr=++
(a) Tự Chú ý ☓☓
0.4 0.1 0.2 0.3 với lực lượng là linh hồn
Đúng Sai 0.8 0.2
10 Đúng Sai
Nhị phân (0 hoặc 1)
: Mặt nạ Token Token 01230123
Qerr Bộ giải mã

Hình 1: (a) Minh họa cơ chế chú ý trong mô hình bộ mã hóa (trên) và bộ giải mã (dưới). (b) Trái: thực hiện nhiệm vụ NLU [32] bằng mô hình bộ mã hóa. Phải: thực hiện nhiệm vụ mô hình hóa ngôn ngữ bằng mô hình bộ giải mã với teacher-forcing (token đầu vào độc lập với token được sinh trước đó)

3.3 Thách thức Lượng tử hóa trên GLM

Trong phần này, chúng tôi so sánh các phép tính của bộ mã hóa và bộ giải mã Transformer để hiểu sâu hơn những thách thức mới xuất hiện trong lĩnh vực GLM.

Lỗi Lượng tử hóa Tích lũy trong Chú ý Nhân quả. Chú ý nhân quả, tích hợp mặt nạ vào tự chú ý để tránh tham chiếu các token tương lai, rất quan trọng cho các nhiệm vụ mô hình hóa ngôn ngữ nhân quả. Để hiểu các đặc điểm lượng tử hóa của GLM, chúng tôi đối chiếu các quy trình tính toán của bản đồ tự chú ý. Đối với bộ mã hóa Transformer, lỗi lượng tử hóa phản ánh trên bản đồ tự chú ý do quá trình chiếu Query, Key và Value được phân bố đều qua các token vì bản chất song song token của việc tính toán vốn có trong bộ mã hóa Transformer. Tuy nhiên, mặt nạ trong chú ý nhân quả tích lũy lỗi lượng tử hóa từ mỗi token, tạo ra lỗi không đều trong việc tính toán đầu ra.

Hình 1(a) đối chiếu cơ chế chú ý của mô hình bộ mã hóa và bộ giải mã dưới lượng tử hóa. Tự chú ý của bộ mã hóa được minh họa ở phần trên của hình, được phân tách thành các thành phần tự chú ý và lỗi lượng tử hóa (Qerr). Xác suất chú ý kết hợp được sử dụng trong tổng có trọng số với Value, trong đó token 1 và 3 (trong hộp màu xanh và đỏ tương ứng) bị ảnh hưởng bởi số lượng xác suất chú ý giống nhau với lỗi lượng tử hóa. Ngược lại, chú ý nhân quả của bộ giải mã, được hiển thị ở phần dưới, chỉ sử dụng xác suất chú ý của token hiện tại và những token trước đó. Ví dụ, Value cho token 1 ở phần dưới của Hình 1(a) (trong hộp màu xanh) chỉ sử dụng hai xác suất chú ý bị ảnh hưởng bởi lỗi lượng tử hóa, trong khi token 3 (trong hộp màu đỏ) bao gồm những xác suất từ tất cả các token trước đó. Minh họa này làm nổi bật rằng chú ý nhân quả vốn dẫn đến sự tích lũy không cân xứng của lỗi lượng tử hóa trong các token sau. Do đó, chúng ta cần một chiến lược QAT bộ giải mã giải quyết sự mất cân bằng này trong mô-đun chú ý nhân quả. Trong Phần 4.1, chúng tôi đánh giá những hạn chế của các phương pháp KD hiện tại trong việc quản lý lỗi lượng tử hóa tích lũy và giới thiệu cơ chế bù đắp lỗi chưng cất logit nâng cao.

Sự Cần thiết của Mất mát Sự thật Cơ bản. Trong điều chỉnh tinh, các mô hình bộ mã hóa, thường được sử dụng trong Hiểu Ngôn ngữ Tự nhiên (NLU), và các mô hình bộ giải mã, phổ biến trong Sinh Ngôn ngữ Tự nhiên (NLG), sử dụng các cơ chế khác biệt để nhận GT Loss, như được hiển thị trong Hình 1(b). Các mô hình bộ mã hóa cho nhiệm vụ NLU sử dụng một token đặc biệt duy nhất để tính mất mát entropy chéo với số lượng lớp hạn chế [27], như được mô tả ở bên trái của Hình 1(b). Mặt khác, các mô hình bộ giải mã trong nhiệm vụ NLG dự đoán mỗi token tiếp theo, chuyển đổi biểu diễn của mỗi token thành một vector logit với kích thước lớp tương đương với kích thước từ vựng, thường vượt quá 50k [1], được hiển thị ở bên phải của Hình 1(b). Quá trình này cho phép các mô hình bộ giải mã có được GT Loss cho mỗi token đầu vào, cung cấp thông tin dự đoán cấp token chi tiết. Với những khác biệt này, có nhu cầu thuyết phục để xem xét sự cần thiết của GT Loss trong QAT của mô hình bộ giải mã theo cách từng token. Tuy nhiên, QAT trước đây [24] trên các mô hình bộ giải mã bỏ qua việc xem xét GT Loss do sự suy giảm hiệu suất được nhận thức khi GT Loss được sử dụng. Theo đó, Phần 4.2 cung cấp phân tích sâu về sự tương tác giữa KD và GT Loss trong QAT.

--- TRANG 5 ---
Chỉ số Token Chỉ số Token
(c)
(b)
Chỉ số Token Chỉ số Token
Mất mát Chú ý Token Mất mát Chú ý Token
Chỉ số Kênh Đầu ra
Chỉ số Token
Phạm vi Min-Max Kích hoạt
Số Đầu Chú ý
Tỷ lệ Xếp hạng Chú ý
Khoảng cách MSE Khoảng cách Logit
Số Lớp
Bản đồ nhiệt Mất mát Chú ý Từng Token Bản đồ nhiệt Mất mát Chú ý Từng Token
(a)

Hình 2: So sánh lỗi lượng tử hóa trọng số trên bản đồ chú ý thông qua mất mát MSE trong (a) mô hình bộ mã hóa (BERT-base, nhiệm vụ RTE) và (b) mô hình bộ giải mã (GPT-2, nhiệm vụ PTB). (c) Trái: phạm vi động min-max trên mỗi lớp (Chưng cất Logit vs Chưng cất L2L). Giữa: so sánh tỷ lệ xếp hạng chú ý (FP vs Chưng cất Logit vs Chưng cất L2L). Phải: khoảng cách logit từng token theo lớp và khoảng cách MSE

4 Phương pháp

4.1 Chưng cất Logit cho Lỗi Lượng tử hóa Tích lũy

Động lực. Bản chất vốn có của chú ý nhân quả, nơi mỗi biểu diễn token được xây dựng dựa trên biểu diễn của các token trước đó, đặt ra những thách thức chưa từng thấy khi áp dụng lượng tử hóa cho các mô hình bộ giải mã. Để hiểu rõ hơn về mô hình bộ giải mã, chúng tôi tiến hành phân tích so sánh với mô hình bộ mã hóa để xem xét tác động của lỗi lượng tử hóa lên mô hình. Trong Hình 2 (a), lỗi lượng tử hóa của bản đồ tự chú ý bộ mã hóa thể hiện sự hiện diện rộng rãi của lỗi do không có mặt nạ trong tự chú ý, và các lỗi lượng tử hóa từng token dọc theo các lớp cũng cho thấy các mẫu không đều tùy thuộc vào chỉ số token. Tuy nhiên, trong Hình 2 (b), bản đồ nhiệt của mô hình bộ giải mã tiết lộ sự gia tăng độ sáng của lỗi lượng tử hóa khi chúng ta di chuyển về phía các token sau. Khi xem xét chỉ số token, hiện tượng lỗi lượng tử hóa tích lũy về phía các token sau trở nên càng rõ ràng hơn. Hiện tượng tích lũy lỗi lượng tử hóa token chưa được xem xét trước đây trong mô hình bộ giải mã này là một đặc điểm quan trọng cần xem xét trong QAT GLM. Phản ánh về đặc điểm này, chúng tôi phân tích hiệu quả của các phương pháp KD trước đây cho mô hình hóa ngôn ngữ và khám phá các cách tiếp cận KD phù hợp cho mô hình bộ giải mã. Phân tích về lỗi lượng tử hóa tích lũy cho nhiều loại GLM rộng rãi hơn có thể được tìm thấy trong Phụ lục A.3.

So sánh các Phương pháp KD cho QAT Bộ giải mã. Dựa trên sự hiểu biết sâu hơn về mô hình bộ giải mã, chúng tôi đánh giá hiệu quả của các phương pháp KD hiện tại cho QAT trong bộ giải mã và đề xuất một cách tiếp cận KD nâng cao được thông báo bởi phân tích mô hình bộ giải mã của chúng tôi. Chúng tôi phân tích cách hai phương pháp KD khác nhau, chưng cất Từng lớp (L2L KD) và chưng cất logit (Logit KD), giải quyết các giá trị ngoại lai có hệ thống trong QAT [15], sử dụng phạm vi động min-max trên mỗi token và mỗi kênh của đầu ra trung gian mỗi lớp. Như được hiển thị trong Hình 2(c) bên trái, cả hai phương pháp KD đều thể hiện các chiến lược khác biệt trong việc giải quyết các giá trị ngoại lai có hệ thống của mô hình giáo viên. Trong khi chưng cất L2L hướng dẫn quá trình QAT để phản ánh các giá trị ngoại lai của mô hình giáo viên, Logit KD lệch khỏi mẫu này, tạo ra các giá trị ngoại lai mới không thấy trong mô hình giáo viên. Những giá trị ngoại lai này liên tục xuất hiện trong các chỉ số kênh cụ thể nơi có các giá trị ngoại lai của mô hình giáo viên. Ngoài ra, để so sánh thứ tự chú ý token tương đối trong bản đồ tự chú ý của mỗi mô hình QAT, chúng tôi sử dụng phương pháp so sánh tỷ lệ xếp hạng [22]. Kỹ thuật này truyền đạt tầm quan trọng tương đối trung bình của một token duy nhất trong mỗi bản đồ chú ý. Như được mô tả trong Hình 2(c) giữa, phương pháp L2L KD phản ánh chặt chẽ các thay đổi xếp hạng của mô hình giáo viên. Tuy nhiên, phương pháp Logit KD thể hiện sự biến đổi đáng kể trong sự thay đổi xếp hạng này trong một phạm vi đầu nhất định.

--- TRANG 6 ---
(b) Vùng Tin cậy Thấp Vùng Tin cậy Cao
Tin cậy Từng Token Mất mát CE Từng Token Entropy Từng Token Mất mát CE Từng Token
Mất mát Huấn luyện Mất mát Đánh giá Số Epoch
(c)(a)
00.020102030400.040.06 Tỷ lệ Từng Token
Chỉ số Token 01020304050
151050010203040 Chỉ số Token Mất mát CE Từng Token

Hình 3: (a) Đường cong mất mát huấn luyện/đánh giá với các phương pháp QAT-KD khác nhau. (b) biểu đồ phân tán thống kê dự đoán theo token (trái: mất mát entropy chéo và tin cậy token, phải: mất mát entropy chéo và entropy token). (c) Tác động của TSLD: tỷ lệ từng token và mất mát entropy chéo (trái: tỷ lệ từng token, Phải: mất mát entropy chéo từng token). Phân tích sử dụng OPT-125m cho nhiệm vụ mô hình hóa ngôn ngữ PTB. Xem Phụ lục A.2 để phân tích thêm về các GLM khác

Chưng cất Logit cho Khôi phục Dự đoán Theo Token. Chúng tôi phân tích thêm các phân phối logit token của mô hình QAT. Vì các biểu diễn token phát triển dọc theo các lớp để tạo thành xác suất token tiếp theo [33], chúng tôi đánh giá phân phối logit của mỗi lớp và khoảng cách logit từ mô hình giáo viên. Như được mô tả trong Hình 2(c), L2L KD tạo ra một biểu diễn token phản ánh chặt chẽ mô hình giáo viên trong cả phân phối logit và khoảng cách lỗi bình phương trung bình (MSE) trong các giai đoạn giữa lớp nhưng không thể khớp với phân phối logit cuối cùng. Ngược lại, Logit KD, mặc dù lệch khỏi phân phối logit của mô hình giáo viên trong các lớp giữa, nhưng tái tạo chính xác phân phối logit cuối cùng. Những quan sát này làm nổi bật cơ chế khác biệt của Logit KD cho khôi phục dự đoán theo token, quản lý lỗi lượng tử hóa tích lũy trong các mô hình bộ giải mã. Trong các lớp trung gian, Logit KD thay đổi các giá trị chú ý qua các kênh như được hiển thị trong Hình 2(c), dẫn đến biểu diễn token khác biệt so với mô hình FP, với điều chỉnh giai đoạn giữa này hoạt động để chống lại lỗi lượng tử hóa tích lũy trong các token sau. Do đó, Logit KD điều chỉnh phân phối logit cuối cùng cho mỗi token, quan trọng cho độ chính xác của mô hình hóa ngôn ngữ nhân quả. Vì vậy, Logit KD, phù hợp với các đặc điểm của mô hình bộ giải mã, nổi bật như một lựa chọn tự nhiên cho QAT. Phần tiếp theo sẽ đi sâu vào các vấn đề chưa được xem xét trước đây mà Logit KD gặp phải trong QAT bộ giải mã.

4.2 Chưng cất Logit Có Tỷ lệ Token để Tránh Quá khớp với GT Loss

Động lực. Phần này giải quyết vấn đề quá khớp phát sinh từ sự kết hợp của Logit KD và GT Loss trong QAT. Chúng tôi cũng điều tra hành vi xác suất được thể hiện bởi mô hình bộ giải mã trong các nhiệm vụ mô hình hóa ngôn ngữ. Nghiên cứu của [24] làm nổi bật các trường hợp mà việc sử dụng GT Loss và Logit KD ảnh hưởng bất lợi đến hiệu suất của QAT bộ giải mã. Để hiểu vấn đề này tốt hơn, chúng tôi tiến hành thử nghiệm sử dụng Logit KD cả độc lập và kết hợp với mất mát sự thật cơ bản. Như được mô tả trong Hình 3 (a), quá khớp được quan sát trong QAT khi cả mất mát sự thật cơ bản và Logit KD được áp dụng.

Hiểu Nguyên nhân của Quá khớp. Để hiểu rõ hơn nguyên nhân của quá khớp, chúng tôi phân tích đầu ra logit cho mỗi token mà mô hình giáo viên tạo ra trong mô hình hóa ngôn ngữ. Từ thông tin logit của mô hình giáo viên, chúng tôi tạo ra phân phối xác suất (PT_i=softmax(ZT_i)) cho dự đoán token thứ i. Dựa trên phân phối này, chúng tôi tính toán thêm entropy chéo (−yn,i log(PT_n,i)), điểm tin cậy (max(PT_i)) và entropy (−∑_i PT_i log(PT_i)) cho mỗi dự đoán token. Những chỉ số này tiết lộ sự khác biệt tin cậy trong mô hình hóa ngôn ngữ—một xu hướng được quan sát đồng nhất qua các mô hình bộ giải mã với quy mô khác nhau. Mất mát entropy chéo của logit dự đoán token được vẽ đối với điểm tin cậy xác suất, như được minh họa trong Hình 3 (b), phân định rõ ràng Vùng Tin cậy Thấp (hộp màu xanh) với tin cậy xác suất thấp và mất mát entropy chéo cao khỏi Vùng Tin cậy Cao (hộp màu vàng) với tin cậy xác suất cao và mất mát entropy chéo thấp. Quan sát này ngụ ý sự chồng chéo tiềm năng giữa Logit KD tin cậy cao và vai trò của GT Loss entropy chéo, gợi ý rằng thông tin dư thừa từ Logit KD tin cậy cao có thể phản ánh tác động của mất mát sự thật cơ bản, do đó góp phần vào quá khớp được quan sát.

Chưng cất Logit Có Tỷ lệ Token (TSLD). Dựa trên các điều tra về mối quan hệ xác suất của dự đoán token và quá khớp trong QAT, chúng tôi đề xuất một phương pháp KD thích ứng điều chỉnh Logit KD dựa trên tin cậy token. Cách tiếp cận này sử dụng hiện tượng khác biệt tin cậy trong dự đoán token từ mô hình giáo viên. Phương pháp của chúng tôi, được gọi là Chưng cất Logit Có Tỷ lệ Token (TSLD), giảm nhấn mạnh Logit KD cho các token tin cậy cao để ngăn chặn quá khớp trong QAT trong khi nhấn mạnh Logit KD cho các token tin cậy thấp có phân phối xác suất entropy cao. Cụ thể, Logit KD tỷ lệ thấp (tin cậy cao, entropy thấp) hiệu quả giảm sự chồng chéo với GT Loss, dẫn đến cải thiện trong quá khớp. Mặt khác, logit KD tỷ lệ cao (tin cậy thấp, entropy cao) nhấn mạnh việc chưng cất phân phối dự đoán token có nhiều thông tin hơn từ mô hình giáo viên, có thông tin nhãn mềm phong phú.

CET_n=−∑(i=1 to V) yn,i log(PT_n,i), scale_n = e^(CET_n/τ) / ∑(k=1 to N) e^(CET_k/τ) (6)

LTSLD = ∑(n=1 to N) [scale_n × (−∑(i=1 to V) PT_n,i log(PS_n,i))] (7)

Việc triển khai TSLD rất đơn giản. Bằng cách xem xét mối quan hệ giữa tin cậy token và mất mát entropy chéo dự đoán token trong Hình 3(b), chúng ta có thể xác định các giá trị tỷ lệ token thứ n (scale_n) dựa trên mất mát entropy chéo của mô hình giáo viên (CET_n) sử dụng hàm softmax như được hiển thị trong Eq. 6. Lưu ý rằng yn,i là nhãn sự thật cơ bản trong đó yn,i = 1 nếu token i là token tiếp theo đúng tại vị trí n và τ là tham số nhiệt độ cho hàm softmax. Tỷ lệ của mỗi token (scale_n) sau đó được áp dụng trong chưng cất logit bằng cách nhân nó với mất mát entropy chéo giữa mô hình học sinh và giáo viên như được hiển thị trong Eq. 7. Như được mô tả trong Hình 3(c) bên trái, các giá trị tỷ lệ cho Logit KD cụ thể token được xác định một cách thích ứng dựa trên mất mát entropy chéo từng token của mô hình giáo viên. Trong biểu đồ bên phải của Hình 3(c), chúng ta có thể quan sát rằng các token có mất mát entropy chéo cao hơn trong mô hình giáo viên tương ứng với các giá trị tỷ lệ cao hơn trong biểu đồ tỷ lệ từng token, so với phương pháp Logit KD áp dụng cùng một giá trị tỷ lệ (1/N) qua tất cả các token như được hiển thị trong biểu đồ bên trái của Hình 3(c).

TSLD mang lại hai tác động đáng kể bằng cách áp dụng các tỷ lệ khác nhau dựa trên sự khác biệt tin cậy, với chi phí tính toán không đáng kể. Như được hiển thị trong Hình 3(a), TSLD giảm nhấn mạnh Logit KD cho các token tin cậy cao, do đó ngăn chặn quá khớp. Ngược lại, đối với các token tin cậy thấp có phân phối xác suất entropy cao, TSLD nhấn mạnh Logit KD. Hành động này cho phép mô hình học sinh mô phỏng mất mát entropy chéo của mô hình giáo viên chặt chẽ hơn như thấy trong biểu đồ bên phải của Hình 3(c). Phân tích chi tiết về chi phí tính toán của phương pháp TSLD có thể được tìm thấy trong Phụ lục A.1.

5 Thí nghiệm

5.1 Cài đặt Thí nghiệm

Trong phần này, chúng tôi đánh giá hiệu quả của TSLD trong QAT của các mô hình bộ giải mã với kích thước khác nhau với lượng tử hóa dưới 4-bit. Chúng tôi đã thiết lập các thí nghiệm so sánh để chứng minh thành thạo của phương pháp TSLD chúng tôi so với các phương pháp PTQ hiện có và các phương pháp QAT KD khác. Những phát hiện của chúng tôi minh họa rằng TSLD nâng cao đáng kể cả hiệu suất mô hình hóa ngôn ngữ (được đo bằng Perplexity hoặc PPL) và độ chính xác trong các nhiệm vụ liên quan đến lý luận (QA thường thức và số học) và hiểu ngôn ngữ tự nhiên.

Nhiệm vụ và Mô hình. Chúng tôi đánh giá phương pháp được đề xuất cho mô hình hóa ngôn ngữ (PTB [34]), các nhiệm vụ QA thường thức (PIQA [35], OpenbookQA [36], ARC_easy [37], ARC_challenge [37] và nhiệm vụ sinh văn bản dựa trên lý luận số học (GSM8K [38]). Ngoài ra, đánh giá của chúng tôi mở rộng đến nhiệm vụ Hiểu Ngôn ngữ Tự nhiên (NLU) (GLUE [39]), đảm bảo phân tích toàn diện. Các mô hình chuẩn của chúng tôi bao gồm các GLM được sử dụng rộng rãi, như GPT-2 [2], OPT [4], GPT-Neo [40] và LLaMA [5] [41] với nhiều kích thước khác nhau từ 0.1B đến 7B tham số.

Cài đặt Điều chỉnh tinh. Trong điều chỉnh tinh nhiệm vụ mô hình hóa ngôn ngữ, chúng tôi sử dụng phương pháp tiền xử lý dựa trên khối: tất cả các tập dữ liệu huấn luyện được nối lại, sau đó chia thành các khối ngắn hơn được xác định bởi độ dài chuỗi đầu vào. Để điều chỉnh tinh nhiệm vụ lý luận, chúng tôi sử dụng cách tiếp cận dựa trên câu, nối các phần câu hỏi và câu trả lời của mỗi tập dữ liệu để tạo thành các câu mới, được sử dụng riêng biệt làm tập dữ liệu điều chỉnh tinh. Cài đặt siêu tham số chi tiết và các chi tiết khác có trong Phụ lục C.2. Các thí nghiệm được tiến hành trên GPU A100-40GB. Các thí nghiệm QAT của chúng tôi bắt đầu với các mô hình đã trải qua điều chỉnh tinh cụ thể cho nhiệm vụ. Trong quá trình lượng tử hóa, quá trình KD sử dụng mô hình FP được điều chỉnh tinh làm mô hình giáo viên, trong khi mô hình được lượng tử hóa hoạt động như học sinh.

Cài đặt Triển khai. Chúng tôi thiết kế một khung QAT-KD tận dụng song song pipeline với PyTorch Pipe API cho phép huấn luyện các mô hình có công suất vượt quá 1.3 tỷ. Chúng tôi áp dụng lượng tử hóa trọng số cho các lớp nhân ma trận trong mỗi lớp bộ giải mã của GLM. Chúng tôi tiến hành thí nghiệm trên L2L KD [25] nhưng gặp phải vấn đề hết bộ nhớ cho các mô hình có hơn 1.3B tham số trong GPU A100-40GB. Vấn đề này được cho là phát sinh do yêu cầu cho cả mô hình giáo viên và học sinh lưu trữ các đầu ra được tạo bởi tất cả các lớp trung gian tương ứng của chúng trong quá trình chưng cất kiến thức. So sánh tiêu thụ bộ nhớ GPU cho mỗi phương pháp QAT-KD có thể được tìm thấy trong Phụ lục A1.

5.2 Đánh giá trên Nhiệm vụ Mô hình hóa Ngôn ngữ

Bảng 1 nêu bật so sánh hiệu suất của TSLD với các phương pháp PTQ và QAT hàng đầu [14, 24] cho mô hình hóa ngôn ngữ của tập dữ liệu PTB. Đối với lượng tử hóa trọng số 4-bit, OPTQ thấy sự sụt giảm hiệu suất đáng chú ý trong các mô hình GPT-2 và OPT lên đến 6.7 tỷ tham số, phù hợp với các quan sát của bài báo gốc [14]. Tuy nhiên, các phương pháp QAT cho thấy perplexity thấp hơn do các tham số trọng số được điều chỉnh tinh cho suy luận độ chính xác giảm bền vững. QuantGPT [24], chỉ sử dụng Logit KD đạt được perplexity ấn tượng, trong khi Logit+GT KD thấy sự suy giảm. Ngược lại, TSLD cung cấp perplexity thấp nhất, nhấn mạnh hiệu quả của phương pháp tỷ lệ theo token trong việc tích hợp kiến thức GT. Đáng chú ý, sự tăng cường hiệu suất của TSLD cho phép các mô hình QAT khớp với hiệu suất độ chính xác đầy đủ qua các phạm vi công suất khác nhau trong tất cả các mô hình bộ giải mã.

Đối với lượng tử hóa trọng số 2-bit, L2L KD thấy sự suy giảm độ chính xác đáng kể, và Logit+GT KD chịu quá khớp. TSLD vượt trội hơn Logit KD [24] qua tất cả các kích thước mô hình, duy trì sự suy giảm PPL không quá 1.0 từ đường cơ sở. Chúng tôi cũng thử nghiệm khả năng áp dụng chung của TLSD trên các mô hình GLM mã nguồn mở phổ biến (GPT-Neo-1.3B [40], LLaMA-7B [5]) trong nhiệm vụ mô hình hóa ngôn ngữ PTB. Bảng 2-dưới chỉ ra rằng TSLD liên tục vượt trội so với đối thủ cạnh tranh, Logit KD [24]. Đáng chú ý, TSLD 2-bit sử dụng lượng tử hóa trọng số tam phân đơn giản, thân thiện với phần cứng.

5.3 Đánh giá trên Nhiệm vụ Lý luận

Chúng tôi đánh giá hiệu quả của phương pháp được đề xuất trong QA thường thức (PIQA, OpenbookQA, ARC_easy, ARC_challenge) và nhiệm vụ sinh văn bản dựa trên lý luận (GSM8K) sử dụng khung LM Evaluation Harness từ EleutherAI [42]. Với yêu cầu công suất cho các nhiệm vụ lý luận, chúng tôi sử dụng các mô hình OPT-2.7B/6.7B và LLaMA-7B làm đường cơ sở, thay vì các mô hình nhỏ hơn.

Bảng 2 trình bày so sánh hiệu suất của lượng tử hóa trọng số 2-bit với các phương pháp KD khác nhau. Trong các nhiệm vụ QA thường thức, TSLD liên tục cho thấy perplexity thấp nhất và do đó, độ chính xác cao nhất, đưa kết quả lượng tử hóa 2-bit thậm chí gần hơn với hiệu suất FP như được hiển thị trong Bảng 2-trên.

Xem xét nhiệm vụ GSM8K, Bảng 2 tiết lộ rằng TSLD vượt trội hơn Logit KD về perplexity và độ chính xác với các mô hình OPT-2.7B/6.7B và LLaMA. Đáng chú ý, trong khi QuantGPT (Logit KD) đạt được perplexity tương đương hoặc tốt hơn, độ chính xác nhiệm vụ lý luận của nó thấp hơn, có khả năng do thông tin GT không đủ. Ngược lại, TSLD đạt được độ chính xác lý luận xuất sắc trong khi duy trì perplexity cạnh tranh, nhấn mạnh khả năng của TSLD trong việc cân bằng hiệu suất mô hình hóa ngôn ngữ và lý luận thông qua tỷ lệ theo token để tránh quá khớp. Kết quả mẫu văn bản được tạo của nhiệm vụ GSM8K được cung cấp trong Phụ lục D.

5.4 Đánh giá trên Nhiệm vụ Hiểu Ngôn ngữ

Chúng tôi điều chỉnh tinh mô hình bộ giải mã cho các nhiệm vụ Hiểu Ngôn ngữ Tự nhiên (NLU) sử dụng cách tiếp cận mô hình hóa ngôn ngữ như được minh họa trong Hình 1(b). Trong các thí nghiệm của chúng tôi được nêu trong Bảng 3, chúng tôi so sánh hiệu suất của các phương pháp PTQ mới nhất (AWQ [13], OPTQ [14]) và các phương pháp QAT-KD với mô hình OPT-1.3B. Đối với lượng tử hóa 4-bit, kỹ thuật PTQ cho thấy sự suy giảm hiệu suất đáng chú ý so với kết quả QAT, loại trừ nhiệm vụ SST-2. TSLD đạt được perplexity thấp nhất và độ chính xác cao nhất qua tất cả các thí nghiệm ngoại trừ SST-2, nơi độ chính xác của nó ngang bằng với trường hợp độ chính xác đầy đủ. Những phát hiện này chứng minh rằng TSLD có thể củng cố hiệu suất của GLM được lượng tử hóa 4-bit cho các nhiệm vụ NLU khác nhau, trong khi PTQ 4-bit có thể chịu sự suy giảm hiệu suất.

Trong lượng tử hóa tam phân, TSLD liên tục vượt trội hơn các phương pháp QAT-KD thay thế cho tất cả các trường hợp, chứng minh hiệu suất vượt trội trong việc thu hẹp khoảng cách độ chính xác với các trường hợp độ chính xác đầy đủ. Thú vị là TSLD tam phân thậm chí đạt được độ chính xác tương tự hoặc vượt trội so với PTQ 4-bit trong nhiều nhiệm vụ (ví dụ, CoLA, MRPC, SST-2), làm nổi bật lợi ích của nó về cả độ chính xác và tiết kiệm bộ nhớ.

--- TRANG 10 ---
Cấu hình mô hình 6.7B 13B 175B
Kênh đầu vào 4096 4096 16384 5120 5120 20480 12288 12288 49152
Kênh đầu ra 4096 16384 4096 5120 20480 5120 12288 49152 12288
Cơ sở FP32 0.067 0.201 0.194 0.100 0.285 0.287 0.391 1.472 1.522
8-bit 0.039 0.068 0.068 0.043 0.095 0.092 0.121 0.407 0.395
(×tăng tốc) ×1.71 ×2.93 ×2.83 ×2.32 ×3.01 ×3.13 ×3.23 ×3.61 ×3.85
4-bit 0.030 0.057 0.057 0.041 0.076 0.075 0.096 0.326 0.318
(×tăng tốc) ×2.23 ×3.52 ×3.40 ×2.45 ×3.75 ×3.82 ×4.07 ×4.51 ×4.78
2-bit 0.025 0.053 0.053 0.039 0.072 0.064 0.077 0.221 0.232
(×tăng tốc) ×2.68 ×3.77 ×3.65 ×2.57 ×3.95 ×4.48 ×5.07 ×6.66 ×6.56
Bảng 4: Thời gian thực thi kernel (msec)

5.5 Nghiên cứu Triệt để

Thời gian Thực thi Kernel Độ chính xác Giảm. Chúng tôi phát triển các kernel CUDA tùy chỉnh để tăng cường tốc độ suy luận với lượng tử hóa được áp dụng (2-, 4-, 8-bit). Giống như OPTQ, chúng tôi đóng gói các trọng số để giảm thiểu kích thước mô hình và chi phí tải. Kernel của chúng tôi loại bỏ nhu cầu giải nén trọng số trong quá trình chuyển tiếp mô hình, dẫn đến tăng tốc được hiển thị trong Bảng 4. Chúng tôi thử nghiệm kernel chủ yếu trên các mô hình lớn hơn 6.7B, nơi chi phí tải trọng số đáng chú ý cao. Thời gian được báo cáo là thời gian thực thi trung bình cho 10.000 lần chạy kernel trên một GPU A100-80GB duy nhất. Đối với cơ sở FP32, chúng tôi sử dụng nn.Linear của PyTorch. Như được hiển thị trong Bảng 4, kernel 2-bit của chúng tôi cho mô hình 175B có thể tăng tốc một phép toán nhân ma trận đơn lẻ trung bình khoảng 6.1 lần so với FP32.

Độ chính xác Lượng tử hóa Granularity GPT-2 OPT GPT-Neo LLaMA
0.1B 0.3B 0.8B 0.1B 1.3B 1.3B 7B
Cơ sở FP16 20.91 18.21 15.20 18.17 13.75 17.62 8.76
W2A16 Theo tensor 21.74 18.57 16.14 18.58 14.60 30.60 12.31
Theo kênh 21.30 18.48 15.97 18.42 14.46 19.27 11.60
Bảng 5: So sánh lượng tử hóa theo tensor và theo kênh qua các GLM khác nhau (GPT-2, OPT, GPT-Neo và LLaMA). Phương pháp KD TSLD được sử dụng trong thí nghiệm này

Tác động Granularity Lượng tử hóa. Để tính đến các biến thể trọng số kênh đầu ra, lượng tử hóa theo kênh được sử dụng [15,14]. Bằng cách tích hợp cách tiếp cận QAT-KD của chúng tôi với lượng tử hóa theo kênh, chúng ta có thể đạt được nâng cao hiệu suất thêm. Một quan sát thú vị xuất hiện: các lợi ích từ lượng tử hóa theo kênh khác nhau theo loại GLM. Như được minh họa trong Bảng 5, đối với chuỗi GPT-2 và OPT, sự gia tăng hiệu suất PPL do lượng tử hóa theo kênh ít hơn 1. Tuy nhiên, đối với GPT-Neo và LLaMA, hiệu ứng nâng cao hiệu suất do lượng tử hóa theo kênh rất rõ rệt. Sự biến đổi trong lợi ích hiệu suất này gợi ý các phân phối trọng số theo kênh khác biệt qua các mô hình GLM khác nhau. Phân tích chi tiết về phân phối trọng số cho mỗi GLM được đề cập trong Phụ lục A.4.

6 Kết luận

Chúng tôi giới thiệu chưng cất logit có tỷ lệ token, một cách tiếp cận mới cho Huấn luyện Nhận biết Lượng tử hóa của Mô hình Ngôn ngữ Sinh. Phương pháp này hiệu quả giảm quá khớp và nâng cao học tập từ mô hình giáo viên và sự thật cơ bản. Quan trọng là, nghiên cứu này là đầu tiên đánh giá huấn luyện nhận biết lượng tử hóa tam phân trên GLM quy mô lớn, đạt được sự suy giảm perplexity ít hơn 1.0 và bảo tồn độ chính xác nhiệm vụ QA thường thức và lý luận số học.

--- TRANG 11 ---
Lời cảm ơn và Tiết lộ về Nguồn tài trợ

Công trình này được hỗ trợ bởi các khoản tài trợ từ Viện Lập kế hoạch và Đánh giá Công nghệ Thông tin & Truyền thông (IITP) được tài trợ bởi chính phủ Hàn Quốc (MSIT) (2020-0-01373, Chương trình Trường Đại học Trí tuệ Nhân tạo Đại học Hanyang, 2023-RS-2023-00253914, chương trình hỗ trợ bán dẫn trí tuệ nhân tạo để nuôi dưỡng tài năng tốt nhất), tài trợ từ Quỹ Nghiên cứu Quốc gia Hàn Quốc (NRF), được tài trợ bởi chính phủ Hàn Quốc (MSIT) (Số 2021R1A2C1013513, Số RS-2023-00260527), và Dự án Phát triển Cụm Hội tụ Công nghiệp Trí tuệ Nhân tạo, được tài trợ bởi Bộ Khoa học và ICT (MSIT, Hàn Quốc) và Thành phố Gwangju.

Tài liệu tham khảo

[1] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training, 2018.

[2] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019.

[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

[4] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.

[5] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

[7] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

[8] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=gEZrGCozdqR.

[9] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022.

[10] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.

[11] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582–4597, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353. URL https://aclanthology.org/2021.acl-long.353.

[12] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.

[13] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv, 2023.

[14] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: Accurate quantization for generative pre-trained transformers. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=tcbBPnfwxS.

[15] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix multiplication for transformers at scale. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 30318–30332. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/c3ba4962c05c49636d4c6206a97e9c8a-Paper-Conference.pdf.

[16] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling, 2023.

[17] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers. arXiv, 2022.

[18] Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural networks, 2018.

[19] Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu. Ternarybert: Distillation-aware ultra-low bit bert. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 509–521, 2020.

[20] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin King. Binarybert: Pushing the limit of bert quantization. arXiv preprint arXiv:2012.15701, 2020.

[21] Jing Jin, Cai Liang, Tiancheng Wu, Liqin Zou, and Zhiliang Gan. Kdlsq-bert: A quantized bert combining knowledge distillation with learned step size quantization, 2021.

[22] Minsoo Kim, Sihwa Lee, Suk-Jin Hong, Du-Seong Chang, and Jungwook Choi. Understanding and improving knowledge distillation for quantization aware training of large transformer encoders. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6713–6725, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.450.

[23] Minsoo Kim, Kyuhong Shim, Seongmin Park, Wonyong Sung, and Jungwook Choi. Teacher intervention: Improving convergence of quantization aware training for ultra-low precision transformers. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 916–929, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.eacl-main.64.

[24] Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, and Ngai Wong. Compression of generative pre-trained language models via quantization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4821–4836, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.331. URL https://aclanthology.org/2022.acl-long.331.

[25] Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, and Yuxiong He. Understanding int4 quantization for language models: Latency speedup, composability, and failure cases. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 37524–37539. PMLR, 23–29 Jul 2023. URL https://proceedings.mlr.press/v202/wu23k.html.

[26] Ronald J. Williams and David Zipser. A learning algorithm for continually running fully recurrent neural networks. Neural Computation, 1(2):270–280, 1989. doi: 10.1162/neco.1989.1.2.270.

[27] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.

[28] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.

[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, 2017.

[30] Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. arXiv preprint arXiv:1612.01064, 2016.

[31] Xiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, and Yuxiong He. Extreme compression for pre-trained transformers made simple and efficient. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=xNeAhc2CNAl.

[32] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471, 2018.

[33] Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 30–45, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.3.

[34] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330, 1993. URL https://aclanthology.org/J93-2004.

[35] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language, 2019.

[36] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP, 2018.

[37] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018.

[38] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021.

[39] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, 2018.

[40] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https://doi.org/10.5281/zenodo.5297715.

[41] Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cantón Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288, 2023. URL https://arxiv.org/abs/2307.09288.

[42] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021. URL https://doi.org/10.5281/zenodo.5371628.

[43] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. GLM-130b: An open bilingual pre-trained model. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=-Aw0rrrPUF.

[44] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2020.

[45] Abien Fred Agarap. Deep learning using rectified linear units (relu). arXiv preprint arXiv:1803.08375, 2018.

[46] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2023.

[47] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.

[48] Noam Shazeer. Glu variants improve transformer, 2020.

[49] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.

[50] Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, and Ngai Wong. Compression of generative pre-trained language models via quantization, 2022.

--- TRANG 15 ---
A Phân tích Bổ sung

A.1 Yêu cầu Tính toán của TSLD

Phương pháp Huấn luyện QAT-KD GPT2-0.3B 512 GPT2-0.8B 512 OPT-1.3B 256 OPT-1.3B 1024
Tốc độ (iter/sec) Bộ nhớ (MiB) Tốc độ (iter/sec) Bộ nhớ (MiB) Tốc độ (iter/sec) Bộ nhớ (MiB) Tốc độ (iter/sec) Bộ nhớ (MiB)
QAT GT 2.03 19663 1.03 36317 5.15 10051 2.83 15167
QAT-KD Logit 1.57 22622 0.81 40989 4.44 11589 2.27 17529
GT+Logit 1.56 22622 0.81 40989 4.44 11589 2.27 17529
L2L+Logit 1.51 31462 OOM OOM 4.28 12143 2.12 25315
TSLD 1.57 22622 0.81 40989 4.43 11589 2.26 17529

Bảng A1: Nghiên cứu tiêu thụ bộ nhớ và tốc độ huấn luyện QAT cho phương pháp KD. Kết quả được báo cáo trên tập dữ liệu PTB trên QAT-KD Tam phân của các mô hình chuỗi GPT-2 và OPT với độ dài chuỗi đầu vào từ 256 đến 1024

Phương pháp TSLD tích hợp mất mát entropy chéo theo token với Logit KD, bao gồm hai thao tác như được chi tiết trong Eq.7. Cụ thể, thuật ngữ ∑(i=1 to V) yn,i log(PT_n,i) tính mất mát entropy chéo từ logit giáo viên (ZT_n). Kết quả này, được xử lý thông qua hàm softmax, tạo ra giá trị tỷ lệ cho mỗi token. Nhân theo phần tử với thuật ngữ Logit KD, ∑(i=1 to V) PT_n,i log(PS_n,i), nó mang lại Logit KD có tỷ lệ theo token. Thực tế, TSLD tận dụng logit giáo viên được tính toán trước trong Logit KD, tránh việc sử dụng bộ nhớ thêm. Hơn nữa, các tính toán liên quan có độ phức tạp O(N), làm cho chi phí của TSLD không đáng kể cho huấn luyện.

Để đánh giá hiệu quả của TSLD, chúng tôi chi tiết tốc độ huấn luyện và tiêu thụ bộ nhớ GPU cho các phương pháp QAT-KD khác nhau sử dụng mô hình GPT-2 trong Bảng A1 bên trái. So với các phương pháp dựa trên Logit, TSLD duy trì tốc độ mà không tiêu thụ bộ nhớ thêm. Ngược lại, L2L KD lưu trữ các kích hoạt trung gian từ cả mô hình giáo viên và học sinh cho KD, dẫn đến yêu cầu bộ nhớ tăng đáng kể, rõ ràng từ Bảng A1 bên trái. Khi kích thước mô hình tăng, như được chứng minh trong các tình huống sử dụng GPT2-Large, yêu cầu bộ nhớ tăng, dẫn đến lỗi "Hết bộ nhớ" trên GPU A100-40GB. Những phát hiện này làm nổi bật hiệu quả của TSLD, nâng cao hiệu suất QAT-KD với bộ nhớ tương đương với Logit KD, trong khi L2L KD đòi hỏi nhiều hơn đáng kể. Ngay cả khi độ dài chuỗi được mở rộng từ 256 đến 1024, như Bảng A1 bên phải cho thấy, TSLD duy trì cùng tiêu thụ bộ nhớ GPU và tốc độ huấn luyện như phương pháp Logit KD.

A.2 Phân tích Khác biệt Tin cậy Token

Phân tích của chúng tôi về sự khác biệt tin cậy trong dự đoán token, được chi tiết trong Phần 4.2, mở rộng ngoài một mô hình GLM cụ thể. Thực tế, xu hướng được quan sát này hiện diện nhất quán qua các mô hình GLM khác nhau. Như được hiển thị trong Hình A1, chúng ta có thể quan sát rõ ràng sự xuất hiện của Vùng Tin cậy Thấp (hộp màu xanh) và Vùng Tin cậy Cao (hộp màu vàng) nhất quán qua các mô hình: OPT-6.7B (trái), LLaMA-7B (giữa), và LLaMA-2-7B (phải). Ngoài ra, như được hiển thị trong Hình A1 phải, chúng tôi vẽ thống kê dự đoán token với độ dài chuỗi đầu vào khác nhau là 128 và 512. Bất kể độ dài chuỗi, sự phân định của khác biệt tin cậy vẫn nhất quán. Quan sát này chứng minh rằng phương pháp luận TSLD, dựa trên động lực xác suất của dự đoán token, có thể được áp dụng toàn cầu qua các GLM khác nhau.

A.3 Phân tích Lỗi Lượng tử hóa Tích lũy với LLM

Trong phần này, chúng tôi nhằm mở rộng phân tích của chúng tôi về lỗi lượng tử hóa tích lũy được thảo luận trong Phần 4.1 đến GLM lớn hơn 6B tham số. Bằng cách triển khai lượng tử hóa tam phân 2-bit [30] trên các mô hình OPT-6.7B và LLaMA-7B, chúng tôi đánh giá lỗi lượng tử hóa bản đồ chú ý so với mô hình FP thông qua mất mát MSE. Những lỗi này được hiển thị bằng biểu đồ bản đồ nhiệt (Hình A2 trên), và mất mát bản đồ chú ý trung bình từng token được vẽ đối với mỗi lớp (Hình A2 dưới). Đối với mô hình OPT-6.7B, lỗi lượng tử hóa được đo cho lớp thứ 5 và 15. Về mô hình LLaMA-7B, lỗi lượng tử hóa được mô tả cho độ dài chuỗi đầu vào 128 và 512.

Đối với mô hình OPT-6.7B tại lớp thứ 5 và mô hình LLaMA-7B với độ dài chuỗi 128, chúng tôi lưu ý sự tích lũy lỗi lượng tử hóa về phía các token sau, như được thảo luận trong Phần 4.1.

--- TRANG 16 ---
Vùng Tin cậy Thấp Vùng Tin cậy Cao
Entropy Từng Token
Entropy Từng Token Mất mát CE Từng Token
Entropy Từng Token OPT-6.7B LLaMA-7B LLaMA-2-7B
Tin cậy Từng Token
Mất mát CE Từng Token Tin cậy Từng Token
Tin cậy Từng Token
Entropy Từng Token

Hình A1: Biểu đồ phân tán đại diện cho mối quan hệ xác suất của dự đoán token. Các biểu đồ trên cho thấy mất mát CE so với tin cậy cho mỗi dự đoán token, trong khi các biểu đồ dưới vẽ mất mát CE với entropy. Từ trái sang phải: OPT-6.7B, LLaMA-7B, và LLaMA-2-7B. Đối với LLaMA-2, kết quả cho hai độ dài chuỗi đầu vào (128, 512) được vẽ. Tập dữ liệu đầu vào là wikitext-2

OPT-6.7B Lớp thứ 5 Chỉ số Token
Mất mát Chú ý Token Chỉ số Token
LLaMA-7B (Lớp thứ 5)
Lớp thứ 15 Độ dài Chuỗi 128 Độ dài Chuỗi 512
Chỉ số Token Chỉ số Token Chỉ số Token
Mất mát Chú ý Token
Mất mát Chú ý Token
Mất mát Chú ý Token (b)(a)

Hình A2: Trên: Bản đồ nhiệt của lỗi lượng tử hóa trọng số tam phân 2-bit trên mất mát MSE bản đồ chú ý. (a) Mất mát chú ý lớp thứ 5 và 15 của OPT-6.7B. (b) Mất mát chú ý LLaMA-7B cho độ dài chuỗi 128 đến 512. Dưới: Mất mát MSE chú ý trung bình từng token qua các lớp từ mỗi bản đồ nhiệt mất mát chú ý

Tuy nhiên, khi chúng ta đi sâu hơn vào các lớp của OPT-6.7B hoặc giới thiệu các chuỗi đầu vào dài hơn cho LLaMA-7B, hiện tượng này trở nên ít rõ ràng hơn. Chúng tôi suy đoán rằng sự suy giảm này có thể phát sinh từ sự tương tác phức tạp của lỗi lượng tử hóa khi độ sâu của GLM tăng, và các mẫu chú ý đang phát triển liên quan đến độ dài chuỗi khác nhau ảnh hưởng đến việc tích lũy lỗi lượng tử hóa. Một khám phá kỹ lưỡng về lỗi lượng tử hóa tích lũy cho GLM lớn hơn sẽ được dành cho nghiên cứu tương lai.

--- TRANG 17 ---
OPT-6.7B
010002000300040000.20.10-0.1-0.2 (a)
GPT-Neo-1.3B
0500100015002000210-1-2 (c)
010002000300040001.51.00.0-0.5-1.5-1.00.5
(b) LLaMA-7B
Chỉ số Kênh Đầu ra Chỉ số Kênh Đầu ra Chỉ số Kênh Đầu ra
Phạm vi Min-Max Trọng số

Hình A3: Phạm vi Min-Max của trọng số mô-đun Linear cho ba loại GLM trên mỗi kênh đầu ra. Trong các biểu đồ, mỗi màu đại diện cho phạm vi min-max của trọng số mô-đun mỗi. (a) OPT-6.7B (b) LLaMA-7B (c) GPT-Neo-1.3B.

A.4 So sánh Phân phối Trọng số GLM

GLM được huấn luyện trước cho thấy nhiều loại phân phối trọng số [43]. Chúng tôi xem xét phạm vi Min-Max của trọng số cho mỗi mô-đun linear qua kênh đầu ra trong các mô hình GLM khác nhau (OPT, LLaMA, và GPT-Neo) như được hiển thị trong Hình A3. Phân tích này nhằm làm sáng tỏ sự khác biệt hiệu suất được quan sát trong Phần 5.5 do granularity lượng tử hóa (theo tensor và theo kênh). Đối với mô hình OPT, chúng tôi quan sát rằng mỗi mô-đun thể hiện phạm vi min-max theo kênh nhất quán, đáng chú ý là hẹp, trải dài từ -0.2 đến 0.2. Ngược lại, cả LLaMA và GPT-Neo đều thể hiện phạm vi min-max đa dạng hơn nhiều qua các kênh đầu ra cho mỗi mô-đun, với chính phạm vi rộng hơn đáng kể, khoảng từ -2 đến 2. Sự đa dạng trong phạm vi min-max cụ thể kênh đầu ra này làm rõ sự khác biệt hiệu suất giữa các cách tiếp cận theo tensor và theo kênh, như được làm nổi bật trong Bảng 5. Cụ thể, OPT, có đa dạng kênh đầu ra hạn chế, cho thấy sự khác biệt hiệu suất tối thiểu giữa các phương pháp theo tensor và theo kênh. Ngược lại, các mô hình như GPT-Neo và LLaMA, được đặc trưng bởi sự đa dạng kênh rộng rãi, thể hiện hiệu suất được nâng cao đáng kể với lượng tử hóa theo kênh. Những phát hiện này gợi ý rằng việc xác định granularity lượng tử hóa thích hợp trong QAT, với mục tiêu giảm thiểu lỗi lượng tử hóa, đòi hỏi hiểu biết toàn diện về phân phối trọng số theo kênh của GLM mục tiêu.

B Kết quả Thí nghiệm Bổ sung

B.1 Lượng tử hóa Kích hoạt 8-bit

Bảng A2 thể hiện kết quả thí nghiệm áp dụng cả lượng tử hóa trọng số tam phân và lượng tử hóa kích hoạt 8-bit (W2A8). Chúng tôi áp dụng lượng tử hóa min-max cho lượng tử hóa kích hoạt theo cách tương tự như trong [19] [24] [25] [22], tính đến phân phối bất đối xứng của một số phần kích hoạt. Cụ thể, lượng tử hóa min-max bất đối xứng được triển khai trong phép nhân của Query và Key trong tự chú ý và trong kích hoạt đầu vào của lớp linear FC2³.

Trong W2A8, phù hợp với các quan sát từ Phần 5.2, L2L KD thể hiện sự suy giảm độ chính xác đáng kể hơn Logit KD. Mặc dù Logit + GT hoạt động kém hơn Logit KD do tác động quá khớp được đề cập trước đó, phương pháp của chúng tôi vượt trội hơn những phương pháp khác qua tất cả các kích thước mô hình, do đó nhấn mạnh hiệu quả của phương pháp TSLD.

B.2 Khám phá Giá trị Cắt trong Lượng tử hóa Trọng số 4-bit

Khi áp dụng phương pháp QAT như QuantGPT, xác định giá trị cắt với hệ số tỷ lệ có thể học, điều quan trọng là khởi tạo hệ số tỷ lệ một cách thích hợp để khớp với phân phối trọng số của mô hình được lượng tử hóa. Trong trường hợp mô hình OPT, một phân phối hẹp hơn nhiều được quan sát so với GPT-2, như được minh họa trong Hình A4(a). Nếu chúng ta đặt giá trị cắt (γ=1.0) theo cùng cách như QuantGPT, chúng ta có thể quan sát rằng hơn 40% phần tử trọng số bị cắt có hại, như được hiển thị trong Hình A4(b). Để giảm thiểu hiện tượng cắt phá hoại trong lượng tử hóa 4-bit, chúng tôi tiến hành một

³ Chúng tôi sử dụng mã lượng tử hóa kích hoạt trong kho lưu trữ sau https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TernaryBERT

--- TRANG 18 ---
Độ chính xác Phương pháp Tối ưu hóa GPT OPT
0.1B 0.3B 0.6B 1.5B 0.1B 1.3B 2.7B 6.7B
Cơ sở FP32 20.91 18.21 15.20 14.26 18.17 13.75 11.43 10.21
W2A8 L2L+Logit[25] 24.88 21.61 - - 20.50 - - -
Logit [24]. 23.14 20.13 16.59 15.34 19.21 15.28 12.87 11.70
Logit+GT. 24.37 20.78 18.01 16.87 21.59 16.58 13.49 12.81
TSLD 22.01 18.83 16.26 15.23 18.92 14.95 12.14 11.43

Bảng A2: Tác động của lượng tử hóa kích hoạt (Kết quả Lượng tử hóa Trọng số Tam phân, Kích hoạt 8-bit) trong QAT-KD (theo tensor)

(a)
(b) GPT-1.5B
(c)

Hình A4: (a) Phân phối trọng số của GPT-2-1.5B/OPT-2.7B (lớp thứ 4, FC-1) (b) Chúng tôi đo tỷ lệ đại diện cho số phần tử trọng số bị cắt trong lượng tử hóa trọng số lớp FFN-2. Khi áp dụng QAT với công thức QuantGPT gốc (γ= 1.0), chúng tôi quan sát rằng hơn 40% giá trị bị cắt trong OPT-2.7B, một tỷ lệ cao hơn đáng kể so với GPT-2-1.5B. (c) Kết quả PPL khám phá khởi tạo γ trong OPT-0.1B với tập dữ liệu PTB.

thí nghiệm khám phá giá trị ban đầu của tỷ lệ γ trong QuantGPT. (tỷ lệ γ xác định giá trị cắt trong QuantGPT. Triển khai lượng tử hóa chi tiết của QuantGPT được elaborated thêm trong C.2)

Thông qua khám phá khởi tạo γ, chúng tôi có thể giảm tỷ lệ trọng số bị cắt như trong Hình A4(b) bằng cách tăng giá trị ban đầu của tỷ lệ γ, và do đó, đạt được cải thiện hiệu suất như được hiển thị trong Hình A4(c). Thông qua khám phá siêu tham số tỷ lệ γ phù hợp với phân phối trọng số OPT, chúng tôi quản lý để so sánh công bằng nhiều phương pháp luận KD trong QAT OPT 4-bit mà không có tác động bất lợi của việc cắt lượng tử hóa quá mức. Những kết quả thí nghiệm này gợi ý rằng, khi khởi tạo giá trị clip trong phương pháp QAT cắt có thể học, người ta nên xem xét các đặc điểm phân phối trọng số của GLM mục tiêu.

B.3 Kết quả QAT BERT Kiểu Bộ giải mã

Trong Phần 4.1, chúng tôi thảo luận về lỗi lượng tử hóa tích lũy do đặc điểm cấu trúc của tự chú ý có mặt nạ của GLM, và so sánh hiệu quả của Logit KD và L2L KD trong QAT. Trong thí nghiệm này, chúng tôi so sánh các phương pháp chưng cất trong mô hình Bộ mã hóa (BERT-base [27]), nơi, do không có mặt nạ, lỗi lượng tử hóa được phân bố đều giữa tất cả các token. Theo [22], L2L KD là quan trọng trong QAT KD mô hình Bộ mã hóa, và có nhiều lớp để chưng cất đã được chứng minh là có lợi cho hiệu suất QAT.

Nhiệm vụ RTE STS-B MRPC CoLA
Độ chính xác đầy đủ 73.28 89.24 87.77 58.04
Logit - Token [CLS] 55.59 86.46 82.43 38.60 (Logit KD)
Logit - Tất cả Token 70.54 87.46 87.03 48.36
Logit - Token [CLS] + L2L 72.34 88.98 87.70 51.12 (L2L KD)

Bảng A3: Hiệu suất QAT-KD (theo tensor) với nhiều tùy chọn KD trên các nhiệm vụ GLUE [39] được chọn với mô hình BERT-base [27].

Như được giải thích trong Phần 3.3, trong các nhiệm vụ Hiểu Ngôn ngữ Tự nhiên của mô hình bộ mã hóa, chúng tôi tính mất mát entropy chéo sử dụng biểu diễn của một token đặc biệt duy nhất (token lớp, [CLS]) làm logit. Rút ra từ thực tế rằng điều chỉnh tinh mô hình hóa ngôn ngữ của mô hình bộ giải mã sử dụng mất mát entropy chéo của tất cả biểu diễn token, chúng tôi cố gắng sử dụng đầu ra biểu diễn cuối cùng của mỗi token làm logit trong mô hình bộ mã hóa và đo mất mát entropy chéo với logit biểu diễn token cuối cùng của mô hình giáo viên và sử dụng mất mát này làm Logit KD (chúng tôi gọi phương pháp KD này là "Logit - Tất cả Token").

--- TRANG 19 ---
Như có thể thấy trong Bảng A3, phương pháp Logit - Tất cả Token, sử dụng tất cả biểu diễn token cuối cùng làm logit, có lợi đáng kể cho hiệu suất hơn việc sử dụng biểu diễn của một token đặc biệt làm Logit (Logit - Token [CLS]). Tuy nhiên, khi so sánh với Logit - Token [CLS] + L2L KD, chúng tôi phát hiện rằng việc sử dụng L2L KD mang lại hiệu suất vượt trội trong QAT của mô hình bộ mã hóa.

Thí nghiệm bổ sung này tiết lộ rằng trong QAT KD mô hình bộ mã hóa nơi lỗi lượng tử hóa được phân bố giữa tất cả các token, L2L KD, buộc đầu ra mỗi lớp của mô hình học sinh mô phỏng chặt chẽ của mô hình giáo viên, là phương pháp chưng cất hiệu quả nhất trong QAT. Hiểu biết này mở rộng sự hiểu biết của chúng tôi về cách điều chỉnh các phương pháp luận QAT KD để phù hợp với bản chất cấu trúc của mỗi mô hình.

C Chi tiết Thí nghiệm

C.1 Mô tả Mô hình

Cấu hình GPT OPT GPT-Neo LLaMA
0.1B 0.3B 0.6B 1.5B 0.1B 1.3B 2.7B 6.7B 1.3B 7B
# Lớp 12 24 36 48 12 24 32 32 24 32
# Kích thước Ẩn 768 1024 1280 1600 768 2048 2560 4096 2048 4096
# Đầu 12 16 20 25 12 32 32 32 16 32
Tỷ lệ Học (FP) 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4 5e-5 5e-5 1e-4 5e-5
Epoch (FP) 3 3 3 3 3 3 3 3 3 1
Tỷ lệ Học (QAT) 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4 5e-5 1e-4 7e-5
Epoch (QAT) 90 60 30 30 90 30 30 10 30 5

Bảng A4: Cấu hình của mỗi mô hình bộ giải mã được huấn luyện trước với các kích thước khác nhau và lựa chọn siêu tham số cho điều chỉnh tinh FP và QAT-KD. Tất cả thí nghiệm đặt nhất quán kích thước batch là 4, và độ dài chuỗi là 512 trong điều chỉnh tinh mô hình hóa ngôn ngữ

Trong các thí nghiệm của chúng tôi, chúng tôi tiến hành điều chỉnh tinh cụ thể nhiệm vụ cho các GLM được huấn luyện trước khác nhau (GPT-2 [2], OPT [4]), GPT-Neo [40], và LLaMA [5]) với các kích thước khác nhau (0.1B đến 7B). Mô hình GPT-2 được huấn luyện trước có kích thước từ vựng (V) là 50257 và sử dụng hàm kích hoạt GeLU [44]. Mô hình OPT được huấn luyện trước có kích thước từ vựng (V) là 50272 và sử dụng hàm kích hoạt ReLU [45]. Mặt khác, mô hình GPT-Neo được huấn luyện trước có cùng kích thước từ vựng (V) như OPT và sử dụng hàm kích hoạt new GeLU [46]. Nó cũng tích hợp Rotary Positional Embedding (RoPE) [47] cho các embedding vị trí. Đối với các mô hình LLaMA được huấn luyện trước, chúng có kích thước từ vựng (V) là 32000 và sử dụng hàm kích hoạt SwiGLU [48]. Những mô hình này cũng sử dụng RoPE cho các embedding vị trí. Để biết thông tin cấu hình chi tiết cho mỗi kích thước mô hình, vui lòng tham khảo Bảng A4.

C.2 Chi tiết Lượng tử hóa

Huấn luyện Nhận biết Lượng tử hóa với KD. Để sử dụng KD trong QAT, chúng tôi cần khởi tạo mô hình giáo viên và học sinh tương ứng. Mô hình giáo viên trải qua điều chỉnh tinh cụ thể nhiệm vụ ở độ chính xác đầy đủ (FP) dựa trên mô hình được huấn luyện trước. Mô hình học sinh sau đó được khởi tạo từ mô hình giáo viên, sau đó lượng tử hóa được áp dụng. Cài đặt siêu tham số của điều chỉnh tinh FP và QAT-KD, qua các loại và kích thước mô hình khác nhau, có thể được tìm thấy trong Bảng A4. Hơn nữa, triển khai thí nghiệm của chúng tôi sử dụng cơ sở mã mô hình hóa ngôn ngữ Huggingface⁴.

Lượng tử hóa Sau Huấn luyện. Chúng tôi tiến hành các thí nghiệm lượng tử hóa sau huấn luyện với OPTQ và AWQ [14,13], sử dụng mã từ bài báo gốc tương ứng⁵ ⁶. Chúng tôi sử dụng tập dữ liệu hiệu chuẩn bao gồm 128 đoạn token 2048 được chọn ngẫu nhiên từ tập dữ liệu PTB [34] cho OPTQ và tập dữ liệu Pile [49] cho AWQ. Để đảm bảo so sánh công bằng với QAT, chúng tôi áp dụng lượng tử hóa theo kênh làm granularity lượng tử hóa của chúng tôi.

⁴ https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling
⁵ https://github.com/IST-DASLab/gptq
⁶ https://github.com/mit-han-lab/llm-awq

--- TRANG 20 ---
Triển khai QuantGPT. Trong bài báo này, chúng tôi chủ yếu so sánh với QuantGPT, một phương pháp luận tiên tiến so với các công trình trước đây về QAT bộ giải mã. Cách tiếp cận này giới thiệu hai đóng góp chính: phương pháp tỷ lệ phụ thuộc mô-đun và chưng cất đối lập cấp token. Đối với các thí nghiệm QAT-KD 4-bit, chúng tôi áp dụng phương pháp lượng tử hóa QuantGPT [24] (tỷ lệ động phụ thuộc mô-đun). QuantGPT xem xét hệ số tỷ lệ lượng tử hóa như một tham số có thể học và tối ưu hóa nó thông qua QAT. Theo phương pháp tỷ lệ động của QuantGPT, chúng tôi xác định giá trị cắt α cho lượng tử hóa bằng cách nhân độ lớn trọng số trung bình ||w||₁/n với hệ số tỷ lệ có thể học γ, trong đó ||·||₁ biểu thị chuẩn ℓ1: α=γ·||w||₁/n. Trong trường hợp này, giá trị ban đầu cho γ được đặt là 1, và tỷ lệ học cho γ là 0.0002.

Khi triển khai chưng cất đối lập cấp token, chúng tôi quan sát các vấn đề về tính bền vững trong việc sao chép phương pháp KD chưng cất đối lập cấp token, nơi các lựa chọn không chính xác trong mẫu âm có thể dẫn đến sự suy giảm hiệu suất⁷. Do đó, Để đảm bảo so sánh công bằng, chúng tôi loại trừ mất mát đối lập khỏi triển khai Logit KD của chúng tôi.

Điều chỉnh tinh Kiểu ALPACA cho Nhiệm vụ Lý luận Số học. Trong điều chỉnh tinh nhiệm vụ lý luận số học (GSM8K), Chúng tôi sử dụng phương pháp điều chỉnh tinh kiểu ALPACA [12], được đề xuất cho điều chỉnh tinh demo tuân theo hướng dẫn. Phương pháp điều chỉnh tinh này về cơ bản sử dụng cách tiếp cận mô hình hóa ngôn ngữ, như được chứng minh trong Hình 1(b), dự đoán từ tiếp theo trong một chuỗi. Tuy nhiên, quá trình điều chỉnh tinh kiểu ALPACA có một đặc điểm đặc biệt: nó chuyển đổi các tập dữ liệu thành định dạng bao gồm các cặp hướng dẫn-phản hồi, như được minh họa trong Bảng A5. Chúng tôi áp dụng phương pháp điều chỉnh tinh kiểu ALPACA này cho các GLM được huấn luyện trước lớn vượt quá 2 tỷ tham số (OPT-2.7B/6.7B, LLaMA-7B).

D Ví dụ về Sinh Văn bản Lý luận Số học

Trong phần này, chúng tôi xem xét phương pháp QAT KD trên nhiệm vụ lý luận số học thông qua so sánh kết quả sinh từ mô hình QAT. Tập dữ liệu GSM8K phục vụ như một chuẩn mực để đo khả năng lý luận số học, và các mô hình được kỳ vọng tạo ra phản hồi văn bản tự hồi quy dựa trên các câu hỏi được cung cấp. Nhiệm vụ này yêu cầu không chỉ các tính toán toán học chính xác để tạo ra câu trả lời đúng, mà còn một quá trình giải quyết vấn đề logic, và câu trả lời cuối cùng là chính xác nếu cả logic và tính toán đều chính xác. Trong việc đánh giá GSM8K, chúng tôi sử dụng chiến lược giải mã tham lam cho quá trình sinh văn bản.

Trong Bảng A5, chúng ta có thể quan sát rằng các câu trả lời được tạo bởi mô hình QAT QuantGPT dường như có ý nghĩa thoạt nhìn (tương ứng với kết quả PPL thấp trong Bảng 2), nhưng khi xem xét kỹ hơn, rõ ràng là quá trình giải quyết vấn đề và tính toán cần thiết là không chính xác. Đặc biệt trong Câu hỏi 1, mô hình viết rằng nó nên thực hiện phép nhân trong quá trình giải pháp, nhưng thực tế thực hiện phép chia, dẫn đến kết quả trung gian không chính xác. Từ đó, nó tiếp tục phát triển một giải pháp hoàn toàn sai. Trong Câu hỏi 2, trong khi quá trình giải pháp và tính toán phù hợp, các phương pháp không chính xác được sử dụng để tạo ra kết quả trung gian, cuối cùng dẫn đến câu trả lời sai. Trong Bảng A6, có thể thấy rằng trong Câu hỏi 3, mô hình bỏ qua các bước trung gian cần thiết trong quá trình giải quyết vấn đề, dẫn đến câu trả lời không chính xác. Trong Câu hỏi 4, trong khi quá trình giải pháp là chính xác, có lỗi trong tính toán dẫn đến câu trả lời không chính xác. Ngược lại, chúng ta có thể xác nhận rằng kết quả sinh sử dụng phương pháp của chúng tôi liệt kê chính xác quá trình giải quyết vấn đề cần thiết để giải quyết câu hỏi, giống như sự thật cơ bản. Hơn nữa, kết quả tính toán là chính xác, do đó dẫn đến câu trả lời đúng.

Những so sánh mẫu như vậy cho phép chúng ta thấy những hạn chế của việc chỉ sử dụng Logit KD trong các nhiệm vụ lý luận. Như được hiển thị trong Bảng A5 và A6, khi Logit KD được sử dụng duy nhất cho QAT, như kết quả PPL thấp trong Bảng 2 cho thấy, nó dường như khôi phục tốt khả năng sinh văn bản tự nhiên của mô hình giáo viên. Tuy nhiên, trong kết quả của các nhiệm vụ lý luận yêu cầu tiến triển logic của giải pháp và tính toán chính xác, các khiếm khuyết như lỗi trong quá trình giải pháp và lỗi tính toán được quan sát. Với phương pháp của chúng tôi kết hợp hiệu quả GT Loss và Logit KD, chúng tôi đạt được khả năng sinh văn bản tự nhiên của mô hình giáo viên trong khi đồng thời nâng cao khả năng lý luận logic và tính toán chính xác thông qua sự thật cơ bản trong QAT. Do đó, phương pháp của chúng tôi thể hiện độ chính xác vượt trội trong chuẩn mực GSM8K.

⁷ Vấn đề này đã được thừa nhận trong bài báo QuantGPT được sửa đổi [50]

--- TRANG 21 ---
Câu hỏi 1: Dưới đây là một hướng dẫn mô tả một nhiệm vụ, được ghép nối với đầu vào cung cấp thêm bối cảnh. Viết một phản hồi hoàn thành một cách thích hợp yêu cầu. Người trông coi sở thú cho ăn tất cả các loài vượn trong sở thú. Anh ta đặt hàng tất cả chuối từ một trang trại địa phương mỗi 2 tháng. Nếu khỉ cần 200 chuối, khỉ đột cần 400 chuối, và khỉ baboon cần 100 chuối mỗi tháng, anh ta cần đặt hàng bao nhiêu chuối để kéo dài trong 2 tháng? Phản hồi:

Sự thật Cơ bản:
Tổng số chuối mà các loài vượn chủ yếu cần mỗi tháng là 200+400+100= «200+400+100=700»700.
Do đó trong 2 tháng, tổng số chuối chúng cần là 700x2= «700*2=1400»1400. #### 1400

QuantGPT:
Sở thú cần 200 chuối/khỉ x 2 khỉ = «200*2=400»400 chuối cho khỉ đột.
Nó cần 400 chuối/khỉ đột x 4 khỉ đột = «400*4=1600»1600 chuối cho khỉ baboon.
Nó cần 1600 chuối/baboon x 100 baboon = «1600/100=16»16 chuối cho khỉ baboon.
Nó cần 16 chuối/baboon x 4 baboon = «16*4=64»64 chuối cho khỉ baboon.
Nó cần 64 chuối/baboon x 4 baboon = «64*4=256»256 chuối cho sở thú.
Nó cần 256 chuối/2 tháng = «256/2=128»128 chuối để kéo dài trong 2 tháng. #### 128

Của chúng tôi:
Sở thú cần mua 200 chuối/tháng x 2 tháng = «200*2=400»400 chuối.
Khỉ đột cần 400 chuối/tháng x 2 tháng = «400*2=800»800 chuối.
Khỉ baboon cần 100 chuối/tháng x 2 tháng = «100*2=200»200 chuối.
Tổng số chuối cần thiết là 400 chuối + 800 chuối + 200 chuối = «400+800+200=1400»1400 chuối. #### 1400

Câu hỏi 2: Dưới đây là một hướng dẫn mô tả một nhiệm vụ, được ghép nối với đầu vào cung cấp thêm bối cảnh. Viết một phản hồi hoàn thành một cách thích hợp yêu cầu. Nhân viên của Google đã đi du lịch một ngày. 4 xe buýt được thuê có sức chứa 60 nhân viên. 6 xe buýt nhỏ có thể chứa 30 nhân viên, và 10 xe van nhỏ có thể chứa 15 nhân viên. Có bao nhiêu nhân viên có thể tham gia chuyến du lịch? Phản hồi:

Sự thật Cơ bản:
60 x 4 = «60*4=240»240 nhân viên có thể đi xe buýt.
30 x 6 = «30*6=180»180 nhân viên có thể đi xe buýt nhỏ.
15 x 10 = «15*10=150»150 nhân viên có thể đi xe van nhỏ.
Do đó 240 + 180 + 150 = «240+180+150=570»570 nhân viên có thể tham gia chuyến du lịch. #### 570

QuantGPT:
Sức chứa của mỗi xe buýt là 60 nhân viên / 4 xe buýt = «60/4=15»15 nhân viên.
Sức chứa của mỗi xe buýt nhỏ là 30 nhân viên / 6 xe buýt nhỏ = «30/6=5»5 nhân viên.
Sức chứa của mỗi xe van nhỏ là 15 nhân viên / 10 xe van nhỏ = «15/10=1.5»1.5 nhân viên.
Tổng sức chứa của xe buýt và xe buýt nhỏ là 15 nhân viên + 5 nhân viên + 1.5 nhân viên = «15+5+1.5=20»20 nhân viên.
Tổng số nhân viên có thể tham gia chuyến du lịch là 20 nhân viên + 60 nhân viên = «20+60=80»80 nhân viên. #### 80

Của chúng tôi:
4 xe buýt có thể chứa 60 x 4 = «60*4=240»240 nhân viên.
6 xe buýt nhỏ có thể chứa 30 x 6 = «30*6=180»180 nhân viên.
10 xe van nhỏ có thể chứa 15 x 10 = «15*10=150»150 nhân viên.
Do đó, 240 + 180 + 150 = «240+180+150=570»570 nhân viên có thể tham gia chuyến du lịch. #### 570

Bảng A5: Mẫu đầu ra lý luận số học được tạo bởi OPT-6.7B lượng tử hóa trọng số tam phân với các phương pháp khác nhau. Phần in nghiêng đề cập đến thành phần định dạng hướng dẫn trong câu hỏi.

--- TRANG 22 ---
Câu hỏi 3: Dưới đây là một hướng dẫn mô tả một nhiệm vụ, được ghép nối với đầu vào cung cấp thêm bối cảnh. Viết một phản hồi hoàn thành một cách thích hợp yêu cầu. Trong một tuần cụ thể, một xe cứu hộ đã kéo mười chiếc xe trong mỗi ngày của ba ngày đầu và sau đó ít hơn bốn chiếc xe trong mỗi ngày còn lại của tuần. Tính tổng số xe nó đã kéo trong tuần đó. Phản hồi:

Sự thật Cơ bản:
Trong ba ngày đầu, kéo mười chiếc xe một ngày, xe cứu hộ đã kéo 3*10 = «3*10=30»30 chiếc xe.
Nó kéo ít hơn bốn chiếc xe mỗi ngày, đó là 10-4 = «10-4=6»6 chiếc xe trong những ngày còn lại.
Nếu nó kéo mười chiếc xe trong ba ngày và ít hơn bốn chiếc trong những ngày còn lại, thì nó kéo trong 7-3 = 4 ngày, ít hơn bốn chiếc xe mỗi ngày.
Trong bốn ngày, xe đã kéo 6*4= «6*4=24»24 chiếc xe.
Tổng số xe nó đã kéo trong tuần đó là 24+30 = «24+30=54»54 chiếc xe.
#### 54

QuantGPT:
Trong ba ngày đầu, xe tải đã kéo tổng cộng 3*10 = «3*10=30»30 chiếc xe.
Trong những ngày còn lại của tuần, nó đã kéo tổng cộng 10-4 = «10-4=6»6 chiếc xe.
Tổng số xe nó đã kéo trong tuần đó là 30+6 = «30+6=36»36 chiếc xe.
#### 36

Của chúng tôi:
Trong ba ngày đầu, xe tải đã kéo 10*3 = «10*3=30»30 chiếc xe.
Trong những ngày còn lại, nó đã kéo 10-4 = «10-4=6»6 chiếc xe mỗi ngày.
Tổng số xe nó đã kéo trong những ngày còn lại là 6*4 = «6*4=24»24
Tổng cộng, xe tải đã kéo 30+24 = «30+24=54»54 chiếc xe trong tuần đó.
#### 54

Câu hỏi 4: Dưới đây là một hướng dẫn mô tả một nhiệm vụ, được ghép nối với đầu vào cung cấp thêm bối cảnh. Viết một phản hồi hoàn thành một cách thích hợp yêu cầu. Nick, Richard, Jason và DJ mỗi người đều có súng paintball. DJ có 8 khẩu súng, Nick có 10 khẩu súng, RJ có 1 khẩu súng và Richard có 5 khẩu súng. Nếu họ chia đều súng của mình, mỗi người sẽ có bao nhiêu khẩu súng? Phản hồi:

Sự thật Cơ bản:
Tổng số súng là 8+10+1+5 = «8+10+1+5=24» 24 khẩu súng.
Mỗi người sẽ có 24/4= «24/4=6» 6 khẩu súng.
#### 6

QuantGPT:
DJ có 8 khẩu súng, Nick có 10 khẩu súng, RJ có 1 khẩu súng và Richard có 5 khẩu súng.
Tổng cộng là 8+10+1+5 = «8+10+1+5=22» 22 khẩu súng.
Nếu họ chia đều súng của mình, mỗi người sẽ có 22/4 = «22/4=5» 5 khẩu súng.
#### 5

Của chúng tôi:
DJ có 8 khẩu súng, Nick có 10 khẩu súng, RJ có 1 khẩu súng Richard có 5 khẩu súng.
Tổng cộng họ có 8+10+1+5 = «8+10+1+5=24» 24 khẩu súng.
Nếu họ chia đều, mỗi người sẽ nhận được 24/4 = «24/4=6» 6 khẩu súng
#### 6

Bảng A6: Mẫu đầu ra lý luận số học được tạo bởi OPT-6.7B lượng tử hóa trọng số tam phân với các phương pháp khác nhau. Phần in nghiêng đề cập đến thành phần định dạng hướng dẫn trong câu hỏi.
