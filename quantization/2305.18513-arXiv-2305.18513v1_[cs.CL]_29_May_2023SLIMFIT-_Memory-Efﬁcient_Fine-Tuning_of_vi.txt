arXiv:2305.18513v1 [cs.CL] 29 Tháng 5 năm 2023SLIMFIT: Điều chỉnh tinh hiệu quả bộ nhớ của các mô hình dựa trên Transformer sử dụng động lực học huấn luyện

Arash Ardakani1Altan Haan1Shangyin Tan1Doru Thom Popovici2
Alvin Cheung1Costin Iancu2Koushik Sen1
Đại học California, Berkeley1Phòng thí nghiệm quốc gia Lawrence Berkeley2
{arash.ardakani,altanh,shangyin,akcheung,ksen}@berkeley.edu
{dtpopovici,cciancu}@lbl.gov

Tóm tắt

Các mô hình dựa trên Transformer, như BERT và ViT, đã đạt được kết quả tối tân trên các tác vụ xử lý ngôn ngữ tự nhiên (NLP) và thị giác máy tính (CV) khác nhau. Tuy nhiên, các mô hình này cực kỳ tốn bộ nhớ trong quá trình điều chỉnh tinh, khiến chúng khó triển khai trên GPU có tài nguyên bộ nhớ hạn chế. Để giải quyết vấn đề này, chúng tôi giới thiệu một công cụ mới gọi là SLIMFIT để giảm yêu cầu bộ nhớ của các mô hình này bằng cách phân tích động lực học huấn luyện của chúng một cách động và đóng băng các lớp ít đóng góp trong quá trình điều chỉnh tinh. Các lớp để đóng băng được chọn bằng thuật toán lập lịch liên lớp thời gian thực. SLIMFIT áp dụng lượng tử hóa và cắt tỉa cho các lớp cụ thể để cân bằng tải của các activation động và giảm thiểu dấu chân bộ nhớ của các activation tĩnh, trong đó activation tĩnh đề cập đến những activation không thể bị loại bỏ bất kể việc đóng băng. Điều này cho phép SLIMFIT đóng băng lên đến 95% các lớp và giảm việc sử dụng bộ nhớ GPU trên thiết bị tổng thể của các mô hình dựa trên transformer như ViT và BERT trung bình 2,2×, trên các benchmark/dataset NLP và CV khác nhau như GLUE, SQuAD 2.0, CIFAR-10, CIFAR-100 và ImageNet với độ suy giảm chính xác trung bình 0,2%. Đối với các tác vụ NLP và CV như vậy, SLIMFIT có thể giảm lên đến 3,1× tổng việc sử dụng bộ nhớ trên thiết bị với độ suy giảm chính xác chỉ lên đến 0,4%. Kết quả là, trong khi điều chỉnh tinh ViT trên ImageNet và BERT trên SQuAD 2.0 với kích thước batch 128 yêu cầu lần lượt 3 và 2 GPU 32GB, SLIMFIT cho phép điều chỉnh tinh chúng trên một GPU 32GB mà không có bất kỳ suy giảm chính xác đáng kể nào. Mã của bài báo này có sẵn tại https://github.com/arashardakani/SlimFit.

1 Giới thiệu

Trong vài năm qua, nhiều mô hình dựa trên transformer đã được phát triển với việc áp dụng cơ chế attention để cân nhắc tầm quan trọng của từng phần dữ liệu đầu vào khác nhau. Việc pre-training các mô hình dựa trên transformer như vậy trên dữ liệu lớn đã dẫn đến sự tăng cường đáng kể về độ chính xác khi được điều chỉnh tinh trên các tác vụ downstream xử lý ngôn ngữ tự nhiên (NLP) và thị giác máy tính (CV) khác nhau [1, 2]. Mặc dù hiệu suất tuyệt vời trong việc đạt độ chính xác tối tân (SOTA), các mô hình này tốn bộ nhớ và yêu cầu một lượng lớn bộ nhớ GPU trên thiết bị trong giai đoạn điều chỉnh tinh so với các mạng nơ-ron tích chập và hồi quy thông thường [3]. Yêu cầu bộ nhớ của các mô hình dựa trên transformer hiện tại đã khiến chúng khó điều chỉnh tinh ngay cả trên GPU mạnh mẽ. Với việc giới thiệu các mô hình dựa trên transformer lớn hơn trong vài năm qua, bộ nhớ GPU trên thiết bị đã trở thành nút thắt cổ chai chính cho quá trình điều chỉnh tinh của chúng [3, 4, 5].

Tổng việc sử dụng bộ nhớ trên thiết bị của GPU chủ yếu bao gồm activations, tham số, gradients, trạng thái optimizer và ngữ cảnh CUDA. Trong số các yếu tố này, activations chiếm phần lớn việc sử dụng bộ nhớ do batching, khiến chúng lớn hơn các yếu tố khác vài bậc độ lớn (xem Hình 1). Do đó, activation compressed training (ACT) đã nổi lên như giải pháp chính cho điều chỉnh tinh hiệu quả bộ nhớ [6, 4]. Phương pháp này đầu tiên nén activations trong quá trình truyền tiến và sau đó giải nén chúng trong quá trình truyền ngược. Theo cách này, dấu chân bộ nhớ có thể được giảm đáng kể bằng cách lưu trữ các activations đã nén. Trong ACT, lượng tử hóa [7, 8, 6, 4] đã là lựa chọn phổ biến để nén activations trong số các bộ nén khác như JPEG [9] hoặc cắt tỉa [5]. ACT SOTA hiện tại gán adaptive các bit lượng tử hóa cho mỗi lớp cho một kiến trúc nhất định [4].

Trong khi ACT SOTA thành công trong việc giảm dấu chân bộ nhớ của activations, việc giảm bộ nhớ GPU trên thiết bị tổng thể của nó không đáng kể. Ví dụ, việc giảm bộ nhớ GPU trên thiết bị tổng thể của ACT SOTA bị giới hạn ở 0,1GB mặc dù giảm 6,4× bộ nhớ activations khi điều chỉnh tinh BERT trên dataset CoLA với kích thước batch 32. Đáng chú ý rằng chúng tôi đề cập đến việc sử dụng bộ nhớ được báo cáo bởi "nvidia-smi" là bộ nhớ trên thiết bị tổng thể trong bài báo này (xem Phụ lục A để biết thêm thông tin về quản lý bộ nhớ).

Tensor rematerialization [3, 10, 11, 12], còn được gọi là gradient checkpointing, là một phương pháp nổi bật khác để giảm bộ nhớ activation bằng cách đánh đổi tính toán với bộ nhớ. Trong tensor rematerialization, chỉ các activations cụ thể được lưu trữ trong quá trình truyền tiến, trong khi phần còn lại được tính toán lại trong quá trình truyền ngược. Tất nhiên, việc tính toán lại activations yêu cầu nhiều phép toán hơn và kéo dài đáng kể quá trình điều chỉnh tinh [4]. Reduced precision training, như một phương pháp khác, thực hiện các tính toán của cả truyền tiến và truyền ngược ở độ chính xác thấp [13, 14, 15, 16]. Trong khi các công trình này có thể huấn luyện thành công các mô hình thông thường, điều chỉnh tinh mô hình few-bit không hề đơn giản. Ví dụ, lượng tử hóa 8-bit của BERT cho inference dẫn đến mất mát độ chính xác đáng kể [17], khiến điều chỉnh tinh trên few bits trở thành một tác vụ thách thức.

Low-rank adaptation (LoRA) [18] là một phương pháp chính khác để giảm bộ nhớ GPU trên thiết bị tổng thể, trong đó các mô hình dựa trên transformer được điều chỉnh tinh bằng cách chèn một số lượng nhỏ tham số có thể huấn luyện vào mỗi lớp trong khi giữ các tham số mô hình đã pre-train bị đóng băng. Phương pháp như vậy cho phép điều chỉnh tinh các mô hình dựa trên transformer với số lượng tham số có thể huấn luyện ít hơn đáng kể, dẫn đến giảm dấu chân bộ nhớ của trạng thái optimizer và gradients. Việc giảm bộ nhớ như vậy trở nên đáng kể đối với các mô hình transformer cực lớn như GPT [19] với hơn một trăm tỷ tham số.

Khác với các phương pháp này, chúng tôi đưa ra một phương pháp mới để giảm việc sử dụng bộ nhớ trên thiết bị tổng thể bằng cách phân tích động lực học huấn luyện. Cụ thể hơn, chúng tôi phân tích động các đóng góp gradient của các lớp trong mô hình dựa trên transformer và thực hiện cập nhật tham số chỉ cho các lớp cụ thể trong khi các lớp còn lại được giữ đóng băng. Động lực học huấn luyện đã được sử dụng để phân tích hành vi của mô hình trong quá trình huấn luyện/điều chỉnh tinh [20, 21, 22]. Tuy nhiên, công trình của chúng tôi sử dụng động lực học huấn luyện để phát hiện và loại bỏ các activations không quan trọng trong quá trình điều chỉnh tinh bằng cách đóng băng các lớp liên quan, dẫn đến giảm dấu chân bộ nhớ. Phương pháp của chúng tôi trực giao với các phương pháp hiện có bao gồm rematerialization và LoRA, có thể được kết hợp để giảm thêm.

Đóng băng các lớp hoặc tham số đã được nghiên cứu trong các lĩnh vực khác nhau, bao gồm các mô hình dựa trên transformer để bảo tồn thông tin đã học trước đó trong quá trình điều chỉnh tinh [23]. Đóng băng tham số cũng đã được sử dụng để điều chỉnh điều chỉnh tinh (ví dụ: giảm over-fitting) trong các mô hình đã pre-train [24]. Gần đây, đóng băng đã được sử dụng để tăng tốc điều chỉnh tinh bằng cách đóng băng progressively các khối mô hình [25, 26, 27]. Tuy nhiên, vì phương pháp như vậy bắt đầu quá trình điều chỉnh tinh mà không đóng băng ít nhất trong vài iteration huấn luyện, yêu cầu bộ nhớ trên thiết bị tổng thể của nó vẫn tương tự như huấn luyện không có đóng băng. Ví dụ, điều chỉnh tinh ViT trên ImageNet với kích thước batch 128 sử dụng phương pháp đóng băng như vậy trên một GPU 32GB dẫn đến lỗi out-of-memory (xem Phụ lục B để biết thêm chi tiết).

Để điều phối các quyết định đóng băng lớp hiệu quả, chúng tôi giới thiệu thuật toán lập lịch liên lớp thời gian thực (ILS). Phương pháp của chúng tôi tìm và đóng băng một tập hợp các lớp trong mô hình dựa trên transformer ít đóng góp hơn, tức là các lớp với ít cập nhật hơn trong tham số của chúng, cho quá trình điều chỉnh tinh tại mỗi iteration. Trong khi thuật toán ILS thành công trong việc phát hiện và đóng băng các lớp không quan trọng, việc giảm bộ nhớ của nó không tỷ lệ với tỷ lệ đóng băng. Lý do đằng sau sự không tỷ lệ này là hai mặt: số lượng activations không cân bằng giữa các lớp và sự tồn tại của activations tĩnh. Activations tĩnh đề cập đến những activations không thể bị loại bỏ bất kể việc đóng băng (ví dụ: activations của các hàm phi tuyến như GELU). Chúng tôi giải quyết hai vấn đề này bằng cách sử dụng lượng tử hóa và cắt tỉa để cân bằng số lượng activations trên tất cả các lớp và giảm overhead bộ nhớ của activations tĩnh. Chúng tôi sử dụng lượng tử hóa và cắt tỉa cho một vài lớp cụ thể của các mô hình dựa trên transformer trái ngược với các phương pháp reduced precision training trong đó tất cả các lớp đều được lượng tử hóa. Kết quả là, tác động của lượng tử hóa và cắt tỉa lên độ chính xác là không đáng kể trong công trình của chúng tôi. Ví dụ, suy giảm độ chính xác do lượng tử hóa và cắt tỉa chỉ là 0,1% trên dataset MRPC.

Bằng cách kết hợp ILS với lượng tử hóa và cắt tỉa, chúng tôi giới thiệu một công cụ hiệu suất gọi là SLIMFIT để giảm việc sử dụng bộ nhớ GPU trên thiết bị của các mô hình dựa trên transformer trong quá trình điều chỉnh tinh. Chúng tôi chứng minh hiệu quả của SLIMFIT trong việc giảm dấu chân bộ nhớ trên các mô hình phổ biến BERT và ViT. Chúng tôi cho thấy SLIMFIT có thể đóng băng lên đến 95% các lớp và giảm việc sử dụng bộ nhớ trên thiết bị tổng thể trung bình 2,2× khi điều chỉnh tinh các mô hình BERT và ViT trên các benchmarks và datasets khác nhau, như GLUE, SQuAD 2.0, CIFAR-10, CIFAR-100 và ImageNet với suy giảm độ chính xác trung bình 0,2%. Cụ thể hơn, SLIMFIT giảm việc sử dụng bộ nhớ trên thiết bị tổng thể của quá trình điều chỉnh tinh trên GLUE từ 6,1GB xuống 4,0GB (giảm 1,5×) với kích thước batch 32, trên SQuAD 2.0 từ 58,5GB xuống 19,1GB (giảm 3,1×) với kích thước batch 128, trên CIFAR-10 từ 7,2GB xuống 4,3GB (giảm 1,7×) với kích thước batch 32, trên CIFAR-100 từ 7,2GB xuống 4,5GB (giảm 1,6×) với kích thước batch 32, và trên ImageNet từ 77,4GB xuống 26,1GB (3,0×) với kích thước batch 128 với chi phí lên đến 0,4% suy giảm độ chính xác. Kết quả là, SLIMFIT cho phép thực hiện các quá trình điều chỉnh tinh tốn bộ nhớ trên một GPU 32GB như điều chỉnh tinh ViT trên ImageNet với kích thước batch 128 trong khi điều này thường yêu cầu ba GPU 32GB.

2 Kiến thức chuẩn bị

Trong vài năm qua, việc pre-training các mô hình dựa trên attention đã dẫn đến những tiến bộ đáng kể trên nhiều tác vụ NLP và CV với các mô hình BERT [1] và ViT [2] phổ biến. Quá trình pre-training cung cấp một điểm khởi tạo tốt để các mô hình này có thể tổng quát hóa tốt hơn trên dữ liệu chưa thấy của các tác vụ downstream. Do đó, các mô hình này có thể đạt kết quả tối tân bằng cách điều chỉnh tinh thông qua các điều chỉnh nhỏ đối với tham số của chúng. Về mặt kiến trúc, các mô hình này bao gồm một lớp embedding ban đầu, theo sau là các khối lặp lại của multi-head attention (MHA) được đưa vào một module feed-forward network (FFN) (xem Phụ lục C để biết thêm chi tiết). Các kiến trúc cơ sở của BERT và ViT chứa hơn một trăm lớp được xây dựng theo cách này.

Mặc dù có số lượng lớp lớn, không phải tất cả đều cần được cập nhật trong quá trình điều chỉnh tinh để đạt hiệu suất tốt trên các tác vụ downstream, như được chỉ ra trong [28]. Đáng chú ý, các tác giả phát hiện rằng đóng băng khoảng 60% các lớp attention sớm trong BERT dẫn đến suy giảm hiệu suất không đáng kể. Điều này cho thấy rằng mô hình đã điều chỉnh tinh có xu hướng bảo tồn các đặc trưng chung được học trong quá trình pre-training. Được thúc đẩy bởi nghiên cứu này, chúng tôi tìm cách phân tích động lực học huấn luyện của các mô hình đã pre-train và tự động phát hiện các lớp với ít đóng góp cho quá trình điều chỉnh tinh.

3 Học tầm quan trọng của các lớp

Động lực học huấn luyện là một lĩnh vực nghiên cứu tích cực cung cấp cái nhìn sâu sắc về hành vi của các mô hình đã pre-train khi điều chỉnh tinh trên các tác vụ downstream. Chứng minh hội tụ của các thuật toán tối ưu hóa như stochastic gradient descent [29] cho thấy rằng khoảng cách giữa các tham số và giải pháp tối ưu được giảm qua các iteration huấn luyện và do đó, khoảng cách trọng số (hoặc lượng cập nhật trọng số) giữa các iteration liên tiếp giảm. Do đó, có thể một số lớp chỉ có thể nhận được những thay đổi tối thiểu đối với tham số của chúng khi chúng ta tiếp cận cuối quá trình huấn luyện. Tất nhiên, việc phát hiện và đóng băng các lớp như vậy, khi chúng cho thấy cập nhật tối thiểu, sẽ không ảnh hưởng đến độ chính xác. Vì các mô hình dựa trên transformer đã được pre-train, chúng đã cho thấy các cập nhật nhỏ trong quá trình điều chỉnh tinh so với pre-training. Do đó, việc phát hiện và đóng băng các lớp với cập nhật tối thiểu (tức là các giá trị khoảng cách trọng số) sẽ không ảnh hưởng đáng kể đến quá trình điều chỉnh tinh và do đó là độ chính xác cuối cùng. Dựa trên các quan sát trên, chúng tôi xem xét chuẩn ℓ1 của cập nhật nhận được bởi tham số của mỗi lớp qua tất cả các iteration điều chỉnh tinh như động lực học huấn luyện trong bài báo này. Cũng đáng chú ý rằng việc đóng băng các lớp không có tác động đến sự hội tụ huấn luyện vì nó gây ra sự tạm dừng trong quy trình huấn luyện của các lớp bị đóng băng như được chỉ ra bởi phân tích lý thuyết của chúng tôi trong Phụ lục D.1.

3.1 Động lực học huấn luyện

Hãy xem xét một mô hình đã pre-train với một tập hợp tham số W trong đó các tham số liên quan đến lớp thứ i tại iteration t được ký hiệu là Wt_i ∈ R^(M×I). Động lực học huấn luyện cho lớp thứ i tại iteration t được định nghĩa là chuẩn ℓ1 của khoảng cách giữa Wt-1_i và Wt_i, tức là

dt_i = (1/(M×I)) ||Wt_i - Wt-1_i / Wt-1_i||_ℓ1    (1)

trong đó dt ∈ R^n_+ chứa tất cả dis tại iteration t được gọi là vector khoảng cách, và n biểu thị tổng số lớp. Trên thực tế, Phương trình (1) tính toán thay đổi được chuẩn hóa trong các tham số của lớp thứ i.

3.2 Thuật toán lập lịch liên lớp

Chúng tôi sử dụng các giá trị khoảng cách làm động lực học huấn luyện để phân tích hành vi điều chỉnh tinh của các mô hình đã pre-train. Ví dụ, hãy xem xét các giá trị khoảng cách trên tất cả các iteration điều chỉnh tinh cho các dataset CoLA [30] và MRPC [31]. Hình 2a cho thấy các giá trị khoảng cách của ma trận trọng số query cho các lớp attention thứ nhất, thứ năm và thứ mười một của BERT-base được điều chỉnh tinh trên dataset CoLA trong khi Hình 2b mô tả những giá trị tương tự cho các lớp giống nhau của BERT-base được điều chỉnh tinh trên dataset MRPC.

Chúng tôi quan sát những điều sau dựa trên kết quả thực nghiệm của hai dataset này. Đầu tiên, lượng cập nhật cho mỗi lớp trở nên nhỏ hơn qua các iteration điều chỉnh tinh. Thứ hai, lượng cập nhật của mỗi lớp là cụ thể theo tác vụ và độc lập với vị trí của nó. Thứ ba, có một số lớp cho thấy các giá trị khoảng cách nhỏ hơn so với các lớp khác trên hầu hết tất cả các iteration. Cuối cùng, các lớp với giá trị khoảng cách cao hơn ở đầu có thể trở nên nhỏ hơn qua các iteration điều chỉnh tinh so với các lớp bắt đầu với giá trị khoảng cách thấp hơn.

Với các quan sát trên, chúng tôi giới thiệu thuật toán ILS để quyết định về ưu tiên cập nhật của các lớp sử dụng các giá trị khoảng cách của chúng. Hình 3 cho thấy tổng quan về thuật toán ILS. Tại mỗi iteration từ iteration đầu tiên đến iteration cuối cùng, thuật toán ILS của chúng tôi chọn những lớp có giá trị khoảng cách lớn để được cập nhật và những lớp có giá trị khoảng cách nhỏ để bị đóng băng. Cụ thể hơn, các lớp đầu tiên được xếp hạng dựa trên các giá trị khoảng cách của chúng tại mỗi iteration huấn luyện và sau đó những lớp có giá trị khoảng cách nhỏ được giữ đóng băng theo tỷ lệ đóng băng như một siêu tham số. Trực giác là các lớp với giá trị khoảng cách nhỏ ít đóng góp hơn cho quá trình điều chỉnh tinh vì tham số của chúng không được cập nhật nhiều. Mặt khác, các lớp với giá trị khoảng cách lớn đang học các mẫu cụ thể theo tác vụ bằng cách thực hiện các điều chỉnh đáng kể hơn đối với tham số của chúng. Lưu ý rằng việc đóng băng các lớp giữa không làm gián đoạn việc truyền gradient đến các lớp sớm của mạng như được chỉ ra thông qua một ví dụ trong Phụ lục D.2.

Tỷ lệ đóng băng của thuật toán ILS có thể được quyết định dựa trên ngân sách bộ nhớ GPU trên thiết bị. Tất nhiên, sử dụng tỷ lệ đóng băng cực cao có thể dẫn đến suy giảm hiệu suất tùy thuộc vào tác vụ downstream, cung cấp sự đánh đổi xứng đáng giữa độ chính xác và bộ nhớ GPU trên thiết bị. Mặt khác, trong khi suy giảm hiệu suất không có khả năng với tỷ lệ đóng băng rất nhỏ, việc giảm bộ nhớ cũng không đáng kể.

Vì không có kiến thức trước về các giá trị khoảng cách của mỗi lớp ở đầu quá trình điều chỉnh tinh, thuật toán ILS của chúng tôi khởi tạo vector khoảng cách với các giá trị ngẫu nhiên lớn. Tùy thuộc vào tỷ lệ đóng băng, mỗi lớp cùng với giá trị khoảng cách của nó được cập nhật trong vài iteration đầu tiên cho đến khi tất cả các số ngẫu nhiên trong vector khoảng cách được thay thế bằng giá trị khoảng cách thực tế. Sau đó, các lớp được giữ đóng băng theo giá trị khoảng cách thực tế của chúng. Giá trị khoảng cách của các lớp hoạt động chỉ được cập nhật tại mỗi iteration trong khi giá trị của các lớp bị đóng băng vẫn không thay đổi. Mã giả của thuật toán ILS của chúng tôi thực hiện đóng băng lặp được hiển thị trong Thuật toán 1.

Để hiểu rõ hơn thuật toán ILS, chúng tôi minh họa quá trình đóng băng lặp sử dụng một ví dụ như được hiển thị trong Hình 4. Giả sử chúng ta có một mô hình dựa trên transformer 8 lớp và do đó một vector khoảng cách 8 phần tử tại iteration t. Xem xét tỷ lệ đóng băng 50% cho ví dụ này, 4 lớp với các giá trị khoảng cách thấp nhất được giữ đóng băng và phần còn lại được cập nhật tại mỗi iteration.

4 Cân bằng tải liên lớp

Cho đến nay, chúng tôi đã giới thiệu thuật toán ILS ưu tiên cập nhật các lớp cụ thể trong khi giữ các lớp còn lại đóng băng theo giá trị khoảng cách của chúng. Đối với tỷ lệ đóng băng nhất định là 50% như một ví dụ, chúng ta mong đợi thấy giảm 2× trong dấu chân bộ nhớ của activations. Tuy nhiên, đây không phải là trường hợp trong các mô hình dựa trên transformer do sự mất cân bằng số lượng activations trên tất cả các lớp. Trên thực tế, sự mất cân bằng trong số lượng activations làm suy yếu khả năng của thuật toán ILS trong việc giảm dấu chân bộ nhớ trong quá trình điều chỉnh tinh như được hiển thị trong Hình 5.

Vì trọng tâm của bài báo này là về các mô hình dựa trên transformer như BERT và ViT, chúng tôi phân tích kiến trúc của chúng cho các lớp mất cân bằng. Bảng 1 tóm tắt số lượng activations liên quan đến đầu vào của các lớp với tham số có thể huấn luyện trong BERT hoặc ViT. Trong số tất cả các lớp có thể huấn luyện, chỉ có một lớp mất cân bằng trong khối attention chứa 4× nhiều activations hơn các lớp khác.

Để giải quyết vấn đề cân bằng tải trong số lượng activations cho lớp nói trên, chúng tôi sử dụng lượng tử hóa. Vì yếu tố mất cân bằng giữa các lớp là 4×, chúng tôi áp dụng lượng tử hóa 8-bit cho activations của lớp mất cân bằng trong đó 4 bit được sử dụng cho cả phần số nguyên và phân số. Theo cách này, chi phí bộ nhớ của activations được cân bằng bằng cách sử dụng lượng tử hóa. Trong sơ đồ lượng tử hóa của chúng tôi, chúng tôi lưu trữ activations của lớp mất cân bằng sử dụng 8 bit trong quá trình truyền tiến. Trong quá trình truyền ngược, chúng tôi chuyển đổi activations 8-bit sang định dạng floating-point 32-bit. Do đó, tất cả các tính toán truyền tiến và truyền ngược vẫn được thực hiện sử dụng định dạng floating-point độ chính xác đơn. Quá trình chuyển đổi giữa các định dạng fixed-point 8-bit và floating-point 32-bit được cung cấp trong Phụ lục E.

5 Activations động và tĩnh

Loại activations trong các mô hình dựa trên transformer có thể được chia thành hai loại: động và tĩnh. Chúng tôi đề cập đến các activations có thể bị loại bỏ bằng cách đóng băng lớp của chúng là activations động. Mặt khác, activations tĩnh không thể bị loại bỏ bất kể việc đóng băng. Trong số các loại lớp khác nhau, GELU, MatMul, Softmax và LayerNorm chứa activations tĩnh như được hiển thị trong Bảng 2. Lưu ý rằng MatMul và Softmax chia sẻ cùng các activations. Đối với các tính toán ngược của Softmax, đầu ra của nó trong quá trình truyền tiến được lưu như activations của nó. Mặt khác, đầu vào của MatMul được yêu cầu cho các tính toán ngược của nó như activations. Vì đầu ra của Softmax là đầu vào cho MatMul trong quá trình truyền tiến, chúng chia sẻ cùng các activations.

GELU và MatMul/Softmax không có bất kỳ tham số có thể huấn luyện nào và do đó không thể bị đóng băng. Do đó, hai lớp này giữ activations của chúng trong suốt quá trình điều chỉnh tinh. Phương pháp tốt nhất để giảm chi phí bộ nhớ của chúng là lượng tử hóa. Chúng tôi sử dụng 4 và 8 bit cho lượng tử hóa activations trong GELU và MatMul/Softmax, tương ứng. Vì không có hỗ trợ tensor 4-bit trong PyTorch, chúng tôi lưu trữ mỗi hai activations 4-bit như một activations 8-bit duy nhất sử dụng các phép toán shift. Lưu ý rằng việc sử dụng các mức bit như vậy dẫn đến suy giảm độ chính xác không đáng kể trong khi lượng tử hóa thêm các activations đó gây ra mất mát độ chính xác đáng kể.

Trái ngược với GELU và MatMul/Softmax, LayerNorm chứa các tham số có thể huấn luyện và có thể bị đóng băng bởi thuật toán ILS. Tuy nhiên, activations của nó vẫn tĩnh. Quá trình truyền tiến của LayerNorm được tính bởi:

x̃ = (x - E(x)) / √(Var(x) + ε)    (2)
y = x̃ * γ + β    (3)

trong đó γ và β là các tham số có thể huấn luyện. Đầu vào và đầu ra của LayerNorm được ký hiệu lần lượt là x ∈ R^H và y ∈ R^H. E(·) và Var(·) tính toán trung bình và phương sai, tương ứng. Đạo hàm của loss đối với γ (tức là ∇γ) được tính bởi

∇γ = x̃ * ∇y    (4)

và đối với β (tức là ∇β) bởi:

∇β = ∇y    (5)

trong đó ∇y biểu thị đạo hàm của loss đối với y. Chúng ta cũng cần tính đạo hàm của loss đối với x (tức là ∇x) như:

g = (γ * ∇y) / (H * √(Var(x) + ε))    (6)
∇x = H * g - ∑_H g - x̃ * ∑_H(g * x̃)    (7)

Khi LayerNorm bị đóng băng, không cần tính Phương trình (4). Tuy nhiên, activations của lớp này không thể bị loại bỏ vì chúng vẫn là một phần của các tính toán trong Phương trình (7). Cụ thể hơn, phiên bản chuẩn hóa của x (tức là x̃) được yêu cầu ngay cả khi lớp này bị đóng băng.

Đóng góp của số hạng cuối trong Phương trình (7) (tức là ∑_H(g * x̃)) chỉ đáng kể đối với các giá trị lớn của x̃. Do đó, các giá trị nhỏ của x̃ có thể bị loại bỏ. Lý tưởng nhất, chúng ta muốn có tất cả activations của lớp này bị loại bỏ khi lớp này bị đóng băng. Tuy nhiên, điều này sẽ dẫn đến suy giảm độ chính xác. Do đó, chúng tôi cắt tỉa các giá trị nhỏ trong x̃ và giữ 10% các giá trị lớn nhất. Theo cách này, tải bộ nhớ của activations được giảm đáng kể. Tất nhiên, khi lớp này không bị đóng băng, backpropagation được thực hiện mà không có bất kỳ xấp xỉ nào. Thủ thuật như vậy chuyển đổi LayerNorm từ một lớp tĩnh thành một lớp bán tĩnh. Đáng chú ý rằng các chỉ số đến activations bị cắt tỉa cũng được lưu trữ cùng với activations. Chi tiết của quy trình cắt tỉa được cung cấp trong Phụ lục F.

6 SLIMFIT

SLIMFIT là một công cụ hiệu suất khai thác thuật toán ILS của chúng tôi cùng với lượng tử hóa và cắt tỉa để giảm dấu chân bộ nhớ của activations thông qua một quá trình đóng băng lặp. Việc giảm bộ nhớ GPU trên thiết bị tổng thể của SLIMFIT là kết quả của việc giảm bộ nhớ trong cả activations động và tĩnh. Activations tĩnh đóng góp một lượng bộ nhớ cố định trong khi việc sử dụng bộ nhớ của activations động phụ thuộc vào tỷ lệ đóng băng. Với tỷ lệ đóng băng cao, dấu chân bộ nhớ của activations và do đó việc sử dụng bộ nhớ GPU trên thiết bị tổng thể có thể được giảm đáng kể. Việc lựa chọn tỷ lệ đóng băng phụ thuộc vào ngân sách bộ nhớ của người dùng. Bằng cách tăng tỷ lệ đóng băng lên đến một điểm nhất định, sẽ không có suy giảm hiệu suất. Tuy nhiên, việc sử dụng tỷ lệ đóng băng cực cao đánh đổi bộ nhớ lấy độ chính xác. Việc tìm điểm ngắt của phương pháp phụ thuộc vào tác vụ và thay đổi từ dataset này sang dataset khác.

7 Kết quả thực nghiệm

Chúng tôi sử dụng phiên bản cơ sở của BERT và ViT cho các thực nghiệm của chúng tôi. Chúng tôi điều chỉnh tinh hai mô hình này sử dụng SLIMFIT được triển khai trên PyTorch. Chúng tôi đánh giá BERT [1] sử dụng benchmark GLUE [31] và SQuAD 2.0 [32]. Đối với ViT [2], chúng tôi sử dụng các dataset CIFAR-10, CIFAR-100 và ImageNet [33, 34] cho mục đích đánh giá. Chúng tôi thảo luận về việc sử dụng bộ nhớ của activations và bộ nhớ GPU trên thiết bị tổng thể trên GPU NVIDIA V100 32GB. Chúng tôi báo cáo việc sử dụng bộ nhớ GPU trên thiết bị tổng thể sử dụng "nvidia-smi". Đối với tất cả các thực nghiệm trong phần này, chúng tôi sử dụng 3 epoch cho điều chỉnh tinh. Chi tiết về các tác vụ CV/NLP, đo lường và cài đặt siêu tham số được cung cấp trong Phụ lục G.

7.1 Đánh giá độ chính xác trên GLUE và SQuAD 2.0

Để đánh giá khả năng hiểu ngôn ngữ của các mô hình BERT, benchmark GLUE được hình thành bởi một loạt các tác vụ downstream bao gồm phân loại cảm xúc (SST-2), suy luận ngôn ngữ tự nhiên (RTE, QNLI và MNLI), phát hiện paraphrase (MRPC, QQP và STS-B), và tính chấp nhận ngôn ngữ học (CoLA). Chúng tôi sử dụng tương quan Spearman cho STS-B, tương quan Matthew cho CoLA, độ chính xác phần trăm cho RTE, MRPC, SST-2, QQP, QNLI và MNLI m, và điểm F1 cho SQuAD 2.0. Trong công trình này, chúng tôi điều chỉnh tinh mô hình BERT-base sử dụng SLIMFIT trên các tác vụ downstream của benchmark GLUE cũng như tác vụ trả lời câu hỏi trên SQuAD 2.0. Bảng 3 cho thấy độ chính xác trên tập validation của các tác vụ nói trên và việc sử dụng bộ nhớ của SLIMFIT so với baseline. Kết quả của baseline được thu thập mà không có đóng băng. Chúng tôi báo cáo các kết quả liên quan đến tỷ lệ đóng băng cao nhất có thể đạt độ chính xác tương tự như baseline bằng cách thay đổi tỷ lệ học. Kết quả thực nghiệm trên benchmark GLUE cho thấy lên đến 95% activations động có thể bị loại bỏ với lên đến 0,4% suy giảm độ chính xác, dẫn đến trung bình 1,9GB giảm trong việc sử dụng bộ nhớ GPU trên thiết bị tổng thể. Mặt khác, trong khi điều chỉnh tinh SQuAD 2.0 mà không có đóng băng yêu cầu tối thiểu 2 GPU NVIDIA V100 32GB trên kích thước batch 128, SLIMFIT cho phép điều chỉnh tinh trên một GPU NVIDIA V100 32GB, giảm yêu cầu bộ nhớ trên thiết bị tổng thể của tác vụ như vậy từ 58,5GB xuống 19,1GB (giảm 3,1×).

Hình 6 cho thấy việc sử dụng bộ nhớ GPU trên thiết bị tổng thể của BERT khi được điều chỉnh tinh sử dụng SLIMFIT cho các kích thước batch khác nhau tại tỷ lệ đóng băng 95% trên benchmark GLUE và 80% trên SQuAD 2.0. Theo kết quả thực nghiệm, SLIMFIT cho phép giảm từ 1,5× đến 3,1× trong bộ nhớ GPU trên thiết bị tổng thể trên các tác vụ NLP. Việc giảm trong việc sử dụng bộ nhớ trên thiết bị tổng thể đáng kể hơn đối với các kích thước batch lớn hơn vì activations chiếm ưu thế trong dấu chân bộ nhớ.

7.2 Đánh giá độ chính xác trên CIFAR và ImageNet

Để đánh giá hiệu quả của phương pháp của chúng tôi trên các tác vụ CV, chúng tôi điều chỉnh tinh mô hình ViT-base trên các dataset CIFAR-10, CIFAR-100 và ImageNet. Chúng tôi sử dụng tập test của CIFAR-10/CIFAR-100 và tập validation của ImageNet để đánh giá độ chính xác của chúng trên ViT. Bảng 4 cho thấy SLIMFIT có thể điều chỉnh tinh mô hình ViT-base với tỷ lệ đóng băng lên đến 95% với lên đến 0,3% mất mát độ chính xác trong khi giảm đáng kể việc sử dụng bộ nhớ GPU trên thiết bị tổng thể. Cụ thể hơn, SLIMFIT giảm việc sử dụng bộ nhớ tổng thể của quá trình điều chỉnh tinh trên CIFAR-10 từ 7,2GB xuống 4,3GB (giảm 1,7×) với kích thước batch 32, trên CIFAR-100 từ 7,2GB xuống 4,5GB (giảm 1,6×) với kích thước batch 32, và trên ImageNet từ 77,4GB xuống 26,1GB (giảm 3×) với kích thước batch 128. Hình 6 cũng cho thấy việc sử dụng bộ nhớ GPU trên thiết bị tổng thể của SLIMFIT trên các kích thước batch khác nhau trên các tác vụ CV.

8 Nghiên cứu ablation

Trong phần này, chúng tôi nghiên cứu các khía cạnh khác nhau của SLIMFIT trong điều chỉnh tinh các mô hình dựa trên transformer thông qua một loạt các nghiên cứu ablation. Do hạn chế về không gian, chúng tôi thảo luận về tác động của lượng tử hóa/cắt tỉa và tổng thời gian wall-clock trong Phụ lục H và Phụ lục I, tương ứng. Đối với tất cả các thực nghiệm trong phần này, chúng tôi sử dụng kích thước batch 32 và 3 epoch cho điều chỉnh tinh.

8.1 Độ chính xác so với tỷ lệ đóng băng

Trong Phần (3.2), chúng tôi đã thảo luận rằng thuật toán ILS của chúng tôi điều phối lịch trình đóng băng dựa trên một quy tắc đơn giản: các lớp có giá trị khoảng cách lớn nhất được cập nhật trong khi những lớp có giá trị khoảng cách thấp nhất được giữ đóng băng cho tỷ lệ đóng băng nhất định. Tất nhiên, phương pháp đóng băng lặp như vậy đánh đổi giữa độ chính xác và tỷ lệ đóng băng. Để hiển thị rõ hơn sự đánh đổi này, chúng tôi đo và minh họa độ chính xác của các dataset CoLA và MRPC trên các tỷ lệ đóng băng khác nhau trong Hình 7. Đường cong đánh đổi cho thấy thuật toán ILS của chúng tôi có thể duy trì độ chính xác ở cùng mức với baseline bằng cách đóng băng lên đến 95% các lớp.

Bên cạnh thuật toán ILS của chúng tôi, lịch trình đóng băng có thể được quyết định sử dụng các phương pháp đóng băng ngẫu nhiên hoặc tiến triển. Trong phương pháp lập lịch ngẫu nhiên, các lớp bị đóng băng được chọn ngẫu nhiên tại mỗi iteration. Trong phương pháp tiến triển, mặt khác, các lớp sớm được dần dần giữ đóng băng trong khi các lớp sau được cập nhật trong suốt quá trình điều chỉnh tinh. Trong số các phương pháp này, thuật toán ILS của chúng tôi nổi bật đáng kể về cả độ chính xác và tỷ lệ đóng băng như được hiển thị trong Hình 7. Lý do đằng sau hiệu suất vượt trội của nó là ILS cho phép nhiều cập nhật hơn cho các lớp có giá trị khoảng cách lớn bằng cách giữ các lớp có giá trị khoảng cách tối thiểu đóng băng trong một số iteration cụ thể. Mặt khác, trong phương pháp ngẫu nhiên, các lớp được chọn ngẫu nhiên để được cập nhật. Do đó, các lớp có giá trị khoảng cách lớn nhận được ít cập nhật hơn trong phương pháp ngẫu nhiên so với ILS. Tất nhiên, cơ hội của các lớp có giá trị khoảng cách lớn được chọn ngẫu nhiên như các lớp hoạt động giảm khi tỷ lệ đóng băng tăng, điều này giải thích khoảng cách độ chính xác giữa ILS và phương pháp ngẫu nhiên với tỷ lệ đóng băng cao hơn 70%. Trong phương pháp đóng băng tiến triển, các lớp sớm không nhận được cập nhật nào trong quá trình điều chỉnh tinh, dẫn đến suy giảm độ chính xác đáng kể đối với các tỷ lệ đóng băng lớn.

8.2 Tần suất xuất hiện cập nhật

Để hiển thị tần suất xuất hiện cập nhật cho mỗi lớp, chúng tôi sử dụng heatmap như được hiển thị trong Hình 8 cho cả hai dataset CoLA và MRPC trong đó số lượng lớn hơn được liên kết với màu sắc tối hơn. Như được hiển thị trong heatmap, các lớp dense bên trong module MHA nhận được nhiều cập nhật hơn các lớp khác cho cả hai dataset. Hơn nữa, các mẫu cập nhật của các dataset này tương tự đối với các tỷ lệ đóng băng nhỏ trong khi chúng trở nên cụ thể hơn theo tác vụ đối với các tỷ lệ đóng băng cao. Trên thực tế, thuật toán ILS ưu tiên cập nhật một số lớp cụ thể hơn các lớp khác đối với các tỷ lệ đóng băng cao.

9 So sánh với các kỹ thuật tối tân và hạn chế

Tiếp theo, chúng tôi so sánh SLIMFIT với các phương pháp nén tối tân nhắm vào giảm bộ nhớ, tức là GACT [4] và DropIT [5]. Bảng 5 tóm tắt kết quả so sánh về độ chính xác, bộ nhớ và độ trễ. Để so sánh công bằng, chúng tôi đo hiệu suất của chúng dưới cùng framework và siêu tham số (tức là kích thước batch và số epoch huấn luyện) trong quá trình điều chỉnh tinh BERT trên CoLA. Kết quả thực nghiệm của GACT và DropIT được thu thập sử dụng các thư viện PyTorch chính thức của chúng. Theo kết quả thực nghiệm, GACT cho thấy lượng bộ nhớ thấp nhất cho activations. Tuy nhiên, về việc sử dụng bộ nhớ GPU trên thiết bị, SLIMFIT vượt trội hơn GACT. Về độ chính xác, tất cả các mô hình đều cho thấy độ chính xác có thể so sánh trên CoLA so với baseline. Cuối cùng, về tốc độ, SLIMFIT cho thấy tốc độ điều chỉnh tinh nhanh nhất trong số các công trình hiện có trong khi nó vẫn kém hơn so với baseline (xem Phụ lục I để biết thêm chi tiết về tốc độ tính toán của SLIMFIT). Mặc dù độ chính xác tốt hơn của SLIMFIT trên CoLA, nó cho thấy lên đến 0,4% suy giảm độ chính xác trên các tác vụ CV/NLP khác nhau, đây là một hạn chế khác của SLIMFIT bên cạnh tốc độ điều chỉnh tinh của nó so với baseline.

10 Kết luận

Trong bài báo này, chúng tôi đã trình bày một công cụ hiệu suất gọi là SLIMFIT để giảm việc sử dụng bộ nhớ của activations và do đó việc sử dụng bộ nhớ GPU trên thiết bị tổng thể của các mô hình dựa trên transformer thông qua việc đóng băng lặp các lớp trong quá trình điều chỉnh tinh. SLIMFIT áp dụng phương pháp lập lịch liên lớp để điều phối lịch trình đóng băng tại mỗi iteration. Để cân bằng số lượng activations trên tất cả các lớp và giảm việc sử dụng bộ nhớ của activations tĩnh, SLIMFIT sử dụng lượng tử hóa và cắt tỉa cho một vài lớp cụ thể. Chúng tôi đánh giá hiệu suất của SLIMFIT trên các tác vụ NLP và CV khác nhau. Chúng tôi cho thấy SLIMFIT giảm đáng kể việc sử dụng bộ nhớ GPU trên thiết bị của quá trình điều chỉnh tinh lên đến 3,1× khi sử dụng kích thước batch 128.
