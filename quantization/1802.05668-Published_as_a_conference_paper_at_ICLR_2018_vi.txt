Theo các tác giả của bài báo, chúng tôi không sử dụng các lớp dropout khi huấn luyện các mô hình sử dụng mất mát chưng cất. Mất mát chưng cất được tính với nhiệt độ T = 5.

Bảng 9 báo cáo độ chính xác của các mô hình được huấn luyện (ở full precision) và kích thước của chúng. Bảng 10 báo cáo độ chính xác đạt được với mỗi phương pháp, và bảng 11 báo cáo độ dài bit trung bình tối ưu sử dụng mã hóa Huffman và kích thước mô hình kết quả.

Bảng 9: CIFAR10: Độ chính xác mô hình giáo viên và chưng cất, full precision

[THIS IS TABLE: Shows accuracy, parameters, and size for teacher and student models]

Bảng 10: CIFAR10: Độ chính xác kiểm tra cho các mô hình lượng tử hóa. Kết quả tính với kích thước bucket = 256

[THIS IS TABLE: Shows test accuracy for quantized models with 2, 4, and 8 bits]

Bảng 11: CIFAR10: Mã hóa Huffman độ dài tối ưu và kích thước mô hình kết quả. Kích thước bucket = 256

[THIS IS TABLE: Shows optimal Huffman encoding lengths and resulting model sizes]

--- TRANG 14 ---
Chúng tôi cũng thực hiện thí nghiệm với mô hình học sinh sâu hơn. Kiến trúc là 76c3-mp-dp-126c3-mp-dp-148c5-mp-dp-1000fc-dp-1000fc-dp-1000fc (theo cùng ký hiệu như trong bảng 8). Chúng tôi sử dụng cùng giáo viên như trong các thí nghiệm trước. Kết quả trong bảng 13.

Bảng 12: CIFAR10: Độ chính xác mô hình giáo viên và chưng cất, full precision

[THIS IS TABLE: Shows model accuracy, parameters, and size for teacher and deeper student models]

Bảng 13: CIFAR10: Độ chính xác kiểm tra cho mô hình học sinh sâu hơn lượng tử hóa. Kết quả tính với kích thước bucket = 256

[THIS IS TABLE: Shows test accuracy for deeper quantized student model]

Bảng 14: CIFAR10: Mã hóa Huffman độ dài tối ưu và kích thước mô hình kết quả cho mô hình học sinh sâu hơn. Kích thước bucket = 256

[THIS IS TABLE: Shows optimal Huffman encoding and model sizes for deeper student]

A.1.1 CIFAR10 - KIẾN TRÚC WIDERESNET

Cho tập thí nghiệm thứ hai của chúng tôi trên CIFAR10 với kiến trúc WideResNet, xem bảng 15. Lưu ý rằng chúng tôi tăng số bộ lọc nhưng giảm độ sâu của mô hình. Việc thực hiện WideResNet được sử dụng có thể tìm thấy trên GitHub². Kết quả của các phương pháp lượng tử hóa trong bảng 16 trong khi kích thước của các mô hình kết quả được chi tiết trong bảng 17.

Bảng 15: CIFAR10: Độ chính xác mô hình giáo viên và chưng cất, full precision, wide resnet

[THIS IS TABLE: Shows model structure, accuracy, parameters, and size for wide resnet models]

Bảng 16: CIFAR10: Độ chính xác kiểm tra cho các mô hình lượng tử hóa. Kết quả tính với kích thước bucket = 256

[THIS IS TABLE: Shows test accuracy for quantized models]

A.2 CIFAR100

Cho các thí nghiệm CIFAR100 của chúng tôi, chúng tôi sử dụng cùng việc thực hiện wide residual network như trong các thí nghiệm CIFAR10. Hệ số wide là hệ số nhân kiểm soát số lượng bộ lọc

²https://github.com/meliketoy/wide-resnet.pytorch

--- TRANG 15 ---
Bảng 17: CIFAR10: Mã hóa Huffman độ dài tối ưu và kích thước mô hình kết quả. Kích thước bucket = 256

[THIS IS TABLE: Shows optimal Huffman encoding lengths and model sizes for CIFAR10]

trong mỗi lớp; để biết thêm chi tiết xin tham khảo bài báo gốc Zagoruyko & Komodakis (2016). Chúng tôi huấn luyện trong 200 epoch với tốc độ học ban đầu là 0.1.

Cho các thí nghiệm CIFAR100 chúng tôi tập trung vào một mô hình học sinh. Mất mát chưng cất được tính với nhiệt độ T = 5.

Bảng 18: CIFAR100: Độ chính xác mô hình giáo viên và chưng cất, full precision

[THIS IS TABLE: Shows teacher and student model accuracy for CIFAR100]

Bảng 19: CIFAR100: Độ chính xác kiểm tra cho các mô hình lượng tử hóa. Kết quả tính với kích thước bucket = 256

[THIS IS TABLE: Shows test accuracy for quantized models on CIFAR100]

Bảng 20: CIFAR100: Mã hóa Huffman độ dài tối ưu và kích thước mô hình kết quả. Kích thước bucket = 256

[THIS IS TABLE: Shows optimal Huffman encoding and model sizes for CIFAR100]

A.3 TẬP DỮ LIỆU KIỂM TRA TÍCH HỢP OPENNMT

Như đã đề cập trong văn bản chính, chúng tôi sử dụng codebase openNMT-py. Chúng tôi sửa đổi nhẹ nó để thêm mất mát chưng cất và các phương pháp lượng tử hóa được đề xuất. Chúng tôi chủ yếu sử dụng các tùy chọn tiêu chuẩn để huấn luyện mô hình; cụ thể, tốc độ học bắt đầu ở 1 và được giảm một nửa mỗi epoch bắt đầu từ epoch đầu tiên mà perplexity không giảm trên tập kiểm tra. Chúng tôi huấn luyện mọi mô hình trong 15 epoch. Mất mát chưng cất được tính với nhiệt độ T = 1.

A.4 TẬP DỮ LIỆU WMT13

Cho tập dữ liệu WMT13, chúng tôi chạy kiến trúc tương tự. Chúng tôi chạy tất cả mô hình trong 15 epoch; mô hình nhỏ hơn overfit với 15 epoch, vì vậy chúng tôi chạy nó trong 5 epoch thay thế.

A.4.1 CHƯNG CẤT SO VỚI MẤT MÁT TIÊU CHUẨN CHO LƯỢNG TỬ HÓA

Trong phần này chúng tôi làm nổi bật các tác động tích cực của việc sử dụng mất mát chưng cất trong quá trình lượng tử hóa. Chúng tôi lấy các mô hình có cùng kiến trúc và chúng tôi huấn luyện chúng với cùng số bit; một trong các mô hình được huấn luyện với mất mát bình thường, mô hình khác với mất mát chưng cất với trọng số bằng nhau giữa soft cross entropy và normal cross entropy (tức là, nó là mô hình chưng cất lượng tử hóa).

--- TRANG 16 ---
Bảng 21: openNMT integ: Perplexity và BLEU của mô hình giáo viên và chưng cất, full precision

[THIS IS TABLE: Shows model performance metrics including structure, perplexity, BLEU scores, parameters, and size for teacher and student models]

Bảng 22: openNMT integ: Độ chính xác kiểm tra cho các mô hình lượng tử hóa. Kết quả tính với kích thước bucket = 256

[THIS IS TABLE: Shows test accuracy for quantized models with 2 bits and 4 bits across different approaches]

Bảng 23: openNMT integ: Mã hóa Huffman độ dài tối ưu và kích thước mô hình kết quả. Kích thước bucket = 256

[THIS IS TABLE: Shows optimal Huffman encoding lengths and resulting model sizes]

Bảng 27 hiển thị kết quả trên tập dữ liệu CIFAR10; các mô hình chúng tôi huấn luyện có cùng cấu trúc như Mô hình nhỏ hơn 1, xem Phần A.1.

Bảng 28 hiển thị kết quả trên tập dữ liệu kiểm tra tích hợp openNMT; các mô hình được huấn luyện có cùng cấu trúc của Mô hình nhỏ hơn 1, xem Phần A.3. Lưu ý rằng mất mát chưng cất có thể cải thiện đáng kể độ chính xác của các mô hình lượng tử hóa.

--- TRANG 17 ---
Bảng 24: WMT13: Perplexity và BLEU của mô hình giáo viên và chưng cất, full precision

[THIS IS TABLE: Shows model performance with structure details, perplexity, BLEU scores, parameters, and size]

Bảng 25: WMT13: Độ chính xác kiểm tra cho các mô hình lượng tử hóa. Kết quả tính với kích thước bucket = 256

[THIS IS TABLE: Shows test accuracy for quantized models with 4 bits]

Bảng 26: WMT13: Mã hóa Huffman độ dài tối ưu và kích thước mô hình kết quả. Kích thước bucket = 256

[THIS IS TABLE: Shows optimal Huffman encoding lengths and model sizes]

Bảng 27: CIFAR10: Mất mát chưng cất so với mất mát bình thường khi lượng tử hóa

[THIS IS TABLE: Compares distillation loss vs normal loss for 2 and 4 bits]

Bảng 28: openNMT integ: Mất mát chưng cất so với mất mát bình thường khi lượng tử hóa

[THIS IS TABLE: Shows comparison for 4 bits between normal and distillation loss]

Những kết quả này gợi ý rằng lượng tử hóa hoạt động tốt hơn khi kết hợp với chưng cất, và chúng ta nên cố gắng tận dụng điều này bất cứ khi nào chúng ta đang lượng tử hóa một mạng nơ-ron.

A.4.2 CÁC HEURISTIC KHÁC NHAU CHO LƯỢNG TỬ HÓA CÓ THỂ VI PHÂN

Để kiểm tra các heuristic khác nhau được trình bày trong Phần 4.2, chúng tôi huấn luyện với lượng tử hóa có thể vi phân kiến trúc Mô hình nhỏ hơn 1 được chỉ định trong Phần A.1 trên tập dữ liệu cifar10. Cùng mô hình được huấn luyện với các heuristic khác nhau để cung cấp cảm nhận về tầm quan trọng của chúng; thí nghiệm được thực hiện với 2 và 4 bit.

Kết quả gợi ý rằng khi sử dụng 4 bit, phương pháp này mạnh mẽ và hoạt động bất kể gì. Khi sử dụng 2 bit, việc phân phối lại bit theo norm gradient của các lớp là hoàn toàn cần thiết để phương pháp này hoạt động; điểm khởi đầu quantile cũng dường như cung cấp cải thiện nhỏ, trong khi sử dụng mất mát chưng cất trong trường hợp này dường như không quan trọng.

B LƯỢNG TỬ HÓA TƯƠNG ĐƯƠNG VỚI NHIỄU PHÂN PHỐI CHUẨN TIỆM CẬN

Trong phần này chúng tôi sẽ chứng minh một số kết quả về hàm lượng tử hóa đồng nhất, bao gồm việc nó được phân phối chuẩn tiệm cận, xem tiểu mục B.1 bên dưới. Rõ ràng, chúng tôi đề cập đến phiên bản ngẫu nhiên, xem Phần 2.1.

--- TRANG 18 ---
Bảng 29: Kết quả với bit được phân phối lại tự động

[THIS IS TABLE: Shows results for Quantiles and Uniform across 2 bits and 4 bits with distillation and normal loss]

Bảng 30: Kết quả không có bit được phân phối lại tự động

[THIS IS TABLE: Shows results without automatic bit redistribution for Quantiles and Uniform]

Tính không thiên vị

Chúng tôi bắt đầu bằng cách chứng minh tính không thiên vị của Q̂;

E[Q̂(v̂)_i] = (⌊sv̂_i⌋c/s) + (1/s)E[ε_i] = (⌊sv̂_i⌋c/s) + (1/s)(sv̂_i - ⌊sv̂_i⌋c) = v̂_i (8)

Sau đó hiển nhiên rằng

E[Q(v)] = E[Q̂((v-β)/α) · α + β] = ((v-β)/α) · α + β = v (9)

Ràng buộc về moment thứ hai và thứ ba

Chúng tôi sẽ viết ra các ràng buộc về Q̂; các ràng buộc tương tự về Q sau đó là đơn giản. Để thuận tiện, hãy gọi ℓ̂_i = ⌊sv̂_i⌋c

E[Q̂(v̂)^2_i] = (ℓ̂^2_i/s^2) + (1/s^2)E[ε^2_i] + (2ℓ̂_i/s^2)E[ε_i] = (10)

= (ℓ̂^2_i/s^2) + (1/s^2)(sv̂_i - ℓ̂_i) + (2ℓ̂_i/s^2)(sv̂_i - ℓ̂_i) = (11)

= (1/s^2)[v̂_is(1 + 2ℓ̂_i) - ℓ̂_i(ℓ̂_i + 1)] (12)

Và cho rằng ℓ̂_i ≤ sv̂_i ≤ ℓ̂_i + 1, chúng ta dễ dàng tìm thấy

(ℓ̂^2_i/s^2) ≤ E[Q̂(v̂)^2_i] ≤ ((ℓ̂_i + 1)^2/s^2) (13)

Cho moment thứ ba, chúng ta có

E[Q̂(v̂)^3_i] = (ℓ̂^3_i/s^3) + (1/s^3)E[ε^3_i] + (3ℓ̂_i/s^3)E[ε^2_i] + (3ℓ̂^2_i/s^3)E[ε_i] = (14)

= (ℓ̂^3_i/s^3) + (1/s^3)(sv̂_i - ℓ̂_i) + (3ℓ̂_i/s^3)(sv̂_i - ℓ̂_i) + (3ℓ̂^2_i/s^3)(sv̂_i - ℓ̂_i) = (15)

= (1/s^3)[v̂_is(3ℓ̂^2_i + 3ℓ̂_i + 1) - ℓ̂_i(2ℓ̂^2_i + 3ℓ̂_i + 1)] (16)

Và như trước, các ràng buộc là

(ℓ̂^3_i/s^3) ≤ E[Q̂(v̂)^3_i] ≤ ((ℓ̂_i + 1)^3/s^3) (17)

B.1 TÍNH CHUẨN TIỆM CẬN

Hầu hết các phép toán mạng nơ-ron là tính toán tích vô hướng. Do đó, tích vô hướng của trọng số lượng tử hóa và đầu vào là một đại lượng quan trọng:

Q(v)^T x = Σ^n_{i=1} Q(v_i)x_i

Chúng ta đã biết từ phần B rằng hàm lượng tử hóa không thiên vị; do đó chúng ta biết rằng

Σ^n_{i=1} Q(v_i)x_i = Σ^n_{i=1} v_i x_i + ε_n (18)

với ε_n là biến ngẫu nhiên có trung bình bằng không. Chúng tôi sẽ chỉ ra rằng ε_n có xu hướng theo phân phối về một biến ngẫu nhiên chuẩn. Để chứng minh tính chuẩn tiệm cận, chúng tôi sẽ sử dụng một phiên bản tổng quát của định lý giới hạn trung tâm do Lyapunov:

Định lý B.1 (Định lý Giới hạn Trung tâm Lyapunov). Gọi {X_1, X_2, ...} là một dãy các biến ngẫu nhiên độc lập, mỗi biến có kỳ vọng hữu hạn μ_i và phương sai σ^2_i. Định nghĩa s^2_n = Σ^n_{i=1} σ^2_i. Nếu, với một δ > 0 nào đó, điều kiện Lyapunov

lim_{n→∞} (1/s^{2+δ}_n) Σ^n_{i=1} E[|X_i - μ_i|^{2+δ}] = 0 (19)

được thỏa mãn, thì

(1/s_n) Σ^n_{i=1} (X_i - μ_i) →^D N(0,1) (20)

Bây giờ chúng ta có thể phát biểu định lý:

Định lý B.2. Gọi v, x là hai vector với n phần tử. Gọi Q là hàm lượng tử hóa đồng nhất với s mức được định nghĩa trong 2.1 và định nghĩa s^2_n = Σ^n_{i=1} Var[Q(v_i)x_i]. Nếu các phần tử của v, x được ràng buộc đồng nhất bởi M³ và lim_{n→∞} s_n = ∞, thì

Σ^n_{i=1} Q(v_i)x_i = Σ^n_{i=1} v_i x_i + ε_n (21)

với E[ε_n] = 0 và

lim_{n→∞} (1/s_n)ε_n →^D N(0,1) (22)

Chứng minh. Sử dụng cùng ký hiệu như định lý B.1, gọi X_i = Q(v_i)x_i, μ_i = E[X_i] = v_i x_i. Chúng ta đã đề cập trong 2.1 rằng đây là các biến ngẫu nhiên độc lập. Chúng tôi sẽ chỉ ra rằng điều kiện Lyapunov được thỏa mãn với δ = 1.

Chúng ta biết rằng

E[|X_i - μ_i|^3] = E[(X_i - μ_i)^2|X_i - μ_i|] ≤ (M^2/s)E[(X_i - μ_i)^2] (23)

Thực tế,

|X_i - μ_i| = |x_i||Q(v_i) - v_i| = |x_i||Q̂((v_i - β_i)/α_i) · α_i + β_i - v_i| (24)

≤ |x_i||α_i||Q̂((v_i - β_i)/α_i) - (v_i - β_i)/α_i| ≤ |x_i|M/(s) ≤ M^2/s (26)

vì trong quá trình lượng tử hóa chúng ta có các bin có kích thước 1/s, vì vậy đó là lỗi lớn nhất chúng ta có thể mắc phải. Cũng vậy, theo giả thiết |α_i|, |x_i| ≤ M cho mọi i.

Do đó

0 ≤ (1/s^3_n) Σ^n_{i=1} E[|X_i - μ_i|^3] ≤ (1/s^3_n) · (M^2/s) Σ^n_{i=1} E[(X_i - μ_i)^2] = (M^2/s) · (1/s_n) (27)

và vì lim_{n→∞} s_n = ∞, chúng ta có điều kiện Lyapunov được thỏa mãn. Do đó

(1/s_n) Σ^n_{i=1} (X_i - μ_i) = (1/s_n) Σ^n_{i=1} (Q(v_i)x_i - v_i x_i) = (1/s_n)ε_n →^D N(0,1) (28)

Ghi chú về giả thiết. Hai giả thiết được sử dụng để chứng minh định lý là hợp lý và nên được thỏa mãn bởi bất kỳ tập dữ liệu thực tế nào. Thông thường chúng ta biết hoặc có thể ước tính phạm vi của giá trị đầu vào và trọng số, vì vậy giả thiết rằng chúng không trở nên lớn tùy ý với n được thỏa mãn. Giả thiết về phương sai cũng hợp lý; thực tế, s^2_n = Σ^n_{i=1} Var[Q(v_i)x_i] bao gồm tổng của n giá trị. Mặc dù có thể tất cả các giá trị này bằng 0 (nếu tất cả v_i có dạng k/s, ví dụ, thì s^2_n = 0) nhưng không chắc rằng một tập dữ liệu thực tế sẽ có đặc tính này. Thực tế, đủ rằng tồn tại ρ > 0 và 0 < c < 1 sao cho ít nhất c-phần trăm của σ^2_i ≥ ρ. Điều này ngụ ý s^2_n ≥ cnρ → ∞.

Tính chuẩn tiệm cận khi lượng tử hóa đầu vào. Định lý B.2 có thể dễ dàng mở rộng cho trường hợp khi x_i cũng được lượng tử hóa. Chứng minh gần như giống hệt; chúng ta chỉ cần đặt X_i = Q(v_i)Q(x_i) và sử dụng tính độc lập của Q(x_i) và Q(v_i). Để hoàn chỉnh, chúng tôi báo cáo phát biểu của định lý:

Định lý B.3. Gọi v, x là hai vector với n phần tử. Gọi Q là hàm lượng tử hóa đồng nhất với s mức được định nghĩa trong 2.1 và định nghĩa s^2_n = Σ^n_{i=1} Var[Q(v_i)Q(x_i)]. Nếu các phần tử của v, x được ràng buộc đồng nhất bởi M⁴ và lim_{n→∞} s_n = ∞, thì

Σ^n_{i=1} Q(v_i)Q(x_i) = Σ^n_{i=1} v_i x_i + ε_n (29)

với E[ε_n] = 0 và

lim_{n→∞} (1/s_n)ε_n →^D N(0,1) (30)

³tức là tồn tại hằng số M sao cho với mọi n, |v_i| ≤ M, |x_i| ≤ M cho mọi i ∈ {1, ..., n}
⁴tức là tồn tại hằng số M sao cho với mọi n, |v_i| ≤ M, |x_i| ≤ M cho mọi i ∈ {1, ..., n}

--- TRANG 19 ---

--- TRANG 20 ---

--- TRANG 21 ---
